PROCESS
CONTROL
INTRODUCTION
WE
NOW
TURN
TO
THE
PROCESS
CONTROL
PROVIDED
BY
THE
UNIX
SYSTEM
THIS
INCLUDES
THE
CREATION
OF
NEW
PROCESSES
PROGRAM
EXECUTION
AND
PROCESS
TERMINATION
WE
ALSO
LOOK
AT
THE
VARIOUS
IDS
THAT
ARE
THE
PROPERTY
OF
THE
PROCESS
REAL
EFFECTIVE
AND
SAVED
USER
AND
GROUP
IDS
AND
HOW
THEY
RE
AFFECTED
BY
THE
PROCESS
CONTROL
PRIMITIVES
INTERPRETER
FILES
AND
THE
SYSTEM
FUNCTION
ARE
ALSO
COVERED
WE
CONCLUDE
THE
CHAPTER
BY
LOOKING
AT
THE
PROCESS
ACCOUNTING
PROVIDED
BY
MOST
UNIX
SYSTEMS
THIS
LETS
US
LOOK
AT
THE
PROCESS
CONTROL
FUNCTIONS
FROM
A
DIFFERENT
PERSPECTIVE
PROCESS
IDENTIFIERS
EVERY
PROCESS
HAS
A
UNIQUE
PROCESS
ID
A
NON
NEGATIVE
INTEGER
BECAUSE
THE
PROCESS
ID
IS
THE
ONLY
WELL
KNOWN
IDENTIFIER
OF
A
PROCESS
THAT
IS
ALWAYS
UNIQUE
IT
IS
OFTEN
USED
AS
A
PIECE
OF
OTHER
IDENTIFIERS
TO
GUARANTEE
UNIQUENESS
FOR
EXAMPLE
APPLICATIONS
SOMETIMES
INCLUDE
THE
PROCESS
ID
AS
PART
OF
A
FILENAME
IN
AN
ATTEMPT
TO
GENERATE
UNIQUE
FILENAMES
ALTHOUGH
UNIQUE
PROCESS
IDS
ARE
REUSED
AS
PROCESSES
TERMINATE
THEIR
IDS
BECOME
CANDIDATES
FOR
REUSE
MOST
UNIX
SYSTEMS
IMPLEMENT
ALGORITHMS
TO
DELAY
REUSE
HOWEVER
SO
THAT
NEWLY
CREATED
PROCESSES
ARE
ASSIGNED
IDS
DIFFERENT
FROM
THOSE
USED
BY
PROCESSES
THAT
TERMINATED
RECENTLY
THIS
PREVENTS
A
NEW
PROCESS
FROM
BEING
MISTAKEN
FOR
THE
PREVIOUS
PROCESS
TO
HAVE
USED
THE
SAME
ID
THERE
ARE
SOME
SPECIAL
PROCESSES
BUT
THE
DETAILS
DIFFER
FROM
IMPLEMENTATION
TO
IMPLEMENTATION
PROCESS
ID
IS
USUALLY
THE
SCHEDULER
PROCESS
AND
IS
OFTEN
KNOWN
AS
THE
SWAPPER
NO
PROGRAM
ON
DISK
CORRESPONDS
TO
THIS
PROCESS
WHICH
IS
PART
OF
THE
KERNEL
AND
IS
KNOWN
AS
A
SYSTEM
PROCESS
PROCESS
ID
IS
USUALLY
THE
INIT
PROCESS
AND
IS
INVOKED
BY
THE
KERNEL
AT
THE
END
OF
THE
BOOTSTRAP
PROCEDURE
THE
PROGRAM
FILE
FOR
THIS
PROCESS
WAS
ETC
INIT
IN
OLDER
VERSIONS
OF
THE
UNIX
SYSTEM
AND
IS
SBIN
INIT
IN
NEWER
VERSIONS
THIS
PROCESS
IS
RESPONSIBLE
FOR
BRINGING
UP
A
UNIX
SYSTEM
AFTER
THE
KERNEL
HAS
BEEN
BOOTSTRAPPED
INIT
USUALLY
READS
THE
SYSTEM
DEPENDENT
INITIALIZATION
FILES
THE
ETC
RC
FILES
OR
ETC
INITTAB
AND
THE
FILES
IN
ETC
INIT
D
AND
BRINGS
THE
SYSTEM
TO
A
CERTAIN
STATE
SUCH
AS
MULTIUSER
THE
INIT
PROCESS
NEVER
DIES
IT
IS
A
NORMAL
USER
PROCESS
NOT
A
SYSTEM
PROCESS
WITHIN
THE
KERNEL
LIKE
THE
SWAPPER
ALTHOUGH
IT
DOES
RUN
WITH
SUPERUSER
PRIVILEGES
LATER
IN
THIS
CHAPTER
WE
LL
SEE
HOW
INIT
BECOMES
THE
PARENT
PROCESS
OF
ANY
ORPHANED
CHILD
PROCESS
IN
MAC
OS
X
THE
INIT
PROCESS
WAS
REPLACED
WITH
THE
LAUNCHD
PROCESS
WHICH
PERFORMS
THE
SAME
SET
OF
TASKS
AS
INIT
BUT
HAS
EXPANDED
FUNCTIONALITY
SEE
SECTION
IN
SINGH
FOR
A
DISCUSSION
OF
HOW
LAUNCHD
OPERATES
EACH
UNIX
SYSTEM
IMPLEMENTATION
HAS
ITS
OWN
SET
OF
KERNEL
PROCESSES
THAT
PROVIDE
OPERATING
SYSTEM
SERVICES
FOR
EXAMPLE
ON
SOME
VIRTUAL
MEMORY
IMPLEMENTATIONS
OF
THE
UNIX
SYSTEM
PROCESS
ID
IS
THE
PAGEDAEMON
THIS
PROCESS
IS
RESPONSIBLE
FOR
SUPPORTING
THE
PAGING
OF
THE
VIRTUAL
MEMORY
SYSTEM
IN
ADDITION
TO
THE
PROCESS
ID
THERE
ARE
OTHER
IDENTIFIERS
FOR
EVERY
PROCESS
THE
FOLLOWING
FUNCTIONS
RETURN
THESE
IDENTIFIERS
NOTE
THAT
NONE
OF
THESE
FUNCTIONS
HAS
AN
ERROR
RETURN
WE
LL
RETURN
TO
THE
PARENT
PROCESS
ID
IN
THE
NEXT
SECTION
WHEN
WE
DISCUSS
THE
FORK
FUNCTION
THE
REAL
AND
EFFECTIVE
USER
AND
GROUP
IDS
WERE
DISCUSSED
IN
SECTION
FORK
FUNCTION
AN
EXISTING
PROCESS
CAN
CREATE
A
NEW
ONE
BY
CALLING
THE
FORK
FUNCTION
THE
NEW
PROCESS
CREATED
BY
FORK
IS
CALLED
THE
CHILD
PROCESS
THIS
FUNCTION
IS
CALLED
ONCE
BUT
RETURNS
TWICE
THE
ONLY
DIFFERENCE
IN
THE
RETURNS
IS
THAT
THE
RETURN
VALUE
IN
THE
CHILD
IS
WHEREAS
THE
RETURN
VALUE
IN
THE
PARENT
IS
THE
PROCESS
ID
OF
THE
NEW
CHILD
THE
REASON
THE
CHILD
PROCESS
ID
IS
RETURNED
TO
THE
PARENT
IS
THAT
A
PROCESS
CAN
HAVE
MORE
THAN
ONE
CHILD
AND
THERE
IS
NO
FUNCTION
THAT
ALLOWS
A
PROCESS
TO
OBTAIN
THE
PROCESS
IDS
OF
ITS
CHILDREN
THE
REASON
FORK
RETURNS
TO
THE
CHILD
IS
THAT
A
PROCESS
CAN
HAVE
ONLY
A
SINGLE
PARENT
AND
THE
CHILD
CAN
ALWAYS
CALL
GETPPID
TO
OBTAIN
THE
PROCESS
ID
OF
ITS
PARENT
PROCESS
ID
IS
RESERVED
FOR
USE
BY
THE
KERNEL
SO
IT
NOT
POSSIBLE
FOR
TO
BE
THE
PROCESS
ID
OF
A
CHILD
BOTH
THE
CHILD
AND
THE
PARENT
CONTINUE
EXECUTING
WITH
THE
INSTRUCTION
THAT
FOLLOWS
THE
CALL
TO
FORK
THE
CHILD
IS
A
COPY
OF
THE
PARENT
FOR
EXAMPLE
THE
CHILD
GETS
A
COPY
OF
THE
PARENT
DATA
SPACE
HEAP
AND
STACK
NOTE
THAT
THIS
IS
A
COPY
FOR
THE
CHILD
THE
PARENT
AND
THE
CHILD
DO
NOT
SHARE
THESE
PORTIONS
OF
MEMORY
THE
PARENT
AND
THE
CHILD
DO
SHARE
THE
TEXT
SEGMENT
HOWEVER
SECTION
MODERN
IMPLEMENTATIONS
DON
T
PERFORM
A
COMPLETE
COPY
OF
THE
PARENT
DATA
STACK
AND
HEAP
SINCE
A
FORK
IS
OFTEN
FOLLOWED
BY
AN
EXEC
INSTEAD
A
TECHNIQUE
CALLED
COPY
ON
WRITE
COW
IS
USED
THESE
REGIONS
ARE
SHARED
BY
THE
PARENT
AND
THE
CHILD
AND
HAVE
THEIR
PROTECTION
CHANGED
BY
THE
KERNEL
TO
READ
ONLY
IF
EITHER
PROCESS
TRIES
TO
MODIFY
THESE
REGIONS
THE
KERNEL
THEN
MAKES
A
COPY
OF
THAT
PIECE
OF
MEMORY
ONLY
TYPICALLY
A
PAGE
IN
A
VIRTUAL
MEMORY
SYSTEM
SECTION
OF
BACH
AND
SECTIONS
AND
OF
MCKUSICK
ET
AL
PROVIDE
MORE
DETAIL
ON
THIS
FEATURE
VARIATIONS
OF
THE
FORK
FUNCTION
ARE
PROVIDED
BY
SOME
PLATFORMS
ALL
FOUR
PLATFORMS
DISCUSSED
IN
THIS
BOOK
SUPPORT
THE
VFORK
VARIANT
DISCUSSED
IN
THE
NEXT
SECTION
LINUX
ALSO
PROVIDES
NEW
PROCESS
CREATION
THROUGH
THE
CLONE
SYSTEM
CALL
THIS
IS
A
GENERALIZED
FORM
OF
FORK
THAT
ALLOWS
THE
CALLER
TO
CONTROL
WHAT
IS
SHARED
BETWEEN
PARENT
AND
CHILD
FREEBSD
PROVIDES
THE
RFORK
SYSTEM
CALL
WHICH
IS
SIMILAR
TO
THE
LINUX
CLONE
SYSTEM
CALL
THE
RFORK
CALL
IS
DERIVED
FROM
THE
PLAN
OPERATING
SYSTEM
PIKE
ET
AL
SOLARIS
PROVIDES
TWO
THREADS
LIBRARIES
ONE
FOR
POSIX
THREADS
PTHREADS
AND
ONE
FOR
SOLARIS
THREADS
IN
PREVIOUS
RELEASES
THE
BEHAVIOR
OF
FORK
DIFFERED
BETWEEN
THE
TWO
THREAD
LIBRARIES
FOR
POSIX
THREADS
FORK
CREATED
A
PROCESS
CONTAINING
ONLY
THE
CALLING
THREAD
BUT
FOR
SOLARIS
THREADS
FORK
CREATED
A
PROCESS
CONTAINING
COPIES
OF
ALL
THREADS
FROM
THE
PROCESS
OF
THE
CALLING
THREAD
IN
SOLARIS
THIS
BEHAVIOR
HAS
CHANGED
FORK
CREATES
A
CHILD
CONTAINING
A
COPY
OF
THE
CALLING
THREAD
ONLY
REGARDLESS
OF
WHICH
THREAD
LIBRARY
IS
USED
SOLARIS
ALSO
PROVIDES
THE
FUNCTION
WHICH
CAN
BE
USED
TO
CREATE
A
PROCESS
THAT
DUPLICATES
ONLY
THE
CALLING
THREAD
AND
THE
FORKALL
FUNCTION
WHICH
CAN
BE
USED
TO
CREATE
A
PROCESS
THAT
DUPLICATES
ALL
THE
THREADS
IN
THE
PROCESS
THREADS
ARE
DISCUSSED
IN
DETAIL
IN
CHAPTERS
AND
IN
GENERAL
WE
NEVER
KNOW
WHETHER
THE
CHILD
STARTS
EXECUTING
BEFORE
THE
PARENT
OR
VICE
VERSA
THE
ORDER
DEPENDS
ON
THE
SCHEDULING
ALGORITHM
USED
BY
THE
KERNEL
IF
IT
REQUIRED
THAT
THE
CHILD
AND
PARENT
SYNCHRONIZE
THEIR
ACTIONS
SOME
FORM
OF
INTERPROCESS
COMMUNICATION
IS
REQUIRED
IN
THE
PROGRAM
SHOWN
IN
FIGURE
WE
SIMPLY
HAVE
THE
PARENT
PUT
ITSELF
TO
SLEEP
FOR
SECONDS
TO
LET
THE
CHILD
EXECUTE
THERE
IS
NO
GUARANTEE
THAT
THE
LENGTH
OF
THIS
DELAY
IS
ADEQUATE
AND
WE
TALK
ABOUT
THIS
AND
OTHER
TYPES
OF
SYNCHRONIZATION
IN
SECTION
WHEN
WE
DISCUSS
RACE
CONDITIONS
IN
SECTION
WE
SHOW
HOW
TO
USE
SIGNALS
TO
SYNCHRONIZE
A
PARENT
AND
A
CHILD
AFTER
A
FORK
WHEN
WE
WRITE
TO
STANDARD
OUTPUT
WE
SUBTRACT
FROM
THE
SIZE
OF
BUF
TO
AVOID
WRITING
THE
TERMINATING
NULL
BYTE
ALTHOUGH
STRLEN
WILL
CALCULATE
THE
LENGTH
OF
A
STRING
NOT
INCLUDING
THE
TERMINATING
NULL
BYTE
SIZEOF
CALCULATES
THE
SIZE
OF
THE
BUFFER
WHICH
DOES
INCLUDE
THE
TERMINATING
NULL
BYTE
ANOTHER
DIFFERENCE
IS
THAT
USING
STRLEN
REQUIRES
A
FUNCTION
CALL
WHEREAS
SIZEOF
CALCULATES
THE
BUFFER
LENGTH
AT
COMPILE
TIME
AS
THE
BUFFER
IS
INITIALIZED
WITH
A
KNOWN
STRING
AND
ITS
SIZE
IS
FIXED
NOTE
THE
INTERACTION
OF
FORK
WITH
THE
I
O
FUNCTIONS
IN
THE
PROGRAM
IN
FIGURE
RECALL
FROM
CHAPTER
THAT
THE
WRITE
FUNCTION
IS
NOT
BUFFERED
BECAUSE
WRITE
IS
CALLED
BEFORE
THE
FORK
ITS
DATA
IS
WRITTEN
ONCE
TO
STANDARD
OUTPUT
THE
STANDARD
I
O
LIBRARY
HOWEVER
IS
BUFFERED
RECALL
FROM
SECTION
THAT
STANDARD
OUTPUT
IS
LINE
BUFFERED
IF
IT
CONNECTED
TO
A
TERMINAL
DEVICE
OTHERWISE
IT
FULLY
BUFFERED
WHEN
WE
RUN
THE
PROGRAM
INTERACTIVELY
WE
GET
ONLY
A
SINGLE
COPY
OF
THE
FIRST
PRINTF
LINE
BECAUSE
THE
STANDARD
OUTPUT
BUFFER
IS
FLUSHED
BY
THE
NEWLINE
WHEN
WE
REDIRECT
STANDARD
OUTPUT
TO
A
FILE
HOWEVER
WE
GET
TWO
COPIES
OF
THE
PRINTF
LINE
IN
THIS
SECOND
CASE
THE
PRINTF
BEFORE
THE
FORK
IS
CALLED
ONCE
BUT
THE
LINE
REMAINS
IN
THE
BUFFER
WHEN
FORK
IS
CALLED
THIS
BUFFER
IS
THEN
COPIED
INTO
THE
CHILD
WHEN
THE
PARENT
DATA
SPACE
IS
COPIED
TO
THE
CHILD
BOTH
THE
PARENT
AND
THE
CHILD
NOW
HAVE
A
STANDARD
I
O
BUFFER
WITH
THIS
LINE
IN
IT
THE
SECOND
PRINTF
RIGHT
BEFORE
THE
EXIT
JUST
APPENDS
ITS
DATA
TO
THE
EXISTING
BUFFER
WHEN
EACH
PROCESS
TERMINATES
ITS
COPY
OF
THE
BUFFER
IS
FINALLY
FLUSHED
FILE
SHARING
WHEN
WE
REDIRECT
THE
STANDARD
OUTPUT
OF
THE
PARENT
FROM
THE
PROGRAM
IN
FIGURE
THE
CHILD
STANDARD
OUTPUT
IS
ALSO
REDIRECTED
INDEED
ONE
CHARACTERISTIC
OF
FORK
IS
THAT
ALL
FILE
DESCRIPTORS
THAT
ARE
OPEN
IN
THE
PARENT
ARE
DUPLICATED
IN
THE
CHILD
WE
SAY
DUPLICATED
BECAUSE
IT
AS
IF
THE
DUP
FUNCTION
HAD
BEEN
CALLED
FOR
EACH
DESCRIPTOR
THE
PARENT
AND
THE
CHILD
SHARE
A
FILE
TABLE
ENTRY
FOR
EVERY
OPEN
DESCRIPTOR
RECALL
FIGURE
CONSIDER
A
PROCESS
THAT
HAS
THREE
DIFFERENT
FILES
OPENED
FOR
STANDARD
INPUT
STANDARD
OUTPUT
AND
STANDARD
ERROR
ON
RETURN
FROM
FORK
WE
HAVE
THE
ARRANGEMENT
SHOWN
IN
FIGURE
IT
IS
IMPORTANT
THAT
THE
PARENT
AND
THE
CHILD
SHARE
THE
SAME
FILE
OFFSET
CONSIDER
A
PROCESS
THAT
FORKS
A
CHILD
THEN
WAITS
FOR
THE
CHILD
TO
COMPLETE
ASSUME
THAT
BOTH
PROCESSES
WRITE
TO
STANDARD
OUTPUT
AS
PART
OF
THEIR
NORMAL
PROCESSING
IF
THE
PARENT
HAS
ITS
STANDARD
OUTPUT
REDIRECTED
BY
A
SHELL
PERHAPS
IT
IS
ESSENTIAL
THAT
THE
PARENT
FILE
OFFSET
BE
UPDATED
BY
THE
CHILD
WHEN
THE
CHILD
WRITES
TO
STANDARD
OUTPUT
IN
THIS
CASE
THE
CHILD
CAN
WRITE
TO
STANDARD
OUTPUT
WHILE
THE
PARENT
IS
WAITING
FOR
IT
ON
COMPLETION
OF
THE
CHILD
THE
PARENT
CAN
CONTINUE
WRITING
TO
STANDARD
OUTPUT
KNOWING
THAT
ITS
OUTPUT
WILL
BE
APPENDED
TO
WHATEVER
THE
CHILD
WROTE
IF
THE
PARENT
AND
THE
CHILD
DID
NOT
SHARE
THE
SAME
FILE
OFFSET
THIS
TYPE
OF
INTERACTION
WOULD
BE
MORE
DIFFICULT
TO
ACCOMPLISH
AND
WOULD
REQUIRE
EXPLICIT
ACTIONS
BY
THE
PARENT
PARENT
PROCESS
TABLE
ENTRY
FILE
TABLE
V
NODE
TABLE
FIGURE
SHARING
OF
OPEN
FILES
BETWEEN
PARENT
AND
CHILD
AFTER
FORK
IF
BOTH
PARENT
AND
CHILD
WRITE
TO
THE
SAME
DESCRIPTOR
WITHOUT
ANY
FORM
OF
SYNCHRONIZATION
SUCH
AS
HAVING
THE
PARENT
WAIT
FOR
THE
CHILD
THEIR
OUTPUT
WILL
BE
INTERMIXED
ASSUMING
IT
A
DESCRIPTOR
THAT
WAS
OPEN
BEFORE
THE
FORK
ALTHOUGH
THIS
IS
POSSIBLE
WE
SAW
IT
IN
FIGURE
IT
NOT
THE
NORMAL
MODE
OF
OPERATION
THERE
ARE
TWO
NORMAL
CASES
FOR
HANDLING
THE
DESCRIPTORS
AFTER
A
FORK
THE
PARENT
WAITS
FOR
THE
CHILD
TO
COMPLETE
IN
THIS
CASE
THE
PARENT
DOES
NOT
NEED
TO
DO
ANYTHING
WITH
ITS
DESCRIPTORS
WHEN
THE
CHILD
TERMINATES
ANY
OF
THE
SHARED
DESCRIPTORS
THAT
THE
CHILD
READ
FROM
OR
WROTE
TO
WILL
HAVE
THEIR
FILE
OFFSETS
UPDATED
ACCORDINGLY
BOTH
THE
PARENT
AND
THE
CHILD
GO
THEIR
OWN
WAYS
HERE
AFTER
THE
FORK
THE
PARENT
CLOSES
THE
DESCRIPTORS
THAT
IT
DOESN
T
NEED
AND
THE
CHILD
DOES
THE
SAME
THING
THIS
WAY
NEITHER
INTERFERES
WITH
THE
OTHER
OPEN
DESCRIPTORS
THIS
SCENARIO
IS
OFTEN
FOUND
WITH
NETWORK
SERVERS
BESIDES
THE
OPEN
FILES
NUMEROUS
OTHER
PROPERTIES
OF
THE
PARENT
ARE
INHERITED
BY
THE
CHILD
REAL
USER
ID
REAL
GROUP
ID
EFFECTIVE
USER
ID
AND
EFFECTIVE
GROUP
ID
SUPPLEMENTARY
GROUP
IDS
PROCESS
GROUP
ID
SESSION
ID
CONTROLLING
TERMINAL
THE
SET
USER
ID
AND
SET
GROUP
ID
FLAGS
CURRENT
WORKING
DIRECTORY
ROOT
DIRECTORY
FILE
MODE
CREATION
MASK
SIGNAL
MASK
AND
DISPOSITIONS
THE
CLOSE
ON
EXEC
FLAG
FOR
ANY
OPEN
FILE
DESCRIPTORS
ENVIRONMENT
ATTACHED
SHARED
MEMORY
SEGMENTS
MEMORY
MAPPINGS
RESOURCE
LIMITS
THE
DIFFERENCES
BETWEEN
THE
PARENT
AND
CHILD
ARE
THE
RETURN
VALUES
FROM
FORK
ARE
DIFFERENT
THE
PROCESS
IDS
ARE
DIFFERENT
THE
TWO
PROCESSES
HAVE
DIFFERENT
PARENT
PROCESS
IDS
THE
PARENT
PROCESS
ID
OF
THE
CHILD
IS
THE
PARENT
THE
PARENT
PROCESS
ID
OF
THE
PARENT
DOESN
T
CHANGE
THE
CHILD
AND
VALUES
ARE
SET
TO
THESE
TIMES
ARE
DISCUSSED
IN
SECTION
FILE
LOCKS
SET
BY
THE
PARENT
ARE
NOT
INHERITED
BY
THE
CHILD
PENDING
ALARMS
ARE
CLEARED
FOR
THE
CHILD
THE
SET
OF
PENDING
SIGNALS
FOR
THE
CHILD
IS
SET
TO
THE
EMPTY
SET
MANY
OF
THESE
FEATURES
HAVEN
T
BEEN
DISCUSSED
YET
WE
LL
COVER
THEM
IN
LATER
CHAPTERS
THE
TWO
MAIN
REASONS
FOR
FORK
TO
FAIL
ARE
A
IF
TOO
MANY
PROCESSES
ARE
ALREADY
IN
THE
SYSTEM
WHICH
USUALLY
MEANS
THAT
SOMETHING
ELSE
IS
WRONG
OR
B
IF
THE
TOTAL
NUMBER
OF
PROCESSES
FOR
THIS
REAL
USER
ID
EXCEEDS
THE
SYSTEM
LIMIT
RECALL
FROM
FIGURE
THAT
SPECIFIES
THE
MAXIMUM
NUMBER
OF
SIMULTANEOUS
PROCESSES
PER
REAL
USER
ID
THERE
ARE
TWO
USES
FOR
FORK
WHEN
A
PROCESS
WANTS
TO
DUPLICATE
ITSELF
SO
THAT
THE
PARENT
AND
THE
CHILD
CAN
EACH
EXECUTE
DIFFERENT
SECTIONS
OF
CODE
AT
THE
SAME
TIME
THIS
IS
COMMON
FOR
NETWORK
SERVERS
THE
PARENT
WAITS
FOR
A
SERVICE
REQUEST
FROM
A
CLIENT
WHEN
THE
REQUEST
ARRIVES
THE
PARENT
CALLS
FORK
AND
LETS
THE
CHILD
HANDLE
THE
REQUEST
THE
PARENT
GOES
BACK
TO
WAITING
FOR
THE
NEXT
SERVICE
REQUEST
TO
ARRIVE
WHEN
A
PROCESS
WANTS
TO
EXECUTE
A
DIFFERENT
PROGRAM
THIS
IS
COMMON
FOR
SHELLS
IN
THIS
CASE
THE
CHILD
DOES
AN
EXEC
WHICH
WE
DESCRIBE
IN
SECTION
RIGHT
AFTER
IT
RETURNS
FROM
THE
FORK
SOME
OPERATING
SYSTEMS
COMBINE
THE
OPERATIONS
FROM
STEP
A
FORK
FOLLOWED
BY
AN
EXEC
INTO
A
SINGLE
OPERATION
CALLED
A
SPAWN
THE
UNIX
SYSTEM
SEPARATES
THE
TWO
AS
THERE
ARE
NUMEROUS
CASES
WHERE
IT
IS
USEFUL
TO
FORK
WITHOUT
DOING
AN
EXEC
ALSO
SEPARATING
THE
TWO
OPERATIONS
ALLOWS
THE
CHILD
TO
CHANGE
THE
PER
PROCESS
ATTRIBUTES
BETWEEN
THE
FORK
AND
THE
EXEC
SUCH
AS
I
O
REDIRECTION
USER
ID
SIGNAL
DISPOSITION
AND
SO
ON
WE
LL
SEE
NUMEROUS
EXAMPLES
OF
THIS
IN
CHAPTER
THE
SINGLE
UNIX
SPECIFICATION
DOES
INCLUDE
SPAWN
INTERFACES
IN
THE
ADVANCED
REAL
TIME
OPTION
GROUP
THESE
INTERFACES
ARE
NOT
INTENDED
TO
BE
REPLACEMENTS
FOR
FORK
AND
EXEC
HOWEVER
THEY
ARE
INTENDED
TO
SUPPORT
SYSTEMS
THAT
HAVE
DIFFICULTY
IMPLEMENTING
FORK
EFFICIENTLY
ESPECIALLY
SYSTEMS
WITHOUT
HARDWARE
SUPPORT
FOR
MEMORY
MANAGEMENT
VFORK
FUNCTION
THE
FUNCTION
VFORK
HAS
THE
SAME
CALLING
SEQUENCE
AND
SAME
RETURN
VALUES
AS
FORK
BUT
THE
SEMANTICS
OF
THE
TWO
FUNCTIONS
DIFFER
THE
VFORK
FUNCTION
ORIGINATED
WITH
SOME
CONSIDER
THE
FUNCTION
A
BLEMISH
BUT
ALL
THE
PLATFORMS
COVERED
IN
THIS
BOOK
SUPPORT
IT
IN
FACT
THE
BSD
DEVELOPERS
REMOVED
IT
FROM
THE
RELEASE
BUT
ALL
THE
OPEN
SOURCE
BSD
DISTRIBUTIONS
THAT
DERIVE
FROM
ADDED
SUPPORT
FOR
IT
BACK
INTO
THEIR
OWN
RELEASES
THE
VFORK
FUNCTION
WAS
MARKED
AS
AN
OBSOLESCENT
INTERFACE
IN
VERSION
OF
THE
SINGLE
UNIX
SPECIFICATION
AND
WAS
REMOVED
ENTIRELY
IN
VERSION
WE
INCLUDE
IT
HERE
FOR
HISTORICAL
REASONS
ONLY
PORTABLE
APPLICATIONS
SHOULD
NOT
USE
IT
THE
VFORK
FUNCTION
WAS
INTENDED
TO
CREATE
A
NEW
PROCESS
FOR
THE
PURPOSE
OF
EXECUTING
A
NEW
PROGRAM
STEP
AT
THE
END
OF
THE
PREVIOUS
SECTION
SIMILAR
TO
THE
METHOD
USED
BY
THE
BARE
BONES
SHELL
FROM
FIGURE
THE
VFORK
FUNCTION
CREATES
THE
NEW
PROCESS
JUST
LIKE
FORK
WITHOUT
COPYING
THE
ADDRESS
SPACE
OF
THE
PARENT
INTO
THE
CHILD
AS
THE
CHILD
WON
T
REFERENCE
THAT
ADDRESS
SPACE
THE
CHILD
SIMPLY
CALLS
EXEC
OR
EXIT
RIGHT
AFTER
THE
VFORK
INSTEAD
THE
CHILD
RUNS
IN
THE
ADDRESS
SPACE
OF
THE
PARENT
UNTIL
IT
CALLS
EITHER
EXEC
OR
EXIT
THIS
OPTIMIZATION
IS
MORE
EFFICIENT
ON
SOME
IMPLEMENTATIONS
OF
THE
UNIX
SYSTEM
BUT
LEADS
TO
UNDEFINED
RESULTS
IF
THE
CHILD
MODIFIES
ANY
DATA
EXCEPT
THE
VARIABLE
USED
TO
HOLD
THE
RETURN
VALUE
FROM
VFORK
MAKES
FUNCTION
CALLS
OR
RETURNS
WITHOUT
CALLING
EXEC
OR
EXIT
AS
WE
MENTIONED
IN
THE
PREVIOUS
SECTION
IMPLEMENTATIONS
USE
COPY
ON
WRITE
TO
IMPROVE
THE
EFFICIENCY
OF
A
FORK
FOLLOWED
BY
AN
EXEC
BUT
NO
COPYING
IS
STILL
FASTER
THAN
SOME
COPYING
ANOTHER
DIFFERENCE
BETWEEN
THE
TWO
FUNCTIONS
IS
THAT
VFORK
GUARANTEES
THAT
THE
CHILD
RUNS
FIRST
UNTIL
THE
CHILD
CALLS
EXEC
OR
EXIT
WHEN
THE
CHILD
CALLS
EITHER
OF
THESE
FUNCTIONS
THE
PARENT
RESUMES
THIS
CAN
LEAD
TO
DEADLOCK
IF
THE
CHILD
DEPENDS
ON
FURTHER
ACTIONS
OF
THE
PARENT
BEFORE
CALLING
EITHER
OF
THESE
TWO
FUNCTIONS
EXAMPLE
THE
PROGRAM
IN
FIGURE
IS
A
MODIFIED
VERSION
OF
THE
PROGRAM
FROM
FIGURE
WE
VE
REPLACED
THE
CALL
TO
FORK
WITH
VFORK
AND
REMOVED
THE
WRITE
TO
STANDARD
OUTPUT
ALSO
WE
DON
T
NEED
TO
HAVE
THE
PARENT
CALL
SLEEP
AS
WE
RE
GUARANTEED
THAT
IT
IS
PUT
TO
SLEEP
BY
THE
KERNEL
UNTIL
THE
CHILD
CALLS
EITHER
EXEC
OR
EXIT
PID
GLOB
VAR
HERE
THE
INCREMENTING
OF
THE
VARIABLES
DONE
BY
THE
CHILD
CHANGES
THE
VALUES
IN
THE
PARENT
BECAUSE
THE
CHILD
RUNS
IN
THE
ADDRESS
SPACE
OF
THE
PARENT
THIS
DOESN
T
SURPRISE
US
THIS
BEHAVIOR
HOWEVER
DIFFERS
FROM
THE
BEHAVIOR
OF
FORK
NOTE
IN
FIGURE
THAT
WE
CALL
INSTEAD
OF
EXIT
AS
WE
DESCRIBED
IN
SECTION
DOES
NOT
PERFORM
ANY
FLUSHING
OF
STANDARD
I
O
BUFFERS
IF
WE
CALL
EXIT
INSTEAD
THE
RESULTS
ARE
INDETERMINATE
DEPENDING
ON
THE
IMPLEMENTATION
OF
THE
STANDARD
I
O
LIBRARY
WE
MIGHT
SEE
NO
DIFFERENCE
IN
THE
OUTPUT
OR
WE
MIGHT
FIND
THAT
THE
OUTPUT
FROM
THE
FIRST
PRINTF
IN
THE
PARENT
HAS
DISAPPEARED
IF
THE
CHILD
CALLS
EXIT
THE
IMPLEMENTATION
FLUSHES
THE
STANDARD
I
O
STREAMS
IF
THIS
IS
THE
ONLY
ACTION
TAKEN
BY
THE
LIBRARY
THEN
WE
WILL
SEE
NO
DIFFERENCE
FROM
THE
OUTPUT
GENERATED
IF
THE
CHILD
CALLED
IF
THE
IMPLEMENTATION
ALSO
CLOSES
THE
STANDARD
I
O
STREAMS
HOWEVER
THE
MEMORY
REPRESENTING
THE
FILE
OBJECT
FOR
THE
STANDARD
OUTPUT
WILL
BE
CLEARED
OUT
BECAUSE
THE
CHILD
IS
BORROWING
THE
PARENT
ADDRESS
SPACE
WHEN
THE
PARENT
RESUMES
AND
CALLS
PRINTF
NO
OUTPUT
WILL
APPEAR
AND
PRINTF
WILL
RETURN
NOTE
THAT
THE
PARENT
IS
STILL
VALID
AS
THE
CHILD
GETS
A
COPY
OF
THE
PARENT
FILE
DESCRIPTOR
ARRAY
REFER
BACK
TO
FIGURE
MOST
MODERN
IMPLEMENTATIONS
OF
EXIT
DO
NOT
BOTHER
TO
CLOSE
THE
STREAMS
BECAUSE
THE
PROCESS
IS
ABOUT
TO
EXIT
THE
KERNEL
WILL
CLOSE
ALL
THE
FILE
DESCRIPTORS
OPEN
IN
THE
PROCESS
CLOSING
THEM
IN
THE
LIBRARY
SIMPLY
ADDS
OVERHEAD
WITHOUT
ANY
BENEFIT
SECTION
OF
MCKUSICK
ET
AL
CONTAINS
ADDITIONAL
INFORMATION
ON
THE
IMPLEMENTATION
ISSUES
OF
FORK
AND
VFORK
EXERCISES
AND
CONTINUE
THE
DISCUSSION
OF
VFORK
EXIT
FUNCTIONS
AS
WE
DESCRIBED
IN
SECTION
A
PROCESS
CAN
TERMINATE
NORMALLY
IN
FIVE
WAYS
EXECUTING
A
RETURN
FROM
THE
MAIN
FUNCTION
AS
WE
SAW
IN
SECTION
THIS
IS
EQUIVALENT
TO
CALLING
EXIT
CALLING
THE
EXIT
FUNCTION
THIS
FUNCTION
IS
DEFINED
BY
ISO
C
AND
INCLUDES
THE
CALLING
OF
ALL
EXIT
HANDLERS
THAT
HAVE
BEEN
REGISTERED
BY
CALLING
ATEXIT
AND
CLOSING
ALL
STANDARD
I
O
STREAMS
BECAUSE
ISO
C
DOES
NOT
DEAL
WITH
FILE
DESCRIPTORS
MULTIPLE
PROCESSES
PARENTS
AND
CHILDREN
AND
JOB
CONTROL
THE
DEFINITION
OF
THIS
FUNCTION
IS
INCOMPLETE
FOR
A
UNIX
SYSTEM
CALLING
THE
OR
FUNCTION
ISO
C
DEFINES
TO
PROVIDE
A
WAY
FOR
A
PROCESS
TO
TERMINATE
WITHOUT
RUNNING
EXIT
HANDLERS
OR
SIGNAL
HANDLERS
WHETHER
STANDARD
I
O
STREAMS
ARE
FLUSHED
DEPENDS
ON
THE
IMPLEMENTATION
ON
UNIX
SYSTEMS
AND
ARE
SYNONYMOUS
AND
DO
NOT
FLUSH
STANDARD
I
O
STREAMS
THE
FUNCTION
IS
CALLED
BY
EXIT
AND
HANDLES
THE
UNIX
SYSTEM
SPECIFIC
DETAILS
IS
SPECIFIED
BY
POSIX
IN
MOST
UNIX
SYSTEM
IMPLEMENTATIONS
EXIT
IS
A
FUNCTION
IN
THE
STANDARD
C
LIBRARY
WHEREAS
IS
A
SYSTEM
CALL
EXECUTING
A
RETURN
FROM
THE
START
ROUTINE
OF
THE
LAST
THREAD
IN
THE
PROCESS
THE
RETURN
VALUE
OF
THE
THREAD
IS
NOT
USED
AS
THE
RETURN
VALUE
OF
THE
PROCESS
HOWEVER
WHEN
THE
LAST
THREAD
RETURNS
FROM
ITS
START
ROUTINE
THE
PROCESS
EXITS
WITH
A
TERMINATION
STATUS
OF
CALLING
THE
FUNCTION
FROM
THE
LAST
THREAD
IN
THE
PROCESS
AS
WITH
THE
PREVIOUS
CASE
THE
EXIT
STATUS
OF
THE
PROCESS
IN
THIS
SITUATION
IS
ALWAYS
REGARDLESS
OF
THE
ARGUMENT
PASSED
TO
WE
LL
SAY
MORE
ABOUT
IN
SECTION
THE
THREE
FORMS
OF
ABNORMAL
TERMINATION
ARE
AS
FOLLOWS
CALLING
ABORT
THIS
IS
A
SPECIAL
CASE
OF
THE
NEXT
ITEM
AS
IT
GENERATES
THE
SIGABRT
SIGNAL
WHEN
THE
PROCESS
RECEIVES
CERTAIN
SIGNALS
WE
DESCRIBE
SIGNALS
IN
MORE
DETAIL
IN
CHAPTER
THE
SIGNAL
CAN
BE
GENERATED
BY
THE
PROCESS
ITSELF
E
G
BY
CALLING
THE
ABORT
FUNCTION
BY
SOME
OTHER
PROCESS
OR
BY
THE
KERNEL
EXAMPLES
OF
SIGNALS
GENERATED
BY
THE
KERNEL
INCLUDE
THE
PROCESS
REFERENCING
A
MEMORY
LOCATION
NOT
WITHIN
ITS
ADDRESS
SPACE
OR
TRYING
TO
DIVIDE
BY
THE
LAST
THREAD
RESPONDS
TO
A
CANCELLATION
REQUEST
BY
DEFAULT
CANCELLATION
OCCURS
IN
A
DEFERRED
MANNER
ONE
THREAD
REQUESTS
THAT
ANOTHER
BE
CANCELED
AND
SOMETIME
LATER
THE
TARGET
THREAD
TERMINATES
WE
DISCUSS
CANCELLATION
REQUESTS
IN
DETAIL
IN
SECTIONS
AND
REGARDLESS
OF
HOW
A
PROCESS
TERMINATES
THE
SAME
CODE
IN
THE
KERNEL
IS
EVENTUALLY
EXECUTED
THIS
KERNEL
CODE
CLOSES
ALL
THE
OPEN
DESCRIPTORS
FOR
THE
PROCESS
RELEASES
THE
MEMORY
THAT
IT
WAS
USING
AND
SO
ON
FOR
ANY
OF
THE
PRECEDING
CASES
WE
WANT
THE
TERMINATING
PROCESS
TO
BE
ABLE
TO
NOTIFY
ITS
PARENT
HOW
IT
TERMINATED
FOR
THE
THREE
EXIT
FUNCTIONS
EXIT
AND
THIS
IS
DONE
BY
PASSING
AN
EXIT
STATUS
AS
THE
ARGUMENT
TO
THE
FUNCTION
IN
THE
CASE
OF
AN
ABNORMAL
TERMINATION
HOWEVER
THE
KERNEL
NOT
THE
PROCESS
GENERATES
A
TERMINATION
STATUS
TO
INDICATE
THE
REASON
FOR
THE
ABNORMAL
TERMINATION
IN
ANY
CASE
THE
PARENT
OF
THE
PROCESS
CAN
OBTAIN
THE
TERMINATION
STATUS
FROM
EITHER
THE
WAIT
OR
THE
WAITPID
FUNCTION
DESCRIBED
IN
THE
NEXT
SECTION
NOTE
THAT
WE
DIFFERENTIATE
BETWEEN
THE
EXIT
STATUS
WHICH
IS
THE
ARGUMENT
TO
ONE
OF
THE
THREE
EXIT
FUNCTIONS
OR
THE
RETURN
VALUE
FROM
MAIN
AND
THE
TERMINATION
STATUS
THE
EXIT
STATUS
IS
CONVERTED
INTO
A
TERMINATION
STATUS
BY
THE
KERNEL
WHEN
IS
FINALLY
CALLED
RECALL
FIGURE
FIGURE
DESCRIBES
THE
VARIOUS
WAYS
THE
PARENT
CAN
EXAMINE
THE
TERMINATION
STATUS
OF
A
CHILD
IF
THE
CHILD
TERMINATED
NORMALLY
THE
PARENT
CAN
OBTAIN
THE
EXIT
STATUS
OF
THE
CHILD
WHEN
WE
DESCRIBED
THE
FORK
FUNCTION
IT
WAS
OBVIOUS
THAT
THE
CHILD
HAS
A
PARENT
PROCESS
AFTER
THE
CALL
TO
FORK
NOW
WE
RE
TALKING
ABOUT
RETURNING
A
TERMINATION
STATUS
TO
THE
PARENT
BUT
WHAT
HAPPENS
IF
THE
PARENT
TERMINATES
BEFORE
THE
CHILD
THE
ANSWER
IS
THAT
THE
INIT
PROCESS
BECOMES
THE
PARENT
PROCESS
OF
ANY
PROCESS
WHOSE
PARENT
TERMINATES
IN
SUCH
A
CASE
WE
SAY
THAT
THE
PROCESS
HAS
BEEN
INHERITED
BY
INIT
WHAT
NORMALLY
HAPPENS
IS
THAT
WHENEVER
A
PROCESS
TERMINATES
THE
KERNEL
GOES
THROUGH
ALL
ACTIVE
PROCESSES
TO
SEE
WHETHER
THE
TERMINATING
PROCESS
IS
THE
PARENT
OF
ANY
PROCESS
THAT
STILL
EXISTS
IF
SO
THE
PARENT
PROCESS
ID
OF
THE
SURVIVING
PROCESS
IS
CHANGED
TO
BE
THE
PROCESS
ID
OF
INIT
THIS
WAY
WE
RE
GUARANTEED
THAT
EVERY
PROCESS
HAS
A
PARENT
ANOTHER
CONDITION
WE
HAVE
TO
WORRY
ABOUT
IS
WHEN
A
CHILD
TERMINATES
BEFORE
ITS
PARENT
IF
THE
CHILD
COMPLETELY
DISAPPEARED
THE
PARENT
WOULDN
T
BE
ABLE
TO
FETCH
ITS
TERMINATION
STATUS
WHEN
AND
IF
THE
PARENT
WAS
FINALLY
READY
TO
CHECK
IF
THE
CHILD
HAD
TERMINATED
THE
KERNEL
KEEPS
A
SMALL
AMOUNT
OF
INFORMATION
FOR
EVERY
TERMINATING
PROCESS
SO
THAT
THE
INFORMATION
IS
AVAILABLE
WHEN
THE
PARENT
OF
THE
TERMINATING
PROCESS
CALLS
WAIT
OR
WAITPID
MINIMALLY
THIS
INFORMATION
CONSISTS
OF
THE
PROCESS
ID
THE
TERMINATION
STATUS
OF
THE
PROCESS
AND
THE
AMOUNT
OF
CPU
TIME
TAKEN
BY
THE
PROCESS
THE
KERNEL
CAN
DISCARD
ALL
THE
MEMORY
USED
BY
THE
PROCESS
AND
CLOSE
ITS
OPEN
FILES
IN
UNIX
SYSTEM
TERMINOLOGY
A
PROCESS
THAT
HAS
TERMINATED
BUT
WHOSE
PARENT
HAS
NOT
YET
WAITED
FOR
IT
IS
CALLED
A
ZOMBIE
THE
PS
COMMAND
PRINTS
THE
STATE
OF
A
ZOMBIE
PROCESS
AS
Z
IF
WE
WRITE
A
LONG
RUNNING
PROGRAM
THAT
FORKS
MANY
CHILD
PROCESSES
THEY
BECOME
ZOMBIES
UNLESS
WE
WAIT
FOR
THEM
AND
FETCH
THEIR
TERMINATION
STATUS
SOME
SYSTEMS
PROVIDE
WAYS
TO
PREVENT
THE
CREATION
OF
ZOMBIES
AS
WE
DESCRIBE
IN
SECTION
THE
FINAL
CONDITION
TO
CONSIDER
IS
THIS
WHAT
HAPPENS
WHEN
A
PROCESS
THAT
HAS
BEEN
INHERITED
BY
INIT
TERMINATES
DOES
IT
BECOME
A
ZOMBIE
THE
ANSWER
IS
NO
BECAUSE
INIT
IS
WRITTEN
SO
THAT
WHENEVER
ONE
OF
ITS
CHILDREN
TERMINATES
INIT
CALLS
ONE
OF
THE
WAIT
FUNCTIONS
TO
FETCH
THE
TERMINATION
STATUS
BY
DOING
THIS
INIT
PREVENTS
THE
SYSTEM
FROM
BEING
CLOGGED
BY
ZOMBIES
WHEN
WE
SAY
ONE
OF
INIT
CHILDREN
WE
MEAN
EITHER
A
PROCESS
THAT
INIT
GENERATES
DIRECTLY
SUCH
AS
GETTY
WHICH
WE
DESCRIBE
IN
SECTION
OR
A
PROCESS
WHOSE
PARENT
HAS
TERMINATED
AND
HAS
BEEN
SUBSEQUENTLY
INHERITED
BY
INIT
WAIT
AND
WAITPID
FUNCTIONS
WHEN
A
PROCESS
TERMINATES
EITHER
NORMALLY
OR
ABNORMALLY
THE
KERNEL
NOTIFIES
THE
PARENT
BY
SENDING
THE
SIGCHLD
SIGNAL
TO
THE
PARENT
BECAUSE
THE
TERMINATION
OF
A
CHILD
IS
AN
ASYNCHRONOUS
EVENT
IT
CAN
HAPPEN
AT
ANY
TIME
WHILE
THE
PARENT
IS
RUNNING
THIS
SIGNAL
IS
THE
ASYNCHRONOUS
NOTIFICATION
FROM
THE
KERNEL
TO
THE
PARENT
THE
PARENT
CAN
CHOOSE
TO
IGNORE
THIS
SIGNAL
OR
IT
CAN
PROVIDE
A
FUNCTION
THAT
IS
CALLED
WHEN
THE
SIGNAL
OCCURS
A
SIGNAL
HANDLER
THE
DEFAULT
ACTION
FOR
THIS
SIGNAL
IS
TO
BE
IGNORED
WE
DESCRIBE
THESE
OPTIONS
IN
CHAPTER
FOR
NOW
WE
NEED
TO
BE
AWARE
THAT
A
PROCESS
THAT
CALLS
WAIT
OR
WAITPID
CAN
BLOCK
IF
ALL
OF
ITS
CHILDREN
ARE
STILL
RUNNING
RETURN
IMMEDIATELY
WITH
THE
TERMINATION
STATUS
OF
A
CHILD
IF
A
CHILD
HAS
TERMINATED
AND
IS
WAITING
FOR
ITS
TERMINATION
STATUS
TO
BE
FETCHED
RETURN
IMMEDIATELY
WITH
AN
ERROR
IF
IT
DOESN
T
HAVE
ANY
CHILD
PROCESSES
IF
THE
PROCESS
IS
CALLING
WAIT
BECAUSE
IT
RECEIVED
THE
SIGCHLD
SIGNAL
WE
EXPECT
WAIT
TO
RETURN
IMMEDIATELY
BUT
IF
WE
CALL
IT
AT
ANY
RANDOM
POINT
IN
TIME
IT
CAN
BLOCK
THE
DIFFERENCES
BETWEEN
THESE
TWO
FUNCTIONS
ARE
AS
FOLLOWS
THE
WAIT
FUNCTION
CAN
BLOCK
THE
CALLER
UNTIL
A
CHILD
PROCESS
TERMINATES
WHEREAS
WAITPID
HAS
AN
OPTION
THAT
PREVENTS
IT
FROM
BLOCKING
THE
WAITPID
FUNCTION
DOESN
T
WAIT
FOR
THE
CHILD
THAT
TERMINATES
FIRST
IT
HAS
A
NUMBER
OF
OPTIONS
THAT
CONTROL
WHICH
PROCESS
IT
WAITS
FOR
IF
A
CHILD
HAS
ALREADY
TERMINATED
AND
IS
A
ZOMBIE
WAIT
RETURNS
IMMEDIATELY
WITH
THAT
CHILD
STATUS
OTHERWISE
IT
BLOCKS
THE
CALLER
UNTIL
A
CHILD
TERMINATES
IF
THE
CALLER
BLOCKS
AND
HAS
MULTIPLE
CHILDREN
WAIT
RETURNS
WHEN
ONE
TERMINATES
WE
CAN
ALWAYS
TELL
WHICH
CHILD
TERMINATED
BECAUSE
THE
PROCESS
ID
IS
RETURNED
BY
THE
FUNCTION
FOR
BOTH
FUNCTIONS
THE
ARGUMENT
STATLOC
IS
A
POINTER
TO
AN
INTEGER
IF
THIS
ARGUMENT
IS
NOT
A
NULL
POINTER
THE
TERMINATION
STATUS
OF
THE
TERMINATED
PROCESS
IS
STORED
IN
THE
LOCATION
POINTED
TO
BY
THE
ARGUMENT
IF
WE
DON
T
CARE
ABOUT
THE
TERMINATION
STATUS
WE
SIMPLY
PASS
A
NULL
POINTER
AS
THIS
ARGUMENT
TRADITIONALLY
THE
INTEGER
STATUS
THAT
THESE
TWO
FUNCTIONS
RETURN
HAS
BEEN
DEFINED
BY
THE
IMPLEMENTATION
WITH
CERTAIN
BITS
INDICATING
THE
EXIT
STATUS
FOR
A
NORMAL
RETURN
OTHER
BITS
INDICATING
THE
SIGNAL
NUMBER
FOR
AN
ABNORMAL
RETURN
ONE
BIT
INDICATING
WHETHER
A
CORE
FILE
WAS
GENERATED
AND
SO
ON
POSIX
SPECIFIES
THAT
THE
TERMINATION
STATUS
IS
TO
BE
LOOKED
AT
USING
VARIOUS
MACROS
THAT
ARE
DEFINED
IN
SYS
WAIT
H
FOUR
MUTUALLY
EXCLUSIVE
MACROS
TELL
US
HOW
THE
PROCESS
TERMINATED
AND
THEY
ALL
BEGIN
WITH
WIF
BASED
ON
WHICH
OF
THESE
FOUR
MACROS
IS
TRUE
OTHER
MACROS
ARE
USED
TO
OBTAIN
THE
EXIT
STATUS
SIGNAL
NUMBER
AND
THE
LIKE
THE
FOUR
MUTUALLY
EXCLUSIVE
MACROS
ARE
SHOWN
IN
FIGURE
MACRO
DESCRIPTION
WIFEXITED
STATUS
TRUE
IF
STATUS
WAS
RETURNED
FOR
A
CHILD
THAT
TERMINATED
NORMALLY
IN
THIS
CASE
WE
CAN
EXECUTE
WEXITSTATUS
STATUS
TO
FETCH
THE
LOW
ORDER
BITS
OF
THE
ARGUMENT
THAT
THE
CHILD
PASSED
TO
EXIT
OR
WIFSIGNALED
STATUS
TRUE
IF
STATUS
WAS
RETURNED
FOR
A
CHILD
THAT
TERMINATED
ABNORMALLY
BY
RECEIPT
OF
A
SIGNAL
THAT
IT
DIDN
T
CATCH
IN
THIS
CASE
WE
CAN
EXECUTE
WTERMSIG
STATUS
TO
FETCH
THE
SIGNAL
NUMBER
THAT
CAUSED
THE
TERMINATION
ADDITIONALLY
SOME
IMPLEMENTATIONS
BUT
NOT
THE
SINGLE
UNIX
SPECIFICATION
DEFINE
THE
MACRO
WCOREDUMP
STATUS
THAT
RETURNS
TRUE
IF
A
CORE
FILE
OF
THE
TERMINATED
PROCESS
WAS
GENERATED
WIFSTOPPED
STATUS
TRUE
IF
STATUS
WAS
RETURNED
FOR
A
CHILD
THAT
IS
CURRENTLY
STOPPED
IN
THIS
CASE
WE
CAN
EXECUTE
WSTOPSIG
STATUS
TO
FETCH
THE
SIGNAL
NUMBER
THAT
CAUSED
THE
CHILD
TO
STOP
WIFCONTINUED
STATUS
TRUE
IF
STATUS
WAS
RETURNED
FOR
A
CHILD
THAT
HAS
BEEN
CONTINUED
AFTER
A
JOB
CONTROL
STOP
XSI
OPTION
WAITPID
ONLY
FIGURE
MACROS
TO
EXAMINE
THE
TERMINATION
STATUS
RETURNED
BY
WAIT
AND
WAITPID
WE
LL
DISCUSS
HOW
A
PROCESS
CAN
BE
STOPPED
IN
SECTION
WHEN
WE
DISCUSS
JOB
CONTROL
EXAMPLE
THE
FUNCTION
IN
FIGURE
USES
THE
MACROS
FROM
FIGURE
TO
PRINT
A
DESCRIPTION
OF
THE
TERMINATION
STATUS
WE
LL
CALL
THIS
FUNCTION
FROM
NUMEROUS
PROGRAMS
IN
THE
TEXT
NOTE
THAT
THIS
FUNCTION
HANDLES
THE
WCOREDUMP
MACRO
IF
IT
IS
DEFINED
FREEBSD
LINUX
MAC
OS
X
AND
SOLARIS
ALL
SUPPORT
THE
WCOREDUMP
MACRO
HOWEVER
SOME
PLATFORMS
HIDE
ITS
DEFINITION
IF
THE
CONSTANT
IS
DEFINED
RECALL
SECTION
THE
PROGRAM
SHOWN
IN
FIGURE
CALLS
THE
FUNCTION
DEMONSTRATING
THE
VARIOUS
VALUES
FOR
THE
TERMINATION
STATUS
IF
WE
RUN
THE
PROGRAM
IN
FIGURE
WE
GET
A
OUT
NORMAL
TERMINATION
EXIT
STATUS
ABNORMAL
TERMINATION
SIGNAL
NUMBER
CORE
FILE
GENERATED
ABNORMAL
TERMINATION
SIGNAL
NUMBER
CORE
FILE
GENERATED
FOR
NOW
WE
PRINT
THE
SIGNAL
NUMBER
FROM
WTERMSIG
WE
CAN
LOOK
AT
THE
SIGNAL
H
HEADER
TO
VERIFY
THAT
SIGABRT
HAS
A
VALUE
OF
AND
THAT
SIGFPE
HAS
A
VALUE
OF
WE
LL
SEE
A
PORTABLE
WAY
TO
MAP
A
SIGNAL
NUMBER
TO
A
DESCRIPTIVE
NAME
IN
SECTION
AS
WE
MENTIONED
IF
WE
HAVE
MORE
THAN
ONE
CHILD
WAIT
RETURNS
ON
TERMINATION
OF
ANY
OF
THE
CHILDREN
BUT
WHAT
IF
WE
WANT
TO
WAIT
FOR
A
SPECIFIC
PROCESS
TO
TERMINATE
ASSUMING
WE
KNOW
WHICH
PROCESS
ID
WE
WANT
TO
WAIT
FOR
IN
OLDER
VERSIONS
OF
THE
UNIX
SYSTEM
WE
WOULD
HAVE
TO
CALL
WAIT
AND
COMPARE
THE
RETURNED
PROCESS
ID
WITH
THE
ONE
WE
RE
INTERESTED
IN
IF
THE
TERMINATED
PROCESS
WASN
T
THE
ONE
WE
WANTED
WE
WOULD
HAVE
TO
SAVE
THE
PROCESS
ID
AND
TERMINATION
STATUS
AND
CALL
WAIT
AGAIN
WE
WOULD
NEED
TO
CONTINUE
DOING
THIS
UNTIL
THE
DESIRED
PROCESS
TERMINATED
THE
NEXT
TIME
WE
WANTED
TO
WAIT
FOR
A
SPECIFIC
PROCESS
WE
WOULD
GO
THROUGH
THE
LIST
OF
ALREADY
TERMINATED
PROCESSES
TO
SEE
WHETHER
WE
HAD
ALREADY
WAITED
FOR
IT
AND
IF
NOT
CALL
WAIT
INCLUDE
APUE
H
INCLUDE
SYS
WAIT
H
INT
MAIN
VOID
PID
INT
STATUS
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
CHILD
EXIT
IF
WAIT
STATUS
PID
WAIT
FOR
CHILD
WAIT
ERROR
STATUS
AND
PRINT
ITS
STATUS
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
CHILD
ABORT
GENERATES
SIGABRT
IF
WAIT
STATUS
PID
WAIT
FOR
CHILD
WAIT
ERROR
STATUS
AND
PRINT
ITS
STATUS
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
CHILD
STATUS
DIVIDE
BY
GENERATES
SIGFPE
IF
WAIT
STATUS
PID
WAIT
FOR
CHILD
WAIT
ERROR
STATUS
AND
PRINT
ITS
STATUS
EXIT
FIGURE
DEMONSTRATE
VARIOUS
EXIT
STATUSES
AGAIN
WHAT
WE
NEED
IS
A
FUNCTION
THAT
WAITS
FOR
A
SPECIFIC
PROCESS
THIS
FUNCTIONALITY
AND
MORE
IS
PROVIDED
BY
THE
POSIX
WAITPID
FUNCTION
THE
INTERPRETATION
OF
THE
PID
ARGUMENT
FOR
WAITPID
DEPENDS
ON
ITS
VALUE
PID
WAITS
FOR
ANY
CHILD
PROCESS
IN
THIS
RESPECT
WAITPID
IS
EQUIVALENT
TO
WAIT
PID
WAITS
FOR
THE
CHILD
WHOSE
PROCESS
ID
EQUALS
PID
PID
WAITS
FOR
ANY
CHILD
WHOSE
PROCESS
GROUP
ID
EQUALS
THAT
OF
THE
CALLING
PROCESS
WE
DISCUSS
PROCESS
GROUPS
IN
SECTION
PID
WAITS
FOR
ANY
CHILD
WHOSE
PROCESS
GROUP
ID
EQUALS
THE
ABSOLUTE
VALUE
OF
PID
THE
WAITPID
FUNCTION
RETURNS
THE
PROCESS
ID
OF
THE
CHILD
THAT
TERMINATED
AND
STORES
THE
CHILD
TERMINATION
STATUS
IN
THE
MEMORY
LOCATION
POINTED
TO
BY
STATLOC
WITH
WAIT
THE
ONLY
REAL
ERROR
IS
IF
THE
CALLING
PROCESS
HAS
NO
CHILDREN
ANOTHER
ERROR
RETURN
IS
POSSIBLE
IN
CASE
THE
FUNCTION
CALL
IS
INTERRUPTED
BY
A
SIGNAL
WE
LL
DISCUSS
THIS
IN
CHAPTER
WITH
WAITPID
HOWEVER
IT
ALSO
POSSIBLE
TO
GET
AN
ERROR
IF
THE
SPECIFIED
PROCESS
OR
PROCESS
GROUP
DOES
NOT
EXIST
OR
IS
NOT
A
CHILD
OF
THE
CALLING
PROCESS
THE
OPTIONS
ARGUMENT
LETS
US
FURTHER
CONTROL
THE
OPERATION
OF
WAITPID
THIS
ARGUMENT
EITHER
IS
OR
IS
CONSTRUCTED
FROM
THE
BITWISE
OR
OF
THE
CONSTANTS
IN
FIGURE
FREEBSD
AND
SOLARIS
SUPPORT
ONE
ADDITIONAL
BUT
NONSTANDARD
OPTION
CONSTANT
WNOWAIT
HAS
THE
SYSTEM
KEEP
THE
PROCESS
WHOSE
TERMINATION
STATUS
IS
RETURNED
BY
WAITPID
IN
A
WAIT
STATE
SO
THAT
IT
MAY
BE
WAITED
FOR
AGAIN
CONSTANT
DESCRIPTION
WCONTINUED
IF
THE
IMPLEMENTATION
SUPPORTS
JOB
CONTROL
THE
STATUS
OF
ANY
CHILD
SPECIFIED
BY
PID
THAT
HAS
BEEN
CONTINUED
AFTER
BEING
STOPPED
BUT
WHOSE
STATUS
HAS
NOT
YET
BEEN
REPORTED
IS
RETURNED
XSI
OPTION
WNOHANG
THE
WAITPID
FUNCTION
WILL
NOT
BLOCK
IF
A
CHILD
SPECIFIED
BY
PID
IS
NOT
IMMEDIATELY
AVAILABLE
IN
THIS
CASE
THE
RETURN
VALUE
IS
WUNTRACED
IF
THE
IMPLEMENTATION
SUPPORTS
JOB
CONTROL
THE
STATUS
OF
ANY
CHILD
SPECIFIED
BY
PID
THAT
HAS
STOPPED
AND
WHOSE
STATUS
HAS
NOT
BEEN
REPORTED
SINCE
IT
HAS
STOPPED
IS
RETURNED
THE
WIFSTOPPED
MACRO
DETERMINES
WHETHER
THE
RETURN
VALUE
CORRESPONDS
TO
A
STOPPED
CHILD
PROCESS
FIGURE
THE
OPTIONS
CONSTANTS
FOR
WAITPID
THE
WAITPID
FUNCTION
PROVIDES
THREE
FEATURES
THAT
AREN
T
PROVIDED
BY
THE
WAIT
FUNCTION
THE
WAITPID
FUNCTION
LETS
US
WAIT
FOR
ONE
PARTICULAR
PROCESS
WHEREAS
THE
WAIT
FUNCTION
RETURNS
THE
STATUS
OF
ANY
TERMINATED
CHILD
WE
LL
RETURN
TO
THIS
FEATURE
WHEN
WE
DISCUSS
THE
POPEN
FUNCTION
THE
WAITPID
FUNCTION
PROVIDES
A
NONBLOCKING
VERSION
OF
WAIT
THERE
ARE
TIMES
WHEN
WE
WANT
TO
FETCH
A
CHILD
STATUS
BUT
WE
DON
T
WANT
TO
BLOCK
THE
WAITPID
FUNCTION
PROVIDES
SUPPORT
FOR
JOB
CONTROL
WITH
THE
WUNTRACED
AND
WCONTINUED
OPTIONS
EXAMPLE
RECALL
OUR
DISCUSSION
IN
SECTION
ABOUT
ZOMBIE
PROCESSES
IF
WE
WANT
TO
WRITE
A
PROCESS
SO
THAT
IT
FORKS
A
CHILD
BUT
WE
DON
T
WANT
TO
WAIT
FOR
THE
CHILD
TO
COMPLETE
AND
WE
DON
T
WANT
THE
CHILD
TO
BECOME
A
ZOMBIE
UNTIL
WE
TERMINATE
THE
TRICK
IS
TO
CALL
FORK
TWICE
THE
PROGRAM
IN
FIGURE
DOES
THIS
WE
CALL
SLEEP
IN
THE
SECOND
CHILD
TO
ENSURE
THAT
THE
FIRST
CHILD
TERMINATES
BEFORE
PRINTING
THE
PARENT
PROCESS
ID
AFTER
A
FORK
EITHER
THE
PARENT
OR
THE
CHILD
CAN
CONTINUE
EXECUTING
WE
NEVER
KNOW
WHICH
WILL
RESUME
EXECUTION
FIRST
IF
WE
DIDN
T
PUT
THE
SECOND
CHILD
TO
SLEEP
AND
IF
IT
RESUMED
EXECUTION
AFTER
THE
FORK
BEFORE
ITS
PARENT
THE
PARENT
PROCESS
ID
THAT
IT
PRINTED
WOULD
BE
THAT
OF
ITS
PARENT
NOT
PROCESS
ID
EXECUTING
THE
PROGRAM
IN
FIGURE
GIVES
US
A
OUT
SECOND
CHILD
PARENT
PID
NOTE
THAT
THE
SHELL
PRINTS
ITS
PROMPT
WHEN
THE
ORIGINAL
PROCESS
TERMINATES
WHICH
IS
BEFORE
THE
SECOND
CHILD
PRINTS
ITS
PARENT
PROCESS
ID
WAITID
FUNCTION
THE
SINGLE
UNIX
SPECIFICATION
INCLUDES
AN
ADDITIONAL
FUNCTION
TO
RETRIEVE
THE
EXIT
STATUS
OF
A
PROCESS
THE
WAITID
FUNCTION
IS
SIMILAR
TO
WAITPID
BUT
PROVIDES
EXTRA
FLEXIBILITY
LIKE
WAITPID
WAITID
ALLOWS
A
PROCESS
TO
SPECIFY
WHICH
CHILDREN
TO
WAIT
FOR
INSTEAD
OF
ENCODING
THIS
INFORMATION
IN
A
SINGLE
ARGUMENT
COMBINED
WITH
THE
PROCESS
ID
OR
PROCESS
GROUP
ID
TWO
SEPARATE
ARGUMENTS
ARE
USED
THE
ID
PARAMETER
IS
INTERPRETED
BASED
ON
THE
VALUE
OF
IDTYPE
THE
TYPES
SUPPORTED
ARE
SUMMARIZED
IN
FIGURE
CONSTANT
DESCRIPTION
WAIT
FOR
A
PARTICULAR
PROCESS
ID
CONTAINS
THE
PROCESS
ID
OF
THE
CHILD
TO
WAIT
FOR
WAIT
FOR
ANY
CHILD
PROCESS
IN
A
PARTICULAR
PROCESS
GROUP
ID
CONTAINS
THE
PROCESS
GROUP
ID
OF
THE
CHILDREN
TO
WAIT
FOR
WAIT
FOR
ANY
CHILD
PROCESS
ID
IS
IGNORED
FIGURE
THE
IDTYPE
CONSTANTS
FOR
WAITID
THE
OPTIONS
ARGUMENT
IS
A
BITWISE
OR
OF
THE
FLAGS
SHOWN
IN
FIGURE
THESE
FLAGS
INDICATE
WHICH
STATE
CHANGES
THE
CALLER
IS
INTERESTED
IN
CONSTANT
DESCRIPTION
WCONTINUED
WAIT
FOR
A
PROCESS
THAT
HAS
PREVIOUSLY
STOPPED
AND
HAS
BEEN
CONTINUED
AND
WHOSE
STATUS
HAS
NOT
YET
BEEN
REPORTED
WEXITED
WAIT
FOR
PROCESSES
THAT
HAVE
EXITED
WNOHANG
RETURN
IMMEDIATELY
INSTEAD
OF
BLOCKING
IF
THERE
IS
NO
CHILD
EXIT
STATUS
AVAILABLE
WNOWAIT
DON
T
DESTROY
THE
CHILD
EXIT
STATUS
THE
CHILD
EXIT
STATUS
CAN
BE
RETRIEVED
BY
A
SUBSEQUENT
CALL
TO
WAIT
WAITID
OR
WAITPID
WSTOPPED
WAIT
FOR
A
PROCESS
THAT
HAS
STOPPED
AND
WHOSE
STATUS
HAS
NOT
YET
BEEN
REPORTED
FIGURE
THE
OPTIONS
CONSTANTS
FOR
WAITID
AT
LEAST
ONE
OF
WCONTINUED
WEXITED
OR
WSTOPPED
MUST
BE
SPECIFIED
IN
THE
OPTIONS
ARGUMENT
THE
INFOP
ARGUMENT
IS
A
POINTER
TO
A
SIGINFO
STRUCTURE
THIS
STRUCTURE
CONTAINS
DETAILED
INFORMATION
ABOUT
THE
SIGNAL
GENERATED
THAT
CAUSED
THE
STATE
CHANGE
IN
THE
CHILD
PROCESS
THE
SIGINFO
STRUCTURE
IS
DISCUSSED
FURTHER
IN
SECTION
OF
THE
FOUR
PLATFORMS
COVERED
IN
THIS
BOOK
ONLY
LINUX
MAC
OS
X
AND
SOLARIS
PROVIDE
SUPPORT
FOR
WAITID
NOTE
HOWEVER
THAT
MAC
OS
X
DOESN
T
SET
ALL
THE
INFORMATION
WE
EXPECT
IN
THE
SIGINFO
STRUCTURE
AND
FUNCTIONS
MOST
UNIX
SYSTEM
IMPLEMENTATIONS
PROVIDE
TWO
ADDITIONAL
FUNCTIONS
AND
HISTORICALLY
THESE
TWO
VARIANTS
DESCEND
FROM
THE
BSD
BRANCH
OF
THE
UNIX
SYSTEM
THE
ONLY
FEATURE
PROVIDED
BY
THESE
TWO
FUNCTIONS
THAT
ISN
T
PROVIDED
BY
THE
WAIT
WAITID
AND
WAITPID
FUNCTIONS
IS
AN
ADDITIONAL
ARGUMENT
THAT
ALLOWS
THE
KERNEL
TO
RETURN
A
SUMMARY
OF
THE
RESOURCES
USED
BY
THE
TERMINATED
PROCESS
AND
ALL
ITS
CHILD
PROCESSES
THE
RESOURCE
INFORMATION
INCLUDES
SUCH
STATISTICS
AS
THE
AMOUNT
OF
USER
CPU
TIME
AMOUNT
OF
SYSTEM
CPU
TIME
NUMBER
OF
PAGE
FAULTS
NUMBER
OF
SIGNALS
RECEIVED
AND
THE
LIKE
REFER
TO
THE
GETRUSAGE
MANUAL
PAGE
FOR
ADDITIONAL
DETAILS
THIS
RESOURCE
INFORMATION
DIFFERS
FROM
THE
RESOURCE
LIMITS
WE
DESCRIBED
IN
SECTION
FIGURE
DETAILS
THE
VARIOUS
ARGUMENTS
SUPPORTED
BY
THE
WAIT
FUNCTIONS
FUNCTION
PID
OPTIONS
RUSAGE
POSIX
FREEBSD
LINUX
MAC
OS
X
SOLARIS
WAIT
WAITID
WAITPID
FIGURE
ARGUMENTS
SUPPORTED
BY
WAIT
FUNCTIONS
ON
VARIOUS
SYSTEMS
THE
FUNCTION
WAS
INCLUDED
IN
EARLIER
VERSIONS
OF
THE
SINGLE
UNIX
SPECIFICATION
IN
VERSION
WAS
MOVED
TO
THE
LEGACY
CATEGORY
WAS
REMOVED
FROM
THE
SPECIFICATION
IN
VERSION
RACE
CONDITIONS
FOR
OUR
PURPOSES
A
RACE
CONDITION
OCCURS
WHEN
MULTIPLE
PROCESSES
ARE
TRYING
TO
DO
SOMETHING
WITH
SHARED
DATA
AND
THE
FINAL
OUTCOME
DEPENDS
ON
THE
ORDER
IN
WHICH
THE
PROCESSES
RUN
THE
FORK
FUNCTION
IS
A
LIVELY
BREEDING
GROUND
FOR
RACE
CONDITIONS
IF
ANY
OF
THE
LOGIC
AFTER
THE
FORK
EITHER
EXPLICITLY
OR
IMPLICITLY
DEPENDS
ON
WHETHER
THE
PARENT
OR
CHILD
RUNS
FIRST
AFTER
THE
FORK
IN
GENERAL
WE
CANNOT
PREDICT
WHICH
PROCESS
RUNS
FIRST
EVEN
IF
WE
KNEW
WHICH
PROCESS
WOULD
RUN
FIRST
WHAT
HAPPENS
AFTER
THAT
PROCESS
STARTS
RUNNING
DEPENDS
ON
THE
SYSTEM
LOAD
AND
THE
KERNEL
SCHEDULING
ALGORITHM
WE
SAW
A
POTENTIAL
RACE
CONDITION
IN
THE
PROGRAM
IN
FIGURE
WHEN
THE
SECOND
CHILD
PRINTED
ITS
PARENT
PROCESS
ID
IF
THE
SECOND
CHILD
RUNS
BEFORE
THE
FIRST
CHILD
THEN
ITS
PARENT
PROCESS
WILL
BE
THE
FIRST
CHILD
BUT
IF
THE
FIRST
CHILD
RUNS
FIRST
AND
HAS
ENOUGH
TIME
TO
EXIT
THEN
THE
PARENT
PROCESS
OF
THE
SECOND
CHILD
IS
INIT
EVEN
CALLING
SLEEP
AS
WE
DID
GUARANTEES
NOTHING
IF
THE
SYSTEM
WAS
HEAVILY
LOADED
THE
SECOND
CHILD
COULD
RESUME
AFTER
SLEEP
RETURNS
BEFORE
THE
FIRST
CHILD
HAS
A
CHANCE
TO
RUN
PROBLEMS
OF
THIS
FORM
CAN
BE
DIFFICULT
TO
DEBUG
BECAUSE
THEY
TEND
TO
WORK
MOST
OF
THE
TIME
A
PROCESS
THAT
WANTS
TO
WAIT
FOR
A
CHILD
TO
TERMINATE
MUST
CALL
ONE
OF
THE
WAIT
FUNCTIONS
IF
A
PROCESS
WANTS
TO
WAIT
FOR
ITS
PARENT
TO
TERMINATE
AS
IN
THE
PROGRAM
FROM
FIGURE
A
LOOP
OF
THE
FOLLOWING
FORM
COULD
BE
USED
WHILE
GETPPID
SLEEP
THE
PROBLEM
WITH
THIS
TYPE
OF
LOOP
CALLED
POLLING
IS
THAT
IT
WASTES
CPU
TIME
AS
THE
CALLER
IS
AWAKENED
EVERY
SECOND
TO
TEST
THE
CONDITION
TO
AVOID
RACE
CONDITIONS
AND
TO
AVOID
POLLING
SOME
FORM
OF
SIGNALING
IS
REQUIRED
BETWEEN
MULTIPLE
PROCESSES
SIGNALS
CAN
BE
USED
FOR
THIS
PURPOSE
AND
WE
DESCRIBE
ONE
WAY
TO
DO
THIS
IN
SECTION
VARIOUS
FORMS
OF
INTERPROCESS
COMMUNICATION
IPC
CAN
ALSO
BE
USED
WE
LL
DISCUSS
SOME
OF
THESE
OPTIONS
IN
CHAPTERS
AND
FOR
A
PARENT
AND
CHILD
RELATIONSHIP
WE
OFTEN
HAVE
THE
FOLLOWING
SCENARIO
AFTER
THE
FORK
BOTH
THE
PARENT
AND
THE
CHILD
HAVE
SOMETHING
TO
DO
FOR
EXAMPLE
THE
PARENT
COULD
UPDATE
A
RECORD
IN
A
LOG
FILE
WITH
THE
CHILD
PROCESS
ID
AND
THE
CHILD
MIGHT
HAVE
TO
CREATE
A
FILE
FOR
THE
PARENT
IN
THIS
EXAMPLE
WE
REQUIRE
THAT
EACH
PROCESS
TELL
THE
OTHER
WHEN
IT
HAS
FINISHED
ITS
INITIAL
SET
OF
OPERATIONS
AND
THAT
EACH
WAIT
FOR
THE
OTHER
TO
COMPLETE
BEFORE
HEADING
OFF
ON
ITS
OWN
THE
FOLLOWING
CODE
ILLUSTRATES
THIS
SCENARIO
INCLUDE
APUE
H
SET
THINGS
UP
FOR
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
CHILD
CHILD
DOES
WHATEVER
IS
NECESSARY
GETPPID
TELL
PARENT
WE
RE
DONE
AND
WAIT
FOR
PARENT
AND
THE
CHILD
CONTINUES
ON
ITS
WAY
EXIT
PARENT
DOES
WHATEVER
IS
NECESSARY
PID
TELL
CHILD
WE
RE
DONE
AND
WAIT
FOR
CHILD
AND
THE
PARENT
CONTINUES
ON
ITS
WAY
EXIT
WE
ASSUME
THAT
THE
HEADER
APUE
H
DEFINES
WHATEVER
VARIABLES
ARE
REQUIRED
THE
FIVE
ROUTINES
AND
CAN
BE
EITHER
MACROS
OR
FUNCTIONS
WE
LL
SHOW
VARIOUS
WAYS
TO
IMPLEMENT
THESE
TELL
AND
WAIT
ROUTINES
IN
LATER
CHAPTERS
SECTION
SHOWS
AN
IMPLEMENTATION
USING
SIGNALS
FIGURE
SHOWS
AN
IMPLEMENTATION
USING
PIPES
LET
LOOK
AT
AN
EXAMPLE
THAT
USES
THESE
FIVE
ROUTINES
EXAMPLE
THE
PROGRAM
IN
FIGURE
OUTPUTS
TWO
STRINGS
ONE
FROM
THE
CHILD
AND
ONE
FROM
THE
PARENT
THE
PROGRAM
CONTAINS
A
RACE
CONDITION
BECAUSE
THE
OUTPUT
DEPENDS
ON
THE
ORDER
IN
WHICH
THE
PROCESSES
ARE
RUN
BY
THE
KERNEL
AND
THE
LENGTH
OF
TIME
FOR
WHICH
EACH
PROCESS
RUNS
WE
SET
THE
STANDARD
OUTPUT
UNBUFFERED
SO
EVERY
CHARACTER
OUTPUT
GENERATES
A
WRITE
THE
GOAL
IN
THIS
EXAMPLE
IS
TO
ALLOW
THE
KERNEL
TO
SWITCH
BETWEEN
THE
TWO
PROCESSES
AS
OFTEN
AS
POSSIBLE
TO
DEMONSTRATE
THE
RACE
CONDITION
IF
WE
DIDN
T
DO
THIS
WE
MIGHT
NEVER
SEE
THE
TYPE
OF
OUTPUT
THAT
FOLLOWS
NOT
SEEING
THE
ERRONEOUS
OUTPUT
DOESN
T
MEAN
THAT
THE
RACE
CONDITION
DOESN
T
EXIST
IT
SIMPLY
MEANS
THAT
WE
CAN
T
SEE
IT
ON
THIS
PARTICULAR
SYSTEM
THE
FOLLOWING
ACTUAL
OUTPUT
SHOWS
HOW
THE
RESULTS
CAN
VARY
WE
MENTIONED
IN
SECTION
THAT
ONE
USE
OF
THE
FORK
FUNCTION
IS
TO
CREATE
A
NEW
PROCESS
THE
CHILD
THAT
THEN
CAUSES
ANOTHER
PROGRAM
TO
BE
EXECUTED
BY
CALLING
ONE
OF
THE
EXEC
FUNCTIONS
WHEN
A
PROCESS
CALLS
ONE
OF
THE
EXEC
FUNCTIONS
THAT
PROCESS
IS
COMPLETELY
REPLACED
BY
THE
NEW
PROGRAM
AND
THE
NEW
PROGRAM
STARTS
EXECUTING
AT
ITS
MAIN
FUNCTION
THE
PROCESS
ID
DOES
NOT
CHANGE
ACROSS
AN
EXEC
BECAUSE
A
NEW
PROCESS
IS
NOT
CREATED
EXEC
MERELY
REPLACES
THE
CURRENT
PROCESS
ITS
TEXT
DATA
HEAP
AND
STACK
SEGMENTS
WITH
A
BRAND
NEW
PROGRAM
FROM
DISK
THERE
ARE
SEVEN
DIFFERENT
EXEC
FUNCTIONS
BUT
WE
LL
OFTEN
SIMPLY
REFER
TO
THE
EXEC
FUNCTION
WHICH
MEANS
THAT
WE
COULD
USE
ANY
OF
THE
SEVEN
FUNCTIONS
THESE
SEVEN
FUNCTIONS
ROUND
OUT
THE
UNIX
SYSTEM
PROCESS
CONTROL
PRIMITIVES
WITH
FORK
WE
CAN
CREATE
NEW
PROCESSES
AND
WITH
THE
EXEC
FUNCTIONS
WE
CAN
INITIATE
NEW
PROGRAMS
THE
EXIT
FUNCTION
AND
THE
WAIT
FUNCTIONS
HANDLE
TERMINATION
AND
WAITING
FOR
TERMINATION
THESE
ARE
THE
ONLY
PROCESS
CONTROL
PRIMITIVES
WE
NEED
WE
LL
USE
THESE
PRIMITIVES
IN
LATER
SECTIONS
TO
BUILD
ADDITIONAL
FUNCTIONS
SUCH
AS
POPEN
AND
SYSTEM
THE
FIRST
DIFFERENCE
IN
THESE
FUNCTIONS
IS
THAT
THE
FIRST
FOUR
TAKE
A
PATHNAME
ARGUMENT
THE
NEXT
TWO
TAKE
A
FILENAME
ARGUMENT
AND
THE
LAST
ONE
TAKES
A
FILE
DESCRIPTOR
ARGUMENT
WHEN
A
FILENAME
ARGUMENT
IS
SPECIFIED
IF
FILENAME
CONTAINS
A
SLASH
IT
IS
TAKEN
AS
A
PATHNAME
OTHERWISE
THE
EXECUTABLE
FILE
IS
SEARCHED
FOR
IN
THE
DIRECTORIES
SPECIFIED
BY
THE
PATH
ENVIRONMENT
VARIABLE
THE
PATH
VARIABLE
CONTAINS
A
LIST
OF
DIRECTORIES
CALLED
PATH
PREFIXES
THAT
ARE
SEPARATED
BY
COLONS
FOR
EXAMPLE
THE
NAME
VALUE
ENVIRONMENT
STRING
PATH
BIN
USR
BIN
USR
LOCAL
BIN
SPECIFIES
FOUR
DIRECTORIES
TO
SEARCH
THE
LAST
PATH
PREFIX
SPECIFIES
THE
CURRENT
DIRECTORY
A
ZERO
LENGTH
PREFIX
ALSO
MEANS
THE
CURRENT
DIRECTORY
IT
CAN
BE
SPECIFIED
AS
A
COLON
AT
THE
BEGINNING
OF
THE
VALUE
TWO
COLONS
IN
A
ROW
OR
A
COLON
AT
THE
END
OF
THE
VALUE
THERE
ARE
SECURITY
REASONS
FOR
NEVER
INCLUDING
THE
CURRENT
DIRECTORY
IN
THE
SEARCH
PATH
SEE
GARFINKEL
ET
AL
IF
EITHER
EXECLP
OR
EXECVP
FINDS
AN
EXECUTABLE
FILE
USING
ONE
OF
THE
PATH
PREFIXES
BUT
THE
FILE
ISN
T
A
MACHINE
EXECUTABLE
THAT
WAS
GENERATED
BY
THE
LINK
EDITOR
THE
FUNCTION
ASSUMES
THAT
THE
FILE
IS
A
SHELL
SCRIPT
AND
TRIES
TO
INVOKE
BIN
SH
WITH
THE
FILENAME
AS
INPUT
TO
THE
SHELL
WITH
FEXECVE
WE
AVOID
THE
ISSUE
OF
FINDING
THE
CORRECT
EXECUTABLE
FILE
ALTOGETHER
AND
RELY
ON
THE
CALLER
TO
DO
THIS
BY
USING
A
FILE
DESCRIPTOR
THE
CALLER
CAN
VERIFY
THE
FILE
IS
IN
FACT
THE
INTENDED
FILE
AND
EXECUTE
IT
WITHOUT
A
RACE
OTHERWISE
A
MALICIOUS
USER
WITH
APPROPRIATE
PRIVILEGES
COULD
REPLACE
THE
EXECUTABLE
FILE
OR
A
PORTION
OF
THE
PATH
TO
THE
EXECUTABLE
FILE
AFTER
IT
HAS
BEEN
LOCATED
AND
VERIFIED
BUT
BEFORE
THE
CALLER
CAN
EXECUTE
IT
RECALL
THE
DISCUSSION
OF
TOCTTOU
ERRORS
IN
SECTION
THE
NEXT
DIFFERENCE
CONCERNS
THE
PASSING
OF
THE
ARGUMENT
LIST
L
STANDS
FOR
LIST
AND
V
STANDS
FOR
VECTOR
THE
FUNCTIONS
EXECL
EXECLP
AND
EXECLE
REQUIRE
EACH
OF
THE
COMMAND
LINE
ARGUMENTS
TO
THE
NEW
PROGRAM
TO
BE
SPECIFIED
AS
SEPARATE
ARGUMENTS
WE
MARK
THE
END
OF
THE
ARGUMENTS
WITH
A
NULL
POINTER
FOR
THE
OTHER
FOUR
FUNCTIONS
EXECV
EXECVP
EXECVE
AND
FEXECVE
WE
HAVE
TO
BUILD
AN
ARRAY
OF
POINTERS
TO
THE
ARGUMENTS
AND
THE
ADDRESS
OF
THIS
ARRAY
IS
THE
ARGUMENT
TO
THESE
THREE
FUNCTIONS
BEFORE
USING
ISO
C
PROTOTYPES
THE
NORMAL
WAY
TO
SHOW
THE
COMMAND
LINE
ARGUMENTS
FOR
THE
THREE
FUNCTIONS
EXECL
EXECLE
AND
EXECLP
WAS
CHAR
CHAR
CHAR
ARGN
CHAR
THIS
SYNTAX
EXPLICITLY
SHOWS
THAT
THE
FINAL
COMMAND
LINE
ARGUMENT
IS
FOLLOWED
BY
A
NULL
POINTER
IF
THIS
NULL
POINTER
IS
SPECIFIED
BY
THE
CONSTANT
WE
MUST
CAST
IT
TO
A
POINTER
IF
WE
DON
T
IT
INTERPRETED
AS
AN
INTEGER
ARGUMENT
IF
THE
SIZE
OF
AN
INTEGER
IS
DIFFERENT
FROM
THE
SIZE
OF
A
CHAR
THE
ACTUAL
ARGUMENTS
TO
THE
EXEC
FUNCTION
WILL
BE
WRONG
THE
FINAL
DIFFERENCE
IS
THE
PASSING
OF
THE
ENVIRONMENT
LIST
TO
THE
NEW
PROGRAM
THE
THREE
FUNCTIONS
WHOSE
NAMES
END
IN
AN
E
EXECLE
EXECVE
AND
FEXECVE
ALLOW
US
TO
PASS
A
POINTER
TO
AN
ARRAY
OF
POINTERS
TO
THE
ENVIRONMENT
STRINGS
THE
OTHER
FOUR
FUNCTIONS
HOWEVER
USE
THE
ENVIRON
VARIABLE
IN
THE
CALLING
PROCESS
TO
COPY
THE
EXISTING
ENVIRONMENT
FOR
THE
NEW
PROGRAM
RECALL
OUR
DISCUSSION
OF
THE
ENVIRONMENT
STRINGS
IN
SECTION
AND
FIGURE
WE
MENTIONED
THAT
IF
THE
SYSTEM
SUPPORTED
SUCH
FUNCTIONS
AS
SETENV
AND
PUTENV
WE
COULD
CHANGE
THE
CURRENT
ENVIRONMENT
AND
THE
ENVIRONMENT
OF
ANY
SUBSEQUENT
CHILD
PROCESSES
BUT
WE
COULDN
T
AFFECT
THE
ENVIRONMENT
OF
THE
PARENT
PROCESS
NORMALLY
A
PROCESS
ALLOWS
ITS
ENVIRONMENT
TO
BE
PROPAGATED
TO
ITS
CHILDREN
BUT
IN
SOME
CASES
A
PROCESS
WANTS
TO
SPECIFY
A
CERTAIN
ENVIRONMENT
FOR
A
CHILD
ONE
EXAMPLE
OF
THE
LATTER
IS
THE
LOGIN
PROGRAM
WHEN
A
NEW
LOGIN
SHELL
IS
INITIATED
NORMALLY
LOGIN
CREATES
A
SPECIFIC
ENVIRONMENT
WITH
ONLY
A
FEW
VARIABLES
DEFINED
AND
LETS
US
THROUGH
THE
SHELL
START
UP
FILE
ADD
VARIABLES
TO
THE
ENVIRONMENT
WHEN
WE
LOG
IN
BEFORE
USING
ISO
C
PROTOTYPES
THE
ARGUMENTS
TO
EXECLE
WERE
SHOWN
AS
CHAR
PATHNAME
CHAR
CHAR
ARGN
CHAR
CHAR
ENVP
THIS
SYNTAX
SPECIFICALLY
SHOWS
THAT
THE
FINAL
ARGUMENT
IS
THE
ADDRESS
OF
THE
ARRAY
OF
CHARACTER
POINTERS
TO
THE
ENVIRONMENT
STRINGS
THE
ISO
C
PROTOTYPE
DOESN
T
SHOW
THIS
AS
ALL
THE
COMMAND
LINE
ARGUMENTS
THE
NULL
POINTER
AND
THE
ENVP
POINTER
ARE
SHOWN
WITH
THE
ELLIPSIS
NOTATION
THE
ARGUMENTS
FOR
THESE
SEVEN
EXEC
FUNCTIONS
ARE
DIFFICULT
TO
REMEMBER
THE
LETTERS
IN
THE
FUNCTION
NAMES
HELP
SOMEWHAT
THE
LETTER
P
MEANS
THAT
THE
FUNCTION
TAKES
A
FILENAME
ARGUMENT
AND
USES
THE
PATH
ENVIRONMENT
VARIABLE
TO
FIND
THE
EXECUTABLE
FILE
THE
LETTER
L
MEANS
THAT
THE
FUNCTION
TAKES
A
LIST
OF
ARGUMENTS
AND
IS
MUTUALLY
EXCLUSIVE
WITH
THE
LETTER
V
WHICH
MEANS
THAT
IT
TAKES
AN
ARGV
VECTOR
FINALLY
THE
LETTER
E
MEANS
THAT
THE
FUNCTION
TAKES
AN
ENVP
ARRAY
INSTEAD
OF
USING
THE
CURRENT
ENVIRONMENT
FIGURE
SHOWS
THE
DIFFERENCES
AMONG
THESE
SEVEN
FUNCTIONS
FUNCTION
PATHNAME
FILENAME
FD
ARG
LIST
ARGV
ENVIRON
ENVP
EXECL
EXECLP
EXECLE
EXECV
EXECVP
EXECVE
FEXECVE
LETTER
IN
NAME
P
F
L
V
E
FIGURE
DIFFERENCES
AMONG
THE
SEVEN
EXEC
FUNCTIONS
EVERY
SYSTEM
HAS
A
LIMIT
ON
THE
TOTAL
SIZE
OF
THE
ARGUMENT
LIST
AND
THE
ENVIRONMENT
LIST
FROM
SECTION
AND
FIGURE
THIS
LIMIT
IS
GIVEN
BY
THIS
VALUE
MUST
BE
AT
LEAST
BYTES
ON
A
POSIX
SYSTEM
WE
SOMETIMES
ENCOUNTER
THIS
LIMIT
WHEN
USING
THE
SHELL
FILENAME
EXPANSION
FEATURE
TO
GENERATE
A
LIST
OF
FILENAMES
ON
SOME
SYSTEMS
FOR
EXAMPLE
THE
COMMAND
GREP
GETRLIMIT
USR
SHARE
MAN
CAN
GENERATE
A
SHELL
ERROR
OF
THE
FORM
ARGUMENT
LIST
TOO
LONG
HISTORICALLY
THE
LIMIT
IN
OLDER
SYSTEM
V
IMPLEMENTATIONS
WAS
BYTES
OLDER
BSD
SYSTEMS
HAD
A
LIMIT
OF
BYTES
THE
LIMIT
IN
CURRENT
SYSTEMS
IS
MUCH
HIGHER
SEE
THE
OUTPUT
FROM
THE
PROGRAM
IN
FIGURE
WHICH
IS
SUMMARIZED
IN
FIGURE
TO
GET
AROUND
THE
LIMITATION
IN
ARGUMENT
LIST
SIZE
WE
CAN
USE
THE
XARGS
COMMAND
TO
BREAK
UP
LONG
ARGUMENT
LISTS
TO
LOOK
FOR
ALL
THE
OCCURRENCES
OF
GETRLIMIT
IN
THE
MAN
PAGES
ON
OUR
SYSTEM
WE
COULD
USE
FIND
USR
SHARE
MAN
TYPE
F
PRINT
XARGS
GREP
GETRLIMIT
IF
THE
MAN
PAGES
ON
OUR
SYSTEM
ARE
COMPRESSED
HOWEVER
WE
COULD
TRY
FIND
USR
SHARE
MAN
TYPE
F
PRINT
XARGS
BZGREP
GETRLIMIT
WE
USE
THE
TYPE
F
OPTION
TO
THE
FIND
COMMAND
TO
RESTRICT
THE
LIST
SO
THAT
IT
CONTAINS
ONLY
REGULAR
FILES
BECAUSE
THE
GREP
COMMANDS
CAN
T
SEARCH
FOR
PATTERNS
IN
DIRECTORIES
AND
WE
WANT
TO
AVOID
UNNECESSARY
ERROR
MESSAGES
WE
VE
MENTIONED
THAT
THE
PROCESS
ID
DOES
NOT
CHANGE
AFTER
AN
EXEC
BUT
THE
NEW
PROGRAM
INHERITS
ADDITIONAL
PROPERTIES
FROM
THE
CALLING
PROCESS
PROCESS
ID
AND
PARENT
PROCESS
ID
REAL
USER
ID
AND
REAL
GROUP
ID
SUPPLEMENTARY
GROUP
IDS
PROCESS
GROUP
ID
SESSION
ID
CONTROLLING
TERMINAL
TIME
LEFT
UNTIL
ALARM
CLOCK
CURRENT
WORKING
DIRECTORY
ROOT
DIRECTORY
FILE
MODE
CREATION
MASK
FILE
LOCKS
PROCESS
SIGNAL
MASK
PENDING
SIGNALS
RESOURCE
LIMITS
NICE
VALUE
ON
XSI
CONFORMANT
SYSTEMS
SEE
SECTION
VALUES
FOR
AND
THE
HANDLING
OF
OPEN
FILES
DEPENDS
ON
THE
VALUE
OF
THE
CLOSE
ON
EXEC
FLAG
FOR
EACH
DESCRIPTOR
RECALL
FROM
FIGURE
AND
OUR
MENTION
OF
THE
FLAG
IN
SECTION
THAT
EVERY
OPEN
DESCRIPTOR
IN
A
PROCESS
HAS
A
CLOSE
ON
EXEC
FLAG
IF
THIS
FLAG
IS
SET
THE
DESCRIPTOR
IS
CLOSED
ACROSS
AN
EXEC
OTHERWISE
THE
DESCRIPTOR
IS
LEFT
OPEN
ACROSS
THE
EXEC
THE
DEFAULT
IS
TO
LEAVE
THE
DESCRIPTOR
OPEN
ACROSS
THE
EXEC
UNLESS
WE
SPECIFICALLY
SET
THE
CLOSE
ON
EXEC
FLAG
USING
FCNTL
POSIX
SPECIFICALLY
REQUIRES
THAT
OPEN
DIRECTORY
STREAMS
RECALL
THE
OPENDIR
FUNCTION
FROM
SECTION
BE
CLOSED
ACROSS
AN
EXEC
THIS
IS
NORMALLY
DONE
BY
THE
OPENDIR
FUNCTION
CALLING
FCNTL
TO
SET
THE
CLOSE
ON
EXEC
FLAG
FOR
THE
DESCRIPTOR
CORRESPONDING
TO
THE
OPEN
DIRECTORY
STREAM
NOTE
THAT
THE
REAL
USER
ID
AND
THE
REAL
GROUP
ID
REMAIN
THE
SAME
ACROSS
THE
EXEC
BUT
THE
EFFECTIVE
IDS
CAN
CHANGE
DEPENDING
ON
THE
STATUS
OF
THE
SET
USER
ID
AND
THE
SET
GROUP
ID
BITS
FOR
THE
PROGRAM
FILE
THAT
IS
EXECUTED
IF
THE
SET
USER
ID
BIT
IS
SET
FOR
THE
NEW
PROGRAM
THE
EFFECTIVE
USER
ID
BECOMES
THE
OWNER
ID
OF
THE
PROGRAM
FILE
OTHERWISE
THE
EFFECTIVE
USER
ID
IS
NOT
CHANGED
IT
NOT
SET
TO
THE
REAL
USER
ID
THE
GROUP
ID
IS
HANDLED
IN
THE
SAME
WAY
IN
MANY
UNIX
SYSTEM
IMPLEMENTATIONS
ONLY
ONE
OF
THESE
SEVEN
FUNCTIONS
EXECVE
IS
A
SYSTEM
CALL
WITHIN
THE
KERNEL
THE
OTHER
SIX
ARE
JUST
LIBRARY
FUNCTIONS
THAT
EVENTUALLY
INVOKE
THIS
SYSTEM
CALL
WE
CAN
ILLUSTRATE
THE
RELATIONSHIP
AMONG
THESE
SEVEN
FUNCTIONS
AS
SHOWN
IN
FIGURE
BUILD
ARGV
BUILD
ARGV
BUILD
ARGV
TRY
EACH
PATH
PREFIX
USE
ENVIRON
ATH
FROM
SELF
FD
FIGURE
RELATIONSHIP
OF
THE
SEVEN
EXEC
FUNCTIONS
IN
THIS
ARRANGEMENT
THE
LIBRARY
FUNCTIONS
EXECLP
AND
EXECVP
PROCESS
THE
PATH
ENVIRONMENT
VARIABLE
LOOKING
FOR
THE
FIRST
PATH
PREFIX
THAT
CONTAINS
AN
EXECUTABLE
FILE
NAMED
FILENAME
THE
FEXECVE
LIBRARY
FUNCTION
USES
PROC
TO
CONVERT
THE
FILE
DESCRIPTOR
ARGUMENT
INTO
A
PATHNAME
THAT
CAN
BE
USED
BY
EXECVE
TO
EXECUTE
THE
PROGRAM
THIS
DESCRIBES
HOW
FEXECVE
IS
IMPLEMENTED
IN
FREEBSD
AND
LINUX
OTHER
SYSTEMS
MIGHT
TAKE
A
DIFFERENT
APPROACH
FOR
EXAMPLE
A
SYSTEM
WITHOUT
PROC
OR
DEV
FD
COULD
IMPLEMENT
FEXECVE
AS
A
SYSTEM
CALL
VENEER
THAT
TRANSLATES
THE
FILE
DESCRIPTOR
ARGUMENT
INTO
AN
I
NODE
POINTER
IMPLEMENT
EXECVE
AS
A
SYSTEM
CALL
VENEER
THAT
TRANSLATES
THE
PATHNAME
ARGUMENT
INTO
AN
I
NODE
POINTER
AND
PLACE
ALL
THE
REST
OF
THE
EXEC
CODE
COMMON
TO
BOTH
EXECVE
AND
FEXECVE
IN
A
SEPARATE
FUNCTION
TO
BE
CALLED
WITH
AN
I
NODE
POINTER
FOR
THE
FILE
TO
BE
EXECUTED
EXAMPLE
THE
PROGRAM
IN
FIGURE
DEMONSTRATES
THE
EXEC
FUNCTIONS
INCLUDE
APUE
H
INCLUDE
SYS
WAIT
H
CHAR
USER
UNKNOWN
PATH
TMP
NULL
INT
MAIN
VOID
PID
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
SPECIFY
PATHNAME
SPECIFY
ENVIRONMENT
IF
EXECLE
HOME
SAR
BIN
ECHOALL
ECHOALL
MY
CHAR
EXECLE
ERROR
IF
WAITPID
PID
NULL
WAIT
ERROR
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
SPECIFY
FILENAME
INHERIT
ENVIRONMENT
IF
EXECLP
ECHOALL
ECHOALL
ONLY
ARG
CHAR
EXECLP
ERROR
EXIT
FIGURE
EXAMPLE
OF
EXEC
FUNCTIONS
WE
FIRST
CALL
EXECLE
WHICH
REQUIRES
A
PATHNAME
AND
A
SPECIFIC
ENVIRONMENT
THE
NEXT
CALL
IS
TO
EXECLP
WHICH
USES
A
FILENAME
AND
PASSES
THE
CALLER
ENVIRONMENT
TO
THE
NEW
PROGRAM
THE
ONLY
REASON
THE
CALL
TO
EXECLP
WORKS
IS
THAT
THE
DIRECTORY
HOME
SAR
BIN
IS
ONE
OF
THE
CURRENT
PATH
PREFIXES
NOTE
ALSO
THAT
WE
SET
THE
FIRST
ARGUMENT
ARGV
IN
THE
NEW
PROGRAM
TO
BE
THE
FILENAME
COMPONENT
OF
THE
PATHNAME
SOME
SHELLS
SET
THIS
ARGUMENT
TO
BE
THE
COMPLETE
PATHNAME
THIS
IS
A
CONVENTION
ONLY
WE
CAN
SET
ARGV
TO
ANY
STRING
WE
LIKE
THE
LOGIN
COMMAND
DOES
THIS
WHEN
IT
EXECUTES
THE
SHELL
BEFORE
EXECUTING
THE
SHELL
LOGIN
ADDS
A
DASH
AS
A
PREFIX
TO
ARGV
TO
INDICATE
TO
THE
SHELL
THAT
IT
IS
BEING
INVOKED
AS
A
LOGIN
SHELL
A
LOGIN
SHELL
WILL
EXECUTE
THE
START
UP
PROFILE
COMMANDS
WHEREAS
A
NONLOGIN
SHELL
WILL
NOT
THE
PROGRAM
ECHOALL
THAT
IS
EXECUTED
TWICE
IN
THE
PROGRAM
IN
FIGURE
IS
SHOWN
IN
FIGURE
IT
IS
A
TRIVIAL
PROGRAM
THAT
ECHOES
ALL
ITS
COMMAND
LINE
ARGUMENTS
AND
ITS
ENTIRE
ENVIRONMENT
LIST
WHEN
WE
EXECUTE
THE
PROGRAM
FROM
FIGURE
WE
GET
A
OUT
ARGV
ECHOALL
ARGV
ARGV
MY
USER
UNKNOWN
PATH
TMP
ARGV
ECHOALL
ARGV
ONLY
ARG
USER
SAR
LOGNAME
SAR
SHELL
BIN
BASH
MORE
LINES
THAT
AREN
T
SHOWN
HOME
HOME
SAR
NOTE
THAT
THE
SHELL
PROMPT
APPEARED
BEFORE
THE
PRINTING
OF
ARGV
FROM
THE
SECOND
EXEC
THIS
OCCURRED
BECAUSE
THE
PARENT
DID
NOT
WAIT
FOR
THIS
CHILD
PROCESS
TO
FINISH
CHANGING
USER
IDS
AND
GROUP
IDS
IN
THE
UNIX
SYSTEM
PRIVILEGES
SUCH
AS
BEING
ABLE
TO
CHANGE
THE
SYSTEM
NOTION
OF
THE
CURRENT
DATE
AND
ACCESS
CONTROL
SUCH
AS
BEING
ABLE
TO
READ
OR
WRITE
A
PARTICULAR
FILE
ARE
BASED
ON
USER
AND
GROUP
IDS
WHEN
OUR
PROGRAMS
NEED
ADDITIONAL
PRIVILEGES
OR
NEED
TO
GAIN
ACCESS
TO
RESOURCES
THAT
THEY
CURRENTLY
AREN
T
ALLOWED
TO
ACCESS
THEY
NEED
TO
CHANGE
THEIR
USER
OR
GROUP
ID
TO
AN
ID
THAT
HAS
THE
APPROPRIATE
PRIVILEGE
OR
ACCESS
SIMILARLY
WHEN
OUR
PROGRAMS
NEED
TO
LOWER
THEIR
PRIVILEGES
OR
PREVENT
ACCESS
TO
CERTAIN
RESOURCES
THEY
DO
SO
BY
CHANGING
EITHER
THEIR
USER
ID
OR
GROUP
ID
TO
AN
ID
WITHOUT
THE
PRIVILEGE
OR
ABILITY
ACCESS
TO
THE
RESOURCE
IN
GENERAL
WE
TRY
TO
USE
THE
LEAST
PRIVILEGE
MODEL
WHEN
WE
DESIGN
OUR
APPLICATIONS
ACCORDING
TO
THIS
MODEL
OUR
PROGRAMS
SHOULD
USE
THE
LEAST
PRIVILEGE
NECESSARY
TO
ACCOMPLISH
ANY
GIVEN
TASK
THIS
REDUCES
THE
RISK
THAT
SECURITY
MIGHT
BE
COMPROMISED
BY
A
MALICIOUS
USER
TRYING
TO
TRICK
OUR
PROGRAMS
INTO
USING
THEIR
PRIVILEGES
IN
UNINTENDED
WAYS
WE
CAN
SET
THE
REAL
USER
ID
AND
EFFECTIVE
USER
ID
WITH
THE
SETUID
FUNCTION
SIMILARLY
WE
CAN
SET
THE
REAL
GROUP
ID
AND
THE
EFFECTIVE
GROUP
ID
WITH
THE
SETGID
FUNCTION
INCLUDE
UNISTD
H
INT
SETUID
UID
INT
SETGID
GID
BOTH
RETURN
IF
OK
ON
ERROR
THERE
ARE
RULES
FOR
WHO
CAN
CHANGE
THE
IDS
LET
CONSIDER
ONLY
THE
USER
ID
FOR
NOW
EVERYTHING
WE
DESCRIBE
FOR
THE
USER
ID
ALSO
APPLIES
TO
THE
GROUP
ID
IF
THE
PROCESS
HAS
SUPERUSER
PRIVILEGES
THE
SETUID
FUNCTION
SETS
THE
REAL
USER
ID
EFFECTIVE
USER
ID
AND
SAVED
SET
USER
ID
TO
UID
IF
THE
PROCESS
DOES
NOT
HAVE
SUPERUSER
PRIVILEGES
BUT
UID
EQUALS
EITHER
THE
REAL
USER
ID
OR
THE
SAVED
SET
USER
ID
SETUID
SETS
ONLY
THE
EFFECTIVE
USER
ID
TO
UID
THE
REAL
USER
ID
AND
THE
SAVED
SET
USER
ID
ARE
NOT
CHANGED
IF
NEITHER
OF
THESE
TWO
CONDITIONS
IS
TRUE
ERRNO
IS
SET
TO
EPERM
AND
IS
RETURNED
HERE
WE
ARE
ASSUMING
THAT
IS
TRUE
IF
THIS
FEATURE
ISN
T
PROVIDED
THEN
DELETE
ALL
PRECEDING
REFERENCES
TO
THE
SAVED
SET
USER
ID
THE
SAVED
IDS
ARE
A
MANDATORY
FEATURE
IN
THE
VERSION
OF
POSIX
THEY
WERE
OPTIONAL
IN
OLDER
VERSIONS
OF
POSIX
TO
SEE
WHETHER
AN
IMPLEMENTATION
SUPPORTS
THIS
FEATURE
AN
APPLICATION
CAN
TEST
FOR
THE
CONSTANT
AT
COMPILE
TIME
OR
CALL
SYSCONF
WITH
THE
ARGUMENT
AT
RUNTIME
WE
CAN
MAKE
A
FEW
STATEMENTS
ABOUT
THE
THREE
USER
IDS
THAT
THE
KERNEL
MAINTAINS
ONLY
A
SUPERUSER
PROCESS
CAN
CHANGE
THE
REAL
USER
ID
NORMALLY
THE
REAL
USER
ID
IS
SET
BY
THE
LOGIN
PROGRAM
WHEN
WE
LOG
IN
AND
NEVER
CHANGES
BECAUSE
LOGIN
IS
A
SUPERUSER
PROCESS
IT
SETS
ALL
THREE
USER
IDS
WHEN
IT
CALLS
SETUID
THE
EFFECTIVE
USER
ID
IS
SET
BY
THE
EXEC
FUNCTIONS
ONLY
IF
THE
SET
USER
ID
BIT
IS
SET
FOR
THE
PROGRAM
FILE
IF
THE
SET
USER
ID
BIT
IS
NOT
SET
THE
EXEC
FUNCTIONS
LEAVE
THE
EFFECTIVE
USER
ID
AS
ITS
CURRENT
VALUE
WE
CAN
CALL
SETUID
AT
ANY
TIME
TO
SET
THE
EFFECTIVE
USER
ID
TO
EITHER
THE
REAL
USER
ID
OR
THE
SAVED
SET
USER
ID
NATURALLY
WE
CAN
T
SET
THE
EFFECTIVE
USER
ID
TO
ANY
RANDOM
VALUE
THE
SAVED
SET
USER
ID
IS
COPIED
FROM
THE
EFFECTIVE
USER
ID
BY
EXEC
IF
THE
FILE
SET
USER
ID
BIT
IS
SET
THIS
COPY
IS
SAVED
AFTER
EXEC
STORES
THE
EFFECTIVE
USER
ID
FROM
THE
FILE
USER
ID
FIGURE
SUMMARIZES
THE
VARIOUS
WAYS
THESE
THREE
USER
IDS
CAN
BE
CHANGED
ID
EXEC
SETUID
UID
SET
USER
ID
BIT
OFF
SET
USER
ID
BIT
ON
SUPERUSER
UNPRIVILEGED
USER
REAL
USER
ID
EFFECTIVE
USER
ID
SAVED
SET
USER
ID
UNCHANGED
UNCHANGED
COPIED
FROM
EFFECTIVE
USER
ID
UNCHANGED
SET
FROM
USER
ID
OF
PROGRAM
FILE
COPIED
FROM
EFFECTIVE
USER
ID
SET
TO
UID
SET
TO
UID
SET
TO
UID
UNCHANGED
SET
TO
UID
UNCHANGED
FIGURE
WAYS
TO
CHANGE
THE
THREE
USER
IDS
NOTE
THAT
WE
CAN
OBTAIN
ONLY
THE
CURRENT
VALUE
OF
THE
REAL
USER
ID
AND
THE
EFFECTIVE
USER
ID
WITH
THE
FUNCTIONS
GETUID
AND
GETEUID
FROM
SECTION
WE
HAVE
NO
PORTABLE
WAY
TO
OBTAIN
THE
CURRENT
VALUE
OF
THE
SAVED
SET
USER
ID
FREEBSD
AND
LINUX
PROVIDE
THE
GETRESUID
AND
GETRESGID
FUNCTIONS
WHICH
CAN
BE
USED
TO
GET
THE
SAVED
SET
USER
ID
AND
SAVED
SET
GROUP
ID
RESPECTIVELY
SETREUID
AND
SETREGID
FUNCTIONS
HISTORICALLY
BSD
SUPPORTED
THE
SWAPPING
OF
THE
REAL
USER
ID
AND
THE
EFFECTIVE
USER
ID
WITH
THE
SETREUID
FUNCTION
WE
CAN
SUPPLY
A
VALUE
OF
FOR
ANY
OF
THE
ARGUMENTS
TO
INDICATE
THAT
THE
CORRESPONDING
ID
SHOULD
REMAIN
UNCHANGED
THE
RULE
IS
SIMPLE
AN
UNPRIVILEGED
USER
CAN
ALWAYS
SWAP
BETWEEN
THE
REAL
USER
ID
AND
THE
EFFECTIVE
USER
ID
THIS
ALLOWS
A
SET
USER
ID
PROGRAM
TO
SWAP
TO
THE
USER
NORMAL
PERMISSIONS
AND
SWAP
BACK
AGAIN
LATER
FOR
SET
USER
ID
OPERATIONS
WHEN
THE
SAVED
SET
USER
ID
FEATURE
WAS
INTRODUCED
WITH
POSIX
THE
RULE
WAS
ENHANCED
TO
ALSO
ALLOW
AN
UNPRIVILEGED
USER
TO
SET
ITS
EFFECTIVE
USER
ID
TO
ITS
SAVED
SET
USER
ID
BOTH
SETREUID
AND
SETREGID
ARE
INCLUDED
IN
THE
XSI
OPTION
IN
POSIX
AS
SUCH
ALL
UNIX
SYSTEM
IMPLEMENTATIONS
ARE
EXPECTED
TO
PROVIDE
SUPPORT
FOR
THEM
DIDN
T
HAVE
THE
SAVED
SET
USER
ID
FEATURE
DESCRIBED
EARLIER
IT
USED
SETREUID
AND
SETREGID
INSTEAD
THIS
ALLOWED
AN
UNPRIVILEGED
USER
TO
SWAP
BACK
AND
FORTH
BETWEEN
THE
TWO
VALUES
BE
AWARE
HOWEVER
THAT
WHEN
PROGRAMS
THAT
USED
THIS
FEATURE
SPAWNED
A
SHELL
THEY
HAD
TO
SET
THE
REAL
USER
ID
TO
THE
NORMAL
USER
ID
BEFORE
THE
EXEC
IF
THEY
DIDN
T
DO
THIS
THE
REAL
USER
ID
COULD
BE
PRIVILEGED
FROM
THE
SWAP
DONE
BY
SETREUID
AND
THE
SHELL
PROCESS
COULD
CALL
SETREUID
TO
SWAP
THE
TWO
AND
ASSUME
THE
PERMISSIONS
OF
THE
MORE
PRIVILEGED
USER
AS
A
DEFENSIVE
PROGRAMMING
MEASURE
TO
SOLVE
THIS
PROBLEM
PROGRAMS
SET
BOTH
THE
REAL
USER
ID
AND
THE
EFFECTIVE
USER
ID
TO
THE
NORMAL
USER
ID
BEFORE
THE
CALL
TO
EXEC
IN
THE
CHILD
SETEUID
AND
SETEGID
FUNCTIONS
POSIX
INCLUDES
THE
TWO
FUNCTIONS
SETEUID
AND
SETEGID
THESE
FUNCTIONS
ARE
SIMILAR
TO
SETUID
AND
SETGID
BUT
ONLY
THE
EFFECTIVE
USER
ID
OR
EFFECTIVE
GROUP
ID
IS
CHANGED
INCLUDE
UNISTD
H
INT
SETEUID
UID
INT
SETEGID
GID
BOTH
RETURN
IF
OK
ON
ERROR
AN
UNPRIVILEGED
USER
CAN
SET
ITS
EFFECTIVE
USER
ID
TO
EITHER
ITS
REAL
USER
ID
OR
ITS
SAVED
SET
USER
ID
FOR
A
PRIVILEGED
USER
ONLY
THE
EFFECTIVE
USER
ID
IS
SET
TO
UID
THIS
BEHAVIOR
DIFFERS
FROM
THAT
OF
THE
SETUID
FUNCTION
WHICH
CHANGES
ALL
THREE
USER
IDS
FIGURE
SUMMARIZES
ALL
THE
FUNCTIONS
THAT
WE
VE
DESCRIBED
HERE
THAT
MODIFY
THE
THREE
USER
IDS
SUPERUSER
SETREUID
RUID
EUID
SUPERUSER
SETUID
UID
SUPERUSER
SETEUID
UID
RUID
UNPRIVILEGED
SETREUID
UNPRIVILEGED
SETREUID
UNPRIVILEGED
SETUID
OR
SETEUID
UNPRIVILEGED
SETUID
OR
SETEUID
FIGURE
SUMMARY
OF
ALL
THE
FUNCTIONS
THAT
SET
THE
VARIOUS
USER
IDS
GROUP
IDS
EVERYTHING
THAT
WE
VE
SAID
SO
FAR
IN
THIS
SECTION
ALSO
APPLIES
IN
A
SIMILAR
FASHION
TO
GROUP
IDS
THE
SUPPLEMENTARY
GROUP
IDS
ARE
NOT
AFFECTED
BY
SETGID
SETREGID
OR
SETEGID
EXAMPLE
TO
SEE
THE
UTILITY
OF
THE
SAVED
SET
USER
ID
FEATURE
LET
EXAMINE
THE
OPERATION
OF
A
PROGRAM
THAT
USES
IT
WE
LL
LOOK
AT
THE
AT
PROGRAM
WHICH
WE
CAN
USE
TO
SCHEDULE
COMMANDS
TO
BE
RUN
AT
SOME
TIME
IN
THE
FUTURE
ON
LINUX
THE
AT
PROGRAM
IS
INSTALLED
SET
USER
ID
TO
USER
DAEMON
ON
FREEBSD
MAC
OS
X
AND
SOLARIS
THE
AT
PROGRAM
IS
INSTALLED
SET
USER
ID
TO
USER
ROOT
THIS
ALLOWS
THE
AT
COMMAND
TO
WRITE
PRIVILEGED
FILES
OWNED
BY
THE
DAEMON
THAT
WILL
RUN
THE
COMMANDS
ON
BEHALF
OF
THE
USER
RUNNING
THE
AT
COMMAND
ON
LINUX
THE
PROGRAMS
ARE
RUN
BY
THE
ATD
DAEMON
ON
FREEBSD
AND
SOLARIS
THE
PROGRAMS
ARE
RUN
BY
THE
CRON
DAEMON
ON
MAC
OS
X
THE
PROGRAMS
ARE
RUN
BY
THE
LAUNCHD
DAEMON
TO
PREVENT
BEING
TRICKED
INTO
RUNNING
COMMANDS
THAT
WE
AREN
T
ALLOWED
TO
RUN
OR
READING
OR
WRITING
FILES
THAT
WE
AREN
T
ALLOWED
TO
ACCESS
THE
AT
COMMAND
AND
THE
DAEMON
THAT
ULTIMATELY
RUNS
THE
COMMANDS
ON
OUR
BEHALF
HAVE
TO
SWITCH
BETWEEN
SETS
OF
PRIVILEGES
OURS
AND
THOSE
OF
THE
DAEMON
THE
FOLLOWING
STEPS
TAKE
PLACE
ASSUMING
THAT
THE
AT
PROGRAM
FILE
IS
OWNED
BY
ROOT
AND
HAS
ITS
SET
USER
ID
BIT
SET
WHEN
WE
RUN
IT
WE
HAVE
REAL
USER
ID
OUR
USER
ID
UNCHANGED
EFFECTIVE
USER
ID
ROOT
SAVED
SET
USER
ID
ROOT
THE
FIRST
THING
THE
AT
COMMAND
DOES
IS
REDUCE
ITS
PRIVILEGES
SO
THAT
IT
RUNS
WITH
OUR
PRIVILEGES
IT
CALLS
THE
SETEUID
FUNCTION
TO
SET
THE
EFFECTIVE
USER
ID
TO
OUR
REAL
USER
ID
AFTER
THIS
WE
HAVE
REAL
USER
ID
OUR
USER
ID
UNCHANGED
EFFECTIVE
USER
ID
OUR
USER
ID
SAVED
SET
USER
ID
ROOT
UNCHANGED
THE
AT
PROGRAM
RUNS
WITH
OUR
PRIVILEGES
UNTIL
IT
NEEDS
TO
ACCESS
THE
CONFIGURATION
FILES
THAT
CONTROL
WHICH
COMMANDS
ARE
TO
BE
RUN
AND
THE
TIME
AT
WHICH
THEY
NEED
TO
RUN
THESE
FILES
ARE
OWNED
BY
THE
DAEMON
THAT
WILL
RUN
THE
COMMANDS
FOR
US
THE
AT
COMMAND
CALLS
SETEUID
TO
SET
THE
EFFECTIVE
USER
ID
TO
ROOT
THIS
CALL
IS
ALLOWED
BECAUSE
THE
ARGUMENT
TO
SETEUID
EQUALS
THE
SAVED
SET
USER
ID
THIS
IS
WHY
WE
NEED
THE
SAVED
SET
USER
ID
AFTER
THIS
WE
HAVE
REAL
USER
ID
OUR
USER
ID
UNCHANGED
EFFECTIVE
USER
ID
ROOT
SAVED
SET
USER
ID
ROOT
UNCHANGED
BECAUSE
THE
EFFECTIVE
USER
ID
IS
ROOT
FILE
ACCESS
IS
ALLOWED
AFTER
THE
FILES
ARE
MODIFIED
TO
RECORD
THE
COMMANDS
TO
BE
RUN
AND
THE
TIME
AT
WHICH
THEY
ARE
TO
BE
RUN
THE
AT
COMMAND
LOWERS
ITS
PRIVILEGES
BY
CALLING
SETEUID
TO
SET
ITS
EFFECTIVE
USER
ID
TO
OUR
USER
ID
THIS
PREVENTS
ANY
ACCIDENTAL
MISUSE
OF
PRIVILEGE
AT
THIS
POINT
WE
HAVE
REAL
USER
ID
OUR
USER
ID
UNCHANGED
EFFECTIVE
USER
ID
OUR
USER
ID
SAVED
SET
USER
ID
ROOT
UNCHANGED
THE
DAEMON
STARTS
OUT
RUNNING
WITH
ROOT
PRIVILEGES
TO
RUN
COMMANDS
ON
OUR
BEHALF
THE
DAEMON
CALLS
FORK
AND
THE
CHILD
CALLS
SETUID
TO
CHANGE
ITS
USER
ID
TO
OUR
USER
ID
BECAUSE
THE
CHILD
IS
RUNNING
WITH
ROOT
PRIVILEGES
THIS
CHANGES
ALL
OF
THE
IDS
WE
HAVE
REAL
USER
ID
OUR
USER
ID
EFFECTIVE
USER
ID
OUR
USER
ID
SAVED
SET
USER
ID
OUR
USER
ID
NOW
THE
DAEMON
CAN
SAFELY
EXECUTE
COMMANDS
ON
OUR
BEHALF
BECAUSE
IT
CAN
ACCESS
ONLY
THE
FILES
TO
WHICH
WE
NORMALLY
HAVE
ACCESS
WE
HAVE
NO
ADDITIONAL
PERMISSIONS
BY
USING
THE
SAVED
SET
USER
ID
IN
THIS
FASHION
WE
CAN
USE
THE
EXTRA
PRIVILEGES
GRANTED
TO
US
BY
THE
SET
USER
ID
OF
THE
PROGRAM
FILE
ONLY
WHEN
WE
NEED
ELEVATED
PRIVILEGES
ANY
OTHER
TIME
HOWEVER
THE
PROCESS
RUNS
WITH
OUR
NORMAL
PERMISSIONS
IF
WE
WEREN
T
ABLE
TO
SWITCH
BACK
TO
THE
SAVED
SET
USER
ID
AT
THE
END
WE
MIGHT
BE
TEMPTED
TO
RETAIN
THE
EXTRA
PERMISSIONS
THE
WHOLE
TIME
WE
WERE
RUNNING
WHICH
IS
ASKING
FOR
TROUBLE
INTERPRETER
FILES
ALL
CONTEMPORARY
UNIX
SYSTEMS
SUPPORT
INTERPRETER
FILES
THESE
FILES
ARE
TEXT
FILES
THAT
BEGIN
WITH
A
LINE
OF
THE
FORM
PATHNAME
OPTIONAL
ARGUMENT
THE
SPACE
BETWEEN
THE
EXCLAMATION
POINT
AND
THE
PATHNAME
IS
OPTIONAL
THE
MOST
COMMON
OF
THESE
INTERPRETER
FILES
BEGIN
WITH
THE
LINE
BIN
SH
THE
PATHNAME
IS
NORMALLY
AN
ABSOLUTE
PATHNAME
SINCE
NO
SPECIAL
OPERATIONS
ARE
PERFORMED
ON
IT
I
E
PATH
IS
NOT
USED
THE
RECOGNITION
OF
THESE
FILES
IS
DONE
WITHIN
THE
KERNEL
AS
PART
OF
PROCESSING
THE
EXEC
SYSTEM
CALL
THE
ACTUAL
FILE
THAT
GETS
EXECUTED
BY
THE
KERNEL
IS
NOT
THE
INTERPRETER
FILE
BUT
RATHER
THE
FILE
SPECIFIED
BY
THE
PATHNAME
ON
THE
FIRST
LINE
OF
THE
INTERPRETER
FILE
BE
SURE
TO
DIFFERENTIATE
BETWEEN
THE
INTERPRETER
FILE
A
TEXT
FILE
THAT
BEGINS
WITH
AND
THE
INTERPRETER
WHICH
IS
SPECIFIED
BY
THE
PATHNAME
ON
THE
FIRST
LINE
OF
THE
INTERPRETER
FILE
BE
AWARE
THAT
SYSTEMS
PLACE
A
SIZE
LIMIT
ON
THE
FIRST
LINE
OF
AN
INTERPRETER
FILE
THIS
LIMIT
INCLUDES
THE
THE
PATHNAME
THE
OPTIONAL
ARGUMENT
THE
TERMINATING
NEWLINE
AND
ANY
SPACES
ON
FREEBSD
THIS
LIMIT
IS
BYTES
ON
LINUX
THE
LIMIT
IS
BYTES
MAC
OS
X
SUPPORTS
A
LIMIT
OF
BYTES
WHEREAS
SOLARIS
PLACES
THE
LIMIT
AT
BYTES
EXAMPLE
LET
LOOK
AT
AN
EXAMPLE
TO
SEE
WHAT
THE
KERNEL
DOES
WITH
THE
ARGUMENTS
TO
THE
EXEC
FUNCTION
WHEN
THE
FILE
BEING
EXECUTED
IS
AN
INTERPRETER
FILE
AND
THE
OPTIONAL
ARGUMENT
ON
THE
FIRST
LINE
OF
THE
INTERPRETER
FILE
THE
PROGRAM
IN
FIGURE
EXECS
AN
INTERPRETER
FILE
INCLUDE
APUE
H
INCLUDE
SYS
WAIT
H
INT
MAIN
VOID
PID
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
CHILD
IF
EXECL
HOME
SAR
BIN
TESTINTERP
TESTINTERP
MY
CHAR
EXECL
ERROR
IF
WAITPID
PID
NULL
PARENT
WAITPID
ERROR
EXIT
FIGURE
A
PROGRAM
THAT
EXECS
AN
INTERPRETER
FILE
THE
FOLLOWING
SHOWS
THE
CONTENTS
OF
THE
ONE
LINE
INTERPRETER
FILE
THAT
IS
EXECUTED
AND
THE
RESULT
FROM
RUNNING
THE
PROGRAM
IN
FIGURE
CAT
HOME
SAR
BIN
TESTINTERP
HOME
SAR
BIN
ECHOARG
FOO
A
OUT
ARGV
HOME
SAR
BIN
ECHOARG
ARGV
FOO
ARGV
HOME
SAR
BIN
TESTINTERP
ARGV
ARGV
MY
THE
PROGRAM
ECHOARG
THE
INTERPRETER
JUST
ECHOES
EACH
OF
ITS
COMMAND
LINE
ARGUMENTS
THIS
IS
THE
PROGRAM
FROM
FIGURE
NOTE
THAT
WHEN
THE
KERNEL
EXECS
THE
INTERPRETER
HOME
SAR
BIN
ECHOARG
ARGV
IS
THE
PATHNAME
OF
THE
INTERPRETER
ARGV
IS
THE
OPTIONAL
ARGUMENT
FROM
THE
INTERPRETER
FILE
AND
THE
REMAINING
ARGUMENTS
ARE
THE
PATHNAME
HOME
SAR
BIN
TESTINTERP
AND
THE
SECOND
AND
THIRD
ARGUMENTS
FROM
THE
CALL
TO
EXECL
IN
THE
PROGRAM
SHOWN
IN
FIGURE
AND
MY
BOTH
ARGV
AND
ARGV
FROM
THE
CALL
TO
EXECL
HAVE
BEEN
SHIFTED
RIGHT
TWO
POSITIONS
NOTE
THAT
THE
KERNEL
TAKES
THE
PATHNAME
FROM
THE
EXECL
CALL
INSTEAD
OF
THE
FIRST
ARGUMENT
TESTINTERP
ON
THE
ASSUMPTION
THAT
THE
PATHNAME
MIGHT
CONTAIN
MORE
INFORMATION
THAN
THE
FIRST
ARGUMENT
EXAMPLE
A
COMMON
USE
FOR
THE
OPTIONAL
ARGUMENT
FOLLOWING
THE
INTERPRETER
PATHNAME
IS
TO
SPECIFY
THE
F
OPTION
FOR
PROGRAMS
THAT
SUPPORT
THIS
OPTION
FOR
EXAMPLE
AN
AWK
PROGRAM
CAN
BE
EXECUTED
AS
AWK
F
MYFILE
WHICH
TELLS
AWK
TO
READ
THE
AWK
PROGRAM
FROM
THE
FILE
MYFILE
SYSTEMS
DERIVED
FROM
UNIX
SYSTEM
V
OFTEN
INCLUDE
TWO
VERSIONS
OF
THE
AWK
LANGUAGE
ON
THESE
SYSTEMS
AWK
IS
OFTEN
CALLED
OLD
AWK
AND
CORRESPONDS
TO
THE
ORIGINAL
VERSION
DISTRIBUTED
WITH
VERSION
IN
CONTRAST
NAWK
NEW
AWK
CONTAINS
NUMEROUS
ENHANCEMENTS
AND
CORRESPONDS
TO
THE
LANGUAGE
DESCRIBED
IN
AHO
KERNIGHAN
AND
WEINBERGER
THIS
NEWER
VERSION
PROVIDES
ACCESS
TO
THE
COMMAND
LINE
ARGUMENTS
WHICH
WE
NEED
FOR
THE
EXAMPLE
THAT
FOLLOWS
SOLARIS
PROVIDES
BOTH
VERSIONS
THE
AWK
PROGRAM
IS
ONE
OF
THE
UTILITIES
INCLUDED
BY
POSIX
IN
ITS
STANDARD
WHICH
IS
NOW
PART
OF
THE
BASE
POSIX
SPECIFICATION
IN
THE
SINGLE
UNIX
SPECIFICATION
THIS
UTILITY
IS
ALSO
BASED
ON
THE
LANGUAGE
DESCRIBED
IN
AHO
KERNIGHAN
AND
WEINBERGER
THE
VERSION
OF
AWK
IN
MAC
OS
X
IS
BASED
ON
THE
BELL
LABORATORIES
VERSION
WHICH
HAS
BEEN
PLACED
IN
THE
PUBLIC
DOMAIN
FREEBSD
AND
SOME
LINUX
DISTRIBUTIONS
SHIP
WITH
GNU
AWK
CALLED
GAWK
WHICH
IS
LINKED
TO
THE
NAME
AWK
GAWK
CONFORMS
TO
THE
POSIX
STANDARD
BUT
ALSO
INCLUDES
OTHER
EXTENSIONS
BECAUSE
THEY
ARE
MORE
UP
TO
DATE
GAWK
AND
THE
VERSION
OF
AWK
FROM
BELL
LABORATORIES
ARE
PREFERRED
TO
EITHER
NAWK
OR
OLD
AWK
THE
BELL
LABS
VERSION
OF
AWK
IS
AVAILABLE
AT
HTTP
CM
BELL
LABS
COM
CM
CS
AWKBOOK
INDEX
HTML
USING
THE
F
OPTION
WITH
AN
INTERPRETER
FILE
LETS
US
WRITE
BIN
AWK
F
AWK
PROGRAM
FOLLOWS
IN
THE
INTERPRETER
FILE
FOR
EXAMPLE
FIGURE
SHOWS
USR
LOCAL
BIN
AWKEXAMPLE
AN
INTERPRETER
FILE
USR
BIN
AWK
F
NOTE
ON
SOLARIS
USE
NAWK
INSTEAD
BEGIN
FOR
I
I
ARGC
I
PRINTF
ARGV
D
N
I
ARGV
I
EXIT
FIGURE
AN
AWK
PROGRAM
AS
AN
INTERPRETER
FILE
IF
ONE
OF
THE
PATH
PREFIXES
IS
USR
LOCAL
BIN
WE
CAN
EXECUTE
THE
PROGRAM
IN
FIGURE
ASSUMING
THAT
WE
VE
TURNED
ON
THE
EXECUTE
BIT
FOR
THE
FILE
AS
AWKEXAMPLE
ARGV
AWK
ARGV
ARGV
ARGV
WHEN
BIN
AWK
IS
EXECUTED
ITS
COMMAND
LINE
ARGUMENTS
ARE
BIN
AWK
F
USR
LOCAL
BIN
AWKEXAMPLE
THE
PATHNAME
OF
THE
INTERPRETER
FILE
USR
LOCAL
BIN
AWKEXAMPLE
IS
PASSED
TO
THE
INTERPRETER
THE
FILENAME
PORTION
OF
THIS
PATHNAME
WHAT
WE
TYPED
TO
THE
SHELL
ISN
T
ADEQUATE
BECAUSE
THE
INTERPRETER
BIN
AWK
IN
THIS
EXAMPLE
CAN
T
BE
EXPECTED
TO
USE
THE
PATH
VARIABLE
TO
LOCATE
FILES
WHEN
IT
READS
THE
INTERPRETER
FILE
AWK
IGNORES
THE
FIRST
LINE
SINCE
THE
POUND
SIGN
IS
AWK
COMMENT
CHARACTER
WE
CAN
VERIFY
THESE
COMMAND
LINE
ARGUMENTS
WITH
THE
FOLLOWING
COMMANDS
BIN
SU
BECOME
SUPERUSER
PASSWORD
ENTER
SUPERUSER
PASSWORD
MV
USR
BIN
AWK
USR
BIN
AWK
SAVE
SAVE
THE
ORIGINAL
PROGRAM
CP
HOME
SAR
BIN
ECHOARG
USR
BIN
AWK
AND
REPLACE
IT
TEMPORARILY
SUSPEND
SUSPEND
THE
SUPERUSER
SHELL
STOPPED
BIN
SU
USING
JOB
CONTROL
AWKEXAMPLE
ARGV
BIN
AWK
ARGV
F
ARGV
USR
LOCAL
BIN
AWKEXAMPLE
ARGV
ARGV
ARGV
FG
RESUME
SUPERUSER
SHELL
USING
JOB
CONTROL
BIN
SU
MV
USR
BIN
AWK
SAVE
USR
BIN
AWK
RESTORE
THE
ORIGINAL
PROGRAM
EXIT
AND
EXIT
THE
SUPERUSER
SHELL
IN
THIS
EXAMPLE
THE
F
OPTION
FOR
THE
INTERPRETER
IS
REQUIRED
AS
WE
SAID
THIS
TELLS
AWK
WHERE
TO
LOOK
FOR
THE
AWK
PROGRAM
IF
WE
REMOVE
THE
F
OPTION
FROM
THE
INTERPRETER
FILE
AN
ERROR
MESSAGE
USUALLY
RESULTS
WHEN
WE
TRY
TO
RUN
IT
THE
EXACT
TEXT
OF
THE
MESSAGE
VARIES
DEPENDING
ON
WHERE
THE
INTERPRETER
FILE
IS
STORED
AND
WHETHER
THE
REMAINING
ARGUMENTS
REPRESENT
EXISTING
FILES
THIS
IS
BECAUSE
THE
COMMAND
LINE
ARGUMENTS
IN
THIS
CASE
ARE
BIN
AWK
USR
LOCAL
BIN
AWKEXAMPLE
AND
AWK
IS
TRYING
TO
INTERPRET
THE
STRING
USR
LOCAL
BIN
AWKEXAMPLE
AS
AN
AWK
PROGRAM
IF
WE
COULDN
T
PASS
AT
LEAST
A
SINGLE
OPTIONAL
ARGUMENT
TO
THE
INTERPRETER
F
IN
THIS
CASE
THESE
INTERPRETER
FILES
WOULD
BE
USABLE
ONLY
WITH
THE
SHELLS
ARE
INTERPRETER
FILES
REQUIRED
NOT
REALLY
THEY
PROVIDE
AN
EFFICIENCY
GAIN
FOR
THE
USER
AT
SOME
EXPENSE
IN
THE
KERNEL
SINCE
IT
THE
KERNEL
THAT
RECOGNIZES
THESE
FILES
INTERPRETER
FILES
ARE
USEFUL
FOR
THE
FOLLOWING
REASONS
THEY
HIDE
THAT
CERTAIN
PROGRAMS
ARE
SCRIPTS
IN
SOME
OTHER
LANGUAGE
FOR
EXAMPLE
TO
EXECUTE
THE
PROGRAM
IN
FIGURE
WE
JUST
SAY
AWKEXAMPLE
OPTIONAL
ARGUMENTS
INSTEAD
OF
NEEDING
TO
KNOW
THAT
THE
PROGRAM
IS
REALLY
AN
AWK
SCRIPT
THAT
WE
WOULD
OTHERWISE
HAVE
TO
EXECUTE
AS
AWK
F
AWKEXAMPLE
OPTIONAL
ARGUMENTS
INTERPRETER
SCRIPTS
PROVIDE
AN
EFFICIENCY
GAIN
CONSIDER
THE
PREVIOUS
EXAMPLE
AGAIN
WE
COULD
STILL
HIDE
THAT
THE
PROGRAM
IS
AN
AWK
SCRIPT
BY
WRAPPING
IT
IN
A
SHELL
SCRIPT
AWK
BEGIN
FOR
I
I
ARGC
I
PRINTF
ARGV
D
N
I
ARGV
I
EXIT
THE
PROBLEM
WITH
THIS
SOLUTION
IS
THAT
MORE
WORK
IS
REQUIRED
FIRST
THE
SHELL
READS
THE
COMMAND
AND
TRIES
TO
EXECLP
THE
FILENAME
BECAUSE
THE
SHELL
SCRIPT
IS
AN
EXECUTABLE
FILE
BUT
ISN
T
A
MACHINE
EXECUTABLE
AN
ERROR
IS
RETURNED
AND
EXECLP
ASSUMES
THAT
THE
FILE
IS
A
SHELL
SCRIPT
WHICH
IT
IS
THEN
BIN
SH
IS
EXECUTED
WITH
THE
PATHNAME
OF
THE
SHELL
SCRIPT
AS
ITS
ARGUMENT
THE
SHELL
CORRECTLY
RUNS
OUR
SCRIPT
BUT
TO
RUN
THE
AWK
PROGRAM
THE
SHELL
DOES
A
FORK
EXEC
AND
WAIT
THUS
THERE
IS
MORE
OVERHEAD
INVOLVED
IN
REPLACING
AN
INTERPRETER
SCRIPT
WITH
A
SHELL
SCRIPT
INTERPRETER
SCRIPTS
LET
US
WRITE
SHELL
SCRIPTS
USING
SHELLS
OTHER
THAN
BIN
SH
WHEN
IT
FINDS
AN
EXECUTABLE
FILE
THAT
ISN
T
A
MACHINE
EXECUTABLE
EXECLP
HAS
TO
CHOOSE
A
SHELL
TO
INVOKE
AND
IT
ALWAYS
USES
BIN
SH
USING
AN
INTERPRETER
SCRIPT
HOWEVER
WE
CAN
SIMPLY
WRITE
BIN
CSH
C
SHELL
SCRIPT
FOLLOWS
IN
THE
INTERPRETER
FILE
AGAIN
WE
COULD
WRAP
ALL
OF
THIS
IN
A
BIN
SH
SCRIPT
THAT
INVOKES
THE
C
SHELL
AS
WE
DESCRIBED
EARLIER
BUT
MORE
OVERHEAD
IS
REQUIRED
NONE
OF
THIS
WOULD
WORK
AS
WE
VE
SHOWN
HERE
IF
THE
THREE
SHELLS
AND
AWK
DIDN
T
USE
THE
POUND
SIGN
AS
THEIR
COMMENT
CHARACTER
SYSTEM
FUNCTION
IT
IS
CONVENIENT
TO
EXECUTE
A
COMMAND
STRING
FROM
WITHIN
A
PROGRAM
FOR
EXAMPLE
ASSUME
THAT
WE
WANT
TO
PUT
A
TIME
AND
DATE
STAMP
INTO
A
CERTAIN
FILE
WE
COULD
USE
THE
FUNCTIONS
DESCRIBED
IN
SECTION
TO
DO
THIS
CALL
TIME
TO
GET
THE
CURRENT
CALENDAR
TIME
THEN
CALL
LOCALTIME
TO
CONVERT
IT
TO
A
BROKEN
DOWN
TIME
THEN
CALL
STRFTIME
TO
FORMAT
THE
RESULT
AND
FINALLY
WRITE
THE
RESULT
TO
THE
FILE
IT
IS
MUCH
EASIER
HOWEVER
TO
SAY
SYSTEM
DATE
FILE
ISO
C
DEFINES
THE
SYSTEM
FUNCTION
BUT
ITS
OPERATION
IS
STRONGLY
SYSTEM
DEPENDENT
POSIX
INCLUDES
THE
SYSTEM
INTERFACE
EXPANDING
ON
THE
ISO
C
DEFINITION
TO
DESCRIBE
ITS
BEHAVIOR
IN
A
POSIX
ENVIRONMENT
IF
CMDSTRING
IS
A
NULL
POINTER
SYSTEM
RETURNS
NONZERO
ONLY
IF
A
COMMAND
PROCESSOR
IS
AVAILABLE
THIS
FEATURE
DETERMINES
WHETHER
THE
SYSTEM
FUNCTION
IS
SUPPORTED
ON
A
GIVEN
OPERATING
SYSTEM
UNDER
THE
UNIX
SYSTEM
SYSTEM
IS
ALWAYS
AVAILABLE
BECAUSE
SYSTEM
IS
IMPLEMENTED
BY
CALLING
FORK
EXEC
AND
WAITPID
THERE
ARE
THREE
TYPES
OF
RETURN
VALUES
IF
EITHER
THE
FORK
FAILS
OR
WAITPID
RETURNS
AN
ERROR
OTHER
THAN
EINTR
SYSTEM
RETURNS
WITH
ERRNO
SET
TO
INDICATE
THE
ERROR
IF
THE
EXEC
FAILS
IMPLYING
THAT
THE
SHELL
CAN
T
BE
EXECUTED
THE
RETURN
VALUE
IS
AS
IF
THE
SHELL
HAD
EXECUTED
EXIT
OTHERWISE
ALL
THREE
FUNCTIONS
FORK
EXEC
AND
WAITPID
SUCCEED
AND
THE
RETURN
VALUE
FROM
SYSTEM
IS
THE
TERMINATION
STATUS
OF
THE
SHELL
IN
THE
FORMAT
SPECIFIED
FOR
WAITPID
SOME
OLDER
IMPLEMENTATIONS
OF
SYSTEM
RETURNED
AN
ERROR
EINTR
IF
WAITPID
WAS
INTERRUPTED
BY
A
CAUGHT
SIGNAL
BECAUSE
THERE
IS
NO
STRATEGY
THAT
AN
APPLICATION
CAN
USE
TO
RECOVER
FROM
THIS
TYPE
OF
ERROR
THE
PROCESS
ID
OF
THE
CHILD
IS
HIDDEN
FROM
THE
CALLER
POSIX
LATER
ADDED
THE
REQUIREMENT
THAT
SYSTEM
NOT
RETURN
AN
ERROR
IN
THIS
CASE
WE
DISCUSS
INTERRUPTED
SYSTEM
CALLS
IN
SECTION
FIGURE
SHOWS
AN
IMPLEMENTATION
OF
THE
SYSTEM
FUNCTION
THE
ONE
FEATURE
THAT
IT
DOESN
T
HANDLE
IS
SIGNALS
WE
LL
UPDATE
THIS
FUNCTION
WITH
SIGNAL
HANDLING
IN
SECTION
THE
SHELL
C
OPTION
TELLS
IT
TO
TAKE
THE
NEXT
COMMAND
LINE
ARGUMENT
CMDSTRING
IN
THIS
CASE
AS
ITS
COMMAND
INPUT
INSTEAD
OF
READING
FROM
STANDARD
INPUT
OR
FROM
A
GIVEN
FILE
THE
SHELL
PARSES
THIS
NULL
TERMINATED
C
STRING
AND
BREAKS
IT
UP
INTO
SEPARATE
COMMAND
LINE
ARGUMENTS
FOR
THE
COMMAND
THE
ACTUAL
COMMAND
STRING
THAT
IS
PASSED
TO
THE
SHELL
CAN
CONTAIN
ANY
VALID
SHELL
COMMANDS
FOR
EXAMPLE
INPUT
AND
OUTPUT
REDIRECTION
USING
AND
CAN
BE
USED
IF
WE
DIDN
T
USE
THE
SHELL
TO
EXECUTE
THE
COMMAND
BUT
TRIED
TO
EXECUTE
THE
COMMAND
OURSELF
IT
WOULD
BE
MORE
DIFFICULT
FIRST
WE
WOULD
WANT
TO
CALL
EXECLP
INSTEAD
OF
EXECL
TO
USE
THE
PATH
VARIABLE
LIKE
THE
SHELL
WE
WOULD
ALSO
HAVE
TO
BREAK
UP
THE
NULL
TERMINATED
C
STRING
INTO
SEPARATE
COMMAND
LINE
ARGUMENTS
FOR
THE
CALL
TO
EXECLP
FINALLY
WE
WOULDN
T
BE
ABLE
TO
USE
ANY
OF
THE
SHELL
METACHARACTERS
NOTE
THAT
WE
CALL
INSTEAD
OF
EXIT
WE
DO
THIS
TO
PREVENT
ANY
STANDARD
I
O
BUFFERS
WHICH
WOULD
HAVE
BEEN
COPIED
FROM
THE
PARENT
TO
THE
CHILD
ACROSS
THE
FORK
FROM
BEING
FLUSHED
IN
THE
CHILD
WE
CAN
TEST
THIS
VERSION
OF
SYSTEM
WITH
THE
PROGRAM
SHOWN
IN
FIGURE
THE
FUNCTION
WAS
DEFINED
IN
FIGURE
RUNNING
THE
PROGRAM
IN
FIGURE
GIVES
US
A
OUT
SAT
FEB
EST
NORMAL
TERMINATION
EXIT
STATUS
FOR
DATE
SH
NOSUCHCOMMAND
COMMAND
NOT
FOUND
NORMAL
TERMINATION
EXIT
STATUS
FOR
NOSUCHCOMMAND
SAR
CONSOLE
JAN
SAR
FEB
SAR
JAN
SAR
JAN
SAR
JAN
NORMAL
TERMINATION
EXIT
STATUS
FOR
EXIT
THE
ADVANTAGE
IN
USING
SYSTEM
INSTEAD
OF
USING
FORK
AND
EXEC
DIRECTLY
IS
THAT
SYSTEM
DOES
ALL
THE
REQUIRED
ERROR
HANDLING
AND
IN
OUR
NEXT
VERSION
OF
THIS
FUNCTION
IN
SECTION
ALL
THE
REQUIRED
SIGNAL
HANDLING
EARLIER
SYSTEMS
INCLUDING
AND
DIDN
T
HAVE
THE
WAITPID
FUNCTION
AVAILABLE
INSTEAD
THE
PARENT
WAITED
FOR
THE
CHILD
USING
A
STATEMENT
SUCH
AS
WHILE
LASTPID
WAIT
STATUS
PID
LASTPID
A
PROBLEM
OCCURS
IF
THE
PROCESS
THAT
CALLS
SYSTEM
HAS
SPAWNED
ITS
OWN
CHILDREN
BEFORE
CALLING
SYSTEM
BECAUSE
THE
WHILE
STATEMENT
ABOVE
KEEPS
LOOPING
UNTIL
THE
CHILD
THAT
WAS
GENERATED
BY
SYSTEM
TERMINATES
IF
ANY
CHILDREN
OF
THE
PROCESS
TERMINATE
BEFORE
THE
PROCESS
IDENTIFIED
BY
PID
THEN
THE
PROCESS
ID
AND
TERMINATION
STATUS
OF
THESE
OTHER
CHILDREN
ARE
DISCARDED
BY
THE
WHILE
STATEMENT
INDEED
THIS
INABILITY
TO
WAIT
FOR
A
SPECIFIC
CHILD
IS
ONE
OF
THE
REASONS
GIVEN
IN
THE
POSIX
RATIONALE
FOR
INCLUDING
THE
WAITPID
FUNCTION
WE
LL
SEE
IN
SECTION
THAT
THE
SAME
PROBLEM
OCCURS
WITH
THE
POPEN
AND
PCLOSE
FUNCTIONS
IF
THE
SYSTEM
DOESN
T
PROVIDE
A
WAITPID
FUNCTION
SET
USER
ID
PROGRAMS
WHAT
HAPPENS
IF
WE
CALL
SYSTEM
FROM
A
SET
USER
ID
PROGRAM
DOING
SO
CREATES
A
SECURITY
HOLE
AND
SHOULD
NEVER
BE
ATTEMPTED
FIGURE
SHOWS
A
SIMPLE
PROGRAM
THAT
JUST
CALLS
SYSTEM
FOR
ITS
COMMAND
LINE
ARGUMENT
WE
LL
COMPILE
THIS
PROGRAM
INTO
THE
EXECUTABLE
FILE
PRINTUIDS
RUNNING
BOTH
PROGRAMS
GIVES
US
THE
FOLLOWING
TSYS
PRINTUIDS
NORMAL
EXECUTION
NO
SPECIAL
PRIVILEGES
REAL
UID
EFFECTIVE
UID
NORMAL
TERMINATION
EXIT
STATUS
SU
BECOME
SUPERUSER
PASSWORD
ENTER
SUPERUSER
PASSWORD
CHOWN
ROOT
TSYS
CHANGE
OWNER
CHMOD
U
TSYS
MAKE
SET
USER
ID
LS
L
TSYS
VERIFY
FILE
PERMISSIONS
AND
OWNER
RWSRWXR
X
ROOT
FEB
TSYS
EXIT
LEAVE
SUPERUSER
SHELL
TSYS
PRINTUIDS
REAL
UID
EFFECTIVE
UID
OOPS
THIS
IS
A
SECURITY
HOLE
NORMAL
TERMINATION
EXIT
STATUS
THE
SUPERUSER
PERMISSIONS
THAT
WE
GAVE
THE
TSYS
PROGRAM
ARE
RETAINED
ACROSS
THE
FORK
AND
EXEC
THAT
ARE
DONE
BY
SYSTEM
SOME
IMPLEMENTATIONS
HAVE
CLOSED
THIS
SECURITY
HOLE
BY
CHANGING
BIN
SH
TO
RESET
THE
EFFECTIVE
USER
ID
TO
THE
REAL
USER
ID
WHEN
THEY
DON
T
MATCH
ON
THESE
SYSTEMS
THE
PREVIOUS
EXAMPLE
DOESN
T
WORK
AS
SHOWN
INSTEAD
THE
SAME
EFFECTIVE
USER
ID
WILL
BE
PRINTED
REGARDLESS
OF
THE
STATUS
OF
THE
SET
USER
ID
BIT
ON
THE
PROGRAM
CALLING
SYSTEM
IF
IT
IS
RUNNING
WITH
SPECIAL
PERMISSIONS
EITHER
SET
USER
ID
OR
SET
GROUP
ID
AND
WANTS
TO
SPAWN
ANOTHER
PROCESS
A
PROCESS
SHOULD
USE
FORK
AND
EXEC
DIRECTLY
BEING
CERTAIN
TO
CHANGE
BACK
TO
NORMAL
PERMISSIONS
AFTER
THE
FORK
BEFORE
CALLING
EXEC
THE
SYSTEM
FUNCTION
SHOULD
NEVER
BE
USED
FROM
A
SET
USER
ID
OR
A
SET
GROUP
ID
PROGRAM
ONE
REASON
FOR
THIS
ADMONITION
IS
THAT
SYSTEM
INVOKES
THE
SHELL
TO
PARSE
THE
COMMAND
STRING
AND
THE
SHELL
USES
ITS
IFS
VARIABLE
AS
THE
INPUT
FIELD
SEPARATOR
OLDER
VERSIONS
OF
THE
SHELL
DIDN
T
RESET
THIS
VARIABLE
TO
A
NORMAL
SET
OF
CHARACTERS
WHEN
INVOKED
AS
A
RESULT
A
MALICIOUS
USER
COULD
SET
IFS
BEFORE
SYSTEM
WAS
CALLED
CAUSING
SYSTEM
TO
EXECUTE
A
DIFFERENT
PROGRAM
PROCESS
ACCOUNTING
MOST
UNIX
SYSTEMS
PROVIDE
AN
OPTION
TO
DO
PROCESS
ACCOUNTING
WHEN
ENABLED
THE
KERNEL
WRITES
AN
ACCOUNTING
RECORD
EACH
TIME
A
PROCESS
TERMINATES
THESE
ACCOUNTING
RECORDS
TYPICALLY
CONTAIN
A
SMALL
AMOUNT
OF
BINARY
DATA
WITH
THE
NAME
OF
THE
COMMAND
THE
AMOUNT
OF
CPU
TIME
USED
THE
USER
ID
AND
GROUP
ID
THE
STARTING
TIME
AND
SO
ON
WE
LL
TAKE
A
CLOSER
LOOK
AT
THESE
ACCOUNTING
RECORDS
IN
THIS
SECTION
AS
IT
GIVES
US
A
CHANCE
TO
LOOK
AT
PROCESSES
AGAIN
AND
TO
USE
THE
FREAD
FUNCTION
FROM
SECTION
PROCESS
ACCOUNTING
IS
NOT
SPECIFIED
BY
ANY
OF
THE
STANDARDS
THUS
ALL
THE
IMPLEMENTATIONS
HAVE
ANNOYING
DIFFERENCES
FOR
EXAMPLE
THE
I
O
COUNTS
MAINTAINED
ON
SOLARIS
ARE
IN
UNITS
OF
BYTES
WHEREAS
FREEBSD
AND
MAC
OS
X
MAINTAIN
UNITS
OF
BLOCKS
ALTHOUGH
THERE
IS
NO
DISTINCTION
BETWEEN
DIFFERENT
BLOCK
SIZES
MAKING
THE
COUNTER
EFFECTIVELY
USELESS
LINUX
ON
THE
OTHER
HAND
DOESN
T
TRY
TO
MAINTAIN
I
O
STATISTICS
AT
ALL
EACH
IMPLEMENTATION
ALSO
HAS
ITS
OWN
SET
OF
ADMINISTRATIVE
COMMANDS
TO
PROCESS
RAW
ACCOUNTING
DATA
FOR
EXAMPLE
SOLARIS
PROVIDES
RUNACCT
AND
ACCTCOM
WHEREAS
FREEBSD
PROVIDES
THE
SA
COMMAND
TO
PROCESS
AND
SUMMARIZE
THE
RAW
ACCOUNTING
DATA
A
FUNCTION
WE
HAVEN
T
DESCRIBED
ACCT
ENABLES
AND
DISABLES
PROCESS
ACCOUNTING
THE
ONLY
USE
OF
THIS
FUNCTION
IS
FROM
THE
ACCTON
COMMAND
WHICH
HAPPENS
TO
BE
ONE
OF
THE
FEW
SIMILARITIES
AMONG
PLATFORMS
A
SUPERUSER
EXECUTES
ACCTON
WITH
A
PATHNAME
ARGUMENT
TO
ENABLE
ACCOUNTING
THE
ACCOUNTING
RECORDS
ARE
WRITTEN
TO
THE
SPECIFIED
FILE
WHICH
IS
USUALLY
VAR
ACCOUNT
ACCT
ON
FREEBSD
AND
MAC
OS
X
VAR
LOG
ACCOUNT
PACCT
ON
LINUX
AND
VAR
ADM
PACCT
ON
SOLARIS
ACCOUNTING
IS
TURNED
OFF
BY
EXECUTING
ACCTON
WITHOUT
ANY
ARGUMENTS
THE
STRUCTURE
OF
THE
ACCOUNTING
RECORDS
IS
DEFINED
IN
THE
HEADER
SYS
ACCT
H
ALTHOUGH
THE
IMPLEMENTATION
OF
EACH
SYSTEM
DIFFERS
THE
ACCOUNTING
RECORDS
LOOK
SOMETHING
LIKE
TYPEDEF
BIT
BASE
EXPONENT
BIT
FRACTION
STRUCT
ACCT
CHAR
FLAG
SEE
FIGURE
CHAR
TERMINATION
STATUS
SIGNAL
CORE
FLAG
ONLY
SOLARIS
ONLY
REAL
USER
ID
REAL
GROUP
ID
CONTROLLING
TERMINAL
STARTING
CALENDAR
TIME
USER
CPU
TIME
SYSTEM
CPU
TIME
ELAPSED
TIME
AVERAGE
MEMORY
USAGE
BYTES
TRANSFERRED
BY
READ
AND
WRITE
BLOCKS
ON
BSD
SYSTEMS
BLOCKS
READ
OR
WRITTEN
NOT
PRESENT
ON
BSD
SYSTEMS
CHAR
COMMAND
NAME
FOR
SOLARIS
FOR
MAC
OS
X
FOR
FREEBSD
AND
FOR
LINUX
TIMES
ARE
RECORDED
IN
UNITS
OF
CLOCK
TICKS
ON
MOST
PLATFORMS
BUT
FREEBSD
STORES
MICROSECONDS
INSTEAD
THE
MEMBER
RECORDS
CERTAIN
EVENTS
DURING
THE
EXECUTION
OF
THE
PROCESS
THESE
EVENTS
ARE
DESCRIBED
IN
FIGURE
THE
DATA
REQUIRED
FOR
THE
ACCOUNTING
RECORD
SUCH
AS
CPU
TIMES
AND
NUMBER
OF
CHARACTERS
TRANSFERRED
IS
KEPT
BY
THE
KERNEL
IN
THE
PROCESS
TABLE
AND
INITIALIZED
WHENEVER
A
NEW
PROCESS
IS
CREATED
AS
IN
THE
CHILD
AFTER
A
FORK
EACH
ACCOUNTING
RECORD
IS
WRITTEN
WHEN
THE
PROCESS
TERMINATES
THIS
HAS
TWO
CONSEQUENCES
FIRST
WE
DON
T
GET
ACCOUNTING
RECORDS
FOR
PROCESSES
THAT
NEVER
TERMINATE
PROCESSES
LIKE
INIT
THAT
RUN
FOR
THE
LIFETIME
OF
THE
SYSTEM
DON
T
GENERATE
ACCOUNTING
RECORDS
THIS
ALSO
APPLIES
TO
KERNEL
DAEMONS
WHICH
NORMALLY
DON
T
EXIT
SECOND
THE
ORDER
OF
THE
RECORDS
IN
THE
ACCOUNTING
FILE
CORRESPONDS
TO
THE
TERMINATION
ORDER
OF
THE
PROCESSES
NOT
THE
ORDER
IN
WHICH
THEY
WERE
STARTED
TO
KNOW
THE
STARTING
ORDER
WE
WOULD
HAVE
TO
GO
THROUGH
THE
ACCOUNTING
FILE
AND
SORT
BY
THE
STARTING
CALENDAR
TIME
BUT
THIS
ISN
T
PERFECT
SINCE
CALENDAR
TIMES
ARE
IN
UNITS
OF
SECONDS
SECTION
AND
IT
POSSIBLE
FOR
MANY
PROCESSES
TO
BE
STARTED
IN
ANY
GIVEN
SECOND
ALTERNATIVELY
THE
ELAPSED
TIME
IS
GIVEN
IN
CLOCK
TICKS
WHICH
ARE
USUALLY
BETWEEN
AND
TICKS
PER
SECOND
BUT
WE
DON
T
KNOW
THE
ENDING
TIME
OF
A
PROCESS
ALL
WE
KNOW
IS
ITS
STARTING
TIME
AND
ENDING
ORDER
THUS
EVEN
THOUGH
THE
ELAPSED
TIME
IS
MORE
ACCURATE
THAN
THE
STARTING
TIME
WE
STILL
CAN
T
RECONSTRUCT
THE
EXACT
STARTING
ORDER
OF
VARIOUS
PROCESSES
GIVEN
THE
DATA
IN
THE
ACCOUNTING
FILE
THE
ACCOUNTING
RECORDS
CORRESPOND
TO
PROCESSES
NOT
PROGRAMS
A
NEW
RECORD
IS
INITIALIZED
BY
THE
KERNEL
FOR
THE
CHILD
AFTER
A
FORK
NOT
WHEN
A
NEW
PROGRAM
IS
EXECUTED
ALTHOUGH
EXEC
DOESN
T
CREATE
A
NEW
ACCOUNTING
RECORD
THE
COMMAND
NAME
CHANGES
AND
THE
AFORK
FLAG
IS
CLEARED
THIS
MEANS
THAT
IF
WE
HAVE
A
CHAIN
OF
THREE
PROGRAMS
A
DESCRIPTION
FREEBSD
LINUX
MAC
OS
X
SOLARIS
AFORK
PROCESS
IS
THE
RESULT
OF
FORK
BUT
NEVER
CALLED
EXEC
ASU
PROCESS
USED
SUPERUSER
PRIVILEGES
ACORE
PROCESS
DUMPED
CORE
AXSIG
PROCESS
WAS
KILLED
BY
A
SIGNAL
AEXPND
EXPANDED
ACCOUNTING
ENTRY
ANVER
NEW
RECORD
FORMAT
FIGURE
VALUES
FOR
FROM
ACCOUNTING
RECORD
EXECS
B
THEN
B
EXECS
C
AND
C
EXITS
ONLY
A
SINGLE
ACCOUNTING
RECORD
IS
WRITTEN
THE
COMMAND
NAME
IN
THE
RECORD
CORRESPONDS
TO
PROGRAM
C
BUT
THE
CPU
TIMES
FOR
EXAMPLE
ARE
THE
SUM
FOR
PROGRAMS
A
B
AND
C
EXAMPLE
TO
HAVE
SOME
ACCOUNTING
DATA
TO
EXAMINE
WE
LL
CREATE
A
TEST
PROGRAM
TO
IMPLEMENT
THE
DIAGRAM
SHOWN
IN
FIGURE
PARENT
FIRST
CHILD
THIRD
CHILD
FOURTH
CHILD
EXECL
BIN
DD
FIGURE
PROCESS
STRUCTURE
FOR
ACCOUNTING
EXAMPLE
THE
SOURCE
FOR
THE
TEST
PROGRAM
IS
SHOWN
IN
FIGURE
IT
CALLS
FORK
FOUR
TIMES
EACH
CHILD
DOES
SOMETHING
DIFFERENT
AND
THEN
TERMINATES
WE
LL
RUN
THE
TEST
PROGRAM
ON
SOLARIS
AND
THEN
USE
THE
PROGRAM
IN
FIGURE
TO
PRINT
OUT
SELECTED
FIELDS
FROM
THE
ACCOUNTING
RECORDS
BSD
DERIVED
PLATFORMS
DON
T
SUPPORT
THE
MEMBER
SO
WE
DEFINE
THE
CONSTANT
ON
THE
PLATFORMS
THAT
DO
SUPPORT
THIS
MEMBER
BASING
THE
DEFINED
SYMBOL
ON
THE
FEATURE
INSTEAD
OF
ON
THE
PLATFORM
MAKES
THE
CODE
READ
BETTER
AND
ALLOWS
US
TO
MODIFY
THE
PROGRAM
SIMPLY
BY
ADDING
THE
NEW
DEFINITION
TO
OUR
COMPILATION
COMMAND
THE
ALTERNATIVE
WOULD
BE
TO
USE
IF
DEFINED
BSD
DEFINED
MACOS
WHICH
BECOMES
UNWIELDY
AS
WE
PORT
OUR
APPLICATION
TO
ADDITIONAL
PLATFORMS
WE
DEFINE
SIMILAR
CONSTANTS
TO
DETERMINE
WHETHER
THE
PLATFORM
SUPPORTS
THE
ACORE
AND
AXSIG
ACCOUNTING
FLAGS
WE
CAN
T
USE
THE
FLAG
SYMBOLS
THEMSELVES
BECAUSE
ON
LINUX
THEY
ARE
DEFINED
AS
ENUM
VALUES
WHICH
WE
CAN
T
USE
IN
A
IFDEF
EXPRESSION
TO
PERFORM
OUR
TEST
WE
DO
THE
FOLLOWING
BECOME
SUPERUSER
AND
ENABLE
ACCOUNTING
WITH
THE
ACCTON
COMMAND
NOTE
THAT
WHEN
THIS
COMMAND
TERMINATES
ACCOUNTING
SHOULD
BE
ON
THEREFORE
THE
FIRST
RECORD
IN
THE
ACCOUNTING
FILE
SHOULD
BE
FROM
THIS
COMMAND
EXIT
THE
SUPERUSER
SHELL
AND
RUN
THE
PROGRAM
IN
FIGURE
THIS
SHOULD
APPEND
SIX
RECORDS
TO
THE
ACCOUNTING
FILE
ONE
FOR
THE
SUPERUSER
SHELL
ONE
FOR
THE
TEST
PARENT
AND
ONE
FOR
EACH
OF
THE
FOUR
TEST
CHILDREN
A
NEW
PROCESS
IS
NOT
CREATED
BY
THE
EXECL
IN
THE
SECOND
CHILD
THERE
IS
ONLY
A
SINGLE
ACCOUNTING
RECORD
FOR
THE
SECOND
CHILD
BECOME
SUPERUSER
AND
TURN
ACCOUNTING
OFF
SINCE
ACCOUNTING
IS
OFF
WHEN
THIS
ACCTON
COMMAND
TERMINATES
IT
SHOULD
NOT
APPEAR
IN
THE
ACCOUNTING
FILE
RUN
THE
PROGRAM
IN
FIGURE
TO
PRINT
THE
SELECTED
FIELDS
FROM
THE
ACCOUNTING
FILE
THE
OUTPUT
FROM
STEP
FOLLOWS
WE
HAVE
APPENDED
THE
DESCRIPTION
OF
THE
PROCESS
IN
ITALICS
TO
SELECTED
LINES
FOR
THE
DISCUSSION
LATER
ACCTON
E
CHARS
STAT
SH
E
CHARS
STAT
DD
E
CHARS
STAT
SECOND
CHILD
A
OUT
E
CHARS
STAT
PARENT
A
OUT
E
CHARS
STAT
F
FIRST
CHILD
A
OUT
E
CHARS
STAT
F
FOURTH
CHILD
A
OUT
E
CHARS
STAT
F
THIRD
CHILD
FOR
THIS
SYSTEM
THE
ELAPSED
TIME
VALUES
ARE
MEASURED
IN
UNITS
OF
CLOCK
TICKS
FIGURE
SHOWS
THAT
THIS
SYSTEM
GENERATES
CLOCK
TICKS
PER
SECOND
FOR
EXAMPLE
THE
SLEEP
IN
THE
PARENT
CORRESPONDS
TO
THE
ELAPSED
TIME
OF
CLOCK
TICKS
FOR
THE
FIRST
CHILD
THE
SLEEP
BECOMES
CLOCK
TICKS
NOTE
THAT
THE
AMOUNT
OF
TIME
A
PROCESS
SLEEPS
IS
NOT
EXACT
WE
LL
RETURN
TO
THE
SLEEP
FUNCTION
IN
CHAPTER
ALSO
THE
CALLS
TO
FORK
AND
EXIT
TAKE
SOME
AMOUNT
OF
TIME
NOTE
THAT
THE
MEMBER
IS
NOT
THE
TRUE
TERMINATION
STATUS
OF
THE
PROCESS
BUT
RATHER
CORRESPONDS
TO
A
PORTION
OF
THE
TERMINATION
STATUS
THAT
WE
DISCUSSED
IN
SECTION
THE
ONLY
INFORMATION
IN
THIS
BYTE
IS
A
CORE
FLAG
BIT
USUALLY
THE
HIGH
ORDER
BIT
AND
THE
SIGNAL
NUMBER
USUALLY
THE
SEVEN
LOW
ORDER
BITS
IF
THE
PROCESS
TERMINATED
ABNORMALLY
IF
THE
PROCESS
TERMINATED
NORMALLY
WE
ARE
NOT
ABLE
TO
OBTAIN
THE
EXIT
STATUS
FROM
THE
ACCOUNTING
FILE
FOR
THE
FIRST
CHILD
THIS
VALUE
IS
THE
IS
THE
CORE
FLAG
BIT
AND
HAPPENS
TO
BE
THE
VALUE
ON
THIS
SYSTEM
FOR
SIGABRT
WHICH
IS
GENERATED
BY
THE
CALL
TO
ABORT
THE
VALUE
FOR
THE
FOURTH
CHILD
CORRESPONDS
TO
THE
VALUE
OF
SIGKILL
WE
CAN
T
TELL
FROM
THE
ACCOUNTING
DATA
THAT
THE
PARENT
ARGUMENT
TO
EXIT
WAS
AND
THAT
THE
THIRD
CHILD
ARGUMENT
TO
EXIT
WAS
THE
SIZE
OF
THE
FILE
ETC
PASSWD
THAT
THE
DD
PROCESS
COPIES
IN
THE
SECOND
CHILD
IS
BYTES
THE
NUMBER
OF
CHARACTERS
OF
I
O
IS
JUST
OVER
TWICE
THIS
VALUE
IT
IS
TWICE
THE
VALUE
AS
BYTES
ARE
READ
IN
THEN
BYTES
ARE
WRITTEN
OUT
EVEN
THOUGH
THE
OUTPUT
GOES
TO
THE
NULL
DEVICE
THE
BYTES
ARE
STILL
ACCOUNTED
FOR
THE
ADDITIONAL
BYTES
COME
FROM
THE
DD
COMMAND
REPORTING
THE
SUMMARY
OF
BYTES
READ
AND
WRITTEN
WHICH
IT
PRINTS
TO
STDOUT
THE
VALUES
ARE
ARE
WHAT
WE
WOULD
EXPECT
THE
F
FLAG
IS
SET
FOR
ALL
THE
CHILD
PROCESSES
EXCEPT
THE
SECOND
CHILD
WHICH
DOES
THE
EXECL
THE
F
FLAG
IS
NOT
SET
FOR
THE
PARENT
BECAUSE
THE
INTERACTIVE
SHELL
THAT
EXECUTED
THE
PARENT
DID
A
FORK
AND
THEN
AN
EXEC
OF
THE
A
OUT
FILE
THE
FIRST
CHILD
PROCESS
CALLS
ABORT
WHICH
GENERATES
A
SIGABRT
SIGNAL
TO
GENERATE
THE
CORE
DUMP
NOTE
THAT
NEITHER
THE
X
FLAG
NOR
THE
D
FLAG
IS
ON
AS
THEY
ARE
NOT
SUPPORTED
ON
SOLARIS
THE
INFORMATION
THEY
REPRESENT
CAN
BE
DERIVED
FROM
THE
FIELD
THE
FOURTH
CHILD
ALSO
TERMINATES
BECAUSE
OF
A
SIGNAL
BUT
THE
SIGKILL
SIGNAL
DOES
NOT
GENERATE
A
CORE
DUMP
IT
JUST
TERMINATES
THE
PROCESS
AS
A
FINAL
NOTE
THE
FIRST
CHILD
HAS
A
COUNT
FOR
THE
NUMBER
OF
CHARACTERS
OF
I
O
YET
THIS
PROCESS
GENERATED
A
CORE
FILE
IT
APPEARS
THAT
THE
I
O
REQUIRED
TO
WRITE
THE
CORE
FILE
IS
NOT
CHARGED
TO
THE
PROCESS
USER
IDENTIFICATION
ANY
PROCESS
CAN
FIND
OUT
ITS
REAL
AND
EFFECTIVE
USER
ID
AND
GROUP
ID
SOMETIMES
HOWEVER
WE
WANT
TO
FIND
OUT
THE
LOGIN
NAME
OF
THE
USER
WHO
RUNNING
THE
PROGRAM
WE
COULD
CALL
GETPWUID
GETUID
BUT
WHAT
IF
A
SINGLE
USER
HAS
MULTIPLE
LOGIN
NAMES
EACH
WITH
THE
SAME
USER
ID
A
PERSON
MIGHT
HAVE
MULTIPLE
ENTRIES
IN
THE
PASSWORD
FILE
WITH
THE
SAME
USER
ID
TO
HAVE
A
DIFFERENT
LOGIN
SHELL
FOR
EACH
ENTRY
THE
SYSTEM
NORMALLY
KEEPS
TRACK
OF
THE
NAME
WE
LOG
IN
UNDER
SECTION
AND
THE
GETLOGIN
FUNCTION
PROVIDES
A
WAY
TO
FETCH
THAT
LOGIN
NAME
THIS
FUNCTION
CAN
FAIL
IF
THE
PROCESS
IS
NOT
ATTACHED
TO
A
TERMINAL
THAT
A
USER
LOGGED
IN
TO
WE
NORMALLY
CALL
THESE
PROCESSES
DAEMONS
WE
DISCUSS
THEM
IN
CHAPTER
GIVEN
THE
LOGIN
NAME
WE
CAN
THEN
USE
IT
TO
LOOK
UP
THE
USER
IN
THE
PASSWORD
FILE
TO
DETERMINE
THE
LOGIN
SHELL
FOR
EXAMPLE
USING
GETPWNAM
TO
FIND
THE
LOGIN
NAME
UNIX
SYSTEMS
HAVE
HISTORICALLY
CALLED
THE
TTYNAME
FUNCTION
SECTION
AND
THEN
TRIED
TO
FIND
A
MATCHING
ENTRY
IN
THE
UTMP
FILE
SECTION
FREEBSD
AND
MAC
OS
X
STORE
THE
LOGIN
NAME
IN
THE
SESSION
STRUCTURE
ASSOCIATED
WITH
THE
PROCESS
TABLE
ENTRY
AND
PROVIDE
SYSTEM
CALLS
TO
FETCH
AND
STORE
THIS
NAME
SYSTEM
V
PROVIDED
THE
CUSERID
FUNCTION
TO
RETURN
THE
LOGIN
NAME
THIS
FUNCTION
CALLED
GETLOGIN
AND
IF
THAT
FAILED
DID
A
GETPWUID
GETUID
THE
IEEE
STANDARD
SPECIFIED
CUSERID
BUT
IT
CALLED
FOR
THE
EFFECTIVE
USER
ID
TO
BE
USED
INSTEAD
OF
THE
REAL
USER
ID
THE
VERSION
OF
POSIX
DROPPED
THE
CUSERID
FUNCTION
THE
ENVIRONMENT
VARIABLE
LOGNAME
IS
USUALLY
INITIALIZED
WITH
THE
USER
LOGIN
NAME
BY
LOGIN
AND
INHERITED
BY
THE
LOGIN
SHELL
REALIZE
HOWEVER
THAT
A
USER
CAN
MODIFY
AN
ENVIRONMENT
VARIABLE
SO
WE
SHOULDN
T
USE
LOGNAME
TO
VALIDATE
THE
USER
IN
ANY
WAY
INSTEAD
WE
SHOULD
USE
GETLOGIN
PROCESS
SCHEDULING
HISTORICALLY
THE
UNIX
SYSTEM
PROVIDED
PROCESSES
WITH
ONLY
COARSE
CONTROL
OVER
THEIR
SCHEDULING
PRIORITY
THE
SCHEDULING
POLICY
AND
PRIORITY
WERE
DETERMINED
BY
THE
KERNEL
A
PROCESS
COULD
CHOOSE
TO
RUN
WITH
LOWER
PRIORITY
BY
ADJUSTING
ITS
NICE
VALUE
THUS
A
PROCESS
COULD
BE
NICE
AND
REDUCE
ITS
SHARE
OF
THE
CPU
BY
ADJUSTING
ITS
NICE
VALUE
ONLY
A
PRIVILEGED
PROCESS
WAS
ALLOWED
TO
INCREASE
ITS
SCHEDULING
PRIORITY
THE
REAL
TIME
EXTENSIONS
IN
POSIX
ADDED
INTERFACES
TO
SELECT
AMONG
MULTIPLE
SCHEDULING
CLASSES
AND
FINE
TUNE
THEIR
BEHAVIOR
WE
DISCUSS
ONLY
THE
INTERFACES
USED
TO
ADJUST
THE
NICE
VALUE
HERE
THEY
ARE
PART
OF
THE
XSI
OPTION
IN
POSIX
REFER
TO
GALLMEISTER
FOR
MORE
INFORMATION
ON
THE
REAL
TIME
SCHEDULING
EXTENSIONS
IN
THE
SINGLE
UNIX
SPECIFICATION
NICE
VALUES
RANGE
FROM
TO
NZERO
ALTHOUGH
SOME
IMPLEMENTATIONS
SUPPORT
A
RANGE
FROM
TO
NZERO
LOWER
NICE
VALUES
HAVE
HIGHER
SCHEDULING
PRIORITY
ALTHOUGH
THIS
MIGHT
SEEM
BACKWARD
IT
ACTUALLY
MAKES
SENSE
THE
MORE
NICE
YOU
ARE
THE
LOWER
YOUR
SCHEDULING
PRIORITY
IS
NZERO
IS
THE
DEFAULT
NICE
VALUE
OF
THE
SYSTEM
BE
AWARE
THAT
THE
HEADER
FILE
DEFINING
NZERO
DIFFERS
AMONG
SYSTEMS
IN
ADDITION
TO
THE
HEADER
FILE
LINUX
MAKES
THE
VALUE
OF
NZERO
ACCESSIBLE
THROUGH
A
NONSTANDARD
SYSCONF
ARGUMENT
A
PROCESS
CAN
RETRIEVE
AND
CHANGE
ITS
NICE
VALUE
WITH
THE
NICE
FUNCTION
WITH
THIS
FUNCTION
A
PROCESS
CAN
AFFECT
ONLY
ITS
OWN
NICE
VALUE
IT
CAN
T
AFFECT
THE
NICE
VALUE
OF
ANY
OTHER
PROCESS
THE
INCR
ARGUMENT
IS
ADDED
TO
THE
NICE
VALUE
OF
THE
CALLING
PROCESS
IF
INCR
IS
TOO
LARGE
THE
SYSTEM
SILENTLY
REDUCES
IT
TO
THE
MAXIMUM
LEGAL
VALUE
SIMILARLY
IF
INCR
IS
TOO
SMALL
THE
SYSTEM
SILENTLY
INCREASES
IT
TO
THE
MINIMUM
LEGAL
VALUE
BECAUSE
IS
A
LEGAL
SUCCESSFUL
RETURN
VALUE
WE
NEED
TO
CLEAR
ERRNO
BEFORE
CALLING
NICE
AND
CHECK
ITS
VALUE
IF
NICE
RETURNS
IF
THE
CALL
TO
NICE
SUCCEEDS
AND
THE
RETURN
VALUE
IS
THEN
ERRNO
WILL
STILL
BE
ZERO
IF
ERRNO
IS
NONZERO
IT
MEANS
THAT
THE
CALL
TO
NICE
FAILED
THE
GETPRIORITY
FUNCTION
CAN
BE
USED
TO
GET
THE
NICE
VALUE
FOR
A
PROCESS
JUST
LIKE
THE
NICE
FUNCTION
HOWEVER
GETPRIORITY
CAN
ALSO
GET
THE
NICE
VALUE
FOR
A
GROUP
OF
RELATED
PROCESSES
THE
WHICH
ARGUMENT
CAN
TAKE
ON
ONE
OF
THREE
VALUES
TO
INDICATE
A
PROCESS
TO
INDICATE
A
PROCESS
GROUP
AND
TO
INDICATE
A
USER
ID
THE
WHICH
ARGUMENT
CONTROLS
HOW
THE
WHO
ARGUMENT
IS
INTERPRETED
AND
THE
WHO
ARGUMENT
SELECTS
THE
PROCESS
OR
PROCESSES
OF
INTEREST
IF
THE
WHO
ARGUMENT
IS
THEN
IT
INDICATES
THE
CALLING
PROCESS
PROCESS
GROUP
OR
USER
DEPENDING
ON
THE
VALUE
OF
THE
WHICH
ARGUMENT
WHEN
WHICH
IS
SET
TO
AND
WHO
IS
THE
REAL
USER
ID
OF
THE
CALLING
PROCESS
IS
USED
WHEN
THE
WHICH
ARGUMENT
APPLIES
TO
MORE
THAN
ONE
PROCESS
THE
HIGHEST
PRIORITY
LOWEST
VALUE
OF
ALL
THE
APPLICABLE
PROCESSES
IS
RETURNED
THE
SETPRIORITY
FUNCTION
CAN
BE
USED
TO
SET
THE
PRIORITY
OF
A
PROCESS
A
PROCESS
GROUP
OR
ALL
THE
PROCESSES
BELONGING
TO
A
PARTICULAR
USER
ID
THE
WHICH
AND
WHO
ARGUMENTS
ARE
THE
SAME
AS
IN
THE
GETPRIORITY
FUNCTION
THE
VALUE
IS
ADDED
TO
NZERO
AND
THIS
BECOMES
THE
NEW
NICE
VALUE
THE
NICE
SYSTEM
CALL
ORIGINATED
WITH
AN
EARLY
PDP
VERSION
OF
THE
RESEARCH
UNIX
SYSTEM
THE
GETPRIORITY
AND
SETPRIORITY
FUNCTIONS
ORIGINATED
WITH
THE
SINGLE
UNIX
SPECIFICATION
LEAVES
IT
UP
TO
THE
IMPLEMENTATION
WHETHER
THE
NICE
VALUE
IS
INHERITED
BY
A
CHILD
PROCESS
AFTER
A
FORK
HOWEVER
XSI
COMPLIANT
SYSTEMS
ARE
REQUIRED
TO
PRESERVE
THE
NICE
VALUE
ACROSS
A
CALL
TO
EXEC
A
CHILD
PROCESS
INHERITS
THE
NICE
VALUE
FROM
ITS
PARENT
PROCESS
IN
FREEBSD
LINUX
MAC
OS
X
AND
SOLARIS
EXAMPLE
THE
PROGRAM
IN
FIGURE
MEASURES
THE
EFFECT
OF
ADJUSTING
THE
NICE
VALUE
OF
A
PROCESS
TWO
PROCESSES
RUN
IN
PARALLEL
EACH
INCREMENTING
ITS
OWN
COUNTER
THE
PARENT
RUNS
WITH
THE
DEFAULT
NICE
VALUE
AND
THE
CHILD
RUNS
WITH
AN
ADJUSTED
NICE
VALUE
AS
SPECIFIED
BY
THE
OPTIONAL
COMMAND
ARGUMENT
AFTER
RUNNING
FOR
SECONDS
BOTH
PROCESSES
PRINT
THE
VALUE
OF
THEIR
COUNTER
AND
EXIT
BY
COMPARING
THE
COUNTER
VALUES
FOR
DIFFERENT
NICE
VALUES
WE
CAN
GET
AN
IDEA
HOW
THE
NICE
VALUE
AFFECTS
PROCESS
SCHEDULING
FIGURE
EVALUATE
THE
EFFECT
OF
CHANGING
THE
NICE
VALUE
WE
RUN
THE
PROGRAM
TWICE
ONCE
WITH
THE
DEFAULT
NICE
VALUE
AND
ONCE
WITH
THE
HIGHEST
VALID
NICE
VALUE
THE
LOWEST
SCHEDULING
PRIORITY
WE
RUN
THIS
ON
A
UNIPROCESSOR
LINUX
SYSTEM
TO
SHOW
HOW
THE
SCHEDULER
SHARES
THE
CPU
AMONG
PROCESSES
WITH
DIFFERENT
NICE
VALUES
WITH
AN
OTHERWISE
IDLE
SYSTEM
A
MULTIPROCESSOR
SYSTEM
OR
A
MULTICORE
CPU
WOULD
ALLOW
BOTH
PROCESSES
TO
RUN
WITHOUT
THE
NEED
TO
SHARE
A
CPU
AND
WE
WOULDN
T
SEE
MUCH
DIFFERENCE
BETWEEN
TWO
PROCESSES
WITH
DIFFERENT
NICE
VALUES
A
OUT
NZERO
CURRENT
NICE
VALUE
IN
PARENT
IS
CURRENT
NICE
VALUE
IN
CHILD
IS
ADJUSTING
BY
NOW
CHILD
NICE
VALUE
IS
CHILD
COUNT
PARENT
COUNT
A
OUT
NZERO
CURRENT
NICE
VALUE
IN
PARENT
IS
CURRENT
NICE
VALUE
IN
CHILD
IS
ADJUSTING
BY
NOW
CHILD
NICE
VALUE
IS
PARENT
COUNT
CHILD
COUNT
WHEN
BOTH
PROCESSES
HAVE
THE
SAME
NICE
VALUE
THE
PARENT
PROCESS
GETS
OF
THE
CPU
AND
THE
CHILD
GETS
OF
THE
CPU
NOTE
THAT
THE
TWO
PROCESSES
ARE
EFFECTIVELY
TREATED
EQUALLY
THE
PERCENTAGES
AREN
T
EXACTLY
EQUAL
BECAUSE
PROCESS
SCHEDULING
ISN
T
EXACT
AND
BECAUSE
THE
CHILD
AND
PARENT
PERFORM
DIFFERENT
AMOUNTS
OF
PROCESSING
BETWEEN
THE
TIME
THAT
THE
END
TIME
IS
CALCULATED
AND
THE
TIME
THAT
THE
PROCESSING
LOOP
BEGINS
IN
CONTRAST
WHEN
THE
CHILD
HAS
THE
HIGHEST
POSSIBLE
NICE
VALUE
THE
LOWEST
PRIORITY
WE
SEE
THAT
THE
PARENT
GETS
OF
THE
CPU
WHILE
THE
CHILD
GETS
ONLY
OF
THE
CPU
THESE
VALUES
WILL
VARY
BASED
ON
HOW
THE
PROCESS
SCHEDULER
USES
THE
NICE
VALUE
SO
A
DIFFERENT
UNIX
SYSTEM
WILL
PRODUCE
DIFFERENT
RATIOS
PROCESS
TIMES
IN
SECTION
WE
DESCRIBED
THREE
TIMES
THAT
WE
CAN
MEASURE
WALL
CLOCK
TIME
USER
CPU
TIME
AND
SYSTEM
CPU
TIME
ANY
PROCESS
CAN
CALL
THE
TIMES
FUNCTION
TO
OBTAIN
THESE
VALUES
FOR
ITSELF
AND
ANY
TERMINATED
CHILDREN
THIS
FUNCTION
FILLS
IN
THE
TMS
STRUCTURE
POINTED
TO
BY
BUF
STRUCT
TMS
USER
CPU
TIME
SYSTEM
CPU
TIME
USER
CPU
TIME
TERMINATED
CHILDREN
SYSTEM
CPU
TIME
TERMINATED
CHILDREN
NOTE
THAT
THE
STRUCTURE
DOES
NOT
CONTAIN
ANY
MEASUREMENT
FOR
THE
WALL
CLOCK
TIME
INSTEAD
THE
FUNCTION
RETURNS
THE
WALL
CLOCK
TIME
AS
THE
VALUE
OF
THE
FUNCTION
EACH
TIME
IT
CALLED
THIS
VALUE
IS
MEASURED
FROM
SOME
ARBITRARY
POINT
IN
THE
PAST
SO
WE
CAN
T
USE
ITS
ABSOLUTE
VALUE
INSTEAD
WE
USE
ITS
RELATIVE
VALUE
FOR
EXAMPLE
WE
CALL
TIMES
AND
SAVE
THE
RETURN
VALUE
AT
SOME
LATER
TIME
WE
CALL
TIMES
AGAIN
AND
SUBTRACT
THE
EARLIER
RETURN
VALUE
FROM
THE
NEW
RETURN
VALUE
THE
DIFFERENCE
IS
THE
WALL
CLOCK
TIME
IT
IS
POSSIBLE
THOUGH
UNLIKELY
FOR
A
LONG
RUNNING
PROCESS
TO
OVERFLOW
THE
WALL
CLOCK
TIME
SEE
EXERCISE
THE
TWO
STRUCTURE
FIELDS
FOR
CHILD
PROCESSES
CONTAIN
VALUES
ONLY
FOR
CHILDREN
THAT
WE
HAVE
WAITED
FOR
WITH
ONE
OF
THE
WAIT
FUNCTIONS
DISCUSSED
EARLIER
IN
THIS
CHAPTER
ALL
THE
VALUES
RETURNED
BY
THIS
FUNCTION
ARE
CONVERTED
TO
SECONDS
USING
THE
NUMBER
OF
CLOCK
TICKS
PER
SECOND
THE
VALUE
RETURNED
BY
SYSCONF
SECTION
MOST
IMPLEMENTATIONS
PROVIDE
THE
GETRUSAGE
FUNCTION
THIS
FUNCTION
RETURNS
THE
CPU
TIMES
AND
OTHER
VALUES
INDICATING
RESOURCE
USAGE
HISTORICALLY
THIS
FUNCTION
ORIGINATED
WITH
THE
BSD
OPERATING
SYSTEM
SO
BSD
DERIVED
IMPLEMENTATIONS
GENERALLY
SUPPORT
MORE
OF
THE
FIELDS
THAN
DO
OTHER
IMPLEMENTATIONS
EXAMPLE
THE
PROGRAM
IN
FIGURE
EXECUTES
EACH
COMMAND
LINE
ARGUMENT
AS
A
SHELL
COMMAND
STRING
TIMING
THE
COMMAND
AND
PRINTING
THE
VALUES
FROM
THE
TMS
STRUCTURE
IF
WE
RUN
THIS
PROGRAM
WE
GET
A
OUT
SLEEP
DATE
MAN
BASH
DEV
NULL
NORMAL
TERMINATION
EXIT
STATUS
COMMAND
DATE
SUN
FEB
EST
REAL
USER
SYS
CHILD
USER
CHILD
SYS
NORMAL
TERMINATION
EXIT
STATUS
COMMAND
MAN
BASH
DEV
NULL
REAL
USER
SYS
CHILD
USER
CHILD
SYS
NORMAL
TERMINATION
EXIT
STATUS
IN
THE
FIRST
TWO
COMMANDS
EXECUTION
IS
FAST
ENOUGH
TO
AVOID
REGISTERING
ANY
CPU
TIME
AT
THE
REPORTED
RESOLUTION
IN
THE
THIRD
COMMAND
HOWEVER
WE
RUN
A
COMMAND
THAT
TAKES
ENOUGH
PROCESSING
TIME
TO
NOTE
THAT
ALL
THE
CPU
TIME
APPEARS
IN
THE
CHILD
PROCESS
WHICH
IS
WHERE
THE
SHELL
AND
THE
COMMAND
EXECUTE
SUMMARY
A
THOROUGH
UNDERSTANDING
OF
THE
UNIX
SYSTEM
PROCESS
CONTROL
IS
ESSENTIAL
FOR
ADVANCED
PROGRAMMING
THERE
ARE
ONLY
A
FEW
FUNCTIONS
TO
MASTER
FORK
THE
EXEC
FAMILY
WAIT
AND
WAITPID
THESE
PRIMITIVES
ARE
USED
IN
MANY
APPLICATIONS
THE
FORK
FUNCTION
ALSO
GAVE
US
AN
OPPORTUNITY
TO
LOOK
AT
RACE
CONDITIONS
OUR
EXAMINATION
OF
THE
SYSTEM
FUNCTION
AND
PROCESS
ACCOUNTING
GAVE
US
ANOTHER
LOOK
AT
ALL
THESE
PROCESS
CONTROL
FUNCTIONS
WE
ALSO
LOOKED
AT
ANOTHER
VARIATION
OF
THE
EXEC
FUNCTIONS
INTERPRETER
FILES
AND
HOW
THEY
OPERATE
AN
UNDERSTANDING
OF
THE
VARIOUS
USER
IDS
AND
GROUP
IDS
THAT
ARE
PROVIDED
REAL
EFFECTIVE
AND
SAVED
IS
CRITICAL
TO
WRITING
SAFE
SET
USER
ID
PROGRAMS
GIVEN
AN
UNDERSTANDING
OF
A
SINGLE
PROCESS
AND
ITS
CHILDREN
IN
THE
NEXT
CHAPTER
WE
EXAMINE
THE
RELATIONSHIP
OF
A
PROCESS
TO
OTHER
PROCESSES
SESSIONS
AND
JOB
CONTROL
WE
THEN
COMPLETE
OUR
DISCUSSION
OF
PROCESSES
IN
CHAPTER
WHEN
WE
DESCRIBE
SIGNALS
EXERCISES
IN
FIGURE
WE
SAID
THAT
REPLACING
THE
CALL
TO
WITH
A
CALL
TO
EXIT
MIGHT
CAUSE
THE
STANDARD
OUTPUT
TO
BE
CLOSED
AND
PRINTF
TO
RETURN
MODIFY
THE
PROGRAM
TO
CHECK
WHETHER
YOUR
IMPLEMENTATION
BEHAVES
THIS
WAY
IF
IT
DOES
NOT
HOW
CAN
YOU
SIMULATE
THIS
BEHAVIOR
RECALL
THE
TYPICAL
ARRANGEMENT
OF
MEMORY
IN
FIGURE
BECAUSE
THE
STACK
FRAMES
CORRESPONDING
TO
EACH
FUNCTION
CALL
ARE
USUALLY
STORED
IN
THE
STACK
AND
BECAUSE
AFTER
A
VFORK
THE
CHILD
RUNS
IN
THE
ADDRESS
SPACE
OF
THE
PARENT
WHAT
HAPPENS
IF
THE
CALL
TO
VFORK
IS
FROM
A
FUNCTION
OTHER
THAN
MAIN
AND
THE
CHILD
DOES
A
RETURN
FROM
THIS
FUNCTION
AFTER
THE
VFORK
WRITE
A
TEST
PROGRAM
TO
VERIFY
THIS
AND
DRAW
A
PICTURE
OF
WHAT
HAPPENING
REWRITE
THE
PROGRAM
IN
FIGURE
TO
USE
WAITID
INSTEAD
OF
WAIT
INSTEAD
OF
CALLING
DETERMINE
THE
EQUIVALENT
INFORMATION
FROM
THE
SIGINFO
STRUCTURE
WHEN
WE
EXECUTE
THE
PROGRAM
IN
FIGURE
ONE
TIME
AS
IN
A
OUT
THE
OUTPUT
IS
CORRECT
BUT
IF
WE
EXECUTE
THE
PROGRAM
MULTIPLE
TIMES
ONE
RIGHT
AFTER
THE
OTHER
AS
IN
A
OUT
A
OUT
A
OUT
OUTPUT
FROM
PARENT
OOUTPUT
FROM
PARENT
OUOTUPTUT
FROM
CHILD
PUT
FROM
PARENT
OUTPUT
FROM
CHILD
UTPUT
FROM
CHILD
THE
OUTPUT
IS
NOT
CORRECT
WHAT
HAPPENING
HOW
CAN
WE
CORRECT
THIS
CAN
THIS
PROBLEM
HAPPEN
IF
WE
LET
THE
CHILD
WRITE
ITS
OUTPUT
FIRST
IN
THE
PROGRAM
SHOWN
IN
FIGURE
WE
CALL
EXECL
SPECIFYING
THE
PATHNAME
OF
THE
INTERPRETER
FILE
IF
WE
CALLED
EXECLP
INSTEAD
SPECIFYING
A
FILENAME
OF
TESTINTERP
AND
IF
THE
DIRECTORY
HOME
SAR
BIN
WAS
A
PATH
PREFIX
WHAT
WOULD
BE
PRINTED
AS
ARGV
WHEN
THE
PROGRAM
IS
RUN
WRITE
A
PROGRAM
THAT
CREATES
A
ZOMBIE
AND
THEN
CALL
SYSTEM
TO
EXECUTE
THE
PS
COMMAND
TO
VERIFY
THAT
THE
PROCESS
IS
A
ZOMBIE
WE
MENTIONED
IN
SECTION
THAT
POSIX
REQUIRES
OPEN
DIRECTORY
STREAMS
TO
BE
CLOSED
ACROSS
AN
EXEC
VERIFY
THIS
AS
FOLLOWS
CALL
OPENDIR
FOR
THE
ROOT
DIRECTORY
PEEK
AT
YOUR
SYSTEM
IMPLEMENTATION
OF
THE
DIR
STRUCTURE
AND
PRINT
THE
CLOSE
ON
EXEC
FLAG
THEN
OPEN
THE
SAME
DIRECTORY
FOR
READING
AND
PRINT
THE
CLOSE
ON
EXEC
FLAG
THIS
PAGE
INTENTIONALLY
LEFT
BLANK
PROCESS
RELATIONSHIPS
INTRODUCTION
WE
LEARNED
IN
THE
PREVIOUS
CHAPTER
THAT
THERE
ARE
RELATIONSHIPS
BETWEEN
PROCESSES
FIRST
EVERY
PROCESS
HAS
A
PARENT
PROCESS
THE
INITIAL
KERNEL
LEVEL
PROCESS
IS
USUALLY
ITS
OWN
PARENT
THE
PARENT
IS
NOTIFIED
WHEN
THE
CHILD
TERMINATES
AND
THE
PARENT
CAN
OBTAIN
THE
CHILD
EXIT
STATUS
WE
ALSO
MENTIONED
PROCESS
GROUPS
WHEN
WE
DESCRIBED
THE
WAITPID
FUNCTION
SECTION
AND
EXPLAINED
HOW
WE
CAN
WAIT
FOR
ANY
PROCESS
IN
A
PROCESS
GROUP
TO
TERMINATE
IN
THIS
CHAPTER
WE
LL
LOOK
AT
PROCESS
GROUPS
IN
MORE
DETAIL
AND
THE
CONCEPT
OF
SESSIONS
THAT
WAS
INTRODUCED
BY
POSIX
WE
LL
ALSO
LOOK
AT
THE
RELATIONSHIP
BETWEEN
THE
LOGIN
SHELL
THAT
IS
INVOKED
FOR
US
WHEN
WE
LOG
IN
AND
ALL
THE
PROCESSES
THAT
WE
START
FROM
OUR
LOGIN
SHELL
IT
IS
IMPOSSIBLE
TO
DESCRIBE
THESE
RELATIONSHIPS
WITHOUT
TALKING
ABOUT
SIGNALS
AND
TO
TALK
ABOUT
SIGNALS
WE
NEED
MANY
OF
THE
CONCEPTS
IN
THIS
CHAPTER
IF
YOU
ARE
UNFAMILIAR
WITH
THE
UNIX
SYSTEM
SIGNAL
MECHANISM
YOU
MAY
WANT
TO
SKIM
THROUGH
CHAPTER
AT
THIS
POINT
TERMINAL
LOGINS
LET
START
BY
LOOKING
AT
THE
PROGRAMS
THAT
ARE
EXECUTED
WHEN
WE
LOG
IN
TO
A
UNIX
SYSTEM
IN
EARLY
UNIX
SYSTEMS
SUCH
AS
VERSION
USERS
LOGGED
IN
USING
DUMB
TERMINALS
THAT
WERE
CONNECTED
TO
THE
HOST
WITH
HARD
WIRED
CONNECTIONS
THE
TERMINALS
WERE
EITHER
LOCAL
DIRECTLY
CONNECTED
OR
REMOTE
CONNECTED
THROUGH
A
MODEM
IN
EITHER
CASE
THESE
LOGINS
CAME
THROUGH
A
TERMINAL
DEVICE
DRIVER
IN
THE
KERNEL
FOR
EXAMPLE
THE
COMMON
DEVICES
ON
PDP
WERE
DH
AND
DZ
A
HOST
HAD
A
FIXED
NUMBER
OF
THESE
TERMINAL
DEVICES
SO
THERE
WAS
A
KNOWN
UPPER
LIMIT
ON
THE
NUMBER
OF
SIMULTANEOUS
LOGINS
AS
BITMAPPED
GRAPHICAL
TERMINALS
BECAME
AVAILABLE
WINDOWING
SYSTEMS
WERE
DEVELOPED
TO
PROVIDE
USERS
WITH
NEW
WAYS
TO
INTERACT
WITH
HOST
COMPUTERS
APPLICATIONS
WERE
DEVELOPED
TO
CREATE
TERMINAL
WINDOWS
TO
EMULATE
CHARACTER
BASED
TERMINALS
ALLOWING
USERS
TO
INTERACT
WITH
HOSTS
IN
FAMILIAR
WAYS
I
E
VIA
THE
SHELL
COMMAND
LINE
TODAY
SOME
PLATFORMS
ALLOW
YOU
TO
START
A
WINDOWING
SYSTEM
AFTER
LOGGING
IN
WHEREAS
OTHER
PLATFORMS
AUTOMATICALLY
START
THE
WINDOWING
SYSTEM
FOR
YOU
IN
THE
LATTER
CASE
YOU
MIGHT
STILL
HAVE
TO
LOG
IN
DEPENDING
ON
HOW
THE
WINDOWING
SYSTEM
IS
CONFIGURED
SOME
WINDOWING
SYSTEMS
CAN
BE
CONFIGURED
TO
LOG
YOU
IN
AUTOMATICALLY
THE
PROCEDURE
THAT
WE
NOW
DESCRIBE
IS
USED
TO
LOG
IN
TO
A
UNIX
SYSTEM
USING
A
TERMINAL
THE
PROCEDURE
IS
SIMILAR
REGARDLESS
OF
THE
TYPE
OF
TERMINAL
WE
USE
IT
COULD
BE
A
CHARACTER
BASED
TERMINAL
A
GRAPHICAL
TERMINAL
EMULATING
A
SIMPLE
CHARACTER
BASED
TERMINAL
OR
A
GRAPHICAL
TERMINAL
RUNNING
A
WINDOWING
SYSTEM
BSD
TERMINAL
LOGINS
THE
BSD
TERMINAL
LOGIN
PROCEDURE
HAS
NOT
CHANGED
MUCH
OVER
THE
PAST
YEARS
THE
SYSTEM
ADMINISTRATOR
CREATES
A
FILE
USUALLY
ETC
TTYS
THAT
HAS
ONE
LINE
PER
TERMINAL
DEVICE
EACH
LINE
SPECIFIES
THE
NAME
OF
THE
DEVICE
AND
OTHER
PARAMETERS
THAT
ARE
PASSED
TO
THE
GETTY
PROGRAM
ONE
PARAMETER
IS
THE
BAUD
RATE
OF
THE
TERMINAL
FOR
EXAMPLE
WHEN
THE
SYSTEM
IS
BOOTSTRAPPED
THE
KERNEL
CREATES
PROCESS
ID
THE
INIT
PROCESS
AND
IT
IS
INIT
THAT
BRINGS
THE
SYSTEM
UP
IN
MULTIUSER
MODE
THE
INIT
PROCESS
READS
THE
FILE
ETC
TTYS
AND
FOR
EVERY
TERMINAL
DEVICE
THAT
ALLOWS
A
LOGIN
DOES
A
FORK
FOLLOWED
BY
AN
EXEC
OF
THE
PROGRAM
GETTY
THIS
GIVES
US
THE
PROCESSES
SHOWN
IN
FIGURE
PROCESS
ID
K
FORKS
ONCE
PER
TERMINAL
C
EACH
CHILD
EXECS
GETTY
FIGURE
PROCESSES
INVOKED
BY
INIT
TO
ALLOW
TERMINAL
LOGINS
ALL
THE
PROCESSES
SHOWN
IN
FIGURE
HAVE
A
REAL
USER
ID
OF
AND
AN
EFFECTIVE
USER
ID
OF
I
E
THEY
ALL
HAVE
SUPERUSER
PRIVILEGES
THE
INIT
PROCESS
ALSO
EXECS
THE
GETTY
PROGRAM
WITH
AN
EMPTY
ENVIRONMENT
IT
IS
GETTY
THAT
CALLS
OPEN
FOR
THE
TERMINAL
DEVICE
THE
TERMINAL
IS
OPENED
FOR
READING
AND
WRITING
IF
THE
DEVICE
IS
A
MODEM
THE
OPEN
MAY
DELAY
INSIDE
THE
DEVICE
DRIVER
UNTIL
THE
MODEM
IS
DIALED
AND
THE
CALL
IS
ANSWERED
ONCE
THE
DEVICE
IS
OPEN
FILE
DESCRIPTORS
AND
ARE
SET
TO
THE
DEVICE
THEN
GETTY
OUTPUTS
SOMETHING
LIKE
LOGIN
AND
WAITS
FOR
US
TO
ENTER
OUR
USER
NAME
IF
THE
TERMINAL
SUPPORTS
MULTIPLE
SPEEDS
GETTY
CAN
DETECT
SPECIAL
CHARACTERS
THAT
TELL
IT
TO
CHANGE
THE
TERMINAL
SPEED
BAUD
RATE
CONSULT
YOUR
UNIX
SYSTEM
MANUALS
FOR
ADDITIONAL
DETAILS
ON
THE
GETTY
PROGRAM
AND
THE
DATA
FILES
GETTYTAB
THAT
CAN
DRIVE
ITS
ACTIONS
WHEN
WE
ENTER
OUR
USER
NAME
GETTY
JOB
IS
COMPLETE
AND
IT
THEN
INVOKES
THE
LOGIN
PROGRAM
SIMILAR
TO
EXECLE
BIN
LOGIN
LOGIN
P
USERNAME
CHAR
ENVP
THERE
CAN
BE
OPTIONS
IN
THE
GETTYTAB
FILE
TO
HAVE
IT
INVOKE
OTHER
PROGRAMS
BUT
THE
DEFAULT
IS
THE
LOGIN
PROGRAM
INIT
INVOKES
GETTY
WITH
AN
EMPTY
ENVIRONMENT
GETTY
CREATES
AN
ENVIRONMENT
FOR
LOGIN
THE
ENVP
ARGUMENT
WITH
THE
NAME
OF
THE
TERMINAL
SOMETHING
LIKE
TERM
FOO
WHERE
THE
TYPE
OF
TERMINAL
FOO
IS
TAKEN
FROM
THE
GETTYTAB
FILE
AND
ANY
ENVIRONMENT
STRINGS
THAT
ARE
SPECIFIED
IN
THE
GETTYTAB
THE
P
FLAG
TO
LOGIN
TELLS
IT
TO
PRESERVE
THE
ENVIRONMENT
THAT
IT
IS
PASSED
AND
TO
ADD
TO
THAT
ENVIRONMENT
NOT
REPLACE
IT
FIGURE
SHOWS
THE
STATE
OF
THESE
PROCESSES
RIGHT
AFTER
LOGIN
HAS
BEEN
INVOKED
PROCESS
ID
INIT
FORK
INIT
EXEC
READS
ETC
TTYS
FORKS
ONCE
PER
TERMINAL
CREATES
EMPTY
ENVIRONMENT
OPENS
TERMINAL
DEVICE
FILE
DESCRIPTORS
READS
USER
NAME
INITIAL
ENVIRONMENT
SET
FIGURE
STATE
OF
PROCESSES
AFTER
LOGIN
HAS
BEEN
INVOKED
ALL
THE
PROCESSES
SHOWN
IN
FIGURE
HAVE
SUPERUSER
PRIVILEGES
SINCE
THE
ORIGINAL
INIT
PROCESS
HAS
SUPERUSER
PRIVILEGES
THE
PROCESS
ID
OF
THE
BOTTOM
THREE
PROCESSES
IN
FIGURE
IS
THE
SAME
SINCE
THE
PROCESS
ID
DOES
NOT
CHANGE
ACROSS
AN
EXEC
ALSO
ALL
THE
PROCESSES
OTHER
THAN
THE
ORIGINAL
INIT
PROCESS
HAVE
A
PARENT
PROCESS
ID
OF
THE
LOGIN
PROGRAM
DOES
MANY
THINGS
SINCE
IT
HAS
OUR
USER
NAME
IT
CAN
CALL
GETPWNAM
TO
FETCH
OUR
PASSWORD
FILE
ENTRY
THEN
LOGIN
CALLS
GETPASS
TO
DISPLAY
THE
PROMPT
PASSWORD
AND
READ
OUR
PASSWORD
WITH
ECHOING
DISABLED
OF
COURSE
IT
CALLS
CRYPT
TO
ENCRYPT
THE
PASSWORD
THAT
WE
ENTERED
AND
COMPARES
THE
ENCRYPTED
RESULT
TO
THE
FIELD
FROM
OUR
SHADOW
PASSWORD
FILE
ENTRY
IF
THE
LOGIN
ATTEMPT
FAILS
BECAUSE
OF
AN
INVALID
PASSWORD
AFTER
A
FEW
TRIES
LOGIN
CALLS
EXIT
WITH
AN
ARGUMENT
OF
THIS
TERMINATION
WILL
BE
NOTICED
BY
THE
PARENT
INIT
AND
IT
WILL
DO
ANOTHER
FORK
FOLLOWED
BY
AN
EXEC
OF
GETTY
STARTING
THE
PROCEDURE
OVER
AGAIN
FOR
THIS
TERMINAL
THIS
IS
THE
TRADITIONAL
AUTHENTICATION
PROCEDURE
USED
ON
UNIX
SYSTEMS
MODERN
UNIX
SYSTEMS
HOWEVER
HAVE
EVOLVED
TO
SUPPORT
MULTIPLE
AUTHENTICATION
PROCEDURES
FOR
EXAMPLE
FREEBSD
LINUX
MAC
OS
X
AND
SOLARIS
ALL
SUPPORT
A
MORE
FLEXIBLE
SCHEME
KNOWN
AS
PAM
PLUGGABLE
AUTHENTICATION
MODULES
PAM
ALLOWS
AN
ADMINISTRATOR
TO
CONFIGURE
THE
AUTHENTICATION
METHODS
TO
BE
USED
TO
ACCESS
SERVICES
THAT
ARE
WRITTEN
TO
USE
THE
PAM
LIBRARY
IF
OUR
APPLICATION
NEEDS
TO
VERIFY
THAT
A
USER
HAS
THE
APPROPRIATE
PERMISSION
TO
PERFORM
A
TASK
WE
CAN
EITHER
HARD
CODE
THE
AUTHENTICATION
MECHANISM
IN
THE
APPLICATION
OR
USE
THE
PAM
LIBRARY
TO
GIVE
US
THE
EQUIVALENT
FUNCTIONALITY
THE
ADVANTAGE
TO
USING
PAM
IS
THAT
ADMINISTRATORS
CAN
CONFIGURE
DIFFERENT
WAYS
TO
AUTHENTICATE
USERS
FOR
DIFFERENT
TASKS
BASED
ON
THE
LOCAL
SITE
POLICIES
IF
WE
LOG
IN
CORRECTLY
LOGIN
WILL
CHANGE
TO
OUR
HOME
DIRECTORY
CHDIR
CHANGE
THE
OWNERSHIP
OF
OUR
TERMINAL
DEVICE
CHOWN
SO
WE
OWN
IT
CHANGE
THE
ACCESS
PERMISSIONS
FOR
OUR
TERMINAL
DEVICE
SO
WE
HAVE
PERMISSION
TO
READ
FROM
AND
WRITE
TO
IT
SET
OUR
GROUP
IDS
BY
CALLING
SETGID
AND
INITGROUPS
INITIALIZE
THE
ENVIRONMENT
WITH
ALL
THE
INFORMATION
THAT
LOGIN
HAS
OUR
HOME
DIRECTORY
HOME
SHELL
SHELL
USER
NAME
USER
AND
LOGNAME
AND
A
DEFAULT
PATH
PATH
CHANGE
TO
OUR
USER
ID
SETUID
AND
INVOKE
OUR
LOGIN
SHELL
AS
IN
EXECL
BIN
SH
SH
CHAR
THE
MINUS
SIGN
AS
THE
FIRST
CHARACTER
OF
ARGV
IS
A
FLAG
TO
ALL
THE
SHELLS
THAT
INDICATES
THEY
ARE
BEING
INVOKED
AS
A
LOGIN
SHELL
THE
SHELLS
CAN
LOOK
AT
THIS
CHARACTER
AND
MODIFY
THEIR
START
UP
ACCORDINGLY
THE
LOGIN
PROGRAM
REALLY
DOES
MORE
THAN
WE
VE
DESCRIBED
HERE
IT
OPTIONALLY
PRINTS
THE
MESSAGE
OF
THE
DAY
FILE
CHECKS
FOR
NEW
MAIL
AND
PERFORMS
OTHER
TASKS
IN
THIS
CHAPTER
WE
RE
INTERESTED
ONLY
IN
THE
FEATURES
THAT
WE
VE
DESCRIBED
RECALL
FROM
OUR
DISCUSSION
OF
THE
SETUID
FUNCTION
IN
SECTION
THAT
SINCE
IT
IS
CALLED
BY
A
SUPERUSER
PROCESS
SETUID
CHANGES
ALL
THREE
USER
IDS
THE
REAL
USER
ID
EFFECTIVE
USER
ID
AND
SAVED
SET
USER
ID
THE
CALL
TO
SETGID
THAT
WAS
DONE
EARLIER
BY
LOGIN
HAS
THE
SAME
EFFECT
ON
ALL
THREE
GROUP
IDS
AT
THIS
POINT
OUR
LOGIN
SHELL
IS
RUNNING
ITS
PARENT
PROCESS
ID
IS
THE
ORIGINAL
INIT
PROCESS
PROCESS
ID
SO
WHEN
OUR
LOGIN
SHELL
TERMINATES
INIT
IS
NOTIFIED
IT
IS
SENT
A
SIGCHLD
SIGNAL
AND
IT
STARTS
THE
WHOLE
PROCEDURE
OVER
AGAIN
FOR
THIS
TERMINAL
FILE
DESCRIPTORS
AND
FOR
OUR
LOGIN
SHELL
ARE
SET
TO
THE
TERMINAL
DEVICE
FIGURE
SHOWS
THIS
ARRANGEMENT
PROCESS
ID
THROUGH
GETTY
AND
LOGIN
FD
HARD
WIRED
CONNECTION
FIGURE
ARRANGEMENT
OF
PROCESSES
AFTER
EVERYTHING
IS
SET
FOR
A
TERMINAL
LOGIN
OUR
LOGIN
SHELL
NOW
READS
ITS
START
UP
FILES
PROFILE
FOR
THE
BOURNE
SHELL
AND
KORN
SHELL
OR
PROFILE
FOR
THE
GNU
BOURNE
AGAIN
SHELL
AND
CSHRC
AND
LOGIN
FOR
THE
C
SHELL
THESE
START
UP
FILES
USUALLY
CHANGE
SOME
OF
THE
ENVIRONMENT
VARIABLES
AND
ADD
MANY
OTHER
VARIABLES
TO
THE
ENVIRONMENT
FOR
EXAMPLE
MOST
USERS
SET
THEIR
OWN
PATH
AND
OFTEN
PROMPT
FOR
THE
ACTUAL
TERMINAL
TYPE
TERM
WHEN
THE
START
UP
FILES
ARE
DONE
WE
FINALLY
GET
THE
SHELL
PROMPT
AND
CAN
ENTER
COMMANDS
MAC
OS
X
TERMINAL
LOGINS
ON
MAC
OS
X
THE
TERMINAL
LOGIN
PROCESS
FOLLOWS
ESSENTIALLY
THE
SAME
STEPS
AS
IN
THE
BSD
LOGIN
PROCESS
SINCE
MAC
OS
X
IS
BASED
IN
PART
ON
FREEBSD
WITH
MAC
OS
X
HOWEVER
THERE
ARE
SOME
DIFFERENCES
THE
WORK
OF
INIT
IS
PERFORMED
BY
LAUNCHD
WE
ARE
PRESENTED
WITH
A
GRAPHICAL
BASED
LOGIN
SCREEN
FROM
THE
START
LINUX
TERMINAL
LOGINS
THE
LINUX
LOGIN
PROCEDURE
IS
VERY
SIMILAR
TO
THE
BSD
PROCEDURE
INDEED
THE
LINUX
LOGIN
COMMAND
IS
DERIVED
FROM
THE
LOGIN
COMMAND
THE
MAIN
DIFFERENCE
BETWEEN
THE
BSD
LOGIN
PROCEDURE
AND
THE
LINUX
LOGIN
PROCEDURE
IS
IN
THE
WAY
THE
TERMINAL
CONFIGURATION
IS
SPECIFIED
SOME
LINUX
DISTRIBUTIONS
SHIP
WITH
A
VERSION
OF
THE
INIT
PROGRAM
THAT
USES
ADMINISTRATIVE
FILES
PATTERNED
AFTER
SYSTEM
V
INIT
FILE
FORMATS
ON
THESE
SYSTEMS
ETC
INITTAB
CONTAINS
THE
CONFIGURATION
INFORMATION
SPECIFYING
THE
TERMINAL
DEVICES
FOR
WHICH
INIT
SHOULD
START
A
GETTY
PROCESS
OTHER
LINUX
DISTRIBUTIONS
SUCH
AS
RECENT
UBUNTU
DISTRIBUTIONS
SHIP
WITH
A
VERSION
OF
INIT
THAT
IS
KNOWN
AS
UPSTART
IT
USES
CONFIGURATION
FILES
NAMED
CONF
THAT
ARE
STORED
IN
THE
ETC
INIT
DIRECTORY
FOR
EXAMPLE
THE
SPECIFICATIONS
FOR
RUNNING
GETTY
ON
DEV
MIGHT
BE
FOUND
IN
THE
FILE
ETC
INIT
CONF
DEPENDING
ON
THE
VERSION
OF
GETTY
IN
USE
THE
TERMINAL
CHARACTERISTICS
ARE
SPECIFIED
EITHER
ON
THE
COMMAND
LINE
AS
WITH
AGETTY
OR
IN
THE
FILE
ETC
GETTYDEFS
AS
WITH
MGETTY
SOLARIS
TERMINAL
LOGINS
SOLARIS
SUPPORTS
TWO
FORMS
OF
TERMINAL
LOGINS
A
GETTY
STYLE
AS
DESCRIBED
PREVIOUSLY
FOR
BSD
AND
B
TTYMON
LOGINS
A
FEATURE
INTRODUCED
WITH
NORMALLY
GETTY
IS
USED
FOR
THE
CONSOLE
AND
TTYMON
IS
USED
FOR
OTHER
TERMINAL
LOGINS
THE
TTYMON
COMMAND
IS
PART
OF
A
LARGER
FACILITY
TERMED
SAF
THE
SERVICE
ACCESS
FACILITY
THE
GOAL
OF
THE
SAF
WAS
TO
PROVIDE
A
CONSISTENT
WAY
TO
ADMINISTER
SERVICES
THAT
PROVIDE
ACCESS
TO
A
SYSTEM
SEE
CHAPTER
OF
RAGO
FOR
MORE
DETAILS
FOR
OUR
PURPOSES
WE
END
UP
WITH
THE
SAME
PICTURE
AS
IN
FIGURE
WITH
A
DIFFERENT
SET
OF
STEPS
BETWEEN
INIT
AND
THE
LOGIN
SHELL
INIT
IS
THE
PARENT
OF
SAC
THE
SERVICE
ACCESS
CONTROLLER
WHICH
DOES
A
FORK
AND
EXEC
OF
THE
TTYMON
PROGRAM
WHEN
THE
SYSTEM
ENTERS
MULTIUSER
STATE
THE
TTYMON
PROGRAM
MONITORS
ALL
THE
TERMINAL
PORTS
LISTED
IN
ITS
CONFIGURATION
FILE
AND
DOES
A
FORK
WHEN
WE
ENTER
OUR
LOGIN
NAME
THIS
CHILD
OF
TTYMON
DOES
AN
EXEC
OF
LOGIN
AND
LOGIN
PROMPTS
US
FOR
OUR
PASSWORD
ONCE
THIS
IS
DONE
LOGIN
EXECS
OUR
LOGIN
SHELL
AND
WE
RE
AT
THE
POSITION
SHOWN
IN
FIGURE
ONE
DIFFERENCE
IS
THAT
THE
PARENT
OF
OUR
LOGIN
SHELL
IS
NOW
TTYMON
WHEREAS
THE
PARENT
OF
THE
LOGIN
SHELL
FROM
A
GETTY
LOGIN
IS
INIT
NETWORK
LOGINS
THE
MAIN
PHYSICAL
DIFFERENCE
BETWEEN
LOGGING
IN
TO
A
SYSTEM
THROUGH
A
SERIAL
TERMINAL
AND
LOGGING
IN
TO
A
SYSTEM
THROUGH
A
NETWORK
IS
THAT
THE
CONNECTION
BETWEEN
THE
TERMINAL
AND
THE
COMPUTER
ISN
T
POINT
TO
POINT
IN
THIS
CASE
LOGIN
IS
SIMPLY
A
SERVICE
AVAILABLE
JUST
LIKE
ANY
OTHER
NETWORK
SERVICE
SUCH
AS
FTP
OR
SMTP
WITH
THE
TERMINAL
LOGINS
THAT
WE
DESCRIBED
IN
THE
PREVIOUS
SECTION
INIT
KNOWS
WHICH
TERMINAL
DEVICES
ARE
ENABLED
FOR
LOGINS
AND
SPAWNS
A
GETTY
PROCESS
FOR
EACH
DEVICE
IN
THE
CASE
OF
NETWORK
LOGINS
HOWEVER
ALL
THE
LOGINS
COME
THROUGH
THE
KERNEL
NETWORK
INTERFACE
DRIVERS
E
G
THE
ETHERNET
DRIVER
AND
WE
DON
T
KNOW
AHEAD
OF
TIME
HOW
MANY
OF
THESE
WILL
OCCUR
INSTEAD
OF
HAVING
A
PROCESS
WAITING
FOR
EACH
POSSIBLE
LOGIN
WE
NOW
HAVE
TO
WAIT
FOR
A
NETWORK
CONNECTION
REQUEST
TO
ARRIVE
TO
ALLOW
THE
SAME
SOFTWARE
TO
PROCESS
LOGINS
OVER
BOTH
TERMINAL
LOGINS
AND
NETWORK
LOGINS
A
SOFTWARE
DRIVER
CALLED
A
PSEUDO
TERMINAL
IS
USED
TO
EMULATE
THE
BEHAVIOR
OF
A
SERIAL
TERMINAL
AND
MAP
TERMINAL
OPERATIONS
TO
NETWORK
OPERATIONS
AND
VICE
VERSA
IN
CHAPTER
WE
LL
TALK
ABOUT
PSEUDO
TERMINALS
IN
DETAIL
BSD
NETWORK
LOGINS
IN
BSD
A
SINGLE
PROCESS
WAITS
FOR
MOST
NETWORK
CONNECTIONS
THE
INETD
PROCESS
SOMETIMES
CALLED
THE
INTERNET
SUPERSERVER
IN
THIS
SECTION
WE
LL
LOOK
AT
THE
SEQUENCE
OF
PROCESSES
INVOLVED
IN
NETWORK
LOGINS
FOR
A
BSD
SYSTEM
WE
ARE
NOT
INTERESTED
IN
THE
DETAILED
NETWORK
PROGRAMMING
ASPECTS
OF
THESE
PROCESSES
REFER
TO
STEVENS
FENNER
AND
RUDOFF
FOR
ALL
THE
DETAILS
AS
PART
OF
THE
SYSTEM
START
UP
INIT
INVOKES
A
SHELL
THAT
EXECUTES
THE
SHELL
SCRIPT
ETC
RC
ONE
OF
THE
DAEMONS
THAT
IS
STARTED
BY
THIS
SHELL
SCRIPT
IS
INETD
ONCE
THE
SHELL
SCRIPT
TERMINATES
THE
PARENT
PROCESS
OF
INETD
BECOMES
INIT
INETD
WAITS
FOR
TCP
IP
CONNECTION
REQUESTS
TO
ARRIVE
AT
THE
HOST
WHEN
A
CONNECTION
REQUEST
ARRIVES
FOR
IT
TO
HANDLE
INETD
DOES
A
FORK
AND
EXEC
OF
THE
APPROPRIATE
PROGRAM
LET
ASSUME
THAT
A
TCP
CONNECTION
REQUEST
ARRIVES
FOR
THE
TELNET
SERVER
TELNET
IS
A
REMOTE
LOGIN
APPLICATION
THAT
USES
THE
TCP
PROTOCOL
A
USER
ON
ANOTHER
HOST
THAT
IS
CONNECTED
TO
THE
SERVER
HOST
THROUGH
A
NETWORK
OF
SOME
FORM
OR
ON
THE
SAME
HOST
INITIATES
THE
LOGIN
BY
STARTING
THE
TELNET
CLIENT
TELNET
HOSTNAME
THE
CLIENT
OPENS
A
TCP
CONNECTION
TO
HOSTNAME
AND
THE
PROGRAM
THAT
STARTED
ON
HOSTNAME
IS
CALLED
THE
TELNET
SERVER
THE
CLIENT
AND
THE
SERVER
THEN
EXCHANGE
DATA
ACROSS
THE
TCP
CONNECTION
USING
THE
TELNET
APPLICATION
PROTOCOL
WHAT
HAS
HAPPENED
IS
THAT
THE
USER
WHO
STARTED
THE
CLIENT
PROGRAM
IS
NOW
LOGGED
IN
TO
THE
SERVER
HOST
THIS
ASSUMES
OF
COURSE
THAT
THE
USER
HAS
A
VALID
ACCOUNT
ON
THE
SERVER
HOST
FIGURE
SHOWS
THE
SEQUENCE
OF
PROCESSES
INVOLVED
IN
EXECUTING
THE
TELNET
SERVER
CALLED
TELNETD
PROCESS
ID
TCP
CONNECTION
REQUEST
FROM
TELNET
CLIENT
FORK
FORK
EXEC
OF
BIN
SH
WHICH
EXECUTES
SHELL
SCRIPT
ETC
RC
WHEN
SYSTEM
COMES
UP
MULTIUSER
WHEN
CONNECTION
REQUEST
ARRIVES
FROM
TELNET
CLIENT
INETD
EXEC
FIGURE
SEQUENCE
OF
PROCESSES
INVOLVED
IN
EXECUTING
TELNET
SERVER
THE
TELNETD
PROCESS
THEN
OPENS
A
PSEUDO
TERMINAL
DEVICE
AND
SPLITS
INTO
TWO
PROCESSES
USING
FORK
THE
PARENT
HANDLES
THE
COMMUNICATION
ACROSS
THE
NETWORK
CONNECTION
AND
THE
CHILD
DOES
AN
EXEC
OF
THE
LOGIN
PROGRAM
THE
PARENT
AND
THE
CHILD
ARE
CONNECTED
THROUGH
THE
PSEUDO
TERMINAL
BEFORE
DOING
THE
EXEC
THE
CHILD
SETS
UP
FILE
DESCRIPTORS
AND
TO
THE
PSEUDO
TERMINAL
IF
WE
LOG
IN
CORRECTLY
LOGIN
PERFORMS
THE
SAME
STEPS
WE
DESCRIBED
IN
SECTION
IT
CHANGES
TO
OUR
HOME
DIRECTORY
AND
SETS
OUR
GROUP
IDS
USER
ID
AND
OUR
INITIAL
ENVIRONMENT
THEN
LOGIN
REPLACES
ITSELF
WITH
OUR
LOGIN
SHELL
BY
CALLING
EXEC
FIGURE
SHOWS
THE
ARRANGEMENT
OF
THE
PROCESSES
AT
THIS
POINT
PROCESS
ID
FD
THROUGH
INETD
TELNETD
AND
LOGIN
NETWORK
CONNECTION
THROUGH
TELNETD
SERVER
AND
TELNET
CLIENT
FIGURE
ARRANGEMENT
OF
PROCESSES
AFTER
EVERYTHING
IS
SET
FOR
A
NETWORK
LOGIN
OBVIOUSLY
A
LOT
IS
GOING
ON
BETWEEN
THE
PSEUDO
TERMINAL
DEVICE
DRIVER
AND
THE
ACTUAL
USER
AT
THE
TERMINAL
WE
LL
SHOW
ALL
THE
PROCESSES
INVOLVED
IN
THIS
TYPE
OF
ARRANGEMENT
IN
CHAPTER
WHEN
WE
TALK
ABOUT
PSEUDO
TERMINALS
IN
MORE
DETAIL
THE
IMPORTANT
THING
TO
UNDERSTAND
IS
THAT
WHETHER
WE
LOG
IN
THROUGH
A
TERMINAL
FIGURE
OR
A
NETWORK
FIGURE
WE
HAVE
A
LOGIN
SHELL
WITH
ITS
STANDARD
INPUT
STANDARD
OUTPUT
AND
STANDARD
ERROR
CONNECTED
TO
EITHER
A
TERMINAL
DEVICE
OR
A
PSEUDO
TERMINAL
DEVICE
WE
LL
SEE
IN
THE
COMING
SECTIONS
THAT
THIS
LOGIN
SHELL
IS
THE
START
OF
A
POSIX
SESSION
AND
THAT
THE
TERMINAL
OR
PSEUDO
TERMINAL
IS
THE
CONTROLLING
TERMINAL
FOR
THE
SESSION
MAC
OS
X
NETWORK
LOGINS
LOGGING
IN
TO
A
MAC
OS
X
SYSTEM
OVER
A
NETWORK
IS
IDENTICAL
TO
LOGGING
IN
TO
A
BSD
SYSTEM
BECAUSE
MAC
OS
X
IS
BASED
PARTIALLY
ON
FREEBSD
HOWEVER
ON
MAC
OS
X
THE
TELNET
DAEMON
IS
RUN
FROM
LAUNCHD
BY
DEFAULT
THE
TELNET
DAEMON
IS
DISABLED
ON
MAC
OS
X
ALTHOUGH
IT
CAN
BE
ENABLED
WITH
THE
LAUNCHCTL
COMMAND
THE
PREFERRED
WAY
TO
PERFORM
A
NETWORK
LOGIN
ON
MAC
OS
X
IS
WITH
SSH
THE
SECURE
SHELL
COMMAND
LINUX
NETWORK
LOGINS
NETWORK
LOGINS
UNDER
LINUX
ARE
THE
SAME
AS
UNDER
BSD
EXCEPT
THAT
SOME
DISTRIBUTIONS
USE
AN
ALTERNATIVE
INETD
PROCESS
CALLED
THE
EXTENDED
INTERNET
SERVICES
DAEMON
XINETD
THE
XINETD
PROCESS
PROVIDES
A
FINER
LEVEL
OF
CONTROL
OVER
SERVICES
IT
STARTS
COMPARED
TO
INETD
SOLARIS
NETWORK
LOGINS
THE
SCENARIO
FOR
NETWORK
LOGINS
UNDER
SOLARIS
IS
ALMOST
IDENTICAL
TO
THE
STEPS
UNDER
BSD
AND
LINUX
AN
INETD
SERVER
IS
USED
THAT
IS
SIMILAR
IN
CONCEPT
TO
THE
BSD
VERSION
EXCEPT
THAT
THE
SOLARIS
VERSION
RUNS
AS
A
RESTARTER
IN
THE
SERVICE
MANAGEMENT
FACILITY
SMF
A
RESTARTER
IS
A
DAEMON
THAT
HAS
THE
RESPONSIBILITY
TO
START
AND
MONITOR
OTHER
DAEMON
PROCESSES
AND
RESTART
THEM
IF
THEY
FAIL
ALTHOUGH
THE
INETD
SERVER
IS
STARTED
BY
THE
MASTER
RESTARTER
IN
THE
SMF
THE
MASTER
RESTARTER
IS
STARTED
BY
INIT
AND
WE
END
UP
WITH
THE
SAME
OVERALL
PICTURE
AS
IN
FIGURE
THE
SOLARIS
SERVICE
MANAGEMENT
FACILITY
IS
A
FRAMEWORK
THAT
MANAGES
AND
MONITORS
SYSTEM
SERVICES
AND
PROVIDES
A
WAY
TO
RECOVER
FROM
FAILURES
AFFECTING
SYSTEM
SERVICES
FOR
MORE
DETAILS
ON
THE
SERVICE
MANAGEMENT
FACILITY
SEE
ADAMS
AND
THE
SOLARIS
MANUAL
PAGES
SMF
AND
INETD
PROCESS
GROUPS
IN
ADDITION
TO
HAVING
A
PROCESS
ID
EACH
PROCESS
BELONGS
TO
A
PROCESS
GROUP
WE
LL
ENCOUNTER
PROCESS
GROUPS
AGAIN
WHEN
WE
DISCUSS
SIGNALS
IN
CHAPTER
A
PROCESS
GROUP
IS
A
COLLECTION
OF
ONE
OR
MORE
PROCESSES
USUALLY
ASSOCIATED
WITH
THE
SAME
JOB
JOB
CONTROL
IS
DISCUSSED
IN
SECTION
THAT
CAN
RECEIVE
SIGNALS
FROM
THE
SAME
TERMINAL
EACH
PROCESS
GROUP
HAS
A
UNIQUE
PROCESS
GROUP
ID
PROCESS
GROUP
IDS
ARE
SIMILAR
TO
PROCESS
IDS
THEY
ARE
POSITIVE
INTEGERS
AND
CAN
BE
STORED
IN
A
DATA
TYPE
THE
FUNCTION
GETPGRP
RETURNS
THE
PROCESS
GROUP
ID
OF
THE
CALLING
PROCESS
IN
OLDER
BSD
DERIVED
SYSTEMS
THE
GETPGRP
FUNCTION
TOOK
A
PID
ARGUMENT
AND
RETURNED
THE
PROCESS
GROUP
FOR
THAT
PROCESS
THE
SINGLE
UNIX
SPECIFICATION
DEFINES
THE
GETPGID
FUNCTION
THAT
MIMICS
THIS
BEHAVIOR
INCLUDE
UNISTD
H
GETPGID
PID
RETURNS
PROCESS
GROUP
ID
IF
OK
ON
ERROR
IF
PID
IS
THE
PROCESS
GROUP
ID
OF
THE
CALLING
PROCESS
IS
RETURNED
THUS
GETPGID
IS
EQUIVALENT
TO
GETPGRP
EACH
PROCESS
GROUP
CAN
HAVE
A
PROCESS
GROUP
LEADER
THE
LEADER
IS
IDENTIFIED
BY
ITS
PROCESS
GROUP
ID
BEING
EQUAL
TO
ITS
PROCESS
ID
IT
IS
POSSIBLE
FOR
A
PROCESS
GROUP
LEADER
TO
CREATE
A
PROCESS
GROUP
CREATE
PROCESSES
IN
THE
GROUP
AND
THEN
TERMINATE
THE
PROCESS
GROUP
STILL
EXISTS
AS
LONG
AS
AT
LEAST
ONE
PROCESS
IS
IN
THE
GROUP
REGARDLESS
OF
WHETHER
THE
GROUP
LEADER
TERMINATES
THIS
IS
CALLED
THE
PROCESS
GROUP
LIFETIME
THE
PERIOD
OF
TIME
THAT
BEGINS
WHEN
THE
GROUP
IS
CREATED
AND
ENDS
WHEN
THE
LAST
REMAINING
PROCESS
LEAVES
THE
GROUP
THE
LAST
REMAINING
PROCESS
IN
THE
PROCESS
GROUP
CAN
EITHER
TERMINATE
OR
ENTER
SOME
OTHER
PROCESS
GROUP
A
PROCESS
JOINS
AN
EXISTING
PROCESS
GROUP
OR
CREATES
A
NEW
PROCESS
GROUP
BY
CALLING
SETPGID
IN
THE
NEXT
SECTION
WE
LL
SEE
THAT
SETSID
ALSO
CREATES
A
NEW
PROCESS
GROUP
THIS
FUNCTION
SETS
THE
PROCESS
GROUP
ID
TO
PGID
IN
THE
PROCESS
WHOSE
PROCESS
ID
EQUALS
PID
IF
THE
TWO
ARGUMENTS
ARE
EQUAL
THE
PROCESS
SPECIFIED
BY
PID
BECOMES
A
PROCESS
GROUP
LEADER
IF
PID
IS
THE
PROCESS
ID
OF
THE
CALLER
IS
USED
ALSO
IF
PGID
IS
THE
PROCESS
ID
SPECIFIED
BY
PID
IS
USED
AS
THE
PROCESS
GROUP
ID
A
PROCESS
CAN
SET
THE
PROCESS
GROUP
ID
OF
ONLY
ITSELF
OR
ANY
OF
ITS
CHILDREN
FURTHERMORE
IT
CAN
T
CHANGE
THE
PROCESS
GROUP
ID
OF
ONE
OF
ITS
CHILDREN
AFTER
THAT
CHILD
HAS
CALLED
ONE
OF
THE
EXEC
FUNCTIONS
IN
MOST
JOB
CONTROL
SHELLS
THIS
FUNCTION
IS
CALLED
AFTER
A
FORK
TO
HAVE
THE
PARENT
SET
THE
PROCESS
GROUP
ID
OF
THE
CHILD
AND
TO
HAVE
THE
CHILD
SET
ITS
OWN
PROCESS
GROUP
ID
ONE
OF
THESE
CALLS
IS
REDUNDANT
BUT
BY
DOING
BOTH
WE
ARE
GUARANTEED
THAT
THE
CHILD
IS
PLACED
INTO
ITS
OWN
PROCESS
GROUP
BEFORE
EITHER
PROCESS
ASSUMES
THAT
THIS
HAS
HAPPENED
IF
WE
DIDN
T
DO
THIS
WE
WOULD
HAVE
A
RACE
CONDITION
SINCE
THE
CHILD
PROCESS
GROUP
MEMBERSHIP
WOULD
DEPEND
ON
WHICH
PROCESS
EXECUTES
FIRST
WHEN
WE
DISCUSS
SIGNALS
WE
LL
SEE
HOW
WE
CAN
SEND
A
SIGNAL
TO
EITHER
A
SINGLE
PROCESS
IDENTIFIED
BY
ITS
PROCESS
ID
OR
A
PROCESS
GROUP
IDENTIFIED
BY
ITS
PROCESS
GROUP
ID
SIMILARLY
THE
WAITPID
FUNCTION
FROM
SECTION
LETS
US
WAIT
FOR
EITHER
A
SINGLE
PROCESS
OR
ONE
PROCESS
FROM
A
SPECIFIED
PROCESS
GROUP
SESSIONS
A
SESSION
IS
A
COLLECTION
OF
ONE
OR
MORE
PROCESS
GROUPS
FOR
EXAMPLE
WE
COULD
HAVE
THE
ARRANGEMENT
SHOWN
IN
FIGURE
HERE
WE
HAVE
THREE
PROCESS
GROUPS
IN
A
SINGLE
SESSION
PROCESS
GROUP
PROCESS
GROUP
PROCESS
GROUP
SESSION
FIGURE
ARRANGEMENT
OF
PROCESSES
INTO
PROCESS
GROUPS
AND
SESSIONS
THE
PROCESSES
IN
A
PROCESS
GROUP
ARE
USUALLY
PLACED
THERE
BY
A
SHELL
PIPELINE
FOR
EXAMPLE
THE
ARRANGEMENT
SHOWN
IN
FIGURE
COULD
HAVE
BEEN
GENERATED
BY
SHELL
COMMANDS
OF
THE
FORM
A
PROCESS
ESTABLISHES
A
NEW
SESSION
BY
CALLING
THE
SETSID
FUNCTION
IF
THE
CALLING
PROCESS
IS
NOT
A
PROCESS
GROUP
LEADER
THIS
FUNCTION
CREATES
A
NEW
SESSION
THREE
THINGS
HAPPEN
THE
PROCESS
BECOMES
THE
SESSION
LEADER
OF
THIS
NEW
SESSION
A
SESSION
LEADER
IS
THE
PROCESS
THAT
CREATES
A
SESSION
THE
PROCESS
IS
THE
ONLY
PROCESS
IN
THIS
NEW
SESSION
THE
PROCESS
BECOMES
THE
PROCESS
GROUP
LEADER
OF
A
NEW
PROCESS
GROUP
THE
NEW
PROCESS
GROUP
ID
IS
THE
PROCESS
ID
OF
THE
CALLING
PROCESS
THE
PROCESS
HAS
NO
CONTROLLING
TERMINAL
WE
LL
DISCUSS
CONTROLLING
TERMINALS
IN
THE
NEXT
SECTION
IF
THE
PROCESS
HAD
A
CONTROLLING
TERMINAL
BEFORE
CALLING
SETSID
THAT
ASSOCIATION
IS
BROKEN
THIS
FUNCTION
RETURNS
AN
ERROR
IF
THE
CALLER
IS
ALREADY
A
PROCESS
GROUP
LEADER
TO
ENSURE
THIS
IS
NOT
THE
CASE
THE
USUAL
PRACTICE
IS
TO
CALL
FORK
AND
HAVE
THE
PARENT
TERMINATE
AND
THE
CHILD
CONTINUE
WE
ARE
GUARANTEED
THAT
THE
CHILD
IS
NOT
A
PROCESS
GROUP
LEADER
BECAUSE
THE
PROCESS
GROUP
ID
OF
THE
PARENT
IS
INHERITED
BY
THE
CHILD
BUT
THE
CHILD
GETS
A
NEW
PROCESS
ID
HENCE
IT
IS
IMPOSSIBLE
FOR
THE
CHILD
PROCESS
ID
TO
EQUAL
ITS
INHERITED
PROCESS
GROUP
ID
THE
SINGLE
UNIX
SPECIFICATION
TALKS
ONLY
ABOUT
A
SESSION
LEADER
THERE
IS
NO
SESSION
ID
SIMILAR
TO
A
PROCESS
ID
OR
A
PROCESS
GROUP
ID
OBVIOUSLY
A
SESSION
LEADER
IS
A
SINGLE
PROCESS
THAT
HAS
A
UNIQUE
PROCESS
ID
SO
WE
COULD
TALK
ABOUT
A
SESSION
ID
THAT
IS
THE
PROCESS
ID
OF
THE
SESSION
LEADER
THIS
CONCEPT
OF
A
SESSION
ID
WAS
INTRODUCED
IN
HISTORICALLY
BSD
BASED
SYSTEMS
DIDN
T
SUPPORT
THIS
NOTION
BUT
HAVE
SINCE
BEEN
UPDATED
TO
INCLUDE
IT
THE
GETSID
FUNCTION
RETURNS
THE
PROCESS
GROUP
ID
OF
A
PROCESS
SESSION
LEADER
SOME
IMPLEMENTATIONS
SUCH
AS
SOLARIS
JOIN
WITH
THE
SINGLE
UNIX
SPECIFICATION
IN
THE
PRACTICE
OF
AVOIDING
THE
USE
OF
THE
PHRASE
SESSION
ID
OPTING
INSTEAD
TO
REFER
TO
THIS
AS
THE
PROCESS
GROUP
ID
OF
THE
SESSION
LEADER
THE
TWO
ARE
EQUIVALENT
SINCE
THE
SESSION
LEADER
IS
ALWAYS
THE
LEADER
OF
A
PROCESS
GROUP
IF
PID
IS
GETSID
RETURNS
THE
PROCESS
GROUP
ID
OF
THE
CALLING
PROCESS
SESSION
LEADER
FOR
SECURITY
REASONS
SOME
IMPLEMENTATIONS
MAY
RESTRICT
THE
CALLING
PROCESS
FROM
OBTAINING
THE
PROCESS
GROUP
ID
OF
THE
SESSION
LEADER
IF
PID
DOESN
T
BELONG
TO
THE
SAME
SESSION
AS
THE
CALLER
CONTROLLING
TERMINAL
SESSIONS
AND
PROCESS
GROUPS
HAVE
A
FEW
OTHER
CHARACTERISTICS
A
SESSION
CAN
HAVE
A
SINGLE
CONTROLLING
TERMINAL
THIS
IS
USUALLY
THE
TERMINAL
DEVICE
IN
THE
CASE
OF
A
TERMINAL
LOGIN
OR
PSEUDO
TERMINAL
DEVICE
IN
THE
CASE
OF
A
NETWORK
LOGIN
ON
WHICH
WE
LOG
IN
THE
SESSION
LEADER
THAT
ESTABLISHES
THE
CONNECTION
TO
THE
CONTROLLING
TERMINAL
IS
CALLED
THE
CONTROLLING
PROCESS
THE
PROCESS
GROUPS
WITHIN
A
SESSION
CAN
BE
DIVIDED
INTO
A
SINGLE
FOREGROUND
PROCESS
GROUP
AND
ONE
OR
MORE
BACKGROUND
PROCESS
GROUPS
IF
A
SESSION
HAS
A
CONTROLLING
TERMINAL
IT
HAS
A
SINGLE
FOREGROUND
PROCESS
GROUP
AND
ALL
OTHER
PROCESS
GROUPS
IN
THE
SESSION
ARE
BACKGROUND
PROCESS
GROUPS
WHENEVER
WE
PRESS
THE
TERMINAL
INTERRUPT
KEY
OFTEN
DELETE
OR
CONTROL
C
THE
INTERRUPT
SIGNAL
IS
SENT
TO
ALL
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
WHENEVER
WE
PRESS
THE
TERMINAL
QUIT
KEY
OFTEN
CONTROL
BACKSLASH
THE
QUIT
SIGNAL
IS
SENT
TO
ALL
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
IF
A
MODEM
OR
NETWORK
DISCONNECT
IS
DETECTED
BY
THE
TERMINAL
INTERFACE
THE
HANG
UP
SIGNAL
IS
SENT
TO
THE
CONTROLLING
PROCESS
THE
SESSION
LEADER
THESE
CHARACTERISTICS
ARE
SHOWN
IN
FIGURE
SESSION
BACKGROUND
PROCESS
GROUP
SESSION
LEADER
CONTROLLING
PROCESS
BACKGROUND
PROCESS
GROUP
FOREGROUND
PROCESS
GROUP
CONTROLLING
TERMINAL
FIGURE
PROCESS
GROUPS
AND
SESSIONS
SHOWING
CONTROLLING
TERMINAL
USUALLY
WE
DON
T
HAVE
TO
WORRY
ABOUT
THE
CONTROLLING
TERMINAL
IT
IS
ESTABLISHED
AUTOMATICALLY
WHEN
WE
LOG
IN
POSIX
LEAVES
THE
CHOICE
OF
THE
MECHANISM
USED
TO
ALLOCATE
A
CONTROLLING
TERMINAL
UP
TO
EACH
INDIVIDUAL
IMPLEMENTATION
WE
LL
SHOW
THE
ACTUAL
STEPS
IN
SECTION
SYSTEMS
DERIVED
FROM
UNIX
SYSTEM
V
ALLOCATE
THE
CONTROLLING
TERMINAL
FOR
A
SESSION
WHEN
THE
SESSION
LEADER
OPENS
THE
FIRST
TERMINAL
DEVICE
THAT
IS
NOT
ALREADY
ASSOCIATED
WITH
A
SESSION
AS
LONG
AS
THE
CALL
TO
OPEN
DOES
NOT
SPECIFY
THE
FLAG
SECTION
BSD
BASED
SYSTEMS
ALLOCATE
THE
CONTROLLING
TERMINAL
FOR
A
SESSION
WHEN
THE
SESSION
LEADER
CALLS
IOCTL
WITH
A
REQUEST
ARGUMENT
OF
TIOCSCTTY
THE
THIRD
ARGUMENT
IS
A
NULL
POINTER
THE
SESSION
CANNOT
ALREADY
HAVE
A
CONTROLLING
TERMINAL
FOR
THIS
CALL
TO
SUCCEED
NORMALLY
THIS
CALL
TO
IOCTL
FOLLOWS
A
CALL
TO
SETSID
WHICH
GUARANTEES
THAT
THE
PROCESS
IS
A
SESSION
LEADER
WITHOUT
A
CONTROLLING
TERMINAL
THE
POSIX
FLAG
TO
OPEN
IS
NOT
USED
BY
BSD
BASED
SYSTEMS
EXCEPT
IN
COMPATIBILITY
MODE
SUPPORT
FOR
OTHER
SYSTEMS
FIGURE
SUMMARIZES
THE
WAY
EACH
PLATFORM
DISCUSSED
IN
THIS
BOOK
ALLOCATES
A
CONTROLLING
TERMINAL
NOTE
THAT
ALTHOUGH
MAC
OS
X
IS
DERIVED
FROM
BSD
IT
BEHAVES
LIKE
SYSTEM
V
WHEN
ALLOCATING
A
CONTROLLING
TERMINAL
METHOD
FREEBSD
LINUX
MAC
OS
X
SOLARIS
OPEN
WITHOUT
TIOCSCTTY
IOCTL
COMMAND
FIGURE
HOW
VARIOUS
IMPLEMENTATIONS
ALLOCATE
CONTROLLING
TERMINALS
THERE
ARE
TIMES
WHEN
A
PROGRAM
WANTS
TO
TALK
TO
THE
CONTROLLING
TERMINAL
REGARDLESS
OF
WHETHER
THE
STANDARD
INPUT
OR
STANDARD
OUTPUT
IS
REDIRECTED
THE
WAY
A
PROGRAM
GUARANTEES
THAT
IT
IS
TALKING
TO
THE
CONTROLLING
TERMINAL
IS
TO
OPEN
THE
FILE
DEV
TTY
THIS
SPECIAL
FILE
IS
A
SYNONYM
WITHIN
THE
KERNEL
FOR
THE
CONTROLLING
TERMINAL
NATURALLY
IF
THE
PROGRAM
DOESN
T
HAVE
A
CONTROLLING
TERMINAL
THE
OPEN
OF
THIS
DEVICE
WILL
FAIL
THE
CLASSIC
EXAMPLE
IS
THE
GETPASS
FUNCTION
WHICH
READS
A
PASSWORD
WITH
TERMINAL
ECHOING
TURNED
OFF
OF
COURSE
THIS
FUNCTION
IS
CALLED
BY
THE
CRYPT
PROGRAM
AND
CAN
BE
USED
IN
A
PIPELINE
FOR
EXAMPLE
CRYPT
SALARIES
LPR
DECRYPTS
THE
FILE
SALARIES
AND
PIPES
THE
OUTPUT
TO
THE
PRINT
SPOOLER
BECAUSE
CRYPT
READS
ITS
INPUT
FILE
ON
ITS
STANDARD
INPUT
THE
STANDARD
INPUT
CAN
T
BE
USED
TO
ENTER
THE
PASSWORD
ALSO
CRYPT
IS
DESIGNED
SO
THAT
WE
HAVE
TO
ENTER
THE
ENCRYPTION
PASSWORD
EACH
TIME
WE
RUN
THE
PROGRAM
TO
PREVENT
US
FROM
SAVING
THE
PASSWORD
IN
A
FILE
WHICH
COULD
BE
A
SECURITY
HOLE
THERE
ARE
KNOWN
WAYS
TO
BREAK
THE
ENCODING
USED
BY
THE
CRYPT
PROGRAM
SEE
GARFINKEL
ET
AL
FOR
MORE
DETAILS
ON
ENCRYPTING
FILES
TCGETPGRP
TCSETPGRP
AND
TCGETSID
FUNCTIONS
WE
NEED
A
WAY
TO
TELL
THE
KERNEL
WHICH
PROCESS
GROUP
IS
THE
FOREGROUND
PROCESS
GROUP
SO
THAT
THE
TERMINAL
DEVICE
DRIVER
KNOWS
WHERE
TO
SEND
THE
TERMINAL
INPUT
AND
THE
TERMINAL
GENERATED
SIGNALS
FIGURE
THE
FUNCTION
TCGETPGRP
RETURNS
THE
PROCESS
GROUP
ID
OF
THE
FOREGROUND
PROCESS
GROUP
ASSOCIATED
WITH
THE
TERMINAL
OPEN
ON
FD
IF
THE
PROCESS
HAS
A
CONTROLLING
TERMINAL
THE
PROCESS
CAN
CALL
TCSETPGRP
TO
SET
THE
FOREGROUND
PROCESS
GROUP
ID
TO
PGRPID
THE
VALUE
OF
PGRPID
MUST
BE
THE
PROCESS
GROUP
ID
OF
A
PROCESS
GROUP
IN
THE
SAME
SESSION
AND
FD
MUST
REFER
TO
THE
CONTROLLING
TERMINAL
OF
THE
SESSION
MOST
APPLICATIONS
DON
T
CALL
THESE
TWO
FUNCTIONS
DIRECTLY
INSTEAD
THE
FUNCTIONS
ARE
NORMALLY
CALLED
BY
JOB
CONTROL
SHELLS
THE
TCGETSID
FUNCTION
ALLOWS
AN
APPLICATION
TO
OBTAIN
THE
PROCESS
GROUP
ID
FOR
THE
SESSION
LEADER
GIVEN
A
FILE
DESCRIPTOR
FOR
THE
CONTROLLING
TTY
APPLICATIONS
THAT
NEED
TO
MANAGE
CONTROLLING
TERMINALS
CAN
USE
TCGETSID
TO
IDENTIFY
THE
SESSION
ID
OF
THE
CONTROLLING
TERMINAL
SESSION
LEADER
WHICH
IS
EQUIVALENT
TO
THE
SESSION
LEADER
PROCESS
GROUP
ID
JOB
CONTROL
JOB
CONTROL
IS
A
FEATURE
THAT
WAS
ADDED
TO
BSD
AROUND
THIS
FEATURE
ALLOWS
US
TO
START
MULTIPLE
JOBS
GROUPS
OF
PROCESSES
FROM
A
SINGLE
TERMINAL
AND
TO
CONTROL
WHICH
JOBS
CAN
ACCESS
THE
TERMINAL
AND
WHICH
JOBS
ARE
RUN
IN
THE
BACKGROUND
JOB
CONTROL
REQUIRES
THREE
FORMS
OF
SUPPORT
A
SHELL
THAT
SUPPORTS
JOB
CONTROL
THE
TERMINAL
DRIVER
IN
THE
KERNEL
MUST
SUPPORT
JOB
CONTROL
THE
KERNEL
MUST
SUPPORT
CERTAIN
JOB
CONTROL
SIGNALS
PROVIDED
A
DIFFERENT
FORM
OF
JOB
CONTROL
CALLED
SHELL
LAYERS
THE
BSD
FORM
OF
JOB
CONTROL
HOWEVER
WAS
SELECTED
BY
POSIX
AND
IS
WHAT
WE
DESCRIBE
HERE
IN
EARLIER
VERSIONS
OF
THE
STANDARD
JOB
CONTROL
SUPPORT
WAS
OPTIONAL
BUT
POSIX
NOW
REQUIRES
PLATFORMS
TO
SUPPORT
IT
FROM
OUR
PERSPECTIVE
WHEN
USING
JOB
CONTROL
FROM
A
SHELL
WE
CAN
START
A
JOB
IN
EITHER
THE
FOREGROUND
OR
THE
BACKGROUND
A
JOB
IS
SIMPLY
A
COLLECTION
OF
PROCESSES
OFTEN
A
PIPELINE
OF
PROCESSES
FOR
EXAMPLE
VI
MAIN
C
STARTS
A
JOB
CONSISTING
OF
ONE
PROCESS
IN
THE
FOREGROUND
THE
COMMANDS
PR
C
LPR
MAKE
ALL
START
TWO
JOBS
IN
THE
BACKGROUND
ALL
THE
PROCESSES
INVOKED
BY
THESE
BACKGROUND
JOBS
ARE
IN
THE
BACKGROUND
AS
WE
SAID
TO
USE
THE
FEATURES
PROVIDED
BY
JOB
CONTROL
WE
NEED
TO
USE
A
SHELL
THAT
SUPPORTS
JOB
CONTROL
WITH
OLDER
SYSTEMS
IT
WAS
SIMPLE
TO
SAY
WHICH
SHELLS
SUPPORTED
JOB
CONTROL
AND
WHICH
DIDN
T
THE
C
SHELL
SUPPORTED
JOB
CONTROL
THE
BOURNE
SHELL
DIDN
T
AND
IT
WAS
AN
OPTION
WITH
THE
KORN
SHELL
DEPENDING
ON
WHETHER
THE
HOST
SUPPORTED
JOB
CONTROL
BUT
THE
C
SHELL
HAS
BEEN
PORTED
TO
SYSTEMS
E
G
EARLIER
VERSIONS
OF
SYSTEM
V
THAT
DON
T
SUPPORT
JOB
CONTROL
AND
THE
BOURNE
SHELL
WHEN
INVOKED
BY
THE
NAME
JSH
INSTEAD
OF
SH
SUPPORTS
JOB
CONTROL
THE
KORN
SHELL
CONTINUES
TO
SUPPORT
JOB
CONTROL
IF
THE
HOST
DOES
THE
BOURNE
AGAIN
SHELL
ALSO
SUPPORTS
JOB
CONTROL
WE
LL
JUST
TALK
GENERICALLY
ABOUT
A
SHELL
THAT
SUPPORTS
JOB
CONTROL
VERSUS
ONE
THAT
DOESN
T
WHEN
THE
DIFFERENCE
BETWEEN
THE
VARIOUS
SHELLS
DOESN
T
MATTER
WHEN
WE
START
A
BACKGROUND
JOB
THE
SHELL
ASSIGNS
IT
A
JOB
IDENTIFIER
AND
PRINTS
ONE
OR
MORE
OF
THE
PROCESS
IDS
THE
FOLLOWING
SCRIPT
SHOWS
HOW
THE
KORN
SHELL
HANDLES
THIS
MAKE
ALL
MAKE
OUT
PR
C
LPR
JUST
PRESS
RETURN
DONE
PR
C
LPR
DONE
MAKE
ALL
MAKE
OUT
THE
MAKE
IS
JOB
NUMBER
AND
THE
STARTING
PROCESS
ID
IS
THE
NEXT
PIPELINE
IS
JOB
NUMBER
AND
THE
PROCESS
ID
OF
THE
FIRST
PROCESS
IS
WHEN
THE
JOBS
ARE
DONE
AND
WE
PRESS
RETURN
THE
SHELL
TELLS
US
THAT
THE
JOBS
ARE
COMPLETE
THE
REASON
WE
HAVE
TO
PRESS
RETURN
IS
TO
HAVE
THE
SHELL
PRINT
ITS
PROMPT
THE
SHELL
DOESN
T
PRINT
THE
CHANGED
STATUS
OF
BACKGROUND
JOBS
AT
ANY
RANDOM
TIME
ONLY
RIGHT
BEFORE
IT
PRINTS
ITS
PROMPT
TO
LET
US
ENTER
A
NEW
COMMAND
LINE
IF
THE
SHELL
DIDN
T
DO
THIS
IT
COULD
PRODUCE
OUTPUT
WHILE
WE
WERE
ENTERING
AN
INPUT
LINE
THE
INTERACTION
WITH
THE
TERMINAL
DRIVER
ARISES
BECAUSE
A
SPECIAL
TERMINAL
CHARACTER
AFFECTS
THE
FOREGROUND
JOB
THE
SUSPEND
KEY
TYPICALLY
CONTROL
Z
ENTERING
THIS
CHARACTER
CAUSES
THE
TERMINAL
DRIVER
TO
SEND
THE
SIGTSTP
SIGNAL
TO
ALL
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
THE
JOBS
IN
ANY
BACKGROUND
PROCESS
GROUPS
AREN
T
AFFECTED
THE
TERMINAL
DRIVER
LOOKS
FOR
THREE
SPECIAL
CHARACTERS
WHICH
GENERATE
SIGNALS
TO
THE
FOREGROUND
PROCESS
GROUP
THE
INTERRUPT
CHARACTER
TYPICALLY
DELETE
OR
CONTROL
C
GENERATES
SIGINT
THE
QUIT
CHARACTER
TYPICALLY
CONTROL
BACKSLASH
GENERATES
SIGQUIT
THE
SUSPEND
CHARACTER
TYPICALLY
CONTROL
Z
GENERATES
SIGTSTP
IN
CHAPTER
WE
LL
SEE
HOW
WE
CAN
CHANGE
THESE
THREE
CHARACTERS
TO
BE
ANY
CHARACTERS
WE
CHOOSE
AND
HOW
WE
CAN
DISABLE
THE
TERMINAL
DRIVER
PROCESSING
OF
THESE
SPECIAL
CHARACTERS
ANOTHER
JOB
CONTROL
CONDITION
CAN
ARISE
THAT
MUST
BE
HANDLED
BY
THE
TERMINAL
DRIVER
SINCE
WE
CAN
HAVE
A
FOREGROUND
JOB
AND
ONE
OR
MORE
BACKGROUND
JOBS
WHICH
OF
THESE
RECEIVES
THE
CHARACTERS
THAT
WE
ENTER
AT
THE
TERMINAL
ONLY
THE
FOREGROUND
JOB
RECEIVES
TERMINAL
INPUT
IT
IS
NOT
AN
ERROR
FOR
A
BACKGROUND
JOB
TO
TRY
TO
READ
FROM
THE
TERMINAL
BUT
THE
TERMINAL
DRIVER
DETECTS
THIS
AND
SENDS
A
SPECIAL
SIGNAL
TO
THE
BACKGROUND
JOB
SIGTTIN
THIS
SIGNAL
NORMALLY
STOPS
THE
BACKGROUND
JOB
BY
USING
THE
SHELL
WE
ARE
NOTIFIED
OF
THIS
EVENT
AND
CAN
BRING
THE
JOB
INTO
THE
FOREGROUND
SO
THAT
IT
CAN
READ
FROM
THE
TERMINAL
THE
FOLLOWING
EXAMPLE
DEMONSTRATES
THIS
CAT
TEMP
FOO
START
IN
BACKGROUND
BUT
IT
LL
READ
FROM
STANDARD
INPUT
WE
PRESS
RETURN
STOPPED
SIGTTIN
CAT
TEMP
FOO
FG
BRING
JOB
NUMBER
INTO
THE
FOREGROUND
CAT
TEMP
FOO
THE
SHELL
TELLS
US
WHICH
JOB
IS
NOW
IN
THE
FOREGROUND
HELLO
WORLD
ENTER
ONE
LINE
ˆD
TYPE
THE
END
OF
FILE
CHARACTER
CAT
TEMP
FOO
CHECK
THAT
THE
ONE
LINE
WAS
PUT
INTO
THE
FILE
HELLO
WORLD
NOTE
THAT
THIS
EXAMPLE
DOESN
T
WORK
ON
MAC
OS
X
WHEN
WE
TRY
TO
BRING
THE
CAT
COMMAND
INTO
THE
FOREGROUND
THE
READ
FAILS
WITH
ERRNO
SET
TO
EINTR
SINCE
MAC
OS
X
IS
BASED
ON
FREEBSD
AND
FREEBSD
WORKS
AS
EXPECTED
THIS
MUST
BE
A
BUG
IN
MAC
OS
X
THE
SHELL
STARTS
THE
CAT
PROCESS
IN
THE
BACKGROUND
BUT
WHEN
CAT
TRIES
TO
READ
ITS
STANDARD
INPUT
THE
CONTROLLING
TERMINAL
THE
TERMINAL
DRIVER
KNOWING
THAT
IT
IS
A
BACKGROUND
JOB
SENDS
THE
SIGTTIN
SIGNAL
TO
THE
BACKGROUND
JOB
THE
SHELL
DETECTS
THIS
CHANGE
IN
STATUS
OF
ITS
CHILD
RECALL
OUR
DISCUSSION
OF
THE
WAIT
AND
WAITPID
FUNCTION
IN
SECTION
AND
TELLS
US
THAT
THE
JOB
HAS
BEEN
STOPPED
WE
THEN
MOVE
THE
STOPPED
JOB
INTO
THE
FOREGROUND
WITH
THE
SHELL
FG
COMMAND
REFER
TO
THE
MANUAL
PAGE
FOR
THE
SHELL
THAT
YOU
ARE
USING
FOR
ALL
THE
DETAILS
ON
ITS
JOB
CONTROL
COMMANDS
SUCH
AS
FG
AND
BG
AND
THE
VARIOUS
WAYS
TO
IDENTIFY
THE
DIFFERENT
JOBS
DOING
THIS
CAUSES
THE
SHELL
TO
PLACE
THE
JOB
INTO
THE
FOREGROUND
PROCESS
GROUP
TCSETPGRP
AND
SEND
THE
CONTINUE
SIGNAL
SIGCONT
TO
THE
PROCESS
GROUP
SINCE
IT
IS
NOW
IN
THE
FOREGROUND
PROCESS
GROUP
THE
JOB
CAN
READ
FROM
THE
CONTROLLING
TERMINAL
WHAT
HAPPENS
IF
A
BACKGROUND
JOB
SENDS
ITS
OUTPUT
TO
THE
CONTROLLING
TERMINAL
THIS
IS
AN
OPTION
THAT
WE
CAN
ALLOW
OR
DISALLOW
NORMALLY
WE
USE
THE
STTY
COMMAND
TO
CHANGE
THIS
OPTION
WE
LL
SEE
IN
CHAPTER
HOW
WE
CAN
CHANGE
THIS
OPTION
FROM
A
PROGRAM
THE
FOLLOWING
EXAMPLE
SHOWS
HOW
THIS
WORKS
CAT
TEMP
FOO
EXECUTE
IN
BACKGROUND
HELLO
WORLD
THE
OUTPUT
FROM
THE
BACKGROUND
JOB
APPEARS
AFTER
THE
PROMPT
WE
PRESS
RETURN
DONE
CAT
TEMP
FOO
STTY
TOSTOP
DISABLE
ABILITY
OF
BACKGROUND
JOBS
TO
OUTPUT
TO
CONTROLLING
TERMINAL
CAT
TEMP
FOO
TRY
IT
AGAIN
IN
THE
BACKGROUND
WE
PRESS
RETURN
AND
FIND
THE
JOB
IS
STOPPED
STOPPED
SIGTTOU
CAT
TEMP
FOO
FG
RESUME
STOPPED
JOB
IN
THE
FOREGROUND
CAT
TEMP
FOO
THE
SHELL
TELLS
US
WHICH
JOB
IS
NOW
IN
THE
FOREGROUND
HELLO
WORLD
AND
HERE
IS
ITS
OUTPUT
WHEN
WE
DISALLOW
BACKGROUND
JOBS
FROM
WRITING
TO
THE
CONTROLLING
TERMINAL
CAT
WILL
BLOCK
WHEN
IT
TRIES
TO
WRITE
TO
ITS
STANDARD
OUTPUT
BECAUSE
THE
TERMINAL
DRIVER
IDENTIFIES
THE
WRITE
AS
COMING
FROM
A
BACKGROUND
PROCESS
AND
SENDS
THE
JOB
THE
SIGTTOU
SIGNAL
AS
WITH
THE
PREVIOUS
EXAMPLE
WHEN
WE
USE
THE
SHELL
FG
COMMAND
TO
BRING
THE
JOB
INTO
THE
FOREGROUND
THE
JOB
COMPLETES
FIGURE
SUMMARIZES
SOME
OF
THE
FEATURES
OF
JOB
CONTROL
THAT
WE
VE
BEEN
DESCRIBING
THE
SOLID
LINES
THROUGH
THE
TERMINAL
DRIVER
BOX
MEAN
THAT
THE
TERMINAL
I
O
AND
THE
TERMINAL
GENERATED
SIGNALS
ARE
ALWAYS
CONNECTED
FROM
THE
FOREGROUND
PROCESS
INIT
INETD
OR
LAUNCHD
EXEC
AFTER
SETSID
THEN
ESTABLISHING
CONTROLLING
TERMINAL
LOGIN
EXEC
SESSION
FIGURE
SUMMARY
OF
JOB
CONTROL
FEATURES
WITH
FOREGROUND
AND
BACKGROUND
JOBS
AND
TERMINAL
DRIVER
GROUP
TO
THE
ACTUAL
TERMINAL
THE
DASHED
LINE
CORRESPONDING
TO
THE
SIGTTOU
SIGNAL
MEANS
THAT
WHETHER
THE
OUTPUT
FROM
A
PROCESS
IN
THE
BACKGROUND
PROCESS
GROUP
APPEARS
ON
THE
TERMINAL
IS
AN
OPTION
IS
JOB
CONTROL
NECESSARY
OR
DESIRABLE
JOB
CONTROL
WAS
ORIGINALLY
DESIGNED
AND
IMPLEMENTED
BEFORE
WINDOWING
TERMINALS
WERE
WIDESPREAD
SOME
PEOPLE
CLAIM
THAT
A
WELL
DESIGNED
WINDOWING
SYSTEM
REMOVES
ANY
NEED
FOR
JOB
CONTROL
SOME
COMPLAIN
THAT
THE
IMPLEMENTATION
OF
JOB
CONTROL
REQUIRING
SUPPORT
FROM
THE
KERNEL
THE
TERMINAL
DRIVER
THE
SHELL
AND
SOME
APPLICATIONS
IS
A
HACK
SOME
USE
JOB
CONTROL
WITH
A
WINDOWING
SYSTEM
CLAIMING
A
NEED
FOR
BOTH
REGARDLESS
OF
YOUR
OPINION
JOB
CONTROL
IS
A
REQUIRED
FEATURE
OF
POSIX
SHELL
EXECUTION
OF
PROGRAMS
LET
EXAMINE
HOW
THE
SHELLS
EXECUTE
PROGRAMS
AND
HOW
THIS
RELATES
TO
THE
CONCEPTS
OF
PROCESS
GROUPS
CONTROLLING
TERMINALS
AND
SESSIONS
TO
DO
THIS
WE
LL
USE
THE
PS
COMMAND
AGAIN
FIRST
WE
LL
USE
A
SHELL
THAT
DOESN
T
SUPPORT
JOB
CONTROL
THE
CLASSIC
BOURNE
SHELL
RUNNING
ON
SOLARIS
IF
WE
EXECUTE
PS
O
PID
PPID
PGID
SID
COMM
THE
OUTPUT
IS
PID
PPID
PGID
SID
COMMAND
SH
PS
THE
PARENT
OF
THE
PS
COMMAND
IS
THE
SHELL
WHICH
WE
WOULD
EXPECT
BOTH
THE
SHELL
AND
THE
PS
COMMAND
ARE
IN
THE
SAME
SESSION
AND
FOREGROUND
PROCESS
GROUP
WE
SAY
THAT
IS
THE
FOREGROUND
PROCESS
GROUP
BECAUSE
THAT
IS
WHAT
YOU
GET
WHEN
YOU
EXECUTE
A
COMMAND
WITH
A
SHELL
THAT
DOESN
T
SUPPORT
JOB
CONTROL
SOME
PLATFORMS
SUPPORT
AN
OPTION
TO
HAVE
THE
PS
COMMAND
PRINT
THE
PROCESS
GROUP
ID
ASSOCIATED
WITH
THE
SESSION
CONTROLLING
TERMINAL
THIS
VALUE
WOULD
BE
SHOWN
UNDER
THE
TPGID
COLUMN
UNFORTUNATELY
THE
OUTPUT
OF
THE
PS
COMMAND
OFTEN
DIFFERS
AMONG
VERSIONS
OF
THE
UNIX
SYSTEM
FOR
EXAMPLE
SOLARIS
DOESN
T
SUPPORT
THIS
OPTION
UNDER
FREEBSD
LINUX
AND
MAC
OS
X
THE
COMMAND
PS
O
PID
PPID
PGID
SID
TPGID
COMM
PRINTS
EXACTLY
THE
INFORMATION
WE
WANT
NOTE
THAT
IT
IS
MISLEADING
TO
ASSOCIATE
A
PROCESS
WITH
A
TERMINAL
PROCESS
GROUP
ID
THE
TPGID
COLUMN
A
PROCESS
DOES
NOT
HAVE
A
TERMINAL
PROCESS
CONTROL
GROUP
A
PROCESS
BELONGS
TO
A
PROCESS
GROUP
AND
THE
PROCESS
GROUP
BELONGS
TO
A
SESSION
THE
SESSION
MAY
OR
MAY
NOT
HAVE
A
CONTROLLING
TERMINAL
IF
THE
SESSION
DOES
HAVE
A
CONTROLLING
TERMINAL
THEN
THE
TERMINAL
DEVICE
KNOWS
THE
PROCESS
GROUP
ID
OF
THE
FOREGROUND
PROCESS
THIS
VALUE
CAN
BE
SET
IN
THE
TERMINAL
DRIVER
WITH
THE
TCSETPGRP
FUNCTION
AS
WE
SHOW
IN
FIGURE
THE
FOREGROUND
PROCESS
GROUP
ID
IS
AN
ATTRIBUTE
OF
THE
TERMINAL
NOT
THE
PROCESS
THIS
VALUE
FROM
THE
TERMINAL
DEVICE
DRIVER
IS
WHAT
PS
PRINTS
AS
THE
TPGID
IF
IT
FINDS
THAT
THE
SESSION
DOESN
T
HAVE
A
CONTROLLING
TERMINAL
PS
PRINTS
EITHER
OR
DEPENDING
ON
THE
PLATFORM
IF
WE
EXECUTE
THE
COMMAND
IN
THE
BACKGROUND
PS
O
PID
PPID
PGID
SID
COMM
THE
ONLY
VALUE
THAT
CHANGES
IS
THE
PROCESS
ID
OF
THE
COMMAND
PID
PPID
PGID
SID
COMMAND
SH
PS
THIS
SHELL
DOESN
T
KNOW
ABOUT
JOB
CONTROL
SO
THE
BACKGROUND
JOB
IS
NOT
PUT
INTO
ITS
OWN
PROCESS
GROUP
AND
THE
CONTROLLING
TERMINAL
ISN
T
TAKEN
AWAY
FROM
THE
BACKGROUND
JOB
NOW
LET
LOOK
AT
HOW
THE
BOURNE
SHELL
HANDLES
A
PIPELINE
WHEN
WE
EXECUTE
PS
O
PID
PPID
PGID
SID
COMM
THE
OUTPUT
IS
PID
PPID
PGID
SID
COMMAND
SH
PS
THE
PROGRAM
IS
JUST
A
COPY
OF
THE
STANDARD
CAT
PROGRAM
WITH
A
DIFFERENT
NAME
WE
HAVE
ANOTHER
COPY
OF
CAT
WITH
THE
NAME
WHICH
WE
LL
USE
LATER
IN
THIS
SECTION
WHEN
WE
HAVE
TWO
COPIES
OF
CAT
IN
A
PIPELINE
THE
DIFFERENT
NAMES
LET
US
DIFFERENTIATE
BETWEEN
THE
TWO
PROGRAMS
NOTE
THAT
THE
LAST
PROCESS
IN
THE
PIPELINE
IS
THE
CHILD
OF
THE
SHELL
AND
THAT
THE
FIRST
PROCESS
IN
THE
PIPELINE
IS
A
CHILD
OF
THE
LAST
PROCESS
IT
APPEARS
THAT
THE
SHELL
FORKS
A
COPY
OF
ITSELF
AND
THAT
THIS
COPY
THEN
FORKS
TO
MAKE
EACH
OF
THE
PREVIOUS
PROCESSES
IN
THE
PIPELINE
IF
WE
EXECUTE
THE
PIPELINE
IN
THE
BACKGROUND
PS
O
PID
PPID
PGID
SID
COMM
ONLY
THE
PROCESS
IDS
CHANGE
SINCE
THE
SHELL
DOESN
T
HANDLE
JOB
CONTROL
THE
PROCESS
GROUP
ID
OF
THE
BACKGROUND
PROCESSES
REMAINS
AS
DOES
THE
PROCESS
GROUP
ID
OF
THE
SESSION
WHAT
HAPPENS
IN
THIS
CASE
IF
A
BACKGROUND
PROCESS
TRIES
TO
READ
FROM
ITS
CONTROLLING
TERMINAL
FOR
EXAMPLE
SUPPOSE
THAT
WE
EXECUTE
CAT
TEMP
FOO
WITH
JOB
CONTROL
THIS
IS
HANDLED
BY
PLACING
THE
BACKGROUND
JOB
INTO
A
BACKGROUND
PROCESS
GROUP
WHICH
CAUSES
THE
SIGNAL
SIGTTIN
TO
BE
GENERATED
IF
THE
BACKGROUND
JOB
TRIES
TO
READ
FROM
THE
CONTROLLING
TERMINAL
THE
WAY
THIS
IS
HANDLED
WITHOUT
JOB
CONTROL
IS
THAT
THE
SHELL
AUTOMATICALLY
REDIRECTS
THE
STANDARD
INPUT
OF
A
BACKGROUND
PROCESS
TO
DEV
NULL
IF
THE
PROCESS
DOESN
T
REDIRECT
STANDARD
INPUT
ITSELF
A
READ
FROM
DEV
NULL
GENERATES
AN
END
OF
FILE
THIS
MEANS
THAT
OUR
BACKGROUND
CAT
PROCESS
IMMEDIATELY
READS
AN
END
OF
FILE
AND
TERMINATES
NORMALLY
THE
PREVIOUS
PARAGRAPH
ADEQUATELY
HANDLES
THE
CASE
OF
A
BACKGROUND
PROCESS
ACCESSING
THE
CONTROLLING
TERMINAL
THROUGH
ITS
STANDARD
INPUT
BUT
WHAT
HAPPENS
IF
A
BACKGROUND
PROCESS
SPECIFICALLY
OPENS
DEV
TTY
AND
READS
FROM
THE
CONTROLLING
TERMINAL
THE
ANSWER
IS
IT
DEPENDS
BUT
THE
RESULT
IS
PROBABLY
NOT
WHAT
WE
WANT
FOR
EXAMPLE
CRYPT
SALARIES
LPR
IS
SUCH
A
PIPELINE
WE
RUN
IT
IN
THE
BACKGROUND
BUT
THE
CRYPT
PROGRAM
OPENS
DEV
TTY
CHANGES
THE
TERMINAL
CHARACTERISTICS
TO
DISABLE
ECHOING
READS
FROM
THE
DEVICE
AND
RESETS
THE
TERMINAL
CHARACTERISTICS
WHEN
WE
EXECUTE
THIS
BACKGROUND
PIPELINE
THE
PROMPT
PASSWORD
FROM
CRYPT
IS
PRINTED
ON
THE
TERMINAL
BUT
WHAT
WE
ENTER
THE
ENCRYPTION
PASSWORD
IS
READ
BY
THE
SHELL
WHICH
TRIES
TO
EXECUTE
A
COMMAND
OF
THAT
NAME
THE
NEXT
LINE
WE
ENTER
TO
THE
SHELL
IS
TAKEN
AS
THE
PASSWORD
AND
THE
FILE
IS
NOT
ENCRYPTED
CORRECTLY
SENDING
JUNK
TO
THE
PRINTER
HERE
WE
HAVE
TWO
PROCESSES
TRYING
TO
READ
FROM
THE
SAME
DEVICE
AT
THE
SAME
TIME
AND
THE
RESULT
DEPENDS
ON
THE
SYSTEM
JOB
CONTROL
AS
WE
DESCRIBED
EARLIER
HANDLES
THIS
MULTIPLEXING
OF
A
SINGLE
TERMINAL
BETWEEN
MULTIPLE
PROCESSES
IN
A
BETTER
FASHION
RETURNING
TO
OUR
BOURNE
SHELL
EXAMPLE
IF
WE
EXECUTE
THREE
PROCESSES
IN
THE
PIPELINE
WE
CAN
EXAMINE
THE
PROCESS
CONTROL
USED
BY
THIS
SHELL
PS
O
PID
PPID
PGID
SID
COMM
THIS
PIPELINE
GENERATES
THE
FOLLOWING
OUTPUT
PID
PPID
PGID
SID
COMMAND
SH
PS
DON
T
BE
ALARMED
IF
THE
OUTPUT
ON
YOUR
SYSTEM
DOESN
T
SHOW
THE
PROPER
COMMAND
NAMES
SOMETIMES
YOU
MIGHT
GET
RESULTS
SUCH
AS
PID
PPID
PGID
SID
COMMAND
SH
SH
PS
SH
WHAT
HAPPENING
HERE
IS
THAT
THE
PS
PROCESS
IS
RACING
WITH
THE
SHELL
WHICH
IS
FORKING
AND
EXECUTING
THE
CAT
COMMANDS
IN
THIS
CASE
THE
SHELL
HASN
T
YET
COMPLETED
THE
CALL
TO
EXEC
WHEN
PS
HAS
OBTAINED
THE
LIST
OF
PROCESSES
TO
PRINT
AGAIN
THE
LAST
PROCESS
IN
THE
PIPELINE
IS
THE
CHILD
OF
THE
SHELL
AND
ALL
PREVIOUS
PROCESSES
IN
THE
PIPELINE
ARE
CHILDREN
OF
THE
LAST
PROCESS
FIGURE
SHOWS
WHAT
IS
HAPPENING
FORK
C
EXEC
FIGURE
PROCESSES
IN
THE
PIPELINE
PS
WHEN
INVOKED
BY
BOURNE
SHELL
SINCE
THE
LAST
PROCESS
IN
THE
PIPELINE
IS
THE
CHILD
OF
THE
LOGIN
SHELL
THE
SHELL
IS
NOTIFIED
WHEN
THAT
PROCESS
TERMINATES
NOW
LET
EXAMINE
THE
SAME
EXAMPLES
USING
A
JOB
CONTROL
SHELL
RUNNING
ON
LINUX
THIS
SHOWS
THE
WAY
THESE
SHELLS
HANDLE
BACKGROUND
JOBS
WE
LL
USE
THE
BOURNE
AGAIN
SHELL
IN
THIS
EXAMPLE
THE
RESULTS
WITH
OTHER
JOB
CONTROL
SHELLS
ARE
ALMOST
IDENTICAL
PS
O
PID
PPID
PGID
SID
TPGID
COMM
GIVES
US
PID
PPID
PGID
SID
TPGID
COMMAND
BASH
PS
STARTING
WITH
THIS
EXAMPLE
WE
SHOW
THE
FOREGROUND
PROCESS
GROUP
IN
A
BOLDER
FONT
WE
IMMEDIATELY
SEE
A
DIFFERENCE
FROM
OUR
BOURNE
SHELL
EXAMPLE
THE
BOURNE
AGAIN
SHELL
PLACES
THE
FOREGROUND
JOB
PS
INTO
ITS
OWN
PROCESS
GROUP
THE
PS
COMMAND
IS
THE
PROCESS
GROUP
LEADER
AND
THE
ONLY
PROCESS
IN
THIS
PROCESS
GROUP
FURTHERMORE
THIS
PROCESS
GROUP
IS
THE
FOREGROUND
PROCESS
GROUP
SINCE
IT
HAS
THE
CONTROLLING
TERMINAL
OUR
LOGIN
SHELL
IS
A
BACKGROUND
PROCESS
GROUP
WHILE
THE
PS
COMMAND
EXECUTES
NOTE
HOWEVER
THAT
BOTH
PROCESS
GROUPS
AND
ARE
MEMBERS
OF
THE
SAME
SESSION
INDEED
WE
LL
SEE
THAT
THE
SESSION
NEVER
CHANGES
THROUGH
OUR
EXAMPLES
IN
THIS
SECTION
EXECUTING
THIS
PROCESS
IN
THE
BACKGROUND
PS
O
PID
PPID
PGID
SID
TPGID
COMM
GIVES
US
PID
PPID
PGID
SID
TPGID
COMMAND
BASH
PS
AGAIN
THE
PS
COMMAND
IS
PLACED
INTO
ITS
OWN
PROCESS
GROUP
BUT
THIS
TIME
THE
PROCESS
GROUP
IS
NO
LONGER
THE
FOREGROUND
PROCESS
GROUP
IT
IS
A
BACKGROUND
PROCESS
GROUP
THE
TPGID
OF
INDICATES
THAT
THE
FOREGROUND
PROCESS
GROUP
IS
OUR
LOGIN
SHELL
EXECUTING
TWO
PROCESSES
IN
A
PIPELINE
AS
IN
PS
O
PID
PPID
PGID
SID
TPGID
COMM
GIVES
US
PID
PPID
PGID
SID
TPGID
COMMAND
BASH
PS
BOTH
PROCESSES
PS
AND
ARE
PLACED
INTO
A
NEW
PROCESS
GROUP
AND
THIS
IS
THE
FOREGROUND
PROCESS
GROUP
WE
CAN
ALSO
SEE
ANOTHER
DIFFERENCE
BETWEEN
THIS
EXAMPLE
AND
THE
SIMILAR
BOURNE
SHELL
EXAMPLE
THE
BOURNE
SHELL
CREATED
THE
LAST
PROCESS
IN
THE
PIPELINE
FIRST
AND
THIS
FINAL
PROCESS
WAS
THE
PARENT
OF
THE
FIRST
PROCESS
HERE
THE
BOURNE
AGAIN
SHELL
IS
THE
PARENT
OF
BOTH
PROCESSES
IF
WE
EXECUTE
THIS
PIPELINE
IN
THE
BACKGROUND
PS
O
PID
PPID
PGID
SID
TPGID
COMM
THE
RESULTS
ARE
SIMILAR
BUT
NOW
PS
AND
ARE
PLACED
IN
THE
SAME
BACKGROUND
PROCESS
GROUP
PID
PPID
PGID
SID
TPGID
COMMAND
BASH
PS
NOTE
THAT
THE
ORDER
IN
WHICH
A
SHELL
CREATES
PROCESSES
CAN
DIFFER
DEPENDING
ON
THE
PARTICULAR
SHELL
IN
USE
ORPHANED
PROCESS
GROUPS
WE
VE
MENTIONED
THAT
A
PROCESS
WHOSE
PARENT
TERMINATES
IS
CALLED
AN
ORPHAN
AND
IS
INHERITED
BY
THE
INIT
PROCESS
WE
NOW
LOOK
AT
ENTIRE
PROCESS
GROUPS
THAT
CAN
BE
ORPHANED
AND
SEE
HOW
POSIX
HANDLES
THIS
SITUATION
EXAMPLE
CONSIDER
A
PROCESS
THAT
FORKS
A
CHILD
AND
THEN
TERMINATES
ALTHOUGH
THIS
IS
NOTHING
ABNORMAL
IT
HAPPENS
ALL
THE
TIME
WHAT
HAPPENS
IF
THE
CHILD
IS
STOPPED
USING
JOB
CONTROL
WHEN
THE
PARENT
TERMINATES
HOW
WILL
THE
CHILD
EVER
BE
CONTINUED
AND
DOES
THE
CHILD
KNOW
THAT
IT
HAS
BEEN
ORPHANED
FIGURE
SHOWS
THIS
SITUATION
THE
PARENT
PROCESS
HAS
FORKED
A
CHILD
THAT
STOPS
AND
THE
PARENT
IS
ABOUT
TO
EXIT
SESSION
FIGURE
EXAMPLE
OF
A
PROCESS
GROUP
ABOUT
TO
BE
ORPHANED
THE
PROGRAM
THAT
CREATES
THIS
SITUATION
IS
SHOWN
IN
FIGURE
THIS
PROGRAM
HAS
SOME
NEW
FEATURES
HERE
WE
ARE
ASSUMING
A
JOB
CONTROL
SHELL
RECALL
FROM
THE
PREVIOUS
SECTION
THAT
THE
SHELL
PLACES
THE
FOREGROUND
PROCESS
INTO
ITS
OWN
PROCESS
GROUP
IN
INCLUDE
APUE
H
INCLUDE
ERRNO
H
STATIC
VOID
INT
SIGNO
PRINTF
SIGHUP
RECEIVED
PID
LD
N
LONG
GETPID
STATIC
VOID
CHAR
NAME
INT
PRINTF
PID
LD
PPID
LD
PGRP
LD
TPGRP
LD
N
NAME
LONG
GETPID
LONG
GETPPID
LONG
GETPGRP
LONG
TCGETPGRP
FFLUSH
STDOUT
MAIN
VOID
CHAR
C
PID
PARENT
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
PARENT
SLEEP
SLEEP
TO
LET
CHILD
STOP
ITSELF
ELSE
CHILD
CHILD
SIGNAL
SIGHUP
ESTABLISH
SIGNAL
HANDLER
KILL
GETPID
SIGTSTP
STOP
OURSELF
CHILD
PRINTS
ONLY
IF
WE
RE
CONTINUED
IF
READ
C
PRINTF
READ
ERROR
D
ON
CONTROLLING
TTY
N
ERRNO
EXIT
FIGURE
CREATING
AN
ORPHANED
PROCESS
GROUP
THIS
EXAMPLE
AND
THAT
THE
SHELL
STAYS
IN
ITS
OWN
PROCESS
GROUP
THE
CHILD
INHERITS
THE
PROCESS
GROUP
OF
ITS
PARENT
AFTER
THE
FORK
THE
PARENT
SLEEPS
FOR
SECONDS
THIS
IS
OUR
IMPERFECT
WAY
OF
LETTING
THE
CHILD
EXECUTE
BEFORE
THE
PARENT
TERMINATES
THE
CHILD
ESTABLISHES
A
SIGNAL
HANDLER
FOR
THE
HANG
UP
SIGNAL
SIGHUP
SO
WE
CAN
SEE
WHETHER
IT
IS
SENT
TO
THE
CHILD
WE
DISCUSS
SIGNAL
HANDLERS
IN
CHAPTER
THE
CHILD
SENDS
ITSELF
THE
STOP
SIGNAL
SIGTSTP
WITH
THE
KILL
FUNCTION
THIS
STOPS
THE
CHILD
SIMILAR
TO
OUR
STOPPING
A
FOREGROUND
JOB
WITH
OUR
TERMINAL
SUSPEND
CHARACTER
CONTROL
Z
WHEN
THE
PARENT
TERMINATES
THE
CHILD
IS
ORPHANED
SO
THE
CHILD
PARENT
PROCESS
ID
BECOMES
WHICH
IS
THE
INIT
PROCESS
ID
AT
THIS
POINT
THE
CHILD
IS
NOW
A
MEMBER
OF
AN
ORPHANED
PROCESS
GROUP
THE
POSIX
DEFINITION
OF
AN
ORPHANED
PROCESS
GROUP
IS
ONE
IN
WHICH
THE
PARENT
OF
EVERY
MEMBER
IS
EITHER
ITSELF
A
MEMBER
OF
THE
GROUP
OR
IS
NOT
A
MEMBER
OF
THE
GROUP
SESSION
ANOTHER
WAY
OF
SAYING
THIS
IS
THAT
THE
PROCESS
GROUP
IS
NOT
ORPHANED
AS
LONG
AS
A
PROCESS
IN
THE
GROUP
HAS
A
PARENT
IN
A
DIFFERENT
PROCESS
GROUP
BUT
IN
THE
SAME
SESSION
IF
THE
PROCESS
GROUP
IS
NOT
ORPHANED
THERE
IS
A
CHANCE
THAT
ONE
OF
THOSE
PARENTS
IN
A
DIFFERENT
PROCESS
GROUP
BUT
IN
THE
SAME
SESSION
WILL
RESTART
A
STOPPED
PROCESS
IN
THE
PROCESS
GROUP
THAT
IS
NOT
ORPHANED
HERE
THE
PARENT
OF
EVERY
PROCESS
IN
THE
GROUP
E
G
PROCESS
IS
THE
PARENT
OF
PROCESS
BELONGS
TO
ANOTHER
SESSION
SINCE
THE
PROCESS
GROUP
IS
ORPHANED
WHEN
THE
PARENT
TERMINATES
AND
THE
PROCESS
GROUP
CONTAINS
A
STOPPED
PROCESS
POSIX
REQUIRES
THAT
EVERY
PROCESS
IN
THE
NEWLY
ORPHANED
PROCESS
GROUP
BE
SENT
THE
HANG
UP
SIGNAL
SIGHUP
FOLLOWED
BY
THE
CONTINUE
SIGNAL
SIGCONT
THIS
CAUSES
THE
CHILD
TO
BE
CONTINUED
AFTER
PROCESSING
THE
HANG
UP
SIGNAL
THE
DEFAULT
ACTION
FOR
THE
HANG
UP
SIGNAL
IS
TO
TERMINATE
THE
PROCESS
SO
WE
HAVE
TO
PROVIDE
A
SIGNAL
HANDLER
TO
CATCH
THE
SIGNAL
WE
THEREFORE
EXPECT
THE
PRINTF
IN
THE
FUNCTION
TO
APPEAR
BEFORE
THE
PRINTF
IN
THE
FUNCTION
HERE
IS
THE
OUTPUT
FROM
THE
PROGRAM
SHOWN
IN
FIGURE
A
OUT
PARENT
PID
PPID
PGRP
TPGRP
CHILD
PID
PPID
PGRP
TPGRP
SIGHUP
RECEIVED
PID
CHILD
PID
PPID
PGRP
TPGRP
READ
ERROR
ON
CONTROLLING
TTY
NOTE
THAT
OUR
SHELL
PROMPT
APPEARS
WITH
THE
OUTPUT
FROM
THE
CHILD
SINCE
TWO
PROCESSES
OUR
LOGIN
SHELL
AND
THE
CHILD
ARE
WRITING
TO
THE
TERMINAL
AS
WE
EXPECT
THE
PARENT
PROCESS
ID
OF
THE
CHILD
HAS
BECOME
AFTER
CALLING
IN
THE
CHILD
THE
PROGRAM
TRIES
TO
READ
FROM
STANDARD
INPUT
AS
WE
SAW
EARLIER
IN
THIS
CHAPTER
WHEN
A
PROCESS
IN
A
BACKGROUND
PROCESS
GROUP
TRIES
TO
READ
FROM
ITS
CONTROLLING
TERMINAL
SIGTTIN
IS
GENERATED
FOR
THE
BACKGROUND
PROCESS
GROUP
BUT
HERE
WE
HAVE
AN
ORPHANED
PROCESS
GROUP
IF
THE
KERNEL
WERE
TO
STOP
IT
WITH
THIS
SIGNAL
THE
PROCESSES
IN
THE
PROCESS
GROUP
WOULD
PROBABLY
NEVER
BE
CONTINUED
POSIX
SPECIFIES
THAT
THE
READ
IS
TO
RETURN
AN
ERROR
WITH
ERRNO
SET
TO
EIO
WHOSE
VALUE
IS
ON
THIS
SYSTEM
IN
THIS
SITUATION
FINALLY
NOTE
THAT
OUR
CHILD
WAS
PLACED
IN
A
BACKGROUND
PROCESS
GROUP
WHEN
THE
PARENT
TERMINATED
SINCE
THE
PARENT
WAS
EXECUTED
AS
A
FOREGROUND
JOB
BY
THE
SHELL
WE
LL
SEE
ANOTHER
EXAMPLE
OF
ORPHANED
PROCESS
GROUPS
IN
SECTION
WITH
THE
PTY
PROGRAM
FREEBSD
IMPLEMENTATION
HAVING
TALKED
ABOUT
THE
VARIOUS
ATTRIBUTES
OF
A
PROCESS
PROCESS
GROUP
SESSION
AND
CONTROLLING
TERMINAL
IT
WORTH
LOOKING
AT
HOW
ALL
THIS
CAN
BE
IMPLEMENTED
WE
LL
LOOK
BRIEFLY
AT
THE
IMPLEMENTATION
USED
BY
FREEBSD
SOME
DETAILS
OF
THE
IMPLEMENTATION
OF
THESE
FEATURES
CAN
BE
FOUND
IN
WILLIAMS
FIGURE
SHOWS
THE
VARIOUS
DATA
STRUCTURES
USED
BY
FREEBSD
TTY
STRUCTURE
LINKED
LIST
OF
PROCESS
GROUP
MEMBERS
PGRP
STRUCTURE
SESSION
STRUCTURE
VNODE
STRUCTURE
PROC
STRUCTURE
PROC
STRUCTURE
PROC
STRUCTURE
FIGURE
FREEBSD
IMPLEMENTATION
OF
SESSIONS
AND
PROCESS
GROUPS
LET
LOOK
AT
ALL
THE
FIELDS
THAT
WE
VE
LABELED
STARTING
WITH
THE
SESSION
STRUCTURE
ONE
OF
THESE
STRUCTURES
IS
ALLOCATED
FOR
EACH
SESSION
E
G
EACH
TIME
SETSID
IS
CALLED
IS
THE
NUMBER
OF
PROCESS
GROUPS
IN
THE
SESSION
WHEN
THIS
COUNTER
IS
DECREMENTED
TO
THE
STRUCTURE
CAN
BE
FREED
IS
A
POINTER
TO
THE
PROC
STRUCTURE
OF
THE
SESSION
LEADER
IS
A
POINTER
TO
THE
VNODE
STRUCTURE
OF
THE
CONTROLLING
TERMINAL
IS
A
POINTER
TO
THE
TTY
STRUCTURE
OF
THE
CONTROLLING
TERMINAL
IS
THE
SESSION
ID
RECALL
THAT
THE
CONCEPT
OF
A
SESSION
ID
IS
NOT
PART
OF
THE
SINGLE
UNIX
SPECIFICATION
WHEN
SETSID
IS
CALLED
A
NEW
SESSION
STRUCTURE
IS
ALLOCATED
WITHIN
THE
KERNEL
NOW
IS
SET
TO
IS
SET
TO
POINT
TO
THE
PROC
STRUCTURE
OF
THE
CALLING
PROCESS
IS
SET
TO
THE
PROCESS
ID
AND
AND
ARE
SET
TO
NULL
POINTERS
SINCE
THE
NEW
SESSION
DOESN
T
HAVE
A
CONTROLLING
TERMINAL
LET
MOVE
TO
THE
TTY
STRUCTURE
THE
KERNEL
CONTAINS
ONE
OF
THESE
STRUCTURES
FOR
EACH
TERMINAL
DEVICE
AND
EACH
PSEUDO
TERMINAL
DEVICE
WE
TALK
MORE
ABOUT
PSEUDO
TERMINALS
IN
CHAPTER
POINTS
TO
THE
SESSION
STRUCTURE
THAT
HAS
THIS
TERMINAL
AS
ITS
CONTROLLING
TERMINAL
NOTE
THAT
THE
TTY
STRUCTURE
POINTS
TO
THE
SESSION
STRUCTURE
AND
VICE
VERSA
THIS
POINTER
IS
USED
BY
THE
TERMINAL
TO
SEND
A
HANG
UP
SIGNAL
TO
THE
SESSION
LEADER
IF
THE
TERMINAL
LOSES
CARRIER
FIGURE
POINTS
TO
THE
PGRP
STRUCTURE
OF
THE
FOREGROUND
PROCESS
GROUP
THIS
FIELD
IS
USED
BY
THE
TERMINAL
DRIVER
TO
SEND
SIGNALS
TO
THE
FOREGROUND
PROCESS
GROUP
THE
THREE
SIGNALS
GENERATED
BY
ENTERING
SPECIAL
CHARACTERS
INTERRUPT
QUIT
AND
SUSPEND
ARE
SENT
TO
THE
FOREGROUND
PROCESS
GROUP
IS
A
STRUCTURE
CONTAINING
ALL
THE
SPECIAL
CHARACTERS
AND
RELATED
INFORMATION
FOR
THIS
TERMINAL
SUCH
AS
BAUD
RATE
WHETHER
ECHO
IS
ENABLED
AND
SO
ON
WE
LL
RETURN
TO
THIS
STRUCTURE
IN
CHAPTER
IS
A
WINSIZE
STRUCTURE
THAT
CONTAINS
THE
CURRENT
SIZE
OF
THE
TERMINAL
WINDOW
WHEN
THE
SIZE
OF
THE
TERMINAL
WINDOW
CHANGES
THE
SIGWINCH
SIGNAL
IS
SENT
TO
THE
FOREGROUND
PROCESS
GROUP
WE
SHOW
HOW
TO
SET
AND
FETCH
THE
TERMINAL
CURRENT
WINDOW
SIZE
IN
SECTION
TO
FIND
THE
FOREGROUND
PROCESS
GROUP
OF
A
PARTICULAR
SESSION
THE
KERNEL
HAS
TO
START
WITH
THE
SESSION
STRUCTURE
FOLLOW
TO
GET
TO
THE
CONTROLLING
TERMINAL
TTY
STRUCTURE
AND
THEN
FOLLOW
TO
GET
TO
THE
FOREGROUND
PROCESS
GROUP
PGRP
STRUCTURE
THE
PGRP
STRUCTURE
CONTAINS
THE
INFORMATION
FOR
A
PARTICULAR
PROCESS
GROUP
IS
THE
PROCESS
GROUP
ID
POINTS
TO
THE
SESSION
STRUCTURE
FOR
THE
SESSION
TO
WHICH
THIS
PROCESS
GROUP
BELONGS
IS
A
POINTER
TO
THE
LIST
OF
PROC
STRUCTURES
THAT
ARE
MEMBERS
OF
THIS
PROCESS
GROUP
THE
STRUCTURE
IN
THAT
PROC
STRUCTURE
IS
A
DOUBLY
LINKED
LIST
ENTRY
THAT
POINTS
TO
BOTH
THE
NEXT
PROCESS
AND
THE
PREVIOUS
PROCESS
IN
THE
GROUP
AND
SO
ON
UNTIL
A
NULL
POINTER
IS
ENCOUNTERED
IN
THE
PROC
STRUCTURE
OF
THE
LAST
PROCESS
IN
THE
GROUP
THE
PROC
STRUCTURE
CONTAINS
ALL
THE
INFORMATION
FOR
A
SINGLE
PROCESS
CONTAINS
THE
PROCESS
ID
IS
A
POINTER
TO
THE
PROC
STRUCTURE
OF
THE
PARENT
PROCESS
POINTS
TO
THE
PGRP
STRUCTURE
OF
THE
PROCESS
GROUP
TO
WHICH
THIS
PROCESS
BELONGS
IS
A
STRUCTURE
CONTAINING
POINTERS
TO
THE
NEXT
AND
PREVIOUS
PROCESSES
IN
THE
PROCESS
GROUP
AS
WE
MENTIONED
EARLIER
FINALLY
WE
HAVE
THE
VNODE
STRUCTURE
THIS
STRUCTURE
IS
ALLOCATED
WHEN
THE
CONTROLLING
TERMINAL
DEVICE
IS
OPENED
ALL
REFERENCES
TO
DEV
TTY
IN
A
PROCESS
GO
THROUGH
THIS
VNODE
STRUCTURE
SUMMARY
THIS
CHAPTER
HAS
DESCRIBED
THE
RELATIONSHIPS
BETWEEN
GROUPS
OF
PROCESSES
SESSIONS
WHICH
ARE
MADE
UP
OF
PROCESS
GROUPS
JOB
CONTROL
IS
A
FEATURE
SUPPORTED
BY
MOST
UNIX
SYSTEMS
TODAY
AND
WE
VE
DESCRIBED
HOW
IT
IMPLEMENTED
BY
A
SHELL
THAT
SUPPORTS
JOB
CONTROL
THE
CONTROLLING
TERMINAL
FOR
A
PROCESS
DEV
TTY
IS
ALSO
INVOLVED
IN
THESE
PROCESS
RELATIONSHIPS
WE
VE
MADE
NUMEROUS
REFERENCES
TO
THE
SIGNALS
THAT
ARE
USED
IN
ALL
THESE
PROCESS
RELATIONSHIPS
THE
NEXT
CHAPTER
CONTINUES
THE
DISCUSSION
OF
SIGNALS
LOOKING
AT
ALL
THE
UNIX
SYSTEM
SIGNALS
IN
DETAIL
EXERCISES
REFER
BACK
TO
OUR
DISCUSSION
OF
THE
UTMP
AND
WTMP
FILES
IN
SECTION
WHY
ARE
THE
LOGOUT
RECORDS
WRITTEN
BY
THE
INIT
PROCESS
IS
THIS
HANDLED
THE
SAME
WAY
FOR
A
NETWORK
LOGIN
WRITE
A
SMALL
PROGRAM
THAT
CALLS
FORK
AND
HAS
THE
CHILD
CREATE
A
NEW
SESSION
VERIFY
THAT
THE
CHILD
BECOMES
A
PROCESS
GROUP
LEADER
AND
THAT
THE
CHILD
NO
LONGER
HAS
A
CONTROLLING
TERMINAL
SIGNALS
INTRODUCTION
SIGNALS
ARE
SOFTWARE
INTERRUPTS
MOST
NONTRIVIAL
APPLICATION
PROGRAMS
NEED
TO
DEAL
WITH
SIGNALS
SIGNALS
PROVIDE
A
WAY
OF
HANDLING
ASYNCHRONOUS
EVENTS
FOR
EXAMPLE
A
USER
AT
A
TERMINAL
TYPING
THE
INTERRUPT
KEY
TO
STOP
A
PROGRAM
OR
THE
NEXT
PROGRAM
IN
A
PIPELINE
TERMINATING
PREMATURELY
SIGNALS
HAVE
BEEN
PROVIDED
SINCE
THE
EARLY
VERSIONS
OF
THE
UNIX
SYSTEM
BUT
THE
SIGNAL
MODEL
PROVIDED
WITH
SYSTEMS
SUCH
AS
VERSION
WAS
NOT
RELIABLE
SIGNALS
COULD
GET
LOST
AND
IT
WAS
DIFFICULT
FOR
A
PROCESS
TO
TURN
OFF
SELECTED
SIGNALS
WHEN
EXECUTING
CRITICAL
REGIONS
OF
CODE
BOTH
AND
MADE
CHANGES
TO
THE
SIGNAL
MODEL
ADDING
WHAT
ARE
CALLED
RELIABLE
SIGNALS
BUT
THE
CHANGES
MADE
BY
BERKELEY
AND
AT
T
WERE
INCOMPATIBLE
FORTUNATELY
POSIX
STANDARDIZED
THE
RELIABLE
SIGNAL
ROUTINES
AND
THAT
IS
WHAT
WE
DESCRIBE
HERE
IN
THIS
CHAPTER
WE
START
WITH
AN
OVERVIEW
OF
SIGNALS
AND
A
DESCRIPTION
OF
WHAT
EACH
SIGNAL
IS
NORMALLY
USED
FOR
THEN
WE
LOOK
AT
THE
PROBLEMS
WITH
EARLIER
IMPLEMENTATIONS
IT
IS
OFTEN
IMPORTANT
TO
UNDERSTAND
WHAT
IS
WRONG
WITH
AN
IMPLEMENTATION
BEFORE
SEEING
HOW
TO
DO
THINGS
CORRECTLY
THIS
CHAPTER
CONTAINS
NUMEROUS
EXAMPLES
THAT
ARE
NOT
ENTIRELY
CORRECT
AND
A
DISCUSSION
OF
THE
DEFECTS
SIGNAL
CONCEPTS
FIRST
EVERY
SIGNAL
HAS
A
NAME
THESE
NAMES
ALL
BEGIN
WITH
THE
THREE
CHARACTERS
SIG
FOR
EXAMPLE
SIGABRT
IS
THE
ABORT
SIGNAL
THAT
IS
GENERATED
WHEN
A
PROCESS
CALLS
THE
ABORT
FUNCTION
SIGALRM
IS
THE
ALARM
SIGNAL
THAT
IS
GENERATED
WHEN
THE
TIMER
SET
BY
THE
ALARM
FUNCTION
GOES
OFF
VERSION
HAD
DIFFERENT
SIGNALS
AND
BOTH
HAD
DIFFERENT
SIGNALS
FREEBSD
SUPPORTS
DIFFERENT
SIGNALS
MAC
OS
X
AND
LINUX
EACH
SUPPORT
DIFFERENT
SIGNALS
WHEREAS
SOLARIS
SUPPORTS
DIFFERENT
SIGNALS
FREEBSD
LINUX
AND
SOLARIS
HOWEVER
SUPPORT
ADDITIONAL
APPLICATION
DEFINED
SIGNALS
INTRODUCED
TO
SUPPORT
REAL
TIME
APPLICATIONS
ALTHOUGH
THE
POSIX
REAL
TIME
EXTENSIONS
AREN
T
COVERED
IN
THIS
BOOK
REFER
TO
GALLMEISTER
FOR
MORE
INFORMATION
AS
OF
THE
REAL
TIME
SIGNAL
INTERFACES
HAVE
MOVED
TO
THE
BASE
SPECIFICATION
SIGNAL
NAMES
ARE
ALL
DEFINED
BY
POSITIVE
INTEGER
CONSTANTS
THE
SIGNAL
NUMBER
IN
THE
HEADER
SIGNAL
H
IMPLEMENTATIONS
ACTUALLY
DEFINE
THE
INDIVIDUAL
SIGNALS
IN
A
DIFFERENT
HEADER
FILE
BUT
THIS
HEADER
FILE
IS
INCLUDED
BY
SIGNAL
H
IT
IS
CONSIDERED
BAD
FORM
FOR
THE
KERNEL
TO
INCLUDE
HEADER
FILES
MEANT
FOR
USER
LEVEL
APPLICATIONS
SO
IF
THE
APPLICATIONS
AND
THE
KERNEL
BOTH
NEED
THE
SAME
DEFINITIONS
THE
INFORMATION
IS
PLACED
IN
A
KERNEL
HEADER
FILE
THAT
IS
THEN
INCLUDED
BY
THE
USER
LEVEL
HEADER
FILE
THUS
BOTH
FREEBSD
AND
MAC
OS
X
DEFINE
THE
SIGNALS
IN
SYS
SIGNAL
H
LINUX
DEFINES
THE
SIGNALS
IN
BITS
SIGNUM
H
AND
SOLARIS
DEFINES
THEM
IN
SYS
ISO
H
NO
SIGNAL
HAS
A
SIGNAL
NUMBER
OF
WE
LL
SEE
IN
SECTION
THAT
THE
KILL
FUNCTION
USES
THE
SIGNAL
NUMBER
OF
FOR
A
SPECIAL
CASE
POSIX
CALLS
THIS
VALUE
THE
NULL
SIGNAL
NUMEROUS
CONDITIONS
CAN
GENERATE
A
SIGNAL
THE
TERMINAL
GENERATED
SIGNALS
OCCUR
WHEN
USERS
PRESS
CERTAIN
TERMINAL
KEYS
PRESSING
THE
DELETE
KEY
ON
THE
TERMINAL
OR
CONTROL
C
ON
MANY
SYSTEMS
NORMALLY
CAUSES
THE
INTERRUPT
SIGNAL
SIGINT
TO
BE
GENERATED
THIS
IS
HOW
TO
STOP
A
RUNAWAY
PROGRAM
WE
LL
SEE
IN
CHAPTER
HOW
THIS
SIGNAL
CAN
BE
MAPPED
TO
ANY
CHARACTER
ON
THE
TERMINAL
HARDWARE
EXCEPTIONS
GENERATE
SIGNALS
DIVIDE
BY
INVALID
MEMORY
REFERENCE
AND
THE
LIKE
THESE
CONDITIONS
ARE
USUALLY
DETECTED
BY
THE
HARDWARE
AND
THE
KERNEL
IS
NOTIFIED
THE
KERNEL
THEN
GENERATES
THE
APPROPRIATE
SIGNAL
FOR
THE
PROCESS
THAT
WAS
RUNNING
AT
THE
TIME
THE
CONDITION
OCCURRED
FOR
EXAMPLE
SIGSEGV
IS
GENERATED
FOR
A
PROCESS
THAT
EXECUTES
AN
INVALID
MEMORY
REFERENCE
THE
KILL
FUNCTION
ALLOWS
A
PROCESS
TO
SEND
ANY
SIGNAL
TO
ANOTHER
PROCESS
OR
PROCESS
GROUP
NATURALLY
THERE
ARE
LIMITATIONS
WE
HAVE
TO
BE
THE
OWNER
OF
THE
PROCESS
THAT
WE
RE
SENDING
THE
SIGNAL
TO
OR
WE
HAVE
TO
BE
THE
SUPERUSER
THE
KILL
COMMAND
ALLOWS
US
TO
SEND
SIGNALS
TO
OTHER
PROCESSES
THIS
PROGRAM
IS
JUST
AN
INTERFACE
TO
THE
KILL
FUNCTION
THIS
COMMAND
IS
OFTEN
USED
TO
TERMINATE
A
RUNAWAY
BACKGROUND
PROCESS
SOFTWARE
CONDITIONS
CAN
GENERATE
SIGNALS
WHEN
A
PROCESS
SHOULD
BE
NOTIFIED
OF
VARIOUS
EVENTS
THESE
AREN
T
HARDWARE
GENERATED
CONDITIONS
AS
IS
THE
DIVIDE
BY
CONDITION
BUT
SOFTWARE
CONDITIONS
EXAMPLES
ARE
SIGURG
GENERATED
WHEN
OUT
OF
BAND
DATA
ARRIVES
OVER
A
NETWORK
CONNECTION
SIGPIPE
GENERATED
WHEN
A
PROCESS
WRITES
TO
A
PIPE
THAT
HAS
NO
READER
AND
SIGALRM
GENERATED
WHEN
AN
ALARM
CLOCK
SET
BY
THE
PROCESS
EXPIRES
SIGNALS
ARE
CLASSIC
EXAMPLES
OF
ASYNCHRONOUS
EVENTS
THEY
OCCUR
AT
WHAT
APPEAR
TO
BE
RANDOM
TIMES
TO
THE
PROCESS
THE
PROCESS
CAN
T
SIMPLY
TEST
A
VARIABLE
SUCH
AS
ERRNO
TO
SEE
WHETHER
A
SIGNAL
HAS
OCCURRED
INSTEAD
THE
PROCESS
HAS
TO
TELL
THE
KERNEL
IF
AND
WHEN
THIS
SIGNAL
OCCURS
DO
THE
FOLLOWING
WE
CAN
TELL
THE
KERNEL
TO
DO
ONE
OF
THREE
THINGS
WHEN
A
SIGNAL
OCCURS
WE
CALL
THIS
THE
DISPOSITION
OF
THE
SIGNAL
OR
THE
ACTION
ASSOCIATED
WITH
A
SIGNAL
IGNORE
THE
SIGNAL
THIS
WORKS
FOR
MOST
SIGNALS
BUT
TWO
SIGNALS
CAN
NEVER
BE
IGNORED
SIGKILL
AND
SIGSTOP
THE
REASON
THESE
TWO
SIGNALS
CAN
T
BE
IGNORED
IS
TO
PROVIDE
THE
KERNEL
AND
THE
SUPERUSER
WITH
A
SUREFIRE
WAY
OF
EITHER
KILLING
OR
STOPPING
ANY
PROCESS
ALSO
IF
WE
IGNORE
SOME
OF
THE
SIGNALS
THAT
ARE
GENERATED
BY
A
HARDWARE
EXCEPTION
SUCH
AS
ILLEGAL
MEMORY
REFERENCE
OR
DIVIDE
BY
THE
BEHAVIOR
OF
THE
PROCESS
IS
UNDEFINED
CATCH
THE
SIGNAL
TO
DO
THIS
WE
TELL
THE
KERNEL
TO
CALL
A
FUNCTION
OF
OURS
WHENEVER
THE
SIGNAL
OCCURS
IN
OUR
FUNCTION
WE
CAN
DO
WHATEVER
WE
WANT
TO
HANDLE
THE
CONDITION
IF
WE
RE
WRITING
A
COMMAND
INTERPRETER
FOR
EXAMPLE
WHEN
THE
USER
GENERATES
THE
INTERRUPT
SIGNAL
AT
THE
KEYBOARD
WE
PROBABLY
WANT
TO
RETURN
TO
THE
MAIN
LOOP
OF
THE
PROGRAM
TERMINATING
WHATEVER
COMMAND
WE
WERE
EXECUTING
FOR
THE
USER
IF
THE
SIGCHLD
SIGNAL
IS
CAUGHT
IT
MEANS
THAT
A
CHILD
PROCESS
HAS
TERMINATED
SO
THE
SIGNAL
CATCHING
FUNCTION
CAN
CALL
WAITPID
TO
FETCH
THE
CHILD
PROCESS
ID
AND
TERMINATION
STATUS
AS
ANOTHER
EXAMPLE
IF
THE
PROCESS
HAS
CREATED
TEMPORARY
FILES
WE
MAY
WANT
TO
WRITE
A
SIGNAL
CATCHING
FUNCTION
FOR
THE
SIGTERM
SIGNAL
THE
TERMINATION
SIGNAL
THAT
IS
THE
DEFAULT
SIGNAL
SENT
BY
THE
KILL
COMMAND
TO
CLEAN
UP
THE
TEMPORARY
FILES
NOTE
THAT
THE
TWO
SIGNALS
SIGKILL
AND
SIGSTOP
CAN
T
BE
CAUGHT
LET
THE
DEFAULT
ACTION
APPLY
EVERY
SIGNAL
HAS
A
DEFAULT
ACTION
SHOWN
IN
FIGURE
NOTE
THAT
THE
DEFAULT
ACTION
FOR
MOST
SIGNALS
IS
TO
TERMINATE
THE
PROCESS
FIGURE
LISTS
THE
NAMES
OF
ALL
THE
SIGNALS
AN
INDICATION
OF
WHICH
SYSTEMS
SUPPORT
THE
SIGNAL
AND
THE
DEFAULT
ACTION
FOR
THE
SIGNAL
THE
SUS
COLUMN
CONTAINS
IF
THE
SIGNAL
IS
DEFINED
AS
PART
OF
THE
BASE
POSIX
SPECIFICATION
AND
XSI
IF
IT
IS
DEFINED
AS
PART
OF
THE
XSI
OPTION
WHEN
THE
DEFAULT
ACTION
IS
LABELED
TERMINATE
CORE
IT
MEANS
THAT
A
MEMORY
IMAGE
OF
THE
PROCESS
IS
LEFT
IN
THE
FILE
NAMED
CORE
OF
THE
CURRENT
WORKING
DIRECTORY
OF
THE
PROCESS
BECAUSE
THE
FILE
IS
NAMED
CORE
IT
SHOWS
HOW
LONG
THIS
FEATURE
HAS
BEEN
PART
OF
THE
UNIX
SYSTEM
THIS
FILE
CAN
BE
USED
WITH
MOST
UNIX
SYSTEM
DEBUGGERS
TO
EXAMINE
THE
STATE
OF
THE
PROCESS
AT
THE
TIME
IT
TERMINATED
THE
GENERATION
OF
THE
CORE
FILE
IS
AN
IMPLEMENTATION
FEATURE
OF
MOST
VERSIONS
OF
THE
UNIX
SYSTEM
ALTHOUGH
THIS
FEATURE
IS
NOT
PART
OF
POSIX
IT
IS
MENTIONED
AS
A
POTENTIAL
IMPLEMENTATION
SPECIFIC
ACTION
IN
THE
SINGLE
UNIX
SPECIFICATION
XSI
OPTION
THE
NAME
OF
THE
CORE
FILE
VARIES
AMONG
IMPLEMENTATIONS
ON
FREEBSD
FOR
EXAMPLE
THE
CORE
FILE
IS
NAMED
CMDNAME
CORE
WHERE
CMDNAME
IS
THE
NAME
OF
THE
COMMAND
CORRESPONDING
TO
THE
PROCESS
THAT
RECEIVED
THE
SIGNAL
ON
MAC
OS
X
THE
CORE
FILE
IS
NAMED
CORE
PID
WHERE
PID
IS
THE
ID
OF
THE
PROCESS
THAT
RECEIVED
THE
SIGNAL
THESE
SYSTEMS
ALLOW
THE
CORE
FILENAME
TO
BE
CONFIGURED
VIA
A
SYSCTL
PARAMETER
ON
LINUX
THE
NAME
IS
CONFIGURED
THROUGH
PROC
SYS
KERNEL
MOST
IMPLEMENTATIONS
LEAVE
THE
CORE
FILE
IN
THE
CURRENT
WORKING
DIRECTORY
OF
THE
CORRESPONDING
PROCESS
MAC
OS
X
PLACES
ALL
CORE
FILES
IN
CORES
INSTEAD
NAME
DESCRIPTION
ISO
C
SUS
FREEBSD
LINUX
MAC
OS
X
SOLARIS
DEFAULT
ACTION
SIGABRT
ABNORMAL
TERMINATION
ABORT
TERMINATE
CORE
SIGALRM
TIMER
EXPIRED
ALARM
TERMINATE
SIGBUS
HARDWARE
FAULT
TERMINATE
CORE
SIGCANCEL
THREADS
LIBRARY
INTERNAL
USE
IGNORE
SIGCHLD
CHANGE
IN
STATUS
OF
CHILD
IGNORE
SIGCONT
CONTINUE
STOPPED
PROCESS
CONTINUE
IGNORE
SIGEMT
HARDWARE
FAULT
TERMINATE
CORE
SIGFPE
ARITHMETIC
EXCEPTION
TERMINATE
CORE
SIGFREEZE
CHECKPOINT
FREEZE
IGNORE
SIGHUP
HANGUP
TERMINATE
SIGILL
ILLEGAL
INSTRUCTION
TERMINATE
CORE
SIGINFO
STATUS
REQUEST
FROM
KEYBOARD
IGNORE
SIGINT
TERMINAL
INTERRUPT
CHARACTER
TERMINATE
SIGIO
ASYNCHRONOUS
I
O
TERMINATE
IGNORE
SIGIOT
HARDWARE
FAULT
TERMINATE
CORE
JAVA
VIRTUAL
MACHINE
INTERNAL
USE
IGNORE
JAVA
VIRTUAL
MACHINE
INTERNAL
USE
IGNORE
SIGKILL
TERMINATION
TERMINATE
SIGLOST
RESOURCE
LOST
TERMINATE
SIGLWP
THREADS
LIBRARY
INTERNAL
USE
TERMINATE
IGNORE
SIGPIPE
WRITE
TO
PIPE
WITH
NO
READERS
TERMINATE
SIGPOLL
POLLABLE
EVENT
POLL
TERMINATE
SIGPROF
PROFILING
TIME
ALARM
SETITIMER
TERMINATE
SIGPWR
POWER
FAIL
RESTART
TERMINATE
IGNORE
SIGQUIT
TERMINAL
QUIT
CHARACTER
TERMINATE
CORE
SIGSEGV
INVALID
MEMORY
REFERENCE
TERMINATE
CORE
SIGSTKFLT
COPROCESSOR
STACK
FAULT
TERMINATE
SIGSTOP
STOP
STOP
PROCESS
SIGSYS
INVALID
SYSTEM
CALL
XSI
TERMINATE
CORE
SIGTERM
TERMINATION
TERMINATE
SIGTHAW
CHECKPOINT
THAW
IGNORE
SIGTHR
THREADS
LIBRARY
INTERNAL
USE
TERMINATE
SIGTRAP
HARDWARE
FAULT
XSI
TERMINATE
CORE
SIGTSTP
TERMINAL
STOP
CHARACTER
STOP
PROCESS
SIGTTIN
BACKGROUND
READ
FROM
CONTROL
TTY
STOP
PROCESS
SIGTTOU
BACKGROUND
WRITE
TO
CONTROL
TTY
STOP
PROCESS
SIGURG
URGENT
CONDITION
SOCKETS
IGNORE
USER
DEFINED
SIGNAL
TERMINATE
USER
DEFINED
SIGNAL
TERMINATE
SIGVTALRM
VIRTUAL
TIME
ALARM
SETITIMER
XSI
TERMINATE
SIGWAITING
THREADS
LIBRARY
INTERNAL
USE
IGNORE
SIGWINCH
TERMINAL
WINDOW
SIZE
CHANGE
IGNORE
SIGXCPU
CPU
LIMIT
EXCEEDED
SETRLIMIT
XSI
TERMINATE
OR
TERMINATE
CORE
SIGXFSZ
FILE
SIZE
LIMIT
EXCEEDED
SETRLIMIT
XSI
TERMINATE
OR
TERMINATE
CORE
SIGXRES
RESOURCE
CONTROL
EXCEEDED
IGNORE
FIGURE
UNIX
SYSTEM
SIGNALS
THE
CORE
FILE
WILL
NOT
BE
GENERATED
IF
A
THE
PROCESS
WAS
SET
USER
ID
AND
THE
CURRENT
USER
IS
NOT
THE
OWNER
OF
THE
PROGRAM
FILE
B
THE
PROCESS
WAS
SET
GROUP
ID
AND
THE
CURRENT
USER
IS
NOT
THE
GROUP
OWNER
OF
THE
FILE
C
THE
USER
DOES
NOT
HAVE
PERMISSION
TO
WRITE
IN
THE
CURRENT
WORKING
DIRECTORY
D
THE
FILE
ALREADY
EXISTS
AND
THE
USER
DOES
NOT
HAVE
PERMISSION
TO
WRITE
TO
IT
OR
E
THE
FILE
IS
TOO
BIG
RECALL
THE
LIMIT
IN
SECTION
THE
PERMISSIONS
OF
THE
CORE
FILE
ASSUMING
THAT
THE
FILE
DOESN
T
ALREADY
EXIST
ARE
USUALLY
USER
READ
AND
USER
WRITE
ALTHOUGH
MAC
OS
X
SETS
ONLY
USER
READ
IN
FIGURE
THE
SIGNALS
WITH
A
DESCRIPTION
OF
HARDWARE
FAULT
CORRESPOND
TO
IMPLEMENTATION
DEFINED
HARDWARE
FAULTS
MANY
OF
THESE
NAMES
ARE
TAKEN
FROM
THE
ORIGINAL
PDP
IMPLEMENTATION
OF
THE
UNIX
SYSTEM
CHECK
YOUR
SYSTEM
MANUALS
TO
DETERMINE
EXACTLY
WHICH
TYPE
OF
ERROR
THESE
SIGNALS
CORRESPOND
TO
WE
NOW
DESCRIBE
EACH
OF
THESE
SIGNALS
IN
MORE
DETAIL
SIGABRT
THIS
SIGNAL
IS
GENERATED
BY
CALLING
THE
ABORT
FUNCTION
SECTION
THE
PROCESS
TERMINATES
ABNORMALLY
SIGALRM
THIS
SIGNAL
IS
GENERATED
WHEN
A
TIMER
SET
WITH
THE
ALARM
FUNCTION
EXPIRES
SEE
SECTION
FOR
MORE
DETAILS
THIS
SIGNAL
IS
ALSO
GENERATED
WHEN
AN
INTERVAL
TIMER
SET
BY
THE
SETITIMER
FUNCTION
EXPIRES
SIGBUS
THIS
SIGNAL
INDICATES
AN
IMPLEMENTATION
DEFINED
HARDWARE
FAULT
IMPLEMENTATIONS
USUALLY
GENERATE
THIS
SIGNAL
ON
CERTAIN
TYPES
OF
MEMORY
FAULTS
AS
WE
DESCRIBE
IN
SECTION
SIGCANCEL
THIS
SIGNAL
IS
USED
INTERNALLY
BY
THE
SOLARIS
THREADS
LIBRARY
IT
IS
NOT
MEANT
FOR
GENERAL
USE
SIGCHLD
WHENEVER
A
PROCESS
TERMINATES
OR
STOPS
THE
SIGCHLD
SIGNAL
IS
SENT
TO
THE
PARENT
BY
DEFAULT
THIS
SIGNAL
IS
IGNORED
SO
THE
PARENT
MUST
CATCH
THIS
SIGNAL
IF
IT
WANTS
TO
BE
NOTIFIED
WHENEVER
A
CHILD
STATUS
CHANGES
THE
NORMAL
ACTION
IN
THE
SIGNAL
CATCHING
FUNCTION
IS
TO
CALL
ONE
OF
THE
WAIT
FUNCTIONS
TO
FETCH
THE
CHILD
PROCESS
ID
AND
TERMINATION
STATUS
EARLIER
RELEASES
OF
SYSTEM
V
HAD
A
SIMILAR
SIGNAL
NAMED
SIGCLD
WITHOUT
THE
H
THE
SEMANTICS
OF
THIS
SIGNAL
WERE
DIFFERENT
FROM
THOSE
OF
OTHER
SIGNALS
AND
AS
FAR
BACK
AS
THE
MANUAL
PAGE
STRONGLY
DISCOURAGED
ITS
USE
IN
NEW
PROGRAMS
STRANGELY
ENOUGH
THIS
WARNING
DISAPPEARED
IN
THE
AND
VERSIONS
OF
THE
MANUAL
PAGE
APPLICATIONS
SHOULD
USE
THE
STANDARD
SIGCHLD
SIGNAL
BUT
BE
AWARE
THAT
MANY
SYSTEMS
DEFINE
SIGCLD
TO
BE
THE
SAME
AS
SIGCHLD
FOR
BACKWARD
COMPATIBILITY
IF
YOU
MAINTAIN
SOFTWARE
THAT
USES
SIGCLD
YOU
NEED
TO
CHECK
YOUR
SYSTEM
MANUAL
PAGE
TO
SEE
WHICH
SEMANTICS
IT
FOLLOWS
WE
DISCUSS
THESE
TWO
SIGNALS
IN
SECTION
SIGCONT
THIS
JOB
CONTROL
SIGNAL
IS
SENT
TO
A
STOPPED
PROCESS
WHEN
IT
IS
CONTINUED
THE
DEFAULT
ACTION
IS
TO
CONTINUE
A
STOPPED
PROCESS
BUT
TO
IGNORE
THE
SIGNAL
IF
THE
PROCESS
WASN
T
STOPPED
A
FULL
SCREEN
EDITOR
FOR
EXAMPLE
MIGHT
CATCH
THIS
SIGNAL
AND
USE
THE
SIGNAL
HANDLER
TO
MAKE
A
NOTE
TO
REDRAW
THE
TERMINAL
SCREEN
SEE
SECTION
FOR
ADDITIONAL
DETAILS
SIGEMT
THIS
INDICATES
AN
IMPLEMENTATION
DEFINED
HARDWARE
FAULT
THE
NAME
EMT
COMES
FROM
THE
PDP
EMULATOR
TRAP
INSTRUCTION
NOT
ALL
PLATFORMS
SUPPORT
THIS
SIGNAL
ON
LINUX
FOR
EXAMPLE
SIGEMT
IS
SUPPORTED
ONLY
FOR
SELECTED
ARCHITECTURES
SUCH
AS
SPARC
MIPS
AND
PA
RISC
SIGFPE
THIS
SIGNALS
AN
ARITHMETIC
EXCEPTION
SUCH
AS
DIVIDE
BY
FLOATING
POINT
OVERFLOW
AND
SO
ON
SIGFREEZE
THIS
SIGNAL
IS
DEFINED
ONLY
BY
SOLARIS
IT
IS
USED
TO
NOTIFY
PROCESSES
THAT
NEED
TO
TAKE
SPECIAL
ACTION
BEFORE
FREEZING
THE
SYSTEM
STATE
SUCH
AS
MIGHT
HAPPEN
WHEN
A
SYSTEM
GOES
INTO
HIBERNATION
OR
SUSPENDED
MODE
SIGHUP
THIS
SIGNAL
IS
SENT
TO
THE
CONTROLLING
PROCESS
SESSION
LEADER
ASSOCIATED
WITH
A
CONTROLLING
TERMINAL
IF
A
DISCONNECT
IS
DETECTED
BY
THE
TERMINAL
INTERFACE
REFERRING
TO
FIGURE
WE
SEE
THAT
THE
SIGNAL
IS
SENT
TO
THE
PROCESS
POINTED
TO
BY
THE
FIELD
IN
THE
SESSION
STRUCTURE
THIS
SIGNAL
IS
GENERATED
FOR
THIS
CONDITION
ONLY
IF
THE
TERMINAL
CLOCAL
FLAG
IS
NOT
SET
THE
CLOCAL
FLAG
FOR
A
TERMINAL
IS
SET
IF
THE
ATTACHED
TERMINAL
IS
LOCAL
THE
FLAG
TELLS
THE
TERMINAL
DRIVER
TO
IGNORE
ALL
MODEM
STATUS
LINES
WE
DESCRIBE
HOW
TO
SET
THIS
FLAG
IN
CHAPTER
NOTE
THAT
THE
SESSION
LEADER
THAT
RECEIVES
THIS
SIGNAL
MAY
BE
IN
THE
BACKGROUND
SEE
FIGURE
FOR
AN
EXAMPLE
THIS
DIFFERS
FROM
THE
NORMAL
TERMINAL
GENERATED
SIGNALS
INTERRUPT
QUIT
AND
SUSPEND
WHICH
ARE
ALWAYS
DELIVERED
TO
THE
FOREGROUND
PROCESS
GROUP
THIS
SIGNAL
IS
ALSO
GENERATED
IF
THE
SESSION
LEADER
TERMINATES
IN
THIS
CASE
THE
SIGNAL
IS
SENT
TO
EACH
PROCESS
IN
THE
FOREGROUND
PROCESS
GROUP
THIS
SIGNAL
IS
COMMONLY
USED
TO
NOTIFY
DAEMON
PROCESSES
CHAPTER
TO
REREAD
THEIR
CONFIGURATION
FILES
THE
REASON
SIGHUP
IS
CHOSEN
FOR
THIS
TASK
IS
THAT
A
DAEMON
SHOULD
NOT
HAVE
A
CONTROLLING
TERMINAL
AND
WOULD
NORMALLY
NEVER
RECEIVE
THIS
SIGNAL
SIGILL
THIS
SIGNAL
INDICATES
THAT
THE
PROCESS
HAS
EXECUTED
AN
ILLEGAL
HARDWARE
INSTRUCTION
GENERATED
THIS
SIGNAL
FROM
THE
ABORT
FUNCTION
SIGABRT
IS
NOW
USED
FOR
THIS
PURPOSE
SIGINFO
THIS
BSD
SIGNAL
IS
GENERATED
BY
THE
TERMINAL
DRIVER
WHEN
WE
TYPE
THE
STATUS
KEY
OFTEN
CONTROL
T
THIS
SIGNAL
IS
SENT
TO
ALL
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
REFER
TO
FIGURE
THIS
SIGNAL
NORMALLY
CAUSES
STATUS
INFORMATION
ON
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
TO
BE
DISPLAYED
ON
THE
TERMINAL
LINUX
DOESN
T
PROVIDE
SUPPORT
FOR
SIGINFO
ALTHOUGH
THE
SYMBOL
IS
DEFINED
TO
BE
THE
SAME
VALUE
AS
SIGPWR
ON
THE
ALPHA
PLATFORM
THIS
IS
MOST
LIKELY
TO
PROVIDE
SOME
LEVEL
OF
COMPATIBILITY
WITH
SOFTWARE
DEVELOPED
FOR
OSF
SIGINT
THIS
SIGNAL
IS
GENERATED
BY
THE
TERMINAL
DRIVER
WHEN
WE
PRESS
THE
INTERRUPT
KEY
OFTEN
DELETE
OR
CONTROL
C
THIS
SIGNAL
IS
SENT
TO
ALL
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
REFER
TO
FIGURE
THIS
SIGNAL
IS
OFTEN
USED
TO
TERMINATE
A
RUNAWAY
PROGRAM
ESPECIALLY
WHEN
IT
GENERATING
A
LOT
OF
UNWANTED
OUTPUT
ON
THE
SCREEN
SIGIO
THIS
SIGNAL
INDICATES
AN
ASYNCHRONOUS
I
O
EVENT
WE
DISCUSS
IT
IN
SECTION
IN
FIGURE
WE
LABELED
THE
DEFAULT
ACTION
FOR
SIGIO
AS
EITHER
TERMINATE
OR
IGNORE
UNFORTUNATELY
THE
DEFAULT
DEPENDS
ON
THE
SYSTEM
UNDER
SYSTEM
V
SIGIO
IS
IDENTICAL
TO
SIGPOLL
SO
ITS
DEFAULT
ACTION
IS
TO
TERMINATE
THE
PROCESS
UNDER
BSD
THE
DEFAULT
IS
TO
IGNORE
THE
SIGNAL
LINUX
AND
SOLARIS
DEFINE
SIGIO
TO
BE
THE
SAME
VALUE
AS
SIGPOLL
SO
THE
DEFAULT
BEHAVIOR
IS
TO
TERMINATE
THE
PROCESS
ON
FREEBSD
AND
MAC
OS
X
THE
DEFAULT
IS
TO
IGNORE
THE
SIGNAL
SIGIOT
THIS
INDICATES
AN
IMPLEMENTATION
DEFINED
HARDWARE
FAULT
THE
NAME
IOT
COMES
FROM
THE
PDP
MNEMONIC
FOR
THE
INPUT
OUTPUT
TRAP
INSTRUCTION
EARLIER
VERSIONS
OF
SYSTEM
V
GENERATED
THIS
SIGNAL
FROM
THE
ABORT
FUNCTION
SIGABRT
IS
NOW
USED
FOR
THIS
PURPOSE
ON
FREEBSD
LINUX
MAC
OS
X
AND
SOLARIS
SIGIOT
IS
DEFINED
TO
BE
THE
SAME
VALUE
AS
SIGABRT
A
SIGNAL
RESERVED
FOR
USE
BY
THE
JAVA
VIRTUAL
MACHINE
ON
SOLARIS
ANOTHER
SIGNAL
RESERVED
FOR
USE
BY
THE
JAVA
VIRTUAL
MACHINE
ON
SOLARIS
SIGKILL
THIS
SIGNAL
IS
ONE
OF
THE
TWO
THAT
CAN
T
BE
CAUGHT
OR
IGNORED
IT
PROVIDES
THE
SYSTEM
ADMINISTRATOR
WITH
A
SURE
WAY
TO
KILL
ANY
PROCESS
SIGLOST
THIS
SIGNAL
IS
USED
TO
NOTIFY
A
PROCESS
RUNNING
ON
A
SOLARIS
CLIENT
SYSTEM
THAT
A
LOCK
COULD
NOT
BE
REACQUIRED
DURING
RECOVERY
SIGLWP
THIS
SIGNAL
IS
USED
INTERNALLY
BY
THE
SOLARIS
THREADS
LIBRARY
IT
IS
NOT
AVAILABLE
FOR
GENERAL
USE
ON
FREEBSD
SIGLWP
IS
DEFINED
TO
BE
AN
ALIAS
FOR
SIGTHR
SIGPIPE
IF
WE
WRITE
TO
A
PIPELINE
BUT
THE
READER
HAS
TERMINATED
SIGPIPE
IS
GENERATED
WE
DESCRIBE
PIPES
IN
SECTION
THIS
SIGNAL
IS
ALSO
GENERATED
WHEN
A
PROCESS
WRITES
TO
A
SOCKET
OF
TYPE
THAT
IS
NO
LONGER
CONNECTED
WE
DESCRIBE
SOCKETS
IN
CHAPTER
SIGPOLL
THIS
SIGNAL
IS
MARKED
OBSOLESCENT
IN
SO
IT
MIGHT
BE
REMOVED
IN
A
FUTURE
VERSION
OF
THE
STANDARD
IT
CAN
BE
GENERATED
WHEN
A
SPECIFIC
EVENT
OCCURS
ON
A
POLLABLE
DEVICE
WE
DESCRIBE
THIS
SIGNAL
WITH
THE
POLL
FUNCTION
IN
SECTION
SIGPOLL
ORIGINATED
WITH
AND
LOOSELY
CORRESPONDS
TO
THE
BSD
SIGIO
AND
SIGURG
SIGNALS
ON
LINUX
AND
SOLARIS
SIGPOLL
IS
DEFINED
TO
HAVE
THE
SAME
VALUE
AS
SIGIO
SIGPROF
THIS
SIGNAL
IS
MARKED
OBSOLESCENT
IN
SO
IT
MIGHT
BE
REMOVED
IN
A
FUTURE
VERSION
OF
THE
STANDARD
THIS
SIGNAL
IS
GENERATED
WHEN
A
PROFILING
INTERVAL
TIMER
SET
BY
THE
SETITIMER
FUNCTION
EXPIRES
SIGPWR
THIS
SIGNAL
IS
SYSTEM
DEPENDENT
ITS
MAIN
USE
IS
ON
A
SYSTEM
THAT
HAS
AN
UNINTERRUPTIBLE
POWER
SUPPLY
UPS
IF
POWER
FAILS
THE
UPS
TAKES
OVER
AND
THE
SOFTWARE
CAN
USUALLY
BE
NOTIFIED
NOTHING
NEEDS
TO
BE
DONE
AT
THIS
POINT
AS
THE
SYSTEM
CONTINUES
RUNNING
ON
BATTERY
POWER
BUT
IF
THE
BATTERY
GETS
LOW
FOR
EXAMPLE
IF
THE
POWER
IS
OFF
FOR
AN
EXTENDED
PERIOD
THE
SOFTWARE
IS
USUALLY
NOTIFIED
AGAIN
AT
THIS
POINT
IT
BEHOOVES
THE
SYSTEM
TO
SHUT
EVERYTHING
DOWN
THIS
IS
WHEN
SIGPWR
SHOULD
BE
SENT
ON
MOST
SYSTEMS
THE
PROCESS
THAT
IS
NOTIFIED
OF
THE
LOW
BATTERY
CONDITION
SENDS
THE
SIGPWR
SIGNAL
TO
THE
INIT
PROCESS
AND
INIT
HANDLES
THE
SYSTEM
SHUTDOWN
SOLARIS
AND
SOME
LINUX
DISTRIBUTIONS
HAVE
ENTRIES
IN
THE
INITTAB
FILE
FOR
THIS
PURPOSE
POWERFAIL
AND
POWERWAIT
OR
POWEROKWAIT
IN
FIGURE
WE
LABELED
THE
DEFAULT
ACTION
FOR
SIGPWR
AS
EITHER
TERMINATE
OR
IGNORE
UNFORTUNATELY
THE
DEFAULT
DEPENDS
ON
THE
SYSTEM
THE
DEFAULT
ON
LINUX
IS
TO
TERMINATE
THE
PROCESS
ON
SOLARIS
THE
SIGNAL
IS
IGNORED
BY
DEFAULT
SIGQUIT
THIS
SIGNAL
IS
GENERATED
BY
THE
TERMINAL
DRIVER
WHEN
WE
PRESS
THE
TERMINAL
QUIT
KEY
OFTEN
CONTROL
BACKSLASH
THIS
SIGNAL
IS
SENT
TO
ALL
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
REFER
TO
FIGURE
THIS
SIGNAL
NOT
ONLY
TERMINATES
THE
FOREGROUND
PROCESS
GROUP
AS
DOES
SIGINT
BUT
ALSO
GENERATES
A
CORE
FILE
SIGSEGV
THIS
SIGNAL
INDICATES
THAT
THE
PROCESS
HAS
MADE
AN
INVALID
MEMORY
REFERENCE
WHICH
IS
USUALLY
A
SIGN
THAT
THE
PROGRAM
HAS
A
BUG
SUCH
AS
DEREFERENCING
AN
UNINITIALIZED
POINTER
THE
NAME
SEGV
STANDS
FOR
SEGMENTATION
VIOLATION
SIGSTKFLT
THIS
SIGNAL
IS
DEFINED
ONLY
BY
LINUX
IT
SHOWED
UP
IN
THE
EARLIEST
VERSIONS
OF
LINUX
WHERE
IT
WAS
INTENDED
TO
BE
USED
FOR
STACK
FAULTS
TAKEN
BY
THE
MATH
COPROCESSOR
THIS
SIGNAL
IS
NOT
GENERATED
BY
THE
KERNEL
BUT
REMAINS
FOR
BACKWARD
COMPATIBILITY
SIGSTOP
THIS
JOB
CONTROL
SIGNAL
STOPS
A
PROCESS
IT
IS
SIMILAR
TO
THE
INTERACTIVE
STOP
SIGNAL
SIGTSTP
BUT
SIGSTOP
CANNOT
BE
CAUGHT
OR
IGNORED
SIGSYS
THIS
SIGNALS
AN
INVALID
SYSTEM
CALL
SOMEHOW
THE
PROCESS
EXECUTED
A
MACHINE
INSTRUCTION
THAT
THE
KERNEL
THOUGHT
WAS
A
SYSTEM
CALL
BUT
THE
PARAMETER
WITH
THE
INSTRUCTION
THAT
INDICATES
THE
TYPE
OF
SYSTEM
CALL
WAS
INVALID
THIS
MIGHT
HAPPEN
IF
YOU
BUILD
A
PROGRAM
THAT
USES
A
NEW
SYSTEM
CALL
AND
YOU
THEN
TRY
TO
RUN
THE
SAME
BINARY
ON
AN
OLDER
VERSION
OF
THE
OPERATING
SYSTEM
WHERE
THE
SYSTEM
CALL
DOESN
T
EXIST
SIGTERM
THIS
IS
THE
TERMINATION
SIGNAL
SENT
BY
THE
KILL
COMMAND
BY
DEFAULT
BECAUSE
IT
CAN
BE
CAUGHT
BY
APPLICATIONS
USING
SIGTERM
GIVES
PROGRAMS
A
CHANCE
TO
TERMINATE
GRACEFULLY
BY
CLEANING
UP
BEFORE
EXITING
IN
CONTRAST
TO
SIGKILL
WHICH
CAN
T
BE
CAUGHT
OR
IGNORED
SIGTHAW
THIS
SIGNAL
IS
DEFINED
ONLY
BY
SOLARIS
AND
IS
USED
TO
NOTIFY
PROCESSES
THAT
NEED
TO
TAKE
SPECIAL
ACTION
WHEN
THE
SYSTEM
RESUMES
OPERATION
AFTER
BEING
SUSPENDED
SIGTHR
THIS
IS
A
SIGNAL
RESERVED
FOR
USE
BY
THE
THREAD
LIBRARY
ON
FREEBSD
IT
IS
DEFINED
TO
HAVE
THE
SAME
VALUE
AS
SIGLWP
SIGTRAP
THIS
SIGNAL
INDICATES
AN
IMPLEMENTATION
DEFINED
HARDWARE
FAULT
THE
SIGNAL
NAME
COMES
FROM
THE
PDP
TRAP
INSTRUCTION
IMPLEMENTATIONS
OFTEN
USE
THIS
SIGNAL
TO
TRANSFER
CONTROL
TO
A
DEBUGGER
WHEN
A
BREAKPOINT
INSTRUCTION
IS
EXECUTED
SIGTSTP
THIS
INTERACTIVE
STOP
SIGNAL
IS
GENERATED
BY
THE
TERMINAL
DRIVER
WHEN
WE
PRESS
THE
TERMINAL
SUSPEND
KEY
OFTEN
CONTROL
Z
THIS
SIGNAL
IS
SENT
TO
ALL
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
REFER
TO
FIGURE
UNFORTUNATELY
THE
TERM
STOP
HAS
DIFFERENT
MEANINGS
WHEN
DISCUSSING
JOB
CONTROL
AND
SIGNALS
WE
TALK
ABOUT
STOPPING
AND
CONTINUING
JOBS
THE
TERMINAL
DRIVER
HOWEVER
HAS
HISTORICALLY
USED
THE
TERM
STOP
TO
REFER
TO
STOPPING
AND
STARTING
THE
TERMINAL
OUTPUT
USING
THE
CONTROL
AND
CONTROL
Q
CHARACTERS
THEREFORE
THE
TERMINAL
DRIVER
CALLS
THE
CHARACTER
THAT
GENERATES
THE
INTERACTIVE
STOP
SIGNAL
THE
SUSPEND
CHARACTER
NOT
THE
STOP
CHARACTER
SIGTTIN
THIS
SIGNAL
IS
GENERATED
BY
THE
TERMINAL
DRIVER
WHEN
A
PROCESS
IN
A
BACKGROUND
PROCESS
GROUP
TRIES
TO
READ
FROM
ITS
CONTROLLING
TERMINAL
REFER
TO
THE
DISCUSSION
OF
THIS
TOPIC
IN
SECTION
AS
SPECIAL
CASES
IF
EITHER
A
THE
READING
PROCESS
IS
IGNORING
OR
BLOCKING
THIS
SIGNAL
OR
B
THE
PROCESS
GROUP
OF
THE
READING
PROCESS
IS
ORPHANED
THEN
THE
SIGNAL
IS
NOT
GENERATED
INSTEAD
THE
READ
OPERATION
FAILS
WITH
ERRNO
SET
TO
EIO
SIGTTOU
THIS
SIGNAL
IS
GENERATED
BY
THE
TERMINAL
DRIVER
WHEN
A
PROCESS
IN
A
BACKGROUND
PROCESS
GROUP
TRIES
TO
WRITE
TO
ITS
CONTROLLING
TERMINAL
THIS
IS
DISCUSSED
IN
SECTION
UNLIKE
THE
CASE
WITH
BACKGROUND
READS
A
PROCESS
CAN
CHOOSE
TO
ALLOW
BACKGROUND
WRITES
TO
THE
CONTROLLING
TERMINAL
WE
DESCRIBE
HOW
TO
MODIFY
THIS
OPTION
IN
CHAPTER
IF
BACKGROUND
WRITES
ARE
NOT
ALLOWED
THEN
LIKE
THE
SIGTTIN
SIGNAL
THERE
ARE
TWO
SPECIAL
CASES
IF
EITHER
A
THE
WRITING
PROCESS
IS
IGNORING
OR
BLOCKING
THIS
SIGNAL
OR
B
THE
PROCESS
GROUP
OF
THE
WRITING
PROCESS
IS
ORPHANED
THEN
THE
SIGNAL
IS
NOT
GENERATED
INSTEAD
THE
WRITE
OPERATION
RETURNS
AN
ERROR
WITH
ERRNO
SET
TO
EIO
REGARDLESS
OF
WHETHER
BACKGROUND
WRITES
ARE
ALLOWED
CERTAIN
TERMINAL
OPERATIONS
OTHER
THAN
WRITING
CAN
ALSO
GENERATE
THE
SIGTTOU
SIGNAL
THESE
INCLUDE
TCSETATTR
TCSENDBREAK
TCDRAIN
TCFLUSH
TCFLOW
AND
TCSETPGRP
WE
DESCRIBE
THESE
TERMINAL
OPERATIONS
IN
CHAPTER
SIGURG
THIS
SIGNAL
NOTIFIES
THE
PROCESS
THAT
AN
URGENT
CONDITION
HAS
OCCURRED
IT
IS
OPTIONALLY
GENERATED
WHEN
OUT
OF
BAND
DATA
IS
RECEIVED
ON
A
NETWORK
CONNECTION
THIS
IS
A
USER
DEFINED
SIGNAL
FOR
USE
IN
APPLICATION
PROGRAMS
THIS
IS
ANOTHER
USER
DEFINED
SIGNAL
SIMILAR
TO
FOR
USE
IN
APPLICATION
PROGRAMS
SIGVTALRM
THIS
SIGNAL
IS
GENERATED
WHEN
A
VIRTUAL
INTERVAL
TIMER
SET
BY
THE
SETITIMER
FUNCTION
EXPIRES
SIGWAITING
THIS
SIGNAL
IS
USED
INTERNALLY
BY
THE
SOLARIS
THREADS
LIBRARY
AND
IS
NOT
AVAILABLE
FOR
GENERAL
USE
SIGWINCH
THE
KERNEL
MAINTAINS
THE
SIZE
OF
THE
WINDOW
ASSOCIATED
WITH
EACH
TERMINAL
AND
PSEUDO
TERMINAL
A
PROCESS
CAN
GET
AND
SET
THE
WINDOW
SIZE
WITH
THE
IOCTL
FUNCTION
WHICH
WE
DESCRIBE
IN
SECTION
IF
A
PROCESS
CHANGES
THE
WINDOW
SIZE
FROM
ITS
PREVIOUS
VALUE
USING
THE
IOCTL
SET
WINDOW
SIZE
COMMAND
THE
KERNEL
GENERATES
THE
SIGWINCH
SIGNAL
FOR
THE
FOREGROUND
PROCESS
GROUP
SIGXCPU
THE
SINGLE
UNIX
SPECIFICATION
SUPPORTS
THE
CONCEPT
OF
RESOURCE
LIMITS
AS
PART
OF
THE
XSI
OPTION
REFER
TO
SECTION
IF
THE
PROCESS
EXCEEDS
ITS
SOFT
CPU
TIME
LIMIT
THE
SIGXCPU
SIGNAL
IS
GENERATED
IN
FIGURE
WE
LABELED
THE
DEFAULT
ACTION
FOR
SIGXCPU
AS
EITHER
TERMINATE
OR
TERMINATE
WITH
A
CORE
FILE
THE
DEFAULT
DEPENDS
ON
THE
OPERATING
SYSTEM
LINUX
AND
SOLARIS
SUPPORT
A
DEFAULT
ACTION
OF
TERMINATE
WITH
A
CORE
FILE
WHEREAS
FREEBSD
AND
MAC
OS
X
SUPPORT
A
DEFAULT
ACTION
OF
TERMINATE
WITHOUT
GENERATING
A
CORE
FILE
THE
SINGLE
UNIX
SPECIFICATION
REQUIRES
THAT
THE
DEFAULT
ACTION
BE
TO
TERMINATE
THE
PROCESS
ABNORMALLY
WHETHER
A
CORE
FILE
IS
GENERATED
IS
LEFT
UP
TO
THE
IMPLEMENTATION
SIGXFSZ
THIS
SIGNAL
IS
GENERATED
IF
THE
PROCESS
EXCEEDS
ITS
SOFT
FILE
SIZE
LIMIT
REFER
TO
SECTION
JUST
AS
WITH
SIGXCPU
THE
DEFAULT
ACTION
TAKEN
WITH
SIGXFSZ
DEPENDS
ON
THE
OPERATING
SYSTEM
ON
LINUX
AND
SOLARIS
THE
DEFAULT
IS
TO
TERMINATE
THE
PROCESS
AND
CREATE
A
CORE
FILE
ON
FREEBSD
AND
MAC
OS
X
THE
DEFAULT
IS
TO
TERMINATE
THE
PROCESS
WITHOUT
GENERATING
A
CORE
FILE
THE
SINGLE
UNIX
SPECIFICATION
REQUIRES
THAT
THE
DEFAULT
ACTION
BE
TO
TERMINATE
THE
PROCESS
ABNORMALLY
WHETHER
A
CORE
FILE
IS
GENERATED
IS
LEFT
UP
TO
THE
IMPLEMENTATION
SIGXRES
THIS
SIGNAL
IS
DEFINED
ONLY
BY
SOLARIS
IT
IS
OPTIONALLY
USED
TO
NOTIFY
PROCESSES
THAT
HAVE
EXCEEDED
A
PRECONFIGURED
RESOURCE
VALUE
THE
SOLARIS
RESOURCE
CONTROL
MECHANISM
IS
A
GENERAL
FACILITY
FOR
CONTROLLING
THE
USE
OF
SHARED
RESOURCES
AMONG
INDEPENDENT
APPLICATION
SETS
SIGNAL
FUNCTION
THE
SIMPLEST
INTERFACE
TO
THE
SIGNAL
FEATURES
OF
THE
UNIX
SYSTEM
IS
THE
SIGNAL
FUNCTION
THE
SIGNAL
FUNCTION
IS
DEFINED
BY
ISO
C
WHICH
DOESN
T
INVOLVE
MULTIPLE
PROCESSES
PROCESS
GROUPS
TERMINAL
I
O
AND
THE
LIKE
THEREFORE
ITS
DEFINITION
OF
SIGNALS
IS
VAGUE
ENOUGH
TO
BE
ALMOST
USELESS
FOR
UNIX
SYSTEMS
IMPLEMENTATIONS
DERIVED
FROM
UNIX
SYSTEM
V
SUPPORT
THE
SIGNAL
FUNCTION
BUT
IT
PROVIDES
THE
OLD
UNRELIABLE
SIGNAL
SEMANTICS
WE
DESCRIBE
THESE
OLDER
SEMANTICS
IN
SECTION
THE
SIGNAL
FUNCTION
PROVIDES
BACKWARD
COMPATIBILITY
FOR
APPLICATIONS
THAT
REQUIRE
THE
OLDER
SEMANTICS
NEW
APPLICATIONS
SHOULD
NOT
USE
THESE
UNRELIABLE
SIGNALS
ALSO
PROVIDES
THE
SIGNAL
FUNCTION
BUT
IT
IS
DEFINED
IN
TERMS
OF
THE
SIGACTION
FUNCTION
WHICH
WE
DESCRIBE
IN
SECTION
SO
USING
IT
UNDER
PROVIDES
THE
NEWER
RELIABLE
SIGNAL
SEMANTICS
MOST
CURRENT
SYSTEMS
FOLLOW
THIS
STRATEGY
BUT
SOLARIS
FOLLOWS
THE
SYSTEM
V
SEMANTICS
FOR
THE
SIGNAL
FUNCTION
BECAUSE
THE
SEMANTICS
OF
SIGNAL
DIFFER
AMONG
IMPLEMENTATIONS
WE
MUST
USE
THE
SIGACTION
FUNCTION
INSTEAD
WE
PROVIDE
AN
IMPLEMENTATION
OF
SIGNAL
THAT
USES
SIGACTION
IN
SECTION
ALL
THE
EXAMPLES
IN
THIS
TEXT
USE
THE
SIGNAL
FUNCTION
FROM
FIGURE
TO
GIVE
US
CONSISTENT
SEMANTICS
REGARDLESS
OF
WHICH
PARTICULAR
PLATFORM
WE
USE
THE
SIGNO
ARGUMENT
IS
JUST
THE
NAME
OF
THE
SIGNAL
FROM
FIGURE
THE
VALUE
OF
FUNC
IS
A
THE
CONSTANT
B
THE
CONSTANT
OR
C
THE
ADDRESS
OF
A
FUNCTION
TO
BE
CALLED
WHEN
THE
SIGNAL
OCCURS
IF
WE
SPECIFY
WE
ARE
TELLING
THE
SYSTEM
TO
IGNORE
THE
SIGNAL
REMEMBER
THAT
WE
CANNOT
IGNORE
THE
TWO
SIGNALS
SIGKILL
AND
SIGSTOP
WHEN
WE
SPECIFY
WE
ARE
SETTING
THE
ACTION
ASSOCIATED
WITH
THE
SIGNAL
TO
ITS
DEFAULT
VALUE
SEE
THE
FINAL
COLUMN
IN
FIGURE
WHEN
WE
SPECIFY
THE
ADDRESS
OF
A
FUNCTION
TO
BE
CALLED
WHEN
THE
SIGNAL
OCCURS
WE
ARE
ARRANGING
TO
CATCH
THE
SIGNAL
WE
CALL
THE
FUNCTION
EITHER
THE
SIGNAL
HANDLER
OR
THE
SIGNAL
CATCHING
FUNCTION
THE
PROTOTYPE
FOR
THE
SIGNAL
FUNCTION
STATES
THAT
THE
FUNCTION
REQUIRES
TWO
ARGUMENTS
AND
RETURNS
A
POINTER
TO
A
FUNCTION
THAT
RETURNS
NOTHING
VOID
THE
SIGNAL
FUNCTION
FIRST
ARGUMENT
SIGNO
IS
AN
INTEGER
THE
SECOND
ARGUMENT
IS
A
POINTER
TO
A
FUNCTION
THAT
TAKES
A
SINGLE
INTEGER
ARGUMENT
AND
RETURNS
NOTHING
THE
FUNCTION
WHOSE
ADDRESS
IS
RETURNED
AS
THE
VALUE
OF
SIGNAL
TAKES
A
SINGLE
INTEGER
ARGUMENT
THE
FINAL
INT
IN
PLAIN
ENGLISH
THIS
DECLARATION
SAYS
THAT
THE
SIGNAL
HANDLER
IS
PASSED
A
SINGLE
INTEGER
ARGUMENT
THE
SIGNAL
NUMBER
AND
THAT
IT
RETURNS
NOTHING
WHEN
WE
CALL
SIGNAL
TO
ESTABLISH
THE
SIGNAL
HANDLER
THE
SECOND
ARGUMENT
IS
A
POINTER
TO
THE
FUNCTION
THE
RETURN
VALUE
FROM
SIGNAL
IS
THE
POINTER
TO
THE
PREVIOUS
SIGNAL
HANDLER
MANY
SYSTEMS
CALL
THE
SIGNAL
HANDLER
WITH
ADDITIONAL
IMPLEMENTATION
DEPENDENT
ARGUMENTS
WE
DISCUSS
THIS
FURTHER
IN
SECTION
THE
PERPLEXING
SIGNAL
FUNCTION
PROTOTYPE
SHOWN
AT
THE
BEGINNING
OF
THIS
SECTION
CAN
BE
MADE
MUCH
SIMPLER
THROUGH
THE
USE
OF
THE
FOLLOWING
TYPEDEF
PLAUGER
TYPEDEF
VOID
SIGFUNC
INT
THEN
THE
PROTOTYPE
BECOMES
SIGFUNC
SIGNAL
INT
SIGFUNC
WE
VE
INCLUDED
THIS
TYPEDEF
IN
APUE
H
APPENDIX
B
AND
USE
IT
WITH
THE
FUNCTIONS
IN
THIS
CHAPTER
IF
WE
EXAMINE
THE
SYSTEM
HEADER
SIGNAL
H
WE
WILL
PROBABLY
FIND
DECLARATIONS
OF
THE
FORM
DEFINE
VOID
DEFINE
VOID
DEFINE
VOID
THESE
CONSTANTS
CAN
BE
USED
IN
PLACE
OF
THE
POINTER
TO
A
FUNCTION
THAT
TAKES
AN
INTEGER
ARGUMENT
AND
RETURNS
NOTHING
THE
SECOND
ARGUMENT
TO
SIGNAL
AND
THE
RETURN
VALUE
FROM
SIGNAL
THE
THREE
VALUES
USED
FOR
THESE
CONSTANTS
NEED
NOT
BE
AND
THEY
MUST
BE
THREE
VALUES
THAT
CAN
NEVER
BE
THE
ADDRESS
OF
ANY
DECLARABLE
FUNCTION
MOST
UNIX
SYSTEMS
USE
THE
VALUES
SHOWN
EXAMPLE
FIGURE
SHOWS
A
SIMPLE
SIGNAL
HANDLER
THAT
CATCHES
EITHER
OF
THE
TWO
USER
DEFINED
SIGNALS
AND
PRINTS
THE
SIGNAL
NUMBER
IN
SECTION
WE
DESCRIBE
THE
PAUSE
FUNCTION
WHICH
SIMPLY
SUSPENDS
THE
CALLING
PROCESS
UNTIL
A
SIGNAL
IS
RECEIVED
INCLUDE
APUE
H
STATIC
VOID
INT
ONE
HANDLER
FOR
BOTH
SIGNALS
INT
MAIN
VOID
IF
SIGNAL
CAN
T
CATCH
IF
SIGNAL
CAN
T
CATCH
FOR
PAUSE
STATIC
VOID
INT
SIGNO
ARGUMENT
IS
SIGNAL
NUMBER
IF
SIGNO
PRINTF
RECEIVED
N
ELSE
IF
SIGNO
PRINTF
RECEIVED
N
ELSE
RECEIVED
SIGNAL
D
N
SIGNO
FIGURE
SIMPLE
PROGRAM
TO
CATCH
AND
WE
INVOKE
THE
PROGRAM
IN
THE
BACKGROUND
AND
USE
THE
KILL
COMMAND
TO
SEND
IT
SIGNALS
NOTE
THAT
THE
TERM
KILL
IN
THE
UNIX
SYSTEM
IS
A
MISNOMER
THE
KILL
COMMAND
AND
THE
KILL
FUNCTION
JUST
SEND
A
SIGNAL
TO
A
PROCESS
OR
PROCESS
GROUP
WHETHER
THAT
SIGNAL
TERMINATES
THE
PROCESS
DEPENDS
ON
WHICH
SIGNAL
IS
SENT
AND
WHETHER
THE
PROCESS
HAS
ARRANGED
TO
CATCH
THE
SIGNAL
A
OUT
START
PROCESS
IN
BACKGROUND
JOB
CONTROL
SHELL
PRINTS
JOB
NUMBER
AND
PROCESS
ID
KILL
SEND
IT
RECEIVED
KILL
SEND
IT
RECEIVED
KILL
NOW
SEND
IT
SIGTERM
TERMINATED
A
OUT
WHEN
WE
SEND
THE
SIGTERM
SIGNAL
THE
PROCESS
IS
TERMINATED
SINCE
IT
DOESN
T
CATCH
THE
SIGNAL
AND
THE
DEFAULT
ACTION
FOR
THE
SIGNAL
IS
TERMINATION
PROGRAM
START
UP
WHEN
A
PROGRAM
IS
EXECUTED
THE
STATUS
OF
ALL
SIGNALS
IS
EITHER
DEFAULT
OR
IGNORE
NORMALLY
ALL
SIGNALS
ARE
SET
TO
THEIR
DEFAULT
ACTION
UNLESS
THE
PROCESS
THAT
CALLS
EXEC
IS
IGNORING
THE
SIGNAL
SPECIFICALLY
THE
EXEC
FUNCTIONS
CHANGE
THE
DISPOSITION
OF
ANY
SIGNALS
BEING
CAUGHT
TO
THEIR
DEFAULT
ACTION
AND
LEAVE
THE
STATUS
OF
ALL
OTHER
SIGNALS
ALONE
NATURALLY
A
SIGNAL
THAT
IS
BEING
CAUGHT
BY
A
PROCESS
THAT
CALLS
EXEC
CANNOT
BE
CAUGHT
BY
THE
SAME
FUNCTION
IN
THE
NEW
PROGRAM
SINCE
THE
ADDRESS
OF
THE
SIGNAL
CATCHING
FUNCTION
IN
THE
CALLER
PROBABLY
HAS
NO
MEANING
IN
THE
NEW
PROGRAM
FILE
THAT
IS
EXECUTED
ONE
SPECIFIC
EXAMPLE
OF
THIS
SIGNAL
STATUS
BEHAVIOR
IS
HOW
AN
INTERACTIVE
SHELL
TREATS
THE
INTERRUPT
AND
QUIT
SIGNALS
FOR
A
BACKGROUND
PROCESS
WITH
A
SHELL
THAT
DOESN
T
SUPPORT
JOB
CONTROL
WHEN
WE
EXECUTE
A
PROCESS
IN
THE
BACKGROUND
AS
IN
CC
MAIN
C
THE
SHELL
AUTOMATICALLY
SETS
THE
DISPOSITION
OF
THE
INTERRUPT
AND
QUIT
SIGNALS
IN
THE
BACKGROUND
PROCESS
TO
BE
IGNORED
THIS
IS
DONE
SO
THAT
IF
WE
TYPE
THE
INTERRUPT
CHARACTER
IT
DOESN
T
AFFECT
THE
BACKGROUND
PROCESS
IF
THIS
WEREN
T
DONE
AND
WE
TYPED
THE
INTERRUPT
CHARACTER
IT
WOULD
TERMINATE
NOT
ONLY
THE
FOREGROUND
PROCESS
BUT
ALSO
ALL
THE
BACKGROUND
PROCESSES
MANY
INTERACTIVE
PROGRAMS
THAT
CATCH
THESE
TWO
SIGNALS
HAVE
CODE
THAT
LOOKS
LIKE
VOID
INT
INT
IF
SIGNAL
SIGINT
SIGNAL
SIGINT
IF
SIGNAL
SIGQUIT
SIGNAL
SIGQUIT
FOLLOWING
THIS
APPROACH
THE
PROCESS
CATCHES
THE
SIGNAL
ONLY
IF
THE
SIGNAL
IS
NOT
CURRENTLY
BEING
IGNORED
THESE
TWO
CALLS
TO
SIGNAL
ALSO
SHOW
A
LIMITATION
OF
THE
SIGNAL
FUNCTION
WE
ARE
NOT
ABLE
TO
DETERMINE
THE
CURRENT
DISPOSITION
OF
A
SIGNAL
WITHOUT
CHANGING
THE
DISPOSITION
WE
LL
SEE
LATER
IN
THIS
CHAPTER
HOW
THE
SIGACTION
FUNCTION
ALLOWS
US
TO
DETERMINE
A
SIGNAL
DISPOSITION
WITHOUT
CHANGING
IT
PROCESS
CREATION
WHEN
A
PROCESS
CALLS
FORK
THE
CHILD
INHERITS
THE
PARENT
SIGNAL
DISPOSITIONS
HERE
SINCE
THE
CHILD
STARTS
OFF
WITH
A
COPY
OF
THE
PARENT
MEMORY
IMAGE
THE
ADDRESS
OF
A
SIGNAL
CATCHING
FUNCTION
HAS
MEANING
IN
THE
CHILD
UNRELIABLE
SIGNALS
IN
EARLIER
VERSIONS
OF
THE
UNIX
SYSTEM
SUCH
AS
VERSION
SIGNALS
WERE
UNRELIABLE
BY
THIS
WE
MEAN
THAT
SIGNALS
COULD
GET
LOST
A
SIGNAL
COULD
OCCUR
AND
THE
PROCESS
WOULD
NEVER
KNOW
ABOUT
IT
ALSO
A
PROCESS
HAD
LITTLE
CONTROL
OVER
A
SIGNAL
A
PROCESS
COULD
CATCH
THE
SIGNAL
OR
IGNORE
IT
SOMETIMES
WE
WOULD
LIKE
TO
TELL
THE
KERNEL
TO
BLOCK
A
SIGNAL
DON
T
IGNORE
IT
JUST
REMEMBER
IF
IT
OCCURS
AND
TELL
US
LATER
WHEN
WE
RE
READY
CHANGES
WERE
MADE
WITH
TO
PROVIDE
WHAT
ARE
CALLED
RELIABLE
SIGNALS
A
DIFFERENT
SET
OF
CHANGES
WAS
THEN
MADE
IN
TO
PROVIDE
RELIABLE
SIGNALS
UNDER
SYSTEM
V
POSIX
CHOSE
THE
BSD
MODEL
TO
STANDARDIZE
ONE
PROBLEM
WITH
THESE
EARLY
VERSIONS
WAS
THAT
THE
ACTION
FOR
A
SIGNAL
WAS
RESET
TO
ITS
DEFAULT
EACH
TIME
THE
SIGNAL
OCCURRED
IN
THE
PREVIOUS
EXAMPLE
WHEN
WE
RAN
THE
PROGRAM
IN
FIGURE
WE
AVOIDED
THIS
DETAIL
BY
CATCHING
EACH
SIGNAL
ONLY
ONCE
THE
CLASSIC
EXAMPLE
FROM
PROGRAMMING
BOOKS
THAT
DESCRIBED
THESE
EARLIER
SYSTEMS
CONCERNS
HOW
TO
HANDLE
THE
INTERRUPT
SIGNAL
THE
CODE
THAT
WAS
DESCRIBED
USUALLY
LOOKED
LIKE
INT
MY
SIGNAL
HANDLING
FUNCTION
SIGNAL
SIGINT
ESTABLISH
HANDLER
SIGNAL
SIGINT
REESTABLISH
HANDLER
FOR
NEXT
TIME
PROCESS
THE
SIGNAL
THE
REASON
THE
SIGNAL
HANDLER
IS
DECLARED
AS
RETURNING
AN
INTEGER
IS
THAT
THESE
EARLY
SYSTEMS
DIDN
T
SUPPORT
THE
ISO
C
VOID
DATA
TYPE
THE
PROBLEM
WITH
THIS
CODE
FRAGMENT
IS
THAT
THERE
IS
A
WINDOW
OF
TIME
AFTER
THE
SIGNAL
HAS
OCCURRED
BUT
BEFORE
THE
CALL
TO
SIGNAL
IN
THE
SIGNAL
HANDLER
WHEN
THE
INTERRUPT
SIGNAL
COULD
OCCUR
ANOTHER
TIME
THIS
SECOND
SIGNAL
WOULD
CAUSE
THE
DEFAULT
ACTION
TO
OCCUR
WHICH
FOR
THIS
SIGNAL
TERMINATES
THE
PROCESS
THIS
IS
ONE
OF
THOSE
CONDITIONS
THAT
WORKS
CORRECTLY
MOST
OF
THE
TIME
CAUSING
US
TO
THINK
THAT
IT
IS
CORRECT
WHEN
IT
ISN
T
ANOTHER
PROBLEM
WITH
THESE
EARLIER
SYSTEMS
WAS
THAT
THE
PROCESS
WAS
UNABLE
TO
TURN
A
SIGNAL
OFF
WHEN
IT
DIDN
T
WANT
THE
SIGNAL
TO
OCCUR
ALL
THE
PROCESS
COULD
DO
WAS
IGNORE
THE
SIGNAL
THERE
ARE
TIMES
WHEN
WE
WOULD
LIKE
TO
TELL
THE
SYSTEM
PREVENT
THE
FOLLOWING
SIGNALS
FROM
INTERRUPTING
ME
BUT
REMEMBER
IF
THEY
DO
OCCUR
THE
CLASSIC
EXAMPLE
THAT
DEMONSTRATES
THIS
FLAW
IS
SHOWN
BY
A
PIECE
OF
CODE
THAT
CATCHES
A
SIGNAL
AND
SETS
A
FLAG
FOR
THE
PROCESS
THAT
INDICATES
THAT
THE
SIGNAL
OCCURRED
INT
MY
SIGNAL
HANDLING
FUNCTION
INT
SET
NONZERO
WHEN
SIGNAL
OCCURS
MAIN
SIGNAL
SIGINT
ESTABLISH
HANDLER
WHILE
PAUSE
GO
TO
SLEEP
WAITING
FOR
SIGNAL
SIGNAL
SIGINT
REESTABLISH
HANDLER
FOR
NEXT
TIME
SET
FLAG
FOR
MAIN
LOOP
TO
EXAMINE
HERE
THE
PROCESS
IS
CALLING
THE
PAUSE
FUNCTION
TO
PUT
IT
TO
SLEEP
UNTIL
A
SIGNAL
IS
CAUGHT
WHEN
THE
SIGNAL
IS
CAUGHT
THE
SIGNAL
HANDLER
JUST
SETS
THE
FLAG
TO
A
NONZERO
VALUE
THE
PROCESS
IS
AUTOMATICALLY
AWAKENED
BY
THE
KERNEL
AFTER
THE
SIGNAL
HANDLER
RETURNS
NOTICES
THAT
THE
FLAG
IS
NONZERO
AND
DOES
WHATEVER
IT
NEEDS
TO
DO
BUT
THERE
IS
A
WINDOW
OF
TIME
WHEN
THINGS
CAN
GO
WRONG
IF
THE
SIGNAL
OCCURS
AFTER
THE
TEST
OF
BUT
BEFORE
THE
CALL
TO
PAUSE
THE
PROCESS
COULD
GO
TO
SLEEP
FOREVER
ASSUMING
THAT
THE
SIGNAL
IS
NEVER
GENERATED
AGAIN
THIS
OCCURRENCE
OF
THE
SIGNAL
IS
LOST
THIS
IS
ANOTHER
EXAMPLE
OF
SOME
CODE
THAT
ISN
T
RIGHT
YET
IT
WORKS
MOST
OF
THE
TIME
DEBUGGING
THIS
TYPE
OF
PROBLEM
CAN
BE
DIFFICULT
INTERRUPTED
SYSTEM
CALLS
A
CHARACTERISTIC
OF
EARLIER
UNIX
SYSTEMS
WAS
THAT
IF
A
PROCESS
CAUGHT
A
SIGNAL
WHILE
THE
PROCESS
WAS
BLOCKED
IN
A
SLOW
SYSTEM
CALL
THE
SYSTEM
CALL
WAS
INTERRUPTED
THE
SYSTEM
CALL
RETURNED
AN
ERROR
AND
ERRNO
WAS
SET
TO
EINTR
THIS
WAS
DONE
UNDER
THE
ASSUMPTION
THAT
SINCE
A
SIGNAL
OCCURRED
AND
THE
PROCESS
CAUGHT
IT
THERE
IS
A
GOOD
CHANCE
THAT
SOMETHING
HAS
HAPPENED
THAT
SHOULD
WAKE
UP
THE
BLOCKED
SYSTEM
CALL
HERE
WE
HAVE
TO
DIFFERENTIATE
BETWEEN
A
SYSTEM
CALL
AND
A
FUNCTION
IT
IS
A
SYSTEM
CALL
WITHIN
THE
KERNEL
THAT
IS
INTERRUPTED
WHEN
A
SIGNAL
IS
CAUGHT
TO
SUPPORT
THIS
FEATURE
THE
SYSTEM
CALLS
ARE
DIVIDED
INTO
TWO
CATEGORIES
THE
SLOW
SYSTEM
CALLS
AND
ALL
THE
OTHERS
THE
SLOW
SYSTEM
CALLS
ARE
THOSE
THAT
CAN
BLOCK
FOREVER
INCLUDED
IN
THIS
CATEGORY
ARE
READS
THAT
CAN
BLOCK
THE
CALLER
FOREVER
IF
DATA
ISN
T
PRESENT
WITH
CERTAIN
FILE
TYPES
PIPES
TERMINAL
DEVICES
AND
NETWORK
DEVICES
WRITES
THAT
CAN
BLOCK
THE
CALLER
FOREVER
IF
THE
DATA
CAN
T
BE
ACCEPTED
IMMEDIATELY
BY
THESE
SAME
FILE
TYPES
OPENS
ON
CERTAIN
FILE
TYPES
THAT
BLOCK
THE
CALLER
UNTIL
SOME
CONDITION
OCCURS
SUCH
AS
A
TERMINAL
DEVICE
OPEN
WAITING
UNTIL
AN
ATTACHED
MODEM
ANSWERS
THE
PHONE
THE
PAUSE
FUNCTION
WHICH
BY
DEFINITION
PUTS
THE
CALLING
PROCESS
TO
SLEEP
UNTIL
A
SIGNAL
IS
CAUGHT
AND
THE
WAIT
FUNCTION
CERTAIN
IOCTL
OPERATIONS
SOME
OF
THE
INTERPROCESS
COMMUNICATION
FUNCTIONS
CHAPTER
THE
NOTABLE
EXCEPTION
TO
THESE
SLOW
SYSTEM
CALLS
IS
ANYTHING
RELATED
TO
DISK
I
O
ALTHOUGH
A
READ
OR
A
WRITE
OF
A
DISK
FILE
CAN
BLOCK
THE
CALLER
TEMPORARILY
WHILE
THE
DISK
DRIVER
QUEUES
THE
REQUEST
AND
THEN
THE
REQUEST
IS
EXECUTED
UNLESS
A
HARDWARE
ERROR
OCCURS
THE
I
O
OPERATION
ALWAYS
RETURNS
AND
UNBLOCKS
THE
CALLER
QUICKLY
ONE
CONDITION
THAT
IS
HANDLED
BY
INTERRUPTED
SYSTEM
CALLS
FOR
EXAMPLE
IS
WHEN
A
PROCESS
INITIATES
A
READ
FROM
A
TERMINAL
DEVICE
AND
THE
USER
AT
THE
TERMINAL
WALKS
AWAY
FROM
THE
TERMINAL
FOR
AN
EXTENDED
PERIOD
IN
THIS
EXAMPLE
THE
PROCESS
COULD
BE
BLOCKED
FOR
HOURS
OR
DAYS
AND
WOULD
REMAIN
SO
UNLESS
THE
SYSTEM
WAS
TAKEN
DOWN
POSIX
SEMANTICS
FOR
INTERRUPTED
READS
AND
WRITES
CHANGED
WITH
THE
VERSION
OF
THE
STANDARD
EARLIER
VERSIONS
GAVE
IMPLEMENTATIONS
A
CHOICE
OF
HOW
TO
DEAL
WITH
READS
AND
WRITES
THAT
HAVE
PROCESSED
PARTIAL
AMOUNTS
OF
DATA
IF
READ
HAS
RECEIVED
AND
TRANSFERRED
DATA
TO
AN
APPLICATION
BUFFER
BUT
HAS
NOT
YET
RECEIVED
ALL
THAT
THE
APPLICATION
REQUESTED
AND
IS
THEN
INTERRUPTED
THE
OPERATING
SYSTEM
COULD
EITHER
FAIL
THE
SYSTEM
CALL
WITH
ERRNO
SET
TO
EINTR
OR
ALLOW
THE
SYSTEM
CALL
TO
SUCCEED
RETURNING
THE
PARTIAL
AMOUNT
OF
DATA
RECEIVED
SIMILARLY
IF
WRITE
IS
INTERRUPTED
AFTER
TRANSFERRING
SOME
OF
THE
DATA
IN
AN
APPLICATION
BUFFER
THE
OPERATION
SYSTEM
COULD
EITHER
FAIL
THE
SYSTEM
CALL
WITH
ERRNO
SET
TO
EINTR
OR
ALLOW
THE
SYSTEM
CALL
TO
SUCCEED
RETURNING
THE
PARTIAL
AMOUNT
OF
DATA
WRITTEN
HISTORICALLY
IMPLEMENTATIONS
DERIVED
FROM
SYSTEM
V
FAIL
THE
SYSTEM
CALL
WHEREAS
BSD
DERIVED
IMPLEMENTATIONS
RETURN
PARTIAL
SUCCESS
WITH
THE
VERSION
OF
THE
POSIX
STANDARD
THE
BSD
STYLE
SEMANTICS
ARE
REQUIRED
THE
PROBLEM
WITH
INTERRUPTED
SYSTEM
CALLS
IS
THAT
WE
NOW
HAVE
TO
HANDLE
THE
ERROR
RETURN
EXPLICITLY
THE
TYPICAL
CODE
SEQUENCE
ASSUMING
A
READ
OPERATION
AND
ASSUMING
THAT
WE
WANT
TO
RESTART
THE
READ
EVEN
IF
IT
INTERRUPTED
WOULD
BE
AGAIN
IF
N
READ
FD
BUF
BUFFSIZE
IF
ERRNO
EINTR
GOTO
AGAIN
JUST
AN
INTERRUPTED
SYSTEM
CALL
HANDLE
OTHER
ERRORS
TO
PREVENT
APPLICATIONS
FROM
HAVING
TO
HANDLE
INTERRUPTED
SYSTEM
CALLS
INTRODUCED
THE
AUTOMATIC
RESTARTING
OF
CERTAIN
INTERRUPTED
SYSTEM
CALLS
THE
SYSTEM
CALLS
THAT
WERE
AUTOMATICALLY
RESTARTED
ARE
IOCTL
READ
READV
WRITE
WRITEV
WAIT
AND
WAITPID
AS
WE
VE
MENTIONED
THE
FIRST
FIVE
OF
THESE
FUNCTIONS
ARE
INTERRUPTED
BY
A
SIGNAL
ONLY
IF
THEY
ARE
OPERATING
ON
A
SLOW
DEVICE
WAIT
AND
WAITPID
ARE
ALWAYS
INTERRUPTED
WHEN
A
SIGNAL
IS
CAUGHT
SINCE
THIS
CAUSED
A
PROBLEM
FOR
SOME
APPLICATIONS
THAT
DIDN
T
WANT
THE
OPERATION
RESTARTED
IF
IT
WAS
INTERRUPTED
ALLOWED
THE
PROCESS
TO
DISABLE
THIS
FEATURE
ON
A
PER
SIGNAL
BASIS
POSIX
REQUIRES
AN
IMPLEMENTATION
TO
RESTART
SYSTEM
CALLS
ONLY
WHEN
THE
FLAG
IS
IN
EFFECT
FOR
THE
INTERRUPTING
SIGNAL
AS
WE
WILL
SEE
IN
SECTION
THIS
FLAG
IS
USED
WITH
THE
SIGACTION
FUNCTION
TO
ALLOW
APPLICATIONS
TO
REQUEST
THAT
INTERRUPTED
SYSTEM
CALLS
BE
RESTARTED
HISTORICALLY
WHEN
USING
THE
SIGNAL
FUNCTION
TO
ESTABLISH
A
SIGNAL
HANDLER
IMPLEMENTATIONS
VARIED
WITH
RESPECT
TO
HOW
INTERRUPTED
SYSTEM
CALLS
WERE
HANDLED
SYSTEM
V
NEVER
RESTARTED
SYSTEM
CALLS
BY
DEFAULT
BSD
IN
CONTRAST
RESTARTED
THEM
IF
THE
CALLS
WERE
INTERRUPTED
BY
SIGNALS
ON
FREEBSD
LINUX
AND
MAC
OS
X
WHEN
SIGNAL
HANDLERS
ARE
INSTALLED
WITH
THE
SIGNAL
FUNCTION
INTERRUPTED
SYSTEM
CALLS
WILL
BE
RESTARTED
THE
DEFAULT
ON
SOLARIS
HOWEVER
IS
TO
RETURN
AN
ERROR
EINTR
INSTEAD
WHEN
SYSTEM
CALLS
ARE
INTERRUPTED
BY
SIGNAL
HANDLERS
INSTALLED
WITH
THE
SIGNAL
FUNCTION
BY
USING
OUR
OWN
IMPLEMENTATION
OF
THE
SIGNAL
FUNCTION
SHOWN
IN
FIGURE
WE
AVOID
HAVING
TO
DEAL
WITH
THESE
DIFFERENCES
ONE
OF
THE
REASONS
INTRODUCED
THE
AUTOMATIC
RESTART
FEATURE
IS
THAT
SOMETIMES
WE
DON
T
KNOW
THAT
THE
INPUT
OR
OUTPUT
DEVICE
IS
A
SLOW
DEVICE
IF
THE
PROGRAM
WE
WRITE
CAN
BE
USED
INTERACTIVELY
THEN
IT
MIGHT
BE
READING
OR
WRITING
A
SLOW
DEVICE
SINCE
TERMINALS
FALL
INTO
THIS
CATEGORY
IF
WE
CATCH
SIGNALS
IN
THIS
PROGRAM
AND
IF
THE
SYSTEM
DOESN
T
PROVIDE
THE
RESTART
CAPABILITY
THEN
WE
HAVE
TO
TEST
EVERY
READ
OR
WRITE
FOR
THE
INTERRUPTED
ERROR
RETURN
AND
REISSUE
THE
READ
OR
WRITE
FIGURE
SUMMARIZES
THE
SIGNAL
FUNCTIONS
AND
THEIR
SEMANTICS
PROVIDED
BY
THE
VARIOUS
IMPLEMENTATIONS
FUNCTIONS
SYSTEM
SIGNAL
HANDLER
REMAINS
INSTALLED
ABILITY
TO
BLOCK
SIGNALS
AUTOMATIC
RESTART
OF
INTERRUPTED
SYSTEM
CALLS
SIGNAL
ISO
C
POSIX
UNSPECIFIED
UNSPECIFIED
UNSPECIFIED
NEVER
SOLARIS
NEVER
ALWAYS
FREEBSD
LINUX
MAC
OS
X
DEFAULT
SIGACTION
POSIX
FREEBSD
LINUX
MAC
OS
X
SOLARIS
OPTIONAL
FIGURE
FEATURES
PROVIDED
BY
VARIOUS
SIGNAL
IMPLEMENTATIONS
BE
AWARE
THAT
UNIX
SYSTEMS
FROM
OTHER
VENDORS
CAN
HAVE
VALUES
DIFFERENT
FROM
THOSE
SHOWN
IN
THIS
FIGURE
FOR
EXAMPLE
SIGACTION
UNDER
SUNOS
RESTARTS
AN
INTERRUPTED
SYSTEM
CALL
BY
DEFAULT
UNLIKE
THE
PLATFORMS
LISTED
IN
FIGURE
IN
FIGURE
WE
PROVIDE
OUR
OWN
VERSION
OF
THE
SIGNAL
FUNCTION
THAT
AUTOMATICALLY
TRIES
TO
RESTART
INTERRUPTED
SYSTEM
CALLS
OTHER
THAN
FOR
THE
SIGALRM
SIGNAL
IN
FIGURE
WE
PROVIDE
ANOTHER
FUNCTION
THAT
TRIES
TO
NEVER
DO
THE
RESTART
WE
TALK
MORE
ABOUT
INTERRUPTED
SYSTEM
CALLS
IN
SECTION
WITH
REGARD
TO
THE
SELECT
AND
POLL
FUNCTIONS
REENTRANT
FUNCTIONS
WHEN
A
SIGNAL
THAT
IS
BEING
CAUGHT
IS
HANDLED
BY
A
PROCESS
THE
NORMAL
SEQUENCE
OF
INSTRUCTIONS
BEING
EXECUTED
BY
THE
PROCESS
IS
TEMPORARILY
INTERRUPTED
BY
THE
SIGNAL
HANDLER
THE
PROCESS
THEN
CONTINUES
EXECUTING
BUT
THE
INSTRUCTIONS
IN
THE
SIGNAL
HANDLER
ARE
NOW
EXECUTED
IF
THE
SIGNAL
HANDLER
RETURNS
INSTEAD
OF
CALLING
EXIT
OR
LONGJMP
FOR
EXAMPLE
THEN
THE
NORMAL
SEQUENCE
OF
INSTRUCTIONS
THAT
THE
PROCESS
WAS
EXECUTING
WHEN
THE
SIGNAL
WAS
CAUGHT
CONTINUES
EXECUTING
THIS
IS
SIMILAR
TO
WHAT
HAPPENS
WHEN
A
HARDWARE
INTERRUPT
OCCURS
BUT
IN
THE
SIGNAL
HANDLER
WE
CAN
T
TELL
WHERE
THE
PROCESS
WAS
EXECUTING
WHEN
THE
SIGNAL
WAS
CAUGHT
WHAT
IF
THE
PROCESS
WAS
IN
THE
MIDDLE
OF
ALLOCATING
ADDITIONAL
MEMORY
ON
ITS
HEAP
USING
MALLOC
AND
WE
CALL
MALLOC
FROM
THE
SIGNAL
HANDLER
OR
WHAT
IF
THE
PROCESS
WAS
IN
THE
MIDDLE
OF
A
CALL
TO
A
FUNCTION
SUCH
AS
GETPWNAM
SECTION
THAT
STORES
ITS
RESULT
IN
A
STATIC
LOCATION
AND
WE
CALL
THE
SAME
FUNCTION
FROM
THE
SIGNAL
HANDLER
IN
THE
MALLOC
EXAMPLE
HAVOC
CAN
RESULT
FOR
THE
PROCESS
SINCE
MALLOC
USUALLY
MAINTAINS
A
LINKED
LIST
OF
ALL
ITS
ALLOCATED
AREAS
AND
IT
MAY
HAVE
BEEN
IN
THE
MIDDLE
OF
CHANGING
THIS
LIST
IN
THE
CASE
OF
GETPWNAM
THE
INFORMATION
RETURNED
TO
THE
NORMAL
CALLER
CAN
GET
OVERWRITTEN
WITH
THE
INFORMATION
RETURNED
TO
THE
SIGNAL
HANDLER
THE
SINGLE
UNIX
SPECIFICATION
SPECIFIES
THE
FUNCTIONS
THAT
ARE
GUARANTEED
TO
BE
SAFE
TO
CALL
FROM
WITHIN
A
SIGNAL
HANDLER
THESE
FUNCTIONS
ARE
REENTRANT
AND
ARE
CALLED
ASYNC
SIGNAL
SAFE
BY
THE
SINGLE
UNIX
SPECIFICATION
BESIDES
BEING
REENTRANT
THEY
BLOCK
ANY
SIGNALS
DURING
OPERATION
IF
DELIVERY
OF
A
SIGNAL
MIGHT
CAUSE
INCONSISTENCIES
FIGURE
LISTS
THESE
ASYNC
SIGNAL
SAFE
FUNCTIONS
MOST
OF
THE
FUNCTIONS
THAT
ARE
NOT
INCLUDED
IN
FIGURE
ARE
MISSING
BECAUSE
A
THEY
ARE
KNOWN
TO
USE
STATIC
DATA
STRUCTURES
B
THEY
CALL
MALLOC
OR
FREE
OR
C
THEY
ARE
PART
OF
THE
STANDARD
I
O
LIBRARY
MOST
IMPLEMENTATIONS
OF
THE
STANDARD
I
O
LIBRARY
USE
GLOBAL
DATA
STRUCTURES
IN
A
NONREENTRANT
WAY
NOTE
THAT
EVEN
THOUGH
WE
CALL
PRINTF
FROM
SIGNAL
HANDLERS
IN
SOME
OF
OUR
EXAMPLES
IT
IS
NOT
GUARANTEED
TO
PRODUCE
THE
EXPECTED
RESULTS
SINCE
THE
SIGNAL
HANDLER
CAN
INTERRUPT
A
CALL
TO
PRINTF
FROM
OUR
MAIN
PROGRAM
BE
AWARE
THAT
EVEN
IF
WE
CALL
A
FUNCTION
LISTED
IN
FIGURE
FROM
A
SIGNAL
HANDLER
THERE
IS
ONLY
ONE
ERRNO
VARIABLE
PER
THREAD
RECALL
THE
DISCUSSION
OF
ERRNO
AND
THREADS
IN
SECTION
AND
WE
MIGHT
POTENTIALLY
MODIFY
ITS
VALUE
CONSIDER
A
SIGNAL
HANDLER
THAT
IS
INVOKED
RIGHT
AFTER
MAIN
HAS
SET
ERRNO
IF
THE
SIGNAL
HANDLER
CALLS
READ
FOR
EXAMPLE
THIS
CALL
CAN
CHANGE
THE
VALUE
OF
ERRNO
WIPING
OUT
THE
VALUE
THAT
WAS
JUST
ABORT
FACCESSAT
LINKAT
SELECT
SOCKETPAIR
ACCEPT
FCHMOD
LISTEN
STAT
ACCESS
FCHMODAT
LSEEK
SEND
SYMLINK
FCHOWN
LSTAT
SENDMSG
SYMLINKAT
FCHOWNAT
MKDIR
SENDTO
TCDRAIN
FCNTL
MKDIRAT
SETGID
TCFLOW
ALARM
FDATASYNC
MKFIFO
SETPGID
TCFLUSH
BIND
FEXECVE
MKFIFOAT
SETSID
TCGETATTR
CFGETISPEED
FORK
MKNOD
SETSOCKOPT
TCGETPGRP
CFGETOSPEED
FSTAT
MKNODAT
SETUID
TCSENDBREAK
CFSETISPEED
FSTATAT
OPEN
SHUTDOWN
TCSETATTR
CFSETOSPEED
FSYNC
OPENAT
SIGACTION
TCSETPGRP
CHDIR
FTRUNCATE
PAUSE
SIGADDSET
TIME
CHMOD
FUTIMENS
PIPE
SIGDELSET
CHOWN
GETEGID
POLL
SIGEMPTYSET
GETEUID
SIGFILLSET
CLOSE
GETGID
PSELECT
SIGISMEMBER
TIMES
CONNECT
GETGROUPS
RAISE
SIGNAL
UMASK
CREAT
GETPEERNAME
READ
SIGPAUSE
UNAME
DUP
GETPGRP
READLINK
SIGPENDING
UNLINK
GETPID
READLINKAT
SIGPROCMASK
UNLINKAT
EXECL
GETPPID
RECV
SIGQUEUE
UTIME
EXECLE
GETSOCKNAME
RECVFROM
SIGSET
UTIMENSAT
EXECV
GETSOCKOPT
RECVMSG
SIGSUSPEND
UTIMES
EXECVE
GETUID
RENAME
SLEEP
WAIT
KILL
RENAMEAT
SOCKATMARK
WAITPID
LINK
RMDIR
SOCKET
WRITE
FIGURE
REENTRANT
FUNCTIONS
THAT
MAY
BE
CALLED
FROM
A
SIGNAL
HANDLER
STORED
IN
MAIN
THEREFORE
AS
A
GENERAL
RULE
WHEN
CALLING
THE
FUNCTIONS
LISTED
IN
FIGURE
FROM
A
SIGNAL
HANDLER
WE
SHOULD
SAVE
AND
RESTORE
ERRNO
BE
AWARE
THAT
A
COMMONLY
CAUGHT
SIGNAL
IS
SIGCHLD
AND
ITS
SIGNAL
HANDLER
USUALLY
CALLS
ONE
OF
THE
WAIT
FUNCTIONS
ALL
THE
WAIT
FUNCTIONS
CAN
CHANGE
ERRNO
NOTE
THAT
LONGJMP
SECTION
AND
SIGLONGJMP
SECTION
ARE
MISSING
FROM
FIGURE
BECAUSE
THE
SIGNAL
MAY
HAVE
OCCURRED
WHILE
THE
MAIN
ROUTINE
WAS
UPDATING
A
DATA
STRUCTURE
IN
A
NONREENTRANT
WAY
THIS
DATA
STRUCTURE
COULD
BE
LEFT
HALF
UPDATED
IF
WE
CALL
SIGLONGJMP
INSTEAD
OF
RETURNING
FROM
THE
SIGNAL
HANDLER
IF
IT
IS
GOING
TO
DO
SUCH
THINGS
AS
UPDATE
GLOBAL
DATA
STRUCTURES
AS
WE
DESCRIBE
HERE
WHILE
CATCHING
SIGNALS
THAT
CAUSE
SIGSETJMP
TO
BE
EXECUTED
AN
APPLICATION
NEEDS
TO
BLOCK
THE
SIGNALS
WHILE
UPDATING
THE
DATA
STRUCTURES
EXAMPLE
FIGURE
SHOWS
A
PROGRAM
THAT
CALLS
THE
NONREENTRANT
FUNCTION
GETPWNAM
FROM
A
SIGNAL
HANDLER
THAT
IS
CALLED
EVERY
SECOND
WE
DESCRIBE
THE
ALARM
FUNCTION
IN
SECTION
WE
USE
IT
HERE
TO
GENERATE
A
SIGALRM
SIGNAL
EVERY
SECOND
INCLUDE
APUE
H
INCLUDE
PWD
H
STATIC
VOID
INT
SIGNO
INT
STRUCT
PASSWD
ROOTPTR
PRINTF
IN
SIGNAL
HANDLER
N
IF
ROOTPTR
GETPWNAM
ROOT
NULL
GETPWNAM
ROOT
ERROR
ALARM
MAIN
VOID
STRUCT
PASSWD
PTR
SIGNAL
SIGALRM
ALARM
FOR
IF
PTR
GETPWNAM
SAR
NULL
GETPWNAM
ERROR
IF
STRCMP
PTR
SAR
PRINTF
RETURN
VALUE
CORRUPTED
N
PTR
FIGURE
CALL
A
NONREENTRANT
FUNCTION
FROM
A
SIGNAL
HANDLER
WHEN
THIS
PROGRAM
WAS
RUN
THE
RESULTS
WERE
RANDOM
USUALLY
THE
PROGRAM
WOULD
BE
TERMINATED
BY
A
SIGSEGV
SIGNAL
WHEN
THE
SIGNAL
HANDLER
RETURNED
AFTER
SEVERAL
ITERATIONS
AN
EXAMINATION
OF
THE
CORE
FILE
SHOWED
THAT
THE
MAIN
FUNCTION
HAD
CALLED
GETPWNAM
BUT
THAT
WHEN
GETPWNAM
CALLED
FREE
THE
SIGNAL
HANDLER
INTERRUPTED
IT
AND
CALLED
GETPWNAM
WHICH
IN
TURN
CALLED
FREE
THE
DATA
STRUCTURES
MAINTAINED
BY
MALLOC
AND
FREE
HAD
BEEN
CORRUPTED
WHEN
THE
SIGNAL
HANDLER
INDIRECTLY
CALLED
FREE
WHILE
THE
MAIN
FUNCTION
WAS
ALSO
CALLING
FREE
OCCASIONALLY
THE
PROGRAM
WOULD
RUN
FOR
SEVERAL
SECONDS
BEFORE
CRASHING
WITH
A
SIGSEGV
ERROR
WHEN
THE
MAIN
FUNCTION
DID
RUN
CORRECTLY
AFTER
THE
SIGNAL
HAD
BEEN
CAUGHT
THE
RETURN
VALUE
WAS
SOMETIMES
CORRUPTED
AND
SOMETIMES
FINE
AS
SHOWN
BY
THIS
EXAMPLE
IF
WE
CALL
A
NONREENTRANT
FUNCTION
FROM
A
SIGNAL
HANDLER
THE
RESULTS
ARE
UNPREDICTABLE
SIGCLD
SEMANTICS
TWO
SIGNALS
THAT
CONTINUALLY
GENERATE
CONFUSION
ARE
SIGCLD
AND
SIGCHLD
THE
NAME
SIGCLD
WITHOUT
THE
H
IS
FROM
SYSTEM
V
AND
THIS
SIGNAL
HAS
DIFFERENT
SEMANTICS
FROM
THE
BSD
SIGNAL
NAMED
SIGCHLD
THE
POSIX
SIGNAL
IS
ALSO
NAMED
SIGCHLD
THE
SEMANTICS
OF
THE
BSD
SIGCHLD
SIGNAL
ARE
NORMAL
IN
THE
SENSE
THAT
ITS
SEMANTICS
ARE
SIMILAR
TO
THOSE
OF
ALL
OTHER
SIGNALS
WHEN
THE
SIGNAL
OCCURS
THE
STATUS
OF
A
CHILD
HAS
CHANGED
AND
WE
NEED
TO
CALL
ONE
OF
THE
WAIT
FUNCTIONS
TO
DETERMINE
WHAT
HAS
HAPPENED
SYSTEM
V
HOWEVER
HAS
TRADITIONALLY
HANDLED
THE
SIGCLD
SIGNAL
DIFFERENTLY
FROM
OTHER
SIGNALS
BASED
SYSTEMS
CONTINUE
THIS
QUESTIONABLE
TRADITION
I
E
COMPATIBILITY
CONSTRAINT
IF
WE
SET
ITS
DISPOSITION
USING
EITHER
SIGNAL
OR
SIGSET
THE
OLDER
COMPATIBLE
FUNCTIONS
TO
SET
THE
DISPOSITION
OF
A
SIGNAL
THIS
OLDER
HANDLING
OF
SIGCLD
CONSISTS
OF
THE
FOLLOWING
BEHAVIOR
IF
THE
PROCESS
SPECIFICALLY
SETS
ITS
DISPOSITION
TO
CHILDREN
OF
THE
CALLING
PROCESS
WILL
NOT
GENERATE
ZOMBIE
PROCESSES
NOTE
THAT
THIS
IS
DIFFERENT
FROM
ITS
DEFAULT
ACTION
WHICH
FROM
FIGURE
IS
TO
BE
IGNORED
INSTEAD
ON
TERMINATION
THE
STATUS
OF
THESE
CHILD
PROCESSES
IS
DISCARDED
IF
IT
SUBSEQUENTLY
CALLS
ONE
OF
THE
WAIT
FUNCTIONS
THE
CALLING
PROCESS
WILL
BLOCK
UNTIL
ALL
ITS
CHILDREN
HAVE
TERMINATED
AND
THEN
WAIT
RETURNS
WITH
ERRNO
SET
TO
ECHILD
THE
DEFAULT
DISPOSITION
OF
THIS
SIGNAL
IS
TO
BE
IGNORED
BUT
THIS
DEFAULT
WILL
NOT
CAUSE
THE
PRECEDING
SEMANTICS
TO
OCCUR
INSTEAD
WE
SPECIFICALLY
HAVE
TO
SET
ITS
DISPOSITION
TO
POSIX
DOES
NOT
SPECIFY
WHAT
HAPPENS
WHEN
SIGCHLD
IS
IGNORED
SO
THIS
BEHAVIOR
IS
ALLOWED
THE
XSI
OPTION
REQUIRES
THIS
BEHAVIOR
TO
BE
SUPPORTED
FOR
SIGCHLD
ALWAYS
GENERATES
ZOMBIES
IF
SIGCHLD
IS
IGNORED
IF
WE
WANT
TO
AVOID
ZOMBIES
WE
HAVE
TO
WAIT
FOR
OUR
CHILDREN
WITH
IF
EITHER
SIGNAL
OR
SIGSET
IS
CALLED
TO
SET
THE
DISPOSITION
OF
SIGCHLD
TO
BE
IGNORED
ZOMBIES
ARE
NEVER
GENERATED
ALL
FOUR
PLATFORMS
DESCRIBED
IN
THIS
BOOK
FOLLOW
IN
THIS
BEHAVIOR
WITH
SIGACTION
WE
CAN
SET
THE
FLAG
FIGURE
TO
AVOID
ZOMBIES
THIS
ACTION
IS
ALSO
SUPPORTED
ON
ALL
FOUR
PLATFORMS
IF
WE
SET
THE
DISPOSITION
OF
SIGCLD
TO
BE
CAUGHT
THE
KERNEL
IMMEDIATELY
CHECKS
WHETHER
ANY
CHILD
PROCESSES
ARE
READY
TO
BE
WAITED
FOR
AND
IF
SO
CALLS
THE
SIGCLD
HANDLER
ITEM
CHANGES
THE
WAY
WE
HAVE
TO
WRITE
A
SIGNAL
HANDLER
FOR
THIS
SIGNAL
AS
ILLUSTRATED
IN
THE
FOLLOWING
EXAMPLE
EXAMPLE
RECALL
FROM
SECTION
THAT
THE
FIRST
THING
TO
DO
ON
ENTRY
TO
A
SIGNAL
HANDLER
IS
TO
CALL
SIGNAL
AGAIN
TO
REESTABLISH
THE
HANDLER
THIS
ACTION
IS
INTENDED
TO
MINIMIZE
THE
WINDOW
OF
TIME
WHEN
THE
SIGNAL
IS
RESET
BACK
TO
ITS
DEFAULT
AND
COULD
GET
LOST
WE
SHOW
THIS
IN
FIGURE
THIS
PROGRAM
DOESN
T
WORK
ON
TRADITIONAL
SYSTEM
V
PLATFORMS
THE
OUTPUT
IS
A
CONTINUAL
STRING
OF
SIGCLD
RECEIVED
LINES
EVENTUALLY
THE
PROCESS
RUNS
OUT
OF
STACK
SPACE
AND
TERMINATES
ABNORMALLY
INCLUDE
APUE
H
INCLUDE
SYS
WAIT
H
STATIC
VOID
INT
INT
MAIN
PID
IF
SIGNAL
SIGCLD
PERROR
SIGNAL
ERROR
IF
PID
FORK
PERROR
FORK
ERROR
ELSE
IF
PID
CHILD
SLEEP
PAUSE
PARENT
EXIT
STATIC
VOID
INT
SIGNO
INTERRUPTS
PAUSE
PID
INT
STATUS
PRINTF
SIGCLD
RECEIVED
N
IF
SIGNAL
SIGCLD
REESTABLISH
HANDLER
PERROR
SIGNAL
ERROR
IF
PID
WAIT
STATUS
FETCH
CHILD
STATUS
PERROR
WAIT
ERROR
PRINTF
PID
D
N
PID
FIGURE
SYSTEM
V
SIGCLD
HANDLER
THAT
DOESN
T
WORK
FREEBSD
AND
MAC
OS
X
DON
T
EXHIBIT
THIS
PROBLEM
BECAUSE
BSD
BASED
SYSTEMS
GENERALLY
DON
T
SUPPORT
HISTORICAL
SYSTEM
V
SEMANTICS
FOR
SIGCLD
LINUX
ALSO
DOESN
T
EXHIBIT
THIS
PROBLEM
BECAUSE
IT
DOESN
T
CALL
THE
SIGCHLD
SIGNAL
HANDLER
WHEN
A
PROCESS
ARRANGES
TO
CATCH
SIGCHLD
AND
CHILD
PROCESSES
ARE
READY
TO
BE
WAITED
FOR
EVEN
THOUGH
SIGCLD
AND
SIGCHLD
ARE
DEFINED
TO
BE
THE
SAME
VALUE
SOLARIS
ON
THE
OTHER
HAND
DOES
CALL
THE
SIGNAL
HANDLER
IN
THIS
SITUATION
BUT
INCLUDES
EXTRA
CODE
IN
THE
KERNEL
TO
AVOID
THIS
PROBLEM
ALTHOUGH
THE
FOUR
PLATFORMS
DESCRIBED
IN
THIS
BOOK
SOLVE
THIS
PROBLEM
REALIZE
THAT
PLATFORMS
SUCH
AS
AIX
STILL
EXIST
THAT
HAVEN
T
ADDRESSED
IT
THE
PROBLEM
WITH
THIS
PROGRAM
IS
THAT
THE
CALL
TO
SIGNAL
AT
THE
BEGINNING
OF
THE
SIGNAL
HANDLER
INVOKES
ITEM
FROM
THE
PRECEDING
DISCUSSION
THE
KERNEL
CHECKS
WHETHER
A
CHILD
NEEDS
TO
BE
WAITED
FOR
WHICH
IS
THE
CASE
SINCE
WE
RE
PROCESSING
A
SIGCLD
SIGNAL
SO
IT
GENERATES
ANOTHER
CALL
TO
THE
SIGNAL
HANDLER
THE
SIGNAL
HANDLER
CALLS
SIGNAL
AND
THE
WHOLE
PROCESS
STARTS
OVER
AGAIN
TO
FIX
THIS
PROGRAM
WE
HAVE
TO
MOVE
THE
CALL
TO
SIGNAL
AFTER
THE
CALL
TO
WAIT
BY
DOING
THIS
WE
CALL
SIGNAL
AFTER
FETCHING
THE
CHILD
TERMINATION
STATUS
THE
SIGNAL
IS
GENERATED
AGAIN
BY
THE
KERNEL
ONLY
IF
SOME
OTHER
CHILD
HAS
SINCE
TERMINATED
POSIX
STATES
THAT
WHEN
WE
ESTABLISH
A
SIGNAL
HANDLER
FOR
SIGCHLD
AND
THERE
EXISTS
A
TERMINATED
CHILD
WE
HAVE
NOT
YET
WAITED
FOR
IT
IS
UNSPECIFIED
WHETHER
THE
SIGNAL
IS
GENERATED
THIS
ALLOWS
THE
BEHAVIOR
DESCRIBED
PREVIOUSLY
BUT
SINCE
POSIX
DOES
NOT
RESET
A
SIGNAL
DISPOSITION
TO
ITS
DEFAULT
WHEN
THE
SIGNAL
OCCURS
ASSUMING
THAT
WE
RE
USING
THE
POSIX
SIGACTION
FUNCTION
TO
SET
ITS
DISPOSITION
THERE
IS
NO
NEED
FOR
US
TO
EVER
ESTABLISH
A
SIGNAL
HANDLER
FOR
SIGCHLD
WITHIN
THAT
HANDLER
BE
COGNIZANT
OF
THE
SIGCHLD
SEMANTICS
FOR
YOUR
IMPLEMENTATION
BE
ESPECIALLY
AWARE
OF
SOME
SYSTEMS
THAT
DEFINE
SIGCHLD
TO
BE
SIGCLD
OR
VICE
VERSA
CHANGING
THE
NAME
MAY
ALLOW
YOU
TO
COMPILE
A
PROGRAM
THAT
WAS
WRITTEN
FOR
ANOTHER
SYSTEM
BUT
IF
THAT
PROGRAM
DEPENDS
ON
THE
OTHER
SEMANTICS
IT
MAY
NOT
WORK
OF
THE
FOUR
PLATFORMS
DESCRIBED
IN
THIS
TEXT
ONLY
LINUX
AND
SOLARIS
DEFINE
SIGCLD
ON
THESE
PLATFORMS
SIGCLD
IS
EQUIVALENT
TO
SIGCHLD
RELIABLE
SIGNAL
TERMINOLOGY
AND
SEMANTICS
WE
NEED
TO
DEFINE
SOME
OF
THE
TERMS
USED
THROUGHOUT
OUR
DISCUSSION
OF
SIGNALS
FIRST
A
SIGNAL
IS
GENERATED
FOR
A
PROCESS
OR
SENT
TO
A
PROCESS
WHEN
THE
EVENT
THAT
CAUSES
THE
SIGNAL
OCCURS
THE
EVENT
COULD
BE
A
HARDWARE
EXCEPTION
E
G
DIVIDE
BY
A
SOFTWARE
CONDITION
E
G
AN
ALARM
TIMER
EXPIRING
A
TERMINAL
GENERATED
SIGNAL
OR
A
CALL
TO
THE
KILL
FUNCTION
WHEN
THE
SIGNAL
IS
GENERATED
THE
KERNEL
USUALLY
SETS
A
FLAG
OF
SOME
FORM
IN
THE
PROCESS
TABLE
WE
SAY
THAT
A
SIGNAL
IS
DELIVERED
TO
A
PROCESS
WHEN
THE
ACTION
FOR
A
SIGNAL
IS
TAKEN
DURING
THE
TIME
BETWEEN
THE
GENERATION
OF
A
SIGNAL
AND
ITS
DELIVERY
THE
SIGNAL
IS
SAID
TO
BE
PENDING
A
PROCESS
HAS
THE
OPTION
OF
BLOCKING
THE
DELIVERY
OF
A
SIGNAL
IF
A
SIGNAL
THAT
IS
BLOCKED
IS
GENERATED
FOR
A
PROCESS
AND
IF
THE
ACTION
FOR
THAT
SIGNAL
IS
EITHER
THE
DEFAULT
ACTION
OR
TO
CATCH
THE
SIGNAL
THEN
THE
SIGNAL
REMAINS
PENDING
FOR
THE
PROCESS
UNTIL
THE
PROCESS
EITHER
A
UNBLOCKS
THE
SIGNAL
OR
B
CHANGES
THE
ACTION
TO
IGNORE
THE
SIGNAL
THE
SYSTEM
DETERMINES
WHAT
TO
DO
WITH
A
BLOCKED
SIGNAL
WHEN
THE
SIGNAL
IS
DELIVERED
NOT
WHEN
IT
GENERATED
THIS
ALLOWS
THE
PROCESS
TO
CHANGE
THE
ACTION
FOR
THE
SIGNAL
BEFORE
IT
DELIVERED
THE
SIGPENDING
FUNCTION
SECTION
CAN
BE
CALLED
BY
A
PROCESS
TO
DETERMINE
WHICH
SIGNALS
ARE
BLOCKED
AND
PENDING
WHAT
HAPPENS
IF
A
BLOCKED
SIGNAL
IS
GENERATED
MORE
THAN
ONCE
BEFORE
THE
PROCESS
UNBLOCKS
THE
SIGNAL
POSIX
ALLOWS
THE
SYSTEM
TO
DELIVER
THE
SIGNAL
EITHER
ONCE
OR
MORE
THAN
ONCE
IF
THE
SYSTEM
DELIVERS
THE
SIGNAL
MORE
THAN
ONCE
WE
SAY
THAT
THE
SIGNALS
ARE
QUEUED
MOST
UNIX
SYSTEMS
HOWEVER
DO
NOT
QUEUE
SIGNALS
UNLESS
THEY
SUPPORT
THE
REAL
TIME
EXTENSIONS
TO
POSIX
INSTEAD
THE
UNIX
KERNEL
SIMPLY
DELIVERS
THE
SIGNAL
ONCE
WITH
THE
REAL
TIME
SIGNAL
FUNCTIONALITY
MOVED
FROM
THE
REAL
TIME
EXTENSIONS
TO
THE
BASE
SPECIFICATION
AS
TIME
GOES
ON
MORE
SYSTEMS
WILL
SUPPORT
QUEUEING
SIGNALS
EVEN
IF
THEY
DON
T
SUPPORT
THE
REAL
TIME
EXTENSIONS
WE
DISCUSS
QUEUEING
SIGNALS
FURTHER
IN
SECTION
THE
MANUAL
PAGES
FOR
CLAIMED
THAT
THE
SIGCLD
SIGNAL
WAS
QUEUED
WHILE
THE
PROCESS
WAS
EXECUTING
ITS
SIGCLD
SIGNAL
HANDLER
ALTHOUGH
THIS
MIGHT
HAVE
BEEN
TRUE
ON
A
CONCEPTUAL
LEVEL
THE
ACTUAL
IMPLEMENTATION
WAS
DIFFERENT
INSTEAD
THE
SIGNAL
WAS
REGENERATED
BY
THE
KERNEL
AS
WE
DESCRIBED
IN
SECTION
IN
THE
MANUAL
WAS
CHANGED
TO
INDICATE
THAT
THE
SIGCLD
SIGNAL
WAS
IGNORED
WHILE
THE
PROCESS
WAS
EXECUTING
ITS
SIGNAL
HANDLER
FOR
SIGCLD
THE
MANUAL
REMOVED
ANY
MENTION
OF
WHAT
HAPPENS
TO
SIGCLD
SIGNALS
THAT
ARE
GENERATED
WHILE
A
PROCESS
IS
EXECUTING
ITS
SIGCLD
SIGNAL
HANDLER
THE
SIGACTION
MANUAL
PAGE
IN
AT
T
CLAIMS
THAT
THE
FLAG
FIGURE
CAUSES
SIGNALS
TO
BE
RELIABLY
QUEUED
THIS
IS
WRONG
APPARENTLY
THIS
FEATURE
WAS
PARTIALLY
IMPLEMENTED
WITHIN
THE
KERNEL
BUT
IT
IS
NOT
ENABLED
IN
CURIOUSLY
THE
SVID
DIDN
T
MAKE
THE
SAME
CLAIMS
OF
RELIABLE
QUEUING
WHAT
HAPPENS
IF
MORE
THAN
ONE
SIGNAL
IS
READY
TO
BE
DELIVERED
TO
A
PROCESS
POSIX
DOES
NOT
SPECIFY
THE
ORDER
IN
WHICH
THE
SIGNALS
ARE
DELIVERED
TO
THE
PROCESS
THE
RATIONALE
FOR
POSIX
DOES
SUGGEST
HOWEVER
THAT
SIGNALS
RELATED
TO
THE
CURRENT
STATE
OF
THE
PROCESS
BE
DELIVERED
BEFORE
OTHER
SIGNALS
SIGSEGV
IS
ONE
SUCH
SIGNAL
EACH
PROCESS
HAS
A
SIGNAL
MASK
THAT
DEFINES
THE
SET
OF
SIGNALS
CURRENTLY
BLOCKED
FROM
DELIVERY
TO
THAT
PROCESS
WE
CAN
THINK
OF
THIS
MASK
AS
HAVING
ONE
BIT
FOR
EACH
POSSIBLE
SIGNAL
IF
THE
BIT
IS
ON
FOR
A
GIVEN
SIGNAL
THAT
SIGNAL
IS
CURRENTLY
BLOCKED
A
PROCESS
CAN
EXAMINE
AND
CHANGE
ITS
CURRENT
SIGNAL
MASK
BY
CALLING
SIGPROCMASK
WHICH
WE
DESCRIBE
IN
SECTION
SINCE
IT
IS
POSSIBLE
FOR
THE
NUMBER
OF
SIGNALS
TO
EXCEED
THE
NUMBER
OF
BITS
IN
AN
INTEGER
POSIX
DEFINES
A
DATA
TYPE
CALLED
THAT
HOLDS
A
SIGNAL
SET
THE
SIGNAL
MASK
FOR
EXAMPLE
IS
STORED
IN
ONE
OF
THESE
SIGNAL
SETS
WE
DESCRIBE
FIVE
FUNCTIONS
THAT
OPERATE
ON
SIGNAL
SETS
IN
SECTION
KILL
AND
RAISE
FUNCTIONS
THE
KILL
FUNCTION
SENDS
A
SIGNAL
TO
A
PROCESS
OR
A
GROUP
OF
PROCESSES
THE
RAISE
FUNCTION
ALLOWS
A
PROCESS
TO
SEND
A
SIGNAL
TO
ITSELF
THE
RAISE
FUNCTION
WAS
ORIGINALLY
DEFINED
BY
ISO
C
POSIX
INCLUDES
IT
TO
ALIGN
ITSELF
WITH
THE
ISO
C
STANDARD
BUT
POSIX
EXTENDS
THE
SPECIFICATION
OF
RAISE
TO
DEAL
WITH
THREADS
WE
DISCUSS
HOW
THREADS
INTERACT
WITH
SIGNALS
IN
SECTION
SINCE
ISO
C
DOES
NOT
DEAL
WITH
MULTIPLE
PROCESSES
IT
COULD
NOT
DEFINE
A
FUNCTION
SUCH
AS
KILL
THAT
REQUIRES
A
PROCESS
ID
ARGUMENT
INCLUDE
SIGNAL
H
INT
KILL
PID
INT
SIGNO
INT
RAISE
INT
SIGNO
THE
CALL
RAISE
SIGNO
IS
EQUIVALENT
TO
THE
CALL
KILL
GETPID
SIGNO
BOTH
RETURN
IF
OK
ON
ERROR
THERE
ARE
FOUR
DIFFERENT
CONDITIONS
FOR
THE
PID
ARGUMENT
TO
KILL
PID
THE
SIGNAL
IS
SENT
TO
THE
PROCESS
WHOSE
PROCESS
ID
IS
PID
PID
THE
SIGNAL
IS
SENT
TO
ALL
PROCESSES
WHOSE
PROCESS
GROUP
ID
EQUALS
THE
PROCESS
GROUP
ID
OF
THE
SENDER
AND
FOR
WHICH
THE
SENDER
HAS
PERMISSION
TO
SEND
THE
SIGNAL
NOTE
THAT
THE
TERM
ALL
PROCESSES
EXCLUDES
AN
IMPLEMENTATION
DEFINED
SET
OF
SYSTEM
PROCESSES
FOR
MOST
UNIX
SYSTEMS
THIS
SET
OF
SYSTEM
PROCESSES
INCLUDES
THE
KERNEL
PROCESSES
AND
INIT
PID
PID
THE
SIGNAL
IS
SENT
TO
ALL
PROCESSES
WHOSE
PROCESS
GROUP
ID
EQUALS
THE
ABSOLUTE
VALUE
OF
PID
AND
FOR
WHICH
THE
SENDER
HAS
PERMISSION
TO
SEND
THE
SIGNAL
AGAIN
THE
SET
OF
ALL
PROCESSES
EXCLUDES
CERTAIN
SYSTEM
PROCESSES
AS
DESCRIBED
EARLIER
PID
THE
SIGNAL
IS
SENT
TO
ALL
PROCESSES
ON
THE
SYSTEM
FOR
WHICH
THE
SENDER
HAS
PERMISSION
TO
SEND
THE
SIGNAL
AS
BEFORE
THE
SET
OF
PROCESSES
EXCLUDES
CERTAIN
SYSTEM
PROCESSES
AS
WE
VE
MENTIONED
A
PROCESS
NEEDS
PERMISSION
TO
SEND
A
SIGNAL
TO
ANOTHER
PROCESS
THE
SUPERUSER
CAN
SEND
A
SIGNAL
TO
ANY
PROCESS
FOR
OTHER
USERS
THE
BASIC
RULE
IS
THAT
THE
REAL
OR
EFFECTIVE
USER
ID
OF
THE
SENDER
HAS
TO
EQUAL
THE
REAL
OR
EFFECTIVE
USER
ID
OF
THE
RECEIVER
IF
THE
IMPLEMENTATION
SUPPORTS
AS
POSIX
NOW
REQUIRES
THE
SAVED
SET
USER
ID
OF
THE
RECEIVER
IS
CHECKED
INSTEAD
OF
ITS
EFFECTIVE
USER
ID
ONE
SPECIAL
CASE
FOR
THE
PERMISSION
TESTING
ALSO
EXISTS
IF
THE
SIGNAL
BEING
SENT
IS
SIGCONT
A
PROCESS
CAN
SEND
IT
TO
ANY
OTHER
PROCESS
IN
THE
SAME
SESSION
POSIX
DEFINES
SIGNAL
NUMBER
AS
THE
NULL
SIGNAL
IF
THE
SIGNO
ARGUMENT
IS
THEN
THE
NORMAL
ERROR
CHECKING
IS
PERFORMED
BY
KILL
BUT
NO
SIGNAL
IS
SENT
THIS
TECHNIQUE
IS
OFTEN
USED
TO
DETERMINE
IF
A
SPECIFIC
PROCESS
STILL
EXISTS
IF
WE
SEND
THE
PROCESS
THE
NULL
SIGNAL
AND
IT
DOESN
T
EXIST
KILL
RETURNS
AND
ERRNO
IS
SET
TO
ESRCH
BE
AWARE
HOWEVER
THAT
UNIX
SYSTEMS
RECYCLE
PROCESS
IDS
AFTER
SOME
AMOUNT
OF
TIME
SO
THE
EXISTENCE
OF
A
PROCESS
WITH
A
GIVEN
PROCESS
ID
DOES
NOT
NECESSARILY
MEAN
THAT
IT
THE
PROCESS
THAT
YOU
THINK
IT
IS
ALSO
UNDERSTAND
THAT
THE
TEST
FOR
PROCESS
EXISTENCE
IS
NOT
ATOMIC
BY
THE
TIME
THAT
KILL
RETURNS
THE
ANSWER
TO
THE
CALLER
THE
PROCESS
IN
QUESTION
MIGHT
HAVE
EXITED
SO
THE
ANSWER
IS
OF
LIMITED
VALUE
IF
THE
CALL
TO
KILL
CAUSES
THE
SIGNAL
TO
BE
GENERATED
FOR
THE
CALLING
PROCESS
AND
IF
THE
SIGNAL
IS
NOT
BLOCKED
EITHER
SIGNO
OR
SOME
OTHER
PENDING
UNBLOCKED
SIGNAL
IS
DELIVERED
TO
THE
PROCESS
BEFORE
KILL
RETURNS
ADDITIONAL
CONDITIONS
OCCUR
WITH
THREADS
SEE
SECTION
FOR
MORE
INFORMATION
ALARM
AND
PAUSE
FUNCTIONS
THE
ALARM
FUNCTION
ALLOWS
US
TO
SET
A
TIMER
THAT
WILL
EXPIRE
AT
A
SPECIFIED
TIME
IN
THE
FUTURE
WHEN
THE
TIMER
EXPIRES
THE
SIGALRM
SIGNAL
IS
GENERATED
IF
WE
IGNORE
OR
DON
T
CATCH
THIS
SIGNAL
ITS
DEFAULT
ACTION
IS
TO
TERMINATE
THE
PROCESS
THE
SECONDS
VALUE
IS
THE
NUMBER
OF
CLOCK
SECONDS
IN
THE
FUTURE
WHEN
THE
SIGNAL
SHOULD
BE
GENERATED
WHEN
THAT
TIME
OCCURS
THE
SIGNAL
IS
GENERATED
BY
THE
KERNEL
ALTHOUGH
ADDITIONAL
TIME
COULD
ELAPSE
BEFORE
THE
PROCESS
GETS
CONTROL
TO
HANDLE
THE
SIGNAL
BECAUSE
OF
PROCESSOR
SCHEDULING
DELAYS
EARLIER
UNIX
SYSTEM
IMPLEMENTATIONS
WARNED
THAT
THE
SIGNAL
COULD
ALSO
BE
SENT
UP
TO
SECOND
EARLY
POSIX
DOES
NOT
ALLOW
THIS
BEHAVIOR
THERE
IS
ONLY
ONE
OF
THESE
ALARM
CLOCKS
PER
PROCESS
IF
WHEN
WE
CALL
ALARM
A
PREVIOUSLY
REGISTERED
ALARM
CLOCK
FOR
THE
PROCESS
HAS
NOT
YET
EXPIRED
THE
NUMBER
OF
SECONDS
LEFT
FOR
THAT
ALARM
CLOCK
IS
RETURNED
AS
THE
VALUE
OF
THIS
FUNCTION
THAT
PREVIOUSLY
REGISTERED
ALARM
CLOCK
IS
REPLACED
BY
THE
NEW
VALUE
IF
A
PREVIOUSLY
REGISTERED
ALARM
CLOCK
FOR
THE
PROCESS
HAS
NOT
YET
EXPIRED
AND
IF
THE
SECONDS
VALUE
IS
THE
PREVIOUS
ALARM
CLOCK
IS
CANCELED
THE
NUMBER
OF
SECONDS
LEFT
FOR
THAT
PREVIOUS
ALARM
CLOCK
IS
STILL
RETURNED
AS
THE
VALUE
OF
THE
FUNCTION
ALTHOUGH
THE
DEFAULT
ACTION
FOR
SIGALRM
IS
TO
TERMINATE
THE
PROCESS
MOST
PROCESSES
THAT
USE
AN
ALARM
CLOCK
CATCH
THIS
SIGNAL
IF
THE
PROCESS
THEN
WANTS
TO
TERMINATE
IT
CAN
PERFORM
WHATEVER
CLEANUP
IS
REQUIRED
BEFORE
TERMINATING
IF
WE
INTEND
TO
CATCH
SIGALRM
WE
NEED
TO
BE
CAREFUL
TO
INSTALL
ITS
SIGNAL
HANDLER
BEFORE
CALLING
ALARM
IF
WE
CALL
ALARM
FIRST
AND
ARE
SENT
SIGALRM
BEFORE
WE
CAN
INSTALL
THE
SIGNAL
HANDLER
OUR
PROCESS
WILL
TERMINATE
THE
PAUSE
FUNCTION
SUSPENDS
THE
CALLING
PROCESS
UNTIL
A
SIGNAL
IS
CAUGHT
INCLUDE
UNISTD
H
INT
PAUSE
VOID
RETURNS
WITH
ERRNO
SET
TO
EINTR
THE
ONLY
TIME
PAUSE
RETURNS
IS
IF
A
SIGNAL
HANDLER
IS
EXECUTED
AND
THAT
HANDLER
RETURNS
IN
THAT
CASE
PAUSE
RETURNS
WITH
ERRNO
SET
TO
EINTR
EXAMPLE
USING
ALARM
AND
PAUSE
WE
CAN
PUT
A
PROCESS
TO
SLEEP
FOR
A
SPECIFIED
AMOUNT
OF
TIME
THE
FUNCTION
IN
FIGURE
APPEARS
TO
DO
THIS
BUT
IT
HAS
PROBLEMS
AS
WE
SHALL
SEE
SHORTLY
INCLUDE
SIGNAL
H
INCLUDE
UNISTD
H
STATIC
VOID
INT
SIGNO
NOTHING
TO
DO
JUST
RETURN
TO
WAKE
UP
THE
PAUSE
UNSIGNED
INT
UNSIGNED
INT
SECONDS
IF
SIGNAL
SIGALRM
RETURN
SECONDS
ALARM
SECONDS
START
THE
TIMER
PAUSE
NEXT
CAUGHT
SIGNAL
WAKES
US
UP
RETURN
ALARM
TURN
OFF
TIMER
RETURN
UNSLEPT
TIME
FIGURE
SIMPLE
INCOMPLETE
IMPLEMENTATION
OF
SLEEP
THIS
FUNCTION
LOOKS
LIKE
THE
SLEEP
FUNCTION
WHICH
WE
DESCRIBE
IN
SECTION
BUT
THIS
SIMPLE
IMPLEMENTATION
HAS
THREE
PROBLEMS
IF
THE
CALLER
ALREADY
HAS
AN
ALARM
SET
THAT
ALARM
IS
ERASED
BY
THE
FIRST
CALL
TO
ALARM
WE
CAN
CORRECT
THIS
BY
LOOKING
AT
ALARM
RETURN
VALUE
IF
THE
NUMBER
OF
SECONDS
UNTIL
SOME
PREVIOUSLY
SET
ALARM
IS
LESS
THAN
THE
ARGUMENT
THEN
WE
SHOULD
WAIT
ONLY
UNTIL
THE
EXISTING
ALARM
EXPIRES
IF
THE
PREVIOUSLY
SET
ALARM
WILL
GO
OFF
AFTER
OURS
THEN
BEFORE
RETURNING
WE
SHOULD
RESET
THIS
ALARM
TO
OCCUR
AT
ITS
DESIGNATED
TIME
IN
THE
FUTURE
WE
HAVE
MODIFIED
THE
DISPOSITION
FOR
SIGALRM
IF
WE
RE
WRITING
A
FUNCTION
FOR
OTHERS
TO
CALL
WE
SHOULD
SAVE
THE
DISPOSITION
WHEN
OUR
FUNCTION
IS
CALLED
AND
RESTORE
IT
WHEN
WE
RE
DONE
WE
CAN
CORRECT
THIS
BY
SAVING
THE
RETURN
VALUE
FROM
SIGNAL
AND
RESETTING
THE
DISPOSITION
BEFORE
OUR
FUNCTION
RETURNS
THERE
IS
A
RACE
CONDITION
BETWEEN
THE
FIRST
CALL
TO
ALARM
AND
THE
CALL
TO
PAUSE
ON
A
BUSY
SYSTEM
IT
POSSIBLE
FOR
THE
ALARM
TO
GO
OFF
AND
THE
SIGNAL
HANDLER
TO
BE
CALLED
BEFORE
WE
CALL
PAUSE
IF
THAT
HAPPENS
THE
CALLER
IS
SUSPENDED
FOREVER
IN
THE
CALL
TO
PAUSE
ASSUMING
THAT
SOME
OTHER
SIGNAL
ISN
T
CAUGHT
EARLIER
IMPLEMENTATIONS
OF
SLEEP
LOOKED
LIKE
OUR
PROGRAM
WITH
PROBLEMS
AND
CORRECTED
AS
DESCRIBED
THERE
ARE
TWO
WAYS
TO
CORRECT
PROBLEM
THE
FIRST
USES
SETJMP
WHICH
WE
SHOW
IN
THE
NEXT
EXAMPLE
THE
OTHER
USES
SIGPROCMASK
AND
SIGSUSPEND
AND
WE
DESCRIBE
IT
IN
SECTION
EXAMPLE
THE
IMPLEMENTATION
OF
SLEEP
USED
SETJMP
AND
LONGJMP
SECTION
TO
AVOID
THE
RACE
CONDITION
DESCRIBED
IN
PROBLEM
OF
THE
PREVIOUS
EXAMPLE
A
SIMPLE
VERSION
OF
THIS
FUNCTION
CALLED
IS
SHOWN
IN
FIGURE
TO
REDUCE
THE
SIZE
OF
THIS
EXAMPLE
WE
DON
T
HANDLE
PROBLEMS
AND
DESCRIBED
EARLIER
INCLUDE
SETJMP
H
INCLUDE
SIGNAL
H
INCLUDE
UNISTD
H
STATIC
STATIC
VOID
INT
SIGNO
LONGJMP
UNSIGNED
INT
UNSIGNED
INT
SECONDS
IF
SIGNAL
SIGALRM
RETURN
SECONDS
IF
SETJMP
ALARM
SECONDS
START
THE
TIMER
PAUSE
NEXT
CAUGHT
SIGNAL
WAKES
US
UP
RETURN
ALARM
TURN
OFF
TIMER
RETURN
UNSLEPT
TIME
FIGURE
ANOTHER
IMPERFECT
IMPLEMENTATION
OF
SLEEP
THE
FUNCTION
AVOIDS
THE
RACE
CONDITION
FROM
FIGURE
EVEN
IF
THE
PAUSE
IS
NEVER
EXECUTED
THE
FUNCTION
RETURNS
WHEN
THE
SIGALRM
OCCURS
THERE
IS
HOWEVER
ANOTHER
SUBTLE
PROBLEM
WITH
THE
FUNCTION
THAT
INVOLVES
ITS
INTERACTION
WITH
OTHER
SIGNALS
IF
THE
SIGALRM
INTERRUPTS
SOME
OTHER
SIGNAL
HANDLER
THEN
WHEN
WE
CALL
LONGJMP
WE
ABORT
THE
OTHER
SIGNAL
HANDLER
FIGURE
SHOWS
THIS
SCENARIO
THE
LOOP
IN
THE
SIGINT
HANDLER
WAS
WRITTEN
SO
THAT
IT
EXECUTES
FOR
LONGER
THAN
SECONDS
ON
ONE
OF
THE
SYSTEMS
USED
BY
THE
AUTHOR
WE
SIMPLY
WANT
IT
TO
EXECUTE
LONGER
THAN
THE
ARGUMENT
TO
THE
INTEGER
K
IS
DECLARED
AS
VOLATILE
TO
PREVENT
AN
OPTIMIZING
COMPILER
FROM
DISCARDING
THE
LOOP
INCLUDE
APUE
H
UNSIGNED
INT
UNSIGNED
INT
STATIC
VOID
INT
INT
MAIN
VOID
UNSIGNED
INT
UNSLEPT
IF
SIGNAL
SIGINT
SIGNAL
SIGINT
ERROR
UNSLEPT
PRINTF
RETURNED
U
N
UNSLEPT
EXIT
STATIC
VOID
INT
SIGNO
INT
I
J
VOLATILE
INT
K
TUNE
THESE
LOOPS
TO
RUN
FOR
MORE
THAN
SECONDS
ON
WHATEVER
SYSTEM
THIS
TEST
PROGRAM
IS
RUN
PRINTF
STARTING
N
FOR
I
I
I
FOR
J
J
J
K
I
J
PRINTF
FINISHED
N
FIGURE
CALLING
FROM
A
PROGRAM
THAT
CATCHES
OTHER
SIGNALS
WHEN
WE
EXECUTE
THE
PROGRAM
SHOWN
IN
FIGURE
AND
INTERRUPT
THE
SLEEP
BY
TYPING
THE
INTERRUPT
CHARACTER
WE
GET
THE
FOLLOWING
OUTPUT
A
OUT
ˆC
WE
TYPE
THE
INTERRUPT
CHARACTER
STARTING
RETURNED
WE
CAN
SEE
THAT
THE
LONGJMP
FROM
THE
FUNCTION
ABORTED
THE
OTHER
SIGNAL
HANDLER
EVEN
THOUGH
IT
WASN
T
FINISHED
THIS
IS
WHAT
YOU
LL
ENCOUNTER
IF
YOU
MIX
THE
SLEEP
FUNCTION
WITH
OTHER
SIGNAL
HANDLING
SEE
EXERCISE
THE
PURPOSE
OF
THE
AND
EXAMPLES
IS
TO
SHOW
THE
PITFALLS
IN
DEALING
NAIVELY
WITH
SIGNALS
THE
FOLLOWING
SECTIONS
WILL
SHOW
WAYS
AROUND
ALL
THESE
PROBLEMS
SO
WE
CAN
HANDLE
SIGNALS
RELIABLY
WITHOUT
INTERFERING
WITH
OTHER
PIECES
OF
CODE
EXAMPLE
A
COMMON
USE
FOR
ALARM
IN
ADDITION
TO
IMPLEMENTING
THE
SLEEP
FUNCTION
IS
TO
PUT
AN
UPPER
TIME
LIMIT
ON
OPERATIONS
THAT
CAN
BLOCK
FOR
EXAMPLE
IF
WE
HAVE
A
READ
OPERATION
ON
A
DEVICE
THAT
CAN
BLOCK
A
SLOW
DEVICE
AS
DESCRIBED
IN
SECTION
WE
MIGHT
WANT
THE
READ
TO
TIME
OUT
AFTER
SOME
AMOUNT
OF
TIME
THE
PROGRAM
IN
FIGURE
DOES
THIS
READING
ONE
LINE
FROM
STANDARD
INPUT
AND
WRITING
IT
TO
STANDARD
OUTPUT
INCLUDE
APUE
H
STATIC
VOID
INT
INT
MAIN
VOID
INT
N
CHAR
LINE
MAXLINE
IF
SIGNAL
SIGALRM
SIGNAL
SIGALRM
ERROR
ALARM
IF
N
READ
LINE
MAXLINE
READ
ERROR
ALARM
WRITE
LINE
N
EXIT
STATIC
VOID
INT
SIGNO
NOTHING
TO
DO
JUST
RETURN
TO
INTERRUPT
THE
READ
FIGURE
CALLING
READ
WITH
A
TIMEOUT
THIS
SEQUENCE
OF
CODE
IS
COMMON
IN
UNIX
APPLICATIONS
BUT
THIS
PROGRAM
HAS
TWO
PROBLEMS
THE
PROGRAM
IN
FIGURE
HAS
ONE
OF
THE
SAME
FLAWS
THAT
WE
DESCRIBED
IN
FIGURE
A
RACE
CONDITION
BETWEEN
THE
FIRST
CALL
TO
ALARM
AND
THE
CALL
TO
READ
IF
THE
KERNEL
BLOCKS
THE
PROCESS
BETWEEN
THESE
TWO
FUNCTION
CALLS
FOR
LONGER
THAN
THE
ALARM
PERIOD
THE
READ
COULD
BLOCK
FOREVER
MOST
OPERATIONS
OF
THIS
TYPE
USE
A
LONG
ALARM
PERIOD
SUCH
AS
A
MINUTE
OR
MORE
MAKING
THIS
UNLIKELY
NEVERTHELESS
IT
IS
A
RACE
CONDITION
IF
SYSTEM
CALLS
ARE
AUTOMATICALLY
RESTARTED
THE
READ
IS
NOT
INTERRUPTED
WHEN
THE
SIGALRM
SIGNAL
HANDLER
RETURNS
IN
THIS
CASE
THE
TIMEOUT
DOES
NOTHING
HERE
WE
SPECIFICALLY
WANT
A
SLOW
SYSTEM
CALL
TO
BE
INTERRUPTED
WE
LL
SEE
A
PORTABLE
WAY
TO
DO
THIS
IN
SECTION
EXAMPLE
LET
REDO
THE
PRECEDING
EXAMPLE
USING
LONGJMP
THIS
WAY
WE
DON
T
NEED
TO
WORRY
ABOUT
WHETHER
A
SLOW
SYSTEM
CALL
IS
INTERRUPTED
INCLUDE
APUE
H
INCLUDE
SETJMP
H
STATIC
VOID
INT
STATIC
INT
MAIN
VOID
INT
N
CHAR
LINE
MAXLINE
IF
SIGNAL
SIGALRM
SIGNAL
SIGALRM
ERROR
IF
SETJMP
READ
TIMEOUT
ALARM
IF
N
READ
LINE
MAXLINE
READ
ERROR
ALARM
WRITE
LINE
N
EXIT
STATIC
VOID
INT
SIGNO
LONGJMP
FIGURE
CALLING
READ
WITH
A
TIMEOUT
USING
LONGJMP
THIS
VERSION
WORKS
AS
EXPECTED
REGARDLESS
OF
WHETHER
THE
SYSTEM
RESTARTS
INTERRUPTED
SYSTEM
CALLS
REALIZE
HOWEVER
THAT
WE
STILL
HAVE
THE
PROBLEM
OF
INTERACTIONS
WITH
OTHER
SIGNAL
HANDLERS
AS
IN
FIGURE
IF
WE
WANT
TO
SET
A
TIME
LIMIT
ON
AN
I
O
OPERATION
WE
NEED
TO
USE
LONGJMP
AS
SHOWN
PREVIOUSLY
WHILE
RECOGNIZING
ITS
POSSIBLE
INTERACTION
WITH
OTHER
SIGNAL
HANDLERS
ANOTHER
OPTION
IS
TO
USE
THE
SELECT
OR
POLL
FUNCTIONS
DESCRIBED
IN
SECTIONS
AND
SIGNAL
SETS
WE
NEED
A
DATA
TYPE
TO
REPRESENT
MULTIPLE
SIGNALS
A
SIGNAL
SET
WE
LL
USE
THIS
DATA
TYPE
WITH
SUCH
FUNCTIONS
AS
SIGPROCMASK
IN
THE
NEXT
SECTION
TO
TELL
THE
KERNEL
NOT
TO
ALLOW
ANY
OF
THE
SIGNALS
IN
THE
SET
TO
OCCUR
AS
WE
MENTIONED
EARLIER
THE
NUMBER
OF
DIFFERENT
SIGNALS
CAN
EXCEED
THE
NUMBER
OF
BITS
IN
AN
INTEGER
SO
IN
GENERAL
WE
CAN
T
USE
AN
INTEGER
TO
REPRESENT
THE
SET
WITH
ONE
BIT
PER
SIGNAL
POSIX
DEFINES
THE
DATA
TYPE
TO
CONTAIN
A
SIGNAL
SET
AND
THE
FOLLOWING
FIVE
FUNCTIONS
TO
MANIPULATE
SIGNAL
SETS
THE
FUNCTION
SIGEMPTYSET
INITIALIZES
THE
SIGNAL
SET
POINTED
TO
BY
SET
SO
THAT
ALL
SIGNALS
ARE
EXCLUDED
THE
FUNCTION
SIGFILLSET
INITIALIZES
THE
SIGNAL
SET
SO
THAT
ALL
SIGNALS
ARE
INCLUDED
ALL
APPLICATIONS
HAVE
TO
CALL
EITHER
SIGEMPTYSET
OR
SIGFILLSET
ONCE
FOR
EACH
SIGNAL
SET
BEFORE
USING
THE
SIGNAL
SET
BECAUSE
WE
CANNOT
ASSUME
THAT
THE
C
INITIALIZATION
FOR
EXTERNAL
AND
STATIC
VARIABLES
CORRESPONDS
TO
THE
IMPLEMENTATION
OF
SIGNAL
SETS
ON
A
GIVEN
SYSTEM
ONCE
WE
HAVE
INITIALIZED
A
SIGNAL
SET
WE
CAN
ADD
AND
DELETE
SPECIFIC
SIGNALS
IN
THE
SET
THE
FUNCTION
SIGADDSET
ADDS
A
SINGLE
SIGNAL
TO
AN
EXISTING
SET
AND
SIGDELSET
REMOVES
A
SINGLE
SIGNAL
FROM
A
SET
IN
ALL
THE
FUNCTIONS
THAT
TAKE
A
SIGNAL
SET
AS
AN
ARGUMENT
WE
ALWAYS
PASS
THE
ADDRESS
OF
THE
SIGNAL
SET
AS
THE
ARGUMENT
IMPLEMENTATION
IF
THE
IMPLEMENTATION
HAS
FEWER
SIGNALS
THAN
BITS
IN
AN
INTEGER
A
SIGNAL
SET
CAN
BE
IMPLEMENTED
USING
ONE
BIT
PER
SIGNAL
FOR
THE
REMAINDER
OF
THIS
SECTION
ASSUME
THAT
AN
IMPLEMENTATION
HAS
SIGNALS
AND
BIT
INTEGERS
THE
SIGEMPTYSET
FUNCTION
ZEROS
THE
INTEGER
AND
THE
SIGFILLSET
FUNCTION
TURNS
ON
ALL
THE
BITS
IN
THE
INTEGER
THESE
TWO
FUNCTIONS
CAN
BE
IMPLEMENTED
AS
MACROS
IN
THE
SIGNAL
H
HEADER
DEFINE
SIGEMPTYSET
PTR
PTR
DEFINE
SIGFILLSET
PTR
PTR
NOTE
THAT
SIGFILLSET
MUST
RETURN
IN
ADDITION
TO
SETTING
ALL
THE
BITS
ON
IN
THE
SIGNAL
SET
SO
WE
USE
C
COMMA
OPERATOR
WHICH
RETURNS
THE
VALUE
AFTER
THE
COMMA
AS
THE
VALUE
OF
THE
EXPRESSION
USING
THIS
IMPLEMENTATION
SIGADDSET
TURNS
ON
A
SINGLE
BIT
AND
SIGDELSET
TURNS
OFF
A
SINGLE
BIT
SIGISMEMBER
TESTS
A
CERTAIN
BIT
SINCE
NO
SIGNAL
IS
EVER
NUMBERED
WE
SUBTRACT
FROM
THE
SIGNAL
NUMBER
TO
OBTAIN
THE
BIT
TO
MANIPULATE
FIGURE
SHOWS
IMPLEMENTATIONS
OF
THESE
FUNCTIONS
INCLUDE
SIGNAL
H
INCLUDE
ERRNO
H
SIGNAL
H
USUALLY
DEFINES
NSIG
TO
INCLUDE
SIGNAL
NUMBER
DEFINE
SIGBAD
SIGNO
SIGNO
SIGNO
NSIG
INT
SIGADDSET
SET
INT
SIGNO
IF
SIGBAD
SIGNO
ERRNO
EINVAL
RETURN
INT
SET
SIGNO
TURN
BIT
ON
RETURN
SIGDELSET
SET
INT
SIGNO
IF
SIGBAD
SIGNO
ERRNO
EINVAL
RETURN
INT
SET
SIGNO
TURN
BIT
OFF
RETURN
SIGISMEMBER
CONST
SET
INT
SIGNO
IF
SIGBAD
SIGNO
ERRNO
EINVAL
RETURN
RETURN
SET
SIGNO
FIGURE
AN
IMPLEMENTATION
OF
SIGADDSET
SIGDELSET
AND
SIGISMEMBER
WE
MIGHT
BE
TEMPTED
TO
IMPLEMENT
THESE
THREE
FUNCTIONS
AS
ONE
LINE
MACROS
IN
THE
SIGNAL
H
HEADER
BUT
POSIX
REQUIRES
US
TO
CHECK
THE
SIGNAL
NUMBER
ARGUMENT
FOR
VALIDITY
AND
TO
SET
ERRNO
IF
IT
IS
INVALID
THIS
IS
MORE
DIFFICULT
TO
DO
IN
A
MACRO
THAN
IN
A
FUNCTION
SIGPROCMASK
FUNCTION
RECALL
FROM
SECTION
THAT
THE
SIGNAL
MASK
OF
A
PROCESS
IS
THE
SET
OF
SIGNALS
CURRENTLY
BLOCKED
FROM
DELIVERY
TO
THAT
PROCESS
A
PROCESS
CAN
EXAMINE
ITS
SIGNAL
MASK
CHANGE
ITS
SIGNAL
MASK
OR
PERFORM
BOTH
OPERATIONS
IN
ONE
STEP
BY
CALLING
THE
FOLLOWING
FUNCTION
FIRST
IF
OSET
IS
A
NON
NULL
POINTER
THE
CURRENT
SIGNAL
MASK
FOR
THE
PROCESS
IS
RETURNED
THROUGH
OSET
SECOND
IF
SET
IS
A
NON
NULL
POINTER
THE
HOW
ARGUMENT
INDICATES
HOW
THE
CURRENT
SIGNAL
MASK
IS
MODIFIED
FIGURE
DESCRIBES
THE
POSSIBLE
VALUES
FOR
HOW
IS
AN
INCLUSIVE
OR
OPERATION
WHEREAS
IS
AN
ASSIGNMENT
NOTE
THAT
SIGKILL
AND
SIGSTOP
CAN
T
BE
BLOCKED
HOW
DESCRIPTION
THE
NEW
SIGNAL
MASK
FOR
THE
PROCESS
IS
THE
UNION
OF
ITS
CURRENT
SIGNAL
MASK
AND
THE
SIGNAL
SET
POINTED
TO
BY
SET
THAT
IS
SET
CONTAINS
THE
ADDITIONAL
SIGNALS
THAT
WE
WANT
TO
BLOCK
THE
NEW
SIGNAL
MASK
FOR
THE
PROCESS
IS
THE
INTERSECTION
OF
ITS
CURRENT
SIGNAL
MASK
AND
THE
COMPLEMENT
OF
THE
SIGNAL
SET
POINTED
TO
BY
SET
THAT
IS
SET
CONTAINS
THE
SIGNALS
THAT
WE
WANT
TO
UNBLOCK
THE
NEW
SIGNAL
MASK
FOR
THE
PROCESS
IS
REPLACED
BY
THE
VALUE
OF
THE
SIGNAL
SET
POINTED
TO
BY
SET
FIGURE
WAYS
TO
CHANGE
THE
CURRENT
SIGNAL
MASK
USING
SIGPROCMASK
IF
SET
IS
A
NULL
POINTER
THE
SIGNAL
MASK
OF
THE
PROCESS
IS
NOT
CHANGED
AND
HOW
IS
IGNORED
AFTER
CALLING
SIGPROCMASK
IF
ANY
UNBLOCKED
SIGNALS
ARE
PENDING
AT
LEAST
ONE
OF
THESE
SIGNALS
IS
DELIVERED
TO
THE
PROCESS
BEFORE
SIGPROCMASK
RETURNS
THE
SIGPROCMASK
FUNCTION
IS
DEFINED
ONLY
FOR
SINGLE
THREADED
PROCESSES
A
SEPARATE
FUNCTION
IS
PROVIDED
TO
MANIPULATE
A
THREAD
SIGNAL
MASK
IN
A
MULTITHREADED
PROCESS
WE
LL
DISCUSS
THIS
IN
SECTION
EXAMPLE
FIGURE
SHOWS
A
FUNCTION
THAT
PRINTS
THE
NAMES
OF
THE
SIGNALS
IN
THE
SIGNAL
MASK
OF
THE
CALLING
PROCESS
WE
CALL
THIS
FUNCTION
FROM
THE
PROGRAMS
SHOWN
IN
FIGURE
AND
FIGURE
INCLUDE
APUE
H
INCLUDE
ERRNO
H
VOID
CONST
CHAR
STR
SIGSET
INT
ERRNO
WE
CAN
BE
CALLED
BY
SIGNAL
HANDLERS
IF
SIGPROCMASK
NULL
SIGSET
SIGPROCMASK
ERROR
ELSE
PRINTF
STR
IF
SIGISMEMBER
SIGSET
SIGINT
PRINTF
SIGINT
IF
SIGISMEMBER
SIGSET
SIGQUIT
PRINTF
SIGQUIT
IF
SIGISMEMBER
SIGSET
PRINTF
IF
SIGISMEMBER
SIGSET
SIGALRM
PRINTF
SIGALRM
REMAINING
SIGNALS
CAN
GO
HERE
PRINTF
N
ERRNO
RESTORE
ERRNO
FIGURE
PRINT
THE
SIGNAL
MASK
FOR
THE
PROCESS
TO
SAVE
SPACE
WE
DON
T
TEST
THE
SIGNAL
MASK
FOR
EVERY
SIGNAL
THAT
WE
LISTED
IN
FIGURE
SEE
EXERCISE
SIGPENDING
FUNCTION
THE
SIGPENDING
FUNCTION
RETURNS
THE
SET
OF
SIGNALS
THAT
ARE
BLOCKED
FROM
DELIVERY
AND
CURRENTLY
PENDING
FOR
THE
CALLING
PROCESS
THE
SET
OF
SIGNALS
IS
RETURNED
THROUGH
THE
SET
ARGUMENT
EXAMPLE
FIGURE
SHOWS
MANY
OF
THE
SIGNAL
FEATURES
THAT
WE
VE
BEEN
DESCRIBING
INCLUDE
APUE
H
STATIC
VOID
INT
INT
MAIN
VOID
NEWMASK
OLDMASK
PENDMASK
IF
SIGNAL
SIGQUIT
CAN
T
CATCH
SIGQUIT
BLOCK
SIGQUIT
AND
SAVE
CURRENT
SIGNAL
MASK
SIGEMPTYSET
NEWMASK
SIGADDSET
NEWMASK
SIGQUIT
IF
SIGPROCMASK
NEWMASK
OLDMASK
ERROR
SLEEP
SIGQUIT
HERE
WILL
REMAIN
PENDING
IF
SIGPENDING
PENDMASK
SIGPENDING
ERROR
IF
SIGISMEMBER
PENDMASK
SIGQUIT
PRINTF
NSIGQUIT
PENDING
N
RESTORE
SIGNAL
MASK
WHICH
UNBLOCKS
SIGQUIT
IF
SIGPROCMASK
OLDMASK
NULL
ERROR
PRINTF
SIGQUIT
UNBLOCKED
N
SLEEP
SIGQUIT
HERE
WILL
TERMINATE
WITH
CORE
FILE
EXIT
STATIC
VOID
INT
SIGNO
PRINTF
CAUGHT
SIGQUIT
N
IF
SIGNAL
SIGQUIT
CAN
T
RESET
SIGQUIT
FIGURE
EXAMPLE
OF
SIGNAL
SETS
AND
SIGPROCMASK
THE
PROCESS
BLOCKS
SIGQUIT
SAVING
ITS
CURRENT
SIGNAL
MASK
TO
RESTORE
LATER
AND
THEN
GOES
TO
SLEEP
FOR
SECONDS
ANY
OCCURRENCE
OF
THE
QUIT
SIGNAL
DURING
THIS
PERIOD
IS
BLOCKED
AND
WON
T
BE
DELIVERED
UNTIL
THE
SIGNAL
IS
UNBLOCKED
AT
THE
END
OF
THE
SECOND
SLEEP
WE
CHECK
WHETHER
THE
SIGNAL
IS
PENDING
AND
UNBLOCK
THE
SIGNAL
NOTE
THAT
WE
SAVED
THE
OLD
MASK
WHEN
WE
BLOCKED
THE
SIGNAL
TO
UNBLOCK
THE
SIGNAL
WE
DID
A
OF
THE
OLD
MASK
ALTERNATIVELY
WE
COULD
ONLY
THE
SIGNAL
THAT
WE
HAD
BLOCKED
BE
AWARE
HOWEVER
IF
WE
WRITE
A
FUNCTION
THAT
CAN
BE
CALLED
BY
OTHERS
AND
IF
WE
NEED
TO
BLOCK
A
SIGNAL
IN
OUR
FUNCTION
WE
CAN
T
USE
TO
UNBLOCK
THE
SIGNAL
IN
THIS
CASE
WE
HAVE
TO
USE
AND
RESTORE
THE
SIGNAL
MASK
TO
ITS
PRIOR
VALUE
BECAUSE
IT
POSSIBLE
THAT
THE
CALLER
HAD
SPECIFICALLY
BLOCKED
THIS
SIGNAL
BEFORE
CALLING
OUR
FUNCTION
WE
LL
SEE
AN
EXAMPLE
OF
THIS
IN
THE
SYSTEM
FUNCTION
IN
SECTION
IF
WE
GENERATE
THE
QUIT
SIGNAL
DURING
THIS
SLEEP
PERIOD
THE
SIGNAL
IS
NOW
PENDING
AND
UNBLOCKED
SO
IT
IS
DELIVERED
BEFORE
SIGPROCMASK
RETURNS
WE
LL
SEE
THIS
OCCUR
BECAUSE
THE
PRINTF
IN
THE
SIGNAL
HANDLER
IS
OUTPUT
BEFORE
THE
PRINTF
THAT
FOLLOWS
THE
CALL
TO
SIGPROCMASK
THE
PROCESS
THEN
GOES
TO
SLEEP
FOR
ANOTHER
SECONDS
IF
WE
GENERATE
THE
QUIT
SIGNAL
DURING
THIS
SLEEP
PERIOD
THE
SIGNAL
SHOULD
TERMINATE
THE
PROCESS
SINCE
WE
RESET
THE
HANDLING
OF
THE
SIGNAL
TO
ITS
DEFAULT
WHEN
WE
CAUGHT
IT
IN
THE
FOLLOWING
OUTPUT
THE
TERMINAL
PRINTS
ˆ
WHEN
WE
INPUT
CONTROL
BACKSLASH
THE
TERMINAL
QUIT
CHARACTER
A
OUT
ˆ
GENERATE
SIGNAL
ONCE
SECONDS
ARE
UP
SIGQUIT
PENDING
AFTER
RETURN
FROM
SLEEP
CAUGHT
SIGQUIT
IN
SIGNAL
HANDLER
SIGQUIT
UNBLOCKED
AFTER
RETURN
FROM
SIGPROCMASK
ˆ
QUIT
COREDUMP
GENERATE
SIGNAL
AGAIN
A
OUT
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
GENERATE
SIGNAL
TIMES
SECONDS
ARE
UP
SIGQUIT
PENDING
CAUGHT
SIGQUIT
SIGNAL
IS
GENERATED
ONLY
ONCE
SIGQUIT
UNBLOCKED
ˆ
QUIT
COREDUMP
GENERATE
SIGNAL
AGAIN
THE
MESSAGE
QUIT
COREDUMP
IS
PRINTED
BY
THE
SHELL
WHEN
IT
SEES
THAT
ITS
CHILD
TERMINATED
ABNORMALLY
NOTE
THAT
WHEN
WE
RUN
THE
PROGRAM
THE
SECOND
TIME
WE
GENERATE
THE
QUIT
SIGNAL
TEN
TIMES
WHILE
THE
PROCESS
IS
ASLEEP
YET
THE
SIGNAL
IS
DELIVERED
ONLY
ONCE
TO
THE
PROCESS
WHEN
IT
UNBLOCKED
THIS
DEMONSTRATES
THAT
SIGNALS
ARE
NOT
QUEUED
ON
THIS
SYSTEM
SIGACTION
FUNCTION
THE
SIGACTION
FUNCTION
ALLOWS
US
TO
EXAMINE
OR
MODIFY
OR
BOTH
THE
ACTION
ASSOCIATED
WITH
A
PARTICULAR
SIGNAL
THIS
FUNCTION
SUPERSEDES
THE
SIGNAL
FUNCTION
FROM
EARLIER
RELEASES
OF
THE
UNIX
SYSTEM
INDEED
AT
THE
END
OF
THIS
SECTION
WE
SHOW
AN
IMPLEMENTATION
OF
SIGNAL
USING
SIGACTION
THE
ARGUMENT
SIGNO
IS
THE
SIGNAL
NUMBER
WHOSE
ACTION
WE
ARE
EXAMINING
OR
MODIFYING
IF
THE
ACT
POINTER
IS
NON
NULL
WE
ARE
MODIFYING
THE
ACTION
IF
THE
OACT
POINTER
IS
NON
NULL
THE
SYSTEM
RETURNS
THE
PREVIOUS
ACTION
FOR
THE
SIGNAL
THROUGH
THE
OACT
POINTER
THIS
FUNCTION
USES
THE
FOLLOWING
STRUCTURE
STRUCT
SIGACTION
VOID
INT
ADDR
OF
SIGNAL
HANDLER
OR
OR
ADDITIONAL
SIGNALS
TO
BLOCK
INT
SIGNAL
OPTIONS
FIGURE
ALTERNATE
HANDLER
VOID
INT
VOID
WHEN
CHANGING
THE
ACTION
FOR
A
SIGNAL
IF
THE
FIELD
CONTAINS
THE
ADDRESS
OF
A
SIGNAL
CATCHING
FUNCTION
AS
OPPOSED
TO
EITHER
OF
THE
CONSTANTS
OR
THEN
THE
FIELD
SPECIFIES
A
SET
OF
SIGNALS
THAT
ARE
ADDED
TO
THE
SIGNAL
MASK
OF
THE
PROCESS
BEFORE
THE
SIGNAL
CATCHING
FUNCTION
IS
CALLED
IF
AND
WHEN
THE
SIGNAL
CATCHING
FUNCTION
RETURNS
THE
SIGNAL
MASK
OF
THE
PROCESS
IS
RESET
TO
ITS
PREVIOUS
VALUE
THIS
WAY
WE
ARE
ABLE
TO
BLOCK
CERTAIN
SIGNALS
WHENEVER
A
SIGNAL
HANDLER
IS
INVOKED
THE
OPERATING
SYSTEM
INCLUDES
THE
SIGNAL
BEING
DELIVERED
IN
THE
SIGNAL
MASK
WHEN
THE
HANDLER
IS
INVOKED
HENCE
WE
ARE
GUARANTEED
THAT
WHENEVER
WE
ARE
PROCESSING
A
GIVEN
SIGNAL
ANOTHER
OCCURRENCE
OF
THAT
SAME
SIGNAL
IS
BLOCKED
UNTIL
WE
RE
FINISHED
PROCESSING
THE
FIRST
OCCURRENCE
RECALL
FROM
SECTION
THAT
ADDITIONAL
OCCURRENCES
OF
THE
SAME
SIGNAL
ARE
USUALLY
NOT
QUEUED
IF
THE
SIGNAL
OCCURS
FIVE
TIMES
WHILE
IT
IS
BLOCKED
WHEN
WE
UNBLOCK
THE
SIGNAL
THE
SIGNAL
HANDLING
FUNCTION
FOR
THAT
SIGNAL
WILL
USUALLY
BE
INVOKED
ONLY
ONE
TIME
THIS
CHARACTERISTIC
WAS
ILLUSTRATED
IN
THE
PREVIOUS
EXAMPLE
ONCE
WE
INSTALL
AN
ACTION
FOR
A
GIVEN
SIGNAL
THAT
ACTION
REMAINS
INSTALLED
UNTIL
WE
EXPLICITLY
CHANGE
IT
BY
CALLING
SIGACTION
UNLIKE
EARLIER
SYSTEMS
WITH
THEIR
UNRELIABLE
SIGNALS
POSIX
REQUIRES
THAT
A
SIGNAL
HANDLER
REMAIN
INSTALLED
UNTIL
EXPLICITLY
CHANGED
THE
FIELD
OF
THE
ACT
STRUCTURE
SPECIFIES
VARIOUS
OPTIONS
FOR
THE
HANDLING
OF
THIS
SIGNAL
FIGURE
DETAILS
THE
MEANING
OF
THESE
OPTIONS
WHEN
SET
THE
SUS
COLUMN
CONTAINS
IF
THE
FLAG
IS
DEFINED
AS
PART
OF
THE
BASE
POSIX
SPECIFICATION
AND
XSI
IF
IT
IS
DEFINED
AS
PART
OF
THE
XSI
OPTION
THE
FIELD
IS
AN
ALTERNATIVE
SIGNAL
HANDLER
USED
WHEN
THE
FLAG
IS
USED
WITH
SIGACTION
IMPLEMENTATIONS
MIGHT
USE
THE
SAME
STORAGE
FOR
BOTH
THE
FIELD
AND
THE
FIELD
SO
APPLICATIONS
CAN
USE
ONLY
ONE
OF
THESE
FIELDS
AT
A
TIME
OPTION
SUS
FREEBSD
LINUX
MAC
OS
X
SOLARIS
DESCRIPTION
SYSTEM
CALLS
INTERRUPTED
BY
THIS
SIGNAL
ARE
NOT
AUTOMATICALLY
RESTARTED
THE
XSI
DEFAULT
FOR
SIGACTION
SEE
SECTION
FOR
MORE
INFORMATION
IF
SIGNO
IS
SIGCHLD
DO
NOT
GENERATE
THIS
SIGNAL
WHEN
A
CHILD
PROCESS
STOPS
JOB
CONTROL
THIS
SIGNAL
IS
STILL
GENERATED
OF
COURSE
WHEN
A
CHILD
TERMINATES
BUT
SEE
THE
OPTION
BELOW
WHEN
THE
XSI
OPTION
IS
SUPPORTED
SIGCHLD
WON
T
BE
SENT
WHEN
A
STOPPED
CHILD
CONTINUES
IF
THIS
FLAG
IS
SET
IF
SIGNO
IS
SIGCHLD
THIS
OPTION
PREVENTS
THE
SYSTEM
FROM
CREATING
ZOMBIE
PROCESSES
WHEN
CHILDREN
OF
THE
CALLING
PROCESS
TERMINATE
IF
IT
SUBSEQUENTLY
CALLS
WAIT
THE
CALLING
PROCESS
BLOCKS
UNTIL
ALL
ITS
CHILD
PROCESSES
HAVE
TERMINATED
AND
THEN
RETURNS
WITH
ERRNO
SET
TO
ECHILD
RECALL
SECTION
WHEN
THIS
SIGNAL
IS
CAUGHT
THE
SIGNAL
IS
NOT
AUTOMATICALLY
BLOCKED
BY
THE
SYSTEM
WHILE
THE
SIGNAL
CATCHING
FUNCTION
EXECUTES
UNLESS
THE
SIGNAL
IS
ALSO
INCLUDED
IN
NOTE
THAT
THIS
TYPE
OF
OPERATION
CORRESPONDS
TO
THE
EARLIER
UNRELIABLE
SIGNALS
XSI
IF
AN
ALTERNATIVE
STACK
HAS
BEEN
DECLARED
WITH
SIGALTSTACK
THIS
SIGNAL
IS
DELIVERED
TO
THE
PROCESS
ON
THE
ALTERNATIVE
STACK
THE
DISPOSITION
FOR
THIS
SIGNAL
IS
RESET
TO
AND
THE
FLAG
IS
CLEARED
ON
ENTRY
TO
THE
SIGNAL
CATCHING
FUNCTION
NOTE
THAT
THIS
TYPE
OF
OPERATION
CORRESPONDS
TO
THE
EARLIER
UNRELIABLE
SIGNALS
THE
DISPOSITION
FOR
THE
TWO
SIGNALS
SIGILL
AND
SIGTRAP
CAN
T
BE
RESET
AUTOMATICALLY
HOWEVER
SETTING
THIS
FLAG
CAN
OPTIONALLY
CAUSE
SIGACTION
TO
BEHAVE
AS
IF
IS
ALSO
SET
SYSTEM
CALLS
INTERRUPTED
BY
THIS
SIGNAL
ARE
AUTOMATICALLY
RESTARTED
REFER
TO
SECTION
THIS
OPTION
PROVIDES
ADDITIONAL
INFORMATION
TO
A
SIGNAL
HANDLER
A
POINTER
TO
A
SIGINFO
STRUCTURE
AND
A
POINTER
TO
AN
IDENTIFIER
FOR
THE
PROCESS
CONTEXT
FIGURE
OPTION
FLAGS
FOR
THE
HANDLING
OF
EACH
SIGNAL
NORMALLY
THE
SIGNAL
HANDLER
IS
CALLED
AS
VOID
HANDLER
INT
SIGNO
BUT
IF
THE
FLAG
IS
SET
THE
SIGNAL
HANDLER
IS
CALLED
AS
VOID
HANDLER
INT
SIGNO
INFO
VOID
CONTEXT
THE
SIGINFO
STRUCTURE
CONTAINS
INFORMATION
ABOUT
WHY
THE
SIGNAL
WAS
GENERATED
AN
EXAMPLE
OF
WHAT
IT
MIGHT
LOOK
LIKE
IS
SHOWN
BELOW
ALL
POSIX
COMPLIANT
IMPLEMENTATIONS
MUST
INCLUDE
AT
LEAST
THE
AND
MEMBERS
ADDITIONALLY
IMPLEMENTATIONS
THAT
ARE
XSI
COMPLIANT
CONTAIN
AT
LEAST
THE
FOLLOWING
FIELDS
STRUCT
SIGINFO
INT
SIGNAL
NUMBER
INT
IF
NONZERO
ERRNO
VALUE
FROM
ERRNO
H
INT
ADDITIONAL
INFO
DEPENDS
ON
SIGNAL
SENDING
PROCESS
ID
SENDING
PROCESS
REAL
USER
ID
VOID
ADDRESS
THAT
CAUSED
THE
FAULT
INT
EXIT
VALUE
OR
SIGNAL
NUMBER
UNION
SIGVAL
APPLICATION
SPECIFIC
VALUE
POSSIBLY
OTHER
FIELDS
ALSO
THE
SIGVAL
UNION
CONTAINS
THE
FOLLOWING
FIELDS
INT
VOID
APPLICATIONS
PASS
AN
INTEGER
VALUE
IN
OR
PASS
A
POINTER
VALUE
IN
WHEN
DELIVERING
SIGNALS
FIGURE
SHOWS
VALUES
OF
FOR
VARIOUS
SIGNALS
AS
DEFINED
BY
THE
SINGLE
UNIX
SPECIFICATION
NOTE
THAT
IMPLEMENTATIONS
MAY
DEFINE
ADDITIONAL
CODE
VALUES
IF
THE
SIGNAL
IS
SIGCHLD
THEN
THE
AND
FIELDS
WILL
BE
SET
IF
THE
SIGNAL
IS
SIGBUS
SIGILL
SIGFPE
OR
SIGSEGV
THEN
THE
CONTAINS
THE
ADDRESS
RESPONSIBLE
FOR
THE
FAULT
ALTHOUGH
THE
ADDRESS
MIGHT
NOT
BE
ACCURATE
THE
FIELD
CONTAINS
THE
ERROR
NUMBER
CORRESPONDING
TO
THE
CONDITION
THAT
CAUSED
THE
SIGNAL
TO
BE
GENERATED
ALTHOUGH
ITS
USE
IS
IMPLEMENTATION
DEFINED
THE
CONTEXT
ARGUMENT
TO
THE
SIGNAL
HANDLER
IS
A
TYPELESS
POINTER
THAT
CAN
BE
CAST
TO
A
STRUCTURE
IDENTIFYING
THE
PROCESS
CONTEXT
AT
THE
TIME
OF
SIGNAL
DELIVERY
THIS
STRUCTURE
CONTAINS
AT
LEAST
THE
FOLLOWING
FIELDS
POINTER
TO
CONTEXT
RESUMED
WHEN
THIS
CONTEXT
RETURNS
SIGNALS
BLOCKED
WHEN
THIS
CONTEXT
IS
ACTIVE
STACK
USED
BY
THIS
CONTEXT
MACHINE
SPECIFIC
REPRESENTATION
OF
SAVED
CONTEXT
THE
FIELD
DESCRIBES
THE
STACK
USED
BY
THE
CURRENT
CONTEXT
IT
CONTAINS
AT
LEAST
THE
FOLLOWING
MEMBERS
VOID
STACK
BASE
OR
POINTER
STACK
SIZE
INT
FLAGS
WHEN
AN
IMPLEMENTATION
SUPPORTS
THE
REAL
TIME
SIGNAL
EXTENSIONS
SIGNAL
HANDLERS
ESTABLISHED
WITH
THE
FLAG
WILL
RESULT
IN
SIGNALS
BEING
QUEUED
RELIABLY
A
SEPARATE
RANGE
OF
RESERVED
SIGNAL
NUMBERS
IS
AVAILABLE
FOR
REAL
TIME
APPLICATION
USE
APPLICATIONS
CAN
PASS
INFORMATION
ALONG
WITH
THE
SIGNAL
BY
USING
THE
SIGQUEUE
FUNCTION
SECTION
SIGNAL
CODE
REASON
SIGILL
ILLEGAL
OPCODE
ILLEGAL
OPERAND
ILLEGAL
ADDRESSING
MODE
ILLEGAL
TRAP
PRIVILEGED
OPCODE
PRIVILEGED
REGISTER
COPROCESSOR
ERROR
INTERNAL
STACK
ERROR
SIGFPE
INTEGER
DIVIDE
BY
ZERO
INTEGER
OVERFLOW
FLOATING
POINT
DIVIDE
BY
ZERO
FLOATING
POINT
OVERFLOW
FLOATING
POINT
UNDERFLOW
FLOATING
POINT
INEXACT
RESULT
INVALID
FLOATING
POINT
OPERATION
SUBSCRIPT
OUT
OF
RANGE
SIGSEGV
ADDRESS
NOT
MAPPED
TO
OBJECT
INVALID
PERMISSIONS
FOR
MAPPED
OBJECT
SIGBUS
INVALID
ADDRESS
ALIGNMENT
NONEXISTENT
PHYSICAL
ADDRESS
OBJECT
SPECIFIC
HARDWARE
ERROR
SIGTRAP
PROCESS
BREAKPOINT
TRAP
PROCESS
TRACE
TRAP
SIGCHLD
CHILD
HAS
EXITED
CHILD
HAS
TERMINATED
ABNORMALLY
NO
CORE
CHILD
HAS
TERMINATED
ABNORMALLY
WITH
CORE
TRACED
CHILD
HAS
TRAPPED
CHILD
HAS
STOPPED
STOPPED
CHILD
HAS
CONTINUED
ANY
SIGNAL
SENT
BY
KILL
SIGNAL
SENT
BY
SIGQUEUE
EXPIRATION
OF
A
TIMER
SET
BY
COMPLETION
OF
ASYNCHRONOUS
I
O
REQUEST
ARRIVAL
OF
A
MESSAGE
ON
A
MESSAGE
QUEUE
REAL
TIME
EXTENSION
FIGURE
CODE
VALUES
EXAMPLE
SIGNAL
FUNCTION
LET
NOW
IMPLEMENT
THE
SIGNAL
FUNCTION
USING
SIGACTION
THIS
IS
WHAT
MANY
PLATFORMS
DO
AND
WHAT
A
NOTE
IN
THE
POSIX
RATIONALE
STATES
WAS
THE
INTENT
OF
POSIX
SYSTEMS
WITH
BINARY
COMPATIBILITY
CONSTRAINTS
ON
THE
OTHER
HAND
MIGHT
PROVIDE
A
SIGNAL
FUNCTION
THAT
SUPPORTS
THE
OLDER
UNRELIABLE
SIGNAL
SEMANTICS
UNLESS
YOU
SPECIFICALLY
REQUIRE
THESE
OLDER
UNRELIABLE
SEMANTICS
FOR
BACKWARD
COMPATIBILITY
YOU
SHOULD
USE
THE
FOLLOWING
IMPLEMENTATION
OF
SIGNAL
OR
CALL
SIGACTION
DIRECTLY
AS
YOU
MIGHT
GUESS
AN
IMPLEMENTATION
OF
SIGNAL
WITH
THE
OLD
SEMANTICS
COULD
CALL
SIGACTION
SPECIFYING
AND
ALL
THE
EXAMPLES
IN
THIS
TEXT
THAT
CALL
SIGNAL
CALL
THE
FUNCTION
SHOWN
IN
FIGURE
INCLUDE
APUE
H
RELIABLE
VERSION
OF
SIGNAL
USING
POSIX
SIGACTION
SIGFUNC
SIGNAL
INT
SIGNO
SIGFUNC
FUNC
STRUCT
SIGACTION
ACT
OACT
ACT
FUNC
SIGEMPTYSET
ACT
ACT
IF
SIGNO
SIGALRM
IFDEF
ACT
ENDIF
ELSE
ACT
IF
SIGACTION
SIGNO
ACT
OACT
RETURN
RETURN
OACT
FIGURE
AN
IMPLEMENTATION
OF
SIGNAL
USING
SIGACTION
NOTE
THAT
WE
MUST
USE
SIGEMPTYSET
TO
INITIALIZE
THE
MEMBER
OF
THE
STRUCTURE
WE
RE
NOT
GUARANTEED
THAT
ACT
DOES
THE
SAME
THING
WE
INTENTIONALLY
SET
THE
FLAG
FOR
ALL
SIGNALS
OTHER
THAN
SIGALRM
SO
THAT
ANY
SYSTEM
CALL
INTERRUPTED
BY
THESE
OTHER
SIGNALS
WILL
BE
AUTOMATICALLY
RESTARTED
THE
REASON
WE
DON
T
WANT
SIGALRM
RESTARTED
IS
TO
ALLOW
US
TO
SET
A
TIMEOUT
FOR
I
O
OPERATIONS
RECALL
THE
DISCUSSION
OF
FIGURE
SOME
OLDER
SYSTEMS
SUCH
AS
SUNOS
DEFINE
THE
FLAG
THESE
SYSTEMS
RESTART
INTERRUPTED
SYSTEM
CALLS
BY
DEFAULT
SO
SPECIFYING
THIS
FLAG
CAUSES
SYSTEM
CALLS
TO
BE
INTERRUPTED
LINUX
DEFINES
THE
FLAG
FOR
COMPATIBILITY
WITH
APPLICATIONS
THAT
USE
IT
BUT
BY
DEFAULT
DOES
NOT
RESTART
SYSTEM
CALLS
WHEN
THE
SIGNAL
HANDLER
IS
INSTALLED
WITH
SIGACTION
THE
SINGLE
UNIX
SPECIFICATION
SPECIFIES
THAT
THE
SIGACTION
FUNCTION
NOT
RESTART
INTERRUPTED
SYSTEM
CALLS
UNLESS
THE
FLAG
IS
SPECIFIED
EXAMPLE
FUNCTION
FIGURE
SHOWS
A
VERSION
OF
THE
SIGNAL
FUNCTION
THAT
TRIES
TO
PREVENT
ANY
INTERRUPTED
SYSTEM
CALLS
FROM
BEING
RESTARTED
INCLUDE
APUE
H
SIGFUNC
INT
SIGNO
SIGFUNC
FUNC
STRUCT
SIGACTION
ACT
OACT
ACT
FUNC
SIGEMPTYSET
ACT
ACT
IFDEF
ACT
ENDIF
IF
SIGACTION
SIGNO
ACT
OACT
RETURN
RETURN
OACT
FIGURE
THE
FUNCTION
FOR
IMPROVED
PORTABILITY
WE
SPECIFY
THE
FLAG
IF
DEFINED
BY
THE
SYSTEM
TO
PREVENT
INTERRUPTED
SYSTEM
CALLS
FROM
BEING
RESTARTED
SIGSETJMP
AND
SIGLONGJMP
FUNCTIONS
IN
SECTION
WE
DESCRIBED
THE
SETJMP
AND
LONGJMP
FUNCTIONS
WHICH
CAN
BE
USED
FOR
NONLOCAL
BRANCHING
THE
LONGJMP
FUNCTION
IS
OFTEN
CALLED
FROM
A
SIGNAL
HANDLER
TO
RETURN
TO
THE
MAIN
LOOP
OF
A
PROGRAM
INSTEAD
OF
RETURNING
FROM
THE
HANDLER
WE
SAW
THIS
APPROACH
IN
FIGURES
AND
THERE
IS
A
PROBLEM
IN
CALLING
LONGJMP
HOWEVER
WHEN
A
SIGNAL
IS
CAUGHT
THE
SIGNAL
CATCHING
FUNCTION
IS
ENTERED
WITH
THE
CURRENT
SIGNAL
AUTOMATICALLY
BEING
ADDED
TO
THE
SIGNAL
MASK
OF
THE
PROCESS
THIS
PREVENTS
SUBSEQUENT
OCCURRENCES
OF
THAT
SIGNAL
FROM
INTERRUPTING
THE
SIGNAL
HANDLER
IF
WE
LONGJMP
OUT
OF
THE
SIGNAL
HANDLER
WHAT
HAPPENS
TO
THE
SIGNAL
MASK
FOR
THE
PROCESS
UNDER
FREEBSD
AND
MAC
OS
X
SETJMP
AND
LONGJMP
SAVE
AND
RESTORE
THE
SIGNAL
MASK
LINUX
AND
SOLARIS
HOWEVER
DO
NOT
DO
THIS
ALTHOUGH
LINUX
SUPPORTS
AN
OPTION
TO
PROVIDE
BSD
BEHAVIOR
FREEBSD
AND
MAC
OS
X
PROVIDE
THE
FUNCTIONS
AND
WHICH
DO
NOT
SAVE
AND
RESTORE
THE
SIGNAL
MASK
TO
ALLOW
EITHER
FORM
OF
BEHAVIOR
POSIX
DOES
NOT
SPECIFY
THE
EFFECT
OF
SETJMP
AND
LONGJMP
ON
SIGNAL
MASKS
INSTEAD
TWO
NEW
FUNCTIONS
SIGSETJMP
AND
SIGLONGJMP
ARE
DEFINED
BY
POSIX
THESE
TWO
FUNCTIONS
SHOULD
ALWAYS
BE
USED
WHEN
BRANCHING
FROM
A
SIGNAL
HANDLER
THE
ONLY
DIFFERENCE
BETWEEN
THESE
FUNCTIONS
AND
THE
SETJMP
AND
LONGJMP
FUNCTIONS
IS
THAT
SIGSETJMP
HAS
AN
ADDITIONAL
ARGUMENT
IF
SAVEMASK
IS
NONZERO
THEN
SIGSETJMP
ALSO
SAVES
THE
CURRENT
SIGNAL
MASK
OF
THE
PROCESS
IN
ENV
WHEN
SIGLONGJMP
IS
CALLED
IF
THE
ENV
ARGUMENT
WAS
SAVED
BY
A
CALL
TO
SIGSETJMP
WITH
A
NONZERO
SAVEMASK
THEN
SIGLONGJMP
RESTORES
THE
SAVED
SIGNAL
MASK
EXAMPLE
THE
PROGRAM
IN
FIGURE
DEMONSTRATES
HOW
THE
SIGNAL
MASK
THAT
IS
INSTALLED
BY
THE
SYSTEM
WHEN
A
SIGNAL
HANDLER
IS
INVOKED
AUTOMATICALLY
INCLUDES
THE
SIGNAL
BEING
CAUGHT
THIS
PROGRAM
ALSO
ILLUSTRATES
THE
USE
OF
THE
SIGSETJMP
AND
SIGLONGJMP
FUNCTIONS
INCLUDE
APUE
H
INCLUDE
SETJMP
H
INCLUDE
TIME
H
STATIC
VOID
INT
STATIC
VOID
INT
STATIC
JMPBUF
STATIC
VOLATILE
CANJUMP
INT
MAIN
VOID
IF
SIGNAL
SIGNAL
ERROR
IF
SIGNAL
SIGALRM
SIGNAL
SIGALRM
ERROR
STARTING
MAIN
FIGURE
IF
SIGSETJMP
JMPBUF
ENDING
MAIN
EXIT
CANJUMP
NOW
SIGSETJMP
IS
OK
FOR
PAUSE
STATIC
VOID
INT
SIGNO
STARTTIME
IF
CANJUMP
RETURN
UNEXPECTED
SIGNAL
IGNORE
STARTING
ALARM
SIGALRM
IN
SECONDS
STARTTIME
TIME
NULL
FOR
BUSY
WAIT
FOR
SECONDS
IF
TIME
NULL
STARTTIME
BREAK
FINISHING
CANJUMP
SIGLONGJMP
JMPBUF
JUMP
BACK
TO
MAIN
DON
T
RETURN
STATIC
VOID
INT
SIGNO
IN
FIGURE
EXAMPLE
OF
SIGNAL
MASKS
SIGSETJMP
AND
SIGLONGJMP
THIS
PROGRAM
DEMONSTRATES
ANOTHER
TECHNIQUE
THAT
SHOULD
BE
USED
WHENEVER
SIGLONGJMP
IS
CALLED
FROM
A
SIGNAL
HANDLER
WE
SET
THE
VARIABLE
CANJUMP
TO
A
NONZERO
VALUE
ONLY
AFTER
WE
VE
CALLED
SIGSETJMP
THIS
VARIABLE
IS
EXAMINED
IN
THE
SIGNAL
HANDLER
AND
SIGLONGJMP
IS
CALLED
ONLY
IF
THE
FLAG
CANJUMP
IS
NONZERO
THIS
TECHNIQUE
PROVIDES
PROTECTION
AGAINST
THE
SIGNAL
HANDLER
BEING
CALLED
AT
SOME
EARLIER
OR
LATER
TIME
WHEN
THE
JUMP
BUFFER
HASN
T
BEEN
INITIALIZED
BY
SIGSETJMP
IN
THIS
TRIVIAL
PROGRAM
WE
TERMINATE
QUICKLY
AFTER
THE
SIGLONGJMP
BUT
IN
LARGER
PROGRAMS
THE
SIGNAL
HANDLER
MAY
REMAIN
INSTALLED
LONG
AFTER
THE
SIGLONGJMP
PROVIDING
THIS
TYPE
OF
PROTECTION
USUALLY
ISN
T
REQUIRED
WITH
LONGJMP
IN
NORMAL
C
CODE
AS
OPPOSED
TO
A
SIGNAL
HANDLER
SINCE
A
SIGNAL
CAN
OCCUR
AT
ANY
TIME
HOWEVER
WE
NEED
THE
ADDED
PROTECTION
IN
A
SIGNAL
HANDLER
HERE
WE
USE
THE
DATA
TYPE
WHICH
IS
DEFINED
BY
THE
ISO
C
STANDARD
TO
BE
THE
TYPE
OF
VARIABLE
THAT
CAN
BE
WRITTEN
WITHOUT
BEING
INTERRUPTED
BY
THIS
WE
MEAN
THAT
A
VARIABLE
OF
THIS
TYPE
SHOULD
NOT
EXTEND
ACROSS
PAGE
BOUNDARIES
ON
A
SYSTEM
WITH
VIRTUAL
MEMORY
AND
CAN
BE
ACCESSED
WITH
A
SINGLE
MACHINE
INSTRUCTION
FOR
EXAMPLE
WE
ALWAYS
INCLUDE
THE
ISO
TYPE
QUALIFIER
VOLATILE
FOR
THESE
DATA
TYPES
AS
WELL
SINCE
THE
VARIABLE
IS
BEING
ACCESSED
BY
TWO
DIFFERENT
THREADS
OF
CONTROL
THE
MAIN
FUNCTION
AND
THE
ASYNCHRONOUSLY
EXECUTING
SIGNAL
HANDLER
FIGURE
SHOWS
A
TIMELINE
FOR
THIS
PROGRAM
WE
CAN
DIVIDE
FIGURE
INTO
THREE
PARTS
THE
LEFT
PART
CORRESPONDING
TO
MAIN
THE
CENTER
PART
AND
THE
RIGHT
PART
WHILE
THE
PROCESS
IS
EXECUTING
IN
THE
LEFT
PART
ITS
SIGNAL
MASK
IS
NO
SIGNALS
ARE
BLOCKED
WHILE
EXECUTING
IN
THE
CENTER
PART
ITS
SIGNAL
MASK
IS
WHILE
EXECUTING
IN
THE
RIGHT
PART
ITS
SIGNAL
MASK
IS
SIGALRM
SIGNAL
SIGNAL
SIGSETJMP
PAUSE
DELIVERED
ALARM
TIME
TIME
TIME
SIGALRM
DELIVERED
RETURN
SIGSETJMP
EXIT
SIGLONGJMP
FIGURE
TIMELINE
FOR
EXAMPLE
PROGRAM
HANDLING
TWO
SIGNALS
LET
EXAMINE
THE
OUTPUT
WHEN
THE
PROGRAM
IN
FIGURE
IS
EXECUTED
A
OUT
START
PROCESS
IN
BACKGROUND
STARTING
MAIN
THE
JOB
CONTROL
SHELL
PRINTS
ITS
PROCESS
ID
KILL
SEND
THE
PROCESS
STARTING
IN
SIGALRM
FINISHING
ENDING
MAIN
JUST
PRESS
RETURN
DONE
A
OUT
THE
OUTPUT
IS
WHAT
WE
EXPECT
WHEN
A
SIGNAL
HANDLER
IS
INVOKED
THE
SIGNAL
BEING
CAUGHT
IS
ADDED
TO
THE
CURRENT
SIGNAL
MASK
OF
THE
PROCESS
THE
ORIGINAL
MASK
IS
RESTORED
WHEN
THE
SIGNAL
HANDLER
RETURNS
ALSO
SIGLONGJMP
RESTORES
THE
SIGNAL
MASK
THAT
WAS
SAVED
BY
SIGSETJMP
IF
WE
CHANGE
THE
PROGRAM
IN
FIGURE
SO
THAT
THE
CALLS
TO
SIGSETJMP
AND
SIGLONGJMP
ARE
REPLACED
WITH
CALLS
TO
SETJMP
AND
LONGJMP
ON
LINUX
OR
AND
ON
FREEBSD
THE
FINAL
LINE
OF
OUTPUT
BECOMES
ENDING
MAIN
THIS
MEANS
THAT
THE
MAIN
FUNCTION
IS
EXECUTING
WITH
THE
SIGNAL
BLOCKED
AFTER
THE
CALL
TO
SETJMP
THIS
PROBABLY
ISN
T
WHAT
WE
WANT
SIGSUSPEND
FUNCTION
WE
HAVE
SEEN
HOW
WE
CAN
CHANGE
THE
SIGNAL
MASK
FOR
A
PROCESS
TO
BLOCK
AND
UNBLOCK
SELECTED
SIGNALS
WE
CAN
USE
THIS
TECHNIQUE
TO
PROTECT
CRITICAL
REGIONS
OF
CODE
THAT
WE
DON
T
WANT
INTERRUPTED
BY
A
SIGNAL
BUT
WHAT
IF
WE
WANT
TO
UNBLOCK
A
SIGNAL
AND
THEN
PAUSE
WAITING
FOR
THE
PREVIOUSLY
BLOCKED
SIGNAL
TO
OCCUR
ASSUMING
THAT
THE
SIGNAL
IS
SIGINT
THE
INCORRECT
WAY
TO
DO
THIS
IS
NEWMASK
OLDMASK
SIGEMPTYSET
NEWMASK
SIGADDSET
NEWMASK
SIGINT
BLOCK
SIGINT
AND
SAVE
CURRENT
SIGNAL
MASK
IF
SIGPROCMASK
NEWMASK
OLDMASK
ERROR
CRITICAL
REGION
OF
CODE
RESTORE
SIGNAL
MASK
WHICH
UNBLOCKS
SIGINT
IF
SIGPROCMASK
OLDMASK
NULL
ERROR
WINDOW
IS
OPEN
PAUSE
WAIT
FOR
SIGNAL
TO
OCCUR
CONTINUE
PROCESSING
IF
THE
SIGNAL
IS
SENT
TO
THE
PROCESS
WHILE
IT
IS
BLOCKED
THE
SIGNAL
DELIVERY
WILL
BE
DEFERRED
UNTIL
THE
SIGNAL
IS
UNBLOCKED
TO
THE
APPLICATION
THIS
CAN
LOOK
AS
IF
THE
SIGNAL
OCCURS
BETWEEN
THE
UNBLOCKING
AND
THE
PAUSE
DEPENDING
ON
HOW
THE
KERNEL
IMPLEMENTS
SIGNALS
IF
THIS
HAPPENS
OR
IF
THE
SIGNAL
DOES
OCCUR
BETWEEN
THE
UNBLOCKING
AND
THE
PAUSE
WE
HAVE
A
PROBLEM
ANY
OCCURRENCE
OF
THE
SIGNAL
IN
THIS
WINDOW
OF
TIME
IS
LOST
IN
THE
SENSE
THAT
WE
MIGHT
NOT
SEE
THE
SIGNAL
AGAIN
IN
WHICH
CASE
THE
PAUSE
WILL
BLOCK
INDEFINITELY
THIS
IS
ANOTHER
PROBLEM
WITH
THE
EARLIER
UNRELIABLE
SIGNALS
TO
CORRECT
THIS
PROBLEM
WE
NEED
A
WAY
TO
BOTH
RESTORE
THE
SIGNAL
MASK
AND
PUT
THE
PROCESS
TO
SLEEP
IN
A
SINGLE
ATOMIC
OPERATION
THIS
FEATURE
IS
PROVIDED
BY
THE
SIGSUSPEND
FUNCTION
THE
SIGNAL
MASK
OF
THE
PROCESS
IS
SET
TO
THE
VALUE
POINTED
TO
BY
SIGMASK
THEN
THE
PROCESS
IS
SUSPENDED
UNTIL
A
SIGNAL
IS
CAUGHT
OR
UNTIL
A
SIGNAL
OCCURS
THAT
TERMINATES
THE
PROCESS
IF
A
SIGNAL
IS
CAUGHT
AND
IF
THE
SIGNAL
HANDLER
RETURNS
THEN
SIGSUSPEND
RETURNS
AND
THE
SIGNAL
MASK
OF
THE
PROCESS
IS
SET
TO
ITS
VALUE
BEFORE
THE
CALL
TO
SIGSUSPEND
NOTE
THAT
THERE
IS
NO
SUCCESSFUL
RETURN
FROM
THIS
FUNCTION
IF
IT
RETURNS
TO
THE
CALLER
IT
ALWAYS
RETURNS
WITH
ERRNO
SET
TO
EINTR
INDICATING
AN
INTERRUPTED
SYSTEM
CALL
EXAMPLE
FIGURE
SHOWS
THE
CORRECT
WAY
TO
PROTECT
A
CRITICAL
REGION
OF
CODE
FROM
A
SPECIFIC
SIGNAL
INCLUDE
APUE
H
STATIC
VOID
INT
INT
MAIN
VOID
NEWMASK
OLDMASK
WAITMASK
PROGRAM
START
IF
SIGNAL
SIGINT
SIGNAL
SIGINT
ERROR
SIGEMPTYSET
WAITMASK
SIGADDSET
WAITMASK
SIGEMPTYSET
NEWMASK
SIGADDSET
NEWMASK
SIGINT
BLOCK
SIGINT
AND
SAVE
CURRENT
SIGNAL
MASK
IF
SIGPROCMASK
NEWMASK
OLDMASK
ERROR
CRITICAL
REGION
OF
CODE
IN
CRITICAL
REGION
PAUSE
ALLOWING
ALL
SIGNALS
EXCEPT
IF
SIGSUSPEND
WAITMASK
SIGSUSPEND
ERROR
AFTER
RETURN
FROM
SIGSUSPEND
RESET
SIGNAL
MASK
WHICH
UNBLOCKS
SIGINT
IF
SIGPROCMASK
OLDMASK
NULL
ERROR
AND
CONTINUE
PROCESSING
PROGRAM
EXIT
EXIT
STATIC
VOID
INT
SIGNO
NIN
FIGURE
PROTECTING
A
CRITICAL
REGION
FROM
A
SIGNAL
WHEN
SIGSUSPEND
RETURNS
IT
SETS
THE
SIGNAL
MASK
TO
ITS
VALUE
BEFORE
THE
CALL
IN
THIS
EXAMPLE
THE
SIGINT
SIGNAL
WILL
BE
BLOCKED
SO
WE
RESTORE
THE
SIGNAL
MASK
TO
THE
VALUE
THAT
WE
SAVED
EARLIER
OLDMASK
RUNNING
THE
PROGRAM
FROM
FIGURE
PRODUCES
THE
FOLLOWING
OUTPUT
A
OUT
PROGRAM
START
IN
CRITICAL
REGION
SIGINT
ˆC
TYPE
THE
INTERRUPT
CHARACTER
IN
SIGINT
AFTER
RETURN
FROM
SIGSUSPEND
SIGINT
PROGRAM
EXIT
WE
ADDED
TO
THE
MASK
INSTALLED
WHEN
WE
CALLED
SIGSUSPEND
SO
THAT
WHEN
THE
SIGNAL
HANDLER
RAN
WE
COULD
TELL
THAT
THE
MASK
HAD
ACTUALLY
CHANGED
WE
CAN
SEE
THAT
WHEN
SIGSUSPEND
RETURNS
IT
RESTORES
THE
SIGNAL
MASK
TO
ITS
VALUE
BEFORE
THE
CALL
EXAMPLE
ANOTHER
USE
OF
SIGSUSPEND
IS
TO
WAIT
FOR
A
SIGNAL
HANDLER
TO
SET
A
GLOBAL
VARIABLE
IN
THE
PROGRAM
SHOWN
IN
FIGURE
WE
CATCH
BOTH
THE
INTERRUPT
SIGNAL
AND
THE
QUIT
SIGNAL
BUT
WANT
TO
WAKE
UP
THE
MAIN
ROUTINE
ONLY
WHEN
THE
QUIT
SIGNAL
IS
CAUGHT
INCLUDE
APUE
H
VOLATILE
QUITFLAG
SET
NONZERO
BY
SIGNAL
HANDLER
STATIC
VOID
INT
SIGNO
ONE
SIGNAL
HANDLER
FOR
SIGINT
AND
SIGQUIT
INT
IF
SIGNO
SIGINT
PRINTF
NINTERRUPT
N
ELSE
IF
SIGNO
SIGQUIT
QUITFLAG
SET
FLAG
FOR
MAIN
LOOP
MAIN
VOID
NEWMASK
OLDMASK
ZEROMASK
IF
SIGNAL
SIGINT
SIGNAL
SIGINT
ERROR
IF
SIGNAL
SIGQUIT
SIGNAL
SIGQUIT
ERROR
SIGEMPTYSET
ZEROMASK
SIGEMPTYSET
NEWMASK
SIGADDSET
NEWMASK
SIGQUIT
BLOCK
SIGQUIT
AND
SAVE
CURRENT
SIGNAL
MASK
IF
SIGPROCMASK
NEWMASK
OLDMASK
ERROR
WHILE
QUITFLAG
SIGSUSPEND
ZEROMASK
SIGQUIT
HAS
BEEN
CAUGHT
AND
IS
NOW
BLOCKED
DO
WHATEVER
QUITFLAG
RESET
SIGNAL
MASK
WHICH
UNBLOCKS
SIGQUIT
IF
SIGPROCMASK
OLDMASK
NULL
ERROR
EXIT
FIGURE
USING
SIGSUSPEND
TO
WAIT
FOR
A
GLOBAL
VARIABLE
TO
BE
SET
SAMPLE
OUTPUT
FROM
THIS
PROGRAM
IS
A
OUT
ˆC
TYPE
THE
INTERRUPT
CHARACTER
INTERRUPT
ˆC
TYPE
THE
INTERRUPT
CHARACTER
AGAIN
INTERRUPT
ˆC
AND
AGAIN
INTERRUPT
ˆ
NOW
TERMINATE
WITH
THE
QUIT
CHARACTER
FOR
PORTABILITY
BETWEEN
NON
POSIX
SYSTEMS
THAT
SUPPORT
ISO
C
AND
POSIX
SYSTEMS
THE
ONLY
THING
WE
SHOULD
DO
WITHIN
A
SIGNAL
HANDLER
IS
ASSIGN
A
VALUE
TO
A
VARIABLE
OF
TYPE
NOTHING
ELSE
POSIX
GOES
FURTHER
AND
SPECIFIES
A
LIST
OF
FUNCTIONS
THAT
ARE
SAFE
TO
CALL
FROM
WITHIN
A
SIGNAL
HANDLER
FIGURE
BUT
IF
WE
DO
THIS
OUR
CODE
MAY
NOT
RUN
CORRECTLY
ON
NON
POSIX
SYSTEMS
EXAMPLE
AS
ANOTHER
EXAMPLE
OF
SIGNALS
WE
SHOW
HOW
SIGNALS
CAN
BE
USED
TO
SYNCHRONIZE
A
PARENT
AND
CHILD
FIGURE
SHOWS
IMPLEMENTATIONS
OF
THE
FIVE
ROUTINES
AND
FROM
SECTION
INCLUDE
APUE
H
STATIC
VOLATILE
SIGFLAG
SET
NONZERO
BY
SIG
HANDLER
STATIC
NEWMASK
OLDMASK
ZEROMASK
STATIC
VOID
INT
SIGNO
ONE
SIGNAL
HANDLER
FOR
AND
SIGFLAG
VOID
VOID
IF
SIGNAL
SIGNAL
ERROR
IF
SIGNAL
SIGNAL
ERROR
SIGEMPTYSET
ZEROMASK
SIGEMPTYSET
NEWMASK
SIGADDSET
NEWMASK
SIGADDSET
NEWMASK
BLOCK
AND
AND
SAVE
CURRENT
SIGNAL
MASK
IF
SIGPROCMASK
NEWMASK
OLDMASK
ERROR
VOID
PID
KILL
PID
TELL
PARENT
WE
RE
DONE
VOID
VOID
WHILE
SIGFLAG
SIGSUSPEND
ZEROMASK
AND
WAIT
FOR
PARENT
SIGFLAG
RESET
SIGNAL
MASK
TO
ORIGINAL
VALUE
IF
SIGPROCMASK
OLDMASK
NULL
ERROR
VOID
PID
KILL
PID
TELL
CHILD
WE
RE
DONE
VOID
VOID
WHILE
SIGFLAG
SIGSUSPEND
ZEROMASK
AND
WAIT
FOR
CHILD
SIGFLAG
RESET
SIGNAL
MASK
TO
ORIGINAL
VALUE
IF
SIGPROCMASK
OLDMASK
NULL
ERROR
FIGURE
ROUTINES
TO
ALLOW
A
PARENT
AND
CHILD
TO
SYNCHRONIZE
WE
USE
THE
TWO
USER
DEFINED
SIGNALS
IS
SENT
BY
THE
PARENT
TO
THE
CHILD
AND
IS
SENT
BY
THE
CHILD
TO
THE
PARENT
IN
FIGURE
WE
SHOW
ANOTHER
IMPLEMENTATION
OF
THESE
FIVE
FUNCTIONS
USING
PIPES
THE
SIGSUSPEND
FUNCTION
IS
FINE
IF
WE
WANT
TO
GO
TO
SLEEP
WHILE
WE
RE
WAITING
FOR
A
SIGNAL
TO
OCCUR
AS
WE
VE
SHOWN
IN
THE
PREVIOUS
TWO
EXAMPLES
BUT
WHAT
IF
WE
WANT
TO
CALL
OTHER
SYSTEM
FUNCTIONS
WHILE
WE
RE
WAITING
UNFORTUNATELY
THIS
PROBLEM
HAS
NO
BULLETPROOF
SOLUTION
UNLESS
WE
USE
MULTIPLE
THREADS
AND
DEDICATE
A
SEPARATE
THREAD
TO
HANDLING
SIGNALS
AS
WE
DISCUSS
IN
SECTION
WITHOUT
USING
THREADS
THE
BEST
WE
CAN
DO
IS
TO
SET
A
GLOBAL
VARIABLE
IN
THE
SIGNAL
HANDLER
WHEN
THE
SIGNAL
OCCURS
FOR
EXAMPLE
IF
WE
CATCH
BOTH
SIGINT
AND
SIGALRM
AND
INSTALL
THE
SIGNAL
HANDLERS
USING
THE
FUNCTION
THE
SIGNALS
WILL
INTERRUPT
ANY
SLOW
SYSTEM
CALL
THAT
IS
BLOCKED
THE
SIGNALS
ARE
MOST
LIKELY
TO
OCCUR
WHEN
WE
RE
BLOCKED
IN
A
CALL
TO
THE
READ
FUNCTION
WAITING
FOR
INPUT
FROM
A
SLOW
DEVICE
THIS
IS
ESPECIALLY
TRUE
FOR
SIGALRM
SINCE
WE
SET
THE
ALARM
CLOCK
TO
PREVENT
US
FROM
WAITING
FOREVER
FOR
INPUT
THE
CODE
TO
HANDLE
THIS
LOOKS
SIMILAR
TO
THE
FOLLOWING
IF
FLAG
SET
BY
OUR
SIGINT
HANDLER
IF
FLAG
SET
BY
OUR
SIGALRM
HANDLER
SIGNALS
OCCURRING
IN
HERE
ARE
LOST
WHILE
READ
IF
ERRNO
EINTR
IF
ELSE
IF
ELSE
SOME
OTHER
ERROR
ELSE
IF
N
END
OF
FILE
ELSE
PROCESS
INPUT
WE
TEST
EACH
OF
THE
GLOBAL
FLAGS
BEFORE
CALLING
READ
AND
AGAIN
IF
READ
RETURNS
AN
INTERRUPTED
SYSTEM
CALL
ERROR
THE
PROBLEM
OCCURS
IF
EITHER
SIGNAL
IS
CAUGHT
BETWEEN
THE
FIRST
TWO
IF
STATEMENTS
AND
THE
SUBSEQUENT
CALL
TO
READ
SIGNALS
OCCURRING
IN
HERE
ARE
LOST
AS
INDICATED
BY
THE
CODE
COMMENT
THE
SIGNAL
HANDLERS
ARE
CALLED
AND
THEY
SET
THE
APPROPRIATE
GLOBAL
VARIABLE
BUT
THE
READ
NEVER
RETURNS
UNLESS
SOME
DATA
IS
READY
TO
BE
READ
WHAT
WE
WOULD
LIKE
TO
BE
ABLE
TO
DO
IS
THE
FOLLOWING
SEQUENCE
OF
STEPS
IN
ORDER
BLOCK
SIGINT
AND
SIGALRM
TEST
THE
TWO
GLOBAL
VARIABLES
TO
SEE
WHETHER
EITHER
SIGNAL
HAS
OCCURRED
AND
IF
SO
HANDLE
THE
CONDITION
CALL
READ
OR
ANY
OTHER
SYSTEM
FUNCTION
AND
UNBLOCK
THE
TWO
SIGNALS
AS
AN
ATOMIC
OPERATION
THE
SIGSUSPEND
FUNCTION
HELPS
US
ONLY
IF
STEP
IS
A
PAUSE
OPERATION
ABORT
FUNCTION
WE
MENTIONED
EARLIER
THAT
THE
ABORT
FUNCTION
CAUSES
ABNORMAL
PROGRAM
TERMINATION
INCLUDE
STDLIB
H
VOID
ABORT
VOID
THIS
FUNCTION
NEVER
RETURNS
THIS
FUNCTION
SENDS
THE
SIGABRT
SIGNAL
TO
THE
CALLER
PROCESSES
SHOULD
NOT
IGNORE
THIS
SIGNAL
ISO
C
STATES
THAT
CALLING
ABORT
WILL
DELIVER
AN
UNSUCCESSFUL
TERMINATION
NOTIFICATION
TO
THE
HOST
ENVIRONMENT
BY
CALLING
RAISE
SIGABRT
ISO
C
REQUIRES
THAT
IF
THE
SIGNAL
IS
CAUGHT
AND
THE
SIGNAL
HANDLER
RETURNS
ABORT
STILL
DOESN
T
RETURN
TO
ITS
CALLER
IF
THIS
SIGNAL
IS
CAUGHT
THE
ONLY
WAY
THE
SIGNAL
HANDLER
CAN
T
RETURN
IS
IF
IT
CALLS
EXIT
LONGJMP
OR
SIGLONGJMP
SECTION
DISCUSSES
THE
DIFFERENCES
BETWEEN
LONGJMP
AND
SIGLONGJMP
POSIX
ALSO
SPECIFIES
THAT
ABORT
OVERRIDES
THE
BLOCKING
OR
IGNORING
OF
THE
SIGNAL
BY
THE
PROCESS
THE
INTENT
OF
LETTING
THE
PROCESS
CATCH
THE
SIGABRT
IS
TO
ALLOW
IT
TO
PERFORM
ANY
CLEANUP
THAT
IT
WANTS
TO
DO
BEFORE
THE
PROCESS
TERMINATES
IF
THE
PROCESS
DOESN
T
TERMINATE
ITSELF
FROM
THIS
SIGNAL
HANDLER
POSIX
STATES
THAT
WHEN
THE
SIGNAL
HANDLER
RETURNS
ABORT
TERMINATES
THE
PROCESS
THE
ISO
C
SPECIFICATION
OF
THIS
FUNCTION
LEAVES
IT
UP
TO
THE
IMPLEMENTATION
AS
TO
WHETHER
OUTPUT
STREAMS
ARE
FLUSHED
AND
WHETHER
TEMPORARY
FILES
SECTION
ARE
DELETED
POSIX
GOES
FURTHER
AND
ALLOWS
AN
IMPLEMENTATION
TO
CALL
FCLOSE
ON
OPEN
STANDARD
I
O
STREAMS
BEFORE
TERMINATING
IF
THE
CALL
TO
ABORT
TERMINATES
THE
PROCESS
EARLIER
VERSIONS
OF
SYSTEM
V
GENERATED
THE
SIGIOT
SIGNAL
FROM
THE
ABORT
FUNCTION
FURTHERMORE
IT
WAS
POSSIBLE
FOR
A
PROCESS
TO
IGNORE
THIS
SIGNAL
OR
TO
CATCH
IT
AND
RETURN
FROM
THE
SIGNAL
HANDLER
IN
WHICH
CASE
ABORT
RETURNED
TO
ITS
CALLER
GENERATED
THE
SIGILL
SIGNAL
BEFORE
DOING
THIS
THE
FUNCTION
UNBLOCKED
THE
SIGNAL
AND
RESET
ITS
DISPOSITION
TO
TERMINATE
WITH
CORE
FILE
THIS
PREVENTED
A
PROCESS
FROM
EITHER
IGNORING
THE
SIGNAL
OR
CATCHING
IT
HISTORICALLY
IMPLEMENTATIONS
OF
ABORT
HAVE
DIFFERED
IN
HOW
THEY
DEAL
WITH
STANDARD
I
O
STREAMS
FOR
DEFENSIVE
PROGRAMMING
AND
IMPROVED
PORTABILITY
IF
WE
WANT
STANDARD
I
O
STREAMS
TO
BE
FLUSHED
WE
SPECIFICALLY
DO
IT
BEFORE
CALLING
ABORT
WE
DO
THIS
IN
THE
FUNCTION
APPENDIX
B
SINCE
MOST
UNIX
SYSTEM
IMPLEMENTATIONS
OF
TMPFILE
CALL
UNLINK
IMMEDIATELY
AFTER
CREATING
THE
FILE
THE
ISO
C
WARNING
ABOUT
TEMPORARY
FILES
DOES
NOT
USUALLY
CONCERN
US
EXAMPLE
FIGURE
SHOWS
AN
IMPLEMENTATION
OF
THE
ABORT
FUNCTION
AS
SPECIFIED
BY
POSIX
INCLUDE
SIGNAL
H
INCLUDE
STDIO
H
INCLUDE
STDLIB
H
INCLUDE
UNISTD
H
VOID
ABORT
VOID
POSIX
STYLE
ABORT
FUNCTION
MASK
STRUCT
SIGACTION
ACTION
CALLER
CAN
T
IGNORE
SIGABRT
IF
SO
RESET
TO
DEFAULT
SIGACTION
SIGABRT
NULL
ACTION
IF
ACTION
ACTION
SIGACTION
SIGABRT
ACTION
NULL
IF
ACTION
FFLUSH
NULL
FLUSH
ALL
OPEN
STDIO
STREAMS
CALLER
CAN
T
BLOCK
SIGABRT
MAKE
SURE
IT
UNBLOCKED
SIGFILLSET
MASK
SIGDELSET
MASK
SIGABRT
MASK
HAS
ONLY
SIGABRT
TURNED
OFF
SIGPROCMASK
MASK
NULL
KILL
GETPID
SIGABRT
SEND
THE
SIGNAL
IF
WE
RE
HERE
PROCESS
CAUGHT
SIGABRT
AND
RETURNED
FFLUSH
NULL
FLUSH
ALL
OPEN
STDIO
STREAMS
ACTION
SIGACTION
SIGABRT
ACTION
NULL
RESET
TO
DEFAULT
SIGPROCMASK
MASK
NULL
JUST
IN
CASE
KILL
GETPID
SIGABRT
AND
ONE
MORE
TIME
EXIT
THIS
SHOULD
NEVER
BE
EXECUTED
FIGURE
IMPLEMENTATION
OF
POSIX
ABORT
WE
FIRST
SEE
WHETHER
THE
DEFAULT
ACTION
WILL
OCCUR
IF
SO
WE
FLUSH
ALL
THE
STANDARD
I
O
STREAMS
THIS
IS
NOT
EQUIVALENT
TO
CALLING
FCLOSE
ON
ALL
THE
OPEN
STREAMS
SINCE
IT
JUST
FLUSHES
THEM
AND
DOESN
T
CLOSE
THEM
BUT
WHEN
THE
PROCESS
TERMINATES
THE
SYSTEM
CLOSES
ALL
OPEN
FILES
IF
THE
PROCESS
CATCHES
THE
SIGNAL
AND
RETURNS
WE
FLUSH
ALL
THE
STREAMS
AGAIN
SINCE
THE
PROCESS
COULD
HAVE
GENERATED
MORE
OUTPUT
THE
ONLY
CONDITION
WE
DON
T
HANDLE
IS
THE
CASE
WHERE
THE
PROCESS
CATCHES
THE
SIGNAL
AND
CALLS
OR
IN
THIS
CASE
ANY
UNFLUSHED
STANDARD
I
O
BUFFERS
IN
MEMORY
ARE
DISCARDED
WE
ASSUME
THAT
A
CALLER
THAT
DOES
THIS
DOESN
T
WANT
THE
BUFFERS
FLUSHED
RECALL
FROM
SECTION
THAT
IF
CALLING
KILL
CAUSES
THE
SIGNAL
TO
BE
GENERATED
FOR
THE
CALLER
AND
IF
THE
SIGNAL
IS
NOT
BLOCKED
WHICH
WE
GUARANTEE
IN
FIGURE
THEN
THE
SIGNAL
OR
SOME
OTHER
PENDING
UNLOCKED
SIGNAL
IS
DELIVERED
TO
THE
PROCESS
BEFORE
KILL
RETURNS
WE
BLOCK
ALL
SIGNALS
EXCEPT
SIGABRT
SO
WE
KNOW
THAT
IF
THE
CALL
TO
KILL
RETURNS
THE
PROCESS
CAUGHT
THE
SIGNAL
AND
THE
SIGNAL
HANDLER
RETURNED
SYSTEM
FUNCTION
IN
SECTION
WE
SHOWED
AN
IMPLEMENTATION
OF
THE
SYSTEM
FUNCTION
THAT
VERSION
HOWEVER
DID
NOT
DO
ANY
SIGNAL
HANDLING
POSIX
REQUIRES
THAT
SYSTEM
IGNORE
SIGINT
AND
SIGQUIT
AND
BLOCK
SIGCHLD
BEFORE
SHOWING
A
VERSION
THAT
HANDLES
THESE
SIGNALS
CORRECTLY
LET
SEE
WHY
WE
NEED
TO
WORRY
ABOUT
SIGNAL
HANDLING
EXAMPLE
THE
PROGRAM
SHOWN
IN
FIGURE
USES
THE
VERSION
OF
SYSTEM
FROM
SECTION
TO
INVOKE
THE
ED
EDITOR
THIS
EDITOR
HAS
BEEN
PART
OF
UNIX
SYSTEMS
FOR
A
LONG
TIME
WE
USE
IT
HERE
BECAUSE
IT
IS
AN
INTERACTIVE
PROGRAM
THAT
CATCHES
THE
INTERRUPT
AND
QUIT
SIGNALS
IF
WE
INVOKE
ED
FROM
A
SHELL
AND
TYPE
THE
INTERRUPT
CHARACTER
IT
CATCHES
THE
INTERRUPT
SIGNAL
AND
PRINTS
A
QUESTION
MARK
THE
ED
PROGRAM
ALSO
SETS
THE
DISPOSITION
OF
THE
QUIT
SIGNAL
SO
THAT
IT
IS
IGNORED
THE
PROGRAM
IN
FIGURE
CATCHES
BOTH
SIGINT
AND
SIGCHLD
IF
WE
INVOKE
THE
PROGRAM
WE
GET
A
OUT
A
APPEND
TEXT
TO
THE
EDITOR
BUFFER
HERE
IS
ONE
LINE
OF
TEXT
PERIOD
ON
A
LINE
BY
ITSELF
STOPS
APPEND
MODE
P
PRINT
FIRST
THROUGH
LAST
LINES
OF
BUFFER
TO
SEE
WHAT
THERE
HERE
IS
ONE
LINE
OF
TEXT
W
TEMP
FOO
WRITE
THE
BUFFER
TO
A
FILE
EDITOR
SAYS
IT
WROTE
BYTES
Q
AND
LEAVE
THE
EDITOR
CAUGHT
SIGCHLD
WHEN
THE
EDITOR
TERMINATES
THE
SYSTEM
SENDS
THE
SIGCHLD
SIGNAL
TO
THE
PARENT
THE
A
OUT
PROCESS
WE
CATCH
IT
AND
RETURN
FROM
THE
SIGNAL
HANDLER
BUT
IF
IT
IS
CATCHING
THE
SIGCHLD
SIGNAL
THE
PARENT
SHOULD
BE
DOING
SO
BECAUSE
IT
HAS
CREATED
ITS
OWN
CHILDREN
SO
THAT
IT
KNOWS
WHEN
ITS
CHILDREN
HAVE
TERMINATED
THE
DELIVERY
OF
THIS
SIGNAL
IN
THE
INCLUDE
APUE
H
STATIC
VOID
INT
SIGNO
PRINTF
CAUGHT
SIGINT
N
STATIC
VOID
INT
SIGNO
PRINTF
CAUGHT
SIGCHLD
N
INT
MAIN
VOID
IF
SIGNAL
SIGINT
SIGNAL
SIGINT
ERROR
IF
SIGNAL
SIGCHLD
SIGNAL
SIGCHLD
ERROR
IF
SYSTEM
BIN
ED
SYSTEM
ERROR
EXIT
FIGURE
USING
SYSTEM
TO
INVOKE
THE
ED
EDITOR
PARENT
SHOULD
BE
BLOCKED
WHILE
THE
SYSTEM
FUNCTION
IS
EXECUTING
INDEED
THIS
IS
WHAT
POSIX
SPECIFIES
OTHERWISE
WHEN
THE
CHILD
CREATED
BY
SYSTEM
TERMINATES
IT
WOULD
FOOL
THE
CALLER
OF
SYSTEM
INTO
THINKING
THAT
ONE
OF
ITS
OWN
CHILDREN
TERMINATED
THE
CALLER
WOULD
THEN
USE
ONE
OF
THE
WAIT
FUNCTIONS
TO
GET
THE
TERMINATION
STATUS
OF
THE
CHILD
THEREBY
PREVENTING
THE
SYSTEM
FUNCTION
FROM
BEING
ABLE
TO
OBTAIN
THE
CHILD
TERMINATION
STATUS
FOR
ITS
RETURN
VALUE
IF
WE
RUN
THE
PROGRAM
AGAIN
THIS
TIME
SENDING
THE
EDITOR
AN
INTERRUPT
SIGNAL
WE
GET
A
OUT
A
APPEND
TEXT
TO
THE
EDITOR
BUFFER
HELLO
WORLD
PERIOD
ON
A
LINE
BY
ITSELF
STOPS
APPEND
MODE
P
PRINT
FIRST
THROUGH
LAST
LINES
TO
SEE
WHAT
THERE
HELLO
WORLD
W
TEMP
FOO
WRITE
THE
BUFFER
TO
A
FILE
EDITOR
SAYS
IT
WROTE
BYTES
ˆC
TYPE
THE
INTERRUPT
CHARACTER
EDITOR
CATCHES
SIGNAL
PRINTS
QUESTION
MARK
CAUGHT
SIGINT
AND
SO
DOES
THE
PARENT
PROCESS
Q
LEAVE
EDITOR
CAUGHT
SIGCHLD
RECALL
FROM
SECTION
THAT
TYPING
THE
INTERRUPT
CHARACTER
CAUSES
THE
INTERRUPT
SIGNAL
TO
BE
SENT
TO
ALL
THE
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
FIGURE
SHOWS
THE
ARRANGEMENT
OF
THE
PROCESSES
WHEN
THE
EDITOR
IS
RUNNING
FORK
EXEC
FORK
EXEC
BACKGROUND
PROCESS
GROUP
FOREGROUND
PROCESS
GROUP
FIGURE
FOREGROUND
AND
BACKGROUND
PROCESS
GROUPS
FOR
FIGURE
IN
THIS
EXAMPLE
SIGINT
IS
SENT
TO
ALL
THREE
FOREGROUND
PROCESSES
THE
SHELL
IGNORES
IT
AS
WE
CAN
SEE
FROM
THE
OUTPUT
BOTH
THE
A
OUT
PROCESS
AND
THE
EDITOR
CATCH
THE
SIGNAL
BUT
WHEN
WE
RE
RUNNING
ANOTHER
PROGRAM
WITH
THE
SYSTEM
FUNCTION
WE
SHOULDN
T
HAVE
BOTH
THE
PARENT
AND
THE
CHILD
CATCHING
THE
TWO
TERMINAL
GENERATED
SIGNALS
INTERRUPT
AND
QUIT
INSTEAD
THESE
TWO
SIGNALS
SHOULD
BE
SENT
TO
THE
PROGRAM
THAT
IS
RUNNING
THE
CHILD
SINCE
THE
COMMAND
THAT
IS
EXECUTED
BY
SYSTEM
CAN
BE
AN
INTERACTIVE
COMMAND
AS
IS
THE
ED
PROGRAM
IN
THIS
EXAMPLE
AND
SINCE
THE
CALLER
OF
SYSTEM
GIVES
UP
CONTROL
WHILE
THE
PROGRAM
EXECUTES
WAITING
FOR
IT
TO
FINISH
THE
CALLER
OF
SYSTEM
SHOULD
NOT
BE
RECEIVING
THESE
TWO
TERMINAL
GENERATED
SIGNALS
FOR
THIS
REASON
POSIX
SPECIFIES
THAT
THE
SYSTEM
FUNCTION
SHOULD
IGNORE
THESE
TWO
SIGNALS
WHILE
WAITING
FOR
THE
COMMAND
TO
COMPLETE
EXAMPLE
FIGURE
SHOWS
AN
IMPLEMENTATION
OF
THE
SYSTEM
FUNCTION
WITH
THE
REQUIRED
SIGNAL
HANDLING
INCLUDE
SYS
WAIT
H
INCLUDE
ERRNO
H
INCLUDE
SIGNAL
H
INCLUDE
UNISTD
H
INT
SYSTEM
CONST
CHAR
CMDSTRING
WITH
APPROPRIATE
SIGNAL
HANDLING
PID
INT
STATUS
STRUCT
SIGACTION
IGNORE
SAVEINTR
SAVEQUIT
CHLDMASK
SAVEMASK
IF
CMDSTRING
NULL
RETURN
ALWAYS
A
COMMAND
PROCESSOR
WITH
UNIX
IGNORE
IGNORE
SIGINT
AND
SIGQUIT
SIGEMPTYSET
IGNORE
IGNORE
IF
SIGACTION
SIGINT
IGNORE
SAVEINTR
RETURN
IF
SIGACTION
SIGQUIT
IGNORE
SAVEQUIT
RETURN
SIGEMPTYSET
CHLDMASK
NOW
BLOCK
SIGCHLD
SIGADDSET
CHLDMASK
SIGCHLD
IF
SIGPROCMASK
CHLDMASK
SAVEMASK
RETURN
IF
PID
FORK
STATUS
PROBABLY
OUT
OF
PROCESSES
ELSE
IF
PID
CHILD
RESTORE
PREVIOUS
SIGNAL
ACTIONS
RESET
SIGNAL
MASK
SIGACTION
SIGINT
SAVEINTR
NULL
SIGACTION
SIGQUIT
SAVEQUIT
NULL
SIGPROCMASK
SAVEMASK
NULL
EXECL
BIN
SH
SH
C
CMDSTRING
CHAR
EXEC
ERROR
ELSE
PARENT
WHILE
WAITPID
PID
STATUS
IF
ERRNO
EINTR
STATUS
ERROR
OTHER
THAN
EINTR
FROM
WAITPID
BREAK
RESTORE
PREVIOUS
SIGNAL
ACTIONS
RESET
SIGNAL
MASK
IF
SIGACTION
SIGINT
SAVEINTR
NULL
RETURN
IF
SIGACTION
SIGQUIT
SAVEQUIT
NULL
RETURN
IF
SIGPROCMASK
SAVEMASK
NULL
RETURN
RETURN
STATUS
FIGURE
CORRECT
POSIX
IMPLEMENTATION
OF
SYSTEM
FUNCTION
IF
WE
LINK
THE
PROGRAM
IN
FIGURE
WITH
THIS
IMPLEMENTATION
OF
THE
SYSTEM
FUNCTION
THE
RESULTING
BINARY
DIFFERS
FROM
THE
LAST
FLAWED
ONE
IN
THE
FOLLOWING
WAYS
NO
SIGNAL
IS
SENT
TO
THE
CALLING
PROCESS
WHEN
WE
TYPE
THE
INTERRUPT
OR
QUIT
CHARACTER
WHEN
THE
ED
COMMAND
EXITS
SIGCHLD
IS
NOT
SENT
TO
THE
CALLING
PROCESS
INSTEAD
IT
IS
BLOCKED
UNTIL
WE
UNBLOCK
IT
IN
THE
LAST
CALL
TO
SIGPROCMASK
AFTER
THE
SYSTEM
FUNCTION
RETRIEVES
THE
CHILD
TERMINATION
STATUS
BY
CALLING
WAITPID
POSIX
STATES
THAT
IF
WAIT
OR
WAITPID
RETURNS
THE
STATUS
OF
A
CHILD
PROCESS
WHILE
SIGCHLD
IS
PENDING
THEN
SIGCHLD
SHOULD
NOT
BE
DELIVERED
TO
THE
PROCESS
UNLESS
THE
STATUS
OF
ANOTHER
CHILD
PROCESS
IS
ALSO
AVAILABLE
FREEBSD
MAC
OS
X
AND
SOLARIS
ALL
IMPLEMENT
THIS
SEMANTIC
LINUX
HOWEVER
DOESN
T
SIGCHLD
REMAINS
PENDING
AFTER
THE
SYSTEM
FUNCTION
CALLS
WAITPID
WHEN
THE
SIGNAL
IS
UNBLOCKED
IT
IS
DELIVERED
TO
THE
CALLER
IF
WE
CALLED
WAIT
IN
THE
FUNCTION
IN
FIGURE
A
LINUX
SYSTEM
WOULD
RETURN
WITH
ERRNO
SET
TO
ECHILD
SINCE
THE
SYSTEM
FUNCTION
ALREADY
RETRIEVED
THE
TERMINATION
STATUS
OF
THE
CHILD
MANY
OLDER
TEXTS
SHOW
THE
IGNORING
OF
THE
INTERRUPT
AND
QUIT
SIGNALS
AS
FOLLOWS
IF
PID
FORK
FORK
ERROR
ELSE
IF
PID
CHILD
EXECL
PARENT
SIGNAL
SIGINT
SIGNAL
SIGQUIT
WAITPID
PID
STATUS
SIGNAL
SIGINT
SIGNAL
SIGQUIT
THE
PROBLEM
WITH
THIS
SEQUENCE
OF
CODE
IS
THAT
WE
HAVE
NO
GUARANTEE
AFTER
THE
FORK
REGARDING
WHETHER
THE
PARENT
OR
CHILD
RUNS
FIRST
IF
THE
CHILD
RUNS
FIRST
AND
THE
PARENT
DOESN
T
RUN
FOR
SOME
TIME
AFTER
AN
INTERRUPT
SIGNAL
MIGHT
BE
GENERATED
BEFORE
THE
PARENT
IS
ABLE
TO
CHANGE
ITS
DISPOSITION
TO
BE
IGNORED
FOR
THIS
REASON
IN
FIGURE
WE
CHANGE
THE
DISPOSITION
OF
THE
SIGNALS
BEFORE
THE
FORK
NOTE
THAT
WE
HAVE
TO
RESET
THE
DISPOSITIONS
OF
THESE
TWO
SIGNALS
IN
THE
CHILD
BEFORE
THE
CALL
TO
EXECL
THIS
ALLOWS
EXECL
TO
CHANGE
THEIR
DISPOSITIONS
TO
THE
DEFAULT
BASED
ON
THE
CALLER
DISPOSITIONS
AS
WE
DESCRIBED
IN
SECTION
RETURN
VALUE
FROM
SYSTEM
THE
RETURN
VALUE
FROM
SYSTEM
IS
THE
TERMINATION
STATUS
OF
THE
SHELL
WHICH
ISN
T
ALWAYS
THE
TERMINATION
STATUS
OF
THE
COMMAND
STRING
WE
SAW
SOME
EXAMPLES
IN
FIGURE
AND
THE
RESULTS
WERE
AS
WE
EXPECTED
IF
WE
EXECUTE
A
SIMPLE
COMMAND
SUCH
AS
DATE
THE
TERMINATION
STATUS
IS
EXECUTING
THE
SHELL
COMMAND
EXIT
GAVE
US
A
TERMINATION
STATUS
OF
WHAT
HAPPENS
WITH
SIGNALS
LET
RUN
THE
PROGRAM
IN
FIGURE
AND
SEND
SOME
SIGNALS
TO
THE
COMMAND
THAT
EXECUTING
TSYS
SLEEP
ˆCNORMAL
TERMINATION
EXIT
STATUS
WE
PRESS
THE
INTERRUPT
KEY
TSYS
SLEEP
ˆ
SH
QUIT
WE
PRESS
THE
QUIT
KEY
NORMAL
TERMINATION
EXIT
STATUS
WHEN
WE
TERMINATE
THE
SLEEP
CALL
WITH
THE
INTERRUPT
SIGNAL
THE
FUNCTION
FIGURE
THINKS
THAT
IT
TERMINATED
NORMALLY
THE
SAME
THING
HAPPENS
WHEN
WE
KILL
THE
SLEEP
CALL
WITH
THE
QUIT
KEY
AS
THIS
EXAMPLE
DEMONSTRATES
THE
BOURNE
SHELL
HAS
A
POORLY
DOCUMENTED
FEATURE
IN
WHICH
ITS
TERMINATION
STATUS
IS
PLUS
THE
SIGNAL
NUMBER
WHEN
THE
COMMAND
IT
WAS
EXECUTING
IS
TERMINATED
BY
A
SIGNAL
WE
CAN
SEE
THIS
WITH
THE
SHELL
INTERACTIVELY
SH
MAKE
SURE
WE
RE
RUNNING
THE
BOURNE
SHELL
SH
C
SLEEP
ˆC
PRESS
THE
INTERRUPT
KEY
ECHO
PRINT
TERMINATION
STATUS
OF
LAST
COMMAND
SH
C
SLEEP
ˆ
SH
QUIT
CORE
DUMPED
PRESS
THE
QUIT
KEY
ECHO
PRINT
TERMINATION
STATUS
OF
LAST
COMMAND
EXIT
LEAVE
BOURNE
SHELL
ON
THE
SYSTEM
BEING
USED
SIGINT
HAS
A
VALUE
OF
AND
SIGQUIT
HAS
A
VALUE
OF
GIVING
US
THE
SHELL
TERMINATION
STATUSES
OF
AND
LET
TRY
A
SIMILAR
EXAMPLE
BUT
THIS
TIME
WE
LL
SEND
A
SIGNAL
DIRECTLY
TO
THE
SHELL
AND
SEE
WHAT
IS
RETURNED
BY
SYSTEM
TSYS
SLEEP
START
IT
IN
BACKGROUND
THIS
TIME
PS
F
LOOK
AT
THE
PROCESS
IDS
UID
PID
PPID
TTY
TIME
CMD
SAR
PTS
PS
F
SAR
PTS
SH
C
SLEEP
SAR
947
PTS
BIN
SH
SAR
PTS
TSYS
SLEEP
SAR
PTS
SLEEP
KILL
KILL
KILL
THE
SHELL
ITSELF
ABNORMAL
TERMINATION
SIGNAL
NUMBER
HERE
WE
CAN
SEE
THAT
THE
RETURN
VALUE
FROM
SYSTEM
REPORTS
AN
ABNORMAL
TERMINATION
ONLY
WHEN
THE
SHELL
ITSELF
TERMINATES
ABNORMALLY
OTHER
SHELLS
BEHAVE
DIFFERENTLY
WHEN
HANDLING
TERMINAL
GENERATED
SIGNALS
SUCH
AS
SIGINT
AND
SIGQUIT
WITH
BASH
AND
DASH
FOR
EXAMPLE
PRESSING
THE
INTERRUPT
OR
QUIT
KEY
WILL
RESULT
IN
AN
EXIT
STATUS
INDICATING
ABNORMAL
TERMINATION
WITH
THE
CORRESPONDING
SIGNAL
NUMBER
HOWEVER
IF
WE
FIND
OUR
PROCESS
EXECUTING
SLEEP
AND
SEND
IT
A
SIGNAL
DIRECTLY
SO
THAT
THE
SIGNAL
GOES
ONLY
TO
THE
INDIVIDUAL
PROCESS
INSTEAD
OF
THE
ENTIRE
FOREGROUND
PROCESS
GROUP
WE
WILL
FIND
THAT
THESE
SHELLS
BEHAVE
LIKE
THE
BOURNE
SHELL
AND
EXIT
WITH
A
NORMAL
TERMINATION
STATUS
OF
PLUS
THE
SIGNAL
NUMBER
WHEN
WRITING
PROGRAMS
THAT
USE
THE
SYSTEM
FUNCTION
BE
SURE
TO
INTERPRET
THE
RETURN
VALUE
CORRECTLY
IF
YOU
CALL
FORK
EXEC
AND
WAIT
YOURSELF
THE
TERMINATION
STATUS
IS
NOT
THE
SAME
AS
IF
YOU
CALL
SYSTEM
SLEEP
NANOSLEEP
AND
FUNCTIONS
WE
VE
USED
THE
SLEEP
FUNCTION
IN
NUMEROUS
EXAMPLES
THROUGHOUT
THE
TEXT
AND
WE
SHOWED
TWO
FLAWED
IMPLEMENTATIONS
OF
IT
IN
FIGURES
AND
THIS
FUNCTION
CAUSES
THE
CALLING
PROCESS
TO
BE
SUSPENDED
UNTIL
EITHER
THE
AMOUNT
OF
WALL
CLOCK
TIME
SPECIFIED
BY
SECONDS
HAS
ELAPSED
A
SIGNAL
IS
CAUGHT
BY
THE
PROCESS
AND
THE
SIGNAL
HANDLER
RETURNS
AS
WITH
AN
ALARM
SIGNAL
THE
ACTUAL
RETURN
MAY
OCCUR
AT
A
TIME
LATER
THAN
REQUESTED
BECAUSE
OF
OTHER
SYSTEM
ACTIVITY
IN
CASE
THE
RETURN
VALUE
IS
WHEN
SLEEP
RETURNS
EARLY
BECAUSE
OF
SOME
SIGNAL
BEING
CAUGHT
CASE
THE
RETURN
VALUE
IS
THE
NUMBER
OF
UNSLEPT
SECONDS
THE
REQUESTED
TIME
MINUS
THE
ACTUAL
TIME
SLEPT
ALTHOUGH
SLEEP
CAN
BE
IMPLEMENTED
WITH
THE
ALARM
FUNCTION
SECTION
THIS
ISN
T
REQUIRED
IF
ALARM
IS
USED
HOWEVER
THERE
CAN
BE
INTERACTIONS
BETWEEN
THE
TWO
FUNCTIONS
THE
POSIX
STANDARD
LEAVES
ALL
THESE
INTERACTIONS
UNSPECIFIED
FOR
EXAMPLE
IF
WE
DO
AN
ALARM
AND
WALL
CLOCK
SECONDS
LATER
DO
A
SLEEP
WHAT
HAPPENS
THE
SLEEP
WILL
RETURN
IN
SECONDS
ASSUMING
THAT
SOME
OTHER
SIGNAL
ISN
T
CAUGHT
IN
THE
INTERIM
BUT
WILL
ANOTHER
SIGALRM
BE
GENERATED
SECONDS
LATER
THESE
DETAILS
DEPEND
ON
THE
IMPLEMENTATION
FREEBSD
LINUX
MAC
OS
X
AND
SOLARIS
IMPLEMENT
SLEEP
USING
THE
NANOSLEEP
FUNCTION
WHICH
ALLOWS
THE
IMPLEMENTATION
TO
BE
INDEPENDENT
OF
SIGNALS
AND
THE
ALARM
TIMER
FOR
PORTABILITY
YOU
SHOULDN
T
MAKE
ANY
ASSUMPTIONS
ABOUT
THE
IMPLEMENTATION
OF
SLEEP
BUT
IF
YOU
HAVE
ANY
INTENTIONS
OF
MIXING
CALLS
TO
SLEEP
WITH
ANY
OTHER
TIMING
FUNCTIONS
YOU
NEED
TO
BE
AWARE
OF
POSSIBLE
INTERACTIONS
EXAMPLE
FIGURE
SHOWS
AN
IMPLEMENTATION
OF
THE
POSIX
SLEEP
FUNCTION
THIS
FUNCTION
IS
A
MODIFICATION
OF
FIGURE
WHICH
HANDLES
SIGNALS
RELIABLY
AVOIDING
THE
RACE
CONDITION
IN
THE
EARLIER
IMPLEMENTATION
WE
STILL
DO
NOT
HANDLE
ANY
INTERACTIONS
WITH
PREVIOUSLY
SET
ALARMS
AS
WE
MENTIONED
THESE
INTERACTIONS
ARE
EXPLICITLY
UNDEFINED
BY
POSIX
INCLUDE
APUE
H
STATIC
VOID
INT
SIGNO
NOTHING
TO
DO
JUST
RETURNING
WAKES
UP
SIGSUSPEND
UNSIGNED
INT
SLEEP
UNSIGNED
INT
SECONDS
STRUCT
SIGACTION
NEWACT
OLDACT
NEWMASK
OLDMASK
SUSPMASK
UNSIGNED
INT
UNSLEPT
SET
OUR
HANDLER
SAVE
PREVIOUS
INFORMATION
NEWACT
SIGEMPTYSET
NEWACT
NEWACT
SIGACTION
SIGALRM
NEWACT
OLDACT
BLOCK
SIGALRM
AND
SAVE
CURRENT
SIGNAL
MASK
SIGEMPTYSET
NEWMASK
SIGADDSET
NEWMASK
SIGALRM
SIGPROCMASK
NEWMASK
OLDMASK
ALARM
SECONDS
SUSPMASK
OLDMASK
MAKE
SURE
SIGALRM
ISN
T
BLOCKED
SIGDELSET
SUSPMASK
SIGALRM
WAIT
FOR
ANY
SIGNAL
TO
BE
CAUGHT
SIGSUSPEND
SUSPMASK
SOME
SIGNAL
HAS
BEEN
CAUGHT
SIGALRM
IS
NOW
BLOCKED
UNSLEPT
ALARM
RESET
PREVIOUS
ACTION
SIGACTION
SIGALRM
OLDACT
NULL
RESET
SIGNAL
MASK
WHICH
UNBLOCKS
SIGALRM
SIGPROCMASK
OLDMASK
NULL
RETURN
UNSLEPT
FIGURE
RELIABLE
IMPLEMENTATION
OF
SLEEP
IT
TAKES
MORE
CODE
TO
WRITE
THIS
RELIABLE
IMPLEMENTATION
THAN
WHAT
IS
SHOWN
IN
FIGURE
WE
DON
T
USE
ANY
FORM
OF
NONLOCAL
BRANCHING
AS
WE
DID
IN
FIGURE
TO
AVOID
THE
RACE
CONDITION
BETWEEN
ALARM
AND
PAUSE
SO
THERE
IS
NO
EFFECT
ON
OTHER
SIGNAL
HANDLERS
THAT
MAY
BE
EXECUTING
WHEN
THE
SIGALRM
IS
HANDLED
THE
NANOSLEEP
FUNCTION
IS
SIMILAR
TO
THE
SLEEP
FUNCTION
BUT
PROVIDES
NANOSECOND
LEVEL
GRANULARITY
THIS
FUNCTION
SUSPENDS
THE
CALLING
PROCESS
UNTIL
EITHER
THE
REQUESTED
TIME
HAS
ELAPSED
OR
THE
FUNCTION
IS
INTERRUPTED
BY
A
SIGNAL
THE
REQTP
PARAMETER
SPECIFIES
THE
AMOUNT
OF
TIME
TO
SLEEP
IN
SECONDS
AND
NANOSECONDS
IF
THE
SLEEP
INTERVAL
IS
INTERRUPTED
BY
A
SIGNAL
AND
THE
PROCESS
DOESN
T
TERMINATE
THE
TIMESPEC
STRUCTURE
POINTED
TO
BY
THE
REMTP
PARAMETER
WILL
BE
SET
TO
THE
AMOUNT
OF
TIME
LEFT
IN
THE
SLEEP
INTERVAL
WE
CAN
SET
THIS
PARAMETER
TO
NULL
IF
WE
ARE
UNINTERESTED
IN
THE
TIME
UNSLEPT
IF
THE
SYSTEM
DOESN
T
SUPPORT
NANOSECOND
GRANULARITY
THE
REQUESTED
TIME
IS
ROUNDED
UP
BECAUSE
THE
NANOSLEEP
FUNCTION
DOESN
T
INVOLVE
THE
GENERATION
OF
ANY
SIGNALS
WE
CAN
USE
IT
WITHOUT
WORRYING
ABOUT
INTERACTIONS
WITH
OTHER
FUNCTIONS
THE
NANOSLEEP
FUNCTION
USED
TO
BELONG
TO
THE
TIMERS
OPTION
IN
THE
SINGLE
UNIX
SPECIFICATION
BUT
WAS
MOVED
TO
THE
BASE
IN
WITH
THE
INTRODUCTION
OF
MULTIPLE
SYSTEM
CLOCKS
RECALL
SECTION
WE
NEED
A
WAY
TO
SUSPEND
THE
CALLING
THREAD
USING
A
DELAY
TIME
RELATIVE
TO
A
PARTICULAR
CLOCK
THE
FUNCTION
PROVIDES
US
WITH
THIS
CAPABILITY
THE
ARGUMENT
SPECIFIES
THE
CLOCK
AGAINST
WHICH
THE
TIME
DELAY
IS
EVALUATED
IDENTIFIERS
FOR
CLOCKS
ARE
LISTED
IN
FIGURE
THE
FLAGS
ARGUMENT
IS
USED
TO
CONTROL
WHETHER
THE
DELAY
IS
ABSOLUTE
OR
RELATIVE
WHEN
FLAGS
IS
SET
TO
THE
SLEEP
TIME
IS
RELATIVE
I
E
HOW
LONG
WE
WANT
TO
SLEEP
WHEN
IT
IS
SET
TO
THE
SLEEP
TIME
IS
ABSOLUTE
I
E
WE
WANT
TO
SLEEP
UNTIL
THE
CLOCK
REACHES
THE
SPECIFIED
TIME
THE
OTHER
ARGUMENTS
REQTP
AND
REMTP
ARE
THE
SAME
AS
IN
THE
NANOSLEEP
FUNCTION
HOWEVER
WHEN
WE
USE
AN
ABSOLUTE
TIME
THE
REMTP
ARGUMENT
IS
UNUSED
BECAUSE
IT
ISN
T
NEEDED
WE
CAN
REUSE
THE
SAME
VALUE
FOR
THE
REQTP
ARGUMENT
FOR
ADDITIONAL
CALLS
TO
UNTIL
THE
CLOCK
REACHES
THE
SPECIFIED
ABSOLUTE
TIME
VALUE
NOTE
THAT
EXCEPT
FOR
ERROR
RETURNS
THE
CALL
REQTP
REMTP
HAS
THE
SAME
EFFECT
AS
THE
CALL
NANOSLEEP
REQTP
REMTP
THE
PROBLEM
WITH
USING
A
RELATIVE
SLEEP
IS
THAT
SOME
APPLICATIONS
REQUIRE
PRECISION
WITH
HOW
LONG
THEY
SLEEP
AND
A
RELATIVE
SLEEP
TIME
CAN
LEAD
TO
SLEEPING
LONGER
THAN
DESIRED
FOR
EXAMPLE
IF
AN
APPLICATION
WANTS
TO
PERFORM
A
TASK
AT
REGULAR
INTERVALS
IT
WOULD
HAVE
TO
GET
THE
CURRENT
TIME
CALCULATE
THE
AMOUNT
OF
TIME
UNTIL
THE
NEXT
TIME
TO
EXECUTE
THE
TASK
AND
THEN
CALL
NANOSLEEP
BETWEEN
THE
TIME
THAT
THE
CURRENT
TIME
IS
OBTAINED
AND
THE
CALL
TO
NANOSLEEP
IS
MADE
PROCESSOR
SCHEDULING
AND
PREEMPTION
CAN
RESULT
IN
THE
RELATIVE
SLEEP
TIME
EXTENDING
PAST
THE
DESIRED
INTERVAL
USING
AN
ABSOLUTE
TIME
IMPROVES
THE
PRECISION
EVEN
THOUGH
A
TIME
SHARING
PROCESS
SCHEDULER
MAKES
NO
GUARANTEE
THAT
OUR
TASK
WILL
EXECUTE
IMMEDIATELY
AFTER
OUR
SLEEP
TIME
HAS
ENDED
IN
OLDER
VERSIONS
OF
THE
SINGLE
UNIX
SPECIFICATION
THE
FUNCTION
BELONGED
TO
THE
CLOCK
SELECTION
OPTION
IN
IT
WAS
MOVED
TO
THE
BASE
SIGQUEUE
FUNCTION
IN
SECTION
WE
SAID
THAT
MOST
UNIX
SYSTEMS
DON
T
QUEUE
SIGNALS
WITH
THE
REAL
TIME
EXTENSIONS
TO
POSIX
SOME
SYSTEMS
BEGAN
ADDING
SUPPORT
FOR
QUEUEING
SIGNALS
WITH
THE
QUEUED
SIGNAL
FUNCTIONALITY
HAS
MOVED
FROM
THE
REAL
TIME
EXTENSIONS
TO
THE
BASE
SPECIFICATION
GENERALLY
A
SIGNAL
CARRIES
ONE
BIT
OF
INFORMATION
THE
SIGNAL
ITSELF
IN
ADDITION
TO
QUEUEING
SIGNALS
THESE
EXTENSIONS
ALLOW
APPLICATIONS
TO
PASS
MORE
INFORMATION
ALONG
WITH
THE
DELIVERY
RECALL
SECTION
THIS
INFORMATION
IS
EMBEDDED
IN
A
SIGINFO
STRUCTURE
ALONG
WITH
SYSTEM
PROVIDED
INFORMATION
APPLICATIONS
CAN
PASS
AN
INTEGER
OR
A
POINTER
TO
A
BUFFER
CONTAINING
MORE
INFORMATION
TO
THE
SIGNAL
HANDLER
TO
USE
QUEUED
SIGNALS
WE
HAVE
TO
DO
THE
FOLLOWING
SPECIFY
THE
FLAG
WHEN
WE
INSTALL
A
SIGNAL
HANDLER
USING
THE
SIGACTION
FUNCTION
IF
WE
DON
T
SPECIFY
THIS
FLAG
THE
SIGNAL
WILL
BE
POSTED
BUT
IT
IS
LEFT
UP
TO
THE
IMPLEMENTATION
WHETHER
THE
SIGNAL
IS
QUEUED
PROVIDE
A
SIGNAL
HANDLER
IN
THE
MEMBER
OF
THE
SIGACTION
STRUCTURE
INSTEAD
OF
USING
THE
USUAL
FIELD
IMPLEMENTATIONS
MIGHT
ALLOW
US
TO
USE
THE
FIELD
BUT
WE
WON
T
BE
ABLE
TO
OBTAIN
THE
EXTRA
INFORMATION
SENT
WITH
THE
SIGQUEUE
FUNCTION
USE
THE
SIGQUEUE
FUNCTION
TO
SEND
SIGNALS
THE
SIGQUEUE
FUNCTION
IS
SIMILAR
TO
THE
KILL
FUNCTION
EXCEPT
THAT
WE
CAN
ONLY
DIRECT
SIGNALS
TO
A
SINGLE
PROCESS
WITH
SIGQUEUE
AND
WE
CAN
USE
THE
VALUE
ARGUMENT
TO
TRANSMIT
EITHER
AN
INTEGER
OR
A
POINTER
VALUE
TO
THE
SIGNAL
HANDLER
SIGNALS
CAN
T
BE
QUEUED
INFINITELY
RECALL
THE
LIMIT
FROM
FIGURE
AND
FIGURE
WHEN
THIS
LIMIT
IS
REACHED
SIGQUEUE
CAN
FAIL
WITH
ERRNO
SET
TO
EAGAIN
WITH
THE
REAL
TIME
SIGNAL
ENHANCEMENTS
A
SEPARATE
SET
OF
SIGNALS
WAS
INTRODUCED
FOR
APPLICATION
USE
THESE
ARE
THE
SIGNAL
NUMBERS
BETWEEN
SIGRTMIN
AND
SIGRTMAX
INCLUSIVE
BE
AWARE
THAT
THE
DEFAULT
ACTION
FOR
THESE
SIGNALS
IS
TO
TERMINATE
THE
PROCESS
FIGURE
SUMMARIZES
THE
WAY
QUEUED
SIGNALS
DIFFER
IN
BEHAVIOR
AMONG
THE
IMPLEMENTATIONS
COVERED
IN
THIS
TEXT
BEHAVIOR
SUS
FREEBSD
LINUX
MAC
OS
X
SOLARIS
SUPPORTS
SIGQUEUE
QUEUES
OTHER
SIGNALS
BESIDES
SIGRTMIN
TO
SIGRTMAX
OPTIONAL
QUEUES
SIGNALS
EVEN
IF
THE
CALLER
DOESN
T
USE
THE
FLAG
OPTIONAL
FIGURE
BEHAVIOR
OF
QUEUED
SIGNALS
ON
VARIOUS
PLATFORMS
MAC
OS
X
DOESN
T
SUPPORT
SIGQUEUE
OR
REAL
TIME
SIGNALS
ON
SOLARIS
SIGQUEUE
IS
IN
THE
REAL
TIME
LIBRARY
LIBRT
JOB
CONTROL
SIGNALS
OF
THE
SIGNALS
SHOWN
IN
FIGURE
POSIX
CONSIDERS
SIX
TO
BE
JOB
CONTROL
SIGNALS
SIGCHLD
CHILD
PROCESS
HAS
STOPPED
OR
TERMINATED
SIGCONT
CONTINUE
PROCESS
IF
STOPPED
SIGSTOP
STOP
SIGNAL
CAN
T
BE
CAUGHT
OR
IGNORED
SIGTSTP
INTERACTIVE
STOP
SIGNAL
SIGTTIN
READ
FROM
CONTROLLING
TERMINAL
BY
BACKGROUND
PROCESS
GROUP
MEMBER
SIGTTOU
WRITE
TO
CONTROLLING
TERMINAL
BY
A
BACKGROUND
PROCESS
GROUP
MEMBER
EXCEPT
FOR
SIGCHLD
MOST
APPLICATION
PROGRAMS
DON
T
HANDLE
THESE
SIGNALS
INTERACTIVE
SHELLS
USUALLY
DO
ALL
THE
WORK
REQUIRED
TO
HANDLE
THEM
WHEN
WE
TYPE
THE
SUSPEND
CHARACTER
USUALLY
CONTROL
Z
SIGTSTP
IS
SENT
TO
ALL
PROCESSES
IN
THE
FOREGROUND
PROCESS
GROUP
WHEN
WE
TELL
THE
SHELL
TO
RESUME
A
JOB
IN
THE
FOREGROUND
OR
BACKGROUND
THE
SHELL
SENDS
ALL
THE
PROCESSES
IN
THE
JOB
THE
SIGCONT
SIGNAL
SIMILARLY
IF
SIGTTIN
OR
SIGTTOU
IS
DELIVERED
TO
A
PROCESS
THE
PROCESS
IS
STOPPED
BY
DEFAULT
AND
THE
JOB
CONTROL
SHELL
RECOGNIZES
THIS
AND
NOTIFIES
US
AN
EXCEPTION
IS
A
PROCESS
THAT
IS
MANAGING
THE
TERMINAL
THE
VI
EDITOR
FOR
EXAMPLE
IT
NEEDS
TO
KNOW
WHEN
THE
USER
WANTS
TO
SUSPEND
IT
SO
THAT
IT
CAN
RESTORE
THE
TERMINAL
STATE
TO
THE
WAY
IT
WAS
WHEN
VI
WAS
STARTED
ALSO
WHEN
IT
RESUMES
IN
THE
FOREGROUND
THE
VI
EDITOR
NEEDS
TO
SET
THE
TERMINAL
STATE
BACK
TO
THE
WAY
IT
WANTS
IT
AND
IT
NEEDS
TO
REDRAW
THE
TERMINAL
SCREEN
WE
SEE
HOW
A
PROGRAM
SUCH
AS
VI
HANDLES
THIS
IN
THE
EXAMPLE
THAT
FOLLOWS
THERE
ARE
SOME
INTERACTIONS
BETWEEN
THE
JOB
CONTROL
SIGNALS
WHEN
ANY
OF
THE
FOUR
STOP
SIGNALS
SIGTSTP
SIGSTOP
SIGTTIN
OR
SIGTTOU
IS
GENERATED
FOR
A
PROCESS
ANY
PENDING
SIGCONT
SIGNAL
FOR
THAT
PROCESS
IS
DISCARDED
SIMILARLY
WHEN
THE
SIGCONT
SIGNAL
IS
GENERATED
FOR
A
PROCESS
ANY
PENDING
STOP
SIGNALS
FOR
THAT
SAME
PROCESS
ARE
DISCARDED
NOTE
THAT
THE
DEFAULT
ACTION
FOR
SIGCONT
IS
TO
CONTINUE
THE
PROCESS
IF
IT
IS
STOPPED
OTHERWISE
THE
SIGNAL
IS
IGNORED
NORMALLY
WE
DON
T
HAVE
TO
DO
ANYTHING
WITH
THIS
SIGNAL
WHEN
SIGCONT
IS
GENERATED
FOR
A
PROCESS
THAT
IS
STOPPED
THE
PROCESS
IS
CONTINUED
EVEN
IF
THE
SIGNAL
IS
BLOCKED
OR
IGNORED
EXAMPLE
THE
PROGRAM
IN
FIGURE
DEMONSTRATES
THE
NORMAL
SEQUENCE
OF
CODE
USED
WHEN
A
PROGRAM
HANDLES
JOB
CONTROL
THIS
PROGRAM
SIMPLY
COPIES
ITS
STANDARD
INPUT
TO
ITS
STANDARD
OUTPUT
BUT
COMMENTS
ARE
GIVEN
IN
THE
SIGNAL
HANDLER
FOR
TYPICAL
ACTIONS
PERFORMED
BY
A
PROGRAM
THAT
MANAGES
A
SCREEN
INCLUDE
APUE
H
DEFINE
BUFFSIZE
STATIC
VOID
INT
SIGNO
SIGNAL
HANDLER
FOR
SIGTSTP
INT
MASK
MOVE
CURSOR
TO
LOWER
LEFT
CORNER
RESET
TTY
MODE
UNBLOCK
SIGTSTP
SINCE
IT
BLOCKED
WHILE
WE
RE
HANDLING
IT
SIGEMPTYSET
MASK
SIGADDSET
MASK
SIGTSTP
SIGPROCMASK
MASK
NULL
SIGNAL
SIGTSTP
RESET
DISPOSITION
TO
DEFAULT
KILL
GETPID
SIGTSTP
AND
SEND
THE
SIGNAL
TO
OURSELF
WE
WON
T
RETURN
FROM
THE
KILL
UNTIL
WE
RE
CONTINUED
SIGNAL
SIGTSTP
REESTABLISH
SIGNAL
HANDLER
RESET
TTY
MODE
REDRAW
SCREEN
MAIN
VOID
INT
N
CHAR
BUF
BUFFSIZE
ONLY
CATCH
SIGTSTP
IF
WE
RE
RUNNING
WITH
A
JOB
CONTROL
SHELL
IF
SIGNAL
SIGTSTP
SIGNAL
SIGTSTP
WHILE
N
READ
BUF
BUFFSIZE
IF
WRITE
BUF
N
N
WRITE
ERROR
IF
N
READ
ERROR
EXIT
FIGURE
HOW
TO
HANDLE
SIGTSTP
WHEN
THE
PROGRAM
IN
FIGURE
STARTS
IT
ARRANGES
TO
CATCH
THE
SIGTSTP
SIGNAL
ONLY
IF
THE
SIGNAL
DISPOSITION
IS
THE
REASON
IS
THAT
WHEN
THE
PROGRAM
IS
STARTED
BY
A
SHELL
THAT
DOESN
T
SUPPORT
JOB
CONTROL
BIN
SH
FOR
EXAMPLE
THE
SIGNAL
DISPOSITION
SHOULD
BE
SET
TO
IN
FACT
THE
SHELL
DOESN
T
EXPLICITLY
IGNORE
THIS
SIGNAL
INIT
SETS
THE
DISPOSITION
OF
THE
THREE
JOB
CONTROL
SIGNALS
SIGTSTP
SIGTTIN
AND
SIGTTOU
TO
THIS
DISPOSITION
IS
THEN
INHERITED
BY
ALL
LOGIN
SHELLS
ONLY
A
JOB
CONTROL
SHELL
SHOULD
RESET
THE
DISPOSITION
OF
THESE
THREE
SIGNALS
TO
WHEN
WE
TYPE
THE
SUSPEND
CHARACTER
THE
PROCESS
RECEIVES
THE
SIGTSTP
SIGNAL
AND
THE
SIGNAL
HANDLER
IS
INVOKED
AT
THIS
POINT
WE
WOULD
DO
ANY
TERMINAL
RELATED
PROCESSING
MOVE
THE
CURSOR
TO
THE
LOWER
LEFT
CORNER
RESTORE
THE
TERMINAL
MODE
AND
SO
ON
WE
THEN
SEND
OURSELF
THE
SAME
SIGNAL
SIGTSTP
AFTER
RESETTING
ITS
DISPOSITION
TO
ITS
DEFAULT
STOP
THE
PROCESS
AND
UNBLOCKING
THE
SIGNAL
WE
HAVE
TO
UNBLOCK
IT
SINCE
WE
RE
CURRENTLY
HANDLING
THAT
SAME
SIGNAL
AND
THE
SYSTEM
BLOCKS
IT
AUTOMATICALLY
WHILE
IT
BEING
CAUGHT
AT
THIS
POINT
THE
SYSTEM
STOPS
THE
PROCESS
IT
IS
CONTINUED
ONLY
WHEN
IT
RECEIVES
USUALLY
FROM
THE
JOB
CONTROL
SHELL
IN
RESPONSE
TO
AN
INTERACTIVE
FG
COMMAND
A
SIGCONT
SIGNAL
WE
DON
T
CATCH
SIGCONT
ITS
DEFAULT
DISPOSITION
IS
TO
CONTINUE
THE
STOPPED
PROCESS
WHEN
THIS
HAPPENS
THE
PROGRAM
CONTINUES
AS
THOUGH
IT
RETURNED
FROM
THE
KILL
FUNCTION
WHEN
THE
PROGRAM
IS
CONTINUED
WE
RESET
THE
DISPOSITION
FOR
THE
SIGTSTP
SIGNAL
AND
DO
WHATEVER
TERMINAL
PROCESSING
WE
WANT
WE
COULD
REDRAW
THE
SCREEN
FOR
EXAMPLE
SIGNAL
NAMES
AND
NUMBERS
IN
THIS
SECTION
WE
DESCRIBE
HOW
TO
MAP
BETWEEN
SIGNAL
NUMBERS
AND
NAMES
SOME
SYSTEMS
PROVIDE
THE
ARRAY
EXTERN
CHAR
THE
ARRAY
INDEX
IS
THE
SIGNAL
NUMBER
GIVING
A
POINTER
TO
THE
CHARACTER
STRING
NAME
OF
THE
SIGNAL
FREEBSD
LINUX
AND
MAC
OS
X
ALL
PROVIDE
THIS
ARRAY
OF
SIGNAL
NAMES
SOLARIS
DOES
TOO
BUT
IT
USES
THE
NAME
INSTEAD
TO
PRINT
THE
CHARACTER
STRING
CORRESPONDING
TO
A
SIGNAL
NUMBER
IN
A
PORTABLE
MANNER
WE
CAN
USE
THE
PSIGNAL
FUNCTION
THE
STRING
MSG
WHICH
NORMALLY
INCLUDES
THE
NAME
OF
THE
PROGRAM
IS
OUTPUT
TO
THE
STANDARD
ERROR
FOLLOWED
BY
A
COLON
AND
A
SPACE
FOLLOWED
BY
A
DESCRIPTION
OF
THE
SIGNAL
FOLLOWED
BY
A
NEWLINE
IF
MSG
IS
NULL
THEN
ONLY
THE
DESCRIPTION
IS
WRITTEN
TO
THE
STANDARD
ERROR
THIS
FUNCTION
IS
SIMILAR
TO
PERROR
SECTION
IF
YOU
HAVE
A
SIGINFO
STRUCTURE
FROM
AN
ALTERNATIVE
SIGACTION
SIGNAL
HANDLER
YOU
CAN
PRINT
THE
SIGNAL
INFORMATION
WITH
THE
PSIGINFO
FUNCTION
IT
OPERATES
IN
A
SIMILAR
MANNER
TO
THE
PSIGNAL
FUNCTION
ALTHOUGH
THIS
FUNCTION
HAS
ACCESS
TO
MORE
INFORMATION
THAN
JUST
THE
SIGNAL
NUMBER
PLATFORMS
VARY
IN
EXACTLY
WHAT
ADDITIONAL
INFORMATION
IS
PRINTED
IF
YOU
ONLY
NEED
THE
STRING
DESCRIPTION
OF
THE
SIGNAL
AND
DON
T
NECESSARILY
WANT
TO
WRITE
IT
TO
STANDARD
ERROR
YOU
MIGHT
WANT
TO
WRITE
IT
TO
A
LOG
FILE
FOR
EXAMPLE
YOU
CAN
USE
THE
STRSIGNAL
FUNCTION
THIS
FUNCTION
IS
SIMILAR
TO
STRERROR
ALSO
DESCRIBED
IN
SECTION
GIVEN
A
SIGNAL
NUMBER
STRSIGNAL
WILL
RETURN
A
STRING
THAT
DESCRIBES
THE
SIGNAL
THIS
STRING
CAN
BE
USED
BY
APPLICATIONS
TO
PRINT
ERROR
MESSAGES
ABOUT
SIGNALS
RECEIVED
ALL
THE
PLATFORMS
DISCUSSED
IN
THIS
BOOK
PROVIDE
THE
PSIGNAL
AND
STRSIGNAL
FUNCTIONS
BUT
DIFFERENCES
DO
OCCUR
ON
SOLARIS
STRSIGNAL
WILL
RETURN
A
NULL
POINTER
IF
THE
SIGNAL
NUMBER
IS
INVALID
WHEREAS
FREEBSD
LINUX
AND
MAC
OS
X
RETURN
A
STRING
INDICATING
THAT
THE
SIGNAL
NUMBER
IS
UNRECOGNIZED
ONLY
LINUX
AND
SOLARIS
SUPPORT
THE
PSIGINFO
FUNCTION
SOLARIS
PROVIDES
A
COUPLE
OF
FUNCTIONS
TO
MAP
A
SIGNAL
NUMBER
TO
A
SIGNAL
NAME
AND
VICE
VERSA
THESE
FUNCTIONS
ARE
USEFUL
WHEN
WRITING
INTERACTIVE
PROGRAMS
THAT
NEED
TO
ACCEPT
AND
PRINT
SIGNAL
NAMES
AND
NUMBERS
THE
FUNCTION
TRANSLATES
THE
GIVEN
SIGNAL
NUMBER
INTO
A
STRING
AND
STORES
THE
RESULT
IN
THE
MEMORY
POINTED
TO
BY
STR
THE
CALLER
MUST
ENSURE
THAT
THE
MEMORY
IS
LARGE
ENOUGH
TO
HOLD
THE
LONGEST
STRING
INCLUDING
THE
TERMINATING
NULL
BYTE
SOLARIS
PROVIDES
THE
CONSTANT
IN
SIGNAL
H
TO
DEFINE
THE
MAXIMUM
STRING
LENGTH
THE
STRING
CONSISTS
OF
THE
SIGNAL
NAME
WITHOUT
THE
SIG
PREFIX
FOR
EXAMPLE
TRANSLATING
SIGKILL
WOULD
RESULT
IN
THE
STRING
KILL
BEING
STORED
IN
THE
STR
MEMORY
BUFFER
THE
FUNCTION
TRANSLATES
THE
GIVEN
NAME
INTO
A
SIGNAL
NUMBER
THE
SIGNAL
NUMBER
IS
STORED
IN
THE
INTEGER
POINTED
TO
BY
SIGNOP
THE
NAME
CAN
BE
EITHER
THE
SIGNAL
NAME
WITHOUT
THE
SIG
PREFIX
OR
A
STRING
REPRESENTATION
OF
THE
DECIMAL
SIGNAL
NUMBER
I
E
NOTE
THAT
AND
DEPART
FROM
COMMON
PRACTICE
AND
DON
T
SET
ERRNO
WHEN
THEY
FAIL
SUMMARY
SIGNALS
ARE
USED
IN
MOST
NONTRIVIAL
APPLICATIONS
AN
UNDERSTANDING
OF
THE
HOWS
AND
WHYS
OF
SIGNAL
HANDLING
IS
ESSENTIAL
TO
ADVANCED
UNIX
SYSTEM
PROGRAMMING
THIS
CHAPTER
HAS
TAKEN
A
LONG
AND
THOROUGH
LOOK
AT
UNIX
SYSTEM
SIGNALS
WE
STARTED
BY
LOOKING
AT
THE
WARTS
IN
PREVIOUS
IMPLEMENTATIONS
OF
SIGNALS
AND
HOW
THEY
MANIFEST
THEMSELVES
WE
THEN
PROCEEDED
TO
THE
POSIX
RELIABLE
SIGNAL
CONCEPT
AND
ALL
THE
RELATED
FUNCTIONS
ONCE
WE
COVERED
ALL
THESE
DETAILS
WE
WERE
ABLE
TO
PROVIDE
IMPLEMENTATIONS
OF
THE
POSIX
ABORT
SYSTEM
AND
SLEEP
FUNCTIONS
WE
FINISHED
WITH
A
LOOK
AT
THE
JOB
CONTROL
SIGNALS
AND
THE
WAYS
THAT
WE
CAN
CONVERT
BETWEEN
SIGNAL
NAMES
AND
SIGNAL
NUMBERS
EXERCISES
IN
FIGURE
REMOVE
THE
FOR
STATEMENT
WHAT
HAPPENS
AND
WHY
IMPLEMENT
THE
FUNCTION
DESCRIBED
IN
SECTION
DRAW
PICTURES
OF
THE
STACK
FRAMES
WHEN
WE
RUN
THE
PROGRAM
FROM
FIGURE
IN
FIGURE
WE
SHOWED
A
TECHNIQUE
THAT
OFTEN
USED
TO
SET
A
TIMEOUT
ON
AN
I
O
OPERATION
USING
SETJMP
AND
LONGJMP
THE
FOLLOWING
CODE
HAS
ALSO
BEEN
SEEN
SIGNAL
SIGALRM
ALARM
IF
SETJMP
HANDLE
TIMEOUT
WHAT
ELSE
IS
WRONG
WITH
THIS
SEQUENCE
OF
CODE
USING
ONLY
A
SINGLE
TIMER
EITHER
ALARM
OR
THE
HIGHER
PRECISION
SETITIMER
PROVIDE
A
SET
OF
FUNCTIONS
THAT
ALLOWS
A
PROCESS
TO
SET
ANY
NUMBER
OF
TIMERS
WRITE
THE
FOLLOWING
PROGRAM
TO
TEST
THE
PARENT
CHILD
SYNCHRONIZATION
FUNCTIONS
IN
FIGURE
THE
PROCESS
CREATES
A
FILE
AND
WRITES
THE
INTEGER
TO
THE
FILE
THE
PROCESS
THEN
CALLS
FORK
AND
THE
PARENT
AND
CHILD
ALTERNATE
INCREMENTING
THE
COUNTER
IN
THE
FILE
EACH
TIME
THE
COUNTER
IS
INCREMENTED
PRINT
WHICH
PROCESS
PARENT
OR
CHILD
IS
DOING
THE
INCREMENT
IN
THE
FUNCTION
SHOWN
IN
FIGURE
IF
THE
CALLER
CATCHES
SIGABRT
AND
RETURNS
FROM
THE
SIGNAL
HANDLER
WHY
DO
WE
GO
TO
THE
TROUBLE
OF
RESETTING
THE
DISPOSITION
TO
ITS
DEFAULT
AND
CALL
KILL
THE
SECOND
TIME
INSTEAD
OF
SIMPLY
CALLING
WHY
DO
YOU
THINK
THE
SIGINFO
STRUCTURE
SECTION
INCLUDES
THE
REAL
USER
ID
INSTEAD
OF
THE
EFFECTIVE
USER
ID
IN
THE
FIELD
REWRITE
THE
FUNCTION
IN
FIGURE
TO
HANDLE
ALL
THE
SIGNALS
FROM
FIGURE
THE
FUNCTION
SHOULD
CONSIST
OF
A
SINGLE
LOOP
THAT
ITERATES
ONCE
FOR
EVERY
SIGNAL
IN
THE
CURRENT
SIGNAL
MASK
NOT
ONCE
FOR
EVERY
POSSIBLE
SIGNAL
WRITE
A
PROGRAM
THAT
CALLS
SLEEP
IN
AN
INFINITE
LOOP
EVERY
FIVE
TIMES
THROUGH
THE
LOOP
EVERY
MINUTES
FETCH
THE
CURRENT
TIME
OF
DAY
AND
PRINT
THE
FIELD
RUN
THE
PROGRAM
OVERNIGHT
AND
EXPLAIN
THE
RESULTS
HOW
WOULD
A
PROGRAM
SUCH
AS
THE
CRON
DAEMON
WHICH
RUNS
EVERY
MINUTE
ON
THE
MINUTE
HANDLE
THIS
SITUATION
MODIFY
FIGURE
AS
FOLLOWS
A
CHANGE
BUFFSIZE
TO
B
CATCH
THE
SIGXFSZ
SIGNAL
USING
THE
FUNCTION
PRINTING
A
MESSAGE
WHEN
IT
CAUGHT
AND
RETURNING
FROM
THE
SIGNAL
HANDLER
AND
C
PRINT
THE
RETURN
VALUE
FROM
WRITE
IF
THE
REQUESTED
NUMBER
OF
BYTES
WASN
T
WRITTEN
MODIFY
THE
SOFT
RESOURCE
LIMIT
SECTION
TO
BYTES
AND
RUN
YOUR
NEW
PROGRAM
COPYING
A
FILE
THAT
IS
LARGER
THAN
BYTES
TRY
TO
SET
THE
SOFT
RESOURCE
LIMIT
FROM
YOUR
SHELL
IF
YOU
CAN
T
DO
THIS
FROM
YOUR
SHELL
CALL
SETRLIMIT
DIRECTLY
FROM
THE
PROGRAM
RUN
THIS
PROGRAM
ON
THE
DIFFERENT
SYSTEMS
THAT
YOU
HAVE
ACCESS
TO
WHAT
HAPPENS
AND
WHY
WRITE
A
PROGRAM
THAT
CALLS
FWRITE
WITH
A
LARGE
BUFFER
ABOUT
ONE
GIGABYTE
BEFORE
CALLING
FWRITE
CALL
ALARM
TO
SCHEDULE
A
SIGNAL
IN
SECOND
IN
YOUR
SIGNAL
HANDLER
PRINT
THAT
THE
SIGNAL
WAS
CAUGHT
AND
RETURN
DOES
THE
CALL
TO
FWRITE
COMPLETE
WHAT
HAPPENING
THREADS
INTRODUCTION
WE
DISCUSSED
PROCESSES
IN
EARLIER
CHAPTERS
WE
LEARNED
ABOUT
THE
ENVIRONMENT
OF
A
UNIX
PROCESS
THE
RELATIONSHIPS
BETWEEN
PROCESSES
AND
WAYS
TO
CONTROL
PROCESSES
WE
SAW
THAT
A
LIMITED
AMOUNT
OF
SHARING
CAN
OCCUR
BETWEEN
RELATED
PROCESSES
IN
THIS
CHAPTER
WE
LL
LOOK
INSIDE
A
PROCESS
FURTHER
TO
SEE
HOW
WE
CAN
USE
MULTIPLE
THREADS
OF
CONTROL
OR
SIMPLY
THREADS
TO
PERFORM
MULTIPLE
TASKS
WITHIN
THE
ENVIRONMENT
OF
A
SINGLE
PROCESS
ALL
THREADS
WITHIN
A
SINGLE
PROCESS
HAVE
ACCESS
TO
THE
SAME
PROCESS
COMPONENTS
SUCH
AS
FILE
DESCRIPTORS
AND
MEMORY
ANYTIME
YOU
TRY
TO
SHARE
A
SINGLE
RESOURCE
AMONG
MULTIPLE
USERS
YOU
HAVE
TO
DEAL
WITH
CONSISTENCY
WE
LL
CONCLUDE
THIS
CHAPTER
WITH
A
LOOK
AT
THE
SYNCHRONIZATION
MECHANISMS
AVAILABLE
TO
PREVENT
MULTIPLE
THREADS
FROM
VIEWING
INCONSISTENCIES
IN
THEIR
SHARED
RESOURCES
THREAD
CONCEPTS
A
TYPICAL
UNIX
PROCESS
CAN
BE
THOUGHT
OF
AS
HAVING
A
SINGLE
THREAD
OF
CONTROL
EACH
PROCESS
IS
DOING
ONLY
ONE
THING
AT
A
TIME
WITH
MULTIPLE
THREADS
OF
CONTROL
WE
CAN
DESIGN
OUR
PROGRAMS
TO
DO
MORE
THAN
ONE
THING
AT
A
TIME
WITHIN
A
SINGLE
PROCESS
WITH
EACH
THREAD
HANDLING
A
SEPARATE
TASK
THIS
APPROACH
CAN
HAVE
SEVERAL
BENEFITS
WE
CAN
SIMPLIFY
CODE
THAT
DEALS
WITH
ASYNCHRONOUS
EVENTS
BY
ASSIGNING
A
SEPARATE
THREAD
TO
HANDLE
EACH
EVENT
TYPE
EACH
THREAD
CAN
THEN
HANDLE
ITS
EVENT
USING
A
SYNCHRONOUS
PROGRAMMING
MODEL
A
SYNCHRONOUS
PROGRAMMING
MODEL
IS
MUCH
SIMPLER
THAN
AN
ASYNCHRONOUS
ONE
MULTIPLE
PROCESSES
HAVE
TO
USE
COMPLEX
MECHANISMS
PROVIDED
BY
THE
OPERATING
SYSTEM
TO
SHARE
MEMORY
AND
FILE
DESCRIPTORS
AS
WE
WILL
SEE
IN
CHAPTERS
AND
THREADS
IN
CONTRAST
AUTOMATICALLY
HAVE
ACCESS
TO
THE
SAME
MEMORY
ADDRESS
SPACE
AND
FILE
DESCRIPTORS
SOME
PROBLEMS
CAN
BE
PARTITIONED
SO
THAT
OVERALL
PROGRAM
THROUGHPUT
CAN
BE
IMPROVED
A
SINGLE
THREADED
PROCESS
WITH
MULTIPLE
TASKS
TO
PERFORM
IMPLICITLY
SERIALIZES
THOSE
TASKS
BECAUSE
THERE
IS
ONLY
ONE
THREAD
OF
CONTROL
WITH
MULTIPLE
THREADS
OF
CONTROL
THE
PROCESSING
OF
INDEPENDENT
TASKS
CAN
BE
INTERLEAVED
BY
ASSIGNING
A
SEPARATE
THREAD
PER
TASK
TWO
TASKS
CAN
BE
INTERLEAVED
ONLY
IF
THEY
DON
T
DEPEND
ON
THE
PROCESSING
PERFORMED
BY
EACH
OTHER
SIMILARLY
INTERACTIVE
PROGRAMS
CAN
REALIZE
IMPROVED
RESPONSE
TIME
BY
USING
MULTIPLE
THREADS
TO
SEPARATE
THE
PORTIONS
OF
THE
PROGRAM
THAT
DEAL
WITH
USER
INPUT
AND
OUTPUT
FROM
THE
OTHER
PARTS
OF
THE
PROGRAM
SOME
PEOPLE
ASSOCIATE
MULTITHREADED
PROGRAMMING
WITH
MULTIPROCESSOR
OR
MULTICORE
SYSTEMS
THE
BENEFITS
OF
A
MULTITHREADED
PROGRAMMING
MODEL
CAN
BE
REALIZED
EVEN
IF
YOUR
PROGRAM
IS
RUNNING
ON
A
UNIPROCESSOR
A
PROGRAM
CAN
BE
SIMPLIFIED
USING
THREADS
REGARDLESS
OF
THE
NUMBER
OF
PROCESSORS
BECAUSE
THE
NUMBER
OF
PROCESSORS
DOESN
T
AFFECT
THE
PROGRAM
STRUCTURE
FURTHERMORE
AS
LONG
AS
YOUR
PROGRAM
HAS
TO
BLOCK
WHEN
SERIALIZING
TASKS
YOU
CAN
STILL
SEE
IMPROVEMENTS
IN
RESPONSE
TIME
AND
THROUGHPUT
WHEN
RUNNING
ON
A
UNIPROCESSOR
BECAUSE
SOME
THREADS
MIGHT
BE
ABLE
TO
RUN
WHILE
OTHERS
ARE
BLOCKED
A
THREAD
CONSISTS
OF
THE
INFORMATION
NECESSARY
TO
REPRESENT
AN
EXECUTION
CONTEXT
WITHIN
A
PROCESS
THIS
INCLUDES
A
THREAD
ID
THAT
IDENTIFIES
THE
THREAD
WITHIN
A
PROCESS
A
SET
OF
REGISTER
VALUES
A
STACK
A
SCHEDULING
PRIORITY
AND
POLICY
A
SIGNAL
MASK
AN
ERRNO
VARIABLE
RECALL
SECTION
AND
THREAD
SPECIFIC
DATA
SECTION
EVERYTHING
WITHIN
A
PROCESS
IS
SHARABLE
AMONG
THE
THREADS
IN
A
PROCESS
INCLUDING
THE
TEXT
OF
THE
EXECUTABLE
PROGRAM
THE
PROGRAM
GLOBAL
AND
HEAP
MEMORY
THE
STACKS
AND
THE
FILE
DESCRIPTORS
THE
THREADS
INTERFACES
WE
RE
ABOUT
TO
SEE
ARE
FROM
POSIX
THE
THREADS
INTERFACES
ALSO
KNOWN
AS
PTHREADS
FOR
POSIX
THREADS
ORIGINALLY
WERE
OPTIONAL
IN
POSIX
BUT
MOVED
THEM
TO
THE
BASE
THE
FEATURE
TEST
MACRO
FOR
POSIX
THREADS
IS
APPLICATIONS
CAN
EITHER
USE
THIS
IN
AN
IFDEF
TEST
TO
DETERMINE
AT
COMPILE
TIME
WHETHER
THREADS
ARE
SUPPORTED
OR
CALL
SYSCONF
WITH
THE
CONSTANT
TO
DETERMINE
THIS
AT
RUNTIME
SYSTEMS
CONFORMING
TO
DEFINE
THE
SYMBOL
TO
HAVE
THE
VALUE
THREAD
IDENTIFICATION
JUST
AS
EVERY
PROCESS
HAS
A
PROCESS
ID
EVERY
THREAD
HAS
A
THREAD
ID
UNLIKE
THE
PROCESS
ID
WHICH
IS
UNIQUE
IN
THE
SYSTEM
THE
THREAD
ID
HAS
SIGNIFICANCE
ONLY
WITHIN
THE
CONTEXT
OF
THE
PROCESS
TO
WHICH
IT
BELONGS
RECALL
THAT
A
PROCESS
ID
REPRESENTED
BY
THE
DATA
TYPE
IS
A
NON
NEGATIVE
INTEGER
A
THREAD
ID
IS
REPRESENTED
BY
THE
DATA
TYPE
IMPLEMENTATIONS
ARE
ALLOWED
TO
USE
A
STRUCTURE
TO
REPRESENT
THE
DATA
TYPE
SO
PORTABLE
IMPLEMENTATIONS
CAN
T
TREAT
THEM
AS
INTEGERS
THEREFORE
A
FUNCTION
MUST
BE
USED
TO
COMPARE
TWO
THREAD
IDS
LINUX
USES
AN
UNSIGNED
LONG
INTEGER
FOR
THE
DATA
TYPE
SOLARIS
REPRESENTS
THE
DATA
TYPE
AS
AN
UNSIGNED
INTEGER
FREEBSD
AND
MAC
OS
X
USE
A
POINTER
TO
THE
PTHREAD
STRUCTURE
FOR
THE
DATA
TYPE
A
CONSEQUENCE
OF
ALLOWING
THE
DATA
TYPE
TO
BE
A
STRUCTURE
IS
THAT
THERE
IS
NO
PORTABLE
WAY
TO
PRINT
ITS
VALUE
SOMETIMES
IT
IS
USEFUL
TO
PRINT
THREAD
IDS
DURING
PROGRAM
DEBUGGING
BUT
THERE
IS
USUALLY
NO
NEED
TO
DO
SO
OTHERWISE
AT
WORST
THIS
RESULTS
IN
NONPORTABLE
DEBUG
CODE
SO
IT
IS
NOT
MUCH
OF
A
LIMITATION
A
THREAD
CAN
OBTAIN
ITS
OWN
THREAD
ID
BY
CALLING
THE
FUNCTION
THIS
FUNCTION
CAN
BE
USED
WITH
WHEN
A
THREAD
NEEDS
TO
IDENTIFY
DATA
STRUCTURES
THAT
ARE
TAGGED
WITH
ITS
THREAD
ID
FOR
EXAMPLE
A
MASTER
THREAD
MIGHT
PLACE
WORK
ASSIGNMENTS
ON
A
QUEUE
AND
USE
THE
THREAD
ID
TO
CONTROL
WHICH
JOBS
GO
TO
EACH
WORKER
THREAD
THIS
SITUATION
IS
ILLUSTRATED
IN
FIGURE
A
SINGLE
MASTER
THREAD
PLACES
NEW
JOBS
ON
A
WORK
QUEUE
A
POOL
OF
THREE
WORKER
THREADS
REMOVES
JOBS
FROM
THE
QUEUE
INSTEAD
OF
ALLOWING
EACH
THREAD
TO
PROCESS
WHICHEVER
JOB
IS
AT
THE
HEAD
OF
THE
QUEUE
THE
MASTER
THREAD
CONTROLS
JOB
ASSIGNMENT
BY
PLACING
THE
ID
OF
THE
THREAD
THAT
SHOULD
PROCESS
THE
JOB
IN
EACH
JOB
STRUCTURE
EACH
WORKER
THREAD
THEN
REMOVES
ONLY
JOBS
THAT
ARE
TAGGED
WITH
ITS
OWN
THREAD
ID
THREAD
CREATION
THE
TRADITIONAL
UNIX
PROCESS
MODEL
SUPPORTS
ONLY
ONE
THREAD
OF
CONTROL
PER
PROCESS
CONCEPTUALLY
THIS
IS
THE
SAME
AS
A
THREADS
BASED
MODEL
WHEREBY
EACH
PROCESS
IS
MADE
UP
OF
ONLY
ONE
THREAD
WITH
PTHREADS
WHEN
A
PROGRAM
RUNS
IT
ALSO
STARTS
OUT
AS
A
SINGLE
PROCESS
WITH
A
SINGLE
THREAD
OF
CONTROL
AS
THE
PROGRAM
RUNS
ITS
BEHAVIOR
SHOULD
BE
INDISTINGUISHABLE
FROM
THE
TRADITIONAL
PROCESS
UNTIL
IT
CREATES
MORE
THREADS
OF
CONTROL
ADDITIONAL
THREADS
CAN
BE
CREATED
BY
CALLING
THE
FUNCTION
FIGURE
WORK
QUEUE
EXAMPLE
THE
MEMORY
LOCATION
POINTED
TO
BY
TIDP
IS
SET
TO
THE
THREAD
ID
OF
THE
NEWLY
CREATED
THREAD
WHEN
RETURNS
SUCCESSFULLY
THE
ATTR
ARGUMENT
IS
USED
TO
CUSTOMIZE
VARIOUS
THREAD
ATTRIBUTES
WE
LL
COVER
THREAD
ATTRIBUTES
IN
SECTION
BUT
FOR
NOW
WE
LL
SET
THIS
TO
NULL
TO
CREATE
A
THREAD
WITH
THE
DEFAULT
ATTRIBUTES
THE
NEWLY
CREATED
THREAD
STARTS
RUNNING
AT
THE
ADDRESS
OF
THE
FUNCTION
THIS
FUNCTION
TAKES
A
SINGLE
ARGUMENT
ARG
WHICH
IS
A
TYPELESS
POINTER
IF
YOU
NEED
TO
PASS
MORE
THAN
ONE
ARGUMENT
TO
THE
FUNCTION
THEN
YOU
NEED
TO
STORE
THEM
IN
A
STRUCTURE
AND
PASS
THE
ADDRESS
OF
THE
STRUCTURE
IN
ARG
WHEN
A
THREAD
IS
CREATED
THERE
IS
NO
GUARANTEE
WHICH
WILL
RUN
FIRST
THE
NEWLY
CREATED
THREAD
OR
THE
CALLING
THREAD
THE
NEWLY
CREATED
THREAD
HAS
ACCESS
TO
THE
PROCESS
ADDRESS
SPACE
AND
INHERITS
THE
CALLING
THREAD
FLOATING
POINT
ENVIRONMENT
AND
SIGNAL
MASK
HOWEVER
THE
SET
OF
PENDING
SIGNALS
FOR
THE
THREAD
IS
CLEARED
NOTE
THAT
THE
PTHREAD
FUNCTIONS
USUALLY
RETURN
AN
ERROR
CODE
WHEN
THEY
FAIL
THEY
DON
T
SET
ERRNO
LIKE
THE
OTHER
POSIX
FUNCTIONS
THE
PER
THREAD
COPY
OF
ERRNO
IS
PROVIDED
ONLY
FOR
COMPATIBILITY
WITH
EXISTING
FUNCTIONS
THAT
USE
IT
WITH
THREADS
IT
IS
CLEANER
TO
RETURN
THE
ERROR
CODE
FROM
THE
FUNCTION
THEREBY
RESTRICTING
THE
SCOPE
OF
THE
ERROR
TO
THE
FUNCTION
THAT
CAUSED
IT
INSTEAD
OF
RELYING
ON
SOME
GLOBAL
STATE
THAT
IS
CHANGED
AS
A
SIDE
EFFECT
OF
THE
FUNCTION
EXAMPLE
ALTHOUGH
THERE
IS
NO
PORTABLE
WAY
TO
PRINT
THE
THREAD
ID
WE
CAN
WRITE
A
SMALL
TEST
PROGRAM
THAT
DOES
TO
GAIN
SOME
INSIGHT
INTO
HOW
THREADS
WORK
THE
PROGRAM
IN
FIGURE
CREATES
ONE
THREAD
AND
PRINTS
THE
PROCESS
AND
THREAD
IDS
OF
THE
NEW
THREAD
AND
THE
INITIAL
THREAD
INCLUDE
APUE
H
INCLUDE
PTHREAD
H
NTID
VOID
PRINTIDS
CONST
CHAR
PID
TID
PID
GETPID
TID
PRINTF
PID
LU
TID
LU
LX
N
UNSIGNED
LONG
PID
UNSIGNED
LONG
TID
UNSIGNED
LONG
TID
VOID
VOID
ARG
PRINTIDS
NEW
THREAD
RETURN
VOID
INT
MAIN
VOID
INT
ERR
ERR
NTID
NULL
NULL
IF
ERR
ERR
CAN
T
CREATE
THREAD
PRINTIDS
MAIN
THREAD
SLEEP
EXIT
FIGURE
PRINTING
THREAD
IDS
THIS
EXAMPLE
HAS
TWO
ODDITIES
WHICH
ARE
NECESSARY
TO
HANDLE
RACES
BETWEEN
THE
MAIN
THREAD
AND
THE
NEW
THREAD
WE
LL
LEARN
BETTER
WAYS
TO
DEAL
WITH
THESE
CONDITIONS
LATER
IN
THIS
CHAPTER
THE
FIRST
IS
THE
NEED
TO
SLEEP
IN
THE
MAIN
THREAD
IF
IT
DOESN
T
SLEEP
THE
MAIN
THREAD
MIGHT
EXIT
THEREBY
TERMINATING
THE
ENTIRE
PROCESS
BEFORE
THE
NEW
THREAD
GETS
A
CHANCE
TO
RUN
THIS
BEHAVIOR
IS
DEPENDENT
ON
THE
OPERATING
SYSTEM
THREADS
IMPLEMENTATION
AND
SCHEDULING
ALGORITHMS
THE
SECOND
ODDITY
IS
THAT
THE
NEW
THREAD
OBTAINS
ITS
THREAD
ID
BY
CALLING
INSTEAD
OF
READING
IT
OUT
OF
SHARED
MEMORY
OR
RECEIVING
IT
AS
AN
ARGUMENT
TO
ITS
THREAD
START
ROUTINE
RECALL
THAT
WILL
RETURN
THE
THREAD
ID
OF
THE
NEWLY
CREATED
THREAD
THROUGH
THE
FIRST
PARAMETER
TIDP
IN
OUR
EXAMPLE
THE
MAIN
THREAD
STORES
THIS
ID
IN
NTID
BUT
THE
NEW
THREAD
CAN
T
SAFELY
USE
IT
IF
THE
NEW
THREAD
RUNS
BEFORE
THE
MAIN
THREAD
RETURNS
FROM
CALLING
THEN
THE
NEW
THREAD
WILL
SEE
THE
UNINITIALIZED
CONTENTS
OF
NTID
INSTEAD
OF
THE
THREAD
ID
RUNNING
THE
PROGRAM
IN
FIGURE
ON
SOLARIS
GIVES
US
A
OUT
MAIN
THREAD
PID
TID
NEW
THREAD
PID
TID
AS
WE
EXPECT
BOTH
THREADS
HAVE
THE
SAME
PROCESS
ID
BUT
DIFFERENT
THREAD
IDS
RUNNING
THE
PROGRAM
IN
FIGURE
ON
FREEBSD
GIVES
US
A
OUT
MAIN
THREAD
PID
TID
NEW
THREAD
PID
TID
AS
WE
EXPECT
BOTH
THREADS
HAVE
THE
SAME
PROCESS
ID
IF
WE
LOOK
AT
THE
THREAD
IDS
AS
DECIMAL
INTEGERS
THE
VALUES
LOOK
STRANGE
BUT
IF
WE
LOOK
AT
THEM
IN
HEXADECIMAL
FORMAT
THEY
MAKE
MORE
SENSE
AS
WE
NOTED
EARLIER
FREEBSD
USES
A
POINTER
TO
THE
THREAD
DATA
STRUCTURE
FOR
ITS
THREAD
ID
WE
WOULD
EXPECT
MAC
OS
X
TO
BE
SIMILAR
TO
FREEBSD
HOWEVER
THE
THREAD
ID
FOR
THE
MAIN
THREAD
IS
FROM
A
DIFFERENT
ADDRESS
RANGE
THAN
THE
THREAD
IDS
FOR
THREADS
CREATED
WITH
A
OUT
MAIN
THREAD
PID
TID
NEW
THREAD
PID
TID
RUNNING
THE
SAME
PROGRAM
ON
LINUX
GIVES
US
A
OUT
MAIN
THREAD
PID
TID
NEW
THREAD
PID
TID
THE
LINUX
THREAD
IDS
LOOK
LIKE
POINTERS
EVEN
THOUGH
THEY
ARE
REPRESENTED
AS
UNSIGNED
LONG
INTEGERS
THE
THREADS
IMPLEMENTATION
CHANGED
BETWEEN
LINUX
AND
LINUX
IN
LINUX
LINUXTHREADS
IMPLEMENTED
EACH
THREAD
WITH
A
SEPARATE
PROCESS
THIS
MADE
IT
DIFFICULT
TO
MATCH
THE
BEHAVIOR
OF
POSIX
THREADS
IN
LINUX
THE
LINUX
KERNEL
AND
THREADS
LIBRARY
WERE
OVERHAULED
TO
USE
A
NEW
THREADS
IMPLEMENTATION
CALLED
THE
NATIVE
POSIX
THREAD
LIBRARY
NPTL
THIS
SUPPORTED
A
MODEL
OF
MULTIPLE
THREADS
WITHIN
A
SINGLE
PROCESS
AND
MADE
IT
EASIER
TO
SUPPORT
POSIX
THREADS
SEMANTICS
THREAD
TERMINATION
IF
ANY
THREAD
WITHIN
A
PROCESS
CALLS
EXIT
OR
THEN
THE
ENTIRE
PROCESS
TERMINATES
SIMILARLY
WHEN
THE
DEFAULT
ACTION
IS
TO
TERMINATE
THE
PROCESS
A
SIGNAL
SENT
TO
A
THREAD
WILL
TERMINATE
THE
ENTIRE
PROCESS
WE
LL
TALK
MORE
ABOUT
THE
INTERACTIONS
BETWEEN
SIGNALS
AND
THREADS
IN
SECTION
A
SINGLE
THREAD
CAN
EXIT
IN
THREE
WAYS
THEREBY
STOPPING
ITS
FLOW
OF
CONTROL
WITHOUT
TERMINATING
THE
ENTIRE
PROCESS
THE
THREAD
CAN
SIMPLY
RETURN
FROM
THE
START
ROUTINE
THE
RETURN
VALUE
IS
THE
THREAD
EXIT
CODE
THE
THREAD
CAN
BE
CANCELED
BY
ANOTHER
THREAD
IN
THE
SAME
PROCESS
THE
THREAD
CAN
CALL
THE
ARGUMENT
IS
A
TYPELESS
POINTER
SIMILAR
TO
THE
SINGLE
ARGUMENT
PASSED
TO
THE
START
ROUTINE
THIS
POINTER
IS
AVAILABLE
TO
OTHER
THREADS
IN
THE
PROCESS
BY
CALLING
THE
FUNCTION
THE
CALLING
THREAD
WILL
BLOCK
UNTIL
THE
SPECIFIED
THREAD
CALLS
RETURNS
FROM
ITS
START
ROUTINE
OR
IS
CANCELED
IF
THE
THREAD
SIMPLY
RETURNED
FROM
ITS
START
ROUTINE
WILL
CONTAIN
THE
RETURN
CODE
IF
THE
THREAD
WAS
CANCELED
THE
MEMORY
LOCATION
SPECIFIED
BY
IS
SET
TO
BY
CALLING
WE
AUTOMATICALLY
PLACE
THE
THREAD
WITH
WHICH
WE
RE
JOINING
IN
THE
DETACHED
STATE
DISCUSSED
SHORTLY
SO
THAT
ITS
RESOURCES
CAN
BE
RECOVERED
IF
THE
THREAD
WAS
ALREADY
IN
THE
DETACHED
STATE
CAN
FAIL
RETURNING
EINVAL
ALTHOUGH
THIS
BEHAVIOR
IS
IMPLEMENTATION
SPECIFIC
IF
WE
RE
NOT
INTERESTED
IN
A
THREAD
RETURN
VALUE
WE
CAN
SET
TO
NULL
IN
THIS
CASE
CALLING
ALLOWS
US
TO
WAIT
FOR
THE
SPECIFIED
THREAD
BUT
DOES
NOT
RETRIEVE
THE
THREAD
TERMINATION
STATUS
EXAMPLE
FIGURE
SHOWS
HOW
TO
FETCH
THE
EXIT
CODE
FROM
A
THREAD
THAT
HAS
TERMINATED
INCLUDE
APUE
H
INCLUDE
PTHREAD
H
VOID
VOID
ARG
PRINTF
THREAD
RETURNING
N
RETURN
VOID
VOID
VOID
ARG
PRINTF
THREAD
EXITING
N
VOID
INT
MAIN
VOID
INT
ERR
VOID
TRET
ERR
NULL
NULL
IF
ERR
ERR
CAN
T
CREATE
THREAD
ERR
NULL
NULL
IF
ERR
ERR
CAN
T
CREATE
THREAD
ERR
TRET
IF
ERR
ERR
CAN
T
JOIN
WITH
THREAD
PRINTF
THREAD
EXIT
CODE
LD
N
LONG
TRET
ERR
TRET
IF
ERR
ERR
CAN
T
JOIN
WITH
THREAD
PRINTF
THREAD
EXIT
CODE
LD
N
LONG
TRET
EXIT
FIGURE
FETCHING
THE
THREAD
EXIT
STATUS
RUNNING
THE
PROGRAM
IN
FIGURE
GIVES
US
A
OUT
THREAD
RETURNING
THREAD
EXITING
THREAD
EXIT
CODE
THREAD
EXIT
CODE
AS
WE
CAN
SEE
WHEN
A
THREAD
EXITS
BY
CALLING
OR
BY
SIMPLY
RETURNING
FROM
THE
START
ROUTINE
THE
EXIT
STATUS
CAN
BE
OBTAINED
BY
ANOTHER
THREAD
BY
CALLING
THE
TYPELESS
POINTER
PASSED
TO
AND
CAN
BE
USED
TO
PASS
MORE
THAN
A
SINGLE
VALUE
THE
POINTER
CAN
BE
USED
TO
PASS
THE
ADDRESS
OF
A
STRUCTURE
CONTAINING
MORE
COMPLEX
INFORMATION
BE
CAREFUL
THAT
THE
MEMORY
USED
FOR
THE
STRUCTURE
IS
STILL
VALID
WHEN
THE
CALLER
HAS
COMPLETED
IF
THE
STRUCTURE
WAS
ALLOCATED
ON
THE
CALLER
STACK
FOR
EXAMPLE
THE
MEMORY
CONTENTS
MIGHT
HAVE
CHANGED
BY
THE
TIME
THE
STRUCTURE
IS
USED
IF
A
THREAD
ALLOCATES
A
STRUCTURE
ON
ITS
STACK
AND
PASSES
A
POINTER
TO
THIS
STRUCTURE
TO
THEN
THE
STACK
MIGHT
BE
DESTROYED
AND
ITS
MEMORY
REUSED
FOR
SOMETHING
ELSE
BY
THE
TIME
THE
CALLER
OF
TRIES
TO
USE
IT
EXAMPLE
THE
PROGRAM
IN
FIGURE
SHOWS
THE
PROBLEM
WITH
USING
AN
AUTOMATIC
VARIABLE
ALLOCATED
ON
THE
STACK
AS
THE
ARGUMENT
TO
INCLUDE
APUE
H
INCLUDE
PTHREAD
H
STRUCT
FOO
INT
A
B
C
D
VOID
PRINTFOO
CONST
CHAR
CONST
STRUCT
FOO
FP
PRINTF
PRINTF
STRUCTURE
AT
LX
N
UNSIGNED
LONG
FP
PRINTF
FOO
A
D
N
FP
A
PRINTF
FOO
B
D
N
FP
B
PRINTF
FOO
C
D
N
FP
C
PRINTF
FOO
D
D
N
FP
D
VOID
VOID
ARG
STRUCT
FOO
FOO
PRINTFOO
THREAD
N
FOO
VOID
FOO
VOID
VOID
ARG
INT
PRINTF
THREAD
ID
IS
LU
N
UNSIGNED
LONG
VOID
MAIN
VOID
INT
ERR
STRUCT
FOO
FP
ERR
NULL
NULL
IF
ERR
ERR
CAN
T
CREATE
THREAD
ERR
VOID
FP
IF
ERR
ERR
CAN
T
JOIN
WITH
THREAD
SLEEP
PRINTF
PARENT
STARTING
SECOND
THREAD
N
ERR
NULL
NULL
IF
ERR
ERR
CAN
T
CREATE
THREAD
SLEEP
PRINTFOO
PARENT
N
FP
EXIT
FIGURE
INCORRECT
USE
OF
ARGUMENT
WHEN
WE
RUN
THIS
PROGRAM
ON
LINUX
WE
GET
A
OUT
THREAD
STRUCTURE
AT
FOO
A
FOO
B
FOO
C
FOO
D
PARENT
STARTING
SECOND
THREAD
THREAD
ID
IS
PARENT
STRUCTURE
AT
FOO
A
FOO
B
FOO
C
FOO
D
OF
COURSE
THE
RESULTS
VARY
DEPENDING
ON
THE
MEMORY
ARCHITECTURE
THE
COMPILER
AND
THE
IMPLEMENTATION
OF
THE
THREADS
LIBRARY
THE
RESULTS
ON
SOLARIS
ARE
SIMILAR
A
OUT
THREAD
STRUCTURE
AT
FOO
A
FOO
B
FOO
C
FOO
D
PARENT
STARTING
SECOND
THREAD
THREAD
ID
IS
PARENT
STRUCTURE
AT
FOO
A
FOO
B
FOO
C
FOO
D
AS
WE
CAN
SEE
THE
CONTENTS
OF
THE
STRUCTURE
ALLOCATED
ON
THE
STACK
OF
THREAD
HAVE
CHANGED
BY
THE
TIME
THE
MAIN
THREAD
CAN
ACCESS
THE
STRUCTURE
NOTE
HOW
THE
STACK
OF
THE
SECOND
THREAD
HAS
OVERWRITTEN
THE
FIRST
THREAD
STACK
TO
SOLVE
THIS
PROBLEM
WE
CAN
EITHER
USE
A
GLOBAL
STRUCTURE
OR
ALLOCATE
THE
STRUCTURE
USING
MALLOC
ON
MAC
OS
X
WE
GET
DIFFERENT
RESULTS
A
OUT
THREAD
STRUCTURE
AT
FOO
A
FOO
B
FOO
C
FOO
D
PARENT
STARTING
SECOND
THREAD
THREAD
ID
IS
PARENT
STRUCTURE
AT
SEGMENTATION
FAULT
CORE
DUMPED
IN
THIS
CASE
THE
MEMORY
IS
NO
LONGER
VALID
WHEN
THE
PARENT
TRIES
TO
ACCESS
THE
STRUCTURE
PASSED
TO
IT
BY
THE
FIRST
THREAD
THAT
EXITED
AND
THE
PARENT
IS
SENT
THE
SIGSEGV
SIGNAL
ON
FREEBSD
THE
MEMORY
HASN
T
BEEN
OVERWRITTEN
BY
THE
TIME
THE
PARENT
ACCESSES
IT
AND
WE
GET
THREAD
STRUCTURE
AT
FOO
A
FOO
B
FOO
C
FOO
D
PARENT
STARTING
SECOND
THREAD
THREAD
ID
IS
PARENT
STRUCTURE
AT
FOO
A
FOO
B
FOO
C
FOO
D
EVEN
THOUGH
THE
MEMORY
IS
STILL
INTACT
AFTER
THE
THREAD
EXITS
WE
CAN
T
DEPEND
ON
THIS
ALWAYS
BEING
THE
CASE
IT
CERTAINLY
ISN
T
WHAT
WE
OBSERVE
ON
THE
OTHER
PLATFORMS
ONE
THREAD
CAN
REQUEST
THAT
ANOTHER
IN
THE
SAME
PROCESS
BE
CANCELED
BY
CALLING
THE
FUNCTION
INCLUDE
PTHREAD
H
INT
TID
RETURNS
IF
OK
ERROR
NUMBER
ON
FAILURE
IN
THE
DEFAULT
CIRCUMSTANCES
WILL
CAUSE
THE
THREAD
SPECIFIED
BY
TID
TO
BEHAVE
AS
IF
IT
HAD
CALLED
WITH
AN
ARGUMENT
OF
HOWEVER
A
THREAD
CAN
ELECT
TO
IGNORE
OR
OTHERWISE
CONTROL
HOW
IT
IS
CANCELED
WE
WILL
DISCUSS
THIS
IN
DETAIL
IN
SECTION
NOTE
THAT
DOESN
T
WAIT
FOR
THE
THREAD
TO
TERMINATE
IT
MERELY
MAKES
THE
REQUEST
A
THREAD
CAN
ARRANGE
FOR
FUNCTIONS
TO
BE
CALLED
WHEN
IT
EXITS
SIMILAR
TO
THE
WAY
THAT
THE
ATEXIT
FUNCTION
SECTION
CAN
BE
USED
BY
A
PROCESS
TO
ARRANGE
THAT
FUNCTIONS
ARE
TO
BE
CALLED
WHEN
THE
PROCESS
EXITS
THE
FUNCTIONS
ARE
KNOWN
AS
THREAD
CLEANUP
HANDLERS
MORE
THAN
ONE
CLEANUP
HANDLER
CAN
BE
ESTABLISHED
FOR
A
THREAD
THE
HANDLERS
ARE
RECORDED
IN
A
STACK
WHICH
MEANS
THAT
THEY
ARE
EXECUTED
IN
THE
REVERSE
ORDER
FROM
THAT
WITH
WHICH
THEY
WERE
REGISTERED
THE
FUNCTION
SCHEDULES
THE
CLEANUP
FUNCTION
RTN
TO
BE
CALLED
WITH
THE
SINGLE
ARGUMENT
ARG
WHEN
THE
THREAD
PERFORMS
ONE
OF
THE
FOLLOWING
ACTIONS
MAKES
A
CALL
TO
RESPONDS
TO
A
CANCELLATION
REQUEST
MAKES
A
CALL
TO
WITH
A
NONZERO
EXECUTE
ARGUMENT
IF
THE
EXECUTE
ARGUMENT
IS
SET
TO
ZERO
THE
CLEANUP
FUNCTION
IS
NOT
CALLED
IN
EITHER
CASE
REMOVES
THE
CLEANUP
HANDLER
ESTABLISHED
BY
THE
LAST
CALL
TO
A
RESTRICTION
WITH
THESE
FUNCTIONS
IS
THAT
BECAUSE
THEY
CAN
BE
IMPLEMENTED
AS
MACROS
THEY
MUST
BE
USED
IN
MATCHED
PAIRS
WITHIN
THE
SAME
SCOPE
IN
A
THREAD
THE
MACRO
DEFINITION
OF
CAN
INCLUDE
A
CHARACTER
IN
WHICH
CASE
THE
MATCHING
CHARACTER
IS
IN
THE
DEFINITION
EXAMPLE
FIGURE
SHOWS
HOW
TO
USE
THREAD
CLEANUP
HANDLERS
ALTHOUGH
THE
EXAMPLE
IS
SOMEWHAT
CONTRIVED
IT
ILLUSTRATES
THE
MECHANICS
INVOLVED
NOTE
THAT
ALTHOUGH
WE
NEVER
INTEND
TO
PASS
ZERO
AS
AN
ARGUMENT
TO
THE
THREAD
START
UP
ROUTINES
WE
STILL
NEED
TO
MATCH
CALLS
TO
WITH
THE
CALLS
TO
OTHERWISE
THE
PROGRAM
MIGHT
NOT
COMPILE
INCLUDE
APUE
H
INCLUDE
PTHREAD
H
VOID
CLEANUP
VOID
ARG
PRINTF
CLEANUP
N
CHAR
ARG
VOID
VOID
ARG
PRINTF
THREAD
START
N
CLEANUP
THREAD
FIRST
HANDLER
CLEANUP
THREAD
SECOND
HANDLER
PRINTF
THREAD
PUSH
COMPLETE
N
IF
ARG
RETURN
VOID
RETURN
VOID
VOID
VOID
ARG
PRINTF
THREAD
START
N
CLEANUP
THREAD
FIRST
HANDLER
CLEANUP
THREAD
SECOND
HANDLER
PRINTF
THREAD
PUSH
COMPLETE
N
IF
ARG
VOID
VOID
INT
MAIN
VOID
INT
ERR
VOID
TRET
ERR
NULL
VOID
IF
ERR
ERR
CAN
T
CREATE
THREAD
ERR
NULL
VOID
IF
ERR
ERR
CAN
T
CREATE
THREAD
ERR
TRET
IF
ERR
ERR
CAN
T
JOIN
WITH
THREAD
PRINTF
THREAD
EXIT
CODE
LD
N
LONG
TRET
ERR
TRET
IF
ERR
ERR
CAN
T
JOIN
WITH
THREAD
PRINTF
THREAD
EXIT
CODE
LD
N
LONG
TRET
EXIT
FIGURE
THREAD
CLEANUP
HANDLER
RUNNING
THE
PROGRAM
IN
FIGURE
ON
LINUX
OR
SOLARIS
GIVES
US
A
OUT
THREAD
START
THREAD
PUSH
COMPLETE
THREAD
START
THREAD
PUSH
COMPLETE
CLEANUP
THREAD
SECOND
HANDLER
CLEANUP
THREAD
FIRST
HANDLER
THREAD
EXIT
CODE
THREAD
EXIT
CODE
FROM
THE
OUTPUT
WE
CAN
SEE
THAT
BOTH
THREADS
START
PROPERLY
AND
EXIT
BUT
THAT
ONLY
THE
SECOND
THREAD
CLEANUP
HANDLERS
ARE
CALLED
THUS
IF
THE
THREAD
TERMINATES
BY
RETURNING
FROM
ITS
START
ROUTINE
ITS
CLEANUP
HANDLERS
ARE
NOT
CALLED
ALTHOUGH
THIS
BEHAVIOR
VARIES
AMONG
IMPLEMENTATIONS
ALSO
NOTE
THAT
THE
CLEANUP
HANDLERS
ARE
CALLED
IN
THE
REVERSE
ORDER
FROM
WHICH
THEY
WERE
INSTALLED
IF
WE
RUN
THE
SAME
PROGRAM
ON
FREEBSD
OR
MAC
OS
X
WE
SEE
THAT
THE
PROGRAM
INCURS
A
SEGMENTATION
VIOLATION
AND
DROPS
CORE
THIS
HAPPENS
BECAUSE
ON
THESE
SYSTEMS
IS
IMPLEMENTED
AS
A
MACRO
THAT
STORES
SOME
CONTEXT
ON
THE
STACK
WHEN
THREAD
RETURNS
IN
BETWEEN
THE
CALL
TO
AND
THE
CALL
TO
THE
STACK
IS
OVERWRITTEN
AND
THESE
PLATFORMS
TRY
TO
USE
THIS
NOW
CORRUPTED
CONTEXT
WHEN
THEY
INVOKE
THE
CLEANUP
HANDLERS
IN
THE
SINGLE
UNIX
SPECIFICATION
RETURNING
WHILE
IN
BETWEEN
A
MATCHED
PAIR
OF
CALLS
TO
AND
RESULTS
IN
UNDEFINED
BEHAVIOR
THE
ONLY
PORTABLE
WAY
TO
RETURN
IN
BETWEEN
THESE
TWO
FUNCTIONS
IS
TO
CALL
BY
NOW
YOU
SHOULD
BEGIN
TO
SEE
SIMILARITIES
BETWEEN
THE
THREAD
FUNCTIONS
AND
THE
PROCESS
FUNCTIONS
FIGURE
SUMMARIZES
THE
SIMILAR
FUNCTIONS
PROCESS
PRIMITIVE
THREAD
PRIMITIVE
DESCRIPTION
FORK
CREATE
A
NEW
FLOW
OF
CONTROL
EXIT
EXIT
FROM
AN
EXISTING
FLOW
OF
CONTROL
WAITPID
GET
EXIT
STATUS
FROM
FLOW
OF
CONTROL
ATEXIT
REGISTER
FUNCTION
TO
BE
CALLED
AT
EXIT
FROM
FLOW
OF
CONTROL
GETPID
GET
ID
FOR
FLOW
OF
CONTROL
ABORT
REQUEST
ABNORMAL
TERMINATION
OF
FLOW
OF
CONTROL
FIGURE
COMPARISON
OF
PROCESS
AND
THREAD
PRIMITIVES
BY
DEFAULT
A
THREAD
TERMINATION
STATUS
IS
RETAINED
UNTIL
WE
CALL
FOR
THAT
THREAD
A
THREAD
UNDERLYING
STORAGE
CAN
BE
RECLAIMED
IMMEDIATELY
ON
TERMINATION
IF
THE
THREAD
HAS
BEEN
DETACHED
AFTER
A
THREAD
IS
DETACHED
WE
CAN
T
USE
THE
FUNCTION
TO
WAIT
FOR
ITS
TERMINATION
STATUS
BECAUSE
CALLING
FOR
A
DETACHED
THREAD
RESULTS
IN
UNDEFINED
BEHAVIOR
WE
CAN
DETACH
A
THREAD
BY
CALLING
INCLUDE
PTHREAD
H
INT
TID
RETURNS
IF
OK
ERROR
NUMBER
ON
FAILURE
AS
WE
WILL
SEE
IN
THE
NEXT
CHAPTER
WE
CAN
CREATE
A
THREAD
THAT
IS
ALREADY
IN
THE
DETACHED
STATE
BY
MODIFYING
THE
THREAD
ATTRIBUTES
WE
PASS
TO
THREAD
SYNCHRONIZATION
WHEN
MULTIPLE
THREADS
OF
CONTROL
SHARE
THE
SAME
MEMORY
WE
NEED
TO
MAKE
SURE
THAT
EACH
THREAD
SEES
A
CONSISTENT
VIEW
OF
ITS
DATA
IF
EACH
THREAD
USES
VARIABLES
THAT
OTHER
THREADS
DON
T
READ
OR
MODIFY
NO
CONSISTENCY
PROBLEMS
WILL
EXIST
SIMILARLY
IF
A
VARIABLE
IS
READ
ONLY
THERE
IS
NO
CONSISTENCY
PROBLEM
WITH
MORE
THAN
ONE
THREAD
READING
ITS
VALUE
AT
THE
SAME
TIME
HOWEVER
WHEN
ONE
THREAD
CAN
MODIFY
A
VARIABLE
THAT
OTHER
THREADS
CAN
READ
OR
MODIFY
WE
NEED
TO
SYNCHRONIZE
THE
THREADS
TO
ENSURE
THAT
THEY
DON
T
USE
AN
INVALID
VALUE
WHEN
ACCESSING
THE
VARIABLE
MEMORY
CONTENTS
WHEN
ONE
THREAD
MODIFIES
A
VARIABLE
OTHER
THREADS
CAN
POTENTIALLY
SEE
INCONSISTENCIES
WHEN
READING
THE
VALUE
OF
THAT
VARIABLE
ON
PROCESSOR
ARCHITECTURES
IN
WHICH
THE
MODIFICATION
TAKES
MORE
THAN
ONE
MEMORY
CYCLE
THIS
CAN
HAPPEN
WHEN
THE
MEMORY
READ
IS
INTERLEAVED
BETWEEN
THE
MEMORY
WRITE
CYCLES
OF
COURSE
THIS
BEHAVIOR
IS
ARCHITECTURE
DEPENDENT
BUT
PORTABLE
PROGRAMS
CAN
T
MAKE
ANY
ASSUMPTIONS
ABOUT
WHAT
TYPE
OF
PROCESSOR
ARCHITECTURE
IS
BEING
USED
FIGURE
SHOWS
A
HYPOTHETICAL
EXAMPLE
OF
TWO
THREADS
READING
AND
WRITING
THE
SAME
VARIABLE
IN
THIS
EXAMPLE
THREAD
A
READS
THE
VARIABLE
AND
THEN
WRITES
A
NEW
VALUE
TO
IT
BUT
THE
WRITE
OPERATION
TAKES
TWO
MEMORY
CYCLES
IF
THREAD
B
READS
THE
SAME
VARIABLE
BETWEEN
THE
TWO
WRITE
CYCLES
IT
WILL
SEE
AN
INCONSISTENT
VALUE
THREAD
A
THREAD
B
TIME
FIGURE
INTERLEAVED
MEMORY
CYCLES
WITH
TWO
THREADS
TO
SOLVE
THIS
PROBLEM
THE
THREADS
HAVE
TO
USE
A
LOCK
THAT
WILL
ALLOW
ONLY
ONE
THREAD
TO
ACCESS
THE
VARIABLE
AT
A
TIME
FIGURE
SHOWS
THIS
SYNCHRONIZATION
IF
IT
WANTS
TO
READ
THE
VARIABLE
THREAD
B
ACQUIRES
A
LOCK
SIMILARLY
WHEN
THREAD
A
UPDATES
THE
VARIABLE
IT
ACQUIRES
THE
SAME
LOCK
THUS
THREAD
B
WILL
BE
UNABLE
TO
READ
THE
VARIABLE
UNTIL
THREAD
A
RELEASES
THE
LOCK
THREAD
A
THREAD
B
TIME
FIGURE
TWO
THREADS
SYNCHRONIZING
MEMORY
ACCESS
WE
ALSO
NEED
TO
SYNCHRONIZE
TWO
OR
MORE
THREADS
THAT
MIGHT
TRY
TO
MODIFY
THE
SAME
VARIABLE
AT
THE
SAME
TIME
CONSIDER
THE
CASE
IN
WHICH
WE
INCREMENT
A
VARIABLE
FIGURE
THE
INCREMENT
OPERATION
IS
USUALLY
BROKEN
DOWN
INTO
THREE
STEPS
READ
THE
MEMORY
LOCATION
INTO
A
REGISTER
INCREMENT
THE
VALUE
IN
THE
REGISTER
WRITE
THE
NEW
VALUE
BACK
TO
THE
MEMORY
LOCATION
IF
TWO
THREADS
TRY
TO
INCREMENT
THE
SAME
VARIABLE
AT
ALMOST
THE
SAME
TIME
WITHOUT
SYNCHRONIZING
WITH
EACH
OTHER
THE
RESULTS
CAN
BE
INCONSISTENT
YOU
END
UP
WITH
A
VALUE
THAT
IS
EITHER
ONE
OR
TWO
GREATER
THAN
BEFORE
DEPENDING
ON
THE
VALUE
OBSERVED
WHEN
THE
SECOND
THREAD
STARTS
ITS
OPERATION
IF
THE
SECOND
THREAD
PERFORMS
STEP
BEFORE
THE
FIRST
THREAD
PERFORMS
STEP
THE
SECOND
THREAD
WILL
READ
THE
SAME
INITIAL
VALUE
AS
THE
FIRST
THREAD
INCREMENT
IT
AND
WRITE
IT
BACK
WITH
NO
NET
EFFECT
IF
THE
MODIFICATION
IS
ATOMIC
THEN
THERE
ISN
T
A
RACE
IN
THE
PREVIOUS
EXAMPLE
IF
THE
INCREMENT
TAKES
ONLY
ONE
MEMORY
CYCLE
THEN
NO
RACE
EXISTS
IF
OUR
DATA
ALWAYS
APPEARS
TO
BE
SEQUENTIALLY
CONSISTENT
THEN
WE
NEED
NO
ADDITIONAL
SYNCHRONIZATION
OUR
OPERATIONS
ARE
SEQUENTIALLY
CONSISTENT
WHEN
MULTIPLE
THREADS
CAN
T
OBSERVE
INCONSISTENCIES
IN
OUR
DATA
IN
MODERN
COMPUTER
SYSTEMS
MEMORY
ACCESSES
TAKE
MULTIPLE
BUS
CYCLES
AND
MULTIPROCESSORS
GENERALLY
INTERLEAVE
BUS
CYCLES
AMONG
MULTIPLE
PROCESSORS
SO
WE
AREN
T
GUARANTEED
THAT
OUR
DATA
IS
SEQUENTIALLY
CONSISTENT
THREAD
A
THREAD
B
CONTENTS
OF
I
TIME
FIGURE
TWO
UNSYNCHRONIZED
THREADS
INCREMENTING
THE
SAME
VARIABLE
IN
A
SEQUENTIALLY
CONSISTENT
ENVIRONMENT
WE
CAN
EXPLAIN
MODIFICATIONS
TO
OUR
DATA
AS
A
SEQUENTIAL
STEP
OF
OPERATIONS
TAKEN
BY
THE
RUNNING
THREADS
WE
CAN
SAY
SUCH
THINGS
AS
THREAD
A
INCREMENTED
THE
VARIABLE
THEN
THREAD
B
INCREMENTED
THE
VARIABLE
SO
ITS
VALUE
IS
TWO
GREATER
THAN
BEFORE
OR
THREAD
B
INCREMENTED
THE
VARIABLE
THEN
THREAD
A
INCREMENTED
THE
VARIABLE
SO
ITS
VALUE
IS
TWO
GREATER
THAN
BEFORE
NO
POSSIBLE
ORDERING
OF
THE
TWO
THREADS
CAN
RESULT
IN
ANY
OTHER
VALUE
OF
THE
VARIABLE
BESIDES
THE
COMPUTER
ARCHITECTURE
RACES
CAN
ARISE
FROM
THE
WAYS
IN
WHICH
OUR
PROGRAMS
USE
VARIABLES
CREATING
PLACES
WHERE
IT
IS
POSSIBLE
TO
VIEW
INCONSISTENCIES
FOR
EXAMPLE
WE
MIGHT
INCREMENT
A
VARIABLE
AND
THEN
MAKE
A
DECISION
BASED
ON
ITS
VALUE
THE
COMBINATION
OF
THE
INCREMENT
STEP
AND
THE
DECISION
MAKING
STEP
ISN
T
ATOMIC
WHICH
OPENS
A
WINDOW
WHERE
INCONSISTENCIES
CAN
ARISE
MUTEXES
WE
CAN
PROTECT
OUR
DATA
AND
ENSURE
ACCESS
BY
ONLY
ONE
THREAD
AT
A
TIME
BY
USING
THE
PTHREADS
MUTUAL
EXCLUSION
INTERFACES
A
MUTEX
IS
BASICALLY
A
LOCK
THAT
WE
SET
LOCK
BEFORE
ACCESSING
A
SHARED
RESOURCE
AND
RELEASE
UNLOCK
WHEN
WE
RE
DONE
WHILE
IT
IS
SET
ANY
OTHER
THREAD
THAT
TRIES
TO
SET
IT
WILL
BLOCK
UNTIL
WE
RELEASE
IT
IF
MORE
THAN
ONE
THREAD
IS
BLOCKED
WHEN
WE
UNLOCK
THE
MUTEX
THEN
ALL
THREADS
BLOCKED
ON
THE
LOCK
WILL
BE
MADE
RUNNABLE
AND
THE
FIRST
ONE
TO
RUN
WILL
BE
ABLE
TO
SET
THE
LOCK
THE
OTHERS
WILL
SEE
THAT
THE
MUTEX
IS
STILL
LOCKED
AND
GO
BACK
TO
WAITING
FOR
IT
TO
BECOME
AVAILABLE
AGAIN
IN
THIS
WAY
ONLY
ONE
THREAD
WILL
PROCEED
AT
A
TIME
THIS
MUTUAL
EXCLUSION
MECHANISM
WORKS
ONLY
IF
WE
DESIGN
OUR
THREADS
TO
FOLLOW
THE
SAME
DATA
ACCESS
RULES
THE
OPERATING
SYSTEM
DOESN
T
SERIALIZE
ACCESS
TO
DATA
FOR
US
IF
WE
ALLOW
ONE
THREAD
TO
ACCESS
A
SHARED
RESOURCE
WITHOUT
FIRST
ACQUIRING
A
LOCK
THEN
INCONSISTENCIES
CAN
OCCUR
EVEN
THOUGH
THE
REST
OF
OUR
THREADS
DO
ACQUIRE
THE
LOCK
BEFORE
ATTEMPTING
TO
ACCESS
THE
SHARED
RESOURCE
A
MUTEX
VARIABLE
IS
REPRESENTED
BY
THE
DATA
TYPE
BEFORE
WE
CAN
USE
A
MUTEX
VARIABLE
WE
MUST
FIRST
INITIALIZE
IT
BY
EITHER
SETTING
IT
TO
THE
CONSTANT
FOR
STATICALLY
ALLOCATED
MUTEXES
ONLY
OR
CALLING
IF
WE
ALLOCATE
THE
MUTEX
DYNAMICALLY
BY
CALLING
MALLOC
FOR
EXAMPLE
THEN
WE
NEED
TO
CALL
BEFORE
FREEING
THE
MEMORY
TO
INITIALIZE
A
MUTEX
WITH
THE
DEFAULT
ATTRIBUTES
WE
SET
ATTR
TO
NULL
WE
WILL
DISCUSS
MUTEX
ATTRIBUTES
IN
SECTION
TO
LOCK
A
MUTEX
WE
CALL
IF
THE
MUTEX
IS
ALREADY
LOCKED
THE
CALLING
THREAD
WILL
BLOCK
UNTIL
THE
MUTEX
IS
UNLOCKED
TO
UNLOCK
A
MUTEX
WE
CALL
IF
A
THREAD
CAN
T
AFFORD
TO
BLOCK
IT
CAN
USE
TO
LOCK
THE
MUTEX
CONDITIONALLY
IF
THE
MUTEX
IS
UNLOCKED
AT
THE
TIME
IS
CALLED
THEN
WILL
LOCK
THE
MUTEX
WITHOUT
BLOCKING
AND
RETURN
OTHERWISE
WILL
FAIL
RETURNING
EBUSY
WITHOUT
LOCKING
THE
MUTEX
EXAMPLE
FIGURE
ILLUSTRATES
A
MUTEX
USED
TO
PROTECT
A
DATA
STRUCTURE
WHEN
MORE
THAN
ONE
THREAD
NEEDS
TO
ACCESS
A
DYNAMICALLY
ALLOCATED
OBJECT
WE
CAN
EMBED
A
REFERENCE
COUNT
IN
THE
OBJECT
TO
ENSURE
THAT
WE
DON
T
FREE
ITS
MEMORY
BEFORE
ALL
THREADS
ARE
DONE
USING
IT
INCLUDE
STDLIB
H
INCLUDE
PTHREAD
H
STRUCT
FOO
INT
INT
MORE
STUFF
HERE
STRUCT
FOO
INT
ID
ALLOCATE
THE
OBJECT
STRUCT
FOO
FP
IF
FP
MALLOC
SIZEOF
STRUCT
FOO
NULL
FP
FP
ID
IF
FP
NULL
FREE
FP
RETURN
NULL
CONTINUE
INITIALIZATION
RETURN
FP
VOID
STRUCT
FOO
FP
ADD
A
REFERENCE
TO
THE
OBJECT
FP
FP
FP
VOID
STRUCT
FOO
FP
RELEASE
A
REFERENCE
TO
THE
OBJECT
FP
IF
FP
LAST
REFERENCE
FP
FP
FREE
FP
ELSE
FP
FIGURE
USING
A
MUTEX
TO
PROTECT
A
DATA
STRUCTURE
WE
LOCK
THE
MUTEX
BEFORE
INCREMENTING
THE
REFERENCE
COUNT
DECREMENTING
THE
REFERENCE
COUNT
AND
CHECKING
WHETHER
THE
REFERENCE
COUNT
REACHES
ZERO
NO
LOCKING
IS
NECESSARY
WHEN
WE
INITIALIZE
THE
REFERENCE
COUNT
TO
IN
THE
FUNCTION
BECAUSE
THE
ALLOCATING
THREAD
IS
THE
ONLY
REFERENCE
TO
IT
SO
FAR
IF
WE
WERE
TO
PLACE
THE
STRUCTURE
ON
A
LIST
AT
THIS
POINT
IT
COULD
BE
FOUND
BY
OTHER
THREADS
SO
WE
WOULD
NEED
TO
LOCK
IT
FIRST
BEFORE
USING
THE
OBJECT
THREADS
ARE
EXPECTED
TO
ADD
A
REFERENCE
TO
IT
BY
CALLING
WHEN
THEY
ARE
DONE
THEY
MUST
CALL
TO
RELEASE
THE
REFERENCE
WHEN
THE
LAST
REFERENCE
IS
RELEASED
THE
OBJECT
MEMORY
IS
FREED
IN
THIS
EXAMPLE
WE
HAVE
IGNORED
HOW
THREADS
FIND
AN
OBJECT
BEFORE
CALLING
EVEN
THOUGH
THE
REFERENCE
COUNT
IS
ZERO
IT
WOULD
BE
A
MISTAKE
FOR
TO
FREE
THE
OBJECT
MEMORY
IF
ANOTHER
THREAD
IS
BLOCKED
ON
THE
MUTEX
IN
A
CALL
TO
WE
CAN
AVOID
THIS
PROBLEM
BY
ENSURING
THAT
THE
OBJECT
CAN
T
BE
FOUND
BEFORE
FREEING
ITS
MEMORY
WE
LL
SEE
HOW
TO
DO
THIS
IN
THE
EXAMPLES
THAT
FOLLOW
DEADLOCK
AVOIDANCE
A
THREAD
WILL
DEADLOCK
ITSELF
IF
IT
TRIES
TO
LOCK
THE
SAME
MUTEX
TWICE
BUT
THERE
ARE
LESS
OBVIOUS
WAYS
TO
CREATE
DEADLOCKS
WITH
MUTEXES
FOR
EXAMPLE
WHEN
WE
USE
MORE
THAN
ONE
MUTEX
IN
OUR
PROGRAMS
A
DEADLOCK
CAN
OCCUR
IF
WE
ALLOW
ONE
THREAD
TO
HOLD
A
MUTEX
AND
BLOCK
WHILE
TRYING
TO
LOCK
A
SECOND
MUTEX
AT
THE
SAME
TIME
THAT
ANOTHER
THREAD
HOLDING
THE
SECOND
MUTEX
TRIES
TO
LOCK
THE
FIRST
MUTEX
NEITHER
THREAD
CAN
PROCEED
BECAUSE
EACH
NEEDS
A
RESOURCE
THAT
IS
HELD
BY
THE
OTHER
SO
WE
HAVE
A
DEADLOCK
DEADLOCKS
CAN
BE
AVOIDED
BY
CAREFULLY
CONTROLLING
THE
ORDER
IN
WHICH
MUTEXES
ARE
LOCKED
FOR
EXAMPLE
ASSUME
THAT
YOU
HAVE
TWO
MUTEXES
A
AND
B
THAT
YOU
NEED
TO
LOCK
AT
THE
SAME
TIME
IF
ALL
THREADS
ALWAYS
LOCK
MUTEX
A
BEFORE
MUTEX
B
NO
DEADLOCK
CAN
OCCUR
FROM
THE
USE
OF
THE
TWO
MUTEXES
BUT
YOU
CAN
STILL
DEADLOCK
ON
OTHER
RESOURCES
SIMILARLY
IF
ALL
THREADS
ALWAYS
LOCK
MUTEX
B
BEFORE
MUTEX
A
NO
DEADLOCK
WILL
OCCUR
YOU
LL
HAVE
THE
POTENTIAL
FOR
A
DEADLOCK
ONLY
WHEN
ONE
THREAD
ATTEMPTS
TO
LOCK
THE
MUTEXES
IN
THE
OPPOSITE
ORDER
FROM
ANOTHER
THREAD
SOMETIMES
AN
APPLICATION
ARCHITECTURE
MAKES
IT
DIFFICULT
TO
APPLY
A
LOCK
ORDERING
IF
ENOUGH
LOCKS
AND
DATA
STRUCTURES
ARE
INVOLVED
THAT
THE
FUNCTIONS
YOU
HAVE
AVAILABLE
CAN
T
BE
MOLDED
TO
FIT
A
SIMPLE
HIERARCHY
THEN
YOU
LL
HAVE
TO
TRY
SOME
OTHER
APPROACH
IN
THIS
CASE
YOU
MIGHT
BE
ABLE
TO
RELEASE
YOUR
LOCKS
AND
TRY
AGAIN
AT
A
LATER
TIME
YOU
CAN
USE
THE
INTERFACE
TO
AVOID
DEADLOCKING
IN
THIS
CASE
IF
YOU
ARE
ALREADY
HOLDING
LOCKS
AND
IS
SUCCESSFUL
THEN
YOU
CAN
PROCEED
IF
IT
CAN
T
ACQUIRE
THE
LOCK
HOWEVER
YOU
CAN
RELEASE
THE
LOCKS
YOU
ALREADY
HOLD
CLEAN
UP
AND
TRY
AGAIN
LATER
EXAMPLE
IN
THIS
EXAMPLE
WE
UPDATE
FIGURE
TO
SHOW
THE
USE
OF
TWO
MUTEXES
WE
AVOID
DEADLOCKS
BY
ENSURING
THAT
WHEN
WE
NEED
TO
ACQUIRE
TWO
MUTEXES
AT
THE
SAME
TIME
WE
ALWAYS
LOCK
THEM
IN
THE
SAME
ORDER
THE
SECOND
MUTEX
PROTECTS
A
HASH
LIST
THAT
WE
USE
TO
KEEP
TRACK
OF
THE
FOO
DATA
STRUCTURES
THUS
THE
HASHLOCK
MUTEX
PROTECTS
BOTH
THE
FH
HASH
TABLE
AND
THE
HASH
LINK
FIELD
IN
THE
FOO
STRUCTURE
THE
MUTEX
IN
THE
FOO
STRUCTURE
PROTECTS
ACCESS
TO
THE
REMAINDER
OF
THE
FOO
STRUCTURE
FIELDS
INCLUDE
STDLIB
H
INCLUDE
PTHREAD
H
DEFINE
NHASH
DEFINE
HASH
ID
UNSIGNED
LONG
ID
NHASH
STRUCT
FOO
FH
NHASH
HASHLOCK
STRUCT
FOO
INT
INT
STRUCT
FOO
PROTECTED
BY
HASHLOCK
MORE
STUFF
HERE
STRUCT
FOO
INT
ID
ALLOCATE
THE
OBJECT
STRUCT
FOO
FP
INT
IDX
IF
FP
MALLOC
SIZEOF
STRUCT
FOO
NULL
FP
FP
ID
IF
FP
NULL
FREE
FP
RETURN
NULL
IDX
HASH
ID
HASHLOCK
FP
FH
IDX
FH
IDX
FP
FP
HASHLOCK
CONTINUE
INITIALIZATION
FP
RETURN
FP
VOID
STRUCT
FOO
FP
ADD
A
REFERENCE
TO
THE
OBJECT
FP
FP
FP
STRUCT
FOO
INT
ID
FIND
AN
EXISTING
OBJECT
STRUCT
FOO
FP
HASHLOCK
FOR
FP
FH
HASH
ID
FP
NULL
FP
FP
IF
FP
ID
FP
BREAK
HASHLOCK
RETURN
FP
VOID
STRUCT
FOO
FP
RELEASE
A
REFERENCE
TO
THE
OBJECT
STRUCT
FOO
TFP
INT
IDX
FP
IF
FP
LAST
REFERENCE
FP
HASHLOCK
FP
NEED
TO
RECHECK
THE
CONDITION
IF
FP
FP
FP
HASHLOCK
RETURN
REMOVE
FROM
LIST
IDX
HASH
FP
TFP
FH
IDX
IF
TFP
FP
FH
IDX
FP
ELSE
WHILE
TFP
FP
TFP
TFP
TFP
FP
HASHLOCK
FP
FP
FREE
FP
ELSE
FP
FP
FIGURE
USING
TWO
MUTEXES
COMPARING
FIGURE
WITH
FIGURE
WE
SEE
THAT
OUR
ALLOCATION
FUNCTION
NOW
LOCKS
THE
HASH
LIST
LOCK
ADDS
THE
NEW
STRUCTURE
TO
A
HASH
BUCKET
AND
BEFORE
UNLOCKING
THE
HASH
LIST
LOCK
LOCKS
THE
MUTEX
IN
THE
NEW
STRUCTURE
SINCE
THE
NEW
STRUCTURE
IS
PLACED
ON
A
GLOBAL
LIST
OTHER
THREADS
CAN
FIND
IT
SO
WE
NEED
TO
BLOCK
THEM
IF
THEY
TRY
TO
ACCESS
THE
NEW
STRUCTURE
UNTIL
WE
ARE
DONE
INITIALIZING
IT
THE
FUNCTION
LOCKS
THE
HASH
LIST
LOCK
AND
SEARCHES
FOR
THE
REQUESTED
STRUCTURE
IF
IT
IS
FOUND
WE
INCREASE
THE
REFERENCE
COUNT
AND
RETURN
A
POINTER
TO
THE
STRUCTURE
NOTE
THAT
WE
HONOR
THE
LOCK
ORDERING
BY
LOCKING
THE
HASH
LIST
LOCK
IN
BEFORE
LOCKS
THE
FOO
STRUCTURE
MUTEX
NOW
WITH
TWO
LOCKS
THE
FUNCTION
IS
MORE
COMPLICATED
IF
THIS
IS
THE
LAST
REFERENCE
WE
NEED
TO
UNLOCK
THE
STRUCTURE
MUTEX
SO
THAT
WE
CAN
ACQUIRE
THE
HASH
LIST
LOCK
SINCE
WE
LL
NEED
TO
REMOVE
THE
STRUCTURE
FROM
THE
HASH
LIST
THEN
WE
REACQUIRE
THE
STRUCTURE
MUTEX
BECAUSE
WE
COULD
HAVE
BLOCKED
SINCE
THE
LAST
TIME
WE
HELD
THE
STRUCTURE
MUTEX
WE
NEED
TO
RECHECK
THE
CONDITION
TO
SEE
WHETHER
WE
STILL
NEED
TO
FREE
THE
STRUCTURE
IF
ANOTHER
THREAD
FOUND
THE
STRUCTURE
AND
ADDED
A
REFERENCE
TO
IT
WHILE
WE
BLOCKED
TO
HONOR
THE
LOCK
ORDERING
WE
SIMPLY
NEED
TO
DECREMENT
THE
REFERENCE
COUNT
UNLOCK
EVERYTHING
AND
RETURN
THIS
LOCKING
APPROACH
IS
COMPLEX
SO
WE
NEED
TO
REVISIT
OUR
DESIGN
WE
CAN
SIMPLIFY
THINGS
CONSIDERABLY
BY
USING
THE
HASH
LIST
LOCK
TO
PROTECT
THE
STRUCTURE
REFERENCE
COUNT
TOO
THE
STRUCTURE
MUTEX
CAN
BE
USED
TO
PROTECT
EVERYTHING
ELSE
IN
THE
FOO
STRUCTURE
FIGURE
REFLECTS
THIS
CHANGE
INCLUDE
STDLIB
H
INCLUDE
PTHREAD
H
DEFINE
NHASH
DEFINE
HASH
ID
UNSIGNED
LONG
ID
NHASH
STRUCT
FOO
FH
NHASH
HASHLOCK
STRUCT
FOO
INT
PROTECTED
BY
HASHLOCK
INT
STRUCT
FOO
PROTECTED
BY
HASHLOCK
MORE
STUFF
HERE
STRUCT
FOO
INT
ID
ALLOCATE
THE
OBJECT
STRUCT
FOO
FP
INT
IDX
IF
FP
MALLOC
SIZEOF
STRUCT
FOO
NULL
FP
FP
ID
IF
FP
NULL
FREE
FP
RETURN
NULL
IDX
HASH
ID
HASHLOCK
FP
FH
IDX
FH
IDX
FP
FP
HASHLOCK
CONTINUE
INITIALIZATION
FP
RETURN
FP
VOID
STRUCT
FOO
FP
ADD
A
REFERENCE
TO
THE
OBJECT
HASHLOCK
FP
HASHLOCK
STRUCT
FOO
INT
ID
FIND
AN
EXISTING
OBJECT
STRUCT
FOO
FP
HASHLOCK
FOR
FP
FH
HASH
ID
FP
NULL
FP
FP
IF
FP
ID
FP
BREAK
HASHLOCK
RETURN
FP
VOID
STRUCT
FOO
FP
RELEASE
A
REFERENCE
TO
THE
OBJECT
STRUCT
FOO
TFP
INT
IDX
HASHLOCK
IF
FP
LAST
REFERENCE
REMOVE
FROM
LIST
IDX
HASH
FP
TFP
FH
IDX
IF
TFP
FP
FH
IDX
FP
ELSE
WHILE
TFP
FP
TFP
TFP
TFP
FP
HASHLOCK
FP
FREE
FP
ELSE
HASHLOCK
FIGURE
SIMPLIFIED
LOCKING
NOTE
HOW
MUCH
SIMPLER
THE
PROGRAM
IN
FIGURE
IS
COMPARED
TO
THE
PROGRAM
IN
FIGURE
THE
LOCK
ORDERING
ISSUES
SURROUNDING
THE
HASH
LIST
AND
THE
REFERENCE
COUNT
GO
AWAY
WHEN
WE
USE
THE
SAME
LOCK
FOR
BOTH
PURPOSES
MULTITHREADED
SOFTWARE
DESIGN
INVOLVES
THESE
TYPES
OF
TRADE
OFFS
IF
YOUR
LOCKING
GRANULARITY
IS
TOO
COARSE
YOU
END
UP
WITH
TOO
MANY
THREADS
BLOCKING
BEHIND
THE
SAME
LOCKS
WITH
LITTLE
IMPROVEMENT
POSSIBLE
FROM
CONCURRENCY
IF
YOUR
LOCKING
GRANULARITY
IS
TOO
FINE
THEN
YOU
SUFFER
BAD
PERFORMANCE
FROM
EXCESS
LOCKING
OVERHEAD
AND
YOU
END
UP
WITH
COMPLEX
CODE
AS
A
PROGRAMMER
YOU
NEED
TO
FIND
THE
CORRECT
BALANCE
BETWEEN
CODE
COMPLEXITY
AND
PERFORMANCE
WHILE
STILL
SATISFYING
YOUR
LOCKING
REQUIREMENTS
FUNCTION
ONE
ADDITIONAL
MUTEX
PRIMITIVE
ALLOWS
US
TO
BOUND
THE
TIME
THAT
A
THREAD
BLOCKS
WHEN
A
MUTEX
IT
IS
TRYING
TO
ACQUIRE
IS
ALREADY
LOCKED
THE
FUNCTION
IS
EQUIVALENT
TO
BUT
IF
THE
TIMEOUT
VALUE
IS
REACHED
WILL
RETURN
THE
ERROR
CODE
ETIMEDOUT
WITHOUT
LOCKING
THE
MUTEX
THE
TIMEOUT
SPECIFIES
HOW
LONG
WE
ARE
WILLING
TO
WAIT
IN
TERMS
OF
ABSOLUTE
TIME
AS
OPPOSED
TO
RELATIVE
TIME
WE
SPECIFY
THAT
WE
ARE
WILLING
TO
BLOCK
UNTIL
TIME
X
INSTEAD
OF
SAYING
THAT
WE
ARE
WILLING
TO
BLOCK
FOR
Y
SECONDS
THE
TIMEOUT
IS
REPRESENTED
BY
THE
TIMESPEC
STRUCTURE
WHICH
DESCRIBES
TIME
IN
TERMS
OF
SECONDS
AND
NANOSECONDS
EXAMPLE
IN
FIGURE
WE
SEE
HOW
TO
USE
TO
AVOID
BLOCKING
INDEFINITELY
INCLUDE
APUE
H
INCLUDE
PTHREAD
H
INT
MAIN
VOID
INT
ERR
STRUCT
TIMESPEC
TOUT
STRUCT
TM
TMP
CHAR
BUF
LOCK
LOCK
PRINTF
MUTEX
IS
LOCKED
N
TOUT
TMP
LOCALTIME
TOUT
STRFTIME
BUF
SIZEOF
BUF
R
TMP
PRINTF
CURRENT
TIME
IS
N
BUF
TOUT
SECONDS
FROM
NOW
CAUTION
THIS
COULD
LEAD
TO
DEADLOCK
ERR
LOCK
TOUT
TOUT
TMP
LOCALTIME
TOUT
STRFTIME
BUF
SIZEOF
BUF
R
TMP
PRINTF
THE
TIME
IS
NOW
N
BUF
IF
ERR
PRINTF
MUTEX
LOCKED
AGAIN
N
ELSE
PRINTF
CAN
T
LOCK
MUTEX
AGAIN
N
STRERROR
ERR
EXIT
FIGURE
USING
HERE
IS
THE
OUTPUT
FROM
THE
PROGRAM
IN
FIGURE
A
OUT
MUTEX
IS
LOCKED
CURRENT
TIME
IS
AM
THE
TIME
IS
NOW
AM
CAN
T
LOCK
MUTEX
AGAIN
CONNECTION
TIMED
OUT
THIS
PROGRAM
DELIBERATELY
LOCKS
A
MUTEX
IT
ALREADY
OWNS
TO
DEMONSTRATE
HOW
WORKS
THIS
STRATEGY
IS
NOT
RECOMMENDED
IN
PRACTICE
BECAUSE
IT
CAN
LEAD
TO
DEADLOCK
NOTE
THAT
THE
TIME
BLOCKED
CAN
VARY
FOR
SEVERAL
REASONS
THE
START
TIME
COULD
HAVE
BEEN
IN
THE
MIDDLE
OF
A
SECOND
THE
RESOLUTION
OF
THE
SYSTEM
CLOCK
MIGHT
NOT
BE
FINE
ENOUGH
TO
SUPPORT
THE
RESOLUTION
OF
OUR
TIMEOUT
OR
SCHEDULING
DELAYS
COULD
PROLONG
THE
AMOUNT
OF
TIME
UNTIL
THE
PROGRAM
CONTINUES
EXECUTION
MAC
OS
X
DOESN
T
SUPPORT
YET
BUT
FREEBSD
LINUX
AND
SOLARIS
DO
SUPPORT
IT
ALTHOUGH
SOLARIS
STILL
BUNDLES
IT
IN
THE
REAL
TIME
LIBRARY
LIBRT
SOLARIS
ALSO
PROVIDES
AN
ALTERNATIVE
FUNCTION
THAT
USES
A
RELATIVE
TIMEOUT
READER
WRITER
LOCKS
READER
WRITER
LOCKS
ARE
SIMILAR
TO
MUTEXES
EXCEPT
THAT
THEY
ALLOW
FOR
HIGHER
DEGREES
OF
PARALLELISM
WITH
A
MUTEX
THE
STATE
IS
EITHER
LOCKED
OR
UNLOCKED
AND
ONLY
ONE
THREAD
CAN
LOCK
IT
AT
A
TIME
THREE
STATES
ARE
POSSIBLE
WITH
A
READER
WRITER
LOCK
LOCKED
IN
READ
MODE
LOCKED
IN
WRITE
MODE
AND
UNLOCKED
ONLY
ONE
THREAD
AT
A
TIME
CAN
HOLD
A
READER
WRITER
LOCK
IN
WRITE
MODE
BUT
MULTIPLE
THREADS
CAN
HOLD
A
READER
WRITER
LOCK
IN
READ
MODE
AT
THE
SAME
TIME
WHEN
A
READER
WRITER
LOCK
IS
WRITE
LOCKED
ALL
THREADS
ATTEMPTING
TO
LOCK
IT
BLOCK
UNTIL
IT
IS
UNLOCKED
WHEN
A
READER
WRITER
LOCK
IS
READ
LOCKED
ALL
THREADS
ATTEMPTING
TO
LOCK
IT
IN
READ
MODE
ARE
GIVEN
ACCESS
BUT
ANY
THREADS
ATTEMPTING
TO
LOCK
IT
IN
WRITE
MODE
BLOCK
UNTIL
ALL
THE
THREADS
HAVE
RELEASED
THEIR
READ
LOCKS
ALTHOUGH
IMPLEMENTATIONS
VARY
READER
WRITER
LOCKS
USUALLY
BLOCK
ADDITIONAL
READERS
IF
A
LOCK
IS
ALREADY
HELD
IN
READ
MODE
AND
A
THREAD
IS
BLOCKED
TRYING
TO
ACQUIRE
THE
LOCK
IN
WRITE
MODE
THIS
PREVENTS
A
CONSTANT
STREAM
OF
READERS
FROM
STARVING
WAITING
WRITERS
READER
WRITER
LOCKS
ARE
WELL
SUITED
FOR
SITUATIONS
IN
WHICH
DATA
STRUCTURES
ARE
READ
MORE
OFTEN
THAN
THEY
ARE
MODIFIED
WHEN
A
READER
WRITER
LOCK
IS
HELD
IN
WRITE
MODE
THE
DATA
STRUCTURE
IT
PROTECTS
CAN
BE
MODIFIED
SAFELY
SINCE
ONLY
ONE
THREAD
AT
A
TIME
CAN
HOLD
THE
LOCK
IN
WRITE
MODE
WHEN
THE
READER
WRITER
LOCK
IS
HELD
IN
READ
MODE
THE
DATA
STRUCTURE
IT
PROTECTS
CAN
BE
READ
BY
MULTIPLE
THREADS
AS
LONG
AS
THE
THREADS
FIRST
ACQUIRE
THE
LOCK
IN
READ
MODE
READER
WRITER
LOCKS
ARE
ALSO
CALLED
SHARED
EXCLUSIVE
LOCKS
WHEN
A
READER
WRITER
LOCK
IS
READ
LOCKED
IT
IS
SAID
TO
BE
LOCKED
IN
SHARED
MODE
WHEN
IT
IS
WRITE
LOCKED
IT
IS
SAID
TO
BE
LOCKED
IN
EXCLUSIVE
MODE
AS
WITH
MUTEXES
READER
WRITER
LOCKS
MUST
BE
INITIALIZED
BEFORE
USE
AND
DESTROYED
BEFORE
FREEING
THEIR
UNDERLYING
MEMORY
A
READER
WRITER
LOCK
IS
INITIALIZED
BY
CALLING
WE
CAN
PASS
A
NULL
POINTER
FOR
ATTR
IF
WE
WANT
THE
READER
WRITER
LOCK
TO
HAVE
THE
DEFAULT
ATTRIBUTES
WE
DISCUSS
READER
WRITER
LOCK
ATTRIBUTES
IN
SECTION
THE
SINGLE
UNIX
SPECIFICATION
DEFINES
THE
CONSTANT
IN
THE
XSI
OPTION
IT
CAN
BE
USED
TO
INITIALIZE
A
STATICALLY
ALLOCATED
READER
WRITER
LOCK
WHEN
THE
DEFAULT
ATTRIBUTES
ARE
SUFFICIENT
BEFORE
FREEING
THE
MEMORY
BACKING
A
READER
WRITER
LOCK
WE
NEED
TO
CALL
TO
CLEAN
IT
UP
IF
ALLOCATED
ANY
RESOURCES
FOR
THE
READER
WRITER
LOCK
FREES
THOSE
RESOURCES
IF
WE
FREE
THE
MEMORY
BACKING
A
READER
WRITER
LOCK
WITHOUT
FIRST
CALLING
ANY
RESOURCES
ASSIGNED
TO
THE
LOCK
WILL
BE
LOST
TO
LOCK
A
READER
WRITER
LOCK
IN
READ
MODE
WE
CALL
TO
WRITE
LOCK
A
READER
WRITER
LOCK
WE
CALL
REGARDLESS
OF
HOW
WE
LOCK
A
READER
WRITER
LOCK
WE
CAN
UNLOCK
IT
BY
CALLING
IMPLEMENTATIONS
MIGHT
PLACE
A
LIMIT
ON
THE
NUMBER
OF
TIMES
A
READER
WRITER
LOCK
CAN
BE
LOCKED
IN
SHARED
MODE
SO
WE
NEED
TO
CHECK
THE
RETURN
VALUE
OF
EVEN
THOUGH
AND
HAVE
ERROR
RETURNS
AND
TECHNICALLY
WE
SHOULD
ALWAYS
CHECK
FOR
ERRORS
WHEN
WE
CALL
FUNCTIONS
THAT
CAN
POTENTIALLY
FAIL
WE
DON
T
NEED
TO
CHECK
THEM
IF
WE
DESIGN
OUR
LOCKING
PROPERLY
THE
ONLY
ERROR
RETURNS
DEFINED
ARE
WHEN
WE
USE
THEM
IMPROPERLY
SUCH
AS
WITH
AN
UNINITIALIZED
LOCK
OR
WHEN
WE
MIGHT
DEADLOCK
BY
ATTEMPTING
TO
ACQUIRE
A
LOCK
WE
ALREADY
OWN
HOWEVER
BE
AWARE
THAT
SPECIFIC
IMPLEMENTATIONS
MIGHT
DEFINE
ADDITIONAL
ERROR
RETURNS
THE
SINGLE
UNIX
SPECIFICATION
ALSO
DEFINES
CONDITIONAL
VERSIONS
OF
THE
READER
WRITER
LOCKING
PRIMITIVES
WHEN
THE
LOCK
CAN
BE
ACQUIRED
THESE
FUNCTIONS
RETURN
OTHERWISE
THEY
RETURN
THE
ERROR
EBUSY
THESE
FUNCTIONS
CAN
BE
USED
TO
AVOID
DEADLOCKS
IN
SITUATIONS
WHERE
CONFORMING
TO
A
LOCK
HIERARCHY
IS
DIFFICULT
AS
WE
DISCUSSED
PREVIOUSLY
EXAMPLE
THE
PROGRAM
IN
FIGURE
ILLUSTRATES
THE
USE
OF
READER
WRITER
LOCKS
A
QUEUE
OF
JOB
REQUESTS
IS
PROTECTED
BY
A
SINGLE
READER
WRITER
LOCK
THIS
EXAMPLE
SHOWS
A
POSSIBLE
IMPLEMENTATION
OF
FIGURE
WHEREBY
MULTIPLE
WORKER
THREADS
OBTAIN
JOBS
ASSIGNED
TO
THEM
BY
A
SINGLE
MASTER
THREAD
INCLUDE
STDLIB
H
INCLUDE
PTHREAD
H
STRUCT
JOB
STRUCT
JOB
STRUCT
JOB
TELLS
WHICH
THREAD
HANDLES
THIS
JOB
MORE
STUFF
HERE
STRUCT
QUEUE
STRUCT
JOB
STRUCT
JOB
INITIALIZE
A
QUEUE
INT
STRUCT
QUEUE
QP
INT
ERR
QP
NULL
QP
NULL
ERR
QP
NULL
IF
ERR
RETURN
ERR
CONTINUE
INITIALIZATION
RETURN
INSERT
A
JOB
AT
THE
HEAD
OF
THE
QUEUE
VOID
STRUCT
QUEUE
QP
STRUCT
JOB
JP
QP
JP
QP
JP
NULL
IF
QP
NULL
QP
JP
ELSE
QP
JP
LIST
WAS
EMPTY
QP
JP
QP
APPEND
A
JOB
ON
THE
TAIL
OF
THE
QUEUE
VOID
STRUCT
QUEUE
QP
STRUCT
JOB
JP
QP
JP
NULL
JP
QP
IF
QP
NULL
QP
JP
ELSE
QP
JP
LIST
WAS
EMPTY
QP
JP
QP
REMOVE
THE
GIVEN
JOB
FROM
A
QUEUE
VOID
STRUCT
QUEUE
QP
STRUCT
JOB
JP
QP
IF
JP
QP
QP
JP
IF
QP
JP
QP
NULL
ELSE
JP
JP
ELSE
IF
JP
QP
QP
JP
JP
JP
ELSE
JP
JP
JP
JP
QP
FIND
A
JOB
FOR
THE
GIVEN
THREAD
ID
STRUCT
JOB
STRUCT
QUEUE
QP
ID
STRUCT
JOB
JP
IF
QP
RETURN
NULL
FOR
JP
QP
JP
NULL
JP
JP
IF
JP
ID
BREAK
QP
RETURN
JP
FIGURE
USING
READER
WRITER
LOCKS
IN
THIS
EXAMPLE
WE
LOCK
THE
QUEUE
READER
WRITER
LOCK
IN
WRITE
MODE
WHENEVER
WE
NEED
TO
ADD
A
JOB
TO
THE
QUEUE
OR
REMOVE
A
JOB
FROM
THE
QUEUE
WHENEVER
WE
SEARCH
THE
QUEUE
WE
GRAB
THE
LOCK
IN
READ
MODE
ALLOWING
ALL
THE
WORKER
THREADS
TO
SEARCH
THE
QUEUE
CONCURRENTLY
USING
A
READER
WRITER
LOCK
WILL
IMPROVE
PERFORMANCE
IN
THIS
CASE
ONLY
IF
THREADS
SEARCH
THE
QUEUE
MUCH
MORE
FREQUENTLY
THAN
THEY
ADD
OR
REMOVE
JOBS
THE
WORKER
THREADS
TAKE
ONLY
THOSE
JOBS
THAT
MATCH
THEIR
THREAD
ID
OFF
THE
QUEUE
SINCE
THE
JOB
STRUCTURES
ARE
USED
ONLY
BY
ONE
THREAD
AT
A
TIME
THEY
DON
T
NEED
ANY
EXTRA
LOCKING
READER
WRITER
LOCKING
WITH
TIMEOUTS
JUST
AS
WITH
MUTEXES
THE
SINGLE
UNIX
SPECIFICATION
PROVIDES
FUNCTIONS
TO
LOCK
READER
WRITER
LOCKS
WITH
A
TIMEOUT
TO
GIVE
APPLICATIONS
A
WAY
TO
AVOID
BLOCKING
INDEFINITELY
WHILE
TRYING
TO
ACQUIRE
A
READER
WRITER
LOCK
THESE
FUNCTIONS
ARE
AND
THESE
FUNCTIONS
BEHAVE
LIKE
THEIR
UNTIMED
COUNTERPARTS
THE
TSPTR
ARGUMENT
POINTS
TO
A
TIMESPEC
STRUCTURE
SPECIFYING
THE
TIME
AT
WHICH
THE
THREAD
SHOULD
STOP
BLOCKING
IF
THEY
CAN
T
ACQUIRE
THE
LOCK
THESE
FUNCTIONS
RETURN
THE
ETIMEDOUT
ERROR
WHEN
THE
TIMEOUT
EXPIRES
LIKE
THE
FUNCTION
THE
TIMEOUT
SPECIFIES
AN
ABSOLUTE
TIME
NOT
A
RELATIVE
ONE
CONDITION
VARIABLES
CONDITION
VARIABLES
ARE
ANOTHER
SYNCHRONIZATION
MECHANISM
AVAILABLE
TO
THREADS
THESE
SYNCHRONIZATION
OBJECTS
PROVIDE
A
PLACE
FOR
THREADS
TO
RENDEZVOUS
WHEN
USED
WITH
MUTEXES
CONDITION
VARIABLES
ALLOW
THREADS
TO
WAIT
IN
A
RACE
FREE
WAY
FOR
ARBITRARY
CONDITIONS
TO
OCCUR
THE
CONDITION
ITSELF
IS
PROTECTED
BY
A
MUTEX
A
THREAD
MUST
FIRST
LOCK
THE
MUTEX
TO
CHANGE
THE
CONDITION
STATE
OTHER
THREADS
WILL
NOT
NOTICE
THE
CHANGE
UNTIL
THEY
ACQUIRE
THE
MUTEX
BECAUSE
THE
MUTEX
MUST
BE
LOCKED
TO
BE
ABLE
TO
EVALUATE
THE
CONDITION
BEFORE
A
CONDITION
VARIABLE
IS
USED
IT
MUST
FIRST
BE
INITIALIZED
A
CONDITION
VARIABLE
REPRESENTED
BY
THE
DATA
TYPE
CAN
BE
INITIALIZED
IN
TWO
WAYS
WE
CAN
ASSIGN
THE
CONSTANT
TO
A
STATICALLY
ALLOCATED
CONDITION
VARIABLE
BUT
IF
THE
CONDITION
VARIABLE
IS
ALLOCATED
DYNAMICALLY
WE
CAN
USE
THE
FUNCTION
TO
INITIALIZE
IT
WE
CAN
USE
THE
FUNCTION
TO
DEINITIALIZE
A
CONDITION
VARIABLE
BEFORE
FREEING
ITS
UNDERLYING
MEMORY
UNLESS
YOU
NEED
TO
CREATE
A
CONDITIONAL
VARIABLE
WITH
NONDEFAULT
ATTRIBUTES
THE
ATTR
ARGUMENT
TO
CAN
BE
SET
TO
NULL
WE
WILL
DISCUSS
CONDITION
VARIABLE
ATTRIBUTES
IN
SECTION
WE
USE
TO
WAIT
FOR
A
CONDITION
TO
BE
TRUE
A
VARIANT
IS
PROVIDED
TO
RETURN
AN
ERROR
CODE
IF
THE
CONDITION
HASN
T
BEEN
SATISFIED
IN
THE
SPECIFIED
AMOUNT
OF
TIME
THE
MUTEX
PASSED
TO
PROTECTS
THE
CONDITION
THE
CALLER
PASSES
IT
LOCKED
TO
THE
FUNCTION
WHICH
THEN
ATOMICALLY
PLACES
THE
CALLING
THREAD
ON
THE
LIST
OF
THREADS
WAITING
FOR
THE
CONDITION
AND
UNLOCKS
THE
MUTEX
THIS
CLOSES
THE
WINDOW
BETWEEN
THE
TIME
THAT
THE
CONDITION
IS
CHECKED
AND
THE
TIME
THAT
THE
THREAD
GOES
TO
SLEEP
WAITING
FOR
THE
CONDITION
TO
CHANGE
SO
THAT
THE
THREAD
DOESN
T
MISS
A
CHANGE
IN
THE
CONDITION
WHEN
RETURNS
THE
MUTEX
IS
AGAIN
LOCKED
THE
FUNCTION
PROVIDES
THE
SAME
FUNCTIONALITY
AS
THE
FUNCTION
WITH
THE
ADDITION
OF
THE
TIMEOUT
TSPTR
THE
TIMEOUT
VALUE
SPECIFIES
HOW
LONG
WE
ARE
WILLING
TO
WAIT
EXPRESSED
AS
A
TIMESPEC
STRUCTURE
JUST
AS
WE
SAW
IN
FIGURE
WE
NEED
TO
SPECIFY
HOW
LONG
WE
ARE
WILLING
TO
WAIT
AS
AN
ABSOLUTE
TIME
INSTEAD
OF
A
RELATIVE
TIME
FOR
EXAMPLE
SUPPOSE
WE
ARE
WILLING
TO
WAIT
MINUTES
INSTEAD
OF
TRANSLATING
MINUTES
INTO
A
TIMESPEC
STRUCTURE
WE
NEED
TO
TRANSLATE
NOW
MINUTES
INTO
A
TIMESPEC
STRUCTURE
WE
CAN
USE
THE
FUNCTION
SECTION
TO
GET
THE
CURRENT
TIME
EXPRESSED
AS
A
TIMESPEC
STRUCTURE
HOWEVER
THIS
FUNCTION
IS
NOT
YET
SUPPORTED
ON
ALL
PLATFORMS
ALTERNATIVELY
WE
CAN
USE
THE
GETTIMEOFDAY
FUNCTION
TO
GET
THE
CURRENT
TIME
EXPRESSED
AS
A
TIMEVAL
STRUCTURE
AND
TRANSLATE
IT
INTO
A
TIMESPEC
STRUCTURE
TO
OBTAIN
THE
ABSOLUTE
TIME
FOR
THE
TIMEOUT
VALUE
WE
CAN
USE
THE
FOLLOWING
FUNCTION
ASSUMING
THE
MAXIMUM
TIME
BLOCKED
IS
EXPRESSED
IN
MINUTES
INCLUDE
SYS
TIME
H
INCLUDE
STDLIB
H
VOID
MAKETIMEOUT
STRUCT
TIMESPEC
TSP
LONG
MINUTES
STRUCT
TIMEVAL
NOW
GET
THE
CURRENT
TIME
GETTIMEOFDAY
NOW
NULL
TSP
NOW
TSP
NOW
USEC
TO
NSEC
ADD
THE
OFFSET
TO
GET
TIMEOUT
VALUE
TSP
MINUTES
IF
THE
TIMEOUT
EXPIRES
WITHOUT
THE
CONDITION
OCCURRING
WILL
REACQUIRE
THE
MUTEX
AND
RETURN
THE
ERROR
ETIMEDOUT
WHEN
IT
RETURNS
FROM
A
SUCCESSFUL
CALL
TO
OR
A
THREAD
NEEDS
TO
REEVALUATE
THE
CONDITION
SINCE
ANOTHER
THREAD
MIGHT
HAVE
RUN
AND
ALREADY
CHANGED
THE
CONDITION
THERE
ARE
TWO
FUNCTIONS
TO
NOTIFY
THREADS
THAT
A
CONDITION
HAS
BEEN
SATISFIED
THE
FUNCTION
WILL
WAKE
UP
AT
LEAST
ONE
THREAD
WAITING
ON
A
CONDITION
WHEREAS
THE
FUNCTION
WILL
WAKE
UP
ALL
THREADS
WAITING
ON
A
CONDITION
THE
POSIX
SPECIFICATION
ALLOWS
FOR
IMPLEMENTATIONS
OF
TO
WAKE
UP
MORE
THAN
ONE
THREAD
TO
MAKE
THE
IMPLEMENTATION
SIMPLER
WHEN
WE
CALL
OR
WE
ARE
SAID
TO
BE
SIGNALING
THE
THREAD
OR
CONDITION
WE
HAVE
TO
BE
CAREFUL
TO
SIGNAL
THE
THREADS
ONLY
AFTER
CHANGING
THE
STATE
OF
THE
CONDITION
EXAMPLE
FIGURE
SHOWS
AN
EXAMPLE
OF
HOW
TO
USE
A
CONDITION
VARIABLE
AND
A
MUTEX
TOGETHER
TO
SYNCHRONIZE
THREADS
INCLUDE
PTHREAD
H
STRUCT
MSG
STRUCT
MSG
MORE
STUFF
HERE
STRUCT
MSG
WORKQ
QREADY
QLOCK
VOID
VOID
STRUCT
MSG
MP
FOR
QLOCK
WHILE
WORKQ
NULL
QREADY
QLOCK
MP
WORKQ
WORKQ
MP
QLOCK
NOW
PROCESS
THE
MESSAGE
MP
VOID
STRUCT
MSG
MP
QLOCK
MP
WORKQ
WORKQ
MP
QLOCK
QREADY
FIGURE
USING
A
CONDITION
VARIABLE
THE
CONDITION
IS
THE
STATE
OF
THE
WORK
QUEUE
WE
PROTECT
THE
CONDITION
WITH
A
MUTEX
AND
EVALUATE
THE
CONDITION
IN
A
WHILE
LOOP
WHEN
WE
PUT
A
MESSAGE
ON
THE
WORK
QUEUE
WE
NEED
TO
HOLD
THE
MUTEX
BUT
WE
DON
T
NEED
TO
HOLD
THE
MUTEX
WHEN
WE
SIGNAL
THE
WAITING
THREADS
AS
LONG
AS
IT
IS
OKAY
FOR
A
THREAD
TO
PULL
THE
MESSAGE
OFF
THE
QUEUE
BEFORE
WE
CALL
WE
CAN
DO
THIS
AFTER
RELEASING
THE
MUTEX
SINCE
WE
CHECK
THE
CONDITION
IN
A
WHILE
LOOP
THIS
DOESN
T
PRESENT
A
PROBLEM
A
THREAD
WILL
WAKE
UP
FIND
THAT
THE
QUEUE
IS
STILL
EMPTY
AND
GO
BACK
TO
WAITING
AGAIN
IF
THE
CODE
COULDN
T
TOLERATE
THIS
RACE
WE
WOULD
NEED
TO
HOLD
THE
MUTEX
WHEN
WE
SIGNAL
THE
THREADS
SPIN
LOCKS
A
SPIN
LOCK
IS
LIKE
A
MUTEX
EXCEPT
THAT
INSTEAD
OF
BLOCKING
A
PROCESS
BY
SLEEPING
THE
PROCESS
IS
BLOCKED
BY
BUSY
WAITING
SPINNING
UNTIL
THE
LOCK
CAN
BE
ACQUIRED
A
SPIN
LOCK
COULD
BE
USED
IN
SITUATIONS
WHERE
LOCKS
ARE
HELD
FOR
SHORT
PERIODS
OF
TIMES
AND
THREADS
DON
T
WANT
TO
INCUR
THE
COST
OF
BEING
DESCHEDULED
SPIN
LOCKS
ARE
OFTEN
USED
AS
LOW
LEVEL
PRIMITIVES
TO
IMPLEMENT
OTHER
TYPES
OF
LOCKS
DEPENDING
ON
THE
SYSTEM
ARCHITECTURE
THEY
CAN
BE
IMPLEMENTED
EFFICIENTLY
USING
TEST
AND
SET
INSTRUCTIONS
ALTHOUGH
EFFICIENT
THEY
CAN
LEAD
TO
WASTING
CPU
RESOURCES
WHILE
A
THREAD
IS
SPINNING
AND
WAITING
FOR
A
LOCK
TO
BECOME
AVAILABLE
THE
CPU
CAN
T
DO
ANYTHING
ELSE
THIS
IS
WHY
SPIN
LOCKS
SHOULD
BE
HELD
ONLY
FOR
SHORT
PERIODS
OF
TIME
SPIN
LOCKS
ARE
USEFUL
WHEN
USED
IN
A
NONPREEMPTIVE
KERNEL
BESIDES
PROVIDING
A
MUTUAL
EXCLUSION
MECHANISM
THEY
BLOCK
INTERRUPTS
SO
AN
INTERRUPT
HANDLER
CAN
T
DEADLOCK
THE
SYSTEM
BY
TRYING
TO
ACQUIRE
A
SPIN
LOCK
THAT
IS
ALREADY
LOCKED
THINK
OF
INTERRUPTS
AS
ANOTHER
TYPE
OF
PREEMPTION
IN
THESE
TYPES
OF
KERNELS
INTERRUPT
HANDLERS
CAN
T
SLEEP
SO
THE
ONLY
SYNCHRONIZATION
PRIMITIVES
THEY
CAN
USE
ARE
SPIN
LOCKS
HOWEVER
AT
USER
LEVEL
SPIN
LOCKS
ARE
NOT
AS
USEFUL
UNLESS
YOU
ARE
RUNNING
IN
A
REAL
TIME
SCHEDULING
CLASS
THAT
DOESN
T
ALLOW
PREEMPTION
USER
LEVEL
THREADS
RUNNING
IN
A
TIME
SHARING
SCHEDULING
CLASS
CAN
BE
DESCHEDULED
WHEN
THEIR
TIME
QUANTUM
EXPIRES
OR
WHEN
A
THREAD
WITH
A
HIGHER
SCHEDULING
PRIORITY
BECOMES
RUNNABLE
IN
THESE
CASES
IF
A
THREAD
IS
HOLDING
A
SPIN
LOCK
IT
WILL
BE
PUT
TO
SLEEP
AND
OTHER
THREADS
BLOCKED
ON
THE
LOCK
WILL
CONTINUE
SPINNING
LONGER
THAN
INTENDED
MANY
MUTEX
IMPLEMENTATIONS
ARE
SO
EFFICIENT
THAT
THE
PERFORMANCE
OF
APPLICATIONS
USING
MUTEX
LOCKS
IS
EQUIVALENT
TO
THEIR
PERFORMANCE
IF
THEY
HAD
USED
SPIN
LOCKS
IN
FACT
SOME
MUTEX
IMPLEMENTATIONS
WILL
SPIN
FOR
A
LIMITED
AMOUNT
OF
TIME
TRYING
TO
ACQUIRE
THE
MUTEX
AND
ONLY
SLEEP
WHEN
THE
SPIN
COUNT
THRESHOLD
IS
REACHED
THESE
FACTORS
COMBINED
WITH
ADVANCES
IN
MODERN
PROCESSORS
THAT
ALLOW
THEM
TO
CONTEXT
SWITCH
AT
FASTER
AND
FASTER
RATES
MAKE
SPIN
LOCKS
USEFUL
ONLY
IN
LIMITED
CIRCUMSTANCES
THE
INTERFACES
FOR
SPIN
LOCKS
ARE
SIMILAR
TO
THOSE
FOR
MUTEXES
MAKING
IT
RELATIVELY
EASY
TO
REPLACE
ONE
WITH
THE
OTHER
WE
CAN
INITIALIZE
A
SPIN
LOCK
WITH
THE
FUNCTION
TO
DEINITIALIZE
A
SPIN
LOCK
WE
CAN
CALL
THE
FUNCTION
ONLY
ONE
ATTRIBUTE
IS
SPECIFIED
FOR
SPIN
LOCKS
WHICH
MATTERS
ONLY
IF
THE
PLATFORM
SUPPORTS
THE
THREAD
PROCESS
SHARED
SYNCHRONIZATION
OPTION
NOW
MANDATORY
IN
THE
SINGLE
UNIX
SPECIFICATION
RECALL
FIGURE
THE
PSHARED
ARGUMENT
REPRESENTS
THE
PROCESS
SHARED
ATTRIBUTE
WHICH
INDICATES
HOW
THE
SPIN
LOCK
WILL
BE
ACQUIRED
IF
IT
IS
SET
TO
THEN
THE
SPIN
LOCK
CAN
BE
ACQUIRED
BY
THREADS
THAT
HAVE
ACCESS
TO
THE
LOCK
UNDERLYING
MEMORY
EVEN
IF
THOSE
THREADS
ARE
FROM
DIFFERENT
PROCESSES
OTHERWISE
THE
PSHARED
ARGUMENT
IS
SET
TO
AND
THE
SPIN
LOCK
CAN
BE
ACCESSED
ONLY
FROM
THREADS
WITHIN
THE
PROCESS
THAT
INITIALIZED
IT
TO
LOCK
THE
SPIN
LOCK
WE
CAN
CALL
EITHER
WHICH
WILL
SPIN
UNTIL
THE
LOCK
IS
ACQUIRED
OR
WHICH
WILL
RETURN
THE
EBUSY
ERROR
IF
THE
LOCK
CAN
T
BE
ACQUIRED
IMMEDIATELY
NOTE
THAT
DOESN
T
SPIN
REGARDLESS
OF
HOW
IT
WAS
LOCKED
A
SPIN
LOCK
CAN
BE
UNLOCKED
BY
CALLING
NOTE
THAT
IF
A
SPIN
LOCK
IS
CURRENTLY
UNLOCKED
THEN
THE
FUNCTION
CAN
LOCK
IT
WITHOUT
SPINNING
IF
THE
THREAD
ALREADY
HAS
IT
LOCKED
THE
RESULTS
ARE
UNDEFINED
THE
CALL
TO
COULD
FAIL
WITH
THE
EDEADLK
ERROR
OR
SOME
OTHER
ERROR
OR
THE
CALL
COULD
SPIN
INDEFINITELY
THE
BEHAVIOR
DEPENDS
ON
THE
IMPLEMENTATION
IF
WE
TRY
TO
UNLOCK
A
SPIN
LOCK
THAT
IS
NOT
LOCKED
THE
RESULTS
ARE
ALSO
UNDEFINED
IF
EITHER
OR
RETURNS
THEN
THE
SPIN
LOCK
IS
LOCKED
WE
NEED
TO
BE
CAREFUL
NOT
TO
CALL
ANY
FUNCTIONS
THAT
MIGHT
SLEEP
WHILE
HOLDING
THE
SPIN
LOCK
IF
WE
DO
THEN
WE
LL
WASTE
CPU
RESOURCES
BY
EXTENDING
THE
TIME
OTHER
THREADS
WILL
SPIN
IF
THEY
TRY
TO
ACQUIRE
IT
BARRIERS
BARRIERS
ARE
A
SYNCHRONIZATION
MECHANISM
THAT
CAN
BE
USED
TO
COORDINATE
MULTIPLE
THREADS
WORKING
IN
PARALLEL
A
BARRIER
ALLOWS
EACH
THREAD
TO
WAIT
UNTIL
ALL
COOPERATING
THREADS
HAVE
REACHED
THE
SAME
POINT
AND
THEN
CONTINUE
EXECUTING
FROM
THERE
WE
VE
ALREADY
SEEN
ONE
FORM
OF
BARRIER
THE
FUNCTION
ACTS
AS
A
BARRIER
TO
ALLOW
ONE
THREAD
TO
WAIT
UNTIL
ANOTHER
THREAD
EXITS
BARRIER
OBJECTS
ARE
MORE
GENERAL
THAN
THIS
HOWEVER
THEY
ALLOW
AN
ARBITRARY
NUMBER
OF
THREADS
TO
WAIT
UNTIL
ALL
OF
THE
THREADS
HAVE
COMPLETED
PROCESSING
BUT
THE
THREADS
DON
T
HAVE
TO
EXIT
THEY
CAN
CONTINUE
WORKING
AFTER
ALL
THREADS
HAVE
REACHED
THE
BARRIER
WE
CAN
USE
THE
FUNCTION
TO
INITIALIZE
A
BARRIER
AND
WE
CAN
USE
THE
FUNCTION
TO
DEINITIALIZE
A
BARRIER
WHEN
WE
INITIALIZE
A
BARRIER
WE
USE
THE
COUNT
ARGUMENT
TO
SPECIFY
THE
NUMBER
OF
THREADS
THAT
MUST
REACH
THE
BARRIER
BEFORE
ALL
OF
THE
THREADS
WILL
BE
ALLOWED
TO
CONTINUE
WE
USE
THE
ATTR
ARGUMENT
TO
SPECIFY
THE
ATTRIBUTES
OF
THE
BARRIER
OBJECT
WHICH
WE
LL
LOOK
AT
MORE
CLOSELY
IN
THE
NEXT
CHAPTER
FOR
NOW
WE
CAN
SET
ATTR
TO
NULL
TO
INITIALIZE
A
BARRIER
WITH
THE
DEFAULT
ATTRIBUTES
IF
THE
FUNCTION
ALLOCATED
ANY
RESOURCES
FOR
THE
BARRIER
THE
RESOURCES
WILL
BE
FREED
WHEN
WE
DEINITIALIZE
THE
BARRIER
BY
CALLING
THE
FUNCTION
WE
USE
THE
FUNCTION
TO
INDICATE
THAT
A
THREAD
IS
DONE
WITH
ITS
WORK
AND
IS
READY
TO
WAIT
FOR
ALL
THE
OTHER
THREADS
TO
CATCH
UP
THE
THREAD
CALLING
IS
PUT
TO
SLEEP
IF
THE
BARRIER
COUNT
SET
IN
THE
CALL
TO
IS
NOT
YET
SATISFIED
IF
THE
THREAD
IS
THE
LAST
ONE
TO
CALL
THEREBY
SATISFYING
THE
BARRIER
COUNT
ALL
OF
THE
THREADS
ARE
AWAKENED
TO
ONE
ARBITRARY
THREAD
IT
WILL
APPEAR
AS
IF
THE
FUNCTION
RETURNED
A
VALUE
OF
THE
REMAINING
THREADS
SEE
A
RETURN
VALUE
OF
THIS
ALLOWS
ONE
THREAD
TO
CONTINUE
AS
THE
MASTER
TO
ACT
ON
THE
RESULTS
OF
THE
WORK
DONE
BY
ALL
OF
THE
OTHER
THREADS
ONCE
THE
BARRIER
COUNT
IS
REACHED
AND
THE
THREADS
ARE
UNBLOCKED
THE
BARRIER
CAN
BE
USED
AGAIN
HOWEVER
THE
BARRIER
COUNT
CAN
T
BE
CHANGED
UNLESS
WE
CALL
THE
FUNCTION
FOLLOWED
BY
THE
FUNCTION
WITH
A
DIFFERENT
COUNT
EXAMPLE
FIGURE
SHOWS
HOW
A
BARRIER
CAN
BE
USED
TO
SYNCHRONIZE
THREADS
COOPERATING
ON
A
SINGLE
TASK
INCLUDE
APUE
H
INCLUDE
PTHREAD
H
INCLUDE
LIMITS
H
INCLUDE
SYS
TIME
H
DEFINE
NTHR
NUMBER
OF
THREADS
DEFINE
NUMNUM
NUMBER
OF
NUMBERS
TO
SORT
DEFINE
TNUM
NUMNUM
NTHR
NUMBER
TO
SORT
PER
THREAD
LONG
NUMS
NUMNUM
LONG
SNUMS
NUMNUM
B
IFDEF
SOLARIS
DEFINE
HEAPSORT
QSORT
ELSE
EXTERN
INT
HEAPSORT
VOID
INT
CONST
VOID
CONST
VOID
ENDIF
COMPARE
TWO
LONG
INTEGERS
HELPER
FUNCTION
FOR
HEAPSORT
INT
COMPLONG
CONST
VOID
CONST
VOID
LONG
LONG
LONG
LONG
IF
RETURN
ELSE
IF
RETURN
ELSE
RETURN
WORKER
THREAD
TO
SORT
A
PORTION
OF
THE
SET
OF
NUMBERS
VOID
VOID
ARG
LONG
IDX
LONG
ARG
HEAPSORT
NUMS
IDX
TNUM
SIZEOF
LONG
COMPLONG
B
GO
OFF
AND
PERFORM
MORE
WORK
RETURN
VOID
MERGE
THE
RESULTS
OF
THE
INDIVIDUAL
SORTED
RANGES
VOID
MERGE
LONG
IDX
NTHR
LONG
I
MINIDX
SIDX
NUM
FOR
I
I
NTHR
I
IDX
I
I
TNUM
FOR
SIDX
SIDX
NUMNUM
SIDX
NUM
FOR
I
I
NTHR
I
IF
IDX
I
I
TNUM
NUMS
IDX
I
NUM
NUM
NUMS
IDX
I
MINIDX
I
INT
SNUMS
SIDX
NUMS
IDX
MINIDX
IDX
MINIDX
MAIN
UNSIGNED
LONG
I
STRUCT
TIMEVAL
START
END
LONG
LONG
STARTUSEC
ENDUSEC
DOUBLE
ELAPSED
INT
ERR
TID
CREATE
THE
INITIAL
SET
OF
NUMBERS
TO
SORT
SRANDOM
FOR
I
I
NUMNUM
I
NUMS
I
RANDOM
CREATE
THREADS
TO
SORT
THE
NUMBERS
GETTIMEOFDAY
START
NULL
B
NULL
NTHR
FOR
I
I
NTHR
I
ERR
TID
NULL
VOID
I
TNUM
IF
ERR
ERR
CAN
T
CREATE
THREAD
B
MERGE
GETTIMEOFDAY
END
NULL
PRINT
THE
SORTED
LIST
STARTUSEC
START
START
ENDUSEC
END
END
ELAPSED
DOUBLE
ENDUSEC
STARTUSEC
PRINTF
SORT
TOOK
SECONDS
N
ELAPSED
FOR
I
I
NUMNUM
I
PRINTF
LD
N
SNUMS
I
EXIT
FIGURE
USING
A
BARRIER
THIS
EXAMPLE
SHOWS
THE
USE
OF
A
BARRIER
IN
A
SIMPLIFIED
SITUATION
WHERE
THE
THREADS
PERFORM
ONLY
ONE
TASK
IN
MORE
REALISTIC
SITUATIONS
THE
WORKER
THREADS
WILL
CONTINUE
WITH
OTHER
ACTIVITIES
AFTER
THE
CALL
TO
RETURNS
IN
THE
EXAMPLE
WE
USE
EIGHT
THREADS
TO
DIVIDE
THE
JOB
OF
SORTING
MILLION
NUMBERS
EACH
THREAD
SORTS
MILLION
NUMBERS
USING
THE
HEAPSORT
ALGORITHM
SEE
KNUTH
FOR
DETAILS
THEN
THE
MAIN
THREAD
CALLS
A
FUNCTION
TO
MERGE
THE
RESULTS
WE
DON
T
NEED
TO
USE
THE
RETURN
VALUE
FROM
TO
DECIDE
WHICH
THREAD
MERGES
THE
RESULTS
BECAUSE
WE
USE
THE
MAIN
THREAD
FOR
THIS
TASK
THAT
IS
WHY
WE
SPECIFY
THE
BARRIER
COUNT
AS
ONE
MORE
THAN
THE
NUMBER
OF
WORKER
THREADS
THE
MAIN
THREAD
COUNTS
AS
ONE
WAITER
IF
WE
WRITE
A
PROGRAM
TO
SORT
MILLION
NUMBERS
WITH
HEAPSORT
USING
THREAD
ONLY
WE
WILL
SEE
A
PERFORMANCE
IMPROVEMENT
WHEN
COMPARING
IT
TO
THE
PROGRAM
IN
FIGURE
ON
A
SYSTEM
WITH
CORES
THE
SINGLE
THREADED
PROGRAM
SORTED
MILLION
NUMBERS
IN
SECONDS
ON
THE
SAME
SYSTEM
USING
THREADS
IN
PARALLEL
AND
THREAD
TO
MERGE
THE
RESULTS
THE
SAME
SET
OF
MILLION
NUMBERS
WAS
SORTED
IN
SECONDS
TIMES
FASTER
SUMMARY
IN
THIS
CHAPTER
WE
INTRODUCED
THE
CONCEPT
OF
THREADS
AND
DISCUSSED
THE
POSIX
PRIMITIVES
AVAILABLE
TO
CREATE
AND
DESTROY
THEM
WE
ALSO
INTRODUCED
THE
PROBLEM
OF
THREAD
SYNCHRONIZATION
WE
DISCUSSED
FIVE
FUNDAMENTAL
SYNCHRONIZATION
MECHANISMS
MUTEXES
READER
WRITER
LOCKS
CONDITION
VARIABLES
SPIN
LOCKS
AND
BARRIERS
AND
WE
SAW
HOW
TO
USE
THEM
TO
PROTECT
SHARED
RESOURCES
EXERCISES
MODIFY
THE
EXAMPLE
CODE
SHOWN
IN
FIGURE
TO
PASS
THE
STRUCTURE
BETWEEN
THE
THREADS
PROPERLY
IN
THE
EXAMPLE
CODE
SHOWN
IN
FIGURE
WHAT
ADDITIONAL
SYNCHRONIZATION
IF
ANY
IS
NECESSARY
TO
ALLOW
THE
MASTER
THREAD
TO
CHANGE
THE
THREAD
ID
ASSOCIATED
WITH
A
PENDING
JOB
HOW
WOULD
THIS
AFFECT
THE
FUNCTION
APPLY
THE
TECHNIQUES
SHOWN
IN
FIGURE
TO
THE
WORKER
THREAD
EXAMPLE
FIGURES
AND
TO
IMPLEMENT
THE
WORKER
THREAD
FUNCTION
DON
T
FORGET
TO
UPDATE
THE
FUNCTION
TO
INITIALIZE
THE
CONDITION
VARIABLE
AND
CHANGE
THE
AND
FUNCTIONS
TO
SIGNAL
THE
WORKER
THREADS
WHAT
DIFFICULTIES
ARISE
WHICH
SEQUENCE
OF
STEPS
IS
CORRECT
LOCK
A
MUTEX
CHANGE
THE
CONDITION
PROTECTED
BY
THE
MUTEX
SIGNAL
THREADS
WAITING
ON
THE
CONDITION
UNLOCK
THE
MUTEX
OR
LOCK
A
MUTEX
CHANGE
THE
CONDITION
PROTECTED
BY
THE
MUTEX
UNLOCK
THE
MUTEX
SIGNAL
THREADS
WAITING
ON
THE
CONDITION
WHAT
SYNCHRONIZATION
PRIMITIVES
WOULD
YOU
NEED
TO
IMPLEMENT
A
BARRIER
PROVIDE
AN
IMPLEMENTATION
OF
THE
FUNCTION
THIS
PAGE
INTENTIONALLY
LEFT
BLANK
THREAD
CONTROL
INTRODUCTION
IN
CHAPTER
WE
LEARNED
THE
BASICS
ABOUT
THREADS
AND
THREAD
SYNCHRONIZATION
IN
THIS
CHAPTER
WE
WILL
LEARN
THE
DETAILS
OF
CONTROLLING
THREAD
BEHAVIOR
WE
WILL
LOOK
AT
THREAD
ATTRIBUTES
AND
SYNCHRONIZATION
PRIMITIVE
ATTRIBUTES
WHICH
WE
IGNORED
IN
THE
PREVIOUS
CHAPTER
IN
FAVOR
OF
THE
DEFAULT
BEHAVIOR
WE
WILL
FOLLOW
THIS
WITH
A
LOOK
AT
HOW
THREADS
CAN
KEEP
DATA
PRIVATE
FROM
OTHER
THREADS
IN
THE
SAME
PROCESS
THEN
WE
WILL
WRAP
UP
THE
CHAPTER
WITH
A
LOOK
AT
HOW
SOME
PROCESS
BASED
SYSTEM
CALLS
INTERACT
WITH
THREADS
THREAD
LIMITS
WE
DISCUSSED
THE
SYSCONF
FUNCTION
IN
SECTION
THE
SINGLE
UNIX
SPECIFICATION
DEFINES
SEVERAL
LIMITS
ASSOCIATED
WITH
THE
OPERATION
OF
THREADS
WHICH
WE
DIDN
T
SHOW
IN
FIGURE
AS
WITH
OTHER
SYSTEM
LIMITS
THE
THREAD
LIMITS
CAN
BE
QUERIED
USING
SYSCONF
FIGURE
SUMMARIZES
THESE
LIMITS
AS
WITH
THE
OTHER
LIMITS
REPORTED
BY
SYSCONF
USE
OF
THESE
LIMITS
IS
INTENDED
TO
PROMOTE
APPLICATION
PORTABILITY
AMONG
DIFFERENT
OPERATING
SYSTEM
IMPLEMENTATIONS
FOR
EXAMPLE
IF
YOUR
APPLICATION
REQUIRES
THAT
YOU
CREATE
FOUR
THREADS
FOR
EVERY
FILE
YOU
MANAGE
YOU
MIGHT
HAVE
TO
LIMIT
THE
NUMBER
OF
FILES
YOU
CAN
MANAGE
CONCURRENTLY
IF
THE
SYSTEM
WON
T
LET
YOU
CREATE
ENOUGH
THREADS
NAME
OF
LIMIT
DESCRIPTION
NAME
ARGUMENT
MAXIMUM
NUMBER
OF
TIMES
AN
IMPLEMENTATION
WILL
TRY
TO
DESTROY
THE
THREAD
SPECIFIC
DATA
WHEN
A
THREAD
EXITS
SECTION
MAXIMUM
NUMBER
OF
KEYS
THAT
CAN
BE
CREATED
BY
A
PROCESS
SECTION
MINIMUM
NUMBER
OF
BYTES
THAT
CAN
BE
USED
FOR
A
THREAD
STACK
SECTION
MAXIMUM
NUMBER
OF
THREADS
THAT
CAN
BE
CREATED
IN
A
PROCESS
SECTION
FIGURE
THREAD
LIMITS
AND
NAME
ARGUMENTS
TO
SYSCONF
FIGURE
SHOWS
THE
VALUES
OF
THE
THREAD
LIMITS
FOR
THE
FOUR
IMPLEMENTATIONS
DESCRIBED
IN
THIS
BOOK
IF
THE
IMPLEMENTATION
LIMIT
IS
INDETERMINATE
NO
LIMIT
IS
LISTED
THIS
DOESN
T
MEAN
THAT
THE
VALUE
IS
UNLIMITED
HOWEVER
NOTE
THAT
ALTHOUGH
AN
IMPLEMENTATION
MAY
NOT
PROVIDE
ACCESS
TO
THESE
LIMITS
THAT
DOESN
T
MEAN
THAT
THE
LIMITS
DON
T
EXIST
IT
JUST
MEANS
THAT
THE
IMPLEMENTATION
DOESN
T
PROVIDE
US
WITH
A
WAY
TO
GET
AT
THEM
USING
SYSCONF
LIMIT
FREEBSD
LINUX
MAC
OS
X
SOLARIS
NO
LIMIT
NO
LIMIT
NO
LIMIT
NO
LIMIT
NO
LIMIT
NO
LIMIT
FIGURE
EXAMPLES
OF
THREAD
CONFIGURATION
LIMITS
THREAD
ATTRIBUTES
THE
PTHREAD
INTERFACE
ALLOWS
US
TO
FINE
TUNE
THE
BEHAVIOR
OF
THREADS
AND
SYNCHRONIZATION
OBJECTS
BY
SETTING
VARIOUS
ATTRIBUTES
ASSOCIATED
WITH
EACH
OBJECT
GENERALLY
THE
FUNCTIONS
FOR
MANAGING
THESE
ATTRIBUTES
FOLLOW
THE
SAME
PATTERN
EACH
OBJECT
IS
ASSOCIATED
WITH
ITS
OWN
TYPE
OF
ATTRIBUTE
OBJECT
THREADS
WITH
THREAD
ATTRIBUTES
MUTEXES
WITH
MUTEX
ATTRIBUTES
AND
SO
ON
AN
ATTRIBUTE
OBJECT
CAN
REPRESENT
MULTIPLE
ATTRIBUTES
THE
ATTRIBUTE
OBJECT
IS
OPAQUE
TO
APPLICATIONS
THIS
MEANS
THAT
APPLICATIONS
AREN
T
SUPPOSED
TO
KNOW
ANYTHING
ABOUT
ITS
INTERNAL
STRUCTURE
WHICH
PROMOTES
APPLICATION
PORTABILITY
INSTEAD
FUNCTIONS
ARE
PROVIDED
TO
MANAGE
THE
ATTRIBUTES
OBJECTS
AN
INITIALIZATION
FUNCTION
EXISTS
TO
SET
THE
ATTRIBUTES
TO
THEIR
DEFAULT
VALUES
ANOTHER
FUNCTION
EXISTS
TO
DESTROY
THE
ATTRIBUTES
OBJECT
IF
THE
INITIALIZATION
FUNCTION
ALLOCATED
ANY
RESOURCES
ASSOCIATED
WITH
THE
ATTRIBUTES
OBJECT
THE
DESTROY
FUNCTION
FREES
THOSE
RESOURCES
EACH
ATTRIBUTE
HAS
A
FUNCTION
TO
GET
THE
VALUE
OF
THE
ATTRIBUTE
FROM
THE
ATTRIBUTE
OBJECT
BECAUSE
THE
FUNCTION
RETURNS
ON
SUCCESS
OR
AN
ERROR
NUMBER
ON
FAILURE
THE
VALUE
IS
RETURNED
TO
THE
CALLER
BY
STORING
IT
IN
THE
MEMORY
LOCATION
SPECIFIED
BY
ONE
OF
THE
ARGUMENTS
EACH
ATTRIBUTE
HAS
A
FUNCTION
TO
SET
THE
VALUE
OF
THE
ATTRIBUTE
IN
THIS
CASE
THE
VALUE
IS
PASSED
AS
AN
ARGUMENT
BY
VALUE
IN
ALL
THE
EXAMPLES
IN
WHICH
WE
CALLED
IN
CHAPTER
WE
PASSED
IN
A
NULL
POINTER
INSTEAD
OF
PASSING
IN
A
POINTER
TO
A
STRUCTURE
WE
CAN
USE
THE
STRUCTURE
TO
MODIFY
THE
DEFAULT
ATTRIBUTES
AND
ASSOCIATE
THESE
ATTRIBUTES
WITH
THREADS
THAT
WE
CREATE
WE
USE
THE
FUNCTION
TO
INITIALIZE
THE
STRUCTURE
AFTER
CALLING
THE
STRUCTURE
CONTAINS
THE
DEFAULT
VALUES
FOR
ALL
THE
THREAD
ATTRIBUTES
SUPPORTED
BY
THE
IMPLEMENTATION
TO
DEINITIALIZE
A
STRUCTURE
WE
CALL
IF
AN
IMPLEMENTATION
OF
ALLOCATED
ANY
DYNAMIC
MEMORY
FOR
THE
ATTRIBUTE
OBJECT
WILL
FREE
THAT
MEMORY
IN
ADDITION
WILL
INITIALIZE
THE
ATTRIBUTE
OBJECT
WITH
INVALID
VALUES
SO
IF
IT
IS
USED
BY
MISTAKE
WILL
RETURN
AN
ERROR
CODE
THE
THREAD
ATTRIBUTES
DEFINED
BY
POSIX
ARE
SUMMARIZED
IN
FIGURE
POSIX
DEFINES
ADDITIONAL
ATTRIBUTES
IN
THE
THREAD
EXECUTION
SCHEDULING
OPTION
INTENDED
TO
SUPPORT
REAL
TIME
APPLICATIONS
BUT
WE
DON
T
DISCUSS
THEM
HERE
IN
FIGURE
WE
ALSO
SHOW
WHICH
PLATFORMS
SUPPORT
EACH
THREAD
ATTRIBUTE
NAME
DESCRIPTION
FREEBSD
LINUX
MAC
OS
X
SOLARIS
DETACHSTATE
DETACHED
THREAD
ATTRIBUTE
GUARDSIZE
GUARD
BUFFER
SIZE
IN
BYTES
AT
END
OF
THREAD
STACK
STACKADDR
LOWEST
ADDRESS
OF
THREAD
STACK
STACKSIZE
MINIMUM
SIZE
IN
BYTES
OF
THREAD
STACK
FIGURE
POSIX
THREAD
ATTRIBUTES
IN
SECTION
WE
INTRODUCED
THE
CONCEPT
OF
DETACHED
THREADS
IF
WE
ARE
NO
LONGER
INTERESTED
IN
AN
EXISTING
THREAD
TERMINATION
STATUS
WE
CAN
USE
TO
ALLOW
THE
OPERATING
SYSTEM
TO
RECLAIM
THE
THREAD
RESOURCES
WHEN
THE
THREAD
EXITS
IF
WE
KNOW
THAT
WE
DON
T
NEED
THE
THREAD
TERMINATION
STATUS
AT
THE
TIME
WE
CREATE
THE
THREAD
WE
CAN
ARRANGE
FOR
THE
THREAD
TO
START
OUT
IN
THE
DETACHED
STATE
BY
MODIFYING
THE
DETACHSTATE
THREAD
ATTRIBUTE
IN
THE
STRUCTURE
WE
CAN
USE
THE
FUNCTION
TO
SET
THE
DETACHSTATE
THREAD
ATTRIBUTE
TO
ONE
OF
TWO
LEGAL
VALUES
TO
START
THE
THREAD
IN
THE
DETACHED
STATE
OR
TO
START
THE
THREAD
NORMALLY
SO
ITS
TERMINATION
STATUS
CAN
BE
RETRIEVED
BY
THE
APPLICATION
WE
CAN
CALL
TO
OBTAIN
THE
CURRENT
DETACHSTATE
ATTRIBUTE
THE
INTEGER
POINTED
TO
BY
THE
SECOND
ARGUMENT
IS
SET
TO
EITHER
OR
DEPENDING
ON
THE
VALUE
OF
THE
ATTRIBUTE
IN
THE
GIVEN
STRUCTURE
EXAMPLE
FIGURE
SHOWS
A
FUNCTION
THAT
CAN
BE
USED
TO
CREATE
A
THREAD
IN
THE
DETACHED
STATE
INCLUDE
APUE
H
INCLUDE
PTHREAD
H
INT
MAKETHREAD
VOID
FN
VOID
VOID
ARG
INT
ERR
TID
ATTR
ERR
ATTR
IF
ERR
RETURN
ERR
ERR
ATTR
IF
ERR
ERR
TID
ATTR
FN
ARG
ATTR
RETURN
ERR
FIGURE
CREATING
A
THREAD
IN
THE
DETACHED
STATE
NOTE
THAT
WE
IGNORE
THE
RETURN
VALUE
FROM
THE
CALL
TO
IN
THIS
CASE
WE
INITIALIZED
THE
THREAD
ATTRIBUTES
PROPERLY
SO
SHOULDN
T
FAIL
NONETHELESS
IF
IT
DOES
FAIL
CLEANING
UP
WOULD
BE
DIFFICULT
WE
WOULD
HAVE
TO
DESTROY
THE
THREAD
WE
JUST
CREATED
WHICH
MIGHT
ALREADY
BE
RUNNING
ASYNCHRONOUS
TO
THE
EXECUTION
OF
THIS
FUNCTION
WHEN
WE
CHOOSE
TO
IGNORE
THE
ERROR
RETURN
FROM
THE
WORST
THAT
CAN
HAPPEN
IS
THAT
WE
LEAK
A
SMALL
AMOUNT
OF
MEMORY
IF
HAD
ALLOCATED
ANY
BUT
IF
SUCCEEDED
IN
INITIALIZING
THE
THREAD
ATTRIBUTES
AND
THEN
FAILED
TO
CLEAN
UP
WE
HAVE
NO
RECOVERY
STRATEGY
ANYWAY
BECAUSE
THE
ATTRIBUTES
STRUCTURE
IS
OPAQUE
TO
THE
APPLICATION
THE
ONLY
INTERFACE
DEFINED
TO
CLEAN
UP
THE
STRUCTURE
IS
AND
IT
JUST
FAILED
SUPPORT
FOR
THREAD
STACK
ATTRIBUTES
IS
OPTIONAL
FOR
A
POSIX
CONFORMING
OPERATING
SYSTEM
BUT
IS
REQUIRED
IF
THE
SYSTEM
SUPPORTS
THE
XSI
OPTION
IN
THE
SINGLE
UNIX
SPECIFICATION
AT
COMPILE
TIME
YOU
CAN
CHECK
WHETHER
YOUR
SYSTEM
SUPPORTS
EACH
THREAD
STACK
ATTRIBUTE
BY
USING
THE
AND
SYMBOLS
IF
ONE
OF
THESE
SYMBOLS
IS
DEFINED
THEN
THE
SYSTEM
SUPPORTS
THE
CORRESPONDING
THREAD
STACK
ATTRIBUTE
ALTERNATIVELY
YOU
CAN
CHECK
FOR
SUPPORT
AT
RUNTIME
BY
USING
THE
AND
PARAMETERS
TO
THE
SYSCONF
FUNCTION
WE
CAN
MANAGE
THE
STACK
ATTRIBUTES
USING
THE
AND
FUNCTIONS
WITH
A
PROCESS
THE
AMOUNT
OF
VIRTUAL
ADDRESS
SPACE
IS
FIXED
SINCE
THERE
IS
ONLY
ONE
STACK
ITS
SIZE
USUALLY
ISN
T
A
PROBLEM
WITH
THREADS
HOWEVER
THE
SAME
AMOUNT
OF
VIRTUAL
ADDRESS
SPACE
MUST
BE
SHARED
BY
ALL
THE
THREAD
STACKS
YOU
MIGHT
HAVE
TO
REDUCE
YOUR
DEFAULT
THREAD
STACK
SIZE
IF
YOUR
APPLICATION
USES
SO
MANY
THREADS
THAT
THE
CUMULATIVE
SIZE
OF
THEIR
STACKS
EXCEEDS
THE
AVAILABLE
VIRTUAL
ADDRESS
SPACE
ON
THE
OTHER
HAND
IF
YOUR
THREADS
CALL
FUNCTIONS
THAT
ALLOCATE
LARGE
AUTOMATIC
VARIABLES
OR
CALL
FUNCTIONS
MANY
STACK
FRAMES
DEEP
YOU
MIGHT
NEED
MORE
THAN
THE
DEFAULT
STACK
SIZE
IF
YOU
RUN
OUT
OF
VIRTUAL
ADDRESS
SPACE
FOR
THREAD
STACKS
YOU
CAN
USE
MALLOC
OR
MMAP
SEE
SECTION
TO
ALLOCATE
SPACE
FOR
AN
ALTERNATIVE
STACK
AND
USE
TO
CHANGE
THE
STACK
LOCATION
OF
THREADS
YOU
CREATE
THE
ADDRESS
SPECIFIED
BY
THE
STACKADDR
PARAMETER
IS
THE
LOWEST
ADDRESSABLE
ADDRESS
IN
THE
RANGE
OF
MEMORY
TO
BE
USED
AS
THE
THREAD
STACK
ALIGNED
AT
THE
PROPER
BOUNDARY
FOR
THE
PROCESSOR
ARCHITECTURE
OF
COURSE
THIS
ASSUMES
THAT
THE
VIRTUAL
ADDRESS
RANGE
USED
BY
MALLOC
OR
MMAP
IS
DIFFERENT
FROM
THE
RANGE
CURRENTLY
IN
USE
FOR
A
THREAD
STACK
THE
STACKADDR
THREAD
ATTRIBUTE
IS
DEFINED
AS
THE
LOWEST
MEMORY
ADDRESS
FOR
THE
STACK
THIS
IS
NOT
NECESSARILY
THE
START
OF
THE
STACK
HOWEVER
IF
STACKS
GROW
FROM
HIGHER
ADDRESSES
TO
LOWER
ADDRESSES
FOR
A
GIVEN
PROCESSOR
ARCHITECTURE
THE
STACKADDR
THREAD
ATTRIBUTE
WILL
BE
THE
END
OF
THE
STACK
INSTEAD
OF
THE
BEGINNING
AN
APPLICATION
CAN
ALSO
GET
AND
SET
THE
STACKSIZE
THREAD
ATTRIBUTE
USING
THE
AND
FUNCTIONS
THE
FUNCTION
IS
USEFUL
WHEN
YOU
WANT
TO
CHANGE
THE
DEFAULT
STACK
SIZE
BUT
DON
T
WANT
TO
DEAL
WITH
ALLOCATING
THE
THREAD
STACKS
ON
YOUR
OWN
WHEN
SETTING
THE
STACKSIZE
ATTRIBUTE
THE
SIZE
WE
CHOOSE
CAN
T
BE
SMALLER
THAN
THE
GUARDSIZE
THREAD
ATTRIBUTE
CONTROLS
THE
SIZE
OF
THE
MEMORY
EXTENT
AFTER
THE
END
OF
THE
THREAD
STACK
TO
PROTECT
AGAINST
STACK
OVERFLOW
ITS
DEFAULT
VALUE
IS
IMPLEMENTATION
DEFINED
BUT
A
COMMONLY
USED
VALUE
IS
THE
SYSTEM
PAGE
SIZE
WE
CAN
SET
THE
GUARDSIZE
THREAD
ATTRIBUTE
TO
TO
DISABLE
THIS
FEATURE
NO
GUARD
BUFFER
WILL
BE
PROVIDED
IN
THIS
CASE
ALSO
IF
WE
CHANGE
THE
STACKADDR
THREAD
ATTRIBUTE
THE
SYSTEM
ASSUMES
THAT
WE
WILL
BE
MANAGING
OUR
OWN
STACKS
AND
DISABLES
STACK
GUARD
BUFFERS
JUST
AS
IF
WE
HAD
SET
THE
GUARDSIZE
THREAD
ATTRIBUTE
TO
IF
THE
GUARDSIZE
THREAD
ATTRIBUTE
IS
MODIFIED
THE
OPERATING
SYSTEM
MIGHT
ROUND
IT
UP
TO
AN
INTEGRAL
MULTIPLE
OF
THE
PAGE
SIZE
IF
THE
THREAD
STACK
POINTER
OVERFLOWS
INTO
THE
GUARD
AREA
THE
APPLICATION
WILL
RECEIVE
AN
ERROR
POSSIBLY
WITH
A
SIGNAL
THE
SINGLE
UNIX
SPECIFICATION
DEFINES
SEVERAL
OTHER
OPTIONAL
THREAD
ATTRIBUTES
INTENDED
FOR
USE
BY
REAL
TIME
APPLICATIONS
WE
WILL
NOT
DISCUSS
THEM
HERE
THREADS
HAVE
OTHER
ATTRIBUTES
NOT
REPRESENTED
BY
THE
STRUCTURE
THE
CANCELABILITY
STATE
AND
THE
CANCELABILITY
TYPE
WE
DISCUSS
THEM
IN
SECTION
SYNCHRONIZATION
ATTRIBUTES
JUST
AS
THREADS
HAVE
ATTRIBUTES
SO
TOO
DO
THEIR
SYNCHRONIZATION
OBJECTS
IN
SECTION
WE
SAW
HOW
SPIN
LOCKS
HAVE
ONE
ATTRIBUTE
CALLED
THE
PROCESS
SHARED
ATTRIBUTE
IN
THIS
SECTION
WE
DISCUSS
THE
ATTRIBUTES
OF
MUTEXES
READER
WRITER
LOCKS
CONDITION
VARIABLES
AND
BARRIERS
MUTEX
ATTRIBUTES
MUTEX
ATTRIBUTES
ARE
REPRESENTED
BY
A
STRUCTURE
WHENEVER
WE
INITIALIZED
A
MUTEX
IN
CHAPTER
WE
ACCEPTED
THE
DEFAULT
ATTRIBUTES
BY
USING
THE
CONSTANT
OR
BY
CALLING
THE
FUNCTION
WITH
A
NULL
POINTER
FOR
THE
ARGUMENT
THAT
POINTS
TO
THE
MUTEX
ATTRIBUTE
STRUCTURE
WHEN
DEALING
WITH
NONDEFAULT
ATTRIBUTES
WE
USE
TO
INITIALIZE
A
STRUCTURE
AND
TO
DEINITIALIZE
ONE
THE
FUNCTION
WILL
INITIALIZE
THE
STRUCTURE
WITH
THE
DEFAULT
MUTEX
ATTRIBUTES
THERE
ARE
THREE
ATTRIBUTES
OF
INTEREST
THE
PROCESS
SHARED
ATTRIBUTE
THE
ROBUST
ATTRIBUTE
AND
THE
TYPE
ATTRIBUTE
WITHIN
POSIX
THE
PROCESS
SHARED
ATTRIBUTE
IS
OPTIONAL
YOU
CAN
TEST
WHETHER
A
PLATFORM
SUPPORTS
IT
BY
CHECKING
WHETHER
THE
SYMBOL
IS
DEFINED
YOU
CAN
ALSO
CHECK
AT
RUNTIME
BY
PASSING
THE
PARAMETER
TO
THE
SYSCONF
FUNCTION
ALTHOUGH
THIS
OPTION
IS
NOT
REQUIRED
TO
BE
PROVIDED
BY
POSIX
CONFORMING
OPERATING
SYSTEMS
THE
SINGLE
UNIX
SPECIFICATION
REQUIRES
THAT
XSI
CONFORMING
OPERATING
SYSTEMS
DO
SUPPORT
IT
WITHIN
A
PROCESS
MULTIPLE
THREADS
CAN
ACCESS
THE
SAME
SYNCHRONIZATION
OBJECT
THIS
IS
THE
DEFAULT
BEHAVIOR
AS
WE
SAW
IN
CHAPTER
IN
THIS
CASE
THE
PROCESS
SHARED
MUTEX
ATTRIBUTE
IS
SET
TO
AS
WE
SHALL
SEE
IN
CHAPTERS
AND
MECHANISMS
EXIST
THAT
ALLOW
INDEPENDENT
PROCESSES
TO
MAP
THE
SAME
EXTENT
OF
MEMORY
INTO
THEIR
INDEPENDENT
ADDRESS
SPACES
ACCESS
TO
SHARED
DATA
BY
MULTIPLE
PROCESSES
USUALLY
REQUIRES
SYNCHRONIZATION
JUST
AS
DOES
ACCESS
TO
SHARED
DATA
BY
MULTIPLE
THREADS
IF
THE
PROCESS
SHARED
MUTEX
ATTRIBUTE
IS
SET
TO
A
MUTEX
ALLOCATED
FROM
A
MEMORY
EXTENT
SHARED
BETWEEN
MULTIPLE
PROCESSES
MAY
BE
USED
FOR
SYNCHRONIZATION
BY
THOSE
PROCESSES
WE
CAN
USE
THE
FUNCTION
TO
QUERY
A
STRUCTURE
FOR
ITS
PROCESS
SHARED
ATTRIBUTE
WE
CAN
CHANGE
THE
PROCESS
SHARED
ATTRIBUTE
WITH
THE
FUNCTION
THE
PROCESS
SHARED
MUTEX
ATTRIBUTE
ALLOWS
THE
PTHREAD
LIBRARY
TO
PROVIDE
MORE
EFFICIENT
MUTEX
IMPLEMENTATIONS
WHEN
THE
ATTRIBUTE
IS
SET
TO
WHICH
IS
THE
DEFAULT
CASE
WITH
MULTITHREADED
APPLICATIONS
THE
PTHREAD
LIBRARY
CAN
THEN
RESTRICT
THE
MORE
EXPENSIVE
IMPLEMENTATION
TO
THE
CASE
IN
WHICH
MUTEXES
ARE
SHARED
AMONG
PROCESSES
THE
ROBUST
MUTEX
ATTRIBUTE
IS
RELATED
TO
MUTEXES
THAT
ARE
SHARED
AMONG
MULTIPLE
PROCESSES
IT
IS
MEANT
TO
ADDRESS
THE
PROBLEM
OF
MUTEX
STATE
RECOVERY
WHEN
A
PROCESS
TERMINATES
WHILE
HOLDING
A
MUTEX
WHEN
THIS
HAPPENS
THE
MUTEX
IS
LEFT
IN
A
LOCKED
STATE
AND
RECOVERY
IS
DIFFICULT
THREADS
BLOCKED
ON
THE
LOCK
IN
OTHER
PROCESSES
WILL
BLOCK
INDEFINITELY
WE
CAN
USE
THE
FUNCTION
TO
GET
THE
VALUE
OF
THE
ROBUST
MUTEX
ATTRIBUTE
TO
SET
THE
VALUE
OF
THE
ROBUST
MUTEX
ATTRIBUTE
WE
CAN
CALL
THE
FUNCTION
THERE
ARE
TWO
POSSIBLE
VALUES
FOR
THE
ROBUST
ATTRIBUTE
THE
DEFAULT
IS
WHICH
MEANS
THAT
NO
SPECIAL
ACTION
IS
TAKEN
WHEN
A
PROCESS
TERMINATES
WHILE
HOLDING
A
MUTEX
IN
THIS
CASE
USE
OF
THE
MUTEX
CAN
RESULT
IN
UNDEFINED
BEHAVIOR
AND
APPLICATIONS
WAITING
FOR
IT
TO
BE
UNLOCKED
ARE
EFFECTIVELY
STALLED
THE
OTHER
VALUE
IS
THIS
VALUE
WILL
CAUSE
A
THREAD
BLOCKED
IN
A
CALL
TO
TO
ACQUIRE
THE
LOCK
WHEN
ANOTHER
PROCESS
HOLDING
THE
LOCK
TERMINATES
WITHOUT
FIRST
UNLOCKING
IT
BUT
THE
RETURN
VALUE
FROM
IS
EOWNERDEAD
INSTEAD
OF
APPLICATIONS
CAN
USE
THIS
SPECIAL
RETURN
VALUE
AS
AN
INDICATION
THAT
THEY
NEED
TO
RECOVER
WHATEVER
STATE
THE
MUTEX
WAS
PROTECTING
IF
POSSIBLE
THE
DETAILS
OF
WHAT
STATE
IS
BEING
PROTECTED
AND
HOW
IT
CAN
BE
RECOVERED
WILL
VARY
AMONG
APPLICATIONS
NOTE
THAT
THE
EOWNERDEAD
ERROR
RETURN
ISN
T
REALLY
AN
ERROR
IN
THIS
CASE
BECAUSE
THE
CALLER
WILL
OWN
THE
LOCK
USING
ROBUST
MUTEXES
CHANGES
THE
WAY
WE
USE
BECAUSE
WE
NOW
HAVE
TO
CHECK
FOR
THREE
RETURN
VALUES
INSTEAD
OF
TWO
SUCCESS
WITH
NO
RECOVERY
NEEDED
SUCCESS
BUT
RECOVERY
NEEDED
AND
FAILURE
HOWEVER
IF
WE
DON
T
USE
ROBUST
MUTEXES
THEN
WE
CAN
CONTINUE
TO
CHECK
ONLY
FOR
SUCCESS
AND
FAILURE
OF
THE
FOUR
PLATFORMS
COVERED
IN
THIS
TEXT
ONLY
LINUX
CURRENTLY
SUPPORTS
ROBUST
PTHREAD
MUTEXES
SOLARIS
SUPPORTS
ROBUST
MUTEXES
ONLY
IN
ITS
SOLARIS
THREADS
LIBRARY
SEE
THE
SOLARIS
MANUAL
PAGE
FOR
MORE
INFORMATION
HOWEVER
IN
SOLARIS
ROBUST
PTHREAD
MUTEXES
ARE
SUPPORTED
IF
THE
APPLICATION
STATE
CAN
T
BE
RECOVERED
THE
MUTEX
WILL
BE
IN
A
PERMANENTLY
UNUSABLE
STATE
AFTER
THE
THREAD
UNLOCKS
THE
MUTEX
TO
PREVENT
THIS
PROBLEM
THE
THREAD
CAN
CALL
THE
FUNCTION
TO
INDICATE
THAT
THE
STATE
ASSOCIATED
WITH
THE
MUTEX
IS
CONSISTENT
BEFORE
UNLOCKING
THE
MUTEX
IF
A
THREAD
UNLOCKS
A
MUTEX
WITHOUT
FIRST
CALLING
THEN
OTHER
THREADS
THAT
ARE
BLOCKED
WHILE
TRYING
TO
ACQUIRE
THE
MUTEX
WILL
SEE
ERROR
RETURNS
OF
ENOTRECOVERABLE
IF
THIS
HAPPENS
THE
MUTEX
IS
NO
LONGER
USABLE
BY
CALLING
BEFOREHAND
A
THREAD
ALLOWS
THE
MUTEX
TO
BEHAVE
NORMALLY
SO
IT
CAN
CONTINUE
TO
BE
USED
THE
TYPE
MUTEX
ATTRIBUTE
CONTROLS
THE
LOCKING
CHARACTERISTICS
OF
THE
MUTEX
POSIX
DEFINES
FOUR
TYPES
A
STANDARD
MUTEX
TYPE
THAT
DOESN
T
DO
ANY
SPECIAL
ERROR
CHECKING
OR
DEADLOCK
DETECTION
A
MUTEX
TYPE
THAT
PROVIDES
ERROR
CHECKING
A
MUTEX
TYPE
THAT
ALLOWS
THE
SAME
THREAD
TO
LOCK
IT
MULTIPLE
TIMES
WITHOUT
FIRST
UNLOCKING
IT
A
RECURSIVE
MUTEX
MAINTAINS
A
LOCK
COUNT
AND
ISN
T
RELEASED
UNTIL
IT
IS
UNLOCKED
THE
SAME
NUMBER
OF
TIMES
IT
IS
LOCKED
THUS
IF
YOU
LOCK
A
RECURSIVE
MUTEX
TWICE
AND
THEN
UNLOCK
IT
THE
MUTEX
REMAINS
LOCKED
UNTIL
IT
IS
UNLOCKED
A
SECOND
TIME
A
MUTEX
TYPE
PROVIDING
DEFAULT
CHARACTERISTICS
AND
BEHAVIOR
IMPLEMENTATIONS
ARE
FREE
TO
MAP
IT
TO
ONE
OF
THE
OTHER
MUTEX
TYPES
FOR
EXAMPLE
LINUX
MAPS
THIS
TYPE
TO
THE
NORMAL
MUTEX
TYPE
WHEREAS
FREEBSD
MAPS
IT
TO
THE
ERROR
CHECKING
TYPE
THE
BEHAVIOR
OF
THE
FOUR
TYPES
IS
SUMMARIZED
IN
FIGURE
THE
UNLOCK
WHEN
NOT
OWNED
COLUMN
REFERS
TO
ONE
THREAD
UNLOCKING
A
MUTEX
THAT
WAS
LOCKED
BY
A
DIFFERENT
THREAD
THE
UNLOCK
WHEN
UNLOCKED
COLUMN
REFERS
TO
WHAT
HAPPENS
WHEN
A
THREAD
UNLOCKS
A
MUTEX
THAT
IS
ALREADY
UNLOCKED
WHICH
USUALLY
IS
A
CODING
MISTAKE
MUTEX
TYPE
RELOCK
WITHOUT
UNLOCK
UNLOCK
WHEN
NOT
OWNED
UNLOCK
WHEN
UNLOCKED
DEADLOCK
UNDEFINED
UNDEFINED
RETURNS
ERROR
RETURNS
ERROR
RETURNS
ERROR
ALLOWED
RETURNS
ERROR
RETURNS
ERROR
UNDEFINED
UNDEFINED
UNDEFINED
FIGURE
MUTEX
TYPE
BEHAVIOR
WE
CAN
USE
THE
FUNCTION
TO
GET
THE
MUTEX
TYPE
ATTRIBUTE
TO
CHANGE
THE
ATTRIBUTE
WE
CAN
USE
THE
FUNCTION
RECALL
FROM
SECTION
THAT
A
MUTEX
IS
USED
TO
PROTECT
THE
CONDITION
THAT
IS
ASSOCIATED
WITH
A
CONDITION
VARIABLE
BEFORE
BLOCKING
THE
THREAD
THE
AND
THE
FUNCTIONS
RELEASE
THE
MUTEX
ASSOCIATED
WITH
THE
CONDITION
THIS
ALLOWS
OTHER
THREADS
TO
ACQUIRE
THE
MUTEX
CHANGE
THE
CONDITION
RELEASE
THE
MUTEX
AND
SIGNAL
THE
CONDITION
VARIABLE
SINCE
THE
MUTEX
MUST
BE
HELD
TO
CHANGE
THE
CONDITION
IT
IS
NOT
A
GOOD
IDEA
TO
USE
A
RECURSIVE
MUTEX
IF
A
RECURSIVE
MUTEX
IS
LOCKED
MULTIPLE
TIMES
AND
USED
IN
A
CALL
TO
THE
CONDITION
CAN
NEVER
BE
SATISFIED
BECAUSE
THE
UNLOCK
DONE
BY
DOESN
T
RELEASE
THE
MUTEX
RECURSIVE
MUTEXES
ARE
USEFUL
WHEN
YOU
NEED
TO
ADAPT
EXISTING
SINGLE
THREADED
INTERFACES
TO
A
MULTITHREADED
ENVIRONMENT
BUT
CAN
T
CHANGE
THE
INTERFACES
TO
YOUR
FUNCTIONS
BECAUSE
OF
COMPATIBILITY
CONSTRAINTS
HOWEVER
USING
RECURSIVE
LOCKS
CAN
BE
TRICKY
AND
THEY
SHOULD
BE
USED
ONLY
WHEN
NO
OTHER
SOLUTION
IS
POSSIBLE
EXAMPLE
FIGURE
ILLUSTRATES
A
SITUATION
IN
WHICH
A
RECURSIVE
MUTEX
MIGHT
SEEM
TO
SOLVE
A
CONCURRENCY
PROBLEM
ASSUME
THAT
AND
ARE
EXISTING
FUNCTIONS
IN
A
LIBRARY
WHOSE
INTERFACES
CAN
T
BE
CHANGED
BECAUSE
APPLICATIONS
EXIST
THAT
CALL
THEM
AND
THOSE
APPLICATIONS
CAN
T
BE
CHANGED
TO
KEEP
THE
INTERFACES
THE
SAME
WE
EMBED
A
MUTEX
IN
THE
DATA
STRUCTURE
WHOSE
ADDRESS
X
IS
PASSED
IN
AS
AN
ARGUMENT
THIS
IS
POSSIBLE
ONLY
IF
WE
HAVE
PROVIDED
AN
ALLOCATOR
FUNCTION
FOR
THE
STRUCTURE
SO
THE
APPLICATION
DOESN
T
KNOW
ABOUT
ITS
SIZE
ASSUMING
WE
MUST
INCREASE
ITS
SIZE
WHEN
WE
ADD
A
MUTEX
TO
IT
THIS
IS
ALSO
POSSIBLE
IF
WE
ORIGINALLY
DEFINED
THE
STRUCTURE
WITH
ENOUGH
PADDING
TO
ALLOW
US
NOW
TO
REPLACE
SOME
PAD
FIELDS
WITH
A
MUTEX
UNFORTUNATELY
MOST
PROGRAMMERS
ARE
UNSKILLED
AT
PREDICTING
THE
FUTURE
SO
THIS
IS
NOT
A
COMMON
PRACTICE
IF
BOTH
AND
MUST
MANIPULATE
THE
STRUCTURE
AND
IT
IS
POSSIBLE
TO
ACCESS
IT
FROM
MORE
THAN
ONE
THREAD
AT
A
TIME
THEN
AND
MUST
LOCK
THE
MUTEX
BEFORE
MANIPULATING
THE
STRUCTURE
IF
MUST
CALL
WE
WILL
DEADLOCK
IF
THE
MUTEX
TYPE
IS
NOT
RECURSIVE
WE
COULD
AVOID
USING
A
RECURSIVE
MUTEX
IF
WE
COULD
RELEASE
X
X
LOCK
X
X
LOCK
X
X
LOCK
X
LOCK
FIGURE
RECURSIVE
LOCKING
OPPORTUNITY
THE
MUTEX
BEFORE
CALLING
AND
REACQUIRE
IT
AFTER
RETURNS
BUT
THIS
APPROACH
OPENS
A
WINDOW
WHERE
ANOTHER
THREAD
CAN
POSSIBLY
GRAB
CONTROL
OF
THE
MUTEX
AND
CHANGE
THE
DATA
STRUCTURE
IN
THE
MIDDLE
OF
THIS
MAY
NOT
BE
ACCEPTABLE
DEPENDING
ON
WHAT
PROTECTION
THE
MUTEX
IS
INTENDED
TO
PROVIDE
FIGURE
SHOWS
AN
ALTERNATIVE
TO
USING
A
RECURSIVE
MUTEX
IN
THIS
CASE
WE
CAN
LEAVE
THE
INTERFACES
TO
AND
UNCHANGED
AND
AVOID
A
RECURSIVE
MUTEX
BY
PROVIDING
A
PRIVATE
VERSION
OF
CALLED
TO
CALL
WE
MUST
HOLD
THE
MUTEX
EMBEDDED
IN
THE
DATA
STRUCTURE
WHOSE
ADDRESS
WE
PASS
AS
THE
ARGUMENT
THE
BODY
OF
CONTAINS
A
COPY
OF
AND
NOW
SIMPLY
ACQUIRES
THE
MUTEX
CALLS
AND
THEN
RELEASES
THE
MUTEX
IF
WE
DIDN
T
HAVE
TO
LEAVE
THE
INTERFACES
TO
THE
LIBRARY
FUNCTIONS
UNCHANGED
WE
COULD
HAVE
ADDED
A
SECOND
PARAMETER
TO
EACH
FUNCTION
TO
INDICATE
WHETHER
THE
STRUCTURE
IS
LOCKED
BY
THE
CALLER
IT
IS
USUALLY
BETTER
TO
LEAVE
THE
INTERFACES
UNCHANGED
IF
WE
CAN
HOWEVER
INSTEAD
OF
POLLUTING
THEM
WITH
IMPLEMENTATION
ARTIFACTS
THE
STRATEGY
OF
PROVIDING
LOCKED
AND
UNLOCKED
VERSIONS
OF
FUNCTIONS
IS
USUALLY
APPLICABLE
IN
SIMPLE
SITUATIONS
IN
MORE
COMPLEX
SITUATIONS
SUCH
AS
WHEN
THE
LIBRARY
NEEDS
TO
CALL
A
FUNCTION
OUTSIDE
THE
LIBRARY
WHICH
THEN
MIGHT
CALL
BACK
INTO
THE
LIBRARY
WE
NEED
TO
RELY
ON
RECURSIVE
LOCKS
X
X
LOCK
X
X
X
X
LOCK
FIGURE
AVOIDING
A
RECURSIVE
LOCKING
OPPORTUNITY
EXAMPLE
THE
PROGRAM
IN
FIGURE
ILLUSTRATES
ANOTHER
SITUATION
IN
WHICH
A
RECURSIVE
MUTEX
IS
NECESSARY
HERE
WE
HAVE
A
TIMEOUT
FUNCTION
THAT
ALLOWS
US
TO
SCHEDULE
ANOTHER
FUNCTION
TO
BE
RUN
AT
SOME
TIME
IN
THE
FUTURE
ASSUMING
THAT
THREADS
ARE
AN
INEXPENSIVE
RESOURCE
WE
CAN
CREATE
A
THREAD
FOR
EACH
PENDING
TIMEOUT
THE
THREAD
WAITS
UNTIL
THE
TIME
HAS
BEEN
REACHED
AND
THEN
IT
CALLS
THE
FUNCTION
WE
VE
REQUESTED
THE
PROBLEM
ARISES
WHEN
WE
CAN
T
CREATE
A
THREAD
OR
WHEN
THE
SCHEDULED
TIME
TO
RUN
THE
FUNCTION
HAS
ALREADY
PASSED
IN
THESE
CASES
WE
SIMPLY
CALL
THE
REQUESTED
FUNCTION
NOW
FROM
THE
CURRENT
CONTEXT
SINCE
THE
FUNCTION
ACQUIRES
THE
SAME
LOCK
THAT
WE
CURRENTLY
HOLD
A
DEADLOCK
WILL
OCCUR
UNLESS
THE
LOCK
IS
RECURSIVE
INCLUDE
APUE
H
INCLUDE
PTHREAD
H
INCLUDE
TIME
H
INCLUDE
SYS
TIME
H
EXTERN
INT
MAKETHREAD
VOID
VOID
VOID
STRUCT
VOID
VOID
FUNCTION
VOID
ARGUMENT
STRUCT
TIMESPEC
TIME
TO
WAIT
DEFINE
SECTONSEC
SECONDS
TO
NANOSECONDS
IF
DEFINED
DEFINED
BSD
DEFINE
ID
FL
REQ
REM
NANOSLEEP
REQ
REM
ENDIF
IFNDEF
DEFINE
DEFINE
USECTONSEC
MICROSECONDS
TO
NANOSECONDS
VOID
INT
ID
STRUCT
TIMESPEC
TSP
STRUCT
TIMEVAL
TV
GETTIMEOFDAY
TV
NULL
TSP
TV
TSP
TV
USECTONSEC
ENDIF
VOID
VOID
ARG
STRUCT
TIP
TIP
STRUCT
ARG
TIP
NULL
TIP
TIP
FREE
ARG
RETURN
VOID
TIMEOUT
CONST
STRUCT
TIMESPEC
WHEN
VOID
FUNC
VOID
VOID
ARG
STRUCT
TIMESPEC
NOW
STRUCT
TIP
INT
ERR
NOW
IF
WHEN
NOW
WHEN
NOW
WHEN
NOW
TIP
MALLOC
SIZEOF
STRUCT
IF
TIP
NULL
TIP
FUNC
TIP
ARG
TIP
WHEN
NOW
IF
WHEN
NOW
TIP
WHEN
NOW
ELSE
TIP
TIP
SECTONSEC
NOW
WHEN
ERR
MAKETHREAD
VOID
TIP
IF
ERR
RETURN
ELSE
FREE
TIP
WE
GET
HERE
IF
A
WHEN
NOW
OR
B
MALLOC
FAILS
OR
C
WE
CAN
T
MAKE
A
THREAD
SO
WE
JUST
CALL
THE
FUNCTION
NOW
FUNC
ARG
ATTR
MUTEX
VOID
RETRY
VOID
ARG
INT
MUTEX
PERFORM
RETRY
STEPS
MUTEX
MAIN
VOID
INT
ERR
CONDITION
ARG
STRUCT
TIMESPEC
WHEN
IF
ERR
ATTR
ERR
FAILED
IF
ERR
ATTR
ERR
CAN
T
SET
RECURSIVE
TYPE
IF
ERR
MUTEX
ATTR
ERR
CAN
T
CREATE
RECURSIVE
MUTEX
CONTINUE
PROCESSING
MUTEX
CHECK
THE
CONDITION
UNDER
THE
PROTECTION
OF
A
LOCK
TO
MAKE
THE
CHECK
AND
THE
CALL
TO
TIMEOUT
ATOMIC
IF
CONDITION
CALCULATE
THE
ABSOLUTE
TIME
WHEN
WE
WANT
TO
RETRY
WHEN
WHEN
SECONDS
FROM
NOW
TIMEOUT
WHEN
RETRY
VOID
UNSIGNED
LONG
ARG
MUTEX
CONTINUE
PROCESSING
EXIT
FIGURE
USING
A
RECURSIVE
MUTEX
WE
USE
THE
MAKETHREAD
FUNCTION
FROM
FIGURE
TO
CREATE
A
THREAD
IN
THE
DETACHED
STATE
BECAUSE
THE
FUNC
FUNCTION
ARGUMENT
PASSED
TO
THE
TIMEOUT
FUNCTION
WILL
RUN
IN
THE
FUTURE
WE
DON
T
WANT
TO
WAIT
AROUND
FOR
THE
THREAD
TO
COMPLETE
WE
COULD
CALL
SLEEP
TO
WAIT
FOR
THE
TIMEOUT
TO
EXPIRE
BUT
THAT
GIVES
US
ONLY
SECOND
GRANULARITY
IF
WE
WANT
TO
WAIT
FOR
SOME
TIME
OTHER
THAN
AN
INTEGRAL
NUMBER
OF
SECONDS
WE
NEED
TO
USE
NANOSLEEP
OR
BOTH
OF
WHICH
ALLOW
US
TO
SLEEP
AT
HIGHER
RESOLUTION
ON
SYSTEMS
THAT
DON
T
DEFINE
WE
DEFINE
IN
TERMS
OF
NANOSLEEP
HOWEVER
FREEBSD
DEFINES
THIS
SYMBOL
TO
SUPPORT
AND
BUT
DOESN
T
SUPPORT
ONLY
LINUX
AND
SOLARIS
CURRENTLY
SUPPORT
ADDITIONALLY
ON
SYSTEMS
THAT
DON
T
DEFINE
WE
PROVIDE
OUR
OWN
IMPLEMENTATION
OF
THAT
CALLS
GETTIMEOFDAY
AND
TRANSLATES
MICROSECONDS
TO
NANOSECONDS
THE
CALLER
OF
TIMEOUT
NEEDS
TO
HOLD
A
MUTEX
TO
CHECK
THE
CONDITION
AND
TO
SCHEDULE
THE
RETRY
FUNCTION
AS
AN
ATOMIC
OPERATION
THE
RETRY
FUNCTION
WILL
TRY
TO
LOCK
THE
SAME
MUTEX
UNLESS
THE
MUTEX
IS
RECURSIVE
A
DEADLOCK
WILL
OCCUR
IF
THE
TIMEOUT
FUNCTION
CALLS
RETRY
DIRECTLY
READER
WRITER
LOCK
ATTRIBUTES
READER
WRITER
LOCKS
ALSO
HAVE
ATTRIBUTES
SIMILAR
TO
MUTEXES
WE
USE
TO
INITIALIZE
A
STRUCTURE
AND
TO
DEINITIALIZE
THE
STRUCTURE
THE
ONLY
ATTRIBUTE
SUPPORTED
FOR
READER
WRITER
LOCKS
IS
THE
PROCESS
SHARED
ATTRIBUTE
IT
IS
IDENTICAL
TO
THE
MUTEX
PROCESS
SHARED
ATTRIBUTE
JUST
AS
WITH
THE
MUTEX
PROCESS
SHARED
ATTRIBUTES
A
PAIR
OF
FUNCTIONS
IS
PROVIDED
TO
GET
AND
SET
THE
PROCESS
SHARED
ATTRIBUTES
OF
READER
WRITER
LOCKS
ALTHOUGH
POSIX
DEFINES
ONLY
ONE
READER
WRITER
LOCK
ATTRIBUTE
IMPLEMENTATIONS
ARE
FREE
TO
DEFINE
ADDITIONAL
NONSTANDARD
ONES
CONDITION
VARIABLE
ATTRIBUTES
THE
SINGLE
UNIX
SPECIFICATION
CURRENTLY
DEFINES
TWO
ATTRIBUTES
FOR
CONDITION
VARIABLES
THE
PROCESS
SHARED
ATTRIBUTE
AND
THE
CLOCK
ATTRIBUTE
AS
WITH
THE
OTHER
ATTRIBUTE
OBJECTS
A
PAIR
OF
FUNCTIONS
INITIALIZE
AND
DEINITIALIZE
CONDITION
VARIABLE
ATTRIBUTE
OBJECTS
THE
PROCESS
SHARED
ATTRIBUTE
IS
THE
SAME
AS
WITH
THE
OTHER
SYNCHRONIZATION
ATTRIBUTES
IT
CONTROLS
WHETHER
CONDITION
VARIABLES
CAN
BE
USED
BY
THREADS
WITHIN
A
SINGLE
PROCESS
ONLY
OR
FROM
WITHIN
MULTIPLE
PROCESSES
TO
FIND
THE
CURRENT
VALUE
OF
THE
PROCESS
SHARED
ATTRIBUTE
WE
USE
THE
FUNCTION
TO
SET
ITS
VALUE
WE
USE
THE
FUNCTION
THE
CLOCK
ATTRIBUTE
CONTROLS
WHICH
CLOCK
IS
USED
WHEN
EVALUATING
THE
TIMEOUT
ARGUMENT
TSPTR
OF
THE
FUNCTION
THE
LEGAL
VALUES
ARE
THE
CLOCK
IDS
LISTED
IN
FIGURE
WE
CAN
USE
THE
FUNCTION
TO
RETRIEVE
THE
CLOCK
ID
THAT
WILL
BE
USED
BY
THE
FUNCTION
FOR
THE
CONDITION
VARIABLE
THAT
WAS
INITIALIZED
WITH
THE
OBJECT
WE
CAN
CHANGE
THE
CLOCK
ID
WITH
THE
FUNCTION
CURIOUSLY
THE
SINGLE
UNIX
SPECIFICATION
DOESN
T
DEFINE
THE
CLOCK
ATTRIBUTE
FOR
ANY
OF
THE
OTHER
ATTRIBUTE
OBJECTS
THAT
HAVE
A
WAIT
FUNCTION
WITH
A
TIMEOUT
BARRIER
ATTRIBUTES
BARRIERS
HAVE
ATTRIBUTES
TOO
WE
CAN
USE
THE
FUNCTION
TO
INITIALIZE
A
BARRIER
ATTRIBUTES
OBJECT
AND
THE
FUNCTION
TO
DEINITIALIZE
A
BARRIER
ATTRIBUTES
OBJECT
THE
ONLY
BARRIER
ATTRIBUTE
CURRENTLY
DEFINED
IS
THE
PROCESS
SHARED
ATTRIBUTE
WHICH
CONTROLS
WHETHER
A
BARRIER
CAN
BE
USED
BY
THREADS
FROM
MULTIPLE
PROCESSES
OR
ONLY
FROM
WITHIN
THE
PROCESS
THAT
INITIALIZED
THE
BARRIER
AS
WITH
THE
OTHER
ATTRIBUTE
OBJECTS
WE
HAVE
ONE
FUNCTION
TO
GET
THE
ATTRIBUTE
VALUE
AND
ONE
FUNCTION
TO
SET
THE
VALUE
THE
VALUE
OF
THE
PROCESS
SHARED
ATTRIBUTE
CAN
BE
EITHER
ACCESSIBLE
TO
THREADS
FROM
MULTIPLE
PROCESSES
OR
ACCESSIBLE
TO
ONLY
THREADS
IN
THE
PROCESS
THAT
INITIALIZED
THE
BARRIER
REENTRANCY
WE
DISCUSSED
REENTRANT
FUNCTIONS
AND
SIGNAL
HANDLERS
IN
SECTION
THREADS
ARE
SIMILAR
TO
SIGNAL
HANDLERS
WHEN
IT
COMES
TO
REENTRANCY
IN
BOTH
CASES
MULTIPLE
THREADS
OF
CONTROL
CAN
POTENTIALLY
CALL
THE
SAME
FUNCTION
AT
THE
SAME
TIME
BASENAME
GETSERVENT
CATGETS
GETDATE
GETUTXENT
CRYPT
GETENV
GETUTXID
PUTENV
GETGRENT
GETUTXLINE
PUTUTXLINE
GETGRGID
GMTIME
RAND
GETGRNAM
HCREATE
READDIR
GETHOSTENT
HDESTROY
SETENV
GETLOGIN
HSEARCH
SETGRENT
GETNETBYADDR
SETKEY
GETNETBYNAME
SETPWENT
GETNETENT
LGAMMA
SETUTXENT
GETOPT
LGAMMAF
STRERROR
DIRNAME
GETPROTOBYNAME
LGAMMAL
STRSIGNAL
DLERROR
GETPROTOBYNUMBER
LOCALECONV
STRTOK
GETPROTOENT
LOCALTIME
SYSTEM
ENCRYPT
GETPWENT
TTYNAME
ENDGRENT
GETPWNAM
UNSETENV
ENDPWENT
GETPWUID
NFTW
WCSTOMBS
ENDUTXENT
GETSERVBYNAME
WCTOMB
GETSERVBYPORT
PTSNAME
FIGURE
FUNCTIONS
NOT
GUARANTEED
TO
BE
THREAD
SAFE
BY
POSIX
IF
A
FUNCTION
CAN
BE
SAFELY
CALLED
BY
MULTIPLE
THREADS
AT
THE
SAME
TIME
WE
SAY
THAT
THE
FUNCTION
IS
THREAD
SAFE
ALL
FUNCTIONS
DEFINED
IN
THE
SINGLE
UNIX
SPECIFICATION
ARE
GUARANTEED
TO
BE
THREAD
SAFE
EXCEPT
THOSE
LISTED
IN
FIGURE
IN
ADDITION
THE
CTERMID
AND
TMPNAM
FUNCTIONS
ARE
NOT
GUARANTEED
TO
BE
THREAD
SAFE
IF
THEY
ARE
PASSED
A
NULL
POINTER
SIMILARLY
THERE
IS
NO
GUARANTEE
THAT
WCRTOMB
AND
WCSRTOMBS
ARE
THREAD
SAFE
WHEN
THEY
ARE
PASSED
A
NULL
POINTER
FOR
THEIR
ARGUMENT
IMPLEMENTATIONS
THAT
SUPPORT
THREAD
SAFE
FUNCTIONS
WILL
DEFINE
THE
SYMBOL
IN
UNISTD
H
APPLICATIONS
CAN
ALSO
USE
THE
ARGUMENT
WITH
SYSCONF
TO
CHECK
FOR
SUPPORT
OF
THREAD
SAFE
FUNCTIONS
AT
RUNTIME
PRIOR
TO
VERSION
OF
THE
SINGLE
UNIX
SPECIFICATION
ALL
XSI
CONFORMING
IMPLEMENTATIONS
WERE
REQUIRED
TO
SUPPORT
THREAD
SAFE
FUNCTIONS
WITH
HOWEVER
THREAD
SAFE
FUNCTION
SUPPORT
IS
NOW
REQUIRED
FOR
AN
IMPLEMENTATION
TO
BE
CONSIDERED
POSIX
CONFORMING
WITH
THREAD
SAFE
FUNCTIONS
IMPLEMENTATIONS
PROVIDE
ALTERNATIVE
THREAD
SAFE
VERSIONS
OF
SOME
OF
THE
POSIX
FUNCTIONS
THAT
AREN
T
THREAD
SAFE
FIGURE
LISTS
THE
THREAD
SAFE
VERSIONS
OF
THESE
FUNCTIONS
THE
FUNCTIONS
HAVE
THE
SAME
NAMES
AS
THEIR
NON
THREAD
SAFE
RELATIVES
BUT
WITH
AN
APPENDED
AT
THE
END
OF
THE
NAME
SIGNIFYING
THAT
THESE
VERSIONS
ARE
REENTRANT
MANY
FUNCTIONS
ARE
NOT
THREAD
SAFE
BECAUSE
THEY
RETURN
DATA
STORED
IN
A
STATIC
MEMORY
BUFFER
THEY
ARE
MADE
THREAD
SAFE
BY
CHANGING
THEIR
INTERFACES
TO
REQUIRE
THAT
THE
CALLER
PROVIDE
ITS
OWN
BUFFER
FIGURE
ALTERNATIVE
THREAD
SAFE
FUNCTIONS
IF
A
FUNCTION
IS
REENTRANT
WITH
RESPECT
TO
MULTIPLE
THREADS
WE
SAY
THAT
IT
IS
THREAD
SAFE
THIS
DOESN
T
TELL
US
HOWEVER
WHETHER
THE
FUNCTION
IS
REENTRANT
WITH
RESPECT
TO
SIGNAL
HANDLERS
WE
SAY
THAT
A
FUNCTION
THAT
IS
SAFE
TO
BE
REENTERED
FROM
AN
ASYNCHRONOUS
SIGNAL
HANDLER
IS
ASYNC
SIGNAL
SAFE
WE
SAW
THE
ASYNC
SIGNAL
SAFE
FUNCTIONS
IN
FIGURE
WHEN
WE
DISCUSSED
REENTRANT
FUNCTIONS
IN
SECTION
IN
ADDITION
TO
THE
FUNCTIONS
LISTED
IN
FIGURE
POSIX
PROVIDES
A
WAY
TO
MANAGE
FILE
OBJECTS
IN
A
THREAD
SAFE
WAY
YOU
CAN
USE
FLOCKFILE
AND
FTRYLOCKFILE
TO
OBTAIN
A
LOCK
ASSOCIATED
WITH
A
GIVEN
FILE
OBJECT
THIS
LOCK
IS
RECURSIVE
YOU
CAN
ACQUIRE
IT
AGAIN
WHILE
YOU
ALREADY
HOLD
IT
WITHOUT
DEADLOCKING
ALTHOUGH
THE
EXACT
IMPLEMENTATION
OF
THE
LOCK
IS
UNSPECIFIED
ALL
STANDARD
I
O
ROUTINES
THAT
MANIPULATE
FILE
OBJECTS
ARE
REQUIRED
TO
BEHAVE
AS
IF
THEY
CALL
FLOCKFILE
AND
FUNLOCKFILE
INTERNALLY
ALTHOUGH
THE
STANDARD
I
O
ROUTINES
MIGHT
BE
IMPLEMENTED
TO
BE
THREAD
SAFE
FROM
THE
PERSPECTIVE
OF
THEIR
OWN
INTERNAL
DATA
STRUCTURES
IT
IS
STILL
USEFUL
TO
EXPOSE
THE
LOCKING
TO
APPLICATIONS
THIS
ALLOWS
APPLICATIONS
TO
COMPOSE
MULTIPLE
CALLS
TO
STANDARD
I
O
FUNCTIONS
INTO
ATOMIC
SEQUENCES
OF
COURSE
WHEN
DEALING
WITH
MULTIPLE
FILE
OBJECTS
YOU
NEED
TO
BEWARE
OF
POTENTIAL
DEADLOCKS
AND
TO
ORDER
YOUR
LOCKS
CAREFULLY
IF
THE
STANDARD
I
O
ROUTINES
ACQUIRE
THEIR
OWN
LOCKS
THEN
WE
CAN
RUN
INTO
SERIOUS
PERFORMANCE
DEGRADATION
WHEN
DOING
CHARACTER
AT
A
TIME
I
O
IN
THIS
SITUATION
WE
END
UP
ACQUIRING
AND
RELEASING
A
LOCK
FOR
EVERY
CHARACTER
READ
OR
WRITTEN
TO
AVOID
THIS
OVERHEAD
UNLOCKED
VERSIONS
OF
THE
CHARACTER
BASED
STANDARD
I
O
ROUTINES
ARE
AVAILABLE
THESE
FOUR
FUNCTIONS
SHOULD
NOT
BE
CALLED
UNLESS
THEY
ARE
SURROUNDED
BY
CALLS
TO
FLOCKFILE
OR
FTRYLOCKFILE
AND
FUNLOCKFILE
OTHERWISE
UNPREDICTABLE
RESULTS
CAN
OCCUR
I
E
THE
TYPES
OF
PROBLEMS
THAT
RESULT
FROM
UNSYNCHRONIZED
ACCESS
TO
DATA
BY
MULTIPLE
THREADS
OF
CONTROL
ONCE
YOU
LOCK
THE
FILE
OBJECT
YOU
CAN
MAKE
MULTIPLE
CALLS
TO
THESE
FUNCTIONS
BEFORE
RELEASING
THE
LOCK
THIS
AMORTIZES
THE
LOCKING
OVERHEAD
ACROSS
THE
AMOUNT
OF
DATA
READ
OR
WRITTEN
EXAMPLE
FIGURE
SHOWS
A
POSSIBLE
IMPLEMENTATION
OF
GETENV
SECTION
THIS
VERSION
IS
NOT
REENTRANT
IF
TWO
THREADS
CALL
IT
AT
THE
SAME
TIME
THEY
WILL
SEE
INCONSISTENT
RESULTS
BECAUSE
THE
STRING
RETURNED
IS
STORED
IN
A
SINGLE
STATIC
BUFFER
THAT
IS
SHARED
BY
ALL
THREADS
CALLING
GETENV
WE
SHOW
A
REENTRANT
VERSION
OF
GETENV
IN
FIGURE
THIS
VERSION
IS
CALLED
IT
USES
THE
FUNCTION
TO
ENSURE
THAT
THE
FUNCTION
IS
CALLED
ONLY
ONCE
PER
PROCESS
REGARDLESS
OF
HOW
MANY
THREADS
MIGHT
RACE
TO
CALL
AT
THE
SAME
TIME
WE
LL
HAVE
MORE
TO
SAY
ABOUT
THE
FUNCTION
IN
SECTION
TO
MAKE
REENTRANT
WE
CHANGED
THE
INTERFACE
SO
THAT
THE
CALLER
MUST
PROVIDE
ITS
OWN
BUFFER
THUS
EACH
THREAD
CAN
USE
A
DIFFERENT
BUFFER
TO
AVOID
INTERFERING
WITH
THE
OTHERS
NOTE
HOWEVER
THAT
THIS
IS
NOT
ENOUGH
TO
MAKE
THREAD
SAFE
TO
MAKE
THREAD
SAFE
WE
NEED
TO
PROTECT
AGAINST
CHANGES
TO
THE
ENVIRONMENT
WHILE
WE
ARE
SEARCHING
FOR
THE
REQUESTED
STRING
WE
CAN
USE
A
MUTEX
TO
SERIALIZE
ACCESS
TO
THE
ENVIRONMENT
LIST
BY
AND
PUTENV
WE
COULD
HAVE
USED
A
READER
WRITER
LOCK
TO
ALLOW
MULTIPLE
CONCURRENT
CALLS
TO
BUT
THE
ADDED
CONCURRENCY
PROBABLY
WOULDN
T
IMPROVE
THE
PERFORMANCE
OF
OUR
PROGRAM
BY
VERY
MUCH
FOR
TWO
REASONS
FIRST
THE
ENVIRONMENT
LIST
USUALLY
ISN
T
VERY
LONG
SO
WE
WON
T
HOLD
THE
MUTEX
FOR
TOO
LONG
WHILE
WE
SCAN
THE
LIST
SECOND
CALLS
TO
GETENV
AND
PUTENV
ARE
INFREQUENT
SO
IF
WE
IMPROVE
THEIR
PERFORMANCE
WE
WON
T
AFFECT
THE
OVERALL
PERFORMANCE
OF
THE
PROGRAM
VERY
MUCH
EVEN
THOUGH
WE
CAN
MAKE
THREAD
SAFE
THAT
DOESN
T
MEAN
THAT
IT
IS
REENTRANT
WITH
RESPECT
TO
SIGNAL
HANDLERS
IF
WE
WERE
TO
USE
A
NONRECURSIVE
MUTEX
WE
WOULD
RUN
THE
RISK
THAT
A
THREAD
WOULD
DEADLOCK
ITSELF
IF
IT
CALLED
FROM
A
SIGNAL
HANDLER
IF
THE
SIGNAL
HANDLER
INTERRUPTED
THE
THREAD
WHILE
IT
WAS
EXECUTING
WE
WOULD
ALREADY
BE
HOLDING
LOCKED
SO
ANOTHER
ATTEMPT
TO
LOCK
IT
WOULD
BLOCK
CAUSING
THE
THREAD
TO
DEADLOCK
THUS
WE
MUST
USE
A
RECURSIVE
MUTEX
TO
PREVENT
OTHER
THREADS
FROM
CHANGING
THE
DATA
STRUCTURES
WHILE
WE
LOOK
AT
THEM
AND
TO
PREVENT
DEADLOCKS
FROM
SIGNAL
HANDLERS
THE
PROBLEM
IS
THAT
THE
PTHREAD
FUNCTIONS
ARE
NOT
GUARANTEED
TO
BE
ASYNC
SIGNAL
SAFE
SO
WE
CAN
T
USE
THEM
TO
MAKE
ANOTHER
FUNCTION
ASYNC
SIGNAL
SAFE
PROGRAMMERS
ON
GNU
LINUX
HAVE
TWO
SETS
OF
INPUT
OUTPUT
FUNCTIONS
AT
THEIR
DISPOSAL
THE
STANDARD
C
LIBRARY
PROVIDES
I
O
FUNCTIONS
PRINTF
FOPEN
AND
SO
ON
THE
LINUX
KERNEL
ITSELF
PROVIDES
ANOTHER
SET
OF
I
O
OPERATIONS
THAT
OPERATE
AT
A
LOWER
LEVEL
THAN
THE
C
LIBRARY
FUNCTIONS
BECAUSE
THIS
BOOK
IS
FOR
PEOPLE
WHO
ALREADY
KNOW
THE
C
LANGUAGE
WE
LL
ASSUME
THAT
YOU
HAVE
ENCOUNTERED
AND
KNOW
HOW
TO
USE
THE
C
LIBRARY
I
O
FUNCTIONS
OFTEN
THERE
ARE
GOOD
REASONS
TO
USE
LINUX
LOW
LEVEL
I
O
FUNCTIONS
MANY
OF
THESE
ARE
KERNEL
SYSTEM
AND
PROVIDE
THE
MOST
DIRECT
ACCESS
TO
UNDERLYING
SYSTEM
CAPA
BILITIES
THAT
IS
AVAILABLE
TO
APPLICATION
PROGRAMS
IN
FACT
THE
STANDARD
C
LIBRARY
I
O
ROUTINES
ARE
IMPLEMENTED
ON
TOP
OF
THE
LINUX
LOW
LEVEL
I
O
SYSTEM
CALLS
USING
THE
LATTER
IS
USUALLY
THE
MOST
EFFICIENT
WAY
TO
PERFORM
INPUT
AND
OUTPUT
OPERATIONS
AND
IS
SOMETIMES
MORE
CONVENIENT
TOO
THE
C
STANDARD
LIBRARY
PROVIDES
IOSTREAMS
WITH
SIMILAR
FUNCTIONALITY
THE
STANDARD
C
LIBRARY
IS
ALSO
AVAILABLE
IN
THE
C
LANGUAGE
SEE
CHAPTER
LINUX
SYSTEM
CALLS
FOR
AN
EXPLANATION
OF
THE
DIFFERENCE
BETWEEN
A
SYSTEM
CALL
AND
AN
ORDINARY
FUNCTION
CALL
THROUGHOUT
THIS
BOOK
WE
ASSUME
THAT
YOU
RE
FAMILIAR
WITH
THE
CALLS
DESCRIBED
IN
THIS
APPENDIX
YOU
MAY
ALREADY
BE
FAMILIAR
WITH
THEM
BECAUSE
THEY
RE
NEARLY
THE
SAME
AS
THOSE
PROVIDED
ON
OTHER
UNIX
AND
UNIX
LIKE
OPERATING
SYSTEMS
AND
ON
THE
PLATFORM
AS
WELL
IF
YOU
RE
NOT
FAMILIAR
WITH
THEM
HOWEVER
READ
ON
YOU
LL
FIND
THE
REST
OF
THE
BOOK
MUCH
EASIER
TO
UNDERSTAND
IF
YOU
FAMILIARIZE
YOURSELF
WITH
THIS
MATERIAL
FIRST
B
READING
AND
WRITING
DATA
THE
FIRST
I
O
FUNCTION
YOU
LIKELY
ENCOUNTERED
WHEN
YOU
FIRST
LEARNED
THE
C
LANGUAGE
WAS
PRINTF
THIS
FORMATS
A
TEXT
STRING
AND
THEN
PRINTS
IT
TO
STANDARD
OUTPUT
THE
GENER
ALIZED
VERSION
FPRINTF
CAN
PRINT
THE
TEXT
TO
A
STREAM
OTHER
THAN
STANDARD
OUTPUT
A
STREAM
IS
REPRESENTED
BY
A
FILE
POINTER
YOU
OBTAIN
A
FILE
POINTER
BY
OPENING
A
FILE
WITH
FOPEN
WHEN
YOU
RE
DONE
YOU
CAN
CLOSE
IT
WITH
FCLOSE
IN
ADDITION
TO
FPRINTF
YOU
CAN
USE
SUCH
FUNCTIONS
AS
FPUTC
FPUTS
AND
FWRITE
TO
WRITE
DATA
TO
THE
STREAM
OR
FSCANF
FGETC
FGETS
AND
FREAD
TO
READ
DATA
WITH
THE
LINUX
LOW
LEVEL
I
O
OPERATIONS
YOU
USE
A
HANDLE
CALLED
A
FILE
DESCRIPTOR
INSTEAD
OF
A
FILE
POINTER
A
FILE
DESCRIPTOR
IS
AN
INTEGER
VALUE
THAT
REFERS
TO
A
PARTICU
LAR
INSTANCE
OF
AN
OPEN
FILE
IN
A
SINGLE
PROCESS
IT
CAN
BE
OPEN
FOR
READING
FOR
WRITING
OR
FOR
BOTH
READING
AND
WRITING
A
FILE
DESCRIPTOR
DOESN
T
HAVE
TO
REFER
TO
AN
OPEN
FILE
IT
CAN
REPRESENT
A
CONNECTION
WITH
ANOTHER
SYSTEM
COMPONENT
THAT
IS
CAPABLE
OF
SEND
ING
OR
RECEIVING
DATA
FOR
EXAMPLE
A
CONNECTION
TO
A
HARDWARE
DEVICE
IS
REPRESENTED
BY
A
FILE
DESCRIPTOR
SEE
CHAPTER
DEVICES
AS
IS
AN
OPEN
SOCKET
SEE
CHAPTER
INTERPROCESS
COMMUNICATION
SECTION
SOCKETS
OR
ONE
END
OF
A
PIPE
SEE
SECTION
PIPES
INCLUDE
THE
HEADER
FILES
FCNTL
H
SYS
TYPES
H
SYS
STAT
H
AND
UNISTD
H
IF
YOU
USE
ANY
OF
THE
LOW
LEVEL
I
O
FUNCTIONS
DESCRIBED
HERE
B
OPENING
A
FILE
TO
OPEN
A
FILE
AND
PRODUCE
A
FILE
DESCRIPTOR
THAT
CAN
ACCESS
THAT
FILE
USE
THE
OPEN
CALL
IT
TAKES
AS
ARGUMENTS
THE
PATH
NAME
OF
THE
FILE
TO
OPEN
AS
A
CHARACTER
STRING
AND
FLAGS
SPECIFYING
HOW
TO
OPEN
IT
YOU
CAN
USE
OPEN
TO
CREATE
A
NEW
FILE
IF
YOU
DO
PASS
A
THIRD
ARGUMENT
THAT
SPECIFIES
THE
ACCESS
PERMISSIONS
TO
SET
FOR
THE
NEW
FILE
IF
THE
SECOND
ARGUMENT
IS
THE
FILE
IS
OPENED
FOR
READING
ONLY
AN
ERROR
WILL
RESULT
IF
YOU
SUBSEQUENTLY
TRY
TO
WRITE
TO
THE
RESULTING
FILE
DESCRIPTOR
SIMILARLY
CAUSES
THE
FILE
DESCRIPTOR
TO
BE
WRITE
ONLY
SPECIFYING
PRODUCES
A
FILE
DESCRIPTOR
THAT
CAN
BE
USED
BOTH
FOR
READING
AND
FOR
WRITING
NOTE
THAT
NOT
ALL
FILES
MAY
BE
OPENED
IN
ALL
THREE
MODES
FOR
INSTANCE
THE
PERMISSIONS
ON
A
FILE
MIGHT
FORBID
A
PARTICULAR
PROCESS
FROM
OPENING
IT
FOR
READING
OR
FOR
WRITING
A
FILE
ON
A
READ
ONLY
DEVICE
SUCH
AS
A
CD
ROM
DRIVE
MAY
NOT
BE
OPENED
FOR
WRITING
YOU
CAN
SPECIFY
ADDITIONAL
OPTIONS
BY
USING
THE
BITWISE
OR
OF
THIS
VALUE
WITH
ONE
OR
MORE
FLAGS
THESE
ARE
THE
MOST
COMMONLY
USED
VALUES
N
SPECIFY
TO
TRUNCATE
THE
OPENED
FILE
IF
IT
PREVIOUSLY
EXISTED
DATA
WRITTEN
TO
THE
FILE
DESCRIPTOR
WILL
REPLACE
PREVIOUS
CONTENTS
OF
THE
FILE
N
SPECIFY
TO
APPEND
TO
AN
EXISTING
FILE
DATA
WRITTEN
TO
THE
FILE
DESCRIPTOR
WILL
BE
ADDED
TO
THE
END
OF
THE
FILE
N
SPECIFY
TO
CREATE
A
NEW
FILE
IF
THE
FILENAME
THAT
YOU
PROVIDE
TO
OPEN
DOES
NOT
EXIST
A
NEW
FILE
WILL
BE
CREATED
PROVIDED
THAT
THE
DIRECTORY
CONTAINING
IT
EXISTS
AND
THAT
THE
PROCESS
HAS
PERMISSION
TO
CREATE
FILES
IN
THAT
DIRECTORY
IF
THE
FILE
ALREADY
EXISTS
IT
IS
OPENED
INSTEAD
N
SPECIFY
WITH
TO
FORCE
CREATION
OF
A
NEW
FILE
IF
THE
FILE
ALREADY
EXISTS
THE
OPEN
CALL
WILL
FAIL
IF
YOU
CALL
OPEN
WITH
PROVIDE
AN
ADDITIONAL
THIRD
ARGUMENT
SPECIFYING
THE
PER
MISSIONS
FOR
THE
NEW
FILE
SEE
CHAPTER
SECURITY
SECTION
FILE
SYSTEM
PERMISSIONS
FOR
A
DESCRIPTION
OF
PERMISSION
BITS
AND
HOW
TO
USE
THEM
FOR
EXAMPLE
THE
PROGRAM
IN
LISTING
B
CREATES
A
NEW
FILE
WITH
THE
FILENAME
SPECI
FIED
ON
THE
COMMAND
LINE
IT
USES
THE
FLAG
WITH
OPEN
SO
IF
THE
FILE
ALREADY
EXISTS
AN
ERROR
OCCURS
THE
NEW
FILE
IS
GIVEN
READ
AND
WRITE
PERMISSIONS
FOR
THE
OWNER
AND
OWNING
GROUP
AND
READ
PERMISSIONS
ONLY
FOR
OTHERS
IF
YOUR
UMASK
IS
SET
TO
A
NONZERO
VALUE
THE
ACTUAL
PERMISSIONS
MAY
BE
MORE
RESTRICTIVE
UMASKS
WHEN
YOU
CREATE
A
NEW
FILE
WITH
OPEN
SOME
PERMISSION
BITS
THAT
YOU
SPECIFY
MAY
BE
TURNED
OFF
THIS
IS
BECAUSE
YOUR
UMASK
IS
SET
TO
A
NONZERO
VALUE
A
PROCESS
UMASK
SPECIFIES
BITS
THAT
ARE
MASKED
OUT
OF
ALL
NEWLY
CREATED
FILES
PERMISSIONS
THE
ACTUAL
PERMISSIONS
USED
ARE
THE
BITWISE
AND
OF
THE
PERMISSIONS
YOU
SPECIFY
TO
OPEN
AND
THE
BITWISE
COMPLEMENT
OF
THE
UMASK
TO
CHANGE
YOUR
UMASK
FROM
THE
SHELL
USE
THE
UMASK
COMMAND
AND
SPECIFY
THE
NUMERICAL
VALUE
OF
THE
MASK
IN
OCTAL
NOTATION
TO
CHANGE
THE
UMASK
FOR
A
RUNNING
PROCESS
USE
THE
UMASK
CALL
PASSING
IT
THE
DESIRED
MASK
VALUE
TO
USE
FOR
SUBSEQUENT
OPEN
CALLS
FOR
EXAMPLE
CALLING
THIS
LINE
UMASK
IN
A
PROGRAM
OR
INVOKING
THIS
COMMAND
UMASK
SPECIFIES
THAT
WRITE
PERMISSIONS
FOR
GROUP
MEMBERS
AND
READ
WRITE
AND
EXECUTE
PERMISSIONS
FOR
OTHERS
WILL
ALWAYS
BE
MASKED
OUT
OF
A
NEW
FILE
PERMISSIONS
LISTING
B
CREATE
FILE
C
CREATE
A
NEW
FILE
INCLUDE
FCNTL
H
INCLUDE
STDIO
H
INCLUDE
SYS
STAT
H
INCLUDE
SYS
TYPES
H
INCLUDE
UNISTD
H
INT
MAIN
INT
ARGC
CHAR
ARGV
THE
PATH
AT
WHICH
TO
CREATE
THE
NEW
FILE
CHAR
PATH
ARGV
THE
PERMISSIONS
FOR
THE
NEW
FILE
MODE
CREATE
THE
FILE
INT
FD
OPEN
PATH
MODE
IF
FD
AN
ERROR
OCCURRED
PRINT
AN
ERROR
MESSAGE
AND
BAIL
PERROR
OPEN
RETURN
RETURN
HERE
THE
PROGRAM
IN
ACTION
CREATE
FILE
TESTFILE
LS
L
TESTFILE
RW
RW
R
SAMUEL
USERS
FEB
TESTFILE
CREATE
FILE
TESTFILE
OPEN
FILE
EXISTS
NOTE
THAT
THE
LENGTH
OF
THE
NEW
FILE
IS
BECAUSE
THE
PROGRAM
DIDN
T
WRITE
ANY
DATA
TO
IT
B
CLOSING
FILE
DESCRIPTORS
WHEN
YOU
RE
DONE
WITH
A
FILE
DESCRIPTOR
CLOSE
IT
WITH
CLOSE
IN
SOME
CASES
SUCH
AS
THE
PROGRAM
IN
LISTING
B
IT
NOT
NECESSARY
TO
CALL
CLOSE
EXPLICITLY
BECAUSE
LINUX
CLOSES
ALL
OPEN
FILE
DESCRIPTORS
WHEN
A
PROCESS
TERMINATES
THAT
IS
WHEN
THE
PROGRAM
ENDS
OF
COURSE
ONCE
YOU
CLOSE
A
FILE
DESCRIPTOR
YOU
SHOULD
NO
LONGER
USE
IT
CLOSING
A
FILE
DESCRIPTOR
MAY
CAUSE
LINUX
TO
TAKE
A
PARTICULAR
ACTION
DEPENDING
ON
THE
NATURE
OF
THE
FILE
DESCRIPTOR
FOR
EXAMPLE
WHEN
YOU
CLOSE
A
FILE
DESCRIPTOR
FOR
A
NETWORK
SOCKET
LINUX
CLOSES
THE
NETWORK
CONNECTION
BETWEEN
THE
TWO
COMPUTERS
COMMUNICATING
THROUGH
THE
SOCKET
LINUX
LIMITS
THE
NUMBER
OF
OPEN
FILE
DESCRIPTORS
THAT
A
PROCESS
MAY
HAVE
OPEN
AT
A
TIME
OPEN
FILE
DESCRIPTORS
USE
KERNEL
RESOURCES
SO
IT
GOOD
TO
CLOSE
FILE
DESCRIPTORS
WHEN
YOU
RE
DONE
WITH
THEM
A
TYPICAL
LIMIT
IS
FILE
DESCRIPTORS
PER
PROCESS
YOU
CAN
ADJUST
THIS
LIMIT
WITH
THE
SETRLIMIT
SYSTEM
CALL
SEE
SECTION
GETRLIMIT
AND
SETRLIMIT
RESOURCE
LIMITS
FOR
MORE
INFORMATION
B
WRITING
DATA
WRITE
DATA
TO
A
FILE
DESCRIPTOR
USING
THE
WRITE
CALL
PROVIDE
THE
FILE
DESCRIPTOR
A
POINTER
TO
A
BUFFER
OF
DATA
AND
THE
NUMBER
OF
BYTES
TO
WRITE
THE
FILE
DESCRIPTOR
MUST
BE
OPEN
FOR
WRITING
THE
DATA
WRITTEN
TO
THE
FILE
NEED
NOT
BE
A
CHARACTER
STRING
WRITE
COPIES
ARBITRARY
BYTES
FROM
THE
BUFFER
TO
THE
FILE
DESCRIPTOR
THE
PROGRAM
IN
LISTING
B
APPENDS
THE
CURRENT
TIME
TO
THE
FILE
SPECIFIED
ON
THE
COMMAND
LINE
IF
THE
FILE
DOESN
T
EXIST
IT
IS
CREATED
THIS
PROGRAM
ALSO
USES
THE
TIME
LOCALTIME
AND
ASCTIME
FUNCTIONS
TO
OBTAIN
AND
FORMAT
THE
CURRENT
TIME
SEE
THEIR
RESPECTIVE
MAN
PAGES
FOR
MORE
INFORMATION
LISTING
B
TIMESTAMP
C
APPEND
A
TIMESTAMP
TO
A
FILE
INCLUDE
FCNTL
H
INCLUDE
STDIO
H
INCLUDE
STRING
H
INCLUDE
SYS
STAT
H
INCLUDE
SYS
TYPES
H
INCLUDE
TIME
H
INCLUDE
UNISTD
H
RETURN
A
CHARACTER
STRING
REPRESENTING
THE
CURRENT
DATE
AND
TIME
CHAR
NOW
TIME
NULL
RETURN
ASCTIME
LOCALTIME
NOW
INT
MAIN
INT
ARGC
CHAR
ARGV
THE
FILE
TO
WHICH
TO
APPEND
THE
TIMESTAMP
CHAR
FILENAME
ARGV
GET
THE
CURRENT
TIMESTAMP
CHAR
TIMESTAMP
OPEN
THE
FILE
FOR
WRITING
IF
IT
EXISTS
APPEND
TO
IT
OTHERWISE
CREATE
A
NEW
FILE
INT
FD
OPEN
FILENAME
COMPUTE
THE
LENGTH
OF
THE
TIMESTAMP
STRING
LENGTH
STRLEN
TIMESTAMP
WRITE
THE
TIMESTAMP
TO
THE
FILE
WRITE
FD
TIMESTAMP
LENGTH
ALL
DONE
CLOSE
FD
RETURN
HERE
HOW
THE
TIMESTAMP
PROGRAM
WORKS
TIMESTAMP
TSFILE
CAT
TSFILE
THU
FEB
TIMESTAMP
TSFILE
CAT
TSFILE
THU
FEB
THU
FEB
NOTE
THAT
THE
FIRST
TIME
WE
INVOKE
TIMESTAMP
IT
CREATES
THE
FILE
TSFILE
WHILE
THE
SECOND
TIME
IT
APPENDS
TO
IT
THE
WRITE
CALL
RETURNS
THE
NUMBER
OF
BYTES
THAT
WERE
ACTUALLY
WRITTEN
OR
IF
AN
ERROR
OCCURRED
FOR
CERTAIN
KINDS
OF
FILE
DESCRIPTORS
THE
NUMBER
OF
BYTES
ACTUALLY
WRIT
TEN
MAY
BE
LESS
THAN
THE
NUMBER
OF
BYTES
REQUESTED
IN
THIS
CASE
IT
UP
TO
YOU
TO
CALL
WRITE
AGAIN
TO
WRITE
THE
REST
OF
THE
DATA
THE
FUNCTION
IN
LISTING
B
DEMONSTRATES
HOW
YOU
MIGHT
DO
THIS
NOTE
THAT
FOR
SOME
APPLICATIONS
YOU
MAY
HAVE
TO
CHECK
FOR
SPECIAL
CONDITIONS
IN
THE
MIDDLE
OF
THE
WRITING
OPERATION
FOR
EXAMPLE
IF
YOU
RE
WRIT
ING
TO
A
NETWORK
SOCKET
YOU
LL
HAVE
TO
AUGMENT
THIS
FUNCTION
TO
DETECT
WHETHER
THE
NETWORK
CONNECTION
WAS
CLOSED
IN
THE
MIDDLE
OF
THE
WRITE
OPERATION
AND
IF
IT
HAS
TO
REACT
APPROPRIATELY
D
ONLINE
RESOURCES
HIS
APPENDIX
LISTS
SOME
PLACES
TO
VISIT
ON
THE
INTERNET
TO
LEARN
MORE
ABOUT
PROGRAMMING
FOR
THE
GNU
LINUX
SYSTEM
D
GENERAL
INFORMATION
N
HTTP
WWW
ADVANCEDLINUXPROGRAMMING
COM
IS
THIS
BOOK
HOME
ON
THE
INTERNET
HERE
YOU
CAN
DOWNLOAD
THE
FULL
TEXT
OF
THIS
BOOK
AND
PROGRAM
SOURCE
CODE
FIND
LINKS
TO
OTHER
ONLINE
RESOURCES
AND
GET
MORE
INFORMATION
ABOUT
PRO
GRAMMING
GNU
LINUX
THE
SAME
INFORMATION
CAN
ALSO
BE
FOUND
AT
HTTP
WWW
NEWRIDERS
COM
N
HTTP
WWW
LINUXDOC
ORG
IS
THE
HOME
OF
THE
LINUX
DOCUMENTATION
PROJECT
THIS
SITE
IS
A
REPOSITORY
FOR
A
WEALTH
OF
DOCUMENTATION
FAQ
LISTS
HOWTOS
AND
OTHER
DOCUMENTATION
ABOUT
GNU
LINUX
SYSTEMS
AND
SOFTWARE
APPENDIX
D
ONLINE
RESOURCES
D
INFORMATION
ABOUT
GNU
LINUX
SOFTWARE
N
HTTP
WWW
GNU
ORG
IS
THE
HOME
OF
THE
GNU
PROJECT
FROM
THIS
SITE
YOU
CAN
DOWNLOAD
A
STAGGERING
ARRAY
OF
SOPHISTICATED
FREE
SOFTWARE
APPLICATIONS
AMONG
THEM
IS
THE
GNU
C
LIBRARY
WHICH
IS
PART
OF
EVERY
GNU
LINUX
SYSTEM
AND
CONTAINS
MANY
OF
THE
FUNCTIONS
DESCRIBED
IN
THIS
BOOK
THE
GNU
PROJECT
SITE
ALSO
PROVIDES
INFORMATION
ABOUT
HOW
YOU
CAN
CONTRIBUTE
TO
THE
DEVELOPMENT
OF
THE
GNU
LINUX
SYSTEM
BY
WRITING
CODE
OR
DOCUMENTATION
BY
USING
FREE
SOFT
WARE
AND
BY
SPREADING
THE
FREE
SOFTWARE
MESSAGE
N
HTTP
WWW
KERNEL
ORG
IS
THE
PRIMARY
SITE
FOR
DISTRIBUTION
OF
THE
LINUX
KERNEL
SOURCE
CODE
FOR
THE
TRICKIEST
AND
MOST
TECHNICALLY
DETAILED
QUESTIONS
ABOUT
HOW
LINUX
WORKS
THE
SOURCE
CODE
IS
THE
BEST
PLACE
TO
LOOK
SEE
ALSO
THE
DOCUMENTATION
DIRECTORY
FOR
EXPLANATION
OF
THE
KERNEL
INTERNALS
N
HTTP
WWW
LINUXHQ
COM
ALSO
DISTRIBUTES
LINUX
KERNEL
SOURCES
PATCHES
AND
RELATED
INFORMATION
N
HTTP
GCC
GNU
ORG
IS
THE
HOME
OF
THE
GNU
COMPILER
COLLECTION
GCC
GCC
IS
THE
PRIMARY
COMPILER
USED
ON
GNU
LINUX
SYSTEMS
AND
IT
INCLUDES
COMPILERS
FOR
C
C
OBJECTIVE
C
JAVA
CHILL
AND
FORTRAN
N
HTTP
WWW
GNOME
ORG
AND
HTTP
WWW
KDE
ORG
ARE
THE
HOMES
OF
THE
TWO
MOST
POPULAR
GNU
LINUX
WINDOWING
ENVIRONMENTS
GNOME
AND
KDE
IF
YOU
PLAN
TO
WRITE
AN
APPLICATION
WITH
A
GRAPHICAL
USER
INTERFACE
YOU
SHOULD
FAMILIARIZE
YOURSELF
WITH
EITHER
OR
BOTH
D
OTHER
SITES
N
HTTP
DEVELOPER
INTEL
COM
PROVIDES
INFORMATION
ABOUT
INTEL
PROCESSOR
ARCHI
TECTURES
INCLUDING
THE
ARCHITECTURE
IF
YOU
ARE
DEVELOPING
FOR
LINUX
AND
YOU
USE
INLINE
ASSEMBLY
INSTRUCTIONS
THE
TECHNICAL
MANUALS
AVAILABLE
HERE
WILL
BE
VERY
USEFUL
N
HTTP
WWW
AMD
COM
DEVCONN
PROVIDES
SIMILAR
INFORMATION
ABOUT
AMD
LINE
OF
MICROPROCESSORS
AND
ITS
SPECIAL
FEATURES
N
HTTP
FRESHMEAT
NET
IS
AN
INDEX
OF
OPEN
SOURCE
SOFTWARE
GENERALLY
FOR
GNU
LINUX
THIS
SITE
IS
ONE
OF
THE
BEST
PLACES
TO
STAY
ABREAST
OF
THE
NEWEST
RELEASES
OF
GNU
LINUX
SOFTWARE
FROM
CORE
SYSTEM
COMPONENTS
TO
MORE
OBSCURE
SPECIALIZED
APPLICATIONS
M
HTTP
WWW
LINUXSECURITY
COM
CONTAINS
INFORMATION
TECHNIQUES
AND
LINKS
TO
SOFTWARE
RELATED
TO
GNU
LINUX
SECURITY
THE
SITE
IS
OF
INTEREST
TO
USERS
SYSTEM
ADMINISTRATORS
AND
DEVELOPERS
F
GNU
GENERAL
PUBLIC
VERSION
JUNE
COPYRIGHT
FREE
SOFTWARE
FOUNDATION
INC
TEMPLE
PLACE
SUITE
BOSTON
MA
USA
EVERYONE
IS
PERMITTED
TO
COPY
AND
DISTRIBUTE
VERBATIM
COPIES
OF
THIS
LICENSE
DOCU
MENT
BUT
CHANGING
IT
IS
NOT
ALLOWED
PREAMBLE
THE
LICENSES
FOR
MOST
SOFTWARE
ARE
DESIGNED
TO
TAKE
AWAY
YOUR
FREEDOM
TO
SHARE
AND
CHANGE
IT
BY
CONTRAST
THE
GNU
GENERAL
PUBLIC
LICENSE
IS
INTENDED
TO
GUARANTEE
YOUR
FREEDOM
TO
SHARE
AND
CHANGE
FREE
SOFTWARE
TO
MAKE
SURE
THE
SOFTWARE
IS
FREE
FOR
ALL
ITS
USERS
THIS
GENERAL
PUBLIC
LICENSE
APPLIES
TO
MOST
OF
THE
FREE
SOFTWARE
FOUNDATION
SOFTWARE
AND
TO
ANY
OTHER
PROGRAM
WHOSE
AUTHORS
COMMIT
TO
USING
IT
SOME
OTHER
FREE
SOFTWARE
FOUNDATION
SOFTWARE
IS
COVERED
BY
THE
GNU
LIBRARY
GENERAL
PUBLIC
LICENSE
INSTEAD
YOU
CAN
APPLY
IT
TO
YOUR
PROGRAMS
TOO
WHEN
WE
SPEAK
OF
FREE
SOFTWARE
WE
ARE
REFERRING
TO
FREEDOM
NOT
PRICE
OUR
GENERAL
PUBLIC
LICENSES
ARE
DESIGNED
TO
MAKE
SURE
THAT
YOU
HAVE
THE
FREEDOM
TO
DISTRIBUTE
COPIES
OF
FREE
SOFTWARE
AND
CHARGE
FOR
THIS
SERVICE
IF
YOU
WISH
THAT
YOU
RECEIVE
SOURCE
CODE
OR
CAN
GET
IT
IF
YOU
WANT
IT
THAT
YOU
CAN
CHANGE
THE
SOFTWARE
OR
USE
PIECES
OF
IT
IN
NEW
FREE
PROGRAMS
AND
THAT
YOU
KNOW
YOU
CAN
DO
THESE
THINGS
THIS
LICENSE
CAN
ALSO
BE
FOUND
ONLINE
AT
HTTP
WWW
GNU
ORG
COPYLEFT
GPL
HTML
TO
PROTECT
YOUR
RIGHTS
WE
NEED
TO
MAKE
RESTRICTIONS
THAT
FORBID
ANYONE
TO
DENY
YOU
THESE
RIGHTS
OR
TO
ASK
YOU
TO
SURRENDER
THE
RIGHTS
THESE
RESTRICTIONS
TRANSLATE
TO
CERTAIN
RESPONSIBILITIES
FOR
YOU
IF
YOU
DISTRIBUTE
COPIES
OF
THE
SOFTWARE
OR
IF
YOU
MODIFY
IT
FOR
EXAMPLE
IF
YOU
DISTRIBUTE
COPIES
OF
SUCH
A
PROGRAM
WHETHER
GRATIS
OR
FOR
A
FEE
YOU
MUST
GIVE
THE
RECIPIENTS
ALL
THE
RIGHTS
THAT
YOU
HAVE
YOU
MUST
MAKE
SURE
THAT
THEY
TOO
RECEIVE
OR
CAN
GET
THE
SOURCE
CODE
AND
YOU
MUST
SHOW
THEM
THESE
TERMS
SO
THEY
KNOW
THEIR
RIGHTS
WE
PROTECT
YOUR
RIGHTS
WITH
TWO
STEPS
COPYRIGHT
THE
SOFTWARE
AND
OFFER
YOU
THIS
LICENSE
WHICH
GIVES
YOU
LEGAL
PERMISSION
TO
COPY
DISTRIBUTE
AND
OR
MODIFY
THE
SOFTWARE
ALSO
FOR
EACH
AUTHOR
PROTECTION
AND
OURS
WE
WANT
TO
MAKE
CERTAIN
THAT
EVERYONE
UNDERSTANDS
THAT
THERE
IS
NO
WARRANTY
FOR
THIS
FREE
SOFTWARE
IF
THE
SOFTWARE
IS
MODIFIED
BY
SOMEONE
ELSE
AND
PASSED
ON
WE
WANT
ITS
RECIPIENTS
TO
KNOW
THAT
WHAT
THEY
HAVE
IS
NOT
THE
ORIGINAL
SO
THAT
ANY
PROBLEMS
INTRODUCED
BY
OTHERS
WILL
NOT
REFLECT
ON
THE
ORIGINAL
AUTHORS
REPUTATIONS
FINALLY
ANY
FREE
PROGRAM
IS
THREATENED
CONSTANTLY
BY
SOFTWARE
PATENTS
WE
WISH
TO
AVOID
THE
DANGER
THAT
REDISTRIBUTORS
OF
A
FREE
PROGRAM
WILL
INDIVIDUALLY
OBTAIN
PATENT
LICENSES
IN
EFFECT
MAKING
THE
PROGRAM
PROPRIETARY
TO
PREVENT
THIS
WE
HAVE
MADE
IT
CLEAR
THAT
ANY
PATENT
MUST
BE
LICENSED
FOR
EVERYONE
FREE
USE
OR
NOT
LICENSED
AT
ALL
THE
PRECISE
TERMS
AND
CONDITIONS
FOR
COPYING
DISTRIBUTION
AND
MODIFICATION
FOLLOW
TERMS
AND
CONDITIONS
FOR
COPYING
DISTRIBUTION
AND
MODIFICATION
THIS
LICENSE
APPLIES
TO
ANY
PROGRAM
OR
OTHER
WORK
WHICH
CONTAINS
A
NOTICE
PLACED
BY
THE
COPYRIGHT
HOLDER
SAYING
IT
MAY
BE
DISTRIBUTED
UNDER
THE
TERMS
OF
THIS
GENERAL
PUBLIC
LICENSE
THE
PROGRAM
BELOW
REFERS
TO
ANY
SUCH
PROGRAM
OR
WORK
AND
A
WORK
BASED
ON
THE
PROGRAM
MEANS
EITHER
THE
PROGRAM
OR
ANY
DERIVATIVE
WORK
UNDER
COPYRIGHT
LAW
THAT
IS
TO
SAY
A
WORK
CONTAINING
THE
PROGRAM
OR
A
PORTION
OF
IT
EITHER
VERBATIM
OR
WITH
MODIFICATIONS
AND
OR
TRANS
LATED
INTO
ANOTHER
LANGUAGE
HEREINAFTER
TRANSLATION
IS
INCLUDED
WITHOUT
LIMITA
TION
IN
THE
TERM
MODIFICATION
EACH
LICENSEE
IS
ADDRESSED
AS
YOU
ACTIVITIES
OTHER
THAN
COPYING
DISTRIBUTION
AND
MODIFICATION
ARE
NOT
COVERED
BY
THIS
LICENSE
THEY
ARE
OUTSIDE
ITS
SCOPE
THE
ACT
OF
RUNNING
THE
PROGRAM
IS
NOT
RESTRICTED
AND
THE
OUTPUT
FROM
THE
PROGRAM
IS
COVERED
ONLY
IF
ITS
CONTENTS
CONSTITUTE
A
WORK
BASED
ON
THE
PROGRAM
INDEPENDENT
OF
HAVING
BEEN
MADE
BY
RUNNING
THE
PROGRAM
WHETHER
THAT
IS
TRUE
DEPENDS
ON
WHAT
THE
PROGRAM
DOES
YOU
MAY
COPY
AND
DISTRIBUTE
VERBATIM
COPIES
OF
THE
PROGRAM
SOURCE
CODE
AS
YOU
RECEIVE
IT
IN
ANY
MEDIUM
PROVIDED
THAT
YOU
CONSPICUOUSLY
AND
APPROPRI
ATELY
PUBLISH
ON
EACH
COPY
AN
APPROPRIATE
COPYRIGHT
NOTICE
AND
DISCLAIMER
OF
WARRANTY
KEEP
INTACT
ALL
THE
NOTICES
THAT
REFER
TO
THIS
LICENSE
AND
TO
THE
ABSENCE
OF
ANY
WARRANTY
AND
GIVE
ANY
OTHER
RECIPIENTS
OF
THE
PROGRAM
A
COPY
OF
THIS
LICENSE
ALONG
WITH
THE
PROGRAM
YOU
MAY
CHARGE
A
FEE
FOR
THE
PHYSICAL
ACT
OF
TRANSFERRING
A
COPY
AND
YOU
MAY
AT
YOUR
OPTION
OFFER
WARRANTY
PROTECTION
IN
EXCHANGE
FOR
A
FEE
YOU
MAY
MODIFY
YOUR
COPY
OR
COPIES
OF
THE
PROGRAM
OR
ANY
PORTION
OF
IT
THUS
FORMING
A
WORK
BASED
ON
THE
PROGRAM
AND
COPY
AND
DISTRIBUTE
SUCH
MODIFICA
TIONS
OR
WORK
UNDER
THE
TERMS
OF
SECTION
ABOVE
PROVIDED
THAT
YOU
ALSO
MEET
ALL
OF
THESE
CONDITIONS
N
A
YOU
MUST
CAUSE
THE
MODIFIED
FILES
TO
CARRY
PROMINENT
NOTICES
STATING
THAT
YOU
CHANGED
THE
FILES
AND
THE
DATE
OF
ANY
CHANGE
N
B
YOU
MUST
CAUSE
ANY
WORK
THAT
YOU
DISTRIBUTE
OR
PUBLISH
THAT
IN
WHOLE
OR
IN
PART
CONTAINS
OR
IS
DERIVED
FROM
THE
PROGRAM
OR
ANY
PART
THEREOF
TO
BE
LICENSED
AS
A
WHOLE
AT
NO
CHARGE
TO
ALL
THIRD
PARTIES
UNDER
THE
TERMS
OF
THIS
LICENSE
N
C
IF
THE
MODIFIED
PROGRAM
NORMALLY
READS
COMMANDS
INTERACTIVELY
WHEN
RUN
YOU
MUST
CAUSE
IT
WHEN
STARTED
RUNNING
FOR
SUCH
INTERACTIVE
USE
IN
THE
MOST
ORDINARY
WAY
TO
PRINT
OR
DISPLAY
AN
ANNOUNCEMENT
INCLUDING
AN
APPROPRIATE
COPYRIGHT
NOTICE
AND
A
NOTICE
THAT
THERE
IS
NO
WARRANTY
OR
ELSE
SAYING
THAT
YOU
PROVIDE
A
WARRANTY
AND
THAT
USERS
MAY
REDISTRIBUTE
THE
PROGRAM
UNDER
THESE
CONDITIONS
AND
TELLING
THE
USER
HOW
TO
VIEW
A
COPY
OF
THIS
LICENSE
EXCEPTION
IF
THE
PROGRAM
ITSELF
IS
INTERACTIVE
BUT
DOES
NOT
NORMALLY
PRINT
SUCH
AN
ANNOUNCEMENT
YOUR
WORK
BASED
ON
THE
PROGRAM
IS
NOT
REQUIRED
TO
PRINT
AN
ANNOUNCEMENT
THESE
REQUIREMENTS
APPLY
TO
THE
MODIFIED
WORK
AS
A
WHOLE
IF
IDENTIFIABLE
SEC
TIONS
OF
THAT
WORK
ARE
NOT
DERIVED
FROM
THE
PROGRAM
AND
CAN
BE
REASONABLY
CONSIDERED
INDEPENDENT
AND
SEPARATE
WORKS
IN
THEMSELVES
THEN
THIS
LICENSE
AND
ITS
TERMS
DO
NOT
APPLY
TO
THOSE
SECTIONS
WHEN
YOU
DISTRIBUTE
THEM
AS
SEPARATE
WORKS
BUT
WHEN
YOU
DISTRIBUTE
THE
SAME
SECTIONS
AS
PART
OF
A
WHOLE
WHICH
IS
A
WORK
BASED
ON
THE
PROGRAM
THE
DISTRIBUTION
OF
THE
WHOLE
MUST
BE
ON
THE
TERMS
OF
THIS
LICENSE
WHOSE
PERMISSIONS
FOR
OTHER
LICENSEES
EXTEND
TO
THE
ENTIRE
WHOLE
AND
THUS
TO
EACH
AND
EVERY
PART
REGARDLESS
OF
WHO
WROTE
IT
THUS
IT
IS
NOT
THE
INTENT
OF
THIS
SECTION
TO
CLAIM
RIGHTS
OR
CONTEST
YOUR
RIGHTS
TO
WORK
WRITTEN
ENTIRELY
BY
YOU
RATHER
THE
INTENT
IS
TO
EXERCISE
THE
RIGHT
TO
CONTROL
THE
DISTRIBUTION
OF
DERIVATIVE
OR
COLLECTIVE
WORKS
BASED
ON
THE
PROGRAM
IN
ADDITION
MERE
AGGREGATION
OF
ANOTHER
WORK
NOT
BASED
ON
THE
PROGRAM
WITH
THE
PROGRAM
OR
WITH
A
WORK
BASED
ON
THE
PROGRAM
ON
A
VOLUME
OF
A
STORAGE
OR
DISTRIBUTION
MEDIUM
DOES
NOT
BRING
THE
OTHER
WORK
UNDER
THE
SCOPE
OF
THIS
LICENSE
YOU
MAY
COPY
AND
DISTRIBUTE
THE
PROGRAM
OR
A
WORK
BASED
ON
IT
UNDER
SECTION
IN
OBJECT
CODE
OR
EXECUTABLE
FORM
UNDER
THE
TERMS
OF
SECTIONS
AND
ABOVE
PROVIDED
THAT
YOU
ALSO
DO
ONE
OF
THE
FOLLOWING
N
A
ACCOMPANY
IT
WITH
THE
COMPLETE
CORRESPONDING
MACHINE
READABLE
SOURCE
CODE
WHICH
MUST
BE
DISTRIBUTED
UNDER
THE
TERMS
OF
SECTIONS
AND
ABOVE
ON
A
MEDIUM
CUSTOMARILY
USED
FOR
SOFTWARE
INTERCHANGE
OR
N
B
ACCOMPANY
IT
WITH
A
WRITTEN
OFFER
VALID
FOR
AT
LEAST
THREE
YEARS
TO
GIVE
ANY
THIRD
PARTY
FOR
A
CHARGE
NO
MORE
THAN
YOUR
COST
OF
PHYSICALLY
PERFORM
ING
SOURCE
DISTRIBUTION
A
COMPLETE
MACHINE
READABLE
COPY
OF
THE
CORRE
SPONDING
SOURCE
CODE
TO
BE
DISTRIBUTED
UNDER
THE
TERMS
OF
SECTIONS
AND
ABOVE
ON
A
MEDIUM
CUSTOMARILY
USED
FOR
SOFTWARE
INTERCHANGE
OR
N
C
ACCOMPANY
IT
WITH
THE
INFORMATION
YOU
RECEIVED
AS
TO
THE
OFFER
TO
DIS
TRIBUTE
CORRESPONDING
SOURCE
CODE
THIS
ALTERNATIVE
IS
ALLOWED
ONLY
FOR
NONCOMMERCIAL
DISTRIBUTION
AND
ONLY
IF
YOU
RECEIVED
THE
PROGRAM
IN
OBJECT
CODE
OR
EXECUTABLE
FORM
WITH
SUCH
AN
OFFER
IN
ACCORD
WITH
SUBSECTION
B
ABOVE
THE
SOURCE
CODE
FOR
A
WORK
MEANS
THE
PREFERRED
FORM
OF
THE
WORK
FOR
MAKING
MODIFICATIONS
TO
IT
FOR
AN
EXECUTABLE
WORK
COMPLETE
SOURCE
CODE
MEANS
ALL
THE
SOURCE
CODE
FOR
ALL
MODULES
IT
CONTAINS
PLUS
ANY
ASSOCIATED
INTERFACE
DEFINITION
FILES
PLUS
THE
SCRIPTS
USED
TO
CONTROL
COMPILATION
AND
INSTALLATION
OF
THE
EXE
CUTABLE
HOWEVER
AS
A
SPECIAL
EXCEPTION
THE
SOURCE
CODE
DISTRIBUTED
NEED
NOT
INCLUDE
ANYTHING
THAT
IS
NORMALLY
DISTRIBUTED
IN
EITHER
SOURCE
OR
BINARY
FORM
WITH
THE
MAJOR
COMPONENTS
COMPILER
KERNEL
AND
SO
ON
OF
THE
OPERATING
SYS
TEM
ON
WHICH
THE
EXECUTABLE
RUNS
UNLESS
THAT
COMPONENT
ITSELF
ACCOMPANIES
THE
EXECUTABLE
IF
DISTRIBUTION
OF
EXECUTABLE
OR
OBJECT
CODE
IS
MADE
BY
OFFERING
ACCESS
TO
COPY
FROM
A
DESIGNATED
PLACE
THEN
OFFERING
EQUIVALENT
ACCESS
TO
COPY
THE
SOURCE
CODE
FROM
THE
SAME
PLACE
COUNTS
AS
DISTRIBUTION
OF
THE
SOURCE
CODE
EVEN
THOUGH
THIRD
PARTIES
ARE
NOT
COMPELLED
TO
COPY
THE
SOURCE
ALONG
WITH
THE
OBJECT
CODE
YOU
MAY
NOT
COPY
MODIFY
SUBLICENSE
OR
DISTRIBUTE
THE
PROGRAM
EXCEPT
AS
EXPRESSLY
PROVIDED
UNDER
THIS
LICENSE
ANY
ATTEMPT
OTHERWISE
TO
COPY
MODIFY
SUBLICENSE
OR
DISTRIBUTE
THE
PROGRAM
IS
VOID
AND
WILL
AUTOMATICALLY
TERMINATE
YOUR
RIGHTS
UNDER
THIS
LICENSE
HOWEVER
PARTIES
WHO
HAVE
RECEIVED
COPIES
OR
RIGHTS
FROM
YOU
UNDER
THIS
LICENSE
WILL
NOT
HAVE
THEIR
LICENSES
TERMINATED
SO
LONG
AS
SUCH
PARTIES
REMAIN
IN
FULL
COMPLIANCE
YOU
ARE
NOT
REQUIRED
TO
ACCEPT
THIS
LICENSE
SINCE
YOU
HAVE
NOT
SIGNED
IT
HOWEVER
NOTHING
ELSE
GRANTS
YOU
PERMISSION
TO
MODIFY
OR
DISTRIBUTE
THE
PROGRAM
OR
ITS
DERIVATIVE
WORKS
THESE
ACTIONS
ARE
PROHIBITED
BY
LAW
IF
YOU
DO
NOT
ACCEPT
THIS
LICENSE
THEREFORE
BY
MODIFYING
OR
DISTRIBUTING
THE
PROGRAM
OR
ANY
WORK
BASED
ON
THE
PROGRAM
YOU
INDICATE
YOUR
ACCEPTANCE
OF
THIS
LICENSE
TO
DO
SO
AND
ALL
ITS
TERMS
AND
CONDITIONS
FOR
COPYING
DISTRIBUTING
OR
MODIFYING
THE
PROGRAM
OR
WORKS
BASED
ON
IT
EACH
TIME
YOU
REDISTRIBUTE
THE
PROGRAM
OR
ANY
WORK
BASED
ON
THE
PROGRAM
THE
RECIPIENT
AUTOMATICALLY
RECEIVES
A
LICENSE
FROM
THE
ORIGINAL
LICENSOR
TO
COPY
DISTRIBUTE
OR
MODIFY
THE
PROGRAM
SUBJECT
TO
THESE
TERMS
AND
CONDITIONS
YOU
MAY
NOT
IMPOSE
ANY
FURTHER
RESTRICTIONS
ON
THE
RECIPIENTS
EXERCISE
OF
THE
RIGHTS
GRANTED
HEREIN
YOU
ARE
NOT
RESPONSIBLE
FOR
ENFORCING
COMPLIANCE
BY
THIRD
PARTIES
TO
THIS
LICENSE
IF
AS
A
CONSEQUENCE
OF
A
COURT
JUDGMENT
OR
ALLEGATION
OF
PATENT
INFRINGEMENT
OR
FOR
ANY
OTHER
REASON
NOT
LIMITED
TO
PATENT
ISSUES
CONDITIONS
ARE
IMPOSED
ON
YOU
WHETHER
BY
COURT
ORDER
AGREEMENT
OR
OTHERWISE
THAT
CONTRADICT
THE
CONDI
TIONS
OF
THIS
LICENSE
THEY
DO
NOT
EXCUSE
YOU
FROM
THE
CONDITIONS
OF
THIS
LICENSE
IF
YOU
CANNOT
DISTRIBUTE
SO
AS
TO
SATISFY
SIMULTANEOUSLY
YOUR
OBLIGATIONS
UNDER
THIS
LICENSE
AND
ANY
OTHER
PERTINENT
OBLIGATIONS
THEN
AS
A
CONSEQUENCE
YOU
MAY
NOT
DISTRIBUTE
THE
PROGRAM
AT
ALL
FOR
EXAMPLE
IF
A
PATENT
LICENSE
WOULD
NOT
PER
MIT
ROYALTY
FREE
REDISTRIBUTION
OF
THE
PROGRAM
BY
ALL
THOSE
WHO
RECEIVE
COPIES
DIRECTLY
OR
INDIRECTLY
THROUGH
YOU
THEN
THE
ONLY
WAY
YOU
COULD
SATISFY
BOTH
IT
AND
THIS
LICENSE
WOULD
BE
TO
REFRAIN
ENTIRELY
FROM
DISTRIBUTION
OF
THE
PROGRAM
IF
ANY
PORTION
OF
THIS
SECTION
IS
HELD
INVALID
OR
UNENFORCEABLE
UNDER
ANY
PARTICU
LAR
CIRCUMSTANCE
THE
BALANCE
OF
THE
SECTION
IS
INTENDED
TO
APPLY
AND
THE
SECTION
AS
A
WHOLE
IS
INTENDED
TO
APPLY
IN
OTHER
CIRCUMSTANCES
IT
IS
NOT
THE
PURPOSE
OF
THIS
SECTION
TO
INDUCE
YOU
TO
INFRINGE
ANY
PATENTS
OR
OTHER
PROPERTY
RIGHT
CLAIMS
OR
TO
CONTEST
VALIDITY
OF
ANY
SUCH
CLAIMS
THIS
SECTION
HAS
THE
SOLE
PURPOSE
OF
PROTECTING
THE
INTEGRITY
OF
THE
FREE
SOFTWARE
DISTRIBUTION
SYSTEM
WHICH
IS
IMPLEMENTED
BY
PUBLIC
LICENSE
PRACTICES
MANY
PEOPLE
HAVE
MADE
GENEROUS
CONTRIBUTIONS
TO
THE
WIDE
RANGE
OF
SOFTWARE
DISTRIBUTED
THROUGH
THAT
SYSTEM
IN
RELIANCE
ON
CONSISTENT
APPLICATION
OF
THAT
SYSTEM
IT
IS
UP
TO
THE
AUTHOR
DONOR
TO
DECIDE
IF
HE
OR
SHE
IS
WILLING
TO
DISTRIBUTE
SOFTWARE
THROUGH
ANY
OTHER
SYSTEM
AND
A
LICENSEE
CANNOT
IMPOSE
THAT
CHOICE
THIS
SECTION
IS
INTENDED
TO
MAKE
THOROUGHLY
CLEAR
WHAT
IS
BELIEVED
TO
BE
A
CON
SEQUENCE
OF
THE
REST
OF
THIS
LICENSE
IF
THE
DISTRIBUTION
AND
OR
USE
OF
THE
PROGRAM
IS
RESTRICTED
IN
CERTAIN
COUNTRIES
EITHER
BY
PATENTS
OR
BY
COPYRIGHTED
INTERFACES
THE
ORIGINAL
COPYRIGHT
HOLDER
WHO
PLACES
THE
PROGRAM
UNDER
THIS
LICENSE
MAY
ADD
AN
EXPLICIT
GEOGRAPHICAL
DISTRIBU
TION
LIMITATION
EXCLUDING
THOSE
COUNTRIES
SO
THAT
DISTRIBUTION
IS
PERMITTED
ONLY
IN
OR
AMONG
COUNTRIES
NOT
THUS
EXCLUDED
IN
SUCH
CASE
THIS
LICENSE
INCORPORATES
THE
LIMITATION
AS
IF
WRITTEN
IN
THE
BODY
OF
THIS
LICENSE
THE
FREE
SOFTWARE
FOUNDATION
MAY
PUBLISH
REVISED
AND
OR
NEW
VERSIONS
OF
THE
GENERAL
PUBLIC
LICENSE
FROM
TIME
TO
TIME
SUCH
NEW
VERSIONS
WILL
BE
SIMILAR
IN
SPIRIT
TO
THE
PRESENT
VERSION
BUT
MAY
DIFFER
IN
DETAIL
TO
ADDRESS
NEW
PROBLEMS
OR
CONCERNS
EACH
VERSION
IS
GIVEN
A
DISTINGUISHING
VERSION
NUMBER
IF
THE
PROGRAM
SPECIFIES
A
VERSION
NUMBER
OF
THIS
LICENSE
WHICH
APPLIES
TO
IT
AND
ANY
LATER
VERSION
YOU
HAVE
THE
OPTION
OF
FOLLOWING
THE
TERMS
AND
CONDITIONS
EITHER
OF
THAT
VERSION
OR
OF
ANY
LATER
VERSION
PUBLISHED
BY
THE
FREE
SOFTWARE
FOUNDATION
IF
THE
PROGRAM
DOES
NOT
SPECIFY
A
VERSION
NUMBER
OF
THIS
LICENSE
YOU
MAY
CHOOSE
ANY
VERSION
EVER
PUBLISHED
BY
THE
FREE
SOFTWARE
FOUNDATION
IF
YOU
WISH
TO
INCORPORATE
PARTS
OF
THE
PROGRAM
INTO
OTHER
FREE
PROGRAMS
WHOSE
DISTRIBUTION
CONDITIONS
ARE
DIFFERENT
WRITE
TO
THE
AUTHOR
TO
ASK
FOR
PERMISSION
FOR
SOFTWARE
WHICH
IS
COPYRIGHTED
BY
THE
FREE
SOFTWARE
FOUNDATION
WRITE
TO
THE
FREE
SOFTWARE
FOUNDATION
WE
SOMETIMES
MAKE
EXCEPTIONS
FOR
THIS
OUR
DECISION
WILL
BE
GUIDED
BY
THE
TWO
GOALS
OF
PRESERVING
THE
FREE
STATUS
OF
ALL
DERIVATIVES
OF
OUR
FREE
SOFTWARE
AND
OF
PROMOTING
THE
SHARING
AND
REUSE
OF
SOFT
WARE
GENERALLY
NO
WARRANTY
BECAUSE
THE
PROGRAM
IS
LICENSED
FREE
OF
CHARGE
THERE
IS
NO
WARRANTY
FOR
THE
PROGRAM
TO
THE
EXTENT
PERMIT
TED
BY
APPLICABLE
LAW
EXCEPT
WHEN
OTHERWISE
STATED
IN
WRITING
THE
COPYRIGHT
HOLDERS
AND
OR
OTHER
PARTIES
PROVIDE
THE
PROGRAM
AS
IS
WITHOUT
WARRANTY
OF
ANY
KIND
EITHER
EXPRESSED
OR
IMPLIED
INCLUDING
BUT
NOT
LIMITED
TO
THE
IMPLIED
WARRANTIES
OF
MERCHANTABILITY
AND
FITNESS
FOR
A
PARTICULAR
PURPOSE
THE
ENTIRE
RISK
AS
TO
THE
QUALITY
AND
PERFORMANCE
OF
THE
PROGRAM
IS
WITH
YOU
SHOULD
THE
PROGRAM
PROVE
DEFECTIVE
YOU
ASSUME
THE
COST
OF
ALL
NECESSARY
SERVICING
REPAIR
OR
CORRECTION
IN
NO
EVENT
UNLESS
REQUIRED
BY
APPLICABLE
LAW
OR
AGREED
TO
IN
WRITING
WILL
ANY
COPYRIGHT
HOLDER
OR
ANY
OTHER
PARTY
WHO
MAY
MODIFY
AND
OR
REDISTRIBUTE
THE
PROGRAM
AS
PERMITTED
ABOVE
BE
LIABLE
TO
YOU
FOR
DAMAGES
INCLUDING
ANY
GENERAL
SPECIAL
INCIDENTAL
OR
CONSEQUENTIAL
DAMAGES
ARISING
OUT
OF
THE
USE
OR
INABILITY
TO
USE
THE
PROGRAM
INCLUDING
BUT
NOT
LIM
ITED
TO
LOSS
OF
DATA
OR
DATA
BEING
RENDERED
INACCU
RATE
OR
LOSSES
SUSTAINED
BY
YOU
OR
THIRD
PARTIES
OR
A
FAILURE
OF
THE
PROGRAM
TO
OPERATE
WITH
ANY
OTHER
PROGRAMS
EVEN
IF
SUCH
HOLDER
OR
OTHER
PARTY
HAS
BEEN
ADVISED
OF
THE
POSSIBILITY
OF
SUCH
DAMAGES
HOW
TO
APPLY
THESE
TERMS
TO
YOUR
NEW
PROGRAMS
END
OF
TERMS
AND
CONDITIONS
HOW
TO
APPLY
THESE
TERMS
TO
YOUR
NEW
PROGRAMS
IF
YOU
DEVELOP
A
NEW
PROGRAM
AND
YOU
WANT
IT
TO
BE
OF
THE
GREATEST
POSSIBLE
USE
TO
THE
PUBLIC
THE
BEST
WAY
TO
ACHIEVE
THIS
IS
TO
MAKE
IT
FREE
SOFTWARE
WHICH
EVERYONE
CAN
REDISTRIBUTE
AND
CHANGE
UNDER
THESE
TERMS
TO
DO
SO
ATTACH
THE
FOLLOWING
NOTICES
TO
THE
PROGRAM
IT
IS
SAFEST
TO
ATTACH
THEM
TO
THE
START
OF
EACH
SOURCE
FILE
TO
MOST
EFFECTIVELY
CONVEY
THE
EXCLUSION
OF
WARRANTY
AND
EACH
FILE
SHOULD
HAVE
AT
LEAST
THE
COPYRIGHT
LINE
AND
A
POINTER
TO
WHERE
THE
FULL
NOTICE
IS
FOUND
ONE
LINE
TO
GIVE
THE
PROGRAM
NAME
AND
AN
IDEA
OF
WHAT
IT
DOES
COPYRIGHT
YYYY
NAME
OF
AUTHOR
THIS
PROGRAM
IS
FREE
SOFTWARE
YOU
CAN
REDISTRIBUTE
IT
AND
OR
MODIFY
IT
UNDER
THE
TERMS
OF
THE
GNU
GENERAL
PUBLIC
LICENSE
AS
PUBLISHED
BY
THE
FREE
SOFTWARE
FOUNDATION
EITHER
VERSION
OF
THE
LICENSE
OR
AT
YOUR
OPTION
ANY
LATER
VERSION
THIS
PROGRAM
IS
DISTRIBUTED
IN
THE
HOPE
THAT
IT
WILL
BE
USEFUL
BUT
WITHOUT
ANY
WARRANTY
WITHOUT
EVEN
THE
IMPLIED
WARRANTY
OF
MERCHANTABILITY
OR
FITNESS
FOR
A
PARTICULAR
PURPOSE
SEE
THE
GNU
GENERAL
PUBLIC
LICENSE
FOR
MORE
DETAILS
YOU
SHOULD
HAVE
RECEIVED
A
COPY
OF
THE
GNU
GENERAL
PUBLIC
LICENSE
ALONG
WITH
THIS
PROGRAM
IF
NOT
WRITE
TO
THE
FREE
SOFTWARE
FOUNDATION
INC
TEMPLE
PLACE
SUITE
BOSTON
MA
USA
ALSO
ADD
INFORMATION
ON
HOW
TO
CONTACT
YOU
BY
ELECTRONIC
AND
PAPER
MAIL
IF
THE
PROGRAM
IS
INTERACTIVE
MAKE
IT
OUTPUT
A
SHORT
NOTICE
LIKE
THIS
WHEN
IT
STARTS
IN
AN
INTERACTIVE
MODE
GNOMOVISION
VERSION
COPYRIGHT
YEAR
NAME
OF
AUTHOR
GNOMOVISION
COMES
WITH
ABSOLUTELY
NO
WARRANTY
FOR
DETAILS
TYPE
SHOW
W
THIS
IS
FREE
SOFTWARE
AND
YOU
ARE
WELCOME
TO
REDISTRIBUTE
IT
UNDER
CERTAIN
CONDITIONS
TYPE
SHOW
C
FOR
DETAILS
THE
HYPOTHETICAL
COMMANDS
SHOW
W
AND
SHOW
C
SHOULD
SHOW
THE
APPROPRIATE
PARTS
OF
THE
GENERAL
PUBLIC
LICENSE
OF
COURSE
THE
COMMANDS
YOU
USE
MAY
BE
CALLED
SOME
THING
OTHER
THAN
SHOW
W
AND
SHOW
C
THEY
COULD
EVEN
BE
MOUSE
CLICKS
OR
MENU
ITEMS
WHATEVER
SUITS
YOUR
PROGRAM
YOU
SHOULD
ALSO
GET
YOUR
EMPLOYER
IF
YOU
WORK
AS
A
PROGRAMMER
OR
YOUR
SCHOOL
IF
ANY
TO
SIGN
A
COPYRIGHT
DISCLAIMER
FOR
THE
PROGRAM
IF
NECESSARY
HERE
IS
A
SAMPLE
ALTER
THE
NAMES
YOYODYNE
INC
HEREBY
DISCLAIMS
ALL
COPYRIGHT
INTEREST
IN
THE
PROGRAM
GNOMOVISION
WHICH
MAKES
PASSES
AT
COMPILERS
WRITTEN
BY
JAMES
HACKER
SIGNATURE
OF
TY
COON
APRIL
TY
COON
PRESIDENT
OF
VICE
THIS
GENERAL
PUBLIC
LICENSE
DOES
NOT
PERMIT
INCORPORATING
YOUR
PROGRAM
INTO
PROPRI
ETARY
PROGRAMS
IF
YOUR
PROGRAM
IS
A
SUBROUTINE
LIBRARY
YOU
MAY
CONSIDER
IT
MORE
USE
FUL
TO
PERMIT
LINKING
PROPRIETARY
APPLICATIONS
WITH
THE
LIBRARY
IF
THIS
IS
WHAT
YOU
WANT
TO
DO
USE
THE
GNU
LIBRARY
GENERAL
PUBLIC
LICENSE
INSTEAD
OF
THIS
LICENSE
FSF
GNU
INQUIRIES
QUESTIONS
TO
GNU
GNU
ORG
COMMENTS
ON
THESE
WEB
PAGES
TO
WEBMASTERS
WWW
GNU
ORG
SEND
OTHER
QUESTIONS
TO
GNU
GNU
ORG
COPYRIGHT
NOTICE
ABOVE
FREE
SOFTWARE
FOUNDATION
INC
TEMPLE
PLACE
SUITE
BOSTON
MA
USA
UPDATED
JUL
JONAS
WHEN
WRITING
A
PROGRAM
YOU
FREQUENTLY
CAN
T
KNOW
HOW
MUCH
MEMORY
THE
PROGRAM
WILL
NEED
WHEN
IT
RUNS
FOR
EXAMPLE
A
LINE
READ
FROM
A
FILE
AT
RUNTIME
MIGHT
HAVE
ANY
FINITE
LENGTH
C
AND
C
PROGRAMS
USE
MALLOC
FREE
AND
THEIR
VARIANTS
TO
DYNAMICALLY
ALLOCATE
MEMORY
WHILE
THE
PROGRAM
IS
RUNNING
THE
RULES
FOR
DYNAMIC
MEMORY
USE
INCLUDE
THESE
N
THE
NUMBER
OF
ALLOCATION
CALLS
CALLS
TO
MALLOC
MUST
EXACTLY
MATCH
THE
NUMBER
OF
DEALLOCATION
CALLS
CALLS
TO
FREE
N
READS
AND
WRITES
TO
THE
ALLOCATED
MEMORY
MUST
OCCUR
WITHIN
THE
MEMORY
NOT
OUTSIDE
ITS
RANGE
N
THE
ALLOCATED
MEMORY
CANNOT
BE
USED
BEFORE
IT
IS
ALLOCATED
OR
AFTER
IT
IS
DEALLOCATED
BECAUSE
DYNAMIC
MEMORY
ALLOCATION
AND
DEALLOCATION
OCCUR
AT
RUNTIME
STATIC
PROGRAM
ANALYSIS
RARELY
FIND
VIOLATIONS
INSTEAD
MEMORY
CHECKING
TOOLS
RUN
THE
PROGRAM
COL
LECTING
DATA
TO
DETERMINE
IF
ANY
OF
THESE
RULES
HAVE
BEEN
VIOLATED
THE
VIOLATIONS
A
TOOL
MAY
FIND
INCLUDE
THE
FOLLOWING
N
READING
FROM
MEMORY
BEFORE
ALLOCATING
IT
N
WRITING
TO
MEMORY
BEFORE
ALLOCATING
IT
N
READING
BEFORE
THE
BEGINNING
OF
ALLOCATED
MEMORY
N
WRITING
BEFORE
THE
BEGINNING
OF
ALLOCATED
MEMORY
N
READING
AFTER
THE
END
OF
ALLOCATED
MEMORY
N
WRITING
AFTER
THE
END
OF
ALLOCATED
MEMORY
N
READING
FROM
MEMORY
AFTER
ITS
DEALLOCATION
N
WRITING
TO
MEMORY
AFTER
ITS
DEALLOCATION
N
FAILING
TO
DEALLOCATE
ALLOCATED
MEMORY
N
DEALLOCATING
THE
SAME
MEMORY
TWICE
N
DEALLOCATING
MEMORY
THAT
IS
NOT
ALLOCATED
IT
IS
ALSO
USEFUL
TO
WARN
ABOUT
REQUESTING
AN
ALLOCATION
WITH
BYTES
WHICH
PROBABLY
INDICATES
PROGRAMMER
ERROR
TABLE
A
INDICATES
FOUR
DIFFERENT
TOOLS
DIAGNOSTIC
CAPABILITIES
UNFORTUNATELY
NO
SINGLE
TOOL
DIAGNOSES
ALL
THE
MEMORY
USE
ERRORS
ALSO
NO
TOOL
CLAIMS
TO
DETECT
READING
OR
WRITING
BEFORE
ALLOCATING
MEMORY
BUT
DOING
SO
WILL
PROBABLY
CAUSE
A
SEGMENTATION
FAULT
DEALLOCATING
MEMORY
TWICE
WILL
PROBABLY
ALSO
CAUSE
A
SEGMENTATION
FAULT
THESE
TOOLS
DIAGNOSE
ONLY
ERRORS
THAT
ACTUALLY
OCCUR
WHILE
THE
PROGRAM
IS
RUNNING
IF
YOU
RUN
THE
PROGRAM
WITH
INPUTS
THAT
CAUSE
NO
MEMORY
TO
BE
ALLOCATED
THE
TOOLS
WILL
INDICATE
NO
MEMORY
ERRORS
TO
TEST
A
PROGRAM
THOROUGHLY
YOU
MUST
RUN
THE
PROGRAM
USING
DIF
FERENT
INPUTS
TO
ENSURE
THAT
EVERY
POSSIBLE
PATH
THROUGH
THE
PROGRAM
OCCURS
ALSO
YOU
MAY
USE
ONLY
ONE
TOOL
AT
A
TIME
SO
YOU
LL
HAVE
TO
REPEAT
TESTING
WITH
SEVERAL
TOOLS
TO
GET
THE
BEST
ERROR
CHECKING
TABLE
A
CAPABILITIES
OF
DYNAMIC
MEMORY
CHECKING
TOOLS
X
INDICATES
DETECTION
AND
O
INDICATES
DETECTION
FOR
SOME
CASES
ERRONEOUS
BEHAVIOR
MALLOC
MTRACE
CCMALLOC
ELECTRIC
CHECKING
FENCE
READ
BEFORE
ALLOCATING
MEMORY
WRITE
BEFORE
ALLOCATING
MEMORY
READ
BEFORE
BEGINNING
OF
ALLOCATION
X
WRITE
BEFORE
BEGINNING
OF
ALLOCATION
O
O
X
READ
AFTER
END
OF
ALLOCATION
X
WRITE
AFTER
END
OF
ALLOCATION
X
X
READ
AFTER
DEALLOCATION
X
WRITE
AFTER
DEALLOCATION
X
FAILURE
TO
DEALLOCATE
MEMORY
X
X
DEALLOCATING
MEMORY
TWICE
X
X
DEALLOCATING
NONALLOCATED
MEMORY
X
X
ZERO
SIZE
MEMORY
ALLOCATION
X
X
IN
THE
SECTIONS
THAT
FOLLOW
WE
FIRST
DESCRIBE
HOW
TO
USE
THE
MORE
EASILY
USED
MALLOC
CHECKING
AND
MTRACE
AND
THEN
CCMALLOC
AND
ELECTRIC
FENCE
A
A
PROGRAM
TO
TEST
MEMORY
ALLOCATION
AND
DEALLOCATION
WE
LL
USE
THE
MALLOC
USE
PROGRAM
IN
LISTING
A
TO
ILLUSTRATE
MEMORY
ALLOCATION
DEAL
LOCATION
AND
USE
TO
BEGIN
RUNNING
IT
SPECIFY
THE
MAXIMUM
NUMBER
OF
ALLOCATED
MEMORY
REGIONS
AS
ITS
ONLY
COMMAND
LINE
ARGUMENT
FOR
EXAMPLE
MALLOC
USE
CREATES
AN
ARRAY
A
WITH
CHARACTER
POINTERS
THAT
DO
NOT
POINT
TO
ANYTHING
THE
PROGRAM
ACCEPTS
FIVE
DIFFERENT
COMMANDS
N
TO
ALLOCATE
B
BYTES
POINTED
TO
BY
ARRAY
ENTRY
A
I
ENTER
A
I
B
THE
ARRAY
INDEX
I
CAN
BE
ANY
NON
NEGATIVE
NUMBER
SMALLER
THAN
THE
COMMAND
LINE
ARGUMENT
THE
NUMBER
OF
BYTES
MUST
BE
NON
NEGATIVE
N
TO
DEALLOCATE
MEMORY
AT
ARRAY
INDEX
I
ENTER
D
I
N
TO
READ
THE
PTH
CHARACTER
FROM
THE
ALLOCATED
MEMORY
AT
INDEX
I
AS
IN
A
I
P
ENTER
R
I
P
HERE
P
CAN
HAVE
AN
INTEGRAL
VALUE
N
TO
WRITE
A
CHARACTER
TO
THE
PTH
POSITION
IN
THE
ALLOCATED
MEMORY
AT
INDEX
I
ENTER
W
I
P
N
WHEN
FINISHED
ENTER
Q
WE
LL
PRESENT
THE
PROGRAM
CODE
LATER
IN
SECTION
A
AND
ILLUSTRATE
HOW
TO
USE
IT
A
MALLOC
CHECKING
THE
MEMORY
ALLOCATION
FUNCTIONS
PROVIDED
BY
THE
GNU
C
LIBRARY
CAN
DETECT
WRITING
BEFORE
THE
BEGINNING
OF
AN
ALLOCATION
AND
DEALLOCATING
THE
SAME
ALLOCATION
TWICE
DEFINING
THE
ENVIRONMENT
VARIABLE
TO
THE
VALUE
CAUSES
A
PROGRAM
TO
HALT
WHEN
SUCH
AN
ERROR
IS
DETECTED
NOTE
THE
ENVIRONMENT
VARIABLE
ENDING
UNDER
SCORE
THERE
IS
NO
NEED
TO
RECOMPILE
THE
PROGRAM
WE
ILLUSTRATE
DIAGNOSING
A
WRITE
TO
MEMORY
TO
A
POSITION
JUST
BEFORE
THE
BEGINNING
OF
AN
ALLOCATION
EXPORT
MALLOC
USE
PLEASE
ENTER
A
COMMAND
A
PLEASE
ENTER
A
COMMAND
W
PLEASE
ENTER
A
COMMAND
D
ABORTED
CORE
DUMPED
EXPORT
TURNS
ON
MALLOC
CHECKING
SPECIFYING
THE
VALUE
CAUSES
THE
PROGRAM
TO
HALT
AS
SOON
AS
AN
ERROR
IS
DETECTED
USING
MALLOC
CHECKING
IS
ADVANTAGEOUS
BECAUSE
THE
PROGRAM
NEED
NOT
BE
RECOM
PILED
BUT
ITS
CAPABILITY
TO
DIAGNOSE
ERRORS
IS
LIMITED
BASICALLY
IT
CHECKS
THAT
THE
ALLOCA
TOR
DATA
STRUCTURES
HAVE
NOT
BEEN
CORRUPTED
THUS
IT
CAN
DETECT
DOUBLE
DEALLOCATION
OF
THE
SAME
ALLOCATION
ALSO
WRITING
JUST
BEFORE
THE
BEGINNING
OF
A
MEMORY
ALLOCATION
CAN
USUALLY
BE
DETECTED
BECAUSE
THE
ALLOCATOR
STORES
THE
SIZE
OF
EACH
MEMORY
ALLOCATION
JUST
BEFORE
THE
ALLOCATED
REGION
THUS
WRITING
JUST
BEFORE
THE
ALLOCATED
MEMORY
WILL
CORRUPT
THIS
NUMBER
UNFORTUNATELY
CONSISTENCY
CHECKING
CAN
OCCUR
ONLY
WHEN
YOUR
PROGRAM
CALLS
ALLOCATION
ROUTINES
NOT
WHEN
IT
ACCESSES
MEMORY
SO
MANY
ILLEGAL
READS
AND
WRITES
CAN
OCCUR
BEFORE
AN
ERROR
IS
DETECTED
IN
THE
PREVIOUS
EXAMPLE
THE
ILLEGAL
WRITE
WAS
DETECTED
ONLY
WHEN
THE
ALLOCATED
MEMORY
WAS
DEALLOCATED
A
FINDING
MEMORY
LEAKS
USING
MTRACE
THE
MTRACE
TOOL
HELPS
DIAGNOSE
THE
MOST
COMMON
ERROR
WHEN
USING
DYNAMIC
MEMORY
FAILURE
TO
MATCH
ALLOCATIONS
AND
DEALLOCATIONS
THERE
ARE
FOUR
STEPS
TO
USING
MTRACE
WHICH
IS
AVAILABLE
WITH
THE
GNU
C
LIBRARY
MODIFY
THE
SOURCE
CODE
TO
INCLUDE
MCHECK
H
AND
TO
INVOKE
MTRACE
AS
SOON
AS
THE
PROGRAM
STARTS
AT
THE
BEGINNING
OF
MAIN
THE
CALL
TO
MTRACE
TURNS
ON
TRACKING
OF
MEMORY
ALLOCATIONS
AND
DEALLOCATIONS
SPECIFY
THE
NAME
OF
A
FILE
TO
STORE
INFORMATION
ABOUT
ALL
MEMORY
ALLOCATIONS
AND
DEALLOCATIONS
EXPORT
MEMORY
LOG
RUN
THE
PROGRAM
ALL
MEMORY
ALLOCATIONS
AND
DEALLOCATIONS
ARE
STORED
IN
THE
LOGGING
FILE
USING
THE
MTRACE
COMMAND
ANALYZE
THE
MEMORY
ALLOCATIONS
AND
DEALLOCATIONS
TO
ENSURE
THAT
THEY
MATCH
MTRACE
THE
MESSAGES
PRODUCED
BY
MTRACE
ARE
RELATIVELY
EASY
TO
UNDERSTAND
FOR
EXAMPLE
FOR
OUR
MALLOC
USE
EXAMPLE
THE
OUTPUT
WOULD
LOOK
LIKE
THIS
FREE
WAS
NEVER
ALLOC
D
MALLOC
USE
C
MEMORY
NOT
FREED
ADDRESS
SIZE
CALLER
AT
MALLOC
USE
C
THESE
MESSAGES
INDICATE
AN
ATTEMPT
ON
LINE
OF
MALLOC
USE
C
TO
FREE
MEMORY
THAT
WAS
NEVER
ALLOCATED
AND
AN
ALLOCATION
OF
MEMORY
ON
LINE
THAT
WAS
NEVER
FREED
MTRACE
DIAGNOSES
ERRORS
BY
HAVING
THE
EXECUTABLE
RECORD
ALL
MEMORY
ALLOCATIONS
AND
DEALLOCATIONS
IN
THE
FILE
SPECIFIED
BY
THE
ENVIRONMENT
VARIABLE
THE
EXECUTABLE
MUST
TERMINATE
NORMALLY
FOR
THE
DATA
TO
BE
WRITTEN
THE
MTRACE
COMMAND
ANALYZES
THIS
FILE
AND
LISTS
UNMATCHED
ALLOCATIONS
AND
DEALLOCATIONS
A
USING
CCMALLOC
THE
CCMALLOC
LIBRARY
DIAGNOSES
DYNAMIC
MEMORY
ERRORS
BY
REPLACING
MALLOC
AND
FREE
WITH
CODE
TRACING
THEIR
USE
IF
THE
PROGRAM
TERMINATES
GRACEFULLY
IT
PRODUCES
A
REPORT
OF
MEMORY
LEAKS
AND
OTHER
ERRORS
THE
CCMALLOC
LIBRARY
WAS
WRITTEN
BY
ARMIN
BIERCE
YOU
LL
PROBABLY
HAVE
TO
DOWNLOAD
AND
INSTALL
THE
CCMALLOC
LIBRARY
YOURSELF
DOWNLOAD
IT
FROM
HTTP
WWW
INF
ETHZ
CH
PERSONAL
BIERE
PROJECTS
CCMALLOC
UNPACK
THE
CODE
AND
RUN
CONFIGURE
RUN
MAKE
AND
MAKE
INSTALL
COPY
THE
CCMALLOC
CFG
FILE
TO
THE
DIRECTORY
WHERE
YOU
LL
RUN
THE
PROGRAM
YOU
WANT
TO
CHECK
AND
RENAME
THE
COPY
TO
CCMALLOC
NOW
YOU
ARE
READY
TO
USE
THE
TOOL
THE
PROGRAM
OBJECT
FILES
MUST
BE
LINKED
WITH
CCMALLOC
LIBRARY
AND
THE
DYNAMIC
LINKING
LIBRARY
APPEND
LCCMALLOC
LDL
TO
YOUR
LINK
COMMAND
FOR
INSTANCE
GCC
G
WALL
PEDANTIC
MALLOC
USE
O
O
CCMALLOC
USE
LCCMALLOC
LDL
EXECUTE
THE
PROGRAM
TO
PRODUCE
A
REPORT
FOR
EXAMPLE
RUNNING
OUR
MALLOC
USE
PRO
GRAM
TO
ALLOCATE
BUT
NOT
DEALLOCATE
MEMORY
PRODUCES
THE
FOLLOWING
REPORT
CCMALLOC
USE
FILE
NAME
A
OUT
DOES
NOT
CONTAIN
VALID
SYMBOLS
TRYING
TO
FIND
EXECUTABLE
IN
CURRENT
DIRECTORY
USING
SYMBOLS
FROM
CCMALLOC
USE
TO
SPEED
UP
THIS
SEARCH
SPECIFY
FILE
CCMALLOC
USE
IN
THE
STARTUP
FILE
CCMALLOC
PLEASE
ENTER
A
COMMAND
A
PLEASE
ENTER
A
COMMAND
Q
CCMALLOC
REPORT
TOTAL
OF
ALLOCATED
DEALLOCATED
GARBAGE
BYTES
ALLOCATIONS
NUMBER
OF
CHECKS
NUMBER
OF
COUNTS
RETRIEVING
FUNCTION
NAMES
FOR
ADDRESSES
DONE
READING
FILE
INFO
FROM
GDB
DONE
SORTING
BY
NUMBER
OF
NOT
RECLAIMED
BYTES
DONE
NUMBER
OF
CALL
CHAINS
NUMBER
OF
IGNORED
CALL
CHAINS
NUMBER
OF
REPORTED
CALL
CHAINS
NUMBER
OF
INTERNAL
CALL
CHAINS
NUMBER
OF
LIBRARY
CALL
CHAINS
BYTES
OF
GARBAGE
ALLOCATED
IN
ALLOCATION
IN
IN
MAIN
AT
MALLOC
USE
C
IN
ALLOCATE
AT
MALLOC
USE
C
IN
MALLOC
AT
SRC
WRAPPER
C
THE
LAST
FEW
LINES
INDICATE
THE
CHAIN
OF
FUNCTION
CALLS
THAT
ALLOCATED
MEMORY
THAT
WAS
NOT
DEALLOCATED
TO
USE
CCMALLOC
TO
DIAGNOSE
WRITES
BEFORE
THE
BEGINNING
OR
AFTER
THE
END
OF
THE
ALLOCATED
REGION
YOU
LL
HAVE
TO
MODIFY
THE
CCMALLOC
FILE
IN
THE
CURRENT
DIRECTORY
THIS
FILE
IS
READ
WHEN
THE
PROGRAM
STARTS
EXECUTION
A
ELECTRIC
FENCE
WRITTEN
BY
BRUCE
PERENS
ELECTRIC
FENCE
HALTS
EXECUTING
PROGRAMS
ON
THE
EXACT
LINE
WHERE
A
WRITE
OR
A
READ
OUTSIDE
AN
ALLOCATION
OCCURS
THIS
IS
THE
ONLY
TOOL
THAT
DISCOVERS
ILLEGAL
READS
IT
IS
INCLUDED
IN
MOST
GNU
LINUX
DISTRIBUTIONS
BUT
THE
SOURCE
CODE
CAN
BE
FOUND
AT
HTTP
WWW
PERENS
COM
FREESOFTWARE
AS
WITH
CCMALLOC
YOUR
PROGRAM
OBJECT
FILES
MUST
BE
LINKED
WITH
ELECTRIC
FENCE
LIBRARY
BY
APPENDING
LEFENCE
TO
THE
LINKING
COMMAND
FOR
INSTANCE
GCC
G
WALL
PEDANTIC
MALLOC
USE
O
O
EMALLOC
USE
LEFENCE
AS
THE
PROGRAM
RUNS
ALLOCATED
MEMORY
USES
ARE
CHECKED
FOR
CORRECTNESS
A
VIOLATION
CAUSES
A
SEGMENTATION
FAULT
EMALLOC
USE
ELECTRIC
FENCE
COPYRIGHT
C
BRUCE
PERENS
PLEASE
ENTER
A
COMMAND
A
PLEASE
ENTER
A
COMMAND
R
SEGMENTATION
FAULT
USING
A
DEBUGGER
YOU
CAN
DETERMINE
THE
CONTEXT
OF
THE
ILLEGAL
ACTION
BY
DEFAULT
ELECTRIC
FENCE
DIAGNOSES
ONLY
ACCESSES
BEYOND
THE
ENDS
OF
ALLOCATIONS
TO
FIND
ACCESSES
BEFORE
THE
BEGINNING
OF
ALLOCATIONS
INSTEAD
OF
ACCESSES
BEYOND
THE
END
OF
ALLOCATIONS
USE
THIS
CODE
EXPORT
TO
FIND
ACCESSES
TO
DEALLOCATED
MEMORY
SET
TO
MORE
CAPABILITIES
ARE
DESCRIBED
IN
THE
LIBEFENCE
MANUAL
PAGE
ELECTRIC
FENCE
DIAGNOSES
ILLEGAL
MEMORY
ACCESSES
BY
STORING
EACH
ALLOCATION
ON
AT
LEAST
TWO
MEMORY
PAGES
IT
PLACES
THE
ALLOCATION
AT
THE
END
OF
THE
FIRST
PAGE
ANY
ACCESS
BEYOND
THE
END
OF
THE
ALLOCATION
ON
THE
SECOND
PAGE
CAUSES
A
SEGMENTATION
FAULT
IF
YOU
SET
TO
IT
PLACES
THE
ALLOCATION
AT
THE
BEGINNING
OF
THE
SECOND
PAGE
INSTEAD
BECAUSE
IT
ALLOCATES
TWO
MEMORY
PAGES
PER
CALL
TO
MALLOC
ELECTRIC
FENCE
CAN
USE
AN
ENORMOUS
AMOUNT
OF
MEMORY
USE
THIS
LIBRARY
FOR
DEBUGGING
ONLY
A
CHOOSING
AMONG
THE
DIFFERENT
MEMORY
DEBUGGING
TOOLS
WE
HAVE
DISCUSSED
FOUR
SEPARATE
INCOMPATIBLE
TOOLS
TO
DIAGNOSE
ERRONEOUS
USE
OF
DYNAMIC
MEMORY
HOW
DOES
A
GNU
LINUX
PROGRAMMER
ENSURE
THAT
DYNAMIC
MEM
ORY
IS
CORRECTLY
USED
NO
TOOL
GUARANTEES
DIAGNOSING
ALL
ERRORS
BUT
USING
ANY
OF
THEM
DOES
INCREASE
THE
PROBABILITY
OF
FINDING
ERRORS
TO
EASE
FINDING
DYNAMICALLY
ALLOCATED
MEMORY
ERRORS
SEPARATELY
DEVELOP
AND
TEST
THE
CODE
THAT
DEALS
WITH
DYNAMIC
MEMORY
THIS
REDUCES
THE
AMOUNT
OF
CODE
THAT
YOU
MUST
SEARCH
FOR
ERRORS
IF
YOU
ARE
USING
C
WRITE
A
CLASS
THAT
HANDLES
ALL
DYNAMIC
MEMORY
USE
IF
YOU
ARE
USING
C
MINIMIZE
THE
NUMBER
OF
FUNCTIONS
USING
ALLOCATION
AND
DEALLOCATION
WHEN
TESTING
THIS
CODE
BE
SURE
TO
USE
ONLY
ONE
TOOL
AT
A
ONE
TIME
BECAUSE
THEY
ARE
INCOMPATIBLE
WHEN
TESTING
A
PROGRAM
BE
SURE
TO
VARY
HOW
THE
PROGRAM
EXECUTES
TO
TEST
THE
MOST
COMMONLY
EXE
CUTED
PORTIONS
OF
THE
CODE
WHICH
OF
THE
FOUR
TOOLS
SHOULD
YOU
USE
BECAUSE
FAILING
TO
MATCH
ALLOCATIONS
AND
DEALLOCATIONS
IS
THE
MOST
COMMON
DYNAMIC
MEMORY
ERROR
USE
MTRACE
DURING
INITIAL
DEVELOPMENT
THE
PROGRAM
IS
AVAILABLE
ON
ALL
GNU
LINUX
SYSTEMS
AND
HAS
BEEN
WELL
TESTED
AFTER
ENSURING
THAT
THE
NUMBER
OF
ALLOCATIONS
AND
DEALLOCATIONS
MATCH
USE
ELECTRIC
FENCE
TO
FIND
ILLEGAL
MEMORY
ACCESSES
THIS
WILL
ELIMINATE
ALMOST
ALL
MEMORY
ERRORS
WHEN
USING
ELECTRIC
FENCE
YOU
WILL
NEED
TO
BE
CAREFUL
TO
NOT
PERFORM
TOO
MANY
ALLOCATIONS
AND
DEALLOCATIONS
BECAUSE
EACH
ALLOCATION
REQUIRES
AT
LEAST
TWO
PAGES
OF
MEMORY
USING
THESE
TWO
TOOLS
WILL
REVEAL
MOST
MEMORY
ERRORS
A
SOURCE
CODE
FOR
THE
DYNAMIC
MEMORY
PROGRAM
LISTING
A
SHOWS
THE
SOURCE
CODE
FOR
A
PROGRAM
ILLUSTRATING
DYNAMIC
MEMORY
ALLOCA
TION
DEALLOCATION
AND
USE
SEE
SECTION
A
A
PROGRAM
TO
TEST
MEMORY
ALLOCATION
AND
DEALLOCATION
FOR
A
DESCRIPTION
OF
HOW
TO
USE
IT
LISTING
A
MALLOC
USE
C
DYNAMIC
MEMORY
ALLOCATION
CHECKING
EXAMPLE
USE
C
DYNAMIC
MEMORY
ALLOCATION
FUNCTIONS
INVOKE
THE
PROGRAM
USING
ONE
COMMAND
LINE
ARGUMENT
SPECIFYING
THE
SIZE
OF
AN
ARRAY
THIS
ARRAY
CONSISTS
OF
POINTERS
TO
POSSIBLY
ALLOCATED
ARRAYS
WHEN
THE
PROGRAMMING
IS
RUNNING
SELECT
AMONG
THE
FOLLOWING
COMMANDS
O
ALLOCATE
MEMORY
A
INDEX
MEMORY
SIZE
O
DEALLOCATE
MEMORY
D
INDEX
O
READ
FROM
MEMORY
R
INDEX
POSITION
WITHIN
ALLOCATION
O
WRITE
TO
MEMORY
W
INDEX
POSITION
WITHIN
ALLOCATION
O
QUIT
Q
THE
USER
IS
RESPONSIBLE
FOR
OBEYING
OR
DISOBEYING
THE
RULES
ON
DYNAMIC
MEMORY
USE
IFDEF
MTRACE
INCLUDE
MCHECK
H
ENDIF
MTRACE
INCLUDE
STDIO
H
INCLUDE
STDLIB
H
INCLUDE
ASSERT
H
ALLOCATE
MEMORY
WITH
THE
SPECIFIED
SIZE
RETURNING
NONZERO
UPON
SUCCESS
VOID
ALLOCATE
CHAR
ARRAY
SIZE
ARRAY
MALLOC
SIZE
DEALLOCATE
MEMORY
VOID
DEALLOCATE
CHAR
ARRAY
CONTINUES
LISTING
A
CONTINUED
FREE
VOID
ARRAY
READ
FROM
A
POSITION
IN
MEMORY
VOID
CHAR
ARRAY
INT
POSITION
CHAR
CHARACTER
ARRAY
POSITION
WRITE
TO
A
POSITION
IN
MEMORY
VOID
CHAR
ARRAY
INT
POSITION
ARRAY
POSITION
A
INT
MAIN
INT
ARGC
CHAR
ARGV
CHAR
ARRAY
UNSIGNED
CHAR
COMMAND
UNSIGNED
CHAR
INT
INT
ERROR
IFDEF
MTRACE
MTRACE
ENDIF
MTRACE
IF
ARGC
FPRINTF
STDERR
ARRAY
SIZE
N
ARGV
RETURN
STRTOUL
ARGV
ARRAY
CHAR
CALLOC
SIZEOF
CHAR
ASSERT
ARRAY
FOLLOW
THE
USER
COMMANDS
WHILE
ERROR
PRINTF
PLEASE
ENTER
A
COMMAND
GETCHAR
ASSERT
EOF
SWITCH
CASE
A
FGETS
COMMAND
SIZEOF
COMMAND
STDIN
IF
SSCANF
COMMAND
U
I
ALLOCATE
ARRAY
ELSE
ERROR
BREAK
CASE
D
FGETS
COMMAND
SIZEOF
COMMAND
STDIN
IF
SSCANF
COMMAND
U
DEALLOCATE
ARRAY
ELSE
ERROR
BREAK
CASE
R
FGETS
COMMAND
SIZEOF
COMMAND
STDIN
IF
SSCANF
COMMAND
U
I
ARRAY
ELSE
ERROR
BREAK
CASE
W
FGETS
COMMAND
SIZEOF
COMMAND
STDIN
IF
SSCANF
COMMAND
U
I
ARRAY
ELSE
ERROR
BREAK
CASE
Q
FREE
VOID
ARRAY
RETURN
DEFAULT
ERROR
FREE
VOID
ARRAY
RETURN
A
PROFILING
NOW
THAT
YOUR
PROGRAM
IS
HOPEFULLY
CORRECT
WE
TURN
TO
SPEEDING
ITS
EXECUTION
USING
THE
PROFILER
GPROF
YOU
CAN
DETERMINE
WHICH
FUNCTIONS
REQUIRE
THE
MOST
EXECU
TION
TIME
THIS
CAN
HELP
YOU
DETERMINE
WHICH
PARTS
OF
THE
PROGRAM
TO
OPTIMIZE
OR
REWRITE
TO
EXECUTE
MORE
QUICKLY
IT
CAN
ALSO
HELP
YOU
FIND
ERRORS
FOR
EXAMPLE
YOU
MAY
FIND
THAT
A
PARTICULAR
FUNCTION
IS
CALLED
MANY
MORE
TIMES
THAN
YOU
EXPECT
IN
THIS
SECTION
WE
DESCRIBE
HOW
TO
USE
GPROF
REWRITING
CODE
TO
RUN
MORE
QUICKLY
REQUIRES
CREATIVITY
AND
CAREFUL
CHOICE
OF
ALGORITHMS
OBTAINING
PROFILING
INFORMATION
REQUIRES
THREE
STEPS
COMPILE
AND
LINK
YOUR
PROGRAM
TO
ENABLE
PROFILING
EXECUTE
YOUR
PROGRAM
TO
GENERATE
PROFILING
DATA
USE
GPROF
TO
ANALYZE
AND
DISPLAY
THE
PROFILING
DATA
BEFORE
WE
ILLUSTRATE
THESE
STEPS
WE
INTRODUCE
A
LARGE
ENOUGH
PROGRAM
TO
MAKE
PROFILING
INTERESTING
A
A
SIMPLE
CALCULATOR
TO
ILLUSTRATE
PROFILING
WE
LL
USE
A
SIMPLE
CALCULATOR
PROGRAM
TO
ENSURE
THAT
THE
CALCULA
TOR
TAKES
A
NONTRIVIAL
AMOUNT
OF
TIME
WE
LL
USE
UNARY
NUMBERS
FOR
CALCULATIONS
SOME
THING
WE
WOULD
DEFINITELY
NOT
WANT
TO
DO
IN
A
REAL
WORLD
PROGRAM
CODE
FOR
THIS
PROGRAM
APPEARS
AT
THE
END
OF
THIS
CHAPTER
A
UNARY
NUMBER
IS
REPRESENTED
BY
AS
MANY
SYMBOLS
AS
ITS
VALUE
FOR
EXAMPLE
THE
NUMBER
IS
REPRESENTED
BY
X
BY
XX
AND
BY
XXX
INSTEAD
OF
USING
X
OUR
PROGRAM
REPRESENTS
A
NON
NEGATIVE
NUMBER
USING
A
LINKED
LIST
WITH
AS
MANY
ELEMENTS
AS
THE
NUMBER
VALUE
THE
NUMBER
C
FILE
CONTAINS
ROUTINES
TO
CREATE
THE
NUMBER
ADD
TO
A
NUMBER
SUBTRACT
FROM
A
NUMBER
AND
ADD
SUBTRACT
AND
MULTIPLY
NUMBERS
ANOTHER
FUNCTION
CONVERTS
A
STRING
HOLDING
A
NON
NEGATIVE
DECIMAL
NUMBER
TO
A
UNARY
NUMBER
AND
A
FUNCTION
CONVERTS
FROM
A
UNARY
NUMBER
TO
AN
INT
ADDITION
IS
IMPLE
MENTED
USING
REPEATED
ADDITION
OF
WHILE
SUBTRACTION
USES
REPEATED
REMOVAL
OF
MULTIPLICATION
IS
DEFINED
USING
REPEATED
ADDITION
THE
UNARY
PREDICATES
EVEN
AND
ODD
EACH
RETURN
THE
UNARY
NUMBER
FOR
IF
AND
ONLY
IF
ITS
ONE
OPERAND
IS
EVEN
OR
ODD
RESPECTIVELY
OTHERWISE
THEY
RETURN
THE
UNARY
NUMBER
FOR
THE
TWO
PREDICATES
ARE
MUTUALLY
RECURSIVE
FOR
EXAMPLE
A
NUMBER
IS
EVEN
IF
IT
IS
ZERO
OR
IF
ONE
LESS
THAN
THE
NUMBER
IS
ODD
THUS
FAR
WE
HAVE
USED
KMALLOC
AND
KFREE
FOR
THE
ALLOCATION
AND
FREEING
OF
MEMORY
THE
LINUX
KERNEL
OFFERS
A
RICHER
SET
OF
MEMORY
ALLOCATION
PRIMITIVES
HOWEVER
IN
THIS
CHAPTER
WE
LOOK
AT
OTHER
WAYS
OF
USING
MEMORY
IN
DEVICE
DRIVERS
AND
HOW
TO
OPTI
MIZE
YOUR
SYSTEM
MEMORY
RESOURCES
WE
DO
NOT
GET
INTO
HOW
THE
DIFFERENT
ARCHITEC
TURES
ACTUALLY
ADMINISTER
MEMORY
MODULES
ARE
NOT
INVOLVED
IN
ISSUES
OF
SEGMENTATION
PAGING
AND
SO
ON
SINCE
THE
KERNEL
OFFERS
A
UNIFIED
MEMORY
MANAGE
MENT
INTERFACE
TO
THE
DRIVERS
IN
ADDITION
WE
WON
T
DESCRIBE
THE
INTERNAL
DETAILS
OF
MEMORY
MANAGEMENT
IN
THIS
CHAPTER
BUT
DEFER
IT
TO
CHAPTER
THE
REAL
STORY
OF
KMALLOC
THE
KMALLOC
ALLOCATION
ENGINE
IS
A
POWERFUL
TOOL
AND
EASILY
LEARNED
BECAUSE
OF
ITS
SIMILARITY
TO
MALLOC
THE
FUNCTION
IS
FAST
UNLESS
IT
BLOCKS
AND
DOESN
T
CLEAR
THE
MEM
ORY
IT
OBTAINS
THE
ALLOCATED
REGION
STILL
HOLDS
ITS
PREVIOUS
CONTENT
THE
ALLOCATED
REGION
IS
ALSO
CONTIGUOUS
IN
PHYSICAL
MEMORY
IN
THE
NEXT
FEW
SECTIONS
WE
TALK
IN
DETAIL
ABOUT
KMALLOC
SO
YOU
CAN
COMPARE
IT
WITH
THE
MEMORY
ALLOCATION
TECHNIQUES
THAT
WE
DISCUSS
LATER
THE
FLAGS
ARGUMENT
REMEMBER
THAT
THE
PROTOTYPE
FOR
KMALLOC
IS
INCLUDE
LINUX
SLAB
H
VOID
KMALLOC
SIZE
INT
FLAGS
AMONG
OTHER
THINGS
THIS
IMPLIES
THAT
YOU
SHOULD
EXPLICITLY
CLEAR
ANY
MEMORY
THAT
MIGHT
BE
EXPOSED
TO
USER
SPACE
OR
WRITTEN
TO
A
DEVICE
OTHERWISE
YOU
RISK
DISCLOSING
INFORMATION
THAT
SHOULD
BE
KEPT
PRIVATE
THE
FIRST
ARGUMENT
TO
KMALLOC
IS
THE
SIZE
OF
THE
BLOCK
TO
BE
ALLOCATED
THE
SECOND
ARGUMENT
THE
ALLOCATION
FLAGS
IS
MUCH
MORE
INTERESTING
BECAUSE
IT
CONTROLS
THE
BEHAVIOR
OF
KMALLOC
IN
A
NUMBER
OF
WAYS
THE
MOST
COMMONLY
USED
FLAG
MEANS
THAT
THE
ALLOCATION
INTERNALLY
PER
FORMED
BY
CALLING
EVENTUALLY
WHICH
IS
THE
SOURCE
OF
THE
PRE
FIX
IS
PERFORMED
ON
BEHALF
OF
A
PROCESS
RUNNING
IN
KERNEL
SPACE
IN
OTHER
WORDS
THIS
MEANS
THAT
THE
CALLING
FUNCTION
IS
EXECUTING
A
SYSTEM
CALL
ON
BEHALF
OF
A
PROCESS
USING
MEANS
THAT
KMALLOC
CAN
PUT
THE
CURRENT
PROCESS
TO
SLEEP
WAITING
FOR
A
PAGE
WHEN
CALLED
IN
LOW
MEMORY
SITUATIONS
A
FUNCTION
THAT
ALLOCATES
MEMORY
USING
MUST
THEREFORE
BE
REENTRANT
AND
CANNOT
BE
RUNNING
IN
ATOMIC
CON
TEXT
WHILE
THE
CURRENT
PROCESS
SLEEPS
THE
KERNEL
TAKES
PROPER
ACTION
TO
LOCATE
SOME
FREE
MEMORY
EITHER
BY
FLUSHING
BUFFERS
TO
DISK
OR
BY
SWAPPING
OUT
MEMORY
FROM
A
USER
PROCESS
ISN
T
ALWAYS
THE
RIGHT
ALLOCATION
FLAG
TO
USE
SOMETIMES
KMALLOC
IS
CALLED
FROM
OUTSIDE
A
PROCESS
CONTEXT
THIS
TYPE
OF
CALL
CAN
HAPPEN
FOR
INSTANCE
IN
INTER
RUPT
HANDLERS
TASKLETS
AND
KERNEL
TIMERS
IN
THIS
CASE
THE
CURRENT
PROCESS
SHOULD
NOT
BE
PUT
TO
SLEEP
AND
THE
DRIVER
SHOULD
USE
A
FLAG
OF
INSTEAD
THE
KER
NEL
NORMALLY
TRIES
TO
KEEP
SOME
FREE
PAGES
AROUND
IN
ORDER
TO
FULFILL
ATOMIC
ALLOCA
TION
WHEN
IS
USED
KMALLOC
CAN
USE
EVEN
THE
LAST
FREE
PAGE
IF
THAT
LAST
PAGE
DOES
NOT
EXIST
HOWEVER
THE
ALLOCATION
FAILS
OTHER
FLAGS
CAN
BE
USED
IN
PLACE
OF
OR
IN
ADDITION
TO
AND
ALTHOUGH
THOSE
TWO
COVER
MOST
OF
THE
NEEDS
OF
DEVICE
DRIVERS
ALL
THE
FLAGS
ARE
DEFINED
IN
LINUX
GFP
H
AND
INDIVIDUAL
FLAGS
ARE
PREFIXED
WITH
A
DOUBLE
UNDERSCORE
SUCH
AS
IN
ADDITION
THERE
ARE
SYMBOLS
THAT
REPRESENT
FREQUENTLY
USED
COMBINA
TIONS
OF
FLAGS
THESE
LACK
THE
PREFIX
AND
ARE
SOMETIMES
CALLED
ALLOCATION
PRIORITIES
THE
LATTER
INCLUDE
USED
TO
ALLOCATE
MEMORY
FROM
INTERRUPT
HANDLERS
AND
OTHER
CODE
OUTSIDE
OF
A
PROCESS
CONTEXT
NEVER
SLEEPS
NORMAL
ALLOCATION
OF
KERNEL
MEMORY
MAY
SLEEP
USED
TO
ALLOCATE
MEMORY
FOR
USER
SPACE
PAGES
IT
MAY
SLEEP
LIKE
BUT
ALLOCATES
FROM
HIGH
MEMORY
IF
ANY
HIGH
MEMORY
IS
DESCRIBED
IN
THE
NEXT
SUBSECTION
THESE
FLAGS
FUNCTION
LIKE
BUT
THEY
ADD
RESTRICTIONS
ON
WHAT
THE
KER
NEL
CAN
DO
TO
SATISFY
THE
REQUEST
A
ALLOCATION
IS
NOT
ALLOWED
TO
PERFORM
ANY
FILESYSTEM
CALLS
WHILE
DISALLOWS
THE
INITIATION
OF
ANY
I
O
AT
ALL
THEY
ARE
USED
PRIMARILY
IN
THE
FILESYSTEM
AND
VIRTUAL
MEMORY
CODE
WHERE
AN
ALLO
CATION
MAY
BE
ALLOWED
TO
SLEEP
BUT
RECURSIVE
FILESYSTEM
CALLS
WOULD
BE
A
BAD
IDEA
THE
ALLOCATION
FLAGS
LISTED
ABOVE
CAN
BE
AUGMENTED
BY
AN
ORING
IN
ANY
OF
THE
FOLLOW
ING
FLAGS
WHICH
CHANGE
HOW
THE
ALLOCATION
IS
CARRIED
OUT
THIS
FLAG
REQUESTS
ALLOCATION
TO
HAPPEN
IN
THE
DMA
CAPABLE
MEMORY
ZONE
THE
EXACT
MEANING
IS
PLATFORM
DEPENDENT
AND
IS
EXPLAINED
IN
THE
FOLLOWING
SECTION
THIS
FLAG
INDICATES
THAT
THE
ALLOCATED
MEMORY
MAY
BE
LOCATED
IN
HIGH
MEMORY
NORMALLY
THE
MEMORY
ALLOCATOR
TRIES
TO
RETURN
CACHE
WARM
PAGES
PAGES
THAT
ARE
LIKELY
TO
BE
FOUND
IN
THE
PROCESSOR
CACHE
INSTEAD
THIS
FLAG
REQUESTS
A
COLD
PAGE
WHICH
HAS
NOT
BEEN
USED
IN
SOME
TIME
IT
IS
USEFUL
FOR
ALLOCATING
PAGES
FOR
DMA
READS
WHERE
PRESENCE
IN
THE
PROCESSOR
CACHE
IS
NOT
USEFUL
SEE
THE
SECTION
DIRECT
MEMORY
ACCESS
IN
CHAPTER
FOR
A
FULL
DISCUSSION
OF
HOW
TO
ALLOCATE
DMA
BUFFERS
THIS
RARELY
USED
FLAG
PREVENTS
THE
KERNEL
FROM
ISSUING
WARNINGS
WITH
PRINTK
WHEN
AN
ALLOCATION
CANNOT
BE
SATISFIED
THIS
FLAG
MARKS
A
HIGH
PRIORITY
REQUEST
WHICH
IS
ALLOWED
TO
CONSUME
EVEN
THE
LAST
PAGES
OF
MEMORY
SET
ASIDE
BY
THE
KERNEL
FOR
EMERGENCIES
THESE
FLAGS
MODIFY
HOW
THE
ALLOCATOR
BEHAVES
WHEN
IT
HAS
DIFFICULTY
SATISFYING
AN
ALLOCATION
MEANS
TRY
A
LITTLE
HARDER
BY
REPEATING
THE
ATTEMPT
BUT
THE
ALLOCATION
CAN
STILL
FAIL
THE
FLAG
TELLS
THE
ALLOCATOR
NEVER
TO
FAIL
IT
WORKS
AS
HARD
AS
NEEDED
TO
SATISFY
THE
REQUEST
USE
OF
IS
VERY
STRONGLY
DISCOURAGED
THERE
WILL
PROBABLY
NEVER
BE
A
VALID
REASON
TO
USE
IT
IN
A
DEVICE
DRIVER
FINALLY
TELLS
THE
ALLOCATOR
TO
GIVE
UP
IMMEDIATELY
IF
THE
REQUESTED
MEMORY
IS
NOT
AVAILABLE
MEMORY
ZONES
BOTH
AND
HAVE
A
PLATFORM
DEPENDENT
ROLE
ALTHOUGH
THEIR
USE
IS
VALID
FOR
ALL
PLATFORMS
THE
LINUX
KERNEL
KNOWS
ABOUT
A
MINIMUM
OF
THREE
MEMORY
ZONES
DMA
CAPABLE
MEMORY
NORMAL
MEMORY
AND
HIGH
MEMORY
WHILE
ALLOCATION
NORMALLY
HAPPENS
IN
THE
NORMAL
ZONE
SETTING
EITHER
OF
THE
BITS
JUST
MENTIONED
REQUIRES
MEMORY
TO
BE
ALLO
CATED
FROM
A
DIFFERENT
ZONE
THE
IDEA
IS
THAT
EVERY
COMPUTER
PLATFORM
THAT
MUST
KNOW
ABOUT
SPECIAL
MEMORY
RANGES
INSTEAD
OF
CONSIDERING
ALL
RAM
EQUIVALENTS
WILL
FALL
INTO
THIS
ABSTRACTION
DMA
CAPABLE
MEMORY
IS
MEMORY
THAT
LIVES
IN
A
PREFERENTIAL
ADDRESS
RANGE
WHERE
PERIPHERALS
CAN
PERFORM
DMA
ACCESS
ON
MOST
SANE
PLATFORMS
ALL
MEMORY
LIVES
IN
THIS
ZONE
ON
THE
THE
DMA
ZONE
IS
USED
FOR
THE
FIRST
MB
OF
RAM
WHERE
LEG
ACY
ISA
DEVICES
CAN
PERFORM
DMA
PCI
DEVICES
HAVE
NO
SUCH
LIMIT
HIGH
MEMORY
IS
A
MECHANISM
USED
TO
ALLOW
ACCESS
TO
RELATIVELY
LARGE
AMOUNTS
OF
MEMORY
ON
BIT
PLATFORMS
THIS
MEMORY
CANNOT
BE
DIRECTLY
ACCESSED
FROM
THE
KER
NEL
WITHOUT
FIRST
SETTING
UP
A
SPECIAL
MAPPING
AND
IS
GENERALLY
HARDER
TO
WORK
WITH
IF
YOUR
DRIVER
USES
LARGE
AMOUNTS
OF
MEMORY
HOWEVER
IT
WILL
WORK
BETTER
ON
LARGE
SYS
TEMS
IF
IT
CAN
USE
HIGH
MEMORY
SEE
THE
SECTION
HIGH
AND
LOW
MEMORY
IN
CHAPTER
FOR
A
DETAILED
DESCRIPTION
OF
HOW
HIGH
MEMORY
WORKS
AND
HOW
TO
USE
IT
WHENEVER
A
NEW
PAGE
IS
ALLOCATED
TO
FULFILL
A
MEMORY
ALLOCATION
REQUEST
THE
KERNEL
BUILDS
A
LIST
OF
ZONES
THAT
CAN
BE
USED
IN
THE
SEARCH
IF
IS
SPECIFIED
ONLY
THE
DMA
ZONE
IS
SEARCHED
IF
NO
MEMORY
IS
AVAILABLE
AT
LOW
ADDRESSES
ALLOCATION
FAILS
IF
NO
SPECIAL
FLAG
IS
PRESENT
BOTH
NORMAL
AND
DMA
MEMORY
ARE
SEARCHED
IF
IS
SET
ALL
THREE
ZONES
ARE
USED
TO
SEARCH
A
FREE
PAGE
NOTE
HOWEVER
THAT
KMALLOC
CANNOT
ALLOCATE
HIGH
MEMORY
THE
SITUATION
IS
MORE
COMPLICATED
ON
NONUNIFORM
MEMORY
ACCESS
NUMA
SYSTEMS
AS
A
GENERAL
RULE
THE
ALLOCATOR
ATTEMPTS
TO
LOCATE
MEMORY
LOCAL
TO
THE
PROCESSOR
PER
FORMING
THE
ALLOCATION
ALTHOUGH
THERE
ARE
WAYS
OF
CHANGING
THAT
BEHAVIOR
THE
MECHANISM
BEHIND
MEMORY
ZONES
IS
IMPLEMENTED
IN
MM
C
WHILE
INI
TIALIZATION
OF
THE
ZONE
RESIDES
IN
PLATFORM
SPECIFIC
FILES
USUALLY
IN
MM
INIT
C
WITHIN
THE
ARCH
TREE
WE
LL
REVISIT
THESE
TOPICS
IN
CHAPTER
THE
SIZE
ARGUMENT
THE
KERNEL
MANAGES
THE
SYSTEM
PHYSICAL
MEMORY
WHICH
IS
AVAILABLE
ONLY
IN
PAGE
SIZED
CHUNKS
AS
A
RESULT
KMALLOC
LOOKS
RATHER
DIFFERENT
FROM
A
TYPICAL
USER
SPACE
MALLOC
IMPLEMENTATION
A
SIMPLE
HEAP
ORIENTED
ALLOCATION
TECHNIQUE
WOULD
QUICKLY
RUN
INTO
TROUBLE
IT
WOULD
HAVE
A
HARD
TIME
WORKING
AROUND
THE
PAGE
BOUNDARIES
THUS
THE
KERNEL
USES
A
SPECIAL
PAGE
ORIENTED
ALLOCATION
TECHNIQUE
TO
GET
THE
BEST
USE
FROM
THE
SYSTEM
RAM
LINUX
HANDLES
MEMORY
ALLOCATION
BY
CREATING
A
SET
OF
POOLS
OF
MEMORY
OBJECTS
OF
FIXED
SIZES
ALLOCATION
REQUESTS
ARE
HANDLED
BY
GOING
TO
A
POOL
THAT
HOLDS
SUFFICIENTLY
LARGE
OBJECTS
AND
HANDING
AN
ENTIRE
MEMORY
CHUNK
BACK
TO
THE
REQUESTER
THE
MEM
ORY
MANAGEMENT
SCHEME
IS
QUITE
COMPLEX
AND
THE
DETAILS
OF
IT
ARE
NOT
NORMALLY
ALL
THAT
INTERESTING
TO
DEVICE
DRIVER
WRITERS
THE
ONE
THING
DRIVER
DEVELOPERS
SHOULD
KEEP
IN
MIND
THOUGH
IS
THAT
THE
KERNEL
CAN
ALLOCATE
ONLY
CERTAIN
PREDEFINED
FIXED
SIZE
BYTE
ARRAYS
IF
YOU
ASK
FOR
AN
ARBITRARY
AMOUNT
OF
MEMORY
YOU
RE
LIKELY
TO
GET
SLIGHTLY
MORE
THAN
YOU
ASKED
FOR
UP
TO
TWICE
AS
MUCH
ALSO
PROGRAMMERS
SHOULD
REMEMBER
THAT
THE
SMALLEST
ALLOCATION
THAT
KMALLOC
CAN
HANDLE
IS
AS
BIG
AS
OR
BYTES
DEPENDING
ON
THE
PAGE
SIZE
USED
BY
THE
SYSTEM
ARCHITECTURE
THERE
IS
AN
UPPER
LIMIT
TO
THE
SIZE
OF
MEMORY
CHUNKS
THAT
CAN
BE
ALLOCATED
BY
KMAL
LOC
THAT
LIMIT
VARIES
DEPENDING
ON
ARCHITECTURE
AND
KERNEL
CONFIGURATION
OPTIONS
IF
YOUR
CODE
IS
TO
BE
COMPLETELY
PORTABLE
IT
CANNOT
COUNT
ON
BEING
ABLE
TO
ALLOCATE
ANY
THING
LARGER
THAN
KB
IF
YOU
NEED
MORE
THAN
A
FEW
KILOBYTES
HOWEVER
THERE
ARE
BETTER
WAYS
THAN
KMALLOC
TO
OBTAIN
MEMORY
WHICH
WE
DESCRIBE
LATER
IN
THIS
CHAPTER
LOOKASIDE
CACHES
A
DEVICE
DRIVER
OFTEN
ENDS
UP
ALLOCATING
MANY
OBJECTS
OF
THE
SAME
SIZE
OVER
AND
OVER
GIVEN
THAT
THE
KERNEL
ALREADY
MAINTAINS
A
SET
OF
MEMORY
POOLS
OF
OBJECTS
THAT
ARE
ALL
THE
SAME
SIZE
WHY
NOT
ADD
SOME
SPECIAL
POOLS
FOR
THESE
HIGH
VOLUME
OBJECTS
IN
FACT
THE
KERNEL
DOES
IMPLEMENT
A
FACILITY
TO
CREATE
THIS
SORT
OF
POOL
WHICH
IS
OFTEN
CALLED
A
LOOKASIDE
CACHE
DEVICE
DRIVERS
NORMALLY
DO
NOT
EXHIBIT
THE
SORT
OF
MEMORY
BEHAVIOR
THAT
JUSTIFIES
USING
A
LOOKASIDE
CACHE
BUT
THERE
CAN
BE
EXCEPTIONS
THE
USB
AND
SCSI
DRIVERS
IN
LINUX
USE
CACHES
THE
CACHE
MANAGER
IN
THE
LINUX
KERNEL
IS
SOMETIMES
CALLED
THE
SLAB
ALLOCATOR
FOR
THAT
REASON
ITS
FUNCTIONS
AND
TYPES
ARE
DECLARED
IN
LINUX
SLAB
H
THE
SLAB
ALLOCATOR
IMPLEMENTS
CACHES
THAT
HAVE
A
TYPE
OF
THEY
ARE
CREATED
WITH
A
CALL
TO
CONST
CHAR
NAME
SIZE
OFFSET
UNSIGNED
LONG
FLAGS
VOID
CONSTRUCTOR
VOID
UNSIGNED
LONG
FLAGS
VOID
DESTRUCTOR
VOID
UNSIGNED
LONG
FLAGS
THE
FUNCTION
CREATES
A
NEW
CACHE
OBJECT
THAT
CAN
HOST
ANY
NUMBER
OF
MEMORY
AREAS
ALL
OF
THE
SAME
SIZE
SPECIFIED
BY
THE
SIZE
ARGUMENT
THE
NAME
ARGUMENT
IS
ASSOCIATED
WITH
THIS
CACHE
AND
FUNCTIONS
AS
HOUSEKEEPING
INFORMATION
USABLE
IN
TRACKING
PROB
LEMS
USUALLY
IT
IS
SET
TO
THE
NAME
OF
THE
TYPE
OF
STRUCTURE
THAT
IS
CACHED
THE
CACHE
KEEPS
A
POINTER
TO
THE
NAME
RATHER
THAN
COPYING
IT
SO
THE
DRIVER
SHOULD
PASS
IN
A
POINTER
TO
A
NAME
IN
STATIC
STORAGE
USUALLY
THE
NAME
IS
JUST
A
LITERAL
STRING
THE
NAME
CANNOT
CONTAIN
BLANKS
THE
OFFSET
IS
THE
OFFSET
OF
THE
FIRST
OBJECT
IN
THE
PAGE
IT
CAN
BE
USED
TO
ENSURE
A
PAR
TICULAR
ALIGNMENT
FOR
THE
ALLOCATED
OBJECTS
BUT
YOU
MOST
LIKELY
WILL
USE
TO
REQUEST
THE
DEFAULT
VALUE
FLAGS
CONTROLS
HOW
ALLOCATION
IS
DONE
AND
IS
A
BIT
MASK
OF
THE
FOL
LOWING
FLAGS
SETTING
THIS
FLAG
PROTECTS
THE
CACHE
FROM
BEING
REDUCED
WHEN
THE
SYSTEM
IS
LOOK
ING
FOR
MEMORY
SETTING
THIS
FLAG
IS
NORMALLY
A
BAD
IDEA
IT
IS
IMPORTANT
TO
AVOID
RESTRICTING
THE
MEMORY
ALLOCATOR
FREEDOM
OF
ACTION
UNNECESSARILY
THIS
FLAG
REQUIRES
EACH
DATA
OBJECT
TO
BE
ALIGNED
TO
A
CACHE
LINE
ACTUAL
ALIGNMENT
DEPENDS
ON
THE
CACHE
LAYOUT
OF
THE
HOST
PLATFORM
THIS
OPTION
CAN
BE
A
GOOD
CHOICE
IF
YOUR
CACHE
CONTAINS
ITEMS
THAT
ARE
FREQUENTLY
ACCESSED
ON
SMP
MACHINES
THE
PADDING
REQUIRED
TO
ACHIEVE
CACHE
LINE
ALIGNMENT
CAN
END
UP
WASTING
SIGNIFICANT
AMOUNTS
OF
MEMORY
HOWEVER
THIS
FLAG
REQUIRES
EACH
DATA
OBJECT
TO
BE
ALLOCATED
IN
THE
DMA
MEMORY
ZONE
THERE
IS
ALSO
A
SET
OF
FLAGS
THAT
CAN
BE
USED
DURING
THE
DEBUGGING
OF
CACHE
ALLOCA
TIONS
SEE
MM
SLAB
C
FOR
THE
DETAILS
USUALLY
HOWEVER
THESE
FLAGS
ARE
SET
GLOBALLY
VIA
A
KERNEL
CONFIGURATION
OPTION
ON
SYSTEMS
USED
FOR
DEVELOPMENT
THE
CONSTRUCTOR
AND
DESTRUCTOR
ARGUMENTS
TO
THE
FUNCTION
ARE
OPTIONAL
FUNCTIONS
BUT
THERE
CAN
BE
NO
DESTRUCTOR
WITHOUT
A
CONSTRUCTOR
THE
FORMER
CAN
BE
USED
TO
INI
TIALIZE
NEWLY
ALLOCATED
OBJECTS
AND
THE
LATTER
CAN
BE
USED
TO
CLEAN
UP
OBJECTS
PRIOR
TO
THEIR
MEMORY
BEING
RELEASED
BACK
TO
THE
SYSTEM
AS
A
WHOLE
CONSTRUCTORS
AND
DESTRUCTORS
CAN
BE
USEFUL
BUT
THERE
ARE
A
FEW
CONSTRAINTS
THAT
YOU
SHOULD
KEEP
IN
MIND
A
CONSTRUCTOR
IS
CALLED
WHEN
THE
MEMORY
FOR
A
SET
OF
OBJECTS
IS
ALLOCATED
BECAUSE
THAT
MEMORY
MAY
HOLD
SEVERAL
OBJECTS
THE
CONSTRUCTOR
MAY
BE
CALLED
MULTIPLE
TIMES
YOU
CANNOT
ASSUME
THAT
THE
CONSTRUCTOR
WILL
BE
CALLED
AS
AN
IMMEDIATE
EFFECT
OF
ALLOCATING
AN
OBJECT
SIMILARLY
DESTRUCTORS
CAN
BE
CALLED
AT
SOME
UNKNOWN
FUTURE
TIME
NOT
IMMEDIATELY
AFTER
AN
OBJECT
HAS
BEEN
FREED
CONSTRUCTORS
AND
DESTRUCTORS
MAY
OR
MAY
NOT
BE
ALLOWED
TO
SLEEP
ACCORDING
TO
WHETHER
THEY
ARE
PASSED
THE
FLAG
WHERE
CTOR
IS
SHORT
FOR
CONSTRUCTOR
FOR
CONVENIENCE
A
PROGRAMMER
CAN
USE
THE
SAME
FUNCTION
FOR
BOTH
THE
CONSTRUCTOR
AND
DESTRUCTOR
THE
SLAB
ALLOCATOR
ALWAYS
PASSES
THE
FLAG
WHEN
THE
CALLEE
IS
A
CONSTRUCTOR
ONCE
A
CACHE
OF
OBJECTS
IS
CREATED
YOU
CAN
ALLOCATE
OBJECTS
FROM
IT
BY
CALLING
VOID
CACHE
INT
FLAGS
HERE
THE
CACHE
ARGUMENT
IS
THE
CACHE
YOU
HAVE
CREATED
PREVIOUSLY
THE
FLAGS
ARE
THE
SAME
AS
YOU
WOULD
PASS
TO
KMALLOC
AND
ARE
CONSULTED
IF
NEEDS
TO
GO
OUT
AND
ALLOCATE
MORE
MEMORY
ITSELF
TO
FREE
AN
OBJECT
USE
VOID
CACHE
CONST
VOID
OBJ
WHEN
DRIVER
CODE
IS
FINISHED
WITH
THE
CACHE
TYPICALLY
WHEN
THE
MODULE
IS
UNLOADED
IT
SHOULD
FREE
ITS
CACHE
AS
FOLLOWS
INT
CACHE
THE
DESTROY
OPERATION
SUCCEEDS
ONLY
IF
ALL
OBJECTS
ALLOCATED
FROM
THE
CACHE
HAVE
BEEN
RETURNED
TO
IT
THEREFORE
A
MODULE
SHOULD
CHECK
THE
RETURN
STATUS
FROM
A
FAILURE
INDICATES
SOME
SORT
OF
MEMORY
LEAK
WITHIN
THE
MOD
ULE
SINCE
SOME
OF
THE
OBJECTS
HAVE
BEEN
DROPPED
ONE
SIDE
BENEFIT
TO
USING
LOOKASIDE
CACHES
IS
THAT
THE
KERNEL
MAINTAINS
STATISTICS
ON
CACHE
USAGE
THESE
STATISTICS
MAY
BE
OBTAINED
FROM
PROC
SLABINFO
A
SCULL
BASED
ON
THE
SLAB
CACHES
SCULLC
TIME
FOR
AN
EXAMPLE
SCULLC
IS
A
CUT
DOWN
VERSION
OF
THE
SCULL
MODULE
THAT
IMPLE
MENTS
ONLY
THE
BARE
DEVICE
THE
PERSISTENT
MEMORY
REGION
UNLIKE
SCULL
WHICH
USES
KMALLOC
SCULLC
USES
MEMORY
CACHES
THE
SIZE
OF
THE
QUANTUM
CAN
BE
MODIFIED
AT
COMPILE
TIME
AND
AT
LOAD
TIME
BUT
NOT
AT
RUNTIME
THAT
WOULD
REQUIRE
CREATING
A
NEW
MEMORY
CACHE
AND
WE
DIDN
T
WANT
TO
DEAL
WITH
THESE
UNNEEDED
DETAILS
SCULLC
IS
A
COMPLETE
EXAMPLE
THAT
CAN
BE
USED
TO
TRY
OUT
THE
SLAB
ALLOCATOR
IT
DIFFERS
FROM
SCULL
ONLY
IN
A
FEW
LINES
OF
CODE
FIRST
WE
MUST
DECLARE
OUR
OWN
SLAB
CACHE
DECLARE
ONE
CACHE
POINTER
USE
IT
FOR
ALL
DEVICES
THE
CREATION
OF
THE
SLAB
CACHE
IS
HANDLED
AT
MODULE
LOAD
TIME
IN
THIS
WAY
CREATE
A
CACHE
FOR
OUR
QUANTA
SCULLC
NULL
NULL
NO
CTOR
DTOR
IF
RETURN
ENOMEM
THIS
IS
HOW
IT
ALLOCATES
MEMORY
QUANTA
ALLOCATE
A
QUANTUM
USING
THE
MEMORY
CACHE
IF
DPTR
DATA
DPTR
DATA
IF
DPTR
DATA
GOTO
NOMEM
MEMSET
DPTR
DATA
AND
THESE
LINES
RELEASE
MEMORY
FOR
I
I
QSET
I
IF
DPTR
DATA
I
DPTR
DATA
I
FINALLY
AT
MODULE
UNLOAD
TIME
WE
HAVE
TO
RETURN
THE
CACHE
TO
THE
SYSTEM
RELEASE
THE
CACHE
OF
OUR
QUANTA
IF
THE
MAIN
DIFFERENCES
IN
PASSING
FROM
SCULL
TO
SCULLC
ARE
A
SLIGHT
SPEED
IMPROVEMENT
AND
BETTER
MEMORY
USE
SINCE
QUANTA
ARE
ALLOCATED
FROM
A
POOL
OF
MEMORY
FRAGMENTS
OF
EXACTLY
THE
RIGHT
SIZE
THEIR
PLACEMENT
IN
MEMORY
IS
AS
DENSE
AS
POSSIBLE
AS
OPPOSED
TO
SCULL
QUANTA
WHICH
BRING
IN
AN
UNPREDICTABLE
MEMORY
FRAGMENTATION
MEMORY
POOLS
THERE
ARE
PLACES
IN
THE
KERNEL
WHERE
MEMORY
ALLOCATIONS
CANNOT
BE
ALLOWED
TO
FAIL
AS
A
WAY
OF
GUARANTEEING
ALLOCATIONS
IN
THOSE
SITUATIONS
THE
KERNEL
DEVELOPERS
CRE
ATED
AN
ABSTRACTION
KNOWN
AS
A
MEMORY
POOL
OR
MEMPOOL
A
MEMORY
POOL
IS
REALLY
JUST
A
FORM
OF
A
LOOKASIDE
CACHE
THAT
TRIES
TO
ALWAYS
KEEP
A
LIST
OF
FREE
MEMORY
AROUND
FOR
USE
IN
EMERGENCIES
A
MEMORY
POOL
HAS
A
TYPE
OF
DEFINED
IN
LINUX
MEMPOOL
H
YOU
CAN
CRE
ATE
ONE
WITH
INT
VOID
THE
ARGUMENT
IS
THE
MINIMUM
NUMBER
OF
ALLOCATED
OBJECTS
THAT
THE
POOL
SHOULD
ALWAYS
KEEP
AROUND
THE
ACTUAL
ALLOCATION
AND
FREEING
OF
OBJECTS
IS
HANDLED
BY
AND
WHICH
HAVE
THESE
PROTOTYPES
TYPEDEF
VOID
INT
VOID
TYPEDEF
VOID
VOID
ELEMENT
VOID
THE
FINAL
PARAMETER
TO
IS
PASSED
TO
AND
IF
NEED
BE
YOU
CAN
WRITE
SPECIAL
PURPOSE
FUNCTIONS
TO
HANDLE
MEMORY
ALLOCATIONS
FOR
MEMPOOLS
USUALLY
HOWEVER
YOU
JUST
WANT
TO
LET
THE
KERNEL
SLAB
ALLOCATOR
HANDLE
THAT
TASK
FOR
YOU
THERE
ARE
TWO
FUNCTIONS
AND
THAT
PERFORM
THE
IMPEDANCE
MATCHING
BETWEEN
THE
MEMORY
POOL
ALLOCATION
PROTO
TYPES
AND
AND
THUS
CODE
THAT
SETS
UP
MEMORY
POOLS
OFTEN
LOOKS
LIKE
THE
FOLLOWING
CACHE
POOL
CACHE
ONCE
THE
POOL
HAS
BEEN
CREATED
OBJECTS
CAN
BE
ALLOCATED
AND
FREED
WITH
VOID
POOL
INT
VOID
VOID
ELEMENT
POOL
WHEN
THE
MEMPOOL
IS
CREATED
THE
ALLOCATION
FUNCTION
WILL
BE
CALLED
ENOUGH
TIMES
TO
CREATE
A
POOL
OF
PREALLOCATED
OBJECTS
THEREAFTER
CALLS
TO
ATTEMPT
TO
ACQUIRE
ADDITIONAL
OBJECTS
FROM
THE
ALLOCATION
FUNCTION
SHOULD
THAT
ALLOCATION
FAIL
ONE
OF
THE
PREALLOCATED
OBJECTS
IF
ANY
REMAIN
IS
RETURNED
WHEN
AN
OBJECT
IS
FREED
WITH
IT
IS
KEPT
IN
THE
POOL
IF
THE
NUMBER
OF
PREALLOCATED
OBJECTS
IS
CUR
RENTLY
BELOW
THE
MINIMUM
OTHERWISE
IT
IS
TO
BE
RETURNED
TO
THE
SYSTEM
A
MEMPOOL
CAN
BE
RESIZED
WITH
INT
POOL
INT
INT
THIS
CALL
IF
SUCCESSFUL
RESIZES
THE
POOL
TO
HAVE
AT
LEAST
OBJECTS
IF
YOU
NO
LONGER
NEED
A
MEMORY
POOL
RETURN
IT
TO
THE
SYSTEM
WITH
VOID
POOL
YOU
MUST
RETURN
ALL
ALLOCATED
OBJECTS
BEFORE
DESTROYING
THE
MEMPOOL
OR
A
KERNEL
OOPS
RESULTS
IF
YOU
ARE
CONSIDERING
USING
A
MEMPOOL
IN
YOUR
DRIVER
PLEASE
KEEP
ONE
THING
IN
MIND
MEMPOOLS
ALLOCATE
A
CHUNK
OF
MEMORY
THAT
SITS
IN
A
LIST
IDLE
AND
UNAVAILABLE
FOR
ANY
REAL
USE
IT
IS
EASY
TO
CONSUME
A
GREAT
DEAL
OF
MEMORY
WITH
MEMPOOLS
IN
ALMOST
EVERY
CASE
THE
PREFERRED
ALTERNATIVE
IS
TO
DO
WITHOUT
THE
MEMPOOL
AND
SIMPLY
DEAL
WITH
THE
POSSIBILITY
OF
ALLOCATION
FAILURES
INSTEAD
IF
THERE
IS
ANY
WAY
FOR
YOUR
DRIVER
TO
RESPOND
TO
AN
ALLOCATION
FAILURE
IN
A
WAY
THAT
DOES
NOT
ENDANGER
THE
INTEG
RITY
OF
THE
SYSTEM
DO
THINGS
THAT
WAY
USE
OF
MEMPOOLS
IN
DRIVER
CODE
SHOULD
BE
RARE
AND
FRIENDS
IF
A
MODULE
NEEDS
TO
ALLOCATE
BIG
CHUNKS
OF
MEMORY
IT
IS
USUALLY
BETTER
TO
USE
A
PAGE
ORIENTED
TECHNIQUE
REQUESTING
WHOLE
PAGES
ALSO
HAS
OTHER
ADVANTAGES
WHICH
ARE
INTRODUCED
IN
CHAPTER
TO
ALLOCATE
PAGES
THE
FOLLOWING
FUNCTIONS
ARE
AVAILABLE
UNSIGNED
INT
FLAGS
RETURNS
A
POINTER
TO
A
NEW
PAGE
AND
FILLS
THE
PAGE
WITH
ZEROS
UNSIGNED
INT
FLAGS
SIMILAR
TO
BUT
DOESN
T
CLEAR
THE
PAGE
UNSIGNED
INT
FLAGS
UNSIGNED
INT
ORDER
ALLOCATES
AND
RETURNS
A
POINTER
TO
THE
FIRST
BYTE
OF
A
MEMORY
AREA
THAT
IS
POTEN
TIALLY
SEVERAL
PHYSICALLY
CONTIGUOUS
PAGES
LONG
BUT
DOESN
T
ZERO
THE
AREA
THE
FLAGS
ARGUMENT
WORKS
IN
THE
SAME
WAY
AS
WITH
KMALLOC
USUALLY
EITHER
OR
IS
USED
PERHAPS
WITH
THE
ADDITION
OF
THE
FLAG
FOR
MEMORY
THAT
CAN
BE
USED
FOR
ISA
DIRECT
MEMORY
ACCESS
OPERATIONS
OR
WHEN
HIGH
MEMORY
CAN
BE
USED
ORDER
IS
THE
BASE
TWO
LOGARITHM
OF
THE
NUMBER
OF
PAGES
YOU
ARE
REQUESTING
OR
FREEING
I
E
FOR
EXAMPLE
ORDER
IS
IF
YOU
WANT
ONE
PAGE
AND
IF
YOU
REQUEST
EIGHT
PAGES
IF
ORDER
IS
TOO
BIG
NO
CONTIGUOUS
AREA
OF
THAT
SIZE
IS
AVAILABLE
THE
PAGE
ALLOCATION
FAILS
THE
FUNCTION
WHICH
TAKES
AN
INTE
GER
ARGUMENT
CAN
BE
USED
TO
EXTRACT
THE
ORDER
FROM
A
SIZE
THAT
MUST
BE
A
POWER
OF
TWO
FOR
THE
HOSTING
PLATFORM
THE
MAXIMUM
ALLOWED
VALUE
FOR
ORDER
IS
OR
COR
RESPONDING
TO
OR
PAGES
DEPENDING
ON
THE
ARCHITECTURE
THE
CHANCES
OF
AN
ORDER
ALLOCATION
SUCCEEDING
ON
ANYTHING
OTHER
THAN
A
FRESHLY
BOOTED
SYSTEM
WITH
A
LOT
OF
MEMORY
ARE
SMALL
HOWEVER
IF
YOU
ARE
CURIOUS
PROC
BUDDYINFO
TELLS
YOU
HOW
MANY
BLOCKS
OF
EACH
ORDER
ARE
AVAIL
ABLE
FOR
EACH
MEMORY
ZONE
ON
THE
SYSTEM
WHEN
A
PROGRAM
IS
DONE
WITH
THE
PAGES
IT
CAN
FREE
THEM
WITH
ONE
OF
THE
FOLLOWING
FUNCTIONS
THE
FIRST
FUNCTION
IS
A
MACRO
THAT
FALLS
BACK
ON
THE
SECOND
VOID
UNSIGNED
LONG
ADDR
VOID
UNSIGNED
LONG
ADDR
UNSIGNED
LONG
ORDER
IF
YOU
TRY
TO
FREE
A
DIFFERENT
NUMBER
OF
PAGES
FROM
WHAT
YOU
ALLOCATED
THE
MEMORY
MAP
BECOMES
CORRUPTED
AND
THE
SYSTEM
GETS
IN
TROUBLE
AT
A
LATER
TIME
IT
WORTH
STRESSING
THAT
AND
THE
OTHER
FUNCTIONS
CAN
BE
CALLED
AT
ANY
TIME
SUBJECT
TO
THE
SAME
RULES
WE
SAW
FOR
KMALLOC
THE
FUNCTIONS
CAN
FAIL
TO
ALLOCATE
MEMORY
IN
CERTAIN
CIRCUMSTANCES
PARTICULARLY
WHEN
IS
USED
THEREFORE
THE
PROGRAM
CALLING
THESE
ALLOCATION
FUNCTIONS
MUST
BE
PREPARED
TO
HANDLE
AN
ALLOCA
TION
FAILURE
ALTHOUGH
KMALLOC
SOMETIMES
FAILS
WHEN
THERE
IS
NO
AVAILABLE
MEMORY
THE
KERNEL
DOES
ITS
BEST
TO
FULFILL
ALLOCATION
REQUESTS
THEREFORE
IT
EASY
TO
DEGRADE
SYSTEM
RESPONSIVENESS
BY
ALLOCATING
TOO
MUCH
MEMORY
FOR
EXAMPLE
YOU
CAN
BRING
THE
COMPUTER
DOWN
BY
PUSHING
TOO
MUCH
DATA
INTO
A
SCULL
DEVICE
THE
SYSTEM
STARTS
CRAWLING
WHILE
IT
TRIES
TO
SWAP
OUT
AS
MUCH
AS
POSSIBLE
IN
ORDER
TO
FULFILL
THE
KMALLOC
REQUEST
SINCE
EVERY
RESOURCE
IS
BEING
SUCKED
UP
BY
THE
GROWING
DEVICE
THE
COM
PUTER
IS
SOON
RENDERED
UNUSABLE
AT
THAT
POINT
YOU
CAN
NO
LONGER
EVEN
START
A
NEW
PROCESS
TO
TRY
TO
DEAL
WITH
THE
PROBLEM
WE
DON
T
ADDRESS
THIS
ISSUE
IN
SCULL
SINCE
IT
IS
JUST
A
SAMPLE
MODULE
AND
NOT
A
REAL
TOOL
TO
PUT
INTO
A
MULTIUSER
SYSTEM
AS
A
PRO
GRAMMER
YOU
MUST
BE
CAREFUL
NONETHELESS
BECAUSE
A
MODULE
IS
PRIVILEGED
CODE
AND
CAN
OPEN
NEW
SECURITY
HOLES
IN
THE
SYSTEM
THE
MOST
LIKELY
IS
A
DENIAL
OF
SERVICE
HOLE
LIKE
THE
ONE
JUST
OUTLINED
ALTHOUGH
DESCRIBED
SHORTLY
SHOULD
REALLY
BE
USED
FOR
ALLOCATING
HIGH
MEMORY
PAGES
FOR
REASONS
WE
CAN
T
REALLY
GET
INTO
UNTIL
CHAPTER
A
SCULL
USING
WHOLE
PAGES
SCULLP
IN
ORDER
TO
TEST
PAGE
ALLOCATION
FOR
REAL
WE
HAVE
RELEASED
THE
SCULLP
MODULE
TOGETHER
WITH
OTHER
SAMPLE
CODE
IT
IS
A
REDUCED
SCULL
JUST
LIKE
SCULLC
INTRODUCED
EARLIER
MEMORY
QUANTA
ALLOCATED
BY
SCULLP
ARE
WHOLE
PAGES
OR
PAGE
SETS
THE
VARIABLE
DEFAULTS
TO
BUT
CAN
BE
CHANGED
AT
EITHER
COMPILE
OR
LOAD
TIME
THE
FOLLOWING
LINES
SHOW
HOW
IT
ALLOCATES
MEMORY
HERE
THE
ALLOCATION
OF
A
SINGLE
QUANTUM
IF
DPTR
DATA
DPTR
DATA
VOID
DPTR
ORDER
IF
DPTR
DATA
GOTO
NOMEM
MEMSET
DPTR
DATA
DPTR
ORDER
THE
CODE
TO
DEALLOCATE
MEMORY
IN
SCULLP
LOOKS
LIKE
THIS
THIS
CODE
FREES
A
WHOLE
QUANTUM
SET
FOR
I
I
QSET
I
IF
DPTR
DATA
I
UNSIGNED
LONG
DPTR
DATA
I
DPTR
ORDER
AT
THE
USER
LEVEL
THE
PERCEIVED
DIFFERENCE
IS
PRIMARILY
A
SPEED
IMPROVEMENT
AND
BET
TER
MEMORY
USE
BECAUSE
THERE
IS
NO
INTERNAL
FRAGMENTATION
OF
MEMORY
WE
RAN
SOME
TESTS
COPYING
MB
FROM
TO
AND
THEN
FROM
TO
THE
RESULTS
SHOWED
A
SLIGHT
IMPROVEMENT
IN
KERNEL
SPACE
PROCESSOR
USAGE
THE
PERFORMANCE
IMPROVEMENT
IS
NOT
DRAMATIC
BECAUSE
KMALLOC
IS
DESIGNED
TO
BE
FAST
THE
MAIN
ADVANTAGE
OF
PAGE
LEVEL
ALLOCATION
ISN
T
ACTUALLY
SPEED
BUT
RATHER
MORE
EFFICIENT
MEMORY
USAGE
ALLOCATING
BY
PAGES
WASTES
NO
MEMORY
WHEREAS
USING
KMALLOC
WASTES
AN
UNPREDICTABLE
AMOUNT
OF
MEMORY
BECAUSE
OF
ALLOCATION
GRANULARITY
BUT
THE
BIGGEST
ADVANTAGE
OF
THE
FUNCTIONS
IS
THAT
THE
PAGES
OBTAINED
ARE
COMPLETELY
YOURS
AND
YOU
COULD
IN
THEORY
ASSEMBLE
THE
PAGES
INTO
A
LINEAR
AREA
BY
APPROPRIATE
TWEAKING
OF
THE
PAGE
TABLES
FOR
EXAMPLE
YOU
CAN
ALLOW
A
USER
PRO
CESS
TO
MMAP
MEMORY
AREAS
OBTAINED
AS
SINGLE
UNRELATED
PAGES
WE
DISCUSS
THIS
KIND
OF
OPERATION
IN
CHAPTER
WHERE
WE
SHOW
HOW
SCULLP
OFFERS
MEMORY
MAPPING
SOMETHING
THAT
SCULL
CANNOT
OFFER
THE
INTERFACE
FOR
COMPLETENESS
WE
INTRODUCE
ANOTHER
INTERFACE
FOR
MEMORY
ALLOCATION
EVEN
THOUGH
WE
WILL
NOT
BE
PREPARED
TO
USE
IT
UNTIL
AFTER
CHAPTER
FOR
NOW
SUFFICE
IT
TO
SAY
THAT
STRUCT
PAGE
IS
AN
INTERNAL
KERNEL
STRUCTURE
THAT
DESCRIBES
A
PAGE
OF
MEMORY
AS
WE
WILL
SEE
THERE
ARE
MANY
PLACES
IN
THE
KERNEL
WHERE
IT
IS
NECESSARY
TO
WORK
WITH
PAGE
STRUCTURES
THEY
ARE
ESPECIALLY
USEFUL
IN
ANY
SITUATION
WHERE
YOU
MIGHT
BE
DEAL
ING
WITH
HIGH
MEMORY
WHICH
DOES
NOT
HAVE
A
CONSTANT
ADDRESS
IN
KERNEL
SPACE
THE
REAL
CORE
OF
THE
LINUX
PAGE
ALLOCATOR
IS
A
FUNCTION
CALLED
STRUCT
PAGE
INT
NID
UNSIGNED
INT
FLAGS
UNSIGNED
INT
ORDER
THIS
FUNCTION
ALSO
HAS
TWO
VARIANTS
WHICH
ARE
SIMPLY
MACROS
THESE
ARE
THE
VERSIONS
THAT
YOU
WILL
MOST
LIKELY
USE
STRUCT
PAGE
UNSIGNED
INT
FLAGS
UNSIGNED
INT
ORDER
STRUCT
PAGE
UNSIGNED
INT
FLAGS
THE
CORE
FUNCTION
TAKES
THREE
ARGUMENTS
NID
IS
THE
NUMA
NODE
ID
WHOSE
MEMORY
SHOULD
BE
ALLOCATED
FLAGS
IS
THE
USUAL
ALLOCATION
FLAGS
AND
ORDER
IS
THE
SIZE
OF
THE
ALLOCATION
THE
RETURN
VALUE
IS
A
POINTER
TO
THE
FIRST
OF
POSSI
BLY
MANY
PAGE
STRUCTURES
DESCRIBING
THE
ALLOCATED
MEMORY
OR
AS
USUAL
NULL
ON
FAILURE
SIMPLIFIES
THE
SITUATION
BY
ALLOCATING
THE
MEMORY
ON
THE
CURRENT
NUMA
NODE
IT
CALLS
WITH
THE
RETURN
VALUE
FROM
AS
THE
NID
PARAMETER
AND
OF
COURSE
OMITS
THE
ORDER
PARAMETER
AND
ALLOCATES
A
SIN
GLE
PAGE
TO
RELEASE
PAGES
ALLOCATED
IN
THIS
MANNER
YOU
SHOULD
USE
ONE
OF
THE
FOLLOWING
VOID
STRUCT
PAGE
PAGE
VOID
STRUCT
PAGE
PAGE
UNSIGNED
INT
ORDER
VOID
STRUCT
PAGE
PAGE
VOID
STRUCT
PAGE
PAGE
IF
YOU
HAVE
SPECIFIC
KNOWLEDGE
OF
WHETHER
A
SINGLE
PAGE
CONTENTS
ARE
LIKELY
TO
BE
RESIDENT
IN
THE
PROCESSOR
CACHE
YOU
SHOULD
COMMUNICATE
THAT
TO
THE
KERNEL
WITH
FOR
CACHE
RESIDENT
PAGES
OR
THIS
INFORMATION
HELPS
THE
MEMORY
ALLOCATOR
OPTIMIZE
ITS
USE
OF
MEMORY
ACROSS
THE
SYSTEM
VMALLOC
AND
FRIENDS
THE
NEXT
MEMORY
ALLOCATION
FUNCTION
THAT
WE
SHOW
YOU
IS
VMALLOC
WHICH
ALLOCATES
A
CONTIGUOUS
MEMORY
REGION
IN
THE
VIRTUAL
ADDRESS
SPACE
ALTHOUGH
THE
PAGES
ARE
NOT
CON
SECUTIVE
IN
PHYSICAL
MEMORY
EACH
PAGE
IS
RETRIEVED
WITH
A
SEPARATE
CALL
TO
THE
KERNEL
SEES
THEM
AS
A
CONTIGUOUS
RANGE
OF
ADDRESSES
VMALLOC
RETURNS
THE
NULL
ADDRESS
IF
AN
ERROR
OCCURS
OTHERWISE
IT
RETURNS
A
POINTER
TO
A
LINEAR
MEMORY
AREA
OF
SIZE
AT
LEAST
SIZE
NUMA
NONUNIFORM
MEMORY
ACCESS
COMPUTERS
ARE
MULTIPROCESSOR
SYSTEMS
WHERE
MEMORY
IS
LOCAL
TO
SPECIFIC
GROUPS
OF
PROCESSORS
NODES
ACCESS
TO
LOCAL
MEMORY
IS
FASTER
THAN
ACCESS
TO
NONLOCAL
MEMORY
ON
SUCH
SYSTEMS
ALLOCATING
MEMORY
ON
THE
CORRECT
NODE
IS
IMPORTANT
DRIVER
AUTHORS
DO
NOT
NORMALLY
HAVE
TO
WORRY
ABOUT
NUMA
ISSUES
HOWEVER
WE
DESCRIBE
VMALLOC
HERE
BECAUSE
IT
IS
ONE
OF
THE
FUNDAMENTAL
LINUX
MEMORY
ALLOCA
TION
MECHANISMS
WE
SHOULD
NOTE
HOWEVER
THAT
USE
OF
VMALLOC
IS
DISCOURAGED
IN
MOST
SITUATIONS
MEMORY
OBTAINED
FROM
VMALLOC
IS
SLIGHTLY
LESS
EFFICIENT
TO
WORK
WITH
AND
ON
SOME
ARCHITECTURES
THE
AMOUNT
OF
ADDRESS
SPACE
SET
ASIDE
FOR
VMALLOC
IS
RELA
TIVELY
SMALL
CODE
THAT
USES
VMALLOC
IS
LIKELY
TO
GET
A
CHILLY
RECEPTION
IF
SUBMITTED
FOR
INCLUSION
IN
THE
KERNEL
IF
POSSIBLE
YOU
SHOULD
WORK
DIRECTLY
WITH
INDIVIDUAL
PAGES
RATHER
THAN
TRYING
TO
SMOOTH
THINGS
OVER
WITH
VMALLOC
THAT
SAID
LET
SEE
HOW
VMALLOC
WORKS
THE
PROTOTYPES
OF
THE
FUNCTION
AND
ITS
RELA
TIVES
IOREMAP
WHICH
IS
NOT
STRICTLY
AN
ALLOCATION
FUNCTION
IS
DISCUSSED
LATER
IN
THIS
SECTION
ARE
AS
FOLLOWS
INCLUDE
LINUX
VMALLOC
H
VOID
VMALLOC
UNSIGNED
LONG
SIZE
VOID
VFREE
VOID
ADDR
VOID
IOREMAP
UNSIGNED
LONG
OFFSET
UNSIGNED
LONG
SIZE
VOID
IOUNMAP
VOID
ADDR
IT
WORTH
STRESSING
THAT
MEMORY
ADDRESSES
RETURNED
BY
KMALLOC
AND
ARE
ALSO
VIRTUAL
ADDRESSES
THEIR
ACTUAL
VALUE
IS
STILL
MASSAGED
BY
THE
MMU
THE
MEM
ORY
MANAGEMENT
UNIT
USUALLY
PART
OF
THE
CPU
BEFORE
IT
IS
USED
TO
ADDRESS
PHYSICAL
MEMORY
VMALLOC
IS
NOT
DIFFERENT
IN
HOW
IT
USES
THE
HARDWARE
BUT
RATHER
IN
HOW
THE
KERNEL
PERFORMS
THE
ALLOCATION
TASK
THE
VIRTUAL
ADDRESS
RANGE
USED
BY
KMALLOC
AND
FEATURES
A
ONE
TO
ONE
MAPPING
TO
PHYSICAL
MEMORY
POSSIBLY
SHIFTED
BY
A
CONSTANT
VALUE
THE
FUNCTIONS
DON
T
NEED
TO
MODIFY
THE
PAGE
TABLES
FOR
THAT
ADDRESS
RANGE
THE
ADDRESS
RANGE
USED
BY
VMALLOC
AND
IOREMAP
ON
THE
OTHER
HAND
IS
COMPLETELY
SYN
THETIC
AND
EACH
ALLOCATION
BUILDS
THE
VIRTUAL
MEMORY
AREA
BY
SUITABLY
SETTING
UP
THE
PAGE
TABLES
THIS
DIFFERENCE
CAN
BE
PERCEIVED
BY
COMPARING
THE
POINTERS
RETURNED
BY
THE
ALLOCA
TION
FUNCTIONS
ON
SOME
PLATFORMS
FOR
EXAMPLE
THE
ADDRESSES
RETURNED
BY
VMALLOC
ARE
JUST
BEYOND
THE
ADDRESSES
THAT
KMALLOC
USES
ON
OTHER
PLATFORMS
FOR
EXAMPLE
MIPS
IA
AND
THEY
BELONG
TO
A
COMPLETELY
DIFFERENT
ADDRESS
RANGE
ADDRESSES
AVAILABLE
FOR
VMALLOC
ARE
IN
THE
RANGE
FROM
TO
BOTH
SYMBOLS
ARE
DEFINED
IN
ASM
PGTABLE
H
ADDRESSES
ALLOCATED
BY
VMALLOC
CAN
T
BE
USED
OUTSIDE
OF
THE
MICROPROCESSOR
BECAUSE
THEY
MAKE
SENSE
ONLY
ON
TOP
OF
THE
PROCESSOR
MMU
WHEN
A
DRIVER
NEEDS
A
REAL
PHYSICAL
ADDRESS
SUCH
AS
A
DMA
ADDRESS
USED
BY
PERIPHERAL
HARDWARE
TO
DRIVE
THE
SYSTEM
BUS
YOU
CAN
T
EASILY
USE
VMALLOC
THE
RIGHT
TIME
TO
CALL
VMALLOC
IS
WHEN
ACTUALLY
SOME
ARCHITECTURES
DEFINE
RANGES
OF
VIRTUAL
ADDRESSES
AS
RESERVED
TO
ADDRESS
PHYSICAL
MEMORY
WHEN
THIS
HAPPENS
THE
LINUX
KERNEL
TAKES
ADVANTAGE
OF
THE
FEATURE
AND
BOTH
THE
KERNEL
AND
ADDRESSES
LIE
IN
ONE
OF
THOSE
MEMORY
RANGES
THE
DIFFERENCE
IS
TRANSPARENT
TO
DEVICE
DRIVERS
AND
OTHER
CODE
THAT
IS
NOT
DIRECTLY
INVOLVED
WITH
THE
MEMORY
MANAGEMENT
KERNEL
SUBSYSTEM
YOU
ARE
ALLOCATING
MEMORY
FOR
A
LARGE
SEQUENTIAL
BUFFER
THAT
EXISTS
ONLY
IN
SOFTWARE
IT
IMPORTANT
TO
NOTE
THAT
VMALLOC
HAS
MORE
OVERHEAD
THAN
BECAUSE
IT
MUST
BOTH
RETRIEVE
THE
MEMORY
AND
BUILD
THE
PAGE
TABLES
THEREFORE
IT
DOESN
T
MAKE
SENSE
TO
CALL
VMALLOC
TO
ALLOCATE
JUST
ONE
PAGE
AN
EXAMPLE
OF
A
FUNCTION
IN
THE
KERNEL
THAT
USES
VMALLOC
IS
THE
SYSTEM
CALL
WHICH
USES
VMALLOC
TO
GET
SPACE
FOR
THE
MODULE
BEING
CREATED
CODE
AND
DATA
OF
THE
MODULE
ARE
LATER
COPIED
TO
THE
ALLOCATED
SPACE
USING
IN
THIS
WAY
THE
MODULE
APPEARS
TO
BE
LOADED
INTO
CONTIGUOUS
MEMORY
YOU
CAN
VERIFY
BY
LOOK
ING
IN
PROC
KALLSYMS
THAT
KERNEL
SYMBOLS
EXPORTED
BY
MODULES
LIE
IN
A
DIFFERENT
MEMORY
RANGE
FROM
SYMBOLS
EXPORTED
BY
THE
KERNEL
PROPER
MEMORY
ALLOCATED
WITH
VMALLOC
IS
RELEASED
BY
VFREE
IN
THE
SAME
WAY
THAT
KFREE
RELEASES
MEMORY
ALLOCATED
BY
KMALLOC
LIKE
VMALLOC
IOREMAP
BUILDS
NEW
PAGE
TABLES
UNLIKE
VMALLOC
HOWEVER
IT
DOESN
T
ACTUALLY
ALLOCATE
ANY
MEMORY
THE
RETURN
VALUE
OF
IOREMAP
IS
A
SPECIAL
VIRTUAL
ADDRESS
THAT
CAN
BE
USED
TO
ACCESS
THE
SPECIFIED
PHYSICAL
ADDRESS
RANGE
THE
VIRTUAL
ADDRESS
OBTAINED
IS
EVENTUALLY
RELEASED
BY
CALLING
IOUNMAP
IOREMAP
IS
MOST
USEFUL
FOR
MAPPING
THE
PHYSICAL
ADDRESS
OF
A
PCI
BUFFER
TO
VIRTUAL
KERNEL
SPACE
FOR
EXAMPLE
IT
CAN
BE
USED
TO
ACCESS
THE
FRAME
BUFFER
OF
A
PCI
VIDEO
DEVICE
SUCH
BUFFERS
ARE
USUALLY
MAPPED
AT
HIGH
PHYSICAL
ADDRESSES
OUTSIDE
OF
THE
ADDRESS
RANGE
FOR
WHICH
THE
KERNEL
BUILDS
PAGE
TABLES
AT
BOOT
TIME
PCI
ISSUES
ARE
EXPLAINED
IN
MORE
DETAIL
IN
CHAPTER
IT
WORTH
NOTING
THAT
FOR
THE
SAKE
OF
PORTABILITY
YOU
SHOULD
NOT
DIRECTLY
ACCESS
ADDRESSES
RETURNED
BY
IOREMAP
AS
IF
THEY
WERE
POINTERS
TO
MEMORY
RATHER
YOU
SHOULD
ALWAYS
USE
READB
AND
THE
OTHER
I
O
FUNCTIONS
INTRODUCED
IN
CHAPTER
THIS
REQUIREMENT
APPLIES
BECAUSE
SOME
PLATFORMS
SUCH
AS
THE
ALPHA
ARE
UNABLE
TO
DIRECTLY
MAP
PCI
MEMORY
REGIONS
TO
THE
PROCESSOR
ADDRESS
SPACE
BECAUSE
OF
DIFFER
ENCES
BETWEEN
PCI
SPECS
AND
ALPHA
PROCESSORS
IN
HOW
DATA
IS
TRANSFERRED
BOTH
IOREMAP
AND
VMALLOC
ARE
PAGE
ORIENTED
THEY
WORK
BY
MODIFYING
THE
PAGE
TABLES
CONSEQUENTLY
THE
RELOCATED
OR
ALLOCATED
SIZE
IS
ROUNDED
UP
TO
THE
NEAREST
PAGE
BOUNDARY
IOREMAP
SIMULATES
AN
UNALIGNED
MAPPING
BY
ROUNDING
DOWN
THE
ADDRESS
TO
BE
REMAPPED
AND
BY
RETURNING
AN
OFFSET
INTO
THE
FIRST
REMAPPED
PAGE
ONE
MINOR
DRAWBACK
OF
VMALLOC
IS
THAT
IT
CAN
T
BE
USED
IN
ATOMIC
CONTEXT
BECAUSE
INTERNALLY
IT
USES
KMALLOC
TO
ACQUIRE
STORAGE
FOR
THE
PAGE
TABLES
AND
THEREFORE
COULD
SLEEP
THIS
SHOULDN
T
BE
A
PROBLEM
IF
THE
USE
OF
ISN
T
GOOD
ENOUGH
FOR
AN
INTERRUPT
HANDLER
THE
SOFTWARE
DESIGN
NEEDS
SOME
CLEANING
UP
A
SCULL
USING
VIRTUAL
ADDRESSES
SCULLV
SAMPLE
CODE
USING
VMALLOC
IS
PROVIDED
IN
THE
SCULLV
MODULE
LIKE
SCULLP
THIS
MODULE
IS
A
STRIPPED
DOWN
VERSION
OF
SCULL
THAT
USES
A
DIFFERENT
ALLOCATION
FUNCTION
TO
OBTAIN
SPACE
FOR
THE
DEVICE
TO
STORE
DATA
THE
MODULE
ALLOCATES
MEMORY
PAGES
AT
A
TIME
THE
ALLOCATION
IS
DONE
IN
LARGE
CHUNKS
TO
ACHIEVE
BETTER
PERFORMANCE
THAN
SCULLP
AND
TO
SHOW
SOMETHING
THAT
TAKES
TOO
LONG
WITH
OTHER
ALLOCATION
TECHNIQUES
TO
BE
FEASIBLE
ALLOCATING
MORE
THAN
ONE
PAGE
WITH
IS
FAILURE
PRONE
AND
EVEN
WHEN
IT
SUCCEEDS
IT
CAN
BE
SLOW
AS
WE
SAW
EARLIER
VMALLOC
IS
FASTER
THAN
OTHER
FUNCTIONS
IN
ALLOCATING
SEVERAL
PAGES
BUT
SOMEWHAT
SLOWER
WHEN
RETRIEVING
A
SINGLE
PAGE
BECAUSE
OF
THE
OVERHEAD
OF
PAGE
TABLE
BUILDING
SCULLV
IS
DESIGNED
LIKE
SCULLP
ORDER
SPECIFIES
THE
ORDER
OF
EACH
ALLOCATION
AND
DEFAULTS
TO
THE
ONLY
DIFFERENCE
BETWEEN
SCULLV
AND
SCULLP
IS
IN
ALLOCATION
MANAGEMENT
THESE
LINES
USE
VMALLOC
TO
OBTAIN
NEW
MEMORY
ALLOCATE
A
QUANTUM
USING
VIRTUAL
ADDRESSES
IF
DPTR
DATA
DPTR
DATA
VOID
VMALLOC
DPTR
ORDER
IF
DPTR
DATA
GOTO
NOMEM
MEMSET
DPTR
DATA
DPTR
ORDER
AND
THESE
LINES
RELEASE
MEMORY
RELEASE
THE
QUANTUM
SET
FOR
I
I
QSET
I
IF
DPTR
DATA
I
VFREE
DPTR
DATA
I
IF
YOU
COMPILE
BOTH
MODULES
WITH
DEBUGGING
ENABLED
YOU
CAN
LOOK
AT
THEIR
DATA
ALLOCATION
BY
READING
THE
FILES
THEY
CREATE
IN
PROC
THIS
SNAPSHOT
WAS
TAKEN
ON
AN
SYSTEM
SALMA
CAT
TMP
BIGFILE
DEV
HEAD
PROC
SCULLPMEM
DEVICE
QSET
ORDER
SZ
ITEM
AT
QSET
AT
SALMA
CAT
TMP
BIGFILE
DEV
HEAD
PROC
SCULLVMEM
DEVICE
QSET
ORDER
SZ
ITEM
AT
QSET
AT
THE
FOLLOWING
OUTPUT
INSTEAD
CAME
FROM
AN
SYSTEM
RUDO
CAT
TMP
BIGFILE
DEV
HEAD
PROC
SCULLPMEM
DEVICE
QSET
ORDER
SZ
ITEM
AT
QSET
AT
RUDO
CAT
TMP
BIGFILE
DEV
HEAD
PROC
SCULLVMEM
DEVICE
QSET
ORDER
SZ
ITEM
AT
QSET
AT
THE
VALUES
SHOW
TWO
DIFFERENT
BEHAVIORS
ON
PHYSICAL
ADDRESSES
AND
VIRTUAL
ADDRESSES
ARE
MAPPED
TO
COMPLETELY
DIFFERENT
ADDRESS
RANGES
AND
WHILE
ON
COMPUTERS
VMALLOC
RETURNS
VIRTUAL
ADDRESSES
JUST
ABOVE
THE
MAPPING
USED
FOR
PHYSICAL
MEMORY
PER
CPU
VARIABLES
PER
CPU
VARIABLES
ARE
AN
INTERESTING
KERNEL
FEATURE
WHEN
YOU
CREATE
A
PER
CPU
VARIABLE
EACH
PROCESSOR
ON
THE
SYSTEM
GETS
ITS
OWN
COPY
OF
THAT
VARIABLE
THIS
MAY
SEEM
LIKE
A
STRANGE
THING
TO
WANT
TO
DO
BUT
IT
HAS
ITS
ADVANTAGES
ACCESS
TO
PER
CPU
VARIABLES
REQUIRES
ALMOST
NO
LOCKING
BECAUSE
EACH
PROCESSOR
WORKS
WITH
ITS
OWN
COPY
PER
CPU
VARIABLES
CAN
ALSO
REMAIN
IN
THEIR
RESPECTIVE
PROCESSORS
CACHES
WHICH
LEADS
TO
SIGNIFICANTLY
BETTER
PERFORMANCE
FOR
FREQUENTLY
UPDATED
QUANTITIES
A
GOOD
EXAMPLE
OF
PER
CPU
VARIABLE
USE
CAN
BE
FOUND
IN
THE
NETWORKING
SUBSYSTEM
THE
KERNEL
MAINTAINS
NO
END
OF
COUNTERS
TRACKING
HOW
MANY
OF
EACH
TYPE
OF
PACKET
WAS
RECEIVED
THESE
COUNTERS
CAN
BE
UPDATED
THOUSANDS
OF
TIMES
PER
SECOND
RATHER
THAN
DEAL
WITH
THE
CACHING
AND
LOCKING
ISSUES
THE
NETWORKING
DEVELOPERS
PUT
THE
STA
TISTICS
COUNTERS
INTO
PER
CPU
VARIABLES
UPDATES
ARE
NOW
LOCKLESS
AND
FAST
ON
THE
RARE
OCCASION
THAT
USER
SPACE
REQUESTS
TO
SEE
THE
VALUES
OF
THE
COUNTERS
IT
IS
A
SIMPLE
MATTER
TO
ADD
UP
EACH
PROCESSOR
VERSION
AND
RETURN
THE
TOTAL
THE
DECLARATIONS
FOR
PER
CPU
VARIABLES
CAN
BE
FOUND
IN
LINUX
PERCPU
H
TO
CREATE
A
PER
CPU
VARIABLE
AT
COMPILE
TIME
USE
THIS
MACRO
TYPE
NAME
IF
THE
VARIABLE
TO
BE
CALLED
NAME
IS
AN
ARRAY
INCLUDE
THE
DIMENSION
INFORMATION
WITH
THE
TYPE
THUS
A
PER
CPU
ARRAY
OF
THREE
INTEGERS
WOULD
BE
CREATED
WITH
INT
PER
CPU
VARIABLES
CAN
BE
MANIPULATED
WITHOUT
EXPLICIT
LOCKING
ALMOST
REMEMBER
THAT
THE
KERNEL
IS
PREEMPTIBLE
IT
WOULD
NOT
DO
FOR
A
PROCESSOR
TO
BE
PREEMPTED
IN
THE
MIDDLE
OF
A
CRITICAL
SECTION
THAT
MODIFIES
A
PER
CPU
VARIABLE
IT
ALSO
WOULD
NOT
BE
GOOD
IF
YOUR
PROCESS
WERE
TO
BE
MOVED
TO
ANOTHER
PROCESSOR
IN
THE
MIDDLE
OF
A
PER
CPU
VARIABLE
ACCESS
FOR
THIS
REASON
YOU
MUST
EXPLICITLY
USE
THE
MACRO
TO
ACCESS
THE
CURRENT
PROCESSOR
COPY
OF
A
GIVEN
VARIABLE
AND
CALL
WHEN
YOU
ARE
DONE
THE
CALL
TO
RETURNS
AN
LVALUE
FOR
THE
CURRENT
PROCESSOR
VERSION
OF
THE
VARIABLE
AND
DISABLES
PREEMPTION
SINCE
AN
LVALUE
IS
RETURNED
IT
CAN
BE
ASSIGNED
TO
OR
OPERATED
ON
DIRECTLY
FOR
EXAMPLE
ONE
COUNTER
IN
THE
NETWORKING
CODE
IS
INCREMENTED
WITH
THESE
TWO
STATEMENTS
YOU
CAN
ACCESS
ANOTHER
PROCESSOR
COPY
OF
THE
VARIABLE
WITH
VARIABLE
INT
IF
YOU
WRITE
CODE
THAT
INVOLVES
PROCESSORS
REACHING
INTO
EACH
OTHER
PER
CPU
VARI
ABLES
YOU
OF
COURSE
HAVE
TO
IMPLEMENT
A
LOCKING
SCHEME
THAT
MAKES
THAT
ACCESS
SAFE
DYNAMICALLY
ALLOCATED
PER
CPU
VARIABLES
ARE
ALSO
POSSIBLE
THESE
VARIABLES
CAN
BE
ALLOCATED
WITH
VOID
TYPE
VOID
SIZE
ALIGN
IN
MOST
CASES
DOES
THE
JOB
YOU
CAN
CALL
IN
CASES
WHERE
A
PARTICULAR
ALIGNMENT
IS
REQUIRED
IN
EITHER
CASE
A
PER
CPU
VARIABLE
CAN
BE
RETURNED
TO
THE
SYSTEM
WITH
ACCESS
TO
A
DYNAMICALLY
ALLOCATED
PER
CPU
VARIABLE
IS
DONE
VIA
VOID
INT
THIS
MACRO
RETURNS
A
POINTER
TO
THE
VERSION
OF
CORRESPONDING
TO
THE
GIVEN
IF
YOU
ARE
SIMPLY
READING
ANOTHER
CPU
VERSION
OF
THE
VARIABLE
YOU
CAN
DEREF
ERENCE
THAT
POINTER
AND
BE
DONE
WITH
IT
IF
HOWEVER
YOU
ARE
MANIPULATING
THE
CURRENT
PROCESSOR
VERSION
YOU
PROBABLY
NEED
TO
ENSURE
THAT
YOU
CANNOT
BE
MOVED
OUT
OF
THAT
PROCESSOR
FIRST
IF
THE
ENTIRETY
OF
YOUR
ACCESS
TO
THE
PER
CPU
VARIABLE
HAPPENS
WITH
A
SPINLOCK
HELD
ALL
IS
WELL
USUALLY
HOWEVER
YOU
NEED
TO
USE
TO
BLOCK
PREEMPTION
WHILE
WORKING
WITH
THE
VARIABLE
THUS
CODE
USING
DYNAMIC
PER
CPU
VARI
ABLES
TENDS
TO
LOOK
LIKE
THIS
INT
CPU
CPU
PTR
CPU
WORK
WITH
PTR
WHEN
USING
COMPILE
TIME
PER
CPU
VARIABLES
THE
AND
MACROS
TAKE
CARE
OF
THESE
DETAILS
DYNAMIC
PER
CPU
VARIABLES
REQUIRE
MORE
EXPLICIT
PROTECTION
PER
CPU
VARIABLES
CAN
BE
EXPORTED
TO
MODULES
BUT
YOU
MUST
USE
A
SPECIAL
VERSION
OF
THE
MACROS
TO
ACCESS
SUCH
A
VARIABLE
WITHIN
A
MODULE
DECLARE
IT
WITH
TYPE
NAME
THE
USE
OF
INSTEAD
OF
TELLS
THE
COMPILER
THAT
AN
EXTERNAL
REFERENCE
IS
BEING
MADE
IF
YOU
WANT
TO
USE
PER
CPU
VARIABLES
TO
CREATE
A
SIMPLE
INTEGER
COUNTER
TAKE
A
LOOK
AT
THE
CANNED
IMPLEMENTATION
IN
LINUX
H
FINALLY
NOTE
THAT
SOME
ARCHITECTURES
HAVE
A
LIMITED
AMOUNT
OF
ADDRESS
SPACE
AVAILABLE
FOR
PER
CPU
VARI
ABLES
IF
YOU
CREATE
PER
CPU
VARIABLES
IN
YOUR
CODE
YOU
SHOULD
TRY
TO
KEEP
THEM
SMALL
OBTAINING
LARGE
BUFFERS
AS
WE
HAVE
NOTED
IN
PREVIOUS
SECTIONS
ALLOCATIONS
OF
LARGE
CONTIGUOUS
MEMORY
BUFF
ERS
ARE
PRONE
TO
FAILURE
SYSTEM
MEMORY
FRAGMENTS
OVER
TIME
AND
CHANCES
ARE
THAT
A
TRULY
LARGE
REGION
OF
MEMORY
WILL
SIMPLY
NOT
BE
AVAILABLE
SINCE
THERE
ARE
USUALLY
WAYS
OF
GETTING
THE
JOB
DONE
WITHOUT
HUGE
BUFFERS
THE
KERNEL
DEVELOPERS
HAVE
NOT
PUT
A
HIGH
PRIORITY
ON
MAKING
LARGE
ALLOCATIONS
WORK
BEFORE
YOU
TRY
TO
OBTAIN
A
LARGE
MEMORY
AREA
YOU
SHOULD
REALLY
CONSIDER
THE
ALTERNATIVES
BY
FAR
THE
BEST
WAY
OF
PER
FORMING
LARGE
I
O
OPERATIONS
IS
THROUGH
SCATTER
GATHER
OPERATIONS
WHICH
WE
DISCUSS
IN
THE
SECTION
SCATTER
GATHER
MAPPINGS
IN
CHAPTER
ACQUIRING
A
DEDICATED
BUFFER
AT
BOOT
TIME
IF
YOU
REALLY
NEED
A
HUGE
BUFFER
OF
PHYSICALLY
CONTIGUOUS
MEMORY
THE
BEST
APPROACH
IS
OFTEN
TO
ALLOCATE
IT
BY
REQUESTING
MEMORY
AT
BOOT
TIME
ALLOCATION
AT
BOOT
TIME
IS
THE
ONLY
WAY
TO
RETRIEVE
CONSECUTIVE
MEMORY
PAGES
WHILE
BYPASSING
THE
LIMITS
IMPOSED
BY
ON
THE
BUFFER
SIZE
BOTH
IN
TERMS
OF
MAXIMUM
ALLOWED
SIZE
AND
LIMITED
CHOICE
OF
SIZES
ALLOCATING
MEMORY
AT
BOOT
TIME
IS
A
DIRTY
TECH
NIQUE
BECAUSE
IT
BYPASSES
ALL
MEMORY
MANAGEMENT
POLICIES
BY
RESERVING
A
PRIVATE
MEMORY
POOL
THIS
TECHNIQUE
IS
INELEGANT
AND
INFLEXIBLE
BUT
IT
IS
ALSO
THE
LEAST
PRONE
TO
FAILURE
NEEDLESS
TO
SAY
A
MODULE
CAN
T
ALLOCATE
MEMORY
AT
BOOT
TIME
ONLY
DRIV
ERS
DIRECTLY
LINKED
TO
THE
KERNEL
CAN
DO
THAT
ONE
NOTICEABLE
PROBLEM
WITH
BOOT
TIME
ALLOCATION
IS
THAT
IT
IS
NOT
A
FEASIBLE
OPTION
FOR
THE
AVERAGE
USER
SINCE
THIS
MECHANISM
IS
AVAILABLE
ONLY
FOR
CODE
LINKED
IN
THE
KER
NEL
IMAGE
A
DEVICE
DRIVER
USING
THIS
KIND
OF
ALLOCATION
CAN
BE
INSTALLED
OR
REPLACED
ONLY
BY
REBUILDING
THE
KERNEL
AND
REBOOTING
THE
COMPUTER
WHEN
THE
KERNEL
IS
BOOTED
IT
GAINS
ACCESS
TO
ALL
THE
PHYSICAL
MEMORY
AVAILABLE
IN
THE
SYSTEM
IT
THEN
INITIALIZES
EACH
OF
ITS
SUBSYSTEMS
BY
CALLING
THAT
SUBSYSTEM
INITIALIZA
TION
FUNCTION
ALLOWING
INITIALIZATION
CODE
TO
ALLOCATE
A
MEMORY
BUFFER
FOR
PRIVATE
USE
BY
REDUCING
THE
AMOUNT
OF
RAM
LEFT
FOR
NORMAL
SYSTEM
OPERATION
BOOT
TIME
MEMORY
ALLOCATION
IS
PERFORMED
BY
CALLING
ONE
OF
THESE
FUNCTIONS
INCLUDE
LINUX
BOOTMEM
H
VOID
UNSIGNED
LONG
SIZE
VOID
UNSIGNED
LONG
SIZE
VOID
UNSIGNED
LONG
SIZE
VOID
UNSIGNED
LONG
SIZE
THE
FUNCTIONS
ALLOCATE
EITHER
WHOLE
PAGES
IF
THEY
END
WITH
OR
NON
PAGE
ALIGNED
MEMORY
AREAS
THE
ALLOCATED
MEMORY
MAY
BE
HIGH
MEMORY
UNLESS
ONE
OF
THE
VERSIONS
IS
USED
IF
YOU
ARE
ALLOCATING
THIS
BUFFER
FOR
A
DEVICE
DRIVER
YOU
PROBA
BLY
WANT
TO
USE
IT
FOR
DMA
OPERATIONS
AND
THAT
IS
NOT
ALWAYS
POSSIBLE
WITH
HIGH
MEMORY
THUS
YOU
PROBABLY
WANT
TO
USE
ONE
OF
THE
VARIANTS
IT
IS
RARE
TO
FREE
MEMORY
ALLOCATED
AT
BOOT
TIME
YOU
WILL
ALMOST
CERTAINLY
BE
UNABLE
TO
GET
IT
BACK
LATER
IF
YOU
WANT
IT
THERE
IS
AN
INTERFACE
TO
FREE
THIS
MEMORY
HOWEVER
VOID
UNSIGNED
LONG
ADDR
UNSIGNED
LONG
SIZE
NOTE
THAT
PARTIAL
PAGES
FREED
IN
THIS
MANNER
ARE
NOT
RETURNED
TO
THE
SYSTEM
BUT
IF
YOU
ARE
USING
THIS
TECHNIQUE
YOU
HAVE
PROBABLY
ALLOCATED
A
FAIR
NUMBER
OF
WHOLE
PAGES
TO
BEGIN
WITH
IF
YOU
MUST
USE
BOOT
TIME
ALLOCATION
YOU
NEED
TO
LINK
YOUR
DRIVER
DIRECTLY
INTO
THE
KERNEL
SEE
THE
FILES
IN
THE
KERNEL
SOURCE
UNDER
DOCUMENTATION
KBUILD
FOR
MORE
INFOR
MATION
ON
HOW
THIS
SHOULD
BE
DONE
QUICK
REFERENCE
THE
FUNCTIONS
AND
SYMBOLS
RELATED
TO
MEMORY
ALLOCATION
ARE
INCLUDE
LINUX
SLAB
H
VOID
KMALLOC
SIZE
INT
FLAGS
VOID
KFREE
VOID
OBJ
THE
MOST
FREQUENTLY
USED
INTERFACE
TO
MEMORY
ALLOCATION
INCLUDE
LINUX
MM
H
FLAGS
THAT
CONTROL
HOW
MEMORY
ALLOCATIONS
ARE
PERFORMED
FROM
THE
LEAST
RESTRIC
TIVE
TO
THE
MOST
THE
AND
PRIORITIES
ALLOW
THE
CURRENT
PROCESS
TO
BE
PUT
TO
SLEEP
TO
SATISFY
THE
REQUEST
AND
DISABLE
FILESYSTEM
OPERATIONS
AND
ALL
I
O
OPERATIONS
RESPECTIVELY
WHILE
ALLOCATIONS
CAN
NOT
SLEEP
AT
ALL
THESE
FLAGS
MODIFY
THE
KERNEL
BEHAVIOR
WHEN
ALLOCATING
MEMORY
INCLUDE
LINUX
MALLOC
H
CHAR
NAME
SIZE
OFFSET
UNSIGNED
LONG
FLAGS
CONSTRUCTOR
DESTRUCTOR
INT
CACHE
CREATE
AND
DESTROY
A
SLAB
CACHE
THE
CACHE
CAN
BE
USED
TO
ALLOCATE
SEVERAL
OBJECTS
OF
THE
SAME
SIZE
FLAGS
THAT
CAN
BE
SPECIFIED
WHILE
CREATING
A
CACHE
FLAGS
THAT
THE
ALLOCATOR
CAN
PASS
TO
THE
CONSTRUCTOR
AND
THE
DESTRUCTOR
FUNCTIONS
VOID
CACHE
INT
FLAGS
VOID
CACHE
CONST
VOID
OBJ
ALLOCATE
AND
RELEASE
A
SINGLE
OBJECT
FROM
THE
CACHE
PROC
SLABINFO
A
VIRTUAL
FILE
CONTAINING
STATISTICS
ON
SLAB
CACHE
USAGE
INCLUDE
LINUX
MEMPOOL
H
INT
VOID
DATA
VOID
POOL
FUNCTIONS
FOR
THE
CREATION
OF
MEMORY
POOLS
WHICH
TRY
TO
AVOID
MEMORY
ALLOCA
TION
FAILURES
BY
KEEPING
AN
EMERGENCY
LIST
OF
ALLOCATED
ITEMS
VOID
POOL
INT
VOID
VOID
ELEMENT
POOL
FUNCTIONS
FOR
ALLOCATING
ITEMS
FROM
AND
RETURNING
THEM
TO
MEMORY
POOLS
UNSIGNED
LONG
INT
FLAGS
UNSIGNED
LONG
INT
FLAGS
UNSIGNED
LONG
INT
FLAGS
UNSIGNED
LONG
ORDER
THE
PAGE
ORIENTED
ALLOCATION
FUNCTIONS
RETURNS
A
SINGLE
ZERO
FILLED
PAGE
ALL
THE
OTHER
VERSIONS
OF
THE
CALL
DO
NOT
INITIALIZE
THE
CONTENTS
OF
THE
RETURNED
PAGE
INT
UNSIGNED
LONG
SIZE
RETURNS
THE
ALLOCATION
ORDER
ASSOCIATED
TO
SIZE
IN
THE
CURRENT
PLATFORM
ACCORDING
TO
THE
ARGUMENT
MUST
BE
A
POWER
OF
TWO
AND
THE
RETURN
VALUE
IS
AT
LEAST
VOID
UNSIGNED
LONG
ADDR
VOID
UNSIGNED
LONG
ADDR
UNSIGNED
LONG
ORDER
FUNCTIONS
THAT
RELEASE
PAGE
ORIENTED
ALLOCATIONS
STRUCT
PAGE
INT
NID
UNSIGNED
INT
FLAGS
UNSIGNED
INT
ORDER
STRUCT
PAGE
UNSIGNED
INT
FLAGS
UNSIGNED
INT
ORDER
STRUCT
PAGE
UNSIGNED
INT
FLAGS
ALL
VARIANTS
OF
THE
LOWEST
LEVEL
PAGE
ALLOCATOR
IN
THE
LINUX
KERNEL
VOID
STRUCT
PAGE
PAGE
VOID
STRUCT
PAGE
PAGE
UNSIGNED
INT
ORDER
VOID
STRUCT
PAGE
PAGE
VOID
STRUCT
PAGE
PAGE
VARIOUS
WAYS
OF
FREEING
PAGES
ALLOCATED
WITH
ONE
OF
THE
FORMS
OF
INCLUDE
LINUX
VMALLOC
H
VOID
VMALLOC
UNSIGNED
LONG
SIZE
VOID
VFREE
VOID
ADDR
INCLUDE
ASM
IO
H
VOID
IOREMAP
UNSIGNED
LONG
OFFSET
UNSIGNED
LONG
SIZE
VOID
IOUNMAP
VOID
ADDR
FUNCTIONS
THAT
ALLOCATE
OR
FREE
A
CONTIGUOUS
VIRTUAL
ADDRESS
SPACE
IOREMAP
ACCESSES
PHYSICAL
MEMORY
THROUGH
VIRTUAL
ADDRESSES
WHILE
VMALLOC
ALLOCATES
FREE
PAGES
REGIONS
MAPPED
WITH
IOREMAP
ARE
FREED
WITH
IOUNMAP
WHILE
PAGES
OBTAINED
FROM
VMALLOC
ARE
RELEASED
WITH
VFREE
INCLUDE
LINUX
PERCPU
H
TYPE
NAME
TYPE
NAME
MACROS
THAT
DEFINE
AND
DECLARE
PER
CPU
VARIABLES
VARIABLE
INT
VARIABLE
VARIABLE
MACROS
THAT
PROVIDE
ACCESS
TO
STATICALLY
DECLARED
PER
CPU
VARIABLES
VOID
TYPE
VOID
SIZE
ALIGN
VOID
VOID
VARIABLE
FUNCTIONS
THAT
PERFORM
RUNTIME
ALLOCATION
AND
FREEING
OF
PER
CPU
VARIABLES
INT
VOID
VOID
VARIABLE
INT
OBTAINS
A
REFERENCE
TO
THE
CURRENT
PROCESSOR
THEREFORE
PREVENTING
PRE
EMPTION
AND
MOVEMENT
TO
ANOTHER
PROCESSOR
AND
RETURNS
THE
ID
NUMBER
OF
THE
PROCESSOR
RETURNS
THAT
REFERENCE
TO
ACCESS
A
DYNAMICALLY
ALLOCATED
PER
CPU
VARIABLE
USE
WITH
THE
ID
OF
THE
CPU
WHOSE
VERSION
SHOULD
BE
ACCESSED
MANIPULATIONS
OF
THE
CURRENT
CPU
VERSION
OF
A
DYNAMIC
PER
CPU
VARIABLE
SHOULD
PROBABLY
BE
SURROUNDED
BY
CALLS
TO
AND
INCLUDE
LINUX
BOOTMEM
H
VOID
UNSIGNED
LONG
SIZE
VOID
UNSIGNED
LONG
SIZE
VOID
UNSIGNED
LONG
SIZE
VOID
UNSIGNED
LONG
SIZE
VOID
UNSIGNED
LONG
ADDR
UNSIGNED
LONG
SIZE
FUNCTIONS
WHICH
CAN
BE
USED
ONLY
BY
DRIVERS
DIRECTLY
LINKED
INTO
THE
KERNEL
THAT
PERFORM
ALLOCATION
AND
FREEING
OF
MEMORY
AT
SYSTEM
BOOTSTRAP
TIME
ALTHOUGH
SOME
DEVICES
CAN
BE
CONTROLLED
USING
NOTHING
BUT
THEIR
I
O
REGIONS
MOST
REAL
DEVICES
ARE
A
BIT
MORE
COMPLICATED
THAN
THAT
DEVICES
HAVE
TO
DEAL
WITH
THE
EXTERNAL
WORLD
WHICH
OFTEN
INCLUDES
THINGS
SUCH
AS
SPINNING
DISKS
MOVING
TAPE
WIRES
TO
DISTANT
PLACES
AND
SO
ON
MUCH
HAS
TO
BE
DONE
IN
A
TIME
FRAME
THAT
IS
DIFFER
ENT
FROM
AND
FAR
SLOWER
THAN
THAT
OF
THE
PROCESSOR
SINCE
IT
IS
ALMOST
ALWAYS
UNDESIR
ABLE
TO
HAVE
THE
PROCESSOR
WAIT
ON
EXTERNAL
EVENTS
THERE
MUST
BE
A
WAY
FOR
A
DEVICE
TO
LET
THE
PROCESSOR
KNOW
WHEN
SOMETHING
HAS
HAPPENED
THAT
WAY
OF
COURSE
IS
INTERRUPTS
AN
INTERRUPT
IS
SIMPLY
A
SIGNAL
THAT
THE
HARDWARE
CAN
SEND
WHEN
IT
WANTS
THE
PROCESSOR
ATTENTION
LINUX
HANDLES
INTERRUPTS
IN
MUCH
THE
SAME
WAY
THAT
IT
HANDLES
SIGNALS
IN
USER
SPACE
FOR
THE
MOST
PART
A
DRIVER
NEED
ONLY
REGISTER
A
HANDLER
FOR
ITS
DEVICE
INTERRUPTS
AND
HANDLE
THEM
PROPERLY
WHEN
THEY
ARRIVE
OF
COURSE
UNDERNEATH
THAT
SIMPLE
PICTURE
THERE
IS
SOME
COMPLEXITY
IN
PARTICULAR
INTERRUPT
HANDLERS
ARE
SOMEWHAT
LIMITED
IN
THE
ACTIONS
THEY
CAN
PERFORM
AS
A
RESULT
OF
HOW
THEY
ARE
RUN
IT
IS
DIFFICULT
TO
DEMONSTRATE
THE
USE
OF
INTERRUPTS
WITHOUT
A
REAL
HARDWARE
DEVICE
TO
GENERATE
THEM
THUS
THE
SAMPLE
CODE
USED
IN
THIS
CHAPTER
WORKS
WITH
THE
PARALLEL
PORT
SUCH
PORTS
ARE
STARTING
TO
BECOME
SCARCE
ON
MODERN
HARDWARE
BUT
WITH
LUCK
MOST
PEOPLE
ARE
STILL
ABLE
TO
GET
THEIR
HANDS
ON
A
SYSTEM
WITH
AN
AVAILABLE
PORT
WE
LL
BE
WORKING
WITH
THE
SHORT
MODULE
FROM
THE
PREVIOUS
CHAPTER
WITH
SOME
SMALL
ADDI
TIONS
IT
CAN
GENERATE
AND
HANDLE
INTERRUPTS
FROM
THE
PARALLEL
PORT
THE
MODULE
NAME
SHORT
ACTUALLY
MEANS
SHORT
INT
IT
IS
C
ISN
T
IT
TO
REMIND
US
THAT
IT
HANDLES
INTERRUPTS
BEFORE
WE
GET
INTO
THE
TOPIC
HOWEVER
IT
IS
TIME
FOR
ONE
CAUTIONARY
NOTE
INTERRUPT
HANDLERS
BY
THEIR
NATURE
RUN
CONCURRENTLY
WITH
OTHER
CODE
THUS
THEY
INEVITABLY
RAISE
ISSUES
OF
CONCURRENCY
AND
CONTENTION
FOR
DATA
STRUCTURES
AND
HARDWARE
IF
YOU
SUCCUMBED
TO
THE
TEMPTATION
TO
PASS
OVER
THE
DISCUSSION
IN
CHAPTER
WE
UNDER
STAND
BUT
WE
ALSO
RECOMMEND
THAT
YOU
TURN
BACK
AND
HAVE
ANOTHER
LOOK
NOW
A
SOLID
UNDERSTANDING
OF
CONCURRENCY
CONTROL
TECHNIQUES
IS
VITAL
WHEN
WORKING
WITH
INTERRUPTS
PREPARING
THE
PARALLEL
PORT
ALTHOUGH
THE
PARALLEL
INTERFACE
IS
SIMPLE
IT
CAN
TRIGGER
INTERRUPTS
THIS
CAPABILITY
IS
USED
BY
THE
PRINTER
TO
NOTIFY
THE
LP
DRIVER
THAT
IT
IS
READY
TO
ACCEPT
THE
NEXT
CHARACTER
IN
THE
BUFFER
LIKE
MOST
DEVICES
THE
PARALLEL
PORT
DOESN
T
ACTUALLY
GENERATE
INTERRUPTS
BEFORE
IT
INSTRUCTED
TO
DO
SO
THE
PARALLEL
STANDARD
STATES
THAT
SETTING
BIT
OF
PORT
OR
WHATEVER
ENABLES
INTERRUPT
REPORTING
A
SIMPLE
OUTB
CALL
TO
SET
THE
BIT
IS
PERFORMED
BY
SHORT
AT
MODULE
INITIALIZATION
ONCE
INTERRUPTS
ARE
ENABLED
THE
PARALLEL
INTERFACE
GENERATES
AN
INTERRUPT
WHENEVER
THE
ELECTRICAL
SIGNAL
AT
PIN
THE
SO
CALLED
ACK
BIT
CHANGES
FROM
LOW
TO
HIGH
THE
SIMPLEST
WAY
TO
FORCE
THE
INTERFACE
TO
GENERATE
INTERRUPTS
SHORT
OF
HOOKING
UP
A
PRINTER
TO
THE
PORT
IS
TO
CONNECT
PINS
AND
OF
THE
PARALLEL
CONNECTOR
A
SHORT
LENGTH
OF
WIRE
INSERTED
INTO
THE
APPROPRIATE
HOLES
IN
THE
PARALLEL
PORT
CONNECTOR
ON
THE
BACK
OF
YOUR
SYSTEM
CREATES
THIS
CONNECTION
THE
PINOUT
OF
THE
PARALLEL
PORT
IS
SHOWN
IN
FIGURE
PIN
IS
THE
MOST
SIGNIFICANT
BIT
OF
THE
PARALLEL
DATA
BYTE
IF
YOU
WRITE
BINARY
DATA
TO
DEV
YOU
GENERATE
SEVERAL
INTERRUPTS
WRITING
ASCII
TEXT
TO
THE
PORT
WON
T
GENERATE
ANY
INTERRUPTS
THOUGH
BECAUSE
THE
ASCII
CHARACTER
SET
HAS
NO
ENTRIES
WITH
THE
TOP
BIT
SET
IF
YOU
D
RATHER
AVOID
WIRING
PINS
TOGETHER
BUT
YOU
DO
HAVE
A
PRINTER
AT
HAND
YOU
CAN
RUN
THE
SAMPLE
INTERRUPT
HANDLER
USING
A
REAL
PRINTER
AS
SHOWN
LATER
HOWEVER
NOTE
THAT
THE
PROBING
FUNCTIONS
WE
INTRODUCE
DEPEND
ON
THE
JUMPER
BETWEEN
PIN
AND
BEING
IN
PLACE
AND
YOU
NEED
IT
TO
EXPERIMENT
WITH
PROBING
USING
OUR
CODE
INSTALLING
AN
INTERRUPT
HANDLER
IF
YOU
WANT
TO
ACTUALLY
SEE
INTERRUPTS
BEING
GENERATED
WRITING
TO
THE
HARDWARE
DEVICE
ISN
T
ENOUGH
A
SOFTWARE
HANDLER
MUST
BE
CONFIGURED
IN
THE
SYSTEM
IF
THE
LINUX
KERNEL
HASN
T
BEEN
TOLD
TO
EXPECT
YOUR
INTERRUPT
IT
SIMPLY
ACKNOWLEDGES
AND
IGNORES
IT
INTERRUPT
LINES
ARE
A
PRECIOUS
AND
OFTEN
LIMITED
RESOURCE
PARTICULARLY
WHEN
THERE
ARE
ONLY
OR
OF
THEM
THE
KERNEL
KEEPS
A
REGISTRY
OF
INTERRUPT
LINES
SIMILAR
TO
THE
REGISTRY
OF
I
O
PORTS
A
MODULE
IS
EXPECTED
TO
REQUEST
AN
INTERRUPT
CHANNEL
OR
IRQ
FOR
INTERRUPT
REQUEST
BEFORE
USING
IT
AND
TO
RELEASE
IT
WHEN
FINISHED
IN
MANY
SITUA
TIONS
MODULES
ARE
ALSO
EXPECTED
TO
BE
ABLE
TO
SHARE
INTERRUPT
LINES
WITH
OTHER
DRIV
ERS
AS
WE
WILL
SEE
THE
FOLLOWING
FUNCTIONS
DECLARED
IN
LINUX
INTERRUPT
H
IMPLEMENT
THE
INTERRUPT
REGISTRATION
INTERFACE
INT
UNSIGNED
INT
IRQ
HANDLER
INT
VOID
STRUCT
UNSIGNED
LONG
FLAGS
CONST
CHAR
VOID
VOID
UNSIGNED
INT
IRQ
VOID
THE
VALUE
RETURNED
FROM
TO
THE
REQUESTING
FUNCTION
IS
EITHER
TO
INDICATE
SUCCESS
OR
A
NEGATIVE
ERROR
CODE
AS
USUAL
IT
NOT
UNCOMMON
FOR
THE
FUNCTION
TO
RETURN
EBUSY
TO
SIGNAL
THAT
ANOTHER
DRIVER
IS
ALREADY
USING
THE
REQUESTED
INTERRUPT
LINE
THE
ARGUMENTS
TO
THE
FUNCTIONS
ARE
AS
FOLLOWS
UNSIGNED
INT
IRQ
THE
INTERRUPT
NUMBER
BEING
REQUESTED
HANDLER
INT
VOID
STRUCT
THE
POINTER
TO
THE
HANDLING
FUNCTION
BEING
INSTALLED
WE
DISCUSS
THE
ARGUMENTS
TO
THIS
FUNCTION
AND
ITS
RETURN
VALUE
LATER
IN
THIS
CHAPTER
UNSIGNED
LONG
FLAGS
AS
YOU
MIGHT
EXPECT
A
BIT
MASK
OF
OPTIONS
DESCRIBED
LATER
RELATED
TO
INTERRUPT
MANAGEMENT
CONST
CHAR
THE
STRING
PASSED
TO
IS
USED
IN
PROC
INTERRUPTS
TO
SHOW
THE
OWNER
OF
THE
INTERRUPT
SEE
THE
NEXT
SECTION
VOID
POINTER
USED
FOR
SHARED
INTERRUPT
LINES
IT
IS
A
UNIQUE
IDENTIFIER
THAT
IS
USED
WHEN
THE
INTERRUPT
LINE
IS
FREED
AND
THAT
MAY
ALSO
BE
USED
BY
THE
DRIVER
TO
POINT
TO
ITS
OWN
PRIVATE
DATA
AREA
TO
IDENTIFY
WHICH
DEVICE
IS
INTERRUPTING
IF
THE
INTERRUPT
IS
NOT
SHARED
CAN
BE
SET
TO
NULL
BUT
IT
A
GOOD
IDEA
ANYWAY
TO
USE
THIS
ITEM
TO
POINT
TO
THE
DEVICE
STRUCTURE
WE
LL
SEE
A
PRACTICAL
USE
FOR
IN
THE
SEC
TION
IMPLEMENTING
A
HANDLER
THE
BITS
THAT
CAN
BE
SET
IN
FLAGS
ARE
AS
FOLLOWS
WHEN
SET
THIS
INDICATES
A
FAST
INTERRUPT
HANDLER
FAST
HANDLERS
ARE
EXECUTED
WITH
INTERRUPTS
DISABLED
ON
THE
CURRENT
PROCESSOR
THE
TOPIC
IS
COVERED
IN
THE
SEC
TION
FAST
AND
SLOW
HANDLERS
THIS
BIT
SIGNALS
THAT
THE
INTERRUPT
CAN
BE
SHARED
BETWEEN
DEVICES
THE
CONCEPT
OF
SHARING
IS
OUTLINED
IN
THE
SECTION
INTERRUPT
SHARING
THIS
BIT
INDICATES
THAT
THE
GENERATED
INTERRUPTS
CAN
CONTRIBUTE
TO
THE
ENTROPY
POOL
USED
BY
DEV
RANDOM
AND
DEV
URANDOM
THESE
DEVICES
RETURN
TRULY
RANDOM
NUM
BERS
WHEN
READ
AND
ARE
DESIGNED
TO
HELP
APPLICATION
SOFTWARE
CHOOSE
SECURE
KEYS
FOR
ENCRYPTION
SUCH
RANDOM
NUMBERS
ARE
EXTRACTED
FROM
AN
ENTROPY
POOL
THAT
IS
CONTRIBUTED
BY
VARIOUS
RANDOM
EVENTS
IF
YOUR
DEVICE
GENERATES
INTERRUPTS
AT
TRULY
RANDOM
TIMES
YOU
SHOULD
SET
THIS
FLAG
IF
ON
THE
OTHER
HAND
YOUR
INTERRUPTS
ARE
PREDICTABLE
FOR
EXAMPLE
VERTICAL
BLANKING
OF
A
FRAME
GRABBER
THE
FLAG
IS
NOT
WORTH
SETTING
IT
WOULDN
T
CONTRIBUTE
TO
SYSTEM
ENTROPY
ANYWAY
DEVICES
THAT
COULD
BE
INFLUENCED
BY
ATTACKERS
SHOULD
NOT
SET
THIS
FLAG
FOR
EXAMPLE
NETWORK
DRIVERS
CAN
BE
SUBJECTED
TO
PREDICTABLE
PACKET
TIMING
FROM
OUTSIDE
AND
SHOULD
NOT
CONTRIBUTE
TO
THE
ENTROPY
POOL
SEE
THE
COMMENTS
IN
DRIVERS
CHAR
RANDOM
C
FOR
MORE
INFORMATION
THE
INTERRUPT
HANDLER
CAN
BE
INSTALLED
EITHER
AT
DRIVER
INITIALIZATION
OR
WHEN
THE
DEVICE
IS
FIRST
OPENED
ALTHOUGH
INSTALLING
THE
INTERRUPT
HANDLER
FROM
WITHIN
THE
MOD
ULE
INITIALIZATION
FUNCTION
MIGHT
SOUND
LIKE
A
GOOD
IDEA
IT
OFTEN
ISN
T
ESPECIALLY
IF
YOUR
DEVICE
DOES
NOT
SHARE
INTERRUPTS
BECAUSE
THE
NUMBER
OF
INTERRUPT
LINES
IS
LIM
ITED
YOU
DON
T
WANT
TO
WASTE
THEM
YOU
CAN
EASILY
END
UP
WITH
MORE
DEVICES
IN
YOUR
COMPUTER
THAN
THERE
ARE
INTERRUPTS
IF
A
MODULE
REQUESTS
AN
IRQ
AT
INITIALIZATION
IT
PREVENTS
ANY
OTHER
DRIVER
FROM
USING
THE
INTERRUPT
EVEN
IF
THE
DEVICE
HOLDING
IT
IS
NEVER
USED
REQUESTING
THE
INTERRUPT
AT
DEVICE
OPEN
ON
THE
OTHER
HAND
ALLOWS
SOME
SHARING
OF
RESOURCES
IT
IS
POSSIBLE
FOR
EXAMPLE
TO
RUN
A
FRAME
GRABBER
ON
THE
SAME
INTERRUPT
AS
A
MODEM
AS
LONG
AS
YOU
DON
T
USE
THE
TWO
DEVICES
AT
THE
SAME
TIME
IT
IS
QUITE
COMMON
FOR
USERS
TO
LOAD
THE
MODULE
FOR
A
SPECIAL
DEVICE
AT
SYSTEM
BOOT
EVEN
IF
THE
DEVICE
IS
RARELY
USED
A
DATA
ACQUISITION
GADGET
MIGHT
USE
THE
SAME
INTERRUPT
AS
THE
SECOND
SERIAL
PORT
WHILE
IT
NOT
TOO
HARD
TO
AVOID
CONNECTING
TO
YOUR
INTERNET
SERVICE
PRO
VIDER
ISP
DURING
DATA
ACQUISITION
BEING
FORCED
TO
UNLOAD
A
MODULE
IN
ORDER
TO
USE
THE
MODEM
IS
REALLY
UNPLEASANT
THE
CORRECT
PLACE
TO
CALL
IS
WHEN
THE
DEVICE
IS
FIRST
OPENED
BEFORE
THE
HARDWARE
IS
INSTRUCTED
TO
GENERATE
INTERRUPTS
THE
PLACE
TO
CALL
IS
THE
LAST
TIME
THE
DEVICE
IS
CLOSED
AFTER
THE
HARDWARE
IS
TOLD
NOT
TO
INTERRUPT
THE
PROCESSOR
ANY
MORE
THE
DISADVANTAGE
OF
THIS
TECHNIQUE
IS
THAT
YOU
NEED
TO
KEEP
A
PER
DEVICE
OPEN
COUNT
SO
THAT
YOU
KNOW
WHEN
INTERRUPTS
CAN
BE
DISABLED
THIS
DISCUSSION
NOTWITHSTANDING
SHORT
REQUESTS
ITS
INTERRUPT
LINE
AT
LOAD
TIME
THIS
WAS
DONE
SO
THAT
YOU
CAN
RUN
THE
TEST
PROGRAMS
WITHOUT
HAVING
TO
RUN
AN
EXTRA
PRO
CESS
TO
KEEP
THE
DEVICE
OPEN
SHORT
THEREFORE
REQUESTS
THE
INTERRUPT
FROM
WITHIN
ITS
INITIALIZATION
FUNCTION
INSTEAD
OF
DOING
IT
IN
AS
A
REAL
DEVICE
DRIVER
WOULD
THE
INTERRUPT
REQUESTED
BY
THE
FOLLOWING
CODE
IS
THE
ACTUAL
ASSIGNMENT
OF
THE
VARIABLE
I
E
DETERMINING
WHICH
IRQ
TO
USE
IS
SHOWN
LATER
SINCE
IT
IS
NOT
RELE
VANT
TO
THE
CURRENT
DISCUSSION
IS
THE
BASE
I
O
ADDRESS
OF
THE
PARALLEL
INTER
FACE
BEING
USED
REGISTER
OF
THE
INTERFACE
IS
WRITTEN
TO
ENABLE
INTERRUPT
REPORTING
IF
RESULT
SHORT
NULL
IF
RESULT
PRINTK
SHORT
CAN
T
GET
ASSIGNED
IRQ
I
N
ELSE
ACTUALLY
ENABLE
IT
ASSUME
THIS
IS
A
PARALLEL
PORT
OUTB
THE
CODE
SHOWS
THAT
THE
HANDLER
BEING
INSTALLED
IS
A
FAST
HANDLER
DOESN
T
SUPPORT
INTERRUPT
SHARING
IS
MISSING
AND
DOESN
T
CONTRIBUTE
TO
SYSTEM
ENTROPY
IS
MISSING
TOO
THE
OUTB
CALL
THEN
ENABLES
INTER
RUPT
REPORTING
FOR
THE
PARALLEL
PORT
FOR
WHAT
IT
WORTH
THE
AND
ARCHITECTURES
DEFINE
A
FUNCTION
FOR
QUERY
ING
THE
AVAILABILITY
OF
AN
INTERRUPT
LINE
INT
UNSIGNED
INT
IRQ
UNSIGNED
LONG
FLAGS
THIS
FUNCTION
RETURNS
A
NONZERO
VALUE
IF
AN
ATTEMPT
TO
ALLOCATE
THE
GIVEN
INTERRUPT
SUC
CEEDS
NOTE
HOWEVER
THAT
THINGS
CAN
ALWAYS
CHANGE
BETWEEN
CALLS
TO
AND
THE
PROC
INTERFACE
WHENEVER
A
HARDWARE
INTERRUPT
REACHES
THE
PROCESSOR
AN
INTERNAL
COUNTER
IS
INCRE
MENTED
PROVIDING
A
WAY
TO
CHECK
WHETHER
THE
DEVICE
IS
WORKING
AS
EXPECTED
REPORTED
INTERRUPTS
ARE
SHOWN
IN
PROC
INTERRUPTS
THE
FOLLOWING
SNAPSHOT
WAS
TAKEN
ON
A
TWO
PROCESSOR
PENTIUM
SYSTEM
ROOT
MONTALCINO
BIKE
CORBET
WRITE
SRC
SHORT
M
PROC
INTERRUPTS
IO
APIC
EDGE
TIMER
XT
PIC
CASCADE
IO
APIC
EDGE
RTC
IO
APIC
LEVEL
IO
APIC
LEVEL
IO
APIC
EDGE
NMI
LOC
ERR
MIS
THE
FIRST
COLUMN
IS
THE
IRQ
NUMBER
YOU
CAN
SEE
FROM
THE
IRQS
THAT
ARE
MISSING
THAT
THE
FILE
SHOWS
ONLY
INTERRUPTS
CORRESPONDING
TO
INSTALLED
HANDLERS
FOR
EXAMPLE
THE
FIRST
SERIAL
PORT
WHICH
USES
INTERRUPT
NUMBER
IS
NOT
SHOWN
INDICATING
THAT
THE
MODEM
ISN
T
BEING
USED
IN
FACT
EVEN
IF
THE
MODEM
HAD
BEEN
USED
EARLIER
BUT
WASN
T
IN
USE
AT
THE
TIME
OF
THE
SNAPSHOT
IT
WOULD
NOT
SHOW
UP
IN
THE
FILE
THE
SERIAL
PORTS
ARE
WELL
BEHAVED
AND
RELEASE
THEIR
INTERRUPT
HANDLERS
WHEN
THE
DEVICE
IS
CLOSED
THE
PROC
INTERRUPTS
DISPLAY
SHOWS
HOW
MANY
INTERRUPTS
HAVE
BEEN
DELIVERED
TO
EACH
CPU
ON
THE
SYSTEM
AS
YOU
CAN
SEE
FROM
THE
OUTPUT
THE
LINUX
KERNEL
GENERALLY
HANDLES
INTERRUPTS
ON
THE
FIRST
CPU
AS
A
WAY
OF
MAXIMIZING
CACHE
LOCALITY
THE
LAST
TWO
COL
UMNS
GIVE
INFORMATION
ON
THE
PROGRAMMABLE
INTERRUPT
CONTROLLER
THAT
HANDLES
THE
INTER
RUPT
AND
THAT
A
DRIVER
WRITER
DOES
NOT
NEED
TO
WORRY
ABOUT
AND
THE
NAME
OF
THE
DEVICE
THAT
HAVE
REGISTERED
HANDLERS
FOR
THE
INTERRUPT
AS
SPECIFIED
IN
THE
ARGUMENT
TO
THE
PROC
TREE
CONTAINS
ANOTHER
INTERRUPT
RELATED
FILE
PROC
STAT
SOMETIMES
YOU
LL
FIND
ONE
FILE
MORE
USEFUL
AND
SOMETIMES
YOU
LL
PREFER
THE
OTHER
PROC
STAT
RECORDS
SEVERAL
LOW
LEVEL
STATISTICS
ABOUT
SYSTEM
ACTIVITY
INCLUDING
BUT
NOT
LIMITED
TO
THE
NUMBER
OF
INTERRUPTS
RECEIVED
SINCE
SYSTEM
BOOT
EACH
LINE
OF
STAT
BEGINS
WITH
A
TEXT
STRING
THAT
IS
THE
KEY
TO
THE
LINE
THE
INTR
MARK
IS
WHAT
WE
ARE
LOOKING
FOR
THE
FOL
LOWING
TRUNCATED
SNAPSHOT
WAS
TAKEN
SHORTLY
AFTER
THE
PREVIOUS
ONE
INTR
THE
FIRST
NUMBER
IS
THE
TOTAL
OF
ALL
INTERRUPTS
WHILE
EACH
OF
THE
OTHERS
REPRESENTS
A
SINGLE
IRQ
LINE
STARTING
WITH
INTERRUPT
ALL
OF
THE
COUNTS
ARE
SUMMED
ACROSS
ALL
PROCESSORS
IN
THE
SYSTEM
THIS
SNAPSHOT
SHOWS
THAT
INTERRUPT
NUMBER
HAS
BEEN
USED
TIMES
EVEN
THOUGH
NO
HANDLER
IS
CURRENTLY
INSTALLED
IF
THE
DRIVER
YOU
RE
TESTING
ACQUIRES
AND
RELEASES
THE
INTERRUPT
AT
EACH
OPEN
AND
CLOSE
CYCLE
YOU
MAY
FIND
PROC
STAT
MORE
USEFUL
THAN
PROC
INTERRUPTS
ANOTHER
DIFFERENCE
BETWEEN
THE
TWO
FILES
IS
THAT
INTERRUPTS
IS
NOT
ARCHITECTURE
DEPEN
DENT
EXCEPT
PERHAPS
FOR
A
COUPLE
OF
LINES
AT
THE
END
WHEREAS
STAT
IS
THE
NUMBER
OF
FIELDS
DEPENDS
ON
THE
HARDWARE
UNDERLYING
THE
KERNEL
THE
NUMBER
OF
AVAILABLE
INTER
RUPTS
VARIES
FROM
AS
FEW
AS
ON
THE
SPARC
TO
AS
MANY
AS
ON
THE
IA
AND
A
FEW
OTHER
SYSTEMS
IT
INTERESTING
TO
NOTE
THAT
THE
NUMBER
OF
INTERRUPTS
DEFINED
ON
THE
IS
CURRENTLY
NOT
AS
YOU
MAY
EXPECT
THIS
AS
EXPLAINED
IN
INCLUDE
ASM
IRQ
H
DEPENDS
ON
LINUX
USING
THE
ARCHITECTURAL
LIMIT
INSTEAD
OF
AN
IMPLE
MENTATION
SPECIFIC
LIMIT
SUCH
AS
THE
INTERRUPT
SOURCES
OF
THE
OLD
FASHIONED
PC
INTERRUPT
CONTROLLER
THE
FOLLOWING
IS
A
SNAPSHOT
OF
PROC
INTERRUPTS
TAKEN
ON
AN
IA
SYSTEM
AS
YOU
CAN
SEE
BESIDES
DIFFERENT
HARDWARE
ROUTING
OF
COMMON
INTERRUPT
SOURCES
THE
OUTPUT
IS
VERY
SIMILAR
TO
THAT
FROM
THE
BIT
SYSTEM
SHOWN
EARLIER
IO
SAPIC
LEVEL
SAPIC
PERFMON
IO
SAPIC
LEVEL
IO
SAPIC
LEVEL
USB
UHCI
IO
SAPIC
EDGE
IO
SAPIC
EDGE
KEYBOARD
IO
SAPIC
EDGE
PS
MOUSE
SAPIC
TIMER
ALTHOUGH
SOME
LARGER
SYSTEMS
EXPLICITLY
USE
INTERRUPT
BALANCING
SCHEMES
TO
SPREAD
THE
INTERRUPT
LOAD
ACROSS
THE
SYSTEM
SAPIC
IPI
NMI
ERR
AUTODETECTING
THE
IRQ
NUMBER
ONE
OF
THE
MOST
CHALLENGING
PROBLEMS
FOR
A
DRIVER
AT
INITIALIZATION
TIME
CAN
BE
HOW
TO
DETERMINE
WHICH
IRQ
LINE
IS
GOING
TO
BE
USED
BY
THE
DEVICE
THE
DRIVER
NEEDS
THE
INFORMATION
IN
ORDER
TO
CORRECTLY
INSTALL
THE
HANDLER
EVEN
THOUGH
A
PROGRAMMER
COULD
REQUIRE
THE
USER
TO
SPECIFY
THE
INTERRUPT
NUMBER
AT
LOAD
TIME
THIS
IS
A
BAD
PRAC
TICE
BECAUSE
MOST
OF
THE
TIME
THE
USER
DOESN
T
KNOW
THE
NUMBER
EITHER
BECAUSE
HE
DIDN
T
CONFIGURE
THE
JUMPERS
OR
BECAUSE
THE
DEVICE
IS
JUMPERLESS
MOST
USERS
WANT
THEIR
HARDWARE
TO
JUST
WORK
AND
ARE
NOT
INTERESTED
IN
ISSUES
LIKE
INTERRUPT
NUM
BERS
SO
AUTODETECTION
OF
THE
INTERRUPT
NUMBER
IS
A
BASIC
REQUIREMENT
FOR
DRIVER
USABILITY
SOMETIMES
AUTODETECTION
DEPENDS
ON
THE
KNOWLEDGE
THAT
SOME
DEVICES
FEATURE
A
DEFAULT
BEHAVIOR
THAT
RARELY
IF
EVER
CHANGES
IN
THIS
CASE
THE
DRIVER
MIGHT
ASSUME
THAT
THE
DEFAULT
VALUES
APPLY
THIS
IS
EXACTLY
HOW
SHORT
BEHAVES
BY
DEFAULT
WITH
THE
PARALLEL
PORT
THE
IMPLEMENTATION
IS
STRAIGHTFORWARD
AS
SHOWN
BY
SHORT
ITSELF
IF
NOT
YET
SPECIFIED
FORCE
THE
DEFAULT
ON
SWITCH
CASE
BREAK
CASE
BREAK
CASE
BREAK
THE
CODE
ASSIGNS
THE
INTERRUPT
NUMBER
ACCORDING
TO
THE
CHOSEN
BASE
I
O
ADDRESS
WHILE
ALLOWING
THE
USER
TO
OVERRIDE
THE
DEFAULT
AT
LOAD
TIME
WITH
SOMETHING
LIKE
INSMOD
SHORT
KO
IRQ
X
DEFAULTS
TO
SO
DEFAULTS
TO
SOME
DEVICES
ARE
MORE
ADVANCED
IN
DESIGN
AND
SIMPLY
ANNOUNCE
WHICH
INTERRUPT
THEY
RE
GOING
TO
USE
IN
THIS
CASE
THE
DRIVER
RETRIEVES
THE
INTERRUPT
NUMBER
BY
READ
ING
A
STATUS
BYTE
FROM
ONE
OF
THE
DEVICE
I
O
PORTS
OR
PCI
CONFIGURATION
SPACE
WHEN
THE
TARGET
DEVICE
IS
ONE
THAT
HAS
THE
ABILITY
TO
TELL
THE
DRIVER
WHICH
INTERRUPT
IT
IS
GOING
TO
USE
AUTODETECTING
THE
IRQ
NUMBER
JUST
MEANS
PROBING
THE
DEVICE
WITH
NO
ADDITIONAL
WORK
REQUIRED
TO
PROBE
THE
INTERRUPT
MOST
MODERN
HARDWARE
WORKS
THIS
WAY
FORTUNATELY
FOR
EXAMPLE
THE
PCI
STANDARD
SOLVES
THE
PROBLEM
BY
REQUIRING
PERIPHERAL
DEVICES
TO
DECLARE
WHAT
INTERRUPT
LINE
THEY
ARE
GOING
TO
USE
THE
PCI
STANDARD
IS
DISCUSSED
IN
CHAPTER
UNFORTUNATELY
NOT
EVERY
DEVICE
IS
PROGRAMMER
FRIENDLY
AND
AUTODETECTION
MIGHT
REQUIRE
SOME
PROBING
THE
TECHNIQUE
IS
QUITE
SIMPLE
THE
DRIVER
TELLS
THE
DEVICE
TO
GENERATE
INTERRUPTS
AND
WATCHES
WHAT
HAPPENS
IF
EVERYTHING
GOES
WELL
ONLY
ONE
INTERRUPT
LINE
IS
ACTIVATED
ALTHOUGH
PROBING
IS
SIMPLE
IN
THEORY
THE
ACTUAL
IMPLEMENTATION
MIGHT
BE
UNCLEAR
WE
LOOK
AT
TWO
WAYS
TO
PERFORM
THE
TASK
CALLING
KERNEL
DEFINED
HELPER
FUNCTIONS
AND
IMPLEMENTING
OUR
OWN
VERSION
KERNEL
ASSISTED
PROBING
THE
LINUX
KERNEL
OFFERS
A
LOW
LEVEL
FACILITY
FOR
PROBING
THE
INTERRUPT
NUMBER
IT
WORKS
FOR
ONLY
NONSHARED
INTERRUPTS
BUT
MOST
HARDWARE
THAT
IS
CAPABLE
OF
WORKING
IN
A
SHARED
INTERRUPT
MODE
PROVIDES
BETTER
WAYS
OF
FINDING
THE
CONFIGURED
INTERRUPT
NUM
BER
ANYWAY
THE
FACILITY
CONSISTS
OF
TWO
FUNCTIONS
DECLARED
IN
LINUX
INTERRUPT
H
WHICH
ALSO
DESCRIBES
THE
PROBING
MACHINERY
UNSIGNED
LONG
VOID
THIS
FUNCTION
RETURNS
A
BIT
MASK
OF
UNASSIGNED
INTERRUPTS
THE
DRIVER
MUST
PRE
SERVE
THE
RETURNED
BIT
MASK
AND
PASS
IT
TO
LATER
AFTER
THIS
CALL
THE
DRIVER
SHOULD
ARRANGE
FOR
ITS
DEVICE
TO
GENERATE
AT
LEAST
ONE
INTERRUPT
INT
UNSIGNED
LONG
AFTER
THE
DEVICE
HAS
REQUESTED
AN
INTERRUPT
THE
DRIVER
CALLS
THIS
FUNCTION
PASSING
AS
ITS
ARGUMENT
THE
BIT
MASK
PREVIOUSLY
RETURNED
BY
RETURNS
THE
NUMBER
OF
THE
INTERRUPT
THAT
WAS
ISSUED
AFTER
IF
NO
INTER
RUPTS
OCCURRED
IS
RETURNED
THEREFORE
IRQ
CAN
T
BE
PROBED
FOR
BUT
NO
CUS
TOM
DEVICE
CAN
USE
IT
ON
ANY
OF
THE
SUPPORTED
ARCHITECTURES
ANYWAY
IF
MORE
THAN
ONE
INTERRUPT
OCCURRED
AMBIGUOUS
DETECTION
RETURNS
A
NEGATIVE
VALUE
THE
PROGRAMMER
SHOULD
BE
CAREFUL
TO
ENABLE
INTERRUPTS
ON
THE
DEVICE
AFTER
THE
CALL
TO
AND
TO
DISABLE
THEM
BEFORE
CALLING
ADDITIONALLY
YOU
MUST
REMEMBER
TO
SERVICE
THE
PENDING
INTERRUPT
IN
YOUR
DEVICE
AFTER
THE
SHORT
MODULE
DEMONSTRATES
HOW
TO
USE
SUCH
PROBING
IF
YOU
LOAD
THE
MODULE
WITH
PROBE
THE
FOLLOWING
CODE
IS
EXECUTED
TO
DETECT
YOUR
INTERRUPT
LINE
PROVIDED
PINS
AND
OF
THE
PARALLEL
CONNECTOR
ARE
BOUND
TOGETHER
INT
COUNT
DO
UNSIGNED
LONG
MASK
MASK
ENABLE
REPORTING
CLEAR
THE
BIT
SET
THE
BIT
INTERRUPT
DISABLE
REPORTING
UDELAY
GIVE
IT
SOME
TIME
MASK
IF
NONE
OF
THEM
PRINTK
SHORT
NO
IRQ
REPORTED
BY
PROBE
N
IF
MORE
THAN
ONE
LINE
HAS
BEEN
ACTIVATED
THE
RESULT
IS
NEGATIVE
WE
SHOULD
SERVICE
THE
INTERRUPT
NO
NEED
FOR
LPT
PORT
AND
LOOP
OVER
AGAIN
LOOP
AT
MOST
FIVE
TIMES
THEN
GIVE
UP
WHILE
COUNT
IF
PRINTK
SHORT
PROBE
FAILED
I
TIMES
GIVING
UP
N
COUNT
NOTE
THE
USE
OF
UDELAY
BEFORE
CALLING
DEPENDING
ON
THE
SPEED
OF
YOUR
PROCESSOR
YOU
MAY
HAVE
TO
WAIT
FOR
A
BRIEF
PERIOD
TO
GIVE
THE
INTERRUPT
TIME
TO
ACTU
ALLY
BE
DELIVERED
PROBING
MIGHT
BE
A
LENGTHY
TASK
WHILE
THIS
IS
NOT
TRUE
FOR
SHORT
PROBING
A
FRAME
GRABBER
FOR
EXAMPLE
REQUIRES
A
DELAY
OF
AT
LEAST
MS
WHICH
IS
AGES
FOR
THE
PROCES
SOR
AND
OTHER
DEVICES
MIGHT
TAKE
EVEN
LONGER
THEREFORE
IT
BEST
TO
PROBE
FOR
THE
INTERRUPT
LINE
ONLY
ONCE
AT
MODULE
INITIALIZATION
INDEPENDENTLY
OF
WHETHER
YOU
INSTALL
THE
HANDLER
AT
DEVICE
OPEN
AS
YOU
SHOULD
OR
WITHIN
THE
INITIALIZATION
FUNC
TION
WHICH
IS
NOT
RECOMMENDED
IT
INTERESTING
TO
NOTE
THAT
ON
SOME
PLATFORMS
POWERPC
MOST
MIPS
IMPLE
MENTATIONS
AND
BOTH
SPARC
VERSIONS
PROBING
IS
UNNECESSARY
AND
THEREFORE
THE
PREVIOUS
FUNCTIONS
ARE
JUST
EMPTY
PLACEHOLDERS
SOMETIMES
CALLED
USELESS
ISA
NON
SENSE
ON
OTHER
PLATFORMS
PROBING
IS
IMPLEMENTED
ONLY
FOR
ISA
DEVICES
ANYWAY
MOST
ARCHITECTURES
DEFINE
THE
FUNCTIONS
EVEN
IF
THEY
ARE
EMPTY
TO
EASE
PORTING
EXIST
ING
DEVICE
DRIVERS
DO
IT
YOURSELF
PROBING
PROBING
CAN
ALSO
BE
IMPLEMENTED
IN
THE
DRIVER
ITSELF
WITHOUT
TOO
MUCH
TROUBLE
IT
IS
A
RARE
DRIVER
THAT
MUST
IMPLEMENT
ITS
OWN
PROBING
BUT
SEEING
HOW
IT
WORKS
GIVES
SOME
INSIGHT
INTO
THE
PROCESS
TO
THAT
END
THE
SHORT
MODULE
PERFORMS
DO
IT
YOURSELF
DETEC
TION
OF
THE
IRQ
LINE
IF
IT
IS
LOADED
WITH
PROBE
THE
MECHANISM
IS
THE
SAME
AS
THE
ONE
DESCRIBED
EARLIER
ENABLE
ALL
UNUSED
INTER
RUPTS
THEN
WAIT
AND
SEE
WHAT
HAPPENS
WE
CAN
HOWEVER
EXPLOIT
OUR
KNOWLEDGE
OF
THE
DEVICE
OFTEN
A
DEVICE
CAN
BE
CONFIGURED
TO
USE
ONE
IRQ
NUMBER
FROM
A
SET
OF
THREE
OR
FOUR
PROBING
JUST
THOSE
IRQS
ENABLES
US
TO
DETECT
THE
RIGHT
ONE
WITHOUT
HAVING
TO
TEST
FOR
ALL
POSSIBLE
IRQS
THE
SHORT
IMPLEMENTATION
ASSUMES
THAT
AND
ARE
THE
ONLY
POSSIBLE
IRQ
VAL
UES
THESE
NUMBERS
ARE
ACTUALLY
THE
VALUES
THAT
SOME
PARALLEL
DEVICES
ALLOW
YOU
TO
SELECT
THE
FOLLOWING
CODE
PROBES
BY
TESTING
ALL
POSSIBLE
INTERRUPTS
AND
LOOKING
AT
WHAT
HAPPENS
THE
TRIALS
ARRAY
LISTS
THE
IRQS
TO
TRY
AND
HAS
AS
THE
END
MARKER
THE
TRIED
ARRAY
IS
USED
TO
KEEP
TRACK
OF
WHICH
HANDLERS
HAVE
ACTUALLY
BEEN
REGISTERED
BY
THIS
DRIVER
INT
TRIALS
INT
TRIED
INT
I
COUNT
INSTALL
THE
PROBING
HANDLER
FOR
ALL
POSSIBLE
LINES
REMEMBER
THE
RESULT
FOR
SUCCESS
OR
EBUSY
IN
ORDER
TO
ONLY
FREE
WHAT
HAS
BEEN
ACQUIRED
FOR
I
TRIALS
I
I
TRIED
I
TRIALS
I
SHORT
PROBE
NULL
DO
NONE
GOT
YET
ENABLE
TOGGLE
THE
BIT
DISABLE
UDELAY
GIVE
IT
SOME
TIME
THE
VALUE
HAS
BEEN
SET
BY
THE
HANDLER
IF
NONE
OF
THEM
PRINTK
SHORT
NO
IRQ
REPORTED
BY
PROBE
N
IF
MORE
THAN
ONE
LINE
HAS
BEEN
ACTIVATED
THE
RESULT
IS
NEGATIVE
WE
SHOULD
SERVICE
THE
INTERRUPT
BUT
THE
LPT
PORT
DOESN
T
NEED
IT
AND
LOOP
OVER
AGAIN
DO
IT
AT
MOST
TIMES
WHILE
COUNT
END
OF
LOOP
UNINSTALL
THE
HANDLER
FOR
I
TRIALS
I
I
IF
TRIED
I
TRIALS
I
NULL
IF
PRINTK
SHORT
PROBE
FAILED
I
TIMES
GIVING
UP
N
COUNT
YOU
MIGHT
NOT
KNOW
IN
ADVANCE
WHAT
THE
POSSIBLE
IRQ
VALUES
ARE
IN
THAT
CASE
YOU
NEED
TO
PROBE
ALL
THE
FREE
INTERRUPTS
INSTEAD
OF
LIMITING
YOURSELF
TO
A
FEW
TRIALS
TO
PROBE
FOR
ALL
INTERRUPTS
YOU
HAVE
TO
PROBE
FROM
IRQ
TO
IRQ
WHERE
IS
DEFINED
IN
ASM
IRQ
H
AND
IS
PLATFORM
DEPENDENT
NOW
WE
ARE
MISSING
ONLY
THE
PROBING
HANDLER
ITSELF
THE
HANDLER
ROLE
IS
TO
UPDATE
ACCORDING
TO
WHICH
INTERRUPTS
ARE
ACTUALLY
RECEIVED
A
VALUE
IN
MEANS
NOTHING
YET
WHILE
A
NEGATIVE
VALUE
MEANS
AMBIGUOUS
THESE
VALUES
WERE
CHOSEN
TO
BE
CONSISTENT
WITH
AND
TO
ALLOW
THE
SAME
CODE
TO
CALL
EITHER
KIND
OF
PROBING
WITHIN
SHORT
C
INT
IRQ
VOID
STRUCT
REGS
IF
IRQ
FOUND
IF
IRQ
IRQ
AMBIGUOUS
RETURN
THE
ARGUMENTS
TO
THE
HANDLER
ARE
DESCRIBED
LATER
KNOWING
THAT
IRQ
IS
THE
INTERRUPT
BEING
HANDLED
SHOULD
BE
SUFFICIENT
TO
UNDERSTAND
THE
FUNCTION
JUST
SHOWN
FAST
AND
SLOW
HANDLERS
OLDER
VERSIONS
OF
THE
LINUX
KERNEL
TOOK
GREAT
PAINS
TO
DISTINGUISH
BETWEEN
FAST
AND
SLOW
INTERRUPTS
FAST
INTERRUPTS
WERE
THOSE
THAT
COULD
BE
HANDLED
VERY
QUICKLY
WHEREAS
HANDLING
SLOW
INTERRUPTS
TOOK
SIGNIFICANTLY
LONGER
SLOW
INTERRUPTS
COULD
BE
SUFFICIENTLY
DEMANDING
OF
THE
PROCESSOR
AND
IT
WAS
WORTHWHILE
TO
REENABLE
INTER
RUPTS
WHILE
THEY
WERE
BEING
HANDLED
OTHERWISE
TASKS
REQUIRING
QUICK
ATTENTION
COULD
BE
DELAYED
FOR
TOO
LONG
IN
MODERN
KERNELS
MOST
OF
THE
DIFFERENCES
BETWEEN
FAST
AND
SLOW
INTERRUPTS
HAVE
DIS
APPEARED
THERE
REMAINS
ONLY
ONE
FAST
INTERRUPTS
THOSE
THAT
WERE
REQUESTED
WITH
THE
FLAG
ARE
EXECUTED
WITH
ALL
OTHER
INTERRUPTS
DISABLED
ON
THE
CURRENT
PROCESSOR
NOTE
THAT
OTHER
PROCESSORS
CAN
STILL
HANDLE
INTERRUPTS
ALTHOUGH
YOU
WILL
NEVER
SEE
TWO
PROCESSORS
HANDLING
THE
SAME
IRQ
AT
THE
SAME
TIME
SO
WHICH
TYPE
OF
INTERRUPT
SHOULD
YOUR
DRIVER
USE
ON
MODERN
SYSTEMS
IS
INTENDED
ONLY
FOR
USE
IN
A
FEW
SPECIFIC
SITUATIONS
SUCH
AS
TIMER
INTERRUPTS
UNLESS
YOU
HAVE
A
STRONG
REASON
TO
RUN
YOUR
INTERRUPT
HANDLER
WITH
OTHER
INTERRUPTS
DISABLED
YOU
SHOULD
NOT
USE
THIS
DESCRIPTION
SHOULD
SATISFY
MOST
READERS
ALTHOUGH
SOMEONE
WITH
A
TASTE
FOR
HARD
WARE
AND
SOME
EXPERIENCE
WITH
HER
COMPUTER
MIGHT
BE
INTERESTED
IN
GOING
DEEPER
IF
YOU
DON
T
CARE
ABOUT
THE
INTERNAL
DETAILS
YOU
CAN
SKIP
TO
THE
NEXT
SECTION
THE
INTERNALS
OF
INTERRUPT
HANDLING
ON
THE
THIS
DESCRIPTION
HAS
BEEN
EXTRAPOLATED
FROM
ARCH
KERNEL
IRQ
C
ARCH
KERNEL
APIC
C
ARCH
KERNEL
ENTRY
ARCH
KERNEL
C
AND
INCLUDE
ASM
H
AS
THEY
APPEAR
IN
THE
KERNELS
ALTHOUGH
THE
GENERAL
CONCEPTS
REMAIN
THE
SAME
THE
HARDWARE
DETAILS
DIFFER
ON
OTHER
PLATFORMS
THE
LOWEST
LEVEL
OF
INTERRUPT
HANDLING
CAN
BE
FOUND
IN
ENTRY
AN
ASSEMBLY
LANGUAGE
FILE
THAT
HANDLES
MUCH
OF
THE
MACHINE
LEVEL
WORK
BY
WAY
OF
A
BIT
OF
ASSEMBLER
TRICK
ERY
AND
SOME
MACROS
A
BIT
OF
CODE
IS
ASSIGNED
TO
EVERY
POSSIBLE
INTERRUPT
IN
EACH
CASE
THE
CODE
PUSHES
THE
INTERRUPT
NUMBER
ON
THE
STACK
AND
JUMPS
TO
A
COMMON
SEGMENT
WHICH
CALLS
DEFINED
IN
IRQ
C
THE
FIRST
THING
DOES
IS
TO
ACKNOWLEDGE
THE
INTERRUPT
SO
THAT
THE
INTERRUPT
CON
TROLLER
CAN
GO
ON
TO
OTHER
THINGS
IT
THEN
OBTAINS
A
SPINLOCK
FOR
THE
GIVEN
IRQ
NUMBER
THUS
PREVENTING
ANY
OTHER
CPU
FROM
HANDLING
THIS
IRQ
IT
CLEARS
A
COUPLE
OF
STATUS
BITS
INCLUDING
ONE
CALLED
THAT
WE
LL
LOOKAT
SHORTLY
AND
THEN
LOOKS
UP
THE
HANDLER
FOR
THIS
PARTICULAR
IRQ
IF
THERE
IS
NO
HANDLER
THERE
NOTHING
TO
DO
THE
SPINLOCK
IS
RELEASED
ANY
PENDING
SOFTWARE
INTERRUPTS
ARE
HANDLED
AND
RETURNS
USUALLY
HOWEVER
IF
A
DEVICE
IS
INTERRUPTING
THERE
IS
AT
LEAST
ONE
HANDLER
REGISTERED
FOR
ITS
IRQ
AS
WELL
THE
FUNCTION
IS
CALLED
TO
ACTUALLY
INVOKE
THE
HANDLERS
IF
THE
HANDLER
IS
OF
THE
SLOW
VARIETY
IS
NOT
SET
INTERRUPTS
ARE
REENABLED
IN
THE
HARDWARE
AND
THE
HANDLER
IS
INVOKED
THEN
IT
JUST
A
MATTER
OF
CLEANING
UP
RUNNING
SOFTWARE
INTERRUPTS
AND
GETTING
BACK
TO
REGULAR
WORK
THE
REG
ULAR
WORK
MAY
WELL
HAVE
CHANGED
AS
A
RESULT
OF
AN
INTERRUPT
THE
HANDLER
COULD
A
PROCESS
FOR
EXAMPLE
SO
THE
LAST
THING
THAT
HAPPENS
ON
RETURN
FROM
AN
INTERRUPT
IS
A
POSSIBLE
RESCHEDULING
OF
THE
PROCESSOR
PROBING
FOR
IRQS
IS
DONE
BY
SETTING
THE
STATUS
BIT
FOR
EACH
IRQ
THAT
CUR
RENTLY
LACKS
A
HANDLER
WHEN
THE
INTERRUPT
HAPPENS
CLEARS
THAT
BIT
AND
THEN
RETURNS
BECAUSE
NO
HANDLER
IS
REGISTERED
WHEN
CALLED
BY
A
DRIVER
NEEDS
TO
SEARCH
FOR
ONLY
THE
IRQ
THAT
NO
LONGER
HAS
SET
IMPLEMENTING
A
HANDLER
SO
FAR
WE
VE
LEARNED
TO
REGISTER
AN
INTERRUPT
HANDLER
BUT
NOT
TO
WRITE
ONE
ACTUALLY
THERE
NOTHING
UNUSUAL
ABOUT
A
HANDLER
IT
ORDINARY
C
CODE
THE
ONLY
PECULIARITY
IS
THAT
A
HANDLER
RUNS
AT
INTERRUPT
TIME
AND
THEREFORE
SUFFERS
SOME
RESTRICTIONS
ON
WHAT
IT
CAN
DO
THESE
RESTRICTIONS
ARE
THE
SAME
AS
THOSE
WE
SAW
WITH
KERNEL
TIMERS
A
HANDLER
CAN
T
TRANSFER
DATA
TO
OR
FROM
USER
SPACE
BECAUSE
IT
DOESN
T
EXECUTE
IN
THE
CONTEXT
OF
A
PROCESS
HANDLERS
ALSO
CANNOT
DO
ANYTHING
THAT
WOULD
SLEEP
SUCH
AS
CALLING
ALLOCATING
MEMORY
WITH
ANYTHING
OTHER
THAN
OR
LOCKING
A
SEMAPHORE
FINALLY
HANDLERS
CANNOT
CALL
SCHEDULE
THE
ROLE
OF
AN
INTERRUPT
HANDLER
IS
TO
GIVE
FEEDBACK
TO
ITS
DEVICE
ABOUT
INTERRUPT
RECEPTION
AND
TO
READ
OR
WRITE
DATA
ACCORDING
TO
THE
MEANING
OF
THE
INTERRUPT
BEING
SERVICED
THE
FIRST
STEP
USUALLY
CONSISTS
OF
CLEARING
A
BIT
ON
THE
INTERFACE
BOARD
MOST
HARDWARE
DEVICES
WON
T
GENERATE
OTHER
INTERRUPTS
UNTIL
THEIR
INTERRUPT
PENDING
BIT
HAS
BEEN
CLEARED
DEPENDING
ON
HOW
YOUR
HARDWARE
WORKS
THIS
STEP
MAY
NEED
TO
BE
PERFORMED
LAST
INSTEAD
OF
FIRST
THERE
IS
NO
CATCH
ALL
RULE
HERE
SOME
DEVICES
DON
T
REQUIRE
THIS
STEP
BECAUSE
THEY
DON
T
HAVE
AN
INTERRUPT
PENDING
BIT
SUCH
DEVICES
ARE
A
MINORITY
ALTHOUGH
THE
PARALLEL
PORT
IS
ONE
OF
THEM
FOR
THAT
REASON
SHORT
DOES
NOT
HAVE
TO
CLEAR
SUCH
A
BIT
A
TYPICAL
TASK
FOR
AN
INTERRUPT
HANDLER
IS
AWAKENING
PROCESSES
SLEEPING
ON
THE
DEVICE
IF
THE
INTERRUPT
SIGNALS
THE
EVENT
THEY
RE
WAITING
FOR
SUCH
AS
THE
ARRIVAL
OF
NEW
DATA
TO
STICK
WITH
THE
FRAME
GRABBER
EXAMPLE
A
PROCESS
COULD
ACQUIRE
A
SEQUENCE
OF
IMAGES
BY
CONTINUOUSLY
READING
THE
DEVICE
THE
READ
CALL
BLOCKS
BEFORE
READING
EACH
FRAME
WHILE
THE
INTERRUPT
HANDLER
AWAKENS
THE
PROCESS
AS
SOON
AS
EACH
NEW
FRAME
ARRIVES
THIS
ASSUMES
THAT
THE
GRABBER
INTERRUPTS
THE
PROCESSOR
TO
SIGNAL
SUCCESSFUL
ARRIVAL
OF
EACH
NEW
FRAME
THE
PROGRAMMER
SHOULD
BE
CAREFUL
TO
WRITE
A
ROUTINE
THAT
EXECUTES
IN
A
MINIMUM
AMOUNT
OF
TIME
INDEPENDENT
OF
ITS
BEING
A
FAST
OR
SLOW
HANDLER
IF
A
LONG
COMPUTA
TION
NEEDS
TO
BE
PERFORMED
THE
BEST
APPROACH
IS
TO
USE
A
TASKLET
OR
WORKQUEUE
TO
SCHEDULE
COMPUTATION
AT
A
SAFER
TIME
WE
LL
LOOK
AT
HOW
WORK
CAN
BE
DEFERRED
IN
THIS
MANNER
IN
THE
SECTION
TOP
AND
BOTTOM
HALVES
OUR
SAMPLE
CODE
IN
SHORT
RESPONDS
TO
THE
INTERRUPT
BY
CALLING
AND
PRINTING
THE
CURRENT
TIME
INTO
A
PAGE
SIZED
CIRCULAR
BUFFER
IT
THEN
AWAKENS
ANY
READ
ING
PROCESS
BECAUSE
THERE
IS
NOW
DATA
AVAILABLE
TO
BE
READ
INT
IRQ
VOID
STRUCT
REGS
STRUCT
TIMEVAL
TV
INT
WRITTEN
TV
WRITE
A
BYTE
RECORD
ASSUME
IS
A
MULTIPLE
OF
WRITTEN
SPRINTF
CHAR
N
INT
TV
INT
TV
WRITTEN
WRITTEN
AWAKE
ANY
READING
PROCESS
RETURN
THIS
CODE
THOUGH
SIMPLE
REPRESENTS
THE
TYPICAL
JOB
OF
AN
INTERRUPT
HANDLER
IT
IN
TURN
CALLS
WHICH
IS
DEFINED
AS
FOLLOWS
STATIC
INLINE
VOID
VOLATILE
UNSIGNED
LONG
INDEX
INT
DELTA
UNSIGNED
LONG
NEW
INDEX
DELTA
BARRIER
DON
T
OPTIMIZE
THESE
TWO
TOGETHER
INDEX
NEW
NEW
THIS
FUNCTION
HAS
BEEN
CAREFULLY
WRITTEN
TO
WRAP
A
POINTER
INTO
THE
CIRCULAR
BUFFER
WITHOUT
EVER
EXPOSING
AN
INCORRECT
VALUE
THE
BARRIER
CALL
IS
THERE
TO
BLOCK
COMPILER
OPTIMIZATIONS
ACROSS
THE
OTHER
TWO
LINES
OF
THE
FUNCTION
WITHOUT
THE
BARRIER
THE
COMPILER
MIGHT
DECIDE
TO
OPTIMIZE
OUT
THE
NEW
VARIABLE
AND
ASSIGN
DIRECTLY
TO
INDEX
THAT
OPTIMIZATION
COULD
EXPOSE
AN
INCORRECT
VALUE
OF
THE
INDEX
FOR
A
BRIEF
PERIOD
IN
THE
CASE
WHERE
IT
WRAPS
BY
TAKING
CARE
TO
PREVENT
IN
INCONSISTENT
VALUE
FROM
EVER
BEING
VISIBLE
TO
OTHER
THREADS
WE
CAN
MANIPULATE
THE
CIRCULAR
BUFFER
POINTERS
SAFELY
WITHOUT
LOCKS
THE
DEVICE
FILE
USED
TO
READ
THE
BUFFER
BEING
FILLED
AT
INTERRUPT
TIME
IS
DEV
SHORTINT
THIS
DEVICE
SPECIAL
FILE
TOGETHER
WITH
DEV
SHORTPRINT
WASN
T
INTRODUCED
IN
CHAPTER
BECAUSE
ITS
USE
IS
SPECIFIC
TO
INTERRUPT
HANDLING
THE
INTERNALS
OF
DEV
SHORTINT
ARE
SPECIFICALLY
TAILORED
FOR
INTERRUPT
GENERATION
AND
REPORTING
WRITING
TO
THE
DEVICE
GENERATES
ONE
INTERRUPT
EVERY
OTHER
BYTE
READING
THE
DEVICE
GIVES
THE
TIME
WHEN
EACH
INTERRUPT
WAS
REPORTED
IF
YOU
CONNECT
TOGETHER
PINS
AND
OF
THE
PARALLEL
CONNECTOR
YOU
CAN
GENERATE
INTERRUPTS
BY
RAISING
THE
HIGH
BIT
OF
THE
PARALLEL
DATA
BYTE
THIS
CAN
BE
ACCOMPLISHED
BY
WRITING
BINARY
DATA
TO
DEV
OR
BY
WRITING
ANYTHING
TO
DEV
SHORTINT
THE
FOLLOWING
CODE
IMPLEMENTS
READ
AND
WRITE
FOR
DEV
SHORTINT
STRUCT
FILE
FILP
CHAR
BUF
COUNT
INT
WAIT
WHILE
WAIT
IF
SCHEDULE
WAIT
IF
CURRENT
A
SIGNAL
ARRIVED
RETURN
ERESTARTSYS
TELL
THE
FS
LAYER
TO
HANDLE
IT
IS
THE
NUMBER
OF
READABLE
DATA
BYTES
IF
WRAPPED
IF
COUNT
COUNT
IF
BUF
CHAR
COUNT
RETURN
EFAULT
COUNT
RETURN
COUNT
STRUCT
FILE
FILP
CONST
CHAR
BUF
COUNT
INT
WRITTEN
ODD
UNSIGNED
LONG
PORT
OUTPUT
TO
THE
PARALLEL
DATA
LATCH
VOID
ADDRESS
VOID
IF
WHILE
WRITTEN
COUNT
WRITTEN
ODD
ADDRESS
ELSE
THE
SHORTINT
DEVICE
ACCOMPLISHES
ITS
TASK
BY
ALTERNATELY
WRITING
AND
TO
THE
PARALLEL
PORT
WHILE
WRITTEN
COUNT
OUTB
WRITTEN
ODD
PORT
COUNT
RETURN
WRITTEN
THE
OTHER
DEVICE
SPECIAL
FILE
DEV
SHORTPRINT
USES
THE
PARALLEL
PORT
TO
DRIVE
A
PRINTER
YOU
CAN
USE
IT
IF
YOU
WANT
TO
AVOID
CONNECTING
PINS
AND
OF
A
D
CONNECTOR
THE
WRITE
IMPLEMENTATION
OF
SHORTPRINT
USES
A
CIRCULAR
BUFFER
TO
STORE
DATA
TO
BE
PRINTED
WHILE
THE
READ
IMPLEMENTATION
IS
THE
ONE
JUST
SHOWN
SO
YOU
CAN
READ
THE
TIME
YOUR
PRINTER
TAKES
TO
EAT
EACH
CHARACTER
IN
ORDER
TO
SUPPORT
PRINTER
OPERATION
THE
INTERRUPT
HANDLER
HAS
BEEN
SLIGHTLY
MODI
FIED
FROM
THE
ONE
JUST
SHOWN
ADDING
THE
ABILITY
TO
SEND
THE
NEXT
DATA
BYTE
TO
THE
PRINTER
IF
THERE
IS
MORE
DATA
TO
TRANSFER
HANDLER
ARGUMENTS
AND
RETURN
VALUE
THOUGH
SHORT
IGNORES
THEM
THREE
ARGUMENTS
ARE
PASSED
TO
AN
INTERRUPT
HANDLER
IRQ
AND
REGS
LET
LOOK
AT
THE
ROLE
OF
EACH
THE
INTERRUPT
NUMBER
INT
IRQ
IS
USEFUL
AS
INFORMATION
YOU
MAY
PRINT
IN
YOUR
LOG
MESSAGES
IF
ANY
THE
SECOND
ARGUMENT
VOID
IS
A
SORT
OF
CLIENT
DATA
A
VOID
ARGUMENT
IS
PASSED
TO
AND
THIS
SAME
POINTER
IS
THEN
PASSED
BACK
AS
AN
ARGUMENT
TO
THE
HANDLER
WHEN
THE
INTERRUPT
HAPPENS
YOU
USUALLY
PASS
A
POINTER
TO
YOUR
DEVICE
DATA
STRUCTURE
IN
SO
A
DRIVER
THAT
MANAGES
SEVERAL
INSTANCES
OF
THE
SAME
DEVICE
DOESN
T
NEED
ANY
EXTRA
CODE
IN
THE
INTERRUPT
HANDLER
TO
FIND
OUT
WHICH
DEVICE
IS
IN
CHARGE
OF
THE
CURRENT
INTERRUPT
EVENT
TYPICAL
USE
OF
THE
ARGUMENT
IN
AN
INTERRUPT
HANDLER
IS
AS
FOLLOWS
STATIC
INT
IRQ
VOID
STRUCT
REGS
STRUCT
DEV
NOW
DEV
POINTS
TO
THE
RIGHT
HARDWARE
ITEM
THE
TYPICAL
OPEN
CODE
ASSOCIATED
WITH
THIS
HANDLER
LOOKS
LIKE
THIS
STATIC
VOID
STRUCT
INODE
INODE
STRUCT
FILE
FILP
STRUCT
DEV
HWINFO
MINOR
INODE
DEV
IRQ
FLAGS
SAMPLE
DEV
RETURN
THE
LAST
ARGUMENT
STRUCT
REGS
IS
RARELY
USED
IT
HOLDS
A
SNAPSHOT
OF
THE
PROCESSOR
CONTEXT
BEFORE
THE
PROCESSOR
ENTERED
INTERRUPT
CODE
THE
REGISTERS
CAN
BE
USED
FOR
MONITORING
AND
DEBUGGING
THEY
ARE
NOT
NORMALLY
NEEDED
FOR
REGULAR
DEVICE
DRIVER
TASKS
INTERRUPT
HANDLERS
SHOULD
RETURN
A
VALUE
INDICATING
WHETHER
THERE
WAS
ACTUALLY
AN
INTERRUPT
TO
HANDLE
IF
THE
HANDLER
FOUND
THAT
ITS
DEVICE
DID
INDEED
NEED
ATTENTION
IT
SHOULD
RETURN
OTHERWISE
THE
RETURN
VALUE
SHOULD
BE
YOU
CAN
ALSO
GENERATE
THE
RETURN
VALUE
WITH
THIS
MACRO
HANDLED
WHERE
HANDLED
IS
NONZERO
IF
YOU
WERE
ABLE
TO
HANDLE
THE
INTERRUPT
THE
RETURN
VALUE
IS
USED
BY
THE
KERNEL
TO
DETECT
AND
SUPPRESS
SPURIOUS
INTERRUPTS
IF
YOUR
DEVICE
GIVES
YOU
NO
WAY
TO
TELL
WHETHER
IT
REALLY
INTERRUPTED
YOU
SHOULD
RETURN
ENABLING
AND
DISABLING
INTERRUPTS
THERE
ARE
TIMES
WHEN
A
DEVICE
DRIVER
MUST
BLOCK
THE
DELIVERY
OF
INTERRUPTS
FOR
A
HOPEFULLY
SHORT
PERIOD
OF
TIME
WE
SAW
ONE
SUCH
SITUATION
IN
THE
SECTION
SPIN
LOCKS
IN
CHAPTER
OFTEN
INTERRUPTS
MUST
BE
BLOCKED
WHILE
HOLDING
A
SPINLOCK
TO
AVOID
DEADLOCKING
THE
SYSTEM
THERE
ARE
WAYS
OF
DISABLING
INTERRUPTS
THAT
DO
NOT
INVOLVE
SPINLOCKS
BUT
BEFORE
WE
DISCUSS
THEM
NOTE
THAT
DISABLING
INTERRUPTS
SHOULD
BE
A
RELATIVELY
RARE
ACTIVITY
EVEN
IN
DEVICE
DRIVERS
AND
THIS
TECHNIQUE
SHOULD
NEVER
BE
USED
AS
A
MUTUAL
EXCLUSION
MECHANISM
WITHIN
A
DRIVER
DISABLING
A
SINGLE
INTERRUPT
SOMETIMES
BUT
RARELY
A
DRIVER
NEEDS
TO
DISABLE
INTERRUPT
DELIVERY
FOR
A
SPECIFIC
INTERRUPT
LINE
THE
KERNEL
OFFERS
THREE
FUNCTIONS
FOR
THIS
PURPOSE
ALL
DECLARED
IN
ASM
IRQ
H
THESE
FUNCTIONS
ARE
PART
OF
THE
KERNEL
API
SO
WE
DESCRIBE
THEM
BUT
THEIR
USE
IS
DISCOURAGED
IN
MOST
DRIVERS
AMONG
OTHER
THINGS
YOU
CANNOT
DISABLE
SHARED
INTERRUPT
LINES
AND
ON
MODERN
SYSTEMS
SHARED
INTERRUPTS
ARE
THE
NORM
THAT
SAID
HERE
THEY
ARE
VOID
INT
IRQ
VOID
INT
IRQ
VOID
INT
IRQ
CALLING
ANY
OF
THESE
FUNCTIONS
MAY
UPDATE
THE
MASK
FOR
THE
SPECIFIED
IRQ
IN
THE
PRO
GRAMMABLE
INTERRUPT
CONTROLLER
PIC
THUS
DISABLING
OR
ENABLING
THE
SPECIFIED
IRQ
ACROSS
ALL
PROCESSORS
CALLS
TO
THESE
FUNCTIONS
CAN
BE
NESTED
IF
IS
CALLED
TWICE
IN
SUCCESSION
TWO
CALLS
ARE
REQUIRED
BEFORE
THE
IRQ
IS
TRULY
REEN
ABLED
IT
IS
POSSIBLE
TO
CALL
THESE
FUNCTIONS
FROM
AN
INTERRUPT
HANDLER
BUT
ENABLING
YOUR
OWN
IRQ
WHILE
HANDLING
IT
IS
NOT
USUALLY
GOOD
PRACTICE
NOT
ONLY
DISABLES
THE
GIVEN
INTERRUPT
BUT
ALSO
WAITS
FOR
A
CURRENTLY
EXECUTING
INTERRUPT
HANDLER
IF
ANY
TO
COMPLETE
BE
AWARE
THAT
IF
THE
THREAD
CALLING
HOLDS
ANY
RESOURCES
SUCH
AS
SPINLOCKS
THAT
THE
INTERRUPT
HANDLER
NEEDS
THE
SYSTEM
CAN
DEADLOCK
DIFFERS
FROM
IN
THAT
IT
RETURNS
IMMEDI
ATELY
THUS
USING
IS
A
LITTLE
FASTER
BUT
MAY
LEAVE
YOUR
DRIVER
OPEN
TO
RACE
CONDITIONS
BUT
WHY
DISABLE
AN
INTERRUPT
STICKING
TO
THE
PARALLEL
PORT
LET
LOOK
AT
THE
PLIP
NET
WORK
INTERFACE
A
PLIP
DEVICE
USES
THE
BARE
BONES
PARALLEL
PORT
TO
TRANSFER
DATA
SINCE
ONLY
FIVE
BITS
CAN
BE
READ
FROM
THE
PARALLEL
CONNECTOR
THEY
ARE
INTERPRETED
AS
FOUR
DATA
BITS
AND
A
CLOCK
HANDSHAKE
SIGNAL
WHEN
THE
FIRST
FOUR
BITS
OF
A
PACKET
ARE
TRANS
MITTED
BY
THE
INITIATOR
THE
INTERFACE
SENDING
THE
PACKET
THE
CLOCK
LINE
IS
RAISED
CAUSING
THE
RECEIVING
INTERFACE
TO
INTERRUPT
THE
PROCESSOR
THE
PLIP
HANDLER
IS
THEN
INVOKED
TO
DEAL
WITH
NEWLY
ARRIVED
DATA
AFTER
THE
DEVICE
HAS
BEEN
ALERTED
THE
DATA
TRANSFER
PROCEEDS
USING
THE
HANDSHAKE
LINE
TO
CLOCK
NEW
DATA
TO
THE
RECEIVING
INTERFACE
THIS
MIGHT
NOT
BE
THE
BEST
IMPLE
MENTATION
BUT
IT
IS
NECESSARY
FOR
COMPATIBILITY
WITH
OTHER
PACKET
DRIVERS
USING
THE
PARALLEL
PORT
PERFORMANCE
WOULD
BE
UNBEARABLE
IF
THE
RECEIVING
INTERFACE
HAD
TO
HAN
DLE
TWO
INTERRUPTS
FOR
EVERY
BYTE
RECEIVED
THEREFORE
THE
DRIVER
DISABLES
THE
INTER
RUPT
DURING
THE
RECEPTION
OF
THE
PACKET
INSTEAD
A
POLL
AND
DELAY
LOOP
IS
USED
TO
BRING
IN
THE
DATA
SIMILARLY
BECAUSE
THE
HANDSHAKE
LINE
FROM
THE
RECEIVER
TO
THE
TRANSMITTER
IS
USED
TO
ACKNOWLEDGE
DATA
RECEPTION
THE
TRANSMITTING
INTERFACE
DISABLES
ITS
IRQ
LINE
DURING
PACKET
TRANSMISSION
DISABLING
ALL
INTERRUPTS
WHAT
IF
YOU
NEED
TO
DISABLE
ALL
INTERRUPTS
IN
THE
KERNEL
IT
IS
POSSIBLE
TO
TURN
OFF
ALL
INTERRUPT
HANDLING
ON
THE
CURRENT
PROCESSOR
WITH
EITHER
OF
THE
FOLLOWING
TWO
FUNC
TIONS
WHICH
ARE
DEFINED
IN
ASM
SYSTEM
H
VOID
UNSIGNED
LONG
FLAGS
VOID
VOID
A
CALL
TO
DISABLES
INTERRUPT
DELIVERY
ON
THE
CURRENT
PROCESSOR
AFTER
SAV
ING
THE
CURRENT
INTERRUPT
STATE
INTO
FLAGS
NOTE
THAT
FLAGS
IS
PASSED
DIRECTLY
NOT
BY
POINTER
SHUTS
OFF
LOCAL
INTERRUPT
DELIVERY
WITHOUT
SAVING
THE
STATE
YOU
SHOULD
USE
THIS
VERSION
ONLY
IF
YOU
KNOW
THAT
INTERRUPTS
HAVE
NOT
ALREADY
BEEN
DISABLED
ELSEWHERE
TURNING
INTERRUPTS
BACK
ON
IS
ACCOMPLISHED
WITH
VOID
UNSIGNED
LONG
FLAGS
VOID
VOID
THE
FIRST
VERSION
RESTORES
THAT
STATE
WHICH
WAS
STORED
INTO
FLAGS
BY
WHILE
ENABLES
INTERRUPTS
UNCONDITIONALLY
UNLIKE
DOES
NOT
KEEP
TRACK
OF
MULTIPLE
CALLS
IF
MORE
THAN
ONE
FUNCTION
IN
THE
CALL
CHAIN
MIGHT
NEED
TO
DISABLE
INTERRUPTS
SHOULD
BE
USED
IN
THE
KERNEL
THERE
IS
NO
WAY
TO
DISABLE
ALL
INTERRUPTS
GLOBALLY
ACROSS
THE
ENTIRE
SYSTEM
THE
KERNEL
DEVELOPERS
HAVE
DECIDED
THAT
THE
COST
OF
SHUTTING
OFF
ALL
INTER
RUPTS
IS
TOO
HIGH
AND
THAT
THERE
IS
NO
NEED
FOR
THAT
CAPABILITY
IN
ANY
CASE
IF
YOU
ARE
WORKING
WITH
AN
OLDER
DRIVER
THAT
MAKES
CALLS
TO
FUNCTIONS
SUCH
AS
CLI
AND
STI
YOU
NEED
TO
UPDATE
IT
TO
USE
PROPER
LOCKING
BEFORE
IT
WILL
WORK
UNDER
TOP
AND
BOTTOM
HALVES
ONE
OF
THE
MAIN
PROBLEMS
WITH
INTERRUPT
HANDLING
IS
HOW
TO
PERFORM
LENGTHY
TASKS
WITHIN
A
HANDLER
OFTEN
A
SUBSTANTIAL
AMOUNT
OF
WORK
MUST
BE
DONE
IN
RESPONSE
TO
A
DEVICE
INTERRUPT
BUT
INTERRUPT
HANDLERS
NEED
TO
FINISH
UP
QUICKLY
AND
NOT
KEEP
INTER
RUPTS
BLOCKED
FOR
LONG
THESE
TWO
NEEDS
WORK
AND
SPEED
CONFLICT
WITH
EACH
OTHER
LEAVING
THE
DRIVER
WRITER
IN
A
BIT
OF
A
BIND
LINUX
ALONG
WITH
MANY
OTHER
SYSTEMS
RESOLVES
THIS
PROBLEM
BY
SPLITTING
THE
INTER
RUPT
HANDLER
INTO
TWO
HALVES
THE
SO
CALLED
TOP
HALF
IS
THE
ROUTINE
THAT
ACTUALLY
RESPONDS
TO
THE
INTERRUPT
THE
ONE
YOU
REGISTER
WITH
THE
BOTTOM
HALF
IS
A
ROUTINE
THAT
IS
SCHEDULED
BY
THE
TOP
HALF
TO
BE
EXECUTED
LATER
AT
A
SAFER
TIME
THE
BIG
DIFFERENCE
BETWEEN
THE
TOP
HALF
HANDLER
AND
THE
BOTTOM
HALF
IS
THAT
ALL
INTERRUPTS
ARE
ENABLED
DURING
EXECUTION
OF
THE
BOTTOM
HALF
THAT
WHY
IT
RUNS
AT
A
SAFER
TIME
IN
THE
TYPICAL
SCENARIO
THE
TOP
HALF
SAVES
DEVICE
DATA
TO
A
DEVICE
SPECIFIC
BUFFER
SCHED
ULES
ITS
BOTTOM
HALF
AND
EXITS
THIS
OPERATION
IS
VERY
FAST
THE
BOTTOM
HALF
THEN
PER
FORMS
WHATEVER
OTHER
WORK
IS
REQUIRED
SUCH
AS
AWAKENING
PROCESSES
STARTING
UP
ANOTHER
I
O
OPERATION
AND
SO
ON
THIS
SETUP
PERMITS
THE
TOP
HALF
TO
SERVICE
A
NEW
INTERRUPT
WHILE
THE
BOTTOM
HALF
IS
STILL
WORKING
ALMOST
EVERY
SERIOUS
INTERRUPT
HANDLER
IS
SPLIT
THIS
WAY
FOR
INSTANCE
WHEN
A
NET
WORK
INTERFACE
REPORTS
THE
ARRIVAL
OF
A
NEW
PACKET
THE
HANDLER
JUST
RETRIEVES
THE
DATA
AND
PUSHES
IT
UP
TO
THE
PROTOCOL
LAYER
ACTUAL
PROCESSING
OF
THE
PACKET
IS
PERFORMED
IN
A
BOTTOM
HALF
THE
LINUX
KERNEL
HAS
TWO
DIFFERENT
MECHANISMS
THAT
MAY
BE
USED
TO
IMPLEMENT
BOT
TOM
HALF
PROCESSING
BOTH
OF
WHICH
WERE
INTRODUCED
IN
CHAPTER
TASKLETS
ARE
OFTEN
THE
PREFERRED
MECHANISM
FOR
BOTTOM
HALF
PROCESSING
THEY
ARE
VERY
FAST
BUT
ALL
TASKLET
CODE
MUST
BE
ATOMIC
THE
ALTERNATIVE
TO
TASKLETS
IS
WORKQUEUES
WHICH
MAY
HAVE
A
HIGHER
LATENCY
BUT
THAT
ARE
ALLOWED
TO
SLEEP
THE
FOLLOWING
DISCUSSION
WORKS
ONCE
AGAIN
WITH
THE
SHORT
DRIVER
WHEN
LOADED
WITH
A
MODULE
OPTION
SHORT
CAN
BE
TOLD
TO
DO
INTERRUPT
PROCESSING
IN
A
TOP
BOTTOM
HALF
MODE
WITH
EITHER
A
TASKLET
OR
WORKQUEUE
HANDLER
IN
THIS
CASE
THE
TOP
HALF
EXECUTES
QUICKLY
IT
SIMPLY
REMEMBERS
THE
CURRENT
TIME
AND
SCHEDULES
THE
BOTTOM
HALF
PRO
CESSING
THE
BOTTOM
HALF
IS
THEN
CHARGED
WITH
ENCODING
THIS
TIME
AND
AWAKENING
ANY
USER
PROCESSES
THAT
MAY
BE
WAITING
FOR
DATA
TASKLETS
REMEMBER
THAT
TASKLETS
ARE
A
SPECIAL
FUNCTION
THAT
MAY
BE
SCHEDULED
TO
RUN
IN
SOFT
WARE
INTERRUPT
CONTEXT
AT
A
SYSTEM
DETERMINED
SAFE
TIME
THEY
MAY
BE
SCHEDULED
TO
RUN
MULTIPLE
TIMES
BUT
TASKLET
SCHEDULING
IS
NOT
CUMULATIVE
THE
TASKLET
RUNS
ONLY
ONCE
EVEN
IF
IT
IS
REQUESTED
REPEATEDLY
BEFORE
IT
IS
LAUNCHED
NO
TASKLET
EVER
RUNS
IN
PARALLEL
WITH
ITSELF
SINCE
THEY
RUN
ONLY
ONCE
BUT
TASKLETS
CAN
RUN
IN
PARALLEL
WITH
OTHER
TASKLETS
ON
SMP
SYSTEMS
THUS
IF
YOUR
DRIVER
HAS
MULTIPLE
TASKLETS
THEY
MUST
EMPLOY
SOME
SORT
OF
LOCKING
TO
AVOID
CONFLICTING
WITH
EACH
OTHER
TASKLETS
ARE
ALSO
GUARANTEED
TO
RUN
ON
THE
SAME
CPU
AS
THE
FUNCTION
THAT
FIRST
SCHED
ULES
THEM
THEREFORE
AN
INTERRUPT
HANDLER
CAN
BE
SECURE
THAT
A
TASKLET
DOES
NOT
BEGIN
EXECUTING
BEFORE
THE
HANDLER
HAS
COMPLETED
HOWEVER
ANOTHER
INTERRUPT
CAN
CER
TAINLY
BE
DELIVERED
WHILE
THE
TASKLET
IS
RUNNING
SO
LOCKING
BETWEEN
THE
TASKLET
AND
THE
INTERRUPT
HANDLER
MAY
STILL
BE
REQUIRED
TASKLETS
MUST
BE
DECLARED
WITH
THE
MACRO
NAME
FUNCTION
DATA
NAME
IS
THE
NAME
TO
BE
GIVEN
TO
THE
TASKLET
FUNCTION
IS
THE
FUNCTION
THAT
IS
CALLED
TO
EXECUTE
THE
TASKLET
IT
TAKES
ONE
UNSIGNED
LONG
ARGUMENT
AND
RETURNS
VOID
AND
DATA
IS
AN
UNSIGNED
LONG
VALUE
TO
BE
PASSED
TO
THE
TASKLET
FUNCTION
THE
SHORT
DRIVER
DECLARES
ITS
TASKLET
AS
FOLLOWS
VOID
UNSIGNED
LONG
THE
FUNCTION
IS
USED
TO
SCHEDULE
A
TASKLET
FOR
RUNNING
IF
SHORT
IS
LOADED
WITH
TASKLET
IT
INSTALLS
A
DIFFERENT
INTERRUPT
HANDLER
THAT
SAVES
DATA
AND
SCHEDULES
THE
TASKLET
AS
FOLLOWS
INT
IRQ
VOID
STRUCT
REGS
STRUCT
TIMEVAL
CAST
TO
STOP
VOLATILE
WARNING
RECORD
THAT
AN
INTERRUPT
ARRIVED
RETURN
THE
ACTUAL
TASKLET
ROUTINE
WILL
BE
EXECUTED
SHORTLY
SO
TO
SPEAK
AT
THE
SYSTEM
CONVENIENCE
AS
MENTIONED
EARLIER
THIS
ROUTINE
PERFORMS
THE
BULK
OF
THE
WORK
OF
HANDLING
THE
INTERRUPT
IT
LOOKS
LIKE
THIS
VOID
UNSIGNED
LONG
UNUSED
INT
SAVECOUNT
WRITTEN
WE
HAVE
ALREADY
BEEN
REMOVED
FROM
THE
QUEUE
THE
BOTTOM
HALF
READS
THE
TV
ARRAY
FILLED
BY
THE
TOP
HALF
AND
PRINTS
IT
TO
THE
CIRCULAR
TEXT
BUFFER
WHICH
IS
THEN
CONSUMED
BY
READING
PROCESSES
FIRST
WRITE
THE
NUMBER
OF
INTERRUPTS
THAT
OCCURRED
BEFORE
THIS
BH
WRITTEN
SPRINTF
CHAR
BH
AFTER
N
SAVECOUNT
WRITTEN
THEN
WRITE
THE
TIME
VALUES
WRITE
EXACTLY
BYTES
AT
A
TIME
SO
IT
ALIGNS
WITH
DO
WRITTEN
SPRINTF
CHAR
N
INT
INT
WRITTEN
WHILE
AWAKE
ANY
READING
PROCESS
AMONG
OTHER
THINGS
THIS
TASKLET
MAKES
A
NOTE
OF
HOW
MANY
INTERRUPTS
HAVE
ARRIVED
SINCE
IT
WAS
LAST
CALLED
A
DEVICE
SUCH
AS
SHORT
CAN
GENERATE
A
GREAT
MANY
INTERRUPTS
IN
A
BRIEF
PERIOD
SO
IT
IS
NOT
UNCOMMON
FOR
SEVERAL
TO
ARRIVE
BEFORE
THE
BOTTOM
HALF
IS
EXECUTED
DRIVERS
MUST
ALWAYS
BE
PREPARED
FOR
THIS
POSSIBILITY
AND
MUST
BE
ABLE
TO
DETERMINE
HOW
MUCH
WORK
THERE
IS
TO
PERFORM
FROM
THE
INFORMATION
LEFT
BY
THE
TOP
HALF
WORKQUEUES
RECALL
THAT
WORKQUEUES
INVOKE
A
FUNCTION
AT
SOME
FUTURE
TIME
IN
THE
CONTEXT
OF
A
SPE
CIAL
WORKER
PROCESS
SINCE
THE
WORKQUEUE
FUNCTION
RUNS
IN
PROCESS
CONTEXT
IT
CAN
SLEEP
IF
NEED
BE
YOU
CANNOT
HOWEVER
COPY
DATA
INTO
USER
SPACE
FROM
A
WORKQUEUE
UNLESS
YOU
USE
THE
ADVANCED
TECHNIQUES
WE
DEMONSTRATE
IN
CHAPTER
THE
WORKER
PROCESS
DOES
NOT
HAVE
ACCESS
TO
ANY
OTHER
PROCESS
ADDRESS
SPACE
THE
SHORT
DRIVER
IF
LOADED
WITH
THE
WQ
OPTION
SET
TO
A
NONZERO
VALUE
USES
A
WORK
QUEUE
FOR
ITS
BOTTOM
HALF
PROCESSING
IT
USES
THE
SYSTEM
DEFAULT
WORKQUEUE
SO
THERE
IS
NO
SPECIAL
SETUP
CODE
REQUIRED
IF
YOUR
DRIVER
HAS
SPECIAL
LATENCY
REQUIREMENTS
OR
MIGHT
SLEEP
FOR
A
LONG
TIME
IN
THE
WORKQUEUE
FUNCTION
YOU
MAY
WANT
TO
CREATE
YOUR
OWN
DEDICATED
WORKQUEUE
WE
DO
NEED
A
STRUCTURE
WHICH
IS
DECLARED
AND
INITIALIZED
WITH
THE
FOLLOWING
STATIC
STRUCT
THIS
LINE
IS
IN
VOID
VOID
NULL
OUR
WORKER
FUNCTION
IS
WHICH
WE
HAVE
ALREADY
SEEN
IN
THE
PREVIOUS
SECTION
WHEN
WORKING
WITH
A
WORKQUEUE
SHORT
ESTABLISHES
YET
ANOTHER
INTERRUPT
HANDLER
THAT
LOOKS
LIKE
THIS
INT
IRQ
VOID
STRUCT
REGS
GRAB
THE
CURRENT
TIME
INFORMATION
STRUCT
TIMEVAL
QUEUE
THE
BH
DON
T
WORRY
ABOUT
MULTIPLE
ENQUEUEING
RECORD
THAT
AN
INTERRUPT
ARRIVED
RETURN
AS
YOU
CAN
SEE
THE
INTERRUPT
HANDLER
LOOKS
VERY
MUCH
LIKE
THE
TASKLET
VERSION
WITH
THE
EXCEPTION
THAT
IT
CALLS
TO
ARRANGE
THE
BOTTOM
HALF
PROCESSING
INTERRUPT
SHARING
THE
NOTION
OF
AN
IRQ
CONFLICT
IS
ALMOST
SYNONYMOUS
WITH
THE
PC
ARCHITECTURE
IN
THE
PAST
IRQ
LINES
ON
THE
PC
HAVE
NOT
BEEN
ABLE
TO
SERVE
MORE
THAN
ONE
DEVICE
AND
THERE
HAVE
NEVER
BEEN
ENOUGH
OF
THEM
AS
A
RESULT
FRUSTRATED
USERS
HAVE
OFTEN
SPENT
MUCH
TIME
WITH
THEIR
COMPUTER
CASE
OPEN
TRYING
TO
FIND
A
WAY
TO
MAKE
ALL
OF
THEIR
PERIPHERALS
PLAY
WELL
TOGETHER
MODERN
HARDWARE
OF
COURSE
HAS
BEEN
DESIGNED
TO
ALLOW
THE
SHARING
OF
INTERRUPTS
THE
PCI
BUS
REQUIRES
IT
THEREFORE
THE
LINUX
KERNEL
SUPPORTS
INTERRUPT
SHARING
ON
ALL
BUSES
EVEN
THOSE
SUCH
AS
THE
ISA
BUS
WHERE
SHARING
HAS
TRADITIONALLY
NOT
BEEN
SUP
PORTED
DEVICE
DRIVERS
FOR
THE
KERNEL
SHOULD
BE
WRITTEN
TO
WORK
WITH
SHARED
INTER
RUPTS
IF
THE
TARGET
HARDWARE
CAN
SUPPORT
THAT
MODE
OF
OPERATION
FORTUNATELY
WORKING
WITH
SHARED
INTERRUPTS
IS
EASY
MOST
OF
THE
TIME
INSTALLING
A
SHARED
HANDLER
SHARED
INTERRUPTS
ARE
INSTALLED
THROUGH
JUST
LIKE
NONSHARED
ONES
BUT
THERE
ARE
TWO
DIFFERENCES
THE
BIT
MUST
BE
SPECIFIED
IN
THE
FLAGS
ARGUMENT
WHEN
REQUESTING
THE
INTERRUPT
THE
ARGUMENT
MUST
BE
UNIQUE
ANY
POINTER
INTO
THE
MODULE
ADDRESS
SPACE
WILL
DO
BUT
DEFINITELY
CANNOT
BE
SET
TO
NULL
THE
KERNEL
KEEPS
A
LIST
OF
SHARED
HANDLERS
ASSOCIATED
WITH
THE
INTERRUPT
AND
CAN
BE
THOUGHT
OF
AS
THE
SIGNATURE
THAT
DIFFERENTIATES
BETWEEN
THEM
IF
TWO
DRIVERS
WERE
TO
REGISTER
NULL
AS
THEIR
SIGNATURE
ON
THE
SAME
INTERRUPT
THINGS
MIGHT
GET
MIXED
UP
AT
UNLOAD
TIME
CAUSING
THE
KERNEL
TO
OOPS
WHEN
AN
INTERRUPT
ARRIVED
FOR
THIS
REA
SON
MODERN
KERNELS
COMPLAIN
LOUDLY
IF
PASSED
A
NULL
WHEN
REGISTERING
SHARED
INTERRUPTS
WHEN
A
SHARED
INTERRUPT
IS
REQUESTED
SUCCEEDS
IF
ONE
OF
THE
FOLLOWING
IS
TRUE
THE
INTERRUPT
LINE
IS
FREE
ALL
HANDLERS
ALREADY
REGISTERED
FOR
THAT
LINE
HAVE
ALSO
SPECIFIED
THAT
THE
IRQ
IS
TO
BE
SHARED
WHENEVER
TWO
OR
MORE
DRIVERS
ARE
SHARING
AN
INTERRUPT
LINE
AND
THE
HARDWARE
INTER
RUPTS
THE
PROCESSOR
ON
THAT
LINE
THE
KERNEL
INVOKES
EVERY
HANDLER
REGISTERED
FOR
THAT
INTERRUPT
PASSING
EACH
ITS
OWN
THEREFORE
A
SHARED
HANDLER
MUST
BE
ABLE
TO
RECOGNIZE
ITS
OWN
INTERRUPTS
AND
SHOULD
QUICKLY
EXIT
WHEN
ITS
OWN
DEVICE
HAS
NOT
INTERRUPTED
BE
SURE
TO
RETURN
WHENEVER
YOUR
HANDLER
IS
CALLED
AND
FINDS
THAT
THE
DEVICE
IS
NOT
INTERRUPTING
IF
YOU
NEED
TO
PROBE
FOR
YOUR
DEVICE
BEFORE
REQUESTING
THE
IRQ
LINE
THE
KERNEL
CAN
T
HELP
YOU
NO
PROBING
FUNCTION
IS
AVAILABLE
FOR
SHARED
HANDLERS
THE
STANDARD
PROB
ING
MECHANISM
WORKS
IF
THE
LINE
BEING
USED
IS
FREE
BUT
IF
THE
LINE
IS
ALREADY
HELD
BY
ANOTHER
DRIVER
WITH
SHARING
CAPABILITIES
THE
PROBE
FAILS
EVEN
IF
YOUR
DRIVER
WOULD
HAVE
WORKED
PERFECTLY
FORTUNATELY
MOST
HARDWARE
DESIGNED
FOR
INTERRUPT
SHARING
IS
ALSO
ABLE
TO
TELL
THE
PROCESSOR
WHICH
INTERRUPT
IT
IS
USING
THUS
ELIMINATING
THE
NEED
FOR
EXPLICIT
PROBING
RELEASING
THE
HANDLER
IS
PERFORMED
IN
THE
NORMAL
WAY
USING
HERE
THE
ARGUMENT
IS
USED
TO
SELECT
THE
CORRECT
HANDLER
TO
RELEASE
FROM
THE
LIST
OF
SHARED
HAN
DLERS
FOR
THE
INTERRUPT
THAT
WHY
THE
POINTER
MUST
BE
UNIQUE
A
DRIVER
USING
A
SHARED
HANDLER
NEEDS
TO
BE
CAREFUL
ABOUT
ONE
MORE
THING
IT
CAN
T
PLAY
WITH
OR
IF
IT
DOES
THINGS
MIGHT
GO
HAYWIRE
FOR
OTHER
DEVICES
SHARING
THE
LINE
DISABLING
ANOTHER
DEVICE
INTERRUPTS
FOR
EVEN
A
SHORT
TIME
MAY
CREATE
LATENCIES
THAT
ARE
PROBLEMATIC
FOR
THAT
DEVICE
AND
IT
USER
GENERALLY
THE
PROGRAMMER
MUST
REMEMBER
THAT
HIS
DRIVER
DOESN
T
OWN
THE
IRQ
AND
ITS
BEHAVIOR
SHOULD
BE
MORE
SOCIAL
THAN
IS
NECESSARY
IF
ONE
OWNS
THE
INTERRUPT
LINE
RUNNING
THE
HANDLER
AS
SUGGESTED
EARLIER
WHEN
THE
KERNEL
RECEIVES
AN
INTERRUPT
ALL
THE
REGISTERED
HAN
DLERS
ARE
INVOKED
A
SHARED
HANDLER
MUST
BE
ABLE
TO
DISTINGUISH
BETWEEN
INTERRUPTS
THAT
IT
NEEDS
TO
HANDLE
AND
INTERRUPTS
GENERATED
BY
OTHER
DEVICES
LOADING
SHORT
WITH
THE
OPTION
SHARED
INSTALLS
THE
FOLLOWING
HANDLER
INSTEAD
OF
THE
DEFAULT
INT
IRQ
VOID
STRUCT
REGS
INT
VALUE
WRITTEN
STRUCT
TIMEVAL
TV
IF
IT
WASN
T
SHORT
RETURN
IMMEDIATELY
VALUE
INB
IF
VALUE
RETURN
CLEAR
THE
INTERRUPTING
BIT
OUTB
VALUE
THE
REST
IS
UNCHANGED
TV
WRITTEN
SPRINTF
CHAR
N
INT
TV
INT
TV
WRITTEN
AWAKE
ANY
READING
PROCESS
RETURN
AN
EXPLANATION
IS
DUE
HERE
SINCE
THE
PARALLEL
PORT
HAS
NO
INTERRUPT
PENDING
BIT
TO
CHECK
THE
HANDLER
USES
THE
ACK
BIT
FOR
THIS
PURPOSE
IF
THE
BIT
IS
HIGH
THE
INTERRUPT
BEING
REPORTED
IS
FOR
SHORT
AND
THE
HANDLER
CLEARS
THE
BIT
THE
HANDLER
RESETS
THE
BIT
BY
ZEROING
THE
HIGH
BIT
OF
THE
PARALLEL
INTERFACE
DATA
PORT
SHORT
ASSUMES
THAT
PINS
AND
ARE
CONNECTED
TOGETHER
IF
ONE
OF
THE
OTHER
DEVICES
SHARING
THE
IRQ
WITH
SHORT
GENERATES
AN
INTERRUPT
SHORT
SEES
THAT
ITS
OWN
LINE
IS
STILL
INACTIVE
AND
DOES
NOTHING
A
FULL
FEATURED
DRIVER
PROBABLY
SPLITS
THE
WORK
INTO
TOP
AND
BOTTOM
HALVES
OF
COURSE
BUT
THAT
EASY
TO
ADD
AND
DOES
NOT
HAVE
ANY
IMPACT
ON
THE
CODE
THAT
IMPLEMENTS
SHARING
A
REAL
DRIVER
WOULD
ALSO
LIKELY
USE
THE
ARGUMENT
TO
DETERMINE
WHICH
OF
POSSIBLY
MANY
DEVICES
MIGHT
BE
INTERRUPTING
NOTE
THAT
IF
YOU
ARE
USING
A
PRINTER
INSTEAD
OF
THE
JUMPER
WIRE
TO
TEST
INTERRUPT
MAN
AGEMENT
WITH
SHORT
THIS
SHARED
HANDLER
WON
T
WORK
AS
ADVERTISED
BECAUSE
THE
PRINTER
PROTOCOL
DOESN
T
ALLOW
FOR
SHARING
AND
THE
DRIVER
CAN
T
KNOW
WHETHER
THE
INTERRUPT
WAS
FROM
THE
PRINTER
THE
PROC
INTERFACE
AND
SHARED
INTERRUPTS
INSTALLING
SHARED
HANDLERS
IN
THE
SYSTEM
DOESN
T
AFFECT
PROC
STAT
WHICH
DOESN
T
EVEN
KNOW
ABOUT
HANDLERS
HOWEVER
PROC
INTERRUPTS
CHANGES
SLIGHTLY
ALL
THE
HANDLERS
INSTALLED
FOR
THE
SAME
INTERRUPT
NUMBER
APPEAR
ON
THE
SAME
LINE
OF
PROC
INTERRUPTS
THE
FOLLOWING
OUTPUT
FROM
AN
SYSTEM
SHOWS
HOW
SHARED
INTERRUPT
HANDLERS
ARE
DISPLAYED
XT
PIC
TIMER
XT
PIC
XT
PIC
CASCADE
XT
PIC
LIBATA
XT
PIC
RTC
XT
PIC
ACPI
XT
PIC
SYSKONNECT
SK
XT
PIC
XT
PIC
XT
PIC
XT
PIC
NMI
LOC
ERR
MIS
THIS
SYSTEM
HAS
SEVERAL
SHARED
INTERRUPT
LINES
IRQ
IS
USED
FOR
THE
SERIAL
ATA
AND
IEEE
CONTROLLERS
IRQ
HAS
SEVERAL
DEVICES
INCLUDING
AN
IDE
CONTROLLER
TWO
USB
CONTROLLERS
AN
ETHERNET
INTERFACE
AND
A
SOUND
CARD
AND
IRQ
ALSO
IS
USED
BY
TWO
USB
CONTROLLERS
INTERRUPT
DRIVEN
I
O
WHENEVER
A
DATA
TRANSFER
TO
OR
FROM
THE
MANAGED
HARDWARE
MIGHT
BE
DELAYED
FOR
ANY
REASON
THE
DRIVER
WRITER
SHOULD
IMPLEMENT
BUFFERING
DATA
BUFFERS
HELP
TO
DETACH
DATA
TRANSMISSION
AND
RECEPTION
FROM
THE
WRITE
AND
READ
SYSTEM
CALLS
AND
OVERALL
SYS
TEM
PERFORMANCE
BENEFITS
A
GOOD
BUFFERING
MECHANISM
LEADS
TO
INTERRUPT
DRIVEN
I
O
IN
WHICH
AN
INPUT
BUFFER
IS
FILLED
AT
INTERRUPT
TIME
AND
IS
EMPTIED
BY
PROCESSES
THAT
READ
THE
DEVICE
AN
OUTPUT
BUFFER
IS
FILLED
BY
PROCESSES
THAT
WRITE
TO
THE
DEVICE
AND
IS
EMPTIED
AT
INTERRUPT
TIME
AN
EXAMPLE
OF
INTERRUPT
DRIVEN
OUTPUT
IS
THE
IMPLEMENTATION
OF
DEV
SHORTPRINT
FOR
INTERRUPT
DRIVEN
DATA
TRANSFER
TO
HAPPEN
SUCCESSFULLY
THE
HARDWARE
SHOULD
BE
ABLE
TO
GENERATE
INTERRUPTS
WITH
THE
FOLLOWING
SEMANTICS
FOR
INPUT
THE
DEVICE
INTERRUPTS
THE
PROCESSOR
WHEN
NEW
DATA
HAS
ARRIVED
AND
IS
READY
TO
BE
RETRIEVED
BY
THE
SYSTEM
PROCESSOR
THE
ACTUAL
ACTIONS
TO
PERFORM
DEPEND
ON
WHETHER
THE
DEVICE
USES
I
O
PORTS
MEMORY
MAPPING
OR
DMA
FOR
OUTPUT
THE
DEVICE
DELIVERS
AN
INTERRUPT
EITHER
WHEN
IT
IS
READY
TO
ACCEPT
NEW
DATA
OR
TO
ACKNOWLEDGE
A
SUCCESSFUL
DATA
TRANSFER
MEMORY
MAPPED
AND
DMA
CAPABLE
DEVICES
USUALLY
GENERATE
INTERRUPTS
TO
TELL
THE
SYSTEM
THEY
ARE
DONE
WITH
THE
BUFFER
THE
TIMING
RELATIONSHIPS
BETWEEN
A
READ
OR
WRITE
AND
THE
ACTUAL
ARRIVAL
OF
DATA
WERE
INTRODUCED
IN
THE
SECTION
BLOCKING
AND
NONBLOCKING
OPERATIONS
IN
CHAPTER
A
WRITE
BUFFERING
EXAMPLE
WE
HAVE
MENTIONED
THE
SHORTPRINT
DRIVER
A
COUPLE
OF
TIMES
NOW
IT
IS
TIME
TO
ACTUALLY
TAKE
A
LOOK
THIS
MODULE
IMPLEMENTS
A
VERY
SIMPLE
OUTPUT
ORIENTED
DRIVER
FOR
THE
PARALLEL
PORT
IT
IS
SUFFICIENT
HOWEVER
TO
ENABLE
THE
PRINTING
OF
FILES
IF
YOU
CHOSE
TO
TEST
THIS
DRIVER
OUT
HOWEVER
REMEMBER
THAT
YOU
MUST
PASS
THE
PRINTER
A
FILE
IN
A
FOR
MAT
IT
UNDERSTANDS
NOT
ALL
PRINTERS
RESPOND
WELL
WHEN
GIVEN
A
STREAM
OF
ARBITRARY
DATA
THE
SHORTPRINT
DRIVER
MAINTAINS
A
ONE
PAGE
CIRCULAR
OUTPUT
BUFFER
WHEN
A
USER
SPACE
PROCESS
WRITES
DATA
TO
THE
DEVICE
THAT
DATA
IS
FED
INTO
THE
BUFFER
BUT
THE
WRITE
METHOD
DOES
NOT
ACTUALLY
PERFORM
ANY
I
O
INSTEAD
THE
CORE
OF
LOOKS
LIKE
THIS
WHILE
WRITTEN
COUNT
HANG
OUT
UNTIL
SOME
BUFFER
SPACE
IS
AVAILABLE
SPACE
IF
SPACE
IF
SPACE
GOTO
OUT
MOVE
DATA
INTO
THE
BUFFER
IF
SPACE
WRITTEN
COUNT
SPACE
COUNT
WRITTEN
IF
CHAR
BUF
SPACE
UP
RETURN
EFAULT
SPACE
BUF
SPACE
WRITTEN
SPACE
IF
NO
OUTPUT
IS
ACTIVE
MAKE
IT
ACTIVE
FLAGS
IF
FLAGS
OUT
WRITTEN
A
SEMAPHORE
CONTROLS
ACCESS
TO
THE
CIRCULAR
BUFFER
OBTAINS
THAT
SEMAPHORE
JUST
PRIOR
TO
THE
CODE
FRAGMENT
ABOVE
WHILE
HOLDING
THE
SEMA
PHORE
IT
ATTEMPTS
TO
FEED
DATA
INTO
THE
CIRCULAR
BUFFER
THE
FUNCTION
RETURNS
THE
AMOUNT
OF
CONTIGUOUS
SPACE
AVAILABLE
SO
THERE
IS
NO
NEED
TO
WORRY
ABOUT
BUFFER
WRAPS
IF
THAT
AMOUNT
IS
THE
DRIVER
WAITS
UNTIL
SOME
SPACE
IS
FREED
IT
THEN
COPIES
AS
MUCH
DATA
AS
IT
CAN
INTO
THE
BUFFER
ONCE
THERE
IS
DATA
TO
OUTPUT
MUST
ENSURE
THAT
THE
DATA
IS
WRITTEN
TO
THE
DEVICE
THE
ACTUAL
WRITING
IS
DONE
BY
WAY
OF
A
WORKQUEUE
FUNCTION
MUST
KICK
THAT
FUNCTION
OFF
IF
IT
IS
NOT
ALREADY
RUNNING
AFTER
OBTAINING
A
SEPARATE
SPINLOCK
THAT
CONTROLS
ACCESS
TO
VARIABLES
USED
ON
THE
CONSUMER
SIDE
OF
THE
OUTPUT
BUFFER
INCLUDING
IT
CALLS
IF
NEED
BE
THEN
IT
JUST
A
MATTER
OF
NOTING
HOW
MUCH
DATA
WAS
WRITTEN
TO
THE
BUFFER
AND
RETURNING
THE
FUNCTION
THAT
STARTS
THE
OUTPUT
PROCESS
LOOKS
LIKE
THE
FOLLOWING
STATIC
VOID
VOID
IF
SHOULD
NEVER
HAPPEN
RETURN
SET
UP
OUR
MISSED
INTERRUPT
TIMER
EXPIRES
JIFFIES
TIMEOUT
AND
GET
THE
PROCESS
GOING
THE
REALITY
OF
DEALING
WITH
HARDWARE
IS
THAT
YOU
CAN
OCCASIONALLY
LOSE
AN
INTERRUPT
FROM
THE
DEVICE
WHEN
THIS
HAPPENS
YOU
REALLY
DO
NOT
WANT
YOUR
DRIVER
TO
STOP
FOR
EVERMORE
UNTIL
THE
SYSTEM
IS
REBOOTED
THAT
IS
NOT
A
USER
FRIENDLY
WAY
OF
DOING
THINGS
IT
IS
FAR
BETTER
TO
REALIZE
THAT
AN
INTERRUPT
HAS
BEEN
MISSED
PICK
UP
THE
PIECES
AND
GO
ON
TO
THAT
END
SHORTPRINT
SETS
A
KERNEL
TIMER
WHENEVER
IT
OUTPUTS
DATA
TO
THE
DEVICE
IF
THE
TIMER
EXPIRES
WE
MAY
HAVE
MISSED
AN
INTERRUPT
WE
LOOK
AT
THE
TIMER
FUNCTION
SHORTLY
BUT
FOR
THE
MOMENT
LET
STICK
WITH
THE
MAIN
OUTPUT
FUNCTIONALITY
THAT
IS
IMPLEMENTED
IN
OUR
WORKQUEUE
FUNCTION
WHICH
AS
YOU
CAN
SEE
ABOVE
IS
SCHEDULED
HERE
THE
CORE
OF
THAT
FUNCTION
LOOKS
LIKE
THE
FOLLOWING
FLAGS
HAVE
WE
WRITTEN
EVERYTHING
IF
EMPTY
NOPE
WRITE
ANOTHER
BYTE
ELSE
IF
SOMEBODY
WAITING
MAYBE
WAKE
THEM
UP
IF
FLAGS
SINCE
WE
ARE
DEALING
WITH
THE
OUTPUT
SIDE
SHARED
VARIABLES
WE
MUST
OBTAIN
THE
SPINLOCK
THEN
WE
LOOK
TO
SEE
WHETHER
THERE
IS
ANY
MORE
DATA
TO
SEND
OUT
IF
NOT
WE
NOTE
THAT
OUTPUT
IS
NO
LONGER
ACTIVE
DELETE
THE
TIMER
AND
WAKE
UP
ANYBODY
WHO
MIGHT
HAVE
BEEN
WAITING
FOR
THE
QUEUE
TO
BECOME
COMPLETELY
EMPTY
THIS
SORT
OF
WAIT
IS
DONE
WHEN
THE
DEVICE
IS
CLOSED
IF
INSTEAD
THERE
REMAINS
DATA
TO
WRITE
WE
CALL
TO
ACTUALLY
SEND
A
BYTE
TO
THE
HARDWARE
THEN
SINCE
WE
MAY
HAVE
FREED
SPACE
IN
THE
OUTPUT
BUFFER
WE
CONSIDER
WAKING
UP
ANY
PROCESSES
WAITING
TO
ADD
MORE
DATA
TO
THAT
BUFFER
WE
DO
NOT
PERFORM
THAT
WAKEUP
UNCONDITIONALLY
HOWEVER
INSTEAD
WE
WAIT
UNTIL
A
MINIMUM
AMOUNT
OF
SPACE
IS
AVAILABLE
THERE
IS
NO
POINT
IN
AWAKENING
A
WRITER
EVERY
TIME
WE
TAKE
ONE
BYTE
OUT
OF
THE
BUFFER
THE
COST
OF
AWAKENING
THE
PROCESS
SCHEDULING
IT
TO
RUN
AND
PUTTING
IT
BACK
TO
SLEEP
IS
TOO
HIGH
FOR
THAT
INSTEAD
WE
SHOULD
WAIT
UNTIL
THAT
PRO
CESS
IS
ABLE
TO
MOVE
A
SUBSTANTIAL
AMOUNT
OF
DATA
INTO
THE
BUFFER
AT
ONCE
THIS
TECH
NIQUE
IS
COMMON
IN
BUFFERING
INTERRUPT
DRIVEN
DRIVERS
FOR
COMPLETENESS
HERE
IS
THE
CODE
THAT
ACTUALLY
WRITES
THE
DATA
TO
THE
PORT
STATIC
VOID
VOID
UNSIGNED
CHAR
CR
INB
SOMETHING
HAPPENED
RESET
THE
TIMER
JIFFIES
TIMEOUT
STROBE
A
BYTE
OUT
TO
THE
DEVICE
IF
UDELAY
CR
IF
UDELAY
CR
HERE
WE
RESET
THE
TIMER
TO
REFLECT
THE
FACT
THAT
WE
HAVE
MADE
SOME
PROGRESS
STROBE
THE
BYTE
OUT
TO
THE
DEVICE
AND
UPDATE
THE
CIRCULAR
BUFFER
POINTER
THE
WORKQUEUE
FUNCTION
DOES
NOT
RESUBMIT
ITSELF
DIRECTLY
SO
ONLY
A
SINGLE
BYTE
WILL
BE
WRITTEN
TO
THE
DEVICE
AT
SOME
POINT
THE
PRINTER
WILL
IN
ITS
SLOW
WAY
CONSUME
THE
BYTE
AND
BECOME
READY
FOR
THE
NEXT
ONE
IT
WILL
THEN
INTERRUPT
THE
PROCESSOR
THE
INTERRUPT
HANDLER
USED
IN
SHORTPRINT
IS
SHORT
AND
SIMPLE
STATIC
INT
IRQ
VOID
STRUCT
REGS
IF
RETURN
REMEMBER
THE
TIME
AND
FARM
OFF
THE
REST
TO
THE
WORKQUEUE
FUNCTION
RETURN
SINCE
THE
PARALLEL
PORT
DOES
NOT
REQUIRE
AN
EXPLICIT
INTERRUPT
ACKNOWLEDGMENT
ALL
THE
INTERRUPT
HANDLER
REALLY
NEEDS
TO
DO
IS
TO
TELL
THE
KERNEL
TO
RUN
THE
WORKQUEUE
FUNC
TION
AGAIN
WHAT
IF
THE
INTERRUPT
NEVER
COMES
THE
DRIVER
CODE
THAT
WE
HAVE
SEEN
THUS
FAR
WOULD
SIMPLY
COME
TO
A
HALT
TO
KEEP
THAT
FROM
HAPPENING
WE
SET
A
TIMER
BACK
A
FEW
PAGES
AGO
THE
FUNCTION
THAT
IS
EXECUTED
WHEN
THAT
TIMER
EXPIRES
IS
STATIC
VOID
UNSIGNED
LONG
UNUSED
UNSIGNED
LONG
FLAGS
UNSIGNED
CHAR
STATUS
IF
RETURN
FLAGS
STATUS
INB
IF
THE
PRINTER
IS
STILL
BUSY
WE
JUST
RESET
THE
TIMER
IF
STATUS
STATUS
EXPIRES
JIFFIES
TIMEOUT
FLAGS
RETURN
OTHERWISE
WE
MUST
HAVE
DROPPED
AN
INTERRUPT
FLAGS
NULL
NULL
IF
NO
OUTPUT
IS
SUPPOSED
TO
BE
ACTIVE
THE
TIMER
FUNCTION
SIMPLY
RETURNS
THIS
KEEPS
THE
TIMER
FROM
RESUBMITTING
ITSELF
WHEN
THINGS
ARE
BEING
SHUT
DOWN
THEN
AFTER
TAK
ING
THE
LOCK
WE
QUERY
THE
STATUS
OF
THE
PORT
IF
IT
CLAIMS
TO
BE
BUSY
IT
SIMPLY
HASN
T
GOTTEN
AROUND
TO
INTERRUPTING
US
YET
SO
WE
RESET
THE
TIMER
AND
RETURN
PRINTERS
CAN
AT
TIMES
TAKE
A
VERY
LONG
TIME
TO
MAKE
THEMSELVES
READY
CONSIDER
THE
PRINTER
THAT
RUNS
OUT
OF
PAPER
WHILE
EVERYBODY
IS
GONE
OVER
A
LONG
WEEKEND
IN
SUCH
SITUATIONS
THERE
IS
NOTHING
TO
DO
OTHER
THAN
TO
WAIT
PATIENTLY
UNTIL
SOMETHING
CHANGES
IF
HOWEVER
THE
PRINTER
CLAIMS
TO
BE
READY
WE
MUST
HAVE
MISSED
ITS
INTERRUPT
IN
THAT
CASE
WE
SIMPLY
INVOKE
OUR
INTERRUPT
HANDLER
MANUALLY
TO
GET
THE
OUTPUT
PROCESS
MOVING
AGAIN
THE
SHORTPRINT
DRIVER
DOES
NOT
SUPPORT
READING
FROM
THE
PORT
INSTEAD
IT
BEHAVES
LIKE
SHORTINT
AND
RETURNS
INTERRUPT
TIMING
INFORMATION
THE
IMPLEMENTATION
OF
AN
INTER
RUPT
DRIVEN
READ
METHOD
WOULD
BE
VERY
SIMILAR
TO
WHAT
WE
HAVE
SEEN
HOWEVER
DATA
FROM
THE
DEVICE
WOULD
BE
READ
INTO
A
DRIVER
BUFFER
IT
WOULD
BE
COPIED
OUT
TO
USER
SPACE
ONLY
WHEN
A
SIGNIFICANT
AMOUNT
OF
DATA
HAS
ACCUMULATED
IN
THE
BUFFER
THE
FULL
READ
REQUEST
HAS
BEEN
SATISFIED
OR
SOME
SORT
OF
TIMEOUT
OCCURS
QUICK
REFERENCE
THESE
SYMBOLS
RELATED
TO
INTERRUPT
MANAGEMENT
WERE
INTRODUCED
IN
THIS
CHAPTER
INCLUDE
LINUX
INTERRUPT
H
INT
UNSIGNED
INT
IRQ
HANDLER
UNSIGNED
LONG
FLAGS
CONST
CHAR
VOID
VOID
UNSIGNED
INT
IRQ
VOID
CALLS
THAT
REGISTER
AND
UNREGISTER
AN
INTERRUPT
HANDLER
INCLUDE
LINUX
IRQ
H
H
INT
UNSIGNED
INT
IRQ
UNSIGNED
LONG
FLAGS
THIS
FUNCTION
AVAILABLE
ON
THE
AND
ARCHITECTURES
RETURNS
A
NONZERO
VALUE
IF
AN
ATTEMPT
TO
ALLOCATE
THE
GIVEN
INTERRUPT
LINE
SUCCEEDS
INCLUDE
ASM
SIGNAL
H
FLAGS
FOR
REQUESTS
INSTALLATION
OF
A
FAST
HANDLER
AS
OPPOSED
TO
A
SLOW
ONE
INSTALLS
A
SHARED
HANDLER
AND
THE
THIRD
FLAG
ASSERTS
THAT
INTERRUPT
TIMESTAMPS
CAN
BE
USED
TO
GENERATE
SYSTEM
ENTROPY
PROC
INTERRUPTS
PROC
STAT
FILESYSTEM
NODES
THAT
REPORT
INFORMATION
ABOUT
HARDWARE
INTERRUPTS
AND
INSTALLED
HANDLERS
UNSIGNED
LONG
VOID
INT
UNSIGNED
LONG
FUNCTIONS
USED
BY
THE
DRIVER
WHEN
IT
HAS
TO
PROBE
TO
DETERMINE
WHICH
INTERRUPT
LINE
IS
BEING
USED
BY
A
DEVICE
THE
RESULT
OF
MUST
BE
PASSED
BACK
TO
AFTER
THE
INTERRUPT
HAS
BEEN
GENERATED
THE
RETURN
VALUE
OF
IS
THE
DETECTED
INTERRUPT
NUMBER
INT
X
THE
POSSIBLE
RETURN
VALUES
FROM
AN
INTERRUPT
HANDLER
INDICATING
WHETHER
AN
ACTUAL
INTERRUPT
FROM
THE
DEVICE
WAS
PRESENT
VOID
INT
IRQ
VOID
INT
IRQ
VOID
INT
IRQ
A
DRIVER
CAN
ENABLE
AND
DISABLE
INTERRUPT
REPORTING
IF
THE
HARDWARE
TRIES
TO
GEN
ERATE
AN
INTERRUPT
WHILE
INTERRUPTS
ARE
DISABLED
THE
INTERRUPT
IS
LOST
FOREVER
A
DRIVER
USING
A
SHARED
HANDLER
MUST
NOT
USE
THESE
FUNCTIONS
VOID
UNSIGNED
LONG
FLAGS
VOID
UNSIGNED
LONG
FLAGS
USE
TO
DISABLE
INTERRUPTS
ON
THE
LOCAL
PROCESSOR
AND
REMEMBER
THEIR
PREVIOUS
STATE
THE
FLAGS
CAN
BE
PASSED
TO
TO
RESTORE
THE
PREVIOUS
INTERRUPT
STATE
VOID
VOID
VOID
VOID
FUNCTIONS
THAT
UNCONDITIONALLY
DISABLE
AND
ENABLE
INTERRUPTS
ON
THE
CURRENT
PROCESSOR
WHILE
CHAPTER
INTRODUCED
THE
LOWEST
LEVELS
OF
HARDWARE
CONTROL
THIS
CHAPTER
PRO
VIDES
AN
OVERVIEW
OF
THE
HIGHER
LEVEL
BUS
ARCHITECTURES
A
BUS
IS
MADE
UP
OF
BOTH
AN
ELECTRICAL
INTERFACE
AND
A
PROGRAMMING
INTERFACE
IN
THIS
CHAPTER
WE
DEAL
WITH
THE
PROGRAMMING
INTERFACE
THIS
CHAPTER
COVERS
A
NUMBER
OF
BUS
ARCHITECTURES
HOWEVER
THE
PRIMARY
FOCUS
IS
ON
THE
KERNEL
FUNCTIONS
THAT
ACCESS
PERIPHERAL
COMPONENT
INTERCONNECT
PCI
PERIPHER
ALS
BECAUSE
THESE
DAYS
THE
PCI
BUS
IS
THE
MOST
COMMONLY
USED
PERIPHERAL
BUS
ON
DESKTOPS
AND
BIGGER
COMPUTERS
THE
BUS
IS
THE
ONE
THAT
IS
BEST
SUPPORTED
BY
THE
KER
NEL
ISA
IS
STILL
COMMON
FOR
ELECTRONIC
HOBBYISTS
AND
IS
DESCRIBED
LATER
ALTHOUGH
IT
IS
PRETTY
MUCH
A
BARE
METAL
KIND
OF
BUS
AND
THERE
ISN
T
MUCH
TO
SAY
IN
ADDITION
TO
WHAT
IS
COVERED
IN
CHAPTERS
AND
THE
PCI
INTERFACE
ALTHOUGH
MANY
COMPUTER
USERS
THINK
OF
PCI
AS
A
WAY
OF
LAYING
OUT
ELECTRICAL
WIRES
IT
IS
ACTUALLY
A
COMPLETE
SET
OF
SPECIFICATIONS
DEFINING
HOW
DIFFERENT
PARTS
OF
A
COMPUTER
SHOULD
INTERACT
THE
PCI
SPECIFICATION
COVERS
MOST
ISSUES
RELATED
TO
COMPUTER
INTERFACES
WE
ARE
NOT
GOING
TO
COVER
IT
ALL
HERE
IN
THIS
SECTION
WE
ARE
MAINLY
CONCERNED
WITH
HOW
A
PCI
DRIVER
CAN
FIND
ITS
HARDWARE
AND
GAIN
ACCESS
TO
IT
THE
PROBING
TECHNIQUES
DISCUSSED
IN
THE
SECTIONS
MODULE
PARAMETERS
IN
CHAPTER
AND
AUTODETECTING
THE
IRQ
NUMBER
IN
CHAPTER
CAN
BE
USED
WITH
PCI
DEVICES
BUT
THE
SPECIFICATION
OFFERS
AN
ALTERNATIVE
THAT
IS
PREFERABLE
TO
PROBING
THE
PCI
ARCHITECTURE
WAS
DESIGNED
AS
A
REPLACEMENT
FOR
THE
ISA
STANDARD
WITH
THREE
MAIN
GOALS
TO
GET
BETTER
PERFORMANCE
WHEN
TRANSFERRING
DATA
BETWEEN
THE
COMPUTER
AND
ITS
PERIPHERALS
TO
BE
AS
PLATFORM
INDEPENDENT
AS
POSSIBLE
AND
TO
SIMPLIFY
ADD
ING
AND
REMOVING
PERIPHERALS
TO
THE
SYSTEM
THE
PCI
BUS
ACHIEVES
BETTER
PERFORMANCE
BY
USING
A
HIGHER
CLOCK
RATE
THAN
ISA
ITS
CLOCK
RUNS
AT
OR
MHZ
ITS
ACTUAL
RATE
BEING
A
FACTOR
OF
THE
SYSTEM
CLOCK
AND
MHZ
AND
EVEN
MHZ
IMPLEMENTATIONS
HAVE
RECENTLY
BEEN
DEPLOYED
AS
WELL
MOREOVER
IT
IS
EQUIPPED
WITH
A
BIT
DATA
BUS
AND
A
BIT
EXTENSION
HAS
BEEN
INCLUDED
IN
THE
SPECIFICATION
PLATFORM
INDEPENDENCE
IS
OFTEN
A
GOAL
IN
THE
DESIGN
OF
A
COMPUTER
BUS
AND
IT
AN
ESPECIALLY
IMPORTANT
FEATURE
OF
PCI
BECAUSE
THE
PC
WORLD
HAS
ALWAYS
BEEN
DOMINATED
BY
PROCESSOR
SPECIFIC
INTERFACE
STANDARDS
PCI
IS
CUR
RENTLY
USED
EXTENSIVELY
ON
IA
ALPHA
POWERPC
AND
IA
SYSTEMS
AND
SOME
OTHER
PLATFORMS
AS
WELL
WHAT
IS
MOST
RELEVANT
TO
THE
DRIVER
WRITER
HOWEVER
IS
PCI
SUPPORT
FOR
AUTODETEC
TION
OF
INTERFACE
BOARDS
PCI
DEVICES
ARE
JUMPERLESS
UNLIKE
MOST
OLDER
PERIPHERALS
AND
ARE
AUTOMATICALLY
CONFIGURED
AT
BOOT
TIME
THEN
THE
DEVICE
DRIVER
MUST
BE
ABLE
TO
ACCESS
CONFIGURATION
INFORMATION
IN
THE
DEVICE
IN
ORDER
TO
COMPLETE
INITIALIZATION
THIS
HAPPENS
WITHOUT
THE
NEED
TO
PERFORM
ANY
PROBING
PCI
ADDRESSING
EACH
PCI
PERIPHERAL
IS
IDENTIFIED
BY
A
BUS
NUMBER
A
DEVICE
NUMBER
AND
A
FUNCTION
NUMBER
THE
PCI
SPECIFICATION
PERMITS
A
SINGLE
SYSTEM
TO
HOST
UP
TO
BUSES
BUT
BECAUSE
BUSES
ARE
NOT
SUFFICIENT
FOR
MANY
LARGE
SYSTEMS
LINUX
NOW
SUPPORTS
PCI
DOMAINS
EACH
PCI
DOMAIN
CAN
HOST
UP
TO
BUSES
EACH
BUS
HOSTS
UP
TO
DEVICES
AND
EACH
DEVICE
CAN
BE
A
MULTIFUNCTION
BOARD
SUCH
AS
AN
AUDIO
DEVICE
WITH
AN
ACCOMPANYING
CD
ROM
DRIVE
WITH
A
MAXIMUM
OF
EIGHT
FUNCTIONS
THEREFORE
EACH
FUNCTION
CAN
BE
IDENTIFIED
AT
HARDWARE
LEVEL
BY
A
BIT
ADDRESS
OR
KEY
DEVICE
DRIVERS
WRITTEN
FOR
LINUX
THOUGH
DON
T
NEED
TO
DEAL
WITH
THOSE
BINARY
ADDRESSES
BECAUSE
THEY
USE
A
SPECIFIC
DATA
STRUCTURE
CALLED
TO
ACT
ON
THE
DEVICES
MOST
RECENT
WORKSTATIONS
FEATURE
AT
LEAST
TWO
PCI
BUSES
PLUGGING
MORE
THAN
ONE
BUS
IN
A
SINGLE
SYSTEM
IS
ACCOMPLISHED
BY
MEANS
OF
BRIDGES
SPECIAL
PURPOSE
PCI
PERIPHER
ALS
WHOSE
TASK
IS
JOINING
TWO
BUSES
THE
OVERALL
LAYOUT
OF
A
PCI
SYSTEM
IS
A
TREE
WHERE
EACH
BUS
IS
CONNECTED
TO
AN
UPPER
LAYER
BUS
UP
TO
BUS
AT
THE
ROOT
OF
THE
TREE
THE
CARDBUS
PC
CARD
SYSTEM
IS
ALSO
CONNECTED
TO
THE
PCI
SYSTEM
VIA
BRIDGES
A
TYPICAL
PCI
SYSTEM
IS
REPRESENTED
IN
FIGURE
WHERE
THE
VARIOUS
BRIDGES
ARE
HIGHLIGHTED
THE
BIT
HARDWARE
ADDRESSES
ASSOCIATED
WITH
PCI
PERIPHERALS
ALTHOUGH
MOSTLY
HID
DEN
IN
THE
STRUCT
OBJECT
ARE
STILL
VISIBLE
OCCASIONALLY
ESPECIALLY
WHEN
LISTS
OF
DEVICES
ARE
BEING
USED
ONE
SUCH
SITUATION
IS
THE
OUTPUT
OF
LSPCI
PART
OF
THE
PCIUTILS
PACKAGE
AVAILABLE
WITH
MOST
DISTRIBUTIONS
AND
THE
LAYOUT
OF
INFORMATION
IN
PROC
PCI
AND
PROC
BUS
PCI
THE
SYSFS
REPRESENTATION
OF
PCI
DEVICES
ALSO
SHOWS
THIS
ADDRESSING
SCHEME
WITH
THE
ADDITION
OF
THE
PCI
DOMAIN
INFORMATION
WHEN
THE
HARDWARE
ADDRESS
IS
DISPLAYED
IT
CAN
BE
SHOWN
AS
TWO
VALUES
AN
BIT
BUS
NUMBER
AND
AN
BIT
SOME
ARCHITECTURES
ALSO
DISPLAY
THE
PCI
DOMAIN
INFORMATION
IN
THE
PROC
PCI
AND
PROC
BUS
PCI
FILES
FIGURE
LAYOUT
OF
A
TYPICAL
PCI
SYSTEM
DEVICE
AND
FUNCTION
NUMBER
AS
THREE
VALUES
BUS
DEVICE
AND
FUNCTION
OR
AS
FOUR
VALUES
DOMAIN
BUS
DEVICE
AND
FUNCTION
ALL
THE
VALUES
ARE
USUALLY
DISPLAYED
IN
HEXADECIMAL
FOR
EXAMPLE
PROC
BUS
PCI
DEVICES
USES
A
SINGLE
BIT
FIELD
TO
EASE
PARSING
AND
SORT
ING
WHILE
PROC
BUS
BUSNUMBER
SPLITS
THE
ADDRESS
INTO
THREE
FIELDS
THE
FOLLOWING
SHOWS
HOW
THOSE
ADDRESSES
APPEAR
SHOWING
ONLY
THE
BEGINNING
OF
THE
OUTPUT
LINES
LSPCI
CUT
D
HOST
BRIDGE
RAM
MEMORY
RAM
MEMORY
USB
CONTROLLER
MULTIMEDIA
AUDIO
CONTROLLER
BRIDGE
ISA
BRIDGE
USB
CONTROLLER
USB
CONTROLLER
USB
CONTROLLER
CARDBUS
BRIDGE
IDE
INTERFACE
ETHERNET
CONTROLLER
NETWORK
CONTROLLER
FIREWIRE
IEEE
VGA
COMPATIBLE
CONTROLLER
CAT
PROC
BUS
PCI
DEVICES
CUT
TREE
SYS
BUS
PCI
DEVICES
SYS
BUS
PCI
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
DEVICES
ALL
THREE
LISTS
OF
DEVICES
ARE
SORTED
IN
THE
SAME
ORDER
SINCE
LSPCI
USES
THE
PROC
FILES
AS
ITS
SOURCE
OF
INFORMATION
TAKING
THE
VGA
VIDEO
CONTROLLER
AS
AN
EXAMPLE
MEANS
WHEN
SPLIT
INTO
DOMAIN
BITS
BUS
BITS
DEVICE
BITS
AND
FUNCTION
BITS
THE
HARDWARE
CIRCUITRY
OF
EACH
PERIPHERAL
BOARD
ANSWERS
QUERIES
PERTAINING
TO
THREE
ADDRESS
SPACES
MEMORY
LOCATIONS
I
O
PORTS
AND
CONFIGURATION
REGISTERS
THE
FIRST
TWO
ADDRESS
SPACES
ARE
SHARED
BY
ALL
THE
DEVICES
ON
THE
SAME
PCI
BUS
I
E
WHEN
YOU
ACCESS
A
MEMORY
LOCATION
ALL
THE
DEVICES
ON
THAT
PCI
BUS
SEE
THE
BUS
CYCLE
AT
THE
SAME
TIME
THE
CONFIGURATION
SPACE
ON
THE
OTHER
HAND
EXPLOITS
GEOGRAPHICAL
ADDRESSING
CONFIGURATION
QUERIES
ADDRESS
ONLY
ONE
SLOT
AT
A
TIME
SO
THEY
NEVER
COLLIDE
AS
FAR
AS
THE
DRIVER
IS
CONCERNED
MEMORY
AND
I
O
REGIONS
ARE
ACCESSED
IN
THE
USUAL
WAYS
VIA
INB
READB
AND
SO
FORTH
CONFIGURATION
TRANSACTIONS
ON
THE
OTHER
HAND
ARE
PERFORMED
BY
CALLING
SPECIFIC
KERNEL
FUNCTIONS
TO
ACCESS
CONFIGURATION
REGISTERS
WITH
REGARD
TO
INTERRUPTS
EVERY
PCI
SLOT
HAS
FOUR
INTERRUPT
PINS
AND
EACH
DEVICE
FUNCTION
CAN
USE
ONE
OF
THEM
WITHOUT
BEING
CONCERNED
ABOUT
HOW
THOSE
PINS
ARE
ROUTED
TO
THE
CPU
SUCH
ROUTING
IS
THE
RESPONSIBILITY
OF
THE
COMPUTER
PLATFORM
AND
IS
IMPLE
MENTED
OUTSIDE
OF
THE
PCI
BUS
SINCE
THE
PCI
SPECIFICATION
REQUIRES
INTERRUPT
LINES
TO
BE
SHAREABLE
EVEN
A
PROCESSOR
WITH
A
LIMITED
NUMBER
OF
IRQ
LINES
SUCH
AS
THE
CAN
HOST
MANY
PCI
INTERFACE
BOARDS
EACH
WITH
FOUR
INTERRUPT
PINS
THE
I
O
SPACE
IN
A
PCI
BUS
USES
A
BIT
ADDRESS
BUS
LEADING
TO
GB
OF
I
O
PORTS
WHILE
THE
MEMORY
SPACE
CAN
BE
ACCESSED
WITH
EITHER
BIT
OR
BIT
ADDRESSES
BIT
ADDRESSES
ARE
AVAILABLE
ON
MORE
RECENT
PLATFORMS
ADDRESSES
ARE
SUPPOSED
TO
BE
UNIQUE
TO
ONE
DEVICE
BUT
SOFTWARE
MAY
ERRONEOUSLY
CONFIGURE
TWO
DEVICES
TO
THE
SAME
ADDRESS
MAKING
IT
IMPOSSIBLE
TO
ACCESS
EITHER
ONE
BUT
THIS
PROBLEM
NEVER
OCCURS
UNLESS
A
DRIVER
IS
WILLINGLY
PLAYING
WITH
REGISTERS
IT
SHOULDN
T
TOUCH
THE
GOOD
NEWS
IS
THAT
EVERY
MEMORY
AND
I
O
ADDRESS
REGION
OFFERED
BY
THE
INTERFACE
BOARD
CAN
BE
REMAPPED
BY
MEANS
OF
CONFIGURATION
TRANSACTIONS
THAT
IS
THE
FIRMWARE
INITIAL
IZES
PCI
HARDWARE
AT
SYSTEM
BOOT
MAPPING
EACH
REGION
TO
A
DIFFERENT
ADDRESS
TO
AVOID
COLLISIONS
THE
ADDRESSES
TO
WHICH
THESE
REGIONS
ARE
CURRENTLY
MAPPED
CAN
BE
READ
FROM
THE
CONFIGURATION
SPACE
SO
THE
LINUX
DRIVER
CAN
ACCESS
ITS
DEVICES
WITHOUT
PROBING
AFTER
READING
THE
CONFIGURATION
REGISTERS
THE
DRIVER
CAN
SAFELY
ACCESS
ITS
HARDWARE
THE
PCI
CONFIGURATION
SPACE
CONSISTS
OF
BYTES
FOR
EACH
DEVICE
FUNCTION
EXCEPT
FOR
PCI
EXPRESS
DEVICES
WHICH
HAVE
KB
OF
CONFIGURATION
SPACE
FOR
EACH
FUNCTION
AND
THE
LAYOUT
OF
THE
CONFIGURATION
REGISTERS
IS
STANDARDIZED
FOUR
BYTES
OF
THE
CONFIG
URATION
SPACE
HOLD
A
UNIQUE
FUNCTION
ID
SO
THE
DRIVER
CAN
IDENTIFY
ITS
DEVICE
BY
LOOK
ING
FOR
THE
SPECIFIC
ID
FOR
THAT
PERIPHERAL
IN
SUMMARY
EACH
DEVICE
BOARD
IS
GEOGRAPHICALLY
ADDRESSED
TO
RETRIEVE
ITS
CONFIGURATION
REGISTERS
THE
INFORMATION
IN
THOSE
REGISTERS
CAN
THEN
BE
USED
TO
PERFORM
NORMAL
I
O
ACCESS
WITHOUT
THE
NEED
FOR
FURTHER
GEOGRAPHIC
ADDRESSING
IT
SHOULD
BE
CLEAR
FROM
THIS
DESCRIPTION
THAT
THE
MAIN
INNOVATION
OF
THE
PCI
INTERFACE
STANDARD
OVER
ISA
IS
THE
CONFIGURATION
ADDRESS
SPACE
THEREFORE
IN
ADDITION
TO
THE
USUAL
DRIVER
CODE
A
PCI
DRIVER
NEEDS
THE
ABILITY
TO
ACCESS
THE
CONFIGURATION
SPACE
IN
ORDER
TO
SAVE
ITSELF
FROM
RISKY
PROBING
TASKS
FOR
THE
REMAINDER
OF
THIS
CHAPTER
WE
USE
THE
WORD
DEVICE
TO
REFER
TO
A
DEVICE
FUNC
TION
BECAUSE
EACH
FUNCTION
IN
A
MULTIFUNCTION
BOARD
ACTS
AS
AN
INDEPENDENT
ENTITY
WHEN
WE
REFER
TO
A
DEVICE
WE
MEAN
THE
TUPLE
DOMAIN
NUMBER
BUS
NUMBER
DEVICE
NUMBER
AND
FUNCTION
NUMBER
BOOT
TIME
TO
SEE
HOW
PCI
WORKS
WE
START
FROM
SYSTEM
BOOT
SINCE
THAT
WHEN
THE
DEVICES
ARE
CONFIGURED
ACTUALLY
THAT
CONFIGURATION
IS
NOT
RESTRICTED
TO
THE
TIME
THE
SYSTEM
BOOTS
HOTPLUGGABLE
DEVICES
FOR
EXAMPLE
CANNOT
BE
AVAILABLE
AT
BOOT
TIME
AND
APPEAR
LATER
INSTEAD
THE
MAIN
POINT
HERE
IS
THAT
THE
DEVICE
DRIVER
MUST
NOT
CHANGE
THE
ADDRESS
OF
I
O
OR
MEMORY
REGIONS
YOU
LL
FIND
THE
ID
OF
ANY
DEVICE
IN
ITS
OWN
HARDWARE
MANUAL
A
LIST
IS
INCLUDED
IN
THE
FILE
PCI
IDS
PART
OF
THE
PCIUTILS
PACKAGE
AND
THE
KERNEL
SOURCES
IT
DOESN
T
PRETEND
TO
BE
COMPLETE
BUT
JUST
LISTS
THE
MOST
RENOWNED
VENDORS
AND
DEVICES
THE
KERNEL
VERSION
OF
THIS
FILE
WILL
NOT
BE
INCLUDED
IN
FUTURE
KERNEL
SERIES
WHEN
POWER
IS
APPLIED
TO
A
PCI
DEVICE
THE
HARDWARE
REMAINS
INACTIVE
IN
OTHER
WORDS
THE
DEVICE
RESPONDS
ONLY
TO
CONFIGURATION
TRANSACTIONS
AT
POWER
ON
THE
DEVICE
HAS
NO
MEMORY
AND
NO
I
O
PORTS
MAPPED
IN
THE
COMPUTER
ADDRESS
SPACE
EVERY
OTHER
DEVICE
SPECIFIC
FEATURE
SUCH
AS
INTERRUPT
REPORTING
IS
DISABLED
AS
WELL
FORTUNATELY
EVERY
PCI
MOTHERBOARD
IS
EQUIPPED
WITH
PCI
AWARE
FIRMWARE
CALLED
THE
BIOS
NVRAM
OR
PROM
DEPENDING
ON
THE
PLATFORM
THE
FIRMWARE
OFFERS
ACCESS
TO
THE
DEVICE
CONFIGURATION
ADDRESS
SPACE
BY
READING
AND
WRITING
REGISTERS
IN
THE
PCI
CONTROLLER
AT
SYSTEM
BOOT
THE
FIRMWARE
OR
THE
LINUX
KERNEL
IF
SO
CONFIGURED
PERFORMS
CONFIG
URATION
TRANSACTIONS
WITH
EVERY
PCI
PERIPHERAL
IN
ORDER
TO
ALLOCATE
A
SAFE
PLACE
FOR
EACH
ADDRESS
REGION
IT
OFFERS
BY
THE
TIME
A
DEVICE
DRIVER
ACCESSES
THE
DEVICE
ITS
MEM
ORY
AND
I
O
REGIONS
HAVE
ALREADY
BEEN
MAPPED
INTO
THE
PROCESSOR
ADDRESS
SPACE
THE
DRIVER
CAN
CHANGE
THIS
DEFAULT
ASSIGNMENT
BUT
IT
NEVER
NEEDS
TO
DO
THAT
AS
SUGGESTED
THE
USER
CAN
LOOK
AT
THE
PCI
DEVICE
LIST
AND
THE
DEVICES
CONFIGURATION
REGISTERS
BY
READING
PROC
BUS
PCI
DEVICES
AND
PROC
BUS
PCI
THE
FORMER
IS
A
TEXT
FILE
WITH
HEXADECIMAL
DEVICE
INFORMATION
AND
THE
LATTER
ARE
BINARY
FILES
THAT
REPORT
A
SNAPSHOT
OF
THE
CONFIGURATION
REGISTERS
OF
EACH
DEVICE
ONE
FILE
PER
DEVICE
THE
INDI
VIDUAL
PCI
DEVICE
DIRECTORIES
IN
THE
SYSFS
TREE
CAN
BE
FOUND
IN
SYS
BUS
PCI
DEVICES
A
PCI
DEVICE
DIRECTORY
CONTAINS
A
NUMBER
OF
DIFFERENT
FILES
TREE
SYS
BUS
PCI
DEVICES
SYS
BUS
PCI
DEVICES
CLASS
CONFIG
DEVICE
IRQ
POWER
STATE
RESOURCE
VENDOR
THE
FILE
CONFIG
IS
A
BINARY
FILE
THAT
ALLOWS
THE
RAW
PCI
CONFIG
INFORMATION
TO
BE
READ
FROM
THE
DEVICE
JUST
LIKE
THE
PROC
BUS
PCI
PROVIDES
THE
FILES
VENDOR
DEVICE
AND
CLASS
ALL
REFER
TO
THE
SPECIFIC
VALUES
OF
THIS
PCI
DEVICE
ALL
PCI
DEVICES
PROVIDE
THIS
INFORMATION
THE
FILE
IRQ
SHOWS
THE
CURRENT
IRQ
ASSIGNED
TO
THIS
PCI
DEVICE
AND
THE
FILE
RESOURCE
SHOWS
THE
CURRENT
MEMORY
RESOURCES
ALLOCATED
BY
THIS
DEVICE
CONFIGURATION
REGISTERS
AND
INITIALIZATION
IN
THIS
SECTION
WE
LOOK
AT
THE
CONFIGURATION
REGISTERS
THAT
PCI
DEVICES
CONTAIN
ALL
PCI
DEVICES
FEATURE
AT
LEAST
A
BYTE
ADDRESS
SPACE
THE
FIRST
BYTES
ARE
STANDARD
IZED
WHILE
THE
REST
ARE
DEVICE
DEPENDENT
FIGURE
SHOWS
THE
LAYOUT
OF
THE
DEVICE
INDEPENDENT
CONFIGURATION
SPACE
VENDOR
ID
DEVICE
ID
COMMAND
REG
STATUS
REG
REVIS
ION
ID
CLASS
CODE
CACHE
LINE
LATENCY
TIMER
HEADER
TYPE
BIST
EXPANSION
ROM
BASE
ADDRESS
RESE
RVED
IRQ
LINE
IRQ
PIN
FIGURE
THE
STANDARDIZED
PCI
CONFIGURATION
REGISTERS
AS
THE
FIGURE
SHOWS
SOME
OF
THE
PCI
CONFIGURATION
REGISTERS
ARE
REQUIRED
AND
SOME
ARE
OPTIONAL
EVERY
PCI
DEVICE
MUST
CONTAIN
MEANINGFUL
VALUES
IN
THE
REQUIRED
REGIS
TERS
WHEREAS
THE
CONTENTS
OF
THE
OPTIONAL
REGISTERS
DEPEND
ON
THE
ACTUAL
CAPABILITIES
OF
THE
PERIPHERAL
THE
OPTIONAL
FIELDS
ARE
NOT
USED
UNLESS
THE
CONTENTS
OF
THE
REQUIRED
FIELDS
INDICATE
THAT
THEY
ARE
VALID
THUS
THE
REQUIRED
FIELDS
ASSERT
THE
BOARD
CAPABIL
ITIES
INCLUDING
WHETHER
THE
OTHER
FIELDS
ARE
USABLE
IT
INTERESTING
TO
NOTE
THAT
THE
PCI
REGISTERS
ARE
ALWAYS
LITTLE
ENDIAN
ALTHOUGH
THE
STANDARD
IS
DESIGNED
TO
BE
ARCHITECTURE
INDEPENDENT
THE
PCI
DESIGNERS
SOMETIMES
SHOW
A
SLIGHT
BIAS
TOWARD
THE
PC
ENVIRONMENT
THE
DRIVER
WRITER
SHOULD
BE
CAREFUL
ABOUT
BYTE
ORDERING
WHEN
ACCESSING
MULTIBYTE
CONFIGURATION
REGISTERS
CODE
THAT
WORKS
ON
THE
PC
MIGHT
NOT
WORK
ON
OTHER
PLATFORMS
THE
LINUX
DEVELOPERS
HAVE
TAKEN
CARE
OF
THE
BYTE
ORDERING
PROBLEM
SEE
THE
NEXT
SECTION
ACCESSING
THE
CONFIG
URATION
SPACE
BUT
THE
ISSUE
MUST
BE
KEPT
IN
MIND
IF
YOU
EVER
NEED
TO
CONVERT
DATA
FROM
HOST
ORDER
TO
PCI
ORDER
OR
VICE
VERSA
YOU
CAN
RESORT
TO
THE
FUNCTIONS
DEFINED
IN
ASM
BYTEORDER
H
INTRODUCED
IN
CHAPTER
KNOWING
THAT
PCI
BYTE
ORDER
IS
LITTLE
ENDIAN
DESCRIBING
ALL
THE
CONFIGURATION
ITEMS
IS
BEYOND
THE
SCOPE
OF
THIS
BOOK
USUALLY
THE
TECHNICAL
DOCUMENTATION
RELEASED
WITH
EACH
DEVICE
DESCRIBES
THE
SUPPORTED
REGISTERS
WHAT
WE
RE
INTERESTED
IN
IS
HOW
A
DRIVER
CAN
LOOK
FOR
ITS
DEVICE
AND
HOW
IT
CAN
ACCESS
THE
DEVICE
CONFIGURATION
SPACE
THREE
OR
FIVE
PCI
REGISTERS
IDENTIFY
A
DEVICE
VENDORID
DEVICEID
AND
CLASS
ARE
THE
THREE
THAT
ARE
ALWAYS
USED
EVERY
PCI
MANUFACTURER
ASSIGNS
PROPER
VALUES
TO
THESE
READ
ONLY
REGISTERS
AND
THE
DRIVER
CAN
USE
THEM
TO
LOOK
FOR
THE
DEVICE
ADDITIONALLY
THE
FIELDS
SUBSYSTEM
VENDORID
AND
SUBSYSTEM
DEVICEID
ARE
SOMETIMES
SET
BY
THE
VEN
DOR
TO
FURTHER
DIFFERENTIATE
SIMILAR
DEVICES
LET
LOOK
AT
THESE
REGISTERS
IN
MORE
DETAIL
VENDORID
THIS
BIT
REGISTER
IDENTIFIES
A
HARDWARE
MANUFACTURER
FOR
INSTANCE
EVERY
INTEL
DEVICE
IS
MARKED
WITH
THE
SAME
VENDOR
NUMBER
THERE
IS
A
GLOBAL
REGIS
TRY
OF
SUCH
NUMBERS
MAINTAINED
BY
THE
PCI
SPECIAL
INTEREST
GROUP
AND
MANU
FACTURERS
MUST
APPLY
TO
HAVE
A
UNIQUE
NUMBER
ASSIGNED
TO
THEM
DEVICEID
THIS
IS
ANOTHER
BIT
REGISTER
SELECTED
BY
THE
MANUFACTURER
NO
OFFICIAL
REGISTRA
TION
IS
REQUIRED
FOR
THE
DEVICE
ID
THIS
ID
IS
USUALLY
PAIRED
WITH
THE
VENDOR
ID
TO
MAKE
A
UNIQUE
BIT
IDENTIFIER
FOR
A
HARDWARE
DEVICE
WE
USE
THE
WORD
SIGNA
TURE
TO
REFER
TO
THE
VENDOR
AND
DEVICE
ID
PAIR
A
DEVICE
DRIVER
USUALLY
RELIES
ON
THE
SIGNATURE
TO
IDENTIFY
ITS
DEVICE
YOU
CAN
FIND
WHAT
VALUE
TO
LOOK
FOR
IN
THE
HARDWARE
MANUAL
FOR
THE
TARGET
DEVICE
CLASS
EVERY
PERIPHERAL
DEVICE
BELONGS
TO
A
CLASS
THE
CLASS
REGISTER
IS
A
BIT
VALUE
WHOSE
TOP
BITS
IDENTIFY
THE
BASE
CLASS
OR
GROUP
FOR
EXAMPLE
ETHERNET
AND
TOKEN
RING
ARE
TWO
CLASSES
BELONGING
TO
THE
NETWORK
GROUP
WHILE
THE
SERIAL
AND
PARALLEL
CLASSES
BELONG
TO
THE
COMMUNICATION
GROUP
SOME
DRIV
ERS
CAN
SUPPORT
SEVERAL
SIMILAR
DEVICES
EACH
OF
THEM
FEATURING
A
DIFFERENT
SIGNA
TURE
BUT
ALL
BELONGING
TO
THE
SAME
CLASS
THESE
DRIVERS
CAN
RELY
ON
THE
CLASS
REGISTER
TO
IDENTIFY
THEIR
PERIPHERALS
AS
SHOWN
LATER
SUBSYSTEM
VENDORID
SUBSYSTEM
DEVICEID
THESE
FIELDS
CAN
BE
USED
FOR
FURTHER
IDENTIFICATION
OF
A
DEVICE
IF
THE
CHIP
IS
A
GENERIC
INTERFACE
CHIP
TO
A
LOCAL
ONBOARD
BUS
IT
IS
OFTEN
USED
IN
SEVERAL
COM
PLETELY
DIFFERENT
ROLES
AND
THE
DRIVER
MUST
IDENTIFY
THE
ACTUAL
DEVICE
IT
IS
TALKING
WITH
THE
SUBSYSTEM
IDENTIFIERS
ARE
USED
TO
THIS
END
USING
THESE
DIFFERENT
IDENTIFIERS
A
PCI
DRIVER
CAN
TELL
THE
KERNEL
WHAT
KIND
OF
DEVICES
IT
SUPPORTS
THE
STRUCT
STRUCTURE
IS
USED
TO
DEFINE
A
LIST
OF
THE
DIFFERENT
TYPES
OF
PCI
DEVICES
THAT
A
DRIVER
SUPPORTS
THIS
STRUCTURE
CONTAINS
THE
FOLLOWING
FIELDS
VENDOR
DEVICE
THESE
SPECIFY
THE
PCI
VENDOR
AND
DEVICE
IDS
OF
A
DEVICE
IF
A
DRIVER
CAN
HANDLE
ANY
VENDOR
OR
DEVICE
ID
THE
VALUE
SHOULD
BE
USED
FOR
THESE
FIELDS
SUBVENDOR
SUBDEVICE
THESE
SPECIFY
THE
PCI
SUBSYSTEM
VENDOR
AND
SUBSYSTEM
DEVICE
IDS
OF
A
DEVICE
IF
A
DRIVER
CAN
HANDLE
ANY
TYPE
OF
SUBSYSTEM
ID
THE
VALUE
SHOULD
BE
USED
FOR
THESE
FIELDS
CLASS
THESE
TWO
VALUES
ALLOW
THE
DRIVER
TO
SPECIFY
THAT
IT
SUPPORTS
A
TYPE
OF
PCI
CLASS
DEVICE
THE
DIFFERENT
CLASSES
OF
PCI
DEVICES
A
VGA
CONTROLLER
IS
ONE
EXAMPLE
ARE
DESCRIBED
IN
THE
PCI
SPECIFICATION
IF
A
DRIVER
CAN
HANDLE
ANY
TYPE
OF
SUB
SYSTEM
ID
THE
VALUE
SHOULD
BE
USED
FOR
THESE
FIELDS
THIS
VALUE
IS
NOT
USED
TO
MATCH
A
DEVICE
BUT
IS
USED
TO
HOLD
INFORMATION
THAT
THE
PCI
DRIVER
CAN
USE
TO
DIFFERENTIATE
BETWEEN
DIFFERENT
DEVICES
IF
IT
WANTS
TO
THERE
ARE
TWO
HELPER
MACROS
THAT
SHOULD
BE
USED
TO
INITIALIZE
A
STRUCT
STRUCTURE
VENDOR
DEVICE
THIS
CREATES
A
STRUCT
THAT
MATCHES
ONLY
THE
SPECIFIC
VENDOR
AND
DEVICE
ID
THE
MACRO
SETS
THE
SUBVENDOR
AND
SUBDEVICE
FIELDS
OF
THE
STRUCTURE
TO
THIS
CREATES
A
STRUCT
THAT
MATCHES
A
SPECIFIC
PCI
CLASS
AN
EXAMPLE
OF
USING
THESE
MACROS
TO
DEFINE
THE
TYPE
OF
DEVICES
A
DRIVER
SUPPORTS
CAN
BE
FOUND
IN
THE
FOLLOWING
KERNEL
FILES
DRIVERS
USB
HOST
EHCI
HCD
C
STATIC
CONST
STRUCT
HANDLE
ANY
USB
EHCI
CONTROLLER
UNSIGNED
LONG
END
ALL
ZEROES
DRIVERS
BUSSES
C
STATIC
STRUCT
THESE
EXAMPLES
CREATE
A
LIST
OF
STRUCT
STRUCTURES
WITH
AN
EMPTY
STRUC
TURE
SET
TO
ALL
ZEROS
AS
THE
LAST
VALUE
IN
THE
LIST
THIS
ARRAY
OF
IDS
IS
USED
IN
THE
STRUCT
DESCRIBED
BELOW
AND
IT
IS
ALSO
USED
TO
TELL
USER
SPACE
WHICH
DEVICES
THIS
SPECIFIC
DRIVER
SUPPORTS
THIS
STRUCTURE
NEEDS
TO
BE
EXPORTED
TO
USER
SPACE
TO
ALLOW
THE
HOTPLUG
AND
MODULE
LOADING
SYSTEMS
KNOW
WHAT
MODULE
WORKS
WITH
WHAT
HARDWARE
DEVICES
THE
MACRO
ACCOMPLISHES
THIS
AN
EXAMPLE
IS
PCI
THIS
STATEMENT
CREATES
A
LOCAL
VARIABLE
CALLED
THAT
POINTS
TO
THE
LIST
OF
STRUCT
LATER
IN
THE
KERNEL
BUILD
PROCESS
THE
DEPMOD
PRO
GRAM
SEARCHES
ALL
MODULES
FOR
THE
SYMBOL
IF
THAT
SYMBOL
IS
FOUND
IT
PULLS
THE
DATA
OUT
OF
THE
MODULE
AND
ADDS
IT
TO
THE
FILE
LIB
MODULES
MODULES
PCIMAP
AFTER
DEPMOD
COMPLETES
ALL
PCI
DEVICES
THAT
ARE
SUPPORTED
BY
MODULES
IN
THE
KERNEL
ARE
LISTED
ALONG
WITH
THEIR
MODULE
NAMES
IN
THAT
FILE
WHEN
THE
KERNEL
TELLS
THE
HOTPLUG
SYSTEM
THAT
A
NEW
PCI
DEVICE
HAS
BEEN
FOUND
THE
HOTPLUG
SYSTEM
USES
THE
MODULES
PCIMAP
FILE
TO
FIND
THE
PROPER
DRIVER
TO
LOAD
REGISTERING
A
PCI
DRIVER
THE
MAIN
STRUCTURE
THAT
ALL
PCI
DRIVERS
MUST
CREATE
IN
ORDER
TO
BE
REGISTERED
WITH
THE
KERNEL
PROPERLY
IS
THE
STRUCT
STRUCTURE
THIS
STRUCTURE
CONSISTS
OF
A
NUM
BER
OF
FUNCTION
CALLBACKS
AND
VARIABLES
THAT
DESCRIBE
THE
PCI
DRIVER
TO
THE
PCI
CORE
HERE
ARE
THE
FIELDS
IN
THIS
STRUCTURE
THAT
A
PCI
DRIVER
NEEDS
TO
BE
AWARE
OF
CONST
CHAR
NAME
THE
NAME
OF
THE
DRIVER
IT
MUST
BE
UNIQUE
AMONG
ALL
PCI
DRIVERS
IN
THE
KERNEL
AND
IS
NORMALLY
SET
TO
THE
SAME
NAME
AS
THE
MODULE
NAME
OF
THE
DRIVER
IT
SHOWS
UP
IN
SYSFS
UNDER
SYS
BUS
PCI
DRIVERS
WHEN
THE
DRIVER
IS
IN
THE
KERNEL
CONST
STRUCT
POINTER
TO
THE
STRUCT
TABLE
DESCRIBED
EARLIER
IN
THIS
CHAPTER
INT
PROBE
STRUCT
DEV
CONST
STRUCT
ID
POINTER
TO
THE
PROBE
FUNCTION
IN
THE
PCI
DRIVER
THIS
FUNCTION
IS
CALLED
BY
THE
PCI
CORE
WHEN
IT
HAS
A
STRUCT
THAT
IT
THINKS
THIS
DRIVER
WANTS
TO
CONTROL
A
POINTER
TO
THE
STRUCT
THAT
THE
PCI
CORE
USED
TO
MAKE
THIS
DECISION
IS
ALSO
PASSED
TO
THIS
FUNCTION
IF
THE
PCI
DRIVER
CLAIMS
THE
STRUCT
THAT
IS
PASSED
TO
IT
IT
SHOULD
INITIALIZE
THE
DEVICE
PROPERLY
AND
RETURN
IF
THE
DRIVER
DOES
NOT
WANT
TO
CLAIM
THE
DEVICE
OR
AN
ERROR
OCCURS
IT
SHOULD
RETURN
A
NEGATIVE
ERROR
VALUE
MORE
DETAILS
ABOUT
THIS
FUNCTION
FOLLOW
LATER
IN
THIS
CHAPTER
VOID
REMOVE
STRUCT
DEV
POINTER
TO
THE
FUNCTION
THAT
THE
PCI
CORE
CALLS
WHEN
THE
STRUCT
IS
BEING
REMOVED
FROM
THE
SYSTEM
OR
WHEN
THE
PCI
DRIVER
IS
BEING
UNLOADED
FROM
THE
KERNEL
MORE
DETAILS
ABOUT
THIS
FUNCTION
FOLLOW
LATER
IN
THIS
CHAPTER
INT
SUSPEND
STRUCT
DEV
STATE
POINTER
TO
THE
FUNCTION
THAT
THE
PCI
CORE
CALLS
WHEN
THE
STRUCT
IS
BEING
SUSPENDED
THE
SUSPEND
STATE
IS
PASSED
IN
THE
STATE
VARIABLE
THIS
FUNCTION
IS
OPTIONAL
A
DRIVER
DOES
NOT
HAVE
TO
PROVIDE
IT
INT
RESUME
STRUCT
DEV
POINTER
TO
THE
FUNCTION
THAT
THE
PCI
CORE
CALLS
WHEN
THE
STRUCT
IS
BEING
RESUMED
IT
IS
ALWAYS
CALLED
AFTER
SUSPEND
HAS
BEEN
CALLED
THIS
FUNCTION
IS
OPTIONAL
A
DRIVER
DOES
NOT
HAVE
TO
PROVIDE
IT
IN
SUMMARY
TO
CREATE
A
PROPER
STRUCT
STRUCTURE
ONLY
FOUR
FIELDS
NEED
TO
BE
INITIALIZED
STATIC
STRUCT
NAME
IDS
PROBE
PROBE
REMOVE
REMOVE
TO
REGISTER
THE
STRUCT
WITH
THE
PCI
CORE
A
CALL
TO
IS
MADE
WITH
A
POINTER
TO
THE
STRUCT
THIS
IS
TRADITIONALLY
DONE
IN
THE
MOD
ULE
INITIALIZATION
CODE
FOR
THE
PCI
DRIVER
STATIC
INT
VOID
RETURN
NOTE
THAT
THE
FUNCTION
EITHER
RETURNS
A
NEGATIVE
ERROR
NUMBER
OR
IF
EVERYTHING
WAS
REGISTERED
SUCCESSFULLY
IT
DOES
NOT
RETURN
THE
NUMBER
OF
DEVICES
THAT
WERE
BOUND
TO
THE
DRIVER
OR
AN
ERROR
NUMBER
IF
NO
DEVICES
WERE
BOUND
TO
THE
DRIVER
THIS
IS
A
CHANGE
FROM
KERNELS
PRIOR
TO
THE
RELEASE
AND
WAS
DONE
BECAUSE
OF
THE
FOLLOWING
SITUATIONS
ON
SYSTEMS
THAT
SUPPORT
PCI
HOTPLUG
OR
CARDBUS
SYSTEMS
A
PCI
DEVICE
CAN
APPEAR
OR
DISAPPEAR
AT
ANY
POINT
IN
TIME
IT
IS
HELPFUL
IF
DRIVERS
CAN
BE
LOADED
BEFORE
THE
DEVICE
APPEARS
TO
REDUCE
THE
TIME
IT
TAKES
TO
INITIALIZE
A
DEVICE
THE
KERNEL
ALLOWS
NEW
PCI
IDS
TO
BE
DYNAMICALLY
ALLOCATED
TO
A
DRIVER
AFTER
IT
HAS
BEEN
LOADED
THIS
IS
DONE
THROUGH
THE
FILE
THAT
IS
CREATED
IN
ALL
PCI
DRIVER
DIRECTORIES
IN
SYSFS
THIS
IS
VERY
USEFUL
IF
A
NEW
DEVICE
IS
BEING
USED
THAT
THE
KERNEL
DOESN
T
KNOW
ABOUT
JUST
YET
A
USER
CAN
WRITE
THE
PCI
ID
VALUES
TO
THE
FILE
AND
THEN
THE
DRIVER
BINDS
TO
THE
NEW
DEVICE
IF
A
DRIVER
WAS
NOT
ALLOWED
TO
LOAD
UNTIL
A
DEVICE
WAS
PRESENT
IN
THE
SYSTEM
THIS
INTERFACE
WOULD
NOT
BE
ABLE
TO
WORK
WHEN
THE
PCI
DRIVER
IS
TO
BE
UNLOADED
THE
STRUCT
NEEDS
TO
BE
UNREGIS
TERED
FROM
THE
KERNEL
THIS
IS
DONE
WITH
A
CALL
TO
WHEN
THIS
CALL
HAPPENS
ANY
PCI
DEVICES
THAT
WERE
CURRENTLY
BOUND
TO
THIS
DRIVER
ARE
REMOVED
AND
THE
REMOVE
FUNCTION
FOR
THIS
PCI
DRIVER
IS
CALLED
BEFORE
THE
FUNC
TION
RETURNS
STATIC
VOID
VOID
OLD
STYLE
PCI
PROBING
IN
OLDER
KERNEL
VERSIONS
THE
FUNCTION
WAS
NOT
ALWAYS
USED
BY
PCI
DRIVERS
INSTEAD
THEY
WOULD
EITHER
WALK
THE
LIST
OF
PCI
DEVICES
IN
THE
SYSTEM
BY
HAND
OR
THEY
WOULD
CALL
A
FUNCTION
THAT
COULD
SEARCH
FOR
A
SPECIFIC
PCI
DEVICE
THE
ABILITY
TO
WALK
THE
LIST
OF
PCI
DEVICES
IN
THE
SYSTEM
WITHIN
A
DRIVER
HAS
BEEN
REMOVED
FROM
THE
KERNEL
IN
ORDER
TO
PREVENT
DRIVERS
FROM
CRASHING
THE
KERNEL
IF
THEY
HAP
PENED
TO
MODIFY
THE
PCI
DEVICE
LISTS
WHILE
A
DEVICE
WAS
BEING
REMOVED
AT
THE
SAME
TIME
IF
THE
ABILITY
TO
FIND
A
SPECIFIC
PCI
DEVICE
IS
REALLY
NEEDED
THE
FOLLOWING
FUNCTIONS
ARE
AVAILABLE
STRUCT
UNSIGNED
INT
VENDOR
UNSIGNED
INT
DEVICE
STRUCT
FROM
THIS
FUNCTION
SCANS
THE
LIST
OF
PCI
DEVICES
CURRENTLY
PRESENT
IN
THE
SYSTEM
AND
IF
THE
INPUT
ARGUMENTS
MATCH
THE
SPECIFIED
VENDOR
AND
DEVICE
IDS
IT
INCREMENTS
THE
REFERENCE
COUNT
ON
THE
STRUCT
VARIABLE
FOUND
AND
RETURNS
IT
TO
THE
CALLER
THIS
PREVENTS
THE
STRUCTURE
FROM
DISAPPEARING
WITHOUT
ANY
NOTICE
AND
ENSURES
THAT
THE
KERNEL
DOES
NOT
OOPS
AFTER
THE
DRIVER
IS
DONE
WITH
THE
STRUCT
RETURNED
BY
THE
FUNCTION
IT
MUST
CALL
THE
FUNCTION
TO
DECRE
MENT
THE
USAGE
COUNT
PROPERLY
BACK
TO
ALLOW
THE
KERNEL
TO
CLEAN
UP
THE
DEVICE
IF
IT
IS
REMOVED
THE
FROM
ARGUMENT
IS
USED
TO
GET
HOLD
OF
MULTIPLE
DEVICES
WITH
THE
SAME
SIGNA
TURE
THE
ARGUMENT
SHOULD
POINT
TO
THE
LAST
DEVICE
THAT
HAS
BEEN
FOUND
SO
THAT
THE
SEARCH
CAN
CONTINUE
INSTEAD
OF
RESTARTING
FROM
THE
HEAD
OF
THE
LIST
TO
FIND
THE
FIRST
DEVICE
FROM
IS
SPECIFIED
AS
NULL
IF
NO
FURTHER
DEVICE
IS
FOUND
NULL
IS
RETURNED
AN
EXAMPLE
OF
HOW
TO
USE
THIS
FUNCTION
PROPERLY
IS
STRUCT
DEV
DEV
NULL
IF
DEV
USE
THE
PCI
DEVICE
DEV
THIS
FUNCTION
CAN
NOT
BE
CALLED
FROM
INTERRUPT
CONTEXT
IF
IT
IS
A
WARNING
IS
PRINTED
OUT
TO
THE
SYSTEM
LOG
STRUCT
UNSIGNED
INT
VENDOR
UNSIGNED
INT
DEVICE
UNSIGNED
INT
UNSIGNED
INT
STRUCT
FROM
THIS
FUNCTION
WORKS
JUST
LIKE
BUT
IT
ALLOWS
THE
SUBSYSTEM
VENDOR
AND
SUBSYSTEM
DEVICE
IDS
TO
BE
SPECIFIED
WHEN
LOOKING
FOR
THE
DEVICE
THIS
FUNCTION
CAN
NOT
BE
CALLED
FROM
INTERRUPT
CONTEXT
IF
IT
IS
A
WARNING
IS
PRINTED
OUT
TO
THE
SYSTEM
LOG
STRUCT
STRUCT
BUS
UNSIGNED
INT
DEVFN
THIS
FUNCTION
SEARCHES
THE
LIST
OF
PCI
DEVICES
IN
THE
SYSTEM
ON
THE
SPECIFIED
STRUCT
FOR
THE
SPECIFIED
DEVICE
AND
FUNCTION
NUMBER
OF
THE
PCI
DEVICE
IF
A
DEVICE
IS
FOUND
THAT
MATCHES
ITS
REFERENCE
COUNT
IS
INCREMENTED
AND
A
POINTER
TO
IT
IS
RETURNED
WHEN
THE
CALLER
IS
FINISHED
ACCESSING
THE
STRUCT
IT
MUST
CALL
ALL
OF
THESE
FUNCTIONS
CAN
NOT
BE
CALLED
FROM
INTERRUPT
CONTEXT
IF
THEY
ARE
A
WARNING
IS
PRINTED
OUT
TO
THE
SYSTEM
LOG
ENABLING
THE
PCI
DEVICE
IN
THE
PROBE
FUNCTION
FOR
THE
PCI
DRIVER
BEFORE
THE
DRIVER
CAN
ACCESS
ANY
DEVICE
RESOURCE
I
O
REGION
OR
INTERRUPT
OF
THE
PCI
DEVICE
THE
DRIVER
MUST
CALL
THE
FUNCTION
INT
STRUCT
DEV
THIS
FUNCTION
ACTUALLY
ENABLES
THE
DEVICE
IT
WAKES
UP
THE
DEVICE
AND
IN
SOME
CASES
ALSO
ASSIGNS
ITS
INTERRUPT
LINE
AND
I
O
REGIONS
THIS
HAPPENS
FOR
EXAMPLE
WITH
CARDBUS
DEVICES
WHICH
HAVE
BEEN
MADE
COMPLETELY
EQUIVALENT
TO
PCI
AT
THE
DRIVER
LEVEL
ACCESSING
THE
CONFIGURATION
SPACE
AFTER
THE
DRIVER
HAS
DETECTED
THE
DEVICE
IT
USUALLY
NEEDS
TO
READ
FROM
OR
WRITE
TO
THE
THREE
ADDRESS
SPACES
MEMORY
PORT
AND
CONFIGURATION
IN
PARTICULAR
ACCESSING
THE
CONFIGURATION
SPACE
IS
VITAL
TO
THE
DRIVER
BECAUSE
IT
IS
THE
ONLY
WAY
IT
CAN
FIND
OUT
WHERE
THE
DEVICE
IS
MAPPED
IN
MEMORY
AND
IN
THE
I
O
SPACE
BECAUSE
THE
MICROPROCESSOR
HAS
NO
WAY
TO
ACCESS
THE
CONFIGURATION
SPACE
DIRECTLY
THE
COMPUTER
VENDOR
HAS
TO
PROVIDE
A
WAY
TO
DO
IT
TO
ACCESS
CONFIGURATION
SPACE
THE
CPU
MUST
WRITE
AND
READ
REGISTERS
IN
THE
PCI
CONTROLLER
BUT
THE
EXACT
IMPLEMEN
TATION
IS
VENDOR
DEPENDENT
AND
NOT
RELEVANT
TO
THIS
DISCUSSION
BECAUSE
LINUX
OFFERS
A
STANDARD
INTERFACE
TO
ACCESS
THE
CONFIGURATION
SPACE
AS
FAR
AS
THE
DRIVER
IS
CONCERNED
THE
CONFIGURATION
SPACE
CAN
BE
ACCESSED
THROUGH
BIT
BIT
OR
BIT
DATA
TRANSFERS
THE
RELEVANT
FUNCTIONS
ARE
PROTOTYPED
IN
LINUX
PCI
H
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
READ
ONE
TWO
OR
FOUR
BYTES
FROM
THE
CONFIGURATION
SPACE
OF
THE
DEVICE
IDENTI
FIED
BY
DEV
THE
WHERE
ARGUMENT
IS
THE
BYTE
OFFSET
FROM
THE
BEGINNING
OF
THE
CON
FIGURATION
SPACE
THE
VALUE
FETCHED
FROM
THE
CONFIGURATION
SPACE
IS
RETURNED
THROUGH
THE
VAL
POINTER
AND
THE
RETURN
VALUE
OF
THE
FUNCTIONS
IS
AN
ERROR
CODE
THE
WORD
AND
DWORD
FUNCTIONS
CONVERT
THE
VALUE
JUST
READ
FROM
LITTLE
ENDIAN
TO
THE
NATIVE
BYTE
ORDER
OF
THE
PROCESSOR
SO
YOU
NEED
NOT
DEAL
WITH
BYTE
ORDERING
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
WRITE
ONE
TWO
OR
FOUR
BYTES
TO
THE
CONFIGURATION
SPACE
THE
DEVICE
IS
IDENTIFIED
BY
DEV
AS
USUAL
AND
THE
VALUE
BEING
WRITTEN
IS
PASSED
AS
VAL
THE
WORD
AND
DWORD
FUNCTIONS
CONVERT
THE
VALUE
TO
LITTLE
ENDIAN
BEFORE
WRITING
TO
THE
PERIPH
ERAL
DEVICE
ALL
OF
THE
PREVIOUS
FUNCTIONS
ARE
IMPLEMENTED
AS
INLINE
FUNCTIONS
THAT
REALLY
CALL
THE
FOLLOWING
FUNCTIONS
FEEL
FREE
TO
USE
THESE
FUNCTIONS
INSTEAD
OF
THE
ABOVE
IN
CASE
THE
DRIVER
DOES
NOT
HAVE
ACCESS
TO
A
STRUCT
AT
ANY
PATICULAR
MOMENT
IN
TIME
INT
STRUCT
BUS
UNSIGNED
INT
DEVFN
INT
WHERE
VAL
INT
STRUCT
BUS
UNSIGNED
INT
DEVFN
INT
WHERE
VAL
INT
STRUCT
BUS
UNSIGNED
INT
DEVFN
INT
WHERE
VAL
JUST
LIKE
THE
FUNCTIONS
BUT
STRUCT
AND
DEVFN
VARIABLES
ARE
NEEDED
INSTEAD
OF
A
STRUCT
INT
STRUCT
BUS
UNSIGNED
INT
DEVFN
INT
WHERE
VAL
INT
STRUCT
BUS
UNSIGNED
INT
DEVFN
INT
WHERE
VAL
INT
STRUCT
BUS
UNSIGNED
INT
DEVFN
INT
WHERE
VAL
JUST
LIKE
THE
FUNCTIONS
BUT
STRUCT
AND
DEVFN
VARIABLES
ARE
NEEDED
INSTEAD
OF
A
STRUCT
THE
BEST
WAY
TO
ADDRESS
THE
CONFIGURATION
VARIABLES
USING
THE
FUNCTIONS
IS
BY
MEANS
OF
THE
SYMBOLIC
NAMES
DEFINED
IN
LINUX
PCI
H
FOR
EXAMPLE
THE
FOLLOW
ING
SMALL
FUNCTION
RETRIEVES
THE
REVISION
ID
OF
A
DEVICE
BY
PASSING
THE
SYMBOLIC
NAME
FOR
WHERE
TO
STATIC
UNSIGNED
CHAR
STRUCT
DEV
REVISION
DEV
REVISION
RETURN
REVISION
ACCESSING
THE
I
O
AND
MEMORY
SPACES
A
PCI
DEVICE
IMPLEMENTS
UP
TO
SIX
I
O
ADDRESS
REGIONS
EACH
REGION
CONSISTS
OF
EITHER
MEMORY
OR
I
O
LOCATIONS
MOST
DEVICES
IMPLEMENT
THEIR
I
O
REGISTERS
IN
MEMORY
REGIONS
BECAUSE
IT
GENERALLY
A
SANER
APPROACH
AS
EXPLAINED
IN
THE
SECTION
I
O
PORTS
AND
I
O
MEMORY
IN
CHAPTER
HOWEVER
UNLIKE
NORMAL
MEMORY
I
O
REGIS
TERS
SHOULD
NOT
BE
CACHED
BY
THE
CPU
BECAUSE
EACH
ACCESS
CAN
HAVE
SIDE
EFFECTS
THE
PCI
DEVICE
THAT
IMPLEMENTS
I
O
REGISTERS
AS
A
MEMORY
REGION
MARKS
THE
DIFFERENCE
BY
SETTING
A
MEMORY
IS
PREFETCHABLE
BIT
IN
ITS
CONFIGURATION
REGISTER
IF
THE
MEMORY
REGION
IS
MARKED
AS
PREFETCHABLE
THE
CPU
CAN
CACHE
ITS
CONTENTS
AND
DO
ALL
SORTS
OF
OPTIMIZATION
WITH
IT
NONPREFETCHABLE
MEMORY
ACCESS
ON
THE
OTHER
HAND
CAN
T
BE
OPTIMIZED
BECAUSE
EACH
ACCESS
CAN
HAVE
SIDE
EFFECTS
JUST
AS
WITH
I
O
PORTS
PERIPHER
ALS
THAT
MAP
THEIR
CONTROL
REGISTERS
TO
A
MEMORY
ADDRESS
RANGE
DECLARE
THAT
RANGE
AS
NONPREFETCHABLE
WHEREAS
SOMETHING
LIKE
VIDEO
MEMORY
ON
PCI
BOARDS
IS
PREFETCH
ABLE
IN
THIS
SECTION
WE
USE
THE
WORD
REGION
TO
REFER
TO
A
GENERIC
I
O
ADDRESS
SPACE
THAT
IS
MEMORY
MAPPED
OR
PORT
MAPPED
AN
INTERFACE
BOARD
REPORTS
THE
SIZE
AND
CURRENT
LOCATION
OF
ITS
REGIONS
USING
CONFIGURA
TION
REGISTERS
THE
SIX
BIT
REGISTERS
SHOWN
IN
FIGURE
WHOSE
SYMBOLIC
NAMES
ARE
THROUGH
SINCE
THE
I
O
SPACE
DEFINED
BY
PCI
IS
A
BIT
ADDRESS
SPACE
IT
MAKES
SENSE
TO
USE
THE
SAME
CONFIGURATION
INTERFACE
THE
INFORMATION
LIVES
IN
ONE
OF
THE
LOW
ORDER
BITS
OF
THE
BASE
ADDRESS
PCI
REGISTERS
THE
BITS
ARE
DEFINED
IN
LINUX
PCI
H
FOR
MEMORY
AND
I
O
IF
THE
DEVICE
USES
A
BIT
ADDRESS
BUS
IT
CAN
DECLARE
REGIONS
IN
THE
BIT
MEMORY
SPACE
BY
USING
TWO
CONSECUTIVE
REGISTERS
FOR
EACH
REGION
LOW
BITS
FIRST
IT
IS
POSSIBLE
FOR
ONE
DEVICE
TO
OFFER
BOTH
BIT
REGIONS
AND
BIT
REGIONS
IN
THE
KERNEL
THE
I
O
REGIONS
OF
PCI
DEVICES
HAVE
BEEN
INTEGRATED
INTO
THE
GENERIC
RESOURCE
MANAGEMENT
FOR
THIS
REASON
YOU
DON
T
NEED
TO
ACCESS
THE
CONFIGURATION
VARIABLES
IN
ORDER
TO
KNOW
WHERE
YOUR
DEVICE
IS
MAPPED
IN
MEMORY
OR
I
O
SPACE
THE
PREFERRED
INTERFACE
FOR
GETTING
REGION
INFORMATION
CONSISTS
OF
THE
FOLLOWING
FUNCTIONS
UNSIGNED
LONG
STRUCT
DEV
INT
BAR
THE
FUNCTION
RETURNS
THE
FIRST
ADDRESS
MEMORY
ADDRESS
OR
I
O
PORT
NUMBER
ASSOCIATED
WITH
ONE
OF
THE
SIX
PCI
I
O
REGIONS
THE
REGION
IS
SELECTED
BY
THE
INTE
GER
BAR
THE
BASE
ADDRESS
REGISTER
RANGING
FROM
INCLUSIVE
UNSIGNED
LONG
STRUCT
DEV
INT
BAR
THE
FUNCTION
RETURNS
THE
LAST
ADDRESS
THAT
IS
PART
OF
THE
I
O
REGION
NUMBER
BAR
NOTE
THAT
THIS
IS
THE
LAST
USABLE
ADDRESS
NOT
THE
FIRST
ADDRESS
AFTER
THE
REGION
UNSIGNED
LONG
STRUCT
DEV
INT
BAR
THIS
FUNCTION
RETURNS
THE
FLAGS
ASSOCIATED
WITH
THIS
RESOURCE
RESOURCE
FLAGS
ARE
USED
TO
DEFINE
SOME
FEATURES
OF
THE
INDIVIDUAL
RESOURCE
FOR
PCI
RESOURCES
ASSOCIATED
WITH
PCI
I
O
REGIONS
THE
INFORMATION
IS
EXTRACTED
FROM
THE
BASE
ADDRESS
REGISTERS
BUT
CAN
COME
FROM
ELSEWHERE
FOR
RESOURCES
NOT
ASSOCIATED
WITH
PCI
DEVICES
ALL
RESOURCE
FLAGS
ARE
DEFINED
IN
LINUX
IOPORT
H
THE
MOST
IMPORTANT
ARE
IF
THE
ASSOCIATED
I
O
REGION
EXISTS
ONE
AND
ONLY
ONE
OF
THESE
FLAGS
IS
SET
THESE
FLAGS
TELL
WHETHER
A
MEMORY
REGION
IS
PREFETCHABLE
AND
OR
WRITE
PROTECTED
THE
LATTER
FLAG
IS
NEVER
SET
FOR
PCI
RESOURCES
BY
MAKING
USE
OF
THE
FUNCTIONS
A
DEVICE
DRIVER
CAN
COMPLETELY
IGNORE
THE
UNDERLYING
PCI
REGISTERS
SINCE
THE
SYSTEM
ALREADY
USED
THEM
TO
STRUCTURE
RESOURCE
INFORMATION
PCI
INTERRUPTS
AS
FAR
AS
INTERRUPTS
ARE
CONCERNED
PCI
IS
EASY
TO
HANDLE
BY
THE
TIME
LINUX
BOOTS
THE
COMPUTER
FIRMWARE
HAS
ALREADY
ASSIGNED
A
UNIQUE
INTERRUPT
NUMBER
TO
THE
DEVICE
AND
THE
DRIVER
JUST
NEEDS
TO
USE
IT
THE
INTERRUPT
NUMBER
IS
STORED
IN
CONFIGU
RATION
REGISTER
WHICH
IS
ONE
BYTE
WIDE
THIS
ALLOWS
FOR
AS
MANY
AS
INTERRUPT
LINES
BUT
THE
ACTUAL
LIMIT
DEPENDS
ON
THE
CPU
BEING
USED
THE
DRIVER
DOESN
T
NEED
TO
BOTHER
CHECKING
THE
INTERRUPT
NUMBER
BECAUSE
THE
VALUE
FOUND
IN
IS
GUARANTEED
TO
BE
THE
RIGHT
ONE
IF
THE
DEVICE
DOESN
T
SUPPORT
INTERRUPTS
REGISTER
IS
OTHER
WISE
IT
NONZERO
HOWEVER
SINCE
THE
DRIVER
KNOWS
IF
ITS
DEVICE
IS
INTERRUPT
DRIVEN
OR
NOT
IT
DOESN
T
USUALLY
NEED
TO
READ
THUS
PCI
SPECIFIC
CODE
FOR
DEALING
WITH
INTERRUPTS
JUST
NEEDS
TO
READ
THE
CONFIGURA
TION
BYTE
TO
OBTAIN
THE
INTERRUPT
NUMBER
THAT
IS
SAVED
IN
A
LOCAL
VARIABLE
AS
SHOWN
IN
THE
FOLLOWING
CODE
BEYOND
THAT
THE
INFORMATION
IN
CHAPTER
APPLIES
RESULT
DEV
MYIRQ
IF
RESULT
DEAL
WITH
ERROR
THE
REST
OF
THIS
SECTION
PROVIDES
ADDITIONAL
INFORMATION
FOR
THE
CURIOUS
READER
BUT
ISN
T
NEEDED
FOR
WRITING
DRIVERS
A
PCI
CONNECTOR
HAS
FOUR
INTERRUPT
PINS
AND
PERIPHERAL
BOARDS
CAN
USE
ANY
OR
ALL
OF
THEM
EACH
PIN
IS
INDIVIDUALLY
ROUTED
TO
THE
MOTHERBOARD
INTERRUPT
CONTROLLER
SO
INTERRUPTS
CAN
BE
SHARED
WITHOUT
ANY
ELECTRICAL
PROBLEMS
THE
INTERRUPT
CONTROLLER
IS
THEN
RESPONSIBLE
FOR
MAPPING
THE
INTERRUPT
WIRES
PINS
TO
THE
PROCESSOR
HARDWARE
THIS
PLATFORM
DEPENDENT
OPERATION
IS
LEFT
TO
THE
CONTROLLER
IN
ORDER
TO
ACHIEVE
PLAT
FORM
INDEPENDENCE
IN
THE
BUS
ITSELF
THE
READ
ONLY
CONFIGURATION
REGISTER
LOCATED
AT
IS
USED
TO
TELL
THE
COMPUTER
WHICH
SINGLE
PIN
IS
ACTUALLY
USED
IT
WORTH
REMEMBERING
THAT
EACH
DEVICE
BOARD
CAN
HOST
UP
TO
EIGHT
DEVICES
EACH
DEVICE
USES
A
SINGLE
INTERRUPT
PIN
AND
REPORTS
IT
IN
ITS
OWN
CONFIGURATION
REGISTER
DIFFERENT
DEVICES
ON
THE
SAME
DEVICE
BOARD
CAN
USE
DIFFERENT
INTERRUPT
PINS
OR
SHARE
THE
SAME
ONE
THE
REGISTER
ON
THE
OTHER
HAND
IS
READ
WRITE
WHEN
THE
COM
PUTER
IS
BOOTED
THE
FIRMWARE
SCANS
ITS
PCI
DEVICES
AND
SETS
THE
REGISTER
FOR
EACH
DEVICE
ACCORDING
TO
HOW
THE
INTERRUPT
PIN
IS
ROUTED
FOR
ITS
PCI
SLOT
THE
VALUE
IS
ASSIGNED
BY
THE
FIRMWARE
BECAUSE
ONLY
THE
FIRMWARE
KNOWS
HOW
THE
MOTHERBOARD
ROUTES
THE
DIFFERENT
INTERRUPT
PINS
TO
THE
PROCESSOR
FOR
THE
DEVICE
DRIVER
HOWEVER
THE
REGISTER
IS
READ
ONLY
INTERESTINGLY
RECENT
VERSIONS
OF
THE
LINUX
KERNEL
UNDER
SOME
CIRCUMSTANCES
CAN
ASSIGN
INTERRUPT
LINES
WITHOUT
RESORTING
TO
THE
BIOS
HARDWARE
ABSTRACTIONS
WE
COMPLETE
THE
DISCUSSION
OF
PCI
BY
TAKING
A
QUICK
LOOK
AT
HOW
THE
SYSTEM
HAN
DLES
THE
PLETHORA
OF
PCI
CONTROLLERS
AVAILABLE
ON
THE
MARKETPLACE
THIS
IS
JUST
AN
INFORMATIONAL
SECTION
MEANT
TO
SHOW
THE
CURIOUS
READER
HOW
THE
OBJECT
ORIENTED
LAY
OUT
OF
THE
KERNEL
EXTENDS
DOWN
TO
THE
LOWEST
LEVELS
THE
MECHANISM
USED
TO
IMPLEMENT
HARDWARE
ABSTRACTION
IS
THE
USUAL
STRUCTURE
CON
TAINING
METHODS
IT
A
POWERFUL
TECHNIQUE
THAT
ADDS
JUST
THE
MINIMAL
OVERHEAD
OF
DEREFERENCING
A
POINTER
TO
THE
NORMAL
OVERHEAD
OF
A
FUNCTION
CALL
IN
THE
CASE
OF
PCI
MANAGEMENT
THE
ONLY
HARDWARE
DEPENDENT
OPERATIONS
ARE
THE
ONES
THAT
READ
AND
WRITE
CONFIGURATION
REGISTERS
BECAUSE
EVERYTHING
ELSE
IN
THE
PCI
WORLD
IS
ACCOM
PLISHED
BY
DIRECTLY
READING
AND
WRITING
THE
I
O
AND
MEMORY
ADDRESS
SPACES
AND
THOSE
ARE
UNDER
DIRECT
CONTROL
OF
THE
CPU
THUS
THE
RELEVANT
STRUCTURE
FOR
CONFIGURATION
REGISTER
ACCESS
INCLUDES
ONLY
TWO
FIELDS
STRUCT
INT
READ
STRUCT
BUS
UNSIGNED
INT
DEVFN
INT
WHERE
INT
SIZE
VAL
INT
WRITE
STRUCT
BUS
UNSIGNED
INT
DEVFN
INT
WHERE
INT
SIZE
VAL
THE
STRUCTURE
IS
DEFINED
IN
LINUX
PCI
H
AND
USED
BY
DRIVERS
PCI
PCI
C
WHERE
THE
ACTUAL
PUBLIC
FUNCTIONS
ARE
DEFINED
THE
TWO
FUNCTIONS
THAT
ACT
ON
THE
PCI
CONFIGURATION
SPACE
HAVE
MORE
OVERHEAD
THAN
DEREFERENCING
A
POINTER
THEY
USE
CASCADING
POINTERS
DUE
TO
THE
HIGH
OBJECT
ORIENTEDNESS
OF
THE
CODE
BUT
THE
OVERHEAD
IS
NOT
AN
ISSUE
IN
OPERATIONS
THAT
ARE
PERFORMED
QUITE
RARELY
AND
NEVER
IN
SPEED
CRITICAL
PATHS
THE
ACTUAL
IMPLEMENTA
TION
OF
DEV
WHERE
VAL
FOR
INSTANCE
EXPANDS
TO
DEV
BUS
OPS
READ
BUS
DEVFN
WHERE
VAL
THE
VARIOUS
PCI
BUSES
IN
THE
SYSTEM
ARE
DETECTED
AT
SYSTEM
BOOT
AND
THAT
WHEN
THE
STRUCT
ITEMS
ARE
CREATED
AND
ASSOCIATED
WITH
THEIR
FEATURES
INCLUDING
THE
OPS
FIELD
IMPLEMENTING
HARDWARE
ABSTRACTION
VIA
HARDWARE
OPERATIONS
DATA
STRUCTURES
IS
TYP
ICAL
IN
THE
LINUX
KERNEL
ONE
IMPORTANT
EXAMPLE
IS
THE
STRUCT
DATA
STRUCTURE
IT
IS
DEFINED
IN
ASM
ALPHA
MACHVEC
H
AND
TAKES
CARE
OF
EVERYTHING
THAT
MAY
CHANGE
ACROSS
DIFFERENT
ALPHA
BASED
COMPUTERS
A
LOOK
BACK
ISA
THE
ISA
BUS
IS
QUITE
OLD
IN
DESIGN
AND
IS
A
NOTORIOUSLY
POOR
PERFORMER
BUT
IT
STILL
HOLDS
A
GOOD
PART
OF
THE
MARKET
FOR
EXTENSION
DEVICES
IF
SPEED
IS
NOT
IMPORTANT
AND
YOU
WANT
TO
SUPPORT
OLD
MOTHERBOARDS
AN
ISA
IMPLEMENTATION
IS
PREFERABLE
TO
PCI
AN
ADDITIONAL
ADVANTAGE
OF
THIS
OLD
STANDARD
IS
THAT
IF
YOU
ARE
AN
ELECTRONIC
HOBBYIST
YOU
CAN
EASILY
BUILD
YOUR
OWN
ISA
DEVICES
SOMETHING
DEFINITELY
NOT
POSSIBLE
WITH
PCI
ON
THE
OTHER
HAND
A
GREAT
DISADVANTAGE
OF
ISA
IS
THAT
IT
TIGHTLY
BOUND
TO
THE
PC
ARCHITECTURE
THE
INTERFACE
BUS
HAS
ALL
THE
LIMITATIONS
OF
THE
PROCESSOR
AND
CAUSES
ENDLESS
PAIN
TO
SYSTEM
PROGRAMMERS
THE
OTHER
GREAT
PROBLEM
WITH
THE
ISA
DESIGN
INHERITED
FROM
THE
ORIGINAL
IBM
PC
IS
THE
LACK
OF
GEOGRAPHICAL
ADDRESSING
WHICH
HAS
LED
TO
MANY
PROBLEMS
AND
LENGTHY
UNPLUG
REJUMPER
PLUG
TEST
CYCLES
TO
ADD
NEW
DEVICES
IT
INTERESTING
TO
NOTE
THAT
EVEN
THE
OLDEST
APPLE
II
COMPUTERS
WERE
ALREADY
EXPLOITING
GEOGRAPHICAL
ADDRESSING
AND
THEY
FEATURED
JUMPERLESS
EXPANSION
BOARDS
DESPITE
ITS
GREAT
DISADVANTAGES
ISA
IS
STILL
USED
IN
SEVERAL
UNEXPECTED
PLACES
FOR
EXAMPLE
THE
SERIES
OF
MIPS
PROCESSORS
USED
IN
SEVERAL
PALMTOPS
FEATURES
AN
ISA
COMPATIBLE
EXPANSION
BUS
STRANGE
AS
IT
SEEMS
THE
REASON
BEHIND
THESE
UNEX
PECTED
USES
OF
ISA
IS
THE
EXTREME
LOW
COST
OF
SOME
LEGACY
HARDWARE
SUCH
AS
BASED
ETHERNET
CARDS
SO
A
CPU
WITH
ISA
ELECTRICAL
SIGNALING
CAN
EASILY
EXPLOIT
THE
AWFUL
BUT
CHEAP
PC
DEVICES
HARDWARE
RESOURCES
AN
ISA
DEVICE
CAN
BE
EQUIPPED
WITH
I
O
PORTS
MEMORY
AREAS
AND
INTERRUPT
LINES
EVEN
THOUGH
THE
PROCESSORS
SUPPORT
KB
OF
I
O
PORT
MEMORY
I
E
THE
PROCES
SOR
ASSERTS
ADDRESS
LINES
SOME
OLD
PC
HARDWARE
DECODES
ONLY
THE
LOWEST
ADDRESS
LINES
THIS
LIMITS
THE
USABLE
ADDRESS
SPACE
TO
PORTS
BECAUSE
ANY
ADDRESS
IN
THE
RANGE
KB
TO
KB
IS
MISTAKEN
FOR
A
LOW
ADDRESS
BY
ANY
DEVICE
THAT
DECODES
ONLY
THE
LOW
ADDRESS
LINES
SOME
PERIPHERALS
CIRCUMVENT
THIS
LIMITATION
BY
MAPPING
ONLY
ONE
PORT
INTO
THE
LOW
KILOBYTE
AND
USING
THE
HIGH
ADDRESS
LINES
TO
SELECT
BETWEEN
DIFFERENT
DEVICE
REGISTERS
FOR
EXAMPLE
A
DEVICE
MAPPED
AT
CAN
SAFELY
USE
PORT
AND
SO
ON
IF
THE
AVAILABILITY
OF
I
O
PORTS
IS
LIMITED
MEMORY
ACCESS
IS
STILL
WORSE
AN
ISA
DEVICE
CAN
USE
ONLY
THE
MEMORY
RANGE
BETWEEN
KB
AND
MB
AND
BETWEEN
MB
AND
MB
FOR
I
O
REGISTER
AND
DEVICE
CONTROL
THE
KB
TO
MB
RANGE
IS
USED
BY
THE
PC
BIOS
BY
VGA
COMPATIBLE
VIDEO
BOARDS
AND
BY
VARIOUS
OTHER
DEVICES
LEAVING
LIT
TLE
SPACE
AVAILABLE
FOR
NEW
DEVICES
MEMORY
AT
MB
ON
THE
OTHER
HAND
IS
NOT
DIRECTLY
SUPPORTED
BY
LINUX
AND
HACKING
THE
KERNEL
TO
SUPPORT
IT
IS
A
WASTE
OF
PRO
GRAMMING
TIME
NOWADAYS
THE
THIRD
RESOURCE
AVAILABLE
TO
ISA
DEVICE
BOARDS
IS
INTERRUPT
LINES
A
LIMITED
NUM
BER
OF
INTERRUPT
LINES
IS
ROUTED
TO
THE
ISA
BUS
AND
THEY
ARE
SHARED
BY
ALL
THE
INTERFACE
BOARDS
AS
A
RESULT
IF
DEVICES
AREN
T
PROPERLY
CONFIGURED
THEY
CAN
FIND
THEMSELVES
USING
THE
SAME
INTERRUPT
LINES
ALTHOUGH
THE
ORIGINAL
ISA
SPECIFICATION
DOESN
T
ALLOW
INTERRUPT
SHARING
ACROSS
DEVICES
MOST
DEVICE
BOARDS
ALLOW
IT
INTERRUPT
SHARING
AT
THE
SOFTWARE
LEVEL
IS
DESCRIBED
IN
THE
SECTION
INTERRUPT
SHARING
IN
CHAPTER
ISA
PROGRAMMING
AS
FAR
AS
PROGRAMMING
IS
CONCERNED
THERE
NO
SPECIFIC
AID
IN
THE
KERNEL
OR
THE
BIOS
TO
EASE
ACCESS
TO
ISA
DEVICES
LIKE
THERE
IS
FOR
EXAMPLE
FOR
PCI
THE
ONLY
FACILITIES
YOU
CAN
USE
ARE
THE
REGISTRIES
OF
I
O
PORTS
AND
IRQ
LINES
DESCRIBED
IN
THE
SECTION
INSTALLING
AN
INTERRUPT
HANDLER
IN
CHAPTER
THE
PROGRAMMING
TECHNIQUES
SHOWN
THROUGHOUT
THE
FIRST
PART
OF
THIS
BOOK
APPLY
TO
ISA
DEVICES
THE
DRIVER
CAN
PROBE
FOR
I
O
PORTS
AND
THE
INTERRUPT
LINE
MUST
BE
AUTO
DETECTED
WITH
ONE
OF
THE
TECHNIQUES
SHOWN
IN
THE
SECTION
AUTODETECTING
THE
IRQ
NUMBER
IN
CHAPTER
THE
HELPER
FUNCTIONS
AND
FRIENDS
HAVE
BEEN
BRIEFLY
INTRODUCED
IN
THE
SEC
TION
USING
I
O
MEMORY
IN
CHAPTER
AND
THERE
NOTHING
MORE
TO
SAY
ABOUT
THEM
THE
PLUG
AND
PLAY
SPECIFICATION
SOME
NEW
ISA
DEVICE
BOARDS
FOLLOW
PECULIAR
DESIGN
RULES
AND
REQUIRE
A
SPECIAL
INITIAL
IZATION
SEQUENCE
INTENDED
TO
SIMPLIFY
INSTALLATION
AND
CONFIGURATION
OF
ADD
ON
INTER
FACE
BOARDS
THE
SPECIFICATION
FOR
THE
DESIGN
OF
THESE
BOARDS
IS
CALLED
PLUG
AND
PLAY
PNP
AND
CONSISTS
OF
A
CUMBERSOME
RULE
SET
FOR
BUILDING
AND
CONFIGURING
JUMPERLESS
ISA
DEVICES
PNP
DEVICES
IMPLEMENT
RELOCATABLE
I
O
REGIONS
THE
PC
BIOS
IS
RESPON
SIBLE
FOR
THE
RELOCATION
REMINISCENT
OF
PCI
IN
SHORT
THE
GOAL
OF
PNP
IS
TO
OBTAIN
THE
SAME
FLEXIBILITY
FOUND
IN
PCI
DEVICES
WITH
OUT
CHANGING
THE
UNDERLYING
ELECTRICAL
INTERFACE
THE
ISA
BUS
TO
THIS
END
THE
SPECS
DEFINE
A
SET
OF
DEVICE
INDEPENDENT
CONFIGURATION
REGISTERS
AND
A
WAY
TO
GEOGRAPHI
CALLY
ADDRESS
THE
INTERFACE
BOARDS
EVEN
THOUGH
THE
PHYSICAL
BUS
DOESN
T
CARRY
PER
BOARD
GEOGRAPHICAL
WIRING
EVERY
ISA
SIGNAL
LINE
CONNECTS
TO
EVERY
AVAILABLE
SLOT
GEOGRAPHICAL
ADDRESSING
WORKS
BY
ASSIGNING
A
SMALL
INTEGER
CALLED
THE
CARD
SELECT
NUMBER
CSN
TO
EACH
PNP
PERIPHERAL
IN
THE
COMPUTER
EACH
PNP
DEVICE
FEATURES
A
UNIQUE
SERIAL
IDENTIFIER
BITS
WIDE
THAT
IS
HARDWIRED
INTO
THE
PERIPHERAL
BOARD
CSN
ASSIGNMENT
USES
THE
UNIQUE
SERIAL
NUMBER
TO
IDENTIFY
THE
PNP
DEVICES
BUT
THE
CSNS
CAN
BE
ASSIGNED
SAFELY
ONLY
AT
BOOT
TIME
WHICH
REQUIRES
THE
BIOS
TO
BE
PNP
THE
PROBLEM
WITH
INTERRUPT
SHARING
IS
A
MATTER
OF
ELECTRICAL
ENGINEERING
IF
A
DEVICE
DRIVES
THE
SIGNAL
LINE
INAC
TIVE
BY
APPLYING
A
LOW
IMPEDANCE
VOLTAGE
LEVEL
THE
INTERRUPT
CAN
T
BE
SHARED
IF
ON
THE
OTHER
HAND
THE
DEVICE
USES
A
PULL
UP
RESISTOR
TO
THE
INACTIVE
LOGIC
LEVEL
SHARING
IS
POSSIBLE
THIS
IS
THE
NORM
NOWADAYS
HOW
EVER
THERE
STILL
A
POTENTIAL
RISK
OF
LOSING
INTERRUPT
EVENTS
SINCE
ISA
INTERRUPTS
ARE
EDGE
TRIGGERED
INSTEAD
OF
LEVEL
TRIGGERED
EDGE
TRIGGERED
INTERRUPTS
ARE
EASIER
TO
IMPLEMENT
IN
HARDWARE
BUT
DON
T
LEND
THEMSELVES
TO
SAFE
SHARING
AWARE
FOR
THIS
REASON
OLD
COMPUTERS
REQUIRE
THE
USER
TO
OBTAIN
AND
INSERT
A
SPECIFIC
CONFIGURATION
DISKETTE
EVEN
IF
THE
DEVICE
IS
PNP
CAPABLE
INTERFACE
BOARDS
FOLLOWING
THE
PNP
SPECS
ARE
COMPLICATED
AT
THE
HARDWARE
LEVEL
THEY
ARE
MUCH
MORE
ELABORATE
THAN
PCI
BOARDS
AND
REQUIRE
COMPLEX
SOFTWARE
IT
NOT
UNUSUAL
TO
HAVE
DIFFICULTY
INSTALLING
THESE
DEVICES
AND
EVEN
IF
THE
INSTALLATION
GOES
WELL
YOU
STILL
FACE
THE
PERFORMANCE
CONSTRAINTS
AND
THE
LIMITED
I
O
SPACE
OF
THE
ISA
BUS
IT
MUCH
BETTER
TO
INSTALL
PCI
DEVICES
WHENEVER
POSSIBLE
AND
ENJOY
THE
NEW
TECHNOLOGY
INSTEAD
IF
YOU
ARE
INTERESTED
IN
THE
PNP
CONFIGURATION
SOFTWARE
YOU
CAN
BROWSE
DRIVERS
NET
C
WHOSE
PROBING
FUNCTION
DEALS
WITH
PNP
DEVICES
THE
KERNEL
SAW
A
LOT
OF
WORK
IN
THE
PNP
DEVICE
SUPPORT
AREA
SO
A
LOT
OF
THE
INFLEXIBLE
INTERFACES
HAVE
BEEN
CLEANED
UP
COMPARED
TO
PREVIOUS
KERNEL
RELEASES
PC
AND
PC
CURRENTLY
IN
THE
INDUSTRIAL
WORLD
TWO
BUS
ARCHITECTURES
ARE
QUITE
FASHIONABLE
PC
AND
PC
BOTH
ARE
STANDARD
IN
PC
CLASS
SINGLE
BOARD
COMPUTERS
BOTH
STANDARDS
REFER
TO
SPECIFIC
FORM
FACTORS
FOR
PRINTED
CIRCUIT
BOARDS
AS
WELL
AS
ELECTRICAL
MECHANICAL
SPECIFICATIONS
FOR
BOARD
INTERCONNECTIONS
THE
PRACTICAL
ADVAN
TAGE
OF
THESE
BUSES
IS
THAT
THEY
ALLOW
CIRCUIT
BOARDS
TO
BE
STACKED
VERTICALLY
USING
A
PLUG
AND
SOCKET
KIND
OF
CONNECTOR
ON
ONE
SIDE
OF
THE
DEVICE
THE
ELECTRICAL
AND
LOGICAL
LAYOUT
OF
THE
TWO
BUSES
IS
IDENTICAL
TO
ISA
PC
AND
PCI
PC
SO
SOFTWARE
WON
T
NOTICE
ANY
DIFFERENCE
BETWEEN
THE
USUAL
DESKTOP
BUSES
AND
THESE
TWO
OTHER
PC
BUSES
PCI
AND
ISA
ARE
THE
MOST
COMMONLY
USED
PERIPHERAL
INTERFACES
IN
THE
PC
WORLD
BUT
THEY
AREN
T
THE
ONLY
ONES
HERE
A
SUMMARY
OF
THE
FEATURES
OF
OTHER
BUSES
FOUND
IN
THE
PC
MARKET
MCA
MICRO
CHANNEL
ARCHITECTURE
MCA
IS
AN
IBM
STANDARD
USED
IN
PS
COMPUTERS
AND
SOME
LAPTOPS
AT
THE
HARDWARE
LEVEL
MICRO
CHANNEL
HAS
MORE
FEATURES
THAN
ISA
IT
SUPPORTS
MULTIMASTER
DMA
BIT
ADDRESS
AND
DATA
LINES
SHARED
INTERRUPT
LINES
AND
GEOGRAPHICAL
ADDRESSING
TO
ACCESS
PER
BOARD
CONFIGURATION
REGISTERS
SUCH
REGISTERS
ARE
CALLED
PROGRAMMABLE
OPTION
SELECT
POS
BUT
THEY
DON
T
HAVE
ALL
THE
FEATURES
OF
THE
PCI
REGISTERS
LINUX
SUPPORT
FOR
MICRO
CHANNEL
INCLUDES
FUNCTIONS
THAT
ARE
EXPORTED
TO
MODULES
A
DEVICE
DRIVER
CAN
READ
THE
INTEGER
VALUE
TO
SEE
IF
IT
IS
RUNNING
ON
A
MICRO
CHANNEL
COMPUTER
IF
THE
SYMBOL
IS
A
PREPROCESSOR
MACRO
THE
MACRO
MACRO
IS
DEFINED
AS
WELL
IF
IS
UNDEFINED
THEN
IS
AN
INTE
GER
VARIABLE
EXPORTED
TO
MODULARIZED
CODE
BOTH
AND
ARE
DEFINED
IN
ASM
PROCESSOR
H
EISA
THE
EXTENDED
ISA
EISA
BUS
IS
A
BIT
EXTENSION
TO
ISA
WITH
A
COMPATIBLE
INTER
FACE
CONNECTOR
ISA
DEVICE
BOARDS
CAN
BE
PLUGGED
INTO
AN
EISA
CONNECTOR
THE
ADDI
TIONAL
WIRES
ARE
ROUTED
UNDER
THE
ISA
CONTACTS
LIKE
PCI
AND
MCA
THE
EISA
BUS
IS
DESIGNED
TO
HOST
JUMPERLESS
DEVICES
AND
IT
HAS
THE
SAME
FEATURES
AS
MCA
BIT
ADDRESS
AND
DATA
LINES
MULTIMASTER
DMA
AND
SHARED
INTERRUPT
LINES
EISA
DEVICES
ARE
CONFIGURED
BY
SOFTWARE
BUT
THEY
DON
T
NEED
ANY
PARTICULAR
OPERATING
SYSTEM
SUPPORT
EISA
DRIVERS
ALREADY
EXIST
IN
THE
LINUX
KER
NEL
FOR
ETHERNET
DEVICES
AND
SCSI
CONTROLLERS
AN
EISA
DRIVER
CHECKS
THE
VALUE
TO
DETERMINE
IF
THE
HOST
COMPUTER
CARRIES
AN
EISA
BUS
LIKE
IS
EITHER
A
MACRO
OR
A
VARIABLE
DEPENDING
ON
WHETHER
IS
DEFINED
BOTH
SYMBOLS
ARE
DEFINED
IN
ASM
PROCESSOR
H
THE
KERNEL
HAS
FULL
EISA
SUPPORT
FOR
DEVICES
WITH
SYSFS
AND
RESOURCE
MANAGEMENT
FUNCTIONALITY
THIS
IS
LOCATED
IN
THE
DRIVERS
EISA
DIRECTORY
VLB
ANOTHER
EXTENSION
TO
ISA
IS
THE
VESA
LOCAL
BUS
VLB
INTERFACE
BUS
WHICH
EXTENDS
THE
ISA
CONNECTORS
BY
ADDING
A
THIRD
LENGTHWISE
SLOT
A
DEVICE
CAN
JUST
PLUG
INTO
THIS
EXTRA
CONNECTOR
WITHOUT
PLUGGING
IN
THE
TWO
ASSOCIATED
ISA
CONNECTORS
BECAUSE
THE
VLB
SLOT
DUPLICATES
ALL
IMPORTANT
SIGNALS
FROM
THE
ISA
CONNECTORS
SUCH
STANDAL
ONE
VLB
PERIPHERALS
NOT
USING
THE
ISA
SLOT
ARE
RARE
BECAUSE
MOST
DEVICES
NEED
TO
REACH
THE
BACK
PANEL
SO
THAT
THEIR
EXTERNAL
CONNECTORS
ARE
AVAILABLE
THE
VESA
BUS
IS
MUCH
MORE
LIMITED
IN
ITS
CAPABILITIES
THAN
THE
EISA
MCA
AND
PCI
BUSES
AND
IS
DISAPPEARING
FROM
THE
MARKET
NO
SPECIAL
KERNEL
SUPPORT
EXISTS
FOR
VLB
HOWEVER
BOTH
THE
LANCE
ETHERNET
DRIVER
AND
THE
IDE
DISK
DRIVER
IN
LINUX
CAN
DEAL
WITH
VLB
VERSIONS
OF
THEIR
DEVICES
SBUS
WHILE
MOST
COMPUTERS
NOWADAYS
ARE
EQUIPPED
WITH
A
PCI
OR
ISA
INTERFACE
BUS
MOST
OLDER
SPARC
BASED
WORKSTATIONS
USE
SBUS
TO
CONNECT
THEIR
PERIPHERALS
SBUS
IS
QUITE
AN
ADVANCED
DESIGN
ALTHOUGH
IT
HAS
BEEN
AROUND
FOR
A
LONG
TIME
IT
IS
MEANT
TO
BE
PROCESSOR
INDEPENDENT
EVEN
THOUGH
ONLY
SPARC
COMPUTERS
USE
IT
AND
IS
OPTIMIZED
FOR
I
O
PERIPHERAL
BOARDS
IN
OTHER
WORDS
YOU
CAN
T
PLUG
ADDITIONAL
RAM
INTO
SBUS
SLOTS
RAM
EXPANSION
BOARDS
HAVE
LONG
BEEN
FORGOTTEN
EVEN
IN
THE
ISA
WORLD
AND
PCI
DOES
NOT
SUPPORT
THEM
EITHER
THIS
OPTIMIZATION
IS
MEANT
TO
SIMPLIFY
THE
DESIGN
OF
BOTH
HARDWARE
DEVICES
AND
SYSTEM
SOFTWARE
AT
THE
EXPENSE
OF
SOME
ADDITIONAL
COMPLEXITY
IN
THE
MOTHERBOARD
THIS
I
O
BIAS
OF
THE
BUS
RESULTS
IN
PERIPHERALS
USING
VIRTUAL
ADDRESSES
TO
TRANSFER
DATA
THUS
BYPASSING
THE
NEED
TO
ALLOCATE
A
CONTIGUOUS
DMA
BUFFER
THE
MOTHERBOARD
IS
RESPONSIBLE
FOR
DECODING
THE
VIRTUAL
ADDRESSES
AND
MAPPING
THEM
TO
PHYSICAL
ADDRESSES
THIS
REQUIRES
ATTACHING
AN
MMU
MEMORY
MANAGEMENT
UNIT
TO
THE
BUS
THE
CHIPSET
IN
CHARGE
OF
THE
TASK
IS
CALLED
IOMMU
ALTHOUGH
SOMEHOW
MORE
COM
PLEX
THAN
USING
PHYSICAL
ADDRESSES
ON
THE
INTERFACE
BUS
THIS
DESIGN
IS
GREATLY
SIMPLI
FIED
BY
THE
FACT
THAT
SPARC
PROCESSORS
HAVE
ALWAYS
BEEN
DESIGNED
BY
KEEPING
THE
MMU
CORE
SEPARATE
FROM
THE
CPU
CORE
EITHER
PHYSICALLY
OR
AT
LEAST
CONCEPTUALLY
ACTUALLY
THIS
DESIGN
CHOICE
IS
SHARED
BY
OTHER
SMART
PROCESSOR
DESIGNS
AND
IS
BENEFI
CIAL
OVERALL
ANOTHER
FEATURE
OF
THIS
BUS
IS
THAT
DEVICE
BOARDS
EXPLOIT
MASSIVE
GEO
GRAPHICAL
ADDRESSING
SO
THERE
NO
NEED
TO
IMPLEMENT
AN
ADDRESS
DECODER
IN
EVERY
PERIPHERAL
OR
TO
DEAL
WITH
ADDRESS
CONFLICTS
SBUS
PERIPHERALS
USE
THE
FORTH
LANGUAGE
IN
THEIR
PROMS
TO
INITIALIZE
THEMSELVES
FORTH
WAS
CHOSEN
BECAUSE
THE
INTERPRETER
IS
LIGHTWEIGHT
AND
THEREFORE
CAN
BE
EASILY
IMPLEMENTED
IN
THE
FIRMWARE
OF
ANY
COMPUTER
SYSTEM
IN
ADDITION
THE
SBUS
SPECIFI
CATION
OUTLINES
THE
BOOT
PROCESS
SO
THAT
COMPLIANT
I
O
DEVICES
FIT
EASILY
INTO
THE
SYS
TEM
AND
ARE
RECOGNIZED
AT
SYSTEM
BOOT
THIS
WAS
A
GREAT
STEP
TO
SUPPORT
MULTI
PLATFORM
DEVICES
IT
A
COMPLETELY
DIFFERENT
WORLD
FROM
THE
PC
CENTRIC
ISA
STUFF
WE
WERE
USED
TO
HOWEVER
IT
DIDN
T
SUCCEED
FOR
A
VARIETY
OF
COMMERCIAL
REASONS
ALTHOUGH
CURRENT
KERNEL
VERSIONS
OFFER
QUITE
FULL
FEATURED
SUPPORT
FOR
SBUS
DEVICES
THE
BUS
IS
USED
SO
LITTLE
NOWADAYS
THAT
IT
NOT
WORTH
COVERING
IN
DETAIL
HERE
INTER
ESTED
READERS
CAN
LOOK
AT
SOURCE
FILES
IN
ARCH
SPARC
KERNEL
AND
ARCH
SPARC
MM
NUBUS
ANOTHER
INTERESTING
BUT
NEARLY
FORGOTTEN
INTERFACE
BUS
IS
NUBUS
IT
IS
FOUND
ON
OLDER
MAC
COMPUTERS
THOSE
WITH
THE
FAMILY
OF
CPUS
ALL
OF
THE
BUS
IS
MEMORY
MAPPED
LIKE
EVERYTHING
WITH
THE
AND
THE
DEVICES
ARE
ONLY
GEOGRAPHICALLY
ADDRESSED
THIS
IS
GOOD
AND
TYPICAL
OF
APPLE
AS
THE
MUCH
OLDER
APPLE
II
ALREADY
HAD
A
SIMILAR
BUS
LAYOUT
WHAT
IS
BAD
IS
THAT
IT
ALMOST
IMPOS
SIBLE
TO
FIND
DOCUMENTATION
ON
NUBUS
DUE
TO
THE
CLOSE
EVERYTHING
POLICY
APPLE
HAS
ALWAYS
FOLLOWED
WITH
ITS
MAC
COMPUTERS
AND
UNLIKE
THE
PREVIOUS
APPLE
II
WHOSE
SOURCE
CODE
AND
SCHEMATICS
WERE
AVAILABLE
AT
LITTLE
COST
THE
FILE
DRIVERS
NUBUS
NUBUS
C
INCLUDES
ALMOST
EVERYTHING
WE
KNOW
ABOUT
THIS
BUS
AND
IT
INTERESTING
READING
IT
SHOWS
HOW
MUCH
HARD
REVERSE
ENGINEERING
DEVELOPERS
HAD
TO
DO
EXTERNAL
BUSES
ONE
OF
THE
MOST
RECENT
ENTRIES
IN
THE
FIELD
OF
INTERFACE
BUSES
IS
THE
WHOLE
CLASS
OF
EXTERNAL
BUSES
THIS
INCLUDES
USB
FIREWIRE
AND
PARALLEL
PORT
BASED
EXTERNAL
BUS
THESE
INTERFACES
ARE
SOMEWHAT
SIMILAR
TO
OLDER
AND
NOT
SO
EXTERNAL
TECHNOLOGY
SUCH
AS
PCMCIA
CARDBUS
AND
EVEN
SCSI
CONCEPTUALLY
THESE
BUSES
ARE
NEITHER
FULL
FEATURED
INTERFACE
BUSES
LIKE
PCI
IS
NOR
DUMB
COMMUNICATION
CHANNELS
LIKE
THE
SERIAL
PORTS
ARE
IT
HARD
TO
CLASSIFY
THE
SOFTWARE
THAT
IS
NEEDED
TO
EXPLOIT
THEIR
FEATURES
AS
IT
USUALLY
SPLIT
INTO
TWO
LEVELS
THE
DRIVER
FOR
THE
HARDWARE
CONTROLLER
LIKE
DRIVERS
FOR
PCI
SCSI
ADAPTORS
OR
PCI
CON
TROLLERS
INTRODUCED
IN
THE
SECTION
THE
PCI
INTERFACE
AND
THE
DRIVER
FOR
THE
SPECIFIC
CLIENT
DEVICE
LIKE
SD
C
HANDLES
GENERIC
SCSI
DISKS
AND
SO
CALLED
PCI
DRIVERS
DEAL
WITH
CARDS
PLUGGED
IN
THE
BUS
QUICK
REFERENCE
THIS
SECTION
SUMMARIZES
THE
SYMBOLS
INTRODUCED
IN
THE
CHAPTER
INCLUDE
LINUX
PCI
H
HEADER
THAT
INCLUDES
SYMBOLIC
NAMES
FOR
THE
PCI
REGISTERS
AND
SEVERAL
VENDOR
AND
DEVICE
ID
VALUES
STRUCT
STRUCTURE
THAT
REPRESENTS
A
PCI
DEVICE
WITHIN
THE
KERNEL
STRUCT
STRUCTURE
THAT
REPRESENTS
A
PCI
DRIVER
ALL
PCI
DRIVERS
MUST
DEFINE
THIS
STRUCT
STRUCTURE
THAT
DESCRIBES
THE
TYPES
OF
PCI
DEVICES
THIS
DRIVER
SUPPORTS
INT
STRUCT
DRV
INT
STRUCT
DRV
VOID
STRUCT
DRV
FUNCTIONS
THAT
REGISTER
OR
UNREGISTER
A
PCI
DRIVER
FROM
THE
KERNEL
STRUCT
UNSIGNED
INT
VENDOR
UNSIGNED
INT
DEVICE
STRUCT
FROM
STRUCT
UNSIGNED
INT
VENDOR
UNSIGNED
INT
DEVICE
CONST
STRUCT
FROM
STRUCT
UNSIGNED
INT
VENDOR
UNSIGNED
INT
DEVICE
UNSIGNED
INT
UNSIGNED
INT
CONST
STRUCT
FROM
STRUCT
UNSIGNED
INT
CLASS
STRUCT
FROM
FUNCTIONS
THAT
SEARCH
THE
DEVICE
LIST
FOR
DEVICES
WITH
A
SPECIFIC
SIGNATURE
OR
THOSE
BELONGING
TO
A
SPECIFIC
CLASS
THE
RETURN
VALUE
IS
NULL
IF
NONE
IS
FOUND
FROM
IS
USED
TO
CONTINUE
A
SEARCH
IT
MUST
BE
NULL
THE
FIRST
TIME
YOU
CALL
EITHER
FUNCTION
AND
IT
MUST
POINT
TO
THE
DEVICE
JUST
FOUND
IF
YOU
ARE
SEARCHING
FOR
MORE
DEVICES
THESE
FUNCTIONS
ARE
NOT
RECOMMENDED
TO
BE
USED
USE
THE
VARIANTS
INSTEAD
STRUCT
UNSIGNED
INT
VENDOR
UNSIGNED
INT
DEVICE
STRUCT
FROM
STRUCT
UNSIGNED
INT
VENDOR
UNSIGNED
INT
DEVICE
UNSIGNED
INT
UNSIGNED
INT
STRUCT
FROM
STRUCT
STRUCT
BUS
UNSIGNED
INT
DEVFN
FUNCTIONS
THAT
SEARCH
THE
DEVICE
LIST
FOR
DEVICES
WITH
A
SPECIFIC
SIGNATURE
OR
BELONGING
TO
A
SPECIFIC
CLASS
THE
RETURN
VALUE
IS
NULL
IF
NONE
IS
FOUND
FROM
IS
USED
TO
CONTINUE
A
SEARCH
IT
MUST
BE
NULL
THE
FIRST
TIME
YOU
CALL
EITHER
FUNCTION
AND
IT
MUST
POINT
TO
THE
DEVICE
JUST
FOUND
IF
YOU
ARE
SEARCHING
FOR
MORE
DEVICES
THE
STRUCTURE
RETURNED
HAS
ITS
REFERENCE
COUNT
INCREMENTED
AND
AFTER
THE
CALLER
IS
FINISHED
WITH
IT
THE
FUNCTION
MUST
BE
CALLED
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
INT
STRUCT
DEV
INT
WHERE
VAL
FUNCTIONS
THAT
READ
OR
WRITE
A
PCI
CONFIGURATION
REGISTER
ALTHOUGH
THE
LINUX
KERNEL
TAKES
CARE
OF
BYTE
ORDERING
THE
PROGRAMMER
MUST
BE
CAREFUL
ABOUT
BYTE
ORDERING
WHEN
ASSEMBLING
MULTIBYTE
VALUES
FROM
INDIVIDUAL
BYTES
THE
PCI
BUS
IS
LITTLE
ENDIAN
INT
STRUCT
DEV
ENABLES
A
PCI
DEVICE
UNSIGNED
LONG
STRUCT
DEV
INT
BAR
UNSIGNED
LONG
STRUCT
DEV
INT
BAR
UNSIGNED
LONG
STRUCT
DEV
INT
BAR
FUNCTIONS
THAT
HANDLE
PCI
DEVICE
RESOURCES
PROGRAMMING
PRINCIPLES
AND
PRACTICE
TERM
FALL
SYLLABUS
INDEX
DESCRIPTION
PREREQUISITES
SCHEDULING
WEBSITE
INSTRUCTOR
INFORMATION
COURSE
OBJECTIVES
STUDENT
EVALUATION
TEXTS
AND
LIBRARY
MATERIALS
TOPIC
OUTLINE
LECTURESCHEDULE
INSTRUCTIONAL
LABORATORY
ASSIGNMENTS
LABORATORY
RESOURCES
POLICIES
DESCRIPTION
THE
PURPOSE
OF
THIS
COURSE
IS
TO
BROADEN
THE
STUDENT
VIEW
OF
SOFTWARE
DEVELOPMENT
TOPICS
INCLUDE
AN
SCRIPTING
LANGUAGES
LIBRARIES
AND
TOOLS
AND
TECHNIQUES
FOR
PROGRAM
DEVELOPMENT
AND
MAINTENANCE
A
THEME
FOR
THIS
COURSE
IS
QUALITY
PROGRAMMING
IN
THE
SMALL
THE
IDEA
IS
TO
TEACH
THE
TOOLS
AND
TECHNIQUES
TO
WRITE
A
QUALITY
COMPONENT
WHETHER
IT
IS
A
STAND
ALONE
PROGRAM
TO
DO
SOME
SMALL
TASK
OR
IT
IS
TO
BE
A
PART
OF
A
LARGER
PROJECT
THESE
ARE
FUNDAMENTAL
PRINCIPLES
SKILLS
REQUIRED
BY
PROFESSIONAL
PROGRAMMERS
THE
UNIVERSITY
COURSE
CALENDAR
DESCRIPTION
OF
THE
CLASS
IS
AS
FOLLOWS
A
HANDS
ON
APPROACH
TO
SOFTWARE
DEVELOPMENT
AT
THE
INDIVIDUAL
AND
SMALL
TEAM
LEVEL
APPLICATION
OF
SOFTWARE
TOOLS
INCLUDING
SCRIPTING
LANGUAGES
SYSTEM
UTILITIES
AND
LIBRARIES
FOR
CONSTRUCTION
OF
SMALL
SOFTWARE
SYSTEMS
INTEGRATED
WITH
AND
MOTIVATED
BY
PROGRAMMING
PRACTICES
SYSTEM
DEVELOPMENT
TESTING
AND
MAINTENANCE
ISSUES
PREREQUISITES
OR
AND
MATH
SCHEDULING
CLASS
DAY
TIME
CLASS
LOCATION
TUESDAY
AND
THURSDAY
A
M
TO
P
M
ARTS
CLASS
DURATION
SEPTEMBER
THROUGH
DECEMBER
MIDTERM
EXAM
IN
CLASS
OCTOBER
THE
LAST
DAY
FOR
WITHDRAWLS
IS
NOVEMBER
LABS
SECTION
FRIDAYS
SECTION
TUESDAYS
SECTION
THURSDAYS
LAB
LOCATION
ALL
TUTORIALS
ARE
HELD
IN
ROOM
IN
THE
SPINKS
ADDITION
OF
THE
THORVALDSON
BUILDING
LABS
START
WEEK
OF
SEPTEMBER
GUEST
LECTURE
THE
INSTRUCTOR
WILL
BE
AT
A
CONFERENCE
ON
SEPTEMBER
AND
THE
CLASS
ON
THAT
DAY
WILL
BE
TAUGHT
BY
A
GUEST
LECTURER
WEBSITE
THE
WEBSITE
FOR
THIS
COURSE
IS
ON
THE
MOODLE
SERVER
OF
THE
DEPARTMENT
OF
COMPUTER
SCIENCE
THE
URL
IS
HTTPS
MOODLE
CS
USASK
CA
COURSE
VIEW
PHP
ID
FINALLY
REMEMBER
THAT
IF
YOU
NEED
HELP
E
MAIL
WORKS
HOURS
A
DAY
AND
YOU
LL
PROBABLY
EVEN
GET
A
RESPONSE
IN
SHORT
ORDER
ALTERNATIVELY
POST
SOMETHING
TO
ONE
OF
THE
CLASS
FORUMS
ON
MOODLE
TEACHING
ASSISTANT
SCOTT
JOHNSTON
MAIL
USASK
CA
COURSE
OBJECTIVES
BY
THE
COMPLETION
OF
THIS
COURSE
STUDENTS
WILL
BE
EXPECTED
TO
BE
FAMILIAR
WITH
MANY
COMMON
UNIX
LINUX
COMMANDS
UNDERSTAND
FUNDAMENTAL
CONCEPTS
REGARDING
OPERATING
SYSTEMS
VERSION
CONTROL
SYSTEMS
THE
SOFTWARE
BUILT
PROCESS
VIA
COMPILATION
AND
LINKING
PORTABLE
REPRESENTATION
OF
CHARACTERS
REPRESENTATION
AND
STORAGE
OF
PROGRAM
VARIABLES
IN
MEMORY
UNIX
PROCESSES
AND
THE
UNIX
FILE
SYSTEM
BY
THE
COMPLETION
OF
THIS
COURSE
STUDENTS
WILL
BE
EXPECTED
TO
BE
ABLE
TO
SKILLFULLY
USE
COMPLEX
UNIX
LINUX
SHELL
COMMANDS
WRITE
NON
TRIVIAL
SCRIPTS
IN
UNIX
SHELL
BASH
AND
AWK
SEARCH
FILES
USING
THE
GREP
COMMAND
AND
REGULAR
EXPRESSIONS
EFFECTIVELY
UTILIZE
TEST
DRIVEN
DESIGN
TO
WRITE
NON
TRIVIAL
PROGRAMS
EFFECTIVELY
USE
SVN
COMMANDS
TO
MANAGE
THE
EVOLUTION
OF
A
NON
TRIVIAL
PROGRAM
DECOMPOSE
A
LARGE
PROGRAM
INTO
COHESIVE
SEPARATELY
COMPILED
MODULES
AND
TO
MANAGE
THE
COMPILATION
AND
LINKING
OF
THOSE
MODULES
INTO
A
SINGLE
EXECUTABLE
USING
A
MAKEFILE
USE
A
LINE
ORIENTED
C
C
DEBUGGER
TO
EXAMINE
THE
EXECUTION
OF
A
PROGRAM
AND
IDENTIFY
BUGS
IN
THE
CODE
USE
MORE
SOPHISTICATED
DEBUGGING
AND
TESTING
TECHNIQUES
THAN
USED
IN
FIRST
YEAR
BUILD
AND
USE
AN
OBJECT
MODULE
LIBRARY
RECOGNIZE
AND
USE
IMPROVED
STYLES
OF
PROGRAMMING
NOTE
THAT
THE
ABOVE
TWO
LISTS
ARE
NOT
EXCLUSIVE
COURSE
SYLLABUS
CMPT
OPERATING
SYSTEMS
PRINCIPLES
CATALOGUE
DESCRIPTION
AN
INTRODUCTION
TO
THE
PRINCIPLES
OF
MODERN
OPERATING
SYSTEMS
THE
SYNCHRONIZATION
AND
COMMUNICATION
OF
COOPERATING
PROCESSES
PROCESS
SCHEDULING
VIRTUAL
MEMORY
FILE
SYSTEM
DESIGN
AND
ORGANIZATION
INTRODUCTION
TO
DISTRIBUTED
SYSTEMS
COURSE
OBJECTIVES
AFTER
COMPLETING
THIS
COURSE
STUDENTS
SHOULD
BE
ABLE
TO
DO
THE
FOLLOWING
TASKS
DEMONSTRATE
AND
ILLUSTRATE
HOW
APPLICATION
SOFTWARE
ACCESSES
COMPUTER
HARDWARE
THROUGH
THE
ABSTRAC
TIONS
PROVIDED
BY
THE
OPERATING
SYSTEM
AND
HOW
THE
OPERATING
SYSTEM
SHARES
HARDWARE
RESOURCES
BETWEEN
PROCESSES
TASKS
THREADS
AND
USERS
UTILIZE
SYSTEM
LIBRARY
FUNCTIONS
ROBUSTLY
IN
THE
IMPLEMENTATION
OF
APPLICATIONS
THAT
ACCESS
OPERATING
SYSTEM
FACILITIES
CORRECTLY
ABSTRACT
OPERATIONS
THROUGH
THE
USE
OF
APPLICATION
PROGRAMMER
INTERFACES
AND
VIRTUAL
FUNCTION
INTERFACES
DESIGN
IMPLEMENT
AND
DOCUMENT
SYSTEM
LEVEL
SOFTWARE
IN
A
SMALL
TEAM
ENVIRONMENT
DEMONSTRATE
THE
OPERATION
OF
WELL
KNOWN
THEORETICAL
ALGORITHMS
WITH
RESPECT
TO
DEADLOCK
PROCESS
AND
DISK
SCHEDULING
AND
MEMORY
MANAGEMENT
ILLUSTRATE
THE
SEPARATION
OF
POLICY
AND
MECHANISM
WITH
EXAMPLES
FROM
OPERATING
SYSTEM
DESIGN
AND
IMPLEMENTATION
DESIGN
ALGORITHMS
TO
PROVIDE
CONCURRENT
ACCESS
TO
SHARED
RESOURCES
AND
IMPLEMENT
THESE
ALGORITHMS
IN
THE
FOLLOWING
ENVIRONMENTS
IN
THE
C
PROGRAMMING
LANGUAGE
UNIX
PROCESSES
VARIOUS
UNIX
AND
WINDOWS
THREADS
PACKAGES
EXPLAIN
TIME
SPACE
AND
COMPLEXITY
TRADEOFFS
IN
OPERATING
SYSTEM
IMPLEMENTATION
ISSUES
DEMONSTRATING
THEIR
APPLICATION
WITH
APPROXIMATE
SOLUTIONS
FOR
VARIOUS
RESOURCE
SHARING
PROBLEMS
MODIFY
EXISTING
SYSTEM
LEVEL
SOURCE
CODE
TO
ADD
NEW
FUNCTIONALITY
COMPARE
AND
EVALUATE
OPTIONS
FOR
FILESYSTEM
IMPLEMENTATION
AND
USE
AS
WELL
AS
ALTERNATIVE
MECHANISMS
FOR
CONCURRENCY
CONTROL
BETWEEN
PROCESSES
STUDENT
EVALUATION
GRADING
SCHEME
REQUIREMENTS
THERE
WILL
BE
EQUALLY
WEIGHTED
ASSIGNMENTS
DUE
APPROXIMATELY
EVERY
WEEKS
A
MID
TERM
EXAM
HELD
IN
CLASS
AND
A
FINAL
EXAMINATION
DURING
THE
REGULAR
EXAMINATION
PERIOD
EXACT
DATES
WILL
BE
ANNOUNCED
AS
THE
COURSE
PROGRESSES
THE
APPROXIMATE
WEIGHTINGS
FOR
THE
ASSIGNMENTS
AND
EXAMINATIONS
ARE
AS
FOLLOWS
PARTICIPATION
INCLUDING
ONLINE
DISCUSSIONS
ASSIGNMENTS
MIDTERM
EXAM
OCTOBER
FINAL
EXAM
TOTAL
CRITERIA
THAT
MUST
BE
MET
TO
PASS
ALL
COMPONENTS
OF
THE
COURSE
MUST
BE
COMPLETE
IN
ORDER
TO
ACHIEVE
A
PASSING
GRADE
IN
THE
COURSE
THIS
INCLUDES
BOTH
EXAMS
AND
EVERY
ASSIGNMENT
FAILURE
TO
DO
SO
WILL
RESULT
IN
AN
AUTOMATIC
FAILURE
OF
THE
COURSE
SUBMISSION
OF
ASSIGNMENTS
MUST
BE
A
CREDIBLE
ATTEMPT
TO
SOLVE
THE
PROBLEMS
IN
THE
ASSIGNMENT
IN
THE
JUDGMENT
OF
THE
MARKER
FINAL
EXAM
SCHEDULING
THE
REGISTRAR
SCHEDULES
ALL
FINAL
EXAMINATIONS
INCLUDING
DEFERRED
AND
SUPPLEMENTAL
EXAMINATIONS
STUDENTS
ARE
ADVISED
NOT
TO
MAKE
TRAVEL
ARRANGEMENTS
FOR
THE
EXAM
PERIOD
UNTIL
THE
OFFICIAL
EXAM
SCHEDULE
HAS
BEEN
POSTED
NOTE
ALL
STUDENTS
MUST
BE
PROPERLY
REGISTERED
IN
ORDER
TO
ATTEND
LECTURES
AND
RECEIVE
CREDIT
FOR
THIS
COURSE
TEXTBOOK
INFORMATION
REQUIRED
TEXT
TITLE
MODERN
OPERATING
SYSTEMS
AUTHOR
ANDREW
TANENBAUM
PUBLISHER
PEARSON
PRENTICE
HALL
EDITION
YEAR
ISBN
EDITION
ADDITIONAL
INFORMATION
THIS
BOOK
COVERS
MORE
MATERIAL
THAN
WE
CAN
DO
IN
ONE
COURSE
WE
WILL
COVER
THE
FIRST
CHAPTERS
WITH
SOME
DISCUSSION
ON
LATER
CHAPTERS
IF
TIME
PERMITS
IT
IS
LIKELY
THAT
THE
EDITION
WILL
BE
SUFFICIENT
FOR
MOST
STUDENTS
NEVERTHELESS
YOU
SHOULD
ENSURE
THAT
YOU
HAVE
ACCESS
TO
A
EDITION
FOR
POSSIBLE
REFERENCE
TO
EXERCISES
AND
SPECIFIC
SECTIONS
IN
THE
NEW
TEXTBOOK
RECOMMENDED
TEXTS
ADDITIONAL
INFORMATION
OPTIONAL
BUT
NOT
NECESSARY
FOR
ANYONE
THIS
PROVIDES
A
DIFFERENT
POINT
OF
VIEW
TITLE
OPERATING
SYSTEMS
INTERNALS
AND
DESIGN
PRINCIPLES
AUTHOR
WILLIAM
STALLINGS
PUBLISHER
PEARSON
PRENTICE
HALL
EDITION
YEAR
ISBN
EDITION
YET
A
THIRD
OPTION
TITLE
OPERATING
SYSTEM
CONCEPTS
AUTHOR
ABRAHAM
SILBERSCHATZ
PETER
BAER
GALVIN
AND
GREG
GAGNE
PUBLISHER
JOHN
WILEY
AND
SONS
EDITION
YEAR
ISBN
EDITION
ESSENTIAL
FOR
EVERYONE
TO
HAVE
FOR
THEIR
CAREER
MANY
SHOULD
ALREADY
HAVE
THIS
TITLE
THE
C
PROGRAMMING
LANGUAGE
AUTHOR
BRIAN
KERNIGHAN
AND
DENNIS
RITCHIE
PUBLISHER
PRENTICE
HALL
EDITION
YEAR
ISBN
LECTURE
SCHEDULE
TOPIC
TOPIC
SUBTOPICS
INTRODUCTION
OVERVIEW
HISTORY
OF
OPERATING
SYSTEMS
INTRODUCTION
TO
THE
COM
PUTING
ENVIRONMENT
FOR
THIS
COURSE
POSSIBLE
REVIEW
OF
C
AND
UNIX
LINUX
READINGS
CHAPTER
PP
TANENBAUM
PROCESS
DESCRIPTION
CONCURRENCY
CONTROL
AND
INTER
PROCESS
COMMUNICATION
PROCESSES
THREADS
AND
ADDRESS
SPACES
I
O
DEVICES
SYNCHRONIZATION
MUTUAL
EXCLUSION
SEMAPHORES
MONITORS
INTER
PROCESS
COMMUNICATION
DEADLOCK
EXAMPLES
DURATION
WEEKS
READINGS
CHAPTER
SECTION
PROCESS
SCHEDULING
TYPES
OF
SCHEDULING
UNIPROCESSOR
SCHEDULING
MULTIPROCESSOR
SCHEDULING
EXAMPLES
DURATION
WEEKS
READINGS
CHAPTER
SECTION
MEMORY
MANAGEMENT
ADDRESS
BINDING
VIRTUAL
MEMORY
ADDRESS
TRANSLATION
MANAGE
MENT
POLICIES
FOR
DEMAND
PAGED
VIRTUAL
MEMORY
EXAMPLES
DURATION
WEEKS
READINGS
CHAPTER
I
O
AND
FILE
SYSTEMS
ORGANIZING
THE
I
O
FUNCTION
DISK
ARCHITECTURES
RAID
DISK
CACHING
FILE
ORGANIZATION
SECONDARY
STORAGE
MANAGEMENT
PRO
TECTION
AND
SECURITY
EXAMPLES
DURATION
WEEKS
READINGS
CHAPTER
AND
SECTIONS
AND
COURSE
PROCEDURES
FACILITIES
STUDENTS
WILL
BE
USING
THE
LINUX
WINDOWS
COMPUTERS
IN
THE
SPINKS
LAB
TO
DO
MOST
OF
THEIR
WORK
AND
ARE
EXPECTED
TO
BE
FAMILIAR
WITH
THE
USE
OF
THESE
FACILITIES
THIS
COURSE
WILL
BE
ADMINISTERED
WITH
BLACKBOARD
THOSE
UNFAMILIAR
WITH
THE
USE
OF
BLACKBOARD
ARE
STRONGLY
ENCOURAGED
TO
MAKE
USE
OF
THE
RESOURCES
AVAILABLE
TO
LEARN
BLACKBOARD
BLACKBOARD
HAS
A
DIS
CUSSION
FORUM
AND
IT
WILL
BE
USED
TO
DISSEMINATE
ANSWERS
TO
QUESTIONS
REGARDING
ASSIGNMENTS
LECTURE
MATERIAL
ETC
MANY
IMPORTANT
THINGS
MAY
HAPPEN
THERE
THAT
WILL
NOT
BE
REPEATED
IN
CLASS
SO
DAILY
READING
OF
THE
BULLETIN
BOARD
IS
REQUIRED
THE
COURSE
WILL
MAKE
REGULAR
USE
OF
INTERNET
FACILITIES
PAGE
LOCATIONS
WILL
BE
GIVEN
DURING
LECTURES
AND
ON
BLACKBOARD
ASSIGNMENT
DESCRIPTIONS
WILL
BE
MADE
AVAILABLE
IN
ELECTRONIC
FORM
ONLY
SUBMISSIONS
WILL
BE
IN
ELECTRONIC
FORM
ONLY
PART
OF
THE
EVALUATION
IN
THE
COURSE
WILL
INCLUDE
PARTICIPATION
INCLUDING
PARTICIPATION
IN
THE
ELECTRONIC
COMPONENT
OF
THE
COURSE
THROUGH
BLACKBOARD
YOUR
INVOLVEMENT
IN
THIS
AREA
OF
THE
COURSE
WILL
BE
ELEC
TRONICALLY
TRACKED
ON
A
PERIODIC
BASIS
ASSIGNMENTS
ARE
TO
BE
DONE
IN
GROUPS
OF
AT
MOST
TWO
STUDENTS
VERSION
CONTROL
USING
SVN
AT
SVN
CS
USASK
CA
IS
REQUIRED
FOR
EVERY
ASSIGNMENT
AND
WILL
BE
VERIFIED
BY
SUBMITTING
MEANINGFUL
SVN
LOGS
FINAL
NOTE
THE
PURPOSE
OF
CMPT
IS
TO
PROVIDE
A
BASIC
UNDERSTANDING
OF
OPERATING
SYSTEMS
PRINCI
PLES
THE
PARTS
OF
AN
OPERATING
SYSTEM
HOW
THEY
ARE
STRUCTURED
THE
IMPORTANT
POLICIES
GOVERNING
THEIR
OPERATION
AND
THE
IMPLEMENTATION
ISSUES
ALTHOUGH
EXAMPLES
WILL
BE
DRAWN
FROM
SEVERAL
OPERATING
SYSTEMS
THROUGHOUT
THE
COURSE
IT
IS
NOT
THE
PURPOSE
OF
THIS
COURSE
TO
PROVIDE
TRAINING
IN
ANY
PARTICULAR
OPERATING
SYSTEM
THE
SPECIFIC
OPERATING
SYSTEM
THAT
WE
WILL
BE
DEVELOPING
CODE
FOR
IS
UNIX
LIKE
IN
SOME
RUDIMENTARY
WAYS
BUT
IS
NOT
IN
ANY
WAY
REQUIRED
TO
HAVE
THE
SAME
TYPES
OF
FUNCTIONALITY
THE
CLASS
WILL
FOLLOW
THE
TEXTBOOK
IN
CONTENT
BUT
NOT
NECESSARILY
ORGANIZATION
STUDENTS
WILL
BE
RESPONSIBLE
FOR
READING
THE
TEXT
AND
LEARNING
THE
MATERIAL
IN
IT
AS
MUCH
AS
POSSIBLE
LECTURE
TIME
WILL
BE
USED
PRIMARILY
FOR
HIGHLIGHTING
SPECIFIC
MATERIAL
IN
THE
TEXT
COVERING
SUPPLEMENTARY
MATERIAL
AS
REQUIRED
AND
ANSWERING
STUDENT
QUESTIONS
H
SYLLABUS
EDIT
MODE
IS
OFF
SYLLABUS
CMPT
SYLLABUS
COURSE
GOALS
AND
REQUIREMENTS
COURSE
GOALS
THE
PURPOSE
OF
CMPT
IS
TO
PROVIDE
A
DEEP
UNDERSTANDING
OF
OPERATING
SYSTEMS
PRINCIPLES
THE
PARTS
OF
AN
OPERATING
SYSTEM
HOW
THEY
ARE
STRUCTURED
THE
IMPORTANT
POLICIES
GOVERNING
THEIR
OPERATION
AND
THE
IMPLEMENTATION
ISSUES
ALTHOUGH
EXAMPLES
WILL
BE
DRAWN
FROM
SEVERAL
OPERATING
SYSTEMS
THROUGHOUT
THE
COURSE
IT
IS
NOT
THE
PURPOSE
OF
THIS
COURSE
TO
PROVIDE
TRAINING
IN
ANY
PARTICULAR
OPERATING
SYSTEM
THIS
COURSE
WILL
HAVE
A
SIGNIFICANT
PROJECT
COMPONENT
LECTURE
TIME
WILL
BE
USED
PRIMARILY
FOR
COVERAGE
OF
BASIC
OS
IMPLEMENTATION
ISSUES
FROM
MATERIAL
IN
THE
TEXTS
COVERING
SUPPLEMENTARY
MATERIAL
AS
REQUIRED
PROJECT
DESIGN
MEETINGS
AND
REVIEWS
AND
ANSWERING
STUDENT
QUESTIONS
COURSE
OBJECTIVES
AFTER
COMPLETING
THIS
COURSE
STUDENTS
SHOULD
BE
ABLE
TO
DO
THE
FOLLOWING
TASKS
IMPLEMENT
SYSTEM
CALLS
IN
THE
LINUX
OPERATING
SYSTEM
EXPLAIN
DESIGN
PRINCIPLES
INFLUENCING
THE
STRUCTURE
OF
DIFFERENT
OS
PARADIGMS
DESIGN
THE
COMPONENTS
OF
A
SIMPLE
BUT
COMPLETE
OPERATING
SYSTEM
DESIGN
AND
IMPLEMENT
AN
API
FOR
USER
PROCESSES
TO
ACCESS
OS
FACILITIES
IMPLEMENT
INTEGRATE
AND
DOCUMENT
OPERATING
SYSTEM
COMPONENTS
IN
A
SMALL
TEAM
ENVIRONMENT
INTEGRATE
DEVICE
DRIVERS
AND
BOOTLOADERS
INTO
THE
OPERATING
SYSTEM
FOR
COMMUNICATION
WITH
PERIPHERAL
DEVICES
COMPARE
AND
EVALUATE
DESIGN
ALTERNATIVES
FOR
PROCESSES
THREADS
SCHEDULERS
AND
OR
MEMORY
MANAGE
MENT
EVALUATE
RESEARCH
LITERATURE
IN
OPERATING
SYSTEMS
DESIGN
AND
IMPLEMENTATION
AND
EXPLAIN
OPEN
ISSUES
AND
POTENTIAL
SOLUTIONS
COURSE
REQUIREMENTS
NOTE
REQUIREMENTS
AND
EXPECTATIONS
DIFFER
SLIGHTLY
BETWEEN
THE
UNDERGRAD
COURSE
AND
THE
GRAD
COURSE
THERE
WILL
BE
OR
ASSIGNMENTS
DUE
APPROXIMATELY
IN
LATE
JANUARY
AND
MID
MARCH
RESPECTIVELY
A
COURSE
IMPLEMENTATION
PROJECT
DUE
IN
EARLY
APRIL
A
MID
TERM
EXAM
HELD
IN
CLASS
IN
THE
BEGINNING
PART
OF
MARCH
AND
A
FINAL
EXAMINATION
DURING
THE
REGULAR
EXAMINATION
PERIOD
EXACT
DATES
WILL
BE
ANNOUNCED
AS
THE
COURSE
PROGRESSES
THE
APPROXIMATE
WEIGHTINGS
FOR
THE
ASSIGNMENTS
AND
EXAMINATIONS
ARE
AS
FOLLOWS
COMPONENT
ASSIGNMENTS
IMPLEMENTATION
PROJECT
RESEARCH
PROJECT
PAPER
PRESENTATION
MID
TERM
EXAM
FINAL
EXAM
RESOURCES
TEXTBOOKS
TITLE
MODERN
OPERATING
SYSTEMS
AUTHOR
ANDREW
TANENBAUM
PUBLISHER
PEARSON
PRENTICE
HALL
EDITION
YEAR
EDITION
ISBN
ADDITIONAL
INFORMATION
NONE
TYPE
REQUIRED
RESOURCE
TITLE
LINUX
KERNEL
DEVELOPMENT
AUTHOR
ROBERT
LOVE
PUBLISHER
ADDISON
WESLEY
EDITION
YEAR
THIRD
EDITION
ISBN
ADDITIONAL
INFORMATION
AVAILABLE
AS
A
FREE
E
BOOK
TYPE
RECOMMENDED
RESOURCE
TITLE
ADVANCED
PROGRAMMING
IN
THE
UNIX
ENVIRONMENT
AUTHOR
W
RICHARD
STEVENS
STEPHEN
A
RAGO
PUBLISHER
ADDISON
WESLEY
EDITION
YEAR
EDITION
ISBN
ADDITIONAL
INFORMATION
NONE
TYPE
RECOMMENDED
RESOURCE
TITLE
OPERATING
SYSTEMS
DESIGN
AND
IMPLEMENTATION
AUTHOR
ANDREW
TANENBAUM
AND
ALBERT
WOODHULL
PUBLISHER
PEARSON
PRENTICE
HALL
EDITION
YEAR
EDITION
ISBN
TYPE
RECOMMENDED
RESOURCE
COURSE
INFORMATION
AND
POLICIES
INSTRUCTOR
CONTACT
INFORMATION
USE
OF
CLASS
TIME
THIS
COURSE
WILL
BE
PRAGMATIC
AND
HANDS
ON
IN
ADDITION
TO
THE
THEORETICAL
MATERIAL
A
SIGNIFICANT
AMOUNT
OF
CLASS
TIME
WILL
BE
SPENT
ON
DESIGN
PROGRAMMING
AND
DEBUGGING
THE
IMPLEMENTATION
PROJECT
POLICIES
FACILITIES
STUDENTS
WILL
BE
USING
THE
UNIX
TIME
SHARING
FACILITIES
OF
THE
SPINKS
LAB
TO
DO
MOST
OF
THEIR
WORK
AND
ARE
EXPECTED
TO
BE
FAMILIAR
WITH
THE
USE
OF
THESE
FACILITIES
VIRTUAL
MACHINE
ENVIRONMENTS
WILL
BE
CREATED
ON
A
SUBSET
OF
THE
MACHINES
FOR
YOUR
KERNEL
PROGRAMMING
PLEASURE
OTHER
A
FEW
MISCELLANEOUS
POINTS
THIS
COURSE
WILL
BE
ADMINISTERED
WITH
BLACKBOARD
THE
DISCUSSION
FORUM
WILL
BE
USED
TO
DISSEMINATE
ANSWERS
TO
QUESTIONS
REGARDING
ASSIGNMENTS
LECTURE
MATERIAL
ETC
MANY
IMPORTANT
THINGS
MAY
HAPPEN
THERE
THAT
WILL
NOT
BE
REPEATED
IN
CLASS
SO
READING
OF
THE
BULLETIN
BOARD
IS
REQUIRED
THE
COURSE
WILL
MAKE
REGULAR
USE
OF
THE
WORLD
WIDE
WEB
AND
OTHER
INTERNET
FACILITIES
LOCATIONS
WILL
BE
GIVEN
DURING
LECTURES
AND
ON
BLACKBOARD
ASSIGNMENT
DESCRIPTIONS
WILL
BE
MADE
AVAILABLE
IN
ELECTRONIC
FORM
ONLY
SUBMISSIONS
WILL
BE
IN
ELECTRONIC
FORM
ONLY
ADDITIONAL
INFORMATION
ALL
STUDENTS
MUST
BE
PROPERLY
REGISTERED
IN
ORDER
TO
ATTEND
LECTURES
AND
RECEIVE
CREDIT
FOR
THIS
COURSE
FAILURE
TO
WRITE
THE
FINAL
EXAM
WILL
RESULT
IN
FAILURE
OF
THE
COURSE
PLAGIARISM
IS
STRONGLY
FORBIDDEN
AS
IN
ALL
COURSES
WE
WILL
BE
CHECKING
ASSIGNMENTS
PROJECTS
FOR
PLAGIARISM
USING
NIFTY
CHEATING
DETECTION
SOFTWARE
AND
PENALTIES
WILL
BE
ENFORCED
REFER
TO
THE
UNIVERSITY
OF
SASKATCHEWAN
POLICY
ON
ACADEMIC
HONESTY
FOR
FURTHER
DETAILS
LESSONS
INTRODUCTION
AND
OVERVIEW
INTRODUCTION
OBJECTIVES
OVERVIEW
TOPICS
REVIEW
OF
IN
A
NUTSHELL
WITH
A
LITTLE
MORE
JANUARY
PM
PM
PROCESS
DESCRIPTION
AND
CONTROL
PROCESS
DESCRIPTION
AND
CONTROL
TOPICS
DURATION
WEEKS
JANUARY
PM
PM
BOOTING
AND
BOOT
LOADERS
EXAMINING
SOURCE
CODE
OF
THE
BOOTLOADER
FOR
REAL
OPERATING
SYSTEMS
TO
REVIEW
AND
UNDERSTAND
THE
ARCHITECTURE
AND
MEMORY
STRUCTURE
OF
THE
INTEL
PROCESSOR
POSSIBLE
DISCUSSION
OF
ARM
ASSEMBLY
LANGUAGE
AND
PROCESS
FOR
BOOTING
ON
ARM
PROCESSORS
DURATION
WEEKS
JANUARY
PM
PM
INTERRUPTS
TRAPS
INTERRUPTS
TRAPS
TOPICS
LOW
LEVEL
HANDLING
OF
EXTERNAL
EVENTS
DURATION
WEEKS
JANUARY
PM
PM
DEVICE
DRIVERS
DEVICE
DRIVERS
TOPICS
INTERFACING
WITH
OBJECTS
IN
THE
REAL
WORLD
DURATION
WEEK
FEBRUARY
PM
PM
I
O
AND
FILE
SYSTEMS
I
O
AND
FILE
SYSTEMS
TOPICS
FILE
ORGANIZATION
SECONDARY
STORAGE
MANAGEMENT
EXAMPLES
DURATION
WEEKS
FEBRUARY
PM
PM
IPC
AND
FRIENDS
IPC
AND
FRIENDS
TOPICS
COMMUNICATING
BETWEEN
PROCESSES
SHARED
MEMORY
ETC
DURATION
WEEKS
MARCH
PM
PM
NETWORK
AND
SYSTEM
ADMINISTRATION
CMPT
SYLLABUS
UNIVERSITY
OF
SASKATCHEWAN
TERM
DESCRIPTION
AND
LEARNING
OBJECTIVES
CALENDAR
DESCRIPTION
THIS
COURSE
CONCERNS
DEPLOYMENT
AND
MAINTENENCE
OF
MODERN
COMPUTER
SYSTEMS
IN
AN
OPERATIONAL
ENVIRONMENT
THE
COURSE
PROVIDES
BOTH
CONCEPTUAL
KNOWLEDGE
AND
PRACTICAL
EXPERIENCE
TOPICS
TO
BE
COVERED
INCLUDE
ARCHITECTURES
HETEROGENEOUS
SYSTEMS
AUTHENTICATION
AND
SECURITY
NETWORK
SERVICES
INCLUDING
FIREWALLS
STORAGE
SERVICES
PERFORMANCE
ANALYSIS
AND
TUNING
MANAGEMENT
AND
CONFIGURATION
OF
SERVICES
AND
SYSTEM
RESOURCES
SYSTEM
INITIALIZATION
DRIVERS
CROSS
PLATFORM
SERVICES
POLICIES
AND
PROCEDURES
INSTRUCTOR
DWIGHT
MAKAROFF
THORVALDSON
AVAILABLE
VIA
APPOINTMENT
CLASS
MEETING
TIME
TUESDAY
THURSDAY
P
M
ROOM
THORVALDSON
TEACHING
ASSISTANT
KHADIJA
RASUL
LEARNING
OBJECTIVES
UPON
SUCCESSFUL
COMPLETION
OF
THIS
COURSE
STUDENTS
SHOULD
BE
ABLE
TO
DEMONSTRATE
COMPETENCY
AT
THE
FOLLOWING
TASKS
MANAGE
USERS
FILES
AND
SOFTWARE
ON
A
COMPUTER
SYSTEM
INSTALLATION
CONSISTING
OF
CLIENTS
AND
SERVERS
MANAGEMENT
INSTALL
AND
CONFIGURE
NETWORKING
SERVICES
FOR
INTRANET
AND
INTERNET
DOMAINS
NETWORKING
ADMINISTER
NETWORK
SECURITY
POLICIES
IN
LINUX
AND
WINDOWS
ENVIRONMENTS
SECURITY
UNDERSTAND
AND
APPLY
TECHNIQUES
TO
INTEROPERATE
COMPUTER
SYSTEMS
COMPRISED
OF
LINUX
AND
WINDOWS
MACHINES
INTEROPERABILITY
IDENTIFY
POTENTIAL
SOURCES
OF
POOR
COMPUTER
PERFORMANCE
AND
EVALUATE
POTENTIAL
SOLUTIONS
PERFORMANCE
DEBUGGING
DESIGN
SMALL
AND
MEDIUM
SIZED
BUSINESS
IT
INFRASTRUCTURE
ORGANIZATION
CAPACITY
PLANNING
DEVELOP
SCRIPTING
MECHANISMS
AND
AUTOMATED
SCRIPTS
FOR
PERFORMING
COMPLICATED
ADMINISTRATION
TASKS
SYSTEM
SCRIPTING
EVALUATE
ALTERNATIVE
POLICIES
AND
MECHANISMS
FOR
PROVIDING
RELIABILITY
FEATURES
OF
COMPUTER
SYSTEM
SERVICES
AND
OPERATIONS
BACKUPS
INSTALL
AND
CONFIGURE
LINUX
AND
WINDOWS
VIRTUAL
MACHINES
VIRTUALIZATION
DEPLOY
SYSTEMS
TO
MANAGE
LARGE
AMOUNTS
OF
DATA
FOR
A
WIDE
VARIETY
OF
USERS
DATA
CENTRES
RESOURCES
REQUIRED
TEXTBOOK
THE
PRACTICE
OF
SYSTEM
AND
NETWORK
ADMINISTRATION
ED
THOMAS
A
LIMONCELLI
CHRISTINA
J
HOGAN
AND
STRATA
R
CHALUP
ADDISON
WESLEY
ISBN
RECOMMENDED
ADDITIONAL
READINGS
RESOURCES
UNIX
AND
LINUX
SYSTEM
ADMINISTRATION
HANDBOOK
ED
EVI
NEMETH
GARTH
SNYDER
TRENT
R
HEIN
BEN
WHALEY
PRENTICE
HALL
ISBN
LINUX
ADMINISTRATION
A
BEGINNERS
GUIDE
ED
WALE
SOYINKA
MCGRAW
HILL
ISBN
ESSENTIAL
SYSTEM
ADMINISTRATION
ED
A
FRISCH
O
REILLY
ISBN
TCP
IP
NETWORK
ADMINISTRATION
ED
C
HUNT
O
REILLY
ISBN
MICROSOFT
WINDOWS
SERVER
ADMINISTRATION
ESSENTIALS
TOM
CARPENTER
SYBEX
ISBN
INTRODUCING
WINDOWS
SERVER
RTM
EDITION
MITCH
TULLOCH
WITH
THE
WINDOWS
SERVER
TEAM
MICROSOFT
PRESS
ISBN
COURSE
TOPICS
POLICIES
EVALUATION
COURSE
TOPICS
THE
TOPICS
OF
DISCUSSIONS
ASSIGNMENTS
AND
LECTURE
TIME
SHALL
INCLUDE
BUT
NOT
NECESSARILY
BE
LIMITED
TO
THE
FOLLOWING
CLUSTERS
VS
SUPERCOMPUTERS
VS
LANS
MULTI
TIERED
SERVER
ARCHITECTURES
HETEROGENEOUS
SYSTEMS
AUTHENTICATION
AND
SECURITY
FILE
SERVERS
DISK
CONFIGURATION
BACKUPS
NETWORK
FILE
SYSTEMS
PERFORMANCE
ANALYSIS
AND
TUNING
MANAGING
SYSTEM
RESOURCES
DEVICES
AND
DRIVERS
CONFIGURING
AND
BUILDING
KERNELS
BOOTSTRAPPING
AND
SYSTEM
INITIALIZATION
NETWORK
ADMINISTRATION
NETWORK
SERVICES
CROSS
PLATFORM
SERVICES
FIREWALLS
PROXIES
ROUTERS
AND
NAME
SERVERS
POLICIES
AND
PROCEDURES
POLICIES
THE
COURSE
WILL
BE
STRUCTURED
AS
INFORMAL
LECTURE
DISCUSSION
INFORMATION
WILL
BE
DISCUSSED
FROM
THE
VARIOUS
TEXTBOOKS
AND
READINGS
WITH
APPLICATIONS
TO
DEPLOYING
THE
CONCEPTS
ON
SYSTEMS
CONSISTING
OF
RESOURCES
IN
A
VIRTUAL
MACHINE
ENVIRONMENT
STUDENTS
WILL
BE
EXPECTED
TO
BE
PREPARED
TO
DISCUSS
TOPICS
AND
COMPLETE
EXERCISES
IN
CLASSROOM
AND
LAB
ENVIRONMENTS
THE
U
OF
POLICY
ON
ACADEMC
HONESTY
WILL
APPLY
FOR
THIS
COURSE
THE
UNIVERSITY
POLICY
WILL
BE
THE
TERMS
OF
REFERENCE
FOR
STUDENT
ACADEMIC
CONDUCT
EVALUATION
THE
COURSE
EVALUATION
WILL
CONSIST
OF
ASSIGNMENTS
A
MIDTERM
EXAM
COMPONENTS
WRITTEN
AND
LAB
A
TERM
PROJECT
AND
A
FINAL
EXAM
AGAIN
COMPONENTS
WRITTEN
AND
LAB
ALL
COMPONENTS
ARE
MANDATORY
IN
ORDER
TO
ACHIEVE
A
PASSING
GRADE
FOR
THE
COURSE
ALL
EVALUATION
COMPONENTS
MUST
RECEIVE
A
PASSING
GRADE
TO
ACHIEVE
A
PASSING
GRADE
IN
THE
COURSE
ASSIGNMENTS
THERE
WILL
BE
APPROXIMATELY
ASSIGNMENTS
CONSISTING
OF
A
PRACTICAL
PROBLEM
IN
SYSTEM
ADMINISTRATION
THAT
STUDENTS
WILL
BE
ASKED
TO
SOLVE
AND
EITHER
WRITE
UP
A
REPORT
ON
THE
SOLUTION
OR
DEMONSTRATE
THE
SOLUTION
TO
THE
INSTRUCTOR
OR
A
TEACHING
ASSISTANT
OUTSIDE
OF
CLASS
TIME
MIDTERM
EXAM
THERE
WILL
BE
A
MID
TERM
EXAM
CONSISTING
OF
A
WRITTEN
PORTION
WHERE
STUDENTS
DEMONSTRATE
UNDERSTANDING
OF
THE
CONCEPTS
BEHIND
SYSTEM
AND
NETWORK
ADMINISTRATION
AND
A
LAB
COMPONENT
WHERE
PRACTICAL
SOLUTIONS
WILL
BE
REQUIRED
IN
A
TIME
LIMITED
ENVIRONMENT
THE
MIDTERM
WILL
BE
HELD
DURING
CLASS
TIME
IN
LATE
FEBRUARY
EARLY
MARCH
TERM
PROJECT
EACH
STUDENT
WILL
PROPOSE
A
RESEARCH
PROJECT
ASSOCIATED
WITH
ONE
OR
MORE
MAJOR
TOPICS
OF
THE
COURSE
THIS
COULD
INVOLVE
LITERATURE
REVIEWS
INDUSTRY
EVALUATIONS
OR
TECHNICAL
PROTOTYPE
DEVELOPMENT
AND
EVALUATION
FINAL
EXAM
THE
FINAL
EXAM
WILL
HAVE
TWO
COMPONENTS
A
WRITTEN
EXAM
AND
A
LAB
EXAM
WRITTEN
DURING
THE
EXAM
PERIOD
RELATIVE
WEIGHT
OF
EVALUATION
COMPONENTS
ASSIGNMENTS
MIDTERM
WRITTEN
MIDTERM
LAB
TERM
PROJECT
FINAL
EXAM
WRITTEN
FINAL
EXAM
LAB
TOTAL
ADMINISTRATION
THE
COURSE
WILL
BE
ADMINISTERED
WITH
THE
BLACKBOARD
COURSE
TOOLS
THIS
WILL
INCLUDE
COURSE
NOTES
DISCUSSION
FORUMS
ANNOUNCEMENTS
AND
INTERNAL
EMAIL
TENTATIVE
OUTLINE
OF
TOPICS
LESSON
TITLE
DATE
REVIEW
AND
ROLE
OF
SYSTEM
ADMINISTRATORS
JANUARY
LIMONCELLI
NEMETH
AND
SOYINKA
ORGANIZATION
OF
IT
SYSTEMS
DOCUMENTATION
JANUARY
LIMONCELLI
HARDWARE
AND
SOFTWARE
CONFIGURATION
BASICS
JANUARY
LIMONCELLI
NEMETH
SOYINKA
H
W
W
USER
MGMT
CONTINUED
JANUARY
FILE
SYSTEMS
AND
DISK
ISSUES
JANUARY
NEMETH
SOYINKA
TANENBAUM
NETWORKING
INTRO
JANUARY
LIMONCELLI
SOYINKA
NEMETH
STARTUP
AND
SHUTDOWN
JANUARY
NEMETH
SOYINKA
STARTUP
AND
SHUTDOWN
PART
JANUARY
THE
KERNEL
JANUARY
NEMETH
SOYINKA
OBSERVATION
AUTOMATION
LOGGING
AND
BACKUPS
FEBRUARY
LIMONCELLI
NEMETH
SOYINKA
SECURITY
INTRODUCED
FEBRUARY
LIMONCELLI
SECURITY
CONTINUED
NEMETH
SOYINKA
FEBRUARY
ETHICS
FEBRUARY
LIMONCELLI
LOCAL
AND
NETWORK
SERVICES
AND
APPLICATIONS
FEBRUARY
LIMONCELLI
AND
NEMETH
SOYINKA
SERVICES
AND
APPLICATIONS
CONTINUED
FEBRUARY
NEMETH
SOYINKA
APPLICATIONS
AND
SERVICES
PART
MARCH
MIDTERM
MARCH
NETWORK
CONNECTIONS
AND
MANAGEMENT
MARCH
LIMONCELLI
NEMETH
SOYINKA
NETWORK
STUFF
CONTINUED
MARCH
PERFORMANCE
ANALYSIS
AND
TUNING
MARCH
LIMONCELLI
CMPT
FOUNDATIONS
OF
CONCURRENT
PROGRAMMING
CATALOGUE
DESCRIPTION
THEORY
AND
PRACTICE
OF
CONCURRENT
PROGRAMMING
PROCESS
INTERACTION
USING
SHARED
VARIABLES
AND
MESSAGE
PASSING
PARALLEL
COMPUTING
DEVELOPMENT
OF
CORRECT
PROGRAMS
GENERAL
PROBLEM
SOLVING
TECHNIQUES
SCIENTIFIC
COMPUTING
DISTRIBUTED
PROGRAMMING
COURSE
OBJECTIVES
PROVIDE
AN
INTRODUCTION
TO
FOUNDATIONAL
CONCEPTS
PARADIGMS
AND
LANGUAGE
SUPPORT
FOR
CONCURRENT
PRO
GRAMMING
STUDENT
EVALUATION
GRADING
SCHEME
ASSIGNMENTS
MIDTERM
EXAM
FINAL
EXAM
TOTAL
CRITERIA
THAT
MUST
BE
MET
TO
PASS
SCHEDULE
ASSIGNMENT
ASSIGNED
DUE
ASSIGNMENT
ASSIGNED
DUE
MIDTERM
EXAM
ASSIGNMENT
ASSIGNED
DUE
ASSIGNMENT
ASSIGNED
DUE
ASSIGNMENT
ASSIGNED
DUE
MIDTERM
AND
FINAL
CUMULATIVE
MARKS
WILL
BE
SCALED
ATTENDENCE
EXPECTATION
STUDENTS
WILL
BE
EXPECTED
TO
KNOW
ALL
INFORMATION
PASSED
ON
DURING
LECTURES
AND
TUTORIALS
AND
THROUGH
THE
WEBPAGE
MOODLE
AND
EMAIL
IF
THEY
MISS
A
LECTURE
OR
TUTORIAL
THEY
ARE
RESPONSIBLE
FOR
ACQUIRING
MATERIAL
COVERED
IN
THE
SESSION
FINAL
EXAM
SCHEDULING
THE
REGISTRAR
SCHEDULES
ALL
FINAL
EXAMINATIONS
INCLUDING
DEFERRED
AND
SUPPLEMENTAL
EXAMINATIONS
STUDENTS
ARE
ADVISED
NOT
TO
MAKE
TRAVEL
ARRANGEMENTS
FOR
THE
EXAM
PERIOD
UNTIL
THE
OFFICIAL
EXAM
SCHEDULE
HAS
BEEN
POSTED
NOTE
ALL
STUDENTS
MUST
BE
PROPERLY
REGISTERED
IN
ORDER
TO
ATTEND
LECTURES
AND
RECEIVE
CREDIT
FOR
THIS
COURSE
TEXTBOOK
INFORMATION
REQUIRED
TEXT
GREGORY
ANDREWS
FOUNDATIONS
OF
MULTITHREADED
PARALLEL
AND
DISTRIBUTED
PROGRAMMING
ADDISON
WESLEY
RECOMMENDED
TEXTS
CARLOS
A
VARELA
PROGRAMMING
DISTRIBUTED
COMPUTING
SYSTEMS
A
FOUNDATIONAL
APPROACH
MIT
PRESS
LECTURE
SCHEDULE
TOPICS
TO
BE
COVERED
OVERVIEW
DISTRIBUTED
PROGRAMMING
MESSAGE
PASSING
RPC
AND
RENDEZVOUS
PARADIGMS
FOR
PROCESS
INTERACTION
IMPLEMENTATIONS
SHARED
VARIABLE
PROGRAMMING
PROCESSES
AND
SYNCHRONIZATION
LOCKS
AND
BARRIERS
SEMAPHORES
MONITORS
IMPLEMENTATIONS
PARALLEL
PROGRAMMING
SCIENTIFIC
COMPUTING
LANGUAGES
COMPILERS
LIBRARIES
AND
TOOLS
COURSE
OVERVIEW
THERE
WILL
BE
ABOUT
TUTORIALS
IN
TOTAL
AT
THE
SCHEDULED
TIME
BUT
ON
DATES
TO
BE
DECIDED
LATER
CMPT
FOUNDATIONS
OF
CONCURRENT
PROGRAMMING
CATALOGUE
DESCRIPTION
THEORY
AND
PRACTICE
OF
CONCURRENT
PROGRAMMING
PROCESS
INTERACTION
USING
SHARED
VARIABLES
AND
MESSAGE
PASSING
PARALLEL
COMPUTING
DEVELOPMENT
OF
CORRECT
PROGRAMS
GENERAL
PROBLEM
SOLVING
TECHNIQUES
SCIENTIFIC
COMPUTING
DISTRIBUTED
PROGRAMMING
PREREQUISITE
CMPT
AND
CMPT
OR
EQUIVALENT
COURSE
OBJECTIVES
PROVIDE
AN
INTRODUCTION
TO
FOUNDATIONAL
CONCEPTS
PARADIGMS
LANGUAGE
SUPPORT
AND
RESEARCH
FOR
CON
CURRENT
PROGRAMMING
STUDENT
EVALUATION
GRADING
SCHEME
ASSIGNMENTS
MIDTERM
EXAM
PAPER
FINAL
EXAM
TOTAL
CRITERIA
THAT
MUST
BE
MET
TO
PASS
SCHEDULE
ASSIGNMENT
ASSIGNED
DUE
PAPER
PROPOSAL
DUE
ASSIGNMENT
ASSIGNED
DUE
MIDTERM
EXAM
ASSIGNMENT
ASSIGNED
DUE
ASSIGNMENT
ASSIGNED
DUE
ASSIGNMENT
ASSIGNED
DUE
PAPER
DUE
MIDTERM
AND
FINAL
CUMULATIVE
MARKS
WILL
BE
SCALED
ATTENDENCE
EXPECTATION
STUDENTS
WILL
BE
EXPECTED
TO
KNOW
ALL
INFORMATION
PASSED
ON
DURING
LECTURES
AND
TUTORIALS
AND
THROUGH
THE
WEBPAGE
MOODLE
AND
EMAIL
IF
THEY
MISS
A
LECTURE
OR
TUTORIAL
THEY
ARE
RESPONSIBLE
FOR
ACQUIRING
MATERIAL
COVERED
IN
THE
SESSION
FINAL
EXAM
SCHEDULING
THE
REGISTRAR
SCHEDULES
ALL
FINAL
EXAMINATIONS
INCLUDING
DEFERRED
AND
SUPPLEMENTAL
EXAMINATIONS
STUDENTS
ARE
ADVISED
NOT
TO
MAKE
TRAVEL
ARRANGEMENTS
FOR
THE
EXAM
PERIOD
UNTIL
THE
OFFICIAL
EXAM
SCHEDULE
HAS
BEEN
POSTED
NOTE
ALL
STUDENTS
MUST
BE
PROPERLY
REGISTERED
IN
ORDER
TO
ATTEND
LECTURES
AND
RECEIVE
CREDIT
FOR
THIS
COURSE
TEXTBOOK
INFORMATION
REQUIRED
TEXT
GREGORY
ANDREWS
FOUNDATIONS
OF
MULTITHREADED
PARALLEL
AND
DISTRIBUTED
PROGRAMMING
ADDISON
WESLEY
RECOMMENDED
TEXTS
CARLOS
A
VARELA
PROGRAMMING
DISTRIBUTED
COMPUTING
SYSTEMS
A
FOUNDATIONAL
APPROACH
MIT
PRESS
LECTURE
SCHEDULE
TOPICS
TO
BE
COVERED
OVERVIEW
DISTRIBUTED
PROGRAMMING
MESSAGE
PASSING
RPC
AND
RENDEZVOUS
PARADIGMS
FOR
PROCESS
INTERACTION
IMPLEMENTATIONS
SHARED
VARIABLE
PROGRAMMING
PROCESSES
AND
SYNCHRONIZATION
LOCKS
AND
BARRIERS
SEMAPHORES
MONITORS
IMPLEMENTATIONS
PARALLEL
PROGRAMMING
SCIENTIFIC
COMPUTING
LANGUAGES
COMPILERS
LIBRARIES
AND
TOOLS
COURSE
OVERVIEW
THERE
WILL
BE
ABOUT
TUTORIALS
IN
TOTAL
AT
THE
SCHEDULED
TIME
BUT
ON
DATES
TO
BE
DECIDED
LATER
FOUNDATIONS
OF
CONCURRENT
PROGRAMMING
NADEEM
JAMALI
LINKS
WEB
HTTP
AGENTS
USASK
CA
HTTP
AGENTS
USASK
CA
NEWS
MOODLE
DISCUSSION
GROUP
FOR
CMPT
CMPT
INTRODUCTION
WHAT
YOU
NEED
TO
KNOW
ARCHITECTURE
PROGRAMMING
UNIX
DATA
STRUCTURES
DISCRETE
MATH
PLAN
FOR
TODAY
OVERVIEW
DETAILS
SEQUENTIAL
VS
CONCURRENT
SEQUENTIAL
PROGRAM
SEQUENCE
OF
ACTIONS
THAT
PRODUCE
A
RESULT
STATEMENTS
VARIABLES
CALLED
A
PROCESS
TASK
OR
THREAD
OF
CONTROL
CONCURRENT
PROGRAM
TWO
OR
MORE
PROCESSES
THAT
WORK
TOGETHER
COMMUNICATION
SHARED
VARIABLES
OR
SYNCHRONIZATION
MESSAGE
PASSING
HARDWARE
SINGLE
PROCESSOR
MULTIPROCESSOR
SHARED
MEMORY
MULTICOMPUTER
SEPARATE
MEMORIES
NETWORK
SLOWER
COMMUNICATION
SINGLE
PROCESSOR
MULTIPROCESSOR
SHARED
MEMORY
MULTICOMPUTER
SEPARATE
MEMORIES
MULTITHREADED
APPLICATIONS
WHAT
MORE
THAN
THREAD
USUALLY
SHARE
CPU
TIME
WHY
GOOD
WAY
TO
ORGANIZE
MODERN
SOFTWARE
SYSTEMS
OS
TIMESHARING
SERVERS
PC
WINDOWS
BROWSER
APPLETS
USER
UNIX
PIPES
THIS
BOOK
SED
EQN
GROFF
PARALLEL
APPLICATIONS
WHAT
PROCESSES
EXECUTE
ON
THEIR
OWN
PROCESSOR
WHY
SOLVE
A
PROBLEM
FASTER
OR
SOLVE
A
LARGER
PROBLEM
TWO
MAIN
ALGORITHM
PROGRAMMING
STYLES
ITERATIVE
LOOPS
DIVIDE
THEM
UP
RECURSIVE
DIVIDE
AND
CONQUER
WITH
CALLS
IN
PARALLEL
SEE
CHAPTERS
AND
FOR
EXAMPLES
DISTRIBUTED
APPLICATIONS
WHAT
PROCESSES
COMMUNICATE
OVER
A
NETWORK
WHY
OFFLOAD
WORK
SERVERS
CONNECT
TO
REMOTE
DATA
INTERNET
AIRLINES
BANKS
SCALABLE
PARALLEL
COMPUTING
ON
MULTICOMPUTERS
AND
NETWORKS
PROGRAMMING
PARADIGMS
ITERATIVE
PARALLELISM
RECURSIVE
PARALLELISM
PRODUCERS
AND
CONSUMERS
CLIENTS
AND
SERVERS
INTERACTING
PEERS
COURSE
COURSE
CONTENTS
APPLICATIONS
METHODS
PRINCIPLES
LANGUAGES
AND
LIBRARIES
PROGRAMMING
TECHNIQUES
IMPLEMENTATIONS
RELATED
COURSES
OS
ARCHITECTURE
ALGORITHMS
VARIOUS
APPLICATIONS
COURSE
CONTINUED
COURSE
DETAILS
GO
OVER
THE
SYLLABUS
FOR
NEXT
TIME
SKIM
PREFACE
AND
TOC
READ
CHAPTER
PLEASE
NOTE
THAT
THE
TOPICS
MAY
BE
CHANGED
BASICS
A
DIFFERENT
MODELS
OF
COMPUTING
B
VIRTUAL
MACHINES
OO
LANGUAGES
JAVA
C
A
CORE
JAVA
CONCEPTS
B
CORE
C
CONCEPTS
CONCURRENCY
A
THREADS
B
SHARED
MEMORY
VERSUS
NON
SHARED
MEMORY
COMMUNICATION
A
SOCKETS
NON
OO
LANGUAGES
A
ERLANG
SERVERS
A
JAVA
B
C
C
ERLANG
COMMUNICATION
II
A
SERIALIZATION
B
REFLECTION
MIDDLEWARE
I
A
CORBA
B
RMI
MIDDLEWARE
II
A
REMOTING
B
WEB
SERVICES
SERVICE
ORIENTED
SYSTEMS
RESOURCE
ORIENTED
SYSTEMS
A
SOA
B
SOAP
REST
CLOUD
COMPUTING
A
IAAS
B
PAAS
C
SAAS
MOBILE
DEVICES
A
NATIVE
VERSUS
MOBILE
WEB
B
HYBRID
POLICIES
MISSED
EXAMINATIONS
STUDENTS
WHO
HAVE
MISSED
AN
EXAM
OR
ASSIGNMENT
MUST
CONTACT
THEIR
INSTRUCTOR
AS
SOON
AS
POSSIBLE
ARRANGEMENTS
TO
MAKE
UP
THE
EXAM
MAY
BE
ARRANGED
WITH
THE
INSTRUCTOR
MISSED
EXAMS
THROUGHOUT
THE
YEAR
ARE
LEFT
UP
TO
THE
DISCRETION
OF
THE
INSTRUCTOR
IF
A
STUDENT
MAY
MAKE
UP
THE
EXAM
OR
WRITE
AT
A
DIFFERENT
TIME
IF
A
STUDENT
KNOWS
PRIOR
TO
THE
EXAM
THAT
SHE
HE
WILL
NOT
BE
ABLE
TO
ATTEND
THEY
SHOULD
LET
THE
INSTRUCTOR
KNOW
BEFORE
THE
EXAM
FINAL
EXAMS
A
STUDENT
WHO
IS
ABSENT
FROM
A
FINAL
EXAMINATION
THROUGH
NO
FAULT
OF
HIS
OR
HER
OWN
FOR
MEDICAL
OR
OTHER
VALID
REASONS
MAY
APPLY
TO
THE
COLLEGE
OF
ARTS
AND
SCIENCE
DEAN
OFFICE
THE
APPLICATION
MUST
BE
MADE
WITHIN
THREE
DAYS
OF
THE
MISSED
EXAMINATION
ALONG
WITH
SUPPORTING
DOCUMENTARY
EVIDENCE
DEFERRED
EXAMS
ARE
WRITTEN
DURING
THE
FEBRUARY
MID
TERM
BREAK
FOR
TERM
COURSES
AND
IN
EARLY
JUNE
FOR
TERM
AND
FULL
YEAR
COURSES
HTTP
WWW
ARTS
USASK
CA
STUDENTS
TRANSITION
TIPS
PHP
INCOMPLETE
COURSE
WORK
AND
FINAL
GRADES
WHEN
A
STUDENT
HAS
NOT
COMPLETED
THE
REQUIRED
COURSE
WORK
WHICH
INCLUDES
ANY
ASSIGNMENT
OR
EXAMINATION
INCLUDING
THE
FINAL
EXAMINATION
BY
THE
TIME
OF
SUBMISSION
OF
THE
FINAL
GRADES
THEY
MAY
BE
GRANTED
AN
EXTENSION
TO
PERMIT
COMPLETION
OF
AN
ASSIGNMENT
OR
GRANTED
A
DEFERRED
EXAMINATION
IN
THE
CASE
OF
ABSENCE
FROM
A
FINAL
EXAMINATION
EXTENSIONS
FOR
THE
COMPLETION
OF
ASSIGNMENTS
MUST
BE
APPROVED
BY
THE
DEPARTMENT
HEAD
OR
DEAN
IN
NON
DEPARTMENTALIZED
COLLEGES
AND
MAY
EXCEED
THIRTY
DAYS
ONLY
IN
UNUSUAL
CIRCUMSTANCES
THE
STUDENT
MUST
APPLY
TO
THE
INSTRUCTOR
FOR
SUCH
AN
EXTENSION
AND
FURNISH
SATISFACTORY
REASONS
FOR
THE
DEFICIENCY
DEFERRED
FINAL
EXAMINATIONS
ARE
GRANTED
AS
PER
COLLEGE
POLICY
IN
THE
INTERIM
THE
INSTRUCTOR
WILL
SUBMIT
A
COMPUTED
PERCENTILE
GRADE
FOR
THE
COURSE
WHICH
FACTORS
IN
THE
INCOMPLETE
COURSE
WORK
AS
A
ZERO
ALONG
WITH
A
GRADE
COMMENT
OF
INF
INCOMPLETE
FAILURE
IF
A
FAILING
GRADE
IN
THE
CASE
WHERE
THE
INSTRUCTOR
HAS
INDICATED
IN
THE
COURSE
OUTLINE
THAT
FAILURE
TO
COMPLETE
THE
REQUIRED
COURSE
WORK
WILL
RESULT
IN
FAILURE
IN
THE
COURSE
AND
THE
STUDENT
HAS
A
COMPUTED
PASSING
PERCENTILE
GRADE
A
FINAL
GRADE
OF
WILL
BE
SUBMITTED
ALONG
WITH
A
GRADE
COMMENT
OF
INF
INCOMPLETE
FAILURE
IF
AN
EXTENSION
IS
GRANTED
AND
THE
REQUIRED
ASSIGNMENT
IS
SUBMITTED
WITHIN
THE
ALLOTTED
TIME
OR
IF
A
DEFERRED
EXAMINATION
IS
GRANTED
AND
WRITTEN
IN
THE
CASE
OF
ABSENCE
FROM
THE
FINAL
EXAMINATION
THE
INSTRUCTOR
WILL
SUBMIT
A
REVISED
COMPUTED
FINAL
PERCENTAGE
GRADE
THE
GRADE
CHANGE
WILL
REPLACE
THE
PREVIOUS
GRADE
AND
ANY
GRADE
COMMENT
OF
INF
INCOMPLETE
FAILURE
WILL
BE
REMOVED
FOR
PROVISIONS
GOVERNING
EXAMINATIONS
AND
GRADING
STUDENTS
ARE
REFERRED
TO
THE
UNIVERSITY
COUNCIL
REGULATIONS
ON
EXAMINATIONS
SECTION
OF
THE
CALENDAR
UNIVERSITY
OF
SASKATCHEWAN
CALENDAR
EXAMS
GRADES
GRADING
SYSTEM
DEPARTMENT
POLICY
ON
ACADEMIC
HONESTY
STUDENTS
ARE
EXPECTED
TO
BE
ACADEMICALLY
HONEST
IN
ALL
OF
THEIR
SCHOLARLY
WORK
INCLUDING
COURSE
ASSIGNMENTS
AND
EXAMINATIONS
ACADEMIC
HONESTY
IS
DEFINED
AND
DESCRIBED
IN
THE
DEPARTMENT
OF
COMPUTER
SCIENCE
STATEMENT
ON
ACADEMIC
HONESTY
HTTP
WWW
CS
USASK
CA
CLASSES
ACADEMICHONESTY
SHTML
AND
THE
UNIVERSITY
OF
SASKATCHEWAN
ACADEMIC
HONESTY
WEBSITE
HTTP
WWW
USASK
CA
HONESTY
STUDENTS
USING
THE
E
HANDIN
SYSTEM
TO
SUBMIT
AN
ASSIGNMENT
FOR
THE
FIRST
TIME
WILL
BE
PROMPTED
WITH
A
DECLARATION
OF
ACADEMIC
HONESTY
AFTER
READING
THE
DECLARATION
YOU
WILL
BE
PRESENTED
WITH
THREE
OPTIONS
AGREE
DISAGREE
OR
DECIDE
LATER
IF
YOU
CHOOSE
DECIDE
LATER
YOU
WILL
BE
PROMPTED
AGAIN
THE
NEXT
TIME
YOU
LOG
INTO
THE
E
HANDIN
SYSTEM
AGREEMENT
OR
DISAGREEMENT
APPLIES
TO
ALL
CMPT
COURSES
IN
WHICH
YOU
ARE
REGISTERED
FOR
CLASSES
THAT
DO
NOT
USE
THE
E
HANDIN
SYSTEM
EACH
STUDENT
MAY
BE
REQUESTED
TO
SUBMIT
A
SIGNED
PAPER
COPY
OF
THE
DECLARATION
OF
ACADEMIC
HONESTY
TO
THEIR
COURSE
INSTRUCTOR
AN
INSTRUCTOR
AT
THEIR
OPTION
MAY
ASK
STUDENTS
TO
SIGN
THE
PAPER
DECLARATION
EVEN
IF
THEIR
CLASS
IS
MAKING
USE
OF
E
HANDIN
THE
DEPARTMENT
HAS
DECIDED
TO
CREATE
A
DEPARTMENTAL
ACADEMIC
DISHONESTY
PANEL
WHICH
CONSISTS
OF
TWO
FACULTY
MEMBERS
APPOINTED
BY
THE
DEPARTMENT
HEAD
AND
ONE
STUDENT
MEMBER
APPOINTED
BY
THE
CSSS
IF
AN
INSTRUCTOR
HAS
REASON
TO
SUSPECT
ACADEMIC
DISHONESTY
ON
AN
ASSIGNMENT
THE
INSTRUCTOR
WILL
NOT
RETURN
THE
ORIGINAL
ASSIGNMENT
TO
THE
STUDENT
INSTEAD
IT
WILL
BE
FORWARDED
ALONG
WITH
ALL
RELEVANT
EVIDENCE
TO
THE
DEPARTMENT
ACADEMIC
DISHONESTY
PANEL
THE
PANEL
WILL
DELIBERATE
ON
THE
CASE
POSSIBLY
REQUIRING
TESTIMONY
FROM
THE
STUDENT
INVOLVED
THE
PANEL
IN
CONSULTATION
WITH
THE
INSTRUCTOR
WILL
DECIDE
WHETHER
THE
CASE
SHOULD
BE
FORWARDED
TO
THE
ARTS
AND
SCIENCE
STUDENT
ACADEMIC
AFFAIRS
COMMITTEE
FOR
A
DISCIPLINARY
HEARING
OR
WHETHER
THE
CASE
SHOULD
BE
HANDLED
LOCALLY
WITHIN
THE
DEPARTMENT
SHOULD
THE
PANEL
FIND
NO
EVIDENCE
OF
DISHONESTY
THE
CASE
WILL
BE
EXCUSED
IN
CASES
WHERE
DISHONESTY
IS
DETERMINED
THE
PANEL
MAY
DECIDE
TO
ISSUE
A
WARNING
AND
DELETE
ANY
MARK
ASSOCIATED
WITH
THAT
ASSIGNMENT
IN
SUCH
CASES
A
FORMAL
WARNING
LETTER
WILL
BE
SENT
TO
THE
STUDENT
COPIED
TO
THE
INSTRUCTOR
AND
THE
ARTS
AND
SCIENCE
STUDENT
ACADEMIC
AFFAIRS
COMMITTEE
AND
NO
OTHER
ACADEMIC
PENALTY
WILL
BE
APPLIED
IF
THE
STUDENT
OR
INSTRUCTOR
IS
DISSATISFIED
WITH
THE
PANEL
DECISION
EITHER
PARTY
MAY
REQUEST
THAT
THE
CASE
BE
REVIEWED
BY
THE
STUDENT
ACADEMIC
AFFAIRS
COMMITTEE
THE
STUDENT
ACADEMIC
AFFAIRS
COMMITTEE
TREATS
ALL
CASES
ACCORDING
TO
THE
UNIVERSITY
POLICY
AND
HAS
THE
RIGHT
TO
APPLY
STRICT
ACADEMIC
PENALTIES
SEE
HTTP
WWW
USASK
CA
UNIVERSITY
REPORTS
SHTML
CS
INTRODUCTION
TO
SYSTEM
SOFTWARE
BY
WONSUN
AHN
INSTRUCTOR
WONSUN
AHN
COURSE
WEBSITE
CS
PITT
EDU
WAHN
TEACHING
WHAT
IS
SYSTEM
PROGRAMMING
SYSTEM
PROGRAMMING
IS
THE
ACT
OF
WRITING
SYSTEM
CODE
THEN
WHAT
IS
A
SYSTEM
AND
WHAT
IS
SYSTEM
CODE
A
SYSTEM
IN
PLAIN
ENGLISH
ACCORDING
TO
THE
MERRIAM
WEBSTER
DICTIONARY
MEANS
A
GROUP
OF
RELATED
PARTS
THAT
WORK
TOGETHER
IN
THE
CONTEXT
OF
COMPUTING
A
SYSTEM
IS
A
GROUP
OF
SOFTWARE
OR
HARDWARE
PARTS
THAT
WORK
TOGETHER
TO
ALLOW
A
COMPUTER
TO
RUN
HARDWARE
PARTS
COMPRISE
COMPONENTS
LIKE
THE
CPU
THE
MAIN
MEMORY
AND
PERIPHERAL
DEVICES
SOFTWARE
PARTS
COMPRISE
COMPONENTS
LIKE
THE
OPERATING
SYSTEM
DEVICE
DRIVERS
THE
COMPILER
AND
ALSO
APPLICATIONS
IN
PARTICULAR
APPLICATIONS
ARE
CODES
THAT
PERFORM
TASKS
THAT
A
USER
WISHES
TO
APPLY
THE
COMPUTER
TO
SUCH
AS
WORD
PROCESSING
WEB
BROWSING
AND
GAMING
SYSTEM
CODE
IS
EVERYTHING
ELSE
BESIDES
APPLICATION
CODE
SYSTEM
CODE
DOES
NOT
PERFORM
ANY
USER
TASK
IN
ITSELF
BUT
ALLOWS
APPLICATION
CODE
TO
RUN
ON
TOP
OF
THE
GIVEN
HARDWARE
COMPONENTS
FOR
EXAMPLE
OPERATING
SYSTEM
CODE
MANAGES
THE
HARDWARE
THE
CPU
THE
MAIN
MEMORY
PERIPHERAL
DEVICES
THAT
APPLICATIONS
RUN
ON
TOP
OF
IT
ALSO
PROVIDES
COMMON
SERVICES
TO
APPLICATIONS
SUCH
AS
FILE
STORAGE
SERVICE
AND
NETWORK
SERVICE
ANOTHER
EXAMPLE
IS
THE
COMPILER
WHICH
TRANSLATES
APPLICATIONS
WRITTEN
IN
PROGRAMMING
LANGUAGES
SUCH
AS
JAVA
PYTHON
OR
C
TO
MACHINE
LANGUAGE
UNDERSTOOD
BY
THE
UNDERLYING
HARDWARE
WHY
IS
LEARNING
SYSTEM
PROGRAMMING
IMPORTANT
WELL
MOST
OF
YOU
ARE
COMPUTER
SCIENCE
OR
COMPUTER
ENGINEERING
MAJORS
OR
PLANNING
TO
BE
HOPEFULLY
AND
YOUR
FUTURE
JOBS
WHETHER
THEY
BE
IN
RESEARCH
OR
INDUSTRY
WILL
OFTEN
INVOLVE
BUILDING
AND
MODIFYING
PARTS
OF
A
SYSTEM
AND
WRITING
SYSTEM
CODE
EVEN
IF
YOU
DO
NOT
DIRECTLY
WRITE
SYSTEM
CODE
AND
WRITE
APPLICATION
CODE
LEARNING
SYSTEM
PROGRAMMING
WILL
GIVE
YOU
A
DEEPER
UNDERSTANDING
OF
HOW
YOUR
APPLICATION
INTERACTS
WITH
THE
REST
OF
THE
SYSTEM
THIS
WILL
HELP
YOU
PROGRAM
IN
A
CORRECT
AND
PERFORMANT
FASHION
ALSO
IT
WILL
HELP
YOU
UNDERSTAND
WHEN
YOUR
PROGRAM
DOES
NOT
BEHAVE
WELL
WHAT
WE
WILL
LEARN
IN
THIS
COURSE
THE
AREA
OF
SYSTEMS
IS
TOO
VAST
TO
COVER
IN
A
SINGLE
COURSE
SO
IN
THIS
COURSE
WE
WILL
FOCUS
ON
LEARNING
HOW
AN
APPLICATION
INTERACTS
WITH
THE
REST
OF
THE
SYSTEM
AND
HOW
VARIOUS
COMPONENTS
OF
A
SYSTEM
INTERACT
WITH
EACH
OTHER
WE
WILL
ONLY
LEARN
PARTS
OF
THE
SYSTEM
THAT
IS
RELEVANT
TO
THIS
TOPIC
AND
THE
DETAILED
IMPLEMENTATION
OF
EACH
COMPONENT
WILL
BE
DEALT
IN
MORE
ADVANCED
COURSEWORK
SUCH
AS
CS
OPERATING
SYSTEMS
AND
CS
STRUCTURE
OF
PROGRAMMING
LANGUAGES
COMPILERS
MOST
OF
YOU
HAVE
PROBABLY
DONE
SOME
APPLICATION
PROGRAMMING
USING
PROGRAMMING
LANGUAGES
SUCH
AS
JAVA
PYTHON
OR
JAVASCRIPT
THESE
APPLICATION
CENTRIC
LANGUAGES
TYPICALLY
RUN
ON
TOP
OF
A
PIECE
OF
SOFTWARE
CALLED
A
VIRTUAL
MACHINE
THIS
VIRTUAL
AS
IN
NOT
REAL
MACHINE
SOFTWARE
MAINTAINS
A
FAÇADE
OVER
THE
REAL
MACHINE
AND
PROVIDES
AN
ILLUSION
OF
AN
IDEALIZED
HARDWARE
MACHINE
THAT
IS
EASY
TO
PROGRAM
FOR
EXAMPLE
THE
JAVA
VIRTUAL
MACHINE
PROVIDES
THE
ILLUSION
OF
OBJECTS
FLOATING
AROUND
IN
LIMITLESS
SPACE
AND
PERFORMING
TASKS
BY
CALLING
METHODS
ON
THOSE
OBJECTS
HOWEVER
TAKE
THAT
FACADE
AWAY
AND
EVERY
PROGRAM
MUST
LIVE
AND
OPERATE
IN
CONCRETE
HARDWARE
WITH
LIMITED
HARDWARE
RESOURCES
AND
VARIED
HARDWARE
COMPONENTS
WHAT
MORE
THAT
PROGRAM
MUST
SHARE
THE
LIMITED
HARDWARE
RESOURCES
WITH
OTHER
PROGRAMS
AND
PLAY
NICE
WITH
EACH
OTHER
IN
FACT
THE
REASON
JAVA
PROGRAMS
DON
T
HAVE
TO
DEAL
WITH
THE
NITTY
GRITTY
DETAILS
OF
THE
SYSTEM
IS
BECAUSE
THE
JAVA
VIRTUAL
MACHINE
DOES
ALL
THE
DIRTY
WORK
BENEATH
IT
SO
HOW
DOES
A
PROGRAM
ACTUALLY
RUN
AND
INTERACT
WITH
THE
UNDERLYING
HARDWARE
A
PROGRAM
STARTS
OUT
ITS
LIFE
BY
FIRST
BEING
TRANSLATED
FROM
PROGRAMMING
LANGUAGE
TO
MACHINE
LANGUAGE
USING
A
COMPILER
THE
RESULTING
MACHINE
LANGUAGE
CODE
IS
ALSO
CALLED
AN
EXECUTABLE
SINCE
IT
CAN
BE
EXECUTED
DIRECTLY
ON
THE
MACHINE
WITHOUT
FURTHER
TRANSLATION
AN
EXECUTABLE
IS
ALSO
CALLED
A
BINARY
SINCE
THE
MACHINE
LANGUAGE
LOOKS
LIKE
A
SERIES
OF
BINARY
AND
TO
THE
HUMAN
EYE
WHEN
THIS
BINARY
IS
EXECUTED
A
PROCESS
FOR
THIS
PROGRAM
IS
CREATED
BY
THE
OPERATING
SYSTEM
A
PROCESS
IS
AN
INCARNATION
OF
A
PROGRAM
IN
THE
SYSTEM
IT
STATE
IS
REPRESENTED
IN
MEMORY
AS
CODE
AND
DATA
AND
EXECUTION
TAKES
PLACE
BY
CONTINUOUSLY
MODIFYING
THIS
STATE
IN
ORDER
FOR
THIS
INTERNAL
MEMORY
STATE
TO
BE
USEFUL
IT
MUST
BE
MADE
VISIBLE
TO
THE
EXTERNAL
WORLD
BY
WRITING
TO
A
FILE
SHOWING
SOMETHING
ON
THE
MONITOR
OR
SENDING
DATA
OVER
THE
NETWORK
THIS
IS
WHERE
A
LOT
OF
THE
SYSTEM
INTERACTION
TAKES
PLACE
SYSTEM
CODE
IS
TYPICALLY
ORGANIZED
INTO
LAYERS
DEPENDING
ON
HOW
CLOSE
THE
CODE
IS
TO
THE
UNDERLYING
COMPUTER
HARDWARE
AS
AN
EXAMPLE
HERE
ARE
VARIOUS
LAYERS
OF
THE
SYSTEM
USING
OPERATING
SYSTEM
CODE
AS
AN
EXAMPLE
LAYER
APPLICATIONS
USER
LEVEL
LIBRARIES
LAYER
DEVICE
INDEPENDENT
OPERATING
SYSTEM
CODE
FILE
SYSTEM
NETWORK
STACK
LAYER
DEVICE
DEPENDENT
OPERATING
SYSTEM
CODE
DEVICE
DRIVERS
LAYER
COMPUTER
HARDWARE
APPLICATIONS
INTERACT
WITH
THE
SYSTEM
THROUGH
SYSTEM
CALLS
THE
APPLICATION
REQUESTING
SOMETHING
FROM
THE
SYSTEM
AND
SIGNALS
THE
SYSTEM
SIGNALING
TO
THE
APPLICATION
THAT
SOME
EVENT
OF
INTEREST
TO
THE
APPLICATION
HAS
HAPPENED
WHEN
AN
APPLICATION
PLACES
A
SYSTEM
CALL
THE
REQUEST
TRAVERSES
THE
VARIOUS
LAYERS
OF
AN
OPERATING
SYSTEM
TO
GET
TO
THE
UNDERLYING
HARDWARE
AS
AN
EXAMPLE
LET
SAY
A
CHAT
APPLICATION
WANTS
TO
SEND
A
MESSAGE
ACROSS
THE
NETWORK
LAYER
THE
APPLICATION
WRITES
A
MESSAGE
TO
A
NETWORK
SOCKET
BY
CALLING
A
SOCKET
API
IN
GLIBC
THE
C
RUNTIME
LIBRARY
THE
GLIBC
LIBRARY
PERFORMS
A
WRITE
SYSTEM
CALL
ON
THE
SOCKET
LAYER
THE
MESSAGE
TRAVERSES
THE
VARIOUS
LAYERS
OF
THE
TCP
IP
PROTOCOL
STACK
TO
FORM
A
PACKET
AND
THEN
THE
PACKET
GETS
SENT
TO
THE
UNDERLYING
ETHERNET
DEVICE
DRIVER
LAYER
THE
ETHERNET
DEVICE
DRIVER
TALKS
TO
THE
UNDERLYING
ETHERNET
NETWORK
CARD
TO
WRITE
THE
PACKET
INTO
THE
INTERNAL
BUFFER
OF
THE
CARD
LAYER
THE
ETHERNET
CARD
SENDS
ALONG
THE
PACKET
IN
ITS
BUFFER
THROUGH
THE
PHYSICAL
LAN
LINE
THROUGH
THIS
COURSE
WE
WILL
LEARN
THIS
ENTIRE
PROCESS
STARTING
FROM
HOW
THE
COMPILER
TRANSLATES
YOUR
PROGRAM
TO
MACHINE
CODE
EXECUTABLE
TO
HOW
THE
OPERATING
SYSTEM
TAKES
THAT
EXECUTABLE
AND
RUNS
IT
ON
THE
GIVEN
HARDWARE
SYSTEM
WHY
LEARN
C
WE
WILL
START
THE
COURSE
BY
LEARNING
C
BUT
WHY
GO
THROUGH
THE
PAINS
OF
LEARNING
A
NEW
LANGUAGE
CAN
WE
NOT
STAY
IN
THE
NICE
WORLD
OF
JAVA
OR
PYTHON
IT
IS
BECAUSE
C
IS
THE
LANGUAGE
IN
WHICH
THE
VAST
MAJORITY
OF
SYSTEM
CODE
IS
WRITTEN
IN
MOST
OPERATING
SYSTEMS
ARE
WRITTEN
IN
C
MOST
FILE
SYSTEMS
ARE
WRITTEN
IN
C
MOST
DEVICE
DRIVERS
ARE
WRITTEN
IN
C
MOST
COMPILERS
ARE
WRITTEN
IN
C
AND
FOR
GOOD
REASON
C
STARTED
ITS
LIFE
AS
A
LANGUAGE
FOR
WRITING
OPERATING
SYSTEM
CODE
UNLIKE
JAVA
OR
PYTHON
A
C
PROGRAM
EXECUTES
DIRECTLY
ON
THE
REAL
HARDWARE
MACHINE
INSTEAD
OF
AN
IDEALIZED
VIRTUAL
MACHINE
SUCH
AS
THE
JAVA
VIRTUAL
MACHINE
AS
SUCH
IT
ALLOWS
MUCH
GREATER
CONTROL
OF
THE
UNDERLYING
HARDWARE
COMPARED
TO
JAVA
THAT
ALSO
MEANS
YOU
WILL
BE
EXPOSED
TO
ALL
THE
IDIOSYNCRASIES
OF
THE
UNDERLYING
HARDWARE
WHICH
WILL
MAKE
YOUR
LIFE
HARDER
AS
THE
SAYING
GOES
WITH
GREAT
POWER
COMES
GREAT
RESPONSIBILITY
YOU
WILL
KNOW
WHAT
THIS
MEANS
FIRST
HAND
AFTER
YOUR
C
PROGRAM
CRASHES
ON
YOU
A
FEW
TIMES
BUT
AFTER
MASTERING
C
PROGRAMMING
YOU
WILL
GAIN
A
MUCH
GREATER
UNDERSTANDING
OF
HOW
SYSTEM
CODE
WORKS
IN
SHORT
C
DOES
NOT
PROVIDE
THE
BELLS
AND
WHISTLES
OF
A
VERY
HIGH
LEVEL
LANGUAGE
LIKE
JAVA
THAT
MAKES
PROGRAMMING
EASIER
BUT
IT
DOES
COME
WITH
UNIQUE
ADVANTAGES
THAT
MAKE
IT
SUITABLE
FOR
SYSTEM
PROGRAMMING
OR
WRITING
APPLICATIONS
CLOSE
TO
THE
SYSTEM
SUCH
AS
THE
JAVA
VIRTUAL
MACHINE
CONS
HARD
TO
USE
PROS
EFFICIENCY
CONTROL
NO
CONCEPT
OF
OBJECT
ORIENTED
PROGRAMMING
NO
LANGUAGE
SAFETY
FEATURES
SUCH
THAT
A
PROGRAM
EITHER
BEHAVES
OR
THROWS
A
WELL
DEFINED
EXCEPTION
E
G
A
NULL
POINTER
EXCEPTION
NO
AUTOMATIC
GARBAGE
COLLECTION
OF
MEMORY
CAN
OPERATE
IN
RESOURCE
CONSTRAINED
ENVIRONMENTS
I
E
EXPLICIT
MEMORY
MANAGEMENT
INSTEAD
OF
GARBAGE
COLLECTION
IS
VERY
EFFICIENT
AND
HAS
LITTLE
RUNTIME
OVERHEAD
I
E
NO
LANGUAGE
SAFETY
FEATURES
MEANS
NO
RUNTIME
CHECKING
OF
SAFETY
PROPERTIES
ALLOWS
FOR
DIRECT
AND
RAW
CONTROL
OVER
HARDWARE
I
E
CAN
ACCESS
AN
ARBITRARY
LOCATION
IN
YOUR
DRAM
OR
DIRECT
YOUR
CPU
TO
START
EXECUTING
AT
AN
ARBITRARY
LOCATION
LETS
THE
PROGRAMMER
WRITE
PARTS
OF
THE
PROGRAM
DIRECTLY
IN
ASSEMBLY
LANGUAGE
I
E
CRUCIAL
FOR
DIRECT
MANAGEMENT
OF
HARDWARE
RESOURCES
LEARNING
C
IS
LIKE
SWALLOWING
THE
RED
PILL
IN
THE
MOVIE
MATRIX
IT
IS
A
PAINFUL
PROCESS
BUT
IT
ALLOWS
YOU
TO
SEE
THE
TRUTH
BEHIND
THE
IDEALIZED
WORLD
OF
THE
VIRTUAL
MACHINE
CS
PROJECT
BLACKJACK
EXIF
VIEWER
DUE
SUNDAY
OCTOBER
AT
YOUR
FIRST
PROJECT
IS
TO
WRITE
TWO
PROGRAMS
IN
C
THAT
PROVIDE
SOME
EXPERIENCE
WITH
A
WIDE
RANGE
OF
THE
TOPICS
WE
HAVE
BEEN
DISCUSSING
IN
CLASS
BLACKJACK
IMPLEMENTATION
POINTS
BLACKJACK
ALSO
KNOWN
AS
IS
A
MULTIPLAYER
CARD
GAME
WITH
FAIRLY
SIMPLE
RULES
FOR
THIS
ASSIGNMENT
YOU
WILL
BE
IMPLEMENTING
A
SIMPLIFIED
VERSION
WHERE
A
USER
CAN
PLAY
AGAINST
THE
COMPUTER
WHO
ACTS
AS
DEALER
TWO
CARDS
ARE
DEALT
TO
EACH
PLAYER
THE
DEALER
SHOWS
ONE
CARD
FACE
UP
AND
THE
OTHER
IS
FACE
DOWN
THE
PLAYER
GETS
TO
SEE
BOTH
OF
HIS
OR
HER
CARDS
AND
THE
TOTAL
OF
THEM
IS
ADDED
FACE
CARDS
KINGS
QUEENS
AND
JACKS
ARE
WORTH
POINTS
ACES
ARE
WORTH
OR
POINTS
AND
ALL
OTHER
CARDS
ARE
WORTH
THEIR
FACE
VALUE
THE
GOAL
OF
THE
GAME
IS
TO
GET
AS
CLOSE
TO
BLACKJACK
WITHOUT
GOING
OVER
CALLED
BUSTING
THE
HUMAN
PLAYER
GOES
FIRST
MAKING
HIS
OR
HER
DECISIONS
BASED
ON
THE
SINGLE
DEALER
CARD
SHOWING
THE
PLAYER
HAS
TWO
CHOICES
HIT
OR
STAND
HIT
MEANS
TO
TAKE
ANOTHER
CARD
STAND
MEANS
THAT
THE
PLAYER
WISHES
NO
MORE
CARDS
AND
ENDS
THE
TURN
ALLOWING
FOR
THE
DEALER
TO
PLAY
THE
DEALER
MUST
HIT
IF
THEIR
CARD
TOTAL
IS
LESS
THAN
AND
MUST
STAND
IF
IT
IS
OR
HIGHER
WHICHEVER
PLAYER
GETS
CLOSEST
TO
WITHOUT
EXCEEDING
IT
WINS
REQUIREMENTS
AND
HINTS
HAVE
THE
PROGRAM
INTELLIGENTLY
DETERMINE
IF
AN
ACE
SHOULD
BE
INTERPRETED
AS
A
OR
AN
BY
COUNTING
THE
NUMBER
OF
ACES
DEALT
AND
ADJUSTING
THE
TOTAL
DOWN
IF
OVER
AND
AN
ACE
EXISTS
GENERATING
RANDOM
NUMBERS
IN
C
IS
A
TWO
STEP
PART
FIRST
WE
NEED
TO
SEED
THE
RANDOM
NUMBER
GENERATOR
ONCE
PER
PROGRAM
THE
IDIOM
TO
DO
THIS
IS
SRAND
UNSIGNED
INT
TIME
NULL
WHEN
WE
NEED
RANDOM
NUMBERS
WE
CAN
USE
THE
RAND
FUNCTION
IT
RETURNS
AN
UNSIGNED
INTEGER
BETWEEN
AND
WE
CAN
USE
MODULUS
TO
REDUCE
IT
TO
THE
RANGE
WE
NEED
INT
VALUE
RAND
HIGH
LOW
LOW
REMEMBER
THAT
GETTING
A
CARD
WORTH
IS
MORE
COMMON
BECAUSE
OF
THE
FACE
CARDS
SO
GENERATE
A
RANDOM
CARD
NOT
A
RANDOM
VALUE
YOU
CAN
ASSUME
THERE
IS
AN
INFINITE
DECK
OF
CARDS
I
E
DRAWING
AN
ACE
WILL
NOT
DECREASE
THE
PROBABILITY
OF
DRAWING
AN
ACE
THE
NEXT
TIME
EXIF
VIEWER
POINTS
AN
EXIF
TAG
IS
EMBEDDED
IN
MANY
IMAGE
FILES
TAKEN
ON
A
DIGITAL
CAMERA
TO
ADD
METADATA
ABOUT
THE
CAMERA
AND
EXPOSURE
IT
IS
A
COMPLICATED
FORMAT
BUT
WE
CAN
SIMPLIFY
IT
TO
WHERE
WE
CAN
WRITE
A
SIMPLE
VIEWER
THAT
WILL
WORK
WITH
MANY
JPEG
FILES
A
JPEG
FILE
BEGINS
WITH
THE
BYTE
SEQUENCE
AFTER
THAT
A
SPECIAL
JPEG
MARKER
INDICATES
AN
APPLICATION
SPECIFIC
SEGMENT
CALLED
WE
WILL
ASSUME
BUT
YOU
MUST
VERIFY
THAT
THERE
IS
NO
SECTION
PRIOR
TO
THIS
MEANS
THE
FIRST
BYTES
OF
A
JPEG
WITH
AN
EXIF
TAG
WILL
BE
OFFSET
LENGTH
VALUE
DESCRIPTION
JPEG
START
OF
FILE
MARKER
JPEG
MARKER
LENGTH
OF
THE
BLOCK
ALWAYS
BIG
ENDIAN
EXIF
EXIF
STRING
NUL
TERMINATOR
AND
ZERO
BYTE
II
OR
MM
ENDIANNESS
II
MEANS
INTEL
LITTLE
ENDIAN
THIS
IS
THE
START
OF
THE
TIFF
HEADER
VERSION
NUMBER
IS
ALWAYS
OFFSET
TO
START
OF
EXIF
BLOCK
FROM
START
OF
TIFF
BLOCK
BYTE
OF
THE
FILE
NEXT
AN
ARRAY
OF
TIFF
TAGGED
IMAGE
FILE
FORMAT
TAGS
WILL
STORE
THE
INFORMATION
WE
ARE
LOOKING
FOR
AT
OFFSET
WE
FIND
A
BYTE
UNSIGNED
SHORT
COUNT
THAT
TELLS
US
HOW
MANY
TAGS
THERE
WILL
BE
IN
THIS
SECTION
A
TIFF
TAG
IS
BYTES
THAT
ARE
DEFINED
AS
OFFSET
LENGTH
DESCRIPTION
TAG
IDENTIFIER
DATA
TYPE
NUMBER
OF
DATA
ITEMS
VALUE
OR
OFFSET
OF
DATA
ITEMS
LOOP
THROUGH
THE
FILE
COUNT
TIMES
READING
A
SINGLE
TIFF
TAG
AT
A
TIME
WE
WILL
ONLY
BE
CONCERNED
WITH
DIFFERENT
TAGS
IN
THIS
SECTION
SIMPLY
IGNORE
ANY
OTHER
TAG
THAT
APPEARS
THE
THREE
TAGS
WE
ARE
CONCERNED
WITH
HAVE
THE
BYTE
IDENTIFIERS
IN
THE
TABLE
BELOW
TAG
IDENTIFIER
DATA
TYPE
DESCRIPTION
ASCII
STRING
MANUFACTURER
STRING
ACSII
STRING
CAMERA
MODEL
STRING
BIT
INTEGER
EXIF
SUB
BLOCK
ADDRESS
LET
US
TAKE
THE
FIRST
ONE
AS
AN
EXAMPLE
WE
READ
IN
A
BYTE
TIFF
TAG
AND
FIND
THAT
ITS
IDENTIFIER
FIELD
IS
ITS
DATA
TYPE
FIELD
WILL
BE
WHICH
MEANS
THAT
THE
DATA
IS
ENCODED
IN
AN
ASCII
STRING
THE
NUMBER
OF
DATA
ITEMS
FIELD
WILL
TELL
US
HOW
MANY
BYTES
OUR
STRING
HAS
THE
FINAL
FIELD
IN
THE
TAG
CAN
CONTAIN
THE
VALUE
OF
THE
DATA
ITSELF
IF
IT
FITS
IN
BYTES
OR
IT
CAN
CONTAIN
AN
OFFSET
TO
THE
DATA
ELSEWHERE
IN
THE
FILE
SINCE
AN
ARBITRARY
STRING
CANNOT
FIT
IN
BYTES
IN
OUR
CASE
THIS
VALUE
IS
AN
OFFSET
IT
AN
OFFSET
FROM
THE
BEGINNING
OF
THE
TIFF
HEADER
WHICH
OCCURRED
AT
BYTE
OF
THE
FILE
SO
WE
SEEK
TO
OFFSET
IN
THE
FILE
AND
READ
EACH
LETTER
FROM
THAT
POSITION
IN
THE
FILE
UNTIL
WE
HAVE
READ
THEM
ALL
WE
LL
ENCOUNTER
A
NUL
TERMINATOR
AT
THE
END
AT
THIS
POINT
WE
VE
READ
THE
MANUFACTURER
STRING
WE
MUST
SEEK
BACK
TO
THE
LOCATION
IN
THE
FILE
WHERE
WE
WERE
READING
TAGS
AND
CONTINUE
ON
TO
THE
NEXT
ONE
TO
MAKE
THIS
CONCRETE
SAY
WE
ENCOUNTER
OUR
MANUFACTURER
STRING
TAG
WHILE
READING
BYTES
OF
OUR
FILE
THE
TAG
WOULD
BE
THE
TELLS
US
HOW
MANY
BYTES
THE
STRING
WILL
BE
INCLUDING
THE
NUL
TERMINATOR
THE
TELLS
US
TO
SEEK
TO
BYTES
FROM
THE
START
OF
THE
FILE
THE
BYTES
IS
THE
OFFSET
OF
THE
TIFF
HEADER
FROM
THE
START
OF
THE
FILE
WHEN
WE
SEEK
TO
OFFSET
WE
READ
IN
CANON
THE
MANUFACTURER
OF
THE
CAMERA
WE
MUST
NOW
SEEK
BACK
TO
OFFSET
SO
THAT
WE
CAN
READ
THE
NEXT
TAG
IN
THIS
SECTION
IF
WE
ENCOUNTER
THE
IDENTIFIER
THERE
IS
AN
ADDITIONAL
EXIF
BLOCK
ELSEWHERE
IN
THE
FILE
WE
CAN
STOP
READING
AT
THIS
POINT
EVEN
IF
WE
HAVEN
T
READ
ALL
COUNT
TAGS
BECAUSE
THE
TIFF
FORMAT
STATES
THAT
ALL
IDENTIFIERS
MUST
BE
IN
SORTED
ORDER
WE
WILL
SEEK
TO
THE
OFFSET
SPECIFIED
IN
THIS
EXIF
SUB
BLOCK
TAG
AGAIN
BYTES
THERE
WE
LL
REPEAT
THE
ABOVE
PROCESS
ONE
MORE
TIME
TO
GET
MORE
SPECIFIC
INFORMATION
ABOUT
THE
PICTURE
FIRST
READ
IN
A
NEW
COUNT
AS
AN
UNSIGNED
SHORT
NEXT
LOOP
READING
MORE
BYTE
TIFF
TAGS
FROM
THE
FILE
THIS
TIME
WE
LL
BE
CONCERNED
WITH
THE
FOLLOWING
FIELDS
TAG
IDENTIFIER
DATA
TYPE
DESCRIPTION
BIT
INTEGER
WIDTH
IN
PIXELS
BIT
INTEGER
HEIGHT
IN
PIXELS
BIT
INTEGER
ISO
SPEED
FRACTION
OF
BIT
UNSIGNED
INTEGERS
EXPOSURE
SPEED
FRACTION
OF
BIT
UNSIGNED
INTEGERS
F
STOP
FRACTION
OF
BIT
UNSIGNED
INTEGERS
LENS
FOCAL
LENGTH
ASCII
STRING
DATE
TAKEN
THE
GOOD
NEWS
IS
THAT
TYPE
MEANS
THAT
THE
VALUE
IS
DIRECTLY
ENCODED
IN
THE
LAST
BYTES
OF
OUR
TAG
AND
NO
SEEKING
NEEDS
TO
BE
DONE
TYPE
REQUIRES
US
TO
BEHAVE
LIKE
WE
DID
WITH
THE
STRING
BUT
RATHER
THAN
READING
SEVERAL
SINGLE
BYTE
CHARACTERS
WE
WILL
READ
UNSIGNED
INTS
DISPLAY
THE
RATIO
OF
THE
TWO
NUMBERS
AS
SHOWN
IN
THE
EXAMPLE
BELOW
WHAT
TO
DO
FOR
YOUR
PROJECT
YOU
WILL
MAKE
A
UTILITY
THAT
CAN
PRINT
THE
CONTENTS
OF
AN
EXISTING
TAG
IF
THERE
MAKE
A
PROGRAM
CALLED
EXIFVIEW
AND
MAKE
IT
SO
THAT
IT
RUNS
WITH
THE
FOLLOWING
COMMAND
LINE
IT
SHOULD
PRINT
THE
CONTENTS
OF
THE
EXIF
TAG
TO
THE
CONSOLE
IF
PRESENT
OR
GIVE
A
MESSAGE
IF
NOT
PRESENT
OR
READABLE
BY
OUR
PROGRAM
OUTPUT
HINTS
AND
REQUIREMENTS
WE
NEED
TO
TREAT
THESE
FILES
AS
BINARY
FILES
RATHER
THAN
TEXT
FILES
MAKE
SURE
TO
OPEN
THE
FILE
CORRECTLY
AND
TO
USE
FREAD
FOR
I
O
PLEASE
USE
A
STRUCTURE
TO
REPRESENT
A
JPEG
TIFF
EXIF
HEADER
AND
ANOTHER
STRUCT
TO
REPRESENT
A
TIFF
TAG
DO
NOT
USE
A
BUNCH
OF
DISJOINT
VARIABLES
DO
YOUR
FREAD
WITH
THE
WHOLE
STRUCTURE
AT
ONCE
THAT
IS
READ
AN
ENTIRE
TAG
IN
ONE
FILE
OPERATION
IF
THE
HEADER
FIELD
DOES
NOT
CONTAIN
THE
EXIF
STRING
IN
THE
RIGHT
PLACE
PRINT
AN
ERROR
MESSAGE
THAT
THE
TAG
WAS
NOT
FOUND
IF
THE
TIFF
HEADER
CONTAINS
MM
INSTEAD
OF
II
PRINT
AN
ERROR
MESSAGE
THAT
WE
DO
NOT
SUPPORT
THE
ENDIANNESS
ENVIRONMENT
ENSURE
THAT
YOUR
PROGRAM
BUILDS
AND
RUNS
ON
THOTH
CS
PITT
EDU
AS
THAT
WILL
BE
WHERE
WE
ARE
TESTING
WE
HAVE
PROVIDED
TWO
SAMPLE
JPG
FILES
WITH
VALID
EXIF
TAGS
ACCORDING
TO
OUR
LIMITATIONS
COPY
THEM
TO
YOUR
WORKING
DIRECTORY
ON
THOTH
BY
USING
THE
COMMAND
THE
DOT
AT
THE
END
IS
IMPORTANT
AS
IT
REPRESENTS
THE
CURRENT
DIRECTORY
IN
LINUX
SUBMISSION
WHEN
YOU
RE
DONE
CREATE
A
GZIPPED
TARBALL
AS
WE
DID
IN
THE
FIRST
LAB
OF
YOUR
COMMENTED
SOURCE
FILES
AND
COMPILED
EXECUTABLES
NAME
THE
GZIPPED
TARBALL
TAR
GZ
SIMILARLY
AS
WE
DID
FOR
COPY
YOUR
ARCHIVE
TO
THE
DIRECTORY
WAHN
SUBMIT
MAKE
SURE
YOU
NAME
THE
FILE
WITH
YOUR
USERNAME
AND
THAT
YOU
HAVE
YOUR
NAME
IN
THE
COMMENTS
OF
YOUR
SOURCE
FILE
PLEASE
DO
NOT
SUBMIT
THE
SAMPLE
PICTURE
FILES
WE
PROVIDE
NOTE
THAT
THIS
DIRECTORY
IS
INSERT
ONLY
YOU
MAY
NOT
DELETE
OR
MODIFY
YOUR
SUBMISSIONS
ONCE
IN
THE
DIRECTORY
IF
YOU
VE
MADE
A
MISTAKE
BEFORE
THE
DEADLINE
RESUBMIT
WITH
A
NUMBER
SUFFIX
LIKE
TAR
GZ
PROJECT
WHAT
THE
PASSWORD
DUE
TUESDAY
NOVEMBER
AT
DESCRIPTION
THROUGHOUT
MOST
OF
YOUR
CS
OR
COE
STUDIES
YOU
WORK
CREATING
OR
MODIFYING
PROGRAMS
OR
COMPUTERS
IN
A
WORD
BUILDING
HOWEVER
SOMETIMES
THE
BEST
WAY
TO
LEARN
ABOUT
SOMETHING
IS
TO
BREAK
IT
IN
THIS
PROJECT
YOU
WILL
BE
DECONSTRUCTING
EXISTING
PROGRAMS
THAT
EACH
HAVE
A
SECRET
PASSWORD
OR
PASSPHRASE
THAT
NEEDS
TO
BE
INPUT
IN
ORDER
TO
UNLOCK
THE
PROGRAM
I
AM
PROVIDING
YOU
WITH
COMPILED
EXECUTABLES
EACH
ONE
REQUIRES
YOU
TO
ENTER
A
SEQUENCE
OF
ASCII
CHARACTERS
TO
UNLOCK
UNLOCKING
THE
PROGRAMS
WILL
DRAW
UPON
THE
THINGS
WE
ARE
STUDYING
THIS
TERM
YOU
WILL
ALSO
WRITE
A
TOOL
TO
HELP
YOU
WITH
SOLVING
THE
FIRST
PROGRAM
IN
UNIX
LINUX
THERE
IS
A
PROGRAM
CALLED
STRINGS
THAT
DUMPS
OUT
THE
SEQUENCES
OF
ASCII
CHARACTERS
THAT
ARE
OR
MORE
CHARACTERS
LONG
PART
MYSTRINGS
POINTS
THE
MYSTRINGS
PROGRAM
SHOULD
TAKE
A
FILENAME
FROM
THE
COMMAND
LINE
AND
READ
THE
BYTES
OF
THE
FILE
LOOKING
FOR
STRINGS
OF
PRINTABLE
CHARACTERS
PRINTABLE
CHARACTERS
ARE
ASCII
VALUES
BETWEEN
AND
DECIMAL
AND
THE
ASCII
VALUE
THE
TAB
CHARACTER
A
STRING
IS
A
RUN
OF
AT
LEAST
CONSECUTIVE
PRINTABLE
CHARACTERS
AND
ENDS
WHENEVER
A
NON
PRINTABLE
CHARACTER
IS
ENCOUNTERED
WHENEVER
YOU
FIND
SUCH
A
STRING
PRINT
IT
OUT
ONE
PER
LINE
YOU
CAN
CHECK
THE
OPERATION
OF
YOUR
PROGRAM
VIA
THE
REAL
STRINGS
PROGRAM
AND
DO
A
MAN
STRINGS
TO
LEARN
ABOUT
HOW
IT
WORKS
THE
OUTPUT
FROM
YOUR
MYSTRINGS
PROGRAM
SHOULD
MATCH
EXACTLY
THE
OUTPUT
FROM
STRINGS
A
FILENAME
TRY
COMPARING
THE
OUTPUT
FROM
OBJECT
FILES
O
IMAGE
FILES
JPG
AND
TEXT
FILES
C
THE
OUTPUT
SHOULD
BE
THE
SAME
REGARDLESS
OF
FILE
TYPE
MAKE
SURE
YOUR
PROGRAM
CAN
HANDLE
STRINGS
THAT
ARE
ARBITRARILY
LONG
PART
PASSWORDS
POINTS
FOR
EACH
OF
THE
THREE
PROGRAMS
YOU
WILL
BE
REQUIRED
TO
PROVIDE
TWO
THINGS
THE
SOLUTION
PASSPHRASE
AND
A
WRITTEN
DESCRIPTION
OF
YOUR
ATTEMPTS
TO
DISCOVER
IT
STATING
WHAT
YOU
LEARNED
TO
HELP
YOU
ALONG
THE
WAY
YOU
SHOULD
RELATE
YOUR
EXPERIENCES
BACK
TO
THE
COURSE
MATERIAL
USING
THE
TERMS
AND
CONCEPTS
WE
VE
DISCUSSED
WRITE
IT
UP
IN
A
FORMAL
ORGANIZED
FASHION
YOU
DO
NOT
NEED
TO
DESCRIBE
EVERY
COMMAND
YOU
HAVE
TRIED
OR
EVERY
WRONG
IDEA
DESCRIBE
BRIEFLY
YOUR
FAILED
ATTEMPTS
AND
MOTIVATION
BUT
DESCRIBE
IN
DETAIL
YOUR
SUCCESSFUL
APPROACH
TOOLS
THE
MOST
OBVIOUS
TOOL
YOU
WILL
NEED
IS
A
GOOD
DEBUGGER
LIKE
GDB
YOU
MAY
ALSO
FIND
A
HEX
VIEWER
LIKE
OD
X
USEFUL
OBJDUMP
CAN
DO
A
LOT
OF
INDIVIDUAL
TASKS
THAT
CAN
BE
HELPFUL
ADDITIONALLY
YOU
MIGHT
FIND
THE
MYSTRINGS
COMMAND
YOU
WROTE
SOMEWHAT
USEFUL
ENVIRONMENT
FOR
THIS
PROJECT
WE
WILL
BE
WORKING
ON
THOTH
CS
PITT
EDU
WHEN
YOU
LOGIN
VIA
SSH
WITH
YOUR
PITT
ACCOUNT
YOU
WILL
FIND
A
LOCAL
DIRECTORY
UNDER
U
SYSLAB
NAMED
WITH
YOUR
USERNAME
IN
THIS
DIRECTORY
YOU
WILL
FIND
THE
THREE
EXECUTABLES
AND
SPACE
TO
WORK
ON
THEM
IF
YOU
STORE
ANY
FILES
OF
YOUR
OWN
IN
THIS
DIRECTORY
NOTE
THAT
IT
IS
NOT
PART
OF
AFS
AND
ONLY
EXISTS
ON
THIS
MACHINE
WE
WILL
DELETE
YOUR
DIRECTORY
WHEN
THE
TERM
IS
OVER
ANYTHING
YOU
WANT
TO
SAVE
OR
BACKUP
SHOULD
BE
COPIED
INTO
YOUR
AFS
PRIVATE
DIRECTORY
HINTS
NOTES
EACH
PROGRAM
IS
WRITTEN
IN
C
EACH
PROGRAM
WILL
HAVE
A
DIFFERENT
PASSPHRASE
PER
STUDENT
ALTHOUGH
HOW
TO
FIND
IT
WILL
BE
CONSISTENT
FOR
EVERYONE
ALL
PASSPHRASES
WILL
BE
PRINTABLE
ASCII
CHARACTERS
AND
BE
LESS
THAN
CHARACTERS
IN
LENGTH
A
PASSPHRASE
MAY
BE
DIFFERENT
EACH
RUN
OF
A
PROGRAM
MAKE
SURE
TO
TEST
IT
SEVERAL
TIMES
THERE
MAY
BE
SEVERAL
PASSPHRASES
THAT
WORK
TRY
TO
DESCRIBE
THEM
OR
EXPLAIN
WHY
THIS
IS
NOT
AN
ATTEMPT
TO
PROVE
HOW
CLEVER
I
AM
EACH
PROGRAM
WILL
BE
SOLVABLE
FROM
COURSE
MATERIAL
AND
THE
STANDARD
TOOLS
ON
THE
SYSTEM
WHAT
TO
TURN
IN
YOUR
MYSTRINGS
PROGRAM
AND
SOURCE
CODE
A
WRITTEN
DESCRIPTION
FOR
EACH
PROGRAM
DOCUMENTING
YOUR
ATTEMPTS
TO
ARRIVE
AT
THE
SOLUTION
AND
THE
PASSPHRASE
ITSELF
SUBMITTED
AS
A
WORD
DOC
OR
PDF
DOCUMENT
ALL
IN
A
TAR
GZ
FILE
NAMED
WITH
YOUR
USER
ID
COPY
YOUR
ARCHIVE
TO
THE
APPROPRIATE
DIRECTORY
WAHN
SUBMIT
PROJECT
A
CUSTOM
MALLOC
DUE
TUESDAY
NOVEMBER
AT
DESCRIPTION
IN
OUR
DISCUSSIONS
OF
DYNAMIC
MEMORY
MANAGEMENT
WE
DISCUSSED
THE
OPERATION
OF
THE
STANDARD
C
LIBRARY
CALL
MALLOC
SOME
IMPLEMENTATIONS
OF
MALLOC
DESIGNATE
A
REGION
OF
A
PROCESS
ADDRESS
SPACE
FROM
THE
SYMBOL
WHERE
THE
CODE
AND
GLOBAL
DATA
ENDS
TO
BRK
AS
THE
HEAP
WE
WILL
BE
TAKING
A
SLIGHTLY
DIFFERENT
APPROACH
AND
ASKING
THE
OS
FOR
A
LARGE
REGION
OF
MEMORY
TO
ACT
AS
OUR
HEAP
AS
PART
OF
DYNAMIC
MEMORY
MANAGEMENT
WE
ALSO
DISCUSSED
VARIOUS
ALGORITHMS
FOR
THE
MANAGEMENT
OF
THE
EMPTY
SPACES
THAT
MAY
BE
CREATED
AFTER
A
MALLOC
MANAGED
HEAP
HAS
HAD
SOME
OF
IT
ALLOCATIONS
FREED
IN
THIS
ASSIGNMENT
YOU
ARE
ASKED
TO
CREATE
YOUR
OWN
VERSION
OF
MALLOC
BASED
UPON
THE
BUDDY
ALLOCATOR
SCHEME
DETAILS
THE
BUDDY
ALLOCATION
ALGORITHM
REQUIRES
A
LARGE
FREE
SPACE
THAT
WE
CAN
REPEATEDLY
DIVIDED
TO
FIND
A
REASONABLY
SIZED
CHUNK
TO
RETURN
WE
WILL
REQUEST
THIS
INITIAL
FREE
SPACE
DIRECTLY
FROM
THE
OS
BY
REQUESTING
THE
OS
ALLOCATE
US
MANY
CONTIGUOUS
PAGES
USING
THE
MMAP
SYSTEM
CALL
WE
WILL
CREATE
AN
INITIAL
SPACE
OF
BY
DOING
THE
FOLLOWING
VOID
BASE
MMAP
NULL
WE
CAN
EASILY
SET
TO
BY
USING
A
LEFT
SHIFT
TRICK
THE
POINTER
BASE
WILL
NOW
POINT
TO
A
REGION
OF
CONTIGUOUSLY
ALLOCATED
IN
SIZE
THAT
WE
CAN
USE
FOR
OUR
BUDDY
ALLOCATOR
AS
WE
DISCUSSED
IN
CLASS
WE
WILL
USE
A
DOUBLE
LINKED
LIST
REPRESENTATION
FOR
THE
FREE
LISTS
EACH
CHUNK
IN
THE
FREE
LIST
CONTAINS
CHUNK
HEADER
INFORMATION
THE
PREV
POINTER
AND
THE
NEXT
POINTER
THE
CHUNK
HEADER
IS
ONE
BYTE
LONG
WHERE
THE
FIRST
BIT
IS
THE
OCCUPANCY
BIT
AND
THE
REMAINING
BITS
REPRESENT
THE
SIZE
OF
THE
CHUNK
THE
SIZE
IS
REPRESENTED
BY
WHERE
N
IS
THE
SIZE
IN
BYTES
HENCE
THE
BITS
CAN
REPRESENT
A
SIZE
OF
UP
TO
BYTES
THE
OCCUPANCY
BIT
AND
THE
SIZE
ARE
REQUIRED
FOR
THE
COALESCING
ALGORITHM
THE
MINIMAL
SIZE
OF
A
CHUNK
IS
BYTES
THE
BLOCK
HEADER
AND
THE
POINTERS
TAKE
UP
BYTES
THE
BLOCK
HEADER
IS
ALWAYS
PRESENT
IN
A
CHUNK
REGARDLESS
OF
WHETHER
THAT
CHUNK
IS
ALLOCATED
OR
FREE
HOWEVER
THE
PREV
AND
NEXT
POINTERS
ARE
ONLY
NEEDED
WHEN
A
CHUNK
IS
FREE
BECAUSE
THEY
ARE
ONLY
USED
TO
LINK
CHUNKS
IN
A
FREE
LIST
HENCE
THE
SIZE
OF
USABLE
SPACE
IN
AN
ALLOCATED
CHUNK
IS
N
BYTES
ONE
BYTE
IS
TAKEN
UP
BY
THE
BLOCK
HEADER
OBVIOUSLY
YOUR
MALLOC
SHOULD
RETURN
A
POINTER
TO
THIS
USABLE
SPACE
NOT
A
POINTER
TO
THE
CHUNK
HEADER
NOW
WE
SHOULD
BUILD
A
TABLE
OF
POINTERS
TO
BE
THE
HEADS
OF
THE
DOUBLE
LINKED
LISTS
OF
OUR
CHUNKS
OF
EACH
POWER
OF
TWO
SIZE
WE
SUPPORT
TO
THE
FIRST
POINTER
WOULD
BE
THE
HEAD
FOR
THE
BYTE
CHUNK
LIST
THE
LAST
POINTER
WOULD
BE
THE
HEAD
FOR
THE
GIB
CHUNK
LIST
WE
HAVE
DISCUSSED
IN
CLASS
HOW
TO
DISCOVER
A
BUDDY
FOR
A
GIVEN
CHUNK
USING
THE
XOR
OPERATION
IN
C
IT
IS
IMPORTANT
TO
NOTE
THAT
THIS
ONLY
WORKS
WHEN
THE
BASE
OF
THE
CHUNK
IS
AT
ADDRESS
WHICH
IS
NEVER
GOING
TO
BE
THE
CASE
FOR
US
AS
MMAP
WILL
RETURN
AN
ADDRESS
FROM
THE
MEMORY
MAPPED
AREA
AT
HIGH
ADDRESSES
IN
OUR
ADDRESS
SPACE
THAT
MEANS
TO
USE
THE
XOR
TRICK
WE
WILL
ALWAYS
NEED
TO
SUBTRACT
BASE
FROM
THE
ADDRESS
TO
GET
THE
OFFSET
INTO
OUR
REGION
REQUIREMENTS
YOU
ARE
TO
CREATE
THREE
FUNCTIONS
FOR
THIS
PROJECT
A
MALLOC
REPLACEMENT
CALLED
VOID
INT
SIZE
THAT
ALLOCATES
MEMORY
USING
THE
BUDDY
ALLOCATION
SCHEME
A
FREE
REPLACEMENT
CALLED
VOID
VOID
PTR
THAT
DEALLOCATES
A
POINTER
THAT
WAS
ORIGINALLY
ALLOCATED
BY
THE
MALLOC
YOU
WROTE
ABOVE
A
FUNCTION
THAT
DUMPS
ALL
THE
FREE
BLOCKS
IN
ALL
THE
FREE
LISTS
IN
YOUR
HEAP
IN
A
WAY
VERY
SIMILAR
TO
THE
GIVEN
IN
THE
MEMORY
MANAGEMENT
LAB
IN
WAHN
PUBLIC
HEAP
HEAP
C
THE
TA
WILL
INSERT
THIS
FUNCTION
AT
VARIOUS
POINTS
IN
YOUR
CODE
TO
CHECK
THE
INTEGRITY
OF
YOUR
HEAP
FOR
GRADING
PURPOSES
MAKE
SURE
YOU
IMPLEMENT
THIS
FUNCTION
FIRST
OR
YOU
WILL
NOT
GET
ANY
POINTS
FOR
IMPLEMENTING
EITHER
MALLOC
OR
FREE
YOUR
FREE
FUNCTION
SHOULD
COALESCE
BUDDY
FREE
BLOCKS
AS
WE
DESCRIBED
IN
CLASS
AS
YOU
ARE
DEVELOPING
YOU
WILL
WANT
TO
CREATE
A
DRIVER
PROGRAM
THAT
TESTS
YOUR
CALLS
TO
YOUR
MALLOCS
AND
FREES
FOR
GRADING
WE
WILL
USE
THE
DRIVER
U
SYSLAB
SHARED
MALLOCDRV
C
IN
ORDER
TO
TEST
THAT
YOUR
CODE
WORKS
MALLOCDRV
C
IS
DESIGNED
TO
USE
THE
STANDARD
C
LIBRARY
MALLOC
AS
IS
MAKE
SURE
YOU
MODIFY
THE
MALLOC
AND
FREE
MACROS
DEFINED
AT
THE
TOP
TO
YOUR
OWN
FUNCTIONS
TO
HAVE
IT
WORK
WITH
YOUR
CODE
ALSO
YOU
WILL
NEED
TO
INCLUDE
YOUR
MYMALLOC
H
HEADER
FILE
APPROPRIATELY
THE
OUTPUT
FROM
USING
YOUR
OWN
BUDDY
MALLOC
AND
FROM
USING
THE
C
LIBRARY
MALLOC
SHOULD
BE
IDENTICAL
THE
OUTPUT
FOR
THE
FUNCTION
IMMEDIATELY
AFTER
INITIALIZING
THE
HEAP
SHOULD
LOOK
AS
FOLLOWS
THE
HEAP
SHOWS
THE
FREE
LISTS
FOR
THE
BINS
THAT
ARE
ALL
EMPTY
EXCEPT
FOR
THE
LARGEST
BIN
OF
SIZE
THERE
IS
ONE
BLOCK
IN
THIS
BIN
THAT
HAS
THE
OCCUPANCY
BIT
SET
TO
AN
OFFSET
OF
FROM
THE
BASE
AND
A
SIZE
OF
OR
THIS
BLOCK
ENCOMPASSES
THE
ENTIRE
HEAP
INITIALLY
ENVIRONMENT
FOR
THIS
PROJECT
WE
WILL
AGAIN
BE
WORKING
ON
THOTH
CS
PITT
EDU
HINTS
NOTES
IN
C
THE
SENTINEL
VALUE
FOR
THE
END
OF
A
LINKED
LIST
IS
HAVING
THE
NEXT
POINTER
SET
TO
NULL
GDB
IS
YOUR
FRIEND
NO
MATTER
WHAT
YOU
THINK
AFTER
PROJECT
TRY
INSERTING
THE
FUNCTION
YOURSELF
AT
VARIOUS
POINTS
IN
THE
TEST
PROGRAM
IT
WILL
HELP
YOU
DETECT
IMMEDIATELY
WHEN
YOUR
HEAP
IS
CORRUPTED
AND
WHERE
WHAT
TO
TURN
IN
A
HEADER
FILE
NAMED
MYMALLOC
H
WITH
THE
PROTOTYPES
OF
YOUR
THREE
FUNCTIONS
A
C
FILE
NAMED
MYMALLOC
C
WITH
THE
IMPLEMENTATIONS
OF
YOUR
THREE
FUNCTIONS
THE
TEST
PROGRAM
YOU
USED
DURING
YOUR
INITIAL
TESTING
ANY
DOCUMENTATION
YOU
PROVIDE
TO
HELP
US
GRADE
YOUR
PROJECT
TO
CREATE
A
TAR
GZ
FILE
IF
YOUR
CODE
IS
IN
A
FOLDER
NAMED
EXECUTE
THE
FOLLOWING
COMMANDS
WHERE
USERNAME
IS
YOUR
USERNAME
THEN
COPY
YOUR
FILE
TO
WAHN
SUBMIT
CS
PROJECT
DEV
DICE
DUE
THURSDAY
DECEMBER
AT
PROJECT
DESCRIPTION
STANDARD
UNIX
AND
LINUX
SYSTEMS
COME
WITH
A
FEW
SPECIAL
FILES
LIKE
DEV
ZERO
WHICH
RETURNS
NOTHING
BUT
ZEROS
WHEN
IT
IS
READ
AND
DEV
RANDOM
WHICH
RETURNS
RANDOM
BYTES
IN
THIS
PROJECT
YOU
WILL
WRITE
A
DEVICE
DRIVER
TO
CREATE
A
NEW
DEVICE
DEV
DICE
WHICH
RETURNS
A
RANDOMLY
SELECTED
ROLL
OF
A
SIDED
DIE
HOW
IT
WILL
WORK
FOR
THIS
PROJECT
WE
WILL
NEED
TO
CREATE
THREE
PARTS
THE
DEVICE
DRIVER
THE
SPECIAL
FILE
DEV
DICE
AND
A
TEST
PROGRAM
TO
CONVINCE
OURSELVES
IT
WORKS
THE
TEST
PROGRAM
WILL
BE
A
SOLITAIRE
IMPLEMENTATION
OF
THE
GAME
OF
YAHTZEE
DRIVER
IMPLEMENTATION
OUR
DEVICE
DRIVER
WILL
BE
A
CHARACTER
DEVICE
THAT
WILL
IMPLEMENT
A
READ
FUNCTION
WHICH
IS
THE
IMPLEMENTATION
OF
THE
READ
SYSCALL
AND
RETURNS
AN
APPROPRIATE
DIE
ROLL
A
BYTE
VALUE
FROM
TO
AS
WE
DISCUSSED
IN
CLASS
THE
KERNEL
DOES
NOT
HAVE
THE
FULL
C
STANDARD
LIBRARY
AVAILABLE
TO
IT
AND
SO
WE
NEED
TO
GET
USE
A
DIFFERENT
FUNCTION
TO
GET
RANDOM
NUMBERS
BY
INCLUDING
LINUX
RANDOM
H
WE
CAN
USE
THE
FUNCTION
WHICH
YOU
CAN
TURN
INTO
A
HELPER
FUNCTION
TO
GET
A
SINGLE
BYTE
YAHTZEE
IMPLEMENTATION
YAHTZEE
ALSO
KNOWN
AS
POKER
DICE
IS
A
MULTIPLAYER
DICE
GAME
WITH
FAIRLY
SIMPLE
RULES
FOR
THIS
ASSIGNMENT
YOU
WILL
BE
IMPLEMENTING
A
SOLITAIRE
VERSION
WHERE
A
USER
PLAYS
BY
HERSELF
I
HIGHLY
SUGGEST
PLAYING
THE
GAME
OR
AT
LEAST
READING
THE
RULES
HTTP
EN
WIKIPEDIA
ORG
WIKI
YAHTZEE
DICE
ARE
ROLLED
THE
PLAYER
CAN
CHOOSE
TO
KEEP
ANY
OF
THE
DICE
AND
IS
ALLOWED
TO
REROLL
ANY
THEY
DO
NOT
WANT
TO
KEEP
THEY
MAY
REROLL
UP
TO
TWICE
BUT
AT
THE
END
OF
THE
THIRD
ROLL
THE
FIVE
DICE
ARE
FINALIZED
AFTER
THE
USER
HAS
FINALIZED
THEIR
FIVE
DICE
THEY
MAY
PLACE
THEM
INTO
ONE
OF
N
CATEGORIES
EVENT
SCORE
UPPER
SECTION
ONES
TWOS
THREES
FOURS
FIVES
SIXES
THE
SUM
OF
THE
DICE
WITH
THE
APPROPRIATE
VALUE
LOWER
SECTION
THREE
OF
A
KIND
FOUR
OF
A
KIND
THE
TOTAL
OF
ALL
DICE
FULL
HOUSE
SMALL
STRAIGHT
LARGE
STRAIGHT
YAHTZEE
CHANCE
THE
SUM
OF
THE
DICE
WE
WILL
NOT
DO
ANYTHING
SPECIAL
FOR
A
SECOND
YAHTZEE
NOTE
THAT
THE
PLAYER
CAN
SCORE
POINTS
IN
A
CATEGORY
IF
THE
TOTAL
POINTS
OF
THE
ONES
TWOS
THREES
FOURS
FIVES
AND
SIXES
IS
OR
MORE
THE
USER
GETS
A
BONUS
OF
POINTS
YOUR
ROLL
WHICH
DICE
TO
REROLL
YOUR
SECOND
ROLL
WHICH
DICE
TO
REROLL
PLACE
DICE
INTO
UPPER
SECTION
LOWER
SECTION
SELECTION
PLACE
DICE
INTO
ONES
TWOS
THREES
FOURS
FIVES
SIXES
SELECTION
YOUR
SCORE
SO
FAR
IS
ONES
FOURS
TWOS
FIVES
THREES
SIXES
THE
GAME
ENDS
WHEN
ALL
CATEGORIES
HAVE
BEEN
ASSIGNED
A
POINT
TURNS
FOR
THIS
ASSIGNMENT
YOU
NEED
TO
DO
THE
FOLLOWING
WRITE
A
PROGRAM
THAT
ALLOWS
A
PLAYER
TO
PLAY
YAHTZEE
GETS
EACH
DIE
TO
DISPLAY
BY
READING
ONE
BYTE
FROM
THE
DEV
DICE
FILE
HINTS
AND
SUGGESTIONS
USE
THE
QSORT
FUNCTION
TO
SORT
THE
DICE
FOR
SCORING
IT
MAKES
FINDING
MULTIPLES
AND
STRAIGHTS
MUCH
EASIER
YOU
MAY
WRITE
THE
YAHTZEE
PROGRAM
USING
STDLIB
RAND
FOR
TESTING
BUT
MAKE
SURE
YOU
EVENTUALLY
REPLACE
IT
WITH
YOUR
READ
OF
DEV
DICE
INSTALLATION
AND
EXAMPLE
ON
THOTH
CS
PITT
EDU
LOGIN
AND
CD
TO
YOUR
U
SYSLAB
USERNAME
DIRECTORY
TAR
XVFZ
SHARED
TAR
GZ
CD
OPEN
THE
MAKEFILE
WITH
THE
EDITOR
OF
YOUR
CHOICE
E
G
PICO
MAKEFILE
WE
NEED
TO
SETUP
THE
PATH
TO
COMPILE
AGAINST
THE
PROPER
VERSION
OF
THE
KERNEL
TO
DO
THIS
CHANGE
THE
LINE
KDIR
LIB
MODULES
SHELL
UNAME
R
BUILD
TO
KDIR
U
SYSLAB
SHARED
LINUX
BUILD
THE
KERNEL
OBJECT
THE
ARCH
IS
IMPORTANT
BECAUSE
WE
ARE
BUILDING
A
BIT
KERNEL
ON
A
BIT
MACHINE
DOWNLOAD
AND
LAUNCH
QEMU
FOR
WINDOWS
USERS
YOU
CAN
JUST
DOUBLE
CLICK
THE
QEMU
WIN
BAT
THAT
IS
SUPPLIED
IN
THE
ZIPFILE
AVAILABLE
ON
MY
WEBSITE
MAC
USERS
SHOULD
DOWNLOAD
AND
INSTALL
VIRTUALBOX
AND
USE
THE
TTY
DISK
IMAGE
PROVIDED
IN
THE
MAIN
ZIPFILE
LINUX
USERS
ARE
LEFT
TO
INSTALL
QEMU
AS
APPROPRIATE
AND
ALSO
USE
THE
TTY
DISK
IMAGE
PROVIDED
IN
THE
MAIN
ZIPFILE
WHEN
LINUX
BOOTS
UNDER
QEMU
LOGIN
USING
THE
ROOT
ROOT
ACCOUNT
USERNAME
PASSWORD
WE
NOW
NEED
TO
DOWNLOAD
THE
KERNEL
MODULE
YOU
JUST
BUILT
INTO
THE
KERNEL
USING
SCP
LOAD
THE
DRIVER
USING
INSMOD
WE
NOW
NEED
TO
MAKE
THE
DEVICE
FILE
IN
DEV
FIRST
WE
NEED
TO
FIND
THE
MAJOR
AND
MINOR
NUMBERS
THAT
IDENTIFY
THE
NEW
DEVICE
THE
OUTPUT
SHOULD
BE
A
NUMBER
LIKE
THE
IS
THE
MAJOR
AND
THE
IS
THE
MINOR
WE
USE
MKNOD
TO
MAKE
A
SPECIAL
FILE
THE
NAME
WILL
BE
HELLO
AND
IT
IS
A
CHARACTER
DEVICE
THE
AND
THE
CORRESPOND
TO
THE
MAJOR
AND
MINOR
NUMBERS
WE
DISCOVERED
ABOVE
IF
DIFFERENT
USE
THE
ONES
YOU
SAW
WE
CAN
NOW
READ
THE
DATA
OUT
OF
DEV
HELLO
WITH
A
SIMPLE
CALL
TO
CAT
YOU
SHOULD
SEE
HELLO
WORLD
WHICH
CAME
FROM
THE
DRIVER
WE
CAN
CLEAN
UP
BY
REMOVING
THE
DEVICE
AND
UNLOADING
THE
MODULE
WHAT
TO
DO
NEXT
THE
CODE
FOR
THE
EXAMPLE
WE
WENT
THROUGH
COMES
FROM
HTTP
WWW
LINUXDEVCENTER
COM
PUB
A
LINUX
DEVHELLOWORLD
A
SIMPLE
INTRODUCTION
TO
DEVICE
DRIVERS
UNDER
LINUX
HTML
PAGE
READ
THAT
WHILE
GOING
THROUGH
THE
SOURCE
TO
GET
AN
IDEA
OF
WHAT
THE
MODULE
IS
DOING
START
WITH
THE
THIRD
SECTION
ENTITLED
HELLO
WORLD
USING
DEV
AND
READ
UP
UNTIL
THE
AUTHOR
STARTS
DESCRIBING
UDEV
RULES
WE
ARE
NOT
USING
UDEV
UNDER
QEMU
WHEN
YOU
HAVE
AN
IDEA
OF
WHAT
IS
GOING
ON
MAKE
A
NEW
DIRECTORY
UNDER
YOUR
U
SYSLAB
USERNAME
DIRECTORY
CALLED
COPY
AND
RENAME
THE
C
FROM
THE
EXAMPLE
AND
COPY
OVER
THE
MAKEFILE
EDIT
THE
MAKEFILE
TO
BUILD
YOUR
NEW
FILE
CHANGE
ALL
THE
REFERENCES
OF
HELLO
TO
BUILDING
THE
DRIVER
TO
BUILD
ANY
CHANGES
YOU
HAVE
MADE
ON
THOTH
IN
YOUR
DIRECTORY
SIMPLY
IF
YOU
WANT
TO
FORCE
A
REBUILD
OF
EVERYTHING
YOU
MAY
WANT
TO
REMOVE
THE
OBJECT
FILES
FIRST
COPYING
THE
FILES
TO
QEMU
FROM
QEMU
YOU
WILL
NEED
TO
DOWNLOAD
THE
DRIVER
THAT
YOU
JUST
BUILT
USE
SCP
TO
DOWNLOAD
THE
DRIVER
TO
A
HOME
DIRECTORY
ROOT
IF
ROOT
LOADING
THE
DRIVER
INTO
THE
KERNEL
IN
QEMU
AS
ROOT
EITHER
BY
LOGGING
IN
OR
VIA
SU
MAKING
THE
DEV
DICE
DEVICE
LIKE
IN
THE
EXAMPLE
WE
LL
NEED
TO
DETERMINE
THE
MAJOR
AND
MINOR
NUMBERS
FOR
THIS
PARTICULAR
DRIVER
AND
USE
THOSE
NUMBERS
TO
MAKE
THE
DEV
DICE
FILE
UNLOADING
THE
DRIVER
FROM
THE
KERNEL
IN
QEMU
AS
ROOT
EITHER
BY
LOGGING
IN
OR
VIA
SU
THEN
YOU
CAN
REMOVE
THE
DEV
DICE
FILE
IMPLEMENTING
AND
BUILDING
THE
YAHTZEE
PROGRAM
SINCE
THOTH
IS
A
BIT
MACHINE
AND
QEMU
EMULATES
A
BIT
MACHINE
WE
SHOULD
BUILD
WITH
THE
FLAG
RUNNING
YAHTZEE
WE
CANNOT
RUN
OUR
YAHTZEE
PROGRAM
ON
THOTH
CS
PITT
EDU
BECAUSE
ITS
KERNEL
DOES
NOT
HAVE
THE
DEVICE
DRIVER
LOADED
HOWEVER
WE
CAN
TEST
THE
PROGRAM
UNDER
QEMU
ONCE
WE
HAVE
INSTALLED
THE
NEW
DRIVER
WE
FIRST
NEED
TO
DOWNLOAD
YAHTZEE
USING
SCP
AS
WE
DID
FOR
THE
DRIVER
HOWEVER
WE
CAN
JUST
RUN
IT
FROM
OUR
HOME
DIRECTORY
WITHOUT
ANY
INSTALLATION
NECESSARY
FILE
BACKUPS
ONE
OF
THE
MAJOR
CONTRIBUTIONS
THE
UNIVERSITY
PROVIDES
FOR
THE
AFS
FILESYSTEM
IS
NIGHTLY
BACKUPS
HOWEVER
THE
U
SYSLAB
PARTITION
IS
NOT
PART
OF
AFS
SPACE
THUS
ANY
FILES
YOU
MODIFY
UNDER
YOUR
PERSONAL
DIRECTORY
IN
U
SYSLAB
ARE
NOT
BACKED
UP
IF
THERE
IS
A
CATASTROPHIC
DISK
FAILURE
ALL
OF
YOUR
WORK
WILL
BE
IRRECOVERABLY
LOST
AS
SUCH
IT
IS
MY
RECOMMENDATION
THAT
YOU
BACKUP
ALL
THE
FILES
YOU
CHANGE
UNDER
U
SYSLAB
TO
YOUR
PRIVATE
DIRECTORY
FREQUENTLY
LOSS
OF
WORK
NOT
BACKED
UP
IS
NOT
GROUNDS
FOR
AN
EXTENSION
YOU
HAVE
BEEN
WARNED
HINTS
AND
NOTES
PRINTK
IS
THE
VERSION
OF
PRINTF
YOU
CAN
USE
FOR
DEBUGGING
MESSAGES
FROM
THE
KERNEL
IN
THE
DRIVER
YOU
CAN
USE
SOME
STANDARD
C
FUNCTIONS
BUT
NOT
ALL
THEY
MUST
BE
PART
OF
THE
KERNEL
TO
WORK
IN
THE
YAHTZEE
PROGRAM
YOU
MAY
USE
ANY
OF
THE
C
STANDARD
LIBRARY
FUNCTIONS
AS
ROOT
TYPING
POWEROFF
IN
QEMU
WILL
SHUT
IT
DOWN
CLEANLY
IF
THE
MODULE
CRASHES
IT
MAY
BECOME
IMPOSSIBLE
TO
DELETE
THE
FILE
YOU
CREATED
WITH
MKDEV
IN
DEV
IF
THAT
HAPPENS
JUST
GRAB
A
NEW
DISK
IMAGE
AND
START
OVER
IT
WHY
WE
RE
DEVELOPING
IN
A
VIRTUAL
MACHINE
AS
OPPOSED
TO
A
REAL
ONE
REQUIREMENTS
AND
SUBMISSION
YOU
NEED
TO
SUBMIT
YOUR
C
FILE
AND
THE
MAKEFILE
YOUR
WELL
COMMENTED
YAHTZEE
PROGRAM
SOURCE
MAKE
A
TAR
GZ
FILE
NAMED
USERNAME
TAR
GZ
COPY
IT
TO
WAHN
SUBMIT
BY
THE
DEADLINE
FOR
CREDIT
CS
PROJECT
MULTI
THREADED
WEB
SERVER
DUE
TUESDAY
DECEMBER
AT
DESCRIPTION
LAUNCHING
A
BROWSER
AND
VISITING
A
SITE
ON
THE
INTERNET
INVOLVES
AT
LEAST
TWO
PARTIES
THE
WEB
SERVER
AND
THE
REQUESTOR
YOU
IN
THIS
ASSIGNMENT
WE
WILL
USE
PTHREADS
AND
BERKLEY
SOCKETS
TO
IMPLEMENT
A
PRIMITIVE
WEB
SERVER
WHICH
TALKS
HTTP
TO
BROWSERS
OVER
TCP
IP
HTTP
HYPERTEXT
TRANSFER
PROTOCOL
IS
A
SIMPLE
TEXT
BASED
COMMUNICATION
PROTOCOL
BY
WHICH
A
WEB
BROWSER
REQUESTS
DOCUMENTS
FROM
A
WEB
SERVER
AND
THE
WEB
SERVER
REPLIES
FOR
INSTANCE
IF
YOU
VISIT
A
WEB
SITE
SUCH
AS
HTTP
WWW
EXAMPLE
COM
TEST
HTML
YOUR
BROWSER
DOES
THE
FOLLOWING
CONNECT
TO
THE
IP
ADDRESS
OF
HTTP
WWW
EXAMPLE
COM
TEST
HTML
OBTAINED
VIA
DNS
LOOKUP
AT
PORT
STANDARD
WEB
SERVER
PORT
SEND
THE
HTTP
REQUEST
MESSAGE
GET
TEST
HTML
HTTP
HOST
WWW
EXAMPLE
COM
TO
WHICH
THE
SERVER
SHOULD
REPLY
ASSUMING
IT
FINDS
THE
FILE
HTTP
OK
DATE
THU
NOV
GMT
CONTENT
LENGTH
CONNECTION
CLOSE
CONTENT
TYPE
TEXT
HTML
FOLLOWED
BY
A
BLANK
LINE
AND
THE
CONTENTS
OF
THE
FILE
WHICH
IS
BYTES
IF
THE
FILE
IS
NOT
FOUND
IT
SHOULD
RETURN
THE
INFAMOUS
ERROR
CODE
LIKE
SO
HTTP
NOT
FOUND
TO
GET
THE
CURRENT
DATE
USE
A
COMBINATION
OF
TIME
AND
LOCALTIME
FUNCTIONS
DECLARED
IN
TIME
H
REFER
TO
THE
MANPAGES
FOR
THEIR
USAGE
HERE
IS
AN
EXAMPLE
PIECE
OF
CODE
HTTP
WWW
GNU
ORG
SOFTWARE
LIBC
MANUAL
TIME
FUNCTIONS
EXAMPLE
HTML
REQUIREMENTS
YOUR
TASK
IS
TO
USE
BERKLEY
SOCKETS
TO
ACCEPT
GET
REQUESTS
FOR
HTML
PAGES
OVER
HTTP
YOUR
MAIN
THREAD
SHOULD
WAIT
FOR
A
CONNECTION
TO
OCCUR
SPAWN
OFF
A
WORKER
THREAD
AND
HAVE
THAT
THREAD
COMMUNICATE
TO
THE
REQUESTOR
ACCORDING
TO
THE
ABOVE
PROTOCOL
WHEN
THE
THREAD
IS
DONE
IT
SHOULD
ADD
THE
REQUEST
FOR
THAT
PARTICULAR
WEBPAGE
TO
A
FILE
NAMED
STATS
TXT
NOTE
THAT
THIS
FILE
NEEDS
TO
BE
EXCLUSIVELY
ACCESSED
SO
YOU
LL
NEED
TO
DO
SOME
SORT
OF
SYNCHRONIZATION
EACH
REQUEST
SHOULD
APPEND
SOMETHING
LIKE
THE
FOLLOWING
TO
STATS
TXT
ASSUMING
THE
WEB
CLIENT
CONNECTED
FROM
IP
AND
PORT
GET
TEST
HTML
HTTP
HOST
WWW
EXAMPLE
COM
CLIENT
THE
IP
AND
PORT
OF
THE
CLIENT
CAN
BE
GATHERED
WHEN
ACCEPTING
THE
CONNECTION
AS
WE
VE
LEARNED
DURING
THE
LECTURES
PORTS
AND
ADDRESSES
PORT
IS
THE
NORMAL
WEB
SERVER
PORT
BUT
WE
CAN
T
ALL
USE
IT
AT
THE
SAME
TIME
PLEASE
USE
YOUR
DESIGNATED
PERSONAL
PORT
NUMBER
LISTED
ON
HTTP
WWW
CS
PITT
EDU
WAHN
TEACHING
MISC
PORTS
PDF
PLEASE
USE
THIS
PORT
AND
ONLY
THIS
PORT
FOR
AN
ADDRESS
OF
THE
MACHINE
WE
WILL
SIMPLY
REFER
TO
IT
AS
LOCALHOST
OR
LOCALHOST
RESERVED
IP
ADDRESS
TESTING
THOTH
CS
PITT
EDU
IS
FIREWALLED
FROM
THE
OUTSIDE
WORLD
MEANING
THAT
YOU
WILL
ONLY
BE
ABLE
TO
CONNECT
TO
YOUR
SERVER
FROM
THOTH
ITSELF
IN
ORDER
TO
DO
THIS
YOUR
BEST
BET
FOR
TESTING
IS
TO
USE
A
FEW
PROGRAMS
TELNET
LOCALHOST
PORT
WGET
HTTP
PORT
PAGE
HTML
LINKS
NO
CONNECT
HTTP
PORT
PAGE
HTML
TELNET
IS
A
TERMINAL
EMULATOR
YOU
CAN
CONNECT
DIRECTLY
TO
YOUR
SERVER
AND
ANYTHING
YOU
TYPE
WILL
BE
SENT
THIS
MEANS
YOU
CAN
MANUALLY
CREATE
A
GET
REQUEST
AND
YOU
WILL
SEE
THE
REPLY
OF
THE
SERVER
IN
PLAIN
TEXT
ON
YOUR
SCREEN
WGET
DOWNLOADS
FILES
FROM
THE
INTERNET
LINKS
IS
A
TEXT
MODE
WEB
BROWSER
NO
GRAPHICS
NO
TABLES
AND
MINIMAL
FONT
SUPPORT
BUT
IT
A
REAL
WORKING
BROWSER
EACH
OF
THE
ABOVE
WEB
CLIENTS
SENDS
HTTP
REQUESTS
OF
SLIGHTLY
DIFFERENT
FORMATS
BUT
THEY
ARE
ALL
GOVERNED
BY
THE
HTTP
PROTOCOL
THE
REQUEST
FORMAT
IS
GIVEN
IN
THE
BELOW
LINK
HTTP
WWW
ORG
PROTOCOLS
HTML
AS
YOU
CAN
SEE
THE
REQUEST
MESSAGE
HEADER
AND
MESSAGE
BODY
IS
ALWAYS
SEPARATED
BY
TWO
CRLFS
IN
OUR
SIMPLE
INTERACTIONS
YOU
CAN
ASSUME
THAT
THE
WEB
CLIENT
NEVER
SENDS
THE
MESSAGE
BODY
HENCE
IN
YOUR
WEB
SERVER
YOU
CAN
SAFELY
ASSUME
THAT
YOU
HAVE
FINISHED
RECEIVING
AN
HTTP
REQUEST
WHEN
YOU
RECEIVE
TWO
CRLFS
A
CRLF
IS
A
SEQUENCE
OF
A
CR
ASCII
CODE
AND
A
LF
ASCII
CODE
CHARACTER
MY
ADVICE
IS
TO
WORK
WITH
TWO
SSH
WINDOWS
OPEN
ONE
WITH
YOUR
SERVER
PRINTING
ERROR
MESSAGES
TO
STDERR
AND
ONE
THAT
YOU
ARE
USING
ONE
OF
THE
ABOVE
PROGRAMS
TO
REQUEST
PAGES
SUBMISSION
YOU
NEED
TO
SUBMIT
YOUR
WELL
COMMENTED
PROGRAM
SOURCE
MAKE
A
TAR
GZ
FILE
AS
IN
THE
FIRST
ASSIGNMENT
NAMED
USERNAME
TAR
GZ
COPY
IT
TO
WAHN
SUBMIT
BY
THE
DEADLINE
FOR
CREDIT
CS
THOTH
CS
PITT
EDU
WE
RE
GOING
TO
USE
THOTH
CS
PITT
EDU
TO
DO
OUR
WORK
FOR
THIS
COURSE
THIS
IS
A
QUAD
CORE
MACHINE
WITH
MANY
GBS
OF
RAM
AND
OVER
A
TERABYTE
OF
DISK
SPACE
IN
OTHER
WORDS
A
MODERN
POWERFUL
MACHINE
TO
USE
IT
WE
FIRST
NEED
TO
MODIFY
THE
ENVIRONMENT
TO
BE
MORE
SUITABLE
FOR
OUR
PURPOSES
WE
DO
THIS
BY
EDITING
A
FILE
NAMED
WHICH
IS
A
SCRIPT
THAT
SETUPS
UP
THE
ENVIRONMENT
WHEN
YOU
FIRST
LOG
IN
MODIFYING
THE
MANPATH
TO
BE
ABLE
TO
USE
THE
MAN
PAGES
YOU
WILL
NEED
TO
DO
THE
FOLLOWING
FOR
IT
TO
WORK
PROPERLY
PLEASE
ENTER
THE
FOLLOWING
COMMANDS
AFTER
YOU
FIRST
LOG
IN
THIS
WILL
GIVE
YOU
WRITE
PERMISSIONS
TO
AND
OPEN
THE
NANO
EDITOR
NOW
SCROLL
DOWN
TO
THE
END
OF
THE
FILE
UNTIL
YOU
SEE
THE
LINE
ADD
THE
FOLLOWING
LINES
SPACING
AROUND
THE
AND
CHARACTERS
NEED
TO
BE
THERE
SAVE
THE
FILE
AND
QUIT
BASICALLY
YOU
ARE
DIRECTING
THE
SCRIPT
TO
RUN
ANOTHER
SCRIPT
LOCATED
IN
OPT
SH
WHICH
IF
YOU
LOOK
INSIDE
IT
SETS
UP
THE
MANPATH
ENVIRONMENT
VARIABLE
THE
MAN
COMMAND
SEARCHES
THIS
PATH
TO
LOOK
FOR
THE
MAN
PAGES
LASTLY
REMOVE
WRITE
PERMISSIONS
FROM
FOR
SAFETY
USING
THE
FOLLOWING
COMMAND
NOW
WILL
NOT
RUN
UNTIL
THE
NEXT
TIME
YOU
LOG
IN
SO
LET
FORCE
RUN
THE
SCRIPT
SO
THAT
IT
IS
APPLIED
TO
THIS
LOGIN
SESSION
RUN
THE
FOLLOWING
COMMAND
TO
CHECK
THAT
IT
WORKED
TYPE
AT
THE
PROMPT
IF
YOU
SEE
A
HELP
PAGE
FOR
THE
LS
COMMAND
IT
WORKED
IF
YOU
SEE
THE
FOLLOWING
THEN
YOU
DID
SOMETHING
WRONG
IF
YOU
GET
THE
ABOVE
GO
BACK
AND
CHECK
THE
AND
TRY
AGAIN
ASK
THE
TA
FOR
HELP
IF
YOU
CAN
T
FIGURE
IT
OUT
MODIFYING
CHARACTER
ENCODING
ERROR
MESSAGES
FROM
THE
COMPILER
IS
CRUCIAL
TO
DEBUGGING
YOUR
PROGRAM
IF
YOU
ARE
HAVING
TROUBLE
SEEING
ERROR
MESSAGES
FROM
GCC
WHEN
IT
HAS
A
COMPILE
ERROR
THE
ISSUE
MAY
BE
BECAUSE
OF
CHARACTER
ENCODING
THE
DEFAULT
IN
THOTH
IS
SET
TO
UTF
THIS
IS
WHAT
YOU
PROBABLY
SEE
WHEN
YOU
ECHO
THE
LANG
ENVIRONMENT
VARIABLE
BY
TYPING
THE
SOLUTION
IS
TO
CONFIGURE
YOUR
TERMINAL
SO
THAT
IT
CAN
VIEW
UTF
PROPERLY
FOR
EXAMPLE
IN
MAC
OS
X
TERMINALS
THERE
IS
A
SETTING
TO
CHANGE
ENCODING
IN
PREFERENCES
ADVANCED
IF
THAT
IS
NOT
FEASIBLE
YOU
NEED
TO
ADD
THE
FOLLOWING
LINE
TO
THE
BOTTOM
OF
YOUR
USING
THE
METHOD
EXPLAINED
ABOVE
THEN
THE
NEXT
TIME
YOU
LOGIN
AND
TRY
ECHOING
LANG
YOU
LL
GET
THE
FOLLOWING
OUTPUT
NOW
THINGS
SHOULD
WORK
FINE
ON
ALL
TERMINALS
NAME
LAB
INTRODUCTION
TO
UNIX
AND
C
THIS
FIRST
LAB
IS
MEANT
TO
BE
AN
INTRODUCTION
TO
COMPUTER
ENVIRONMENTS
WE
WILL
BE
USING
THIS
TERM
YOU
MUST
HAVE
A
PITT
USERNAME
TO
COMPLETE
THIS
LAB
NOTE
TEXT
IN
UNIX
IS
CASE
SENSITIVE
IS
DIFFERENT
FROM
IS
DIFFERENT
FROM
ALL
FILENAMES
OF
CONCERN
IN
THIS
LAB
ARE
LOWERCASE
PLEASE
FOLLOW
THE
INSTRUCTIONS
AS
LISTED
IN
THIS
DOCUMENT
HERE
ARE
A
FEW
COMMON
UNIX
COMMANDS
CD
CHANGE
DIRECTORY
LS
LIST
ALL
FILES
IN
CURRENT
DIRECTORY
PWD
DISPLAY
THE
CURRENT
DIRECTORY
MV
MOVE
OR
RENAME
A
FILE
CP
COPY
A
FILE
GCC
COMPILE
A
C
FILE
PICO
EDIT
A
TEXT
FILE
PART
I
TO
LOGIN
TO
THE
COMPUTERS
YOU
WILL
NEED
TO
USE
AN
SSH
CLIENT
THE
SSH
CLIENT
THAT
WE
WILL
BE
USING
IS
PUTTY
AT
HOME
DOWNLOAD
FROM
HTTP
WWW
CHIARK
GREENEND
ORG
UK
SGTATHAM
PUTTY
DOWNLOAD
HTML
WE
WILL
CONNECT
TO
THE
MACHINE
HOST
THOTH
CS
PITT
EDU
WHEN
YOU
LOGIN
FIRST
YOU
ARE
PLACED
IN
YOUR
HOME
DIRECTORY
THE
COMMAND
WILL
LIST
ALL
OF
THE
FILES
AND
DIRECTORIES
THERE
THE
ONE
WE
ARE
MOST
CONCERNED
WITH
IS
THE
PRIVATE
DIRECTORY
IT
IS
SPECIAL
IN
THAT
ONLY
YOU
CAN
ACCESS
FILES
INSIDE
THIS
DIRECTORY
IT
WILL
KEEP
YOUR
WORK
SAFE
FROM
OTHER
PEOPLE
LET
MOVE
INTO
THE
PRIVATE
DIRECTORY
SO
WE
CAN
WORK
THERE
CHANGES
DIRECTORY
TO
THE
PRIVATE
DIRECTORY
FOR
THIS
CLASS
WE
LL
KEEP
ALL
OF
OUR
FILES
ORGANIZED
INTO
A
DIRECTORY
MAKE
IT
BY
TYPING
IF
YOU
WANT
TO
DOUBLE
CHECK
THAT
IT
WORKED
TYPE
LS
TO
LIST
WE
NOW
WANT
TO
MOVE
INTO
THE
DIRECTORY
TO
DO
OUR
ACTUAL
WORK
PART
II
WHILE
STILL
IN
THE
DIRECTORY
TYPE
TO
MAKE
A
DIRECTORY
FOR
TODAY
LAB
NOW
TYPE
NANO
IS
A
VERY
SIMPLE
TEXT
EDITOR
A
LOT
LIKE
NOTEPAD
ON
WINDOWS
IT
IS
ONE
OPTION
FOR
CREATING
AND
EDITING
CODE
UNDER
UNIX
LINUX
TYPE
THE
FOLLOWING
TEXT
IN
EXACTLY
AS
IT
IS
SHOWN
SAVE
THE
FILE
BY
HITTING
CTRL
O
AND
THEN
ENTER
EXIT
PICO
BY
TYPING
CTRL
X
AT
THE
BOTTOM
OF
THE
PICO
WINDOW
IT
SHOWS
WHAT
KEYS
DO
SPECIAL
THINGS
THE
MEANS
TO
HOLD
CTRL
WHILE
PRESSING
THE
KEY
BACK
AT
THE
PROMPT
TYPE
WHICH
WILL
MAKE
OUR
PROGRAM
A
FILE
NAMED
WILL
BE
IN
THE
DIRECTORY
IF
WE
TYPE
LS
RUN
IT
BY
TYPING
PART
III
ARCHIVES
AND
PROJECT
SUBMISSION
WHENEVER
YOU
TURN
IN
A
PROJECT
FOR
THIS
COURSE
YOU
WILL
NEED
TO
SUBMIT
A
COPY
OF
YOUR
CODE
AND
EXECUTABLE
TO
BE
GRADED
WE
WILL
TRY
THIS
NOW
YOU
ARE
PROBABLY
FAMILIAR
WITH
A
ZIP
FILE
WHICH
DOES
TWO
THINGS
IT
ARCHIVES
A
BUNCH
OF
FILES
INTO
A
SINGLE
FILE
AND
ALSO
COMPRESSES
IT
TO
SAVE
SPACE
IN
UNIX
WE
DO
THIS
IN
TWO
STEPS
WE
CREATE
A
TAPE
ARCHIVE
TAR
AND
THEN
COMPRESS
IT
GZIP
FIRST
LET
US
GO
BACK
UP
TO
OUR
DIRECTORY
NOW
LET
US
FIRST
MAKE
THE
ARCHIVE
TYPE
YOUR
USERNAME
FOR
THE
USERNAME
PART
OF
THE
FILENAME
AND
THEN
WE
CAN
COMPRESS
IT
WHICH
WILL
PRODUCE
A
TAR
GZ
FILE
WE
WILL
THEN
COPY
THAT
FILE
TO
THE
SUBMISSION
DIRECTORY
ONCE
A
FILE
IS
COPIED
INTO
THAT
DIRECTORY
YOU
CANNOT
CHANGE
IT
RENAME
IT
OR
DELETE
IT
IF
YOU
MAKE
A
MISTAKE
RESUBMIT
A
NEW
FILE
WITH
A
NEW
NAME
BEING
SURE
TO
INCLUDE
YOUR
USERNAME
PART
IV
MANUAL
PAGES
IF
YOU
EVER
WANT
TO
SEE
HOW
A
COMMAND
WORKS
OR
YOU
FORGET
THE
VARIOUS
OPTIONS
YOU
COULD
USE
YOU
CAN
CONSULT
THE
MAN
PAGES
ON
THE
COMMAND
BY
TYPING
FOR
EXAMPLE
THIS
WILL
LET
YOU
SCROLL
THROUGH
THE
ONLINE
HELP
ABOUT
THE
LS
COMMAND
THE
SPACE
BAR
WILL
SCROLL
THE
DOCUMENT
ONE
SCREENFUL
AT
A
TIME
AND
THE
ENTER
KEY
WILL
MOVE
ONE
LINE
AT
A
TIME
AT
ANY
TIME
YOU
CAN
QUIT
BY
PRESSING
Q
IN
THE
SPACE
BELOW
EXPLAIN
THE
PURPOSE
OF
THE
SWITCH
CAPITAL
NOT
LOWERCASE
PART
V
RECORDING
YOUR
WORK
WITH
SCRIPT
SOMETIMES
SOMETHING
WILL
GO
AWRY
IN
YOUR
PROGRAM
AND
YOU
MAY
NOT
KNOW
THE
SOURCE
OF
AN
ERROR
MESSAGE
FROM
THE
COMPILER
FOR
US
TO
HELP
YOU
WE
NEED
TO
SEE
THE
OUTPUT
OF
THE
COMPILER
WE
CAN
CAPTURE
THAT
WITH
A
PROGRAM
CALLED
SCRIPT
TYPE
NOW
TO
READ
UP
ON
HOW
IT
WORKS
NOW
OPEN
THE
C
FILE
FROM
PART
I
BY
NAVIGATING
TO
IT
AND
THEN
OPENING
IT
IN
PICO
REMOVE
THE
SEMICOLON
AFTER
THE
PRINTF
STATEMENT
AND
SAVE
NOW
ISSUE
THE
SCRIPT
COMMAND
THEN
COMPILE
THE
MODIFIED
PROGRAM
THIS
SHOULD
RESULT
IN
AN
ERROR
MESSAGE
TYPE
EXIT
OR
HIT
CTRL
D
TO
STOP
SCRIPT
FROM
RECORDING
IF
YOU
DO
AN
LS
NOW
YOU
SHOULD
SEE
A
FILE
NAMED
TYPESCRIPT
WE
CAN
USE
THE
PROGRAM
CALLED
MORE
TO
DISPLAY
THE
CONTENTS
OF
THIS
FILE
MORE
IS
ALSO
USED
ON
THE
MAN
PAGES
AND
CAN
BE
OPERATED
IN
THE
SAME
WAY
IF
THE
FILE
IS
LONGER
THAN
THE
SCREEN
IT
WILL
ALLOW
YOU
TO
SCROLL
OR
TO
QUIT
AT
ANYTIME
TYPE
TO
SEE
THE
OUTPUT
YOU
SAW
WHEN
YOU
COMPILED
THE
FIRST
LAB
HELPFUL
HINTS
KEEP
THIS
SHEET
AS
A
GUIDE
UNTIL
YOU
GET
COMFORTABLE
IN
UNIX
EVERY
TIME
YOU
WANT
TO
WRITE
A
PROGRAM
YOU
WILL
DO
THE
FOLLOWING
AFTER
LOGGING
INTO
YOUR
ACCOUNT
IF
YOU
WANT
TO
USE
VIM
OR
EMACS
INSTEAD
OF
NANO
THAT
IS
FINE
YOU
MAY
PREFER
TO
EDIT
FILES
UNDER
A
GUI
AND
USE
FTP
TO
UPLOAD
FILES
THE
CHOICE
IS
YOURS
HOWEVER
WE
WILL
ONLY
OFFICIALLY
SUPPORT
THE
STEPS
DESCRIBED
HERE
REMEMBER
CASE
MATTERS
INT
SHORT
LONG
LONG
LONG
UNSIGNED
INT
CHAR
FLOAT
DOUBLE
LONG
DOUBLE
THE
HIGHEST
NUMBERED
FILE
BEFORE
THE
DEADLINE
WILL
BE
THE
ONE
THAT
IS
GRADED
HOWEVER
FOR
SIMPLICITY
PLEASE
MAKE
SURE
YOU
VE
DONE
ALL
THE
WORK
AND
INCLUDED
ALL
NECESSARY
FILES
BEFORE
YOU
SUBMIT
ANATOMY
OF
A
PROGRAM
IN
MEMORY
GUSTAVO
DUARTE
HTTP
DUARTES
ORG
GUSTAVO
BLOG
POST
ANATOMYOFAPROGRAMINMEMORY
GUSTAVO
DUARTE
GUSTAVO
BLOG
BRAIN
FOOD
FOR
HACKERS
ANATOMY
OF
A
PROGRAM
IN
MEMORY
JAN
MEMORY
MANAGEMENT
IS
THE
HEART
OF
OPERATING
SYSTEMS
IT
IS
CRUCIAL
FOR
BOTH
PROGRAMMING
AND
SYSTEM
ADMINISTRATION
IN
THE
NEXT
FEW
POSTS
I
LL
COVER
MEMORY
WITH
AN
EYE
TOWARDS
PRACTICAL
ASPECTS
BUT
WITHOUT
SHYING
AWAY
FROM
INTERNALS
WHILE
THE
CONCEPTS
ARE
GENERIC
EXAMPLES
ARE
MOSTLY
FROM
LINUX
AND
WINDOWS
ON
BIT
THIS
RST
POST
DESCRIBES
HOW
PROGRAMS
ARE
LAID
OUT
IN
MEMORY
EACH
PROCESS
IN
A
MULTI
TASKING
OS
RUNS
IN
ITS
OWN
MEMORY
SANDBOX
THIS
SANDBOX
IS
THE
VIRTUAL
ADDRESS
SPACE
WHICH
IN
BIT
MODE
IS
ALWAYS
A
BLOCK
OF
MEMORY
ADDRESSES
THESE
VIRTUAL
ADDRESSES
ARE
MAPPED
TO
PHYSICAL
MEMORY
BY
PAGE
TABLES
WHICH
ARE
MAINTAINED
BY
THE
OPERATING
SYSTEM
KERNEL
AND
CONSULTED
BY
THE
PROCESSOR
EACH
PROCESS
HAS
ITS
OWN
SET
OF
PAGE
TABLES
BUT
THERE
IS
A
CATCH
ONCE
VIRTUAL
ADDRESSES
ARE
ENABLED
THEY
APPLY
TO
ALL
SOFTWARE
RUNNING
IN
THE
MACHINE
INCLUDING
THE
KERNEL
ITSELF
THUS
A
PORTION
OF
THE
VIRTUAL
ADDRESS
SPACE
MUST
BE
RESERVED
TO
THE
KERNEL
THIS
DOES
NOT
MEAN
THE
KERNEL
USES
THAT
MUCH
PHYSICAL
MEMORY
ONLY
THAT
IT
HAS
THAT
PORTION
OF
ADDRESS
SPACE
AVAILABLE
TO
MAP
WHATEVER
PHYSICAL
MEMORY
IT
WISHES
KERNEL
SPACE
IS
AGGED
IN
THE
PAGE
TABLES
AS
EXCLUSIVE
TO
PRIVILEGED
CODE
HTTP
DUARTES
ORG
GUSTAVO
BLOG
POST
CPU
RINGS
PRIVILEGEAND
PROTECTION
RING
OR
LOWER
HENCE
A
PAGE
FAULT
IS
TRIGGERED
IF
USER
MODE
PROGRAMS
TRY
TO
TOUCH
IT
IN
LINUX
KERNEL
SPACE
IS
CONSTANTLY
PRESENT
AND
MAPS
THE
SAME
PHYSICAL
MEMORY
IN
ALL
PROCESSES
KERNEL
CODE
AND
DATA
ARE
ALWAYS
ADDRESSABLE
READY
TO
HANDLE
INTERRUPTS
OR
SYSTEM
CALLS
AT
ANY
TIME
BY
CONTRAST
THE
MAPPING
FOR
THE
USER
MODE
PORTION
OF
THE
ADDRESS
SPACE
CHANGES
WHENEVER
A
PROCESS
SWITCH
HAPPENS
BLUE
REGIONS
REPRESENT
VIRTUAL
ADDRESSES
THAT
ARE
MAPPED
TO
PHYSICAL
MEMORY
WHEREAS
WHITE
REGIONS
ARE
UNMAPPED
IN
THE
EXAMPLE
ABOVE
FIREFOX
HAS
USED
FAR
MORE
OF
ITS
VIRTUAL
ADDRESS
SPACE
DUE
TO
ITS
LEGENDARY
MEMORY
HUNGER
THE
DISTINCT
BANDS
IN
THE
ADDRESS
SPACE
CORRESPOND
TO
MEMORY
SEGMENTS
LIKE
THE
HEAP
STACK
AND
SO
ON
KEEP
IN
MIND
THESE
SEGMENTS
ARE
SIMPLY
A
RANGE
OF
MEMORY
ADDRESSES
AND
HAVE
NOTHING
TO
DO
WITH
INTEL
STYLE
SEGMENTS
HTTP
DUARTES
ORG
GUSTAVO
BLOG
POST
MEMORY
TRANSLATION
AND
SEGMENTATION
ANYWAY
HERE
IS
THE
STANDARD
SEGMENT
LAYOUT
IN
A
LINUX
PROCESS
ANATOMY
OF
A
PROGRAM
IN
MEMORY
GUSTAVO
DUARTE
HTTP
DUARTES
ORG
GUSTAVO
BLOG
POST
ANATOMYOFAPROGRAMINMEMORY
WHEN
COMPUTING
WAS
HAPPY
AND
SAFE
AND
CUDDLY
THE
STARTING
VIRTUAL
ADDRESSES
FOR
THE
SEGMENTS
SHOWN
ABOVE
WERE
EXACTLY
THE
SAME
FOR
NEARLY
EVERY
PROCESS
IN
A
MACHINE
THIS
MADE
IT
EASY
TO
EXPLOIT
SECURITY
VULNERABILITIES
REMOTELY
AN
EXPLOIT
OFTEN
NEEDS
TO
REFERENCE
ABSOLUTE
MEMORY
LOCATIONS
AN
ADDRESS
ON
THE
STACK
THE
ADDRESS
FOR
A
LIBRARY
FUNCTION
ETC
REMOTE
ATTACKERS
MUST
CHOOSE
THIS
LOCATION
BLINDLY
COUNTING
ON
THE
FACT
THAT
ADDRESS
SPACES
ARE
ALL
THE
SAME
WHEN
THEY
ARE
PEOPLE
GET
PWNED
THUS
ADDRESS
SPACE
RANDOMIZATION
HAS
BECOME
POPULAR
LINUX
RANDOMIZES
THE
STACK
HTTP
LXR
LINUX
NO
LINUX
FS
C
MEMORY
MAPPING
SEGMENT
HTTP
LXR
LINUX
NO
LINUX
ARCH
MM
MMAP
C
AND
HEAP
HTTP
LXR
LINUX
NO
LINUX
ARCH
KERNEL
C
BY
ADDING
O
SETS
TO
THEIR
STARTING
ADDRESSES
UNFORTUNATELY
THE
BIT
ADDRESS
SPACE
IS
PRETTY
TIGHT
LEAVING
LITTLE
ROOM
FOR
RANDOMIZATION
AND
HAMPERING
ITS
E
ECTIVENESS
HTTP
WWW
STANFORD
EDU
BLP
PAPERS
ASRANDOM
PDF
THE
TOPMOST
SEGMENT
IN
THE
PROCESS
ADDRESS
SPACE
IS
THE
STACK
WHICH
STORES
LOCAL
VARIABLES
AND
FUNCTION
PARAMETERS
IN
MOST
PROGRAMMING
LANGUAGES
CALLING
A
METHOD
OR
FUNCTION
PUSHES
A
NEW
STACK
FRAME
ONTO
THE
STACK
THE
STACK
FRAME
IS
DESTROYED
WHEN
THE
FUNCTION
RETURNS
THIS
SIMPLE
DESIGN
POSSIBLE
BECAUSE
THE
DATA
OBEYS
STRICT
LIFO
HTTP
EN
WIKIPEDIA
ORG
WIKI
LIFO
ORDER
MEANS
THAT
NO
COMPLEX
DATA
STRUCTURE
IS
NEEDED
TO
TRACK
STACK
CONTENTS
A
SIMPLE
POINTER
TO
THE
TOP
OF
THE
STACK
WILL
DO
PUSHING
AND
POPPING
ARE
THUS
VERY
FAST
AND
DETERMINISTIC
ALSO
THE
CONSTANT
REUSE
OF
STACK
REGIONS
TENDS
TO
KEEP
ACTIVE
STACK
MEMORY
IN
THE
CPU
CACHES
HTTP
DUARTES
ORG
GUSTAVO
BLOG
POST
INTEL
CPU
CACHES
SPEEDING
UP
ACCESS
EACH
THREAD
IN
A
PROCESS
GETS
ITS
OWN
STACK
IT
IS
POSSIBLE
TO
EXHAUST
THE
AREA
MAPPING
THE
STACK
BY
PUSHING
MORE
DATA
THAN
IT
CAN
T
THIS
TRIGGERS
A
PAGE
FAULT
THAT
IS
HANDLED
IN
LINUX
BY
HTTP
LXR
LINUX
NO
LINUX
MM
MMAP
C
WHICH
IN
TURN
CALLS
HTTP
LXR
LINUX
NO
LINUX
MM
MMAP
C
TO
CHECK
WHETHER
IT
APPROPRIATE
TO
GROW
THE
STACK
IF
THE
STACK
SIZE
IS
BELOW
USUALLY
THEN
NORMALLY
THE
STACK
GROWS
AND
THE
PROGRAM
CONTINUES
MERRILY
UNAWARE
OF
WHAT
JUST
HAPPENED
THIS
IS
THE
NORMAL
MECHANISM
WHEREBY
STACK
SIZE
ADJUSTS
TO
DEMAND
HOWEVER
IF
THE
MAXIMUM
STACK
SIZE
HAS
BEEN
REACHED
WE
HAVE
A
STACK
OVER
OW
AND
THE
PROGRAM
RECEIVES
A
SEGMENTATION
FAULT
WHILE
THE
MAPPED
STACK
AREA
EXPANDS
TO
MEET
DEMAND
IT
DOES
NOT
SHRINK
BACK
WHEN
THE
STACK
GETS
SMALLER
LIKE
THE
FEDERAL
BUDGET
IT
ONLY
EXPANDS
ANATOMY
OF
A
PROGRAM
IN
MEMORY
GUSTAVO
DUARTE
HTTP
DUARTES
ORG
GUSTAVO
BLOG
POST
ANATOMYOFAPROGRAMINMEMORY
DYNAMIC
STACK
GROWTH
IS
THE
ONLY
SITUATION
HTTP
LXR
LINUX
NO
LINUX
ARCH
MM
FAULT
C
IN
WHICH
ACCESS
TO
AN
UNMAPPED
MEMORY
REGION
SHOWN
IN
WHITE
ABOVE
MIGHT
BE
VALID
ANY
OTHER
ACCESS
TO
UNMAPPED
MEMORY
TRIGGERS
A
PAGE
FAULT
THAT
RESULTS
IN
A
SEGMENTATION
FAULT
SOME
MAPPED
AREAS
ARE
READ
ONLY
HENCE
WRITE
ATTEMPTS
TO
THESE
AREAS
ALSO
LEAD
TO
SEGFAULTS
BELOW
THE
STACK
WE
HAVE
THE
MEMORY
MAPPING
SEGMENT
HERE
THE
KERNEL
MAPS
CONTENTS
OF
LES
DIRECTLY
TO
MEMORY
ANY
APPLICATION
CAN
ASK
FOR
SUCH
A
MAPPING
VIA
THE
LINUX
MMAP
HTTP
WWW
KERNEL
ORG
DOC
MAN
PAGES
ONLINE
PAGES
MMAP
HTML
SYSTEM
CALL
IMPLEMENTATION
HTTP
LXR
LINUX
NO
LINUX
ARCH
KERNEL
C
OR
CREATEFILEMAPPING
HTTP
MSDN
MICROSOFT
COM
EN
US
LIBRARY
VS
ASPX
MAPVIEWOFFILE
HTTP
MSDN
MICROSOFT
COM
EN
US
LIBRARY
VS
ASPX
IN
WINDOWS
MEMORY
MAPPING
IS
A
CONVENIENT
AND
HIGH
PERFORMANCE
WAY
TO
DO
LE
I
O
SO
IT
IS
USED
FOR
LOADING
DYNAMIC
LIBRARIES
IT
IS
ALSO
POSSIBLE
TO
CREATE
AN
ANONYMOUS
MEMORY
MAPPING
THAT
DOES
NOT
CORRESPOND
TO
ANY
LES
BEING
USED
INSTEAD
FOR
PROGRAM
DATA
IN
LINUX
IF
YOU
REQUEST
A
LARGE
BLOCK
OF
MEMORY
VIA
MALLOC
HTTP
WWW
KERNEL
ORG
DOC
MAN
PAGES
ONLINE
PAGES
MALLOC
HTML
THE
C
LIBRARY
WILL
CREATE
SUCH
AN
ANONYMOUS
MAPPING
INSTEAD
OF
USING
HEAP
MEMORY
LARGE
MEANS
LARGER
THAN
BYTES
KB
BY
DEFAULT
AND
ADJUSTABLE
VIA
MALLOPT
HTTP
WWW
KERNEL
ORG
DOC
MAN
PAGES
ONLINE
PAGES
UNDOCUMENTED
HTML
SPEAKING
OF
THE
HEAP
IT
COMES
NEXT
IN
OUR
PLUNGE
INTO
ADDRESS
SPACE
THE
HEAP
PROVIDES
RUNTIME
MEMORY
ALLOCATION
LIKE
THE
STACK
MEANT
FOR
DATA
THAT
MUST
OUTLIVE
THE
FUNCTION
DOING
THE
ALLOCATION
UNLIKE
THE
STACK
MOST
LANGUAGES
PROVIDE
HEAP
MANAGEMENT
TO
PROGRAMS
SATISFYING
MEMORY
REQUESTS
IS
THUS
A
JOINT
A
AIR
BETWEEN
THE
LANGUAGE
RUNTIME
AND
THE
KERNEL
IN
C
THE
INTERFACE
TO
HEAP
ALLOCATION
IS
MALLOC
HTTP
WWW
KERNEL
ORG
DOC
MAN
PAGES
ONLINE
PAGES
MALLOC
HTML
AND
FRIENDS
WHEREAS
IN
A
GARBAGE
COLLECTED
LANGUAGE
LIKE
C
THE
INTERFACE
IS
THE
NEW
KEYWORD
IF
THERE
IS
ENOUGH
SPACE
IN
THE
HEAP
TO
SATISFY
A
MEMORY
REQUEST
IT
CAN
BE
HANDLED
BY
THE
LANGUAGE
RUNTIME
WITHOUT
KERNEL
INVOLVEMENT
OTHERWISE
THE
HEAP
IS
ENLARGED
VIA
THE
BRK
HTTP
WWW
KERNEL
ORG
DOC
MAN
PAGES
ONLINE
PAGES
BRK
HTML
SYSTEM
CALL
IMPLEMENTATION
HTTP
LXR
LINUX
NO
LINUX
MM
MMAP
C
TO
MAKE
ROOM
FOR
THE
REQUESTED
BLOCK
HEAP
MANAGEMENT
IS
COMPLEX
HTTP
G
OSWEGO
EDU
DL
HTML
MALLOC
HTML
REQUIRING
SOPHISTICATED
ALGORITHMS
THAT
STRIVE
FOR
SPEED
AND
E
CIENT
MEMORY
USAGE
IN
THE
FACE
OF
OUR
PROGRAMS
CHAOTIC
ALLOCATION
PATTERNS
THE
TIME
NEEDED
TO
SERVICE
A
HEAP
REQUEST
CAN
VARY
SUBSTANTIALLY
REAL
TIME
SYSTEMS
HAVE
SPECIAL
PURPOSE
ALLOCATORS
HTTP
RTPORTAL
UPV
ES
RTMALLOC
TO
DEAL
WITH
THIS
PROBLEM
HEAPS
ALSO
BECOME
FRAGMENTED
SHOWN
BELOW
FINALLY
WE
GET
TO
THE
LOWEST
SEGMENTS
OF
MEMORY
BSS
DATA
AND
PROGRAM
TEXT
BOTH
BSS
AND
DATA
STORE
CONTENTS
FOR
STATIC
GLOBAL
VARIABLES
IN
C
THE
DI
ERENCE
IS
THAT
BSS
STORES
THE
CONTENTS
OF
UNINITIALIZED
STATIC
VARIABLES
WHOSE
VALUES
ARE
NOT
SET
BY
THE
PROGRAMMER
IN
SOURCE
CODE
THE
BSS
MEMORY
AREA
IS
ANONYMOUS
IT
DOES
NOT
MAP
ANY
LE
IF
YOU
SAY
STATIC
INT
CNTACTIVEUSERS
THE
CONTENTS
OF
CNTACTIVEUSERS
LIVE
IN
THE
BSS
THE
DATA
SEGMENT
ON
THE
OTHER
HAND
HOLDS
THE
CONTENTS
FOR
STATIC
VARIABLES
INITIALIZED
IN
SOURCE
CODE
THIS
MEMORY
AREA
IS
NOT
ANONYMOUS
IT
MAPS
THE
PART
OF
THE
PROGRAM
BINARY
IMAGE
THAT
CONTAINS
THE
INITIAL
STATIC
VALUES
GIVEN
IN
SOURCE
CODE
SO
IF
YOU
SAY
STATIC
INT
CNTWORKERBEES
THE
CONTENTS
OF
CNTWORKERBEES
LIVE
IN
THE
DATA
SEGMENT
AND
START
OUT
AS
EVEN
THOUGH
THE
DATA
SEGMENT
MAPS
A
LE
IT
IS
A
PRIVATE
MEMORY
MAPPING
WHICH
MEANS
THAT
UPDATES
TO
MEMORY
ARE
NOT
RE
ECTED
IN
THE
UNDERLYING
LE
THIS
MUST
BE
THE
CASE
OTHERWISE
ASSIGNMENTS
TO
GLOBAL
VARIABLES
WOULD
CHANGE
YOUR
ON
DISK
BINARY
IMAGE
INCONCEIVABLE
ANATOMY
OF
A
PROGRAM
IN
MEMORY
GUSTAVO
DUARTE
HTTP
DUARTES
ORG
GUSTAVO
BLOG
POST
ANATOMYOFAPROGRAMINMEMORY
TWITTER
COM
M
AILTO
DUARTES
ORG
THE
DATA
EXAMPLE
IN
THE
DIAGRAM
IS
TRICKIER
BECAUSE
IT
USES
A
POINTER
IN
THAT
CASE
THE
CONTENTS
OF
POINTER
GONZO
A
BYTE
MEMORY
ADDRESS
LIVE
IN
THE
DATA
SEGMENT
THE
ACTUAL
STRING
IT
POINTS
TO
DOES
NOT
HOWEVER
THE
STRING
LIVES
IN
THE
TEXT
SEGMENT
WHICH
IS
READ
ONLY
AND
STORES
ALL
OF
YOUR
CODE
IN
ADDITION
TO
TIDBITS
LIKE
STRING
LITERALS
THE
TEXT
SEGMENT
ALSO
MAPS
YOUR
BINARY
LE
IN
MEMORY
BUT
WRITES
TO
THIS
AREA
EARN
YOUR
PROGRAM
A
SEGMENTATION
FAULT
THIS
HELPS
PREVENT
POINTER
BUGS
THOUGH
NOT
AS
E
ECTIVELY
AS
AVOIDING
C
IN
THE
RST
PLACE
HERE
A
DIAGRAM
SHOWING
THESE
SEGMENTS
AND
OUR
EXAMPLE
VARIABLES
YOU
CAN
EXAMINE
THE
MEMORY
AREAS
IN
A
LINUX
PROCESS
BY
READING
THE
LE
PROC
MAPS
KEEP
IN
MIND
THAT
A
SEGMENT
MAY
CONTAIN
MANY
AREAS
FOR
EXAMPLE
EACH
MEMORY
MAPPED
LE
NORMALLY
HAS
ITS
OWN
AREA
IN
THE
MMAP
SEGMENT
AND
DYNAMIC
LIBRARIES
HAVE
EXTRA
AREAS
SIMILAR
TO
BSS
AND
DATA
THE
NEXT
POST
WILL
CLARIFY
WHAT
AREA
REALLY
MEANS
ALSO
SOMETIMES
PEOPLE
SAY
DATA
SEGMENT
MEANING
ALL
OF
DATA
BSS
HEAP
YOU
CAN
EXAMINE
BINARY
IMAGES
USING
THE
NM
HTTP
MANPAGES
UBUNTU
COM
MANPAGES
INTREPID
EN
NM
HTML
AND
OBJDUMP
HTTP
MANPAGES
UBUNTU
COM
MANPAGES
INTREPID
EN
OBJDUMP
HTML
COMMANDS
TO
DISPLAY
SYMBOLS
THEIR
ADDRESSES
SEGMENTS
AND
SO
ON
FINALLY
THE
VIRTUAL
ADDRESS
LAYOUT
DESCRIBED
ABOVE
IS
THE
EXIBLE
LAYOUT
IN
LINUX
WHICH
HAS
BEEN
THE
DEFAULT
FOR
A
FEW
YEARS
IT
ASSUMES
THAT
WE
HAVE
A
VALUE
FOR
WHEN
THAT
NOT
THE
CASE
LINUX
REVERTS
BACK
TO
THE
CLASSIC
LAYOUT
SHOWN
BELOW
THAT
IT
FOR
VIRTUAL
ADDRESS
SPACE
LAYOUT
THE
NEXT
POST
DISCUSSES
HOW
THE
KERNEL
KEEPS
TRACK
OF
THESE
MEMORY
AREAS
COMING
UP
WE
LL
LOOK
AT
MEMORY
MAPPING
HOW
LE
READING
AND
WRITING
TIES
INTO
INTRODUCTION
TO
OS
WHY
TAKE
THIS
CLASS
WHY
WITH
MOSSE
IT
MANDATORY
IT
A
GREAT
CLASS
IT
A
GREAT
PROF
IT
EASY
NOT
DO
NOT
FOOL
THYSELF
IT
GOOD
FOR
YOU
LIFE
IS
NOT
LIFE
ANYMORE
WHILE
THIS
CLASS
IS
GOING
ON
BE
CAREFUL
SPECIALLY
IF
YOU
RE
TAKING
ALSO
COMPILERS
OR
SOME
OTHER
HARD
PROGRAMMING
CLASS
CLASS
OUTLINE
BOOK
TANENBAUM
MODERN
OSS
INTRO
TO
OSS
INCLUDING
REAL
TIME
OSS
PROCESSES
DEFINITION
SYNCHRONIZATION
MANAGEMENT
MEMORY
VIRTUAL
MEMORY
MEMORY
ALLOCATION
IO
DISKS
SENSORS
ACTUATORS
KEYBOARDS
ETC
INTERPROCESS
COMMUNICATION
NETWORKING
DATA
TRANSMISSION
ETC
FAULT
TOLERANCE
REAL
TIME
AND
SECURITY
TIME
PERMITTING
SCHEDULE
AND
GRADING
POP
QUIZZES
OF
GRADE
ABOUT
EVERY
WEEKS
PROGRAMMING
ASSIGNMENT
NACHOS
ON
UNIX
OF
GRADE
MIDTERM
OF
GRADE
AROUND
MARCH
SECOND
EXAM
OF
GRADE
APRIL
CLASS
PARTICIPATION
MAY
CARRY
OF
GRADE
FOR
EXTRA
CREDIT
PROJECT
IS
SELF
TEST
FOR
YOU
TO
TEST
WHETHER
YOU
WILL
DIE
OR
NOT
TAKING
THIS
CLASS
ADD
DROP
PERIOD
ENDS
TUESDAY
JAN
OPERATING
SYSTEMS
MANAGES
DIFFERENT
RESOURCES
CPU
MEM
DISK
ETC
IMPROVES
PERFORMANCE
RESPONSE
TIME
THROUGHPUT
ETC
ALLOWS
PORTABILITY
ENABLES
EASIER
PROGRAMMING
NO
NEED
TO
KNOW
WHAT
THE
UNDERLYING
HARDWARE
INTERFACE
BETWEEN
THE
HARDWARE
AND
THE
REST
OF
THE
MACHINE
EDITORS
COMPILERS
USER
PROGRAMS
ETC
STANDARD
INTERFACE
IS
TYPICALLY
DONE
IN
TWO
WAYS
SYSTEM
CALLS
CONTROL
GOES
TO
THE
OPERATING
SYSTEM
LIBRARY
CALLS
CONTROL
REMAINS
WITH
THE
USER
FIRST
GENERATION
OF
COMPUTERS
HAD
NO
OS
SINGLE
USER
ALL
CODING
DONE
DIRECTLY
IN
MACHINE
LANGUAGE
MEMORY
RESIDENT
CODE
NO
OTHER
RESOURCES
TO
MANAGE
SECOND
GENERATION
HAS
BASIC
OS
BATCH
PROCESSING
READ
INPUT
TAPE
CARDS
PROCESS
OUTPUT
TO
TAPE
OR
PRINT
THIRD
GENERATION
IMPROVED
LIFE
MULTIPROGRAMMING
CAREFUL
PARTITIONING
OF
MEMORY
SPACE
DRUMS
AND
DISKS
ADDED
FOR
READING
CARDS
AND
SPOOLING
OUTPUTS
SIMULTANEOUS
PERIPHERALS
OPERATIONS
ON
LINE
TIME
SHARING
CREATED
SEVERAL
VIRTUAL
MACHINES
FOURTH
GENERATION
PCS
AND
WORKSTATIONS
CHEAPER
FASTER
MORE
USER
FRIENDLY
THANK
MACS
FOR
INTERFACES
UNIX
PRECURSOR
MULTICS
MULTIPLEXED
INFORMATION
AND
COMPUTING
SERVICES
WAS
THE
FIRST
MODERN
OS
BELL
MIT
GE
MULTICS
UNITS
UNIX
BERKELEY
IMPROVED
ON
IT
PAGING
VIRTUAL
MEMORY
FILE
SYSTEMS
SIGNALS
INTERRUPTS
NETWORKING
NETWORKED
OSS
ARE
CONNECTED
THROUGH
A
NETWORK
BUT
USER
NEEDS
TO
KNOW
THE
NAME
TYPE
LOCATION
OF
EVERYTHING
DISTRIBUTED
OSS
E
G
AMOEBA
MACH
LOCUS
PROVIDE
TRANSPARENCY
TO
USER
YIELDING
ONE
HUGE
VIRTUAL
MACHINE
SPECIALIZED
OSS
ARE
BUILT
FOR
SPECIFIC
PURPOSES
ROUTING
ENGINES
NETWORKING
LISP
MACHINES
AI
TIME
CONSTRAINED
APPLICATIONS
REAL
TIME
INTERNET
WWW
SERVERS
MASSIVELY
PARALLEL
USES
SUPERCOMPUTERS
ETC
ALL
THESE
ARE
COMING
TOGETHER
HARD
TO
IDENTIFY
BOUNDARIES
ANYMORE
EXCELLENT
MARKETING
SOME
GOOD
PRODUCTS
OSS
STARTED
WITH
DOS
DISK
OS
NO
NOTHING
JUST
VERY
SIMPLE
COMMANDS
WINDOWS
WAS
A
HUGE
JUMP
BASED
ON
DECADES
OLD
TECHNOLOGY
INITIALLY
DEVELOPED
AT
XEROX
THEN
MACS
WINDOWS
RELEASED
IN
IMPROVED
TREMENDOUSLY
THE
STATE
OF
THE
AFFAIRS
FOR
MS
BUT
STILL
UNRELIABLE
WINDOWS
NT
APPROACHES
UNIX
DISTRIBUTIONS
WITH
MORE
USER
FRIENDLY
INTERFACE
CREATED
AT
AT
T
RE
WRITTEN
IMPROVED
BY
BERKELEY
ATT
HAD
MAJORITY
CONTROL
AND
GOOD
SUPPORT
RELIABLE
OS
OSF
OPEN
SW
FOUNDATION
NOW
OPEN
GROUP
IS
A
CONSORTIUM
OF
SEVERAL
COMPANIES
TO
STANDARDIZE
UNIX
DIFFERENT
SUBGROUPS
SYSCALLS
SHELLS
RT
ETC
STANDARDIZATION
IS
WITH
RESPECT
TO
INTERFACES
AND
NOT
IMPLEMENTATION
OF
PRIMITIVES
IMPLN
IS
LEFT
TO
THE
IMPLR
MODERN
APPLICATIONS
ARE
TIME
CONSTRAINED
TEL
VIDEO
ETC
REAL
TIME
PLAYING
AN
INCREASINGLY
IMPORTANT
ROLE
INTERFACE
CAN
BE
DONE
AT
ANY
LEVEL
DEPENDS
ON
LEVEL
OF
SECURITY
OF
OS
INTERFACE
WITH
THE
LOWER
LEVEL
LAYER
GETS
TRANSLATED
MACHINE
DEPENDENT
LANGUAGE
USED
FOR
ACCESSING
HARDWARE
MAIN
ADVANTAGE
OF
DIRECT
RESOURCE
ACCESS
IS
EFFICIENCY
MAIN
ADVANTAGE
OF
INDIRECT
ACCESS
IS
PORTABILITY
COMPLETELY
LAYERED
OS
WHY
OR
WHY
NOT
CONTROLS
AND
MANAGES
RESOURCES
DISKS
MEMORY
CPU
SENDS
RECEIVES
CONTROL
COMMANDS
AND
DATA
ALLOWS
MULTIPROGRAMMING
SEVERAL
PROGRAMS
AT
THE
SAME
TIME
IN
THE
SAME
RESOURCE
CARRIES
OUT
COMMUNICATION
BETWEEN
PROCESSES
INTER
AND
INTRA
PROCESSOR
MANAGES
INTERRUPT
HANDLERS
FOR
HW
AND
SW
INTERRUPTS
PROVIDES
PROTECTION
AND
SECURITY
TO
PROCESSES
PRIORITIZES
REQUESTS
AND
MANAGES
MULTIPLE
RESOURCES
IN
A
SINGLE
MACHINE
EG
MULTIPROCESSORS
OR
CPU
IO
REQS
OS
MANAGES
RESOURCES
INCLUDING
MANAGEMENT
OF
PROCESSES
CREATION
DELETION
SUSPENSION
COMM
SYNCH
MAIN
MEMORY
USAGE
ALLOC
DE
ALLOC
WHICH
PROCESSES
GET
IT
STORAGE
DISK
SCHEDULING
ALLOC
DE
ALLOC
SWAPPING
FILES
IO
INTERFACES
AND
DEVICES
EG
KEYBOARD
CACHING
MEMORY
PROTECTION
AUTHORIZATION
FILE
AND
MEMORY
PROTECTION
ETC
INTERPROCESS
COMMUNICATION
INTRA
AND
INTER
MACHINES
COMMAND
INTERPRETATION
SHELLS
TO
XLATE
USER
TO
OS
TYPICALLY
INCLUDES
THE
USER
INTERFACE
THAT
THE
OS
USES
OS
STRUCTURE
HARDWARE
AT
THE
BOTTOM
LAYER
ACCESSING
THE
LOWER
LAYER
THRU
THE
HIGHER
LAYERS
DOS
PROGRAMS
CAN
ACCESS
HW
UNIX
HAS
CONTROLLERS
AND
DEV
DRIVERS
DD
CONTROLLING
DEVICES
SYSTEM
CALLS
ARE
THE
INTERFACE
BETWEEN
USER
AND
OS
DDS
LIBRARIES
AND
SYSTEM
PROGRAMS
INVOKE
TYPICAL
DOS
TYPICAL
UNIX
OS
STRUCTURE
INTERFACE
CAN
BE
DONE
AT
ANY
LEVEL
DEPENDS
ON
SECURITY
MACHINE
DEPENDENT
LANGUAGE
USED
FOR
ACCESSING
HW
MAIN
ADVANTAGE
OF
DIRECT
RESOURCE
ACCESS
IS
EFFICIENCY
LESS
LAYERS
MEANS
LESS
OVERHEAD
IE
BETTER
PERFORMANCE
MAIN
ADVANTAGE
OF
INDIRECT
ACCESS
SYSCALL
IS
PORTABILITY
MODULAR
APPROACHES
IND
ACCESS
HAVE
LESS
FLEXIBILITY
SINCE
APPLS
ONLY
ACCESS
HW
THRU
LIBRARIES
AND
LAYERING
MEANS
THAT
ONE
LEVEL
IS
DEFINED
IN
TERMS
OF
THE
LEVEL
BELOW
LEVEL
IS
THE
HW
LEVEL
N
IS
THE
USER
APPLS
MODULAR
APPROACH
CREATE
WELL
DEFINED
INTERFACES
BETWEEN
ANY
TWO
LAYERS
CREATE
WELL
DEFINED
PROPERTIES
OF
EACH
LAYER
ATTEMPT
TO
DECREASE
THE
NUMBER
OF
LAYERS
TO
IMPROVE
EFFICIENCY
AND
PERFORMANCE
THE
FINAL
GOAL
IS
TO
MAKE
THE
OS
FLEXIBLE
AND
EFFICIENT
CREATE
THE
LAYERS
SUCH
THAT
EACH
USER
PERCEIVES
THE
MACHINE
AS
BELONGING
SOLELY
TO
HIMSELF
OR
HERSELF
THIS
IS
THE
CONCEPT
OF
A
VIRTUAL
MACHINE
WHICH
ALLOWS
EACH
USER
TO
AVOID
THINKING
ABOUT
OTHERS
PROCESSES
LANGUAGE
SYSTEM
CALLS
ARE
THE
INTERFACE
BETWEEN
USER
AND
OS
ACCESS
TO
THE
RESOURCES
IS
DONE
THROUGH
PRIVILEDGED
INSTRUCTIONS
FOR
PROTECTION
USER
APPLICATIONS
CANNOT
EXECUTE
IN
KERNEL
MODE
USER
APPLICATIONS
USER
LIBRARIES
THAT
INVOKE
SYSTEM
PROCEDURES
ARE
EXECUTED
TO
ACCESS
RESOURCES
VIA
PRIVILEDGED
INSTRUCTIONS
CALLED
FROM
THIS
WAY
NO
PROCESS
CAN
INFLUENCE
OTHER
EXECUTIONS
ON
PURPOSE
OR
BY
ACCIDENT
RESOURCE
PROTECTION
EXAMPLE
ACCOUNTING
PRIORITY
INFORMATION
SYSTEM
CALLS
CAN
BE
DIVIDED
INTO
CATEGORIES
PROCESS
CONTROL
FILE
MANIPULATION
DEVICE
MANIPULATION
INFOMATION
MAINTENANCE
COMMUNICATION
SPECIAL
PURPOSE
OSS
CAN
ALSO
HAVE
SPECIAL
PRIMITIVES
SPECIFICATION
OF
DEADLINES
PRIORITIES
PERIODICITY
OF
PROCESSES
SPECIFICATION
OF
PRECEDENCE
CONSTRAINTS
AND
OR
SYNCHRONIZATION
AMONG
PROCESSES
EXAMPLES
OF
LIBRARIES
ARE
LANGUAGE
CONSTRUCTS
TO
CARRY
OUT
FORMATTED
PRINTING
EXAMPLES
OF
ARE
PRIMITIVES
TO
CREATE
A
PROCESS
FOR
EXAMPLE
THE
READING
OF
BYTES
OF
A
FILE
THE
USER
DOES
FSCANF
THE
KERNEL
REQUESTS
A
BLOCK
OF
BYTES
FROM
THE
DEVICE
DRIVER
DD
WHICH
TALKS
TO
THE
CONTROLLER
OF
THE
DISK
TO
OBTAIN
A
BLOCK
OF
DATA
THE
BLOCK
IS
TRANSFERED
INTO
A
BUFFER
IN
THE
KERNEL
ADDRESS
SPACE
THE
KERNEL
THEN
PICKS
THE
BYTES
AND
COPIES
THEM
INTO
THE
USER
SPECIFIED
LOCATION
THIS
WAY
THE
KERNEL
ACCESSES
KERNEL
AND
USER
SPACE
BUT
THE
USER
ONLY
ACCESSES
USER
SPACE
SYSTEM
PROGRAMS
DO
NOT
INTERACT
DIRECTLY
WITH
RUNNING
USER
PROGRAMS
BUT
DEFINE
A
BETTER
ENVIRONMNT
FOR
THE
DEVELOPMENT
OF
APPLICATION
PROGRAMS
SYS
PROGRAMS
INCLUDE
COMPILERS
FILE
MANIPULATION
AND
MODIFICATION
EDITORS
LINKER
LOADERS
ETC
AN
IMPORTANT
ONE
IS
THE
COMMAND
INTERPRETER
OR
SHELL
WHICH
PARSES
USER
INPUT
INTERPRETS
IT
AND
EXECUTES
IT
SHELLS
CAN
EITHER
EXECUTE
THE
COMMAND
OR
INVOKE
OTHER
SYSTEM
PROGRAMS
OR
SYSTEM
CALLS
TO
DO
IT
TRADE
OFFS
PERFORMANCE
INCREASING
UPDATING
OF
COMMANDS
DIFFERENT
PROCESS
TYPES
HAVE
DIFFERENT
REQUIREMENTS
DIFFERENT
REQUIREMENTS
BEG
FOR
DIFFERENT
LANGUAGES
ASSEMBLY
LISP
PROLOG
JAVA
RT
C
ETC
REAL
TIME
LANGUAGES
INFORM
THE
OS
ABOUT
ITS
NEEDS
IN
ORDER
TO
ENHANCE
THE
PREDICTABILITY
OF
ITS
EXECUTION
DEADLINE
OF
A
THREAD
BY
WHEN
DO
I
NEED
THIS
DONE
PERIOD
OF
A
THREAD
WHAT
IS
THE
FREQUENCY
OF
THIS
TASK
RESOURCES
TO
BE
USED
AMOUNT
OF
MEMORY
OR
SEMAPHORES
PRECEDENCE
CONSTRAINTS
DOOR
MUST
BE
OPEN
FOR
A
ROBOT
TO
EXIT
PROCESSES
AND
THREADS
WHAT
IS
A
PROCESS
WHAT
IS
A
THREAD
WHAT
TYPES
A
PROGRAM
HAS
ONE
OR
MORE
LOCUS
OF
EXECUTION
EACH
EXECUTION
IS
CALLED
A
THREAD
OF
EXECUTION
THE
SET
OF
THREADS
COMPRISE
A
PROCESS
NOT
AN
OBJECT
OR
EXECUTABLE
FILES
MUST
BE
EXECUTING
EACH
THREAD
CONTAINS
AN
INSTRUCTION
POINTER
IP
A
REGISTER
WITH
NEXT
INSTRUCTION
A
STACK
FOR
TEMPORARY
DATA
EG
RETURN
ADDRESSES
PARAMETERS
A
DATA
AREA
FOR
DATA
DECLARED
GLOBALLY
AND
STATICALLY
A
PROCESS
THREAD
IS
ACTIVE
WHILE
A
PROGRAM
IS
NOT
HOW
TO
RUN
A
PROGRAM
THE
EXECUTABLE
CODE
IS
LOADED
ONTO
MEMORY
WHERE
SPACE
IS
ALLOCATED
TO
VARIABLES
WHAT
TYPES
OF
VARS
A
STACK
IS
ALLOCATED
TO
THE
PROCESS
FOR
WHAT
REGISTERS
ARE
UPDATED
WHICH
REGISTERS
CONTROL
OF
EXECUTION
GOES
TO
THE
PROCESS
HOW
PROCESS
RUNS
ONE
INSTRUCTION
AT
A
TIME
IN
A
CYCLE
FETCH
THE
INSTRUCTION
FROM
MEMORY
DECODE
THE
INSTRUCTION
UPDATE
THE
IP
EXECUTE
THE
INSTRUCTION
PROCESSES
AND
THREADS
REVISITED
ADDRESS
SPACE
MEMORY
RESERVED
FOR
A
PROCESS
A
HEAVYWEIGHT
PROCESS
HAS
A
SINGLE
LOCUS
OF
EXECUTION
PER
ADDRESS
SPACE
A
SINGLE
IP
A
SINGLE
PCB
A
PROCESS
CONTROL
BLOCK
PCB
CONTAINS
INFORMATION
PERTAINING
TO
THE
PROCESS
ITSELF
STATE
RUNNING
READY
ETC
REGISTERS
AND
FLAGS
STACK
POINTER
IP
ETC
RESOURCE
INFORMATION
MEMORY
CPU
USAGE
OPEN
FILES
ETC
PROCESS
ID
SECURITY
AND
PROTECTION
INFORMATION
ACCOUNTING
INFO
WHO
TO
BILL
LIMITS
SIMILAR
TO
RESOURCE
INFO
PROCESSES
AND
THREADS
CONT
THREAD
LIGHTWEIGHT
PROCESS
OF
A
PROCESS
SHARE
SOME
RESOURCES
E
G
MEMORY
OPEN
FILES
ETC
THREADS
HAVE
THEIR
OWN
STACK
IP
LOCAL
ADDRESS
SPACE
WITH
ONLY
A
SINGLE
PROCESS
IN
MEMORY
EASY
TO
MANAGE
FOR
EXAMPLE
IF
A
PROCESS
REQUESTS
IO
EG
READ
FROM
KEYBOARD
IT
JUST
STAYS
IN
THE
MEMORY
WAITING
SEVERAL
THREADS
OR
PROCESSES
COMPLICATE
THINGS
WHEN
IO
IS
REQUESTED
WHY
MAKE
OTHER
PROCESSES
WAIT
CONTEXT
SWITCHING
TAKES
PLACE
TAKE
A
WAITING
THREAD
OUT
OF
THE
CPU
AND
PUT
A
THREAD
THAT
IS
READY
IN
CPU
STATE
DIAGRAM
CREATE
PCB
AND
OTHER
RESOURCES
ARE
SETUP
END
RESOURCES
HELD
ARE
RETURNED
TO
THE
OS
FREED
CONTEXT
SWITCHING
SAVES
HW
CONTEXT
UPDATES
PCB
STATES
ARE
TYPICALLY
IMPLEMENTED
AS
QUEUES
LISTS
SETS
COMPLETE
STATE
DIAGRAM
MULTIPLE
THREADS
AND
PROCESSES
SEVERAL
PROBLEMS
WITH
MULTITASKING
FAIRNESS
IN
USAGE
OF
CPU
FAIRNESS
IN
USAGE
OF
OTHER
RESOURCES
COORDINATED
INPUT
AND
OUTPUT
LACK
OF
PROGRESS
DEADLOCKS
AND
LIVELOCKS
SYNCHRONIZATION
ACCESS
TO
THE
SAME
DATA
ITEM
BY
SEVERAL
PROCESSES
THREADS
TYPICAL
EXAMPLE
DEPOSITS
INTO
ACCOUNT
SYNCHRONIZATION
EXAMPLE
AT
TIME
T
REQUESTS
THE
WITHDRAWAL
OF
AND
GETS
PREEMPTED
AT
T
REQUESTS
THE
DEPOSIT
OF
T
READS
BALANCE
AT
T
ADDS
AT
T
WRITES
NEW
BALANCE
AT
T
RESUMES
AND
READS
BALANCE
AT
T
SUBTRACTS
AT
T
WRITES
NEW
BALANCE
WHAT
IF
ORDER
WAS
CHANGED
OTHER
COMBINATIONS
MORE
ON
SYNCHRONIZATION
SEMAPHORES
AND
PRIMITIVES
SERVE
TO
ACHIEVE
MUTUAL
EXCLUSION
AND
SYNCHRONIZED
ACCESS
TO
RESOURCES
CLEARLY
THERE
IS
MORE
DELAYS
ASSOCIATED
WITH
ATTEMP
TING
TO
ACCESS
A
RESOURCE
PROTECTED
BY
SEMAPHORES
NON
PREEMPTIVE
SCHEDULING
SOLVES
THAT
PROBLEM
AS
AN
ASIDE
WHICH
SCHEDULING
MECHANISM
IS
MORE
EFFICIENT
PREEMPTIVE
OR
NON
PREEMPTIVE
WITHIN
EACH
TYPE
OF
SCHEDULING
PR
OR
NON
PR
ONE
CAN
CHOOSE
WHICH
POLICY
HE
SHE
WANTS
TO
USE
USING
SEMAPHORES
WHEN
THREADS
USE
SEMAPHORES
THEY
ARE
ATTEMPTING
TO
RESERVE
A
RESOURCE
FOR
USAGE
ONLY
THE
THREADS
THAT
SUCCEED
ARE
ALLOWED
IN
THE
CS
IF
THERE
IS
A
THREAD
IN
THE
CS
ALREADY
OTHER
REQUESTING
THREADS
ARE
BLOCKED
WAITING
ON
AN
EVENT
WHEN
THE
THREAD
EXITS
THE
CS
THE
OS
UNBLOCKS
THE
WAITING
THREAD
TYPICALLY
DURING
THE
V
THE
NOW
UNBLOCKED
THREAD
BECOMES
READY
THE
OS
MAY
DECIDE
TO
INVOKE
THE
SCHEDULER
OR
NOT
TYPES
OF
SYNCHRONIZATION
THERE
ARE
BASIC
TYPES
OF
SYNC
MUTEX
BARRIER
SYNC
DEADLOCKS
WHEN
A
PROGRAM
IS
WRITTEN
CARELESSLY
IT
MAY
CAUSE
A
DEADLOCK
EACH
PROCESS
WILL
WAIT
FOR
THE
OTHER
PROCESS
INDEFINITELY
HOW
CAN
WE
DEAL
WITH
THESE
ABNORMALITIES
AVOIDANCE
PREVENTION
OR
DETECTION
AND
RESOLUTION
WHICH
ONE
IS
MORE
EFFICIENT
DINING
PHILOSOPHERS
THERE
WERE
SOME
HUNGRY
PHILOSOPHERS
AND
SOME
ANGRY
PHILOSOPHERS
NOT
THE
SAME
NOT
RASTAS
EACH
NEEDED
TWO
FORKS
EACH
SHARED
BOTH
HIS
HER
FORKS
POSSIBLE
DEADLOCK
SITUATION
HOW
IS
IT
POSSIBLE
TO
HAVE
A
LITERALLY
STARVATION
SITUATION
WHAT
IS
THE
BEST
SOLUTION
ROUND
ROBIN
FIFO
HOW
GOOD
ARE
FIFO
AND
RR
IMPORTANT
WHAT
IS
THE
METRIC
TO
JUDGE
A
SOLUTION
BY
LEAST
OVERHEAD
MINIMUM
STARVATION
HOW
TO
EVALUATE
FAIRNESS
ASSIGNMENT
NACHOS
SURVEY
QUESTIONS
DUE
WEDNESDAY
SEPT
WRITTEN
ANSWERS
HAND
IN
HARDCOPIES
NOT
CODE
GENERAL
QUESTIONS
EXPLAIN
RELATIONS
AMONG
THREE
FILES
UNDER
THE
CODE
DIRECTORY
MAKEFILE
MAKEFILE
COMMON
MAKEFILE
DEP
EXPLAIN
WHAT
EACH
FILE
DOES
EXPLAIN
WHAT
EACH
SUB
DIRECTORY
IS
FOR
UNDER
THE
CODE
DIRECTORY
THEY
DIRECTORIES
YOU
SHOULD
CONSIDER
ARE
BIN
FILESYS
MACHINE
NETWORK
TEST
THREADS
USERPROG
AND
VM
NACHOS
TIME
FIND
THE
VARIABLE
RESPONSIBLE
FOR
KEEP
TRACKING
THE
TIME
IN
NACHOS
WHAT
IS
IT
EXPLAIN
HOW
NACHOS
TIME
ADVANCES
IS
IT
THE
SAME
AS
THE
REAL
TIME
WHAT
IS
THE
PURPOSE
OF
THE
PRIVATE
VARIABLE
RANDOMIZE
IN
CLASS
TIMER
LIST
QUESTIONS
FIND
IN
NACHOS
CODE
AN
EXAMPLE
OF
CONSTRUCTING
A
LIST
OBJECT
USING
THE
LIST
CLASS
DEFINED
IN
LIST
CC
H
DESRIBE
HOW
THE
OBJECT
IS
INSTANTIATED
WHETHER
IT
IS
SORTED
LIST
OR
NON
SORTED
LIST
STUDY
LIST
H
CC
HOW
IS
THE
LIST
SORTED
BY
SORTEDINSERT
WRITE
THE
CODE
NO
PROGRAMMING
NEEDED
FOR
INSTANTIATING
A
LIST
OBJECT
THAT
CAN
SORT
ITEMS
IN
THE
INCREASING
ORDER
OF
INTEGER
NUMBER
USING
THE
LIST
CLASS
MAIN
WHAT
DOES
THE
RS
FLAG
DO
WHAT
DOES
THE
D
FLAG
DO
HOW
DO
WE
ENABLE
ALL
FLAGS
BESIDES
WRITING
THEM
ALL
OUT
EXPLAIN
DEBUG
T
ENTERING
MAIN
AT
LINE
OF
MAIN
CC
EXPLAIN
IFDEF
THREAD
THREADTEST
ENDIF
AT
LINE
OF
MAIN
CC
RUN
NACHOS
UNDER
THE
THREADS
DIRECTORY
IT
WILL
PRINT
OUT
THREAD
LOOPED
TIMES
THREAD
LOOPED
TIMES
ETC
GIVE
NAMES
OF
THREAD
AND
THREAD
I
E
WHAT
ARE
THREAD
AND
THREAD
THREAD
QUESTIONS
WHAT
ARE
THE
PCB
CONTENTS
IN
NACHOS
EXPLAIN
THE
FUNCTIONALITIES
OF
THREAD
YIELD
AND
THREAD
SLEEP
DIFFERENCES
HOW
DO
THREAD
RESTOREUSERSTATE
AND
ADDRESSSPACE
RESTORESTATE
DIFFER
SCHEDULER
QUESTIONS
WHAT
DOES
CURRENTTHREAD
SPACE
RESTORESTATE
DO
WHAT
IS
THE
SCOPE
OF
THE
VARIABLE
CURRENTTHREAD
WHAT
IS
THE
NAME
OF
READY
QUEUE
IN
NACHOS
HOW
SCHEDULER
READYTORUN
AND
SCHEDULER
RUN
DIFFER
IN
FUNCTIONALITY
SWITCH
QUESTIONS
WHAT
INFORMATION
DO
WE
NEED
TO
SAVE
FOR
A
CONTEXT
SWITCH
FOR
THE
DEC
MIPS
EXPLAIN
IN
A
BRIEF
PARAGRAPH
WHAT
HAPPENS
IN
SWITCH
SYNCH
QUESTIONS
THE
IMPLEMENTATION
OF
SEMAPHORE
IN
SYNCH
CC
HAS
A
WHILE
LOOP
WHICH
IS
DIFFERENT
FROM
THE
ONE
IN
THE
TEXTBOOK
WHY
DO
WE
NEED
THIS
WHILE
LOOP
EXPLAIN
THE
FOLLOWING
CODE
IN
SYNCH
CC
INTSTATUS
OLDLEVEL
INTERRUPT
SETLEVEL
INTOFF
SOME
CODE
VOID
INTERRUPT
SETLEVEL
OLDLEVEL
STUDY
THE
CONSTRUCTOR
AND
DESTRUCTOR
FUNCTIONS
FOR
THE
SEMAPHORE
CLASS
A
QUEUE
IS
CREATED
AND
DELETED
IN
THE
RESPECTIVE
FUNCTIONS
WHY
DO
WE
NEED
A
QUEUE
I
E
WHAT
IS
THE
PURPOSE
OF
THE
QUEUE
WHAT
ARE
THE
ITEMS
STORED
IN
THE
QUEUE
SYSTEM
QUESTIONS
EXPLAIN
THE
USAGE
OF
TIMERINTERRUPTHANDLER
BY
NACHOS
HOW
CAN
WE
USE
TIMERINTERRUPTHANDLER
AS
AN
OPERATING
SYSTEM
CPU
SCHEDULER
WHAT
DOES
THE
FLAG
DO
WHERE
ARE
THE
DEBUG
FLAGS
FINALLY
STORED
THREADTEST
QUESTIONS
TRACE
SIMPLETEST
AND
THREADTEST
IN
THREADTEST
CC
WHO
CALLS
THREADTEST
WHAT
DOES
THE
FORK
FUNCTION
CALL
DO
IN
THE
LINE
T
FORK
SIMPLETHREAD
WHAT
DOES
THE
REPRESENT
HOW
DO
YOU
INSTANTIATE
A
MUTEX
SEMAPHORE
IN
THREADTEST
CC
WHAT
DOES
THREAD
YIELD
DO
WHAT
DOES
THREAD
SLEEP
DO
HOW
IS
THIS
FUNCTION
DIFFERENT
FROM
THREAD
YIELD
UTILITY
QUESTIONS
WHAT
IS
ENABLEFLAGS
WHAT
DOES
ASSERT
DO
IF
THE
CONDITION
IS
FALSE
CS
HTTP
PEOPLE
CS
PITT
EDU
MELHEM
COURSES
INDEX
HTML
CS
AND
CS
INTRODUCTION
TO
HIGH
PERFORMANCE
COMPUTING
SYSTEMS
SPRING
TUESDAYS
AND
THURSDAYS
FROM
PM
TO
PM
SENNOTT
SQUARE
PURPOSE
THIS
COURSE
IS
AN
INTRODUCTION
TO
THE
ARCHITECTURE
OF
AND
SOFTWARE
TECHNIQUES
FOR
PARALLEL
AND
HIGH
PERFORMANCE
COMPUTING
SYSTEMS
THE
CONTENT
INCLUDES
FUNDAMENTAL
ARCHITECTURE
ASPECTS
OF
SHAREDMEMORY
AND
DISTRIBUTEDMEMORY
SYSTEMS
AS
WELL
AS
PARADIGMS
ALGORITHMS
AND
LANGUAGES
USED
TO
PROGRAM
PARALLEL
SYSTEMS
STUDENTS
WILL
COMPLETE
A
NUMBER
OF
PROJECTS
DEMONSTRATING
SPECIFIC
APPLICATIONS
ON
PARALLEL
PROCESSING
SYSTEMS
TEXTBOOKS
ABOUT
OF
THE
MATERIAL
COVERED
WILL
BE
FROM
AN
INTRODUCTION
TO
PARALLEL
PROGRAMMING
BY
PETER
PACHECO
PUBLISHER
MORGAN
KAUFMAN
THE
REMAINING
WILL
BE
IN
LECTURE
SLIDES
AND
MATERIAL
PUBLICLY
AVAILABLE
ON
THE
WEB
PREREQUISITES
AND
OR
KNOWLEDGE
OF
PROGRAMMING
AND
FUNDAMENTALS
OF
COMPUTER
SYSTEMS
PROGRAMMING
ASSIGNMENTS
WILL
USE
THE
C
LANGUAGE
THE
COURSE
WILL
COVER
THE
FOLLOWING
TOPICS
CS
HTTP
PEOPLE
CS
PITT
EDU
MELHEM
COURSES
INDEX
HTML
INTRODUCTION
TO
PARALLEL
SYSTEMS
AN
INTRODUCTION
TO
CHIP
MULTIPROCESSOR
ARCHITECTURES
MODELS
OF
PARALLEL
PROCESSING
A
TASTE
OF
PARALLEL
ALGORITHMS
AND
PROGRAMS
INTERCONNECTION
NETWORKS
CACHE
COHERENCE
IN
SYMMETRIC
MULTIPROCESSORS
PROGRAMMING
USING
MULTIPLE
THREADS
PROGRAMMING
USING
THE
MESSAGE
PASSING
INTERFACE
MPI
PROGRAMMING
SHARED
MEMORY
MACHINES
OPENMP
PROGRAMMING
USING
UNIVERSAL
PARALLEL
C
UPC
GPU
ARCHITECTURES
AND
CUDA
REQUIREMENTS
AND
GRADING
TWO
EXAMS
HOMEWORKS
AND
PROJECTS
STUDENTS
WILL
DO
A
RESEARCH
PROJECT
GRAPHIC
PROCESSING
UNITS
GPU
HISTORY
OF
GPUS
VGA
IN
EARLY
A
MEMORY
CONTROLLER
AND
DISPLAY
GENERATOR
CONNECTED
TO
SOME
VIDEO
RAM
BY
VGA
CONTROLLERS
WERE
INCORPORATING
SOME
ACCELERATION
FUNCTIONS
IN
A
SINGLE
CHIP
GRAPHICS
PROCESSOR
INCORPORATED
ALMOST
EVERY
DETAIL
OF
THE
TRADITIONAL
HIGH
END
WORKSTATION
GRAPHICS
PIPELINE
PROCESSORS
ORIENTED
TO
GRAPHICS
TASKS
VERTEX
PIXEL
PROCESSING
SHADING
TEXTURE
MAPPING
RASTERIZATION
MORE
RECENTLY
PROCESSOR
INSTRUCTIONS
AND
MEMORY
HARDWARE
WERE
ADDED
TO
SUPPORT
GENERAL
PURPOSE
PROGRAMMING
LANGUAGES
OPENGL
A
STANDARD
SPECIFICATION
DEFINING
AN
API
FOR
WRITING
APPLICATIONS
THAT
PRODUCE
AND
COMPUTER
GRAPHICS
CUDA
COMPUTE
UNIFIED
DEVICE
ARCHITECTURE
A
SCALABLE
PARALLEL
PROGRAMMING
MODEL
AND
LANGUAGE
FOR
GPUS
BASED
ON
C
C
HISTORICAL
PC
ARCHITECTURE
CONTEMPORARY
PC
ARCHITECTURE
BASIC
UNIFIED
GPU
ARCHITECTURE
STREAMING
MULTIPROCESSOR
SPECIAL
FUNCTION
UNIT
ROP
RASTER
OPERTASTIONS
PIPELINE
TPC
TEXTURE
PROCESSING
CLUSTER
TUTORIAL
CUDA
CYRIL
ZELLER
NVIDIA
DEVELOPER
TECHNOLOGY
NOTE
THESE
SLIDES
ARE
TRUNCATED
FROM
A
LONGER
VERSION
WHICH
IS
PUBLICLY
AVAILABLE
ON
THE
WEB
ENTER
THE
GPU
GPU
GRAPHICS
PROCESSING
UNIT
CHIP
IN
COMPUTER
VIDEO
CARDS
PLAYSTATION
XBOX
ETC
TWO
MAJOR
VENDORS
NVIDIA
AND
ATI
NOW
AMD
NVIDIA
CORPORATION
ENTER
THE
GPU
GPUS
ARE
MASSIVELY
MULTITHREADED
MANYCORE
CHIPS
NVIDIA
TESLA
PRODUCTS
HAVE
UP
TO
SCALAR
PROCESSORS
OVER
CONCURRENT
THREADS
IN
FLIGHT
OVER
GFLOPS
SUSTAINED
PERFORMANCE
USERS
ACROSS
SCIENCE
ENGINEERING
DISCIPLINES
ARE
ACHIEVING
OR
BETTER
SPEEDUPS
ON
GPUS
CS
RESEARCHERS
CAN
USE
GPUS
AS
A
RESEARCH
PLATFORM
FOR
MANYCORE
COMPUTING
ARCH
PL
NUMERIC
NVIDIA
CORPORATION
ENTER
CUDA
CUDA
IS
A
SCALABLE
PARALLEL
PROGRAMMING
MODEL
AND
A
SOFTWARE
ENVIRONMENT
FOR
PARALLEL
COMPUTING
MINIMAL
EXTENSIONS
TO
FAMILIAR
C
C
ENVIRONMENT
HETEROGENEOUS
SERIAL
PARALLEL
PROGRAMMING
MODEL
NVIDIA
TESLA
GPU
ARCHITECTURE
ACCELERATES
CUDA
EXPOSE
THE
COMPUTATIONAL
HORSEPOWER
OF
NVIDIA
GPUS
ENABLE
GENERAL
PURPOSE
GPU
COMPUTING
CUDA
ALSO
MAPS
WELL
TO
MULTICORE
CPUS
NVIDIA
CORPORATION
CUDA
PROGRAMMING
MODEL
NVIDIA
CORPORATION
O
FOR
I
O
I
NU
I
I
I
CAN
CHOSE
ANY
SEED
HERE
I
IS
CHOSEN
I
ATTR
VOID
I
FOR
I
O
I
NU
I
I
NULL
I
HITS
DOUBLE
DOUBLE
HETEROGENEOUS
PROGRAMMING
CUDA
SERIAL
PROGRAM
WITH
PARALLEL
KERNELS
ALL
IN
C
SERIAL
C
CODE
EXECUTES
IN
A
HOST
THREAD
I
E
CPU
THREAD
PARALLEL
KERNEL
C
CODE
EXECUTES
IN
MANY
DEVICE
THREADS
ACROSS
MULTIPLE
PROCESSING
ELEMENTS
I
E
GPU
THREADS
NVI
KERNEL
MANY
CONCURRENT
THREADS
ONE
KERNEL
IS
EXECUTED
AT
A
TIME
ON
THE
DEVICE
MANY
THREADS
EXECUTE
EACH
KERNEL
EACH
THREAD
EXECUTES
THE
SAME
CODE
ON
DIFFERENT
DATA
BASED
ON
ITS
THREADID
CUDA
THREADS
MIGHT
BE
PHYSICAL
THREADS
AS
ON
NVIDIA
GPUS
GPU
THREAD
CREATION
AND
CONTEXT
SWITCHING
ARE
ESSENTIALLY
FREE
OR
VIRTUAL
THREADS
THREADID
NVIDIA
CORPORATION
E
G
CPU
CORE
MIGHT
EXECUTE
MULTIPLE
CUDA
THREADS
HIERARCHY
OF
CONCURRENT
THREADS
THREADS
ARE
GROUPED
INTO
THREAD
BLOCKS
KERNEL
GRID
OF
THREAD
BLOCKS
THREADID
THREAD
BLOCK
THREAD
BLOCK
THREAD
BLOCK
N
BY
DEFINITION
THREADS
IN
THE
SAME
BLOCK
MAY
SYNCHRONIZE
WITH
NV
BARRIERS
THREADS
WAIT
AT
THE
BARRIER
UNTIL
ALL
THREADS
IN
THE
SAME
BLOCK
REACH
THE
BARRIER
TRANSPARENT
SCALABILITY
THREAD
BLOCKS
CANNOT
SYNCHRONIZE
SO
THEY
CAN
RUN
IN
ANY
ORDER
CONCURRENTLY
OR
SEQUENTIALLY
THIS
INDEPENDENCE
GIVES
SCALABILITY
A
KERNEL
SCALES
ACROSS
ANY
NUMBER
OF
PARALLEL
CORES
CORE
DEVICE
NVIDIA
CORPORATION
IMPLICIT
BARRIER
BETWEEN
DEPENDENT
KERNELS
NBLOCKS
BLKSIZE
A
B
C
NBLOCKS
BLKSIZE
C
C
HETEROGENEOUS
MEMORY
MODEL
CUDAMEMCPY
NVIDIA
CORPORATION
KERNEL
MEMORY
ACCESS
PER
THREAD
PER
BLOCK
THREAD
REGISTERS
LOCAL
MEMORY
ON
CHIP
OFF
CHIP
UNCACHED
BLOCK
PER
DEVICE
KERNEL
KERNEL
ON
CHIP
SMALL
FAST
OFF
CHIP
LARGE
UNCACHED
PERSISTENT
ACROSS
KERNEL
LAUNCHES
KERNEL
I
O
NVIDIA
CORPORATION
PHYSICAL
MEMORY
LAYOUT
LOCAL
MEMORY
RESIDES
IN
DEVICE
DRAM
USE
REGISTERS
AND
SHARED
MEMORY
TO
MINIMIZE
LOCAL
MEMORY
USE
HOST
CAN
READ
AND
WRITE
GLOBAL
MEMORY
BUT
NOT
SHARED
MEMORY
HOST
DRAM
CPU
CHIPSET
DEVICE
DRAM
LOCAL
MEMORY
GLOBAL
MEMORY
NVIDIA
CORPORATION
SERIES
ARCHITECTURE
THREAD
PROCESSORS
EXECUTE
KERNEL
THREADS
MULTIPROCESSORS
EACH
CONTAINS
THREAD
PROCESSORS
ONE
DOUBLE
PRECISION
UNIT
SHARED
MEMORY
ENABLES
THREAD
COOPERATION
MULTIPROCESSOR
NVIDIA
CORPORATION
THREAD
PROCESSORS
EXECUTION
MODEL
SOFTWARE
HARDWARE
THREAD
THREAD
PROCESSOR
THREADS
ARE
EXECUTED
BY
THREAD
PROCESSORS
THREAD
BLOCKS
ARE
EXECUTED
ON
MULTIPROCESSORS
THREAD
BLOCKS
DO
NOT
MIGRATE
THREAD
BLOCK
MULTIPROCESSOR
SEVERAL
CONCURRENT
THREAD
BLOCKS
CAN
RESIDE
ON
ONE
MULTIPROCESSOR
LIMITED
BY
MULTIPROCESSOR
RESOURCES
SHARED
MEMORY
AND
REGISTER
FILE
GRID
DEVICE
A
KERNEL
IS
LAUNCHED
AS
A
GRID
OF
THREAD
BLOCKS
ONLY
ONE
KERNEL
CAN
EXECUTE
ON
A
DEVICE
AT
ONE
TIME
NVIDIA
CORPORATION
CUDA
PROGRAMMING
BASICS
PART
I
SOFTWARE
STACK
AND
MEMORY
MANAGEMENT
COMPILER
ANY
SOURCE
FILE
CONTAINING
LANGUAGE
EXTENSIONS
LIKE
MUST
BE
COMPILED
WITH
NVCC
NVCC
IS
A
COMPILER
DRIVER
INVOKES
ALL
THE
NECESSARY
TOOLS
AND
COMPILERS
LIKE
CUDACC
G
CL
NVCC
CAN
OUTPUT
EITHER
C
CODE
CPU
CODE
THAT
MUST
THEN
BE
COMPILED
WITH
THE
REST
OF
THE
APPLICATION
USING
ANOTHER
TOOL
PTX
OR
OBJECT
CODE
DIRECTLY
AN
EXECUTABLE
REQUIRES
LINKING
TO
RUNTIME
LIBRARY
CUDART
CORE
LIBRARY
CUDA
NVIDIA
CORPORATION
COMPILING
NVIDIA
CORPORATION
GPU
MEMORY
ALLOCATION
RELEASE
HOST
CPU
MANAGES
DEVICE
GPU
MEMORY
CUDAMALLOC
VOID
POINTER
NBYTES
CUDAMEMSET
VOID
POINTER
INT
VALUE
COUNT
CUDAFREE
VOID
POINTER
INT
N
INT
NBYTES
SIZEOF
INT
INT
CUDAMALLOC
VOID
NBYTES
CUDAMEMSET
NBYTES
CUDAFREE
NVIDIA
CORPORATION
DATA
COPIES
CUDAMEMCPY
VOID
DST
VOID
SRC
NBYTES
ENUM
CUDAMEMCPYKIND
DIRECTION
DIRECTION
SPECIFIES
LOCATIONS
HOST
OR
DEVICE
OF
SRC
AND
DST
BLOCKS
CPU
THREAD
RETURNS
AFTER
THE
COPY
IS
COMPLETE
DOESN
T
START
COPYING
UNTIL
PREVIOUS
CUDA
CALLS
COMPLETE
ENUM
CUDAMEMCPYKIND
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPYDEVICETOHOST
CUDAMEMCPYDEVICETODEVICE
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
HOST
DEVICE
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
HOST
DEVICE
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
DATA
MOVEMENT
EXAMPLE
INT
MAIN
VOID
INT
N
NBYTES
I
NBYTES
N
SIZEOF
FLOAT
FLOAT
MALLOC
NBYTES
FLOAT
MALLOC
NBYTES
CUDAMALLOC
VOID
NBYTES
CUDAMALLOC
VOID
NBYTES
FOR
I
I
N
I
I
F
I
CUDAMEMCPY
NBYTES
CUDAMEMCPYHOSTTODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETODEVICE
CUDAMEMCPY
NBYTES
CUDAMEMCPYDEVICETOHOST
FOR
I
I
N
I
ASSERT
I
I
FREE
FREE
CUDAFREE
CUDAFREE
RETURN
NVIDIA
CORPORATION
CUDA
PROGRAMMING
BASICS
PART
II
KERNELS
THREAD
HIERARCHY
THREADS
LAUNCHED
FOR
A
PARALLEL
SECTION
ARE
PARTITIONED
INTO
THREAD
BLOCKS
GRID
ALL
BLOCKS
FOR
A
GIVEN
LAUNCH
THREAD
BLOCK
IS
A
GROUP
OF
THREADS
THAT
CAN
SYNCHRONIZE
THEIR
EXECUTION
COMMUNICATE
VIA
SHARED
MEMORY
NVIDIA
CORPORATION
EXECUTING
CODE
ON
THE
GPU
KERNELS
ARE
C
FUNCTIONS
WITH
SOME
RESTRICTIONS
CANNOT
ACCESS
HOST
MEMORY
MUST
HAVE
VOID
RETURN
TYPE
NO
VARIABLE
NUMBER
OF
ARGUMENTS
VARARGS
NOT
RECURSIVE
NO
STATIC
VARIABLES
FUNCTION
ARGUMENTS
AUTOMATICALLY
COPIED
FROM
HOST
TO
DEVICE
NVIDIA
CORPORATION
FUNCTION
QUALIFIERS
KERNELS
DESIGNATED
BY
FUNCTION
QUALIFIER
GLOBAL
FUNCTION
CALLED
FROM
HOST
AND
EXECUTED
ON
DEVICE
MUST
RETURN
VOID
OTHER
CUDA
FUNCTION
QUALIFIERS
DEVICE
FUNCTION
CALLED
FROM
DEVICE
AND
RUN
ON
DEVICE
CANNOT
BE
CALLED
FROM
HOST
CODE
HOST
FUNCTION
CALLED
FROM
HOST
AND
EXECUTED
ON
HOST
DEFAULT
HOST
AND
DEVICE
QUALIFIERS
CAN
BE
COMBINED
TO
GENERATE
BOTH
CPU
AND
GPU
CODE
NVIDIA
CORPORATION
LAUNCHING
KERNELS
MODIFIED
C
FUNCTION
CALL
SYNTAX
KERNEL
DG
DB
EXECUTION
CONFIGURATION
DG
DIMENSION
AND
SIZE
OF
GRID
IN
BLOCKS
TWO
DIMENSIONAL
X
AND
Y
BLOCKS
LAUNCHED
IN
THE
GRID
DG
X
DG
Y
DB
DIMENSION
AND
SIZE
OF
BLOCKS
IN
THREADS
THREE
DIMENSIONAL
X
Y
AND
Z
THREADS
PER
BLOCK
DB
X
DB
Y
DB
Z
UNSPECIFIED
FIELDS
INITIALIZE
TO
NVIDIA
CORPORATION
MORE
ON
THREAD
AND
BLOCK
IDS
THREADS
AND
BLOCKS
HAVE
IDS
SO
EACH
THREAD
CAN
DECIDE
WHAT
DATA
TO
WORK
ON
BLOCK
ID
OR
THREAD
ID
OR
HOST
DEVICE
GRID
BLOCK
BLOCK
GRID
BLOCK
BLOCK
BLOCK
BLOCK
SIMPLIFIES
MEMORY
ADDRESSING
WHEN
BLOCK
PROCESSING
MULTIDIMENSIONAL
DATA
IMAGE
PROCESSING
SOLVING
PDES
ON
VOLUMES
NVIDIA
CORPORATION
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
EXECUTION
CONFIGURATION
EXAMPLES
EQUIVALENT
ASSIGNMENT
USING
CONSTRUCTOR
FUNCTIONS
KERNEL
NVIDIA
CORPORATION
CUDA
BUILT
IN
DEVICE
VARIABLES
ALL
GLOBAL
AND
DEVICE
FUNCTIONS
HAVE
ACCESS
TO
THESE
AUTOMATICALLY
DEFINED
VARIABLES
GRIDDIM
DIMENSIONS
OF
THE
GRID
IN
BLOCKS
AT
MOST
BLOCKDIM
DIMENSIONS
OF
THE
BLOCK
IN
THREADS
BLOCKIDX
BLOCK
INDEX
WITHIN
THE
GRID
THREADIDX
THREAD
INDEX
WITHIN
THE
BLOCK
NVIDIA
CORPORATION
UNIQUE
THREAD
IDS
BUILT
IN
VARIABLES
ARE
USED
TO
DETERMINE
UNIQUE
THREAD
IDS
MAP
FROM
LOCAL
THREAD
ID
THREADIDX
TO
A
GLOBAL
ID
WHICH
CAN
BE
USED
AS
ARRAY
INDICES
BLOCKIDX
X
BLOCKDIM
X
THREADIDX
X
BLOCKIDX
X
BLOCKDIM
X
THREADIDX
X
NVIDIA
CORPORATION
MINIMAL
KERNELS
GLOBAL
VOID
KERNEL
INT
A
INT
IDX
BLOCKIDX
X
BLOCKDIM
X
THREADIDX
X
A
IDX
OUTPUT
GLOBAL
VOID
KERNEL
INT
A
INT
IDX
BLOCKIDX
X
BLOCKDIM
X
THREADIDX
X
A
IDX
BLOCKIDX
X
OUTPUT
GLOBAL
VOID
KERNEL
INT
A
INT
IDX
BLOCKIDX
X
BLOCKDIM
X
THREADIDX
X
A
IDX
THREADIDX
X
OUTPUT
NVIDIA
CORPORATION
INCREMENT
ARRAY
EXAMPLE
CPU
PROGRAM
VOID
INT
A
INT
CUDA
PROGRAM
N
GLOBAL
VOID
INT
INT
N
INT
IDX
INT
IDX
BLOCKIDX
X
BLOCKDIM
X
FOR
IDX
IDX
N
IDX
IF
IDX
N
THREADIDX
X
A
IDX
A
IDX
IDX
IDX
VOID
MAIN
VOID
MAIN
A
N
DIMBLOCK
BLOCKSIZE
DIMGRID
CEIL
N
FLOAT
BLOCKSIZE
DIMGRID
DIMBLOCK
N
NVIDIA
CORPORATION
HOST
SYNCHRONIZATION
ALL
KERNEL
LAUNCHES
ARE
ASYNCHRONOUS
CONTROL
RETURNS
TO
CPU
IMMEDIATELY
KERNEL
EXECUTES
AFTER
ALL
PREVIOUS
CUDA
CALLS
HAVE
COMPLETED
CUDAMEMCPY
IS
SYNCHRONOUS
CONTROL
RETURNS
TO
CPU
AFTER
COPY
COMPLETES
COPY
STARTS
AFTER
ALL
PREVIOUS
CUDA
CALLS
HAVE
COMPLETED
CUDATHREADSYNCHRONIZE
BLOCKS
UNTIL
ALL
PREVIOUS
CUDA
CALLS
COMPLETE
NVIDIA
CORPORATION
HOST
SYNCHRONIZATION
EXAMPLE
COPY
DATA
FROM
HOST
TO
DEVICE
CUDAMEMCPY
NUMBYTES
CUDAMEMCPYHOSTTODEVICE
EXECUTE
THE
KERNEL
CEIL
N
FLOAT
BLOCKSIZE
BLOCKSIZE
N
RUN
INDEPENDENT
CPU
CODE
COPY
DATA
FROM
DEVICE
BACK
TO
HOST
CUDAMEMCPY
NUMBYTES
CUDAMEMCPYDEVICETOHOST
NVIDIA
CORPORATION
VARIABLE
QUALIFIERS
GPU
CODE
DEVICE
STORED
IN
GLOBAL
MEMORY
LARGE
HIGH
LATENCY
NO
CACHE
ALLOCATED
WITH
CUDAMALLOC
DEVICE
ACCESSIBLE
BY
ALL
THREADS
LIFETIME
APPLICATION
SHARED
QUALIFIER
IMPLIED
STORED
IN
ON
CHIP
SHARED
MEMORY
VERY
LOW
LATENCY
SPECIFIED
BY
EXECUTION
CONFIGURATION
OR
AT
COMPILE
TIME
ACCESSIBLE
BY
ALL
THREADS
IN
THE
SAME
THREAD
BLOCK
LIFETIME
THREAD
BLOCK
UNQUALIFIED
VARIABLES
SCALARS
AND
BUILT
IN
VECTOR
TYPES
ARE
STORED
IN
REGISTERS
ARRAYS
MAY
BE
IN
REGISTERS
OR
LOCAL
MEMORY
NVIDIA
CORPORATION
GPU
THREAD
SYNCHRONIZATION
VOID
SYNCTHREADS
SYNCHRONIZES
ALL
THREADS
IN
A
BLOCK
GENERATES
BARRIER
SYNCHRONIZATION
INSTRUCTION
NO
THREAD
CAN
PASS
THIS
BARRIER
UNTIL
ALL
THREADS
IN
THE
BLOCK
REACH
IT
USED
TO
AVOID
RAW
WAR
WAW
HAZARDS
WHEN
ACCESSING
SHARED
MEMORY
ALLOWED
IN
CONDITIONAL
CODE
ONLY
IF
THE
CONDITIONAL
IS
UNIFORM
ACROSS
THE
ENTIRE
THREAD
BLOCK
NVIDIA
CORPORATION
GPU
ATOMIC
INTEGER
OPERATIONS
REQUIRES
HARDWARE
WITH
COMPUTE
CAPABILITY
COMPUTE
CAPABILITY
COMPUTE
CAPABILITY
COMPUTE
CAPABILITY
ATOMIC
OPERATIONS
ON
INTEGERS
IN
GLOBAL
MEMORY
ASSOCIATIVE
OPERATIONS
ON
SIGNED
UNSIGNED
INTS
ADD
SUB
MIN
MAX
AND
OR
XOR
INCREMENT
DECREMENT
EXCHANGE
COMPARE
AND
SWAP
ATOMIC
OPERATIONS
ON
INTEGERS
IN
SHARED
MEMORY
REQUIRES
COMPUTE
CAPABILITY
OPTIMIZING
CUDA
EXECUTION
MODEL
SOFTWARE
HARDWARE
THREAD
THREAD
PROCESSOR
THREADS
ARE
EXECUTED
BY
THREAD
PROCESSORS
THREAD
BLOCKS
ARE
EXECUTED
ON
MULTIPROCESSORS
THREAD
BLOCKS
DO
NOT
MIGRATE
THREAD
BLOCK
MULTIPROCESSOR
SEVERAL
CONCURRENT
THREAD
BLOCKS
CAN
RESIDE
ON
ONE
MULTIPROCESSOR
LIMITED
BY
MULTIPROCESSOR
RESOURCES
SHARED
MEMORY
AND
REGISTER
FILE
GRID
DEVICE
A
KERNEL
IS
LAUNCHED
AS
A
GRID
OF
THREAD
BLOCKS
ONLY
ONE
KERNEL
CAN
EXECUTE
ON
A
DEVICE
AT
ONE
TIME
NVIDIA
CORPORATION
WARPS
AND
HALF
WARPS
THREAD
BLOCK
THREADS
THREADS
THREADS
WARPS
MULTIPROCESSOR
A
THREAD
BLOCK
CONSISTS
OF
THREAD
WARPS
A
WARP
IS
EXECUTED
PHYSICALLY
IN
PARALLEL
SIMD
ON
A
MULTIPROCESSOR
HALF
WARPS
DEVICE
MEMORY
A
HALF
WARP
OF
THREADS
CAN
COORDINATE
GLOBAL
MEMORY
ACCESSES
INTO
A
SINGLE
TRANSACTION
NVIDIA
CORPORATION
MEMORY
ARCHITECTURE
HOST
CPU
CHIPSET
DEVICE
DRAM
LOCAL
GLOBAL
CONSTANT
TEXTURE
NVIDIA
CORPORATION
MEMORY
ARCHITECTURE
MEMORY
LOCATION
CACHED
ACCESS
SCOPE
LIFETIME
REGISTER
ON
CHIP
N
A
R
W
ONE
THREAD
THREAD
LOCAL
OFF
CHIP
NO
R
W
ONE
THREAD
THREAD
SHARED
ON
CHIP
N
A
R
W
ALL
THREADS
IN
A
BLOCK
BLOCK
GLOBAL
OFF
CHIP
NO
R
W
ALL
THREADS
HOST
APPLICATION
CONSTANT
OFF
CHIP
YES
R
ALL
THREADS
HOST
APPLICATION
TEXTURE
OFF
CHIP
YES
R
ALL
THREADS
HOST
APPLICATION
NVIDIA
CORPORATION
OUTLINE
OVERVIEW
HARDWARE
MEMORY
OPTIMIZATIONS
DATA
TRANSFERS
BETWEEN
HOST
AND
DEVICE
DEVICE
MEMORY
OPTIMIZATIONS
EXECUTION
CONFIGURATION
OPTIMIZATIONS
INSTRUCTION
OPTIMIZATIONS
SUMMARY
NVIDIA
CORPORATION
HOST
DEVICE
DATA
TRANSFERS
DEVICE
TO
HOST
MEMORY
BANDWIDTH
MUCH
LOWER
THAN
DEVICE
TO
DEVICE
BANDWIDTH
GB
PEAK
PCI
E
GEN
VS
GB
PEAK
GTX
MINIMIZE
TRANSFERS
INTERMEDIATE
DATA
CAN
BE
ALLOCATED
OPERATED
ON
AND
DEALLOCATED
WITHOUT
EVER
COPYING
THEM
TO
HOST
MEMORY
GROUP
TRANSFERS
ONE
LARGE
TRANSFER
MUCH
BETTER
THAN
MANY
SMALL
ONES
NVIDIA
CORPORATION
PAGE
LOCKED
DATA
TRANSFERS
CUDAMALLOCHOST
ALLOWS
ALLOCATION
OF
PAGE
LOCKED
PINNED
HOST
MEMORY
ENABLES
HIGHEST
CUDAMEMCPY
PERFORMANCE
GB
ON
PCI
E
GB
ON
PCI
E
SEE
THE
BANDWIDTHTEST
CUDA
SDK
SAMPLE
USE
WITH
CAUTION
ALLOCATING
TOO
MUCH
PAGE
LOCKED
MEMORY
CAN
REDUCE
OVERALL
SYSTEM
PERFORMANCE
TEST
YOUR
SYSTEMS
AND
APPS
TO
LEARN
THEIR
LIMITS
NVIDIA
CORPORATION
OVERLAPPING
DATA
TRANSFERS
AND
COMPUTATION
ASYNC
AND
STREAM
APIS
ALLOW
OVERLAP
OF
OR
DATA
TRANSFERS
WITH
COMPUTATION
CPU
COMPUTATION
CAN
OVERLAP
DATA
TRANSFERS
ON
ALL
CUDA
CAPABLE
DEVICES
KERNEL
COMPUTATION
CAN
OVERLAP
DATA
TRANSFERS
ON
DEVICES
WITH
CONCURRENT
COPY
AND
EXECUTION
ROUGHLY
COMPUTE
CAPABILITY
STREAM
SEQUENCE
OF
OPERATIONS
THAT
EXECUTE
IN
ORDER
ON
GPU
OPERATIONS
FROM
DIFFERENT
STREAMS
CAN
BE
INTERLEAVED
STREAM
ID
USED
AS
ARGUMENT
TO
ASYNC
CALLS
AND
KERNEL
LAUNCHES
NVIDIA
CORPORATION
ASYNCHRONOUS
DATA
TRANSFERS
ASYNCHRONOUS
HOST
DEVICE
MEMORY
COPY
RETURNS
CONTROL
IMMEDIATELY
TO
CPU
CUDAMEMCPYASYNC
DST
SRC
SIZE
DIR
STREAM
REQUIRES
PINNED
HOST
MEMORY
ALLOCATED
WITH
CUDAMALLOCHOST
OVERLAP
CPU
COMPUTATION
WITH
DATA
TRANSFER
DEFAULT
STREAM
CUDAMEMCPYASYNC
SIZE
CUDAMEMCPYHOSTTODEVICE
KERNEL
GRID
BLOCK
CPUFUNCTION
OVERLAPPED
NVIDIA
CORPORATION
OVERLAPPING
KERNEL
AND
DATA
TRANSFER
REQUIRES
CONCURRENT
COPY
AND
EXECUTE
DEVICEOVERLAP
FIELD
OF
A
CUDADEVICEPROP
VARIABLE
KERNEL
AND
TRANSFER
USE
DIFFERENT
NON
ZERO
STREAMS
A
CUDA
CALL
TO
STREAM
BLOCKS
UNTIL
ALL
PREVIOUS
CALLS
COMPLETE
AND
CANNOT
BE
OVERLAPPED
EXAMPLE
CUDASTREAMCREATE
CUDASTREAMCREATE
CUDAMEMCPYASYNC
DST
SRC
SIZE
DIR
KERNEL
GRID
BLOCK
OVERLAPPED
NVIDIA
CORPORATION
GPU
CPU
SYNCHRONIZATION
CONTEXT
BASED
CUDATHREADSYNCHRONIZE
BLOCKS
UNTIL
ALL
PREVIOUSLY
ISSUED
CUDA
CALLS
FROM
A
CPU
THREAD
COMPLETE
STREAM
BASED
CUDASTREAMSYNCHRONIZE
STREAM
BLOCKS
UNTIL
ALL
CUDA
CALLS
ISSUED
TO
GIVEN
STREAM
COMPLETE
CUDASTREAMQUERY
STREAM
INDICATES
WHETHER
STREAM
IS
IDLE
RETURNS
CUDASUCCESS
CUDAERRORNOTREADY
DOES
NOT
BLOCK
CPU
THREAD
NVIDIA
CORPORATION
GPU
CPU
SYNCHRONIZATION
STREAM
BASED
USING
EVENTS
EVENTS
CAN
BE
INSERTED
INTO
STREAMS
CUDAEVENTRECORD
EVENT
STREAM
EVENT
IS
RECORDED
THEN
GPU
REACHES
IT
IN
A
STREAM
RECORDED
ASSIGNED
A
TIMESTAMP
GPU
CLOCKTICK
USEFUL
FOR
TIMING
CUDAEVENTSYNCHRONIZE
EVENT
BLOCKS
UNTIL
GIVEN
EVENT
IS
RECORDED
CUDAEVENTQUERY
EVENT
INDICATES
WHETHER
EVENT
HAS
RECORDED
RETURNS
CUDASUCCESS
CUDAERRORNOTREADY
DOES
NOT
BLOCK
CPU
THREAD
NVIDIA
CORPORATION
OUTLINE
OVERVIEW
HARDWARE
MEMORY
OPTIMIZATIONS
DATA
TRANSFERS
BETWEEN
HOST
AND
DEVICE
DEVICE
MEMORY
OPTIMIZATIONS
MEASURING
PERFORMANCE
EFFECTIVE
BANDWIDTH
COALESCING
SHARED
MEMORY
TEXTURES
EXECUTION
CONFIGURATION
OPTIMIZATIONS
INSTRUCTION
OPTIMIZATIONS
SUMMARY
NVIDIA
CORPORATION
THEORETICAL
BANDWIDTH
DEVICE
BANDWIDTH
OF
GTX
DDR
GB
MEMORY
CLOCK
HZ
MEMORY
INTERFACE
BYTES
SPECS
REPORT
GB
USE
B
GB
CONVERSION
RATHER
THAN
WHICHEVER
YOU
USE
BE
CONSISTENT
NVIDIA
CORPORATION
EFFECTIVE
BANDWIDTH
EFFECTIVE
BANDWIDTH
FOR
COPYING
ARRAY
OF
N
FLOATS
N
B
ELEMENT
TIME
IN
SECS
GB
ARRAY
SIZE
BYTES
READ
AND
WRITE
B
GB
OR
NVIDIA
CORPORATION
OUTLINE
OVERVIEW
HARDWARE
MEMORY
OPTIMIZATIONS
DATA
TRANSFERS
BETWEEN
HOST
AND
DEVICE
DEVICE
MEMORY
OPTIMIZATIONS
MEASURING
PERFORMANCE
EFFECTIVE
BANDWIDTH
COALESCING
SHARED
MEMORY
TEXTURES
EXECUTION
CONFIGURATION
OPTIMIZATIONS
INSTRUCTION
OPTIMIZATIONS
SUMMARY
NVIDIA
CORPORATION
COALESCING
GLOBAL
MEMORY
ACCESS
OF
OR
BIT
WORDS
BY
A
HALF
WARP
OF
THREADS
CAN
RESULT
IN
AS
FEW
AS
ONE
OR
TWO
TRANSACTION
IF
CERTAIN
ACCESS
REQUIREMENTS
ARE
MET
DEPENDS
ON
COMPUTE
CAPABILITY
AND
HAVE
STRICTER
ACCESS
REQUIREMENTS
EXAMPLES
FLOAT
BIT
DATA
GLOBAL
MEMORY
HALF
WARP
OF
THREADS
ALIGNED
SEGMENT
FLOATS
ALIGNED
SEGMENT
FLOATS
NVIDIA
CORPORATION
COALESCING
COMPUTE
CAPABILITY
AND
K
TH
THREAD
MUST
ACCESS
K
TH
WORD
IN
THE
SEGMENT
OR
K
TH
WORD
IN
CONTIGUOUS
SEGMENTS
FOR
BIT
WORDS
NOT
ALL
THREADS
NEED
TO
PARTICIPATE
COALESCES
TRANSACTION
OUT
OF
SEQUENCE
TRANSACTIONS
MISALIGNED
TRANSACTIONS
NVIDIA
CORPORATION
COALESCING
COMPUTE
CAPABILITY
AND
HIGHER
COALESCING
IS
ACHIEVED
FOR
ANY
PATTERN
OF
ADDRESSES
THAT
FITS
INTO
A
SEGMENT
OF
SIZE
FOR
BIT
WORDS
FOR
BIT
WORDS
FOR
AND
BIT
WORDS
SMALLER
TRANSACTIONS
MAY
BE
ISSUED
TO
AVOID
WASTED
BANDWIDTH
DUE
TO
UNUSED
WORDS
TRANSACTION
SEGMENT
TRANSACTIONS
AND
SEGMENTS
TRANSACTION
SEGMENT
NVIDIA
CORPORATION
COALESCING
EXAMPLES
EFFECTIVE
BANDWIDTH
OF
SMALL
KERNELS
THAT
COPY
DATA
EFFECTS
OF
OFFSET
AND
STRIDE
ON
PERFORMANCE
TWO
GPUS
GTX
COMPUTE
CAPABILITY
PEAK
BANDWIDTH
OF
GB
FX
COMPUTE
CAPABILITY
PEAK
BANDWIDTH
OF
GB
NVIDIA
CORPORATION
COALESCING
EXAMPLES
GLOBAL
VOID
OFFSETCOPY
FLOAT
ODATA
FLOAT
IDATA
INT
OFFSET
INT
XID
BLOCKIDX
X
BLOCKDIM
X
THREADIDX
X
OFFSET
ODATA
XID
IDATA
XID
NVIDIA
CORPORATION
OUTLINE
OVERVIEW
HARDWARE
MEMORY
OPTIMIZATIONS
DATA
TRANSFERS
BETWEEN
HOST
AND
DEVICE
DEVICE
MEMORY
OPTIMIZATIONS
MEASURING
PERFORMANCE
EFFECTIVE
BANDWIDTH
COALESCING
SHARED
MEMORY
TEXTURES
EXECUTION
CONFIGURATION
OPTIMIZATIONS
INSTRUCTION
OPTIMIZATIONS
SUMMARY
NVIDIA
CORPORATION
SHARED
MEMORY
HUNDRED
TIMES
FASTER
THAN
GLOBAL
MEMORY
CACHE
DATA
TO
REDUCE
GLOBAL
MEMORY
ACCESSES
THREADS
CAN
COOPERATE
VIA
SHARED
MEMORY
USE
IT
TO
AVOID
NON
COALESCED
ACCESS
STAGE
LOADS
AND
STORES
IN
SHARED
MEMORY
TO
RE
ORDER
NON
COALESCEABLE
ADDRESSING
NVIDIA
CORPORATION
MAXIMIZE
USE
OF
SHARED
MEMORY
SHARED
MEMORY
IS
HUNDREDS
OF
TIMES
FASTER
THAN
GLOBAL
MEMORY
THREADS
CAN
COOPERATE
VIA
SHARED
MEMORY
NOT
SO
VIA
GLOBAL
MEMORY
A
COMMON
WAY
OF
SCHEDULING
SOME
COMPUTATION
ON
THE
DEVICE
IS
TO
BLOCK
IT
UP
TO
TAKE
ADVANTAGE
OF
SHARED
MEMORY
PARTITION
THE
DATA
SET
INTO
DATA
SUBSETS
THAT
FIT
INTO
SHARED
MEMORY
HANDLE
EACH
DATA
SUBSET
WITH
ONE
THREAD
BLOCK
LOAD
THE
SUBSET
FROM
GLOBAL
MEMORY
TO
SHARED
MEMORY
SYNCTHREADS
PERFORM
THE
COMPUTATION
ON
THE
SUBSET
FROM
SHARED
MEMORY
EACH
THREAD
CAN
EFFICIENTLY
MULTI
PASS
OVER
ANY
DATA
ELEMENT
SYNCTHREADS
IF
NEEDED
COPY
RESULTS
FROM
SHARED
MEMORY
TO
GLOBAL
MEMORY
NVIDIA
CORPORATION
EXAMPLE
SQUARE
MATRIX
MULTIPLICATION
C
A
B
OF
SIZE
N
X
N
WITHOUT
BLOCKING
ONE
THREAD
HANDLES
ONE
ELEMENT
OF
C
A
AND
B
ARE
LOADED
N
TIMES
FROM
GLOBAL
MEMORY
WASTES
BANDWIDTH
POOR
BALANCE
OF
WORK
TO
BANDWIDTH
N
NVIDIA
CORPORATION
EXAMPLE
SQUARE
MATRIX
MULTIPLICATION
EXAMPLE
C
A
B
OF
SIZE
N
X
N
WITH
BLOCKING
ONE
THREAD
BLOCK
HANDLES
ONE
M
X
M
SUB
MATRIX
CSUB
OF
C
A
AND
B
ARE
ONLY
LOADED
N
M
TIMES
FROM
GLOBAL
MEMORY
MUCH
LESS
BANDWIDTH
MUCH
BETTER
BALANCE
OF
WORK
TO
BANDWIDTH
NVIDIA
CORPORATION
SHARED
MEMORY
ARCHITECTURE
MANY
THREADS
ACCESSING
MEMORY
THEREFORE
MEMORY
IS
DIVIDED
INTO
BANKS
SUCCESSIVE
BIT
WORDS
ASSIGNED
TO
SUCCESSIVE
BANKS
EACH
BANK
CAN
SERVICE
ONE
ADDRESS
PER
CYCLE
A
MEMORY
CAN
SERVICE
AS
MANY
SIMULTANEOUS
ACCESSES
AS
IT
HAS
BANKS
MULTIPLE
SIMULTANEOUS
ACCESSES
TO
A
BANK
RESULT
IN
A
BANK
CONFLICT
CONFLICTING
ACCESSES
ARE
SERIALIZED
BANK
BANK
BANK
BANK
BANK
BANK
BANK
BANK
BANK
NVIDIA
CORPORATION
BANK
ADDRESSING
EXAMPLES
NO
BANK
CONFLICTS
LINEAR
ADDRESSING
STRIDE
NO
BANK
CONFLICTS
RANDOM
PERMUTATION
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
BANK
BANK
BANK
BANK
BANK
BANK
BANK
BANK
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
BANK
BANK
BANK
BANK
BANK
BANK
BANK
BANK
THREAD
BANK
THREAD
BANK
NVIDIA
CORPORATION
BANK
ADDRESSING
EXAMPLES
WAY
BANK
CONFLICTS
LINEAR
ADDRESSING
STRIDE
WAY
BANK
CONFLICTS
LINEAR
ADDRESSING
STRIDE
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
BANK
BANK
BANK
BANK
BANK
BANK
BANK
BANK
BANK
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
THREAD
BANK
BANK
BANK
BANK
BANK
BANK
BANK
NVIDIA
CORPORATION
SHARED
MEMORY
BANK
CONFLICTS
SHARED
MEMORY
IS
AS
FAST
AS
REGISTERS
IF
THERE
ARE
NO
BANK
CONFLICTS
PROFILER
SIGNAL
REFLECTS
CONFLICTS
THE
FAST
CASE
IF
ALL
THREADS
OF
A
HALF
WARP
ACCESS
DIFFERENT
BANKS
THERE
IS
NO
BANK
CONFLICT
IF
ALL
THREADS
OF
A
HALF
WARP
READ
THE
IDENTICAL
ADDRESS
THERE
IS
NO
BANK
CONFLICT
BROADCAST
THE
SLOW
CASE
BANK
CONFLICT
MULTIPLE
THREADS
IN
THE
SAME
HALF
WARP
ACCESS
THE
SAME
BANK
MUST
SERIALIZE
THE
ACCESSES
COST
MAX
OF
SIMULTANEOUS
ACCESSES
TO
A
SINGLE
BANK
NVIDIA
CORPORATION
SHARED
MEMORY
EXAMPLE
TRANSPOSE
EACH
THREAD
BLOCK
WORKS
ON
A
TILE
OF
THE
MATRIX
NAÏVE
IMPLEMENTATION
EXHIBITS
STRIDED
ACCESS
TO
GLOBAL
MEMORY
IDATA
ODATA
ELEMENTS
TRANSPOSED
BY
A
HALF
WARP
OF
THREADS
NVIDIA
CORPORATION
COALESCING
THROUGH
SHARED
MEMORY
ACCESS
COLUMNS
OF
A
TILE
IN
SHARED
MEMORY
TO
WRITE
CONTIGUOUS
DATA
TO
GLOBAL
MEMORY
REQUIRES
SYNCTHREADS
SINCE
THREADS
ACCESS
DATA
IN
SHARED
MEMORY
STORED
BY
OTHER
THREADS
IDATA
ODATA
TILE
ELEMENTS
TRANSPOSED
BY
A
HALF
WARP
OF
THREADS
NVIDIA
CORPORATION
OUTLINE
OVERVIEW
HARDWARE
MEMORY
OPTIMIZATIONS
EXECUTION
CONFIGURATION
OPTIMIZATIONS
INSTRUCTION
OPTIMIZATIONS
SUMMARY
NVIDIA
CORPORATION
OCCUPANCY
THREAD
INSTRUCTIONS
ARE
EXECUTED
SEQUENTIALLY
SO
EXECUTING
OTHER
WARPS
IS
THE
ONLY
WAY
TO
HIDE
LATENCIES
AND
KEEP
THE
HARDWARE
BUSY
OCCUPANCY
NUMBER
OF
WARPS
RUNNING
CONCURRENTLY
ON
A
MULTIPROCESSOR
DIVIDED
BY
MAXIMUM
NUMBER
OF
WARPS
THAT
CAN
RUN
CONCURRENTLY
LIMITED
BY
RESOURCE
USAGE
REGISTERS
SHARED
MEMORY
NVIDIA
CORPORATION
BLOCKS
PER
GRID
HEURISTICS
OF
BLOCKS
OF
MULTIPROCESSORS
SO
ALL
MULTIPROCESSORS
HAVE
AT
LEAST
ONE
BLOCK
TO
EXECUTE
OF
BLOCKS
OF
MULTIPROCESSORS
MULTIPLE
BLOCKS
CAN
RUN
CONCURRENTLY
IN
A
MULTIPROCESSOR
BLOCKS
THAT
AREN
T
WAITING
AT
A
SYNCTHREADS
KEEP
THE
HARDWARE
BUSY
SUBJECT
TO
RESOURCE
AVAILABILITY
REGISTERS
SHARED
MEMORY
OF
BLOCKS
TO
SCALE
TO
FUTURE
DEVICES
BLOCKS
EXECUTED
IN
PIPELINE
FASHION
BLOCKS
PER
GRID
WILL
SCALE
ACROSS
MULTIPLE
GENERATIONS
NVIDIA
CORPORATION
REGISTER
DEPENDENCY
READ
AFTER
WRITE
REGISTER
DEPENDENCY
INSTRUCTION
RESULT
CAN
BE
READ
CYCLES
LATER
SCENARIOS
CUDA
PTX
TO
COMPLETELY
HIDE
THE
LATENCY
RUN
AT
LEAST
THREADS
WARPS
PER
MULTIPROCESSOR
AT
LEAST
OCCUPANCY
THREADS
DO
NOT
HAVE
TO
BELONG
TO
THE
SAME
THREAD
BLOCK
NVIDIA
CORPORATION
REGISTER
PRESSURE
HIDE
LATENCY
BY
USING
MORE
THREADS
PER
MULTIPROCESSOR
LIMITING
FACTORS
NUMBER
OF
REGISTERS
PER
KERNEL
PER
MULTIPROCESSOR
PARTITIONED
AMONG
CONCURRENT
THREADS
AMOUNT
OF
SHARED
MEMORY
PER
MULTIPROCESSOR
PARTITIONED
AMONG
CONCURRENT
THREADBLOCKS
COMPILE
WITH
PTXAS
OPTIONS
V
FLAG
USE
MAXRREGCOUNT
N
FLAG
TO
NVCC
N
DESIRED
MAXIMUM
REGISTERS
KERNEL
AT
SOME
POINT
SPILLING
INTO
LOCAL
MEMORY
MAY
OCCUR
REDUCES
PERFORMANCE
LOCAL
MEMORY
IS
SLOW
NVIDIA
CORPORATION
OPTIMIZING
THREADS
PER
BLOCK
CHOOSE
THREADS
PER
BLOCK
AS
A
MULTIPLE
OF
WARP
SIZE
AVOID
WASTING
COMPUTATION
ON
UNDER
POPULATED
WARPS
FACILITATES
COALESCING
MORE
THREADS
PER
BLOCK
HIGHER
OCCUPANCY
GRANULARITY
OF
ALLOCATION
EG
COMPUTE
CAPABILITY
MAX
THREADS
MULTIPROCESSOR
THREADS
BLOCK
OCCUPANCY
THREADS
BLOCK
CAN
HAVE
OCCUPANCY
HEURISTICS
MINIMUM
THREADS
PER
BLOCK
ONLY
IF
MULTIPLE
CONCURRENT
BLOCKS
OR
THREADS
A
BETTER
CHOICE
USUALLY
STILL
ENOUGH
REGS
TO
COMPILE
AND
INVOKE
SUCCESSFULLY
THIS
ALL
DEPENDS
ON
YOUR
COMPUTATION
SO
EXPERIMENT
REPLICAS
WHY
AND
WHY
NOT
RELIABILITY
TOLERANCE
TO
COMPONENT
FAILURES
TOLERANCE
TO
CORRUPTED
DATA
PERFORMANCE
BENEFITS
SCALABILITY
ALLOWS
FOR
CONCURRENT
ACCESS
TO
RESOURCES
DATA
OBJECTS
PROCESSORS
REDUCES
THE
DELAY
FOR
GEOGRAPHICALLY
DISPERSED
RESOURCES
DISADVANTAGES
COST
OF
REPLICATIONS
IS
HIGH
MAINTAINING
CONSISTENCY
OF
MULTIPLE
COPIES
IS
TOUGH
IMPLEMENTATION
IS
HARDER
E
G
DIFFERENT
USERS
HAVE
DIFFERENT
NEEDS
OF
NUMBER
OF
REPLICAS
AND
MORE
OR
LESS
ACCURATE
CONSISTENCY
MODELS
A
GROUP
OF
OTHERWISE
AUTHORIZED
USERS
OF
A
SPECIFIC
SERVICE
IS
SAID
TO
DENY
SERVICE
TO
ANOTHER
GROUP
OF
OTHERWISE
AUTHORIZED
USERS
IF
THE
FORMER
GROUP
MAKES
THE
SPECIFIED
SERVICE
UNAVAILABLE
TO
THE
LATTER
GROUP
FOR
A
PERIOD
OF
TIME
WHICH
EXCEEDS
THE
INTENDED
AND
ADVERTISED
WAITING
TIME
DOS
ATTACKS
AIM
AT
REDUCING
LEGITIMATE
UTILIZATION
OF
NETWORK
AND
OR
SERVER
RESOURCES
THROUGH
RESOURCE
DESTRUCTION
EXPLOIT
BUGS
IN
THE
OS
RESOURCE
EXHAUSTION
VULNERABILITY
EXPLOITATION
E
G
SYN
ATTACK
BRUTE
FORCE
FLOODING
NETWORK
LEVEL
E
G
LOTS
OF
PACKETS
AS
IN
UDP
FLOODS
SERVICE
LEVEL
E
G
FLASH
CROWDS
A
LARGE
NUMBER
OF
ATTACK
HOSTS
REQUEST
SERVICE
FROM
THE
VICTIM
SERVER
AT
A
HIGH
RATE
FOR
INSTANCE
DOWNLOAD
FILES
FROM
AN
FTP
SERVER
OR
GET
WEB
PAGES
FROM
AN
WWW
SERVER
FRONT
ENDS
FORM
A
TREE
WITH
THE
BACK
ENDS
AS
ITS
LOGICAL
ROOT
TREE
LEVEL
OF
EACH
FRONT
END
DEPENDS
ON
ITS
ATTACK
TOLERANCE
FRONT
ENDS
CAN
BE
THE
BOTTLENECK
THAT
GETS
ATTACKED
IT
USUALLY
CAN
WITHSTAND
A
GOOD
AMOUNT
OF
ATTACK
TRAFFIC
TO
JOIN
THE
NETWORK
OR
RECONFIGURE
A
FRONT
END
PERFORMS
PARENT
REGISTRATION
ADDRESS
REGISTRATION
LEGITIMATE
CLIENT
NETWORK
LEVEL
DOS
ATTACKS
FLOOD
NETWORK
RESOURCES
SERVICE
LEVEL
DOS
ATTACKS
EXPLOIT
VULNERABILITIES
TO
CRASH
SERVERS
SERVICE
LEVEL
DOS
ATTACKS
FLOOD
SERVER
RESOURCES
SO
THAT
LEGITIMATE
CLIENTS
PACKETS
WILL
BE
DROPPED
DISTINGUISH
ATTACK
PACKETS
REQUESTS
FROM
LEGITIMATE
PACKETS
REQUESTS
QUICKLY
ACCURATELY
LOW
FALSE
POSITIVES
AND
FALSE
NEGATIVES
AND
EFFICIENTLY
SMALL
OVERHEAD
PRIMARY
METRICS
LEGITIMATE
RESPONSE
TIME
LEGITIMATE
THROUGHPUT
PREVENTION
DETECTION
RECOVERY
MITIGATION
NETWORK
LEVEL
NETWORK
LEVEL
PUZZLES
PACKETSCORE
RED
PD
REPLICATION
OVERLAY
BASED
HEAVY
HITTER
DETECTION
DCAP
PUSHBACK
MOVE
CAPABILITIES
IP
HOPPING
SERVICE
LEVEL
APPLICATION
LEVEL
PUZZLES
RESERVATION
BASED
SCHEMES
DDOS
SHIELD
SHADOW
HONEYPOTS
KILL
BOTS
REPLICATION
HONEYPOTS
ARE
DECOY
RESOURCES
TO
TRAP
ATTACKERS
USEFUL
IN
DETECTING
WORM
INFECTED
HOSTS
HOWEVER
HONEYPOTS
ARE
AT
FIXED
LOCATIONS
SEPARATE
FROM
REAL
SERVERS
IN
ROAMING
HONEYPOTS
THE
LOCATIONS
OF
HONEYPOTS
ARE
CONTINUOUSLY
CHANGING
UNPREDICTABLE
TO
NON
COMPLIANT
ATTACKERS
DISGUISED
WITHIN
SERVERS
UNIQUE
UN
SPOOFABLE
USER
IDENTIFIER
DEALING
WITH
PROXY
SERVERS
IS
AN
OPEN
PROBLEM
WHITE
LIST
ALLOW
PACKETS
FROM
CERTAIN
USERS
IPS
NOT
SCALABLE
BECAUSE
LIST
GROWS
WITH
NUMBER
OF
USERS
BLACK
LIST
DO
NOT
ALLOW
CERTAIN
IPS
OR
USERS
MORE
SCALABLE
ATTACKERS
USERS
WITH
INCREASING
COSTS
OF
ENERGY
CONSUMPTION
AND
COOL
ING
POWER
MANAGEMENT
IN
SERVER
CLUSTERS
HAS
BECOME
AN
INCREASINGLY
IMPORTANT
DESIGN
ISSUE
CURRENT
CLUSTERS
FOR
REAL
TIME
APPLICATIONS
ARE
DESIGNED
TO
HANDLE
PEAK
LOADS
WHERE
ALL
SERVERS
ARE
FULLY
UTILIZED
IN
PRACTICE
PEAK
LOAD
CONDITIONS
RARELY
HAPPEN
AND
CLUSTERS
ARE
MOST
OF
THE
TIME
UNDERUTILIZED
THIS
CREATES
THE
OPPORTUNITY
FOR
USING
SLOWER
FREQUENCIES
AND
THUS
SMALLER
ENERGY
CONSUMPTION
WITH
LIT
TLE
OR
NO
IMPACT
ON
THE
QUALITY
OF
SERVICE
QOS
FOR
EXAM
PLE
PERFORMANCE
AND
TIMELINESS
IN
THIS
WORK
WE
PRESENT
A
CLUSTER
WIDE
QOS
AWARE
TECH
NIQUE
THAT
DYNAMICALLY
RECONFIGURES
THE
CLUSTER
TO
REDUCE
ENERGY
CONSUMPTION
DURING
PERIODS
OF
REDUCED
LOAD
MORE
OVER
WE
ALSO
INVESTIGATE
THE
EFFECTS
OF
LOCAL
QOS
AWARE
POWER
MANAGEMENT
USING
DYNAMIC
VOLTAGE
SCALING
DVS
SINCE
MOST
REAL
WORLD
CLUSTERS
CONSIST
OF
MACHINES
OF
DIF
FERENT
KIND
IN
TERMS
OF
BOTH
PERFORMANCE
AND
ENERGY
CON
SUMPTION
WE
FOCUS
ON
HETEROGENEOUS
CLUSTERS
FOR
VALIDATION
WE
DESCRIBE
AND
EVALUATE
AN
IMPLEMEN
TATION
OF
THE
PROPOSED
SCHEME
USING
THE
APACHE
WEBSERVER
IN
A
SMALL
REALISTIC
CLUSTER
OUR
EXPERIMENTAL
RESULTS
SHOW
THAT
USING
OUR
SCHEME
IT
IS
POSSIBLE
TO
SAVE
UP
TO
OF
THE
TOTAL
ENERGY
CONSUMED
BY
THE
SERVERS
MAINTAINING
AVERAGE
RESPONSE
TIMES
WITHIN
THE
SPECIFIED
DEADLINES
AND
NUMBER
OF
DROPPED
REQUESTS
WITHIN
THE
REQUIRED
AMOUNT
INTRODUCTION
UNTIL
RECENTLY
PERFORMANCE
HAD
BEEN
THE
MAIN
CONCERN
IN
SERVER
FARMS
BUT
ENERGY
CONSUMPTION
HAS
ALSO
BECOME
A
MAIN
CONCERN
IN
SUCH
SYSTEMS
DUE
TO
THE
IMPORTANCE
OF
CUSTOMER
CARE
SERVICE
IN
COMMERCIAL
INSTALLATIONS
AND
THE
IMPORTANCE
OF
TIMELY
RESPONSES
FOR
EMBEDDED
SERVER
CLUS
TERS
CURRENT
CLUSTERS
ARE
TYPICALLY
DESIGNED
TO
HANDLE
PEAK
SUPPORTED
BY
NFS
THROUGH
THE
SECURE
CITI
PROJECT
ANI
AND
THE
POWER
AUTONOMOUS
NETWORKS
PROJECT
ANI
CLAUDIO
SCORDINO
IS
A
PHD
STUDENT
AT
THE
UNIVERSITY
OF
PISA
VISITING
THE
UNIVERSITY
OF
PITTSBURGH
LOADS
HOWEVER
PEAK
LOAD
CONDITIONS
RARELY
HAPPEN
IN
PRAC
TICE
AND
CLUSTERS
ARE
MOST
OF
THE
TIME
UNDERUTILIZED
IN
FACT
THEIR
LOADS
OFTEN
VARY
SIGNIFICANTLY
DEPENDING
ON
THE
TIME
OF
THE
DAY
OR
OTHER
EXTERNAL
FACTORS
THEREFORE
THE
AVERAGE
PRO
CESSOR
USE
OF
SUCH
SYSTEMS
MAY
BE
EVEN
LESS
THAN
WITH
RESPECT
TO
THEIR
PEAK
CAPACITY
CLUSTERS
WITH
HIGH
PEAK
POWER
NEED
COMPLEX
AND
EXPEN
SIVE
COOLING
INFRASTRUCTURES
TO
ENSURE
THE
PROPER
OPERATION
OF
THE
SERVERS
WITH
POWER
DENSITIES
INCREASING
DUE
TO
IN
CREASING
PERFORMANCE
DEMANDS
AND
TIGHTER
PACKING
PROPER
COOLING
BECOMES
EVEN
MORE
CHALLENGING
FANS
DRIVING
THE
COOLING
SYSTEM
MAY
CONSUME
UP
TO
OF
THE
TOTAL
SYS
TEM
POWER
IN
SOME
COMMERCIAL
SERVERS
AND
MAN
UFACTURERS
ARE
FACING
THE
PROBLEM
OF
BUILDING
POWERFUL
SYS
TEMS
WITHOUT
INTRODUCING
ADDITIONAL
TECHNIQUES
SUCH
AS
LIQ
UID
COOLING
ELECTRICITY
COST
IS
A
SIGNIFICANT
FRACTION
OF
THE
OPERATION
COST
OF
DATA
CENTERS
FOR
EXAMPLE
A
GOOGLE
RACK
CONSUMES
ABOUT
A
MONTH
INCLUDING
COOLING
WHICH
IS
AT
LEAST
OF
THE
OPERATION
COST
WITH
THIS
FRACTION
LIKELY
TO
INCREASE
IN
THE
FUTURE
THESE
ISSUES
ARE
EVEN
MORE
CRITICAL
IN
EMBEDDED
CLUS
TERS
TYPICALLY
UNTETHERED
DEVICES
IN
WHICH
PEAK
POWER
HAS
AN
IMPORTANT
IMPACT
ON
THE
SIZE
OF
THE
SYSTEM
WHILE
ENERGY
CONSUMPTION
DETERMINES
THE
DEVICE
LIFETIME
EXAM
PLES
INCLUDE
SATELLITE
SYSTEMS
OR
OTHER
MOBILE
DEVICES
WITH
MULTIPLE
COMPUTING
PLATFORMS
SUCH
AS
THE
MARS
ROVER
AND
ROBOTICS
PLATFORMS
POWER
MANAGEMENT
PM
MECHANISMS
CAN
BE
DIVIDED
INTO
TWO
CATEGORIES
CLUSTER
WIDE
AND
LOCAL
CLUSTER
WIDE
MECHANISMS
INVOLVE
GLOBAL
DECISIONS
SUCH
AS
TURNING
ON
AND
OFF
CLUSTER
MACHINES
ACCORDING
TO
THE
LOAD
LO
CAL
TECHNIQUES
PUT
UNUSED
OR
UNDERUTILIZED
RESOURCES
IN
LOW
POWER
STATES
FOR
EXAMPLE
SELF
REFRESH
STANDBY
AND
OFF
MODES
FOR
DRAM
CHIPS
DYNAMIC
VOLTAGE
SCALING
DVS
AND
LOW
POWER
STATES
FOR
THE
CPU
DISK
SHUTDOWN
ETC
A
PM
MECHANISM
LOCAL
OR
CLUSTER
WIDE
IS
QOS
AWARE
IF
IT
REDUCES
THE
POWER
CONSUMPTION
WHILE
GUARANTEEING
A
CER
TAIN
AMOUNT
OF
QUALITY
OF
SERVICE
QOS
SUCH
AS
AVERAGE
RESPONSE
TIMES
OR
PERCENTAGE
OF
DEADLINES
MET
TO
THE
BEST
OF
OUR
KNOWLEDGE
THIS
IS
THE
FIRST
POWER
MANAGEMENT
SCHEME
THAT
IS
SIMULTANEOUSLY
A
CLUSTER
WIDE
I
E
TURNING
ON
AND
OFF
MACHINES
B
DESIGNED
FOR
HETERO
GENEITY
C
QOS
AWARE
AND
POWER
AWARE
AT
THE
LOCAL
SERVERS
I
E
DEADLINE
AWARE
D
MEASUREMENT
BASED
CONTRARY
TO
THEORETICAL
MODELING
RELYING
ON
MEASUREMENTS
IS
THE
KEY
TO
OUR
APPROACH
E
IMPLEMENTATION
ORIENTED
AND
F
PER
FORMING
RECONFIGURATION
DECISIONS
AT
RUNTIME
OUR
SCHEME
IS
REALISTIC
BECAUSE
MOST
CLUSTERS
HAVE
ONE
OR
MORE
FRONT
ENDS
ARE
COMPOSED
OF
DIFFERENT
KIND
OF
MA
CHINES
AND
NEED
BOTH
LOCAL
AND
CLUSTER
WIDE
QOS
AWARE
PM
SCHEMES
WHILE
THE
METHODOLOGY
AND
THE
ALGORITHMS
PROPOSED
APPLY
TO
ANY
KIND
OF
CLUSTER
WE
SHOW
THEIR
USE
IN
A
WEB
SERVER
CONTEXT
OUR
MEASUREMENTS
SHOW
A
REDUC
TION
OF
ENERGY
CONSUMPTION
EQUAL
TO
USING
ONLY
THE
LOCAL
PM
USING
THE
ON
OFF
SCHEME
AND
USING
BOTH
SCHEMES
WITH
RESPECT
TO
DELAYS
THE
LOCAL
PM
ADDED
WHILE
ON
OFF
ADDED
ABOUT
IN
ALL
CASES
THE
AV
ERAGE
DELAY
WAS
QUITE
SMALL
WITH
RESPECT
TO
DEADLINES
THE
REMAINDER
OF
THE
PAPER
IS
ORGANIZED
AS
FOLLOWS
WE
FIRST
PRESENT
RELATED
WORK
IN
SECTION
THE
CLUSTER
MODEL
IS
GIVEN
IN
SECTION
THE
CLUSTER
WIDE
PM
SCHEME
IS
EX
PLAINED
IN
SECTION
WHILE
THE
LOCAL
REAL
TIME
DVS
SCHEME
IS
PRESENTED
IN
SECTION
BOTH
SCHEMES
ARE
THEN
EVALUATED
IN
SECTION
IN
SECTION
WE
STATE
OUR
CONCLUSIONS
RELATED
WORK
WITH
ENERGY
CONSUMPTION
EMERGING
AS
A
KEY
ASPECT
OF
CLUSTER
COMPUTING
MUCH
RECENT
RESEARCH
HAS
FOCUSED
ON
PM
IN
SERVER
FARMS
A
FIRST
CHARACTERIZATION
OF
POWER
CON
SUMPTION
AND
WORKLOAD
IN
REAL
WORLD
WEBSERVERS
WAS
MADE
IN
DVS
WAS
PROPOSED
AS
THE
MAIN
TECHNIQUE
TO
RE
DUCE
ENERGY
CONSUMPTION
IN
SUCH
SYSTEMS
DVS
AND
RE
QUEST
BATCHING
TECHNIQUES
WERE
FURTHER
EVALUATED
IN
SOFTWARE
PEAK
POWER
CONTROL
TECHNIQUES
WERE
INVESTIGATED
IN
HOWEVER
THESE
STUDIES
CONSIDERED
ONLY
POWER
CONSUMPTION
OF
PROCESSOR
AND
MAIN
MEMORY
IN
SINGLE
PROCESSOR
SYSTEMS
THE
PROBLEM
OF
CLUSTER
CONFIGURATION
I
E
TURNING
ON
AND
OFF
CLUSTER
MACHINES
FOR
HOMOGENEOUS
CLUSTERS
WAS
FIRST
AD
DRESSED
IN
AN
OFFLINE
ALGORITHM
DETERMINES
THE
NUMBER
OF
SERVERS
NEEDED
FOR
A
GIVEN
LOAD
CLUSTER
RECONFIGURATION
IS
THEN
PERFORMED
ONLINE
BY
A
PROCESS
RUNNING
ON
A
SERVER
USING
THRESHOLDS
TO
PREVENT
TOO
FREQUENT
RECONFIGURATIONS
EVEN
THOUGH
THERE
IS
NO
EXPLICIT
QOS
CONSIDERATION
THE
AUTHORS
HAVE
EXTENDED
THEIR
WORK
TO
HETEROGENEOUS
CLUS
TERS
IN
MODELS
HAVE
BEEN
ADDED
FOR
THROUGHPUT
AND
POWER
CONSUMPTION
ESTIMATION
RECONFIGURATION
DECISIONS
ARE
MADE
ONLINE
BASED
ON
THE
PRECOMPUTED
INFORMATION
AND
THE
PREDICTED
LOAD
THE
AUTHORS
ALSO
PROPOSED
TO
ADD
REQUEST
TYPES
TO
IMPROVE
LOAD
ESTIMATION
IN
OUR
WORK
DIFFERS
FROM
THE
ABOVE
PREVIOUS
STUDIES
IN
THE
FOLLOWING
WAYS
WE
CONSIDER
QOS
DIRECTLY
INDIVIDUAL
SERVERS
ARE
BOTH
POWER
AWARE
AND
QOS
AWARE
WE
RELY
ON
OFFLINE
MEASUREMENTS
INSTEAD
OF
USING
MODELS
RECONFIGURA
TION
DECISIONS
I
E
NUMBER
OF
ACTIVE
SERVERS
AND
LOAD
DIS
TRIBUTION
ARE
NOT
EXPENSIVE
AND
ARE
PERFORMED
ONLINE
AND
RECONFIGURATION
THRESHOLDS
ARE
BASED
ON
THE
TIME
NEEDED
TO
BOOT
SHUTDOWN
A
SERVER
ONE
OF
THE
FIRST
ATTEMPTS
TO
COMBINE
CLUSTER
WIDE
AND
LO
CAL
PM
TECHNIQUES
PROPOSED
FIVE
DIFFERENT
POLICIES
COM
BINING
DVS
AND
CLUSTER
CONFIGURATION
HOWEVER
THE
THE
ORY
BEHIND
THIS
WORK
RELIES
ON
A
HOMOGENEOUS
CLUSTERS
AND
CANNOT
BE
EASILY
EXTENDED
TO
HETEROGENEOUS
MACHINES
AND
B
THE
OFTEN
INCORRECT
ASSUMPTION
THAT
POWER
IS
A
CUBIC
FUNCTION
OF
THE
CPU
FREQUENCY
ANOTHER
WORK
PROPOSED
TO
USE
THE
CLUSTER
LOAD
INSTEAD
OF
THE
AVERAGE
CPU
FREQUENCY
AS
THE
CRITERIA
FOR
TURNING
ON
OFF
MACHINES
HOWEVER
THIS
STUDY
ASSUMED
HOMOGENEOUS
CLUSTERS
AS
WELL
TO
THE
BEST
OF
OUR
KNOWLEDGE
THIS
WORK
IS
THE
FIRST
ATTEMPT
TO
COM
BINE
CLUSTER
WIDE
AND
LOCAL
TECHNIQUES
IN
THE
CONTEXT
OF
HET
EROGENEOUS
CLUSTERS
IN
REAL
TIME
COMPUTING
DYNAMIC
VOLTAGE
AND
FRE
QUENCY
SCALING
HAS
BEEN
EXPLORED
TO
REDUCE
ENERGY
CON
SUMPTION
DVS
SCHEMES
TYPICALLY
FOCUS
ON
MINIMIZING
CPU
ENERGY
CONSUMPTION
WHILE
MEETING
A
PERFORMANCE
RE
QUIREMENT
DVS
WORK
FOR
APERIODIC
TASKS
IN
SINGLE
PROCESSORS
INCLUDES
OFFLINE
AND
ONLINE
ALGORITHMS
ASSUM
ING
WORST
CASE
EXECUTION
TIMES
AUTOMATIC
DVS
FOR
LINUX
WITH
DISTINCTION
BETWEEN
BACKGROUND
AND
INTERACTIVE
JOBS
AND
USE
OF
KNOWLEDGE
ABOUT
THE
DISTRIBUTION
OF
JOB
LENGTHS
FOR
VOLTAGE
SCALING
DECISIONS
HOWEVER
THESE
TECHNIQUES
TYPICALLY
AIM
AT
REDUCING
THE
ENERGY
CON
SUMED
ONLY
BY
THE
CPU
AND
DO
NOT
TAKE
INTO
ACCOUNT
OTHER
DEVICES
SUCH
AS
MEMORY
POWER
SUPPLIES
OR
DISK
THAT
CONTRIBUTE
WITH
AN
IMPORTANT
FRACTION
TO
THE
TO
TAL
ENERGY
CONSUMED
BY
THE
SYSTEM
IN
OUR
MODEL
INSTEAD
SERVERS
CAN
PUT
THEIR
RESOURCES
IN
LOW
POWER
STATES
AND
NO
ASSUMPTION
IS
MADE
ABOUT
THEIR
LOCAL
PM
SCHEMES
MOST
RELATED
TO
OUR
LOCAL
SCHEME
IS
SHARMA
ET
AL
WORK
ON
ADAPTIVE
ALGORITHMS
FOR
DVS
FOR
A
QOS
ENABLED
WEB
SERVER
THEIR
SCHEME
USES
A
THEORETICAL
UTILIZATION
BOUND
DERIVED
IN
TO
GUARANTEE
THE
QOS
OF
WEB
REQUESTS
HOWEVER
THEY
TAKE
INTO
ACCOUNT
ONLY
LOCAL
PM
ASSUMING
THAT
A
GOOD
LOAD
BALANCING
ALGORITHM
IS
USED
AT
THE
FRONT
END
IN
THAT
SENSE
OUR
WORKS
ARE
COMPLEMENTARY
SINCE
WE
DESCRIBE
HOW
TO
ACHIEVE
SUCH
LOAD
BALANCING
CLUSTER
MODEL
THIS
SECTION
INTRODUCES
THE
CLUSTER
MODEL
THAT
WE
CON
SIDER
SEE
FIGURE
A
FRONT
END
MACHINE
RECEIVES
REQUESTS
FROM
CLIENTS
AND
REDIRECTS
THEM
TO
A
SET
OF
PROCESSING
NODES
HENCEFORTH
REFERRED
TO
AS
SERVERS
THE
FRONT
END
IS
NOT
A
PROCESSING
NODE
AND
HAS
THREE
MAIN
FUNCTIONS
A
ACCEPT
ING
APERIODIC
REQUESTS
FROM
CLIENTS
B
DISTRIBUTING
THE
LOAD
TO
SERVERS
AND
C
RECONFIGURING
THE
CLUSTER
I
E
TURNING
SERVERS
ON
OFF
TO
REDUCE
THE
GLOBAL
ENERGY
CONSUMPTION
WHILE
KEEPING
THE
OVERALL
PERFORMANCE
WITHIN
A
PRESPECI
FIED
QOS
REQUIREMENT
AFTER
RECEIVING
A
REQUEST
THE
FRONT
END
COMMUNICATES
TO
THE
CLIENT
TO
WHICH
SERVER
THE
REQUEST
MUST
BE
SENT
USING
HTTP
REDIRECTION
THEN
THE
CLIENT
SENDS
ITS
REQUEST
DIRECTLY
TO
THE
SERVER
IN
OUR
CLUSTER
SCHEME
EACH
REQUEST
IS
AN
APERIODIC
TASK
I
E
NO
ASSUMPTIONS
ARE
MADE
ABOUT
TASK
ARRIVAL
TIMES
AND
IS
ASSIGNED
A
DEADLINE
THE
SPECIFICATION
OF
THE
QOS
IS
SYSTEM
WIDE
AND
IS
IN
OUR
CASE
THE
PERCENTAGE
OF
DEADLINES
MET
THE
WAY
TO
ACHIEVE
THE
SOFT
REAL
TIME
PROPERTIES
WILL
BE
PRESENTED
IN
DETAIL
IN
THE
NEXT
SECTIONS
EACH
SERVER
IN
THE
HETEROGENEOUS
CLUSTER
PERFORMS
THE
SAME
SERVICE
I
E
ALL
SERVERS
CAN
PROCESS
ALL
REQUESTS
NO
RESTRICTION
IS
IMPOSED
REGARDING
ANY
ASPECT
OF
THEIR
COM
PUTATION
PROCESS
SCHEDULING
CPU
PERFORMANCE
MEMORY
SPEED
BANDWIDTH
DISK
SPEED
BANDWIDTH
POWER
CONSUMP
TION
NETWORK
BANDWIDTH
ETC
IN
ADDITION
SERVERS
PERIODI
CALLY
INFORM
THE
FRONT
END
ABOUT
THEIR
CURRENT
LOAD
TO
AID
THE
FRONT
END
IN
LOAD
DISTRIBUTION
AND
CLUSTER
CONFIGURATION
DE
CISIONS
AFTER
A
REQUEST
HAS
BEEN
PROCESSED
BY
A
SERVER
THE
RESULT
IS
RETURNED
DIRECTLY
TO
THE
CLIENT
WITHOUT
THE
FRONT
END
AS
INTERMEDIARY
NOTE
THAT
A
MORE
COMMON
CLUSTER
DESIGN
IS
WITH
THE
FRONT
END
ACTING
AS
A
PROXY
I
E
ACTING
AS
INTERMEDIARY
BETWEEN
CLIENTS
AND
SERVERS
IN
OUR
WEBSERVER
EXAMPLE
CHOOSING
ONE
CONFIGURATION
OR
THE
OTHER
I
E
PROXY
VERSUS
NO
PROXY
WITH
REDIRECTION
IS
SIMPLY
A
CONFIGURATION
OPTION
AND
THE
PROPOSED
SCHEME
IN
THIS
PAPER
WORKS
EQUALLY
WELL
WITH
EITHER
TYPE
OF
FRONT
END
IN
OUR
EXPERIMENTS
FOR
HIGH
LOADS
ABOVE
WE
HAD
TO
USE
THE
NO
PROXY
ARCHITEC
TURE
SHOWN
IN
FIGURE
AS
A
PROXY
FRONT
END
CANNOT
FULLY
UTILIZE
THE
CLUSTER
IN
OUR
EXPERIMENTAL
SETUP
OUR
FRONT
END
HAS
ONLY
ONE
GBE
NETWORK
INTERFACE
CARD
WHEN
USING
REDIRECTION
INSTEAD
OF
PROXYING
THE
LINKS
AND
INTERNAL
REFERENCES
SHOULD
USE
THE
FULL
URL
TO
GUARAN
TEE
THAT
ALL
THE
REQUESTS
ARE
SENT
TO
THE
FRONT
END
THIS
WAY
REDIRECTION
WORKS
WITH
EITHER
THE
HTTP
OR
THE
HTTP
PROTOCOLS
IN
HTTP
THE
CLIENT
MAY
KEEP
MULTIPLE
CON
NECTIONS
OPEN
TO
THE
FRONT
END
AND
THE
SERVERS
IT
WAS
REDI
RECTED
TO
BUT
ALL
THE
REQUESTS
WILL
BE
SENT
TO
THE
FRONT
END
FIRST
THE
ASPECTS
RELATED
TO
CLUSTER
CONFIGURATION
PM
AND
LOAD
DISTRIBUTION
PERFORMED
BY
THE
FRONT
END
WILL
BE
PRESENTED
IN
DETAIL
IN
THE
NEXT
SECTION
LOCAL
PM
IS
PERFORMED
INDEPEN
DENTLY
BY
EACH
SERVER
WITHOUT
FRONT
END
CONTROL
AND
WILL
BE
PRESENTED
IN
SECTION
FRONT
END
POWER
MANAGEMENT
OUR
PROPOSED
FRONT
END
FOLLOWS
A
VERY
GENERAL
FRAME
WORK
THAT
IS
APPLICABLE
TO
ANY
HETEROGENEOUS
CLUSTER
TO
ACHIEVE
THIS
GOAL
WE
CANNOT
IMPOSE
ANY
RESTRICTION
ON
FIGURE
CLUSTER
ARCHITECTURE
SERVER
CHARACTERISTICS
HOWEVER
FOR
EASE
OF
PRESENTATION
DEFINITIONS
AND
EXAMPLES
EMPHASIZE
WEB
SERVER
CLUSTERS
LOAD
DEFINITION
AND
ESTIMATION
THE
FRONT
END
DETERMINES
THE
NUMBER
OF
ACTIVE
SERVERS
TO
MEET
THE
DESIRED
LEVEL
OF
QOS
WHILE
MINIMIZING
CLUSTER
EN
ERGY
CONSUMPTION
THE
NUMBER
OF
SERVERS
IS
COMPUTED
OF
FLINE
OR
ONLINE
AS
A
FUNCTION
OF
THE
SYSTEM
LOAD
THUS
DEFIN
ING
LOAD
CORRECTLY
IS
A
CRUCIAL
STEP
A
MEASURE
OF
THE
LOAD
FOR
CLUSTERS
IS
THE
NUMBER
OF
REQUESTS
RECEIVED
PER
SECOND
MEASURED
OVER
SOME
RECENT
INTERVAL
CLEARLY
DEPENDING
ON
THE
KIND
OF
SERVICE
UNDER
CONSIDERATION
OTHER
DEFINITIONS
OF
LOAD
MAY
BE
MORE
APPROPRIATE
SUCH
AS
THE
BANDWIDTH
FOR
A
FILE
SERVER
AT
RUNTIME
THE
FRONT
END
NEEDS
TO
CORRECTLY
ESTIMATE
OR
OBSERVE
THE
LOAD
IN
ORDER
TO
MAKE
PM
DECISIONS
AND
TO
PER
FORM
LOAD
DISTRIBUTION
THE
LOAD
ESTIMATION
CAN
BE
FURTHER
IMPROVED
BY
USING
FEEDBACK
FROM
THE
SERVERS
AS
OBSERVED
IN
PREVIOUS
WORK
LOAD
ESTIMA
TION
CAN
BE
GREATLY
IMPROVED
BY
CONSIDERING
REQUEST
TYPES
THE
TYPE
OF
A
REQUEST
MAY
BE
CONVENIENTLY
DETERMINED
ONLY
BY
THE
HEADER
E
G
THE
NAME
OF
THE
REQUESTED
FILE
NOTICE
THAT
THE
NUMBER
OF
TYPES
IS
A
DESIGN
ISSUE
ON
ONE
HAND
DIFFERENT
TYPES
MAY
NOT
BE
NECESSARY
IF
THE
VARIABILITY
OF
THE
TIME
TO
SERVICE
A
REQUEST
IS
LOW
ON
THE
OTHER
HAND
EACH
REQUEST
COULD
BE
OF
A
DIFFERENT
TYPE
LEADING
TO
AN
IM
PROVED
ESTIMATION
BUT
ALSO
TO
A
HIGHER
OVERHEAD
TO
MEASURE
ALL
DIFFERENT
TYPES
OF
REQUESTS
AND
UPDATE
STATISTICS
TABLES
IN
THE
CASE
OF
A
WEB
SERVER
THERE
ARE
TWO
MAIN
TYPES
OF
REQUESTS
WITH
DIFFERENT
COMPUTATIONAL
CHARACTERISTICS
STATIC
AND
DYNAMIC
PAGES
STATIC
PAGES
RESIDE
IN
SERVER
MEMORY
AND
DO
NOT
REQUIRE
MUCH
COMPUTATION
DYNAMIC
PAGES
IN
STEAD
ARE
CREATED
ON
DEMAND
THROUGH
THE
USE
OF
SOME
EXTER
NAL
LANGUAGE
E
G
PERL
OR
PHP
FOR
THIS
REASON
DYNAMIC
PAGES
TYPICALLY
REQUIRE
MORE
COMPUTATION
THAN
STATIC
ONES
CONSIDER
A
GENERIC
SERVER
AND
LET
ASTATIC
AND
ADYNAMIC
BE
THE
AVERAGE
EXECUTION
TIMES
TO
SERVE
A
STATIC
AND
A
DY
NAMIC
PAGE
RESPECTIVELY
AT
THE
MAXIMUM
CPU
SPEED
FOR
EXAMPLE
FOR
ONE
SERVER
IN
OUR
CLUSTER
WE
MEASURED
AN
AV
ERAGE
EXECUTION
TIME
ASTATIC
FOR
STATIC
PAGES
AND
ADYNAMIC
FOR
DYNAMIC
PAGES
ON
AVERAGE
THE
TIME
NEEDED
BY
THE
CPU
TO
SERVE
NSTATIC
STATIC
REQUESTS
AND
NDYNAMIC
DYNAMIC
REQUESTS
IS
THUS
NSTATICASTATIC
NDYNAMICADYNAMIC
SECONDS
IF
THE
NUMBER
OF
REQUESTS
IS
OBSERVED
OVER
A
PERIOD
OF
MONITOR
PERIOD
SECONDS
THEN
THE
LOAD
OF
THE
MACHINE
SERVING
THE
REQUESTS
IS
ONCE
THE
LOCAL
PM
SCHEME
THE
OS
AND
THE
SCHEDULING
POLICY
HAVE
BEEN
DECIDED
FOR
A
SERVER
THE
POWER
CONSUMP
TION
AS
FUNCTION
OF
THE
LOAD
AND
THE
MAXIMUM
LOAD
CAN
BE
DETERMINED
THROUGH
SIMPLE
MEASUREMENTS
IN
OUR
EXPERIMENTS
AFTER
CHOOSING
THE
LOCAL
PM
SCHEME
SEE
SECTION
WE
MEASURED
THE
AVERAGE
POWER
CONSUMP
TION
FOR
A
LOAD
IN
INCREMENTS
THEN
WE
INTERPOLATED
THE
POINTS
TO
HAVE
VALUES
IN
INCREMENTS
WE
MEASURED
THE
TOTAL
POWER
CONSUMED
BY
THE
WHOLE
MACHINES
NOT
ONLY
BY
THEIR
CPUS
IN
OUR
CASE
RECORDING
THE
AVERAGE
POWER
CONSUMPTION
FOR
A
GIVEN
LOAD
OVER
A
PERIOD
OF
MINUTES
LOAD
NSTATICASTATIC
NDYNAMICADYNAMIC
MONITOR
PERIOD
WAS
SUFFICIENT
TO
OBTAIN
A
GOOD
AVERAGE
WE
MEASURED
AC
POWER
DIRECTLY
WITH
A
SIMPLE
POWER
METER
WITH
ACCU
NOTICE
THAT
THIS
DEFINITION
OF
LOAD
ASSUMES
A
CPU
BOUND
SERVER
THIS
IS
NORMAL
FOR
MOST
WEB
SERVERS
BECAUSE
MUCH
OF
THE
DATA
ARE
ALREADY
IN
MEMORY
IN
FACT
ON
ALL
OUR
MACHINES
WE
HAVE
NOTICED
THAT
THE
BOTTLENECK
OF
THE
SYSTEM
WAS
THE
CPU
HOWEVER
FOR
SYSTEMS
WITH
DIFFERENT
BOTTLE
NECKS
E
G
DISK
I
O
OR
NETWORK
BANDWIDTH
ANOTHER
DEFINI
TION
OF
LOAD
MAY
BE
MORE
APPROPRIATE
IN
FACT
THE
DEFINITION
OF
LOAD
SHOULD
ACCOUNT
FOR
THE
BOTTLENECK
RESOURCE
NOTE
THAT
EVEN
THOUGH
WEB
REQUESTS
MAY
EXHIBIT
A
LARGE
VARIATION
IN
EXECUTION
TIMES
USING
THE
AVERAGE
VALUES
ASTATIC
AND
ADYNAMIC
IN
EQUATION
RESULTS
IN
A
VERY
ACCURATE
LOAD
ES
TIMATION
THIS
IS
BECAUSE
WEB
REQUESTS
ARE
RELATIVELY
SMALL
AND
NUMEROUS
WE
DEFINE
THE
MAXIMUM
LOAD
OF
A
SERVER
AS
THE
MAX
IMUM
NUMBER
OF
REQUESTS
THAT
IT
CAN
HANDLE
MEETING
THE
OF
DEADLINES
THE
FRONT
END
NEVER
DIRECTS
MORE
THAN
THE
MAXIMUM
LOAD
TO
A
SERVER
THE
CLUSTER
LOAD
IS
DE
FINED
AS
THE
SUM
OF
THE
CURRENT
LOADS
OF
ALL
ACTIVE
SERVERS
THEREFORE
THE
MAXIMUM
LOAD
THAT
THE
CLUSTER
CAN
HANDLE
IS
THE
SUM
OF
THE
MAXIMUM
LOADS
OF
ALL
SERVERS
AT
RUNTIME
THE
CLUSTER
LOAD
I
E
BOTH
VARIABLES
NSTATIC
AND
NDYNAMIC
IS
OBSERVED
EVERY
MONITOR
PERIOD
SECONDS
THE
VALUE
OF
MONITOR
PERIOD
IS
A
DESIGN
ISSUE
RELATED
TO
THE
TRADEOFF
BETWEEN
RESPONSE
TIME
AND
OVERHEAD
IN
OUR
CLUSTER
VALUES
IN
THE
ORDER
OF
A
FEW
SECONDS
WERE
FOUND
SUITABLE
SERVER
INFORMATION
IN
ORDER
TO
REDUCE
THE
GLOBAL
POWER
CONSUMPTION
AT
RUN
TIME
WE
FURNISH
THE
FRONT
END
WITH
INFORMATION
ABOUT
THE
AVERAGE
POWER
CONSUMPTION
OF
EACH
SERVER
FOR
ANY
DIFFER
ENT
VALUE
OF
ITS
LOAD
SERVERS
CAN
REDUCE
THEIR
OWN
POWER
CONSUMPTION
IN
A
NUMBER
OF
DIFFERENT
WAYS
SUCH
AS
USING
DVS
AND
LOW
POWER
STATES
FOR
THE
CPU
SELF
REFRESH
MODES
FOR
MEMORY
STOPPING
DISK
SPINNING
AFTER
SOME
TIME
OF
IDLE
NESS
ETC
MOREOVER
EACH
SERVER
MAY
USE
A
DIFFERENT
OS
OR
A
DIFFERENT
SCHEDULING
POLICY
SUCH
AS
A
STANDARD
ROUND
ROBIN
OR
A
REAL
TIME
POLICY
TO
GIVE
HIGHER
PRIORITY
TO
STATIC
PAGES
WITH
RESPECT
TO
DYNAMIC
ONES
NO
ASSUMPTION
IS
MADE
AT
THE
FRONT
END
ABOUT
LOCAL
PM
SCHEMES
RACY
HENCE
THE
WHOLE
PROCESS
REQUIRED
AT
MOST
FEW
HOURS
FOR
EACH
MACHINE
CLEARLY
IDENTICAL
MACHINES
NEED
NOT
TO
BE
MEASURED
TWICE
THE
CURVE
REPRESENTING
THE
POWER
CONSUMPTION
OF
EACH
SERVER
OF
OUR
CLUSTER
IS
SHOWN
IN
FIG
URE
THE
LAST
POINT
ON
EACH
CURVE
REPRESENTS
THE
MAXIMUM
LOAD
THAT
MEETS
OUR
QOS
SPECIFICATION
I
E
OF
DEAD
LINES
MET
NORMALIZED
TO
THE
FASTEST
MACHINE
IN
THE
CLUSTER
THE
PARAMETERS
FOR
EACH
MACHINE
ARE
REPORTED
IN
TABLE
ON
PAGE
LOAD
FIGURE
POWER
CONSUMPTION
VS
LOAD
FOR
SERVERS
THE
INFORMATION
ABOUT
POWER
CONSUMPTION
OF
SERVERS
CAN
THEN
BE
USED
BY
THE
FRONT
END
AT
RUNTIME
SINCE
THE
FRONT
END
CONTROLS
THE
LOAD
FOR
EACH
SERVER
AND
THE
POWER
CONSUMP
TION
OF
EACH
SERVER
FOR
A
GIVEN
LOAD
IS
KNOWN
THE
FRONT
END
HAS
ENOUGH
INFORMATION
FOR
PM
DECISIONS
NOTICE
THAT
THE
COOLING
COSTS
OF
THE
ROOM
HAVE
NOT
BEEN
TAKEN
INTO
ACCOUNT
HOWEVER
SINCE
THESE
COSTS
ARE
EXPECTED
TO
BE
PROPORTIONAL
TO
THE
AC
POWER
DRAWN
THEY
ARE
AUTOMATICALLY
REDUCED
BY
MINIMIZING
CLUSTER
POWER
CONSUMPTION
WE
NOW
PRESENT
ALL
THE
INFORMATION
AND
THE
CORRE
SPONDING
NOTATION
ABOUT
EACH
SERVER
NEEDED
AT
THE
FRONT
END
LEVEL
BOOT
TIMEI
AND
SHUTDOWN
TIMEI
REPRESENT
THE
TIME
TO
BOOT
AND
TO
SHUTDOWN
SERVER
I
INCLUDING
THE
TIME
TO
START
AND
FINISH
THE
WEBSERVER
PROCESS
OF
THE
SERVER
MAX
LOADI
IS
THE
MAXIMUM
LOAD
OF
SERVER
I
THAT
CAN
SATISFY
THE
QOS
REQUIREMENT
OFF
POWERI
IS
THE
POWER
CON
SUMED
WHEN
THE
SERVER
IS
OFF
SOME
COMPONENTS
SUCH
AS
THE
WAKE
ON
LAN
INTERFACE
USED
TO
POWER
UP
THE
MACHINE
AN
ADDITIONAL
INTERVAL
OF
TIME
TO
ACCOUNT
FOR
THE
ERROR
WHEN
MEASURING
THE
CURRENT
LOAD
IN
GENERAL
SERVER
I
IS
TURNED
ON
WHEN
THE
CLUSTER
LOAD
REACHES
MAY
NOT
BE
COMPLETELY
OFF
FINALLY
POWER
VS
LOADI
IS
AN
I
BOOT
TIME
MAX
LOADI
LOAD
INCREMENT
ENTRIES
RECORDING
THE
MEASURED
MAX
LOADJ
I
MONITOR
PERIOD
MAX
LOAD
INCREASE
POWER
CONSUMPTION
OF
SERVER
I
FOR
EACH
VALUE
OF
THE
LOAD
IN
LOAD
INCREMENT
PERCENTS
WE
USED
THE
FIRST
ENTRY
OF
THE
ARRAY
DENOTES
THE
IDLE
POWER
I
E
NO
LOAD
ON
OFF
POLICY
THIS
SECTION
DESCRIBES
THE
KEY
IDEA
BEHIND
OUR
CLUSTER
WIDE
PM
SCHEME
THE
FRONT
END
BESIDES
DISTRIBUTING
THE
LOAD
TO
SERVERS
TO
MINIMIZE
GLOBAL
POWER
CONSUMPTION
DE
TERMINES
THE
CLUSTER
CONFIGURATION
BY
TURNING
ON
OFF
SERVERS
BELOW
IS
THE
ALGORITHM
USED
BY
THE
FRONT
END
TO
DECIDE
WHICH
SERVERS
WILL
BE
TURNED
ON
OFF
THE
ALGORITHM
TURNS
MACHINES
ON
AND
OFF
IN
A
SPECIFIC
ORDER
WHICH
IS
BASED
ON
THE
POWER
EFFICIENCY
OF
SERVERS
I
E
THE
INTEGRAL
OF
POWER
CONSUMPTION
VERSUS
LOAD
IN
OUR
CASE
ACCORDING
TO
THE
VALUES
OF
FIGURE
THE
ORDERING
FOR
OUR
CLUSTER
IS
TRANSMETA
BLUE
SILVER
GREEN
IN
SOME
SITUATIONS
WE
MAY
NEED
TO
CHANGE
THE
ORDER
AT
RUNTIME
AS
EXPLAINED
LATER
THE
FRONT
END
TURNS
ON
SERVERS
AS
THE
CLUSTER
LOAD
IN
CREASES
HOWEVER
SINCE
THE
BOOT
TIME
IS
NOT
NEGLIGI
BLE
WE
NEED
TO
TURN
MACHINES
ON
BEFORE
THEY
ARE
ACTUALLY
NEEDED
FOR
THIS
REASON
THE
FRONT
END
MAINTAINS
A
VARIABLE
CALLED
MAX
LOAD
INCREASE
WHICH
SPECIFIES
THE
MAXIMUM
LOAD
VARIATION
THAT
THE
CLUSTER
IS
PREPARED
TO
SUSTAIN
DURING
MONITOR
PERIOD
THIS
IS
ESSENTIALLY
THE
MAXIMUM
SLOPE
OF
THE
LOAD
CHARACTERIZATION
FOR
THE
CLUSTER
THE
ON
OFF
POLICY
RELIES
ON
TWO
TABLES
COMPUTED
OF
FLINE
THE
FIRST
TABLE
CALLED
MANDATORY
SERVERS
KEEPS
THE
LOAD
AT
WHICH
TO
TURN
SERVERS
ON
AND
IS
USED
TO
DE
TERMINE
THE
LOWEST
NUMBER
OF
SERVERS
NEEDED
AT
A
CERTAIN
LOAD
TO
SATISFY
THE
QOS
REQUIREMENT
FOR
EXAMPLE
CON
SIDER
A
CLUSTER
WITH
THREE
SERVERS
HAVING
MAXIMUM
LOADS
MAX
MAX
AND
MAX
RESPECTIVELY
SUPPOSE
THAT
MONITOR
PERIOD
IS
SEC
ONDS
MAX
LOAD
INCREASE
IS
EQUAL
TO
AND
THE
BOOT
TIME
IS
SECONDS
FOR
EVERY
MACHINE
IDEALLY
WE
NEED
ONLY
ONE
SERVER
WHEN
THE
CLUSTER
LOAD
IS
LESS
THAN
TWO
SERVERS
WHEN
LOAD
IS
BETWEEN
AND
AND
ALL
SERVERS
WHEN
LOAD
IS
HIGHER
THAN
HOWEVER
IF
WE
ACCOUNT
FOR
THE
TIME
TO
BOOT
A
NEW
MACHINE
AND
WE
SUPPOSE
THAT
THE
CLUSTER
LOAD
IS
CHECKED
PERIODICALLY
EVERY
MONITOR
PERIOD
SECONDS
THE
TABLE
BECOMES
MANDATORY
SERVERS
THUS
THE
FIRST
SERVER
IS
ALWAYS
ON
WHEREAS
THE
SECOND
AND
THIRD
SERVERS
ARE
TURNED
ON
WHEN
THE
CLUSTER
LOAD
REACHES
AND
RESPECTIVELY
IN
FACT
IF
WE
CONSIDER
THE
BOOT
TIME
OF
A
NEW
SERVER
WE
HAVE
TO
ACCOUNT
FOR
A
POTENTIAL
J
THE
SECOND
TABLE
CALLED
POWER
SERVERS
PRECOMPUTES
THE
NUMBER
OF
SERVERS
NEEDED
TO
MINIMIZE
THE
POWER
CON
SUMPTION
FOR
A
GIVEN
LOAD
UNLIKE
THE
PREVIOUS
TABLE
THIS
TABLE
IS
COMPUTED
CONSIDERING
THE
POWER
CONSUMPTION
OF
SERVERS
AND
IS
USED
TO
DISTRIBUTE
THE
CURRENT
LOAD
AMONG
ACTIVE
SERVERS
FOR
A
GIVEN
VALUE
OF
N
WE
COMPUTE
THE
POWER
CONSUMPTION
OF
THE
CLUSTER
AS
FOLLOWS
WE
START
CON
SIDERING
A
LOAD
EQUAL
TO
ZERO
AND
WE
INCREASE
ITS
VALUE
IN
LOAD
INCREMENT
INCREMENTS
FOR
ANY
INCREMENT
OF
THE
LOAD
WE
EVALUATE
WHICH
SERVER
CAN
HANDLE
IT
IN
ORDER
TO
MIN
IMIZE
THE
OVERALL
ENERGY
CONSUMPTION
TO
DETERMINE
THE
LOAD
AT
WHICH
N
SERVERS
BECOME
MORE
POWER
EFFICIENT
THAN
N
WE
FOLLOW
THIS
PROCEDURE
CON
SIDERING
BOTH
CASES
OF
N
AND
N
MACHINES
RESPECTIVELY
THE
LOAD
AT
WHICH
N
SERVERS
CONSUME
LESS
POWER
THAN
N
SERVERS
IS
THE
VALUE
AFTER
WHICH
THE
N
TH
SERVER
IS
TURNED
ON
THE
SERVER
TO
BE
TURNED
ON
IS
THE
NEXT
ONE
ACCORDING
TO
THE
POWER
EFFICIENCY
ORDER
THE
COMPLEXITY
OF
COMPUTING
THE
TWO
TABLES
IS
O
N
WHERE
N
IS
THE
NUMBER
OF
SERVERS
FOR
MANDATORY
SERVERS
AND
O
N
M
FOR
POWER
SERVERS
I
LOAD
INCREMENT
IN
OUR
CLUSTER
THE
TIME
TO
COMPUTE
THESE
TWO
TABLES
WAS
LESS
THAN
WHICH
WAS
NEGLIGIBLE
COMPARED
TO
MONITOR
PERIOD
THAT
IS
IN
THE
RANGE
OF
SECONDS
THUS
THIS
COMPUTATION
CAN
ALSO
BE
PERFORMED
ONLINE
FOR
EXAMPLE
A
NEW
ORDERING
OF
THE
SERVERS
AND
AN
ONLINE
RECALCULATION
OF
THE
TABLES
BECOME
NECESSARY
WHEN
A
SERVER
CRASHES
A
HIGH
LEVEL
VIEW
OF
THE
FRONT
END
ON
OFF
POLICY
IS
PRESENTED
IN
FIGURE
EVERY
MONITOR
PERIOD
SECONDS
THE
LOAD
IS
ESTIMATED
ACCORDING
TO
EQUATION
THEN
THE
REQUEST
COUNTERS
ARE
RESET
THE
NUMBER
OF
MANDATORY
SERVERS
NMANDATORY
IS
DETERMINED
BY
A
LOOKUP
IN
THE
MANDATORY
SERVERS
TABLE
IF
NMANDATORY
IS
HIGHER
THAN
THE
CURRENT
NUMBER
OF
ACTIVE
SERVERS
NCURRENT
ALL
NEEDED
SERVERS
ARE
IMMEDIATELY
TURNED
ON
EACH
SERVER
CAN
BE
IN
ONE
OF
THE
FOLLOWING
STATES
OFF
BOOT
ON
OR
SHUTDOWN
AFTER
RECEIVING
THE
BOOT
COM
MAND
SUCH
AS
A
WAKE
ON
LAN
PACKET
THE
SERVER
I
MOVES
FROM
THE
OFF
TO
THE
BOOT
STATE
IT
STAYS
IN
THIS
STATE
FOR
BOOT
TIMEI
SECONDS
I
E
UNTIL
IT
STARTS
THE
SERVER
PROCESS
THEN
INFORMS
THE
FRONT
END
THAT
IT
IS
AVAILABLE
FOR
PROCESSING
MOVING
TO
THE
ON
STATE
WHEN
SERVER
I
IS
SHUTDOWN
IT
STAYS
IN
THE
SHUTDOWN
STATE
FOR
SHUTDOWN
TIMEI
SECONDS
AFTER
LOAD
INCREASE
EQUAL
TO
BOOT
TIME
MAX
LOAD
INCREASE
THAT
THE
FRONT
END
CHANGES
ITS
STATE
TO
OFF
MOREOVER
IF
WE
SUPPOSE
THAT
THE
LOAD
IS
CHECKED
PERIODI
CALLY
EVERY
MONITOR
PERIOD
SECONDS
WE
HAVE
TO
INTRODUCE
THE
VARIABLE
CMD
IN
FIGURE
CAN
HAVE
THREE
DIFFERENT
VALUES
NONE
BOOT
OR
SHUTDOWN
THIS
VARIABLE
ALLOWS
TO
MANDATORY
SERVERS
AND
POWER
SERVERS
CONTAIN
THE
OPTIMAL
TRANSITION
POINTS
IN
THE
DISCRETE
SPACE
FOR
CONTINU
OUS
SPACE
SEE
IN
PRACTICE
HOWEVER
POWER
FUNCTIONS
MAY
HAVE
CONCAVE
REGIONS
THIS
MEANS
THAT
A
SERVER
WITH
AN
ABRUPT
POWER
INCREASE
AT
SOME
LOAD
X
MAY
NOT
BE
ALLOCATED
MORE
THAN
X
LOAD
EVEN
THOUGH
THE
POWER
MAY
BECOME
FLAT
ABOVE
X
Ǫ
MAKING
IT
A
GOOD
TARGET
FOR
LOAD
ALLOCATION
A
SIMPLE
FIX
TO
THE
PROBLEM
IS
TO
CONSIDER
THE
AVERAGE
POWER
CONSUMPTION
OVER
A
LARGER
INTERVAL
RATHER
THAN
THE
EXACT
VALUE
AT
EACH
LOAD
THIS
EFFECTIVELY
RESULTS
IN
SMOOTHING
THE
POWER
FUNCTIONS
IN
OUR
CASE
ALTHOUGH
THE
MEASURED
POWER
FUNCTIONS
HAVE
CONCAVE
REGIONS
WE
HAVE
FOUND
THAT
NO
SMOOTHING
WAS
NECESSARY
REQUEST
DISTRIBUTION
POLICY
THE
FRONT
END
DISTRIBUTES
THE
INCOMING
REQUESTS
TO
A
SUBSET
OF
THE
CURRENT
SERVERS
THAT
ARE
IN
THE
ON
STATE
LOAD
ALLOCATION
IS
A
TABLE
CONTAINING
THE
ESTIMATED
LOAD
ALLOCATED
TO
EACH
SERVER
AND
IS
COMPUTED
WITH
THE
SAME
PROCEDURE
USED
TO
DETERMINE
THE
POWER
SERVERS
TABLE
IN
O
M
N
TIME
THE
LOAD
ALLOCATION
IS
COMPUTED
EVERY
MONITOR
PERIOD
SECONDS
AFTER
THE
ON
OFF
DECISIONS
ANOTHER
TABLE
CALLED
LOAD
ACCUMULATED
STORES
THE
AC
CUMULATED
LOAD
OF
EACH
SERVER
AND
IS
RESET
AFTER
COMPUTING
LOAD
ALLOCATION
THE
SERVER
I
WITH
THE
MINIMUM
WEIGHT
W
LOAD
ACCUMULATEDI
I
LOAD
ALLOCATIONI
FIGURE
ON
OFF
POLICY
DESCRIBE
THE
USE
OF
THRESHOLDS
WHEN
TURNING
ON
OFF
SERVERS
IF
NO
SERVER
IS
IN
TRANSITION
I
E
ALL
SERVERS
ARE
IN
THE
ON
OR
OFF
STATES
A
SERVER
MAY
BE
TURNED
ON
OR
OFF
AS
DECIDED
AFTER
A
LOOKUP
IN
THE
POWER
SERVERS
TABLE
TO
BE
CONSER
VATIVE
ONLY
ONE
SERVER
AT
A
TIME
IS
TURNED
ON
OR
OFF
SERVER
I
IS
TURNED
OFF
IF
THE
SYSTEM
IS
IN
STATE
CMD
SHUTDOWN
FOR
AT
LEAST
TIME
SHUTDOWNI
TIME
BOOTI
CONSECUTIVE
SEC
ONDS
WHICH
IS
THE
RENT
TO
OWN
THRESHOLD
SEE
STEP
FIG
URE
SIMILARLY
SERVER
I
IS
TURNED
ON
IF
CMD
BOOT
FOR
TIME
BOOTI
CONSECUTIVE
SECONDS
SEE
STEP
FIGURE
NOTICE
THAT
THESE
THRESHOLDS
DO
NOT
APPLY
TO
THE
MANDATORY
SERVERS
WHICH
ARE
STARTED
IMMEDIATELY
THE
RUNNING
TIME
OF
THE
ONLINE
PART
OF
THE
ALGORITHM
EVERY
MONITOR
PERIOD
SEC
ONDS
IS
NEGLIGIBLE
BECAUSE
IT
IS
IN
THE
MICROSECOND
RANGE
THE
COMPLEXITY
IS
O
N
BUT
CAN
BE
IMPROVED
TO
O
BY
INCREASING
THE
TABLE
SIZE
FROM
N
TO
M
THAT
IS
STORING
ALL
ENTRIES
IN
AN
ARRAY
FOR
CONVEX
AND
LINEAR
POWER
FUNCTIONS
TABLES
GETS
THE
NEXT
REQUEST
NOTICE
THAT
WI
CAN
BE
HIGHER
THAN
WHEN
THE
LOAD
IS
UNDERESTIMATED
THE
SERVER
THAT
RE
CEIVES
THE
REQUEST
UPDATES
ITS
ACCUMULATED
LOAD
AND
THUS
IN
CREASES
ITS
WEIGHT
BY
ADDING
ASTATIC
MONITOR
PERIOD
OR
ADYNAMIC
MONITOR
PERIOD
DEPENDING
ON
THE
REQUEST
TYPE
THE
COMPLEXITY
TO
FIND
THE
SERVER
WITH
MINIMUM
WEIGHT
IS
O
N
WITH
A
STRAIGHTFORWARD
IMPLEMENTATION
BUT
CAN
BE
IMPROVED
TO
O
LOGN
USING
A
TREE
IMPLEMENTATION
ISSUES
WE
IMPLEMENTED
OUR
PM
SCHEME
IN
THE
APACHE
WEB
SERVER
WE
CREATED
AN
APACHE
MODULE
CALLED
MOD
ON
OFF
WHICH
MAKES
ON
OFF
DECISIONS
MOREOVER
WE
EXTENDED
AN
EXISTING
MODULE
MOD
BACKHAND
TO
SUPPORT
OUR
DISTRIBUTION
POLICY
MOD
BACKHAND
IS
A
MODULE
RESPONSIBLE
FOR
LOAD
DISTRI
BUTION
IN
APACHE
CLUSTERS
IT
ALLOWS
SERVERS
TO
EXCHANGE
INFORMATION
ABOUT
THEIR
CURRENT
USAGE
OF
RESOURCES
IT
ALSO
PROVIDES
A
SET
OF
CANDIDACY
FUNCTIONS
TO
FORWARD
REQUESTS
FROM
AN
OVERLOADED
SERVER
TO
OTHER
LESS
UTILIZED
SERVERS
EX
AMPLES
OF
SUCH
FUNCTIONS
ARE
BYLOAD
WHICH
SELECTS
AS
CAN
DIDATE
THE
LEAST
LOADED
SERVER
AND
BYCOST
WHICH
CONSIDERS
A
COST
FOR
EACH
REQUEST
WE
ADDED
A
NEW
CANDIDACY
FUNCTION
CALLED
BYENERGY
TO
IMPLEMENT
OUR
REQUEST
DISTRIBUTION
POLICY
NOTICE
THAT
ONLY
FRONT
END
MACHINES
USE
THIS
FUNCTION
IN
ADDITION
SERVERS
PROVIDE
SOME
FEEDBACK
ABOUT
THEIR
CURRENT
REAL
TIME
UTILIZA
TION
AS
EXPLAINED
IN
SECTION
TO
FRONT
ENDS
WE
USED
THIS
FEEDBACK
TO
PREVENT
THE
OVERLOADING
OF
THE
SERVERS
IN
PARTIC
ULAR
THE
SERVER
WITH
THE
MINIMUM
WI
IS
SELECTED
PROVIDING
THAT
IT
IS
NOT
OVERLOADED
THE
MOD
ON
OFF
MODULE
COMMUNICATES
WITH
MOD
BACKHAND
THROUGH
SHARED
MEMORY
ON
INITIALIZA
TION
MOD
ON
OFF
ACQUIRES
SERVER
INFORMATION
AND
COMPUTES
BOTH
MANDATORY
SERVERS
AND
POWER
SERVERS
TABLES
MOD
ON
OFF
EXECUTES
PERIODICALLY
EVERY
MONITOR
PERIOD
SECONDS
ON
EACH
INVOCATION
IT
PERFORMS
THE
FOLLOWING
TASKS
A
COMPUTES
THE
CURRENT
LOAD
BASED
ON
THE
COUNTERS
NSTATIC
AND
NDYNAMIC
THAT
ARE
INCREMENTED
IN
THE
APACHE
POST
READ
REQUEST
PHASE
B
LOOKS
UP
IN
THE
TABLE
TO
DETERMINE
THE
NUMBER
OF
SERVERS
NEEDED
FOR
THE
NEXT
PERIOD
C
COMPUTES
THE
LOAD
ALLOCATION
TABLE
FOR
THE
ACTIVE
SERVERS
NOT
SHOWN
IN
FIGURE
D
TURNS
ON
BY
SEND
ING
WAKE
ON
LAN
PACKETS
AND
OFF
BY
INVOKING
SPECIAL
CGI
SCRIPTS
SERVERS
AND
FINALLY
E
RESETS
THE
COUNTERS
NSTATIC
NDYNAMIC
AND
LOAD
ACCUMULATED
IN
ADDITION
IT
DISPLAYS
AT
RUNTIME
THE
ESTIMATED
POWER
AND
ENERGY
CONSUMPTION
OF
EACH
SERVER
BASED
ON
THE
POWER
VS
LOAD
AND
LOAD
ACCUMULATED
TABLES
SERVER
POWER
MANAGEMENT
IN
ADDITION
TO
FRONT
END
DIRECTED
CLUSTER
RECONFIGURATIONS
I
E
TURNING
ON
OFF
MACHINES
THE
SERVERS
PERFORM
THEIR
OWN
LOCAL
PM
TO
REDUCE
POWER
CONSUMPTION
OF
UNUTILIZED
OR
UNDERUTILIZED
RESOURCES
WE
PRESENT
AN
EXAMPLE
OF
A
QOS
AWARE
DVS
SCHEME
AND
WE
DISCUSS
AN
IMPLEMENTATION
US
ING
THE
APACHE
WEBSERVER
LOCAL
DVS
POLICY
WE
RELY
ON
A
LOCAL
REAL
TIME
SCHEME
WHERE
EACH
REQUEST
IS
AN
APERIODIC
TASK
AND
IS
ASSIGNED
A
DEADLINE
EACH
REQUEST
TYPE
HAS
A
DEADLINE
TO
ALLOW
FOR
MORE
ACCURATE
LOAD
ESTIMATION
WE
CONSIDER
A
SOFT
REAL
TIME
SYSTEM
IN
WHICH
THE
SCHED
ULE
IS
NOT
GENERATED
BY
A
REAL
TIME
SCHEDULER
AND
THE
COM
PUTATION
TIME
CI
IS
THE
AVERAGE
EXECUTION
TIME
I
E
ASTATIC
OR
ADYNAMIC
NOT
THE
WORST
CASE
LET
DI
BE
THE
TIME
RE
MAINING
TO
THE
DEADLINE
THEN
THE
REAL
TIME
UTILIZATION
OF
A
U
AND
SETS
THE
CPU
FREQUENCY
TO
THE
CLOSEST
VALUE
HIGHER
THAN
U
FMAX
NOTE
THAT
DVS
ARCHITECTURES
MAY
HAVE
INEFFICIENT
OPER
ATING
FREQUENCIES
WHICH
EXIST
WHEN
THERE
ARE
HIGHER
FREQUENCIES
THAT
CONSUME
LESS
ENERGY
A
SIMPLE
ONLINE
TOOL
FOR
INEFFICIENT
FREQUENCY
ELIMINATION
HAS
BEEN
PROVIDED
IN
REMOVAL
OF
INEFFICIENT
OPERATING
FREQUENCIES
IS
THE
FIRST
STEP
IN
ANY
DVS
SCHEME
THIS
WAS
NOT
NECESSARY
IN
OUR
SERVERS
BECAUSE
SURPRISINGLY
ALL
FREQUENCIES
WERE
EFFICIENT
ALTHOUGH
WE
HAD
A
DIFFERENT
EXPERIENCE
WITH
OTHER
SYSTEMS
WE
TESTED
IMPLEMENTATION
ISSUES
WE
IMPLEMENTED
AN
APACHE
MODULE
CALLED
MOD
CPUFREQ
RESPONSIBLE
FOR
CPU
SPEED
SETTINGS
AT
THE
USER
LEVEL
ON
ATHLON
MACHINES
THE
CPU
SPEED
WAS
CHANGED
BY
WRITING
TO
THE
SYS
FILE
SYSTEM
USING
THE
CPUFREQ
INTERFACE
ON
THE
TRANSMETA
MACHINE
THE
SPEED
WAS
CHANGED
BY
WRITING
TO
A
MODEL
SPECIFIC
REGISTER
MSR
SINCE
THE
REGISTER
CANNOT
BE
WRITTEN
FROM
USER
LEVEL
WE
ADDED
TWO
SYSTEM
CALLS
FOR
SETTING
AND
READING
ITS
VALUE
AFTER
DETECTING
THE
AVAILABLE
FREQUENCIES
OUR
MODULE
CREATES
AN
APACHE
PROCESS
THAT
PERIODICALLY
SETS
THE
CPU
FREQUENCY
ACCORDING
TO
THE
CURRENT
VALUE
OF
U
WE
CHOSE
AS
PERIOD
TO
MATCH
ANY
DEFAULT
LINUX
KERNEL
THE
MEASURED
OVERHEAD
FOR
CHANGING
VOLTAGE
FREQUENCY
IN
THE
MACHINES
IS
APPROXIMATELY
TO
COMPUTE
U
THE
MODULE
NEEDS
TO
KNOW
THE
TYPE
I
E
STATIC
OR
DYNAMIC
AND
THE
ARRIVAL
TIME
OF
EACH
REQUEST
AT
EVERY
REQUEST
ARRIVAL
CALLED
APACHE
POST
READ
REQUEST
PHASE
THE
ARRIVAL
TIME
AND
THE
DEADLINE
ARE
RECORDED
WITH
ΜS
ACCURACY
AND
STORED
IN
A
HASH
TABLE
IN
SHARED
MEMORY
THE
REQUEST
TYPE
IS
DETERMINED
FROM
THE
NAME
OF
THE
RE
QUESTED
FILE
THUS
A
SINGLE
QUEUE
TRAVERSAL
IS
NECESSARY
TO
COMPUTE
U
IN
FACT
THE
CURRENT
VALUE
OF
U
DEPENDS
ON
ALL
QUEUED
REQUESTS
THEREFORE
THE
COMPLEXITY
IS
O
R
WHERE
R
IS
THE
NUMBER
OF
REQUESTS
QUEUED
THE
OVERHEAD
IS
NEG
LIGIBLE
REQUESTS
ARE
REMOVED
FROM
THE
QUEUE
AFTER
BEING
SERVED
CALLED
APACHE
LOGGING
REQUEST
PHASE
A
PROBLEM
WE
ENCOUNTERED
DURING
THE
IMPLEMENTATION
WAS
THAT
OUR
SCHEME
WORKED
VERY
WELL
EXCEPT
FOR
FAST
MA
CHINES
SERVING
A
LARGE
AMOUNT
OF
SMALL
STATIC
PAGES
IN
THIS
CASE
THOSE
MACHINES
WERE
NOT
INCREASING
THEIR
SPEED
RE
SULTING
IN
A
LARGE
NUMBER
OF
DROPPED
REQUESTS
A
FURTHER
INVESTIGATION
REVEALED
THAT
THE
VALUE
OF
U
WAS
CLOSE
TO
ZERO
WE
DID
NOT
SEE
THIS
PHENOMENON
ON
SLOWER
MACHINES
SUCH
AS
TRANSMETA
NOR
USING
BIGGER
PAGES
THE
PROBLEM
WAS
THAT
SERVER
IS
DEFINED
AS
U
CI
I
DI
THE
REQUESTS
WERE
SERVED
TOO
FAST
IN
APPROXIMATELY
ΜS
IF
THE
CPU
IS
THE
BOTTLENECK
OF
THE
SYSTEM
AS
IN
OUR
CASE
THE
CPU
FREQUENCY
TO
HANDLE
THIS
RATE
OF
REQUESTS
IS
U
FMAX
WHERE
FMAX
IS
THE
HIGHEST
POSSIBLE
FREQUENCY
OF
THE
CPU
EACH
SERVER
PERIODICALLY
COMPUTES
ITS
UTILIZATION
SUCH
SHORT
REQUESTS
WERE
QUEUED
SERVED
AND
REMOVED
FROM
THE
QUEUE
BEFORE
OTHER
REQUESTS
WERE
ADDED
TO
THE
QUEUE
THUS
AT
ANY
TIME
ONLY
A
FEW
REQUESTS
USUALLY
JUST
ONE
WAS
IN
THE
QUEUE
AND
WHEN
MOD
CPUFREQ
RECOMPUTED
THE
UTILIZA
TION
IT
RESULTED
IN
AN
UNDERESTIMATION
OF
U
IN
OTHER
WORDS
EVEN
THOUGH
THE
REQUESTS
WERE
RECEIVED
AND
QUEUED
AT
THE
OS
LEVEL
APACHE
WAS
NOT
ABLE
TO
SEE
THEM
BECAUSE
IT
IS
A
USER
LEVEL
SERVER
AND
IT
HAS
NO
INFORMATION
ABOUT
REQUESTS
STORED
AT
THE
OS
LEVEL
WE
CALLED
THIS
PROBLEM
THE
SHORT
REQUEST
OVERLOAD
PROBLEM
PHENOMENON
A
SIMPLE
FIX
WAS
TO
COMPUTE
THE
UTILIZATION
ALSO
OVER
A
RECENT
INTERVAL
OF
TIME
INTERVAL
SIZE
WE
USED
TRANSMETA
BLUE
SILVER
URECENT
NSTATICASTATIC
NDYNAMICADYNAMIC
INTERVAL
SIZE
GREEN
WE
WOULD
LIKE
TO
KEEP
THE
SERVER
UTILIZATION
URECENT
BE
LOW
A
CERTAIN
THRESHOLD
WE
USED
THRESHOLD
THE
MINIMUM
FREQUENCY
THAT
DOES
THAT
IS
URECENT
FMAX
THUS
OUR
MODULE
SETS
THE
CPU
SPEED
TO
MAX
U
URECENT
FMAX
TABLE
IDLE
BUSY
POWER
CONSUMPTION
IN
WATTS
FOR
EACH
NOTE
THAT
SHARMA
ET
AL
WORK
WITH
A
KERNEL
WEBSERVER
KHTTPD
AWARE
OF
SMALL
REQUESTS
AT
THE
OS
LEVEL
HAS
A
NICE
SYNERGY
WITH
OUR
APPROACH
AND
COULD
BE
USED
IN
LIEU
OF
OUR
SCHEME
EXPLORING
THE
COMPOSITION
OF
OUR
CLUSTER
CONFIGURATION
AND
SHARMA
OR
OTHER
SIMILAR
DVS
WORK
IS
LEFT
FOR
FUTURE
WORK
THE
PROBLEM
WITH
INCLUDING
SUCH
WORK
IN
OUR
SCHEME
IS
EXACTLY
THE
REASON
WHY
THE
AUTHORS
DISCON
TINUED
THE
DEVELOPMENT
OF
KHTTPD
THE
DIFFICULTY
OF
MAIN
TAINING
DEVELOPING
AND
DEBUGGING
A
KERNEL
LEVEL
SERVER
EVALUATION
SERVER
AT
EACH
FREQUENCY
SERVERS
PROPOSED
IN
WHICH
WE
IMPLEMENTED
AT
USER
LEVEL
IN
OUR
MOD
CPUFREQ
MODULE
THIS
SCHEME
ADJUSTS
THE
SPEED
OF
THE
PROCESSOR
TO
THE
MINIMUM
SPEED
THAT
MAINTAINS
A
QUANTITY
CALLED
SYNTHETIC
UTILIZATION
BELOW
THE
THEORETICAL
UTILIZATION
BOUND
UBOUND
THAT
ENSURES
THAT
ALL
DEADLINES
ARE
MET
TO
EVALUATE
OUR
QOS
AWARE
PM
SCHEME
WE
USED
A
SMALL
CLUSTER
COMPOSED
BY
ONE
FRONT
END
AND
DIFFERENT
SERVERS
EVERY
MACHINE
RAN
GENTOO
LINUX
AS
OPERATING
SYSTEM
AND
APACHE
SERVERS
THE
PARAMETERS
OF
THE
MACHINES
ARE
SHOWN
IN
TABLES
AND
THE
CLUSTER
HAS
BEEN
TESTED
USING
CLIENTS
CONNECTED
TO
THE
CLUSTER
WITH
A
GBE
INTERFACE
AND
GBPS
SWITCH
THE
CLIENTS
GENERATE
UP
TO
REQUESTS
PER
SECOND
WHICH
COR
RESPONDS
TO
A
TOTAL
MAXIMUM
CLUSTER
LOAD
EQUAL
TO
ALL
LOADS
WERE
NORMALIZED
TO
THAT
OF
SILVER
MACHINE
A
TOTAL
CLUSTER
LOAD
OF
OR
CORRESPONDS
ON
AVERAGE
TO
RE
QUESTS
SECOND
CONSIDERING
REQUEST
TYPES
HOWEVER
GREATLY
IMPROVES
THE
PREDICTION
AS
REQUESTS
SECOND
MAY
CORRE
SPOND
TO
A
LOAD
RANGING
FROM
IF
NDYNAMIC
TO
DEFAULT
LINUX
SHARMA
SCHEME
OUR
SCHEME
LOAD
FIGURE
COMPARISON
OF
DVS
POLICIES
IF
NSTATIC
WE
ASSIGNED
DEADLINES
OF
AND
FOR
REQUESTS
OF
STATIC
AND
DYNAMIC
PAGES
WE
SET
MAX
LOAD
INCREASE
THEREFORE
WE
HAD
MANDATORY
SERVERS
012
AND
POWER
SERVERS
DVS
POLICY
AS
FIRST
EXPERIMENT
WE
EVALUATED
THE
EFFECTIVENESS
OF
OUR
LOCAL
DVS
SCHEME
WE
COMPARED
OUR
MOD
CPUFREQ
MODULE
WITH
THE
DEFAULT
PM
IN
LINUX
I
E
HALT
INSTRUCTION
WHEN
IDLE
AND
WITH
SHARMA
DVS
SCHEME
FOR
QOS
AWARE
WEB
THE
MEASURED
POWER
CONSUMPTION
OF
EACH
SCHEME
ON
THE
BLUE
MACHINE
IS
SHOWN
AS
FUNCTION
OF
THE
LOAD
IN
FIGURE
THE
GRAPH
SHOWS
THAT
OUR
SCHEME
OUTPERFORMS
THE
OTHER
SCHEMES
ESPECIALLY
FOR
THE
MID
RANGE
LOAD
VALUES
HIGHER
SAVINGS
ARE
OBTAINED
ON
MACHINES
WITH
A
MORE
CONVEX
POWER
FUNCTION
THE
POWER
FUNCTION
OF
THE
BLUE
MACHINE
IS
RATHER
LINEAR
SEE
FIGURE
IN
FACT
FOR
A
RATE
OF
REQUESTS
SEC
APPROXIMATELY
LOAD
THE
AVERAGE
PROCESSOR
FREQUENCY
IS
USING
OUR
SCHEME
AND
USING
SHARMA
SCHEME
BUT
THE
AMOUNT
OF
ENERGY
SAVED
IS
ONLY
IMPOR
TANTLY
WE
OBSERVED
THAT
BOTH
SCHEMES
MAINTAINED
THE
QOS
LEVEL
ABOVE
EVEN
AT
THE
HIGHEST
LOAD
TRANSMETA
TRANSMETA
CRUSOE
MB
KB
BLUE
AMD
ATHLON
MOBILE
MB
SILVER
AMD
ATHLON
KB
GREEN
FRONT
END
AMD
ATHLON
AMD
ATHLON
MOBILE
KB
MB
NOT
APPLICABLE
TABLE
PARAMETERS
OF
THE
MACHINES
OF
THE
CLUSTER
OVERALL
SCHEME
TO
EVALUATE
THE
OVERALL
SCHEME
WE
PERFORMED
MANY
EX
PERIMENTS
WITH
AND
WITHOUT
THE
CLUSTER
WIDE
PM
SCHEME
ON
OFF
SCHEME
AND
WITH
AND
WITHOUT
THE
LOCAL
PM
SCHEME
DVS
SCHEME
FOR
EACH
LOAD
VALUE
WE
MEASURED
THE
POWER
CONSUMPTION
OF
THE
ENTIRE
MACHINE
NOT
ONLY
CPU
FOR
EACH
SCHEME
INDEPENDENTLY
SEE
FIGURE
FOR
FAIRNESS
WE
USED
THE
LOAD
BALANCING
POLICY
IN
SECTION
FOR
ALL
THE
SCHEMES
THE
ON
OFF
POLICY
ALLOWS
A
STRIKING
REDUCTION
OF
THE
EN
ERGY
CONSUMPTION
FOR
LOW
VALUES
OF
THE
LOAD
BECAUSE
OB
VIOUSLY
IT
ALLOWS
TO
TURN
OFF
UNUTILIZED
SERVERS
IN
FIG
URE
WE
CAN
SEE
THAT
WHEN
LOAD
THE
CLUSTER
CONSUMP
TION
IS
AROUND
BECAUSE
EACH
ATHLON
SERVER
CONSUMES
WHEN
IN
THE
OFF
STATE
AND
THE
TRANSMETA
ALSO
CONSUMES
WHEN
IN
THE
ON
STATE
THE
DVS
TECHNIQUE
INSTEAD
HAS
ITS
BIGGEST
IMPACT
WHENEVER
A
NEW
SERVER
IS
TURNED
ON
SINCE
NOT
ALL
ACTIVE
SERVERS
ARE
FULLY
UTILIZED
HOWEVER
ITS
IM
PORTANCE
DECREASES
AS
THE
UTILIZATION
OF
THE
ACTIVE
SERVERS
INCREASES
FOR
HIGH
VALUES
OF
THE
LOAD
IN
OUR
CASE
AT
BEING
ON
AT
ALL
TIMES
AND
RUNNING
AT
MAXIMUM
FREQUENCY
ADDING
DVS
LOCAL
PM
HAD
A
VERY
SMALL
IMPACT
ON
THE
DE
LAY
WITH
THE
AVERAGE
DELAY
MEASURED
AT
HOWEVER
WITH
ON
OFF
SCHEME
WE
MEASURED
AN
AVERAGE
DELAY
EQUAL
TO
WITHOUT
DVS
AND
WITH
DVS
IN
BOTH
CASES
THE
AVERAGE
DELAY
WAS
NOT
HIGHER
THAN
OF
THE
NO
PM
DELAY
AND
WAS
QUITE
SMALL
WITH
RESPECT
TO
DEADLINES
REQUEST
TYPE
REQUEST
TYPE
MS
CGI
KB
HTML
MS
CGI
KB
HTML
MS
CGI
KB
HTML
MS
CGI
KB
HTML
MS
CGI
KB
HTML
KB
HTML
KB
HTML
KB
HTML
KB
HTML
KB
HTML
KB
HTML
KB
HTML
KB
HTML
KB
HTML
KB
HTML
KB
HTML
ABOVE
KB
HTML
TABLE
WEB
SERVER
STATISTICS
PERCENTAGE
OF
ACCESSES
APPROXIMATE
SIZE
FOR
STATIC
PAGES
AND
RUNNING
TIME
FOR
DYNAMIC
PAGES
OR
HIGHER
ALL
SERVERS
ARE
ON
THEREFORE
THE
ON
OFF
TECHNIQUE
DOES
NOT
ALLOW
TO
REDUCE
ENERGY
CONSUMPTION
IN
THOSE
SIT
UATIONS
HOWEVER
THERE
IS
STILL
ROOM
FOR
THE
DVS
TECHNIQUE
THAT
BECOMES
MORE
IMPORTANT
THAN
THE
ON
OFF
TECHNIQUE
THE
ENERGY
CONSUMPTION
OF
ALL
SERVERS
WITHOUT
ANY
POWER
MANAGEMENT
SCHEME
WAS
H
ON
AVERAGE
WE
MEASURED
ENERGY
SAVINGS
OF
USING
DVS
USING
ON
OFF
AND
USING
BOTH
SCHEMES
IT
IS
WORTH
NOTING
THAT
THE
FRONT
END
ESTIMATION
OF
THE
TO
TAL
ENERGY
CONSUMED
WHEN
USING
DVS
WAS
EXTREMELY
AC
CURATE
THE
DIFFERENCE
FROM
THE
ACTUAL
VALUES
WAS
LESS
THAN
FOR
EXAMPLE
WHEN
USING
THE
ON
OFF
SCHEME
THE
MEA
SURED
VALUE
WAS
H
WHILE
THE
FRONT
END
ESTIMATED
VALUE
WAS
H
THE
RESOLUTION
OF
OUR
POWER
ENERGY
METER
IS
H
LOAD
TO
MEASURE
THE
IMPACT
OF
CLUSTER
WIDE
AND
LOCAL
PM
SCHEMES
IN
THE
LOSS
OF
QOS
WE
RAN
MANY
FOUR
HOUR
EX
PERIMENTS
WITH
WORKLOADS
DERIVED
FROM
ACTUAL
WEBSERVER
TRACES
AND
GENERATED
WITH
THE
SAME
SHAPE
OF
STATISTICS
TAKEN
FROM
OUR
CS
PITT
EDU
DOMAIN
SEE
TABLE
THE
AVERAGE
DE
LAY
OBSERVED
AT
THE
CLIENT
SIDE
WITHOUT
ANY
PM
SCHEME
WAS
A
SMALL
RESPONSE
TIME
IS
DUE
TO
ALL
MACHINES
FIGURE
EVALUATION
OF
CLUSTER
WIDE
AND
LOCAL
TECHNIQUES
CONCLUSIONS
AND
FUTURE
WORK
WE
HAVE
PRESENTED
A
NEW
QOS
AWARE
POWER
MANAGE
MENT
SCHEME
THAT
COMBINES
CLUSTER
WIDE
ON
OFF
AND
LO
CAL
DVS
POWER
MANAGEMENT
TECHNIQUES
IN
THE
CONTEXT
OF
HETEROGENEOUS
CLUSTERS
WE
HAVE
ALSO
DESCRIBED
AND
EVALU
ATED
AN
IMPLEMENTATION
OF
THE
PROPOSED
SCHEME
USING
THE
APACHE
WEBSERVER
IN
A
SMALL
REALISTIC
CLUSTER
OUR
EXPERIMENTAL
RESULTS
SHOW
THAT
A
OUR
LOAD
ESTIMA
TION
IS
VERY
ACCURATE
B
THE
ON
OFF
POLICY
ALLOWS
A
STRIKING
REDUCTION
OF
THE
POWER
CONSUMPTION
C
DVS
IS
VERY
IM
PORTANT
WHENEVER
A
NEW
SERVER
IS
TURNED
ON
OR
AS
SHOWN
BEFORE
WHEN
ALL
SERVERS
ARE
ON
D
AS
EXPECTED
FOR
HIGH
VALUES
OF
THE
LOAD
THE
ON
OFF
TECHNIQUE
DOES
NOT
HELP
TO
REDUCE
ENERGY
CONSUMPTION
BUT
THERE
IS
STILL
ROOM
FOR
DVS
USING
BOTH
TECHNIQUES
WE
SAVED
UP
TO
OF
THE
TOTAL
ENERGY
WITH
A
LIMITED
LOSS
IN
TERMS
OF
QOS
IN
THE
WORST
CASE
THE
AVERAGE
DELAY
WAS
INCREASED
BY
AT
MOST
AND
WAS
STILL
VERY
SMALL
WHEN
COMPARED
TO
THE
DEADLINES
AS
IMMEDIATE
FUTURE
WORK
WE
PLAN
TO
INVESTIGATE
THE
USE
OF
BOTH
SUSPEND
TO
DISK
AND
SUSPEND
TO
RAM
TECHNIQUES
TO
REDUCE
THE
TIME
TO
BOOT
AND
SHUTDOWN
A
SERVER
WE
ALSO
PLAN
AN
INTEGRATION
OF
OUR
CLUSTER
PM
SCHEMES
WITH
OTHER
GRID
LIKE
OR
CLUSTER
E
G
CONDOR
LOAD
BALANCING
SCHEMES
MARK
D
HILL
UNIVERSITY
OF
WISCONSIN
MADISON
MICHAEL
R
MARTY
GOOGLE
AUGMENTING
AMDAHL
LAW
WITH
A
COROLLARY
FOR
MULTICORE
HARDWARE
MAKES
IT
RELEVANT
TO
FUTURE
GENERATIONS
OF
CHIPS
WITH
MULTIPLE
PROCESSOR
CORES
OBTAINING
OPTIMAL
MULTICORE
PERFORMANCE
WILL
REQUIRE
FURTHER
RESEARCH
IN
BOTH
EXTRACTING
MORE
PARALLELISM
AND
MAKING
SEQUENTIAL
CORES
FASTER
WE
ENTER
THE
MULTICORE
ERA
WE
RE
AT
AN
INFLECTION
POINT
IN
THE
COMPUTING
LANDSCAPE
COMPUTING
VENDORS
HAVE
ANNOUNCED
CHIPS
WITH
MULTIPLE
PROCESSOR
CORES
MOREOVER
VENDOR
ROAD
MAPS
PROMISE
TO
REPEATEDLY
DOUBLE
THE
NUMBER
OF
CORES
PER
CHIP
THESE
FUTURE
CHIPS
ARE
VARIOUSLY
CALLED
CHIP
MULTIPROCESSORS
MULTICORE
CHIPS
AND
MANY
CORE
CHIPS
DESIGNERS
MUST
SUBDUE
MORE
DEGREES
OF
FREEDOM
FOR
MULTICORE
CHIPS
THAN
FOR
SINGLE
CORE
DESIGNS
THEY
MUST
ADDRESS
SUCH
QUESTIONS
AS
HOW
MANY
CORES
SHOULD
CORES
USE
SIMPLE
PIPELINES
OR
POWERFUL
MULTI
ISSUE
PIPELINE
DESIGNS
SHOULD
CORES
USE
THE
SAME
OR
DIFFERENT
MICRO
ARCHITECTURES
IN
ADDITION
DESIGNERS
MUST
CONCURRENTLY
MANAGE
POWER
FROM
BOTH
DYNAMIC
AND
STATIC
SOURCES
ALTHOUGH
ANSWERING
THESE
QUESTIONS
FOR
TODAY
MULTI
CORE
CHIP
WITH
TWO
TO
EIGHT
CORES
IS
CHALLENGING
NOW
IT
WILL
BECOME
MUCH
MORE
CHALLENGING
IN
THE
FUTURE
SOURCES
AS
VARIED
AS
INTEL
AND
THE
UNIVERSITY
OF
CALIFORNIA
BERKELEY
PREDICT
A
HUNDRED
IF
NOT
A
THOUSAND
CORES
AS
THE
AMDAHL
LAW
SIDEBAR
DESCRIBES
THIS
MODEL
HAS
IMPORTANT
CONSEQUENCES
FOR
THE
MULTICORE
ERA
TO
COMPLEMENT
AMDAHL
SOFTWARE
MODEL
WE
OFFER
A
COROL
LARY
OF
A
SIMPLE
MODEL
OF
MULTICORE
HARDWARE
RESOURCES
OUR
RESULTS
SHOULD
ENCOURAGE
MULTICORE
DESIGNERS
TO
VIEW
THE
ENTIRE
CHIP
PERFORMANCE
RATHER
THAN
FOCUSING
ON
CORE
EFFICIENCIES
WE
ALSO
DISCUSS
SEVERAL
IMPORTANT
LIMITATIONS
OF
OUR
MODELS
TO
STIMULATE
DISCUSSION
AND
FUTURE
WORK
A
COROLLARY
FOR
MULTICORE
CHIP
COST
TO
APPLY
AMDAHL
LAW
TO
A
MULTICORE
CHIP
WE
NEED
A
COST
MODEL
FOR
THE
NUMBER
AND
PERFORMANCE
OF
CORES
THAT
THE
CHIP
CAN
SUPPORT
WE
FIRST
ASSUME
THAT
A
MULTICORE
CHIP
OF
GIVEN
SIZE
AND
TECHNOLOGY
GENERATION
CAN
CONTAIN
AT
MOST
N
BASE
CORE
EQUIVALENTS
WHERE
A
SINGLE
BCE
IMPLEMENTS
THE
BASELINE
CORE
THIS
LIMIT
COMES
FROM
THE
RESOURCES
A
CHIP
DESIGNER
IS
WILLING
TO
DEVOTE
TO
PROCESSOR
CORES
WITH
CACHES
IT
DOESN
T
INCLUDE
CHIP
RESOURCES
EXPENDED
ON
SHARED
CACHES
INTERCONNECTION
NETWORKS
MEMORY
CONTROLLERS
AND
SO
ON
RATHER
WE
SIMPLISTICALLY
ASSUME
THAT
THESE
NONPROCESSOR
RESOURCES
ARE
ROUGHLY
CONSTANT
IN
THE
MUL
TICORE
VARIATIONS
WE
CONSIDER
WE
ARE
AGNOSTIC
ON
WHAT
LIMITS
A
CHIP
TO
N
BCES
IT
MIGHT
BE
POWER
AREA
OR
SOME
COMBINATION
OF
POWER
AREA
AND
OTHER
FACTORS
SECOND
WE
ASSUME
THAT
MICRO
ARCHITECTS
HAVE
TECHNIQUES
FOR
USING
THE
RESOURCES
OF
MULTIPLE
BCES
TO
CREATE
A
CORE
WITH
GREATER
SEQUENTIAL
PERFORMANCE
LET
THE
PERFORMANCE
OF
A
SINGLE
BCE
CORE
BE
WE
ASSUME
THAT
ARCHITECTS
CAN
EXPEND
THE
RESOURCES
OF
R
BCES
TO
CREATE
A
POWERFUL
CORE
WITH
SEQUENTIAL
PER
FORMANCE
PERF
R
ARCHITECTS
SHOULD
ALWAYS
INCREASE
CORE
RESOURCES
WHEN
PERF
R
R
BECAUSE
DOING
SO
SPEEDS
UP
BOTH
SEQUENTIAL
AND
PARALLEL
EXECUTION
WHEN
PERF
R
R
HOWEVER
THE
TRADE
OFF
BEGINS
INCREASING
CORE
PERFORMANCE
AIDS
SEQUENTIAL
EXECUTION
BUT
HURTS
PARALLEL
EXECUTION
IEEE
PUBLISHED
BY
THE
IEEE
COMPUTER
SOCIETY
JULY
AMDAHL
LAW
EVERYONE
KNOWS
AMDAHL
LAW
BUT
QUICKLY
FOR
GETS
IT
THOMAS
PUZAK
IBM
MOST
COMPUTER
SCIENTISTS
LEARN
AMDAHL
LAW
IN
SCHOOL
LET
SPEEDUP
BE
THE
ORIGINAL
EXECUTION
TIME
DIVIDED
BY
AN
ENHANCED
EXECUTION
TIME
THE
MODERN
VERSION
OF
AMDAHL
LAW
STATES
THAT
IF
YOU
ENHANCE
A
FRACTION
F
OF
A
COMPUTATION
BY
A
SPEEDUP
THE
OVERALL
SPEEDUP
IS
FINALLY
AMDAHL
ARGUED
THAT
TYPICAL
VALUES
OF
F
WERE
LARGE
ENOUGH
TO
FAVOR
SINGLE
PROCESSORS
DESPITE
THEIR
SIMPLICITY
AMDAHL
ARGUMENTS
HELD
AND
MAINFRAMES
WITH
ONE
OR
A
FEW
PROCES
SORS
DOMINATED
THE
COMPUTING
LANDSCAPE
THEY
ALSO
LARGELY
HELD
IN
THE
MINICOMPUTER
AND
PERSONAL
COMPUTER
ERAS
THAT
FOLLOWED
AS
RECENT
TECHNOLOGY
TRENDS
USHER
US
INTO
THE
MULTICORE
ERA
AMDAHL
LAW
IS
STILL
RELEVANT
SPEEDUP
ENHANCED
F
AMDAHL
EQUATIONS
ASSUME
HOWEVER
THAT
THE
COMPUTATION
PROBLEM
SIZE
DOESN
T
CHANGE
WHEN
RUNNING
ON
ENHANCED
MACHINES
THAT
IS
THE
FRAC
AMDAHL
LAW
APPLIES
BROADLY
AND
HAS
IMPORTANT
COROLLARIES
SUCH
AS
ATTACK
THE
COMMON
CASE
WHEN
F
IS
SMALL
OPTIMI
ZATIONS
WILL
HAVE
LITTLE
EFFECT
THE
ASPECTS
YOU
IGNORE
ALSO
LIMIT
SPEEDUP
AS
APPROACHES
INFINITY
SPEEDUP
IS
BOUND
BY
F
FOUR
DECADES
AGO
GENE
AMDAHL
DEFINED
HIS
LAW
FOR
THE
SPECIAL
CASE
OF
USING
N
PROCESSORS
CORES
IN
PARALLEL
WHEN
HE
ARGUED
FOR
THE
SINGLE
PROCESSOR
APPROACH
VALIDITY
FOR
ACHIEVING
LARGE
SCALE
COMPUTING
CAPA
BILITIES
HE
USED
A
LIMIT
ARGUMENT
TO
ASSUME
THAT
A
FRACTION
F
OF
A
PROGRAM
EXECUTION
TIME
WAS
INFINITELY
PARALLELIZABLE
WITH
NO
SCHEDULING
OVERHEAD
WHILE
THE
REMAINING
FRACTION
F
WAS
TOTALLY
SEQUENTIAL
WITHOUT
PRESENTING
AN
EQUATION
HE
NOTED
THAT
THE
SPEEDUP
ON
N
PROCESSORS
IS
GOVERNED
BY
TION
OF
A
PROGRAM
THAT
IS
PARALLELIZABLE
REMAINS
FIXED
JOHN
GUSTAFSON
ARGUED
THAT
AMDAHL
LAW
DOESN
T
DO
JUSTICE
TO
MASSIVELY
PARALLEL
MACHINES
BECAUSE
THEY
ALLOW
COMPUTATIONS
PREVIOUSLY
INTRAC
TABLE
IN
THE
GIVEN
TIME
CONSTRAINTS
A
MACHINE
WITH
GREATER
PARALLEL
COMPUTATION
ABILITY
LETS
COM
PUTATIONS
OPERATE
ON
LARGER
DATA
SETS
IN
THE
SAME
AMOUNT
OF
TIME
WHEN
GUSTAFSON
ARGUMENTS
APPLY
PARALLELISM
WILL
BE
AMPLE
IN
OUR
VIEW
HOW
EVER
ROBUST
GENERAL
PURPOSE
MULTICORE
DESIGNS
SHOULD
ALSO
OPERATE
WELL
UNDER
AMDAHL
MORE
PESSIMISTIC
ASSUMPTIONS
REFERENCES
G
M
AMDAHL
VALIDITY
OF
THE
SINGLE
PROCESSOR
APPROACH
TO
ACHIEVING
LARGE
SCALE
COMPUTING
CAPABILITIES
PROC
AM
FEDERATION
OF
INFORMATION
PROCESSING
SOCIETIES
CONF
AFIPS
PRESS
PP
J
L
GUSTAFSON
REEVALUATING
AMDAHL
LAW
COMM
ACM
SPEEDUP
PARALLEL
F
N
N
MAY
PP
OUR
EQUATIONS
ALLOW
PERF
R
TO
BE
AN
ARBITRARY
FUNC
TION
BUT
ALL
OUR
GRAPHS
FOLLOW
SHEKHAR
AND
AND
SHOW
TWO
HYPOTHETICAL
SYMMETRIC
MULTICORE
CHIPS
FOR
N
ASSUME
PERF
R
IN
OTHER
WORDS
WE
ASSUME
EFFORTS
UNDER
AMDAHL
LAW
THE
SPEEDUP
OF
A
SYMMETRIC
THAT
DEVOTE
R
BCE
RESOURCES
WILL
RESULT
IN
SEQUENTIAL
MULTICORE
CHIP
RELATIVE
TO
USING
ONE
SINGLE
BCE
CORE
PERFORMANCE
THUS
ARCHITECTURES
CAN
DOUBLE
PER
DEPENDS
ON
THE
SOFTWARE
FRACTION
THAT
IS
PARALLELIZABLE
FORMANCE
AT
A
COST
OF
FOUR
BCES
TRIPLE
IT
FOR
NINE
BCES
AND
SO
ON
WE
TRIED
OTHER
SIMILAR
FUNCTIONS
FOR
EXAMPLE
R
BUT
FOUND
NO
IMPORTANT
CHANGES
TO
OUR
RESULTS
SYMMETRIC
MULTICORE
CHIPS
A
SYMMETRIC
MULTICORE
CHIP
REQUIRES
THAT
ALL
ITS
CORES
HAVE
THE
SAME
COST
A
SYMMETRIC
MULTICORE
CHIP
WITH
A
F
THE
TOTAL
CHIP
RESOURCES
IN
BCES
N
AND
THE
BCE
RESOURCES
R
DEVOTED
TO
INCREASE
EACH
CORE
PERFOR
MANCE
THE
CHIP
USES
ONE
CORE
TO
EXECUTE
SEQUENTIALLY
AT
PERFORMANCE
PERF
R
IT
USES
ALL
N
R
CORES
TO
EXE
CUTE
IN
PARALLEL
AT
PERFORMANCE
PERF
R
N
R
OVERALL
WE
GET
RESOURCE
BUDGET
OF
N
BCES
FOR
EXAMPLE
CAN
SUP
PORT
CORES
OF
ONE
BCE
EACH
FOUR
CORES
OF
FOUR
BCES
EACH
OR
IN
GENERAL
N
R
CORES
OF
R
BCES
EACH
OUR
EQUA
SPEEDUP
SYMMETRIC
F
F
R
PERF
R
PERF
R
N
TIONS
AND
GRAPHS
USE
A
CONTINUOUS
APPROXIMATION
INSTEAD
CONSIDER
FIGURE
IT
ASSUMES
A
SYMMETRIC
MULTI
OF
ROUNDING
DOWN
TO
AN
INTEGER
NUMBER
OF
CORES
FIGURES
CORE
CHIP
OF
N
BCES
AND
PERF
R
THE
X
AXIS
COMPUTER
FIGURE
VARIETIES
OF
MULTICORE
CHIPS
A
SYMMETRIC
MULTICORE
WITH
ONE
BASE
CORE
EQUIVALENT
CORES
B
SYMMETRIC
MULTICORE
WITHFOURFOUR
BCECORES
AND
C
ASYMMETRICMULTICOREWITHONEFOUR
BCECOREAND
ONE
BCECORES
THESEFIGURESOMITIMPORTANT
STRUCTURES
SUCH
AS
MEMORY
INTERFACES
SHARED
CACHES
AND
INTERCONNECTS
AND
ASSUME
THAT
AREA
NOT
POWER
IS
A
CHIP
LIMITING
RESOURCE
FIGURE
SPEEDUP
OF
A
B
SYMMETRIC
C
D
ASYMMETRIC
AND
E
F
DYNAMIC
MULTICORE
CHIPS
WITH
N
BCES
A
C
AND
E
OR
N
BCES
B
D
AND
F
JULY
GIVES
RESOURCES
USED
TO
INCREASE
EACH
CORE
PERFOR
MANCE
A
VALUE
SAYS
THE
CHIP
HAS
BASE
CORES
WHILE
A
VALUE
OF
R
USES
ALL
RESOURCES
FOR
A
SINGLE
CORE
LINES
ASSUME
DIFFERENT
VALUES
FOR
THE
PARALLEL
FRACTION
F
THE
Y
AXIS
GIVES
THE
SYMMET
WITH
MORE
RESOURCES
TO
EXECUTE
SEQUENTIALLY
AT
PER
FORMANCE
PERF
R
IN
THE
PARALLEL
FRACTION
HOWEVER
IT
GETS
PERFORMANCE
PERF
R
FROM
THE
LARGE
CORE
AND
PERFORMANCE
FROM
EACH
OF
THE
N
R
BASE
CORES
OVERALL
WE
GET
RIC
MULTICORE
CHIP
SPEEDUP
RELATIVE
TO
ITS
RUNNING
ON
ONE
SINGLE
BCE
BASE
CORE
THE
MAXIMUM
SPEEDUP
FOR
F
FOR
EXAMPLE
IS
USING
EIGHT
CORES
AT
A
COST
OF
TWO
BCES
EACH
SPEEDUP
ASYMMETRIC
F
F
PERF
R
PERF
R
N
R
SIMILARLY
FIGURE
ILLUSTRATES
HOW
TRADEOFFS
CHANGE
WHEN
MOORE
LAW
ALLOWS
N
BCES
PER
CHIP
WITH
F
FOR
EXAMPLE
THE
MAXIMUM
SPEEDUP
OF
OCCURS
WITH
CORES
OF
BCES
EACH
RESULT
AMDAHL
LAW
APPLIES
TO
MULTICORE
CHIPS
BECAUSE
ACHIEVING
THE
BEST
SPEEDUPS
REQUIRES
FS
THAT
ARE
NEAR
THUS
FINDING
PARALLELISM
IS
STILL
CRITICAL
IMPLICATION
RESEARCHERS
SHOULD
TARGET
INCREASING
F
THROUGH
ARCHITECTURAL
SUPPORT
COMPILER
TECHNIQUES
PRO
GRAMMING
MODEL
IMPROVEMENTS
AND
SO
ON
THIS
IMPLICATION
IS
THE
MOST
OBVIOUS
AND
IMPORTANT
RECALL
HOWEVER
THAT
A
SYSTEM
IS
COST
EFFECTIVE
IF
SPEEDUP
EXCEEDS
ITS
COSTUP
MULTICORE
COSTUP
IS
THE
MULTICORE
SYSTEM
COST
DIVIDED
BY
THE
SINGLE
CORE
SYSTEM
COST
BECAUSE
THIS
COSTUP
IS
OFTEN
MUCH
LESS
THAN
N
SPEEDUPS
LESS
THAN
N
CAN
BE
COST
EFFECTIVE
RESULT
USING
MORE
BCES
PER
CORE
R
CAN
BE
OPTI
FIGURE
SHOWS
ASYMMETRIC
SPEEDUP
CURVES
FOR
N
BCES
WHILE
FIGURE
GIVES
CURVES
FOR
N
BCES
THESE
CURVES
ARE
MARKEDLY
DIFFERENT
FROM
THE
COR
RESPONDING
SYMMETRIC
SPEEDUPS
IN
FIGURES
AND
THE
SYMMETRIC
CURVES
TYPICALLY
SHOW
EITHER
IMMEDIATE
PERFORMANCE
IMPROVEMENT
OR
PERFORMANCE
LOSS
AS
THE
CHIP
USES
MORE
POWERFUL
CORES
DEPENDING
ON
THE
LEVEL
OF
PARALLELISM
IN
CONTRAST
ASYMMETRIC
CHIPS
OFTEN
REACH
A
MAXIMUM
SPEEDUP
BETWEEN
THE
EXTREMES
RESULT
ASYMMETRIC
MULTICORE
CHIPS
CAN
OFFER
POTEN
TIAL
SPEEDUPS
THAT
ARE
MUCH
GREATER
THAN
SYMMETRIC
MULTICORE
CHIPS
AND
NEVER
WORSE
FOR
F
AND
N
FOR
EXAMPLE
THE
BEST
ASYMMETRIC
SPEEDUP
IS
WHEREAS
THE
BEST
SYMMETRIC
SPEEDUP
IS
IMPLICATION
RESEARCHERS
SHOULD
CONTINUE
TO
INVESTI
GATE
ASYMMETRIC
MULTICORE
CHIPS
INCLUDING
DEALING
WITH
THE
SCHEDULING
AND
OVERHEAD
CHALLENGES
THAT
AMDAHL
MAL
EVEN
WHEN
PERFORMANCE
GROWS
BY
ONLY
FOR
A
MODEL
DOESN
T
CAPTURE
GIVEN
F
THE
MAXIMUM
SPEEDUP
CAN
OCCUR
AT
ONE
BIG
CORE
N
BASE
CORES
OR
WITH
AN
INTERMEDIATE
NUMBER
OF
MIDDLE
SIZED
CORES
RECALL
THAT
FOR
N
AND
F
THE
MAXIMUM
SPEEDUP
OCCURS
USING
BCES
PER
CORE
IMPLICATION
RESEARCHERS
SHOULD
SEEK
METHODS
OF
INCREASING
CORE
PERFORMANCE
EVEN
AT
A
HIGH
COST
RESULT
MOVING
TO
DENSER
CHIPS
INCREASES
THE
LIKELI
HOOD
THAT
CORES
WILL
BE
NONMINIMAL
EVEN
AT
F
RESULT
DENSER
MULTICORE
CHIPS
INCREASE
BOTH
THE
SPEEDUP
BENEFIT
OF
GOING
ASYMMETRIC
AND
THE
OPTIMAL
PERFORMANCE
OF
THE
SINGLE
LARGE
CORE
FOR
F
AND
N
AN
EXAMPLE
NOT
SHOWN
IN
OUR
GRAPHS
THE
BEST
SPEEDUP
IS
AT
A
HYPOTHETICAL
DESIGN
WITH
ONE
CORE
OF
BCES
AND
SINGLE
BCE
CORES
IMPLICATION
RESEARCHERS
SHOULD
INVESTIGATE
METHODS
OF
SPEEDING
SEQUENTIAL
PERFORMANCE
EVEN
IF
THEY
APPEAR
MINIMAL
BASE
CORES
ARE
OPTIMAL
AT
CHIP
SIZE
N
BUT
LOCALLY
INEFFICIENT
FOR
EXAMPLE
PERF
R
THIS
IS
MORE
POWERFUL
CORES
HELP
AT
N
IMPLICATION
AS
MOORE
LAW
LEADS
TO
LARGER
MULTI
CORE
CHIPS
RESEARCHERS
SHOULD
LOOK
FOR
WAYS
TO
DESIGN
MORE
POWERFUL
CORES
ASYMMETRIC
MULTICORE
CHIPS
AN
ALTERNATIVE
TO
A
SYMMETRIC
MULTICORE
CHIP
IS
AN
ASYMMETRIC
OR
HETEROGENEOUS
MULTICORE
CHIP
IN
WHICH
ONE
OR
MORE
CORES
ARE
MORE
POWERFUL
THAN
THE
OTHERS
WITH
THE
SIMPLISTIC
ASSUMPTIONS
OF
AMDAHL
LAW
IT
MAKES
MOST
SENSE
TO
DEVOTE
EXTRA
RESOURCES
TO
INCREASE
ONLY
ONE
CORE
CAPABILITY
AS
FIGURE
SHOWS
WITH
A
RESOURCE
BUDGET
OF
N
BCES
FOR
EXAMPLE
AN
ASYMMETRIC
MULTICORE
CHIP
CAN
HAVE
ONE
FOUR
BCE
CORE
AND
ONE
BCE
CORES
ONE
NINE
BCE
CORE
AND
SEVEN
ONE
BECAUSE
THESE
METHODS
CAN
BE
GLOBALLY
EFFICIENT
AS
THEY
REDUCE
THE
SEQUENTIAL
PHASE
WHEN
THE
CHIP
OTHER
N
R
CORES
ARE
IDLE
DYNAMIC
MULTICORE
CHIPS
WHAT
IF
ARCHITECTS
COULD
HAVE
THEIR
CAKE
AND
EAT
IT
TOO
CONSIDER
DYNAMICALLY
COMBINING
UP
TO
R
CORES
TO
BOOST
PERFORMANCE
OF
ONLY
THE
SEQUENTIAL
COMPONENT
AS
FIG
URE
SHOWS
THIS
COULD
BE
POSSIBLE
WITH
FOR
EXAMPLE
THREAD
LEVEL
SPECULATION
OR
HELPER
THREADS
IN
SEQUEN
TIAL
MODE
THIS
DYNAMIC
MULTICORE
CHIP
CAN
EXECUTE
WITH
PERFORMANCE
PERF
R
WHEN
THE
DYNAMIC
TECHNIQUES
CAN
USE
R
BCES
IN
PARALLEL
MODE
A
DYNAMIC
MULTICORE
GETS
PERFORMANCE
N
USING
ALL
BASE
CORES
IN
PARALLEL
OVERALL
WE
GET
BCE
CORES
AND
SO
ON
IN
GENERAL
THE
CHIP
CAN
HAVE
N
R
CORES
BECAUSE
THE
SINGLE
LARGER
CORE
USES
R
RESOURCES
AND
SPEEDUP
DYNAMIC
F
N
R
F
F
LEAVES
N
R
RESOURCES
FOR
THE
ONE
BCE
CORES
AMDAHL
LAW
HAS
A
DIFFERENT
EFFECT
ON
AN
ASYM
PERF
R
N
FIGURE
DISPLAYS
DYNAMIC
SPEEDUPS
WHEN
USING
R
METRIC
MULTICORE
CHIP
THIS
CHIP
USES
THE
ONE
CORE
CORES
IN
SEQUENTIAL
MODE
FOR
PERF
R
FOR
N
COMPUTER
BCES
WHILE
FIGURE
GIVES
CURVES
FOR
N
BCES
AS
THE
GRAPHS
SHOW
PERFORMANCE
ALWAYS
GETS
BETTER
AS
THE
SOFTWARE
CAN
EXPLOIT
MORE
BCE
RESOURCES
TO
IMPROVE
THE
SEQUENTIAL
COMPONENT
PRACTICAL
CONSID
ERATIONS
HOWEVER
MIGHT
KEEP
R
MUCH
SMALLER
THAN
ITS
MAXIMUM
OF
N
RESULT
DYNAMIC
MULTICORE
CHIPS
CAN
OFFER
SPEED
UPS
THAT
CAN
BE
GREATER
AND
ARE
NEVER
WORSE
THAN
ASYMMETRIC
CHIPS
WITH
IDENTICAL
PERF
R
FUNCTIONS
WITH
AMDAHL
SEQUENTIAL
PARALLEL
ASSUMPTION
HOW
EVER
ACHIEVING
MUCH
GREATER
SPEEDUP
THAN
ASYMMETRIC
CHIPS
REQUIRES
DYNAMIC
TECHNIQUES
THAT
HARNESS
MORE
CORES
FOR
SEQUENTIAL
MODE
THAN
IS
POSSIBLE
TODAY
FOR
F
AND
N
FOR
EXAMPLE
EFFECTIVELY
HARNESS
ING
ALL
CORES
WOULD
ACHIEVE
A
SPEEDUP
OF
WHICH
IS
MUCH
GREATER
THAN
THE
COMPARABLE
ASYMMET
RIC
SPEEDUP
OF
THIS
RESULT
FOLLOWS
BECAUSE
WE
ASSUME
THAT
DYNAMIC
CHIPS
CAN
BOTH
GANG
ALL
RESOURCES
TOGETHER
FOR
SEQUENTIAL
EXECUTION
AND
FREE
THEM
FOR
PARALLEL
EXECUTION
IMPLICATION
RESEARCHERS
SHOULD
CONTINUE
TO
INVES
TIGATE
METHODS
THAT
APPROXIMATE
A
DYNAMIC
MULTICORE
CHIP
SUCH
AS
THREAD
LEVEL
SPECULATION
AND
HELPER
THREADS
EVEN
IF
THE
METHODS
APPEAR
LOCALLY
INEFFICIENT
AS
WITH
ASYMMETRIC
CHIPS
THE
METHODS
CAN
BE
GLOBALLY
EFFICIENT
ALTHOUGH
THESE
METHODS
CAN
BE
DIFFICULT
TO
APPLY
UNDER
AMDAHL
EXTREME
ASSUMPTIONS
THEY
COULD
FLOURISH
FOR
SOFTWARE
WITH
SUBSTANTIAL
PHASES
OF
INTERMEDIATE
LEVEL
PARALLELISM
SIMPLE
AS
POSSIBLE
BUT
NO
SIMPLER
AMDAHL
LAW
AND
THE
COROLLARY
WE
OFFER
FOR
MULTICORE
HARDWARE
SEEK
TO
PROVIDE
INSIGHT
TO
STIMULATE
DISCUSSION
AND
FUTURE
WORK
NEVERTHELESS
OUR
SPECIFIC
QUANTITATIVE
FIGURE
DYNAMIC
MULTICORE
CHIP
WITH
ONE
BCE
CORES
ESSIMISTS
WILL
BEMOAN
OUR
MODEL
SIMPLICITY
AND
LAMENT
THAT
MUCH
OF
THE
DESIGN
SPACE
WE
EXPLORE
CAN
T
BE
BUILT
WITH
KNOWN
TECHNIQUES
WE
CHARGE
YOU
THE
READER
TO
DEVELOP
BETTER
MODELS
AND
MORE
IMPORTANTLY
TO
INVENT
NEW
SOFTWARE
AND
HARDWARE
DESIGNS
THAT
REALIZE
THE
SPEEDUP
POTENTIALS
THIS
ARTICLE
DISPLAYS
MOREOVER
RESEARCH
LEADERS
SHOULD
TEMPER
THE
CURRENT
PENDULUM
SWING
FROM
THE
PAST
UNDEREMPHASIS
ON
PARALLEL
RESEARCH
TO
A
FUTURE
WITH
TOO
LITTLE
SEQUEN
TIAL
RESEARCH
TO
HELP
YOU
GET
STARTED
WE
PROVIDE
SLIDES
FROM
A
KEYNOTE
TALK
AS
WELL
AS
THE
CODE
EXAMPLES
FOR
THIS
ARTICLE
MODELS
AT
WWW
CS
WISC
EDU
MULTIFACET
AMDAHL
RESULTS
ARE
SUSPECT
BECAUSE
THE
REAL
WORLD
IS
MUCH
MORE
COMPLEX
CURRENTLY
HARDWARE
DESIGNERS
CAN
T
BUILD
CORES
THAT
ACHIEVE
ARBITRARY
HIGH
PERFORMANCE
BY
ADDING
MORE
RESOURCES
NOR
DO
THEY
KNOW
HOW
TO
DYNAMICALLY
HAR
NESS
MANY
CORES
FOR
SEQUENTIAL
USE
WITHOUT
UNDUE
PERFOR
MANCE
AND
HARDWARE
RESOURCE
OVERHEAD
MOREOVER
OUR
MODELS
IGNORE
IMPORTANT
EFFECTS
OF
DYNAMIC
AND
STATIC
POWER
AS
WELL
AS
ON
AND
OFF
CHIP
MEMORY
SYSTEM
AND
INTERCONNECT
DESIGN
SOFTWARE
IS
NOT
JUST
INFINITELY
PARALLEL
AND
SEQUENTIAL
SOFTWARE
TASKS
AND
DATA
MOVEMENTS
ADD
OVERHEAD
IT
MORE
COSTLY
TO
DEVELOP
PARALLEL
SOFTWARE
THAN
SEQUEN
TIAL
SOFTWARE
FURTHERMORE
SCHEDULING
SOFTWARE
TASKS
ON
ASYMMETRIC
AND
DYNAMIC
MULTICORE
CHIPS
COULD
BE
DIFFICULT
AND
ADD
OVERHEAD
TO
THIS
END
TOMER
MORAD
AND
HIS
AND
JOANN
PAUL
AND
BRETT
DEVELOPED
SOPHISTICATED
MODELS
THAT
QUESTION
THE
VALIDITY
OF
AMDHAL
LAW
TO
FUTURE
SYSTEMS
ESPECIALLY
EMBEDDED
ONES
ON
THE
OTHER
HAND
MORE
CORES
MIGHT
ADVANTAGEOUSLY
ALLOW
GREATER
PARALLELISM
FROM
LARGER
PROBLEM
SIZES
AS
JOHN
GUSTAFSON
ENVISIONED
ACKNOWLEDGMENTS
WE
THANK
SHAILENDER
CHAUDHRY
ROBERT
CYPHER
ANDERS
LANDIN
JOSÉ
F
MARTÍNEZ
KEVIN
MOORE
ANDY
PHELPS
THOMAS
PUZAK
PARTHA
RANGANATHAN
KARU
SANKARALINGAM
MIKE
SWIFT
MARC
TREMBLAY
SAM
WILLIAMS
DAVID
WOOD
AND
THE
WISCONSIN
MULTI
FACET
GROUP
FOR
THEIR
COMMENTS
OR
PROOFREADING
THE
US
NATIONAL
SCIENCE
FOUNDATION
SUPPORTED
THIS
WORK
IN
PART
THROUGH
GRANTS
EIA
CNS
CCR
CNS
CNS
AND
CNS
DONATIONS
FROM
INTEL
AND
SUN
MICROSYSTEMS
ALSO
HELPED
FUND
THE
WORK
MARK
HILL
HAS
SIGNIFICANT
FINANCIAL
INTEREST
IN
SUN
MICROSYSTEMS
THE
VIEWS
EXPRESSED
HEREIN
AREN
T
NECESSARILY
THOSE
OF
THE
NSF
INTEL
GOOGLE
OR
SUN
MICROSYSTEMS
JULY
A
RISING
HORIZON
IN
CHIP
FABRICATION
IS
THE
INTEGRATION
TECHNOLOGY
IT
STACKS
TWO
OR
MORE
DIES
VERTICALLY
WITH
A
DENSE
HIGH
SPEED
INTERFACE
TO
INCREASE
THE
DEVICE
DENSITY
AND
REDUCE
THE
DELAY
OF
INTERCONNECTS
SIGNIFICANTLY
ACROSS
THE
DIES
HOWEVER
A
MAJOR
CHALLENGE
IN
TECHNOLOGY
IS
THE
INCREASED
POWER
DENSITY
WHICH
GIVES
RISE
TO
THE
CONCERN
OF
HEAT
DISSIPATION
WITHIN
THE
PROCESSOR
HIGH
TEMPERATURES
TRIGGER
VOLTAGE
AND
FREQUENCY
THROTTLINGS
IN
HARDWARE
WHICH
DEGRADE
THE
CHIP
PERFORMANCE
MOREOVER
HIGH
TEMPERATURES
IMPAIR
THE
PROCESSOR
RELIABILITY
AND
REDUCE
ITS
LIFETIME
TO
ALLEVIATE
THIS
PROBLEM
WE
PROPOSE
IN
THIS
PAPER
AN
OS
LEVEL
SCHEDULING
ALGORITHM
THAT
PERFORMS
THERMAL
AWARE
TASK
SCHEDULING
ON
A
CHIP
OUR
ALGORITHM
LEVERAGES
THE
INHERENT
THERMAL
VARIATIONS
WITHIN
AND
ACROSS
DIFFERENT
TASKS
AND
SCHEDULES
THEM
TO
KEEP
THE
CHIP
TEMPERATURE
LOW
WE
OBSERVED
THAT
VERTICALLY
ADJACENT
DIES
HAVE
STRONG
THERMAL
CORRELATIONS
AND
THE
SCHEDULER
SHOULD
CONSIDER
THEM
JOINTLY
COMPARED
WITH
OTHER
INTUITIVE
ALGORITHMS
SUCH
AS
A
RANDOM
AND
A
ROUND
ROBIN
ALGORITHM
OUR
PROPOSED
ALGORITHM
BRINGS
LOWER
PEAK
TEMPERATURE
AND
AVERAGE
TEMPERATURE
ON
CHIP
MOREOVER
IT
CAN
REMOVE
ON
AVERAGE
OF
THERMAL
EMERGENCY
TIME
AND
RESULT
IN
PERFORMANCE
IMPROVEMENT
OVER
THE
BASE
CASE
ON
THERMALLY
HOMOGENEOUS
HETEROGENEOUS
FLOORPLANS
INDEX
TERMS
PROCESSORS
THERMAL
AWARE
SCHEDULING
INTRODUCTION
THE
INTEGRATION
TECHNOLOGY
HAS
GAINED
SIGNIFICANT
ATTENTION
RECENTLY
THIS
IS
A
TECHNOLOGY
THAT
REDUCES
WIRING
BOTH
WITHIN
AND
ACROSS
DISPARATE
DIES
AS
WIRING
HAS
BECOME
A
MAJOR
LATENCY
AREA
AND
POWER
OVERHEAD
IN
MODERN
MICROPROCESSORS
STUDIES
HAVE
SHOWN
THAT
WIRES
CAN
CONSUME
MORE
THAN
OF
THE
POWER
WITHIN
A
CMP
THE
TECHNOLOGY
PROVIDES
VERTICAL
STACKING
OF
TWO
OR
MORE
DIES
WITH
A
DENSE
HIGH
SPEED
INTERFACE
REDUCING
THE
WIRE
LENGTH
BY
A
FACTOR
OF
THE
SQUARE
ROOT
OF
THE
NUMBER
OF
LAYERS
USED
THIS
SIGNIFICANT
REDUCTION
LEADS
TO
IMPROVED
PERFORMANCE
AND
LOWER
POWER
DISSIPATION
ON
THE
INTERCONNECTION
ONE
KEY
CHALLENGE
IN
DIE
STACKING
IS
THE
HEAT
GENERATION
FROM
THE
INTERNAL
ACTIVE
LAYERS
BECAUSE
THE
POWER
DENSITY
PER
UNIT
VOLUME
INCREASES
DRASTICALLY
IN
THIS
EXACERBATES
EXISTING
HOTSPOTS
AND
CAN
CREATE
NEW
HOTSPOTS
WITHIN
THE
CHIP
ESPECIALLY
WHEN
ACTIVE
LOGIC
CIRCUITS
ARE
VERTICALLY
ALIGNED
FOR
EXAMPLE
THE
PEAK
TEMPERATURE
CAN
INCREASE
BY
C
IN
A
TWO
LAYER
IMPLEMENTATION
FOR
AN
ALPHA
LIKE
PROCESSOR
COMPARED
TO
A
DESIGN
OTHER
STUDIES
ON
LOGIC
LOGIC
STACKING
FLOORPLANS
ALSO
SHOW
SIMILAR
THERMAL
CONSTRAINT
THERE
ARE
EXISTING
DYNAMIC
THERMAL
MANAGEMENT
DTM
TECHNIQUES
SUCH
AS
DYNAMIC
VOLTAGE
AND
FRE
QUENCY
SCALING
DVFS
AT
THE
ARCHITECTURE
LEVEL
TO
MIT
IGATE
THIS
PROBLEM
HARDWARE
DTMS
CAN
RESPOND
TO
THERMAL
CRISIS
QUICKLY
AND
CONTROL
THE
TEMPERATURE
EFFI
CIENTLY
BY
REDUCING
THE
PROCESSOR
POWER
BUT
INEVITABLY
LEADS
TO
DEGRADED
PERFORMANCE
RECENTLY
THERE
HAS
BEEN
AN
INCREASING
INTEREST
IN
OS
ASSISTED
TASK
SCHEDULING
ON
BOTH
SINGLE
CORE
AND
CHIP
MULTIPROCESSORS
TO
ALLEVIATE
THE
THERMAL
CONDITION
ON
CHIP
OS
ASSISTED
TASK
SCHEDULING
CAN
REDUCE
THE
NUMBER
OF
TIMES
DTMS
ARE
TRIGGERED
WHILE
STILL
MEETING
THE
THERMAL
CONSTRAINT
THIS
TECHNIQUE
NOT
ONLY
IMPROVES
THE
CHIP
PERFORMANCE
UNDER
THE
SAME
THERMAL
CONSTRAINT
BUT
ALSO
DOES
NOT
REQUIRE
ANY
HARDWARE
MODIFICATIONS
HARDWARE
DTMS
ARE
ENGAGED
ONLY
WHEN
TASK
SCHEDULING
CANNOT
KEEP
THE
TEMPERATURE
BELOW
THE
THERMAL
THRESHOLD
IN
THIS
PAPER
WE
PROPOSE
A
HEURISTIC
OS
LEVEL
TECH
NIQUE
THAT
PERFORMS
THERMAL
AWARE
TASK
SCHEDULING
ON
A
CHIP
MULTIPROCESSOR
CMP
THE
PROPOSED
TECHNIQUE
AIMS
TO
IMPROVE
TASK
PERFORMANCE
BY
KEEPING
THE
TEM
PERATURE
BELOW
THE
THRESHOLD
TO
REDUCE
THE
AMOUNT
OF
DTMS
UNLIKE
PREVIOUS
THERMAL
AWARE
OS
TASK
SCHEDULER
FOR
SINGLE
CORE
OR
CMP
OUR
SCHEDULER
FOR
CHIPS
MUST
TAKE
INTO
ACCOUNT
THE
THERMAL
CONDUCTION
IN
THE
VERTICAL
DIRECTION
EARLY
STUDIES
HAVE
SHOWN
THAT
VERTI
CALLY
ADJACENT
DIES
HAVE
STRONG
THERMAL
CORRELATIONS
FOR
EXAMPLE
A
CORE
IN
ONE
LAYER
COULD
BECOME
HOT
BECAUSE
OF
A
HIGH
POWER
TASK
RUNNING
IN
THE
SAME
VERTICAL
COLUMN
BUT
AT
A
DIFFERENT
LAYER
BASED
ON
THESE
OBSERVATIONS
OUR
PROPOSED
SCHEDULER
ALWAYS
CONSIDERS
THE
AGGREGATED
POWER
OF
CORES
THAT
ARE
VERTICALLY
ALIGNED
FURTHER
WHEN
A
CORE
IS
OVERHEATED
WE
CHOOSE
TO
ENGAGE
DTM
ON
A
VERTICALLY
ALIGNED
CORE
THAT
GENERATES
THE
MOST
POWER
SUCH
AN
APPROACH
CAN
GREATLY
REDUCE
THE
TOTAL
POWER
IN
ONE
VERTICAL
COLUMN
AND
QUICKLY
COOL
DOWN
THE
OVERHEATED
CORE
OUR
EXPERIMENTS
SHOW
THAT
THE
PROPOSED
SCHEDULER
OUTPERFORMS
A
RANDOM
AND
A
ROUND
ROBIN
SCHEDULER
ON
AVERAGE
WE
CAN
REMOVE
OF
HARDWARE
DTMS
AND
OBTAIN
A
SPEEDUP
OF
OVER
THE
BASELINE
ON
THE
THERMALLY
HOMOGENEOUS
FLOORPLAN
WITH
AN
ENHANCED
VERSION
FEATURED
WITH
DYNAMIC
TUNING
SCHEME
WE
CAN
REMOVE
HARDWARE
DTMS
AND
RESULT
IN
PERFORMANCE
IMPROVEMENT
OVER
THE
BASE
CASE
ON
THE
THERMALLY
HETEROGENEOUS
FLOORPLAN
THE
REMAINDER
OF
THIS
PAPER
IS
ORGANIZED
AS
FOLLOWS
SECTION
DISCUSSES
PREVIOUS
RELATED
WORKS
SECTION
ELABORATES
THE
MOTIVATION
OF
OUR
THERMAL
AWARE
HEURISTIC
ALGORITHMS
SECTION
COMPARES
OUR
PROPOSED
SCHEDULING
ALGORITHM
WITH
OTHER
ALTERNATIVES
SECTION
INTRODUCES
THE
EXPERIMENTAL
METHODOLOGY
SECTION
REPORTS
THE
RESULTS
AND
COMPARES
DIFFERENT
ALGORITHMS
SECTION
CONCLUDES
THIS
PAPER
PRIOR
WORK
THERE
HAVE
BEEN
MANY
WORKS
RECENTLY
INVESTIGATING
THE
PERFORMANCE
POTENTIAL
AND
THE
CHALLENGES
IN
CMP
DESIGNS
MYSORE
ET
AL
PROPOSED
TO
STACK
ON
TOP
OF
A
NORMAL
PROCESSOR
A
PROFILING
DIE
THAT
CAN
IDENTIFY
MEMORY
LEAKAGE
PERFORM
DIAGNOSIS
ETC
TO
SAVE
THE
AREA
AND
POWER
ON
THE
MAIN
DIE
BLACK
ET
AL
STUDIED
THE
PERFORMANCE
ADVANTAGES
AND
THERMAL
CHALLENGES
FOR
STACKING
A
LARGE
DRAM
AND
SRAM
CACHE
ON
A
PROCESSOR
AS
WELL
AS
IMPLEMENTING
A
PROCESSOR
IN
TWO
LAYERS
XIE
ET
AL
REPORTED
THAT
THE
PEAK
TEMPERATURE
IN
A
CHIP
OF
LAYERS
AND
ONE
DIE
PER
LAYER
CAN
BE
AS
HIGH
AS
C
MORE
IMPORTANTLY
THERE
IS
ONLY
A
DIFFERENCE
OF
A
COUPLE
OF
DEGREES
IN
THE
WORST
CASE
BETWEEN
THE
HOTSPOTS
IN
THE
TOP
DIE
AND
THE
BOTTOM
DIE
THIS
INDICATES
A
STRONG
THERMAL
CORRELATION
AMONG
ADJACENT
LAYERS
IN
A
PROCESSOR
TO
ENSURE
BETTER
HEAT
DISSIPATION
IN
A
CHIP
PUTTASWAMY
ET
AL
PROPOSED
A
THERMAL
HERDING
DESIGN
WHICH
LOWERS
THE
POWER
OF
THE
CHIP
BY
SPLITTING
INDIVIDUAL
FUNCTION
UNIT
BLOCKS
ACROSS
MULTIPLE
LAYERS
AND
PLACES
THE
MOST
FREQUENTLY
SWITCHED
PART
OR
ACTIVITY
CLOSEST
TO
THE
HEAT
SINK
ALTERNATIVELY
ADDING
THERMAL
VIAS
CAN
ALSO
ALLEVIATE
THE
THERMAL
CONDITIONS
WITHIN
A
CHIP
GOPLEN
ET
AL
STUDIED
THAT
PROPER
PLACEMENT
OF
THERMAL
VIAS
IN
IC
DESIGN
CAN
OBTAIN
A
MAXIMUM
OF
REDUCTION
IN
TEMPERATURE
IN
THE
MULTICORE
DOMAIN
LOH
ET
AL
INTRODUCED
DIFFERENT
APPROACHES
FOR
IMPLEMENTING
SINGLE
CORE
AND
MULTICORE
PROCESSORS
PARTICULARLY
THEY
POINTED
OUT
THAT
STACK
ING
SEPERATE
CORES
IN
MULTICORE
DESIGN
CAN
SIGNIFICANTLY
REUSE
THE
EXISTING
DESIGNS
AND
THE
INTERFACE
BETWEEN
THE
CORES
NEEDS
NO
MORE
THAN
A
FEW
THOUSAND
CONNEC
TIONS
COMPARED
TO
THE
PREVIOUS
WORK
THIS
PAPER
FOCUSES
MAINLY
ON
SOFTWARE
APPROACHES
TO
THERMAL
MANAGEMENT
IN
CMP
THERE
HAVE
BEEN
PROPOSALS
ON
OS
ASSISTED
THERMAL
MANAGEMENT
FOR
SINGLE
CORE
CHIP
THE
HYB
DTM
TECHNIQUE
CONTROLS
TEMPERATURE
BY
LIMITING
THE
EXECUTION
OF
A
HOT
JOB
ONCE
IT
ENTERS
AN
ALARM
ZONE
THIS
IS
ACHIEVED
BY
LOWERING
THE
PRIORITY
OF
THE
HOT
JOB
SO
THAT
THE
OS
ALLOCATES
FEWER
TIMESLICES
TO
IT
AND
GIVES
COOL
JOBS
RELATIVELY
MORE
TIMESLICES
TO
EXECUTE
AN
IDEAL
SIMULATION
STUDY
WAS
PERFORMED
IN
TO
SHOW
THE
BENEFITS
OF
INTERLEAVING
HOT
AND
COOL
JOB
EXECUTIONS
HOWEVER
NEITHER
PERFORMANCE
STUDY
NOR
TASK
SWITCHING
OVERHEAD
WAS
CONSIDERED
IN
THE
MULTICORE
DOMAIN
CHOI
ET
AL
COMPARED
AND
IMPLEMENTED
THREE
DIFFERENT
TASK
SCHEDULERS
HEAT
BALANCING
DEFERRED
EXECUTION
AND
THREADING
WITH
COOL
LOOPS
TO
LEVERAGE
TEMPORAL
AND
SPA
TIAL
HEAT
SLACKS
AMONG
APPLICATION
THREADS
THE
PROPOSED
MECHANISMS
ARE
IMPLEMENTED
IN
CHONG
ET
AL
PROPOSED
A
MPSOC
THERMAL
OPTIMIZATION
AL
GORITHM
THAT
CONDUCTS
TASK
ASSIGNMENT
SCHEDULING
AND
VOLTAGE
SCALING
FOR
A
SET
OF
REAL
TIME
WORKLOADS
THE
GOAL
WAS
TO
SLOWDOWN
THE
WORKLOADS
AS
LONG
AS
THE
DEADLINES
ARE
MET
THIS
IS
QUITE
DIFFERENT
FROM
OUR
APPROACH
WHICH
FOCUSES
ON
BEST
PERFORMANCE
AND
LOW
THERMAL
PROFILE
MOTIVATION
AND
RATIONALE
A
REPRESENTATIVE
FLOORPLAN
THERE
HAVE
BEEN
A
NUMBER
OF
CMP
FLOORPLANS
AS
SHOWN
IN
FIGURE
A
C
PROPOSED
IN
LITERATURE
IN
THESE
FIGURES
CORES
ARE
STACKED
ON
EACH
OTHER
WITH
EXTENDED
CACHE
OR
MEMORY
IN
BETWEEN
WE
OBSERVED
THAT
FOR
A
STACKED
CHIP
TO
BE
SCALABLE
IN
LAYER
COUNT
IT
IS
INEVITABLE
TO
ENCOUNTER
MORE
THAN
ONE
ACTIVE
CORES
IN
ONE
VERTICAL
CORE
COLUMN
NO
MATTER
HOW
THE
ACTIVE
CORES
AND
CACHE
BANKS
ARE
PLACED
IN
THE
FLOORPLAN
FURTHER
IF
WE
LOOK
AT
THE
DISTANCE
OF
EACH
CORE
STACK
TO
THE
HEATSINK
ON
EITHER
THE
TOP
OR
BOTTOM
OF
THE
CHIP
WE
CAN
CLASSIFY
THESE
FLOOPLANS
INTO
TWO
CATEGORIES
FIG
CHIP
MULTIPROCESSOR
FLOORPLAN
OPTIONS
FIGURE
A
AND
B
REPRESENT
THE
FIRST
CATEGORY
IN
WHICH
THE
DISTANCE
OF
SOME
CORE
STACKS
E
G
CORE
STACK
IN
A
TO
THE
HEATSINK
IS
DIFFERENT
FROM
OTHERS
SUCH
AS
CORE
STACK
THESE
FLOOPLANS
ARE
THERMALLY
HETEROGENEOUS
MEANING
THAT
THE
HEAT
DISSIPATION
PROPERTY
OF
DIFFERENT
CORE
STACKS
IS
DIFFERENT
FOR
EXAMPLE
IF
THE
HEATSINK
IS
ON
THE
BOTTOM
OF
THE
STACKED
CHIP
AS
ILLUSTRATED
IN
FIGURE
CORE
STACK
IS
FURTHER
AWAY
FROM
THE
HEAT
SINK
THAN
CORE
STACK
THUS
HEAT
DISSIPATION
FOR
CORES
IN
STACK
WILL
BE
MORE
DIFFICULT
THAN
THOSE
IN
STACK
IN
CONTRAST
FIGURE
C
HAS
A
RATHER
HOMOGENEOUS
THERMAL
PROPERTY
BECAUSE
ALL
CORES
ARE
EQUALLY
DISTANT
FROM
THE
HEAT
SINK
OUR
PRELIMINARY
WORK
FOCUSED
ONLY
ON
HOMOGENEOUS
FLOORPLANS
WHILE
THIS
PAPER
CONSIDERS
BOTH
DESPITE
THESE
DISTINCTIONS
AMONG
DIFFERENT
FLOORPLANS
THEY
STILL
SHARE
SOME
IMPORTANT
PROPERTY
THE
HEAT
FROM
ANY
CORE
CAN
QUICKLY
PROPAGATE
VERTICALLY
TO
OTHER
CORES
ABOVE
AND
BELOW
FOR
ALL
THESE
FLOORPLANS
THE
CACHE
LAYERS
ALMOST
SERVE
AS
HEAT
CONDUCTANCE
BETWEEN
THE
CORE
LAYERS
CONSIDERING
THIS
COMMONALITY
AMONG
VARIOUS
FLOORPLANS
WE
CHOOSE
TO
USE
THE
FLOORPLAN
IN
FIG
URE
D
AS
A
REPRESENTATIVE
TO
FIRST
INTRODUCE
THE
GENERAL
RATIONALES
BEHIND
OUR
SCHEDULING
ALGORITHMS
THEN
WE
WILL
DISCUSS
DETAILS
OF
OUR
ALGORITHMS
FOR
HOMOGENEOUS
AND
HETEROGENEOUS
FLOOPLANS
RESPECTIVELY
IN
FIGURE
D
THERE
ARE
TWO
LAYERS
AND
EACH
LAYER
CONTAINS
FOUR
CORES
THE
CACHE
BANKS
ARE
SUBSUMED
WITHIN
EACH
CORE
VERTICALLY
ADJACENT
LAYERS
HAVE
STRONG
THERMAL
CORRELATIONS
FIG
A
FACE
TO
BACK
DIE
STACKING
STRUCTURE
ADAPTED
FROM
AND
THE
CORRESPONDING
THERMAL
MODEL
SIMILAR
TO
A
REGULAR
PROCESSOR
WHERE
HEAT
DISSIPATES
MOSTLY
IN
THE
VERTICAL
DIRECTION
CHIPS
ALSO
HAVE
BETTER
HEAT
CONDUCTIVITY
IN
VERTICAL
THAN
HORIZONTAL
DI
RECTION
THIS
IMPLIES
THAT
VERTICALLY
ADJACENT
CORES
HAVE
LARGER
THERMAL
IMPACT
AMONG
EACH
OTHER
THAN
HORIZON
TALLY
ADJACENT
CORES
WE
WILL
USE
A
SIMPLE
HEAT
TRANSFER
MODEL
TO
EXPLAIN
THIS
PHENOMENON
FIGURE
SHOWS
A
BASIC
TWO
LAYER
CHIP
STRUCTURE
ADAPTED
FROM
WE
USE
A
FACE
TO
BACK
BONDING
TECHNOLOGY
FOR
BETTER
SCALABILITY
IN
LAYER
COUNT
THE
TOP
LAYER
IS
THINNED
FOR
BETTER
ELECTRICAL
CHARACTERISTICS
AND
IMPROVED
PHYSICAL
CONSTRUCTION
OF
THE
THROUGH
SILICON
VIAS
FOR
POWER
DELIV
ERY
AND
I
O
A
THIN
DIE
ALSO
HAS
BETTER
HEAT
CONDUCTIVITY
THAN
A
THICK
DIE
SUCH
AS
THE
BOTTOM
DIE
AS
WE
CAN
SEE
THE
DISTANCE
BETWEEN
THE
TWO
ACTIVE
SILICON
DIES
ARE
VERY
SMALL
THIS
DIRECTLY
DETERMINES
THE
HIGH
HEAT
CONDUCTIVITY
BETWEEN
THE
TWO
ADJACENT
DIES
THE
HEAT
TRANSFER
MODEL
FOR
THIS
CHIP
IS
SHOWN
ON
THE
RIGHT
OF
THE
FIGURE
HERE
ONE
DIE
IS
MODELED
USING
ONE
NODE
ITS
TEMPERATURE
AND
POWER
ARE
DENOTED
AS
T
AND
P
RESPECTIVELY
REPRESENTS
THE
THERMAL
RESISTANCE
BETWEEN
THE
TWO
NODES
AMB
REPRESENTS
THE
THERMAL
RESISTANCE
BETWEEN
THE
BOTTOM
NODE
AND
THE
AMBIENT
AIR
WE
OMIT
THE
THERMAL
CAPACITANCE
HERE
TO
MODEL
ONLY
THE
STEADY
STATE
TEMPERATURE
IN
OUR
EXPERIMENTS
LATER
BOTH
THERMAL
RESISTANCE
AND
CAPACITANCE
ARE
MODELED
LET
AND
BE
THE
TEMPERATURE
RELATIVE
TO
THE
AMBIENT
AIR
IN
THE
BOTTOM
AND
TOP
NODE
RESPECTIVELY
THEN
AMB
AMB
HENCE
THE
TEMPERATURE
DIFFERENCE
BETWEEN
THE
TWO
NODES
IS
FROM
THE
PARAMETER
USED
IN
LITERATURE
IS
W
REPRESENTS
THE
POWER
GENERATED
BY
THE
ENTIRE
DIE
THIS
VALUE
IS
IN
THE
RANGE
OF
FOR
A
TYPICAL
SINGLE
CORE
PROCESSOR
THEREFORE
THE
TEMPERATURE
DIFFERENCE
BETWEEN
THE
TOP
AND
BOTTOM
DIE
IS
MERELY
A
FIG
THERMAL
CORRELATION
BETWEEN
ADJACENT
DIES
SUCH
A
STRONG
THERMAL
CORRELATION
BETWEEN
THE
TWO
ADJACENT
DIES
CAN
ALSO
BE
DEMONSTRATED
FROM
OUR
SIM
ULATION
FIGURE
SHOWS
A
TYPICAL
THERMAL
PROFILE
OF
RUNNING
EIGHT
THREADS
CONCURRENTLY
ON
EIGHT
CORES
AS
FLOORPLANED
IN
FIGURE
THE
EXPERIMENTAL
SETUP
WILL
BE
INTRODUCED
IN
SECTION
HERE
EIGHT
THREADS
ARE
EIGHT
DIFFERENT
BENCHMARKS
CHOSEN
FROM
THE
BENCHMARK
SUITE
WE
USE
WE
REFER
TO
VERTICALLY
ALIGNED
TWO
CORES
AS
A
CORE
STACK
WE
CAN
SEE
FROM
FIGURE
THAT
THERE
ARE
FOUR
DISTINCT
CLUSTERS
OF
TEMPERATURE
CURVES
EACH
CLUSTER
HAS
DRASTICALLY
DIFFERENT
VARIATIONS
FROM
OTHERS
HOWEVER
EACH
CLUSTER
HAS
TWO
LINES
THAT
ARE
VERY
CLOSE
TO
EACH
OTHER
THEIR
VARIATIONS
ARE
ALMOST
ALWAYS
SYNCHRONIZED
THE
FOUR
CLUSTERS
CORRESPOND
TO
THE
FOUR
CORE
STACKS
IN
THE
FLOORPLAN
AND
THE
TWO
LINES
IN
EACH
CLUSTER
CORRESPOND
TO
THE
TEMPERATURE
VARIATION
OF
THE
TWO
CORES
PER
STACK
THIS
EXPERIMENT
SHOWS
CLEARLY
THE
STRONG
CORRELATION
BETWEEN
ADJACENT
DIES
AS
THE
TEMPERATURES
FOR
DIFFERENT
CORE
STACKS
HARDLY
HAVE
ANY
DEPENDENCIES
AMONG
THEM
BUT
WITHIN
EACH
CORE
STACK
THE
TEMPERATURES
OF
THE
TWO
CORES
ARE
STRONGLY
CORRELATED
SUCH
CORRELATION
CAN
STILL
BE
OBSERVED
FOR
A
LAYER
FLOORPLAN
IN
OUR
EXPERIMENTS
AS
THE
INTERMEDIATE
THIN
CACHE
LAYERS
SERVE
AS
GOOD
HEAT
CONDUCTORS
AMONG
THEIR
VERTICAL
CORE
NEIGHBORS
THE
DIE
LAYERS
FURTHER
FROM
THE
HEAT
SINK
ARE
USUALLY
HOTTER
NOT
ONLY
ARE
THE
CORES
IN
A
STACK
STRONGLY
CORRELATED
IN
THEIR
TEMPERATURES
BUT
ALSO
THE
ONES
ON
THE
TOP
ARE
USUALLY
HOTTER
THAN
THOSE
NEAR
THE
BOTTOM
THIS
HAS
ALSO
BEEN
NOTED
IN
THE
LITERATURE
FOR
STEADY
STATE
TEMPERATURES
FOR
CLARITY
WE
REFER
TO
THE
CORES
FURTHER
FROM
THE
HEAT
SINK
AS
TOP
CORES
AS
ILLUSTRATED
IN
FIGURE
THE
INTUITION
IS
THAT
THE
BOTTOM
CORES
ARE
CLOSER
TO
THE
HEAT
SINK
THEREFORE
THEIR
HEAT
CAN
BE
REMOVED
MORE
QUICKLY
HERE
WE
GIVE
A
MORE
ANALYTICAL
ANALYSIS
TAKING
INTO
ACCOUNT
THE
THERMAL
CAPACITANCE
AS
WELL
SUPPOSE
IN
THE
THERMAL
MODEL
DEPICTED
IN
FIGURE
THE
THERMAL
CAPACITANCE
BETWEEN
THE
TOP
DIE
AND
AMBIENT
AIR
IS
THEN
SCHEDULING
ALGORITHMS
THE
STRONG
CORRELATIONS
AMONG
THE
CORES
IN
ONE
STACK
LEADS
TO
A
SCHEDULING
THAT
CONSIDERS
THE
ENTIRE
STACK
AS
A
WHOLE
THE
FACT
THAT
TOP
CORES
ARE
HOTTER
THAN
THE
BOTTOM
CORES
SUGGESTS
THAT
THREADS
WITHIN
A
CORE
STACK
SHOULD
BE
PLACED
WITH
CARE
FURTHERMORE
WE
TAKE
ADVANTAGE
OF
THIS
OBSERVATION
AND
INTRODUCE
A
NEW
VOLTAGE
FREQUENCY
SCALING
MECHANISM
THAT
RESULTS
IN
THE
FASTEST
TEMPERATURE
DROP
WITHIN
THE
SHORTEST
AMOUNT
OF
TIME
ONCE
THE
PEAK
TEMPERATURE
WITHIN
A
STACK
REACHES
THE
THERMAL
THRESH
OLD
IN
THIS
SECTION
WE
PRESENT
A
SEQUENCE
OF
THREAD
SCHEDULING
ALGORITHMS
SINCE
WE
HAVE
TWO
CATEGORIES
OF
FLOORPLANS
WE
WILL
SELECT
A
REPRESENTATIVE
HOMOGENEOUS
FLOORPLAN
AS
SHOWN
IN
FIGURE
C
AND
A
REPRESENTATIVE
HETEROGENEOUS
FLOOR
DT
PLAN
AS
SHOWN
IN
FIGURE
A
BOTH
THE
HOMOGENEOUS
AND
HETEROGENEOUS
FLOORPLAN
WILL
BE
APPLIED
WITH
FIVE
ALGORITHMS
BASELINE
RANDOM
ROUND
ROBIN
BALANCING
AS
MENTIONED
EARLIER
WHICH
REPRESENTS
THE
POWER
OF
A
MODERN
PROCESSOR
HAS
A
TYPICAL
VALUE
RANGE
OF
REPRESENTS
HOW
QUICKLY
TEMPERATURE
CHANGES
FROM
THE
TOP
DIE
FOR
A
THIN
DIE
WITHIN
IN
A
LAYER
CHIP
THE
THERMAL
CAPACITANCE
IS
REPORTED
AS
K
DT
IS
THE
TEMPERATURE
CHANGE
RATE
WITHIN
A
SHORT
TIME
FROM
OUR
EXPERIMENTAL
EXPERIENCE
AND
MANY
OTHER
RESULTS
IN
THE
LITERATURE
TEMPERATURE
VARIES
SLOWLY
WITH
TIME
FOR
EXAMPLE
WE
OBSERVED
A
LESS
THAN
C
INCREASE
IN
TEMPERATURE
IN
A
WINDOW
USING
HOTSPOT
FOR
CHIPS
HENCE
THE
RIGHT
HAND
SIDE
OF
EQUATION
IS
USUALLY
POSITIVE
WITH
A
RANGE
OF
THEREFORE
IS
USUALLY
HIGHER
THAN
WE
ALSO
PERFORMED
SIMULATIONS
TO
TESTIFY
THE
ABOVE
OBSERVATION
WE
INTENTIONALLY
PUT
THE
COOLEST
JOB
LOWEST
AVERAGE
TEMPERATURE
IN
A
CHIP
IN
OUR
BENCHMARK
SUITE
ON
THE
TOP
DIE
AND
THE
HOTTEST
JOB
ON
THE
BOTTOM
DIE
IN
A
CORE
STACKED
CHIP
SETTING
THE
TEMPERATURES
OF
THE
TWO
CORES
ARE
SHOWN
IN
FIGURE
WE
CAN
SEE
THAT
THE
TOP
CORE
HAS
HIGHER
TEMPERATURES
THAN
THE
BOTTOM
LAYER
ALMOST
ALWAYS
SUCH
AN
OBSERVATION
SERVES
AS
A
GUIDELINE
TO
THE
DEVELOPMENT
OF
OUR
HEURISTIC
SCHEDULING
ALGORITHM
FIG
DEMONSTRATION
OF
THE
TOP
DIE
BEING
HOTTER
THAN
THE
BOTTOM
DIE
BY
CORE
AND
OUR
PROPOSED
BALANCING
BY
STACK
ALGORITHM
THE
BASELINE
WE
USE
THE
LINUX
SCHEDULER
AS
OUR
BASELINE
ALGORITHM
IN
THIS
SCHEDULER
EACH
CORE
HAS
A
TASK
QUEUE
THAT
KEEPS
TRACK
OF
ALL
RUNNING
TASKS
ON
THAT
CORE
EACH
QUEUE
CONTAINS
TWO
PRIORITY
LISTS
ACTIVE
AND
EXPIRED
LIST
AT
RUNTIME
THE
CORE
SELECTS
TO
EXECUTE
THE
TASKS
IN
THE
ACTIVE
LIST
ACCORDING
TO
SOME
POLICY
ONCE
A
TASK
USES
UP
ITS
TIME
QUOTA
IT
IS
MOVED
TO
THE
EXPIRED
LIST
IF
ALL
TASKS
ARE
IN
THE
EXPIRED
LIST
AN
EPOCH
HAS
FINISHED
AND
THE
SCHEDULER
ITERATES
THE
PROCESS
BY
SWAPPING
THE
TWO
LISTS
EACH
TASK
IN
THE
ACTIVE
LIST
HAS
OF
CPU
CYCLE
QUOTA
DEPENDING
ON
ITS
OWN
PRIORITY
BY
DEFAULT
THE
CORE
SWITCHES
TO
A
DIFFERENT
TASK
EVERY
THUS
IN
OUR
CORE
CHIP
UPON
THE
SCHEDULING
INTERVAL
OF
EVERY
THE
SCHEDULER
SELECTS
A
TASK
FROM
EACH
CORE
ACTIVE
LIST
ACCORDING
TO
ITS
ORIGINAL
POLICY
AND
THEN
ASSIGNS
IT
TO
A
DIFFERENT
RANDOMLY
SELECTED
CORE
THIS
ALGORITHM
IS
SIMPLE
AND
HAS
LOW
CONTEXT
SWITCH
OVERHEAD
COMPARED
TO
OTHER
ALGORITHMS
INTRODUCED
LATER
HOWEVER
IT
MAY
RUN
INTO
THE
RISK
OF
PUTTING
TWO
HOT
TASKS
INTO
THE
SAME
CORE
STACK
WHICH
MAY
LEAD
TO
EXTREMELY
HIGH
TEMPERATURE
THAT
RESULTS
IN
LONG
AND
HARSH
VOLTAGE
FREQUENCY
SCALING
PENALTY
TO
BOTH
TASKS
MOREOVER
ONCE
A
POOR
SCHEDULING
HAS
BEEN
MADE
IT
STAYS
IN
THAT
CONDITION
FOR
A
LONG
PERIOD
OF
TIME
UNTIL
THE
NEXT
SCHEDULING
TIME
EXACERBATING
THE
ALREADY
SERIOUS
THERMAL
CONDITION
WITHIN
THE
CHIP
RANDOM
BASELINE
A
QUICK
FIX
OF
THE
BASELINE
SCHEDULER
IS
TO
INCREASE
THE
SCHEDULING
FREQUENCY
IN
THE
NORMAL
LINUX
OS
ANY
CONTEXT
SWITCH
INTERVAL
BETWEEN
MAY
BE
USED
A
MINIMUM
OF
IS
RECOMMENDED
TO
AVOID
UNNECESSARY
CONTEXT
SWITCH
OVERHEAD
WE
USED
AS
OUR
SCHEDULING
INTERVAL
MAINLY
DUE
TO
EXPERIMENTAL
RESTRICTION
ON
COLLECTING
THE
POWER
TRACES
ALSO
IS
CLOSE
TO
THE
THERMAL
CONSTANT
OF
THE
CORE
UNDER
TESTING
HOWEVER
THE
ALGORITHM
CAN
BE
DIRECTLY
APPLIED
TO
ANY
SCHEDULING
INTERVAL
RECOMMENDED
IN
LINUX
SUCH
AS
IF
THOSE
RESTRICTIONS
DON
T
APPLY
FURTHER
WE
TAKE
INTO
ACCOUNT
THE
EXTRA
CONTEXT
SWITCH
OVERHEAD
USING
AN
SCHEDULING
INTERVAL
DURING
OUR
EXPERIMENTS
WE
PERFORMED
A
REAL
MACHINE
MEASUREMENT
ON
THE
TIME
REQUIRED
TO
PERFORM
A
SINGLE
CONTEXT
SWITCH
FOR
AN
INTERVAL
IT
IS
A
MILD
PENALTY
THAT
CAN
BE
EASILY
OFFSET
BY
THE
PERFORMANCE
GAIN
FROM
A
BETTER
SCHEDULING
WITH
THE
IMPROVED
BASELINE
SCHEDULING
ALGORITHM
TERMED
RANDOM
TO
REFLECT
THE
SCHEDULING
DECISION
THE
CHIP
CAN
EXIT
A
POOR
THERMAL
CONDITION
DUE
TO
AN
UN
WISE
SCHEDULING
MORE
QUICKLY
RESULTING
IN
LESS
HARMFUL
IMPACT
ROUND
ROBIN
THE
RANDOM
SCHEDULER
MAY
RESULT
IN
UNEVEN
DISTRIBU
TION
OF
POWER
AND
TEMPERATURE
AS
TASKS
ARE
ASSIGNED
RANDOMLY
TO
ANY
CORE
A
ROUND
ROBIN
SCHEDULER
RR
CAN
OVERCOME
THIS
BY
ROTATING
TASKS
AMONG
CORES
IN
A
FIXED
ORDER
PERIODICALLY
THEREFORE
AFTER
N
ITERATIONS
WHERE
N
IS
THE
NUMBER
OF
CORES
EACH
TASK
HAS
EXECUTED
ON
EVERY
CORE
FOR
ONE
SCHEDULING
INTERVAL
E
G
THIS
CAN
HELP
BALANCING
THE
POWER
AND
TEMPERATURE
DISTRIBUTION
IN
THE
LONG
RUN
TEMPERATURE
BALANCING
BY
CORE
AN
ALTERNATIVE
WAY
TO
BALANCE
THE
HEAT
AMONG
THE
CORES
IS
TO
EXPLICITLY
ARRANGE
THE
TASKS
ACCORDING
TO
THEIR
POWER
CONSUMPTION
AND
THE
CORE
TEMPERATURES
ESSENTIALLY
A
HIGH
POWER
TASK
SHOULD
BE
ASSIGNED
TO
A
LOW
TEMPERATURE
CORE
AT
EACH
SCHEDULING
POINT
THE
SCHEDULER
SORTS
THE
POWER
CONSUMPTION
OF
ALL
TASKS
AND
THE
CURRENT
TEMPERA
TURE
OF
EACH
CORE
IT
THEN
ASSIGNS
THE
TASK
WITH
THE
HIGHEST
POWER
TO
THE
COOLEST
CORE
THE
HIGHEST
POWER
TO
THE
COOLEST
CORE
AND
SO
FORTH
SUCH
A
MECHANISM
SHOULD
PERFORM
A
BETTER
JOB
IN
BALANCING
THE
TEMPERATURE
DISTRIBUTION
AMONG
CORES
THAN
RR
HOWEVER
RECALL
THAT
THERE
IS
A
STRONG
THERMAL
COR
RELATION
BETWEEN
TWO
ADJACENT
LAYERS
AND
THE
CORES
IN
ONE
STACK
HAVE
ONLY
A
SMALL
DIFFERENCE
IN
TEMPERATURES
THIS
IMPLIES
THAT
IF
A
CORE
STACK
CONTAINS
THE
HOTTEST
CORE
IT
PROBABLY
ALSO
CONTAINS
THE
HOTTEST
CORE
WHEN
THE
TEMPERATURE
BALANCING
BY
CORE
ALGORITHM
IS
APPLIED
THE
TASKS
WITH
THE
LOWEST
AND
LOWEST
POWER
ARE
SCHEDULED
TO
THIS
HOT
CORE
STACK
SIMILARLY
THE
TASKS
WITH
THE
HIGHEST
AND
HIGHEST
POWER
WILL
BE
SCHEDULED
TO
THE
COOLEST
CORE
STACK
AFTER
THAT
THE
HOTTEST
COOLEST
CORE
STACK
WILL
HAVE
THE
LARGEST
TEMPERATURE
DROP
RISE
WHICH
MAY
LEAD
TO
TEMPERATURE
OSCILLATIONS
AND
TASK
THRASHING
BETWEEN
THOSE
TWO
STACKS
POTENTIALLY
LEADING
TO
MORE
THERMAL
EMERGENCIES
IN
THAT
CASE
A
RR
OR
A
RANDOM
ALGORITHM
MAY
BE
A
BETTER
SOLUTION
ANOTHER
ISSUE
WITH
THIS
MECHANISM
IS
HOW
THE
POWER
CONSUMPTION
OF
EACH
TASK
IS
OBTAINED
RECENTLY
THERE
HAS
BEEN
PROPOSALS
ON
OBTAINING
THE
RUNTIME
POWER
CONSUMPTION
OF
AN
APPLICATION
THROUGH
PROBING
THE
PER
FORMANCE
COUNTERS
IN
A
PROCESSOR
WE
ALSO
ADOPT
THIS
APPROACH
AND
ASSUME
THAT
EACH
CORE
IS
EQUIPPED
WITH
SUCH
COUNTERS
THAT
CAN
BE
USED
FOR
POWER
ESTIMATION
NOTE
THAT
OUR
POWER
ESTIMATION
NEED
NOT
BE
VERY
ACCU
RATE
AS
WE
ONLY
NEED
THE
SORTED
ORDER
OF
THE
POWER
NOT
THE
ABSOLUTE
VALUES
TEMPERATURE
BALANCING
BY
STACK
THE
CORE
BASED
TEMPERATURE
BALANCING
ALGORITHM
CAN
CREATE
THRASHING
OF
TASKS
BETWEEN
THE
HOTTEST
CORE
STACK
AND
THE
COOLEST
CORE
STACK
AS
WE
ANALYZED
EARLIER
THIS
IS
BECAUSE
THE
ALGORITHM
WHILE
TRYING
TO
BALANCE
THE
TEMPERATURES
AMONG
ALL
CORES
TREATS
EACH
CORE
INDEPEN
DENTLY
HOWEVER
AS
ADJACENT
DIES
HAVE
STRONG
TEMPER
ATURE
CORRELATIONS
CORES
IN
THE
SAME
STACK
SHOULD
IN
DEED
BE
CONSIDERED
TOGETHER
INTUITIVELY
WE
CAN
ASSUME
THAT
EACH
STACK
IS
A
SUPER
CORE
THAT
HAS
CORES
WITH
SIMILAR
TEMPERATURES
HENCE
SCHEDULING
OF
THE
TASKS
WITHIN
THREE
DIMENSIONS
CAN
BE
REDUCED
TO
SCHEDULING
OF
SUPER
TASKS
WITHIN
TWO
DIMENSIONS
APPARENTLY
A
SUPER
TASK
IS
A
SET
OF
TASKS
THAT
ARE
ASSIGNED
TO
A
SUPER
CORE
I
E
A
CORE
STACK
ALGORITHM
FOR
HOMOGENEOUS
FLOORPLANS
WE
TREAT
HOMOGENEOUS
AND
HETEROGENEOUS
FLOORPLANS
DIFFERENTLY
IN
THIS
ALGORITHM
AS
THEIR
SUPER
CORES
HAVE
DIFFERENT
THERMAL
PROPERTY
WE
FIRST
ELABORATE
ON
THE
ALGORITHM
FOR
HOMOGENEOUS
FLOORPLAN
SUPER
TASKS
LET
L
BE
THE
NUMBER
OF
LAYERS
IN
A
CHIP
AND
N
BE
THE
NUMBER
OF
CORES
PER
LAYER
AS
A
SUPER
CORE
CONTAINS
L
CORES
A
SUPER
TASK
SHOULD
ALSO
CONTAIN
L
TASKS
AND
THERE
ARE
N
SUPER
TASKS
THE
SCHEDULING
OF
N
SUPER
TASKS
AMONG
N
SUPER
CORES
IS
NOW
SIMPLY
A
PROBLEM
WHERE
A
BALANCED
TEMPERATURE
DISTRIBUTION
IS
DESIRED
HENCE
WE
FIRST
BALANCE
THE
POWER
AMONG
SUPER
TASKS
I
E
LET
EACH
SUPER
TASK
HAVE
ABOUT
THE
SAME
POWER
AND
THEN
BALANCE
THE
TEMPERATURES
AMONG
SUPER
CORES
BY
SCHEDULING
A
RELATIVELY
HIGH
POWER
SUPER
TASK
ONTO
A
RELATIVELY
COOL
SUPER
CORE
TO
BALANCE
THE
POWER
AMONG
SUPER
TASKS
WE
FIRST
SORT
THE
POWERS
OF
ALL
N
L
TASKS
LET
N
BE
N
INITIALLY
EMPTY
BINS
WE
WILL
FILL
POWERS
INTO
THESE
BINS
SUCH
THAT
EACH
BIN
WILL
CONTAIN
L
TASKS
AND
THE
TOTAL
POWERS
OF
EACH
BIN
ARE
ABOUT
THE
SAME
IN
DESCENDING
ORDER
OF
POWERS
WE
PUT
EACH
POWER
VALUE
INTO
A
BIN
THAT
HAS
THE
SMALLEST
CURRENT
TOTAL
POWER
AMONG
ALL
BINS
THIS
POLICY
ATTEMPTS
TO
REDUCE
THE
GAP
BETWEEN
THE
SMALLEST
AND
THE
LARGEST
TOTAL
POWER
IN
EACH
STEP
IN
ORDER
TO
GENERATE
A
RELATIVELY
BALANCED
TOTAL
POWER
ACROSS
N
BINS
FINALLY
ALL
POWERS
WITHIN
A
BIN
FORM
A
SUPER
TASK
WE
REMARK
THAT
OUR
POLICY
IS
ONLY
A
HEURISTIC
AS
AN
OPTIMUM
SOLUTION
MAY
REQUIRE
AN
EXHAUSTIVE
SEARCH
WE
AIM
FOR
A
SIMPLE
EFFECTIVE
YET
LOW
COMPLEXITY
HEURISTIC
BECAUSE
THE
SCHEDULER
MAKES
THE
DECISION
AT
RUNTIME
TASK
DISTRIBUTION
AMONG
AND
WITHIN
SUPER
CORES
THE
GOAL
OF
PRODUCING
SUPER
TASKS
IS
TO
GENERATE
RELATIVELY
BALANCED
POWER
DISTRIBUTION
ACROSS
SUPER
CORES
ONCE
THE
SUPER
TASKS
ARE
FORMED
WE
SUM
UP
THE
TEMPERATURES
OF
ALL
L
CORES
IN
A
SUPER
CORE
AND
SORT
THEM
SIMILAR
TO
THE
PREVIOUS
PROCEDURE
WE
ASSIGN
THE
HOTTEST
SUPER
CORE
WITH
THE
SUPER
TASK
OF
THE
LOWEST
POWER
AND
SO
ON
FIGURE
SHOWS
AN
EXAMPLE
OF
SCHEDULING
TASKS
ONTO
A
LAYER
CORE
PER
LAYER
CHIP
STEP
A
C
DEPICT
THE
PROCEDURE
EXCEPT
FOR
HOW
TASKS
WITHIN
A
SUPER
TASK
ARE
ALLOCATED
ONTO
DIFFERENT
CORES
WITHIN
A
STACK
AS
DISCUSSED
EARLIER
THE
TOP
CORES
ARE
USUALLY
HOTTER
THAN
THE
BOTTOM
CORES
IN
A
CORE
STACK
HENCE
WE
SHOULD
ALLOCATE
TASKS
OF
HIGHER
POWERS
ONTO
THE
BOTTOM
CORES
FOR
BETTER
HEAT
REMOVAL
AND
TASKS
OF
LOWER
POWERS
ONTO
THE
TOP
CORES
FOR
EXAMPLE
IF
THE
TEMPERATURES
OF
THE
CORES
FROM
BOTTOM
UP
ARE
STRICTLY
INCREASING
THEN
THE
TASKS
ALLOCATED
TO
THEM
SHOULD
HAVE
STRICTLY
DECREASING
POWERS
FROM
BOTTOM
UP
FIGURE
LAST
STEP
ILLUSTRATES
THIS
POLICY
IN
A
TWO
LAYER
FLOORPLAN
OUR
ALGORITHM
INVOLVES
MOSTLY
SORTING
OF
THE
POWERS
AND
TEMPERATURES
THEREFORE
ITS
TIME
COMPLEXITY
IS
O
NL
LOG
NL
ALGORITHM
FOR
HETEROGENEOUS
FLOORPLANS
THE
MAJOR
DIFFERENCE
IN
HETEROGENEOUS
FLOORPLANS
IS
THAT
DIFFERENT
SUPER
CORES
HAVE
DIFFERENT
HEAT
DISSIPATION
CA
PABILITY
DUE
TO
THEIR
VARYING
DISTANCES
TO
THE
HEATSINK
FOR
THIS
REASON
EVEN
IF
TWO
SUPER
CORES
ARE
OF
THE
SAME
PRESENT
TEMPERATURE
THE
SAME
SUPER
TASK
ASSIGNED
TO
THEM
WILL
RESULT
IN
DIFFERENT
FUTURE
TEMPERATURES
THERE
FORE
UNLIKE
THE
ALGORITHM
FOR
HOMOGENEOUS
FLOORPLANS
WHERE
THE
TOTAL
POWER
AMONG
SUPER
TASKS
SHOULD
BE
WELL
BALANCED
THE
TASK
BUNDLING
IN
HETEROGENEOUS
FLOORPLAN
SHOULD
INTENTIONALLY
CREATE
POWER
IMBALANCE
TO
GENERATE
BALANCED
TEMPERATURE
DISTRIBUTION
AMONG
SUPER
CORES
HOWEVER
IT
IS
DIFFICULT
TO
ESTIMATE
HOW
MUCH
POWER
DIFFERENCE
WE
SHOULD
CREATE
AMONG
SUPER
TASKS
BECAUSE
THE
FUTURE
TEMPERATURE
DEPENDS
ON
NOT
ONLY
POWER
BUT
ALSO
THE
PRESENT
TEMPERATURE
AND
DURATION
OF
THE
POWER
THEREFORE
FOR
A
GIVEN
SET
OF
POWER
VALUES
OUR
ALGORITHM
FORMS
THE
SUPER
TASKS
WITH
MINIMUM
MODERATE
AND
MAXIMUM
TOTAL
POWER
DIFFERENCE
DENOTED
AS
MIN
DIFF
MOD
DIFF
AND
MAX
DIFF
RESPECTIVELY
AND
DYNAMICALLY
MAKE
THE
SELECTION
OF
SUPER
TASKS
LET
PN
BE
N
POWERS
IN
ASCENDING
ORDER
SUPER
TASKS
WITH
MIN
DIFF
MOD
DIFF
AND
MAX
DIFF
ARE
FORMED
AS
FOLLOWS
ASSUMING
EACH
SUPER
TASK
CONTAINS
L
TASKS
MAX
DIFF
PL
PL
MOD
DIFF
PL
PL
MIN
DIFF
THE
PRINCIPLE
IS
TO
BALANCE
THE
TOTAL
POW
ERS
OF
SUPER
TASKS
THIS
IS
IDENTICAL
TO
THE
ALGORITHM
FOR
HOMOGENEOUS
FLOORPLANS
SECTION
INTUITIVELY
WHEN
THE
TEMPERATURE
DIFFERENCE
AMONG
SUPER
CORES
IS
LARGE
A
SUPER
TASK
WITH
MAX
DIFF
IS
DESIRED
HOWEVER
IF
THE
POWER
DIFFERENCE
AMONG
TASKS
IS
ALSO
LARGE
USING
THE
MAX
DIFF
MAY
BE
AN
OVERKILL
A
MOD
DIFF
COMBINATION
MAY
BE
SUFFICIENT
THEREFORE
OUR
DECISION
RELIES
ON
BOTH
THE
TEMPERATURE
GRADIENT
DENOTED
AS
T
AMONG
THE
SUPER
CORES
AND
THE
POWER
RANGE
DENOTED
AS
P
OF
THE
TASKS
LET
FIG
THE
TEMPERATURE
BALANCING
BY
STACK
ALGORITHM
SCHEDULING
PROCEDURE
TO
SUM
UP
ON
EVERY
SCHEDULING
INTERVAL
IN
OUR
CASE
THE
SCHEDULER
PERFORMS
THE
FOLLOWING
STEPS
SORT
THE
POWERS
OF
ALL
TASKS
FORM
SUPER
TASKS
SORT
THE
POWER
SUMS
OF
THE
SUPER
TASKS
FROM
LOW
TO
HIGH
FOR
EACH
SUPER
CORE
SUM
UP
THE
TEMPERATURES
FOR
ALL
CORES
SORT
THE
TEMPERATURE
SUMS
FOR
ALL
SUPER
CORES
FROM
HIGH
TO
LOW
CREATE
A
SEQUENTIAL
ONE
ONE
MAPPING
BETWEEN
THE
SORTED
SUPER
TASKS
AND
SORTED
SUPER
CORES
IN
EACH
SUPER
CORE
ALLOCATE
THE
TASKS
IN
THEIR
IN
CREASING
POWER
ORDER
ONTO
THE
CORES
WITH
DECREAS
ING
TEMPERATURE
ORDER
T
Θ
P
WHEN
Θ
IS
SMALL
THE
TEMPERATURE
GRADIENT
T
IS
REL
ATIVELY
SMALL
COMPARED
WITH
THE
POWER
RANGE
P
OF
THE
TASKS
SUPER
TASKS
OF
MIN
DIFF
ARE
MORE
APPROPRIATE
IN
THIS
SITUATION
BECAUSE
WE
NEED
ONLY
TO
PERFORM
MILD
TEMPERATURE
ADJUSTMENT
ON
THE
OTHER
HAND
WHEN
Θ
IS
LARGE
A
MORE
AGGRESSIVE
TASK
BUNDLING
TO
CREATE
POWER
DIFFERENCE
IS
NECESSARY
HENCE
THE
SELECTION
WILL
FAVOR
MAX
DIFF
DURING
OUR
EXPERIMENTS
WE
USE
TWO
HEURISTIC
Θ
VAL
UES
AND
AS
THE
THRESHOLDS
FOR
CHOOSING
DIFFERENT
ALGORITHMS
THE
CHOICE
OF
THESE
TWO
VALUES
IS
BASED
ON
OUR
EXPERIMENTAL
SETTINGS
AND
MAY
VARY
WITH
THERMAL
PROPERTIES
OF
THE
FLOORPLAN
IF
Θ
FALLS
IN
THE
RANGE
OF
MIN
DIFF
WILL
BE
CHOSEN
IF
Θ
IS
IN
THE
RANGE
OF
MOD
DIFF
IS
SELECTED
IF
Θ
IS
GREATER
THAN
MAX
DIFF
WILL
BE
SELECTED
A
NEW
THERMAL
MANAGEMENT
SCHEME
A
CRITICAL
COMPONENT
IN
COMPANY
WITH
OUR
PROPOSED
SCHEDULING
ALGORITHM
IS
HOW
TO
HANDLE
THERMAL
EMER
GENCIES
ONCE
A
CORE
TEMPERATURE
INCREASES
ABOVE
THE
HARDWARE
THRESHOLD
CONVENTIONALLY
SUCH
A
CORE
WILL
BE
PUT
TO
A
LOW
POWER
STATE
THROUGH
DVFS
IN
A
CHIP
SINCE
THE
TOP
CORES
ARE
USUALLY
HOTTER
THERMAL
EMERGENCIES
USUALLY
OCCUR
IN
THE
TOP
LAYERS
MOREOVER
OUR
SCHEDULER
PUTS
COOLER
TASKS
ON
THE
TOP
LAYERS
WHICH
MEANS
THAT
THOSE
TASKS
ARE
MORE
LIKELY
TO
UNDERGO
DVFS
LEADING
TO
THEIR
DEGRADED
PERFORMANCE
THE
PROBLEMS
OF
SUCH
CONVENTIONAL
THERMAL
MANAGE
MENT
ARE
TWOFOLD
FIRST
THE
COOLER
TASKS
COULD
BE
PENAL
IZED
MORE
OFTEN
THAN
THE
HOTTER
TASKS
WHICH
BRINGS
A
FAIRNESS
ISSUE
AMONG
DIFFERENT
TASKS
INTUITIVELY
HOTTER
TASKS
SHOULD
BE
RESTRAINED
BY
THE
SYSTEM
DUE
TO
THEIR
POTENTIAL
HARMFUL
IMPACT
TO
THE
CHIP
SECOND
APPLYING
DVFS
TO
THE
COOLER
TASKS
ON
THE
TOP
LAYERS
DOES
NOT
YIELD
THE
SAME
EFFICIENCY
AS
IN
A
CHIP
THIS
IS
BECAUSE
IT
TAKES
LONGER
TIME
TO
COOL
DOWN
THE
TOP
CORES
DUE
TO
THEIR
HIGH
POWER
NEIGHBORS
AT
THE
BOTTOM
IN
FACT
IT
IS
BECAUSE
OF
THOSE
HOT
BOTTOM
TASKS
ARE
TOP
CORES
OVERLY
HEATED
THEREFORE
A
MORE
RATIONAL
THERMAL
MANAGEMENT
SHOULD
EMPLOY
THE
SCALINGS
TO
THE
SOURCE
OF
AN
OVERHEATING
THE
BOTTOM
CORES
THAT
ARE
RUNNING
HIGH
POWER
TASKS
MORE
FORMALLY
WHEN
CORE
A
OF
A
SUPER
CORE
IS
OVERHEATED
THE
THERMAL
MANAGEMENT
WILL
SELECT
CORE
B
WITH
THE
HIGHEST
POWER
IN
TO
ENGAGE
DVFS
B
MAY
OR
MAY
NOT
BE
IDENTICAL
TO
A
SUCH
A
THERMAL
MANAGEMENT
STRATEGY
SOLVES
THE
TWO
ABOVE
PROBLEMS
EFFECTIVELY
FIRST
COOL
TASKS
ARE
NOT
PENALIZED
MORE
OFTEN
THAN
HOT
TASKS
BECAUSE
IF
A
COOL
TASK
BECOMES
A
TEMPERATURE
VICTIM
THE
HOT
TASK
THAT
CAUSED
THE
PROBLEM
IS
PENALIZED
SEC
OND
ALL
CORES
IN
INCLUDING
A
AND
B
ARE
QUICKLY
COOLED
BECAUSE
THE
TOTAL
POWER
OF
IS
REDUCED
WITH
THE
MAXIMUM
STRENGTH
FOR
EXAMPLE
IN
FIGURE
IF
THE
SUPER
CORE
CONTAINING
THE
SUPER
TASK
TRIPPED
A
THERMAL
EMERGENCY
ON
THE
CORE
AND
SUPPOSE
THE
DFVS
REDUCES
THE
POWER
OF
A
CORE
BY
HALF
THEN
OUR
SCHEME
WILL
REDUCE
THE
TOTAL
POWER
OF
THIS
SUPER
CORE
TO
WHILE
THE
CONVENTIONAL
THERMAL
MANAGEMENT
WILL
ONLY
REDUCE
IT
TO
AS
WE
CAN
SEE
IF
DVFS
IS
APPLIED
TO
A
RELATIVELY
LOW
POWER
TASK
THE
RESULT
IS
INFERIOR
BECAUSE
A
TASK
IS
BEING
PENALIZED
BUT
THE
TOTAL
POWER
IN
THE
CHIP
IS
NOT
REDUCED
AS
MUCH
THIS
IS
OFTEN
THE
CASE
FOR
THE
TEMPERATURE
BALANCING
BY
CORE
SCHEDULER
AS
IT
TENDS
TO
ALLOCATE
COOL
TASKS
ON
THE
TOP
LAYER
SINCE
IT
IS
USUALLY
HOTTER
AS
A
RESULT
OUR
MECHANISM
BRINGS
DOWN
THE
TEMPERA
TURE
OF
THE
HOTSPOT
AT
THE
HIGHEST
SPEED
RESULTING
IN
MIN
IMUM
PENALTY
TO
THE
OVERALL
PERFORMANCE
OF
THIS
SUPER
CORE
WE
WILL
SHOW
LATER
THAT
OUR
PROPOSED
TEMPERATURE
BALANCING
BY
STACK
SCHEDULING
ALGORITHM
WITH
IMPROVED
THERMAL
MANAGEMENT
RESULTS
IN
THE
MUCH
LESS
AMOUNT
OF
THERMAL
EMERGENCIES
AND
THE
MUCH
BETTER
PERFORMANCE
AMONG
ALL
PREVIOUS
SCHEMES
EXPERIMENTAL
METHODOLOGY
FLOORPLAN
SETUP
OUR
DETAILED
EXPERIMENTS
ARE
CONDUCTED
ON
FLOORPLANS
AS
DEPICTED
IN
FIGURE
A
AND
C
EACH
FLOORPLAN
HAS
FOUR
LAYERS
AND
A
TOTAL
OF
EIGHT
CORES
WE
SIMULATED
NORTHWOOD
CORES
IN
CLOCK
FREQUENCY
EACH
CORE
HAS
A
SIZE
OF
THE
REMAINING
SPACE
IS
LEFT
FOR
EXTENDED
CACHE
OR
MEMORY
SO
THE
DIE
SIZE
IS
OTHER
PHYSICAL
PARAMETERS
OF
THE
FLOORPLAN
ARE
SIMILAR
TO
SIMULATION
TOOL
AND
POWER
TRACE
COLLECTION
WE
USED
HOTSPOT
VERSION
AS
OUR
SIMULATION
TOOL
WE
CHOSE
THE
GRID
MODEL
TO
EXPERIMENT
OUR
FLOORPLAN
WE
SUBSTITUTED
THE
ORDER
RUNGE
KUTTA
METHOD
WITH
TILTS
TO
GENERATE
ACCURATE
TEMPERA
TURES
AT
HIGH
SPEED
HOTSPOT
TAKES
POWER
TRACES
AS
INPUTS
AND
TEMPERATURE
VARIATION
WITHIN
A
DIE
IS
A
SLOWER
PROCESS
COMPARED
TO
OTHER
METRICS
SUCH
AS
IPC
HENCE
WE
NEED
TO
COLLECT
EXTENDED
POWER
TRACES
TO
MODEL
REALISTIC
TEMPERATURE
VARIATIONS
SUCH
AS
WARMING
UP
AND
COOLING
DOWN
DUE
TO
TASK
SCHEDULING
AS
MENTIONED
EARLIER
WE
ADOPT
THE
RE
CENTLY
PROPOSED
PERFORMANCE
COUNTER
BASED
METHOD
TO
COLLECT
RUNTIME
HARDWARE
ACTIVITIES
OF
A
PROGRAM
ON
A
REAL
MACHINE
WE
OBTAINED
THE
POWER
MODEL
CAL
IBRATED
FROM
TO
PRODUCE
LONG
POWER
TRACES
FOR
PROGRAMS
FROM
A
LINUX
MACHINE
WITH
A
PENTIUM
CORE
THE
TRACES
CONTAIN
POWERS
FOR
EACH
FUNCTIONAL
UNIT
AND
ALL
TRACES
ARE
COMPLETE
EXECUTION
OF
THE
PROGRAMS
IN
FOR
SCHEDULING
ALGORITHMS
THAT
REQUIRE
POWER
INFORMA
TION
BALANCING
BY
CORE
AND
BALANCING
BY
STACK
WE
USE
THE
POWER
IN
THE
LAST
INTERVAL
TO
PREDICT
THE
POWER
IN
THE
NEXT
INTERVAL
THAT
IS
THE
SCHEDULING
DECISIONS
ARE
BASED
ON
LOCAL
POWER
INFORMATION
THE
SCHEDULER
NEEDS
NOT
TO
KNOW
WHETHER
A
PROGRAM
IS
GLOBALLY
HOT
OR
COOL
ALSO
WE
USE
THE
LAST
POWER
PREDICTOR
IN
THE
SCHEDULER
DUE
TO
ITS
SIMPLICITY
WE
EXPERIMENTED
WITH
MORE
COMPLEX
POWER
PREDICTORS
AND
FOUND
THAT
THEIR
OVERHEAD
BOTH
IN
TIME
AND
SPACE
IS
NOT
APPROPRIATE
FOR
ON
LINE
SCHEDULING
MOST
OF
THE
BENCHMARKS
EXHIBIT
POWER
MIS
PREDICTION
RATE
OUR
EXPERIMENTS
SHOW
THAT
AN
ERROR
WITHIN
MAKES
LAST
POWER
PREDICTION
ACCURATE
ENOUGH
FOR
THE
SCHEDULER
BENCHMARK
CLASSIFICATION
WE
FIRST
RAN
THE
POWER
TRACES
OF
EACH
BENCHMARK
TO
OBTAIN
ITS
TEMPERATURE
PROFILE
AS
SHOWN
IN
FIGURE
WE
NEXT
CLASSIFIED
THESE
BENCHMARKS
AS
HOT
POWER
INTENSIVE
COOL
POWER
NON
INTENSIVE
AND
MILD
BETWEEN
HOT
AND
COOL
AFTER
THAT
WE
CREATED
WORKLOAD
COM
BINATIONS
AS
LISTED
IN
TABLE
EACH
WITH
ONE
OR
MORE
HOT
TASKS
THE
WORKLOAD
MIXES
WITHOUT
HOT
TASKS
ARE
LESS
THERMALLY
CRITICAL
AND
THUS
ARE
NOT
CONSIDERED
HERE
IN
TABLE
WHEN
THE
NUMBER
OF
BENCHMARKS
IN
ONE
COMBINATION
IS
LESS
THAN
COPIES
OF
THE
BENCHMARKS
WILL
BE
CREATED
TO
ENSURE
THAT
EVERY
CORE
IN
THE
FLOORPLAN
HAS
ONE
TASK
TO
RUN
THIS
RESEMBLES
THE
SITUATION
OF
RUNNING
PARALLEL
THREADS
OF
THE
SAME
PROGRAM
IN
MULTICORE
PRO
CESSORS
FIG
TEMPERATURES
OF
THE
BENCHMARK
IN
TABLE
THE
COMBINATION
OF
BENCHMARKS
IN
SIMULATION
HC
CRAFTY
MCF
HC
SIXTRACK
SWIM
HHCC
BZIP
TWOLF
ART
AMMP
HMMC
WUPWISE
EQUAKE
APPLU
AMMP
HM
GZIP
MGRID
HM
PARSER
EQUAKE
HHMM
CRAFTY
GZIP
MGRID
APSI
HHMMMCCC
GAP
TWOLF
EQUAKE
MGRID
VORTEX
AMMP
ART
SWIM
HHHHCCCC
BZIP
GZIP
SIXTRACK
WUPWISE
AMMP
ART
MCF
SWIM
DVFS
IMPLEMENTATION
AND
CONTEXT
SWITCHING
OVERHEAD
WE
MODIFIED
HOTSPOT
TO
INCORPORATE
THE
HARDWARE
DVFS
EVERY
OF
A
SCHEDULING
INTERVAL
HOTSPOT
CHECKS
IF
THE
TEMPERATURE
HAS
TRESPASSED
THE
THRESHOLD
IF
SO
THE
VOLTAGE
IS
LOWERED
FROM
TO
AND
THE
FREQUENCY
IS
REDUCED
BY
WE
CHARGE
ΜS
OF
OVERHEAD
ON
EVERY
VOLTAGE
FREQUENCY
TRANSITION
DURING
A
DVFS
SCALING
IF
THE
TEMPERATURE
PERSISTS
ABOVE
THE
THRESHOLD
AFTER
ONE
THE
SCALING
CONTINUES
AND
NO
ADDITIONAL
DVFS
SWITCH
OVERHEAD
IS
CHARGED
WE
DO
NOT
CHOOSE
MULTI
LEVEL
DVFS
SCHEME
TO
AVOID
UNNECESSARY
SWITCH
OVERHEAD
IN
EVERY
LEVEL
TRANSITION
OTHER
OVERHEADS
IN
OUR
PROPOSED
SCHEDULER
IS
MAINLY
THE
INCREASED
NUMBER
OF
CONTEXT
SWITCHES
WE
MEASURED
THIS
TIME
IN
A
LINUX
MACHINE
BY
ENFORCING
A
LARGE
NUMBER
OF
CONTEXT
SWITCHES
BETWEEN
TWO
TASKS
AND
CALCULATING
THE
AVERAGE
SWITCH
TIME
FROM
THE
INCREASED
EXECUTION
TIME
OF
THESE
TWO
TASKS
THIS
QUANTITY
IN
OUR
TEST
MACHINE
IS
LATER
WE
WILL
SEE
THAT
OUR
PROPOSED
SCHEDULER
CAN
STILL
OUTPERFORM
LINUX
BASELINE
SCHEDULER
EVEN
WITH
MUCH
HIGHER
CONTEXT
SWITCH
FREQUENCY
RESULTS
AND
ANALYSIS
THE
METRICS
WE
USE
TO
EVALUATE
DIFFERENT
SCHEDULING
ALGO
RITHMS
ARE
PEAK
TEMPERATURE
OF
ALL
CORES
THE
REDUCTION
IN
TIME
THAT
A
TASK
STAYS
ABOVE
THE
THERMAL
THRESHOLD
TERMED
THERMAL
EMERGENCY
REDUCTION
IN
LATER
DISCUS
SION
AND
PERFORMANCE
IMPROVEMENT
IN
TERMS
OF
TOTAL
EXECUTION
TIME
REDUCTION
OF
ALL
TASKS
THE
PEAK
TEM
PERATURE
INDICATES
HOW
WELL
A
SCHEDULER
CAN
ALLEVIATE
THE
WORST
CASES
OF
THE
THERMAL
CONDITION
ON
CHIP
THE
THERMAL
EMERGENCY
REDUCTION
INDICATES
THE
CAPABILITY
OF
A
SCHEDULER
TO
CONTROL
THE
TEMPERATURE
BELOW
THE
HARDWARE
THRESHOLD
THE
PERFORMANCE
IMPROVEMENT
IS
THE
RESULT
OF
BOTH
THE
THERMAL
EMERGENCY
REDUCTION
AND
THE
EFFICIENCY
OF
LOWERING
THE
TEMPERATURE
DURING
AN
EMERGENCY
NEXT
WE
PRESENT
THE
RESULTS
FOR
HOMOGE
NEOUS
AND
HETEROGENEOUS
FLOORPLANS
SEPARATELY
HOMOGENEOUS
FLOORPLAN
IN
THE
FOLLOWING
WE
WILL
INTRODUCE
THE
EXPERIMENT
RESULTS
ON
THE
THERMALLY
HOMOGENEOUS
FLOORPLAN
FIVE
SCHED
ULERS
BASELINE
RANDOM
ROUNDROBIN
BALANCING
BY
CORE
BALANCING
BY
STACK
ARE
TESTED
IN
THE
EXPERIMENTS
THERMAL
BEHAVIOR
COMPARISON
OF
DIFFERENT
SCHED
ULERS
FIRST
LET
US
SEE
A
QUALITATIVE
COMPARISON
AMONG
DIFFER
ENT
SCHEDULERS
ON
THE
HOMOGENEOUS
FLOORPLAN
FIGURE
SHOWS
A
CLOSE
UP
OF
TEMPERATURE
TRACES
FOR
CORES
RUNNING
THE
HMMC
WORKLOAD
UNDER
DIFFERENT
SCHEDUL
ING
ALGORITHMS
HERE
WE
DID
NOT
ENFORCE
DVFS
AT
THE
THRESHOLD
BECAUSE
OTHERWISE
MANY
HIGH
TEMPERATURE
CURVES
WOULD
BE
CAPPED
AT
THE
THRESHOLD
AS
WE
CAN
SEE
THE
BASELINE
ALGORITHM
CAN
RESULT
IN
A
LARGE
TEMPERATURE
GRADIENT
ACROSS
DIFFERENT
CORE
STACKS
A
C
DIFFER
ENCE
BETWEEN
THE
HOTTEST
AND
THE
COOLEST
CORE
STACK
IS
OBSERVED
IN
THIS
FIGURE
FOR
RANDOM
AND
RR
SCHEDULER
THE
TEMPERATURE
GRADIENT
WITHIN
THE
CHIP
GRADUALLY
REDUCES
BECAUSE
THEIR
SCHEDULING
INTERVAL
IS
MUCH
SMALLER
THAN
THAT
IN
THE
BASELINE
TEMPERATURE
GRADI
ENT
IS
BETWEEN
C
IN
THESE
SCHEDULERS
FINALLY
BOTH
THE
BALANCING
BY
CORE
AND
OUR
PROPOSED
BALANCING
BY
STACK
SCHEDULERS
CREATE
THE
SMALLEST
TEMPERATURE
GRADIENT
AMONG
ALL
CORES
THE
TEMPERATURE
CURVES
OF
ALL
CORES
ALMOST
OVERLAP
ENTIRELY
THE
WIDTH
OF
THE
TEMPERATURE
BAND
IS
C
ONLY
INDICATING
AN
EXCELLENT
BALANCE
OF
TEMPERATURE
AMONG
THE
CORES
HOWEVER
THE
BALANCING
BY
CORE
SCHEDULER
GENERATES
MORE
FLUCTUATION
NOTE
THAT
AN
IDEAL
TEMPERATURE
BALANCER
WOULD
CREATE
A
C
AMONG
ALL
CORES
HENCE
OUR
PROPOSED
BALANCING
BY
STACK
ALGO
RITHM
IS
ONLY
A
COUPLE
OF
DEGREES
FROM
THE
IDEAL
CASE
PEAK
TEMPERATURE
REDUCTION
BALANCING
THE
TEMPERATURES
ACROSS
THE
CHIP
CAN
HELP
TO
REDUCE
THE
PEAK
TEMPERATURES
AMONG
ALL
CORES
FIG
URE
SHOWS
THE
PEAK
TEMPERATURE
GENERATED
FROM
EACH
FIG
A
ZOOM
IN
OF
TEMPERATURE
VARIATION
OVER
TIME
UNDER
DIFFERENT
SCHEDULING
ALGORITHMS
SCHEDULING
ALGORITHM
ASSUMING
THERE
ARE
NO
DVFS
EM
PLOYED
OTHERWISE
THE
PEAK
TEMPERATURE
IS
JUST
THE
THER
MAL
THRESHOLD
WE
CAN
SEE
FROM
THE
FIGURES
THAT
BASELINE
ALGORITHM
CAN
GENERATE
THE
HIGHEST
PEAK
TEMPERATURE
OF
C
THE
RANDOM
RR
BALANCING
BY
CORE
AND
BALANCING
BY
STACK
CAN
REDUCE
THE
PEAK
TEMPERATURE
BET
TER
AND
BETTER
OUR
PROPOSED
BALANCING
BY
STACK
SCHEDUL
ING
GENERATES
THE
SECOND
LOWEST
PEAK
TEMPERATURE
OF
C
A
C
LOWER
THAN
THE
BASELINE
AND
A
MERE
C
HIGHER
THAN
THAT
OF
BALANCING
BY
CORE
FIG
PEAK
TEMPERATURES
OF
DIFFERENT
SCHEDULING
ALGO
RITHMS
THERMAL
EMERGENCY
REDUCTION
A
DIRECT
BENEFIT
FROM
SCHEDULING
THE
TASKS
IS
THE
REDUCED
THERMAL
EMERGENCY
TIME
I
E
THE
TIME
A
CORE
TEMPERATURE
STAYS
ABOVE
THE
HARDWARE
THERMAL
THRESHOLD
NOTE
THAT
THIS
METRIC
DOES
NOT
NECESSARILY
CORRELATE
WITH
THE
PEAK
TEMPERATURES
REPORTED
IN
FIGURE
WHICH
ARE
COLLECTED
UNDER
NO
DVFS
FOR
EXAMPLE
A
RELATIVELY
LOW
PEAK
TEMPERATURE
MAY
STILL
TRIP
DVFS
IF
TEMPERATURE
OSCILLATES
AROUND
THE
THRESHOLD
OFTEN
FIGURE
SHOWS
THERMAL
EMERGENCY
TIME
REDUCTIONS
FROM
DIFFERENT
ALGORITHMS
NORMALIZED
TO
THE
BASELINE
CASE
AS
WE
CAN
SEE
THE
RANDOM
RR
AND
BALANCING
BY
CORE
CAN
REDUCE
THE
EMER
GENCY
TIME
BY
AND
ON
AVERAGE
RESPECTIVELY
OUR
BALANCING
BY
STACK
ALGORITHM
REMOVES
THE
MOST
EMERGENCY
TIME
IN
CASES
OF
BENCHMARKS
AN
AVERAGE
OF
REDUCTION
IS
OBSERVED
WITH
A
RANGE
OF
ALSO
THE
BALANCING
BY
CORE
ALGORITHM
TURNS
OUT
TO
INTRODUCE
AS
MUCH
EMERGENCY
TIME
AS
RR
ALGORITHMS
EVEN
WITH
LOWER
PEAK
TEMPERATURE
THIS
IS
BECAUSE
IT
TENDS
TO
CREATE
TEMPERATURE
OSCILLATIONS
AMONG
CORE
STACKS
AS
DISCUSSED
IN
SECTION
AND
IT
TENDS
TO
ALLOCATE
COOLER
TASKS
ON
THE
TOP
LAYER
WHERE
DVFS
IS
USUALLY
ENGAGED
FOR
A
LONG
TIME
THE
CONSEQUENCE
IS
THAT
THE
OVERALL
POWER
IN
THE
ENTIRE
CHIP
IS
NOT
REDUCED
AS
MUCH
AS
IN
OTHER
SCHEDULERS
WHERE
HIGH
POWER
TASKS
CAN
BE
SCALED
DURING
EMERGENCIES
THEREFORE
A
BALANCING
BY
CORE
SCHEDULER
MAY
NOT
BE
A
GOOD
SCHEDULING
CANDIDATE
IN
PRACTICE
FIG
THERMAL
EMERGENCY
TIME
REDUCTIONS
IN
HOMOGE
NEOUS
FLOORPLANS
PERFORMANCE
IMPROVEMENT
CORRESPONDING
TO
THE
THERMAL
EMERGENCIES
REMOVED
OUR
PROPOSED
BALANCING
BY
STACK
ALGORITHM
ACHIEVES
THE
BEST
PERFORMANCE
SPEEDUP
AMONG
ALL
ALGORITHMS
DIS
CUSSED
THIS
IS
SHOWN
IN
FIGURE
THE
PERFORMANCE
IS
THE
TOTAL
EXECUTION
TIME
OF
ALL
TASKS
IN
A
WORKLOAD
THE
RESULTS
ARE
NORMALIZED
TO
THE
BASELINE
PERFORMANCE
ON
AVERAGE
THE
BALANCING
BY
STACK
ACHIEVES
A
SPEEDUP
WHILE
THE
RANDOM
RR
AND
BALANCING
BY
CORE
ALGORITHM
ACHIEVE
AND
IMPROVEMENT
RESPECTIVELY
THIS
IS
PRIMARILY
DUE
TO
THE
AMOUNT
OF
THERMAL
EMERGENCIES
OUR
ALGORITHM
REMOVED
AS
WELL
AS
THE
HIGH
EFFICIENCY
IN
HANDLING
THEM
WITH
OUR
NEW
THERMAL
MANAGEMENT
MECHANISM
WE
ALSO
NOTICE
THAT
IN
SOME
OCCASIONS
THE
PERFORMANCE
MAY
NOT
IMPROVE
EVEN
IF
THE
THERMAL
EMERGENCY
TIME
IS
REDUCED
THIS
COULD
HAPPEN
WHEN
THE
TEMPERATURE
FLOATS
AROUND
THE
THERMAL
THRESHOLD
BUT
DOES
NOT
IN
CREASE
OVERLY
HIGH
IN
SUCH
A
SCENARIO
THERE
COULD
BE
MANY
DVFS
TRIGGERED
WHICH
INTRODUCE
HIGH
TRANSITION
PENALTY
AND
OVERKILLS
THE
GAINS
FROM
SCHEDULING
FOR
EXAMPLE
IN
THE
HMMC
WORKLOAD
THE
BALANCING
BY
CORE
REMOVED
OF
THERMAL
EMERGENCY
TIME
IN
THE
BALANCING
BY
STACK
BUT
ITS
PERFORMANCE
IS
WORSE
THAN
THE
BALANCING
BY
STACK
OUR
BALANCING
BY
STACK
RE
MOVES
MORE
THERMAL
EMERGENCY
TIME
THAN
OTHER
SCHED
ULERS
IN
TH
CASES
OF
BENCHMARK
COMBINATIONS
AND
THEREFORE
ACHIEVES
THE
MOST
PERFORMANCE
IMPROVEMENT
FIG
PERFORMANCE
SPEEDUPS
FOR
HOMOGENEOUS
FLOOR
PLANS
HETEROGENEOUS
FLOORPLAN
IN
ADDITION
TO
THE
FIVE
ALGORITHMS
APPLIED
TO
THE
HOMO
GENEOUS
FLOORPLAN
TWO
ADDITIONAL
ALGORITHMS
ARE
ALSO
TESTED
FOR
HETEROGENEOUS
FLOORPLANS
THE
FIRST
IS
THE
RE
VISED
BALANCING
BY
STACK
ALGORITHM
WITH
DYNAMIC
SUPER
TASK
FORMING
MECHANISMS
THE
ALGORITHM
IS
DESIGNED
TO
TACKLE
THE
THERMAL
HETEROGENEITY
OF
THE
FLOORPLAN
AS
DISCUSSED
IN
SECTION
THE
SECOND
IS
A
PSEUDO
OPTIMAL
ALGORITHM
THAT
TESTS
THE
QUALITY
OF
EACH
DISCUSSED
ALGORITHM
WE
TERM
THIS
ALGORITHM
A
STEP
OPTIMAL
SINCE
IT
TRIES
ALL
TASK
BUNDLING
MECHANISMS
AND
CHOOSES
THE
ONE
THAT
TRIGGERS
THE
FEWEST
DTMS
IN
ONE
NEXT
STEP
NOTICE
THAT
THIS
IS
NOT
A
TRUE
OPTIMAL
ALGORITHM
WHICH
WOULD
GO
BEYOND
ONE
STEP
TO
ENUMERATE
ALL
POSSIBLE
SCHEDULES
AND
PICK
THE
OPTIMUM
ONE
AND
SO
IS
TERMED
STEP
ONLY
ALTHOUGH
IT
IS
NOT
REALISTIC
TO
ADOPT
STEP
OPTIMAL
ALGORITHM
ONLINE
DUE
TO
ITS
COMPLEXITY
IT
DOES
INDICATE
THE
POTENTIAL
FOR
IMPROVEMENT
OF
THE
DISCUSSED
ALGORITHMS
THERMAL
EMERGENCY
REDUCTION
FIGURE
SHOWS
THE
THERMAL
EMERGENCY
TIME
REDUCTION
FOR
DIFFERENT
ALGORITHMS
NORMALIZED
TO
THE
BASELINE
CASE
AS
WE
CAN
SEE
RANDOM
AND
RR
PERFORM
RELATIVELY
POORLY
COMPARED
WITH
OTHER
ALGORITHMS
BECAUSE
OF
THE
HETEROGENEITY
IN
THE
FLOORPLAN
THEY
ACHIEVE
AND
OF
THERMAL
EMERGENCY
TIME
REDUCTION
RE
SPECTIVELY
OUR
PROPOSED
DYNAMIC
BALANCING
BY
STACK
ALGORITHM
ACHIEVES
A
TOTAL
OF
REDUCTION
ONLY
AWAY
FROM
THE
STEP
OPTIMAL
ON
AVERAGE
AND
IS
BETTER
THAN
THE
REMAINING
ALGORITHMS
FOR
EXAMPLE
IT
REMOVES
MORE
EMERGENCY
TIME
THAN
THE
ORIGINAL
BALANCING
BY
STACK
ALGORITHM
THIS
INDICATES
THAT
DY
NAMICALLY
TUNING
OF
THE
TASK
BUNDLING
IS
VERY
HELPFUL
TO
A
THERMALLY
HETEROGENEOUS
FLOORPLAN
THE
BALANCING
BY
CORE
ALGORITHM
IS
SLIGHTLY
BETTER
THAN
DYNAMIC
BALANCING
BY
STACK
IN
THREE
CASES
HHHHCCCC
HHMMMCCC
AND
HM
GZIP
THIS
IS
BECAUSE
WHEN
T
AND
P
DO
NOT
CHANGE
OUR
DYNAMIC
BALANCING
BY
STACK
ALGORITHM
WILL
SELECT
THE
SAME
ONE
FROM
THE
FIXED
POWER
BUNDLING
SCHEMES
WHILE
A
SLIGHT
RE
ORDERING
OF
CORE
TEMPERATURE
WILL
CAUSE
BALANCING
BY
CORE
TO
FORM
A
DIFFERENT
AND
BETTER
POWER
BUNDLE
MORE
FLEXIBLY
ALSO
BALANCING
BY
CORE
SLIGHTLY
SURPASSES
STEP
OPTIMAL
IN
HHMMMCCC
AND
HM
GZIP
WORKLOADS
THIS
IS
BECAUSE
THE
STEP
OPTIMAL
DOES
NOT
GENERATE
A
GLOBAL
OPTIMAL
SCHEDULE
FIG
THERMAL
EMERGENCY
TIME
REDUCTIONS
IN
HETEROGE
NEOUS
FLOORPLANS
PERFORMANCE
IMPROVEMENT
COMPARED
WITH
THE
THERMAL
EMERGENCIES
REMOVED
OUR
DYNAMIC
BALANCING
BY
STACK
ALGORITHM
ACHIEVES
THE
BEST
PERFORMANCE
SPEEDUPS
ON
AVERAGE
AMONG
ALL
THE
ALGO
RITHMS
EXCEPT
STEP
OPTIMAL
FIGURE
SHOWS
THAT
RANDOM
AND
RR
ACHIEVE
AND
SPEEDUP
RESPECTIVELY
WHICH
IS
NOTEBALY
LOWER
THAN
AND
SPEEDUP
SHOWN
IN
FIGURE
INDICATING
THAT
RANDOM
AND
RR
ARE
NOT
AS
HELPFUL
IN
HETEROGENEOUS
FLOORPLANS
AS
IN
HOMOGENEOUS
ONES
BALANCING
BY
STACK
ACHIEVES
SPEEDUP
MORE
THAN
BALANCING
BY
CORE
THOUGH
FIGURE
SHOWS
IT
REMOVES
LESS
THERMAL
EMERGENCY
TIME
THAN
BALANCING
BY
CORE
THE
REASON
BEHIND
THIS
IS
THAT
BALANCING
BY
CORE
TENDS
TO
GENERATE
MANY
OVERHEAD
IN
DTM
MODE
SWITCHES
THOUGH
THE
TOTAL
TIME
ABOVE
THE
EMERGENCY
THRESHOLD
IS
LOW
WHICH
WAS
REPORTED
IN
FIGURE
FINALLY
THE
DYNAMIC
BALANCING
BY
STACK
ALGORITHM
ACHIEVES
THE
BEST
PERFORMANCE
SPEEDUP
OF
WITH
NEGLEGIBLE
GAP
FROM
THE
STEP
OPTIMAL
FIG
PERFORMANCE
SPEEDUPS
FOR
HETEROGENEOUS
FLOOR
PLAN
CONCLUSIONS
WE
HAVE
DEMONSTRATED
IN
THIS
PAPER
THAT
OS
TASK
SCHEDULING
IS
AN
EFFECTIVE
APPROACH
TO
IMPROVE
THE
THER
MAL
CONDITIONS
IN
CHIP
MULTIPROCESSORS
IT
CAN
REDUCE
THE
PEAK
TEMPERATURE
WITHIN
THE
CHIP
REDUCE
THE
THERMAL
EMERGENCY
TRIGGERING
AMOUNT
AND
IMPROVE
THE
OVERALL
PERFORMANCE
OF
THE
EXECUTING
TASKS
IN
PARTICULAR
WE
HAVE
SHOWN
THAT
OUR
PROPOSED
SCHEDULING
MECHANISM
BALANCING
BY
STACK
OUTPERFORMS
OTHER
INTUITIVE
ALGORITHMS
IN
THE
THERMALLY
HOMOGENEOUS
FLOORPLAN
BECAUSE
OF
THE
FOLLOWING
THREE
PROPERTIES
FIRST
OUR
SCHEDULER
TAKES
INTO
ACCOUNT
THE
HIGH
THERMAL
CORRE
LATIONS
AMONG
THE
LAYERS
IN
ONE
CORE
STACK
AND
SCHEDULES
TASKS
IN
BUNDLES
SECOND
WITHIN
EVERY
STACK
OF
CORES
HOT
TASKS
ARE
ALLOCATED
TO
THE
LAYERS
THAT
ARE
CLOSEST
TO
THE
HEAT
SINK
FOR
BEST
HEAT
DISSIPATION
THIRD
UPON
A
THERMAL
EMERGENCY
POWER
SCALING
IS
ENGAGED
IN
A
CORE
STACK
WHOSE
TEMPERATURE
EXCEEDS
THE
THRESHOLD
AND
TO
THE
CORE
THAT
GENERATES
THE
LARGEST
POWER
IN
THIS
STACK
THIS
CAN
QUICKLY
COOL
DOWN
THE
CORE
STACK
REDUCING
THE
PERFORMANCE
PENALTY
IMPOSED
TO
THE
TASK
WE
ALSO
PRESENTED
A
REVISED
BALANCING
BY
STACK
ALGO
RITHM
FOR
HETEROGENEOUS
FLOORPLANS
THIS
ALGORITHM
REFINED
TASK
BUNDLING
MECHANISM
TO
ADAPT
TO
THE
HETERO
GENEITY
IN
THERMAL
DISTRIBUTION
AMONG
THE
CORES
SUCH
A
REFINEMENT
SHOWS
ITS
STRENGTH
OVER
OTHER
ALGORITHMS
IN
BOTH
THERMAL
EMERGENCY
REDUCTION
AND
PERFORMANCE
SPEEDUP
MOREOVER
IT
ACHIEVES
NEARLY
THE
FULL
POTENTIAL
SUGGESTED
BY
A
LOCAL
OPTIMAL
ALGORITHM
WE
STUDY
SCHEDULING
PROBLEMS
MOTIVATED
BY
RECENTLY
DEVELOPED
TECHNIQUES
FOR
MICRO
PROCESSOR
THERMAL
MANAGEMENT
AT
THE
OPERATING
SYSTEMS
LEVEL
THE
GENERAL
SCENARIO
CAN
BE
DESCRIBED
AS
FOLLOWS
THE
MICROPROCESSOR
TEMPERATURE
IS
CONTROLLED
BY
THE
HARDWARE
THERMAL
MANAGEMENT
SYSTEM
THAT
CONTINUOUSLY
MONITORS
THE
CHIP
TEMPERATURE
AND
AU
TOMATICALLY
REDUCES
THE
PROCESSOR
SPEED
AS
SOON
AS
THE
THERMAL
THRESHOLD
IS
EXCEEDED
SOME
TASKS
ARE
MORE
CPU
INTENSIVE
THAN
OTHER
AND
THUS
GENERATE
MORE
HEAT
DURING
EX
ECUTION
THE
COOLING
SYSTEM
OPERATES
NON
STOP
REDUCING
AT
AN
EXPONENTIAL
RATE
THE
DEVIATION
OF
THE
PROCESSOR
TEMPERATURE
FROM
THE
AMBIENT
TEMPERATURE
AS
A
RESULT
THE
PROCESSOR
TEMPERATURE
AND
THUS
THE
PERFORMANCE
AS
WELL
DEPENDS
ON
THE
ORDER
OF
THE
TASK
EXECUTION
GIVEN
A
VARIETY
OF
POSSIBLE
UNDERLYING
ARCHITECTURES
MODELS
FOR
COOLING
AND
FOR
HARDWARE
THERMAL
MANAGEMENT
AS
WELL
AS
TYPES
OF
TASKS
THIS
SCENARIO
GIVES
RISE
TO
A
PLETHORA
OF
INTERESTING
AND
NEVER
STUDIED
SCHEDULING
PROBLEMS
WE
FOCUS
ON
SCHEDULING
REAL
TIME
JOBS
IN
A
SIMPLIFIED
MODEL
FOR
COOLING
AND
THERMAL
MANAGEMENT
A
COLLECTION
OF
UNIT
LENGTH
JOBS
IS
GIVEN
EACH
JOB
SPECIFIED
BY
ITS
RELEASE
TIME
DEADLINE
AND
HEAT
CONTRIBUTION
IF
AT
SOME
TIME
STEP
THE
TEMPERATURE
OF
THE
SYSTEM
IS
Τ
AND
THE
PROCESSOR
EXECUTES
A
JOB
WITH
HEAT
CONTRIBUTION
H
THEN
THE
TEMPERATURE
AT
THE
NEXT
STEP
IS
Τ
H
THE
TEMPERATURE
CANNOT
EXCEED
THE
GIVEN
THERMAL
THRESHOLD
T
THE
OBJECTIVE
IS
TO
MAXIMIZE
THE
THROUGHPUT
THAT
IS
THE
NUMBER
OF
TASKS
THAT
MEET
THEIR
DEADLINES
WE
PROVE
THAT
IN
THE
OFFLINE
CASE
COMPUTING
THE
OPTIMUM
SCHEDULE
IS
NP
HARD
EVEN
IF
ALL
JOBS
ARE
RELEASED
AT
THE
SAME
TIME
IN
THE
ONLINE
CASE
WE
SHOW
A
COMPETITIVE
DETERMINISTIC
ALGORITHM
AND
A
MATCHING
LOWER
BOUND
INTRODUCTION
BACKGROUND
THE
PROBLEM
OF
MANAGING
THE
TEMPERATURE
OF
PROCESSOR
SYSTEMS
IS
NOT
NEW
IN
FACT
THE
SYSTEM
BUILDERS
HAD
TO
DEAL
WITH
THIS
CHALLENGE
SINCE
THE
INCEPTION
OF
COMPUTERS
SINCE
EARLY
THE
INTRODUCTION
OF
BATTERY
OPERATED
LAPTOP
COMPUTERS
AND
SENSOR
SYSTEMS
HIGHLIGHTED
THE
RELATED
ISSUE
OF
CONTROLLING
THE
ENERGY
CONSUMPTION
MOST
OF
THE
INITIAL
WORK
ON
THESE
PROBLEMS
WAS
HARDWARE
AND
SYSTEMS
ORIENTED
AND
ONLY
DURING
THE
LAST
DECADE
SUBSTANTIAL
PROGRESS
HAS
BEEN
ACHIEVED
ON
DEVELOPING
MODELS
AND
ALGORITHMIC
TECHNIQUES
FOR
MICROPROCESSOR
TEMPERATURE
AND
ENERGY
MANAGEMENT
THIS
WORK
PROCEEDED
IN
SEVERAL
DIRECTIONS
ONE
DIRECTION
IS
BASED
ON
THE
FACT
THAT
THE
ENERGY
CONSUMPTION
IS
A
FAST
GROWING
FUNCTION
OF
THE
PROCESSOR
SPEED
OR
FREQUENCY
THUS
WE
CAN
SAVE
DEPARTMENT
OF
COMPUTER
SCIENCE
UNIVERSITY
OF
CALIFORNIA
RIVERSIDE
CA
USA
SUPPORTED
BY
NSF
GRANTS
OISE
AND
CCF
CNRS
LIX
UMR
ECOLE
POLYTECHNIQUE
PALAISEAU
FRANCE
SUPPORTED
BY
ANR
ALPAGE
LABORATOIRE
DE
L
INFORMATIQUE
DU
PARALL
ELISME
ECOLE
NORMALE
SUP
ERIEURE
DE
LYON
ENS
LYON
FRANCE
ENERGY
BY
SIMPLY
SLOWING
DOWN
THE
PROCESSOR
HERE
ALGORITHMIC
RESEARCH
FOCUSSED
ON
SPEED
SCALING
NAMELY
DYNAMICALLY
ADJUSTING
THE
PROCESSOR
SPEED
OVER
TIME
TO
OPTIMIZE
THE
ENERGY
CONSUMPTION
WHILE
ENSURING
THAT
THE
SYSTEM
MEETS
THE
DESIRED
PERFORMANCE
REQUIREMENTS
ANOTHER
TECHNIQUE
APPLICABLE
TO
THE
WHOLE
SYSTEM
NOT
JUST
THE
MICROPROCESSOR
INVOLVES
POWER
DOWN
STRATEGIES
WHERE
THE
SYSTEM
IS
POWERED
DOWN
OR
EVEN
COMPLETELY
TURNED
OFF
WHEN
SOME
OF
ITS
COMPONENTS
ARE
IDLE
SINCE
CHANGING
THE
POWER
LEVEL
OF
A
SYSTEM
INTRODUCES
SOME
OVERHEAD
SCHEDULING
THE
WORK
TO
MINIMIZE
THE
OVERALL
ENERGY
USAGE
IN
THIS
MODEL
BECOMES
A
CHALLENGING
OPTIMIZATION
PROBLEM
MODELS
HAVE
ALSO
BEEN
DEVELOPED
FOR
THE
PROCESSOR
THERMAL
BEHAVIOR
HERE
THE
MAIN
OBJECTIVE
IS
TO
ENSURE
THAT
THE
SYSTEM
TEMPERATURE
DOES
NOT
EXCEED
THE
SO
CALLED
THERMAL
THRESHOLD
ABOVE
WHICH
THE
PROCESSOR
CANNOT
OPERATE
CORRECTLY
OR
MAY
EVEN
BE
DAMAGED
IN
THIS
DIRECTION
TECHNIQUES
AND
ALGORITHMS
HAVE
BEEN
PROPOSED
FOR
USING
SPEED
SCALING
TO
OPTIMIZE
THE
SYSTEM
PERFORMANCE
WHILE
MAINTAINING
THE
TEMPERATURE
BELOW
THE
THRESHOLD
WE
REFER
THE
READER
TO
THE
SURVEY
BY
IRANI
AND
PRUHS
AND
REFERENCES
THEREIN
FOR
MORE
IN
DEPTH
INFORMATION
ON
THE
MODELS
AND
ALGORITHMS
FOR
THERMAL
AND
ENERGY
MANAGEMENT
TEMPERATURE
AWARE
SCHEDULING
THE
ABOVE
MODELS
ADDRESS
ENERGY
AND
THERMAL
MAN
AGEMENT
AT
THE
MICRO
ARCHITECTURE
LEVEL
IN
CONTRAST
THE
PROBLEM
WE
STUDY
IN
THIS
PAPER
ADDRESSES
THE
ISSUE
OF
THERMAL
MANAGEMENT
AT
THE
OPERATING
SYSTEMS
LEVEL
MOST
OF
THE
PREVI
OUS
WORK
IN
THIS
DIRECTION
FOCUSSED
ON
MULTI
CORE
SYSTEMS
WHERE
ONE
CAN
MOVE
TASKS
BETWEEN
THE
PROCESSORS
TO
MINIMIZE
THE
MAXIMUM
TEMPERATURE
HOWEVER
AS
IT
HAS
BEEN
RECENTLY
DISCOVERED
EVEN
IN
SINGLE
CORE
SYSTEMS
ONE
CAN
EXPLOIT
VARIATIONS
IN
HEAT
CONTRIBUTIONS
AMONG
DIFFERENT
TASKS
TO
REDUCE
THE
PROCESSOR
TEMPERATURE
THROUGH
APPRO
PRIATE
TASK
SCHEDULING
IN
THIS
SCENARIO
THE
MICROPROCESSOR
TEMPERATURE
IS
CONTROLLED
BY
THE
HARDWARE
DYNAMIC
THERMAL
MANAGEMENT
DTM
SYSTEM
THAT
CONTINUOUSLY
MONITORS
THE
CHIP
TEMPERATURE
AND
AUTOMATICALLY
REDUCES
THE
PROCESSOR
SPEED
AS
SOON
AS
THE
THERMAL
THRESHOLD
MAXIMUM
SAFE
OPERATING
TEMPERATURE
IS
EXCEEDED
TYPICALLY
THE
FREQUENCY
IS
REDUCED
BY
HALF
ALTHOUGH
IT
CAN
BE
FURTHER
REDUCED
TO
ONE
FOURTH
OR
EVEN
ONE
EIGHTH
IF
NEEDED
RUNNING
AT
A
LOWER
FREQUENCY
THE
CPU
GENERATES
LESS
HEAT
THE
COOLING
SYSTEM
OPERATES
NON
STOP
REDUCING
AT
AN
EXPONENTIAL
RATE
THE
DEVIATION
OF
THE
PROCESSOR
TEMPERATURE
FROM
THE
AMBIENT
TEMPERATURE
ONCE
THE
CHIP
COOLS
DOWN
TO
BELOW
THE
THRESHOLD
THE
FREQUENCY
IS
INCREASED
AGAIN
DIFFERENT
TASKS
USE
DIFFERENT
MICROPROCESSOR
UNITS
IN
DIFFERENT
WAYS
IN
PARTICULAR
SOME
TASKS
ARE
MORE
CPU
INTENSIVE
THAN
OTHER
AS
A
RESULT
THE
PROCESSOR
THERMAL
BEHAVIOR
AND
THUS
THE
PERFORMANCE
AS
WELL
DEPENDS
ON
THE
ORDER
OF
THE
TASK
EXECUTION
IN
PARTICULAR
YANG
ET
AL
POINT
OUT
THAT
BASED
ON
THE
STANDARD
MODEL
FOR
THE
MICROPROCESSOR
THERMAL
BEHAVIOR
FOR
ANY
GIVEN
TWO
TASKS
SCHEDULING
THE
HOTTER
JOB
BEFORE
THE
COOLER
ONE
RESULTS
IN
A
LOWER
FINAL
TEMPERATURE
THAN
AFTER
THE
REVERSE
ORDER
THEY
TAKE
ADVANTAGE
OF
THIS
PHE
NOMENON
TO
REDUCE
THE
NUMBER
OF
DTM
INVOCATIONS
THUS
IMPROVING
THE
PERFORMANCE
OF
THE
OS
SCHEDULER
WITH
MULTITUDES
OF
POSSIBLE
UNDERLYING
ARCHITECTURES
FOR
EXAMPLE
SINGLE
VS
MULTI
CORE
SYSTEMS
MODELS
FOR
COOLING
AND
HARDWARE
THERMAL
MANAGEMENT
AS
WELL
AS
TYPES
OF
JOBS
REAL
TIME
BATCH
ETC
THE
SCENARIO
OUTLINED
ABOVE
GIVES
RISE
TO
A
PLETHORA
OF
INTERESTING
AND
NEVER
YET
STUDIED
SCHEDULING
PROBLEMS
OUR
MODEL
WE
FOCUS
ON
SCHEDULING
REAL
TIME
JOBS
IN
A
SOMEWHAT
SIMPLIFIED
MODEL
FOR
COOLING
AND
THERMAL
MANAGEMENT
THE
TIME
IS
DIVIDED
INTO
UNIT
TIME
SLOTS
AND
EACH
JOB
HAS
UNIT
LENGTH
THESE
JOBS
REPRESENT
UNIT
SLICES
OF
THE
PROCESSES
PRESENT
IN
THE
OS
SCHEDULER
QUEUE
WE
ASSUME
THAT
THE
HEAT
CONTRIBUTIONS
OF
THESE
JOBS
ARE
KNOWN
THIS
IS
COUNTERINTU
ITIVE
BUT
REASONABLY
REALISTIC
FOR
AS
DISCUSSED
IN
THESE
VALUES
CAN
BE
WELL
APPROXIMATED
USING
APPROPRIATE
PREDICTION
METHODS
IN
OUR
THERMAL
MODEL
WE
ASSUME
WITHOUT
LOSS
OF
GENERALITY
THAT
THE
AMBIENT
TEMPERATURE
IS
AND
THAT
THE
HEAT
CONTRIBUTIONS
ARE
EXPRESSED
IN
THE
UNITS
OF
TEMPERATURE
THAT
IS
BY
HOW
MUCH
THEY
WOULD
INCREASE
THE
CHIP
TEMPERATURE
IN
THE
ABSENCE
OF
COOLING
IN
REALITY
DURING
THE
EXECUTION
OF
A
JOB
ITS
HEAT
CONTRIBUTION
IS
SPREAD
OVER
THE
WHOLE
TIME
SLOT
AND
SO
IS
THE
EFFECT
OF
COOLING
THUS
THE
FINAL
TEMPERATURE
CAN
BE
EXPRESSED
USING
AN
INTEGRAL
FUNCTION
IN
THIS
PAPER
WE
USE
A
SIMPLIFIED
MODEL
WHERE
WE
FIRST
TAKE
INTO
ACCOUNT
THE
JOB
HEAT
CONTRIBUTION
AND
THEN
APPLY
THE
COOLING
WHERE
THE
COOLING
SIMPLY
REDUCES
THE
TEMPERATURE
BY
HALF
FINALLY
WE
ASSUME
THAT
ONLY
ONE
PROCESSOR
FREQUENCY
IS
AVAILABLE
CONSEQUENTLY
IF
THERE
IS
NO
JOB
WHOSE
EXECUTION
DOES
NOT
CAUSE
A
THERMAL
VIOLATION
THE
PROCESSOR
MUST
STAY
IDLE
THROUGH
THE
NEXT
TIME
SLOT
OUR
RESULTS
SUMMARIZING
OUR
SCHEDULING
PROBLEM
CAN
BE
NOW
FORMALIZED
AS
FOLLOWS
A
COLLECTION
OF
UNIT
LENGTH
JOBS
IS
GIVEN
EACH
JOB
J
WITH
A
RELEASE
TIME
RJ
DEADLINE
DJ
AND
HEAT
CONTRIBUTION
HJ
IF
AT
SOME
TIME
STEP
THE
TEMPERATURE
OF
THE
SYSTEM
IS
Τ
AND
THE
PROCESSOR
EXECUTES
A
JOB
J
THEN
THE
TEMPERATURE
AT
THE
NEXT
STEP
IS
Τ
HJ
THE
TEMPERATURE
CANNOT
EXCEED
THE
GIVEN
THERMAL
THRESHOLD
T
THE
OBJECTIVE
IS
TO
COMPUTE
A
SCHEDULE
WHICH
MAXIMIZES
THE
NUMBER
OF
TASKS
THAT
MEET
THEIR
DEADLINES
WE
PROVE
THAT
IN
THE
OFFLINE
CASE
COMPUTING
THE
OPTIMUM
SCHEDULE
IS
NP
HARD
EVEN
IF
ALL
JOBS
ARE
RELEASED
AT
THE
SAME
TIME
AND
HAVE
EQUAL
DEADLINES
IN
THE
ONLINE
CASE
WE
SHOW
A
COMPETITIVE
DETERMINISTIC
ALGORITHM
AND
A
MATCHING
LOWER
BOUND
TERMINOLOGY
AND
NOTATION
THE
INPUT
CONSISTS
OF
N
UNIT
LENGTH
JOBS
THAT
WE
NUMBER
N
EACH
JOB
J
IS
SPECIFIED
BY
A
TRIPLE
RJ
DJ
HJ
N
N
Q
WHERE
RJ
IS
ITS
RELEASE
TIME
DJ
IS
THE
DEADLINE
AND
HJ
IS
ITS
HEAT
CONTRIBUTION
THE
TIME
IS
DIVIDED
INTO
UNIT
LENGTH
SLOTS
AND
EACH
JOB
CAN
BE
EXECUTED
IN
ANY
TIME
SLOT
IN
THE
INTERVAL
RJ
DJ
BY
ΤU
WE
DENOTE
THE
PROCESSOR
TEMPERATURE
AT
TIME
U
THE
INITIAL
TEMPERATURE
IS
AND
IT
CHANGES
ACCORDING
TO
THE
FOLLOWING
RULES
IF
THE
TEMPERATURE
OF
THE
SYSTEM
AT
A
TIME
U
IS
ΤU
AND
THE
PROCESSOR
EXECUTES
A
JOB
J
THEN
THE
TEMPERATURE
AT
TIME
U
IS
ΤU
ΤU
HJ
THE
TEMPERATURE
CANNOT
EXCEED
THE
GIVEN
THERMAL
THRESHOLD
T
WITHOUT
LOSS
OF
GENERALITY
WE
ASSUME
THAT
T
THUS
IF
ΤU
HJ
THEN
J
CANNOT
BE
EXECUTED
AT
TIME
U
IDLE
SLOTS
ARE
TREATED
AS
EXECUTING
A
JOB
WITH
HEAT
CONTRIBUTION
THAT
IS
AFTER
AN
IDLE
SLOT
THE
TEMPERATURE
DECREASES
BY
HALF
GIVEN
AN
INSTANCE
AS
ABOVE
THE
OBJECTIVE
IS
TO
COMPUTE
A
SCHEDULE
WITH
MAXIMUM
THROUGHPUT
WHERE
THROUGHPUT
IS
DEFINED
AS
THE
NUMBER
OF
COMPLETED
JOBS
EXTENDING
THE
STANDARD
NOTATION
FOR
SCHEDULING
PROBLEMS
WE
DENOTE
THE
OFFLINE
VERSION
OF
THIS
PROBLEM
BY
RI
HI
PI
UI
IN
THE
ONLINE
VERSION
DENOTED
ONLINE
RI
HI
PI
UI
JOBS
ARE
AVAILABLE
TO
THE
ALGO
RITHM
AT
THEIR
RELEASE
TIME
SCHEDULING
DECISIONS
OF
THE
ALGORITHM
CANNOT
DEPEND
ON
THE
JOBS
THAT
HAVE
NOT
YET
BEEN
RELEASED
EXAMPLE
SUPPOSE
WE
HAVE
FOUR
JOBS
SPECIFIED
IN
NOTATION
J
RJ
DJ
HJ
SCHEDULE
SCHEDULE
FIGURE
EXAMPLE
OF
TWO
SCHEDULES
FIGURE
SHOWS
THESE
JOBS
AND
TWO
SCHEDULES
NUMBERS
ABOVE
THE
SCHEDULES
DENOTE
TEM
PERATURES
IN
THE
FIRST
SCHEDULE
WHEN
WE
SCHEDULE
JOB
AT
TIME
THE
PROCESSOR
IS
TOO
HOT
TO
EXECUTE
JOB
SO
IT
WILL
NOT
COMPLETE
JOB
AT
ALL
IN
THE
SECOND
SCHEDULE
WE
STAY
IDLE
IN
STEP
ALLOWING
US
TO
COMPLETE
ALL
JOBS
THE
NP
COMPLETENESS
PROOFS
IN
THIS
SECTION
WE
PROVE
THAT
THE
SCHEDULING
PROBLEM
RI
HI
PI
UI
IS
NP
HARD
FOR
THE
SAKE
OF
EXPOSITION
WE
START
WITH
A
PROOF
FOR
THE
GENERAL
CASE
AND
LATER
WE
GIVE
A
PROOF
FOR
THE
SPECIAL
CASE
WHEN
ALL
RELEASE
TIMES
AND
DEADLINES
ARE
EQUAL
THEOREM
THE
OFFLINE
PROBLEM
RI
HI
PI
UI
IS
NP
HARD
PROOF
WE
USE
A
REDUCTION
FROM
THE
PARTITION
PROBLEM
DEFINED
AS
FOLLOWS
WE
ARE
GIVEN
A
SET
OF
POSITIVE
INTEGERS
SUCH
THAT
Β
AI
Β
FOR
ALL
I
WHERE
Β
AI
THE
GOAL
IS
TO
PARTITION
INTO
N
SUBSETS
EACH
SUBSET
WITH
THE
TOTAL
SUM
EQUAL
EXACTLY
Β
BY
THE
ASSUMPTION
ON
THE
AI
EACH
SUBSET
WILL
HAVE
TO
HAVE
EXACTLY
ELEMENTS
THIS
PARTITION
OF
WILL
BE
CALLED
A
PARTITION
PARTITION
IS
WELL
KNOWN
TO
BE
NP
HARD
IN
THE
STRONG
SENSE
THAT
IS
EVEN
IF
MAXI
AI
P
N
FOR
SOME
POLYNOMIAL
P
N
WE
NOW
DESCRIBE
THE
REDUCTION
FOR
THE
GIVEN
INSTANCE
OF
PARTITION
WE
CONSTRUCT
AN
INSTANCE
OF
RI
HI
PI
UI
WITH
JOBS
THESE
JOBS
WILL
BE
OF
TWO
TYPES
I
FIRST
WE
HAVE
JOBS
THAT
CORRESPOND
TO
THE
INSTANCE
OF
PARTITION
FOR
EVERY
I
WE
CREATE
A
JOB
OF
HEAT
CONTRIBUTION
AI
RELEASE
TIME
AND
DEADLINE
N
Β
II
NEXT
WE
CREATE
N
ADDITIONAL
GADGET
JOBS
THESE
JOBS
ARE
TIGHT
MEANING
THAT
THEIR
DEADLINE
IS
JUST
ONE
UNIT
AFTER
THE
RELEASE
TIME
THE
FIRST
OF
THESE
JOBS
HAS
HEAT
CONTRI
BUTION
AND
RELEASE
TIME
THEN
FOR
EACH
J
N
WE
HAVE
A
JOB
WITH
HEAT
CONTRIBUTION
AND
RELEASE
TIME
J
Β
WE
CLAIM
THAT
HAS
A
PARTITION
IF
AND
ONLY
IF
THE
INSTANCE
OF
RI
HI
PI
UI
CONSTRUCTED
ABOVE
HAS
A
SCHEDULE
WITH
THROUGHPUT
THAT
IS
WITH
ALL
JOBS
COMPLETED
THE
MAIN
IDEA
IS
THIS
IMAGINE
THAT
AT
SOME
MOMENT
THE
TEMPERATURE
IS
AND
WE
WANT
TO
SCHEDULE
A
JOB
OF
HEAT
CONTRIBUTION
X
FOR
SOME
INTEGER
X
THEN
WE
MUST
FIRST
WAIT
X
TIME
UNITS
SO
THAT
THE
PROCESSOR
COOLS
DOWN
TO
X
X
BEFORE
WE
CAN
SCHEDULE
THAT
JOB
AND
THEN
RIGHT
AT
THE
COMPLETION
OF
THIS
JOB
THE
TEMPERATURE
IS
AGAIN
THE
ANALOGOUS
PROPERTY
HOLDS
IN
FACT
EVEN
IF
AT
THE
BEGINNING
THE
TEMPERATURE
WAS
SOME
Τ
EXCEPT
THAT
THEN
AFTER
COMPLETING
THIS
JOB
THE
NEW
TEMPERATURE
WILL
BE
Τ
L
Τ
X
X
X
X
X
THAT
IS
IT
MAY
BE
DIFFERENT
THAN
Τ
BUT
STILL
STRICTLY
GREATER
THAN
WITH
THIS
OBSERVATION
IN
MIND
THE
PROOF
OF
THE
ABOVE
CLAIM
IS
QUITE
EASY
FIRST
WE
SHOW
THAT
IF
THERE
IS
A
SOLUTION
TO
THE
SCHEDULING
PROBLEM
THEN
HAS
A
PARTITION
NOTE
THAT
THE
TIGHT
JOBS
DIVIDE
THE
TIME
INTO
N
INTERVALS
OF
LENGTH
Β
EACH
ALSO
EACH
OF
THE
OTHER
JOBS
IS
SCHEDULED
IN
EXACTLY
ONE
OF
THESE
INTERVALS
THIS
DEFINES
A
PARTITION
OF
INTO
N
SETS
NOW
WE
CLAIM
THAT
AFTER
EVERY
JOB
EXECUTION
THE
TEMPERATURE
IS
STRICTLY
MORE
THAN
THIS
IS
TRUE
FOR
THE
FIRST
JOB
TO
BE
SCHEDULED
SINCE
IT
HAS
HEAT
CONTRIBUTION
EACH
OTHER
JOB
IN
THE
INSTANCE
INCLUDING
THE
TIGHT
JOBS
HAS
HEAT
CONTRIBUTION
AT
LEAST
THEREFORE
RIGHT
AFTER
ITS
EXECUTION
THE
TEMPERATURE
IS
AT
LEAST
ALREADY
IF
WE
TAKE
ONLY
THIS
JOB
INTO
ACCOUNT
BUT
THERE
IS
ALSO
A
DECLINING
BUT
NON
ZERO
TEMPERATURE
CONTRIBUTION
FROM
THE
FIRST
TIGHT
JOB
SO
OVERALL
THE
TEMPERATURE
AFTER
EVERY
EXECUTION
IS
STRICTLY
MORE
THAN
TOGETHER
WITH
THE
EARLIER
OBSERVATION
THIS
IMPLIES
THAT
EVERY
NON
TIGHT
JOB
OF
HEAT
CONTRI
BUTION
AI
MUST
BE
PRECEDED
BY
AI
IDLE
UNITS
THUS
USING
AI
TIME
SLOTS
IN
TOTAL
THEREFORE
EVERY
SET
IN
THE
ABOVE
MENTIONED
PARTITION
HAS
THE
TOTAL
SUM
AT
MOST
Β
SINCE
THERE
ARE
AT
MOST
N
SETS
IN
THE
PARTITION
THE
TOTAL
SUM
OF
EACH
MUST
BE
EXACTLY
Β
THIS
PROVES
THAT
HAS
A
PARTITION
NOW
WE
SHOW
THE
OTHER
IMPLICATION
NAMELY
THAT
IF
HAS
A
PARTITION
THEN
THERE
IS
A
SOLUTION
TO
THE
SCHEDULING
INSTANCE
SIMPLY
SCHEDULE
THE
TIGHT
JOBS
AT
THEIR
RELEASE
TIME
THIS
DIVIDES
THE
TIME
INTO
N
INTERVALS
OF
LENGTH
Β
EACH
ASSIGN
EACH
OF
THE
N
SETS
IN
THE
PARTITION
TO
A
DISTINCT
INTERVAL
AND
SCHEDULE
ITS
JOBS
IN
THIS
INTERVAL
EVERY
INTEGER
AI
IN
THE
SET
CORRESPONDS
TO
A
JOB
OF
HEAT
CONTRIBUTION
AI
AND
WE
SCHEDULE
IT
PRECEDED
WITH
AI
IDLE
TIME
UNITS
THE
JOBS
OF
THE
SET
CAN
BE
SCHEDULED
IN
AN
ARBITRARY
ORDER
THE
IMPORTANT
PROPERTY
BEING
THAT
SINCE
THEIR
TOTAL
SUM
IS
Β
THEY
ALL
FIT
EXACTLY
IN
THIS
INTERVAL
AFTER
ALL
JOBS
IN
ONE
SET
ARE
EXECUTED
THE
TEMPERATURE
IS
EXACTLY
AND
DURING
THE
EXECUTION
THE
TEMPERATURE
DOES
NOT
EXCEED
BECAUSE
WE
PAD
THE
SCHEDULE
WITH
ENOUGH
IDLE
SLOTS
ALL
RELEASE
TIME
AND
DEADLINE
CONSTRAINTS
ARE
SATISFIED
SO
THE
SCHEDULING
INSTANCE
HAS
A
FEASIBLE
SOLUTION
AS
WELL
IT
REMAINS
TO
SHOW
THAT
THE
ABOVE
INSTANCE
OF
RI
HI
PI
UI
CAN
BE
PRODUCED
IN
POLYNOMIAL
TIME
FROM
THE
INSTANCE
OF
PARTITION
INDEED
EVERY
NUMBER
AI
IS
MAPPED
TO
SOME
NUMBER
AI
WHICH
IS
DESCRIBED
WITH
O
AI
BITS
SINCE
WE
ASSUMED
THAT
ALL
NUMBERS
AI
FOR
I
N
ARE
POLYNOMIAL
IN
N
THE
REDUCTION
WILL
TAKE
POLYNOMIAL
TIME
AND
THE
PROOF
IS
COMPLETE
D
THE
ABOVE
CONSTRUCTION
USED
THE
CONSTRAINTS
OF
THE
RELEASE
TIMES
AND
DEADLINES
TO
FIX
TIGHT
JOBS
THAT
FORCE
A
PARTITION
OF
THE
TIME
INTO
INTERVALS
WE
CAN
ACTUALLY
PROVE
A
STRONGER
RESULT
NAMELY
THAT
THE
PROBLEM
REMAINS
NP
COMPLETE
EVEN
IF
ALL
RELEASE
TIMES
ARE
AND
ALL
DEADLINES
ARE
EQUAL
WHY
IS
IT
INTERESTING
ONE
COMMON
APPROACH
IN
DESIGNING
ON
LINE
ALGORITHMS
FOR
SCHEDULING
IS
TO
COMPUTE
THE
OPTIMAL
SCHEDULE
FOR
THE
PENDING
JOBS
AND
USE
THIS
SCHEDULE
TO
MAKE
THE
DECISION
AS
TO
WHAT
EXECUTE
NEXT
THE
SET
OF
PENDING
JOBS
FORMS
AN
INSTANCE
WHERE
ALL
JOBS
HAVE
THE
SAME
RELEASE
TIME
OUR
NP
HARDNESS
RESULT
DOES
NOT
NECESSARILY
IMPLY
THAT
THE
ABOVE
METHOD
CANNOT
WORK
ASSUMING
THAT
WE
DO
NOT
CARE
ABOUT
THE
RUNNING
TIME
OF
THE
ONLINE
ALGORITHM
BUT
IT
MAKES
THIS
APPROACH
MUCH
LESS
APPEALING
SINCE
REASONING
ABOUT
THE
PROPERTIES
OF
THE
PENDING
JOBS
IS
LIKELY
TO
BE
VERY
DIFFICULT
THEOREM
THE
OFFLINE
PROBLEM
RI
HI
PI
UI
IS
STRONGLY
NP
HARD
EVEN
FOR
THE
SPECIAL
CASE
WHEN
JOBS
ARE
RELEASED
AT
TIME
AND
ALL
DEADLINES
ARE
EQUAL
PROOF
THE
REDUCTION
IS
FROM
NUMERICAL
MATCHING
IN
THIS
PROBLEM
WE
ARE
GIVEN
SETS
A
B
C
OF
N
NON
NEGATIVE
INTEGERS
EACH
AND
A
POSITIVE
INTEGER
Β
A
DIMENSIONAL
NUMERICAL
MATCHING
IS
A
SET
OF
N
TRIPLES
A
B
C
A
B
C
SUCH
THAT
EACH
NUMBER
IS
MATCHED
EXACTLY
ONCE
APPEARS
IN
EXACTLY
ONE
TRIPLE
AND
ALL
TRIPLES
A
B
C
IN
THE
SET
SATISFY
A
B
C
Β
NUMERICAL
MATCHING
IS
KNOWN
TO
BE
NP
COMPLETE
EVEN
WHEN
THE
VALUES
OF
ALL
NUMBERS
ARE
BOUNDED
BY
A
POLYNOMIAL
IN
N
IT
IS
REFERENCED
AS
PROBLEM
IN
CLEARLY
THIS
PROBLEM
IS
QUITE
SIMILAR
TO
PARTITION
THAT
WE
USED
IN
THE
PREVIOUS
PROOF
WITHOUT
LOSS
OF
GENERALITY
WE
CAN
ASSUME
THAT
EVERY
X
A
B
C
SATISFIES
X
Β
X
A
B
C
X
ΒN
WE
NOW
DESCRIBE
THE
REDUCTION
LET
BE
THE
CONSTANT
AND
THE
FUNCTION
Α
X
THE
INSTANCE
OF
RI
HI
PI
UI
WILL
HAVE
JOBS
ALL
WITH
RELEASE
TIME
AND
DEADLINE
THESE
JOBS
WILL
BE
OF
TWO
TYPES
I
FIRST
WE
HAVE
JOBS
THAT
CORRESPOND
TO
THE
INSTANCE
OF
NUMERICAL
MATCHING
FOR
EVERY
A
A
THERE
IS
A
JOB
OF
HEAT
CONTRIBUTION
A
FOR
EVERY
B
B
THERE
IS
A
JOB
OF
HEAT
CONTRIBUTION
B
AND
FOR
EVERY
C
C
THERE
IS
A
JOB
OF
HEAT
CONTRIBUTION
C
WE
CALL
THESE
JOBS
RESPECTIVELY
A
JOBS
B
JOBS
AND
C
JOBS
II
NEXT
WE
HAVE
N
GADGET
JOBS
THE
FIRST
OF
THESE
JOBS
HAS
HEAT
CONTRIBUTION
AND
THE
REMAINING
ONES
WE
CALL
THESE
JOBS
RESPECTIVELY
AND
JOBS
WE
CLAIM
THAT
THE
INSTANCE
A
B
C
Β
HAS
A
NUMERICAL
DIMENSIONAL
MATCHING
IF
AND
ONLY
IF
THE
INSTANCE
OF
RI
HI
PI
UI
THAT
WE
CONSTRUCTED
HAS
A
SCHEDULE
WITH
ALL
JOBS
COMPLETED
NOT
LATER
THAN
AT
TIME
THE
IDEA
OF
THE
PROOF
IS
THAT
THE
GADGET
JOBS
ARE
SO
HOT
THAT
THEY
NEED
TO
BE
SCHEDULED
ONLY
EVERY
TH
TIME
UNIT
SEPARATING
THE
TIME
INTO
N
BLOCKS
OF
CONSECUTIVE
TIME
SLOTS
EACH
EVERY
REMAINING
JOB
HAS
A
HEAT
CONTRIBUTION
THAT
CONSISTS
OF
TWO
PARTS
A
CONSTANT
PART
OR
AND
A
TINY
VARIABLE
PART
THAT
DEPENDS
ON
THE
INSTANCE
OF
THE
MATCHING
PROBLEM
THIS
CONSTANT
PART
IS
SO
LARGE
THAT
IN
EVERY
BLOCK
THERE
IS
A
SINGLE
A
JOB
A
SINGLE
B
JOB
AND
A
SINGLE
C
JOB
AND
THEY
MUST
BE
SCHEDULED
IN
THAT
ORDER
THIS
DEFINES
A
PARTITIONING
OF
A
B
C
INTO
TRIPLETS
OF
THE
FORM
A
B
C
A
B
C
SINCE
THE
GADGET
JOBS
ARE
SO
HOT
THEY
FORCE
EVERY
TRIPLE
A
B
C
TO
SATISFY
A
B
C
Β
WE
NOW
MAKE
THIS
ARGUMENT
FORMAL
SUPPOSE
THERE
IS
A
SOLUTION
TO
THE
INSTANCE
OF
NUMERICAL
MATCHING
WE
CON
STRUCT
A
SCHEDULE
WHERE
ALL
JOBS
COMPLETE
AT
OR
BEFORE
TIME
SCHEDULE
THE
JOB
AT
TIME
AND
ALL
OTHER
GADGET
JOBS
EVERY
TH
TIME
SLOT
NOW
THE
REMAINING
SLOTS
ARE
GROUPED
INTO
BLOCKS
CONSISTING
OF
CONSECUTIVE
TIME
SLOTS
EACH
FOR
I
N
ASSOCIATE
THE
I
TH
TRIPLE
A
B
C
FROM
THE
MATCHING
WITH
THE
I
TH
BLOCK
AND
THE
CORRESPONDING
A
B
AND
C
JOBS
ARE
EXECUTED
IN
THIS
BLOCK
IN
THE
ORDER
A
B
C
SEE
FIGURE
A
B
C
A
B
C
A
B
C
A
B
C
FIGURE
THE
STRUCTURE
OF
THE
SCHEDULES
IN
THE
PROOF
BY
CONSTRUCTION
EVERY
JOB
MEETS
THE
DEADLINE
SO
IT
REMAINS
TO
SHOW
THAT
THE
TEMPERATURE
NEVER
EXCEEDS
THE
NON
GADGET
JOBS
HAVE
ALL
HEAT
CONTRIBUTION
SMALLER
THAN
BY
ASSUMPTION
SO
EXECUTION
OF
A
NON
GADGET
JOB
CANNOT
INCREASE
THE
TEMPERATURE
TO
ABOVE
AS
LONG
AS
THE
TEMPERATURE
BEFORE
WAS
NOT
GREATER
THAN
NOW
WE
SHOW
BY
INDUCTION
THAT
RIGHT
AFTER
THE
EXECUTION
OF
A
GADGET
JOB
THE
TEMPERATURE
IS
EXACTLY
THIS
IS
CLEARLY
THE
CASE
AFTER
EXECUTION
OF
THE
FIRST
JOB
SINCE
ITS
HEAT
CONTRIBUTION
IS
NOW
LET
U
BE
THE
TIME
WHEN
A
JOB
IS
SCHEDULED
AND
SUPPOSE
THAT
AT
TIME
U
THE
TEMPERATURE
WAS
LET
A
B
C
BE
THE
TRIPLE
ASSOCIATED
WITH
THE
BLOCK
CONSISTING
OF
TIME
SLOTS
BETWEEN
U
AND
U
THEN
BY
A
B
C
Β
AT
TIME
U
THE
TEMPERATURE
IS
A
B
C
Α
A
B
C
Α
THIS
SHOWS
THAT
AT
TIME
U
AFTER
SCHEDULING
A
JOB
THE
TEMPERATURE
IS
AGAIN
WE
CONCLUDE
THAT
THE
SCHEDULE
IS
FEASIBLE
NOW
WE
SHOW
THE
REMAINING
IMPLICATION
SUPPOSE
THE
INSTANCE
OF
RI
HI
PI
UI
CONSTRUCTED
ABOVE
HAS
A
SCHEDULE
WHERE
ALL
JOBS
MEET
THE
DEADLINE
WE
SHOW
THAT
THERE
EXISTS
A
MATCHING
OF
A
B
C
WE
FIRST
SHOW
THAT
THIS
SCHEDULE
MUST
HAVE
THE
FORM
FROM
FIGURE
FIRST
NOTE
THAT
SINCE
ALL
JOBS
HAVE
DEADLINE
ALL
JOBS
MUST
BE
SCHEDULED
WITHOUT
ANY
IDLE
TIME
BETWEEN
TIME
AND
THIS
MEANS
THAT
THE
GADGET
JOB
OF
HEAT
CONTRIBUTION
MUST
BE
SCHEDULED
AT
TIME
BECAUSE
THAT
IS
THE
ONLY
MOMENT
OF
TEMPERATURE
ALSO
NOTE
THAT
EVERY
JOB
HAS
HEAT
CONTRIBUTION
AT
LEAST
NOW
WE
CLAIM
THAT
ALL
JOBS
HAVE
TO
BE
SCHEDULED
EVERY
TH
TIME
UNIT
THIS
HOLDS
BECAUSE
TWO
UNITS
AFTER
SCHEDULING
A
JOB
THE
TEMPERATURE
IS
AT
LEAST
THEREFORE
TWO
EXECUTIONS
OF
JOBS
MUST
BE
AT
LEAST
TIME
UNITS
APART
AND
THIS
IS
ONLY
POSSIBLE
IF
THEY
ARE
SCHEDULED
EXACTLY
AT
TIMES
FOR
I
N
WE
CLAIM
THAT
AFTER
EVERY
EXECUTION
OF
A
GADGET
JOB
THE
TEMPERATURE
IS
AT
LEAST
Τ
CLEARLY
THIS
IS
THE
CASE
AFTER
THE
EXECUTION
OF
THE
JOB
NOW
ASSUME
THAT
AT
TIME
FOR
SOME
I
N
THE
CLAIM
HOLDS
THEN
AT
TIME
AFTER
THE
EXECUTION
OF
THE
NEXT
JOB
THE
TEMPERATURE
IS
AT
LEAST
Τ
Τ
WE
SHOW
NOW
THAT
EVERY
BLOCK
CONTAINS
EXACTLY
ONE
A
JOB
ONE
B
JOB
AND
ONE
C
JOB
IN
THAT
ORDER
TOWARDS
CONTRADICTION
SUPPOSE
THAT
SOME
A
JOB
IS
SCHEDULED
AT
THE
POSITION
OF
A
BLOCK
SAY
AT
TIME
FOR
SOME
I
N
ITS
HEAT
CONTRIBUTION
IS
AT
LEAST
THEREFORE
THE
TEMPERATURE
AT
TIME
WOULD
BE
AT
LEAST
Τ
CONTRADICTING
THAT
A
JOB
IS
SCHEDULED
AT
THAT
TIME
A
SIMILAR
ARGUMENT
SHOWS
THAT
A
JOBS
CANNOT
BE
SCHEDULED
AT
POSITION
IN
A
BLOCK
AND
THEREFORE
THE
POSITION
OF
A
BLOCK
IS
ALWAYS
OCCUPIED
BY
AN
A
JOB
BY
AN
ANALOGOUS
REASONING
WE
SHOW
THAT
A
B
JOB
CANNOT
BE
SCHEDULED
AT
THE
POSITION
OF
SOME
BLOCK
IT
IT
WERE
SCHEDULED
THERE
THE
TEMPERATURE
AT
THE
END
OF
BLOCK
WOULD
BE
AT
LEAST
Τ
AGAIN
CONTRADICTING
THAT
A
JOB
IS
SCHEDULED
AT
THE
END
OF
THE
BLOCK
WE
SHOWED
THAT
EVERY
BLOCK
CONTAINS
JOBS
THAT
CORRESPOND
TO
SOME
TRIPLE
A
B
C
A
B
C
IT
REMAINS
TO
SHOW
THAT
EACH
SUCH
TRIPLE
SATISFIES
A
B
C
Β
LET
AI
BI
CI
BE
THE
TRIPLE
CORRESPONDING
TO
THE
I
TH
BLOCK
FOR
I
N
DEFINE
AND
TI
AI
BI
CI
AI
BI
CI
Β
THUS
TI
REPRESENTS
THE
CONTRIBUTION
OF
THE
ITH
BLOCK
AND
A
FOLLOWING
GADGET
JOB
TO
THE
TEMPER
ATURE
RIGHT
AFTER
THIS
GADGET
JOB
THIS
IMPLIES
THAT
FOR
K
N
THE
TEMPERATURE
AT
TIME
IS
EXACTLY
K
K
ITI
BY
ASSUMPTION
N
AI
BI
CI
NΒ
AND
THUS
N
I
I
I
TI
N
DEFINE
PI
TI
FOR
I
N
FROM
THE
PREVIOUS
PARAGRAPH
N
PI
I
AS
MENTIONED
EARLIER
K
K
ITI
IS
THE
TEMPERATURE
AT
TIME
SO
WE
HAVE
K
K
ITI
THEREFORE
FOR
ALL
K
N
WE
GET
K
K
K
K
TI
I
I
K
K
K
ITI
K
K
I
K
K
WE
CONCLUDE
THAT
FOR
K
N
WE
HAVE
K
I
TO
COMPLETE
THE
PROOF
IT
REMAINS
TO
SHOW
THAT
PI
FOR
ALL
I
FOR
THIS
WILL
IMPLY
THAT
AI
BI
CI
Β
WHICH
IN
TURN
IMPLIES
THAT
THERE
IS
A
MATCHING
WE
PROVE
THIS
CLAIM
BY
CONTRADICTION
SUPPOSE
THAT
NOT
ALL
PI
ARE
ZERO
LET
F
BE
THE
SMALLEST
INDEX
SUCH
THAT
PF
AND
PF
CLEARLY
F
BY
THE
MINIMALITY
OF
F
FOR
EVERY
K
F
WE
HAVE
PK
AND
PK
PF
THERE
ARE
ΣI
I
F
SUCH
THAT
J
ΣI
THEN
F
F
J
F
F
PJ
ΣIPJ
ΣI
PI
ΣFPF
BECAUSE
ALL
TERMS
ARE
NON
NEGATIVE
AND
PF
THIS
CONTRADICTS
IT
REMAINS
TO
SHOW
THAT
THE
ABOVE
INSTANCE
OF
HI
PI
UI
CAN
BE
PRODUCED
IN
POLYNOMIAL
TIME
FROM
THE
INSTANCE
OF
NUMERICAL
MATCHING
INDEED
EVERY
NUMBER
X
A
B
C
IS
MAPPED
TO
SOME
FRACTION
WHERE
BOTH
THE
DENOMINATOR
AND
NUMERATOR
ARE
LINEAR
IN
X
AND
Β
THEREFORE
IF
WE
REPRESENT
FRACTIONS
BY
WRITING
THE
DENOMINATOR
AND
NUMERATOR
AND
NOT
AS
SOME
ROUNDED
DECIMAL
EXPANSION
THE
REDUCTION
WILL
TAKE
POLYNOMIAL
TIME
AND
THE
PROOF
IS
COMPLETE
D
THEOREM
IMPLIES
THAT
OTHER
VARIANTS
OF
TEMPERATURE
AWARE
SCHEDULING
ARE
NP
HARD
AS
WELL
CONSIDER
FOR
EXAMPLE
THE
PROBLEM
HI
PI
CMAX
WHERE
THE
OBJECTIVE
IS
TO
MINIMIZE
THE
MAKESPAN
THAT
IS
THE
MAXIMUM
COMPLETION
TIME
IN
THE
DECISION
VERSION
OF
THIS
PROBLEM
WE
ASK
IF
ALL
JOBS
CAN
BE
COMPLETED
BY
SOME
GIVEN
DEADLINE
C
WHICH
IS
EXACTLY
WHAT
WE
PROVED
ABOVE
TO
BE
NP
HARD
IT
ALSO
GIVES
US
NP
HARDNESS
OF
HI
PI
CJ
TO
PROVE
THIS
WE
CAN
USE
THE
DECISION
VERSION
OF
THIS
PROBLEM
WHERE
WE
ASK
IF
THERE
IS
A
SCHEDULE
FOR
WHICH
THE
TOTAL
COMPLETION
TIME
IS
AT
MOST
N
N
WHERE
N
IS
THE
NUMBER
OF
JOBS
AN
ONLINE
COMPETITIVE
ALGORITHM
IN
THIS
SECTION
WE
SHOW
THAT
THERE
IS
A
COMPETITIVE
ALGORITHM
FOR
ONLINE
RI
HI
PI
UI
WE
WILL
SHOW
IN
FACT
THAT
A
LARGE
CLASS
OF
DETERMINISTIC
ALGORITHMS
IS
COMPETITIVE
GIVEN
A
SCHEDULE
WE
WILL
SAY
THAT
A
JOB
J
IS
PENDING
AT
TIME
U
IF
J
IS
RELEASED
NOT
EXPIRED
THAT
IS
RJ
U
DJ
AND
J
HAS
NOT
BEEN
SCHEDULED
BEFORE
U
IF
THE
TEMPERATURE
AT
TIME
U
IS
ΤU
AND
J
IS
PENDING
THEN
WE
CALL
J
ADMISSIBLE
IF
ΤU
HJ
THAT
IS
J
IS
NOT
TOO
HOT
TO
BE
EXECUTED
WE
SAY
THAT
A
JOB
J
DOMINATES
A
JOB
K
IF
J
IS
BOTH
NOT
HOTTER
AND
HAS
THE
SAME
OR
SMALLER
DEADLINE
THAN
K
THAT
IS
HJ
HK
AND
DJ
DK
IF
AT
LEAST
ONE
OF
THESE
INEQUALITIES
IS
STRICT
THEN
WE
SAY
THAT
J
STRICTLY
DOMINATES
K
AN
ONLINE
ALGORITHM
IS
CALLED
REASONABLE
IF
AT
EACH
STEP
I
IT
SCHEDULES
A
JOB
WHENEVER
ONE
IS
ADMISSIBLE
THE
NON
WAITING
PROPERTY
AND
IF
THERE
IS
ONE
II
IT
SCHEDULES
AN
ADMISSIBLE
JOB
THAT
IS
NOT
STRICTLY
DOMINATED
BY
ANOTHER
PENDING
JOB
THE
CLASS
OF
REASONABLE
ALGORITHMS
CONTAINS
FOR
EXAMPLE
THE
FOLLOWING
TWO
NATURAL
ALGORITHMS
COOLESTFIRST
ALWAYS
SCHEDULE
A
COOLEST
ADMISSIBLE
JOB
IF
THERE
IS
ANY
BREAKING
TIES
IN
FAVOR
OF
JOBS
WITH
EARLIER
DEADLINES
EARLIESTDEADLINEFIRST
ALWAYS
SCHEDULE
AN
ADMISSIBLE
JOB
IF
THERE
IS
ONE
WITH
THE
EARLIEST
DEADLINE
BREAKING
TIES
IN
FAVOR
OF
COOLER
JOBS
IF
TWO
JOBS
HAVE
THE
SAME
DEADLINES
AND
HEAT
CONTRIBUTIONS
BOTH
ALGORITHMS
GIVE
PREFERENCE
TO
ONE
OF
THEM
ARBITRARILY
THEOREM
ANY
REASONABLE
ALGORITHM
FOR
ONLINE
RI
HI
PI
UI
IS
COMPETITIVE
PROOF
LET
A
BE
ANY
REASONABLE
ALGORITHM
WE
FIX
SOME
INSTANCE
AND
WE
COMPARE
THE
SCHEDULES
PRODUCED
BY
A
AND
THE
ADVERSARY
ON
THIS
INSTANCE
THE
PROOF
IS
BASED
ON
A
CHARGING
SCHEME
THAT
MAPS
JOBS
EXECUTED
BY
THE
ADVERSARY
TO
JOBS
EXECUTED
BY
A
IN
SUCH
A
WAY
THAT
NO
JOB
IN
A
SCHEDULE
GETS
MORE
THAN
TWO
CHARGES
TYPE
CHARGE
A
TYPE
CHARGE
A
A
TYPE
CHARGE
ADV
ADV
HQ
HP
ADV
HJ
HP
HJ
HJ
HJ
HJ
FIGURE
FOUR
TYPES
OF
CHARGES
THE
VERTICAL
INEQUALITY
SIGNS
BETWEEN
THE
SCHEDULES
SHOW
THE
RELATION
BETWEEN
THE
TEMPERATURES
WE
NOW
DESCRIBE
THIS
CHARGING
SCHEME
SEE
FIGURE
FOR
ILLUSTRATION
THERE
WILL
BE
THREE
TYPES
OF
CHARGES
DEPENDING
ON
WHETHER
A
IS
BUSY
OR
IDLE
AT
A
GIVEN
TIME
STEP
AND
ON
THE
RELATIVE
TEMPERATURES
OF
THE
SCHEDULES
OF
A
AND
THE
ADVERSARY
THE
TEMPERATURE
AT
TIME
U
IN
THE
SCHEDULES
OF
AND
THE
ADVERSARY
WILL
BE
DENOTED
BY
ΤU
AND
ΤUL
RESPECTIVELY
SUPPOSE
THAT
AT
SOME
TIME
U
SCHEDULES
A
JOB
K
WHILE
THE
ADVERSARY
SCHEDULES
A
JOB
J
OR
THAT
THE
ADVERSARY
IS
IDLE
WE
TREAT
THIS
CASE
AS
IF
EXECUTING
A
JOB
J
WITH
HJ
THEN
STEP
U
WILL
BE
CALLED
A
RELATIVE
HEATING
STEP
IF
K
IS
STRICTLY
HOTTER
THAN
J
THAT
IS
HK
HJ
NOTE
THAT
IF
ΤV
ΤVL
V
FOR
SOME
TIME
V
THEN
A
RELATIVE
HEATING
STEP
MUST
HAVE
OCCURRED
BEFORE
TIME
CONSIDER
NOW
A
JOB
J
SCHEDULED
BY
THE
ADVERSARY
SAY
AT
TIME
V
THE
CHARGE
OF
J
IS
DEFINED
AS
FOLLOWS
TYPE
CHARGES
IF
SCHEDULES
A
JOB
K
AT
TIME
V
CHARGE
J
TO
K
OTHERWISE
WE
ARE
IDLE
AND
THEN
WE
HAVE
TWO
MORE
CASES
TYPE
CHARGES
SUPPOSE
THAT
A
IS
HOTTER
THAN
THE
ADVERSARY
AT
TIME
V
BUT
NOT
AT
V
THAT
IS
ΤU
ΤUL
AND
ΤU
ΤUL
IN
THIS
CASE
WE
CHARGE
J
TO
THE
JOB
Q
EXECUTED
BY
A
IN
THE
LAST
RELATIVE
HEATING
STEP
BEFORE
V
AS
EXPLAINED
ABOVE
THIS
STEP
EXISTS
TYPE
CHARGES
SUPPOSE
NOW
THAT
EITHER
A
IS
NOT
HOTTER
THAN
THE
ADVERSARY
AT
V
OR
A
IS
HOTTER
THAN
THE
ADVERSARY
AT
V
IN
OTHER
WORDS
ΤV
ΤVL
OR
ΤV
ΤVL
NOTE
THAT
IN
THE
LATTER
CASE
WE
MUST
ALSO
HAVE
ΤV
ΤVL
AS
WELL
SINCE
THE
ALGORITHM
IS
IDLE
WE
CLAIM
THAT
ΤV
HJ
WHICH
MEANS
THAT
NEITHER
J
OR
ANY
JOB
F
WITH
HF
HJ
CAN
BE
PENDING
AT
V
TO
JUSTIFY
THIS
WE
CONSIDER
THE
TWO
SUB
CASES
OF
THE
CONDITION
FOR
TYPE
CHARGES
IF
ΤV
ΤVL
THE
CLAIM
IS
TRIVIAL
SINCE
THEN
ΤV
HJ
ΤVL
HJ
BECAUSE
THE
ADVERSARY
EXECUTES
J
AT
TIME
V
SO
ASSUME
NOW
THAT
ΤV
ΤVL
SINCE
A
IS
IDLE
WE
HAVE
ΤV
THEREFORE
HJ
ΤVL
AND
THE
CLAIM
FOLLOWS
BECAUSE
ΤV
AS
WELL
FROM
THE
ABOVE
CLAIM
J
WAS
SCHEDULED
BY
AT
SOME
TIME
U
V
TO
FIND
A
JOB
THAT
WE
CAN
CHARGE
J
TO
WE
CONSTRUCT
A
CHAIN
OF
JOBS
J
JL
JLL
J
WITH
STRICTLY
DECREASING
HEAT
CONTRIBUTIONS
FURTHER
ALL
JOBS
IN
THIS
CHAIN
EXCEPT
J
WILL
BE
EXECUTED
BY
AT
RELATIVE
HEATING
STEPS
THIS
CHAIN
WILL
BE
DETERMINED
UNIQUELY
BY
J
AND
WE
WILL
CHARGE
J
TO
J
IF
AT
TIME
U
THE
ADVERSARY
IS
IDLE
OR
SCHEDULES
AN
EQUAL
OR
HOTTER
JOB
THEN
J
J
THAT
IS
WE
CHARGE
J
TO
ITSELF
ITS
COPY
IN
A
SCHEDULE
OTHERWISE
IF
THE
ADVERSARY
SCHEDULES
JL
AT
TIME
U
THEN
JL
IS
STRICTLY
COOLER
THAN
J
THAT
IS
HJL
HJ
NOW
WE
CLAIM
THAT
THE
ALGORITHM
SCHEDULES
JL
AT
SOME
TIME
BEFORE
V
INDEED
IF
JL
IS
SCHEDULED
BEFORE
U
WE
ARE
DONE
OTHERWISE
JL
IS
PENDING
AT
U
AND
SINCE
THE
ALGORITHM
NEVER
SCHEDULES
A
DOMINATED
JOB
WE
MUST
HAVE
DJL
DJ
V
BY
OUR
EARLIER
OBSERVATION
AND
BY
HJL
HJ
IF
DID
NOT
SCHEDULE
JL
BEFORE
V
THEN
JL
WOULD
HAVE
BEEN
ADMISSIBLE
AT
V
CONTRADICTING
THE
FACT
IS
IDLE
AT
V
SO
NOW
THE
CHAIN
IS
J
JL
LET
UL
BE
THE
TIME
WHEN
SCHEDULES
JL
IF
THE
ADVERSARY
IS
IDLE
AT
TIME
UL
OR
IF
JL
IS
NOT
HOTTER
THAN
THE
JOB
EXECUTED
BY
THE
ADVERSARY
AT
TIME
UL
WE
TAKE
J
JL
OTHERWISE
WE
TAKE
JLL
TO
BE
THE
JOB
EXECUTED
BY
THE
ADVERSARY
AT
TIME
UL
AND
SO
ON
THIS
PROCESS
MUST
END
AT
SOME
POINT
SINCE
WE
DEAL
WITH
STRICTLY
COOLER
AND
COOLER
JOBS
SO
THE
JOB
J
IS
WELL
DEFINED
THIS
COMPLETES
THE
DESCRIPTION
OF
THE
CHARGING
SCHEME
NOW
WE
SHOW
THAT
ANY
JOB
SCHED
ULED
BY
WILL
GET
AT
MOST
TWO
CHARGES
OBVIOUSLY
EACH
JOB
IN
SCHEDULE
GETS
AT
MOST
ONE
TYPE
CHARGE
IN
BETWEEN
ANY
TWO
TIME
STEPS
THAT
SATISFY
THE
CONDITION
OF
THE
TYPE
CHARGE
THERE
MUST
BE
A
RELATIVE
HEATING
STEP
SO
THE
TYPE
CHARGES
ARE
ASSIGNED
TO
DISTINCT
RELATIVE
HEATING
STEPS
AS
FOR
TYPE
CHARGES
EVERY
CHAIN
DEFINING
A
TYPE
CHARGE
IS
UNIQUELY
DEFINED
BY
THE
FIRST
CONSIDERED
JOB
AND
THESE
CHAINS
ARE
DISJOINT
THEREFORE
TYPE
CHARGES
ARE
ASSIGNED
TO
DISTINCT
JOBS
NOW
LET
K
BE
A
JOB
SCHEDULED
BY
AT
SOME
TIME
V
BY
THE
PREVIOUS
PARAGRAPH
K
CAN
GET
AT
MOST
ONE
CHARGE
OF
EACH
TYPE
WE
CLAIM
THAT
K
CANNOT
GET
CHARGES
OF
EACH
TYPE
AND
INDEED
IF
K
RECEIVES
A
TYPE
CHARGE
THEN
THE
ADVERSARY
IS
NOT
IDLE
AT
TIME
V
AND
SCHEDULES
SOME
JOB
F
IF
K
ALSO
RECEIVES
A
TYPE
CHARGE
THEN
V
MUST
BE
A
RELATIVE
HEATING
STEP
THAT
IS
HK
HF
BUT
TO
RECEIVE
A
TYPE
CHARGE
K
WOULD
BE
THE
LAST
JOB
J
IN
A
CHAIN
OF
SOME
JOB
J
AND
SINCE
THE
CHAIN
WAS
NOT
EXTENDED
FURTHER
WE
MUST
HAVE
HK
HF
SO
TYPE
TYPE
AND
TYPE
CHARGES
CANNOT
COINCIDE
SUMMARIZING
THE
ARGUMENT
WE
HAVE
THAT
EVERY
JOB
SCHEDULED
BY
THE
ADVERSARY
IS
CHARGED
TO
SOME
JOB
SCHEDULED
BY
A
AND
EVERY
JOB
SCHEDULED
BY
A
RECEIVES
NO
MORE
THAN
CHARGES
THEREFORE
THE
COMPETITIVE
RATIO
OF
A
IS
NOT
MORE
THAN
D
A
LOWER
BOUND
ON
THE
COMPETITIVE
RATIO
THEOREM
EVERY
DETERMINISTIC
ONLINE
ALGORITHM
FOR
ONLINE
RI
HI
PI
UI
HAS
COM
PETITIVE
RATIO
AT
LEAST
PROOF
FIX
SOME
DETERMINISTIC
ONLINE
ALGORITHM
WE
THE
ADVERSARY
RELEASE
A
JOB
IN
NOTATION
J
RJ
DJ
HJ
IF
SCHEDULES
IT
AT
TIME
WE
RELEASE
AT
TIME
A
TIGHT
JOB
AND
SCHEDULE
IT
FOLLOWED
BY
SCHEDULE
IS
TOO
HOT
AT
TIME
TO
EXECUTE
JOB
IF
DOES
NOT
SCHEDULE
JOB
AT
TIME
THEN
WE
SCHEDULE
IT
AT
AND
RELEASE
AT
TIME
AND
SCHEDULE
A
TIGHT
JOB
AT
TIME
IN
THIS
CASE
CANNOT
COMPLETE
BOTH
JOBS
AND
WITHOUT
VIOLATING
THE
THERMAL
THRESHOLD
IN
BOTH
CASES
WE
SCHEDULE
TWO
JOBS
WHILE
A
SCHEDULES
ONLY
ONE
COMPLETING
THE
PROOF
SEE
FIGURE
D
ADV
ADV
A
A
FIGURE
THE
LOWER
BOUND
FOR
DETERMINISTIC
ALGORITHMS
FINAL
COMMENTS
MANY
OPEN
PROBLEMS
REMAIN
PERHAPS
THE
MOST
INTRIGUING
ONE
IS
TO
DETERMINE
THE
RANDOMIZED
COMPETITIVE
RATIO
FOR
THE
PROBLEM
WE
STUDIED
THE
PROOF
OF
THEOREM
CAN
EASILY
BE
ADAPTED
TO
PROVE
THE
LOWER
BOUND
OF
BUT
WE
HAVE
NOT
BEEN
ABLE
TO
IMPROVE
THE
UPPER
BOUND
OF
THIS
IS
IN
FACT
THE
MAIN
FOCUS
OF
OUR
CURRENT
WORK
ON
THIS
SCHEDULING
PROBLEM
ONE
APPROACH
BASED
ON
THEOREM
WOULD
BE
TO
RANDOMLY
CHOOSE
AT
THE
BEGINNING
OF
COMPUTATION
TWO
DIFFERENT
REASONABLE
ALGORITHMS
EACH
WITH
PROBABILITY
AND
THEN
DETERMINISTICALLY
EXECUTE
THE
CHOSEN
I
SO
FAR
WE
HAVE
BEEN
ABLE
TO
SHOW
THAT
FOR
MANY
NATURAL
CHOICES
FOR
AND
SAY
COOLESTFIRST
AND
EARLIESTDEADLINEFIRST
THIS
APPROACH
DOES
NOT
WORK
EXTENSIONS
OF
THE
COOLING
MODEL
CAN
BE
CONSIDERED
WHERE
THE
TEMPERATURE
AFTER
EXECUTING
J
IS
Τ
HJ
R
FOR
SOME
R
EVEN
THIS
FORMULA
HOWEVER
IS
ONLY
A
DISCRETE
APPROXIMATION
FOR
THE
TRUE
MODEL
SEE
FOR
EXAMPLE
AND
IT
WOULD
BE
INTERESTING
TO
SEE
IF
THE
IDEAS
BEHIND
OUR
COMPETITIVE
ALGORITHM
CAN
BE
ADAPTED
TO
THESE
MORE
REALISTIC
CASES
IN
REALITY
THERMAL
VIOLATIONS
DO
NOT
CAUSE
THE
SYSTEM
TO
IDLE
BUT
ONLY
TO
REDUCE
THE
FREQUENCY
WITH
FREQUENCY
REDUCED
TO
HALF
A
UNIT
JOB
WILL
EXECUTE
FOR
TWO
TIME
SLOTS
SEVERAL
FREQUENCY
LEVELS
MAY
BE
AVAILABLE
WE
ASSUMED
THAT
THE
HEAT
CONTRIBUTIONS
ARE
KNOWN
THIS
IS
COUNTER
INTUITIVE
BUT
NOT
UNREALISTIC
SINCE
THE
JOBS
IN
OUR
MODEL
ARE
UNIT
SLICES
OF
LONGER
JOBS
PREDICTION
METHODS
ARE
AVAILABLE
THAT
CAN
QUITE
ACCURATELY
PREDICT
THE
HEAT
CONTRIBUTION
OF
EACH
SLICE
BASED
ON
THE
HEAT
CONTRIBUTIONS
OF
THE
PREVIOUS
SLICES
NEVERTHELESS
IT
MAY
BE
INTERESTING
TO
STUDY
A
MODEL
WHERE
EXACT
HEAT
CONTRIBUTIONS
ARE
NOT
KNOWN
OTHER
TYPES
OF
JOBS
MAY
BE
STUDIED
FOR
REAL
TIME
JOBS
ONE
CAN
CONSIDER
THE
CASE
WHEN
NOT
ALL
JOBS
ARE
EQUALLY
IMPORTANT
WHICH
CAN
BE
MODELED
BY
ASSIGNING
WEIGHTS
TO
JOBS
AND
MAXI
MIZING
THE
WEIGHTED
THROUGHPUT
FOR
BATCH
JOBS
OTHER
OBJECTIVE
FUNCTIONS
CAN
BE
OPTIMIZED
FOR
EXAMPLE
THE
FLOW
TIME
ONE
MORE
REALISTIC
SCENARIO
WOULD
BE
TO
REPRESENT
THE
WHOLE
PROCESSES
AS
JOBS
RATHER
THEN
THEIR
SLICES
THIS
NATURALLY
LEADS
TO
SCHEDULING
PROBLEMS
WITH
PREEMPTION
AND
WITH
JOBS
OF
ARBITRARY
PROCESSING
TIMES
WHEN
THE
THERMAL
THRESHOLD
IS
REACHED
THE
EXECUTION
OF
A
JOB
IS
SLOWED
DOWN
BY
A
FACTOR
OF
HERE
A
SCHEDULING
ALGORITHM
MAY
DECIDE
TO
PREEMPT
A
JOB
WHEN
ANOTHER
ONE
IS
RELEASED
OR
SAY
WHEN
THE
PROCESSOR
GETS
TOO
HOT
FINALLY
IN
MULTI
CORE
SYSTEMS
ONE
CAN
EXPLORE
THE
MIGRATIONS
SAY
MOVING
JOBS
FROM
HOTTER
TO
COOLER
CORES
TO
KEEP
THE
TEMPERATURE
UNDER
CONTROL
THIS
LEADS
TO
EVEN
MORE
SCHEDULING
PROBLEMS
THAT
MAY
BE
WORTH
TO
STUDY
NETWORKED
END
SYSTEMS
SUCH
AS
DESKTOPS
AND
SET
TOP
BOXES
ARE
OFTEN
LEFT
POWERED
ON
BUT
IDLE
LEADING
TO
WASTED
ENERGY
CONSUMPTION
AN
ALTERNATIVE
WOULD
BE
FOR
THESE
IDLE
SYSTEMS
TO
ENTER
LOW
POWER
SLEEP
MODES
UN
FORTUNATELY
TODAY
A
SLEEPING
SYSTEM
SEES
DEGRADED
FUNC
TIONALITY
FIRST
A
SLEEPING
DEVICE
LOSES
ITS
NETWORK
PRES
ENCE
WHICH
IS
PROBLEMATIC
TO
USERS
AND
APPLICATIONS
THAT
EXPECT
TO
MAINTAIN
ACCESS
TO
A
REMOTE
MACHINE
AND
SEC
OND
SLEEPING
CAN
PREVENT
RUNNING
TASKS
SCHEDULED
DUR
ING
TIMES
OF
LOW
UTILIZATION
E
G
NETWORK
BACKUPS
VAR
IOUS
SOLUTIONS
TO
THESE
PROBLEMS
HAVE
BEEN
PROPOSED
OVER
THE
YEARS
INCLUDING
WAKE
ON
LAN
WOL
MECHANISMS
THAT
WAKE
HOSTS
WHEN
SPECIFIC
PACKETS
ARRIVE
AND
THE
USE
OF
A
PROXY
THAT
HANDLES
IDLE
TIME
TRAFFIC
ON
BEHALF
OF
A
SLEEP
ING
HOST
AS
OF
YET
HOWEVER
AN
IN
DEPTH
EVALUATION
OF
THE
POTENTIAL
FOR
ENERGY
SAVINGS
AND
THE
EFFECTIVENESS
OF
PROPOSED
SOLUTIONS
HAS
NOT
BEEN
CARRIED
OUT
TO
REMEDY
THIS
IN
THIS
PAPER
WE
COLLECT
DATA
DIRECTLY
FROM
EN
TERPRISE
USERS
ON
THEIR
END
HOST
MACHINES
CAPTURING
NET
WORK
TRAFFIC
PATTERNS
AND
USER
PRESENCE
INDICATORS
WITH
THIS
DATA
WE
ANSWER
SEVERAL
QUESTIONS
WHAT
IS
THE
PO
TENTIAL
VALUE
OF
PROXYING
OR
USING
MAGIC
PACKETS
WHICH
PROTOCOLS
AND
APPLICATIONS
REQUIRE
PROXYING
HOW
COM
PREHENSIVE
DOES
PROXYING
NEED
TO
BE
FOR
ENERGY
BENEFITS
TO
BE
COMPELLING
AND
SO
ON
WE
FIND
THAT
ALTHOUGH
THERE
IS
INDEED
MUCH
POTENTIAL
FOR
ENERGY
SAVINGS
TRIVIAL
APPROACHES
ARE
NOT
EFFECTIVE
WE
ALSO
FIND
THAT
ACHIEVING
SUBSTANTIAL
SAVINGS
REQUIRES
A
CAREFUL
CONSIDERATION
OF
THE
TRADEOFFS
BETWEEN
THE
PROXY
COMPLEXITY
AND
THE
IDLE
TIME
FUNCTIONALITY
AVAILABLE
TO
USERS
AND
THAT
THESE
TRADEOFFS
VARY
WITH
USER
ENVIRON
MENT
BASED
ON
OUR
FINDINGS
WE
PROPOSE
AND
EVALUATE
A
PROXY
ARCHITECTURE
THAT
EXPOSES
A
MINIMAL
SET
OF
APIS
TO
SUPPORT
DIFFERENT
FORMS
OF
IDLE
TIME
BEHAVIOR
INTERNATIONAL
COMPUTER
SCIENCE
INSTITUTE
INTEL
RESEARCH
UNIVERSITY
OF
CALIFORNIA
BERKELEY
LAWRENCE
BERKELEY
NATIONAL
LABORATORIES
INTRODUCTION
RECENT
YEARS
HAVE
SEEN
RISING
CONCERN
OVER
THE
ENERGY
CONSUMPTION
OF
OUR
COMPUTING
INFRASTRUCTURE
A
RECENT
STUDY
ESTIMATES
THAT
IN
THE
U
ALONE
ENERGY
CON
SUMPTION
FOR
NETWORKED
SYSTEMS
APPROACHES
TWH
WITH
AN
ASSOCIATED
COST
OF
AROUND
BILLION
DOLLARS
ABOUT
OF
THIS
CONSUMPTION
CAN
BE
ATTRIBUTED
TO
HOMES
AND
ENTERPRISES
AND
THE
REMAINING
TO
NET
WORKS
AND
DATA
CENTERS
OUR
FOCUS
IN
THIS
PAPER
IS
ON
RE
DUCING
THE
CONSUMED
IN
HOMES
AND
ENTERPRISES
TO
PUT
THIS
IN
PERSPECTIVE
THIS
ENERGY
TWH
IS
ROUGHLY
EQUIVALENT
TO
THE
YEARLY
OUTPUT
OF
NUCLEAR
PLANTS
OF
EQUAL
CONCERN
IS
THAT
THIS
CONSUMPTION
HAS
GROWN
AND
CONTINUES
TO
GROW
AT
A
RAPID
PACE
IN
RESPONSE
TO
THESE
ENERGY
CONCERNS
COMPUTER
VEN
DORS
HAVE
DEVELOPED
SOPHISTICATED
POWER
MANAGEMENT
TECHNIQUES
THAT
OFFER
VARIOUS
OPTIONS
BY
WHICH
TO
REDUCE
COMPUTER
POWER
CONSUMPTION
BROADLY
THESE
TECHNIQUES
ALL
BUILD
ON
HARDWARE
SUPPORT
FOR
SLEEP
STATES
AND
FREQUENCY
VOLTAGE
SCALING
PROCESSOR
P
STATES
THE
FORMER
IS
INTENDED
TO
REDUCE
POWER
CONSUMPTION
DURING
IDLE
TIMES
BY
POWERING
DOWN
SUB
COMPONENTS
TO
DIFFERENT
EXTENTS
WHILE
THE
LATTER
REDUCES
POWER
CON
SUMPTION
WHILE
ACTIVE
BY
LOWERING
PROCESSOR
OPERATING
FREQUENCY
AND
VOLTAGE
DURING
ACTIVE
PERIODS
OF
LOW
SYS
TEM
UTILIZATION
OF
THESE
SLEEP
MODES
OFFER
THE
GREATEST
REDUCTION
IN
THE
POWER
DRAW
OF
MACHINES
THAT
ARE
IDLE
FOR
EXAMPLE
A
TYPICAL
SLEEPING
DESKTOP
DRAWS
NO
MORE
THAN
AS
COMPARED
TO
AT
LEAST
WHEN
ON
BUT
IDLE
AN
ORDER
OF
MAGNITUDE
REDUCTION
IT
IS
THUS
UNFORTUNATE
THAT
SLEEP
MODES
ARE
NOT
TAKEN
ADVANTAGE
OF
TO
ANYWHERE
CLOSE
TO
THEIR
FULLEST
POTENTIAL
SURVEYS
OF
OFFICE
BUILDINGS
HAVE
SHOWN
THAT
ABOUT
TWO
THIRDS
OF
DESKTOPS
ARE
FULLY
ON
AT
NIGHT
WITH
ONLY
ASLEEP
OUR
OWN
MEASUREMENTS
SECTION
REVEAL
THAT
ENTERPRISE
DESKTOPS
REMAIN
IDLE
FOR
AN
AVERAGE
OF
HOURS
DAY
TIME
THAT
COULD
IN
THEORY
BE
SPENT
MOSTLY
SLEEPING
RELATIVE
TO
AN
IDLE
MACHINE
THE
ONLY
LOSS
OF
FUNCTIONAL
ITY
TO
A
SLEEPING
MACHINE
IS
TWOFOLD
FIRST
SINCE
A
SLEEP
ING
COMPUTER
CANNOT
RECEIVE
OR
TRANSMIT
NETWORK
MES
SAGES
IT
EFFECTIVELY
LOSES
ITS
PRESENCE
ON
THE
NETWORK
THIS
CAN
LEAD
TO
BROKEN
CONNECTIONS
AND
SESSIONS
WHEN
THE
MACHINE
RESUMES
E
G
A
SLEEPING
MACHINE
DOES
NOT
RENEW
ITS
DHCP
LEASE
AND
HENCE
LOSES
ITS
IP
ADDRESS
AND
ALSO
PREVENTS
REMOTE
ACCESS
TO
A
SLEEPING
COMPUTER
THIS
LOSS
OF
FUNCTIONALITY
IS
PROBLEMATIC
IN
AN
INCREAS
INGLY
NETWORKED
WORLD
FOR
EXAMPLE
A
USER
AT
HOME
MIGHT
WANT
TO
ACCESS
FILES
ON
HIS
DESKTOP
AT
WORK
AN
ON
THE
ROAD
USER
MIGHT
WANT
TO
DOWNLOAD
FILES
FROM
HIS
HOME
MACHINE
TO
HIS
HANDHELD
SYSTEM
ADMINISTRATORS
MIGHT
DESIRE
ACCESS
TO
ENTERPRISE
MACHINES
FOR
SOFTWARE
UPDATES
SECURITY
CHECKS
AND
SO
FORTH
IN
FACT
SOME
EN
TERPRISES
REQUIRE
THAT
USERS
NOT
POWER
OFF
THEIR
DESK
TOPS
TO
ENSURE
ADMINISTRATORS
CAN
ACCESS
MACHINES
AT
ALL
TIMES
THE
SECOND
PROBLEMATIC
SCENARIO
IS
WHEN
USERS
OR
ADMINISTRATORS
DELIBERATELY
WANT
TO
SCHEDULE
TASKS
TO
RUN
DURING
IDLE
TIMES
E
G
NETWORK
BACKUPS
THAT
RUN
AT
NIGHT
CRITICAL
SOFTWARE
UPDATES
AND
SO
ON
UNFORTU
NATELY
THESE
DRAWBACKS
CAUSE
USERS
TO
FOREGO
THE
USE
OF
SLEEP
MODES
LEADING
TO
WASTEFUL
ENERGY
CONSUMPTION
THE
ABOVE
OBSERVATIONS
ARE
NOT
NEW
HAVING
BEEN
RE
PEATEDLY
ARTICULATED
ALSO
BY
SOME
OF
THE
AUTHORS
IN
BOTH
THE
TECHNICAL
LITERATURE
AND
POPULAR
PRESS
LIKEWISE
THERE
HAVE
BEEN
TWO
LONG
STANDING
PRO
POSALS
ON
HOW
TO
TACKLE
THE
PROBLEM
THE
FIRST
IS
TO
GEN
ERALIZE
THE
OLD
TECHNOLOGY
OF
WAKE
ON
LAN
WOL
AN
ETHERNET
COMPUTER
NETWORKING
STANDARD
THAT
ALLOWS
A
MA
CHINE
TO
BE
TURNED
ON
OR
WOKEN
UP
REMOTELY
BY
A
SPECIAL
MAGIC
PACKET
A
SECOND
MORE
HEAVYWEIGHT
PROPOSAL
HAS
BEEN
TO
USE
A
PROXY
THAT
HANDLES
IDLE
TIME
TRAFFIC
ON
BEHALF
OF
A
SLEEPING
HOST
WAKING
THE
SLEEPING
HOST
WHEN
APPROPRIATE
THUS
BOTH
PROBLEM
WASTED
ENERGY
CONSUMPTION
BY
IDLE
COMPUTERS
AND
PROPOSED
SOLUTIONS
WAKE
UP
PACKETS
AND
OR
PROXIES
FOR
SLEEPING
MACHINES
HAVE
EXISTED
FOR
A
WHILE
NOW
IN
FACT
THE
TECHNOLOGY
FOR
WOL
HAS
BEEN
IMPLEMENTED
AND
DEPLOYED
ALTHOUGH
NOT
WIDELY
USED
WE
EXPLORE
POSSIBLE
CAUSES
FOR
THIS
LATER
IN
THE
PAPER
HOWEVER
THE
RECENT
FOCUS
ON
ENERGY
CON
SUMPTION
HAS
LED
TO
RENEWED
INTEREST
IN
THE
TOPIC
WITH
CALLS
FOR
RESEARCH
CALLS
FOR
STANDARDIZATION
AND
EVEN
SOME
COMMERCIAL
PROTOTYPES
AS
YET
HOW
EVER
THERE
HAS
BEEN
LITTLE
SYSTEMATIC
AND
IN
DEPTH
EVALUA
TION
OF
THE
PROBLEM
OR
ITS
SOLUTIONS
WHAT
SAVINGS
MIGHT
SUCH
SOLUTIONS
ENABLE
WHAT
IS
THE
BROADER
DESIGN
SPACE
FOR
SOLUTIONS
WHAT
IF
ANY
MIGHT
BE
THE
ROLE
OF
STANDARD
IZATION
ARE
THESE
THE
RIGHT
LONG
TERM
SOLUTIONS
ETC
IN
THIS
PAPER
WE
EXPLORE
THESE
QUESTIONS
BY
STUDYING
USER
BEHAVIOR
AND
NETWORK
TRAFFIC
IN
AN
ENTERPRISE
ENVI
RONMENT
SPECIFICALLY
WE
FOCUS
ON
ANSWERING
THE
FOLLOW
ING
QUESTIONS
IS
THE
PROBLEM
WORTH
SOLVING
JUST
HOW
MUCH
ENERGY
IS
SQUANDERED
DUE
TO
POOR
COMPUTER
SLEEPING
HABITS
THIS
WILL
TELL
US
THE
POTENTIAL
ENERGY
SAVINGS
THESE
SOLUTIONS
STAND
TO
ENABLE
AND
HENCE
THE
COMPLEXITY
THEY
WARRANT
ALSO
IS
PROXYING
REALLY
NEEDED
TO
REALIZE
THESE
POTENTIAL
SAVINGS
OR
CAN
WE
HOPE
THAT
WOL
SUFFICES
TO
MAINTAIN
NETWORK
PRESENCE
WHILE
STILL
SLEEPING
USEFULLY
WHAT
NETWORK
TRAFFIC
DO
IDLE
MACHINES
SEE
UN
DERSTANDING
THIS
WILL
SHED
LIGHT
ON
HOW
THIS
IDLE
TIME
TRAF
FIC
MIGHT
BE
DEALT
WITH
AND
CONSEQUENTLY
WHAT
PROTOCOLS
AND
APPLICATIONS
MIGHT
TRIGGER
WAKE
UP
PACKETS
AND
OR
REQUIRE
PROXYING
ON
THE
FACE
OF
IT
IT
WOULD
SEEM
LIKE
AN
IDLE
MACHINE
OUGHT
NOT
TO
BE
ENGAGED
IN
MUCH
USEFUL
ACTIVITY
AND
HENCE
IDEALLY
ONE
MIGHT
HOPE
THAT
A
SMALL
NUMBER
OF
WAKE
UP
EVENTS
ARE
REQUIRED
AND
OR
THAT
A
REL
ATIVELY
SMALL
SET
OF
PROTOCOLS
MUST
BE
PROXIED
TO
REALIZE
USEFUL
SAVINGS
WHAT
IS
THE
DESIGN
SPACE
FOR
A
PROXY
IN
GENERAL
THE
SPACE
APPEARS
LARGE
DIFFERENT
PROXY
IMPLEMENTATIONS
MIGHT
VARY
IN
THE
COMPLEXITY
THEY
UNDERTAKE
IN
TERMS
OF
WHAT
WORK
IS
HANDLED
BY
THE
PROXY
VS
WAKING
THE
MA
CHINE
TO
DO
SO
IN
SOME
CASES
ONE
MIGHT
OPT
FOR
A
RELA
TIVELY
SIMPLE
PROXY
THAT
FOR
EXAMPLE
ONLY
RESPONDS
TO
CERTAIN
PROTOCOLS
SUCH
AS
ARP
SPECIFIED
BY
THE
DMTF
STANDARD
AND
NETBIOS
BUT
MORE
COMPLEX
PROXIES
ARE
ALSO
CONCEIVABLE
FOR
EXAMPLE
A
PROXY
MIGHT
TAKE
ON
APPLICATION
SPECIFIC
PROCESSING
SUCH
AS
INITIAT
ING
COMPLETING
BITTORRENT
DOWNLOADS
DURING
IDLE
TIMES
AND
SO
FORTH
LIKEWISE
THERE
ARE
MANY
CONCEIVABLE
DE
PLOYMENT
OPTIONS
A
PROXY
MIGHT
RUN
AT
A
NETWORK
MID
DLEBOX
E
G
FIREWALL
NAT
ETC
AT
A
SEPARATE
MACHINE
ON
EACH
SUBNET
OR
EVEN
AT
INDIVIDUAL
MACHINES
E
G
ON
ITS
NIC
ON
A
MOTHERBOARD
CONTROLLER
OR
ON
A
USB
ATTACHED
LIGHTWEIGHT
MICROENGINE
GIVEN
THIS
BREADTH
OF
OPTIONS
WE
ARE
INTERESTED
IN
WHETHER
ONE
CAN
IDEN
TIFY
A
MINIMAL
PROXY
ARCHITECTURE
THAT
EXPOSES
A
SET
OF
OPEN
APIS
THAT
WOULD
ACCOMMODATE
A
SPECTRUM
OF
DESIGN
CHOICES
AND
DEPLOYMENT
MODELS
DOING
SO
APPEARS
IM
PORTANT
BECAUSE
A
PROXY
POTENTIALLY
INTERACTS
WITH
A
DIVER
SITY
OF
SYSTEM
COMPONENTS
AND
EVEN
VENDORS
HARDWARE
POWER
MANAGEMENT
OPERATING
SYSTEMS
HIGHER
LAYER
AP
PLICATIONS
NETWORK
SWITCHES
NICS
ETC
AND
HENCE
IDEN
TIFYING
A
CORE
SET
OF
OPEN
APIS
WOULD
ALLOW
DIFFERENT
VEN
DORS
TO
CO
EXIST
AND
YET
INNOVATE
INDEPENDENTLY
FOR
EX
AMPLE
AN
APPLICATION
DEVELOPER
SHOULD
BE
ABLE
TO
DEFINE
THE
MANNER
IN
WHICH
HIS
APPLICATION
INTERACTS
WITH
THE
PROXY
WITH
NO
CONCERN
FOR
WHETHER
THE
PROXY
IS
DEPLOYED
AT
A
FIREWALL
A
SEPARATE
MACHINE
OR
A
NIC
WHAT
IMPLICATIONS
DOES
PROXYING
HAVE
FOR
FUTURE
PROTOCOL
AND
SYSTEM
DESIGN
THE
NEED
FOR
A
PROXY
ARISES
LARGELY
BECAUSE
NETWORK
PROTOCOLS
AND
APPLICA
TIONS
WERE
NEVER
DESIGNED
WITH
ENERGY
EFFICIENCY
IN
MIND
NOR
TO
USEFULLY
EXPLOIT
OR
EVEN
CO
EXIST
WITH
POWER
MAN
AGEMENT
IN
MODERN
PCS
AND
OPERATING
SYSTEMS
WHILE
PROXIES
OFFER
A
PRAGMATIC
APPROACH
TO
DEALING
WITH
THIS
MISMATCH
FOR
CURRENTLY
DEPLOYED
PROTOCOLS
AND
SOFTWARE
ONE
MIGHT
ALSO
TAKE
A
LONGER
TERM
VIEW
OF
THE
PROBLEM
AND
ASK
HOW
WE
MIGHT
REDESIGN
PROTOCOLS
APPLICATIONS
OR
EVEN
HARDWARE
POWER
MANAGEMENT
TO
EVENTUALLY
OBVI
ATE
THE
NEED
FOR
SUCH
PROXYING
ALTOGETHER
IN
THIS
PAPER
WE
STUDY
THE
NETWORK
RELATED
BEHAVIOR
OF
USERS
AND
MACHINES
IN
ENTERPRISE
AND
HOME
ENVIRON
MENTS
AND
EVALUATE
EACH
OF
THE
ABOVE
QUESTIONS
IN
SEC
TIONS
TO
RESPECTIVELY
MEASUREMENT
DATA
AND
METHODOLOGY
WE
COLLECTED
NETWORK
AND
USER
LEVEL
ACTIVITY
TRACES
FROM
APPROXIMATELY
CLIENT
MACHINES
BELONGING
TO
INTEL
CORPORATION
EMPLOYEES
FOR
A
PERIOD
OF
APPROXIMATELY
WEEKS
THE
MACHINES
RUNNING
WINDOWS
XP
INCLUDE
BOTH
DESKTOPS
AND
NOTEBOOKS
APPROXIMATELY
ARE
DESKTOPS
AND
THE
REST
NOTEBOOKS
OUR
TRACE
COLLECTION
SOFTWARE
WAS
RUN
AT
THE
INDIVID
UAL
END
HOSTS
THEMSELVES
AND
HENCE
IN
THE
CASE
OF
NOTE
BOOKS
TRACE
COLLECTION
CONTINUED
UNINTERRUPTED
AS
THE
USER
MOVED
BETWEEN
ENTERPRISE
AND
HOME
ENABLING
US
TO
ANALYZE
TRAFFIC
FROM
BOTH
OF
THESE
ENVIRONMENTS
OUR
PACKET
TRACES
WERE
COLLECTED
USING
WINDUMP
TO
CAPTURE
USER
ACTIVITY
WE
DEVELOPED
AN
APPLICATION
THAT
SAMPLED
A
NUMBER
OF
USER
ACTIVITY
INDICATORS
AT
ONE
SEC
OND
INTERVALS
THE
USER
ACTIVITY
INDICATORS
WE
COLLECTED
INCLUDED
KEYBOARD
ACTIVITY
AND
MOUSE
MOVEMENTS
AND
CLICKS
NOTICEABLE
GAPS
IN
THE
TRACES
OCCUR
WHEN
THE
HOST
WAS
TURNED
OFF
PUT
TO
SLEEP
OR
IN
HIBERNATION
THUS
EACH
END
HOST
IS
ASSOCIATED
WITH
A
TRACE
OF
ITS
NETWORK
AND
USER
ACTIVITY
WE
THEN
USED
BRO
TO
REASSEMBLE
CONNECTION
LEVEL
INFORMATION
FROM
EACH
PACKET
LEVEL
TRACE
THUS
FOR
THE
WEEK
DURATION
OF
OUR
MEASUREMENT
STUDY
WE
HAVE
THE
FOLLOWING
INFORMATION
FOR
EACH
END
HOST
A
PACKET
LEVEL
PCAP
TRACE
CAPTURING
PACKET
HEADERS
FOR
THE
ENTIRE
DURATION
PER
SECOND
INDICATORS
OF
USER
PRESENCE
AT
THE
MACHINE
THE
SET
OF
ALL
CONNECTIONS
INCOMING
AND
OUTGOING
AS
RECONSTRUCTED
BY
BRO
FROM
THE
PACKET
TRACES
THE
RESULT
IS
A
REPOSITORY
OF
TRACE
DATA
TO
PRO
CESS
THIS
WE
DEVELOPED
A
CUSTOM
TOOL
THAT
EXTENDS
THE
PUBLICLY
AVAILABLE
WIRESHARK
NETWORK
PROTOCOL
AN
ALYZER
WITH
DIFFERENT
FUNCTION
CALLBACKS
IMPLEMENTING
THE
ADDITIONAL
PROCESSING
REQUIRED
FOR
OUR
STUDY
LOW
POWER
PROXYING
POTENTIAL
AND
NEED
IN
THIS
SECTION
WE
ESTIMATE
THE
ENERGY
WASTED
BY
HOME
AND
OFFICE
COMPUTERS
THAT
REMAIN
POWERED
ON
EVEN
WHEN
IDLE
I
E
EVEN
WHEN
THERE
IS
NO
HUMAN
INTERACTING
WITH
THE
COMPUTER
SUBSEQUENTLY
WE
INVESTIGATE
WHETHER
VERY
SIMPLE
APPROACHES
E
G
THE
COMPUTER
IS
WOKEN
UP
TO
PROCESS
EVERY
NETWORK
PACKET
AND
THEN
RETURNS
TO
SLEEP
IMMEDIATELY
AFTER
WOULD
SUFFICE
IN
ALLOWING
HOSTS
TO
SLEEP
MORE
WHILE
PRESERVING
THEIR
NETWORK
PRESENCE
HOW
MUCH
ENERGY
IS
SQUANDERED
BY
NOT
SLEEPING
VIRTUALLY
ALL
MODERN
COMPUTERS
SUPPORT
ADVANCED
SLEEP
STATES
AS
DEFINED
IN
THE
ACPI
SPECIFICATION
FIGURE
DISTRIBUTION
OF
THE
SPLIT
AMONG
OFF
IDLE
AND
ACTIVE
PERIODS
ACROSS
USERS
THESE
STATES
VARY
IN
THEIR
CHARACTERISTICS
WHETHER
THE
CPU
IS
POWERED
OFF
HOW
MUCH
MEMORY
STATE
IS
LOST
WHICH
BUSES
ARE
CLOCKED
AND
SO
ON
HOWEVER
COMMON
TO
ALL
STATES
IS
THAT
THE
CPU
STOPS
EXECUTING
INSTRUCTIONS
AND
HENCE
THE
COMPUTER
APPEARS
TO
BE
POWERED
DOWN
THUS
ALTHOUGH
THESE
SLEEP
STATES
CONSERVE
ENERGY
THE
UN
DESIRABLE
SIDE
EFFECT
IS
THAT
A
SLEEPING
COMPUTER
EFFEC
TIVELY
FALLS
OFF
THE
NETWORK
MAKING
IT
UNAVAILABLE
FOR
REMOTE
ACCESS
AND
UNABLE
TO
PERFORM
ROUTINE
TASKS
THAT
MAY
HAVE
BEEN
SCHEDULED
AT
PARTICULAR
TIMES
THIS
LEADS
MANY
USERS
TO
DISABLE
POWER
MANAGEMENT
ALTOGETHER
AND
INSTEAD
LEAVE
MACHINES
RUNNING
FOR
EXAMPLE
STUD
IES
HAVE
SHOWN
THAT
APPROXIMATELY
OF
THE
PCS
IN
OF
FICE
BUILDINGS
REMAIN
POWERED
ON
OVERNIGHT
AND
ALMOST
ALL
OF
THESE
HAVE
POWER
MANAGEMENT
DISABLED
TO
MORE
CAREFULLY
QUANTIFY
THE
AMOUNT
OF
WASTED
EN
ERGY
AND
HENCE
POTENTIAL
SAVINGS
WE
ANALYZED
THE
TRACE
DATA
COLLECTED
AT
OUR
ENTERPRISE
MACHINES
TO
DETERMINE
WHETHER
A
MACHINE
HAS
A
LOCALLY
PRESENT
AND
ACTIVE
USER
WE
EXAMINE
THE
RECORDED
MOUSE
AND
KEYBOARD
ACTIVITY
FOR
THE
MACHINE
IF
NO
SUCH
ACTIVITY
IS
RECORDED
FOR
MINUTES
WE
SAY
THAT
THE
MACHINE
IS
IDLE
WE
USE
MIN
UTES
BECAUSE
IT
IS
THE
DEFAULT
TIMEOUT
RECOMMENDED
BY
ENERGYSTAR
FOR
PUTTING
MACHINES
TO
SLEEP
AND
BECAUSE
IT
REPRESENTS
A
SIMPLE
AND
FAIRLY
LIBERAL
APPROXIMATION
FOR
THE
NOTION
OF
IDLE
NESS
FOR
WHICH
A
STANDARD
DEFINITION
DOES
NOT
EXIST
WE
MAINTAIN
THIS
DEFINITION
OF
IDLE
NESS
FOR
THE
REMAINDER
OF
THE
PAPER
AT
ANY
POINT
IN
TIME
WE
CLASSIFY
A
MACHINE
AS
BEING
IN
ONE
OF
FOUR
POSSIBLE
STATES
A
ON
AND
ACTIVELY
USED
WE
CALL
THIS
ACTIVE
B
ON
BUT
NOT
USED
IDLE
C
IN
A
SLEEP
STATE
SUCH
AS
OR
AND
D
POWERED
DOWN
OFF
NOTE
THAT
THIS
NOTION
OF
IDLE
REFERS
HERE
TO
THE
USER
AND
NOT
THE
MACHINE
BEING
INACTIVE
IN
FIGURE
WE
PRESENT
THIS
DATA
FOR
OUR
ENTERPRISE
DESK
TOPS
WE
FOCUS
HERE
ON
THE
DESKTOPS
SINCE
THIS
REPRESENTS
THE
POTENTIAL
ENERGY
SAVINGS
AN
ENTERPRISE
COULD
GARNER
BECAUSE
THE
BULK
OF
OUR
TRACES
COME
FROM
MOBILE
USERS
WE
HAVE
A
LIMITED
NUMBER
OF
DESKTOPS
WE
SEE
THAT
THE
FRACTION
OF
TIME
WHEN
THESE
MACHINES
ARE
ACTIVE
IS
QUITE
LOW
FALLING
BELOW
ON
AVERAGE
MOREOVER
THE
AVER
HOME
OFFICE
ALL
USAGE
ENVIRONMENT
SECOND
LONG
BINS
FOR
INTER
PACKET
GAPS
FIGURE
AVERAGE
NUMBER
OF
DIRECTED
AND
BROAD
CAST
MULTICAST
PACKETS
RECEIVED
ON
AVERAGE
BY
A
NETWORK
HOST
AT
HOME
AND
IN
THE
OFFICE
AGE
FRACTION
OF
TIME
WHEN
MACHINES
ARE
IDLE
IS
HIGH
ABOUT
SIMILAR
TO
OTHER
STUDIES
WE
NOTE
THAT
A
SMALL
FRACTION
OF
OUR
DESKTOPS
ONLY
OUT
USE
SLEEP
MODE
AT
ALL
OVERALL
THIS
INDICATES
THAT
THERE
IS
A
TREMENDOUS
OPPORTUNITY
FOR
ENERGY
SAVINGS
ON
ENTERPRISE
DESKTOPS
THE
OPPORTUNITY
ON
OUR
CORPORATE
LAPTOPS
EXISTS
TOO
BUT
IS
MODERATE
BECAUSE
WE
FOUND
THAT
OUR
LAPTOP
USERS
WERE
MORE
LIKELY
TO
EMPLOY
AGGRESSIVE
SLEEPING
CONFIGURATIONS
THAT
COME
PRE
CONFIGURED
ON
LAPTOPS
WHILE
THE
SAMPLE
OF
THE
DESKTOP
MACHINES
IN
OUR
EXPER
IMENTS
IS
SMALL
THE
RESULTS
ARE
CONSISTENT
WITH
EXISTING
STUDIES
WE
THEREFORE
USE
THESE
MEASURED
IDLE
TIMES
TO
EXTRAPOLATE
THE
ENERGY
THAT
COULD
BE
SAVED
BY
SLEEPING
INSTEAD
OF
REMAINING
IDLE
THERE
ARE
ESTIMATED
TO
BE
ABOUT
MILLION
DESKTOP
PCS
IN
THE
US
DATA
SUMMARIZED
IN
ASSUMING
AN
POWER
CONSUMPTION
OF
AN
IDLE
PC
AND
ASSUMING
THESE
MACHINES
ARE
IDLE
FOR
OF
THE
TIME
THIS
AMOUNTS
TO
ROUGHLY
TWH
YEAR
OF
WASTED
ELECTRICITY
OR
BILLION
DOLLARS
AT
US
PER
KWH
IS
LOW
POWER
PROXYING
NEEDED
BEFORE
DEVELOPING
NEW
SOLUTIONS
TO
REDUCING
HOST
IDLE
TIMES
WE
INVESTIGATE
WHETHER
VERY
SIMPLE
APPROACHES
LIKE
WAKING
UP
FOR
EV
ERY
PACKET
CAN
DELIVER
THESE
SAVINGS
WHILE
MAINTAINING
FULL
NETWORK
PRESENCE
IN
THIS
APPROACH
WHICH
WE
DENOTE
WOP
WAKE
ON
PACKET
THE
MACHINE
IS
WOKEN
UP
FOR
EVERY
PACKET
IT
NEEDS
TO
RECEIVE
DIRECTED
OR
BROADCAST
AND
PUT
BACK
TO
SLEEP
AFTER
THE
PACKET
IS
SERVED
THE
PER
FORMANCE
OF
SUCH
AN
APPROACH
DEPENDS
ON
WHETHER
THE
INTER
PACKET
GAP
IPG
IS
SMALLER
OR
COMPARABLE
TO
THE
TIME
IT
TAKES
TO
TRANSITION
IN
AND
OUT
OF
SLEEP
IF
IT
ISN
T
THEN
THERE
IS
NO
GAIN
OVER
SIMPLY
LEAVING
THE
MACHINE
IN
AN
IDLE
STATE
TO
EXAMINE
THE
TRAFFIC
DURING
IDLE
TIMES
WE
USED
BOTH
OUR
DESKTOP
AND
LAPTOP
MACHINES
WE
CONSIDER
BOTH
TYPES
EVEN
THOUGH
WE
RE
PRIMARILY
INTERESTED
IN
DESKTOPS
BE
CAUSE
THIS
GIVES
US
A
SIGNIFICANTLY
LARGER
SET
OF
SAMPLES
WE
SEPARATE
THE
IDLE
TIME
TRAFFIC
INTO
TWO
CATEGORIES
OF
FICE
AND
HOME
IN
FIGURE
WE
PLOT
THE
AVERAGE
NUMBER
OF
PACKETS
SEC
FOR
IDLE
TRAFFIC
BOTH
IN
THE
OFFICE
AND
AT
HOME
IN
THE
OFFICE
ENVIRONMENT
THE
AVERAGE
NUMBER
OF
PACKETS
FIGURE
HISTOGRAM
OF
THE
FRACTION
OF
THE
IDLE
TIME
MADE
UP
OF
INTER
PACKET
GAPS
OF
DIFFERENT
SIZE
PER
SECOND
IS
ROUGHLY
WHILE
AT
HOME
IT
IS
ROUGHLY
THIS
INDICATES
A
FAIRLY
CONSTANT
LEVEL
OF
BACKGROUND
CHAT
TER
ON
THE
NETWORK
INDEPENDENT
OF
THE
USER
ACTIVITY
BE
CAUSE
THIS
NUMBER
IS
AN
AVERAGE
WE
NEED
TO
UNDERSTAND
IF
THESE
PACKETS
OCCUR
IN
BURSTS
OR
NOT
IF
THE
PACKETS
ARE
BURSTY
MOST
OF
THE
TIME
THEN
THERE
MAY
STILL
BE
OPPORTU
NITIES
TO
SLEEP
AS
THE
HOST
CAN
BE
WOKEN
UP
TO
SERVICE
A
BURST
OF
PACKETS
AND
THEN
BE
PUT
TO
SLEEP
FOR
SOME
REASON
ABLE
PERIOD
OF
TIME
CERTAINLY
MORE
THAN
A
FEW
SECONDS
IF
THESE
PACKETS
OCCUR
FAIRLY
EVENLY
SPACED
THEN
IT
IS
NOT
WORTH
GOING
TO
SLEEP
UNLESS
THE
TIME
TO
TRANSITION
IN
AND
OUT
OF
SLEEP
IS
VERY
SMALL
ON
THE
ORDER
OF
TO
SECONDS
TO
QUANTIFY
THE
BURSTINESS
LEVEL
OF
OUR
TRAFFIC
WE
GROUP
INTER
PACKET
GAPS
INTO
SECOND
LONG
BINS
I
E
ETC
WE
THEN
COMPUTE
THE
SUM
OF
THE
INTER
PACKET
GAPS
IN
EACH
OF
THESE
BINS
AND
FINALLY
COMPUTE
THE
FRACTION
OF
TOTAL
IDLE
TIME
REPRESENTED
BY
EACH
BIN
WE
PRESENT
THESE
RESULTS
IN
FIGURE
FOR
BOTH
HOME
AND
OFFICE
ENVI
RONMENTS
IN
THE
OFFICE
OVER
OF
THE
TIME
THE
IPG
IS
LESS
THAN
SECONDS
ALTHOUGH
THE
DISTRIBUTION
IS
MORE
UNIFORMLY
SPREAD
FOR
THE
HOME
ENVIRONMENT
WE
STILL
SEE
THAT
ROUGHLY
OF
THE
TIME
THE
IPG
IS
LESS
THAN
SECONDS
OVERALL
WE
OBSERVE
THAT
A
NEITHER
OF
THE
EN
VIRONMENTS
ENJOYS
MANY
LONG
PERIODS
OF
QUIET
TIME
B
WE
FIND
THIS
DISTRIBUTION
TO
BE
VERY
DIFFERENT
FOR
THE
TWO
ENVIRONMENTS
IN
HOME
NETWORKS
THE
DISTRIBUTION
HAS
A
MUCH
HEAVIER
TAIL
THE
TRAFFIC
IS
BURSTIER
AND
WE
DO
SEE
LONGER
PERIODS
OF
QUIET
TIME
WE
NOW
TRANSLATE
THESE
OBSERVATIONS
INTO
ACTUAL
SLEEP
TIME
IN
ORDER
TO
PERFORM
THIS
COMPUTATION
WE
MUST
CON
SIDER
A
REPRESENTATIVE
VALUE
FOR
THE
TIME
INTERVAL
IT
TAKES
THE
HOST
TO
WAKE
UP
PROCESS
THE
PACKET
AND
THEN
GO
TO
SLEEP
AGAIN
WE
CALL
THIS
THE
TRANSITION
TIME
DENOTED
TS
TODAY
TYPICAL
MACHINES
TAKE
SECONDS
TO
ENTER
SLEEP
AND
SECONDS
TO
FULLY
RESUME
FROM
AS
MEA
SURED
IN
A
RECENT
STUDY
THEREFORE
IT
IS
REASONABLE
TO
ASSUME
AN
AVERAGE
TRANSITION
TIME
TS
OF
WHEN
A
PACKET
ARRIVES
THE
MACHINE
IS
WOKEN
UP
TO
SERVE
THE
PACKET
AFTER
PROCESSING
A
PACKET
THE
MACHINE
ONLY
GOES
TO
SLEEP
AGAIN
IF
IT
KNOWS
THE
NEXT
PACKET
WILL
NOT
ARRIVE
BEFORE
IT
TRANSITIONS
TO
SLEEP
THIS
IDEALIZED
TEST
SORTED
USERS
FIGURE
THE
FRACTION
OF
IDLE
TIME
USERS
CAN
SLEEP
IF
THEY
WAKE
UP
FOR
EVERY
PACKET
ACROSS
DIFFERENT
ENVIRONMENTS
FOR
A
TRANSITION
TIME
TS
THUS
ASSUMES
THAT
THE
HOST
KNOWS
THE
FUTURE
INCOMING
PACKET
STREAM
AND
CAPTURES
THE
BEST
THE
MACHINE
COULD
DO
IN
TERMS
OF
ENERGY
SAVINGS
FIGURE
PRESENTS
THE
FRACTION
OF
IDLE
TIME
FOR
WHICH
USERS
CAN
SLEEP
ASSUMING
THE
POLICY
DESCRIBED
ABOVE
THE
RESULTS
ARE
RATHER
DRAMATICALLY
DIFFERENT
FOR
ACROSS
ENVIRONMENTS
IN
THE
OFFICE
THERE
IS
ALMOST
NO
OPPOR
TUNITY
TO
SLEEP
FOR
THE
MAJORITY
OF
THE
USERS
THIS
INDI
CATES
THAT
THE
MAGIC
PACKET
LIKE
APPROACH
WILL
NOT
SUC
CEED
IN
SAVING
ANY
ENERGY
FOR
MACHINES
IN
A
TYPICAL
COR
PORATE
OFFICE
ENVIRONMENT
FOR
THE
HOME
ENVIRONMENT
WE
SEE
THAT
ROUGHLY
HALF
THE
USERS
CAN
SLEEP
FOR
OVER
OF
THEIR
IDLE
TIMES
THUS
IN
THESE
ENVIRONMENTS
A
TRANSITION
TIME
COUPLED
WITH
A
WOP
TYPE
POLICY
CAN
BE
SOMEWHAT
EFFECTIVE
HOWEVER
THESE
ESTIMATES
ASSUME
PERFECT
KNOWLEDGE
OF
FUTURE
TRAFFIC
ARRIVALS
AND
ALSO
FRE
QUENT
TRANSITIONS
IN
AND
OUT
OF
SLEEP
IN
PRACTICE
WE
EX
PECT
THE
ACHIEVABLE
SAVINGS
WOULD
BE
SOMEWHAT
LOWER
NONETHELESS
THIS
DOES
SUGGEST
THAT
EFFORTS
TO
REDUCE
SYS
TEM
TRANSITION
TIMES
IN
FUTURE
HARDWARE
COULD
OBVIATE
THE
NEED
FOR
MORE
COMPLEX
POWER
SAVING
STRATEGIES
IN
CERTAIN
ENVIRONMENTS
WE
CONCLUDE
THAT
WHILE
SIGNIFICANT
OPPORTUNITY
FOR
SLEEP
EXISTS
CAPITALIZING
ON
THIS
OPPORTUNITY
REQUIRES
SO
LUTIONS
THAT
GO
BEYOND
MERELY
WAKING
THE
HOST
TO
HAN
DLE
NETWORK
TRAFFIC
WE
THUS
CONSIDER
SOLUTIONS
BASED
ON
PROXYING
IDLE
TIME
TRAFFIC
IN
THE
FOLLOWING
SECTIONS
DECONSTRUCTING
TRAFFIC
IN
THE
PREVIOUS
SECTION
WE
SAW
THAT
BY
JUST
WAKING
UP
TO
HANDLE
ALL
PACKETS
OUR
ABILITY
TO
INCREASE
A
MACHINE
SLEEP
TIME
IS
LIMITED
IN
PARTICULAR
WE
SEE
VIRTUALLY
NO
ENERGY
SAVINGS
IN
THE
DOMINANT
OFFICE
ENVIRONMENTS
THIS
SUGGESTS
THAT
WE
NEED
AN
APPROACH
THAT
IS
MORE
DISCRIM
INATING
IN
CHOOSING
WHEN
TO
WAKE
HOSTS
THIS
LEADS
US
TO
AN
ALTERNATE
SOLUTION
TO
THE
WOL
WHICH
IS
TO
EMPLOY
A
NETWORK
PROXY
WHOSE
JOB
IS
TO
HANDLE
IDLE
TIME
TRAFFIC
ON
BEHALF
OF
ONE
OR
MORE
SLEEPING
HOSTS
PACKETS
DESTINED
FOR
A
SLEEPING
HOST
ARE
INTERCEPTED
BY
OR
ROUTED
TO
DE
FIGURE
COMPOSITION
OF
INCOMING
AND
OUTGOING
TRAF
FIC
DURING
IDLE
TIMES
FOR
HOME
AND
OFFICE
ENVIRONMENTS
BASED
ON
COMMUNICATION
PARADIGMS
PENDING
ON
THE
PROXY
DEPLOYMENT
MODEL
ITS
PROXY
AT
THIS
POINT
THE
PROXY
MUST
KNOW
WHAT
TO
DO
WITH
THIS
IN
TERCEPTED
TRAFFIC
BROADLY
THE
PROXY
MUST
CHOOSE
BETWEEN
THREE
REACTIONS
A
IGNORE
DROP
THE
PACKET
B
RESPOND
TO
THE
PACKET
ON
BEHALF
OF
THE
MACHINE
OR
C
WAKE
UP
THE
MACHINE
TO
SERVICE
IT
TO
MAKE
A
JUDICIOUS
CHOICE
THE
PROXY
MUST
HAVE
SOME
KNOWLEDGE
OF
NETWORK
TRAFFIC
WHAT
TRAFFIC
IS
SAFELY
IGNORABLE
WHAT
APPLICATIONS
DO
PACKETS
BELONG
TO
WHICH
APPLICATIONS
ARE
ESSENTIAL
AND
SO
FORTH
IN
THIS
SECTION
WE
DO
A
TOP
DOWN
DECONSTRUC
TION
OF
THE
IDLE
TIME
TRAFFIC
TRACES
AIMED
AT
LEARNING
THE
ANSWERS
TO
THESE
QUESTIONS
TRAFFIC
CLASSES
BY
COMMUNICATION
PARADIGM
TO
BEGIN
WE
LOOK
AT
ALL
PACKETS
EXCHANGED
DURING
IDLE
PERIODS
AND
CLASSIFY
EACH
PACKET
AS
EITHER
BEING
A
BROAD
CAST
MULTICAST
OR
UNICAST
PACKET
WITHIN
THESE
BROAD
TRAF
FIC
CLASSES
WE
FURTHER
PARTITION
THE
TRAFFIC
BY
WHETHER
THE
PACKETS
ARE
INCOMING
OR
OUTGOING
FOR
BOTH
THE
HOME
AND
OFFICE
ENVIRONMENTS
WE
SEPARATE
INCOMING
AND
OUTGOING
TRAFFIC
BECAUSE
WE
EXPECT
THEM
TO
LOOK
DIFFERENT
IN
TERMS
OF
THE
PROPORTION
OF
EACH
CLASS
IN
DIFFERENT
DIRECTIONS
E
G
MOST
END
HOSTS
OUGHT
TO
SEND
LITTLE
BROADCAST
TRAF
FIC
SIMILARLY
WE
LOOK
AT
DIFFERENT
USAGE
ENVIRONMENTS
BECAUSE
IT
IS
INTUITIVE
THAT
THE
DOMINANT
PROTOCOLS
AND
AP
PLICATIONS
USED
IN
EACH
ENVIRONMENT
MAY
DIFFER
SINCE
WE
EXPECT
THESE
DIFFERENCES
WE
TREAT
THEM
AS
SUCH
TO
AVOID
MISCHARACTERIZATIONS
THE
BREAKDOWN
OF
OUR
TRAFFIC
AC
CORDING
TO
ALL
THESE
PARTITIONS
IN
DEPICTED
IN
FIG
WE
NOTE
THAT
OUTGOING
TRAFFIC
IS
DOMINATED
BY
UNICAST
TRAFFIC
SINCE
AS
EXPECTED
EACH
HOST
GENERATES
LITTLE
BROAD
CAST
OR
MULTICAST
TRAFFIC
WE
ALSO
FIND
THAT
INCOMING
TRAFFIC
AT
A
HOST
SEES
SIGNIFICANT
PROPORTIONS
OF
ALL
THREE
CLASSES
OF
TRAFFIC
AND
THIS
IS
TRUE
IN
BOTH
ENTERPRISE
AND
HOME
ENVIRONMENTS
THIS
SUGGESTS
THAT
A
POWER
SAVING
PROXY
MIGHT
HAVE
TO
TACKLE
ALL
THREE
TRAFFIC
CLASSES
TO
SEE
SIGNIF
ICANT
SAVINGS
SO
FAR
WE
LOOKED
AT
TRAFFIC
VOLUMES
AS
INDICATIVE
OF
THE
NEED
TO
PROXY
THE
CORRESPONDING
TRAFFIC
TYPE
WE
NOW
DI
RECTLY
EVALUATE
THE
OPPORTUNITY
FOR
SLEEP
REPRESENTED
BY
EACH
TRAFFIC
TYPE
TO
UNDERSTAND
THE
MAXIMUM
SLEEPING
OPPORTUNITIES
WE
CONSIDER
FOR
A
MOMENT
AN
IDEALIZED
SCENARIO
IN
WHICH
WE
USE
OUR
PROXY
TO
IGNORE
ALL
INCOM
ING
PACKETS
FROM
EITHER
OR
BOTH
OF
THE
BROADCAST
AND
MUL
TICAST
TRAFFIC
CLASSES
A
MACHINE
ALWAYS
WAKES
UP
FOR
UNI
CAST
PACKETS
FIG
SHOWS
THE
SLEEP
POTENTIAL
IN
FOUR
SCE
NARIOS
A
IGNORE
ONLY
BROADCAST
AND
WAKE
FOR
THE
REST
C
IGNORE
ONLY
MULTICAST
AND
WAKE
FOR
THE
REST
C
IGNORE
BOTH
BROADCAST
AND
MULTICAST
FOR
COMPARISON
PURPOSES
HOME
OFFICE
WE
ALSO
INCLUDE
THE
RESULTS
FOR
A
SCENARIO
D
IN
WHICH
WE
WAKE
UP
FOR
ALL
PACKETS
THIS
COMPARISON
ALLOWS
US
TO
COMPARE
THE
BENEFITS
DERIVED
FROM
THESE
FOUR
DIFFERENT
PROXY
POLICIES
FOR
EACH
USER
WE
COMPUTED
THE
FRACTION
OF
ITS
IDLE
TIME
THAT
COULD
HAVE
BEEN
SPENT
SLEEPING
UN
DER
THE
SCENARIO
IN
QUESTION
WE
USE
A
TRANSITION
TIME
OF
TS
AND
THE
RESULTS
ARE
AVERAGED
OVER
USERS
FOR
BOTH
HOME
AND
OFFICE
ENVIRONMENTS
WE
MAKE
THE
FOLLOWING
OBSERVATIONS
I
BROADCAST
AND
MULTICAST
ARE
LARGELY
RESPONSIBLE
FOR
POOR
SLEEP
IF
WE
CAN
PROXY
THESE
THEN
WE
CAN
RECUPER
ATE
OVER
OF
THE
IDLE
TIME
IN
HOME
ENVIRONMENTS
AND
IN
THE
OFFICE
WHERE
PREVIOUSLY
SLEEP
WAS
BARELY
POSSIBLE
WE
CAN
NOW
SLEEP
FOR
OVER
OF
THE
IDLE
TIME
II
DOING
AWAY
WITH
ONLY
ONE
OF
EITHER
BROADCAST
OR
MULTICAST
IS
NOT
VERY
EFFECTIVE
WE
SUSPECT
THIS
IS
DUE
TO
THE
PERIODICITY
OF
MULTICAST
AND
BROADCAST
PROTOCOLS
AND
EVALUATE
THIS
IN
LATER
SECTIONS
MORE
GENERALLY
THE
GRAPH
CLEARLY
INDICATES
A
VALUABLE
CONCLUSION
IF
WE
RE
LOOKING
TO
NARROW
THE
SET
OF
TRAF
FIC
CLASSES
TO
PROXY
THEN
MULTICAST
AND
BROADCAST
TRAF
FIC
APPEAR
TO
BE
CLEAR
LOW
HANGING
FRUIT
AND
SHOULD
BE
OUR
PRIMARY
CANDIDATES
FOR
PROXYING
THAT
SAID
PROXYING
UNICAST
TRAFFIC
APPEARS
KEY
TO
ACHIEVING
HIGHER
SAVINGS
BEYOND
IN
THE
ENTERPRISE
AND
HENCE
SHOULD
NOT
BE
DISMISSED
EITHER
WE
THUS
CONTINUE
FOR
NOW
TO
STUDY
ALL
THREE
TRAFFIC
TYPES
OF
COURSE
WHETHER
THESE
POTENTIAL
SAVINGS
CAN
ACTUALLY
BE
REALIZED
DEPENDS
ON
WHETHER
A
PARTICULAR
TRAFFIC
TYPE
CAN
INDEED
BE
HANDLED
BY
A
PROXY
WITHOUT
WAKING
THE
HOST
THIS
DEPENDS
ON
THE
SPECIFIC
PROTOCOLS
AND
APPLICA
TIONS
WITHIN
THAT
CLASS
AND
HENCE
IN
THE
REMAINDER
OF
THIS
SECTION
WE
PROCEED
IN
TURN
TO
DECONSTRUCT
EACH
OF
BROAD
CAST
MULTICAST
AND
UNICAST
TRAFFIC
DECONSTRUCTING
BROADCAST
OUR
GOAL
IN
THIS
SECTION
IS
TO
EVALUATE
INDIVIDUAL
BROAD
CAST
PROTOCOLS
LOOKING
FOR
WHICH
OF
THESE
PROTOCOLS
ARE
THE
MAIN
OFFENDERS
IN
TERMS
OF
PREVENTING
HOSTS
FROM
SLEEPING
AND
WHAT
PURPOSE
DO
THESE
PROTOCOLS
SERVE
AND
HOW
MIGHT
A
PROXY
HANDLE
THEM
ANSWERING
THE
FIRST
QUESTION
REQUIRES
A
MEASURE
OF
PROTOCOL
BADNESS
WITH
RESPECT
TO
PREVENTING
HOSTS
FROM
SLEEPING
WE
USE
TWO
METRICS
FOR
OUR
EVALUATION
THE
FIRST
IS
SIMPLY
THE
TOTAL
VOLUME
OF
TRAFFIC
DUE
TO
THE
PROTOCOL
IN
QUESTION
WHILE
HIGH
VOLUME
TRAFFIC
OFTEN
MAKES
SLEEP
HARDER
THIS
IS
AN
FIGURE
AVERAGE
SLEEP
OPPORTUNITY
WHEN
IGNORING
MUL
TICAST
AND
OR
BROADCAST
TRAFFIC
FOR
DIFFERENT
ENVIRONMENTS
IMPERFECT
METRIC
SINCE
THE
IN
ABILITY
TO
SLEEP
DEPENDS
AS
MUCH
ON
THE
PRECISE
TEMPORAL
PACKET
ARRIVAL
PATTERN
DUE
TO
THE
PROTOCOL
AS
ON
PACKET
VOLUMES
NONETHELESS
WE
RE
TAIN
TRAFFIC
VOLUME
AS
AN
INTUITIVE
ALTHOUGH
INDIRECT
MEA
SURE
OF
PROTOCOL
BADNESS
OUR
SECOND
METRIC
WHICH
WE
TERM
THE
HALF
SLEEP
TIME
DENOTED
TS
MORE
DIRECTLY
MEASURES
A
PROTOCOL
ROLE
IN
PREVENTING
SLEEP
WE
DEFINE
THE
HALF
SLEEP
TIME
FOR
A
PROTOCOL
OR
TRAFFIC
TYPE
P
AS
THE
LARGEST
HOST
TRANSITION
TIME
THAT
WOULD
BE
REQUIRED
TO
ALLOW
THE
HOST
TO
SLEEP
FOR
AT
LEAST
OF
ITS
IDLE
TIME
UNDER
THE
SCENARIO
WHERE
THE
MACHINE
WAKES
UP
FOR
ALL
PACKETS
OF
TYPE
P
AND
IGNORES
ALL
OTHER
TRAFFIC
IN
EFFECT
TS
QUANTIFIES
THE
INTUITION
THAT
IF
WE
IGNORE
ALL
TRAFFIC
OTHER
THAN
THAT
DUE
TO
THE
PROTOCOL
OF
INTEREST
THEN
A
PROTOCOL
WHOSE
PACKETS
ARRIVE
SPACED
FAR
ENOUGH
APART
IN
TIME
IS
MORE
CONDUCIVE
TO
SLEEP
SINCE
THE
HOST
HAS
SUFFICIENT
TIME
TO
TRANSITION
IN
AND
OUT
OF
SLEEP
IN
MORE
DETAIL
TS
IS
COMPUTED
FROM
OUR
TRACES
AS
FOLLOWS
WE
MEASURE
THE
TOTAL
TIME
A
GIVEN
HOST
CAN
SLEEP
ASSUMING
IT
WAKES
UP
FOR
ALL
THE
PACKETS
OF
THE
PROTOCOL
UNDER
CONSIDERATION
AND
IGNORES
ALL
OTHERS
WE
COMPUTE
THIS
NUMBER
FOR
ALL
HOSTS
AND
TAKE
THE
AVERAGE
THIS
GIVES
US
AN
UPPER
BOUND
ON
ACHIEVABLE
SLEEP
IF
THE
PROTOCOL
IS
HANDLED
BY
WAKING
THE
HOST
WE
ESTIMATE
THIS
SLEEP
DURATION
FOR
DIFFERENT
VALUES
OF
THE
HOST
TRANSITION
TIME
TS
RANGING
FROM
SECONDS
IDEAL
TO
MINUTES
THE
LARGEST
OF
THESE
TRANSITION
TIMES
TS
THAT
ALLOWS
THE
HOST
TO
SLEEP
FOR
OVER
OF
ITS
IDLE
TIME
IS
THE
PROTOCOL
TS
INTUITIVELY
TS
INDICATES
THE
EXTENT
TO
WHICH
A
PRO
TOCOL
IS
SLEEP
FRIENDLY
SINCE
PROTOCOLS
WITH
LARGE
VAL
UES
OF
TS
COULD
SIMPLY
BE
HANDLED
BY
ALLOWING
THE
MACHINE
TO
WAKE
UP
WHEREAS
THOSE
WITH
LOW
VALUES
OF
TS
IMPLY
THAT
TO
ACHIEVE
USEFUL
SLEEP
THE
PROXY
MUST
HANDLE
SUCH
TRAFFIC
WITHOUT
WAKING
THE
HOST
FOR
OUR
EVALUATION
WE
CLASSIFY
EACH
PACKET
BY
PROTOCOL
AND
RANK
THEM
BY
BOTH
METRICS
TRAFFIC
VOLUME
AND
THE
HALF
SLEEP
TIME
WE
BEGIN
BY
MEASURING
TRAFFIC
VOLUME
WE
THEN
ESTABLISH
THE
TOP
RANKING
PROTOCOLS
BY
VOLUME
AND
USE
THESE
AS
CANDIDATES
FOR
OUR
SECOND
METRIC
THE
HALF
SLEEP
TIME
WHEN
PRESENTING
THE
TOP
RANKING
PROTO
COLS
BY
EACH
OF
THE
METRICS
WE
CONSIDER
THE
PROTO
COLS
WHOSE
TRAFFIC
VOLUMES
REPRESENTS
MORE
THAN
OF
OFFICE
HOME
PROTOCOL
OF
TRAFFIC
PROTOCOL
OF
TRAFFIC
HSRP
SSDP
PIM
IGMP
EIGRP
OTHER
SSDP
HSRP
IGMP
OTHER
TOTAL
TOTAL
FIGURE
PROTOCOL
COMPOSITION
OF
INCOMING
BROADCAST
TRAFFIC
IN
BOTH
OFFICE
AND
HOME
ENVIRONMENTS
RANKED
BY
PER
PROTOCOL
TRAFFIC
VOLUMES
OFFICE
HOME
ALL
BCAST
ALL
BCAST
ARP
ARP
NBDGM
NBDGM
NBNS
NBNS
IPX
FIGURE
PROTOCOL
COMPOSITION
FOR
BROADCAST
PROTOCOLS
RANKED
BY
TS
THE
TOTAL
TRAFFIC
AT
THE
HOST
AND
THE
PROTOCOLS
WITH
A
HALF
SLEEP
TIME
OF
LESS
THAN
MINUTES
TABLE
AND
PRESENT
OUR
RESULTS
FOR
BROADCAST
TRAFFIC
FOR
COMPLETE
NESS
WE
ALSO
PRESENT
THE
VALUE
OF
TS
WHEN
CONSIDER
ING
ALL
BROADCAST
TRAFFIC
TOGETHER
IN
TERMS
OF
TRAFFIC
VOLUMES
WE
SEE
THAT
THE
BULK
OF
BROADCAST
TRAFFIC
IS
IN
THE
CAUSE
OF
ADDRESS
RESOLUTION
AND
VARIOUS
SERVICE
DISCOVERY
PROTOCOLS
E
G
ARP
NETBIOS
NAME
SERVICE
NBNS
THE
SIMPLE
SERVICE
DISCOVERY
PROTOCOL
USED
BY
UPNP
DEVICES
SSDP
THESE
PROTO
COLS
ARE
WELL
REPRESENTED
IN
BOTH
HOME
AND
OFFICE
LANS
A
SECOND
WELL
REPRESENTED
CATEGORY
OF
TRAFFIC
IS
FROM
ROUTER
SPECIFIC
PROTOCOLS
E
G
ROUTING
PROTOCOLS
IMPLE
MENTED
ON
TOP
OF
THE
IPX
IN
TERMS
OF
THE
HALF
SLEEP
TIME
WE
SEE
THAT
BROADCAST
AS
A
WHOLE
ALLOWS
VERY
LITTLE
SLEEP
IN
THE
OFFICE
ACHIEV
ING
SLEEP
WOULD
REQUIRE
VERY
FAST
TRANSITIONS
BE
TWEEN
AND
SECONDS
NOT
FEASIBLE
WITH
TODAY
HARD
WARE
SUPPORT
THE
SITUATION
IN
HOME
LANS
IS
SIGNIFI
CANTLY
BETTER
TS
IN
TERMS
OF
PROTOCOLS
WE
SEE
THAT
THE
GREATEST
OFFENDERS
ARE
SIMILAR
TO
THOSE
FROM
OUR
TRAFFIC
VOLUME
ANALYSIS
NAMELY
ARP
NETBIOS
DATA
GRAMS
NBDGM
AND
NAME
QUERIES
NBNS
AND
IPX
ON
CLOSER
EXAMINATION
WE
FIND
THAT
MOST
OF
THESE
OF
FENDING
PROTOCOLS
COULD
BE
EASILY
HANDLED
BY
A
PROXY
FOR
EXAMPLE
IPX
IS
SAFELY
IGNORABLE
ARP
TRAFFIC
THAT
IS
NOT
DESTINED
TO
THE
MACHINE
IN
QUESTION
IS
LIKEWISE
SAFELY
IGNORABLE
FOR
ARP
QUERIES
DESTINED
TO
THE
MACHINE
IT
WOULD
BE
FAIRLY
STRAIGHTFORWARD
FOR
A
PROXY
TO
AUTOMATI
CALLY
CONSTRUCT
AND
GENERATE
THE
REQUISITE
RESPONSE
WITH
OUT
HAVING
TO
WAKE
THE
HOST
FIGURE
PROTOCOL
COMPOSITION
FOR
INCOMING
MULTICAST
TRAFFIC
IN
BOTH
OFFICE
AND
HOME
ENVIROMENTS
RANKED
BY
PER
PROTOCOL
TRAFFIC
VOLUMES
OFFICE
HOME
ALL
MCAST
ALL
MCAST
HSRP
SSDP
PIM
HSRP
IGMP
IGMP
SSDP
FIGURE
PROTOCOL
COMPOSITION
FOR
INCOMING
MULTICAST
TRAFFIC
IN
BOTH
OFFICE
AND
HOME
ENVIRONMENTS
RANKED
BY
TS
DECONSTRUCTING
MULTICAST
TABLE
AND
PRESENT
OUR
PROTOCOL
RANKINGS
FOR
MULTICAST
TRAFFIC
AGAIN
WE
ALSO
PRESENT
THE
VALUE
OF
TS
WHEN
CONSIDERING
ALL
MULTICAST
TRAFFIC
TAKEN
TO
GETHER
WE
SEE
THAT
MULTICAST
TRAFFIC
AS
A
WHOLE
CAN
BE
A
BAD
OFFENDER
IN
ENTERPRISE
ENVIRONMENTS
WITH
AN
TS
IT
TURNS
OUT
THAT
THIS
IS
LARGELY
CAUSED
BY
ROUTER
TRAFFIC
THE
HOT
STANDBY
ROUTER
PROTOCOL
HSRP
PROTOCOL
INDEPENDENT
MULTICAST
PIM
EIGRP
ETC
THIS
TRAFFIC
IS
EITHER
ABSENT
E
G
PIM
OR
GREATLY
RE
DUCED
E
G
HSRP
IN
HOME
ENVIRONMENTS
WHICH
EX
PLAINS
WHY
MULTICAST
IS
MUCH
LESS
PROBLEMATIC
IN
HOMES
WITH
AN
TS
MINUTES
COMPARED
TO
FOR
BROADCAST
THE
GOOD
NEWS
IS
THAT
ALL
ROUTER
TRAFFIC
HSRP
PIM
IGRP
IS
SAFELY
IGNORABLE
IN
FACT
MANY
MODERN
ETHER
NET
CARDS
ALREADY
INCLUDE
A
HARDWARE
MULTICAST
FILTER
THAT
DISCARDS
MOST
UNWANTED
MULTICAST
TRAFFIC
AS
WITH
BROADCAST
TRAFFIC
WE
ALSO
SEE
SIGNIFICANT
TRAFFIC
CONTRIBUTED
BY
SERVICE
DISCOVERY
PROTOCOLS
IN
THIS
CASE
SSDP
THE
SIMPLE
SERVICE
DISCOVERY
PROTOCOL
USED
BY
UPNP
DEVICES
ONCE
AGAIN
FOR
PROTOCOLS
SUCH
AS
SSDP
AND
IGMP
IT
IS
FAIRLY
STRAIGHTFORWARD
FOR
A
PROXY
TO
AUTO
MATICALLY
RESPOND
TO
INCOMING
TRAFFIC
WITHOUT
WAKING
THE
HOST
DOING
SO
WOULD
REQUIRE
SOME
AMOUNT
OF
STATE
AT
THE
PROXY
SUCH
AS
THE
LIST
OF
MULTICAST
GROUPS
THE
INTERFACE
BELONGS
TO
AND
THE
SERVICES
RUNNING
ON
THE
MACHINE
DECONSTRUCTING
UNICAST
FINALLY
WE
PRESENT
OUR
PROTOCOL
RANKING
FOR
UNICAST
TRAF
FIC
IN
TABLES
AND
BECAUSE
MUCH
OF
UNICAST
TRAF
FIC
IS
EITHER
TCP
OR
UDP
AND
THIS
LEVEL
OF
CLASSIFICA
TION
IS
UNLIKELY
TO
BE
INFORMATIVE
WE
FURTHER
BREAK
EACH
FIGURE
FRACTION
OF
PACKETS
GENERATED
BY
INCOMING
VS
OUTGOING
CONNECTIONS
FOR
HOME
AND
OFFICE
BOTH
RECEIVED
AND
TRANSMITTED
PACKETS
FIGURE
PROTOCOL
COMPOSITION
OF
INCOMING
UNICAST
TRAFFIC
IN
OFFICE
ENVIROMENTS
RANKED
BY
PER
PROTOCOL
TRAF
FIC
VOLUMES
OFFICE
HOME
ALL
UCAST
ALL
UCAST
TCP
UDP
UDP
DNS
DCE
RPC
TCP
DNS
SMB
NBNS
HTTP
FIGURE
PROTOCOL
COMPOSITION
OF
INCOMING
UNICAST
TRAFFIC
IN
OFFICE
ENVIRONMENTS
RANKED
BY
TS
PORT
APP
TCP
KEEP
ALIVES
UDP
TCP
TCP
TCP
TCP
TCP
UDP
TCP
SYNS
MANY
DNS
DCE
RPC
SMB
CIFS
BIGFIX
DNS
HTTP
BIGFIX
MANY
15MIN
FIGURE
PROTOCOL
COMPOSITION
FOR
UNICAST
TRAFFIC
BASED
ON
TCP
AND
UDP
PORTS
RAN
KED
BY
TS
DOWN
BY
SESSION
LAYER
PROTOCOL
WITH
AN
ADDITIONAL
MAP
PING
FROM
PORTS
IN
TABLE
UNFORTUNATELY
UNLIKE
THE
CASE
OF
BROADCAST
AND
MULTICAST
WITH
UNICAST
IT
IS
HARDER
TO
DEDUCE
THE
ULTIMATE
PURPOSE
FOR
MUCH
OF
THIS
TRAFFIC
SINCE
EVEN
THE
SESSION
OR
APPLICATION
LEVEL
PROTOCOL
IDEN
TIFIERS
ARE
FAIRLY
GENERIC
ONE
EXCEPTION
IS
THE
BIGFIX
APPLICATION
LISTED
IN
FIG
BIGFIX
IS
AN
ENTERPRISE
SOFT
WARE
PATCHING
SERVICE
THAT
CHECKS
SECURITY
COMPLIANCE
OF
ENTERPRISE
MACHINES
BASED
ON
THE
FREQUENCY
AND
VOLUME
OF
BIGFIX
TRAFFIC
WE
SEE
IT
APPEARS
TO
HAVE
BEEN
CONFIG
URED
BY
AN
OVER
ZEALOUS
SYSTEM
ADMINISTRATOR
STYMIED
IN
OUR
ATTEMPTS
TO
DECONSTRUCT
UNICAST
TRAFFIC
BASED
ON
WHETHER
AND
HOW
IT
MIGHT
BE
PROXIED
WE
TRY
AN
ALTERNATE
STRATEGY
WE
CLASSIFY
TCP
AND
UDP
PACK
ETS
BASED
ON
THE
CONNECTIONS
THEY
BELONG
TO
AND
CATEGO
RIZE
CONNECTIONS
AS
INCOMING
VS
OUTGOING
OUR
INTEREST
IN
THIS
CLASSIFICATION
IS
BECAUSE
WE
SUSPECT
THAT
A
LARGE
PORTION
OF
PACKETS
ARE
LIKELY
TO
BELONG
TO
OUTGOING
CON
NECTIONS
AND
WHILE
A
HOST
MIGHT
WAKE
FOR
INCOMING
CON
NECTIONS
WAKING
FOR
OUTGOING
CONNECTIONS
MIGHT
WELL
BE
AVOIDABLE
FOR
REASONS
DISCUSSED
BELOW
FROM
THE
RESULTS
IN
FIG
WE
SEE
THAT
OUTGOING
CONNECTIONS
DO
INDEED
DOMINATE
NOW
FOR
A
SLEEPING
MACHINE
THERE
ARE
THREE
POSSIBILITIES
FOR
THESE
OUTGOING
CONNECTIONS
THE
CON
NECTION
WAS
INITIATED
BY
THE
HOST
BEFORE
THE
IDLE
PERIOD
IN
THIS
CASE
SUCH
TRAFFIC
MIGHT
NOT
BE
IGNORABLE
IF
THE
HOST
PROXY
WANTS
TO
MAINTAIN
THIS
CONNECTION
HENCE
WE
HOPE
THIS
PERCENTAGE
OF
TRAFFIC
IS
SMALL
THE
CONNEC
TION
WAS
INITIATED
BUT
FAILED
THE
CONNECTION
WAS
INI
TIATED
BY
THE
HOST
AFTER
THE
START
OF
THE
IDLE
PERIOD
FOR
A
SLEEPING
HOST
THESE
CONNECTIONS
WOULD
EITHER
SIMPLY
NEVER
HAVE
BEEN
INITIATED
IF
THE
CONNECTION
WERE
DEEMED
UNNCESSARY
OR
THE
HOST
WOULD
BE
DELIBERATELY
WOKEN
TO
INITIATE
THESE
CONNECTIONS
IF
THE
CONNECTION
WERE
DEEMED
NECESSARY
AS
FOR
SERVICES
SCHEDULED
TO
RUN
DURING
IDLE
TIMES
FOR
THE
FORMER
THE
TRAFFIC
CAN
SIMPLY
BE
IGNORED
FROM
OUR
ACCOUNTING
AND
IN
THE
LATTER
CASE
SUCH
SCHED
ULED
PROCESSING
IS
EASILY
BATCHED
AND
HENCE
NEEDN
T
DIS
RUPT
SLEEP
HENCE
FOR
ALL
BUT
THE
FIRST
CASE
WAKING
THE
MACHINE
MIGHT
BE
AVOIDABLE
WE
PLOT
THIS
BREAKDOWN
OF
OUTGOING
CONNECTIONS
IN
FIGURE
WE
SEE
THAT
ONLY
A
RELATIVELY
SMALL
PERCENTAGE
OF
OUTGOING
CONNECTIONS
AL
WAYS
LESS
THAN
BELONG
TO
THE
FIRST
CATEGORY
WHICH
MIGHT
REQUIRE
WAKING
THE
HOST
BASED
ON
THIS
WE
SPECU
LATE
THAT
IT
MIGHT
BE
POSSIBLE
TO
ELIMINATE
OR
IGNORE
MUCH
OF
EVEN
UNICAST
TRAFFIC
EARLY
IN
THIS
SECTION
WE
ASKED
WHETHER
ONE
MIGHT
IDEN
TIFY
A
SMALL
SET
OF
OF
PROTOCOLS
OR
PROXY
BEHAVIORS
THAT
COULD
YIELD
SIGNIFICANT
SAVINGS
WE
FIND
THAT
THE
ANSWER
IS
POSITIVE
IN
THE
CASE
OF
MULTICAST
AND
BROADCAST
BUT
LESS
CLEAR
FOR
UNICAST
TRAFFIC
IN
THE
NEXT
SECTION
WE
CONSIDER
THE
IMPLICATIONS
OF
OUR
TRAFFIC
ANALYSIS
FOR
PROXY
DESIGN
DON
T
WAKE
DON
T
IGNORE
HSRP
ARP
PIM
NBDGM
ICMP
IGMP
SSDP
ARP
FOR
ME
NBNS
DHCP
FOR
ME
HOME
OFFICE
ALL
TABLE
PROTOCOLS
THAT
SHOULDN
T
CAUSE
A
WAKE
UP
TOO
EXPEN
SIVE
IN
TERMS
OF
SLEEP
AND
PROTOCOLS
THAT
SHOULD
NOT
BE
IGNORED
FOR
CORRECTNESS
IGNORABLE
HSRP
PIM
ARP
FOR
OTHERS
IPX
LLC
EIGRP
DHCP
FIGURE
FOR
OUTGOING
CONNECTIONS
THE
FRACTION
OF
INCOMING
PACKETS
THAT
BELONG
TO
NEW
CONNECTIONS
AND
FAILED
CONNECTION
ATTEMPTS
A
MEASUREMENT
DRIVEN
APPROACH
TO
PROXY
DESIGN
HAVING
STUDIED
THE
NATURE
OF
IDLE
TIME
TRAFFIC
WE
NOW
AP
PLY
OUR
FINDINGS
TO
THE
DESIGN
OF
A
PRACTICAL
POWER
SAVING
PROXY
WE
START
IN
SECTION
BY
EXTRACTING
THE
HIGH
LEVEL
DESIGN
IMPLICATIONS
OF
OUR
TRAFFIC
ANALYSIS
FROM
THE
PREVI
OUS
SECTION
BUILDING
ON
THIS
IN
SECTION
WE
ILLUSTRATE
MECHANICAL
RESPONSE
PROTOCOL
STATE
ARP
IP
ADDRESS
NBNS
NB
NAMES
OF
MACHINE
AND
LOCAL
SERVICES
SSDP
NAMES
OF
LOCAL
PLUG
N
PLAY
SERVICES
IGMP
MULTICAST
GROUPS
THE
INTER
FACE
BELONGS
TO
ICMP
IP
ADDRESS
NBDGM
NB
NAMES
OF
MACHINE
AND
LOCAL
SERVICES
IGNORES
PKTS
NOT
DESTINED
TO
HOST
WAKES
HOST
FOR
REST
THE
SPACE
OF
DESIGN
TRADEOFFS
BY
CONSIDERING
FOUR
SPECIFIC
EXAMPLES
OF
PROXIES
IN
SECTION
WE
DISTILL
OUR
FIND
INGS
INTO
A
PROPOSAL
FOR
A
CORE
PROXY
ARCHITECTURE
THAT
OF
FERS
A
SINGLE
FRAMEWORK
CAPABLE
OF
SUPPORTING
THE
BROAD
DESIGN
SPACE
WE
IDENTIFY
DESIGN
IMPLICATIONS
AT
MINIMUM
A
POWER
SAVING
PROXY
SHOULD
A
ALLOW
THE
HOST
TO
SLEEP
FOR
A
SIGNIFICANT
FRACTION
OF
THE
TIME
AND
B
MAINTAIN
THE
BASIC
NETWORK
PRESENCE
OF
THE
HOST
BY
ENSURING
REMOTE
ENTITIES
CAN
STILL
ADDRESS
AND
REACH
THE
MACHINE
AND
THE
SERVICES
IT
SUPPORTS
BEYOND
THIS
WE
HAVE
A
SIGNIFICANT
MARGIN
OF
FREEDOM
IN
CHOOSING
HOW
A
PROXY
MIGHT
HANDLE
THE
REMAINING
IDLE
TIME
TRAFFIC
AND
APPLICATIONS
VIEWED
THROUGH
THIS
LENS
OUR
RESULTS
FROM
SECTION
LEAD
US
TO
DIFFERENTIATE
IDLE
TIME
TRAFFIC
ALONG
TWO
DIFFERENT
DIMENSIONS
THE
FIRST
CLASSIFIES
TRAFFIC
BASED
ON
THE
NEED
TO
PROXY
THE
TRAFFIC
IN
QUESTION
DON
T
WAKE
PROTOCOLS
THESE
ARE
PROTOCOLS
THAT
GEN
ERATE
SUSTAINED
AND
PERIODIC
TRAFFIC
AND
HENCE
IDEALLY
WOULD
BE
DEALT
WITH
BY
A
PROXY
WITHOUT
WAKING
THE
HOST
SINCE
OTHERWISE
THE
HOST
WOULD
ENJOY
LITTLE
SLEEP
EXAM
PLES
OF
SUCH
PROTOCOLS
IDENTIFIED
IN
THE
PREVIOUS
SECTION
INCLUDE
IGMP
PIM
ARP
TABLE
LISTS
A
SET
OF
PROTOCOLS
WE
CLASSIFY
AS
DON
T
WAKE
DON
T
IGNORE
PROTOCOLS
THESE
ARE
PROTOCOLS
THAT
RE
QUIRE
ATTENTION
TO
ENSURE
THE
CORRECT
OPERATION
OF
HIGHER
LAYER
PROTOCOLS
AND
APPLICATIONS
FOR
EXAMPLE
WE
MUST
ENSURE
THE
DHCP
LEASE
ON
AN
IP
ADDRESS
MUST
BE
MAIN
TAINED
AND
THAT
A
MACHINE
MUST
RESPOND
TO
NETBIOS
NAME
QUERIES
TO
ENSURE
THE
SERVICES
IT
RUNS
OVER
NETBIOS
REMAIN
ADDRESSABLE
THE
PROTOCOLS
WE
IDENTIFIED
AS
DON
T
IGNORE
ARE
LISTED
IN
TABLE
NOTE
THAT
THE
LIST
OF
DON
T
WAKE
AND
DON
T
IGNORE
PROTOCOLS
NEED
NOT
BE
MUTUALLY
TABLE
PROTOCOLS
THAT
CAN
BE
HANDLED
BY
IGNORING
OR
BY
ME
CHANICAL
RESPONSE
WE
CLASSIFY
DHCP
AS
IGNORABLE
BECAUSE
WE
CHOOSE
TO
SCHEDULE
THE
MACHINE
TO
WAKE
UP
AND
ISSUE
DHCP
REQUESTS
TO
RENEW
THE
IP
LEASE
AN
INFREQUENT
EVENT
EXCLUSIVE
FOR
EXAMPLE
ARP
TRAFFIC
IS
BOTH
FREQUENT
AND
CRITICAL
AND
HENCE
FALLS
UNDER
BOTH
CATEGORIES
POLICY
DEPENDENT
TRAFFIC
FOR
THE
REMAINDER
OF
TRAF
FIC
THE
CHOICE
OF
WHETHER
AND
HOW
A
PROXY
SHOULD
HANDLE
THE
TRAFFIC
IS
A
MATTER
OF
THE
TRADEOFF
THE
USER
OR
SOFT
WARE
DESIGNER
IS
SEEKING
TO
ACHIEVE
BETWEEN
THE
SOPHIS
TICATION
OF
IDLE
TIME
FUNCTIONALITY
THE
COMPLEXITY
OF
THE
PROXY
IMPLEMENTATION
AND
ENERGY
SAVINGS
WE
SHALL
EX
PLORE
THESE
TRADEOFFS
IN
THE
CONTEXT
OF
CONCRETE
PROXY
IM
PLEMENTATIONS
IN
SECTION
A
COMPLEMENTARY
DIMENSION
ALONG
WHICH
WE
CAN
CLAS
SIFY
TRAFFIC
IS
BASED
ON
THE
COMPLEXITY
REQUIRED
TO
PROXY
THE
TRAFFIC
IN
QUESTION
A
IGNORABLE
DROP
THIS
IS
TRAFFIC
THAT
CAN
SAFELY
BE
IGNORED
SECTION
IDENTIFIED
SEVERAL
SUCH
PROTOCOLS
AND
THE
TOP
RANKED
OF
THESE
ARE
LISTED
IN
TABLE
COMPARING
TABLES
AND
WE
SEE
THAT
FORTUNATELY
THERE
IS
A
SIG
NIFICANT
OVERLAP
BETWEEN
DON
T
WAKE
AND
IGNORABLE
PROTOCOLS
POLICY
DEPENDENT
TRAFFIC
APPLICATIONS
THAT
ARE
DEEMED
UNIMPORTANT
TO
MAINTAIN
DURING
IDLE
TIMES
COULD
LIKEWISE
BE
IGNORED
WHILE
DON
T
IGNORE
PROTOCOLS
OBVIOUSLY
CANNOT
BE
B
HANDLED
VIA
MECHANICAL
RESPONSES
THIS
INCLUDES
IN
COMING
OUTGOING
PROTOCOL
TRAFFIC
FOR
WHICH
IT
IS
EASY
TO
CONSTRUCT
THE
REQUIRED
RESPONSE
REQUEST
USING
LITTLE
TO
NO
STATE
TRANSFERRED
FROM
THE
SLEEPING
HO
NCTION
IS
SOME
WHAT
SUBJECTIVE
BASED
FOR
EXAMPLE
A
PROXY
CAN
EASILY
RESPOND
TO
NETBIOS
NAME
QUERIES
ASKING
ABOUT
LOCAL
NETBIOS
SERVICES
ONCE
THESE
SERVICES
ARE
KNOWN
BY
THE
PROXY
TABLE
LISTS
KEY
PROTOCOLS
THAT
CAN
BE
DEALT
WITH
THROUGH
MECHANICAL
RESPONSES
C
REQUIRE
SPECIALIZED
PROCESSING
THIS
COVERS
PROTO
COL
TRAFFIC
THAT
IF
PROXIED
WOULD
REQUIRE
MORE
COMPLEX
STATE
MAINTENANCE
TRANSFER
CREATION
PROCESSING
AND
UP
DATE
BETWEEN
THE
PROXY
AND
HOST
FOR
EXAMPLE
CONSIDER
A
PROXY
THAT
TAKES
ON
THE
ROLE
OF
COMPLETING
ONGOING
DOWNLOADS
ON
BEHALF
OF
A
SLEEPING
HOST
THIS
REQUIRES
THAT
THE
PROXY
LEARN
THE
STATUS
OF
ONGOING
AND
SCHED
ULED
DOWNLOADS
THE
ADDRESSES
OF
PEERS
ETC
AND
MORE
OVER
THAT
THE
PROXY
APPROPRIATELY
UPDATE
TRANSFER
STATE
AT
THE
HOST
ONCE
IT
RESUMES
IN
THEORY
SPECIALIZED
PROCESS
ING
WOULD
BE
ATTRACTIVE
FOR
POLICY
DEPENDENT
TRAF
FIC
THAT
IS
BOTH
IMPORTANT
AND
FREQUENTLY
OCCURRING
SINCE
OTHERWISE
WE
COULD
SIMPLY
DROP
UNIMPORTANT
TRAFFIC
AND
WAKE
THE
HOST
TO
PROCESS
INFREQUENT
TRAFFIC
OF
COURSE
IN
ADDITION
TO
THE
THE
ABOVE
CLASSES
A
C
FOR
TRAFFIC
THAT
A
PROXY
DOESN
T
IGNORE
BUT
DOESN
T
WANT
KNOW
TO
HANDLE
A
PROXY
ALWAYS
HAS
THE
OPTION
OF
WAKING
THE
HOST
ESSENTIALLY
THE
DECISION
OF
WHETHER
TO
HANDLE
DESIRED
TRAFFIC
IN
THE
PROXY
VERSUS
WAKING
THE
HOST
REPRESENTS
A
TRADEOFF
BETWEEN
THE
COMPLEXITY
OF
A
PROXY
IMPLEMENTATION
AND
THE
SLEEP
TIME
OF
HOSTS
EXAMPLE
PROXIES
WE
NOW
PRESENT
FOUR
CONCRETE
PROXY
DESIGNS
DERIVED
FROM
THE
DISTINCTIONS
DRAWN
ABOVE
WE
SELECT
THESE
PROX
IES
TO
BE
ILLUSTRATIVE
OF
THE
DESIGN
TRADEOFFS
POSSIBLE
BUT
ALSO
REPRESENTATIVE
OF
PRACTICAL
AND
USEFUL
PROXY
DESIGNS
PROXY
WE
START
WITH
A
VERY
SIMPLE
PROXY
THAT
IGNORES
ALL
TRAFFIC
LISTED
AS
IGNORABLE
IN
TABLE
AND
WAKES
THE
MACHINE
TO
HANDLE
ALL
OTHER
INCOMING
TRAFFIC
BESIDES
CLEARLY
IGNORABLE
PROTOCOLS
WE
CHOOSE
TO
ALSO
IGNORE
TRAFFIC
GENERATED
BY
THE
BIGFIX
APPLICATION
TCP
PORT
WHICH
WE
PREVIOUSLY
IDENTIFIED
SECTION
TO
BE
ONE
OF
THE
BIG
OFFENDERS
WE
DO
SO
BECAUSE
THIS
TRAF
FIC
IS
A
NOT
REPRESENTATIVE
FOR
NON
INTEL
MACHINES
AND
B
THE
APPLICATION
IS
VERY
BADLY
CONFIGURED
SENDING
VERY
LARGE
AMOUNTS
OF
TRAFFIC
FOR
LITTLE
OFFERED
FUNCTIONALITY
MAKING
SLEEP
ALMOST
IMPOSSIBLE
THIS
PROXY
IS
SIMPLE
IT
REQUIRES
NO
MECHANICAL
OR
SPE
CIALIZED
PROCESSING
AT
THE
SAME
TIME
BECAUSE
IT
MAKES
THE
CONSERVATIVE
CHOICE
OF
WAKING
THE
HOST
FOR
ALL
TRAF
FIC
NOT
KNOWN
TO
BE
SAFELY
IGNORABLE
THIS
PROXY
IS
FULLY
TRANSPARENT
TO
USERS
AND
APPLICATIONS
IN
THE
SENSE
THAT
THE
EFFECTIVE
BEHAVIOR
OF
THE
SLEEPING
MACHINE
IS
NEVER
DIFFERENT
FROM
HAD
IT
BEEN
IDLE
EXCEPT
FOR
THE
PERFOR
MANCE
PENALTIES
DUE
TO
THE
ADDITIONAL
WAKE
UP
TIME
PROXY
OUR
SECOND
PROXY
IS
ALSO
FULLY
TRANSPARENT
BUT
TAKES
ON
GREATER
COMPLEXITY
IN
ORDER
TO
REDUCE
THE
FRE
QUENCY
WITH
WHICH
THE
MACHINE
MUST
BE
WOKEN
THIS
PROXY
IGNORES
ALL
TRAFFIC
LISTED
AS
IGNORABLE
IN
TABLE
AND
ISSUES
RESPONSES
FOR
PROTOCOL
TRAFFIC
LISTED
IN
THE
SAME
TABLE
AS
TO
BE
HANDLED
WITH
MECHANICAL
RESPONSES
AND
WAKES
THE
MACHINE
FOR
ALL
OTHER
INCOMING
TRAFFIC
SINCE
THIS
PROXY
NEEDS
MORE
STATE
TO
GENERATE
MECHANI
CAL
RESPONSES
E
G
THE
NETBIOS
NAMES
OF
LOCAL
SERVICES
NEEDED
TO
ANSWER
NBNS
QUERIES
IT
CAN
ALSO
USE
THIS
EX
TRA
INFORMATION
TO
SELECTIVELY
IGNORE
MORE
PACKETS
THAN
PROXY
E
G
IGNORE
ALL
NETBIOS
DATAGRAMS
NOT
DES
TINED
FOR
LOCAL
SERVICES
PROXY
OUR
THIRD
PROXY
GENERATES
EVEN
DEEPER
SAVINGS
BY
ONLY
MAINTAINING
A
SMALL
SET
OF
APPLICATIONS
CHOSEN
BY
THE
USER
OPERABLE
DURING
IDLE
TIMES
WHILE
IGNORING
ALL
OTHER
TRAFFIC
WE
USE
TELNET
SSH
VNC
SMB
FILE
SHARING
AND
NETBIOS
AS
OUR
APPLICATIONS
OF
INTEREST
THIS
PROXY
PERFORMS
THE
SAME
ACTIONS
AND
AS
IMPLEMENTED
BY
PROXY
IGNORE
AND
RESPONDS
TO
THE
SAME
SET
OF
PROTO
COLS
BUT
IT
WAKES
UP
FOR
ALL
TRAFFIC
BELONGING
TO
ANY
OF
TELNET
SSH
VNC
SMB
FILE
SHARING
AND
NETBIOS
AND
DROPS
ANY
OTHER
INCOMING
TRAFFIC
RELATIVE
TO
OUR
PRE
VIOUS
EXAMPLE
PROXY
IS
LESS
TRANSPARENT
IN
THAT
THE
MACHINE
APPEARS
NOT
TO
BE
SLEEPING
FOR
SOME
SELECT
RE
MOTE
APPLICATIONS
BUT
IS
INACCESSIBLE
TO
ALL
OTHERS
PROXY
ALL
THE
ABOVE
PROXIES
IMPLEMENT
FUNCTIONALITY
RELATED
TO
HANDLING
INCOMING
PACKETS
IN
OUR
FINAL
PROXY
WE
ALSO
CONSIDER
WAKING
UP
FOR
SCHEDULED
TASKS
INITIATED
LOCALLY
THIS
PROXY
BEHAVES
IDENTICALLY
TO
PROXY
WITH
RESPECT
TO
INCOMING
PACKET
BUT
SUPPORTS
AN
ADDITIONAL
ACTION
WAKE
UP
FOR
THE
FOLLOWING
TASKS
FOR
WHICH
WE
ASSUME
THAT
THE
SYSTEM
IS
CONFIGURED
TO
WAKE
UP
IN
ORDER
TO
PERFORM
THEM
REGULAR
NETWORK
BACKUPS
ANTI
VIRUS
MCAFEE
SOFTWARE
UPDATES
FTP
TRAFFIC
FOR
AUTO
MATIC
SOFTWARE
UPDATES
AND
INTEL
SPECIFIC
UPDATES
EVALUATING
TRADEOFFS
IN
THE
FOLLOWING
WE
COMPARE
THE
SLEEP
ACHIEVABLE
BY
OUR
PROPOSED
PROXIES
AND
COM
PARE
IT
WITH
THE
BASELINE
WOP
CASE
WE
PERFORM
THIS
EVAL
UATION
FOR
BOTH
OFFICE
AND
HOME
ENVIRONMENTS
AND
IN
EACH
CASE
WE
EVALUATE
POSSIBLE
VALUES
FOR
TRANSITION
TIMES
TS
AND
SECONDS
THE
FIRST
OF
THESE
IS
A
VERY
OPTIMISTIC
TRANSITION
TIME
NOT
ACHIEVABLE
TODAY
USING
SLEEP
STATES
BUT
FORESEEABLE
IN
THE
NEAR
FUTURE
TODAY
MICROSOFT
VISTA
SPECIFICATIONS
REQUIRE
COMPUTERS
TO
RESUME
FROM
SLEEP
IN
UNDER
THE
SECOND
IS
REPRESENTATIVE
OF
THE
SHORTEST
TRANSITIONS
ACHIEV
ABLE
TODAY
AND
THE
LAST
IS
REPRESENTATIVE
OF
A
SETTING
THAT
ALLOWS
ALMOST
A
MINUTE
FOR
PROCESSING
SUB
SEQUENT
RELEVANT
NETWORK
PACKETS
BEFORE
GOING
TO
SLEEP
AGAIN
THE
ADVANTAGE
OF
USING
A
VERY
SHORT
TIMER
BEFORE
GOING
TO
SLEEP
IS
THE
INCREASED
ACHIEVABLE
SLEEP
THE
DIS
ADVANTAGE
IS
THAT
THE
DELAY
PENALTY
FOR
WAKING
THE
HOST
WILL
BE
INCURRED
AT
MORE
PACKETS
IN
THE
EXTREME
CASE
OF
VERY
SHORT
SLEEP
TIMERS
THIS
COULD
MAKE
REMOTE
APPLI
CATIONS
SLUGGISH
AND
UN
RESPONSIVE
FOR
THE
WAKE
EVENTS
GENERATED
BY
SCHEDULED
TASKS
WE
USE
A
LONGER
TRANSITION
TIME
AND
THUS
A
LONGER
SLEEP
TIMER
VALUE
OF
SINCE
SUCH
TASKS
USUALLY
TAKE
LONGER
TIME
TO
COMPLETE
TS
TS
TS
A
OFFICE
ENVIRONMENT
TS
TS
TS
B
HOME
ENVIRONMENT
SISTS
OF
A
TRIGGER
AN
ACTION
AND
A
TIMEOUT
TRIGGERS
ARE
EITHER
TIMER
EVENTS
OR
REGULAR
EXPRESSIONS
DESCRIBING
SOME
NETWORK
TRAFFIC
OF
INTEREST
WHEN
A
TRIG
GER
TIMER
EVENT
FIRES
OR
IF
AN
INCOMING
PACKET
MATCHES
A
TRIGGER
REGULAR
EXPRESSION
THE
PROXY
EXECUTES
THE
COR
RESPONDING
ACTION
IF
THE
ACTION
INVOLVES
WAKING
THE
HOST
THE
TIMEOUT
VALUE
SPECIFIES
THE
MINIMUM
PERIOD
OF
TIME
FOR
WHICH
THE
HOST
MUST
STAY
AWAKE
BEFORE
CONTEMPLATING
SLEEP
AGAIN
TO
RESOLVE
MULTIPLE
MATCHING
RULES
STANDARD
TECHNIQUES
SUCH
AS
ORDERING
THE
RULES
BY
SPECIFICITY
POL
ICY
ETC
CAN
BE
USED
THE
PROXY
TABLE
MUST
ALSO
INCLUDE
A
DEFAULT
RULE
THAT
DETERMINES
THE
TREATMENT
OF
PACKETS
THAT
DO
NOT
MATCH
ON
ANY
OF
THE
EXPLICITLY
ENUMERATED
RULES
WE
PROPOSE
THE
FOLLOWING
ACTIONS
DROP
THE
INCOMING
PACKET
IS
DROPPED
FIGURE
SAVINGS
ACHIEVED
BY
DIFFERENT
PROXIES
IN
HOME
AND
OFFICE
ENVIRONMENTS
EXAMINING
THE
PERFORMANCE
OF
OUR
PROXIES
WE
MAKE
THE
FOLLOWING
HIGH
LEVEL
OBSERVATIONS
A
AT
ONE
END
OF
THE
SPECTRUM
PROXY
THE
SIMPLEST
IS
INADEQUATE
IN
OFFICE
ENVIRONMENTS
AND
BORDERLINE
ADEQUATE
IN
HOME
ENVIRONMENTS
B
AT
THE
OTHER
END
OF
THE
SPECTRUM
WE
HAVE
PROXY
WHICH
ONLY
HANDLES
A
SELECT
NUMBER
OF
APPLICATIONS
BUT
IN
RETURN
ACHIEVES
GOOD
SLEEP
IN
ALL
SCE
NARIOS
MORE
THAN
OF
IDLE
TIME
EVEN
IN
THE
OFFICE
AND
WITH
A
TRANSITION
TIME
OF
C
THE
EFFICIENCY
OF
PROXY
DEPENDS
HEAVILY
ON
ENVIRONMENT
WHILE
THE
ADDITIONAL
COMPLEXITY
COMPARED
TO
PROXY
MAKES
IT
A
GOOD
FIT
IN
HOME
ENVIRONMENTS
SLEEPING
CLOSE
TO
EVEN
FOR
TS
HAVING
TO
HANDLE
ALL
TRAFFIC
MAKES
IT
A
WORSE
FIT
FOR
THE
OFFICE
SLEEPING
FOR
THE
SAME
TRANSITION
TIME
THIS
SHOWS
THAT
UNLESS
THEY
SUPPORT
A
LARGE
NUMBER
OF
RULES
TRANSPARENT
PROXIES
ARE
A
BETTER
FIT
FOR
HOME
BUT
NOT
THE
OFFICE
D
THE
BEST
TRADEOFF
BETWEEN
FUNCTIONALITY
AND
SAVINGS
AND
THEREFORE
THE
APPROPRIATE
PROXY
CONFIGURATION
DEPENDS
ON
THE
OPERATING
ENVIRON
MENT
E
SINCE
SCHEDULED
WAKE
UPS
ARE
TYPICALLY
INFRE
QUENT
THE
IMPACT
THEY
HAVE
ON
SLEEP
IS
MINIMAL
IN
OUR
CASE
PROXY
SLEEPS
ALMOST
AS
MUCH
AS
PROXY
IN
ALL
CONSIDERED
SCENARIOS
A
STRAWMAN
PROXY
ARCHITECTURE
OUR
STUDY
LEADS
US
TO
PROPOSE
A
SIMPLE
PROXY
ARCHITECTURE
THAT
OFFERS
A
UNIFIED
FRAMEWORK
WITHIN
WHICH
WE
CAN
AC
COMMODATE
THE
MULTIPLICITY
OF
DESIGN
OPTIONS
IDENTIFIED
ABOVE
THE
PROPOSAL
WE
PRESENT
IS
A
HIGH
LEVEL
ONE
SINCE
OUR
INTENT
HERE
IS
MERELY
TO
PROVIDE
AN
INITIAL
SKETCH
OF
AN
ARCHITECTURE
THAT
COULD
SERVE
AS
THE
STARTING
POINT
FOR
FUTURE
DISCUSSION
ON
STANDARDIZATION
EFFORTS
THE
CORE
OF
OUR
PROPOSAL
IS
A
TABLE
THE
POWER
PROXY
TABLE
PPT
THAT
STORES
A
LIST
OF
RULES
EACH
RULE
DE
SCRIBES
THE
MANNER
IN
WHICH
A
SPECIFIED
TRAFFIC
TYPE
SHOULD
BE
HANDLED
BY
THE
PROXY
WHEN
IDLE
A
RULE
CON
WAKE
THE
PROXY
WAKES
THE
HOST
AND
FORWARDS
THE
PACK
ETS
TO
IT
OTHER
PACKETS
BUFFERED
WHILE
WAITING
FOR
THE
WAKE
WILL
BE
FORWARDED
AS
WELL
RESPOND
TEMPLATE
STATE
THE
PROXY
USES
THE
SPECIFIED
TEMPLATE
TO
CRAFT
A
RESPONSE
BASED
ON
THE
IN
COMING
PACKET
AND
SOME
STATE
STORED
BY
THE
PROXY
THIS
ACTION
IS
USED
TO
GENERATE
MECHANICAL
RESPONSES
AS
DE
SCRIBED
BELOW
REDIRECT
HANDLE
THE
PROXY
FORWARDS
THE
PACKET
TO
A
DESTINATION
SPECIFIED
BY
THE
HANDLE
PARAMETER
THIS
IS
USED
TO
ACCOMMODATE
SPECIALIZED
PROCESSING
AS
DE
SCRIBED
BELOW
A
RESPONSE
TEMPLATE
IS
A
FUNCTION
THAT
COMPUTES
THE
MECHANICAL
RESPONSE
BASED
ON
THE
INCOMING
PACKET
AND
ONE
OR
MORE
IMMUTABLE
PIECES
OF
STATE
THIS
MEANS
THAT
OUR
FUNCTION
DOES
NOT
MAINTAIN
OR
CHANGE
ANY
STATE
THERE
IS
NO
STATE
CARRIED
OVER
BETWEEN
SUCCESSIVE
INCOMING
PACKETS
SUCH
AS
SEQUENCE
NUMBERS
AND
NO
STATE
TRANS
FER
BETWEEN
THE
PROXY
AND
THE
HOST
UPON
WAKE
UP
WE
CHOOSE
TO
SUPPORT
THIS
FUNCTIONALITY
BECAUSE
A
IT
IS
REL
ATIVELY
SIMPLE
TO
IMPLEMENT
IN
PRACTICE
AND
B
IT
COVERS
MOST
OF
THE
NON
APPLICATION
SPECIFIC
TRAFFIC
AS
SHOWN
IN
SECTION
AND
ILLUSTRATED
IN
OUR
PROXY
EXAMPLES
TO
ACCOMMODATE
MORE
SPECIALIZED
PROCESSING
WE
AS
SUME
DEVELOPERS
WILL
WRITE
APPLICATION
SPECIFIC
STUBS
AND
THEN
ENTER
A
REDIRECT
RULE
INTO
THE
PROXY
PPT
WHERE
THE
HANDLE
SPECIFIES
THE
LOCATION
TO
WHICH
THE
PROXY
SHOULD
SEND
THE
PACKET
SUCH
STUBS
CAN
RUN
ON
MACHINE
ACCESSIBLE
OVER
THE
NETWORK
E
G
A
SERVER
DEDICATED
TO
PROXYING
FOR
MANY
SLEEPING
MACHINES
IN
A
CORPORATE
LAN
OR
ON
A
LOW
POWER
MICRO
ENGINE
SUPPORTED
ON
THE
LOCAL
HOST
E
G
A
CONTROLLER
ON
THE
MOTHERBOARD
OR
A
USB
CONNECTED
GUMSTICK
IN
ALL
THESE
CASES
THE
HAN
DLE
WOULD
BE
SPECIFIED
BY
ITS
ADDRESS
FOR
EXAMPLE
A
IP
ADDRESS
PORT
COMBINATION
THE
REDIRECT
ABSTRACTION
THUS
ALLOWS
US
TO
ACCOMMODATE
SPECIALIZED
PROCESSING
WITH
OUT
EMBEDDING
APPLICATION
SPECIFIC
KNOWLEDGE
INTO
THE
CORE
PROXY
ARCHITECTURE
THE
EXTERNAL
API
TO
THIS
PROXY
IS
TWOFOLD
APIS
TO
FIGURE
EXAMPLE
CLICK
IMPLEMENTATION
ACTIVATE
DEACTIVATE
THE
PROXY
AS
THE
HOST
ENTERS
EXITS
SLEEP
AND
APIS
TO
INSERT
AND
DELETE
RULES
THE
PROCESS
BY
WHICH
TO
INSTALL
AND
EXECUTE
STUBS
IS
OUTSIDE
OF
THE
CORE
PROXY
SPECIFICATION
WHICH
ONLY
PROVIDES
THE
MECHANISM
TO
REGISTER
AND
INVOKE
SUCH
STUBS
THE
ARCHITECTURE
IS
AG
NOSTIC
TO
WHERE
THE
PROXY
RUNS
ALLOWING
IMPLEMENTATIONS
IN
HARDWARE
E
G
AT
HOST
NICS
IN
PC
SOFTWARE
E
G
A
PROXY
SERVER
RUNNING
ON
THE
SAME
LAN
OR
IN
NETWORK
EQUIPMENT
E
G
A
FIREWALL
NAT
BOX
FINALLY
THE
USE
OF
TIMER
EVENTS
TO
WAKE
A
HOST
ALREADY
EXISTS
TODAY
OUR
CONTRIBUTION
HERE
IS
MERELY
TO
INTEGRATE
THE
MECHANISM
INTO
A
UNIFIED
PROXY
ARCHITECTURE
PROXY
PROTOTYPE
IMPLEMENTATION
TO
ILLUSTRATE
THE
FEASIBILITY
OF
OUR
ARCHITECTURE
WE
BUILD
A
SIMPLE
PROXY
PROTOTYPE
USING
THE
CLICK
MODULAR
ROUTER
WE
CHOOSE
TO
DEPLOY
THE
PROXYING
FUNCTION
ALITY
IN
A
STANDALONE
MACHINE
RESPONSIBLE
FOR
MAINTAINING
THE
NETWORK
PRESENCE
OF
SEVERAL
HOSTS
ON
THE
SAME
LAN
TO
ALLOW
OUR
PROXY
LET
US
CALL
IT
P
TO
SNIFF
THE
TRAFFIC
FOR
EACH
HOST
WE
ENSURE
THAT
P
SHARES
THE
SAME
BROADCAST
DOMAIN
WITH
THESE
HOSTS
THIS
CAN
BE
ACHIEVED
EITHER
BY
CONNECTING
THE
PROXY
AND
THE
MACHINES
TO
A
COMMON
NET
WORK
HUB
OR
BY
CONFIGURING
THE
LAN
SWITCH
TO
FORWARD
ALL
TRAFFIC
TO
THE
PORT
THAT
SERVES
P
IN
OUR
INITIAL
DESIGN
WE
DON
T
IMPLEMENT
PROXIES
THAT
INVOLVE
TRANSFERRING
STATE
BETWEEN
THE
HOST
AND
THE
PROXY
INSTEAD
P
LEARNS
THE
PIECES
OF
STATE
REQUIRED
E
G
THE
IP
ADDRESS
AND
THE
NETBIOS
NAME
FOR
EACH
HOST
BY
SNIFF
ING
HOST
TRAFFIC
AND
EXTRACTING
THE
STATE
EXCHANGED
E
G
ARP
AND
NBNS
EXCHANGES
THIS
DESIGN
CIRCUMVENT
THE
NEED
FOR
ANY
END
HOST
MODIFICATIONS
AND
SUPPORT
PROXY
ING
FOR
MACHINES
WITH
DIFFERENT
HARDWARE
PLATFORMS
NEW
AND
OLD
AND
OPERATING
SYSTEMS
THE
PROXY
REQUIRES
MIN
IMAL
CONFIGURATION
A
LIST
OF
THE
MAC
ADDRESSES
OF
THE
HOSTS
THAT
NEED
TO
BE
PROXIED
AND
CAN
BE
INCREMEN
TALLY
DEPLOYED
AS
A
LOW
POWER
STAND
ALONE
NETWORK
BOX
ONCE
LOW
POWER
PROXYING
STANDARDS
ARE
DEVELOPED
THE
DESIGN
CAN
BE
EXTENDED
TO
SUPPORT
STATE
TRANSFER
AND
ACHIEVE
EVEN
DEEPER
ENERGY
SAVINGS
OUR
PROTOTYPE
IMPLEMENTS
VERY
BASIC
PROXYING
FUNC
TIONALITY
BUT
THE
SOFTWARE
ARCHITECTURE
PRESENTED
IN
FIG
URE
CAN
BE
EASILY
EXTENDED
TO
MORE
PROTOCOLS
AND
USE
CASES
CURRENTLY
WE
SUPPORT
THREE
TYPES
OF
ACTIONS
WAKE
RESPOND
AND
DROP
THE
PROXY
AWAKES
ITS
HOSTS
FOR
TCP
CONNECTION
REQUESTS
INCOMING
TCP
SYN
PACKETS
AND
INCOMING
NETBIOS
NAME
QUERIES
FOR
THE
HOST
NB
NAME
IF
SUCH
A
WAKE
PACKET
FOR
A
SLEEPING
HOST
ARRIVES
P
BUFFERS
THE
REQUEST
SENDS
A
MAGIC
PACKET
TO
WAKE
THE
HOST
AND
RELAYS
THE
BUFFERED
PACKET
ONCE
THE
HOST
BE
COMES
AVAILABLE
THE
PROXY
RESPONDS
AUTOMATICALLY
TO
INCOMING
ARP
REQUESTS
AND
DROPS
ALL
OTHER
INCOMING
PACKETS
IN
RELATION
TO
THE
EXAMPLES
DISCUSSED
IN
SEC
TION
THIS
PROTOTYPE
HAS
A
SIMPLE
AND
NON
TRANSPARENT
DESIGN
TO
DETERMINE
WHETHER
A
HOST
IS
AWAKE
THE
PROXY
SENDS
PERIODIC
ARP
QUERIES
TO
EACH
HOST
IF
THESE
QUERIES
RECEIVE
NO
RESPONSE
THE
HOST
IS
ASSUMED
TO
BE
ASLEEP
WHEN
THE
PROXY
ATTEMPTS
TO
WAKE
A
HOST
AND
FAILS
REPEAT
EDLY
THE
HOST
IS
ASSUMED
TO
BE
OFF
RATHER
THAN
JUST
ASLEEP
AND
THE
PROXY
CEASES
TO
MAINTAIN
ITS
NETWORK
PRESENCE
FIGURE
PRESENTS
THE
SOFTWARE
ARCHITECTURE
OF
OUR
CLICK
PROXY
AND
HIGHLIGHTS
THE
MAPPING
BETWEEN
CLICK
MODULES
AND
THE
GENERIC
CATEGORIES
OF
TRIGGERS
ACTIONS
AND
STATE
DISCUSSED
IN
THE
STRAWMAN
PROXY
ARCHITECTURE
WE
TEST
OUR
CLICK
BASED
PROXY
IMPLEMENTATION
BY
IN
STALLING
IT
ON
ONE
OF
OUR
ENTERPRISE
DESKTOPS
AND
CON
FIGURING
THE
PROXY
TO
MAINTAIN
THE
NETWORK
PRESENCE
OF
SEVERAL
IBM
THINKPAD
LAPTOPS
WE
USE
THIS
DEPLOYMENT
TO
MEASURE
THE
DELAYS
EXPERIENCED
BY
APPLICATIONS
WAK
ING
A
SLEEPING
HOST
AND
FIND
THESE
TO
BE
SURPRISINGLY
LOW
ON
AVERAGE
AND
AT
MAXIMUM
MUCH
LOWER
THAN
THE
TCP
SYN
TIMEOUT
THESE
DELAYS
INCLUDES
THE
HOST
WAKE
UP
DELAY
AND
THE
ADDITIONAL
TIME
RE
QUIRED
FOR
THE
PROXY
TO
DETECT
THE
STATE
CHANGE
AND
RELAY
THE
BUFFERED
PACKET
CAUSING
THE
WAKE
WE
DEFER
A
COMPREHENSIVE
DEPLOYMENT
BASED
EVALUATION
TO
FUTURE
WORK
POWER
AWARE
SYSTEM
REDESIGN
IN
THIS
SECTION
WE
CONSIDER
APPROACHES
THAT
MIGHT
ASSIST
IN
REDUCING
IDLE
TIME
ENERGY
CONSUMPTION
BY
EITHER
SIM
PLIFYING
THE
IMPLEMENTATION
OF
PROXIES
OR
ALTOGETHER
OB
VIATING
THE
NEED
FOR
PROXYING
SOFTWARE
REDESIGN
OUR
IDLE
TRAFFIC
ANALYSIS
SHOWS
THAT
SOLUTIONS
RELYING
ON
WAKE
ON
LAN
FUNCTIONALITY
FACE
THE
FOLLOWING
CHAL
LENGES
I
IT
IS
DIFFICULT
TO
DECIDE
IF
VARIOUS
PACKETS
AND
PROTOCOLS
WARRANT
A
MACHINE
WAKE
UP
II
HOSTS
RECEIVE
MANY
PACKETS
EVEN
WHEN
IDLE
PER
SECOND
ON
AVERAGE
III
MANY
PROTOCOLS
EXCHANGE
PACKETS
PERIODICALLY
PRE
VENTING
LONG
QUIET
PERIODS
WHEN
HOSTS
COULD
SLEEP
THESE
CHALLENGES
COULD
BE
DEALT
WITH
AT
BOTH
APPLICATION
AND
PROTOCOL
LEVEL
POWER
AWARE
APPLICATION
CONFIGURATION
TODAY
APPLI
CATIONS
AND
SERVICES
ARE
TYPICALLY
DESIGNED
OR
CONFIGURED
WITHOUT
TAKING
INTO
ACCOUNT
THEIR
POTENTIAL
IMPACT
ON
THE
POWER
MANAGEMENT
AT
END
SYSTEMS
FOR
EXAMPLE
IN
SEC
TION
WE
DISCUSSED
A
TOOL
CALLED
BIGFIX
THAT
CHECKS
IF
NETWORK
HOSTS
CONFORM
TO
INTEL
CORPORATE
SECURITY
SPEC
IFICATIONS
THIS
APPLICATION
WAS
CONFIGURED
TO
PERFORM
THESE
CHECKS
VERY
AGGRESSIVELY
CONTINUOUSLY
GENERATING
LARGE
AMOUNTS
OF
TRAFFIC
UNDER
A
WOL
APPROACH
THIS
AP
PLICATION
ALONE
WOULD
HAVE
MADE
PROLONGED
SLEEP
VIRTU
ALLY
IMPOSSIBLE
THIS
IS
A
PERFECT
EXAMPLE
OF
THE
BEHAVIOUR
THAT
COULD
BE
AVOIDED
BY
CONFIGURING
APPLICATIONS
TO
BE
MORE
POWER
AWARE
AND
PERFORM
PERIODIC
TASKS
LESS
FREQUENTLY
REDUC
ING
THE
VOLUME
OF
NETWORK
TRAFFIC
SEEN
BY
HOSTS
PROTOCOL
SPECIFICATION
THE
DECISION
TO
IGNORE
OR
WAKE
ON
A
PACKET
CAN
BE
DIFFICULT
AND
INVOLVES
PROTOCOL
PARS
ING
MAINTAIING
A
LONG
SET
OF
FILTERS
AND
RULES
AND
FOR
SOME
PROTOCOLS
HOST
OR
APPLICATION
SPECIFIC
STATE
TO
ELIMINATE
THE
COMPLEXITY
OF
THIS
DECISION
AND
AL
LOW
HOSTS
TO
SLEEP
LONGER
EVEN
WHEN
USING
VERY
SIMPLE
RULES
FOR
WAKING
PROTOCOLS
COULD
BE
AUGMENTED
TO
CARRY
EXPLICIT
POWER
RELATED
INFORMATION
IN
THEIR
PACKETS
AN
EXAMPLE
OF
SUCH
INFORMATION
WOULD
BE
A
SIMPLE
BIT
INDI
CATING
WHETHER
A
PACKET
CAN
BE
IGNORED
PROTOCOL
REDESIGN
WE
BELIEVE
THESE
PRINCIPLES
SHOULD
BE
FOLLOWED
WHEN
DESIGNING
POWER
AWARE
PROTOCOLS
CONSIDERATION
WHEN
USING
BROADCAST
AND
MULTICAST
WE
SAW
EARLIER
THAT
BROADCAST
AND
MULTICAST
ARE
MAINLY
RE
SPONSIBLE
FOR
KEEPING
HOSTS
AWAKE
THIS
TYPE
OF
TRAFFIC
COULD
BE
SUBSTANTIALLY
REDUCED
BY
REDESIGNING
PROTOCOLS
TO
USE
BROADCASTS
SPARINGLY
SOME
PROTOCOLS
ARE
PARTIC
ULARLY
INEFFICIENT
IN
THIS
RESPECT
FOR
EXAMPLE
ALL
NET
BIOS
DATAGRAMS
ARE
ALWAYS
SENT
OVER
ETHERNET
BROADCAST
FRAMES
THESE
FRAMES
ARE
RECEIVED
BY
ALL
HOSTS
ON
THE
LAN
AND
THEN
DISCARDED
BY
MOST
OF
THEM
THIS
RANKS
NBDGM
AS
ONE
OF
THE
TOP
OFFENDERS
YET
THIS
COULD
BE
EASILY
AVOIDED
BY
USING
UNICAST
TRANSMISSIONS
WHEN
POS
SIBLE
ANOTHER
APPROACH
IS
BASED
ON
THE
OBSERVATION
THAT
MANY
SERVICE
DISCOVERY
PROTOCOLS
HAVE
REDUNDANT
FUNC
TIONALITY
THIS
REDUNDANT
FUNCTIONALITY
COULD
CONCEIVABLE
BE
REPLACED
BY
A
SINGLE
SERVICE
THAT
CAN
BE
SHARED
BY
A
MULTIPLICITY
OF
APPLICATIONS
SYNCHRONIZATION
OF
PERIODIC
TRAFFIC
ONE
WAY
TO
IN
CREASE
THE
NUMBER
OF
LONG
PERIODS
OF
NETWORK
QUIES
CENCE
WOULD
BE
TO
IDENTIFY
PROTOCOLS
THAT
USE
PERIODIC
UPDATES
MESSAGE
EXCHANGES
AND
TRY
TO
SYNCHRONIZE
OR
BULK
THESE
EXCHANGES
TOGETHER
THIS
WOULD
ALLOW
MA
CHINES
TO
PERIODICALLY
WAKE
UP
PROCESS
ALL
NOTIFICATIONS
AND
REQUEST
AND
RESUME
SLEEP
COMPLEMENTING
SOFT
STATE
MANY
PROTOCOLS
E
G
SSDP
NETBIOS
ETC
MAINTAIN
AND
UPDATE
STATE
USING
PERI
ODIC
BROADCAST
NOTIFICATIONS
FOR
SUCH
PROTOCOLS
AND
FOR
SIMILAR
APPLICATIOS
IT
WOULD
BE
ESSENTIAL
TO
MAKE
THEM
DISCONNECTION
TOLERANT
BY
PROVIDING
COMPLEMEN
TARY
STATE
QUERY
MECHANISMS
THAT
COULD
BE
USED
QUICKLY
BUILD
UP
TO
DATE
COPIES
OF
THE
SOFT
STATE
UPON
WAKING
THIS
WOULD
ENABLE
IGNORING
ANY
SOFT
STATE
NOTIFICATIONS
TODAY
SUCH
QUERY
MECHANISMS
EXIST
ONLY
FOR
SOME
OF
THESE
PROTOCOLS
AND
THEY
ARE
OFTEN
INEFFICIENT
HARDWARE
REDESIGN
A
GENERAL
GOAL
OF
ENERGY
SAVING
MECHANISMS
ESPECIALLY
HARDWARE
DESIGNS
IS
TO
LEAD
THE
INDUSTRY
TOWARDS
ENERGY
PROPORTIONAL
COMPUTING
IF
ENERGY
CONSUMPTION
OF
A
MACHINE
WOULD
ACCURATELY
REFLECT
ITS
LEVEL
OF
UTILIZATION
THE
ENERGY
WOULD
BE
ZERO
WHEN
IDLE
SLEEP
STATES
ARE
A
STEP
IN
THIS
DIRECTION
P
STATES
LOW
POWER
ACTIVE
OPERA
TION
ARE
ANOTHER
RELATED
TO
THIS
IT
WOULD
BE
VERY
DESIR
ABLE
TO
EXPOSE
POWER
SAVING
STATES
STATES
THAT
FEATURE
BETTER
TRANSITION
TIMES
EVEN
IF
THEY
OFFER
SMALLER
SAVINGS
GIVEN
THE
SMALL
INTER
PACKET
GAPS
THESE
STATES
WILL
COME
IN
HANDIER
THAN
THE
DEEP
SLEEP
ONES
RELATED
WORK
THE
NOTION
THAT
INTERNETWORKED
SYSTEMS
WASTE
ENERGY
DUE
TO
IDLE
PERIODS
HAS
BEEN
FREQUENTLY
REITERATED
NETWORK
PRESENCE
PROXYING
FOR
THE
PURPOSE
OF
SAVING
ENERGY
IN
END
DEVICES
WAS
FIRST
PROPOSED
OVER
TEN
YEARS
AGO
BY
CHRISTENSEN
ET
AL
IN
FOLLOW
UP
WORK
THE
AUTHORS
QUANTIFY
THE
POTENTIAL
SAVINGS
USING
TRAFFIC
TRACES
FROM
A
SINGLE
DORMITORY
AC
CESS
POINT
AND
IN
EXAMINE
THE
TRAFFIC
RECEIVED
AT
A
SINGLE
IDLE
MACHINE
TO
IDENTIFY
DOMINANT
PROTOCOLS
AND
DISCUSS
WHETHER
THESE
CAN
BE
SAFELY
IGNORED
OUR
WORK
DRAWS
INSPIRATION
FROM
THIS
EARLY
WORK
EXTENDING
IT
WITH
A
LARGE
SCALE
AND
MORE
IN
DEPTH
EVALUATION
OF
IDLE
TIME
TRAFFIC
IN
ENTERPRISE
AND
HOME
ENVIRONMENTS
A
MORE
RE
CENT
PROPOSAL
POSTULATES
THE
NOTION
OF
SELECTIVE
CON
NECTIVITY
WHEREBY
A
HOST
CAN
DICTATE
OR
MANAGE
ITS
NET
WORK
CONNECTIVITY
GOING
TO
SLEEP
WHEN
IT
DOES
NOT
WANT
TO
RESPOND
TO
TRAFFIC
THERE
IS
AN
EXTENSIVE
LITERATURE
ON
ENERGY
SAVING
TECH
NIQUES
FOR
INDIVIDUAL
PC
PLATFORMS
BROADLY
THESE
AIM
FOR
REDUCED
POWER
DRAWS
AT
THE
HARDWARE
LEVEL
AND
FASTER
TRANSITION
TIMES
AT
THE
SYSTEM
LEVEL
THESE
OFFER
A
COM
PLEMENTARY
APPROACH
TO
REDUCING
THE
POWER
DRAW
OF
IDLE
MACHINES
IF
AND
WHEN
THESE
TECHNIQUES
LEAD
US
TO
PERFECTLY
ENERGY
PROPORTIONAL
COMPUTERS
THE
IDLE
TIME
CONSUMPTION
WILL
BE
LESS
PROBLEMATIC
AND
PROXYING
WILL
FADE
IN
IMPORTANCE
SO
FAR
HOWEVER
ACHIEVING
SUCH
EN
ERGY
PROPORTIONALITY
HAS
PROVED
CHALLENGING
IN
PARALLEL
WORK
THE
AUTHORS
BUILD
A
PROTOTYPE
PROXY
SUPPORTING
BIT
TORRENT
AND
IM
AS
EXAMPLE
APPLICA
TIONS
OUR
WORK
CONSIDERS
A
BROADER
PROXY
DESIGN
SPACE
EVALUATING
THE
TRADEOFFS
BETWEEN
DESIGN
OPTIONS
AND
THE
RESULTANT
ENERGY
SAVINGS
INFORMED
BY
DETAILED
ANALYSIS
OF
NETWORK
TRAFFIC
IN
RELATION
TO
OUR
DESIGN
SPACE
THEIR
PROXY
SUPPORTS
BT
AND
IM
USING
APPLICATION
STUBS
CONCLUSIONS
IN
GENERAL
THE
QUESTION
OF
HOW
A
PROXY
SHOULD
HANDLE
THE
USER
IDLE
TIME
TRAFFIC
PRESENTS
A
COMPLEX
TRADEOFF
BE
TWEEN
BALANCING
THE
COMPLEXITY
OF
THE
PROXY
THE
AMOUNT
OF
ENERGY
SAVED
AND
THE
SOPHISTICATION
OF
IDLE
TIME
FUNC
TIONALITY
THROUGH
THE
USE
OF
AN
UNUSUAL
DATASET
COLLECTED
DIRECTLY
ON
ENDHOSTS
WE
EXPLORED
THE
POTENTIAL
SAVINGS
REQUIREMENTS
AND
EFFECTIVENESS
OF
TECHNOLOGIES
THAT
AIM
TO
PUT
ENDHOST
MACHINES
TO
SLEEP
WHEN
USERS
ARE
IDLE
FOR
THE
FIRST
TIME
HERE
WE
DISSECT
THE
DIFFERENT
CATEGORIES
OF
TRAFFIC
THAT
ARE
PRESENT
DURING
IDLE
TIMES
AND
QUAN
TIFY
WHICH
OF
THEM
HAVE
TRAFFIC
ARRIVAL
PATTERNS
THAT
PRE
VENT
PERIODS
OF
DEEP
SLEEP
WE
SEE
THAT
BROADCAST
AND
MULTICAST
TRAFFIC
CONSTITUTE
A
SUBSTANTIAL
AMOUNT
OF
THE
BACKGROUND
CHATTER
DUE
TO
SERVICE
DISCOVERY
AND
ROUTING
PROTOCOLS
OUR
DATA
ALSO
REVEALED
A
SIGNIFICANT
AMOUNT
OF
OUTGOING
CONNECTIONS
GENERATED
IN
PART
BY
ENTERPRISE
AP
PLICATIONS
WE
TRIED
TO
IDENTIFY
WHICH
TRAFFIC
CAN
BE
IG
NORED
AND
FOUND
THAT
MOST
OF
THE
BROADCAST
AND
MULTICAST
TRAFFIC
AS
WELL
AS
ROUGHLY
OF
OUTGOING
CONNECTIONS
APPEARS
SAFELY
IGNORABLE
HANDLING
UNICAST
TRAFFIC
IS
MORE
INVOLVED
BECAUSE
IT
HARDER
TO
INFER
THE
INTENT
OF
SUCH
TRAF
FIC
AND
OFTEN
NEEDS
SOME
STATE
INFORMATION
TO
BE
MAIN
TAINED
ON
THE
PROXY
AFTER
HAVING
STUDIED
OUR
TRAFFIC
AND
THE
SLEEP
POTEN
TIAL
THOSE
PATTERNS
CONTAIN
WE
DISCUSS
THE
DESIGN
SPACE
FOR
PROXIES
AND
EVALUATE
THE
SAVINGS
OFFERED
BY
SAM
PLE
PROXY
DESIGNS
THESE
CASES
REVEAL
THE
TRADEOFFS
BE
TWEEN
DESIGN
COMPLEXITY
AVAILABLE
FUNCTIONALITY
AND
EN
ERGY
SAVINGS
AND
DISCUSS
THE
APPROPRIATENESS
OF
VARI
OUS
DESIGN
POINTS
IN
DIFFERENT
USE
ENVIRONMENTS
SUCH
AS
HOME
AND
OFFICE
FINALLY
WE
PRESENT
A
GENERAL
AND
FLEXIBLE
STRAWMAN
PROXY
ARCHITECTURE
AND
WE
BUILD
AN
EXTENSIBLE
CLICK
BASED
PROXY
THAT
EXEMPLIFIES
ONE
WAY
IN
WHICH
THIS
AR
CHITECTURE
CAN
BE
IMPLEMENTED
A
CONTENT
AWARE
BLOCK
PLACEMENT
ALGORITHM
FOR
REDUCING
PRAM
STORAGE
BIT
WRITES
BRIAN
WONGCHAOWART
MARIAN
K
ISKANDER
SANGYEUN
CHO
MARCH
MOTIVATION
SUPPOSE
THAT
YOU
HAD
A
BLOCK
STORAGE
DEVICE
WITH
THE
FOLLOWING
CHARACTERISTICS
THERE
ARE
MANY
FREE
BLOCKS
THAT
CAN
BE
OVERWRITTEN
THE
COST
OF
WRITING
A
NEW
BLOCK
DEPENDS
ON
THE
CHOICE
OF
WHICH
FREE
BLOCK
IS
OVERWRITTEN
HOW
CAN
YOU
MINIMIZE
THE
COST
OF
WRITING
A
SEQUENCE
OF
NEW
BLOCKS
TO
SUCH
A
DEVICE
OFFLINE
PROBLEM
THE
OFFLINE
VERSION
OF
THE
PROBLEM
CAN
EASILY
BE
REDUCED
TO
MAXIMUM
WEIGHT
BIPARTITE
MATCHING
WE
NEED
TO
MATCH
NEW
BLOCKS
TO
BE
WRITTEN
WITH
FREE
BLOCKS
TO
OVERWRITE
THE
EDGE
BETWEEN
A
NEW
BLOCK
X
AND
A
FREE
BLOCK
A
HAS
COST
W
D
X
A
WHERE
D
X
A
IS
THE
COST
OF
OVERWRITING
A
WITH
X
HAMMING
DISTANCE
OR
FLIP
N
WRITE
COST
AND
W
IS
A
CONSTANT
CHOSEN
SO
THAT
W
D
X
A
IS
ALWAYS
POSITIVE
EFFICIENT
POLYNOMIAL
TIME
ALGORITHMS
FOR
MAXIMUM
WEIGHT
BIPARTITE
MATCHING
ARE
KNOWN
FOR
A
TABLE
OF
RESULTS
SEE
PIOTR
SANKOWSKI
MAXIMUM
WEIGHT
BIPARTITE
MATCHING
IN
MATRIX
MULTIPLICATION
TIME
THEORETICAL
COMPUTER
SCIENCE
NO
ONLINE
PROBLEM
IN
THE
ONLINE
VERSION
OF
THE
PROBLEM
NEW
BLOCKS
TO
BE
WRITTEN
ARRIVE
ONE
AT
A
TIME
THE
NATURAL
GREEDY
ALGORITHM
IS
TO
WRITE
A
NEW
BLOCK
TO
THE
FREE
LOCATION
WITH
THE
LEAST
COST
THIS
GREEDY
ALGORITHM
IS
NOT
OPTIMAL
HOWEVER
EVERYTHING
ON
COMPETITIVENESS
HAS
BEEN
REMOVED
FROM
THIS
VERSION
OF
MY
SLIDES
IMPLEMENTING
THE
GREEDY
ALGORITHM
GIVEN
A
SET
OF
FREE
BLOCKS
AND
A
NEW
BLOCK
TO
BE
WRITTEN
HOW
CAN
ONE
QUICKLY
FIND
THE
FREE
BLOCK
CLOSEST
TO
THE
NEW
BLOCK
THE
ONE
THAT
COSTS
THE
LEAST
TO
OVERWRITE
THIS
IS
THE
WELL
STUDIED
NEAREST
NEIGHBOR
SEARCH
PROBLEM
GIVEN
A
SET
OF
POINTS
FIND
THE
POINT
IN
THAT
IS
CLOSEST
TO
A
SPECIFIED
POINT
NOT
NECESSARILY
IN
SINCE
THE
POINTS
THAT
WE
ARE
INTERESTED
IN
ARE
BINARY
VECTORS
MANHATTAN
DISTANCE
HERE
EQUIVALENT
TO
HAMMING
DISTANCE
IS
THE
RIGHT
DISTANCE
METRIC
IN
THE
CASE
OF
FLIP
N
WRITE
ONE
CAN
SEARCH
FOR
THE
NEAREST
NEIGHBOR
OF
EACH
OF
THE
TWO
POSSIBLE
REPRESENTATIONS
OF
A
NEW
BLOCK
ELIAS
IF
WE
WANT
TO
LOCATE
A
POINT
CLOSE
TO
A
GIVEN
POINT
IN
CONSTANT
TIME
WE
CAN
TRY
USING
A
HASH
FUNCTION
SPECIFICALLY
OUR
GOAL
IS
TO
PARTITION
THE
SPACE
OF
POSSIBLE
BIT
STRINGS
POINTS
INTO
REGIONS
SUCH
THAT
TWO
POINTS
THAT
LIE
IN
THE
SAME
REGION
ARE
CLOSE
TO
EACH
OTHER
THE
HASH
VALUE
OF
A
POINT
IS
AN
IDENTIFIER
FOR
THE
REGION
IN
WHICH
IT
LIES
AN
IDEAL
PARTITIONING
IS
A
SET
OF
SPHERES
OF
RADIUS
D
THAT
COMPLETELY
FILL
THE
SPACE
WHICH
GUARANTEES
THAT
POINTS
THAT
LIE
IN
THE
SAME
SPHERE
WILL
BE
WITHIN
DISTANCE
OF
ONE
ANOTHER
L
RIVEST
ON
THE
OPTIMALITY
OF
ELIAS
ALGORITHM
FOR
PERFORMING
BEST
MATCH
SEARCHES
INFORMATION
PROCESSING
ELIAS
OBSERVED
THAT
GIVEN
A
PERFECT
ERROR
CORRECTING
CODE
WITH
LENGTH
N
AND
MINIMUM
DISTANCE
BETWEEN
CODEWORDS
THE
CODEWORDS
ARE
PRECISELY
THE
CENTERS
OF
SPHERES
OF
RADIUS
D
THAT
COMPLETELY
FILL
THE
SPACE
OF
POSSIBLE
BINARY
VECTORS
OF
LENGTH
N
FINDING
THE
UNIQUE
CODEWORD
WITHIN
HAMMING
DISTANCE
D
OF
AN
ARBITRARY
POINT
IS
JUST
THE
DECODING
PROBLEM
FOR
AN
ERROR
CORRECTING
CODE
THAT
CAN
CORRECT
UP
TO
D
ERRORS
GIVEN
A
NEW
BLOCK
TO
BE
WRITTEN
THE
SPHERE
IN
WHICH
IT
LIES
AND
POSSIBLY
NEIGHBORING
SPHERES
CAN
BE
SEARCHED
UNTIL
A
FREE
BLOCK
IS
FOUND
THE
MAJOR
PROBLEM
WITH
THIS
SCHEME
IS
THAT
IT
IS
DIFFICULT
TO
DECODE
A
BINARY
VECTOR
OF
BYTES
EFFICIENTLY
BLOCK
SIGNATURE
COMPUTATION
ALGORITHM
IN
MY
PAPER
DIVIDE
A
BLOCK
INTO
EQUAL
SIZE
SETS
OF
BITS
AND
USE
A
VECTOR
CONTAINING
THE
APPROXIMATE
COUNT
OF
BITS
IN
EACH
SET
AS
THE
BLOCK
SIGNATURE
HASH
VALUE
PROBLEMS
BLOCKS
WITH
THE
SAME
SIGNATURE
MAY
NOT
BE
SIMILAR
SIMILAR
BLOCKS
MAY
NOT
HAVE
THE
SAME
SIGNATURE
NONUNIFORM
DISTRIBUTION
OF
SIGNATURE
VALUES
EVEN
FOR
UNIFORM
RANDOM
DATA
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
UNIFORMLY
DISTRIBUTED
RANDOM
DATA
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
THE
WRITE
REQUEST
DATA
BLOCKS
OF
THE
JPEG
TRACE
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
THE
WRITE
REQUEST
DATA
BLOCKS
OF
THE
DNG
TRACE
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
THE
WRITE
REQUEST
DATA
BLOCKS
OF
THE
KERNELBUILD
TRACE
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
THE
WRITE
REQUEST
DATA
BLOCKS
OF
THE
SWSUSP
TRACE
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
THE
WRITE
REQUEST
DATA
BLOCKS
OF
THE
BT
TRACE
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
THE
WRITE
REQUEST
DATA
BLOCKS
OF
THE
CG
TRACE
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
THE
WRITE
REQUEST
DATA
BLOCKS
OF
THE
FT
TRACE
FIGURE
DISTRIBUTION
OF
SIGNATURE
VALUES
FOR
THE
WRITE
REQUEST
DATA
BLOCKS
OF
THE
MG
TRACE
SETS
BLOCK
BITS
SET
TOTAL
BITS
SEARCH
DISTANCE
LIMIT
TABLE
PERCENTAGE
OF
THE
RANDOM
TRACE
REQUIRING
A
BIT
WRITE
SETS
BLOCK
BITS
SET
TOTAL
BITS
SEARCH
DISTANCE
LIMIT
TABLE
PERCENTAGE
OF
THE
PERMUTATION
TRACE
REQUIRING
A
BIT
WRITE
SETS
BLOCK
BITS
SET
TOTAL
BITS
SEARCH
DISTANCE
LIMIT
TABLE
PERCENTAGE
OF
THE
JPEG
TRACE
REQUIRING
A
BIT
WRITE
SETS
BLOCK
BITS
SET
TOTAL
BITS
SEARCH
DISTANCE
LIMIT
TABLE
PERCENTAGE
OF
THE
DNG
TRACE
REQUIRING
A
BIT
WRITE
SETS
BLOCK
BITS
SET
TOTAL
BITS
SEARCH
DISTANCE
LIMIT
TABLE
PERCENTAGE
OF
THE
KERNELBUILD
TRACE
REQUIRING
A
BIT
WRITE
SETS
BLOCK
BITS
SET
TOTAL
BITS
SEARCH
DISTANCE
LIMIT
TABLE
PERCENTAGE
OF
THE
SWSUSP
TRACE
REQUIRING
A
BIT
WRITE
TRACE
INITIAL
PRAM
CONTENTS
ZEROS
BT
CG
FT
MG
BT
CG
FT
49
MG
49
TABLE
PERCENTAGE
OF
THE
NAS
SNAPSHOT
TRACES
REQUIRING
A
BIT
WRITE
WHEN
DCW
IS
USED
WITH
RANDOM
BLOCK
PLACEMENT
TRACE
INITIAL
PRAM
CONTENTS
ZEROS
BT
CG
FT
MG
BT
CG
FT
MG
TABLE
PERCENTAGE
OF
THE
NAS
SNAPSHOT
TRACES
REQUIRING
A
BIT
WRITE
WHEN
SIGNATURE
BASED
BLOCK
PLACEMENT
IS
USED
WITH
SETS
PER
BLOCK
BITS
PER
SET
AND
A
SEARCH
DISTANCE
LIMIT
OF
TRACE
INITIAL
PRAM
CONTENTS
ZEROS
BT
CG
FT
MG
BT
CG
FT
MG
TABLE
PERCENTAGE
OF
THE
NAS
SNAPSHOT
TRACES
REQUIRING
A
BIT
WRITE
WHEN
DCW
IS
USED
WITH
A
MANUAL
BLOCK
PLACEMENT
STRATEGY
SUMMARY
THE
NUMBER
OF
BIT
PROGRAMMING
OPERATIONS
NEEDED
TO
STORE
A
NEW
DATA
BLOCK
IN
A
PRAM
STORAGE
DEVICE
DEPENDS
ON
THE
CURRENT
CONTENTS
OF
THE
LOCATION
AT
WHICH
THE
BLOCK
IS
WRITTEN
WE
PROPOSED
A
SIGNATURE
BASED
BLOCK
PLACEMENT
ALGORITHM
FOR
REDUCING
THE
NUMBER
OF
BIT
WRITES
REQUIRED
TO
STORE
A
SEQUENCE
OF
NEW
DATA
BLOCKS
WHICH
SAVES
ENERGY
WITH
THE
RIGHT
PARAMETER
SETTINGS
OUR
BLOCK
PLACEMENT
ALGORITHM
WAS
ABLE
TO
REDUCE
THE
NUMBER
OF
BIT
WRITES
NEEDED
TO
AS
LOW
AS
OF
THE
NUMBER
NEEDED
WHEN
DCW
DATA
COMPARISON
WRITE
ALONE
IS
USED
THIS
FIGURE
WAS
ACHIEVED
WITHOUT
READING
AND
COMPARING
MULTIPLE
FREE
BLOCKS
TEMPERATURE
INDUCED
RELIABILITY
ISSUES
ARE
AMONG
THE
MAJOR
CHAL
LENGES
FOR
MULTICORE
ARCHITECTURES
THERMAL
HOT
SPOTS
AND
THER
MAL
CYCLES
COMBINE
TO
DEGRADE
RELIABILITY
THIS
RESEARCH
PRESENTS
NEW
RELIABILITY
AWARE
JOB
SCHEDULING
AND
POWER
MANAGEMENT
AP
PROACHES
FOR
CHIP
MULTIPROCESSORS
ACCURATE
EVALUATION
OF
THESE
POLICIES
REQUIRES
A
NOVEL
SIMULATION
FRAMEWORK
THAT
CAN
CAPTURE
ARCHITECTURE
LEVEL
EFFECTS
OVER
TENS
OF
SECONDS
OR
LONGER
WHILE
ALSO
CAPTURING
THERMAL
INTERACTIONS
AMONG
CORES
RESULTING
FROM
DYNAMIC
SCHEDULING
POLICIES
USING
THIS
FRAMEWORK
AND
A
SET
OF
NEW
THERMAL
MANAGEMENT
POLICIES
THIS
WORK
SHOWS
THAT
TECHNIQUES
THAT
OFFER
SIMILAR
PERFORMANCE
ENERGY
AND
EVEN
PEAK
TEMPERATURE
CAN
DIFFER
SIGNIFICANTLY
IN
THEIR
EFFECTS
ON
THE
EXPECTED
PROCESSOR
LIFETIME
CATEGORIES
AND
SUBJECT
DESCRIPTORS
C
PERFORMANCE
OF
SYSTEMS
MODELING
TECHNIQUES
GENERAL
TERMS
RELIABILITY
INTRODUCTION
THE
MICROPROCESSOR
INDUSTRY
HAS
MOVED
TO
CHIP
MULTIPROCESSING
TO
ENABLE
THE
SCALING
OF
PERFORMANCE
BEYOND
THE
LIMITS
OF
UNIPRO
CESSOR
EXECUTION
AS
THE
CHIP
AREA
SHRINKS
THE
POWER
DENSITY
GROWS
FOR
NEW
PROCESS
TECHNOLOGIES
CAUSING
HIGHER
TEMPERATURES
THERE
FORE
OUR
SUCCESS
AT
FINDING
WAYS
TO
PROFITABLY
USE
THE
AVAILABLE
TRAN
SISTORS
HAS
A
COST
AS
WE
ARE
NOW
FACED
WITH
SIGNIFICANT
CHALLENGES
IN
MANAGING
THE
POWER
AND
THERMAL
EFFECTS
ON
THESE
CHIPS
HIGH
TEMPERATURES
INCREASE
THE
COST
OF
COOLING
DEGRADE
RELIABILITY
AND
REDUCE
PERFORMANCE
A
NUMBER
OF
MECHANISMS
FOR
THERMAL
CONTROL
ARE
CURRENTLY
AVAIL
ABLE
FOR
MULTICORE
PROCESSORS
INCLUDING
JOB
SCHEDULING
JOB
MI
GRATION
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
ETC
THIS
PAPER
PRESENTS
A
FRAMEWORK
FOR
EVALUATING
THE
EFFECTIVENESS
OF
THESE
TECH
NIQUES
IN
VARIOUS
COMBINATIONS
AND
PRESENTS
EFFECTIVE
NEW
POLICIES
FOR
MANAGING
THERMAL
EFFECTS
THE
PRIMARY
GOAL
OF
MANAGING
TEMPERATURE
IS
TO
PREVENT
PRO
CESSOR
FAILURE
THIS
RESEARCH
IS
FOCUSED
ON
HARD
FAILURES
WHICH
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
SIGMETRICS
PERFORMANCE
JUNE
SEATTLE
WA
USA
COPYRIGHT
ACM
CAUSE
PERMANENT
DAMAGE
TO
THE
UNDERLYING
CIRCUIT
AND
THE
PHYS
ICAL
AND
ELECTRICAL
PHENOMENA
THAT
INDUCE
THEM
SILICON
DEVICES
HAVE
A
NUMBER
OF
FAILURE
MODES
THAT
ARE
IMPACTED
BY
TEMPERATURE
AND
IN
SOME
CASES
THESE
MODES
ARE
AT
ODDS
THERMAL
MANAGEMENT
TECHNIQUES
THAT
REDUCE
THE
RATE
OF
ONE
FAILURE
MODE
MAY
EXACERBATE
ANOTHER
IN
FACT
WE
SHOW
THAT
TECHNIQUES
THAT
ARE
NEARLY
IDENTICAL
IN
PERFORMANCE
POWER
AND
EVEN
PEAK
TEMPERATURE
CAN
DIFFER
BY
A
FACTOR
OF
TWO
IN
EXPECTED
PROCESSOR
LIFETIME
THEREFORE
IT
IS
CRIT
ICAL
THAT
WE
HAVE
A
MODEL
OF
POWER
TEMPERATURE
AND
PARTICULARLY
RELIABILITY
THAT
INCORPORATES
ALL
CRITICAL
FAILURE
MODES
THIS
PAPER
IN
TRODUCES
SUCH
AN
INTEGRATED
MODELING
FRAMEWORK
SHOWS
THAT
SOME
POLICIES
HAVE
UNINTENDED
CONSEQUENCES
WHEN
ALL
SOURCES
OF
FAILURE
ARE
CONSIDERED
AND
PROPOSES
NEW
POLICIES
THAT
PROVIDE
SIGNIFICANT
GAINS
IN
PROCESSOR
LIFETIME
WITH
LITTLE
LOSS
IN
PERFORMANCE
MOST
POWER
AND
THERMAL
MANAGEMENT
TECHNIQUES
HAVE
FOCUSED
PRIMARILY
ON
CONTROLLING
PEAK
TEMPERATURE
ALTHOUGH
SEVERAL
TYPES
OF
FAILURES
CLEARLY
SCALE
WITH
THE
PEAK
TEMPERATURE
THAT
FACTOR
ALONE
DOES
NOT
ACCURATELY
MODEL
ALL
TYPES
OF
FAILURES
OTHER
FAILURES
SUCH
AS
CRACKS
AND
FATIGUE
FAILURES
ARE
CREATED
NOT
BY
SUSTAINED
HIGH
TEMPERATURES
BUT
RATHER
BY
THE
REPEATED
HEATING
AND
COOLING
OF
SEC
TIONS
OF
THE
PROCESSOR
THIS
PHENOMENON
IS
REFERRED
TO
AS
THERMAL
CYCLING
THE
PARTICULAR
FAILURES
THAT
OUR
INFRASTRUCTURE
MODELS
IN
CLUDE
ELECTROMIGRATION
TIME
DEPENDENT
DIELECTRIC
BREAKDOWN
AND
THE
THERMAL
CYCLING
INDUCED
ERRORS
MENTIONED
ABOVE
FAILING
TO
IN
CLUDE
THERMAL
CYCLING
IN
THE
FAILURE
MODEL
CAN
LEAD
TO
MISLEADING
RESULTS
AND
HIGHLY
SUBOPTIMAL
TEMPERATURE
MITIGATION
STRATEGIES
MODELING
THERMAL
CYCLES
IS
DIFFICULT
THE
PRIMARY
CHALLENGE
IS
THE
NEED
TO
ACCURATELY
MODEL
THESE
SYSTEMS
OVER
THE
TIMESCALES
WHICH
THERMAL
CYCLES
OCCUR
THIS
FAR
EXCEEDS
THE
ABILITY
OF
CUR
RENT
PROCESSOR
MODELING
TECHNIQUES
WHICH
TYPICALLY
SIMULATE
SYS
TEM
BEHAVIOR
AT
INSTRUCTION
OR
CYCLE
LEVEL
THEREFORE
WE
INTRO
DUCE
NEW
PERFORMANCE
MODELING
MECHANISMS
INTEGRATED
WITH
OUR
POWER
THERMAL
AND
RELIABILITY
MODELS
THAT
ALLOWS
ACCURATE
MODEL
ING
OF
EXECUTION
BEHAVIOR
OVER
TENS
OR
HUNDREDS
OF
SECONDS
BEING
ABLE
TO
CAPTURE
ALL
THE
THERMAL
FAILURE
EFFECTS
IS
CRITICAL
TO
AN
ACCU
RATE
UNDERSTANDING
OF
PROCESSOR
LIFETIME
WE
SHOW
FOR
EXAMPLE
THAT
SOME
PROPOSED
MECHANISMS
THAT
APPEAR
TO
IMPROVE
RELIABILITY
IF
THERMAL
CYCLING
EFFECTS
ARE
IGNORED
ACTUALLY
HAVE
THE
OPPOSITE
EF
FECT
WHEN
THERMAL
CYCLING
IS
TAKEN
INTO
ACCOUNT
IN
THIS
WORK
WE
DEFINE
SEVERAL
NEW
SCHEDULING
AND
POWER
MAN
AGEMENT
POLICIES
THIS
RESEARCH
SHOWS
THAT
THE
MOST
CRITICAL
FACTORS
FOR
INCREASING
PROCESSOR
LIFETIME
WITH
ACCEPTABLE
PERFORMANCE
ARE
THE
ASYMMETRIC
THERMAL
CHARACTERISTICS
OF
THE
CORES
CORES
IN
THE
CENTER
HAVING
VERY
DIFFERENT
PROPERTIES
THAN
THOSE
ON
THE
EDGES
ETC
THE
FREQUENCY
OF
MIGRATION
WHICH
CAN
BOTH
INHIBIT
SLEEP
STATES
AND
CAUSE
THERMAL
CYCLING
OUR
MOST
EFFECTIVE
POLICY
THAT
EM
PLOYS
VOLTAGE
FREQUENCY
SCALING
AS
WELL
AS
OUR
BEST
ONE
THAT
DOES
NOT
BOTH
ACCOUNT
FOR
THE
LOCATION
ASYMMETRY
AND
REDUCE
THE
NUM
BER
OF
THREAD
MOVEMENTS
WE
PRESENT
NEW
SCHEDULING
POLICIES
THAT
CAN
DECREASE
THE
FAILURE
RATE
BY
A
FACTOR
OF
TWO
OVER
NAIVE
MANAGE
MENT
WITH
A
PERFORMANCE
COST
OF
LESS
THAN
THIS
PAPER
IS
ORGANIZED
AS
FOLLOWS
SECTION
DISCUSSES
RECENT
WORK
IN
THERMAL
AND
RELIABILITY
MANAGEMENT
SECTION
DESCRIBES
THE
INTEGRATED
PERFORMANCE
POWER
THERMAL
AND
RELIABILITY
SIM
ULATION
FRAMEWORK
WE
PROVIDE
THE
DETAILS
OF
THE
EXPERIMENTAL
METHODOLOGY
IN
SECTION
AND
EXPLAIN
THE
THERMAL
MANAGEMENT
TECHNIQUES
IN
SECTION
A
THOROUGH
EVALUATION
OF
THE
TECHNIQUES
IS
PRESENTED
IN
SECTION
AND
WE
CONCLUDE
IN
SECTION
BACKGROUND
AND
RELATED
WORK
IN
THIS
SECTION
WE
PROVIDE
AN
OVERVIEW
OF
PREVIOUSLY
PROPOSED
DYNAMIC
THERMAL
MANAGEMENT
DTM
TECHNIQUES
AND
ALSO
SIM
ULATION
METHODOLOGIES
FOR
MODELING
PERFORMANCE
POWER
AND
RE
LIABILITY
MANY
OF
THE
DTM
TECHNIQUES
ARE
REACTIVE
IN
NATURE
DEPENDING
ON
SENSORS
TO
INDICATE
TEMPERATURES
BEYOND
ASSIGNED
THRESHOLDS
AND
ADAPTING
THE
PROCESSOR
TO
EITHER
REDUCE
OR
MIGRATE
ACTIVITY
TO
BRING
THE
TEMPERATURE
DOWN
BROOKS
AND
MARTONOSI
INTRODUCED
THE
CONCEPT
OF
DYNAMIC
THERMAL
MANAGEMENT
IN
REACTION
TO
THERMAL
MEASUREMENTS
SOME
THERMAL
MANAGEMENT
TECHNIQUES
STALL
EXECUTION
OR
MIGRATE
COMPU
TATION
TO
OTHER
UNITS
TO
CONTROL
TEMPERATURE
A
WELL
KNOWN
EXAMPLE
OF
SUCH
TECHNIQUES
IS
CLOCK
GATING
WHICH
FREEZES
ALL
DYNAMIC
OP
ERATIONS
UNTIL
THE
THERMAL
EMERGENCY
IS
OVER
CAUSING
TYPICALLY
SIG
NIFICANT
PERFORMANCE
COST
CLOCK
GATING
IS
IMPLEMENTED
IN
INTEL
PENTIUM
DONALD
ET
AL
PROPOSE
A
DYNAMIC
THERMAL
MANAGEMENT
TECH
NIQUE
FOR
SIMULTANEOUS
MULTITHREADING
SMT
ARCHITECTURES
THEIR
TECHNIQUE
SELECTIVELY
MANAGES
THE
EXECUTION
OF
INTEGER
OR
FLOATING
POINT
INTENSIVE
THREADS
TO
PREVENT
HOT
SPOTS
IN
THE
REGISTER
FILES
TO
IDENTIFY
INTEGER
OR
FLOATING
POINT
INTENSIVE
THREADS
HARDWARE
EVENT
COUNTERS
ARE
SAMPLED
DURING
EXECUTION
IN
ACTIVITY
MIGRATION
THE
HEAT
IS
SPREAD
BY
MOVING
COMPUTATION
TO
A
DIFFERENT
LOCATION
ON
THE
DIE
MIGRATION
CAN
HAPPEN
AT
MULTIPLE
LEVELS
FROM
ONE
CORE
TO
ANOTHER
OR
WITHIN
A
CORE
EXISTING
REDUNDANCY
IN
A
SUPERSCALAR
PIPELINE
CAN
BE
UTILIZED
FOR
CONTROLLING
TEMPERATURE
IN
THIS
TECHNIQUE
THE
POWER
DENSITY
IS
CONTROLLED
BY
BALANCING
THE
UTILIZATION
OF
ISSUE
QUEUES
REGIS
TER
FILES
AND
ALUS
FETCH
GATING
ALTERNATES
BETWEEN
FETCHING
AND
STALLING
FETCH
IN
ORDER
TO
REDUCE
THE
ACTIVITY
AND
POWER
DENSITY
IN
THE
PIPELINE
SKADRON
ET
AL
INTRODUCE
A
FEEDBACK
CONTROL
LOOP
TO
CONTROL
THE
DUTY
CYCLE
FOR
FETCH
GATING
ANOTHER
CLASS
OF
THERMAL
MANAGEMENT
TECHNIQUES
USE
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
DVFS
WHERE
THE
SYSTEM
IS
ABLE
TO
ALTER
THE
PROCESSOR
FREQUENCY
AND
SUPPLY
VOLTAGE
DYNAMICALLY
TO
RESPOND
TO
THERMAL
EMERGENCIES
DVFS
CAN
USE
DIFFERENT
NUM
BER
OF
STEPS
FOR
THE
GLOBAL
VOLTAGE
AND
FREQUENCY
SETTINGS
RANGING
FROM
TWO
IN
INTEL
SPEEDSTEP
TECHNOLOGY
TO
FOR
THE
INTEL
XSCALE
IN
HYBRID
THERMAL
MANAGEMENT
FOR
MILD
LEVELS
OF
THERMAL
STRESS
FETCH
GATING
WHERE
WE
STALL
FETCH
BUT
ALLOW
OTHER
STAGES
OF
THE
PIPELINE
TO
PROCEED
WITH
PREVIOUSLY
FETCHED
INSTRUCTIONS
IS
USED
AS
THE
RESPONSE
MECHANISM
WHEN
THE
OVERHEAD
OF
THE
FETCH
GAT
ING
INCREASES
AND
INSTRUCTION
LEVEL
PARALLELISM
ILP
CANNOT
SUFFI
CIENTLY
HIDE
THE
ILL
PERFORMANCE
EFFECT
DTM
SWITCHES
TO
DVFS
ANOTHER
HYBRID
DTM
TECHNIQUE
MINIMIZES
THE
PERFORMANCE
IM
PACT
BY
PROACTIVE
USE
OF
SOFTWARE
TECHNIQUES
LIKE
THERMAL
AWARE
PROCESS
SCHEDULING
COMBINED
WITH
REACTIVE
USE
OF
HARDWARE
TECH
NIQUES
SUCH
AS
CLOCK
GATING
DONALD
ET
AL
EVALUATE
VARIOUS
COMBINATIONS
OF
DVFS
CLOCK
GATING
I
E
STOP
GO
AND
MIGRATION
FOR
MANAGING
THE
TEMPERATURE
OF
MULTICORE
PROCESSORS
THEY
SHOW
THAT
DISTRIBUTED
DVFS
COMBINED
WITH
THREAD
MIGRATION
PRO
VIDES
THE
BEST
PERFORMANCE
AMONG
DIFFERENT
ALTERNATIVES
PRIOR
WORK
HAS
ALSO
INVESTIGATED
LOW
OVERHEAD
TEMPERATURE
AWARE
TASK
SCHEDULING
AT
THE
OPERATING
SYSTEM
LEVEL
FOR
MULTIPROCESSOR
SYSTEM
ON
CHIPS
MPSOC
AND
THE
AUTHORS
PROPOSED
AN
ADAP
TIVE
PROBABILISTIC
POLICY
ADDRESSING
BOTH
TEMPERATURE
VARIATIONS
AND
HOT
SPOTS
THE
ADAPTIVE
SCHEDULING
TECHNIQUE
IS
COMBINED
WITH
THREAD
MIGRATION
OR
DVFS
TO
FURTHER
IMPROVE
THERMAL
BEHAVIOR
MURALI
ET
AL
PROPOSE
A
TECHNIQUE
THAT
ASSIGNS
FREQUENCIES
TO
DIFFERENT
CORES
IN
AN
MPSOC
TO
GUARANTEE
MEETING
THE
THERMAL
CON
STRAINTS
IN
THE
OFFLINE
PHASE
THE
SET
OF
FEASIBLE
FREQUENCIES
FOR
DIFFERENT
TEMPERATURE
AND
WORKLOAD
CONSTRAINTS
ARE
CALCULATED
BY
SOLVING
CONVEX
OPTIMIZATION
MODELS
AT
RUN
TIME
THE
MANAGEMENT
POLICY
SELECTS
THE
APPROPRIATE
FREQUENCY
VALUES
THAT
MEET
THE
CUR
RENT
WORKLOADS
AND
OPERATING
CONDITIONS
IN
HEAT
AND
RUN
THE
AUTHORS
PROPOSE
A
DTM
SOLUTION
FOR
CHIP
MULTIPROCESSORS
WITH
SMT
CORES
SMT
THREAD
ASSIGNMENT
IS
USED
TO
MAXIMIZE
PROCESSOR
RESOURCE
UTILIZATION
BY
CO
SCHEDULING
THREADS
THAT
USE
COMPLEMENTARY
RESOURCES
BEFORE
COOLING
IS
NECES
SARY
IN
THIS
WAY
THE
COST
OF
THREAD
MIGRATION
IS
REDUCED
FEW
PAPERS
IN
THE
THERMAL
MANAGEMENT
LITERATURE
HAVE
TAKEN
RE
LIABILITY
EXPLICITLY
INTO
ACCOUNT
RELIABILITY
MANAGEMENT
HAS
BEEN
MOSTLY
ADDRESSED
PREVIOUSLY
AS
A
WAY
OF
OPTIMIZING
THE
POLICIES
OR
ARCHITECTURE
AT
DESIGN
TIME
THE
RELIABILITY
AWARE
MICROPROCESSOR
RAMP
PROVIDES
A
RELIABILITY
MODEL
AT
THE
ARCHITECTURE
LEVEL
FOR
TEMPERATURE
RELATED
INTRINSIC
HARD
FAILURES
IT
ANALYZES
THE
EF
FECTS
OF
APPLICATION
BEHAVIOR
ON
RELIABILITY
AND
OPTIMIZES
THE
ARCHI
TECTURAL
CONFIGURATION
AND
THE
VOLTAGE
FREQUENCY
SETTING
STATICALLY
AT
DESIGN
TIME
TO
MEET
THE
RELIABILITY
TARGET
PREVIOUS
WORK
ALSO
SHOWS
THAT
AGGRESSIVE
POWER
MANAGEMENT
CAN
ADVERSELY
AFFECT
RE
LIABILITY
DUE
TO
FAST
THERMAL
CYCLES
AND
OPTIMIZATION
METHODS
THAT
CONSIDER
RELIABILITY
CONSTRAINTS
CAN
PROVIDE
ENERGY
SAVINGS
WHILE
IMPROVING
THE
MPSOC
LIFETIME
TO
THE
BEST
OF
OUR
KNOWLEDGE
A
SIMULATION
FRAMEWORK
TO
EVALU
ATE
THE
RELIABILITY
IMPACT
OF
DYNAMIC
MANAGEMENT
POLICIES
IN
A
FAST
AND
ACCURATE
WAY
HAS
NOT
BEEN
INTRODUCED
PREVIOUSLY
THE
SIM
POINT
TOOL
ALSO
ADDRESSES
THE
PROBLEM
OF
LONG
SIMULATION
TIMES
BUT
IT
PROVIDES
CLUSTERING
ANALYSIS
TO
IDENTIFY
A
FEW
REPRESENTATIVE
POINTS
THAT
CAN
BE
SIMULATED
TO
PREDICT
THE
PERFORMANCE
OF
THE
EN
TIRE
APPLICATION
INSTEAD
WE
WANT
TO
CAPTURE
THE
ENTIRE
BEHAVIOR
RATHER
THAN
SUMMARIZE
HOWEVER
WE
USE
SIMPOINT
PHASE
IDEN
TIFICATION
MECHANISM
TO
CAPTURE
A
COMPLETE
PHASE
TRACE
AS
PART
OF
OUR
SIMULATION
PROCESS
BIESBROUCK
ET
AL
USE
INDIVIDUAL
PRO
GRAM
PHASE
INFORMATION
A
COMPLETE
PHASE
TRACE
NOT
UNLIKE
OURS
TO
GUIDE
MULTITHREADED
SIMULATION
THIS
IS
ACCOMPLISHED
BY
CREATING
A
CO
PHASE
MATRIX
WHICH
REPRESENTS
THE
PER
THREAD
PERFORMANCE
FOR
EACH
POTENTIAL
COMBINATION
OF
THE
SINGLE
THREADED
PHASE
BEHAV
IORS
THAT
OCCUR
WHEN
MULTIPLE
PROGRAMS
ARE
RUN
TOGETHER
ALTHOUGH
RAMP
ALSO
INTEGRATES
AN
ARCHITECTURE
LEVEL
PERFORMANCE
SIM
ULATOR
WITH
A
POWER
MODEL
AND
A
THERMAL
SIMULATOR
IT
DOES
NOT
IN
CLUDE
THE
PHASE
BASED
APPROACH
WE
INTRODUCED
USING
OUR
FRAME
WORK
WE
ARE
ABLE
TO
SIMULATE
MUCH
LONGER
PERIODS
OF
REAL
LIFE
EXE
CUTION
IN
REASONABLE
SIMULATION
TIME
A
NOVEL
FRAMEWORK
FOR
MULTI
CORE
RELIABILITY
MODELING
THIS
AREA
OF
RESEARCH
PRESENTS
NEW
METHODOLOGICAL
CHALLENGES
THAT
REQUIRE
TOOLS
AND
SOLUTIONS
RADICALLY
DIFFERENT
THAN
TRADITIONAL
ARCHITECTURAL
INVESTIGATION
THIS
SECTION
DESCRIBES
THE
ENTIRE
SIMU
LATION
INFRASTRUCTURE
BUT
WITH
A
FOCUS
ON
THE
TWO
MOST
NOVEL
ASPECTS
OF
THE
FRAMEWORK
WHICH
ARE
THE
LONG
TIME
FRAME
PERFORMANCE
MOD
ELING
AND
THE
INTEGRATED
RELIABILITY
MODEL
OVERVIEW
FOR
A
STUDY
SUCH
AS
THIS
ONE
IT
IS
CRITICAL
THAT
WE
HAVE
A
FULLY
FIGURE
DESIGN
FLOW
INTEGRATED
PERFORMANCE
POWER
AND
THERMAL
MODEL
OF
THE
ENTIRE
CHIP
MULTIPROCESSOR
THIS
IS
BECAUSE
WE
ARE
MODELING
INTERACTIVE
SCHEDULING
TECHNIQUES
THAT
OBSERVE
THE
TEMPERATURE
AND
POSSIBLY
POWER
CHARACTERISTICS
OF
THE
PROCESSOR
AND
MAKE
SCHEDULING
DECI
SIONS
ACCORDINGLY
FOR
EXAMPLE
IT
IS
IMPOSSIBLE
TO
COMPLETELY
DE
COUPLE
THE
PERFORMANCE
AND
THE
THERMAL
MODELS
HOWEVER
FULL
IN
TERACTIVE
ARCHITECTURE
LEVEL
SIMULATION
IS
ALSO
NOT
POSSIBLE
AS
JUST
A
SINGLE
SIMULATION
AT
THESE
TIME
FRAMES
CORRESPONDING
TO
SEVERAL
MINUTES
OF
REAL
EXECUTION
TIME
COULD
REQUIRE
MONTHS
TO
COMPLETE
OUR
SIMULATION
FRAMEWORK
IS
SHOWN
IN
FIGURE
THE
PERFOR
MANCE
MODELING
FRONT
END
COMBINES
A
FULL
PROGRAM
PHASE
PROFILE
COMBINED
WITH
DETAILED
ARCHITECTURE
LEVEL
SIMULATION
OF
EVERY
DIS
TINCT
PROGRAM
PHASE
AT
ALL
POSSIBLE
FREQUENCY
SETTINGS
INCLUDING
BOTH
PERFORMANCE
AND
POWER
CHARACTERISTICS
THIS
CHARACTERIZA
TION
ALL
GOES
INTO
A
DATABASE
THAT
CAN
BE
QUERIED
AS
THE
FULL
CMP
SIMULATION
PROGRESSES
IN
THIS
WAY
WE
CAN
MODEL
THE
EFFECTS
OF
CHANGING
FREQUENCY
STOPPING
OR
MIGRATING
JOBS
ETC
WITHOUT
FUR
THER
ARCHITECTURE
LEVEL
SIMULATION
AFTER
SCHEDULING
DECISIONS
ARE
MADE
AND
THE
RESULTING
PERFORMANCE
AND
POWER
DATA
PRODUCED
WE
CAN
MODEL
TIME
VARYING
TEMPERATURE
EFFECTS
ACROSS
THE
ENTIRE
CHIP
THE
TEMPERATURE
CURVES
ARE
THEN
FED
INTO
THE
RELIABILITY
MODELS
PRO
DUCING
THE
EXPECTED
FAILURE
RATES
LONG
TERM
PERFORMANCE
MODELING
TO
ACCURATELY
MODEL
THE
RELIABILITY
OF
THE
SYSTEM
INCLUDING
THER
MAL
CYCLING
EFFECTS
WE
NEED
TO
CAPTURE
TEMPORAL
THERMAL
BEHAVIOR
OVER
TIME
PERIODS
ORDERS
OF
MAGNITUDE
LONGER
THAN
TYPICALLY
MOD
ELED
IN
ARCHITECTURAL
SIMULATION
AT
THE
SAME
TIME
WE
WOULD
LIKE
TO
CAPTURE
VARIOUS
TYPES
OF
EFFECTS
THAT
THE
ARCHITECTURAL
SIMULA
TION
PROVIDES
E
G
WORKLOAD
DEPENDENT
UTILIZATION
OF
SPECIFIC
AR
CHITECTURAL
STRUCTURES
AND
THEIR
IMPACT
ON
POWER
AND
TEMPERATURE
THE
TIME
VARYING
BEHAVIOR
OF
INDIVIDUAL
APPLICATIONS
ETC
THIS
RE
QUIRES
THE
DEVELOPMENT
OF
NEW
SIMULATION
TOOLS
AND
METHODOLOGIES
NOT
CURRENTLY
AVAILABLE
WE
INITIALLY
USE
SIMPOINT
TO
CAPTURE
THE
PHASES
OF
EACH
APPLICATION
BUT
INSTEAD
OF
CAPTURING
ONE
OR
A
FEW
REPRESENTATIVE
PHASES
WE
USE
IT
TO
CAPTURE
A
COMPLETE
PHASE
PROFILE
OF
EACH
AP
PLICATION
BEGINNING
TO
END
THEN
USING
THE
PERFORMANCE
SIM
ULATOR
INTEGRATED
WITH
THE
WATTCH
POWER
MODELING
TOOL
AND
UTILIZING
A
FINITE
NUMBER
OF
SIMULATION
SAMPLES
FOR
EACH
PHASE
WE
CAN
RECONSTRUCT
THE
POWER
AND
EXECUTION
PROPERTIES
OF
THE
COMPLETE
PROGRAM
IN
FACT
WE
DO
THIS
FOR
ALL
VOLTAGE
AND
FREQUENCY
SETTINGS
AVAILABLE
SO
THAT
WE
CAN
RECONSTRUCT
THE
COMPLETE
PROGRAM
EVEN
IN
THE
FACE
OF
AN
ARBITRARY
NUMBER
OF
VOLTAGE
FREQUENCY
CHANGES
WE
CAPTURE
THESE
PROGRAM
TRACES
IN
A
DATABASE
WHICH
CAN
BE
QUERIED
BY
THE
SCHEDULE
MANAGER
AT
DISTINCT
INTERVALS
GIVEN
A
PROGRAM
START
POINT
AN
INTERVAL
LENGTH
IN
CYCLES
AND
A
FREQUENCY
SETTING
THE
QUERY
TOOL
RETURNS
THE
AVERAGE
INSTRUCTIONS
PER
SECOND
IPC
AND
POWER
LEVELS
ACROSS
THE
INTERVAL
AND
THE
POINT
IN
EXECU
TION
THE
PROGRAM
REACHES
AT
THE
END
OF
THE
INTERVAL
THUS
AT
RUNTIME
THE
SCHEDULING
MANAGER
CAN
MAKE
DECISIONS
ABOUT
THREAD
MIGRA
TION
THREAD
STOPPAGE
OR
VOLTAGE
FREQUENCY
CHANGES
AND
QUERY
THE
DATABASE
TO
MODEL
THE
PRECISE
EFFECTS
THIS
FRAMEWORK
RELIES
ON
TWO
SIMPLIFYING
ASSUMPTIONS
THAT
ARE
CRITICAL
TO
MAKING
THIS
PROBLEM
TRACTABLE
THE
FIRST
IS
THAT
THE
TIME
CONSTANTS
OVER
WHICH
TEMPERATURE
VARIES
DO
NOT
REQUIRE
US
TO
FULLY
CAPTURE
CYCLE
BY
CYCLE
VARIANCES
IN
THE
TEMPERATURE
PORTION
OF
THE
MODEL
THE
INSTRUCTION
LEVEL
VARIATIONS
ARE
CAPTURED
IN
THE
PERFOR
MANCE
MODEL
BUT
ONLY
SUMMARIZED
IN
THE
LATTER
STAGES
THIS
ALLOWS
US
TO
REPLACE
THE
CYCLE
BY
CYCLE
DATA
WITH
A
STAIR
STEP
GRAPH
PRE
SENTING
PERFORMANCE
AND
POWER
BEHAVIOR
AS
CONSTANT
AT
THE
AVERAGE
VALUES
OVER
INDIVIDUAL
INTERVALS
THIS
WAY
WE
CAPTURE
THE
PROGRAM
BEHAVIOR
WITH
LITTLE
LOSS
OF
ACCURACY
THE
SECOND
ASSUMPTION
IS
THAT
THE
BEHAVIOR
OF
INDIVIDUAL
THREADS
IS
SEPARABLE
THIS
IS
ACCURATE
BECAUSE
WE
MODEL
SYSTEMS
WITH
PRI
VATE
CACHES
WHICH
IS
A
LIKELY
ARCHITECTURAL
SCENARIO
IN
FUTURE
SYSTEMS
AT
CORES
AND
ABOVE
THE
INTERCONNECT
COST
OF
A
SHARED
CACHE
WOULD
BE
EXTREMELY
HIGH
THIS
ASSUMPTION
HAS
BEEN
USED
AND
DEMONSTRATED
TO
BE
ACCURATE
EVEN
ON
RESEARCH
THAT
DOES
NOT
REQUIRE
THIS
TYPE
OF
LONG
SIMULATION
EVEN
WITH
THE
SMALL
CORE
COUNTS
IN
CURRENT
MULTICORES
THE
AMD
DUAL
CORE
AND
QUAD
CORE
OPTERON
THE
IBM
AND
THE
COMING
INTEL
NEHALEM
PRO
CESSOR
ALL
HAVE
PRIVATE
AND
CACHES
FOR
SHARED
CACHES
INTERACTIONS
BETWEEN
THREADS
WILL
BE
HIGHER
AND
SYSTEM
LEVEL
AC
CURACY
WILL
BE
REDUCED
HOWEVER
RECENT
RESEARCH
ON
MULTICORE
CACHING
HAS
FOCUSED
ON
REDUCING
THOSE
INTERACTIONS
AND
IN
THE
EXTREME
THE
PROPOSED
TECHNIQUES
COULD
BE
CONFIGURED
TO
MAKE
THE
SHARED
CACHES
ESSENTIALLY
ACT
AS
PRIVATE
CACHES
THUS
EVEN
IN
THAT
SCENARIO
WE
CAN
REPRESENT
A
REASONABLE
SYSTEM
ACCURATELY
THIS
AS
SUMPTION
MAKES
IT
DIFFICULT
TO
MODEL
PARALLEL
APPLICATIONS
WITH
ANY
SIGNIFICANT
COMMUNICATION
BETWEEN
THREADS
IF
THAT
COMMUNICATION
IMPACTS
THE
RUNTIME
CHARACTERISTICS
OF
THE
APPLICATION
PHASE
MODELING
WE
USED
SIMPOINT
TO
IDENTIFY
THE
VARIOUS
PHASES
WITHIN
THE
APPLICATIONS
AND
TO
CHARACTERIZE
COMPLETE
PROGRAM
EXECUTION
A
PROGRAM
EXECUTION
IS
DIVIDED
INTO
INTERVALS
OF
MILLION
INSTRUC
TIONS
ONCE
WE
ASSIGN
EACH
INTERVAL
TO
A
REPRESENTATIVE
PHASE
WE
REPRESENT
A
PROGRAM
EXECUTION
BY
A
PHASE
ID
TRACE
THUS
AT
ANY
INSTRUCTION
DURING
A
PROGRAM
EXECUTION
WE
USE
THIS
FILE
TO
DETERMINE
THE
CURRENT
PHASE
AND
TO
IDENTIFY
POINTS
OF
TRANSITION
BE
TWEEN
PHASES
BY
RUNNING
SIMULATIONS
AT
EACH
PHASE
POINT
IN
AND
COMPOS
ING
PERFORMANCE
POWER
STATISTICS
WITH
THE
PHASE
ID
TRACE
WE
CRE
ATE
BOTH
A
POWER
AND
A
PERFORMANCE
TRACE
THE
SCHEDULING
MANAGER
THEN
ACCESSES
THESE
TRACES
VIA
THE
QUERY
TOOL
POWER
MODELING
AND
MANAGEMENT
POWER
MODELING
REQUIRES
COUPLING
THE
EXECUTION
TRACES
OBTAINED
FROM
WITH
A
TOOL
THAT
COMPUTES
THE
POWER
CONSUMPTION
FOR
EACH
FUNCTIONAL
UNIT
THIS
COUPLING
CONVERTS
THE
PERFORMANCE
PARAME
TERS
E
G
CACHE
ACCESSES
BRANCH
PREDICTIONS
ETC
INTO
ESTIMATES
FOR
TRANSISTOR
SWITCHING
AND
THEN
THE
POWER
MODEL
UTILIZES
THESE
ESTIMATES
FOR
CALCULATING
THE
INSTANTANEOUS
POWER
VALUES
TRANSISTORS
CONSUME
POWER
WHEN
THEY
SWITCH
OUTPUT
VALUES
BUT
THEY
ALSO
LEAK
POWER
EVEN
WHEN
THEY
DO
NOT
SWITCH
THE
FORMER
IS
REFERRED
TO
AS
DYNAMIC
POWER
AND
HISTORICALLY
HAS
BEEN
THE
DOM
INANT
FACTOR
HOWEVER
AS
TECHNOLOGY
SHRINKS
LEAKAGE
POWER
BE
COMES
INCREASINGLY
IMPORTANT
WE
UTILIZED
WATTCH
FOR
THE
DY
NAMIC
POWER
MODELING
OF
CORES
IN
OUR
FRAMEWORK
WE
INTEGRATED
WATTCH
WITH
TO
PROVIDE
DYNAMIC
AND
CYCLE
ACCURATE
POWER
MEA
SUREMENTS
FOR
EACH
APPLICATION
TO
MODEL
POWER
DISSIPATION
OF
CACHES
WE
USED
CACTI
AN
INTEGRATED
MEMORY
PERFORMANCE
AREA
LEAKAGE
AND
DYNAMIC
POWER
MODEL
AND
OBTAINED
THE
TYPI
CAL
POWER
CONSUMPTION
OF
A
MEMORY
BLOCK
WITH
THE
GIVEN
SIZE
AND
PROPERTIES
AND
THEN
USED
THESE
VALUES
THROUGHOUT
THE
SIMULATION
WE
DEVELOPED
A
POWER
MODEL
FOR
BY
SCALING
THE
PARAMETERS
WITHIN
WATTCH
TO
MATCH
PUBLISHED
POWER
VALUES
FOR
TECHNOL
OGY
THE
VARIATION
IN
DYNAMIC
POWER
RANGE
WE
OBSERVED
MATCHES
THE
POWER
DISTRIBUTION
ON
A
SIMILAR
CORE
ON
WHICH
THE
MAJOR
ITY
OF
APPLICATIONS
HAD
LESS
THAN
POWER
DISSIPATION
DIFFERENCE
FROM
THE
OTHER
APPLICATIONS
AMONG
THE
APPLICATIONS
SAMPLED
IN
THAT
DISTRIBUTION
WERE
THE
SPEC
SUITE
WHICH
WE
USE
IN
THIS
STUDY
WE
COMPUTE
THE
LEAKAGE
POWER
OF
CPU
CORES
BASED
ON
STRUC
TURE
AREAS
TEMPERATURE
AND
SUPPLY
VOLTAGE
FOR
THE
PROCESS
TECHNOLOGY
WE
ASSUME
A
LEAKAGE
POWER
DENSITY
OF
AT
TO
ACCOUNT
FOR
THE
TEMPERATURE
AND
VOLTAGE
WE
USED
THE
SECOND
ORDER
POLYNOMIAL
MODEL
PROPOSED
BY
SU
ET
AL
THIS
MODEL
COMPUTES
THE
CHANGE
IN
LEAKAGE
POWER
FOR
THE
GIVEN
DIFFER
ENTIAL
TEMPERATURE
AND
VOLTAGE
VALUES
WE
DETERMINED
THE
COEFFI
CIENTS
IN
THE
POLYNOMIAL
MODEL
EMPIRICALLY
TO
MATCH
THE
NORMALIZED
LEAKAGE
VALUES
IN
THE
PAPER
THIS
MODEL
IS
FOUND
TO
MATCH
CLOSELY
WITH
MEASUREMENTS
AND
WE
FOUND
THE
LEAKAGE
VALUES
PRODUCED
WERE
IN
LINE
WITH
EXPECTED
VALUES
I
E
OF
THE
TOTAL
POWER
CONSUMPTION
BASED
ON
THE
TECHNOLOGY
ONE
OF
THE
TECHNIQUES
WE
INVESTIGATE
TO
MANAGE
POWER
IS
DY
NAMIC
POWER
MANAGEMENT
DPM
DPM
PUTS
CORES
IN
SLEEP
STATE
TO
SAVE
ENERGY
WE
IMPLEMENTED
A
FIXED
TIMEOUT
POLICY
WHICH
IS
ONE
OF
THE
COMMONLY
USED
DPM
POLICIES
FOR
EACH
CORE
THE
POL
ICY
WAITS
FOR
A
TIMEOUT
PERIOD
WHEN
THE
CORE
IS
IDLE
AND
THEN
TURNS
OFF
THE
CORE
THIS
IS
TO
ENSURE
THAT
WE
DO
NOT
TURN
OFF
CORES
FOR
VERY
SHORT
IDLE
TIMES
WHERE
TURNING
OFF
THE
CORE
WOULD
NOT
AMORTIZE
THE
COST
OF
TRANSITIONING
TO
AND
FROM
THE
SLEEP
STATE
THE
TIME
PERIOD
TO
AMORTIZE
THE
COST
OF
GOING
TO
SLEEP
IS
CALLED
THE
BREAKEVEN
TIME
TBE
WE
ASSUME
A
SLEEP
STATE
POWER
VALUE
OF
AND
BASED
ON
THE
ACTIVE
AND
IDLE
POWER
DISSIPATION
VALUES
WE
COMPUTED
THE
TBE
TO
BE
AROUND
A
SIMPLE
AND
EFFECTIVE
WAY
TO
SET
THE
TIMEOUT
PERIOD
IS
TTIMEOUT
TBE
THREAD
MANAGEMENT
AND
THERMAL
MOD
ELING
WE
IMPLEMENTED
A
SCHEDULING
MANAGER
WHICH
ENABLES
THE
SIMU
LATION
OF
A
LARGE
ARRAY
OF
THREAD
MANAGEMENT
POLICIES
THE
MECH
ANISMS
AVAILABLE
FOR
MANAGING
TEMPERATURE
INCLUDE
ADJUSTING
THE
FREQUENCY
VOLTAGE
OF
A
CORE
DVFS
PUTTING
AN
IDLE
CORE
INTO
A
LOW
POWER
SLEEP
MODE
DPM
MIGRATING
COMPUTATION
OFF
OF
A
HOT
CORE
AND
POLICIES
THAT
STOP
ACTIVITY
ON
A
HOT
CORE
I
E
CLOCK
OR
FETCH
GATING
WE
PRESENT
THE
SPECIFIC
POLICIES
WE
MODEL
IN
SECTION
IN
EACH
POLICY
THE
SCHEDULING
MANAGER
MAKES
A
SET
OF
DECISIONS
AFTER
EACH
SCHEDULING
INTERVAL
AND
IT
MAY
INCORPORATE
PERFORMANCE
AND
THERMAL
INFORMATION
FROM
THE
PRIOR
INTERVAL
AFTER
MAKING
THOSE
DECISIONS
FOR
EACH
THREAD
AND
CORE
THE
SCHEDULING
MANAGER
THEN
QUERIES
THE
PERFORMANCE
DATABASE
TO
OBTAIN
THE
POWER
AND
PERFOR
MANCE
BEHAVIOR
OF
EACH
CORE
OVER
THE
NEXT
INTERVAL
OUR
SIMULATION
SAMPLING
INTERVALS
MS
ARE
SHORTER
THAN
A
SCHEDULING
INTERVAL
SO
THERE
WOULD
BE
MULTIPLE
EXCHANGES
WITH
THE
PERFORMANCE
DATABASE
BEFORE
ANOTHER
SCHEDULING
DECISION
IS
MADE
SINCE
THE
SCHEDULING
MANAGER
KEEPS
TRACK
OF
PERFORMANCE
AND
POWER
INFORMATION
IT
ALSO
HAS
THE
RESPONSIBILITY
OF
MODELING
COM
PLEX
PHENOMENA
SUCH
AS
THE
DELAY
FROM
THREAD
MIGRATIONS
THE
MODEL
SIMULATES
THE
EFFECTS
ON
POWER
AND
PERFORMANCE
FOR
THE
FOL
LOWING
PHENOMENA
THREAD
MIGRATION
DVFS
STARTING
A
NEW
APPLI
CATION
ON
A
CORE
CORE
SLEEP
AND
CORE
WAKEUP
THE
ASSUMPTIONS
WE
MADE
FOR
SEVERAL
OF
THE
DELAYS
MODELED
ARE
PRESENTED
IN
TABLE
BUT
ONE
OF
THE
MORE
COMPLEX
PHENOMENA
DESERVES
SPECIAL
ATTENTION
WE
MODELED
TWO
ASPECTS
OF
THE
COST
OF
THREAD
MIGRATION
AMONG
CORES
WE
MEASURED
THE
SOFTWARE
OVERHEAD
IN
FULL
SYSTEM
MODE
AS
THE
TIME
FOR
LINUX
TO
MIGRATE
A
THREAD
FROM
ONE
CORE
TO
AN
OTHER
IDLE
CORE
AND
TO
START
EXECUTION
THIS
THREAD
MIGRATION
TAKES
TABLE
DELAY
AND
POWER
MODEL
ASSUMPTIONS
TABLE
HOTSPOT
PARAMETERS
LESS
THAN
ΜS
WE
ALSO
ATTRIBUTED
ARCHITECTURE
OVERHEAD
TO
COLD
START
EFFECTS
IN
THE
BRANCH
PREDICTOR
CACHES
TLBS
ETC
WE
MEA
SURED
COLD
START
EFFECTS
BY
INDUCING
MANY
RANDOM
MIGRATIONS
FOR
EACH
BENCHMARK
AND
COMPUTING
THE
AVERAGE
LOSS
IN
PERFORMANCE
THE
AVERAGE
LOSS
WAS
ΜS
BUT
VARIED
WILDLY
BY
BENCHMARK
I
E
FROM
TO
ΜS
NOTE
THAT
COLD
START
EFFECTS
DOMINATE
THE
MIGRATION
PENALTY
TO
ADDRESS
THE
HIGHLY
VARIABLE
OVERHEAD
WE
MODELED
A
DISTINCT
MIGRATION
PENALTY
FOR
EACH
BENCHMARK
AUTOMATED
THERMAL
MODELING
REQUIRES
POWER
TRACES
FOR
EACH
UNIT
AS
INPUT
IN
ADDITION
TO
THE
CHIP
AND
PACKAGE
CHARACTERISTICS
SUCH
AS
DIE
THICKNESS
HEAT
SINK
CONVECTION
PROPERTIES
ETC
THEREFORE
WE
FEED
THE
DETAILED
POWER
TRACE
DERIVED
BY
THE
COMBINATION
OF
THE
SCHEDULING
MANAGER
AND
THE
PERFORMANCE
POWER
DATABASE
INTO
THE
THERMAL
MODEL
WE
MODIFIED
HOTSPOT
VERSION
BLOCK
MODEL
SETTINGS
TO
MODEL
THE
THERMAL
CHARACTERISTICS
OF
THE
CORE
DIE
WE
USED
THE
STEADY
STATE
TEMPERATURE
OF
EACH
UNIT
AS
THE
INI
TIAL
TEMPERATURE
VALUES
WE
SUMMARIZE
THE
HOTSPOT
PARAMETERS
IN
TABLE
WE
CALCULATED
THE
DIE
CHARACTERISTICS
BASED
ON
THE
TRENDS
REPORTED
FOR
PROCESS
TECHNOLOGY
THE
DESCRIBED
METHODOLOGY
ALLOWS
US
TO
DO
FULL
PROGRAM
SIMU
LATION
WITH
SIMPLE
LOOKUPS
OF
SAMPLED
SIMULATION
DATA
THIS
SAC
RIFICES
SOME
ACCURACY
HOWEVER
THE
RATE
AT
WHICH
TEMPERATURE
CHANGES
TYPICALLY
DWARFS
THE
TIME
OF
EVEN
COMPLETE
PHASES
SO
WE
WOULD
EXPECT
THIS
TECHNIQUE
TO
ACTUALLY
SACRIFICE
LITTLE
ACCURACY
WE
VALIDATED
OUR
METHODOLOGY
BY
COMPARING
THE
RESULTS
WITH
DI
RECT
WATTCH
POWER
OUTPUT
FOR
EACH
PHASE
SIMULATION
POINT
OF
EACH
SPEC
BENCHMARK
WE
RAN
AND
WATTCH
FOR
OF
SIM
ULATED
EXECUTION
AND
GATHERED
POWER
STATISTICS
EVERY
WE
COMPARED
THE
POWER
STATISTICS
OF
WATTCH
AND
OUR
FRAMEWORK
AND
WE
FOUND
THAT
OUR
PHASE
BASED
APPROACH
HAS
ERROR
OVER
ALL
TABLE
SHOWS
THE
DETAILED
RESULTS
FOR
THE
BENCHMARKS
WITH
INPUT
SET
PROGRAM
HAD
THE
LARGEST
AVERAGE
ERROR
OF
THE
LOW
ERROR
MARGIN
IN
OUR
POWER
COMPUTATION
METHODOLOGY
TRANSLATES
TO
EVEN
LOWER
ERROR
IN
TEMPERATURE
COMPUTATION
BECAUSE
OF
THE
THERMAL
TIME
CONSTANTS
TO
VERIFY
THE
ACCURACY
OF
OUR
METHOD
OLOGY
IN
TERMS
OF
THE
TEMPERATURE
RESPONSE
WE
EXPERIMENTED
WITH
AS
IT
HAS
THE
HIGHEST
POWER
ERROR
MARGIN
FIGURE
SHOWS
ONE
PARTICULAR
WORST
CASE
DATA
POINT
THE
TEMPERATURE
TRACE
FOR
A
CORE
RUNNING
AND
THEN
GOING
TO
SLEEP
STATE
ON
A
SYSTEM
RUNNING
THREADS
THE
WATTCH
THERMAL
TRACE
CORRESPONDS
TO
THE
DETAILED
POWER
TRACE
SAMPLED
AT
AND
THE
PHASE
TRACE
IS
THE
THERMAL
OUTPUT
OF
RUNNING
THE
SAME
WORKLOAD
AND
USING
OUR
POWER
COMPUTATION
METHODOLOGY
WE
OBSERVE
THAT
THE
TRACE
GENER
ATED
WITH
OUR
METHODOLOGY
CLOSELY
MATCHES
THE
TRACE
SAMPLED
AT
A
SIMILAR
TO
THE
EM
FAILURE
RATE
EQUATION
WE
USE
TO
REPRE
SENT
THE
FIRST
HALF
OF
THE
EQUATION
BOTH
EM
AND
TDDB
FAILURE
RATES
ARE
EXPONENTIALLY
DEPENDENT
ON
TEMPERATURE
ΛT
DDB
E
EA
KT
E
EA
KT
OVERALL
TABLE
POWER
ESTIMATION
ERROR
OF
OUR
FRONT
END
TOOL
COM
PARED
WITH
RESPECT
TO
WATTCH
THERMAL
CYCLING
TC
IS
CAUSED
BY
THE
LARGE
DIFFERENCE
IN
THER
MAL
EXPANSION
COEFFICIENTS
OF
METALLIC
AND
DIELECTRIC
MATERIALS
AND
LEADS
TO
CRACKS
AND
OTHER
PERMANENT
FAILURES
THE
THERMAL
CYCLING
EFFECT
IS
MODELED
BY
THE
COFFIN
MASON
EQUATION
SLOW
THER
MAL
CYCLES
HAPPEN
BECAUSE
OF
LOW
FREQUENCY
POWER
CHANGES
SUCH
AS
POWER
ON
OFF
CYCLES
FAST
CYCLES
OCCUR
DUE
TO
EVENTS
SUCH
AS
POWER
MANAGEMENT
DECISIONS
ALTHOUGH
LOWER
FREQUENCY
CYCLES
HAVE
GENERALLY
RECEIVED
MORE
ATTENTION
RECENT
WORK
SHOWS
THAT
THERMAL
CYCLES
DUE
TO
POWER
OR
WORKLOAD
VARIATIONS
CAN
ALSO
DE
GRADE
RELIABILITY
THE
FAILURE
RATE
DUE
TO
THERMAL
CYCLING
IS
FORMULATED
AS
IN
EQUATION
TIME
MS
WATTCH
PHASE
Λ
T
TO
Q
F
IN
THIS
EQUATION
T
IS
THE
TEMPERATURE
CYCLING
RANGE
THE
ELAS
TIC
PORTION
OF
THE
THERMAL
CYCLE
IS
SHOWN
AS
TO
ELASTIC
THERMAL
FIGURE
COMPARISON
OF
TEMPERATURE
RESPONSES
FOR
US
ING
TWO
SIMULATION
METHODOLOGIES
HIGHER
GRANULARITY
AS
IS
ONE
OF
THE
MOST
POWER
VARIANT
AP
PLICATIONS
THE
REST
OF
THE
BENCHMARKS
DEMONSTRATE
EVEN
LESS
DIFFER
ENCE
BECAUSE
THERMAL
CYCLING
EFFECTS
ARE
INSIGNIFICANT
UNLESS
THE
TEMPERATURE
VARIATIONS
ARE
MORE
THAN
A
FEW
DEGREES
THESE
RESULTS
ARE
MORE
THAN
ACCURATE
ENOUGH
TO
CAPTURE
BOTH
TEMPERATURE
INDUCED
AND
CYCLE
INDUCED
EFFECTS
ONCE
WE
GENERATE
A
FULL
THERMAL
TRACE
WE
USE
THIS
TRACE
AS
INPUT
TO
OUR
RELIABILITY
MODEL
DESCRIBED
IN
THE
NEXT
SECTION
RELIABILITY
MODELING
OUR
WORK
TARGETS
TEMPERATURE
INDUCED
RELIABILITY
PROBLEMS
OUR
SIMULATION
AND
MODELING
FRAMEWORK
ALLOWS
US
TO
EVALUATE
SCHEDUL
ING
POLICIES
BASED
ON
THEIR
SUCCESS
IN
REDUCING
THE
FAILURE
RATE
DUE
TO
THERMAL
HOT
SPOTS
AND
THERMAL
CYCLES
ACHIEVING
A
LOWER
FAILURE
RATE
INCREASES
THE
MEAN
TIME
TO
FAILURE
WHICH
IS
THE
EXPECTED
LIFETIME
OF
THE
CIRCUIT
THE
MOST
COMMONLY
STUDIED
TEMPERATURE
INDUCED
INTRINSIC
HARD
FAILURE
MECHANISMS
ARE
ELECTROMIGRATION
TIME
DEPEN
DENT
DIELECTRIC
BREAKDOWN
AND
THERMAL
CYCLING
ELECTROMIGRATION
EM
OCCURS
IN
INTERCONNECTS
AS
A
RESULT
OF
THE
MOMENTUM
TRANSFER
FROM
ELECTRONS
TO
IONS
THAT
CONSTRUCT
THE
INTER
CONNECT
LATTICE
AND
LEADS
TO
HARD
FAILURES
SUCH
AS
OPENS
AND
SHORTS
IN
METAL
LINES
THE
EM
FAILURE
RATE
ΛEM
BASED
ON
BLACK
MODEL
IS
GIVEN
IN
EQUATION
IN
THE
EQUATION
EA
IS
THE
ACTIVATION
ENERGY
K
IS
THE
BOLTZMANN
CONSTANT
T
IS
THE
TEMPERATURE
J
AND
JCRIT
ARE
THE
CURRENT
DENSITY
AND
THE
THRESHOLD
CURRENT
DENSITY
RESPECTIVELY
AND
IS
A
MATERIAL
DEPENDENT
CONSTANT
WE
REPRESENT
THE
FIRST
HALF
STRESS
REFERS
TO
REVERSIBLE
DEFORMATION
OCCURRING
DURING
A
CYCLE
AND
TO
SHOULD
BE
SUBTRACTED
FROM
THE
TOTAL
STRAIN
RANGE
TYPICALLY
TO
T
SO
THE
TO
COMPONENT
CAN
BE
DROPPED
FROM
THE
EQUATION
IS
A
MATERIAL
DEPENDENT
CONSTANT
Q
IS
THE
COFFIN
MANSON
EXPONENT
AND
F
IS
THE
FREQUENCY
OF
THERMAL
CYCLES
NOTE
THAT
THE
COFFIN
MANSON
EQUATION
COMPUTES
THE
NUMBER
OF
CY
CLES
TO
FAILURE
THEREFORE
THE
MTTF
IN
YEARS
IS
THE
NUMBER
OF
CYCLES
MULTIPLIED
BY
THE
PERIOD
OF
THE
CYCLES
COMPUTING
THE
FREQUENCY
OF
CYCLES
IS
NOT
STRAIGHTFORWARD
IN
A
SIMULATION
OF
AN
IRREGULAR
DYNAMIC
SYSTEM
TO
RESOLVE
THIS
PROB
LEM
WE
OBSERVED
THE
RECENT
TEMPERATURE
HISTORY
ON
EACH
CORE
TO
COMPUTE
T
AND
F
WE
SET
THE
INITIAL
LENGTH
OF
THE
HISTORY
WIN
DOW
TO
SECONDS
AND
ADJUSTED
THE
LENGTH
DYNAMICALLY
DEPENDING
ON
HOW
MANY
CYCLES
WERE
OBSERVED
FOR
EXAMPLE
IF
NO
TEMPERA
TURE
CYCLES
WERE
OBSERVED
IN
THE
LAST
INTERVAL
WE
INCREMENTED
THE
HISTORY
WINDOW
LENGTH
TO
CAPTURE
SLOWER
CYCLES
T
IS
THE
TEMPER
ATURE
DIFFERENTIAL
WE
OBSERVED
IN
THE
LAST
INTERVAL
WE
SET
A
HIGHER
BAND
OF
AND
A
LOWER
BAND
OF
OF
THE
TEMPERATURE
RANGE
RECORDED
IN
THE
LAST
INTERVAL
AND
COUNTED
THE
NUMBER
OF
TIMES
THE
TEMPERATURE
EXCEEDED
THE
HIGHER
BAND
OR
WENT
BELOW
THE
LOWER
BAND
AND
USED
THAT
TO
CALCULATE
THE
NUMBER
OF
CYCLES
IN
THIS
PE
RIOD
IN
THIS
WAY
WE
COULD
ACCOUNT
FOR
THE
CONTRIBUTION
OF
CYCLES
WITH
VARYING
TEMPERATURE
DIFFERENTIALS
AND
VARYING
PERIODS
TO
COMBINE
THE
FAILURE
RATES
WE
USED
THE
SUM
OF
FAILURE
RATES
MODEL
AS
IN
RAMP
THIS
MODEL
ASSUMES
THAT
ALL
THE
INDIVID
UAL
FAILURE
RATES
ARE
INDEPENDENT
MEAN
TIME
TO
FAILURE
MTTF
IS
Λ
FOR
CONSTANT
FAILURE
RATES
THEREFORE
WE
AVERAGED
THE
FAILURE
RATE
OBSERVED
THROUGHOUT
THE
SIMULATION
AND
COMPUTED
THE
CORRE
SPONDING
AVERAGE
MTTF
THE
AVERAGE
MTTF
VALUE
REPORTED
FOR
TECHNOLOGY
IS
YEARS
OF
THE
EQUATION
WITH
THE
TERM
WHICH
CAN
BE
CONSIDERED
AS
A
FOR
MODERATE
TEMPERATURES
AT
TECHNOLOGY
SRINIVASAN
ET
CONSTANT
AN
AVERAGE
TECHNOLOGY
CIRCUIT
DEPENDENT
VALUE
AL
DEMONSTRATE
THAT
THE
CONTRIBUTION
OF
ELECTROMIGRATION
DI
ELECTRIC
BREAKDOWN
AND
CYCLING
TO
THE
OVERALL
FAILURE
RATE
ARE
SIM
Λ
A
J
J
N
EA
KT
EA
KT
ILAR
TO
EACH
OTHER
THIS
ALLOWS
US
TO
CALIBRATE
THE
CONSTANTS
IN
EACH
EM
CRIT
E
ΛEM
E
FAILURE
EQUATION
T
DDB
AND
TO
GIVE
A
SYSTEM
MTTF
TIME
DEPENDENT
DIELECTRIC
BREAKDOWN
TDDB
IS
A
WEAR
OUT
MECHANISM
OF
THE
GATE
DIELECTRIC
AND
FAILURE
OCCURS
WHEN
A
CONDUC
TIVE
PATH
IS
FORMED
IN
THE
DIELECTRIC
TDDB
IS
CAUSED
BY
THE
ELECTRIC
FIELD
AND
TEMPERATURE
AND
THE
FAILURE
RATE
IS
DEFINED
IN
EQUATION
OF
YEARS
AT
NOMINAL
TEMPERATURE
WE
USED
THE
SAME
CONSTANTS
ALL
THROUGHOUT
THE
EXPERIMENTS
WHICH
MEANS
THAT
THE
RELATIVE
IM
PACT
OF
DIFFERENT
FAILURE
MECHANISMS
MIGHT
CHANGE
DEPENDING
ON
THE
CONDITIONS
FOR
EXAMPLE
IF
THE
TEMPERATURE
IS
HIGH
THEN
THE
EFFECT
OF
EM
OR
TDDB
IS
HIGHER
THAN
TC
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
CORE
FIGURE
FLOORPLAN
OF
THE
CORE
CPU
TABLE
ARCHITECTURAL
PARAMETERS
WE
ALSO
EXAMINED
THERMAL
GRADIENTS
WHICH
REFER
TO
THE
TEMPER
ATURE
DIFFERENCES
AMONG
DIFFERENT
LOCATIONS
ON
THE
DIE
HOWEVER
WE
DO
NOT
EXPLICITLY
INCLUDE
THE
EFFECTS
OF
GRADIENTS
IN
OUR
OVER
ALL
RELIABILITY
MODEL
THIS
IS
BECAUSE
ALTHOUGH
THERMAL
GRADIENTS
CAN
INDUCE
HARD
ERRORS
THEIR
PRIMARY
EFFECT
IS
ON
DEVICE
LATENCIES
WHICH
ARE
THEN
MANIFESTED
AS
AN
INCREASE
IN
TIMING
ERRORS
RATHER
THAN
HARD
FAILURES
METHODOLOGY
THIS
SECTION
DESCRIBES
OTHER
DETAILS
OF
OUR
SIMULATION
INFRASTRUC
TURE
THAT
IMPACT
THE
RESULTS
SHOWN
IN
THE
FOLLOWING
SECTIONS
THESE
IN
GENERAL
ARE
DETAILS
THAT
ARE
RELATIVELY
INDEPENDENT
OF
OUR
FRAME
WORK
DESCRIBED
IN
SECTION
AND
EASILY
CHANGED
SUCH
AS
THE
PRO
CESSOR
CORE
MODEL
THE
WORKLOAD
ETC
THE
SIMULATOR
OUT
OF
ORDER
EXECUTION
MODEL
IS
BASED
ON
SIMPLESCALAR
AND
PROVIDES
A
DETAILED
MODEL
OF
AN
ALPHA
PROCESSOR
ANTICIPATING
CONTINUED
SCALING
OF
CORE
COUNTS
THE
CPU
WE
MODEL
IS
A
CORE
MULTIPROCESSOR
MANUFACTURED
AT
THE
FLOORPLAN
FOR
THIS
CPU
IS
PROVIDED
IN
FIGURE
EACH
CORE
HAS
OUT
OF
ORDER
ISSUE
A
PRIVATE
DATA
CACHE
INSTRUCTION
CACHE
CACHE
AND
MEMORY
CHANNELS
EACH
CORE
POSSESSES
THREE
VOLT
AGE
AND
FREQUENCY
SETTINGS
FOR
DYNAMIC
VOLTAGE
FREQUENCY
SCALING
AT
AT
AND
AT
WHICH
REPRESENT
DVFS
SETTINGS
OF
ORIGINAL
STEP
AND
STEP
RESPECTIVELY
THE
ARCHITECTURAL
PARAMETERS
OF
EACH
CORE
ARE
DEPICTED
IN
TABLE
CREATING
REPRESENTATIVE
WORKLOADS
IS
A
CHALLENGE
IN
A
CORE
EN
VIRONMENT
TO
ASSIST
THIS
PROCESS
WE
CLASSIFY
ALL
BENCH
MARKS
IN
TERMS
OF
THEIR
VARIABILITY
AND
MEMORY
BOUNDEDNESS
DIS
CUSSED
BELOW
THE
DISTINCTION
BETWEEN
CPU
BOUND
AND
MEMORY
BOUND
APPLICATIONS
IS
PARTICULARLY
IMPORTANT
IN
THIS
STUDY
BECAUSE
IT
IMPACTS
HOW
PERFORMANCE
SCALES
AS
THE
FREQUENCY
CHANGES
WE
MODEL
BOTH
HOMOGENEOUS
AND
HETEROGENEOUS
WORKLOADS
IN
TERMS
OF
THE
APPLICATIONS
CPU
OR
MEMORY
BOUNDEDNESS
AS
OUR
EXE
CUTION
MODEL
DOES
NOT
EXTEND
TO
PARALLEL
PROGRAMS
THE
HOMOGE
NEOUS
WORKLOADS
STAND
IN
FOR
BOTH
HOMOGENEOUS
SERVER
TYPE
WORK
LOADS
AND
PARALLEL
APPLICATIONS
WITH
FEW
STALLS
FOR
COMMUNICATION
HOWEVER
OUR
HOMOGENEOUS
AND
HETEROGENEOUS
MULTIPROGRAMMED
WORKLOADS
BEST
REPRESENT
A
SERVER
ENVIRONMENT
WHERE
THE
AVERAGE
LIFETIME
OF
THE
PROCESSOR
CAN
SIGNIFICANTLY
AFFECT
OVERALL
COSTS
WE
USE
THE
RATIO
OF
MEMORY
BUS
TRANSACTIONS
TO
INSTRUCTIONS
AS
A
METRIC
TO
CLASSIFY
APPLICATIONS
AS
MEMORY
OR
CPU
BOUND
AS
SUG
GESTED
BY
WU
ET
AL
WE
CLASSIFY
APPLICATIONS
ALONG
SEVERAL
OTHER
DIMENSIONS
BY
CONSTRUCTING
OUR
WORKLOADS
FROM
APPLICA
TIONS
WITH
DIFFERENT
PHASE
VARIABILITY
POWER
SAVINGS
POTENTIAL
AND
CPU
MEMORY
BOUNDEDNESS
WE
SEEK
TO
REPRESENT
A
WIDE
RANGE
OF
REAL
WORLD
WORKLOADS
TABLE
DESCRIBES
EACH
WORKLOAD
WE
MODEL
WORKLOADS
WITH
THREADS
OUR
CMP
ARCHITECTURE
IS
CONSTRUCTED
TO
NOT
HAVE
THERMAL
ISSUES
WHEN
LIGHTLY
LOADED
WHICH
IS
THE
EXPECTED
BEHAVIOR
FOR
THE
NEXT
FEW
PROCESSOR
GENERATIONS
WE
CONSTRUCT
BOTH
HOMO
GENEOUS
AND
HETEROGENEOUS
WORKLOADS
AND
CPU
BOUND
MEMORY
BOUND
AND
MIXED
WORKLOADS
THE
MIXED
WORKLOADS
CONTAIN
APPLI
CATIONS
FROM
BOTH
EXTREMES
AS
WELL
AS
SOME
IN
THE
MIDDLE
OF
OUR
CATEGORIZATION
IN
THE
TIME
FRAMES
WE
MODEL
SEVERAL
OF
THE
APPLI
CATIONS
COMPLETE
EXECUTION
IN
THOSE
CASES
WE
CONTINUALLY
RESTART
THE
APPLICATION
AT
THE
BEGINNING
TO
GET
CONSISTENT
BEHAVIOR
ACROSS
THE
EXPERIMENT
A
COMMON
PERFORMANCE
METRIC
ON
MULTICORE
PLATFORMS
IS
A
RAW
COUNT
OF
IPC
HOWEVER
THIS
METRIC
GIVES
UNDESERVED
BIAS
TOWARDS
HIGH
IPC
THREADS
AS
PERFORMANCE
MAY
BE
INCREASED
BY
RUNNING
MORE
CPU
BOUND
THREADS
TO
CIRCUMVENT
THIS
DIFFICULTY
WE
USED
THE
FAIR
SPEEDUP
METRIC
FS
FS
IS
COMPUTED
BY
FINDING
THE
HAR
MONIC
MEAN
OF
EACH
THREAD
SPEED
UP
OVER
A
BASELINE
POLICY
OF
RUNNING
THE
THREAD
AT
THE
HIGHEST
FREQUENCY
AND
VOLTAGE
ALTHOUGH
SOME
APPLICATIONS
COMPLETE
MULTIPLE
TIMES
DURING
OUR
SIMULATIONS
WE
COMPUTE
FS
IN
SUCH
A
WAY
THAT
THE
OVERALL
CONTRI
BUTION
OF
EACH
APPLICATION
IS
THE
SAME
RELIABILITY
AWARE
SCHEDULING
THE
SIMULATION
AND
MODELING
INFRASTRUCTURE
DESCRIBED
ALLOWS
US
TO
DESIGN
AND
EVALUATE
SEVERAL
JOB
ALLOCATION
AND
THERMAL
MANAGE
MENT
STRATEGIES
WE
DIVIDE
THESE
TECHNIQUES
INTO
THREE
CATEGORIES
THOSE
THAT
CHANGE
WHAT
IS
RUNNING
ON
A
CORE
VIA
GATING
OR
MIGRA
TION
THOSE
THAT
CONTINUE
TO
EXECUTE
THE
SAME
THREAD
BUT
CHANGE
SPEED
VIA
DVFS
AND
HYBRIDS
THAT
COMBINE
THE
TWO
TYPES
EACH
OF
THESE
METHODS
CAN
BE
INTEGRATED
WITH
DYNAMIC
POWER
MANAGEMENT
DPM
AS
WELL
DPM
TURNS
OFF
CORES
AFTER
THEY
HAVE
BEEN
IDLE
FOR
A
GIVEN
TIMEOUT
PERIOD
THE
SCHEDULING
AND
THERMAL
MANAGEMENT
POLICIES
EVALUATE
THE
SYSTEM
CHARACTERISTICS
AT
EVERY
SCHEDULING
PERIOD
AND
MAKE
A
DECISION
ACCORDINGLY
IN
ALL
CASES
THE
SCHEDULING
TICK
IS
SET
TO
EVERY
THE
THRESHOLD
TEMPERA
TURE
FOR
ALL
THE
TEMPERATURE
TRIGGERED
POLICIES
IS
C
THE
DEFAULT
POLICY
KEEPS
THE
INITIAL
ASSIGNMENT
OF
JOBS
TO
CORES
FIXED
AND
NO
WORKLOAD
MIGRATION
OR
VOLTAGE
FREQUENCY
SCALING
OCCURS
ON
THE
FLY
MIGRATION
AND
GATING
SCHEDULING
POLICIES
THESE
TECHNIQUES
ATTEMPT
TO
MOVE
COMPUTATION
OFF
OF
HOT
CORES
EITHER
VIA
MIGRATION
OR
STALLED
EXECUTION
AS
A
RESPONSE
TO
A
THERMAL
EVENT
HIGH
TEMPERATURE
OR
AS
A
MATTER
OF
POLICY
RUNS
EACH
CORE
AT
THE
DEFAULT
HIGHEST
FREQUENCY
AND
VOLTAGE
SETTING
UNTIL
A
CORE
REACHES
THE
THERMAL
THRESHOLD
AT
THIS
POINT
THE
CORE
IS
STALLED
AND
THE
CLOCK
IS
GATED
TO
REDUCE
POWER
WKLOAD
NAME
DESCRIPTION
HOMOGENEOUS
CPU
BOUND
CORES
UTILIZED
BENCHMARKS
SIXTRACK
HOMOGENEOUS
MEM
BOUND
MCF
HETEROGENEOUS
CPU
BOUND
MESA
CRAFTY
SIXTRACK
HETEROGENEOUS
MEM
BOUND
MCF
EQUAKE
SWIM
HETEROGENEOUS
MIX
MCF
MESA
SIXTRACK
EQUAKE
SWIM
APPLU
TWOLF
CRAFTY
APSI
LUCAS
HETEROGENEOUS
CPU
BOUND
MESA
CRAFTY
SIXTRACK
HETEROGENEOUS
CPU
BOUND
MESA
CRAFTY
SIXTRACK
HETEROGENEOUS
MIX
MCF
MESA
SIXTRACK
SWIM
CRAFTY
APSI
LUCAS
HETEROGENEOUS
MIX
MCF
MESA
SIXTRACK
EQUAKE
SWIM
TWOLF
CRAFTY
APSI
LUCAS
TABLE
WORKLOAD
CHARACTERISTICS
EXECUTION
CHARACTERISTICS
OF
THE
THREADS
IN
DETERMINING
CORE
TEM
PERATURE
THUS
THOSE
TECHNIQUES
ENDED
UP
CONSTANTLY
MOVING
JOBS
BETWEEN
HOT
AND
COLD
CORES
FIGURE
THREAD
ASSIGNMENT
STRATEGY
FOR
BALANCE
LOCATION
CONSUMPTION
IF
THE
CORE
TEMPERATURE
GOES
BELOW
THE
TEMPERATURE
THRESHOLD
IN
THE
NEXT
SAMPLING
INTERVAL
EXECUTION
CONTINUES
WE
ASSUME
THAT
EACH
CORE
CAN
BE
CLOCK
GATED
INDIVIDUALLY
MIGRATION
SENDS
JOBS
THAT
HAVE
EXCEEDED
A
THERMAL
THRESHOLD
TO
THE
COOLEST
CORE
THAT
HAS
NOT
BEEN
ASSIGNED
A
NEW
THREAD
DURING
THE
CURRENT
SCHEDULING
PERIOD
IF
THE
COOLEST
CORE
SELECTED
IS
ALREADY
RUNNING
A
JOB
WE
SWAP
THE
JOBS
AMONG
THE
HOT
AND
COOL
CORES
THIS
TECHNIQUE
CAN
BE
THOUGHT
OF
AS
AN
EXTENSION
OF
CORE
HOPPING
OR
ACTIVITY
MIGRATION
TECHNIQUES
TO
THE
CASE
OF
MANY
CORES
AND
MANY
THREADS
BALANCE
ASSIGNS
JOBS
WITH
THE
HIGHEST
COMMITTED
IPC
DURING
THE
LAST
INTERVAL
I
E
BETWEEN
THE
LAST
TWO
SCHEDULING
TICKS
TO
CORES
THAT
HAVE
THE
LOWEST
TEMPERATURE
THIS
SCHEDULING
IDEA
REPRESENTS
A
MORE
PROACTIVE
FORM
OF
MIGRATION
IN
WHICH
THREADS
ARE
DYNAMICALLY
ASSIGNED
TO
LOCATIONS
BEFORE
THERMAL
THRESHOLDS
NECESSITATE
ACTION
IS
SIMILAR
TO
BALANCE
BUT
INSTEAD
OF
ASSIGNING
THE
THREADS
WITH
THE
HIGHEST
COMMITTED
IPC
TO
THE
COOLEST
CORES
IT
ASSIGNS
THEM
TO
CORES
THAT
ARE
EXPECTED
TO
BE
COOLEST
BASED
ON
LOCATION
THE
CORES
ON
THE
CORNER
LOCATIONS
OF
THE
FLOORPLAN
ARE
EXPECTED
TO
BE
THE
COOLEST
THE
REMAINING
CORES
ON
THE
SIDES
ARE
EX
PECTED
TO
BE
THE
SECOND
COOLEST
AND
THE
CORES
IN
THE
CENTER
OF
THE
FLOORPLAN
ARE
HOTTEST
THIS
IS
BECAUSE
THE
TEMPERATURE
OF
A
CORE
IS
A
RESULT
NOT
ONLY
OF
ACTIVITY
ON
THAT
CORE
BUT
ALSO
ON
THE
ACTIVITY
OF
ITS
NEIGHBORS
HIGHER
NUMBER
OF
ACTIVE
NEIGHBORS
RESULTS
IN
HOT
TER
CORES
FIGURE
SHOWS
THE
STRATEGY
WE
USED
TO
ASSIGN
JOBS
TO
TO
CORES
WHERE
THE
JOBS
HAVE
DECREASING
COMMITTED
IPC
IP
IP
IP
WHEREAS
THE
OPTIMAL
ALLOCA
TION
OF
THREADS
TO
CORES
MIGHT
DIVERGE
FROM
THE
ALLOCATION
SHOWN
IN
THE
FIGURE
DEPENDING
ON
THE
IPC
DIFFERENCE
AMONG
THREADS
THIS
ALLOCATION
GENERALLY
RESULTS
IN
TEMPERATURE
CHARACTERISTICS
CLOSE
TO
THE
BEST
ALLOCATION
WITH
THIS
SCHEME
AND
THREADS
FOR
EXAMPLE
CORES
LABELED
AND
IN
THIS
FIGURE
WOULD
ALWAYS
BE
IDLE
WE
ALSO
EXPERIMENTED
WITH
HEURISTICS
THAT
CHOOSE
A
THREAD
NEXT
CORE
ALLOCATION
BASED
ON
THE
TEMPERATURE
OF
THE
THREAD
CURRENT
CORE
E
G
MOVE
THE
THREAD
ON
THE
HOTTEST
TO
THE
COOLEST
CORE
BUT
THESE
HEURISTICS
PERFORMED
POORLY
IN
MULTICORE
ARCHITECTURES
LIKE
THE
ONE
WE
MODEL
LOCATION
IS
A
MORE
SIGNIFICANT
FACTOR
THAN
THE
METHODS
WITH
VOLTAGE
FREQUENCY
SCALING
THIS
SET
OF
TECHNIQUES
RELY
EXCLUSIVELY
ON
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
TO
CONTROL
THERMAL
DYNAMICS
THEY
DIFFER
IN
HOW
AND
WHEN
DVFS
IS
APPLIED
DVFS
THRESHOLD
REDUCES
VOLTAGE
AND
FREQUENCY
V
F
ONE
STEP
AT
A
TIME
WHEN
A
CORE
TEMPERATURE
EXCEEDS
A
THRESH
OLD
AFTER
REDUCING
THE
V
F
TO
THE
STEP
SETTING
IF
THE
CORE
IS
STILL
ABOVE
THE
THRESHOLD
IN
THE
NEXT
SCHEDULING
INTERVAL
USES
THE
STEP
SETTING
WHEN
A
CORE
TEMPERATURE
IS
BELOW
THE
THRESHOLD
THE
V
F
SETTING
IS
INCREASED
AGAIN
ONE
STEP
AT
EACH
SCHEDULING
INTERVAL
DVFS
LOCATION
USES
A
FIXED
V
F
SETTING
FOR
EACH
CORE
AND
THERE
IS
NO
DYNAMIC
SCALING
AT
RUNTIME
AS
THE
CENTER
CORES
TEND
TO
HEAT
UP
MORE
QUICKLY
THE
FOUR
CORES
IN
THE
CENTER
OF
THE
FLOORPLAN
HAVE
THE
SETTING
THE
CORNER
CORES
ARE
TYPICALLY
THE
COOLEST
CORES
HENCE
THEY
USE
THE
ORIGINAL
V
F
SETTING
THE
REST
OF
THE
CORES
I
E
THE
EIGHT
REMAINING
CORES
ON
THE
SIDES
HAVE
THE
SETTING
DVFS
PERFORMANCE
REDUCES
THE
VOLTAGE
AND
FRE
QUENCY
DYNAMICALLY
ON
A
CORE
DEPENDING
ON
THE
MEMORY
BOUND
EDNESS
OF
THE
CURRENT
APPLICATION
PHASE
PREVIOUSLY
IT
WAS
SHOWN
THAT
CPU
INTENSIVE
TASKS
DO
NOT
GAIN
MUCH
IN
TERMS
OF
ENERGY
SAV
INGS
FROM
RUNNING
AT
LOW
FREQUENCIES
AND
CONVERSELY
IT
IS
BENEFI
CIAL
TO
RUN
MEMORY
BOUND
TASKS
AT
A
LOWER
FREQUENCY
AS
THEIR
PERFORMANCE
IS
MUCH
MORE
TOLERANT
OF
FREQUENCY
SCALING
DVFS
PERFORMANCE
SEEKS
TO
REDUCE
THE
OVERALL
CHIP
TEMPERATURE
WITH
MIN
IMAL
PERFORMANCE
COST
BY
PROACTIVELY
SCALING
BACK
THOSE
APPLICA
TIONS
THAT
ARE
LEAST
IMPACTED
TO
DETERMINE
THE
MEMORY
BOUND
PHASES
WE
USE
A
CYCLES
PER
INSTRUCTION
CPI
BASED
METRIC
Μ
AS
DEFINED
BY
DHIMAN
ET
AL
IT
COMPARES
THE
OBSERVED
CPI
WITH
A
POTENTIAL
CPI
WE
MIGHT
HAVE
GOTTEN
WITHOUT
MEMORY
EVENTS
IF
THE
Μ
IS
NEAR
ONE
THE
APPLICA
TION
IS
CPU
BOUND
IF
IT
IS
LOW
THE
APPLICATION
IS
MEMORY
BOUND
NOTE
THAT
Μ
CAN
ALSO
TAKE
NEGATIVE
VALUES
ANALYSIS
ON
OUR
OWN
APPLICATION
SET
CONFIRMS
THAT
THIS
METRIC
TRACKS
EXTREMELY
WELL
WITH
PERFORMANCE
DEGRADATION
IN
THE
PRESENCE
OF
DVFS
IF
THE
Μ
OBSERVED
IN
THE
LAST
INTERVAL
IS
LESS
THAN
THEN
WE
USE
THE
SETTING
AND
WE
FIND
LESS
THAN
PERFORMANCE
LOSS
DURING
THOSE
PHASES
IF
Μ
WE
APPLY
THE
SETTING
WHICH
INDUCES
LESS
THAN
LOSS
IN
PERFORMANCE
FOR
Μ
WE
DO
NOT
PERFORM
ANY
V
F
SCALING
WHEN
Μ
IF
WE
USED
THE
SCALING
FOR
CPU
BOUND
APPLICATIONS
THE
PERFORMANCE
LOSS
WOULD
BE
IN
THE
RANGE
OF
DVFS
BEHAVES
EXACTLY
LIKE
UNLESS
A
CORE
REACHES
A
THERMAL
THRESHOLD
IF
THE
TEM
PERATURE
EXCEEDS
THE
THRESHOLD
ON
A
CORE
THEN
THE
POLICY
ACTIVATES
TO
REDUCE
THE
TEMPERATURE
ON
THAT
CORE
AFTER
THE
CORE
TEM
PERATURE
RETURNS
WITHIN
THRESHOLD
WE
SWITCH
BACK
TO
THIS
TECHNIQUE
WINS
IF
BY
PROACTIVELY
SLOWING
A
THREAD
THAT
IS
TOLERANT
OF
FREQUENCY
CHANGES
IT
CAN
ENABLE
A
NEARBY
THREAD
THAT
IS
NOT
SO
TOLERANT
OF
FREQUENCY
CHANGE
TO
FOREGO
A
DVFS
SLOWDOWN
TECHNIQUES
COMBINING
WORKLOAD
ALLOCA
TION
AND
DVFS
IN
INVESTIGATING
THE
INTERACTION
OF
SCHEDULING
AND
DVFS
POLICIES
WE
EMPLOY
TO
REPRESENT
THE
SCHEDULING
POLICIES
IT
HAS
USEFUL
PROPERTIES
IN
TERMS
OF
BOTH
RELIABILITY
AND
PERFORMANCE
IT
DOES
ONLY
ENOUGH
MIGRATION
TO
FIND
THE
BEST
LOCATION
FOR
EACH
THREAD
THEN
ONLY
MIGRATES
WHEN
APPLICATION
CHARACTERISTICS
CHANGE
DVFS
THRESHOLD
WORKS
BY
INITIALLY
US
ING
TO
ASSIGN
POTENTIALLY
HOTTER
THREADS
TO
COOLER
LOCATIONS
ON
THE
DIE
IF
THIS
TECHNIQUE
FAILS
TO
KEEP
A
GIVEN
CORE
UN
DER
THE
SPECIFIED
THRESHOLD
THE
CORE
EMPLOYS
UNTIL
IT
IS
UNDER
THE
THERMAL
THRESHOLD
USES
THE
LOCATION
POLICY
TO
ALLOCATE
JOBS
TO
CORES
AND
RUNS
AT
THE
SAME
TIME
TO
DECIDE
ON
THE
V
F
SETTINGS
OF
THE
CORES
DVFS
LOCATION
ASSIGNS
THE
LOCATION
V
F
SETTINGS
AS
IN
THE
POLICY
TO
CORES
AND
PERFORMS
BALAN
FOR
ALLOCATING
THE
THREADS
THIS
TENDS
TO
HAVE
THE
EFFECT
OF
ASSIGNING
THE
MOST
MEMORY
BOUND
THREADS
IN
THE
CENTER
ZONE
WHICH
RUNS
AT
THE
SETTING
USES
IPC
IN
ASSIGNING
THREADS
TO
LOCATIONS
WHEN
COMBINED
WITH
DVFS
WE
MUST
ACCOUNT
FOR
THE
V
F
AND
ITS
EFFECT
ON
THE
MEASURED
IPC
THUS
IF
A
CORE
IS
RUNNING
AT
A
LOWER
V
F
SETTING
WE
SCALE
THE
MEASURED
IPC
BASED
ON
THE
AVERAGE
PER
FORMANCE
HIT
OBSERVED
AT
THAT
V
F
LEVEL
EXPERIMENTAL
RESULTS
IN
THIS
SECTION
WE
DEMONSTRATE
THAT
THE
FRAMEWORK
WE
PROPOSED
ALLOWS
US
TO
EVALUATE
A
LARGE
SET
OF
PREVIOUSLY
PROPOSED
AND
NEW
SCHEDULING
ALGORITHMS
IN
TERMS
OF
PERFORMANCE
POWER
TEMPERA
TURE
AND
PROCESSOR
LIFETIME
RELIABILITY
WE
SHOW
THAT
HAVING
A
FULLY
INTEGRATED
MODEL
INCLUDING
A
RELIABILITY
MODEL
THAT
ACCOUNTS
FOR
ALL
THE
MAJOR
CAUSES
OF
TEMPERATURE
INDUCED
HARD
FAILURES
SHEDS
SOME
NEW
LIGHT
ON
CMP
SCHEDULING
SECTION
IDENTIFIED
A
WIDE
ASSORTMENT
OF
THREAD
MANAGEMENT
POLICIES
IN
EVALUATING
THOSE
POLICIES
IN
VARIOUS
EXECUTION
SCENAR
IOS
THIS
SECTION
ATTEMPTS
TO
SORT
OUT
THE
KEY
ISSUES
FACING
THE
DE
SIGNER
OF
A
MULTICORE
THREAD
MANAGEMENT
POLICY
SUCH
AS
HOW
TO
PROPERLY
COMBINE
SCHEDULING
MIGRATION
POLICIES
DVFS
POLICIES
AND
DPM
POLICIES
HOW
TO
ADDRESS
PEAK
TEMPERATURE
EFFECTS
WITHOUT
EXACERBATING
THERMAL
CYCLING
WHETHER
TO
USE
REACTIVE
OR
PROACTIVE
DVFS
POLICIES
AND
HOW
TO
ADDRESS
THERMAL
ASYM
METRIES
IN
THE
CHIP
MULTIPROCESSOR
WE
GROUP
THE
EXPERIMENTS
IN
FOUR
MAJOR
CATEGORIES
SECTION
LOOKS
AT
FULL
CORE
UTILIZATION
SCENARIOS
WITH
A
VARYING
NUMBER
OF
MEMORY
BOUND
AND
CPU
BOUND
THREADS
USING
THE
FIVE
THREAD
WORKLOADS
FROM
TABLE
FOR
THESE
EXPERIMENTS
THREADS
ARE
INITIALLY
PLACED
ON
THE
CORES
RANDOMLY
I
E
WITH
NEITHER
A
CLEARLY
GOOD
OR
BAD
INITIAL
ALLOCATION
SECTION
EXAMINES
SYSTEMS
THAT
ARE
LESS
THAN
FULLY
UTILIZED
WITH
OR
JOBS
I
E
OR
IDLE
CORES
SEC
TION
TAKES
A
DEEPER
LOOK
AT
THE
CONSEQUENCES
OF
THE
INITIAL
ALLOCA
TION
OF
IDLE
CORES
FINALLY
SECTION
INVESTIGATES
HOW
RELIABILITY
PERFORMANCE
AND
ENERGY
VARY
WHEN
THE
SYSTEM
HAS
DPM
CAPABIL
ITIES
AND
WHICH
SCHEDULERS
BEST
COMPLEMENT
DPM
TO
ACHIEVE
THE
DESIRED
TRADE
OFFS
FOR
RELIABILITY
ENERGY
AND
PERFORMANCE
TO
DELIVER
A
FAIR
COMPARISON
WE
PRESENT
THE
ENERGY
AND
PER
FORMANCE
OF
THE
POLICIES
IN
ADDITION
TO
RELIABILITY
MEAN
TIME
TO
FAILURE
THIS
IS
IN
LIEU
OF
TRYING
TO
CREATE
A
SINGLE
ARTIFICIAL
METRIC
THAT
CAPTURES
ALL
THREE
SUCH
USER
DEFINED
METRICS
ARE
SUSCEPTIBLE
TO
PROVIDING
RESULTS
THAT
ARE
SPECIFIC
TO
THE
ASSUMPTIONS
MADE
WHILE
CREATING
THE
METRIC
OR
WHILE
WEIGHING
THE
INDIVIDUAL
PARAMETERS
WE
NORMALIZED
ALL
RESULTS
IN
THE
FOLLOWING
SECTIONS
WITH
RESPECT
TO
THE
DEFAULT
CASE
OF
NO
THERMAL
MANAGEMENT
I
E
ALL
THREADS
RUN
NING
FULL
SPEED
ON
THE
INITIALLY
ASSIGNED
CORES
HENCE
THE
Y
AXES
IN
OUR
MTTF
PERFORMANCE
TEMPERATURE
AND
ENERGY
PLOTS
DEMON
STRATE
THE
NORMALIZED
VALUES
FOR
THESE
PARAMETERS
THIS
ALLOWS
US
TO
EVALUATE
EACH
POLICY
ON
THE
SAME
SCALE
SRINIVASAN
ET
AL
REPORTED
THE
AVERAGE
MTTF
OF
THE
SPEC
SUITE
SIMULATED
FOR
AT
OF
SUPPLY
VOLTAGE
AS
YEARS
AND
OUR
MODEL
IS
CALIBRATED
TO
THE
SAME
VALUE
HOWEVER
IF
THE
RELIABIL
ITY
MODEL
WAS
RE
CALIBRATED
TO
ASSUME
A
SHORTER
OR
LONGER
MTTF
THE
POLICIES
WOULD
STILL
DISPLAY
THE
SAME
TRENDS
ONLY
THE
ABSOLUTE
NUMBERS
WOULD
CHANGE
DEPENDING
ON
PROCESS
TECHNOLOGY
BASELINE
MTTF
AND
THE
SYSTEM
BEING
MODELED
THEREFORE
WE
SHOW
RESULTS
BASED
ON
THE
CHANGE
IN
MTTF
VALUES
RATHER
THAN
ABSOLUTE
NUM
BERS
SO
THAT
THE
DEPENDENCE
ON
THE
ABSOLUTE
CALIBRATION
IS
MINIMAL
FULL
CORE
UTILIZATION
SECTIONS
TO
EXAMINE
THE
CASE
WHERE
ALL
CORES
ARE
AC
TIVELY
RUNNING
THREADS
TECHNIQUES
UTILIZING
WORKLOAD
ALLOCATION
THIS
SECTION
ANALYZES
THE
WORKLOAD
ALLOCATION
POLICIES
ABILITY
TO
IMPROVE
THERMAL
CHARACTERISTICS
THE
POLICIES
THAT
WE
ANALYZED
IN
THIS
SECTION
INCLUDE
MIGRATION
BALANCE
AND
BALAN
THE
RESULTS
IN
FIGURE
WHICH
ARE
THE
AVERAGE
VAL
UES
FOR
ALL
THE
THREAD
WORKLOADS
INDICATE
THAT
MIGRATION
BAL
ANCE
AND
HAVE
LITTLE
IMPACT
ON
RELIABILITY
IN
THIS
SCENARIO
THIS
IS
BECAUSE
CORES
ARE
FULLY
UTILIZED
AND
MOST
OF
OUR
WORKLOADS
ARE
HIGHLY
HOMOGENEOUS
IN
THE
ONE
HETEROGENEOUS
WORKLOAD
THE
EFFECT
IS
STILL
SMALL
IN
THAT
CASE
THE
BALANCE
AND
POLICIES
EACH
IMPROVE
RELIABILITY
BY
WITH
MINIMAL
IMPACT
LESS
THAN
ON
PERFOR
MANCE
ENERGY
AND
AVERAGE
TEMPERATURE
THUS
IN
THE
ABSENCE
OF
IDLE
CORES
THESE
POLICIES
ARE
LESS
EFFECTIVE
THAN
THE
ONES
WITH
VOLT
AGE
AND
FREQUENCY
SCALING
WHICH
WE
DISCUSS
IN
SECTION
THE
POLICY
WAS
NOTABLY
DIFFERENT
THAN
THE
POLICIES
DIS
CUSSED
ABOVE
AS
IT
HAS
THE
ABILITY
TO
COOL
A
CORE
EVEN
IN
THE
ABSENCE
OF
IDLE
CORES
IMPROVED
THE
MTTF
BY
BUT
WITH
A
HEFTY
DECREASE
IN
PERFORMANCE
AND
A
INCREASE
IN
ENERGY
CONSUMPTION
AVERAGE
TEMPERATURE
OF
THE
PROCESSOR
WAS
REDUCED
BY
THE
POLICY
IS
PRONE
TO
CREATING
LARGE
TEMPERATURE
VARIATIONS
DUE
TO
SWITCHING
AMONG
ACTIVE
AND
IDLE
STATES
HOWEVER
THE
FREQUENCY
OF
STALLING
RESUMING
EXECUTION
WAS
HIGH
ENOUGH
THAT
THE
TEMPERATURE
VARIATIONS
WERE
OF
A
RELATIVELY
LOW
MAGNITUDE
AND
THE
RELIABILITY
OF
THE
CORE
WAS
DOMINATED
BY
THE
THERMAL
HOT
SPOTS
ONLY
I
E
NO
SIGNIFICANT
INCREASE
IN
CYCLING
BASED
FAILURES
TECHNIQUES
WITH
VOLTAGE
FREQUENCY
SCALING
FIGURE
SHOWS
THE
EFFECT
OF
THE
DVFS
POLICIES
ON
RELIABILITY
WHEN
THE
CORES
ARE
FULLY
UTILIZED
DVFS
HAS
A
MUCH
MORE
SIGNIFI
CANT
IMPACT
THAN
THE
WORKLOAD
ALLOCATION
POLICIES
DUE
TO
ITS
ABILITY
TO
REDUCE
TEMPERATURE
EVEN
IN
THE
FACE
OF
FULL
UTILIZATION
IN
PARTICULAR
WE
FIND
SEVERAL
KEY
INSIGHTS
IN
THESE
RESULTS
FIRST
IT
IS
IMPORTANT
TO
ALWAYS
KEEP
AN
EYE
ON
PEAK
TEMPERATURE
BY
SELECTIVELY
CHOOSING
WHICH
THREADS
TO
SCALE
SACRIFICES
VERY
LITTLE
IN
PERFORMANCE
BUT
DOES
LAG
A
BIT
BEHIND
IN
MTTF
IN
COMPARISON
TO
OTHER
DVFS
POLICIES
THIS
IS
BECAUSE
IT
IGNORES
THERMAL
WARNINGS
FIGURE
COMPARISON
OF
WORKLOAD
ALLOCATION
TECHNIQUES
REACTS
UPON
REACHING
A
THRESHOLD
AS
WELL
AND
AS
A
RESULT
LOSES
SOME
PERFORMANCE
BUT
IT
HAS
ONE
OF
THE
LOWEST
FAILURE
RATES
SECOND
WE
SEE
SIGNIFICANT
BENEFITS
OF
PROACTIVE
TECHNIQUES
OVER
TRADITIONAL
REACTIVE
TECHNIQUES
IT
IS
INTERESTING
TO
NOTE
THAT
THE
OTHER
DVFS
POLICIES
BEAT
ALONG
ALL
AXES
WHICH
IS
PARTICULARLY
SUR
PRISING
ON
THE
PERFORMANCE
FRONT
THIS
IS
SURPRISING
BECAUSE
ONLY
SCALES
WHEN
IT
HAS
TO
AND
THE
OTHER
DVFS
POLI
CIES
DEFAULT
TO
UPON
REACHING
THE
THRESHOLD
TEMPERATURE
THE
REASON
THAT
OTHER
DVFS
POLICIES
PERFORM
BETTER
IS
THAT
PROACTIVELY
SCALING
A
THREAD
WHOSE
PERFORMANCE
IS
TOLERANT
TO
SCALING
REDUCES
THE
TEMPERATURE
IN
THAT
AREA
AND
OFTEN
PREVENTS
OTHER
NEIGHBORING
THREADS
FROM
REACHING
THE
THRESHOLD
THIRD
WE
SEE
THAT
IT
IS
CRITICAL
THAT
OUR
THREAD
MANAGEMENT
POLICY
UNDERSTANDS
THE
INHERENT
THERMAL
ASYMMETRY
OF
THE
MULTICORE
SYS
TEM
AN
ASYMMETRY
THAT
WILL
EXIST
IN
ALL
LIKELIHOOD
FOR
ANY
MUL
TICORE
GREATER
THAN
FOUR
CORES
THE
POLICY
THAT
PROVIDES
THE
BEST
BALANCE
AMONG
ALL
THREE
METRICS
IS
WITH
A
FAILURE
RATE
THAT
IS
HALF
OF
THE
BASELINE
AND
A
MINIMAL
PERFORMANCE
LOSS
OF
DEFAULT
TO
FURTHER
INVESTIGATE
THIS
POINT
WE
COMPARED
LOCA
WITH
HOMOGENEOUS
PROACTIVE
SCALING
ALL
CORES
AT
DVFS
AND
ALL
CORES
AT
DVFS
AMONG
THESE
DVFS
TECHNIQUES
STILL
DEMONSTRATE
THE
BEST
TRADE
OFF
POINT
THE
DVFS
RESULT
IMPROVED
PERFORMANCE
OVER
BY
LESS
THAN
BUT
GAVE
UP
IN
PROCESSOR
LIFETIME
THE
DVFS
IN
CREASED
RELIABILITY
SIGNIFICANTLY
BUT
MORE
THAN
DOUBLED
THE
PERFOR
MANCE
COST
COMPARED
TO
OUR
TECHNIQUES
ARE
EASILY
ADAPTED
TO
OTHER
SOURCES
OF
ASYMMETRY
SUCH
AS
PROCESS
VARIATIONS
AS
LONG
AS
WE
CAN
QUANTIFY
THE
EFFECTS
OF
SUCH
VARIATIONS
ON
THE
THERMAL
AND
POWER
PROPERTIES
OF
EACH
CORE
HYBRID
TECHNIQUES
WE
EXAMINE
THE
HYBRID
TECHNIQUES
IN
THIS
SECTION
AND
SHOW
THE
RESULTS
IN
FIGURE
WHEN
WE
COMPARE
THE
HYBRID
POLICIES
AGAINST
THE
DVFS
BASED
POLICIES
WE
SEE
THAT
DVFS
BASED
POLICIES
ARE
IMPROVED
LITTLE
BY
COMBINING
THEM
WITH
JOB
ALLOCATION
POLICIES
AGAIN
THIS
IS
DUE
TO
THE
LIMITED
GAINS
FROM
REORGANIZING
RUNNING
THREADS
ON
A
FULLY
UTILIZED
SYSTEM
IMPACT
OF
PARTIAL
UTILIZATION
IT
IS
EXPECTED
THAT
MOST
MULTICORE
SYSTEMS
WILL
BE
UTILIZED
LESS
THAN
MOST
OF
THE
TIME
THIS
IS
TRUE
ESPECIALLY
FOR
THE
CMPS
IN
THE
SERVER
DOMAIN
TO
EVALUATE
THE
IMPACT
OF
SCHEDULING
MECH
ANISMS
ON
RELIABILITY
WHEN
SOME
CORES
ARE
IDLE
WE
USED
THE
AND
THREAD
WORKLOADS
DESCRIBED
IN
TABLE
A
CPU
BOUND
AND
A
MIXED
CPU
BOUND
MEMORY
BOUND
WORKLOAD
FOR
EACH
OF
THE
AND
THREAD
CASES
THE
RESULTS
REPRESENT
THE
AVERAGE
OF
THE
CPU
BOUND
AND
MIXED
CASES
FOR
THE
AND
THREAD
EXPERIMENTS
AT
THE
BEGINNING
OF
EACH
SIMULATION
WE
DECIDED
WHICH
CORES
TO
LEAVE
IDLE
FIGURE
COMPARISON
OF
DVFS
BASED
TECHNIQUES
FIGURE
COMPARISON
OF
HYBRID
TECHNIQUES
BY
CHOOSING
THE
ALLOCATION
WITH
THE
LOWEST
PEAK
TEMPERATURE
ONCE
WE
DETERMINED
THE
ACTIVE
CORES
WE
PERFORMED
THE
INITIAL
PLACEMENT
OF
THREADS
ON
THESE
CORES
RANDOMLY
WE
FIRST
FOCUS
ON
THE
CASE
WITH
ACTIVE
THREADS
IN
FIGURE
ALTHOUGH
THIS
UTILIZATION
IS
CLOSE
TO
THE
FULL
UTILIZATION
EXAMPLES
EXPLORED
IN
SECTION
THE
IMPACT
ON
THE
RELIABILITY
OF
THE
VARIOUS
POLICIES
CHANGES
SIGNIFICANTLY
POLICIES
WITH
FREQUENT
WORKLOAD
RE
ALLOCATION
I
E
BALANCE
MI
GRATION
RESULT
IN
POORER
RELIABILITY
WITH
RESPECT
TO
THE
OTHER
POLI
CIES
THE
BALANCE
POLICY
ASSIGNS
JOBS
TO
CORES
BASED
ON
TEMPERA
TURE
RATHER
THAN
LOCATION
AND
OFTEN
MISTAKES
A
CORE
THAT
IS
COOL
NOW
FOR
A
CORE
THAT
WILL
STAY
COOL
IN
THE
FUTURE
MIGRATION
POLICIES
THAT
FOCUS
HEAVILY
ON
CURRENT
TEMPERATURES
ARE
PRONE
TO
THIS
TYPE
OF
ERROR
THE
MIGRATION
RESULT
HAS
THE
SAME
ISSUE
POLICIES
THAT
MIGRATE
MORE
THAN
NECESSARY
HAVE
TWO
DISTINCT
RELIABILITY
DISADVANTAGES
OVER
THE
OTHER
TECHNIQUES
FIRST
MIGRATING
TOO
OFTEN
WILL
TEND
TO
THWART
THE
DPM
MANAGER
WHICH
DOES
NOT
PUT
A
CORE
TO
SLEEP
UNTIL
IT
HAS
BEEN
IDLE
FOR
AWHILE
THIS
INCREASES
THE
TIME
CORES
SPEND
RUNNING
AT
HOT
TER
TEMPERATURES
SECOND
MIGRATION
CAUSES
THERMAL
CYCLING
THIS
WAS
THE
DOMINANT
CAUSE
OF
THE
LOW
MTTF
RESULTS
AS
THE
POWER
VARIATIONS
BETWEEN
IDLE
AND
ACTIVE
STATES
CREATE
CYCLES
OF
A
SIGNIFI
CANT
MAGNITUDE
WE
EXAMINE
THE
EFFECTS
OF
MIGRATIONS
IN
DETAIL
IN
SECTION
THE
POLICY
ACHIEVES
TIMES
IMPROVEMENT
IN
MTTF
HOWEVER
THIS
COMES
AT
THE
COST
OF
A
DRASTIC
PERFORMANCE
AND
ENERGY
COST
ALTHOUGH
COULD
BE
UTILIZED
EFFECTIVELY
AS
A
BACK
UP
POLICY
FOR
THERMAL
EMERGENCIES
TO
GUARANTEE
THAT
TEMPERATURE
DOES
NOT
EXCEED
A
GIVEN
PEAK
VALUE
IT
IS
INEFFICIENT
IF
USED
FREQUENTLY
AMONG
THE
DVFS
POLICIES
ACHIEVES
THE
BEST
PERFOR
MANCE
OF
LESS
THAN
DEGRADATION
WHILE
RESULTS
IN
THE
LONGEST
SYSTEM
LIFE
TIME
WITH
A
IMPROVEMENT
THE
HYBRID
POLICY
SEEMS
TO
PROVIDE
THE
BEST
TRADE
OFF
POINT
AMONG
THE
POLICIES
AS
IT
ACHIEVES
ALMOST
THE
SAME
MTTF
AS
WITH
BETTER
PERFORMANCE
AND
LOWER
ENERGY
CONSUMPTION
THE
REASON
THE
HYBRID
SCHEDULING
POLICIES
STILL
PRO
VIDE
ONLY
SMALL
GAINS
OVER
DVFS
POLICIES
ALONE
IS
THAT
WE
START
THE
EXPERIMENTS
WITH
AN
OPTIMAL
PLACEMENT
OF
IDLE
CORES
WE
EXAMINE
THIS
FURTHER
IN
SECTION
WE
EXPECT
THAT
AS
TECHNOLOGY
SCALING
CONTINUES
THE
BANDWIDTH
FOR
PERFORMING
VOLTAGE
SCALING
WILL
DECREASE
DUE
TO
THE
LEAKAGE
POWER
AND
TRANSISTOR
THRESHOLD
VOLTAGE
LIMITATIONS
THIS
SITUATION
WILL
REQUIRE
OTHER
MECHANISMS
FOR
MANAGING
POWER
AND
TEMPERA
TURE
IS
OUR
BEST
CANDIDATE
FOR
WORKLOAD
ALLOCA
FIGURE
EFFECT
OF
SYSTEM
UTILIZATION
IDLE
CORES
FIGURE
EFFECT
OF
SYSTEM
UTILIZATION
IDLE
CORES
TION
AS
IT
SIGNIFICANTLY
INCREASES
RELIABILITY
WITH
NEGLIGIBLE
PERFOR
MANCE
LOSS
FIGURE
SHOWS
THE
CORE
UTILIZATION
RESULTS
BECAUSE
CHIP
TEM
PERATURES
ARE
LOWER
OVERALL
THE
MAGNITUDE
OF
POTENTIAL
RELIABILITY
GAINS
IS
REDUCED
IN
FACT
POLICIES
THAT
ONLY
REACT
TO
THERMAL
THRESH
OLDS
SEE
NO
ACTIVITY
IN
THIS
SCENARIO
E
G
MIGRATION
ETC
AND
THEY
GIVE
THE
SAME
RESULTS
AS
THE
DEFAULT
POLICY
AS
THE
CORE
TEMPERATURES
DO
NOT
EXCEED
THE
THRESHOLD
POLICIES
THAT
PROACTIVELY
LOOK
FOR
OPPORTUNITIES
CAN
STILL
IMPROVE
PROCESSOR
LIFETIME
SIGNIFI
CANTLY
AND
EVEN
PROVIDES
SMALL
GAINS
POLICIES
THAT
PROACTIVELY
MIGRATE
BASED
ON
CURRENT
TEMPERATURE
BALANCE
MAKE
MISTAKES
AND
CREATE
THERMAL
CYCLING
EFFECT
OF
INITIAL
IDLE
CORE
LOCATIONS
IN
THIS
SECTION
WE
EXAMINE
EACH
POLICY
ABILITY
TO
ADAPT
TO
DIF
FERENT
INITIAL
WORKLOAD
MAPPINGS
ON
THE
PROCESSOR
TOPOLOGY
FOR
EX
AMPLE
WHAT
HAPPENS
WHEN
THE
INITIAL
MAPPING
OF
THREADS
TO
CORES
IS
HIGHLY
SUBOPTIMAL
THIS
COULD
HAPPEN
WITH
A
TOPOLOGY
IGNORANT
SCHEDULER
A
LIKELY
SCENARIO
EARLY
ON
OR
JUST
BECAUSE
OF
JOBS
ENTER
ING
OR
LEAVING
THE
SYSTEM
WE
EXAMINED
SEVERAL
WAYS
OF
PERFORMING
THE
INITIAL
ALLOCATION
BEST
POSSIBLE
WORST
POSSIBLE
AND
AN
IN
ORDER
PLACEMENT
OF
JOBS
ON
CORES
THE
BEST
CASE
I
E
THE
CASE
WITH
THE
LOWEST
PEAK
TEMPERATURE
FOR
ACTIVE
THREADS
IS
LEAVING
THE
CENTER
CORES
IDLE
AND
FOR
ACTIVE
THREADS
WHEN
CORES
AND
ARE
IDLE
THE
WORST
CASE
OC
CURS
WHEN
THE
CORNER
CORES
ARE
IDLE
SPECIFICALLY
THE
WORST
CASE
FOR
A
SYSTEM
WITH
ACTIVE
CORES
IS
LEAVING
THE
CORES
AND
IDLE
SIMILARLY
WHEN
CORES
ARE
ACTIVE
LEAVING
TWO
OF
THE
CORNER
CORES
ON
THE
OPPOSITE
SIDES
IDLE
SUCH
AS
CORES
AND
REPRESENT
THE
WORST
ASSIGNMENT
THE
IN
ORDER
INITIAL
ASSIGNMENT
ALLOCATES
ALL
AVAILABLE
THREADS
ON
THE
CORES
STARTING
FROM
CORE
ASCENDING
THIS
METHOD
INITIALLY
LEAVES
CORES
IDLE
WHEN
THREADS
ARE
AC
TIVE
AND
CORES
IDLE
WITH
THREADS
ARE
ACTIVE
THE
IN
ORDER
METHOD
ATTEMPTS
TO
MODEL
A
NAIVE
SCHEDULER
THAT
ASSIGNS
JOBS
TO
CORES
USING
A
FIRST
AVAILABLE
STRATEGY
WE
HAVE
OBSERVED
NOTABLE
DIFFERENCES
IN
RELIABILITY
BETWEEN
THE
EXPERIMENTS
FOR
EXAMPLE
THE
POLICY
EXPERIENCES
A
REDUCTION
IN
MTTF
IN
COMPARISON
TO
THE
BEST
ALLOCATION
WHEN
EITHER
THE
WORST
OR
IN
ORDER
IDLE
CORE
LOCATIONS
ARE
USED
THIS
DE
CREASE
IN
RELIABILITY
IS
COMPARABLE
TO
THE
DEFAULT
POLICY
RE
DUCTION
IN
MTTF
WHEN
USING
THE
IN
ORDER
AND
WORST
CASE
INITIAL
ASSIGNMENTS
ON
THE
OTHER
HAND
WHEN
WAS
COMBINED
WITH
BAL
WE
WERE
ABLE
TO
ACHIEVE
A
LEVEL
OF
RELIABILITY
TO
MATCH
THAT
OF
THE
OPTIMAL
INITIAL
PLACEMENT
THIS
INDICATES
THAT
ONE
OF
THE
MAJOR
ROLES
OF
THE
ALLOCATION
POLICY
IS
REASSIGNING
THREAD
TOPOLOGIES
TO
ASSIST
OTHER
POLICIES
THAT
OPTIMALLY
SET
CORE
VOLTAGE
AND
FREQUENCY
THUS
IN
A
REAL
CMP
SYSTEM
IT
IS
CRITICAL
TO
COM
BINE
A
CONSERVATIVE
MIGRATION
TECHNIQUE
I
E
ONE
WHICH
AVOIDS
UN
NECESSARY
MIGRATIONS
AND
DOES
NOT
CREATE
CYCLING
WITH
DVFS
TECH
NIQUES
IN
THE
ABSENCE
OF
AN
INTELLIGENT
MIGRATION
AND
SCHEDULING
POLICY
IT
IS
DIFFICULT
TO
AVOID
DETRIMENTAL
CONFIGURATIONS
OVER
TIME
INTERACTIONS
WITH
POWER
MANAGEMENT
DYNAMIC
POWER
MANAGEMENT
DPM
TAKES
ADVANTAGE
OF
PRO
LONGED
CORE
IDLENESS
TO
PUT
THE
CORE
INTO
A
SLEEP
MODE
IN
SLEEP
THE
POWER
CONSUMPTION
OF
THE
CORE
IS
GREATLY
DIMINISHED
EACH
OF
THE
POLICIES
PRESENTED
IS
COMPATIBLE
WITH
DYNAMIC
POWER
MANAGE
MENT
BUT
SOME
ARE
ABLE
TO
USE
DPM
OPPORTUNITIES
BETTER
TAKING
A
CLOSER
LOOK
AT
TWO
EXTREMES
WE
FIRST
EXAMINE
TWO
POLICIES
MI
GRATION
AND
FOR
THE
WORKLOAD
WITH
CPU
AND
MEMORY
BOUND
THREADS
COMPARING
THE
THERMAL
TRACES
FOR
MIGRATION
AND
FIGURE
THE
MIGRATION
POLICY
SUFFERS
SIGNIFICANT
THERMAL
CY
CLE
VARIATIONS
FOR
MIGRATION
WE
DEMONSTRATE
THE
THERMAL
CYCLES
OBSERVED
ON
TWO
CORES
DUE
TO
FREQUENT
RE
ALLOCATION
OF
WORKLOADS
FOR
THE
POLICY
WE
SHOW
ALL
THE
CORES
THERMAL
TRACES
AND
OBSERVE
THAT
EACH
CORE
TEMPERATURE
IS
STABLE
AND
LOWER
THAN
THE
THRESHOLD
THIS
STABILITY
ALONG
WITH
A
LOWER
PEAK
TEMPERATURE
RESULTS
IN
SIGNIFICANTLY
HIGHER
RELIABILITY
TURNS
OUT
TO
BE
THE
BEST
POLICY
WHEN
PAIRED
WITH
DPM
AND
IT
PRO
VIDES
AN
INCREASE
IN
MTTF
OF
OVER
MIGRATION
WHILE
THE
PER
FORMANCE
DIFFERENCE
IS
ONLY
SO
WE
SEE
THAT
SCHEDULING
POLI
CIES
WHICH
EFFECTIVELY
MANAGE
THREAD
LOCATIONS
AND
DPM
POLICIES
CAN
REDUCE
PROCESSOR
TEMPERATURES
AND
IMPROVE
RELIABILITY
AT
THE
SAME
TIME
DPM
CAN
ALSO
LEAD
TO
GREATER
THERMAL
CYCLING
WHICH
CAN
COUNTERACT
SOME
OF
THE
MTTF
GAINS
THAT
RESULT
FROM
THE
LOWER
POWER
LEVELS
OF
SLEEPING
CORES
THE
ADVERSE
EFFECT
OF
DPM
ON
RE
LIABILITY
DUE
TO
THERMAL
CYCLES
IS
ALSO
EMPHASIZED
IN
PREVIOUS
WORK
THUS
WHEN
WE
INCLUDE
THE
EFFECTS
OF
THERMAL
CYCLING
FAIL
URES
WE
OBSERVE
THAT
THE
TRADITIONAL
ASSUMPTIONS
FOR
FINDING
OP
TIMAL
STRATEGIES
ARE
INCOMPLETE
IT
WOULD
BE
WISE
TO
RE
DESIGN
THE
DPM
POLICIES
WITH
A
RELIABILITY
PERSPECTIVE
DESPITE
DPM
POSSIBLE
IMPACT
ON
RELIABILITY
WE
DO
SEE
FIG
URE
THAT
EVEN
IN
THE
FACE
OF
THIS
CYCLING
PHENOMENA
DPM
WAS
AN
OVERALL
WIN
FOR
ALL
POLICIES
WITH
THE
EXCEPTION
OF
BALANCE
IN
THIS
FIGURE
WE
SHOW
THE
AVERAGE
RESULTS
OVER
HETEROGENEOUS
CPU
BOUND
WORKLOADS
THE
REASON
THAT
BALANCE
RECEIVED
NO
BENEFIT
FROM
USING
DPM
IS
ITS
PROACTIVE
MECHANISM
THAT
KEEPS
MOVING
HOT
THREADS
TO
COLDER
CORES
THE
RESULT
IS
THAT
NO
CORE
IS
IDLE
LONG
ENOUGH
TO
TRIGGER
THE
SLEEP
MODE
ON
THE
OTHER
END
OF
THE
SPECTRUM
MIGRATION
AND
SHOW
GAINS
OF
AND
IN
MTTF
FOR
THE
AVERAGE
CASE
RESPECTIVELY
THE
RELIABILITY
IMPROVEMENT
IN
DVFS
BASED
TECHNIQUES
ARE
LESS
PROMINENT
AND
RANGE
BETWEEN
MTTF
INCREASE
THE
POLICY
RECEIVES
A
LARGE
BENEFIT
IN
ENERGY
FROM
USING
DPM
MECHANISMS
REDUCING
POWER
CONSUMPTION
BY
IN
COM
PARISON
TO
THE
NO
DPM
CASE
IF
CONFRONTED
WITH
A
DESIGN
CHOICE
THAT
FIGURE
A
CYCLES
CAUSED
BY
THE
MIGRATION
POLICY
B
STABLE
THERMAL
PROFILE
OF
MIGRATIONS
ALL
CORNER
CENTER
SIDE
BALANCE
MIGRATION
V
F
SETTING
CHANGES
BALANCE_LOC
TABLE
NUMBER
OF
MIGRATIONS
AND
V
F
CHANGES
PER
SECOND
FIGURE
MTTF
AND
ENERGY
EFFECTS
OF
DPM
REQUIRES
THE
SIMPLICITY
OF
DPM
COULD
HELP
REGAIN
MUCH
OF
THE
ENERGY
LOST
FROM
THE
CONSTANT
START
AND
STOP
OF
INDIVIDUAL
CORES
IN
TABLE
WE
SHOW
THE
NUMBER
OF
MIGRATIONS
AND
NUMBER
OF
V
F
SETTING
CHANGES
PER
SECOND
FOR
THE
POLICIES
TO
PROVIDE
A
MORE
COM
PLETE
UNDERSTANDING
OF
THE
RUNTIME
BEHAVIOR
THE
POLICIES
THAT
ARE
NOT
LISTED
DO
NOT
UTILIZE
MIGRATIONS
OR
DVFS
THE
COLUMNS
MARKED
AS
ALL
CORNER
CENTER
AND
SIDE
REFER
TO
THE
AVERAGE
NUMBER
ACROSS
ALL
CORES
ACROSS
ONLY
THE
CORNER
CORES
CENTER
CORES
AND
SIDE
CORES
RESPECTIVELY
THE
RESULTS
ARE
WITH
DPM
AND
FOR
THE
CPU
BOUND
HETEROGENEOUS
WORKLOAD
WITH
THREADS
I
E
IDLE
CORES
MI
GRATION
HAS
A
SIGNIFICANTLY
HIGHER
NUMBER
OF
THREAD
MOVEMENTS
IN
COMPARISON
TO
OTHER
POLICIES
ALMOST
TIMES
MORE
THAN
BAL
THE
LOW
MIGRATION
COUNT
OF
IS
A
RESULT
OF
ITS
ABILITY
TO
MATCH
THE
PERFORMANCE
CHARACTERISTICS
OF
APPLICATIONS
WITH
THE
THERMAL
BEHAVIOR
OF
CORES
COMPARED
TO
BAL
ONLY
COMBINING
WITH
DVFS
IN
CREASES
THE
FREQUENCY
OF
MIGRATIONS
AS
THE
TEMPERATURE
PROFILE
OF
THE
CORES
VARY
MORE
WHEN
THEIR
V
F
SETTINGS
ARE
DYNAMICALLY
AD
JUSTED
AMONG
THE
DVFS
POLICIES
HAS
THE
LOWEST
NUMBER
OF
CHANGES
AS
IT
ONLY
ALTERS
THE
V
F
SETTING
OF
APPLICATIONS
TOLERANT
TO
OPERATING
AT
A
SLOWER
SPEED
ALSO
REDUCES
THE
FRE
QUENCY
OF
CHANGES
IN
COMPARISON
TO
AS
IT
PROACTIVELY
ADJUSTS
THE
V
F
SETTING
AND
TRIGGERS
THE
THERMAL
THRESHOLD
FEWER
TIMES
TO
BETTER
UNDERSTAND
THE
TENSION
BETWEEN
THE
DIFFERENT
FAILURE
MECHANISMS
FIGURE
PRESENTS
A
BREAKDOWN
OF
THE
CONTRIBUTION
OF
DIFFERENT
FAILURE
TYPES
TO
RELIABILITY
THIS
FIGURE
DEMONSTRATES
THE
NORMALIZED
AVERAGE
FAILURE
RATE
FOR
OUR
TWO
BEST
AND
TWO
WORST
POLICIES
I
E
BEST
WORST
IN
TERMS
OF
THEIR
AVERAGE
MTTF
RESULTS
THE
WORKLOAD
FOR
THIS
EXPERIMENT
IS
THE
HETEROGENEOUS
CPU
BOUND
WORKLOAD
WITH
THREADS
RECALL
THAT
THE
FAILURE
RATE
IS
INVERSELY
PROPORTIONAL
TO
MTTF
THIS
FIGURE
SHOWS
THAT
BALANCE
AND
MI
GRATION
REDUCE
THE
PROBABILITY
OF
FAILURES
DUE
TO
ELECTROMIGRATION
EM
AND
DIELECTRIC
BREAKDOWN
TDDB
IF
WE
IGNORED
THE
EFFECT
FIGURE
CONTRIBUTIONS
OF
FAILURE
MECHANISMS
OF
THERMAL
CYCLES
WE
WOULD
CONCLUDE
THAT
RELIABILITY
HAD
INCREASED
HOWEVER
BECAUSE
OF
THE
NUMBER
OF
THREAD
MIGRATIONS
THEY
CREATE
LARGE
THERMAL
CYCLES
TC
THE
AND
THE
HYBRID
BAL
POLICIES
ON
THE
OTHER
HAND
REDUCE
THE
FAILURE
RATES
CAUSED
BY
THERMAL
HOT
SPOTS
WITHOUT
INTRODUCING
A
SIGNIFICANT
AMOUNT
OF
THERMAL
CYCLING
FAILURES
NOTE
THAT
IN
THE
DEFAULT
CASE
AS
THERE
IS
NO
WORKLOAD
RE
ALLOCATION
TEMPERATURE
IS
STABLE
AND
NO
CYCLES
ARE
OBSERVED
CONCLUSIONS
THIS
PAPER
ANALYZES
HOW
JOB
SCHEDULING
AND
POWER
MANAGEMENT
POLICIES
AFFECT
SYSTEM
LIFETIME
IT
DEMONSTRATES
A
NOVEL
CMP
SIMU
LATION
FRAMEWORK
WHICH
IS
ABLE
TO
SIMULATE
THERMAL
DYNAMICS
OVER
FAR
LONGER
TIME
PERIODS
THAN
TYPICAL
ARCHITECTURAL
SIMULATORS
AT
HIGH
ACCURACY
IT
EVALUATES
A
NUMBER
OF
TECHNIQUES
IN
TERMS
OF
THEIR
EF
FECT
ON
RELIABILITY
TEMPERATURE
ENERGY
AND
PERFORMANCE
THE
RESULTS
IN
THIS
PAPER
PROVIDE
SEVERAL
KEY
INSIGHTS
THAT
WILL
SERVE
US
WELL
IN
THE
DESIGN
OF
FUTURE
THERMAL
MANAGEMENT
POLICIES
IT
IS
CRITICAL
TO
CONSIDER
THERMAL
CYCLING
EFFECTS
IN
ADDITION
TO
PEAK
TEMPERATURE
EFFECTS
WE
SAW
TWO
POLICIES
THAT
ERRONEOUSLY
APPEAR
TO
INCREASE
LIFETIME
WHEN
THERMAL
CYCLING
WAS
IGNORED
THERMAL
CYCLING
IS
NOT
A
SIGNIFICANT
EFFECT
IN
A
FULLY
UTILIZED
SYS
TEM
AS
THE
VARIANCE
IN
POWER
BETWEEN
RUNNING
THREADS
WAS
NOT
SHOWN
TO
BE
SUFFICIENTLY
HIGH
TO
CAUSE
HARMFUL
EFFECTS
HOWEVER
WHEN
CORES
ARE
IDLE
IT
IS
IMPORTANT
THAT
WE
MANAGE
THE
IDLE
CORES
IN
A
WAY
THAT
DOES
NOT
EXACERBATE
THERMAL
CYCLING
CONSERVATIVE
POLICIES
THAT
MINIMIZE
MIGRATION
NOT
ONLY
REDUCE
THERMAL
CYCLING
BUT
ALSO
MAXIMIZE
OUR
ABILITY
TO
EXPLOIT
SLEEP
STATES
VIA
DPM
UNDERSTANDING
THERMAL
ASYMMETRIES
WHICH
ARE
EITHER
DUE
TO
THE
LAYOUT
OF
THE
PROCESSOR
OR
DUE
TO
PROCESS
VARIATION
IS
CRITICAL
TO
EFFECTIVE
THERMAL
MANAGEMENT
NOT
UNDERSTANDING
THERMAL
VARIANCE
CAUSES
MUCH
UNNECESSARY
MOVEMENT
BECAUSE
WE
CAN
NOT
DISCERN
BETWEEN
A
HOT
THREAD
AND
A
HOT
CORE
UNDERSTANDING
THE
THERMAL
VARIANCE
ALLOWS
US
TO
EMPLOY
AN
ASYMMETRIC
THERMAL
POLICY
THAT
ACCOUNTS
FOR
AND
EVEN
EXPLOITS
THAT
ASYMMETRY
PROACTIVE
TECHNIQUES
THAT
APPLY
DVFS
TO
FREQUENCY
TOLERANT
AP
PLICATIONS
CAN
RAISE
THE
PERFORMANCE
OF
THE
ENTIRE
SYSTEM
THIS
IS
SOMEWHAT
NON
INTUITIVE
AS
THE
FREQUENCY
TOLERANT
APPLICATIONS
ARE
ALSO
THE
COOLEST
APPLICATIONS
HOWEVER
BY
LOWERING
OVERALL
TEMPERATURES
CHIP
WIDE
THIS
ALLOWS
THE
HOT
APPLICATIONS
TO
RUN
LONGER
WITHOUT
TRIGGERING
THERMAL
EVENTS
IN
FUTURE
WORK
WE
WILL
BE
ADDRESSING
RELIABILITY
MANAGEMENT
OF
MULTITHREADED
MULTICORE
SYSTEMS
WE
WILL
SEEK
TO
PROVIDE
A
COM
PREHENSIVE
UNDERSTANDING
OF
HOW
PARALLEL
WORKLOADS
DIFFER
FROM
SINGLE
THREADED
BENCHMARKS
AND
PROPOSE
NOVEL
MANAGEMENT
TECH
NIQUES
TO
ADDRESS
THE
PARTICULAR
CHARACTERISTICS
OF
SUCH
WORKLOADS
ABSTRACT
SPEED
SCALING
IS
A
POWER
MANAGEMENT
TECHNIQUE
THAT
INVOLVES
DYNAMICALLY
CHANGING
THE
SPEED
OF
A
PROCESSOR
THIS
GIVES
RISE
TO
DUAL
OBJECTIVE
SCHEDULING
PROBLEMS
WHERE
THE
OPERATING
SYSTEM
BOTH
WANTS
TO
CONSERVE
ENERGY
AND
OPTIMIZE
SOME
QUALITY
OF
SERVICE
QOS
MEASURE
OF
THE
RESULTING
SCHEDULE
IN
THE
MOST
INVESTIGATED
SPEED
SCALING
PROBLEM
IN
THE
LITERATURE
THE
QOS
CONSTRAINT
IS
DEADLINE
FEASIBILITY
AND
THE
OBJECTIVE
IS
TO
MINIMIZE
THE
ENERGY
USED
THE
STANDARD
ASSUMPTION
IS
THAT
THE
POWER
CONSUMPTION
IS
THE
SPEED
TO
SOME
CONSTANT
POWER
Α
WE
GIVE
THE
FIRST
NON
TRIVIAL
LOWER
BOUND
NAMELY
EΑ
Α
ON
THE
COMPETITIVE
RATIO
FOR
THIS
PROBLEM
THIS
COMES
CLOSE
TO
THE
BEST
UPPER
BOUND
WHICH
IS
ABOUT
WE
ANALYZE
A
NATURAL
CLASS
OF
ALGORITHMS
CALLED
QOA
WHERE
AT
ANY
TIME
THE
PROCESSOR
WORKS
AT
Q
TIMES
THE
MINIMUM
SPEED
REQUIRED
TO
ENSURE
FEASIBILITY
ASSUMING
NO
NEW
JOBS
ARRIVE
FOR
CMOS
BASED
PROCESSORS
AND
MANY
OTHER
TYPES
OF
DEVICES
Α
THAT
IS
THEY
SATISFY
THE
CUBE
ROOT
RULE
WHEN
Α
WE
SHOW
THAT
QOA
IS
COMPETITIVE
IMPROVING
UPON
THE
PREVIOUS
BEST
GUARANTEE
OF
ACHIEVED
BY
THE
ALGORITHM
OPTIMAL
AVAILABLE
OA
SO
WHEN
THE
CUBE
ROOT
RULE
HOLDS
OUR
RESULTS
REDUCE
THE
RANGE
FOR
THE
OPTIMAL
COMPETITIVE
RATIO
FROM
TO
WE
ALSO
ANALYZE
QOA
FOR
GENERAL
Α
AND
GIVE
ALMOST
MATCHING
UPPER
AND
LOWER
BOUNDS
INTRODUCTION
CURRENT
PROCESSORS
PRODUCED
BY
INTEL
AND
AMD
ALLOW
THE
SPEED
OF
THE
PROCESSOR
TO
BE
CHANGED
DYNAMICALLY
INTEL
SPEEDSTEP
AND
AMD
POWERNOW
TECHNOLO
GIES
ALLOW
THE
WINDOWS
XP
OPERATING
SYSTEM
TO
DYNAMICALLY
CHANGE
THE
SPEED
OF
SUCH
A
PROCESSOR
TO
CONSERVE
ENERGY
IN
THIS
SETTING
THE
OPERATING
SYSTEM
MUST
NOT
ONLY
HAVE
A
JOB
SELECTION
POLICY
TO
DETERMINE
WHICH
JOB
TO
RUN
BUT
ALSO
A
SPEED
SCALING
POLICY
TO
DETERMINE
THE
SPEED
AT
WHICH
THE
JOB
WILL
BE
RUN
ALL
THEORETICAL
STUDIES
WE
KNOW
OF
ASSUME
A
SPEED
TO
POWER
FUNCTION
P
SΑ
WHERE
IS
THE
THE
WORK
OF
H
L
CHAN
WAS
DONE
WHEN
HE
WAS
A
POSTDOC
IN
UNIVERSITY
OF
PITTSBURGH
K
PRUHS
WAS
SUPPORTED
IN
PART
BY
NSF
GRANTS
CNS
CCF
IIS
AND
CCF
AND
AN
IBM
FACULTY
AWARD
SPEED
AND
Α
IS
SOME
CONSTANT
ENERGY
CONSUMPTION
IS
POWER
INTEGRATED
OVER
TIME
THE
OPERATING
SYSTEM
IS
FACED
WITH
A
DUAL
OBJECTIVE
OPTIMIZATION
PROBLEM
AS
IT
BOTH
WANTS
TO
CONSERVE
ENERGY
AND
OPTIMIZE
SOME
QUALITY
OF
SERVICE
QOS
MEASURE
OF
THE
RESULTING
SCHEDULE
THE
FIRST
THEORETICAL
STUDY
OF
SPEED
SCALING
ALGORITHMS
WAS
IN
THE
SEMINAL
PAPER
BY
YAO
DEMERS
AND
SHENKER
IN
THE
PROBLEM
INTRODUCED
IN
THE
QOS
OBJECTIVE
WAS
DEADLINE
FEASIBILITY
AND
THE
OBJECTIVE
WAS
TO
MINIMIZE
THE
ENERGY
USED
TO
DATE
THIS
IS
THE
MOST
INVESTIGATED
SPEED
SCALING
PROBLEM
IN
THE
LITERATURE
IN
THIS
PROBLEM
EACH
JOB
I
HAS
A
RELEASE
TIME
RI
WHEN
IT
ARRIVES
IN
THE
SYSTEM
A
WORK
REQUIREMENT
WI
AND
A
DEADLINE
DI
BY
WHICH
THE
JOB
MUST
BE
FINISHED
THE
DEADLINES
MIGHT
COME
FROM
THE
APPLICATION
OR
MIGHT
ARISE
FROM
THE
SYSTEM
IMPOSING
A
WORST
CASE
QUALITY
OF
SERVICE
METRIC
SUCH
AS
MAXIMUM
RESPONSE
TIME
OR
MAXIMUM
SLOW
DOWN
IT
IS
CLEAR
THAT
AN
OPTIMAL
JOB
SELECTION
POLICY
IS
EARLIEST
DEADLINE
FIRST
EDF
THUS
THE
REMAINING
ISSUE
IS
TO
FIND
AN
ONLINE
SPEED
SCALING
POLICY
TO
MINIMIZE
ENERGY
THE
STORY
TO
DATE
YAO
DEMERS
AND
SHENKER
SHOWED
THAT
THE
OPTIMAL
OFFLINE
SCHEDULE
CAN
BE
EFFI
CIENTLY
COMPUTED
BY
A
GREEDY
ALGORITHM
PROPOSED
TWO
NATURAL
ONLINE
SPEED
SCALING
ALGORITHMS
AVERAGE
RATE
AVR
AND
OPTIMAL
AVAILABLE
OA
CONCEPTUALLY
AVR
IS
OBLIVIOUS
IN
THAT
IT
RUNS
EACH
JOB
IN
THE
WAY
THAT
WOULD
BE
OPTIMAL
IF
THERE
WERE
NO
OTHER
JOBS
IN
THE
SYSTEM
THAT
IS
AVR
RUNS
EACH
JOB
I
IN
PARALLEL
WITH
OTHER
JOBS
AT
THE
CONSTANT
SPEED
WI
DI
RI
THROUGH
OUT
INTERVAL
RI
DI
THE
ALGORITHM
OA
MAINTAINS
THE
INVARIANT
THAT
THE
SPEED
AT
EACH
TIME
IS
OPTIMAL
GIVEN
THE
CURRENT
STATE
AND
UNDER
THE
ASSUMPTION
THAT
NO
MORE
JOBS
WILL
ARRIVE
IN
THE
FUTURE
IN
PARTICULAR
LET
W
X
DENOTE
THE
AMOUNT
OF
UNFINISHED
WORK
THAT
HAS
DEADLINE
WITHIN
X
TIME
UNITS
FROM
THE
CUR
RENT
TIME
THEN
THE
CURRENT
SPEED
OF
OA
IS
MAXX
W
X
X
ANOTHER
ONLINE
AL
GORITHM
BKP
IS
PROPOSED
IN
BKP
RUNS
AT
SPEED
E
V
T
AT
TIME
T
WHERE
V
T
MAXT
T
W
T
ET
E
T
T
E
T
T
AND
W
T
IS
THE
AMOUNT
OF
WORK
THAT
HAS
RELEASE
TIME
AT
LEAST
DEADLINE
AT
MOST
AND
THAT
HAS
ALREADY
ARRIVED
BY
TIME
T
CLEARLY
IF
W
IS
THE
TOTAL
WORK
OF
JOBS
THAT
ARE
RELEASED
AFTER
AND
HAVE
DEADLINE
BEFORE
THEN
ANY
ALGORITHM
MUST
HAVE
AN
AVERAGE
SPEED
OF
AT
LEAST
W
DURING
THUS
BKP
CAN
BE
VIEWED
AS
COMPUTING
A
LOWER
BOUND
ON
THE
AVERAGE
SPEED
IN
AN
ONLINE
MANNER
AND
RUNNING
AT
E
TIMES
THAT
SPEED
TABLE
SUMMARIZES
THE
PREVIOUS
RESULTS
THE
COMPETITIVE
RATIO
OF
AVR
IS
AT
MOST
THIS
WAS
FIRST
SHOWN
IN
AND
A
SIMPLER
AMORTIZED
LOCAL
COMPETITIVENESS
ANALYSIS
WAS
GIVEN
IN
THE
COMPETITIVE
RATIO
OF
AVR
IS
LEAST
Δ
Α
WHERE
Δ
IS
A
FUNCTION
OF
Α
THAT
APPROACHES
ZERO
AS
Α
APPROACHES
INFINITY
THE
COMPETITIVE
RATIO
OF
OA
IS
EXACTLY
ΑΑ
WHERE
THE
UPPER
BOUND
IS
PROVED
USING
AN
AMORTIZED
LOCAL
COMPETITIVENESS
ARGUMENT
THUS
THE
COMPETITIVE
RATIO
OF
AVR
IS
STRICTLY
INFERIOR
TO
THAT
OF
OA
THE
COMPETITIVE
RATIO
OF
BKP
IS
AT
MOST
Α
Α
ΑEΑ
WHICH
IS
ABOUT
FOR
LARGE
Α
IT
IS
BETTER
THAN
THAT
OF
OA
ONLY
FOR
Α
ON
THE
OTHER
HAND
THE
LOWER
BOUNDS
FOR
PREVIOUS
RESULTS
ALGORITHM
GENERAL
Α
Α
Α
UPPER
LOWER
UPPER
LOWER
UPPER
LOWER
GENERAL
Α
AVR
Α
Α
Δ
Α
48
OA
ΑΑ
ΑΑ
BKP
Α
Α
Α
Α
E
OUR
CONTRIBUTIONS
ALGORITHM
GENERAL
Α
Α
Α
UPPER
LOWER
UPPER
LOWER
UPPER
LOWER
GENERAL
EΑ
Α
QOA
EΑ
Α
Α
TABLE
RESULTS
ON
THE
COMPETITIVE
RATIO
FOR
ENERGY
MINIMIZATION
WITH
DEADLINE
FEA
SIBILITY
GENERAL
ALGORITHMS
ARE
RATHER
WEAK
SOMEWHAT
SURPRISINGLY
THE
BEST
KNOWN
LOWER
BOUND
INSTANCE
IS
THE
WORST
POSSIBLE
INSTANCE
CONSISTING
OF
TWO
JOBS
SHOWS
A
LOWER
BOUND
OF
ON
THE
COMPETITIVE
RATIO
USING
A
TWO
JOB
INSTANCE
IF
ONE
TRIES
TO
FIND
THE
WORST
JOB
INSTANCES
THE
CALCULATIONS
GET
MESSY
QUICKLY
THE
MOST
INTERESTING
VALUE
OF
Α
SEEMS
TO
BE
THREE
MOST
IMPORTANTLY
IN
CURRENT
CMOS
BASED
PROCESSORS
THE
SPEED
SATISFIES
THE
WELL
KNOWN
CUBE
ROOT
RULE
THAT
THE
SPEED
IS
APPROXIMATELY
THE
CUBE
ROOT
OF
THE
POWER
THE
POWER
IS
ALSO
ROUGHLY
PROPORTIONAL
TO
THE
CUBE
OF
THE
SPEED
IN
MANY
COMMON
DE
VICES
MACHINES
SUCH
AS
VEHICLES
AUTOMOBILES
AND
SOME
TYPES
OF
MOTORS
IT
SEEMS
LIKELY
THAT
Α
WOULD
BE
IN
THE
RANGE
FOR
MOST
CONCEIVABLE
DEVICES
THE
BEST
KNOWN
GUARANTEE
FOR
Α
IN
THIS
RANGE
IS
ΑΑ
ACHIEVED
BY
OA
WHICH
EVALUATES
TO
FOR
Α
AND
FOR
Α
OUR
MOTIVATING
GOAL
IS
TO
FOCUS
ON
THE
CASE
THAT
Α
AND
TO
A
LESSER
EXTENT
ON
Α
AND
TO
OBTAIN
BETTER
ALGORITHMS
AND
LOWER
BOUNDS
IN
THESE
CASES
OUR
CONTRIBUTIONS
WE
SHOW
USING
AN
AMORTIZED
LOCAL
COMPETITIVENESS
ANALYSIS
THAT
IF
Q
IS
SET
TO
THEN
THE
COMPETITIVE
RATIO
OF
QOA
IS
AT
MOST
EΑ
THIS
BOUND
IS
APPROXIMATELY
WHEN
Α
AND
WHEN
Α
USING
AN
ANALYSIS
SPECIALIZED
TO
THE
SPECIFIC
CASES
THAT
Α
AND
Α
WE
SHOW
THAT
QOA
IS
AT
WORST
COMPETITIVE
WHEN
Α
AND
AT
WORST
COMPETITIVE
WHEN
Α
OUR
MAIN
TECHNICAL
IDEA
IS
TO
INTRODUCE
A
NEW
POTENTIAL
FUNCTION
WHICH
IS
QUITE
DIFFERENT
FROM
THE
ONE
USED
IN
THE
ANALYSIS
OF
OA
IN
AND
THE
POTENTIAL
FUNCTION
USED
TO
ANALYZE
AVR
IN
THIS
IS
NECESSARY
SINCE
POTENTIAL
FUNCTIONS
SIMILAR
TO
THOSE
USED
EARLIER
CANNOT
YIELD
GUARANTEES
OF
THE
FORM
CΑ
WHERE
C
IS
INDEPENDENT
OF
Α
THE
POTENTIAL
FUNCTION
WE
USE
IS
MORE
SIMILAR
TO
THE
ONE
USED
IN
TO
ANALYZE
A
SPEED
SCALING
ALGORITHM
FOR
THE
DIFFERENT
OBJECTIVE
OF
MINIMIZING
FLOW
TIME
PLUS
ENERGY
HOWEVER
HERE
WE
WILL
NEED
A
DIFFERENT
ANALYSIS
APPROACH
THE
ANALYSIS
IN
AND
ALMOST
ALL
OF
THE
AMORTIZED
LOCAL
COMPETITIVENESS
ANALYSES
IN
THE
SPEED
SCALING
LITERATURE
RELY
CRITICALLY
ON
THE
YOUNG
INEQUALITY
HOWEVER
IN
THE
CURRENT
SETTING
YOUNG
INEQUALITY
GIVES
A
BOUND
THAT
IS
TOO
WEAK
TO
BE
USEFUL
WHEN
ANALYZING
QOA
THE
KEY
INSIGHT
THAT
ALLOWS
US
TO
AVOID
THE
USE
OF
YOUNG
INEQUALITY
WAS
TO
OBSERVE
THAT
CERTAIN
EXPRESSIONS
THAT
ARISE
IN
THE
ANALYSIS
ARE
CONVEX
WHICH
ALLOWS
US
TO
REDUCE
THE
ANALYSIS
OF
THE
GENERAL
CASE
DOWN
TO
JUST
TWO
EXTREME
CASES
TO
THE
BEST
OF
OUR
KNOWLEDGE
THIS
CONVEXITY
TECHNIQUE
CAN
REPLACE
ALL
OF
THE
USES
OF
YOUNG
INEQUALITY
IN
THE
SPEED
SCALING
LITERATURE
IN
ALL
CASES
THE
RESULTING
BOUND
THAT
ONE
OBTAINS
USING
THIS
CONVEXITY
TECHNIQUE
IS
AT
LEAST
AS
GOOD
AS
THE
BOUND
THAT
ONE
OBTAINS
USING
YOUNG
INEQUALITY
AND
THE
RESULTING
PROOF
IS
SIMPLER
AND
MORE
INTUITIVE
IN
SOME
CASES
THIS
CONVEXITY
TECHNIQUE
GIVES
A
BETTER
BOUND
FOR
EXAMPLE
IF
ONE
APPLIES
THIS
CONVEXITY
TECHNIQUE
TO
THE
ANALYSIS
OF
THE
LAPS
ALGORITHM
IN
ONE
OBTAINS
A
BOUND
ON
THE
COMPETITIVE
RATIO
OF
O
Α
WHEREAS
USING
YOUNG
TECHNIQUE
ONE
CAN
ONLY
GET
A
BOUND
OF
O
IN
SECTION
WE
CONSIDER
LOWER
BOUNDS
WE
GIVE
THE
FIRST
NON
TRIVIAL
LOWER
BOUND
ON
THE
COMPETITIVE
RATIO
FOR
ANY
ALGORITHM
WE
SHOW
THAT
EVERY
DETER
MINISTIC
ALGORITHM
MUST
HAVE
A
COMPETITIVE
RATIO
OF
AT
LEAST
EΑ
Α
THE
BASE
OF
THE
EXPONENT
E
IS
THE
BEST
POSSIBLE
SINCE
BKP
ACHIEVES
A
RATIO
OF
ABOUT
FOR
Α
THIS
RAISES
THE
BEST
KNOWN
LOWER
BOUND
A
MODEST
AMOUNT
FROM
TO
THE
INSTANCE
IS
IDENTICAL
TO
THE
ONE
USED
IN
TO
LOWER
BOUND
THE
COMPETI
TIVE
RATIO
WITH
RESPECT
TO
THE
OBJECTIVE
OF
MINIMIZING
THE
MAXIMUM
SPEED
THE
INNOVATION
REQUIRED
TO
GET
A
LOWER
BOUND
FOR
ENERGY
IS
TO
CATEGORIZE
THE
VARIETY
OF
POSSIBLE
SPEED
SCALING
POLICIES
IN
SUCH
A
WAY
THAT
ONE
CAN
EFFECTIVELY
REASON
ABOUT
THEM
GIVEN
THE
GENERAL
LOWER
BOUND
OF
EΑ
Α
AND
THAT
BKP
ACHIEVES
A
RATIO
WITH
BASE
OF
EXPONENT
E
A
NATURAL
QUESTION
IS
WHETHER
THERE
IS
SOME
CHOICE
OF
THE
PARAMETER
Q
FOR
WHICH
THE
COMPETITIVE
RATIO
OF
QOA
VARIES
WITH
E
AS
THE
BASE
OF
THE
EXPONENT
SOMEWHAT
SURPRISINGLY
WE
SHOW
THAT
THIS
IS
NOT
THE
CASE
AND
THE
BASE
OF
THE
EXPONENT
CANNOT
BE
IMPROVED
BEYOND
IN
PARTICULAR
WE
SHOW
THAT
THE
COMPETITIVE
RATIO
OF
QOA
IS
AT
LEAST
Α
WE
NOTE
THAT
THIS
LOWER
BOUND
IS
QUITE
CLOSE
TO
OUR
UPPER
BOUND
FOR
QOA
ESPECIALLY
AS
Α
INCREASES
OUR
RESULTS
ARE
SUMMARIZED
IN
THE
LAST
TWO
ROWS
OF
TABLE
IN
PARTICULAR
WE
GIVE
ASYMPTOTICALLY
MATCHING
UPPER
AND
LOWER
BOUNDS
FOR
QOA
AND
REDUCE
THE
RANGE
FOR
THE
OPTIMAL
COMPETITIVE
RATIO
IN
THE
CASE
THAT
THE
CUBE
ROOT
RULE
HOLDS
FROM
TO
AND
IN
THE
CASE
THAT
Α
FROM
OBTAINED
IN
TO
DUE
TO
THE
LIMITATION
OF
SPACE
SOME
PROOFS
ARE
OMITTED
AND
WILL
BE
GIVEN
IN
THE
FULL
PAPER
OTHER
RELATED
RESULTS
THERE
ARE
NOW
ENOUGH
SPEED
SCALING
PAPERS
IN
THE
LITERATURE
THAT
IT
IS
NOT
PRAC
TICAL
TO
SURVEY
ALL
SUCH
PAPERS
HERE
WE
LIMIT
OURSELVES
TO
THOSE
PAPERS
MOST
RELATED
TO
THE
RESULTS
PRESENTED
HERE
A
NAIVE
IMPLEMENTATION
OF
YDS
RUNS
IN
TIME
O
THIS
CAN
BE
IMPROVED
TO
O
IF
THE
INTERVALS
HAVE
A
TREE
STRUCTURE
LI
YAO
AND
YAO
GAVE
AN
IMPLEMENTATION
THAT
RUNS
IN
O
LOG
N
TIME
FOR
THE
GENERAL
CASE
FOR
HARD
REAL
TIME
JOBS
WITH
FIXED
PRIORITIES
YUN
AND
KIM
SHOWED
THAT
IT
IS
NP
HARD
TO
COMPUTE
A
MINIMUM
ENERGY
SCHEDULE
THEY
ALSO
GAVE
A
FULLY
POLYNOMIAL
TIME
APPROXIMATION
SCHEME
FOR
THE
PROBLEM
KWON
AND
KIM
GAVE
A
POLYNOMIAL
TIME
ALGORITHM
FOR
THE
CASE
OF
A
PROCESSOR
WITH
DISCRETE
SPEEDS
LI
AND
YAO
GAVE
AN
ALGORITHM
WITH
RUNNING
TIME
O
D
N
LOG
N
WHERE
D
IS
THE
NUMBER
OF
SPEEDS
A
SIMPLER
ALGORITHM
WITH
THIS
RUNNING
TIME
CAN
BE
FOUND
IN
ALBERS
MU
LLER
AND
SCHMELZER
CONSIDER
THE
PROBLEM
OF
FINDING
ENERGY
EFFICIENT
DEADLINE
FEASIBLE
SCHEDULES
ON
MULTIPROCESSORS
SHOWED
THAT
THE
OF
FLINE
PROBLEM
IS
NP
HARD
AND
GAVE
O
APPROXIMATION
ALGORITHMS
ALSO
GAVE
ONLINE
ALGORITHMS
THAT
ARE
O
COMPETITIVE
WHEN
JOB
DEADLINES
OCCUR
IN
THE
SAME
ORDER
AS
THEIR
RELEASE
TIMES
CHAN
ET
AL
CONSIDERED
THE
MORE
GENERAL
AND
REALISTIC
SPEED
SCALING
SETTING
WHERE
THERE
IS
AN
UPPER
BOUND
ON
THE
MAXI
MUM
PROCESSOR
SPEED
THEY
GAVE
AN
O
COMPETITIVE
ALGORITHM
BASED
ON
OA
RECENTLY
BANSAL
CHAN
AND
PRUHS
INVESTIGATED
SPEED
SCALING
FOR
DEADLINE
FEASIBILITY
IN
DEVICES
WITH
A
REGENERATIVE
ENERGY
SOURCE
SUCH
AS
A
SOLAR
CELL
FORMAL
PROBLEM
STATEMENT
A
PROBLEM
INSTANCE
CONSISTS
OF
N
JOBS
JOB
I
HAS
A
RELEASE
TIME
RI
A
DEADLINE
DI
RI
AND
WORK
WI
IN
THE
ONLINE
VERSION
OF
THE
PROBLEM
THE
SCHEDULER
LEARNS
ABOUT
A
JOB
ONLY
AT
ITS
RELEASE
TIME
AT
THIS
TIME
THE
SCHEDULER
ALSO
LEARNS
THE
EXACT
WORK
REQUIREMENT
AND
THE
DEADLINE
OF
THE
JOB
WE
ASSUME
THAT
TIME
IS
CONTINUOUS
A
SCHEDULE
SPECIFIES
FOR
EACH
TIME
A
JOB
TO
BE
RUN
AND
A
SPEED
AT
WHICH
TO
RUN
THE
JOB
THE
SPEED
IS
THE
AMOUNT
OF
WORK
PERFORMED
ON
THE
JOB
PER
UNIT
TIME
A
JOB
WITH
WORK
W
RUN
AT
A
CONSTANT
SPEED
THUS
TAKES
W
TIME
TO
COMPLETE
MORE
GENERALLY
THE
WORK
DONE
ON
A
JOB
DURING
A
TIME
PERIOD
IS
THE
INTEGRAL
OVER
THAT
TIME
PERIOD
OF
THE
SPEED
AT
WHICH
THE
JOB
IS
RUN
A
SCHEDULE
IS
FEASIBLE
IF
FOR
EACH
JOB
I
WORK
AT
LEAST
WI
IS
DONE
ON
JOB
I
DURING
RI
DI
NOTE
THAT
THE
TIMES
AT
WHICH
WORK
IS
PERFORMED
ON
JOB
I
DO
NOT
HAVE
TO
BE
CONTIGUOUS
IF
THE
PROCESSOR
IS
RUN
AT
SPEED
THEN
THE
POWER
IS
P
SΑ
FOR
SOME
CONSTANT
Α
THE
ENERGY
USED
DURING
A
TIME
PERIOD
IS
THE
INTEGRAL
OF
THE
POWER
OVER
THAT
TIME
PERIOD
OUR
OBJECTIVE
IS
TO
MINIMIZE
THE
TOTAL
ENERGY
USED
BY
THE
SCHEDULE
AN
ALGORITHM
A
IS
SAID
TO
BE
C
COMPETITIVE
IF
FOR
ANY
JOB
SEQUENCE
THE
ENERGY
USAGE
OF
A
IS
AT
MOST
C
TIMES
THAT
OF
THE
OPTIMAL
SCHEDULE
UPPER
BOUND
ANALYSIS
OF
QOA
OUR
GOAL
IN
THIS
SECTION
IS
TO
SHOW
THAT
QOA
IS
ABOUT
EΑ
COMPETITIVE
WHEN
Q
Α
WE
WISH
TO
POINT
OUT
THAT
Q
Α
IS
NOT
NECESSARILY
THE
OPTIMUM
VALUE
OF
Q
FOR
GENERAL
Α
IT
IS
NOT
CLEAR
HOW
TO
OBTAIN
THE
OPTIMUM
CHOICE
OF
Q
SINCE
IT
INVOLVES
SOLVING
A
SYSTEM
OF
HIGH
DEGREE
ALGEBRAIC
INEQUALITIES
HOWEVER
THE
LOWER
BOUND
FOR
QOA
WILL
IMPLY
THAT
THE
CHOICE
Q
Α
IS
CLOSE
TO
OPTIMUM
FOR
THE
CASE
OF
Α
AND
THAT
OF
Α
WE
CAN
EXPLICITLY
DETERMINE
THE
OPTIMUM
CHOICE
OF
Q
WHICH
GIVES
BETTER
COMPETITIVE
RATIOS
FOR
THESE
CASES
WE
USE
AN
AMORTIZED
LOCAL
COMPETITIVENESS
ANALYSIS
AND
USE
A
POTENTIAL
FUNC
TION
Φ
T
THAT
IS
A
FUNCTION
OF
TIME
IN
THIS
SETTING
THE
VALUE
OF
Φ
T
WILL
BE
ENERGY
AND
THUS
THE
DERIVATIVE
OF
Φ
T
WITH
RESPECT
TO
TIME
WILL
BE
POWER
WE
NEED
THAT
Φ
IS
INITIALLY
AND
FINALLY
ZERO
LET
SA
AND
SO
BE
THE
CURRENT
SPEED
OF
THE
ONLINE
ALGORITHM
QOA
IN
OUR
CASE
AND
THE
OPTIMAL
ALGORITHM
OPT
RESPECTIVELY
THEN
IN
ORDER
TO
ESTABLISH
THAT
THE
ONLINE
ALGORITHM
IS
C
COMPETITIVE
IT
IS
SUFFICIENT
TO
SHOW
THAT
THE
FOLLOWING
KEY
EQUATION
HOLDS
AT
ALL
TIMES
SΑ
D
Φ
C
SΑ
THE
FACT
THAT
EQUATION
ESTABLISHES
C
COMPETITIVENESS
FOLLOWS
BY
INTEGRATING
THIS
EQUATION
OVER
TIME
AND
FROM
THE
FACT
THAT
Φ
IS
INITIALLY
AND
FINALLY
FOR
MORE
INFORMATION
ON
AMORTIZED
LOCAL
COMPETITIVENESS
ARGUMENTS
SEE
BEFORE
DEFINING
THE
POTENTIAL
FUNCTION
Φ
THAT
WE
USE
WE
NEED
TO
INTRODUCE
SOME
NOTATION
WE
ALWAYS
DENOTE
THE
CURRENT
TIME
AS
FOR
ANY
T
T
LET
WA
T
T
DENOTE
THE
TOTAL
AMOUNT
OF
WORK
REMAINING
IN
QOA
AT
WITH
DEADLINE
IN
T
T
DEFINE
WO
T
T
SIMILARLY
FOR
OPT
RECALL
THAT
QOA
RUNS
AT
SPEED
Q
MAXT
WA
T
T
WHICH
IS
Q
TIMES
THE
SPEED
THAT
OA
WOULD
RUN
LET
D
T
T
MAX
WA
T
T
WO
T
T
DENOTE
THE
AMOUNT
OF
ADDITIONAL
WORK
LEFT
UNDER
THE
ONLINE
ALGORITHM
THAT
HAS
DEADLINE
IN
T
T
WE
DEFINE
A
SEQUENCE
OF
TIME
POINTS
ITERATIVELY
AS
FOLLOWS
LET
BE
THE
TIME
SUCH
THAT
D
IS
MAXIMIZED
IF
THERE
ARE
SEVERAL
SUCH
POINTS
WE
CHOOSE
THE
FURTHEST
ONE
GIVEN
TI
LET
TI
TI
BE
THE
FURTHEST
POINT
THAT
MAXIMIZES
D
TI
TI
TI
TI
WE
USE
GI
TO
DENOTE
D
TI
TI
TI
TI
NOTE
THAT
GI
IS
A
NON
NEGATIVE
MONOTONICALLY
DECREASING
SEQUENCE
WE
FIRST
BOUND
THE
OFFLINE
AND
ONLINE
SPEED
WHICH
WILL
BE
USEFUL
IN
OUR
ANALYSIS
LEMMA
I
SO
MAXT
WO
T
T
II
SA
AND
SA
QSO
WE
ARE
NOW
READY
TO
DEFINE
THE
POTENTIAL
FUNCTION
Φ
THAT
WE
USE
IN
OUR
ANALYSIS
OF
QOA
Φ
Β
TI
TI
GΑ
I
WHERE
Β
IS
SOME
CONSTANT
WHICH
WILL
BE
SET
TO
QΑ
Α
Α
Α
WE
NOW
MAKE
SOME
OBSERVATIONS
ABOUT
THE
POTENTIAL
FUNCTION
Φ
Φ
IS
OBVI
OUSLY
ZERO
BEFORE
ANY
JOBS
ARE
RELEASED
AND
AFTER
THE
LAST
DEADLINE
JOB
ARRIVALS
DO
NOT
AFFECT
Φ
SINCE
D
T
T
DOES
NOT
CHANGE
UPON
A
JOB
ARRIVAL
FOR
ANY
T
AND
T
SIMILARLY
JOB
COMPLETIONS
BY
EITHER
QOA
OR
OPTIMAL
DO
NOT
CHANGE
Φ
SINCE
IT
IS
A
CONTINUOUS
FUNCTION
OF
THE
UNFINISHED
WORK
AND
THE
UNFINISHED
WORK
ON
A
JOB
CONTINUOUSLY
DECREASES
TO
AS
IT
COMPLETES
FINALLY
STRUCTURAL
CHANGES
IN
THE
TI
AND
GI
DO
NOT
CHANGE
THE
VALUE
OF
Φ
IN
PARTICULAR
IF
DECREASES
FOR
INSTANCE
IF
ONLINE
IS
WORKING
FASTER
THAN
OFFLINE
ON
JOBS
WITH
DEADLINE
IN
THEN
AT
SOME
POINT
BECOMES
EQUAL
TO
AND
THE
INTERVALS
AND
MERGE
TOGETHER
UPON
THIS
MERGE
THE
POTENTIAL
DOES
NOT
CHANGE
AS
AT
THIS
POINT
SIMILARLY
IF
OFFLINE
WORKS
TOO
FAST
THE
INTERVAL
TK
TK
WHICH
CONTAINS
THE
EARLIEST
DEADLINE
AMONG
THE
UNFINISHED
JOBS
UNDER
OFFLINE
MIGHT
SPLIT
INTO
TWO
CRITICAL
INTERVALS
TK
T
AND
T
TK
BUT
AGAIN
THIS
CHANGE
DOES
NOT
AFFECT
Φ
SINCE
AT
THE
TIME
OF
SPLITTING
THE
VALUE
OF
G
FOR
THE
NEWLY
FORMED
INTERVALS
IS
IDENTICAL
TO
THE
VALUE
OF
THE
INTERVAL
TK
TK
THUS
TO
COMPLETE
OUR
ANALYSIS
WE
ARE
LEFT
TO
SHOW
THE
FOLLOWING
LEMMA
LEMMA
FOR
GENERAL
Α
SET
Q
Α
Β
C
Α
Α
Α
Α
Α
CONSIDER
A
TIME
T
WHERE
NO
JOBS
ARE
RELEASED
NO
JOBS
ARE
COM
PLETED
BY
QOA
OR
OPTIMAL
AND
THERE
ARE
NO
STRUCTURAL
CHANGES
TO
THE
TI
NOR
GI
THEN
EQUATION
SΑ
DΦ
DT
C
SΑ
HOLDS
AT
TIME
T
PROOF
SUPPOSE
FIRST
THAT
WA
WO
IN
THIS
CASE
D
AND
IS
BASICALLY
INFINITY
NOTE
THAT
DΦ
DT
SINCE
Φ
REMAINS
ZERO
UNTIL
WA
WO
THEREFORE
DΦ
DT
C
BECAUSE
SA
QSO
AND
A
O
C
QΑ
Α
Α
Α
QΑ
HENCE
WE
ASSUME
WA
WO
IN
THE
FOLLOWING
WITHOUT
LOSS
OF
GENERALITY
BOTH
OPT
AND
QOA
SCHEDULE
JOBS
ACCORDING
TO
EARLIEST
DEADLINE
FIRST
AND
HENCE
QOA
IS
WORKING
ON
A
JOB
WITH
DEADLINE
AT
MOST
LET
T
BE
DEADLINE
OF
THE
JOB
THAT
OPT
IS
WORKING
ON
AND
LET
K
BE
SUCH
THAT
TK
T
TK
FIRST
CONSIDER
THE
CASE
THAT
K
WHEN
BOTH
QOA
AND
OPT
WORK
DECREASES
THE
QUANTITIES
GK
AND
GK
STAY
UNCHANGED
AND
GK
IN
CREASES
NOTE
THAT
IS
DECREASING
AND
THE
RATE
OF
DECREASE
IS
THE
SAME
AS
THE
RATE
THAT
TIME
PASSES
THEREFORE
THE
RATE
OF
CHANGE
OF
GΑ
IS
D
T
T
G
T
T
ΑGΑ
SA
D
GΑ
ΑGΑ
Α
GΑ
FOR
THE
RATE
OF
CHANGE
OF
TK
TK
GΑ
WE
NOTE
THAT
TK
TK
STAYS
UNCHANGED
ALSO
THE
RATE
OF
CHANGE
OF
D
TK
TK
MAY
BE
SO
OR
DEPENDING
ON
WHETHER
WA
TK
TK
IS
GREATER
THAN
WO
KK
TK
THEREFORE
D
T
T
G
T
T
ΑGΑ
TK
TK
SO
K
K
K
K
K
K
TK
TK
ΑGΑ
ΑGΑ
THUS
TO
SHOW
SΑ
DΦ
DT
C
SΑ
IT
SUFFICES
TO
SHOW
THAT
A
O
SΑ
Β
ΑGΑ
Α
GΑ
C
SΑ
NOW
CONSIDER
THE
CASE
THAT
K
NOTE
THAT
FOR
I
NEITHER
GI
NOR
TI
TI
CHANGES
SO
WE
NEED
NOT
CONSIDER
THESE
TERMS
IN
THE
POTENTIAL
FUNCTION
THE
RATE
OF
CHANGE
OF
GΑ
IS
D
T
T
G
T
T
ΑGΑ
SA
SO
D
GΑ
ΑGΑ
Α
GΑ
WHICH
LEADS
TO
THE
SAME
INEQUALITY
AS
EQUATION
HENCE
WE
WILL
FOCUS
ON
EQUATION
AND
SHOW
THAT
IT
IS
TRUE
FOR
THE
STATED
VALUES
OF
Q
C
AND
Β
WE
CONSIDER
THE
LEFT
HAND
SIDE
OF
EQUATION
AS
A
FUNCTION
OF
SA
WHILE
G
AND
SO
ARE
FIXED
NOTE
THAT
IT
IS
A
CONVEX
FUNCTION
OF
SA
SINCE
SA
QSO
IT
SUFFICES
TO
SHOW
THAT
EQUATION
HOLDS
AT
THE
ENDPOINTS
SA
AND
SA
QSO
IF
SA
THE
LEFT
HAND
SIDE
OF
EQUATION
BECOMES
QΑGΑ
ΒQΑGΑ
ΒΑGΑ
Β
Α
GΑ
CSΑ
QΑ
ΒΑQ
Β
Α
GΑ
ΒΑGΑ
CS
TAKING
DERIVATIVE
WITH
RESPECT
TO
SO
WE
GET
THAT
THIS
IS
MAXIMIZED
AT
SO
SATISFYING
Α
CSΑ
ΒGΑ
AND
HENCE
Β
G
SUBSTITUTING
THIS
FOR
AND
CANCELING
GΑ
IT
FOLLOWS
THAT
WE
NEED
TO
SATISFY
THE
FOLLOWING
EQUATION
QΑ
ΒΑQ
Β
Α
Β
Α
Β
Α
IF
SA
QSO
THE
LEFT
HAND
SIDE
OF
EQUATION
BECOMES
QΑ
G
Α
Α
Α
Α
ΒQΑ
G
GΑ
ΒΑG
Β
Α
G
CS
QΑ
G
Α
Β
QΑ
Α
GΑ
ΒΑ
Q
GΑ
CS
SETTING
SO
X
AND
CANCELING
GΑ
IT
FOLLOWS
THAT
WE
NEED
TO
SATISFY
QΑ
X
Α
Β
QΑ
Α
ΒΑ
Q
X
CXΑ
WE
SET
Q
Α
AND
Β
C
QΑΗΑ
WHERE
Η
Α
Α
WITH
THESE
CHOICES
OF
Q
Β
AND
C
ΑQ
AND
TO
ESTABLISH
EQUATION
IT
IS
SUFFICIENT
TO
SHOW
THAT
QΑ
Β
WHICH
IS
TRIVIALLY
TRUE
SINCE
Η
SIMILARLY
EQUATION
IS
EQUIVALENT
TO
X
Α
ΑΗΑ
ΗΑ
Α
X
ΗΑ
FOR
ALL
X
SINCE
Α
IT
SUFFICES
TO
SHOW
THAT
X
Α
ΑΗΑ
ΗΑ
TO
SEE
THIS
NOTE
THAT
IF
WE
TAKE
THE
DERIVATIVE
OF
THE
LEFT
SIDE
OF
EQUATION
WE
OBTAIN
THAT
THE
MAXIMUM
IS
ATTAINED
AT
X
SUCH
THAT
X
Α
ΗΑ
AND
HENCE
X
Η
FOR
THIS
VALUE
OF
X
THE
LEFT
SIDE
OF
EQUATION
EVALUATES
TO
AND
HENCE
THE
RESULT
FOLLOWS
HENCE
EQUATION
IS
SATISFIED
AND
THE
LEMMA
FOLLOWS
NOW
OUR
MAIN
THEOREM
FOLLOWS
AS
A
DIRECT
CONSEQUENCE
THEOREM
QOA
IS
Α
Α
Α
Α
COMPETITIVE
FOR
GENERAL
Α
NOTE
THAT
FOR
LARGE
VALUES
OF
Α
THIS
BOUND
ON
THE
COMPETITIVE
RATIO
OF
QOA
IS
APPROXIMATELY
EΑ
FO
R
Α
THIS
BOUND
ON
THE
COMPETITIVE
RATIO
OF
QOA
EVALUATES
TO
WHICH
IS
ALREADY
BETTER
THAN
THE
BEST
KNOWN
BOUND
OF
HOWEVER
FOR
THE
CASES
OF
Α
AND
Α
WE
CAN
DETERMINE
THE
OPTIMUM
VALUES
OF
Q
AND
Β
TO
OBTAIN
THEOREMS
AND
THEOREM
IF
Q
THEN
QOA
IS
COMPETITIVE
FOR
Α
PROOF
WE
FOLLOW
THE
SAME
PROOF
STRUCTURE
AS
THAT
FOR
LEMMA
TO
OBTAIN
THE
INEQUALITIES
AND
BY
PUTTING
Α
IT
FOLLOWS
THAT
WE
NEED
TO
SATISFY
Β
X
Β
Q
X
WE
WROTE
A
PROGRAM
TO
DETERMINE
THE
VALUES
OF
Q
AND
Β
THAT
MINIMIZE
C
THE
BEST
VALUES
WE
OBTAINED
ARE
Q
Β
AND
C
IT
IS
EASY
TO
CHECK
THAT
THE
FIRST
INEQUALITY
IS
SATISFIED
THE
LEFT
HAND
SIDE
OF
THE
SECOND
INEQUALITY
BECOMES
WHICH
CAN
BE
SHOWN
TO
BE
NEGATIVE
BY
DIFFERENTIATION
HENCE
AND
ARE
SATISFIED
AND
THE
THEOREM
FOLLOWS
THEOREM
IF
Q
AND
Β
THEN
QOA
IS
COMPETITIVE
FOR
Α
LOWER
BOUNDS
IN
THIS
SECTION
WE
SHOW
THAT
ANY
ALGORITHM
IS
AT
LEAST
EΑ
COMPETITIVE
NOTE
THAT
WE
ASSUME
Α
IS
FIXED
AND
IS
KNOWN
TO
THE
ALGORITHM
WE
FIRST
GIVE
AN
ADVERSARIAL
STRATEGY
FOR
CONSTRUCTING
A
JOB
INSTANCE
SUCH
THAT
ANY
ALGORITHM
USES
AT
LEAST
EΑ
TIMES
THE
ENERGY
OF
THE
OPTIMAL
ENERGY
EXPENSES
ARE
BECOMING
AN
INCREASINGLY
IMPORTANT
FRACTION
OF
DATA
CENTER
OPERATING
COSTS
AT
THE
SAME
TIME
THE
ENERGY
EXPENSE
PER
UNIT
OF
COMPUTATION
CAN
VARY
SIG
NIFICANTLY
BETWEEN
TWO
DIFFERENT
LOCATIONS
IN
THIS
PAPER
WE
CHARACTERIZE
THE
VARIATION
DUE
TO
FLUCTUATING
ELECTRICITY
PRICES
AND
ARGUE
THAT
EXISTING
DISTRIBUTED
SYSTEMS
SHOULD
BE
ABLE
TO
EXPLOIT
THIS
VARIATION
FOR
SIGNIFICANT
ECONOMIC
GAINS
ELECTRICITY
PRICES
EXHIBIT
BOTH
TEMPORAL
AND
GEOGRAPHIC
VARI
ATION
DUE
TO
REGIONAL
DEMAND
DIFFERENCES
TRANSMISSION
INEF
FICIENCIES
AND
GENERATION
DIVERSITY
STARTING
WITH
HISTORICAL
ELECTRICITY
PRICES
FOR
TWENTY
NINE
LOCATIONS
IN
THE
US
AND
NETWORK
TRAFFIC
DATA
COLLECTED
ON
AKAMAI
CDN
WE
USE
SIM
ULATION
TO
QUANTIFY
THE
POSSIBLE
ECONOMIC
GAINS
FOR
A
REALISTIC
WORKLOAD
OUR
RESULTS
IMPLY
THAT
EXISTING
SYSTEMS
MAY
BE
ABLE
TO
SAVE
MILLIONS
OF
DOLLARS
A
YEAR
IN
ELECTRICITY
COSTS
BY
BEING
COGNIZANT
OF
LOCATIONAL
COMPUTATION
COST
DIFFERENCES
CATEGORIES
AND
SUBJECT
DESCRIPTORS
C
COMPUTER
COMMUNICATION
NETWORKS
DISTRIBUTED
SYSTEMS
GENERAL
TERMS
ECONOMICS
MANAGEMENT
PERFORMANCE
INTRODUCTION
WITH
THE
RISE
OF
INTERNET
SCALE
SYSTEMS
AND
CLOUD
COM
PUTING
SERVICES
THERE
IS
AN
INCREASING
TREND
TOWARD
MASSIVE
GEOGRAPHICALLY
DISTRIBUTED
SYSTEMS
THE
LARGEST
OF
THESE
ARE
MADE
UP
OF
HUNDREDS
OF
THOUSANDS
OF
SERVERS
AND
SEVERAL
DATA
CENTERS
A
LARGE
DATA
CENTER
MAY
REQUIRE
MANY
MEGAWATTS
OF
ELECTRICITY
ENOUGH
TO
POWER
THOUSANDS
OF
HOMES
MILLIONS
OF
DOLLARS
MUST
BE
SPENT
ANNUALLY
ON
THE
ELECTRIC
ITY
NEEDED
TO
POWER
ONE
SUCH
SYSTEM
FURTHERMORE
THESE
ALREADY
LARGE
SYSTEMS
ARE
INCREASING
IN
SIZE
AT
A
RAPID
CLIP
OUTPACING
DATA
CENTER
ENERGY
EFFICIENCY
GAINS
AND
ELEC
TRICITY
PRICES
ARE
EXPECTED
TO
RISE
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
SIGCOMM
AUGUST
BARCELONA
SPAIN
COPYRIGHT
ACM
FIGURE
ESTIMATED
ANNUAL
ELECTRICITY
COSTS
FOR
LARGE
COMPANIES
SERVERS
AND
INFRASTRUCTURE
MWH
THESE
ARE
CONSERVATIVE
ESTIMATES
MEANT
TO
BE
LOWER
BOUNDS
SEE
FOR
DERIVATION
DETAILS
FOR
SCALE
WE
HAVE
INCLUDED
THE
ACTUAL
CONSUMPTION
AND
UTILITY
BILL
FOR
THE
MIT
CAMPUS
INCLUDING
DORMITORIES
AND
LABS
ORGANIZATIONS
SUCH
AS
GOOGLE
MICROSOFT
AMAZON
YA
HOO
AND
MANY
OTHER
OPERATORS
OF
LARGE
NETWORKED
SYSTEMS
CANNOT
IGNORE
THEIR
ENERGY
COSTS
A
BACK
OF
THE
ENVELOPE
CAL
CULATION
FOR
GOOGLE
SUGGESTS
IT
CONSUMES
MORE
THAN
WORTH
OF
ELECTRICITY
ANNUALLY
FIGURE
A
MODEST
REDUC
TION
WOULD
THEREFORE
EXCEED
A
MILLION
DOLLARS
EVERY
YEAR
WE
PROJECT
THAT
EVEN
A
SMALLER
SYSTEM
LIKE
AKAMAI
CONSUMES
AN
ESTIMATED
WORTH
OF
ELECTRICITY
THE
CONVENTIONAL
APPROACH
TO
REDUCING
ENERGY
COSTS
HAS
BEEN
TO
REDUCE
THE
AMOUNT
OF
ENERGY
CONSUMED
NEW
COOLING
TECHNOLOGIES
ARCHITECTURAL
REDESIGNS
DC
POWER
MULTI
CORE
SERVERS
VIRTUALIZATION
AND
ENERGY
AWARE
LOAD
BALANC
ING
ALGORITHMS
HAVE
ALL
BEEN
PROPOSED
AS
WAYS
TO
REDUCE
THE
POWER
DEMANDS
OF
DATA
CENTERS
THAT
WORK
IS
COMPLEMEN
TARY
TO
OURS
THIS
PAPER
DEVELOPS
AND
ANALYZES
A
NEW
METHOD
TO
REDUCE
THE
ENERGY
COSTS
OF
RUNNING
LARGE
INTERNET
SCALE
SYSTEMS
IT
RELIES
ON
TWO
KEY
OBSERVATIONS
ELECTRICITY
PRICES
VARY
IN
THOSE
PARTS
OF
THE
U
WITH
WHOLESALE
ELECTRICITY
MARKETS
PRICES
VARY
ON
AN
HOURLY
BASIS
AND
ARE
OFTEN
NOT
WELL
CORRELATED
AT
DIFFERENT
LO
CATIONS
MOREOVER
THESE
VARIATIONS
ARE
SUBSTANTIAL
AS
MUCH
AS
A
FACTOR
OF
FROM
ONE
HOUR
TO
THE
NEXT
IF
WHEN
COMPUTATIONAL
DEMAND
IS
BELOW
PEAK
WE
CAN
DY
NAMICALLY
MOVE
DEMAND
I
E
ROUTE
SERVICE
REQUESTS
TO
PLACES
WITH
LOWER
PRICES
WE
CAN
REDUCE
ENERGY
COSTS
LARGE
DISTRIBUTED
SYSTEMS
ALREADY
INCORPORATE
REQUEST
ROUTING
AND
REPLICATION
WE
OBSERVE
THAT
MOST
INTERNET
SCALE
SYSTEMS
TODAY
ARE
GEOGRAPHICALLY
DISTRIBUTED
WITH
PAPER
COVERS
WORK
DONE
OUTSIDE
AKAMAI
AND
DOES
NOT
REP
RESENT
THE
OFFICIAL
VIEWS
OF
THE
COMPANY
AKAMAI
SELDOM
PAYS
DIRECTLY
FOR
ELECTRICITY
IT
PAYS
FOR
IT
INDIRECTLY
AS
PART
OF
CO
LOCATION
EXPENSES
MACHINES
AT
TENS
OR
EVEN
HUNDREDS
OF
SITES
AROUND
THE
WORLD
TO
PROVIDE
CLIENTS
GOOD
PERFORMANCE
AND
TO
TOLERATE
FAULTS
THESE
SYSTEMS
IMPLEMENT
SOME
FORM
OF
DYNAMIC
REQUEST
ROUTING
TO
MAP
CLIENTS
TO
SERVERS
AND
OFTEN
HAVE
MECHANISMS
TO
REPLICATE
THE
DATA
NECESSARY
TO
PROCESS
REQUESTS
AT
MULTIPLE
SITES
WE
HYPOTHESIZE
THAT
BY
EXPLOITING
THESE
OBSERVATIONS
LARGE
SYSTEMS
CAN
SAVE
A
SIGNIFICANT
AMOUNT
OF
MONEY
USING
MECH
ANISMS
FOR
REQUEST
ROUTING
AND
REPLICATION
THAT
THEY
ALREADY
IMPLEMENT
TO
EXPLORE
THIS
HYPOTHESIS
WE
DEVELOP
A
SIMPLE
COST
AWARE
REQUEST
ROUTING
POLICY
THAT
PREFERENTIALLY
MAPS
REQUESTS
TO
LOCATIONS
WHERE
ENERGY
IS
CHEAPER
OUR
MAIN
CONTRIBUTION
IS
TO
IDENTIFY
THE
RELEVANCE
OF
ELEC
TRICITY
PRICE
DIFFERENTIALS
TO
LARGE
DISTRIBUTED
SYSTEMS
AND
TO
ESTIMATE
THE
COST
SAVINGS
THAT
COULD
RESULT
IN
PRACTICE
IF
THE
SCHEME
WERE
DEPLOYED
PROBLEM
SPECIFICATION
GIVEN
A
LARGE
SYSTEM
COMPOSED
OF
SERVER
CLUSTERS
SPREAD
OUT
GEOGRAPHICALLY
WE
WISH
TO
MAP
CLIENT
REQUESTS
TO
CLUSTERS
SUCH
THAT
THE
TOTAL
ELECTRICITY
COST
IN
DOLLARS
NOT
JOULES
OF
THE
SYSTEM
IS
MINIMIZED
FOR
SIM
PLICITY
WE
ASSUME
THAT
THE
SYSTEM
IS
FULLY
REPLICATED
ADDI
TIONALLY
WE
OPTIMIZE
FOR
COST
EVERY
HOUR
WITH
NO
KNOWLEDGE
OF
THE
FUTURE
THIS
RATE
OF
CHANGE
IS
SLOW
ENOUGH
TO
BE
COM
PATIBLE
WITH
EXISTING
ROUTING
MECHANISMS
BUT
FAST
ENOUGH
TO
RESPOND
TO
ELECTRICITY
MARKET
FLUCTUATIONS
FINALLY
WE
IN
CORPORATE
BANDWIDTH
AND
PERFORMANCE
GOALS
AS
CONSTRAINTS
EXISTING
FRAMEWORKS
ALREADY
EXIST
TO
OPTIMIZE
FOR
BANDWIDTH
AND
PERFORMANCE
MODELING
THEM
AS
CONSTRAINTS
MAKES
IT
POSSIBLE
TO
ADD
OUR
PROCESS
TO
THE
END
OF
THE
EXISTING
OPTI
MIZATION
PIPELINE
NOTE
THAT
OUR
ANALYSIS
IS
CONCERNED
WITH
REDUCING
COST
NOT
ENERGY
OUR
APPROACH
MAY
ROUTE
CLIENT
REQUESTS
TO
DISTANT
LOCATIONS
TO
TAKE
ADVANTAGE
OF
CHEAP
ENERGY
THESE
LONGER
PATHS
MAY
CAUSE
OVERALL
ENERGY
CONSUMPTION
TO
RISE
SLIGHTLY
ENERGY
ELASTICITY
THE
MAXIMUM
REDUCTION
IN
COST
OUR
APPROACH
CAN
ACHIEVE
HINGES
ON
THE
ENERGY
ELASTICITY
OF
THE
CLUSTERS
THIS
IS
THE
DEGREE
TO
WHICH
THE
ENERGY
CONSUMED
BY
A
CLUSTER
DEPENDS
ON
THE
LOAD
PLACED
ON
IT
IDEALLY
CLUSTERS
WOULD
DRAW
NO
POWER
IN
THE
ABSENCE
OF
LOAD
IN
THE
WORST
CASE
THERE
WOULD
BE
NO
DIFFERENCE
BETWEEN
THE
PEAK
POWER
AND
THE
IDLE
POWER
OF
A
CLUSTER
PRESENT
STATE
OF
THE
ART
SYS
TEMS
FALL
SOMEWHERE
IN
THE
MIDDLE
WITH
IDLE
POWER
BEING
AROUND
OF
PEAK
A
SYSTEM
WITH
INELASTIC
CLUSTERS
IS
FORCED
TO
ALWAYS
CONSUME
ENERGY
EVERYWHERE
EVEN
IN
RE
GIONS
WITH
HIGH
ENERGY
PRICES
WITHOUT
ADEQUATE
ELASTICITY
WE
CANNOT
EFFECTIVELY
ROUTE
THE
SYSTEM
POWER
DEMAND
AWAY
FROM
HIGH
PRICED
AREAS
ZERO
IDLE
POWER
COULD
BE
ACHIEVED
BY
AGGRESSIVELY
CONSOL
IDATING
TURNING
OFF
UNDER
UTILIZED
COMPONENTS
AND
ALWAYS
ACTIVATING
ONLY
THE
MINIMUM
NUMBER
OF
MACHINES
NEEDED
TO
HANDLE
THE
OFFERED
LOAD
AT
PRESENT
ACHIEVING
THIS
WITHOUT
IMPACTING
PERFORMANCE
IS
STILL
AN
OPEN
CHALLENGE
HOWEVER
THERE
IS
AN
INCREASING
INTEREST
IN
ENERGY
PROPORTIONAL
SERVERS
AND
DYNAMIC
SERVER
PROVISIONING
TECHNIQUES
ARE
BEING
EX
PLORED
BY
BOTH
ACADEMICS
AND
INDUSTRY
RESULTS
TO
CONDUCT
OUR
ANALYSIS
WE
USE
TRACE
DRIVEN
SIMULATION
WITH
REAL
WORLD
HOURLY
AND
DAILY
ENERGY
PRICES
OBTAINED
FROM
A
NUMBER
OF
DATA
SOURCES
WE
LOOK
AT
MONTHS
OF
HOURLY
ELECTRICITY
PRICES
FROM
US
LOCATIONS
OUR
REQUEST
TRACES
COME
FROM
THE
AKAMAI
CONTENT
DISTRIBU
TION
NETWORK
CDN
WE
OBTAINED
DAYS
WORTH
OF
REQUEST
TRAFFIC
DATA
FIVE
MINUTE
LOAD
FOR
EACH
SERVER
CLUSTER
LOCATED
AT
A
COMMERCIAL
DATA
CENTER
IN
THE
U
WE
USED
THESE
DATA
SETS
TO
ESTIMATE
THE
PERFORMANCE
OF
OUR
SIMPLE
COST
AWARE
ROUTING
SCHEME
UNDER
DIFFERENT
CONSTRAINTS
WE
SHOW
THAT
EXISTING
SYSTEMS
CAN
REDUCE
ENERGY
COSTS
BY
AT
LEAST
WITHOUT
ANY
INCREASE
IN
BANDWIDTH
COSTS
OR
SIG
NIFICANT
REDUCTION
IN
CLIENT
PERFORMANCE
ASSUMING
A
GOOGLE
LIKE
ENERGY
ELASTICITY
AN
AKAMAI
LIKE
SERVER
DIS
TRIBUTION
AND
BANDWIDTH
CONSTRAINTS
FOR
LARGE
COMPANIES
THIS
CAN
EXCEED
A
MILLION
DOLLARS
A
YEAR
SAVINGS
RAPIDLY
INCREASE
WITH
ENERGY
ELASTICITY
IN
A
FULLY
ELASTIC
SYSTEM
WITH
RELAXED
BANDWIDTH
CONSTRAINTS
WE
CAN
REDUCE
ENERGY
COST
BY
OVER
AROUND
IF
WE
IMPOSE
STRICT
BANDWIDTH
CONSTRAINTS
WITHOUT
A
SIGNIFICANT
INCREASE
IN
CLIENT
SERVER
DISTANCES
ALLOWING
CLIENT
SERVER
DISTANCES
TO
INCREASE
LEADS
TO
IN
CREASED
SAVINGS
IF
WE
REMOVE
THE
DISTANCE
CONSTRAINT
A
DYNAMIC
SOLUTION
HAS
THE
POTENTIAL
TO
BEAT
A
STATIC
SOLUTION
I
E
PLACE
ALL
SERVERS
IN
CHEAPEST
MARKET
BY
A
SUBSTANTIAL
MARGIN
MAXIMUM
SAVINGS
VERSUS
MAXIMUM
SAVINGS
PRESENTLY
ENERGY
COST
AWARE
ROUTING
IS
RELEVANT
ONLY
TO
VERY
LARGE
COMPANIES
HOWEVER
AS
WE
MOVE
FORWARD
AND
THE
ENERGY
ELASTICITY
OF
SYSTEMS
INCREASES
NOT
ONLY
WILL
THIS
ROUTING
TECHNIQUE
BECOME
MORE
RELEVANT
TO
THE
LARGEST
SYS
TEMS
BUT
MUCH
SMALLER
SYSTEMS
WILL
ALSO
BE
ABLE
TO
ACHIEVE
MEANINGFUL
SAVINGS
PAPER
ORGANIZATION
IN
THE
NEXT
SECTION
WE
PROVIDE
SOME
BACKGROUND
ON
SERVER
ELECTRICITY
EXPENDITURE
AND
SKETCH
THE
STRUCTURE
OF
US
ENERGY
MARKETS
IN
SECTION
WE
PRESENT
DATA
ABOUT
THE
VARIATION
IN
REGIONAL
ELECTRIC
PRICES
SECTION
DESCRIBES
THE
AKAMAI
DATA
SET
USED
IN
THIS
PAPER
SECTION
OUTLINES
THE
ENERGY
CONSUMPTION
MODEL
USED
IN
THE
SIMU
LATIONS
COVERED
IN
SECTION
SECTION
CONSIDERS
ALTERNATIVE
MECHANISMS
FOR
MARKET
PARTICIPATION
SECTION
PRESENTS
SOME
IDEAS
FOR
FUTURE
WORK
BEFORE
WE
CONCLUDE
BACKGROUND
THIS
SECTION
FIRST
PRESENTS
EVIDENCE
THAT
ELECTRICITY
IS
BE
COMING
AN
INCREASINGLY
IMPORTANT
ECONOMIC
CONSIDERATION
AND
THEN
DESCRIBES
THE
SALIENT
FEATURES
OF
THE
WHOLESALE
ELEC
TRICITY
MARKETS
IN
THE
U
THE
SCALE
OF
ELECTRICITY
EXPENDITURES
IN
ABSOLUTE
TERMS
SERVERS
CONSUME
A
SUBSTANTIAL
AMOUNT
OF
ELECTRICITY
IN
SERVERS
AND
DATA
CENTERS
ACCOUNTED
FOR
AN
ESTIMATED
MILLION
MWH
OF
US
ELECTRICITY
CON
SUMPTION
COSTING
ABOUT
BILLION
DOLLARS
AT
WORST
BY
DATA
CENTER
ENERGY
USE
COULD
DOUBLE
AT
BEST
BY
RE
PLACING
EVERYTHING
WITH
STATE
OF
THE
ART
EQUIPMENT
WE
MAY
BE
ABLE
TO
REDUCE
USAGE
IN
TO
HALF
THE
CURRENT
LEVEL
MOST
COMPANIES
OPERATING
INTERNET
SCALE
SYSTEMS
ARE
SE
CRETIVE
ABOUT
THEIR
SERVER
DEPLOYMENTS
AND
POWER
CONSUMP
TION
FIGURE
SHOWS
OUR
ESTIMATES
FOR
SEVERAL
SUCH
COM
PANIES
BASED
ON
BACK
OF
THE
ENVELOPE
THE
IN
WH
N
PIDLE
PPEAK
PIDLE
U
P
U
E
PPEAK
WHERE
N
IS
SERVER
COUNT
PPEAK
IS
SERVER
PEAK
POWER
IN
WATTS
PIDLE
IS
IDLE
POWER
AND
U
IS
AVERAGE
SERVER
UTILIZATION
RTO
REGION
SOME
REGIONAL
HUBS
ISONE
NEW
ENGLAND
BOSTON
MA
BOS
MAINE
ME
CONNECTICUT
CT
NYISO
NEW
YORK
NYC
ALBANY
CAPITL
BUFFALO
WEST
PJM
IMPORT
PJM
PJM
EASTERN
CHICAGO
CHI
VIRGINA
DOM
NEW
JERSEY
NJ
MISO
MIDWEST
PEORIA
IL
MINNESOTA
MN
INDIANA
CINERGY
FIGURE
THE
DIFFERENT
REGIONS
STUDIED
IN
THIS
PAPER
THE
LISTED
HUBS
PROVIDE
A
SENSE
OF
RTO
COVERAGE
AND
A
REFERENCE
TO
MAP
ELECTRICITY
MARKET
LOCATION
IDENTIFIERS
HUB
TO
REAL
LOCATIONS
PALO
ALTO
SERVER
NUMBERS
ARE
FROM
PUBLIC
DISCLOSURES
FOR
EBAY
AND
RACKSPACE
EARNINGS
REPORT
TO
CALCULATE
ENERGY
WE
HAVE
MADE
THE
FOLLOWING
ASSUMPTIONS
AVERAGE
DATA
CEN
TER
POWER
USAGE
EFFECTIVENESS
PUE
IS
AND
IS
CAL
CULATED
BASED
ON
PEAK
POWER
AVERAGE
SERVER
UTILIZATION
IS
AROUND
AVERAGE
PEAK
SERVER
POWER
USAGE
IS
WATTS
BASED
ON
MEASUREMENTS
OF
ACTUAL
SERVERS
AT
AKAMAI
AND
IDLE
SERVERS
DRAW
OF
THEIR
PEAK
POWER
OUR
NUMBERS
FOR
MICROSOFT
ARE
BASED
ON
COMPANY
STATEMENTS
AND
ENERGY
FIGURES
MENTIONED
IN
A
PROMOTIONAL
VIDEO
TO
ESTIMATE
GOOGLE
POWER
CONSUMPTION
WE
ASSUMED
SERVERS
BASED
ON
AN
OLD
WIDELY
CIRCULATED
NUMBER
OPERATING
AT
WATTS
EACH
A
PUE
OF
AND
AVERAGE
UTILIZATION
AROUND
SUCH
A
SYSTEM
WOULD
CONSUME
MORE
THAN
MWH
AND
WOULD
INCUR
AN
AN
NUAL
ELECTRICITY
BILL
OF
NEARLY
MILLION
AT
PER
MWH
WHOLESALE
RATE
THESE
NUMBERS
ARE
CONSISTENT
WITH
AN
IN
DEPENDENT
CALCULATION
WE
CAN
MAKE
COMSCORE
ESTIMATED
THAT
GOOGLE
PERFORMED
ABOUT
SEARCHES
DAY
IN
AUGUST
AND
GOOGLE
OFFICIALLY
STATED
RECENTLY
THAT
EACH
SEARCH
TAKES
KJ
OF
ENERGY
ON
AVERAGE
PRESUMABLY
AMOR
TIZED
TO
INCLUDE
INDEXING
AND
OTHER
COSTS
THUS
SEARCH
ALONE
WORKS
OUT
TO
MWH
IN
GOOGLE
SERVERS
HANDLE
GMAIL
YOUTUBE
AND
MANY
OTHER
APPLICATIONS
SO
OUR
EARLIER
ESTIMATES
SEEM
REASONABLE
GOOGLE
MAY
WELL
HAVE
MORE
THAN
A
MILLION
SERVERS
SO
AN
ANNUAL
ELECTRIC
BILL
EX
CEEDING
WOULDN
T
BE
SURPRISING
AKAMAI
ELECTRICITY
COSTS
REPRESENT
INDIRECT
COSTS
NOT
SEEN
BY
THE
COMPANY
ITSELF
LIKE
OTHERS
WHO
RELY
ON
CO
LOCATION
FACILITIES
AKAMAI
SELDOM
PAYS
DIRECTLY
FOR
ELECTRICITY
POWER
IS
MOSTLY
BUILT
INTO
THE
BILLING
MODEL
WITH
CHARGES
BASED
ON
PROVISIONED
CAPACITY
RATHER
THAN
CONSUMPTION
IN
SECTION
WE
DISCUSS
WHY
OUR
IDEAS
ARE
RELEVANT
EVEN
TO
THOSE
NOT
DIRECTLY
CHARGED
PER
UNIT
OF
ELECTRICITY
THEY
USE
WHOLESALE
ELECTRICITY
MARKETS
ALTHOUGH
MARKET
DETAILS
DIFFER
REGIONALLY
THIS
SECTION
PRO
VIDES
A
HIGH
LEVEL
VIEW
OF
DEREGULATED
ELECTRICITY
MARKETS
PROVIDING
A
CONTEXT
FOR
THE
REST
OF
THE
PAPER
THE
DISCUS
SION
IS
BASED
ON
MARKETS
IN
THE
UNITED
STATES
GENERATION
ELECTRICITY
IS
PRODUCED
BY
GOVERNMENT
UTIL
ITIES
AND
INDEPENDENT
POWER
PRODUCERS
FROM
A
VARIETY
OF
SOURCES
IN
THE
UNITED
STATES
COAL
DOMINATES
NEARLY
FOLLOWED
BY
NATURAL
GAS
NUCLEAR
POWER
AND
HYDROELECTRIC
GENERATION
MEASURE
OF
DATA
CENTER
ENERGY
EFFICIENCY
DIFFERENT
REGIONS
MAY
HAVE
VERY
DIFFERENT
POWER
GENERA
TION
PROFILES
FOR
EXAMPLE
IN
HYDROELECTRIC
SOURCES
ACCOUNTED
FOR
OF
THE
POWER
GENERATED
IN
WASHINGTON
STATE
WHILE
IN
TEXAS
OF
THE
ENERGY
WAS
GENERATED
US
ING
NATURAL
GAS
AND
COAL
TRANSMISSION
PRODUCERS
AND
CONSUMERS
ARE
CONNECTED
TO
AN
ELECTRIC
GRID
A
COMPLEX
NETWORK
OF
TRANSMISSION
AND
DISTRIBUTION
LINES
ELECTRICITY
CANNOT
BE
STORED
EASILY
SO
SUPPLY
AND
DEMAND
MUST
CONTINUOUSLY
BE
BALANCED
IN
ADDITION
TO
CONNECTING
NEARBY
NODES
THE
GRID
CAN
BE
USED
TO
TRANSFER
ELECTRICITY
BETWEEN
DISTANT
LOCATIONS
THE
UNITED
STATES
IS
DIVIDED
INTO
EIGHT
RELIABILITY
REGIONS
WITH
VARYING
DEGREES
OF
INTER
CONNECTIVITY
CONGESTION
ON
THE
GRID
TRANSMISSION
LINE
LOSSES
EST
IN
AND
BOUNDARIES
BETWEEN
REGIONS
INTRODUCE
DISTRIBUTION
INEFFICIEN
CIES
AND
LIMIT
HOW
ELECTRICITY
CAN
FLOW
MARKET
STRUCTURE
IN
EACH
REGION
A
PSEUDO
GOVERNMENT
AL
BODY
A
REGIONAL
TRANSMISSION
ORGANIZATION
RTO
MAN
AGES
THE
GRID
FIGURE
AN
RTO
PROVIDES
A
CENTRAL
AUTHOR
ITY
THAT
SETS
UP
AND
DIRECTS
THE
FLOW
OF
ELECTRICITY
BETWEEN
GENERATORS
AND
CONSUMERS
OVER
THE
GRID
RTOS
ALSO
PROVIDE
MECHANISMS
TO
ENSURE
THE
SHORT
TERM
RELIABILITY
OF
THE
GRID
ADDITIONALLY
RTOS
ADMINISTER
WHOLESALE
ELECTRICTY
MAR
KETS
WHILE
BILATERAL
CONTRACTS
ACCOUNT
FOR
THE
MAJORITY
OF
THE
ELECTRICITY
THAT
FLOWS
OVER
THE
GRID
WHOLESALE
ELECTRIC
ITY
TRADING
HAS
BEEN
GROWING
RAPIDLY
AND
PRESENTLY
COVERS
ABOUT
OF
TOTAL
ELECTRICITY
WHOLESALE
MARKET
PARTICIPANTS
CAN
TRADE
FORWARD
CONTRACTS
FOR
THE
DELIVERY
OF
ELECTRICITY
AT
SOME
SPECIFIED
HOUR
IN
OR
DER
TO
DETERMINE
PRICES
FOR
THESE
CONTRACTS
RTOS
SUCH
AS
PJM
USE
AN
AUCTIONING
MECHANISM
POWER
PRODUCERS
PRESENT
SUPPLY
OFFERS
POSSIBLY
PRICE
SENSITIVE
CONSUMERS
PRESENT
DEMAND
BIDS
POSSIBLY
PRICE
SENSITIVE
AND
A
COORDINATING
BODY
DETERMINES
HOW
ELECTRICITY
SHOULD
FLOW
AND
SETS
PRICES
THE
MARKET
CLEARING
PROCESS
SETS
HOURLY
PRICES
FOR
THE
DIF
FERENT
LOCATIONS
IN
THE
MARKET
THE
OUTCOMES
DEPEND
NOT
ONLY
ON
BIDS
AND
OFFERS
BUT
ALSO
ACCOUNT
FOR
A
NUMBER
OF
CONSTRAINTS
GRID
CONNECTIVITY
RELIABILITY
ETC
EACH
RTO
OPERATES
MULTIPLE
PARALLEL
WHOLESALE
MARKETS
THERE
ARE
TWO
COMMON
MARKET
TYPES
DAY
AHEAD
MARKETS
FUTURES
PROVIDE
HOURLY
PRICES
FOR
DELIVERY
DURING
THE
FOLLOWING
DAY
THE
OUTCOME
IS
BASED
ON
EXPECTED
REAL
TIME
MARKETS
SPOT
ARE
BALANCING
MARKETS
WHERE
PRICES
ARE
CALCULATED
EVERY
FIVE
MINUTES
OR
SO
BASED
ON
ACTUAL
CONDITIONS
RATHER
THAN
EXPECTATIONS
TYPICALLY
THIS
MARKET
ACCOUNTS
FOR
A
SMALL
FRACTION
OF
TOTAL
ENERGY
TRANSACTIONS
LESS
THAN
OF
TOTAL
IN
NYISO
GENERALLY
SPEAKING
THE
MOST
EXPENSIVE
ACTIVE
GENERATION
RESOURCE
DETERMINES
THE
MARKET
CLEARING
PRICE
FOR
EACH
HOUR
THE
RTO
ATTEMPTS
TO
MEET
EXPECTED
DEMAND
BY
ACTIVATING
THE
SET
OF
RESOURCES
WITH
THE
LOWEST
OPERATING
COSTS
WHEN
DEMAND
IS
LOW
THE
BASE
LOAD
POWER
PLANTS
SUCH
AS
COAL
AND
NUCLEAR
CAN
FULFILL
IT
WHEN
DEMAND
RISES
ADDITIONAL
RE
SOURCES
SUCH
AS
NATURAL
GAS
TURBINES
NEED
TO
BE
ACTIVATED
SECURITY
CONSTRAINTS
LINE
LOSSES
AND
CONGESTION
COSTS
ALSO
IMPACT
PRICE
WHEN
TRANSMISSION
SYSTEM
RESTRICTIONS
SUCH
AS
LINE
CAPACITIES
PREVENT
THE
LEAST
EXPENSIVE
ENERGY
SUP
PLIER
FROM
SERVING
DEMAND
CONGESTION
IS
SAID
TO
EXIST
MORE
AHEAD
MARKETS
NOT
DISCUSSED
HERE
ARE
ANALOGOUS
JAN
MAY
SEP
JAN
MAY
SEP
JAN
MAY
SEP
JAN
MAY
FIGURE
DAILY
AVERAGES
OF
DAY
AHEAD
PEAK
PRICES
AT
DIFFERENT
HUBS
THE
ELEVATION
IN
CORRELATES
WITH
RECORD
HIGH
NATURAL
GAS
PRICES
AND
DOES
NOT
AFFECT
THE
HYDROELECTRIC
DOMINATED
NORTHWEST
THE
NORTHWEST
CONSISTENTLY
EXPERIENCES
DIPS
NEAR
APRIL
THIS
SEEMS
TO
BE
CORRELATED
WITH
SEASONAL
RAINFALL
CORRELATED
WITH
THE
GLOBAL
ECONOMIC
DOWNTURN
RECENT
PRICES
IN
ALL
FOUR
LOCATIONS
EXHIBIT
A
DOWNWARD
TREND
REAL
TIME
MIN
REAL
TIME
HOURLY
DAY
AHEAD
HOURLY
TIME
EST
EDT
FIGURE
THE
REAL
TIME
MARKET
IS
MORE
VARIABLE
AT
SHORT
TIME
SCALES
THAN
THE
DAY
AHEAD
MARKET
STANDARD
DEVI
ATIONS
FOR
PRICES
AT
THE
NYC
HUB
ARE
SHOWN
AVERAGED
USING
DIFFERENT
WINDOW
SIZES
BODY
OF
ECONOMIC
LITERATURE
DEALS
WITH
THE
STRUCTURE
AND
EVO
LUTION
OF
ENERGY
MARKETS
MARKET
FAILURES
AND
ARBITRAGE
OPPORTUNITIES
FOR
SECURITIES
TRADERS
E
G
EMPIRICAL
MARKET
ANALYSIS
FIGURE
COMPARING
PRICE
VARIATION
IN
DIFFERENT
WHOLE
SALE
MARKETS
FOR
THE
NEW
YORK
CITY
HUB
THE
TOP
GRAPH
SHOWS
A
PERIOD
WHEN
PRICES
WERE
SIMILAR
ACROSS
ALL
MAR
KETS
THE
BOTTOM
GRAPH
SHOWS
A
PERIOD
WHEN
THERE
WAS
SIGNIFICANTLY
MORE
VOLATILITY
IN
THE
REAL
TIME
MARKET
EXPENSIVE
GENERATION
UNITS
WILL
THEN
NEED
TO
BE
ACTIVATED
DRIVING
UP
PRICES
SOME
MARKETS
INCLUDE
AN
EXPLICIT
CONGES
TION
COST
COMPONENT
IN
THEIR
PRICES
SURPRISINGLY
NEGATIVE
PRICES
CAN
SHOW
UP
FOR
BRIEF
PERIODS
REPRESENTING
CONDITIONS
WHERE
IF
ENERGY
WERE
TO
BE
CONSUMED
AT
A
SPECIFIC
LOCATION
AT
A
SPECIFIC
TIME
THE
OVERALL
EFFICIENCY
OF
THE
SYSTEM
WOULD
INCREASE
MARKET
BOUNDARIES
INTRODUCE
ECONOMIC
TRANSACTION
INEFFI
CIENCIES
AS
WE
SHALL
SEE
LATER
EVEN
GEOGRAPHICALLY
CLOSE
LO
CATIONS
IN
DIFFERENT
MARKETS
TEND
TO
SEE
UNCORRELATED
PRICES
PART
OF
THE
PROBLEM
IS
THAT
DIFFERENT
MARKETS
HAVE
EVOLVED
USING
DIFFERENT
RULES
PRICING
MODELS
ETC
CLEARLY
THE
MARKET
FOR
ELECTRICITY
IS
COMPLEX
IN
ADDITION
TO
THE
FACTORS
MENTIONED
HERE
MANY
LOCAL
IDIOSYNCRASIES
EX
IST
IN
THIS
PAPER
WE
USE
A
RELATIVELY
SIMPLE
MARKET
MODEL
THAT
ASSUMES
THE
FOLLOWING
REAL
TIME
PRICES
ARE
KNOWN
AND
VARY
HOURLY
THE
ELECTRIC
BILL
PAID
BY
THE
SERVICE
OPERATOR
IS
PROPOR
TIONAL
TO
CONSUMPTION
AND
INDEXED
TO
WHOLESALE
PRICES
THE
REQUEST
ROUTING
BEHAVIOR
INDUCED
BY
OUR
METHOD
DOES
NOT
SIGNIFICANTLY
ALTER
PRICES
AND
MARKET
BEHAVIOR
THE
VALIDITY
OF
THE
SECOND
ASSUMPTION
DEPENDS
UPON
THE
EXTENT
TO
WHICH
COMPANIES
HEDGE
THEIR
ENERGY
COSTS
BY
CON
TRACTUALLY
LOCKING
IN
FIXED
PRICING
SEE
SECTION
A
LARGE
WE
POSIT
THAT
IMPERFECTLY
CORRELATED
VARIATIONS
IN
LOCAL
ELECTRICITY
PRICES
CAN
BE
EXPLOITED
BY
OPERATORS
OF
LARGE
GEO
GRAPHICALLY
DISTRIBUTED
SYSTEMS
TO
SAVE
MONEY
RATHER
THAN
PRESENTING
A
THEORETICAL
DISCUSSION
WE
TAKE
AN
EMPIRICAL
AP
PROACH
GROUNDING
OUR
ANALYSIS
IN
HISTORICAL
MARKET
DATA
AG
GREGATED
FROM
GOVERNMENT
SOURCES
TRADE
PUBLICATION
ARCHIVES
AND
PUBLIC
DATA
ARCHIVES
MAINTAINED
BY
THE
DIF
FERENT
RTOS
WE
USE
PRICE
DATA
FOR
LOCATIONS
COVERING
JANUARY
THROUGH
MARCH
PRICE
VARIATION
GEOGRAPHIC
PRICE
DIFFERENTIALS
ARE
WHAT
REALLY
MATTER
TO
US
BUT
IT
IS
USEFUL
TO
FIRST
GET
A
FEEL
FOR
THE
BEHAVIOUR
OF
INDIVIDUAL
PRICES
DAILY
VARIATION
FIGURE
SHOWS
DAILY
AVERAGE
PRICES
FOR
FOUR
FROM
JANUARY
THROUGH
APRIL
ALTHOUGH
PRICES
ARE
RELATIVELY
STABLE
AT
LONG
TIME
SCALES
THEY
EXHIBIT
A
SIGNIFICANT
AMOUNT
OF
DAY
TO
DAY
VOLATILITY
SHORT
TERM
SPIKES
SEASONAL
TRENDS
AND
DEPENDENCIES
ON
FUEL
PRICES
AND
CONSUMER
DEMAND
SOME
LOCATIONS
IN
THE
FIGURE
ARE
VISIBLY
CORRELATED
BUT
HOURLY
PRICES
ARE
NOT
CORRELATED
DIFFERENT
MARKET
TYPES
SPOT
AND
FUTURES
MARKETS
HAVE
DIFFERENT
PRICE
DYNAMICS
FIGURES
AND
ILLUSTRATE
THE
DIFFERENCE
FOR
NYC
COMPARED
TO
THE
DAY
AHEAD
MARKET
THE
HOURLY
REAL
TIME
RT
MARKET
IS
MORE
VOLATILE
WITH
MORE
HIGH
FREQUENCY
VARIATION
AND
A
LOWER
AVERAGE
PRICE
THE
UNDERLYING
FIVE
MINUTE
RT
PRICES
ARE
EVEN
MORE
VOLATILE
NORTHWEST
IS
AN
IMPORTANT
REGION
BUT
LACKS
AN
HOURLY
WHOLESALE
MARKET
FORCING
US
TO
OMIT
THE
REGION
FROM
THE
REMAIN
DER
OF
OUR
ANALYSIS
LOCATION
RTO
MEAN
STDEV
KURT
CHICAGO
IL
PJM
INDIANAPOLIS
IN
MISO
PALO
ALTO
CA
CAISO
RICHMOND
VA
PJM
BOSTON
MA
ISONE
NEW
YORK
NY
NYISO
FIGURE
REAL
TIME
MARKET
STATISTICS
COVERING
HOURLY
PRICES
FROM
JANUARY
THROUGH
MARCH
STATISTICS
ARE
FROM
THE
TRIMMED
DATA
DIFFERENT
RTOS
NYISO
ISONE
PJM
MISO
ERCOT
CAISO
SAMPLES
EST
DISTANCE
BETWEEN
TWO
HUBS
KM
FIGURE
THE
RELATIONSHIP
BETWEEN
PRICE
CORRELATION
DISTANCE
AND
PARENT
RTO
EACH
POINT
REPRESENTS
A
PAIR
OF
HUBS
HUBS
PAIRS
AND
THE
CORRELATION
COEF
FICIENT
OF
THEIR
HOURLY
PRICES
SAMPLES
HOURLY
PRICE
CHANGE
MWH
HOURLY
PRICE
CHANGE
MWH
EACH
RED
POINTS
REPRESENT
PAIRED
HUBS
FROM
DIFFERENT
RTOS
BLUE
POINTS
ARE
LABELLED
WITH
THE
RTO
OF
BOTH
A
PALO
ALTO
B
CHICAGO
PJM
FIGURE
HISTOGRAMS
OF
HOUR
TO
HOUR
CHANGE
IN
REAL
TIME
HOURLY
PRICES
FOR
TWO
LOCATIONS
OVER
THE
MONTH
PERIOD
BOTH
DISTRIBUTIONS
ARE
ZERO
MEAN
GAUSSIAN
LIKE
WITH
VERY
LONG
TAILS
FOR
THE
REMAINDER
OF
THIS
PAPER
WE
FOCUS
EXCLUSIVELY
ON
THE
RT
MARKET
OUR
GOAL
IS
TO
EXPLOIT
GEOGRAPHICALLY
UNCOR
RELATED
VOLATILITY
SOMETHING
THAT
IS
MORE
COMMON
IN
THE
RT
MARKET
WE
RESTRICT
OURSELVES
TO
HOURLY
PRICES
BUT
SPECULATE
THAT
THE
ADDITIONAL
VOLATILITY
IN
FIVE
MINUTE
PRICES
PROVIDES
FURTHER
OPPORTUNITIES
FIGURE
PROVIDES
ADDITIONAL
STATISTICS
FOR
HOURLY
RT
PRICES
PALOALTO
MINUS
RICHMOND
AUSTIN
MINUS
RICHMOND
SAT
SUN
MON
TUE
WED
THU
FRI
SAT
SUN
MON
TUE
WED
THU
FRI
SAT
SAT
SUN
MON
TUE
WED
THU
FRI
SAT
SUN
MON
TUE
WED
THU
FRI
SAT
TIME
EDT
HOUR
TO
HOUR
VOLATILITY
AS
SEEN
IN
FIGURE
THE
HOUR
TO
HOUR
VARIATION
IN
NYC
RT
PRICES
CAN
BE
DRAMATIC
FIG
URE
SHOWS
THE
DISTRIBUTION
OF
THE
HOURLY
CHANGE
FOR
PALO
ALTO
AND
CHICAGO
AT
EACH
LOCATION
THE
PRICE
PER
MWH
CHANGED
HOURLY
BY
OR
MORE
ROUGHLY
OF
THE
TIME
A
STEP
REPRESENTS
OF
THE
MEAN
PRICE
FOR
CHICAGO
FUR
THERMORE
THE
MINIMUM
AND
MAXIMUM
PRICE
DURING
A
SINGLE
DAY
CAN
EASILY
DIFFER
BY
A
FACTOR
OF
THE
EXISTENCE
OF
RAPID
PRICE
FLUCTUATIONS
REFLECTS
THE
FACT
THAT
SHORT
TERM
DEMAND
FOR
ELECTRICITY
IS
FAR
MORE
ELASTIC
THAN
SUPPLY
ELECTRICITY
CANNOT
ALWAYS
BE
EFFICIENTLY
MOVED
FROM
LOW
DEMAND
AREAS
TO
HIGH
DEMAND
AREAS
AND
PRODUCERS
CANNOT
ALWAYS
RAMP
UP
OR
DOWN
EASILY
GEOGRAPHIC
CORRELATION
OUR
APPROACH
WOULD
FAIL
IF
HOURLY
PRICES
ARE
WELL
CORRELATED
AT
DIFFERENT
LOCATIONS
HOWEVER
WE
FIND
THAT
LOCATIONS
IN
DIFFERENT
REGIONAL
MARKETS
ARE
NEVER
HIGHLY
CORRELATED
EVEN
WHEN
NEARBY
AND
THAT
LOCATIONS
IN
THE
SAME
REGION
ARE
NOT
ALWAYS
WELL
CORRELATED
FIGURE
SHOWS
A
SCATTER
PLOT
OF
PAIRWISE
CORRELATION
AND
GEOGRAPHIC
NO
PAIRS
WERE
NEGATIVELY
CORRELATED
NOTE
HOW
CORRELATION
DECREASES
WITH
DISTANCE
FURTHER
NOTE
THE
IMPACT
OF
RTO
MARKET
BOUNDARIES
MOST
PAIRS
DRAWN
FROM
THE
SAME
RTO
LIE
ABOVE
THE
CORRELATION
LINE
WHILE
ALL
PAIRS
FROM
DIFFERENT
REGIONS
LIE
BELOW
WE
ALSO
SEE
HAVE
VERIFIED
OUR
RESULTS
USING
SUBSETS
OF
THE
DATA
E
G
LAST
MONTHS
MUTUAL
INFORMATION
IX
Y
SHIFTED
SIGNALS
ETC
Y
MUCH
MORE
CLEARLY
DIVIDES
THE
DATA
BETWEEN
SAME
RTO
AND
DIFFERENT
RTO
PAIRS
SUGGESTING
THAT
THE
SMALL
OVERLAP
IN
FIGURE
IS
DUE
TO
THE
EXISTENCE
OF
NON
LINEAR
RELATIONSHIPS
WITHIN
NYISO
FIGURE
VARIATION
OF
PRICE
DIFFERENTIALS
WITH
TIME
A
SURPRISING
LACK
OF
DIVERSITY
WITHIN
SOME
REGIONS
LA
AND
PALO
ALTO
HAVE
A
COEFFICIENT
OF
HOURLY
PRICES
ARE
NOT
CORRELATED
AT
SHORT
TIME
SCALES
BUT
WE
SHOULD
NOT
EXPECT
PRICES
TO
BE
INDEPENDENT
NATURAL
GAS
PRICES
FOR
EXAMPLE
WILL
INTRODUCE
SOME
COUPLING
SEE
FIGURE
BETWEEN
DISTANT
LOCATIONS
PRICE
DIFFERENTIALS
FIGURE
SHOWS
HOURLY
PRICE
DIFFERENTIALS
FOR
TWO
PAIRS
OF
LOCATIONS
OVER
AN
EIGHT
DAY
PERIOD
BOTH
PAIRS
HAVE
MEAN
DIFFERENTIALS
CLOSE
TO
ZERO
THE
THREE
LOCATIONS
ARE
FAR
FROM
EACH
OTHER
AND
IN
DIFFERENT
RTOS
WE
SEE
PRICE
SPIKES
SOME
EXTEND
FAR
OFF
THE
SCALE
THE
LARGEST
IS
AND
EXTENDED
PERIODS
OF
PRICE
ASYMMETRY
SOMETIMES
THE
ASYMMETRY
FAVOURS
ONE
SOMETIMES
THE
OTHER
THIS
SUGGESTS
THAT
A
PRE
DETERMINED
ASSIGNMENT
OF
CLIENTS
TO
SERVERS
IS
NOT
OPTIMAL
DIFFERENTIAL
DISTRIBUTIONS
CONSIDER
TWO
LOCATIONS
IN
ORDER
FOR
OUR
DYNAMIC
APPROACH
TO
YIELD
SUBSTANTIAL
SAVINGS
OVER
A
STATIC
SOLUTION
THE
PRICE
DIFFERENTIAL
BETWEEN
THOSE
LOCATIONS
MUST
VARY
IN
TIME
AND
THE
DISTRIBUTION
OF
THIS
DIF
FERENTIAL
SHOULD
IDEALLY
HAVE
A
ZERO
MEAN
AND
A
REASONABLY
HIGH
VARIANCE
SUCH
A
DISTRIBUTION
WOULD
IMPLY
THAT
NEITHER
SITE
IS
STRICTLY
BETTER
THAN
THE
OTHER
BUT
ALSO
THAT
A
DYNAMIC
SOLUTION
ALWAYS
BUYING
FROM
WHICHEVER
SITE
IS
LEAST
EXPEN
SIVE
THAT
HOUR
COULD
YIELD
MEANINGFUL
SAVINGS
ADDITIONALLY
THE
DYNAMIC
APPROACH
COULD
WIN
WHEN
PRESENTED
WITH
TWO
LOCATIONS
HAVING
UNCORRELATED
PERIODS
OF
PRICE
ELEVATION
AND
ERCOT
NOT
DETECTED
BY
THE
CORRELATION
COEFFICIENT
PRICE
DIFFERENCE
MWH
PRICE
DIFFERENCE
MWH
PRICE
DIFFERENCE
MWH
PRICE
DIFFERENCE
MWH
PRICE
DIFFERENCE
MWH
A
PALOALTO
VIRGINIA
B
AUSTIN
VIRGINIA
C
BOSTON
NYC
D
CHICAGO
VIRGINIA
E
CHICAGO
PEORIA
FIGURE
PRICE
DIFFERENTIAL
HISTOGRAMS
FOR
FIVE
LOCATION
PAIRS
AND
MONTHS
OF
HOURLY
PRICES
JAN
MAY
SEP
JAN
MAY
SEP
JAN
MAY
SEP
JAN
FIGURE
PALOALTO
VIRGINIA
PRICE
DIFFERENTIAL
DISTRIBU
TIONS
FOR
EACH
MONTH
THE
MONTHLY
MEDIAN
PRICES
AND
INTER
QUARTILE
RANGE
ARE
SHOWN
FIGURE
SHOWS
THE
PAIRWISE
DIFFERENTIAL
DISTRIBUTIONS
FOR
SOME
LOCATIONS
FOR
THE
DATA
THE
CALIFORNIA
VIRGINIA
FIGURE
AND
TEXAS
VIRGINIA
FIGURE
DIS
TRIBUTIONS
ARE
ZERO
MEAN
WITH
A
HIGH
VARIANCE
THERE
ARE
MANY
OTHER
SUCH
BOSTON
NYC
FIGURE
IS
SKEWED
SINCE
BOSTON
TENDS
TO
BE
CHEAPER
THAN
NYC
BUT
NYC
IS
LESS
EXPENSIVE
OF
THE
TIME
THE
SAVINGS
ARE
GREATER
THAN
MWH
OF
THE
TIME
THUS
EVEN
WITH
SUCH
A
SKEWED
DISTRIBUTION
THERE
EXISTS
AN
OPPORTUNITY
TO
DYNAMICALLY
EXPLOIT
DIFFERENTIALS
FOR
MEANINGFUL
SAVINGS
UNSURPRISINGLY
A
NUMBER
OF
PAIRS
EXIST
WHERE
ONE
LOCATION
IS
STRICTLY
BETTER
THAN
THE
OTHER
AND
DYNAMIC
ADAPTATION
IS
UNNECESSARY
CHICAGO
VIRGINIA
FIGURE
IS
AN
EXAMPLE
VIRGINIA
IS
LESS
EXPENSIVE
OF
THE
TIME
BUT
THE
SAVINGS
ALMOST
NEVER
EXCEED
MWH
THE
DISPERSION
INTRODUCED
BY
A
MARKET
BOUNDARY
CAN
BE
SEEN
IN
THE
DYNAMICALLY
EXPLOITABLE
CHICAGO
PEORIA
DISTRIBU
TION
FIGURE
EVOLUTION
IN
TIME
THE
PRICE
DIFFERENTIAL
DISTRIBUTIONS
DO
NOT
REMAIN
STATIC
IN
TIME
FIGURE
SHOWS
HOW
THE
PALOALTO
VIRGINIA
DISTRIBUTION
CHANGED
FROM
MONTH
TO
MONTH
A
SUSTAINED
PRICE
ASYMMETRY
MAY
EXIST
FOR
MANY
MONTHS
BEFORE
REVERSING
ITSELF
THE
SPREAD
OF
PRICES
IN
ONE
MONTH
MAY
DOUBLE
THE
NEXT
MONTH
TIME
OF
DAY
PRICE
DIFFERENTIALS
DEPEND
ON
THE
TIME
OF
DAY
FOR
INSTANCE
BECAUSE
CALIFORNIA
AND
VIRGINIA
ARE
IN
DIFFERENT
TIME
ZONES
PEAK
DEMAND
DOES
NOT
OVERLAP
THIS
IS
LIKELY
AN
IMPORTANT
FACTOR
SHAPING
THE
PRICE
DIFFERENTIAL
FIGURE
SHOWS
HOW
THE
HOUR
OF
DAY
AFFECTS
THE
DIFFER
ENTIALS
FOR
THREE
LOCATION
PAIRS
FOR
PALOALTO
VIRGINIA
WE
SEE
A
STRONG
DEPENDENCY
ON
THE
HOUR
BEFORE
EASTERN
VIRGINIA
HAS
A
SIGNIFICANT
EDGE
BY
THE
SITUATION
HAS
RE
VERSED
FROM
NEITHER
IS
BETTER
FOR
BOSTON
NYC
WE
SEE
A
DIFFERENT
KIND
OF
DEPENDENCY
FROM
NEITHER
ARE
OTHER
PAIRS
A
SET
OF
HUBS
WITH
Μ
Σ
AND
PAIRS
A
SET
OF
HUBS
WITH
Μ
Σ
HOUR
OF
DAY
EST
EDT
FIGURE
PRICE
DIFFERENTIAL
DISTRIBUTIONS
MEDIAN
AND
INTER
QUARTILE
RANGE
FOR
EACH
HOUR
OF
THE
DAY
CALIFORNIA
VIRGINIA
DIFFERENTIAL
DURATION
HOURS
FIGURE
FOR
PALOALTO
VIRGINIA
SHORT
LIVED
PRICE
DIF
FERENTIALS
ACCOUNT
FOR
MOST
OF
THE
TIME
SITE
IS
BETTER
AT
ALL
OTHER
TIMES
BOSTON
HAS
THE
EDGE
THE
EFFECT
OF
HOUR
OF
DAY
ON
CHICAGO
PEORIA
IS
LESS
CLEAR
DIFFERENTIAL
DURATION
WE
DEFINE
THE
DURATION
OF
A
SUS
TAINED
PRICE
DIFFERENTIAL
AS
THE
NUMBER
OF
HOURS
ONE
LOCATION
IS
FAVOURED
OVER
ANOTHER
BY
MORE
THAN
MWH
AS
SOON
AS
THE
DIFFERENTIAL
FALLS
BELOW
THIS
THRESHOLD
OR
REVERSES
TO
FAVOUR
THE
OTHER
LOCATION
WE
MARK
THE
END
OF
THE
DIFFERENTIAL
FIGURE
SHOWS
HOW
MUCH
TIME
WAS
SPENT
IN
SHORT
DURATION
PRICE
DIFFERENTIALS
FOR
PALOALTO
VIRGINIA
SHORT
DIFFERENTIALS
HRS
ARE
MORE
FREQUENT
THAN
OTHER
TYPES
MEDIUM
LENGTH
DIFFERENTIALS
HRS
ARE
COMMON
DIFFERENTIALS
THAT
LAST
LONGER
THAN
A
DAY
ARE
RARE
FOR
A
BALANCED
PAIR
LIKE
THIS
AKAMAI
TRAFFIC
AND
BANDWIDTH
IN
ORDER
TO
UNDERSTAND
THE
INTERACTION
OF
REAL
WORKLOADS
WITH
ELECTRICITY
PRICES
WE
ACQUIRED
A
DATA
SET
DETAILING
TRAFFIC
ON
AKAMAI
INFRASTRUCTURE
THE
DATA
COVERS
DAYS
WORTH
OF
TRAFFIC
ON
A
LARGE
SUBSET
OF
AKAMAI
SERVERS
WITH
A
PEAK
OF
OVER
MILLION
HITS
SEC
FIGURE
THE
REGION
TRAFFIC
IS
THE
SUBSET
OF
SERVERS
FOR
WHICH
WE
HAVE
ELECTRICITY
PRICE
DATA
WE
USE
THE
AKAMAI
TRAFFIC
BECAUSE
IT
IS
A
REALISTIC
WORK
LOAD
AKAMAI
HAS
OVER
CONTENT
PROVIDER
CUSTOMERS
IN
THE
US
HENCE
THE
TRAFFIC
REPRESENTS
A
BROAD
USER
BASE
GLOBAL
TRAFFIC
USA
TRAFFIC
REGION
SUBSET
UTC
TIME
FIGURE
TRAFFIC
IN
THE
AKAMAI
DATA
SET
WE
SEE
A
PEAK
HIT
RATE
OF
OVER
MILLION
HITS
PER
SECOND
OF
THIS
ABOUT
MILLION
HITS
COME
FROM
THE
US
THE
TRAFFIC
IN
THIS
DATA
SET
COMES
FROM
ROUGHLY
HALF
OF
THE
SERVERS
AKAMAI
RUNS
IN
COMPARISON
IN
TOTAL
AKAMAI
SEES
AROUND
BILLION
HITS
DAY
HOWEVER
AKAMAI
DOES
NOT
USE
AGGRESSIVE
SERVER
POWER
MANAGEMENT
THEIR
CDN
IS
SENSITIVE
TO
LATENCY
AND
THEIR
WORKLOAD
CONTAINS
A
LARGE
FRACTION
OF
COMPUTATIONALLY
TRIV
IAL
HITS
E
G
FETCHES
OF
WELL
CACHED
OBJECTS
SO
OUR
WORK
IS
FAR
LESS
RELEVANT
TO
AKAMAI
THAN
TO
SYSTEMS
WHERE
MORE
ENERGY
ELASTICITY
EXISTS
AND
WORKLOADS
ARE
COMPUTATIONALLY
INTENSIVE
FURTHERMORE
IN
MAPPING
CLIENTS
TO
SERVERS
AKA
MAI
SYSTEM
BALANCES
A
NUMBER
OF
CONCERNS
TRYING
TO
OPTI
MIZE
PERFORMANCE
HANDLE
PARTIALLY
REPLICATED
CDN
OBJECTS
OPTIMIZE
NETWORK
BANDWIDTH
COSTS
ETC
TRAFFIC
DATA
TRAFFIC
DATA
WAS
COLLECTED
AT
MINUTE
IN
TERVALS
ON
SERVERS
HOUSED
IN
AKAMAI
PUBLIC
CLUSTERS
AKA
MAI
HAS
TWO
TYPES
OF
CLUSTERS
PUBLIC
AND
PRIVATE
PRI
VATE
CLUSTERS
ARE
TYPICALLY
LOCATED
INSIDE
OF
UNIVERSITIES
LARGE
COMPANIES
SMALL
ISPS
AND
ISPS
OUTSIDE
THE
US
THESE
CLUS
TERS
ARE
DEDICATED
TO
SERVING
A
SPECIFIC
USER
BASE
E
G
THE
MEMBERS
OF
A
UNIVERSITY
COMMUNITY
AND
NO
OTHERS
PUB
LIC
CLUSTERS
ARE
GENERALLY
LOCATED
IN
COMMERCIAL
CO
LOCATION
CENTERS
AND
CAN
SERVE
ANY
USERS
WORLD
WIDE
FOR
ANY
USER
NOT
SERVED
BY
A
PRIVATE
CLUSTER
AKAMAI
HAS
THE
FREEDOM
TO
CHOOSE
WHICH
OF
ITS
PUBLIC
CLUSTERS
TO
DIRECT
THE
USER
CLIENTS
THAT
END
UP
AT
PUBLIC
CLUSTERS
TEND
TO
SEE
LONGER
NETWORK
PATHS
THAN
CLIENTS
THAT
CAN
BE
SERVED
AT
PRIVATE
CLUSTERS
THE
MINUTE
DATA
CONTAINS
FOR
EACH
PUBLIC
CLUSTER
THE
NUMBER
OF
HITS
AND
BYTES
SERVED
TO
CLIENTS
A
ROUGH
GEOGRA
PHY
OF
WHERE
THOSE
CLIENTS
ORIGINATED
AND
THE
LOAD
IN
EACH
OF
THE
CLUSTERS
IN
ADDITION
WE
SURVEYED
THE
HARDWARE
USED
IN
THE
DIFFERENT
CLUSTERS
AND
COLLECTED
VALUES
FOR
OBSERVED
SERVER
POWER
USAGE
WE
ALSO
LOOKED
AT
THE
TOP
LEVEL
MAPPING
SYS
TEM
TO
SEE
HOW
NAME
SERVERS
WERE
MAPPED
TO
CLUSTERS
IN
THE
DATA
WE
COLLECTED
THE
GEOGRAPHIC
LOCALIZATION
OF
CLIENTS
IS
COARSE
THEY
ARE
MAPPED
TO
STATES
IN
THE
US
OR
COUNTRIES
IF
MULTIPLE
CLUSTERS
EXIST
IN
A
CITY
WE
AGGREGATE
THEM
TOGETHER
AND
TREAT
THEM
AS
A
SINGLE
CLUSTER
THIS
AFFECTS
OUR
CALCULATION
OF
CLIENT
SERVER
DISTANCES
IN
BANDWIDTH
COSTS
AN
IMPORTANT
CONTRIBUTOR
TO
DATA
CENTER
COSTS
IS
BANDWIDTH
AND
THERE
MAY
BE
LARGE
DIFFER
ENCES
BETWEEN
COSTS
ON
DIFFERENT
NETWORKS
AND
SOMETIMES
ON
THE
SAME
NETWORK
OVER
TIME
BANDWIDTH
COSTS
ARE
SIGNIF
ICANT
FOR
AKAMAI
AND
THUS
THEIR
SYSTEM
IS
AGGRESSIVELY
OP
TIMIZED
TO
REDUCE
BANDWIDTH
COSTS
WE
NOTE
THAT
CHANGING
AKAMAI
CURRENT
ASSIGNMENTS
OF
CLIENTS
TO
CLUSTERS
TO
REDUCE
ENERGY
COSTS
COULD
INCREASE
ITS
BANDWIDTH
COSTS
SINCE
THEY
HAVE
BEEN
OPTIMIZED
ALREADY
RIGHT
NOW
THE
PORTION
OF
CO
LOCATION
COST
ATTRIBUTABLE
TO
ENERGY
IS
LESS
THAN
BUT
STILL
A
SIGNIFICANT
FRACTION
OF
THE
COST
OF
BANDWIDTH
THE
RELATIVE
COST
OF
ENERGY
VERSUS
BANDWIDTH
HAS
BEEN
RISING
THIS
IS
PRIMARILY
DUE
TO
DECREASES
IN
BANDWIDTH
COSTS
WE
CANNOT
CANNOT
IGNORE
BANDWIDTH
COSTS
IN
OUR
ANALYSIS
THE
COMPLICATION
IS
THAT
THE
BANDWIDTH
PRICING
SPECIFICS
ARE
CONSIDERED
TO
BE
PROPRIETARY
INFORMATION
THEREFORE
OUR
TREATMENT
OF
BANDWIDTH
COSTS
IN
THIS
PAPER
WILL
BE
RELATIVELY
ABSTRACT
AKAMAI
DOES
NOT
VIEW
BANDWIDTH
PRICES
AS
BEING
GEO
GRAPHICALLY
DIFFERENTIATED
IN
SOME
INSTANCES
A
COMPANY
AS
LARGE
AS
AKAMAI
CAN
NEGOTIATE
CONTRACTS
WITH
CARRIERS
ON
A
NATIONWIDE
BASIS
SMALLER
REGIONAL
PROVIDERS
MAY
PROVIDE
TRANSIT
FOR
FREE
PRICES
ARE
USUALLY
SET
PER
NETWORK
PORT
US
ING
THE
BASIC
BILLING
MODEL
TRAFFIC
IS
DIVIDED
INTO
FIVE
MINUTE
INTERVALS
AND
THE
PERCENTILE
IS
USED
FOR
BILLING
OUR
APPROACH
IN
THIS
PAPER
IS
TO
ESTIMATE
PERCENTILES
FROM
THE
TRAFFIC
DATA
AND
THEN
TO
CONSTRAIN
OUR
ENERGY
PRICE
REROUTING
SO
THAT
IT
DOES
NOT
INCREASE
THE
PERCENTILE
BANDWIDTH
FOR
ANY
LOCATION
CLIENT
SERVER
DISTANCES
LACKING
ANY
NETWORK
LEVEL
DATA
ON
CLIENTS
WE
USE
GEOGRAPHIC
DISTANCE
AS
A
COARSE
PROXY
FOR
NETWORK
PERFORMANCE
IN
OUR
SIMULATIONS
WE
SEE
SOME
EVIDENCE
OF
GEO
LOCALITY
IN
THE
AKAMAI
TRAFFIC
DATA
BUT
THERE
ARE
MANY
CASES
WHERE
CLIENTS
ARE
NOT
MAPPED
TO
THE
NEAR
EST
CLUSTER
GEOGRAPHICALLY
ONE
REASON
IS
THAT
GEOGRAPHICAL
DISTANCE
DOES
NOT
ALWAYS
CORRESPOND
TO
OPTIMAL
NETWORK
PER
FORMANCE
ANOTHER
POSSIBILITY
IS
THAT
THE
SYSTEM
IS
TRYING
TO
KEEP
THOSE
CLIENTS
ON
THE
SAME
NETWORK
EVEN
IF
AKAMAI
SERVERS
ON
THAT
NETWORK
ARE
GEOGRAPHICALLY
FAR
AWAY
YET
ANOTHER
POSSIBILITY
IS
THAT
CLIENTS
ARE
BEING
MOVED
TO
DISTANT
CLUSTERS
BECAUSE
OF
BANDWIDTH
CONSTRAINTS
MODELING
ENERGY
CONSUMPTION
IN
ORDER
TO
ESTIMATE
BY
HOW
MUCH
WE
CAN
REDUCE
ENERGY
COSTS
WE
MUST
FIRST
MODEL
THE
SYSTEM
ENERGY
CONSUMPTION
FOR
EACH
CLUSTER
WE
USE
DATA
FROM
THE
AKAMAI
CDN
AS
A
REPRESENTATIVE
REAL
WORLD
WORKLOAD
THIS
DATA
IS
USED
TO
DE
RIVE
A
DISTRIBUTION
OF
CLIENT
ACTIVITY
CLUSTER
SIZES
AND
CLUSTER
LOCATIONS
WE
THEN
USE
AN
ENERGY
MODEL
TO
MAP
PRICES
AND
CLUSTER
TRAFFIC
ALLOCATIONS
TO
ELECTRICITY
EXPENSES
THE
MODEL
IS
ADMITTEDLY
SIMPLISTIC
OUR
GOAL
IS
NOT
TO
PROVIDE
ACCURATE
FIGURES
BUT
RATHER
TO
ESTIMATE
BOUNDS
ON
SAVINGS
CLUSTER
ENERGY
CONSUMPTION
WE
MODEL
THE
ENERGY
CONSUMPTION
OF
A
CLUSTER
AS
BE
ING
PROPORTIONAL
ROUGHLY
LINEAR
TO
ITS
UTILIZATION
MULTIPLE
STUDIES
HAVE
SHOWN
THAT
CPU
UTILIZATION
IS
A
GOOD
ESTIMATOR
FOR
POWER
USAGE
OUR
MODEL
IS
ADAPTED
FROM
GOOGLE
EMPIRICAL
STUDY
OF
A
DATA
CENTER
IN
WHICH
THEIR
MODEL
WAS
FOUND
TO
ACCURATELY
LESS
THAN
ERROR
PREDICT
THE
DY
NAMIC
POWER
DRAWN
BY
A
GROUP
OF
MACHINES
RACKS
WE
AUGMENT
THIS
MODEL
TO
FILL
IN
SOME
MISSING
PIECES
AND
PARAMETRIZE
IT
USING
OTHER
PUBLISHED
STUDIES
AND
MEASURE
MENTS
OF
SERVERS
AT
AKAMAI
LET
PCLUSTER
BE
THE
POWER
USAGE
OF
A
CLUSTER
AND
LET
UT
BE
ITS
AVERAGE
CPU
UTILIZATION
BETWEEN
AND
AT
TIME
T
PCLUSTER
UT
F
N
V
UT
N
Ǫ
WHERE
N
IS
THE
NUMBER
OF
SERVERS
IN
THE
CLUSTER
F
IS
THE
FIXED
POWER
V
IS
THE
VARIABLE
POWER
AND
Ǫ
IS
AN
EMPIRICALLY
DERIVED
CORRECTION
CONSTANT
SEE
F
N
N
PIDLE
P
U
E
PPEAK
V
UT
N
N
PPEAK
PIDLE
UR
WHERE
PIDLE
IS
THE
AVERAGE
IDLE
POWER
DRAW
OF
A
SINGLE
SERVER
PPEAK
IS
THE
AVERAGE
PEAK
POWER
AND
THE
EXPONENT
R
IS
AN
EMPIRICALLY
DERIVED
CONSTANT
EQUAL
TO
SEE
THE
EQUA
TION
FOR
V
IS
TAKEN
DIRECTLY
FROM
THE
ORIGINAL
PAPER
A
LINEAR
MODEL
R
WAS
ALSO
FOUND
TO
BE
REASONABLY
ACCURATE
WE
ADDED
THE
PUE
COMPONENT
SINCE
THE
GOOGLE
STUDY
DID
NOT
ACCOUNT
FOR
COOLING
ETC
WITH
POWER
MANAGEMENT
THE
IDLE
POWER
CONSUMPTION
OF
A
SERVER
CAN
BE
AS
LOW
AS
OF
THE
PEAK
POWER
CONSUMP
TION
WHICH
CAN
RANGE
FROM
WITHOUT
POWER
MANAGEMENT
AN
OFF
THE
SHELF
SERVER
PURCHASED
IN
THE
LAST
SEVERAL
YEARS
AVERAGES
AROUND
AND
DRAWS
OF
ITS
PEAK
POWER
WHEN
IDLE
BASED
ON
MEASURED
VALUES
ULTIMATELY
WE
WANT
TO
USE
THIS
MODEL
IN
SIMULATION
TO
ESTIMATE
THE
MAXIMUM
PERCENTAGE
REDUCTION
IN
THE
ENERGY
COSTS
OF
SOME
SERVER
DEPLOYMENT
PATTERN
CONSEQUENTLY
THE
ABSOLUTE
VALUES
CHOSEN
FOR
PPEAK
AND
PIDLE
ARE
UNIMPORTANT
THEIR
RATIO
IS
WHAT
MATTERS
IN
FACT
IT
TURNS
OUT
THAT
THE
ERGY
DISSIPATED
BY
EACH
PACKET
PASSING
THROUGH
A
CORE
ROUTER
WOULD
BE
AS
LOW
AS
A
ΜJ
PER
MEDIUM
SIZED
PACKET
WE
MUST
ALSO
CONSIDER
WHAT
HAPPENS
IF
THE
NEW
ROUTES
OVERLOAD
EXISTING
ROUTERS
IF
WE
USE
ENOUGH
ADDITIONAL
BAND
WIDTH
THROUGH
A
ROUTER
IT
MAY
HAVE
TO
BE
UPGRADED
TO
HIGHER
CAPACITY
HARDWARE
INCREASING
THE
ENERGY
SIGNIFICANTLY
HOW
EVER
WE
COULD
PREVENT
THIS
BY
INCORPORATING
CONSTRAINTS
LIKE
THE
BANDWIDTH
CONSTRAINTS
WE
USE
SIMULATION
PROJECTING
SAVINGS
IN
ORDER
TO
TEST
THE
CENTRAL
THESIS
OF
THIS
PAPER
WE
CON
DUCTED
A
NUMBER
OF
SIMULATIONS
QUANTIFYING
AND
ANALYSING
THE
IMPACT
OF
DIFFERENT
ROUTING
POLICIES
ON
ENERGY
COSTS
AND
CLIENT
SERVER
DISTANCE
OUR
RESULTS
SHOW
THAT
ELECTRICITY
COSTS
CAN
PLAUSIBLY
BE
RE
DUCED
BY
UP
TO
AND
THAT
THE
DEGREE
OF
SAVINGS
PRIMARILY
DEPENDS
ON
THE
ENERGY
ELASTICITY
OF
THE
SYSTEM
IN
ADDITION
TO
BANDWIDTH
AND
PERFORMANCE
CONSTRAINTS
WE
SIMULATE
AKAMAI
BANDWIDTH
CONSTRAINTS
AND
SHOW
THAT
OVERALL
SYSTEM
COSTS
CAN
BE
REDUCED
WE
ALSO
SKETCH
THE
RELATION
SHIP
BETWEEN
CLIENT
SERVER
DISTANCE
AND
SAVINGS
FINALLY
WE
INVESTIGATE
HOW
DELAYING
THE
SYSTEM
REACTION
TO
PRICE
DIF
FERENTIALS
AFFECTS
SAVINGS
SIMULATION
STRATEGY
WE
CONSTRUCTED
A
SIMPLE
DISCRETE
TIME
SIMULATOR
THAT
STEP
PED
THROUGH
THE
AKAMAI
USAGE
STATISTICS
LETTING
A
ROUTING
MODULE
WITH
A
GLOBAL
VIEW
OF
THE
NETWORK
ALLOCATE
TRAFFIC
TO
CLUSTERS
AT
EACH
TIME
STEP
USING
THESE
ALLOCATIONS
WE
MOD
ELED
EACH
CLUSTER
ENERGY
CONSUMPTION
AND
USED
OBSERVED
HOURLY
MARKET
PRICES
TO
CALCULATE
ENERGY
EXPENDITURES
BE
PCLUSTER
PCLUSTER
IS
CRITICAL
IN
DETERMINING
THE
SAVINGS
THAT
FORE
PRESENTING
THE
RESULTS
WE
PROVIDE
SOME
DETAILS
ABOUT
CAN
BE
ACHIEVED
USING
PRICE
DIFFERENTIAL
AWARE
ROUTING
IDEALLY
PCLUSTER
WOULD
BE
ZERO
AN
IDLE
CLUSTER
WOULD
CONSUME
NO
ENERGY
AT
PRESENT
ACHIEVING
THIS
WITHOUT
IM
PACTING
PERFORMANCE
IS
STILL
AN
OPEN
CHALLENGE
HOWEVER
THERE
IS
AN
INCREASING
INTEREST
IN
ENERGY
PROPORTIONAL
COM
PUTING
AND
DYNAMIC
SERVER
PROVISIONING
TECHNIQUES
ARE
BEING
EXPLORED
BY
BOTH
ACADEMICS
AND
INDUSTRY
WE
ARE
CONFIDENT
THAT
PCLUSTER
WILL
CONTINUE
TO
FALL
INCREASE
IN
ROUTING
ENERGY
IN
OUR
SCHEME
CLIENTS
MAY
BE
ROUTED
TO
DISTANT
SERVERS
IN
SEARCH
OF
CHEAP
ENERGY
FROM
AN
ENERGY
PERSPECTIVE
THIS
NETWORK
PATH
EXPANSION
REPRESENTS
ADDITIONAL
WORK
THAT
MUST
BE
PERFORMED
BY
SOMETHING
IF
THIS
INCREASE
IN
ENERGY
WERE
SIGNIFICANT
NETWORK
PROVIDERS
MIGHT
ATTEMPT
TO
PASS
THE
ADDITIONAL
COST
ON
TO
THE
SERVER
OPERATORS
GIVEN
WHAT
WE
KNOW
ABOUT
BANDWIDTH
PRICING
A
SMALL
INCREASE
IN
ROUTING
ENERGY
SHOULD
NOT
IMPACT
BANDWIDTH
PRICES
ALTER
NATIVELY
SERVER
OPERATORS
MAY
BEAR
ALL
THE
INCREASED
ENERGY
COSTS
SUPPOSE
THEY
RUN
THE
INTERMEDIATE
ROUTERS
A
SIMPLE
ANALYSIS
SUGGESTS
THAT
THE
INCREASED
PATH
LENGTHS
WILL
NOT
SIGNIFICANTLY
ALTER
ENERGY
CONSUMPTION
ROUTERS
ARE
NOT
DESIGNED
TO
BE
ENERGY
PROPORTIONAL
AND
THE
ENERGY
USED
BY
A
PACKET
TO
TRANSIT
A
ROUTER
IS
MANY
ORDERS
OF
MAGNITUDE
BELOW
THE
ENERGY
EXPENDED
AT
THE
ENDPOINTS
E
G
GOOGLE
KJ
QUERY
WE
ESTIMATE
THAT
THE
AVERAGE
ENERGY
NEEDED
FOR
A
PACKET
TO
PASS
THROUGH
A
CORE
ROUTER
IS
ON
THE
ORDER
OF
MJ
FURTHER
WE
ESTIMATE
THAT
THE
INCREMENTAL
EN
REPORTED
FOR
A
CISCO
GSR
ROUTER
MID
SIZED
PACK
ETS
SEC
AND
WATTS
MEASURED
OUR
SIMULATION
SETUP
ELECTRICITY
PRICES
WE
USED
HOURLY
REAL
TIME
MARKET
PRICES
FOR
TWENTY
NINE
DIFFERENT
LOCATIONS
HUBS
HOWEVER
WE
ONLY
HAVE
TRAFFIC
DATA
FOR
AKAMAI
PUBLIC
CLUSTERS
IN
NINE
OF
THESE
LOCATIONS
THEREFORE
MOST
OF
THE
SIMULATIONS
FOCUSED
ON
THESE
NINE
LOCATIONS
OUR
DATA
SET
CONTAINED
MONTHS
OF
PRICE
DATA
SPANNING
JANUARY
THROUGH
MARCH
UNLESS
NOTED
OTHERWISE
WE
ASSUMED
THE
SYSTEM
REACTED
TO
THE
PREVIOUS
HOUR
PRICES
TRAFFIC
AND
SERVER
DATA
THE
AKAMAI
WORKLOAD
DATA
SET
CONTAINS
MINUTE
SAMPLES
FOR
THE
HITS
PER
SECOND
OB
SERVED
AT
PUBLIC
CLUSTERS
IN
TWENTY
FIVE
CITIES
FOR
A
PERIOD
OF
DAYS
AND
SOME
HOURS
EACH
SAMPLE
ALSO
PROVIDES
A
MAP
SPECIFYING
WHERE
HITS
ORIGINATED
GROUPING
CLIENTS
BY
STATE
AND
WHICH
CITY
THEY
WERE
ROUTED
TO
WE
HAD
TO
DISCARD
SEVEN
OF
THESE
CITIES
BECAUSE
OF
A
LACK
OF
ELECTRICITY
MARKET
DATA
FOR
THEM
THE
REMAINING
EIGHTEEN
CITIES
WERE
GROUPED
BY
ELECTRICITY
MARKET
HUB
AS
NINE
CLUS
TERS
IN
OUR
DAY
SIMULATION
WE
USED
THE
TRAFFIC
INCIDENT
ON
THESE
NINE
CLUSTERS
IN
ORDER
TO
SIMULATE
LONGER
PERIODS
WE
DERIVED
A
SYN
THETIC
WORKLOAD
FROM
THE
DAY
AKAMAI
WORKLOAD
US
TRAF
FIC
ONLY
WE
CALCULATED
AN
AVERAGE
HIT
RATE
FOR
EVERY
HUB
AND
CLIENT
STATE
PAIR
WE
PRODUCED
A
DIFFERENT
AVERAGE
FOR
EACH
HOUR
OF
THE
DAY
AND
EACH
DAY
OF
THE
WEEK
ADDITIONALLY
THE
AKAMAI
DATA
ALLOWED
US
TO
DERIVE
CAPAC
POWER
CONSUMPTION
OF
IDLE
ROUTER
IS
THE
PEAK
POWER
IN
THE
FUTURE
POWER
AWARE
HARDWARE
MAY
REDUCE
THIS
DISPARITY
BETWEEN
THE
MARGINAL
AND
AVERAGE
ENERGY
ENERGY
MODEL
PARAMETERS
IDLE
POWER
PUE
FIGURE
THE
SYSTEM
ENERGY
ELASTICITY
IS
KEY
IN
DE
TERMINING
THE
DEGREE
OF
SAVINGS
PRICE
CONSCIOUS
ROUTING
CAN
ACHIEVE
FURTHER
OBEYING
EXISTING
BANDWIDTH
CONSTRAINTS
REDUCES
BUT
DOES
NOT
ELIMINATE
SAVINGS
THE
GRAPH
SHOWS
DAY
SAVINGS
FOR
A
NUMBER
OF
DIFFERENT
PUE
AND
PIDLE
VALUES
WITH
A
DISTANCE
THRESHOLD
THE
SAVINGS
FOR
EACH
ENERGY
MODEL
ARE
GIVEN
AS
A
PER
CENTAGE
OF
THE
TOTAL
ELECTRICITY
COST
OF
RUNNING
AKAMAI
ACTUAL
ROUTING
SCHEME
UNDER
THAT
ENERGY
MODEL
ITY
CONSTRAINTS
AND
THE
PERCENTILE
HITS
AND
BANDWIDTH
FOR
EACH
CLUSTER
CAPACITY
ESTIMATES
WERE
DERIVED
USING
OB
SERVED
HIT
RATES
AND
CORRESPONDING
REGION
LOAD
LEVEL
DATA
PROVIDED
BY
AKAMAI
OUR
SIMULATIONS
USE
HITS
RATHER
THAN
THE
BANDWIDTH
NUMBERS
FROM
THE
DATA
MOST
OF
OUR
SIMULATIONS
USED
AKAMAI
GEOGRAPHIC
SERVER
DISTRIBUTION
ALTHOUGH
THE
DETAILS
OF
THE
DISTRIBUTION
MAY
INTRODUCE
ARTIFACTS
INTO
OUR
RESULTS
THIS
IS
A
REAL
WORLD
DISTRI
BUTION
AS
SUCH
WE
FEEL
RELYING
ON
IT
RATHER
THAN
RELYING
ON
SYNTHETIC
DISTRIBUTIONS
MAKES
OUR
RESULTS
MORE
COMPELLING
ROUTING
SCHEMES
IN
OUR
SIMULATIONS
WE
LOOK
AT
TWO
ROUTING
SCHEMES
AKAMAI
ORIGINAL
ALLOCATION
AND
A
DIS
TANCE
CONSTRAINED
ELECTRICITY
PRICE
OPTIMIZER
GIVEN
A
CLIENT
THE
PRICE
CONSCIOUS
OPTIMIZER
MAPS
IT
TO
A
CLUSTER
WITH
THE
LOWEST
PRICE
ONLY
CONSIDERING
CLUSTERS
WITHIN
SOME
MAXIMUM
RADIAL
GEOGRAPHIC
DISTANCE
FOR
CLIENTS
THAT
DO
NOT
HAVE
ANY
CLUSTERS
WITHIN
THAT
MAXIMUM
DISTANCE
THE
ROUTING
SCHEME
FINDS
THE
CLOSEST
CLUSTER
AND
CONSIDERS
ANY
OTHER
NEARBY
CLUSTERS
IF
THE
SELECTED
CLUSTER
IS
NEARING
ITS
CAPACITY
OR
THE
BOUNDARY
THE
OPTIMIZER
ITERATIVELY
FINDS
ANOTHER
GOOD
CLUSTER
THE
PRICE
OPTIMIZER
HAS
TWO
PARAMETERS
THAT
MODULATE
ITS
BEHAVIOUR
A
DISTANCE
THRESHOLD
AND
A
PRICE
THRESHOLD
ANY
PRICE
DIFFERENTIALS
SMALLER
THAN
THE
PRICE
THRESHOLD
ARE
IGNORED
WE
USE
MWH
SETTING
THE
DISTANCE
THRESHOLD
TO
ZERO
GIVES
AN
OPTIMAL
DISTANCE
SCHEME
SELECT
THE
CLUSTER
GEOGRAPHICALLY
CLOSEST
TO
CLIENT
SETTING
IT
TO
A
VALUE
LARGER
THAN
THE
EAST
WEST
COAST
DISTANCE
GIVES
AN
OPTIMAL
PRICE
SCHEME
ALWAYS
SELECT
THE
CLUSTER
WITH
THE
LOWEST
PRICE
WE
ARE
NOT
PROPOSING
THIS
AS
A
CANDIDATE
FOR
IMPLEMEN
TATION
BUT
IT
ALLOWS
US
TO
BENCHMARK
HOW
WELL
A
PRICE
CONSCIOUS
SCHEME
COULD
DO
AND
TO
INVESTIGATE
TRADE
OFFS
BE
TWEEN
DISTANCE
CONSTRAINTS
AND
ACHIEVABLE
SAVINGS
ENERGY
MODEL
WE
USE
THE
CLUSTER
ENERGY
MODEL
FROM
SECTION
WE
SIMULATED
THE
RUNNING
COST
OF
THE
SYSTEM
USING
A
NUMBER
OF
DIFFERENT
VALUES
FOR
THE
PEAK
SERVER
POWER
PPEAK
IDLE
SERVER
POWER
PIDLE
AND
THE
PUE
THIS
SECTION
DISCUSSES
NORMALIZED
COSTS
AND
PIDLE
IS
ALWAYS
EXPRESSED
AS
A
PERCENTAGE
OF
PPEAK
SOME
ENERGY
PARAMETERS
THAT
WE
USED
OPTIMISTIC
FUTURE
IDLE
PUE
CUTTING
EDGE
GOOGLE
IDLE
PUE
STATE
OF
THE
ART
IDLE
PUE
DISABLED
POWER
MANAGEMENT
IDLE
PUE
CLIENT
SERVER
DISTANCE
GIVEN
A
CLIENT
ORIGIN
STATE
AND
THE
SERVER
LOCATION
HUB
OUR
DISTANCE
METRIC
CALCU
LATES
A
POPULATION
DENSITY
WEIGHTED
GEOGRAPHIC
DISTANCE
WE
USED
CENSUS
DATA
TO
DERIVE
BASIC
POPULATION
DENSITY
FUNCTIONS
FOR
EACH
US
STATE
WHEN
THE
TRAFFIC
CONTAINS
CLIENTS
FROM
OUTSIDE
THE
US
WE
IGNORE
THEM
IN
THE
DISTANCE
CALCULATIONS
WE
USE
THIS
FUNCTION
AS
A
COARSE
MEASURE
FOR
NETWORK
DIS
TANCE
THE
GRANULARITY
OF
THE
AKAMAI
DATA
SET
DOES
NOT
PRO
VIDE
ENOUGH
INFORMATION
FOR
US
TO
ESTIMATE
NETWORK
LATENCY
BETWEEN
CLIENTS
AND
SERVERS
OR
EVEN
TO
ACCURATELY
CALCULATE
GEOGRAPHIC
DISTANCES
BETWEEN
CLIENTS
AND
SERVERS
AT
THE
TURN
OF
THE
YEAR
DAYS
OF
TRAFFIC
WE
BEGIN
BY
ASKING
THE
QUESTION
WHAT
WOULD
HAVE
HAP
PENED
IF
AN
AKAMAI
LIKE
SYSTEM
HAD
USED
PRICE
CONSCIOUS
ROUTING
AT
THE
END
OF
HOW
WOULD
THIS
HAVE
COM
PARED
IN
COST
AND
CLIENT
SERVER
DISTANCE
TO
THE
CURRENT
ROUT
ING
METHODS
EMPLOYED
BY
AKAMAI
ENERGY
ELASTICITY
WE
FIND
THAT
THE
ANSWER
HINGES
ON
THE
ENERGY
ELASTICITY
CHARACTERISTICS
OF
THE
SYSTEM
FIGURE
ILLUSTRATES
THIS
WHEN
CONSUMPTION
IS
COMPLETELY
PROPOR
TIONAL
TO
LOAD
USING
PRICE
CONSCIOUS
ROUTING
COULD
ELIMINATE
OF
THE
ELECTRICITY
EXPENDITURE
OF
AKAMAI
TRAFFIC
ALLOCA
TION
WITHOUT
APPRECIABLY
INCREASING
CLIENT
SERVER
DISTANCES
AS
IDLE
SERVER
POWER
AND
PUE
RISE
WE
SEE
A
DRAMATIC
DROP
IN
POSSIBLE
SAVINGS
AT
GOOGLE
PUBLISHED
ELASTICITY
LEVEL
IDLE
PUE
THE
MAXIMUM
SAVINGS
HAVE
DROPPED
TO
INELASTICITY
CONSTRAINS
OUR
ABILITY
TO
ROUTE
POWER
DEMAND
AWAY
FROM
HIGH
PRICES
BANDWIDTH
COSTS
A
REDUCED
ELECTRIC
BILL
MAY
BE
OVER
SHADOWED
BY
INCREASED
BANDWIDTH
COSTS
FIGURE
THEREFORE
ALSO
SHOWS
THE
SAVINGS
WHEN
WE
PREVENT
CLUSTERS
FROM
HAV
ING
HIGHER
PERCENTILE
HIT
RATES
THAN
WERE
OBSERVED
IN
THE
AKAMAI
DATA
WE
SEE
THAT
CONSTRAINING
BANDWIDTH
IN
THIS
WAY
MAY
CAUSE
ENERGY
SAVINGS
TO
DROP
DOWN
TO
ABOUT
A
THIRD
OF
THEIR
EARLIER
VALUES
HOWEVER
THE
GOOD
NEWS
IS
THAT
THESE
SAVINGS
ARE
REDUCTIONS
IN
THE
TOTAL
OPERATING
COST
BY
JOINTLY
OPTIMIZING
BANDWIDTH
AND
ELECTRICITY
IT
SHOULD
BE
POSSIBLE
TO
ACQUIRE
PART
OF
THE
ECONOMIC
VALUE
REPRESENTED
BY
THE
DIFFERENCE
BETWEEN
SAVINGS
WITH
AND
WITHOUT
BAND
WIDTH
CONSTRAINTS
DISTANCE
AND
SAVINGS
THE
SAVINGS
IN
FIGURE
DO
NOT
REPRESENT
A
FREE
LUNCH
THE
MEAN
CLIENT
SERVER
DISTANCE
MAY
NEED
TO
INCREASE
TO
LEVERAGE
MARKET
DIVERSITY
THE
PRICE
CONSCIOUS
ROUTING
SCHEME
WE
USE
HAS
A
DIS
TANCE
THRESHOLD
PARAMETER
ALLOWING
US
TO
EXPLORE
HOW
HIGHER
CLIENT
SERVER
DISTANCES
LEAD
TO
LOWER
ELECTRIC
BILLS
FIGURE
SHOWS
HOW
INCREASING
THE
DISTANCE
THRESHOLD
CAN
BE
USED
TO
REDUCE
ELECTRICITY
COSTS
FIGURE
SHOWS
HOW
CLIENT
SERVER
DISTANCES
CHANGE
IN
RESPONSE
TO
CHANGES
IN
THE
THRESHOLD
AT
A
DISTANCE
THRESHOLD
OF
THE
PERCENTILE
ESTIMATED
CLIENT
SERVER
DISTANCES
IS
AT
MOST
THIS
SHOULD
PROVIDE
AN
ACCEPTABLE
LEVEL
OF
PERFORMANCE
THE
DIS
TANCE
BETWEEN
BOSTON
AND
ALEXANDRIA
IN
VIRGINIA
IS
ABOUT
AND
NETWORK
RTTS
ARE
AROUND
AT
THIS
THRESHOLD
USING
THE
FUTURE
ENERGY
MODEL
THE
SAV
INGS
IS
SIGNIFICANT
BETWEEN
OBEY
CONSTRAINTS
AND
THERE
IS
AN
ELBOW
AT
A
THRESHOLD
OF
CAUSING
BOTH
SAVINGS
AND
DISTANCES
TO
JUMP
THE
DISTANCE
BETWEEN
BOSTON
AND
CHICAGO
IS
ABOUT
AFTER
THIS
INCREAS
ING
THE
THRESHOLD
PROVIDES
DIMINISHING
RETURNS
00
00
DISTANCE
THRESHOLD
KM
DISTANCE
THRESHOLD
KM
FIGURE
DAY
ELECTRICITY
COSTS
FALL
AS
THE
DISTANCE
THRESHOLD
IS
INCREASED
THE
COSTS
SHOWN
HERE
ARE
FOR
A
IDLE
PUE
MODEL
NORMALIZED
TO
THE
COST
OF
THE
AKAMAI
ALLOCATION
FIGURE
MONTH
ELECTRICITY
COSTS
FALL
AS
THE
DISTANCE
THRESHOLD
IS
INCREASED
THE
COSTS
SHOWN
HERE
ARE
FOR
A
IDLE
PUE
MODEL
NORMALIZED
TO
THE
COST
OF
THE
SYNTHETIC
AKAMAI
LIKE
ALLOCATION
BOSTON
DC
BOSTON
CHICAGO
MEAN
DISTANCE
IGNORE
PERCENT
IGNORE
MEAN
DISTANCE
PERCENT
DISTANCE
THRESHOLD
KM
MA
NY
IL
VA
NJ
MA
NY
IL
VA
NJ
FIGURE
INCREASING
THE
DISTANCE
THRESHOLD
ALLOWS
THE
ROUTING
OF
CLIENTS
TO
CHEAPER
CLUSTERS
FURTHER
AWAY
FIGURE
MA
NY
IL
VA
NJ
MA
NY
IL
VA
NJ
SHOWS
CORRESPONDING
FALLING
COST
FIGURE
SHOWS
RESULTS
FOR
A
SPECIFIC
SET
OF
SERVER
ENERGY
PARAMETERS
BUT
OTHER
PARAMETERS
GIVE
SCALED
CURVES
WITH
THE
SAME
BASIC
SHAPES
THIS
FOLLOWS
ANALYTICALLY
FROM
OUR
ENERGY
MODEL
EQUATIONS
IN
THE
DIFFERENCE
IN
SCALE
CAN
BE
SEEN
IN
FIGURE
SYNTHETIC
WORKLOAD
MONTHS
OF
PRICES
THE
PREVIOUS
SECTION
USES
A
VERY
SMALL
SUBSET
OF
THE
PRICE
DATA
WE
HAVE
USING
A
SYNTHETIC
WORKLOAD
DERIVED
FROM
THE
ORIGINAL
DAY
ONE
WE
RAN
SIMULATIONS
COVERING
JANUARY
THROUGH
MARCH
OUR
RESULTS
SHOW
THAT
SAVINGS
INCREASE
ABOVE
THOSE
FOR
THE
DAY
PERIOD
FIGURE
SHOWS
HOW
ELECTRICITY
COST
VARIED
WITH
THE
DIS
TANCE
THRESHOLD
ANALOGOUS
TO
FIGURE
THE
RESULTS
ARE
SIMILAR
TO
WHAT
WE
SAW
FOR
THE
DAY
CASE
BUT
MAXIMUM
SAVINGS
ARE
HIGHER
NOTABLY
THRESHOLDS
ABOVE
IN
FIGURE
DO
NOT
EXHIBIT
SHARPLY
DIMINISHING
RETURNS
LIKE
THOSE
SEEN
IN
IN
ORDER
TO
NORMALIZE
PRICES
WE
USED
STATIS
TICS
OF
HOW
AKAMAI
ROUTED
CLIENTS
TO
MODEL
AN
AKAMAI
LIKE
ROUTER
AND
CALCULATED
ITS
MONTH
COST
FIGURE
BREAKS
DOWN
THE
SAVINGS
BY
CLUSTER
SHOWING
THE
CHANGE
IN
COST
FOR
EACH
CLUSTER
THE
LARGEST
SAVINGS
IS
SHOWN
AT
NYC
THIS
IS
NOT
SURPRISING
SINCE
THE
HIGHEST
PEAK
PRICES
TEND
TO
BE
IN
NYC
THESE
SAVINGS
ARE
NOT
ACHIEVED
BY
ALWAYS
ROUTING
REQUESTS
AWAY
FROM
NYC
THE
LIKELIHOOD
OF
REQUESTS
BEING
ROUTED
TO
NYC
DEPENDS
ON
THE
TIME
OF
DAY
WE
SIMULATED
OTHER
SERVER
DISTRIBUTIONS
EVENLY
DISTRIBUTED
ACROSS
ALL
HUBS
HETEROGENEOUS
DISTRIBUTIONS
ETC
AND
SAW
SIMILAR
DECREASING
COST
DISTANCE
CURVES
FIGURE
CHANGE
IN
PER
CLUSTER
COST
FOR
MONTH
SIM
ULATIONS
WITH
DIFFERENT
DISTANCE
THRESHOLDS
THIS
USES
THE
FUTURE
MODEL
AND
OBEYS
CONSTRAINTS
DYNAMIC
BEATS
STATIC
IN
PARTICULAR
WE
SEE
THAT
WHEN
CONSTRAINTS
ARE
IGNORED
THE
DYNAMIC
COST
MINIMIZATION
SOLUTION
CAN
BE
SUBSTANTIALLY
BETTER
THAN
A
STATIC
ONE
IN
FIGURE
WE
SEE
THAT
THE
DYNAMIC
SOLUTION
COULD
REDUCE
THE
ELECTRICITY
COST
DOWN
TO
ALMOST
WHILE
MOVING
ALL
THE
SERVERS
TO
THE
REGION
WITH
THE
LOWEST
AVERAGE
PRICE
WOULD
ONLY
REDUCE
COST
DOWN
TO
REACTION
DELAYS
NOT
REACTING
IMMEDIATELY
TO
PRICE
CHANGES
NOTICEABLY
RE
DUCES
OVERALL
SAVINGS
IN
OUR
SIMULATIONS
WE
WERE
CONSERVA
TIVE
AND
ASSUMED
THAT
THERE
WAS
A
ONE
HOUR
DELAY
BETWEEN
THE
MARKET
SETTING
NEW
PRICES
AND
THE
SYSTEM
PROPAGATING
NEW
ROUTES
FIGURE
SHOWS
HOW
INCREASING
THE
REACTION
DELAY
IMPACTS
PRICES
FIRST
NOTE
THE
INITIAL
JUMP
BETWEEN
AN
IMMEDIATE
REACTION
AND
A
NEXT
HOUR
REACTION
THIS
IMPLIES
ACHIEVABLE
SAVINGS
WILL
EXCEED
WHAT
WE
HAVE
CALCULATED
FOR
SYSTEMS
THAT
CAN
UPDATE
THEIR
ROUTES
IN
LESS
THAN
AN
HOUR
FURTHER
NOTE
THE
LOCAL
MINIMA
AT
THE
HOUR
MARK
THIS
IS
PROBABLY
BECAUSE
MARKET
PRICES
CAN
BE
CORRELATED
FOR
A
GIVEN
HOUR
FROM
ONE
DAY
TO
THE
NEXT
THE
INCREASE
IN
COST
IS
SUBSTANTIAL
WITH
THE
IDLE
PUE
ENERGY
MODEL
THE
MAXIMUM
SAVINGS
IS
AROUND
SEE
FIGURE
SO
A
SUBSEQUENT
INCREASE
IN
COST
OF
WOULD
ELIMINATE
A
LARGE
CHUNK
OF
THE
SAVINGS
DELAY
IN
REACTING
TO
PRICES
HOURS
ALTERNATIVELY
CUSTOMERS
COULD
ENROLL
IN
TRIGGERED
DEMAND
RESPONSE
PROGRAMS
AGREEING
TO
REDUCE
THEIR
POWER
USAGE
IN
RESPONSE
TO
A
REQUEST
BY
THE
GRID
OPERATORS
LOAD
REDUCTION
REQUESTS
ARE
SENT
OUT
WHEN
ELECTRICITY
DEMAND
IS
HIGH
ENOUGH
TO
PUT
GRID
RELIABILITY
AT
RISK
OR
RISING
DEMAND
REQUIRES
THE
IMMINENT
ACTIVATION
OF
EXPENSIVE
UNRELIABLE
GENERATION
AS
SETS
THE
ADVANCE
NOTICE
GIVEN
BY
THE
RTO
CAN
RANGE
FROM
DAYS
TO
MINUTES
PARTICIPATING
CUSTOMERS
ARE
COMPENSATED
FIGURE
IMPACT
OF
PRICE
DELAYS
ON
ELECTRICITY
COST
FOR
A
IDLE
PUE
MODEL
WITH
A
DISTANCE
THRESHOLD
OF
ACTUAL
ELECTRICITY
BILLS
IN
THIS
PAPER
WE
ASSUME
THAT
POWER
BILLS
ARE
BASED
ON
HOURLY
MARKET
PRICES
AND
ON
ENERGY
CONSUMPTION
ADDI
TIONALLY
WE
ASSUME
THAT
THE
DECISIONS
OF
SERVER
OPERATORS
WILL
NOT
AFFECT
MARKET
PRICES
THE
STRENGTH
OF
THIS
APPROACH
IS
THAT
WE
CAN
USE
PRICE
DATA
TO
QUANTIFY
HOW
MUCH
MONEY
WOULD
HAVE
BEEN
SAVED
HOWEVER
IN
REALITY
ACHIEVING
THESE
SAVINGS
WOULD
PROBABLY
REQUIRE
A
RENEGOTIATION
OF
EXISTING
UTILITY
CONTRACTS
FURTHER
MORE
RATHER
THAN
PASSIVELY
REACTING
TO
SPOT
PRICES
ACTIVE
PARTICIPATION
OPENS
UP
ADDITIONAL
POSSIBILITIES
EXISTING
CONTRACTS
IT
IS
SAFE
TO
SAY
THAT
MOST
CURRENT
CONTRACTUAL
ARRANGEMENTS
WOULD
REDUCE
THE
POTENTIAL
SAV
INGS
BELOW
WHAT
OUR
ANALYSIS
INDICATES
THAT
SAID
SERVER
OPERATORS
SHOULD
BE
ABLE
TO
NEGOTIATE
DEALS
THAT
ALLOW
THEM
TO
CAPTURE
AT
LEAST
SOME
OF
THIS
VALUE
WHOLESALE
INDEXED
ELECTRIC
BILLING
PLANS
ARE
BECOMING
IN
CREASINGLY
COMMON
THROUGHOUT
THE
US
THIS
ALLOWS
SMALL
COMPANIES
THAT
DO
NOT
PARTICIPATE
DIRECTLY
IN
THE
WHOLESALE
MARKET
TO
TAKE
ADVANTAGE
OF
OUR
TECHNIQUES
THIS
BILLING
STRUCTURE
APPEALS
TO
ELECTRICITY
PROVIDERS
SINCE
RISK
IS
TRANS
FERRED
TO
CONSUMERS
FOR
EXAMPLE
IN
THE
MID
WEST
RTO
COMMONWEALTH
EDISON
OFFERS
A
REAL
TIME
PRICING
PROGRAM
CUSTOMERS
ENROLLED
IN
IT
ARE
BILLED
BASED
ON
HOURLY
CONSUMPTION
AND
CORRESPONDING
WHOLESALE
PJM
MISO
LOCA
TIONAL
MARKET
PRICES
COMPANIES
SUCH
AS
AKAMAI
RENTING
SPACE
IN
CO
LOCATION
FACILITIES
WILL
ALMOST
CERTAINLY
HAVE
TO
NEGOTIATE
A
NEW
BILLING
STRUCTURE
TO
GET
ANY
ADVANTAGE
FROM
OUR
APPROACH
MOST
CO
LOCATION
CENTERS
CHARGE
BY
THE
RACK
EACH
RACK
HAVING
A
MAXIMUM
POWER
RATING
IN
OTHER
WORDS
A
COMPANY
LIKE
AKAMAI
PAYS
FOR
PROVISIONED
POWER
AND
NOT
FOR
ACTUAL
POWER
USED
WE
SPECULATE
THAT
AS
ENERGY
COSTS
RISE
RELATIVE
TO
OTHER
COSTS
IT
WILL
BE
IN
THE
INTEREST
OF
CO
LOCATION
OWNERS
TO
CHARGE
BASED
ON
CONSUMPTION
AND
POSSIBLY
LOCATION
THERE
IS
EVI
DENCE
THAT
BANDWIDTH
COSTS
ARE
FALLING
BUT
ENERGY
COSTS
ARE
NOT
EVEN
IF
NEW
KINDS
OF
CONTRACTS
DO
NOT
ARISE
SERVER
OP
ERATORS
MAY
BE
ABLE
TO
SELL
THEIR
LOAD
FLEXIBILITY
THROUGH
A
SIDE
CHANNEL
LIKE
DEMAND
RESPONSE
AS
DISCUSSED
BELOW
BY
PASSING
INFLEXIBLE
CONTRACTS
SELLING
FLEXIBILITY
DISTRIBUTED
SYSTEMS
WITH
ENERGY
ELASTIC
CLUSTERS
CAN
BE
MORE
FLEXIBLE
THAN
TRADITIONAL
CON
SUMERS
OPERATORS
CAN
QUICKLY
AND
PRECIPITOUSLY
REDUCE
POWER
USAGE
AT
A
LOCATION
BY
SUSPENDING
SERVERS
AND
ROUTING
RE
QUESTS
ELSEWHERE
MARKET
MECHANISMS
ALREADY
EXIST
THAT
WOULD
ALLOW
OPERATORS
TO
VALUE
AND
SELL
THIS
FLEXIBILITY
SOME
RTOS
ALLOW
ENERGY
USERS
TO
BID
NEGAWATTS
NEGA
TIVE
DEMAND
OR
LOAD
REDUCTIONS
INTO
THE
DAY
AHEAD
MARKET
AUCTION
THIS
IS
BELIEVED
TO
MODERATE
PRICES
BASED
ON
THEIR
FLEXIBILITY
AND
LOAD
DEMAND
RESPONSE
VARI
ANTS
EXIST
IN
EVERY
MARKET
WE
COVER
IN
THIS
PAPER
EVEN
CONSUMERS
USING
AS
LITTLE
AS
A
FEW
RACKS
CAN
PARTICIPATE
IN
SUCH
PROGRAMS
CONSUMERS
CAN
ALSO
BE
AGGRE
GATED
INTO
LARGE
BLOCS
THAT
REDUCE
LOAD
IN
CONCERT
THIS
IS
THE
APPROACH
TAKEN
BY
ENERNOC
A
COMPANY
THAT
COLLECTS
MANY
CONSUMERS
PACKAGES
THEM
AND
SELLS
THEIR
AGGREGATE
ABILITY
TO
MAKE
ON
DEMAND
REDUCTIONS
A
PACKAGE
OF
HOTELS
WOULD
FOR
EXAMPLE
REDUCE
LAUNDRY
VOLUME
IN
SYNC
TO
EASE
POWER
DEMAND
ON
THE
GRID
THE
GOOD
THING
ABOUT
SELLING
FLEXIBILITY
AS
A
PRODUCT
IS
THAT
THIS
IS
VALUED
EVEN
WHERE
WHOLESALE
MARKETS
DO
NOT
EXIST
IT
EVEN
WORKS
IF
PRICE
DIFFERENTIALS
DON
T
EXIST
E
G
FIXED
PRICE
CONTRACTS
OR
IN
HIGHLY
REGULATED
MARKETS
HOWEVER
WE
HAVE
IGNORED
THE
DEMAND
SIDE
HOW
DO
OP
ERATORS
CONSTRUCT
BIDS
FOR
THE
DAY
AHEAD
AUCTIONS
IF
THEY
DON
T
KNOW
NEXT
DAY
CLIENT
DEMAND
FOR
EACH
REGION
WHAT
HAPPENS
WHEN
OPERATORS
ARE
TOLD
TO
REDUCE
POWER
CONSUMP
TION
AT
A
LOCATION
WHEN
THERE
IS
A
CONCENTRATION
OF
ACTIVE
CLIENTS
NEARBY
IN
SYSTEMS
LIKE
AKAMAI
DEMAND
IS
GENER
ALLY
PREDICTABLE
BUT
THERE
WILL
BE
HEAVY
TRAFFIC
DAYS
THAT
ARE
IMPOSSIBLE
TO
PREDICT
THERE
IS
ANECDOTAL
EVIDENCE
THAT
DATA
CENTERS
HAVE
PARTIC
IPATED
IN
DEMAND
RESPONSE
PROGRAMS
HOWEVER
THE
AP
PLICABILITY
OF
DEMAND
RESPONSE
TO
SINGLE
DATA
CENTERS
IS
NOT
WIDELY
ACCEPTED
PARTICIPATING
DATA
CENTERS
MAY
FACE
ADDI
TIONAL
DOWNTIME
OR
PERIODS
OF
REDUCED
CAPACITY
CONVERSELY
WHEN
WE
LOOK
AT
LARGE
DISTRIBUTED
SYSTEMS
PARTICIPATION
IN
SUCH
PROGRAMS
IS
ATTRACTIVE
ESPECIALLY
WHEN
THE
BARRIERS
TO
ENTRY
ARE
SO
LOW
ONLY
A
FEW
RACKS
PER
LOCATION
ARE
NEEDED
TO
CONSTRUCT
A
MULTI
MARKET
DEMAND
RESPONSE
SYSTEM
FUTURE
WORK
SOME
CLEAR
AVENUES
FOR
FUTURE
WORK
EXIST
IMPLEMENTING
JOINT
OPTIMIZATION
EXISTING
SYSTEMS
ALREADY
HAVE
FRAMEWORKS
IN
PLACE
THAT
ENGINEER
TRAFFIC
TO
OPTIMIZE
FOR
BANDWIDTH
COSTS
PERFORMANCE
AND
RELIABILITY
DYNAMIC
ENERGY
COSTS
REPRESENT
ANOTHER
INPUT
THAT
SHOULD
BE
INTEGRATED
INTO
SUCH
FRAMEWORKS
RTO
INTERACTION
SERVICE
OPERATORS
CAN
INTERACT
WITH
RTOS
IN
MANY
WAYS
THIS
PAPER
HAS
PROPOSED
A
RELATIVELY
PASSIVE
APPROACH
IN
WHICH
OPERATORS
MONITOR
SPOT
PRICES
AND
REACT
TO
FAVOURABLE
CONDITIONS
AS
WE
DISCUSSED
IN
SECTION
THERE
ARE
OTHER
MARKET
MECHANISMS
IN
PLACE
THAT
SERVICE
OPERATORS
MAY
BE
ABLE
TO
EXPLOIT
THE
OPTIMAL
MARKET
PAR
TICIPATION
STRATEGY
IS
UNCLEAR
WEATHER
DIFFERENTIALS
DATA
CENTERS
EXPEND
A
LOT
OF
ENERGY
RUNNING
AIR
COOLING
SYSTEMS
UP
TO
OF
TOTAL
EN
ERGY
IN
MODERN
SYSTEMS
WHEN
AMBIENT
TEMPERATURES
ARE
LOW
ENOUGH
EXTERNAL
AIR
CAN
BE
USED
TO
RADICALLY
REDUCE
THE
POWER
DRAW
OF
THE
CHILLERS
AT
THE
SAME
TIME
WEATHER
TEMPERATURE
DIFFERENTIALS
ARE
COMMON
THIS
SUGGESTS
THAT
SIGNIFICANT
ENERGY
SAVINGS
CAN
BE
ACHIEVED
BY
DYNAMICALLY
ROUTING
REQUESTS
TO
SITES
WHERE
THE
HEAT
GENERATED
BY
SERV
ING
THE
REQUEST
IS
MOST
INEXPENSIVELY
REMOVED
UNLIKE
PRICE
DIFFERENTIALS
WHICH
REDUCE
COST
BUT
NOT
ENERGY
ROUTING
RE
QUESTS
TO
COOLER
REGIONS
MAY
BE
ABLE
TO
REDUCE
BOTH
ENVIRONMENTAL
COST
RATHER
THAN
ATTEMPTING
TO
MIN
IMIZE
THE
DOLLAR
COST
OF
THE
ENERGY
CONSUMED
A
SOCIALLY
RE
SPONSIBLE
SERVICE
OPERATOR
MAY
INSTEAD
CHOOSE
TO
USE
AN
ENVI
RONMENTAL
IMPACT
COST
FUNCTION
THE
ENVIRONMENTAL
IMPACT
OF
A
SERVICE
IS
TIME
VARYING
AN
OBVIOUS
COST
FUNCTION
IS
THE
CARBON
FOOTPRINT
OF
THE
ENERGY
USED
IN
GRIDS
THAT
AGGRE
GATE
ELECTRICITY
FROM
DIVERSE
PROVIDERS
THE
FOOTPRINT
VARIES
DEPENDING
UPON
WHAT
GENERATING
ASSETS
ARE
ACTIVE
WHETHER
POWER
PLANTS
ARE
OPERATING
NEAR
OPTIMAL
CAPACITY
AND
WHAT
MIXTURE
OF
FUELS
THEY
ARE
CURRENTLY
USING
THE
VARIATION
OC
CURS
AT
MULTIPLE
TIME
SCALES
E
G
SEASONAL
IS
THERE
WATER
TO
POWER
HYDRO
SYSTEMS
WEEKLY
WHAT
ARE
THE
RELATIVE
PRICES
OF
VARIOUS
FOSSIL
FUELS
AND
HOURLY
IS
THE
WIND
BLOWING
OR
THE
TIDE
GOING
OUT
ADDITIONALLY
CARBON
IS
NOT
THE
ONLY
POL
LUTANT
FOR
INSTANCE
POWER
PLANTS
ARE
THE
PRIMARY
STATION
ARY
SOURCES
OF
NITROGEN
OXIDE
IN
THE
US
DUE
TO
VARIATIONS
IN
WEATHER
AND
ATMOSPHERIC
CHEMISTRY
THE
TIMING
AND
LOCATION
OF
NOX
REDUCTIONS
DETERMINE
THEIR
EFFECTIVENESS
IN
REDUCING
GROUND
LEVEL
OZONE
CONCLUSION
THE
BOUNDS
DERIVED
IN
THIS
PAPER
SHOULD
NOT
BE
TAKEN
TOO
LITERALLY
OUR
COST
AND
TRAFFIC
MODELS
ARE
BASED
ON
ACTUAL
DATA
BUT
THEY
DO
INCORPORATE
A
NUMBER
OF
SIMPLIFYING
AS
SUMPTIONS
THE
MOST
RELEVANT
ASSUMPTIONS
ARE
PROBABLY
THAT
OPERATORS
CAN
DO
BETTER
BY
BUYING
ELECTRICITY
ON
THE
OPEN
MARKET
THAN
THROUGH
NEGOTIATED
LONG
TERM
CONTRACTS
AND
THAT
THE
VARIABLE
ENERGY
COSTS
ASSOCIATED
WITH
SER
VICING
A
REQUEST
ARE
A
SIGNIFICANT
FRACTION
OF
THE
TOTAL
COSTS
DESPITE
THESE
CAVEATS
IT
SEEMS
CLEAR
THAT
THE
NATURE
OF
GE
OGRAPHICAL
AND
TEMPORAL
DIFFERENCES
IN
THE
PRICE
OF
ELECTRICITY
OFFERS
OPERATORS
OF
LARGE
DISTRIBUTED
SYSTEMS
AN
OPPORTUNITY
TO
REDUCE
THE
COST
OF
SERVICING
REQUESTS
IT
SHOULD
BE
POS
SIBLE
TO
AUGMENT
EXISTING
OPTIMIZATION
FRAMEWORKS
TO
DEAL
WITH
ELECTRICITY
PRICES
THERMAL
MANAGEMENT
OF
DRAM
MEMORY
HAS
BECOME
A
CRITICAL
IS
SUE
FOR
SERVER
SYSTEMS
WE
HAVE
DONE
TO
OUR
BEST
KNOWLEDGE
THE
FIRST
STUDY
OF
SOFTWARE
THERMAL
MANAGEMENT
FOR
MEMORY
SUBSYS
TEM
ON
REAL
MACHINES
TWO
RECENTLY
PROPOSED
DTM
DYNAMIC
THERMAL
MANAGEMENT
POLICIES
HAVE
BEEN
IMPROVED
AND
IMPLE
MENTED
IN
LINUX
OS
AND
EVALUATED
ON
TWO
MULTICORE
SERVERS
A
DELL
POWEREDGE
SERVER
AND
A
CUSTOMIZED
INTEL
SERVER
TESTBED
THE
EXPERIMENTAL
RESULTS
FIRST
CONFIRM
THAT
A
SYSTEM
LEVEL
MEMORY
DTM
POLICY
MAY
SIGNIFICANTLY
IMPROVE
SYSTEM
PER
FORMANCE
AND
POWER
EFFICIENCY
COMPARED
WITH
EXISTING
MEMORY
BANDWIDTH
THROTTLING
SCHEME
A
POLICY
CALLED
DTM
ACG
ADAP
TIVE
CORE
GATING
SHOWS
PERFORMANCE
IMPROVEMENT
COMPARABLE
TO
THAT
REPORTED
PREVIOUSLY
THE
AVERAGE
PERFORMANCE
IMPROVEMENTS
ARE
AND
ON
THE
POWEREDGE
AND
THE
VS
FROM
THE
PREVIOUS
SIMULATION
BASED
STUDY
RESPECTIVELY
WE
ALSO
HAVE
SURPRISING
FINDINGS
THAT
REVEAL
THE
WEAKNESS
OF
THE
PREVIOUS
STUDY
THE
CPU
HEAT
DISSIPATION
AND
ITS
IMPACT
ON
DRAM
MEMORIES
WHICH
WERE
IGNORED
ARE
SIGNIFICANT
FACTORS
WE
HAVE
OBSERVED
THAT
THE
SECOND
POLICY
CALLED
DTM
CDVFS
COORDINATED
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
HAS
MUCH
BETTER
PERFOR
MANCE
THAN
PREVIOUSLY
REPORTED
FOR
THIS
REASON
THE
AVERAGE
IM
PROVEMENTS
ARE
AND
ON
THE
TWO
MACHINES
VS
FROM
THE
PREVIOUS
STUDY
RESPECTIVELY
IT
ALSO
SIGNIFICANTLY
REDUCES
THE
PROCESSOR
POWER
BY
AND
ENERGY
BY
ON
AVERAGE
CATEGORIES
AND
SUBJECT
DESCRIPTORS
B
PRIMARY
MEMORY
DESIGN
STYLES
GENERAL
TERMS
DESIGN
MANAGEMENT
PERFORMANCE
KEYWORDS
DRAM
MEMORIES
THERMAL
MANAGEMENT
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
SIGMETRICS
JUNE
ANNAPOLIS
MARYLAND
USA
COPYRIGHT
ACM
00
INTRODUCTION
THERMAL
MANAGEMENT
HAS
BEEN
A
FIRST
ORDER
CONSIDERATION
IN
PRO
CESSOR
AND
HARD
DISK
DESIGN
FOR
A
LONG
TIME
AND
NOW
IT
HAS
BECOME
CRITICALLY
IMPORTANT
IN
THE
DESIGN
OF
DRAM
MEMORY
SUBSYSTEMS
THIS
TREND
IS
DRIVEN
BY
THE
WIDE
ADOPTION
OF
MULTI
CORE
PROCESSORS
AND
THE
EVER
INCREASING
DEMAND
FOR
HIGH
CAPACITY
HIGH
BANDWIDTH
FROM
MEMORY
SUBSYSTEMS
FOR
EXAMPLE
A
CURRENT
SMALL
SCALE
TWO
WAY
SMP
SERVER
PROVIDES
PEAK
MEMORY
BANDWIDTH
OF
GB
AND
MAXIMUM
MEMORY
CAPACITY
OF
GB
TO
SUPPORT
UP
TO
EIGHT
PROCESSOR
CORES
THE
DESIGN
OF
HIGH
BANDWIDTH
AND
HIGH
DENSITY
DRAM
SUBSYSTEM
LEADS
TO
INCREASING
POWER
CONSUMPTION
AND
HEAT
GENERATION
THE
MAXIMUM
POWER
CONSUMPTION
OF
THE
MEMORY
SUB
SYSTEM
CAN
REACH
WATTS
WHICH
IS
IN
THE
SAME
RANGE
OF
THE
POWER
CONSUMED
BY
THE
PROCESSORS
WITH
THIS
DEGREE
OF
POWER
CONSUMP
TION
DRAM
POWER
AND
THERMAL
MANAGEMENT
HAS
BECOME
AN
UR
GENT
AND
CRITICAL
ISSUE
WHILE
STUDIES
HAVE
DONE
ON
DRAM
POWER
MANAGEMENT
THERE
LACKS
SYSTEMATIC
RESEARCH
ON
DRAM
THERMAL
MANAGEMENT
MODERN
COMPUTING
SYSTEMS
ARE
DESIGNED
WITH
COOLING
CAPABILI
TIES
TO
ALLOW
FULL
SYSTEM
PERFORMANCE
UNDER
NORMAL
OPERATING
CON
DITIONS
AS
WELL
AS
THERMAL
SOLUTIONS
TO
SAFEGUARD
THE
SYSTEMS
AGAINST
ADVERSE
SITUATIONS
AS
FOR
DRAM
MEMORY
SUBSYSTEMS
A
SIMPLE
THERMAL
SOLUTION
HAS
BEEN
USED
IN
SERVERS
WHEN
THE
MEMORY
SUBSYSTEM
IS
APPROACHING
A
CRITICAL
THERMAL
THRESHOLD
THE
MEM
ORY
CONTROLLER
THROTTLES
THE
MEMORY
BANDWIDTH
ANOTHER
SOLUTION
PROPOSED
FOR
MOBILE
SYSTEMS
SHUTS
DOWN
A
WHOLE
MEMORY
SUBSYS
TEM
IN
THOSE
SYSTEMS
MEMORY
THERMAL
MANAGEMENT
IS
USED
AS
A
PROTECTION
MECHANISM
THAT
ENSURES
SAFE
OPERATION
AND
PREVENTS
THERMAL
EMERGENCIES
UNDER
ABNORMAL
SCENARIOS
THESE
SCENARIOS
WHILE
NOT
COMMON
DO
OCCUR
IN
PRACTICE
THEY
CAN
BE
DUE
TO
A
POORLY
DESIGNED
THERMAL
SOLUTION
IN
OTHER
SUBSYSTEMS
SYSTEM
FAN
FAILURE
OBSTRUCTIONS
TO
AIRFLOW
WITHIN
A
SYSTEM
THERMALLY
CHALLENG
ING
WORKLOAD
MIX
OR
OTHER
REASONS
THAT
CAUSE
A
SYSTEM
TO
OPERATE
OUTSIDE
OF
ITS
THERMAL
DESIGN
BOUNDARIES
THERMAL
MANAGEMENT
IS
ALSO
NECESSARY
SINCE
TIME
CONSTANTS
WITH
WHICH
SILICON
COMPONENTS
CAN
CROSS
A
CRITICAL
THERMAL
LIMIT
ARE
MUCH
FASTER
THAN
THE
RESPONSE
TIME
OF
SYSTEM
LEVEL
COOLING
CONTROL
SUCH
AS
FAN
OUR
RESEARCH
GOAL
IS
TO
DESIGN
AND
IMPLEMENT
MEMORY
DTM
METHODS
FOR
SERVER
SYSTEMS
RUNNING
IN
CONSTRAINED
THERMAL
ENVI
RONMENTS
TO
IMPROVE
THE
SYSTEMS
PERFORMANCE
AND
OR
POWER
EFFI
CIENCY
THERMAL
MANAGEMENT
CAN
BE
A
NORMAL
FORM
WHEN
USERS
OR
SYSTEM
OPERATORS
MAKE
A
DECISION
TO
OPERATE
IN
MORE
CONSTRAINED
THERMAL
ENVIRONMENTS
INCLUDING
UNAVOIDABLE
HIGH
AMBIENT
TEM
PERATURES
THE
NEED
TO
LIMIT
FAN
SPEED
FOR
ACOUSTIC
REASONS
OR
THE
NECESSITY
TO
REDUCE
COOLING
COSTS
IN
DATA
CENTERS
THROTTLING
MEMORY
BANDWIDTH
UNDER
A
CERTAIN
THRESHOLD
CAN
PRE
VENT
MEMORY
TEMPERATURE
FROM
INCREASING
THUS
AVOIDING
ANY
POS
SIBILITY
OF
THERMAL
EMERGENCY
NEVERTHELESS
IT
MAY
LIMIT
THE
SYS
TEM
PERFORMANCE
UNNECESSARILY
A
CAREFULLY
DESIGNED
DTM
DY
NAMIC
THERMAL
MANAGEMENT
SCHEME
WHEN
USED
IN
COMBINATION
WITH
BANDWIDTH
THROTTLING
MAY
IMPROVE
SYSTEM
PERFORMANCE
AND
OR
IMPROVE
SYSTEM
POWER
EFFICIENCY
WITHOUT
PUTTING
THE
SYSTEM
IN
HAZ
ARD
A
RECENT
STUDY
PROPOSED
TWO
SCHEMES
THAT
DIRECTLY
THROT
TLE
THE
PROCESSOR
EXECUTION
THROUGH
CORE
GATING
SELECTIVELY
SHUT
TING
DOWN
PROCESSOR
CORES
OR
DVFS
DYNAMIC
VOLTAGE
AND
FRE
QUENCY
SCALING
FOR
MEMORY
THERMAL
MANAGEMENT
IT
EVALUATED
THOSE
SCHEMES
USING
A
DYNAMIC
DRAM
THERMAL
MODEL
AND
SIM
ULATION
AND
REPORTED
THAT
INTEGRATING
PROCESSOR
POWER
MANAGEMENT
AND
EXECUTION
CONTROL
WITH
MEMORY
THERMAL
MANAGEMENT
YIELDS
SIGNIFICANT
GAINS
IN
SYSTEM
PERFORMANCE
AND
ENERGY
EFFICIENCY
HOW
EVER
THE
WORK
HAS
TWO
LIMITATIONS
FIRST
THE
DRAM
THERMAL
MODEL
USED
FOR
EVALUATION
OF
DIFFERENT
THERMAL
MANAGEMENT
MECHANISMS
HAS
NOT
BEEN
VALIDATED
ON
REAL
SYSTEMS
GIVEN
THE
DEPENDENCY
BE
TWEEN
THE
ACCURACY
OF
THE
THERMAL
MODEL
AND
REPORTED
POWER
AND
PERFORMANCE
GAINS
IT
IS
NECESSARY
TO
CONFIRM
RESULTS
PRESENTED
IN
THE
STUDY
BY
IMPLEMENTING
AND
EVALUATING
THESE
APPROACHES
IN
REAL
SYSTEMS
SECOND
DUE
TO
INHERENT
LIMITATIONS
OF
RUNNING
LONG
WORK
LOAD
TRACES
IN
A
SIMULATOR
THE
DESIGN
SPACE
AND
PARAMETERS
OF
THE
PROPOSED
SCHEMES
WERE
NOT
FULL
EXPLORED
AND
ADEQUATELY
ANALYZED
TO
ADDRESS
THESE
ISSUES
WE
EVALUATE
THE
EXISTING
MEMORY
DTM
SCHEMES
ON
TWO
SERVER
SYSTEMS
OUR
STUDY
USES
MEASUREMENTS
ON
REAL
SYSTEMS
RUNNING
MULTIPROGRAMMING
WORKLOADS
WE
HAVE
IM
PLEMENTED
THESE
SCHEMES
IN
A
LINUX
OS
AND
EVALUATED
THEIR
PERFOR
MANCE
AND
POWER
EFFICIENCY
ON
TWO
SERVERS
CONFIGURED
WITH
LATEST
GENERATION
HARDWARE
A
DELL
POWEREDGE
SERVER
AND
AN
INTEL
SERVER
TESTBED
CALLED
AND
THERE
AFTER
TO
OBTAIN
AN
ACCURATE
PICTURE
OF
THE
POWER
AND
PERFORMANCE
BENEFITS
OF
MECHANISMS
EVALUATED
IN
THIS
PAPER
WE
INSTRUMENTED
THE
WITH
POWER
AND
THERMAL
SENSORS
TO
GET
FINE
GRAIN
MEASUREMENTS
AT
A
COMPONENT
LEVEL
TO
THE
BEST
OF
OUR
KNOWLEDGE
THIS
IS
THE
FIRST
STUDY
OF
SOFTWARE
THERMAL
MANAGEMENT
FOR
MEMORY
SUBSYSTEM
ON
REAL
MACHINES
WE
HAVE
DONE
COMPREHENSIVE
EXPERIMENTS
AND
DETAILED
ANALYSIS
RE
GARDING
PERFORMANCE
MEMORY
TEMPERATURE
PROFILES
AND
SYSTEM
POWER
AND
ENERGY
CONSUMPTION
OUR
EXPERIMENTS
FIRST
CONFIRM
THAT
THE
TWO
RECENTLY
PROPOSED
SCHEMES
SIGNIFICANTLY
IMPROVE
PERFORMANCE
OF
REAL
SERVER
SYSTEMS
IN
CONSTRAINED
THERMAL
ENVIRONMENTS
IN
AD
DITION
WE
HAVE
ENCOURAGING
FINDINGS
THAT
ADDRESS
THE
LIMITATIONS
OF
THE
PREVIOUS
WORK
COMPARED
WITH
THE
SIMPLE
DTM
BW
DTM
THROUGH
MEM
ORY
BANDWIDTH
THROTTLING
METHOD
THE
DTM
ACG
DTM
THROUGH
ADAPTIVE
CORE
GATING
SCHEME
IMPROVES
PER
FORMANCE
BY
UP
TO
AND
ON
THE
AND
SERVERS
RESPECTIVELY
AND
AND
ON
AVERAGE
RESPECTIVELY
THE
IMPROVEMENTS
OF
DTM
CDVFS
DTM
THROUGH
COORDINATED
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
ARE
UP
TO
AND
AND
AND
ON
AVERAGE
ON
THE
TWO
SERVERS
RESPECTIVELY
THE
PERFORMANCE
GAIN
OF
THE
DTM
CDVFS
SCHEME
MEA
SURED
ON
REAL
SYSTEMS
IS
MUCH
BETTER
THAN
THE
PREVIOUSLY
RE
PORTED
SIMULATION
RESULT
WHICH
IS
ONLY
ON
AVERAGE
BE
SIDES
THE
EXPECTED
PERFORMANCE
DIFFERENCE
DUE
TO
DIFFERENT
CONFIGURATIONS
OF
THE
REAL
SYSTEMS
AND
THE
SIMULATED
ONE
OUR
ANALYSIS
INDICATES
THAT
THE
CPU
HEAT
DISSIPATION
AND
ITS
INFLU
ENCE
ON
DRAM
WHICH
WERE
IGNORED
IN
THE
PREVIOUS
STUDY
IS
A
SIGNIFICANT
FACTOR
WE
HAVE
ALSO
FOUND
THAT
THE
DTM
CDVFS
METHOD
IMPROVES
THE
SYSTEM
POWER
EFFICIENCY
IN
ADDITION
TO
THE
PERFORMANCE
GAINS
IT
REDUCES
THE
PROCESSOR
POWER
RATE
BY
ON
THE
THE
ENERGY
CONSUMED
BY
THE
PROCESSOR
AND
MEM
ORY
IS
REDUCED
BY
ON
AVERAGE
WE
HAVE
EVALUATED
A
NEW
SCHEME
CALLED
DTM
COMB
THAT
COMBINES
DTM
ACG
AND
DTM
CDVFS
IT
MAY
STOP
A
SUB
SET
OF
CORES
AND
APPLY
DVFS
TO
THE
OTHERS
OUR
EXPERIMENTAL
RESULTS
SHOW
THAT
THE
NEW
SCHEME
MAY
FURTHER
IMPROVE
THE
PERFORMANCE
BY
UP
TO
WHEN
COMPARED
WITH
THE
SIMULATION
BASED
WORK
THIS
STUDY
IS
NOVEL
IN
MEMORY
DTM
IMPLEMENTATIONS
AND
THE
EXPERIMENTAL
METHODS
IT
IS
THE
FIRST
EFFORT
SHOWING
THAT
MEMORY
DTM
CAN
BE
IM
PLEMENTED
AS
PART
OF
OS
POWER
MANAGEMENT
AND
WORK
IN
CONJUNC
TION
WITH
EXISTING
POWER
MANAGEMENT
MECHANISMS
E
G
DVFS
THE
INSTRUMENTATION
WE
DID
ON
THE
TESTBED
IS
UNIQUE
AND
THE
ESTABLISHED
EXPERIMENTAL
METHOD
IS
MORE
CONVINCING
THAN
SIMULATION
WE
ARE
ABLE
TO
DO
EXTENSIVE
EXPERIMENTS
WHICH
BRING
SEVERAL
NEW
INSIGHTS
THE
REST
OF
THIS
PAPER
IS
ORGANIZED
AS
FOLLOWS
SECTION
PRESENTS
BACKGROUND
AND
RELATED
WORK
OF
THIS
STUDY
SECTION
DISCUSSES
OUR
DESIGN
AND
IMPLEMENTATION
OF
THE
DRAM
DTM
SCHEMES
ON
REAL
SYSTEMS
SECTION
DESCRIBES
THE
EXPERIMENTAL
METHODOLOGY
AND
WORKLOADS
SECTION
ANALYZES
THE
EXPERIMENTAL
RESULTS
AND
FINALLY
SECTION
SUMMARIZES
THE
STUDY
BACKGROUND
AND
RELATED
WORK
THERMAL
MANAGEMENT
IN
COMPUTER
SYSTEMS
THERMAL
MAN
AGEMENT
HAS
BECOME
A
MAJOR
RESEARCH
FOCUS
IN
RECENT
YEARS
MOST
STUDIES
SO
FAR
HAVE
FOCUSED
ON
PROCESSORS
AND
HARD
DISKS
IN
SERVER
SYSTEMS
AND
DATA
CENTERS
BROOKS
AND
MARTONOSI
STUDY
DIFFERENT
PROCESSOR
DTM
MECHANISMS
WHICH
INCLUDE
SCALING
THE
CLOCK
FRE
QUENCY
OR
VOLTAGE
SKADRON
ET
AL
DEVELOP
A
THERMAL
MODEL
FOR
INDIVIDUAL
FUNCTIONAL
BLOCKS
USING
THERMAL
RESISTANCES
AND
CA
PACITANCES
DERIVED
FROM
THE
LAYOUT
OF
THE
MICRO
ARCHITECTURE
STRUC
TURES
THEY
FURTHER
EXTEND
THE
MODEL
TO
HOTSPOT
WHICH
MOD
ELS
THERMAL
BEHAVIOR
AT
MICROARCHITECTURE
LEVEL
USING
A
NETWORK
OF
THERMAL
RESISTANCES
AND
CAPACITANCES
AND
CAN
IDENTIFY
THE
HOTTEST
UNIT
ON
CHIP
THEY
ALSO
PROPOSE
SEVERAL
SYSTEM
LEVEL
DTM
TECHNIQUES
FOR
EXAMPLE
MIGRATING
COMPUTATION
TO
UNDERUTILIZED
HARDWARE
UNITS
FROM
OVERHEATED
ONES
LI
ET
AL
STUDY
THE
THER
MAL
CONSTRAINTS
IN
THE
DESIGN
SPACE
OF
CMPS
DONALD
AND
MARTONOSI
EXPLORE
THE
DESIGN
SPACE
OF
THERMAL
MANAGEMENT
TECH
NIQUES
FOR
MULTICORE
PROCESSORS
REGARDING
THE
DTM
FOR
THE
HARD
DISK
DRIVES
GURUMURTHI
ET
AL
DEVELOP
MODELS
TO
CAPTURE
THE
CAPACITY
PERFORMANCE
AND
THERMAL
BEHAVIOR
OF
DISK
DRIVES
THEY
ALSO
PRESENT
TWO
DTM
TECHNIQUES
FOR
HARD
DISKS
EXPLOITING
THE
THERMAL
SLACK
OR
THROTTLING
DISK
ACTIVITIES
KIM
ET
AL
FURTHER
DEVELOP
A
PERFORMANCE
TEMPERATURE
SIMULATOR
OF
DISK
DRIVES
AND
STUDY
THE
THERMAL
BEHAVIOR
AND
MANAGEMENT
OF
STORAGE
SYSTEMS
US
ING
SERVER
WORKLOADS
ANOTHER
IMPORTANT
AND
RELATED
AREA
IS
APPLYING
DTM
TECHNIQUES
AT
THE
SYSTEM
AND
DATA
CENTER
LEVEL
MOORE
ET
AL
USE
TEMPERATURE
AWARE
WORKLOAD
PLACEMENT
ALGORITHM
TO
REDUCE
DATA
CENTER
COOLING
COSTS
HEATH
ET
AL
PROPOSE
MERCURY
A
TEMPERATURE
EMULATION
SUITE
FOR
SERVERS
THEY
ALSO
DEVELOP
AND
EVALUATE
FREON
A
SYSTEM
FOR
MANAGING
THERMAL
EMERGENCY
IN
SERVER
CLUSTERS
CHOI
ET
AL
PROPOSE
THERMOSTAT
A
CFD
BASED
TOOL
TO
STUDY
THERMAL
OPTIMIZA
TIONS
AT
RUN
TIME
IN
SERVER
SYSTEMS
AS
WELL
AS
THE
LAYOUT
OPTIMIZA
TION
IN
DESIGN
PHASE
THIS
STUDY
FOCUSES
ON
DRAM
MEMORY
SUBSYSTEMS
IN
INDIVIDUAL
SERVERS
THE
PROPOSED
METHODS
CAN
BE
USED
ALONG
WITH
THE
OTHER
METHODS
THERMAL
ISSUE
OF
AND
FB
DIMM
MEMORIES
WITH
THE
RAPID
INCREASE
OF
DRAM
CAPACITY
AND
BANDWIDTH
DRAM
MEM
ORY
SUBSYSTEM
NOW
CONSUMES
A
SIGNIFICANT
PORTION
OF
TOTAL
SYSTEM
POWER
THE
DRAM
MEMORY
HAS
A
GENERAL
TREND
OF
POWER
INCREASE
OF
PER
YEAR
AS
THE
BANDWIDTH
DOUBLES
EVERY
THREE
YEARS
THE
DRAM
DIE
SIZE
ITSELF
HAS
BEEN
GROWING
AT
A
RATE
OF
PER
YEAR
IN
THE
LATEST
GENERATION
SERVER
SYSTEMS
DRAM
POWER
CONSUMPTION
CAN
BE
COMPARABLE
TO
THAT
OF
PROCESSORS
DRAM
THERMAL
PROBLEM
HAS
BECOME
A
REAL
ISSUE
RECENTLY
FOR
BOTH
DRAM
AND
FULLY
BUFFERED
DIMM
FB
DIMM
A
RECENT
STUDY
HAS
REPORTED
THAT
ON
A
MOBILE
SYSTEM
THE
TEMPERATURE
OF
DRAM
DEVICES
MAY
EXCEED
THEIR
THERMAL
DESIGN
POINT
OF
C
WHEN
RUNNING
REAL
WORK
LOADS
AT
AN
AMBIENT
TEMPERATURE
OF
C
ON
SEVER
PLATFORMS
THE
RECENTLY
DEPLOYED
FB
DIMM
HAS
BECOME
A
FOCUS
FOR
DRAM
THERMAL
STUDIES
FB
DIMM
IS
DESIGNED
TO
SUPPORT
BOTH
HIGH
BANDWIDTH
AND
LARGE
CAPACITY
BY
USING
NARROW
AND
HIGH
FREQUENCY
CHANNELS
WITH
POINT
TO
POINT
COMMUNICATION
AN
AMB
ADVANCED
MEMORY
BUFFER
IS
PLACED
INTO
EACH
DIMM
FOR
TRANSFERRING
DATA
BETWEEN
THE
DRAM
DEVICES
AND
THE
MEMORY
CHANNEL
IN
FB
DIMM
AMB
IS
A
HOT
SPOT
BECAUSE
OF
ITS
HIGH
POWER
DENSITY
W
ATT
WITHOUT
THER
MAL
CONTROL
THE
TEMPERATURE
OF
AMB
CAN
EXCEED
ITS
THERMAL
DESIGN
POINT
OF
C
IN
PRACTICE
A
PRODUCT
SERVER
MAY
HOLD
THE
AMB
TEMPERATURE
UNDER
C
FOR
SAFETY
ADDITIONALLY
THE
HEAT
GENER
ATED
BY
AMB
WILL
SPREAD
TO
DRAM
DEVICES
WITH
A
LOWER
THERMAL
DESIGN
POINT
OF
C
MAKING
THEM
POTENTIAL
HOT
SPOTS
DRAM
THERMAL
BEHAVIOR
AS
MODELED
IN
THE
MICRON
POWER
CALCULATOR
AND
AN
INTEL
DATA
SHEET
THE
POWER
CONSUMPTION
OF
A
DRAM
MEMORY
SUBSYSTEM
IS
ALMOST
LINEAR
TO
THE
MEMORY
THROUGHPUT
IT
CONSISTS
OF
TWO
PARTS
STATIC
POWER
WHICH
IS
MOSTLY
A
CONSTANT
BUT
CONFIGURATION
DEPENDENT
AND
DYNAMIC
POWER
WHICH
IS
ALMOST
PROPORTIONAL
TO
MEMORY
THROUGHPUT
IF
THE
MEMORY
THROUGH
PUT
IS
KEPT
AT
A
CONSTANT
LEVEL
THE
DRAM
TEMPERATURE
WILL
RAISE
AND
STABILIZE
IN
A
FEW
MINUTES
THE
STABLE
TEMPERATURE
CAN
ROUGHLY
BE
DEFINED
AS
TSTABLE
TAMBIENT
P
PI
ΨI
WHERE
TAMBIENT
IS
THE
DRAM
AMBIENT
TEMPERATURE
ΨI
IS
THE
THERMAL
RESISTANCE
BETWEEN
A
COMPONENT
I
IN
THE
SYSTEM
AND
THE
DIMM
AND
PI
IS
THE
POWER
CONSUMED
BY
THE
COMPONENT
COMPONENTS
IN
THIS
EQUATION
INCLUDE
THE
DIMM
ITSELF
AND
MAY
INCLUDE
ADJACENT
DIMMS
AND
OTHER
SUB
SYSTEMS
LIKE
PROCESSOR
OR
HARD
DISK
IF
THE
THERMAL
INTERACTION
BE
TWEEN
THEM
AND
THE
DIMM
IS
NOT
NEGLIGIBLE
IN
FB
DIMM
THE
POWER
CONSUMPTION
OF
A
TYPICAL
AMB
IS
IN
THE
RANGE
OF
WATTS
AND
THAT
OF
A
TYPICAL
DRAM
DEVICE
IS
ABOUT
WATTS
DYNAMIC
THERMAL
MANAGEMENT
SCHEMES
FOR
MEMORIES
IN
PRACTICE
TWO
DTM
SCHEMES
HAVE
BEEN
USED
TO
PREVENT
AMB
OR
DRAM
DEVICES
FROM
OVERHEATING
IN
THERMAL
SHUTDOWN
THE
MEM
ORY
CONTROLLER
OR
THE
OPERATING
SYSTEM
PERIODICALLY
READS
THE
TEM
PERATURE
OF
DIMMS
FROM
THERMAL
SENSORS
LOCATED
ON
THE
DIMM
AND
IF
THE
READING
EXCEEDS
A
PRESET
THERMAL
THRESHOLD
STOPS
ALL
AC
CESSES
TO
MEMORY
UNTIL
THE
TEMPERATURE
DROPS
BELOW
THE
SPECIFIED
THRESHOLD
BY
A
CERTAIN
MARGIN
IN
BANDWIDTH
THROTTLING
THE
MEMORY
CONTROLLER
GRADUALLY
THROTTLES
MEMORY
THROUGHPUT
AS
TEM
PERATURE
STARTS
TO
RISE
TO
PREVENT
IT
FROM
CROSSING
CRITICAL
SHUTDOWN
THERMAL
THRESHOLD
THE
THROTTLING
IS
DONE
BY
COUNTING
AND
LIMITING
THE
NUMBER
OF
DRAM
ROW
ACTIVATIONS
IN
A
GIVEN
TIME
WINDOW
LIN
ET
AL
PROPOSE
A
DYNAMIC
DRAM
THERMAL
MODEL
AS
WELL
AS
THE
DTM
ACG
AND
DTM
CDVFS
SCHEMES
AND
EVALUATE
THEM
USING
A
SIMULATOR
THE
TWO
SCHEMES
ARE
DISCUSSED
IN
DETAIL
IN
SEC
TION
WE
FURTHER
EXTEND
THIS
WORK
IN
OUR
PAPER
AND
PROVIDE
DE
TAILED
EVALUATION
AND
ANALYSIS
OF
THESE
SCHEMES
ON
REAL
SYSTEMS
OTHER
RELATED
WORK
ISCI
ET
AL
HAVE
PROPOSED
A
RUN
TIME
PHASE
PREDICTION
METHOD
TO
MANAGE
MOBILE
PROCESSOR
POWER
CONSUMPTION
FOR
MEMORY
INTENSIVE
APPLICATIONS
THEY
USE
DVFS
DURING
MEMORY
BOUND
PHASES
OF
A
WORKLOAD
TO
REDUCE
PROCESSOR
POWER
BY
CONTRAST
DTM
CDVFS
IS
TRIGGERED
IN
THERMALLY
CON
STRAINED
SYSTEMS
AND
THE
OBJECTIVE
IS
TO
IMPROVE
PERFORMANCE
AND
REDUCE
THERMAL
HEAT
EXCHANGE
IN
ADDITION
TO
IMPROVING
PROCESSOR
POWER
EFFICIENCY
AND
THE
EVALUATION
IN
THIS
STUDY
HAS
BEEN
DONE
IN
A
MULTI
CORE
SERVER
SYSTEM
AS
OPPOSED
TO
A
SINGLE
CORE
MOBILE
PLATFORM
SINCE
MEMORY
TEMPERATURE
CHANGE
IS
MUCH
SLOWER
THAN
PROGRAM
PHASE
CHANGE
THERMAL
EMERGENCY
IS
LIKELY
A
MORE
RELIABLE
TRIGGER
FOR
DVFS
WITH
A
PERFORMANCE
TARGET
THOUGH
PHASE
PREDIC
TION
CAN
WORK
WHEN
THERMAL
EMERGENCY
DOES
NOT
APPEAR
ANOTHER
STUDY
BY
ISCI
ET
AL
PROPOSES
METHODS
TO
USE
PER
CORE
DVFS
IN
MANAGING
THE
POWER
BUDGET
OF
A
MULTICORE
PROCESSOR
BESIDES
THE
DIFFERENCE
THAT
THIS
STUDY
IS
FOCUSED
ON
MEMORY
THERMAL
MANAGE
MENT
PER
CORE
DVFS
IS
NOT
AVAILABLE
ON
OUR
PLATFORMS
DESIGN
AND
IMPLEMENTATION
ISSUES
IN
GENERAL
A
DTM
SCHEME
CAN
BE
DIVIDED
TWO
INTERDEPENDENT
PARTS
MECHANISM
AND
POLICY
THE
MECHANISM
ENFORCES
DTM
DE
CISIONS
MADE
BY
THE
POLICY
AND
ALSO
PROVIDES
INPUTS
TO
IT
WHILE
THE
POLICY
DECIDES
WHEN
AND
WHAT
THERMAL
ACTIONS
TO
TRIGGER
THE
GOAL
OF
A
DTM
POLICY
IS
TO
PREVENT
MEMORY
FROM
OVERHEATING
OR
GO
ING
ABOVE
ITS
MAXIMUM
SAFE
OPERATING
TEMPERATURE
THIS
IS
AC
COMPLISHED
BY
CONTINUOUSLY
MONITORING
MEMORY
TEMPERATURE
AND
FORCING
MEMORY
TO
REDUCE
ITS
POWER
BY
PUTTING
A
CAP
ON
MEMORY
THROUGHPUT
WHEN
THE
TEMPERATURE
CROSSES
A
PREDEFINED
THRESHOLD
CALLED
TCRITICAL
FIGURE
THE
BANDWIDTH
ALLOWED
AT
THIS
POINT
IS
DRASTICALLY
LIMITED
TO
PROTECT
THAT
PART
LEADING
TO
SIGNIFICANT
DEGRA
DATION
IN
SYSTEM
RESPONSIVENESS
AND
PERFORMANCE
TO
SMOOTH
THE
EFFECTS
OF
THERMAL
MANAGEMENT
AND
REDUCE
ITS
IM
PACT
ON
SYSTEM
PERFORMANCE
A
WELL
DESIGNED
DTM
POLICY
MAY
TRY
TO
REDUCE
MEMORY
BANDWIDTH
MORE
GRACEFULLY
WHEN
MEMORY
TEM
PERATURE
STARTS
TO
APPROACH
A
CRITICAL
THRESHOLD
TO
ACCOM
PLISH
THIS
A
DTM
POLICY
USUALLY
DEFINES
ANOTHER
THRESHOLD
CALLED
TTM
WHERE
AN
ADAPTIVE
DTM
MECHANISM
WHICH
SUPPORTS
MULTIPLE
RUNNING
LEVELS
STARTS
TO
GET
ACTIVATED
THE
RANGE
TTM
TCRITICAL
IS
NORMALLY
BROKEN
INTO
THERMAL
ZONES
WITH
EACH
THERMAL
ZONE
HAV
ING
AN
ASSOCIATED
SET
OF
ACTIONS
THAT
ARE
DESIGNED
TO
LOWER
MEMORY
TEMPERATURE
THERMAL
ZONES
WITH
HIGHER
TEMPERATURES
TRIGGER
MORE
AGGRESSIVE
ACTIONS
NECESSARY
TO
BRING
MEMORY
TEMPERATURE
WITHIN
THE
TTM
THRESHOLD
NOTE
THAT
IF
TEMPERATURE
EVER
CROSSES
TCRITICAL
THRESHOLD
AND
REACHES
TSHUTDOWN
POINT
THE
MEMORY
CONTROLLER
WILL
SHUT
DOWN
THE
MEMORY
SUBSYSTEM
TO
AVOID
ANY
PHYSICAL
DAMAGE
TO
THAT
PART
THOUGH
THIS
SHOULD
NEVER
HAPPEN
IN
A
PROPERLY
DESIGNED
SYSTEM
WITH
BANDWIDTH
THROTTLING
MEMORY
DTM
MECHANISMS
A
MEMORY
DTM
MECHANISM
SHOULD
GENERALLY
CONSIST
OF
THREE
COMPONENTS
A
MEMORY
TEMPERATURE
MONITOR
OR
ESTIMATOR
A
DTM
POLICY
TRIGGER
AND
A
METHOD
FOR
CONTROLLING
MEMORY
TEMPERATURE
WE
HAVE
DESIGNED
DIFFERENT
MECHANISMS
TO
SUPPORT
FOUR
THERMAL
MANAGEMENT
POLICIES
NAMELY
DTM
BW
DTM
ACG
DTM
CDVFS
AND
DTM
COMB
AND
IMPLEMENTED
THEM
ON
TWO
LINUX
SERVERS
WITH
INTEL
XEON
PROCESSORS
EACH
DTM
MECHANISM
IS
AN
IN
TEGRATION
OF
HARDWARE
SOFTWARE
COMPONENTS
THAT
TOGETHER
PROVIDE
THE
REQUIRED
FUNCTIONS
AND
CAPABILITIES
TO
SUPPORT
DTM
POLICY
FIGURE
THE
IDEA
OF
USING
THERMAL
ZONE
TO
GUIDE
THE
DESIGN
OF
MEMORY
DTM
POLICIES
THERMAL
EMERGENCY
LEVEL
WHICH
IS
USED
IN
THE
DISCUSSIONS
OF
OUR
SPECIFIC
DTM
IMPLEMENTATIONS
IS
THE
SAME
CONCEPT
TEMPERATURE
MONITORING
A
DTM
SCHEME
MAKES
THERMAL
MANAGEMENT
DECISIONS
BASED
ON
CURRENT
AND
POSSIBLY
PAST
OR
PRE
DICTED
FUTURE
MEMORY
TEMPERATURE
THE
TEMPERATURE
CAN
BE
EITHER
MEASURED
IF
THERMAL
SENSORS
ARE
AVAILABLE
OR
CONSERVATIVELY
PRE
DICTED
IF
OTHERWISE
TEMPERATURE
READINGS
AT
MULTIPLE
POINTS
SHOULD
BE
OBTAINED
BECAUSE
A
MEMORY
SUBSYSTEM
MAY
HAVE
MULTIPLE
HOT
SPOTS
WHERE
THE
TEMPERATURE
MAY
POTENTIALLY
CROSS
THE
CRITICAL
THER
MAL
POINT
OF
THAT
PART
IF
NO
THERMAL
SOLUTION
IS
USED
BOTH
OF
OUR
SERVERS
USE
FB
DIMM
BASED
MEMORY
SUBSYSTEMS
AND
BY
THEIR
CON
FIGURATIONS
THE
AMBS
ARE
HOT
THERMAL
SENSORS
ARE
LOCATED
IN
THE
AMB
OF
EACH
FB
DIMM
THUS
THE
AMB
TEMPERATURE
CAN
BE
DIRECTLY
MEASURED
AND
USED
FOR
MAKING
DTM
DECISIONS
IN
AD
DITION
OUR
SYSTEM
HOSTS
MULTIPLE
TEMPERATURE
SENSORS
THAT
MEASURE
THE
FRONT
PANEL
TEMPERATURE
SYSTEM
AMBIENT
CPU
INLET
TEMPERATURE
MEMORY
INLET
TEMPERATURE
AND
MEMORY
EXHAUST
TEMPERATURE
THOSE
TEMPERATURE
READINGS
DO
NOT
AFFECT
DTM
DECI
SIONS
BUT
ALLOW
US
TO
GET
A
DETAILED
THERMAL
PROFILE
DURING
PROGRAM
EXECUTION
POLICY
TRIGGER
A
DTM
POLICY
NEEDS
TO
PERIODICALLY
CHECK
WHETHER
ANY
OF
THE
THERMAL
THRESHOLDS
HAVE
BEEN
CROSSED
AND
IN
VOKE
THERMAL
CONTROL
LOGIC
IF
NECESSARY
WE
IMPLEMENT
THE
DTM
POLICY
AS
A
MONITORING
UTILITY
WHICH
IS
PERIODICALLY
WOKEN
UP
BY
THE
OS
SCHEDULER
WE
USED
A
DEFAULT
INTERVAL
OF
ONE
SECOND
IN
OUR
EXPERIMENTS
SINCE
DRAM
THERMAL
CONSTANTS
ARE
LARGE
IT
TAKES
ROUGHLY
A
FEW
HUNDRED
SECONDS
FOR
DRAM
DEVICES
OR
AMBS
TO
REACH
THEIR
TDPS
FROM
IDLE
STATE
ONE
SECOND
INTERVAL
IS
ADEQUATE
TO
TRIGGER
THERMAL
MANAGEMENT
ACTIONS
AND
PROTECT
MEMORY
FROM
OVERHEATING
IT
IS
ALSO
SUFFICIENTLY
LONG
TO
AVOID
ANY
NOTICEABLE
PER
FORMANCE
OVERHEAD
FROM
THE
MONITOR
ITSELF
MEMORY
THERMAL
CONTROL
METHODS
WHEN
A
THERMAL
THRESH
OLD
IS
CROSSED
AND
THE
EVENT
IS
DETECTED
BY
THE
MONITORING
UTIL
ITY
SOME
ACTIONS
NEED
TO
BE
TAKEN
TO
LOWER
MEMORY
TEMPERATURE
SINCE
DRAM
POWER
AND
THEREFORE
TEMPERATURE
ARE
CLOSELY
RELATED
TO
MEMORY
THROUGHPUT
WITH
ALL
OTHER
CONDITIONS
SUCH
AS
AIRFLOW
AND
INLET
TEMPERATURE
BEING
EQUAL
TEMPERATURE
CAN
BE
LOWERED
BY
REDUCING
MEMORY
THROUGHPUT
WE
HAVE
USED
THREE
APPROACHES
THAT
CONTROL
MEMORY
ACTIVITY
EITHER
FROM
THE
MEMORY
SIDE
OR
THE
PRO
CESSOR
SIDE
THE
FIRST
APPROACH
CALLED
BANDWIDTH
THROTTLING
SETS
A
LIMIT
ON
THE
NUMBER
OF
MEMORY
ACCESSES
THAT
ARE
ALLOWED
IN
A
CER
DRAM
DEVICES
CAN
BE
HOT
SPOTS
IN
OTHER
CONFIGURATIONS
OF
FB
DIMM
TAIN
TIME
WINDOW
INTEL
CHIPSET
USED
IN
BOTH
OF
OUT
SERVERS
ALLOWS
CLAMPING
THE
NUMBER
OF
DRAM
ROW
ACTIVATIONS
OVER
A
SPEC
IFIED
TIME
WINDOW
THE
DTM
BW
POLICY
USES
THIS
CAPABILITY
TO
THROTTLE
MEMORY
THROUGHPUT
AT
DIFFERENT
LEVELS
BASED
ON
CURRENT
THERMAL
ZONE
THE
SECOND
APPROACH
CALLED
CORE
GATING
REDUCES
MEMORY
THROUGHPUT
BY
LIMITING
THE
NUMBER
OF
ACTIVE
CORES
THROUGH
CPU
HOT
PLUG
MODULE
IN
THE
LINUX
KERNEL
VERSION
WHEN
A
CPU
IS
UNPLUGGED
IT
IS
LOGICALLY
REMOVED
FROM
THE
OS
AND
PLACED
INTO
A
SLEEP
STATE
BY
EXECUTING
A
HALT
INSTRUCTION
NOTE
THAT
THE
OVER
HEAD
OF
THIS
OPERATION
IS
VERY
SMALL
GIVEN
ONE
SECOND
GRANULARITY
OF
OUR
DTM
POLICIES
THE
LAST
APPROACH
USES
THE
FEATURE
OF
PROCES
SOR
VOLTAGE
AND
FREQUENCY
SCALING
TO
REDUCE
MEMORY
THROUGHPUT
THE
XEON
PROCESSORS
USED
IN
OUR
SERVERS
SUPPORT
FOUR
DIFFER
ENT
FREQUENCY
VOLTAGE
OPERATING
POINTS
GHZ
V
GHZ
V
GHZ
V
AND
GHZ
V
IN
THE
LATTER
TWO
APPROACHES
BANDWIDTH
THROTTLING
IS
EN
ABLED
WHEN
MEMORY
TEMPERATURE
IS
CLOSE
TO
ITS
THERMAL
THRESHOLD
TO
AVOID
ANY
POSSIBILITY
OF
OVERHEATING
MEMORY
DTM
POLICES
THE
OBJECTIVE
OF
A
DTM
POLICY
IS
TO
MAXIMIZE
THE
OVERALL
SYSTEM
PERFORMANCE
WITHOUT
CROSSING
THE
THERMAL
THRESHOLD
OF
ANY
PART
OF
THE
SYSTEM
A
SECONDARY
OBJECTIVE
IS
TO
IMPROVE
THE
OVERALL
SYSTEM
POWER
EFFICIENCY
IN
GENERAL
DTM
POLICIES
HAVE
TO
REDUCE
MEM
ORY
THROUGHPUT
TO
LOWER
MEMORY
TEMPERATURE
BUT
THEIR
STRATEGIES
CAN
BE
DIFFERENT
AS
DISCUSSED
ABOVE
IT
IS
IMPORTANT
TO
NOTE
THESE
DTM
POLICIES
DO
NOT
GUARANTEE
A
CERTAIN
LEVEL
OF
MEMORY
TEMPERA
TURE
OR
ACTIVITY
RATHER
THEY
CHANGE
MEMORY
OR
PROCESSOR
OPERATING
PARAMETERS
WITH
THE
AIM
OF
REDUCING
MEMORY
BANDWIDTH
WHILE
IN
CREASING
SYSTEM
ABILITY
TO
BETTER
TOLERATE
CONSTRAINED
THERMAL
EN
VIRONMENTS
IF
THESE
POLICIES
ARE
NOT
EFFECTIVE
IN
REDUCING
MEMORY
TEMPERATURE
THEY
WILL
ALL
ENABLE
BANDWIDTH
THROTTLING
AS
A
SAFE
GUARD
WHEN
MEMORY
TEMPERATURE
CROSSES
A
CRITICAL
TCRITICAL
THRESHOLD
AS
SHOWN
IN
FIGURE
THERMAL
EMERGENCY
LEVEL
AND
RUNNING
LEVEL
AS
DIS
CUSSED
EARLIER
A
GENERAL
APPROACH
IN
OUR
DTM
POLICY
DESIGN
IS
TO
QUANTIZE
MEMORY
TEMPERATURE
INTO
THERMAL
ZONES
OR
THERMAL
EMER
GENCY
LEVELS
AND
ASSOCIATE
EACH
LEVEL
WITH
A
SYSTEM
THERMAL
RUNNING
LEVEL
THE
USE
OF
RUNNING
LEVELS
HAS
APPEARED
IN
REAL
SYS
TEMS
I
E
IN
BANDWIDTH
THROTTLING
OF
INTEL
CHIPSET
IN
THIS
STUDY
SYSTEM
RUNNING
LEVELS
ARE
DEFINED
IN
TERMS
OF
PROCESSOR
FRE
QUENCY
NUMBER
OF
ACTIVE
CORES
AND
ALLOWED
MEMORY
BANDWIDTH
IN
GENERAL
A
THERMAL
RUNNING
LEVEL
WITH
BETTER
SYSTEM
PERFORMANCE
ALSO
GENERATES
MORE
HEAT
EVERY
TIME
A
POLICY
MODULE
IS
EXECUTED
IT
READS
THE
TEMPERATURE
SENSORS
DETERMINES
THE
THERMAL
EMERGENCY
LEVEL
AND
THEN
DECIDES
THE
THERMAL
RUNNING
LEVEL
FOR
THE
NEXT
TIME
INTERVAL
IF
THE
NEW
RUNNING
LEVEL
IS
DIFFERENT
FROM
THE
CURRENT
ONE
A
THERMAL
ACTION
WILL
BE
TAKEN
TO
CHANGE
THE
THERMAL
RUNNING
LEVEL
TABLE
DESCRIBES
THE
SETTINGS
OF
THE
THERMAL
EMERGENCY
LEVELS
AND
THE
THERMAL
RUNNING
LEVELS
FOR
THE
TWO
SERVERS
IT
IS
IMPORTANT
TO
NOTE
THAT
THE
NUMBER
OF
THE
EMERGENCY
LEVELS
AND
THAT
OF
THE
RUNNING
LEVELS
DO
NOT
HAVE
TO
EQUAL
THE
NUMBER
OF
THERMAL
ZONES
IS
BASED
ON
THE
INHERENT
RELATIONSHIP
BETWEEN
DRAM
TEMPERATURE
AND
MEMORY
BANDWIDTH
WHILE
THE
NUMBER
OF
THERMAL
RUNNING
LEVELS
IS
BASED
ON
THE
UNDERLYING
HW
CAPABILITIES
FOR
EXAMPLE
THERE
COULD
BE
MORE
THAN
FOUR
RUNNING
LEVELS
IF
THE
TWO
PROCESSORS
ARE
QUAD
CORE
ALSO
IT
IS
A
COINCIDENCE
THAT
DTM
ACG
AND
DTM
CDVFS
HAVE
THE
SAME
NUMBER
OF
RUNNING
LEVELS
WE
USED
THE
FOLLOWING
METHODOLOGY
TO
DEFINE
THERMAL
EMERGENCY
LEVEL
FOR
OUR
TWO
SYSTEMS
ON
THE
INTEL
WE
SET
TCRITICAL
THRESHOLD
TO
C
THIS
THRESHOLD
IS
BASED
ON
A
CONSERVATIVE
C
AMB
THERMAL
THRESHOLD
WITH
A
DEGREE
MARGIN
TO
ENSURE
SAFE
OP
ERATION
OF
THE
SYSTEM
THE
INTEL
IS
PUT
INTO
A
HOT
BOX
AND
THE
SYSTEM
AMBIENT
TEMPERATURE
IS
SET
TO
C
WHICH
EMULATES
A
CONSTRAINED
THERMAL
ENVIRONMENT
FOUR
THERMAL
EMERGENCY
LEV
ELS
ARE
THEN
DEFINED
IN
DECREMENTS
OF
DEGREES
THE
IS
LOCATED
AS
A
STANDALONE
BOX
IN
AN
AIR
CONDITIONED
ROOM
WITH
A
SYSTEM
AMBIENT
TEMPERATURE
OF
C
TO
BETTER
UNDERSTAND
THE
THERMAL
BEHAVIOR
OF
SUCH
A
SERVER
WITH
AN
AMBIENT
TEMPERATURE
OF
C
WE
ARTIFICIALLY
LOWER
THE
C
AMB
THERMAL
THRESHOLD
BY
C
AND
SET
TCRITICAL
TO
C
ACCORD
INGLY
THE
THERMAL
EMERGENCY
LEVERS
FOR
ARE
THEN
DEFINED
IN
DECREMENTS
OF
DEGREES
SIMILAR
TO
THE
SYSTEM
NOTE
THAT
IN
BOTH
SYSTEMS
THE
LOWEST
THERMAL
EMERGENCY
LEVEL
DOES
NOT
IMPOSE
ANY
CONSTRAINTS
AND
AL
LOWS
FOR
FULL
SYSTEM
PERFORMANCE
FOR
SAFETY
CONCERN
THE
CHIPSET
BANDWIDTH
THROTTLING
IS
ENABLED
WHEN
THE
SYSTEM
IS
IN
THE
HIGHEST
THERMAL
EMERGENCY
LEVEL
TO
ENSURE
THAT
OVERHEATING
WILL
NOT
HAPPEN
DTM
BW
POLICY
THIS
POLICY
ONLY
PERFORMS
BANDWIDTH
THROT
TLING
IT
RESEMBLES
THE
BANDWIDTH
THROTTLING
IN
INTEL
CHIPSET
AND
WE
USE
IT
AS
A
REFERENCE
TO
EVALUATE
OTHER
DTM
POLICIES
IT
USES
THE
BANDWIDTH
LIMITING
FUNCTION
OF
THE
CHIPSETS
WHICH
ARE
AVAIL
ABLE
IN
BOTH
SYSTEMS
TO
CAP
THE
BANDWIDTH
USAGE
ACCORDING
TO
THE
CURRENT
THERMAL
EMERGENCY
LEVEL
SETTING
THE
LIMIT
TO
ON
THE
WILL
GUARANTEE
THAT
THE
MEMORY
WILL
NOT
OVERHEAT
AND
SO
DOES
USING
THE
LIMIT
ON
THE
AS
TABLE
DESCRIBES
FOUR
THERMAL
RUNNING
LEVELS
ARE
USED
THE
LIMITS
ARE
ENFORCED
IN
THE
CHIPSET
BY
LIMITING
THE
NUMBER
OF
DRAM
ROW
ACTIVATIONS
IN
A
TIME
WINDOW
BECAUSE
CLOSE
PAGE
MODE
IS
USED
BANDWIDTH
USAGE
IS
MOSTLY
PROPORTIONAL
TO
NUMBER
OF
DRAM
ROW
ACTIVATIONS
THE
DE
FAULT
WINDOW
OF
IS
USED
WHICH
IS
SUGGESTED
BY
THE
CHIPSET
DESIGNERS
DTM
ACG
POLICY
THIS
POLICY
USES
CORE
GATING
TO
INDIRECTLY
REDUCE
MEMORY
TRAFFIC
IT
IS
BASED
ON
THE
OBSERVATION
THAT
WHEN
A
SUBSET
OF
CORES
IS
DISABLED
THE
CONTENTION
FOR
LAST
LEVEL
CACHE
FROM
DIFFERENT
THREADS
IS
REDUCED
LEADING
TO
REDUCED
MEMORY
TRAF
FIC
IF
THE
WORKLOAD
IS
BANDWIDTH
BOUND
THEN
SYSTEM
PERFORMANCE
WILL
BE
IMPROVED
NOTE
THAT
FOR
A
MEMORY
INTENSIVE
WORKLOAD
THE
MEMORY
THERMAL
CONSTRAINT
PUTS
A
LIMIT
ON
MEMORY
BANDWIDTH
THAT
THE
PROGRAM
EXECUTION
MAY
UTILIZE
THEREFORE
THE
GAIN
FROM
MEM
ORY
TRAFFIC
REDUCTION
CAN
MORE
THAN
OFFSET
THE
LOSS
OF
COMPUTATION
POWER
FROM
CORE
GATING
BOTH
SERVERS
HAVE
TWO
DUAL
CORE
PROCES
SORS
WE
ALWAYS
RETAIN
ONE
ACTIVE
CORE
PER
PROCESSOR
TO
FULLY
UTILIZE
THEIR
CACHES
TO
REDUCE
MEMORY
TRAFFIC
THEREFORE
THERMAL
RUNNING
LEVELS
ASSOCIATED
WITH
THE
LAST
TWO
THERMAL
EMERGENCY
LEVELS
SUP
PORT
THE
SAME
NUMBER
OF
ACTIVE
CORES
THE
MAIN
DIFFERENCE
BETWEEN
THEM
IS
THAT
IN
AS
WITH
OTHER
DTM
POLICIES
DESCRIBED
IN
THIS
SECTION
MAXIMUM
MEMORY
BANDWIDTH
THROTTLING
IS
ALSO
ENFORCED
NOTE
THAT
WHEN
ONE
OR
MORE
CORES
ARE
GATED
THE
THREADS
ALLOCATED
TO
THESE
CORES
GET
MIGRATED
TO
ACTIVE
CORES
RESULTING
IN
SERIALIZED
SHARING
OF
CORE
AND
CACHE
RESOURCES
WITH
OTHER
THREADS
WE
FOUND
THAT
THE
DEFAULT
SCHEDULING
INTERVAL
OF
OF
THE
LINUX
KERNEL
WORKS
WELL
CONTAINING
CACHE
THRASHING
WHILE
MAINTAINING
FAIRNESS
AND
SMOOTHNESS
IN
PROCESS
SCHEDULING
DTM
CDVFS
POLICY
THIS
POLICY
USES
PROCESSOR
DVFS
TO
INDIRECTLY
THROTTLE
MEMORY
TRAFFIC
THE
MAIN
SOURCE
OF
PERFORMANCE
IMPROVEMENT
AS
TO
BE
SHOWED
IN
SECTION
IS
THE
REDUCED
PROCESSOR
HEAT
GENERATION
LOWERING
PROCESSOR
VOLTAGE
AND
FREQUENCY
LEADS
TO
REDUCED
PROCESSOR
POWER
CONSUMPTION
AND
HEAT
EXCHANGE
WITH
OTHER
COMPONENTS
INCLUDING
MEMORY
CONSEQUENTLY
THE
MEMORY
SUBSYSTEM
MAY
RUN
AT
HIGH
SPEED
FOR
LONGER
TIME
THAN
NORMALLY
ALLOWED
THIS
EFFECT
HAS
BEEN
OBSERVED
ON
BOTH
SERVERS
BUT
IS
MORE
OBVIOUS
ON
THE
INTEL
BECAUSE
THE
PROCESSORS
ARE
LOCATED
RIGHT
IN
FRONT
OF
FB
DIMMS
ON
THE
AIRFLOW
PATH
IN
THAT
SYS
TEM
THE
FOUR
RUNNING
LEVELS
UNDER
THIS
POLICY
CORRESPOND
TO
FOUR
FREQUENCY
AND
VOLTAGE
SETTINGS
OF
XEON
CPU
IN
THIS
PAPER
DTM
CDVFS
POLICY
PERFORMS
FREQUENCY
VOLTAGE
TRANSITIONS
ON
ALL
PROCESSORS
AND
CORES
SIMULTANEOUSLY
AND
UNIFORMLY
A
POLICY
THAT
SUPPORTS
SUCH
DIFFERENTIATION
IS
WORTH
FURTHER
INVESTIGATION
BUT
WE
LEAVE
IT
TO
FUTURE
WORK
DTM
COMB
POLICY
DTM
COMB
COMBINES
DTM
ACG
AND
DTM
CDVFS
BY
INTEGRATING
CORE
GATING
AND
COORDINATED
DVFS
THE
GOAL
OF
THIS
POLICY
IS
TO
TAKE
FULL
ADVANTAGE
OF
THE
COMBINED
BENEFITS
OFFERED
BY
DTM
CDVFS
LARGER
THERMAL
HEADROOM
AND
REDUCED
MEMORY
TRAFFIC
ENABLED
BY
CORE
GATING
IN
DTM
ACG
TA
BLE
SHOWS
FOUR
THERMAL
RUNNING
LEVELS
DEFINED
FOR
THIS
POLICY
BY
COMBINING
THE
NUMBER
OF
ACTIVE
CORES
AND
PROCESSOR
OPERATING
FRE
QUENCY
SIMILAR
TO
DTM
ACG
WE
KEEP
AT
LEAST
ONE
CORE
ACTIVE
FOR
EACH
CPU
IN
DTM
COMB
TO
FULLY
UTILIZE
THE
CACHES
EXPERIMENTAL
METHODOLOGY
HARDWARE
AND
SOFTWARE
PLATFORMS
WE
CONDUCTED
OUR
EXPERIMENTS
ON
TWO
SMALL
SCALE
SERVERS
AS
STANDALONE
SYSTEMS
BOTH
MACHINES
USE
FB
DIMM
AND
THE
MEM
ORY
HOT
SPOTS
ARE
AMBS
THEREFORE
WE
ARE
ONLY
CONCERNED
WITH
AMB
TEMPERATURES
THEREAFTER
DRAM
DEVICES
CAN
BE
HOT
SPOTS
IN
DIFFERENT
CONFIGURATIONS
THE
FIRST
ONE
IS
A
DELL
POW
EREDGE
SERVER
PUT
IN
AN
AIR
CONDITIONED
ROOM
AS
A
STAN
DALONE
SYSTEM
IT
HAS
INTEL
CHIPSET
AND
TWO
DUAL
CORE
INTEL
XEON
PROCESSORS
EACH
HAS
A
SHARED
WAY
SET
ASSOCIATIVE
CACHE
AND
EACH
CORE
OF
THE
PROCESSOR
HAS
A
PRIVATE
INSTRUCTION
CACHE
AND
A
PRIVATE
DATA
CACHE
THE
MA
CHINE
HAS
TWO
FULLY
BUFFERED
DIMM
FB
DIMM
AS
THE
MAIN
MEMORY
THE
SECOND
MACHINE
IS
AN
INTEL
MA
CHINE
WHICH
IS
INSTRUMENTED
FOR
THERMAL
AND
POWER
STUDY
IT
HAS
ALMOST
THE
SAME
CONFIGURATION
AS
THE
EXCEPT
THAT
IT
HAS
FOUR
FB
DIMM
ON
THE
WE
ARE
ABLE
TO
MEA
SURE
THE
POWER
CONSUMPTION
OF
FB
DIMM
AND
PROCESSORS
AND
PRO
CESSOR
EXHAUST
TEMPERATURE
WHICH
IS
ALSO
THE
MEMORY
AMBIENT
TEM
PERATURE
ON
THIS
SYSTEM
THE
INSTRUMENTATION
ON
THE
AL
TERED
THE
AIR
FLOW
INSIDE
THE
MACHINE
MAKING
IT
LESS
STRONGER
THAN
AS
IN
A
PRODUCTION
MACHINE
THE
REPORTED
TEMPERATURE
READINGS
SHOULD
NOT
BE
ASSUMED
AS
ON
A
PRODUCT
MACHINE
AS
MENTIONED
BEFORE
WE
ARE
MORE
INTERESTED
IN
THE
THERMAL
BE
HAVIORS
OF
THOSE
MACHINES
IN
A
CONSTRAINED
THERMAL
ENVIRONMENT
THERMAL
EMERGENCY
LEVEL
MACHINE
AMB
TEMP
RANGE
C
AMB
TEMP
RANGE
C
THERMAL
RUNNING
LEVEL
MACHINE
DTM
BW
BANDWIDTH
NO
LIMIT
DTM
BW
BANDWIDTH
NO
LIMIT
DTM
ACG
OF
ACTIVE
CORES
BOTH
DTM
CDVFS
FREQUENCY
BOTH
DTM
COMB
OF
CORES
FREQUENCY
BOTH
TABLE
THERMAL
EMERGENCY
LEVELS
AND
THERMAL
RUNNING
LEVELS
FIGURE
INTEL
SYSTEM
WITH
THERMAL
SENSORS
T
FIGURE
THE
DAUGHTER
CARD
TO
EMULATE
SUCH
AN
ENVIRONMENT
WE
PUT
THE
INTEL
INTO
A
HOT
BOX
WHICH
ALLOWS
US
TO
SET
THE
SYSTEM
AMBIENT
TEMPERATURE
TO
C
ASSUMING
A
ROOM
TEMPERATURE
OF
C
AND
AN
ARBITRARY
C
INCREASE
IN
SYSTEM
AMBIENT
TEMPERATURE
WE
PUT
THE
INTO
AN
AIR
CONDITIONED
ROOM
OF
TEMPERATURE
C
AND
ARTIFICIALLY
LOWER
THE
THERMAL
DESIGN
POINT
OF
AMB
BY
C
IN
THE
IMPLEMENTATION
OF
DTM
POLICIES
WE
USE
THE
TWO
DIFFERENT
MACHINES
TO
CROSSCHECK
OUR
EXPERIMENT
RESULTS
AND
THE
ALLOWS
US
TO
EVALUATE
POWER
AND
ENERGY
SAVINGS
FIGURE
SHOWS
A
SYSTEM
DIAGRAM
OF
THE
SERVER
WE
INSTRUMENTED
THE
INTEL
WITH
SENSORS
THAT
MEASURE
VOLT
AGE
CURRENT
AND
TEMPERATURE
OF
DIFFERENT
SYSTEM
COMPONENTS
THE
ANALOG
SIGNALS
FROM
THE
POWER
AND
THERMAL
SENSORS
ARE
ROUTED
TO
A
CUSTOM
DESIGNED
DAUGHTER
CARD
THAT
HOSTS
AN
ARRAY
OF
A
D
CONVERT
ERS
AND
LOW
PASS
FILTERS
THE
DAUGHTER
CARD
IS
SHOWN
IN
FIGURE
THE
DATA
FROM
A
D
CONVERTERS
IS
SAMPLED
BY
A
MICRO
CONTROLLER
THAT
STORES
ALL
THE
DIGITAL
SENSOR
DATA
IN
A
LOCAL
BUFFER
THE
DAUGHTER
CARD
IS
CONNECTED
TO
THE
HOST
SYSTEM
THROUGH
A
LPC
LOW
PIN
COUNT
BUS
WE
HAVE
IMPLEMENTED
A
USER
SPACE
APPLICATION
THAT
ACCESSES
THE
DAUGHTER
CARD
USING
LINUX
LPC
DRIVER
THE
APPLICATION
READS
SENSOR
DATA
PERIODICALLY
AND
STORES
IT
TO
A
LOG
FILE
IN
ALL
EXPERI
MENTS
IN
THIS
PAPER
WE
USED
A
SAMPLING
RATE
OF
ONCE
PER
THIS
SAMPLING
RATE
IS
SUFFICIENT
GIVEN
AMB
THERMAL
CONSTANTS
AND
TIME
SCALES
OF
THERMAL
MANAGEMENT
MECHANISMS
EVALUATED
IN
OUR
STUD
IES
THE
SAMPLE
DATA
ARE
BUFFERED
INSIDE
THE
DAUGHTER
CARD
UNTIL
THE
BUFFER
IS
FULL
THEN
THEY
ARE
TRANSFERRED
TO
THE
SYSTEM
MEMORY
WE
HAVE
DONE
AN
EXTENSIVE
EVALUATION
TO
CALIBRATE
THE
SENSORS
AND
EN
SURE
THAT
SAMPLING
APPLICATION
DOES
NOT
INTRODUCE
ANY
OVERHEAD
OR
ARTIFACTS
IN
OUR
MEASUREMENTS
WE
HAVE
RUN
BENCHMARKS
AND
SYN
THETIC
WORKLOADS
WITH
AND
WITHOUT
OUR
SAMPLING
APPLICATION
AND
HAVE
NEVER
OBSERVED
ANY
MEASURABLE
IMPACT
ON
THEIR
PERFORMANCE
OR
SYSTEM
POWER
CONSUMPTION
THE
TWO
MACHINES
USE
THE
RED
HAT
ENTERPRISE
LINUX
WITH
KER
NEL
PERFORMANCE
DATA
ARE
COLLECTED
BY
PFMON
USING
PERF
MON
KERNEL
INTERFACE
AND
LIBPFM
LIBRARY
WE
ENABLE
THE
CPU
HOT
PLUG
REMOVE
FUNCTIONALITY
OF
THE
KERNEL
TO
SUPPORT
CORE
GATING
THREE
TYPES
OF
PERFORMANCE
STATISTICS
ARE
COLLECTED
USING
HARDWARE
COUNTERS
NUMBERS
OF
RETIRED
UOPS
CACHE
ACCESSES
AND
CACHE
MISSES
WE
USE
THE
PER
THREAD
MODE
OF
PFMON
TO
COLLECT
STATISTICS
FOR
EACH
BENCHMARK
AS
DISCUSSED
IN
SECTION
FOR
DTM
ACG
WHEN
ONE
CORE
ON
DUAL
CORE
PROCESSOR
IS
SHUT
DOWN
TWO
PROGRAMS
WILL
SHARE
THE
REMAINING
CORE
IN
A
ROUND
ROBIN
FASHION
THE
TIME
SLICE
FOR
THE
SHARING
IS
BY
DEFAULT
LINUX
KERNEL
WE
ALSO
PER
FORM
A
SENSITIVITY
ANALYSIS
BY
VARYING
THE
TIME
SLICE
AND
THE
RESULT
WILL
BE
SHOWN
IN
SECTION
WORKLOADS
WE
RUN
MULTIPROGRAMMING
WORKLOADS
CONSTRUCTED
FROM
THE
SPEC
BENCHMARK
THE
APPLICATIONS
ARE
COMPILED
WITH
INTEL
C
COMPILER
AND
INTEL
FORTRAN
COMPILER
WHEN
THE
FOUR
CORE
MACHINES
RUN
FOUR
COPIES
OF
A
SAME
APPLICATION
THIR
TEEN
APPLICATIONS
OF
SPEC
REACH
HIGHER
AMB
TEMPER
ATURE
THAN
OTHERS
WUPWISE
SWIM
MGRID
APPLU
VPR
GALGEL
ART
MCF
EQUAKE
LUCAS
GAP
AND
APSI
TWELVE
OUT
OF
THE
THIRTEEN
APPLICATIONS
COINCIDE
WITH
THOSE
SELECTED
BY
THE
SIMULATION
BASED
STUDY
THE
ONLY
EXCEPTION
IS
GAP
TO
SIMPLIFY
THE
COMPARISON
BETWEEN
THIS
WORK
AND
THE
PREVIOUS
STUDY
WE
DO
NOT
INCLUDE
GAP
IN
OUR
EXPERIMENTS
THEN
WE
CONSTRUCTED
EIGHT
MULTIPROGRAMMING
WORKLOADS
FROM
THESE
SELECTED
APPLICATIONS
AS
SHOWN
IN
TABLE
WE
RAN
ALL
WORKLOADS
TWICE
AND
THE
DIFFERENCES
IN
EXECUTION
TIME
ARE
NEGLIGIBLE
THE
RESULTS
OF
A
SINGLE
SET
OF
EXPERIMENTS
ARE
RE
PORTED
SOME
SPEC
APPLICATIONS
HAD
MORE
THAN
ONE
REFERENCE
INPUTS
HAVE
ALSO
RUN
PARTIAL
EXPERIMENTS
ON
WORKLOADS
CONSTRUCTED
FROM
SPEC
TABLE
WORKLOAD
MIXES
FOR
THOSE
APPLICATIONS
WE
RUN
ALL
INPUTS
AND
COUNT
THEM
AS
A
SINGLE
RUN
IN
ORDER
TO
OBSERVE
THE
LONG
TERM
MEMORY
TEMPERATURE
CHAR
ACTERISTICS
WE
RUN
THE
MULTIPROGRAMMING
WORKLOADS
AS
BATCH
JOBS
FOR
EACH
WORKLOAD
ITS
CORRESPONDING
BATCH
JOB
MIXES
TEN
RUNS
OF
EVERY
APPLICATION
CONTAINED
IN
THE
WORKLOAD
WHEN
ONE
PROGRAM
FINISHES
ITS
EXECUTION
AND
RELEASES
ITS
OCCUPIED
PROCESSOR
A
WAITING
PROGRAM
IS
ASSIGNED
TO
THE
PROCESSOR
IN
A
ROUND
ROBIN
WAY
IT
IS
WORTH
NOTING
THAT
AT
THE
END
OF
THE
BATCH
JOB
THERE
IS
SMALL
FRACTION
OF
PERIOD
THAT
LESS
THAN
FOUR
APPLICATIONS
RUNNING
SIMULTANEOUSLY
WE
OBSERVED
THAT
THE
FRACTION
WAS
LESS
THAN
OF
TOTAL
EXECUTION
TIME
ON
AVERAGE
WE
DO
NOT
STUDY
DTM
TS
THERMAL
SHUTDOWN
IN
THIS
WORK
FOR
THE
FOLLOWING
REASONS
FIRST
DTM
TS
IS
A
SPECIAL
CASE
OF
DTM
BW
SECOND
IT
ABRUPTLY
SHUTS
DOWN
THE
WHOLE
SYSTEM
AND
MAKES
SYSTEM
NOT
RUNNING
SMOOTHLY
RESULTS
AND
ANALYSIS
IN
THIS
SECTION
WE
FIRST
BRIEFLY
DESCRIBE
THE
DRAM
THERMAL
EMER
GENCY
OBSERVED
ON
THE
SERVERS
WE
THEN
PRESENT
THE
PERFORMANCE
RESULTS
OF
THE
FOUR
DTM
POLICIES
ANALYZE
THE
SOURCES
OF
PERFOR
MANCE
GAIN
DISCUSS
THE
RESULTS
OF
POWER
SAVING
AND
FINALLY
STUDY
THE
SENSITIVITY
OF
PARAMETER
SELECTIONS
EXPERIMENTAL
OBSERVATION
OF
DRAM
THERMAL
EMERGENCY
WE
PRESENT
OUR
OBSERVATION
OF
AMB
TEMPERATURE
CHANGES
ON
THE
TWO
SERVER
SYSTEMS
FIGURE
SHOWS
THE
AMB
TEMPERATURE
CHANG
ING
CURVES
ON
THE
THE
WHEN
IT
RUNS
HOMOGENEOUS
WORK
LOADS
DESCRIBED
AS
FOLLOWS
THE
MACHINE
HAS
OPEN
LOOP
BANDWIDTH
THROTTLING
ENABLED
BY
DEFAULT
IN
THE
CHIPSET
WE
DISABLE
THIS
FUNCTION
FOR
AMB
TEMPERATURE
BELOW
DURING
THE
CLOSE
TO
OVERHEATING
PERIODS
C
THE
FUNCTION
IS
ENABLED
TO
LIMIT
THE
MEMORY
BANDWIDTH
UNDER
FOR
SAFETY
CONCERN
WE
RUN
FOUR
COPIES
OF
EACH
PROGRAM
ON
THE
FOUR
CORES
ON
TWO
PROCESSORS
SIMULTANE
OUSLY
FOR
EACH
PROGRAM
WE
RUN
A
JOB
BATCH
WITH
TWENTY
COPIES
IN
TOTAL
TO
OBSERVE
THE
AMB
TEMPERATURE
CHANGES
AND
REPORT
THE
RESULTS
OF
THE
FIRST
FIVE
HUNDRED
SECONDS
THE
DATA
ARE
COLLECTED
EV
ERY
ONE
SECOND
THE
SERVER
HAS
FOUR
DIMMS
AND
THE
HIGHEST
AMB
TEMPERATURE
AMONG
THE
FOUR
DIMMS
IS
SHOWN
MOST
TIME
THE
THIRD
DIMM
HAS
THE
HIGHEST
AMB
TEMPERATURE
TEMPERATURE
CHANGES
OF
FIVE
SELECTED
PROGRAMS
ARE
REPORTED
IN
THE
FIGURE
AMONG
THEM
SWIM
AND
MGRID
ARE
MEMORY
INTENSIVE
AND
THE
OTHER
THREE
ARE
MODERATELY
MEMORY
INTENSIVE
INITIALLY
THE
MACHINE
IS
IDLE
FOR
A
SUFFICIENTLY
LONG
TIME
FOR
THE
AMB
TEMPERA
TURE
TO
STABILIZE
AT
ABOUT
C
AS
IT
SHOWS
WITH
SWIM
AND
MGRID
THE
TEMPERATURE
WILL
REACH
IN
ABOUT
SECONDS
THEN
IT
FLUC
TUATES
AROUND
C
BECAUSE
OF
THE
BANDWIDTH
THROTTLING
FOR
MA
CHINE
SAFETY
WE
HAVE
SIMILAR
OBSERVATIONS
FOR
OTHER
MEMORY
IN
TENSIVE
PROGRAMS
NOT
SHOWN
THE
OTHER
THREE
PROGRAMS
NAMELY
FIGURE
AMB
TEMPERATURE
CURVE
FOR
FIRST
SECONDS
OF
EXECUTION
GALGEL
APSI
AND
VPR
ARE
LESS
MEMORY
INTENSIVE
THEIR
TEMPERATURES
RISES
IN
SIMILAR
CURVES
AND
THEN
THE
TEMPERATURE
CHANGE
PATTERNS
STA
BILIZE
UNDER
C
FIGURE
SHOWS
THE
AVERAGE
AMB
TEMPERATURES
OF
THE
WHEN
IT
RUNS
THE
SAME
HOMOGENEOUS
WORKLOADS
UNLIKE
FIGURE
FIGURE
DOES
NOT
SHOW
MEMORY
OVERHEATING
INSTEAD
IT
SHOWS
HOW
OVERHEATING
WOULD
HAVE
HAPPENED
FOR
THOSE
WORKLOADS
IF
THE
AMBI
ENT
TEMPERATURE
IS
HIGH
ENOUGH
AND
NO
DTM
IS
USED
THE
IS
PUT
IN
A
ROOM
WITH
GOOD
AIR
CONDITIONING
IT
ALSO
HAS
A
DIFFERENT
COOLING
PACKAGE
THEREFORE
WE
ARE
ABLE
TO
RUN
MEMORY
INTENSIVE
WORKLOADS
WITHOUT
HAVING
THE
SYSTEM
TO
OVERHEATING
THE
AMBS
OR
THE
DRAM
DEVICES
ADDITIONALLY
THE
SERVER
CURRENTLY
INCLUDES
ONLY
TWO
DIMMS
IF
FOUR
DIMMS
WERE
USED
AS
WE
OBSERVED
ON
THE
THE
AMB
TEMPERATURE
WOULD
BE
SIGNIFICANTLY
HIGHER
THAN
REPORTED
ONLY
THE
AMB
TEMPERATURE
OF
THE
FIRST
DIMM
IS
SHOWN
BECAUSE
IT
ALWAYS
HAS
HIGHER
TEMPERATURE
THAN
THE
OTHER
ONE
THE
TEMPERATURE
SENSORS
HAVE
NOISES
WHICH
APPEAR
AS
HIGH
SPIKES
IN
TEMPERATURE
READINGS
WHICH
IS
VISIBLE
IN
FIGURE
THEREFORE
WE
EXCLUDE
SAMPLING
POINTS
WITH
THE
HIGHEST
TEMPERATURES
TO
RE
MOVE
THOSE
SPIKES
WE
HAVE
FOLLOWING
OBSERVATIONS
FIRST
AVERAGE
AMB
TEMPERA
TURE
VARIES
SIGNIFICANTLY
ACROSS
THOSE
HOMOGENEOUS
WORKLOADS
TEN
PROGRAMS
HAVE
AVERAGE
AMB
TEMPERATURE
HIGHER
THAN
C
WUP
WISE
SWIM
MGRID
APPLU
ART
MCF
EQUAKE
FACEREC
LUCAS
AND
AS
SHOWN
IN
THE
PREVIOUS
STUDY
AND
CONFIRMED
IN
OUR
EXPERIMENTS
USING
PERFORMANCE
COUNTERS
THESE
TEN
PROGRAMS
HAVE
HIGH
MISS
RATES
CONSEQUENTLY
THEY
HAVE
HIGHER
MEMORY
BAND
WIDTH
UTILIZATION
HIGHER
MEMORY
POWER
CONSUMPTION
AND
THERE
FORE
HIGHER
AMB
TEMPERATURES
THAN
THE
OTHER
WORKLOADS
FOUR
PRO
GRAMS
NAMELY
GALGEL
GAP
AND
APSI
HAVE
MODERATE
MEMORY
BANDWIDTH
UTILIZATION
AND
THEIR
AVERAGE
AMB
TEMPERATURES
RANGE
BETWEEN
C
AND
C
THE
OTHER
TWELVE
PROGRAMS
HAVE
SMALL
MEMORY
BANDWIDTH
UTILIZATION
AND
THEIR
AMB
TEMPERATURES
ARE
BE
LOW
C
SECOND
THERE
ARE
BIG
GAPS
BETWEEN
THE
AVERAGE
AND
THE
HIGHEST
AMB
TEMPERATURES
WE
HAVE
FOUND
THE
MAIN
REASON
IS
THAT
IT
TAKES
A
RELATIVELY
LONG
INITIAL
TIME
AROUND
TWO
HUNDRED
SECONDS
FOR
AMB
TO
REACH
A
STABLE
TEMPERATURE
ADDITIONALLY
FOR
SOME
WORKLOADS
THE
AMB
TEMPERATURES
KEEP
CHANGING
DUE
TO
THE
PRO
GRAM
PHASE
CHANGES
IN
THEIR
LIFESPAN
PERFORMANCE
COMPARISON
OF
DTM
POLICES
FIGURE
COMPARES
THE
PERFORMANCE
OF
THE
FOUR
DTM
POLICIES
AND
A
BASELINE
EXECUTION
WITH
NO
MEMORY
THERMAL
LIMIT
ON
THE
TWO
SERVERS
AS
DISCUSSED
IN
SECTION
ON
THE
WE
USE
AN
AR
TIFICIAL
THERMAL
THRESHOLD
OF
C
TO
REVEAL
THE
IMPACT
OF
MEMORY
THERMAL
LIMIT
THE
NO
LIMIT
EXPERIMENTS
ARE
DONE
WITHOUT
ENFORC
FIGURE
AMB
TEMPERATURE
WHEN
MEMORY
IS
DRIVEN
BY
HOMOGENEOUS
WORKLOADS
ON
THE
WITHOUT
DTM
CONTROL
A
DELL
B
INTEL
FIGURE
NORMALIZED
RUNNING
TIME
ING
THAT
ARTIFICIAL
TDP
ON
THE
WE
ARE
ABLE
TO
CONTROL
THE
AMBIENT
TEMPERATURE
SO
WE
RUN
THE
NO
LIMIT
EXPERIMENTS
WITH
AN
AMBIENT
TEMPERATURE
OF
C
AND
RUN
THE
OTHER
EXPERIMENTS
WITH
AN
AMBIENT
TEMPERATURE
OF
C
WE
DISABLE
THE
BUILT
IN
BANDWIDTH
THROTTLING
FEATURE
OF
THE
CHIPSET
IN
THE
NO
LIMIT
EXPERIMENTS
WE
HAVE
THE
FOLLOWING
OBSERVATIONS
FOR
WORKLOADS
FROM
SPEC
FIRST
OF
ALL
THE
RESULTS
CONFIRM
THAT
THE
USE
OF
SIM
PLE
BANDWIDTH
THROTTLING
DTM
BW
MAY
SEVERELY
DOWNGRADE
THE
SYSTEM
PERFORMANCE
ON
AVERAGE
THE
PERFORMANCE
DEGRADATION
IS
ON
THE
AND
ON
THE
OUR
DETAILED
STATISTICS
SHOW
THAT
THERE
IS
A
STRONG
CORRELATION
BETWEEN
THE
MEM
ORY
BANDWIDTH
UTILIZATION
AND
THE
PERFORMANCE
DEGRADATION
FOR
EXAMPLE
ALL
WORKLOADS
EXCEPT
W
AND
W
HAVE
LARGER
THAN
SLOWDOWN
WITH
DTM
BW
WHILE
THE
SLOWDOWNS
FOR
W
AND
W
ARE
AND
RESPECTIVELY
ON
THE
THE
PERFOR
MANCE
COUNTER
DATA
SHOW
THAT
W
HAS
CACHE
MISSES
PER
MICROSECOND
WHICH
IS
THE
LOWEST
AMONG
THE
EIGHT
WORKLOADS
THIS
MEANS
IT
IS
LESS
MEMORY
INTENSIVE
THAN
OTHERS
SECOND
THE
RESULTS
ALSO
CONFIRM
THAT
DTM
ACG
MAY
SIGNIFI
CANTLY
IMPROVE
PERFORMANCE
OVER
DTM
BW
ON
AVERAGE
DTM
ACG
IMPROVES
THE
PERFORMANCE
OF
WORKLOADS
BY
ON
THE
AND
ON
THE
THE
MAXIMUM
IM
PROVEMENT
IS
AND
RESPECTIVELY
IN
COMPARISON
THE
PREVIOUS
SIMULATION
BASED
STUDY
REPORTS
AN
AVERAGE
IMPROVE
MENT
OF
USING
THE
SAME
WORKLOADS
THE
MAIN
SOURCE
OF
IM
PROVEMENT
COMES
FROM
THE
REDUCTION
ON
CACHE
MISSES
WHICH
WILL
BE
DETAILED
IN
SECTION
AS
FOR
THE
DIFFERENCE
IN
THE
RESULTS
FROM
THE
TWO
SERVERS
SEVERAL
FACTORS
MAY
CONTRIBUTE
TO
IT
INCLUDING
THE
DIFFERENCES
IN
COOLING
PACKAGE
MEMORY
BANDWIDTH
AMBIENT
TEMPERATURE
AND
THE
LAYOUT
OF
THE
PROCESSORS
AND
DIMMS
ON
MOTH
ERBOARD
WE
ALSO
OBSERVE
PERFORMANCE
DEGRADATION
OF
DTM
ACG
OVER
DTM
BW
ON
WORKLOAD
W
WHICH
IS
ON
THE
AND
ON
THE
RESPECTIVELY
THIS
SCENARIO
WAS
NOT
REPORTED
IN
THE
PREVIOUS
STUDY
AS
TO
BE
SHOWN
IN
FIGURE
DTM
ACG
ACTUALLY
REDUCES
THE
CACHE
MISSES
OF
W
BY
ON
THE
AND
ON
THE
WE
BELIEVE
THAT
FOR
THIS
WORK
LOAD
THE
DTM
ACG
POLICY
MAY
STOP
PROCESSOR
CORES
TOO
PROAC
TIVELY
THIS
IS
NOT
A
FUNDAMENTAL
PROBLEM
OF
THE
POLICY
BUT
IN
DICATES
THAT
THE
POLICY
MAY
BE
FURTHER
REFINED
FOR
CERTAIN
TYPES
OF
WORKLOADS
REGARDING
DTM
CDVFS
WE
HAVE
SURPRISING
FINDINGS
THAT
ARE
VERY
DIFFERENT
FROM
THE
PREVIOUS
STUDY
ON
AVERAGE
DTM
CDVFS
MAY
IMPROVE
PERFORMANCE
OVER
DTM
BW
BY
ON
THE
AND
ON
THE
BY
CONTRAST
THE
PREVIOUS
STUDY
RE
PORTS
ONLY
AVERAGE
IMPROVEMENT
IT
IS
ALSO
REMARKABLE
THAT
THE
SCHEME
IMPROVES
THE
PERFORMANCE
OF
EVERY
PROGRAM
ON
RANGING
FROM
TO
ON
THE
MAXIMUM
IMPROVE
MENT
IS
AND
ONLY
W
HAS
SMALL
PERFORMANCE
DEGRADATION
OF
THE
MAIN
REASON
BEHIND
THE
PERFORMANCE
IMPROVEMENTS
AS
TO
BE
DISCUSSED
IN
DETAILS
IN
SECTION
IS
RELATED
TO
THE
THERMAL
INTERACTION
BETWEEN
THE
PROCESSORS
AND
THE
MEMORY
THE
PREVIOUS
STUDY
DID
NOT
CONSIDER
THE
HEAT
DISSIPATION
FROM
THE
PROCESSOR
TO
THE
MEMORY
AS
THE
RESULTS
INDICATE
THAT
FACTOR
SHOULD
BE
SIGNIFICANT
IN
THE
DRAM
THERMAL
MODELING
AND
CANNOT
BE
IGNORED
IN
FACT
THE
PERFORMANCE
IMPROVEMENT
IS
LARGER
ON
THE
THAN
ON
THE
BECAUSE
ON
ITS
MOTHERBOARD
THE
PROCESSORS
ARE
PHYSICALLY
CLOSER
TO
THE
DIMMS
WE
WILL
PRESENT
MORE
EXPERIMENTAL
RESULTS
FROM
THE
TO
SUPPORT
THIS
FINDING
WE
HAVE
ALSO
RUN
TWO
WORKLOADS
FROM
SPEC
ON
W
WITH
APPLICATIONS
MILC
SOPLEX
AND
GEMSFDTD
AND
W
WITH
LIBQUANTUM
LBM
OMNETPP
AND
WRF
THE
FINDINGS
FOR
WORKLOADS
FROM
STILL
HOLD
FOR
THEM
DTM
BW
DEGRADES
THE
PERFORMANCE
BY
AND
FOR
W
AND
W
WHEN
COMPARED
WITH
NO
LIMIT
RESPECTIVELY
DTM
ACG
IMPROVES
PERFOR
MANCE
BY
AND
WHEN
COMPARED
WITH
DTM
BW
RESPEC
TIVELY
DTM
CDVFS
HAS
BETTER
PERFORMANCE
FOR
BOTH
WORKLOADS
IMPROVING
PERFORMANCE
BY
AND
OVER
DTM
BW
ON
THE
TWO
SERVERS
RESPECTIVELY
THE
PERFORMANCE
OF
DTM
COMB
IS
VERY
CLOSE
TO
THAT
OF
DTM
ACG
ON
AVERAGE
ON
BOTH
MACHINES
ON
AVERAGE
FOR
SPEC
WORKLOADS
THE
PERFORMANCE
OF
DTM
COMB
IS
DEGRADED
BY
ON
AND
IMPROVED
BY
ON
COMPARED
WITH
DTM
ACG
THE
DTM
COMB
MAY
IMPROVE
PERFORMANCE
UP
TO
FOR
W
FROM
SPEC
IT
IS
REMARKABLE
THAT
DTM
COMB
CAN
IMPROVE
PERFORMANCE
FOR
W
W
AND
W
ON
WHEN
COMPARED
WITH
NO
LIMIT
THIS
IS
POSSIBLE
BECAUSE
WE
OBSERVE
THAT
FOR
SOME
PROGRAMS
THE
CACHE
MISS
RATE
DECREASES
SHARPLY
WHEN
RUNNING
ALONE
AS
SHOWN
LATER
IN
SECTION
ANALYSIS
OF
PERFORMANCE
IMPROVEMENTS
BY
DIFFERENT
DTM
POLICIES
IN
THIS
SECTION
WE
ANALYZE
THE
SOURCES
OF
PERFORMANCE
IMPROVE
MENTS
BY
DTM
ACG
DTM
CDVFS
AND
DTM
COMB
WHEN
COM
PARED
WITH
DTM
BW
REDUCTION
OF
CACHE
MISSES
IT
HAS
BEEN
REPORTED
IN
THE
PREVIOUS
STUDY
THAT
THE
IMPROVEMENT
BY
DTM
ACG
IS
MOSTLY
FROM
THE
REDUCTION
OF
MEMORY
TRAFFIC
WHICH
IS
FROM
THE
REDUCTION
OF
CACHE
MISSES
WHEN
THE
SHARED
CACHE
IS
USED
BY
FEWER
PROGRAMS
CACHE
CONTENTION
IS
REDUCED
AND
THUS
THERE
WILL
BE
FEWER
CACHE
MISSES
THE
PREVIOUS
STUDY
COLLECTED
MEMORY
TRAFFIC
DATA
TO
DEMONSTRATE
THE
CORRELATION
ON
OUR
PLATFORMS
WE
CAN
ONLY
COL
LECT
THE
NUMBER
OF
CACHE
MISSES
THE
TOTAL
MEMORY
TRAFFIC
CON
SISTS
OF
CACHE
REFILLS
FROM
ON
DEMAND
CACHE
MISSES
CACHE
WRITE
BACKS
MEMORY
PREFETCHES
SPECULATIVE
MEMORY
ACCESSES
AND
OTHER
SOURCES
INCLUDING
CACHE
COHERENCE
TRAFFIC
NEVERTHELESS
CACHE
RE
FILLS
ARE
THE
MAJORITY
PART
OF
MEMORY
TRAFFIC
THEREFORE
THE
NUMBER
OF
CACHE
MISSES
IS
A
GOOD
INDICATION
OF
MEMORY
TRAFFIC
FIGURE
SHOWS
THE
NORMALIZED
NUMBER
OF
CACHE
MISSES
ON
BOTH
MACHINES
WE
HAVE
SEVERAL
OBSERVATIONS
FROM
THE
DATA
FIRST
THE
NUMBER
OF
CACHE
MISSES
CHANGES
VERY
SLIGHTLY
BY
USING
DTM
BW
WHEN
COMPARED
WITH
NO
LIMIT
THIS
IS
EXPECTED
BE
CAUSE
THE
NUMBER
OF
ON
DEMAND
CACHE
MISSES
SHOULD
HAVE
VIR
TUALLY
NO
CHANGE
WHEN
MEMORY
BANDWIDTH
IS
THROTTLED
SECOND
THE
TOTAL
NUMBER
OF
CACHE
MISSES
DOES
DECREASE
SIGNIFICANTLY
BY
DTM
ACG
COMPARED
WITH
THAT
OF
DTM
BW
THE
REDUCTION
IS
UP
TO
AND
ON
THE
AND
THE
RESPEC
TIVELY
THE
AVERAGE
REDUCTION
ARE
AND
RESPECTIVELY
THE
RESULT
CONFIRMS
THE
FINDING
OF
THE
PREVIOUS
STUDY
THAT
DTM
ACG
REDUCES
CACHE
MISSES
SIGNIFICANTLY
ON
THE
OTHER
HAND
DTM
CDVFS
DOES
NOT
CAUSE
ANY
VISIBLE
CHANGES
OF
THE
TOTAL
NUM
BER
OF
CACHE
MISSES
WHILE
THE
PREVIOUS
STUDY
REPORTED
MEMORY
TRAFFIC
MAY
BE
REDUCED
DUE
TO
THE
REDUCTION
OF
SPECULATIVE
MEM
ORY
ACCESSES
THE
DIFFERENCE
IS
LIKELY
RELATED
TO
DIFFERENCES
IN
THE
PROCESSOR
MODELS
PARTICULARLY
HOW
MANY
OUTSTANDING
MEMORY
AC
CESSES
ARE
ALLOWED
AND
WHETHER
A
SPECULATIVE
MEMORY
INSTRUCTION
IS
ALLOWED
TO
TRIGGER
AN
ACCESS
TO
THE
MAIN
MEMORY
THE
DTM
COMB
HAS
VERY
SIMILAR
CACHE
MISS
REDUCTION
AS
DTM
ACG
THE
AVERAGE
REDUCTIONS
ARE
AND
ON
THE
AND
THE
RESPECTIVELY
REDUCTION
OF
MEMORY
AMBIENT
TEMPERATURE
BY
DTM
CDVFS
AS
DISCUSSED
EARLIER
THE
PERFORMANCE
OF
DTM
CDVFS
IS
COMPA
RABLE
TO
THAT
OF
DTM
ACG
IN
FACT
IT
IS
VISIBLY
BETTER
THAN
DTM
FIGURE
MEASURED
MEMORY
INLET
TEMPERATURE
ACG
ON
THE
THIS
IS
A
SURPRISE
FINDING
THE
PREVIOUS
STUDY
REPORTS
THAT
DTM
CDVFS
HAS
ONLY
SLIGHT
PERFORMANCE
AD
VANTAGE
OVER
DTM
BW
AND
THE
MAIN
BENEFIT
OF
DTM
CDVFS
WAS
IMPROVED
SYSTEM
POWER
EFFICIENCY
WE
SPECULATED
THAT
THE
PROCESSOR
HEAT
GENERATION
HAS
AN
IMPACT
ON
THE
MEMORY
DIMMS
WHICH
WAS
IGNORED
IN
THE
THERMAL
MODELING
OF
THE
PREVIOUS
STUDY
IF
THE
PROCESSOR
IS
PHYSICALLY
CLOSE
ENOUGH
TO
THE
DIMMS
THEN
THE
HEAT
DISSIPATION
FROM
THE
PROCESSOR
MAY
FURTHER
INCREASE
THE
DIMM
AMBIENT
TEMPERATURE
CONSEQUENTLY
THE
DIMMS
MAY
OVER
HEAT
MORE
FREQUENTLY
THAN
PREDICTED
BY
THE
THERMAL
MODEL
IN
THE
PREVIOUS
STUDY
SINCE
DTM
CDVFS
IMPROVES
THE
PROCESSOR
POWER
EFFICIENCY
IT
MAY
REDUCE
THE
HEAT
GENERATION
FROM
THE
PROCESSOR
AND
THEREFORE
ALLEVIATE
THE
PROBLEM
WHICH
WILL
IMPROVE
MEMORY
BAND
WIDTH
UTILIZATION
IF
THAT
IS
A
SIGNIFICANT
FACTOR
THEN
THE
OBSERVED
PERFORMANCE
IMPROVEMENT
CAN
BE
EXPLAINED
TO
CONFIRM
THE
ABOVE
THEORY
WE
HAVE
LOOKED
INTO
THE
INSIDE
OF
EACH
MACHINE
ON
BOTH
MACHINES
THE
PROCESSORS
AND
THE
DIMMS
SHARE
THE
SAME
SET
OF
COOLING
FANS
AND
THE
AIR
FLOW
FIRST
PASSES
THE
PROCESSORS
THEN
THE
DIMMS
THE
PROCESSOR
AND
DIMMS
ARE
SLIGHTLY
MISALIGNED
ALONG
THE
COOLING
AIR
FLOW
ON
THE
ON
THE
ONE
OF
THE
TWO
PROCESSORS
IS
ALIGNED
WITH
THE
DIMMS
ALONG
THE
COOLING
AIR
FLOW
ADDITIONALLY
THE
DISTANCE
BETWEEN
THE
PROCESSORS
AND
THE
DIMMS
IS
AS
CLOSE
AS
ABOUT
WE
COLLECT
THE
TEMPERATURE
READINGS
THROUGH
A
SENSOR
PUT
ON
THE
AIR
PATH
BETWEEN
THE
PROCESSORS
AND
THE
DIMMS
INSIDE
THE
SUCH
A
SENSOR
IS
NOT
AVAILABLE
ON
THE
DATA
SHOW
A
STRONG
CORRELATION
BETWEEN
THE
MEMORY
INLET
TEMPERATURE
DIFFERENCE
AND
THE
PERFORMANCE
IMPROVEMENT
OF
DTM
CDVFS
OVER
DTM
BW
FIGURE
COMPARES
THE
AVERAGE
TEMPERATURE
OF
THE
FOUR
DTM
SCHEMES
THE
SYSTEM
AMBIENT
TEMPERATURE
OF
THE
IS
SET
TO
C
AS
THE
FIGURE
SHOWS
THE
COOLING
AIR
IS
HEATED
UP
BY
ABOUT
C
WHEN
IT
REACHES
THE
MEMORY
THE
PROCESSOR
EXHAUST
MEMORY
INLET
TEMPERATURE
IS
VISIBLY
LOWER
WITH
DTM
CDVFS
OR
DTM
COMB
THAN
WITH
DTM
BW
OR
DTM
ACG
FOR
WORKLOADS
W
TO
W
AS
TO
BE
DISCUSSED
IN
SECTION
DTM
CDVFS
AND
DTM
COMB
RE
DUCE
PROCESSOR
POWER
CONSUMPTION
BUT
DTM
ACG
DOES
NOT
WHEN
COMPARED
WITH
DTM
BW
WORKLOADS
W
AND
W
ARE
EXCEPTIONS
FOR
W
THE
TEMPERATURE
IS
SLIGHTLY
HIGHER
WITH
DTM
CDVFS
THAN
WITH
THE
OTHER
SCHEMES
AND
FOR
W
IT
IS
BETWEEN
DTM
BW
AND
DTM
ACG
ON
AVERAGE
THE
TEMPERATURE
IS
C
C
C
AND
WITH
DTM
BW
DTM
ACG
DTM
CDVFS
AND
DTM
COMB
RESPECTIVELY
THE
MAXIMUM
DIFFERENCE
IS
C
WE
HIGH
LIGHT
THAT
WE
CAREFULLY
CALIBRATED
THOSE
SENSORS
AND
THAT
RANDOM
SEN
SOR
NOISES
DO
NOT
AFFECT
THE
ACCURACY
OF
AVERAGE
TEMPERATURE
THOSE
DIFFERENCES
SEEM
TO
BE
SMALL
HOWEVER
THE
RANGE
OF
WORKING
TEM
PERATURES
OF
MEMORY
INTENSIVE
WORKLOADS
IS
LESS
THAN
C
ON
THAT
A
DELL
B
INTEL
FIGURE
NORMALIZED
NUMBERS
OF
CACHE
MISSES
FIGURE
CPU
POWER
CONSUMPTION
SERVER
AS
SHOWN
IN
FIGURE
THEREFORE
A
ONE
DEGREE
DIFFERENCE
CAN
HAVE
A
NOTICEABLE
IMPACT
ON
PERFORMANCE
THE
RESULT
STRONGLY
SUGGESTS
THAT
THE
LAYOUT
DESIGN
OF
SERVER
INSIDE
SHOULD
GIVE
MORE
ATTENTION
TO
THE
MEMORY
SUBSYSTEM
COMPARISON
OF
POWER
AND
ENERGY
CONSUMPTION
ON
THE
WE
ARE
ABLE
TO
MEASURE
THE
POWER
CONSUMP
TION
OF
INDIVIDUAL
SYSTEM
COMPONENTS
INCLUDING
THE
PROCESSORS
DIMMS
SYSTEM
FANS
AND
OTHER
COMPONENTS
POWER
CONSUMPTION
OF
PROCESSORS
AND
DIMMS
WE
ARE
ONLY
INTERESTED
IN
THE
POWER
CONSUMPTION
OF
THE
PROCESSORS
AND
DIMMS
BECAUSE
FOR
OUR
WORKLOADS
THE
POWER
CONSUMPTION
OF
THE
OTHER
COMPONENTS
IS
ALMOST
CONSTANT
THE
PROCESSORS
CONSUME
SLIGHTLY
MORE
THAN
A
THIRD
OF
THE
SYSTEM
POWER
AND
THE
DIMMS
CONSUME
SLIGHTLY
LESS
THAN
A
THIRD
IN
OUR
EXPERIMENTS
WE
ALSO
FOUND
THAT
THE
POWER
CONSUMPTION
OF
THE
DIMMS
IS
VERY
CLOSE
FOR
ALL
WORKLOADS
EXCEPT
WORKLOAD
W
WHICH
IS
LESS
MEMORY
INTENSIVE
THAN
THE
OTH
ERS
PART
OF
THE
REASON
IS
THAT
STATIC
POWER
IS
A
LARGE
COMPONENT
OF
FB
DIMM
POWER
THEREFORE
WE
ONLY
COMPARE
THE
PROCESSOR
POWER
CONSUMPTION
FIGURE
SHOWS
THE
AVERAGE
POWER
CONSUMPTION
WITH
DIFFERENT
DTM
POLICIES
THE
DATA
ARE
NORMALIZED
TO
THOSE
OF
DTM
BW
AS
EXPECTED
DTM
CDVFS
AND
DTM
COMB
CONSUME
LESS
PROCES
SOR
POWER
THAN
THE
OTHER
TWO
POLICIES
ON
AVERAGE
THE
PROCESSOR
POWER
CONSUMPTION
OF
DTM
CDVFS
AND
DTM
COMB
IS
AND
LOWER
THAN
THAT
OF
DTM
BW
RESPECTIVELY
THERE
IS
A
FIGURE
NORMALIZED
ENERGY
CONSUMPTION
OF
DTM
POLICIES
VERY
SMALL
DIFFERENCE
BETWEEN
THE
POWER
CONSUMPTION
BY
DTM
BW
AND
DTM
ACG
THIS
IS
MAINLY
DUE
TO
THE
FACT
THAT
THE
LAT
EST
GENERATION
PROCESSORS
ARE
PACKED
WITH
A
NUMBER
OF
ENERGY
EFFI
CIENT
FEATURES
THEY
APPLY
EXTENSIVE
CLOCK
GATING
TO
IDLE
FUNCTIONAL
BLOCKS
WHEN
PROCESSORS
ARE
STALLED
BY
THE
LONG
LATENCY
MEMORY
AC
CESSES
THUS
FOR
MEMORY
INTENSIVE
WORKLOADS
WITH
FREQUENT
LAST
LEVEL
CACHE
MISSES
MOST
FUNCTIONAL
COMPONENTS
IN
THE
PROCESSOR
CORE
HAVE
ALREADY
BEEN
CLOCK
GATED
YIELDING
LITTLE
ADDITIONAL
BENEFIT
FROM
GATING
THE
ENTIRE
CORE
ENERGY
CONSUMPTION
FIGURE
SHOWS
THE
TOTAL
ENERGY
CON
SUMPTION
OF
PROCESSORS
AND
MEMORY
ALL
VALUES
ARE
NORMALIZED
TO
THOSE
OF
DTM
BW
ON
AVERAGE
COMPARED
WITH
DTM
BW
DTM
ACG
DTM
CDVFS
AND
DTM
COMB
CAN
SAVE
ENERGY
BY
AND
RESPECTIVELY
THE
ENERGY
SAVING
OF
DTM
ACG
COMES
FROM
THE
REDUCTION
OF
RUNNING
TIME
BECAUSE
ITS
POWER
CON
SUMPTION
IS
VERY
CLOSE
TO
THAT
OF
DTM
BW
THE
ENERGY
SAVINGS
FOR
DTM
CDVFS
AND
DTM
COMB
COME
FROM
BOTH
POWER
SAVING
AND
REDUCTION
OF
RUNNING
TIME
SENSITIVITY
ANALYSIS
OF
DTM
PARAMETERS
AMBIENT
TEMPERATURE
THE
PERFORMANCE
OF
DTM
POLICIES
SHOWN
IN
SECTION
ON
THE
IS
FROM
THE
EXPERIMENTS
WITH
A
SYSTEM
AMBIENT
TEMPERATURE
OF
C
AND
AN
AMB
TDP
OF
C
WE
ALSO
HAVE
RUN
EXPERIMENTS
ON
WITH
A
LOWER
SYSTEM
AMBIENT
TEMPERATURE
OF
C
AND
WITH
AN
ARTIFICIAL
AMB
THERMAL
THRESHOLD
OF
C
THIS
SETTING
IS
THE
SAME
AS
THAT
USED
ON
THE
AND
HAS
THE
SAME
GAP
C
BETWEEN
THE
AMBIENT
FIGURE
NORMALIZED
RUNNING
TIME
ON
INTEL
AT
A
ROOM
SYSTEM
AMBIENT
TEMPERATURE
C
TEMPERATURE
AND
THE
TDP
TEMPERATURE
AS
THE
FIRST
SET
OF
EXPERIMENTS
ON
THE
THE
EXPERIMENT
HAS
TWO
PURPOSES
FIRST
BY
KEEPING
THE
TEMPERATURE
GAP
THE
SAME
WHILE
CHANGING
THE
AMBIENT
TEMPERATURE
THE
NEW
RESULT
WILL
HELP
UNDERSTAND
HOW
THE
AMBIENT
TEMPERATURE
AFFECTS
PERFORMANCE
SECOND
BECAUSE
THE
PERFORMANCE
IMPROVEMENTS
ARE
DIFFERENT
ON
THE
TWO
SERVERS
THE
NEW
RESULT
MAY
REVEAL
WHETHER
THE
DIFFERENCE
IS
RELATED
TO
THEIR
DIFFERENCES
IN
AM
BIENT
TEMPERATURES
FIGURE
COMPARES
THE
PERFORMANCE
OF
FOUR
POLICIES
ON
IN
THE
NEW
SETTING
IT
INDICATES
THAT
THE
PERFORMANCE
IS
VERY
SIMILAR
TO
THAT
ON
THE
SAME
MACHINE
WITH
HIGHER
SYSTEM
AMBIENT
TEMPER
ATURE
OF
C
ON
AVERAGE
DTM
BW
DEGRADES
PERFORMANCE
BY
OVER
NO
LIMIT
THE
DEGRADATION
IS
WITH
THE
HIGHER
AMBIENT
TEMPERATURE
ON
AVERAGE
DTM
ACG
AND
DTM
CDVFS
IMPROVE
PERFORMANCE
BY
AND
OVER
DTM
BW
RESPEC
TIVELY
THE
IMPROVEMENTS
ARE
AND
WITH
AN
AMBIENT
TEMPERATURE
OF
C
FIGURE
B
RESPECTIVELY
THE
PERFORMANCE
COMPARISON
REGARDING
INDIVIDUAL
WORKLOAD
ARE
ALSO
SIMILAR
THE
SIMILARITY
INDICATES
THAT
THE
PERFORMANCE
OF
DTM
SCHEMES
IS
STRONGLY
CORRELATED
TO
THE
GAP
BETWEEN
THE
AMBIENT
TEMPERATURE
AND
AMB
TDP
PROCESSOR
FREQUENCY
IN
PREVIOUS
EXPERIMENTS
WE
RUN
PRO
CESSOR
CORES
AT
FULL
SPEED
GHZ
FOR
DTM
BW
AND
DTM
ACG
WE
ALSO
WANT
TO
SEE
WHAT
HAPPENS
IF
A
LOWER
PROCESSOR
SPEED
GHZ
IS
USED
FIGURE
COMPARES
THE
PERFORMANCE
WITH
TWO
PRO
CESSOR
SPEEDS
FOR
DTM
BW
AND
DTM
ACG
ON
THE
FIRST
ON
AVERAGE
THE
PERFORMANCE
WITH
THE
LOWER
PROCESSOR
SPEED
IS
DEGRADED
BY
AND
COMPARED
WITH
THAT
WITH
THE
HIGHER
SPEED
FOR
DTM
BW
AND
DTM
ACG
RESPECTIVELY
WE
FIND
THAT
THE
LESS
MEMORY
INTENSIVE
WORKLOAD
W
HAS
LARGER
PERFORMANCE
DEGRADATION
THAN
THE
OTHERS
THIS
IS
EXPECTED
SINCE
THE
PERFOR
MANCE
OF
COMPUTE
INTENSIVE
WORKLOADS
IS
MORE
SENSITIVE
TO
PROCES
SOR
FREQUENCY
ISCI
ET
AL
ALSO
PRESENT
THAT
THE
PERFORMANCE
DEGRADA
TION
IS
SMALL
FOR
MEMORY
INTENSIVE
WORKLOADS
WITH
LOW
FREQUENCY
MODE
IF
W
IS
EXCLUDED
THE
PERFORMANCE
DEGRADATION
IS
ONLY
AND
FOR
DTM
BW
AND
DTM
ACG
RESPECTIVELY
SECOND
DTM
ACG
IMPROVES
PERFORMANCE
SIMILARLY
UNDER
BOTH
MODES
ON
AVERAGE
THE
PERFORMANCE
IMPROVEMENT
IS
WITH
THE
LOWER
PROCESSOR
SPEED
AND
IS
WITH
THE
HIGHER
SPEED
RE
SPECTIVELY
WHEN
W
IS
EXCLUDED
THE
AVERAGE
PERFORMANCE
IM
PROVEMENT
IS
AND
RESPECTIVELY
DTM
TDP
AND
THERMAL
EMERGENCY
LEVELS
FIGURE
SHOWS
THE
NORMALIZED
RUNNING
TIME
AVERAGED
ON
ALL
WORKLOADS
ON
WHEN
THE
THERMAL
DESIGN
POINT
TDP
OF
AMB
CHANGES
THE
THERMAL
EMERGENCY
LEVELS
ALSO
CHANGE
WITH
THE
AMB
TDPS
FIGURE
COMPARISON
OF
PERFORMANCE
BETWEEN
DTM
ACG
AND
DTM
BW
UNDER
TWO
DIFFERENT
PROCESSOR
FREQUENCIES
ON
INTEL
FIGURE
NORMALIZED
RUNNING
TIME
AVERAGED
FOR
ALL
WORKLOADS
ON
WITH
DIFFERENT
AMB
TDPS
FOLLOWING
THE
RATIONALES
DISCUSSED
IN
SECTION
THE
PERFORMANCE
OF
THREE
AMB
TDPS
IS
SHOWN
C
C
AND
C
AS
EX
PECTED
THE
PERFORMANCE
LOSS
IS
REDUCED
WITH
HIGHER
TDPS
COM
PARED
WITH
THAT
OF
NO
LIMIT
THE
PERFORMANCE
OF
DTM
BW
IS
DE
GRADED
BY
AND
WITH
AMB
TDPS
OF
C
C
AND
C
RESPECTIVELY
THE
PERFORMANCE
IMPROVEMENT
BY
THREE
POLICIES
OVER
DTM
BW
IS
SIMILAR
UNDER
DIFFERENT
AMB
TDPS
THE
PERFORMANCE
IMPROVEMENT
BY
DTM
ACG
IS
AND
RESPECTIVELY
THEY
ARE
AND
BY
DTM
CDVFS
AND
AND
BY
DTM
COMB
RESPEC
TIVELY
THIS
RESULT
INDICATES
THAT
THE
THREE
POLICIES
MAY
WORK
SIMI
LARLY
IN
SYSTEMS
WITH
DIFFERENT
THERMAL
CONSTRAINTS
SWITCHING
FREQUENCY
IN
LINUX
SCHEDULING
FOR
DTM
ACG
IN
DTM
ACG
TWO
PROGRAMS
MAY
SHARE
A
PROCESSOR
CORE
WHEN
AN
OTHER
CORE
IS
DISABLED
THE
TIME
QUANTUM
USED
IN
PROCESS
SCHEDUL
ING
IS
SET
TO
IN
THE
KERNEL
BY
DEFAULT
FIGURE
COMPARES
THE
NORMALIZED
RUNNING
TIME
AND
NUMBER
OF
CACHE
MISSES
AVERAGED
FOR
ALL
WORKLOADS
ON
WITH
DIFFERENT
TIME
QUANTUM
SETTINGS
THE
RUNNING
TIME
AND
NUMBER
OF
CACHE
MISSES
ARE
NORMALIZED
TO
THOSE
WITH
DEFAULT
TIME
QUANTUM
FOR
EACH
WORKLOAD
THE
RE
SULTS
SHOW
THAT
THE
AVERAGE
NORMALIZED
RUNNING
TIME
DOES
NOT
HAVE
VISIBLE
CHANGES
WHEN
THE
BASE
TIME
QUANTUM
IS
LONGER
THAN
WHEN
IT
IS
SET
TO
A
VALUE
SHORTER
THAN
BOTH
RUNNING
TIME
AND
NUMBER
CACHE
MISSES
INCREASE
STEADILY
THE
AVERAGE
RUNNING
TIME
IS
INCREASED
BY
AND
WHEN
THE
BASE
TIME
QUANTUM
IS
SET
TO
AND
RESPECTIVELY
WE
FIND
THAT
THE
MAJOR
REASON
FIGURE
NORMALIZED
RUNNING
TIME
AND
NUMBER
OF
CACHE
MISSES
AVERAGED
FOR
ALL
WORKLOADS
ON
WITH
DIFFERENT
SWITCHING
FREQUEN
CIES
FOR
THE
PERFORMANCE
DEGRADATION
IS
THE
INCREASE
OF
CACHE
MISSES
THE
AVERAGE
NUMBER
OF
CACHE
MISSES
IS
INCREASED
BY
AND
RESPECTIVELY
THIS
INDICATES
THAT
TO
AVOID
CACHE
THRASHING
WITH
DTM
ACG
THE
DEFAULT
TIME
SLICE
CANNOT
BE
SHORTER
THAN
FOR
THE
PROCESSORS
WITH
CACHE
USED
IN
OUR
EXPERIMENTS
CONCLUSION
WE
HAVE
PERFORMED
THE
FIRST
STUDY
OF
SOFTWARE
DYNAMIC
THERMAL
MANAGEMENT
DTM
OF
MEMORY
SUBSYSTEMS
ON
MULTICORE
SYSTEMS
RUNNING
LINUX
OS
IT
HAS
VALIDATED
THE
EFFECTIVENESS
OF
MEMORY
DTM
METHODS
IN
REAL
SYSTEMS
WITH
A
NEW
FINDING
ON
THE
THERMAL
INTERACTION
BETWEEN
THE
PROCESSOR
AND
MEMORY
IN
REAL
MACHINES
FUTURE
WORK
INCLUDES
THE
CORRECTION
OF
THE
PREVIOUS
MEMORY
THER
MAL
MODEL
MORE
EVALUATION
USING
COMMERCIAL
WORKLOADS
AND
VARYING
THERMAL
CONSTRAINTS
AND
THE
DESIGN
OF
NEW
MEMORY
DTM
POLICIES
USING
CONTROL
METHODS
AND
ADDITIONAL
INPUTS
A
LARGE
PORTION
OF
THE
POWER
BUDGET
IN
SERVER
ENVIRON
MENTS
GOES
INTO
THE
I
O
SUBSYSTEM
THE
DISK
ARRAY
IN
PAR
TICULAR
TRADITIONAL
APPROACHES
TO
DISK
POWER
MANAGEMENT
INVOLVE
COMPLETELY
STOPPING
THE
DISK
ROTATION
WHICH
CAN
TAKE
A
CONSIDERABLE
AMOUNT
OF
TIME
MAKING
THEM
LESS
USE
FUL
IN
CASES
WHERE
IDLE
TIMES
BETWEEN
DISK
REQUESTS
MAY
NOT
BE
LONG
ENOUGH
TO
OUTWEIGH
THE
OVERHEADS
THIS
PA
PER
PRESENTS
A
NEW
APPROACH
CALLED
DRPM
TO
MODULATE
DISK
SPEED
RPM
DYNAMICALLY
AND
GIVES
A
PRACTICAL
IMPLE
MENTATION
TO
EXPLOIT
THIS
MECHANISM
EXTENSIVE
SIMULATIONS
WITH
DIFFERENT
WORKLOAD
AND
HARDWARE
PARAMETERS
SHOW
THAT
DRPM
CAN
PROVIDE
SIGNIFICANT
ENERGY
SAVINGS
WITHOUT
COM
PROMISING
MUCH
ON
PERFORMANCE
THIS
PAPER
ALSO
DISCUSSES
PRACTICAL
ISSUES
WHEN
IMPLEMENTING
DRPM
ON
SERVER
DISKS
KEYWORDS
SERVER
DISKS
POWER
MANAGEMENT
INTRODUCTION
DATA
CENTRIC
SERVICES
FILE
AND
MEDIA
SERVERS
WEB
AND
E
COMMERCE
APPLICATIONS
AND
TRANSACTION
PROCESSING
SYS
TEMS
TO
NAME
A
FEW
HAVE
BECOME
COMMONPLACE
IN
THE
COMPUTING
ENVIRONMENTS
OF
LARGE
AND
SMALL
BUSINESS
EN
TERPRISES
AS
WELL
AS
RESEARCH
AND
ACADEMIC
INSTITUTIONS
IN
ADDITION
OTHER
DATA
CENTRIC
SERVICES
SUCH
AS
SEARCH
ENGINES
AND
DATA
REPOSITORIES
ON
THE
INTERNET
ARE
SUSTAINING
THE
NEEDS
OF
THOUSANDS
OF
USERS
EACH
DAY
THE
COMMERCIAL
CONSE
QUENCES
OF
THE
PERFORMANCE
AND
OR
DISRUPTION
OF
SUCH
SER
VICES
HAVE
MADE
PERFORMANCE
RELIABILITY
AND
AVAILABILITY
THE
MAIN
TARGETS
FOR
OPTIMIZATION
TRADITIONALLY
HOWEVER
POWER
CONSUMPTION
IS
INCREASINGLY
BECOMING
A
MAJOR
CON
CERN
IN
THESE
SYSTEMS
OPTIMIZING
FOR
POWER
HAS
BEEN
UNDERSTOOD
TO
BE
IMPORTANT
FOR
EXTENDING
BATTERY
LIFE
IN
EMBEDDED
MOBILE
SYSTEMS
IT
IS
ONLY
RECENTLY
THAT
THE
IM
PORTANCE
OF
POWER
OPTIMIZATION
IN
SERVER
ENVIRONMENTS
HAS
GAINED
INTEREST
BECAUSE
OF
THE
COST
OF
POWER
DELIVERY
COST
OF
COOLING
THE
SYSTEM
COMPONENTS
AND
THE
IMPACT
OF
HIGH
OPERATING
TEMPERATURES
ON
THE
STABILITY
AND
RELIABILITY
OF
THE
COMPONENTS
SEVERAL
RECENT
STUDIES
HAVE
POINTED
OUT
THAT
DATA
CEN
TERS
CAN
CONSUME
SEVERAL
MEGA
WATTS
OF
POWER
IT
HAS
THIS
RESEARCH
IS
SUPPORTED
IN
PART
BY
SEVERAL
NSF
GRANTS
INCLUDING
AND
BEEN
OBSERVED
THAT
POWER
DENSITIES
OF
DATA
CENTERS
COULD
GROW
TO
OVER
WATTS
PER
SQUARE
FOOT
AND
THAT
THE
CA
PACITY
OF
NEW
DATA
CENTERS
FOR
COULD
REQUIRE
NEARLY
TWH
AROUND
PER
YEAR
A
CONSIDERABLE
PORTION
OF
THIS
POWER
BUDGET
ON
THESE
SERVERS
IS
EXPECTED
TO
BE
TAKEN
UP
BY
THE
DISK
SUBSYSTEM
WHEREIN
A
LARGE
NUMBER
OF
DISKS
ARE
EMPLOYED
TO
HANDLE
THE
LOAD
AND
STORAGE
CA
PACITY
REQUIREMENTS
TYPICALLY
SOME
KIND
OF
I
O
PARAL
LELISM
RAID
IS
EMPLOYED
TO
SUSTAIN
THE
HIGH
BAND
WIDTH
THROUGHPUT
NEEDS
OF
SUCH
APPLICATIONS
WHICH
ARE
IN
HERENTLY
DATA
CENTRIC
WHILE
ONE
COULD
KEEP
ADDING
DISKS
FOR
THIS
PURPOSE
AT
SOME
POINT
THE
CONSEQUENT
COSTS
OF
IN
CREASING
POWER
CONSUMPTION
MAY
OVERSHADOW
THE
BENEFITS
IN
PERFORMANCE
DISKS
EVEN
WHEN
IDLE
SPINNING
BUT
NOT
PERFORMING
AN
OPERATION
CAN
DRAIN
A
SIGNIFICANT
AMOUNT
OF
POWER
FOR
INSTANCE
A
SERVER
CLASS
IBM
ULTRASTAR
DISK
IS
RATED
AT
W
COMPARE
THIS
TO
AN
INTEL
XEON
PRO
CESSOR
CLOCKED
AT
GHZ
WHICH
IS
RATED
AT
W
WHEN
WE
GO
TO
SPECIFIC
SERVER
CONFIGURATIONS
E
G
A
WAY
INTEL
XEON
SMP
CLOCKED
AT
GHZ
WITH
DISKS
DRAWN
FROM
THE
DISKS
CONSUME
TIMES
MORE
POWER
THAN
THE
PROCESSORS
ONE
POSSIBLE
SOLUTION
IS
TO
USE
A
LARGE
CACHE
UNDER
THE
ASSUMPTION
THAT
THE
I
O
WORKLOAD
WILL
EXHIBIT
GOOD
LOCALITY
CACHING
CAN
ALSO
POTENTIALLY
BE
USED
TO
DELAY
WRITES
AS
PRO
POSED
BY
COLARELLI
ET
AL
FOR
ARCHIVAL
AND
BACKUP
SYSTEMS
HOWEVER
IN
MOST
SERVERS
THOUGH
LARGE
CACHES
ARE
COMMON
THEY
ARE
TYPICALLY
USED
FOR
PREFETCHING
TO
HIDE
DISK
LATENCIES
SINCE
NOT
ALL
SERVER
WORKLOADS
EXHIBIT
HIGH
TEMPORAL
LOCALITY
TO
EFFECTIVELY
USE
THE
CACHE
PREFETCHING
DOES
NOT
REDUCE
THE
POWER
CONSUMPTION
OF
THE
DISKS
ANOTHER
WAY
OF
ALLEVIATING
THIS
PROBLEM
IS
BY
SHUTTING
DOWN
DISKS
OR
AT
LEAST
STOP
THEM
FROM
SPINNING
SINCE
THE
SPINDLE
MOTOR
FOR
THE
DISKS
CONSUME
MOST
OF
THE
POWER
AS
WILL
BE
DESCRIBED
IN
SECTION
MANY
DISKS
OFFER
DIFFERENT
POWER
MODES
AND
ONE
COULD
CHOOSE
TO
TRANSITION
THEM
TO
A
LOW
POWER
MODE
WHEN
NOT
IN
USE
IDLE
WHICH
ACHIEVES
THIS
FUNCTIONALITY
E
G
STOP
THE
SPINNING
SUCH
TECHNIQUES
HAVE
BEEN
EFFECTIVELY
USED
IN
LAP
TOP
ENVIRONMENTS
WHERE
THE
GOAL
IS
MAINLY
TO
SAVE
BATTERY
ENERGY
WHEN
IN
A
LOW
POWER
MODE
THE
DISK
NEEDS
TO
BE
SPUN
UP
TO
FULL
SPEED
BEFORE
A
REQUEST
CAN
BE
SERVICED
AND
THIS
LATENCY
IS
MUCH
MORE
CRITICAL
IN
SERVERS
THAN
IN
A
LAPTOP
SETTING
FURTHER
THE
APPLICATION
OF
SUCH
TRADITIONAL
MODE
CONTROL
TECHNIQUES
FOR
SERVER
ENVIRONMENTS
IS
CHALLENGING
WHERE
ONE
MAY
NOT
HAVE
ENOUGH
IDLENESS
AND
WHERE
PERFOR
MANCE
IS
MORE
CRITICAL
FROM
THE
ABOVE
DISCUSSION
WE
SEE
TWO
EXTREMES
IN
OP
ERATION
ONE
THAT
IS
PERFORMANCE
EFFICIENT
WITH
DISKS
SPIN
NING
ALL
THE
TIME
AND
THE
OTHER
WHERE
THE
GOAL
IS
POWER
OPTIMIZATION
BY
STOPPING
THE
SPINNING
OF
THE
DISK
WHENEVER
THERE
IS
A
CHANCE
AT
THE
COST
OF
PERFORMANCE
IN
THIS
PAPER
WE
PRESENT
A
NEW
OPTION
DYNAMIC
ROTATIONS
PER
MINUTE
DRPM
WHERE
ONE
COULD
CHOOSE
TO
DYNAMICALLY
OPERATE
BETWEEN
THESE
EXTREMES
AND
ADAPTIVELY
MOVE
TO
WHICHEVER
CRITERION
IS
MORE
IMPORTANT
AT
ANY
TIME
THE
BASIC
IDEA
IS
TO
DYNAMICALLY
MODULATE
THE
SPEED
AT
WHICH
THE
DISK
SPINS
RPM
THEREBY
CONTROLLING
THE
POWER
EXPENDED
IN
THE
SPIN
DLE
MOTOR
DRIVING
THE
PLATTERS
SLOWING
DOWN
THE
SPEED
OF
SPINNING
THE
PLATTERS
CAN
POTENTIALLY
PROVIDE
QUADRATIC
WITH
RESPECT
TO
THE
CHANGE
IN
RPM
POWER
SAVINGS
HOWEVER
A
LOWER
RPM
CAN
HURT
ROTATIONAL
LATENCIES
AND
TRANSFER
COSTS
WHEN
SERVICING
A
REQUEST
AT
BEST
LINEARLY
IN
ADDITION
TO
THESE
ROTATIONAL
LATENCIES
AND
TRANSFER
COSTS
DISK
ACCESSES
INCUR
SEEK
OVERHEADS
TO
POSITION
THE
HEAD
TO
THE
APPROPRIATE
TRACK
AND
THIS
IS
NOT
IMPACTED
BY
THE
RPM
CONSEQUENTLY
IT
IS
POSSIBLE
TO
BENEFIT
MORE
FROM
POWER
THAN
ONE
MAY
LOSE
IN
PERFORMANCE
FROM
SUCH
RPM
MODULATIONS
THIS
DRPM
MECHANISM
PROVIDES
THE
FOLLOWING
BENEFITS
OVER
THE
TRADI
TIONAL
POWER
MODE
CONTROL
TECHNIQUES
REFERRED
TO
AS
TPM
IN
THIS
PAPER
SINCE
TPM
MAY
NEED
A
LOT
MORE
TIME
TO
SPIN
DOWN
THE
DISK
REMAIN
IN
THE
LOW
POWER
MODE
AND
THEN
SPIN
THE
DISK
BACK
UP
THERE
MAY
NOT
BE
A
SUFFICIENT
DURATION
OF
IDLENESS
TO
COVER
ALL
THIS
TIME
WITHOUT
DELAYING
SUBSE
QUENT
DISK
REQUESTS
ON
THE
OTHER
HAND
DRPM
DOES
NOT
NEED
TO
FULLY
SPIN
DOWN
THE
DISK
AND
CAN
MOVE
DOWN
TO
A
LOWER
RPM
AND
THEN
BACK
UP
AGAIN
IF
RE
QUIRED
IN
A
SHORTER
TIME
RPM
CHANGE
COSTS
ARE
MORE
OR
LESS
LINEAR
WITH
THE
AMPLITUDE
OF
THE
CHANGE
THE
SYSTEM
CAN
SERVICE
REQUESTS
MORE
READILY
WHEN
THEY
ARRIVE
THE
DISK
DOES
NOT
NECESSARILY
HAVE
TO
BE
SPUN
BACK
UP
TO
ITS
FULL
SPEED
BEFORE
SERVICING
A
REQUEST
AS
IS
DONE
IN
TPM
ONE
COULD
CHOOSE
TO
SPIN
IT
UP
IF
NEEDED
TO
A
HIGHER
SPEED
THAN
WHAT
IT
IS
AT
CURRENTLY
TAKING
LOWER
TIME
THAN
GETTING
IT
FROM
RPM
TO
FULL
SPEED
OR
SER
VICE
THE
REQUEST
AT
THE
CURRENT
SPEED
ITSELF
WHILE
OPT
ING
TO
SERVICE
THE
REQUEST
AT
A
SPEED
LESS
THAN
THE
FULL
SPEED
MAY
STRETCH
THE
REQUEST
SERVICE
TIME
THE
EXIT
LA
TENCY
FROM
A
LOWER
POWER
MODE
WOULD
BE
MUCH
LOWER
THAN
IN
TPM
DRPM
PROVIDES
THE
FLEXIBILITY
OF
DYNAMICALLY
CHOOS
ING
THE
OPERATING
POINT
IN
POWER
PERFORMANCE
TRADE
OFFS
IT
ALLOWS
THE
SERVER
TO
USE
STATE
OF
THE
ART
DISKS
FASTEST
IN
THE
MARKET
AND
PROVIDES
THE
ABILITY
TO
MOD
ULATE
THEIR
POWER
WHEN
NEEDED
WITHOUT
HAVING
TO
LIVE
WITH
A
STATIC
CHOICE
OF
SLOWER
DISKS
IT
ALSO
PROVIDES
A
LARGER
CONTINUUM
OF
OPERATING
POINTS
FOR
SERVICING
RE
QUESTS
THAN
THE
TWO
EXTREMES
OF
FULL
SPEED
OR
RPM
THIS
ALLOWS
THE
DISK
SUBSYSTEM
TO
ADAPT
ITSELF
TO
THE
LOAD
IMPOSED
ON
IT
TO
SAVE
ENERGY
AND
STILL
PROVIDE
THE
PERFORMANCE
THAT
IS
EXPECTED
OF
IT
THE
DRPM
APPROACH
IS
SOMEWHAT
ANALOGOUS
TO
VOLT
AGE
FREQUENCY
SCALING
IN
INTEGRATED
CIRCUITS
WHICH
PRO
VIDES
MORE
OPERATING
POINTS
FOR
POWER
PERFORMANCE
TRADE
OFFS
THAN
AN
ON
OFF
OPERATION
CAPABILITY
A
LOWER
VOLTAGE
USUALLY
ACCOMPANIED
WITH
A
SLOWER
CLOCK
FOR
LETTING
CIR
CUITS
STABILIZE
PROVIDES
QUADRATIC
POWER
SAVINGS
AND
THE
SLOWER
CLOCK
STRETCHES
RESPONSE
TIME
LINEARLY
THUS
PROVID
ING
ENERGY
SAVINGS
DURING
THE
OVERALL
EXECUTION
THIS
IS
THE
FIRST
PAPER
TO
PROPOSE
AND
INVESTIGATE
A
SIMILAR
IDEA
FOR
DISK
POWER
MANAGEMENT
THE
PRIMARY
CONTRIBUTION
OF
THIS
PAPER
IS
THE
DRPM
MECHANISM
ITSELF
WHERE
WE
IDENTIFY
ALREADY
AVAILABLE
TECH
NOLOGY
THAT
ALLOWS
DISKS
TO
SUPPORT
MULTIPLE
RPMS
MORE
IMPORTANTLY
WE
DEVELOP
A
PERFORMANCE
AND
POWER
MODEL
FOR
SUCH
DISKS
BASED
ON
THIS
TECHNOLOGY
SHOWING
HOW
COSTS
FOR
DYNAMIC
RPM
CHANGES
CAN
BE
MODELED
THE
REST
OF
THIS
PAPER
LOOKS
AT
EVALUATING
THIS
MECHANISM
ACROSS
DIFFERENT
WORKLOAD
BEHAVIORS
WE
FIRST
LOOK
AT
HOW
WELL
AN
OPTIMAL
ALGORITHM
CALLED
THAT
PRO
VIDES
THE
MAXIMUM
ENERGY
SAVINGS
WITHOUT
ANY
DEGRADA
TION
IN
PERFORMANCE
PERFORMS
UNDER
DIFFERENT
WORKLOADS
AND
COMPARE
ITS
PROS
AND
CONS
WITH
AN
OPTIMAL
VERSION
OF
TPM
CALLED
WHICH
PROVIDES
THE
MAXIMUM
POWER
SAVINGS
FOR
TPM
WITHOUT
ANY
DEGRADATION
IN
PERFOR
MANCE
WHEN
THE
LOAD
IS
EXTREMELY
HIGH
I
E
THERE
ARE
VERY
FEW
IDLE
PERIODS
THERE
IS
NOT
MUCH
THAT
CAN
BE
DONE
IN
TERMS
OF
POWER
SAVINGS
IF
ONE
DOES
NOT
WANT
TO
COMPRO
MISE
AT
ALL
ON
PERFORMANCE
REGARDLESS
OF
WHAT
TECHNIQUE
ONE
MAY
WANT
TO
USE
AT
THE
OTHER
END
OF
THE
SPECTRUM
WHEN
THERE
ARE
VERY
LARGE
IDLE
PERIODS
WE
FIND
PROVID
ING
GOOD
POWER
SAVINGS
AS
IS
TO
BE
EXPECTED
SINCE
IT
COM
PLETELY
STOPS
SPINNING
THE
DISKS
AS
OPPOSED
TO
WHICH
KEEPS
THEM
SPINNING
ALBEIT
AT
A
SLOW
SPEED
HOW
EVER
THERE
IS
A
WIDE
RANGE
OF
INTERMEDIATE
OPERATING
CONDI
TIONS
WHEN
DRPM
TURNS
OUT
TO
GIVE
MUCH
BETTER
UPTO
SAVINGS
IN
THE
IDLE
MODE
ENERGY
CONSUMPTION
EVEN
IF
ONE
DOES
NOT
WISH
TO
COMPROMISE
AT
ALL
ON
PERFORMANCE
IT
IS
ALSO
POSSIBLE
TO
INTEGRATE
THE
DRPM
AND
TPM
APPROACHES
WHEREIN
ONE
COULD
USE
TPM
WHEN
IDLE
TIMES
ARE
VERY
LONG
AND
DRPM
OTHERWISE
FINALLY
THIS
PAPER
PRESENTS
A
SIMPLE
HEURISTIC
THAT
DY
NAMICALLY
MODULATES
DISK
SPEED
USING
THE
DRPM
MECHA
NISM
AND
EVALUATES
HOW
WELL
IT
PERFORMS
WITH
RESPECT
TO
WHERE
ONE
HAS
PERFECT
KNOWLEDGE
OF
THE
FUTURE
ONE
COULD
MODULATE
THIS
ALGORITHM
BY
SETTING
TOLERANCE
LEV
ELS
FOR
DEGRADATION
IN
RESPONSE
TIMES
TO
AMPLIFY
THE
POWER
SAVINGS
WE
FIND
THAT
THIS
SOLUTION
COMES
FAIRLY
CLOSE
TO
THE
POWER
SAVINGS
OF
WHICH
DOES
NOT
INCUR
ANY
RESPONSE
TIME
DEGRADATION
WITHOUT
SIGNIFICANT
PENALTIES
IN
RESPONSE
TIME
AND
CAN
SOMETIMES
EVEN
DO
BETTER
IN
TERMS
OF
POWER
SAVINGS
THE
REST
OF
THIS
PAPER
IS
ORGANIZED
AS
FOLLOWS
THE
NEXT
SECTION
GIVES
AN
OVERVIEW
OF
THE
SOURCES
OF
ENERGY
CON
SUMPTION
IN
A
DISK
AND
PRIOR
TECHNIQUES
FOR
POWER
OPTIMIZA
TION
SECTION
PRESENTS
THE
DRPM
MECHANISM
AND
THE
COST
MODELS
FOR
ITS
IMPLEMENTATION
SECTION
GIVES
THE
EXPERI
MENTAL
SETUP
AND
SECTION
GIVES
RESULTS
WITH
COMPARING
ITS
POTENTIAL
WITH
TPM
AND
CONDUCTS
A
SENSITIV
ITY
ANALYSIS
THE
DETAILS
OF
OUR
HEURISTIC
FOR
ONLINE
SPEED
SETTING
AND
ITS
EVALUATION
ARE
GIVEN
IN
SECTION
SECTION
DISCUSSES
SOME
ISSUES
THAT
ARISE
WHEN
IMPLEMENTING
A
REAL
DRPM
DISK
FINALLY
SECTION
SUMMARIZES
THE
CONTRIBU
TIONS
OF
THIS
PAPER
DISK
POWER
AND
TPM
THERE
ARE
SEVERAL
COMPONENTS
AT
THE
DISK
THAT
CONTRIBUTE
TO
ITS
OVERALL
POWER
CONSUMPTION
THESE
INCLUDE
THE
SPINDLE
MOTOR
THAT
IS
RESPONSIBLE
FOR
SPINNING
THE
PLATTERS
THE
ACTU
ATOR
THAT
IS
RESPONSIBLE
FOR
THE
HEAD
MOVEMENTS
SEEKS
THE
ELECTRICAL
COMPONENTS
THAT
ARE
INVOLVED
IN
THE
TRANSFER
OP
ERATIONS
THE
DISK
CACHE
AND
OTHER
ELECTRONIC
CIRCUITRY
OF
THESE
THE
FIRST
TWO
ARE
MECHANICAL
COMPONENTS
AND
TYPICALLY
OVERSHADOW
THE
OTHERS
AND
OF
THESE
THE
SPINDLE
MOTOR
IS
THE
MOST
DOMINANT
STUDIES
OF
POWER
MEASUREMENTS
ON
DIFFER
ENT
DISKS
HAVE
SHOWN
THAT
THE
SPINDLE
MOTOR
ACCOUNTS
FOR
NEARLY
OF
THE
OVERALL
IDLE
POWER
FOR
A
TWO
PLATTER
DISK
AND
THIS
CAN
BE
AS
HIGH
AS
FOR
A
TEN
PLATTER
SERVER
CLASS
DISK
CONSEQUENTLY
TRADITIONAL
POWER
MANAGE
MENT
TECHNIQUES
AT
THE
HIGHER
LEVEL
FOCUS
ON
ADDRESSING
THIS
ISSUE
BY
SHUTTING
DOWN
THIS
MOTOR
WHEN
NOT
IN
ACTIVE
USE
DISK
POWER
MANAGEMENT
HAS
BEEN
EXTENSIVELY
STUDIED
IN
THE
CONTEXT
OF
SINGLE
DISK
SYSTEMS
PARTICULARLY
FOR
THE
LAP
TOP
DESKTOP
ENVIRONMENT
MANY
CURRENT
DISKS
OFFER
DIFFER
ENT
POWER
MODES
OF
OPERATION
SUCH
AS
ACTIVE
WHEN
THE
DISK
IS
SERVICING
A
REQUEST
IDLE
WHEN
IT
IS
SPINNING
AND
IBM
FAMILY
OF
SERVER
DISK
DRIVES
ULTRASTAR
NOT
SERVING
A
REQUEST
AND
ONE
OR
MORE
LOW
POWER
MODES
THAT
CONSUME
LESS
ENERGY
THAN
IDLE
WHERE
THE
DISK
PLATTERS
DO
NOT
SPIN
MANAGING
THE
ENERGY
CONSUMPTION
OF
THE
DISK
CONSISTS
OF
TWO
STEPS
NAMELY
DETECTING
SUITABLE
IDLE
PERIODS
AND
THEN
SPINNING
DOWN
THE
DISK
TO
A
LOW
POWER
MODE
WHEN
EVER
IT
IS
PREDICTED
THAT
THE
ACTION
WOULD
SAVE
ENERGY
DE
WDS
ULTRASTAR
XP
DFHS
SXX
ULTRASTAR
XP
DFMS
SXX
TECTION
OF
IDLE
PERIODS
USUALLY
INVOLVES
TRACKING
SOME
KIND
OF
HISTORY
TO
MAKE
PREDICTIONS
ON
HOW
LONG
THE
NEXT
IDLE
PE
DISK
ROTATION
SPEED
RPM
RIOD
WOULD
LAST
IF
THIS
PERIOD
IS
LONG
ENOUGH
TO
OUTWEIGH
SPINDOWN
SPINUP
COSTS
THE
DISK
IS
EXPLICITLY
SPUN
DOWN
TO
THE
LOW
POWER
MODE
WHEN
AN
I
O
REQUEST
COMES
TO
A
DISK
IN
THE
SPUNDOWN
STATE
THE
DISK
FIRST
NEEDS
TO
BE
SPUN
UP
TO
SERVICE
THIS
REQUEST
INCURRING
ADDITIONAL
EXIT
LATENCIES
AND
POWER
COSTS
IN
THE
PROCESS
MANY
IDLE
TIME
PREDICTORS
USE
A
TIME
THRESHOLD
TO
FIND
OUT
THE
DURATION
OF
THE
NEXT
IDLE
PE
RIOD
A
FIXED
THRESHOLD
IS
USED
IN
WHEREIN
IF
THE
IDLE
PERIOD
LASTS
OVER
SECONDS
THE
DISK
IS
SPUN
DOWN
AND
SPUN
BACK
UP
ONLY
WHEN
THE
NEXT
REQUEST
ARRIVES
THE
THRESH
OLD
COULD
ITSELF
BE
VARIED
ADAPTIVELY
OVER
THE
EXECUTION
OF
THE
PROGRAM
A
DETAILED
STUDY
OF
IDLE
TIME
PREDIC
TORS
AND
THEIR
EFFECTIVENESS
IN
DISK
POWER
MANAGEMENT
HAS
BEEN
CONDUCTED
IN
LU
ET
AL
PROVIDE
AN
EXPERIMEN
TAL
COMPARISON
OF
SEVERAL
DISK
POWER
MANAGEMENT
SCHEMES
PROPOSED
IN
LITERATURE
ON
A
SINGLE
DISK
PLATFORM
WE
BROADLY
REFER
TO
THESE
PREVIOUS
POWER
MODE
CONTROL
MECHANISMS
AS
TPM
IN
THIS
PAPER
IT
IS
TO
BE
NOTED
THAT
TPM
HAS
THE
DISK
SPINNING
AT
EITHER
ITS
FULL
SPEED
OR
FULLY
STATIONARY
AND
DOES
NOT
ALLOW
INTERMEDIATE
RPMS
ANOTHER
POWER
SAVING
APPROACH
THOUGH
ORTHOGONAL
TO
THIS
WORK
IS
TO
REPLACE
A
SINGLE
DISK
WITH
MULTIPLE
SMALLER
FORM
FACTOR
DISKS
THAT
CONSUME
LOWER
POWER
AS
IN
DYNAMIC
RPM
DRPM
THE
TPM
TECHNIQUES
AND
OUR
DRPM
MECHANISM
CAN
BE
USED
IN
CONJUNCTION
WITH
OTHER
TECHNIQUES
THAT
CAN
RE
DUCE
DISK
ACCESSES
BY
AGGREGATION
AND
OR
CACHING
USING
HARDWARE
OS
APPLICATION
SUPPORT
OR
PLACE
DATA
TO
REDUCE
HEAD
MOVEMENTS
WHICH
CAN
SAVE
ACTUATOR
POWER
TO
FURTHER
THE
POWER
SAVINGS
HOWEVER
AS
WAS
MENTIONED
EAR
LIER
THE
SPINDLE
MOTOR
POWER
NEEDED
TO
SPIN
THE
DISKS
IS
STILL
THE
MAJOR
POWER
CONSUMER
WHICH
IS
EXPENDED
EVEN
WHEN
THE
DISK
IS
NOT
SERVING
A
REQUEST
AND
IS
SPINNING
AS
CAN
BE
SEEN
IN
FIGURE
WHICH
SHOWS
THE
RPMS
AND
POWER
CONSUMPTION
OF
DIFFERENT
IBM
SERVER
CLASS
DISKS
OVER
THE
YEARS
THERE
APPEARS
TO
BE
A
STRONG
CORRELATION
BETWEEN
THE
ROTATIONAL
SPEED
AND
THE
IDLE
POWER
IN
THESE
DISKS
THOUGH
IT
SHOULD
BE
NOTED
THAT
RPM
IS
NOT
THE
ONLY
VARIATION
ACROSS
THE
TECHNOLOGIES
EMPLOYED
THIS
MOTIVATES
US
TO
INVESTI
GATE
THE
POSSIBILITY
OF
MODULATING
THE
RPM
DYNAMICALLY
TO
ADJUST
THE
POWER
CONSUMPTION
BASICS
OF
DISK
SPINDLE
MOTORS
A
DETAILED
EXPOSITION
OF
DISK
SPINDLE
MOTORS
SPMS
CAN
BE
FOUND
IN
DISK
SPMS
ARE
PERMANENT
MAGNET
DC
BRUSHLESS
MOTORS
IN
ORDER
TO
OPERATE
AS
A
BRUSHLESS
MOTOR
SENSORS
ARE
REQUIRED
INSIDE
THE
MOTOR
TO
PROVIDE
THE
PULSES
NECESSARY
FOR
COMMUTATION
I
E
ROTATION
THESE
SENSORS
MAY
EITHER
BE
HALL
EFFECT
SENSORS
OR
BACK
EMF
SENSORS
SPEED
CONTROL
OF
THE
MOTORS
CAN
BE
ACHIEVED
BY
USING
PULSE
WIDTH
MODULATION
PWM
TECHNIQUES
WHICH
MAKE
USE
OF
THE
DATA
FROM
THE
SENSORS
FIGURE
IBM
SERVER
DISKS
IDLE
POWER
CON
SUMPTION
FOR
EACH
DISK
THE
FORM
FACTOR
WAS
FIXED
AT
AND
THE
LARGEST
CAPACITY
CONFIG
URATION
WAS
CHOSEN
THE
IDLE
POWER
IS
RELA
TIVELY
INDEPENDENT
OF
THE
FORM
FACTOR
WE
FOUND
THAT
THE
IDLE
POWER
WAS
NOT
THAT
STRONGLY
RELATED
TO
THE
CAPACITY
FOR
INSTANCE
TWO
OTHER
IBM
DISKS
THE
ULTRASTAR
AND
THE
ULTRASTAR
ARE
BOTH
RPM
DISKS
WITH
AND
GB
CAPACITY
RESPECTIVELY
WHILE
THEIR
IDLE
POWER
CONSUMPTION
IS
W
AND
W
RESPECTIVELY
A
LARGE
ACCELERATING
TORQUE
IS
FIRST
NEEDED
TO
MAKE
THE
DISKS
START
SPINNING
THIS
HIGH
TORQUE
IS
ESSENTIALLY
REQUIRED
TO
OVERCOME
THE
STICTION
FORCES
CAUSED
BY
THE
HEADS
STICKING
TO
THE
SURFACE
OF
THE
DISK
PLATTER
THE
USE
OF
TECHNOLOGIES
LIKE
LOAD
UNLOAD
CAN
AMELIORATE
THIS
PROBLEM
BY
LIFT
ING
THE
DISK
ARM
FROM
THE
SURFACE
OF
THE
PLATTER
THESE
TECH
NOLOGIES
ALSO
PROVIDE
POWER
BENEFITS
AND
ARE
USED
FOR
EXAM
PLE
IN
IBM
HARD
DISK
DRIVES
TO
IMPLEMENT
THE
SPECIAL
IDLE
MODE
IN
ADDITION
TO
PROVIDING
THE
STARTING
TORQUE
THE
SPM
ALSO
NEEDS
TO
SUSTAIN
ITS
RPM
ONCE
IT
REACHES
THE
INTENDED
SPEED
ONE
TRADITIONAL
APPROACH
IN
IMPROVING
DISK
PERFORMANCE
OVER
THE
YEARS
HAS
BEEN
TO
INCREASE
THE
RPM
WHICH
RE
DUCES
ROTATIONAL
LATENCIES
AND
TRANSFER
TIMES
WHICH
CAN
PROVE
BENEFICIAL
IN
BANDWIDTH
BOUND
APPLICATIONS
HOW
EVER
SUCH
INCREASES
CAN
CAUSE
CONCERN
IN
ADDITIONAL
IS
SUES
SUCH
AS
NOISE
AND
NON
REPEATABLE
RUN
OUTS
NRROS
NRROS
ARE
OFF
TRACK
ERRORS
THAT
CAN
OCCUR
AT
HIGHER
RPMS
ESPECIALLY
AT
HIGH
TRACK
DENSITIES
THESE
DESIGN
CONSIDERA
TIONS
IN
THE
DEVELOPMENT
OF
HIGH
RPM
DISKS
HAVE
BEEN
AD
DRESSED
BY
THE
USE
OF
ADVANCED
MOTOR
BEARING
TECHNOLOGIES
LIKE
FLUID
AND
AIR
BEARINGS
HOWEVER
THE
POWER
ASSOCIATED
WITH
HIGH
RPMS
STILL
RE
MAINS
AND
THIS
PAPER
FOCUSES
ON
THIS
SPECIFIC
ASPECT
ANALYTICAL
FORMULATIONS
FOR
MOTOR
DYNAMICS
OUR
DRPM
SOLUTION
DYNAMICALLY
CONTROLS
THE
SPINDLE
MOTOR
TO
CHANGE
THE
RPM
OF
THE
SPINNING
PLATTERS
THE
RPM
SELECTION
CAPABILITY
CAN
BE
PROVIDED
BY
ALLOWING
THE
SPINDLE
MOTOR
CONTROL
BLOCK
OF
THE
HARD
DISK
CONTROLLER
TO
BE
PROGRAMMABLE
FOR
EXAMPLE
THE
DESIRED
RPM
CAN
BE
INPUT
VIA
A
PROGRAMMABLE
REGISTER
IN
THE
HARD
DISK
CONTROLLER
THE
VALUE
READ
FROM
THIS
REGISTER
CAN
INTURN
BE
USED
BY
THE
SPINDLE
MOTOR
DRIVER
TO
GENERATE
THE
REQUI
SITE
SIGNALS
FOR
OPERATING
THE
DISK
AT
THAT
RPM
WE
NOW
PRESENT
THE
TIME
OVERHEAD
NEEDED
TO
EFFECT
AN
RPM
CHANGE
AND
THE
POWER
OF
THE
RESULTING
STATE
AS
A
FUNC
TION
OF
THE
RPM
CALCULATING
RPM
TRANSITION
TIMES
IN
ORDER
TO
CALCULATE
THE
TIME
REQUIRED
FOR
A
SPEED
CHANGE
WE
NEED
SOME
PHYSICAL
DATA
OF
THE
SPINDLE
MOTOR
THIS
IN
FORMATION
FOR
A
SPECIFIC
DISK
IS
USUALLY
PROPRIETARY
BUT
THERE
ARE
DC
BRUSHLESS
MOTORS
COMMERCIALLY
AVAILABLE
THAT
WE
CAN
USE
FOR
THIS
PURPOSE
WE
HAVE
OBTAINED
THE
NECESSARY
IN
FORMATION
FROM
THE
DATASHEET
OF
A
MAXON
EC
MM
FLAT
BRUSHLESS
PERMANENT
MAGNET
DC
MOTOR
WHOSE
PHYSI
CAL
CHARACTERISTICS
CLOSELY
MATCH
THOSE
OF
A
HARD
DISK
SPINDLE
MOTOR
TABLE
SUMMARIZES
THE
BASIC
MECHANICAL
CHARACTER
ISTICS
OF
THIS
MOTOR
SPM
CURRENT
PROFILE
OF
MULTIMODE
HARDDISK
DRIVE
SPM
SPEED
RPM
FIGURE
CURRENT
DRAWN
BY
SONY
MULTIMODE
HARD
DISK
TABLE
MAXON
EC
MOTOR
CHARACTERISTICS
THE
MOTOR
SPECIFICATIONS
GIVE
A
FORMULA
FOR
CALCULATING
THE
TIME
IN
MS
REQUIRED
FOR
A
SPEED
CHANGE
OF
RPM
WITH
A
LOAD
INERTIA
AS
THE
LOAD
ON
THE
SPINDLE
MOTOR
IS
THE
PLATTER
ASSEMBLY
WE
DISMANTLED
A
QUANTUM
HARD
DISK
AND
MEASURED
THE
WEIGHT
OF
AN
INDIVIDUAL
PLATTER
USING
A
SENSITIVE
BALANCE
AND
ALSO
ITS
RADIUS
ITS
WEIGHT
WAS
FOUND
TO
BE
GM
AND
RADIUS
WAS
CM
USING
THESE
VALUES
AND
ASSUMING
PLATTERS
PER
DISK
AS
IN
THOUGH
WE
ALSO
HAVE
SENSITIVITY
RESULTS
FOR
DIFFERENT
NUMBER
OF
PLATTERS
WE
CALCULATED
THE
MOMENT
OF
INERTIA
OF
THE
LOAD
IN
GCM
AS
WHERE
IS
THE
NUMBER
OF
PLATTERS
THEREFORE
WE
HAVE
THIS
SHOWS
THAT
THE
TIME
COST
OF
CHANGING
THE
RPM
OF
THE
DISK
IS
DIRECTLY
PROPORTIONAL
LINEAR
TO
THE
AMPLITUDE
OF
THE
RPM
CHANGE
CALCULATING
THE
POWER
CONSUMPTION
AT
AN
RPM
LEVEL
WE
BRIEFY
EXPLAIN
THE
DEPENDENCE
BETWEEN
THE
POWER
CON
SUMPTION
OF
A
MOTOR
AND
ITS
ROTATION
SPEED
A
DETAILED
EXPO
SITION
OF
THIS
TOPIC
CAN
BE
FOUND
IN
THE
MOTOR
VOLTAGE
IS
RELATED
TO
THE
ANGULAR
VELOCITY
ROTATION
SPEED
AS
WHERE
IS
A
CONSTANT
THE
POWER
CONSUMED
BY
THE
MOTOR
IS
WHERE
R
IS
THE
RESISTANCE
OF
THE
MOTOR
THEREFORE
WE
HAVE
THIS
EQUATION
SIMILAR
TO
THAT
RELATING
THE
POWER
AND
VOLTAGE
FOR
CMOS
CIRCUITS
INDICATES
THAT
A
CHANGE
IN
THE
ROTATION
SPEED
OF
THE
DISK
HAS
A
QUADRATIC
EFFECT
ON
ITS
POWER
CON
SUMPTION
IN
ORDER
TO
INVESTIGATE
WHETHER
THIS
RELATIONSHIP
HOLDS
TRUE
IN
THE
CONTEXT
OF
A
HARD
DISK
WE
USED
AN
EXPER
IMENTAL
CURVE
FITTING
APPROACH
THERE
EXISTS
A
COMMERCIAL
HARD
DISK
TODAY
THE
MULTIMODE
HARD
DISK
DRIVE
FROM
SONY
THAT
INDEED
SUPPORTS
A
VARIABLE
SPEED
SPINDLE
MOTOR
THE
SPEED
SETTING
ON
SUCH
A
DISK
IS
ACCOMPLISHED
IN
A
MORE
STATIC
PRE
CONFIGURED
FASHION
RATHER
THAN
MODULATING
THIS
DURING
THE
COURSE
OF
EXECUTION
THE
PUBLISHED
CURRENT
CON
SUMPTION
VALUES
OF
THIS
DISK
FOR
DIFFERENT
RPM
VALUES
PRO
VIDES
INSIGHT
ON
HOW
THE
CURRENT
DRAWN
VARIES
WITH
THE
RPM
FIGURE
SHOWS
THE
CURRENT
DRAWN
BY
THE
SPM
OF
THE
MUL
TIMODE
HARD
DISK
REPEATED
FROM
A
SIMPLE
CURVE
FIT
OF
THESE
DATA
POINTS
CLEARLY
SHOWS
THIS
QUADRATIC
RELATIONSHIP
THIS
RELATIONSHIP
MAY
APPEAR
SOMEWHAT
DIFFERENT
FROM
THE
TRENDS
SHOWN
IN
FIGURE
WHERE
ONE
COULD
ARGUE
THAT
THE
RELATIONSHIP
BETWEEN
RPM
AND
POWER
IS
LINEAR
HOWEVER
FIGURE
SHOWS
THE
TREND
FOR
DISKS
OF
DIFFERENT
GENERATIONS
WHERE
IT
IS
NOT
ONLY
THE
RPM
THAT
CHANGES
BUT
THE
HARDWARE
ITSELF
ON
THE
OTHER
HAND
EQUATION
AND
THE
MULTIMODE
HARD
DISK
CURRENT
CONSUMPTION
PROFILE
FIGURE
SHOWS
THE
RELATION
BETWEEN
THESE
TWO
PARAMETERS
IS
MORE
QUADRATIC
FOR
AN
INDIVIDUAL
DISK
DRIVE
THIS
MULTIMODE
DISK
IS
COMPOSED
OF
ONLY
TWO
PLATTERS
WHILE
WE
ARE
LOOKING
AT
SERVER
CLASS
DISKS
THAT
HAVE
SEVERAL
MORE
PLATTERS
PLATTERS
CONSEQUENTLY
WE
CANNOT
DI
RECTLY
APPLY
THIS
MODEL
TO
OUR
ENVIRONMENT
ON
THE
OTHER
HAND
A
STUDY
FROM
IBM
PROJECTS
THE
RELATION
BETWEEN
IDLE
POWER
AND
RPM
FOR
SERVER
CLASS
IBM
DISKS
IN
THIS
STUDY
OTHER
DESIGN
FACTORS
SUCH
AS
THE
CHANGE
IN
THE
NUMBER
OF
DISK
PLATTERS
HAVE
BEEN
CONSIDERED
BESIDES
JUST
THE
RPM
TO
MAKE
THE
PROJECTIONS
FOR
THE
INDIVIDUAL
DISKS
AND
WE
DE
PICT
THE
RESULTS
FROM
THERE
BY
THE
POINTS
SHOWN
IN
FIGURE
IN
OUR
POWER
MODELING
STRATEGY
FOR
A
VARIABLE
RPM
DISK
WE
EMPLOYED
TWO
APPROACHES
TO
CAPTURE
A
QUADRATIC
AND
LINEAR
RELATIONSHIP
RESPECTIVELY
WE
TOOK
THE
POINTS
FROM
THE
IBM
STUDY
AND
USED
A
QUADRATIC
CURVE
TO
APPROXIMATE
THEIR
BEHAVIOR
AS
IS
SHOWN
BY
THE
SOLID
CURVE
IN
FIGURE
TO
MODEL
THE
IDLE
POWER
AS
DRPM
VS
IBM
DATA
TPM
INCLUDING
THE
POWER
CONSUMPTION
OF
EACH
MODE
AND
THE
TRANSITION
COSTS
ARE
ILLUSTRATED
IN
FIGURE
WE
HAVE
CONSIDERED
SEVERAL
RPM
OPERATING
LEVELS
I
E
DIFFERENT
RESOLUTIONS
FOR
STEPPING
UP
DOWN
THE
SPEED
OF
THE
SPINDLE
MOTOR
THESE
STEP
SIZES
ARE
AS
LOW
AS
RPM
PROVIDING
STEPS
BETWEEN
THE
EXTREMES
OF
AND
RPM
RPM
LEVELS
IN
ALL
THE
DEFAULT
CONFIGURATION
THAT
WE
USE
IN
OUR
EXPERIMENTS
IS
A
DISK
RAID
ARRAY
WITH
A
QUADRATIC
DRPM
POWER
MODEL
AND
A
STEP
SIZE
OF
RPM
9000
SPM
SPEED
RPM
FIGURE
COMPARISON
OF
DRPM
MODEL
TO
THE
IBM
PROJECTIONS
WE
ALSO
PERFORMED
A
LINEAR
LEAST
SQUARES
FIT
THROUGH
THE
POINTS
AS
IS
SHOWN
BY
THE
DOTTED
LINE
IN
FIGURE
TO
MODEL
THE
IDLE
POWER
AS
PARAMETER
VALUE
PARAMETERS
COMMON
TO
TPM
AND
DRPM
WE
HAVE
USED
BOTH
MODELS
IN
OUR
EXPERIMENTS
TO
STUDY
THE
POTENTIAL
OF
DRPM
IN
GENERAL
WE
FIND
THAT
THE
RESULTS
ARE
NOT
VERY
DIFFERENT
IN
THE
RANGES
OF
RPMS
THAT
WERE
STUD
IED
BETWEEN
TO
AS
IS
EVIDENT
EVEN
FROM
FIGURE
WHICH
SHOWS
THE
DIFFERENCES
BETWEEN
LINEAR
AND
QUADRATIC
MODELS
WITHIN
THIS
RANGE
ARE
NOT
VERY
SIGNIFICANT
EQUATIONS
AND
PROVIDE
THE
NECESSARY
INFORMATION
IN
MODELING
THE
RPM
TRANSITION
COSTS
AND
POWER
CHARACTER
ISTICS
OF
OUR
DRPM
STRATEGY
FOR
THE
POWER
COSTS
OF
TRAN
SITIONING
FROM
ONE
RPM
TO
ANOTHER
WE
CONSERVATIVELY
AS
SUME
THAT
THE
POWER
DURING
THIS
TIME
IS
THE
SAME
AS
THAT
OF
THE
HIGHER
RPM
STATE
EXPERIMENTAL
SETUP
AND
WORKLOAD
DE
SCRIPTION
WE
CONDUCTED
OUR
EVALUATIONS
USING
THE
DISKSIM
SIMULATOR
MODELING
A
DISK
ARRAY
FOR
A
SERVER
ENVIRON
MENT
DISKSIM
PROVIDES
A
LARGE
NUMBER
OF
TIMING
AND
CONFIGURATION
PARAMETERS
FOR
SPECIFYING
DISKS
AND
THE
CON
TROLLERS
BUSES
FOR
THE
I
O
INTERFACE
THE
SIMULATOR
WAS
AUG
MENTED
WITH
POWER
MODELS
TO
RECORD
THE
ENERGY
CONSUMP
TION
OF
THE
DISKS
WHEN
PERFORMING
OPERATIONS
LIKE
DATA
TRANSFERS
SEEKS
OR
WHEN
JUST
IDLING
OUR
DRPM
IMPLEMEN
TATION
ACCOUNTS
FOR
THE
QUEUING
AND
SERVICE
DELAYS
CAUSED
BY
THE
CHANGES
IN
THE
RPM
OF
THE
DISKS
IN
THE
ARRAY
THE
DEFAULT
CONFIGURATION
PARAMETERS
USED
IN
THE
SIMULATIONS
ARE
GIVEN
IN
TABLE
MANY
OF
WHICH
HAVE
BEEN
TAKEN
FROM
THE
DATA
SHEET
OF
THE
IBM
ULTRASTAR
SERVER
HARD
DISK
THE
POWER
CONSUMPTION
OF
THE
STANDBY
MODE
WAS
CALCU
LATED
BY
SETTING
THE
SPINDLE
MOTOR
POWER
CONSUMPTION
TO
WHEN
CALCULATING
BASED
ON
THE
METHOD
DESCRIBED
IN
SECTION
NOTE
THAT
THIS
VALUE
FOR
THE
POWER
CONSUMPTION
IS
VERY
AGGRESSIVE
AS
THE
ACTUAL
POWER
CONSUMPTION
EVEN
IN
THIS
MODE
IS
TYPICALLY
MUCH
HIGHER
FOR
EXAMPLE
ITS
VALUE
IS
W
IN
THE
ACTUAL
ULTRASTAR
DISK
HOWEVER
AS
WE
SHALL
LATER
SHOW
EVEN
WITH
SUCH
A
DEEP
LOW
POWER
STANDBY
MODE
THAT
IS
USED
BY
TPM
DRPM
SURPASSES
TPM
IN
TERMS
OF
ENERGY
BENEFITS
IN
SEVERAL
CASES
ALSO
IN
OUR
POWER
MODELS
WE
HAVE
ACCOUNTED
FOR
THE
CASE
THAT
THE
POWER
PENALTIES
FOR
THE
ACTIVE
AND
SEEK
MODES
IN
ADDITION
TO
IDLE
POWER
ALSO
DEPEND
UPON
THE
RPM
OF
THE
DISK
THE
MODES
EXPLOITED
BY
DRPM
SPECIFIC
PARAMETERS
TABLE
SIMULATION
PARAMETERS
WITH
THE
DE
FAULT
CONFIGURATIONS
UNDERLINED
DISK
SPINUPS
AND
SPINDOWNS
OCCUR
FROM
TO
RPM
AND
VICE
VERSA
RESPECTIVELY
FIGURE
TPM
POWER
MODES
SINCE
WE
WANT
TO
DEMONSTRATE
THE
POTENTIAL
OF
THE
DRPM
MECHANISM
ACROSS
A
SPECTRUM
OF
OPERATING
CONDITIONS
DIF
FERENT
LOADS
LONG
IDLE
PERIODS
BURSTS
OF
I
O
REQUESTS
ETC
THAT
SERVER
DISKS
MAY
EXPERIENCE
AND
TO
EVALUATE
THE
PROS
AND
CONS
OF
DRPM
OVER
OTHER
POWER
SAVING
APPROACHES
WE
CHOSE
TO
CONDUCT
THIS
STUDY
WITH
SEVERAL
SYNTHETIC
WORK
LOADS
WHERE
WE
COULD
MODULATE
SUCH
BEHAVIOR
THE
SYN
THETIC
WORKLOAD
GENERATOR
INJECTS
A
MILLION
I
O
REQUESTS
WITH
DIFFERENT
INTER
ARRIVAL
TIMES
AND
REQUEST
PARAMETERS
START
ING
SECTOR
REQUEST
SIZE
AND
THE
TYPE
OF
ACCESS
READ
WRITE
ALL
THE
WORKLOADS
CONSIST
OF
READ
REQUESTS
AND
OF
ALL
REQUESTS
ARE
SEQUENTIAL
IN
NATURE
THESE
CHARACTERISTICS
WERE
CHOSEN
BASED
ON
SINCE
A
CLOSED
SYSTEM
SIMULA
TION
MAY
ALTER
THE
INJECTED
LOAD
BASED
ON
SERVICE
TIMES
OF
THE
DISK
ARRAY
FOR
PREVIOUS
REQUESTS
WE
CONDUCTED
AN
OPEN
SYSTEM
SIMULATION
WITH
THESE
WORKLOADS
WE
CONSIDERED
TWO
TYPES
OF
DISTRIBUTIONS
FOR
THE
INTER
ARRIVAL
TIMES
NAMELY
EXPONENTIAL
AND
PARETO
AS
IS
WELL
UNDERSTOOD
EXPONENTIAL
ARRIVALS
MODEL
A
PURELY
RANDOM
POISSON
PROCESS
AND
TO
A
LARGE
EXTENT
MODELS
A
REGULAR
TRAFFIC
ARRIVAL
BEHAVIOR
WITHOUT
BURSTINESS
ON
THE
OTHER
HAND
THE
PARETO
DISTRIBUTION
INTRODUCES
BURSTINESS
IN
AR
RIVALS
WHICH
CAN
BE
CONTROLLED
THE
PARETO
DISTRIBUTION
IS
CHARACTERIZED
BY
TWO
PARAMETERS
NAMELY
CALLED
THE
SHAPE
PARAMETER
AND
CALLED
THE
LOWER
CUTOFF
VALUE
THE
SMALLEST
VALUE
A
PARETO
RANDOM
VARIABLE
CAN
TAKE
WE
CHOSE
A
PARETO
DISTRIBUTION
WITH
A
FINITE
MEAN
AND
INFINITE
BREAKDOWN
OF
ENERGY
CONSUMPTION
EXP
EXP
PAR
PAR
WORKLOAD
CONFIGURATION
VARIANCE
FOR
BOTH
DISTRIBUTIONS
WE
VARIED
THE
MEAN
INTER
ARRIVAL
TIME
IN
MS
AS
A
PARAMETER
IN
PARETO
THERE
ARE
DIFFERENT
WAYS
BY
WHICH
THE
TRAFFIC
CAN
BE
GENERATED
FOR
A
GIVEN
MEAN
WE
SET
THE
TO
MS
AND
VARIED
I
E
WHEN
THE
MEAN
IS
INCREASED
THE
TIME
BETWEEN
THE
BURSTS
IDLENESS
TEND
TO
INCREASE
WE
USE
THE
TERM
WORKLOAD
TO
DEFINE
THE
COMBINATION
OF
THE
DISTRIBUTION
THAT
IS
BEING
USED
AND
THE
MEAN
INTER
ARRIVAL
TIME
FOR
THIS
DISTRIBUTION
FOR
INSTANCE
THE
WORKLOAD
PAR
DENOTES
A
PARETO
TRAFFIC
WITH
A
MEAN
INTER
ARRIVAL
TIME
OF
MS
WE
COMPARE
THE
SCHEMES
FOR
EACH
WORKLOAD
USING
THREE
METRICS
NAMELY
TOTAL
ENERGY
CONSUMPTION
OVER
ALL
THE
RE
QUESTS
IDLE
MODE
ENERGY
CONSUMPTION
OVER
ALL
THE
RE
QUESTS
AND
RESPONSE
TIME
PER
I
O
REQUEST
T
THESE
CAN
BE
DEFINED
AS
FOLLOWS
THE
TOTAL
ENERGY
CONSUMPTION
IS
THE
ENERGY
CON
SUMED
BY
ALL
THE
DISKS
IN
THE
ARRAY
FROM
THE
BEGINNING
TO
THE
END
OF
THE
SIMULATION
PERIOD
WE
MONITOR
ALL
THE
DISK
ACTIVITY
STATES
AND
THEIR
DURATION
IN
EACH
STATE
AND
USE
THIS
TO
CALCULATE
THE
OVERALL
ENERGY
CONSUMP
TION
BY
THE
DISKS
INTEGRAL
OF
THE
POWER
IN
EACH
STATE
OVER
THE
DURATION
IN
THAT
STATE
THE
IDLE
MODE
ENERGY
CONSUMPTION
IS
THE
EN
ERGY
CONSUMED
BY
ALL
THE
DISKS
IN
THE
ARRAY
WHILE
NOT
SERVICING
AN
I
O
REQUEST
I
E
WHILE
NOT
PERFORMING
SEEKS
OR
DATA
TRANSFERS
THIS
VALUE
IS
DIRECTLY
IM
PACTED
BY
THE
SPINNING
SPEED
OF
THE
SPINDLE
MOTOR
THE
RESPONSE
TIME
IS
THE
TIME
BETWEEN
THE
REQUEST
SUBMISSION
AND
THE
REQUEST
COMPLETION
AVERAGED
OVER
ALL
THE
REQUESTS
THIS
DIRECTLY
HAS
A
BEARING
ON
THE
DE
LIVERED
SYSTEM
THROUGHPUT
FINALLY
WE
USE
THE
TERMS
POWER
AND
ENERGY
INTER
CHANGEABLY
SOMETIMES
POWER
OPTIMIZATION
WITHOUT
PERFORMANCE
DEGRADATION
ENERGY
BREAKDOWN
OF
THE
WORKLOADS
BEFORE
WE
EXAMINE
DETAILED
RESULTS
IT
IS
IMPORTANT
TO
UN
DERSTAND
WHERE
POWER
IS
BEING
DRAINED
OVER
THE
COURSE
OF
EXECUTION
I
E
WHEN
THE
DISK
IS
TRANSFERRING
DATA
ACTIVE
OR
POSITIONING
THE
HEAD
POSITIONING
OR
WHEN
IT
IS
IDLING
IDLE
FIGURE
GIVES
THE
BREAKDOWN
OF
ENERGY
CONSUMPTION
OF
TWO
WORKLOADS
FROM
EACH
OF
THE
INTER
ARRIVAL
TIME
DISTRIBUTIONS
ONE
AT
HIGH
AND
ANOTHER
AT
LOW
LOAD
CONDITIONS
INTO
THESE
THREE
COMPONENTS
WHEN
THERE
IS
NO
POWER
SAVING
TECHNIQUE
PARETO
PROBABILITY
DISTRIBUTION
FUNCTION
IS
GIVEN
BY
THE
MEAN
IS
GIVEN
BY
FIGURE
BREAKDOWN
OF
FOR
THE
DIFFER
ENT
WORKLOADS
ON
THE
X
AXIS
EACH
PAIR
REP
RESENTS
A
WORKLOAD
DEFINED
BY
PROBABILITY
DISTRIBUTION
MEAN
INTER
ARRIVAL
TIME
PAIR
EMPLOYED
THE
HIGH
AND
LOW
LOADS
ALSO
INDICATE
THAT
IDLE
PERIODS
ARE
LOW
AND
HIGH
RESPECTIVELY
AS
IS
TO
BE
EXPECTED
WHEN
THE
LOAD
IS
LIGHT
EXP
PAR
THE
IDLE
ENERGY
IS
THE
MOST
DOM
INANT
COMPONENT
HOWEVER
WE
FIND
THAT
EVEN
WHEN
WE
MOVE
TO
HIGH
LOAD
CONDITIONS
EXP
PAR
THE
IDLE
ENERGY
IS
STILL
THE
MOST
SIGNIFICANT
OF
THE
THREE
WHILE
THE
POSITIONING
ENERGY
DOES
BECOME
IMPORTANT
AT
THESE
HIGH
LOADS
THE
RESULTS
SUGGEST
THAT
MOST
OF
THE
BENEFITS
TO
GAIN
ARE
FROM
OPTIMIZING
THE
IDLE
POWER
IN
PARTICULAR
THE
SPINDLE
MOTOR
COMPONENT
WHICH
CONSUMES
OF
THIS
POWER
CONSEQUENTLY
OUR
FOCUS
IN
THE
REST
OF
THIS
SECTION
IS
ON
THE
IDLE
POWER
COMPONENT
BY
LOOKING
AT
HOW
DIFFERENT
SCHEMES
TPM
AND
DRPM
EXPLOIT
THE
IDLENESS
FOR
ENERGY
SAVINGS
THE
POTENTIAL
BENEFITS
OF
DRPM
THE
POWER
SAVING
EITHER
WITH
TPM
OR
DRPM
IS
BASED
ON
THE
IDLENESS
OF
DISKS
BETWEEN
SERVING
REQUESTS
WHILE
IN
THE
LATTER
CASE
IT
IS
POSSIBLE
TO
GET
MORE
SAVINGS
BY
SERV
ING
REQUESTS
AT
A
LOWER
RPM
THIS
MAY
RESULT
IN
PERFOR
MANCE
DEGRADATION
IN
THE
FIRST
SET
OF
RESULTS
WE
DO
NOT
CONSIDER
THIS
TO
BE
AN
OPTION
I
E
WE
DEFINE
A
SCHEME
CALLED
WHOSE
PERFORMANCE
IS
NOT
ANY
DIFFERENT
FROM
THE
ORIGINAL
DISK
SUBSYSTEM
WHICH
DOES
NOT
EMPLOY
ANY
POWER
MANAGEMENT
TECHNIQUE
FURTHER
TO
INVESTIGATE
WHAT
COULD
BE
THE
POTENTIAL
OF
DRPM
WE
ASSUME
THE
EXISTENCE
OF
AN
IDLE
TIME
PREDICTION
ORACLE
WHICH
CAN
EXACTLY
PREDICT
WHEN
THE
NEXT
REQUEST
WILL
ARRIVE
AFTER
SERVING
EACH
REQUEST
CONSEQUENTLY
USES
THIS
PREDICTION
TO
FIND
OUT
HOW
LOW
AN
RPM
IT
CAN
GO
DOWN
TO
AND
THEN
COME
BACK
UP
TO
FULL
SPEED
BEFORE
SERVICING
THE
NEXT
REQUEST
NOTING
THE
TIMES
AND
ENERGY
REQUIRED
FOR
DOING
SUCH
TRANSITIONS
TO
BE
FAIR
THE
SAME
ORACLE
CAN
BE
USED
BY
TPM
AS
WELL
FOR
EFFECTING
POWER
MODE
TRANSITIONS
AND
WE
CALL
SUCH
A
SCHEME
WHERE
THE
DISK
IS
TRANSITIONED
TO
THE
STANDBY
MODE
IF
THE
TIME
TO
THE
NEXT
REQUEST
IS
LONG
ENOUGH
TO
ACCOMMODATE
THE
SPINDOWN
FOLLOWED
BY
A
SPINUP
NOTE
THAT
CAN
EXPLOIT
MUCH
SMALLER
IDLE
TIMES
FOR
POWER
SAVINGS
COMPARED
TO
ON
THE
OTHER
HAND
WHEN
THE
IDLE
TIME
IS
REALLY
LONG
CAN
SAVE
MORE
ENERGY
BY
STOPPING
THE
SPINNING
COMPLETELY
WHILE
DRPM
CAN
TAKE
IT
DOWN
TO
ONLY
RPM
THERE
FORE
IN
ORDER
TO
INVESTIGATE
THE
POTENTIAL
BENEFITS
IF
BOTH
THESE
TECHNIQUES
WERE
USED
IN
CONJUNCTION
WE
HAVE
ALSO
CONSIDERED
A
SCHEME
CALLED
IN
THE
SCHEME
WE
USE
THE
ORACLE
TO
DETERMINE
WHICH
OF
THE
TWO
EXPONENTIAL
TRAFFIC
PARETO
TRAFFIC
SPINDLE
MOTOR
WHICH
HAS
TO
SPIN
THEM
AS
WAS
DESCRIBED
EAR
LIER
IN
FIGURE
THE
EFFECT
OF
THREE
DIFFERENT
PLATTER
COUNTS
AND
HAS
BEEN
SHOWN
FOR
THE
TWO
TYPES
OF
TRAF
FIC
WITH
DIFFERENT
LOAD
CONDITIONS
IT
CAN
BE
SEEN
THAT
AS
THE
NUMBER
OF
PLATTERS
INCREASES
THE
SAVINGS
DROP
THIS
IS
BECAUSE
A
LARGER
WEIGHT
IS
IMPOSED
ON
THE
SPINDLE
MOTOR
RE
QUIRING
A
HIGHER
TORQUE
FOR
RPM
CHANGES
THEREBY
INCURRING
MORE
OVERHEADS
NEVERTHELESS
EVEN
AT
THE
PLATTER
COUNT
WHICH
IS
SIGNIFICANTLY
HIGHER
THAN
THOSE
IN
USE
TODAY
WE
STILL
FIND
APPRECIABLE
POWER
SAVINGS
EVEN
AT
HIGH
LOAD
CONDITIONS
INTER
ARRIVAL
TIME
MS
INTER
ARRIVAL
TIME
MS
SENSITIVITY
TO
PLATTER
COUNT
EXPONENTIAL
TRAFFIC
SENSITIVITY
TO
PLATTER
COUNT
PARETO
TRAFFIC
FIGURE
SAVINGS
IN
IDLE
ENERGY
USING
AND
ARE
PRE
SENTED
FOR
THE
QUADRATIC
POWER
MODEL
TECHNIQUES
SAVES
THE
MAXIMUM
ENERGY
FOR
EACH
IDLE
TIME
PERIOD
WE
WOULD
LIKE
TO
POINT
OUT
THAT
AND
DO
NOT
PUT
A
BOUND
ON
THE
ENERGY
SAVINGS
INTER
ARRIVAL
TIME
MS
INTER
ARRIVAL
TIME
MS
THAT
ONE
CAN
EVER
GET
RATHER
THEY
GIVE
A
BOUND
WHEN
PER
FORMANCE
CANNOT
BE
COMPROMISED
FIGURE
PRESENTS
THE
IDLE
ENERGY
SAVINGS
WHICH
WAS
SHOWN
TO
BE
THE
MAJOR
CON
TRIBUTOR
OF
OVERALL
ENERGY
FOR
THESE
SCHEMES
AS
A
FUNCTION
OF
THE
INTER
ARRIVAL
TIMES
IN
THE
TWO
DISTRIBUTIONS
WHEN
WE
FIRST
EXAMINE
THE
EXPONENTIAL
TRAFFIC
RESULTS
WE
NOTE
THAT
THE
RESULTS
CONFIRM
OUR
EARLIER
DISCUSSION
WHEREIN
LARGE
INTER
ARRIVAL
TIMES
FAVOR
AT
THE
OTHER
END
OF
THE
SPECTRUM
WHEN
INTER
ARRIVAL
TIMES
GET
VERY
SMALL
THERE
IS
NOT
REALLY
MUCH
SCOPE
FOR
ANY
OF
THESE
SCHEMES
TO
SAVE
ENERGY
IF
PERFORMANCE
COMPROMISE
IS
NOT
AN
OPTION
HOW
EVER
BETWEEN
THESE
TWO
EXTREMES
WE
FIND
THAT
PROVIDES
MUCH
HIGHER
SAVINGS
THAN
IT
FINDS
MORE
IDLE
TIME
OPPORTUNITIES
TO
TRANSITION
TO
A
LOWER
RPM
MODE
WHICH
MAY
NOT
BE
LONG
ENOUGH
FOR
TPM
AS
IS
TO
BE
EX
PECTED
THE
COMBINED
SCHEME
APPROACHES
THE
BETTER
OF
THE
TWO
ACROSS
THE
SPECTRUM
OF
WORKLOADS
WHEN
WE
NEXT
LOOK
AT
THE
PARETO
TRAFFIC
RESULTS
WE
FIND
THAT
THE
ARRIVALS
ARE
FAST
ENOUGH
DUE
TO
BURSTINESS
OF
THIS
DISTRIBUTION
EVEN
AT
THE
HIGHER
MEAN
VALUES
CONSIDERED
THAT
CONSISTENTLY
OUTPERFORMS
IN
THE
RANGE
UNDER
CONSIDERATION
IT
IS
ALSO
THIS
REASON
THAT
MAKES
THE
ENERGY
SAVINGS
OF
ALL
THE
SCHEMES
WITH
THIS
TRAFFIC
DISTRI
BUTION
LOWER
THAN
THAT
FOR
EXPONENTIAL
WHERE
THE
IDLE
TIMES
ARE
LESS
VARYING
THE
PURPOSE
OF
THIS
EXERCISE
WAS
TO
EXAMINE
THE
POTENTIAL
OF
DRPM
WITH
RESPECT
TO
TPM
WHILE
NOT
COMPROMISING
ON
PERFORMANCE
THE
REST
OF
THIS
SECTION
LOOKS
TO
UNDERSTAND
ING
THE
SENSITIVITY
OF
THE
POWER
SAVINGS
WITH
THIS
APPROACH
TO
DIFFERENT
HARDWARE
AND
WORKLOAD
PARAMETERS
SINCE
THE
SENSITIVITY
OF
DRPM
IS
MORE
PROMINENT
AT
THE
INTERMEDIATE
LOAD
CONDITIONS
WHERE
IT
WAS
SHOWN
TO
GIVE
BETTER
SAVINGS
THAN
TPM
WE
FOCUS
MORE
ON
THOSE
REGIONS
IN
THE
REST
OF
THIS
PAPER
SENSITIVITY
ANALYSIS
OF
NUMBER
OF
PLATTERS
DISKS
SHOW
SIGNIFICANT
VARIABILITY
IN
PLATTER
COUNTS
AT
ONE
END
THE
LAPTOP
DISKS
HAVE
OR
PLATTERS
WHILE
SERVER
CLASS
DISKS
CAN
HAVE
AS
MANY
AS
PLATTERS
THE
NUMBER
OF
PLATTERS
HAS
A
CONSEQUENCE
ON
THE
WEIGHT
IMPOSED
ON
THE
FIGURE
SENSITIVITY
TO
NUMBER
OF
PLATTERS
IN
THE
DISK
ASSEMBLY
WHILE
ONE
MAY
THINK
THAT
THE
NEED
FOR
STORAGE
CAPACITY
INCREASE
OVER
TIME
MAY
NECESSITATE
MORE
PLATTERS
IT
IS
TO
BE
NOTED
THAT
THIS
INCREASE
IS
USUALLY
ACHIEVED
BY
DENSER
MEDIA
RATHER
THAN
BY
ADDING
MORE
PLATTERS
FOR
INSTANCE
THE
IBM
DFHS
SXX
AND
HAVE
STORAGE
CAPACITIES
OF
GB
GB
AND
GB
RESPEC
TIVELY
BUT
THEIR
PLATTER
COUNTS
ARE
AND
THEREFORE
WE
DO
NOT
EXPECT
PLATTER
COUNTS
TO
INCREASE
SIGNIFICANTLY
QUADRATIC
VS
LINEAR
POWER
MODEL
AS
WAS
DISCUSSED
IN
SECTION
WE
CONSIDERED
BOTH
QUADRATIC
AND
LINEAR
SCALING
MODELS
FOR
THE
IDLE
POWER
CONSUMPTION
OF
THE
SPINDLE
MOTOR
AT
DIFFERENT
RPMS
WHILE
THE
EARLIER
RESULTS
WERE
PRESENTED
WITH
THE
QUADRATIC
MODEL
WE
COM
PARE
THOSE
RESULTS
WITH
THE
SAVINGS
FOR
WITH
THE
LINEAR
MODEL
IN
FIGURE
WE
CAN
OBSERVE
THAT
THE
DIFFER
ENCES
BETWEEN
THESE
TWO
MODELS
ARE
NOT
VERY
SIGNIFICANT
THOUGH
THE
LINEAR
MODEL
SLIGHTLY
UNDER
PERFORMS
THAT
OF
THE
QUADRATIC
AS
IS
TO
BE
EXPECTED
THIS
AGAIN
CONFIRMS
OUR
EAR
LIER
OBSERVATIONS
THAT
THE
DIFFERENCES
BETWEEN
A
LINEAR
AND
QUADRATIC
MODEL
ARE
NOT
VERY
DIFFERENT
ACROSS
THESE
RANGES
OF
RPM
VALUES
CONSEQUENTLY
WE
FIND
THAT
EVEN
WITH
A
CONSERVATIVE
LINEAR
POWER
SCALING
MODEL
GIVES
BETTER
ENERGY
SAVINGS
THAN
COMPARE
WITH
FIGURE
WE
HAVE
ALSO
CONDUCTED
SIMILAR
SENSITIVITY
ANALYSIS
FOR
OTHER
FACTORS
SUCH
AS
THE
STEP
SIZE
EMPLOYED
FOR
THE
SPINDLE
MOTOR
AND
THE
TYPE
OF
RAID
CONFIGURATION
THE
INTERESTED
READER
IS
REFERRED
TO
FOR
THE
DETAILS
IN
GENERAL
WE
FIND
THAT
CAN
PROVIDE
SIGNIFICANT
ENERGY
SAV
INGS
ACROSS
A
WIDE
SPECTRUM
OF
DISK
AND
ARRAY
CONFIGURA
TIONS
A
HEURISTIC
DRPM
ALGORITHM
HAVING
EVALUATED
THE
POTENTIAL
OF
DRPM
WITHOUT
ANY
PERFORMANCE
DEGRADATION
WHICH
REQUIRES
AN
IDLE
TIME
PRE
UT
PREVIOUS
N
REQUESTS
CURRENT
N
REQUESTS
UT
LT
UT
LT
MS
LT
MS
CHOICE
OF
LOW
WATERMARKS
CHOICE
OF
LOW
WATERMARKS
DIFF
CURRENT
UT
FOR
NEXT
N
REQUESTS
DO
NOTHING
A
B
C
FIGURE
THE
OPERATION
OF
THE
DRPM
HEURISTIC
FOR
AND
IN
EACH
FIGURE
FOR
THE
CHOICE
OF
LOW
WATERMARKS
THE
DOTTED
LINE
SHOWS
WHERE
LOW
WM
IS
BEFORE
THE
HEURISTIC
IS
APPLIED
AND
THE
SOLID
LINE
SHOWS
THE
RESULT
OF
APPLYING
THE
SCHEME
THE
PERCENTAGE
DIFFERENCE
IN
THE
RESPONSE
TIMES
AND
BETWEEN
SUCCESSIVE
REQUEST
WINDOWS
IS
CALCULATED
A
IF
THEN
LOW
WM
IS
SET
TO
THE
MAXIMUM
RPM
FOR
THE
NEXT
REQUESTS
B
IF
LIES
BETWEEN
THE
TWO
TOLERANCE
LIMITS
THE
CURRENT
VALUE
OF
LOW
WM
IS
RETAINED
C
IF
THEN
THE
VALUE
OF
LOW
WM
IS
SET
TO
A
VALUE
LESS
THAN
THE
MAXIMUM
RPM
SINCE
IS
HIGHER
THAN
OF
BUT
LESSER
THAN
OF
IN
THIS
EXAMPLE
IT
IS
SET
TWO
LEVELS
LOWER
THAN
THE
PREVIOUS
LOW
WM
IF
IT
WAS
BETWEEN
AND
IT
WOULD
HAVE
BEEN
SET
THREE
LEVELS
LOWER
AND
SO
ON
EXPONENTIAL
TRAFFIC
INTER
ARRIVAL
TIME
MS
PARETO
TRAFFIC
INTER
ARRIVAL
TIME
MS
SPONSE
TIMES
TO
FIND
POINTS
WHEN
PERFORMANCE
DEGRADATION
BECOMES
MORE
SIGNIFICANT
TO
RAMP
UP
THE
DISKS
OR
TO
LIMIT
HOW
LOW
THEY
CAN
OPERATE
AT
THOSE
INSTANTS
THE
ARRAY
CONTROLLER
TRACKS
AVERAGE
RESPONSE
TIMES
FOR
REQUEST
WINDOWS
AT
THE
END
OF
EACH
WINDOW
IT
CALCULATES
THE
PERCENTAGE
CHANGE
IN
THE
RESPONSE
TIME
OVER
THE
PAST
TWO
WINDOWS
IF
THIS
PERCENTAGE
CHANGE
IS
LARGER
THAN
AN
UPPER
TOLERANCE
LEVEL
THEN
THE
CON
TROLLER
IMMEDIATELY
ISSUES
A
COMMAND
TO
ALL
THE
DISKS
THAT
ARE
OPERATING
AT
LOWER
RPMS
TO
RAMP
UP
TO
THE
FULL
SPEED
THIS
IS
DONE
BY
SETTING
THE
LOW
WATERMARK
AT
EACH
DISK
TO
THE
FULL
RPM
WHICH
SAYS
THAT
THE
DISKS
ARE
NOT
SUPPOSED
TO
OPERATE
BELOW
THIS
FIGURE
BEHAVIOR
OF
FOR
A
POWER
MODEL
THAT
RELATES
THE
RPM
AND
LINEARLY
DICTION
ORACLE
WE
NEXT
MOVE
ON
TO
DESCRIBE
A
SCHEME
THAT
CAN
BE
USED
IN
PRACTICE
TO
BENEFIT
FROM
THIS
MECHANISM
THE
GOAL
IS
TO
SAVE
ENERGY
USING
THE
MULTIPLE
RPM
LEVELS
WITH
OUT
SIGNIFICANTLY
DEGRADING
PERFORMANCE
RESPONSE
TIME
IN
THIS
SCHEME
I
THE
ARRAY
CONTROLLER
COMMUNICATES
A
SET
OF
OPERATING
RPM
VALUES
TO
THE
INDIVIDUAL
DISKS
BASED
ON
HOW
PERFORMANCE
CHARACTERISTICS
RESPONSE
TIME
OF
THE
WORKLOAD
EVOLVE
MORE
SPECIFICALLY
THE
CONTROLLER
SPECI
FIES
WATERMARKS
FOR
DISK
RPM
EXTREMES
BETWEEN
WHICH
THE
DISKS
SHOULD
OPERATE
II
SUBSEQUENTLY
EACH
DISK
USES
LOCAL
INFORMATION
TO
DECIDE
ON
RPM
TRANSITIONS
PERIODICALLY
EACH
DISK
INSPECTS
ITS
REQUEST
QUEUE
TO
CHECK
THE
NUMBER
OF
REQUESTS
WAITING
FOR
IT
IF
THIS
NUM
BER
IS
LESS
THAN
OR
EQUAL
TO
A
SPECIFIC
VALUE
THIS
CAN
INDICATE
A
LOWER
LOAD
AND
THE
DISK
RAMPS
DOWN
ITS
SPEED
BY
ONE
STEP
IT
CAN
SO
HAPPEN
THAT
OVER
A
LENGTH
OF
TIME
THE
DISKS
MAY
GRADUALLY
MOVE
DOWN
TO
A
VERY
LOW
RPM
EVEN
WITH
A
HIGH
LOAD
AND
DO
NOT
MOVE
BACK
UP
CONSEQUENTLY
IT
IS
IMPORTANT
TO
PERIODICALLY
LIMIT
HOW
LOW
AN
RPM
THE
DISKS
SHOULD
BE
ALLOWED
TO
GO
TO
THIS
DECISION
IS
MADE
BY
THE
ARRAY
CONTROLLER
AT
THE
HIGHER
LEVEL
WHICH
CAN
TRACK
RE
VALUE
BETWEEN
AN
UPPER
AND
LOWER
TOLERANCE
LEVEL
THE
CONTROLLER
KEEPS
THE
AT
WHERE
IT
IS
SINCE
THE
RESPONSE
TIME
IS
WITHIN
THE
TOLERANCE
LEVELS
LESS
THAN
THE
LOWER
TOLERANCE
LEVEL
IN
WHICH
CASE
THE
CAN
BE
LOWERED
EVEN
FURTHER
THE
SPE
CIFIC
RPM
THAT
IS
USED
FOR
THE
IS
CALCU
LATED
PROPORTIONALLY
BASED
ON
HOW
MUCH
THE
RESPONSE
TIME
CHANGE
IS
LOWER
THAN
THESE
THREE
SCENARIOS
ARE
DEPICTED
IN
FIGURE
WHICH
SHOWS
THE
CHOICE
OF
THE
FOR
EXAMPLE
DIFFERENCES
IN
RE
SPONSE
TIME
CHANGES
WITH
AND
EIGHT
POSSIBLE
VALUES
FOR
THE
THESE
ARE
ALSO
THE
VAL
UES
USED
IN
THE
RESULTS
TO
BE
PRESENTED
AND
WINDOW
SIZES
ARE
THOUGH
WE
HAVE
EXPERIMENTED
WITH
A
MORE
COMPREHENSIVE
DESIGN
SPACE
IN
OUR
EXPERIMENTS
WE
SET
WHEREBY
THE
DISKS
INITIATE
A
RAMPDOWN
OF
THEIR
RPM
BASED
ON
WHETHER
THEIR
REQUEST
QUEUE
IS
EMPTY
OR
NOT
RESULTS
WITH
DRPM
WE
HAVE
CONDUCTED
EXTENSIVE
EXPERIMENTS
TO
EVALUATE
HOW
WELL
THE
ABOVE
HEURISTIC
DENOTED
AS
SIMPLY
DRPM
IN
THE
REST
OF
THIS
PAPER
FARES
NOT
ONLY
IN
TERMS
OF
ITS
ABSO
LUTE
ENERGY
SAVINGS
AND
RESPONSE
TIME
DEGRADATION
BUT
ALSO
COMPARING
IT
TO
THE
AND
STATIC
RPM
CHOICES
WHERE
NON
DRPM
DISKS
OF
LOWER
RPMS
ARE
USED
THE
COMPLETE
SET
OF
EXPERIMENTAL
RESULTS
IS
GIVEN
IN
AND
WE
PRESENT
THE
HIGHLIGHTS
HERE
THE
FIRST
SET
OF
RESULTS
IN
FIGURE
SHOW
THE
ENERGY
SAV
INGS
AND
RESPONSE
TIME
DEGRADATION
OF
OUR
DRPM
HEURISTIC
WITH
RESPECT
TO
NOT
PERFORMING
ANY
POWER
OPTIMIZATION
RE
FERRED
TO
AS
BASELINE
THE
ENERGY
SAVINGS
ARE
GIVEN
WITH
BOTH
THE
QUADRATIC
AND
LINEAR
POWER
MODELS
DISCUSSED
EAR
LIER
FOR
TWO
DIFFERENT
INTER
ARRIVAL
TIMES
IN
EACH
OF
THE
TWO
DISTRIBUTIONS
NOTE
THAT
THESE
ARE
SAVINGS
AND
NOT
JUST
THOSE
FOR
THE
IDLE
ENERGY
ERGY
COMPARED
TO
THE
ABOVE
HEURISTIC
WHICH
ALLOWS
LOWER
RPMS
FOR
SERVING
REQUESTS
AND
ALSO
CAN
INCUR
HIGHER
TRANSI
TION
COSTS
IN
ALWAYS
GETTING
BACK
TO
THE
HIGHEST
RPM
THESE
EFFECTS
ARE
MORE
SIGNIFICANT
AT
HIGHER
LOADS
SMALLER
IDLE
PE
RIODS
CAUSING
OUR
HEURISTIC
TO
IN
FACT
GIVE
BETTER
ENERGY
SAVINGS
THAN
AT
LIGHTER
LOADS
THE
LONG
IDLE
PERIODS
AMORTIZE
SUCH
COSTS
AND
THE
KNOWLEDGE
OF
HOW
LONG
THEY
ARE
HELPS
TRANSITION
DIRECTLY
TO
THE
APPRO
PRIATE
RPM
INSTEAD
OF
LINGERING
AT
HIGHER
RPMS
FOR
LONGER
TIMES
AS
IS
DONE
IN
THE
HEURISTIC
SCHEME
STILL
THE
ENERGY
SAVINGS
FOR
THE
HEURISTIC
ARE
QUITE
GOOD
AND
ARE
NOT
FAR
AWAY
FROM
WHICH
HAS
PERFECT
KNOWLEDGE
OF
IDLE
TIMES
THE
RESULTS
FOR
THE
HEURISTIC
HAVE
BEEN
SHOWN
WITH
DIFFERENT
CHOICES
FOR
THE
WINDOW
OF
REQUESTS
FOR
WHICH
THE
IS
RECALCULATED
A
LARGE
WINDOW
PERFORMS
ENERGY
SAVINGS
QUADRATIC
MODEL
EXP
EXP
PAR
PAR
WORKLOAD
CONFIGURATION
EXPONENTIAL
TRAFFIC
MS
MEAN
ENERGY
SAVINGS
LINEAR
MODEL
EXP
EXP
PAR
PAR
WORKLOAD
CONFIGURATION
EXPONENTIAL
TRAFFIC
MS
MEAN
MODULATIONS
AT
A
COARSER
GRANULARITY
THUS
ALLOWING
THE
DISKS
TO
LINGER
AT
LOWER
RPMS
LONGER
EVEN
WHEN
THERE
MAY
BE
SOME
PERFORMANCE
DEGRADATION
THIS
CAN
RESULT
IN
GREATER
ENERGY
SAVINGS
FOR
LARGER
VALUES
AS
IS
OBSERVED
IN
MANY
CASES
THE
RESPONSE
TIME
CHARACTERISTICS
OF
THE
HEURISTIC
ARE
SHOWN
AS
CDF
PLOTS
IN
FIGURE
RATHER
THAN
AS
AN
AV
ERAGE
TO
MORE
ACCURATELY
CAPTURE
THE
BEHAVIOR
THROUGH
THE
EXECUTION
IT
CAN
HAPPEN
THAT
A
FEW
REQUESTS
GET
INORDI
NATELY
DELAYED
WHILE
MOST
OF
THE
REQUESTS
INCUR
VERY
LIT
TLE
DELAYS
A
CDF
PLOT
WHICH
SHOWS
THE
FRACTION
OF
RE
QUESTS
THAT
HAVE
RESPONSE
TIMES
LOWER
THAN
A
GIVEN
VALUE
ON
THE
X
AXIS
CAN
CAPTURE
SUCH
BEHAVIOR
WHILE
A
SIMPLE
AV
ERAGE
ACROSS
REQUESTS
CANNOT
THESE
PLOTS
SHOW
THE
BASE
BASELINE
DRPM
BASELINE
DRPM
LINE
BEHAVIOR
WHICH
IS
THE
ORIGINAL
EXECUTION
WITHOUT
ANY
POWER
SAVINGS
BEING
EMPLOYED
AND
IS
ALSO
THE
BEHAVIOR
OF
DRPM
DRPM
DRPM
DRPM
WHICH
DOES
NOT
ALTER
THE
TIMING
BEHAVIOR
OF
RE
QUESTS
THE
CLOSENESS
OF
THE
CDF
PLOTS
OF
THE
HEURISTIC
TO
THE
BASELINE
CURVE
IS
AN
INDICATION
OF
HOW
GOOD
A
JOB
IT
DOES
OF
LIMITING
DEGRADATION
IN
RESPONSE
TIME
AT
HIGHER
LOADS
IT
IS
MORE
IMPORTANT
TO
MODULATE
THE
RPM
LEVELS
AT
A
FINER
GRANULARITY
TO
ENSURE
THAT
THE
DISKS
DO
NOT
KEEP
GOING
DOWN
IN
RPMS
ARBITRAR
ILY
WE
SEE
THAT
A
FINER
RESOLUTION
REQUESTS
DOES
TEND
TO
KEEP
THE
RESPONSE
TIME
CDF
OF
THE
HEURISTIC
RESPONSE
TIME
MS
PARETO
TRAFFIC
MS
MEAN
BASELINE
DRPM
RESPONSE
TIME
MS
PARETO
TRAFFIC
MS
MEAN
BASELINE
DRPM
CLOSE
TO
THE
BASELINE
IN
PAR
AND
PAR
ONE
CAN
HARDLY
DISCERN
DIFFERENCES
BETWEEN
THE
BASELINE
AND
THE
COR
RESPONDING
HEURISTIC
RESULTS
REMEMBER
THAT
THE
PARETO
TRAF
FIC
HAS
BURSTS
OF
I
O
REQUESTS
FOLLOWED
BY
LONGER
IDLE
PERI
DRPM
DRPM
DRPM
DRPM
ODS
SINCE
OUR
HEURISTIC
MODULATES
THE
BASED
ON
THE
NUMBER
OF
REQUESTS
RATHER
THAN
TIME
THIS
MODULA
TION
IS
DONE
FAST
ENOUGH
DURING
THE
BURSTS
SO
THAT
THE
RE
SPONSE
TIME
OF
THOSE
REQUESTS
ARE
NOT
SIGNIFICANTLY
COMPRO
MISED
AND
IS
DONE
SLOW
ENOUGH
DURING
THE
LONGER
IDLE
PERI
ODS
THAT
THE
ENERGY
SAVINGS
ARE
OBTAINED
DURING
THOSE
TIMES
IN
THE
EXPONENTIAL
TRAFFIC
WHILE
THERE
ARE
SOME
DEVIATIONS
FROM
THE
BASELINE
WE
ARE
STILL
ABLE
TO
KEEP
OVER
OF
RE
QUESTS
WITHIN
A
RESPONSE
TIME
DEGRADATION
MARGIN
WITH
RESPONSE
TIME
MS
RESPONSE
TIME
MS
A
WINDOW
WHILE
GIVING
OVER
ENERGY
SAVINGS
IN
THE
QUADRATIC
MODEL
CHANGING
THE
POWER
MODEL
FROM
QUADRATIC
TO
LINEAR
DOES
NOT
CHANGE
THE
TRENDS
AS
WAS
POINTED
FIGURE
DRPM
HEURISTIC
SCHEME
RESULTS
THE
RESULTS
ARE
PRESENTED
FOR
REFERRED
TO
AS
DRPM
DRPM
AND
DRPM
RESPECTIVELY
WE
OBSERVE
THAT
WE
CAN
GET
AS
GOOD
SAVINGS
IF
NOT
BETTER
IN
SOME
CASES
ESPECIALLY
WITH
HIGHER
LOADS
THAN
WHICH
HAS
ALREADY
BEEN
SHOWN
TO
GIVE
GOOD
ENERGY
SAVINGS
REMEMBER
THAT
SERVICES
RE
QUESTS
AT
THE
HIGHEST
RPM
EVEN
IF
IT
TRANSITIONS
TO
LOWER
RPMS
DURING
IDLE
PERIODS
THIS
RESULTS
IN
HIGHER
ACTIVE
EN
OUT
EARLIER
AND
WE
STILL
FIND
OVER
ENERGY
SAVINGS
CONTROLLING
UT
AND
LT
FOR
POWER
PERFORMANCE
TRADE
OFFS
THE
DRPM
HEURISTIC
PROVIDES
TWO
ADDITIONAL
PARAMETERS
IN
ADDITION
TO
ALREADY
CONSIDERED
UT
AND
LT
FOR
MOD
ULATING
THE
RPM
CONTROL
BY
KEEPING
UT
WHERE
IT
IS
AND
MOVING
LT
UP
CLOSER
TO
UT
WE
CAN
ALLOW
THE
DISKS
TO
TRANSITION
TO
EVEN
LOWER
RPM
LEVELS
THEREBY
SAVING
EVEN
MORE
ENERGY
WITHOUT
COMPROMISING
SIGNIFICANTLY
ON
PERFOR
MANCE
THIS
IS
SHOWN
BY
COMPARING
THE
RESULTS
FOR
UT
AND
LT
IN
FIGURE
A
WITH
THOSE
OF
THE
RESULTS
IN
FIGURE
AT
LEAST
FOR
HIGHER
LOADS
SIMILARLY
ONE
CAN
BRING
THE
UT
PARAMETER
CLOSER
TO
LT
TO
REDUCE
RESPONSE
TIME
DEGRADATION
WITHOUT
SIGNIFICANTLY
CHANGING
THE
ENERGY
RESULTS
THIS
IS
SHOWN
BY
COMPARING
THE
RESULTS
FOR
UT
AND
LT
IN
FIGURE
B
WITH
THOSE
OF
THE
RESULTS
IN
FIGURE
THIS
HEURISTIC
THUS
PROVIDES
AN
ELEGANT
APPROACH
FOR
DETERMINING
WHERE
ONE
WANTS
TO
OPERATE
IN
THE
POWER
PERFORMANCE
PROFILE
NIQUES
PWM
ACHIEVES
SPEED
CONTROL
BY
SWITCHING
ON
AND
OFF
THE
POWER
SUPPLY
TO
THE
MOTOR
AT
A
CERTAIN
FRE
QUENCY
CALLED
THE
DUTY
CYCLE
THE
CHOICE
OF
DUTY
CY
CLE
DETERMINES
THE
MOTOR
SPEED
THE
DESIGN
OF
SUCH
SPEED
CONTROL
MECHANISMS
CAN
BE
FOUND
IN
HEAD
FLY
HEIGHT
THE
HEIGHT
AT
WHICH
THE
DISK
HEAD
SLIDER
FLIES
FROM
THE
PLATTER
SURFACE
DEPENDS
ON
THE
LINEAR
VELOCITY
OF
THE
SPINNING
PLATTER
WHICH
CAN
BE
EXPRESSED
AS
ENERGY
SAVINGS
QUADRATIC
MODEL
EXP
EXP
WORKLOAD
CONFIGURATION
EXPONENTIAL
TRAFFIC
MS
MEAN
ENERGY
SAVINGS
QUADRATIC
MODEL
EXP
EXP
WORKLOAD
CONFIGURATION
EXPONENTIAL
TRAFFIC
MS
MEAN
WHERE
IS
THE
RADIUS
OF
THE
DISK
AND
IS
THE
FREQUENCY
OF
ROTATION
MEASURED
IN
RPM
THE
FLY
HEIGHT
NEEDS
TO
BE
MORE
OR
LESS
CONSTANT
OVER
THE
ENTIRE
RANGE
OF
LINEAR
VELOCITIES
SUPPORTED
BY
THE
GIVEN
SPINDLE
SYSTEM
THE
PAPILLON
SLIDER
PRESENTED
IN
IS
CAPABLE
OF
MAINTAINING
THIS
CONSTANT
FLY
HEIGHT
OVER
THE
RANGE
OF
RPMS
THAT
WE
HAVE
CONSIDERED
HEAD
POSITIONING
SERVO
AND
DATA
CHANNEL
DESIGN
IN
HARD
DISKS
POSITIONING
THE
HEAD
REQUIRES
ACCURATE
INFORMATION
ABOUT
THE
LOCATION
OF
THE
TRACKS
THIS
IN
FORMATION
IS
ENCODED
AS
SERVO
SIGNALS
ON
SPECIAL
SERVO
SECTORS
THAT
ARE
NOT
ACCESSIBLE
BY
NORMAL
READ
WRITE
OPERATIONS
TO
THE
DISK
THIS
SERVO
INFORMATION
IS
GIVEN
TO
THE
ACTUATOR
TO
ACCURATELY
POSITION
THE
HEAD
OVER
THE
CENTER
OF
THE
TRACKS
THE
SERVO
INFORMATION
NEEDS
TO
BE
SAMPLED
AT
A
CERTAIN
FREQUENCY
TO
POSITION
THE
HEAD
PROPERLY
AS
THE
STORAGE
DENSITY
INCREASES
THE
NUMBER
OF
TRACKS
PER
INCH
TPI
INCREASES
REQUIRING
HIGHER
SAMPLING
FREQUENCIES
THIS
SAMPLING
FREQUENCY
IS
DI
RECTLY
PROPORTIONAL
TO
THE
SPINNING
SPEED
OF
THE
DISK
THEREFORE
AT
LOWER
IT
MIGHT
NOT
BE
POSSI
BLE
TO
PROPERLY
SAMPLE
THE
SERVO
INFORMATION
AD
DRESSES
THIS
PROBLEM
BY
DESIGNING
A
SERVO
SYSTEM
THAT
RESPONSE
TIME
MS
RESPONSE
TIME
MS
CAN
OPERATE
AT
BOTH
LOW
AND
HIGH
DISK
RPMS
ALONG
WITH
A
DATA
CHANNEL
THAT
CAN
OPERATE
OVER
THE
ENTIRE
RANGE
OF
EXPONENTIAL
TRAFFIC
MS
MEAN
EXPONENTIAL
TRAFFIC
MS
MEAN
DATA
RATES
OVER
THE
DIFFERENT
RPMS
THE
DATA
RATE
OF
A
CHANNEL
IS
DIRECTLY
PROPORTIONAL
TO
IDLE
TIME
ACTIVITIES
SERVER
ENVIRONMENTS
OPTIMIZE
IDLE
PERIODS
IN
DISKS
TO
PERFORM
OTHER
OPERATIONS
SUCH
AS
VALIDATING
THE
DISK
CONTENTS
AND
OPTIMIZING
FOR
ANY
ERRORS
THE
FRE
QUENCIES
OF
SUCH
OPERATIONS
ARE
MUCH
LOWER
THAN
THE
IDLE
TIMES
THEMSELVES
TO
REALLY
HAVE
A
SIGNIFICANT
CON
SEQUENCE
ON
THE
EFFECTIVENESS
OF
POWER
SAVING
TECH
NIQUES
STILL
IT
IS
POSSIBLE
THAT
DRPM
MAY
BE
MORE
120
RESPONSE
TIME
MS
120
200
RESPONSE
TIME
MS
USEFUL
FOR
SUCH
ACTIVITIES
SINCE
IT
ALLOWS
THOSE
PER
A
B
FIGURE
CONTROLLING
UT
AND
LT
FOR
POWER
PERFORMANCE
TRADEOFFS
A
PRESENTS
THE
RE
SULTS
FOR
UT
LT
B
PRESENTS
THE
RESULTS
FOR
UT
LT
ISSUES
IN
IMPLEMENTING
DRPM
DISKS
HAVING
DEMONSTRATED
THE
POTENTIAL
OF
DRPM
IT
IS
IMPOR
TANT
TO
UNDERSTAND
SOME
OF
THE
RAMIFICATIONS
IN
ITS
PHYSICAL
REALIZATION
PROVIDING
SPEED
CONTROL
AS
MENTIONED
IN
SECTION
SPEED
CONTROL
IN
DC
BRUSH
LESS
PM
MOTORS
CAN
BE
ACHIEVED
USING
PWM
TECH
FORMANCE
NON
CRITICAL
OPERATIONS
TO
BE
UNDERTAKEN
AT
A
RELATIVELY
SLOW
RPM
FOR
ENERGY
SAVINGS
WHILE
TRA
DITIONAL
POWER
MODE
CONTROL
OF
TRANSITIONING
THE
DISK
COMPLETELY
TO
A
STANDBY
STATE
PREVENTS
SUCH
ACTIVITIES
SMART
DISK
CAPABILITIES
THE
ANTICIPATED
SMART
DISKS
PROVIDE
AN
EX
CELLENT
PLATFORM
FOR
IMPLEMENTING
DRPM
ALGORITHMS
AND
ALSO
PROVIDE
THE
FLEXIBILITY
OF
MODULATING
THE
ALGO
RITHM
PARAMETERS
OR
EVEN
CHANGING
THE
ALGORITHM
EN
TIRELY
DURING
THE
COURSE
OF
EXECUTION
THE
EFFECT
OF
RPM
MODULATION
ON
DISK
RELIABILITY
NEEDS
FURTHER
INVESTIGATION
ON
THE
ONE
HAND
WE
HAVE
BEEN
IN
CREASING
THE
NUMBER
OF
DISKS
IN
ARRAYS
TO
NOT
ONLY
ENHANCE
PERFORMANCE
BUT
ALSO
FOR
AVAILABILITY
THIS
IN
TURN
HAS
AC
CENTUATED
THE
POWER
PROBLEM
WHICH
THIS
PAPER
HAS
TRIED
TO
ADDRESS
IN
DOING
SO
IT
IS
CONCEIVABLE
THAT
WE
MAY
NEED
MORE
DISKS
FOR
HOT
SPARES
IN
CASE
RPM
MODULATION
CAN
WORSEN
MTTFS
THIS
VICIOUS
CYCLE
BETWEEN
PERFOR
MANCE
POWER
AND
AVAILABILITY
WARRANTS
A
FURTHER
INVESTIGA
TION
WHICH
WE
PLAN
TO
UNDERTAKE
IN
THE
FUTURE
CONCLUDING
REMARKS
THIS
PAPER
HAS
PRESENTED
A
NEW
APPROACH
TO
ADDRESS
THE
GROWING
POWER
PROBLEM
IN
LARGE
DISK
ARRAYS
INSTEAD
OF
COMPLETELY
SPINNING
DOWN
DISKS
WHICH
CAN
INCUR
SIGNIFI
CANT
TIME
AND
POWER
COSTS
THIS
PAPER
PROPOSES
TO
MODULATE
THE
RPM
OF
DISKS
DYNAMICALLY
THE
RESULTING
DRPM
MECH
ANISM
HAS
BEEN
SHOWN
TO
FIND
MORE
SCOPE
FOR
POWER
SAVINGS
WHEN
IDLE
TIMES
ARE
NOT
VERY
LONG
COMPARED
TO
TRADITIONAL
POWER
MANAGEMENT
TPM
TECHNIQUES
THAT
HAVE
BEEN
PRO
POSED
FOR
LAPTOP
DESKTOP
DISKS
IN
ADDITION
IT
ALSO
ALLOWS
THE
OPTION
OF
SERVICING
REQUESTS
AT
A
LOWER
RPM
WHEN
PER
FORMANCE
IS
NOT
VERY
CRITICAL
TO
PROVIDE
ADDITIONAL
POWER
SAVINGS
FINALLY
IT
CAN
BE
COMBINED
WITH
TPM
TECHNIQUES
TO
AMPLIFY
THE
POWER
SAVINGS
WE
HAVE
PROPOSED
TIMING
AND
POWER
MODELS
FOR
THE
DRPM
MECHANISM
AND
HAVE
CONDUCTED
A
SENSITIVITY
ANAL
YSIS
OF
DIFFERENT
HARDWARE
PARAMETERS
IN
ADDITION
WE
HAVE
PRESENTED
A
HEURISTIC
THAT
CAN
BE
USED
IN
PRACTICE
TO
BENE
FIT
FROM
THE
DRPM
MECHANISM
TO
ALLOW
TRADE
OFFS
BETWEEN
POWER
SAVINGS
AND
PERFORMANCE
BENEFITS
DETAILED
SIMULA
TIONS
HAVE
SHOWN
THAT
WE
CAN
GET
CONSIDERABLE
ENERGY
SAV
INGS
WITHOUT
SIGNIFICANTLY
COMPROMISING
ON
PERFORMANCE
IT
IS
TO
BE
NOTED
THAT
THE
HEURISTIC
PRESENTED
HERE
IS
ONE
SIMPLE
WAY
OF
USING
THE
DRPM
MECHANISM
THOUGH
IT
IS
CONCEIVABLE
THAT
ONE
CAN
OPTIMIZE
CHANGE
THIS
FURTHER
TO
GET
HIGHER
POWER
SAVINGS
OR
TO
LIMIT
THE
PERFORMANCE
DEGRADATION
THE
DESIGN
AND
ANALYSIS
OF
ALGORITHMS
REQUIRES
A
MODEL
OF
COMPUTATION
SUCH
A
MODEL
SHOULD
FAITHFULLY
REFLECT
THE
PHYS
ICAL
PROCESSES
OF
COMPUTATION
SO
THAT
A
PROGRAMMER
CAN
DIS
TINGUISH
EFFICIENT
COMPUTATIONS
FROM
INEFFICIENT
ONES
AT
THE
SAME
TIME
THE
MODEL
MUST
BE
SIMPLE
ENOUGH
TO
BE
TRACTABLE
AND
GENERAL
ENOUGH
TO
CONTINUE
TO
APPLY
AS
THE
UNDERLYING
TECHNOLOGIES
EVOLVE
FROM
A
COMPUTER
ARCHITECT
PERSPECTIVE
A
MODEL
OF
COMPUTATION
DESCRIBES
WHAT
THE
PROGRAMMER
EX
PECTS
AND
THEREBY
PROVIDES
CRITERIA
FOR
EVALUATING
ARCHITECTURAL
ALTERNATIVES
THE
MODELS
OF
COMPUTATION
THAT
ARE
PREVALENT
TODAY
ARE
BASED
ON
OPERATION
COUNTING
ASSUMING
SEQUENTIAL
PROGRAM
EXECU
TION
THESE
MODELS
REFLECT
THE
TECHNOLOGY
OF
THE
FIRST
SEVERAL
DECADES
OF
COMPUTING
MEMORY
ACCESSES
WERE
AS
FAST
AS
ALUS
SO
OPERATION
COUNT
DETERMINED
EXECUTION
TIME
GATES
WERE
EX
PENSIVE
BUT
WIRES
WERE
CHEAP
THE
MONETARY
COST
OF
COMPUTING
WAS
DETERMINED
BY
THE
HARDWARE
RATHER
THAN
THE
POWER
BILL
OVER
TIME
EACH
OF
THESE
ASSUMPTIONS
HAVE
BEEN
OVERTURNED
AND
YET
THE
MODELS
OF
COMPUTATION
HAVE
REMAINED
REMARKABLY
STABLE
THIS
HAS
LARGELY
BEEN
MADE
PRACTICAL
THROUGH
INNOVA
TIONS
IN
COMPUTER
ARCHITECTURE
FOR
EXAMPLE
CACHES
AND
SU
PERSCALAR
EXECUTION
HAVE
HIDDEN
THE
COST
OF
MEMORY
ACCESSES
NOW
THE
POWER
WALL
IS
FORCING
A
TRANSITION
TO
EXPLICITLY
PARALLEL
ARCHITECTURES
AND
SOFTWARE
AND
TRADITIONAL
MODELS
OF
COMPUTATION
NO
LONGER
REFLECT
THE
ACTUAL
COSTS
OF
COMPUTATION
PARALLEL
COMPUTING
OFFERS
A
WAY
AROUND
THE
POWER
WALL
BE
CAUSE
CMOS
TECHNOLOGY
ALLOWS
OPERATIONS
TO
BE
PERFORMED
WITH
LESS
ENERGY
BY
USING
MORE
TIME
THUS
A
PARALLEL
ALGO
RITHM
MAY
PERFORM
MORE
OPERATIONS
THAN
ITS
SEQUENTIAL
COUN
TERPART
YET
USE
LESS
TIME
AND
LESS
ENERGY
BY
COMBINING
VOLT
AGE
SCALING
CIRCUIT
DESIGN
TECHNIQUES
AND
MICRO
ARCHITECTURAL
TRADE
OFFS
ENERGY
AND
TIME
CAN
BE
TRADED
OVER
RANGES
OF
SEV
ERAL
ORDERS
OF
MAGNITUDE
WHEN
THESE
ENERGY
TIME
TRADE
OFFS
ARE
CONSIDERED
THE
OPTIMAL
ALGORITHM
FOR
A
TASK
MAY
BE
ONE
THAT
NEITHER
MINIMIZES
OPERATION
COUNT
NOR
COMPUTATION
DEPTH
WHILE
VARIOUS
MODELS
HAVE
BEEN
PROPOSED
FOR
PARALLEL
COMPU
TATION
SUCH
AS
PRAMS
AND
LOGP
CKP
WE
ARE
AWARE
OF
NO
PRIOR
MODEL
THAT
CAN
ADDRESS
THE
QUESTIONS
THAT
ARISE
FROM
THE
ENERGY
TIME
TRADE
OFFS
THAT
ARE
AT
THE
HEART
OF
CURRENT
PARALLEL
COMPUTING
TECHNOLOGIES
FOR
EXAMPLE
HOW
CAN
A
PROGRAMMER
DESIGN
ENERGY
TIME
OPTIMAL
AL
GORITHMS
ARE
THERE
FUNDAMENTAL
ADVANTAGES
TO
HETEROGENEOUS
AR
CHITECTURES
E
G
MIXING
FAST
AND
SLOW
CORES
WHAT
ON
CHIP
INTERCONNECT
TOPOLOGIES
ARE
REQUIRED
TO
RE
ALIZE
ENERGY
TIME
OPTIMAL
COMPUTATIONS
WHAT
ARE
THE
TRADE
OFFS
BETWEEN
LATENCY
THROUGHPUT
AND
POWER
CONSUMPTION
ADDRESSING
THESE
QUESTIONS
REQUIRES
AN
ENERGY
AWARE
MODEL
THE
ENERGY
TIME
TRADE
OFFS
AFFORDED
BY
CMOS
TECHNOLOGY
HAVE
BEEN
LONG
UNDERSTOOD
DG
AND
STUDIED
IN
TENSIVELY
BY
THE
REAL
TIME
SYSTEMS
COMMUNITY
SINCE
THE
SEMI
NAL
PAPER
BY
YAO
ET
AL
THIS
HAS
LED
TO
MANY
PAPERS
THAT
EXAMINE
ENERGY
TRADE
OFFS
FOR
SCHEDULING
PROBLEMS
FOR
UNIPROCESSORS
MULTIPROCESSORS
WITH
OR
WITHOUT
PRECEDENCE
CONSTRAINTS
ETC
E
G
IN
ALL
OF
THESE
PAPERS
THE
SET
OF
TASKS
TO
BE
PERFORMED
IS
TAKEN
AS
A
GIVEN
LIKEWISE
MARTIN
CONSIDERS
THE
ENERGY
TIME
TRADE
OFFS
OF
VARIOUS
CONCURRENT
DECOMPOSITIONS
FOR
AL
GORITHMS
BUT
DOES
NOT
EXAMINE
THE
ENERGY
REQUIREMENTS
OF
PARTICULAR
COMPUTATIONAL
TASKS
SUCH
AS
ADDITION
OR
SORTING
POSITION
STATEMENT
TO
MAKE
EFFECTIVE
USE
OF
PARALLELISM
PROGRAMMERS
AND
ARCHITECTS
MUST
HAVE
A
MODEL
OF
COMPUTA
TION
THAT
REFLECTS
THE
ENERGY
TIME
TRADE
OFFS
OF
CMOS
TECHNOL
OGY
WE
ARE
AWARE
OF
NO
PRIOR
RESEARCH
THAT
HAS
PRESENTED
A
MODEL
THAT
REFLECTS
THESE
TRADE
OFFS
IN
THE
REMAINDER
OF
THIS
PAPER
WE
PRESENT
SUCH
A
MODEL
WE
USE
THE
MODEL
TO
DE
RIVE
LOWER
BOUNDS
FOR
THE
COMPLEXITY
OF
ADDITION
MULTIPLICA
TION
AND
SORTING
WE
SHOW
EXISTING
ALGORITHMS
THAT
MEET
THESE
BOUNDS
TO
WITHIN
CONSTANT
FACTORS
AND
WE
CONSIDER
FUTURE
DI
RECTIONS
FOR
ENERGY
AWARE
ALGORITHM
DESIGN
AND
ANALYSIS
WE
PRESENT
SOME
SURPRISING
RESULTS
FOR
ENERGY
CONSTRAINED
MIN
IMAL
TIME
COMPUTATION
FOR
EXAMPLE
IF
THE
INPUTS
OF
A
SORT
ING
NETWORK
ARE
REQUIRED
TO
LIE
ALONG
A
LINE
THEN
SLOW
AL
GORITHMS
SUCH
AS
BUBBLE
SORT
AND
FAST
ALGORITHMS
SUCH
AS
ODD
EVEN
MERGE
SORT
HAVE
THE
SAME
ASYMPTOTIC
ENERGY
TIME
COMPLEXITY
OUR
CONSTRUCTION
FOR
AN
OPTIMAL
ADDER
SHOWS
THAT
BROADCASTING
A
BIT
TO
ALL
O
MESH
LOCATIONS
WITHIN
DISTANCE
D
OF
A
SOURCE
USES
THE
SAME
ENERGY
AND
TIME
TO
WITHIN
A
CON
STANT
FACTOR
AS
SENDING
THE
BIT
TO
A
SINGLE
LOCATION
DISTANCE
D
AWAY
THE
ALGORITHMS
THAT
WE
PRESENT
FOR
SORTING
AND
MULTI
PLICATION
ARE
ENERGY
TIME
OPTIMAL
YET
THEY
MINIMIZE
NEITHER
OPERATION
COUNT
NOR
COMPUTATION
DEPTH
AN
ENERGY
AWARE
MODEL
WE
PRESENT
TWO
VARIANTS
OF
OUR
MODEL
A
FAIRLY
ABSTRACT
LOWER
BOUND
MODEL
THAT
SIMPLIFIES
ANALYSIS
AND
A
MORE
DE
TAILED
UPPER
BOUND
THAT
ENSURES
THAT
THE
PROPOSED
ALGORITHMS
CAN
BE
IMPLEMENTED
IN
VLSI
TECHNOLOGY
IN
BOTH
MODELS
COMPUTATIONS
ARE
PERFORMED
BY
AN
ENSEMBLE
OF
PROCESSING
EL
EMENTS
PES
A
PE
HAS
O
INPUTS
O
OUTPUTS
AND
O
BITS
OF
STATE
A
PE
CAN
COMPUTE
AN
ARBITRARY
FUNCTION
OF
ITS
INPUTS
AND
STATE
IN
T
UNITS
OF
TIME
USING
T
UNITS
OF
ENERGY
IN
FIGURE
A
PERIMETER
I
O
IMPLEMENTATION
OF
AN
ADDER
OTHER
WORDS
ET
IS
CONSTANT
COMPUTATIONS
ARE
REPRE
SENTED
BY
TASK
GRAPHS
WITH
VERTICES
OF
THE
TASK
GRAPH
MAPPED
TO
PES
WHERE
VERTICES
REPRESENT
ATOMIC
OPERATIONS
THIS
IS
A
MANY
TO
ONE
MAPPING
A
PE
MAY
PERFORM
MULTIPLE
OPERATIONS
IN
AN
ALGORITHM
AS
LONG
AS
THE
PRECEDENCE
CONSTRAINTS
ENSURE
THAT
THESE
OPERATIONS
ARE
PERFORMED
DURING
DISJOINT
TIME
INTER
VALS
TO
MODEL
THE
PLANAR
GEOMETRY
OF
INTEGRATED
CIRCUITS
AND
PRINTED
CIRCUIT
BOARDS
THE
LOWER
BOUND
MODEL
RESTRICTS
A
PE
TO
HAVING
AT
MOST
OTHER
PES
WITHIN
DISTANCE
D
OF
ITSELF
AND
THE
TRIANGLE
INEQUALITY
APPLIES
TO
COMMUNICATE
A
BIT
BETWEEN
PES
SEPARATED
BY
DISTANCE
D
USING
TIME
T
REQUIRES
D
ENERGY
IN
OTHER
WORDS
ET
D
WHICH
MEANS
THAT
ONE
BIT
CAN
BE
SENT
D
UNITS
OF
DISTANCE
USING
D
UNITS
OF
TIME
AND
ENERGY
IN
THE
UPPER
BOUND
MODEL
EACH
PE
OCCUPIES
A
UNIT
SQUARE
AND
ONLY
NEAREST
NEIGHBOR
COMMUNICATION
IS
ALLOWED
THUS
WIRES
ARE
REALIZED
AS
CHAINS
OF
PES
CROSS
OVERS
CORNERS
ETC
CAN
BE
REALIZED
BY
A
PE
WITH
MULTIPLE
INPUTS
AND
MULTIPLE
OUT
PUTS
FURTHER
DETAILS
FOR
THESE
MODELS
ARE
PRESENTED
IN
AND
OMITTED
HERE
DUE
TO
SPACE
CONSTRAINTS
PRELIMINARY
RESULTS
WE
START
WITH
ADDITION
A
BINARY
ADDER
HAS
TWO
N
BIT
INPUT
WORDS
AND
ONE
N
BIT
OUTPUT
WORD
THAT
ENCODES
THE
SUM
AT
THE
BEGINNING
OF
THE
COMPUTATION
EACH
INPUT
BIT
IS
INITIALLY
HELD
BY
A
DISTINCT
PE
AT
A
PREDETERMINED
LOCATION
IN
THE
PLANE
AT
THE
END
OF
THE
COMPUTATION
EACH
OUTPUT
BIT
IS
HELD
BY
A
PE
AT
A
PREDETERMINED
LOCATION
WE
FIRST
CONSIDER
ADDERS
WHERE
THE
CENTERS
OF
THE
THE
IN
PUT
PES
LIE
ALONG
A
LINE
AND
LIKEWISE
FOR
THE
CENTERS
OF
THE
OUTPUT
PES
FIGURE
DEPICTS
ONE
SUCH
ADDER
WE
CALL
SUCH
LAYOUTS
PERIMETER
I
O
AND
NOTE
THAT
SUCH
IMPLEMENTATIONS
ARE
COMMON
IN
PRACTICE
BECAUSE
THE
CARRY
GENERATED
BY
THE
LEAST
SIGNIFICANT
BIT
CAN
AFFECT
ALL
BITS
OF
THE
RESULT
WE
CAN
USE
AN
ADDER
TO
BROADCAST
ONE
INPUT
BIT
TO
ALL
OF
THE
OUTPUT
BITS
THERE
MUST
BE
TWO
OUTPUT
PES
SEPARATED
BY
DISTANCE
N
BECAUSE
THE
OUTPUT
PES
LIE
ALONG
A
LINE
BY
THE
TRIANGLE
INEQUALITY
THERE
MUST
BE
AN
OUTPUT
PE
THAT
IS
AT
LEAST
DISTANCE
N
AWAY
FROM
THE
PE
HOLDING
FOR
THE
LEAST
SIGNIFICANT
BIT
LSB
OF
ONE
OF
THE
OPERAND
WORDS
SENDING
ONE
BIT
DISTANCE
D
REQUIRES
ENERGY
E
AND
TIME
T
WITH
ET
D
LETTING
E
DENOTE
THE
TOTAL
ENERGY
REQUIRED
TO
PERFORM
THE
ADDITION
AND
T
DENOTE
THE
TIME
WE
CONCLUDE
THAT
THE
ET
COMPLEXITY
FOR
PERIMETER
I
O
IM
PLEMENTATIONS
OF
ADDITION
IS
IN
N
THIS
BOUND
IS
TIGHT
IT
IS
READILY
ACHIEVED
BY
A
CARRY
RIPPLE
ADDER
OR
A
BRENT
KUNG
FIGURE
AN
H
TREE
IMPLEMENTATION
OF
AN
ADDER
CARRY
LOOKAHEAD
OBSERVING
THAT
A
CARRY
RIPPLE
AND
A
CARRY
LOOKAHEAD
ADDER
HAVE
THE
SAME
ASYMPTOTIC
COM
PLEXITY
IN
OUR
MODEL
WE
SEE
THAT
WHEN
COMMUNICATION
TIME
AND
ENERGY
ARE
TAKEN
INTO
ACCOUNT
THE
N
LOG
N
TIME
ADVANTAGE
OF
A
FAST
ADDER
IS
REDUCED
TO
A
CONSTANT
FACTOR
MORE
EFFICIENT
ADDERS
CAN
BE
ACHIEVED
BY
ALLOWING
THE
IN
PUT
AND
OUTPUT
PES
TO
OCCUPY
ARBITRARY
LOCATIONS
IN
THE
PLANE
WE
WILL
REFER
TO
SUCH
IMPLEMENTATIONS
AS
HAVING
PLANAR
I
O
FOLLOWING
AN
ARGUMENT
ANALOGOUS
TO
THAT
FOR
THE
PERIMETER
I
O
ADDER
AN
ADDER
WITH
PLANAR
I
O
MUST
HAVE
SOME
OUTPUT
PE
THAT
IS
AT
LEAST
DISTANCE
N
AWAY
FROM
THE
PE
FOR
THE
LSB
OF
ONE
OF
THE
OPERAND
WORDS
THE
ET
COST
OF
SENDING
THE
INPUT
BIT
TO
THAT
OUTPUT
PE
IS
IN
N
N
TO
OBTAIN
THE
MATCHING
UPPER
BOUND
WE
CONSIDER
A
CARRY
LOOK
AHEAD
ADDER
ORGANIZED
AS
AN
H
TREE
AS
SHOWN
IN
FIGURE
LET
THE
ROOT
OF
THE
TREE
BE
AT
LEVEL
AND
THE
LEAVES
AT
LEVEL
M
FOR
K
AN
EDGE
BETWEEN
LEVELS
K
AND
K
IS
IMPLEMENTED
BY
A
CHAIN
OF
LENGTH
M
K
PES
WE
SET
THE
TIME
FOR
PES
AT
LEVEL
K
AND
THE
PES
IN
THE
CHAIN
FROM
LEVEL
K
TO
LEVEL
K
TO
GRINDING
OUT
THE
SUMS
YIELDS
E
T
O
N
AND
THUS
ET
O
N
MATCHING
THE
LOWER
BOUND
THE
LOWER
AND
UPPER
BOUND
RESULTS
TOGETHER
SHOW
THAT
IT
IS
POSSIBLE
TO
BROADCAST
A
BIT
TO
ALL
PES
WITHIN
DISTANCE
D
OF
A
SOURCE
FOR
A
CONSTANT
FACTOR
OF
THE
COST
OF
SENDING
A
BIT
TO
A
SINGLE
PE
AT
DISTANCE
D
IT
IS
STRAIGHTFORWARD
TO
SHOW
A
COM
PLEXITY
FOR
ADDITION
OF
ET
N
IF
ALL
PES
ARE
RE
QUIRED
TO
OPERATE
WITH
THE
SAME
COMPUTATION
TIME
THUS
FOR
ADDITION
A
HETEROGENEOUS
ARCHITECTURE
IS
ASYMPTOTICALLY
SUPE
RIOR
TO
AN
HOMOGENEOUS
ONE
WE
NOTE
THAT
THE
COMMUNICATION
PATTERN
FOR
ADDITION
IS
THE
SAME
AS
THAT
FOR
THE
MAP
REDUCE
DWARF
ABC
THIS
SUGGESTS
THAT
FOR
SOME
COMPUTATIONS
HETEROGENEITY
MAY
PROVIDE
A
SUBSTANTIAL
COMPUTATIONAL
ADVAN
TAGE
THE
MULTIPLICATION
PROBLEM
CAN
BE
FORMULATED
IN
A
SIMI
LAR
MANNER
AS
ADDITION
WE
HAVE
SHOWN
THAT
PERIME
TER
I
O
MULTIPLIERS
HAVE
AN
ET
COMPLEXITY
IN
O
N
AND
WE
FOCUS
HERE
ON
THE
PLANAR
I
O
CASE
FOLLOWING
THE
CLASSICAL
ARGUMENTS
FOR
THE
AT
COMPLEXITY
OF
MULTIPLICATION
SHIFTING
CAN
BE
REDUCED
TO
MULTIPLICATION
FOR
EACH
INPUT
BIT
THERE
ARE
AT
LEAST
OUTPUT
BITS
THAT
ARE
AT
LEAST
DISTANCE
N
AWAY
A
PIGEON
HOLE
ARGUMENT
SHOWS
THAT
THERE
MUST
BE
A
SHIFT
FOR
WHICH
AT
LEAST
INPUT
BITS
MUST
BE
SENT
A
DIS
TANCE
OF
AT
LEAST
N
THIS
YIELDS
A
LOWER
BOUND
OF
ET
N
N
THIS
BOUND
IS
ACHIEVED
BY
PREPARATA
AT
OPTIMAL
MULTIPLIER
AS
VIEWED
IN
OUR
SCALING
OR
HETEROGENEITY
OF
PES
IS
A
REQUIREMENT
FOR
THE
BRENT
KUNG
DESIGN
TO
MEET
THIS
BOUND
THIS
IS
DUE
TO
THE
FEW
LONG
WIRES
PER
STAGE
THAT
MUST
RUN
AT
A
FASTER
SPEED
PROBLEM
ALGORITHM
IMPLEMENTATION
ET
ADDITION
CARRY
RIPPLE
P
O
N
BRENT
KUNG
P
O
N
KOGGE
STONE
P
O
N
CARRY
SELECT
O
N
H
TREE
O
N
MULTIPLICATION
CARRY
SAVE
P
O
N
PREPARATA
O
N
SORTING
BUBBLE
SORT
P
O
N
ODD
EVEN
MERGE
SORT
P
O
N
SCHNORR
AND
SHAMIR
O
N
ALGORITHMS
MARKED
P
ARE
IMPLEMENTED
WITH
PERIMETER
I
O
THE
OTHER
ALGORITHMS
HAVE
INPUT
AND
OUTPUT
PES
PLACED
THROUGHOUT
THE
IMPLEMENTATION
TABLE
SUMMARY
OF
ET
COMPLEXITIES
MODEL
WE
DISCUSS
THE
RELATIONSHIP
BETWEEN
THE
AT
AND
OUR
MODEL
IN
MORE
DETAIL
IN
SECTION
WE
NOW
CONSIDER
THE
PROBLEM
OF
SORTING
N
BINARY
WORDS
OF
W
BITS
EACH
AS
FOR
MULTIPLICATION
AND
ADDITION
WE
ASSUME
THAT
EACH
INPUT
BIT
IS
STORED
IN
A
SEPARATE
PE
AT
A
PREDETER
MINED
LOCATION
AT
THE
BEGINNING
OF
THE
ALGORITHM
AND
THAT
AT
THE
END
EACH
OUTPUT
BIT
IS
STORED
IN
A
PREDETERMINED
PE
FOR
SORTING
WE
ALSO
ASSUME
THAT
THE
BITS
FOR
ANY
GIVEN
INPUT
OR
OUTPUT
WORD
ARE
STORED
IN
CONTIGUOUS
PES
IF
W
N
THEN
WE
CAN
CONSTRUCT
A
PERMUTATION
OF
THE
INPUT
DATA
THAT
FORCES
THE
ITH
INPUT
WORD
TO
GO
TO
PES
THAT
ARE
DISTANCE
N
I
W
AWAY
THIS
LEADS
TO
A
LOWER
BOUND
OF
ET
NW
A
MATCHING
UPPER
BOUND
IS
ACHIEVED
BY
A
VARIATION
ON
SCHNORR
AND
SHAMIR
MESH
SORTING
ALGORITHM
SCHNORR
AND
SHAMIR
ALGORITHM
SORTS
N
WORDS
ON
AN
ARRAY
OF
N
N
WORD
WISE
COMPARE
AND
SWAP
MODULES
IN
O
N
TIME
USING
ONLY
NEAREST
NEIGHBOR
COMMUNICATION
BECAUSE
THEIR
COMPARE
AND
SWAP
MODULE
CAN
WORK
ON
EITHER
VERTICALLY
OR
HORIZONTALLY
ADJACENT
WORDS
IN
UNIT
TIME
A
W
BIT
VERSION
MUST
HAVE
W
HOR
IZONTAL
AND
W
VERTICAL
WIRES
AND
OCCUPY
AREA
O
THIS
PREVENTS
THEIR
ALGORITHM
FROM
ACHIEVING
THE
NW
LOWER
BOUND
WE
DERIVED
ABOVE
WE
REPLACE
EACH
COMPARE
AND
SWAP
MODULE
WITH
A
TILE
OF
W
W
PES
AND
ONLY
USE
NEAREST
NEIGHBOR
COMMUNICATION
OUR
IMPLEMENTATION
TAKES
O
NW
TIME
AND
EACH
STEP
REQUIRES
O
NW
ENERGY
THIS
ESTABLISHES
ET
O
NW
AND
MATCHES
THE
LOWER
BOUND
TABLE
SUMMARIZES
ET
COMPLEXITIES
THAT
WE
HAVE
DERIVED
FOR
THE
PROBLEMS
OF
ADDITION
MULTIPLICATION
AND
SORTING
DUE
TO
LENGTH
LIMITATIONS
WE
HAVE
NOT
PRESENTED
ALL
OF
THESE
ALGO
RITHMS
HERE
DETAILS
ARE
GIVEN
IN
WHAT
NEXT
WE
HAVE
PRESENTED
A
SIMPLE
MODEL
THAT
ACCOUNTS
FOR
ENERGY
TIME
TRADE
OFFS
IN
COMPUTATION
AND
USED
IT
TO
ANALYZE
ADDI
TION
MULTIPLICATION
AND
SORTING
WE
NOTE
THAT
ALL
THREE
OF
THESE
PROBLEMS
ARE
HIGHLY
AMENABLE
FOR
PARALLEL
IMPLEMENTATIONS
FOR
EXAMPLE
ADDITION
WITH
AN
ET
COMPLEXITY
IN
N
CAN
HAVE
BOTH
THE
TIME
AND
THE
ENERGY
FOR
THE
COMPUTATION
GROW
SLOWER
THAN
THE
INPUT
SIZE
AS
BOTH
CAN
GROW
AS
N
LIKE
WISE
FOR
BOTH
MULTIPLICATION
AND
SORTING
CAN
HAVE
EN
ERGY
AND
TIME
BOTH
GROW
AT
A
RATE
THAT
IS
SUBLINEAR
IN
THE
PROB
LEM
SIZE
OUR
MODEL
IS
REMINISCENT
OF
THE
AT
MODELS
FOR
VLSI
COM
PLEXITY
THAT
WERE
STUDIED
IN
THE
IN
FACT
IF
AN
ALGORITHM
USES
ONLY
NEAREST
NEIGH
BOR
COMMUNICATION
AND
ACHIEVES
AN
AT
O
F
N
THEN
IT
ACHIEVES
AN
ET
O
F
N
AS
WELL
IF
EASILY
ADAPTED
TO
OUR
MODEL
THE
TWO
MODELS
HOWEVER
ARE
NOT
EQUIVALENT
NOTICE
THAT
THERE
IS
NO
SENSE
OF
TRADING
AREA
FOR
TIME
AT
THE
PE
LEVEL
WHERE
AS
OUR
MODEL
SUPPORTS
ENERGY
TIME
EXCHANGES
TO
VARY
BOTH
SPATIALLY
AND
TEMPORALLY
ACROSS
OTHERWISE
IDENTICAL
PES
AT
BOUNDS
ARE
TYPICALLY
BASED
ON
CROSS
SECTIONAL
BAND
WIDTH
REQUIREMENTS
WHILE
ET
BOUNDS
ARE
BASED
ON
THE
SPEED
TIME
DISTANCE
THAT
DATA
MUST
MOVE
THIS
HAS
SIGNIFICANT
IM
PLICATIONS
FOR
EXAMPLE
IF
WE
CONSIDER
THROUGHPUT
INSTEAD
OF
LATENCY
WE
CAN
EXAMINE
AP
AND
EP
WHERE
P
IS
THE
PERIOD
OF
THE
COMPUTATION
WE
NOTE
THAT
AP
BOUNDS
GENERALLY
MATCH
THEIR
AT
COUNTERPARTS
BECAUSE
INCREASING
LATENCY
WITH
CONSTANT
THROUGHPUT
INCREASES
BOTH
THE
AMOUNT
OF
DATA
TO
MOVE
AND
THE
AMOUNT
OF
TIME
TO
MOVE
IT
THE
REQUIRED
BANDWIDTH
IS
UN
CHANGED
THUS
AT
BOUNDS
CANNOT
MODEL
THE
ADVANTAGES
OF
PIPELINING
EP
BOUNDS
ON
THE
OTHER
HAND
CAN
BE
LOWER
THAN
THEIR
ET
COUNTERPARTS
BECAUSE
DATA
CAN
MOVE
SLOWER
IF
IT
HAS
MORE
TIME
TO
REACH
ITS
DESTINATION
FOR
EXAMPLE
A
PIPELINED
ADD
PASS
MULTIPLIER
ACHIEVES
AN
EP
O
EVEN
WITH
PE
RIMETER
I
O
WHICH
IS
LOWER
THAN
THE
O
N
BOUND
FOR
THE
ET
COMPLEXITY
THE
TRADE
OFFS
BETWEEN
ENERGY
THROUGHPUT
AND
LATENCY
MERIT
FURTHER
INVESTIGATION
AS
SKETCHED
ABOVE
A
DEEPLY
PIPELINED
MULTIPLIER
CAN
USE
LESS
ENERGY
PER
OPERATION
THAN
A
LOW
LATENCY
DESIGN
FOR
THE
SAME
THROUGHPUT
MANY
BUT
NOT
ALL
NUMERI
CAL
ALGORITHMS
ARE
HIGHLY
TOLERANT
OF
LATENCY
IN
THE
FLOATING
POINT
UNIT
CAN
WE
EXPLOIT
THIS
BY
BUILDING
CHIPS
WITH
MANY
DEEPLY
PIPELINED
FLOATING
POINT
UNITS
AND
A
FEW
LOW
LATENCY
ONES
LIKEWISE
WE
HAVE
SHOWN
THAT
THE
MINIMUM
ET
COM
PLEXITIES
FOR
ADDITION
MULTIPLICATION
AND
SORTING
REQUIRE
PLANAR
I
O
BUT
THE
OBSERVATION
ABOUT
MULTIPLICATION
SUGGESTS
THAT
PE
RIMETER
I
O
IMPLEMENTATIONS
MAY
BE
ADEQUATELY
OPTIMALLY
EFFICIENT
IN
AN
EP
MODEL
THIS
WOULD
BE
GOOD
NEWS
FOR
ARCHI
TECTS
ENERGY
TIME
OPTIMAL
IMPLEMENTATIONS
MAY
BE
POSSIBLE
WHERE
THE
FUNCTIONAL
UNITS
FOR
PRIMITIVE
OPERATIONS
ARE
IMPLE
MENTED
WITH
PERIMETER
I
O
WHILE
MESHES
OF
CORES
MAY
PROVIDE
PLANAR
I
O
FOR
LARGER
ALGORITHMS
FURTHERMORE
BY
DISTINGUISH
ING
LATENCY
AND
THROUGHPUT
WE
CAN
ALSO
CONSIDER
STREAMING
COMPUTATIONS
A
MODEL
LIKE
THE
ONE
WE
HAVE
PRESENTED
HERE
COULD
HELP
ARCHITECTS
AND
ALGORITHM
DESIGNERS
EXPLORE
TRADE
OFFS
SUCH
AS
THESE
WE
HAVE
PRESENTED
OUR
MODEL
WITH
PES
DEFINED
AT
THE
BIT
LEVEL
TO
ENSURE
THAT
OUR
MODEL
ACCOUNTS
FOR
ALL
COSTS
IN
AN
IM
PLEMENTATION
OF
AN
ALGORITHM
HOWEVER
THIS
APPROACH
HAS
THE
SIDE
EFFECT
OF
FORCING
ALL
ANALYSIS
TO
THE
BIT
LEVEL
WHILE
WE
HAVE
ESTABLISHED
BOUNDS
FOR
THE
THREE
PROBLEMS
THAT
WE
CON
SIDERED
ESTABLISHING
LOWER
BOUNDS
FOR
BIT
LEVEL
COMPLEXITY
IS
KNOWN
TO
BE
A
HARD
PROBLEM
THUS
WE
PLAN
TO
EXTEND
OUR
MODEL
TO
ALLOW
PES
THAT
PERFORM
SIMPLE
OPERATIONS
ON
WORDS
OUR
ANALYSIS
OF
ADDITION
AND
MULTIPLICATION
PROVIDES
A
BASIS
FOR
SUCH
AN
EXTENSION
WITH
A
WORD
BASED
MODEL
WE
PLAN
TO
EXAMINE
COMMON
NUMERICAL
TASKS
SUCH
AS
MATRIX
MULTIPLICA
TION
AND
SOLVING
LINEAR
SYSTEMS
AS
WELL
AS
MORE
COMBINATORIAL
PROBLEMS
SUCH
AS
GRAPH
ALGORITHMS
AND
SEARCH
PROBLEMS
POWER
DISSIPATION
IS
THE
MOST
CRITICAL
BOTTLENECK
FOR
COM
PUTER
SYSTEM
PERFORMANCE
EXISTING
ALGORITHM
DESIGN
AND
ANAL
YSIS
IS
BASED
ON
OBSOLETE
MODELS
THAT
IGNORE
THE
UNDERLYING
TRADE
OFFS
BETWEEN
ENERGY
AND
TIME
THUS
NEW
MODELS
ARE
REQUIRED
BEFORE
WE
CAN
EVEN
DESCRIBE
THE
TRADE
OFFS
INVOLVED
A
KEY
OBJECTIVE
OF
TODAY
MULTICORE
PROCESSOR
DESIGNS
IS
TO
INCREASE
PERFORMANCE
WITHOUT
A
PROPORTIONAL
INCREASE
IN
ENERGY
CONSUMPTION
SINGLE
CORE
PROCESSORS
BECAME
OVERLY
COM
PLEX
AND
THEIR
PER
WATT
PERFORMANCE
DETERIORATED
MULTICORE
PROCESSORS
TRY
TO
AMELIORATE
THE
PROBLEM
BY
SPEEDING
UP
A
WORKLOAD
WITH
PARALLEL
PRO
CESSING
AT
A
LOWER
CLOCK
FREQUENCY
AND
LOWER
VOLTAGE
BUT
WHAT
IS
THE
PRECISE
RELATIONSHIP
BETWEEN
PARALLEL
PROCESSING
AND
ENERGY
CONSUMPTION
IN
INTEL
SHEKHAR
BORKAR
SUGGESTED
THAT
A
PERFECT
TWO
WAY
PARALLELIZATION
WOULD
LEAD
TO
HALF
THE
CLOCK
FRE
QUENCY
AND
VOLTAGE
ONE
QUARTER
THE
ENERGY
CONSUMPTION
AND
ONE
EIGHTH
THE
POWER
DENSITY
WHEN
COMPARED
WITH
SEQUENTIAL
EXECUTION
OF
THE
SAME
PROGRAM
IN
THE
SAME
EXECU
TION
TIME
SEE
MICROARCHITECTURE
AND
DESIGN
CHALLENGES
FOR
GIGASCALE
INTEGRATION
IN
THIS
ARTICLE
WE
EXPLORE
HOW
MUCH
ENERGY
SAVINGS
IS
POSSIBLE
WITH
PARALLEL
PROCESSING
IF
PROCESSORS
CAN
DYNAMI
CALLY
CHANGE
THEIR
VOLTAGE
AND
FREQUENCY
THIS
ARTICLE
IS
BASED
ON
OUR
PAPER
COROLLARIES
TO
AMDAHL
LAW
FOR
ENERGY
FROM
IEEE
COMPUTER
ARCHITECTURE
LETTERS
IN
JANUARY
WE
WILL
ADDRESS
SEVERAL
QUESTIONS
WHAT
IS
THE
MAXI
MUM
ENERGY
IMPROVEMENT
TO
BE
GAINED
FROM
PARALLELIZA
TION
HOW
CAN
WE
DETERMINE
THE
PROCESSOR
SPEED
TO
ACHIEVE
THAT
IMPROVEMENT
HOW
DOES
STATIC
POWER
AFFECT
THE
ENERGY
OPTIMAL
PROGRAM
SPEEDUP
AND
ENERGY
CONSUMPTION
GIVEN
A
TARGET
SPEEDUP
HOW
DO
WE
SET
THE
PROCESSOR
SPEEDS
TO
MIN
IMIZE
ENERGY
OUR
EXPLORATION
USES
THE
SAME
SIMPLE
APPLI
CATION
MODEL
AS
THE
WELL
KNOWN
AMDAHL
LAW
PARALLEL
APPLICATIONS
HAVING
A
SERIAL
SECTION
AND
A
PARALLEL
SECTION
WHOSE
RATIO
IS
KNOWN
REVIEWING
AMDAHL
LAW
AMDAHL
LAW
PROVIDES
A
SIMPLE
YET
EXTREMELY
USEFUL
METHOD
FOR
PREDICTING
THE
POTENTIAL
PERFORMANCE
OF
A
PAR
ALLEL
COMPUTER
GIVEN
THE
RATIO
OF
THE
SERIAL
AND
PARALLEL
WORK
IN
A
PROGRAM
AND
THE
NUMBER
OF
PROCESSORS
AVAILABLE
IT
HAS
BEEN
WIDELY
APPLIED
IN
DETERMINING
SPEEDUPS
EVEN
FOR
SIN
GLE
PROCESSOR
SYSTEMS
IT
ALSO
KNOWN
AS
THE
LAW
OF
DIMIN
ISHING
RETURNS
THE
FOLLOWING
EQUATION
SUCCINCTLY
DESCRIBES
AMDAHL
LAW
WHERE
IS
THE
RATIO
OF
THE
SERIAL
PARALLEL
WORK
IN
THE
PROGRAM
AND
IS
THE
NUMBER
OF
PROCESSORS
WE
BEGIN
WITH
THE
SAME
INPUT
PARAMETERS
AS
IN
AMDAHL
LAW
NAMELY
THEN
WE
DERIVE
THE
MINIMUM
ENERGY
CONSUMPTION
ONE
WOULD
GET
WITH
OPTIMAL
FREQUENCY
ALLOCA
TION
TO
THE
SERIAL
AND
PARALLEL
REGIONS
IN
A
PROGRAM
WHILE
THE
EXECUTION
TIME
IS
UNCHANGED
WE
OBTAIN
WHEN
THE
DYNAMIC
POWER
CONSUMPTION
OF
A
PROCESSOR
RUN
NING
AT
CLOCK
FREQUENCY
IS
PROPORTIONAL
TO
IN
LITERA
TURE
DESCRIBING
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
IS
ENERGY
COROLLARIES
TO
AMDAHL
LAW
WE
ALSO
ASSUME
THAT
THE
DYNAMIC
POWER
CON
SUMPTION
OF
A
PROCESSOR
RUNNING
AT
IS
NOR
MALIZED
TO
AND
THAT
STATIC
POWER
CONSUMPTION
IS
THAT
IS
THE
RATIO
OF
STATIC
POWER
TO
DYNAMIC
POWER
AT
IS
OUR
SIMPLE
ASSUMPTION
ABOUT
STATIC
POWER
CONSUMPTION
ALLOWS
US
TO
REVEAL
ITS
EFFECT
IN
CLOSED
FORM
DERIVATIONS
CLOCK
FREQUENCIES
FOR
THE
TWO
REGIONS
IN
THE
WORK
NAMELY
AND
ARE
CALCULATED
AS
FOLLOWS
FIGURE
ACHIEVABLE
DYNAMIC
ENERGY
IMPROVEMENT
ASSUMING
AND
USING
AND
PROCESSORS
GIVEN
THE
RATIO
OF
SERIAL
AND
PARALLEL
WORK
IN
A
PROGRAM
IN
THESE
EQUATIONS
WE
ASSUME
FOR
SIMPLIC
ITY
THAT
THE
EXECUTION
TIME
OF
A
PROGRAM
REGION
IS
DETERMINED
BY
THE
AMOUNT
OF
WORK
E
G
AND
THE
PROCESSOR
SPEED
E
G
IN
REALITY
SOME
PROGRAM
OPERATIONS
SUCH
AS
MAIN
MEMORY
ACCESS
HAVE
LATENCIES
UNRELATED
TO
THE
PROCESSOR
SPEED
BETWEEN
AND
TYPICALLY
EQUATION
SUGGESTS
THAT
MORE
PARALLELISM
LARGER
AND
MORE
PROCESSORS
LARGER
HELP
REDUCE
ENERGY
CONSUMPTION
FIGURE
IS
A
PLOT
OF
EQUATION
FORMULATING
THE
PROBLEM
FOR
THE
PURPOSES
OF
THESE
CALCULATIONS
WE
ASSUME
THAT
PROCESSORS
CAN
RUN
AT
ARBITRARY
CLOCK
FREQUENCIES
SUBJECT
TO
A
MAXIMUM
FREQUENCY
USING
AMDAHL
LAW
IN
EQUATION
AS
A
BASIS
THE
SPEEDUP
ONE
WOULD
ACHIEVE
WITH
PARAL
LELIZATION
AND
FREQUENCY
SCALING
IS
SUBJECT
TO
THE
FOLLOWING
FOR
THE
SAKE
OF
SIMPLICITY
WE
NORMALIZE
THE
SEQUENTIAL
EXECUTION
TIME
OF
THE
PROGRAM
AS
SIMILARLY
WE
NORMALIZE
THE
AMOUNT
OF
WORK
I
E
NUMBER
OF
CYCLES
IN
THE
PROGRAM
AS
THEREFORE
THE
MAXIMUM
CLOCK
FREQUENCY
HAS
A
RELA
TIVE
SPEED
OF
THE
AMOUNT
OF
WORK
IN
THE
SERIAL
PORTION
OF
THE
PROGRAM
IS
REPRESENTED
BY
AND
THE
PARALLEL
PORTION
BY
OR
FIGURE
SHOWS
THIS
ARRANGEMENT
FIGURE
NORMALIZED
WORK
AND
TIME
THE
PARALLEL
TIME
IS
PARTI
TIONED
INTO
SERIAL
AND
PARALLEL
REGIONS
THE
TIME
FOR
THE
SERIAL
REGION
IS
AND
THE
TIME
FOR
THE
PARALLEL
REGION
IS
THE
PARALLEL
TIME
LESS
THE
TIME
FOR
THE
SERIAL
REGION
FOR
A
GIVEN
PROBLEM
IS
FIXED
AND
FOR
A
GIVEN
ARCHI
TECTURE
AND
ARE
FIXED
HENCE
THE
ENERGY
CONSUMPTION
IS
A
FUNCTION
OF
AND
SPECIFICALLY
IN
EQUATION
THE
THREE
TERMS
REPRESENT
ENERGY
FOR
THE
SERIAL
PORTION
ENERGY
FOR
THE
PARALLEL
PORTION
AND
ENERGY
FOR
STATIC
POWER
CONSUMPTION
DURING
THE
WHOLE
EXECUTION
TIME
RESPECTIVELY
WE
ASSUME
THAT
DYNAMIC
POWER
CONSUMPTION
OF
A
PROCESSOR
RUNNING
AT
IS
WE
DO
NOT
CONSIDER
THE
PROCES
SOR
TEMPERATURE
AS
A
FACTOR
HENCE
THE
TERM
FOR
STATIC
ENERGY
IS
THE
PRODUCT
OF
THE
PER
PROCESSOR
POWER
CONSUMPTION
RATE
THE
NUMBER
OF
PROCESSORS
AND
THE
TOTAL
EXECUTION
TIME
ENERGY
IMPROVEMENT
WITH
PARALLELIZATION
LET
CONSIDER
THE
QUESTION
OF
MAXIMUM
ENERGY
IMPROVEMENT
WITH
PARALLEL
PROCESSING
AND
WHICH
CLOCK
FREQUENCY
ACHIEVES
IT
WE
START
WITH
A
SPECIAL
CASE
THE
PROBLEM
OF
OBTAINING
THE
MINIMUM
ENERGY
CONSUMPTION
WHEN
IS
THAT
IS
THE
PRO
GRAM
EXECUTION
TIME
IS
IDENTICAL
TO
THAT
OF
SEQUENTIAL
EXECU
TION
AT
THE
MAXIMUM
PROCESSOR
SPEED
THE
CONDITION
IS
SIMILAR
TO
SETTING
A
DEADLINE
SEQUENTIAL
EXECUTION
TIME
BY
WHICH
TO
FINISH
THE
COMPUTATION
OF
COURSE
ONE
MAYGET
LARGER
ENERGY
SAVINGS
BY
FURTHER
DECREASING
WITH
THE
CONDITION
WE
CAN
REWRITE
EQUATION
AS
FROM
EQUATION
WE
CAN
DERIVE
THE
VALUE
OF
THAT
MINIMIZES
ENERGY
CONSUMPTION
BY
SETTING
TO
WE
GET
ENERGY
COROLLARIES
TO
AMDAHL
LAW
NOW
WE
OBTAIN
THE
VALUES
OF
AND
TO
MINIMIZE
USING
EQUATIONS
AND
SPECIFICALLY
BOTH
AND
ARE
A
FUNCTION
OF
AND
IN
EQUA
TIONS
AND
EQUATION
SHOWS
THE
RELATIONSHIP
BETWEEN
AND
WHEN
IS
MINIMIZED
INTERESTINGLY
THE
RATIO
BETWEEN
THE
TWO
FREQUENCIES
IS
A
FUNCTION
OF
BUT
NOT
OF
EQUATION
SUGGESTS
THAT
TO
ACHIEVE
MINIMUM
ENERGY
CONSUMPTION
WE
NEED
TO
SLOW
THE
CLOCK
FREQUENCY
IN
THE
PARALLEL
REGION
OF
THE
PROGRAM
BY
COMPARED
WITH
THE
FREQUENCY
IN
THE
SERIAL
REGION
FIGURE
ILLUSTRATES
THIS
RELATIONSHIP
FINALLY
FROM
EQUATIONS
AND
WE
OBTAIN
THE
MIN
IMUM
ENERGY
CONSUMPTION
HERE
THE
FIRST
TERM
SHOWS
DYNAMIC
ENERGY
CONSUMP
TION
AND
THE
SECOND
TERM
EXPRESSES
STATIC
ENERGY
CONSUMP
TION
EQUATION
IS
SIMPLY
TAKEN
FROM
EQUATION
FIGURE
DEPICTS
THE
MAXIMUM
ENERGY
IMPROVEMENT
OWING
TO
PAR
ALLELIZATION
WHEN
THE
NUMBER
OF
PROCESSORS
VARIES
FROM
TO
AND
IT
CLEAR
THAT
ENERGY
IMPROVEMENT
IS
A
FUNCTION
MONOTONICALLY
INCREASING
WITH
AND
FIGURE
SHOWS
HOW
THE
OVERALL
ENERGY
CHANGES
AS
WE
ADJUST
IT
ALSO
PRESENTS
THE
VALUE
OF
THAT
MINIMIZES
NOTE
THAT
THE
OPTIMAL
SOLUTION
OBTAINED
FOR
AND
IS
FEASIBLE
BECAUSE
BOTH
CLOCK
FREQUENCIES
ARE
LESS
THAN
THE
PROCESSOR
MAXIMUM
FREQUENCY
DETERMINING
THE
EFFECT
OF
STATIC
POWER
AMDAHL
LAW
EXPLORES
THE
EFFECT
OF
PARALLELIZATION
ON
SPEEDUP
AND
WE
HAVE
DESCRIBED
THE
EFFECT
OF
PARALLELIZATION
ON
ENERGY
CONSUMPTION
WHEN
THE
PROGRAM
EXECUTION
TIME
IS
UNCHANGED
I
E
HOWEVER
DEPENDING
ON
THE
RATE
OF
STATIC
POWER
CONSUMPTION
THE
MINIMUM
AMOUNT
OF
TOTAL
ENERGY
CONSUMPTION
MAY
NOT
OCCUR
AT
IF
STATIC
POWER
CONSUMPTION
IS
HIGH
THE
PROCESSOR
WILL
USE
THE
LEAST
ENERGY
AT
A
HIGHER
CLOCK
FREQUENCY
POSSIBLY
RESULTING
IN
ON
THE
OTHER
HAND
IF
STATIC
POWER
CONSUMPTION
IS
LOW
THE
PROCESSOR
WILL
USE
THE
MINIMUM
ENERGY
AT
A
SLOWER
CLOCK
FRE
QUENCY
LET
REVISIT
THE
PROBLEM
OF
MINIMIZING
TOTAL
ENERGY
CONSUMPTION
WITHOUT
RESTRICTING
FOR
THIS
WE
SET
THE
DERIV
ATIVES
OF
EQUATION
WITH
RESPECT
TO
BOTH
AND
TO
ZERO
WE
OBTAIN
THE
FOLLOWING
FIGURE
DYNAMIC
ENERGY
CONSUMPTION
VS
SERIAL
TIME
FOR
TWO
CASES
AND
WHEN
THE
BOUND
OF
IS
MARKED
WITH
X
WHEN
AND
O
WHEN
THE
MINIMUM
ENERGY
POINT
IN
EACH
CURVE
AT
IS
MARKED
WITH
A
FILLED
RECTANGLE
WITH
AND
WE
CAN
USE
EQUATIONS
AND
TO
CAL
CULATE
THE
OPTIMUM
FREQUENCIES
FROM
WHICH
WE
CAN
COMPUTE
THE
MINIMUM
ENERGY
AN
INTER
ESTING
OBSERVATION
IS
THAT
AT
AND
THE
DYNAMIC
ENERGY
IS
GIVEN
BY
THE
FOLLOWING
WHICH
IS
EQUAL
TO
OF
THE
STATIC
ENERGY
IN
OTHER
WORDS
TOTAL
ENERGY
CONSUMPTION
IS
MINIMIZED
WHEN
THE
DYNAMIC
ENERGY
CONSUMPTION
IS
TIMES
THE
STATIC
ENERGY
CONSUMPTION
THIS
RELATION
HOLDS
DURING
THE
EXECUTION
OF
BOTH
THE
SERIAL
AND
PARALLEL
SECTIONS
OF
THE
PROGRAM
THEABOVE
SOLUTION
ISAPPLICABLE
ONLYIF
BOTH
AND
ARE
LESS
THAN
THAN
HOWEVER
NECESSITATING
THAT
IF
THE
RATIO
BETWEEN
STATIC
AND
DYNAMIC
POWER
IS
LARGE
WE
CAN
T
MAINTAIN
THE
AFOREMENTIONED
RELATIONSHIP
BETWEEN
STATIC
AND
DYNAMIC
ENERGY
IN
THAT
CASE
WE
SHOULD
SET
AND
ENERGY
COROLLARIES
TO
AMDAHL
LAW
FIGURE
CHANGES
THE
SPEEDUP
OF
A
PROGRAM
WHEN
ITS
ENERGY
CON
SUMPTION
IS
MINIMIZED
SATURATES
AT
THE
MAXIMUM
SPEEDUP
THAT
AMDAHL
LAW
DICTATES
WHEN
WE
ASSUME
THAT
FIND
THE
VALUES
OF
AND
THAT
MINIMIZE
TOTAL
ENERGY
CON
SUMPTION
DENOTING
THESE
VALUES
BY
AND
WE
OBTAIN
THE
FOLLOWING
AGAIN
THESE
VALUES
RESULT
IN
DYNAMIC
POWER
CONSUMP
TION
BEING
TIMES
THE
STATIC
POWER
CONSUMPTION
DUR
ING
EXECUTION
OF
THE
PARALLEL
PORTION
OF
THE
PROGRAM
FINALLY
IF
STATIC
POWER
CONSUMPTION
IS
SO
HIGH
THAT
THEN
THE
MINIMUM
ENERGY
IS
OBTAINED
WHEN
THAT
IS
ENERGY
CONSUMPTION
IS
AT
THE
MINI
MUM
WHEN
THE
PROCESSORS
RUN
AT
THEIR
MAXIMUM
SPEED
TO
FINISH
THE
TASK
AS
QUICKLY
AS
POSSIBLE
FIGURE
SUMMARIZES
THE
RELATIONSHIP
BETWEEN
AND
THE
SPEEDUP
THAT
RESULTS
IN
MINIMUM
ENERGY
CONSUMPTION
IN
THIS
FIGURE
THE
VALUES
OF
ARE
DIVIDED
INTO
THREE
REGIONS
WHEN
THE
SOLU
TION
FOR
THE
OPTIMUM
ENERGY
CONSUMPTION
PROBLEM
IS
GIVEN
BY
EQUATIONS
AND
WHEN
THE
SOLUTION
IS
GIVEN
BY
IN
EQUATIONS
AND
WHEN
THE
SOLU
TION
IS
GIVEN
BY
AND
THE
SPEEDUP
IS
THAT
GIVEN
BY
AMDAHL
LAW
IN
EQUATION
FIGURE
DEPICTS
THE
IMPROVEMENT
RATIO
OF
THE
MINI
MUM
ENERGY
AT
DIFFERENT
PROGRAM
SPEEDUPS
RELATIVE
TO
THE
BASELINE
SEQUENTIAL
EXECUTION
OF
A
GIVEN
APPLICATION
THIS
PLOT
CLEARLY
DEMONSTRATES
THAT
A
SMALLER
LEADS
TO
A
LARGER
ENERGY
IMPROVEMENT
RATIO
AT
ANY
SELECTED
PROGRAM
SPEEDUP
MORE
OVER
THE
GREATEST
ENERGY
IMPROVEMENT
RATIO
OCCURS
AT
A
LESSER
PROGRAM
SPEEDUP
IN
OTHER
WORDS
ONE
CAN
SLOW
THE
CLOCK
SPEED
FURTHER
TO
BENEFIT
FROM
REDUCING
DYNAMIC
ENERGY
TO
A
GREATER
DEGREE
BEFORE
STATIC
ENERGY
STARTS
TO
OFFSET
THE
BENEFIT
IF
IS
SMALL
ENERGY
PERFORMANCE
TRADE
OFFS
SO
FAR
WE
HAVE
FOCUSED
ON
THE
PROBLEM
OF
OBTAINING
THE
MINIMUM
ENERGY
CONSUMPTION
WITH
SPECIFIC
PROCESSOR
SPEEDS
HENCE
PROGRAM
SPEEDUP
GIVEN
WE
HAVE
LARGELY
IGNORED
THE
PROGRAM
PERFORMANCE
IN
THIS
SECTION
WE
WILL
CONSIDER
THE
TRADE
OFFS
BETWEEN
PROGRAM
PERFORM
ANCE
AND
ENERGY
CONSUMPTION
THE
MAIN
QUESTION
IS
HOW
TO
SET
THE
CLOCK
FREQUENCY
FOR
A
SPECIFIED
DEGREE
OF
PERFORMANCE
BECAUSE
THE
STATIC
ENERGY
IS
IMMEDIATELY
DETERMINED
GIVEN
WE
NEED
ONLY
MINIMIZE
THE
DYNAMIC
ENERGY
WHILE
MEETING
THE
PROGRAM
SPEEDUP
REQUIREMENT
OUR
SOLUTION
IS
DERIVED
FROM
EQUATIONS
AND
AS
FOLLOWS
WHERE
AND
ARE
THE
OPTIMAL
FREQUENCIES
WHEN
IN
EQUATIONS
AND
WE
CALL
THE
SPEEDUP
INTERVAL
IN
EQUATION
THE
LINEAR
FRE
QUENCY
SCALING
INTERVAL
BECAUSE
THE
ENERGY
OPTIMAL
BE
OBTAINED
BY
SIMPLY
SCALING
BY
A
FACTOR
OF
NOTE
THAT
THE
UPPER
BOUND
OF
THE
CONDITION
IN
EQUATION
IS
EQUIVALENT
TO
FIGURE
SHOWS
HOW
THE
MINIMUM
ENERGY
CON
SUMPTION
CHANGES
AS
WE
TARGET
A
DIFFERENT
PROGRAM
SPEEDUP
THIS
FIGURE
ALSO
SHOWS
THE
CONTRIBUTIONS
OF
DYNAMIC
AND
STATIC
ENERGY
CONSUMPTION
NOTICE
THAT
FIGURE
ENERGY
IMPROVEMENT
AT
DIFFERENT
SPEEDUPS
COMPARED
WITH
SEQUENTIAL
EXECUTION
THE
DYNAMIC
ENERGY
OF
THE
SEQUENTIAL
REGION
SATURATES
AT
AROUND
THAT
BECAUSE
CANNOT
SCALE
ENERGY
COROLLARIES
TO
AMDAHL
LAW
BEYOND
FINALLY
WHEN
I
E
THE
MAXIMUM
SPEEDUP
DYNAMIC
ENERGY
IS
THE
SAME
AS
FOR
SEQUENTIAL
EXECUTION
ANOTHER
WAY
TO
MAKE
THE
TRADE
OFF
BETWEEN
ENERGY
CON
SUMPTION
AND
PROGRAM
PERFORMANCE
IS
TO
MINIMIZE
THE
ENERGY
DELAY
PRODUCT
RATHER
THAN
THE
TOTAL
ENERGY
THE
ENERGY
DELAY
PRODUCT
IS
AS
FOLLOWS
THROUGH
SIMILAR
ANALYSIS
WE
GET
THE
FOLLOWING
IT
IS
INTERESTING
TO
FIND
THAT
AS
IN
EQUATION
IN
FACT
BY
COMPARING
THE
VALUES
OBTAINED
FOR
WITH
EQUATIONS
AND
WE
OBSERVE
THAT
THEY
ARE
THE
SAME
EQUATIONS
REPLACING
THIS
SIMI
LARITY
ALSO
APPEARS
IN
THE
CALCULATION
OF
ENERGY
SPECIFICALLY
WE
CAN
COMPUTE
THE
DYNAMIC
ENERGY
WHEN
IS
MINIMIZED
TASK
COMPLETION
OR
STALLING
BECAUSE
OF
CONTENTION
FOR
SHARED
RESOURCES
THIS
OVERHEAD
HAS
BEEN
EXAMINED
IN
THE
CONTEXT
OF
SCIENTIFIC
COMPUTING
USING
SUPERCOMPUTERS
THE
IMPOR
TANCE
OF
SYNCHRONIZATION
OVERHEAD
IN
CONVENTIONAL
COMPUT
ERS
GROWS
AS
CORE
COUNTS
INCREASE
THE
MODEL
WE
USE
TO
REPRESENT
SYNCHRONIZATION
OVER
HEAD
ADDS
A
SYNCHRONIZATION
FUNCTION
TO
THE
PARALLEL
PORTION
THIS
IS
IN
ADDITION
TO
THE
REGULAR
WORK
THE
PROCESSOR
DOES
AND
IT
INCREASES
WITH
THE
NUMBER
OF
CORES
ALTHOUGH
THE
EXACT
FORM
OF
SYNCHRONIZATION
DEPENDS
ON
BOTH
THE
CPU
OR
SYSTEM
ARCHITECTURE
AND
THE
PROGRAM
EXECUTED
PRIOR
WORK
HAS
FOUND
TO
BE
CLOSE
FOR
MANY
ARCHITECTURES
SEE
H
P
FLATT
A
SIMPLE
MODEL
FOR
PARALLEL
PROCESSING
FIGURE
IS
A
PLOT
SIMILAR
TO
THAT
OF
FIGURE
USING
THAT
ESTIMATION
WITH
AFTER
USING
THE
SAME
DERIVATION
PROCESS
DESCRIBED
EAR
LIER
WE
DISCOVERED
SOMETHING
UNINTUITIVE
THE
RELATION
BETWEEN
AND
WHEN
MINIMIZING
ENERGY
CONSUMPTION
IS
INDEPENDENT
OF
THE
SYNCHRONIZATION
FUNCTION
ALTHOUGH
THIS
MIGHT
SEEM
SURPRISING
IT
MAKES
SENSE
WHEN
CONSIDERING
THAT
THE
RELATIVE
SERIAL
AND
PARALLEL
PORTIONS
OF
WORK
DO
NOT
FACTOR
INTO
THE
RELATION
BETWEEN
AND
EITHER
THE
SYNCHRONIZA
TION
COST
EFFECTIVELY
RAISES
THE
PORTION
OF
PARALLEL
WORK
ALONG
WITH
THE
TOTAL
WORK
CONCLUSIONS
WE
HAVE
CONSIDERED
THE
PROBLEM
OF
MINIMIZING
TOTAL
ENERGY
CONSUMPTION
FOR
A
GIVEN
ARCHITECTURE
VALUES
OF
AND
A
GIVEN
PROBLEM
WITH
A
KNOWN
RATIO
OF
PARALLELIZABLE
WORK
VALUE
OF
WE
HAVE
ANALYTICALLY
DERIVED
THE
FORMULA
FOR
THE
PROCESSOR
SPEEDS
THAT
MINIMIZE
ENERGY
CONSUMPTION
WHICH
IS
EQUAL
TO
OF
THE
STATIC
ENERGY
ADDING
THE
OVERHEAD
OF
PARALLELIZATION
ANALYTICAL
MODELS
OF
COMPUTER
PERFORMANCE
SACRIFICE
ACCURACY
FOR
SIMPLICITY
SOME
ASPECTS
OF
A
COMPUTER
SYSTEM
ARE
NOT
MOD
ELED
AND
ONE
MIGHT
WONDER
ABOUT
THE
EFFECT
OF
THESE
MISSING
PROPERTIES
IN
THIS
SECTION
WE
EXAMINE
THE
IMPACT
OF
ONE
SUCH
PROPERTY
SYNCHRONIZATION
OVERHEAD
SYNCHRONIZATION
OVERHEAD
IS
A
BROAD
TERM
WE
USE
TO
DESCRIBE
ANY
OVERHEAD
INCURRED
WHEN
USING
MULTIPLE
PROCESSORS
TOGETHER
IT
INCLUDES
TIME
SPENT
COMMUNICATING
BY
USING
LOCKS
OR
BARRIERS
WAITING
BECAUSE
OF
UNEVEN
FIGURE
OPTIMAL
ENERGY
GIVEN
THE
SPEEDUP
OF
TOTAL
ENERGY
IS
THE
SUM
OF
DYNAMIC
AND
STATIC
ENERGY
THIS
PLOT
ALSO
SHOWS
DYNAMIC
ENERGY
FOR
THE
SEQUENTIAL
REGION
THE
THICK
DOTTED
LINE
SHOWS
THE
SEQUENTIAL
MACHINE
ENERGY
CONSUMPTION
GIVEN
THESE
PARAMETERS
THE
MAXIMUM
SPEEDUP
AMDAHL
LAW
IS
AND
ACCORDING
TO
EQUATION
THE
DYNAMIC
ENERGY
OF
THE
SEQUENTIAL
REGION
SATURATES
AT
ACCORDING
TO
EQUATION
ENERGY
COROLLARIES
TO
AMDAHL
LAW
FIGURE
ACHIEVABLE
DYNAMIC
ENERGY
IMPROVEMENT
AFTER
ACCOUNTING
FOR
A
SYNCHRONIZATION
OVERHEAD
OF
AS
IN
FIGURE
AND
HAVE
SHOWN
THAT
AT
THOSE
SPEEDS
DYNAMIC
ENERGY
CON
SUMPTION
IS
EQUAL
TO
THE
STATIC
ENERGY
CONSUMPTION
HENCE
TO
MINIMIZE
ENERGY
THIS
RELATION
BETWEEN
STATIC
AND
DYNAMIC
ENERGY
SHOULD
BE
MAINTAINED
AS
LONG
AS
THE
PROCES
SOR
DOES
NOT
EXCEED
ITS
MAXIMUM
ALLOWABLE
CLOCK
FREQUENCY
IN
THAT
CASE
THE
MAXIMUM
SPEED
SHOULD
BE
USED
IN
MANY
SYSTEMS
IT
IS
DESIRABLE
TO
STRIKE
A
TRADE
OFF
BETWEEN
ENERGY
CONSUMPTION
AND
PERFORMANCE
BY
MINIMIZ
ING
THE
ENERGY
DELAY
PRODUCT
RATHER
THAN
THE
TOTAL
ENERGY
OUR
RESULTS
SHOW
THAT
THE
OPTIMAL
ENERGY
DELAY
IS
OBTAINED
WHEN
AND
AND
OUR
RESULTS
ALSO
SHOW
THAT
THE
FREQUENCY
RELATION
OF
ALLOWS
US
TO
OPTIMIZE
BOTH
ENERGY
AND
THE
ENERGY
DELAY
PRODUCT
OUR
FORMULAS
ALSO
SHOW
THAT
FOR
A
GIVEN
PROCESSOR
IMPLEMENTATION
AND
THE
MINIMUM
TOTAL
ENERGY
IS
A
MONOTONICALLY
DECREASING
FUNCTION
OF
THE
NUMBER
OF
PROCESSORS
AS
LONG
AS
THE
PARALLEL
SECTION
OF
CODE
CAN
BE
EXECUTED
ON
PROCESSORS
HENCE
FROM
THE
VIEWPOINT
OF
TOTAL
ENERGY
CONSUMPTION
ALL
AVAILABLE
PROCESSORS
SHOULD
BE
RUNNING
HOWEVER
NOTE
THAT
THIS
RESULT
AND
ALL
RESULTS
IN
THIS
ARTICLE
ASSUME
THAT
THE
PROCESSORS
IN
THE
SYSTEM
CON
SUME
STATIC
POWER
EVEN
WHEN
EXECUTING
SERIAL
CODE
IF
INDI
VIDUAL
PROCESSORS
CAN
BE
TURNED
OFF
AND
BACK
ON
WITH
LOW
OVERHEAD
THEN
THE
FORMULA
FOR
TOTAL
ENERGY
IN
EQUATION
SHOULD
BE
CHANGED
SUCH
THAT
IS
REPLACED
B
Y
IN
THIS
CASE
OUR
ANALYSIS
INDICATES
THAT
THE
MINIMUM
TOTAL
ENERGY
IS
INDEPENDENT
OF
THE
NUMBER
OF
PROCESSORS
USED
FOR
EXECUTING
THE
PARALLEL
SECTION
OF
A
PROGRAM
THE
ENERGY
DELAY
PRODUCT
IS
MINIMIZED
WHEN
THE
MAXIMUM
NUMBER
OF
AVAILABLE
PROCESSORS
EXECUTES
THE
PARALLEL
CODE
THE
MINIMUM
AMOUNT
OF
ENERGY
IS
CONSUMED
WHEN
CLOCK
SPEEDS
ARE
EQUAL
DURING
THE
SERIAL
AND
PARALLEL
SECTIONS
WHICH
AGAIN
RESULTS
IN
STATIC
ENERGY
EQUALING
THE
DYNAMIC
ENERGY
EDITOR
NOTE
SANGYEUN
CHO
IS
AN
ASSISTANT
PROFESSOR
IN
THE
DEPARTMENT
OF
COMPUTER
SCIENCE
AT
THE
UNIVERSITY
OF
PITTSBURGH
IN
PENNSYLVANIA
MICHAEL
MOENG
IS
A
GRADUATE
STUDENT
IN
THAT
DEPARTMENT
DR
RAMI
MELHEM
IS
THE
DEPARTMENT
CHAIRMAN
AND
A
PROFESSOR
OF
COMPUTER
SCIENCE
THIS
PAPER
HIGHLIGHTS
THE
GROWING
IMPORTANCE
OF
STORAGE
ENERGY
CON
SUMPTION
IN
A
TYPICAL
DATA
CENTER
AND
ASSERTS
THAT
STORAGE
ENERGY
RE
SEARCH
SHOULD
DRIVE
TOWARDS
A
VISION
OF
ENERGY
PROPORTIONALITY
FOR
ACHIEVING
SIGNIFICANT
ENERGY
SAVINGS
OUR
ANALYSIS
OF
REAL
WORLD
EN
TERPRISE
WORKLOADS
SHOWS
A
POTENTIAL
ENERGY
REDUCTION
OF
USING
AN
IDEALLY
PROPORTIONAL
SYSTEM
WE
THEN
PRESENT
A
PRELIMI
NARY
ANALYSIS
OF
APPROPRIATE
TECHNIQUES
TO
ACHIEVE
PROPORTIONALITY
CHOSEN
TO
MATCH
BOTH
APPLICATION
REQUIREMENTS
AND
WORKLOAD
CHAR
ACTERISTICS
BASED
ON
THE
TECHNIQUES
WE
HAVE
IDENTIFIED
WE
BELIEVE
THAT
ENERGY
PROPORTIONALITY
IS
ACHIEVABLE
IN
STORAGE
SYSTEMS
AT
A
TIME
SCALE
THAT
WILL
MAKE
SENSE
IN
REAL
WORLD
ENVIRONMENTS
INTRODUCTION
ENERGY
CONSUMPTION
OF
DATA
CENTERS
IS
A
SIGNIFICANT
PORTION
OF
THEIR
OPERATIONAL
COST
AND
HAS
BECOME
A
MAJOR
CONCERN
BOTH
FOR
THEIR
OWN
ERS
AND
THE
COUNTRIES
IN
WHICH
THEY
ARE
PLACED
THUS
A
LARGE
AMOUNT
OF
RECENT
WORK
HAS
FOCUSED
ON
IMPROVING
THE
ENERGY
EFFICIENCY
OF
DATA
CENTERS
IN
ALL
ASPECTS
DISTRIBUTION
OF
POWER
DATA
CENTER
COOL
ING
AND
ENERGY
CONSUMPTION
OF
IT
COMPONENTS
AMONG
THESE
DIF
FERENT
ASPECTS
REDUCING
THE
ENERGY
CONSUMPTION
OF
IT
COMPONENTS
SERVERS
STORAGE
NETWORKING
EQUIPMENT
ETC
PLAYS
AN
IMPORTANT
ROLE
SINCE
IMPROVING
ITS
ENERGY
EFFICIENCY
ALSO
REDUCES
THE
REQUIRED
CAPACITY
OF
THE
COOLING
AND
POWER
DISTRIBUTION
SYSTEMS
OF
ALL
THE
IT
COMPONENTS
IN
A
DATA
CENTER
SERVERS
HAVE
BEEN
THE
DOMINANT
ENERGY
CONSUMERS
WHILE
A
SINGLE
CORE
MICROPROCESSOR
IN
CONSUMED
W
OF
ENERGY
A
DISK
CONSUMED
AROUND
W
THUS
SERVER
ENERGY
CONSUMPTION
ATTRACTED
SIGNIFICANT
RESEARCH
ATTENTION
AND
TYPICAL
SERVER
ENERGY
USAGE
HAS
DECREASED
CONSIDER
ABLY
OVER
THE
LAST
FEW
YEARS
TODAY
AN
IDLING
POWER
GATED
CORE
CONSUMES
AS
LITTLE
AS
W
OF
ENERGY
AND
AN
ACTIVE
CORE
CONSUMES
AROUND
W
ON
THE
OTHER
HAND
DISK
DRIVE
POWER
CONSUMPTION
HAS
REMAINED
RELATIVELY
UNCHANGED
PER
DRIVE
AS
A
RESULT
STORAGE
IS
CONSUMING
AN
INCREASING
PERCENTAGE
OF
ENERGY
IN
THE
DATA
CENTER
RECENT
WORK
HAS
SHOWN
THAT
IN
A
TYPICAL
DATA
CENTER
TODAY
ACCOUNTS
FOR
UP
TO
OF
THE
ENERGY
CONSUMPTION
OF
ALL
IT
COM
PONENTS
WE
EXPECT
STORAGE
ENERGY
CONSUMPTION
TO
CONTINUE
INCREASING
IN
THE
FUTURE
AS
DATA
VOLUMES
GROW
AND
DISK
PERFORMANCE
AND
CAPAC
ITY
SCALING
SLOW
A
RECENT
STUDY
BY
IDC
MAKES
THE
FOLLOW
ING
OBSERVATIONS
THAT
BACK
THIS
TREND
STORAGE
UNIT
ACQUISITION
STORAGE
IN
THE
REST
OF
THIS
PAPER
REFERS
TO
THE
COLLECTION
OF
INDEPENDENTLY
MANAGED
DEDICATED
CLUSTERS
SUBSYSTEMS
FOR
STORAGE
AND
NOT
DIRECT
ATTACHED
STORAGE
WILL
LIKELY
OUTPACE
SERVER
UNIT
ACQUISITION
DURING
THE
NEXT
FIVE
YEARS
STORAGE
CAPACITY
PER
DRIVE
IS
INCREASING
MORE
SLOWLY
WHICH
WILL
FORCE
THE
ACQUISITION
OF
MORE
DRIVES
TO
ACCOMMODATE
GROWING
CA
PACITY
REQUIREMENTS
DATA
CENTERS
ARE
PREDICTED
TO
MOVE
TOWARDS
DRIVES
THAT
TYPICALLY
CONSUME
MORE
ENERGY
PER
GBYTE
THAN
THEIR
EQUIVALENTS
AND
PERFORMANCE
IMPROVEMENTS
PER
DRIVE
HAVE
NOT
AND
WILL
NOT
KEEP
PACE
WITH
CAPACITY
IMPROVEMENTS
THUS
IM
PROVING
IOPS
PER
WATT
CONTINUES
TO
BE
A
CHALLENGE
ATTESTING
TO
THIS
GROWING
IMPORTANCE
OF
STORAGE
ENERGY
CONSUMPTION
EPA
AN
NOUNCED
ENERGYSTAR
SPECIFICATIONS
FOR
STORAGE
COMPONENTS
IN
APRIL
A
RICH
BODY
OF
EXISTING
WORK
E
G
HAS
AL
READY
INVESTIGATED
ENERGY
EFFICIENCY
OF
STORAGE
SYSTEMS
HOWEVER
A
SIGNIFICANT
FRACTION
OF
THIS
WORK
ASSUMES
THE
EXISTENCE
OF
HARD
DISKS
WITH
DYNAMIC
RPM
CAPABILITY
E
G
HOWEVER
DRPM
DRIVES
ARE
NOT
BEING
COMMERCIALIZED
IN
QUANTITY
BY
ANY
MAJOR
DRIVE
VENDOR
DUE
TO
PHYSICAL
AND
COST
CONSTRAINTS
NEVERTHELESS
THE
INCREASING
IMPORTANCE
OF
STORAGE
ENERGY
HAS
SPURRED
INNOVATIONS
IN
HARD
DISK
DESIGN
SUCH
AS
MULTIPLE
IDLE
MODES
AND
JUST
IN
TIME
SEEKS
AND
SOLID
STATE
DISKS
SSDS
ARE
POISED
TO
IMPROVE
IOPS
PER
WATT
DRAMATICALLY
IN
LIGHT
OF
THIS
WE
ARGUE
THAT
THE
RESEARCH
COMMUNITY
SHOULD
RENEW
INTEREST
IN
IMPROVING
STORAGE
ENERGY
CONSUMPTION
AT
THE
STORAGE
SUBSYSTEM
AND
THE
DATA
CENTER
LEVEL
MORE
IMPORTANTLY
WE
CLAIM
THAT
IMPROVING
ENERGY
EFFICIENCY
ALONE
IS
NOT
ADEQUATE
AND
THAT
SIGNIFICANT
EFFORTS
MUST
BE
FOCUSED
ON
ACHIEVING
ENERGY
PROPOR
TIONALITY
FOR
STORAGE
SYSTEMS
ENERGY
PROPORTIONALITY
WAS
FIRST
PROPOSED
FOR
SERVERS
BY
BARROSO
AND
HOLZLE
THIS
WORK
OBSERVED
THAT
SERVERS
OVER
A
SIX
MONTH
PERIOD
SPENT
MOST
OF
THEIR
TIME
BETWEEN
AND
PERCENT
UTILIZATION
THUS
THE
AUTHORS
ARGUED
THAT
ENERGY
USAGE
SHOULD
VARY
AS
UTILIZATION
LEVELS
VARY
SPECIFICALLY
ENERGY
PROPORTIONALITY
SUG
GESTS
THAT
AS
THE
AMOUNT
OF
WORK
DONE
INCREASES
SO
CAN
THE
ENERGY
CONSUMED
TO
PERFORM
IT
THIS
PAPER
INVESTIGATES
WHETHER
THIS
CONCEPT
OF
ENERGY
PROPORTION
ALITY
CAN
AND
SHOULD
BE
EXTENDED
TO
STORAGE
WE
ARGUE
THAT
AN
ENERGY
PROPORTIONAL
STORAGE
SYSTEM
IS
USEFUL
IN
TWO
DIFFERENT
SCENARIOS
WHEN
PERFORMANCE
MATTERS
MOST
STORAGE
ENERGY
CONSUMPTION
SHOULD
VARY
WITH
THE
PERFORMANCE
REQUIREMENT
THIS
PERSPECTIVE
IS
IMPORTANT
FOR
NORMAL
OPERATION
OF
A
DATA
CENTER
WHEN
ENERGY
MATTERS
MOST
PERFORMANCE
PROVIDED
BY
THE
STORAGE
SHOULD
BE
REGULATED
ACCORDING
TO
ENERGY
CONSTRAINTS
THIS
PERSPEC
TIVE
COULD
BE
IMPORTANT
FOR
OPERATION
OF
DATA
CENTERS
THAT
ARE
EXPERI
ENCING
A
TRANSIENT
E
G
BROWNOUT
OR
CHRONIC
ENERGY
CONSTRAINT
THE
FIRST
PERSPECTIVE
PERFORMANCE
MATTERS
MOST
IS
MORE
ACHIEV
ABLE
IN
THE
SHORT
TERM
BUT
WE
BELIEVE
THAT
THE
SECOND
PERSPECTIVE
IS
IMPORTANT
IN
THE
LONG
TERM
ESPECIALLY
AS
THE
FOCUS
SHIFTS
TO
STOR
AGE
ENERGY
MANAGEMENT
ULTIMATELY
STORAGE
ENERGY
MANAGEMENT
SHOULD
BE
PART
OF
AN
INTEGRATED
DATA
CENTER
ENERGY
MANAGEMENT
ARCHI
TECTURE
FOCUSING
ON
ENERGY
PROPORTIONALITY
WILL
ENABLE
STORAGE
EN
ERGY
MANAGEMENT
TO
BE
COORDINATED
WITH
SERVER
NETWORK
AND
COOL
ING
ENERGY
MANAGEMENT
THE
REST
OF
OUR
PAPER
MAKES
THE
FOLLOWING
CONTRIBUTIONS
EX
PLORATION
OF
THE
BENEFIT
FROM
AN
ENERGY
PROPORTIONAL
STORAGE
SYS
TEM
TO
THAT
END
WE
FIRST
STUDY
THE
VARIATION
IN
PERFORMANCE
DE
MANDS
ON
STORAGE
SYSTEMS
IN
REAL
WORLD
ENTERPRISE
DATA
CENTER
ENVI
RONMENTS
AND
ESTIMATE
THE
POTENTIAL
ENERGY
SAVINGS
SECTION
OUTLINING
OF
TECHNIQUES
THAT
CAN
BE
USED
TO
BUILD
ENERGY
PROPORTIONAL
STORAGE
SYSTEMS
AND
SYSTEMATIC
ANALYSIS
OF
THESE
TECHNIQUES
SEC
TION
AND
ILLUSTRATING
HOW
STORAGE
APPLICATION
REQUIREMENTS
AND
WORKLOAD
CHARACTERISTICS
MAP
TO
STORAGE
ENERGY
PROPORTIONALITY
TECHNIQUES
SECTION
ENERGY
PROPORTIONALITY
MATTERS
THIS
SECTION
MOTIVATES
THE
IMPORTANCE
OF
AN
ENERGY
PROPORTIONAL
STORAGE
ARCHITECTURE
BY
STUDYING
THE
VARIATION
IN
UTILIZATION
SEEN
BY
UNITS
OF
STORAGE
METHODOLOGY
WE
USE
TWO
TRACE
SETS
COLLECTED
AT
VOLUME
LEVEL
AND
EXTENT
FIXED
SIZED
PARTITION
OF
A
VOLUME
LEVEL
RESPECTIVELY
FOR
THIS
STUDY
THE
VOLUME
LEVEL
TRACE
WAS
COLLECTED
FROM
THE
DATA
CENTER
OF
A
FINANCIAL
CORPORATION
THE
DATA
CENTER
CONTAINED
LARGE
STORAGE
SYSTEMS
WITH
A
TOTAL
OF
VOLUMES
MOUNTED
ON
RAID
ARRAYS
EACH
WITH
OR
HARD
DISKS
I
O
DATA
METRICS
INCLUDE
I
O
RATE
READ
TO
WRITE
RATIO
RANDOM
TO
SEQUENTIAL
RATIO
AND
AVERAGE
TRANSFER
SIZE
FOR
EACH
VOLUME
IN
THE
STORAGE
SYSTEM
WAS
COLLECTED
BY
A
CENTRALIZED
INDE
PENDENT
SERVER
EVERY
MINUTES
AND
STORED
IN
A
DATABASE
THIS
DATA
WAS
LATER
COMPACTED
INTO
PER
DAY
AVERAGES
ON
A
PER
VOLUME
BASIS
WE
USE
THESE
PER
DAY
SAMPLES
OVER
DAYS
IN
OUR
STUDY
APART
FROM
USING
I
O
RATE
TO
UNDERSTAND
THE
VARIATION
IN
THE
PERFORMANCE
DEMANDS
WE
COMPUTE
UTILIZATION
LEVEL
FOR
EACH
VOLUME
AS
INSTANTANEOUS
I
O
RATE
MAX
I
O
RATE
FOR
THAT
VOLUME
THE
EXTENT
LEVEL
TRACE
WAS
COLLECTED
BY
RUNNING
A
PUBLIC
STORAGE
BENCHMARK
MODELING
A
TRANSACTIONAL
WORKLOAD
FOR
AN
EXTENDED
TIME
PERIOD
THE
SETUP
CONSISTED
OF
AN
IBM
PSERIES
COMPUTER
RUNNING
AIX
CONNECTED
TO
AN
IBM
STORAGE
SYSTEM
WITH
RAID
ARRAYS
EACH
ARRAY
CONSISTING
OF
OR
DISKS
FOURTEEN
TBYTE
VOLUMES
WERE
ALLOCATED
ON
THIS
STORAGE
SYSTEM
AND
THE
BENCHMARK
ACCESSED
DIFFERENT
EXTENTS
IN
THOSE
VOLUMES
A
MONITORING
MODULE
RECORDED
I
O
RATE
PERIODICALLY
FOR
EACH
UNIQUE
EXTENT
ON
ALL
VOLUMES
FIGURE
AVERAGE
FRACTION
OF
TIME
VS
VOLUME
UTILIZATION
VOLUME
LEVEL
TRACE
FIGURE
HEATMAP
DEPICTING
I
O
RATE
ACROSS
VOLUMES
FOR
THE
VOLUME
TRACE
NOTE
THAT
THE
VOLUMES
ARE
SORTED
USING
THE
MEDIAN
I
O
RATE
FROM
BOTTOM
TO
TOP
ON
THE
Y
AXIS
DARKER
COLOR
DEPICTS
HIGHER
I
O
ACTIVITY
FIGURE
HEATMAP
DEPICTING
I
O
RATE
ACROSS
EXTENTS
IN
A
SIN
GLE
VOLUME
IN
THE
EXTENT
TRACE
DARKER
COLOR
DEPICTS
HIGHER
I
O
ACTIVITY
IMPACT
OF
ENERGY
PROPORTIONALITY
FIGURE
DEPICTS
THE
AVERAGE
PERCENTAGE
OF
TIME
A
VOLUME
SPENT
AT
EACH
UTILIZATION
LEVEL
FOR
THE
VOLUME
LEVEL
TRACE
ON
AVERAGE
A
VOL
UME
IS
IDLE
FOR
OF
THE
TIME
FIRST
TWO
COLUMNS
FOR
THE
RE
MAINING
A
DECREASING
AMOUNT
OF
TIME
IS
SPENT
AT
EACH
UTILIZA
TION
LEVEL
CLEARLY
THIS
SHOWS
THAT
ON
AVERAGE
VOLUME
UTILIZATION
IN
THIS
STORAGE
SYSTEM
IS
HIGHLY
VARIABLE
WITH
A
SIGNIFICANT
AMOUNT
OF
TIME
SPENT
AT
MODERATE
UTILIZATION
LEVELS
THIS
HIGHLIGHTS
THE
NEED
FOR
STORAGE
SYSTEMS
TO
EXHIBIT
ENERGY
PROPORTIONAL
BEHAVIOR
WHILE
FIGURE
DEPICTS
THE
AVERAGE
BEHAVIOR
ACROSS
ALL
VOLUMES
IN
THE
TRACE
FIGURE
SHOWS
THE
I
O
ACTIVITY
OVER
TIME
FOR
EACH
VOLUME
WE
SEE
THAT
MOST
VOLUMES
HAVE
VARIABLE
USAGE
OVER
TIME
HOWEVER
A
FEW
VOLUMES
DO
MAINTAIN
A
CONSISTENTLY
HIGH
OR
LOW
I
O
RATE
FOR
A
MAJORITY
OF
THE
TIME
TOP
AND
BOTTOM
PORTIONS
OF
THE
FIGURE
THUS
BOTH
ENERGY
EFFICIENCY
AND
PROPORTIONALITY
ARE
ESSENTIAL
FOR
POTENTIAL
ENERGY
SAVINGS
GIVEN
THE
I
O
DISTRIBUTION
SEEN
IN
THE
VOLUME
TRACE
WE
NOW
ES
TIMATE
THE
ENERGY
SAVINGS
IN
AN
IDEALLY
ENERGY
PROPORTIONAL
SYSTEM
IN
SUCH
A
SYSTEM
ENERGY
USAGE
WOULD
FOLLOW
UTILIZATION
WHICH
ON
AVERAGE
IS
LESS
THAN
THEREFORE
THE
POTENTIAL
FOR
ENERGY
SAV
INGS
IN
AN
IDEALLY
PROPORTIONAL
SYSTEM
EXCEEDS
WHEN
COMPARED
TO
A
SYSTEM
ALWAYS
RUNNING
AT
UTILIZATION
AND
HENCE
ENERGY
INDEPENDENT
OF
SYSTEM
DEMANDS
TO
UNDERSTAND
WHETHER
THERE
ARE
MORE
OPPORTUNITIES
AT
A
FINER
GRAIN
WE
ALSO
STUDIED
AN
EXTENT
LEVEL
TRACE
DESCRIBED
PREVIOUSLY
FIGURE
WHICH
IS
TYPICAL
OF
ALL
THE
VOLUMES
IN
THE
TRACE
SHOWS
THAT
SOME
EXTENTS
EXPERIENCE
VARIABLE
I
O
ACTIVITY
OVER
TIME
WHILE
OTHERS
REMAIN
LARGELY
IDLE
A
FEW
EXTENTS
HAVE
MUCH
HIGHER
AVERAGE
I
O
ACTIVITY
THAN
OTHER
NON
IDLE
EXTENTS
DEPICTED
BY
THE
HOR
IZONTAL
DARK
BANDS
THIS
INDICATES
THE
POTENTIAL
TO
ACHIEVE
PROPOR
TIONALITY
AT
A
GRANULARITY
FINER
THAN
ENTIRE
VOLUMES
OR
ARRAYS
FOR
EXAMPLE
IF
THE
MOST
ACTIVE
OF
EXTENTS
WERE
RESIDENT
ON
ENTER
PRISE
CLASS
SSD
DISKS
AND
THE
REMAINING
ON
SATA
DISKS
WE
CALCULATE
ENERGY
SAVINGS
OF
NEARLY
IN
CONTRAST
TO
A
SYSTEM
COM
PLETELY
BUILT
FROM
ENTERPRISE
DISKS
WHILE
THE
ACQUISITION
COST
OF
THE
SYSTEM
WOULD
BE
ABOUT
THE
SAME
MOREOVER
EXTENTS
CAN
BE
EASILY
MOVED
COMPARED
TO
VOLUMES
TO
AND
BACK
FROM
DIFFERENT
DISK
TIERS
WHEN
THEIR
UTILIZATION
CHANGES
STORAGE
ENERGY
PROPORTIONALITY
GIVEN
THE
SIGNIFICANT
SAVINGS
FROM
AN
IDEALLY
ENERGY
PROPORTIONAL
STORAGE
ARCHITECTURE
WE
NOW
EXPLORE
VARIOUS
TECHNIQUES
THAT
CAN
BE
LEVERAGED
TO
ACHIEVE
PROPORTIONALITY
WE
BEGIN
BY
UNDERSTANDING
THE
TECHNIQUES
USED
TO
ACHIEVE
SERVER
ENERGY
PROPORTIONALITY
IN
ORDER
TO
IDENTIFY
ANALOGOUS
APPROACHES
FOR
STORAGE
IF
POSSIBLE
SERVERS
VS
STORAGE
TECHNIQUES
A
BROAD
CLASS
OF
TECHNIQUES
FOR
ACHIEVING
SERVER
ENERGY
PROPORTION
ALITY
ARE
BASED
ON
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
DVFS
E
G
DVFS
PROVIDES
LOWER
ENERGY
ACTIVE
MODES
MODES
WHERE
SERVERS
CAN
CONTINUE
TO
PERFORM
WORK
AT
LOWER
PERFORMANCE
AND
AL
LOWS
RELATIVELY
FAST
SWITCHING
BETWEEN
MODES
ON
THE
OTHER
HAND
THERE
IS
CURRENTLY
NO
POWERFUL
ANALOGUE
IN
STORAGE
THAT
DIRECTLY
EN
ABLES
FINE
GRAIN
SCALING
OF
PERFORMANCE
AS
A
TRADEOFF
FOR
ENERGY
ANOTHER
CLASS
OF
SERVER
TECHNIQUES
FOCUSES
ON
ACHIEVING
PROPOR
TIONALITY
BY
MIGRATING
WORKLOADS
SUCH
THAT
SERVERS
EITHER
OPERATE
AT
THEIR
PEAK
OR
CAN
BE
SUSPENDED
THIS
IS
IN
CONTRAST
TO
THE
ABOVE
TECHNIQUES
THAT
SCALE
PERFORMANCE
AND
ARE
CONSIDERED
MORE
POWERFUL
SINCE
LOWER
PERFORMANCE
STATES
ARE
LESS
ENERGY
EFFICIENT
ON
THE
OTHER
HAND
THE
TIME
SCALE
TO
ADAPT
FOR
THESE
TECHNIQUES
IS
HIGHER
SINCE
IT
INVOLVES
PROCESS
MIGRATION
ALTHOUGH
THIS
CLASS
OF
TECHNIQUES
IS
DIRECTLY
APPLICABLE
TO
STORAGE
THE
OVERHEAD
INCURRED
IN
STORAGE
WILL
LIKELY
BE
HIGHER
GIVEN
ITS
STATEFULNESS
AND
AMOUNT
OF
DATA
MOVED
MOREOVER
DATA
AVAILABILITY
REQUIREMENTS
OFTEN
IMPLY
THAT
DATA
SHOULD
ALWAYS
BE
REACHABLE
WITHIN
SMALL
NUMBER
OF
MIL
LISECONDS
OR
SECONDS
STORAGE
TECHNIQUES
WE
EXPLORE
SEVERAL
ENERGY
SAVING
TECHNIQUES
A
MAJORITY
FROM
EX
ISTING
LITERATURE
AND
A
FEW
NOVEL
TECHNIQUES
INSPIRED
BY
SERVER
SIDE
APPROACHES
AND
ENERGY
SAVING
HARD
DISK
MODES
THESE
TECHNIQUES
APPLIED
ALONE
OR
IN
CONCERT
WITH
EACH
OTHER
CAN
CONTRIBUTE
TO
AN
EN
ERGY
PROPORTIONAL
STORAGE
ARCHITECTURE
THE
FIRST
SET
OF
TECHNIQUES
USE
INTELLIGENT
DATA
PLACEMENT
AND
OR
CONTINUOUS
DATA
MIGRATION
RESULTING
IN
LARGE
TIME
SCALE
TO
ADAPT
EX
AMPLES
INCLUDE
CONSOLIDATION
AGGREGATION
OF
DATA
INTO
FEWER
STORAGE
DEVICES
WHENEVER
PERFORMANCE
REQUIREMENTS
PERMIT
TIERING
MIGRATION
PLACEMENT
MOVEMENT
OF
DATA
INTO
STORAGE
DE
VICES
THAT
BEST
FIT
ITS
PERFORMANCE
REQUIREMENTS
E
G
WRITE
OFF
LOADING
DIVERSION
OF
NEWLY
WRITTEN
DATA
TO
ENABLE
SPIN
NING
DOWN
DISKS
FOR
LONGER
PERIODS
COUPLED
WITH
OPPORTUNISTIC
MOVEMENT
OF
DATA
TO
STORAGE
DEVICES
WHEN
THEY
BECOME
ACTIVE
THE
SAME
TECHNIQUE
CAN
BE
USED
TO
EFFICIENTLY
SERVICE
PEAKS
IN
I
O
ACTIVITY
THE
NEXT
CLASS
OF
TECHNIQUES
BENEFIT
FROM
THE
AVAILABILITY
OF
HARDWARE
BASED
ACTIVE
AND
INACTIVE
LOW
ENERGY
MODES
IN
DISKS
AS
SUCH
THE
OVERHEAD
OF
THESE
TECHNIQUES
IS
LOW
SINCE
IT
DOES
NOT
IN
VOLVE
DATA
MOVEMENT
EXAMPLES
INCLUDE
ADAPTIVE
SEEK
SPEEDS
ALLOW
TRADING
OFF
PERFORMANCE
FOR
POWER
REDUCTION
BY
SLOWING
THE
SEEK
AND
WAITING
AN
ADDITIONAL
ROTATIONAL
DELAY
BEFORE
SERVICING
THE
I
O
WORKLOAD
SHAPING
BATCHING
I
O
REQUESTS
TO
ALLOW
HARD
DISKS
TO
ENTER
LOW
POWER
MODES
FOR
EXTENDED
PERIODS
OR
TO
ALLOW
WORKLOAD
MIX
OPTIMIZATIONS
OPPORTUNISTIC
SPINDOWN
SPINNING
DOWN
HARD
DISKS
WHEN
IDLE
FOR
A
GIVEN
PERIOD
SPINDOWN
MAID
MAINTAINING
DISKS
WITH
UNUSED
DATA
SPUNDOWN
MOST
OF
THE
TIME
EITHER
BY
CONCENTRATING
IMPORTANT
DATA
OR
TUNING
CACHING
AND
PREFETCHING
ALGORITHMS
TO
SAVE
POWER
IN
ALL
CASES
TRYING
TO
INCREASE
IDLE
PERIODS
OF
DISKS
WITH
LOW
LOAD
FINALLY
ANOTHER
CLASS
OF
TECHNIQUES
REDUCES
THE
AMOUNT
OF
SPACE
REQUIRED
TO
STORE
DATA
DIRECTLY
RESULTING
IN
ENERGY
SAVINGS
FOR
EXAM
PLE
DEDUP
COMPRESSION
INVOLVES
STORING
SMALLER
AMOUNTS
OF
DATA
USING
VERY
EFFICIENT
REPRESENTATIONS
OF
THE
DATA
FRAMEWORK
FOR
TECHNIQUE
SELECTION
GIVEN
THE
ABOVE
TECHNIQUES
AN
IMPORTANT
QUESTION
TO
ANSWER
IS
WHICH
TECHNIQUES
ARE
SUITED
FOR
A
GIVEN
STORAGE
APPLICATION
THIS
SECTION
OUTLINES
A
FRAMEWORK
TO
MAKE
SUCH
RECOMMENDATIONS
DIFFERENT
TECHNIQUES
PRESENT
DIFFERENT
TRADEOFFS
WITH
RESPECT
TO
THEIR
POTENTIAL
TO
ALTER
APPLICATION
PERFORMANCE
AND
INCURRED
OVER
HEAD
VERSUS
RESULTING
BENEFIT
FOR
EXAMPLE
A
TECHNIQUE
THAT
LEVER
AGES
OPPORTUNISTIC
SPINDOWN
CAN
POTENTIALLY
CAUSE
THE
PEAK
RESPONSE
TIME
OF
AN
APPLICATION
TO
BE
AS
HIGH
AS
THE
SPIN
UP
DELAY
SEC
ONDS
SIMILARLY
TECHNIQUES
THAT
RELY
ON
DATA
MIGRATION
INCUR
SIGNIF
ICANT
OVERHEAD
AND
HENCE
RELY
ON
WORKLOADS
THAT
AMORTIZE
THE
OVER
HEAD
TO
RESULT
IN
ENERGY
SAVINGS
THUS
OUR
FRAMEWORK
SELECTS
AP
PROPRIATE
TECHNIQUES
USING
TWO
INPUTS
APPLICATION
PERFORMANCE
REQUIREMENTS
AND
APPLICATION
WORKLOAD
CHARACTERISTICS
IN
THE
REST
OF
THIS
SECTION
WE
CATEGORIZE
THESE
INPUTS
BASED
ON
THEIR
INTER
ACTION
WITH
THE
TECHNIQUES
AND
THEN
MATCH
THE
DIFFERENT
CATEGORIES
WITH
THE
APPROPRIATE
TECHNIQUES
TO
ACHIEVE
ENERGY
EFFICIENCY
AND
PRO
PORTIONALITY
APPLICATION
PERFORMANCE
REQUIREMENTS
SINCE
THE
ABOVE
TECHNIQUES
MAY
IMPACT
APPLICATION
PERFORMANCE
THEIR
USAGE
MUST
BE
ALIGNED
WITH
THE
REQUIREMENTS
OF
THE
STORAGE
AP
PLICATIONS
TO
UNDERSTAND
THIS
BETTER
WE
PROPOSE
THREE
DIFFERENT
CAT
EGORIES
OF
STORAGE
APPLICATIONS
ALTHOUGH
THE
RANGE
OF
APPLICATIONS
PROBABLY
LIES
MORE
ALONG
A
CONTINUUM
THAN
IN
DISCRETE
CATEGORIES
TAKING
INTO
ACCOUNT
TWO
FACTORS
SENSITIVITY
TO
AVERAGE
RESPONSE
TIME
AND
SENSITIVITY
TO
PEAK
RESPONSE
TIME
E
G
FROM
SPIN
UP
DELAY
HIGH
SENSITIVITY
TO
PEAK
RESPONSE
TIME
HIGH
SENSITIVITY
TO
AV
ERAGE
RESPONSE
TIME
THESE
ARE
OFTEN
CRITICAL
BUSINESS
APPLICA
TIONS
TYPICALLY
USING
SAN
STORAGE
WHICH
HAVE
SHORT
DEFAULT
TIMEOUTS
TRANSACTIONAL
DATABASES
ARE
AN
EXAMPLE
LOW
SENSITIVITY
TO
PEAK
RESPONSE
TIME
HIGH
SENSITIVITY
TO
AVER
AGE
RESPONSE
TIME
THESE
ARE
OFTEN
BUT
NOT
ALWAYS
IMPORTANT
BUSI
NESS
OR
CONSUMER
APPLICATIONS
WHICH
REQUIRE
GOOD
STORAGE
PERFOR
MANCE
BUT
CAN
TOLERATE
OCCASIONAL
DELAYS
EXAMPLES
INCLUDE
WEB
STORAGE
MULTIMEDIA
STREAMING
AND
GENERAL
USER
FILE
STORAGE
LOW
SENSITIVITY
TO
PEAK
RESPONSE
TIME
LOW
SENSITIVITY
TO
AV
ERAGE
RESPONSE
TIME
THESE
ARE
OFTEN
ARCHIVAL
BACKUP
APPLICATIONS
WHERE
MULTI
SECOND
DELAYS
ARE
TOLERATED
AND
RESPONSE
TIME
IS
NOT
TABLE
ATTRIBUTES
OF
TECHNIQUES
FOR
STORAGE
ENERGY
PROPORTIONALITY
TECHNIQUE
APP
CATEGORY
TIME
SCALE
GRANULARITY
POTENTIAL
TO
ALTER
PERFORMANCE
CONSOLIDATION
HOURS
COARSE
CAN
LENGTHEN
RESPONSE
TIMES
TIERING
MIGRATION
MINUTES
HOURS
COARSE
CAN
LENGTHEN
RESPONSE
TIMES
WRITE
OFF
LOADING
MILLISECONDS
COARSE
ADDS
BACKGROUND
PROCESS
THAT
CAN
IMPACT
APPLICATION
ADAPTIVE
SEEK
SPEEDS
MILLISECONDS
FINE
CAN
LENGTHEN
RESPONSE
TIMES
WORKLOAD
SHAPING
SECONDS
FINE
CAN
LENGTHEN
RESPONSE
TIMES
OPPORTUNISTIC
SPINDOWN
SECONDS
FINE
DELAYS
DUE
TO
SPINUP
SPINDOWN
MAID
OF
SECONDS
MEDIUM
DELAYS
DUE
TO
SPINUP
DEDUP
COMPRESSION
N
A
N
A
DELAYS
IN
ACCESSING
DATA
DUE
TO
ASSEMBLING
FROM
REPOSITORY
OR
DECOMPRESSION
SO
IMPORTANT
EXAMPLES
INCLUDE
MEDICAL
OR
GENERIC
ARCHIVAL
BACKUP
AND
EDISCOVERY
BECAUSE
PERFORMANCE
REQUIREMENTS
VARY
BETWEEN
THESE
CATE
GORIES
DIFFERENT
TECHNIQUES
WILL
BE
APPROPRIATE
FOR
APPLICATIONS
IN
DIFFERENT
CATEGORIES
FOR
EXAMPLE
IT
MAY
NEVER
BE
PERMISSIBLE
TO
SPINDOWN
A
HARD
DISK
SUPPORTING
A
HIGH
PEAK
RESPONSE
TIME
INTOLER
ANT
APPLICATION
WHEREAS
OPPORTUNISTIC
SPINDOWN
MAY
BE
ACCEPTABLE
FOR
AN
APPLICATION
WHICH
IS
TOLERANT
OF
HIGH
PEAK
RESPONSE
TIME
BUT
GENERALLY
REQUIRES
AVERAGE
RESPONSE
TIME
IN
THE
TENS
OF
MILLISECONDS
TABLE
PRESENTS
A
SUMMARY
OF
WHICH
STORAGE
TECHNIQUES
CAN
BE
USED
BY
DIFFERENT
APPLICATION
CATEGORIES
AS
WELL
AS
THE
TRADEOFFS
PRE
SENTED
BY
THESE
TECHNIQUES
USING
THE
FOLLOWING
METRICS
TIME
SCALE
TO
ADAPT
SOME
TECHNIQUES
AFFECT
THE
ENVIRONMENT
IN
MILLISECONDS
E
G
SLOWING
SEEK
SPEEDS
WHILE
OTHERS
TAKE
SECONDS
E
G
SPINNING
DOWN
DISKS
MINUTES
OR
HOURS
E
G
MIGRATING
DATA
GRANULARITY
OF
CONTROL
SOME
TECHNIQUES
ARE
FINE
GRAIN
E
G
AD
JUSTMENT
OF
I
O
DISPATCH
RATE
WHILE
OTHERS
CAN
BE
COARSE
GRAIN
E
G
MIGRATING
A
VOLUME
POTENTIAL
TO
ALTER
PERFORMANCE
SOME
TECHNIQUES
CAN
INTRODUCE
SMALL
DELAYS
TO
ACCESS
TIMES
E
G
SPINUP
WHILE
OTHERS
WILL
NOT
E
G
CONSOLIDATING
WORKLOAD
CHARACTERISTICS
THE
EXTENT
OF
ENERGY
SAVINGS
AND
THE
INCURRED
OVERHEAD
OF
DIFFERENT
TECHNIQUES
IS
SIGNIFICANTLY
IMPACTED
BY
THE
WORKLOAD
CHARACTERISTICS
OF
THE
APPLICATIONS
USING
THE
STORAGE
SYSTEM
AN
IMPORTANT
CHARACTERISTIC
FOR
DATA
MOVEMENT
BASED
TECHNIQUES
IS
THE
STEADINESS
OF
THE
WORKLOAD
A
STEADY
WORKLOAD
EXHIBITS
A
CON
STANT
UTILIZATION
OVER
TIME
THIS
ENABLES
THESE
TECHNIQUES
TO
PLACE
DATA
OPTIMALLY
AND
ACCRUE
BENEFITS
OVER
TIME
EVEN
IF
THE
WORK
LOAD
IS
NOT
PERFECTLY
STEADY
THESE
TECHNIQUES
BENEFIT
IF
THE
WORK
LOAD
SHOWS
PERIODICITY
ON
THE
OTHER
HAND
THESE
TECHNIQUES
CANNOT
QUICKLY
RESPOND
TO
VARIATION
IN
THE
UTILIZATION
OF
THE
WORKLOAD
AND
CAN
ONLY
ADAPT
TO
AVERAGE
UTILIZATION
THEREBY
REDUCING
THE
BENEFIT
FOR
CONVENIENCE
WE
REFER
TO
THESE
FEATURES
AS
WORKLOAD
STABILITY
IN
FACT
GIVEN
THE
HIGHER
OVERHEAD
OF
DATA
MOVEMENT
BASED
TECHNIQUES
WORKLOAD
STABILITY
DECIDES
IF
SUCH
TECHNIQUES
ARE
APPLICABLE
ANOTHER
WORKLOAD
CHARACTERISTIC
THAT
AFFECTS
THE
AMOUNT
OF
BENEFIT
FROM
THE
TECHNIQUES
IS
THEIR
EXTENT
OF
UTILIZATION
WORKLOADS
WITH
LOW
UTILIZATION
MOST
OF
THE
TIME
CAN
BENEFIT
FROM
SPINDOWN
WHEREAS
WORKLOADS
WITH
HIGH
UTILIZATION
MAJORITY
OF
THE
TIME
HAVE
FEWER
SPIN
DOWN
OPPORTUNITIES
AS
AN
EXAMPLE
WE
CATEGORIZE
THE
WORKLOADS
IN
THE
VOLUME
LEVEL
TRACE
WE
FIRST
BREAK
UP
THE
VOLUMES
BASED
ON
THEIR
STABILITY
AND
THEN
THEIR
AVERAGE
UTILIZATION
OVERALL
WE
SEE
THAT
ALMOST
OF
THE
VOLUMES
EXHIBIT
STABILITY
WE
THEN
DIVIDE
THESE
STABLE
VOLUMES
BASED
ON
THEIR
UTILIZATION
WE
OBSERVE
THAT
OF
THE
VOLUMES
ARE
HIGHLY
UTILIZED
AND
HAVE
UTILIZATION
MORE
THAN
OF
THE
TIME
TABLE
VOLUME
CATEGORIZATION
FOR
THE
FINANCIAL
DATA
CENTER
WORKLOAD
KEY
H
HIGH
LOAD
L
LOW
LOAD
P
PEAKS
IN
LOAD
V
VX
VARIABLE
LOAD
LOWEST
HIGHEST
I
O
RATE
CATEGORY
H
L
P
V
VOL
DATA
IN
THIS
GROUP
CAN
BENEFIT
FROM
SUITABLE
TIERING
AND
CONSOLIDATION
SINCE
THE
REQUIRED
TIME
SCALE
FOR
CHANGE
IS
RATHER
HIGH
SUCH
DATA
CAN
ALSO
BE
COLOCATED
WITH
DATA
EXPERIENCING
LOW
UTILIZATION
ANOTHER
OF
THE
VOLUMES
HAVE
MOSTLY
IDLE
DATA
UNITS
WITH
UTILIZATION
MORE
THAN
OF
THE
TIME
SUCH
DATA
UNITS
CAN
BE
CONSOLIDATED
AND
MIGRATED
ONTO
SLOWER
DEVICES
WHICH
MAY
THEN
BE
SPUNDOWN
IF
THE
APPLICATION
CATEGORY
ALLOWS
FOR
THE
SPINUP
DELAY
WRITE
OFFLOADING
AND
WORKLOAD
SHAPING
TECHNIQUES
ARE
ALSO
SUITABLE
THE
NEXT
OF
VOLUMES
CONSIST
OF
DATA
UNITS
THAT
ARE
MOSTLY
UN
DER
UTILIZED
BUT
HAVE
BURSTY
PEAKS
NOMINALLY
WITH
UTILIZATION
MORE
THAN
OF
THE
TIME
AND
UTILIZED
MORE
THAN
OF
THE
TIME
IN
THIS
CASE
PEAKS
CAN
BE
SERVICED
BY
FINE
GRAIN
MIGRATION
WHICH
HAS
RELATIVELY
LOW
OVERHEAD
HARD
DISKS
WHICH
ALLOW
SLOW
SEEKS
WOULD
ALSO
PROVIDE
A
QUICK
TO
CHANGE
METHOD
OF
ALIGNING
I
O
PERFORMANCE
CHARACTERISTICS
AND
ENERGY
USAGE
AS
I
O
LOAD
CHANGES
IN
ADDITION
WRITE
OFFLOADING
TECHNIQUES
ALLOW
FOR
HANDLING
OF
SMALL
PEAKS
FOR
WRITE
TRAFFIC
WHILE
ALSO
ALLOWING
SPINDOWN
OF
HARD
DISKS
HOLDING
MOST
OF
THE
DATA
THE
REMAINING
OF
VOLUMES
SHOW
A
FAIRLY
VARIABLE
WORKLOAD
WE
FURTHER
DIVIDE
THE
VOLUMES
IN
THIS
CATEGORY
INTO
FOUR
MORE
GROUPS
CHARACTERIZED
BY
THEIR
MAXIMUM
I
O
RATE
IN
COMPARISON
WITH
THE
MAXIMUM
I
O
RATE
OBSERVED
IN
THE
SYSTEM
THE
LAST
FOUR
COLUMNS
OF
THE
TABLE
SUCH
THAT
HOLDS
VOLUMES
WITH
LOWEST
I
O
RATE
AND
HOLDS
VOLUMES
OF
HIGHEST
I
O
RATE
WE
SEE
THAT
MOST
OF
THE
VOL
UMES
HAVE
LOW
I
O
RATE
THESE
NEED
A
SMALLER
RANGE
OF
ENERGY
UTILIZATION
POINTS
FINE
GRAIN
MIGRATIONS
ACROSS
DIFFERENT
RAID
TIERS
RAID
VS
RAID
COULD
POTENTIALLY
BE
USEFUL
VOLUMES
IN
HAVE
THE
MOST
VARIABILITY
IN
THEIR
I
O
RATE
AND
THEREFORE
NEED
TECHNIQUES
WITH
SMALL
TIME
SCALE
TO
ADAPT
SLOW
SEEK
TECHNIQUES
AND
FINE
GRAIN
MIGRATIONS
ACROSS
DEVICE
TYPES
SSD
SAS
SATA
WOULD
BE
THE
MOST
SUITABLE
ALIGNMENT
OF
TECHNIQUES
TABLE
EXPRESSES
THE
APPLICATION
RESPONSE
REQUIREMENTS
IN
TWO
DI
MENSIONS
AS
BEING
SENSITIVE
OR
NOT
TO
LENGTHENING
OF
PEAK
RE
SPONSE
TIME
AND
AS
BEING
SENSITIVE
OR
NOT
TO
DRIVE
SPINUP
DELAYS
IT
ALSO
FACTORS
IN
WORKLOAD
CHARACTERISTICS
WE
SPECIFICALLY
FOCUS
ON
STABILITY
SINCE
IT
AFFECTS
THE
APPLICABILITY
OF
A
TECHNIQUE
AS
OPPOSED
TO
THE
OTHER
CHARACTERISTICS
THAT
ONLY
IMPACT
THE
AMOUNT
OF
BENEFIT
FOR
EXAMPLE
STABILITY
INDICATES
IF
AN
ADAPTIVE
ALGORITHM
WILL
BE
INEF
TABLE
FRAMEWORK
FOR
MAPPING
STORAGE
APPLICATION
PERFOR
MANCE
REQUIREMENTS
AND
WORKLOAD
CHARACTERISTICS
TO
ENERGY
SAVING
TECHNIQUES
TECHNIQUES
C
CONSOLIDATION
T
TIER
ING
MIGRATION
OPPORTUNISTIC
SPIN
DOWN
MAID
W
WRITE
OFF
LOADING
A
ADAPTIVE
SEEK
SPEEDS
H
WORKLOAD
SHAPING
D
DEDUP
COMPRESSION
SENSITIVITY
TO
AVG
RESP
TIME
SENSITIVITY
TO
PEAK
RESP
TIME
STABILITY
OF
WORKLOAD
TECHNIQUES
C
T
W
A
H
D
YES
YES
NO
YES
NO
NO
YES
NO
NO
NO
YES
KEY
APPLICABLE
FECTIVE
DUE
TO
UNEXPECTED
SHIFTS
THE
MORE
UNPREDICTABLE
THE
FUTURE
THE
LESS
LIKELY
THAT
A
HIGH
COST
OVERHEAD
TECHNIQUE
WILL
BE
USEFUL
GIVEN
THOSE
THREE
PARAMETERS
WE
SUGGEST
IN
THE
TABLE
WHICH
TECH
NIQUES
ARE
MOST
LIKELY
TO
BE
EFFECTIVE
IN
WHICH
MIX
OF
APPLICATION
REQUIREMENT
AND
WORKLOAD
FINALLY
WE
ALSO
CONSIDERED
WHETHER
INDIVIDUAL
TECHNIQUES
WOULD
CLASH
WITH
EACH
OTHER
OR
BE
COMPLEMENTARY
ON
THE
WHOLE
WE
FEEL
THAT
THE
TECHNIQUES
NEED
NOT
CLASH
MEANING
THAT
A
NUMBER
OF
THEM
MAY
BE
DEPLOYED
EFFECTIVELY
IN
THE
SAME
SYSTEM
HOWEVER
TECH
NIQUES
THAT
GENERALLY
ARE
COMPLEMENTARY
MIGHT
CLASH
ON
INDIVIDUAL
RESOURCES
OR
AT
POINTS
IN
TIME
A
SIMPLE
EXAMPLE
OF
THIS
WOULD
BE
OPPORTUNISTICALLY
SPINNING
DOWN
OF
SOME
DRIVES
WHILE
THE
SYSTEM
WAS
PLANNING
TO
MIGRATE
DATA
OFF
THE
DRIVES
TO
TURN
THEM
OFF
AN
OTHER
EXAMPLE
MIGHT
BE
LENGTHENING
SEEK
TIMES
ON
A
DRIVE
WHERE
WORKSHAPING
ALGORITHMS
ARE
GETTING
READY
TO
EXECUTE
A
BATCH
OF
I
O
REQUESTS
THE
LESSON
FROM
THIS
IS
THAT
AS
THESE
TECHNIQUES
ARE
IN
CORPORATED
INTO
A
SYSTEM
CAREFUL
ARCHITECTURE
IS
REQUIRED
TO
ENSURE
APPROPRIATE
DECISIONS
ABOUT
USAGE
OF
THE
TECHNIQUES
CONCLUSION
GIVEN
THE
INCREASING
ENERGY
CONSUMPTION
BY
STORAGE
SYSTEMS
AND
THE
AVAILABILITY
OF
SSDS
AND
NEWER
ENERGY
SAVING
MODES
THIS
PAPER
ARGUED
FOR
A
RENEWED
ATTENTION
TOWARDS
IMPROVING
STORAGE
ENERGY
CONSUMPTION
MORE
IMPORTANTLY
WE
SHOWED
THAT
ACHIEVING
ENERGY
PROPORTIONALITY
CAN
HAVE
SIGNIFICANT
BENEFIT
ABOVE
AND
BEYOND
USING
INDIVIDUAL
ENERGY
EFFICIENT
COMPONENTS
ANALYSIS
OF
ENTERPRISE
DATA
CENTER
WORKLOADS
INDICATED
REDUCTION
IN
STORAGE
ENERGY
UNDER
IDEAL
STORAGE
ENERGY
PROPORTIONALITY
WHILE
ANOTHER
INDICATED
A
PO
TENTIAL
REDUCTION
OUR
WORK
ALSO
OUTLINED
THE
CHALLENGES
AND
ANALYZED
TECHNIQUES
FOR
BUILDING
ENERGY
PROPORTIONAL
SYSTEMS
WHILE
ALIGNING
THE
TECHNIQUES
WITH
APPLICATION
REQUIREMENTS
AND
WORKLOAD
CHARACTERISTICS
BASED
ON
THE
TECHNIQUES
WE
HAVE
IDENTIFIED
WE
BE
LIEVE
THAT
ENERGY
PROPORTIONALITY
IS
ACHIEVABLE
IN
STORAGE
SYSTEMS
AT
A
TIME
SCALE
THAT
WILL
MAKE
SENSE
IN
REAL
WORLD
ENVIRONMENTS
CURRENTLY
WE
ARE
INVESTIGATING
POTENTIAL
ENERGY
SAVINGS
FROM
REAL
WORLD
ENTERPRISE
SETTINGS
AND
BUILDING
A
SIMULATION
INFRASTRUCTURE
TO
EXPERIMENTALLY
DEMONSTRATE
THE
UTILITY
OF
THE
TECHNIQUES
WE
HAVE
IDENTIFIED
WE
BELIEVE
THAT
STORAGE
ENERGY
MANAGEMENT
MUST
BE
COU
PLED
INTO
DATA
CENTER
ENERGY
MANAGEMENT
AND
ENERGY
PROPORTIONAL
ITY
HELPS
REALIZE
THAT
GOAL
WITHOUT
SUCH
A
INTEGRAL
APPROACH
ENERGY
MANAGEMENT
WILL
NOT
BE
ABLE
TO
APPROACH
OPTIMAL
CONSERVATION
THIS
PAPER
PRESENTS
THE
DESIGN
CHALLENGES
POSED
BY
A
NEW
CLASS
OF
ULTRA
LOW
POWER
DEVICES
REFERRED
TO
AS
ENERGY
HARVESTING
AC
TIVE
NETWORKED
TAGS
ENHANTS
ENHANTS
ARE
SMALL
FLEXIBLE
AND
SELF
RELIANT
IN
TERMS
OF
ENERGY
DEVICES
THAT
CAN
BE
ATTACHED
TO
OBJECTS
THAT
ARE
TRADITIONALLY
NOT
NETWORKED
E
G
BOOKS
CLOTHING
AND
PRODUCE
THEREBY
PROVIDING
THE
INFRASTRUCTURE
FOR
VARIOUS
NOVEL
TRACKING
APPLICATIONS
EXAMPLES
OF
THESE
APPLICATIONS
INCLUDE
LO
CATING
MISPLACED
ITEMS
CONTINUOUS
MONITORING
OF
OBJECTS
ITEMS
IN
A
STORE
BOXES
IN
TRANSIT
AND
DETERMINING
LOCATIONS
OF
DISASTER
SUR
VIVORS
RECENT
ADVANCES
IN
ULTRA
LOW
POWER
WIRELESS
COMMUNICA
TIONS
ULTRA
WIDEBAND
UWB
CIRCUIT
DESIGN
AND
ORGANIC
ELECTRONIC
HARVESTING
TECHNIQUES
WILL
ENABLE
THE
REALIZATION
OF
ENHANTS
IN
THE
NEAR
FUTURE
IN
ORDER
FOR
ENHANTS
TO
RELY
ON
HARVESTED
ENERGY
THEY
HAVE
TO
SPEND
SIGNIFICANTLY
LESS
ENERGY
THAN
BLUETOOTH
ZIG
BEE
AND
IEEE
DEVICES
MOREOVER
THE
HARVESTING
COM
PONENTS
AND
THE
ULTRA
LOW
POWER
PHYSICAL
LAYER
HAVE
SPECIAL
CHAR
ACTERISTICS
WHOSE
IMPLICATIONS
ON
THE
HIGHER
LAYERS
HAVE
YET
TO
BE
STUDIED
E
G
WHEN
USING
ULTRA
LOW
POWER
CIRCUITS
THE
ENERGY
RE
QUIRED
TO
RECEIVE
A
BIT
IS
AN
ORDER
OF
MAGNITUDE
HIGHER
THAN
THE
EN
ERGY
REQUIRED
TO
TRANSMIT
A
BIT
THESE
SPECIAL
CHARACTERISTICS
POSE
SEVERAL
NEW
CROSS
LAYER
RESEARCH
PROBLEMS
IN
THIS
PAPER
WE
DE
SCRIBE
THE
DESIGN
CHALLENGES
AT
THE
LAYERS
ABOVE
THE
PHYSICAL
LAYER
POINT
OUT
RELEVANT
RESEARCH
DIRECTIONS
AND
OUTLINE
POSSIBLE
STARTING
POINTS
FOR
SOLUTIONS
CATEGORIES
AND
SUBJECT
DESCRIPTORS
C
COMPUTER
COMMUNICATION
NETWORKS
NETWORK
ARCHI
TECTURE
AND
DESIGN
WIRELESS
COMMUNICATION
GENERAL
TERMS
ALGORITHMS
DESIGN
PERFORMANCE
KEYWORDS
ULTRA
LOW
POWER
COMMUNICATIONS
ENERGY
EFFICIENT
NETWORKING
EN
ERGY
HARVESTING
ENERGY
SCAVENGING
ULTRA
WIDEBAND
UWB
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
MOBICOM
SEPTEMBER
BEIJING
CHINA
COPYRIGHT
ACM
SENSOR
NETWORKS
ENHANTS
RFIDS
FIGURE
ENHANTS
IN
COMPARISON
TO
SENSOR
NETWORKS
AND
RFIDS
INTRODUCTION
THIS
PAPER
FOCUSES
ON
THE
NETWORKING
CHALLENGES
POSED
BY
A
NEW
CLASS
OF
ULTRA
LOW
POWER
DEVICES
THAT
WE
REFER
TO
AS
ENERGY
HARVESTING
ACTIVE
NETWORKED
TAGS
ENHANTS
ENHANTS
ARE
SMALL
FLEXIBLE
AND
SELF
RELIANT
IN
TERMS
OF
ENERGY
DEVICES
THAT
CAN
BE
ATTACHED
TO
ARBITRARY
OBJECTS
THAT
ARE
TRADITIONALLY
NOT
NETWORKED
BOOKS
CLOTHING
PRODUCE
ETC
ENHANTS
WILL
ENABLE
NOVEL
OBJECT
TRACKING
APPLICATIONS
SUCH
AS
RECOVERY
OF
LOST
ITEMS
AND
CONTINU
OUS
MONITORING
OF
OBJECTS
PROXIMITY
TO
EACH
OTHER
THE
REALIZA
TION
OF
ENHANTS
IS
BASED
ON
RECENT
ADVANCES
IN
THE
AREAS
OF
SOLAR
AND
PIEZOELECTRIC
ENERGY
HARVESTING
AS
WELL
AS
ULTRA
LOW
POWER
WIRELESS
COMMUNICATIONS
IN
PARTICULAR
RECENT
NOVEL
CIR
CUIT
DESIGNS
THAT
EMPLOY
ULTRA
WIDEBAND
UWB
COMMUNICATIONS
PROVIDE
NEW
LEVELS
OF
ULTRA
LOW
POWER
OPERATION
AT
THE
ORDERS
OF
NJ
BIT
AT
SHORT
RANGES
MOREOVER
SOLAR
ENERGY
HARVESTING
BASED
ON
ORGANIC
SEMICONDUCTORS
ALLOWS
HAVING
FLEXIBLE
SOLAR
PANELS
THEREBY
ALLOWING
A
PERVASIVE
USE
OF
TAGS
THE
WIRELESS
INDUSTRY
IS
ALREADY
TAKING
THE
FIRST
STEPS
TOWARDS
THE
DESIGN
OF
ENERGY
HARVESTING
ULTRA
LOW
POWER
TAGS
HENCE
FOL
LOWING
THE
TRANSITION
FROM
BARCODES
TO
RFIDS
WE
ENVISION
A
FUTURE
TRANSITION
FROM
RFIDS
TO
ENHANTS
THAT
NETWORK
ACTIVELY
COMMUNICATE
WITH
ONE
ANOTHER
AND
WITH
ENHANT
FRIENDLY
DEVICES
IN
ORDER
TO
FORWARD
INFORMATION
OVER
A
MULTIHOP
NETWORK
OPERATE
AT
ULTRA
LOW
POWER
SPEND
A
FEW
NANO
JOULES
OR
LESS
ON
EVERY
TRANSMITTED
BIT
HARVEST
ENERGY
COLLECT
AND
STORE
ENERGY
FROM
SOURCES
SUCH
AS
LIGHT
MOTION
AND
TEMPERATURE
GRADIENTS
ARE
ENERGY
ADAPTIVE
ALTER
COMMUNICATIONS
AND
NETWORK
ING
TO
SATISFY
ENERGY
AND
HARVESTING
CONSTRAINTS
EXCHANGE
SMALL
MESSAGES
EXCHANGE
LIMITED
INFORMATION
BASICALLY
IDS
USING
LOW
DATA
RATES
POSSIBLY
IN
SEVERAL
TRANS
MISSION
BURSTS
TRANSMIT
TO
SHORT
RANGES
COMMUNICATE
ONLY
WHEN
IN
CLOSE
PROXIMITY
TO
METERS
TO
ONE
ANOTHER
ARE
THIN
FLEXIBLE
AND
SMALL
A
FEW
SQUARE
CM
AT
MOST
AS
SHOWN
IN
FIGURE
IN
TERMS
OF
COMPLEXITY
THROUGHPUT
SIZE
AND
ENERGY
REQUIREMENTS
ENHANTS
FIT
BETWEEN
RFIDS
AND
SEN
SOR
NETWORKS
SIMILARLY
TO
RFIDS
THE
TAGS
CAN
BE
AFFIXED
TO
COM
MONPLACE
OBJECTS
HOWEVER
ENHANTS
WILL
HAVE
A
POWER
SOURCE
WILL
BE
ABLE
TO
COMMUNICATE
IN
DISTRIBUTED
MULTIHOP
FASHION
AND
WILL
NOT
HAVE
TO
RELY
ON
HIGH
POWER
READERS
COMPARED
TO
SEN
SOR
NODES
ENHANTS
WILL
OPERATE
AT
SIGNIFICANTLY
LOWER
DATA
RATES
AND
WILL
CONSUME
LESS
ENERGY
MOREOVER
UNLIKE
SENSOR
NODES
EN
HANTS
WILL
TRANSMIT
MOSTLY
ID
INFORMATION
EITHER
IN
ORDER
TO
AN
NOUNCE
THEMSELVES
OR
TO
QUERY
FOR
SPECIFIC
ENHANTS
DESPITE
THESE
DIFFERENCES
SOME
OF
THE
RESULTS
OBTAINED
FOR
SENSOR
NETWORKS
SEE
SHOULD
APPLY
TO
ENHANTS
ENHANTS
WILL
BE
ENABLERS
FOR
THE
INTERNET
OF
THINGS
AND
AS
SUCH
WILL
SUPPORT
A
VARIETY
OF
TRACKING
AND
MONITORING
APPLICATIONS
BE
YOND
WHAT
RFID
PERMITS
WHILE
RFIDS
MAKE
IT
POSSIBLE
TO
IDENTIFY
AN
OBJECT
ENHANTS
WILL
MAKE
IT
POSSIBLE
TO
SEARCH
FOR
AN
OBJECT
AND
TO
CONTINUOUSLY
TRACK
OBJECTS
WHEREABOUTS
AND
THEIR
PROXIM
ITY
TO
EACH
OTHER
RFIDS
ARE
TYPICALLY
ACTIVATED
ONLY
WHEN
PLACED
NEAR
A
READER
AND
ONLY
REPORT
ON
THEMSELVES
ENHANTS
ON
THE
OTHER
HAND
CAN
OPERATE
CONTINUOUSLY
ACHIEVE
PERVASIVE
COVERAGE
DUE
TO
THEIR
NETWORKING
CAPABILITIES
AND
CAN
REPORT
ON
THEMSELVES
AND
OTHER
ENHANTS
AROUND
THEM
THESE
ENHANTS
CAPABILITIES
ENABLE
MANY
EXCITING
APPLICATIONS
SUCH
AS
FOR
EXAMPLE
CONTINU
OUS
PEER
MONITORING
OF
MERCHANDIZE
IN
TRANSIT
WHERE
ENHANTS
WOULD
BE
ABLE
TO
IDENTIFY
IF
A
PARTICULAR
BOX
HAS
BEEN
TAKEN
OUT
AT
ANY
POINT
DURING
THE
JOURNEY
ONE
APPLICATION
THAT
WE
PLAN
TO
DEMONSTRATE
IN
THE
NEAR
FUTURE
IS
A
MISPLACED
LIBRARY
BOOK
LOCATOR
THE
INITIAL
PROTOTYPE
WILL
ENABLE
LIBRARY
BOOKS
TO
IDENTIFY
THOSE
AMONG
THEMSELVES
THAT
ARE
SIGNIFI
CANTLY
MISPLACED
E
G
IN
AN
INCORRECT
SECTION
AND
REPORT
THE
MIS
PLACEMENT
TO
ACCOMPLISH
THIS
TASK
EACH
BOOK
IS
ASSIGNED
A
UNIQUE
ID
USING
AN
ASSIGNMENT
SCHEME
CLOSELY
RELATED
TO
THE
DEWEY
DECI
MAL
CLASSIFICATION
EACH
BOOK
HAS
A
SOLAR
POWERED
TAG
WHOSE
POWER
OUTPUT
IS
SUFFICIENT
TO
TRANSMIT
AND
RECEIVE
INFORMATION
WITHIN
A
RADIUS
OF
ONE
METER
OR
LESS
AND
TO
PERFORM
SOME
BASIC
PROCESS
ING
NEARBY
BOOKS
WIRELESSLY
EXCHANGE
IDS
AND
IDS
OF
BOOKS
THAT
APPEAR
OUT
OF
PLACE
ARE
FURTHER
FORWARDED
THROUGH
THE
NETWORK
OF
BOOKS
EVENTUALLY
PROPAGATING
TO
SINK
NODES
A
LONG
TERM
OBJECTIVE
COULD
BE
TO
PLACE
BOOKS
IN
AN
ARBITRARY
ORDER
AND
BOTH
DETERMINE
THE
BOOK
ORDER
AND
PROPAGATE
THAT
INFORMATION
TO
A
CENTRAL
SERVER
USING
THE
MULTIHOP
WIRELESS
NETWORK
THIS
TYPE
OF
SYSTEM
CAN
SIG
NIFICANTLY
SIMPLIFY
THE
ORGANIZATION
OF
PHYSICAL
OBJECTS
THE
SAME
BUILDING
BLOCKS
USED
IN
THE
LIBRARY
APPLICATION
CAN
EN
ABLE
SEVERAL
OTHER
APPLICATIONS
IN
PARTICULAR
A
LARGE
VARIETY
OF
ITEMS
CAN
BE
TRACKED
AND
A
RANGE
OF
POSSIBLE
DESIRABLE
OR
UNDESIRABLE
CON
FIGURATIONS
OF
OBJECTS
CAN
BE
QUERIED
FOR
AND
CAN
TRIGGER
REPORTS
EXAMPLES
INCLUDE
FINDING
ITEMS
WITH
PARTICULAR
CHARACTERISTICS
IN
A
STORE
LOCATING
MISPLACED
ITEMS
E
G
KEYS
OR
EYEGLASSES
AND
LO
CATING
SURVIVORS
OF
DISASTERS
SUCH
AS
STRUCTURAL
COLLAPSE
TO
ENABLE
THESE
APPLICATIONS
VARIOUS
PROTOCOLS
HAVE
TO
BE
DE
SIGNED
ALTHOUGH
NETWORKING
PROTOCOLS
FOR
ENERGY
HARVESTING
NODES
RECENTLY
STARTED
GAINING
ATTENTION
SEE
SECTION
TO
THE
BEST
OF
OUR
KNOWLEDGE
THE
CROSS
LAYER
IN
TERACTIONS
BETWEEN
CIRCUIT
DESIGN
ENERGY
HARVESTING
COMMUNICA
TIONS
AND
NETWORKING
HAVE
NOT
BEEN
STUDIED
IN
DEPTH
CURRENT
RF
TRANSCEIVER
DESIGNS
COMMUNICATION
AND
NETWORKING
PROTOCOLS
AND
ENERGY
HARVESTING
AND
MANAGEMENT
TECHNIQUES
HAVE
BEEN
DEVEL
OPED
IN
ISOLATION
AND
ARE
INADEQUATE
FOR
THE
ENVISIONED
ENHANTS
APPLICATIONS
HENCE
BASED
ON
OUR
EXPERIENCE
WITH
HARDWARE
DE
SIGN
COMMUNICATIONS
AND
NETWORKING
WE
OUTLINE
THE
CROSS
LAYER
DESIGN
CHALLENGES
THAT
ARE
POSED
BY
THIS
NEW
TECHNOLOGY
WE
NOTE
THAT
ALTHOUGH
THERE
ARE
MANY
HARDWARE
SPECIFIC
DESIGN
CHALLENGES
THEY
ARE
OUT
OF
SCOPE
FOR
THIS
PAPER
ENHANTS
ARE
LIKELY
TO
BE
IMPLEMENTED
BY
COMBINING
FLEXIBLE
ELECTRONICS
TECHNOLOGIES
A
K
A
ORGANIC
ELECTRONICS
WITH
CMOS
CHIPS
SUPPORTING
IMPULSE
RADIO
UWB
FLEXIBLE
TECHNOLOGIES
CAN
REALIZE
ENERGY
HARVESTERS
SOLAR
CELLS
PIEZOELECTRIC
THERMAL
ETC
PASSIVE
RF
COMPONENTS
AND
BATTERIES
BY
EMBEDDING
CMOS
CHIPS
IN
ENHANTS
VERY
LOW
POWER
COMPUTATION
MEMORY
AND
COMMU
NICATION
FUNCTIONS
CAN
BE
ADDED
THIS
TECHNOLOGY
PLATFORM
ALLOWS
FOR
THIN
FLEXIBLE
AND
VERY
LOW
COST
ENHANT
FABRICATION
BY
USING
IMPULSE
RADIO
UWB
DATA
IS
ENCODED
BY
VERY
SHORT
PULSES
AT
THE
ORDER
OF
NANO
SECONDS
AND
THE
ENERGY
CONSUMPTION
IS
VERY
LOW
TO
BETTER
UNDERSTAND
THE
HIGHER
LAYER
DESIGN
CHALLENGES
WE
FIRST
DISCUSS
ENERGY
HARVESTING
AND
ENERGY
STORAGE
TECHNIQUES
ENERGY
STORAGE
IS
REQUIRED
IN
ORDER
TO
USE
THE
HARVESTED
ENERGY
IN
PERIODS
IN
WHICH
HARVESTING
IS
NOT
POSSIBLE
WE
LATER
DESCRIBE
MODELS
THAT
TAKE
THESE
TECHNIQUES
AND
THEIR
CHARACTERISTICS
INTO
ACCOUNT
WHEN
DETERMINING
THE
UWB
PULSE
PATTERNS
DUTY
CYCLES
AND
OVERALL
EN
ERGY
CONSUMPTION
SOME
OF
THE
INTERESTING
CHARACTERISTICS
INCLUDE
THE
DIFFERENCES
IN
PERFORMANCE
BETWEEN
A
CAPACITOR
AND
A
BATTERY
AND
BETWEEN
INDOOR
AND
OUTDOOR
SOLAR
ENERGY
HARVESTING
NEXT
WE
DISCUSS
ULTRA
LOW
POWER
UWB
COMMUNICATIONS
AND
PRESENT
A
NUMBER
OF
DESIGN
CHALLENGES
THESE
INCLUDE
A
PARADIGM
SHIFT
RESULTING
FROM
THE
FACT
THAT
WHEN
USING
ULTRA
LOW
POWER
COM
MUNICATIONS
IT
IS
ENERGETICALLY
CHEAPER
TO
TRANSMIT
DATA
THAN
TO
RE
CEIVE
DATA
MOREOVER
ULTRA
LOW
POWER
TAGS
WILL
OPERATE
WITH
INAC
CURATE
CLOCKS
THEREBY
REQUIRING
THE
REDESIGN
OF
LOW
POWER
METHODS
BASED
ON
COORDINATION
OF
WAKEUP
PERIODS
FINALLY
ENHANTS
MAY
HAVE
ADDITIONAL
CIRCUITRY
E
G
ACCURATE
CLOCKS
THAT
CAN
BE
POW
ERED
UP
IN
SOME
HARVESTING
STATES
AND
CAN
BE
USED
TO
ASSIST
OTHER
ENHANTS
DETERMINING
HOW
AND
WHEN
TO
USE
THIS
CIRCUITRY
IS
AN
OTHER
CHALLENGE
WE
THEN
FOCUS
ON
THE
DESIGN
OF
ENHANTS
COMMUNICATIONS
AND
NETWORKING
PROTOCOLS
THESE
PROTOCOLS
HAVE
TO
DETERMINE
THE
STATE
OF
THE
TAGS
SLEEPING
COMMUNICATING
ETC
THE
OPERATION
WITHIN
A
STATE
TRANSMIT
RECEIVE
RATE
CONTROL
ETC
AND
THE
COORDINATION
WITH
PEER
TAGS
ALL
THESE
HAVE
TO
BE
BASED
ON
THE
HARVESTING
STATES
OF
THE
TAGS
AND
THEIR
CAPABILITIES
CLEARLY
ENERGY
HARVESTING
SHIFTS
THE
NA
TURE
OF
ENERGY
AWARE
PROTOCOLS
FROM
PROLONGING
THE
FINITE
LIFESPAN
OF
A
DEVICE
TO
ENABLING
PERPETUAL
LIFE
AND
FROM
MINIMIZING
ENERGY
EXPENDITURE
TO
OPTIMIZING
IT
THE
CHALLENGES
THAT
WE
DESCRIBE
IN
CLUDE
NOT
ONLY
DETERMINING
THE
STATE
AND
THE
OPERATION
MODEL
WITHIN
THE
STATE
BUT
ALSO
THE
USE
OF
A
HARVESTING
CHANNEL
AS
A
MEAN
FOR
NODES
SYNCHRONIZATION
FINALLY
WE
DRAW
PARALLELS
BETWEEN
ENERGY
HARVESTING
ENHANTS
AND
LARGE
SCALE
MANUFACTURING
SYSTEMS
WE
SHOW
THAT
THE
HARVESTED
ENERGY
CAN
BE
TREATED
SIMILARLY
TO
INVENTORY
IN
PRODUCTION
AND
STOR
AGE
SYSTEMS
WE
SHOW
HOW
INVENTORY
CONTROL
MODELS
EXTENSIVELY
STUDIED
IN
THE
INVENTORY
MANAGEMENT
FIELD
APPLY
TO
EN
HANTS
WE
THEN
DISCUSS
THE
VARIOUS
CHALLENGES
RESULTING
FROM
THE
FACT
THAT
ENHANTS
COMPOSE
A
DISTRIBUTED
NETWORK
WITH
MANY
STOCHASTIC
COMPONENTS
AND
OUTLINE
A
NUMBER
OF
OPEN
PROBLEMS
WE
ARE
CURRENTLY
BUILDING
A
TESTBED
OF
ENHANTS
IN
WHICH
WE
WILL
EVALUATE
VARIOUS
APPROACHES
USING
ENERGY
HARVESTING
AND
ULTRA
LOW
POWER
HARDWARE
WE
CONCLUDE
BY
BRIEFLY
DESCRIBING
THE
IMPLE
MENTATION
PHASES
AND
THE
HARDWARE
COMPONENTS
THIS
PAPER
IS
ORGANIZED
AS
FOLLOWS
IN
SECTION
WE
DESCRIBE
RELATED
WORK
IN
SECTIONS
AND
WE
DISCUSS
ENERGY
HARVESTING
EN
ERGY
STORAGE
ULTRA
LOW
POWER
COMMUNICATIONS
AND
THE
CHALLENGES
THEY
POSE
FOR
DESIGNING
HIGHER
LAYER
PROTOCOLS
SECTION
DESCRIBES
THE
CHALLENGES
POSED
BY
ENHANTS
COMMUNICATIONS
AND
NETWORK
ING
AND
SECTION
DISCUSSES
THE
APPLICATION
OF
INVENTORY
CONTROL
MODELS
TO
ENHANTS
FINALLY
IN
SECTION
WE
PRESENT
OUR
PLAN
FOR
DESIGNING
AN
ENHANTS
TESTBED
RELATED
WORK
WHILE
THE
IDEA
OF
PERVASIVE
NETWORKS
OF
OBJECTS
HAS
BEEN
PRO
POSED
BEFORE
E
G
IN
THE
SMART
DUST
PROJECT
THE
HARVEST
ING
AND
COMMUNICATIONS
TECHNOLOGIES
HAVE
RECENTLY
REACHED
A
POINT
WHERE
NETWORKED
ENERGETICALLY
SELF
RELIANT
TAGS
ARE
BECOMING
PRAC
TICAL
AS
MENTIONED
ABOVE
ENHANTS
FALL
BETWEEN
RFIDS
AND
SENSOR
NETWORKS
TO
OUR
KNOWLEDGE
MOST
OF
THE
NETWORKING
RESEARCH
IN
THE
AREA
OF
RFID
FOCUSES
ON
THE
SCHEDULING
OF
QUERY
RESPONSES
BY
PASSIVE
TAGS
E
G
EXTENSIVELY
STUDIED
SENSOR
NETWORKS
ARE
DESIGNED
TO
DEAL
WITH
ENERGY
BANDWIDTH
AND
OTHER
RESOURCE
CONSTRAINTS
SEE
AND
REFERENCES
THEREIN
YET
THE
UNDERLYING
COMMUNICATION
MECHANISMS
DRAW
POWER
AT
RATES
THAT
ARE
TOO
HIGH
IN
ENVIRONMENTS
WITH
WEAK
ENERGY
SOURCES
INDOOR
LIGHTING
STRAIN
VIBRATION
ENHANTS
WILL
EMPLOY
IMPULSE
RADIO
UWB
COMMUNICATIONS
WHOSE
CORRESPONDING
MAC
PROTOCOLS
E
G
HAVE
BEEN
PROPOSED
FOR
COMMUNICATIONS
AND
ACCURATE
RANGING
IN
SENSOR
NET
WORKS
THE
RECENT
IEEE
STANDARD
IS
BASED
ON
IM
PULSE
RADIO
UWB
AND
INHERITS
MANY
OF
THE
IEEE
ZIG
BEE
FUNCTIONALITIES
LIMITED
HARDWARE
DATA
IS
CURRENTLY
AVAILABLE
BUT
WE
CAN
ASSUME
THAT
THE
OVERHEAD
TO
PROVIDE
RANGING
HIGH
DATA
RATES
UP
TO
AND
BACKWARD
COMPATIBILITY
WILL
LEAD
TO
EN
ERGY
CONSUMPTION
LARGELY
EXCEEDING
THE
ENERGY
WE
ENVISION
AVAIL
ABLE
IN
ENHANTS
ENERGY
EFFICIENCY
IN
WIRELESS
NETWORKS
HAS
LONG
BEEN
A
SUBJECT
OF
RESEARCH
SEE
REVIEWS
IN
COMPARISON
ONLY
A
FEW
WORKS
HAVE
CONSIDERED
ENERGY
HARVESTING
ONE
OF
THE
MAJOR
DIRECTIONS
IN
EXPLOITING
ENERGY
HARVESTING
IS
ADAPTIVE
DUTY
CYCLING
IN
SENSOR
NETWORKS
IN
PARTICULAR
DESCRIBE
ADAPTATION
OF
THE
DUTY
CYCLE
TO
THE
CHARACTERISTICS
OF
A
PERIODIC
ENERGY
SOURCE
AND
DE
VELOPS
A
BATTERY
CENTRIC
NOT
REQUIRING
AN
ENERGY
SOURCE
MODEL
DUTY
CYCLE
ADAPTATION
ALGORITHM
TAKING
ENERGY
HARVESTING
INTO
AC
COUNT
WHEN
MAKING
ROUTING
DECISIONS
IS
STUDIED
IN
RE
CENTLY
JOINTLY
CALCULATES
LINK
FLOWS
AND
DATA
COLLECTION
RATES
FOR
ENERGY
HARVESTING
SENSOR
NETWORKS
DYNAMIC
ACTIVATION
OF
ENERGY
HARVESTING
SENSORS
HAS
BEEN
STUD
IED
IN
A
SYSTEM
WHERE
MOBILE
NODES
DELIVER
HAR
VESTED
ENERGY
TO
DIFFERENT
AREAS
OF
THE
NETWORK
IS
PRESENTED
IN
A
SYSTEM
WHERE
AN
ENERGY
RESTRICTED
NODE
CAN
OUTSOURCE
PACKET
RETRANSMISSIONS
IS
DESCRIBED
IN
A
POWER
SUBSYSTEM
FOR
HYDRO
WATCH
IS
DESCRIBED
IN
WHICH
ALSO
PROVIDES
AN
OVERVIEW
OF
DE
PLOYMENTS
THAT
RELY
ON
ENERGY
HARVESTING
TO
THE
BEST
OF
OUR
KNOWL
EDGE
CURRENT
DEPLOYMENTS
USE
MUCH
MORE
ENERGY
THAN
ENHANTS
WILL
HAVE
AVAILABLE
FINALLY
THERE
IS
AN
INCREASING
INDUSTRY
INTEREST
IN
BRINGING
TO
GETHER
LOW
POWER
COMMUNICATIONS
AND
ENERGY
HARVESTING
E
G
PARTICULARLY
TEXAS
INSTRUMENTS
HAS
RECENTLY
PUT
ON
THE
MAR
KET
A
SOLAR
ENERGY
HARVESTING
DEVELOPMENT
KIT
GEARED
TOWARDS
SENSOR
DEVELOPMENTS
ALTHOUGH
IT
IS
A
FIRST
STEP
TOWARDS
ENHANTS
THIS
KIT
IS
MADE
OF
RIGID
MATERIALS
AND
IS
MORE
THAN
THE
SIZE
OF
THE
ENHANTS
ENVISIONED
IN
THIS
WORK
ENERGY
HARVESTING
IN
ORDER
TO
OUTLINE
THE
HIGHER
LAYER
DESIGN
CHALLENGES
WE
BRIEFLY
DESCRIBE
TWO
MAJOR
HARVESTING
METHODS
AND
POSSIBLE
ENERGY
STORAGE
METHODS
THEN
WE
DISCUSS
THE
EFFECTS
OF
THE
DIFFERENT
METHODS
ON
THE
DESIGN
AND
OPERATION
OF
HIGHER
LAYER
PROTOCOLS
IN
SECTIONS
FIGURE
AN
ORGANIC
SEMICONDUCTOR
BASED
SMALL
MOLECULE
SO
LAR
CELL
SERIES
ARRAY
DEVELOPED
IN
THE
COLUMBIA
LABORATORY
FOR
UNCONVENTIONAL
ELECTRONICS
CLUE
AND
WE
DISCUSS
DIFFERENT
APPROACHES
TO
THE
DESIGN
OF
SUCH
HIGHER
LAYER
PROTOCOLS
ENERGY
SOURCES
MANY
ENVIRONMENTAL
SOURCES
OF
ENERGY
ARE
POTENTIALLY
AVAILABLE
FOR
HARVESTING
BY
SMALL
DEVICES
THESE
INCLUDE
TEMPERATURE
DIF
FERENCES
ELECTROMAGNETIC
ENERGY
AIRFLOW
AND
VIBRATIONS
BELOW
WE
FOCUS
ON
THE
MOST
PROMISING
HARVESTING
TECHNOLOGIES
FOR
ENHANTS
SOLAR
ENERGY
AND
PIEZOELECTRIC
MOTION
HARVESTING
OTHER
HARVESTING
TECHNOLOGIES
POSE
SIMILAR
DESIGN
CHALLENGES
SOLAR
ENERGY
LIGHT
IS
ONE
OF
THE
MOST
USEFUL
ENERGY
SOURCES
WITH
TYPICAL
IRRADIANCE
TOTAL
ENERGY
PROJECTED
AND
AVAILABLE
FOR
COL
LECTION
RANGING
FROM
IN
DIRECT
SUNLIGHT
TO
IN
BRIGHTLY
LIT
RESIDENTIAL
INDOOR
ENVIRONMENTS
NOTICE
THE
SIGNIFICANT
DIFFERENCE
OFFICE
RETAIL
AND
LABORATORY
ENVIRONMENTS
ARE
TYPICALLY
BRIGHTER
THAN
RESIDENTIAL
SETTINGS
BUT
GET
MUCH
LESS
LIGHT
ENERGY
THAN
OUTDOOR
ENVIRONMENTS
THE
EFFICIENCY
OF
A
SOLAR
EN
ERGY
HARVESTING
DEVICE
IS
DEFINED
AS
THE
PERCENTAGE
OF
THE
AVAIL
ABLE
ENERGY
THAT
IS
ACTUALLY
HARVESTED
CONVENTIONAL
SINGLE
CRYSTAL
AND
POLYCRYSTALLINE
SOLAR
CELLS
SUCH
AS
THOSE
THAT
ARE
COMMONLY
USED
IN
CALCULATORS
HAVE
EFFICIENCY
OF
AROUND
IN
DIRECT
SUNLIGHT
HOWEVER
THEIR
EFFICIENCY
DECLINES
WITH
A
DECLINE
IN
ENERGY
AVAILABILITY
THEY
ARE
LESS
EFFICIENT
WITH
DIMMER
SOURCES
WHICH
IS
IMPORTANT
TO
NOTE
DUE
TO
CONSIDERABLE
IRRADIANCE
DIFFERENCE
BETWEEN
DIRECT
SUNLIGHT
AND
INDOOR
ILLUMINATION
CONVENTIONAL
SO
LAR
PANELS
ARE
ALSO
INFLEXIBLE
RIGID
WHICH
MAKES
IT
DIFFICULT
TO
AT
TACH
THEM
TO
NON
RIGID
ITEMS
SUCH
AS
CLOTHING
AND
PAPERBACK
BOOKS
AN
EMERGING
AND
LESS
EXPLORED
OPTION
IS
SOLAR
ENERGY
HARVEST
ING
BASED
ON
ORGANIC
SEMICONDUCTORS
AN
ARRAY
OF
ORGANIC
SOLAR
CELLS
THAT
WE
RECENTLY
DESIGNED
IS
SHOWN
IN
FIGURE
WITH
THIS
TECHNOLOGY
SOLAR
CELLS
CAN
BE
MADE
FLEXIBLE
MOREOVER
ORGANIC
SEMICONDUCTOR
BASED
PANELS
OPERATE
WITH
CONSTANT
EFFICIENCIES
OVER
DIFFERENT
BRIGHTNESS
LEVELS
HOWEVER
THEIR
EFFICIENCY
IS
TYPICALLY
WHICH
IS
MUCH
LOWER
THAN
THE
EFFICIENCY
OF
CONVEN
TIONAL
INORGANIC
SOLAR
PANELS
TO
PUT
THE
NUMBERS
IN
PERSPECTIVE
CONSIDER
A
SYSTEM
WITH
A
ORGANIC
SEMICONDUCTOR
CELL
OUTDOORS
THE
SYSTEM
WILL
HAR
VEST
UNDER
THE
ASSUMPTION
THAT
RECEPTION
OF
A
SINGLE
BIT
REQUIRES
THE
ACHIEVABLE
DATA
RATE
WILL
BE
THE
ACHIEVABLE
DATA
RATE
WITH
INDOOR
LIGHTING
WILL
BE
ANOTHER
POTENTIAL
SOURCE
OF
ENERGY
IS
PIEZOELECTRIC
MOTION
EN
ERGY
IT
CAN
BE
GENERATED
BY
STRAINING
A
MATERIAL
E
G
SQUEEZ
ING
OR
BENDING
FLEXIBLE
ITEMS
AN
EXAMPLE
IS
ENERGY
HARVESTING
THROUGH
FOOTFALL
WHERE
A
HARVESTING
DEVICE
IS
PLACED
IN
A
SHOE
AND
PIEZOELECTRIC
ENERGY
IS
GENERATED
AND
CAPTURED
WITH
EACH
STEP
THAT
RECEPTION
IS
MORE
EXPENSIVE
THAN
TRANSMISSION
FOR
MORE
DETAILS
SEE
SECTION
E
CONSUMPTION
RATE
E
WATTS
NOTE
THAT
THE
ENERGY
CHARGE
RATE
R
DE
PENDS
BOTH
ON
THE
HARVESTING
RATE
AND
THE
PROPERTIES
OF
THE
ENERGY
R
STORAGE
FOR
EXAMPLE
WHEN
A
BATTERY
IS
USED
R
IS
POSITIVE
ONLY
WHEN
THE
VOLTAGE
AT
THE
ENERGY
HARVESTING
COMPONENT
EXCEEDS
THE
C
INTERNAL
CHEMICAL
POTENTIAL
OF
THE
BATTERY
WHEN
A
CAPACITOR
IS
USED
FIGURE
AN
ABSTRACTION
OF
AN
ENERGY
HARVESTING
SYSTEM
FOR
THE
UPPER
LAYERS
THE
ENERGY
STORAGE
CAPACITY
C
THE
CURRENT
ENERGY
LEVEL
E
THE
ENERGY
CHARGE
RATE
R
AND
THE
ENERGY
CONSUMPTION
RATE
E
UNLIKE
SOLAR
HARVESTING
PIEZOELECTRIC
HARVESTING
MAY
BE
SOMEWHAT
CONTROLLED
BY
THE
USER
PIEZOELECTRIC
HARVESTING
IS
CHARACTERIZED
BY
THE
ENERGY
CAPTURED
PER
ACTUATION
AT
A
PARTICULAR
STRAIN
USUALLY
IN
IT
WAS
SHOWN
HOW
TO
HARVEST
PER
DEFLECTION
WITH
A
STRAIN
OF
AP
PROXIMATELY
FROM
STRAINING
POLYVINYLIDENE
FLUORIDE
PVDF
A
HIGHLY
COMPLIANT
PIEZOELECTRIC
POLYMER
ASSUME
THAT
A
OF
MATERIAL
IS
EMPLOYED
IN
AN
ENVIRONMENT
WHERE
IT
IS
STRAINED
TIMES
PER
SECOND
THIS
WOULD
PROVIDE
IF
SIMILAR
TO
ABOVE
WE
ASSUME
THAT
TRANSMISSION
OF
A
BIT
COSTS
THE
BIT
RATE
THAT
CAN
BE
SUPPORTED
IS
ENERGY
STORAGE
WITHOUT
THE
ABILITY
TO
STORE
ENERGY
A
DEVICE
CAN
OPERATE
ONLY
WHEN
DIRECTLY
POWERED
BY
ENVIRONMENTAL
ENERGY
FOR
A
TAG
ENERGY
STORAGE
COMPONENTS
NEED
TO
BE
COMPACT
AND
EFFICIENT
AND
NEED
TO
HAVE
VERY
LOW
SELF
DISCHARGE
RATES
FROM
A
HIGHER
LAYER
POINT
OF
VIEW
IT
IS
ALSO
IMPORTANT
TO
HAVE
STORAGE
ELEMENTS
THAT
ARE
STRAIGHT
FORWARD
TO
MEASURE
AND
CONTROL
RECHARGEABLE
BATTERIES
ARE
AN
EXCELLENT
OPTION
FOR
ENERGY
STOR
AGE
AND
NUMEROUS
BATTERY
OPTIONS
ARE
AVAILABLE
THIN
FILM
BATTERIES
ARE
PARTICULARLY
ATTRACTIVE
FOR
ENHANTS
SINCE
THEY
ARE
ENVIRONMEN
TALLY
FRIENDLY
AND
CAN
BE
MADE
FLEXIBLE
HOWEVER
A
BATTERY
NEEDS
TO
BE
SUPPLIED
WITH
A
VOLTAGE
EXCEEDING
THE
INTERNAL
CHEMICAL
POTENTIAL
TYPICALLY
IN
ORDER
TO
START
STORING
PROVIDED
ENERGY
THIS
IMPLIES
THAT
CHARGE
GENERATED
AT
A
LOW
VOLTAGE
SUCH
AS
THAT
POSSI
BLY
PRODUCED
DURING
LOW
HARVESTER
EXCITATION
FOR
EXAMPLE
WHEN
A
SOLAR
CELL
IS
LOCATED
IN
A
VERY
DIMLY
LIT
PLACE
CANNOT
BE
STORED
WITHOUT
VOLTAGE
UPCONVERSION
CAPACITORS
CAN
ALSO
BE
USED
FOR
ENERGY
STORAGE
CAPACITORS
CAN
RECEIVE
ANY
CHARGE
WHICH
EXCEEDS
THEIR
STORED
VOLTAGE
AND
BE
CY
CLED
MANY
MORE
TIMES
THAN
BATTERIES
THE
DISADVANTAGE
OF
USING
CAPACITORS
HOWEVER
IS
THAT
AS
A
CAPACITOR
GETS
MORE
CHARGED
IT
BE
COMES
MORE
DIFFICULT
TO
ADD
CHARGE
AND
LARGE
ELECTROLYTIC
CAPACITORS
SELF
DISCHARGE
OVER
HOURS
OR
DAYS
THE
ENERGY
DENSITY
HOW
MUCH
ENERGY
CAN
BE
STORED
PER
UNIT
OF
VOLUME
OF
CAPACITORS
IS
ALSO
MUCH
LOWER
A
TYPICAL
BATTERY
CAN
STORE
ABOUT
WHEREAS
HIGH
PERFORMANCE
CERAMIC
CAPACITORS
CAN
STORE
DIFFERENT
ENHANTS
APPLICATIONS
WILL
REQUIRE
DIFFERENT
TYPES
OF
ENERGY
STORAGE
FOR
EXAMPLE
A
TAG
WHICH
FREQUENTLY
EXPERIENCES
SHALLOW
CHARGING
AND
DISCHARGING
EVENTS
WILL
NEED
A
CAPACITOR
FOR
AN
ACCEPTABLE
LIFETIME
WHEREAS
AN
ENHANT
THAT
NEEDS
TO
OPER
ATE
FOR
A
LONG
PERIOD
OF
TIME
WITHOUT
RECHARGING
AND
TO
STORE
LARGE
AMOUNTS
OF
ENERGY
SAY
TO
CHARGE
ALL
DAY
AND
DISCHARGE
ALL
NIGHT
WILL
NEED
THE
ENERGY
DENSITY
THAT
A
BATTERY
OFFERS
HIGHER
LAYER
VIEW
OF
HARVESTING
THE
COMPLEXITY
OF
THE
HARVESTING
SYSTEM
NEEDS
TO
BE
CAPTURED
IN
A
RELATIVELY
SIMPLE
WAY
FOR
THE
HIGHER
LAYERS
A
POSSIBLE
ABSTRACTION
IS
SHOWN
IN
FIGURE
THE
SYSTEM
IS
CHARACTERIZED
BY
THE
MAXIMUM
ENERGY
STORAGE
CAPACITY
C
JOULES
THE
CURRENTLY
AVAILABLE
ENERGY
LEVEL
E
JOULES
THE
ENERGY
CHARGE
RATE
R
WATTS
AND
THE
ENERGY
THE
RELATIONSHIP
OF
R
AND
ENERGY
HARVESTING
RATE
VARIES
WITH
E
THE
ENERGY
CONSUMPTION
RATE
E
IS
CONTROLLED
BY
HIGHER
LAYER
ALGORITHMS
AND
WITH
LOW
DUTY
CYCLE
IS
MOSTLY
AFFECTED
BY
COMMUNICATION
AND
NETWORKING
PROTOCOLS
THE
EFFECT
OF
THESE
PROTOCOLS
ON
E
WILL
BE
DISCUSSED
IN
SECTIONS
AND
THE
AVAILABLE
ENERGY
E
AND
ENERGY
CHARGE
RATE
R
CAN
BE
MEA
SURED
DIRECTLY
FROM
THE
ENERGY
HARVESTING
AND
POWER
CONDITIONING
COMPONENTS
THE
MAXIMUM
STORAGE
CAPACITY
C
SHOULD
BE
KNOWN
NOMINALLY
FROM
THE
TAG
DESIGN
BUT
MORE
PRECISE
CHARACTERIZATION
IS
POSSIBLE
FOR
EXAMPLE
IF
A
BATTERY
IS
USED
C
CAN
BE
PROJECTED
FROM
THE
BATTERY
AGE
THE
STORAGE
TEMPERATURE
HISTORY
AND
THE
CHARGE
LEVEL
OVER
TIME
AS
THE
BATTERY
CONTINUES
TO
AGE
AND
CYCLE
THROUGH
CHARGE
DISCHARGE
EVENTS
THE
CAPACITY
WILL
PREDICTABLY
DECREASE
THE
R
VALUES
OF
DIFFERENT
ENHANTS
OPERATING
IN
THE
SAME
ENVI
RONMENT
WILL
BE
SIGNIFICANTLY
DIFFERENT
OUR
EXPERIMENTS
WITH
COM
MERCIAL
HARDWARE
SHOW
A
LOT
OF
VARIABILITY
IN
THE
HARVESTING
RATES
OF
IDENTICAL
HARVESTING
DEVICES
UNDER
IDENTICAL
LIGHT
CONDITIONS
I
E
IDENTICAL
SOLAR
CELLS
IN
THE
SAME
LOCATION
HARVEST
ENERGY
AT
RATES
THAT
CAN
DIFFER
BY
ABOUT
IN
ADDITION
THE
R
VALUES
WILL
DIFFER
FOR
CLOSELY
LOCATED
SOLAR
CELLS
DUE
TO
DIFFERENCES
IN
HOW
THE
CELLS
ARE
LOCATED
WITH
RESPECT
TO
LIGHT
SOURCES
AND
OBSTACLES
LOW
POWER
COMMUNICATIONS
ULTRA
WIDE
BAND
UWB
IMPULSE
RADIO
IR
IS
A
COMPELLING
TECH
NOLOGY
FOR
SHORT
RANGE
ULTRA
LOW
POWER
WIRELESS
COMMUNICATIONS
IT
USES
VERY
SHORT
PULSES
ON
THE
ORDER
OF
NANO
SECONDS
THAT
ARE
TRANSMITTED
AT
REGULAR
TIME
INTERVALS
WITH
THE
DATA
ENCODED
IN
THE
PULSE
AMPLITUDE
PHASE
FREQUENCY
OR
POSITION
AT
LOW
DATA
RATES
THE
SHORT
DURATION
OF
THE
PULSES
ALLOWS
MOST
CIRCUITRY
IN
THE
TRANSMITTER
OR
RECEIVER
TO
BE
SHUT
DOWN
BETWEEN
PULSES
RESULTING
IN
SIGNIFICANT
POWER
SAVINGS
COMPARED
TO
NARROW
BAND
SYSTEMS
PRACTICAL
CMOS
IR
CIRCUITS
WITH
ENERGY
CONSUMPTION
ON
THE
OR
DER
OF
A
NJ
PER
BIT
HAVE
BEEN
RECENTLY
DEMONSTRATED
FOR
EXAM
PLE
IN
A
UWB
RECEIVER
AND
TRANSMITTER
REQUIRE
BIT
AND
BIT
RESPECTIVELY
AT
A
PULSE
RATE
OF
RECENT
PUB
LICATIONS
AS
WELL
AS
OUR
ONGOING
RESEARCH
DEMONSTRATE
THAT
UWB
IR
TRANSCEIVERS
IN
THE
BAND
WITH
DATA
RATES
IN
THE
TO
RANGE
AND
A
TRANSMITTER
ENERGY
OF
LESS
THAN
BIT
AND
RECEIVER
ENERGY
OF
LESS
THAN
BIT
ARE
WITHIN
REACH
IN
THIS
SECTION
WE
OUTLINE
OUR
ENVISIONED
DESIGN
OF
UWB
TRANS
CEIVERS
FOR
ENHANTS
AND
THE
RESULTING
HIGHER
LAYER
CHALLENGES
THESE
CUSTOMIZED
CIRCUITS
WILL
SUPPORT
ULTRA
LOW
ENERGY
CONSUMP
TION
AND
WILL
BE
INTEGRATED
WITH
ENERGY
HARVESTING
DEVICES
WHILE
SUPPORTING
NETWORKING
CAPABILITIES
ENERGY
COSTS
A
PARADIGM
SHIFT
THE
FIRST
NETWORKING
CHALLENGE
EMERGING
FROM
THE
DESIGN
OF
THE
ULTRA
LOW
POWER
TRANSCEIVERS
IS
THAT
THE
ENERGY
TO
RECEIVE
A
BIT
IS
MUCH
HIGHER
THAN
THE
ENERGY
TO
TRANSMIT
A
BIT
THIS
IS
SIGNIFICANTLY
DIFFERENT
FROM
TRADITIONAL
WLANS
WHERE
THE
ENERGY
TO
TRANSMIT
IS
HIGHER
THAN
THE
ENERGY
TO
RECEIVE
AND
WHERE
THEY
ARE
ON
THE
SAME
ORDER
THIS
REQUIRES
NOVEL
NETWORKING
ALGORITHMS
FOR
ENHANTS
SINCE
MANY
LEGACY
ALGORITHMS
ARE
DEVELOPED
UNDER
THE
ASSUMPTION
OF
TRANSMISSION
BEING
MORE
EXPENSIVE
THAN
RECEPTION
CONSUMPTION
IS
MEASURED
AT
A
PARTICULAR
PULSE
RATE
SINCE
PER
BIT
PARAMETERS
DIFFER
AT
DIFFERENT
BIT
RATES
NOTE
THAT
AT
LOWER
BIT
RATES
THE
ENERGY
PER
BIT
CAN
INCREASE
DUE
TO
THE
IMPACT
OF
FIXED
PULSE
INDEPENDENT
CIRCUITRY
IN
CONVENTIONAL
SYSTEMS
IN
WHICH
NARROW
BAND
MODULATED
SINU
SOIDS
ARE
TRANSMITTED
THE
TRANSMITTER
HAS
TO
BE
ACTIVE
FOR
THE
ENTIRE
DURATION
OF
THE
SIGNAL
TRANSMISSION
AS
MENTIONED
ABOVE
IN
UWB
VERY
SHORT
PULSES
CONVEY
INFORMATION
SO
THE
TRANSMITTER
AND
RE
CEIVER
CAN
WAKE
UP
FOR
VERY
SHORT
TIME
INTERVALS
TO
GENERATE
AND
RECEIVE
PULSES
AND
CAN
SLEEP
BETWEEN
SUBSEQUENT
PULSES
THE
RE
CEIVER
ENERGY
IS
MOSTLY
SPENT
ON
RUNNING
LOW
NOISE
AMPLIFICATION
AND
DATA
DETECTION
CIRCUITS
THAT
ARE
CONSUMING
ENERGY
WHENEVER
THE
DEVICE
LISTENS
TO
THE
MEDIUM
HENCE
THERE
IS
NO
DIFFERENCE
IN
TERMS
OF
ENERGY
BETWEEN
RECEIVING
INFORMATION
AND
LISTENING
TO
THE
MEDIUM
THE
NEW
ENERGY
TRADEOFFS
CALL
FOR
THE
DESIGN
OF
NEW
ALGORITHMIC
APPROACHES
FOR
EXAMPLE
IN
ORDER
TO
TAKE
THE
BURDEN
OFF
A
RECEIVER
A
TRANSMITTER
WOULD
HAVE
TO
ENABLE
A
RECEIVER
TO
LISTEN
TO
THE
MEDIUM
FOR
SHORT
TIME
INTERVALS
E
G
REPEAT
ITS
PULSES
MANY
TIMES
IN
SUCH
A
WAY
THAT
A
RECEIVER
LISTENING
TO
A
SHORT
WINDOW
OF
PULSES
WOULD
GET
ALL
THE
INFORMATION
TRANSMITTED
IN
SECTION
WE
DISCUSS
IN
MORE
DETAIL
THE
EFFECT
OF
THIS
PHENOMENON
ON
THE
DESIGN
OF
HIGHER
LAYER
PROTOCOLS
INACCURATE
CLOCKS
ACCURATE
ON
CHIP
REFERENCES
OR
CLOCKS
CANNOT
BE
POWERED
DOWN
AND
CONSUME
A
LOT
OF
ENERGY
THEY
HAVE
TO
BE
AVOIDED
TO
ACHIEVE
ULTRA
LOW
POWER
OPERATION
ONE
VIABLE
SOLUTION
IS
TO
USE
ENERGETI
CALLY
CHEAP
CLOCKS
E
G
CLOCKS
AVAILABLE
FROM
ULTRA
LOW
POWER
RING
OSCILLATORS
HOWEVER
THE
FREQUENCY
OF
SUCH
CLOCKS
WILL
VARY
SIG
NIFICANTLY
FROM
TAG
TO
TAG
AND
ITS
STABILITY
OVER
TIME
IS
ALSO
POOR
A
UWB
RECEIVER
HAS
TO
WAKE
UP
AT
CERTAIN
TIMES
IN
ORDER
TO
RE
CEIVE
PULSES
DETERMINING
THESE
TIMES
WITH
INACCURATE
CLOCKS
IM
POSES
MAJOR
CHALLENGES
AND
HENCE
WHILE
INACCURATE
CLOCKS
SAVE
EN
ERGY
THEY
INCREASE
THE
ENERGY
SPENT
ON
RECEPTION
MOREOVER
TRA
DITIONAL
LOW
POWER
SLEEP
WAKE
PROTOCOLS
SEE
HEAVILY
RELY
ON
THE
USE
OF
ACCURATE
TIME
SLOTS
HENCE
ELIMINATING
THE
AVAILABILITY
OF
ACCURATE
CLOCKS
IN
A
TAG
REQUIRES
REDESIGNING
PROTOCOLS
THAT
WERE
ORIGINALLY
DESIGNED
SPECIFICALLY
FOR
ENERGY
EFFICIENT
NETWORKING
A
HIGH
POWER
MODE
IN
SOME
CASES
IT
MAY
BE
BENEFICIAL
TO
SPEND
MORE
ENERGY
THAN
WHAT
IS
TYPICALLY
SPENT
BY
A
TAG
E
G
WHEN
THE
BATTERY
IS
FULLY
CHARGED
E
C
AND
THE
TAG
IS
HARVESTING
ENERGY
IN
SUCH
CASES
A
TAG
CAN
OPERATE
IN
A
HIGH
POWER
MODE
ENHANT
CIRCUITRY
CAN
BE
DESIGNED
TO
CONTAIN
OPTIONAL
POWER
HUNGRY
HARDWARE
MODULES
THAT
WOULD
ALLOW
FOR
EXAMPLE
FOR
A
MORE
ACCURATE
FASTER
CLOCK
TO
BE
TURNED
ON
OTHER
COMPONENTS
THAT
COULD
BE
CONSIDERED
ARE
MORE
SENSITIVE
RECEIVER
STAGES
MORE
SELECTIVE
FILTERS
AND
MORE
ELABORATE
PULSE
DETECTION
METHODS
THE
INCLUSION
OF
SUCH
COMPONENTS
WILL
INCREASE
THE
PROBABILITY
OF
SUCCESSFUL
COMMUNICATIONS
BUT
ALL
PER
FORMANCE
ENHANCEMENTS
WILL
REQUIRE
ADDITIONAL
POWER
THE
CHALLENGE
IS
TO
REALIZE
THE
BENEFITS
OF
THESE
HIGH
POWER
MODES
AT
THE
HIGHER
LAYERS
AN
EXAMPLE
OF
A
CLEAR
BENEFIT
IS
A
TAG
WITH
AN
ACCURATE
CLOCK
THAT
HELPS
OTHER
TAGS
TO
SYNCHRONIZE
ANOTHER
EXAM
PLE
IS
RUNNING
THE
RECEIVER
OR
TRANSMITTER
MORE
OFTEN
TO
HELP
OTHER
NODES
TO
DETECT
EACH
OTHER
AND
TO
ESTABLISH
COMMUNICATION
GIVEN
A
SET
OF
HIGH
POWER
PHYSICAL
LAYER
CAPABILITIES
NUMEROUS
QUESTIONS
ARISE
WHAT
IS
THE
RIGHT
HARVESTING
STATUS
TO
USE
STOP
USING
EACH
OF
THESE
CAPABILITIES
HOW
WILL
USING
THESE
CAPABILITIES
IN
ONE
TAG
AF
FECT
OTHER
TAGS
IS
IT
ENOUGH
TO
HAVE
THESE
CAPABILITIES
IN
A
SUBSET
OF
THE
TAGS
AND
WHAT
IS
THE
RIGHT
SIZE
OF
THE
SUBSET
COMMUNICATIONS
NETWORKING
WE
NOW
OUTLINE
ENHANTS
RELATED
COMMUNICATIONS
AND
NETWORK
ING
CHALLENGES
RECALL
THAT
GIVEN
THE
POWER
CONSTRAINTS
ENHANTS
WILL
BE
COMMUNICATING
WITHIN
RANGES
OF
TO
METERS
WE
IDEN
TIFY
THREE
STATES
OF
PAIRWISE
ENHANT
COMMUNICATIONS
AND
NOTE
THAT
THE
RATE
OF
ENERGY
CONSUMPTION
E
CAN
BE
ADJUSTED
WITHIN
EACH
STATE
AND
ALSO
BY
MOVING
BETWEEN
STATES
WE
ALSO
OUTLINE
THE
CHAL
LENGES
RELATED
TO
ROUTING
INFORMATION
DISSEMINATION
AND
NETWORK
SECURITY
SOME
OF
THE
CONCEPTS
DISCUSSED
IN
THIS
SECTION
ARE
WELL
KNOWN
IN
SENSOR
NETWORKING
HENCE
WE
HIGHLIGHT
THE
NEW
CHAL
LENGES
SUCH
AS
PRESENCE
OF
TRANSMIT
ONLY
DEVICES
AND
MAKING
USE
OF
THE
ALWAYS
OPEN
HARVESTING
CHANNEL
PAIRWISE
ENHANT
COMMUNICATIONS
IN
PAIRWISE
ENHANT
COMMUNICATIONS
THREE
STATES
CAN
BE
IDEN
TIFIED
INDEPENDENT
PAIRED
AND
COMMUNICATING
TO
CONTROL
ITS
EN
ERGY
SPENDING
A
TAG
CAN
MOVE
BETWEEN
STATES
WITH
RESPECT
TO
EACH
OF
ITS
NEIGHBORS
IN
EACH
PARTICULAR
STATE
A
TAG
CAN
CONSUME
DIFFER
ENT
AMOUNTS
OF
ENERGY
E
DEPENDING
ON
ITS
OWN
ENERGY
PARAMETERS
C
E
R
AND
WHEN
RELEVANT
ON
THE
ENERGY
PARAMETERS
OF
OTHER
EN
HANTS
INVOLVED
IN
COMMUNICATIONS
IN
THE
INDEPENDENT
STATE
A
TAG
DOES
NOT
MAINTAIN
CONTACT
WITH
THE
OTHER
TAG
IN
THIS
STATE
THE
TAG
NEEDS
TO
DECIDE
HOW
MUCH
ENERGY
IT
WANTS
TO
SPEND
ON
LISTENING
TO
THE
MEDIUM
AND
TRANSMITTING
PULSES
TO
ENABLE
OTHERS
TO
FIND
IT
THE
AMOUNT
OF
ENERGY
CONSUMED
CAN
BE
CONTROLLED
BY
CHANGING
THE
SPACING
BETWEEN
TRANSMITTED
PULSES
AND
LISTENING
PERIODS
AS
WELL
AS
BY
CHANGING
THE
OVERALL
DUTY
CYCLE
IF
A
TAG
IS
VERY
LOW
ON
ENERGY
IT
COULD
TRANSMIT
PULSES
BUT
NOT
LISTEN
TO
THE
MEDIUM
THIS
TRANSMIT
ONLY
MODE
IS
FEASIBLE
AND
LOGICAL
FOR
ENHANTS
SINCE
AS
DESCRIBED
IN
SECTION
IT
IS
ENER
GETICALLY
CHEAPER
FOR
A
TAG
TO
TRANSMIT
THAN
TO
LISTEN
ACCOMMO
DATING
THE
PRESENCE
OF
SUCH
TRANSMIT
ONLY
DEVICES
IS
AN
INTERESTING
NETWORKING
CHALLENGE
TO
START
COMMUNICATING
ENHANTS
NEED
TO
SYNCHRONIZE
WITH
EACH
OTHER
TWO
ENHANTS
WILL
BE
ABLE
TO
START
PAIRING
WHEN
A
PULSE
BURST
SENT
BY
ONE
TAG
OVERLAPS
WITH
A
LISTENING
INTERVAL
OF
THE
OTHER
TAG
OR
WHEN
ONE
TAG
OVERHEARS
ANOTHER
TAG
COMMUNICATIONS
WITH
THIRD
PARTIES
SINCE
ENHANTS
ACTIVITY
INTERVALS
ARE
FUNCTIONS
OF
THEIR
ENERGY
LEVELS
THE
TIME
TO
PAIR
IS
ALSO
A
FUNCTION
OF
THE
TAGS
ENERGY
LEVELS
ONCE
PAIRED
ENHANTS
NEED
TO
REMAIN
SYNCHRONIZED
BY
PE
RIODICALLY
EXCHANGING
SHORT
BITSTREAMS
THE
PAIRED
STATE
IS
SIMI
LAR
TO
LOW
POWER
MODES
OF
IEEE
AND
BLUETOOTH
HOWEVER
IT
SHOULD
BE
NOTED
THAT
THE
KEEP
ALIVE
MESSAGES
ENHANTS
EX
CHANGE
ARE
SHORT
PULSE
BURSTS
RATHER
THAN
BEACONS
THAT
INCLUDE
TENS
OF
BYTES
THE
MINIMUM
FREQUENCY
OF
BURST
EXCHANGES
IS
LIMITED
BY
THE
DEVICES
CLOCK
DRIFTS
IF
HIGH
POWER
NODES
DISCUSSED
IN
SECTION
ARE
PRESENT
IT
WOULD
HELP
WITH
BOTH
IMPROVING
THE
TIME
TO
PAIR
AND
KEEPING
DEVICES
SYNCHRONIZED
COMMUNICATING
ENHANTS
NEED
TO
COORDINATE
THEIR
TRANSMIS
SIONS
IN
ORDER
TO
ENSURE
THAT
THEY
DO
NOT
RUN
OUT
OF
ENERGY
TO
MAKE
JOINT
DECISIONS
ON
COMMUNICATION
RATES
THE
ENHANTS
NEED
TO
EXCHANGE
INFORMATION
ABOUT
THEIR
ENERGY
STATES
ENHANTS
ARE
SO
ENERGY
CONSTRAINED
THAT
EXCHANGES
OF
THEIR
ENERGY
PARAMETERS
C
E
R
E
MAY
BE
TOO
COSTLY
IT
IS
A
CHALLENGE
TO
DETERMINE
HOW
MUCH
INFORMATION
ENHANTS
SHOULD
EXCHANGE
E
G
EXCHANGE
THEIR
CURRENT
C
E
R
E
VALUES
OR
CURRENT
C
E
R
E
VALUES
ALONG
WITH
A
SET
OF
PREDICTIONS
OF
FUTURE
VALUES
AND
HOW
FREQUENTLY
THE
INFORMATION
EXCHANGE
SHOULD
BE
CONDUCTED
IN
THE
COMMUNICATING
STATE
A
TAG
ENERGY
CONSUMPTION
E
IS
CLOSELY
RELATED
TO
ITS
DATA
RATE
LOWER
DATA
RATES
ALLOW
LESS
TRANS
MISSION
AND
LISTENING
ENHANTS
NEED
TO
COMMUNICATE
AT
ULTRA
LOW
DATA
RATES
WHICH
NECESSITATES
MAKING
PROVISIONS
FOR
ENHANTS
TAKING
PAUSES
BETWEEN
TRANSMISSIONS
OF
BITS
THIS
DELAY
TOLERANCE
ON
THE
BIT
LEVEL
IS
A
CHALLENGE
FOR
NETWORKING
PROTOCOLS
THAT
OFTEN
CONSIDER
A
PACKET
AS
AN
ATOMIC
UNIT
COMMUNICATIONS
OF
MULTIPLE
ENHANTS
A
BENEFIT
OF
THE
HARVESTING
SYSTEM
IS
THAT
ENHANTS
IN
CLOSE
PROXIMITY
WILL
BE
SUBJECT
TO
COMMON
STIMULI
THROUGH
THEIR
ENERGY
HARVESTING
CHANNELS
EXAMPLES
INCLUDE
LIGHTS
TURNING
ON
OFF
OR
RUN
NING
WITH
MODULATED
INTENSITY
E
G
THE
VARIATION
IN
FLUORES
CENT
LIGHTING
OR
VIBRATIONS
FELT
BY
MORE
THAN
ONE
TAG
FOR
INSTANCE
WHEN
A
LIGHT
IS
TURNED
ON
A
TAG
CAN
ASSUME
THAT
THE
ENERGY
PARAM
ETERS
OF
ALL
ITS
NEIGHBORS
CHANGE
AND
BEHAVE
ACCORDINGLY
INFOR
MATION
ABOUT
THE
RELATIVE
SIMILARITIES
OR
DIFFERENCES
BETWEEN
EN
HANTS
STIMULI
CAN
PROVIDE
INFORMATION
ABOUT
PROXIMITY
AND
CAN
BE
USED
FOR
SYNCHRONIZATION
VIA
A
CHANNEL
WHICH
IS
EFFECTIVELY
AL
WAYS
OPEN
IN
COMMUNICATION
WITH
EACH
OF
ITS
NEIGHBORS
A
TAG
DECIDES
ON
BOTH
A
STATE
OF
COMMUNICATION
AND
IN
THE
CHOSEN
STATE
RATE
OF
EN
ERGY
CONSUMPTION
E
WHEN
MANY
DEVICES
ARE
INVOLVED
IN
COMMU
NICATION
THE
DECISIONS
ARE
FAR
FROM
TRIVIAL
ENHANTS
JOINT
ENERGY
DECISIONS
ON
STATES
AND
RATES
ARE
A
LARGE
SCALE
OPTIMIZATION
PROB
LEM
AND
A
SUITABLE
SOLUTION
FOR
THE
PROBLEM
NEEDS
TO
BE
CALCULATED
BY
LOW
POWER
ENHANTS
WITHOUT
EXTENSIVE
EXCHANGE
OF
CONTROL
IN
FORMATION
DEVELOPING
THE
ALGORITHMS
THAT
WILL
MAKE
THIS
POSSIBLE
AND
WILL
TAKE
INTO
ACCOUNT
THE
REALISTIC
CONSIDERATIONS
DISCUSSED
IN
SECTIONS
AND
IS
ONE
OF
THE
MAJOR
CHALLENGES
FOR
ENHANTS
HIGHER
LAYER
CHALLENGES
ENHANTS
CAPABILITIES
ALSO
INFLUENCE
THE
DESIGN
OF
HIGHER
LAYER
PROTOCOLS
SUCH
AS
PROTOCOLS
FOR
ROUTING
AND
INFORMATION
DISSEMI
NATION
A
VARIETY
OF
ENERGY
EFFICIENT
ROUTING
SCHEMES
PROPOSED
FOR
AD
HOC
AND
SENSOR
NETWORKS
CAN
SERVE
AS
STARTING
POINTS
FOR
EN
HANTS
ROUTING
BOTH
INFORMATION
PULLING
EXTRACTING
DATA
THROUGH
A
QUERY
AND
INFORMATION
PUSHING
ENHANTS
PROACTIVELY
EXCHANG
ING
INFORMATION
TO
ASSIST
IN
A
PULL
SHOULD
BE
USED
DECIDING
ON
THE
RIGHT
LEVELS
OF
PUSH
AND
PULL
IS
AN
INTERESTING
PROBLEM
THAT
HAS
STRONG
RELATION
TO
WORK
IN
CACHING
AND
PEER
TO
PEER
NETWORKS
SECURITY
AND
PRIVACY
ARE
VERY
IMPORTANT
ISSUES
FOR
THE
PROPOSED
ENHANTS
APPLICATIONS
LIGHTWEIGHT
TECHNIQUES
THAT
HAVE
BEEN
DE
SIGNED
FOR
SENSOR
NETWORKS
AND
FOR
RFIDS
CAN
SERVE
AS
STARTING
POINTS
IN
ENHANTS
SECURITY
RESEARCH
MOREOVER
CONGESTION
CON
TROL
AND
INTERFERENCE
RESOLUTION
TECHNIQUES
FOR
ENHANTS
COULD
BE
A
FUTURE
RESEARCH
DIRECTION
CURRENTLY
SHORT
COMMUNICATION
RANGES
AND
LOW
TRANSMISSION
RATES
ENSURE
THAT
CONGESTION
AND
INTERFERENCE
ARE
NOT
PRIMARY
CONCERNS
ENHANTS
AS
AN
INVENTORY
SYSTEM
IN
SECTION
WE
OUTLINED
ENHANTS
COMMUNICATIONS
AND
NETWORK
ING
CHALLENGES
AND
NOTED
THAT
ENHANTS
JOINT
DECISIONS
ON
ENERGY
MANAGEMENT
ARE
A
LARGE
SCALE
OPTIMIZATION
PROBLEM
IN
THIS
SECTION
WE
INTRODUCE
INVENTORY
CONTROL
THEORY
AS
A
TOOL
THAT
CAN
POTENTIALLY
BE
USED
TO
APPROACH
THIS
PROBLEM
MUCH
LIKE
THE
HARVEST
OF
A
FARMER
THE
HARVEST
OF
A
TAG
NEEDS
TO
BE
CAREFULLY
MANAGED
IN
SPENDING
THEIR
HARVEST
THE
FARMER
AND
THE
TAG
BOTH
MAKE
SURE
THEY
NEITHER
WASTE
HARVEST
DUE
TO
STORAGE
SPACE
LIMITATIONS
NOR
RUN
OUT
OF
HARVEST
WHEN
IT
IS
NEEDED
THIS
TYPE
OF
PROBLEM
IS
NOT
WELL
STUDIED
IN
WIRELESS
NETWORKING
HOWEVER
IT
HAS
BEEN
EXAMINED
IN
DEPTH
IN
THE
MATURE
FIELD
OF
INVENTORY
MANAGE
MENT
IN
THIS
SECTION
WE
SHOW
HOW
CONCEPTS
DEVELOPED
IN
THE
INVENTORY
MANAGEMENT
FIELD
APPLY
TO
ENHANTS
AN
ENERGY
HARVESTING
TAG
CAN
BE
VIEWED
AS
A
MANUFACTURING
SYS
TEM
COMPOSED
OF
A
FACTORY
AND
A
WAREHOUSE
CONSIDER
THE
AB
STRACTION
OF
THE
ENERGY
HARVESTING
SYSTEM
SHOWN
IN
FIGURE
WHICH
IDENTIFIES
SYSTEM
PARAMETERS
C
R
E
AND
E
A
REAL
WORLD
FACTORY
WAREHOUSE
HAS
A
FINITE
CAPACITY
A
RATE
AT
WHICH
PRODUCTS
ARE
MANU
FACTURED
SUPPLY
RATE
A
RATE
AT
WHICH
PRODUCTS
ARE
PURCHASED
DE
MAND
RATE
AND
A
LEVEL
OF
INVENTORY
ALL
OF
WHICH
DIRECTLY
COR
BATTERY
LEVEL
E
C
FIGURE
APPLICATION
OF
THE
ECONOMIC
PRODUCTION
QUANTITY
EPQ
MODEL
TO
THE
ENHANTS
DOMAIN
WITH
A
PERIODIC
ON
OFF
ENERGY
SOURCE
I
THE
ENERGY
CHARGE
RATE
R
T
II
THE
ENERGY
LEVEL
E
T
AND
III
THE
ENERGY
CONSUMPTION
RATE
E
T
RESPOND
TO
THE
LISTED
HARVESTING
SYSTEM
PARAMETERS
A
WAREHOUSE
INVENTORY
MANAGEMENT
SYSTEM
STRIVES
TO
ENSURE
THAT
THE
DEMAND
IS
MET
AND
THAT
THERE
ARE
NO
SHORTAGES
SIMILARLY
THE
TAG
ENERGY
MANAGEMENT
SYSTEM
SHOULD
ENSURE
THAT
IT
UTILIZES
ITS
RESOURCES
TO
COMMUNICATE
EFFICIENTLY
AND
DOES
NOT
RUN
OUT
OF
ENERGY
BELOW
WE
EXPLORE
THE
SIMILARITIES
BETWEEN
THE
ENHANTS
DOMAIN
AND
THE
INVENTORY
MANAGEMENT
DOMAIN
WE
GIVE
TWO
EXAMPLES
OF
DIRECT
APPLICATIONS
OF
INVENTORY
MANAGEMENT
MODELS
TO
THE
ENHANTS
DO
MAIN
AND
DEMONSTRATE
THAT
DUE
TO
THE
DISTRIBUTED
OPERATION
AND
THE
DEPENDENCIES
BETWEEN
ENHANTS
PARAMETERS
SEE
SECTION
A
NET
WORK
COMPOSED
OF
ENHANTS
IS
MORE
COMPLICATED
THAN
A
MANUFAC
TURING
SYSTEM
THIS
CALLS
FOR
THE
EXTENSION
OF
THE
WELL
ESTABLISHED
INVENTORY
THEORY
MODELS
TO
THE
ENHANTS
DOMAIN
DETERMINISTIC
MODEL
OUR
FIRST
EXAMPLE
CONSIDERS
A
TAG
WHICH
HARVESTS
ENERGY
FROM
AN
ON
OFF
PERIODIC
ENERGY
SOURCE
SHOWN
ON
THE
THE
UPPER
GRAPH
OF
FIGURE
SUCH
AN
ON
OFF
SOURCE
CAN
BE
FOUND
FOR
EXAMPLE
IN
AN
OFFICE
ENVIRONMENT
WHERE
AN
INDOOR
LIGHTING
SYSTEM
IS
TURNED
ON
IN
THE
MORNING
AND
OFF
AT
NIGHT
THE
SOURCE
IS
ON
DURING
THE
PE
RIOD
TP
IN
WHICH
THE
TAG
CHARGES
AT
A
CONSTANT
RATE
R
AND
IT
IS
OFF
DURING
TD
THROUGHOUT
BOTH
PERIODS
THE
TAG
CONSUMES
ENERGY
AT
A
CONSTANT
RATE
EC
IN
THE
INVENTORY
MANAGEMENT
DOMAIN
THIS
SCENARIO
DIRECTLY
MATCHES
THE
CLASSIC
ECONOMIC
PRODUCTION
QUANTITY
EPQ
MODEL
IT
CAN
BE
EASILY
SHOWN
THAT
IN
THIS
MODEL
THE
HIGHEST
CONSUMPTION
RATE
THAT
CAN
BE
MAINTAINED
IS
EC
TP
R
TD
TP
FOR
THE
ENHANTS
DOMAIN
THE
CONSUMPTION
RATE
CAN
BE
DIRECTLY
TRANSLATED
TO
BIT
RATE
DUTY
CYCLE
OR
LEVEL
OF
COMMUNICATIONS
CONSIDER
A
SCENARIO
WHERE
A
TAG
WITH
A
SOLAR
CELL
OF
EFFICIENCY
IS
LOCATED
ON
A
DIMLY
LIT
SHELF
IN
AN
ENCLOSED
OFFICE
WHERE
THE
IRRADIANCE
IS
WHEN
THE
OFFICE
LIGHT
IS
ON
AND
WHEN
IT
IS
OFF
IF
THE
LIGHT
IN
THIS
OFFICE
IS
TURNED
ON
FOR
HOURS
PER
DAY
THE
TAG
CAN
SPEND
ENERGY
AT
A
CONSTANT
RATE
OF
ASSUMING
THAT
RECEPTION
OF
ONE
BIT
REQUIRES
SIMILAR
TO
SECTION
THIS
TAG
WILL
BE
ABLE
TO
MAINTAIN
THE
DATA
RATE
OF
THROUGHOUT
THE
ENTIRE
DIURNAL
OFFICE
CYCLE
FIGURE
SHOWS
AN
EXAMPLE
OF
ALL
THE
PARAMETERS
OF
THIS
SYSTEM
WHICH
DIRECTLY
CORRESPOND
TO
THE
EPQ
MODEL
DYNAMICS
OF
THE
SUP
PLY
THE
DEMAND
AND
THE
INVENTORY
IN
A
WAREHOUSE
AS
WELL
AS
IN
A
TAG
THE
EPQ
MODEL
IS
VERY
SIMPLE
YET
IT
DEMONSTRATES
AN
EN
COURAGING
RESEMBLANCE
OF
A
TAG
ENERGY
HARVESTING
SYSTEM
AND
A
LARGE
SCALE
FACTORY
WAREHOUSE
SYSTEM
STOCHASTIC
MODEL
FOR
ENVIRONMENTS
WHERE
ENERGY
SOURCES
ARE
NOT
DETERMINISTIC
OTHER
MODELS
ARE
NEEDED
AN
EXAMPLE
OF
AN
INVENTORY
MODEL
THAT
Q
E
C
E
MIN
WHICH
THE
BEHAVIOR
OF
ONE
SIGNIFICANTLY
AFFECTS
THE
BEHAVIOR
OF
THE
OTHERS
WHILE
SOME
INVENTORY
THEORY
MODELS
CONSIDER
MULTIPLE
WAREHOUSES
MULTI
ECHELON
MODELS
THEY
WILL
NEED
TO
BE
EXTENDED
TO
CAPTURE
THE
COMPLEXITY
OF
ENHANTS
FURTHER
WHILE
IN
A
MANU
FACTURING
SYSTEM
THE
CENTRAL
CONTROLLER
HAS
COMPLETE
KNOWLEDGE
IN
A
NETWORK
OF
TAGS
DISTRIBUTED
LOW
COMPLEXITY
ALGORITHMS
USING
PAR
TIAL
KNOWLEDGE
WILL
HAVE
TO
BE
EMPLOYED
EXTENDING
INVENTORY
MAN
AGEMENT
MODELS
TO
HANDLE
THE
UNPREDICTABILITY
OF
ENHANTS
AND
ENHANTS
DEPENDENCIES
AND
CREATING
REALISTIC
AND
IMPLEMENTABLE
ENHANTS
ENERGY
SPENDING
ALGORITHMS
IS
AN
EXCITING
CHALLENGE
FIGURE
APPLICATION
OF
THE
ORDER
POINT
ORDER
QUANTITY
Q
IN
VENTORY
MODEL
TO
THE
ENHANTS
DOMAIN
I
ENERGY
CHARGE
RATE
R
T
II
THE
ENERGY
LEVEL
E
T
AND
III
THE
ENERGY
CONSUMPTION
RATE
E
T
APPLIES
DIRECTLY
TO
THE
ENHANTS
DOMAIN
IS
THE
ORDER
POINT
ORDER
QUANTITY
Q
MODEL
WHICH
TAKES
THE
STOCHASTIC
NATURE
OF
DEMAND
FOR
INVENTORY
INTO
ACCOUNT
TO
AVOID
SHORTAGES
IN
THIS
MODEL
THE
LEVEL
OF
INVENTORY
IS
TRACKED
AND
WHEN
IT
FALLS
BELOW
A
PREDETERMINED
LEVEL
ADDITIONAL
Q
ITEMS
ARE
ORDERED
THIS
RESULTS
IN
THE
INVENTORY
LEVEL
CURVE
SIMILAR
TO
THE
MIDDLE
GRAPH
IN
FIGURE
THE
FOLLOWING
ENHANT
ENERGY
SPENDING
POLICY
SIMILAR
TO
THE
BATTERY
STATE
BASED
STRATEGY
DESCRIBED
IN
RESULTS
IN
THE
SAME
INVENTORY
LEVEL
BATTERY
LEVEL
DYNAMICS
A
TAG
SPENDS
ENERGY
AT
A
CONSTANT
RATE
EC
BUT
IF
THE
TAG
BATTERY
LEVEL
DROPS
BELOW
A
PREDE
TERMINED
VALUE
THE
TAG
SWITCHES
TO
A
SAFETY
MODE
IN
WHICH
IT
SPENDS
ENERGY
AT
A
RATE
NOT
EXCEEDING
A
MINIMAL
RATE
EMIN
THE
VAL
UES
FOR
AND
EMIN
SHOULD
BE
SELECTED
SUCH
THAT
A
TAG
IN
THE
SAFETY
MODE
IS
ABLE
TO
FUNCTION
AT
SOME
LEVEL
FOR
EXAMPLE
PAIR
WITH
A
FEW
OF
ITS
NEIGHBORS
THE
TAG
STAYS
IN
THE
MINIMUM
SPENDING
MODE
UN
TIL
ITS
BATTERY
LEVEL
REACHES
Q
THEN
IT
RETURNS
TO
SPENDING
ENERGY
AT
ITS
NORMAL
EC
RATE
AN
EXAMPLE
OF
APPLYING
THE
Q
POLICY
TO
A
TAG
WITH
A
STOCHASTIC
SOURCE
IS
SHOWN
IN
FIGURE
NOVEL
ENERGY
INVENTORY
MODELS
WHILE
AS
SHOWN
ABOVE
CERTAIN
INVENTORY
MANAGEMENT
MODELS
DIRECTLY
MAP
TO
THE
ENHANTS
DOMAIN
ENHANTS
AND
PARTICU
LARLY
NETWORKS
OF
ENHANTS
CALL
FOR
EXTENSIONS
OF
EXISTING
INVEN
TORY
MANAGEMENT
MODELS
COMPARED
TO
THE
INVENTORY
MANAGEMENT
DOMAIN
IN
THE
ENHANTS
DOMAIN
THE
ENVIRONMENT
IS
MORE
RANDOM
WITH
FEWER
PARAMETERS
KNOWN
WITH
CERTAINTY
AND
FEWER
PARAMETERS
UNDER
CONTROL
FOR
EXAMPLE
WHILE
WAREHOUSE
CAPACITY
IS
USUALLY
KNOWN
AND
CONSTANT
A
TAG
STORAGE
CAPACITY
C
MAY
NOT
BE
ACCU
RATELY
KNOWN
AND
MAY
CHANGE
OVER
TIME
ALSO
AN
ENHANT
CANNOT
MANUFACTURE
MORE
INVENTORY
ENERGY
WHEN
NEEDED
AND
DOES
NOT
CONTROL
THE
CYCLES
OF
PRODUCTION
MOREOVER
IN
INVENTORY
MANAGE
MENT
THE
DEMAND
IS
USUALLY
STOCHASTIC
BUT
THE
SUPPLY
IS
MOSTLY
DE
TERMINISTIC
IN
THE
ENHANTS
DOMAIN
BOTH
THE
SUPPLY
ENERGY
HAR
VESTED
AND
THE
DEMAND
COMMUNICATIONS
ARE
LIKELY
TO
BE
STOCHAS
TIC
EXISTING
INVENTORY
MODELS
NEED
TO
BE
EXTENDED
TO
TAKE
INTO
ACCOUNT
THIS
UNCERTAINTY
AND
RANDOMNESS
A
PARTICULAR
CHALLENGE
IN
ENHANTS
IS
THE
DEPENDENCY
OF
ENERGY
SPENDING
OF
DIFFERENT
COMMUNICATING
TAGS
A
TAG
SHOULD
SPEND
EN
ERGY
ON
TRANSMISSIONS
ONLY
IF
THE
TAG
IT
COMMUNICATES
WITH
IS
READY
TO
SPEND
ENERGY
ON
RECEPTION
HENCE
ONE
TAG
ENERGY
SPENDING
NE
CESSITATES
ANOTHER
TAG
ENERGY
SPENDING
MOREOVER
THE
TRANSITION
OF
DIFFERENT
TAGS
BETWEEN
STATES
ARE
SOMEWHAT
CORRELATED
HENCE
THE
RESULTING
SYSTEM
CAN
BE
VIEWED
AS
A
NETWORK
OF
FACTORIES
IN
AS
CAN
BE
SEEN
IN
FIGURE
THE
TAG
CAN
SOMEWHAT
CONTROL
ITS
INVENTORY
LEVEL
BY
ADJUSTING
ITS
ENERGY
SPENDING
RATE
TESTBED
DESIGN
EXPERIMENTATION
WITH
THE
VARIOUS
DEVICE
DESIGNS
AND
ALGORITHMS
IN
REAL
WORLD
SETTINGS
IS
CRUCIAL
IN
ORDER
TO
BETTER
UNDERSTAND
THE
DE
SIGN
CONSIDERATIONS
HOWEVER
WIRELESS
MOTE
DESIGNS
ARE
OPTIMIZED
FOR
WIRELESS
SENSOR
APPLICATIONS
AND
TYPICALLY
USE
IEEE
RF
TRANSCEIVERS
THESE
TRANSCEIVERS
DO
NOT
LEAVE
ROOM
FOR
EXPERIMENTATION
WITH
PHYSICAL
LAYER
COMMUNICATIONS
PROTOCOLS
HENCE
WE
ARE
IN
THE
PROCESS
OF
BUILDING
ENHANTS
PROTOTYPES
IN
THE
FIRST
PHASE
THESE
PROTOTYPES
WILL
BE
BASED
ON
COMMERCIAL
OFF
THE
SHELF
COTS
COMPONENTS
CURRENTLY
THEY
ARE
PHYSICALLY
MUCH
LARGER
AND
CONSUME
MORE
POWER
THAN
THE
TARGETED
ENHANT
THEY
DO
NOT
INCLUDE
A
UWB
TRANSCEIVER
FLEXIBLE
SOLAR
CELL
AND
A
CUSTOM
BATTERY
BUT
WILL
SERVE
AS
A
PLATFORM
FOR
PRELIMINARY
EX
PERIMENTS
ONE
PROTOTYPE
IS
BASED
ON
A
MOTE
ATTACHED
TO
A
TAOS
LIGHT
TO
FREQUENCY
CONVERTER
FOR
LIGHT
MEA
SUREMENTS
AND
TO
AN
SI
SOLAR
CELL
FOR
ENERGY
HARVESTING
ANOTHER
PROTOTYPE
IS
BASED
ON
A
LABJACK
AND
A
TAOS
LIGHT
TO
FREQUENCY
CONVERTER
USING
THESE
PROTOTYPES
WE
HAVE
BEEN
PER
FORMING
ENERGY
HARVESTING
MEASUREMENTS
IN
VARIOUS
ENVIRONMENTS
THESE
MEASUREMENTS
WILL
ENABLE
US
TO
DEVELOP
REAL
WORLD
ENERGY
SUPPLY
MODELS
THAT
WILL
SUPPORT
THE
DEVELOPMENT
OF
THE
ALGORITHMS
DESCRIBED
IN
SECTIONS
AND
IN
ADDITION
WE
HAVE
BEEN
USING
THE
BASED
PROTOTYPE
TO
EMULATE
HARVESTING
AWARE
COMMUNICA
TIONS
PROTOCOLS
IN
THE
NEXT
PHASE
WE
WILL
REPLACE
THE
COTS
COMPONENTS
WITH
CUSTOM
DESIGNED
HARDWARE
FLEXIBLE
HARVESTER
SIMILAR
TO
THE
ONE
IN
FIGURE
AND
A
UWB
TRANSCEIVER
THIS
PLATFORM
WILL
ALLOW
US
TO
DEMONSTRATE
INITIAL
FEASIBILITY
FOR
THE
TRANSCEIVER
AND
HARVESTER
AND
TO
TEST
NETWORKING
PROTOCOLS
WITH
THE
ACTUAL
ENERGY
BUDGETS
FURTHER
TESTBED
INFORMATION
AND
RESULTS
WILL
BE
AVAILABLE
AT
CONCLUSIONS
WE
BELIEVE
THAT
ULTRA
LOW
POWER
ENERGY
HARVESTING
ACTIVE
NET
WORKED
TAGS
ENHANTS
ARE
ENABLERS
FOR
A
NEW
TYPE
OF
A
WIRELESS
NETWORK
WHICH
LIES
IN
THE
DOMAIN
BETWEEN
SENSOR
NETWORKS
AND
RFIDS
WHILE
RFIDS
MAKE
IT
POSSIBLE
TO
IDENTIFY
AN
OBJECT
WHICH
IS
IN
PROXIMITY
TO
A
READER
ENHANTS
MAKE
IT
POSSIBLE
TO
SEARCH
FOR
AN
OBJECT
ON
A
NETWORK
OF
DEVICES
AND
CONTINUOUSLY
MONITOR
OBJECTS
LOCATIONS
AND
PROXIMITY
TO
EACH
OTHER
ENHANTS
ENABLE
NOVEL
TRACKING
APPLICATIONS
SUCH
AS
RECOVERY
OF
LOST
ITEMS
LOCAT
ING
ITEMS
WITH
PARTICULAR
CHARACTERISTICS
CONTINUOUS
MONITORING
OF
MERCHANDIZE
AND
ASSISTANCE
IN
LOCATING
SURVIVORS
OF
A
DISASTER
ENHANTS
NECESSITATE
RETHINKING
OF
COMMUNICATION
AND
NETWORK
ING
PRINCIPLES
AND
REQUIRE
CAREFUL
EXAMINATION
OF
THE
PARTICULARITIES
OF
ULTRA
LOW
POWER
AND
ENERGY
HARVESTING
TECHNOLOGIES
WE
HAVE
SHOWN
THAT
THE
NATURE
OF
ENHANTS
REQUIRES
A
CROSS
LAYER
APPROACH
TO
ENABLE
EFFECTIVE
COMMUNICATIONS
AND
NETWORKING
BETWEEN
DE
VICES
WITH
SEVERE
POWER
AND
HARVESTING
CONSTRAINTS
IN
ORDER
TO
DISCUSS
THE
DESIGN
CHALLENGES
WE
OUTLINED
SEVERAL
IMPORTANT
CHAR
ACTERISTICS
OF
ENHANTS
POINTED
OUT
A
NUMBER
OF
OPEN
PROBLEMS
AND
POSSIBLE
RESEARCH
DIRECTIONS
AND
INTRODUCED
INVENTORY
CONTROL
THE
ENERGY
EFFICIENCY
OF
COMPUTER
SYSTEMS
IS
AN
IMPORTANT
CONCERN
IN
A
VARIETY
OF
CONTEXTS
IN
DATA
CENTERS
REDUCING
ENERGY
USE
IMPROVES
OPERATING
COST
SCALABILITY
RELIABILITY
AND
OTHER
FACTORS
FOR
MOBILE
DEVICES
ENERGY
CONSUMPTION
DIRECTLY
AFFECTS
FUNCTIONALITY
AND
USABILITY
WE
PROPOSE
AND
MOTIVATE
JOULESORT
AN
EXTERNAL
SORT
BENCHMARK
FOR
EVALUAT
ING
THE
ENERGY
EFFICIENCY
OF
A
WIDE
RANGE
OF
COMPUTER
SYSTEMS
FROM
CLUSTERS
TO
HANDHELDS
WE
LIST
THE
CRITERIA
CHALLENGES
AND
PITFALLS
FROM
OUR
EXPERIENCE
IN
CREATING
A
FAIR
ENERGY
EFFICIENCY
BENCHMARK
USING
A
COMMERCIAL
SORT
WE
DEMON
STRATE
A
JOULESORT
SYSTEM
THAT
IS
OVER
AS
ENERGY
EFFICIENT
AS
LAST
YEAR
ESTIMATED
WINNER
THIS
SYSTEM
IS
QUITE
DIFFER
ENT
FROM
THOSE
CURRENTLY
USED
IN
DATA
CENTERS
IT
CONSISTS
OF
A
COMMODITY
MOBILE
CPU
AND
LAPTOP
DRIVES
CONNECTED
BY
SERVER
STYLE
I
O
INTERFACES
CATEGORIES
AND
SUBJECT
DESCRIPTORS
H
INFORMATION
SYSTEMS
DATABASE
MANAGEMENT
SYSTEMS
GENERAL
TERMS
DESIGN
EXPERIMENTATION
MEASUREMENT
PERFORMANCE
KEYWORDS
BENCHMARK
ENERGY
EFFICIENCY
POWER
SERVERS
SORT
INTRODUCTION
IN
CONTEXTS
RANGING
FROM
LARGE
SCALE
DATA
CENTERS
TO
MOBILE
DEVICES
ENERGY
USE
IN
COMPUTER
SYSTEMS
IS
AN
IMPORTANT
CONCERN
IN
DATA
CENTER
ENVIRONMENTS
ENERGY
EFFICIENCY
AFFECTS
A
NUMBER
OF
FACTORS
FIRST
POWER
AND
COOLING
COSTS
ARE
SIGNIFI
CANT
COMPONENTS
OF
OPERATIONAL
AND
UP
FRONT
COSTS
TODAY
A
TYPICAL
DATA
CENTER
WITH
RACKS
CONSUMING
TOTAL
POWER
COSTS
TO
POWER
AND
TO
COOL
PER
YEAR
WITH
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
SIGMOD
JUNE
BEIJING
CHINA
COPYRIGHT
ACM
OF
UP
FRONT
COSTS
FOR
COOLING
EQUIPMENT
THESE
COSTS
VARY
DEPENDING
UPON
THE
INSTALLATION
BUT
THEY
ARE
GROWING
RAPIDLY
AND
HAVE
THE
POTENTIAL
EVENTUALLY
TO
OUTSTRIP
THE
COST
OF
HARDWARE
SECOND
ENERGY
USE
HAS
IMPLICATIONS
FOR
DENSITY
RELIABILITY
AND
SCALABILITY
AS
DATA
CENTERS
HOUSE
MORE
SERVERS
AND
CONSUME
MORE
ENERGY
REMOVING
HEAT
FROM
THE
DATA
CENTER
BECOMES
INCREASINGLY
DIFFICULT
SINCE
THE
RELIABILITY
OF
SERVERS
AND
DISKS
DECREASES
WITH
INCREASED
TEMPERATURE
THE
POWER
CONSUMPTION
OF
SERVERS
AND
OTHER
COMPONENTS
LIMITS
THE
ACHIEVABLE
DENSITY
WHICH
IN
TURN
LIM
ITS
SCALABILITY
THIRD
ENERGY
USE
IN
DATA
CENTERS
IS
STARTING
TO
PROMPT
ENVIRONMENTAL
CONCERNS
OF
POLLUTION
AND
EXCESSIVE
LOAD
PLACED
ON
LOCAL
UTILITIES
ENERGY
RELATED
CONCERNS
ARE
SEVERE
ENOUGH
THAT
COMPANIES
LIKE
GOOGLE
ARE
STARTING
TO
BUILD
DATA
CENTERS
CLOSE
TO
ELECTRIC
PLANTS
IN
COLD
WEATHER
CLI
MATES
ALL
THESE
CONCERNS
HAVE
LED
TO
IMPROVEMENTS
IN
COOLING
INFRASTRUCTURE
AND
IN
SERVER
POWER
CONSUMPTION
FOR
MOBILE
DEVICES
BATTERY
CAPACITY
AND
ENERGY
USE
DI
RECTLY
AFFECT
USABILITY
BATTERY
CAPACITY
DETERMINES
HOW
LONG
DEVICES
LAST
CONSTRAINS
FORM
FACTORS
AND
LIMITS
FUNCTIONAL
ITY
SINCE
BATTERY
CAPACITY
IS
LIMITED
AND
IMPROVING
SLOWLY
DEVICE
ARCHITECTS
HAVE
CONCENTRATED
ON
EXTRACTING
GREATER
ENERGY
EFFICIENCY
FROM
THE
UNDERLYING
COMPONENTS
SUCH
AS
THE
PROCESSOR
THE
DISPLAY
AND
THE
WIRELESS
SUBSYSTEMS
IN
ISOLATION
TO
DRIVE
ENERGY
EFFICIENCY
IMPROVEMENTS
WE
NEED
BENCH
MARKS
TO
ASSESS
THEIR
EFFECTIVENESS
UNFORTUNATELY
THERE
HAS
BEEN
NO
FOCUS
ON
A
COMPLETE
BENCHMARK
INCLUDING
A
WORK
LOAD
METRIC
AND
GUIDELINES
TO
GAUGE
THE
EFFICACY
OF
ENERGY
OPTIMIZATIONS
FROM
A
WHOLE
SYSTEM
PERSPECTIVE
SOME
EFFORTS
ARE
UNDER
WAY
TO
ESTABLISH
BENCHMARKS
FOR
ENERGY
EFFICIENCY
IN
DATA
CENTERS
BUT
ARE
INCOMPLETE
OTHER
WORK
HAS
EMPHASIZED
METRICS
SUCH
AS
THE
ENERGY
DELAY
PRODUCT
OR
PER
FORMANCE
PER
WATT
TO
CAPTURE
ENERGY
EFFICIENCY
FOR
PROCES
SORS
AND
SERVERS
WITHOUT
FIXING
A
WORKLOAD
MOREOVER
WHILE
PAST
EMPHASIS
ON
PROCESSOR
ENERGY
EFFICIENCY
HAS
LED
TO
IMPROVEMENTS
IN
OVERALL
POWER
CONSUMPTION
THERE
HAS
BEEN
LITTLE
FOCUS
ON
THE
I
O
SUBSYSTEM
WHICH
PLAYS
A
SIGNIFICANT
ROLE
IN
TOTAL
SYSTEM
POWER
FOR
MANY
IMPORTANT
WORKLOADS
AND
SYSTEMS
IN
THIS
PAPER
WE
PROPOSE
JOULESORT
AS
A
HOLISTIC
BENCH
MARK
TO
DRIVE
THE
DESIGN
OF
ENERGY
EFFICIENT
SYSTEMS
JOULE
SORT
USES
THE
SAME
WORKLOAD
AS
THE
OTHER
EXTERNAL
SORT
BENCH
MARKS
BUT
ITS
METRIC
INCORPORATES
TOTAL
ENERGY
WHICH
IS
A
COMBINATION
OF
POWER
CONSUMPTION
AND
PERFOR
MANCE
THE
BENCHMARK
CAN
BE
SUMMARIZED
AS
FOLLOWS
SORT
A
FIXED
NUMBER
OF
RANDOMLY
PERMUTED
BYTE
RECORDS
WITH
BYTE
KEYS
THE
SORT
MUST
START
WITH
INPUT
IN
A
FILE
ON
NON
VOLATILE
STORE
AND
FINISH
WITH
OUTPUT
IN
A
FILE
ON
NON
VOLATILE
STORE
THERE
ARE
THREE
SCALE
CATEGORIES
FOR
JOULESORT
AND
RECORDS
THE
WINNER
IN
EACH
CATEGORY
IS
THE
SYSTEM
WITH
THE
MINIMUM
TOTAL
ENERGY
USE
WE
CHOOSE
SORT
AS
THE
WORKLOAD
FOR
THE
SAME
BASIC
REA
SON
THAT
THE
TERABYTE
SORT
MINUTESORT
PENNYSORT
AND
PERFORMANCE
PRICE
SORT
BENCHMARKS
DO
IT
IS
SIMPLE
TO
STATE
AND
BALANCES
SYSTEM
COMPONENT
USE
SORT
STRESSES
ALL
CORE
COMPONENTS
OF
A
SYSTEM
MEMORY
CPU
AND
I
O
SORT
ALSO
EXERCISES
THE
OS
AND
FILESYSTEM
SORT
IS
A
PORTABLE
WORKLOAD
IT
IS
APPLICABLE
TO
A
VARIETY
OF
SYSTEMS
FROM
MOBILE
DEVICES
TO
LARGE
SERVER
CONFIGURATIONS
ANOTHER
NATURAL
REASON
FOR
CHOOSING
SORT
IS
THAT
IT
REPRESENTS
SEQUEN
TIAL
I
O
TASKS
IN
DATA
MANAGEMENT
WORKLOADS
JOULESORT
IS
AN
I
O
CENTRIC
BENCHMARK
THAT
MEASURES
THE
ENERGY
EFFICIENCY
OF
SYSTEMS
AT
PEAK
USE
LIKE
PREVIOUS
SORT
BENCHMARKS
ONE
OF
ITS
GOALS
IS
TO
GAUGE
THE
END
TO
END
EF
FECTIVENESS
OF
IMPROVEMENTS
IN
SYSTEM
COMPONENTS
TO
DO
SO
JOULESORT
ALLOWS
US
TO
COMPARE
THE
ENERGY
EFFICIENCIES
OF
A
VARIETY
OF
DISPARATE
SYSTEM
CONFIGURATIONS
BECAUSE
OF
THE
SIMPLICITY
AND
PORTABILITY
OF
SORT
PREVIOUS
SORT
BENCH
MARKS
HAVE
BEEN
TECHNOLOGY
TREND
BELLWETHERS
FOR
EXAMPLE
FORESHADOWING
THE
TRANSITION
FROM
SUPERCOMPUTERS
TO
CLUS
TERS
SIMILARLY
AN
IMPORTANT
PURPOSE
OF
JOULESORT
IS
TO
CHART
PAST
TRENDS
AND
GAIN
INSIGHT
INTO
FUTURE
TRENDS
IN
ENERGY
EF
FICIENCY
BEYOND
THE
BENCHMARK
DEFINITION
OUR
MAIN
CONTRIBUTIONS
ARE
TWOFOLD
FIRST
WE
MOTIVATE
AND
DESCRIBE
PITFALLS
SUR
ROUNDING
THE
CREATION
OF
A
FAIR
ENERGY
EFFICIENCY
BENCHMARK
WE
JUSTIFY
OUR
FAIREST
FORMULATION
WHICH
INCLUDES
THREE
SCALE
FACTORS
THAT
CORRESPOND
NATURALLY
TO
THE
DOMINANT
CLASSES
OF
SYSTEMS
FOUND
TODAY
MOBILE
DESKTOP
AND
SERVER
AL
THOUGH
WE
SUPPORT
BOTH
DAYTONA
COMMERCIALLY
SUPPORTED
AND
INDY
NO
HOLDS
BARRED
CATEGORIES
FOR
EACH
SCALE
WE
CONCENTRATE
ON
DAYTONA
SYSTEMS
IN
THIS
PAPER
SECOND
WE
PRESENT
THE
WINNING
JOULESORT
SYSTEM
THAT
IS
OVER
MORE
EFFICIENT
SORTEDRECS
JOULE
FOR
THAN
LAST
YEAR
ESTIMATED
WINNER
SORTEDRECS
JOULE
FOR
THIS
SYSTEM
SHOWS
THAT
A
FOCUS
ON
ENERGY
EFFI
CIENCY
LEADS
TO
A
UNIQUE
CONFIGURATION
THAT
IS
HARD
TO
FIND
PRE
ASSEMBLED
OUR
WINNER
BALANCES
A
LOW
POWER
MOBILE
PROCESSOR
WITH
NUMEROUS
LAPTOP
DISKS
CONNECTED
VIA
SERVER
CLASS
PCI
E
I
O
CARDS
AND
USES
A
COMMERCIAL
SORT
NSORT
THE
REST
OF
THE
PAPER
IS
ORGANIZED
AS
FOLLOWS
IN
SECTION
WE
ESTIMATE
THE
ENERGY
EFFICIENCY
OF
PAST
SORT
BENCHMARK
WINNERS
WHICH
SUGGESTS
THAT
EXISTING
SORT
BENCHMARKS
CAN
NOT
SERVE
AS
SURROGATES
FOR
AN
ENERGY
EFFICIENCY
BENCHMARK
SECTION
DETAILS
THE
CRITERIA
AND
CHALLENGES
IN
DESIGNING
JOULESORT
AND
LISTS
ISSUES
AND
GUIDELINES
FOR
PROPER
ENERGY
MEASUREMENT
IN
SECTION
WE
MEASURE
THE
ENERGY
CON
SUMPTION
OF
UNBALANCED
AND
BALANCED
SYSTEMS
TO
MOTIVATE
OUR
CHOICES
IN
DESIGNING
OUR
WINNING
SYSTEM
THE
BALANCED
SYSTEM
SHOWS
THAT
THE
I
O
SUBSYSTEM
IS
A
SIGNIFICANT
PART
OF
TOTAL
POWER
SECTION
PROVIDES
AN
IN
DEPTH
STUDY
OF
OUR
JOULE
SORT
SYSTEM
USING
NSORT
IN
PARTICULAR
WE
SHOW
THAT
THE
MOST
ENERGY
EFFICIENT
COST
EFFECTIVE
AND
BEST
PERFORMING
CONFIGURATION
FOR
THIS
SYSTEM
IS
WHEN
THE
SORT
IS
CPU
BOUND
YEAR
FIGURE
ESTIMATED
ENERGY
EFFICIENCY
OF
PREVIOUS
WINNERS
OF
SORT
BENCHMARKS
WE
ALSO
FIND
THAT
BOTH
THE
CHOICE
OF
FILESYSTEM
AND
IN
MEMORY
SORTING
ALGORITHM
AFFECT
ENERGY
EFFICIENCY
SECTION
DISCUSSES
THE
RELATED
WORK
AND
SECTION
PRESENTS
LIMITATIONS
AND
FU
TURE
DIRECTIONS
HISTORICAL
TRENDS
IN
THIS
SECTION
WE
SEEK
TO
UNDERSTAND
IF
ANY
OF
THE
EXIST
ING
SORT
BENCHMARKS
CAN
SERVE
AS
A
SURROGATE
FOR
AN
ENERGY
EFFICIENCY
BENCHMARK
TO
DO
SO
WE
FIRST
ESTIMATE
THE
SORT
EDRECS
JOULE
RATIO
A
MEASURE
OF
ENERGY
EFFICIENCY
OF
THE
PAST
DECADE
SORT
BENCHMARK
WINNERS
THIS
ANALYSIS
REVEALS
THAT
THE
ENERGY
EFFICIENCY
OF
SYSTEMS
DESIGNED
FOR
PURE
PER
FORMANCE
I
E
MINUTESORT
TERABYTE
SORT
AND
DATAMATION
WINNERS
HAS
IMPROVED
SLOWLY
MOREOVER
SYSTEMS
DESIGNED
FOR
PRICE
PERFORMANCE
I
E
PENNYSORT
WINNERS
ARE
COMPAR
ATIVELY
MORE
ENERGY
EFFICIENT
AND
THEIR
ENERGY
EFFICIENCY
IS
GROWING
RAPIDLY
HOWEVER
SINCE
OUR
JOULESORT
SYS
TEM
ENERGY
EFFICIENCY
IS
WELL
BEYOND
WHAT
GROWTH
RATES
WOULD
PREDICT
FOR
THIS
YEAR
PENNYSORT
WINNER
WE
CONCLUDE
THAT
EXISTING
SORT
BENCHMARKS
DO
NOT
INHERENTLY
PROVIDE
AN
INCENTIVE
TO
OPTIMIZE
FOR
ENERGY
EFFICIENCY
SUPPORTING
THE
NEED
FOR
JOULESORT
METHODOLOGY
FIGURE
SHOWS
THE
ESTIMATED
SORTEDRECS
JOULE
METRIC
FOR
THE
PAST
SORT
BENCHMARK
WINNERS
SINCE
WE
COMPUTE
THESE
METRICS
FROM
THE
PUBLISHED
PERFORMANCE
RECORDS
AND
OUR
OWN
ESTIMATES
OF
POWER
CONSUMPTION
SINCE
ENERGY
USE
WAS
NOT
REPORTED
WE
OBTAIN
THE
PERFORMANCE
RECORDS
AND
HARDWARE
CONFIGURATION
INFORMATION
FROM
THE
SORT
BENCH
MARK
WEBSITE
AND
THE
WINNERS
POSTED
REPORTS
WE
ESTIMATE
TOTAL
ENERGY
DURING
SYSTEM
USE
WITH
A
STRAIGHT
FORWARD
APPROACH
FROM
THE
POWER
MANAGEMENT
COMMUNITY
SINCE
CPU
MEMORY
AND
DISK
ARE
USUALLY
THE
MAIN
POWER
CONSUMING
SYSTEM
COMPONENTS
WE
USE
INDIVIDUAL
ESTIMATES
OF
THESE
TO
COMPUTE
TOTAL
POWER
FOR
MEMORY
AND
DISKS
WE
USE
THE
HP
ENTERPRISE
CONFIGURATOR
POWER
CALCU
LATOR
TO
YIELD
A
FIXED
POWER
OF
PER
DISK
AND
PER
DIMM
SOME
OF
THE
SORT
BENCHMARK
REPORTS
ONLY
MENTION
TOTAL
MEMORY
CAPACITY
AND
NOT
THE
NUMBER
OF
DIMMS
IN
THOSE
CASES
WE
ASSUME
A
DIMM
SIZE
APPROPRIATE
TO
THE
ERA
OF
THE
REPORT
THE
MAXIMUM
POWER
SPECS
FOR
CPUS
USUALLY
QUOTED
AS
THERMAL
DESIGN
POWER
TDP
ARE
MUCH
HIGHER
THAN
THE
PEAK
NUMBERS
SEEN
IN
COMMON
USE
THUS
WE
DERATE
THESE
POWER
RATINGS
BY
A
FACTOR
ALTHOUGH
A
BIT
CON
SERVATIVE
THIS
APPROACH
ALLOWS
REASONABLE
APPROXIMATIONS
FOR
A
VARIETY
OF
SYSTEMS
WHEN
UNCERTAIN
WE
ASSUME
THE
NEWEST
POSSIBLE
GENERATION
OF
THE
REPORTED
PROCESSOR
AS
OF
THE
SORT
BENCHMARK
RECORD
BECAUSE
A
GIVEN
CPU
POWER
CONSUMPTION
IMPROVES
WITH
SHRINKING
FEATURE
SIZES
FINALLY
TO
ACCOUNT
FOR
POWER
SUPPLIES
INEFFICIENCIES
WHICH
CAN
VARY
WIDELY
AND
OTHER
COMPONENTS
WE
SCALE
TOTAL
SYSTEM
POWER
DERIVED
FROM
COMPONENT
LEVEL
ESTIMATES
BY
FOR
SINGLE
NODE
SYSTEMS
WE
USE
A
HIGHER
FACTOR
FOR
CLUSTERS
TO
ACCOUNT
FOR
ADDITIONAL
COMPONENTS
SUCH
AS
NETWORKING
MANAGEMENT
HARDWARE
AND
REDUNDANT
POWER
SUPPLIES
OUR
POWER
ESTIMATES
ARE
INTENDED
TO
ILLUMINATE
COARSE
HIS
TORICAL
TRENDS
AND
ARE
ACCURATE
ENOUGH
TO
SUPPORT
THE
HIGH
LEVEL
CONCLUSIONS
IN
THIS
SECTION
WE
EXPERIMENTALLY
VALI
DATED
THIS
APPROACH
AGAINST
SOME
SERVER
AND
DESKTOP
CLASS
SYSTEMS
AND
ITS
ACCURACY
WAS
BETWEEN
AND
ANALYSIS
ALTHOUGH
PREVIOUS
SORT
BENCHMARK
WINNERS
WERE
NOT
CON
FIGURED
WITH
POWER
CONSUMPTION
IN
MIND
THEY
ROUGHLY
RE
FLECT
THE
POWER
CHARACTERISTICS
OF
DESKTOP
AND
HIGHER
END
SYS
TEMS
IN
THEIR
DAY
THUS
FROM
THE
DATA
IN
FIGURE
WE
CAN
IN
FER
QUALITATIVE
INFORMATION
ABOUT
THE
RELATIVE
IMPROVEMENTS
IN
PERFORMANCE
PRICE
PERFORMANCE
AND
ENERGY
EFFICIENCY
IN
THE
LAST
DECADE
FIGURE
COMPARES
THE
ENERGY
EFFICIENCY
OF
PREVIOUS
SORT
WINNERS
USING
THE
SORTEDRECS
JOULE
RATIO
AND
SUPPORTS
THE
FOLLOWING
OBSERVATIONS
SYSTEMS
OPTIMIZED
FOR
PRICE
PERFORMANCE
I
E
PENNYSORT
WINNERS
CLEARLY
ARE
MORE
ENERGY
EFFICIENT
THAN
THE
OTHER
SORT
BENCHMARK
WINNERS
WHICH
WERE
OPTIMIZED
FOR
PURE
PERFOR
MANCE
THERE
ARE
TWO
REASONS
FOR
THIS
EFFECT
FIRST
THE
PRICE
PERFORMANCE
METRIC
MOTIVATES
SYSTEM
DESIGNERS
TO
USE
FEWER
COMPONENTS
AND
THUS
LESS
POWER
SECOND
IT
PROVIDES
INCENTIVE
TO
USE
CHEAPER
COMMODITY
COMPONENTS
WHICH
FOR
A
GIVEN
PERFORMANCE
POINT
TRADITIONALLY
HAVE
USED
LESS
EN
ERGY
THAN
EXPENSIVE
HIGH
PERFORMANCE
COMPONENTS
THE
ENERGY
EFFICIENCY
OF
COST
CONSCIOUS
SYSTEMS
HAS
IM
PROVED
FASTER
THAN
THAT
OF
PERFORMANCE
OPTIMIZED
SYSTEMS
WHICH
HAVE
HARDLY
IMPROVED
OTHERS
HAVE
ALSO
OBSERVED
A
FLAT
ENERGY
EFFICIENCY
TREND
FOR
CLUSTER
HARDWARE
MUCH
OF
THE
GROWTH
IN
THE
PENNYSORT
CURVE
IS
FROM
THE
LAST
TWO
INDY
WINNERS
WHICH
HAVE
MADE
LARGE
LEAPS
IN
ENERGY
EFFICIENCY
IN
ALGORITHMIC
IMPROVEMENTS
AND
A
MINIMAL
HARDWARE
CONFIGURATION
PLAYED
A
ROLE
IN
THIS
IMPROVEMENT
BUT
MOST
IMPORTANTLY
CPU
DESIGN
TRENDS
HAD
FINALLY
SWUNG
TOWARD
ENERGY
EFFICIENCY
THE
PROCESSOR
USED
IN
THE
PENNYSORT
WINNER
HAS
THE
CLOCK
FREQUENCY
OF
ITS
IMMEDIATE
PREDE
CESSOR
WHILE
ONLY
CONSUMING
THE
POWER
OVERALL
THE
SORT
HAD
BETTER
PERFORMANCE
THAN
THE
PREVIOUS
DATA
POINT
WHILE
USING
THE
POWER
THE
PENNYSORT
WIN
NER
GPUTERASORT
INCREASED
ENERGY
EFFICIENCY
BY
INTRODUC
ING
A
NEW
SYSTEM
COMPONENT
THE
GRAPHICS
PROCESSING
UNIT
GPU
AND
UTILIZING
IT
VERY
EFFECTIVELY
THE
CHOSEN
GPU
IS
INEXPENSIVE
AND
COMPARABLE
IN
POWER
CONSUMPTION
TO
THE
CPU
BUT
IT
PROVIDES
BETTER
STREAMING
MEMORY
BANDWIDTH
THAN
THE
CPU
THIS
LATEST
WINNER
IN
PARTICULAR
SHOWS
THE
DANGER
OF
RELY
ING
ON
ENERGY
BENCHMARKS
THAT
FOCUS
ONLY
ON
SPECIFIC
HARD
WARE
LIKE
CPU
OR
DISKS
RATHER
THAN
END
TO
END
EFFICIENCY
SUCH
SPECIFIC
BENCHMARKS
WOULD
ONLY
DRIVE
AND
TRACK
IM
BENCHMARK
SRECS
SEC
SRECS
SRECS
J
PENNYSORT
YR
YR
YR
MINUTE
TERABYTE
AND
DATAMATION
YR
N
A
YR
TABLE
THIS
TABLE
SHOWS
THE
ESTIMATED
YEARLY
GROWTH
IN
PURE
PERFORMANCE
PRICE
PERFORMANCE
AND
ENERGY
EFFICIENCY
OF
PAST
WINNERS
PROVEMENTS
OF
EXISTING
TECHNOLOGIES
AND
MAY
FAIL
TO
ANTICI
PATE
THE
USE
OF
POTENTIALLY
DISRUPTIVE
TECHNOLOGIES
SINCE
PRICE
PERFORMANCE
WINNERS
ARE
MORE
ENERGY
EFFICIENT
WE
NEXT
EXAMINE
WHETHER
THE
MOST
COST
EFFECTIVE
SORT
IMPLIES
THE
BEST
ACHIEVABLE
ENERGY
EFFICIENT
SORT
TO
DO
SO
WE
FIRST
ESTIMATE
THE
GROWTH
RATE
OF
SORT
WINNERS
ALONG
MULTIPLE
DI
MENSIONS
TABLE
SHOWS
THE
GROWTH
RATE
OF
PAST
SORT
BENCH
MARK
WINNERS
ALONG
THREE
DIMENSIONS
PERFORMANCE
SORT
EDRECS
SEC
PRICE
PERFORMANCE
SORTEDRECS
AND
ENERGY
EFFICIENCY
SORTEDRECS
JOULE
WE
SEPARATE
THE
GROWTH
RATES
INTO
TWO
CATEGORIES
BASED
ON
THE
BENCHMARK
OPTIMIZATION
GOAL
PRICE
OR
PURE
PERFORMANCE
SINCE
THE
GOAL
DRIVES
THE
SYSTEM
DESIGN
FOR
EACH
CATEGORY
WE
CALCULATE
THE
GROWTH
RATE
AS
FOLLOWS
WE
CHOOSE
THE
BEST
SYSTEM
ACCORDING
TO
THE
METRIC
IN
EACH
YEAR
AND
FIT
THE
RESULT
WITH
AN
EXPONENTIAL
TABLE
SHOWS
THAT
PENNYSORT
SYSTEMS
ARE
IMPROVING
AL
MOST
AT
THE
PACE
OF
MOORE
LAW
ALONG
THE
PERFORMANCE
AND
PRICE
PERFORMANCE
DIMENSIONS
THE
PURE
PERFORMANCE
SYS
TEMS
HOWEVER
ARE
IMPROVING
MUCH
MORE
SLOWLY
AS
NOTED
ELSEWHERE
MORE
IMPORTANTLY
OUR
ANALYSIS
SHOWS
MUCH
SLOWER
ESTI
MATED
GROWTH
IN
ENERGY
EFFICIENCY
THAN
IN
THE
OTHER
TWO
METRICS
FOR
BOTH
BENCHMARK
CATEGORIES
GIVEN
LAST
YEAR
ESTIMATED
PENNYSORT
WINNER
PROVIDES
SRECS
J
OUR
CURRENT
JOULESORT
WINNER
AT
SRECS
J
IS
NEARLY
THE
EXPECTED
VALUE
OF
SRECS
J
FOR
THIS
YEAR
THIS
RESULT
SUGGESTS
THAT
WE
NEED
A
BENCHMARK
FOCUSED
ON
EN
ERGY
EFFICIENCY
TO
PROMOTE
DEVELOPMENT
OF
THE
MOST
ENERGY
EFFICIENT
SORTING
SYSTEMS
AND
ALLOW
FOR
DISRUPTIVE
TECHNOLOGIES
IN
ENERGY
EFFICIENCY
IRRESPECTIVE
OF
COST
BENCHMARK
DESIGN
IN
THIS
SECTION
WE
DETAIL
THE
CRITERIA
AND
CHALLENGES
IN
DE
SIGNING
AN
ENERGY
EFFICIENCY
BENCHMARK
WE
DESCRIBE
SOME
OF
THE
PITFALLS
OF
OUR
INITIAL
SPECIFICATIONS
AND
HOW
THE
BENCH
MARK
HAS
EVOLVED
WE
ALSO
SPECIFY
RULES
OF
THE
BENCHMARK
WITH
RESPECT
TO
BOTH
WORKLOAD
AND
ENERGY
MEASUREMENT
CRITERIA
ALTHOUGH
PAST
STUDIES
HAVE
PROPOSED
ENERGY
EFFICIENCY
MET
RICS
OR
POWER
MEASUREMENT
TECHNIQUES
NONE
PROVIDE
A
COMPLETE
BENCHMARK
A
WORKLOAD
A
METRIC
OF
COMPARISON
AND
RULES
FOR
RUNNING
THE
WORKLOAD
AND
MEA
SURING
ENERGY
CONSUMPTION
MOREOVER
THESE
STUDIES
TRADI
TIONALLY
HAVE
FOCUSED
ON
COMPARING
EXISTING
SYSTEMS
RATHER
THAN
PROVIDING
INSIGHT
INTO
FUTURE
TECHNOLOGY
TRENDS
WE
SET
OUT
TO
DESIGN
AN
ENERGY
ORIENTED
BENCHMARK
THAT
ADDRESSES
THESE
DRAWBACKS
WITH
THE
CRITERIA
BELOW
IN
MIND
WHILE
ACHIEVING
ALL
THESE
CRITERIA
SIMULTANEOUSLY
IS
HARD
WE
STRIVE
TO
ENCOMPASS
THEM
AS
MUCH
AS
POSSIBLE
ENERGY
EFFICIENCY
THE
BENCHMARK
SHOULD
MEASURE
A
SYS
TEM
BANG
FOR
THE
BUCK
WHERE
BANG
IS
WORK
DONE
AND
THE
COST
REFLECTS
SOME
MEASURE
OF
POWER
USE
E
G
AVERAGE
POWER
PEAK
POWER
TOTAL
ENERGY
AND
ENERGY
DELAY
TO
DRIVE
PRACTICAL
IMPROVEMENTS
IN
POWER
CONSUMPTION
COST
SHOULD
REFLECT
BOTH
A
SYSTEM
PERFORMANCE
AND
POWER
USE
A
SYS
TEM
THAT
USES
ALMOST
NO
POWER
BUT
TAKES
FOREVER
TO
COMPLETE
A
TASK
IS
NOT
PRACTICAL
SO
AVERAGE
AND
PEAK
POWER
ARE
POOR
CHOICES
THUS
THERE
ARE
TWO
REASONABLE
COST
ALTERNATIVES
ENERGY
A
PRODUCT
OF
EXECUTION
TIME
AND
POWER
OR
ENERGY
DELAY
A
PRODUCT
OF
EXECUTION
TIME
AND
ENERGY
THE
FORMER
WEIGHS
PERFORMANCE
AND
POWER
EQUALLY
WHILE
THE
LATTER
POP
ULAR
IN
CPU
CENTRIC
BENCHMARKS
PLACES
MORE
EMPHASIS
ON
PERFORMANCE
SINCE
THERE
ARE
OTHER
SORT
BENCHMARKS
THAT
EMPHASIZE
PERFORMANCE
WE
CHOSE
ENERGY
AS
THE
COST
PEAK
USE
A
BENCHMARK
CAN
CONSIDER
SYSTEM
ENERGY
IN
THREE
IMPORTANT
MODES
IDLE
PEAK
USE
OR
A
REALISTIC
COMBI
NATION
OF
THE
TWO
ALTHOUGH
MINIMIZING
IDLE
MODE
POWER
IS
USEFUL
EVALUATING
THIS
MODE
IS
STRAIGHTFORWARD
REAL
WORLD
WORKLOADS
ARE
OFTEN
A
COMBINATION
BUT
DESIGNING
A
BROAD
BENCHMARK
THAT
ADDRESSES
A
NUMBER
OF
SCENARIOS
IS
DIFFICULT
TO
IMPOSSIBLE
HENCE
WE
CHOSE
TO
FOCUS
OUR
BENCH
MARK
ON
AN
IMPORTANT
BUT
SIMPLER
CASE
ENERGY
EFFICIENCY
DURING
PEAK
USE
ENERGY
EFFICIENCY
AT
PEAK
IS
THE
OPPOSITE
EXTREME
FROM
IDLE
AND
GIVES
AN
UPPER
BOUND
ON
WORK
THAT
CAN
BE
DONE
FOR
A
GIVEN
ENERGY
THIS
OPERATING
POINT
INFLU
ENCES
DESIGN
AND
PROVISIONING
CONSTRAINTS
FOR
DATA
CENTERS
AS
WELL
AS
MOBILE
DEVICES
IN
ADDITION
FOR
SOME
APPLICATIONS
E
G
SCIENTIFIC
COMPUTING
NEAR
PEAK
USE
CAN
BE
THE
NORM
HOLISTIC
AND
BALANCED
A
SINGLE
COMPONENT
CANNOT
ACCU
RATELY
REFLECT
THE
OVERALL
PERFORMANCE
AND
POWER
CHARACTER
ISTICS
OF
A
SYSTEM
THEREFORE
THE
WORKLOAD
SHOULD
EXERCISE
ALL
CORE
COMPONENTS
AND
STRESS
THEM
ROUGHLY
EQUALLY
THE
BENCHMARK
METRICS
SHOULD
INCORPORATE
ENERGY
USED
BY
ALL
CORE
COMPONENTS
INCLUSIVE
AND
PORTABLE
WE
WANT
TO
ASSESS
THE
ENERGY
EF
FICIENCIES
OF
A
WIDE
VARIETY
OF
SYSTEMS
PDAS
LAPTOPS
DESK
TOPS
SERVERS
CLUSTERS
ETC
THUS
THE
BENCHMARK
SHOULD
INCLUDE
AS
MANY
ARCHITECTURES
AS
POSSIBLE
AND
BE
AS
UNBI
ASED
AS
POSSIBLE
IT
SHOULD
ALLOW
INNOVATIONS
IN
HARDWARE
AND
SOFTWARE
TECHNOLOGY
MOREOVER
THE
WORKLOAD
SHOULD
BE
IMPLEMENTABLE
AND
MEANINGFUL
ACROSS
THESE
PLATFORMS
HISTORY
PROOF
IN
ORDER
TO
TRACK
IMPROVEMENTS
OVER
GEN
ERATIONS
OF
SYSTEMS
AND
IDENTIFY
FUTURE
PROFITABLE
DIRECTIONS
WE
WANT
THE
BENCHMARK
SPECIFICATION
TO
REMAIN
MEANINGFUL
AND
COMPARABLE
AS
TECHNOLOGY
EVOLVES
REPRESENTATIVE
AND
SIMPLE
THE
BENCHMARK
SHOULD
BE
REPRESENTATIVE
OF
AN
IMPORTANT
CLASS
OF
WORKLOADS
ON
THE
SYS
TEMS
TESTED
IT
SHOULD
ALSO
BE
EASY
TO
SET
UP
EXECUTE
AND
ADMINISTER
WORKLOAD
WE
BEGIN
WITH
EXTERNAL
SORT
AS
SPECIFIED
IN
THE
PREVIOUS
SORT
BENCHMARKS
AS
THE
WORKLOAD
BECAUSE
IT
COVERS
MOST
OF
OUR
CRITERIA
THE
TASK
IS
TO
SORT
A
FILE
CONTAINING
RANDOMLY
PERMUTED
BYTE
RECORDS
WITH
BYTE
KEYS
THE
INPUT
FILE
MUST
BE
READ
FROM
AND
THE
OUTPUT
FILE
WRITTEN
TO
A
NON
VOLATILE
STORE
AND
ALL
INTERMEDIATE
FILES
MUST
BE
DELETED
THE
OUTPUT
FILE
MUST
BE
NEWLY
CREATED
IT
CANNOT
OVERWRITE
THE
INPUT
FILE
THIS
WORKLOAD
IS
REPRESENTATIVE
BECAUSE
MOST
PLATFORMS
FROM
LARGE
TO
SMALL
MUST
MANAGE
AN
EVER
INCREASING
SUP
PLY
OF
DATA
TO
DO
SO
THEY
ALL
PERFORM
SOME
TYPE
OF
I
O
CENTRIC
TASKS
CRITICAL
FOR
THEIR
USE
FOR
EXAMPLE
LARGE
SCALE
WEBSITES
RUN
PARALLEL
ANALYZES
OVER
VOLUMINOUS
LOG
DATA
ACROSS
THOUSANDS
OF
MACHINES
LAPTOPS
AND
SERVERS
CON
TAIN
VARIOUS
KINDS
OF
FILESYSTEMS
AND
DATABASES
CELL
PHONES
PDAS
AND
CAMERAS
STORE
RETRIEVE
AND
PROCESS
MULTIMEDIA
DATA
FROM
FLASH
MEMORY
WITH
PREVIOUS
SORT
IMPLEMENTATIONS
ON
CLUSTERS
SUPER
COMPUTERS
SMPS
AND
PCS
AS
EVIDENCE
WE
BELIEVE
SORT
IS
PORTABLE
AND
INCLUSIVE
IT
STRESSES
I
O
MEMORY
AND
THE
CPU
MAKING
IT
HOLISTIC
AND
BALANCED
MOREOVER
THE
FASTEST
SORTS
TEND
TO
RUN
MOST
COMPONENTS
AT
NEAR
PEAK
UTILIZATION
SO
SORT
IS
NOT
AN
IDLE
STATE
BENCHMARK
FINALLY
THIS
WORK
LOAD
IS
RELATIVELY
HISTORY
PROOF
WHILE
THE
PARAMETERS
HAVE
CHANGED
OVER
TIME
THE
ESSENTIAL
SORTING
TASK
HAS
BEEN
THE
SAME
SINCE
THE
ORIGINAL
DATAMATIONSORT
BENCHMARK
WAS
PROPOSED
IN
METRIC
AFTER
CHOOSING
THE
WORKLOAD
THE
NEXT
CHALLENGE
IS
CHOOS
ING
THE
METRIC
BY
WHICH
TO
EVALUATE
AND
COMPARE
DIFFERENT
SYSTEMS
THERE
ARE
MANY
WAYS
TO
DEFINE
A
SINGLE
METRIC
THAT
TAKES
BOTH
POWER
AND
PERFORMANCE
INTO
ACCOUNT
WE
LIST
SOME
ALTERNATIVES
THAT
WE
REJECTED
DESCRIBE
WHY
THEY
ARE
INAPPROPRIATE
AND
CHOOSE
THE
ONE
MOST
CONSISTENT
WITH
THE
CRITERIA
PRESENTED
IN
SECTION
FIXED
ENERGY
BUDGET
THE
MOST
INTUITIVE
EXTENSION
OF
MINUTESORT
AND
PENNYSORT
IS
TO
FIX
A
BUDGET
FOR
ENERGY
CONSUMPTION
AND
THEN
COM
PARE
THE
NUMBER
OF
RECORDS
SORTED
BY
DIFFERENT
SYSTEMS
WHILE
STAYING
WITHIN
THAT
ENERGY
BUDGET
THIS
APPROACH
HAS
TWO
DRAWBACKS
FIRST
THE
POWER
CONSUMPTION
OF
CURRENT
PLAT
FORMS
VARIES
BY
SEVERAL
ORDERS
OF
MAGNITUDE
LESS
THAN
FOR
HANDHELDS
TO
OVER
FOR
SERVERS
AND
MUCH
MORE
FOR
CLUSTERS
OR
SUPERCOMPUTERS
IF
THE
FIXED
ENERGY
BUDGET
IS
TOO
SMALL
LARGER
CONFIGURATIONS
CAN
ONLY
SORT
FOR
A
FRAC
TION
OF
A
SECOND
IF
THE
ENERGY
BUDGET
IS
MORE
APPROPRIATE
TO
LARGER
CONFIGURATIONS
SMALLER
CONFIGURATIONS
WOULD
RUN
OUT
OF
EXTERNAL
STORAGE
TO
BE
FAIR
AND
INCLUSIVE
WE
WOULD
NEED
MULTIPLE
BUDGETS
AND
CATEGORIES
FOR
DIFFERENT
CLASSES
OF
SYSTEMS
SECOND
AND
MORE
IMPORTANTLY
FROM
A
PRACTICAL
BENCH
MARKING
PERSPECTIVE
FINDING
THE
NUMBER
OF
RECORDS
TO
FIT
INTO
AN
ENERGY
BUDGET
IS
A
NON
TRIVIAL
TASK
DUE
TO
UNAVOID
ABLE
MEASUREMENT
ERROR
THERE
ARE
INACCURACIES
IN
SYNCHRO
NIZING
READINGS
FROM
A
POWER
METER
TO
THE
ACTUAL
RUNS
AND
FROM
THE
POWER
METER
ITSELF
FOR
THE
ONE
WE
USED
SINCE
ENERGY
IS
THE
PRODUCT
OF
POWER
AND
TIME
IT
IS
SUSCEP
TIBLE
TO
VARIATION
IN
BOTH
QUANTITIES
SO
THIS
CHOICE
IS
NOT
SIMPLE
FIXED
TIME
BUDGET
SIMILAR
TO
THE
MINUTE
AND
PERFORMANCE
PRICE
SORT
WE
CAN
FIX
A
TIME
BUDGET
E
G
ONE
MINUTE
WITHIN
WHICH
THE
GOAL
IS
TO
SORT
AS
MANY
RECORDS
AS
POSSIBLE
THE
WINNERS
FOR
THE
MINUTE
AND
PERFORMANCE
PRICE
SORTS
ARE
THOSE
WITH
THE
MIN
IMUM
TIME
AND
MAXIMUM
SORTEDRECS
RESPECTIVELY
SIM
ILARLY
OUR
FIRST
PROPOSAL
FOR
JOULESORT
SPECIFIED
MEASURING
ENERGY
AND
USED
SORTEDRECS
JOULE
AS
THE
RATIO
TO
MAXIMIZE
THERE
ARE
TWO
PROBLEMS
WITH
THIS
APPROACH
WHICH
ARE
ILLUSTRATED
BY
FIGURE
THIS
FIGURE
SHOWS
THE
SRECS
J
RA
TIO
FOR
VARYING
INPUT
SIZES
N
WITH
OUR
WINNING
JOULESORT
SYSTEM
WE
SEE
THAT
THE
RATIO
VARIES
CONSIDERABLY
WITH
N
THERE
ARE
TWO
DISTINCT
REGIONS
RECORDS
WHICH
RECORDS
SORTED
FIGURE
THIS
FIGURE
SHOWS
THE
BEST
MEASURED
EN
ERGY
EFFICIENCY
OF
OUR
WINNING
SYSTEM
AT
VARY
ING
INPUT
SIZES
CORRESPONDS
TO
PASS
SORTS
AND
RECORDS
WHICH
CORRESPONDS
TO
PASS
SORTS
TO
GET
THE
BEST
PERFORMANCE
FOR
PASS
SORTS
WE
STRIPE
THE
INPUT
AND
OUTPUT
ACROSS
DISKS
USING
AND
USE
DISKS
FOR
TEMPORARY
RUNS
FOR
PASS
SORTS
WE
STRIPE
THE
INPUT
AND
OUTPUT
ACROSS
DISKS
SEE
SECTION
FOR
MORE
SYSTEM
DETAILS
WITH
A
FIXED
TIME
BUDGET
APPROACH
THE
GOALS
OF
OUR
BENCHMARK
CAN
BE
UNDERMINED
IN
THE
FOLLOWING
WAYS
FOR
BOTH
ONE
AND
TWO
PASS
SORTS
SORT
PROGRESS
INCENTIVE
FIRST
IN
ANY
TIME
BUDGET
AP
PROACH
THERE
IS
NO
WAY
TO
ENFORCE
CONTINUAL
PROGRESS
SYS
TEMS
WILL
CONTINUE
SORTING
ONLY
IF
THE
MARGINAL
COST
OF
SORT
ING
AN
ADDITIONAL
RECORD
IS
LOWER
THAN
THE
COST
OF
SLEEPING
FOR
THE
REMAINING
TIME
THIS
TRADEOFF
BECOMES
PROBLEMATIC
WHEN
AN
ADDITIONAL
RECORD
MOVES
THE
SORT
FROM
PASS
TO
PASS
IN
THE
PASS
REGION
OF
FIGURE
THE
SORT
IS
I
O
LIM
ITED
SO
IT
DOES
NOT
RUN
TWICE
AS
FAST
AS
A
PASS
SORT
IT
GOES
FAST
ENOUGH
HOWEVER
TO
PROVIDE
ABOUT
BETTER
EFFICIENCY
THAN
PASS
SORTS
IF
THE
SYSTEM
WAS
DESIGNED
TO
HAVE
A
SUF
FICIENTLY
LOW
SLEEP
STATE
POWER
THEN
WITH
A
MINUTE
BUDGET
THE
BEST
APPROACH
WOULD
BE
TO
SORT
RECORDS
WHICH
TAKES
SEC
AND
SLEEP
FOR
THE
REMAINING
SEC
RE
SULTING
IN
A
BEST
SRECS
J
THUS
FOR
SOME
SYSTEMS
A
FIXED
TIME
BUDGET
DEFAULTS
INTO
ASSESSING
EFFICIENCY
WHEN
NO
WORK
IS
DONE
VIOLATING
OUR
CRITERIA
SORT
COMPLEXITY
SECOND
EVEN
IN
THE
PASS
REGION
TOTAL
ENERGY
IS
A
COMPLEX
FUNCTION
OF
MANY
PERFORMANCE
FACTORS
THAT
VARY
WITH
N
TOTAL
I
O
MEMORY
ACCESSES
COMPARISONS
CPU
UTILIZATION
AND
EFFECTIVE
PARALLELISM
FIGURE
SHOWS
THAT
ONCE
THE
SORT
BECOMES
CPU
BOUND
RECORDS
THE
SRECS
J
RATIO
TRENDS
SLOWLY
DOWNWARD
BECAUSE
TOTAL
EN
ERGY
INCREASES
SUPERLINEARLY
WITH
N
THE
RATIO
FOR
THE
LARGEST
SORT
IS
LOWER
THAN
THE
PEAK
THIS
DECREASE
IS
IN
PART
BECAUSE
SORTING
WORK
GROWS
AS
O
N
LG
N
DUE
TO
COMPAR
ISONS
AND
THE
O
NOTATION
HIDES
CONSTANTS
AND
LOWER
ORDER
OVERHEADS
THIS
EFFECT
IMPLIES
THAT
THE
METRIC
IS
BIASED
TO
WARD
SYSTEMS
THAT
SORT
FEWER
RECORDS
IN
THE
ALLOTTED
TIME
THAT
IS
EVEN
IF
TWO
FULLY
UTILIZED
SYSTEMS
A
AND
B
HAVE
SAME
TRUE
ENERGY
EFFICIENCY
AND
A
CAN
SORT
TWICE
AS
MANY
RECORDS
AS
B
IN
A
MINUTE
THE
SORTEDRECS
JOULE
RATIO
WILL
FAVOR
B
NOTE
SINCE
THIS
EFFECT
IS
SMALL
OUR
RELATIVE
COM
PARISONS
AND
CONCLUSIONS
IN
SECTION
REMAIN
VALID
OUR
CHOICE
FIXED
INPUT
SIZE
THE
FINAL
OPTION
THAT
WE
CONSIDERED
AND
SETTLED
UPON
WAS
TO
FIX
THE
NUMBER
OF
RECORDS
SORTED
AS
IN
THE
TERABYTE
SORT
BENCHMARK
AND
USE
TOTAL
ENERGY
AS
THE
METRIC
TO
MIN
IMIZE
FOR
THE
SAME
FAIRNESS
ISSUES
AS
IN
THE
FIXED
ENERGY
CASE
WE
DECIDED
TO
HAVE
THREE
SCALES
FOR
THE
INPUT
SIZE
AND
RECORDS
SIMILAR
TO
TPC
H
AND
DECLARE
WIN
NERS
IN
EACH
CATEGORY
FOR
CONSISTENCY
HENCEFORTH
WE
USE
MB
GB
AND
TB
FOR
AND
BYTES
RESPECTIVELY
FOR
A
FIXED
INPUT
SIZE
MINIMUM
ENERGY
AND
MAXIMUM
SORTE
DRECS
JOULE
ARE
EQUIVALENT
METRICS
IN
THIS
PAPER
WE
PREFER
THE
LATTER
BECAUSE
LIKE
AN
AUTOMOBILE
MILEAGE
RATING
IT
HIGHLIGHTS
ENERGY
EFFICIENCY
MORE
CLEARLY
THIS
APPROACH
HAS
ADVANTAGES
AND
DRAWBACKS
BUT
OFFERS
THE
BEST
COMPROMISE
GIVEN
OUR
CRITERIA
THESE
SCALES
COVER
A
LARGE
SPECTRUM
AND
NATURALLY
DIVIDE
THE
SYSTEMS
INTO
CLASSES
WE
EXPECT
LAPTOPS
DESKTOPS
AND
SERVERS
MOREOVER
SINCE
ENERGY
IS
A
PRODUCT
OF
POWER
AND
TIME
A
FIXED
WORK
AP
PROACH
IS
THE
SIMPLEST
FORMULATION
THAT
PROVIDES
AN
INCENTIVE
TO
OPTIMIZE
POWER
CONSUMPTION
AND
PERFORMANCE
BOTH
ARE
IMPORTANT
CONCERNS
FOR
CURRENT
COMPUTER
SYSTEMS
ONE
DISADVANTAGE
IS
THAT
AS
TECHNOLOGIES
IMPROVE
SCALES
MUST
BE
ADDED
AT
THE
HIGHER
END
AND
MAY
NEED
TO
BE
DEP
RECATED
AT
THE
LOWER
END
FOR
EXAMPLE
IF
THE
PERFORMANCE
OF
JOULESORT
WINNERS
IMPROVES
AT
THE
RATE
OF
MOORE
LAW
YEAR
A
SYSTEM
WHICH
SORTS
A
IN
SEC
TO
DAY
WOULD
ONLY
TAKE
SEC
IN
YEARS
ONCE
ALL
RELEVANT
SYSTEMS
REQUIRE
ONLY
A
FEW
SECONDS
FOR
A
SCALE
THAT
SCALE
BECOMES
OBSOLETE
SINCE
EVEN
THE
BEST
PERFORMING
SORTS
ARE
NOT
IMPROVING
WITH
MOORE
LAW
WE
EXPECT
THESE
SCALES
TO
BE
RELEVANT
FOR
AT
LEAST
YEARS
FINALLY
BECAUSE
COMPAR
ISON
ACROSS
SCALES
IS
MISLEADING
OUR
APPROACH
IS
NOT
FULLY
HISTORY
PROOF
CATEGORIES
AS
WITH
THE
OTHER
SORT
BENCHMARKS
WE
PRO
POSE
TWO
CATEGORIES
FOR
JOULESORT
DAYTONA
FOR
COMMER
CIALLY
SUPPORTED
SORTS
AND
INDY
FOR
NO
HOLDS
BARRED
IM
PLEMENTATIONS
SINCE
DAYTONA
SORTS
ARE
COMMERCIALLY
SUP
PORTED
THE
HARDWARE
COMPONENTS
MUST
BE
OFF
THE
SHELF
AND
UNMODIFIED
AND
RUN
A
COMMERCIALLY
SUPPORTED
OS
AS
WITH
THE
OTHER
SORT
BENCHMARKS
WE
EXPECT
ENTRANTS
TO
REPORT
THE
COST
OF
THE
SYSTEM
MEASURING
ENERGY
THERE
ARE
A
NUMBER
OF
ISSUES
SURROUNDING
THE
PROPER
AC
COUNTING
OF
ENERGY
USE
SPECIFIC
PROPOSALS
IN
THE
POWER
MANAGEMENT
COMMUNITY
FOR
MEASURING
ENERGY
ARE
BEING
DE
BATED
AND
ARE
STILL
UNTESTED
IN
THE
LARGE
ONCE
THESE
ARE
AGREED
UPON
WE
PLAN
TO
ADOPT
THE
RELEVANT
PORTIONS
FOR
THIS
BENCHMARK
AS
A
START
WE
PROPOSE
GUIDELINES
FOR
THREE
AREAS
THE
BOUNDARIES
OF
THE
SYSTEM
TO
BE
MEASURED
ENVI
RONMENTAL
CONSTRAINTS
AND
ENERGY
MEASUREMENT
SYSTEM
BOUNDARIES
OUR
AIM
IS
TO
ACCOUNT
FOR
ALL
EN
ERGY
CONSUMED
TO
POWER
THE
PHYSICAL
SYSTEM
EXECUTING
THE
SORT
ALL
POWER
IS
MEASURED
FROM
THE
WALL
AND
INCLUDES
ANY
CONVERSION
LOSSES
FROM
POWER
SUPPLIES
FOR
BOTH
AC
AND
DC
SYSTEMS
POWER
SUPPLIES
ARE
A
CRITICAL
COMPONENT
IN
DELIV
ERING
POWER
AND
IN
THE
PAST
HAVE
BEEN
NOTORIOUSLY
INEFFI
CIENT
SOME
DC
SYSTEMS
ESPECIALLY
MOBILE
DEVICES
CAN
RUN
FROM
BATTERIES
AND
THOSE
BATTERIES
MUST
EVENTUALLY
BE
RECHARGED
WHICH
ALSO
INCURS
CONVERSION
LOSS
WHILE
THE
LOSS
FROM
RECHARGING
MAY
BE
DIFFERENT
FROM
THE
LOSS
FROM
SYSTEM
CPU
MEMORY
DISK
OS
FS
INTEL
XEON
GHZ
DDR
LINUX
XFS
BLADE
TRANSMETA
EFFICEON
GHZ
SDRAM
WINDOWS
NTFS
INTEL
CORE
DUO
WINDOWS
XP
NTFS
TABLE
THE
UNBALANCED
SYSTEMS
MEASURED
IN
EXPLORING
ENERGY
EFFICIENCY
TRADEOFFS
FOR
SORT
THE
ADAPTER
THAT
POWERS
A
DEVICE
DIRECTLY
FOR
SIMPLICITY
WE
ALLOW
MEASUREMENTS
THAT
INCLUDE
ONLY
ADAPTERS
ALL
HARDWARE
COMPONENTS
USED
TO
SORT
THE
INPUT
RECORDS
FROM
START
TO
FINISH
IDLE
OR
OTHERWISE
MUST
BE
INCLUDED
IN
THE
ENERGY
MEASUREMENT
IF
SOME
COMPONENT
IS
UNUSED
BUT
CANNOT
BE
POWERED
DOWN
OR
PHYSICALLY
SEPARATED
FROM
ADJA
CENT
PARTICIPATING
COMPONENTS
THEN
ITS
POWER
USE
MUST
BE
INCLUDED
IF
THERE
IS
ANY
POTENTIAL
ENERGY
STORED
WITHIN
THE
SYSTEM
E
G
IN
BATTERIES
THE
NET
CHANGE
IN
POTENTIAL
ENERGY
MUST
BE
NO
GREATER
THAN
ZERO
JOULES
WITH
CONFIDENCE
OR
IT
MUST
BE
INCLUDED
WITHIN
THE
ENERGY
MEASUREMENT
ENVIRONMENT
THE
ENERGY
COSTS
OF
COOLING
ARE
IMPORTANT
AND
COOLING
SYSTEMS
ARE
VARIEGATED
AND
OPERATE
AT
MANY
LEV
ELS
IN
A
TYPICAL
DATA
CENTER
THERE
ARE
AIR
CONDITIONERS
BLOW
ERS
AND
RECIRCULATORS
TO
DIRECT
AND
MOVE
AIR
AMONG
AISLES
AND
HEAT
SINKS
AND
FANS
TO
DISTRIBUTE
AND
EXTRACT
HEAT
AWAY
FROM
SYSTEM
COMPONENTS
GIVEN
RECENT
TRENDS
IN
ENERGY
DENSITY
FUTURE
SYSTEMS
MAY
EVEN
HAVE
LIQUID
COOLING
IT
IS
DIFFI
CULT
TO
INCORPORATE
ANTICIPATE
AND
ENFORCE
RULES
FOR
ALL
SUCH
COSTS
IN
A
SYSTEM
LEVEL
BENCHMARK
FOR
SIMPLICITY
WE
ONLY
INCLUDE
A
PART
OF
THIS
COST
ONE
THAT
IS
EASILY
MEASURABLE
AND
ASSOCIATED
WITH
THE
SYSTEM
BEING
MEASURED
WE
SPECIFY
THAT
A
TEMPERATURE
BETWEEN
C
SHOULD
BE
MAINTAINED
AT
THE
SYSTEM
INLETS
OR
WITHIN
FOOT
OF
THE
SYSTEM
IF
NO
INLET
EXISTS
ENERGY
USED
BY
DEVICES
PHYSICALLY
ATTACHED
TO
THE
SORTING
HARDWARE
THAT
REMOVE
HEAT
TO
MAINTAIN
THIS
TEMPER
ATURE
E
G
FANS
MUST
BE
INCLUDED
ENERGY
USE
TOTAL
ENERGY
IS
THE
PRODUCT
OF
AVERAGE
POWER
OVER
THE
SORT
EXECUTION
AND
WALL
CLOCK
TIME
AS
WITH
THE
OTHER
SORT
BENCHMARKS
WALL
CLOCK
TIME
IS
MEASURED
USING
AN
EXTERNAL
SOFTWARE
TIMER
THE
EASIEST
METHOD
TO
MEASURE
POWER
FOR
MOST
SYSTEMS
WILL
BE
TO
INSERT
A
DIGITAL
POWER
ME
TER
BETWEEN
THE
SYSTEM
AND
THE
WALL
WE
INTEND
TO
LEVERAGE
THE
MINIMUM
POWER
METER
REQUIREMENTS
FROM
THE
SPEC
POWER
DRAFT
IN
PARTICULAR
THE
METER
MUST
REPORT
REAL
POWER
INSTEAD
OF
APPARENT
POWER
SINCE
REAL
POWER
REFLECTS
THE
TRUE
ENERGY
CONSUMED
AND
CHARGED
FOR
BY
UTILITIES
WHILE
WE
DO
NOT
PENALIZE
FOR
POOR
POWER
FACTORS
A
POWER
FACTOR
MEASURED
ANYTIME
DURING
THE
SORT
RUN
SHOULD
BE
RE
PORTED
FINALLY
SINCE
ENERGY
MEASUREMENTS
ARE
OFTEN
NOISY
A
MINIMUM
OF
THREE
CONSECUTIVE
ENERGY
READINGS
MUST
BE
RE
PORTED
THESE
WILL
BE
AVERAGED
AND
THE
SYSTEM
WITH
MEAN
ENERGY
LOWER
THAN
ALL
OTHERS
INCLUDING
PREVIOUS
YEARS
WITH
CONFIDENCE
WILL
BE
DECLARED
THE
WINNER
SUMMARY
IN
SUMMARY
THE
JOULESORT
BENCHMARK
IS
AS
FOLLOWS
SORT
A
FIXED
NUMBER
OF
RANDOMLY
PERMUTED
BYTE
RECORDS
WITH
BYTE
KEYS
THE
SORT
MUST
START
WITH
INPUT
IN
A
FILE
ON
NON
VOLATILE
STORE
AND
FINISH
WITH
OUTPUT
IN
A
FILE
ON
NON
VOLATILE
STORE
THERE
ARE
THREE
SCALE
CATEGORIES
FOR
JOULESORT
AND
RECORDS
THE
TOTAL
TRUE
ENERGY
CONSUMED
BY
THE
ENTIRE
PHYSICAL
SYSTEM
EXECUTING
THE
SORT
WHILE
MAINTAINING
AN
AMBI
ENT
TEMPERATURE
BETWEEN
SHOULD
BE
REPORTED
THE
WINNER
IN
EACH
CATEGORY
IS
THE
SYSTEM
WITH
THE
MAXIMUM
SORTEDRECS
JOULE
I
E
MINIMUM
ENERGY
JOULESORT
IS
A
REASONABLE
CHOICE
AMONG
MANY
POSSIBLE
OPTIONS
FOR
AN
ENERGY
ORIENTED
BENCHMARK
IT
IS
AN
I
O
CENTRIC
SYSTEM
LEVEL
ENERGY
EFFICIENCY
BENCHMARK
THAT
IN
CORPORATES
PERFORMANCE
POWER
AND
SOME
COOLING
COSTS
IT
IS
BALANCED
PORTABLE
REPRESENTATIVE
AND
SIMPLE
WE
CAN
USE
IT
TO
COMPARE
DIFFERENT
EXISTING
SYSTEMS
TO
EVALUATE
THE
ENERGY
EFFICIENCY
BALANCE
OF
COMPONENTS
WITHIN
A
GIVEN
SYSTEM
AND
TO
EVALUATE
DIFFERENT
ALGORITHMS
THAT
USE
THESE
COMPONENTS
THESE
FEATURES
ALLOW
US
TO
CHART
PAST
TRENDS
IN
ENERGY
EFFICIENCY
AND
HOPEFULLY
WILL
HELP
PREDICT
FUTURE
TRENDS
A
LOOK
AT
DIFFERENT
SYSTEMS
IN
THIS
SECTION
WE
MEASURE
THE
ENERGY
AND
PERFORMANCE
OF
A
SORT
WORKLOAD
ON
BOTH
UNBALANCED
AND
BALANCED
SORT
ING
SYSTEMS
WE
ANALYZE
A
VARIETY
OF
SYSTEMS
FROM
LAPTOPS
TO
SERVERS
THAT
WERE
READILY
AVAILABLE
IN
OUR
LAB
FOR
THE
UNBALANCED
SYSTEMS
THE
GOAL
OF
THESE
EXPERIMENTS
IS
NOT
TO
PAINSTAKINGLY
TUNE
THESE
CONFIGURATIONS
RATHER
WE
PRESENT
RESULTS
TO
EXPLORE
THE
SYSTEM
HARDWARE
SPACE
WITH
RESPECT
TO
POWER
CONSUMPTION
AND
ENERGY
EFFICIENCY
FOR
SORT
AFTER
LOOKING
AT
UNBALANCED
SYSTEMS
WE
PRESENT
A
BALANCED
FILE
SERVER
THAT
IS
OUR
DEFAULT
WINNER
WE
USE
INSIGHTS
FROM
THESE
EXPERIMENTS
TO
JUSTIFY
THE
APPROACH
FOR
CONSTRUCTING
OUR
JOULESORT
WINNER
SEE
SECTION
UNBALANCED
SYSTEMS
CONFIGURATIONS
TABLE
SHOWS
THE
DETAILS
OF
THE
UNBAL
ANCED
SYSTEMS
WE
EVALUATED
SPANNING
A
REASONABLE
SPEC
TRUM
OF
POWER
CONSUMPTION
IN
SERVERS
AND
PERSONAL
COM
PUTERS
WE
INCLUDE
A
SERVER
AN
OLDER
LOW
POWER
BLADE
AND
AN
A
MODERN
LAPTOP
WE
CHOSE
THE
LAPTOP
BE
CAUSE
IT
IS
DESIGNED
FOR
WHOLE
SYSTEM
ENERGY
CONSERVATION
AND
AND
FOR
COMPARISON
WE
TURNED
OFF
THE
LAPTOP
DISPLAY
FOR
THESE
EXPERIMENTS
FOR
WE
ONLY
USED
BLADE
IN
AN
ENCLOSURE
THAT
HOLDS
AND
AS
PER
OUR
RULES
REPORT
THE
POWER
OF
THE
ENTIRE
SYSTEM
SORT
WORKLOAD
WE
USE
ORDINAL
TECHNOLOGY
COMMERCIAL
NSORT
SOFTWARE
WHICH
WAS
THE
TERABYTE
SORT
DAYTONA
WINNER
IT
USES
ASYNCHRONOUS
I
O
TO
OVERLAP
READING
WRIT
ING
AND
SORTING
OPERATIONS
IT
PERFORMS
BOTH
ONE
AND
TWO
PASS
SORTS
WE
TUNED
NSORT
PARAMETERS
TO
GET
THE
BEST
PERFORMING
SORT
FOR
EACH
PLATFORM
UNLESS
OTHERWISE
STATED
WE
USE
THE
RADIX
IN
MEMORY
SORT
OPTION
RECS
POWER
W
TIME
SRECS
J
CPU
UTIL
TABLE
ENERGY
EFFICIENCY
OF
UNBALANCED
SYSTEMS
POWER
MEASUREMENT
TO
MEASURE
THE
FULL
SYSTEM
AC
POWER
CONSUMPTION
WE
USED
A
DIGITAL
POWER
METER
INTER
POSED
BETWEEN
THE
SYSTEM
AND
THE
WALL
OUTLET
WE
SAMPLED
THIS
POWER
AT
A
RATE
OF
ONCE
PER
SECOND
THE
METER
USED
WAS
BRAND
ELECTRONICS
MODEL
WHICH
REPORTS
TRUE
POWER
WITH
ACCURACY
IN
THIS
PAPER
WE
ALWAYS
RE
PORT
THE
AVERAGE
POWER
OVER
SEVERAL
TRIALS
AND
THE
STANDARD
DEVIATION
IN
THE
AVERAGE
POWER
RESULTS
THE
JOULESORT
RESULTS
FOR
OUR
UNBALANCED
SYSTEMS
ARE
SHOWN
IN
TABLE
SINCE
DISK
SPACE
ON
THESE
SYSTEMS
WAS
LIMITED
WE
CHOSE
TO
RUN
THE
BENCHMARK
AT
AND
A
SMALLER
DATASET
TO
ALLOW
FAIR
COMPARISON
WE
SEE
THAT
THE
SERVER
IS
THE
FASTEST
BUT
THE
LAPTOP
IS
MOST
ENERGY
EFFICIENT
SYSTEM
USES
OVER
MORE
POWER
THAN
BUT
ONLY
PROVIDES
BETTER
PERFORMANCE
ALTHOUGH
DISKS
CAN
PROVIDE
MORE
SEQUENTIAL
BANDWIDTH
WAS
LIMITED
BY
ITS
SMARTARRAY
I
O
CONTROLLER
TO
MB
IN
EACH
PASS
SYS
TEM
THE
BLADE
IS
NOT
AS
BAD
AS
THE
RESULTS
SHOW
BECAUSE
BLADE
ENCLOSURES
ARE
MOST
EFFICIENT
ONLY
WHEN
FULLY
POPU
LATED
THE
ENCLOSURE
POWER
WITHOUT
ANY
BLADES
WAS
WHEN
WE
SUBTRACT
THIS
FROM
THE
TOTAL
POWER
WE
GET
AN
UPPER
BOUND
OF
SRECS
J
FOR
FOR
ALL
THESE
SYSTEMS
THE
STANDARD
DEVIATION
OF
TOTAL
POWER
DURING
SORT
WAS
AT
MOST
THE
POWER
FACTOR
PF
FOR
AND
WERE
AND
RESPECTIVELY
THE
CPUS
FOR
ALL
THREE
SYSTEMS
WERE
HIGHLY
UNDERUTILIZED
IN
PARTICULAR
ATTAINS
AN
ENERGY
EFFICIENCY
SIMILAR
TO
THAT
OF
LAST
YEAR
ESTIMATED
WINNER
GPUTERASORT
BY
BARELY
US
ING
ITS
CORES
SINCE
THE
CPU
IS
USUALLY
THE
HIGHEST
POWER
COMPONENT
THESE
RESULTS
SUGGEST
THAT
BUILDING
A
SYSTEM
WITH
MORE
I
O
TO
COMPLEMENT
THE
AVAILABLE
PROCESSING
CAPACITY
SHOULD
PROVIDE
BETTER
ENERGY
EFFICIENCIES
BALANCED
SERVER
IN
THIS
SECTION
WE
PRESENT
A
BALANCED
SYSTEM
THAT
USUALLY
FUNCTIONS
AS
A
FILESERVER
IN
OUR
LAB
TABLE
SHOWS
THE
COM
PONENTS
USED
DURING
THE
SORT
AND
COARSE
BREAKDOWNS
OF
TOTAL
SYSTEM
POWER
THE
MAIN
SYSTEM
IS
AN
HP
PROLIANT
THAT
INCLUDES
A
MOTHERBOARD
CPU
LOW
POWER
LAPTOP
DISK
AND
A
HIGH
THROUGHPUT
SAS
I
O
CONTROLLER
FOR
THE
STORAGE
WE
USE
TWO
DISK
TRAYS
ONE
THAT
HOLDS
THE
INPUT
AND
OUTPUT
FILES
AND
THE
OTHER
WHICH
HOLDS
THE
TEMP
DISKS
EACH
TRAY
HAS
DISKS
AND
CAN
HOLD
A
MAXIMUM
OF
THE
DISK
TRAYS
AND
MAIN
SYSTEM
ALL
HAVE
DUAL
POWER
SUPPLIES
BUT
FOR
THESE
EXPERIMENTS
WE
POWERED
THEM
THROUGH
ONE
EACH
FOR
ALL
OUR
EXPERIMENTS
THE
SYSTEM
HAS
BIT
UBUNTU
LINUX
AND
THE
XFS
FILESYSTEM
INSTALLED
TABLE
SHOWS
THAT
FOR
A
SERVER
OF
THIS
KIND
THE
DISKS
AND
THEIR
ENCLOSURES
CONSUME
ROUGHLY
THE
SAME
POWER
AS
THE
REST
OF
THE
SYSTEM
WHEN
A
TRAY
IS
FULLY
POPULATED
WITH
DISKS
TABLE
A
BALANCED
FILESERVER
THE
IDLE
POWER
IS
W
AND
WITH
DISKS
THE
IDLE
POWER
IS
W
THERE
CLEARLY
ARE
INEFFICIENCIES
WHEN
THE
TRAY
IS
UNDER
UTILIZED
TO
ESTIMATE
THE
POWER
OF
THE
DIMMS
WE
ADDED
TWO
DIMMS
AND
MEASURED
THE
SYSTEM
POWER
WITH
AND
WITHOUT
THE
DIMMS
WE
FOUND
THAT
THE
DIMMS
USE
BOTH
DURING
SORT
AND
AT
IDLE
FOR
THIS
SYSTEM
WE
FOUND
THE
MOST
ENERGY
EFFICIENT
CONFIG
URATION
BY
EXPERIMENTING
WITH
A
DATASET
BY
VARYING
THE
NUMBER
OF
DISKS
USED
WE
FOUND
THAT
EVEN
WITH
THE
INEF
FICIENCIES
THE
BEST
PERFORMING
SETUP
USES
DISKS
SPLIT
ACROSS
TWO
TRAYS
THIS
EFFECT
HAPPENS
BECAUSE
THE
I
O
CON
TROLLER
OFFERS
BETTER
BANDWIDTH
WHEN
DATA
IS
SHIPPED
ACROSS
ITS
TWO
CHANNELS
A
SORT
PROVIDES
ON
AV
ERAGE
FOR
EACH
PHASE
ACROSS
THE
TRAYS
WHILE
ONLY
WHEN
THE
ALL
DISKS
ARE
WITHIN
A
TRAY
THE
AVERAGE
POWER
OF
THE
SYSTEM
WITH
ONLY
ONE
TRAY
IS
AND
WITH
TWO
TRAYS
IS
AS
A
RESULT
WITH
TWO
TRAYS
THE
SYSTEM
ATTAINS
A
BEST
SRECS
J
INSTEAD
OF
SRECS
J
WITH
ONE
TRAY
THE
TRAY
DISK
SETUP
IS
ALSO
WHEN
THE
SORT
BECOMES
CPU
BOUND
WHEN
WE
REDUCE
THE
SYSTEM
TO
DISKS
THE
I
O
PERFORMANCE
AND
CPU
UTILIZATION
DROP
AND
WHEN
WE
INCREASE
THE
SYSTEM
TO
DISKS
THE
PERFORMANCE
AND
UTI
LIZATION
REMAIN
THE
SAME
IN
BOTH
CASES
TOTAL
ENERGY
IS
HIGHER
THAN
THE
DISK
POINT
SO
THIS
BALANCED
CPU
BOUND
CONFIGURATION
IS
ALSO
THE
MOST
ENERGY
EFFICIENT
TABLE
SHOWS
THE
PERFORMANCE
AND
ENERGY
CHARACTERISTICS
OF
THE
DISK
SETUP
FOR
SORTS
THIS
SYSTEM
TAKES
NEARLY
MORE
POWER
THAN
BUT
PROVIDES
OVER
THE
THROUGH
PUT
THIS
SYSTEM
SRECS
J
RATIO
BEATS
THE
LAPTOP
AND
LAST
YEAR
ESTIMATED
WINNER
EVEN
WITH
A
LARGER
INPUT
EX
PERIMENTS
SIMILAR
TO
THOSE
FOR
THE
DATASET
SHOW
THAT
THIS
SETUP
PROVIDES
JUST
ENOUGH
I
O
TO
KEEP
THE
TWO
CORES
FULLY
UTILIZED
ON
BOTH
PASSES
AND
USES
THE
MINIMUM
ENERGY
FOR
THE
SCALE
THUS
AT
ALL
SCALES
THE
MOST
ENERGY
EFFICIENT
AND
BEST
PERFORMING
CONFIGURATION
FOR
THIS
SYSTEM
IS
WHEN
SORT
IS
CPU
BOUND
AND
BALANCED
COMP
MODEL
PRICE
POWER
CPU
INTEL
CORE
DUO
TDP
MOTHERBOARD
ASUS
VM
DH
N
A
CASE
PSU
APEVIA
X
NAVIGATOR
BK
N
A
DISK
CTRL
HIGHPOINT
ROCKET
RAID
DISK
CTRL
HIGHPOINT
ROCKET
RAID
MEMORY
KINGSTON
SPEC
DISK
HITACHI
TRAVELSTAR
RPM
GB
A
I
SPEC
ADAPTERS
TABLE
WINNING
SYSTEM
SUMMARY
IN
CONCLUSION
FROM
EXPERIMENTING
WITH
THESE
SYSTEMS
WE
LEARNED
CPU
IS
WASTED
IN
UNBALANCED
SYSTEMS
THE
MOST
ENERGY
EFFICIENT
SERVER
CONFIGURATION
IS
WHEN
THE
SYS
TEM
IS
CPU
BOUND
AN
UNBALANCED
LAPTOP
IS
ALMOST
AS
ENERGY
EFFICIENT
AS
A
BALANCED
SERVER
MOREOVER
CURRENT
LAP
TOP
DRIVES
USE
VS
W
LESS
POWER
THAN
OUR
SERVER
SATA
DRIVES
WHILE
OFFERING
AROUND
VS
MB
THE
BANDWIDTH
THESE
OBSERVATIONS
SUGGEST
A
REASONABLE
AP
PROACH
FOR
BUILDING
THE
MOST
ENERGY
EFFICIENT
SORTING
SYSTEM
IS
TO
USE
MOBILE
CLASS
CPUS
AND
DISKS
AND
CONNECT
THEM
VIA
A
HIGH
SPEED
I
O
INTERCONNECT
JOULESORT
WINNER
IN
THIS
SECTION
WE
FIRST
DESCRIBE
OUR
WINNING
JOULESORT
CONFIGURATION
AND
REPORT
ITS
PERFORMANCE
WE
THEN
STUDY
THIS
SYSTEM
THROUGH
EXPERIMENTS
THAT
ELUCIDATE
POWER
AND
PERFORMANCE
CHARACTERISTICS
OF
THIS
SYSTEM
WINNING
CONFIGURATION
GIVEN
LIMITED
TIME
AND
BUDGET
OUR
GOAL
WAS
TO
CONVINC
INGLY
OVERTAKE
THE
PREVIOUS
ESTIMATED
WINNER
RATHER
THAN
TO
TRY
NUMEROUS
COMBINATIONS
AND
CONSTRUCT
AN
ABSOLUTE
OP
TIMAL
SYSTEM
AS
AS
RESULT
WE
DECIDED
TO
BUILD
A
DAYTONA
SYSTEM
AND
SOLELY
USE
NSORT
AS
THE
SOFTWARE
OUR
DESIGN
STRATEGY
FOR
AN
ENERGY
EFFICIENT
SORT
WAS
TO
BUILD
A
BALANCED
SORTING
SYSTEM
OUT
OF
LOW
POWER
COMPONENTS
AFTER
ESTI
MATING
THE
SORTING
EFFICIENCY
OF
POTENTIAL
SYSTEMS
AMONG
A
LIMITED
COMBINATION
OF
MODERN
LOW
POWER
PROCESSORS
AND
LAPTOP
DISKS
WE
ASSEMBLED
THE
CONFIGURATION
IN
TABLE
THIS
SYSTEM
USES
A
MODERN
LOW
POWER
CPU
WITH
FRE
QUENCY
STATES
AND
A
TDP
OF
FOR
THE
HIGHEST
STATE
WE
USE
A
MOTHERBOARD
THAT
SUPPORTS
BOTH
A
MOBILE
CPU
AND
MULTIPLE
DISK
CONTROLLERS
TO
KEEP
THE
CORES
BUSY
FEW
SUCH
BOARDS
EXIST
BECAUSE
THEY
TARGET
A
NICHE
MARKET
THIS
ONE
INCLUDES
TWO
PCI
E
SLOTS
ONE
CHANNEL
AND
ONE
CHANNEL
TO
FILL
THOSE
SLOTS
WE
USE
CONTROLLERS
THAT
HOLD
AND
SATA
DRIVES
RESPECTIVELY
FINALLY
OUR
CONFIGURATION
USES
LOW
POWER
LAPTOP
DRIVES
WHICH
SUPPORT
THE
SATA
IN
TERFACE
THEY
OFFER
AN
AVERAGE
MS
SEEK
TIME
AND
THEIR
MEASURED
SEQUENTIAL
BANDWIDTH
THROUGH
XFS
IS
AROUND
MB
HITACHI
SPECS
LIST
AN
AVERAGE
FOR
READ
AND
WRITE
AND
FOR
ACTIVE
IDLE
WE
USE
TWO
DIMMS
WHOSE
SPECS
REPORT
FOR
EACH
FINALLY
THE
CASE
COMES
WITH
A
POWER
SUPPLY
OUR
OPTIMAL
CONFIGURATION
USES
DISKS
BECAUSE
THE
PCI
E
CARDS
HOLD
DISKS
MAXIMUM
AND
THE
I
O
PERFORMANCE
OF
THE
MOTHERBOARD
CONTROLLER
WITH
MORE
THAN
DISK
IS
POOR
THE
INPUT
AND
OUTPUT
FILES
ARE
STRIPED
ACROSS
A
DISK
ARRAY
CONFIGURED
VIA
AND
THE
REMAINING
DISKS
ARE
INDE
PENDENT
FOR
THE
TEMPORARY
RUNS
FOR
ALL
EXPERIMENTS
WE
USE
LINUX
KERNEL
AND
THE
XFS
FILESYSTEM
UNLESS
OTHERWISE
STATED
IN
THE
IDLE
STATE
AT
THE
LOWEST
CPU
FREQUENCY
WE
MEASURED
W
FOR
THIS
SYSTEM
TABLE
SHOWS
THE
PERFORMANCE
OF
THE
SYSTEM
WHICH
AT
TAINS
SRECS
J
WHEN
AVERAGED
OVER
CONSECUTIVE
RUNS
THE
PURE
PERFORMANCE
STATISTICS
ARE
REPORTED
BY
NSORT
WE
CONFIGURE
IT
TO
USE
RADIX
SORT
AS
ITS
IN
MEMORY
SORT
ALGO
RITHM
AND
USE
TRANSFER
SIZES
OF
FOR
THE
INPUT
OUTPUT
ARRAY
AND
FOR
THE
TEMPORARY
STORAGE
OUR
SYSTEM
IS
FASTER
THAN
GPUTERASORT
AND
CONSUMES
AN
ESTIMATED
LESS
POWER
THE
POWER
USE
DURING
SORT
IS
MORE
THAN
IDLE
IN
THE
OUTPUT
PASS
THE
CPU
IS
UNDERUTILIZED
SEE
TA
BLE
MAX
FOR
CORES
AND
THE
BANDWIDTH
IS
LOWER
THAN
IN
THE
INPUT
PASS
BECAUSE
THE
OUTPUT
PASS
REQUIRES
RAN
DOM
I
OS
WE
PIN
THE
CPU
TO
MHZ
WHICH
SECTION
SHOWS
IS
THE
MOST
ENERGY
EFFICIENT
FREQUENCY
FOR
THE
SORT
VARYING
SYSTEM
SIZE
IN
THESE
EXPERIMENTS
WE
VARY
THE
SYSTEM
SIZE
DISKS
AND
CONTROLLERS
AND
OBSERVE
OUR
SYSTEM
PURE
PERFORMANCE
COST
EFFICIENCY
AND
ENERGY
EFFICIENCY
WE
INVESTIGATE
THESE
MET
RICS
USING
A
DATASET
FOR
THE
FIRST
TWO
METRICS
WE
SET
THE
CPU
TO
ITS
HIGHEST
FREQUENCY
AND
REPORT
THE
METRICS
FOR
THE
MOST
COST
EFFECTIVE
AND
BEST
PERFORMING
CONFIGURATIONS
AT
EACH
STEP
WE
START
WITH
DISKS
ATTACHED
TO
THE
CHEAPER
DISK
CONTROLLER
AND
AT
EACH
STEP
USE
THE
MINIMUM
COST
HARDWARE
TO
SUPPORT
AN
ADDITIONAL
DISK
THUS
WE
SWITCH
TO
THE
DISK
CONTROLLER
FOR
CONFIGURATIONS
WITH
DISKS
AND
USE
BOTH
CONTROLLERS
COMBINED
FOR
DISKS
FINALLY
WE
ADD
A
DISK
DIRECTLY
TO
THE
MOTHERBOARD
FOR
THE
DISK
CON
FIGURATION
FIGURE
SHOWS
THE
PERFORMANCE
RECORDS
SEC
AND
COST
EFFICIENCY
WITH
INCREASING
SYSTEM
SIZE
THE
DISK
CONFIG
URATION
IS
BOTH
THE
BEST
PERFORMING
AND
MOST
COST
EFFICIENT
POINT
EACH
ADDITIONAL
DISK
ON
AVERAGE
INCREASES
SYSTEM
COST
BY
ABOUT
AND
IMPROVES
PERFORMANCE
BY
ON
AVERAGE
THESE
MARGINAL
CHANGES
VARY
THEY
ARE
LARGER
FOR
SMALL
SYS
TEM
SIZE
AND
SMALLER
FOR
LARGER
SYSTEM
SIZES
THE
DISK
POINT
DROPS
IN
COST
EFFICIENCY
BECAUSE
IT
INCLUDES
THE
EXPEN
SIVE
DISK
CONTROLLER
WITHOUT
A
COMMENSURATE
PERFORMANCE
INCREASE
ALTHOUGH
THE
MOTHERBOARD
AND
CONTROLLERS
LIMIT
THE
SYSTEM
TO
DISKS
WE
SPECULATE
THAT
ADDITIONAL
DISKS
WOULD
NOT
HELP
SINCE
THE
FIRST
PASS
OF
THE
SORT
IS
CPU
BOUND
NEXT
WE
LOOK
AT
HOW
ENERGY
EFFICIENCY
VARIES
WITH
WITH
SYSTEM
SIZE
AT
EACH
STEP
WE
ADD
THE
MINIMUM
ENERGY
HARD
WARE
TO
SUPPORT
THE
ADDED
DISK
AND
REPORT
THE
MOST
ENERGY
EFFICIENT
SETUP
WE
SET
THE
CPU
FREQUENCY
TO
AT
ALL
POINTS
TO
GET
THE
BEST
ENERGY
EFFICIENCY
SEE
SECTION
FOR
CONVENIENCE
WE
HAD
ONE
EXTRA
OS
DISK
ON
THE
MOTHER
BOARD
FROM
WHICH
WE
BOOT
AND
WHICH
WAS
UNUSED
IN
THE
SORT
FOR
ALL
BUT
THE
LAST
POINT
THE
POWER
MEASUREMENTS
INCLUDE
THIS
DISK
BUT
THIS
POWER
IS
NEGLIGIBLE
AT
IDLE
SYSTEM
RECS
SRECS
J
ENERGY
KJ
POWER
W
TIME
SEC
BW
IN
OUT
TOTAL
MB
CPU
UTIL
MAX
PF
TABLE
LOW
POWER
99
TABLE
SERVER
TABLE
PERFORMANCE
OF
WINNING
JOULESORT
SYSTEMS
DISKS
USED
DISKS
USED
FIGURE
SHOWS
HOW
PERFORMANCE
PRICE
AND
PERFOR
MANCE
VARIES
WITH
SYSTEM
SIZE
SORT
IDLE
DISKS
USED
FIGURE
SHOWS
HOW
POWER
VARIES
WITH
SYSTEM
SIZE
FIGURE
SHOWS
IDLE
POWER
AT
THE
LOWEST
FREQUENCY
STATE
VERSUS
AVERAGE
POWER
DURING
SORT
AT
MHZ
FOR
THE
SAME
SYSTEM
CONFIGURATIONS
WITH
THE
SYSTEM
AT
IDLE
AND
ONLY
THE
MOTHERBOARD
DISK
INSTALLED
OUR
MEASUREMENTS
SHOW
THAT
THE
DISK
CONTROLLER
USES
AND
THE
DISK
ONE
USES
THUS
FOR
POINTS
BETWEEN
DISKS
WE
USE
ONLY
THE
DISK
CONTROLLER
BETWEEN
WE
USE
THE
ONLY
DISK
CONTROLLER
AND
FOR
OR
MORE
WE
USE
BOTH
FIGURE
SHOWS
JUMPS
AT
THESE
TRANSITIONS
THE
IDLE
LINE
INDICATES
ADDING
A
DISK
INCREASES
POWER
BY
DURING
SORTING
ADDING
A
DISK
INCREASES
TOTAL
POWER
ON
AVERAGE
AT
SIZES
FEWER
THAN
DISKS
AND
ON
AVERAGE
FOR
MORE
THAN
DISKS
THESE
INCREASES
REFLECT
END
TO
END
UTILIZATION
OF
THE
CPU
DISK
CONTROLLERS
ETC
FIGURE
SHOWS
THE
ENERGY
EFFICIENCY
WITH
INCREASING
NUM
BER
OF
DISKS
USED
IN
THE
SORT
THE
CURVE
IS
SIMILAR
TO
THE
PRICE
PERFORMANCE
CURVE
IN
FIGURE
THE
AVERAGE
INCREASE
FIGURE
SHOWS
HOW
ENERGY
EFFICIENCY
VARIES
WITH
SYSTEM
SIZE
IN
ENERGY
AT
EACH
STEP
IS
WHILE
THE
AVERAGE
INCREASE
IN
PERFORMANCE
IS
ABOUT
THE
DISK
POINT
AGAIN
IS
A
LOCAL
MINIMUM
BECAUSE
IT
INCURS
THE
POWER
OF
THE
LARGER
CONTROLLER
WITHOUT
ENOUGH
DISKS
TO
TAKE
ADVANTAGE
OF
IT
THE
SORT
IS
CPU
BOUND
IN
THE
MOST
ENERGY
EFFICIENT
CONFIGURATION
THERE
ARE
TWO
MAIN
POINTS
TO
TAKE
AWAY
FROM
THESE
EXPER
IMENTS
FIRST
THE
SIMILAR
SHAPES
OF
THESE
CURVES
REFLECT
THAT
THE
BASE
DOLLAR
AND
ENERGY
COSTS
OF
THE
SYSTEM
ARE
HIGH
COM
PARED
TO
THE
MARGINAL
DOLLAR
AND
ENERGY
COST
OF
DISKS
IF
WE
USED
SERVER
CLASS
DISKS
THAT
ARE
SIMILAR
IN
COST
BUT
CONSUME
THE
POWER
OF
MOBILE
DISKS
WE
WOULD
SEE
DIFFERENT
COST
AND
ENERGY
EFFICIENCY
CURVES
SECOND
FOR
THE
COMPONENTS
WE
CHOSE
THE
BEST
PERFORMING
MOST
COST
EFFICIENT
AND
MOST
ENERGY
EFFICIENT
CONFIGURATIONS
ARE
IDENTICAL
MODULO
THE
CPU
FREQUENCY
MOREOVER
IN
THIS
BEST
CONFIGURATION
THE
SYSTEM
IS
BALANCED
WITH
JUST
ENOUGH
I
O
BANDWITH
TO
KEEP
THE
CPU
FULLY
UTILIZED
FOR
THE
FIRST
PASS
SOFTWARE
MATTERS
NEXT
WE
VARY
THE
FILESYSTEM
AND
IN
MEMORY
SORT
ALGO
RITHM
TO
SEE
HOW
THEY
AFFECT
ENERGY
EFFICIENCY
THE
WINNING
CONFIGURATION
USES
THE
XFS
FILESYSTEM
AND
A
RADIX
SORT
FIGURE
EXAMINES
THE
EFFECT
OF
CHANGING
THE
FILESYSTEM
TO
REISERFS
AND
THE
SORT
ALGORITHM
TO
MERGE
SORT
AT
DIFFERENT
CPU
FREQUENCIES
FOR
A
DATASET
AS
EXPECTED
POWER
CONSUMPTION
STEADILY
INCREASES
WITH
FREQUENCY
IN
ALL
CASES
THE
POWER
CONSUMPTIONS
OF
XFS
WITH
RADIX
SORT
AND
MERGE
SORT
ARE
SIMILAR
AT
ALL
FREQUEN
CIES
REISERFS
HOWEVER
CONSUMES
LESS
POWER
AND
ALSO
IS
LESS
ENERGY
EFFICIENT
ALL
THREE
CONFIGURATIONS
SHOW
IMPROVED
ENERGY
EFFICIENCY
FROM
MHZ
TO
MHZ
AND
THEN
LEVEL
OFF
OR
DECREASE
THIS
RESULT
INDICATES
THAT
THE
SORTS
ARE
CPU
BOUND
AT
THE
LOWER
FREQUENCIES
REISERFS
SHOWS
A
IM
PROVEMENT
IN
PERFORMANCE
BETWEEN
THE
LOWEST
AND
HIGHEST
ON
DEMAND
CPU
FREQ
MHZ
ING
THIS
SORT
CPU
UTILIZATION
WAS
AN
AVERAGE
PER
CORE
SO
WE
ASSIGN
TO
THE
CPU
SUBSYSTEM
SIMILARLY
WE
DISCOUNT
THE
COPYING
TEST
FOR
ITS
CPU
UTILIZATION
AND
ESTIMATE
THAT
THE
I
O
SUBSYSTEM
USES
THESE
ESTIMATES
COMBINE
TO
AND
ALMOST
MATCH
ERROR
THE
MEASURED
INCREASE
DUE
TO
THE
DISK
SORT
THUS
OUR
TESTS
IMPLY
THAT
ABOUT
OF
THE
POWER
INCREASE
DURING
SORT
IS
FROM
THE
I
O
SUBSYSTEM
AND
FROM
THE
CPU
SUBSYSTEM
WE
FOUND
SIMILAR
PROPORTIONS
AT
SMALLER
SYSTEM
SIZES
VARY
DIMMS
AND
POWER
SUPPLY
SINCE
NSORT
USES
ONLY
A
FRACTION
OF
THE
AVAILABLE
MEM
ORY
FOR
THESE
EXPERIMENTS
WE
RAN
EXPERIMENTS
WITH
ONLY
DIMM
POWER
USE
AND
EXECUTION
TIME
WERE
STATISTICALLY
IN
DISTINGUISHABLE
FROM
THE
DIMM
CASE
DURING
SORT
POWER
FIGURE
SHOWS
HOW
AVERAGE
POWER
AND
ENERGY
EF
FICIENCY
VARY
WITH
CPU
FREQUENCY
FOR
A
SORT
FREQUENCIES
WHILE
XFS
RADIX
IMPROVES
ONLY
AND
XFS
MERGE
IMPROVES
ONLY
BY
REISERFS
HAS
WORSE
ENERGY
EFFICIENCY
MAINLY
BECAUSE
IT
PROVIDES
LESS
SEQUENTIAL
BANDWIDTH
AND
THUS
WORSE
PERFOR
MANCE
THAN
XFS
ALTHOUGH
WE
TUNED
EACH
CONFIGURATION
THIS
RESULT
MAY
BE
AN
ARTIFACT
OF
OUR
SETUP
AND
NOT
AN
IN
HERENT
FLAW
OF
REISERFS
SIMILARLY
THE
MERGE
SORT
ALSO
GIVES
WORSE
ENERGY
EFFICIENCY
THAN
RADIX
ENTIRELY
BECAUSE
ITS
PER
FORMANCE
IS
WORSE
THE
GRAPH
ALSO
SHOWS
THE
POWER
AND
ENERGY
EFFICIENCY
OF
THE
LINUX
ON
DEMAND
CPU
FREQUENCY
SCALING
POLICY
WHICH
IS
WITHIN
OF
THE
LOWEST
EXECUTION
TIME
AND
OF
THE
LOWEST
POWER
FOR
ALL
THREE
CONFIGURATIONS
FOR
REISERFS
THE
ON
DEMAND
POLICY
OFFERS
THE
SAME
EFFICIENCY
AS
THE
BEST
CON
FIGURATION
IN
SUMMARY
THESE
EXPERIMENTS
SHOW
THAT
THE
ALGORITHMS
AND
UNDERLYING
SOFTWARE
USED
FOR
SORT
AFFECT
EN
ERGY
EFFICIENCY
MAINLY
THROUGH
PERFORMANCE
APPROXIMATE
CPU
VS
I
O
BREAKDOWN
WE
PERFORMED
SOME
MICRO
BENCHMARKS
EXERCISING
THE
I
O
SUBSYSTEM
DISKS
PLUS
CONTROLLERS
AND
THE
CPU
SUBSYSTEM
CPU
PLUS
MEMORY
SEPARATELY
TO
DETERMINE
HOW
MUCH
EACH
CONTRIBUTES
TO
THE
INCREASE
IN
POWER
DURING
SORT
THE
SYSTEM
AT
THE
DISK
POINT
CONSUMES
MORE
DURING
SORT
THAN
WHEN
THE
SYSTEM
IS
AT
IDLE
WITH
MHZ
CPU
FREQUENCY
THIS
INCREASE
IS
NEARLY
OF
THE
TOTAL
POWER
DURING
SORT
OUR
BENCHMARKS
SUGGEST
THAT
THE
I
O
SUBSYSTEM
CONSUMES
A
MUCH
GREATER
FRACTION
OF
THIS
POWER
INCREASE
THAN
THE
CPU
SUBSYSTEM
WE
FIRST
PERFORMED
A
TEST
TO
HELP
APPROXIMATE
THE
CONTRI
BUTION
FROM
THE
I
O
SUBSYSTEM
IN
THIS
TEST
WE
COPIED
DATA
FROM
A
DISK
ARRAY
TO
ANOTHER
DISK
ARRAY
WHICH
HAD
THE
SAME
AVERAGE
DISK
BANDWIDTH
AS
THE
DISK
SORT
WE
FOUND
THAT
THE
SYSTEM
POWER
INCREASED
BY
DURING
THIS
TEST
THE
CPU
UTILIZATION
WAS
OUT
OF
MAX
FOR
CORES
NEXT
WE
PERFORMED
AN
EXPERIMENT
TO
APPROXIMATE
THE
CON
TRIBUTION
FROM
THE
CPU
SUBSYSTEM
WE
PUT
A
SMALL
INPUT
FILE
ON
A
RAM
DISK
AND
REPEATEDLY
SORTED
IT
THIS
TEST
PEGGED
THE
CPU
TO
UTILIZATION
AND
THE
POWER
INCREASE
WAS
USING
THE
ABOVE
VALUES
AND
ASSUMING
THAT
CPU
SUBSYSTEM
POWER
INCREASES
LINEARLY
WITH
CPU
UTILIZATION
WE
ESTIMATE
ITS
CONTRIBUTION
DURING
THE
DISK
SORT
AS
FOLLOWS
DUR
USE
IS
ALSO
WITHIN
MEASUREMENT
ERROR
AT
IDLE
WE
REPLACED
THE
POWER
SUPPLY
WITH
A
ONE
AND
FOUND
THAT
THE
POWER
CONSUMPTION
DURING
SORT
AND
IDLE
INCREASED
BY
THIS
SUGGESTS
THAT
AT
LOAD
OR
LESS
EFFICIENCIES
OF
THE
TWO
POWER
SUPPLIES
ARE
SIMILAR
NOTE
THE
POWER
FACTORS
FOR
THE
LAPTOPS
AND
DESKTOP
SYS
TEMS
IN
THIS
PAPER
INCLUDING
OUR
WINNER
ARE
WELL
BELOW
LOW
POWER
FACTORS
ARE
PROBLEMATIC
IN
DATA
CEN
TERS
BECAUSE
POWER
DELIVERY
MECHANISMS
NEED
TO
BE
OVER
PROVISIONED
TO
CARRY
ADDITIONAL
CURRENT
FOR
LOADS
WITH
LOW
POWER
FACTORS
UTILITIES
OFTEN
CHARGE
EXTRA
FOR
THIS
PRO
VISIONING
SIMILAR
TO
SERVER
CLASS
SYSTEMS
POWER
SUPPLIES
WILL
NEED
TO
PROVIDE
POWER
FACTOR
CORRECTION
FOR
SYSTEMS
LIKE
OUR
WINNER
TO
BECOME
A
REALITY
IN
DATA
CENTERS
SUMMARY
WE
DESCRIBE
THE
DAYTONA
JOULESORT
SYSTEM
THAT
IS
OVER
AS
ENERGY
EFFICIENT
AS
LAST
YEAR
PENNYSORT
WIN
NER
THE
GPUTERASORT
FOR
THIS
SYSTEM
WE
SHOW
THE
MOST
ENERGY
EFFICIENT
SORTING
CONFIGURATION
IS
WHEN
THE
SORT
IS
CPU
BOUND
AND
BALANCED
THIS
CONFIGURATION
IS
ALSO
THE
BEST
PERFORMING
AND
MOST
COST
EFFICIENT
IT
WILL
BE
INTERESTING
TO
SEE
HOW
LONG
THIS
RELATIONSHIP
HOLDS
WE
SEE
THAT
FILESYSTEM
AND
IN
MEMORY
SORT
CHOICE
MAINLY
AFFECT
ENERGY
EFFICIENCY
THROUGH
PERFORMANCE
RATHER
THAN
POWER
FOR
THIS
SYSTEM
IN
THIS
PAPER
WE
FOCUSED
ON
BUILDING
A
BALANCED
SYSTEM
WITH
LOW
POWER
OFF
THE
SHELF
COMPONENTS
TARGETED
FOR
THE
SCALE
UNFORTUNATELY
BECAUSE
OF
HARDWARE
LIMITA
TIONS
AND
MARKET
AVAILABILITY
WE
COULD
NOT
EASILY
SCALE
THIS
SYSTEM
TO
THE
CATEGORY
IN
THE
FUTURE
WE
EXPECT
SYS
TEMS
IN
OTHER
CLASSES
TO
WIN
THE
AND
CATEGORIES
BUT
FOR
COMPLETENESS
WE
REPORT
IN
TABLE
THE
BEST
CONFIGU
RATIONS
WE
ENCOUNTERED
FOR
THOSE
CATEGORIES
RELATED
WORK
OUR
RELATED
WORK
FALLS
INTO
THREE
CATEGORIES
WE
FIRST
DIS
CUSS
THE
HISTORY
OF
SORT
BENCHMARKS
AND
LARGE
SCALE
SORTING
TECHNIQUES
NEXT
WE
COVER
THE
PREVIOUS
WORK
ON
METRICS
FOR
EVALUATING
ENERGY
EFFICIENCY
FINALLY
WE
BRIEFLY
DISCUSS
WORK
ON
TECHNIQUES
FOR
REDUCING
ENERGY
CONSUMPTION
IN
SYSTEMS
SORT
BENCHMARKS
AND
TECHNIQUES
THE
ORIGINAL
DATAMATION
SORT
BENCHMARK
WAS
A
PURE
PER
FORMANCE
BENCHMARK
THAT
MEASURED
THE
TIME
TO
SORT
A
MIL
LION
RECORDS
IN
THE
DEVELOPERS
OF
ALPHASORT
RECOGNIZED
THAT
THE
BENCHMARK
WAS
LOSING
ITS
RELEVANCE
BE
CAUSE
STARTUP
AND
SHUTDOWN
WOULD
EVENTUALLY
DOMINATE
THE
TIME
TO
SORT
SUCH
A
SMALL
NUMBER
OF
RECORDS
THEY
THERE
FORE
PROPOSED
TWO
VARIANTS
MINUTESORT
AND
PENNYSORT
HOP
ING
THEY
WOULD
REMAIN
RELEVANT
AS
TECHNOLOGY
IMPROVED
AT
THE
PACE
OF
MOORE
LAW
RECOGNIZING
THAT
PENNYSORT
WAS
BIASED
AGAINST
LARGE
CONFIGURATIONS
BY
ALLOWING
TOO
SMALL
A
TIME
BUDGET
RESEARCHERS
THEN
PROPOSED
THE
PERFORMANCE
PRICE
SORT
WHICH
IS
TIED
TO
THE
MINUTESORT
TIME
BUDGET
THE
TIME
BUDGET
APPROACH
UNDERMINES
THE
GOALS
OF
JOULESORT
SINCE
THE
ORIGINAL
DATAMATION
SORT
BENCHMARK
THERE
HAVE
BEEN
MANY
DIFFERENT
IMPLEMENTATIONS
OF
EXTERNAL
SORT
ON
A
VARIETY
OF
PLATFORMS
FROM
DESKTOPS
TO
SUPERCOMPUTERS
THE
SORT
BENCHMARK
WEBSITE
MAINTAINED
BY
JIM
GRAY
LISTS
THE
WINNERS
AND
BRIEFLY
SURVEYS
PAST
TRENDS
ENERGY
BENCHMARKS
SEVERAL
DIFFERENT
METRICS
HAVE
BEEN
PROPOSED
FOR
EVALU
ATING
THE
ENERGY
EFFICIENCY
OF
COMPUTER
SYSTEMS
IN
GONZALEZ
ET
AL
PROPOSED
THE
ENERGY
DELAY
PRODUCT
AS
THE
METRIC
OF
ENERGY
EFFICIENT
MICROPROCESSOR
DESIGN
ALTER
NATIVELY
THE
METRIC
OF
PERFORMANCE
PER
WATT
IS
ALSO
WIDELY
USED
TO
EVALUATE
PROCESSORS
ENERGY
EFFICIENCY
THIS
MET
RIC
EMPHASIZES
PERFORMANCE
LESS
THAN
THE
ENERGY
DELAY
PROD
UCT
WHICH
IS
EQUIVALENT
TO
PERFORMANCE
SQUARED
PER
WATT
ENERGY
EFFICIENCY
METRICS
TAILORED
TO
DATA
CENTERS
HAVE
ALSO
BEEN
PROPOSED
SUN
SPACE
WATTS
AND
PERFORMANCE
METRIC
SWAP
CONSIDERS
THE
RACK
SPACE
TAKEN
UP
BY
A
HARDWARE
CONFIGURATION
ALONG
WITH
ITS
POWER
AND
PERFORMANCE
IN
AN
EFFORT
TO
PROMOTE
DATA
CENTER
COMPACTION
METRICS
BASED
ON
EXERGY
WHICH
IS
THE
ENERGY
CONVERTED
INTO
LESS
EFFICIENT
FORMS
SUCH
AS
HEAT
TAKE
INTO
ACCOUNT
EVERY
ASPECT
OF
THE
DATA
CENTER
FROM
PROCESSORS
TO
THE
COOLING
INFRASTRUCTURE
HOWEVER
THESE
METRICS
ARE
NOT
APPLICABLE
TO
THE
ENTIRE
RANGE
OF
SYSTEMS
WE
WANT
TO
EVALUATE
WITH
JOULESORT
COMPARATIVELY
LITTLE
WORK
HAS
BEEN
DONE
ON
WORKLOADS
FOR
ENERGY
EFFICIENCY
BENCHMARKS
IN
THE
EMBEDDED
DOMAIN
THE
EEMBC
ENERGYBENCH
BENCHMARKS
PROVIDE
A
PHYSICAL
IN
FRASTRUCTURE
TO
EVALUATE
A
SINGLE
PROCESSOR
ENERGY
EFFICIENCY
ON
ANY
OF
EEMBC
EXISTING
MOBILE
BENCHMARK
SUITES
IN
THE
ENTERPRISE
DOMAIN
THE
SPEC
POWER
AND
PERFORMANCE
COMMITTEE
IS
CURRENTLY
DEVELOPING
AN
ENERGY
BENCHMARK
SUITE
FOR
SERVERS
AND
THE
UNITED
STATES
ENVIRONMENTAL
PRO
TECTION
AGENCY
ENERGYSTAR
PROGRAM
IS
DEVELOPING
A
WAY
TO
RATE
THE
ENERGY
EFFICIENCY
OF
SERVERS
AND
DATA
CENTERS
ENERGY
EFFICIENCY
THERE
IS
A
LARGE
BODY
OF
PRIOR
WORK
ON
ENERGY
EFFICIENCY
FOR
EXAMPLE
AT
THE
COMPONENT
AND
SYSTEM
LEVELS
MANY
STUDIES
HAVE
BEEN
DEVOTED
TO
ALGORITHMS
FOR
DYNAMICALLY
EXPLOITING
DIFFERENT
POWER
STATES
IN
PROCESSORS
MEMORY
AND
DISKS
IN
ORDER
TO
PROMOTE
ENERGY
EF
FICIENCY
IN
CLUSTERS
AND
DATA
CENTERS
RESEARCH
HAS
FOCUSED
ON
ENERGY
EFFICIENT
WORKLOAD
DISTRIBUTION
AND
POWER
BUDGET
ING
E
G
OTHER
STUDIES
HAVE
FOCUSED
AT
THE
APPLICATION
LEVEL
INCLUDING
ENERGY
AWARE
USER
INTERFACES
AND
FIDELITY
AWARE
ENERGY
MANAGEMENT
CONCLUSIONS
IN
THIS
SECTION
WE
SUMMARIZE
THE
LIMITATIONS
OF
JOULESORT
SPECULATE
ON
FUTURE
ENERGY
EFFICIENT
SYSTEMS
AND
WRAP
UP
LIMITATIONS
JOULESORT
DOES
NOT
ADDRESS
ALL
POSSIBLE
ENERGY
RELATED
CON
CERNS
SINCE
JOULESORT
FOCUSES
ON
DATA
MANAGEMENT
TASKS
IT
MISSES
SOME
IMPORTANT
ENERGY
RELEVANT
COMPONENTS
FOR
MUL
TIMEDIA
APPLICATIONS
JOULESORT
OMITS
DISPLAYS
WHICH
ARE
AN
IMPORTANT
COMPONENT
OF
TOTAL
POWER
FOR
MOBILE
DEVICES
GPUS
ALSO
CONSUME
SIGNIFICANT
P
OWER
AND
A
RE
U
BIQUITOUS
IN
DESKTOP
SYSTEMS
ALTHOUGH
WE
CAN
USE
GPUS
TO
SORT
OUR
BENCHMARK
DOES
NOT
REQUIRE
THEIR
USE
AS
A
RESULT
IT
LOSES
RELEVANCE
FOR
APPLICATIONS
WHERE
THESE
COMPONENTS
ARE
ES
SENTIAL
THERE
ARE
OTHER
ENERGY
RELATED
CONCERNS
IN
DATA
CENTERS
BEYOND
SYSTEM
POWER
THAT
WERE
DIFFICULT
TO
INCORPORATE
AT
A
HIGH
LEVEL
COOLING
REQUIRES
LOWERING
AMBIENT
TEMPER
ATURE
AND
EXTRACTING
HEAT
AWAY
FROM
SYSTEMS
JOULE
SORT
ACCOUNTS
ONLY
FOR
PART
OF
THE
SECOND
DELIVERING
POWER
TO
SYSTEMS
INCURS
LOSSES
AT
THE
RACK
AND
DATA
CENTER
LEVEL
WHICH
ARE
IGNORED
IN
JOULESORT
MOREOVER
MANY
SYSTEMS
ARE
USED
AS
AN
ENSEMBLE
IN
DATA
CENTERS
WITH
SOPHISTI
CATED
SCHEDULING
TECHNIQUES
TO
TRADE
PERFORMANCE
FOR
LOWER
ENERGY
AMONG
SYSTEMS
RATHER
THAN
AT
THE
COMPONENT
LEVEL
AS
A
SYSTEM
LEVEL
BENCHMARK
JOULESORT
MAY
NOT
IDENTIFY
THE
BENEFITS
O
F
UCH
METHODS
GREENER
SYSTEMS
WE
SPECULATE
ON
TWO
EMERGING
TECHNOLOGIES
THAT
MAY
IM
PROVE
THE
ENERGY
EFFICIENCY
OF
SYSTEMS
FOR
THE
SCALE
FLASH
M
EMORY
A
PPEARS
TO
B
E
A
P
ROMISING
TORAGE
TECHNOLOGY
DRIVEN
BY
THE
MOBILE
DEVICE
MARKET
PER
BYTE
IT
IS
ABOUT
CHEAPER
THAN
DRAM
AND
PROVIDES
SEQUENTIAL
READ
AND
WRITE
BANDWIDTH
CLOSE
TO
THAT
OF
DISKS
MORE
IMPORTANTLY
RANDOM
READ
I
OS
WITH
FLASH
A
RE
F
ASTER
T
HAN
D
ISK
AND
FLASH
CONSUMES
LESS
POWER
THAN
D
ISKS
THE
RANDOM
READS
ALLOW
INTERESTING
MODIFICATIONS
T
O
T
RADITIONAL
PASS
SORTING
ALGORITHMS
TO
DATE
THE
LARGEST
CARDS
AT
REASONABLE
COST
ARE
WE
ANTICIPATE
A
SYSTEM
SUCH
AS
A
LAPTOP
OR
LOW
POWER
EMBEDDED
DEVICE
THAT
CAN
LEVERAGE
MULTIPLE
FLASH
DEVICES
AS
THE
NEXT
WINNER
FOR
THE
LARGER
SCALES
AN
INTRIGUING
OPTION
IS
A
HYBRID
SYS
TEM
USING
A
LOW
POWER
CPU
LAPTOP
DISKS
AND
A
GPU
THE
GPUTERASORT
HAS
SHOWN
THAT
GPUS
CAN
PROVIDE
MUCH
BET
TER
IN
MEMORY
SORTING
BANDWIDTH
THAN
CPUS
USING
A
MOTHERBOARD
THAT
SUPPORTS
MORE
I
O
CONTROLLERS
AND
A
GPU
WE
COULD
SCALE
OUR
SYSTEM
TO
USE
MORE
DISKS
AN
INTEREST
ING
QUESTION
IS
WHETHER
THESE
PERFORMANCE
BENEFITS
MIGHT
BE
OFFSET
BY
THE
RECENT
TREND
IN
GPUS
TO
CONSUME
MORE
POWER
CLOSING
THIS
PAPER
PROPOSES
JOULESORT
A
SIMPLE
BALANCED
ENERGY
EFFICIENCY
BENCHMARK
WE
PRESENT
A
COMPLETE
BENCHMARK
A
WORKLOAD
METRIC
AND
GUIDELINES
AND
JUSTIFY
OUR
CHOICES
WE
ALSO
PRESENT
A
WINNER
THAT
IS
OVER
AS
EFFICIENT
AS
LAST
YEAR
ESTIMATED
WINNER
TODAY
THIS
SYSTEM
IS
HARD
TO
FIND
P
RE
ASSEMBLED
I
T
C
ONSISTS
O
F
A
C
OMMODITY
MOBILE
CLASS
CPU
AND
LAPTOP
DISKS
CONNECTED
THROUGH
SERVER
CLASS
PCI
E
I
O
CARDS
THE
DETAILS
OF
JOULESORT
ALREADY
HAVE
UNDERGONE
SIGNIFI
CANT
CHANGES
SINCE
ITS
INCEPTION
SINCE
JOULESORT
HAS
NOT
YET
BEEN
TRIED
IN
THE
WILD
WE
FULLY
EXPECT
FURTHER
REVISIONS
AND
FINE
TUNING
T
O
K
EEP
I
T
F
AIR
A
ND
R
ELEVANT
NEVERTHELESS
WE
LOOK
FORWARD
TO
ITS
USE
IN
GUIDING
ENERGY
EFFICIENCY
OPTI
MIZATIONS
IN
FUTURE
SYSTEMS
WE
CONSIDER
THE
SETTING
OF
A
MULTIPROCESSOR
WHERE
THE
SPEEDS
OF
THE
M
PROCES
SORS
CAN
BE
INDIVIDUALLY
SCALED
JOBS
ARRIVE
OVER
TIME
AND
HAVE
VARYING
DEGREES
OF
PARALLELIZABILITY
A
NONCLAIRVOYANT
SCHEDULER
MUST
ASSIGN
THE
PROCESSES
TO
PROCESSORS
AND
SCALE
THE
SPEEDS
OF
THE
PROCESSORS
WE
CONSIDER
THE
OBJECTIVE
OF
ENERGY
PLUS
FLOW
TIME
WE
ASSUME
THAT
A
PROCESSOR
RUNNING
AT
SPEED
USES
POWER
SΑ
FOR
SOME
CONSTANT
Α
FOR
PROCESSES
THAT
MAY
HAVE
SIDE
EFFECTS
OR
THAT
ARE
NOT
CHECKPOINTABLE
WE
SHOW
AN
Ω
M
Α
Α
BOUND
ON
THE
COMPETITIVE
RATIO
OF
ANY
RANDOMIZED
ALGORITHM
FOR
CHECKPOINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
WE
GIVE
AN
O
LOG
M
COMPETITIVE
AL
GORITHM
THUS
FOR
PROCESSES
THAT
MAY
HAVE
SIDE
EFFECTS
OR
THAT
ARE
NOT
CHECKPOINTABLE
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
QUICKLY
WITH
THE
NUMBER
OF
PROCESSORS
BUT
FOR
CHECKPOINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
SLOWLY
WITH
THE
NUMBER
OF
PROCESSORS
WE
THEN
SHOW
A
LOWER
BOUND
OF
Ω
Α
M
ON
THE
COMPETITIVE
RATIO
OF
ANY
RANDOMIZED
ALGORITHM
FOR
CHECKPOINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
INTRODUCTION
DUE
TO
THE
POWER
RELATED
ISSUES
OF
ENERGY
AND
TEMPERATURE
MAJOR
CHIP
MANUFACTURERS
SUCH
AS
INTEL
AMD
AND
IBM
NOW
PRODUCE
CHIPS
WITH
MULTIPLE
CORES
PROCESSORS
AND
WITH
DY
NAMICALLY
SCALABLE
SPEEDS
AND
PRODUCE
ASSOCIATED
SOFTWARE
SUCH
AS
INTEL
SPEEDSTEP
AND
AMD
POWERNOW
THAT
ENABLES
AN
OPERATING
SYSTEM
TO
MANAGE
POWER
BY
SCALING
PROCESSOR
SPEED
CURRENTLY
MOST
MULTIPROCESSOR
CHIPS
HAVE
ONLY
A
HANDFUL
OF
PROCESSORS
BUT
CHIP
DE
SIGNERS
ARE
AGREED
UPON
THE
FACT
THAT
CHIPS
WITH
HUNDREDS
TO
THOUSANDS
OF
PROCESSORS
WILL
DOMINATE
THE
MARKET
IN
THE
NEXT
DECADE
THE
FOUNDER
OF
CHIP
MAKER
TILERA
ASSERTED
THAT
A
COROLLARY
TO
MOORE
LAW
WILL
BE
THAT
THE
NUMBER
OF
CORES
PROCESSORS
WILL
DOUBLE
EVERY
MONTHS
ACCORDING
TO
THE
WELL
KNOWN
CUBE
ROOT
RULE
A
CMOS
BASED
PROCESSOR
RUNNING
AT
SPEED
WILL
HAVE
A
DYNAMIC
POWER
P
OF
APPROXIMATELY
IN
THE
ALGORITHMIC
LITERATURE
THIS
IS
THE
UNIVERSITY
OF
HONG
KONG
HLCHAN
CS
HKU
HK
YORK
UNIVERSITY
JEFF
CS
YORKU
CA
SUPPORTED
IN
PART
BY
NSERC
CANADA
UNIVERSITY
OF
PITTSBURGH
KIRK
CS
PITT
EDU
SUPPORTED
IN
PART
BY
AN
IBM
FACULTY
AWARD
AND
BY
NSF
GRANTS
CNS
CCF
IIS
AND
CCF
USUALLY
GENERALIZED
TO
P
SΑ
THUS
IN
PRINCIPLE
P
PROCESSORS
RUNNING
AT
SPEED
P
COULD
DO
THE
WORK
OF
ONE
PROCESSOR
RUNNING
AT
SPEED
BUT
AT
PΑ
OF
THE
POWER
BUT
IN
SPITE
OF
THIS
CHIP
MAKERS
WAITED
UNTIL
THE
POWER
COSTS
BECAME
PROHIBITIVE
BEFORE
SWITCHING
TO
MULTIPROCESSOR
CHIPS
BECAUSE
OF
THE
TECHNICAL
DIFFICULTIES
IN
GETTING
P
SPEED
P
PROCESSORS
TO
COME
CLOSE
TO
DOING
THE
WORK
OF
ONE
SPEED
PROCESSOR
THIS
IS
PARTICULARLY
TRUE
WHEN
ONE
HAS
MANY
PROCESSORS
AND
FEW
PROCESSES
WHERE
THESE
PROCESSES
HAVE
WIDELY
VARYING
DEGREES
OF
PARALLELIZABILITY
THAT
IS
SOME
PROCESSES
MAY
BE
CONSIDERABLY
SPED
UP
WHEN
SIMULTANEOUSLY
RUN
ON
MULTIPLE
PROCESSORS
WHILE
SOME
PROCESSES
MAY
NOT
BE
SPED
UP
AT
ALL
THIS
COULD
BE
BECAUSE
THE
UNDERLYING
ALGORITHM
IS
INHERENTLY
SEQUENTIAL
IN
NATURE
OR
BECAUSE
THE
PROCESS
WAS
NOT
CODED
IN
A
WAY
TO
MAKE
IT
EASILY
PARALLELIZABLE
TO
INVESTIGATE
THIS
ISSUE
WE
ADOPT
THE
FOLLOWING
GENERAL
MODEL
OF
PARALLELIZABILITY
USED
IN
EACH
PROCESS
CONSISTS
OF
A
SEQUENCE
OF
PHASES
EACH
PHASE
CONSISTS
OF
A
POSITIVE
REAL
NUMBER
THAT
DENOTES
THE
AMOUNT
OF
WORK
IN
THAT
PHASE
AND
A
SPEEDUP
FUNCTION
THAT
SPECIFIES
THE
RATE
AT
WHICH
WORK
IS
PROCESSED
IN
THIS
PHASE
AS
A
FUNCTION
OF
THE
NUMBER
OF
PROCESSORS
EXECUTING
THE
PROCESS
THE
SPEEDUP
FUNCTIONS
MAY
BE
ARBITRARY
OTHER
THAN
WE
ASSUME
THAT
THEY
ARE
NONDECREASING
A
PROCESS
DOESN
T
RUN
SLOWER
IF
IT
IS
GIVEN
MORE
PROCESSORS
AND
SUBLINEAR
A
PROCESS
SATISFIES
BRENT
THEOREM
THAT
IS
INCREASING
THE
NUMBER
OF
PROCESSORS
DOESN
T
INCREASE
THE
EFFICIENCY
OF
COMPUTATION
THE
OPERATING
SYSTEM
NEEDS
A
PROCESS
ASSIGNMENT
POLICY
FOR
DETERMINING
AT
EACH
TIME
WHICH
PROCESSORS
IF
ANY
A
PARTICULAR
PROCESS
IS
ASSIGNED
TO
WE
ASSUME
THAT
A
PROCESS
MAY
BE
ASSIGNED
TO
MULTIPLE
PROCESSORS
IN
TANDEM
WITH
THIS
THE
OPERATING
SYSTEM
WILL
ALSO
NEED
A
SPEED
SCALING
POLICY
FOR
SETTING
THE
SPEED
OF
EACH
PROCESSOR
IN
ORDER
TO
BE
IMPLEMENTABLE
IN
A
REAL
SYSTEM
THE
SPEED
SCALING
AND
PROCESS
ASSIGNMENT
POLICIES
MUST
BE
ONLINE
SINCE
THE
SYSTEM
WILL
NOT
IN
GENERAL
KNOW
ABOUT
PROCESSES
ARRIVING
IN
THE
FUTURE
FURTHER
TO
BE
IMPLEMENTABLE
IN
A
GENERIC
OPERATING
SYSTEM
THESE
POLICIES
MUST
BE
NONCLAIRVOYANT
SINCE
IN
GENERAL
THE
OPERATING
SYSTEM
DOES
NOT
KNOW
THE
SIZE
WORK
OF
EACH
PROCESS
WHEN
THE
PROCESS
IS
RELEASED
TO
THE
OPERATING
SYSTEM
NOR
THE
DEGREE
TO
WHICH
THAT
PROCESS
IS
PARALLELIZABLE
SO
A
NONCLAIRVOYANT
ALGORITHM
ONLY
KNOWS
WHEN
PROCESSES
HAVE
BEEN
RELEASED
AND
FINISHED
IN
THE
PAST
AND
WHICH
PROCESSES
HAVE
BEEN
RUN
ON
EACH
PROCESSOR
AT
EACH
TIME
IN
THE
PAST
THE
OPERATING
SYSTEM
HAS
COMPETING
DUAL
OBJECTIVES
AS
IT
BOTH
WANTS
TO
OPTIMIZE
SOME
SCHEDULE
QUALITY
OF
SERVICE
OBJECTIVE
AS
WELL
AS
SOME
POWER
RELATED
OBJECTIVE
IN
THIS
PA
PER
WE
WILL
CONSIDER
THE
FORMAL
OBJECTIVE
OF
MINIMIZING
A
LINEAR
COMBINATION
OF
TOTAL
RE
SPONSE
FLOW
TIME
THE
SCHEDULE
OBJECTIVE
AND
TOTAL
ENERGY
USED
THE
POWER
OBJECTIVE
IN
THE
CONCLUSION
WE
WILL
DISCUSS
THE
RELATIONSHIP
BETWEEN
THIS
ENERGY
OBJECTIVE
AND
A
TEMPER
ATURE
OBJECTIVE
THIS
OBJECTIVE
OF
FLOW
PLUS
ENERGY
HAS
A
NATURAL
INTERPRETATION
SUPPOSE
THAT
THE
USER
SPECIFIES
HOW
MUCH
IMPROVEMENT
IN
FLOW
CALL
THIS
AMOUNT
Ρ
IS
NECESSARY
TO
JUSTIFY
SPENDING
ONE
UNIT
OF
ENERGY
FOR
EXAMPLE
THE
USER
MIGHT
SPECIFY
THAT
HE
IS
WILLING
TO
SPEND
ERG
OF
ENERGY
FROM
THE
BATTERY
FOR
A
DECREASE
OF
MICRO
SECONDS
IN
FLOW
THEN
THE
OPTIMAL
SCHEDULE
FROM
THIS
USER
PERSPECTIVE
IS
THE
SCHEDULE
THAT
OPTIMIZES
Ρ
TIMES
THE
ENERGY
USED
PLUS
THE
TOTAL
FLOW
BY
CHANGING
THE
UNITS
OF
EITHER
ENERGY
OR
TIME
ONE
MAY
ASSUME
WITHOUT
LOSS
OF
GENERALITY
THAT
Ρ
SO
THE
PROBLEM
WE
WANT
TO
ADDRESS
HERE
IS
HOW
TO
DESIGN
A
NONCLAIRVOYANT
PROCESS
AS
SIGNMENT
POLICY
AND
A
SPEED
SCALING
POLICY
THAT
WILL
BE
COMPETITIVE
FOR
THE
OBJECTIVE
OF
FLOW
PLUS
ENERGY
THE
CASE
OF
A
SINGLE
PROCESSOR
WAS
CONSIDERED
IN
IN
THE
SINGLE
PROCESSOR
CASE
THE
PARALLELIZABILITY
OF
THE
PROCESSES
IS
NOT
AN
ISSUE
IF
ALL
THE
PROCESSES
ARRIVE
AT
TIME
THEN
IN
THE
OPTIMAL
SCHEDULE
THE
POWER
AT
TIME
T
IS
Θ
NT
WHERE
NT
IS
THE
NUMBER
OF
ACTIVE
PROCESSES
AT
TIME
T
THE
ALGORITHM
CONSIDERED
IN
RUNS
AT
A
SPEED
OF
Δ
Α
FOR
SOME
CONSTANT
Δ
THE
PROCESS
ASSIGNMENT
ALGORITHM
CONSIDERED
IN
IS
LATEST
ARRIVAL
PROCESSOR
SHARING
LAPS
LAPS
WAS
PROPOSED
IN
IN
THE
CONTEXT
OF
RUNNING
PROCESSES
WITH
ARBITRARY
SPEEDUP
FUNCTIONS
ON
FIXED
SPEED
PROCESSORS
AND
IT
WAS
SHOWN
TO
BE
SCALABLE
I
E
Ǫ
SPEED
O
COMPETITIVE
FOR
THE
OBJECTIVE
OF
TOTAL
FLOW
TIME
IN
THIS
SETTING
LAPS
IS
PARAMETERIZED
BY
A
CONSTANT
Β
AND
SHARES
THE
PROCESSING
POWER
EVENLY
AMONG
THE
ΒNT
MOST
RECENTLY
ARRIVING
PROCESSES
NOTE
THAT
THE
SPEED
SCALING
POLICY
AND
LAPS
ARE
BOTH
NONCLAIRVOYANT
SHOWED
THAT
BY
PICKING
Δ
AND
Β
APPROPRIATELY
THE
RESULTING
ALGORITHM
IS
Α
COMPETITIVE
FOR
THE
OBJECTIVE
OF
FLOW
PLUS
ENERGY
ON
A
SINGLE
SPEED
SCALABLE
PROCESSOR
OUR
RESULTS
HERE
WE
CONSIDER
EXTENDING
THE
RESULTS
IN
TO
THE
SETTING
OF
A
MULTIPROCESSOR
WITH
M
PROCESSORS
IT
IS
STRAIGHT
FORWARD
TO
NOTE
THAT
IF
ALL
OF
THE
WORK
IS
PARALLELIZABLE
THEN
THE
MULTIPROCESSOR
SETTING
IS
ESSENTIALLY
EQUIVALENT
TO
THE
UNIPROCESSOR
SETTING
TO
GAIN
SOME
INTUITION
OF
THE
DIFFICULTY
THAT
VARYING
SPEEDUP
FUNCTIONS
POSE
LET
US
FIRST
CONSIDER
AN
INSTANCE
OF
ONE
PROCESS
THAT
MAY
EITHER
BE
SEQUENTIAL
OR
PARALLELIZABLE
IF
AN
ALGORITHM
RUNS
THIS
PROCESS
ON
FEW
OF
THE
PROCESSORS
THEN
THE
ALGORITHM
COMPETITIVE
RATIO
WILL
BE
BAD
IF
THE
PROCESS
IS
PARALLELIZABLE
AND
THE
OPTIMAL
SCHEDULE
RUNS
THE
PROCESS
ON
ALL
OF
THE
PROCESSORS
NOTE
THAT
IF
THE
ALGORITHM
WANTED
TO
BE
COMPETITIVE
ON
FLOW
TIME
IT
WOULD
HAVE
TO
RUN
TOO
FAST
TO
BE
COMPETITIVE
ON
ENERGY
IF
AN
ALGORITHM
RUNS
THIS
PROCESS
ON
MANY
OF
THE
PROCESSORS
THEN
THE
ALGORITHM
COMPETITIVE
RATIO
WILL
BE
BAD
IF
THE
PROCESS
IS
SEQUENTIAL
AND
THE
OPTIMAL
SCHEDULE
RUNS
THE
PROCESS
ON
FEW
PROCESSORS
IF
THE
ALGORITHM
WANTED
TO
BE
COMPETITIVE
ON
ENERGY
IT
WOULD
HAVE
TO
RUN
TOO
SLOW
TO
BE
COMPETITIVE
ON
FLOW
TIME
FORMALIZING
THIS
ARGUMENT
WE
SHOW
IN
SECTION
A
LOWER
BOUND
OF
Ω
M
Α
Α
ON
THE
COMPETITIVE
RATIO
OF
ANY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
AGAINST
AN
OBLIVIOUS
ADVERSARY
WITH
AN
ADDITIONAL
ASSUMPTION
THAT
WE
WILL
NOW
DISCUSS
AT
FIRST
GLANCE
SUCH
A
STRONG
LOWER
BOUND
FOR
SUCH
AN
EASY
INSTANCE
MIGHT
LEAD
ONE
TO
CONCLUDE
THAT
THERE
IS
NO
WAY
THAT
THE
SCHEDULER
CAN
BE
EXPECTED
TO
GUARANTEE
REASONABLY
COMPETITIVE
SCHEDULES
BUT
ON
FURTHER
REFLECTION
ONE
REALIZES
THAT
AN
UNDERLYING
ASSUMPTION
IN
THIS
LOWER
BOUND
IS
THAT
ONLY
ONE
COPY
OF
A
PROCESS
CAN
BE
RUN
IF
A
PROCESS
DOES
NOT
HAVE
SIDE
EFFECTS
THAT
IS
IF
THE
PROCESS
DOESN
T
CHANGE
EFFECT
ANYTHING
EXTERNAL
TO
ITSELF
THEN
THIS
ASSUMPTION
IS
NOT
GENERALLY
VALID
ONE
COULD
RUN
MULTIPLE
COPIES
OF
A
PROCESS
SIMULTANEOUSLY
WITH
EACH
COPY
BEING
RUN
ON
A
DIFFERENT
NUMBER
OF
PROCESSORS
AND
HALT
COMPUTATION
WHEN
THE
FIRST
COPY
FINISHES
FOR
EXAMPLE
IN
THE
INSTANCE
IN
THE
PREVIOUS
PARAGRAPH
ONE
COULD
BE
O
COMPETITIVE
IF
THE
PROCESS
DIDN
T
HAVE
SIDE
EFFECTS
BY
RUNNING
ONE
COPY
ON
A
SINGLE
PROCESSOR
AND
RUNNING
ONE
COPY
ON
THE
REST
OF
THE
PROCESSORS
GENERALIZING
THIS
APPROACH
ONE
CAN
OBTAIN
A
O
LOG
M
COMPETITIVE
ALGORITHM
FOR
INSTANCES
CONSISTING
OF
PROCESSES
THAT
HAVE
NO
SIDE
EFFECTS
AND
WHERE
THE
SPEED
UP
FUNCTION
DOESN
T
CHANGE
UNFORTUNATELY
WE
SHOW
IN
SECTION
THAT
SUCH
A
RESULT
CAN
NOT
BE
OBTAINED
IF
PROCESSES
CAN
HAVE
MULTIPLE
PHASES
WITH
DIFFERENT
SPEED
UP
FUNCTIONS
WE
ACCOMPLISH
THIS
BY
SHOWING
THAT
THE
COMPETITIVE
RATIO
OF
ANY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
THAT
RUNS
MULTIPLE
INDEPENDENT
COPIES
OF
A
PROCESS
AGAINST
AN
OBLIVIOUS
ADVERSARY
IS
Ω
MΩ
Α
CONTEMPLATING
THIS
SECOND
LOWER
BOUND
IT
SUGGESTS
THAT
TO
BE
REASONABLY
COMPETITIVE
THE
ALGORITHM
MUST
BE
ABLE
TO
PROCESS
WORK
ON
ALL
COPIES
OF
A
JOB
AT
THE
MAXIMUM
RATE
OF
WORK
PROCESSING
ON
ANY
COPY
IF
A
PROCESSES
HAD
SMALL
STATE
SO
THAT
THE
OVERHEAD
OF
CHECKPOINTING
ISN
T
PROHIBITIVE
ONE
MIGHT
REASONABLY
APPROXIMATE
THIS
BY
CHECKPOINTING
SAVING
THE
STATE
OF
EACH
COPY
PERIODICALLY
AND
THEN
RESTARTING
EACH
COPY
FROM
THE
POINT
OF
EXECUTION
OF
THE
COPY
THAT
MADE
THE
MOST
PROGRESS
IN
SECTION
WE
FORMALIZE
THIS
INTUITION
WE
GIVE
A
PROCESS
ASSIGNMENT
ALGORITHM
MULTILAPS
WHICH
IS
A
MODIFICATION
OF
LAPS
WE
SHOW
THAT
BY
COMBINING
MULTILAPS
WITH
THE
NATURAL
SPEED
SCALING
ALGORITHM
ONE
OBTAINS
AN
O
LOG
M
COMPETITIVE
ALGORITHM
IF
ALL
COPIES
PROCESS
WORK
AT
THE
RATE
OF
THE
FASTEST
COPY
THERE
ARE
TWO
STEPS
IN
THE
ANALYSIS
OF
MULTILAPS
THE
FIRST
STEP
IS
TO
SHOW
THAT
THERE
IS
A
WORST
CASE
INSTANCE
WHERE
EVERY
SPEEDUP
FUNCTION
IS
PARALLEL
UP
TO
SOME
NUMBER
OF
PROCESSORS
AND
THEN
IS
CONSTANT
THIS
SHOWS
THAT
THE
WORST
CASE
SPEEDUP
FUNCTIONS
FOR
SPEED
SCALABLE
PROCESSORS
ARE
MORE
VARIED
THAN
FOR
FIXED
SPEED
PROCESSORS
WHERE
IT
IS
SUFFICIENT
TO
RESTRICT
ATTENTION
TO
ONLY
PARALLELIZABLE
AND
SEQUENTIAL
SPEEDUP
FUNCTIONS
THE
SECOND
STEP
IN
THE
ANALYSIS
OF
MULTILAPS
IS
A
REDUCTION
TO
ESSENTIALLY
THE
ANALYSIS
OF
LAPS
IN
A
UNIPROCESSOR
SETTING
TECHNICALLY
WE
NEED
TO
ANALYZE
LAPS
WHEN
SOME
WORK
IS
SEQUENTIAL
THAT
IS
IT
HAS
THE
SPECIAL
PROPERTY
THAT
IT
IS
PROCESSED
AT
UNIT
RATE
INDEPENDENT
OF
THE
SPEED
OF
THE
PROCESSOR
WE
THEN
DISCUSS
HOW
TO
GENERALIZE
THE
ANALYSIS
OF
LAPS
IN
TO
ALLOW
SEQUENTIAL
WORK
USING
TECHNIQUES
FROM
IN
SECTION
WE
THEN
SHOW
A
LOWER
BOUND
OF
Ω
Α
M
ON
THE
COMPETITIVE
RATIO
OF
ANY
NONCLAIRVOYANT
RANDOMIZED
ALGORITHM
AGAINST
AN
OBLIVIOUS
ADVERSARY
FOR
CHECKPOINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
IN
FACT
THIS
LOWER
HOLDS
EVEN
IF
THE
RATE
THAT
A
PROCESS
IS
PROCESSED
IS
THE
SUM
NOT
THE
MAXIMUM
OF
RATE
OF
THE
VARIOUS
COPIES
THUS
IN
SUMMARY
FOR
PROCESSES
THAT
MAY
HAVE
SIDE
EFFECTS
OR
THAT
ARE
NOT
CHECKPOINTABLE
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
QUICKLY
WITH
THE
NUMBER
OF
PROCESSORS
BUT
FOR
CHECK
POINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
SLOWLY
WITH
THE
NUMBER
OF
PROCESSORS
THIS
SHOWS
THE
IMPORTANCE
OF
BEING
ABLE
TO
EFFICIENTLY
CHECKPOINT
MULTIPLE
COPIES
OF
A
PROCESS
IN
A
SETTING
OF
PROCESSES
WITH
VARYING
DEGREES
OF
PARALLELIZABILITY
AND
INDIVIDUALLY
SPEED
SCALABLE
MULTIPROCESSORS
RELATED
RESULTS
WE
START
WITH
SOME
RESULTS
IN
THE
LITERATURE
ABOUT
SCHEDULING
WITH
THE
OBJECTIVE
OF
TOTAL
FLOW
TIME
ON
A
SINGLE
FIXED
SPEED
PROCESSOR
IT
IS
WELL
KNOWN
THAT
THE
ONLINE
CLAIRVOYANT
ALGORITHM
SHORTEST
REMAINING
PROCESSING
TIME
SRPT
IS
OPTIMAL
THE
COMPETITIVE
RATIO
OF
ANY
DETERMINISTIC
NONCLAIRVOYANT
ALGORITHM
IS
Ω
AND
THE
COMPETITIVE
RATIO
OF
EVERY
RANDOMIZED
ALGORITHM
AGAINST
AN
OBLIVIOUS
ADVERSARY
IS
Ω
LOG
N
A
RANDOMIZED
VERSION
OF
THE
MULTI
LEVEL
FEEDBACK
QUEUE
ALGORITHM
IS
O
LOG
N
COMPETITIVE
THE
NONCLAIR
VOYANT
ALGORITHM
SHORTEST
ELAPSED
TIME
FIRST
SETF
IS
SCALABLE
THAT
IS
IT
IS
Ǫ
SPEED
O
COMPETITIVE
FOR
ANY
ARBITRARILY
SMALL
BUT
FIXED
Ǫ
SETF
SHARES
THE
PROCESSOR
EQUALLY
AMONG
ALL
PROCESSES
THAT
HAVE
BEEN
RUN
THE
LEAST
WE
NOW
CONSIDER
SCHEDULING
PROCESSES
WITH
ARBITRARY
SPEEDUP
FUNCTIONS
ON
FIXED
SPEED
PROCESSORS
FOR
THE
OBJECTIVE
OF
TOTAL
FLOW
TIME
THE
ALGORITHM
ROUND
ROBIN
RR
ALSO
CALLED
EQUIPARTITION
AND
PROCESSOR
SHARING
THAT
SHARES
THE
PROCESSORS
EQUALLY
AMONG
ALL
PROCESSES
IS
Ǫ
SPEED
O
COMPETITIVE
AS
MENTIONED
BEFORE
LAPS
IS
SCALABLE
WE
NOW
CONSIDER
SPEED
SCALING
ALGORITHMS
ON
A
SINGLE
PROCESSOR
FOR
THE
OBJECTIVE
OF
FLOW
PLUS
ENERGY
GIVE
EFFICIENT
OFFLINE
ALGORITHMS
WE
NOW
DESCRIBE
THE
RESULTS
FOR
ONLINE
CLAIRVOYANT
ALGORITHMS
THIS
SETTING
WAS
STUDIED
IN
A
SEQUENCE
OF
PAPERS
WHICH
CULMINATED
IN
THE
FOLLOWING
RESULT
THE
SCHEDULING
ALGORITHM
THAT
USES
SHORTEST
REMAINING
PROCESSING
TIME
SRPT
FOR
PROCESS
ASSIGNMENT
AND
POWER
EQUAL
TO
ONE
MORE
THAN
THE
NUMBER
OF
ACTIVE
PROCESSES
FOR
SPEED
SCALING
IS
Ǫ
COMPETITIVE
FOR
THE
OBJECTIVE
OF
TOTAL
FLOW
PLUS
ENERGY
ON
ARBITRARY
WORK
UNIT
WEIGHT
PROCESSES
EVEN
IF
THE
POWER
FUNCTION
IS
ARBITRARY
SO
CLAIRVOYANT
ALGORITHMS
CAN
BE
O
COMPETITIVE
INDEPENDENT
OF
THE
POWER
FUNCTION
SHOWED
THAT
NONCLAIRVOYANT
ALGORITHMS
CAN
NOT
BE
O
COMPETITIVE
IF
THE
POWER
FUNCTION
IS
GROWING
TOO
QUICKLY
THE
CASE
OF
WEIGHTED
FLOW
TIME
HAS
ALSO
BEEN
STUDIED
THE
SCHEDULING
ALGORITHM
THAT
USES
HIGHEST
DENSITY
FIRST
HDF
FOR
PROCESS
ASSIGNMENT
AND
POWER
EQUAL
TO
THE
FRACTIONAL
WEIGHT
OF
THE
ACTIVE
PROCESSES
FOR
SPEED
SCALING
IS
Ǫ
COMPETITIVE
FOR
THE
OBJECTIVE
OF
FRACTIONAL
WEIGHTED
FLOW
PLUS
ENERGY
ON
ARBITRARY
WORK
ARBITRARY
WEIGHT
PROCESSES
AN
O
COMPETITIVE
ALGORITHM
FOR
WEIGHTED
FLOW
PLUS
ENERGY
CAN
THEN
BE
OBTAINED
USING
THE
KNOWN
RESOURCE
AUGMENTATION
ANALYSIS
OF
HDF
EXTEND
SOME
OF
THE
RESULTS
FOR
THE
UNBOUNDED
SPEED
MODEL
TO
A
MODEL
WHERE
THERE
IS
AN
UPPER
BOUND
ON
THE
SPEED
OF
A
PROCESSOR
THERE
ARE
MANY
RELATED
SCHEDULING
PROBLEMS
WITH
OTHER
OBJECTIVES
AND
OR
OTHER
ASSUMP
TIONS
ABOUT
THE
PROCESSORS
AND
INSTANCE
SURVEYS
CAN
BE
FOUND
IN
FORMAL
PROBLEM
DEFINITION
AND
NOTATIONS
AN
INSTANCE
CONSISTS
OF
A
COLLECTION
J
JN
WHERE
JOB
JI
HAS
A
RELEASE
ARRIVAL
TIME
RI
AND
A
SEQUENCE
OF
PHASES
JQI
EACH
PHASE
IS
AN
ORDERED
PAIR
WQ
ΓQ
WHERE
I
I
I
I
I
Q
IS
A
POSITIVE
REAL
NUMBER
THAT
DENOTES
THE
AMOUNT
OF
WORK
IN
THE
PHASE
AND
ΓQ
IS
A
FUNCTION
CALLED
THE
SPEEDUP
FUNCTION
THAT
MAPS
A
NONNEGATIVE
REAL
NUMBER
TO
A
NONNEGATIVE
REAL
NUMBER
ΓQ
P
REPRESENTS
THE
RATE
AT
WHICH
WORK
IS
PROCESSED
FOR
PHASE
Q
OF
JOB
I
WHEN
ONE
COPY
OF
THE
JOB
IS
RUN
ON
P
PROCESSORS
RUNNING
AT
SPEED
IF
THESE
PROCESSORS
ARE
RUNNING
AT
SPEED
THEN
WORK
IS
PROCESSED
AT
A
RATE
OF
SΓQ
P
A
SCHEDULE
SPECIFIES
FOR
EACH
TIME
AND
FOR
EACH
COPY
OF
A
JOB
A
NONNEGATIVE
REAL
NUMBER
SPECIFYING
THE
NUMBER
OF
PROCESSORS
ASSIGNED
TO
THE
COPY
OF
THE
JOB
AND
A
NONNEGATIVE
REAL
SPEED
WE
THUS
ASSUME
THAT
IF
SEVERAL
PROCESSORS
ARE
WORKING
ON
THE
SAME
INSTANCE
COPY
OF
A
JOB
THEN
THEY
MUST
ALL
RUN
AT
THE
SAME
SPEED
BUT
DIFFERENT
COPIES
CAN
RUN
AT
DIFFERENT
SPEEDS
THE
NUMBER
OF
PROCESSORS
ASSIGNED
AT
ANY
TIME
CAN
BE
AT
MOST
M
THE
NUMBER
OF
PROCESSORS
NOTE
THAT
FORMALLY
A
SCHEDULE
DOES
NOT
SPECIFY
AN
ASSIGNMENT
OF
COPIES
OF
JOBS
TO
PROCESSORS
A
NONCLAIRVOYANT
ALGORITHM
ONLY
KNOWS
WHEN
PROCESSES
HAVE
BEEN
RELEASED
AND
FINISHED
IN
THE
PAST
AND
WHICH
PROCESSES
HAVE
BEEN
RUN
ON
EACH
PROCESSOR
EACH
TIME
IN
THE
PAST
IN
PARTICULAR
A
NONCLAIRVOYANT
ALGORITHM
DOES
NOT
KNOW
WQ
NOR
THE
CURRENT
PHASE
Q
NOR
THE
SPEEDUP
FUNCTION
ΓQ
IN
THIS
PAPER
WE
CONSIDER
SEVERAL
DIFFERENT
MODELS
DEPENDING
ON
HOW
THE
PROCESSING
ON
DIFFERENT
COPIES
INTERACT
ASSUME
MULTIPLE
COPIES
OF
JOB
I
ARE
RUN
WITH
THE
SPEED
AND
NUMBER
OF
PROCESSORS
ASSIGNED
TO
THE
K
TH
COPY
BEING
SK
AND
PK
IN
THE
INDEPENDENT
PROCESSING
MODEL
IF
COPY
K
IS
RUNNING
IN
A
PHASE
WITH
SPEEDUP
FUNCTION
Γ
THEN
WORK
IS
PROCESSED
ON
THIS
COPY
AT
RATE
SKΓ
PK
INDEPENDENT
OF
THE
RATE
OF
PROCESSING
ON
THE
OTHER
COPIES
IN
THE
MAXIMUM
PROCESSING
MODEL
IF
EACH
COPY
OF
JOB
JI
IS
IN
A
PHASE
WITH
SPEEDUP
FUNCTION
Γ
THEN
EACH
COPY
PROCESSES
WORK
AT
A
RATE
OF
MAXK
SKΓ
PK
IN
THE
SUM
PROCESSING
MODEL
IF
EACH
COPY
OF
JOB
JI
IS
IN
A
PHASE
WITH
SPEEDUP
FUNCTION
Γ
THEN
EACH
COPY
PROCESSES
WORK
AT
A
RATE
OF
K
SKΓ
PK
NOTE
THAT
AS
A
CONSEQUENCE
OF
THESE
DEFINITIONS
IN
THE
MAXIMUM
PROCESSING
MODEL
AND
IN
THE
SUM
PROCESSING
MODEL
IT
IS
THE
CASE
THAT
EACH
COPY
OF
EACH
JOB
IS
ALWAYS
AT
THE
SAME
POINT
IN
ITS
EXECUTION
THE
COMPLETION
TIME
OF
A
JOB
JI
DENOTED
CI
IS
THE
FIRST
POINT
OF
TIME
WHEN
ALL
THE
WORK
ON
SOME
COPY
OF
THE
JOB
HAS
BEEN
PROCESSED
NOTE
THAT
IN
THE
LANGUAGE
OF
SCHEDULING
WE
ARE
ASSUMING
THAT
PREEMPTION
IS
ALLOWED
THAT
IS
A
JOB
MAYBE
BE
SUSPENDED
AND
LATER
RESTARTED
FROM
THE
POINT
OF
SUSPENSION
A
JOB
IS
SAID
TO
BE
ACTIVE
AT
TIME
T
IF
IT
HAS
BEEN
RELEASED
BUT
HAS
NOT
COMPLETED
I
E
RI
T
CI
THE
RESPONSE
FLOW
TIME
OF
JOB
JI
IS
CI
RI
WHICH
IS
THE
LENGTH
OF
THE
TIME
INTERVAL
DURING
WHICH
THE
JOB
IS
ACTIVE
LET
NT
BE
THE
NUMBER
OF
ACTIVE
JOBS
AT
TIME
T
ANOTHER
FORMULATION
OF
TOTAL
FLOW
TIME
IS
NTDT
WHEN
RUNNING
AT
SPEED
A
PROCESSOR
CONSUMES
P
SΑ
UNITS
OF
ENERGY
PER
UNIT
TIME
WHERE
Α
IS
SOME
FIXED
CONSTANT
WE
CALL
P
THE
POWER
FUNCTION
A
PHASE
OF
A
JOB
IS
PARALLELIZABLE
IF
ITS
SPEEDUP
FUNCTION
IS
Γ
P
P
INCREASING
THE
NUMBER
OF
PROCESSORS
ALLOCATED
TO
A
PARALLELIZABLE
PHASE
BY
A
FACTOR
OF
INCREASES
THE
RATE
OF
PROCESSING
BY
A
FACTOR
OF
A
PHASE
OF
A
JOB
IS
PARALLEL
UP
TO
Q
PROCESSORS
IF
Γ
P
P
FOR
P
Q
AND
Γ
P
Q
FOR
P
Q
A
SPEEDUP
FUNCTION
Γ
IS
NONDECREASING
IF
AND
ONLY
IF
Γ
Γ
WHENEVER
A
SPEEDUP
FUNCTION
Γ
IS
SUBLINEAR
IF
AND
ONLY
IF
Γ
Γ
WHENEVER
WE
ASSUME
ALL
SPEEDUP
FUNCTIONS
Γ
IN
THE
INPUT
INSTANCE
ARE
NONDECREASING
AND
SUBLINEAR
WE
FURTHER
ASSUME
THAT
ALL
SPEEDUP
FUNCTIONS
SATISFY
Γ
P
P
FOR
P
THIS
NATURAL
ASSUMPTION
MEANS
THAT
WHEN
A
JOB
JI
IS
ASSIGNED
TO
A
SINGLE
PROCESSOR
AND
SHARES
THIS
PROCESSOR
WITH
OTHER
JOBS
THE
RATE
THAT
JI
IS
PROCESSED
IS
THE
FRACTION
OF
THE
PROCESSOR
THAT
JI
RECEIVES
TIMES
THE
SPEED
OF
THE
PROCESSOR
LET
A
BE
AN
ALGORITHM
AND
J
AN
INSTANCE
WE
DENOTE
THE
SCHEDULE
OUTPUT
BY
A
ON
J
AS
A
J
WE
LET
FA
J
AND
EA
J
DENOTE
THE
TOTAL
FLOW
TIME
AND
ENERGY
INCURRED
IN
A
J
LET
COSTA
J
FA
J
EA
J
DENOTE
THE
COST
WE
WILL
USE
M
AS
A
SHORT
HAND
FOR
MULTILAPS
LET
OPT
BE
THE
OPTIMAL
ALGORITHM
THAT
ALWAYS
MINIMIZES
TOTAL
FLOW
TIME
PLUS
ENERGY
A
RANDOMIZED
ALGORITHM
A
IS
C
COMPETITIVE
OR
HAS
COMPETITIVE
RATIO
C
IF
FOR
ALL
INSTANCES
J
E
COSTA
J
C
COSTOPT
J
LOWER
BOUNDS
FOR
SINGLE
COPY
AND
NON
CHECKPOINTING
ALGORITHMS
IN
THIS
SECTION
WE
SHOW
THAT
THE
COMPETITIVE
RATIO
MUST
GROW
QUICKLY
WITH
THE
NUMBER
OF
PROCESSORS
IF
ONLY
ONE
COPY
OF
EACH
JOB
CAN
BE
RUNNING
LEMMA
OR
IF
MULTIPLE
COPIES
ARE
ALLOWED
BUT
NO
CHECKPOINTING
IS
ALLOWED
LEMMA
WE
FIRST
START
WITH
A
COUPLE
BASIC
LEMMAS
ABOUT
OPTIMAL
SCHEDULES
THAT
WILL
BE
USEFUL
THROUGHOUT
THE
PAPER
LEMMA
CONSIDER
A
JOB
WITH
WORK
W
AND
WITH
A
SINGLE
PHASE
WITH
A
SPEEDUP
FUNCTION
THAT
IS
PARALLELIZABLE
UP
TO
Q
PROCESSORS
ASSUME
THAT
THE
JOB
IS
RUN
ON
P
Q
PROCESSORS
THEN
THE
OPTIMAL
SPEED
IS
Α
FOR
A
COST
OF
Θ
W
ASSUME
THAT
THE
JOB
IS
RUN
ON
P
Q
PROCESSORS
THEN
THE
OPTIMAL
SPEED
IS
Α
FOR
A
COST
OF
Θ
Α
PROOF
FIRST
CONSIDER
THAT
CASE
THAT
P
Q
LET
BE
THE
SPEED
OF
THE
PROCESSORS
THE
FLOW
PLUS
ENERGY
IS
THEN
W
PSΑ
W
W
SΑ
THIS
IS
MINIMIZED
BY
SETTING
Α
PS
PS
PS
FOR
A
COST
OF
Θ
W
Α
P
ENERGY
IS
THEN
W
PSΑ
W
THIS
IS
MINIMIZED
BY
SETTING
Α
GIVING
A
COST
OF
QS
QS
Θ
Α
Q
Α
P
LEMMA
CONSIDER
A
JOB
WITH
WORK
W
WITH
A
SINGLE
PHASE
WITH
A
SPEEDUP
FUNCTION
THAT
IS
PARALLELIZABLE
UP
TO
Q
PROCESSORS
THE
OPTIMAL
SCHEDULE
USES
P
Q
PROCESSORS
RUN
AT
SPEED
Α
Q
Α
FOR
A
COST
OF
Θ
W
PROOF
FROM
THE
PROOF
OF
LEMMA
WE
KNOW
THAT
IF
THE
ALGORITHM
ALLOCATES
P
Q
SPEED
PROCESSORS
TO
THIS
JOB
THE
COST
IS
MINIMIZED
BY
WHEN
FOR
A
COST
OF
Θ
W
Α
THIS
IS
MINIMIZED
BY
MAKING
P
AS
BIG
AS
POSSIBLE
NAMELY
P
Q
FROM
THE
PROOF
OF
LEMMA
WE
KNOW
THAT
IF
THE
ALGORITHM
ALLOCATES
P
Q
SPEED
PROCESSORS
TO
THIS
JOB
THE
COST
IS
MINIMIZED
WHEN
GIVING
A
COST
OF
Θ
Α
Q
THIS
IS
MINIMIZED
BY
MAKING
P
Α
AS
SMALL
AS
POSSIBLE
NAMELY
P
Q
THUS
IN
EITHER
CASE
THE
OPTIMAL
SCHEDULING
POLICY
USES
P
Q
PROCESSORS
RUN
AT
SPEED
FOR
A
COST
OF
Θ
W
Α
LEMMA
ANY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
THAT
ONLY
RUNS
ONE
COPY
OF
EACH
JOB
MUST
BE
Ω
M
Α
COMPETITIVE
AGAINST
AN
OBLIVIOUS
ADVERSARY
THAT
MUST
SPECIFY
THE
INPUT
A
PRIORI
PROOF
APPLYING
YAO
TECHNIQUE
WE
GIVE
A
PROBABILITY
DISTRIBUTION
OVER
THE
INPUT
AND
SHOW
THAT
EVERY
DETERMINISTIC
ALGORITHM
A
WILL
HAVE
AN
EXPECTED
COMPETITIVE
RATIO
OF
Ω
M
Α
Α
THE
INSTANCE
WILL
BE
SELECTED
UNIFORMLY
AT
RANDOM
FROM
TWO
POSSIBILITIES
THE
FIRST
POSSIBLE
INSTANCE
CONSISTS
OF
ONE
JOB
WITH
Α
UNITS
OF
PARALLELIZABLE
WORK
THE
SECOND
POSSIBLE
INSTANCE
WILL
CONSIST
OF
JOB
WITH
ONE
UNIT
OF
WORK
THAT
IS
PARALLEL
UP
TO
ONE
PROCESSOR
BY
PLUGGING
THESE
PARAMETERS
INTO
LEMMA
ONE
CAN
SEE
THAT
THE
OPTIMAL
COST
IS
Θ
FOR
BOTH
INSTANCES
LET
P
DENOTE
THE
NUMBER
OF
PROCESSORS
USED
BY
THE
ALGORITHM
A
BY
LEMMA
THE
COST
FOR
THE
ALGORITHM
A
IS
EITHER
Ω
M
Α
OR
Ω
Α
DEPENDING
ON
THE
INSTANCE
BOTH
THE
MAXIMUM
AND
THE
AVERAGE
OF
THESE
TWO
COSTS
IS
MINIMIZED
BY
BALANCING
THESE
TWO
COSTS
WHICH
IS
ACCOMPLISHED
BY
SETTING
P
Α
THIS
SHOWS
THAT
THE
COMPETITIVE
RATIO
IS
Ω
M
Α
Α
LEMMA
IN
THE
INDEPENDENT
PROCESSING
MODEL
ANY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
MUST
BE
Ω
M
Α
Α
COMPETITIVE
AGAINST
AN
OBLIVIOUS
ADVERSARY
THAT
MUST
SPECIFY
THE
INPUT
A
PRIORI
PROOF
APPLYING
YAO
TECHNIQUE
WE
GIVE
A
PROBABILITY
DISTRIBUTION
OVER
THE
INPUTS
WITH
THE
PROPERTY
THAT
EVERY
DETERMINISTIC
ALGORITHM
A
WILL
HAVE
EXPECTED
COMPETITIVE
RATIO
Ω
M
Α
THE
RANDOM
INSTANCE
CONSISTS
OF
A
SINGLE
JOB
WITH
AN
INFINITELY
LARGE
NUM
BER
OF
PHASES
EACH
PHASE
WILL
BE
RANDOMLY
CHOSEN
TO
BE
ONE
OF
THE
TWO
JOB
INSTANCES
GIVEN
IN
LEMMA
THAT
IS
EACH
PHASE
WILL
EITHER
BE
PARALLELIZABLE
OR
PARALLEL
UP
TO
ONE
PROCESSOR
AND
THE
OPTIMAL
COST
FOR
EACH
PHASE
WILL
BE
Θ
CONSIDER
A
PARTICULAR
COPY
OF
THE
JOB
RUN
BY
A
BECAUSE
EACH
PHASE
IS
SO
SMALL
WE
CAN
ASSUME
THAT
THE
ALGORITHM
A
ALLOCATES
A
FIXED
NUMBER
OF
PROCESSORS
P
RUNNING
AT
A
FIXED
SPEED
FOR
THE
DURATION
OF
THE
PHASE
BY
THE
PROOF
OF
LEMMA
NO
MATTER
WHAT
THE
ALGORITHM
DOES
THE
PROBABILITY
IS
AT
LEAST
A
HALF
THAT
IT
OCCURS
A
COST
OF
Ω
M
Α
DURING
THIS
PHASE
ONE
CAN
THINK
OF
THE
PHASES
AS
BERNOULLI
TRIALS
WITH
OUTCOMES
BEING
COST
Ω
M
Α
WITH
PROBABILITY
AT
LEAST
A
HALF
AND
SMALLER
COST
WITH
PROBABILITY
AT
MOST
A
HALF
APPLYING
A
CHERNOFF
BOUND
WITH
HIGH
PROBABILITY
THE
ALGORITHM
HAS
COST
Ω
M
Α
ON
NEARLY
HALF
OF
THE
STAGES
WHILE
THE
OPTIMAL
COST
ON
EACH
STAGE
IS
BY
A
UNION
BOUND
THE
PROBABILITY
THAT
ANY
COPY
HAS
AVERAGE
COST
PER
PHASE
MUCH
LESS
THAN
Ω
M
Α
IS
SMALL
ANALYSIS
OF
MULTILAPS
IN
THIS
SECTION
WE
ASSUME
THAT
MULTIPLE
COPIES
OF
A
JOB
MAY
BE
RUN
SIMULTANEOUSLY
EACH
COPY
OF
A
JOB
MAY
BE
ASSIGNED
A
DIFFERENT
NUMBER
OF
PROCESSORS
BUT
EACH
PROCESSOR
RUNNING
THIS
COPY
MUST
BE
RUN
AT
THE
SAME
SPEED
WE
ASSUME
THAT
AT
EACH
MOMENT
IN
TIME
THE
RATE
THAT
WORK
IS
PROCESSED
ON
EACH
COPY
OF
A
JOB
IS
THE
MAXIMUM
OF
THE
RATES
OF
THE
DIFFERENT
COPIES
SO
ALL
COPIES
OF
A
JOB
ARE
ALWAYS
AT
THE
SAME
POINT
OF
EXECUTION
WE
GIVE
A
NONCLAIRVOYANT
ALGORITHM
MULTILAPS
FOR
THIS
SETTING
AND
SHOW
THAT
IT
IS
O
LOG
M
COMPETITIVE
FOR
FLOW
TIME
PLUS
ENERGY
WE
NOW
DESCRIBE
THE
ALGORITHM
LAPS
FROM
AND
THE
ALGORITHM
MULTILAPS
THAT
WE
INTRODUCE
HERE
WE
THEN
GIVE
SOME
UNDERLYING
MOTIVATION
FOR
THE
DESIGN
OF
MULTILAPS
ALGORITHM
LAPS
LET
Δ
AND
Β
BE
REAL
CONSTANTS
AT
ANY
TIME
T
THE
PROCESSOR
SPEED
IS
Δ
NA
Α
WHERE
NA
IS
THE
NUMBER
OF
ACTIVE
JOBS
AT
TIME
T
THE
PROCESSOR
PROCESSES
THE
ΒNA
ACTIVE
JOBS
WITH
THE
LATEST
RELEASE
TIMES
BY
SPLITTING
THE
PROCESSING
EQUALLY
AMONG
THESE
JOBS
FOR
OUR
PURPOSES
IN
THIS
PAPER
WE
WILL
TAKE
Δ
ALGORITHM
MULTILAPS
LET
Β
BE
A
REAL
NUMBER
THAT
PARAMETRIZES
MULTILAPS
LET
Μ
CONSIDER
ANY
TIME
T
LET
NA
BE
THE
NUMBER
OF
ACTIVE
JOBS
AT
T
EACH
OF
THE
ΒNA
ACTIVE
JOBS
WITH
THE
LATEST
RELEASE
TIMES
WILL
BE
RUN
AT
THIS
POINT
IN
TIME
CALL
THESE
JOBS
THE
LATE
JOBS
FOR
EACH
LATE
JOB
JI
A
PRIMARY
COPY
OF
JI
IS
RUN
ON
A
GROUP
OF
PA
Μ
M
PROCESSORS
WHERE
EACH
PROCESSOR
IN
THIS
GROUP
IS
RUN
AT
SPEED
SA
NA
Α
NOTE
THAT
THE
Μ
M
PRIMARY
COPIES
OF
THE
LATE
JOBS
ARE
EQUALLY
SHARING
A
Μ
FRACTION
OF
THE
PROCESSORS
FURTHERMORE
FOR
EACH
LATE
JOB
JI
THERE
ARE
LOG
PA
SECONDARY
COPIES
OF
JI
RUN
THE
JTH
J
LOG
PA
SECONDARY
COPY
OF
JI
IS
RUN
ON
A
GROUP
OF
PROCESSORS
WHERE
EACH
PROCESSOR
IN
THIS
GROUP
IS
RUN
AT
SPEED
Α
INTUITION
BEHIND
THE
DESIGN
OF
MULTILAPS
LET
US
GIVE
A
BIT
OF
INTUITION
BEHIND
THE
DESIGN
OF
MULTILAPS
IF
Μ
WAS
AND
NO
SECONDARY
COPIES
WERE
RUN
THEN
MULTILAPS
WOULD
ESSENTIALLY
BE
ADOPTING
THE
STRATEGY
OF
LAPS
OF
SHARING
THE
PROCESSING
POWER
EVENLY
AMONG
THE
LATEST
ARRIVING
Β
FRACTION
OF
THE
JOBS
LAPS
IS
O
COMPETITIVE
WHEN
ALL
WORK
IS
PARALLELIZABLE
UP
TO
THE
NUMBER
OF
AVAILABLE
PROCESSORS
HOWEVER
IF
A
PRIMARY
COPY
OF
A
JOB
IS
RUN
ON
MANY
PROCESSORS
THE
ONLINE
ALGORITHM
MAY
BE
WASTING
A
LOT
OF
ENERGY
IF
THIS
WORK
IS
NOT
HIGHLY
PARALLELIZABLE
TO
ACCOUNT
FOR
THIS
POSSIBILITY
MULTILAPS
RUNS
THE
PRIMARY
COPY
A
LITTLE
FASTER
FREEING
UP
SOME
PROCESSORS
TO
RUN
SECONDARY
COPIES
OF
THE
JOB
ON
FEWER
PROCESSORS
AND
AT
A
FASTER
SPEED
THE
NUMBER
OF
PROCESSORS
RUNNING
THE
SECONDARY
COPIES
ARE
GEOMETRICALLY
DECREASING
BY
A
FACTOR
OF
WHILE
THE
SPEEDS
ARE
INCREASING
BY
A
FACTOR
OF
Α
THUS
EACH
COPY
OF
A
JOB
IS
USING
APPROXIMATELY
THE
SAME
POWER
INTUITIVELY
ONE
OF
THE
COPIES
IS
RUNNING
THE
LATE
JOB
ON
THE
RIGHT
NUMBER
OF
PROCESSORS
THUS
MULTILAPS
USES
A
FACTOR
OF
O
LOG
M
MORE
ENERGY
THAN
OPTIMAL
BECAUSE
OF
THE
LOG
M
DIFFERENT
EQUI
POWER
COPIES
OF
THE
JOB
SETTING
Μ
GUARANTEES
THAT
THAT
MULTILAPS
DOESN
T
USE
MORE
THAN
M
PROCESSORS
THE
REST
OF
THIS
SECTION
IS
DEVOTED
TO
PROVING
THE
FOLLOWING
THEOREM
THEOREM
IN
THE
MAXIMUM
PROCESSING
MODEL
MULTILAPS
IS
O
LOG
M
COMPETITIVE
FOR
TOTAL
FLOW
TIME
PLUS
ENERGY
OVERVIEW
OF
THE
PROOF
OF
THEOREM
WE
NOW
GIVE
AN
OVERVIEW
OF
THE
STRUCTURE
OF
OUR
PROOF
OF
THEOREM
IN
LEMMA
WE
SHOW
HOW
TO
REDUCE
THE
ANALYSIS
OF
MULTILAPS
ON
ARBITRARY
INSTANCES
TO
THE
ANALYSIS
OF
MULTILAPS
ON
CANONICAL
INSTANCES
WE
DEFINE
AN
INSTANCE
TO
BE
CANONICAL
IF
THE
SPEEDUP
FUNCTION
FOR
EACH
JOB
PHASE
IS
PARALLEL
UP
TO
THE
NUMBER
PO
OF
PROCESSORS
THAT
OPT
USES
ON
THAT
PHASE
AND
IS
CONSTANT
THERE
AFTER
THE
VALUE
OF
PO
MAY
BE
DIFFERENT
FOR
EACH
PHASE
MORE
SPECIFICALLY
WE
SHOW
HOW
TO
CONSTRUCT
A
CANONICAL
INSTANCE
J
FROM
AN
ARBITRARY
INSTANCE
K
SUCH
THAT
THE
COST
OF
MULTILAPS
ON
K
IS
IDENTICAL
TO
THE
COST
OF
MULTILAPS
ON
J
AND
THE
OPTIMAL
COST
FOR
J
IS
AT
MOST
THE
OPTIMAL
COST
FOR
K
WE
THEN
DEFINE
A
VARIATION
OF
THE
UNIPROCESSOR
SETTING
THAT
WE
CALL
THE
SEQUENTIAL
SET
TING
IN
THE
SEQUENTIAL
SETTING
A
JOB
CAN
HAVE
SEQUENTIAL
PHASES
WHICH
ARE
PHASES
THAT
ARE
PROCESSED
AT
A
UNIT
RATE
INDEPENDENT
OF
THE
COMPUTATIONAL
RESOURCES
ASSIGNED
TO
THE
JOB
WE
THEN
SHOW
HOW
TO
REDUCE
THE
ANALYSIS
OF
MULTILAPS
ON
CANONICAL
INSTANCES
TO
THE
ANALYSIS
OF
LAPS
IN
THE
SEQUENTIAL
SETTING
MORE
PRECISELY
FROM
AN
ARBITRARY
CANONICAL
INSTANCE
J
WE
SHOW
HOW
TO
CREATE
AN
INSTANCE
J
FOR
THE
SEQUENTIAL
SETTING
WE
SHOW
IN
LEMMA
THAT
THE
FLOW
TIME
FOR
MULTILAPS
ON
J
IS
IDENTICAL
TO
THE
FLOW
TIME
OF
LAPS
ON
J
AND
THE
ENERGY
USED
BY
MULTILAPS
ON
J
IS
AT
MOST
O
LOG
M
TIMES
THE
ENERGY
USED
BY
LAPS
ON
J
WE
THEN
NEED
TO
RELATE
THE
OPTIMAL
SCHEDULE
FOR
J
TO
THE
OPTIMAL
SCHEDULE
FOR
J
TO
ACCOMPLISH
THIS
WE
CLASSIFY
EACH
PHASE
OF
A
JOB
IN
J
AS
EITHER
SATURATED
OR
UNSATURATED
DEPEND
ING
ON
THE
RELATIONSHIP
BETWEEN
THE
SPEEDUP
FUNCTION
AND
HOW
MANY
PROCESSORS
MULTILAPS
USES
FOR
THIS
PHASE
WE
CONSIDER
TWO
INSTANCES
DERIVED
FROM
J
AN
INSTANCE
JSAT
CONSISTING
OF
ONLY
THE
SATURATED
PHASES
IN
J
AND
AN
INSTANCE
JUNS
CONSISTING
OF
ONLY
THE
UNSATURATED
PHASES
IN
J
WE
THEN
CONSIDER
TWO
INSTANCES
DERIVED
FROM
THE
INSTANCE
J
AN
INSTANCE
J
CONSISTING
OF
PARALLEL
PHASES
IN
J
AND
AN
INSTANCE
J
CONSISTING
OF
SEQUENTIAL
PHASES
IN
J
THE
TRANSFORMATION
OF
J
TO
J
TRANSFORMS
PHASES
IN
JSAT
TO
PHASES
IN
J
AND
TRANSFORMS
PHASES
IN
JUNS
TO
PHASES
IN
J
IT
WILL
BE
CLEAR
THAT
THE
OPTIMAL
COST
FOR
J
IS
AT
LEAST
THE
OPTIMAL
COST
FOR
JSAT
PLUS
THE
OPTIMAL
COST
FOR
JUNS
WE
THEN
SHOW
IN
LEMMA
THAT
THE
OPTIMAL
COST
FOR
JSAT
IS
AT
LEAST
THE
OPTIMAL
COST
FOR
J
AND
IN
LEMMA
THAT
THE
OPTIMAL
COST
FOR
JUNS
IS
AT
LEAST
THE
OPTIMAL
COST
FOR
J
WE
THEN
DISCUSS
HOW
TO
GENERALIZE
THE
ANALYSIS
OF
LAPS
IN
USING
TECHNIQUES
FROM
TO
SHOW
THAT
THE
COST
OF
LAPS
IS
AT
MOST
A
CONSTANT
FACTOR
LARGER
THAN
THE
OPTIMAL
COST
FOR
J
PLUS
THE
OPTIMAL
COST
FOR
J
THIS
LINE
OF
REASONING
ALLOWS
US
TO
PROVE
OUR
THEOREM
AS
FOLLOWS
COSTM
K
COSTM
J
O
LOG
M
COSTLAPS
J
O
LOG
M
COSTOPT
J
COSTOPT
J
O
LOG
M
COSTOPT
JSAT
COSTOPT
JUNS
O
LOG
M
COSTOPT
J
O
LOG
M
COSTOPT
K
THE
FIRST
AND
FINAL
EQUALITIES
FOLLOW
FROM
LEMMA
THE
SECOND
EQUALITY
FOLLOWS
FROM
LEMMA
THE
THIRD
EQUALITY
FOLLOWS
FROM
THE
ANALYSIS
OF
LAPS
IN
THE
SEQUENTIAL
SETTING
THE
FOURTH
EQUALITY
FOLLOWS
FROM
LEMMA
AND
LEMMA
THE
FIFTH
EQUALITY
WILL
BE
AN
OBVIOUS
CONSEQUENCE
OF
THE
DEFINITIONS
OF
JSAT
AND
JUNS
WE
NOW
EXECUTE
THE
PROOF
STRATEGY
THAT
WE
HAVE
JUST
OUTLINED
WE
FIRST
SHOW
THAT
THERE
IS
A
WORST
CASE
INSTANCE
FOR
MULTILAPS
THAT
IS
CANONICAL
LEMMA
LET
K
BE
ANY
INPUT
INSTANCE
THERE
IS
A
CANONICAL
INSTANCE
J
SUCH
THAT
FM
J
FM
K
EM
J
EM
K
AND
COSTOPT
J
COSTOPT
K
PROOF
WE
CONSTRUCT
J
BY
MODIFYING
EACH
JOB
IN
K
AS
FOLLOWS
CONSIDER
AN
INFINITESIMALLY
SMALL
PHASE
OF
A
JOB
IN
K
WITH
WORK
W
AND
SPEEDUP
FUNCTION
Γ
LET
PO
BE
THE
NUMBER
OF
PROCESSORS
THAT
OPT
ALLOCATES
TO
THIS
PHASE
WHEN
SCHEDULING
K
WE
MODIFY
THIS
PHASE
SO
THAT
THE
NEW
SPEEDUP
FUNCTION
IS
Γ
P
P
Γ
PO
FOR
P
PO
AND
Γ
P
Γ
PO
FOR
P
PO
NOTE
THAT
MULTILAPS
MAY
PROCESS
THIS
PHASE
IN
SEVERAL
COPIES
OF
THIS
JOB
ASSUME
THAT
THE
I
TH
COPY
IS
PROCESSED
BY
PI
PROCESSORS
OF
SPEED
SI
DUE
TO
THE
MODIFICATION
OF
SPEEDUP
FUNCTION
THE
RATE
OF
PROCESSING
FOR
THE
I
TH
COPY
CHANGES
FROM
Γ
PI
SI
TO
Γ
PI
SI
IF
PI
PO
THEN
THE
RATE
OF
PROCESSING
ON
THE
I
TH
COPY
DOES
NOT
INCREASE
SINCE
Γ
IS
NONDECREASING
NOW
CONSIDER
A
COPY
WHERE
PI
PO
BY
THE
DEFINITION
OF
Γ
THE
RATE
OF
PROCESSING
Γ
PI
SI
PIΓ
PO
SI
SINCE
PI
PO
AND
SINCE
Γ
IS
SUBLINEAR
Γ
PO
Γ
PI
PLUGGING
THIS
BACK
IN
WE
GET
PO
PO
PI
THAT
THE
RATE
OF
PROCESSING
FOR
COPY
IS
AT
MOST
SIΓ
PI
SO
MULTILAPS
DOESN
T
FINISH
THIS
PHASE
IN
THE
MODIFIED
INSTANCE
BEFORE
IT
CAN
FINISH
THE
PHASE
IN
K
WE
THEN
DECREASE
THE
WORK
OF
THIS
PHASE
SO
THAT
THE
TIME
WHEN
THIS
PHASE
IS
FIRST
COMPLETED
AMONG
ALL
OF
THE
COPIES
IS
IDENTICAL
TO
WHEN
IT
COMPLETES
IN
MULTILAPS
K
NOTE
THAT
BY
CONSTRUCTION
THE
SCHEDULE
OF
MULTILAPS
ON
THIS
MODIFIED
INSTANCE
IS
IDENTICAL
TO
MULTILAPS
K
WHILE
OPT
MAY
GET
BETTER
PERFORMANCE
DUE
TO
THE
REDUCTION
OF
WORK
FINALLY
WE
CREATE
J
BY
MULTIPLYING
BOTH
THE
WORK
OF
THIS
PHASE
AND
THE
SPEEDUP
FUNCTION
BY
THE
SAME
FACTOR
OF
PO
O
TO
MAKE
THE
FINAL
SPEED
UP
FUNCTION
FOR
THIS
PHASE
PARALLEL
UP
TO
PO
PROCESSORS
THIS
CHANGE
DOES
NOT
EFFECT
THE
SCHEDULES
OF
EITHER
MULTILAPS
AND
OPT
DEFINITION
OF
THE
SEQUENTIAL
SETTING
EVERYTHING
IS
DEFINED
IDENTICALLY
AS
IN
SUBSECTION
WITH
THE
FOLLOWING
TWO
EXCEPTIONS
FIRSTLY
THERE
IS
ONLY
A
SINGLE
PROCESSOR
SECONDLY
JOB
PHASES
CAN
BE
SEQUENTIAL
WHICH
IN
THE
CONTEXT
OF
THIS
PAPER
MEANS
THAT
WORK
IN
THIS
PHASE
IS
PROCESSED
AT
A
RATE
OF
INDEPENDENT
OF
THE
FRACTION
OF
THE
PROCESSOR
ASSIGNED
TO
THE
JOB
AND
THE
SPEED
OF
THE
PROCESSOR
SO
SEQUENTIAL
WORK
IS
PROCESSED
AT
RATE
EVEN
IF
IT
IS
RUN
AT
A
SPEED
MUCH
GREATER
THAN
OR
IS
NOT
EVEN
RUN
AT
ALL
SEQUENTIAL
WORK
DOESN
T
CORRESPOND
TO
ANY
REALISTIC
SITUATION
BUT
IS
MERELY
MATHEMATICAL
CONSTRUCT
REQUIRED
FOR
THE
PROOF
DEFINITION
OF
THE
TRANSFORMATION
OF
A
CANONICAL
INSTANCE
J
INTO
THE
INSTANCE
J
IN
THE
SEQUENTIAL
SETTING
WE
TRANSFORM
EACH
JOB
IN
J
INTO
A
JOB
IN
J
BY
MODIFYING
EACH
PHASE
OF
THE
ORIGINAL
JOB
AT
EACH
POINT
IN
TIME
CONSIDER
A
PHASE
AND
THE
COPY
IN
MULTILAPS
WITH
THE
HIGHEST
PROCESSING
RATE
ON
THIS
PHASE
LET
ΓPO
BE
THE
SPEEDUP
FUNCTION
OF
THE
PHASE
WHICH
IS
PARALLEL
UP
TO
PO
PROCESSORS
WE
SAY
THE
PHASE
IS
CURRENTLY
SATURATED
M
ΒNA
PO
AND
UNSATURATED
OTHERWISE
NOTE
THAT
Μ
M
IS
THE
NUMBER
OF
PROCESSORS
ASSIGNED
TO
THE
PRIMARY
COPY
IN
MULTILAPS
THUS
A
PHASE
IS
SATURATED
IF
ALL
COPIES
IN
MULTILAPS
ARE
PROCESSING
IN
THE
PARALLEL
RANGE
OF
ΓPO
AND
UNSATURATED
OTHERWISE
CONSIDER
THE
CASE
THAT
THE
PHASE
IS
SATURATED
THE
COPY
WITH
THE
HIGHEST
PROCESSING
RATE
IN
MULTILAPS
IS
THE
ONE
WITH
PA
Μ
M
PROCESSORS
OF
SPEED
SA
NA
Α
GIVING
A
RATE
ΒNA
Μ
M
Α
OF
M
NA
Α
Α
NA
WE
MODIFY
THIS
PHASE
TO
BE
FULLY
PARALLELIZABLE
AND
SCALE
ΒNA
M
ΒNA
DOWN
THE
WORK
BY
A
FACTOR
OF
Α
NOTE
THAT
THE
PROCESSING
RATE
OF
LAPS
IS
NA
Α
SO
IT
A
WILL
COMPLETE
THE
PHASE
USING
THE
SAME
TIME
CONSIDER
THE
CASE
THAT
THE
PHASE
IS
UNSATURATED
LET
R
BE
THE
FASTEST
RATE
THAT
ANY
COPY
IN
MULTILAPS
IS
PROCESSING
WORK
IN
THIS
PHASE
WE
MODIFY
THIS
PHASE
TO
BE
SEQUENTIAL
AND
SCALE
DOWN
THE
WORK
BY
A
FACTOR
OF
R
BY
THE
DEFINITION
OF
SEQUENTIAL
THE
PROCESSING
RATE
OF
LAPS
ON
THIS
PHASE
IS
SO
IT
WILL
COMPLETE
THE
PHASE
USING
THE
SAME
TIME
AS
MULTILAPS
WE
NOW
SHOW
THAT
THE
COST
OF
MULTILAPS
ON
J
IS
AT
MOST
A
LOG
FACTOR
MORE
THAN
THE
COST
OF
LAPS
ON
J
IN
THE
SEQUENTIAL
SETTING
LEMMA
COSTM
J
FLAPS
J
O
LOG
M
ELAPS
J
FROM
THIS
WE
CAN
CONCLUDE
THAT
COSTM
J
O
LOG
M
COSTLAPS
J
PROOF
BY
CONSTRUCTION
THE
FLOW
TIME
FOR
MULTILAPS
J
IS
IDENTICAL
TO
THE
FLOW
TIME
FOR
LAPS
J
WE
NOW
SHOW
THAT
THE
ENERGY
USED
BY
MULTILAPS
J
IS
AT
MOST
A
LOG
FACTOR
MORE
THAN
THE
ENERGY
USED
BY
LAPS
J
THE
POWER
IN
MULTILAPS
J
IS
THE
SUM
OF
THE
POWERS
IN
THE
M
PROCESSORS
NOTE
THAT
FOR
EACH
OF
THE
ΒNA
LATE
JOBS
MULTILAPS
ALLOCATES
PA
PROCESSORS
OF
SPEED
SA
TO
A
PRIMARY
COPY
OF
THIS
JOB
RECALL
THAT
PA
Μ
M
AND
SA
NA
Α
MULTILAPS
ALSO
RUNS
LOG
PA
ΒNA
Μ
M
SECONDARY
COPIES
WHERE
THE
I
TH
COPY
IS
RUN
ON
PROCESSORS
OF
SPEED
Α
HENCE
THE
TOTAL
POWER
FOR
MULTILAPS
J
IS
ΒNA
Μ
M
ΒNA
NA
Μ
M
Α
Α
LOG
PA
I
Α
Α
Μ
Α
NA
ΒNA
LOG
PA
LAPS
J
RUNS
AT
SPEED
Α
AND
HENCE
POWER
N
SINCE
P
M
WE
CONCLUDE
THAT
E
J
A
A
A
M
O
LOG
M
ELAPS
J
WE
NOW
WANT
TO
SHOW
A
LOWER
BOUND
FOR
OPT
J
TO
STATE
THIS
LOWER
BOUND
WE
NEED
TO
INTRODUCE
SOME
NOTATION
DEFINE
JSAT
TO
BE
THE
INSTANCE
OBTAINED
FROM
J
BY
REMOVING
ALL
UNSATURATED
PHASES
IN
EACH
JOB
AND
DIRECTLY
CONCATENATING
THE
SATURATED
PHASES
DEFINE
JUNS
TO
BE
THE
INSTANCE
OBTAINED
FROM
J
BY
REMOVING
ALL
SATURATED
PHASES
AND
DIRECTLY
CONCATENATING
THE
UNSATURATED
PHASES
DEFINE
J
TO
BE
THE
INSTANCE
OBTAINED
FROM
J
BY
REMOVING
ALL
SEQUENTIAL
PHASES
IN
EACH
JOB
AND
DIRECTLY
CONCATENATING
THE
PARALLEL
PHASES
DEFINE
J
TO
BE
THE
INSTANCE
OBTAINED
FROM
J
BY
REMOVING
ALL
PARALLEL
PHASES
IN
EACH
JOB
AND
DIRECTLY
CONCATENATING
THE
SEQUENTIAL
PHASES
NOTE
THAT
THE
TRANSFORMATION
FROM
J
TO
J
TRANSFORMS
A
PHASE
IN
JSAT
TO
A
PHASE
IN
J
AND
TRANSFORMS
A
PHASE
IN
JUNS
TO
A
PHASE
SEQ
OBVIOUSLY
OPT
CAN
ONLY
GAIN
BY
SCHEDULING
JSAT
AND
JUNS
SEPARATELY
THAT
IS
COSTOPT
J
COSTOPT
JSAT
COSTOPT
JUNS
WE
NOW
WANT
TO
SHOW
THAT
COSTOPT
J
O
COSTOPT
JSAT
AND
COSTOPT
J
O
COSTOPT
JUNS
LEMMA
COSTOPT
J
O
COSTOPT
JSAT
PROOF
WE
CONSTRUCT
A
SCHEDULE
OPT
J
FROM
THE
SCHEDULE
OPT
JSAT
PHASE
BY
PHASE
EACH
PHASE
IN
OPT
J
WILL
END
NO
LATER
THAN
THE
CORRESPONDING
PHASE
IN
OPT
JSAT
AND
THE
SCHEDULE
FOR
OPT
J
WILL
USE
LESS
ENERGY
THAN
THE
SCHEDULE
OPT
JSAT
CONSIDER
A
INFINITESIMAL
SATURATED
PHASE
IN
JSAT
LET
PO
AND
SO
BE
THE
NUMBER
OF
PROCESSORS
AND
SPEED
ALLOCATED
BY
OPT
JSAT
TO
THIS
PHASE
BY
THE
DEFINITION
OF
CANONICAL
THE
PHASE
IS
PARALLELIZABLE
UP
TO
PO
PROCESSORS
THUS
OPT
JSAT
IS
PROCESSING
AT
A
RATE
OF
POSO
DEFINE
OPT
J
SO
THAT
IT
RUNS
AT
SPEED
ΑPOSO
ON
THIS
PHASE
SINCE
THE
TRANSFORMATION
SCALES
DOWN
THE
WORK
BY
A
FACTOR
OF
Α
OPT
J
WILL
COMPLETE
THE
PHASE
AT
THE
SAME
TIME
AS
OPT
JSAT
NOW
WE
NEED
TO
ARGUE
THAT
AT
ANY
POINT
OF
TIME
THE
POWER
FOR
OPT
JSAT
WILL
BE
AT
LEAST
THE
POWER
FOR
OPT
JPAR
THE
POWER
AT
THIS
TIME
IN
OPT
JSAT
IS
P
J
PO
J
SO
J
WHERE
THE
SUM
IS
OVER
ALL
JOBS
J
IT
IS
PROCESSING
AND
PO
J
AND
SO
J
ARE
THE
NUMBER
AND
SPEED
OF
THE
PROCESSORS
ALLOCATED
TO
J
KEEPING
R
J
PO
J
SO
J
FIXED
P
IS
MINIMIZED
BY
HAVING
ALL
THE
SO
J
TO
BE
THE
SAME
FIXED
VALUE
SO
THIS
GIVES
R
J
PO
JSO
SOM
AND
Α
P
P
Α
ΑM
R
M
RΑ
BY
OUR
DEFINITION
OF
OPT
J
THE
POWER
P
IN
OPT
J
CAN
BE
BOUNDED
AS
FOLLOWS
Α
P
J
Α
M
PO
J
SO
J
Α
Α
Α
RΑ
P
LEMMA
COSTOPT
J
O
COSTOPT
JUNS
PROOF
CONSIDER
A
UNSATURATED
PHASE
IN
JUNS
THAT
IS
PARALLEL
UP
TO
PO
PROCESSORS
WE
GRA
CIOUSLY
ALLOW
OPT
JUNS
TO
SCHEDULE
EACH
PHASE
IN
JUNS
IN
ISOLATION
OF
THE
OTHER
PHASES
THIS
ONLY
IMPROVES
OPT
JUNS
CONSIDER
A
PARTICULAR
PHASE
IN
JUNS
WITH
A
SPEEDUP
FUNCTION
THAT
IS
PARALLEL
UP
TO
PO
PROCESSORS
AND
THAT
HAS
WORK
W
BY
LEMMA
THE
TOTAL
FLOW
TIME
PLUS
ENERGY
INCURRED
FOR
OPT
JUNS
IS
Θ
W
OPT
J
WILL
ALLOCATE
ZERO
PROCESSORS
TO
THE
CORRESPONDING
PHASE
AND
PROCESS
THE
PHASE
AT
RATE
SINCE
THE
WORK
IN
J
IS
SEQUENTIAL
HENCE
OPT
J
INCURS
NO
ENERGY
COST
FOR
THIS
PHASE
SO
TO
FINISH
THE
PROOF
WE
WILL
SHOW
THAT
THE
FLOW
TIME
FOR
THIS
PHASE
IN
OPT
J
IS
AT
MOST
THE
COST
OF
THIS
PHASE
IN
OPT
JUNS
NAMELY
Θ
W
RECALL
THAT
IN
THE
TRANSFORMATION
FROM
J
TO
J
THIS
WORK
IS
SCALED
DOWN
BY
THE
FASTEST
RATE
THAT
THIS
PHASE
IS
PROCESSED
BY
ANY
COPY
IN
MULTILAPS
CONSIDER
THE
COPY
IN
MULTILAPS
THAT
IS
PROCESSING
IN
THE
PARALLEL
RANGE
WITH
THE
MOST
NUMBER
OF
PROCESSORS
I
E
THE
COPY
WITH
PROCESSORS
SUCH
THAT
IS
MAXIMIZED
AND
AT
MOST
PO
SINCE
THE
PHASE
IS
UNSATURATED
PO
BY
THE
DEFINITION
OF
MULTILAPS
THE
PROCESSING
RATE
OF
THIS
COPY
IS
AT
LEAST
Α
Α
PO
Α
Α
O
THUS
THE
WORK
IN
THIS
PHASE
IN
J
AND
THE
FLOW
TIME
FOR
THIS
PHASE
IN
OPT
J
IS
AT
MOST
Α
O
ONE
CAN
EXTEND
THE
ANALYSIS
FOR
LAPS
IN
THE
UNIPROCESSOR
SETTING
TO
THE
SEQUENTIAL
SETTING
USING
THE
TECHNIQUES
USED
IN
WE
REFER
THE
READER
TO
FOR
FULL
DETAILS
AND
JUST
GIVE
THE
UNDERLYING
INTUITION
HERE
THE
ANALYSIS
USES
AMORTIZED
LOCAL
COMPETITIVENESS
THAT
IS
IT
IS
SHOWN
THAT
AT
EVERY
TIME
PLAPS
NLAPS
DΦ
DT
C
POPT
NOPT
WHERE
P
DENOTES
POWER
N
DENOTES
THE
NUMBER
OF
ACTIVE
JOBS
Φ
IS
THE
POTENTIAL
FUNCTION
AND
C
IS
THE
DESIRED
COMPETITIVE
RATIO
SO
WHEN
PLAPS
NLAPS
IS
LARGE
THE
PROCESSING
OF
LAPS
LOWERS
THE
POTENTIAL
FUNCTION
Φ
ENOUGH
TO
MAKE
THE
EQUATION
TRUE
NOW
CONSIDER
THE
SEQUENTIAL
SETTING
THE
DIFFICULTY
THAT
ARISES
IS
THAT
THE
PROCESSING
THAT
LAPS
DOES
ON
SEQUENTIAL
JOBS
MAY
NOT
LOWER
THE
POTENTIAL
FUNCTION
HOWEVER
IF
THE
NUMBER
OF
SEQUENTIAL
PHASES
THAT
LAPS
IS
PROCESSING
IS
VERY
SMALL
THEN
RAISING
THE
SPEED
OF
LAPS
BY
A
SMALL
AMOUNT
WILL
BE
ENOUGH
SO
THAT
THE
POTENTIAL
FUNCTION
DECREASE
SUFFICIENTLY
QUICKLY
DUE
TO
THE
PROCESSING
ON
THE
NON
SEQUENTIAL
JOBS
IF
THE
NUMBER
OF
SEQUENTIAL
PHASES
IS
LARGE
AT
A
PARTICULAR
TIME
THEN
THE
INCREASE
IN
FLOW
TIME
THAT
LAPS
IS
EXPERIENCING
ON
THESE
JOBS
IS
ALSO
EXPERIENCED
BY
THE
ADVERSARY
AT
SOME
POINT
IN
TIME
THIS
INCREASE
IN
FLOW
TIME
EXPERIENCED
BY
THE
ADVERSARY
PAYS
FOR
THE
INCREASE
IN
FLOW
TIME
FOR
LAPS
AT
THIS
POINT
OF
TIME
NOTE
THAT
BY
DEFINITION
OF
LAPS
THE
POWER
USED
BY
LAPS
IS
COMPARABLE
TO
THE
INCREASE
IN
FLOW
TIME
EXPERIENCE
BY
LAPS
WE
CAN
THUS
DERIVE
THE
FOLLOWING
THEOREM
THEOREM
COSTLAPS
J
O
COSTOPT
J
COSTOPT
J
LOWER
BOUND
FOR
CHECKPOINTABLE
MULTIPLE
COPIES
WE
SHOW
HERE
THAT
EVEN
IF
THE
RATE
THAT
THE
WORK
IS
PROCESSED
IS
THE
SUM
OF
THE
RATE
OF
THE
COPIES
EVERY
RANDOMIZED
ALGORITHM
IS
POLY
LOG
COMPETITIVE
THEOREM
IN
THE
SUM
PROCESSING
MODEL
THE
COMPETITIVE
RATIO
FOR
EVERY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
IS
Ω
Α
M
AGAINST
AN
OBLIVIOUS
ADVERSARY
THAT
MUST
SPECIFY
THE
INPUT
A
PRIORI
PROOF
APPLYING
YAO
TECHNIQUE
WE
GIVE
A
PROBABILITY
DISTRIBUTION
OVER
THE
INPUT
AND
SHOW
THAT
EVERY
DETERMINISTIC
ALGORITHM
A
WILL
HAVE
AN
EXPECTED
COMPETITIVE
RATIO
OF
Ω
Α
M
THERE
ARE
Θ
LOG
M
POSSIBLE
INSTANCES
EACH
SELECTED
WITH
EQUAL
PROBABILITY
FOR
EACH
J
LOG
M
INSTANCE
JJ
WILL
CONSIST
OF
ONE
JOB
WITH
WORK
WJ
Α
J
AND
SPEEDUP
FUNCTION
P
WHERE
ΓQ
P
IS
THE
SPEED
UP
FUNCTION
THAT
IS
PARALLELIZABLE
UP
TO
Q
PROCESSORS
BY
APPLYING
LEMMA
OPT
JJ
ALLOCATES
PJ
PROCESSORS
EACH
OF
SPEED
SJ
Α
RESULTING
IN
A
COST
OF
Θ
NOW
CONSIDER
ANY
DETERMINISTIC
NONCLAIRVOYANT
ALGORITHM
A
ROUNDING
THE
NUMBER
OF
PROCESSORS
A
COPY
IS
RUN
ON
TO
A
FACTOR
OF
TWO
DOESN
T
CHANGE
THE
OBJECTIVE
BY
MORE
THAN
A
CONSTANT
FACTOR
AND
THERE
IS
NO
SIGNIFICANT
BENEFIT
FROM
RUNNING
TWO
COPIES
ON
AN
EQUAL
NUMBER
OF
PROCESSORS
SINCE
THE
ALGORITHM
IS
NONCLAIRVOYANT
IT
WILL
GAIN
NO
INFORMATION
ABOUT
THE
IDENTITY
OF
JJ
UNTIL
SOME
COPY
FINISHES
SINCE
THE
POWER
FUNCTION
IS
CONVEX
IT
IS
BEST
FOR
THE
ALGORITHM
TO
RUN
EACH
COPY
AT
CONSTANT
SPEED
THUS
WE
CAN
ASSUME
THAT
THE
ALGORITHM
RUNS
LOG
M
COPIES
OF
THE
JOB
WITH
COPY
I
RUN
ON
PROCESSORS
AT
AT
SOME
CONSTANT
SPEED
SI
NOTE
THAT
THE
ALGORITHM
CAN
SET
SI
IF
IT
DOESN
T
WANT
TO
RUN
A
COPY
ON
THAT
MANY
PROCESSORS
THE
POWER
OF
COPY
I
IS
P
PISIΑ
AND
THE
TOTAL
POWER
FOR
A
IS
I
I
LET
R
I
J
SI
DENOTE
THE
RATE
THAT
THE
COPY
I
IS
PROCESSING
WORK
ON
JOB
JJ
BECAUSE
WE
ARE
ASSUMING
THAT
THE
WORK
COMPLETED
ON
A
JOB
IS
THE
SUM
OF
THAT
COMPLETED
BY
THE
GROUPS
WORKING
ON
IT
WE
HAVE
THAT
RJ
I
R
I
J
IS
THE
RATE
THAT
A
COMPLETES
WORK
ON
JOB
JJ
AND
MAXIMIZE
THIS
TIME
WE
BOUND
THIS
MAXIMUM
DENOTED
BY
T
AS
FOLLOWS
MIN
T
J
TJ
O
LOG
M
J
TJ
O
LOG
M
I
R
I
J
WJ
J
I
O
LOG
M
SI
WJ
O
I
J
SI
LOG
M
O
I
SI
J
I
WJ
J
I
LOG
M
WJ
LOG
M
I
J
I
Α
J
J
I
LOG
M
Α
J
O
SI
Α
J
LOG
M
I
J
I
J
I
LOG
M
Α
J
O
SI
I
LOG
M
O
LOG
M
I
Α
I
I
Α
Α
I
P
SUBJECT
TO
P
I
PI
THE
SUM
I
PI
IS
MAXIMIZED
BY
SETTING
EACH
PI
TO
LOG
M
GIVING
O
P
Α
O
P
Α
THE
TOTAL
COST
FOR
A
IS
FA
EA
T
P
T
P
T
LOG
M
Α
Ω
Α
M
RECALLING
THAT
THE
OPTIMAL
COST
IS
O
THE
RESULT
FOLLOWS
CONCLUSION
IN
SUMMARY
WE
HAVE
SHOWN
THAT
FOR
JOBS
THAT
MAY
HAVE
SIDE
EFFECTS
OR
THAT
ARE
NOT
CHECK
POINTABLE
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
QUICKLY
WITH
THE
NUMBER
OF
PROCESSORS
AND
FOR
CHECKPOINTABLE
JOBS
WITHOUT
SIDE
EFFECTS
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
SLOWLY
WITH
THE
NUMBER
OF
PROCESSORS
THERE
SEEM
TO
BE
SEVERAL
INTERESTING
LINES
OF
RESEARCH
SPAWNED
BY
THESE
RESULTS
MOST
OBVIOUSLY
THE
UPPER
AND
LOWER
BOUNDS
ON
THE
COMPETITIVE
RATIO
FOR
CHECKPOINTABLE
JOBS
WITHOUT
SIDE
EFFECTS
ARE
NOT
QUITE
TIGHT
IT
IS
PLAUSIBLE
THAT
ONE
COULD
OBTAIN
TIGHT
UPPER
AND
LOWER
BOUNDS
BY
BEING
MORE
CAREFUL
IN
THE
ANALYSES
ONE
MIGHT
ALSO
CONSIDER
THE
SITUATION
WHERE
THE
POWER
OBJECTIVE
IS
TEMPERATURE
IT
IS
NOT
CLEAR
HOW
TO
BEST
FORMALIZE
THIS
PROBLEM
THE
MOST
OBVIOUS
APPROACH
IS
TO
INCLUDE
A
CONSTRAINT
ON
TEMPERATURE
THAT
IS
YOU
CAN
NOT
EXCEED
THE
THRESHOLD
OF
THE
PROCESSOR
IF
THE
PROCESSOR
COOLS
ACCORDING
TO
NEWTON
LAW
THEN
THE
TEMPERATURE
IS
APPROXIMATELY
THE
MAXIMUM
ENERGY
USED
OVER
ANY
TIME
INTERVAL
OF
A
PARTICULAR
LENGTH
WHERE
THE
LENGTH
OF
THE
INTERVAL
IS
DETERMINED
BY
THE
COOLING
PARAMETER
OF
THE
DEVICE
IF
THE
INTERVALS
ARE
LONG
THEN
THE
TEMPERATURE
CONSTRAINT
IS
ESSENTIALLY
AN
ENERGY
CONSTRAINT
BUT
OPTIMIZING
ANY
REASONABLE
SCHEDULING
OBJECTIVE
SUBJECT
TO
AN
ENERGY
CONSTRAINT
IS
KNOWN
TO
BE
DIFFICULT
WITH
POWER
HAVING
BECOME
A
CRITICAL
ISSUE
IN
THE
OPER
ATION
OF
DATA
CENTERS
TODAY
THERE
HAS
BEEN
AN
INCREASED
PUSH
TOWARDS
THE
VISION
OF
ENERGY
PROPORTIONAL
COMPUT
ING
IN
WHICH
NO
POWER
IS
USED
BY
IDLE
SYSTEMS
VERY
LOW
POWER
IS
USED
BY
LIGHTLY
LOADED
SYSTEMS
AND
PROPORTION
ATELY
HIGHER
POWER
AT
HIGHER
LOADS
UNFORTUNATELY
GIVEN
THE
STATE
OF
THE
ART
OF
TODAY
HARDWARE
DESIGNING
INDIVID
UAL
SERVERS
THAT
EXHIBIT
THIS
PROPERTY
REMAINS
AN
OPEN
CHAL
LENGE
HOWEVER
EVEN
IN
THE
ABSENCE
OF
REDESIGNED
HARD
WARE
WE
DEMONSTRATE
HOW
OPTIMIZATION
BASED
TECHNIQUES
CAN
BE
USED
TO
BUILD
SYSTEMS
WITH
OFF
THE
SHELF
HARDWARE
THAT
WHEN
VIEWED
AT
THE
AGGREGATE
LEVEL
APPROXIMATE
THE
BEHAVIOR
OF
ENERGY
PROPORTIONAL
SYSTEMS
THIS
PAPER
EX
PLORES
THE
VIABILITY
AND
TRADEOFFS
OF
OPTIMIZATION
BASED
AP
PROACHES
USING
TWO
DIFFERENT
CASE
STUDIES
FIRST
WE
SHOW
HOW
DIFFERENT
POWER
SAVING
MECHANISMS
CAN
BE
COMBINED
TO
DELIVER
AN
AGGREGATE
SYSTEM
THAT
IS
PROPORTIONAL
IN
ITS
USE
OF
SERVER
POWER
SECOND
WE
SHOW
EARLY
RESULTS
ON
DELIVER
ING
A
PROPORTIONAL
COOLING
SYSTEM
FOR
THESE
SERVERS
WHEN
COMPARED
TO
THE
POWER
CONSUMED
AT
UTILIZATION
RE
SULTS
FROM
OUR
TESTBED
SHOW
THAT
OPTIMIZATION
BASED
SYS
TEMS
CAN
REDUCE
THE
POWER
CONSUMED
AT
UTILIZATION
TO
FOR
SERVER
POWER
AND
FOR
COOLING
POWER
INTRODUCTION
WITH
POWER
HAVING
BECOME
A
CRITICAL
ISSUE
IN
THE
OPERA
TION
OF
DATA
CENTERS
THE
CONCEPT
OF
ENERGY
PROPORTIONAL
COMPUTING
OR
ENERGY
SCALEDOWN
IS
DRAWING
IN
CREASING
INTEREST
A
SYSTEM
BUILT
ACCORDING
TO
THIS
PRIN
CIPLE
WOULD
IN
THEORY
USE
NO
POWER
WHEN
NOT
BEING
UTI
LIZED
WITH
POWER
CONSUMPTION
GROWING
IN
PROPORTION
TO
UTILIZATION
OVER
THE
LAST
FEW
YEARS
A
NUMBER
OF
TECH
NIQUES
HAVE
BEEN
DEVELOPED
TO
MAKE
SERVER
PROCESSORS
MORE
EFFICIENT
INCLUDING
BETTER
MANUFACTURING
TECHNIQUES
AND
THE
USE
OF
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
DVFS
FOR
RUNTIME
POWER
OPTIMIZATION
WHILE
THE
SAVINGS
ARE
SIGNIFICANT
THIS
HAS
LEAD
TO
CPUS
NO
LONGER
BEING
RESPONSIBLE
FOR
THE
MAJORITY
OF
POWER
CON
SUMED
IN
SERVERS
TODAY
INSTEAD
SUBSYSTEMS
THAT
HAVE
NOT
BEEN
OPTIMIZED
FOR
POWER
EFFICIENCY
SUCH
AS
NETWORK
CARDS
HARD
DRIVES
GRAPHICS
PROCESSORS
FANS
AND
POWER
SUPPLIES
HAVE
STARTED
DOMINATING
THE
POWER
CONSUMED
BY
SYSTEMS
ESPECIALLY
DURING
PERIODS
OF
LOW
UTILIZATION
INSTEAD
OF
WAITING
FOR
ALL
THESE
DIFFERENT
TECHNOLOGIES
TO
DELIVER
BETTER
ENERGY
EFFICIENCY
THIS
PAPER
ADVOCATES
THAT
ENERGY
PROPORTIONAL
COMPUTING
CAN
BE
APPROXIMATED
BY
USING
SOFTWARE
TO
CONTROL
POWER
USAGE
AT
THE
ENSEM
BLE
LEVEL
AN
ENSEMBLE
IS
DEFINED
AS
A
LOGICAL
COLLECTION
OF
SERVERS
AND
COULD
RANGE
FROM
AN
ENCLOSURE
OF
BLADES
A
SINGLE
RACK
GROUPS
OF
RACKS
TO
EVEN
AN
ENTIRE
DATA
CENTER
IT
WAS
PREVIOUSLY
DIFFICULT
TO
DYNAMICALLY
BALANCE
WORK
LOADS
TO
CONSERVE
POWER
AT
THE
ENSEMBLE
LEVEL
FOR
A
NUM
BER
OF
REASONS
UNNECESSARILY
SHUTTING
DOWN
APPLICATIONS
TO
SIMPLY
RESTART
THEM
ELSEWHERE
IS
LOOKED
UPON
AS
A
HIGH
RISK
HIGH
COST
CHANGE
BECAUSE
OF
THE
PERFORMANCE
IMPACT
THE
RISK
TO
SYSTEM
STABILITY
AND
THE
COST
OF
DESIGNING
CUS
TOM
CONTROL
SOFTWARE
HOWEVER
THE
RE
EMERGENCE
OF
VIR
TUALIZATION
AND
THE
ABILITY
TO
LIVE
MIGRATE
ENTIRE
VIR
TUAL
MACHINES
VMS
CONSISTING
OF
OSS
AND
APPLICATIONS
IN
A
TRANSPARENT
AND
LOW
OVERHEAD
MANNER
WILL
ENABLE
A
NEW
CATEGORY
OF
SYSTEMS
THAT
CAN
REACT
BETTER
TO
CHANGES
IN
WORKLOADS
AT
THE
AGGREGATE
LEVEL
BY
MOVING
WORK
LOADS
OFF
UNDER
UTILIZED
MACHINES
AND
THEN
TURNING
IDLE
MA
CHINES
OFF
IT
SHOULD
NOW
BE
POSSIBLE
TO
APPROXIMATE
AT
AN
ENSEMBLE
LEVEL
THE
BEHAVIOR
FOUND
IN
THEORETICAL
ENERGY
PROPORTIONAL
SYSTEMS
HOWEVER
VIRTUALIZATION
IS
NOT
A
MAGIC
BULLET
AND
A
NA
IVE
APPROACH
TO
CONSOLIDATION
CAN
HURT
APPLICATION
PER
FORMANCE
IF
SERVER
RESOURCES
ARE
OVERBOOKED
OR
LEAD
TO
RE
DUCED
POWER
SAVINGS
WHEN
COMPARED
TO
THE
MAXIMUM
POS
SIBLE
THIS
PAPER
THEREFORE
ADVOCATES
A
MORE
RIGOROUS
AP
PROACH
IN
THE
OPTIMIZATION
OF
ENSEMBLE
SYSTEMS
INCLUDING
THE
USE
OF
PERFORMANCE
MODELING
OPTIMIZATION
AND
CON
TROL
THEORY
FINALLY
INSTEAD
OF
ONLY
LOOKING
AT
MORE
TRADI
TIONAL
SERVER
COMPONENTS
SUCH
AS
STORAGE
CPUS
AND
THE
NETWORK
OPTIMIZATION
BASED
SYSTEMS
SHOULD
ALSO
CONSIDER
CONTROL
OF
OTHER
COMPONENTS
SUCH
AS
SERVER
FANS
AND
COM
PUTER
ROOM
AIR
CONDITIONERS
CRACS
AS
COOLING
COSTS
ARE
RAPIDLY
BECOMING
A
LIMITING
FACTOR
IN
THE
DESIGN
AND
OPERATION
OF
DATA
CENTERS
TODAY
WE
USE
TWO
CASE
STUDIES
TO
DEMONSTRATE
THAT
IT
IS
POSSI
BLE
BY
APPLYING
OPTIMIZATIONS
AT
THE
ENSEMBLE
LAYER
TO
DE
THE
POWER
CONSUMED
BY
SERVERS
AND
EXHIBIT
POWER
USAGE
BEHAVIOR
CLOSE
TO
THAT
OF
AN
ENERGY
PROPORTIONAL
SYSTEM
SECOND
WE
DEMONSTRATE
HOW
A
POWER
AND
WORKLOAD
AWARE
COOLING
CONTROLLER
CAN
EXHIBIT
THE
SAME
BEHAVIOR
FOR
COOL
ING
EQUIPMENT
SUCH
AS
SERVER
FANS
CASE
STUDIES
1000
NO
DVFS
DVFS
DVFS
OFF
AVERAGE
ENCLOSURE
UTILIZATION
IT
HAS
BEEN
ADVOCATED
THAT
OPTIMIZATION
BASED
ALGORITHMS
SHOULD
BE
PREFERRED
OVER
AD
HOC
HEURISTICS
IN
MAKING
SYS
TEM
RUNTIME
AND
MANAGEMENT
DECISIONS
WE
WILL
THEREFORE
NOT
STRESS
THIS
POINT
FURTHER
BUT
THROUGH
THE
USE
OF
TWO
CASE
STUDIES
SHOW
HOW
OPTIMIZATION
CAN
BE
USED
TO
DELIVER
ENERGY
PROPORTIONALITY
AT
THE
ENSEMBLE
LAYER
IT
SHOULD
BE
STRESSED
THAT
THE
FOCUS
OF
THIS
SECTION
IS
NOT
ON
THE
USE
OF
ANY
PARTICULAR
ALGORITHM
BUT
INSTEAD
ON
HOW
NON
ENERGY
PROPORTIONAL
SYSTEMS
CAN
BE
COMBINED
TO
APPROXI
MATE
THE
BEHAVIOR
OF
AN
ENERGY
PROPORTIONAL
SYSTEM
EXPERIMENTAL
SETUP
TO
EVALUATE
ENERGY
PROPORTIONALITY
IN
THE
CASE
STUDIES
WE
USED
AN
HP
BLADESYSTEM
ENCLOSURE
WITH
PRO
LIANT
SERVER
BLADES
AND
FANS
EACH
BLADE
WAS
EQUIPPED
WITH
GB
OF
RAM
AND
TWO
AMD
HE
DUAL
CORE
PROCESSORS
EACH
PROCESSOR
HAS
P
STATES
A
VOLT
AGE
AND
FREQUENCY
SETTING
CORRESPONDING
TO
FREQUENCIES
OF
AND
GHZ
THE
BLADES
AND
THE
FANS
ARE
EQUALLY
DIVIDED
AMONG
TWO
ROWS
ON
THE
FRONT
AND
BACK
ENDS
OF
THE
ENCLOSURE
RESPECTIVELY
EACH
BLADE
IS
COOLED
BY
MULTIPLE
FANS
AND
EACH
FAN
DRAWS
COOL
AIR
IN
THROUGH
THE
FRONT
OF
THE
BLADES
THE
ENCLOSURE
ALLOWS
US
TO
MEASURE
BLADE
TEMPERATURES
AND
THE
POWER
CONSUMED
BY
DIFFERENT
COMPONENTS
WE
USED
XEN
WITH
BOTH
THE
ADMINISTRATIVE
DOMAIN
AND
THE
VMS
USING
THE
PARA
VIRTUALIZED
LINUX
KERNEL
THE
XEN
ADMINISTRATIVE
DOMAIN
IS
STORED
ON
LOCAL
DISKS
WHILE
THE
VMS
USE
A
STORAGE
AREA
NETWORK
SAN
WE
USED
VMS
CONFIGURED
WITH
MB
OF
RAM
A
GB
VIRTUAL
HARD
DRIVE
AND
ONE
VIRTUAL
CPU
WE
SET
EACH
VM
MEMORY
TO
A
LOW
VALUE
TO
ALLOW
US
TO
EVALUATE
A
LARGE
CONFIGURATION
SPACE
FOR
WORKLOAD
PLACE
MENT
IN
PRODUCTION
ENVIRONMENTS
THE
SAME
EFFECT
COULD
BE
ACHIEVED
AT
RUNTIME
THROUGH
THE
USE
OF
PAGE
SHARING
OR
BALLOONING
WE
USED
GAMUT
TO
EXPERIMENT
WITH
DIFFERENT
VM
AND
PHYSICAL
SERVER
UTILIZATIONS
SERVER
ENERGY
PROPORTIONALITY
WE
EXAMINED
THE
ENERGY
PROPORTIONALITY
IN
THE
ENCLOSURE
LAYER
USING
THREE
DIFFERENT
POLICIES
THE
FIRST
POLICY
NO
EACH
RESULT
PRESENTED
ABOVE
IS
AN
AVERAGE
OF
APPROXIMATELY
READINGS
OVER
A
MINUTE
INTERVAL
FIGURE
ENCLOSURE
POWER
USAGE
BLADES
NETWORK
DVFS
USES
NO
POWER
SAVING
FEATURES
THE
SECOND
POLICY
DVFS
USES
HARDWARE
BASED
VOLTAGE
AND
FREQUENCY
SCALING
AND
IS
VERY
SIMILAR
TO
LINUX
ONDEMAND
GOVERNOR
THE
THIRD
POLICY
DVFS
OFF
USES
DVFS
PLUS
A
VM
MIGRA
TION
CONTROLLER
THAT
CONSOLIDATES
VIRTUAL
MACHINES
AND
TURNS
IDLE
MACHINES
OFF
ITS
ALGORITHM
USES
THE
BLADE
POWER
MODELS
FOR
THE
DIFFERENT
P
STATES
AND
SENSOR
READINGS
FROM
RESOURCE
MONITORING
AGENTS
THE
OPTIMIZATION
PROBLEM
IS
SIMILAR
TO
THAT
IN
WITH
CONSTRAINTS
ON
THE
BLADE
CPU
AND
MEMORY
UTILIZATION
TO
PREVENT
OVERBOOKING
EXAMPLES
OF
OTHER
CONSTRAINTS
THAT
COULD
BE
MODELED
INCLUDE
NETWORK
AND
STORAGE
UTILIZATION
OUR
EXPERIENCE
HAS
SHOWN
THAT
AL
GORITHMS
SUCH
AS
BIN
PACKING
OR
SIMULATED
ANNEALING
WORK
WELL
IN
THIS
SCENARIO
FOR
THIS
CASE
STUDY
WE
VARIED
EACH
VM
UTILIZATION
IN
THE
RANGE
OF
OF
A
SINGLE
CORE
AND
MEA
SURED
THE
POWER
CONSUMED
BY
THE
ENCLOSURE
THE
RESULTS
ARE
PRESENTED
IN
FIGURE
WHERE
THE
X
AXIS
SHOWS
THE
EN
CLOSURE
UTILIZATION
BY
ALL
VMS
AS
A
PERCENTAGE
OF
TOTAL
CAPACITY
OF
ALL
THE
BLADES
AS
EACH
SERVER
HAS
CORES
THE
UTILIZATION
ALSO
CORRESPONDS
TO
EACH
VM
UTILIZATION
AS
A
PERCENTAGE
OF
A
SINGLE
CORE
THE
MEASURED
POWER
SHOWN
ON
THE
Y
AXIS
INCLUDES
THE
POWER
CONSUMED
BY
THE
BLADES
AS
WELL
AS
THE
NETWORKING
SAN
AND
MANAGEMENT
MODULES
WITH
NO
DVFS
WE
NOTICE
A
VERY
SMALL
POWER
RANGE
W
BETWEEN
AND
UTILIZATION
AND
THE
MINIMUM
POWER
USED
IS
W
OR
OF
THE
POWER
CONSUMED
AT
UTILIZATION
SIGNIFICANTLY
AWAY
FROM
THE
THEORETICAL
MINIMUM
OF
W
ONCE
DVFS
IS
ENABLED
THE
POWER
RANGE
INCREASES
BUT
AS
SEEN
IN
THE
FIGURE
THE
SYSTEM
IS
STILL
NOT
ENERGY
PROPORTIONAL
AND
CONSUMES
W
AT
UTILIZATION
IT
IS
ONLY
WHEN
WE
LOOK
AT
THE
DVFS
OFF
POLICY
THAT
THE
SYSTEM
STARTS
APPROXIMATING
ENERGY
PROPOR
TIONALITY
AT
THE
ENSEMBLE
LEVEL
THE
RANGE
OF
POWER
USED
BETWEEN
AND
UTILIZATION
IS
W
AND
AT
UTILIZATION
THERE
IS
ONLY
A
W
DIFFERENCE
OR
OF
THE
POWER
CONSUMED
AT
UTILIZATION
FROM
THE
THEORETICAL
MINIMUM
OF
W
SPEED
RPM
STATIC
FAN
SPEED
REACTIVE
FAN
CONTROLLER
PREDICTIVE
FAN
CONTROLLER
AVERAGE
ENCLOSURE
UTILIZATION
FIGURE
FAN
POWER
CONSUMPTION
AND
MODEL
NOTE
FROM
THE
FIGURE
THAT
AT
HIGH
UTILIZATIONS
ABOVE
ALL
THREE
POLICIES
HAVE
THE
SAME
POWER
USAGE
AS
THEY
ARE
ALL
RUNNING
NEAR
PEAK
PERFORMANCE
WHILE
THE
DVFS
POLICY
ONLY
SHOWS
A
NOTICEABLE
DIFFERENCE
WHEN
IT
CAN
SWITCH
TO
A
LOWER
P
STATE
THE
DVFS
OFF
POLICY
STARTS
SHOWING
BENEFIT
AROUND
UTILIZATION
AS
IT
CAN
START
CON
SOLIDATING
VMS
AND
TURNING
MACHINES
OFF
EVEN
AT
UTILIZATION
ZERO
POWER
USAGE
WAS
NOT
ACHIEVED
WITH
DVFS
OFF
FOR
A
NUMBER
OF
REASONS
FIRST
AT
LEAST
ONE
ACTIVE
SERVER
IS
NEEDED
TO
HOST
ALL
THE
VMS
SECOND
OUR
ENCLOSURE
CONTAINED
TWO
NETWORK
SWITCHES
TWO
SAN
SWITCHES
TWO
MANAGEMENT
MODULES
AND
SIX
POWER
SUPPLIES
MOST
OF
THESE
COMPONENTS
ARE
NOT
ENERGY
PROPORTIONAL
FINALLY
LIKE
ALL
INDUSTRY
STANDARD
SERVERS
EVEN
OFF
SERVERS
HAVE
AN
ACTIVE
MANAGEMENT
PROCESSOR
THAT
IS
USED
FOR
NETWORK
MANAGEMENT
TASKS
SUCH
AS
REMOTE
KVM
AND
POWER
CYCLING
COOLING
ENERGY
PROPORTIONALITY
WHILE
WE
SHOWED
HOW
ENERGY
PROPORTIONALITY
COULD
BE
ACHIEVED
FOR
SERVER
POWER
IN
SECTION
COOLING
EQUIP
MENT
ALSO
CONSUMES
A
SIGNIFICANT
PART
OF
THE
TOTAL
POWER
USED
BY
A
DATA
CENTER
IN
PARTICULAR
SERVER
FANS
CAN
CON
SUME
BETWEEN
OF
TOTAL
SERVER
POWER
AND
AT
THE
DATA
CENTER
LEVEL
COOLING
CAN
ACCOUNT
FOR
AS
MUCH
AS
OF
THE
TOTAL
POWER
CONSUMED
IN
THIS
CASE
STUDY
WE
THEREFORE
EXAMINE
HOW
INTELLIGENT
FAN
CONTROL
CAN
BE
USED
TO
ACHIEVE
BETTER
ENERGY
PROPORTIONALITY
FOR
SERVER
COOLING
RESOURCES
WE
CONTINUE
TO
USE
THE
EXPERIMENTAL
SETUP
DESCRIBED
IN
SECTION
AND
USE
THE
DVFS
OFF
POL
ICY
PRESENTED
IN
SECTION
FOR
MANAGING
SERVER
POWER
THE
OBJECTIVE
OF
FAN
CONTROL
IS
TO
PROVIDE
ENOUGH
COOL
AIR
FOR
THE
BLADES
SO
THAT
THE
SERVER
TEMPERATURES
CAN
BE
MAIN
TAINED
BELOW
THRESHOLDS
FOR
THERMAL
SAFETY
REASONS
IN
THIS
PAPER
WE
SPECIFICALLY
EVALUATE
TWO
DIFFERENT
FAN
CONTROLLERS
ANOMALOUS
READING
NOTICEABLE
IN
FIGURE
IS
FOR
THE
NO
DVFS
AND
DVFS
SETTINGS
AT
AND
UTILIZATION
LEVELS
WHILE
THE
PROCESSOR
REPORTS
BEING
AT
THE
HIGHEST
P
STATE
WE
RECORDED
A
SHARP
DROP
IN
POWER
USAGE
AT
THESE
POINTS
WE
BELIEVE
THAT
THE
PROCESSOR
IS
USING
AN
INTERNAL
POWER
SAVING
SCHEME
THAT
IT
DISABLES
AT
HIGH
UTILIZATIONS
EACH
RESULT
PRESENTED
ABOVE
IS
AN
AVERAGE
OF
APPROXIMATELY
READINGS
OVER
A
MINUTE
INTERVAL
FIGURE
FAN
POWER
A
REACTIVE
FAN
CONTROLLER
RFC
AND
A
PREDICTIVE
FAN
CONTROLLER
PFC
THE
REACTIVE
FAN
CONTROLLER
RFC
IS
A
SIMPLE
FEEDBACK
CONTROLLER
THAT
CHANGES
FAN
SPEEDS
BASED
ON
THE
DATA
GATH
ERED
FROM
THE
HARDWARE
TEMPERATURE
SENSORS
PRESENT
ON
ALL
THE
BLADES
BECAUSE
OF
THE
COMPLEXITY
OF
SHARING
FANS
BETWEEN
BLADES
THE
RFC
SYNCHRONOUSLY
CHANGES
THE
SPEED
OF
ALL
FANS
IN
EACH
ROW
THERE
ARE
TWO
ROWS
OF
BLADES
AND
FANS
IN
THE
ENCLOSURE
IN
RESPONSE
TO
THE
MAXIMUM
OBSERVED
TEMPERATURE
FOR
THAT
ROW
WHEN
THE
MAXIMUM
TEMPERATURE
IS
ABOVE
THE
THRESHOLD
THE
FAN
SPEEDS
ARE
IN
CREASED
TO
PROVIDE
A
LARGER
VOLUME
OF
AIR
FLOW
TO
COOL
THE
SERVERS
AND
VICE
VERSA
THE
RFC
IS
SIMILAR
TO
COMMERCIAL
FAN
CONTROLLERS
USED
IN
INDUSTRY
TODAY
THE
PREDICTIVE
FAN
CONTROLLER
PFC
AIMS
TO
MINIMIZE
THE
TOTAL
FAN
POWER
CONSUMPTION
WITHOUT
VIOLATING
TEMPER
ATURE
CONSTRAINTS
IT
USES
TEMPERATURE
SENSORS
IN
THE
BLADES
AS
WELL
AS
SOFTWARE
SENSORS
TO
MONITOR
SERVER
UTILIZATION
FIGURE
SHOWS
THE
POWER
MODEL
FOR
A
FAN
IN
OUR
ENCLOSURE
WHERE
THE
FAN
POWER
IS
A
CUBIC
FUNCTION
OF
THE
FAN
SPEED
WHICH
CLOSELY
MATCHES
THE
MEASURED
FAN
POWER
NOTE
THAT
THE
VOLUME
AIR
FLOW
RATE
IS
APPROXIMATELY
PROPORTIONAL
TO
THE
FAN
SPEED
ALSO
NOTE
THAT
EACH
FAN
PROVIDES
DIFFERENT
LEVELS
OF
COOLING
RESOURCE
I
E
COOL
AIR
TO
EACH
INDIVIDUAL
BLADE
ACCORDING
TO
THE
LOCATION
OF
THE
FAN
WITH
RESPECT
TO
THE
BLADE
IN
THIS
REGARD
EACH
FAN
HAS
A
UNIQUE
COOLING
EFFI
CIENCY
WITH
RESPECT
TO
EACH
BLADE
THIS
PROVIDES
AN
OPPOR
TUNITY
TO
MINIMIZE
THE
FAN
POWER
CONSUMPTION
BY
EXPLOR
ING
THE
VARIATION
IN
COOLING
EFFICIENCY
OF
DIFFERENT
FANS
FOR
DIFFERENT
BLADES
ALONG
WITH
THE
TIME
VARYING
DEMANDS
OF
THE
WORKLOADS
WE
BUILT
A
THERMAL
MODEL
EMPIRICALLY
THAT
EXPLICITLY
CAPTURES
THE
COOLING
EFFICIENCY
BETWEEN
EACH
PAIR
OF
BLADE
AND
FAN
THIS
THERMAL
MODEL
TOGETHER
WITH
THE
BLADE
AND
THE
FAN
POWER
MODELS
IS
USED
BY
THE
PFC
TO
PREDICT
FUTURE
SERVER
TEMPERATURES
FOR
ANY
GIVEN
FAN
SPEED
AND
MEASURED
SERVER
UTILIZATION
BY
REPRESENTING
THIS
AS
A
CONVEX
CONSTRAINED
OPTIMIZATION
PROBLEM
THE
PFC
IS
ABLE
TO
USE
AN
OFF
THE
SHELF
CONVEX
OPTIMIZATION
SOLVER
TO
SET
THE
FAN
SPEEDS
TO
VALUES
THAT
POTENTIALLY
MINIMIZE
THE
AGGREGATE
POWER
CONSUMPTION
BY
THE
FANS
WHILE
KEEPING
THE
BLADE
TEMPERATURES
BELOW
THEIR
THRESHOLDS
THE
RESULTS
FOR
THE
TWO
CONTROLLERS
ARE
PRESENTED
IN
FIG
URE
THE
FIGURE
ALSO
INCLUDES
THE
POWER
CONSUMED
BY
SETTING
THE
FANS
TO
A
STATIC
SPEED
THAT
INDEPENDENT
OF
THE
WORKLOADS
IS
GUARANTEED
TO
KEEP
TEMPERATURES
BELOW
THE
THRESHOLD
UNDER
NORMAL
AMBIENT
OPERATING
CONDITIONS
WHEN
EXAMINING
THE
RFC
PERFORMANCE
IT
IS
HELPFUL
TO
NOTE
THE
RELATIONSHIP
BETWEEN
VM
UTILIZATION
AND
FAN
SPEED
FOR
A
GIVEN
ROW
OF
FANS
THE
FAN
SPEED
IS
DIRECTLY
CONTROLLED
BY
THE
MAXIMUM
CPU
TEMPERATURE
IN
THE
ROW
FURTHERMORE
CPU
TEMPERATURE
IS
A
FUNCTION
OF
BLADE
UTI
LIZATION
AND
BLADE
AMBIENT
OR
INLET
TEMPERATURE
BLADE
UTI
LIZATION
HOWEVER
DOES
NOT
ALWAYS
DIRECTLY
CORRELATE
TO
VM
UTILIZATION
FOR
EXAMPLE
WITH
EACH
VM
UTILIZATION
SET
TO
ONE
VCPU
THE
SYSTEM
CANNOT
FIT
MORE
THAN
VMS
PER
MACHINE
WITH
AN
OVERALL
BLADE
UTILIZATION
OF
FOUR
CPUS
HOWEVER
WITH
A
REDUCED
VM
UTILIZATION
OF
EACH
BLADE
CAN
ACCOMMODATE
VMS
WITH
AN
OVERALL
BLADE
UTILIZATION
OF
AS
A
BLADE
CPU
TEMPERATURE
WILL
BE
MUCH
HIGHER
WITH
A
UTILIZATION
OF
VS
IT
WILL
REQUIRE
A
GREATER
AMOUNT
OF
COOLING
AND
THEREFORE
USE
MORE
POWER
DUE
TO
INCREASED
FAN
SPEEDS
SIMILARLY
IF
A
BLADE
IS
LOCATED
IN
AN
AREA
OF
INCREASED
AMBIENT
TEMPERATURE
THAT
BLADE
COULD
ALSO
DRIVE
FAN
SPEED
HIGHER
IF
AND
WHEN
IT
BE
COMES
UTILIZED
EVEN
IF
THE
UTILIZATION
LEVELS
ARE
RELATIVELY
LOW
THESE
FACTORS
ARE
RESPONSIBLE
FOR
THE
RFC
OPERATING
IN
TWO
POWER
BANDS
APPROXIMATELY
BETWEEN
W
WHEN
THE
UTILIZATION
RANGES
BETWEEN
AND
BETWEEN
W
WHEN
THE
UTILIZATION
RANGES
BETWEEN
EVEN
AT
UTILIZATION
THE
RFC
STILL
USES
OF
THE
PEAK
POWER
USED
AT
UTILIZATION
IN
CONTRAST
DUE
TO
ITS
KNOWLEDGE
OF
THE
COOLING
EFFICIEN
CIES
OF
DIFFERENT
FANS
THE
DEMAND
LEVELS
OF
THE
INDIVIDUAL
BLADES
AND
THEIR
AMBIENT
TEMPERATURES
THE
PFC
IS
ABLE
TO
SET
FAN
SPEEDS
INDIVIDUALLY
AND
AVOID
THE
CORRELATED
BEHAV
IOR
EXHIBITED
BY
CONTROLLERS
LIKE
THE
RFC
OVERALL
THE
PFC
INDUCES
AN
APPROXIMATELY
LINEAR
RELATIONSHIP
BETWEEN
THE
FAN
POWER
AND
THE
AGGREGATE
ENCLOSURE
UTILIZATION
AND
AT
UTILIZATION
THE
PFC
ONLY
CONSUMES
OF
THE
POWER
IT
USES
AT
UTILIZATION
THE
USE
OF
MODEL
BASED
OPTI
MIZATION
ALSO
ALLOWS
THE
PFC
TO
PERFORM
SIGNIFICANTLY
BET
TER
THAN
THE
RFC
WHEN
WE
COMPARE
THE
TWO
CONTROLLERS
THE
PFC
CAN
REDUCE
FAN
POWER
USAGE
BY
AT
BOTH
AND
UTILIZATION
AT
UTILIZATION
THE
PFC
ONLY
CONSUMES
OF
THE
POWER
USED
BY
THE
RFC
AT
HOWEVER
THE
PFC
AND
THE
RFC
WITH
ZERO
LOAD
IS
UN
ABLE
TO
REDUCE
ITS
POWER
USAGE
TO
W
THIS
WAS
NOT
A
DE
FICIENCY
OF
OUR
OPTIMIZATION
BASED
APPROACH
BUT
WAS
DUE
TO
THE
FACT
THAT
THE
FANS
WERE
ALSO
RESPONSIBLE
FOR
COOLING
THE
BLADE
ENCLOSURE
NETWORKING
AND
MANAGEMENT
MOD
ULES
WE
THEREFORE
HAD
TO
LOWER
BOUND
THE
FAN
SPEEDS
TO
ENSURE
THAT
THESE
NON
ENERGY
PROPORTIONAL
COMPONENTS
DID
NOT
ACCIDENTALLY
OVERHEAT
ASSUMPTIONS
EVEN
THOUGH
WE
USED
A
HOMOGENEOUS
SET
OF
MACHINES
IN
OUR
CASES
STUDIES
OUR
EXPERIENCE
HAS
SHOWN
THAT
THESE
AL
GORITHMS
CAN
BE
EXTENDED
WITH
DIFFERENT
POWER
AND
THERMAL
MODELS
TO
CONTROL
AN
ENSEMBLE
COMPOSED
OF
HETEROGENEOUS
HARDWARE
FURTHER
CURRENT
GENERATION
OF
PROCESSORS
FROM
BOTH
INTEL
AND
AMD
SUPPORT
CPUID
MASKING
THAT
AL
LOWS
VMS
TO
MIGRATE
BETWEEN
PROCESSORS
FROM
DIFFERENT
FAMILIES
THIS
WORK
ALSO
ASSUMES
THAT
IT
IS
POSSIBLE
TO
MI
GRATE
VM
IDENTITIES
SUCH
AS
IP
AND
NETWORK
MAC
ADDRESSES
WITH
THE
VMS
WHILE
THIS
IS
GENERALLY
NOT
A
PROBLEM
IN
A
SINGLE
DATA
CENTER
MIGRATING
STORAGE
AREA
NETWORK
SAN
IDENTITIES
CAN
SOMETIMES
BE
PROBLEMATIC
HOWEVER
VENDOR
PRODUCTS
SUCH
AS
HP
VIRTUAL
CONNECT
AND
EMULEX
VIR
TUAL
HBA
HAVE
INTRODUCED
A
LAYER
OF
VIRTUALIZATION
IN
THE
STORAGE
STACK
TO
SOLVE
THIS
PROBLEM
NOTE
THAT
IT
MIGHT
BE
DIFFICULT
TO
ADOPT
THE
OPTIMIZATION
BASED
SOLUTIONS
SIMILAR
TO
THOSE
PROPOSED
IN
THIS
PAPER
TO
APPLICATIONS
THAT
DEPEND
ON
LOCALLY
ATTACHED
STORAGE
FOR
THEIR
INPUT
DATA
HOWEVER
THE
FACT
THAT
SUCH
LOCALLY
ATTACHED
STORAGE
SYSTEMS
USUALLY
REPLICATE
DATA
FOR
RELIA
BILITY
AND
AVAILABILITY
REASONS
MIGHT
PROVIDE
A
POSSI
BLE
SOLUTION
IN
SUCH
A
SCHEME
THE
OPTIMIZATION
ALGO
RITHMS
COULD
BE
MADE
AWARE
OF
THE
DEPENDENCIES
BETWEEN
THE
VMS
AND
THE
LOCATIONS
OF
THEIR
DATASETS
GIVEN
THIS
INFORMATION
THE
ALGORITHMS
COULD
FIND
A
SUITABLE
CONSOLI
DATED
MAPPING
OF
VMS
TO
PHYSICAL
MACHINES
IN
ORDER
TO
MAKE
THIS
SCHEME
EFFECTIVE
A
HIGHER
DEGREE
OF
REPLICATION
MIGHT
BE
NEEDED
TO
GIVE
THE
ALGORITHMS
MORE
FLEXIBILITY
IN
MAKING
PLACEMENT
DECISIONS
THIS
CHANGE
ESSENTIALLY
BOILS
DOWN
TO
A
TRADEOFF
BETWEEN
THE
COST
OF
INCREASED
STORAGE
CAPACITY
VERSUS
ENERGY
SAVINGS
FINALLY
OUR
APPROACH
APPROXIMATES
ENERGY
PROPORTION
ALITY
BY
TURNING
MACHINES
OFF
CONCERNS
HAVE
BEEN
PREVI
OUSLY
RAISED
ABOUT
RELIABILITY
OF
BOTH
SERVERS
AND
DISK
DRIVES
DUE
TO
AN
INCREASED
NUMBER
OF
ON
OFF
CYCLES
HOWEVER
OUR
CONVERSATIONS
WITH
BLADE
SYSTEM
DESIGNERS
HAVE
SHOWN
THAT
IT
SHOULD
NOT
AFFECT
SERVER
CLASS
MACHINES
OR
GIVEN
THE
LARGE
NUMBER
OF
ON
OFF
CYCLES
SUPPORTED
BY
DISK
DRIVES
THEIR
INTERNAL
STORAGE
SYSTEMS
DURING
THEIR
NORMAL
LIFETIME
AGGRESSIVE
CONSOLIDATION
MIGHT
ALSO
HURT
APPLICATION
AVAIL
ABILITY
IF
THE
UNDERLYING
HARDWARE
IS
UNRELIABLE
AS
MOST
APPLICATIONS
TEND
TO
BE
DISTRIBUTED
FOR
FAULT
TOLERANCE
IN
TRODUCING
APPLICATION
AWARENESS
INTO
THE
CONSOLIDATION
AL
GORITHM
TO
PREVENT
IT
FROM
CONSOLIDATING
SEPARATE
INSTANCES
OF
THE
SAME
APPLICATION
ON
THE
SAME
PHYSICAL
MACHINE
WILL
ADDRESS
THIS
ISSUE
CONCLUDING
REMARKS
THIS
PAPER
HAS
SHOWN
THAT
IT
IS
POSSIBLE
TO
USE
OPTIMIZATION
BASED
TECHNIQUES
TO
APPROXIMATE
ENERGY
PROPORTIONAL
BE
HAVIOR
AT
THE
ENSEMBLE
LEVEL
EVEN
THOUGH
OUR
TECHNIQUES
RESULT
IN
SOME
ADDED
COMPLEXITY
IN
THE
SYSTEM
IN
THE
FORM
OF
MODELS
OPTIMIZATION
ROUTINES
AND
CONTROLLERS
WE
BE
LIEVE
THE
POWER
SAVINGS
ARE
SIGNIFICANT
E
NOUGH
T
O
JUSTIFY
IT
HOWEVER
BETTER
INSTRUMENTATION
CAN
HELP
US
GET
EVEN
CLOSER
TO
THEORETICAL
ENERGY
PROPORTIONALITY
FOR
EXAMPLE
IF
TEMPERATURE
SENSORS
WHICH
ARE
RELATIVELY
CHEAP
TO
DE
PLOY
WERE
INSTALLED
IN
THE
NETWORKING
AND
SAN
SWITCHES
IT
WOULD
HAVE
ALLOWED
OUR
FAN
CONTROLLER
TO
HAVE
MORE
COM
PLETE
KNOWLEDGE
OF
THE
THERMAL
CONDITION
INSIDE
THE
ENCLO
SURE
SO
THAT
MORE
EFFICIENT
FAN
SPEED
OPTIMIZATION
COULD
BE
ACHIEVED
IN
ADDITION
WE
HAVE
USED
CPU
UTILIZATION
IN
THIS
CASE
STUDY
AS
A
PROXY
FOR
APPLICATION
LEVEL
PERFORMANCE
TO
DIRECTLY
EVALUATE
AND
MANAGE
APPLICATION
PERFORMANCE
WE
WILL
NEED
SENSORS
THAT
MEASURE
APPLICATION
LEVEL
METRICS
SUCH
AS
THROUGHPUT
AND
RESPONSE
TIME
THESE
SENSORS
CAN
BE
PROVIDED
BY
PARSING
APPLICATION
LOGS
OR
BY
MONITORING
USER
REQUESTS
AND
RESPONSES
WHILE
THE
CONTROLLERS
SHOWN
IN
THIS
PAPER
ASSUMED
A
SIN
GLE
MANAGEMENT
DOMAIN
FURTHER
STUDY
NEEDS
TO
BE
DONE
TO
SHOW
HOW
THEY
WOULD
WORK
IN
A
FEDERATED
ENVIRONMENT
WHERE
INFORMATION
WOULD
NEED
TO
BE
SHARED
BETWEEN
CON
TROLLERS
AT
DIFFERENT
MANAGEMENT
LAYERS
AND
POSSIBLY
FROM
DIFFERENT
VENDORS
WE
BELIEVE
THAT
SOME
OF
THE
DISTRIBUTED
MANAGEMENT
TASK
FORCE
DMTF
STANDARDS
WOULD
HELP
ADDRESS
AT
LEAST
SOME
OF
THESE
ISSUES
PRIOR
WORK
EXISTS
THAT
LOOKED
AT
DATA
CENTER
LEVEL
COOL
ING
EFFICIENCY
B
Y
M
ANIPULATION
O
F
C
RAC
U
NIT
ETTINGS
OR
BY
TEMPERATURE
AWARE
WORKLOAD
PLACEMENT
HOW
EVER
GIVEN
THAT
THESE
STUDIES
ONLY
LOOKED
AT
TOTAL
POWER
CONSUMPTION
A
MORE
CAREFUL
INVESTIGATION
IS
NEEDED
IN
THE
CONTEXT
OF
THIS
PAPER
FOR
EXAMPLE
THE
MODEL
BASED
OPTI
MIZATION
APPROACH
WE
USED
IN
THE
PREDICTIVE
FAN
CONTROLLER
MAY
BE
APPLIED
TO
THE
CRAC
UNIT
CONTROL
PROBLEM
STUD
IED
IN
TO
ACHIEVE
ENERGY
PROPORTIONALITY
FOR
THE
COOLING
EQUIPMENT
AT
THE
DATA
CENTER
LEVEL
FINALLY
EVEN
THOUGH
THIS
PAPER
DEMONSTRATED
ENERGY
PROPORTIONALITY
AT
THE
ENSEMBLE
LAYER
THIS
DOES
NOT
PRE
CLUDE
THE
NEED
FOR
BETTER
ENERGY
EFFICIENCY
FOR
INDIVIDUAL
COMPONENTS
SUCH
AS
DISKS
MEMORY
AND
POWER
SUPPLIES
WE
BELIEVE
THAT
NEW
HARDWARE
DESIGNS
WITH
FINER
LEVELS
OF
POWER
CONTROL
WILL
HELP
IN
DESIGNING
ENERGY
EFFICIENT
SYS
TEMS
AT
BOTH
THE
SINGLE
SERVER
AND
ENSEMBLE
LAYER
POWER
MANAGEMENT
IS
INCREASINGLY
IMPORTANT
IN
COMPUTER
COMMUNICATIONS
SYSTEMS
NOT
ONLY
IS
THE
ENERGY
CONSUMPTION
OF
THE
INTERNET
BECOMING
A
SIGNIFICANT
FRACTION
OF
THE
ENERGY
CONSUMPTION
OF
DEVELOPED
COUNTRIES
BUT
COOLING
IS
ALSO
BECOMING
A
MAJOR
CONCERN
CONSEQUENTLY
THERE
IS
AN
IMPORTANT
TRADEOFF
IN
MODERN
SYSTEM
DESIGN
BETWEEN
REDUCING
ENERGY
USE
AND
MAINTAINING
GOOD
PERFORMANCE
THERE
IS
AN
EXTENSIVE
LITERATURE
ON
POWER
MANAGEMENT
REVIEWED
IN
A
COMMON
TECHNIQUE
WHICH
IS
THE
FOCUS
OF
THE
CURRENT
PAPER
IS
DYNAMIC
SPEED
SCALING
THIS
DYNAMICALLY
REDUCES
THE
PROCESSING
SPEED
AT
TIMES
OF
LOW
WORKLOAD
SINCE
PROCESSING
MORE
SLOWLY
USES
LESS
ENERGY
PER
OPERATION
THIS
IS
NOW
COMMON
IN
MANY
CHIP
DESIGNS
IN
PARTICULAR
SPEED
SCALING
HAS
BEEN
PROPOSED
FOR
MANY
NETWORK
DEVICES
SUCH
AS
SWITCH
FABRICS
TCP
OFFLOAD
ENGINES
AND
OFDM
MODULATION
CLOCKS
THIS
PAPER
STUDIES
THE
EFFICACY
OF
DYNAMIC
SPEED
SCALING
ANALYTICALLY
THE
GOAL
IS
TWOFOLD
I
TO
ELUCIDATE
THE
STRUCTURE
OF
THE
OPTIMAL
SPEED
SCALING
SCHEME
E
G
HOW
SHOULD
THE
SPEED
DEPEND
ON
THE
CURRENT
WORKLOAD
II
TO
COMPARE
THE
PERFORMANCE
OF
DYNAMIC
SPEED
SCALING
DESIGNS
WITH
THAT
OF
DESIGNS
THAT
USE
STATIC
PROCESSING
SPEEDS
E
G
HOW
MUCH
IMPROVEMENT
DOES
DYNAMIC
SPEED
SCALING
PROVIDE
THERE
ARE
MANY
ANALYTIC
STUDIES
OF
SPEED
SCALING
DESIGNS
BEGINNING
WITH
YAO
ET
AL
THE
FOCUS
HAS
BEEN
ON
EITHER
I
THE
GOAL
OF
MINIMIZING
THE
TOTAL
ENERGY
USED
IN
ORDER
TO
COMPLETE
ARRIVING
JOBS
BY
THEIR
DEADLINES
E
G
OR
II
THE
GOAL
OF
MINIMIZING
THE
AVERAGE
RESPONSE
TIME
OF
JOBS
I
E
THE
TIME
BETWEEN
THEIR
ARRIVAL
AND
THEIR
COMPLETION
OF
SERVICE
GIVEN
A
SET
ENERGY
HEAT
BUDGET
E
G
WEB
SETTINGS
TYPICALLY
HAVE
NEITHER
JOB
COMPLETION
DEADLINES
NOR
FIXED
ENERGY
BUDGETS
INSTEAD
THE
GOAL
IS
TO
OPTIMIZE
A
TRADEOFF
BETWEEN
ENERGY
CONSUMPTION
AND
MEAN
RESPONSE
TIME
THIS
MODEL
IS
THE
FOCUS
OF
THE
CURRENT
PAPER
IN
PARTICULAR
THE
PERFORMANCE
METRIC
CONSIDERED
IS
E
T
E
E
ΒT
WHERE
T
IS
THE
RESPONSE
TIME
OF
A
JOB
E
IS
THE
EXPECTED
ENERGY
EXPENDED
ON
THAT
JOB
AND
ΒT
CONTROLS
THE
RELATIVE
COST
OF
DELAY
THIS
PERFORMANCE
METRIC
HAS
ATTRACTED
ATTENTION
RECENTLY
THE
RELATED
ANALYTIC
WORK
FALLS
INTO
TWO
CATEGORIES
WORST
CASE
ANALYSES
AND
STOCHASTIC
ANALYSES
THE
FORMER
PROVIDE
SPECIFIC
SIMPLE
SPEED
SCALINGS
GUARANTEED
TO
BE
WITHIN
A
CONSTANT
FACTOR
OF
THE
OPTIMAL
PERFORMANCE
REGARDLESS
OF
THE
WORKLOAD
E
G
IN
CONTRAST
STOCHASTIC
RE
SULTS
HAVE
FOCUSED
ON
SERVICE
RATE
CONTROL
IN
THE
M
M
MODEL
UNDER
FIRST
COME
FIRST
SERVED
FCFS
SCHEDULING
WHICH
CAN
BE
SOLVED
NUMERICALLY
USING
DYNAMIC
PROGRAMMING
ONE
SUCH
APPROACH
IS
REVIEWED
IN
SECTION
III
C
UNFORTUNATELY
THE
STRUCTURAL
INSIGHT
OBTAINED
FROM
STOCHASTIC
MODELS
HAS
BEEN
LIMITED
OUR
WORK
EXTENDS
THE
STOCHASTIC
ANALYSIS
OF
DYNAMIC
SPEED
SCALING
WE
FOCUS
ON
THE
M
GI
QUEUE
UNDER
PROCESSOR
SHARING
PS
SCHEDULING
WHICH
SERVES
ALL
JOBS
CURRENTLY
IN
THE
SYSTEM
AT
EQUAL
RATES
WE
FOCUS
ON
PS
BECAUSE
IT
IS
A
TRACTABLE
MODEL
OF
CURRENT
SCHEDULING
POLICIES
IN
CPUS
WEB
SERVERS
ROUTERS
ETC
BASED
ON
THE
MODEL
SECTION
II
AND
THE
SPEED
SCALING
WE
CONSIDER
SECTION
III
OUR
ANALYSIS
MAKES
THREE
MAIN
CONTRIBUTIONS
WE
PROVIDE
BOUNDS
ON
THE
PERFORMANCE
OF
DYNAMIC
SPEED
SCALING
SECTION
IV
A
SURPRISINGLY
THESE
BOUNDS
SHOW
THAT
EVEN
AN
IDEALIZED
VERSION
OF
DYNAMIC
SPEED
SCALING
IM
PROVES
PERFORMANCE
ONLY
MARGINALLY
COMPARED
TO
A
SIMPLE
SCHEME
WHERE
THE
SERVER
USES
A
STATIC
SPEED
WHEN
BUSY
AND
SPEED
WHEN
IDLE
AT
MOST
A
FACTOR
OF
FOR
TYPICAL
PA
RAMETERS
AND
OFTEN
LESS
SEE
SECTION
V
COUNTERINTUITIVELY
THESE
BOUNDS
ALSO
SHOW
THAT
THE
POWER
OPTIMIZED
RESPONSE
TIME
REMAINS
BOUNDED
AS
THE
LOAD
GROWS
WE
PROVIDE
BOUNDS
AND
ASYMPTOTICS
FOR
THE
SPEEDS
USED
BY
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
SECTIONS
IV
B
AND
IV
C
THESE
RESULTS
PROVIDE
INSIGHT
INTO
HOW
THE
SPEEDS
SCALE
WITH
THE
ARRIVING
LOAD
THE
QUEUE
LENGTH
AND
THE
RELATIVE
COST
OF
ENERGY
FURTHER
THEY
UNCOVER
A
CONNECTION
BETWEEN
THE
OPTIMAL
STOCHASTIC
POLICY
AND
RESULTS
FROM
THE
WORST
CASE
COMMUNITY
SECTION
IV
WE
ILLUSTRATE
THROUGH
ANALYTIC
RESULTS
AND
NUMERICAL
EXPERI
MENTS
THAT
THOUGH
DYNAMIC
SPEED
SCALING
PROVIDES
LIMITED
PERFORMANCE
GAINS
IT
DRAMATICALLY
IMPROVES
ROBUSTNESS
TO
MIS
ESTIMATION
OF
WORKLOAD
PARAMETERS
AND
BURSTY
TRAFFIC
SECTION
VI
II
MODEL
AND
NOTATION
IN
ORDER
TO
STUDY
THE
PERFORMANCE
OF
DYNAMIC
SPEED
SCALING
WE
FOCUS
ON
A
SIMPLE
MODEL
AN
M
GI
PS
QUEUE
WITH
CONTROLLABLE
SERVICE
RATES
DEPENDENT
ON
THE
QUEUE
LENGTH
IN
THIS
MODEL
JOBS
ARRIVE
TO
THE
SERVER
AS
A
POISSON
PROCESS
WITH
RATE
Λ
HAVE
INTRINSIC
SIZES
WITH
MEAN
Μ
AND
DEPART
AT
RATE
SNΜ
WHEN
THERE
ARE
N
JOBS
IN
THE
SYSTEM
UNDER
STATIC
SCHEMES
THE
CONSTANT
SERVICE
RATE
IS
DENOTED
BY
DEFINE
THE
LOAD
AS
Ρ
Λ
Μ
AND
NOTE
THAT
THIS
Ρ
IS
NOT
THE
FRACTION
OF
TIME
THE
SERVER
IS
BUSY
THE
PERFORMANCE
METRIC
WE
CONSIDER
IS
E
T
E
E
ΒT
WHERE
T
IS
THE
RESPONSE
TIME
OF
A
JOB
AND
E
IS
THE
ENERGY
EXPENDED
ON
A
JOB
IT
IS
OFTEN
CONVENIENT
TO
WORK
WITH
THE
EXPECTED
COST
PER
UNIT
TIME
INSTEAD
OF
PER
JOB
BY
LITTLE
LAW
THIS
CAN
BE
WRITTEN
AS
Z
E
N
ΛE
F
ΒT
WHERE
N
IS
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
AND
F
DETERMINES
THE
POWER
USED
WHEN
RUNNING
AT
SPEED
THE
REMAINING
PIECE
OF
THE
MODEL
IS
TO
DEFINE
THE
FORM
OF
F
PRIOR
LITERATURE
WITH
THE
NOTABLE
EXCEPTION
OF
HAS
TYPICALLY
ASSUMED
THAT
F
IS
CONVEX
AND
OFTEN
THAT
F
IS
A
POLYNOMIAL
SPECIFICALLY
A
CUBIC
THAT
IS
BECAUSE
THE
DYNAMIC
POWER
OF
CMOS
IS
PROPORTIONAL
TO
V
WHERE
V
IS
THE
SUPPLY
VOLTAGE
AND
F
IS
THE
CLOCK
FREQUENCY
OPERATING
AT
A
HIGHER
FREQUENCY
REQUIRES
DYNAMIC
VOLTAGE
SCALING
DVS
TO
A
HIGHER
VOLTAGE
NOMINALLY
WITH
V
F
YIELDING
A
CUBIC
RELATIONSHIP
TO
VALIDATE
THE
POLYNOMIAL
FORM
OF
F
WE
CONSIDER
DATA
FROM
REAL
NM
CHIPS
IN
FIG
THE
VOLTAGE
VERSUS
SPEED
DATA
COMES
FROM
THE
INTEL
PXA
PENTIUM
M
PRO
CESSOR
AND
THE
TCP
OFFLOAD
ENGINE
STUDIED
IN
SPECIFICALLY
THE
NBB
TRACE
AT
C
IN
FIG
INTERESTINGLY
THE
DYNAMIC
POWER
USE
OF
REAL
CHIPS
IS
WELL
MODELED
BY
A
POLYNOMIAL
SCALING
OF
SPEED
TO
POWER
BUT
THIS
POLYNOMIAL
IS
FAR
FROM
CUBIC
IN
FACT
IT
IS
CLOSER
TO
QUADRATIC
INDICATING
THAT
THE
VOLTAGE
IS
SCALED
DOWN
LESS
AGGRESSIVELY
THAN
LINEARLY
WITH
SPEED
AS
A
RESULT
WE
WILL
MODEL
THE
POWER
USED
BY
RUNNING
LOG
FREQ
FIG
DYNAMIC
POWER
FOR
AN
INTEL
PXA
A
TCP
OFFLOAD
ENGINE
AND
A
PENTIUM
M
THE
SLOPES
OF
THE
FITTED
LINES
ARE
AND
RESPECTIVELY
THE
SCOPE
OF
THIS
PAPER
AND
WE
LEAVE
THE
QUESTION
OF
INCLUDING
BOTH
LEAKAGE
AND
DYNAMIC
POWER
FOR
FUTURE
WORK
III
POWER
AWARE
SPEED
SELECTION
WHEN
PROVISIONING
PROCESSING
SPEED
IN
A
POWER
AWARE
MANNER
THERE
ARE
THREE
NATURAL
THRESHOLDS
IN
THE
CAPABILITY
OF
THE
SERVER
I
STATIC
PROVISIONING
THE
SERVER
USES
A
CONSTANT
STATIC
SPEED
WHICH
IS
DETERMINED
BASED
ON
WORKLOAD
CHARAC
TERISTICS
SO
AS
TO
BALANCE
ENERGY
USE
AND
RESPONSE
TIME
II
GATED
STATIC
PROVISIONING
THE
SERVER
GATES
ITS
CLOCK
SETTING
IF
NO
JOBS
ARE
PRESENT
AND
IF
JOBS
ARE
PRESENT
IT
WORKS
AT
A
CONSTANT
RATE
CHOSEN
TO
BALANCE
ENERGY
USE
AND
RESPONSE
TIME
III
DYNAMIC
SPEED
SCALING
THE
SERVER
ADAPTS
ITS
SPEED
TO
THE
CURRENT
NUMBER
OF
REQUESTS
PRESENT
IN
THE
SYSTEM
THE
GOAL
OF
THIS
PAPER
IS
TO
UNDERSTAND
HOW
TO
CHOOSE
OPTIMAL
SPEEDS
IN
EACH
OF
THESE
SCENARIOS
AND
TO
CONTRAST
THE
RELATIVE
MERITS
OF
EACH
SCHEME
CLEARLY
THE
EXPECTED
COST
IS
REDUCED
EACH
TIME
THE
SERVER
IS
ALLOWED
TO
ADJUST
ITS
SPEED
MORE
DYNAMICALLY
THIS
MUST
BE
TRADED
AGAINST
THE
COSTS
OF
SWITCHING
SUCH
AS
A
DELAY
OF
UP
TO
TENS
OF
MICROSECONDS
TO
CHANGE
SPEEDS
THE
IMPORTANT
QUESTION
IS
WHAT
IS
THE
MAGNITUDE
OF
IMPROVEMENT
AT
EACH
LEVEL
FOR
OUR
COMPARISON
AT
SPEED
BY
Λ
F
ΒT
SΑ
Β
WE
WILL
USE
IDEALIZED
VERSIONS
OF
EACH
SCHEME
IN
PARTICULAR
IN
EACH
CASE
WE
WILL
ASSUME
THAT
THE
SERVER
CAN
BE
RUN
AT
ANY
DESIRED
SPEED
IN
AND
IGNORE
SWITCHING
COSTS
WHERE
Α
AND
Β
TAKES
THE
ROLE
OF
ΒT
BUT
HAS
DIMENSION
TIME
Α
THE
COST
PER
UNIT
TIME
THEN
BECOMES
SΑ
THUS
IN
PARTICULAR
THE
DYNAMIC
SPEED
SCALING
IS
A
SIGNIFICANT
IDEALIZATION
OF
WHAT
IS
POSSIBLE
IN
PRACTICE
HOWEVER
OUR
RESULTS
WILL
SUGGEST
THAT
IT
PROVIDES
VERY
LITTLE
PERFORMANCE
Z
E
N
Β
IMPROVEMENT
OVER
AN
IDEALLY
TUNED
GATED
STATIC
SCHEME
IN
THIS
SECTION
WE
WILL
DERIVE
EXPRESSIONS
FOR
THE
OPTIMAL
WE
WILL
OFTEN
FOCUS
ON
THE
CASE
OF
Α
TO
PROVIDE
INTUITION
CLEARLY
THIS
IS
AN
IDEALIZED
MODEL
SINCE
IN
REALITY
ONLY
A
FEW
DISCRETE
SPEEDS
CAN
BE
USED
THE
IMPACT
OF
THE
WORKLOAD
PARAMETERS
Ρ
Β
AND
Α
CAN
OFTEN
BE
CAPTURED
USING
ONE
SIMPLE
PARAMETER
Γ
Ρ
Α
WHICH
IS
A
DIMENSIONLESS
MEASURE
THUS
WE
WILL
STATE
OUR
RESULTS
IN
TERMS
OF
Γ
TO
SIMPLIFY
THEIR
FORM
ALSO
IT
WILL
OFTEN
BE
CONVENIENT
TO
USE
THE
THE
DIMENSIONLESS
UNIT
OF
SPEED
Α
THOUGH
WE
FOCUS
ON
DYNAMIC
POWER
IN
THIS
PAPER
IT
SHOULD
BE
NOTED
THAT
LEAKAGE
POWER
IS
INCREASINGLY
IMPORTANT
IT
REPRESENTS
OF
THE
POWER
USE
OF
CURRENT
AND
NEAR
FUTURE
CHIPS
HOWEVER
ANALYTIC
MODELS
FOR
LEAKAGE
ARE
MUCH
LESS
UNDERSTOOD
AND
SO
INCLUDING
LEAKAGE
IN
OUR
ANALYSIS
IS
BEYOND
SPEEDS
IN
CASES
I
AND
II
FOR
CASE
III
WE
WILL
DESCRIBE
A
NUMERICAL
APPROACH
FOR
CALCULATING
THE
OPTIMAL
SPEEDS
WHICH
IS
DUE
TO
GEORGE
AND
HARRISON
THOUGH
THIS
NUMERICAL
APPROACH
IS
EFFICIENT
IT
PROVIDES
LITTLE
STRUCTURAL
INSIGHT
INTO
THE
STRUCTURE
OF
THE
DYNAMIC
SPEEDS
OR
THE
OVERALL
PERFORMANCE
PROVIDING
SUCH
RESULTS
WILL
BE
THE
FOCUS
OF
SECTION
IV
A
THE
OPTIMAL
STATIC
SPEED
THE
SIMPLEST
SYSTEM
TO
MANAGE
POWER
IS
ONE
WHICH
SELECTS
AN
OPTIMAL
SPEED
AND
THEN
ALWAYS
RUNS
THE
PROCESSOR
AT
THAT
SPEED
THIS
CASE
WHICH
WE
CALL
PURE
STATIC
IS
THE
LEAST
POWER
AWARE
SCENARIO
WE
CONSIDER
AND
WILL
BE
USED
SIMPLY
AS
A
BENCHMARK
FOR
COMPARISON
EVEN
WHEN
THE
SPEED
IS
STATIC
THE
OPTIMAL
DESIGN
CAN
BE
POWER
AWARE
SINCE
THE
OPTIMAL
SPEED
CAN
BE
CHOSEN
SO
THAT
IT
TRADES
OFF
THE
COST
OF
RESPONSE
TIME
AND
ENERGY
APPROPRIATELY
IN
PARTICULAR
WE
CAN
WRITE
THE
COST
PER
UNIT
TIME
AS
Z
Ρ
Β
THEN
DIFFERENTIATING
AND
SOLVING
FOR
THE
MINIMIZER
GIVES
THAT
THE
OPTIMUM
OCCURS
WHEN
Ρ
AND
SΑ
Ρ
ΒΡ
Α
B
THE
OPTIMAL
STATIC
SPEED
FOR
A
GATED
SYSTEM
THE
NEXT
SIMPLEST
SYSTEM
IS
WHEN
THE
PROCESSOR
IS
ALLOWED
TWO
STATES
HALTED
OR
PROCESSING
WE
MODEL
THIS
SITUATION
WITH
A
SERVER
THAT
RUNS
AT
A
CONSTANT
RATE
EXCEPT
WHEN
THERE
ARE
NO
JOBS
IN
THE
SYSTEM
AT
WHICH
POINT
IT
SETS
USING
ZERO
DYNAMIC
POWER
TO
DETERMINE
THE
OPTIMAL
STATIC
SPEED
WE
PROCEED
AS
WE
DID
IN
THE
PREVIOUS
SECTION
IF
THE
SERVER
CAN
GATE
ITS
CLOCK
THE
ENERGY
COST
IS
ONLY
INCURRED
DURING
THE
FRACTION
OF
TIME
THE
SERVER
IS
BUSY
Ρ
THE
COST
PER
UNIT
TIME
THEN
BECOMES
C
OPTIMAL
DYNAMIC
SPEED
SCALING
A
POPULAR
ALTERNATIVE
TO
STATIC
POWER
MANAGEMENT
IS
TO
ALLOW
THE
SPEED
TO
ADJUST
DYNAMICALLY
TO
THE
NUMBER
OF
REQUESTS
IN
THE
SYSTEM
THE
TASK
OF
DESIGNING
AN
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
IN
OUR
MODEL
CAN
BE
VIEWED
AS
A
STOCHASTIC
CONTROL
PROBLEM
WE
START
WITH
THE
FOLLOWING
OBSERVATION
WHICH
SIMPLIFIES
THE
PROBLEM
DRAMATICALLY
AN
M
GI
PS
SYSTEM
IS
WELL
KNOWN
TO
BE
INSENSITIVE
TO
THE
JOB
SIZE
DISTRIBUTION
THIS
STILL
HOLDS
WHEN
THE
SERVICE
RATE
IS
QUEUE
LENGTH
DEPENDENT
SINCE
THE
POLICY
STILL
FALLS
INTO
THE
CLASS
OF
SYMMETRIC
POLICIES
INTRODUCED
BY
KELLY
AS
A
RESULT
THE
MEAN
RESPONSE
TIME
AND
ENTIRE
QUEUE
LENGTH
DISTRIBUTION
ARE
AFFECTED
BY
THE
SERVICE
DISTRIBUTION
THROUGH
ONLY
ITS
MEAN
THUS
WE
CAN
CONSIDER
AN
M
M
PS
SYSTEM
FURTHER
THE
MEAN
RESPONSE
TIME
AND
ENTIRE
QUEUE
LENGTH
DISTRIBUTION
ARE
EQUIVALENT
UNDER
ALL
NON
SIZE
BASED
SERVICE
DISTRIBUTIONS
IN
THE
M
M
QUEUE
THUS
TO
DETERMINE
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
FOR
AN
M
GI
PS
QUEUE
WE
NEED
ONLY
CONSIDER
AN
M
M
FCFS
QUEUE
THE
SERVICE
RATE
CONTROL
PROBLEM
IN
THE
M
M
FCFS
Z
Ρ
Ρ
SΑ
Β
QUEUE
HAS
BEEN
STUDIED
EXTENSIVELY
IN
PARTIC
ULAR
GEORGE
AND
HARRISON
PROVIDE
AN
ELEGANT
SOLUTION
TO
THE
PROBLEM
OF
SELECTING
THE
STATE
DEPENDENT
PROCESSING
SPEEDS
THE
OPTIMUM
OCCURS
WHEN
Ρ
AND
TO
MINIMIZE
A
WEIGHTED
SUM
OF
AN
ARBITRARY
HOLDING
COST
WITH
A
PROCESSING
SPEED
COST
SPECIFICALLY
THE
OPTIMAL
STATE
DZ
DS
Ρ
Ρ
Α
SΑ
Ρ
Β
DEPENDENT
PROCESSING
SPEEDS
CAN
BE
FRAMED
AS
THE
SOLUTION
TO
A
STOCHASTIC
DYNAMIC
PROGRAM
TO
WHICH
PROVIDES
AN
EFFICIENT
NUMERICAL
SOLUTION
IN
THE
REMAINDER
OF
THIS
SECTION
WHICH
IS
SOLVED
WHEN
Α
SΑ
Ρ
Β
THE
OPTIMAL
SPEED
CAN
BE
SOLVED
FOR
EXPLICITLY
FOR
SOME
Α
FOR
EXAMPLE
WHEN
Α
SGS
Ρ
Β
IN
GENERAL
DEFINE
G
Γ
Α
Σ
T
Σ
Γ
Α
ΣΑ
Γ
Σ
WITH
THIS
NOTATION
THE
OPTIMAL
STATIC
SPEED
FOR
A
SERVER
WHICH
GATES
ITS
CLOCK
IS
SGS
ΑG
Γ
Α
WE
CALL
THIS
POLICY
THE
WE
WILL
PROVIDE
AN
OVERVIEW
OF
THIS
NUMERICAL
APPROACH
THE
CORE
OF
THIS
APPROACH
WILL
FORM
THE
BASIS
OF
OUR
DERIVATION
OF
BOUNDS
ON
THE
OPTIMAL
SPEEDS
IN
SECTION
IV
WE
WILL
DESCRIBE
THE
ALGORITHM
OF
SPECIALIZED
TO
THE
CASE
CONSIDERED
IN
THIS
PAPER
WHERE
THE
HOLDING
COST
IN
STATE
N
IS
SIMPLY
N
FURTHER
WE
WILL
GENERALIZE
THE
DESCRIPTION
TO
ALLOW
ARBITRARY
ARRIVAL
RATES
Λ
THE
SOLUTION
STARTS
WITH
AN
ESTIMATE
Z
OF
THE
MINIMAL
COST
PER
UNIT
TIME
INCLUDING
BOTH
THE
OCCUPANCY
COST
AND
THE
ENERGY
COST
AS
IN
THE
MINIMUM
COST
OF
RETURNING
FROM
STATE
N
TO
THE
EMPTY
SYSTEM
IS
GIVEN
BY
THE
DYNAMIC
PROGRAM
GATED
STATIC
POLICY
AND
DENOTE
THE
CORRESPONDING
COST
ZGS
V
INF
Λ
F
N
ZL
THE
FOLLOWING
LEMMA
BOUNDS
G
THE
PROOF
IS
DEFERRED
TO
APPENDIX
A
LEMMA
FOR
Α
N
A
Λ
ΜS
ΒT
ΜS
Λ
ΜS
VN
Λ
Λ
ΜS
VN
I
Α
Α
WHERE
A
IS
THE
SET
OF
AVAILABLE
SPEEDS
WE
WILL
USUALLY
ASSUME
Γ
Α
G
Γ
Α
Α
Γ
Α
A
WITH
THE
SUBSTITUTION
UN
Λ
VN
VN
THIS
CAN
BE
WRITTEN
AS
AND
THE
INEQUALITIES
ARE
REVERSED
FOR
Α
NOTE
THAT
THE
FIRST
INEQUALITY
BECOMES
TIGHT
FOR
ΓΑ
AND
U
SUP
Z
N
Λ
F
SUN
THE
SECOND
BECOMES
TIGHT
FOR
ΓΑ
FURTHER
WHEN
Α
N
A
ΒT
Ρ
BOTH
BECOME
EQUALITIES
GIVING
G
Γ
Γ
AND
AND
THE
CONDITIONS
ON
Α
HERE
HAVE
BEEN
CORRECTED
SINCE
THE
VERSION
PRESENTED
AT
INFOCOM
TWO
ADDITIONAL
FUNCTIONS
ARE
DEFINED
FIRST
Φ
U
SUP
UX
Ρ
ΛF
X
ΒT
X
A
SECOND
THE
MINIMUM
VALUE
OF
X
WHICH
ACHIEVES
THIS
SUPRE
MUM
NORMALIZED
TO
BE
DIMENSIONLESS
IS
Ψ
U
Β
Α
MIN
X
UX
Ρ
ΛF
X
ΒT
Φ
U
NOTE
THAT
UNDER
IS
CONSTANT
COMPETITIVE
I
E
IN
THE
WORST
CASE
THE
TOTAL
COST
IS
WITHIN
A
CONSTANT
OF
OPTIMAL
THIS
MATCHES
THE
ASYMPTOTIC
BEHAVIOR
OF
THE
BOUNDS
FOR
Α
FOR
LARGE
N
THIS
BEHAVIOR
CAN
ALSO
BE
OBSERVED
FOR
GENERAL
Α
LEMMA
AND
THEOREM
A
BOUNDS
ON
COST
Φ
U
Α
U
Α
Α
ΑΓ
Ψ
U
U
Α
ΑΓ
WE
START
THE
ANALYSIS
BY
PROVIDING
BOUNDS
ON
Z
IN
THIS
SUBSECTION
AND
THEN
USING
THE
BOUNDS
ON
Z
TO
BOUND
N
ABOVE
GIVEN
THE
ESTIMATE
OF
Z
UN
SATISFY
AND
BELOW
SECTIONS
IV
B
AND
IV
C
RECALL
THAT
ZGS
IS
THE
TOTAL
COST
UNDER
GATED
STATIC
Z
UN
Φ
UN
N
Z
THEOREM
MAX
ΓΑ
ΓΑ
Α
Α
THE
OPTIMAL
VALUE
OF
Z
CAN
BE
FOUND
AS
THE
MINIMUM
VALUE
SUCH
THAT
UN
N
IS
AN
INCREASING
SEQUENCE
THIS
ALLOWS
Z
TO
Z
ZGS
Γ
G
Γ
Α
Γ
ΓG
Γ
Α
Α
BE
FOUND
BY
AN
EFFICIENT
BINARY
SEARCH
AFTER
WHICH
UN
CAN
IN
PRINCIPLE
BE
FOUND
RECURSIVELY
THE
OPTIMAL
SPEED
IN
STATE
N
IS
THEN
GIVEN
BY
PROOF
THE
OPTIMAL
COST
Z
IS
BOUNDED
ABOVE
BY
THE
COST
OF
THE
GATED
STATIC
POLICY
WHICH
IS
SIMPLY
N
Ψ
U
Α
N
ZGS
Γ
G
Γ
Α
Γ
ΓG
Γ
Α
Α
THIS
HIGHLIGHTS
THE
FACT
THAT
Γ
Ρ
Α
PROVIDES
THE
APPRO
PRIATE
SCALING
OF
THE
WORKLOAD
INFORMATION
BECAUSE
THE
COST
Z
NORMALIZED
SPEED
SΒ
Α
AND
VARIABLES
UN
DEPEND
ON
Λ
Μ
TWO
LOWER
BOUNDS
CAN
BE
OBTAINED
AS
FOLLOWS
IN
ORDER
TO
MAINTAIN
STABILITY
THE
TIME
AVERAGE
SPEED
MUST
SATISFY
E
Ρ
BUT
Z
E
SΑ
Β
E
Α
Β
BY
JENSEN
INEQUALITY
AND
THE
CONVEXITY
OF
Α
THUS
AND
Β
ONLY
THROUGH
Γ
NOTE
THAT
THIS
FORWARD
APPROACH
ADVOCATED
IN
IS
NUMERICALLY
UNSTABLE
APPENDIX
B
WE
SUGGEST
THAT
A
MORE
E
SΑ
ΡΑ
Z
Β
Β
ΓΑ
STABLE
WAY
TO
CALCULATE
UN
IS
TO
START
WITH
A
GUESS
FOR
LARGE
N
AND
WORK
BACKWARDS
ERRORS
IN
THE
INITIAL
GUESS
DECAY
EXPONENTIALLY
AS
N
DECREASES
AND
ARE
MUCH
SMALLER
THAN
THE
ACCUMULATED
ROUNDOFF
ERRORS
OF
THE
FORWARD
APPROACH
THIS
FOR
SMALL
LOADS
THIS
BOUND
IS
QUITE
LOOSE
ANOTHER
BOUND
COMES
FROM
CONSIDERING
THE
MINIMUM
COST
OF
PROCESSING
A
SINGLE
JOB
OF
SIZE
X
WITH
NO
WAITING
TIME
OR
PROCESSOR
SHARING
IT
IS
OPTIMAL
TO
SERVE
THE
JOB
AT
A
CONSTANT
RATE
THUS
BACKWARD
APPROACH
IS
MADE
POSSIBLE
BY
THE
BOUNDS
WE
DERIVE
IN
SECTION
IV
Z
EX
MIN
X
SΑ
X
L
IV
BOUNDS
ON
OPTIMAL
DYNAMIC
SPEED
SCALING
IN
THE
PRIOR
SECTION
WE
PRESENTED
THE
OPTIMAL
DESIGNS
FOR
THE
CASES
OF
STATIC
GATED
STATIC
AND
DYNAMIC
SPEED
SCALING
IN
THE
FIRST
TWO
CASES
THE
OPTIMAL
SPEEDS
WERE
PRESENTED
MORE
OR
LESS
EXPLICITLY
HOWEVER
IN
THE
THIRD
CASE
WE
PRESENTED
ONLY
A
RECURSIVE
NUMERICAL
ALGORITHM
FOR
DETERMINING
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
EVEN
THOUGH
THIS
APPROACH
PROVIDES
AN
EFFICIENT
MEANS
TO
CALCULATE
N
IT
IS
DIFFICULT
TO
GAIN
INSIGHT
INTO
SYSTEM
DESIGN
IN
THIS
SECTION
WE
PROVIDE
RESULTS
EXHIBITING
THE
STRUCTURE
OF
THE
OPTIMAL
DYNAMIC
SPEEDS
AND
THE
PERFORMANCE
THEY
ACHIEVE
THE
MAIN
RESULTS
OF
THIS
SECTION
ARE
SUMMARIZED
IN
TABLE
I
THE
BOUNDS
ON
Z
FOR
ARBITRARY
Α
ARE
ESSENTIALLY
TIGHT
I
E
AGREE
TO
LEADING
ORDER
IN
THE
LIMITS
OF
SMALL
OR
LARGE
Γ
DUE
THE
RIGHT
HAND
SIDE
IS
MINIMIZED
FOR
Β
Α
Α
INDEPENDENT
OF
X
GIVING
Z
ΡΒ
ΑΑ
Α
Α
THUS
Z
MAX
ΓΑ
ΓΑ
Α
Α
THE
FORM
OF
THE
BOUNDS
ON
Z
ARE
COMPLICATED
SO
IT
IS
USEFUL
TO
LOOK
AT
THE
PARTICULAR
CASE
OF
Α
COROLLARY
FOR
Α
GATED
STATIC
HAS
COST
WITHIN
A
FACTOR
OF
OF
OPTIMAL
SPECIFICALLY
MAX
Z
ZGS
PROOF
FOR
Α
G
Γ
Γ
HENCE
GIVES
Γ
TO
THE
COMPLICATED
FORM
OF
THE
GENERAL
RESULTS
WE
ILLUSTRATE
THE
BOUNDS
FOR
THE
SPECIFIC
CASE
OF
Α
TO
PROVIDE
INSIGHT
IN
ZGS
Γ
Γ
Γ
Γ
PARTICULAR
IT
IS
EASY
TO
SEE
THE
BEHAVIOR
OF
SN
AND
Z
AS
A
FUNC
TION
OF
Γ
AND
N
IN
THE
CASE
OF
Α
THIS
LEADS
TO
INTERESTING
OBSERVATIONS
FOR
EXAMPLE
IT
ILLUSTRATES
A
CONNECTION
BETWEEN
THE
OPTIMAL
STOCHASTIC
POLICY
AND
POLICIES
ANALYZED
IN
THE
WORST
CASE
MODEL
IN
PARTICULAR
BANSAL
PRUHS
AND
STEIN
SHOWED
THAT
WHEN
NOTHING
IS
KNOWN
ABOUT
FUTURE
ARRIVALS
A
POLICY
THAT
GIVES
SPEEDS
OF
THE
FORM
SN
N
Α
Α
WHICH
ESTABLISHES
THE
UPPER
BOUND
THE
LOWER
BOUND
FOLLOWS
FROM
SUBSTITUTING
Α
INTO
Z
MAX
THE
RATIO
OF
ZGS
TO
THE
LOWER
BOUND
ON
Z
HAS
A
MAXIMUM
VALUE
OF
AT
Γ
AND
HENCE
GATED
STATIC
IS
WITHIN
A
FACTOR
OF
OF
THE
TRUE
OPTIMAL
SCHEME
TABLE
I
BOUNDS
ON
TOTAL
COSTS
AND
SPEED
AS
A
FUNCTION
OF
THE
NUMBER
N
OF
JOBS
IN
THE
SYSTEM
FOR
ANY
Α
MAX
ΓΑ
ΓΑ
Α
Α
Z
Γ
ΓG
Γ
Α
Α
THEOREM
G
Γ
Α
Γ
N
N
ΣΑ
ΓΑ
Γ
Α
WHERE
ΣN
SATISFIES
ΣΑ
Α
ΣN
ΑΓ
N
Γ
G
Γ
Α
Γ
ΓG
Γ
Α
Α
FOR
Α
MAX
Z
COROLLARY
P
N
Γ
Γ
FOR
Α
AND
N
A
LOWER
BOUND
ON
SN
RESULTS
FROM
LINEAR
INTERPOLATION
BETWEEN
MAX
Γ
AT
N
AND
Γ
AT
N
IT
IS
PERHAPS
SURPRISING
THAT
SUCH
AN
IDEALIZED
VERSION
OF
DYNAMIC
SPEED
SCALING
PROVIDES
SUCH
A
SMALL
MAGNITUDE
OF
IMPROVEMENT
OVER
A
SIMPLISTIC
POLICY
SUCH
AS
GATED
STATIC
IN
FACT
THE
BOUND
OF
IS
VERY
LOOSE
WHEN
Γ
IS
LARGE
OR
SMALL
FURTHER
EMPIRICALLY
THE
MAXIMUM
RATIOS
FOR
TYPICAL
Α
ARE
BELOW
SEE
FIG
THUS
THERE
IS
LITTLE
TO
BE
GAINED
BY
UNROLLING
THE
DYNAMIC
PROGRAM
GIVES
A
JOINT
MINIMIZATION
OVER
ALL
SN
U
Ρ
MIN
SΑ
Β
N
Z
SN
SN
Ρ
MIN
SΑ
Β
N
Z
U
L
DYNAMIC
SCALING
IN
TERMS
OF
MEAN
COST
HOWEVER
SECTION
VI
SHOWS
THAT
DYNAMIC
SCALING
DRAMATICALLY
IMPROVES
ROBUSTNESS
SN
SN
TTI
N
Ρ
N
REMAINS
BOUNDED
AS
THE
ARRIVAL
RATE
Λ
GROWS
SPECIFICALLY
BY
AN
UPPER
BOUND
CAN
BE
FOUND
BY
TAKING
ANY
POSSIBLY
SUBOPTIMAL
CHOICE
OF
SN
I
FOR
I
AND
BOUNDING
THE
Z
E
Β
OPTIMAL
Z
TAKING
SI
Α
FOR
ALL
I
N
GIVES
E
T
Λ
Λ
Μ
Β
MIN
N
ΣΑ
Z
Γ
L
B
UPPER
BOUNDS
ON
THE
OPTIMAL
DYNAMIC
SPEEDS
Γ
Σ
O
Γ
Σ
Γ
WE
NOW
MOVE
TO
PROVIDING
UPPER
BOUNDS
ON
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
THEOREM
FOR
ALL
N
AND
Α
SINCE
Z
ΓΑ
FROM
EQUATION
FOLLOWS
WITH
THIS
ESTABLISHES
FOR
N
HOLDS
SINCE
OTHERWISE
IT
FOLLOWS
FROM
THE
INEQUALITY
ΣΑ
N
ΓN
Α
Α
N
Γ
Α
AND
THE
FACT
THAT
N
Α
N
ΣΑ
ΓΑ
BY
SPECIALIZING
TO
THE
CASE
WHEN
Α
WE
CAN
PROVIDE
UN
Γ
FOR
ALL
Σ
WHENCE
O
Γ
Σ
Γ
SOME
INTUITION
FOR
THE
UPPER
BOUND
ON
THE
SPEEDS
COROLLARY
FOR
Α
N
Γ
Γ
Α
Α
Σ
Γ
O
Γ
Σ
Γ
PROOF
FACTORING
THE
DIFFERENCE
OF
SQUARES
IN
THE
FIRST
TERM
IN
PARTICULAR
FOR
Σ
Γ
Α
OF
AND
CANCELING
WITH
THE
DENOMINATOR
YIELDS
UN
N
Α
Α
Γ
Γ
Α
ΓN
UN
Σ
Γ
Γ
Σ
Γ
Σ
Γ
WHICH
IS
CONCAVE
IN
N
PROOF
AS
EXPLAINED
IN
CAN
BE
REWRITTEN
AS
ONE
TERM
OF
IS
INCREASING
IN
Σ
AND
TWO
ARE
DECREASING
MINIMIZING
PAIRS
OF
THESE
TERMS
GIVES
UPPER
BOUNDS
ON
UN
A
FIRST
BOUND
CAN
BE
OBTAINED
BY
SETTING
Σ
Γ
N
WHICH
MINIMIZES
THE
SUM
OF
THE
FIRST
TWO
TERMS
AND
GIVES
UN
MIN
SΑ
Β
N
UN
Z
L
U
N
BY
THIS
GIVES
A
BOUND
ON
THE
OPTIMAL
SPEEDS
OF
THEOREM
THE
SCALED
SPEED
ΣN
N
Α
SATISFIES
N
Γ
ΣΑ
Α
Σ
ΑΓ
N
Γ
ΓG
Γ
Α
Α
A
SECOND
BOUND
COMES
BY
MINIMIZING
THE
SUM
OF
THE
SECOND
AND
THIRD
TERMS
WHEN
Σ
Γ
THIS
GIVES
PROOF
NOTE
THAT
UN
UN
THUS
BY
Α
UN
U
N
Z
Α
Α
ΓN
UN
Γ
Γ
ΑΓ
Α
Α
N
BY
THIS
CAN
BE
EXPRESSED
IN
TERMS
OF
N
AS
WHICH
UPON
DIVISION
BY
GIVES
ΑΓ
N
Α
Α
N
Α
N
Z
N
Γ
Α
Β
THE
MINIMUM
OF
THE
RIGHT
HAND
SIDES
OF
AND
IS
A
BOUND
ON
SN
N
Α
Α
Α
N
Α
ΑΓ
N
Z
THE
RESULT
THEN
FOLLOWS
FROM
THE
FACT
THAT
AND
THE
RESULT
FOLLOWS
FROM
SINCE
Z
Z
Γ
Γ
N
FOR
Α
GS
THE
ABOVE
THEOREM
CAN
BE
EXPRESSED
MORE
N
WHICH
FOLLOWS
FROM
TAKING
THE
SQUARE
ROOT
OF
THE
FIRST
INEQUAL
EXPLICITLY
AS
FOLLOWS
COROLLARY
FOR
Α
AND
ANY
N
ITY
AND
REARRANGING
FACTORS
C
LOWER
BOUNDS
ON
THE
OPTIMAL
DYNAMIC
SPEEDS
N
Α
Γ
N
FINALLY
WE
PROVE
LOWER
BOUNDS
ON
THE
DYNAMIC
SPEED
SCALING
SCHEME
WE
BEGIN
BY
BOUNDING
THE
SPEED
USED
WHEN
THERE
IS
ONE
JOB
IN
THE
SYSTEM
THE
FOLLOWING
RESULT
IS
AN
IMMEDIATE
CONSEQUENCE
OF
COROLLARY
AND
PROOF
FOR
Α
CAN
BE
SOLVED
EXPLICITLY
GIVING
UN
N
Z
SINCE
UN
BY
COROLLARY
FOR
Α
N
Γ
N
Z
Γ
Γ
Α
MAX
Β
AND
SUBSTITUTING
Z
FROM
GIVES
THE
RESULT
OBSERVE
THAT
THE
BOUNDS
IN
LIKE
THOSE
IN
COROLLARY
ARE
ESSENTIALLY
TIGHT
FOR
BOTH
LARGE
AND
SMALL
Γ
BUT
LOOSE
FOR
Γ
NEAR
ESPECIALLY
THE
LOWER
BOUND
NEXT
WE
WILL
PROVE
A
BOUND
ON
N
FOR
LARGE
N
LEMMA
FOR
SUFFICIENTLY
LARGE
N
THERE
ARE
TWO
IMPORTANT
OBSERVATIONS
ABOUT
THE
ABOVE
COROLLARY
FIRST
THE
COROLLARY
ONLY
APPLIES
WHEN
Ρ
AND
HENCE
AFTER
THE
MODE
OF
THE
DISTRIBUTION
HOWEVER
IT
ALSO
PROVES
THAT
THE
MODE
OCCURS
AT
N
SECOND
THE
COROLLARY
ONLY
APPLIES
WHEN
N
IN
THIS
CASE
WE
CAN
SIMPLIFY
THE
UPPER
BOUND
ON
SN
IN
AND
COMBINE
IT
WITH
TO
OBTAIN
N
Α
PROOF
REARRANGE
AS
UN
N
Z
UN
Α
Α
N
Α
Α
WHEN
THIS
FORM
HOLDS
IT
IS
TIGHT
FOR
LARGE
N
AND
OR
LARGE
Γ
FINALLY
NOTE
THAT
IN
THE
CASE
WHEN
N
THE
ONLY
BOUNDS
WE
HAVE
ON
THE
OPTIMAL
SPEEDS
ARE
WHERE
THE
INEQUALITY
USES
THE
FACT
THAT
THE
UN
IS
NON
DECREASING
HENCE
UNBOUNDED
AS
N
IS
UNBOUNDED
WHENCE
UN
Z
FOR
LARGE
N
APPLYING
N
Α
UN
ΑΓ
Α
GIVES
THIS
RESULT
HIGHLIGHTS
THE
CONNECTION
BETWEEN
THE
OPTIMAL
STOCHASTIC
POLICY
AND
PRIOR
POLICIES
ANALYZED
IN
THE
WORST
CASE
MODEL
THAT
WE
MENTIONED
AT
THE
BEGINNING
OF
THIS
SECTION
SPECIFICALLY
COMBINING
WITH
AND
SHOWS
THAT
SPEEDS
CHOSEN
TO
PERFORM
WELL
IN
THE
WORST
CASE
ARE
ASYMPTOT
ICALLY
OPTIMAL
FOR
LARGE
N
IN
THE
STOCHASTIC
MODEL
HOWEVER
NOTE
THAT
THE
PROBABILITY
OF
N
BEING
LARGE
IS
SMALL
NEXT
WE
CAN
DERIVE
A
TIGHTER
ALBEIT
IMPLICIT
BOUND
ON
THE
OPTIMAL
SPEEDS
THAT
N
IS
INCREASING
IN
N
THE
FOLLOWING
LEMMA
PROVES
THAT
AN
IMPROVED
LOWER
BOUND
CAN
BE
ATTAINED
BY
INTERPOLATING
LINEARLY
BETWEEN
MAX
Γ
AND
Γ
LEMMA
THE
SEQUENCE
UN
IS
STRICTLY
CONCAVE
INCREASING
PROOF
LET
P
N
BE
THE
PROPOSITION
UN
UN
UN
UN
STRICT
CONCAVITY
OF
UN
IS
EQUIVALENT
TO
THERE
BEING
NO
N
FOR
WHICH
P
N
HOLDS
SINCE
UN
IS
NON
DECREASING
AND
THERE
EXISTS
AN
UPPER
BOUND
ON
UN
WITH
GRADIENT
TENDING
TO
IT
IS
SUFFICIENT
TO
SHOW
THAT
P
N
IMPLIES
P
N
IF
SO
THEN
ANY
LOCAL
NON
CONCAVITY
WOULD
IMPLY
CONVEXITY
FROM
THAT
POINT
ONWARDS
IN
WHICH
CASE
ITS
LONG
TERM
GRADIENT
WOULD
BE
POSITIVE
AND
BOUNDED
AWAY
FROM
ZERO
AND
HENCE
UN
WOULD
EVENTUALLY
VIOLATE
THE
UPPER
BOUND
BY
U
IDENTITY
N
UN
Φ
UN
Φ
U
N
WITH
THIS
P
N
IS
EQUIVALENT
TO
Φ
UN
Φ
UN
UN
UN
THIS
IMPLIES
UN
UN
AND
Φ
UN
Φ
UN
A
ABSOLUTE
COSTS
Α
B
RATIO
OF
COST
FOR
GATED
STATIC
TO
OPTIMAL
ZGS
Z
NOTE
THAT
THE
FIRST
FACTOR
IS
POSITIVE
SINCE
THE
SECOND
FACTOR
IS
POSITIVE
SINCE
Φ
IS
CONVEX
THERE
IS
A
SUBGRADIENT
G
DEFINED
AT
EACH
POINT
THIS
GIVES
SHOWS
THAT
THE
GATED
STATIC
I
E
THE
UPPER
BOUND
HAS
VERY
CLOSE
TO
THE
OPTIMAL
COST
IN
ADDITION
TO
COMPARING
THE
TOTAL
COST
OF
THE
SCHEMES
IT
IS
IMPORTANT
TO
CONTRAST
THE
MEAN
RESPONSE
TIME
AND
MEAN
Φ
UN
Φ
UN
G
U
UN
UN
Φ
UN
Φ
UN
UN
UN
ENERGY
USE
FIGURE
SHOWS
THE
BREAKDOWN
A
REFERENCE
LOAD
OF
Ρ
WITH
DELAY
AVERSION
Β
AND
POWER
SCALING
Α
THIS
AND
IMPLY
THAT
BOTH
OF
THE
FACTORS
OF
IN
CREASE
WHEN
GOING
FROM
P
N
TO
P
N
ESTABLISHING
P
N
AND
THE
STRICT
CONCAVITY
OF
UN
SINCE
IT
IS
ALSO
NON
DECREASING
THE
RESULT
FOLLOWS
V
COMPARING
STATIC
AND
DYNAMIC
SCHEMES
TO
THIS
POINT
WE
HAVE
ONLY
PROVIDED
ANALYTIC
RESULTS
WE
NOW
USE
NUMERICAL
EXPERIMENTS
TO
CONTRAST
STATIC
AND
DYNAMIC
SCHEMES
IN
ADDITION
THESE
EXPERIMENTS
WILL
ILLUSTRATE
THE
TIGHTNESS
OF
THE
BOUNDS
PROVEN
IN
SECTION
IV
ON
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
WE
WILL
START
BY
CONTRASTING
THE
OPTIMAL
SPEEDS
UNDER
EACH
OF
THE
SCHEMES
FIGURE
COMPARES
THE
OPTIMAL
DYNAMIC
SPEEDS
WITH
THE
OPTIMAL
STATIC
SPEEDS
NOTE
THAT
THE
BOUNDS
ON
THE
DYNAMIC
SPEEDS
ARE
QUITE
TIGHT
ESPECIALLY
WHEN
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
N
IS
LARGE
FOR
REFERENCE
THE
MODES
OF
THE
OCCUPANCY
DISTRIBUTIONS
ARE
ABOUT
AND
CLOSE
TO
THE
POINTS
AT
WHICH
THE
OPTIMAL
SPEED
MATCHES
THE
STATIC
SPEEDS
NOTE
ALSO
THAT
THE
OPTIMAL
RATE
GROWS
ONLY
SLOWLY
FOR
N
MUCH
LARGER
THAN
THE
TYPICAL
OCCUPANCY
THIS
IS
IMPORTANT
SINCE
THE
RANGE
OVER
WHICH
DVS
IS
POSSIBLE
IS
LIMITED
ALTHOUGH
THE
SPEED
OF
THE
OPTIMAL
SCHEME
DIFFERS
SIGNIF
ICANTLY
FROM
THAT
OF
GATED
STATIC
THE
ACTUAL
COSTS
ARE
VERY
SIMILAR
AS
PREDICTED
BY
THE
REMARK
AFTER
COROLLARY
THIS
IS
SHOWN
IN
FIG
THE
BOUNDS
ON
THE
OPTIMAL
SPEED
ARE
ALSO
VERY
TIGHT
BOTH
FOR
LARGE
AND
SMALL
Γ
PART
A
SHOWS
THAT
THE
LOWER
BOUND
IS
LOOSEST
FOR
INTERMEDIATE
Γ
WHERE
THE
WEIGHTS
GIVEN
TO
POWER
AND
RESPONSE
TIME
ARE
COMPARABLE
PART
B
WAS
COMPARED
AGAINST
CHANGING
Ρ
FOR
FIXED
Γ
CHANGING
Β
FOR
FIXED
Ρ
AND
CHANGING
Α
NOTE
Γ
WAS
CHOSEN
TO
MAXIMIZE
THE
RATIO
OF
ZGS
Z
THE
SECOND
SCENARIO
SHOWS
THAT
WHEN
Γ
IS
HELD
FIXED
BUT
THE
LOAD
Ρ
IS
REDUCED
AND
DELAY
AVERSION
IS
REDUCED
COMMENSURATELY
THE
ENERGY
CONSUMPTION
BECOMES
NEGLIGIBLE
VI
ROBUST
POWER
AWARE
DESIGN
WE
HAVE
SEEN
BOTH
ANALYTICALLY
AND
NUMERICALLY
THAT
IDE
ALIZED
DYNAMIC
SPEED
SCALING
ONLY
MARGINALLY
REDUCES
THE
COST
COMPARED
TO
THE
SIMPLE
GATED
STATIC
THIS
RAISES
THE
QUESTION
OF
WHETHER
DYNAMIC
SCALING
IS
WORTH
THE
COMPLEXITY
THIS
SECTION
ILLUSTRATES
ONE
REASON
ROBUSTNESS
SPECIFICALLY
DYNAMIC
SCHEMES
PROVIDE
SIGNIFICANTLY
BETTER
PERFORMANCE
IN
THE
FACE
OF
BURSTY
TRAFFIC
AND
MIS
ESTIMATION
OF
WORKLOAD
WE
FOCUS
ON
ROBUSTNESS
WITH
RESPECT
TO
THE
LOAD
Ρ
THE
OPTIMAL
SPEEDS
ARE
SENSITIVE
TO
Ρ
BUT
IN
REALITY
THIS
PARAMETER
MUST
BE
ESTIMATED
AND
WILL
BE
TIME
VARYING
IT
IS
EASY
TO
SEE
THE
PROBLEMS
MIS
ESTIMATION
OF
Ρ
CAUSES
FOR
STATIC
SPEED
DESIGNS
IF
THE
LOAD
IS
NOT
KNOWN
THEN
THE
SELECTED
SPEED
MUST
BE
SATISFACTORY
FOR
ALL
POSSIBLE
ANTICIPATED
LOADS
CONSIDER
THE
CASE
THAT
IT
IS
ONLY
KNOWN
THAT
Ρ
Ρ
Ρ
LET
Z
DENOTE
THE
EXPECTED
COST
PER
UNIT
TIME
IF
THE
ARRIVAL
RATE
IS
BUT
THE
SPEED
WAS
OPTIMIZED
FOR
THEN
THE
ROBUST
DESIGN
PROBLEM
IS
TO
SELECT
THE
SPEED
ΡT
SUCH
THAT
MIN
MAX
Z
Ρ
ΡT
ΡL
Ρ
Ρ
Ρ
THE
OPTIMAL
DESIGN
IS
TO
PROVISION
FOR
THE
HIGHEST
FORESEEN
LOAD
I
E
MAXΡ
Ρ
Ρ
Z
Ρ
ΡT
Z
Ρ
ΡT
HOWEVER
THIS
IS
WASTEFUL
IN
THE
TYPICAL
CASE
THAT
THE
LOAD
IS
LESS
THAN
Ρ
OCCUPANCY
N
A
Γ
OCCUPANCY
N
B
Γ
THE
FRAGILITY
OF
STATIC
SPEED
DESIGNS
IS
ILLUSTRATED
IN
FIG
WHICH
SHOWS
THAT
WHEN
SPEED
IS
UNDERPROVISIONED
THE
SERVER
IS
UNSTABLE
AND
WHEN
IT
IS
OVERPROVISIONED
THE
DESIGN
IS
WASTEFUL
OPTIMAL
DYNAMIC
SCALING
IS
NOT
IMMUNE
TO
MIS
ESTIMATION
OF
Ρ
SINCE
N
IS
HIGHLY
DEPENDENT
ON
Ρ
HOWEVER
BECAUSE
THE
SPEED
ADAPTS
TO
THE
QUEUE
LENGTH
DYNAMIC
SCALING
IS
MORE
ROBUST
FIGURE
SHOWS
THIS
IMPROVEMENT
THIS
ROBUSTNESS
IS
IMPROVED
FURTHER
BY
THE
SPEED
SCALING
FIG
RATE
VS
N
FOR
Α
AND
DIFFERENT
ENERGY
AWARE
LOAD
Γ
SCHEME
WHICH
WE
TERM
LINEAR
THAT
SCALES
THE
SERVER
SPEED
DESIGN
FIG
BREAKDOWN
OF
E
T
AND
E
SΑ
FOR
SEVERAL
SCENARIOS
IN
PROPORTION
TO
THE
QUEUE
LENGTH
I
E
SN
Α
N
NOTE
FIG
COST
AT
LOAD
Β
Α
Ρ
WHEN
SPEEDS
ARE
DESIGNED
FOR
DESIGN
Ρ
USING
THAT
UNDER
THIS
SCALING
THE
QUEUE
IS
EQUIVALENT
TO
AN
M
GI
QUEUE
WITH
HOMOGENEOUS
SERVERS
FIGURE
SHOWS
THAT
LINEAR
SCALING
PROVIDES
SIGNIFICANTLY
BETTER
ROBUSTNESS
THAN
THE
OPTI
MAL
DYNAMIC
SCHEME
INDEED
THE
OPTIMAL
SCHEME
IS
ONLY
AND
ZGS
E
E
Β
EΡ
Ρ
Ρ
Β
Β
Β
E
OPTIMAL
FOR
DESIGNS
WITH
Ρ
EVEN
THEN
ITS
COST
IS
ONLY
SLIGHTLY
LOWER
THAN
THAT
OF
LINEAR
SCALING
THE
SIGNIFICANT
WE
CAN
FURTHER
RELATE
ZGS
TO
ZLIN
BY
EΡ
Ρ
Ρ
PRICE
THAT
LINEAR
SCALING
PAYS
IS
THAT
IT
REQUIRES
VERY
HIGH
PROCESSING
SPEED
WHEN
THE
OCCUPANCY
IS
HIGH
WHICH
MAY
NOT
BE
SUPPORTED
BY
THE
HARDWARE
WE
NOW
COMPARE
THE
ROBUSTNESS
ANALYTICALLY
IN
THE
CASE
OF
Α
FIRST
WE
WILL
SHOW
THAT
IF
Ρ
IS
KNOWN
THE
COST
OF
THE
ZGS
ZLIN
FROM
WHICH
FOLLOWS
Β
Β
E
Β
EΡ
EΡ
Β
Β
Β
E
LINEAR
SCHEME
IS
EXACTLY
THE
SAME
AS
THE
COST
OF
THE
GATED
STATIC
SCHEME
AND
THUS
WITHIN
A
FACTOR
OF
OF
OPTIMAL
THEOREM
THEN
WE
WILL
SHOW
THAT
WHEN
THE
TARGET
LOAD
DIFFERS
FROM
THE
ACTUAL
LOAD
THE
LINEAR
SCHEME
SIGNIFICANTLY
REDUCES
THE
COST
THEOREM
IN
PARTICULAR
THE
LINEAR
SCALING
SCHEME
HAS
COST
INDEPENDENT
OF
THE
DIFFERENCE
BETWEEN
THE
DESIGN
AND
ACTUAL
Ρ
IN
CONTRAST
THE
COST
OF
GATED
STATIC
GROWS
LINEARLY
IN
THIS
DIFFERENCE
AS
SEEN
IN
FIG
THEOREM
WHEN
Α
ZGS
ZLIN
THUS
ZLIN
PROOF
IF
THE
SPEED
IN
STATE
N
IS
KN
THEN
THIS
INSENSITIVITY
TO
DESIGN
LOAD
MIRRORS
WORST
CASE
ANALYSIS
THE
OPTIMUM
AVAILABLE
SCALING
WHICH
DESIGNS
FOR
Ρ
IS
O
WORST
CASE
COMPETITIVE
HOWEVER
FIG
SUGGESTS
THAT
LINEAR
SCALING
IS
MUCH
BETTER
THAN
DESIGNING
FOR
Ρ
TIGHTER
BOUNDS
ARE
KNOWN
FOR
SN
Α
BUT
THOSE
ARE
STILL
LOOSER
THAN
THEOREM
VII
CONCLUDING
REMARKS
SPEED
SCALING
IS
AN
IMPORTANT
METHOD
FOR
REDUCING
ENERGY
CONSUMPTION
IN
COMPUTER
COMMUNICATION
SYSTEMS
INTRINSI
CALLY
IT
TRADES
OFF
THE
MEAN
RESPONSE
TIME
AND
THE
MEAN
ENERGY
N
CONSUMPTION
AND
THIS
PAPER
PROVIDES
INSIGHT
INTO
THIS
TRADEOFF
E
N
Ρ
E
KN
Ρ
K
E
Ρ
K
ΡK
USING
A
STOCHASTIC
ANALYSIS
AND
SO
THE
TOTAL
COST
IS
OPTIMIZED
FOR
K
Β
IN
THIS
CASE
E
Ρ
Ρ
ASYMPTOTICS
FOR
THE
OPTIMAL
SPEED
SCALING
SCHEME
ARE
PROVIDED
THESE
BOUNDS
ARE
TIGHT
FOR
SMALL
AND
LARGE
Γ
AND
PROVIDE
A
NUMBER
OF
INSIGHTS
E
G
THAT
THE
MEAN
RESPONSE
TIME
IS
WHICH
IS
IDENTICAL
TO
THE
COST
FOR
GATED
STATIC
BY
COROLLARY
THIS
IS
WITHIN
A
FACTOR
OF
OF
Z
THEOREM
CONSIDER
A
SYSTEM
DESIGNED
FOR
TARGET
LOAD
ΡT
THAT
IS
OPERATING
AT
LOAD
Ρ
ΡT
E
WHEN
Α
Ρ
SCALING
AND
THAT
THE
OPTIMAL
DYNAMIC
SPEEDS
IN
THE
STOCHASTIC
MODEL
MATCH
FOR
LARGE
N
DYNAMIC
SPEED
SCALINGS
THAT
HAVE
BEEN
SHOWN
TO
HAVE
GOOD
WORST
CASE
PERFORMANCE
SURPRISINGLY
THE
BOUNDS
ALSO
ILLUSTRATE
THAT
A
SIMPLE
SCHEME
WHICH
GATES
THE
CLOCK
WHEN
THE
SYSTEM
IS
IDLE
AND
USES
A
STATIC
RATE
OTHERWISE
PROVIDES
PERFORMANCE
WITHIN
A
FACTOR
OF
OF
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
HOWEVER
THE
VALUE
OF
ZLIN
Β
Β
Ρ
DYNAMIC
SPEED
SCALING
IS
ALSO
ILLUSTRATED
DYNAMIC
SPEED
SCALING
SCHEMES
PROVIDE
SIGNIFICANTLY
IMPROVED
ROBUSTNESS
TO
BURSTY
TRAFFIC
AND
MIS
ESTIMATION
OF
WORKLOAD
PARAMETERS
PROOF
THE
OPTIMAL
RATES
FOR
THE
LINEAR
POLICY
ARE
SN
N
Β
INDEPENDENT
OF
ΡT
THUS
ITS
COST
IS
ALWAYS
THE
OPTIMAL
SPEED
FOR
GATED
STATIC
IN
THIS
CASE
IS
SN
ΡT
Β
FOR
N
WHEN
OPERATED
AT
ACTUAL
LOAD
Ρ
THIS
GIVES
E
Ρ
E
ΡΡT
Ρ
Β
ΡT
Ρ
Β
Β
Β
LONGER
OPTIMAL
WHEN
ROBUSTNESS
IS
CONSIDERED
A
SCHEME
THAT
SCALES
SPEEDS
LINEARLY
WITH
N
PROVIDES
SIGNIFICANTLY
IMPROVED
ROBUSTNESS
WHILE
INCREASING
COST
ONLY
SLIGHTLY
THERE
ARE
A
NUMBER
OF
RELATED
DIRECTIONS
IN
WHICH
TO
EXTEND
THIS
WORK
FOR
EXAMPLE
WE
HAVE
ONLY
CONSIDERED
DYNAMIC
POWER
CONSUMPTION
WHICH
CAN
BE
MODELED
AS
A
POLYNOMIAL
OF
THE
SPEED
HOWEVER
THE
CONTRIBUTION
OF
LEAKAGE
POWER
IS
GROWING
AND
AN
IMPORTANT
EXTENSION
IS
TO
DEVELOP
MODELS
OF
TOTAL
POWER
USE
THAT
CAN
BE
USED
FOR
ANALYSIS
ALSO
IT
WILL
BE
VERY
INTERESTING
TO
EXTEND
THE
ANALYSIS
TO
SCHEDULING
POLICIES
BEYOND
PS
FOR
EXAMPLE
GIVEN
THAT
THE
SPEED
CAN
BE
REDUCED
IF
THERE
ARE
FEWER
JOBS
IN
THE
SYSTEM
IT
IS
NATURAL
TO
SUGGEST
SCHEDULING
ACCORDING
TO
SHORTEST
REMAINING
PROCESSING
TIME
FIRST
SRPT
W
HICH
I
K
NOWN
T
O
M
INIMIZE
T
HE
N
UMBER
O
F
JOBS
IN
THE
SYSTEM
SUPPOSE
YOU
ARE
ABOUT
TO
GO
SKIING
FOR
THE
FIRST
TIME
IN
YOUR
LIFE
NATURALLY
YOU
ASK
YOURSELF
WHETHER
TO
RENT
SKIS
OR
TO
BUY
THEM
RENTING
SKIS
COSTS
SAY
WHEREAS
BUYING
SKIS
COSTS
IF
YOU
KNEW
HOW
MANY
TIMES
YOU
WOULD
GO
SKIING
IN
THE
FUTURE
IGNORING
COMPLICATING
FACTORS
SUCH
AS
INFLATION
AND
CHANGING
MODELS
OF
SKIS
THEN
YOUR
CHOICE
WOULD
BE
CLEAR
IF
YOU
KNEW
YOU
WOULD
GO
AT
LEAST
TIMES
YOU
WOULD
BE
FINANCIALLY
BETTER
OFF
BY
BUYING
SKIS
RIGHT
FROM
THE
BEGINNING
WHEREAS
IF
YOU
KNEW
YOU
WOULD
GO
LESS
THAN
TIMES
YOU
WOULD
BE
BETTER
OFF
RENTING
SKIS
EVERY
TIME
ALAS
THE
FUTURE
IS
UNCLEAR
AND
YOU
MUST
MAKE
A
DECISION
NONETHELESS
ALTHOUGH
THE
SKI
RENTAL
PROBLEM
IS
A
VERY
SIMPLE
ABSTRACTION
THIS
BASIC
PARADIGM
ARISES
IN
MANY
AP
PLICATIONS
IN
COMPUTER
SYSTEMS
IN
THESE
SITUATIONS
THERE
IS
A
SYSTEM
THAT
CAN
RESIDE
IN
EITHER
A
LOW
COST
OR
A
HIGH
COST
STATE
OCCASIONALLY
IT
IS
FORCED
TO
BE
IN
THE
HIGH
COST
STATE
USUALLY
TO
PERFORM
SOME
TASK
A
PERIOD
BETWEEN
ANY
TWO
SUCH
POINTS
IN
TIME
IS
CALLED
AN
IDLE
PERIOD
THE
SYSTEM
PAYS
A
PER
TIME
UNIT
COST
TO
RESIDE
IN
THE
HIGH
COST
STATE
ALTERNATIVELY
IT
CAN
TRANSITION
TO
THE
LOW
COST
STATE
AT
A
FIXED
ONE
TIME
COST
IF
THE
IDLE
PERIOD
IS
LONG
IT
IS
ADVANTAGEOUS
TO
TRANSITION
TO
THE
LOW
COST
STATE
IMMEDIATELY
IF
THE
IDLE
PERIOD
IS
SHORT
IT
IS
BETTER
TO
STAY
IN
THE
HIGH
COST
STATE
AN
ONLINE
ALGORITHM
WHICH
DOES
NOT
KNOW
THE
LENGTH
OF
THE
IDLE
PERIOD
MUST
BALANCE
THESE
TWO
POSSIBILITIES
THIS
PROBLEM
HAS
BEEN
STUDIED
IN
THE
CONTEXT
OF
SHARED
MEMORY
MULTIPROCESSORS
IN
WHICH
A
THREAD
IS
WAITING
FOR
A
LOCKED
PIECE
OF
DATA
AND
MUST
DECIDE
WHETHER
TO
SPIN
OR
BLOCK
RESEARCHERS
INVESTIGATING
RESEARCH
SUPPORTED
PARTIALLY
BY
NSF
GRANTS
CCR
AND
CCF
AND
BY
ONR
AWARD
YWORK
DONE
WHILE
THE
AUTHOR
WAS
A
STUDENT
AT
THE
DEPARTMENT
OF
COMPUTER
SCIENCE
CORNELL
UNIVERSITY
ITHACA
NY
RESEARCH
SUPPORTED
PARTIALLY
BY
NSF
GRANT
CCR
THE
INTERFACE
BETWEEN
IP
NETWORKS
AND
CONNECTION
ORIENTED
NETWORKS
HAVE
DISCOVERED
THIS
SAME
UNDERLYING
PROBLEM
IN
DECIDING
WHETHER
TO
KEEP
A
CONNECTION
OPEN
BETWEEN
BURSTS
OF
PACKETS
THAT
MUST
BE
SENT
ALONG
THE
CONNECTION
KARLIN
KENYON
AND
RANDALL
STUDY
THE
TCP
ACKNOWLEDGMENT
PROBLEM
AND
THE
RELATED
BAHNCARD
PROBLEM
BOTH
OF
WHICH
ARE
AT
HEART
SKI
RENTAL
PROBLEMS
THE
PROBLEM
ALSO
ARISES
IN
CACHE
COHERENCY
IN
DECIDING
WHETHER
TO
UPDATE
OR
INVALIDATE
DATA
THAT
HAS
BEEN
CHANGED
IN
A
PROCESSOR
LOCAL
CACHE
AN
IMPORTANT
APPLICATION
OF
THE
SKI
RENTAL
PROBLEM
IS
IN
MINIMIZING
THE
POWER
CONSUMED
BY
DEVICES
THAT
CAN
TRANSITION
TO
A
LOW
POWER
SLEEP
STATE
WHEN
IDLE
THE
SLEEP
STATE
CONSUMES
LESS
POWER
HOWEVER
ONE
INCURS
A
FIXED
START
UP
COST
IN
MAKING
THE
TRANSITION
TO
THE
HIGH
POWER
ACTIVE
STATE
IN
ORDER
TO
BEGIN
WORK
WHEN
A
NEW
JOB
ARRIVES
AT
THE
ARCHITECTURAL
LEVEL
THE
TECHNIQUE
OF
ELIMINATING
POWER
TO
A
FUNCTIONAL
COMPONENT
IS
CALLED
CLOCK
POWER
GATING
AT
A
HIGHER
LEVEL
THE
POWERED
DOWN
COMPONENT
MIGHT
BE
A
DISK
DRIVE
OR
EVEN
THE
WHOLE
SYSTEM
E
G
A
LAPTOP
THAT
HIBERNATES
THE
EMBEDDED
SYSTEMS
COMMUNITY
HAS
INVESTED
A
GREAT
DEAL
OF
EFFORT
INTO
DEVISING
POLICIES
GOVERNING
THE
SELECTION
OF
POWER
STATES
DURING
IDLE
PERIODS
TERMED
DYNAMIC
POWER
MANAGEMENT
IN
THEIR
LITERATURE
SEE
FOR
EXAMPLE
FOR
A
SURVEY
THESE
TECHNIQUES
HAVE
BEEN
CRITICAL
TO
MAXIMIZING
BATTERY
USE
IN
MOBILE
SYSTEMS
WHILE
POWER
IS
ALREADY
A
FIRST
CLASS
PARAMETER
IN
SYSTEM
DESIGN
IT
WILL
BECOME
INCREASINGLY
IMPORTANT
IN
THE
FUTURE
SINCE
BATTERY
CAPACITIES
ARE
INCREASING
AT
A
MUCH
SLOWER
RATE
THAN
POWER
REQUIREMENTS
MOST
OF
THE
PREVIOUS
WORK
ON
THIS
PROBLEM
HAS
BEEN
CONCERNED
WITH
TWO
STATE
SYSTEMS
WHICH
HAVE
AN
ACTIVE
STATE
AND
A
SINGLE
SLEEP
STATE
THIS
PAPER
FOCUSES
ON
FINDING
POWER
DOWN
THRESHOLDS
FOR
SYSTEMS
THAT
HAVE
MORE
THAN
ONE
LOW
POWER
STATE
AN
EXAMPLE
OF
SUCH
A
SYSTEM
IS
THE
ADVANCED
CONFIGURATION
AND
POWER
INTERFACE
ACPI
INCLUDED
IN
THE
BIOS
ON
MOST
NEWER
COMPUTERS
WHICH
HAS
FIVE
POWER
STATES
INCLUDING
A
HIBERNATION
STATE
AND
THREE
LEVELS
OF
STANDBY
PREVIOUS
WORK
AND
NEW
RESULTS
FOR
THE
TWO
STATE
PROBLEM
AN
ONLINE
ALGORITHM
CONSISTS
OF
A
SINGLE
THRESHOLD
TAFTER
WHICH
TIME
THE
ALGORITHM
WILL
TRANSITION
FROM
THE
ACTIVE
TO
THE
SLEEP
STATE
THE
INPUT
TO
THE
PROBLEM
IS
THE
LENGTH
OF
THE
IDLE
PERIOD
AND
THE
COST
OF
AN
ALGORITHM
IS
THE
TOTAL
AMOUNT
OF
ENERGY
IT
CONSUMES
OVER
A
SINGLE
IDLE
PERIOD
TYPICALLY
AN
ONLINE
ALGORITHM
IS
EVALUATED
IN
TERMS
OF
ITS
COMPETITIVE
RATIO
THE
RATIO
OF
THE
COST
OF
THE
ONLINE
ALGORITHM
TO
THE
COST
OF
THE
OPTIMAL
OFFLINE
ALGORITHM
MAXIMIZED
OVER
ALL
INPUTS
WHEN
RANDOMIZED
ALGORITHMS
ARE
CONSIDERED
WHERE
THE
THRESHOLD
TIS
CHOSEN
AT
RANDOM
WE
LOOK
AT
THE
RATIO
OF
THE
EXPECTED
COST
OF
THE
ONLINE
ALGORITHM
TO
THE
COST
OF
THE
OFFLINE
ALGORITHM
PREVIOUS
WORK
HAS
ALSO
ADDRESSED
THE
TWO
STATE
PROBLEM
WHEN
THE
IDLE
PERIOD
IS
GENERATED
BY
A
KNOWN
PROBABILITY
DISTRIBUTION
IN
THIS
CASE
THE
ONLINE
ALGORITHM
WILL
CHOOSE
A
THRESHOLD
WHICH
MINIMIZES
ITS
EXPECTED
COST
WHERE
THE
EXPECTATION
HERE
IS
TAKEN
OVER
THE
RANDOM
CHOICE
OF
THE
IDLE
PERIOD
WE
CALL
SUCH
ALGORITHMS
PROBABILITY
BASED
ALGORITHMS
THE
BEST
DETERMINISTIC
ONLINE
ALGORITHM
WILL
STAY
IN
THE
HIGH
POWER
STATE
UNTIL
THE
TOTAL
ENERGY
SPENT
IS
EQUAL
TO
THE
COST
TO
POWER
UP
FROM
THE
LOW
POWER
STATE
IT
IS
KNOWN
THAT
THIS
ALGORITHM
ACHIEVES
THE
OPTIMAL
DETERMINISTIC
COMPETITIVE
RATIO
OF
WHEN
ONE
CONSIDERS
RANDOMIZED
IF
THE
IDLE
PERIOD
IS
GENERATED
BY
A
KNOWN
PROBABILITY
DISTRIBUTION
THEN
THE
ALGORITHM
THAT
CHOOSES
OF
OPTIMAL
FURTHERMORE
THIS
BOUND
IS
TIGHT
SINCE
THERE
IS
A
DISTRIBUTION
OVER
THE
IDLE
PERIOD
LENGTHS
WHICH
E
TIMES
LARGER
THAN
THAT
INCURRED
BY
THE
OPTIMAL
OFFLINE
ALGORITHM
NOTE
THAT
IN
THE
CONTEXT
OF
POWER
DOWN
SYSTEMS
IT
MAY
NOT
BE
THE
CASE
THAT
THE
POWER
USAGE
IN
THE
SLEEP
STATE
IS
ZERO
OR
EVEN
THAT
THE
START
UP
COST
IN
THE
ACTIVE
STATE
IS
ZERO
IN
THESE
CASES
BOTH
THE
ONLINE
AND
THE
OFFLINE
ALGORITHM
WILL
INCUR
AN
IDENTICAL
ADDITIONAL
COST
THUS
THE
RATIO
OF
THE
ONLINE
TO
THE
OFFLINE
COST
WILL
DECREASE
AND
THE
OPTIMAL
COMPETITIVE
RATIO
WILL
BE
STRICTLY
LESS
THAN
TWO
HOWEVER
THESE
ADDITIONAL
COSTS
DO
NOT
CHANGE
THE
OPTIMAL
ONLINE
OR
OFFLINE
STRATEGY
IN
EITHER
THE
DETERMINISTIC
OR
THE
PROBABILITY
BASED
CASE
AND
THE
OPTIMAL
COMPETITIVE
RATIO
THAT
CAN
BE
ACHIEVED
FOR
SUCH
SYSTEMS
CAN
EASILY
BE
DETERMINED
AS
A
FUNCTION
OF
ALL
THE
PARAMETERS
OF
THE
SYSTEM
IS
THE
ACTIVE
STATE
AND
THE
SYSTEM
MUST
TRANSITION
TO
I
E
POWER
UP
AT
THE
END
OF
NOTE
THAT
THERE
CAN
BE
COSTS
TO
MOVE
FROM
HIGH
POWER
STATES
TO
LOW
POWER
STATES
AND
VICE
WHERE
MKIS
SOME
LIMITING
INTEGER
CONSTANT
THIS
GENERALIZATION
WOULD
BE
ESPECIALLY
USEFUL
FOR
ENGINEERS
WHO
HAVE
A
LARGE
NUMBER
OF
SLEEP
STATE
OPTIONS
AVAILABLE
IN
THE
DESIGN
PHASE
BUT
ARE
REQUIRED
TO
IMPLEMENT
AT
MOST
A
FIXED
NUMBER
OF
STATES
IN
THE
PRODUCT
THAT
ROLLS
OUT
INTO
THE
MARKET
FROM
THE
PERSPECTIVE
OF
WORST
CASE
GUARANTEES
IS
WHICH
CONSIDERS
THE
SPECIAL
CASE
WHERE
THE
COST
TO
POWER
DOWN
IS
ZERO
AND
THE
ALGORITHM
ONLY
PAYS
TO
MOVE
FROM
LOW
POWER
STATES
TO
HIGHER
POWER
STATES
NOTE
THAT
THIS
ALSO
INCLUDES
THE
CASE
WHERE
THE
TRANSITION
COSTS
ARE
ADDITIVE
DI
J
DJ
K
DI
KFOR
I
J
K
SINCE
THE
COSTS
TO
POWER
DOWN
CAN
THEN
BE
FOLDED
INTO
THE
COSTS
TO
POWER
UP
GIVES
NATURAL
GENERALIZATIONS
OF
THE
ALGORITHMS
FOR
THE
TWO
STATE
CASE
BOTH
FOR
THE
CASE
WHEN
THE
IDLE
PERIOD
LENGTH
IS
UNKNOWN
AND
WHEN
IT
IS
GENERATED
BY
A
KNOWN
PROBABILITY
DISTRIBUTION
IT
IS
SHOWN
THAT
WHEN
THE
E
COMPETITIVE
THUS
MATCHING
THE
GUARANTEES
IN
THE
TWO
STATE
CASE
THERE
ARE
TWO
IMPORTANT
DIRECTIONS
LEFT
OPEN
BY
THIS
WORK
THE
FIRST
IS
BASED
ON
THE
OBSERVATION
THAT
SYSTEMS
IN
GENERAL
DO
NOT
HAVE
ADDITIVE
TRANSITION
COSTS
IN
MANY
SCENARIOS
ADDITIONAL
ENERGY
IS
SPENT
IN
TRANSITIONING
TO
LOWER
POWER
STATES
FURTHERMORE
THERE
COULD
BE
OVERHEAD
IN
STOPPING
AT
INTERMEDIATE
STATES
RESULTING
IN
NON
ADDITIVE
TRANSITION
COSTS
SEE
FOR
AN
EXAMPLE
THE
SECOND
POINT
IS
THAT
THE
KNOWN
UPPER
BOUNDS
ARE
TYPICALLY
NOT
OPTIMAL
FOR
THE
SYSTEM
UNDER
CONSIDERATION
THAT
IS
WHILE
IT
IS
TRUE
THAT
THERE
EXIST
SYSTEMS
FOR
WHICH
THE
OPTIMAL
COMPETITIVE
RATIO
THAT
CAN
BE
BY
ANY
RANDOMIZED
ALGORITHM
IT
IS
POSSIBLE
TO
ACHIEVE
A
BETTER
COMPETITIVE
RATIO
FOR
MANY
SYSTEMS
FOR
MULTI
STATE
SYSTEMS
THE
OPTIMAL
COMPETITIVE
RATIO
THAT
CAN
BE
ACHIEVED
WILL
IN
GENERAL
BE
A
COMPLICATED
FUNCTION
OF
ALL
THE
PARAMETERS
OF
THE
SYSTEM
THE
POWER
CONSUMPTION
RATES
AS
WELL
AS
TRANSITION
COSTS
FOR
PROBABILITY
BASED
ALGORITHMS
THE
OPTIMAL
COMPETITIVE
RATIO
WILL
ALSO
DEPEND
ON
THE
PROBABILITY
DISTRIBUTION
GENERATING
THE
LENGTH
OF
THE
IDLE
PERIOD
WHILE
IT
MAY
NOT
BE
FEASIBLE
TO
EXPRESS
THE
OPTIMAL
COMPETITIVE
RATIO
AS
A
FUNCTION
OF
ALL
THESE
PARAMETERS
A
SYSTEM
DESIGNER
WOULD
IN
GENERAL
LIKE
TO
DESIGN
A
POWER
DOWN
STRATEGY
THAT
OBTAINS
THE
BEST
POSSIBLE
COMPETITIVE
RATIO
GIVEN
THE
CONSTRAINTS
OF
HIS
OR
HER
PARTICULAR
SYSTEM
THIS
PAPER
ESTABLISHES
THE
FOLLOWING
RESULTS
WHOSE
COMPETITIVE
RATIO
IS
WITHIN
AN
ADDITIVE
EOF
THE
BEST
COMPETITIVE
RATIO
THAT
CAN
BE
OG
E
WHERE
K
IS
THE
NUMBER
OF
STATES
IN
THE
SYSTEM
AND
ALSO
OUTPUTS
THE
COMPETITIVE
RATIO
OF
THE
ALGORITHM
WORKS
VIA
A
DECISION
PROCEDURE
WHICH
DETERMINES
FOR
A
SYSTEM
AND
A
CONSTANT
PIF
THERE
IS
A
P
COMPETITIVE
STRATEGY
FOR
THAT
SYSTEM
THIS
DECISION
PROCEDURE
ALSO
ALLOWS
US
TO
OBTAIN
LOWER
BOUNDS
ON
THE
COMPETITIVE
RATIO
ACHIEVABLE
BY
DETERMINISTIC
ALGORITHMS
FOR
SPECIFIC
SYSTEMS
WHICH
IN
TURN
PROVIDES
A
LOWER
BOUND
ON
THE
COMPETITIVE
RATIO
ACHIEVABLE
BY
DETER
MINISTIC
ALGORITHMS
IN
GENERAL
IN
PARTICULAR
WE
OBTAIN
A
LOWER
BOUND
OF
THE
COMPETITIVE
RATIO
FOR
DETERMINISTIC
ALGORITHMS
THIS
IS
THE
FIRST
LOWER
THE
ABOVE
APPROACH
CAN
BE
MODIFIED
TO
SOLVE
THE
MORE
GENERAL
VERSION
WHERE
A
BOUND
OF
MIS
SPECIFIED
ON
THE
NUMBER
OF
STATES
ALLOWED
IN
FINAL
STRATEGY
WE
SHOW
HOW
TO
EXTEND
THE
DECISION
PROCEDURE
TO
ANSWER
IF
THERE
IS
A
P
COMPETITIVE
STRATEGY
FOR
THE
SYSTEM
THAT
USES
AT
MOST
MPOWER
STATES
EXPERIMENTAL
RESULTS
SHOW
THAT
THERE
ARE
SIGNIFICANT
PERFORMANCE
GAINS
TO
BE
MADE
BY
ESTIMATING
THE
DISTRIBUTION
GOVERNING
THE
LENGTH
OF
AN
IDLE
PERIOD
BASED
ON
RECENT
HISTORY
AND
USING
THIS
ESTIMATE
TO
DRIVE
A
PROBABILITY
BASED
STRATEGY
WE
GIVE
AN
ALGORITHM
THAT
TAKES
AS
INPUT
A
DESCRIPTION
OF
A
SYSTEM
AND
A
PROBABILITY
DISTRIBUTION
GENERATING
THE
IDLE
PERIOD
LENGTH
AND
PRODUCES
THE
OPTIMAL
POWER
DOWN
STRATEGY
NATURALLY
THE
RUNNING
TIME
OF
THE
ALGORITHM
WILL
DEPEND
ON
THE
REPRESENTATION
OF
THE
DISTRIBUTION
IN
PRACTICE
THIS
IS
MOST
LIKELY
IS
THE
NUMBER
OF
BINS
IN
THE
HISTOGRAM
AND
K
IS
THE
NUMBER
OF
STATES
ONE
OUTCOME
OF
THE
PROOF
IS
THAT
IT
ALSO
ESTABLISHES
THE
OPTIMALITY
OF
THE
STRATEGY
GIVEN
IN
FOR
ADDITIVE
SYSTEMS
WE
THEN
GENERALIZE
THIS
TO
FIND
THE
BEST
ONLINE
ALGORITHM
SUBJECT
TO
THE
RESTRICTION
THAT
AT
MOST
MSTATES
ARE
USED
AT
THE
EXPENSE
OF
AN
EXTRA
FACTOR
OF
MIN
THE
RUNNING
TIME
FOR
ALL
SYSTEMS
THIS
RESULT
GIVES
A
BOUND
ON
THE
COMPETITIVE
RATIO
ACHIEVED
BY
ALSO
SERVES
AS
A
BOUND
ON
THE
RATIO
OF
THE
EXPECTED
COSTS
OF
THE
ONLINE
AND
OFFLINE
ALGORITHMS
WHEN
THE
INPUT
IS
PROBABILISTICALLY
GENERATED
IN
THE
REMAINDER
OF
THIS
PAPER
WE
USE
THE
TERMS
SCHEDULE
OR
STRATEGY
INTERCHANGEABLY
TO
REFER
TO
THE
CHOICES
OF
STATES
AND
THRESHOLD
TIMES
FOR
POWERING
DOWN
THE
TERM
ALGORITHM
WILL
REFER
TO
A
PROCEDURE
THAT
PRODUCES
A
SCHEDULE
OR
STRATEGY
BASED
ON
A
PARTICULAR
SYSTEM
AZAR
ET
AL
IN
CONSIDER
A
RELATED
PROBLEM
WHICH
THEY
REFER
TO
AS
CAPITAL
INVESTMENT
THIS
PROBLEM
IS
A
DIFFERENT
GENERALIZATION
OF
THE
SKI
RENTAL
PROBLEM
THAN
THE
POWER
DOWN
PROBLEM
CONSIDERED
HERE
HOWEVER
A
SPECIAL
CASE
OF
THEIR
PROBLEM
COINCIDES
WITH
A
SPECIAL
CASE
OF
OUR
PROBLEM
SPECIFICALLY
THEY
GIVE
A
COMPETITIVE
DETERMINISTIC
ALGORITHM
FOR
THE
SPECIAL
CASE
OF
THE
POWER
DOWN
PROBLEM
IN
WHICH
THE
COST
TO
TRANSITION
TO
EACH
STATE
IS
THE
SAME
REGARDLESS
OF
THE
STATE
FROM
WHICH
ONE
IS
TRANSITIONING
LATER
DAMASCHKE
IN
IMPROVES
THE
UPPER
BOUND
ON
THE
COMPETITIVE
RATIO
FOR
THIS
SPECIAL
CASE
ALSO
IN
THE
CONTEXT
OF
CAPITAL
INVESTMENT
TO
DETERMINISTIC
ALGORITHMS
AND
RANOMIZED
LOWER
BOUND
FOR
ANY
DETERMINISTIC
ALGORITHM
WHICH
SUBSUMES
THE
LOWER
BOUND
OF
HERE
PRELIMINARIES
FIRST
WE
WILL
ESTABLISH
THAT
WE
CAN
ASSUME
WITHOUT
LOSS
OF
GENERALITY
THAT
THE
POWER
UP
TRANSITION
J
THE
COST
TO
TRANSITION
FROM
SITO
SJIS
DI
J
DJ
THE
COST
TO
GO
FROM
SJTO
SIIS
SINCE
THERE
IS
NEVER
ANY
REASON
TO
TRANSITION
TO
A
HIGHER
POWER
STATE
UNLESS
THE
SYSTEM
IS
TRANSITIONING
TO
THE
ACTIVE
STATE
AT
THE
ARRIVAL
OF
A
NEW
TASK
ANY
SET
OF
ACTIONS
IN
THE
ORIGINAL
SYSTEM
WILL
INCUR
THE
SAME
COST
IN
THE
NEW
SYSTEM
THUS
IN
THE
SEQUEL
WE
ASSUME
THAT
DI
ALL
I
DENOTE
THE
STATE
WHICH
ATTAINS
THE
MINIMUM
THE
OPTIMAL
STATE
THE
NONE
OF
THE
ONLINE
STRATEGIES
WE
PRESENT
WILL
MAKE
USE
OF
A
STATE
THAT
IS
NEVER
USED
BY
THE
OPTIMAL
OFFLINE
STRATEGY
FOR
ANY
TIME
T
ENERGY
STATE
STATE
STATE
STATE
TIME
FIGURE
ENERGY
CONSUMED
BY
THE
OPTIMAL
STRATEGY
AS
A
FUNCTION
OF
IDLE
PERIOD
LENGTH
AS
THE
IDLE
PERIOD
LENGTH
GETS
LONGER
IT
BECOMES
MORE
WORTHWHILE
TO
PAY
THE
EXTRA
COST
TO
FIGURE
SHOWS
THE
TOTAL
ENERGY
CONSUMED
BY
OPTAS
A
FUNCTION
OF
THE
LENGTH
OF
THE
IDLE
PERIOD
THERE
IS
A
LINE
FOR
EACH
STATE
THE
Y
INTERCEPT
IS
THE
TRANSITION
COST
TO
MOVE
TO
THAT
STATE
FROM
THE
ACTIVE
STATE
AND
THE
SLOPE
IS
THE
POWER
CONSUMPTION
RATE
THE
ENERGY
CONSUMED
BY
THE
OPTIMAL
STRATEGY
IS
THE
LOWER
ENVELOPE
OF
THESE
LINES
SINCE
IT
WILL
FIRST
WE
ESTABLISH
THAT
WE
CAN
ASSUME
THAT
FOR
ALL
I
J
DI
J
J
RECALL
THAT
WE
ARE
REALLY
USING
DI
JTO
DENOTE
J
BY
FIRST
GOING
TO
THEN
DOWN
TO
SJ
THIS
IS
A
NON
TRIVIAL
ASSUMPTION
THAT
WE
WILL
HAVE
TO
HANDLE
LATER
CONSIDER
THE
THE
SAME
STATE
AS
OPT
AT
EVERY
TIME
T
THE
OPTIMAL
STRATEGY
WHICH
KNOWS
THE
LENGTH
OF
THE
IDLE
PERIOD
IN
ADVANCE
WILL
JUST
TRANSITION
TO
THE
OPTIMAL
STATE
STRATEGY
HOWEVER
MUST
FOLLOW
THE
OPTIMAL
STRATEGY
MAKING
EACH
TRANSITION
TO
A
NEW
STATE
AS
THE
IDLE
PERIOD
GETS
LONGER
THIS
IS
THE
STRATEGY
PROPOSED
IN
AND
SHOWN
TO
BE
COMPETITIVE
FOR
ADDITIVE
SYSTEMS
NOTE
THAT
THIS
STRATEGY
IS
THE
SAME
AS
THE
COMPETITIVE
BALANCE
STRATEGY
FOR
THE
TWO
STATE
CASE
COMPETITIVE
STRATEGY
FOR
ANY
SYSTEM
A
NEAR
OPTIMAL
DETERMINISTIC
ALGORITHM
IN
THIS
SECTION
WE
TURN
OUR
ATTENTION
TO
OBTAINING
A
NEAR
OPTIMAL
SCHEDULE
FOR
A
PARTICULAR
SYSTEM
E
THE
ALGORITHM
IS
BASED
ON
A
DECISION
PROCEDURE
WHICH
DETERMINES
WHETHER
A
COMPETITIVE
SCHEDULE
EXISTS
COMPETITIVE
SCHEDULE
WE
ALSO
OUTPUT
THE
RESULTING
SCHEDULE
ENERGY
ENERGY
TIME
TITIME
SHOWS
THE
TRANSFORMED
STRATEGY
WHICH
NOW
HAS
AN
EAGER
TRANSITION
THE
FOLLOWING
LEMMA
SHOWS
THAT
THE
ONLINE
STRATEGY
MUST
EVENTUALLY
GET
TO
A
SUFFICIENTLY
LOW
POWER
STATE
LEMMA
ALLOWS
US
TO
LIMIT
OUR
CONCERN
TO
JUST
THE
TRANSITION
POINTS
IN
ANY
ONLINE
SCHEDULE
THIS
IS
A
CONTRADICTION
IS
MAXIMIZED
IS
A
TRANSITION
POINT
IN
THE
STRATEGY
A
NEVER
AND
IS
THE
EARLIEST
POINT
AT
WHICH
THESE
TWO
FUNCTIONS
HAVE
THE
SAME
VALUE
NOT
CONSID
IS
LARGER
THAN
THE
SLOPE
OF
AND
WE
NOW
EXPLORE
WAYS
TO
RESTRICT
THE
SPACE
OF
SCHEDULES
WE
NEED
TO
CONSIDER
IN
SEARCHING
FOR
A
COMPETITIVE
THAT
IS
ALSO
P
COMPETITIVE
PROOF
FIGURE
SHOWS
A
SCHEMATIC
OF
THE
PROOF
THE
JUMPS
IN
THE
ONLINE
COST
THE
DASHED
LINE
ARE
J
SO
T
T
THE
PROCEDURE
ABOVE
CAN
BE
REPEATED
UNTIL
ALL
THE
TRANSITIONS
ARE
EAGER
THESE
INEQUALITIES
ALLOW
ONE
TO
DO
A
BINARY
SEARCH
USING
THE
LINE
SEGMENTS
OF
NOW
WE
REPEATEDLY
CONSIDER
THE
LEFT
END
POINT
OF
THE
SEGMENT
THAT
IS
IN
THE
MIDDLE
OF
THE
LOW
AND
HIGH
SEGMENTS
AND
USE
THE
ABOVE
INEQUALITIES
TO
UPDATE
THE
LOW
OR
HIGH
SEGMENT
AND
THE
CORRESPONDING
END
POINT
ACCORDINGLY
UNTIL
THE
END
POINTS
OF
THE
LOW
AND
HIGH
SEGMENTS
CORRESPOND
RESPECTIVELY
TO
THE
LEFT
AND
RIGHT
END
POINTS
OF
A
SEGMENT
OF
AND
THIS
SEGMENT
THE
BINARY
SEARCH
CAN
BE
IMPLEMENTED
IN
TIME
LOGK
WHERE
KIS
THE
NUMBER
OF
SEGMENTS
I
E
NUMBER
OF
STATES
LEMMA
IMMEDIATELY
GIVES
AN
ALGORITHM
THAT
IS
EXPONENTIAL
IN
K
THE
NUMBER
OF
STATES
AND
DETERMINES
WHETHER
A
P
COMPETITIVE
STRATEGY
EXISTS
FOR
THE
SYSTEM
THIS
ALGORITHM
ENUMERATES
ALL
SUBSEQUENCES
OF
STATES
AND
DETERMINES
THE
P
EAGER
STRATEGY
FOR
THAT
SUBSEQUENCE
BY
FINDING
THE
EAGER
TRANSITION
TO
EACH
STATE
BASED
ON
THE
EAGER
TRANSITIONS
TO
THE
PREVIOUS
STATES
AS
DESCRIBED
IN
THE
PROOF
OF
LEMMA
A
P
COMPETITIVE
STRATEGY
FOR
THE
SYSTEM
EXISTS
IF
AND
ONLY
IF
ONE
OF
THESE
P
EAGER
THE
REMAINDER
OF
THIS
SECTION
PRESENTS
A
WAY
TO
REMOVE
THE
EXPONENTIAL
DEPENDENCE
ON
K
TO
BE
THE
CONTIGUOUS
SUBSEQUENCE
HSI
SJI
WHERE
SIAND
SJARE
ELEMENTS
OF
SSUCH
THAT
I
J
ONE
CAN
FIND
TRANSITION
TIMES
FOR
THE
STATE
SEQUENCE
SO
THAT
IN
THE
RESULTING
SCHEDULE
EACH
FOR
THE
SEQUENCE
NOTE
THAT
UNIQUELY
DETERMINES
THE
TRANSITION
TIMES
Q
EAGER
OVER
ALL
ITS
TRANSITIONS
UP
TO
AND
INCLUDING
THE
TRANSITION
TO
STATE
OBSERVE
THAT
IF
THERE
IS
P
A
STRATEGY
THAT
CONSISTS
ENTIRELY
OF
EARLY
TRANSITIONS
IS
CALLED
A
P
EARLY
STRATEGY
COMPETITIVE
STRATEGY
AND
THE
STRATEGY
SINCE
EAGER
TRANSITIONS
AND
ENDS
IN
STATE
IT
IS
ALSO
P
COMPETITIVE
WE
NOW
ARGUE
THAT
IS
AN
EARLY
STRATEGY
NOTE
THAT
WAS
CHOSEN
SO
THAT
THE
TRANSITION
TO
STATE
SIS
P
EARLY
WE
HAVE
TO
SHOW
THAT
THE
REMAINING
TRANSITIONS
OF
ARE
ALSO
P
EARLY
AND
THE
SEQUENCE
OF
STATES
IN
THIS
SCHEDULE
IS
GIVEN
BY
CONSIDER
THE
WAS
AN
EARLY
TRANSITION
SO
FROM
LEMMA
WE
CAN
DEDUCE
THAT
WE
ONLY
NEED
TO
CONSIDER
A
SPECIFIC
EARLY
AND
EAGER
SCHEDULE
COMPETITIVE
STRATEGY
EXISTS
WE
CAN
NOW
DEFINE
A
DECISION
PROCEDURE
EXISTS
THAT
TAKES
A
SYSTEM
AND
A
CONSTANT
PAND
OUTPUTS
YES
IF
A
P
COMPETITIVE
STRATEGY
EXISTS
FOR
THE
SYSTEM
AND
NO
OTHERWISE
THE
PROCEDURE
CAN
BE
MODIFIED
TO
ALSO
OUTPUT
A
P
COMPETITIVE
STRATEGY
IF
IT
EXISTS
WE
EMPLOY
A
DYNAMIC
GIVEN
HAS
A
LOWER
COST
THAN
COMPETITIVE
STRATEGY
EXISTS
ONE
CAN
QUICKLY
ELICIT
THE
SCHEDULE
BY
STARTING
FROM
STATE
KAND
RETRACING
THE
STATES
THAT
MINIMIZED
THE
EARLIEST
TRANSITION
TIME
WE
USE
THE
PROCEDURE
EXISTS
TO
DO
A
BISECTION
OG
E
WHERE
A
BOUND
OF
MIS
SPECIFIED
ON
THE
NUMBER
OF
STATES
THAT
CAN
BE
USED
BY
THE
ONLINE
THE
INTUITION
IS
THAT
FUNCTION
EIS
NOW
REQUIRED
TO
RETURN
THE
EARLIEST
TIME
WHEN
THE
IS
NON
INCREASING
AS
IS
THE
EARLIEST
TIME
WHEN
THE
SYSTEM
P
EAGERLY
TRANSITIONS
FROM
SJTO
SIGIVEN
THAT
THE
TRANSITION
TO
SJWAS
P
EAGER
AND
OCCURRED
AT
OG
E
A
PROBABILITY
BASED
ALGORITHM
KARLIN
ET
AL
STUDY
THE
TWO
STATE
CASE
WHEN
THE
LENGTH
OF
THE
IDLE
PERIOD
IS
GENERATED
BY
A
KNOWN
PROBABILITY
DISTRIBUTION
P
ALTHOUGH
THEY
EXAMINED
THE
PROBLEM
IN
THE
CONTEXT
OF
THE
SPIN
BLOCK
PROBLEM
THEIR
PROBLEM
IS
IDENTICAL
TO
OUR
TWO
STATE
CASE
THEY
OBSERVED
THAT
THE
EXPECTED
COST
OF
THE
ONLINE
STRATEGY
THAT
MAKES
THE
TRANSITION
TO
THE
SLEEP
STATE
AT
TIME
TIS
OC
DT
WHERE
THE
POWER
CONSUMPTION
RATE
IN
THE
ACTIVE
STATE
IS
THE
POWER
CONSUMPTION
RATE
IN
THE
SLEEP
STATE
AND
IS
THE
TRANSITION
COST
BETWEEN
THE
TWO
STATES
THE
ONLINE
STRATEGY
THEN
SHOULD
SELECT
THE
TRANSITION
TIME
TTHAT
MINIMIZES
THIS
COST
THE
MULTI
STATE
CASE
PRESENTS
TWO
DISTINCT
CHALLENGES
THE
FIRST
IS
TO
DETERMINE
THE
OPTIMAL
SEQUENCE
OF
STATES
THROUGH
WHICH
AN
ONLINE
STRATEGY
SHOULD
TRANSITION
THROUGHOUT
THE
COURSE
OF
THE
IDLE
PERIOD
THEN
ONCE
THIS
SEQUENCE
HAS
BEEN
DETERMINED
THE
OPTIMAL
TRANSITION
TIMES
NEED
TO
BE
DETERMINED
OUR
PROOF
PROCEEDS
BY
ESTABLISHING
THAT
THE
ONLY
TRANSITION
TIMES
THAT
NEED
TO
BE
CONSIDERED
ARE
THE
OPTIMAL
TRANSITION
TIMES
FOR
TWO
STATES
SYSTEMS
SUPPOSE
FOR
EXAMPLE
THAT
WE
ARE
CONSIDERING
A
SEQUENCE
OF
STATE
TRANSITIONS
IN
WHICH
STATE
SIIS
FOLLOWED
BY
STATE
SJ
LET
TI
JDENOTE
THE
OPTIMAL
TRANSITION
TIME
FROM
STATE
SITO
SJIF
THESE
WERE
THE
ONLY
TWO
STATES
IN
THE
SYSTEM
THAT
IS
IF
SIWERE
THE
ACTIVE
STATE
AND
SJWERE
THE
ONLY
SLEEP
STATE
NOTE
THAT
TI
JCAN
BE
DETERMINED
BY
THE
EXPRESSION
ABOVE
WE
ESTABLISH
THAT
REGARDLESS
OF
THE
REST
OF
THE
SEQUENCE
THE
OPTIMAL
TRANSITION
POINT
FROM
STATE
SITO
SJIS
TI
J
WE
CALL
THE
TI
J
THE
PAIRWISE
OPTIMAL
TRANSITION
TIMES
LEMMAS
AND
ESTABLISH
THAT
THE
PAIRWISE
OPTIMAL
TRANSITION
TIMES
HAPPEN
IN
THE
RIGHT
ORDER
THAT
IS
FOR
I
K
J
TI
KTK
J
IF
THIS
IS
NOT
THE
CASE
THEN
ANY
SUBSEQUENCE
THAT
HAS
SIFOLLOWED
BY
SKFOLLOWED
BY
SJCAN
NOT
POSSIBLY
BE
THE
BEST
SEQUENCE
OF
STATES
NOTE
THAT
THE
TI
J
MAY
NOT
NECESSARILY
BE
UNIQUE
IN
GENERAL
WE
WILL
SELECT
THE
EARLIEST
TRANSITION
TIME
THAT
MINIMIZES
THE
COST
FOR
THE
TWO
STATE
SYSTEM
LEMMA
THEN
SHOWS
THAT
AS
LONG
AS
THE
PAIRWISE
OPTIMAL
TRANSITION
TIMES
ARE
IN
THE
RIGHT
ORDER
THEY
GIVE
THE
GLOBALLY
OPTIMAL
SET
OF
TRANSITION
TIMES
FOR
THAT
SUBSEQUENCE
OUR
ALGORITHM
THEN
USES
THIS
FACT
TO
FIND
THE
OPTIMAL
SEQUENCE
OF
STATES
BY
DYNAMIC
PROGRAMMING
NOTE
THAT
IT
IS
NOT
NECESSARY
TO
EXHAUSTIVELY
CONSIDER
ALL
POSSIBLE
SUBSEQUENCES
OPTIMAL
TRANSITION
TIMES
WE
CAN
ASSUME
THAT
FOR
I
J
DEFINE
I
JTO
BE
THE
COST
TO
TRANSITION
FROM
STATE
QITO
STATE
QJ
THAT
IS
SUP
THE
COST
OF
THE
STRATEGY
THAT
USES
THESE
TRANSITION
TIMES
IS
L
WHOSE
OPTIMAL
COST
IS
NO
GREATER
THAN
THE
OPTIMAL
COST
FOR
QL
BE
THE
SEQUENCE
OF
THRESHOLDS
THAT
MINIMIZES
THE
COST
OF
THIS
SEQUENCE
OF
STATES
DEFINE
THE
FOLLOWING
QUANTITIES
WHICH
MEANS
THAT
IT
MUST
BE
GREATER
THAN
OR
EQUAL
TO
AT
LEAST
ONE
OF
THESE
VALUES
THIS
MEANS
THAT
THE
STRATEGY
THAT
TRANSITIONS
ALTOGETHER
CAN
ONLY
IMPROVE
THE
STRATEGY
SAVE
ON
THE
TRANSITION
COST
WHICH
IS
ACCOUNTED
FOR
IN
THE
LAST
TERM
BELOW
THE
SUMMATIONS
OVER
THE
JINDICES
IN
TELESCOPE
TO
SHOW
THAT
THE
TWO
EXPRESSIONS
ARE
IDENTICAL
THE
OPTIMAL
STATE
SEQUENCE
WE
NOW
PRESENT
A
SIMPLE
POLYNOMIAL
TIME
ALGORITHM
TO
OBTAIN
THE
OPTIMAL
STATE
SEQUENCE
FOR
A
GIVEN
WERE
THE
ONLY
TWO
STATES
IN
THE
SYSTEM
THE
TIME
COMPLEXITY
OF
DETERMINING
A
SINGLE
TI
JDEPENDS
ON
THE
REPRESENTATION
OF
THE
PROBABILITY
DISTRIBUTION
IN
PRACTICE
THIS
IS
MOST
LIKELY
TO
BE
ESTIMATED
BY
A
FINITE
HISTOGRAM
WITH
BBINS
STARTING
AT
TIME
SAMPLED
AT
A
UNIFORM
DISCRETE
INTERVAL
OF
IT
FOLLOWS
THAT
BIN
ICORRESPONDS
TO
TIME
I
IT
IS
NOT
DIFFICULT
TO
GENERALIZE
THIS
FOR
VARIABLE
SIZED
BINS
WE
WILL
ALSO
ASSUME
THAT
ALL
TRANSITION
EQUALS
VALUES
WE
CAN
RE
WRITE
THE
EXPRESSION
FOR
THE
COST
OF
A
TWO
STATE
SYSTEM
IN
EQUATION
AS
RESPECTIVELY
USING
THE
PRE
CALCULATED
VALUES
ABOVE
THE
COST
OF
TRANSITIONING
FROM
STATE
SITO
STATE
SJAT
TIME
LIS
ONCE
THE
TI
J
ARE
FOUND
WE
SWEEP
THROUGH
THEM
IN
NON
DECREASING
ORDER
KEEPING
A
RUNNING
TAB
OF
THE
BEST
SUB
SCHEDULES
THAT
WE
CAN
ACHIEVE
ENDING
IN
EACH
STATE
SIAT
EACH
POINT
IN
TIME
WHEN
WE
ENCOUNTER
A
TI
J
WE
CHECK
TO
SEE
IF
TRANSITIONING
FROM
SITO
SJCAN
IMPROVE
THE
CURRENT
BEST
SUB
SCHEDULE
ENDING
IN
SJ
AND
IF
IT
DOES
UPDATE
OUR
DATA
STRUCTURE
TO
REFLECT
IT
A
GIVEN
STRATEGY
DIVIDES
TIME
INTO
INTERVALS
WHERE
EACH
INTERVAL
IS
THE
PERIOD
OF
TIME
SPENT
IN
A
PARTICULAR
STATE
THE
EXPECTED
COST
FOR
A
STRATEGY
GIVEN
IN
EQUATION
IS
OBTAINED
BY
SUMMING
OVER
THE
EXPECTED
COST
WE
KNOW
THE
RIGHT
HAND
SIDE
OF
THIS
INEQUALITY
IS
EXACTLY
CB
U
HENCE
CB
CB
T
CB
U
SINCE
B
WAS
ARBITRARILY
CHOSEN
IT
FOLLOWS
BY
SUMMING
THAT
F
U
F
F
T
NOTE
THAT
THE
FUNCTION
F
X
IS
A
CONVEX
FUNCTION
WHEN
Α
AND
F
A
B
F
A
F
B
IT
THEN
IMMEDIATELY
FOLLOWS
THAT
E
U
E
E
T
ON
A
JOB
BY
JOB
BASIS
SINCE
EI
U
Α
AND
EI
EI
T
Α
XI
XI
T
XI
XI
T
HOW
TO
COMPUTE
MΦ
A
FOR
A
GIVEN
CONFIGURATION
Φ
WE
CONSIDER
THE
PROBLEM
OF
GIVEN
AN
ENERGY
A
AND
A
FIXED
CONFIGURATION
Φ
FINDING
THE
OPTIMAL
SCHEDULE
AMONG
THOSE
SCHEDULES
WITH
CONFIGURATION
Φ
AND
ENERGY
AT
MOST
A
ACTUALLY
WE
RESTATE
THIS
PROBLEM
AS
GIVEN
A
POWER
PN
AND
A
FIXED
CONFIGURATION
Φ
FINDING
THE
OPTIMAL
SCHEDULE
AMONG
THOSE
SCHEDULES
WITH
CONFIGURATION
Φ
AND
POWER
PN
ON
JOB
N
WE
DEFINE
A
GROUP
AS
A
MAXIMAL
SUBSTRING
OF
JOBS
WHERE
Φ
I
IS
MORE
PRECISELY
A
A
B
IS
A
GROUP
IF
A
OR
Φ
A
IS
NOT
AND
FOR
I
A
B
IT
IS
THE
CASE
THAT
Φ
I
IS
AND
B
N
OR
Φ
B
IS
NOT
A
GROUP
IS
OPEN
IF
Φ
B
IS
OR
B
N
AND
IT
IS
CLOSED
IF
Φ
B
IS
LEMMA
STATES
HOW
TO
COMPUTE
THE
POWERS
OF
THE
JOBS
IN
A
GROUP
GIVEN
THE
POWER
OF
THE
LAST
JOB
OF
THE
GROUP
LEMMA
IF
A
A
B
ARE
JOBS
IN
A
GROUP
IN
THE
OPTIMAL
SCHEDULE
FOR
ENERGY
LEVEL
A
THEN
PI
PB
B
I
PN
FOR
I
A
B
PROOF
THIS
IS
AN
IMMEDIATE
APPLICATION
OF
THE
THIRD
ITEM
IN
LEMMA
IT
SHOULD
NOW
BE
CLEAR
HOW
TO
COMPUTE
THE
POWERS
OF
THE
JOBS
A
A
B
IN
AN
OPEN
GROUP
FROM
LEMMA
THE
POWER
OF
THE
LAST
JOB
IN
THE
GROUP
IS
PN
AND
THE
POWERS
OF
THE
EARLIER
JOBS
IN
THE
GROUP
ARE
GIVEN
BY
LEMMA
TO
COMPUTE
THE
POWERS
OF
THE
JOBS
IN
A
CLOSED
GROUP
IS
A
BIT
MORE
INVOLVED
LEMMA
ESTABLISHES
A
RELATIONSHIP
BETWEEN
THE
POWER
OF
THE
JOBS
IN
A
CLOSED
GROUP
AND
THE
LENGTH
OF
THE
TIME
PERIOD
WHEN
THE
JOBS
IN
THIS
GROUP
ARE
RUN
LEMMA
IF
A
A
B
ARE
JOBS
IN
A
CLOSED
GROUP
IN
THE
OPTIMAL
SCHEDULE
FOR
ENERGY
LEVEL
A
B
THEN
I
A
PB
B
I
PN
Α
RB
RA
PROOF
FROM
THE
DEFINITION
OF
CLOSED
GROUP
JOBS
A
A
B
RUN
BACK
TO
BACK
JOB
A
STARTS
AT
TIME
RA
AND
JOB
B
COMPLETES
AT
TIME
RB
THUS
B
B
B
RB
RA
XI
P
B
I
P
Α
WHERE
THE
LAST
EQUALITY
FOLLOWS
FROM
LEMMA
LEMMA
GIVES
AN
IMPLICIT
DEFINITION
OF
PB
AS
A
FUNCTION
OF
PN
HOWEVER
IT
DOES
NOT
AP
PEAR
POSSIBLE
EXPRESS
PB
AS
A
SIMPLE
CLOSED
FORM
FUNCTION
OF
PN
WE
NEXT
PROVE
IN
LEMMA
THAT
B
I
A
PB
B
I
PN
Α
IS
STRICTLY
INCREASING
AS
A
FUNCTION
OF
PB
THEREFORE
ONE
CAN
DETERMINE
PB
FROM
PN
BY
BINARY
SEARCH
ON
PB
WE
CAN
THEN
COMPUTE
THE
POWERS
OF
OTHER
JOBS
IN
THIS
CLOSED
GROUP
USING
LEMMA
B
LEMMA
WHEN
PN
IS
FIXED
I
A
PB
B
I
PN
Α
DECREASES
AS
PB
INCREASES
PROOF
FOR
I
A
B
AS
PB
INCREASES
PB
B
I
PN
Α
INCREASES
THUS
PB
B
I
PN
Α
B
DECREASES
AND
SO
DOES
I
A
PB
B
I
PN
Α
FINALLY
WE
SHOW
THAT
CONTINUOUSLY
DECREASING
PN
IS
EQUIVALENT
TO
CONTINUOUSLY
DECREASING
THE
ENERGY
BOUND
A
IN
THE
SENSE
THAT
THEY
WILL
TRACE
OUT
THE
SAME
SCHEDULES
ALBEIT
AT
A
DIFFERENT
RATE
LEMMA
IN
THE
OPTIMAL
SCHEDULES
THE
ENERGY
BOUND
A
IS
A
STRICTLY
INCREASING
CONTINUOUS
FUNCTION
OF
PN
AND
SIMILARLY
PN
IS
A
STRICTLY
INCREASING
CONTINUOUS
FUNCTION
OF
A
PROOF
WE
FIRST
PROVE
THAT
THERE
IS
A
BIJECTION
BETWEEN
PN
AND
A
IN
THE
OPTIMAL
SCHEDULES
AS
WELL
AS
IN
THE
OPTIMAL
SCHEDULES
RESTRICTED
TO
A
PARTICULAR
CONFIGURATION
THE
FACT
THAT
A
FIXED
PN
IS
MAPPED
INTO
A
UNIQUE
ENERGY
SHOULD
BE
OBVIOUS
FROM
OUR
DEVELOPMENT
TO
DATE
THAT
TWO
DIFFERENT
PN
CAN
NOT
MAP
TO
OPTIMAL
SCHEDULES
WITH
THE
SAME
ENERGY
FOLLOWS
FROM
LEMMA
SINCE
THE
FUNCTION
FROM
PN
TO
A
IS
OBVIOUSLY
CONTINUOUS
IT
THEN
FOLLOWS
THAT
THE
FUNCTION
FROM
PN
TO
A
IS
EITHER
STRICTLY
INCREASING
OR
STRICTLY
DECREASING
THE
FACT
THAT
FUNCTION
IS
STRICTLY
INCREASING
THEN
FOLLOWS
FROM
LOOKING
AT
THE
EXTREME
POINTS
IF
A
IS
VERY
LARGE
THEN
THE
OPTIMAL
CONFIGURATION
IS
ALL
AND
PN
IS
LARGE
IF
A
IS
VERY
SMALL
THEN
THE
OPTIMAL
CONFIGURATION
IS
ALL
AND
PN
IS
SMALL
THE
RESULTS
OF
LEMMAS
AND
IMPLIES
THAT
IF
WE
KNOW
THE
CONFIGURATION
OF
THE
OPTIMAL
SCHEDULE
THAT
USES
ENERGY
A
THEN
WE
CAN
COMPUTE
THE
ACTUAL
SCHEDULE
SINCE
THERE
ARE
Θ
POSSIBLE
CONFIGURATIONS
THEN
A
SIMPLE
ALGORITHMIC
APPROACH
TO
COMPUTE
THE
OPTIMAL
SCHEDULE
IS
TO
ENUMERATE
ALL
THESE
CONFIGURATIONS
COMPUTE
THE
SCHEDULE
THAT
IS
USES
ENERGY
A
FOR
EACH
OF
THESE
CONFIGURATIONS
AND
SELECT
THE
ONE
WITH
THE
SMALLEST
TOTAL
FLOW
TIME
HOWEVER
SOME
OF
THESE
SCHEDULES
ARE
NOT
FEASIBLE
AND
NEED
TO
BE
REMOVED
FROM
CONSIDERATION
THESE
SCHEDULES
CAN
BE
DETECTED
USING
THE
OBSERVATIONS
IN
SECTION
NOTE
THAT
THE
FACT
THAT
SOME
OF
THESE
SCHEDULES
ARE
NOT
FEASIBLE
DOES
NOT
CONTRADICT
THE
RESULTS
OF
LEMMAS
AND
THESE
LEMMAS
ASSUME
THAT
THE
CONFIGURATION
CORRESPONDS
TO
THE
OPTIMAL
SCHEDULE
IF
THE
CONFIGURATION
DOES
NOT
CORRESPOND
TO
THE
OPTIMAL
SCHEDULE
THEN
THE
SCHEDULE
OBTAINED
MAY
BE
INFEASIBLE
THIS
ALGORITHM
RUNS
IN
TIME
Θ
POLY
N
AS
THERE
ARE
Θ
POSSIBLE
CONFIGURATIONS
THIS
ALGORITHM
IS
DIFFERENT
FROM
THE
ONE
DESCRIBED
IN
SECTION
HOW
TO
RECOGNIZE
WHEN
THE
OPTIMAL
CONFIGURATION
CHANGES
IN
THE
ALGORITHM
DESCRIBED
IN
SECTION
THE
ALGORITHM
GRADUALLY
DECREASES
THE
AMOUNT
OF
ENERGY
A
OR
EQUIVALENTLY
THE
POWER
PN
OF
JOB
N
EVENTUALLY
THE
CONFIGURATION
OF
THE
OPTIMAL
SCHEDULE
WILL
CHANGE
FROM
A
CONFIGURATION
Φ
TO
A
CONFIGURATION
Φ
WE
NOW
EXPLAIN
HOW
TO
RECOGNIZE
THIS
GIVEN
A
CONFIGURATION
Φ
THAT
IS
NO
LONGER
OPTIMAL
FOR
ENERGY
LEVEL
A
THE
FORMULAS
FOR
COMPUTING
THE
POWERS
OF
THE
JOBS
IN
LEMMAS
AND
WILL
YIELD
A
SCHEDULE
WHOSE
CONFIGURATION
IS
NOT
EQUAL
TO
Φ
THERE
ARE
WAYS
THAT
THIS
COULD
HAPPEN
ONE
OF
THE
CONSTRAINTS
IS
VIOLATED
THERE
IS
A
LAST
JOB
B
IN
SOME
CLOSED
GROUP
WITH
PB
PB
PN
IN
A
NON
DEGENERATE
CASE
THE
CONFIGURATION
OF
THE
SCHEDULE
OBTAINED
FROM
LEMMAS
AND
WILL
BE
DIFFERENT
FROM
Φ
BY
EXACTLY
ONE
CONSTRAINT
Φ
I
THE
NEXT
CONFIGURATION
Φ
IS
OBTAINED
FROM
Φ
BY
CHANGING
Φ
I
AS
FOLLOWS
IF
Φ
I
IS
IT
SHOULD
BE
CHANGED
TO
IF
Φ
I
IS
IT
SHOULD
BE
CHANGED
TO
IN
A
DEGENERATE
CASE
ALL
VIOLATED
Φ
I
NEED
TO
BE
CHANGED
IMPLEMENTATION
DETAILS
AND
TIME
COMPLEXITY
TO
CONSTRUCT
AN
EFFICIENT
IMPLEMENTATION
OF
THE
ALGORITHM
DESCRIBED
IN
SECTION
WE
NEED
TO
CHANGE
PN
IN
A
DISCRETE
FASHION
THIS
CAN
BE
ACCOMPLISHED
USING
BINARY
SEARCH
TO
FIND
THE
NEXT
VALUE
FOR
PN
WHEN
THE
OPTIMAL
CONFIGURATION
CHANGES
SUPPOSE
L
IS
THE
RANGE
OF
POSSIBLE
VALUES
OF
PN
DIVIDED
BY
OUR
DESIRED
ACCURACY
THEN
BINARY
SEARCH
NEEDS
TO
CHECK
O
LOG
L
VALUES
OF
PN
TO
DETERMINE
THE
NEXT
VALUE
FOR
PN
WHEN
THE
OPTIMAL
CONFIGURATION
CHANGES
THE
CONDITION
FOR
DETERMINING
WHETHER
THE
CURRENT
CONFIGURATION
IS
OPTIMAL
FOR
A
PARTICULAR
PN
DESCRIBED
IN
THE
SECTION
CAN
BE
COMPUTED
IN
LINEAR
TIME
THUS
IN
TIME
O
N
LOG
L
WE
CAN
FIND
THE
NEXT
CONFIGURATION
CURVE
ON
THE
LOWER
ENVELOPE
THERE
ARE
ONLY
CONFIGURATION
CURVES
ON
THE
LOWER
ENVELOPE
AS
THE
ONLY
POSSIBLE
TRANSITIONS
ARE
FROM
TO
OR
FROM
TO
THUS
WE
GET
A
RUNNING
TIME
OF
O
LOG
L
IF
THERE
ARE
JOBS
WITH
EQUAL
RELEASE
DATES
THEN
THE
ONLY
CHANGE
THAT
IS
REQUIRED
IS
THAT
IN
THE
INITIAL
CONFIGURATION
ALL
JOBS
WITH
EQUAL
RELEASE
DATES
ARE
IN
ONE
OPEN
GROUP
WITH
SPEEDS
GIVEN
BY
LEMMA
MATHEMATICA
AND
JAVA
ANIMATION
PROGRAM
THE
ANIMATION
PROGRAM
MENTIONED
IN
THE
INTRODUCTION
IS
A
VARIATION
OF
OUR
ALGORITHM
TO
ALLOW
QUICK
RESPONSE
TIME
FOR
ANIMATION
THE
SCHEDULES
FOR
ENERGY
LEVELS
WITHIN
THE
GIVEN
RANGE
FOR
THE
GIVEN
ACCURACY
ARE
PRECOMPUTED
TO
SIMPLIFY
THE
IMPLEMENTATION
WHEN
THE
PROGRAM
COMPUTES
THE
SCHEDULE
FOR
A
PARTICULAR
ENERGY
LEVEL
A
IT
DOES
NOT
TRACE
TO
LOWER
ENVELOP
M
FROM
HIGH
ENERGY
TO
LOW
ENERGY
INSTEAD
IT
BEGINS
WITH
A
CONFIGURATION
Φ
WHERE
EACH
Φ
I
IS
THEN
COMPUTES
THE
OPTIMAL
SCHEDULE
IN
THIS
CONFIGURATION
AT
ENERGY
LEVEL
A
IF
THE
RESULTING
SCHEDULE
VIOLATES
THE
CONFIGURATION
Φ
IT
UPDATES
THE
CONFIGURATION
Φ
AS
DESCRIBED
IN
THE
PREVIOUS
SECTION
AND
RECOMPUTES
THE
OPTIMAL
SCHEDULE
FOR
THE
NEW
CONFIGURATION
THE
PROGRAM
REPEATS
THIS
PROCESS
UNTIL
THE
RESULTING
SCHEDULE
MATCHES
Φ
ARBITRARY
WORK
JOBS
WE
NOW
CONSIDER
THE
CASE
OF
ARBITRARY
WORK
JOBS
WE
FIRST
SHOW
THAT
APPLYING
OUR
ALGORITHMIC
APPROACH
IN
THIS
CASE
IS
PROBLEMATIC
AS
THE
ENERGY
OPTIMAL
SCHEDULE
IS
NOT
A
SMOOTH
FUNCTION
OF
OF
THE
ENERGY
BOUND
A
IN
PARTICULAR
IT
IS
SUFFICIENT
TO
CONSIDER
AN
INSTANCE
WITH
TWO
JOBS
WHERE
THE
JOB
WITH
THE
EARLIER
RELEASE
DATE
HAS
MORE
WORK
THAN
THE
JOB
WITH
THE
LATER
RELEASE
DATE
THEN
CONSIDER
WHAT
HAPPENS
AS
THE
ENERGY
BOUND
A
DECREASES
FIRST
BECOMES
EQUAL
TO
THEN
BUT
SRPT
STILL
GIVES
PRIORITY
TO
JOB
AT
TIME
AND
BUT
EVENTUALLY
SRPT
WILL
GIVE
PRIORITY
TO
JOB
AT
TIME
AND
JOB
IS
PREEMPTED
BY
JOB
THUS
WHEN
THIS
HAPPENS
THE
VALUE
OF
MAX
INCREASES
DISCONTINUOUSLY
SEE
FIGURE
THIS
DISCONTINUOUS
CHANGE
CAN
HAVE
A
RIPPLE
EFFECT
AS
IT
CAN
DISTURB
ANY
SCHEDULE
OF
OTHER
JOBS
LYING
BETWEEN
THE
PREVIOUS
VALUE
OF
MAX
AND
THE
NEW
VALUE
OF
MAX
MORE
ENERGY
LESS
ENERGY
FIGURE
OPTIMAL
CONFIGURATIONS
AT
DIFFERENT
AMOUNTS
OF
AVAILABLE
ENERGY
WHEN
JOBS
HAVE
UNIT
WORK
LEFT
AND
ARBITRARY
AMOUNT
OF
WORK
RIGHT
TO
SHOW
THAT
THE
MAKESPAN
OF
THE
SCHEDULE
WITH
JOBS
CHANGES
DISCONTINUOUSLY
WHEN
THE
JOBS
SWITCH
PRIORITY
WE
GIVE
THE
ANALYSIS
OF
THE
SITUATION
LET
BE
THE
OPTIMAL
SCHEDULE
UNDER
THE
RESTRICTION
THAT
JOB
IS
GIVEN
A
HIGHER
PRIORITY
THAN
JOB
LET
BE
THE
OPTIMAL
SCHEDULE
UNDER
THE
RESTRICTION
THAT
JOB
IS
GIVEN
A
HIGHER
PRIORITY
THAN
JOB
IT
CAN
BE
SHOWN
THROUGH
AN
ARGUMENT
SIMILAR
TO
THAT
IN
LEMMA
THAT
AND
OR
EQUIVALENTLY
X
X
AND
X
X
SUPPOSE
A
UNITS
OF
ENERGY
IS
AVAILABLE
THEN
Α
A
P
X
P
X
WΑ
W
WΑ
AND
Α
Α
A
P
X
P
X
WΑ
ΑW
WΑ
Α
WΑ
ΑW
WΑ
Α
Α
THE
OPTIMAL
SCHEDULE
IS
EITHER
OR
AND
IS
EQUAL
TO
THE
ONE
WITH
THE
SMALLER
FLOW
TIME
F
IS
SMALLER
IF
A
IS
LARGE
AND
F
IS
SMALLER
IF
A
IS
SMALLER
F
F
THE
SWITCH
OVER
OCCURS
WHEN
F
F
OR
EQUIVALENT
WHEN
WHEN
THIS
OCCURS
THE
MAKESPAN
SWITCHES
FROM
CMAX
TO
CMAX
NOW
WE
ARGUE
THAT
IN
GENERAL
CMAX
CMAX
SUPPOSE
CMAX
CMAX
THEN
FROM
EQUALITY
THIS
IMPLIES
THAT
HOWEVER
FROM
EQUALITIES
AND
THIS
IS
NOT
TRUE
IN
GENERAL
NOTE
THAT
WE
DON
T
HAVE
THIS
PROBLEM
WHEN
JOBS
HAVE
UNIT
WORK
BECAUSE
SRPT
BEHAVES
LIKE
FCFS
IN
PARTICULAR
JOB
PRIORITIES
ARE
FIXED
ACCORDING
TO
THEIR
ARRIVAL
TIMES
THUS
JOBS
NEVER
SWITCH
PRIORITIES
AS
THE
ENERGY
DECREASES
FOR
TWO
UNIT
SIZE
JOBS
THE
OPTIMAL
SCHEDULE
IS
ALWAYS
AND
NEVER
WE
CAN
USE
THE
ALGORITHM
FOR
EQUI
WORK
JOBS
TO
OBTAIN
AN
ALGORITHM
FOR
ARBITRARY
WORK
JOBS
THAT
IS
O
APPROXIMATE
WITH
RESPECT
TO
AVERAGE
RESPONSE
TIME
GIVEN
AN
ADDITIONAL
FACTOR
OF
Ǫ
ENERGY
THE
FRACTIONAL
RESPONSE
TIME
IS
THE
INTEGRAL
OVER
TIMES
T
OF
THE
SUM
OVER
ALL
JOBS
J
RELEASED
BEFORE
TIME
T
OF
THE
FRACTION
OF
THE
WORK
OF
JOB
J
NOT
COMPLETED
BY
TIME
T
SO
IF
OF
THE
WORK
OF
JOB
J
IS
COMPLETED
BY
TIME
T
THEN
JOB
J
CONTRIBUTES
TOWARDS
THE
SUM
AT
TIME
T
THE
FRACTIONAL
RESPONSE
TIME
IS
A
LOWER
BOUND
TO
THE
TOTAL
RESPONSE
TIME
WHICH
IS
THE
INTEGRAL
OVER
ALL
TIMES
T
OF
THE
NUMBER
OF
JOBS
RELEASED
BUT
NOT
COMPLETED
BY
TIME
T
BY
DISCRETIZING
TIME
WE
CAN
WRITE
A
CONVEX
PROGRAM
FOR
THE
OPTIMAL
FRACTIONAL
RESPONSE
TIME
SCHEDULE
AS
FOLLOWS
T
T
MIN
WJ
WJ
U
U
RJ
N
WJ
J
T
SΑ
E
U
WJ
U
J
N
U
T
THE
TIME
T
IS
SOME
SUFFICIENTLY
LATE
TIME
THAT
WE
MAY
BE
SURE
ALL
JOBS
FINISH
BY
THIS
TIME
IN
THE
OPTIMAL
SCHEDULE
IT
IS
EASY
TO
SEE
THAT
T
MAY
BE
PSEUDO
POLYNOMIALLY
BOUNDED
THE
VARIABLE
WJ
U
IS
THE
AMOUNT
OF
WORK
DONE
ON
JOB
J
AT
TIME
U
THE
OBJECTIVE
FUNCTION
IS
JUST
THE
DEFINITION
OF
FRACTIONAL
FLOW
TIME
IN
A
DISCRETE
SETTING
THE
CONSTRAINTS
IN
LINE
EXPRESS
THE
FACT
THAT
EACH
JOB
MUST
BE
EVENTUALLY
FINISHED
THE
CONSTRAINTS
IN
LINE
DEFINE
THE
SPEED
SU
AT
TIME
U
THE
CONSTRAINT
IN
LINE
IS
THEN
THE
ENERGY
BOUND
THIS
CONVEX
PROGRAM
CAN
BE
SOLVED
TO
ARBITRARY
PRECISION
IN
POLYNOMIAL
TIME
IN
THE
SIZE
OF
THE
PROGRAM
WHICH
IS
PSEUDO
POLYNOMIAL
IN
THE
INPUT
SIZE
USING
THE
ELLIPSOID
ALGORITHM
IT
IS
KNOWN
THAT
THE
RESPONSE
TIME
OF
AN
OPTIMAL
FRACTIONAL
RESPONSE
TIME
SCHEDULE
IS
WITHIN
A
FACTOR
OF
OF
THE
OPTIMAL
RESPONSE
TIME
SCHEDULE
FOR
A
PROCESSOR
THAT
IS
A
FACTOR
OF
Ǫ
SLOWER
THE
PROOF
IN
ASSUMES
A
FIXED
SPEED
PROCESSOR
BUT
EASILY
CARRIES
OVER
TO
THE
CASE
OF
A
VARIABLE
SPEED
PROCESSOR
THUS
FOR
ARBITRARY
WORK
JOBS
WE
CAN
COMPUTE
AN
O
APPROXIMATE
SCHEDULE
GIVEN
AN
EXTRA
FACTOR
OF
Ǫ
Α
ENERGY
CONCLUSIONS
WE
BELIEVE
THAT
SPEED
SCALING
PROBLEMS
PARTICULARLY
THOSE
WITHOUT
DEADLINES
IS
A
RESEARCH
AREA
WHERE
THERE
ARE
MANY
COMBINATORIALLY
INTERESTING
PROBLEMS
WITH
REAL
APPLICATION
PROBABLY
THE
MOST
OBVIOUS
OPEN
QUESTION
THAT
ARISES
FROM
THIS
PAPER
IS
WHETHER
THERE
IS
A
POLYNOMIAL
TIME
ALGORITHM
TO
COMPUTE
THE
ENERGY
OPTIMAL
SCHEDULE
IN
THE
CASE
OF
ARBITRARY
WORK
JOBS
MORE
GENERALLY
IT
WOULD
INTERESTING
TO
DETERMINE
WHETHER
OUR
ALGORITHMIC
APPROACH
IS
APPLICABLE
TO
OTHER
SPEED
SCALING
PROBLEMS
VERY
RECENTLY
CONSIDERS
AN
ONLINE
VARIATION
OF
THE
FLOW
TIME
SCHEDULING
PROBLEM
THAT
WE
CONSIDER
HERE
THEY
CONSIDER
A
COMBINED
OBJECTIVE
OF
THE
TOTAL
FLOW
TIME
PLUS
THE
ENERGY
USED
THEY
ESSENTIALLY
SHOW
THAT
RUNNING
AT
A
POWER
APPROXIMATELY
EQUAL
TO
THE
NUMBER
OF
RELEASED
BUT
UNFINISHED
JOBS
IS
O
COMPETITIVE
ACKNOWLEDGMENTS
WE
THANK
MAHAI
ANITESCU
FOR
SEVERAL
HELPFUL
DISCUSSIONS
THE
OBSERVATION
THAT
OUR
ALGORITHM
FOR
EQUI
WORK
JOBS
CAN
BE
APPLIED
TO
ARBITRARY
WORK
JOBS
AROSE
FROM
DISCUSSIONS
WITH
SANDY
IRANI
AND
JOHN
AUGUSTINE
WE
THANK
ALEKSANDAR
IVETIC
FOR
CREATING
THE
JAVA
APPLET
ILLUSTRATING
THE
CHANGE
OF
THE
OPTIMAL
SCHEDULE
AS
A
FUNCTION
OF
ENERGY
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
WHITE
PAPER
MARCH
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
INFORMATION
IN
THIS
DOCUMENT
IS
PROVIDED
IN
CONNECTION
WITH
INTEL
PRODUCTS
NO
LICENSE
EXPRESS
OR
IMPLIED
BY
ESTOPPEL
OR
OTHERWISE
TO
ANY
INTELLECTUAL
PROPERTY
RIGHTS
IS
GRANTED
BY
THIS
DOCUMENT
EXCEPT
AS
PROVIDED
IN
INTEL
TERMS
AND
CONDITIONS
OF
SALE
FOR
SUCH
PRODUCTS
INTEL
ASSUMES
NO
LIABILITY
WHATSOEVER
AND
INTEL
DISCLAIMS
ANY
EXPRESS
OR
IMPLIED
WARRANTY
RELATING
TO
SALE
AND
OR
USE
OF
INTEL
PRODUCTS
INCLUDING
LIABILITY
OR
WARRANTIES
RELATING
TO
FITNESS
FOR
A
PARTICULAR
PURPOSE
MERCHANTABILITY
OR
INFRINGEMENT
OF
ANY
PATENT
COPYRIGHT
OR
OTHER
INTELLECTUAL
PROPERTY
RIGHT
INTEL
PRODUCTS
ARE
NOT
INTENDED
FOR
USE
IN
MEDICAL
LIFE
SAVING
LIFE
SUSTAINING
APPLICATIONS
INTEL
MAY
MAKE
CHANGES
TO
SPECIFICATIONS
AND
PRODUCT
DESCRIPTIONS
AT
ANY
TIME
WITHOUT
NOTICE
DESIGNERS
MUST
NOT
RELY
ON
THE
ABSENCE
OR
CHARACTERISTICS
OF
ANY
FEATURES
OR
INSTRUCTIONS
MARKED
RESERVED
OR
UNDEFINED
INTEL
RESERVES
THESE
FOR
FUTURE
DEFINITION
AND
SHALL
HAVE
NO
RESPONSIBILITY
WHATSOEVER
FOR
CONFLICTS
OR
INCOMPATIBILITIES
ARISING
FROM
FUTURE
CHANGES
TO
THEM
THIS
DOCUMENT
AS
WELL
AS
THE
SOFTWARE
DESCRIBED
IN
IT
IS
FURNISHED
UNDER
LICENSE
AND
MAY
ONLY
BE
USED
OR
COPIED
IN
ACCORDANCE
WITH
THE
TERMS
OF
THE
LICENSE
THE
INFORMATION
IN
THIS
MANUAL
IS
FURNISHED
FOR
INFORMATIONAL
USE
ONLY
IS
SUBJECT
TO
CHANGE
WITHOUT
NOTICE
AND
SHOULD
NOT
BE
CONSTRUED
AS
A
COMMITMENT
BY
INTEL
CORPORATION
INTEL
CORPORATION
ASSUMES
NO
RESPONSIBILITY
OR
LIABILITY
FOR
ANY
ERRORS
OR
INACCURACIES
THAT
MAY
APPEAR
IN
THIS
DOCUMENT
OR
ANY
SOFTWARE
THAT
MAY
BE
PROVIDED
IN
ASSOCIATION
WITH
THIS
DOCUMENT
EXCEPT
AS
PERMITTED
BY
SUCH
LICENSE
NO
PART
OF
THIS
DOCUMENT
MAY
BE
REPRODUCED
STORED
IN
A
RETRIEVAL
SYSTEM
OR
TRANSMITTED
IN
ANY
FORM
OR
BY
ANY
MEANS
WITHOUT
THE
EXPRESS
WRITTEN
CONSENT
OF
INTEL
CORPORATION
CONTACT
YOUR
LOCAL
INTEL
SALES
OFFICE
OR
YOUR
DISTRIBUTOR
TO
OBTAIN
THE
LATEST
SPECIFICATIONS
AND
BEFORE
PLACING
YOUR
PRODUCT
ORDER
COPIES
OF
DOCUMENTS
WHICH
HAVE
AN
ORDERING
NUMBER
AND
ARE
REFERENCED
IN
THIS
DOCUMENT
OR
OTHER
INTEL
LITERATURE
MAY
BE
OBTAINED
BY
CALLING
OR
BY
VISITING
INTEL
WEBSITE
AT
ANYPOINT
APPCHOICE
BOARDWATCH
BUNNYPEOPLE
CABLEPORT
CELERON
CHIPS
CT
MEDIA
DIALOGIC
ETHEREXPRESS
ETOX
FLASHFILE
ICOMP
INSTANTIP
INTEL
INTEL
CENTRINO
INTEL
LOGO
INTEL
CREATE
SHARE
INTEL
GIGABLADE
INTEL
INBUSINESS
INTEL
INSIDE
INTEL
INSIDE
LOGO
INTEL
NETBURST
INTEL
NETMERGE
INTEL
NETSTRUCTURE
INTEL
PLAY
INTEL
PLAY
LOGO
INTEL
SINGLEDRIVER
INTEL
SPEEDSTEP
INTEL
STRATAFLASH
INTEL
TEAMSTATION
INTEL
XEON
INTEL
XSCALE
IPLINK
ITANIUM
MCS
MMX
MMX
LOGO
OPTIMIZER
LOGO
OVERDRIVE
PARAGON
PC
DADS
PC
PARENTS
PDCHARM
PENTIUM
PENTIUM
II
XEON
PENTIUM
III
XEON
PERFORMANCE
AT
YOUR
COMMAND
REMOTEEXPRESS
SMARTDIE
SOUND
MARK
STORAGEEXPRESS
THE
COMPUTER
INSIDE
THE
JOURNEY
INSIDE
TOKENEXPRESS
VOICEBRICK
VTUNE
AND
XIRCOM
ARE
TRADEMARKS
OR
REGISTERED
TRADEMARKS
OF
INTEL
CORPORATION
OR
ITS
SUBSIDIARIES
IN
THE
UNITED
STATES
AND
OTHER
COUNTRIES
OTHER
NAMES
AND
BRANDS
MAY
BE
CLAIMED
AS
THE
PROPERTY
OF
OTHERS
COPYRIGHT
INTEL
CORPORATION
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
CONTENTS
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
INTRODUCTION
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
HAS
REVOLUTIONIZED
THERMAL
AND
POWER
MANAGEMENT
BY
GIVING
APPLICATION
SOFTWARE
GREATER
CONTROL
OVER
THE
PROCESSOR
OPERATING
FREQUENCY
AND
INPUT
VOLTAGE
SYSTEMS
CAN
EASILY
MANAGE
POWER
CONSUMPTION
DYNAMICALLY
THIS
PAPER
DESCRIBES
HOW
THIS
FLEXIBLE
POWER
MANAGEMENT
SCHEME
IS
IMPLEMENTED
AND
HOW
DESIGNERS
CAN
BEST
INTEGRATE
THIS
TECHNOLOGY
INTO
THEIR
CURRENT
OR
NEXT
GENERATION
BOARD
DESIGN
TODAY
EMBEDDED
SYSTEMS
ARE
DEMANDING
GREATER
PERFORMANCE
AT
EQUIVALENT
LEVELS
OF
POWER
CONSUMPTION
LEGACY
HARDWARE
SUPPORT
FOR
BACKPLANES
BOARD
SIZES
AND
THERMAL
SOLUTIONS
HAVE
FORCED
DESIGN
TEAMS
TO
PLACE
GREATER
EMPHASIS
ON
POWER
AND
THERMAL
BUDGETS
INTEL
HAS
EXTENDED
ARCHITECTURAL
INNOVATION
FOR
SAVING
POWER
BY
IMPLEMENTING
NEW
FEATURES
SUCH
AS
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
ALLOWS
THE
PROCESSOR
PERFORMANCE
AND
POWER
CONSUMPTION
LEVELS
TO
BE
MODIFIED
WHILE
A
SYSTEM
IS
FUNCTIONING
THIS
IS
ACCOMPLISHED
VIA
APPLICATION
SOFTWARE
WHICH
CHANGES
THE
BUS
TO
CORE
FREQUENCY
RATIO
AND
THE
PROCESSOR
CORE
VOLTAGE
VCC
A
VARIETY
OF
INPUTS
SUCH
AS
SYSTEM
POWER
SOURCE
PROCESSOR
THERMAL
STATE
OR
OPERATING
SYSTEM
POLICY
ARE
USED
TO
DETERMINE
THE
PROPER
OPERATING
STATE
THE
SOFTWARE
MODEL
BEHIND
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
HAS
ULTIMATE
CONTROL
OVER
THE
FREQUENCY
AND
VOLTAGE
TRANSITIONS
THIS
SOFTWARE
MODEL
IS
A
MAJOR
STEP
FORWARD
OVER
PREVIOUS
IMPLEMENTATIONS
OF
INTEL
SPEEDSTEP
TECHNOLOGY
LEGACY
VERSIONS
OF
INTEL
SPEEDSTEP
TECHNOLOGY
REQUIRED
HARDWARE
SUPPORT
THROUGH
THE
CHIPSET
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
HAS
REMOVED
THE
CHIPSET
HARDWARE
REQUIREMENT
AND
ONLY
REQUIRES
THE
SUPPORT
OF
THE
VOLTAGE
REGULATOR
PROCESSOR
AND
OPERATING
SYSTEM
CENTRALIZATION
OF
THE
CONTROL
MECHANISM
AND
SOFTWARE
INTERFACE
TO
THE
PROCESSOR
AND
REDUCED
HARDWARE
OVERHEAD
HAS
REDUCED
PROCESSOR
CORE
UNAVAILABILITY
TIME
TO
ΜS
FROM
THE
PREVIOUS
GENERATION
UNAVAILABILITY
OF
ΜS
PROCESSOR
SUPPORT
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
IS
SUPPORTED
ON
CURRENT
AND
FUTURE
GENERATIONS
OF
INTEL
PENTIUM
M
PROCESSORS
THE
INTEL
PENTIUM
M
PROCESSOR
AT
GHZ
SUPPORTS
SIX
FREQUENCY
AND
VOLTAGE
OPERATING
POINTS
TABLE
SUPPORTED
PERFORMANCE
STATES
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
AT
THE
TOP
AND
BOTTOM
MODES
ARE
COMMONLY
KNOWN
AS
HIGH
FREQUENCY
MODE
HFM
AND
LOW
FREQUENCY
MODE
LFM
THESE
FREQUENCY
AND
VOLTAGE
OPERATING
POINTS
ARE
STORED
WITHIN
A
READ
ONLY
PROCESSOR
MODEL
SPECIFIC
REGISTER
MSR
THIS
MSR
ENSURES
BIOS
WILL
NOT
ALLOW
TRANSITIONS
TO
INVALID
STATES
ABOVE
THE
HFM
MAXIMUM
OR
BELOW
THE
LFM
MINIMUM
THE
OTHER
FOUR
OPERATING
POINTS
ARE
STORED
WITHIN
BIOS
CODE
AS
A
DROP
IN
VOLTAGE
TABLE
PROVIDED
BY
INTEL
TO
BIOS
VENDORS
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
THERMAL
DESIGN
POWER
THE
INTEL
POWER
SPECIFICATION
FOR
COMPONENTS
IS
KNOWN
AS
THERMAL
DESIGN
POWER
OR
TDP
THE
TDP
VALUE
ALONG
WITH
THE
MAXIMUM
JUNCTION
TEMPERATURE
DEFINES
INTEL
RECOMMENDED
DESIGN
POINT
FOR
THERMAL
SOLUTION
CAPABILITY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
WHICH
SUPPORTS
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
INTEL
WILL
SPECIFY
THE
TDP
FOR
BOTH
THE
HFM
AND
LFM
OPERATING
STATES
THESE
TDP
VALUES
ARE
SPECIFIED
IN
THE
INTEL
PENTIUM
M
PROCESSOR
DATASHEET
AND
ALSO
SHOWN
IN
THE
FOLLOWING
TABLE
FOR
THE
GHZ
AND
GHZ
PARTS
TABLE
HFM
AND
LFM
THERMAL
DESIGN
POWER
POINTS
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
AT
TABLE
HFM
AND
LFM
THERMAL
DESIGN
POWER
POINTS
FOR
THE
LOW
VOLTAGE
INTEL
PENTIUM
M
PROCESSOR
AT
BETWEEN
THE
HFM
AND
LFM
FREQUENCY
VOLTAGE
POINTS
THE
DISSIPATED
POWER
WILL
BE
APPROXIMATELY
PROPORTIONAL
TO
THE
SQUARE
OF
THE
VOLTAGE
DUE
TO
OHM
LAW
EQUATION
OHM
LAW
WHERE
P
POWER
C
CAPACITANCE
V
VOLTAGE
AND
F
FREQUENCY
P
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
FIGURE
POWER
VS
CORE
VOLTAGE
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
AT
FOR
ITS
SIX
FREQUENCY
VOLTAGE
OPERATING
POINTS
NOT
TO
SCALE
HFM
AND
LFM
POWER
VALUES
ARE
TDP
SPECIFICATIONS
POWER
W
CORE
VOLTAGE
V
IMPLEMENTATION
ALL
INTEL
PENTIUM
M
PROCESSORS
HAVE
SUPPORT
FOR
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
THIS
CAN
BE
VERIFIED
BY
CHECKING
THE
ECX
FEATURE
BIT
IN
THE
CPUID
REGISTER
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
IS
ENABLED
BY
SETTING
THE
MSR
BIT
THIS
BIT
SHOULD
BE
WRITTEN
BY
THE
SYSTEM
BIOS
UPON
BOOT
PROCESSOR
FREQUENCY
VOLTAGE
TRANSITIONS
ARE
INITIATED
BY
WRITING
A
BIT
VALUE
TO
THE
REGISTER
IF
A
TRANSITION
IS
ALREADY
IN
PROGRESS
TRANSITION
TO
A
NEW
VALUE
WILL
TAKE
EFFECT
SUBSEQUENTLY
READS
OF
THE
REGISTER
DETERMINE
THE
LAST
TARGETED
OPERATING
POINT
THE
CURRENT
OPERATING
POINT
CAN
BE
READ
FROM
REGISTER
WHICH
IS
UPDATED
DYNAMICALLY
THE
BIT
ENCODING
DEFINING
VALID
OPERATING
POINTS
IS
MODEL
SPECIFIC
AND
INTEL
PROPRIETARY
SEE
YOUR
INTEL
REPRESENTATIVE
TO
OBTAIN
DOCUMENTATION
OUTLINING
THE
REQUIRED
ENCODING
BY
CENTRALIZING
THE
IMPLEMENTATION
OF
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
TO
THE
PROCESSOR
SOFTWARE
CAN
PERFORM
A
FREQUENCY
VOLTAGE
OPERATING
STATE
TRANSITION
BY
SIMPLY
WRITING
TO
ONE
REGISTER
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
TABLE
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
MSRS
SOFTWARE
CONSIDERATIONS
AS
ALREADY
NOTED
THE
SOFTWARE
MODEL
BEHIND
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
PERFORMS
ALL
MANAGEMENT
FOR
THE
FREQUENCY
AND
VOLTAGE
TRANSITIONS
MICROSOFT
WINDOWS
XP
AND
THE
WINDOWS
SERVER
FAMILY
INCLUDE
COMPLETE
NATIVE
PROCESSOR
PERFORMANCE
CONTROL
TO
SUPPORT
THIS
FUNCTIONALITY
THE
NATIVE
SUPPORT
IN
WINDOWS
XP
AND
THE
WINDOWS
SERVER
FAMILY
CONSISTS
OF
TWO
COMPONENTS
THE
KERNEL
POWER
POLICY
MANAGER
AND
THE
PROCESSOR
DRIVER
THE
KERNEL
POWER
POLICY
MANAGER
OWNS
THE
DECISION
MAKING
AND
THE
SET
OF
RULES
USED
TO
DETERMINE
THE
APPROPRIATE
FREQUENCY
VOLTAGE
OPERATING
STATE
IT
MAY
MAKE
DECISIONS
BASED
ON
SEVERAL
INPUTS
SUCH
AS
END
USER
POWER
POLICY
PROCESSOR
UTILIZATION
BATTERY
LEVEL
OR
THERMAL
CONDITIONS
AND
EVENTS
THE
PROCESSOR
DRIVER
IS
USED
TO
MAKE
ACTUAL
STATE
TRANSITIONS
ON
THE
KERNEL
POWER
POLICY
MANAGER
BEHALF
THE
DRIVER
DOES
NOT
INITIATE
FREQUENCY
VOLTAGE
STATE
TRANSITIONS
INDEPENDENT
OF
THE
KERNEL
POWER
POLICY
MANAGER
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
FIGURE
SOFTWARE
MODEL
FOR
WINDOWS
XP
AND
WINDOWS
SERVER
GET
STATE
CHANGE
STATE
MSR
READ
MSR
WRITE
END
USER
POWER
POLICY
INPUTS
ARE
ENTERED
VIA
THE
POWER
OPTIONS
ICON
WITHIN
THE
CONTROL
PANEL
BASED
ON
THE
SELECTED
POLICY
WINDOWS
WILL
ADJUST
ITS
DECISION
MAKING
FOR
POWER
STATES
ACCORDINGLY
TABLE
RELATIONSHIP
BETWEEN
POWER
POLICY
SCHEME
AND
PROCESSOR
DYNAMIC
THROTTLING
POLICY
THE
PROCESSOR
DYNAMIC
THROTTLE
POLICY
HAS
BEEN
DEFINED
TO
OPERATE
IN
ONE
OF
FOUR
STATES
NONE
ADAPTIVE
CONSTANT
AND
DEGRADE
NONE
WILL
ENSURE
THE
PROCESSOR
IS
ALWAYS
AT
THE
HIGHEST
PERFORMANCE
STATE
CURRENTLY
AVAILABLE
BARRING
ANY
UNIQUE
THERMAL
CONDITIONS
WHERE
HARDWARE
WILL
FORCE
A
LOWER
STATE
ADAPTIVE
MATCHES
THE
PERFORMANCE
STATE
TO
CURRENT
DEMAND
CONSTANT
WILL
RUN
THE
PROCESSOR
IN
THE
LOWEST
AVAILABLE
FREQUENCY
VOLTAGE
STATE
SIMILAR
TO
THE
CONSTANT
STATE
DEGRADE
WILL
ALWAYS
RUN
THE
PROCESSOR
IN
THE
LOWEST
AVAILABLE
FREQUENCY
VOLTAGE
STATE
FURTHERMORE
THIS
POLICY
STATE
WILL
UTILIZE
LINEAR
STOP
CLOCK
THROTTLING
IF
THE
REMAINING
BATTERY
CAPACITY
DROPS
BELOW
A
CERTAIN
THRESHOLD
APART
FROM
WINDOWS
XP
AND
THE
WINDOWS
SERVER
FAMILY
SUPPORT
ALSO
EXISTS
FROM
INTEL
AND
MICROSOFT
FOR
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
TRANSITIONS
ON
LEGACY
MICROSOFT
PLATFORMS
LEGACY
VERSIONS
OF
THE
WINDOWS
OPERATING
SYSTEM
DO
NOT
INCLUDE
COMPLETE
POWER
POLICY
MANAGER
AND
DRIVER
SUPPORT
THUS
INTEL
HAS
RELEASED
AN
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
APPLET
SPECIFICALLY
TO
ENABLE
THIS
FUNCTIONALITY
ON
THE
FOLLOWING
VERSIONS
OF
WINDOWS
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
MICROSOFT
WINDOWS
MICROSOFT
WINDOWS
MICROSOFT
WINDOWS
MILLENNIUM
EDITION
ME
MICROSOFT
WINDOWS
NT
THIS
APPLET
IS
AVAILABLE
FROM
EITHER
INTEL
OR
AN
INTEL
OEM
SEE
YOUR
INTEL
REPRESENTATIVE
FOR
MORE
INFORMATION
OR
TO
ORDER
THE
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
APPLET
THE
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
APPLET
PROVIDES
BOTH
A
POWER
POLICY
AND
A
DRIVER
TO
ENABLE
DYNAMIC
TRANSITIONS
THE
END
USER
POLICY
IS
AGAIN
ESTABLISHED
THROUGH
THE
WINDOWS
CONTROL
PANEL
WITH
VERY
SIMILAR
POWER
STATE
OPERATION
FIGURE
SOFTWARE
MODEL
FOR
THE
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
APPLET
ON
LEGACY
WINDOWS
PLATFORMS
PERIODIC
GET
STATE
SET
STATE
REQUESTS
LOAD
GUI
USER
PREFERENCES
SET
STATE
PERIODIC
GET
STATE
I
O
I
O
NOTICE
THE
SOFTWARE
MODEL
FOR
LEGACY
WINDOWS
PLATFORMS
MUST
USE
SYSTEM
MANAGEMENT
MODE
WHICH
CREATES
SLIGHTLY
MORE
OVERHEAD
ON
THE
SYSTEM
A
DETAILED
OVERVIEW
OF
SYSTEM
MANAGEMENT
MODE
SMM
MAY
BE
FOUND
IN
THE
IA
INTEL
ARCHITECTURE
SOFTWARE
DEVELOPER
MANUAL
SUPPORT
FOR
OPERATING
SYSTEMS
OTHER
THAN
MICROSOFT
WINDOWS
IS
AVAILABLE
TO
VARYING
DEGREES
IN
THE
LINUX
OPEN
SOURCE
COMMUNITY
LINUX
OPERATING
SYSTEMS
WITH
SUPPORT
FOR
ACPI
MAY
ENABLE
PERFORMANCE
STATE
TRANSITIONS
BY
MANUALLY
WRITING
TO
THE
APPROPRIATE
FILE
E
G
PROC
CPUFREQ
OR
PROC
ACPI
PROCESSOR
LINUX
KERNEL
VERSION
AND
NEWER
PROVIDES
FULL
ACPI
SUPPORT
AND
MAPS
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
MSRS
TO
THE
CORRECT
OBJECT
TABLE
SEE
TABLE
ALTHOUGH
SUPPORT
MAY
EXIST
FOR
MANUAL
PERFORMANCE
STATE
CHANGES
THROUGH
ACPI
OBJECT
TABLES
AUTOMATIC
PERFORMANCE
STATE
CHANGE
BASED
ON
THERMAL
CONDITIONS
BATTERY
LIFE
OR
OTHER
USER
PARAMETERS
IS
LIMITED
AND
DEPENDENT
ON
OPERATING
SYSTEM
VENDOR
MORE
INFORMATION
MAY
BE
FOUND
AT
INTEL
INSTANTLY
AVAILABLE
TECHNOLOGY
WEBSITE
AT
ACPI
CONSIDERATIONS
ACPI
OR
ADVANCED
CONFIGURATION
AND
POWER
INTERFACE
HAS
GREATLY
SIMPLIFIED
AND
STANDARDIZED
POWER
SYSTEM
MANAGEMENT
ACPI
HAS
INTRODUCED
NEW
POWER
STATES
ON
A
SYSTEM
DEVICE
AND
PROCESSOR
LEVEL
THIS
PAPER
WILL
INTRODUCE
THE
PROCESSOR
POWER
STATE
LEVELS
P
STATES
AND
MAP
THEM
TO
HOW
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
TRANSITIONS
ARE
MADE
OTHER
POWER
STATES
SUCH
AS
C
STATES
STATES
AND
D
STATES
ARE
COVERED
IN
THE
ACPI
SPECIFICATION
AT
FIGURE
PROCESSOR
PERFORMANCE
STATES
P
STATES
DISCUSSED
HEREIN
ARE
SUB
STATES
OF
PROCESSOR
P
STATES
ARE
DEFINED
AS
FREQUENCY
VOLTAGE
OPERATING
STATES
UNLIKE
OTHER
ACPI
DEFINED
POWER
STATES
SUCH
AS
ETC
THE
PROCESSOR
IS
ACTUALLY
EXECUTING
INSTRUCTIONS
IN
ALL
P
STATES
SINCE
THEY
ARE
SUBSTATES
OF
IS
DEFINED
AS
THE
MAXIMUM
PERFORMANCE
HIGHEST
POWER
CONSUMING
STATE
WHICH
IS
ALSO
TERMED
HFM
ON
AN
INTEL
PENTIUM
M
PROCESSOR
PX
PX
ARE
DEFINED
WITH
INCREMENTALLY
LOWER
PERFORMANCE
AND
POWER
DISSIPATION
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
TABLE
P
STATES
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
AT
ACPI
PROVIDES
A
STANDARD
INTERFACE
FOR
OPERATING
SYSTEMS
TO
COMMUNICATE
WITH
HARDWARE
SPECIFIC
COMPONENT
CAPABILITIES
ARE
MAPPED
INTO
ACPI
TABLES
TABLE
ACPI
OBJECT
TABLE
DEFINITIONS
IT
IS
THE
RESPONSIBILITY
OF
THE
BIOS
TO
CONSTRUCT
AND
FILL
IN
THE
ACPI
TABLES
INTEL
PROVIDES
FREQUENCY
VOLTAGE
TABLES
OR
A
BIOS
ALGORITHM
TO
FILL
THE
OBJECT
TABLE
AFTER
THE
TABLES
HAVE
BEEN
POPULATED
THE
OPERATING
SYSTEM
MAY
FORCE
PERFORMANCE
STATE
CHANGES
BY
ACCESSING
THE
MSRS
VIA
THE
CORRESPONDING
ACPI
OBJECT
TABLES
NON
ACPI
IMPLEMENTATIONS
ACPI
IS
RECOMMENDED
FOR
DESIGNING
SYSTEMS
THAT
UTILIZE
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
DEEPLY
EMBEDDED
DESIGNS
NOT
UTILIZING
THE
ACPI
INTERFACE
MAY
STILL
INCORPORATE
FREQUENCY
AND
VOLTAGE
TRANSITIONS
BY
DIRECTLY
READING
AND
WRITING
TO
THE
CORRESPONDING
MSRS
ON
THE
PROCESSOR
THE
CHOSEN
FREQUENCY
AND
VOLTAGE
COMBINATIONS
MUST
MATCH
THE
VALID
COMBINATIONS
AS
PRESENTED
IN
THE
PROCESSOR
DATASHEET
ANY
OTHER
COMBINATIONS
WILL
FORCE
THE
PROCESSOR
TO
OPERATE
OUTSIDE
OF
SPECIFICATION
AND
MAY
CAUSE
COMPONENT
FAILURE
CENTRALIZING
IMPLEMENTATION
FOR
REAL
TIME
FREQUENCY
VOLTAGE
TRANSITIONS
TO
A
PROCESSOR
MSR
HAS
ENABLED
DEEPLY
EMBEDDED
OPERATING
SYSTEMS
AND
APPLICATIONS
TO
EASILY
CHANGE
AND
CONTROL
OPERATING
STATES
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
VOLTAGE
REGULATOR
SUPPORT
PROCESSORS
FEATURING
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
PROVIDE
MULTIPLE
VID
PINS
THAT
ARE
DIRECTLY
CONNECTED
TO
VOLTAGE
REGULATOR
THESE
VID
PINS
SIGNAL
THE
DESIRED
VOLTAGE
FOR
THE
PROCESSOR
THE
PIN
OUTPUT
IS
MAPPED
TO
A
TABLE
AS
LISTED
IN
THE
CORRESPONDING
PROCESSOR
DATASHEET
THE
CORRESPONDING
VOLTAGE
FOR
THE
VID
OUTPUT
IS
SUBSEQUENTLY
PROVIDED
TO
THE
CORE
PROCESSOR
PLANE
BY
THE
VOLTAGE
REGULATOR
VOLTAGE
REGULATOR
DESIGNS
FOR
THE
INTEL
PENTIUM
M
PROCESSOR
HAVE
BEEN
DESIGNED
TO
ACCEPT
CORE
VOLTAGE
INPUTS
FROM
V
TO
V
AS
SELECTED
BY
SIX
VID
PINS
THIS
WIDE
RANGE
OF
VOLTAGES
ALLOWS
ONE
REGULATOR
DESIGN
TO
SUPPORT
ALL
PROCESSORS
WITHIN
THE
INTEL
PENTIUM
M
AND
INTEL
CELERON
M
FAMILIES
INCLUDING
LOW
VOLTAGE
AND
ULTRA
LOW
VOLTAGE
VERSIONS
ADDITIONALLY
ALL
PROCESSORS
IN
THIS
FAMILY
ARE
BOTH
PACKAGE
AND
PIN
FOOTPRINT
COMPATIBLE
CONCLUSION
ENHANCED
INTEL
SPEEDSTEP
TECHNOLOGY
HAS
REVOLUTIONIZED
THERMAL
AND
POWER
MANAGEMENT
BY
CENTRALIZING
HARDWARE
IMPLEMENTATION
TO
THE
PROCESSOR
BY
ALSO
GIVING
SOFTWARE
GREATER
CONTROL
OVER
THE
TRANSITION
MECHANISM
PERFORMANCE
STATE
TRANSITIONS
MAY
BE
DYNAMICALLY
CONTROLLED
TO
MANAGE
INCREASINGLY
TIGHT
POWER
AND
THERMAL
BUDGETS
REFERENCES
ACPI
ADVANCED
CONFIGURATION
AND
POWER
INTERFACE
IA
INTEL
ARCHITECTURE
SOFTWARE
DEVELOPER
MANUAL
VOLUME
SYSTEM
PROGRAMMING
GUIDE
MICROSOFT
ACPI
POWER
MANAGEMENT
INTEL
PENTIUM
M
PROCESSOR
DATASHEET
WINDOWS
NATIVE
PROCESSOR
PERFORMANCE
CONTROL
INSTANTLY
AVAILABLE
TECHNOLOGY
ACPI
AP
INTEL
PROCESSOR
IDENTIFICATION
AND
THE
CPUID
INSTRUCTION
NOTES
PLEASE
REFER
TO
THE
INTEL
PENTIUM
M
PROCESSOR
DATASHEET
FOR
THE
LATEST
SPECIFICATIONS
ALL
INFORMATION
INCLUDED
IN
THE
DATASHEET
SUPERSEDES
THE
MATERIAL
OF
THIS
DOCUMENT
JOULESORT
A
BALANCED
ENERGY
EFFICIENCY
BENCHMARK
SUZANNE
RIVOIRE
STANFORD
UNIVERSITY
MEHUL
A
SHAH
HP
LABS
PARTHASARATHY
RANGANATHAN
HP
LABS
CHRISTOS
KOZYRAKIS
STANFORD
UNIVERSITY
ABSTRACT
THE
ENERGY
EFFICIENCY
OF
COMPUTER
SYSTEMS
IS
AN
IMPORTANT
CONCERN
IN
A
VARIETY
OF
CONTEXTS
IN
DATA
CENTERS
REDUCING
ENERGY
USE
IMPROVES
OPERATING
COST
SCALABILITY
RELIABILITY
AND
OTHER
FACTORS
FOR
MOBILE
DEVICES
ENERGY
CONSUMPTION
DIRECTLY
AFFECTS
FUNCTIONALITY
AND
USABILITY
WE
PROPOSE
AND
MOTIVATE
JOULESORT
AN
EXTERNAL
SORT
BENCHMARK
FOR
EVALUAT
ING
THE
ENERGY
EFFICIENCY
OF
A
WIDE
RANGE
OF
COMPUTER
SYSTEMS
FROM
CLUSTERS
TO
HANDHELDS
WE
LIST
THE
CRITERIA
CHALLENGES
AND
PITFALLS
FROM
OUR
EXPERIENCE
IN
CREATING
A
FAIR
ENERGY
EFFICIENCY
BENCHMARK
USING
A
COMMERCIAL
SORT
WE
DEMON
STRATE
A
JOULESORT
SYSTEM
THAT
IS
OVER
AS
ENERGY
EFFICIENT
AS
LAST
YEAR
ESTIMATED
WINNER
THIS
SYSTEM
IS
QUITE
DIFFER
ENT
FROM
THOSE
CURRENTLY
USED
IN
DATA
CENTERS
IT
CONSISTS
OF
A
COMMODITY
MOBILE
CPU
AND
LAPTOP
DRIVES
CONNECTED
BY
SERVER
STYLE
I
O
INTERFACES
CATEGORIES
AND
SUBJECT
DESCRIPTORS
H
INFORMATION
SYSTEMS
DATABASE
MANAGEMENT
SYSTEMS
GENERAL
TERMS
DESIGN
EXPERIMENTATION
MEASUREMENT
PERFORMANCE
KEYWORDS
BENCHMARK
ENERGY
EFFICIENCY
POWER
SERVERS
SORT
INTRODUCTION
IN
CONTEXTS
RANGING
FROM
LARGE
SCALE
DATA
CENTERS
TO
MOBILE
DEVICES
ENERGY
USE
IN
COMPUTER
SYSTEMS
IS
AN
IMPORTANT
CONCERN
IN
DATA
CENTER
ENVIRONMENTS
ENERGY
EFFICIENCY
AFFECTS
A
NUMBER
OF
FACTORS
FIRST
POWER
AND
COOLING
COSTS
ARE
SIGNIFI
CANT
COMPONENTS
OF
OPERATIONAL
AND
UP
FRONT
COSTS
TODAY
A
TYPICAL
DATA
CENTER
WITH
RACKS
CONSUMING
TOTAL
POWER
COSTS
TO
POWER
AND
TO
COOL
PER
YEAR
WITH
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
SIGMOD
JUNE
BEIJING
CHINA
COPYRIGHT
ACM
OF
UP
FRONT
COSTS
FOR
COOLING
EQUIPMENT
THESE
COSTS
VARY
DEPENDING
UPON
THE
INSTALLATION
BUT
THEY
ARE
GROWING
RAPIDLY
AND
HAVE
THE
POTENTIAL
EVENTUALLY
TO
OUTSTRIP
THE
COST
OF
HARDWARE
SECOND
ENERGY
USE
HAS
IMPLICATIONS
FOR
DENSITY
RELIABILITY
AND
SCALABILITY
AS
DATA
CENTERS
HOUSE
MORE
SERVERS
AND
CONSUME
MORE
ENERGY
REMOVING
HEAT
FROM
THE
DATA
CENTER
BECOMES
INCREASINGLY
DIFFICULT
SINCE
THE
RELIABILITY
OF
SERVERS
AND
DISKS
DECREASES
WITH
INCREASED
TEMPERATURE
THE
POWER
CONSUMPTION
OF
SERVERS
AND
OTHER
COMPONENTS
LIMITS
THE
ACHIEVABLE
DENSITY
WHICH
IN
TURN
LIM
ITS
SCALABILITY
THIRD
ENERGY
USE
IN
DATA
CENTERS
IS
STARTING
TO
PROMPT
ENVIRONMENTAL
CONCERNS
OF
POLLUTION
AND
EXCESSIVE
LOAD
PLACED
ON
LOCAL
UTILITIES
ENERGY
RELATED
CONCERNS
ARE
SEVERE
ENOUGH
THAT
COMPANIES
LIKE
GOOGLE
ARE
STARTING
TO
BUILD
DATA
CENTERS
CLOSE
TO
ELECTRIC
PLANTS
IN
COLD
WEATHER
CLI
MATES
ALL
THESE
CONCERNS
HAVE
LED
TO
IMPROVEMENTS
IN
COOLING
INFRASTRUCTURE
AND
IN
SERVER
POWER
CONSUMPTION
FOR
MOBILE
DEVICES
BATTERY
CAPACITY
AND
ENERGY
USE
DI
RECTLY
AFFECT
USABILITY
BATTERY
CAPACITY
DETERMINES
HOW
LONG
DEVICES
LAST
CONSTRAINS
FORM
FACTORS
AND
LIMITS
FUNCTIONAL
ITY
SINCE
BATTERY
CAPACITY
IS
LIMITED
AND
IMPROVING
SLOWLY
DEVICE
ARCHITECTS
HAVE
CONCENTRATED
ON
EXTRACTING
GREATER
ENERGY
EFFICIENCY
FROM
THE
UNDERLYING
COMPONENTS
SUCH
AS
THE
PROCESSOR
THE
DISPLAY
AND
THE
WIRELESS
SUBSYSTEMS
IN
ISOLATION
TO
DRIVE
ENERGY
EFFICIENCY
IMPROVEMENTS
WE
NEED
BENCH
MARKS
TO
ASSESS
THEIR
EFFECTIVENESS
UNFORTUNATELY
THERE
HAS
BEEN
NO
FOCUS
ON
A
COMPLETE
BENCHMARK
INCLUDING
A
WORK
LOAD
METRIC
AND
GUIDELINES
TO
GAUGE
THE
EFFICACY
OF
ENERGY
OPTIMIZATIONS
FROM
A
WHOLE
SYSTEM
PERSPECTIVE
SOME
EFFORTS
ARE
UNDER
WAY
TO
ESTABLISH
BENCHMARKS
FOR
ENERGY
EFFICIENCY
IN
DATA
CENTERS
BUT
ARE
INCOMPLETE
OTHER
WORK
HAS
EMPHASIZED
METRICS
SUCH
AS
THE
ENERGY
DELAY
PRODUCT
OR
PER
FORMANCE
PER
WATT
TO
CAPTURE
ENERGY
EFFICIENCY
FOR
PROCES
SORS
AND
SERVERS
WITHOUT
FIXING
A
WORKLOAD
MOREOVER
WHILE
PAST
EMPHASIS
ON
PROCESSOR
ENERGY
EFFICIENCY
HAS
LED
TO
IMPROVEMENTS
IN
OVERALL
POWER
CONSUMPTION
THERE
HAS
BEEN
LITTLE
FOCUS
ON
THE
I
O
SUBSYSTEM
WHICH
PLAYS
A
SIGNIFICANT
ROLE
IN
TOTAL
SYSTEM
POWER
FOR
MANY
IMPORTANT
WORKLOADS
AND
SYSTEMS
IN
THIS
PAPER
WE
PROPOSE
JOULESORT
AS
A
HOLISTIC
BENCH
MARK
TO
DRIVE
THE
DESIGN
OF
ENERGY
EFFICIENT
SYSTEMS
JOULE
SORT
USES
THE
SAME
WORKLOAD
AS
THE
OTHER
EXTERNAL
SORT
BENCH
MARKS
BUT
ITS
METRIC
INCORPORATES
TOTAL
ENERGY
WHICH
IS
A
COMBINATION
OF
POWER
CONSUMPTION
AND
PERFOR
MANCE
THE
BENCHMARK
CAN
BE
SUMMARIZED
AS
FOLLOWS
SORT
A
FIXED
NUMBER
OF
RANDOMLY
PERMUTED
BYTE
RECORDS
WITH
BYTE
KEYS
THE
SORT
MUST
START
WITH
INPUT
IN
A
FILE
ON
NON
VOLATILE
STORE
AND
FINISH
WITH
OUTPUT
IN
A
FILE
ON
NON
VOLATILE
STORE
THERE
ARE
THREE
SCALE
CATEGORIES
FOR
JOULESORT
AND
RECORDS
THE
WINNER
IN
EACH
CATEGORY
IS
THE
SYSTEM
WITH
THE
MINIMUM
TOTAL
ENERGY
USE
WE
CHOOSE
SORT
AS
THE
WORKLOAD
FOR
THE
SAME
BASIC
REA
SON
THAT
THE
TERABYTE
SORT
MINUTESORT
PENNYSORT
AND
PERFORMANCE
PRICE
SORT
BENCHMARKS
DO
IT
IS
SIMPLE
TO
STATE
AND
BALANCES
SYSTEM
COMPONENT
USE
SORT
STRESSES
ALL
CORE
COMPONENTS
OF
A
SYSTEM
MEMORY
CPU
AND
I
O
SORT
ALSO
EXERCISES
THE
OS
AND
FILESYSTEM
SORT
IS
A
PORTABLE
WORKLOAD
IT
IS
APPLICABLE
TO
A
VARIETY
OF
SYSTEMS
FROM
MOBILE
DEVICES
TO
LARGE
SERVER
CONFIGURATIONS
ANOTHER
NATURAL
REASON
FOR
CHOOSING
SORT
IS
THAT
IT
REPRESENTS
SEQUEN
TIAL
I
O
TASKS
IN
DATA
MANAGEMENT
WORKLOADS
JOULESORT
IS
AN
I
O
CENTRIC
BENCHMARK
THAT
MEASURES
THE
ENERGY
EFFICIENCY
OF
SYSTEMS
AT
PEAK
USE
LIKE
PREVIOUS
SORT
BENCHMARKS
ONE
OF
ITS
GOALS
IS
TO
GAUGE
THE
END
TO
END
EF
FECTIVENESS
OF
IMPROVEMENTS
IN
SYSTEM
COMPONENTS
TO
DO
SO
JOULESORT
ALLOWS
US
TO
COMPARE
THE
ENERGY
EFFICIENCIES
OF
A
VARIETY
OF
DISPARATE
SYSTEM
CONFIGURATIONS
BECAUSE
OF
THE
SIMPLICITY
AND
PORTABILITY
OF
SORT
PREVIOUS
SORT
BENCH
MARKS
HAVE
BEEN
TECHNOLOGY
TREND
BELLWETHERS
FOR
EXAMPLE
FORESHADOWING
THE
TRANSITION
FROM
SUPERCOMPUTERS
TO
CLUS
TERS
SIMILARLY
AN
IMPORTANT
PURPOSE
OF
JOULESORT
IS
TO
CHART
PAST
TRENDS
AND
GAIN
INSIGHT
INTO
FUTURE
TRENDS
IN
ENERGY
EF
FICIENCY
BEYOND
THE
BENCHMARK
DEFINITION
OUR
MAIN
CONTRIBUTIONS
ARE
TWOFOLD
FIRST
WE
MOTIVATE
AND
DESCRIBE
PITFALLS
SUR
ROUNDING
THE
CREATION
OF
A
FAIR
ENERGY
EFFICIENCY
BENCHMARK
WE
JUSTIFY
OUR
FAIREST
FORMULATION
WHICH
INCLUDES
THREE
SCALE
FACTORS
THAT
CORRESPOND
NATURALLY
TO
THE
DOMINANT
CLASSES
OF
SYSTEMS
FOUND
TODAY
MOBILE
DESKTOP
AND
SERVER
AL
THOUGH
WE
SUPPORT
BOTH
DAYTONA
COMMERCIALLY
SUPPORTED
AND
INDY
NO
HOLDS
BARRED
CATEGORIES
FOR
EACH
SCALE
WE
CONCENTRATE
ON
DAYTONA
SYSTEMS
IN
THIS
PAPER
SECOND
WE
PRESENT
THE
WINNING
JOULESORT
SYSTEM
THAT
IS
OVER
MORE
EFFICIENT
SORTEDRECS
JOULE
FOR
THAN
LAST
YEAR
ESTIMATED
WINNER
SORTEDRECS
JOULE
FOR
THIS
SYSTEM
SHOWS
THAT
A
FOCUS
ON
ENERGY
EFFI
CIENCY
LEADS
TO
A
UNIQUE
CONFIGURATION
THAT
IS
HARD
TO
FIND
PRE
ASSEMBLED
OUR
WINNER
BALANCES
A
LOW
POWER
MOBILE
PROCESSOR
WITH
NUMEROUS
LAPTOP
DISKS
CONNECTED
VIA
SERVER
CLASS
PCI
E
I
O
CARDS
AND
USES
A
COMMERCIAL
SORT
NSORT
THE
REST
OF
THE
PAPER
IS
ORGANIZED
AS
FOLLOWS
IN
SECTION
WE
ESTIMATE
THE
ENERGY
EFFICIENCY
OF
PAST
SORT
BENCHMARK
WINNERS
WHICH
SUGGESTS
THAT
EXISTING
SORT
BENCHMARKS
CAN
NOT
SERVE
AS
SURROGATES
FOR
AN
ENERGY
EFFICIENCY
BENCHMARK
SECTION
DETAILS
THE
CRITERIA
AND
CHALLENGES
IN
DESIGNING
JOULESORT
AND
LISTS
ISSUES
AND
GUIDELINES
FOR
PROPER
ENERGY
MEASUREMENT
IN
SECTION
WE
MEASURE
THE
ENERGY
CON
SUMPTION
OF
UNBALANCED
AND
BALANCED
SYSTEMS
TO
MOTIVATE
OUR
CHOICES
IN
DESIGNING
OUR
WINNING
SYSTEM
THE
BALANCED
SYSTEM
SHOWS
THAT
THE
I
O
SUBSYSTEM
IS
A
SIGNIFICANT
PART
OF
TOTAL
POWER
SECTION
PROVIDES
AN
IN
DEPTH
STUDY
OF
OUR
JOULE
SORT
SYSTEM
USING
NSORT
IN
PARTICULAR
WE
SHOW
THAT
THE
MOST
ENERGY
EFFICIENT
COST
EFFECTIVE
AND
BEST
PERFORMING
CONFIGURATION
FOR
THIS
SYSTEM
IS
WHEN
THE
SORT
IS
CPU
BOUND
YEAR
FIGURE
ESTIMATED
ENERGY
EFFICIENCY
OF
PREVIOUS
WINNERS
OF
SORT
BENCHMARKS
WE
ALSO
FIND
THAT
BOTH
THE
CHOICE
OF
FILESYSTEM
AND
IN
MEMORY
SORTING
ALGORITHM
AFFECT
ENERGY
EFFICIENCY
SECTION
DISCUSSES
THE
RELATED
WORK
AND
SECTION
PRESENTS
LIMITATIONS
AND
FU
TURE
DIRECTIONS
HISTORICAL
TRENDS
IN
THIS
SECTION
WE
SEEK
TO
UNDERSTAND
IF
ANY
OF
THE
EXIST
ING
SORT
BENCHMARKS
CAN
SERVE
AS
A
SURROGATE
FOR
AN
ENERGY
EFFICIENCY
BENCHMARK
TO
DO
SO
WE
FIRST
ESTIMATE
THE
SORT
EDRECS
JOULE
RATIO
A
MEASURE
OF
ENERGY
EFFICIENCY
OF
THE
PAST
DECADE
SORT
BENCHMARK
WINNERS
THIS
ANALYSIS
REVEALS
THAT
THE
ENERGY
EFFICIENCY
OF
SYSTEMS
DESIGNED
FOR
PURE
PER
FORMANCE
I
E
MINUTESORT
TERABYTE
SORT
AND
DATAMATION
WINNERS
HAS
IMPROVED
SLOWLY
MOREOVER
SYSTEMS
DESIGNED
FOR
PRICE
PERFORMANCE
I
E
PENNYSORT
WINNERS
ARE
COMPAR
ATIVELY
MORE
ENERGY
EFFICIENT
AND
THEIR
ENERGY
EFFICIENCY
IS
GROWING
RAPIDLY
HOWEVER
SINCE
OUR
JOULESORT
SYS
TEM
ENERGY
EFFICIENCY
IS
WELL
BEYOND
WHAT
GROWTH
RATES
WOULD
PREDICT
FOR
THIS
YEAR
PENNYSORT
WINNER
WE
CONCLUDE
THAT
EXISTING
SORT
BENCHMARKS
DO
NOT
INHERENTLY
PROVIDE
AN
INCENTIVE
TO
OPTIMIZE
FOR
ENERGY
EFFICIENCY
SUPPORTING
THE
NEED
FOR
JOULESORT
METHODOLOGY
FIGURE
SHOWS
THE
ESTIMATED
SORTEDRECS
JOULE
METRIC
FOR
THE
PAST
SORT
BENCHMARK
WINNERS
SINCE
WE
COMPUTE
THESE
METRICS
FROM
THE
PUBLISHED
PERFORMANCE
RECORDS
AND
OUR
OWN
ESTIMATES
OF
POWER
CONSUMPTION
SINCE
ENERGY
USE
WAS
NOT
REPORTED
WE
OBTAIN
THE
PERFORMANCE
RECORDS
AND
HARDWARE
CONFIGURATION
INFORMATION
FROM
THE
SORT
BENCH
MARK
WEBSITE
AND
THE
WINNERS
POSTED
REPORTS
WE
ESTIMATE
TOTAL
ENERGY
DURING
SYSTEM
USE
WITH
A
STRAIGHT
FORWARD
APPROACH
FROM
THE
POWER
MANAGEMENT
COMMUNITY
SINCE
CPU
MEMORY
AND
DISK
ARE
USUALLY
THE
MAIN
POWER
CONSUMING
SYSTEM
COMPONENTS
WE
USE
INDIVIDUAL
ESTIMATES
OF
THESE
TO
COMPUTE
TOTAL
POWER
FOR
MEMORY
AND
DISKS
WE
USE
THE
HP
ENTERPRISE
CONFIGURATOR
POWER
CALCU
LATOR
TO
YIELD
A
FIXED
POWER
OF
PER
DISK
AND
PER
DIMM
SOME
OF
THE
SORT
BENCHMARK
REPORTS
ONLY
MENTION
TOTAL
MEMORY
CAPACITY
AND
NOT
THE
NUMBER
OF
DIMMS
IN
THOSE
CASES
WE
ASSUME
A
DIMM
SIZE
APPROPRIATE
TO
THE
ERA
OF
THE
REPORT
THE
MAXIMUM
POWER
SPECS
FOR
CPUS
USUALLY
QUOTED
AS
THERMAL
DESIGN
POWER
TDP
ARE
MUCH
HIGHER
THAN
THE
PEAK
NUMBERS
SEEN
IN
COMMON
USE
THUS
WE
DERATE
THESE
POWER
RATINGS
BY
A
FACTOR
ALTHOUGH
A
BIT
CON
SERVATIVE
THIS
APPROACH
ALLOWS
REASONABLE
APPROXIMATIONS
FOR
A
VARIETY
OF
SYSTEMS
WHEN
UNCERTAIN
WE
ASSUME
THE
NEWEST
POSSIBLE
GENERATION
OF
THE
REPORTED
PROCESSOR
AS
OF
THE
SORT
BENCHMARK
RECORD
BECAUSE
A
GIVEN
CPU
POWER
CONSUMPTION
IMPROVES
WITH
SHRINKING
FEATURE
SIZES
FINALLY
TO
ACCOUNT
FOR
POWER
SUPPLIES
INEFFICIENCIES
WHICH
CAN
VARY
WIDELY
AND
OTHER
COMPONENTS
WE
SCALE
TOTAL
SYSTEM
POWER
DERIVED
FROM
COMPONENT
LEVEL
ESTIMATES
BY
FOR
SINGLE
NODE
SYSTEMS
WE
USE
A
HIGHER
FACTOR
FOR
CLUSTERS
TO
ACCOUNT
FOR
ADDITIONAL
COMPONENTS
SUCH
AS
NETWORKING
MANAGEMENT
HARDWARE
AND
REDUNDANT
POWER
SUPPLIES
OUR
POWER
ESTIMATES
ARE
INTENDED
TO
ILLUMINATE
COARSE
HIS
TORICAL
TRENDS
AND
ARE
ACCURATE
ENOUGH
TO
SUPPORT
THE
HIGH
LEVEL
CONCLUSIONS
IN
THIS
SECTION
WE
EXPERIMENTALLY
VALI
DATED
THIS
APPROACH
AGAINST
SOME
SERVER
AND
DESKTOP
CLASS
SYSTEMS
AND
ITS
ACCURACY
WAS
BETWEEN
AND
ANALYSIS
ALTHOUGH
PREVIOUS
SORT
BENCHMARK
WINNERS
WERE
NOT
CON
FIGURED
WITH
POWER
CONSUMPTION
IN
MIND
THEY
ROUGHLY
RE
FLECT
THE
POWER
CHARACTERISTICS
OF
DESKTOP
AND
HIGHER
END
SYS
TEMS
IN
THEIR
DAY
THUS
FROM
THE
DATA
IN
FIGURE
WE
CAN
IN
FER
QUALITATIVE
INFORMATION
ABOUT
THE
RELATIVE
IMPROVEMENTS
IN
PERFORMANCE
PRICE
PERFORMANCE
AND
ENERGY
EFFICIENCY
IN
THE
LAST
DECADE
FIGURE
COMPARES
THE
ENERGY
EFFICIENCY
OF
PREVIOUS
SORT
WINNERS
USING
THE
SORTEDRECS
JOULE
RATIO
AND
SUPPORTS
THE
FOLLOWING
OBSERVATIONS
SYSTEMS
OPTIMIZED
FOR
PRICE
PERFORMANCE
I
E
PENNYSORT
WINNERS
CLEARLY
ARE
MORE
ENERGY
EFFICIENT
THAN
THE
OTHER
SORT
BENCHMARK
WINNERS
WHICH
WERE
OPTIMIZED
FOR
PURE
PERFOR
MANCE
THERE
ARE
TWO
REASONS
FOR
THIS
EFFECT
FIRST
THE
PRICE
PERFORMANCE
METRIC
MOTIVATES
SYSTEM
DESIGNERS
TO
USE
FEWER
COMPONENTS
AND
THUS
LESS
POWER
SECOND
IT
PROVIDES
INCENTIVE
TO
USE
CHEAPER
COMMODITY
COMPONENTS
WHICH
FOR
A
GIVEN
PERFORMANCE
POINT
TRADITIONALLY
HAVE
USED
LESS
EN
ERGY
THAN
EXPENSIVE
HIGH
PERFORMANCE
COMPONENTS
THE
ENERGY
EFFICIENCY
OF
COST
CONSCIOUS
SYSTEMS
HAS
IM
PROVED
FASTER
THAN
THAT
OF
PERFORMANCE
OPTIMIZED
SYSTEMS
WHICH
HAVE
HARDLY
IMPROVED
OTHERS
HAVE
ALSO
OBSERVED
A
FLAT
ENERGY
EFFICIENCY
TREND
FOR
CLUSTER
HARDWARE
MUCH
OF
THE
GROWTH
IN
THE
PENNYSORT
CURVE
IS
FROM
THE
LAST
TWO
INDY
WINNERS
WHICH
HAVE
MADE
LARGE
LEAPS
IN
ENERGY
EFFICIENCY
IN
ALGORITHMIC
IMPROVEMENTS
AND
A
MINIMAL
HARDWARE
CONFIGURATION
PLAYED
A
ROLE
IN
THIS
IMPROVEMENT
BUT
MOST
IMPORTANTLY
CPU
DESIGN
TRENDS
HAD
FINALLY
SWUNG
TOWARD
ENERGY
EFFICIENCY
THE
PROCESSOR
USED
IN
THE
PENNYSORT
WINNER
HAS
THE
CLOCK
FREQUENCY
OF
ITS
IMMEDIATE
PREDE
CESSOR
WHILE
ONLY
CONSUMING
THE
POWER
OVERALL
THE
SORT
HAD
BETTER
PERFORMANCE
THAN
THE
PREVIOUS
DATA
POINT
WHILE
USING
THE
POWER
THE
PENNYSORT
WIN
NER
GPUTERASORT
INCREASED
ENERGY
EFFICIENCY
BY
INTRODUC
ING
A
NEW
SYSTEM
COMPONENT
THE
GRAPHICS
PROCESSING
UNIT
GPU
AND
UTILIZING
IT
VERY
EFFECTIVELY
THE
CHOSEN
GPU
IS
INEXPENSIVE
AND
COMPARABLE
IN
POWER
CONSUMPTION
TO
THE
CPU
BUT
IT
PROVIDES
BETTER
STREAMING
MEMORY
BANDWIDTH
THAN
THE
CPU
THIS
LATEST
WINNER
IN
PARTICULAR
SHOWS
THE
DANGER
OF
RELY
ING
ON
ENERGY
BENCHMARKS
THAT
FOCUS
ONLY
ON
SPECIFIC
HARD
WARE
LIKE
CPU
OR
DISKS
RATHER
THAN
END
TO
END
EFFICIENCY
SUCH
SPECIFIC
BENCHMARKS
WOULD
ONLY
DRIVE
AND
TRACK
IM
TABLE
THIS
TABLE
SHOWS
THE
ESTIMATED
YEARLY
GROWTH
IN
PURE
PERFORMANCE
PRICE
PERFORMANCE
AND
ENERGY
EFFICIENCY
OF
PAST
WINNERS
PROVEMENTS
OF
EXISTING
TECHNOLOGIES
AND
MAY
FAIL
TO
ANTICI
PATE
THE
USE
OF
POTENTIALLY
DISRUPTIVE
TECHNOLOGIES
SINCE
PRICE
PERFORMANCE
WINNERS
ARE
MORE
ENERGY
EFFICIENT
WE
NEXT
EXAMINE
WHETHER
THE
MOST
COST
EFFECTIVE
SORT
IMPLIES
THE
BEST
ACHIEVABLE
ENERGY
EFFICIENT
SORT
TO
DO
SO
WE
FIRST
ESTIMATE
THE
GROWTH
RATE
OF
SORT
WINNERS
ALONG
MULTIPLE
DI
MENSIONS
TABLE
SHOWS
THE
GROWTH
RATE
OF
PAST
SORT
BENCH
MARK
WINNERS
ALONG
THREE
DIMENSIONS
PERFORMANCE
SORT
EDRECS
SEC
PRICE
PERFORMANCE
SORTEDRECS
AND
ENERGY
EFFICIENCY
SORTEDRECS
JOULE
WE
SEPARATE
THE
GROWTH
RATES
INTO
TWO
CATEGORIES
BASED
ON
THE
BENCHMARK
OPTIMIZATION
GOAL
PRICE
OR
PURE
PERFORMANCE
SINCE
THE
GOAL
DRIVES
THE
SYSTEM
DESIGN
FOR
EACH
CATEGORY
WE
CALCULATE
THE
GROWTH
RATE
AS
FOLLOWS
WE
CHOOSE
THE
BEST
SYSTEM
ACCORDING
TO
THE
METRIC
IN
EACH
YEAR
AND
FIT
THE
RESULT
WITH
AN
EXPONENTIAL
TABLE
SHOWS
THAT
PENNYSORT
SYSTEMS
ARE
IMPROVING
AL
MOST
AT
THE
PACE
OF
MOORE
LAW
ALONG
THE
PERFORMANCE
AND
PRICE
PERFORMANCE
DIMENSIONS
THE
PURE
PERFORMANCE
SYS
TEMS
HOWEVER
ARE
IMPROVING
MUCH
MORE
SLOWLY
AS
NOTED
ELSEWHERE
MORE
IMPORTANTLY
OUR
ANALYSIS
SHOWS
MUCH
SLOWER
ESTI
MATED
GROWTH
IN
ENERGY
EFFICIENCY
THAN
IN
THE
OTHER
TWO
METRICS
FOR
BOTH
BENCHMARK
CATEGORIES
GIVEN
LAST
YEAR
ESTIMATED
PENNYSORT
WINNER
PROVIDES
SRECS
J
OUR
CURRENT
JOULESORT
WINNER
AT
SRECS
J
IS
NEARLY
THE
EXPECTED
VALUE
OF
SRECS
J
FOR
THIS
YEAR
THIS
RESULT
SUGGESTS
THAT
WE
NEED
A
BENCHMARK
FOCUSED
ON
EN
ERGY
EFFICIENCY
TO
PROMOTE
DEVELOPMENT
OF
THE
MOST
ENERGY
EFFICIENT
SORTING
SYSTEMS
AND
ALLOW
FOR
DISRUPTIVE
TECHNOLOGIES
IN
ENERGY
EFFICIENCY
IRRESPECTIVE
OF
COST
BENCHMARK
DESIGN
IN
THIS
SECTION
WE
DETAIL
THE
CRITERIA
AND
CHALLENGES
IN
DE
SIGNING
AN
ENERGY
EFFICIENCY
BENCHMARK
WE
DESCRIBE
SOME
OF
THE
PITFALLS
OF
OUR
INITIAL
SPECIFICATIONS
AND
HOW
THE
BENCH
MARK
HAS
EVOLVED
WE
ALSO
SPECIFY
RULES
OF
THE
BENCHMARK
WITH
RESPECT
TO
BOTH
WORKLOAD
AND
ENERGY
MEASUREMENT
CRITERIA
ALTHOUGH
PAST
STUDIES
HAVE
PROPOSED
ENERGY
EFFICIENCY
MET
RICS
OR
POWER
MEASUREMENT
TECHNIQUES
NONE
PROVIDE
A
COMPLETE
BENCHMARK
A
WORKLOAD
A
METRIC
OF
COMPARISON
AND
RULES
FOR
RUNNING
THE
WORKLOAD
AND
MEA
SURING
ENERGY
CONSUMPTION
MOREOVER
THESE
STUDIES
TRADI
TIONALLY
HAVE
FOCUSED
ON
COMPARING
EXISTING
SYSTEMS
RATHER
THAN
PROVIDING
INSIGHT
INTO
FUTURE
TECHNOLOGY
TRENDS
WE
SET
OUT
TO
DESIGN
AN
ENERGY
ORIENTED
BENCHMARK
THAT
ADDRESSES
THESE
DRAWBACKS
WITH
THE
CRITERIA
BELOW
IN
MIND
WHILE
ACHIEVING
ALL
THESE
CRITERIA
SIMULTANEOUSLY
IS
HARD
WE
STRIVE
TO
ENCOMPASS
THEM
AS
MUCH
AS
POSSIBLE
ENERGY
EFFICIENCY
THE
BENCHMARK
SHOULD
MEASURE
A
SYS
TEM
BANG
FOR
THE
BUCK
WHERE
BANG
IS
WORK
DONE
AND
THE
COST
REFLECTS
SOME
MEASURE
OF
POWER
USE
E
G
AVERAGE
POWER
PEAK
POWER
TOTAL
ENERGY
AND
ENERGY
DELAY
TO
DRIVE
PRACTICAL
IMPROVEMENTS
IN
POWER
CONSUMPTION
COST
SHOULD
REFLECT
BOTH
A
SYSTEM
PERFORMANCE
AND
POWER
USE
A
SYS
TEM
THAT
USES
ALMOST
NO
POWER
BUT
TAKES
FOREVER
TO
COMPLETE
A
TASK
IS
NOT
PRACTICAL
SO
AVERAGE
AND
PEAK
POWER
ARE
POOR
CHOICES
THUS
THERE
ARE
TWO
REASONABLE
COST
ALTERNATIVES
ENERGY
A
PRODUCT
OF
EXECUTION
TIME
AND
POWER
OR
ENERGY
DELAY
A
PRODUCT
OF
EXECUTION
TIME
AND
ENERGY
THE
FORMER
WEIGHS
PERFORMANCE
AND
POWER
EQUALLY
WHILE
THE
LATTER
POP
ULAR
IN
CPU
CENTRIC
BENCHMARKS
PLACES
MORE
EMPHASIS
ON
PERFORMANCE
SINCE
THERE
ARE
OTHER
SORT
BENCHMARKS
THAT
EMPHASIZE
PERFORMANCE
WE
CHOSE
ENERGY
AS
THE
COST
PEAK
USE
A
BENCHMARK
CAN
CONSIDER
SYSTEM
ENERGY
IN
THREE
IMPORTANT
MODES
IDLE
PEAK
USE
OR
A
REALISTIC
COMBI
NATION
OF
THE
TWO
ALTHOUGH
MINIMIZING
IDLE
MODE
POWER
IS
USEFUL
EVALUATING
THIS
MODE
IS
STRAIGHTFORWARD
REAL
WORLD
WORKLOADS
ARE
OFTEN
A
COMBINATION
BUT
DESIGNING
A
BROAD
BENCHMARK
THAT
ADDRESSES
A
NUMBER
OF
SCENARIOS
IS
DIFFICULT
TO
IMPOSSIBLE
HENCE
WE
CHOSE
TO
FOCUS
OUR
BENCH
MARK
ON
AN
IMPORTANT
BUT
SIMPLER
CASE
ENERGY
EFFICIENCY
DURING
PEAK
USE
ENERGY
EFFICIENCY
AT
PEAK
IS
THE
OPPOSITE
EXTREME
FROM
IDLE
AND
GIVES
AN
UPPER
BOUND
ON
WORK
THAT
CAN
BE
DONE
FOR
A
GIVEN
ENERGY
THIS
OPERATING
POINT
INFLU
ENCES
DESIGN
AND
PROVISIONING
CONSTRAINTS
FOR
DATA
CENTERS
AS
WELL
AS
MOBILE
DEVICES
IN
ADDITION
FOR
SOME
APPLICATIONS
E
G
SCIENTIFIC
COMPUTING
NEAR
PEAK
USE
CAN
BE
THE
NORM
HOLISTIC
AND
BALANCED
A
SINGLE
COMPONENT
CANNOT
ACCU
RATELY
REFLECT
THE
OVERALL
PERFORMANCE
AND
POWER
CHARACTER
ISTICS
OF
A
SYSTEM
THEREFORE
THE
WORKLOAD
SHOULD
EXERCISE
ALL
CORE
COMPONENTS
AND
STRESS
THEM
ROUGHLY
EQUALLY
THE
BENCHMARK
METRICS
SHOULD
INCORPORATE
ENERGY
USED
BY
ALL
CORE
COMPONENTS
INCLUSIVE
AND
PORTABLE
WE
WANT
TO
ASSESS
THE
ENERGY
EF
FICIENCIES
OF
A
WIDE
VARIETY
OF
SYSTEMS
PDAS
LAPTOPS
DESK
TOPS
SERVERS
CLUSTERS
ETC
THUS
THE
BENCHMARK
SHOULD
INCLUDE
AS
MANY
ARCHITECTURES
AS
POSSIBLE
AND
BE
AS
UNBI
ASED
AS
POSSIBLE
IT
SHOULD
ALLOW
INNOVATIONS
IN
HARDWARE
AND
SOFTWARE
TECHNOLOGY
MOREOVER
THE
WORKLOAD
SHOULD
BE
IMPLEMENTABLE
AND
MEANINGFUL
ACROSS
THESE
PLATFORMS
HISTORY
PROOF
IN
ORDER
TO
TRACK
IMPROVEMENTS
OVER
GEN
ERATIONS
OF
SYSTEMS
AND
IDENTIFY
FUTURE
PROFITABLE
DIRECTIONS
WE
WANT
THE
BENCHMARK
SPECIFICATION
TO
REMAIN
MEANINGFUL
AND
COMPARABLE
AS
TECHNOLOGY
EVOLVES
REPRESENTATIVE
AND
SIMPLE
THE
BENCHMARK
SHOULD
BE
REPRESENTATIVE
OF
AN
IMPORTANT
CLASS
OF
WORKLOADS
ON
THE
SYS
TEMS
TESTED
IT
SHOULD
ALSO
BE
EASY
TO
SET
UP
EXECUTE
AND
ADMINISTER
WORKLOAD
WE
BEGIN
WITH
EXTERNAL
SORT
AS
SPECIFIED
IN
THE
PREVIOUS
SORT
BENCHMARKS
AS
THE
WORKLOAD
BECAUSE
IT
COVERS
MOST
OF
OUR
CRITERIA
THE
TASK
IS
TO
SORT
A
FILE
CONTAINING
RANDOMLY
PERMUTED
BYTE
RECORDS
WITH
BYTE
KEYS
THE
INPUT
FILE
MUST
BE
READ
FROM
AND
THE
OUTPUT
FILE
WRITTEN
TO
A
NON
VOLATILE
STORE
AND
ALL
INTERMEDIATE
FILES
MUST
BE
DELETED
THE
OUTPUT
FILE
MUST
BE
NEWLY
CREATED
IT
CANNOT
OVERWRITE
THE
INPUT
FILE
THIS
WORKLOAD
IS
REPRESENTATIVE
BECAUSE
MOST
PLATFORMS
FROM
LARGE
TO
SMALL
MUST
MANAGE
AN
EVER
INCREASING
SUP
PLY
OF
DATA
TO
DO
SO
THEY
ALL
PERFORM
SOME
TYPE
OF
I
O
CENTRIC
TASKS
CRITICAL
FOR
THEIR
USE
FOR
EXAMPLE
LARGE
SCALE
WEBSITES
RUN
PARALLEL
ANALYZES
OVER
VOLUMINOUS
LOG
DATA
ACROSS
THOUSANDS
OF
MACHINES
LAPTOPS
AND
SERVERS
CON
TAIN
VARIOUS
KINDS
OF
FILESYSTEMS
AND
DATABASES
CELL
PHONES
PDAS
AND
CAMERAS
STORE
RETRIEVE
AND
PROCESS
MULTIMEDIA
DATA
FROM
FLASH
MEMORY
WITH
PREVIOUS
SORT
IMPLEMENTATIONS
ON
CLUSTERS
SUPER
COMPUTERS
SMPS
AND
PCS
AS
EVIDENCE
WE
BELIEVE
SORT
IS
PORTABLE
AND
INCLUSIVE
IT
STRESSES
I
O
MEMORY
AND
THE
CPU
MAKING
IT
HOLISTIC
AND
BALANCED
MOREOVER
THE
FASTEST
SORTS
TEND
TO
RUN
MOST
COMPONENTS
AT
NEAR
PEAK
UTILIZATION
SO
SORT
IS
NOT
AN
IDLE
STATE
BENCHMARK
FINALLY
THIS
WORK
LOAD
IS
RELATIVELY
HISTORY
PROOF
WHILE
THE
PARAMETERS
HAVE
CHANGED
OVER
TIME
THE
ESSENTIAL
SORTING
TASK
HAS
BEEN
THE
SAME
SINCE
THE
ORIGINAL
DATAMATIONSORT
BENCHMARK
WAS
PROPOSED
IN
METRIC
AFTER
CHOOSING
THE
WORKLOAD
THE
NEXT
CHALLENGE
IS
CHOOS
ING
THE
METRIC
BY
WHICH
TO
EVALUATE
AND
COMPARE
DIFFERENT
SYSTEMS
THERE
ARE
MANY
WAYS
TO
DEFINE
A
SINGLE
METRIC
THAT
TAKES
BOTH
POWER
AND
PERFORMANCE
INTO
ACCOUNT
WE
LIST
SOME
ALTERNATIVES
THAT
WE
REJECTED
DESCRIBE
WHY
THEY
ARE
INAPPROPRIATE
AND
CHOOSE
THE
ONE
MOST
CONSISTENT
WITH
THE
CRITERIA
PRESENTED
IN
SECTION
FIXED
ENERGY
BUDGET
THE
MOST
INTUITIVE
EXTENSION
OF
MINUTESORT
AND
PENNYSORT
IS
TO
FIX
A
BUDGET
FOR
ENERGY
CONSUMPTION
AND
THEN
COM
PARE
THE
NUMBER
OF
RECORDS
SORTED
BY
DIFFERENT
SYSTEMS
WHILE
STAYING
WITHIN
THAT
ENERGY
BUDGET
THIS
APPROACH
HAS
TWO
DRAWBACKS
FIRST
THE
POWER
CONSUMPTION
OF
CURRENT
PLAT
FORMS
VARIES
BY
SEVERAL
ORDERS
OF
MAGNITUDE
LESS
THAN
FOR
HANDHELDS
TO
OVER
FOR
SERVERS
AND
MUCH
MORE
FOR
CLUSTERS
OR
SUPERCOMPUTERS
IF
THE
FIXED
ENERGY
BUDGET
IS
TOO
SMALL
LARGER
CONFIGURATIONS
CAN
ONLY
SORT
FOR
A
FRAC
TION
OF
A
SECOND
IF
THE
ENERGY
BUDGET
IS
MORE
APPROPRIATE
TO
LARGER
CONFIGURATIONS
SMALLER
CONFIGURATIONS
WOULD
RUN
OUT
OF
EXTERNAL
STORAGE
TO
BE
FAIR
AND
INCLUSIVE
WE
WOULD
NEED
MULTIPLE
BUDGETS
AND
CATEGORIES
FOR
DIFFERENT
CLASSES
OF
SYSTEMS
SECOND
AND
MORE
IMPORTANTLY
FROM
A
PRACTICAL
BENCH
MARKING
PERSPECTIVE
FINDING
THE
NUMBER
OF
RECORDS
TO
FIT
INTO
AN
ENERGY
BUDGET
IS
A
NON
TRIVIAL
TASK
DUE
TO
UNAVOID
ABLE
MEASUREMENT
ERROR
THERE
ARE
INACCURACIES
IN
SYNCHRO
NIZING
READINGS
FROM
A
POWER
METER
TO
THE
ACTUAL
RUNS
AND
FROM
THE
POWER
METER
ITSELF
FOR
THE
ONE
WE
USED
SINCE
ENERGY
IS
THE
PRODUCT
OF
POWER
AND
TIME
IT
IS
SUSCEP
TIBLE
TO
VARIATION
IN
BOTH
QUANTITIES
SO
THIS
CHOICE
IS
NOT
SIMPLE
FIXED
TIME
BUDGET
SIMILAR
TO
THE
MINUTE
AND
PERFORMANCE
PRICE
SORT
WE
CAN
FIX
A
TIME
BUDGET
E
G
ONE
MINUTE
WITHIN
WHICH
THE
GOAL
IS
TO
SORT
AS
MANY
RECORDS
AS
POSSIBLE
THE
WINNERS
FOR
THE
MINUTE
AND
PERFORMANCE
PRICE
SORTS
ARE
THOSE
WITH
THE
MIN
IMUM
TIME
AND
MAXIMUM
SORTEDRECS
RESPECTIVELY
SIM
ILARLY
OUR
FIRST
PROPOSAL
FOR
JOULESORT
SPECIFIED
MEASURING
ENERGY
AND
USED
SORTEDRECS
JOULE
AS
THE
RATIO
TO
MAXIMIZE
THERE
ARE
TWO
PROBLEMS
WITH
THIS
APPROACH
WHICH
ARE
ILLUSTRATED
BY
FIGURE
THIS
FIGURE
SHOWS
THE
SRECS
J
RA
TIO
FOR
VARYING
INPUT
SIZES
N
WITH
OUR
WINNING
JOULESORT
SYSTEM
WE
SEE
THAT
THE
RATIO
VARIES
CONSIDERABLY
WITH
N
THERE
ARE
TWO
DISTINCT
REGIONS
RECORDS
WHICH
RECORDS
SORTED
FIGURE
THIS
FIGURE
SHOWS
THE
BEST
MEASURED
EN
ERGY
EFFICIENCY
OF
OUR
WINNING
SYSTEM
AT
VARY
ING
INPUT
SIZES
CORRESPONDS
TO
PASS
SORTS
AND
RECORDS
WHICH
CORRESPONDS
TO
PASS
SORTS
TO
GET
THE
BEST
PERFORMANCE
FOR
PASS
SORTS
WE
STRIPE
THE
INPUT
AND
OUTPUT
ACROSS
DISKS
USING
AND
USE
DISKS
FOR
TEMPORARY
RUNS
FOR
PASS
SORTS
WE
STRIPE
THE
INPUT
AND
OUTPUT
ACROSS
DISKS
SEE
SECTION
FOR
MORE
SYSTEM
DETAILS
WITH
A
FIXED
TIME
BUDGET
APPROACH
THE
GOALS
OF
OUR
BENCHMARK
CAN
BE
UNDERMINED
IN
THE
FOLLOWING
WAYS
FOR
BOTH
ONE
AND
TWO
PASS
SORTS
SORT
PROGRESS
INCENTIVE
FIRST
IN
ANY
TIME
BUDGET
AP
PROACH
THERE
IS
NO
WAY
TO
ENFORCE
CONTINUAL
PROGRESS
SYS
TEMS
WILL
CONTINUE
SORTING
ONLY
IF
THE
MARGINAL
COST
OF
SORT
ING
AN
ADDITIONAL
RECORD
IS
LOWER
THAN
THE
COST
OF
SLEEPING
FOR
THE
REMAINING
TIME
THIS
TRADEOFF
BECOMES
PROBLEMATIC
WHEN
AN
ADDITIONAL
RECORD
MOVES
THE
SORT
FROM
PASS
TO
PASS
IN
THE
PASS
REGION
OF
FIGURE
THE
SORT
IS
I
O
LIM
ITED
SO
IT
DOES
NOT
RUN
TWICE
AS
FAST
AS
A
PASS
SORT
IT
GOES
FAST
ENOUGH
HOWEVER
TO
PROVIDE
ABOUT
BETTER
EFFICIENCY
THAN
PASS
SORTS
IF
THE
SYSTEM
WAS
DESIGNED
TO
HAVE
A
SUF
FICIENTLY
LOW
SLEEP
STATE
POWER
THEN
WITH
A
MINUTE
BUDGET
THE
BEST
APPROACH
WOULD
BE
TO
SORT
RECORDS
WHICH
TAKES
SEC
AND
SLEEP
FOR
THE
REMAINING
SEC
RE
SULTING
IN
A
BEST
SRECS
J
THUS
FOR
SOME
SYSTEMS
A
FIXED
TIME
BUDGET
DEFAULTS
INTO
ASSESSING
EFFICIENCY
WHEN
NO
WORK
IS
DONE
VIOLATING
OUR
CRITERIA
SORT
COMPLEXITY
SECOND
EVEN
IN
THE
PASS
REGION
TOTAL
ENERGY
IS
A
COMPLEX
FUNCTION
OF
MANY
PERFORMANCE
FACTORS
THAT
VARY
WITH
N
TOTAL
I
O
MEMORY
ACCESSES
COMPARISONS
CPU
UTILIZATION
AND
EFFECTIVE
PARALLELISM
FIGURE
SHOWS
THAT
ONCE
THE
SORT
BECOMES
CPU
BOUND
RECORDS
THE
SRECS
J
RATIO
TRENDS
SLOWLY
DOWNWARD
BECAUSE
TOTAL
EN
ERGY
INCREASES
SUPERLINEARLY
WITH
N
THE
RATIO
FOR
THE
LARGEST
SORT
IS
LOWER
THAN
THE
PEAK
THIS
DECREASE
IS
IN
PART
BECAUSE
SORTING
WORK
GROWS
AS
O
N
LG
N
DUE
TO
COMPAR
ISONS
AND
THE
O
NOTATION
HIDES
CONSTANTS
AND
LOWER
ORDER
OVERHEADS
THIS
EFFECT
IMPLIES
THAT
THE
METRIC
IS
BIASED
TO
WARD
SYSTEMS
THAT
SORT
FEWER
RECORDS
IN
THE
ALLOTTED
TIME
THAT
IS
EVEN
IF
TWO
FULLY
UTILIZED
SYSTEMS
A
AND
B
HAVE
SAME
TRUE
ENERGY
EFFICIENCY
AND
A
CAN
SORT
TWICE
AS
MANY
RECORDS
AS
B
IN
A
MINUTE
THE
SORTEDRECS
JOULE
RATIO
WILL
FAVOR
B
NOTE
SINCE
THIS
EFFECT
IS
SMALL
OUR
RELATIVE
COM
PARISONS
AND
CONCLUSIONS
IN
SECTION
REMAIN
VALID
OUR
CHOICE
FIXED
INPUT
SIZE
THE
FINAL
OPTION
THAT
WE
CONSIDERED
AND
SETTLED
UPON
WAS
TO
FIX
THE
NUMBER
OF
RECORDS
SORTED
AS
IN
THE
TERABYTE
SORT
BENCHMARK
AND
USE
TOTAL
ENERGY
AS
THE
METRIC
TO
MIN
IMIZE
FOR
THE
SAME
FAIRNESS
ISSUES
AS
IN
THE
FIXED
ENERGY
CASE
WE
DECIDED
TO
HAVE
THREE
SCALES
FOR
THE
INPUT
SIZE
AND
RECORDS
SIMILAR
TO
TPC
H
AND
DECLARE
WIN
NERS
IN
EACH
CATEGORY
FOR
CONSISTENCY
HENCEFORTH
WE
USE
MB
GB
AND
TB
FOR
AND
BYTES
RESPECTIVELY
FOR
A
FIXED
INPUT
SIZE
MINIMUM
ENERGY
AND
MAXIMUM
SORTE
DRECS
JOULE
ARE
EQUIVALENT
METRICS
IN
THIS
PAPER
WE
PREFER
THE
LATTER
BECAUSE
LIKE
AN
AUTOMOBILE
MILEAGE
RATING
IT
HIGHLIGHTS
ENERGY
EFFICIENCY
MORE
CLEARLY
THIS
APPROACH
HAS
ADVANTAGES
AND
DRAWBACKS
BUT
OFFERS
THE
BEST
COMPROMISE
GIVEN
OUR
CRITERIA
THESE
SCALES
COVER
A
LARGE
SPECTRUM
AND
NATURALLY
DIVIDE
THE
SYSTEMS
INTO
CLASSES
WE
EXPECT
LAPTOPS
DESKTOPS
AND
SERVERS
MOREOVER
SINCE
ENERGY
IS
A
PRODUCT
OF
POWER
AND
TIME
A
FIXED
WORK
AP
PROACH
IS
THE
SIMPLEST
FORMULATION
THAT
PROVIDES
AN
INCENTIVE
TO
OPTIMIZE
POWER
CONSUMPTION
AND
PERFORMANCE
BOTH
ARE
IMPORTANT
CONCERNS
FOR
CURRENT
COMPUTER
SYSTEMS
ONE
DISADVANTAGE
IS
THAT
AS
TECHNOLOGIES
IMPROVE
SCALES
MUST
BE
ADDED
AT
THE
HIGHER
END
AND
MAY
NEED
TO
BE
DEP
RECATED
AT
THE
LOWER
END
FOR
EXAMPLE
IF
THE
PERFORMANCE
OF
JOULESORT
WINNERS
IMPROVES
AT
THE
RATE
OF
MOORE
LAW
YEAR
A
SYSTEM
WHICH
SORTS
A
IN
SEC
TO
DAY
WOULD
ONLY
TAKE
SEC
IN
YEARS
ONCE
ALL
RELEVANT
SYSTEMS
REQUIRE
ONLY
A
FEW
SECONDS
FOR
A
SCALE
THAT
SCALE
BECOMES
OBSOLETE
SINCE
EVEN
THE
BEST
PERFORMING
SORTS
ARE
NOT
IMPROVING
WITH
MOORE
LAW
WE
EXPECT
THESE
SCALES
TO
BE
RELEVANT
FOR
AT
LEAST
YEARS
FINALLY
BECAUSE
COMPAR
ISON
ACROSS
SCALES
IS
MISLEADING
OUR
APPROACH
IS
NOT
FULLY
HISTORY
PROOF
CATEGORIES
AS
WITH
THE
OTHER
SORT
BENCHMARKS
WE
PRO
POSE
TWO
CATEGORIES
FOR
JOULESORT
DAYTONA
FOR
COMMER
CIALLY
SUPPORTED
SORTS
AND
INDY
FOR
NO
HOLDS
BARRED
IM
PLEMENTATIONS
SINCE
DAYTONA
SORTS
ARE
COMMERCIALLY
SUP
PORTED
THE
HARDWARE
COMPONENTS
MUST
BE
OFF
THE
SHELF
AND
UNMODIFIED
AND
RUN
A
COMMERCIALLY
SUPPORTED
OS
AS
WITH
THE
OTHER
SORT
BENCHMARKS
WE
EXPECT
ENTRANTS
TO
REPORT
THE
COST
OF
THE
SYSTEM
MEASURING
ENERGY
THERE
ARE
A
NUMBER
OF
ISSUES
SURROUNDING
THE
PROPER
AC
COUNTING
OF
ENERGY
USE
SPECIFIC
PROPOSALS
IN
THE
POWER
MANAGEMENT
COMMUNITY
FOR
MEASURING
ENERGY
ARE
BEING
DE
BATED
AND
ARE
STILL
UNTESTED
IN
THE
LARGE
ONCE
THESE
ARE
AGREED
UPON
WE
PLAN
TO
ADOPT
THE
RELEVANT
PORTIONS
FOR
THIS
BENCHMARK
AS
A
START
WE
PROPOSE
GUIDELINES
FOR
THREE
AREAS
THE
BOUNDARIES
OF
THE
SYSTEM
TO
BE
MEASURED
ENVI
RONMENTAL
CONSTRAINTS
AND
ENERGY
MEASUREMENT
SYSTEM
BOUNDARIES
OUR
AIM
IS
TO
ACCOUNT
FOR
ALL
EN
ERGY
CONSUMED
TO
POWER
THE
PHYSICAL
SYSTEM
EXECUTING
THE
SORT
ALL
POWER
IS
MEASURED
FROM
THE
WALL
AND
INCLUDES
ANY
CONVERSION
LOSSES
FROM
POWER
SUPPLIES
FOR
BOTH
AC
AND
DC
SYSTEMS
POWER
SUPPLIES
ARE
A
CRITICAL
COMPONENT
IN
DELIV
ERING
POWER
AND
IN
THE
PAST
HAVE
BEEN
NOTORIOUSLY
INEFFI
CIENT
SOME
DC
SYSTEMS
ESPECIALLY
MOBILE
DEVICES
CAN
RUN
FROM
BATTERIES
AND
THOSE
BATTERIES
MUST
EVENTUALLY
BE
RECHARGED
WHICH
ALSO
INCURS
CONVERSION
LOSS
WHILE
THE
LOSS
FROM
RECHARGING
MAY
BE
DIFFERENT
FROM
THE
LOSS
FROM
TABLE
THE
UNBALANCED
SYSTEMS
MEASURED
IN
EXPLORING
ENERGY
EFFICIENCY
TRADEOFFS
FOR
SORT
THE
ADAPTER
THAT
POWERS
A
DEVICE
DIRECTLY
FOR
SIMPLICITY
WE
ALLOW
MEASUREMENTS
THAT
INCLUDE
ONLY
ADAPTERS
ALL
HARDWARE
COMPONENTS
USED
TO
SORT
THE
INPUT
RECORDS
FROM
START
TO
FINISH
IDLE
OR
OTHERWISE
MUST
BE
INCLUDED
IN
THE
ENERGY
MEASUREMENT
IF
SOME
COMPONENT
IS
UNUSED
BUT
CANNOT
BE
POWERED
DOWN
OR
PHYSICALLY
SEPARATED
FROM
ADJA
CENT
PARTICIPATING
COMPONENTS
THEN
ITS
POWER
USE
MUST
BE
INCLUDED
IF
THERE
IS
ANY
POTENTIAL
ENERGY
STORED
WITHIN
THE
SYSTEM
E
G
IN
BATTERIES
THE
NET
CHANGE
IN
POTENTIAL
ENERGY
MUST
BE
NO
GREATER
THAN
ZERO
JOULES
WITH
CONFIDENCE
OR
IT
MUST
BE
INCLUDED
WITHIN
THE
ENERGY
MEASUREMENT
ENVIRONMENT
THE
ENERGY
COSTS
OF
COOLING
ARE
IMPORTANT
AND
COOLING
SYSTEMS
ARE
VARIEGATED
AND
OPERATE
AT
MANY
LEV
ELS
IN
A
TYPICAL
DATA
CENTER
THERE
ARE
AIR
CONDITIONERS
BLOW
ERS
AND
RECIRCULATORS
TO
DIRECT
AND
MOVE
AIR
AMONG
AISLES
AND
HEAT
SINKS
AND
FANS
TO
DISTRIBUTE
AND
EXTRACT
HEAT
AWAY
FROM
SYSTEM
COMPONENTS
GIVEN
RECENT
TRENDS
IN
ENERGY
DENSITY
FUTURE
SYSTEMS
MAY
EVEN
HAVE
LIQUID
COOLING
IT
IS
DIFFI
CULT
TO
INCORPORATE
ANTICIPATE
AND
ENFORCE
RULES
FOR
ALL
SUCH
COSTS
IN
A
SYSTEM
LEVEL
BENCHMARK
FOR
SIMPLICITY
WE
ONLY
INCLUDE
A
PART
OF
THIS
COST
ONE
THAT
IS
EASILY
MEASURABLE
AND
ASSOCIATED
WITH
THE
SYSTEM
BEING
MEASURED
WE
SPECIFY
THAT
A
TEMPERATURE
BETWEEN
C
SHOULD
BE
MAINTAINED
AT
THE
SYSTEM
INLETS
OR
WITHIN
FOOT
OF
THE
SYSTEM
IF
NO
INLET
EXISTS
ENERGY
USED
BY
DEVICES
PHYSICALLY
ATTACHED
TO
THE
SORTING
HARDWARE
THAT
REMOVE
HEAT
TO
MAINTAIN
THIS
TEMPER
ATURE
E
G
FANS
MUST
BE
INCLUDED
ENERGY
USE
TOTAL
ENERGY
IS
THE
PRODUCT
OF
AVERAGE
POWER
OVER
THE
SORT
EXECUTION
AND
WALL
CLOCK
TIME
AS
WITH
THE
OTHER
SORT
BENCHMARKS
WALL
CLOCK
TIME
IS
MEASURED
USING
AN
EXTERNAL
SOFTWARE
TIMER
THE
EASIEST
METHOD
TO
MEASURE
POWER
FOR
MOST
SYSTEMS
WILL
BE
TO
INSERT
A
DIGITAL
POWER
ME
TER
BETWEEN
THE
SYSTEM
AND
THE
WALL
WE
INTEND
TO
LEVERAGE
THE
MINIMUM
POWER
METER
REQUIREMENTS
FROM
THE
SPEC
POWER
DRAFT
IN
PARTICULAR
THE
METER
MUST
REPORT
REAL
POWER
INSTEAD
OF
APPARENT
POWER
SINCE
REAL
POWER
REFLECTS
THE
TRUE
ENERGY
CONSUMED
AND
CHARGED
FOR
BY
UTILITIES
WHILE
WE
DO
NOT
PENALIZE
FOR
POOR
POWER
FACTORS
A
POWER
FACTOR
MEASURED
ANYTIME
DURING
THE
SORT
RUN
SHOULD
BE
RE
PORTED
FINALLY
SINCE
ENERGY
MEASUREMENTS
ARE
OFTEN
NOISY
A
MINIMUM
OF
THREE
CONSECUTIVE
ENERGY
READINGS
MUST
BE
RE
PORTED
THESE
WILL
BE
AVERAGED
AND
THE
SYSTEM
WITH
MEAN
ENERGY
LOWER
THAN
ALL
OTHERS
INCLUDING
PREVIOUS
YEARS
WITH
CONFIDENCE
WILL
BE
DECLARED
THE
WINNER
SUMMARY
IN
SUMMARY
THE
JOULESORT
BENCHMARK
IS
AS
FOLLOWS
SORT
A
FIXED
NUMBER
OF
RANDOMLY
PERMUTED
BYTE
RECORDS
WITH
BYTE
KEYS
THE
SORT
MUST
START
WITH
INPUT
IN
A
FILE
ON
NON
VOLATILE
STORE
AND
FINISH
WITH
OUTPUT
IN
A
FILE
ON
NON
VOLATILE
STORE
THERE
ARE
THREE
SCALE
CATEGORIES
FOR
JOULESORT
AND
RECORDS
THE
TOTAL
TRUE
ENERGY
CONSUMED
BY
THE
ENTIRE
PHYSICAL
SYSTEM
EXECUTING
THE
SORT
WHILE
MAINTAINING
AN
AMBI
ENT
TEMPERATURE
BETWEEN
SHOULD
BE
REPORTED
THE
WINNER
IN
EACH
CATEGORY
IS
THE
SYSTEM
WITH
THE
MAXIMUM
SORTEDRECS
JOULE
I
E
MINIMUM
ENERGY
JOULESORT
IS
A
REASONABLE
CHOICE
AMONG
MANY
POSSIBLE
OPTIONS
FOR
AN
ENERGY
ORIENTED
BENCHMARK
IT
IS
AN
I
O
CENTRIC
SYSTEM
LEVEL
ENERGY
EFFICIENCY
BENCHMARK
THAT
IN
CORPORATES
PERFORMANCE
POWER
AND
SOME
COOLING
COSTS
IT
IS
BALANCED
PORTABLE
REPRESENTATIVE
AND
SIMPLE
WE
CAN
USE
IT
TO
COMPARE
DIFFERENT
EXISTING
SYSTEMS
TO
EVALUATE
THE
ENERGY
EFFICIENCY
BALANCE
OF
COMPONENTS
WITHIN
A
GIVEN
SYSTEM
AND
TO
EVALUATE
DIFFERENT
ALGORITHMS
THAT
USE
THESE
COMPONENTS
THESE
FEATURES
ALLOW
US
TO
CHART
PAST
TRENDS
IN
ENERGY
EFFICIENCY
AND
HOPEFULLY
WILL
HELP
PREDICT
FUTURE
TRENDS
A
LOOK
AT
DIFFERENT
SYSTEMS
IN
THIS
SECTION
WE
MEASURE
THE
ENERGY
AND
PERFORMANCE
OF
A
SORT
WORKLOAD
ON
BOTH
UNBALANCED
AND
BALANCED
SORT
ING
SYSTEMS
WE
ANALYZE
A
VARIETY
OF
SYSTEMS
FROM
LAPTOPS
TO
SERVERS
THAT
WERE
READILY
AVAILABLE
IN
OUR
LAB
FOR
THE
UNBALANCED
SYSTEMS
THE
GOAL
OF
THESE
EXPERIMENTS
IS
NOT
TO
PAINSTAKINGLY
TUNE
THESE
CONFIGURATIONS
RATHER
WE
PRESENT
RESULTS
TO
EXPLORE
THE
SYSTEM
HARDWARE
SPACE
WITH
RESPECT
TO
POWER
CONSUMPTION
AND
ENERGY
EFFICIENCY
FOR
SORT
AFTER
LOOKING
AT
UNBALANCED
SYSTEMS
WE
PRESENT
A
BALANCED
FILE
SERVER
THAT
IS
OUR
DEFAULT
WINNER
WE
USE
INSIGHTS
FROM
THESE
EXPERIMENTS
TO
JUSTIFY
THE
APPROACH
FOR
CONSTRUCTING
OUR
JOULESORT
WINNER
SEE
SECTION
UNBALANCED
SYSTEMS
CONFIGURATIONS
TABLE
SHOWS
THE
DETAILS
OF
THE
UNBAL
ANCED
SYSTEMS
WE
EVALUATED
SPANNING
A
REASONABLE
SPEC
TRUM
OF
POWER
CONSUMPTION
IN
SERVERS
AND
PERSONAL
COM
PUTERS
WE
INCLUDE
A
SERVER
AN
OLDER
LOW
POWER
BLADE
AND
AN
A
MODERN
LAPTOP
WE
CHOSE
THE
LAPTOP
BE
CAUSE
IT
IS
DESIGNED
FOR
WHOLE
SYSTEM
ENERGY
CONSERVATION
AND
AND
FOR
COMPARISON
WE
TURNED
OFF
THE
LAPTOP
DISPLAY
FOR
THESE
EXPERIMENTS
FOR
WE
ONLY
USED
BLADE
IN
AN
ENCLOSURE
THAT
HOLDS
AND
AS
PER
OUR
RULES
REPORT
THE
POWER
OF
THE
ENTIRE
SYSTEM
SORT
WORKLOAD
WE
USE
ORDINAL
TECHNOLOGY
COMMERCIAL
NSORT
SOFTWARE
WHICH
WAS
THE
TERABYTE
SORT
DAYTONA
WINNER
IT
USES
ASYNCHRONOUS
I
O
TO
OVERLAP
READING
WRIT
ING
AND
SORTING
OPERATIONS
IT
PERFORMS
BOTH
ONE
AND
TWO
PASS
SORTS
WE
TUNED
NSORT
PARAMETERS
TO
GET
THE
BEST
PERFORMING
SORT
FOR
EACH
PLATFORM
UNLESS
OTHERWISE
STATED
WE
USE
THE
RADIX
IN
MEMORY
SORT
OPTION
TABLE
ENERGY
EFFICIENCY
OF
UNBALANCED
SYSTEMS
POWER
MEASUREMENT
TO
MEASURE
THE
FULL
SYSTEM
AC
POWER
CONSUMPTION
WE
USED
A
DIGITAL
POWER
METER
INTER
POSED
BETWEEN
THE
SYSTEM
AND
THE
WALL
OUTLET
WE
SAMPLED
THIS
POWER
AT
A
RATE
OF
ONCE
PER
SECOND
THE
METER
USED
WAS
BRAND
ELECTRONICS
MODEL
WHICH
REPORTS
TRUE
POWER
WITH
ACCURACY
IN
THIS
PAPER
WE
ALWAYS
RE
PORT
THE
AVERAGE
POWER
OVER
SEVERAL
TRIALS
AND
THE
STANDARD
DEVIATION
IN
THE
AVERAGE
POWER
RESULTS
THE
JOULESORT
RESULTS
FOR
OUR
UNBALANCED
SYSTEMS
ARE
SHOWN
IN
TABLE
SINCE
DISK
SPACE
ON
THESE
SYSTEMS
WAS
LIMITED
WE
CHOSE
TO
RUN
THE
BENCHMARK
AT
AND
A
SMALLER
DATASET
TO
ALLOW
FAIR
COMPARISON
WE
SEE
THAT
THE
SERVER
IS
THE
FASTEST
BUT
THE
LAPTOP
IS
MOST
ENERGY
EFFICIENT
SYSTEM
USES
OVER
MORE
POWER
THAN
BUT
ONLY
PROVIDES
BETTER
PERFORMANCE
ALTHOUGH
DISKS
CAN
PROVIDE
MORE
SEQUENTIAL
BANDWIDTH
WAS
LIMITED
BY
ITS
SMARTARRAY
I
O
CONTROLLER
TO
MB
IN
EACH
PASS
SYS
TEM
THE
BLADE
IS
NOT
AS
BAD
AS
THE
RESULTS
SHOW
BECAUSE
BLADE
ENCLOSURES
ARE
MOST
EFFICIENT
ONLY
WHEN
FULLY
POPU
LATED
THE
ENCLOSURE
POWER
WITHOUT
ANY
BLADES
WAS
WHEN
WE
SUBTRACT
THIS
FROM
THE
TOTAL
POWER
WE
GET
AN
UPPER
BOUND
OF
SRECS
J
FOR
FOR
ALL
THESE
SYSTEMS
THE
STANDARD
DEVIATION
OF
TOTAL
POWER
DURING
SORT
WAS
AT
MOST
THE
POWER
FACTOR
PF
FOR
AND
WERE
AND
RESPECTIVELY
THE
CPUS
FOR
ALL
THREE
SYSTEMS
WERE
HIGHLY
UNDERUTILIZED
IN
PARTICULAR
ATTAINS
AN
ENERGY
EFFICIENCY
SIMILAR
TO
THAT
OF
LAST
YEAR
ESTIMATED
WINNER
GPUTERASORT
BY
BARELY
US
ING
ITS
CORES
SINCE
THE
CPU
IS
USUALLY
THE
HIGHEST
POWER
COMPONENT
THESE
RESULTS
SUGGEST
THAT
BUILDING
A
SYSTEM
WITH
MORE
I
O
TO
COMPLEMENT
THE
AVAILABLE
PROCESSING
CAPACITY
SHOULD
PROVIDE
BETTER
ENERGY
EFFICIENCIES
BALANCED
SERVER
IN
THIS
SECTION
WE
PRESENT
A
BALANCED
SYSTEM
THAT
USUALLY
FUNCTIONS
AS
A
FILESERVER
IN
OUR
LAB
TABLE
SHOWS
THE
COM
PONENTS
USED
DURING
THE
SORT
AND
COARSE
BREAKDOWNS
OF
TOTAL
SYSTEM
POWER
THE
MAIN
SYSTEM
IS
AN
HP
PROLIANT
THAT
INCLUDES
A
MOTHERBOARD
CPU
LOW
POWER
LAPTOP
DISK
AND
A
HIGH
THROUGHPUT
SAS
I
O
CONTROLLER
FOR
THE
STORAGE
WE
USE
TWO
DISK
TRAYS
ONE
THAT
HOLDS
THE
INPUT
AND
OUTPUT
FILES
AND
THE
OTHER
WHICH
HOLDS
THE
TEMP
DISKS
EACH
TRAY
HAS
DISKS
AND
CAN
HOLD
A
MAXIMUM
OF
THE
DISK
TRAYS
AND
MAIN
SYSTEM
ALL
HAVE
DUAL
POWER
SUPPLIES
BUT
FOR
THESE
EXPERIMENTS
WE
POWERED
THEM
THROUGH
ONE
EACH
FOR
ALL
OUR
EXPERIMENTS
THE
SYSTEM
HAS
BIT
UBUNTU
LINUX
AND
THE
XFS
FILESYSTEM
INSTALLED
TABLE
SHOWS
THAT
FOR
A
SERVER
OF
THIS
KIND
THE
DISKS
AND
THEIR
ENCLOSURES
CONSUME
ROUGHLY
THE
SAME
POWER
AS
THE
REST
OF
THE
SYSTEM
WHEN
A
TRAY
IS
FULLY
POPULATED
WITH
DISKS
TABLE
A
BALANCED
FILESERVER
THE
IDLE
POWER
IS
W
AND
WITH
DISKS
THE
IDLE
POWER
IS
W
THERE
CLEARLY
ARE
INEFFICIENCIES
WHEN
THE
TRAY
IS
UNDER
UTILIZED
TO
ESTIMATE
THE
POWER
OF
THE
DIMMS
WE
ADDED
TWO
DIMMS
AND
MEASURED
THE
SYSTEM
POWER
WITH
AND
WITHOUT
THE
DIMMS
WE
FOUND
THAT
THE
DIMMS
USE
BOTH
DURING
SORT
AND
AT
IDLE
FOR
THIS
SYSTEM
WE
FOUND
THE
MOST
ENERGY
EFFICIENT
CONFIG
URATION
BY
EXPERIMENTING
WITH
A
DATASET
BY
VARYING
THE
NUMBER
OF
DISKS
USED
WE
FOUND
THAT
EVEN
WITH
THE
INEF
FICIENCIES
THE
BEST
PERFORMING
SETUP
USES
DISKS
SPLIT
ACROSS
TWO
TRAYS
THIS
EFFECT
HAPPENS
BECAUSE
THE
I
O
CON
TROLLER
OFFERS
BETTER
BANDWIDTH
WHEN
DATA
IS
SHIPPED
ACROSS
ITS
TWO
CHANNELS
A
SORT
PROVIDES
ON
AV
ERAGE
FOR
EACH
PHASE
ACROSS
THE
TRAYS
WHILE
ONLY
WHEN
THE
ALL
DISKS
ARE
WITHIN
A
TRAY
THE
AVERAGE
POWER
OF
THE
SYSTEM
WITH
ONLY
ONE
TRAY
IS
AND
WITH
TWO
TRAYS
IS
AS
A
RESULT
WITH
TWO
TRAYS
THE
SYSTEM
ATTAINS
A
BEST
SRECS
J
INSTEAD
OF
SRECS
J
WITH
ONE
TRAY
THE
TRAY
DISK
SETUP
IS
ALSO
WHEN
THE
SORT
BECOMES
CPU
BOUND
WHEN
WE
REDUCE
THE
SYSTEM
TO
DISKS
THE
I
O
PERFORMANCE
AND
CPU
UTILIZATION
DROP
AND
WHEN
WE
INCREASE
THE
SYSTEM
TO
DISKS
THE
PERFORMANCE
AND
UTI
LIZATION
REMAIN
THE
SAME
IN
BOTH
CASES
TOTAL
ENERGY
IS
HIGHER
THAN
THE
DISK
POINT
SO
THIS
BALANCED
CPU
BOUND
CONFIGURATION
IS
ALSO
THE
MOST
ENERGY
EFFICIENT
TABLE
SHOWS
THE
PERFORMANCE
AND
ENERGY
CHARACTERISTICS
OF
THE
DISK
SETUP
FOR
SORTS
THIS
SYSTEM
TAKES
NEARLY
MORE
POWER
THAN
BUT
PROVIDES
OVER
THE
THROUGH
PUT
THIS
SYSTEM
SRECS
J
RATIO
BEATS
THE
LAPTOP
AND
LAST
YEAR
ESTIMATED
WINNER
EVEN
WITH
A
LARGER
INPUT
EX
PERIMENTS
SIMILAR
TO
THOSE
FOR
THE
DATASET
SHOW
THAT
THIS
SETUP
PROVIDES
JUST
ENOUGH
I
O
TO
KEEP
THE
TWO
CORES
FULLY
UTILIZED
ON
BOTH
PASSES
AND
USES
THE
MINIMUM
ENERGY
FOR
THE
SCALE
THUS
AT
ALL
SCALES
THE
MOST
ENERGY
EFFICIENT
AND
BEST
PERFORMING
CONFIGURATION
FOR
THIS
SYSTEM
IS
WHEN
SORT
IS
CPU
BOUND
AND
BALANCED
TABLE
WINNING
SYSTEM
SUMMARY
IN
CONCLUSION
FROM
EXPERIMENTING
WITH
THESE
SYSTEMS
WE
LEARNED
CPU
IS
WASTED
IN
UNBALANCED
SYSTEMS
THE
MOST
ENERGY
EFFICIENT
SERVER
CONFIGURATION
IS
WHEN
THE
SYS
TEM
IS
CPU
BOUND
AN
UNBALANCED
LAPTOP
IS
ALMOST
AS
ENERGY
EFFICIENT
AS
A
BALANCED
SERVER
MOREOVER
CURRENT
LAP
TOP
DRIVES
USE
VS
W
LESS
POWER
THAN
OUR
SERVER
SATA
DRIVES
WHILE
OFFERING
AROUND
VS
MB
THE
BANDWIDTH
THESE
OBSERVATIONS
SUGGEST
A
REASONABLE
AP
PROACH
FOR
BUILDING
THE
MOST
ENERGY
EFFICIENT
SORTING
SYSTEM
IS
TO
USE
MOBILE
CLASS
CPUS
AND
DISKS
AND
CONNECT
THEM
VIA
A
HIGH
SPEED
I
O
INTERCONNECT
JOULESORT
WINNER
IN
THIS
SECTION
WE
FIRST
DESCRIBE
OUR
WINNING
JOULESORT
CONFIGURATION
AND
REPORT
ITS
PERFORMANCE
WE
THEN
STUDY
THIS
SYSTEM
THROUGH
EXPERIMENTS
THAT
ELUCIDATE
POWER
AND
PERFORMANCE
CHARACTERISTICS
OF
THIS
SYSTEM
WINNING
CONFIGURATION
GIVEN
LIMITED
TIME
AND
BUDGET
OUR
GOAL
WAS
TO
CONVINC
INGLY
OVERTAKE
THE
PREVIOUS
ESTIMATED
WINNER
RATHER
THAN
TO
TRY
NUMEROUS
COMBINATIONS
AND
CONSTRUCT
AN
ABSOLUTE
OP
TIMAL
SYSTEM
AS
AS
RESULT
WE
DECIDED
TO
BUILD
A
DAYTONA
SYSTEM
AND
SOLELY
USE
NSORT
AS
THE
SOFTWARE
OUR
DESIGN
STRATEGY
FOR
AN
ENERGY
EFFICIENT
SORT
WAS
TO
BUILD
A
BALANCED
SORTING
SYSTEM
OUT
OF
LOW
POWER
COMPONENTS
AFTER
ESTI
MATING
THE
SORTING
EFFICIENCY
OF
POTENTIAL
SYSTEMS
AMONG
A
LIMITED
COMBINATION
OF
MODERN
LOW
POWER
PROCESSORS
AND
LAPTOP
DISKS
WE
ASSEMBLED
THE
CONFIGURATION
IN
TABLE
THIS
SYSTEM
USES
A
MODERN
LOW
POWER
CPU
WITH
FRE
QUENCY
STATES
AND
A
TDP
OF
FOR
THE
HIGHEST
STATE
WE
USE
A
MOTHERBOARD
THAT
SUPPORTS
BOTH
A
MOBILE
CPU
AND
MULTIPLE
DISK
CONTROLLERS
TO
KEEP
THE
CORES
BUSY
FEW
SUCH
BOARDS
EXIST
BECAUSE
THEY
TARGET
A
NICHE
MARKET
THIS
ONE
INCLUDES
TWO
PCI
E
SLOTS
ONE
CHANNEL
AND
ONE
CHANNEL
TO
FILL
THOSE
SLOTS
WE
USE
CONTROLLERS
THAT
HOLD
AND
SATA
DRIVES
RESPECTIVELY
FINALLY
OUR
CONFIGURATION
USES
LOW
POWER
LAPTOP
DRIVES
WHICH
SUPPORT
THE
SATA
IN
TERFACE
THEY
OFFER
AN
AVERAGE
MS
SEEK
TIME
AND
THEIR
MEASURED
SEQUENTIAL
BANDWIDTH
THROUGH
XFS
IS
AROUND
MB
HITACHI
SPECS
LIST
AN
AVERAGE
FOR
READ
AND
WRITE
AND
FOR
ACTIVE
IDLE
WE
USE
TWO
DIMMS
WHOSE
SPECS
REPORT
FOR
EACH
FINALLY
THE
CASE
COMES
WITH
A
POWER
SUPPLY
OUR
OPTIMAL
CONFIGURATION
USES
DISKS
BECAUSE
THE
PCI
E
CARDS
HOLD
DISKS
MAXIMUM
AND
THE
I
O
PERFORMANCE
OF
THE
MOTHERBOARD
CONTROLLER
WITH
MORE
THAN
DISK
IS
POOR
THE
INPUT
AND
OUTPUT
FILES
ARE
STRIPED
ACROSS
A
DISK
ARRAY
CONFIGURED
VIA
AND
THE
REMAINING
DISKS
ARE
INDE
PENDENT
FOR
THE
TEMPORARY
RUNS
FOR
ALL
EXPERIMENTS
WE
USE
LINUX
KERNEL
AND
THE
XFS
FILESYSTEM
UNLESS
OTHERWISE
STATED
IN
THE
IDLE
STATE
AT
THE
LOWEST
CPU
FREQUENCY
WE
MEASURED
W
FOR
THIS
SYSTEM
TABLE
SHOWS
THE
PERFORMANCE
OF
THE
SYSTEM
WHICH
AT
TAINS
SRECS
J
WHEN
AVERAGED
OVER
CONSECUTIVE
RUNS
THE
PURE
PERFORMANCE
STATISTICS
ARE
REPORTED
BY
NSORT
WE
CONFIGURE
IT
TO
USE
RADIX
SORT
AS
ITS
IN
MEMORY
SORT
ALGO
RITHM
AND
USE
TRANSFER
SIZES
OF
FOR
THE
INPUT
OUTPUT
ARRAY
AND
FOR
THE
TEMPORARY
STORAGE
OUR
SYSTEM
IS
FASTER
THAN
GPUTERASORT
AND
CONSUMES
AN
ESTIMATED
LESS
POWER
THE
POWER
USE
DURING
SORT
IS
MORE
THAN
IDLE
IN
THE
OUTPUT
PASS
THE
CPU
IS
UNDERUTILIZED
SEE
TA
BLE
MAX
FOR
CORES
AND
THE
BANDWIDTH
IS
LOWER
THAN
IN
THE
INPUT
PASS
BECAUSE
THE
OUTPUT
PASS
REQUIRES
RAN
DOM
I
OS
WE
PIN
THE
CPU
TO
MHZ
WHICH
SECTION
SHOWS
IS
THE
MOST
ENERGY
EFFICIENT
FREQUENCY
FOR
THE
SORT
VARYING
SYSTEM
SIZE
IN
THESE
EXPERIMENTS
WE
VARY
THE
SYSTEM
SIZE
DISKS
AND
CONTROLLERS
AND
OBSERVE
OUR
SYSTEM
PURE
PERFORMANCE
COST
EFFICIENCY
AND
ENERGY
EFFICIENCY
WE
INVESTIGATE
THESE
MET
RICS
USING
A
DATASET
FOR
THE
FIRST
TWO
METRICS
WE
SET
THE
CPU
TO
ITS
HIGHEST
FREQUENCY
AND
REPORT
THE
METRICS
FOR
THE
MOST
COST
EFFECTIVE
AND
BEST
PERFORMING
CONFIGURATIONS
AT
EACH
STEP
WE
START
WITH
DISKS
ATTACHED
TO
THE
CHEAPER
DISK
CONTROLLER
AND
AT
EACH
STEP
USE
THE
MINIMUM
COST
HARDWARE
TO
SUPPORT
AN
ADDITIONAL
DISK
THUS
WE
SWITCH
TO
THE
DISK
CONTROLLER
FOR
CONFIGURATIONS
WITH
DISKS
AND
USE
BOTH
CONTROLLERS
COMBINED
FOR
DISKS
FINALLY
WE
ADD
A
DISK
DIRECTLY
TO
THE
MOTHERBOARD
FOR
THE
DISK
CON
FIGURATION
FIGURE
SHOWS
THE
PERFORMANCE
RECORDS
SEC
AND
COST
EFFICIENCY
WITH
INCREASING
SYSTEM
SIZE
THE
DISK
CONFIG
URATION
IS
BOTH
THE
BEST
PERFORMING
AND
MOST
COST
EFFICIENT
POINT
EACH
ADDITIONAL
DISK
ON
AVERAGE
INCREASES
SYSTEM
COST
BY
ABOUT
AND
IMPROVES
PERFORMANCE
BY
ON
AVERAGE
THESE
MARGINAL
CHANGES
VARY
THEY
ARE
LARGER
FOR
SMALL
SYS
TEM
SIZE
AND
SMALLER
FOR
LARGER
SYSTEM
SIZES
THE
DISK
POINT
DROPS
IN
COST
EFFICIENCY
BECAUSE
IT
INCLUDES
THE
EXPEN
SIVE
DISK
CONTROLLER
WITHOUT
A
COMMENSURATE
PERFORMANCE
INCREASE
ALTHOUGH
THE
MOTHERBOARD
AND
CONTROLLERS
LIMIT
THE
SYSTEM
TO
DISKS
WE
SPECULATE
THAT
ADDITIONAL
DISKS
WOULD
NOT
HELP
SINCE
THE
FIRST
PASS
OF
THE
SORT
IS
CPU
BOUND
NEXT
WE
LOOK
AT
HOW
ENERGY
EFFICIENCY
VARIES
WITH
WITH
SYSTEM
SIZE
AT
EACH
STEP
WE
ADD
THE
MINIMUM
ENERGY
HARD
WARE
TO
SUPPORT
THE
ADDED
DISK
AND
REPORT
THE
MOST
ENERGY
EFFICIENT
SETUP
WE
SET
THE
CPU
FREQUENCY
TO
AT
ALL
POINTS
TO
GET
THE
BEST
ENERGY
EFFICIENCY
SEE
SECTION
FOR
CONVENIENCE
WE
HAD
ONE
EXTRA
OS
DISK
ON
THE
MOTHER
BOARD
FROM
WHICH
WE
BOOT
AND
WHICH
WAS
UNUSED
IN
THE
SORT
FOR
ALL
BUT
THE
LAST
POINT
THE
POWER
MEASUREMENTS
INCLUDE
THIS
DISK
BUT
THIS
POWER
IS
NEGLIGIBLE
AT
IDLE
TABLE
PERFORMANCE
OF
WINNING
JOULESORT
SYSTEMS
DISKS
USED
DISKS
USED
FIGURE
SHOWS
HOW
PERFORMANCE
PRICE
AND
PERFOR
MANCE
VARIES
WITH
SYSTEM
SIZE
SORT
IDLE
DISKS
USED
FIGURE
SHOWS
HOW
POWER
VARIES
WITH
SYSTEM
SIZE
FIGURE
SHOWS
IDLE
POWER
AT
THE
LOWEST
FREQUENCY
STATE
VERSUS
AVERAGE
POWER
DURING
SORT
AT
MHZ
FOR
THE
SAME
SYSTEM
CONFIGURATIONS
WITH
THE
SYSTEM
AT
IDLE
AND
ONLY
THE
MOTHERBOARD
DISK
INSTALLED
OUR
MEASUREMENTS
SHOW
THAT
THE
DISK
CONTROLLER
USES
AND
THE
DISK
ONE
USES
THUS
FOR
POINTS
BETWEEN
DISKS
WE
USE
ONLY
THE
DISK
CONTROLLER
BETWEEN
WE
USE
THE
ONLY
DISK
CONTROLLER
AND
FOR
OR
MORE
WE
USE
BOTH
FIGURE
SHOWS
JUMPS
AT
THESE
TRANSITIONS
THE
IDLE
LINE
INDICATES
ADDING
A
DISK
INCREASES
POWER
BY
DURING
SORTING
ADDING
A
DISK
INCREASES
TOTAL
POWER
ON
AVERAGE
AT
SIZES
FEWER
THAN
DISKS
AND
ON
AVERAGE
FOR
MORE
THAN
DISKS
THESE
INCREASES
REFLECT
END
TO
END
UTILIZATION
OF
THE
CPU
DISK
CONTROLLERS
ETC
FIGURE
SHOWS
THE
ENERGY
EFFICIENCY
WITH
INCREASING
NUM
BER
OF
DISKS
USED
IN
THE
SORT
THE
CURVE
IS
SIMILAR
TO
THE
PRICE
PERFORMANCE
CURVE
IN
FIGURE
THE
AVERAGE
INCREASE
FIGURE
SHOWS
HOW
ENERGY
EFFICIENCY
VARIES
WITH
SYSTEM
SIZE
IN
ENERGY
AT
EACH
STEP
IS
WHILE
THE
AVERAGE
INCREASE
IN
PERFORMANCE
IS
ABOUT
THE
DISK
POINT
AGAIN
IS
A
LOCAL
MINIMUM
BECAUSE
IT
INCURS
THE
POWER
OF
THE
LARGER
CONTROLLER
WITHOUT
ENOUGH
DISKS
TO
TAKE
ADVANTAGE
OF
IT
THE
SORT
IS
CPU
BOUND
IN
THE
MOST
ENERGY
EFFICIENT
CONFIGURATION
THERE
ARE
TWO
MAIN
POINTS
TO
TAKE
AWAY
FROM
THESE
EXPER
IMENTS
FIRST
THE
SIMILAR
SHAPES
OF
THESE
CURVES
REFLECT
THAT
THE
BASE
DOLLAR
AND
ENERGY
COSTS
OF
THE
SYSTEM
ARE
HIGH
COM
PARED
TO
THE
MARGINAL
DOLLAR
AND
ENERGY
COST
OF
DISKS
IF
WE
USED
SERVER
CLASS
DISKS
THAT
ARE
SIMILAR
IN
COST
BUT
CONSUME
THE
POWER
OF
MOBILE
DISKS
WE
WOULD
SEE
DIFFERENT
COST
AND
ENERGY
EFFICIENCY
CURVES
SECOND
FOR
THE
COMPONENTS
WE
CHOSE
THE
BEST
PERFORMING
MOST
COST
EFFICIENT
AND
MOST
ENERGY
EFFICIENT
CONFIGURATIONS
ARE
IDENTICAL
MODULO
THE
CPU
FREQUENCY
MOREOVER
IN
THIS
BEST
CONFIGURATION
THE
SYSTEM
IS
BALANCED
WITH
JUST
ENOUGH
I
O
BANDWITH
TO
KEEP
THE
CPU
FULLY
UTILIZED
FOR
THE
FIRST
PASS
SOFTWARE
MATTERS
NEXT
WE
VARY
THE
FILESYSTEM
AND
IN
MEMORY
SORT
ALGO
RITHM
TO
SEE
HOW
THEY
AFFECT
ENERGY
EFFICIENCY
THE
WINNING
CONFIGURATION
USES
THE
XFS
FILESYSTEM
AND
A
RADIX
SORT
FIGURE
EXAMINES
THE
EFFECT
OF
CHANGING
THE
FILESYSTEM
TO
REISERFS
AND
THE
SORT
ALGORITHM
TO
MERGE
SORT
AT
DIFFERENT
CPU
FREQUENCIES
FOR
A
DATASET
AS
EXPECTED
POWER
CONSUMPTION
STEADILY
INCREASES
WITH
FREQUENCY
IN
ALL
CASES
THE
POWER
CONSUMPTIONS
OF
XFS
WITH
RADIX
SORT
AND
MERGE
SORT
ARE
SIMILAR
AT
ALL
FREQUEN
CIES
REISERFS
HOWEVER
CONSUMES
LESS
POWER
AND
ALSO
IS
LESS
ENERGY
EFFICIENT
ALL
THREE
CONFIGURATIONS
SHOW
IMPROVED
ENERGY
EFFICIENCY
FROM
MHZ
TO
MHZ
AND
THEN
LEVEL
OFF
OR
DECREASE
THIS
RESULT
INDICATES
THAT
THE
SORTS
ARE
CPU
BOUND
AT
THE
LOWER
FREQUENCIES
REISERFS
SHOWS
A
IM
PROVEMENT
IN
PERFORMANCE
BETWEEN
THE
LOWEST
AND
HIGHEST
ON
DEMAND
CPU
FREQ
MHZ
ING
THIS
SORT
CPU
UTILIZATION
WAS
AN
AVERAGE
PER
CORE
SO
WE
ASSIGN
TO
THE
CPU
SUBSYSTEM
SIMILARLY
WE
DISCOUNT
THE
COPYING
TEST
FOR
ITS
CPU
UTILIZATION
AND
ESTIMATE
THAT
THE
I
O
SUBSYSTEM
USES
THESE
ESTIMATES
COMBINE
TO
AND
ALMOST
MATCH
ERROR
THE
MEASURED
INCREASE
DUE
TO
THE
DISK
SORT
THUS
OUR
TESTS
IMPLY
THAT
ABOUT
OF
THE
POWER
INCREASE
DURING
SORT
IS
FROM
THE
I
O
SUBSYSTEM
AND
FROM
THE
CPU
SUBSYSTEM
WE
FOUND
SIMILAR
PROPORTIONS
AT
SMALLER
SYSTEM
SIZES
VARY
DIMMS
AND
POWER
SUPPLY
SINCE
NSORT
USES
ONLY
A
FRACTION
OF
THE
AVAILABLE
MEM
ORY
FOR
THESE
EXPERIMENTS
WE
RAN
EXPERIMENTS
WITH
ONLY
DIMM
POWER
USE
AND
EXECUTION
TIME
WERE
STATISTICALLY
IN
DISTINGUISHABLE
FROM
THE
DIMM
CASE
DURING
SORT
POWER
FIGURE
SHOWS
HOW
AVERAGE
POWER
AND
ENERGY
EF
FICIENCY
VARY
WITH
CPU
FREQUENCY
FOR
A
SORT
FREQUENCIES
WHILE
XFS
RADIX
IMPROVES
ONLY
AND
XFS
MERGE
IMPROVES
ONLY
BY
REISERFS
HAS
WORSE
ENERGY
EFFICIENCY
MAINLY
BECAUSE
IT
PROVIDES
LESS
SEQUENTIAL
BANDWIDTH
AND
THUS
WORSE
PERFOR
MANCE
THAN
XFS
ALTHOUGH
WE
TUNED
EACH
CONFIGURATION
THIS
RESULT
MAY
BE
AN
ARTIFACT
OF
OUR
SETUP
AND
NOT
AN
IN
HERENT
FLAW
OF
REISERFS
SIMILARLY
THE
MERGE
SORT
ALSO
GIVES
WORSE
ENERGY
EFFICIENCY
THAN
RADIX
ENTIRELY
BECAUSE
ITS
PER
FORMANCE
IS
WORSE
THE
GRAPH
ALSO
SHOWS
THE
POWER
AND
ENERGY
EFFICIENCY
OF
THE
LINUX
ON
DEMAND
CPU
FREQUENCY
SCALING
POLICY
WHICH
IS
WITHIN
OF
THE
LOWEST
EXECUTION
TIME
AND
OF
THE
LOWEST
POWER
FOR
ALL
THREE
CONFIGURATIONS
FOR
REISERFS
THE
ON
DEMAND
POLICY
OFFERS
THE
SAME
EFFICIENCY
AS
THE
BEST
CON
FIGURATION
IN
SUMMARY
THESE
EXPERIMENTS
SHOW
THAT
THE
ALGORITHMS
AND
UNDERLYING
SOFTWARE
USED
FOR
SORT
AFFECT
EN
ERGY
EFFICIENCY
MAINLY
THROUGH
PERFORMANCE
APPROXIMATE
CPU
VS
I
O
BREAKDOWN
WE
PERFORMED
SOME
MICRO
BENCHMARKS
EXERCISING
THE
I
O
SUBSYSTEM
DISKS
PLUS
CONTROLLERS
AND
THE
CPU
SUBSYSTEM
CPU
PLUS
MEMORY
SEPARATELY
TO
DETERMINE
HOW
MUCH
EACH
CONTRIBUTES
TO
THE
INCREASE
IN
POWER
DURING
SORT
THE
SYSTEM
AT
THE
DISK
POINT
CONSUMES
MORE
DURING
SORT
THAN
WHEN
THE
SYSTEM
IS
AT
IDLE
WITH
MHZ
CPU
FREQUENCY
THIS
INCREASE
IS
NEARLY
OF
THE
TOTAL
POWER
DURING
SORT
OUR
BENCHMARKS
SUGGEST
THAT
THE
I
O
SUBSYSTEM
CONSUMES
A
MUCH
GREATER
FRACTION
OF
THIS
POWER
INCREASE
THAN
THE
CPU
SUBSYSTEM
WE
FIRST
PERFORMED
A
TEST
TO
HELP
APPROXIMATE
THE
CONTRI
BUTION
FROM
THE
I
O
SUBSYSTEM
IN
THIS
TEST
WE
COPIED
DATA
FROM
A
DISK
ARRAY
TO
ANOTHER
DISK
ARRAY
WHICH
HAD
THE
SAME
AVERAGE
DISK
BANDWIDTH
AS
THE
DISK
SORT
WE
FOUND
THAT
THE
SYSTEM
POWER
INCREASED
BY
DURING
THIS
TEST
THE
CPU
UTILIZATION
WAS
OUT
OF
MAX
FOR
CORES
NEXT
WE
PERFORMED
AN
EXPERIMENT
TO
APPROXIMATE
THE
CON
TRIBUTION
FROM
THE
CPU
SUBSYSTEM
WE
PUT
A
SMALL
INPUT
FILE
ON
A
RAM
DISK
AND
REPEATEDLY
SORTED
IT
THIS
TEST
PEGGED
THE
CPU
TO
UTILIZATION
AND
THE
POWER
INCREASE
WAS
USING
THE
ABOVE
VALUES
AND
ASSUMING
THAT
CPU
SUBSYSTEM
POWER
INCREASES
LINEARLY
WITH
CPU
UTILIZATION
WE
ESTIMATE
ITS
CONTRIBUTION
DURING
THE
DISK
SORT
AS
FOLLOWS
DUR
USE
IS
ALSO
WITHIN
MEASUREMENT
ERROR
AT
IDLE
WE
REPLACED
THE
POWER
SUPPLY
WITH
A
ONE
AND
FOUND
THAT
THE
POWER
CONSUMPTION
DURING
SORT
AND
IDLE
INCREASED
BY
THIS
SUGGESTS
THAT
AT
LOAD
OR
LESS
EFFICIENCIES
OF
THE
TWO
POWER
SUPPLIES
ARE
SIMILAR
NOTE
THE
POWER
FACTORS
FOR
THE
LAPTOPS
AND
DESKTOP
SYS
TEMS
IN
THIS
PAPER
INCLUDING
OUR
WINNER
ARE
WELL
BELOW
LOW
POWER
FACTORS
ARE
PROBLEMATIC
IN
DATA
CEN
TERS
BECAUSE
POWER
DELIVERY
MECHANISMS
NEED
TO
BE
OVER
PROVISIONED
TO
CARRY
ADDITIONAL
CURRENT
FOR
LOADS
WITH
LOW
POWER
FACTORS
UTILITIES
OFTEN
CHARGE
EXTRA
FOR
THIS
PRO
VISIONING
SIMILAR
TO
SERVER
CLASS
SYSTEMS
POWER
SUPPLIES
WILL
NEED
TO
PROVIDE
POWER
FACTOR
CORRECTION
FOR
SYSTEMS
LIKE
OUR
WINNER
TO
BECOME
A
REALITY
IN
DATA
CENTERS
SUMMARY
WE
DESCRIBE
THE
DAYTONA
JOULESORT
SYSTEM
THAT
IS
OVER
AS
ENERGY
EFFICIENT
AS
LAST
YEAR
PENNYSORT
WIN
NER
THE
GPUTERASORT
FOR
THIS
SYSTEM
WE
SHOW
THE
MOST
ENERGY
EFFICIENT
SORTING
CONFIGURATION
IS
WHEN
THE
SORT
IS
CPU
BOUND
AND
BALANCED
THIS
CONFIGURATION
IS
ALSO
THE
BEST
PERFORMING
AND
MOST
COST
EFFICIENT
IT
WILL
BE
INTERESTING
TO
SEE
HOW
LONG
THIS
RELATIONSHIP
HOLDS
WE
SEE
THAT
FILESYSTEM
AND
IN
MEMORY
SORT
CHOICE
MAINLY
AFFECT
ENERGY
EFFICIENCY
THROUGH
PERFORMANCE
RATHER
THAN
POWER
FOR
THIS
SYSTEM
IN
THIS
PAPER
WE
FOCUSED
ON
BUILDING
A
BALANCED
SYSTEM
WITH
LOW
POWER
OFF
THE
SHELF
COMPONENTS
TARGETED
FOR
THE
SCALE
UNFORTUNATELY
BECAUSE
OF
HARDWARE
LIMITA
TIONS
AND
MARKET
AVAILABILITY
WE
COULD
NOT
EASILY
SCALE
THIS
SYSTEM
TO
THE
CATEGORY
IN
THE
FUTURE
WE
EXPECT
SYS
TEMS
IN
OTHER
CLASSES
TO
WIN
THE
AND
CATEGORIES
BUT
FOR
COMPLETENESS
WE
REPORT
IN
TABLE
THE
BEST
CONFIGU
RATIONS
WE
ENCOUNTERED
FOR
THOSE
CATEGORIES
RELATED
WORK
OUR
RELATED
WORK
FALLS
INTO
THREE
CATEGORIES
WE
FIRST
DIS
CUSS
THE
HISTORY
OF
SORT
BENCHMARKS
AND
LARGE
SCALE
SORTING
TECHNIQUES
NEXT
WE
COVER
THE
PREVIOUS
WORK
ON
METRICS
FOR
EVALUATING
ENERGY
EFFICIENCY
FINALLY
WE
BRIEFLY
DISCUSS
WORK
ON
TECHNIQUES
FOR
REDUCING
ENERGY
CONSUMPTION
IN
SYSTEMS
SORT
BENCHMARKS
AND
TECHNIQUES
THE
ORIGINAL
DATAMATION
SORT
BENCHMARK
WAS
A
PURE
PER
FORMANCE
BENCHMARK
THAT
MEASURED
THE
TIME
TO
SORT
A
MIL
LION
RECORDS
IN
THE
DEVELOPERS
OF
ALPHASORT
RECOGNIZED
THAT
THE
BENCHMARK
WAS
LOSING
ITS
RELEVANCE
BE
CAUSE
STARTUP
AND
SHUTDOWN
WOULD
EVENTUALLY
DOMINATE
THE
TIME
TO
SORT
SUCH
A
SMALL
NUMBER
OF
RECORDS
THEY
THERE
FORE
PROPOSED
TWO
VARIANTS
MINUTESORT
AND
PENNYSORT
HOP
ING
THEY
WOULD
REMAIN
RELEVANT
AS
TECHNOLOGY
IMPROVED
AT
THE
PACE
OF
MOORE
LAW
RECOGNIZING
THAT
PENNYSORT
WAS
BIASED
AGAINST
LARGE
CONFIGURATIONS
BY
ALLOWING
TOO
SMALL
A
TIME
BUDGET
RESEARCHERS
THEN
PROPOSED
THE
PERFORMANCE
PRICE
SORT
WHICH
IS
TIED
TO
THE
MINUTESORT
TIME
BUDGET
THE
TIME
BUDGET
APPROACH
UNDERMINES
THE
GOALS
OF
JOULESORT
SINCE
THE
ORIGINAL
DATAMATION
SORT
BENCHMARK
THERE
HAVE
BEEN
MANY
DIFFERENT
IMPLEMENTATIONS
OF
EXTERNAL
SORT
ON
A
VARIETY
OF
PLATFORMS
FROM
DESKTOPS
TO
SUPERCOMPUTERS
THE
SORT
BENCHMARK
WEBSITE
MAINTAINED
BY
JIM
GRAY
LISTS
THE
WINNERS
AND
BRIEFLY
SURVEYS
PAST
TRENDS
ENERGY
BENCHMARKS
SEVERAL
DIFFERENT
METRICS
HAVE
BEEN
PROPOSED
FOR
EVALU
ATING
THE
ENERGY
EFFICIENCY
OF
COMPUTER
SYSTEMS
IN
GONZALEZ
ET
AL
PROPOSED
THE
ENERGY
DELAY
PRODUCT
AS
THE
METRIC
OF
ENERGY
EFFICIENT
MICROPROCESSOR
DESIGN
ALTER
NATIVELY
THE
METRIC
OF
PERFORMANCE
PER
WATT
IS
ALSO
WIDELY
USED
TO
EVALUATE
PROCESSORS
ENERGY
EFFICIENCY
THIS
MET
RIC
EMPHASIZES
PERFORMANCE
LESS
THAN
THE
ENERGY
DELAY
PROD
UCT
WHICH
IS
EQUIVALENT
TO
PERFORMANCE
SQUARED
PER
WATT
ENERGY
EFFICIENCY
METRICS
TAILORED
TO
DATA
CENTERS
HAVE
ALSO
BEEN
PROPOSED
SUN
SPACE
WATTS
AND
PERFORMANCE
METRIC
SWAP
CONSIDERS
THE
RACK
SPACE
TAKEN
UP
BY
A
HARDWARE
CONFIGURATION
ALONG
WITH
ITS
POWER
AND
PERFORMANCE
IN
AN
EFFORT
TO
PROMOTE
DATA
CENTER
COMPACTION
METRICS
BASED
ON
EXERGY
WHICH
IS
THE
ENERGY
CONVERTED
INTO
LESS
EFFICIENT
FORMS
SUCH
AS
HEAT
TAKE
INTO
ACCOUNT
EVERY
ASPECT
OF
THE
DATA
CENTER
FROM
PROCESSORS
TO
THE
COOLING
INFRASTRUCTURE
HOWEVER
THESE
METRICS
ARE
NOT
APPLICABLE
TO
THE
ENTIRE
RANGE
OF
SYSTEMS
WE
WANT
TO
EVALUATE
WITH
JOULESORT
COMPARATIVELY
LITTLE
WORK
HAS
BEEN
DONE
ON
WORKLOADS
FOR
ENERGY
EFFICIENCY
BENCHMARKS
IN
THE
EMBEDDED
DOMAIN
THE
EEMBC
ENERGYBENCH
BENCHMARKS
PROVIDE
A
PHYSICAL
IN
FRASTRUCTURE
TO
EVALUATE
A
SINGLE
PROCESSOR
ENERGY
EFFICIENCY
ON
ANY
OF
EEMBC
EXISTING
MOBILE
BENCHMARK
SUITES
IN
THE
ENTERPRISE
DOMAIN
THE
SPEC
POWER
AND
PERFORMANCE
COMMITTEE
IS
CURRENTLY
DEVELOPING
AN
ENERGY
BENCHMARK
SUITE
FOR
SERVERS
AND
THE
UNITED
STATES
ENVIRONMENTAL
PRO
TECTION
AGENCY
ENERGYSTAR
PROGRAM
IS
DEVELOPING
A
WAY
TO
RATE
THE
ENERGY
EFFICIENCY
OF
SERVERS
AND
DATA
CENTERS
ENERGY
EFFICIENCY
THERE
IS
A
LARGE
BODY
OF
PRIOR
WORK
ON
ENERGY
EFFICIENCY
FOR
EXAMPLE
AT
THE
COMPONENT
AND
SYSTEM
LEVELS
MANY
STUDIES
HAVE
BEEN
DEVOTED
TO
ALGORITHMS
FOR
DYNAMICALLY
EXPLOITING
DIFFERENT
POWER
STATES
IN
PROCESSORS
MEMORY
AND
DISKS
IN
ORDER
TO
PROMOTE
ENERGY
EF
FICIENCY
IN
CLUSTERS
AND
DATA
CENTERS
RESEARCH
HAS
FOCUSED
ON
ENERGY
EFFICIENT
WORKLOAD
DISTRIBUTION
AND
POWER
BUDGET
ING
E
G
OTHER
STUDIES
HAVE
FOCUSED
AT
THE
APPLICATION
LEVEL
INCLUDING
ENERGY
AWARE
USER
INTERFACES
AND
FIDELITY
AWARE
ENERGY
MANAGEMENT
CONCLUSIONS
IN
THIS
SECTION
WE
SUMMARIZE
THE
LIMITATIONS
OF
JOULESORT
SPECULATE
ON
FUTURE
ENERGY
EFFICIENT
SYSTEMS
AND
WRAP
UP
LIMITATIONS
JOULESORT
DOES
NOT
ADDRESS
ALL
POSSIBLE
ENERGY
RELATED
CON
CERNS
SINCE
JOULESORT
FOCUSES
ON
DATA
MANAGEMENT
TASKS
IT
MISSES
SOME
IMPORTANT
ENERGY
RELEVANT
COMPONENTS
FOR
MUL
TIMEDIA
APPLICATIONS
JOULESORT
OMITS
DISPLAYS
WHICH
ARE
AN
IMPORTANT
COMPONENT
OF
TOTAL
POWER
FOR
MOBILE
DEVICES
GPUS
ALSO
CONSUME
SIGNIFICANT
P
OWER
AND
A
RE
U
BIQUITOUS
IN
DESKTOP
SYSTEMS
ALTHOUGH
WE
CAN
USE
GPUS
TO
SORT
OUR
BENCHMARK
DOES
NOT
REQUIRE
THEIR
USE
AS
A
RESULT
IT
LOSES
RELEVANCE
FOR
APPLICATIONS
WHERE
THESE
COMPONENTS
ARE
ES
SENTIAL
THERE
ARE
OTHER
ENERGY
RELATED
CONCERNS
IN
DATA
CENTERS
BEYOND
SYSTEM
POWER
THAT
WERE
DIFFICULT
TO
INCORPORATE
AT
A
HIGH
LEVEL
COOLING
REQUIRES
LOWERING
AMBIENT
TEMPER
ATURE
AND
EXTRACTING
HEAT
AWAY
FROM
SYSTEMS
JOULE
SORT
ACCOUNTS
ONLY
FOR
PART
OF
THE
SECOND
DELIVERING
POWER
TO
SYSTEMS
INCURS
LOSSES
AT
THE
RACK
AND
DATA
CENTER
LEVEL
WHICH
ARE
IGNORED
IN
JOULESORT
MOREOVER
MANY
SYSTEMS
ARE
USED
AS
AN
ENSEMBLE
IN
DATA
CENTERS
WITH
SOPHISTI
CATED
SCHEDULING
TECHNIQUES
TO
TRADE
PERFORMANCE
FOR
LOWER
ENERGY
AMONG
SYSTEMS
RATHER
THAN
AT
THE
COMPONENT
LEVEL
AS
A
SYSTEM
LEVEL
BENCHMARK
JOULESORT
MAY
NOT
IDENTIFY
THE
BENEFITS
O
F
UCH
METHODS
GREENER
SYSTEMS
WE
SPECULATE
ON
TWO
EMERGING
TECHNOLOGIES
THAT
MAY
IM
PROVE
THE
ENERGY
EFFICIENCY
OF
SYSTEMS
FOR
THE
SCALE
FLASH
M
EMORY
A
PPEARS
TO
B
E
A
P
ROMISING
TORAGE
TECHNOLOGY
DRIVEN
BY
THE
MOBILE
DEVICE
MARKET
PER
BYTE
IT
IS
ABOUT
CHEAPER
THAN
DRAM
AND
PROVIDES
SEQUENTIAL
READ
AND
WRITE
BANDWIDTH
CLOSE
TO
THAT
OF
DISKS
MORE
IMPORTANTLY
RANDOM
READ
I
OS
WITH
FLASH
A
RE
F
ASTER
T
HAN
D
ISK
AND
FLASH
CONSUMES
LESS
POWER
THAN
D
ISKS
THE
RANDOM
READS
ALLOW
INTERESTING
MODIFICATIONS
T
O
T
RADITIONAL
PASS
SORTING
ALGORITHMS
TO
DATE
THE
LARGEST
CARDS
AT
REASONABLE
COST
ARE
WE
ANTICIPATE
A
SYSTEM
SUCH
AS
A
LAPTOP
OR
LOW
POWER
EMBEDDED
DEVICE
THAT
CAN
LEVERAGE
MULTIPLE
FLASH
DEVICES
AS
THE
NEXT
WINNER
FOR
THE
LARGER
SCALES
AN
INTRIGUING
OPTION
IS
A
HYBRID
SYS
TEM
USING
A
LOW
POWER
CPU
LAPTOP
DISKS
AND
A
GPU
THE
GPUTERASORT
HAS
SHOWN
THAT
GPUS
CAN
PROVIDE
MUCH
BET
TER
IN
MEMORY
SORTING
BANDWIDTH
THAN
CPUS
USING
A
MOTHERBOARD
THAT
SUPPORTS
MORE
I
O
CONTROLLERS
AND
A
GPU
WE
COULD
SCALE
OUR
SYSTEM
TO
USE
MORE
DISKS
AN
INTEREST
ING
QUESTION
IS
WHETHER
THESE
PERFORMANCE
BENEFITS
MIGHT
BE
OFFSET
BY
THE
RECENT
TREND
IN
GPUS
TO
CONSUME
MORE
POWER
CLOSING
THIS
PAPER
PROPOSES
JOULESORT
A
SIMPLE
BALANCED
ENERGY
EFFICIENCY
BENCHMARK
WE
PRESENT
A
COMPLETE
BENCHMARK
A
WORKLOAD
METRIC
AND
GUIDELINES
AND
JUSTIFY
OUR
CHOICES
WE
ALSO
PRESENT
A
WINNER
THAT
IS
OVER
AS
EFFICIENT
AS
LAST
YEAR
ESTIMATED
WINNER
TODAY
THIS
SYSTEM
IS
HARD
TO
FIND
P
RE
ASSEMBLED
I
T
C
ONSISTS
O
F
A
C
OMMODITY
MOBILE
CLASS
CPU
AND
LAPTOP
DISKS
CONNECTED
THROUGH
SERVER
CLASS
PCI
E
I
O
CARDS
THE
DETAILS
OF
JOULESORT
ALREADY
HAVE
UNDERGONE
SIGNIFI
CANT
CHANGES
SINCE
ITS
INCEPTION
SINCE
JOULESORT
HAS
NOT
YET
BEEN
TRIED
IN
THE
WILD
WE
FULLY
EXPECT
FURTHER
REVISIONS
AND
FINE
TUNING
T
O
K
EEP
I
T
F
AIR
A
ND
R
ELEVANT
NEVERTHELESS
WE
LOOK
FORWARD
TO
ITS
USE
IN
GUIDING
ENERGY
EFFICIENCY
OPTI
MIZATIONS
IN
FUTURE
SYSTEMS
SPEED
SCALING
OF
PROCESSES
WITH
ARBITRARY
SPEEDUP
CURVES
ON
A
MULTIPROCESSOR
HO
LEUNG
CHAN
JEFF
EDMONDS
KIRK
PRUHS
ABSTRACT
WE
CONSIDER
THE
SETTING
OF
A
MULTIPROCESSOR
WHERE
THE
SPEEDS
OF
THE
M
PROCES
SORS
CAN
BE
INDIVIDUALLY
SCALED
JOBS
ARRIVE
OVER
TIME
AND
HAVE
VARYING
DEGREES
OF
PARALLELIZABILITY
A
NONCLAIRVOYANT
SCHEDULER
MUST
ASSIGN
THE
PROCESSES
TO
PROCESSORS
AND
SCALE
THE
SPEEDS
OF
THE
PROCESSORS
WE
CONSIDER
THE
OBJECTIVE
OF
ENERGY
PLUS
FLOW
TIME
WE
ASSUME
THAT
A
PROCESSOR
RUNNING
AT
SPEED
USES
POWER
SΑ
FOR
SOME
CONSTANT
Α
FOR
PROCESSES
THAT
MAY
HAVE
SIDE
EFFECTS
OR
THAT
ARE
NOT
CHECKPOINTABLE
WE
SHOW
AN
Ω
M
Α
Α
BOUND
ON
THE
COMPETITIVE
RATIO
OF
ANY
RANDOMIZED
ALGORITHM
FOR
CHECKPOINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
WE
GIVE
AN
O
LOG
M
COMPETITIVE
AL
GORITHM
THUS
FOR
PROCESSES
THAT
MAY
HAVE
SIDE
EFFECTS
OR
THAT
ARE
NOT
CHECKPOINTABLE
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
QUICKLY
WITH
THE
NUMBER
OF
PROCESSORS
BUT
FOR
CHECKPOINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
SLOWLY
WITH
THE
NUMBER
OF
PROCESSORS
WE
THEN
SHOW
A
LOWER
BOUND
OF
Ω
Α
M
ON
THE
COMPETITIVE
RATIO
OF
ANY
RANDOMIZED
ALGORITHM
FOR
CHECKPOINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
INTRODUCTION
DUE
TO
THE
POWER
RELATED
ISSUES
OF
ENERGY
AND
TEMPERATURE
MAJOR
CHIP
MANUFACTURERS
SUCH
AS
INTEL
AMD
AND
IBM
NOW
PRODUCE
CHIPS
WITH
MULTIPLE
CORES
PROCESSORS
AND
WITH
DY
NAMICALLY
SCALABLE
SPEEDS
AND
PRODUCE
ASSOCIATED
SOFTWARE
SUCH
AS
INTEL
SPEEDSTEP
AND
AMD
POWERNOW
THAT
ENABLES
AN
OPERATING
SYSTEM
TO
MANAGE
POWER
BY
SCALING
PROCESSOR
SPEED
CURRENTLY
MOST
MULTIPROCESSOR
CHIPS
HAVE
ONLY
A
HANDFUL
OF
PROCESSORS
BUT
CHIP
DE
SIGNERS
ARE
AGREED
UPON
THE
FACT
THAT
CHIPS
WITH
HUNDREDS
TO
THOUSANDS
OF
PROCESSORS
WILL
DOMINATE
THE
MARKET
IN
THE
NEXT
DECADE
THE
FOUNDER
OF
CHIP
MAKER
TILERA
ASSERTED
THAT
A
COROLLARY
TO
MOORE
LAW
WILL
BE
THAT
THE
NUMBER
OF
CORES
PROCESSORS
WILL
DOUBLE
EVERY
MONTHS
ACCORDING
TO
THE
WELL
KNOWN
CUBE
ROOT
RULE
A
CMOS
BASED
PROCESSOR
RUNNING
AT
SPEED
WILL
HAVE
A
DYNAMIC
POWER
P
OF
APPROXIMATELY
IN
THE
ALGORITHMIC
LITERATURE
THIS
IS
THE
UNIVERSITY
OF
HONG
KONG
YORK
UNIVERSITY
SUPPORTED
IN
PART
BY
NSERC
CANADA
UNIVERSITY
OF
PITTSBURGH
SUPPORTED
IN
PART
BY
AN
IBM
FACULTY
AWARD
AND
BY
NSF
GRANTS
CNS
CCF
IIS
AND
CCF
USUALLY
GENERALIZED
TO
P
SΑ
THUS
IN
PRINCIPLE
P
PROCESSORS
RUNNING
AT
SPEED
P
COULD
DO
THE
WORK
OF
ONE
PROCESSOR
RUNNING
AT
SPEED
BUT
AT
PΑ
OF
THE
POWER
BUT
IN
SPITE
OF
THIS
CHIP
MAKERS
WAITED
UNTIL
THE
POWER
COSTS
BECAME
PROHIBITIVE
BEFORE
SWITCHING
TO
MULTIPROCESSOR
CHIPS
BECAUSE
OF
THE
TECHNICAL
DIFFICULTIES
IN
GETTING
P
SPEED
P
PROCESSORS
TO
COME
CLOSE
TO
DOING
THE
WORK
OF
ONE
SPEED
PROCESSOR
THIS
IS
PARTICULARLY
TRUE
WHEN
ONE
HAS
MANY
PROCESSORS
AND
FEW
PROCESSES
WHERE
THESE
PROCESSES
HAVE
WIDELY
VARYING
DEGREES
OF
PARALLELIZABILITY
THAT
IS
SOME
PROCESSES
MAY
BE
CONSIDERABLY
SPED
UP
WHEN
SIMULTANEOUSLY
RUN
ON
MULTIPLE
PROCESSORS
WHILE
SOME
PROCESSES
MAY
NOT
BE
SPED
UP
AT
ALL
THIS
COULD
BE
BECAUSE
THE
UNDERLYING
ALGORITHM
IS
INHERENTLY
SEQUENTIAL
IN
NATURE
OR
BECAUSE
THE
PROCESS
WAS
NOT
CODED
IN
A
WAY
TO
MAKE
IT
EASILY
PARALLELIZABLE
TO
INVESTIGATE
THIS
ISSUE
WE
ADOPT
THE
FOLLOWING
GENERAL
MODEL
OF
PARALLELIZABILITY
USED
IN
EACH
PROCESS
CONSISTS
OF
A
SEQUENCE
OF
PHASES
EACH
PHASE
CONSISTS
OF
A
POSITIVE
REAL
NUMBER
THAT
DENOTES
THE
AMOUNT
OF
WORK
IN
THAT
PHASE
AND
A
SPEEDUP
FUNCTION
THAT
SPECIFIES
THE
RATE
AT
WHICH
WORK
IS
PROCESSED
IN
THIS
PHASE
AS
A
FUNCTION
OF
THE
NUMBER
OF
PROCESSORS
EXECUTING
THE
PROCESS
THE
SPEEDUP
FUNCTIONS
MAY
BE
ARBITRARY
OTHER
THAN
WE
ASSUME
THAT
THEY
ARE
NONDECREASING
A
PROCESS
DOESN
T
RUN
SLOWER
IF
IT
IS
GIVEN
MORE
PROCESSORS
AND
SUBLINEAR
A
PROCESS
SATISFIES
BRENT
THEOREM
THAT
IS
INCREASING
THE
NUMBER
OF
PROCESSORS
DOESN
T
INCREASE
THE
EFFICIENCY
OF
COMPUTATION
THE
OPERATING
SYSTEM
NEEDS
A
PROCESS
ASSIGNMENT
POLICY
FOR
DETERMINING
AT
EACH
TIME
WHICH
PROCESSORS
IF
ANY
A
PARTICULAR
PROCESS
IS
ASSIGNED
TO
WE
ASSUME
THAT
A
PROCESS
MAY
BE
ASSIGNED
TO
MULTIPLE
PROCESSORS
IN
TANDEM
WITH
THIS
THE
OPERATING
SYSTEM
WILL
ALSO
NEED
A
SPEED
SCALING
POLICY
FOR
SETTING
THE
SPEED
OF
EACH
PROCESSOR
IN
ORDER
TO
BE
IMPLEMENTABLE
IN
A
REAL
SYSTEM
THE
SPEED
SCALING
AND
PROCESS
ASSIGNMENT
POLICIES
MUST
BE
ONLINE
SINCE
THE
SYSTEM
WILL
NOT
IN
GENERAL
KNOW
ABOUT
PROCESSES
ARRIVING
IN
THE
FUTURE
FURTHER
TO
BE
IMPLEMENTABLE
IN
A
GENERIC
OPERATING
SYSTEM
THESE
POLICIES
MUST
BE
NONCLAIRVOYANT
SINCE
IN
GENERAL
THE
OPERATING
SYSTEM
DOES
NOT
KNOW
THE
SIZE
WORK
OF
EACH
PROCESS
WHEN
THE
PROCESS
IS
RELEASED
TO
THE
OPERATING
SYSTEM
NOR
THE
DEGREE
TO
WHICH
THAT
PROCESS
IS
PARALLELIZABLE
SO
A
NONCLAIRVOYANT
ALGORITHM
ONLY
KNOWS
WHEN
PROCESSES
HAVE
BEEN
RELEASED
AND
FINISHED
IN
THE
PAST
AND
WHICH
PROCESSES
HAVE
BEEN
RUN
ON
EACH
PROCESSOR
AT
EACH
TIME
IN
THE
PAST
THE
OPERATING
SYSTEM
HAS
COMPETING
DUAL
OBJECTIVES
AS
IT
BOTH
WANTS
TO
OPTIMIZE
SOME
SCHEDULE
QUALITY
OF
SERVICE
OBJECTIVE
AS
WELL
AS
SOME
POWER
RELATED
OBJECTIVE
IN
THIS
PA
PER
WE
WILL
CONSIDER
THE
FORMAL
OBJECTIVE
OF
MINIMIZING
A
LINEAR
COMBINATION
OF
TOTAL
RE
SPONSE
FLOW
TIME
THE
SCHEDULE
OBJECTIVE
AND
TOTAL
ENERGY
USED
THE
POWER
OBJECTIVE
IN
THE
CONCLUSION
WE
WILL
DISCUSS
THE
RELATIONSHIP
BETWEEN
THIS
ENERGY
OBJECTIVE
AND
A
TEMPER
ATURE
OBJECTIVE
THIS
OBJECTIVE
OF
FLOW
PLUS
ENERGY
HAS
A
NATURAL
INTERPRETATION
SUPPOSE
THAT
THE
USER
SPECIFIES
HOW
MUCH
IMPROVEMENT
IN
FLOW
CALL
THIS
AMOUNT
Ρ
IS
NECESSARY
TO
JUSTIFY
SPENDING
ONE
UNIT
OF
ENERGY
FOR
EXAMPLE
THE
USER
MIGHT
SPECIFY
THAT
HE
IS
WILLING
TO
SPEND
ERG
OF
ENERGY
FROM
THE
BATTERY
FOR
A
DECREASE
OF
MICRO
SECONDS
IN
FLOW
THEN
THE
OPTIMAL
SCHEDULE
FROM
THIS
USER
PERSPECTIVE
IS
THE
SCHEDULE
THAT
OPTIMIZES
Ρ
TIMES
THE
ENERGY
USED
PLUS
THE
TOTAL
FLOW
BY
CHANGING
THE
UNITS
OF
EITHER
ENERGY
OR
TIME
ONE
MAY
ASSUME
WITHOUT
LOSS
OF
GENERALITY
THAT
Ρ
SO
THE
PROBLEM
WE
WANT
TO
ADDRESS
HERE
IS
HOW
TO
DESIGN
A
NONCLAIRVOYANT
PROCESS
AS
SIGNMENT
POLICY
AND
A
SPEED
SCALING
POLICY
THAT
WILL
BE
COMPETITIVE
FOR
THE
OBJECTIVE
OF
FLOW
PLUS
ENERGY
THE
CASE
OF
A
SINGLE
PROCESSOR
WAS
CONSIDERED
IN
IN
THE
SINGLE
PROCESSOR
CASE
THE
PARALLELIZABILITY
OF
THE
PROCESSES
IS
NOT
AN
ISSUE
IF
ALL
THE
PROCESSES
ARRIVE
AT
TIME
THEN
IN
THE
OPTIMAL
SCHEDULE
THE
POWER
AT
TIME
T
IS
Θ
NT
WHERE
NT
IS
THE
NUMBER
OF
ACTIVE
PROCESSES
AT
TIME
T
THE
ALGORITHM
CONSIDERED
IN
RUNS
AT
A
SPEED
OF
Δ
Α
FOR
SOME
CONSTANT
Δ
THE
PROCESS
ASSIGNMENT
ALGORITHM
CONSIDERED
IN
IS
LATEST
ARRIVAL
PROCESSOR
SHARING
LAPS
LAPS
WAS
PROPOSED
IN
IN
THE
CONTEXT
OF
RUNNING
PROCESSES
WITH
ARBITRARY
SPEEDUP
FUNCTIONS
ON
FIXED
SPEED
PROCESSORS
AND
IT
WAS
SHOWN
TO
BE
SCALABLE
I
E
Ǫ
SPEED
O
COMPETITIVE
FOR
THE
OBJECTIVE
OF
TOTAL
FLOW
TIME
IN
THIS
SETTING
LAPS
IS
PARAMETERIZED
BY
A
CONSTANT
Β
AND
SHARES
THE
PROCESSING
POWER
EVENLY
AMONG
THE
ΒNT
MOST
RECENTLY
ARRIVING
PROCESSES
NOTE
THAT
THE
SPEED
SCALING
POLICY
AND
LAPS
ARE
BOTH
NONCLAIRVOYANT
SHOWED
THAT
BY
PICKING
Δ
AND
Β
APPROPRIATELY
THE
RESULTING
ALGORITHM
IS
Α
COMPETITIVE
FOR
THE
OBJECTIVE
OF
FLOW
PLUS
ENERGY
ON
A
SINGLE
SPEED
SCALABLE
PROCESSOR
OUR
RESULTS
HERE
WE
CONSIDER
EXTENDING
THE
RESULTS
IN
TO
THE
SETTING
OF
A
MULTIPROCESSOR
WITH
M
PROCESSORS
IT
IS
STRAIGHT
FORWARD
TO
NOTE
THAT
IF
ALL
OF
THE
WORK
IS
PARALLELIZABLE
THEN
THE
MULTIPROCESSOR
SETTING
IS
ESSENTIALLY
EQUIVALENT
TO
THE
UNIPROCESSOR
SETTING
TO
GAIN
SOME
INTUITION
OF
THE
DIFFICULTY
THAT
VARYING
SPEEDUP
FUNCTIONS
POSE
LET
US
FIRST
CONSIDER
AN
INSTANCE
OF
ONE
PROCESS
THAT
MAY
EITHER
BE
SEQUENTIAL
OR
PARALLELIZABLE
IF
AN
ALGORITHM
RUNS
THIS
PROCESS
ON
FEW
OF
THE
PROCESSORS
THEN
THE
ALGORITHM
COMPETITIVE
RATIO
WILL
BE
BAD
IF
THE
PROCESS
IS
PARALLELIZABLE
AND
THE
OPTIMAL
SCHEDULE
RUNS
THE
PROCESS
ON
ALL
OF
THE
PROCESSORS
NOTE
THAT
IF
THE
ALGORITHM
WANTED
TO
BE
COMPETITIVE
ON
FLOW
TIME
IT
WOULD
HAVE
TO
RUN
TOO
FAST
TO
BE
COMPETITIVE
ON
ENERGY
IF
AN
ALGORITHM
RUNS
THIS
PROCESS
ON
MANY
OF
THE
PROCESSORS
THEN
THE
ALGORITHM
COMPETITIVE
RATIO
WILL
BE
BAD
IF
THE
PROCESS
IS
SEQUENTIAL
AND
THE
OPTIMAL
SCHEDULE
RUNS
THE
PROCESS
ON
FEW
PROCESSORS
IF
THE
ALGORITHM
WANTED
TO
BE
COMPETITIVE
ON
ENERGY
IT
WOULD
HAVE
TO
RUN
TOO
SLOW
TO
BE
COMPETITIVE
ON
FLOW
TIME
FORMALIZING
THIS
ARGUMENT
WE
SHOW
IN
SECTION
A
LOWER
BOUND
OF
Ω
M
Α
Α
ON
THE
COMPETITIVE
RATIO
OF
ANY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
AGAINST
AN
OBLIVIOUS
ADVERSARY
WITH
AN
ADDITIONAL
ASSUMPTION
THAT
WE
WILL
NOW
DISCUSS
AT
FIRST
GLANCE
SUCH
A
STRONG
LOWER
BOUND
FOR
SUCH
AN
EASY
INSTANCE
MIGHT
LEAD
ONE
TO
CONCLUDE
THAT
THERE
IS
NO
WAY
THAT
THE
SCHEDULER
CAN
BE
EXPECTED
TO
GUARANTEE
REASONABLY
COMPETITIVE
SCHEDULES
BUT
ON
FURTHER
REFLECTION
ONE
REALIZES
THAT
AN
UNDERLYING
ASSUMPTION
IN
THIS
LOWER
BOUND
IS
THAT
ONLY
ONE
COPY
OF
A
PROCESS
CAN
BE
RUN
IF
A
PROCESS
DOES
NOT
HAVE
SIDE
EFFECTS
THAT
IS
IF
THE
PROCESS
DOESN
T
CHANGE
EFFECT
ANYTHING
EXTERNAL
TO
ITSELF
THEN
THIS
ASSUMPTION
IS
NOT
GENERALLY
VALID
ONE
COULD
RUN
MULTIPLE
COPIES
OF
A
PROCESS
SIMULTANEOUSLY
WITH
EACH
COPY
BEING
RUN
ON
A
DIFFERENT
NUMBER
OF
PROCESSORS
AND
HALT
COMPUTATION
WHEN
THE
FIRST
COPY
FINISHES
FOR
EXAMPLE
IN
THE
INSTANCE
IN
THE
PREVIOUS
PARAGRAPH
ONE
COULD
BE
O
COMPETITIVE
IF
THE
PROCESS
DIDN
T
HAVE
SIDE
EFFECTS
BY
RUNNING
ONE
COPY
ON
A
SINGLE
PROCESSOR
AND
RUNNING
ONE
COPY
ON
THE
REST
OF
THE
PROCESSORS
GENERALIZING
THIS
APPROACH
ONE
CAN
OBTAIN
A
O
LOG
M
COMPETITIVE
ALGORITHM
FOR
INSTANCES
CONSISTING
OF
PROCESSES
THAT
HAVE
NO
SIDE
EFFECTS
AND
WHERE
THE
SPEED
UP
FUNCTION
DOESN
T
CHANGE
UNFORTUNATELY
WE
SHOW
IN
SECTION
THAT
SUCH
A
RESULT
CAN
NOT
BE
OBTAINED
IF
PROCESSES
CAN
HAVE
MULTIPLE
PHASES
WITH
DIFFERENT
SPEED
UP
FUNCTIONS
WE
ACCOMPLISH
THIS
BY
SHOWING
THAT
THE
COMPETITIVE
RATIO
OF
ANY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
THAT
RUNS
MULTIPLE
INDEPENDENT
COPIES
OF
A
PROCESS
AGAINST
AN
OBLIVIOUS
ADVERSARY
IS
Ω
MΩ
Α
CONTEMPLATING
THIS
SECOND
LOWER
BOUND
IT
SUGGESTS
THAT
TO
BE
REASONABLY
COMPETITIVE
THE
ALGORITHM
MUST
BE
ABLE
TO
PROCESS
WORK
ON
ALL
COPIES
OF
A
JOB
AT
THE
MAXIMUM
RATE
OF
WORK
PROCESSING
ON
ANY
COPY
IF
A
PROCESSES
HAD
SMALL
STATE
SO
THAT
THE
OVERHEAD
OF
CHECKPOINTING
ISN
T
PROHIBITIVE
ONE
MIGHT
REASONABLY
APPROXIMATE
THIS
BY
CHECKPOINTING
SAVING
THE
STATE
OF
EACH
COPY
PERIODICALLY
AND
THEN
RESTARTING
EACH
COPY
FROM
THE
POINT
OF
EXECUTION
OF
THE
COPY
THAT
MADE
THE
MOST
PROGRESS
IN
SECTION
WE
FORMALIZE
THIS
INTUITION
WE
GIVE
A
PROCESS
ASSIGNMENT
ALGORITHM
MULTILAPS
WHICH
IS
A
MODIFICATION
OF
LAPS
WE
SHOW
THAT
BY
COMBINING
MULTILAPS
WITH
THE
NATURAL
SPEED
SCALING
ALGORITHM
ONE
OBTAINS
AN
O
LOG
M
COMPETITIVE
ALGORITHM
IF
ALL
COPIES
PROCESS
WORK
AT
THE
RATE
OF
THE
FASTEST
COPY
THERE
ARE
TWO
STEPS
IN
THE
ANALYSIS
OF
MULTILAPS
THE
FIRST
STEP
IS
TO
SHOW
THAT
THERE
IS
A
WORST
CASE
INSTANCE
WHERE
EVERY
SPEEDUP
FUNCTION
IS
PARALLEL
UP
TO
SOME
NUMBER
OF
PROCESSORS
AND
THEN
IS
CONSTANT
THIS
SHOWS
THAT
THE
WORST
CASE
SPEEDUP
FUNCTIONS
FOR
SPEED
SCALABLE
PROCESSORS
ARE
MORE
VARIED
THAN
FOR
FIXED
SPEED
PROCESSORS
WHERE
IT
IS
SUFFICIENT
TO
RESTRICT
ATTENTION
TO
ONLY
PARALLELIZABLE
AND
SEQUENTIAL
SPEEDUP
FUNCTIONS
THE
SECOND
STEP
IN
THE
ANALYSIS
OF
MULTILAPS
IS
A
REDUCTION
TO
ESSENTIALLY
THE
ANALYSIS
OF
LAPS
IN
A
UNIPROCESSOR
SETTING
TECHNICALLY
WE
NEED
TO
ANALYZE
LAPS
WHEN
SOME
WORK
IS
SEQUENTIAL
THAT
IS
IT
HAS
THE
SPECIAL
PROPERTY
THAT
IT
IS
PROCESSED
AT
UNIT
RATE
INDEPENDENT
OF
THE
SPEED
OF
THE
PROCESSOR
WE
THEN
DISCUSS
HOW
TO
GENERALIZE
THE
ANALYSIS
OF
LAPS
IN
TO
ALLOW
SEQUENTIAL
WORK
USING
TECHNIQUES
FROM
IN
SECTION
WE
THEN
SHOW
A
LOWER
BOUND
OF
Ω
Α
M
ON
THE
COMPETITIVE
RATIO
OF
ANY
NONCLAIRVOYANT
RANDOMIZED
ALGORITHM
AGAINST
AN
OBLIVIOUS
ADVERSARY
FOR
CHECKPOINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
IN
FACT
THIS
LOWER
HOLDS
EVEN
IF
THE
RATE
THAT
A
PROCESS
IS
PROCESSED
IS
THE
SUM
NOT
THE
MAXIMUM
OF
RATE
OF
THE
VARIOUS
COPIES
THUS
IN
SUMMARY
FOR
PROCESSES
THAT
MAY
HAVE
SIDE
EFFECTS
OR
THAT
ARE
NOT
CHECKPOINTABLE
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
QUICKLY
WITH
THE
NUMBER
OF
PROCESSORS
BUT
FOR
CHECK
POINTABLE
PROCESSES
WITHOUT
SIDE
EFFECTS
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
SLOWLY
WITH
THE
NUMBER
OF
PROCESSORS
THIS
SHOWS
THE
IMPORTANCE
OF
BEING
ABLE
TO
EFFICIENTLY
CHECKPOINT
MULTIPLE
COPIES
OF
A
PROCESS
IN
A
SETTING
OF
PROCESSES
WITH
VARYING
DEGREES
OF
PARALLELIZABILITY
AND
INDIVIDUALLY
SPEED
SCALABLE
MULTIPROCESSORS
RELATED
RESULTS
WE
START
WITH
SOME
RESULTS
IN
THE
LITERATURE
ABOUT
SCHEDULING
WITH
THE
OBJECTIVE
OF
TOTAL
FLOW
TIME
ON
A
SINGLE
FIXED
SPEED
PROCESSOR
IT
IS
WELL
KNOWN
THAT
THE
ONLINE
CLAIRVOYANT
ALGORITHM
SHORTEST
REMAINING
PROCESSING
TIME
SRPT
IS
OPTIMAL
THE
COMPETITIVE
RATIO
OF
ANY
DETERMINISTIC
NONCLAIRVOYANT
ALGORITHM
IS
Ω
AND
THE
COMPETITIVE
RATIO
OF
EVERY
RANDOMIZED
ALGORITHM
AGAINST
AN
OBLIVIOUS
ADVERSARY
IS
Ω
LOG
N
A
RANDOMIZED
VERSION
OF
THE
MULTI
LEVEL
FEEDBACK
QUEUE
ALGORITHM
IS
O
LOG
N
COMPETITIVE
THE
NONCLAIR
VOYANT
ALGORITHM
SHORTEST
ELAPSED
TIME
FIRST
SETF
IS
SCALABLE
THAT
IS
IT
IS
Ǫ
SPEED
O
COMPETITIVE
FOR
ANY
ARBITRARILY
SMALL
BUT
FIXED
Ǫ
SETF
SHARES
THE
PROCESSOR
EQUALLY
AMONG
ALL
PROCESSES
THAT
HAVE
BEEN
RUN
THE
LEAST
WE
NOW
CONSIDER
SCHEDULING
PROCESSES
WITH
ARBITRARY
SPEEDUP
FUNCTIONS
ON
FIXED
SPEED
PROCESSORS
FOR
THE
OBJECTIVE
OF
TOTAL
FLOW
TIME
THE
ALGORITHM
ROUND
ROBIN
RR
ALSO
CALLED
EQUIPARTITION
AND
PROCESSOR
SHARING
THAT
SHARES
THE
PROCESSORS
EQUALLY
AMONG
ALL
PROCESSES
IS
Ǫ
SPEED
O
COMPETITIVE
AS
MENTIONED
BEFORE
LAPS
IS
SCALABLE
WE
NOW
CONSIDER
SPEED
SCALING
ALGORITHMS
ON
A
SINGLE
PROCESSOR
FOR
THE
OBJECTIVE
OF
FLOW
PLUS
ENERGY
GIVE
EFFICIENT
OFFLINE
ALGORITHMS
WE
NOW
DESCRIBE
THE
RESULTS
FOR
ONLINE
CLAIRVOYANT
ALGORITHMS
THIS
SETTING
WAS
STUDIED
IN
A
SEQUENCE
OF
PAPERS
WHICH
CULMINATED
IN
THE
FOLLOWING
RESULT
THE
SCHEDULING
ALGORITHM
THAT
USES
SHORTEST
REMAINING
PROCESSING
TIME
SRPT
FOR
PROCESS
ASSIGNMENT
AND
POWER
EQUAL
TO
ONE
MORE
THAN
THE
NUMBER
OF
ACTIVE
PROCESSES
FOR
SPEED
SCALING
IS
Ǫ
COMPETITIVE
FOR
THE
OBJECTIVE
OF
TOTAL
FLOW
PLUS
ENERGY
ON
ARBITRARY
WORK
UNIT
WEIGHT
PROCESSES
EVEN
IF
THE
POWER
FUNCTION
IS
ARBITRARY
SO
CLAIRVOYANT
ALGORITHMS
CAN
BE
O
COMPETITIVE
INDEPENDENT
OF
THE
POWER
FUNCTION
SHOWED
THAT
NONCLAIRVOYANT
ALGORITHMS
CAN
NOT
BE
O
COMPETITIVE
IF
THE
POWER
FUNCTION
IS
GROWING
TOO
QUICKLY
THE
CASE
OF
WEIGHTED
FLOW
TIME
HAS
ALSO
BEEN
STUDIED
THE
SCHEDULING
ALGORITHM
THAT
USES
HIGHEST
DENSITY
FIRST
HDF
FOR
PROCESS
ASSIGNMENT
AND
POWER
EQUAL
TO
THE
FRACTIONAL
WEIGHT
OF
THE
ACTIVE
PROCESSES
FOR
SPEED
SCALING
IS
Ǫ
COMPETITIVE
FOR
THE
OBJECTIVE
OF
FRACTIONAL
WEIGHTED
FLOW
PLUS
ENERGY
ON
ARBITRARY
WORK
ARBITRARY
WEIGHT
PROCESSES
AN
O
COMPETITIVE
ALGORITHM
FOR
WEIGHTED
FLOW
PLUS
ENERGY
CAN
THEN
BE
OBTAINED
USING
THE
KNOWN
RESOURCE
AUGMENTATION
ANALYSIS
OF
HDF
EXTEND
SOME
OF
THE
RESULTS
FOR
THE
UNBOUNDED
SPEED
MODEL
TO
A
MODEL
WHERE
THERE
IS
AN
UPPER
BOUND
ON
THE
SPEED
OF
A
PROCESSOR
THERE
ARE
MANY
RELATED
SCHEDULING
PROBLEMS
WITH
OTHER
OBJECTIVES
AND
OR
OTHER
ASSUMP
TIONS
ABOUT
THE
PROCESSORS
AND
INSTANCE
SURVEYS
CAN
BE
FOUND
IN
FORMAL
PROBLEM
DEFINITION
AND
NOTATIONS
AN
INSTANCE
CONSISTS
OF
A
COLLECTION
J
JN
WHERE
JOB
JI
HAS
A
RELEASE
ARRIVAL
TIME
RI
AND
A
SEQUENCE
OF
PHASES
JQI
EACH
PHASE
IS
AN
ORDERED
PAIR
WQ
ΓQ
WHERE
I
I
I
I
I
Q
IS
A
POSITIVE
REAL
NUMBER
THAT
DENOTES
THE
AMOUNT
OF
WORK
IN
THE
PHASE
AND
ΓQ
IS
A
FUNCTION
CALLED
THE
SPEEDUP
FUNCTION
THAT
MAPS
A
NONNEGATIVE
REAL
NUMBER
TO
A
NONNEGATIVE
REAL
NUMBER
ΓQ
P
REPRESENTS
THE
RATE
AT
WHICH
WORK
IS
PROCESSED
FOR
PHASE
Q
OF
JOB
I
WHEN
ONE
COPY
OF
THE
JOB
IS
RUN
ON
P
PROCESSORS
RUNNING
AT
SPEED
IF
THESE
PROCESSORS
ARE
RUNNING
AT
SPEED
THEN
WORK
IS
PROCESSED
AT
A
RATE
OF
SΓQ
P
A
SCHEDULE
SPECIFIES
FOR
EACH
TIME
AND
FOR
EACH
COPY
OF
A
JOB
A
NONNEGATIVE
REAL
NUMBER
SPECIFYING
THE
NUMBER
OF
PROCESSORS
ASSIGNED
TO
THE
COPY
OF
THE
JOB
AND
A
NONNEGATIVE
REAL
SPEED
WE
THUS
ASSUME
THAT
IF
SEVERAL
PROCESSORS
ARE
WORKING
ON
THE
SAME
INSTANCE
COPY
OF
A
JOB
THEN
THEY
MUST
ALL
RUN
AT
THE
SAME
SPEED
BUT
DIFFERENT
COPIES
CAN
RUN
AT
DIFFERENT
SPEEDS
THE
NUMBER
OF
PROCESSORS
ASSIGNED
AT
ANY
TIME
CAN
BE
AT
MOST
M
THE
NUMBER
OF
PROCESSORS
NOTE
THAT
FORMALLY
A
SCHEDULE
DOES
NOT
SPECIFY
AN
ASSIGNMENT
OF
COPIES
OF
JOBS
TO
PROCESSORS
A
NONCLAIRVOYANT
ALGORITHM
ONLY
KNOWS
WHEN
PROCESSES
HAVE
BEEN
RELEASED
AND
FINISHED
IN
THE
PAST
AND
WHICH
PROCESSES
HAVE
BEEN
RUN
ON
EACH
PROCESSOR
EACH
TIME
IN
THE
PAST
IN
PARTICULAR
A
NONCLAIRVOYANT
ALGORITHM
DOES
NOT
KNOW
WQ
NOR
THE
CURRENT
PHASE
Q
NOR
THE
SPEEDUP
FUNCTION
ΓQ
IN
THIS
PAPER
WE
CONSIDER
SEVERAL
DIFFERENT
MODELS
DEPENDING
ON
HOW
THE
PROCESSING
ON
DIFFERENT
COPIES
INTERACT
ASSUME
MULTIPLE
COPIES
OF
JOB
I
ARE
RUN
WITH
THE
SPEED
AND
NUMBER
OF
PROCESSORS
ASSIGNED
TO
THE
K
TH
COPY
BEING
SK
AND
PK
IN
THE
INDEPENDENT
PROCESSING
MODEL
IF
COPY
K
IS
RUNNING
IN
A
PHASE
WITH
SPEEDUP
FUNCTION
Γ
THEN
WORK
IS
PROCESSED
ON
THIS
COPY
AT
RATE
SKΓ
PK
INDEPENDENT
OF
THE
RATE
OF
PROCESSING
ON
THE
OTHER
COPIES
IN
THE
MAXIMUM
PROCESSING
MODEL
IF
EACH
COPY
OF
JOB
JI
IS
IN
A
PHASE
WITH
SPEEDUP
FUNCTION
Γ
THEN
EACH
COPY
PROCESSES
WORK
AT
A
RATE
OF
MAXK
SKΓ
PK
IN
THE
SUM
PROCESSING
MODEL
IF
EACH
COPY
OF
JOB
JI
IS
IN
A
PHASE
WITH
SPEEDUP
FUNCTION
Γ
THEN
EACH
COPY
PROCESSES
WORK
AT
A
RATE
OF
K
SKΓ
PK
NOTE
THAT
AS
A
CONSEQUENCE
OF
THESE
DEFINITIONS
IN
THE
MAXIMUM
PROCESSING
MODEL
AND
IN
THE
SUM
PROCESSING
MODEL
IT
IS
THE
CASE
THAT
EACH
COPY
OF
EACH
JOB
IS
ALWAYS
AT
THE
SAME
POINT
IN
ITS
EXECUTION
THE
COMPLETION
TIME
OF
A
JOB
JI
DENOTED
CI
IS
THE
FIRST
POINT
OF
TIME
WHEN
ALL
THE
WORK
ON
SOME
COPY
OF
THE
JOB
HAS
BEEN
PROCESSED
NOTE
THAT
IN
THE
LANGUAGE
OF
SCHEDULING
WE
ARE
ASSUMING
THAT
PREEMPTION
IS
ALLOWED
THAT
IS
A
JOB
MAYBE
BE
SUSPENDED
AND
LATER
RESTARTED
FROM
THE
POINT
OF
SUSPENSION
A
JOB
IS
SAID
TO
BE
ACTIVE
AT
TIME
T
IF
IT
HAS
BEEN
RELEASED
BUT
HAS
NOT
COMPLETED
I
E
RI
T
CI
THE
RESPONSE
FLOW
TIME
OF
JOB
JI
IS
CI
RI
WHICH
IS
THE
LENGTH
OF
THE
TIME
INTERVAL
DURING
WHICH
THE
JOB
IS
ACTIVE
LET
NT
BE
THE
NUMBER
OF
ACTIVE
JOBS
AT
TIME
T
ANOTHER
FORMULATION
OF
TOTAL
FLOW
TIME
IS
NTDT
WHEN
RUNNING
AT
SPEED
A
PROCESSOR
CONSUMES
P
SΑ
UNITS
OF
ENERGY
PER
UNIT
TIME
WHERE
Α
IS
SOME
FIXED
CONSTANT
WE
CALL
P
THE
POWER
FUNCTION
A
PHASE
OF
A
JOB
IS
PARALLELIZABLE
IF
ITS
SPEEDUP
FUNCTION
IS
Γ
P
P
INCREASING
THE
NUMBER
OF
PROCESSORS
ALLOCATED
TO
A
PARALLELIZABLE
PHASE
BY
A
FACTOR
OF
INCREASES
THE
RATE
OF
PROCESSING
BY
A
FACTOR
OF
A
PHASE
OF
A
JOB
IS
PARALLEL
UP
TO
Q
PROCESSORS
IF
Γ
P
P
FOR
P
Q
AND
Γ
P
Q
FOR
P
Q
A
SPEEDUP
FUNCTION
Γ
IS
NONDECREASING
IF
AND
ONLY
IF
Γ
Γ
WHENEVER
A
SPEEDUP
FUNCTION
Γ
IS
SUBLINEAR
IF
AND
ONLY
IF
Γ
Γ
WHENEVER
WE
ASSUME
ALL
SPEEDUP
FUNCTIONS
Γ
IN
THE
INPUT
INSTANCE
ARE
NONDECREASING
AND
SUBLINEAR
WE
FURTHER
ASSUME
THAT
ALL
SPEEDUP
FUNCTIONS
SATISFY
Γ
P
P
FOR
P
THIS
NATURAL
ASSUMPTION
MEANS
THAT
WHEN
A
JOB
JI
IS
ASSIGNED
TO
A
SINGLE
PROCESSOR
AND
SHARES
THIS
PROCESSOR
WITH
OTHER
JOBS
THE
RATE
THAT
JI
IS
PROCESSED
IS
THE
FRACTION
OF
THE
PROCESSOR
THAT
JI
RECEIVES
TIMES
THE
SPEED
OF
THE
PROCESSOR
LET
A
BE
AN
ALGORITHM
AND
J
AN
INSTANCE
WE
DENOTE
THE
SCHEDULE
OUTPUT
BY
A
ON
J
AS
A
J
WE
LET
FA
J
AND
EA
J
DENOTE
THE
TOTAL
FLOW
TIME
AND
ENERGY
INCURRED
IN
A
J
LET
COSTA
J
FA
J
EA
J
DENOTE
THE
COST
WE
WILL
USE
M
AS
A
SHORT
HAND
FOR
MULTILAPS
LET
OPT
BE
THE
OPTIMAL
ALGORITHM
THAT
ALWAYS
MINIMIZES
TOTAL
FLOW
TIME
PLUS
ENERGY
A
RANDOMIZED
ALGORITHM
A
IS
C
COMPETITIVE
OR
HAS
COMPETITIVE
RATIO
C
IF
FOR
ALL
INSTANCES
J
E
COSTA
J
C
COSTOPT
J
LOWER
BOUNDS
FOR
SINGLE
COPY
AND
NON
CHECKPOINTING
ALGORITHMS
IN
THIS
SECTION
WE
SHOW
THAT
THE
COMPETITIVE
RATIO
MUST
GROW
QUICKLY
WITH
THE
NUMBER
OF
PROCESSORS
IF
ONLY
ONE
COPY
OF
EACH
JOB
CAN
BE
RUNNING
LEMMA
OR
IF
MULTIPLE
COPIES
ARE
ALLOWED
BUT
NO
CHECKPOINTING
IS
ALLOWED
LEMMA
WE
FIRST
START
WITH
A
COUPLE
BASIC
LEMMAS
ABOUT
OPTIMAL
SCHEDULES
THAT
WILL
BE
USEFUL
THROUGHOUT
THE
PAPER
LEMMA
CONSIDER
A
JOB
WITH
WORK
W
AND
WITH
A
SINGLE
PHASE
WITH
A
SPEEDUP
FUNCTION
THAT
IS
PARALLELIZABLE
UP
TO
Q
PROCESSORS
ASSUME
THAT
THE
JOB
IS
RUN
ON
P
Q
PROCESSORS
THEN
THE
OPTIMAL
SPEED
IS
Α
FOR
A
COST
OF
Θ
W
ASSUME
THAT
THE
JOB
IS
RUN
ON
P
Q
PROCESSORS
THEN
THE
OPTIMAL
SPEED
IS
Α
FOR
A
COST
OF
Θ
Α
PROOF
FIRST
CONSIDER
THAT
CASE
THAT
P
Q
LET
BE
THE
SPEED
OF
THE
PROCESSORS
THE
FLOW
PLUS
ENERGY
IS
THEN
W
PSΑ
W
W
SΑ
THIS
IS
MINIMIZED
BY
SETTING
Α
PS
PS
PS
FOR
A
COST
OF
Θ
W
Α
P
ENERGY
IS
THEN
W
PSΑ
W
THIS
IS
MINIMIZED
BY
SETTING
Α
GIVING
A
COST
OF
QS
QS
Θ
Α
Q
Α
P
LEMMA
CONSIDER
A
JOB
WITH
WORK
W
WITH
A
SINGLE
PHASE
WITH
A
SPEEDUP
FUNCTION
THAT
IS
PARALLELIZABLE
UP
TO
Q
PROCESSORS
THE
OPTIMAL
SCHEDULE
USES
P
Q
PROCESSORS
RUN
AT
SPEED
Α
Q
Α
FOR
A
COST
OF
Θ
W
PROOF
FROM
THE
PROOF
OF
LEMMA
WE
KNOW
THAT
IF
THE
ALGORITHM
ALLOCATES
P
Q
SPEED
PROCESSORS
TO
THIS
JOB
THE
COST
IS
MINIMIZED
BY
WHEN
FOR
A
COST
OF
Θ
W
Α
THIS
IS
MINIMIZED
BY
MAKING
P
AS
BIG
AS
POSSIBLE
NAMELY
P
Q
FROM
THE
PROOF
OF
LEMMA
WE
KNOW
THAT
IF
THE
ALGORITHM
ALLOCATES
P
Q
SPEED
PROCESSORS
TO
THIS
JOB
THE
COST
IS
MINIMIZED
WHEN
GIVING
A
COST
OF
Θ
Α
Q
THIS
IS
MINIMIZED
BY
MAKING
P
Α
AS
SMALL
AS
POSSIBLE
NAMELY
P
Q
THUS
IN
EITHER
CASE
THE
OPTIMAL
SCHEDULING
POLICY
USES
P
Q
PROCESSORS
RUN
AT
SPEED
FOR
A
COST
OF
Θ
W
Α
LEMMA
ANY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
THAT
ONLY
RUNS
ONE
COPY
OF
EACH
JOB
MUST
BE
Ω
M
Α
COMPETITIVE
AGAINST
AN
OBLIVIOUS
ADVERSARY
THAT
MUST
SPECIFY
THE
INPUT
A
PRIORI
PROOF
APPLYING
YAO
TECHNIQUE
WE
GIVE
A
PROBABILITY
DISTRIBUTION
OVER
THE
INPUT
AND
SHOW
THAT
EVERY
DETERMINISTIC
ALGORITHM
A
WILL
HAVE
AN
EXPECTED
COMPETITIVE
RATIO
OF
Ω
M
Α
Α
THE
INSTANCE
WILL
BE
SELECTED
UNIFORMLY
AT
RANDOM
FROM
TWO
POSSIBILITIES
THE
FIRST
POSSIBLE
INSTANCE
CONSISTS
OF
ONE
JOB
WITH
Α
UNITS
OF
PARALLELIZABLE
WORK
THE
SECOND
POSSIBLE
INSTANCE
WILL
CONSIST
OF
JOB
WITH
ONE
UNIT
OF
WORK
THAT
IS
PARALLEL
UP
TO
ONE
PROCESSOR
BY
PLUGGING
THESE
PARAMETERS
INTO
LEMMA
ONE
CAN
SEE
THAT
THE
OPTIMAL
COST
IS
Θ
FOR
BOTH
INSTANCES
LET
P
DENOTE
THE
NUMBER
OF
PROCESSORS
USED
BY
THE
ALGORITHM
A
BY
LEMMA
THE
COST
FOR
THE
ALGORITHM
A
IS
EITHER
Ω
M
Α
OR
Ω
Α
DEPENDING
ON
THE
INSTANCE
BOTH
THE
MAXIMUM
AND
THE
AVERAGE
OF
THESE
TWO
COSTS
IS
MINIMIZED
BY
BALANCING
THESE
TWO
COSTS
WHICH
IS
ACCOMPLISHED
BY
SETTING
P
Α
THIS
SHOWS
THAT
THE
COMPETITIVE
RATIO
IS
Ω
M
Α
Α
LEMMA
IN
THE
INDEPENDENT
PROCESSING
MODEL
ANY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
MUST
BE
Ω
M
Α
Α
COMPETITIVE
AGAINST
AN
OBLIVIOUS
ADVERSARY
THAT
MUST
SPECIFY
THE
INPUT
A
PRIORI
PROOF
APPLYING
YAO
TECHNIQUE
WE
GIVE
A
PROBABILITY
DISTRIBUTION
OVER
THE
INPUTS
WITH
THE
PROPERTY
THAT
EVERY
DETERMINISTIC
ALGORITHM
A
WILL
HAVE
EXPECTED
COMPETITIVE
RATIO
Ω
M
Α
THE
RANDOM
INSTANCE
CONSISTS
OF
A
SINGLE
JOB
WITH
AN
INFINITELY
LARGE
NUM
BER
OF
PHASES
EACH
PHASE
WILL
BE
RANDOMLY
CHOSEN
TO
BE
ONE
OF
THE
TWO
JOB
INSTANCES
GIVEN
IN
LEMMA
THAT
IS
EACH
PHASE
WILL
EITHER
BE
PARALLELIZABLE
OR
PARALLEL
UP
TO
ONE
PROCESSOR
AND
THE
OPTIMAL
COST
FOR
EACH
PHASE
WILL
BE
Θ
CONSIDER
A
PARTICULAR
COPY
OF
THE
JOB
RUN
BY
A
BECAUSE
EACH
PHASE
IS
SO
SMALL
WE
CAN
ASSUME
THAT
THE
ALGORITHM
A
ALLOCATES
A
FIXED
NUMBER
OF
PROCESSORS
P
RUNNING
AT
A
FIXED
SPEED
FOR
THE
DURATION
OF
THE
PHASE
BY
THE
PROOF
OF
LEMMA
NO
MATTER
WHAT
THE
ALGORITHM
DOES
THE
PROBABILITY
IS
AT
LEAST
A
HALF
THAT
IT
OCCURS
A
COST
OF
Ω
M
Α
DURING
THIS
PHASE
ONE
CAN
THINK
OF
THE
PHASES
AS
BERNOULLI
TRIALS
WITH
OUTCOMES
BEING
COST
Ω
M
Α
WITH
PROBABILITY
AT
LEAST
A
HALF
AND
SMALLER
COST
WITH
PROBABILITY
AT
MOST
A
HALF
APPLYING
A
CHERNOFF
BOUND
WITH
HIGH
PROBABILITY
THE
ALGORITHM
HAS
COST
Ω
M
Α
ON
NEARLY
HALF
OF
THE
STAGES
WHILE
THE
OPTIMAL
COST
ON
EACH
STAGE
IS
BY
A
UNION
BOUND
THE
PROBABILITY
THAT
ANY
COPY
HAS
AVERAGE
COST
PER
PHASE
MUCH
LESS
THAN
Ω
M
Α
IS
SMALL
ANALYSIS
OF
MULTILAPS
IN
THIS
SECTION
WE
ASSUME
THAT
MULTIPLE
COPIES
OF
A
JOB
MAY
BE
RUN
SIMULTANEOUSLY
EACH
COPY
OF
A
JOB
MAY
BE
ASSIGNED
A
DIFFERENT
NUMBER
OF
PROCESSORS
BUT
EACH
PROCESSOR
RUNNING
THIS
COPY
MUST
BE
RUN
AT
THE
SAME
SPEED
WE
ASSUME
THAT
AT
EACH
MOMENT
IN
TIME
THE
RATE
THAT
WORK
IS
PROCESSED
ON
EACH
COPY
OF
A
JOB
IS
THE
MAXIMUM
OF
THE
RATES
OF
THE
DIFFERENT
COPIES
SO
ALL
COPIES
OF
A
JOB
ARE
ALWAYS
AT
THE
SAME
POINT
OF
EXECUTION
WE
GIVE
A
NONCLAIRVOYANT
ALGORITHM
MULTILAPS
FOR
THIS
SETTING
AND
SHOW
THAT
IT
IS
O
LOG
M
COMPETITIVE
FOR
FLOW
TIME
PLUS
ENERGY
WE
NOW
DESCRIBE
THE
ALGORITHM
LAPS
FROM
AND
THE
ALGORITHM
MULTILAPS
THAT
WE
INTRODUCE
HERE
WE
THEN
GIVE
SOME
UNDERLYING
MOTIVATION
FOR
THE
DESIGN
OF
MULTILAPS
ALGORITHM
LAPS
LET
Δ
AND
Β
BE
REAL
CONSTANTS
AT
ANY
TIME
T
THE
PROCESSOR
SPEED
IS
Δ
NA
Α
WHERE
NA
IS
THE
NUMBER
OF
ACTIVE
JOBS
AT
TIME
T
THE
PROCESSOR
PROCESSES
THE
ΒNA
ACTIVE
JOBS
WITH
THE
LATEST
RELEASE
TIMES
BY
SPLITTING
THE
PROCESSING
EQUALLY
AMONG
THESE
JOBS
FOR
OUR
PURPOSES
IN
THIS
PAPER
WE
WILL
TAKE
Δ
ALGORITHM
MULTILAPS
LET
Β
BE
A
REAL
NUMBER
THAT
PARAMETRIZES
MULTILAPS
LET
Μ
CONSIDER
ANY
TIME
T
LET
NA
BE
THE
NUMBER
OF
ACTIVE
JOBS
AT
T
EACH
OF
THE
ΒNA
ACTIVE
JOBS
WITH
THE
LATEST
RELEASE
TIMES
WILL
BE
RUN
AT
THIS
POINT
IN
TIME
CALL
THESE
JOBS
THE
LATE
JOBS
FOR
EACH
LATE
JOB
JI
A
PRIMARY
COPY
OF
JI
IS
RUN
ON
A
GROUP
OF
PA
Μ
M
PROCESSORS
WHERE
EACH
PROCESSOR
IN
THIS
GROUP
IS
RUN
AT
SPEED
SA
NA
Α
NOTE
THAT
THE
Μ
M
PRIMARY
COPIES
OF
THE
LATE
JOBS
ARE
EQUALLY
SHARING
A
Μ
FRACTION
OF
THE
PROCESSORS
FURTHERMORE
FOR
EACH
LATE
JOB
JI
THERE
ARE
LOG
PA
SECONDARY
COPIES
OF
JI
RUN
THE
JTH
J
LOG
PA
SECONDARY
COPY
OF
JI
IS
RUN
ON
A
GROUP
OF
PROCESSORS
WHERE
EACH
PROCESSOR
IN
THIS
GROUP
IS
RUN
AT
SPEED
Α
INTUITION
BEHIND
THE
DESIGN
OF
MULTILAPS
LET
US
GIVE
A
BIT
OF
INTUITION
BEHIND
THE
DESIGN
OF
MULTILAPS
IF
Μ
WAS
AND
NO
SECONDARY
COPIES
WERE
RUN
THEN
MULTILAPS
WOULD
ESSENTIALLY
BE
ADOPTING
THE
STRATEGY
OF
LAPS
OF
SHARING
THE
PROCESSING
POWER
EVENLY
AMONG
THE
LATEST
ARRIVING
Β
FRACTION
OF
THE
JOBS
LAPS
IS
O
COMPETITIVE
WHEN
ALL
WORK
IS
PARALLELIZABLE
UP
TO
THE
NUMBER
OF
AVAILABLE
PROCESSORS
HOWEVER
IF
A
PRIMARY
COPY
OF
A
JOB
IS
RUN
ON
MANY
PROCESSORS
THE
ONLINE
ALGORITHM
MAY
BE
WASTING
A
LOT
OF
ENERGY
IF
THIS
WORK
IS
NOT
HIGHLY
PARALLELIZABLE
TO
ACCOUNT
FOR
THIS
POSSIBILITY
MULTILAPS
RUNS
THE
PRIMARY
COPY
A
LITTLE
FASTER
FREEING
UP
SOME
PROCESSORS
TO
RUN
SECONDARY
COPIES
OF
THE
JOB
ON
FEWER
PROCESSORS
AND
AT
A
FASTER
SPEED
THE
NUMBER
OF
PROCESSORS
RUNNING
THE
SECONDARY
COPIES
ARE
GEOMETRICALLY
DECREASING
BY
A
FACTOR
OF
WHILE
THE
SPEEDS
ARE
INCREASING
BY
A
FACTOR
OF
Α
THUS
EACH
COPY
OF
A
JOB
IS
USING
APPROXIMATELY
THE
SAME
POWER
INTUITIVELY
ONE
OF
THE
COPIES
IS
RUNNING
THE
LATE
JOB
ON
THE
RIGHT
NUMBER
OF
PROCESSORS
THUS
MULTILAPS
USES
A
FACTOR
OF
O
LOG
M
MORE
ENERGY
THAN
OPTIMAL
BECAUSE
OF
THE
LOG
M
DIFFERENT
EQUI
POWER
COPIES
OF
THE
JOB
SETTING
Μ
GUARANTEES
THAT
THAT
MULTILAPS
DOESN
T
USE
MORE
THAN
M
PROCESSORS
THE
REST
OF
THIS
SECTION
IS
DEVOTED
TO
PROVING
THE
FOLLOWING
THEOREM
THEOREM
IN
THE
MAXIMUM
PROCESSING
MODEL
MULTILAPS
IS
O
LOG
M
COMPETITIVE
FOR
TOTAL
FLOW
TIME
PLUS
ENERGY
OVERVIEW
OF
THE
PROOF
OF
THEOREM
WE
NOW
GIVE
AN
OVERVIEW
OF
THE
STRUCTURE
OF
OUR
PROOF
OF
THEOREM
IN
LEMMA
WE
SHOW
HOW
TO
REDUCE
THE
ANALYSIS
OF
MULTILAPS
ON
ARBITRARY
INSTANCES
TO
THE
ANALYSIS
OF
MULTILAPS
ON
CANONICAL
INSTANCES
WE
DEFINE
AN
INSTANCE
TO
BE
CANONICAL
IF
THE
SPEEDUP
FUNCTION
FOR
EACH
JOB
PHASE
IS
PARALLEL
UP
TO
THE
NUMBER
PO
OF
PROCESSORS
THAT
OPT
USES
ON
THAT
PHASE
AND
IS
CONSTANT
THERE
AFTER
THE
VALUE
OF
PO
MAY
BE
DIFFERENT
FOR
EACH
PHASE
MORE
SPECIFICALLY
WE
SHOW
HOW
TO
CONSTRUCT
A
CANONICAL
INSTANCE
J
FROM
AN
ARBITRARY
INSTANCE
K
SUCH
THAT
THE
COST
OF
MULTILAPS
ON
K
IS
IDENTICAL
TO
THE
COST
OF
MULTILAPS
ON
J
AND
THE
OPTIMAL
COST
FOR
J
IS
AT
MOST
THE
OPTIMAL
COST
FOR
K
WE
THEN
DEFINE
A
VARIATION
OF
THE
UNIPROCESSOR
SETTING
THAT
WE
CALL
THE
SEQUENTIAL
SET
TING
IN
THE
SEQUENTIAL
SETTING
A
JOB
CAN
HAVE
SEQUENTIAL
PHASES
WHICH
ARE
PHASES
THAT
ARE
PROCESSED
AT
A
UNIT
RATE
INDEPENDENT
OF
THE
COMPUTATIONAL
RESOURCES
ASSIGNED
TO
THE
JOB
WE
THEN
SHOW
HOW
TO
REDUCE
THE
ANALYSIS
OF
MULTILAPS
ON
CANONICAL
INSTANCES
TO
THE
ANALYSIS
OF
LAPS
IN
THE
SEQUENTIAL
SETTING
MORE
PRECISELY
FROM
AN
ARBITRARY
CANONICAL
INSTANCE
J
WE
SHOW
HOW
TO
CREATE
AN
INSTANCE
J
FOR
THE
SEQUENTIAL
SETTING
WE
SHOW
IN
LEMMA
THAT
THE
FLOW
TIME
FOR
MULTILAPS
ON
J
IS
IDENTICAL
TO
THE
FLOW
TIME
OF
LAPS
ON
J
AND
THE
ENERGY
USED
BY
MULTILAPS
ON
J
IS
AT
MOST
O
LOG
M
TIMES
THE
ENERGY
USED
BY
LAPS
ON
J
WE
THEN
NEED
TO
RELATE
THE
OPTIMAL
SCHEDULE
FOR
J
TO
THE
OPTIMAL
SCHEDULE
FOR
J
TO
ACCOMPLISH
THIS
WE
CLASSIFY
EACH
PHASE
OF
A
JOB
IN
J
AS
EITHER
SATURATED
OR
UNSATURATED
DEPEND
ING
ON
THE
RELATIONSHIP
BETWEEN
THE
SPEEDUP
FUNCTION
AND
HOW
MANY
PROCESSORS
MULTILAPS
USES
FOR
THIS
PHASE
WE
CONSIDER
TWO
INSTANCES
DERIVED
FROM
J
AN
INSTANCE
JSAT
CONSISTING
OF
ONLY
THE
SATURATED
PHASES
IN
J
AND
AN
INSTANCE
JUNS
CONSISTING
OF
ONLY
THE
UNSATURATED
PHASES
IN
J
WE
THEN
CONSIDER
TWO
INSTANCES
DERIVED
FROM
THE
INSTANCE
J
AN
INSTANCE
J
CONSISTING
OF
PARALLEL
PHASES
IN
J
AND
AN
INSTANCE
J
CONSISTING
OF
SEQUENTIAL
PHASES
IN
J
THE
TRANSFORMATION
OF
J
TO
J
TRANSFORMS
PHASES
IN
JSAT
TO
PHASES
IN
J
AND
TRANSFORMS
PHASES
IN
JUNS
TO
PHASES
IN
J
IT
WILL
BE
CLEAR
THAT
THE
OPTIMAL
COST
FOR
J
IS
AT
LEAST
THE
OPTIMAL
COST
FOR
JSAT
PLUS
THE
OPTIMAL
COST
FOR
JUNS
WE
THEN
SHOW
IN
LEMMA
THAT
THE
OPTIMAL
COST
FOR
JSAT
IS
AT
LEAST
THE
OPTIMAL
COST
FOR
J
AND
IN
LEMMA
THAT
THE
OPTIMAL
COST
FOR
JUNS
IS
AT
LEAST
THE
OPTIMAL
COST
FOR
J
WE
THEN
DISCUSS
HOW
TO
GENERALIZE
THE
ANALYSIS
OF
LAPS
IN
USING
TECHNIQUES
FROM
TO
SHOW
THAT
THE
COST
OF
LAPS
IS
AT
MOST
A
CONSTANT
FACTOR
LARGER
THAN
THE
OPTIMAL
COST
FOR
J
PLUS
THE
OPTIMAL
COST
FOR
J
THIS
LINE
OF
REASONING
ALLOWS
US
TO
PROVE
OUR
THEOREM
AS
FOLLOWS
COSTM
K
COSTM
J
O
LOG
M
COSTLAPS
J
O
LOG
M
COSTOPT
J
COSTOPT
J
O
LOG
M
COSTOPT
JSAT
COSTOPT
JUNS
O
LOG
M
COSTOPT
J
O
LOG
M
COSTOPT
K
THE
FIRST
AND
FINAL
EQUALITIES
FOLLOW
FROM
LEMMA
THE
SECOND
EQUALITY
FOLLOWS
FROM
LEMMA
THE
THIRD
EQUALITY
FOLLOWS
FROM
THE
ANALYSIS
OF
LAPS
IN
THE
SEQUENTIAL
SETTING
THE
FOURTH
EQUALITY
FOLLOWS
FROM
LEMMA
AND
LEMMA
THE
FIFTH
EQUALITY
WILL
BE
AN
OBVIOUS
CONSEQUENCE
OF
THE
DEFINITIONS
OF
JSAT
AND
JUNS
WE
NOW
EXECUTE
THE
PROOF
STRATEGY
THAT
WE
HAVE
JUST
OUTLINED
WE
FIRST
SHOW
THAT
THERE
IS
A
WORST
CASE
INSTANCE
FOR
MULTILAPS
THAT
IS
CANONICAL
LEMMA
LET
K
BE
ANY
INPUT
INSTANCE
THERE
IS
A
CANONICAL
INSTANCE
J
SUCH
THAT
FM
J
FM
K
EM
J
EM
K
AND
COSTOPT
J
COSTOPT
K
PROOF
WE
CONSTRUCT
J
BY
MODIFYING
EACH
JOB
IN
K
AS
FOLLOWS
CONSIDER
AN
INFINITESIMALLY
SMALL
PHASE
OF
A
JOB
IN
K
WITH
WORK
W
AND
SPEEDUP
FUNCTION
Γ
LET
PO
BE
THE
NUMBER
OF
PROCESSORS
THAT
OPT
ALLOCATES
TO
THIS
PHASE
WHEN
SCHEDULING
K
WE
MODIFY
THIS
PHASE
SO
THAT
THE
NEW
SPEEDUP
FUNCTION
IS
Γ
P
P
Γ
PO
FOR
P
PO
AND
Γ
P
Γ
PO
FOR
P
PO
NOTE
THAT
MULTILAPS
MAY
PROCESS
THIS
PHASE
IN
SEVERAL
COPIES
OF
THIS
JOB
ASSUME
THAT
THE
I
TH
COPY
IS
PROCESSED
BY
PI
PROCESSORS
OF
SPEED
SI
DUE
TO
THE
MODIFICATION
OF
SPEEDUP
FUNCTION
THE
RATE
OF
PROCESSING
FOR
THE
I
TH
COPY
CHANGES
FROM
Γ
PI
SI
TO
Γ
PI
SI
IF
PI
PO
THEN
THE
RATE
OF
PROCESSING
ON
THE
I
TH
COPY
DOES
NOT
INCREASE
SINCE
Γ
IS
NONDECREASING
NOW
CONSIDER
A
COPY
WHERE
PI
PO
BY
THE
DEFINITION
OF
Γ
THE
RATE
OF
PROCESSING
Γ
PI
SI
PIΓ
PO
SI
SINCE
PI
PO
AND
SINCE
Γ
IS
SUBLINEAR
Γ
PO
Γ
PI
PLUGGING
THIS
BACK
IN
WE
GET
PO
PO
PI
THAT
THE
RATE
OF
PROCESSING
FOR
COPY
IS
AT
MOST
SIΓ
PI
SO
MULTILAPS
DOESN
T
FINISH
THIS
PHASE
IN
THE
MODIFIED
INSTANCE
BEFORE
IT
CAN
FINISH
THE
PHASE
IN
K
WE
THEN
DECREASE
THE
WORK
OF
THIS
PHASE
SO
THAT
THE
TIME
WHEN
THIS
PHASE
IS
FIRST
COMPLETED
AMONG
ALL
OF
THE
COPIES
IS
IDENTICAL
TO
WHEN
IT
COMPLETES
IN
MULTILAPS
K
NOTE
THAT
BY
CONSTRUCTION
THE
SCHEDULE
OF
MULTILAPS
ON
THIS
MODIFIED
INSTANCE
IS
IDENTICAL
TO
MULTILAPS
K
WHILE
OPT
MAY
GET
BETTER
PERFORMANCE
DUE
TO
THE
REDUCTION
OF
WORK
FINALLY
WE
CREATE
J
BY
MULTIPLYING
BOTH
THE
WORK
OF
THIS
PHASE
AND
THE
SPEEDUP
FUNCTION
BY
THE
SAME
FACTOR
OF
PO
O
TO
MAKE
THE
FINAL
SPEED
UP
FUNCTION
FOR
THIS
PHASE
PARALLEL
UP
TO
PO
PROCESSORS
THIS
CHANGE
DOES
NOT
EFFECT
THE
SCHEDULES
OF
EITHER
MULTILAPS
AND
OPT
DEFINITION
OF
THE
SEQUENTIAL
SETTING
EVERYTHING
IS
DEFINED
IDENTICALLY
AS
IN
SUBSECTION
WITH
THE
FOLLOWING
TWO
EXCEPTIONS
FIRSTLY
THERE
IS
ONLY
A
SINGLE
PROCESSOR
SECONDLY
JOB
PHASES
CAN
BE
SEQUENTIAL
WHICH
IN
THE
CONTEXT
OF
THIS
PAPER
MEANS
THAT
WORK
IN
THIS
PHASE
IS
PROCESSED
AT
A
RATE
OF
INDEPENDENT
OF
THE
FRACTION
OF
THE
PROCESSOR
ASSIGNED
TO
THE
JOB
AND
THE
SPEED
OF
THE
PROCESSOR
SO
SEQUENTIAL
WORK
IS
PROCESSED
AT
RATE
EVEN
IF
IT
IS
RUN
AT
A
SPEED
MUCH
GREATER
THAN
OR
IS
NOT
EVEN
RUN
AT
ALL
SEQUENTIAL
WORK
DOESN
T
CORRESPOND
TO
ANY
REALISTIC
SITUATION
BUT
IS
MERELY
MATHEMATICAL
CONSTRUCT
REQUIRED
FOR
THE
PROOF
DEFINITION
OF
THE
TRANSFORMATION
OF
A
CANONICAL
INSTANCE
J
INTO
THE
INSTANCE
J
IN
THE
SEQUENTIAL
SETTING
WE
TRANSFORM
EACH
JOB
IN
J
INTO
A
JOB
IN
J
BY
MODIFYING
EACH
PHASE
OF
THE
ORIGINAL
JOB
AT
EACH
POINT
IN
TIME
CONSIDER
A
PHASE
AND
THE
COPY
IN
MULTILAPS
WITH
THE
HIGHEST
PROCESSING
RATE
ON
THIS
PHASE
LET
ΓPO
BE
THE
SPEEDUP
FUNCTION
OF
THE
PHASE
WHICH
IS
PARALLEL
UP
TO
PO
PROCESSORS
WE
SAY
THE
PHASE
IS
CURRENTLY
SATURATED
M
ΒNA
PO
AND
UNSATURATED
OTHERWISE
NOTE
THAT
Μ
M
IS
THE
NUMBER
OF
PROCESSORS
ASSIGNED
TO
THE
PRIMARY
COPY
IN
MULTILAPS
THUS
A
PHASE
IS
SATURATED
IF
ALL
COPIES
IN
MULTILAPS
ARE
PROCESSING
IN
THE
PARALLEL
RANGE
OF
ΓPO
AND
UNSATURATED
OTHERWISE
CONSIDER
THE
CASE
THAT
THE
PHASE
IS
SATURATED
THE
COPY
WITH
THE
HIGHEST
PROCESSING
RATE
IN
MULTILAPS
IS
THE
ONE
WITH
PA
Μ
M
PROCESSORS
OF
SPEED
SA
NA
Α
GIVING
A
RATE
ΒNA
Μ
M
Α
OF
M
NA
Α
Α
NA
WE
MODIFY
THIS
PHASE
TO
BE
FULLY
PARALLELIZABLE
AND
SCALE
ΒNA
M
ΒNA
DOWN
THE
WORK
BY
A
FACTOR
OF
Α
NOTE
THAT
THE
PROCESSING
RATE
OF
LAPS
IS
NA
Α
SO
IT
A
WILL
COMPLETE
THE
PHASE
USING
THE
SAME
TIME
CONSIDER
THE
CASE
THAT
THE
PHASE
IS
UNSATURATED
LET
R
BE
THE
FASTEST
RATE
THAT
ANY
COPY
IN
MULTILAPS
IS
PROCESSING
WORK
IN
THIS
PHASE
WE
MODIFY
THIS
PHASE
TO
BE
SEQUENTIAL
AND
SCALE
DOWN
THE
WORK
BY
A
FACTOR
OF
R
BY
THE
DEFINITION
OF
SEQUENTIAL
THE
PROCESSING
RATE
OF
LAPS
ON
THIS
PHASE
IS
SO
IT
WILL
COMPLETE
THE
PHASE
USING
THE
SAME
TIME
AS
MULTILAPS
WE
NOW
SHOW
THAT
THE
COST
OF
MULTILAPS
ON
J
IS
AT
MOST
A
LOG
FACTOR
MORE
THAN
THE
COST
OF
LAPS
ON
J
IN
THE
SEQUENTIAL
SETTING
LEMMA
COSTM
J
FLAPS
J
O
LOG
M
ELAPS
J
FROM
THIS
WE
CAN
CONCLUDE
THAT
COSTM
J
O
LOG
M
COSTLAPS
J
PROOF
BY
CONSTRUCTION
THE
FLOW
TIME
FOR
MULTILAPS
J
IS
IDENTICAL
TO
THE
FLOW
TIME
FOR
LAPS
J
WE
NOW
SHOW
THAT
THE
ENERGY
USED
BY
MULTILAPS
J
IS
AT
MOST
A
LOG
FACTOR
MORE
THAN
THE
ENERGY
USED
BY
LAPS
J
THE
POWER
IN
MULTILAPS
J
IS
THE
SUM
OF
THE
POWERS
IN
THE
M
PROCESSORS
NOTE
THAT
FOR
EACH
OF
THE
ΒNA
LATE
JOBS
MULTILAPS
ALLOCATES
PA
PROCESSORS
OF
SPEED
SA
TO
A
PRIMARY
COPY
OF
THIS
JOB
RECALL
THAT
PA
Μ
M
AND
SA
NA
Α
MULTILAPS
ALSO
RUNS
LOG
PA
ΒNA
Μ
M
SECONDARY
COPIES
WHERE
THE
I
TH
COPY
IS
RUN
ON
PROCESSORS
OF
SPEED
Α
HENCE
THE
TOTAL
POWER
FOR
MULTILAPS
J
IS
ΒNA
Μ
M
ΒNA
NA
Μ
M
Α
Α
LOG
PA
I
Α
Α
Μ
Α
NA
ΒNA
LOG
PA
LAPS
J
RUNS
AT
SPEED
Α
AND
HENCE
POWER
N
SINCE
P
M
WE
CONCLUDE
THAT
E
J
A
A
A
M
O
LOG
M
ELAPS
J
WE
NOW
WANT
TO
SHOW
A
LOWER
BOUND
FOR
OPT
J
TO
STATE
THIS
LOWER
BOUND
WE
NEED
TO
INTRODUCE
SOME
NOTATION
DEFINE
JSAT
TO
BE
THE
INSTANCE
OBTAINED
FROM
J
BY
REMOVING
ALL
UNSATURATED
PHASES
IN
EACH
JOB
AND
DIRECTLY
CONCATENATING
THE
SATURATED
PHASES
DEFINE
JUNS
TO
BE
THE
INSTANCE
OBTAINED
FROM
J
BY
REMOVING
ALL
SATURATED
PHASES
AND
DIRECTLY
CONCATENATING
THE
UNSATURATED
PHASES
DEFINE
J
TO
BE
THE
INSTANCE
OBTAINED
FROM
J
BY
REMOVING
ALL
SEQUENTIAL
PHASES
IN
EACH
JOB
AND
DIRECTLY
CONCATENATING
THE
PARALLEL
PHASES
DEFINE
J
TO
BE
THE
INSTANCE
OBTAINED
FROM
J
BY
REMOVING
ALL
PARALLEL
PHASES
IN
EACH
JOB
AND
DIRECTLY
CONCATENATING
THE
SEQUENTIAL
PHASES
NOTE
THAT
THE
TRANSFORMATION
FROM
J
TO
J
TRANSFORMS
A
PHASE
IN
JSAT
TO
A
PHASE
IN
J
AND
TRANSFORMS
A
PHASE
IN
JUNS
TO
A
PHASE
SEQ
OBVIOUSLY
OPT
CAN
ONLY
GAIN
BY
SCHEDULING
JSAT
AND
JUNS
SEPARATELY
THAT
IS
COSTOPT
J
COSTOPT
JSAT
COSTOPT
JUNS
WE
NOW
WANT
TO
SHOW
THAT
COSTOPT
J
O
COSTOPT
JSAT
AND
COSTOPT
J
O
COSTOPT
JUNS
LEMMA
COSTOPT
J
O
COSTOPT
JSAT
PROOF
WE
CONSTRUCT
A
SCHEDULE
OPT
J
FROM
THE
SCHEDULE
OPT
JSAT
PHASE
BY
PHASE
EACH
PHASE
IN
OPT
J
WILL
END
NO
LATER
THAN
THE
CORRESPONDING
PHASE
IN
OPT
JSAT
AND
THE
SCHEDULE
FOR
OPT
J
WILL
USE
LESS
ENERGY
THAN
THE
SCHEDULE
OPT
JSAT
CONSIDER
A
INFINITESIMAL
SATURATED
PHASE
IN
JSAT
LET
PO
AND
SO
BE
THE
NUMBER
OF
PROCESSORS
AND
SPEED
ALLOCATED
BY
OPT
JSAT
TO
THIS
PHASE
BY
THE
DEFINITION
OF
CANONICAL
THE
PHASE
IS
PARALLELIZABLE
UP
TO
PO
PROCESSORS
THUS
OPT
JSAT
IS
PROCESSING
AT
A
RATE
OF
POSO
DEFINE
OPT
J
SO
THAT
IT
RUNS
AT
SPEED
ΑPOSO
ON
THIS
PHASE
SINCE
THE
TRANSFORMATION
SCALES
DOWN
THE
WORK
BY
A
FACTOR
OF
Α
OPT
J
WILL
COMPLETE
THE
PHASE
AT
THE
SAME
TIME
AS
OPT
JSAT
NOW
WE
NEED
TO
ARGUE
THAT
AT
ANY
POINT
OF
TIME
THE
POWER
FOR
OPT
JSAT
WILL
BE
AT
LEAST
THE
POWER
FOR
OPT
JPAR
THE
POWER
AT
THIS
TIME
IN
OPT
JSAT
IS
P
J
PO
J
SO
J
WHERE
THE
SUM
IS
OVER
ALL
JOBS
J
IT
IS
PROCESSING
AND
PO
J
AND
SO
J
ARE
THE
NUMBER
AND
SPEED
OF
THE
PROCESSORS
ALLOCATED
TO
J
KEEPING
R
J
PO
J
SO
J
FIXED
P
IS
MINIMIZED
BY
HAVING
ALL
THE
SO
J
TO
BE
THE
SAME
FIXED
VALUE
SO
THIS
GIVES
R
J
PO
JSO
SOM
AND
Α
P
P
Α
ΑM
R
M
RΑ
BY
OUR
DEFINITION
OF
OPT
J
THE
POWER
P
IN
OPT
J
CAN
BE
BOUNDED
AS
FOLLOWS
Α
P
J
Α
M
PO
J
SO
J
Α
Α
Α
RΑ
P
LEMMA
COSTOPT
J
O
COSTOPT
JUNS
PROOF
CONSIDER
A
UNSATURATED
PHASE
IN
JUNS
THAT
IS
PARALLEL
UP
TO
PO
PROCESSORS
WE
GRA
CIOUSLY
ALLOW
OPT
JUNS
TO
SCHEDULE
EACH
PHASE
IN
JUNS
IN
ISOLATION
OF
THE
OTHER
PHASES
THIS
ONLY
IMPROVES
OPT
JUNS
CONSIDER
A
PARTICULAR
PHASE
IN
JUNS
WITH
A
SPEEDUP
FUNCTION
THAT
IS
PARALLEL
UP
TO
PO
PROCESSORS
AND
THAT
HAS
WORK
W
BY
LEMMA
THE
TOTAL
FLOW
TIME
PLUS
ENERGY
INCURRED
FOR
OPT
JUNS
IS
Θ
W
OPT
J
WILL
ALLOCATE
ZERO
PROCESSORS
TO
THE
CORRESPONDING
PHASE
AND
PROCESS
THE
PHASE
AT
RATE
SINCE
THE
WORK
IN
J
IS
SEQUENTIAL
HENCE
OPT
J
INCURS
NO
ENERGY
COST
FOR
THIS
PHASE
SO
TO
FINISH
THE
PROOF
WE
WILL
SHOW
THAT
THE
FLOW
TIME
FOR
THIS
PHASE
IN
OPT
J
IS
AT
MOST
THE
COST
OF
THIS
PHASE
IN
OPT
JUNS
NAMELY
Θ
W
RECALL
THAT
IN
THE
TRANSFORMATION
FROM
J
TO
J
THIS
WORK
IS
SCALED
DOWN
BY
THE
FASTEST
RATE
THAT
THIS
PHASE
IS
PROCESSED
BY
ANY
COPY
IN
MULTILAPS
CONSIDER
THE
COPY
IN
MULTILAPS
THAT
IS
PROCESSING
IN
THE
PARALLEL
RANGE
WITH
THE
MOST
NUMBER
OF
PROCESSORS
I
E
THE
COPY
WITH
PROCESSORS
SUCH
THAT
IS
MAXIMIZED
AND
AT
MOST
PO
SINCE
THE
PHASE
IS
UNSATURATED
PO
BY
THE
DEFINITION
OF
MULTILAPS
THE
PROCESSING
RATE
OF
THIS
COPY
IS
AT
LEAST
Α
Α
PO
Α
Α
O
THUS
THE
WORK
IN
THIS
PHASE
IN
J
AND
THE
FLOW
TIME
FOR
THIS
PHASE
IN
OPT
J
IS
AT
MOST
Α
O
ONE
CAN
EXTEND
THE
ANALYSIS
FOR
LAPS
IN
THE
UNIPROCESSOR
SETTING
TO
THE
SEQUENTIAL
SETTING
USING
THE
TECHNIQUES
USED
IN
WE
REFER
THE
READER
TO
FOR
FULL
DETAILS
AND
JUST
GIVE
THE
UNDERLYING
INTUITION
HERE
THE
ANALYSIS
USES
AMORTIZED
LOCAL
COMPETITIVENESS
THAT
IS
IT
IS
SHOWN
THAT
AT
EVERY
TIME
PLAPS
NLAPS
DΦ
DT
C
POPT
NOPT
WHERE
P
DENOTES
POWER
N
DENOTES
THE
NUMBER
OF
ACTIVE
JOBS
Φ
IS
THE
POTENTIAL
FUNCTION
AND
C
IS
THE
DESIRED
COMPETITIVE
RATIO
SO
WHEN
PLAPS
NLAPS
IS
LARGE
THE
PROCESSING
OF
LAPS
LOWERS
THE
POTENTIAL
FUNCTION
Φ
ENOUGH
TO
MAKE
THE
EQUATION
TRUE
NOW
CONSIDER
THE
SEQUENTIAL
SETTING
THE
DIFFICULTY
THAT
ARISES
IS
THAT
THE
PROCESSING
THAT
LAPS
DOES
ON
SEQUENTIAL
JOBS
MAY
NOT
LOWER
THE
POTENTIAL
FUNCTION
HOWEVER
IF
THE
NUMBER
OF
SEQUENTIAL
PHASES
THAT
LAPS
IS
PROCESSING
IS
VERY
SMALL
THEN
RAISING
THE
SPEED
OF
LAPS
BY
A
SMALL
AMOUNT
WILL
BE
ENOUGH
SO
THAT
THE
POTENTIAL
FUNCTION
DECREASE
SUFFICIENTLY
QUICKLY
DUE
TO
THE
PROCESSING
ON
THE
NON
SEQUENTIAL
JOBS
IF
THE
NUMBER
OF
SEQUENTIAL
PHASES
IS
LARGE
AT
A
PARTICULAR
TIME
THEN
THE
INCREASE
IN
FLOW
TIME
THAT
LAPS
IS
EXPERIENCING
ON
THESE
JOBS
IS
ALSO
EXPERIENCED
BY
THE
ADVERSARY
AT
SOME
POINT
IN
TIME
THIS
INCREASE
IN
FLOW
TIME
EXPERIENCED
BY
THE
ADVERSARY
PAYS
FOR
THE
INCREASE
IN
FLOW
TIME
FOR
LAPS
AT
THIS
POINT
OF
TIME
NOTE
THAT
BY
DEFINITION
OF
LAPS
THE
POWER
USED
BY
LAPS
IS
COMPARABLE
TO
THE
INCREASE
IN
FLOW
TIME
EXPERIENCE
BY
LAPS
WE
CAN
THUS
DERIVE
THE
FOLLOWING
THEOREM
THEOREM
COSTLAPS
J
O
COSTOPT
J
COSTOPT
J
LOWER
BOUND
FOR
CHECKPOINTABLE
MULTIPLE
COPIES
WE
SHOW
HERE
THAT
EVEN
IF
THE
RATE
THAT
THE
WORK
IS
PROCESSED
IS
THE
SUM
OF
THE
RATE
OF
THE
COPIES
EVERY
RANDOMIZED
ALGORITHM
IS
POLY
LOG
COMPETITIVE
THEOREM
IN
THE
SUM
PROCESSING
MODEL
THE
COMPETITIVE
RATIO
FOR
EVERY
RANDOMIZED
NONCLAIRVOYANT
ALGORITHM
IS
Ω
Α
M
AGAINST
AN
OBLIVIOUS
ADVERSARY
THAT
MUST
SPECIFY
THE
INPUT
A
PRIORI
PROOF
APPLYING
YAO
TECHNIQUE
WE
GIVE
A
PROBABILITY
DISTRIBUTION
OVER
THE
INPUT
AND
SHOW
THAT
EVERY
DETERMINISTIC
ALGORITHM
A
WILL
HAVE
AN
EXPECTED
COMPETITIVE
RATIO
OF
Ω
Α
M
THERE
ARE
Θ
LOG
M
POSSIBLE
INSTANCES
EACH
SELECTED
WITH
EQUAL
PROBABILITY
FOR
EACH
J
LOG
M
INSTANCE
JJ
WILL
CONSIST
OF
ONE
JOB
WITH
WORK
WJ
Α
J
AND
SPEEDUP
FUNCTION
P
WHERE
ΓQ
P
IS
THE
SPEED
UP
FUNCTION
THAT
IS
PARALLELIZABLE
UP
TO
Q
PROCESSORS
BY
APPLYING
LEMMA
OPT
JJ
ALLOCATES
PJ
PROCESSORS
EACH
OF
SPEED
SJ
Α
RESULTING
IN
A
COST
OF
Θ
NOW
CONSIDER
ANY
DETERMINISTIC
NONCLAIRVOYANT
ALGORITHM
A
ROUNDING
THE
NUMBER
OF
PROCESSORS
A
COPY
IS
RUN
ON
TO
A
FACTOR
OF
TWO
DOESN
T
CHANGE
THE
OBJECTIVE
BY
MORE
THAN
A
CONSTANT
FACTOR
AND
THERE
IS
NO
SIGNIFICANT
BENEFIT
FROM
RUNNING
TWO
COPIES
ON
AN
EQUAL
NUMBER
OF
PROCESSORS
SINCE
THE
ALGORITHM
IS
NONCLAIRVOYANT
IT
WILL
GAIN
NO
INFORMATION
ABOUT
THE
IDENTITY
OF
JJ
UNTIL
SOME
COPY
FINISHES
SINCE
THE
POWER
FUNCTION
IS
CONVEX
IT
IS
BEST
FOR
THE
ALGORITHM
TO
RUN
EACH
COPY
AT
CONSTANT
SPEED
THUS
WE
CAN
ASSUME
THAT
THE
ALGORITHM
RUNS
LOG
M
COPIES
OF
THE
JOB
WITH
COPY
I
RUN
ON
PROCESSORS
AT
AT
SOME
CONSTANT
SPEED
SI
NOTE
THAT
THE
ALGORITHM
CAN
SET
SI
IF
IT
DOESN
T
WANT
TO
RUN
A
COPY
ON
THAT
MANY
PROCESSORS
THE
POWER
OF
COPY
I
IS
P
PISIΑ
AND
THE
TOTAL
POWER
FOR
A
IS
I
I
LET
R
I
J
SI
DENOTE
THE
RATE
THAT
THE
COPY
I
IS
PROCESSING
WORK
ON
JOB
JJ
BECAUSE
WE
ARE
ASSUMING
THAT
THE
WORK
COMPLETED
ON
A
JOB
IS
THE
SUM
OF
THAT
COMPLETED
BY
THE
GROUPS
WORKING
ON
IT
WE
HAVE
THAT
RJ
I
R
I
J
IS
THE
RATE
THAT
A
COMPLETES
WORK
ON
JOB
JJ
AND
MAXIMIZE
THIS
TIME
WE
BOUND
THIS
MAXIMUM
DENOTED
BY
T
AS
FOLLOWS
MIN
T
J
TJ
O
LOG
M
J
TJ
O
LOG
M
I
R
I
J
WJ
J
I
O
LOG
M
SI
WJ
O
I
J
SI
LOG
M
O
I
SI
J
I
WJ
J
I
LOG
M
WJ
LOG
M
I
J
I
Α
J
J
I
LOG
M
Α
J
O
SI
Α
J
LOG
M
I
J
I
J
I
LOG
M
Α
J
O
SI
I
LOG
M
O
LOG
M
I
Α
I
I
Α
Α
I
P
SUBJECT
TO
P
I
PI
THE
SUM
I
PI
IS
MAXIMIZED
BY
SETTING
EACH
PI
TO
LOG
M
GIVING
O
P
Α
O
P
Α
THE
TOTAL
COST
FOR
A
IS
FA
EA
T
P
T
P
T
LOG
M
Α
Ω
Α
M
RECALLING
THAT
THE
OPTIMAL
COST
IS
O
THE
RESULT
FOLLOWS
CONCLUSION
IN
SUMMARY
WE
HAVE
SHOWN
THAT
FOR
JOBS
THAT
MAY
HAVE
SIDE
EFFECTS
OR
THAT
ARE
NOT
CHECK
POINTABLE
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
QUICKLY
WITH
THE
NUMBER
OF
PROCESSORS
AND
FOR
CHECKPOINTABLE
JOBS
WITHOUT
SIDE
EFFECTS
THE
ACHIEVABLE
COMPETITIVE
RATIO
GROWS
SLOWLY
WITH
THE
NUMBER
OF
PROCESSORS
THERE
SEEM
TO
BE
SEVERAL
INTERESTING
LINES
OF
RESEARCH
SPAWNED
BY
THESE
RESULTS
MOST
OBVIOUSLY
THE
UPPER
AND
LOWER
BOUNDS
ON
THE
COMPETITIVE
RATIO
FOR
CHECKPOINTABLE
JOBS
WITHOUT
SIDE
EFFECTS
ARE
NOT
QUITE
TIGHT
IT
IS
PLAUSIBLE
THAT
ONE
COULD
OBTAIN
TIGHT
UPPER
AND
LOWER
BOUNDS
BY
BEING
MORE
CAREFUL
IN
THE
ANALYSES
ONE
MIGHT
ALSO
CONSIDER
THE
SITUATION
WHERE
THE
POWER
OBJECTIVE
IS
TEMPERATURE
IT
IS
NOT
CLEAR
HOW
TO
BEST
FORMALIZE
THIS
PROBLEM
THE
MOST
OBVIOUS
APPROACH
IS
TO
INCLUDE
A
CONSTRAINT
ON
TEMPERATURE
THAT
IS
YOU
CAN
NOT
EXCEED
THE
THRESHOLD
OF
THE
PROCESSOR
IF
THE
PROCESSOR
COOLS
ACCORDING
TO
NEWTON
LAW
THEN
THE
TEMPERATURE
IS
APPROXIMATELY
THE
MAXIMUM
ENERGY
USED
OVER
ANY
TIME
INTERVAL
OF
A
PARTICULAR
LENGTH
WHERE
THE
LENGTH
OF
THE
INTERVAL
IS
DETERMINED
BY
THE
COOLING
PARAMETER
OF
THE
DEVICE
IF
THE
INTERVALS
ARE
LONG
THEN
THE
TEMPERATURE
CONSTRAINT
IS
ESSENTIALLY
AN
ENERGY
CONSTRAINT
BUT
OPTIMIZING
ANY
REASONABLE
SCHEDULING
OBJECTIVE
SUBJECT
TO
AN
ENERGY
CONSTRAINT
IS
KNOWN
TO
BE
DIFFICULT
ACKNOWLEDGMENTS
WE
THANK
NIKHIL
BANSAL
TAK
WAH
LAM
LAP
KEI
LEE
AND
ALBERTO
MARCHETTI
SPACCAMELA
FOR
HELPFUL
DISCUSSIONS
DELIVERING
ENERGY
PROPORTIONALITY
WITH
NON
ENERGY
PROPORTIONAL
SYSTEMS
OPTIMIZING
THE
ENSEMBLE
NIRAJ
TOLIA
ZHIKUI
WANG
MANISH
MARWAH
CULLEN
BASH
PARTHASARATHY
RANGANATHAN
XIAOYUN
ZHU
HP
LABS
PALO
ALTO
ABSTRACT
WITH
POWER
HAVING
BECOME
A
CRITICAL
ISSUE
IN
THE
OPER
ATION
OF
DATA
CENTERS
TODAY
THERE
HAS
BEEN
AN
INCREASED
PUSH
TOWARDS
THE
VISION
OF
ENERGY
PROPORTIONAL
COMPUT
ING
IN
WHICH
NO
POWER
IS
USED
BY
IDLE
SYSTEMS
VERY
LOW
POWER
IS
USED
BY
LIGHTLY
LOADED
SYSTEMS
AND
PROPORTION
ATELY
HIGHER
POWER
AT
HIGHER
LOADS
UNFORTUNATELY
GIVEN
THE
STATE
OF
THE
ART
OF
TODAY
HARDWARE
DESIGNING
INDIVID
UAL
SERVERS
THAT
EXHIBIT
THIS
PROPERTY
REMAINS
AN
OPEN
CHAL
LENGE
HOWEVER
EVEN
IN
THE
ABSENCE
OF
REDESIGNED
HARD
WARE
WE
DEMONSTRATE
HOW
OPTIMIZATION
BASED
TECHNIQUES
CAN
BE
USED
TO
BUILD
SYSTEMS
WITH
OFF
THE
SHELF
HARDWARE
THAT
WHEN
VIEWED
AT
THE
AGGREGATE
LEVEL
APPROXIMATE
THE
BEHAVIOR
OF
ENERGY
PROPORTIONAL
SYSTEMS
THIS
PAPER
EX
PLORES
THE
VIABILITY
AND
TRADEOFFS
OF
OPTIMIZATION
BASED
AP
PROACHES
USING
TWO
DIFFERENT
CASE
STUDIES
FIRST
WE
SHOW
HOW
DIFFERENT
POWER
SAVING
MECHANISMS
CAN
BE
COMBINED
TO
DELIVER
AN
AGGREGATE
SYSTEM
THAT
IS
PROPORTIONAL
IN
ITS
USE
OF
SERVER
POWER
SECOND
WE
SHOW
EARLY
RESULTS
ON
DELIVER
ING
A
PROPORTIONAL
COOLING
SYSTEM
FOR
THESE
SERVERS
WHEN
COMPARED
TO
THE
POWER
CONSUMED
AT
UTILIZATION
RE
SULTS
FROM
OUR
TESTBED
SHOW
THAT
OPTIMIZATION
BASED
SYS
TEMS
CAN
REDUCE
THE
POWER
CONSUMED
AT
UTILIZATION
TO
FOR
SERVER
POWER
AND
FOR
COOLING
POWER
INTRODUCTION
WITH
POWER
HAVING
BECOME
A
CRITICAL
ISSUE
IN
THE
OPERA
TION
OF
DATA
CENTERS
THE
CONCEPT
OF
ENERGY
PROPORTIONAL
COMPUTING
OR
ENERGY
SCALEDOWN
IS
DRAWING
IN
CREASING
INTEREST
A
SYSTEM
BUILT
ACCORDING
TO
THIS
PRIN
CIPLE
WOULD
IN
THEORY
USE
NO
POWER
WHEN
NOT
BEING
UTI
LIZED
WITH
POWER
CONSUMPTION
GROWING
IN
PROPORTION
TO
UTILIZATION
OVER
THE
LAST
FEW
YEARS
A
NUMBER
OF
TECH
NIQUES
HAVE
BEEN
DEVELOPED
TO
MAKE
SERVER
PROCESSORS
MORE
EFFICIENT
INCLUDING
BETTER
MANUFACTURING
TECHNIQUES
AND
THE
USE
OF
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
DVFS
FOR
RUNTIME
POWER
OPTIMIZATION
WHILE
THE
SAVINGS
ARE
SIGNIFICANT
THIS
HAS
LEAD
TO
CPUS
NO
LONGER
BEING
RESPONSIBLE
FOR
THE
MAJORITY
OF
POWER
CON
SUMED
IN
SERVERS
TODAY
INSTEAD
SUBSYSTEMS
THAT
HAVE
NOT
BEEN
OPTIMIZED
FOR
POWER
EFFICIENCY
SUCH
AS
NETWORK
CARDS
HARD
DRIVES
GRAPHICS
PROCESSORS
FANS
AND
POWER
SUPPLIES
HAVE
STARTED
DOMINATING
THE
POWER
CONSUMED
BY
SYSTEMS
ESPECIALLY
DURING
PERIODS
OF
LOW
UTILIZATION
INSTEAD
OF
WAITING
FOR
ALL
THESE
DIFFERENT
TECHNOLOGIES
TO
DELIVER
BETTER
ENERGY
EFFICIENCY
THIS
PAPER
ADVOCATES
THAT
ENERGY
PROPORTIONAL
COMPUTING
CAN
BE
APPROXIMATED
BY
USING
SOFTWARE
TO
CONTROL
POWER
USAGE
AT
THE
ENSEM
BLE
LEVEL
AN
ENSEMBLE
IS
DEFINED
AS
A
LOGICAL
COLLECTION
OF
SERVERS
AND
COULD
RANGE
FROM
AN
ENCLOSURE
OF
BLADES
A
SINGLE
RACK
GROUPS
OF
RACKS
TO
EVEN
AN
ENTIRE
DATA
CENTER
IT
WAS
PREVIOUSLY
DIFFICULT
TO
DYNAMICALLY
BALANCE
WORK
LOADS
TO
CONSERVE
POWER
AT
THE
ENSEMBLE
LEVEL
FOR
A
NUM
BER
OF
REASONS
UNNECESSARILY
SHUTTING
DOWN
APPLICATIONS
TO
SIMPLY
RESTART
THEM
ELSEWHERE
IS
LOOKED
UPON
AS
A
HIGH
RISK
HIGH
COST
CHANGE
BECAUSE
OF
THE
PERFORMANCE
IMPACT
THE
RISK
TO
SYSTEM
STABILITY
AND
THE
COST
OF
DESIGNING
CUS
TOM
CONTROL
SOFTWARE
HOWEVER
THE
RE
EMERGENCE
OF
VIR
TUALIZATION
AND
THE
ABILITY
TO
LIVE
MIGRATE
ENTIRE
VIR
TUAL
MACHINES
VMS
CONSISTING
OF
OSS
AND
APPLICATIONS
IN
A
TRANSPARENT
AND
LOW
OVERHEAD
MANNER
WILL
ENABLE
A
NEW
CATEGORY
OF
SYSTEMS
THAT
CAN
REACT
BETTER
TO
CHANGES
IN
WORKLOADS
AT
THE
AGGREGATE
LEVEL
BY
MOVING
WORK
LOADS
OFF
UNDER
UTILIZED
MACHINES
AND
THEN
TURNING
IDLE
MA
CHINES
OFF
IT
SHOULD
NOW
BE
POSSIBLE
TO
APPROXIMATE
AT
AN
ENSEMBLE
LEVEL
THE
BEHAVIOR
FOUND
IN
THEORETICAL
ENERGY
PROPORTIONAL
SYSTEMS
HOWEVER
VIRTUALIZATION
IS
NOT
A
MAGIC
BULLET
AND
A
NA
IVE
APPROACH
TO
CONSOLIDATION
CAN
HURT
APPLICATION
PER
FORMANCE
IF
SERVER
RESOURCES
ARE
OVERBOOKED
OR
LEAD
TO
RE
DUCED
POWER
SAVINGS
WHEN
COMPARED
TO
THE
MAXIMUM
POS
SIBLE
THIS
PAPER
THEREFORE
ADVOCATES
A
MORE
RIGOROUS
AP
PROACH
IN
THE
OPTIMIZATION
OF
ENSEMBLE
SYSTEMS
INCLUDING
THE
USE
OF
PERFORMANCE
MODELING
OPTIMIZATION
AND
CON
TROL
THEORY
FINALLY
INSTEAD
OF
ONLY
LOOKING
AT
MORE
TRADI
TIONAL
SERVER
COMPONENTS
SUCH
AS
STORAGE
CPUS
AND
THE
NETWORK
OPTIMIZATION
BASED
SYSTEMS
SHOULD
ALSO
CONSIDER
CONTROL
OF
OTHER
COMPONENTS
SUCH
AS
SERVER
FANS
AND
COM
PUTER
ROOM
AIR
CONDITIONERS
CRACS
AS
COOLING
COSTS
ARE
RAPIDLY
BECOMING
A
LIMITING
FACTOR
IN
THE
DESIGN
AND
OPERATION
OF
DATA
CENTERS
TODAY
WE
USE
TWO
CASE
STUDIES
TO
DEMONSTRATE
THAT
IT
IS
POSSI
BLE
BY
APPLYING
OPTIMIZATIONS
AT
THE
ENSEMBLE
LAYER
TO
DE
THE
POWER
CONSUMED
BY
SERVERS
AND
EXHIBIT
POWER
USAGE
BEHAVIOR
CLOSE
TO
THAT
OF
AN
ENERGY
PROPORTIONAL
SYSTEM
SECOND
WE
DEMONSTRATE
HOW
A
POWER
AND
WORKLOAD
AWARE
COOLING
CONTROLLER
CAN
EXHIBIT
THE
SAME
BEHAVIOR
FOR
COOL
ING
EQUIPMENT
SUCH
AS
SERVER
FANS
CASE
STUDIES
NO
DVFS
DVFS
DVFS
OFF
AVERAGE
ENCLOSURE
UTILIZATION
IT
HAS
BEEN
ADVOCATED
THAT
OPTIMIZATION
BASED
ALGORITHMS
SHOULD
BE
PREFERRED
OVER
AD
HOC
HEURISTICS
IN
MAKING
SYS
TEM
RUNTIME
AND
MANAGEMENT
DECISIONS
WE
WILL
THEREFORE
NOT
STRESS
THIS
POINT
FURTHER
BUT
THROUGH
THE
USE
OF
TWO
CASE
STUDIES
SHOW
HOW
OPTIMIZATION
CAN
BE
USED
TO
DELIVER
ENERGY
PROPORTIONALITY
AT
THE
ENSEMBLE
LAYER
IT
SHOULD
BE
STRESSED
THAT
THE
FOCUS
OF
THIS
SECTION
IS
NOT
ON
THE
USE
OF
ANY
PARTICULAR
ALGORITHM
BUT
INSTEAD
ON
HOW
NON
ENERGY
PROPORTIONAL
SYSTEMS
CAN
BE
COMBINED
TO
APPROXI
MATE
THE
BEHAVIOR
OF
AN
ENERGY
PROPORTIONAL
SYSTEM
EXPERIMENTAL
SETUP
TO
EVALUATE
ENERGY
PROPORTIONALITY
IN
THE
CASE
STUDIES
WE
USED
AN
HP
BLADESYSTEM
ENCLOSURE
WITH
PRO
LIANT
SERVER
BLADES
AND
FANS
EACH
BLADE
WAS
EQUIPPED
WITH
GB
OF
RAM
AND
TWO
AMD
HE
DUAL
CORE
PROCESSORS
EACH
PROCESSOR
HAS
P
STATES
A
VOLT
AGE
AND
FREQUENCY
SETTING
CORRESPONDING
TO
FREQUENCIES
OF
AND
GHZ
THE
BLADES
AND
THE
FANS
ARE
EQUALLY
DIVIDED
AMONG
TWO
ROWS
ON
THE
FRONT
AND
BACK
ENDS
OF
THE
ENCLOSURE
RESPECTIVELY
EACH
BLADE
IS
COOLED
BY
MULTIPLE
FANS
AND
EACH
FAN
DRAWS
COOL
AIR
IN
THROUGH
THE
FRONT
OF
THE
BLADES
THE
ENCLOSURE
ALLOWS
US
TO
MEASURE
BLADE
TEMPERATURES
AND
THE
POWER
CONSUMED
BY
DIFFERENT
COMPONENTS
WE
USED
XEN
WITH
BOTH
THE
ADMINISTRATIVE
DOMAIN
AND
THE
VMS
USING
THE
PARA
VIRTUALIZED
LINUX
KERNEL
THE
XEN
ADMINISTRATIVE
DOMAIN
IS
STORED
ON
LOCAL
DISKS
WHILE
THE
VMS
USE
A
STORAGE
AREA
NETWORK
SAN
WE
USED
VMS
CONFIGURED
WITH
MB
OF
RAM
A
GB
VIRTUAL
HARD
DRIVE
AND
ONE
VIRTUAL
CPU
WE
SET
EACH
VM
MEMORY
TO
A
LOW
VALUE
TO
ALLOW
US
TO
EVALUATE
A
LARGE
CONFIGURATION
SPACE
FOR
WORKLOAD
PLACE
MENT
IN
PRODUCTION
ENVIRONMENTS
THE
SAME
EFFECT
COULD
BE
ACHIEVED
AT
RUNTIME
THROUGH
THE
USE
OF
PAGE
SHARING
OR
BALLOONING
WE
USED
GAMUT
TO
EXPERIMENT
WITH
DIFFERENT
VM
AND
PHYSICAL
SERVER
UTILIZATIONS
SERVER
ENERGY
PROPORTIONALITY
WE
EXAMINED
THE
ENERGY
PROPORTIONALITY
IN
THE
ENCLOSURE
LAYER
USING
THREE
DIFFERENT
POLICIES
THE
FIRST
POLICY
NO
EACH
RESULT
PRESENTED
ABOVE
IS
AN
AVERAGE
OF
APPROXIMATELY
READINGS
OVER
A
MINUTE
INTERVAL
FIGURE
ENCLOSURE
POWER
USAGE
BLADES
NETWORK
DVFS
USES
NO
POWER
SAVING
FEATURES
THE
SECOND
POLICY
DVFS
USES
HARDWARE
BASED
VOLTAGE
AND
FREQUENCY
SCALING
AND
IS
VERY
SIMILAR
TO
LINUX
ONDEMAND
GOVERNOR
THE
THIRD
POLICY
DVFS
OFF
USES
DVFS
PLUS
A
VM
MIGRA
TION
CONTROLLER
THAT
CONSOLIDATES
VIRTUAL
MACHINES
AND
TURNS
IDLE
MACHINES
OFF
ITS
ALGORITHM
USES
THE
BLADE
POWER
MODELS
FOR
THE
DIFFERENT
P
STATES
AND
SENSOR
READINGS
FROM
RESOURCE
MONITORING
AGENTS
THE
OPTIMIZATION
PROBLEM
IS
SIMILAR
TO
THAT
IN
WITH
CONSTRAINTS
ON
THE
BLADE
CPU
AND
MEMORY
UTILIZATION
TO
PREVENT
OVERBOOKING
EXAMPLES
OF
OTHER
CONSTRAINTS
THAT
COULD
BE
MODELED
INCLUDE
NETWORK
AND
STORAGE
UTILIZATION
OUR
EXPERIENCE
HAS
SHOWN
THAT
AL
GORITHMS
SUCH
AS
BIN
PACKING
OR
SIMULATED
ANNEALING
WORK
WELL
IN
THIS
SCENARIO
FOR
THIS
CASE
STUDY
WE
VARIED
EACH
VM
UTILIZATION
IN
THE
RANGE
OF
OF
A
SINGLE
CORE
AND
MEA
SURED
THE
POWER
CONSUMED
BY
THE
ENCLOSURE
THE
RESULTS
ARE
PRESENTED
IN
FIGURE
WHERE
THE
X
AXIS
SHOWS
THE
EN
CLOSURE
UTILIZATION
BY
ALL
VMS
AS
A
PERCENTAGE
OF
TOTAL
CAPACITY
OF
ALL
THE
BLADES
AS
EACH
SERVER
HAS
CORES
THE
UTILIZATION
ALSO
CORRESPONDS
TO
EACH
VM
UTILIZATION
AS
A
PERCENTAGE
OF
A
SINGLE
CORE
THE
MEASURED
POWER
SHOWN
ON
THE
Y
AXIS
INCLUDES
THE
POWER
CONSUMED
BY
THE
BLADES
AS
WELL
AS
THE
NETWORKING
SAN
AND
MANAGEMENT
MODULES
WITH
NO
DVFS
WE
NOTICE
A
VERY
SMALL
POWER
RANGE
W
BETWEEN
AND
UTILIZATION
AND
THE
MINIMUM
POWER
USED
IS
W
OR
OF
THE
POWER
CONSUMED
AT
UTILIZATION
SIGNIFICANTLY
AWAY
FROM
THE
THEORETICAL
MINIMUM
OF
W
ONCE
DVFS
IS
ENABLED
THE
POWER
RANGE
INCREASES
BUT
AS
SEEN
IN
THE
FIGURE
THE
SYSTEM
IS
STILL
NOT
ENERGY
PROPORTIONAL
AND
CONSUMES
W
AT
UTILIZATION
IT
IS
ONLY
WHEN
WE
LOOK
AT
THE
DVFS
OFF
POLICY
THAT
THE
SYSTEM
STARTS
APPROXIMATING
ENERGY
PROPOR
TIONALITY
AT
THE
ENSEMBLE
LEVEL
THE
RANGE
OF
POWER
USED
BETWEEN
AND
UTILIZATION
IS
W
AND
AT
UTILIZATION
THERE
IS
ONLY
A
W
DIFFERENCE
OR
OF
THE
POWER
CONSUMED
AT
UTILIZATION
FROM
THE
THEORETICAL
MINIMUM
OF
W
SPEED
RPM
STATIC
FAN
SPEED
REACTIVE
FAN
CONTROLLER
PREDICTIVE
FAN
CONTROLLER
AVERAGE
ENCLOSURE
UTILIZATION
FIGURE
FAN
POWER
CONSUMPTION
AND
MODEL
NOTE
FROM
THE
FIGURE
THAT
AT
HIGH
UTILIZATIONS
ABOVE
ALL
THREE
POLICIES
HAVE
THE
SAME
POWER
USAGE
AS
THEY
ARE
ALL
RUNNING
NEAR
PEAK
PERFORMANCE
WHILE
THE
DVFS
POLICY
ONLY
SHOWS
A
NOTICEABLE
DIFFERENCE
WHEN
IT
CAN
SWITCH
TO
A
LOWER
P
STATE
THE
DVFS
OFF
POLICY
STARTS
SHOWING
BENEFIT
AROUND
UTILIZATION
AS
IT
CAN
START
CON
SOLIDATING
VMS
AND
TURNING
MACHINES
OFF
EVEN
AT
UTILIZATION
ZERO
POWER
USAGE
WAS
NOT
ACHIEVED
WITH
DVFS
OFF
FOR
A
NUMBER
OF
REASONS
FIRST
AT
LEAST
ONE
ACTIVE
SERVER
IS
NEEDED
TO
HOST
ALL
THE
VMS
SECOND
OUR
ENCLOSURE
CONTAINED
TWO
NETWORK
SWITCHES
TWO
SAN
SWITCHES
TWO
MANAGEMENT
MODULES
AND
SIX
POWER
SUPPLIES
MOST
OF
THESE
COMPONENTS
ARE
NOT
ENERGY
PROPORTIONAL
FINALLY
LIKE
ALL
INDUSTRY
STANDARD
SERVERS
EVEN
OFF
SERVERS
HAVE
AN
ACTIVE
MANAGEMENT
PROCESSOR
THAT
IS
USED
FOR
NETWORK
MANAGEMENT
TASKS
SUCH
AS
REMOTE
KVM
AND
POWER
CYCLING
COOLING
ENERGY
PROPORTIONALITY
WHILE
WE
SHOWED
HOW
ENERGY
PROPORTIONALITY
COULD
BE
ACHIEVED
FOR
SERVER
POWER
IN
SECTION
COOLING
EQUIP
MENT
ALSO
CONSUMES
A
SIGNIFICANT
PART
OF
THE
TOTAL
POWER
USED
BY
A
DATA
CENTER
IN
PARTICULAR
SERVER
FANS
CAN
CON
SUME
BETWEEN
OF
TOTAL
SERVER
POWER
AND
AT
THE
DATA
CENTER
LEVEL
COOLING
CAN
ACCOUNT
FOR
AS
MUCH
AS
OF
THE
TOTAL
POWER
CONSUMED
IN
THIS
CASE
STUDY
WE
THEREFORE
EXAMINE
HOW
INTELLIGENT
FAN
CONTROL
CAN
BE
USED
TO
ACHIEVE
BETTER
ENERGY
PROPORTIONALITY
FOR
SERVER
COOLING
RESOURCES
WE
CONTINUE
TO
USE
THE
EXPERIMENTAL
SETUP
DESCRIBED
IN
SECTION
AND
USE
THE
DVFS
OFF
POL
ICY
PRESENTED
IN
SECTION
FOR
MANAGING
SERVER
POWER
THE
OBJECTIVE
OF
FAN
CONTROL
IS
TO
PROVIDE
ENOUGH
COOL
AIR
FOR
THE
BLADES
SO
THAT
THE
SERVER
TEMPERATURES
CAN
BE
MAIN
TAINED
BELOW
THRESHOLDS
FOR
THERMAL
SAFETY
REASONS
IN
THIS
PAPER
WE
SPECIFICALLY
EVALUATE
TWO
DIFFERENT
FAN
CONTROLLERS
ANOMALOUS
READING
NOTICEABLE
IN
FIGURE
IS
FOR
THE
NO
DVFS
AND
DVFS
SETTINGS
AT
AND
UTILIZATION
LEVELS
WHILE
THE
PROCESSOR
REPORTS
BEING
AT
THE
HIGHEST
P
STATE
WE
RECORDED
A
SHARP
DROP
IN
POWER
USAGE
AT
THESE
POINTS
WE
BELIEVE
THAT
THE
PROCESSOR
IS
USING
AN
INTERNAL
POWER
SAVING
SCHEME
THAT
IT
DISABLES
AT
HIGH
UTILIZATIONS
EACH
RESULT
PRESENTED
ABOVE
IS
AN
AVERAGE
OF
APPROXIMATELY
READINGS
OVER
A
MINUTE
INTERVAL
FIGURE
FAN
POWER
A
REACTIVE
FAN
CONTROLLER
RFC
AND
A
PREDICTIVE
FAN
CONTROLLER
PFC
THE
REACTIVE
FAN
CONTROLLER
RFC
IS
A
SIMPLE
FEEDBACK
CONTROLLER
THAT
CHANGES
FAN
SPEEDS
BASED
ON
THE
DATA
GATH
ERED
FROM
THE
HARDWARE
TEMPERATURE
SENSORS
PRESENT
ON
ALL
THE
BLADES
BECAUSE
OF
THE
COMPLEXITY
OF
SHARING
FANS
BETWEEN
BLADES
THE
RFC
SYNCHRONOUSLY
CHANGES
THE
SPEED
OF
ALL
FANS
IN
EACH
ROW
THERE
ARE
TWO
ROWS
OF
BLADES
AND
FANS
IN
THE
ENCLOSURE
IN
RESPONSE
TO
THE
MAXIMUM
OBSERVED
TEMPERATURE
FOR
THAT
ROW
WHEN
THE
MAXIMUM
TEMPERATURE
IS
ABOVE
THE
THRESHOLD
THE
FAN
SPEEDS
ARE
IN
CREASED
TO
PROVIDE
A
LARGER
VOLUME
OF
AIR
FLOW
TO
COOL
THE
SERVERS
AND
VICE
VERSA
THE
RFC
IS
SIMILAR
TO
COMMERCIAL
FAN
CONTROLLERS
USED
IN
INDUSTRY
TODAY
THE
PREDICTIVE
FAN
CONTROLLER
PFC
AIMS
TO
MINIMIZE
THE
TOTAL
FAN
POWER
CONSUMPTION
WITHOUT
VIOLATING
TEMPER
ATURE
CONSTRAINTS
IT
USES
TEMPERATURE
SENSORS
IN
THE
BLADES
AS
WELL
AS
SOFTWARE
SENSORS
TO
MONITOR
SERVER
UTILIZATION
FIGURE
SHOWS
THE
POWER
MODEL
FOR
A
FAN
IN
OUR
ENCLOSURE
WHERE
THE
FAN
POWER
IS
A
CUBIC
FUNCTION
OF
THE
FAN
SPEED
WHICH
CLOSELY
MATCHES
THE
MEASURED
FAN
POWER
NOTE
THAT
THE
VOLUME
AIR
FLOW
RATE
IS
APPROXIMATELY
PROPORTIONAL
TO
THE
FAN
SPEED
ALSO
NOTE
THAT
EACH
FAN
PROVIDES
DIFFERENT
LEVELS
OF
COOLING
RESOURCE
I
E
COOL
AIR
TO
EACH
INDIVIDUAL
BLADE
ACCORDING
TO
THE
LOCATION
OF
THE
FAN
WITH
RESPECT
TO
THE
BLADE
IN
THIS
REGARD
EACH
FAN
HAS
A
UNIQUE
COOLING
EFFI
CIENCY
WITH
RESPECT
TO
EACH
BLADE
THIS
PROVIDES
AN
OPPOR
TUNITY
TO
MINIMIZE
THE
FAN
POWER
CONSUMPTION
BY
EXPLOR
ING
THE
VARIATION
IN
COOLING
EFFICIENCY
OF
DIFFERENT
FANS
FOR
DIFFERENT
BLADES
ALONG
WITH
THE
TIME
VARYING
DEMANDS
OF
THE
WORKLOADS
WE
BUILT
A
THERMAL
MODEL
EMPIRICALLY
THAT
EXPLICITLY
CAPTURES
THE
COOLING
EFFICIENCY
BETWEEN
EACH
PAIR
OF
BLADE
AND
FAN
THIS
THERMAL
MODEL
TOGETHER
WITH
THE
BLADE
AND
THE
FAN
POWER
MODELS
IS
USED
BY
THE
PFC
TO
PREDICT
FUTURE
SERVER
TEMPERATURES
FOR
ANY
GIVEN
FAN
SPEED
AND
MEASURED
SERVER
UTILIZATION
BY
REPRESENTING
THIS
AS
A
CONVEX
CONSTRAINED
OPTIMIZATION
PROBLEM
THE
PFC
IS
ABLE
TO
USE
AN
OFF
THE
SHELF
CONVEX
OPTIMIZATION
SOLVER
TO
SET
THE
FAN
SPEEDS
TO
VALUES
THAT
POTENTIALLY
MINIMIZE
THE
AGGREGATE
POWER
CONSUMPTION
BY
THE
FANS
WHILE
KEEPING
THE
BLADE
TEMPERATURES
BELOW
THEIR
THRESHOLDS
THE
RESULTS
FOR
THE
TWO
CONTROLLERS
ARE
PRESENTED
IN
FIG
URE
THE
FIGURE
ALSO
INCLUDES
THE
POWER
CONSUMED
BY
SETTING
THE
FANS
TO
A
STATIC
SPEED
THAT
INDEPENDENT
OF
THE
WORKLOADS
IS
GUARANTEED
TO
KEEP
TEMPERATURES
BELOW
THE
THRESHOLD
UNDER
NORMAL
AMBIENT
OPERATING
CONDITIONS
WHEN
EXAMINING
THE
RFC
PERFORMANCE
IT
IS
HELPFUL
TO
NOTE
THE
RELATIONSHIP
BETWEEN
VM
UTILIZATION
AND
FAN
SPEED
FOR
A
GIVEN
ROW
OF
FANS
THE
FAN
SPEED
IS
DIRECTLY
CONTROLLED
BY
THE
MAXIMUM
CPU
TEMPERATURE
IN
THE
ROW
FURTHERMORE
CPU
TEMPERATURE
IS
A
FUNCTION
OF
BLADE
UTI
LIZATION
AND
BLADE
AMBIENT
OR
INLET
TEMPERATURE
BLADE
UTI
LIZATION
HOWEVER
DOES
NOT
ALWAYS
DIRECTLY
CORRELATE
TO
VM
UTILIZATION
FOR
EXAMPLE
WITH
EACH
VM
UTILIZATION
SET
TO
ONE
VCPU
THE
SYSTEM
CANNOT
FIT
MORE
THAN
VMS
PER
MACHINE
WITH
AN
OVERALL
BLADE
UTILIZATION
OF
FOUR
CPUS
HOWEVER
WITH
A
REDUCED
VM
UTILIZATION
OF
EACH
BLADE
CAN
ACCOMMODATE
VMS
WITH
AN
OVERALL
BLADE
UTILIZATION
OF
AS
A
BLADE
CPU
TEMPERATURE
WILL
BE
MUCH
HIGHER
WITH
A
UTILIZATION
OF
VS
IT
WILL
REQUIRE
A
GREATER
AMOUNT
OF
COOLING
AND
THEREFORE
USE
MORE
POWER
DUE
TO
INCREASED
FAN
SPEEDS
SIMILARLY
IF
A
BLADE
IS
LOCATED
IN
AN
AREA
OF
INCREASED
AMBIENT
TEMPERATURE
THAT
BLADE
COULD
ALSO
DRIVE
FAN
SPEED
HIGHER
IF
AND
WHEN
IT
BE
COMES
UTILIZED
EVEN
IF
THE
UTILIZATION
LEVELS
ARE
RELATIVELY
LOW
THESE
FACTORS
ARE
RESPONSIBLE
FOR
THE
RFC
OPERATING
IN
TWO
POWER
BANDS
APPROXIMATELY
BETWEEN
W
WHEN
THE
UTILIZATION
RANGES
BETWEEN
AND
BETWEEN
W
WHEN
THE
UTILIZATION
RANGES
BETWEEN
EVEN
AT
UTILIZATION
THE
RFC
STILL
USES
OF
THE
PEAK
POWER
USED
AT
UTILIZATION
IN
CONTRAST
DUE
TO
ITS
KNOWLEDGE
OF
THE
COOLING
EFFICIEN
CIES
OF
DIFFERENT
FANS
THE
DEMAND
LEVELS
OF
THE
INDIVIDUAL
BLADES
AND
THEIR
AMBIENT
TEMPERATURES
THE
PFC
IS
ABLE
TO
SET
FAN
SPEEDS
INDIVIDUALLY
AND
AVOID
THE
CORRELATED
BEHAV
IOR
EXHIBITED
BY
CONTROLLERS
LIKE
THE
RFC
OVERALL
THE
PFC
INDUCES
AN
APPROXIMATELY
LINEAR
RELATIONSHIP
BETWEEN
THE
FAN
POWER
AND
THE
AGGREGATE
ENCLOSURE
UTILIZATION
AND
AT
UTILIZATION
THE
PFC
ONLY
CONSUMES
OF
THE
POWER
IT
USES
AT
UTILIZATION
THE
USE
OF
MODEL
BASED
OPTI
MIZATION
ALSO
ALLOWS
THE
PFC
TO
PERFORM
SIGNIFICANTLY
BET
TER
THAN
THE
RFC
WHEN
WE
COMPARE
THE
TWO
CONTROLLERS
THE
PFC
CAN
REDUCE
FAN
POWER
USAGE
BY
AT
BOTH
AND
UTILIZATION
AT
UTILIZATION
THE
PFC
ONLY
CONSUMES
OF
THE
POWER
USED
BY
THE
RFC
AT
HOWEVER
THE
PFC
AND
THE
RFC
WITH
ZERO
LOAD
IS
UN
ABLE
TO
REDUCE
ITS
POWER
USAGE
TO
W
THIS
WAS
NOT
A
DE
FICIENCY
OF
OUR
OPTIMIZATION
BASED
APPROACH
BUT
WAS
DUE
TO
THE
FACT
THAT
THE
FANS
WERE
ALSO
RESPONSIBLE
FOR
COOLING
THE
BLADE
ENCLOSURE
NETWORKING
AND
MANAGEMENT
MOD
ULES
WE
THEREFORE
HAD
TO
LOWER
BOUND
THE
FAN
SPEEDS
TO
ENSURE
THAT
THESE
NON
ENERGY
PROPORTIONAL
COMPONENTS
DID
NOT
ACCIDENTALLY
OVERHEAT
ASSUMPTIONS
EVEN
THOUGH
WE
USED
A
HOMOGENEOUS
SET
OF
MACHINES
IN
OUR
CASES
STUDIES
OUR
EXPERIENCE
HAS
SHOWN
THAT
THESE
AL
GORITHMS
CAN
BE
EXTENDED
WITH
DIFFERENT
POWER
AND
THERMAL
MODELS
TO
CONTROL
AN
ENSEMBLE
COMPOSED
OF
HETEROGENEOUS
HARDWARE
FURTHER
CURRENT
GENERATION
OF
PROCESSORS
FROM
BOTH
INTEL
AND
AMD
SUPPORT
CPUID
MASKING
THAT
AL
LOWS
VMS
TO
MIGRATE
BETWEEN
PROCESSORS
FROM
DIFFERENT
FAMILIES
THIS
WORK
ALSO
ASSUMES
THAT
IT
IS
POSSIBLE
TO
MI
GRATE
VM
IDENTITIES
SUCH
AS
IP
AND
NETWORK
MAC
ADDRESSES
WITH
THE
VMS
WHILE
THIS
IS
GENERALLY
NOT
A
PROBLEM
IN
A
SINGLE
DATA
CENTER
MIGRATING
STORAGE
AREA
NETWORK
SAN
IDENTITIES
CAN
SOMETIMES
BE
PROBLEMATIC
HOWEVER
VENDOR
PRODUCTS
SUCH
AS
HP
VIRTUAL
CONNECT
AND
EMULEX
VIR
TUAL
HBA
HAVE
INTRODUCED
A
LAYER
OF
VIRTUALIZATION
IN
THE
STORAGE
STACK
TO
SOLVE
THIS
PROBLEM
NOTE
THAT
IT
MIGHT
BE
DIFFICULT
TO
ADOPT
THE
OPTIMIZATION
BASED
SOLUTIONS
SIMILAR
TO
THOSE
PROPOSED
IN
THIS
PAPER
TO
APPLICATIONS
THAT
DEPEND
ON
LOCALLY
ATTACHED
STORAGE
FOR
THEIR
INPUT
DATA
HOWEVER
THE
FACT
THAT
SUCH
LOCALLY
ATTACHED
STORAGE
SYSTEMS
USUALLY
REPLICATE
DATA
FOR
RELIA
BILITY
AND
AVAILABILITY
REASONS
MIGHT
PROVIDE
A
POSSI
BLE
SOLUTION
IN
SUCH
A
SCHEME
THE
OPTIMIZATION
ALGO
RITHMS
COULD
BE
MADE
AWARE
OF
THE
DEPENDENCIES
BETWEEN
THE
VMS
AND
THE
LOCATIONS
OF
THEIR
DATASETS
GIVEN
THIS
INFORMATION
THE
ALGORITHMS
COULD
FIND
A
SUITABLE
CONSOLI
DATED
MAPPING
OF
VMS
TO
PHYSICAL
MACHINES
IN
ORDER
TO
MAKE
THIS
SCHEME
EFFECTIVE
A
HIGHER
DEGREE
OF
REPLICATION
MIGHT
BE
NEEDED
TO
GIVE
THE
ALGORITHMS
MORE
FLEXIBILITY
IN
MAKING
PLACEMENT
DECISIONS
THIS
CHANGE
ESSENTIALLY
BOILS
DOWN
TO
A
TRADEOFF
BETWEEN
THE
COST
OF
INCREASED
STORAGE
CAPACITY
VERSUS
ENERGY
SAVINGS
FINALLY
OUR
APPROACH
APPROXIMATES
ENERGY
PROPORTION
ALITY
BY
TURNING
MACHINES
OFF
CONCERNS
HAVE
BEEN
PREVI
OUSLY
RAISED
ABOUT
RELIABILITY
OF
BOTH
SERVERS
AND
DISK
DRIVES
DUE
TO
AN
INCREASED
NUMBER
OF
ON
OFF
CYCLES
HOWEVER
OUR
CONVERSATIONS
WITH
BLADE
SYSTEM
DESIGNERS
HAVE
SHOWN
THAT
IT
SHOULD
NOT
AFFECT
SERVER
CLASS
MACHINES
OR
GIVEN
THE
LARGE
NUMBER
OF
ON
OFF
CYCLES
SUPPORTED
BY
DISK
DRIVES
THEIR
INTERNAL
STORAGE
SYSTEMS
DURING
THEIR
NORMAL
LIFETIME
AGGRESSIVE
CONSOLIDATION
MIGHT
ALSO
HURT
APPLICATION
AVAIL
ABILITY
IF
THE
UNDERLYING
HARDWARE
IS
UNRELIABLE
AS
MOST
APPLICATIONS
TEND
TO
BE
DISTRIBUTED
FOR
FAULT
TOLERANCE
IN
TRODUCING
APPLICATION
AWARENESS
INTO
THE
CONSOLIDATION
AL
GORITHM
TO
PREVENT
IT
FROM
CONSOLIDATING
SEPARATE
INSTANCES
OF
THE
SAME
APPLICATION
ON
THE
SAME
PHYSICAL
MACHINE
WILL
ADDRESS
THIS
ISSUE
CONCLUDING
REMARKS
THIS
PAPER
HAS
SHOWN
THAT
IT
IS
POSSIBLE
TO
USE
OPTIMIZATION
BASED
TECHNIQUES
TO
APPROXIMATE
ENERGY
PROPORTIONAL
BE
HAVIOR
AT
THE
ENSEMBLE
LEVEL
EVEN
THOUGH
OUR
TECHNIQUES
RESULT
IN
SOME
ADDED
COMPLEXITY
IN
THE
SYSTEM
IN
THE
FORM
OF
MODELS
OPTIMIZATION
ROUTINES
AND
CONTROLLERS
WE
BE
LIEVE
THE
POWER
SAVINGS
ARE
SIGNIFICANT
E
NOUGH
T
O
JUSTIFY
IT
HOWEVER
BETTER
INSTRUMENTATION
CAN
HELP
US
GET
EVEN
CLOSER
TO
THEORETICAL
ENERGY
PROPORTIONALITY
FOR
EXAMPLE
IF
TEMPERATURE
SENSORS
WHICH
ARE
RELATIVELY
CHEAP
TO
DE
PLOY
WERE
INSTALLED
IN
THE
NETWORKING
AND
SAN
SWITCHES
IT
WOULD
HAVE
ALLOWED
OUR
FAN
CONTROLLER
TO
HAVE
MORE
COM
PLETE
KNOWLEDGE
OF
THE
THERMAL
CONDITION
INSIDE
THE
ENCLO
SURE
SO
THAT
MORE
EFFICIENT
FAN
SPEED
OPTIMIZATION
COULD
BE
ACHIEVED
IN
ADDITION
WE
HAVE
USED
CPU
UTILIZATION
IN
THIS
CASE
STUDY
AS
A
PROXY
FOR
APPLICATION
LEVEL
PERFORMANCE
TO
DIRECTLY
EVALUATE
AND
MANAGE
APPLICATION
PERFORMANCE
WE
WILL
NEED
SENSORS
THAT
MEASURE
APPLICATION
LEVEL
METRICS
SUCH
AS
THROUGHPUT
AND
RESPONSE
TIME
THESE
SENSORS
CAN
BE
PROVIDED
BY
PARSING
APPLICATION
LOGS
OR
BY
MONITORING
USER
REQUESTS
AND
RESPONSES
WHILE
THE
CONTROLLERS
SHOWN
IN
THIS
PAPER
ASSUMED
A
SIN
GLE
MANAGEMENT
DOMAIN
FURTHER
STUDY
NEEDS
TO
BE
DONE
TO
SHOW
HOW
THEY
WOULD
WORK
IN
A
FEDERATED
ENVIRONMENT
WHERE
INFORMATION
WOULD
NEED
TO
BE
SHARED
BETWEEN
CON
TROLLERS
AT
DIFFERENT
MANAGEMENT
LAYERS
AND
POSSIBLY
FROM
DIFFERENT
VENDORS
WE
BELIEVE
THAT
SOME
OF
THE
DISTRIBUTED
MANAGEMENT
TASK
FORCE
DMTF
STANDARDS
WOULD
HELP
ADDRESS
AT
LEAST
SOME
OF
THESE
ISSUES
PRIOR
WORK
EXISTS
THAT
LOOKED
AT
DATA
CENTER
LEVEL
COOL
ING
EFFICIENCY
B
Y
M
ANIPULATION
O
F
C
RAC
U
NIT
ETTINGS
OR
BY
TEMPERATURE
AWARE
WORKLOAD
PLACEMENT
HOW
EVER
GIVEN
THAT
THESE
STUDIES
ONLY
LOOKED
AT
TOTAL
POWER
CONSUMPTION
A
MORE
CAREFUL
INVESTIGATION
IS
NEEDED
IN
THE
CONTEXT
OF
THIS
PAPER
FOR
EXAMPLE
THE
MODEL
BASED
OPTI
MIZATION
APPROACH
WE
USED
IN
THE
PREDICTIVE
FAN
CONTROLLER
MAY
BE
APPLIED
TO
THE
CRAC
UNIT
CONTROL
PROBLEM
STUD
IED
IN
TO
ACHIEVE
ENERGY
PROPORTIONALITY
FOR
THE
COOLING
EQUIPMENT
AT
THE
DATA
CENTER
LEVEL
FINALLY
EVEN
THOUGH
THIS
PAPER
DEMONSTRATED
ENERGY
PROPORTIONALITY
AT
THE
ENSEMBLE
LAYER
THIS
DOES
NOT
PRE
CLUDE
THE
NEED
FOR
BETTER
ENERGY
EFFICIENCY
FOR
INDIVIDUAL
COMPONENTS
SUCH
AS
DISKS
MEMORY
AND
POWER
SUPPLIES
WE
BELIEVE
THAT
NEW
HARDWARE
DESIGNS
WITH
FINER
LEVELS
OF
POWER
CONTROL
WILL
HELP
IN
DESIGNING
ENERGY
EFFICIENT
SYS
TEMS
AT
BOTH
THE
SINGLE
SERVER
AND
ENSEMBLE
LAYER
ONLINE
STRATEGIES
FOR
DYNAMIC
POWER
MANAGEMENT
IN
SYSTEMS
WITH
MULTIPLE
POWER
SAVING
STATES
SANDY
IRANI
SANDEEP
SHUKLA
AND
RAJESH
GUPTA
UNIVERSITY
OF
CALIFORNIA
AT
IRVINE
ONLINE
DYNAMIC
POWER
MANAGEMENT
DPM
STRATEGIES
REFER
TO
STRATEGIES
THAT
ATTEMPT
TO
MAKE
POWER
MODE
RELATED
DECISIONS
BASED
ON
INFORMATION
AVAILABLE
AT
RUNTIME
IN
MAKING
SUCH
DECISIONS
THESE
STRATEGIES
DO
NOT
DEPEND
UPON
INFORMATION
OF
FUTURE
BEHAVIOR
OF
THE
SYSTEM
OR
ANY
A
PRIORI
KNOWLEDGE
OF
THE
INPUT
CHARACTERISTICS
IN
THIS
PAPER
WE
PRESENT
ONLINE
STRATEGIES
AND
EVALUATE
THEM
BASED
ON
A
MEASURE
CALLED
THE
COMPETITIVE
RATIO
THAT
ENABLES
A
QUANTITATIVE
ANALYSIS
OF
THE
PERFORMANCE
OF
ONLINE
STRATEGIES
ALL
EARLIER
APPROACHES
ONLINE
OR
PREDICTIVE
HAVE
BEEN
LIMITED
TO
SYSTEMS
WITH
TWO
POWER
SAVING
STATES
E
G
IDLE
AND
SHUTDOWN
THE
ONLY
EARLIER
APPROACHES
THAT
HANDLED
MULTIPLE
POWER
SAVING
STATES
WERE
BASED
ON
STOCHASTIC
OPTIMIZATION
THIS
PAPER
PROVIDES
A
THEORETICAL
BASIS
FOR
THE
ANALYSIS
OF
DPM
STRATEGIES
FOR
SYSTEMS
WITH
MULTIPLE
POWER
DOWN
STATES
WITHOUT
RESORTING
TO
SUCH
COMPLEX
APPROACHES
WE
SHOW
HOW
A
RELATIVELY
SIMPLE
ONLINE
LEARNING
SCHEME
CAN
BE
USED
TO
IMPROVE
THE
COMPETITIVE
RATIO
OVER
DETERMINISTIC
STRATEGIES
USING
THE
NOTION
OF
PROBABILITY
BASED
ONLINE
DPM
STRATEGIES
EXPERIMENTAL
RESULTS
SHOW
THAT
THE
ALGORITHM
PRESENTED
HERE
ATTAINS
THE
BEST
COMPETITIVE
RATIO
IN
COMPARISON
WITH
OTHER
KNOWN
PREDICTIVE
DPM
ALGORITHMS
THE
OTHER
ALGORITHMS
THAT
COME
CLOSE
TO
MATCHING
ITS
PERFORMANCE
IN
POWER
SUFFER
AT
LEAST
AN
ADDITIONAL
WAKE
UP
LATENCY
ON
AVERAGE
MEANWHILE
THE
ALGORITHMS
THAT
HAVE
COMPARABLE
LATENCY
TO
OUR
METHODS
USE
AT
LEAST
MORE
POWER
ON
AVERAGE
CATEGORIES
AND
SUBJECT
DESCRIPTORS
C
COMPUTER
SYSTEMS
ORGANIZATION
PERFORMANCE
OF
SYSTEMS
GENERAL
TERMS
ALGORITHMS
PERFORMANCE
ADDITIONAL
KEY
WORDS
AND
PHRASES
DYNAMIC
POWER
MANAGEMENT
ONLINE
ALGORITHMS
INTRODUCTION
POWER
MANAGEMENT
IN
EMBEDDED
COMPUTING
SYSTEMS
IS
ACHIEVED
BY
ACTIVELY
CHANGING
THE
POWER
CONSUMPTION
PROFILE
OF
THE
SYSTEM
BY
PUTTING
ITS
COMPONENTS
INTO
POWER
ENERGY
STATES
SUFFICIENT
TO
MEETING
FUNCTIONALITY
REQUIREMENTS
FOR
EXAMPLE
AN
IDLING
COMPONENT
SUCH
AS
A
DISK
DRIVE
CAN
BE
PUT
INTO
A
SLOWDOWN
OR
SHUTDOWN
STATE
OF
COURSE
BRINGING
SUCH
A
COMPONENT
BACK
TO
THE
ACTIVE
THIS
WORK
WAS
SUPPORTED
BY
NSF
GRANT
CCR
SRC
AND
DARPA
ITO
SUPPORTED
PADS
PROJECT
UNDER
THE
PAC
C
PROGRAM
IN
ADDITION
THE
FIRST
AUTHOR
IS
PARTIALLY
SUPPORTED
BY
NSF
GRANT
CCR
AND
BY
ONR
AWARD
AUTHORS
ADDRESS
INFORMATION
AND
COMPUTER
SCIENCE
DEPARTMENT
UNIVERSITY
OF
CALIFORNIA
AT
IRVINE
IRVINE
CA
EMAIL
IRANI
SKSHUKLA
RGUPTA
ICS
UCI
EDU
PERMISSION
TO
MAKE
DIGITAL
HARD
COPY
OF
ALL
OR
PART
OF
THIS
MATERIAL
WITHOUT
FEE
FOR
PERSONAL
OR
CLASSROOM
USE
PROVIDED
THAT
THE
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVAN
TAGE
THE
ACM
COPYRIGHT
SERVER
NOTICE
THE
TITLE
OF
THE
PUBLICATION
AND
ITS
DATE
APPEAR
AND
NOTICE
IS
GIVEN
THAT
COPYING
IS
BY
PERMISSION
OF
THE
ACM
INC
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
QC
ACM
ACM
TRANSACTIONS
ON
EMBEDDED
COMPUTING
SYSTEMS
VOL
NO
AUGUST
PAGES
STATE
MAY
REQUIRE
ADDITIONAL
ENERGY
AND
OR
LATENCY
TO
SERVICE
AN
INCOMING
TASK
THE
INPUT
TO
THE
PROBLEM
WE
CONSIDER
HERE
IS
THE
LENGTH
OF
AN
UPCOMING
IDLE
PERIOD
AND
THE
DECISION
TO
BE
MADE
IS
WHETHER
TO
TRANSITION
TO
A
LOWER
POWER
DISSIPATION
STATE
WHILE
THE
SYSTEM
IS
IDLE
THERE
ARE
SEVERAL
ISSUES
IN
COMING
TO
THIS
DECISION
INTELLIGENTLY
FOR
INSTANCE
IMMEDIATE
SHUTDOWN
THAT
IS
SHUT
DOWN
AS
SOON
AS
AN
IDLE
PERIOD
IS
DETECTED
MAY
NOT
SAVE
OVERALL
ENERGY
IF
THE
IDLE
PERIOD
IS
SO
SHORT
THAT
THE
POWERING
UP
COSTS
ARE
GREATER
THAN
THE
ENERGY
SAVED
IN
THE
SLEEP
STATE
ON
THE
OTHER
HAND
WAITING
TOO
LONG
TO
POWER
DOWN
MAY
NOT
ACHIEVE
THE
BEST
ENERGY
REDUCTIONS
POSSIBLE
THUS
THERE
EXISTS
A
NEED
FOR
EFFECTIVE
AND
EFFICIENT
DECISION
PROCEDURES
TO
MANAGE
POWER
CONSUMPTION
DYNAMIC
POWER
MANAGEMENT
DPM
ATTEMPTS
TO
MAKE
SUCH
DECISIONS
USUALLY
UNDER
THE
CONTROL
OF
THE
OPERATING
SYSTEM
AT
RUNTIME
BASED
ON
THE
DYNAM
ICALLY
CHANGING
SYSTEM
STATE
FUNCTIONALITY
AND
TIMING
REQUIREMENTS
BENINI
AND
DE
MICHELI
BENINI
ET
AL
CHUNG
ET
AL
HWANG
ET
AL
IRANI
ET
AL
RAMANATHAN
ET
AL
SHUKLA
AND
GUPTA
SRIVASTAVA
ET
AL
IN
A
SURVEY
OF
DPM
TECHNIQUES
IN
BENINI
ET
AL
THE
AUTHORS
CLASSIFY
DPM
STRATEGIES
INTO
TWO
MAIN
GROUPS
A
PREDICTIVE
SCHEMES
AND
B
STOCHASTIC
OPTIMUM
CONTROL
SCHEMES
PREDICTIVE
SCHEMES
ATTEMPT
TO
PREDICT
THE
TIMING
OF
FUTURE
INPUT
TO
THE
SYSTEM
AND
SCHEDULE
SHUTDOWN
USUALLY
TO
A
SINGLE
LOWER
POWER
STATE
BASED
ON
THESE
PREDICTIONS
STOCHASTIC
OPTIMUM
CONTROL
IS
A
WELL
RESEARCHED
AREA
BENINI
ET
AL
CHUNG
ET
AL
QIU
AND
PEDRAM
QIU
ET
AL
SIMUNIC
ET
AL
THE
CHIEF
CHARACTERISTIC
OF
THESE
APPROACHES
IS
THE
CONSTRUCTION
AND
VALIDATION
OF
A
MATHEMATICAL
MODEL
OF
THE
SYSTEM
THAT
LENDS
ITSELF
TO
A
FORMULATION
OF
A
STOCHASTIC
OPTIMIZATION
PROBLEM
THEN
STRATEGIES
TO
GUIDE
THE
SYSTEM
POWER
PROFILE
ARE
DEVISED
THAT
ACHIEVE
THE
MOST
POWER
SAVINGS
IN
PRESENCE
OF
THE
UNCERTAINTY
RELATED
TO
SYSTEM
INPUTS
WHILE
SEVERAL
USEFUL
AND
PRACTICAL
TECHNIQUES
HAVE
BEEN
DEVELOPED
USING
THE
PREDICTIVE
AND
STOCHASTIC
OPTIMUM
CONTROL
SCHEMES
IT
IS
DIFFICULT
TO
DEVELOP
BOUNDS
ON
THE
QUALITY
OF
THE
RESULTS
WITHOUT
EXTENSIVE
SIMULATIONS
AND
OR
MODEL
JUSTIFICATION
COMPETITIVE
ANALYSIS
AND
ITS
LIMITATIONS
WE
APPROACH
DPM
AS
AN
INHERENTLY
ONLINE
PROBLEM
IN
THAT
AN
ALGORITHM
GOV
ERNING
POWER
MANAGEMENT
MUST
MAKE
DECISIONS
ABOUT
THE
EXPENDITURE
OF
RE
SOURCES
BEFORE
ALL
THE
INPUT
TO
THE
SYSTEM
IS
AVAILABLE
BORODIN
AND
EL
YANIV
SPECIFICALLY
THE
ALGORITHM
DOES
NOT
LEARN
THE
LENGTH
OF
AN
IDLE
PERIOD
UNTIL
THE
MOMENT
THAT
IT
ENDS
ANALYTICAL
SOLUTIONS
TO
SUCH
ONLINE
PROBLEMS
ARE
OFTEN
BEST
CHARACTERIZED
IN
TERMS
OF
A
COMPETITIVE
RATIO
PHILLIPS
AND
WESTBROOK
THAT
COMPARES
THE
COST
OF
AN
ONLINE
ALGORITHM
TO
THE
OPTIMAL
OFFLINE
SOLU
TION
THAT
KNOWS
THE
INPUT
IN
ADVANCE
NOTE
THAT
THE
OFFLINE
ALGORITHM
IS
NOT
A
REALISTIC
ALGORITHM
SINCE
IT
KNOWS
THE
LENGTH
OF
THE
IDLE
PERIOD
IN
ADVANCE
IT
IS
JUST
USED
AS
A
POINT
OF
COMPARISON
TO
QUANTIFY
THE
PERFORMANCE
LOSS
DUE
TO
THE
FACT
THAT
A
DPM
STRATEGY
MUST
MAKE
DECISIONS
WITH
ONLY
PARTIAL
INFORMATION
TO
BE
SPECIFIC
WE
SAY
THAT
AN
ALGORITHM
IS
C
COMPETITIVE
IF
FOR
ANY
INPUT
THE
COST
OF
THE
ONLINE
ALGORITHM
IS
BOUNDED
BY
C
TIMES
THE
COST
OF
THE
OPTIMAL
OFFLINE
TABLE
I
VALUES
FOR
THE
POWER
DISSIPATION
AND
START
UP
ENERGY
FOR
THE
IBM
MOBILE
HARD
DRIVE
ALGORITHM
FOR
THAT
INPUT
THE
OFFLINE
ALGORITHM
HAS
ACCESS
TO
THE
ENTIRE
INPUT
BEFORE
COMMITTING
TO
ANY
DECISIONS
FOR
DPM
THE
COST
OF
AN
ALGORITHM
IS
THE
TOTAL
AMOUNT
OF
ENERGY
CONSUMED
THE
COMPETITIVE
RATIO
CR
OF
AN
ALGORITHM
IS
THE
INFIMUM
OVER
ALL
C
SUCH
THAT
THE
ALGORITHM
IS
C
COMPETITIVE
COMPETITIVE
ANALYSIS
BORODIN
AND
EL
YANIV
THAT
IS
ANALYSIS
TO
DE
TERMINE
BOUNDS
ON
THE
COMPETITIVE
RATIO
CAN
BE
DONE
EITHER
AS
A
CASE
ANALYSIS
OF
THE
VARIOUS
ADVERSARIAL
SCENARIOS
KARLIN
ET
AL
RAMANATHAN
OR
THROUGH
THEOREM
PROVING
OR
AUTOMATIC
MODEL
CHECKING
SHUKLA
AND
GUPTA
COMPETITIVE
ANALYSIS
HAS
PROVEN
TO
BE
A
POWERFUL
TOOL
IN
PROVIDING
A
GUAR
ANTEE
ON
THE
PERFORMANCE
OF
AN
ALGORITHM
FOR
ANY
INPUT
COMPETITIVE
ANALYSIS
IS
VALUABLE
IN
SITUATIONS
WHERE
IT
IS
IMPRACTICAL
TO
OBTAIN
AND
PROCESS
INFORMATION
FOR
PREDICTING
FUTURE
INPUTS
IT
ALSO
PROVIDES
AN
ASSURANCE
FOR
THE
DESIGNERS
OF
THE
ALGORITHM
ABOUT
THE
WORST
POSSIBLE
BEHAVIOR
OF
AN
ONLINE
ALGORITHM
FOR
EXAMPLE
IF
AN
ONLINE
STRATEGY
HAS
A
COMPETITIVE
RATIO
OF
WHICH
ASSURES
THE
DESIGNER
THAT
NO
MATTER
WHAT
INPUT
SEQUENCE
IS
PROVIDED
TO
THE
STRATEGY
IT
WILL
NEVER
COST
THE
STRATEGY
MORE
THAN
TWICE
THE
COST
INCURRED
BY
THE
BEST
POSSIBLE
STRATEGY
THERE
ARE
TWO
CHIEF
LIMITATIONS
OF
THE
COMPETITIVE
ANALYSIS
APPROACH
TO
DPM
THAT
WE
SEEK
TO
ADDRESS
IN
THIS
PAPER
THE
FIRST
IS
THAT
THE
RESULTS
TEND
TO
BE
OVERLY
PESSIMISTIC
DUE
TO
THE
FACT
THAT
THEY
EXAMINE
WORST
CASE
BEHAVIOR
THUS
THERE
IS
A
NEED
TO
REFINE
THE
CR
BOUNDS
THAT
TAKE
INTO
ACCOUNT
TYPICAL
INPUT
BEHAVIOR
IN
MANY
APPLICATIONS
THERE
IS
A
STRUCTURE
IN
THE
INPUT
SEQUENCE
THAT
CAN
BE
UTILIZED
TO
FINE
TUNE
ONLINE
STRATEGIES
AND
IMPROVE
THEIR
PERFORMANCE
THE
SECOND
PROBLEM
WITH
OUR
EARLIER
WORK
ON
CR
BASED
DPM
STRATE
GIES
KARLIN
ET
AL
RAMANATHAN
ET
AL
IS
THAT
THE
SYSTEM
MODEL
CONSISTS
OF
DEVICES
WITH
ONLY
TWO
POWER
SAVING
STATES
NAMELY
AN
IDLE
STATE
AND
A
SHUTDOWN
STATE
THIS
IS
A
LIMITATION
OF
MOST
PREDICTIVE
STRATEGIES
AS
WELL
BENINI
ET
AL
IN
CONTRAST
MOST
REAL
SYSTEMS
CONSIST
OF
COMPONENTS
WITH
MULTIPLE
POWER
STATES
THERE
ARE
TWO
KINDS
OF
SYSTEMS
HERE
SYSTEMS
WITH
MULTIPLE
SHUTDOWN
STATES
AS
WELL
AS
SYSTEMS
WITH
MULTIPLE
OPERATING
STATES
FOR
INSTANCE
TABLE
I
SHOWS
THE
POWER
STATES
OF
A
PORTABLE
HARD
DRIVE
IBM
THAT
CONSISTS
OF
THREE
LOW
POWER
STATES
IN
WHICH
A
DEVICE
CAN
BE
WHEN
IT
IS
NOT
PROCESSING
ANY
REQUESTS
RADIO
MODEMS
REPRESENT
DEVICES
THAT
CAN
OFTEN
BE
IN
MULTIPLE
ACTIVE
STATES
FOR
INSTANCE
TRANSMITTING
DATA
AT
VARIOUS
POWER
LEVELS
TO
THE
RADIO
TRANSMITTER
WE
DO
NOT
ADDRESS
THE
PROBLEM
OF
CHOOSING
AMONG
DIFFERENT
OPERATING
STATES
IN
THIS
PAPER
FOR
SYSTEMS
WITH
MORE
THAN
ONE
DEVICE
OUR
PROTOCOLS
CAN
BE
APPLIED
TO
EACH
DEVICE
SEPARATELY
EACH
DEVICE
EXPERIENCES
PERIODS
OF
IDLE
TIME
THAT
MAY
OR
MAY
NOT
COINCIDE
WITH
THE
IDLE
PERIODS
EXPERIENCED
BY
OTHER
DEVICES
IN
THE
SYSTEM
A
POWER
MANAGEMENT
STRATEGY
CAN
THEN
BE
APPLIED
TO
A
PARTICULAR
DEVICE
DURING
THE
IDLE
PERIODS
EXPERIENCED
BY
THAT
DEVICE
THERE
WILL
BE
SOME
DEPENDENCE
BETWEEN
PERFORMANCES
FOR
DIFFERENT
DEVICES
IN
CASES
WHERE
A
REQUEST
REQUIRES
MORE
THAN
ONE
DEVICE
TO
BE
SATISFIED
IN
THESE
SITUATIONS
A
DEVICE
THAT
IS
IN
AN
ACTIVE
STATE
MAY
EXPERIENCE
SOME
LATENCY
BECAUSE
IT
HAS
TO
WAIT
FOR
ANOTHER
DEVICE
TO
TRANSITION
FROM
A
LOW
POWER
STATE
TO
THE
ACTIVE
STATE
BEFORE
EITHER
CAN
BEGIN
WORK
IN
THIS
PAPER
WE
FOCUS
ON
PERFORMANCE
FOR
A
SINGLE
DEVICE
ALTHOUGH
THIS
PHENOMENON
WOULD
BE
AN
INTERESTING
DIRECTION
FOR
FUTURE
WORK
RELATED
WORK
MANY
STRATEGIES
FOR
DYNAMIC
MANAGEMENT
OF
POWER
AND
ENERGY
HAVE
BEEN
PRO
POSED
AND
STUDIED
THESE
INCLUDE
PREDICTIVE
STRATEGIES
HWANG
ET
AL
SRIVASTAVA
ET
AL
STOCHASTIC
MODELING
BASED
STRATEGIES
BENINI
ET
AL
QIU
AND
PEDRAM
SESSION
CLUSTERING
AND
PREDICTION
STRATEGIES
LU
AND
DE
MICHELI
ONLINE
STRATEGIES
RAMANATHAN
ET
AL
AND
ADAPTIVE
LEARNING
BASED
STRATEGIES
CHUNG
ET
AL
A
SURVEY
OF
DPM
STRATEGIES
THAT
COVERS
THE
CLASSES
OF
ALGORITHMS
IN
PREDICTIVE
AND
STOCHASTIC
OPTIMUM
CON
TROL
CATEGORIES
CAN
BE
FOUND
IN
BENINI
ET
AL
LU
ET
AL
PRESENT
A
QUANTITATIVE
COMPARISON
OF
VARIOUS
EXISTING
MANAGEMENT
STRATEGIES
MANY
PREDICTIVE
DYNAMIC
POWER
MANAGEMENT
STRATEGIES
BENINI
ET
AL
CHUNG
ET
AL
HWANG
ET
AL
KARLIN
ET
AL
LU
AND
DE
MICHELI
RAMANATHAN
ET
AL
SRIVASTAVA
ET
AL
USE
A
SEQUENCE
OF
PAST
IDLE
PERIOD
LENGTHS
TO
PREDICT
THE
LENGTH
OF
THE
NEXT
IDLE
PERIOD
THESE
STRATEGIES
TYPICALLY
DESCRIBE
THEIR
PREDICTION
FOR
THE
NEXT
IDLE
PERIOD
WITH
A
SINGLE
VALUE
GIVEN
THIS
PREDICTION
THEY
TRANSITION
TO
THE
POWER
STATE
THAT
IS
OPTIMAL
FOR
THE
PREDICTED
IDLE
PERIOD
LENGTH
IN
CASE
THE
PREDICTION
IS
WRONG
THEY
TRANSITION
TO
THE
LOWEST
POWER
STATE
IF
THE
IDLE
PERIOD
EXTENDS
BEYOND
A
FIXED
THRESHOLD
VALUE
FOR
THE
SAKE
OF
COMPARISON
WITH
OTHER
APPROACHES
WE
CALL
THESE
PREDICTIVE
DPM
SCHEMES
SINGLE
VALUE
PREDICTION
SVP
SCHEMES
OF
PARTICULAR
INTEREST
IS
THE
WORK
OF
CHUNG
ET
AL
THAT
ADDRESSES
MULTIPLE
IDLE
STATE
SYSTEMS
USING
A
PREDICTION
SCHEME
BASED
ON
ADAPTIVE
LEARNING
TREES
THEIR
METHOD
HAS
SHOWN
AN
IMPRESSIVELY
HIGH
HIT
RATIO
IN
ITS
PREDICTION
IN
THE
PAST
STOCHASTIC
CONTROL
BASED
METHODS
AS
IN
BENINI
ET
AL
SIMUNIC
ET
AL
CHUNG
ET
AL
QIU
AND
PEDRAM
AND
QIU
ET
AL
HAVE
BEEN
LIMITED
BECAUSE
THEY
MAKE
ASSUMPTIONS
ABOUT
THE
CHARACTERISTIC
PROBABILITY
DISTRIBUTION
OF
THE
INPUT
JOB
ARRIVALS
THE
SERVICE
TIME
DISTRIBUTION
OF
THE
DEVICE
AND
SO
ON
ALTHOUGH
THE
POLICIES
OBTAINED
ARE
OPTIMAL
GIVEN
THESE
ASSUMPTIONS
THERE
ARE
NO
GUARANTEES
THAT
THE
ASSUMPTIONS
WILL
ALWAYS
HOLD
THESE
PROBLEMS
HAVE
BEEN
ADDRESSED
TO
SOME
EXTENT
IN
CHUNG
ET
AL
AND
LU
AND
DE
MICHELI
RECENTLY
THE
STOCHASTIC
MODELING
APPROACH
HAS
BEEN
EXTENDED
SO
THAT
THE
ASSUMPTION
THAT
INTERARRIVAL
TIMES
ARE
EXPONENTIALLY
DISTRIBUTED
CAN
BE
REMOVED
GLYNN
ET
AL
STOCHASTIC
MODELING
HAS
ALSO
BEEN
EXTENDED
TO
SYSTEMS
WHICH
CAN
BE
MODELED
BY
PETRI
NETS
QIU
ET
AL
ONE
DRAWBACK
OF
THESE
APPROACHES
IS
THAT
THEY
REQUIRE
SOLVING
COMPUTATIONALLY
EXPENSIVE
OPTIMIZATION
PROBLEMS
TO
DERIVE
THE
OPTIMAL
SOLUTIONS
IF
THE
USAGE
PATTERNS
FOR
THE
DEVICE
CHANGE
THE
ALGORITHM
HAS
TO
BE
REOPTIMIZED
TO
ADJUST
TO
THE
NEW
BEHAVIOR
IN
CHUNG
ET
AL
AN
ADAPTIVE
TECHNIQUE
HAS
BEEN
PROPOSED
WHICH
TRIES
TO
OVERCOME
THIS
LIMITATION
THE
AU
THORS
USE
SLIDING
WINDOWS
TO
KEEP
A
LONG
ENOUGH
HISTORY
OF
THE
INPUT
ARRIVALS
AND
ESTIMATE
THE
PARAMETERS
OF
THE
ARRIVAL
MARKOV
PROCESSES
HOWEVER
THIS
METHOD
IS
COMPLEX
BECAUSE
THEY
HAVE
TO
SOLVE
THE
OPTIMIZATION
PROBLEM
TO
FIND
THE
OPTIMAL
STRATEGY
IN
A
CLOCK
DRIVEN
OR
EVENT
DRIVEN
MANNER
THERE
ARE
ALSO
A
FEW
EXISTING
RESULTS
ON
COMPETITIVE
ANALYSIS
OF
DPM
STRATE
GIES
IN
THE
ONLINE
ALGORITHMS
LITERATURE
SOME
OF
THIS
WORK
DOES
NOT
ADDRESS
DPM
EXPLICITLY
BUT
THE
GENERAL
FRAMEWORK
APPLIES
TO
DPM
AS
WELL
AN
EXAMPLE
OF
THIS
IS
THE
NOW
CLASSICAL
RESULT
DESCRIBED
IN
PHILLIPS
AND
WESTBROOK
THAT
SHOWS
THAT
IS
THE
BEST
COMPETITIVE
RATIO
ACHIEVABLE
BY
ANY
DETERMINISTIC
ONLINE
ALGORITHM
PROBABILISTIC
ANALYSIS
FOR
ONLINE
DPM
ALGORITHMS
IN
TWO
STATE
SYSTEMS
HAS
BEEN
GIVEN
IN
KARLIN
ET
AL
AND
KESHAV
ET
AL
THEY
AS
SUME
THAT
THE
DISTRIBUTION
OVER
THE
UPCOMING
IDLE
PERIOD
IS
KNOWN
AND
OPTIMIZE
THE
ALGORITHM
BASED
ON
THAT
DISTRIBUTION
THEY
GIVE
A
METHOD
TO
DETERMINE
THE
BEST
ONLINE
ALGORITHM
GIVEN
A
DISTRIBUTION
AND
SHOW
THAT
FOR
ANY
DISTRIBUTION
THE
EXPECTED
COST
OF
THIS
ONLINE
ALGORITHM
IS
WITHIN
A
FACTOR
OF
E
E
OF
THE
EXPECTED
COST
OF
THE
OPTIMAL
OFFLINE
ALGORITHM
THIS
RESULT
IS
TIGHT
IN
THAT
THERE
IS
A
DISTRIBUTION
FOR
WHICH
THE
RATIO
IS
EXACTLY
E
E
ALTHOUGH
FOR
SOME
DISTRIBUTIONS
THE
RATIO
MAY
BE
LESS
OUR
CONTRIBUTIONS
AND
PAPER
ORGANIZATION
IN
SECTION
WE
PRESENT
A
DETERMINISTIC
ALGORITHM
FOR
DPM
FOR
MULTISTATE
DE
VICES
AND
SHOW
THAT
IT
IS
COMPETITIVE
THIS
RESULT
WHICH
EXTENDS
THE
EARLIER
ANALYSIS
DESCRIBED
IN
PHILLIPS
AND
WESTBROOK
IS
TIGHT
IN
THE
SENSE
THAT
THERE
IS
NO
CONSTANT
C
SUCH
THAT
THERE
IS
A
DETERMINISTIC
C
COMPETITIVE
ALGO
RITHM
THAT
WORKS
FOR
ALL
MULTIPLE
POWER
DOWN
STATE
DEVICES
HOWEVER
IT
MAY
BE
POSSIBLE
TO
HAVE
A
COMPETITIVE
RATIO
LESS
THAN
FOR
A
SPECIFIC
DEVICE
DEPENDING
ON
THE
PARAMETERS
OF
THE
DEVICE
E
G
NUMBER
OF
STATES
POWER
DISSIPATION
RATES
START
UP
COSTS
AND
SO
ON
NOTE
THAT
THIS
DETERMINISTIC
ALGORITHM
FOCUSES
ON
A
SINGLE
IDLE
PERIOD
AND
DOES
NOT
DEPEND
ON
ANY
HISTORY
OF
PREVIOUS
IDLE
PERIODS
THUS
IT
IS
NONADAPTIVE
IN
THAT
IT
DOES
NOT
USE
ANY
INFORMATION
ABOUT
PREVIOUS
IDLE
PERIODS
TO
TUNE
ITS
BEHAVIOR
AS
WE
HAVE
ARGUED
ABOVE
COMPETITIVE
ANALYSIS
OFTEN
YIELDS
UNDULY
PES
SIMISTIC
RESULTS
INDEED
OUR
GOAL
IS
TO
DEVISE
A
DPM
STRATEGY
WHOSE
TOTAL
ENERGY
EXPENDITURE
IS
MUCH
LOWER
THAN
TWICE
THE
OPTIMAL
OFFLINE
ALGORITHM
WE
SHOW
THAT
BY
KEEPING
TRACK
OF
A
SEQUENCE
OF
PAST
IDLE
PERIODS
WE
CAN
IMPROVE
THE
PERFORMANCE
OF
THE
COMPETITIVE
DETERMINISTIC
STRATEGY
IN
PRACTICE
BY
ADAPTING
THE
BEHAVIOR
OF
THE
STRATEGY
TO
PATTERNS
OBSERVED
IN
THE
INPUT
SEQUENCE
WE
DO
THIS
IN
SECTION
BY
MODELING
THE
FUTURE
INPUT
BY
A
PROBABILITY
DISTRIBUTION
THAT
IS
LEARNED
FROM
THE
RECENT
HISTORY
OF
THE
LENGTH
OF
IDLE
PERIODS
IN
DOING
SO
WE
AVOID
SOME
OF
THE
PROBLEMS
WITH
MANY
OF
THE
PREVIOUS
STRATEGIES
MENTIONED
ABOVE
ONE
OF
THE
CHIEF
LIMITATIONS
OF
A
SINGLE
VALUED
PREDICTION
SVP
APPROACH
IS
THAT
IT
FAILS
TO
CAPTURE
UNCERTAINTY
IN
THE
PREDICTION
FOR
THE
UPCOMING
IDLE
PERIOD
LENGTH
FOR
EXAMPLE
IF
A
VERY
SHORT
IDLE
PERIOD
AND
A
VERY
LONG
IDLE
PERIOD
ARE
EQUALLY
LIKELY
THESE
METHODS
ARE
FORCED
TO
PICK
A
SINGLE
PREDICTION
AND
PAY
A
PENALTY
IN
THE
LIKELY
EVENT
THAT
THE
PREDICTION
IS
WRONG
USING
A
PROBABILITY
DISTRIBUTION
TO
MODEL
THE
LENGTH
OF
THE
UPCOMING
IDLE
PERIOD
ALLOWS
FOR
A
MUCH
RICHER
PREDICTION
SO
THAT
THE
ALGORITHM
CAN
OPTIMIZE
IN
A
WAY
THAT
TAKES
THE
NATURE
OF
THIS
ADDITIONAL
INFORMATION
INTO
ACCOUNT
FURTHERMORE
WE
MAKE
NO
ASSUMPTIONS
ABOUT
THE
FORM
OF
THE
DISTRIBUTION
GOVERNING
IDLE
PERIOD
LENGTH
WHICH
MEANS
THAT
OUR
METHOD
CAN
AUTOMATICALLY
ADAPT
TO
A
VARIETY
OF
APPLICATIONS
AND
ALSO
APPLIES
TO
NONSTATIONARY
INPUT
ARRIVALS
IN
SECTION
WE
DERIVE
ANALYTICAL
BOUNDS
FOR
AN
ONLINE
ALGORITHM
FOR
DPM
THAT
KNOWS
THE
PROBABILITY
DISTRIBUTION
GOVERNING
THE
LENGTH
OF
THE
UPCOMING
IDLE
PERIOD
SECTION
PROVIDES
A
SYSTEMATIC
MEANS
OF
DYNAMICALLY
CONSTRUCT
ING
AN
ESTIMATE
FOR
THE
PROBABILITY
DISTRIBUTION
BASED
ON
ONLINE
OBSERVATIONS
AND
COMBINING
IT
WITH
THE
ALGORITHM
FROM
THE
PREVIOUS
SECTION
TO
BUILD
AN
ONLINE
POWER
MANAGEMENT
STRATEGY
EXPERIMENTAL
RESULTS
IN
SECTION
DEMONSTRATE
THE
UTILITY
OF
OUR
STRATEGY
IN
BALANCING
POWER
USAGE
WITH
LATENCY
SYSTEM
MODEL
CONSIDER
A
DEVICE
THAT
CAN
BE
IN
ONE
OF
THE
K
POWER
STATES
DENOTED
BY
SK
THE
POWER
CONSUMPTION
FOR
STATE
I
IS
DENOTED
BY
ΑI
THE
STATES
ARE
ORDERED
SO
THAT
ΑI
Α
J
AS
LONG
AS
I
J
THUS
STATE
IS
THE
ACTIVE
STATE
THAT
IS
THE
HIGHEST
POWER
CONSUMPTION
STATE
ANY
STRATEGY
FOR
AN
INDIVIDUAL
IDLE
PERIOD
CAN
BE
DESCRIBED
BY
A
SEQUENCE
OF
THRESHOLDS
EACH
OF
WHICH
IS
ASSOCIATED
WITH
A
POWER
CONSUMPTION
STATE
AS
SOON
AS
THE
IDLE
PERIODS
EXTEND
BEYOND
A
GIVEN
THRESHOLD
THE
DEVICE
TRANSITIONS
TO
THE
ASSOCIATED
POWER
CONSUMPTION
STATE
FROM
THE
MANUFACTURER
SPECIFICATION
WE
ARE
ALSO
GIVEN
THE
TRANSITION
POWER
PIJ
AND
TRANSITION
TIMES
TIJ
TO
MOVE
FROM
STATE
SI
TO
SJ
USUALLY
THE
ENERGY
NEEDED
AND
TIME
SPENT
TO
GO
FROM
A
HIGHER
POWER
STATE
TO
A
LOWER
POWER
STATE
IS
NEGLIGIBLE
WHEREAS
THE
CONVERSE
IS
NOT
TRUE
THUS
WE
SIMPLIFY
THE
MODEL
BY
CONSIDERING
ONLY
THE
TIME
AND
POWER
NECESSARY
TO
POWER
UP
THE
SYSTEM
FUR
THERMORE
ALL
OF
THE
ALGORITHMS
CONSIDERED
IN
THIS
PAPER
HAVE
THE
PROPERTY
THAT
THEY
ONLY
TRANSITION
TO
THE
ACTIVE
STATE
WHEN
POWERING
UP
AND
NEVER
TRANSITION
TO
AN
INTERMEDIATE
HIGHER
POWERED
STATE
AS
A
RESULT
WE
ONLY
NEED
THE
TIME
AND
TOTAL
ENERGY
CONSUMED
IN
TRANSITIONING
UP
FROM
EACH
STATE
I
TO
THE
ACTIVE
STATE
STATE
THE
TOTAL
ENERGY
USED
IN
TRANSITIONING
FROM
STATE
I
TO
THE
ACTIVE
STATE
IS
DENOTED
BY
ΒI
WE
NOTE
THAT
IN
CASES
WHERE
THE
TIME
AND
ENERGY
COST
INCURRED
IN
TRANSI
TIONING
TO
LOWER
POWER
CONSUMPTION
STATES
IS
NONNEGLIGIBLE
THEY
CAN
BE
EASILY
INCORPORATED
BY
FOLDING
THEM
INTO
THE
CORRESPONDING
POWER
UP
PARAMETERS
THIS
CAN
BE
DONE
AS
LONG
AS
THE
TIME
AND
ENERGY
USED
IN
TRANSITIONING
DOWN
IS
ADDI
TIVE
THAT
IS
WE
REQUIRE
THAT
FOR
I
J
K
THE
COST
TO
GO
FROM
I
TO
J
AND
THEN
FROM
J
TO
K
IS
THE
SAME
AS
THE
COST
OF
GOING
FROM
I
DIRECTLY
DOWN
TO
K
THE
INPUT
TO
THE
DPM
ALGORITHM
IS
SIMPLY
A
SEQUENCE
OF
IDLE
PERIODS
FOR
EACH
IDLE
PERIOD
THE
DPM
IS
NOTIFIED
WHEN
THE
IDLE
PERIOD
BEGINS
AND
THEN
AGAIN
WHEN
THE
IDLE
PERIOD
ENDS
THIS
IS
THE
ONLY
INFORMATION
REQUIRED
BY
OUR
DPM
IN
OUR
EXPERIMENTS
THE
IDLE
PERIODS
ARE
DERIVED
BY
OUR
SIMULATOR
WHICH
RECEIVES
A
TIME
STAMPED
SEQUENCE
OF
REQUESTS
FOR
SERVICE
WITH
EACH
REQUEST
THE
SIMULATOR
IS
TOLD
THE
TIME
OF
ITS
ARRIVAL
AND
THE
LENGTH
OF
TIME
IT
WILL
TAKE
TO
SATISFY
THE
REQUEST
IF
THE
DEVICE
IS
BUSY
WHEN
A
NEW
REQUEST
ARRIVES
IT
ENTERS
A
QUEUE
AND
IS
SERVED
ON
A
FIRST
COME
FIRST
SERVE
BASIS
IN
THIS
CASE
THERE
IS
NO
IDLE
PERIOD
AND
THE
DEVICE
REMAINS
ACTIVE
THROUGH
THE
TIME
THAT
THE
REQUEST
IS
FINISHED
THIS
MEANS
THAT
THE
NUMBER
OF
IDLE
PERIODS
IS
GENERALLY
LESS
THAN
THE
NUMBER
OF
REQUESTS
SERVICED
WHENEVER
A
REQUEST
TERMINATES
AND
THERE
ARE
NO
OUTSTANDING
REQUESTS
WAITING
IN
THE
SYSTEM
AN
IDLE
PERIOD
BEGINS
IN
THESE
SITUATIONS
THE
DPM
IS
INVOKED
TO
DETERMINE
WHICH
POWER
CONSUMPTION
STATES
THE
DEVICE
SHOULD
TRANSITION
TO
AND
AT
WHAT
TIMES
IF
THE
DEVICE
IS
NOT
BUSY
WHEN
A
NEW
REQUEST
ARRIVES
IT
WILL
IMMEDIATELY
TRANSITION
TO
THE
ACTIVE
STATE
TO
SERVE
THE
NEW
REQUEST
IF
IT
IS
NOT
ALREADY
THERE
IN
THE
CASE
WHERE
THE
DEVICE
IS
NOT
ALREADY
IN
THE
ACTIVE
STATE
THE
REQUEST
CAN
NOT
BE
SERVICED
IMMEDIATELY
BUT
WILL
HAVE
TO
INCUR
SOME
LATENCY
IN
WAITING
FOR
THE
TRANSITION
TO
COMPLETE
THIS
DELAY
WILL
CAUSE
FUTURE
IDLE
PERIODS
TO
BE
SHORTER
IN
FACT
IF
A
REQUEST
IS
DELAYED
SOME
IDLE
PERIODS
MAY
DISAPPEAR
THUS
WE
HAVE
AN
INTERESTING
SITUATION
WHERE
THE
BEHAVIOR
OF
THE
ALGORITHM
AFFECTS
FUTURE
INPUTS
IDLE
PERIOD
LENGTHS
GIVEN
TO
THE
ALGORITHM
NOTE
ALSO
THAT
DELAYING
SERVICING
A
REQUEST
WILL
TEND
TO
RESULT
IN
LOWER
POWER
USAGE
FOR
INSTANCE
CONSIDER
THE
EXTREME
CASE
WHERE
THE
POWER
MANAGER
RE
MAINS
IN
THE
DEEPEST
SLEEP
STATE
WHILE
IT
WAITS
FOR
ALL
THE
REQUESTS
TO
ARRIVE
AND
THEN
PROCESSES
THEM
ALL
CONSECUTIVELY
THIS
EXTREME
CASE
IS
NOT
ALLOWED
IN
OUR
MODEL
SINCE
WE
REQUIRE
THAT
THE
STRATEGY
TRANSITION
TO
THE
ACTIVE
STATE
AND
BEGIN
TO
WORK
ON
A
REQUEST
AS
SOON
AS
ONE
APPEARS
HOWEVER
IT
ILLUSTRATES
THE
NATURAL
TRADE
OFF
THAT
OCCURS
BETWEEN
POWER
CONSUMPTION
AND
LATENCY
OUR
EXPERIMENTAL
RESULTS
EXPLORE
THIS
TRADE
OFF
FOR
THE
SET
OF
ALGORITHMS
STUDIED
A
MORE
EXTENSIVE
DISCUSSION
OF
THIS
TRADE
OFF
IS
PRESENTED
IN
RAMANATHAN
ET
AL
IN
THE
NEXT
TWO
SECTIONS
WE
PRESENT
OUR
ANALYSIS
FOR
THE
DETERMINISTIC
AND
PROBABILITY
BASED
ALGORITHMS
IN
THIS
ANALYSIS
WE
DO
NOT
TAKE
INTO
ACCOUNT
THE
DELAY
INCURRED
IN
RETURNING
TO
THE
ACTIVE
STATE
BECAUSE
THE
ANALYSIS
FOCUSES
ON
AN
INDIVIDUAL
IDLE
PERIOD
THESE
SECTIONS
ADDRESS
THE
PROBLEM
OF
MINIMIZING
TOTAL
ENERGY
EXPENDITURE
GIVEN
THAT
THE
LENGTH
OF
THE
UPCOMING
PERIOD
IS
AR
BITRARY
DETERMINISTIC
CASE
OR
IS
GOVERNED
BY
A
FIXED
PROBABILITY
DISTRIBUTION
PROBABILITY
BASED
CASE
THE
EMPIRICAL
EVALUATIONS
INCORPORATE
THE
EFFECT
OF
START
UP
LATENCY
AS
AN
ENTIRE
SEQUENCE
OF
REQUESTS
FOR
SERVICE
ARRIVE
THROUGH
TIME
THE
DETERMINISTIC
ONLINE
ALGORITHM
WE
NOW
PRESENT
OUR
DETERMINISTIC
ALGORITHM
FOR
ONLINE
DPM
THE
BASIC
IDEA
IS
THAT
THE
ONLINE
ALGORITHM
TRIES
TO
MIMIC
THE
BEHAVIOR
OF
THE
OPTIMAL
OFFLINE
ALGORITHM
AT
EACH
POINT
IN
TIME
T
THE
ALGORITHM
IS
IN
THE
STATE
THAT
THE
OPTIMAL
ALGORITHM
WOULD
HAVE
CHOSEN
IF
THE
LENGTH
OF
THE
IDLE
PERIOD
WERE
EXACTLY
T
TO
GET
THE
OPTIMAL
COST
WE
PLOT
EACH
LINE
C
ΑIT
ΒI
THIS
IS
THE
COST
OF
SPENDING
THE
ENTIRE
INTERVAL
IN
STATE
I
AS
A
FUNCTION
OF
T
THE
LENGTH
OF
THE
INTERVAL
TAKE
THE
LOWER
ENVELOPE
OF
ALL
OF
THESE
LINES
LET
US
CALL
THIS
FUNCTION
LE
T
THE
OPTIMAL
COST
FOR
AN
INTERVAL
OF
LENGTH
T
IS
LE
T
MINI
ΑIT
ΒI
THE
FIG
ENERGY
CONSUMPTION
FOR
EACH
STATE
FOR
A
FOUR
STATE
SYSTEM
EACH
STATE
IS
REPRESENTED
BY
A
LINE
THAT
INDICATES
THE
ENERGY
USED
IF
AN
ALGORITHM
STAYS
IN
THAT
STATE
AS
A
FUNCTION
OF
THE
LENGTH
OF
THE
IDLE
PERIOD
FOR
EACH
STATE
THE
SLOPE
IS
THE
POWER
DISSIPATION
RATE
AND
THE
Y
INTERCEPT
IS
THE
ENERGY
REQUIRED
TO
POWER
UP
FROM
THAT
STATE
ONLINE
ALGORITHM
CALLED
THE
LOWER
ENVELOPE
ALGORITHM
LEA
WILL
ALSO
FOLLOW
THE
FUNCTION
LE
IT
WILL
REMAIN
IN
THE
STATE
THAT
REALIZES
THE
MINIMUM
IN
LE
AND
WILL
TRANSITION
AT
THE
DISCONTINUITIES
OF
THE
CURVE
THAT
IS
LEA
WILL
REMAIN
IN
STATE
J
AS
LONG
AS
Α
J
T
Β
J
MINI
ΑIT
ΒI
FOR
THE
CURRENT
TIME
T
FOR
J
TO
K
LET
T
J
BE
THE
SOLUTION
TO
THE
EQUATION
Α
J
T
Β
J
Α
J
Β
J
WHERE
T
J
IS
THE
TIME
THAT
LEA
WILL
TRANSITION
FROM
STATE
J
TO
STATE
J
WE
WILL
ASSUME
HERE
THAT
WE
HAVE
THROWN
OUT
ALL
THE
LINES
THAT
DO
NOT
APPEAR
ON
THE
LOWER
ENVELOPE
AT
SOME
POINT
THIS
IS
EQUIVALENT
TO
THE
ASSUMPTION
THAT
TK
TK
SEE
FIGURE
THEOREM
THE
LOWER
ENVELOPE
ALGORITHM
IS
COMPETITIVE
THE
PROOF
OF
THEOREM
IS
SHOWN
IN
THE
APPENDIX
AS
EMPHASIZED
EARLIER
THIS
ALGORITHM
DOES
NOT
TAKE
INTO
ACCOUNT
INPUT
PAT
TERNS
THE
WORST
CASE
SCENARIO
OBTAINED
VIA
THEOREM
SHOWS
THAT
THE
ENERGY
COST
RESULTING
FROM
THE
ONLINE
DECISIONS
CAN
BE
NO
WORSE
THAN
TWO
TIMES
THE
ENERGY
COST
OF
THE
OPTIMAL
OFFLINE
STRATEGY
WHICH
KNOWS
THE
INPUT
SEQUENCE
IN
ADVANCE
WE
SHOW
LATER
THAT
DEPENDING
ON
REQUEST
ARRIVAL
PATTERNS
THIS
WORST
CASE
BOUND
MAY
NOT
REALLY
HAPPEN
AND
THE
EMPIRICAL
RATIO
OF
THE
ONLINE
TO
OFFLINE
COSTS
MAY
IN
FACT
BE
MUCH
LOWER
AS
SHOWN
IN
QIU
AND
PEDRAM
CHUNG
ET
AL
AND
BENINI
ET
AL
INPUT
SEQUENCES
ARE
OFTEN
INTERRE
LATED
AND
HENCE
MODELING
OF
THE
INPUT
PATTERN
AND
EXPLOITING
THAT
KNOWLEDGE
IN
THE
DESIGN
OF
THE
ALGORITHM
CAN
HELP
BRIDGE
THE
GAP
BETWEEN
THE
PERFOR
MANCE
OF
ONLINE
STRATEGY
AND
THAT
OF
THE
OPTIMAL
OFFLINE
STRATEGY
IN
THE
NEXT
SECTION
WE
DISCUSS
OUR
PROBABILITY
BASED
ALGORITHM
AND
SHOW
THAT
IF
THE
PROBA
BILITY
DISTRIBUTION
GOVERNING
THE
LENGTH
OF
THE
IDLE
PERIOD
IS
KNOWN
BEFORE
HAND
THE
WORST
CASE
COMPETITIVE
RATIO
CAN
BE
IMPROVED
BY
WITH
RESPECT
TO
THE
DETERMINISTIC
CASE
MOREOVER
WE
SHOW
THROUGH
EXPERIMENTAL
EVALUATION
THAT
THIS
WORST
CASE
BOUND
IS
PATHOLOGICAL
IN
FACT
WE
CAN
BRING
THE
ENERGY
COST
OF
THE
ONLINE
ALGORITHM
WITHIN
OF
THE
OPTIMAL
OFFLINE
ONE
THE
PROBABILITY
BASED
ONLINE
ALGORITHM
OPTIMIZING
POWER
BASED
ON
A
PROBABILITY
DISTRIBUTION
LET
US
ASSUME
THAT
THE
LENGTH
OF
THE
IDLE
INTERVAL
IS
GENERATED
BY
A
FIXED
KNOWN
DISTRIBUTION
WHOSE
DENSITY
FUNCTION
IS
Π
AND
DEVISE
A
METHOD
TO
OPTIMIZE
POWER
MANAGEMENT
DECISIONS
WE
FIRST
DISCUSS
SYSTEMS
WITH
TWO
STATES
AND
THEN
GIVE
OUR
GENERALIZATION
TO
THE
MULTISTATE
CASE
LET
Β
BE
THE
START
UP
ENERGY
OF
THE
SLEEP
STATE
AND
Α
THE
POWER
DISSIPATION
OF
THE
ACTIVE
STATE
SUPPOSE
THAT
THE
ONLINE
ALGORITHM
USES
Τ
AS
THE
THRESHOLD
AT
WHICH
TIME
IT
WILL
TRANSITION
FROM
THE
ACTIVE
STATE
TO
THE
SLEEP
STATE
IF
THE
SYSTEM
IS
STILL
IDLE
IN
THIS
CASE
THE
EXPECTED
ENERGY
COST
FOR
THE
ALGORITHM
FOR
A
SINGLE
IDLE
PERIOD
WILL
BE
Τ
Π
T
ΑT
DT
Π
T
ΑΤ
Β
DT
Τ
THE
BEST
ONLINE
ALGORITHM
WILL
SELECT
A
VALUE
FOR
Τ
THAT
MINIMIZES
THIS
EXPRES
SION
THE
OFFLINE
ALGORITHM
THAT
KNOWS
THE
ACTUAL
LENGTH
OF
AN
UPCOMING
IDLE
PERIOD
WILL
HAVE
AN
EXPECTED
COST
OF
Β
Α
Π
T
ΑT
DT
Β
Α
Π
T
Β
DT
IT
IS
KNOWN
FOR
THE
TWO
STATE
CASE
THAT
THE
ONLINE
ALGORITHM
CAN
PICK
ITS
THRESH
OLD
Τ
SO
THAT
THE
RATIO
OF
ITS
EXPECTED
COST
TO
THE
EXPECTED
COST
OF
THE
OPTIMAL
ALGORITHM
IS
AT
MOST
E
E
KARLIN
ET
AL
THAT
IS
FOR
ANY
Π
AND
ANY
Α
AND
Β
MINΤ
Τ
Π
T
Α
T
DT
Π
T
Τ
ΑΤ
Β
DT
E
Β
Α
Π
T
ΑT
DT
Β
Α
Π
T
Β
DT
E
THIS
IS
OPTIMAL
IN
THAT
FOR
ANY
Α
AND
Β
THERE
IS
A
DISTRIBUTION
Π
SUCH
THAT
THIS
RATIO
IS
AT
LEAST
E
E
LET
US
NOW
CONSIDER
THE
MULTISTATE
CASE
AS
IN
THE
PREVIOUS
SECTION
LET
T
J
BE
THE
SOLUTION
TO
THE
EQUATION
Α
J
T
Β
J
Α
J
Β
J
T
J
IS
THE
TIME
THAT
LEA
WILL
TRANSITION
FROM
STATE
J
TO
STATE
J
WE
ASSUME
HERE
THAT
WE
HAVE
THROWN
OUT
ALL
THE
LINES
THAT
DO
NOT
APPEAR
ON
THE
LOWER
ENVELOPE
AT
SOME
POINT
THIS
IS
EQUIVALENT
TO
THE
ASSUMPTION
THAT
TK
TK
FOR
EASE
OF
NOTATION
WE
DEFINE
TO
BE
AND
TK
TO
BE
THE
COST
EXPECTED
ENERGY
CONSUMPTION
OF
THE
OPTIMAL
OFFLINE
ALGORITHM
IS
I
TI
TI
Π
T
ΑIT
ΒI
DT
NOW
TO
DETERMINE
THE
ONLINE
ALGORITHM
WE
MUST
DETERMINE
K
THRESHOLDS
WHERE
THE
THRESHOLD
ΤI
IS
THE
TIME
AT
WHICH
THE
ONLINE
ALGORITHM
WILL
TRANSITION
FROM
STATE
I
TO
STATE
I
IN
THE
SPIRIT
OF
THE
DETERMINISTIC
ONLINE
ALGORITHM
FOR
THE
MULTISTATE
CASE
WE
LET
ΤI
BE
THE
SAME
AS
THE
THRESHOLD
WHICH
WOULD
BE
CHOSEN
IF
I
AND
I
WERE
THE
ONLY
TWO
STATES
WE
CALL
THIS
ALGORITHM
THE
PROBABILITY
BASED
LOWER
ENVELOPE
ALGORITHM
PLEA
THE
PROOF
OF
THE
FOLLOWING
THEOREM
APPEARS
IN
THE
APPENDIX
THEOREM
FOR
ANY
DISTRIBUTION
THE
EXPECTED
COST
OF
THE
PROBABILITY
BASED
LOWER
ENVELOPE
ALGORITHM
IS
WITHIN
A
FACTOR
OF
E
E
OF
THE
EXPECTED
COST
FOR
THE
OPTIMAL
OFFLINE
ALGORITHM
LEARNING
THE
PROBABILITY
DISTRIBUTION
WHILE
WE
HAVE
PROVEN
THE
COMPETITIVE
BOUNDS
REGARDLESS
OF
THE
CHOSEN
IDLE
TIME
PROBABILITY
DISTRIBUTION
THE
PRACTICAL
PROBLEM
OF
FINDING
Π
T
TO
GUIDE
THE
PLEA
ALGORITHM
REMAINS
OUR
APPROACH
IS
TO
LEARN
Π
T
ONLINE
ACCORDINGLY
THE
ALGORITHM
THAT
USES
PLEA
IN
CONJUNCTION
WITH
OUR
SCHEME
TO
LEARN
THE
PROBA
BILITY
DISTRIBUTION
IS
CALLED
THE
ONLINE
PROBABILITY
BASED
ALGORITHM
OPBA
IT
WORKS
AS
FOLLOWS
A
WINDOW
SIZE
W
IS
CHOSEN
IN
ADVANCE
AND
IS
USED
THROUGHOUT
THE
EXECUTION
OF
THE
ALGORITHM
THE
ALGORITHM
KEEPS
TRACK
OF
THE
LAST
W
IDLE
PERIOD
LENGTHS
AND
SUMMARIZES
THIS
INFORMATION
IN
A
HISTOGRAM
PERIODICALLY
THE
HISTOGRAM
IS
USED
TO
GENERATE
A
NEW
POWER
MANAGEMENT
STRATEGY
THE
SET
OF
ALL
POSSIBLE
IDLE
PERIOD
LENGTHS
IS
PARTITIONED
INTO
N
INTERVALS
WHERE
N
IS
THE
NUMBER
OF
BINS
IN
THE
HISTOGRAM
LET
RI
BE
THE
LEFT
ENDPOINT
OF
THE
ITH
INTERVAL
THE
ITH
BIN
HAS
A
COUNTER
THAT
INDICATES
THE
NUMBER
OF
IDLE
PERIODS
AMONG
THAT
LAST
W
IDLE
PERIODS
WHOSE
LENGTH
FELL
IN
THE
RANGE
RI
RI
THE
BINS
ARE
NUMBERED
FROM
THROUGH
N
AND
RN
THE
LAST
W
IDLE
PERIODS
ARE
HELD
IN
A
QUEUE
WHEN
A
NEW
IDLE
PERIOD
IS
COM
PLETED
THE
IDLE
PERIOD
AT
THE
HEAD
OF
THE
QUEUE
IS
DELETED
FROM
THE
QUEUE
IF
THIS
IDLE
PERIOD
FALLS
IN
BIN
I
THEN
THE
COUNTER
FOR
BIN
I
IS
DECREMENTED
THE
NEW
IDLE
PERIOD
IS
ADDED
TO
THE
TAIL
OF
THE
QUEUE
IF
THIS
IDLE
PERIOD
LENGTH
FALLS
INTO
BIN
J
THE
COUNTER
FOR
BIN
J
IS
INCREMENTED
THUS
THE
HISTOGRAM
ALWAYS
INCLUDES
DATA
FOR
THE
LAST
W
IDLE
PERIODS
OUR
EXPERIMENTAL
RESULTS
IN
SECTION
INCLUDE
A
STUDY
EXPERIMENTING
WITH
VALUES
FOR
W
THE
COUNTER
FOR
BIN
I
IS
DENOTED
BY
CI
THE
THRESHOLD
FOR
CHANGING
STATES
IS
SELECTED
AMONG
N
POSSIBILITIES
THAT
IS
RN
THE
LOWER
END
OF
EACH
RANGE
WE
ESTIMATE
THE
DISTRIBUTION
Π
BY
THE
DISTRIBUTION
THAT
GENERATES
AN
IDLE
PERIOD
OF
LENGTH
RI
WITH
PROBABILITY
CI
W
FOR
EACH
I
N
THE
SUM
OF
THE
COUNTERS
IS
THE
WINDOW
LENGTH
W
THUS
THE
THRESHOLD
IS
TAKEN
TO
BE
ARG
MIN
T
C
J
R
J
ΑI
ΑI
C
J
RT
ΑI
ΑI
ΒI
ΒI
RT
W
J
W
J
T
IN
ORDER
TO
BE
USEFUL
AS
A
DECISION
PROCEDURE
THIS
ALGORITHM
MUST
BE
IMPLE
MENTED
EFFICIENTLY
WE
HAVE
IMPLEMENTED
THE
ALGORITHM
FOR
FINDING
THE
ALL
K
THRESHOLDS
IN
TIME
O
KN
WHERE
K
IS
THE
NUMBER
OF
STATES
AND
N
IS
THE
TABLE
II
A
SNAPSHOT
OF
THE
HISTOGRAM
USED
IN
OPBA
NOTE
THAT
THE
OFFLINE
THRESHOLDS
ARE
AND
MS
THE
NUMBER
OF
BINS
PER
STATE
IS
NUMBER
OF
BINS
IN
THE
HISTOGRAM
TWO
IMPORTANT
FACTORS
DETERMINE
THE
COST
IN
TIME
EXPENDITURE
OF
IMPLEMENTING
OUR
METHOD
A
THE
FREQUENCY
WITH
WHICH
THE
THRESHOLDS
ARE
UPDATED
AND
B
THE
NUMBER
OF
BINS
IN
THE
HISTOGRAM
THESE
NEED
TO
BE
MINIMIZED
FOR
POLICY
IMPLEMENTATION
EFFICIENCY
THE
ALGO
RITHM
RUNS
THE
O
KN
TIME
ALGORITHM
EVERY
TIME
THE
THRESHOLDS
ARE
UPDATED
THE
FREQUENCY
WITH
WHICH
THE
THRESHOLDS
ARE
UPDATED
IS
THE
SUBJECT
OF
ONE
SET
OF
EXPERIMENTS
DISCUSSED
IN
SECTION
NOTE
THAT
THE
COST
TO
IMPLEMENT
OUR
STRATEGY
IS
INDEPENDENT
OF
W
THE
NUMBER
OF
IDLE
PERIODS
TRACKED
IN
THE
HISTOGRAM
MINIMIZATION
OF
THE
NUMBER
OF
BINS
USED
IN
THE
HISTOGRAM
MUST
BE
BALANCED
WITH
THE
FACT
THAT
THE
FINER
GRAINED
THE
HISTOGRAM
THE
MORE
ACCURATE
OUR
CHOICE
OF
THRESHOLDS
WILL
BE
OUR
EXPERIMENTS
AS
DISCUSSED
IN
THE
FOLLOWING
SECTIONS
SHOW
THAT
A
FINER
GRAINED
BINNING
IS
MORE
IMPORTANT
IN
SOME
RANGES
THAN
IT
IS
IN
OTHERS
A
WAY
OF
ADDRESSING
THIS
PROBLEM
KEY
TO
THE
SUCCESS
OF
THE
ALGORITHM
IS
TO
USE
THE
THRESHOLDS
OF
THE
LOWER
ENVELOPE
ALGORITHM
TO
GUIDE
THE
SELECTION
OF
THE
BINS
RECALL
THAT
THESE
THRESHOLDS
ARE
THE
TK
DEFINED
IN
SECTION
WE
THEN
CHOOSE
A
CONSTANT
C
NUMBER
OF
BINS
PER
STATE
THE
RANGE
FROM
TI
TO
TI
IS
DIVIDED
INTO
C
EQUAL
SIZED
BINS
NOTE
THAT
THE
RUNNING
TIME
OF
OUR
ALGORITHM
DEPENDS
LINEARLY
ON
THE
NUMBER
OF
BINS
AND
HENCE
DEPENDS
LINEARLY
ON
C
FOR
THIS
REASON
IT
IS
IMPORTANT
TO
SELECT
A
SMALL
VALUE
FOR
C
WE
EXPERIMENTED
WITH
VALUES
FOR
C
RANGING
FROM
TO
AND
THE
RESULTS
ONLY
VARIED
BY
WE
USE
A
VALUE
OF
C
IN
ALL
OUR
RESULTS
TABLE
II
SHOWS
A
SAMPLE
HISTOGRAM
FROM
OUR
EXPERIMENTS
TABLE
III
THIS
FIGURE
SHOWS
FOR
EACH
TRACE
THE
PERCENTAGE
OF
IDLE
PERIODS
FOR
WHICH
THE
OPTIMAL
ALGORITHM
CHOSE
TO
TRANSITION
TO
EACH
STATE
EXPERIMENTAL
DESIGN
DATA
USED
IN
OUR
EXPERIMENTS
TO
DEMONSTRATE
THE
UTILITY
OF
OUR
PROBABILITY
BASED
ALGORITHM
WE
USE
A
MOBILE
HARD
DRIVE
FROM
IBM
THIS
DRIVE
HAS
FOUR
POWER
DOWN
STATES
AS
SHOWN
IN
TABLE
I
HERE
THE
START
UP
ENERGY
REFERS
TO
THE
ENERGY
COST
IN
TRANSITIONING
FROM
A
STATE
TO
THE
ACTIVE
STATE
FOR
APPLICATION
DISK
ACCESS
DATA
WE
USED
TRACE
DATA
FROM
AUSPEX
FILE
SERVER
ARCHIVE
FROM
THIS
DATA
WE
COLLECTED
THE
ARRIVAL
TIMES
AND
LENGTHS
FOR
REQUESTS
FOR
DISK
ACCESS
FOR
MILLION
DISK
ACCESSES
DIVIDED
INTO
MULTIPLE
TRACE
FILES
CORRESPONDING
TO
DIFFERENT
HOURS
OF
THE
DAY
TABLE
III
GIVES
SOME
DATA
ON
THESE
TRACES
THE
FIRST
COLUMN
GIVES
THE
NUMBER
OF
REQUESTS
THE
SUBSEQUENT
COLUMNS
GIVE
INFORMATION
ABOUT
THE
BEHAVIOR
OF
THE
OPTIMAL
ALGORITHM
WHEN
RUN
ON
EACH
TRACE
SPECIFICALLY
THEY
SHOW
FOR
EACH
STATE
THE
PERCENTAGE
OF
IDLE
PERIODS
IN
WHICH
THE
OPTIMAL
ALGORITHM
TRANSITIONS
TO
THAT
STATE
IN
ALL
THE
TRACES
THERE
IS
A
HIGH
PERCENTAGE
OF
SHORT
SEQUENCES
FOR
WHICH
THE
OPTIMAL
STRATEGY
IS
TO
STAY
IN
THE
ACTIVE
STATE
SHOWN
IN
COLUMN
THE
REMAINING
PERCENTAGES
VARY
SOMEWHAT
FROM
TRACE
TO
TRACE
ALL
OF
THE
RESULTS
REPORTED
IN
THIS
PAPER
ARE
AN
AVERAGE
OF
THE
RESULTS
ON
EACH
INDIVIDUAL
TRACE
WEIGHTED
BY
LENGTH
ALGORITHM
TEST
SUITE
WE
COMPARE
OPBA
AND
LEA
TO
SEVERAL
OTHER
PREDICTIVE
ALGORITHMS
PRESENTED
IN
THE
LITERATURE
THE
ALGORITHMS
COME
IN
TWO
GROUPS
THE
ALGORITHMS
IN
THE
FIRST
GROUP
USE
A
SERIES
OF
THRESHOLDS
THAT
DETERMINE
WHEN
THE
ALGORITHM
WILL
TRANSI
TION
FROM
EACH
STATE
TO
THE
NEXT
LOWER
POWER
CONSUMPTION
STATE
OPBA
AND
LEA
FALL
INTO
THIS
GROUP
THE
SECOND
GROUP
IS
MADE
UP
OF
SINGLE
VALUED
PREDICTION
FILE
TRACES
FROM
THE
NOW
PROJECT
AVAILABLE
AT
TRACES
AUSPEX
HTML
TABLE
IV
PREDICTIONS
VERSUS
OUTCOMES
FOR
THE
EXPONENTIAL
DECAY
ALGORITHM
THE
TOTAL
NUMBER
OF
IDLE
PERIODS
IS
AND
THE
TOTAL
NUMBER
OF
REQUESTS
IS
ALGORITHMS
THEY
USE
A
SINGLE
PREDICTION
FOR
THE
LENGTH
OF
THE
UPCOMING
IDLE
PERIOD
AND
TO
TRANSITION
IMMEDIATELY
TO
THE
OPTIMAL
STATE
FOR
THAT
LENGTH
THEY
DIFFER
ONLY
IN
HOW
THEY
SELECT
A
PREDICTION
FOR
THE
LENGTH
OF
THE
NEXT
IDLE
PERIOD
OPTIMAL
OFFLINE
ALGORITHM
OPT
THIS
ALGORITHM
IS
ASSUMED
TO
KNOW
THE
LENGTH
OF
THE
IDLE
PERIOD
IN
ADVANCE
IT
SELECTS
THE
OPTIMAL
POWER
USAGE
STATE
FOR
THAT
IDLE
PERIOD
AND
THEN
TRANSITIONS
TO
THE
ACTIVE
STATE
BEFORE
THE
NEW
REQUEST
ARRIVES
IN
ORDER
TO
SERVICE
THE
INCOMING
REQUEST
JUST
AS
IT
ARRIVES
LAST
PERIOD
LAST
THIS
IS
A
SINGLE
VALUED
PREDICTION
ALGORITHM
THAT
USES
THE
LAST
PERIOD
AS
A
PREDICTOR
FOR
THE
NEXT
IDLE
PERIOD
EXPONENTIAL
DECAY
EXP
THIS
ALGORITHM
DEVELOPED
BY
HWANG
ET
AL
KEEPS
A
SINGLE
PREDICTION
FOR
THE
UPCOMING
IDLE
PERIOD
AFTER
A
NEW
IDLE
PERIOD
ENDS
THE
PREDICTION
IS
UPDATED
BY
TAKING
A
WEIGHTED
AVERAGE
OF
THE
OLD
PRE
DICTION
AND
THE
NEW
IDLE
PERIOD
LENGTH
LET
P
BE
THE
CURRENT
PREDICTION
AND
L
THE
LENGTH
OF
THE
LAST
IDLE
PERIOD
P
IS
UPDATED
AS
FOLLOWS
P
ΛP
Λ
L
WHERE
Λ
IS
A
VALUE
IN
THE
RANGE
WE
USE
A
VALUE
OF
FOR
Λ
ADAPTIVE
LEARNING
TREE
TREE
THIS
METHOD
USES
AN
ADAPTIVE
LEARNING
TREE
TO
PREDICT
THE
VALUE
OF
THE
NEXT
PERIOD
BASED
ON
THE
SEQUENCE
OF
RECENT
IDLE
PERIOD
LENGTHS
JUST
OBSERVED
DETAILS
OF
THIS
METHOD
CAN
BE
FOUND
IN
CHUNG
ET
AL
THERE
ARE
DIFFERENT
POSSIBLE
VERSIONS
OF
SINGLE
VALUED
PREDICTION
ALGORITHMS
WHICH
ARE
WORTH
MENTIONING
HERE
IN
DESCRIBING
THESE
VARIATIONS
WE
REFER
TO
THE
OFFLINE
THRESHOLDS
TK
DESCRIBED
IN
SECTION
THE
AUTHORS
OF
THE
LEARNING
TREE
ALGORITHM
OBSERVE
THAT
THERE
ARE
MANY
IDLE
PERIODS
THAT
ARE
VERY
SHORT
IN
ORDER
TO
AVOID
TRANSITIONING
TO
A
LOWER
POWERED
STATE
FOR
SUCH
SHORT
IDLE
PERIODS
THEY
KEEP
THEIR
SYSTEM
IN
THE
ACTIVE
STATE
UNTIL
THE
FIRST
OFFLINE
THRESHOLD
HAS
BEEN
PASSED
ONLY
THEN
DO
THEY
TRANSITION
TO
THE
PREDICTED
OPTIMAL
STATE
WE
RAN
ALL
SINGLE
VALUED
PREDICTION
ALGORITHMS
WITH
AND
WITHOUT
THIS
INITIAL
DELAY
WE
FOUND
THAT
THE
RESULTS
WERE
NOT
SIGNIFICANTLY
DIFFERENT
BETWEEN
THE
TWO
VERSIONS
IN
THIS
PAPER
WE
ONLY
REPORT
RESULTS
FOR
THE
VERSIONS
WITHOUT
THIS
DELAY
TO
SEE
WHY
THE
DELAY
DOES
NOT
HELP
SIGNIFICANTLY
WE
GIVE
SOME
STATISTICS
FOR
THE
EXPONENTIAL
DECAY
ALGORITHM
WHEN
RUN
WITHOUT
THE
DELAY
IN
TABLE
IV
AN
ENTRY
IN
ROW
I
COLUMN
J
INDICATES
THE
TOTAL
NUMBER
OF
TIMES
OVER
ALL
TRACES
THAT
THE
ONLINE
ALGORITHM
PREDICTED
THAT
THE
OPTIMAL
STATE
FOR
AN
UPCOMING
IDLE
PERIOD
WOULD
BE
J
AND
THE
ACTUAL
OPTIMAL
STATE
WAS
I
HAVING
A
DELAY
UNTIL
TIME
HELPS
ONLY
WHEN
THE
ALGORITHM
PREDICTS
THAT
THE
OPTIMAL
STATE
IS
STAND
BY
IDLE
OR
SLEEP
AND
THE
ACTUAL
OPTIMAL
STATE
IS
ACTIVE
THE
NUMER
OF
TIMES
THIS
HAPPENS
IS
THE
SUM
OF
THE
VALUES
IN
ROW
ACTIVE
COLUMNS
STAND
BY
IDLE
AND
SLEEP
THE
DEGREE
OF
SAVINGS
IN
LATENCY
IS
THE
LARGEST
FOR
IDLE
PERIODS
COUNTED
IN
ENTRY
ACTIVE
SLEEP
LESS
SO
FOR
ENTRY
ACTIVE
STAND
BY
AND
EVEN
LESS
FOR
ACTIVE
IDLE
GIVEN
THE
VALUES
IN
THE
TABLE
IT
IS
CLEAR
WHY
HAVING
A
DELAY
DOES
NOT
OFFER
A
SIGNIFICANT
SAVING
IN
THE
TOTAL
LATENCY
ANOTHER
FEATURE
THAT
THE
AUTHORS
OF
THE
LEARNING
TREE
ALGORITHM
EMPLOY
IS
TO
TRANSITION
TO
THE
ACTIVE
STATE
AFTER
THE
THRESHOLD
FOR
THE
PREDICTED
OPTIMAL
STATE
IS
REACHED
THIS
IS
DONE
IN
A
HOPE
THAT
A
JOB
WILL
ARRIVE
SHORTLY
THERE
AFTER
AND
THE
SYSTEM
CAN
AVOID
INCURRING
ANY
ADDITIONAL
LATENCY
IN
POWERING
UP
AFTER
THE
NEW
JOB
HAS
ARRIVED
IF
NO
JOB
ARRIVES
BEFORE
THE
LAST
THRESHOLD
TK
THEN
THE
ALGORITHM
TRANSITIONS
TO
THE
LOWEST
SLEEP
STATE
TO
SUMMARIZE
IF
THERE
ARE
K
STATES
AND
THE
PREDICTION
IS
THAT
STATE
I
WILL
BE
THE
OPTIMAL
STATE
FOR
THE
UPCOMING
IDLE
PERIOD
THE
ALGORITHM
WILL
TRANSITION
IMMEDIATELY
TO
STATE
I
IF
A
NEW
REQUEST
HAS
NOT
ARRIVED
BY
TIME
TI
THE
ALGORITHM
WILL
TRAN
SITION
BACK
TO
THE
ACTIVE
STATE
STATE
FINALLY
IF
A
REQUEST
HAS
NOT
ARRIVED
BY
TIME
TK
THE
ALGORITHM
WILL
TRANSITION
TO
THE
DEEPEST
SLEEP
STATE
STATE
K
WE
CALL
THIS
VERSION
OF
SINGLE
VALUED
PREDICTION
ALGORITHMS
THE
PREEMPTIVE
WAKE
UP
VERSION
THE
ALTERNATIVE
PREEMPTIVE
WAKE
UP
IS
TO
TRANSITION
IMMEDIATELY
TO
STATE
I
IF
THAT
IS
THE
PREDICTED
OPTIMAL
STATE
AND
THEN
TO
TRANSITION
DIRECTLY
TO
THE
DEEPEST
SLEEP
STATE
IF
A
REQUEST
HAS
NOT
ARRIVED
BY
TIME
TI
WE
CALL
THIS
VERSION
THE
NON
PREEMPTIVE
WAKE
UP
VERSION
NATURALLY
THE
PREEMPTIVE
WAKE
UP
VERSION
WILL
USE
MORE
POWER
BUT
WILL
TEND
TO
INCUR
LESS
LATENCY
ON
AVERAGE
WE
REPORT
RESULTS
FOR
BOTH
VERSIONS
OF
ALL
THE
SINGLE
VALUED
PREDICTIVE
ALGORITHMS
USED
IN
THE
STUDY
EXPERIMENTAL
RESULTS
EXPERIMENTATION
WITH
WINDOW
SIZE
FIGURE
SHOWS
THE
AVERAGE
ENERGY
CONSUMED
PER
REQUEST
AS
THE
WINDOW
SIZE
IS
VARIED
NOTE
THAT
BEYOND
A
CERTAIN
THRESHOLD
WINDOW
SIZE
BELOW
WHICH
THE
PREDICTIONS
ARE
NOT
ACCURATE
ANYWAY
THE
VARIATION
OF
ENERGY
CONSUMPTION
WITH
RESPECT
TO
THE
INCREASE
IN
WINDOW
SIZE
IS
NOT
LARGE
INDICATING
THAT
OUR
METHOD
IS
FAIRLY
ROBUST
TO
CHOICES
IN
WINDOW
SIZE
OUR
METHOD
PERFORMS
BEST
WITH
A
RELATIVELY
SMALL
VALUE
FOR
THE
WINDOW
SIZE
INDICATING
THAT
IT
IS
THE
MOST
RECENT
HISTORY
THAT
IS
THE
MOST
RELEVANT
PREDICTING
UPCOMING
IDLE
PERIOD
LENGTH
IT
ALSO
INDICATES
THAT
THE
DISTRIBUTION
OVER
IDLE
PERIOD
LENGTHS
IS
NOT
NECESSARILY
STABLE
OVER
TIME
HOWEVER
THE
RESULTS
GET
WORSE
IF
THE
WINDOW
SIZE
GETS
BELOW
SHOWING
THAT
THERE
NEED
TO
BE
ENOUGH
VALUES
TO
GET
A
REPRESENTATIVE
SAMPLE
WE
SELECTED
A
VALUE
OF
FOR
THE
WINDOW
SIZE
THAT
IS
USED
FOR
THE
REMAINDER
OF
THE
RESULTS
PRESENTED
EXPERIMENTATION
WITH
THRESHOLD
UPDATE
FREQUENCY
FIGURE
SHOWS
THE
AVERAGE
ENERGY
CONSUMED
PER
IDLE
PERIOD
AS
THE
FREQUENCY
OF
UPDATING
THE
THRESHOLDS
IS
VARIED
AS
ONE
WOULD
EXPECT
AS
THE
INTERVAL
BETWEEN
FIG
AVERAGE
ENERGY
CONSUMED
PER
REQUEST
AS
A
FUNCTION
OF
WINDOW
SIZE
FOR
THE
ONLINE
PROBABILITY
BASED
ALGORITHM
THE
THRESHOLDS
ARE
UPDATED
EVERY
TEN
REQUESTS
FIG
AVERAGE
ENERGY
CONSUMED
PER
REQUEST
AS
A
FUNCTION
OF
FREQUENCY
OF
UPDATE
FOR
THE
ONLINE
PROBABILITY
BASED
ALGORITHM
THE
WINDOW
SIZE
IS
UPDATES
GROWS
SO
DOES
THE
POWER
USAGE
HOWEVER
THERE
DO
NOT
TO
SEEM
TO
BE
LARGE
DIFFERENCES
IN
THE
COST
SO
WE
ADOPT
A
FREQUENCY
OF
UPDATE
OF
EVALUATION
OF
ALGORITHM
PERFORMANCE
TABLE
V
SHOWS
THE
COMPARISON
OF
ENERGY
CONSUMPTION
AND
WAKE
UP
LATENCY
EF
FECTS
ACROSS
A
NUMBER
OF
PREDICTIVE
DPM
ALGORITHMS
THE
FIRST
COLUMN
OF
NUM
BERS
IN
TABLE
V
IS
THE
AVERAGE
ENERGY
USED
PER
REQUEST
FOR
EACH
OF
THE
ALGORITHMS
THE
SECOND
COLUMN
IS
THE
RATIO
OF
THIS
FIGURE
TO
THE
AVERAGE
ENERGY
CONSUMED
PER
REQUEST
BY
THE
OPTIMAL
OFFLINE
ALGORITHM
INTERESTINGLY
THERE
WERE
SOME
TRACES
WHERE
THIS
RATIO
IS
LESS
THAN
ALTHOUGH
THEY
ALWAYS
AVERAGED
OUT
TO
BE
GREATER
THAN
OVER
ALL
THE
TRACES
THE
REASON
IT
IS
POSSIBLE
FOR
AN
ALGORITHM
TO
BE
BETTER
THAN
THE
OPTIMAL
OFFLINE
ALGORITHM
ON
POWER
CONSUMPTION
IS
BECAUSE
TABLE
V
ENERGY
MEASURED
IN
JOULES
AND
LATENCY
MEASURED
IN
MILLISECONDS
THE
OPTIMAL
ALGORITHM
IS
ALWAYS
A
FORCED
WAKE
UP
PREEMPTIVELY
BEFORE
A
REQUEST
ARRIVES
THIS
MEANS
THAT
THE
OPTIMAL
ALGORITHM
INCURS
NO
ADDITIONAL
LATENCY
DUE
TO
WAKING
UP
THE
DISK
DRIVE
RECALL
THAT
SINCE
THERE
IS
A
POWER
LATENCY
TRADE
OFF
THIS
WILL
TEND
TO
PENALIZE
THE
OPTIMAL
ALGORITHM
WITH
RESPECT
TO
POWER
USAGE
THE
AVERAGE
LATENCY
PER
REQUEST
IS
SHOWN
IN
THE
FINAL
COLUMN
OF
THE
FIGURE
THE
LATENCY
IS
AVERAGED
OVER
THE
NUMBER
OF
REQUESTS
NOT
THE
NUMBER
OF
IDLE
PERIODS
THESE
TWO
NUMBERS
ARE
DIFFERENT
SINCE
SOME
REQUESTS
MAY
ARRIVE
WHILE
THE
DEVICE
IS
BUSY
WORKING
ON
OTHER
JOBS
OR
POWERING
UP
IN
THESE
CASES
THE
INCOMING
REQUEST
WOULD
NOT
CORRESPOND
TO
AN
IDLE
PERIOD
AS
AN
EXAMPLE
WE
REFER
BACK
TO
THE
STATISTICS
FOR
THE
EXPONENTIAL
DELAY
ALGORITHM
IN
TABLE
IV
THE
TOTAL
NUMBER
OF
IDLE
PERIODS
OVER
ALL
TRACES
IS
WHEREAS
THE
TOTAL
NUMBER
OF
REQUESTS
IS
NOTE
THAT
THE
ALGORITHM
ONLY
EXPERIENCES
THE
FULL
OF
TRANSITION
TIME
WHEN
A
REQUEST
ARRIVES
AND
THE
ALGORITHM
IS
IN
THE
SLEEP
STATE
REFER
TO
TABLE
I
FOR
THE
TRANSITION
TIMES
FOR
THE
VARIOUS
STATES
FOR
SINGLE
VALUE
PREDICTION
ALGORITHMS
WITHOUT
PREDICTIVE
WAKE
UP
THIS
ONLY
HAPPENS
IF
THE
ALGORITHM
PREDICTS
THAT
THE
SLEEP
STATE
WILL
BE
THE
OPTIMAL
STATE
FOR
THE
UPCOMING
IDLE
PERIOD
OR
WHEN
THE
IDLE
PERIOD
ACTUALLY
LASTS
SO
LONG
THAT
THE
ALGORITHM
DISCOVERS
THAT
THE
SLEEP
STATE
WAS
IN
FACT
THE
OPTIMAL
STATE
THE
NUMBER
OF
TIMES
THIS
HAPPENS
IS
THE
SUM
OF
THE
NUMBERS
IN
ENTRIES
WHICH
ARE
EITHER
IN
COLUMN
SLEEP
OR
ROW
SLEEP
OR
BOTH
THE
ALGORITHM
ONLY
EXPERIENCES
THE
DELAY
IN
TRANSITIONING
FROM
THE
STAND
BY
STATE
TO
THE
ACTIVE
STATE
FOR
THOSE
IDLE
PERIODS
COUNTED
IN
ENTRIES
ACTIVE
STAND
BY
IDLE
STAND
BY
AND
STAND
BY
STAND
BY
THE
FINAL
RESULTS
IN
TABLE
V
ARE
ALSO
SHOWN
GRAPHICALLY
IN
FIGURE
WHICH
PLOTS
THE
POWER
USAGE
MIDDLE
COLUMN
FROM
TABLE
V
AGAINST
THE
AVERAGE
LATENCY
LAST
COLUMN
FROM
TABLE
V
THE
OPBA
EXHIBITS
THE
LOWEST
POWER
CONSUMPTION
AMONG
ALL
THE
ONLINE
ALGORITHMS
THE
OTHER
ALGORITHMS
THAT
COME
CLOSE
TO
MATCH
ING
ITS
PERFORMANCE
IN
POWER
THE
NONPREEMPTIVE
VERSIONS
OF
LAST
TREE
AND
EXP
ALL
SUFFER
AT
LEAST
AN
ADDITIONAL
LATENCY
ON
AVERAGE
MEANWHILE
THE
ALGORITHMS
THAT
HAVE
A
LOWER
AVERAGE
LATENCY
THAN
OPBA
LEA
AND
THE
PRE
EMPTIVE
VERSIONS
OF
LAST
TREE
AND
EXP
ALL
USE
AT
LEAST
MORE
POWER
ON
AVERAGE
THUS
OPBA
IS
THE
MOST
SUCCESSFUL
ALGORITHM
IN
BALANCING
POWER
USAGE
AS
WELL
AS
LATENCY
INCURRED
FINALLY
WE
STUDY
THE
THE
EFFECT
OF
VARIABLE
WAKE
UP
TIME
SINCE
MANY
DEVICES
VARY
SOMEWHAT
IN
THE
TIME
IT
TAKES
TO
TRANSITION
FROM
ONE
STATE
TO
ANOTHER
FIG
ENERGY
IS
MEASURED
IN
JOULES
AND
LATENCY
IS
MEASURED
IN
MILLISECONDS
TABLE
VI
LATENCY
AS
VARIATION
IN
TRANSITION
TIME
INCREASES
EACH
TRANSITION
TIME
IS
MULTIPLIED
BY
A
NUMBER
GENERATED
UNFORMLY
AT
RANDOM
FROM
THE
RANGE
Ε
Ε
LATENCY
IS
MEASURED
IN
MILLISECONDS
WE
STUDY
THE
TOP
PERFORMERS
FOR
LATENCY
AND
POWER
WHEN
RANDOM
NOISE
IS
ADDED
TO
THE
TIME
TO
TRANSITION
TO
THE
ACTIVE
STATE
EVERY
TIME
A
TRANSITION
IS
PERFORMED
WE
MULTIPLY
THE
TIME
TO
TRANSITION
BY
A
NUMBER
GENERATED
UNFORMLY
AT
RANDOM
FROM
THE
RANGE
Ε
Ε
FOR
DIFFERENT
VALUES
OF
E
THE
RESULTS
ARE
SHOWN
IN
TABLE
VI
WE
DID
NOT
INCLUDE
THE
POWER
USAGE
BECAUSE
IT
DOES
NOT
VARY
SIGNIF
ICANTLY
FOR
DIFFERENT
VALUES
OF
Ε
ALL
OF
THE
METHODS
SUFFER
A
SLIGHT
DEGRADATION
IN
PERFORMANCE
AS
E
INCREASES
THE
VARIATION
SEEMS
TO
HAVE
A
SIMILAR
EFFECT
ON
ALL
ALGORITHMS
STUDIED
CONCLUSION
AND
FUTURE
DIRECTIONS
WE
HAVE
PRESENTED
A
DETERMINISTIC
ONLINE
ALGORITHM
FOR
DYNAMIC
POWER
MAN
AGEMENT
ON
MULTISTATE
DEVICES
AND
PROVED
THAT
IT
IS
COMPETITIVE
AND
THAT
THIS
BOUND
IS
TIGHT
WE
IMPROVE
UPON
THIS
BOUND
CONSIDERABLY
BY
DEVISING
A
PROBABILITY
BASED
SCHEME
TO
SUPPORT
THIS
STRATEGY
IN
AN
ONLINE
DPM
FRAME
WORK
WE
PROVIDE
A
METHOD
TO
EFFICIENTLY
CONSTRUCT
A
PROBABILISTIC
MODEL
FOR
THE
LENGTH
OF
THE
UPCOMING
IDLE
PERIOD
BASED
ON
ONLINE
OBSERVATIONS
OUR
EXPERI
MENTS
SHOW
THAT
THE
ALGORITHM
PRESENTED
IN
THIS
PAPER
ATTAINS
THE
BEST
PER
FORMANCE
IN
PRACTICE
IN
COMPARISON
TO
OTHER
KNOWN
PREDICTIVE
DPM
ALGORITHM
THE
OTHER
ALGORITHMS
THAT
COME
CLOSE
TO
MATCHING
ITS
PERFORMANCE
IN
POWER
ALL
SUFFER
AT
LEAST
AN
ADDITIONAL
WAKE
UP
LATENCY
ON
AVERAGE
MEANWHILE
THE
ALGORITHMS
THAT
HAVE
COMPARABLE
LATENCY
TO
OUR
METHODS
ALL
USE
AT
LEAST
MORE
POWER
ON
AVERAGE
OUR
FUTURE
PLANS
INCLUDE
EXTENSION
OF
THIS
WORK
INTO
SYSTEMS
WITH
MULTIPLE
ACTIVE
AS
WELL
AS
POWER
DOWN
STATES
OUR
EXPERIMENTAL
FRAMEWORK
IS
AVAILABLE
ON
OUR
FOR
TESTING
A
RANGE
OF
ONLINE
DPM
ALGORITHMS
USING
A
JAVA
APPLET
INTERFACE
THE
INTERFACE
ALLOWS
USERS
TO
UPLOAD
THEIR
DATA
AND
EVALUATE
OUR
ALGORITHMS
AND
OTHER
KNOWN
ALGO
RITHMS
THAT
WE
HAVE
IMPLEMENTED
IN
OUR
SIMULATION
FRAMEWORK
APPENDIX
PROOF
OF
THEOREM
FIRST
WE
ESTABLISH
THAT
THE
WORST
CASE
FOR
THE
ALGORITHM
WILL
ALWAYS
BE
JUST
AFTER
A
TRANSITION
TIME
CONSIDER
THE
TIME
T
J
Γ
FOR
SOME
J
K
AND
Γ
T
J
T
J
FOR
ANY
VALUE
OF
Γ
IN
THE
GIVEN
RANGE
THE
OPTIMAL
COST
WILL
BE
Α
J
T
J
Γ
Β
J
FOR
ANY
VALUE
OF
Γ
IN
THE
GIVEN
RANGE
THE
ONLINE
COST
WILL
BE
J
ΑL
TL
TL
Α
J
Γ
Β
J
L
THE
RATIO
OF
THESE
TWO
WILL
BE
MAXIMIZED
FOR
Γ
NOW
SUPPOSE
THAT
THE
INTERVAL
ENDS
JUST
AFTER
T
J
FOR
SOME
J
K
USING
THE
COST
FOR
THE
ONLINE
AND
OFFLINE
DETERMINED
ABOVE
THE
RATIO
OF
THE
ONLINE
COST
TO
THE
OFFLINE
COST
WILL
BE
J
ΑL
TL
TL
Β
J
L
Α
J
T
J
Β
J
THUS
IT
IS
SUFFICIENT
TO
PROVE
THAT
J
J
ΑL
TL
TL
Α
J
T
J
L
Α
J
T
J
Β
J
ΑL
TL
TL
Α
J
T
J
Α
J
T
J
Β
J
L
EACH
TL
WAS
CHOSEN
SO
THAT
ΑL
ΑL
TL
ΒL
ΒL
SO
WE
CAN
SUBSTITUTE
THESE
VALUES
INTO
INEQUALITY
TO
GET
THAT
Β
J
Β
J
Α
J
T
J
Β
J
COLLAPSING
THE
TELESCOPING
SUM
WE
GET
THAT
Β
J
Α
J
T
J
Β
J
WEBSITE
AT
UC
IRVINE
JAVA
APPLET
BASED
DPM
STRATEGY
EVALUATION
WEBSITE
USING
THE
FACT
THAT
AND
THAT
Α
J
AND
T
J
THE
INEQUALITY
HOLDS
PROOF
OF
THEOREM
CONSIDER
A
SYSTEM
IN
WHICH
THERE
ARE
ONLY
TWO
STATES
I
AND
I
BOTH
ONLINE
AND
OFFLINE
MUST
PAY
AT
LEAST
ΑIT
FOR
AN
INTERVAL
OF
LENGTH
T
IN
ADDITION
EACH
MUST
PAY
AT
LEAST
ΒI
FOR
THE
START
UP
COST
THESE
COSTS
WHICH
ARE
INCURRED
BY
BOTH
ALGORITHMS
REGARDLESS
OF
THEIR
CHOICES
WILL
ONLY
SERVE
TO
DECREASE
THE
COMPETITIVE
RATIO
IN
DETERMINING
ΤI
WE
DISREGARD
THESE
ADDITIONAL
COSTS
CONSIDER
THE
SYSTEM
WHERE
THE
POWER
CONSUMPTION
RATE
IN
THE
ON
STATE
IS
ΑI
ΑI
AND
IS
IN
THE
OFF
STATE
THE
ENERGY
REQUIRED
TO
TRANSITION
FROM
THE
ON
TO
THE
OFF
STATE
IS
ΒI
ΒI
WE
CHOOSE
ΤI
TO
THE
BE
THE
TRANSITION
TIME
FOR
THE
OPTIMAL
ONLINE
POLICY
IN
THIS
SYSTEM
THUS
WE
CHOOSE
ΤI
TO
BE
ARG
MIN
R
Τ
T
T
DT
R
T
DT
THE
ONLINE
COST
FOR
THIS
NEW
SYSTEM
IS
THE
ABOVE
EXPRESSION
EVALUATED
AT
Τ
ΤI
ONI
ΤI
Π
T
T
ΑI
ΑI
DT
Π
T
ΤI
ΤI
ΑI
ΑI
ΒI
ΒI
DT
LET
TI
BE
DEFINED
TO
BE
ΒI
ΒI
ΑI
ΑI
NOTE
THAT
THIS
IS
THE
SAME
DEFINITION
IN
THE
PREVIOUS
PROOF
THE
POINT
WHERE
THE
LINES
ΑIT
ΒI
AND
ΑI
ΒI
MEET
THE
OFFLINE
COST
FOR
THE
NEW
SYSTEM
IS
OFFI
TI
Π
T
T
ΑI
ΑI
DT
Π
T
TI
ΒI
ΒI
DT
WE
ARE
GUARANTEED
THAT
THE
RATIO
OF
THE
EXPECTED
ONLINE
TO
OFFLINE
COSTS
IS
AT
MOST
E
E
KARLIN
ET
AL
KESHAV
ET
AL
SINCE
THE
RATIO
OF
ONI
TO
OFFI
IS
AT
MOST
E
E
FOR
EACH
I
WE
KNOW
THAT
K
ONI
I
OFFI
E
E
WE
NOW
PROVE
THAT
K
I
ONI
IS
EXACTLY
THE
EXPECTED
COST
FOR
PLEA
ON
THE
MUL
TILEVEL
SYSTEM
WE
ALSO
PROVE
THAT
K
OFFI
IS
EXACTLY
THE
EXPECTED
COST
OF
THE
OPTIMAL
ALGORITHM
FOR
THE
MULTILEVEL
SYSTEM
WE
REPHRASE
ONI
BY
SEPARATING
THE
INTEGRAL
INTO
THE
INTERVALS
FROM
Τ
J
TO
Τ
J
TO
SIMPLIFY
NOTATION
DENOTES
AND
ΤK
DENOTES
ONI
J
Τ
J
Τ
J
Π
T
T
ΑI
ΑI
DT
K
J
I
Τ
J
Τ
J
Π
T
ΤI
ΑI
ΑI
ΒI
ΒI
DT
IN
THE
SUM
OVER
ALL
ONI
WE
GROUP
TOGETHER
ALL
THE
CONTRIBUTIONS
FROM
EACH
ONI
OVER
THE
INTERVAL
Τ
J
Τ
J
FOR
J
K
NOTE
THAT
THIS
IS
THE
INTERVAL
THAT
THE
ALGORITHM
WILL
SPEND
IN
STATE
J
THIS
VALUE
IS
I
Τ
J
Τ
J
Π
T
ΤI
ΑI
ΑI
ΒI
ΒI
DT
I
J
Τ
J
Τ
J
Π
T
T
ΑI
ΑI
DT
THUS
WE
HAVE
THAT
K
K
ONI
F
J
WHERE
I
J
R
Τ
J
I
J
Τ
J
Τ
J
Π
T
T
ΑI
ΑI
DT
PUTTING
THE
SUMMATIONS
INSIDE
THE
INTEGRALS
AND
COLLAPSING
THE
TELESCOPING
SUMS
THE
EXPRESSION
IN
BECOMES
WHERE
Π
T
COST
T
DT
Τ
J
J
NOTE
THAT
COST
T
Β
J
ΤL
ΤL
ΑL
T
ΤL
ΑL
L
J
Β
J
ΤL
ΤL
ΑL
T
ΤL
ΑL
L
IS
EXACTLY
THE
ENERGY
EXPENDED
BY
PLEA
IF
THE
IDLE
PERIOD
T
IS
IN
THE
RANGE
Τ
J
Τ
J
THUS
THE
EXPECTED
COST
FOR
PLEA
IS
J
Τ
J
Τ
J
Π
T
COST
T
DT
I
ONI
THE
PROOF
THAT
THE
EXPECTED
OFFLINE
COST
IS
EQUAL
TO
K
OFFI
IS
THE
SAME
AS
THE
PROOF
FOR
THE
ONLINE
COST
EXCEPT
THAT
THE
INTEGRALS
AR
E
SEPARATED
INTO
INTERVALS
ACCORDING
TO
THE
TI
INSTEAD
OF
THE
ΤI
POWERNAP
ELIMINATING
SERVER
IDLE
POWER
DAVID
MEISNER
BRIAN
T
GOLD
THOMAS
F
WENISCH
ADVANCED
COMPUTER
ARCHITECTURE
LAB
COMPUTER
ARCHITECTURE
LAB
THE
UNIVERSITY
OF
MICHIGAN
CARNEGIE
MELLON
UNIVERSITY
ABSTRACT
DATA
CENTER
POWER
CONSUMPTION
IS
GROWING
TO
UNPRECE
DENTED
LEVELS
THE
EPA
ESTIMATES
U
DATA
CENTERS
WILL
CON
SUME
BILLION
KILOWATT
HOURS
ANNUALLY
BY
MUCH
OF
THIS
ENERGY
IS
WASTED
IN
IDLE
SYSTEMS
IN
TYPICAL
DEPLOYMENTS
SERVER
UTILIZATION
IS
BELOW
BUT
IDLE
SERVERS
STILL
CON
SUME
OF
THEIR
PEAK
POWER
DRAW
TYPICAL
IDLE
PERIODS
THOUGH
FREQUENT
LAST
SECONDS
OR
LESS
CONFOUNDING
SIMPLE
ENERGY
CONSERVATION
APPROACHES
IN
THIS
PAPER
WE
PROPOSE
POWERNAP
AN
ENERGY
CONSERVATION
APPROACH
WHERE
THE
ENTIRE
SYSTEM
TRANSITIONS
RAPIDLY
BE
TWEEN
A
HIGH
PERFORMANCE
ACTIVE
STATE
AND
A
NEAR
ZERO
POWER
IDLE
STATE
IN
RESPONSE
TO
INSTANTANEOUS
LOAD
RATHER
THAN
REQUIRING
FINE
GRAINED
POWER
PERFORMANCE
STATES
AND
COMPLEX
LOAD
PROPORTIONAL
OPERATION
FROM
EACH
SYSTEM
COM
PONENT
POWERNAP
INSTEAD
CALLS
FOR
MINIMIZING
IDLE
POWER
AND
TRANSITION
TIME
WHICH
ARE
SIMPLER
OPTIMIZATION
GOALS
BASED
ON
THE
POWERNAP
CONCEPT
WE
DEVELOP
REQUIREMENTS
AND
OUTLINE
MECHANISMS
TO
ELIMINATE
IDLE
POWER
WASTE
IN
EN
TERPRISE
BLADE
SERVERS
BECAUSE
POWERNAP
OPERATES
IN
LOW
EFFICIENCY
REGIONS
OF
CURRENT
BLADE
CENTER
POWER
SUPPLIES
WE
INTRODUCE
THE
REDUNDANT
ARRAY
FOR
INEXPENSIVE
LOAD
SHAR
ING
RAILS
A
POWER
PROVISIONING
APPROACH
THAT
PROVIDES
HIGH
CONVERSION
EFFICIENCY
ACROSS
THE
ENTIRE
RANGE
OF
POWER
NAP
POWER
DEMANDS
USING
UTILIZATION
TRACES
COLLECTED
FROM
ENTERPRISE
SCALE
COMMERCIAL
DEPLOYMENTS
WE
DEMON
STRATE
THAT
TOGETHER
POWERNAP
AND
RAILS
REDUCE
AVERAGE
SERVER
POWER
CONSUMPTION
BY
CATEGORIES
AND
SUBJECT
DESCRIPTORS
C
COMPUTER
SYS
TEM
IMPLEMENTATION
SERVERS
GENERAL
TERMS
DESIGN
MEASUREMENT
KEYWORDS
POWER
MANAGEMENT
SERVERS
INTRODUCTION
DATA
CENTER
POWER
CONSUMPTION
IS
UNDERGOING
ALARMING
GROWTH
BY
U
DATA
CENTERS
WILL
CONSUME
BIL
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
ASPLOS
MARCH
WASHINGTON
DC
USA
COPYRIGHT
QC
ACM
LION
KWH
AT
A
COST
OF
BILLION
PER
YEAR
UNFORTU
NATELY
MUCH
OF
THIS
ENERGY
IS
WASTED
BY
SYSTEMS
THAT
ARE
IDLE
AT
IDLE
CURRENT
SERVERS
STILL
DRAW
ABOUT
OF
PEAK
POWER
IN
TYPICAL
DATA
CENTERS
AVERAGE
UTILIZATION
IS
ONLY
LOW
UTILIZATION
IS
ENDEMIC
TO
DATA
CENTER
OPERATION
STRICT
SERVICE
LEVEL
AGREEMENTS
FORCE
OPER
ATORS
TO
PROVISION
FOR
REDUNDANT
OPERATION
UNDER
PEAK
LOAD
IDLE
ENERGY
WASTE
IS
COMPOUNDED
BY
LOSSES
IN
THE
POWER
DELIVERY
AND
COOLING
INFRASTRUCTURE
WHICH
INCREASE
POWER
CONSUMPTION
REQUIREMENTS
BY
IDEALLY
WE
WOULD
LIKE
TO
SIMPLY
TURN
IDLE
SYSTEMS
OFF
UN
FORTUNATELY
A
LARGE
FRACTION
OF
SERVERS
EXHIBIT
FREQUENT
BUT
BRIEF
BURSTS
OF
ACTIVITY
MOREOVER
USER
DEMAND
OFTEN
VARIES
RAPIDLY
AND
OR
UNPREDICTABLY
MAKING
DYNAMIC
CONSOL
IDATION
AND
SYSTEM
SHUTDOWN
DIFFICULT
OUR
ANALYSIS
SHOWS
THAT
SERVER
WORKLOADS
ESPECIALLY
INTERACTIVE
SERVICES
EXHIBIT
FREQUENT
IDLE
PERIODS
OF
LESS
THAN
ONE
SECOND
WHICH
CANNOT
BE
EXPLOITED
BY
EXISTING
MECHANISMS
CONCERN
OVER
IDLE
ENERGY
WASTE
HAS
PROMPTED
CALLS
FOR
A
FUNDAMENTAL
REDESIGN
OF
EACH
COMPUTER
SYSTEM
COMPONENT
TO
CONSUME
ENERGY
IN
PROPORTION
TO
UTILIZATION
PROCES
SOR
DYNAMIC
FREQUENCY
AND
VOLTAGE
SCALING
DVFS
EXEM
PLIFIES
THE
ENERGY
PROPORTIONAL
CONCEPT
PROVIDING
UP
TO
CU
BIC
ENERGY
SAVINGS
UNDER
REDUCED
LOAD
UNFORTUNATELY
PRO
CESSORS
ACCOUNT
FOR
AN
EVER
SHRINKING
FRACTION
OF
TOTAL
SERVER
POWER
ONLY
IN
CURRENT
SYSTEMS
AND
CONTROL
LING
DVFS
REMAINS
AN
ACTIVE
RESEARCH
TOPIC
OTHER
SUBSYSTEMS
INCUR
MANY
FIXED
POWER
OVERHEADS
WHEN
ACTIVE
AND
DO
NOT
YET
OFFER
ENERGY
PROPORTIONAL
OPERATION
WE
PROPOSE
AN
ALTERNATIVE
ENERGY
CONSERVATION
APPROACH
CALLED
POWERNAP
THAT
IS
ATTUNED
TO
SERVER
UTILIZATION
PAT
TERNS
WITH
POWERNAP
WE
DESIGN
THE
ENTIRE
SYSTEM
TO
TRAN
SITION
RAPIDLY
BETWEEN
A
HIGH
PERFORMANCE
ACTIVE
STATE
AND
A
MINIMAL
POWER
NAP
STATE
IN
RESPONSE
TO
INSTANTANEOUS
LOAD
RATHER
THAN
REQUIRING
COMPONENTS
THAT
PROVIDE
FINE
GRAIN
POWER
PERFORMANCE
TRADE
OFFS
POWERNAP
SIMPLIFIES
THE
SYS
TEM
DESIGNER
TASK
TO
FOCUS
ON
TWO
OPTIMIZATION
GOALS
OPTIMIZING
ENERGY
EFFICIENCY
WHILE
NAPPING
AND
MIN
IMIZING
TRANSITION
TIME
INTO
AND
OUT
OF
THE
LOW
POWER
NAP
STATE
BASED
ON
THE
POWERNAP
CONCEPT
WE
DEVELOP
REQUIREMENTS
AND
OUTLINE
MECHANISMS
TO
ELIMINATE
IDLE
POWER
WASTE
IN
A
HIGH
DENSITY
BLADE
SERVER
SYSTEM
WHEREAS
MANY
MECH
ANISMS
REQUIRED
BY
POWERNAP
CAN
BE
ADAPTED
FROM
MO
UTILIZATION
IBM
SUN
GOOGLE
CPU
FANS
I
O
DISK
MEMORY
OTHER
FIGURE
SERVER
UTILIZATION
HISTOGRAM
REAL
DATA
CENTERS
ARE
UNDER
UTILIZED
TABLE
ENTERPRISE
DATA
CENTER
UTILIZATION
TRACES
WORKLOAD
AVG
UTILIZATION
DESCRIPTION
WEB
WEB
APPLICATION
SERVERS
IT
ENTERPRISE
IT
INFRASTRUCTURE
APPS
BILE
AND
HANDHELD
DEVICES
ONE
CRITICAL
SUBSYSTEM
OF
CUR
RENT
BLADE
CHASSIS
FALLS
SHORT
OF
MEETING
POWERNAP
ENERGY
EFFICIENCY
REQUIREMENTS
THE
POWER
CONVERSION
SYSTEM
POWER
NAP
REDUCES
TOTAL
ENSEMBLE
POWER
CONSUMPTION
WHEN
ALL
BLADES
ARE
NAPPING
TO
ONLY
OF
THE
PEAK
WHEN
ALL
ARE
AC
TIVE
POWER
SUPPLIES
ARE
NOTORIOUSLY
INEFFICIENT
AT
LOW
LOADS
TYPICALLY
PROVIDING
CONVERSION
EFFICIENCY
BELOW
UNDER
LOAD
THESE
LOSSES
UNDERMINES
POWERNAP
ENERGY
EFFICIENCY
DIRECTLY
IMPROVING
POWER
SUPPLY
EFFICIENCY
IMPLIES
A
SUB
STANTIAL
COST
PREMIUM
INSTEAD
WE
INTRODUCE
THE
REDUNDANT
ARRAY
FOR
INEXPENSIVE
LOAD
SHARING
RAILS
A
POWER
PRO
VISIONING
APPROACH
WHERE
POWER
DRAW
IS
SHARED
OVER
AN
AR
RAY
OF
LOW
CAPACITY
POWER
SUPPLY
UNITS
PSUS
BUILT
WITH
COMMODITY
COMPONENTS
THE
KEY
INNOVATION
OF
RAILS
IS
TO
SIZE
INDIVIDUAL
POWER
MODULES
SUCH
THAT
THE
POWER
DE
LIVERY
SOLUTION
OPERATES
AT
HIGH
EFFICIENCY
ACROSS
THE
ENTIRE
RANGE
OF
POWERNAP
POWER
DEMANDS
IN
ADDITION
RAILS
PROVIDES
N
REDUNDANCY
GRACEFUL
COMPUTE
CAPACITY
DEGRA
DATION
IN
THE
FACE
OF
MULTIPLE
POWER
MODULE
FAILURES
AND
REDUCED
COMPONENT
COSTS
RELATIVE
TO
CONVENTIONAL
ENTERPRISE
CLASS
POWER
SYSTEMS
THROUGH
MODELING
AND
ANALYSIS
OF
AC
TUAL
DATA
CENTER
WORKLOAD
TRACES
WE
DEMONSTRATE
ANALYSIS
OF
IDLE
BUSY
INTERVALS
IN
ACTUAL
DATA
CENTERS
WE
ANALYZE
UTILIZATION
TRACES
FROM
PRODUCTION
SERVERS
AND
DATA
CENTERS
TO
DETERMINE
THE
DISTRIBUTION
OF
IDLE
AND
ACTIVE
PERIODS
THOUGH
INTERACTIVE
SERVERS
ARE
TYPICALLY
OVER
IDLE
MOST
IDLE
INTERVALS
ARE
UNDER
ONE
SECOND
ENERGY
EFFICIENCY
AND
RESPONSE
TIME
BOUNDS
THROUGH
QUEUING
ANALYSIS
WE
ESTABLISH
BOUNDS
ON
POWERNAP
ENERGY
EFFICIENCY
AND
RESPONSE
TIME
IMPACT
USING
OUR
FIGURE
SERVER
POWER
BREAKDOWN
NO
SINGLE
COM
PONENT
DOMINATES
TOTAL
SYSTEM
POWER
MODELS
WE
DETERMINE
THAT
POWERNAP
IS
EFFECTIVE
IF
STATE
TRANSITION
TIME
IS
BELOW
AND
INCURS
NO
OVERHEADS
BELOW
FURTHERMORE
WE
SHOW
THAT
POWERNAP
PRO
VIDES
GREATER
ENERGY
EFFICIENCY
AND
LOWER
RESPONSE
TIME
THAN
SOLUTIONS
BASED
ON
DVFS
EFFICIENT
POWERNAP
POWER
PROVISIONING
WITH
RAILS
OUR
ANALYSIS
OF
COMMERCIAL
DATA
CENTER
WORKLOAD
TRACES
DEMONSTRATES
THAT
RAILS
IMPROVES
AVERAGE
POWER
CON
VERSION
EFFICIENCY
FROM
TO
IN
POWERNAP
ENABLED
SERVERS
UNDERSTANDING
SERVER
UTILIZATION
IT
HAS
BEEN
WELL
ESTABLISHED
IN
THE
RESEARCH
LITERATURE
THAT
THE
AVERAGE
SERVER
UTILIZATION
OF
DATA
CENTERS
IS
LOW
OFTEN
BELOW
IN
FACILITIES
THAT
PROVIDE
INTERACTIVE
SERVICES
E
G
TRANSACTION
PROCESSING
FILE
SERVERS
WEB
AVERAGE
UTILIZATION
IS
OFTEN
EVEN
WORSE
SOMETIMES
AS
LOW
AS
FIGURE
DEPICTS
A
HISTOGRAM
OF
UTILIZATION
FOR
TWO
PRODUCTION
WORKLOADS
FROM
ENTERPRISE
SCALE
COMMERCIAL
DEPLOYMENTS
TABLE
DESCRIBES
THE
WORKLOADS
RUNNING
ON
THESE
SERVERS
WE
DERIVE
THIS
DATA
FROM
UTILIZATION
TRACES
COLLECTED
OVER
MANY
DAYS
AGGREGATED
OVER
MORE
THAN
SEVERS
PRODUC
TION
UTILIZATION
TRACES
WERE
PROVIDED
COURTESY
OF
HP
LABS
THE
MOST
STRIKING
FEATURE
OF
THIS
DATA
IS
THAT
THE
SERVERS
SPEND
THE
VAST
MAJORITY
OF
TIME
UNDER
UTILIZATION
DATA
CENTER
UTILIZATION
IS
UNLIKELY
TO
INCREASE
FOR
TWO
REASONS
FIRST
DATA
CENTER
OPERATORS
MUST
PROVISION
FOR
PEAK
RATHER
THAN
AVERAGE
LOAD
FOR
INTERACTIVE
SERVICES
PEAK
UTILIZATION
OFTEN
EXCEEDS
AVERAGE
UTILIZATION
BY
MORE
THAN
A
FACTOR
OF
THREE
SECOND
TO
PROVIDE
REDUNDANCY
IN
THE
EVENT
OF
FAILURES
OPERATORS
USUALLY
DEPLOY
MORE
SYSTEMS
THAN
ARE
ACTUALLY
NEEDED
THOUGH
SERVER
CONSOLIDATION
CAN
IMPROVE
AVERAGE
UTILIZATION
PERFORMANCE
ISOLATION
REDUNDANCY
AND
SERVICE
ROBUSTNESS
CONCERNS
OFTEN
PRECLUDE
CONSOLIDATION
OF
MISSION
CRITICAL
SERVICES
LOW
UTILIZATION
CREATES
AN
ENERGY
EFFICIENCY
CHALLENGE
BE
CAUSE
CONVENTIONAL
SERVERS
ARE
NOTORIOUSLY
INEFFICIENT
AT
LOW
LOADS
ALTHOUGH
POWER
SAVING
FEATURES
LIKE
CLOCK
GATING
AND
BUSY
PERIOD
MS
IDLE
PERIOD
MS
FIGURE
BUSY
AND
IDLE
PERIOD
CUMULATIVE
DISTRIBUTIONS
TABLE
FINE
GRAIN
UTILIZATION
TRACES
WORKLOAD
UTILIZATION
AVG
INTERVAL
DESCRIPTION
BUSY
IDLE
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
DVFS
NEARLY
ELIM
INATE
PROCESSOR
POWER
CONSUMPTION
IN
IDLE
SYSTEMS
PRESENT
DAY
SERVERS
STILL
DISSIPATE
ABOUT
AS
MUCH
POWER
WHEN
IDLE
AS
WHEN
FULLY
LOADED
PROCESSORS
OFTEN
ACCOUNT
FOR
ONLY
A
QUARTER
OF
SYSTEM
POWER
MAIN
MEMORY
AND
COOL
ING
FANS
CONTRIBUTE
LARGER
FRACTIONS
FIGURE
REPRODUCES
TYPICAL
SERVER
POWER
BREAKDOWNS
FOR
THE
IBM
SUN
ULTRASPARC
AND
A
GENERIC
SERVER
SPECIFIED
BY
GOOGLE
RESPECTIVELY
FREQUENT
BRIEF
UTILIZATION
CLEARLY
ELIMINATING
SERVER
IDLE
POWER
WASTE
IS
CRITICAL
TO
IM
PROVING
DATA
CENTER
ENERGY
EFFICIENCY
ENGINEERS
HAVE
BEEN
SUCCESSFUL
IN
REDUCING
IDLE
POWER
IN
MOBILE
PLATFORMS
SUCH
AS
CELL
PHONES
AND
LAPTOPS
HOWEVER
SERVERS
POSE
A
FUNDA
MENTALLY
DIFFERENT
CHALLENGE
THAN
THESE
PLATFORMS
THE
KEY
OBSERVATION
UNDERLYING
OUR
WORK
IS
THAT
ALTHOUGH
SERVERS
HAVE
LOW
UTILIZATION
THEIR
ACTIVITY
OCCURS
IN
FREQUENT
BRIEF
BURSTS
AS
A
RESULT
THEY
APPEAR
TO
BE
UNDER
A
CONSTANT
LIGHT
LOAD
TO
INVESTIGATE
THE
TIME
SCALE
OF
SERVERS
IDLE
AND
BUSY
PERI
ODS
WE
HAVE
INSTRUMENTED
A
SERIES
OF
INTERACTIVE
AND
BATCH
PROCESSING
SERVERS
TO
COLLECT
UTILIZATION
TRACES
AT
GRAN
ULARITY
TO
OUR
KNOWLEDGE
OUR
STUDY
IS
THE
FIRST
TO
REPORT
SERVER
UTILIZATION
DATA
MEASURED
AT
SUCH
FINE
GRANULARITY
WE
CLASSIFY
AN
INTERVAL
AS
BUSY
OR
IDLE
BASED
ON
HOW
THE
OS
SCHEDULER
ACCOUNTED
THE
PERIOD
IN
ITS
UTILIZATION
TRACKING
THE
TRACES
WERE
COLLECTED
OVER
A
PERIOD
OF
A
WEEK
FROM
SEVEN
DEPARTMENTAL
IT
SERVERS
AND
A
SCIENTIFIC
COMPUTING
CLUSTER
COMPRISING
OVER
SERVERS
WE
PRESENT
THE
MEAN
IDLE
AND
BUSY
PERIOD
LENGTHS
AVERAGE
UTILIZATION
AND
A
BRIEF
DESCRIP
TION
OF
EACH
TRACE
IN
TABLE
FIGURE
SHOWS
THE
CUMULATIVE
DISTRIBUTION
FOR
THE
BUSY
AND
IDLE
PERIOD
LENGTHS
IN
EACH
TRACE
THE
KEY
RESULT
OF
OUR
TRACES
IS
THAT
THE
VAST
MAJORITY
OF
IDLE
PERIODS
ARE
SHORTER
THAN
WITH
MEAN
LENGTHS
IN
THE
OF
MILLISECONDS
BUSY
PERIODS
ARE
EVEN
SHORTER
TYPICALLY
ONLY
OF
MILLISECONDS
EXISTING
ENERGY
CONSERVATION
TECHNIQUES
THE
RAPID
TRANSITIONS
AND
BRIEF
INTERVALS
OF
SERVER
ACTIVITY
MAKE
IT
DIFFICULT
TO
CONSERVE
IDLE
POWER
WITH
EXISTING
AP
PROACHES
THE
RECENT
TREND
TOWARDS
SERVER
CONSOLIDATION
IS
PARTLY
MOTIVATED
BY
THE
HIGH
ENERGY
COST
OF
IDLE
SYS
TEMS
BY
MOVING
SERVICES
TO
VIRTUAL
MACHINES
SEVERAL
SER
VICES
CAN
BE
TIME
MULTIPLEXED
ON
A
SINGLE
PHYSICAL
SERVER
INCREASING
AVERAGE
UTILIZATION
CONSOLIDATION
ALLOWS
THE
TO
TAL
NUMBER
OF
PHYSICAL
SERVERS
TO
BE
REDUCED
THEREBY
RE
DUCING
IDLE
INEFFICIENCY
HOWEVER
SERVER
CONSOLIDATION
BY
ITSELF
DOES
NOT
CLOSE
THE
GAP
BETWEEN
PEAK
AND
AVERAGE
UTI
LIZATION
DATA
CENTERS
STILL
REQUIRE
SUFFICIENT
CAPACITY
FOR
PEAK
DEMAND
WHICH
INEVITABLY
LEAVES
SOME
SERVERS
IDLE
IN
THE
AV
ERAGE
CASE
FURTHERMORE
CONSOLIDATION
DOES
NOT
SAVE
ENERGY
AUTOMATICALLY
SYSTEM
ADMINISTRATORS
MUST
ACTIVELY
CONSOLI
DATE
SERVICES
AND
REMOVE
UNNEEDED
SYSTEMS
ALTHOUGH
SUPPORT
FOR
SLEEP
STATES
IS
WIDESPREAD
IN
HANDHELD
LAPTOP
AND
DESKTOP
MACHINES
THESE
STATES
ARE
RARELY
USED
IN
CURRENT
SERVER
SYSTEMS
UNFORTUNATELY
THE
HIGH
RESTART
LA
TENCY
TYPICAL
OF
CURRENT
SLEEP
STATES
RENDERS
THEM
UNACCEPT
PACKET
SERVER
OPERATES
AT
FULL
PERFORMANCE
TO
FINISH
EXISTING
WORK
SYSTEM
COMPONENTS
NAP
WHILE
SERVER
IS
IDLE
THE
NIC
DETECTS
THE
ARRIVAL
OF
WORK
SERVER
RETURNS
TO
FULL
PERFORMANCE
TO
FINISH
WORK
AS
QUICKLY
AS
POSSIBLE
FIGURE
POWERNAP
ABLE
FOR
INTERACTIVE
SERVICES
CURRENT
LAPTOPS
AND
DESKTOPS
REQUIRE
SEVERAL
SECONDS
TO
SUSPEND
USING
OPERATING
SYSTEM
INTERFACES
E
G
ACPI
MOREOVER
UNLIKE
CONSUMER
DEVICES
SERVERS
CANNOT
RELY
ON
THE
USER
TO
TRANSITION
BETWEEN
POWER
STATES
THEY
MUST
HAVE
AN
AUTONOMOUS
MECHANISM
THAT
MAN
AGES
STATE
TRANSITIONS
RECENT
SERVER
PROCESSORS
INCLUDE
CPU
THROTTLING
SOLUTIONS
E
G
INTEL
SPEEDSTEP
AMD
COOL
N
QUIET
TO
REDUCE
THE
LARGE
OVERHEAD
OF
LIGHT
LOADS
THESE
PROCESSORS
USE
DVFS
TO
REDUCE
THEIR
OPERATING
FREQUENCY
LINEARLY
WHILE
GAINING
CUBIC
POWER
SAVINGS
DVFS
RELIES
ON
OPERATING
SYSTEM
SUP
PORT
TO
TUNE
PROCESSOR
FREQUENCY
TO
INSTANTANEOUS
LOAD
IN
LINUX
THE
KERNEL
CONTINUES
LOWERING
FREQUENCY
UNTIL
IT
OB
SERVES
IDLE
TIME
IMPROVING
DVFS
CONTROL
ALGORITHMS
REMAINS
AN
ACTIVE
RESEARCH
AREA
NONETHELESS
DVFS
CAN
BE
HIGHLY
EFFECTIVE
IN
REDUCING
CPU
POWER
HOWEVER
AS
FIGURE
SHOWS
CPUS
ACCOUNT
FOR
A
SMALL
PORTION
OF
TOTAL
SYSTEM
POWER
ENERGY
PROPORTIONAL
COMPUTING
SEEKS
TO
EXTEND
THE
SUC
CESS
OF
DVFS
TO
THE
ENTIRE
SYSTEM
IN
THIS
SCHEME
EACH
SYS
TEM
COMPONENT
IS
REDESIGNED
TO
CONSUME
ENERGY
IN
PROPOR
TION
TO
UTILIZATION
IN
AN
ENERGY
PROPORTIONAL
SYSTEM
EXPLICIT
POWER
MANAGEMENT
IS
UNNECESSARY
AS
POWER
CONSUMPTION
VARIES
NATURALLY
WITH
UTILIZATION
HOWEVER
AS
MANY
COMPO
NENTS
INCUR
FIXED
POWER
OVERHEADS
WHEN
ACTIVE
E
G
CLOCK
POWER
ON
SYNCHRONOUS
MEMORY
BUSSES
LEAKAGE
POWER
IN
CPUS
ETC
DESIGNING
ENERGY
PROPORTIONAL
SUBSYSTEMS
RE
MAINS
A
RESEARCH
CHALLENGE
ENERGY
PROPORTIONAL
OPERATION
CAN
BE
APPROXIMATED
WITH
NON
ENERGY
PROPORTIONAL
SYSTEMS
THROUGH
DYNAMIC
VIRTUAL
MACHINE
CONSOLIDATION
OVER
A
LARGE
SERVER
ENSEMBLE
HOWEVER
SUCH
APPROACHES
DO
NOT
ADDRESS
THE
PERFORMANCE
ISOLATION
CONCERNS
OF
DYNAMIC
CONSOLIDATION
AND
OPERATE
AT
COARSE
TIME
SCALES
MINUTES
HENCE
THEY
CANNOT
EXPLOIT
THE
BRIEF
IDLE
PERIODS
FOUND
IN
SERVERS
POWERNAP
ALTHOUGH
SERVERS
SPEND
MOST
OF
THEIR
TIME
IDLE
CONVEN
TIONAL
ENERGY
CONSERVATION
TECHNIQUES
ARE
UNABLE
TO
EXPLOIT
THESE
BRIEF
IDLE
PERIODS
HENCE
WE
PROPOSE
AN
APPROACH
TO
POWER
MANAGEMENT
THAT
ENABLES
THE
ENTIRE
SYSTEM
TO
TRAN
SITION
RAPIDLY
INTO
AND
OUT
OF
A
LOW
POWER
STATE
WHERE
ALL
ACTIVITY
IS
SUSPENDED
UNTIL
NEW
WORK
ARRIVES
WE
CALL
OUR
AP
PROACH
POWERNAP
FIGURE
ILLUSTRATES
THE
POWERNAP
CONCEPT
EACH
TIME
THE
SERVER
EXHAUSTS
ALL
PENDING
WORK
IT
TRANSITIONS
TO
THE
NAP
STATE
IN
THIS
STATE
NEARLY
ALL
SYSTEM
COMPONENTS
ENTER
SLEEP
MODES
WHICH
ARE
ALREADY
AVAILABLE
IN
MANY
COMPONENTS
SEE
SECTION
WHILE
IN
THE
NAP
STATE
POWER
CONSUMPTION
IS
LOW
BUT
NO
PROCESSING
CAN
OCCUR
SYSTEM
COMPONENTS
THAT
SIGNAL
THE
ARRIVAL
OF
NEW
WORK
EXPIRATION
OF
A
SOFTWARE
TIMER
OR
ENVIRONMENTAL
CHANGES
REMAIN
PARTIALLY
POWERED
WHEN
NEW
WORK
ARRIVES
THE
SYSTEM
WAKES
AND
TRANSITIONS
BACK
TO
THE
ACTIVE
STATE
WHEN
THE
WORK
IS
COMPLETE
THE
SYSTEM
RETURNS
TO
THE
NAP
STATE
POWERNAP
IS
SIMPLER
THAN
MANY
OTHER
ENERGY
CONSERVATION
SCHEMES
BECAUSE
IT
REQUIRES
SYSTEM
COMPONENTS
TO
SUPPORT
ONLY
TWO
OPERATING
MODES
AN
ACTIVE
MODE
THAT
PROVIDES
MAXIMUM
PERFORMANCE
AND
A
NAP
MODE
THAT
MINIMIZES
POWER
DRAW
FOR
MANY
DEVICES
PROVIDING
A
LOW
POWER
NAP
MODE
IS
FAR
EASIER
THAN
PROVIDING
MULTIPLE
ACTIVE
MODES
THAT
TRADE
PERFORMANCE
FOR
POWER
SAVINGS
ANY
LEVEL
OF
ACTIVITY
OFTEN
IMPLIES
FIXED
POWER
OVERHEADS
E
G
BUS
CLOCK
SWITCH
ING
POWER
DISTRIBUTION
LOSSES
LEAKAGE
POWER
MECHANICAL
COMPONENTS
ETC
WE
OUTLINE
MECHANISMS
REQUIRED
TO
IM
PLEMENT
POWERNAP
IN
SECTION
POWERNAP
PERFORMANCE
AND
POWER
MODEL
TO
ASSESS
POWERNAP
POTENTIAL
WE
DEVELOP
A
QUEUING
MODEL
THAT
RELATES
ITS
KEY
PERFORMANCE
MEASURES
ENERGY
SAVINGS
AND
RESPONSE
TIME
PENALTY
TO
WORKLOAD
PARAMETERS
AND
POWERNAP
IMPLEMENTATION
CHARACTERISTICS
WE
CONTRAST
POWERNAP
WITH
A
MODEL
OF
THE
UPPER
BOUND
ENERGY
SAVINGS
POSSIBLE
WITH
DVFS
THE
GOAL
OF
OUR
MODEL
IS
THREEFOLD
TO
GAIN
INSIGHT
INTO
POWERNAP
BEHAVIOR
TO
DERIVE
RE
QUIREMENTS
FOR
POWERNAP
IMPLEMENTATIONS
AND
TO
CON
TRAST
POWERNAP
AND
DVFS
WORK
IN
QUEUE
ACTIVE
X
SUSPEND
WAKE
WORK
IN
QUEUE
XX
ARRIVAL
ARRIVAL
X
X
ARRIVAL
TIME
ARRIVAL
ARRIVAL
ARRIVAL
TIME
POWERNAP
B
DVFS
FIGURE
POWERNAP
AND
DVFS
ANALYTIC
MODELS
WE
MODEL
BOTH
POWERNAP
AND
DVFS
UNDER
THE
ASSUMP
TION
THAT
EACH
SEEKS
TO
MINIMIZE
THE
ENERGY
REQUIRED
TO
SERVE
THE
OFFERED
LOAD
HENCE
BOTH
SCHEMES
PROVIDE
IDEN
TT
F
ΛE
ΛT
DT
RTT
T
TT
ΛE
ΛTDT
TICAL
THROUGHPUT
MATCHING
THE
OFFERED
LOAD
BUT
DIFFER
IN
RE
NAP
E
E
I
SPONSE
TIME
AND
ENERGY
CONSUMPTION
POWERNAP
MODEL
WE
MODEL
POWERNAP
AS
AN
M
G
QUEU
ING
SYSTEM
WITH
ARRIVAL
RATE
Λ
AND
A
GENERALIZED
SERVICE
TIME
DISTRIBUTION
WITH
KNOWN
FIRST
AND
SECOND
MOMENTS
E
AND
E
FIGURE
A
SHOWS
THE
WORK
IN
THE
QUEUE
FOR
THREE
ΛE
Λ
E
ΛTT
ΛE
ΛE
I
THE
RESPONSE
TIME
FOR
AN
M
G
SERVER
WITH
EXCEPTIONAL
FIRST
SERVICE
IS
DUE
TO
WELCH
JOB
ARRIVALS
NOTE
THAT
IN
THIS
CONTEXT
WORK
ALSO
INCLUDES
TIME
SPENT
IN
THE
WAKE
AND
SUSPEND
STATES
AVERAGE
SERVER
ΛE
ΛE
I
ΛE
I
ΛE
I
UTILIZATION
IS
GIVEN
BY
Ρ
ΛE
TO
MODEL
THE
EFFECTS
OF
POWERNAP
SUSPEND
AND
WAKE
TRANSITIONS
WE
EXTEND
THE
CONVENTIONAL
M
G
MODEL
WITH
AN
EXCEPTIONAL
FIRST
SERVICE
TIME
WE
ASSUME
POWERNAP
TRANSITIONS
ARE
SYMMETRIC
WITH
LATENCY
TT
SERVICE
OF
THE
FIRST
JOB
IN
EACH
BUSY
PERIOD
IS
DELAYED
BY
AN
INITIAL
SETUP
TIME
I
THE
SETUP
TIME
INCLUDES
THE
WAKE
TRANSITION
AND
MAY
INCLUDE
THE
REMAINING
PORTION
OF
A
SUSPEND
TRANSITION
AS
SHOWN
FOR
THE
RIGHTMOST
ARRIVAL
IN
FIGURE
A
HENCE
FOR
AN
ARRIVAL
X
TIME
UNITS
FROM
THE
START
OF
THE
PRECEDING
IDLE
PERIOD
THE
INITIAL
SETUP
TIME
IS
GIVEN
BY
I
X
IF
X
TT
TT
IF
X
TT
THE
FIRST
AND
SECOND
MOMENTS
E
I
AND
E
ARE
NOTE
THAT
THE
FIRST
TERM
OF
E
R
IS
THE
POLLACZEK
KHINCHIN
FORMULA
FOR
THE
EXPECTED
QUEUING
DELAY
IN
A
STANDARD
M
G
QUEUE
THE
SECOND
TERM
IS
ADDITIONAL
RESIDUAL
DELAY
CAUSED
BY
THE
INITIAL
SETUP
TIME
I
AND
THE
FINAL
TERM
IS
THE
EXPECTED
SERVICE
TIME
E
THE
SECOND
TERM
VANISHES
WHEN
TT
DVFS
MODEL
RATHER
THAN
MODEL
A
REAL
DVFS
FREQUENCY
CONTROL
ALGORITHM
WE
INSTEAD
MODEL
THE
UPPER
BOUND
OF
EN
ERGY
SAVINGS
POSSIBLE
WITH
DVFS
FOR
EACH
JOB
ARRIVAL
WE
SCALE
INSTANTANEOUS
FREQUENCY
F
TO
STRETCH
THE
JOB
TO
FILL
ANY
IDLE
TIME
UNTIL
THE
NEXT
JOB
ARRIVAL
AS
ILLUSTRATED
IN
FIG
URE
B
WHICH
GIVES
E
F
FMAXΡ
THIS
SCHEME
MAXI
MIZES
POWER
SAVINGS
BUT
CANNOT
BE
IMPLEMENTED
IN
PRAC
TICE
BECAUSE
IT
REQUIRES
KNOWLEDGE
OF
FUTURE
ARRIVAL
TIMES
WE
BASE
POWER
SAVINGS
ESTIMATES
ON
THE
THEORETICAL
FORMU
LATION
OF
PROCESSOR
DYNAMIC
POWER
CONSUMPTION
PCP
U
CV
WE
ASSUME
C
AND
A
ARE
FIXED
AND
CHOOSE
THE
OP
E
I
R
IΛE
ΛXDX
E
ΛTT
TIMAL
F
FOR
EACH
JOB
WITHIN
THE
RANGE
FMIN
F
FMAX
E
T
Λ
Λ
ΛXDX
WE
IMPOSE
A
LOWER
BOUND
FMIN
FMAX
TO
PREVENT
RE
SPONSE
TIME
FROM
GROWING
ASYMPTOTICALLY
WHEN
UTILIZATION
IS
LOW
WE
CHOSE
A
FACTOR
OF
BETWEEN
FMIN
AND
FMAX
ΛTT
BASED
ON
THE
FREQUENCY
RANGE
PROVIDED
BY
A
GHZ
AMD
T
T
ATHLON
WE
ASSUME
VOLTAGE
SCALES
LINEARLY
WITH
FREQUENCY
Λ
T
WE
COMPUTE
AVERAGE
POWER
AS
PAVG
PNAP
FNAP
PMAX
FNAP
WHERE
THE
FRACTION
OF
TIME
SPENT
NAPPING
FNAP
IS
GIVEN
BY
I
E
V
VMAX
F
FMAX
WHICH
IS
OPTIMISTIC
WITH
RESPECT
TO
CURRENT
DVFS
IMPLEMENTATIONS
FINALLY
AS
DVFS
ONLY
RE
DUCES
THE
CPU
CONTRIBUTION
TO
SYSTEM
POWER
WE
INCLUDE
A
PARAMETER
FCP
U
TO
CONTROL
THE
FRACTION
OF
TOTAL
SYSTEM
POWER
AFFECTED
BY
DVFS
UNDER
THESE
ASSUMPTIONS
AVERAGE
POWER
PAVG
IS
GIVEN
BY
THE
RATIO
OF
THE
EXPECTED
LENGTH
OF
EACH
NAP
PERIOD
E
N
TO
THE
EXPECTED
BUSY
IDLE
CYCLE
LENGTH
E
C
PAVG
PMAX
FCP
U
E
F
FMAX
DVFS
DVFS
DVFS
FCPU
FCPU
FCPU
POWERNAP
TT
MS
POWERNAP
TT
MS
POWERNAP
TT
MS
UTILIZATION
POWER
SCALING
UTILIZATION
RESPONSE
TIME
SCALING
FIGURE
POWERNAP
AND
DVFS
POWER
AND
RESPONSE
TIME
SCALING
RESPONSE
TIME
IS
GIVEN
BY
E
R
E
RBASE
L
WHERE
RBASE
IS
THE
RESPONSE
TIME
WITHOUT
DVFS
ANALYSIS
POWER
SAVINGS
FIGURE
A
SHOWS
THE
AVERAGE
POWER
AS
A
FRACTION
OF
PEAK
REQUIRED
UNDER
POWERNAP
AND
DVFS
AS
A
FUNCTION
OF
UTILIZATION
FOR
DVFS
WE
SHOW
POWER
SAV
INGS
FOR
THREE
VALUES
OF
FCP
U
FCP
U
REPRESENTS
THE
UPPER
BOUND
IF
DVFS
WERE
APPLICABLE
TO
ALL
SYSTEM
POWER
FCP
U
BOUND
THE
TYPICAL
RANGE
IN
CUR
RENT
SERVERS
FOR
POWERNAP
WE
CONSTRUCT
THE
GRAPHS
WITH
E
AND
E
WHICH
ARE
BOTH
ESTI
MATED
FROM
THE
OBSERVED
BUSY
PERIOD
DISTRIBUTION
IN
OUR
WEB
TRACE
WE
ASSUME
PNAP
IS
OF
PMAX
WE
VARY
Λ
TO
ADJUST
UTILIZATION
AND
PRESENT
RESULTS
FOR
THREE
VALUES
OF
TT
AND
WE
EXPECT
TO
BE
A
CONSERVATIVE
ESTI
MATE
FOR
ACHIEVABLE
POWERNAP
TRANSITION
TIME
FOR
TRANSITION
TIMES
BELOW
TRANSITION
TIME
BECOMES
NEGLIGIBLE
AND
THE
POWER
SAVINGS
FROM
POWERNAP
VARIES
LINEARLY
WITH
UTILIZA
TION
FOR
ALL
WORKLOADS
WE
DISCUSS
TRANSITION
TIMES
FURTHER
IN
SECTION
WHEN
FCP
U
IS
HIGH
DVFS
CLEARLY
OUTPERFORMS
POWERNAP
AS
IT
PROVIDES
CUBIC
POWER
SAVINGS
WHILE
POWERNAP
SAV
INGS
ARE
AT
BEST
LINEAR
IN
UTILIZATION
HOWEVER
FOR
REALISTIC
VALUES
OF
FCP
U
AND
TRANSITION
TIMES
IN
OUR
EXPECTED
RANGE
TT
POWERNAP
SAVINGS
RAPIDLY
OVERTAKE
DVFS
AS
TRANSITION
TIME
INCREASES
THE
BREAK
EVEN
POINT
BETWEEN
DVFS
AND
POWERNAP
SHIFTS
TOWARDS
LOWER
UTILIZATION
EVEN
FOR
A
TRANSITION
TIME
OF
MS
POWERNAP
CAN
PROVIDE
SUB
STANTIAL
ENERGY
SAVINGS
WHEN
UTILIZATION
IS
BELOW
TABLE
PER
WORKLOAD
ENERGY
SAVINGS
RESPONSE
TIME
IN
FIGURE
B
WE
COMPARE
THE
RESPONSE
TIME
IMPACT
OF
DVFS
AND
POWERNAP
THE
VERTICAL
AXIS
SHOWS
RESPONSE
TIME
NORMALIZED
TO
A
SYSTEM
WITHOUT
POWER
MANAGEMENT
I
E
THAT
ALWAYS
OPERATES
AT
FMAX
FOR
DVFS
RESPONSE
TIME
GROWS
RAPIDLY
WHEN
THE
GAP
BETWEEN
JOB
AR
RIVALS
IS
LARGE
AND
REACHES
THE
FMIN
FLOOR
BELOW
UTILIZA
TION
DVFS
RESPONSE
TIME
PENALTY
IS
INDEPENDENT
OF
FCP
U
AND
IS
BOUNDED
AT
BY
THE
RATIO
OF
FMAX
FMIN
FOR
POWER
NAP
THE
RESPONSE
TIME
PENALTY
IS
NEGLIGIBLE
IF
TT
IS
SMALL
RELATIVE
TO
AVERAGE
SERVICE
TIME
E
WHICH
WE
EXPECT
TO
BE
THE
COMMON
CASE
I
E
MOST
JOBS
LAST
LONGER
THAN
HOWEVER
IF
TT
IS
SIGNIFICANT
RELATIVE
TO
E
THE
POWERNAP
RESPONSE
TIME
PENALTY
GROWS
AS
UTILIZATION
SHRINKS
WHEN
UTILIZATION
IS
HIGH
THE
SERVER
IS
RARELY
IDLE
AND
FEW
JOBS
ARE
DELAYED
BY
TRANSITIONS
AS
UTILIZATION
DROPS
THE
ADDITIONAL
DELAY
SEEN
BY
EACH
JOB
CONVERGES
TO
TT
I
E
EVERY
JOB
MUST
WAIT
FOR
WAKE
UP
PER
WORKLOAD
ENERGY
SAVINGS
FINALLY
WE
REPORT
THE
EN
ERGY
SAVINGS
UNDER
SIMULATED
POWERNAP
AND
DVFS
SCHEMES
FOR
OUR
WORKLOAD
TRACES
BECAUSE
THESE
TRACES
ONLY
CONTAIN
BUSY
AND
IDLE
PERIODS
AND
NOT
INDIVIDUAL
JOB
ARRIVALS
WE
CANNOT
ESTIMATE
RESPONSE
TIME
IMPACT
FOR
EACH
WORKLOAD
WE
PERFORM
A
TRACE
BASED
SIMULATION
THAT
ASSUMES
BUSY
PE
RIODS
WILL
START
AT
THE
SAME
TIME
INDEPENDENT
OF
THE
CURRENT
POWERNAP
STATE
I
E
NEW
WORK
STILL
ARRIVES
DURING
WAKE
OR
SUSPEND
TRANSITIONS
WE
ASSUME
A
POWERNAP
TRANSITION
TIME
OF
AND
NAP
POWER
AT
OF
ACTIVE
POWER
WHICH
WE
BE
LIEVE
TO
BE
CONSERVATIVE
ESTIMATES
SEE
SECTION
FOR
DVFS
WE
ASSUME
FCP
U
TABLE
SHOWS
THE
RESULTS
OF
THESE
SIMULATIONS
ALL
WORKLOADS
EXCEPT
MAIL
AND
CLUSTER
HIT
THE
DVFS
FREQUENCY
FLOOR
AND
HENCE
ACHIEVE
A
ENERGY
SAVINGS
IN
ALL
CASES
POWERNAP
ACHIEVES
GREATER
ENERGY
SAV
INGS
ADDITIONALLY
WE
EXTRACTED
THE
AVERAGE
ARRIVAL
RATE
AS
SUMING
A
POISSON
ARRIVAL
PROCESS
AND
COMPARED
THE
RESULTS
IN
TABLE
WITH
THE
M
G
MODEL
OF
FNAP
DERIVED
ABOVE
WE
FOUND
THAT
FOR
THESE
TRACES
THE
ANALYTIC
MODEL
WAS
WITHIN
OF
OUR
SIMULATED
RESULTS
IN
ALL
CASES
WHEN
ARRIVALS
ARE
MORE
DETERMINISTIC
E
G
BACKUP
THAN
THE
EXPONENTIAL
WE
ASSUME
THE
MODEL
SLIGHTLY
OVERESTIMATES
POWERNAP
SAVINGS
FOR
MORE
VARIABLE
ARRIVAL
PROCESSES
E
G
SHELL
THE
MODEL
UNDERESTIMATES
THE
ENERGY
SAVINGS
IMPLEMENTATION
REQUIREMENTS
BASED
ON
THE
RESULTS
OF
OUR
ANALYTIC
MODEL
WE
IDENTIFY
TWO
KEY
POWERNAP
IMPLEMENTATION
REQUIREMENTS
FAST
TRANSITIONS
OUR
MODEL
DEMONSTRATES
THAT
TRANSITION
SPEED
IS
THE
DOMINANT
FACTOR
IN
DETERMINING
BOTH
THE
POWER
SAVINGS
POTENTIAL
AND
RESPONSE
TIME
IMPACT
OF
POWERNAP
OUR
RESULTS
SHOW
THAT
TRANSITION
TIME
MUST
BE
LESS
THAN
ONE
TENTH
OF
AVERAGE
BUSY
PERIOD
LENGTH
ALTHOUGH
A
TRANSITION
SPEED
IS
SUFFICIENT
TO
OBTAIN
SIGNIFICANT
SAVINGS
TRANSITIONS
ARE
NECESSARY
FOR
POWERNAP
OVERHEADS
TO
BECOME
NEGLIGIBLE
TO
ACHIEVE
THESE
TRANSITION
PERIODS
A
POWERNAP
IMPLEMENTATION
MUST
PRESERVE
VOLATILE
SYSTEM
STATE
E
G
MEMORY
WHILE
NAPPING
MASS
STORAGE
DEVICES
TRANSFER
RATES
ARE
INSUFFICIENT
TO
TRANSFER
MULTIPLE
GB
OF
MEMORY
STATE
IN
MILLISECONDS
MINIMIZING
POWER
DRAW
IN
NAP
STATE
GIVEN
THE
LOW
UTI
LIZATION
IN
MOST
ENTERPRISE
DEPLOYMENTS
SERVERS
WILL
SPEND
A
MAJORITY
OF
TIME
IN
THE
NAP
STATE
MAKING
POWERNAP
POWER
REQUIREMENTS
THE
KEY
FACTOR
AFFECTING
AVERAGE
SYS
TEM
POWER
HENCE
IT
IS
CRITICAL
TO
MINIMIZE
THE
POWER
DRAW
OF
NAPPING
SYSTEM
COMPONENTS
AS
A
RESULT
OF
ELIMINATING
IDLE
POWER
POWERNAP
DRASTICALLY
INCREASES
THE
RANGE
BE
TWEEN
THE
MINIMUM
AND
MAXIMUM
POWER
DEMANDS
ON
A
BLADE
CHASSIS
EXISTING
BLADE
CHASSIS
POWER
CONVERSION
SYS
TEMS
ARE
INEFFICIENT
IN
THE
COMMON
CASE
WHERE
ALL
BLADES
ARE
NAPPING
HENCE
TO
MAXIMIZE
POWERNAP
POTENTIAL
WE
MUST
RE
ARCHITECT
THE
BLADE
CHASSIS
POWER
SUBSYSTEM
TO
INCREASE
ITS
EFFICIENCY
AT
LOW
LOADS
ALTHOUGH
POWERNAP
REQUIRES
SYSTEM
WIDE
MODIFICATIONS
IT
DEMANDS
ONLY
TWO
STATES
FROM
EACH
SUBSYSTEM
ACTIVE
AND
NAP
STATES
HENCE
IMPLEMENTING
POWERNAP
IS
SUBSTANTIALLY
SIMPLER
THAN
DEVELOPING
ENERGY
PROPORTIONAL
COMPONENTS
BECAUSE
NO
COMPUTATION
OCCURS
WHILE
NAPPING
MANY
FIXED
TABLE
COMPONENT
POWER
CONSUMPTION
POWER
POWER
DRAWS
SUCH
AS
CLOCKS
AND
LEAKAGE
POWER
CAN
BE
CONSERVED
POWERNAP
MECHANISMS
WE
OUTLINE
THE
DESIGN
OF
A
POWERNAP
ENABLED
BLADE
SERVER
SYSTEM
AND
ENUMERATE
REQUIRED
IMPLEMENTATION
MECHA
NISMS
POWERNAP
REQUIRES
NAP
SUPPORT
IN
ALL
HARDWARE
SUB
SYSTEMS
THAT
HAVE
NON
NEGLIGIBLE
IDLE
POWER
DRAWS
AND
SOFT
WARE
FIRMWARE
SUPPORT
TO
IDENTIFY
AND
MAXIMIZE
IDLE
PERIODS
AND
MANAGE
STATE
TRANSITIONS
HARDWARE
MECHANISMS
MOST
OF
THE
HARDWARE
MECHANISMS
REQUIRED
BY
POWERNAP
ALREADY
EXIST
IN
COMPONENTS
DESIGNED
FOR
MOBILE
DEVICES
HOWEVER
FEW
OF
THESE
MECHANISMS
ARE
EXPLOITED
IN
EXISTING
SERVERS
AND
SOME
ARE
OMITTED
IN
CURRENT
GENERATION
SERVER
CLASS
COMPONENTS
FOR
EACH
HARDWARE
SUBSYSTEM
WE
IDENTIFY
EXISTING
MECHANISMS
OR
OUTLINE
REQUIREMENTS
FOR
NEW
MECH
ANISMS
NECESSARY
TO
IMPLEMENT
POWERNAP
FURTHERMORE
WE
PROVIDE
ESTIMATES
OF
POWER
DISSIPATION
WHILE
NAPPING
AND
TRANSITION
SPEED
WE
SUMMARIZE
THESE
ESTIMATES
ALONG
WITH
OUR
SOURCES
IN
TABLE
OUR
ESTIMATES
FOR
A
TYPICAL
BLADE
ARE
BASED
ON
HP
C
SERIES
HALF
HEIGHT
BLADE
DESIGNS
OUR
POWERNAP
POWER
ESTIMATE
ASSUMES
A
TWO
CPU
SYSTEM
WITH
EIGHT
DRAM
DIMMS
PROCESSOR
ACPI
SLEEP
STATE
THE
ACPI
STANDARD
DE
FINES
THE
SLEEP
STATE
FOR
PROCESSORS
THAT
IS
INTENDED
TO
ALLOW
LOW
LATENCY
TRANSITIONS
ALTHOUGH
THE
ACPI
STANDARD
DOES
NOT
SPECIFY
POWER
OR
PERFORMANCE
REQUIREMENTS
SOME
IMPLEMENTATIONS
OF
ARE
IDEAL
FOR
POWERNAP
FOR
EXAM
PLE
IN
INTEL
MOBILE
PROCESSOR
LINE
PRESERVES
LAST
LEVEL
CACHE
STATE
AND
CONSUMES
ONLY
THESE
PROCESSORS
REQUIRE
APPROXIMATELY
ΜS
FOR
PLL
STABILIZATION
TO
TRANSI
TION
FROM
SLEEP
BACK
TO
ACTIVE
EXECUTION
IF
IS
UNAVAILABLE
CLOCK
GATING
CAN
PROVIDE
SUBSTANTIAL
EN
ERGY
SAVINGS
FOR
EXAMPLE
INTEL
XEON
SERIES
POWER
REQUIREMENTS
DROP
FROM
TO
UPON
EXECUTING
A
HALT
INSTRUCTION
FROM
THIS
STATE
RESUMING
EXECUTION
RE
QUIRES
ONLY
NANOSECOND
SCALE
DELAYS
DRAM
SELF
REFRESH
DRAM
IS
TYPICALLY
THE
SECOND
MOST
POWER
HUNGRY
SYSTEM
COMPONENT
WHEN
ACTIVE
HOWEVER
SEVERAL
RECENT
DRAM
SPECIFICATIONS
FEATURE
AN
OPERATING
MODE
CALLED
SELF
REFRESH
WHERE
THE
DRAM
IS
ISOLATED
FROM
THE
MEMORY
CONTROLLER
AND
AUTONOMOUSLY
REFRESHES
DRAM
CONTENT
IN
THIS
MODE
THE
MEMORY
BUS
CLOCK
AND
PLLS
ARE
DISABLED
AS
ARE
MOST
OF
THE
DRAM
INTERFACE
CIRCUITRY
SELF
REFRESH
SAVES
MORE
THAN
AN
ORDER
OF
MAGNITUDE
OF
POWER
FOR
EXAMPLE
A
SODIMM
DESIGNED
FOR
LAPTOPS
WITH
A
PEAK
POWER
DRAW
ABOVE
USES
ONLY
OF
POWER
DURING
SELF
REFRESH
TRANSITIONS
INTO
AND
OUT
OF
SELF
REFRESH
CAN
BE
COMPLETED
IN
LESS
THAN
A
MICROSECOND
MASS
STORAGE
SOLID
STATE
DISKS
SOLID
STATE
DISKS
DRAW
NEGLIGIBLE
POWER
WHEN
IDLE
AND
HENCE
DO
NOT
NEED
TO
TRAN
SITION
TO
A
SLEEP
STATE
FOR
POWERNAP
A
RECENT
SAM
SUNG
SSD
CONSUMES
ONLY
WHILE
IDLE
NETWORK
INTERFACE
WAKE
ON
LAN
THE
KEY
RESPONSIBILITY
POWERNAP
DEMANDS
OF
THE
NETWORK
INTERFACE
CARD
NIC
IS
TO
WAKE
THE
SYSTEM
UPON
ARRIVAL
OF
A
PACKET
EXISTING
NICS
ALREADY
PROVIDE
SUPPORT
FOR
WAKE
ON
LAN
TO
PERFORM
THIS
FUNCTION
CURRENT
IMPLEMENTATIONS
OF
WAKE
ON
LAN
PRO
VIDE
A
MODE
TO
WAKE
ON
ANY
PHYSICAL
ACTIVITY
THIS
MODE
FORMS
A
BASIS
FOR
POWERNAP
SUPPORT
CURRENT
NICS
CONSUME
ONLY
WHILE
IN
THIS
MODE
ENVIRONMENTAL
MONITORING
SERVICE
PROCESSORS
POWER
NAP
TRANSITION
MANAGEMENT
SERVERS
TYPICALLY
INCLUDE
ADDITIONAL
CIRCUITRY
FOR
ENVIRONMENTAL
MONITORING
REMOTE
MANAGEMENT
E
G
REMOTE
POWER
ON
POWER
CAPPING
POWER
REGULATION
AND
OTHER
FUNCTIONALITY
THESE
COMPONENTS
TYP
ICALLY
MANAGE
ACPI
STATE
TRANSITIONS
AND
WOULD
COORDINATE
POWERNAP
TRANSITIONS
A
TYPICAL
SERVICE
PROCESSOR
DRAWS
LESS
THAN
WHEN
IDLE
FANS
VARIABLE
SPEED
OPERATION
FANS
ARE
A
DOMINANT
POWER
CONSUMER
IN
MANY
RECENT
SERVERS
MODERN
SERVERS
EMPLOY
VARIABLE
SPEED
FANS
WHERE
COOLING
CAPACITY
IS
CON
STANTLY
TUNED
BASED
ON
OBSERVED
TEMPERATURE
OR
POWER
DRAW
FAN
POWER
REQUIREMENTS
TYPICALLY
GROW
CUBICALLY
WITH
AVER
AGE
POWER
THUS
POWERNAP
AVERAGE
POWER
SAVINGS
YIELD
MASSIVE
REDUCTIONS
IN
FAN
POWER
REQUIREMENTS
IN
MOST
BLADE
DESIGNS
COOLING
SYSTEMS
ARE
CENTRALIZED
IN
THE
BLADE
CHASSIS
AMORTIZING
THEIR
ENERGY
COST
OVER
MANY
BLADES
BECAUSE
THER
MAL
CONDUCTION
PROGRESSES
AT
DRASTICALLY
DIFFERENT
TIMESCALES
THAN
POWERNAP
TRANSITION
FREQUENCY
CHASSIS
LEVEL
FAN
CON
TROL
IS
INDEPENDENT
OF
POWERNAP
STATE
I
E
FANS
MAY
CON
TINUE
OPERATING
DURING
NAP
AND
MAY
SPIN
DOWN
DURING
ACTIVE
OPERATION
DEPENDING
ON
TEMPERATURE
CONDITIONS
POWER
PROVISIONING
RAILS
POWERNAP
FUNDAMENTALLY
AL
TERS
THE
RANGE
OF
CURRENTS
OVER
WHICH
A
BLADE
CHASSIS
MUST
EF
FICIENTLY
SUPPLY
POWER
IN
SECTION
WE
EXPLAIN
WHY
CONVEN
TIONAL
POWER
DELIVERY
SCHEMES
ARE
UNABLE
TO
PROVIDE
EFFICIENT
AC
TO
DC
CONVERSION
OVER
THIS
RANGE
AND
PRESENT
RAILS
OUR
POWER
CONVERSION
SOLUTION
SOFTWARE
MECHANISMS
FOR
SCHEMES
LIKE
POWERNAP
THE
PERIODIC
TIMER
INTERRUPT
USED
BY
LEGACY
OS
KERNELS
TO
TRACK
THE
PASSAGE
OF
TIME
AND
IMPLEMENT
SOFTWARE
TIMERS
POSES
A
CHALLENGE
AS
THE
TIMER
INTERRUPT
IS
TRIGGERED
EVERY
CONVENTIONAL
OS
TIME
KEEP
ING
PRECLUDES
THE
USE
OF
POWERNAP
THE
PERIODIC
CLOCK
TICK
ALSO
POSES
A
CHALLENGE
FOR
IDLE
POWER
CONSERVATION
ON
LAP
TOPS
AND
FOR
VIRTUALIZATION
PLATFORMS
THAT
CONSOLIDATE
HUN
DREDS
OF
OS
IMAGES
ON
A
SINGLE
HARDWARE
PLATFORM
HENCE
THE
LINUX
KERNEL
HAS
RECENTLY
BEEN
ENHANCED
TO
SUPPORT
TICKLESS
OPERATION
WHERE
THE
PERIODIC
TIMER
INTERRUPT
IS
ES
CHEWED
IN
FAVOR
OF
HARDWARE
TIMERS
FOR
SCHEDULING
AND
TIME
KEEPING
POWERNAP
DEPENDS
ON
A
KERNEL
THAT
PROVIDES
TICKLESS
OPERATION
POWERNAP
EFFECTIVENESS
INCREASES
WITH
LONGER
IDLE
PERIODS
AND
LESS
FREQUENT
STATE
TRANSITIONS
SOME
EXISTING
HARDWARE
DEVICES
E
G
LEGACY
KEYBOARD
CONTROLLERS
REQUIRE
POLLING
TO
DETECT
INPUT
EVENTS
CURRENT
OPERATING
SYSTEMS
OFTEN
PER
FORM
MAINTENANCE
TASKS
E
G
FLUSHING
DISK
BUFFERS
ZEROING
MEMORY
WHEN
THE
OS
DETECTS
SIGNIFICANT
IDLE
PERIODS
THESE
MAINTENANCE
TASKS
MAY
INTERACT
POORLY
WITH
POWERNAP
AND
CAN
INDUCE
ADDITIONAL
STATE
TRANSITIONS
HOWEVER
EFFORTS
ARE
ALREADY
UNDERWAY
E
G
AS
DESCRIBED
IN
TO
REDESIGN
DE
VICE
DRIVERS
AND
IMPROVE
BACKGROUND
TASK
SCHEDULING
RAILS
AC
TO
DC
CONVERSION
LOSSES
IN
COMPUTER
SYSTEMS
HAVE
RE
CENTLY
BECOME
A
MAJOR
CONCERN
LEADING
TO
A
VARIETY
OF
RE
SEARCH
PROPOSALS
PRODUCT
ANNOUNCEMENTS
E
G
HP
BLADE
SYSTEM
AND
STANDARDIZATION
EFFORTS
TO
IM
PROVE
POWER
SUPPLY
EFFICIENCY
THE
CONCERN
IS
PARTICULARLY
ACUTE
IN
DATA
CENTERS
WHERE
EACH
WATT
WASTED
IN
THE
POWER
DELIVERY
INFRASTRUCTURE
IMPLIES
EVEN
MORE
LOSS
IN
COOLING
BECAUSE
POWERNAP
POWER
DRAW
IS
SUBSTANTIALLY
LOWER
THAN
THE
IDLE
POWER
IN
CONVENTIONAL
SERVERS
POWERNAP
DEMANDS
CONVERSION
EFFICIENCY
OVER
A
WIDE
POWER
RANGE
FROM
AS
FEW
AS
TO
AS
MUCH
AS
IN
A
FULLY
POPULATED
ENCLO
SURE
IN
THIS
SECTION
WE
DISCUSS
WHY
EXISTING
POWER
SOLUTIONS
ARE
INADEQUATE
FOR
POWERNAP
AND
PRESENT
RAILS
OUR
POWER
SOLUTION
RAILS
PROVIDES
HIGH
CONVERSION
EFFICIENCY
ACROSS
POWERNAP
POWER
DEMAND
SPECTRUM
PROVIDES
N
REDUN
DANCY
ALLOWS
FOR
GRACEFUL
DEGRADATION
OF
COMPUTE
CAPACITY
WHEN
PSUS
FAIL
AND
MINIMIZES
COSTS
BY
USING
COMMODITY
PSUS
IN
AN
EFFICIENT
ARRANGEMENT
POWER
SUPPLY
UNIT
BACKGROUND
POOR
EFFICIENCY
AT
LOW
LOADS
ALTHOUGH
MANUFACTURERS
OF
TEN
REPORT
ONLY
A
SINGLE
EFFICIENCY
VALUE
MOST
PSUS
DO
NOT
HAVE
A
CONSTANT
EFFICIENCY
ACROSS
ELECTRICAL
LOAD
A
RECENT
SURVEY
OF
SERVER
AND
DESKTOP
PSUS
REPORTED
THEIR
EFFICIENCY
ACROSS
LOADS
FIGURE
REPRODUCES
THE
RANGE
OF
EFFICIEN
CIES
REPORTED
IN
THAT
STUDY
THOUGH
PSUS
ARE
OFTEN
OVER
RED
YELLOW
GREEN
FURTHERMORE
DESPITE
THEIR
NAME
THE
SPECIFICATION
DOES
NOT
REQUIRE
ENERGY
EFFICIENCY
ABOVE
ACROSS
ALL
LOADS
RATHER
ONLY
WITHIN
THE
TYPICAL
OPERATING
RANGE
OF
CONVENTIONAL
SYSTEMS
THIS
SPECIFIED
EFFICIENCY
RANGE
IS
NOT
WIDE
ENOUGH
FOR
POWERNAP
SINGLE
VOLTAGE
SUPPLIES
UNLIKE
DESKTOP
MACHINES
WHICH
REQUIRE
FIVE
DIFFERENT
DC
OUTPUT
VOLTAGES
TO
SUPPORT
LEGACY
COMPONENTS
SERVER
PSUS
TYPICALLY
PROVIDE
ONLY
A
SINGLE
DC
OUTPUT
VOLTAGE
SIMPLIFYING
THEIR
DESIGN
AND
IMPROVING
RELIABILITY
AND
EFFICIENCY
ALTHOUGH
POWER
LOAD
NAP
BENEFITS
FROM
THIS
FEATURE
A
SINGLE
OUTPUT
VOLTAGE
DOES
NOT
DIRECTLY
ADDRESS
INEFFICIENCY
AT
LOW
LOADS
FIGURE
POWER
SUPPLY
EFFICIENCY
EFFICIENT
AT
THEIR
OPTIMAL
OPERATING
POINT
USUALLY
NEAR
LOAD
EFFICIENCY
DROPS
OFF
RAPIDLY
BELOW
LOAD
SOME
TIMES
DIPPING
BELOW
I
E
IN
FOR
OUT
WE
DIVIDE
THE
OPERATING
EFFICIENCY
OF
POWER
SUPPLIES
INTO
THREE
ZONES
BASED
ON
ELECTRICAL
LOAD
ABOVE
LOAD
THE
PSUS
OPERATE
IN
THE
GREEN
ZONE
WHERE
THEIR
EFFICIENCY
IS
AT
OR
ABOVE
IN
THE
YELLOW
ZONE
PSU
EFFICIENCY
BEGINS
TO
DROP
BUT
TYPICALLY
EXCEEDS
HOWEVER
IN
THE
RED
ZONE
BELOW
EFFICIENCY
DROPS
OFF
PRECIPITOUSLY
TWO
FACTORS
CAUSE
SERVERS
TO
FREQUENTLY
OPERATE
IN
THE
YEL
LOW
OR
RED
EFFICIENCY
ZONES
FIRST
SERVERS
ARE
HIGHLY
CON
FIGURABLE
WHICH
LEADS
TO
A
LARGE
RANGE
OF
POWER
REQUIRE
MENTS
THE
SAME
SERVER
MODEL
MIGHT
BE
SOLD
WITH
ONLY
ONE
OR
AS
MANY
AS
DISKS
INSTALLED
AND
THE
AMOUNT
OF
INSTALLED
DRAM
MIGHT
VARY
BY
A
FACTOR
OF
FURTHERMORE
PERIPHER
ALS
MAY
BE
ADDED
AFTER
THE
SYSTEM
IS
ASSEMBLED
TO
SIMPLIFY
ORDERING
UPGRADES
TESTING
AND
SAFETY
CERTIFICATION
MANU
FACTURERS
TYPICALLY
INSTALL
A
POWER
SUPPLY
RATED
TO
EXCEED
THE
POWER
REQUIREMENTS
OF
THE
MOST
EXTREME
CONFIGURATION
SEC
OND
SERVERS
ARE
OFTEN
CONFIGURED
WITH
REDUNDANT
POWER
SUPPLIES
I
E
TWICE
AS
MANY
AS
ARE
REQUIRED
FOR
A
WORST
CASE
CONFIGURATION
THE
REDUNDANT
SUPPLIES
TYPICALLY
SHARE
THE
ELECTRICAL
LOAD
TO
MINIMIZE
PSU
TEMPERATURE
AND
TO
ENSURE
CURRENT
FLOW
REMAINS
UNINTERRUPTED
IF
A
PSU
FAILS
HOWEVER
THE
EPRI
STUDY
CONCLUDED
THAT
THIS
LOAD
SHARING
ARRANGE
MENT
OFTEN
SHIFTS
PSUS
FROM
YELLOW
ZONE
TO
RED
ZONE
OPERATION
RECENT
EFFICIENCY
IMPROVEMENTS
A
VARIETY
OF
RECENT
INI
TIATIVES
SEEK
TO
IMPROVE
SERVER
POWER
EFFICIENCY
CERTIFICATION
THE
EPA
ENERGY
STAR
PROGRAM
HAS
DEFINED
THE
CERTIFICATION
STANDARD
TO
INCEN
TIVIZE
PSU
MANUFACTURERS
TO
IMPROVE
EFFICIENCY
AT
LOW
LOADS
THE
INCENTIVE
PROGRAM
IS
PRIMARILY
TARGETED
AT
THE
LOW
PEAK
POWER
DESKTOP
PSU
MARKET
SUPPLIES
REQUIRE
CONSIDERABLY
HIGHER
DESIGN
COMPLEXITY
THAN
CON
VENTIONAL
PSUS
WHICH
MAY
POSE
A
BARRIER
TO
WIDESPREAD
ADOPTION
IN
THE
RELIABILITY
CONSCIOUS
SERVER
PSU
MARKET
DC
DISTRIBUTION
RECENT
RESEARCH
HAS
CALLED
FOR
DIS
TRIBUTING
DC
POWER
AMONG
DATA
CENTER
RACKS
ELIMINATING
AC
TO
DC
CONVERSION
EFFICIENCY
CONCERNS
AT
THE
BLADE
EN
CLOSURE
LEVEL
HOWEVER
THE
EFFICIENCY
ADVANTAGES
OF
DC
DISTRIBUTION
ARE
UNCLEAR
AND
DEPLOYING
DC
POWER
WILL
REQUIRE
MULTI
INDUSTRY
COORDINATION
DYNAMIC
LOAD
SHARING
BLADE
ENCLOSURES
CREATE
A
FUR
THER
OPPORTUNITY
TO
IMPROVE
EFFICIENCY
THROUGH
DYNAMIC
LOAD
SHARING
HP
DYNAMIC
POWER
SAVER
FEATURE
IN
THE
HP
BLADE
CENTER
EMPLOYS
UP
TO
SIX
HIGH
EFFICIENCY
PSUS
IN
A
SINGLE
ENCLOSURE
AND
DY
NAMICALLY
VARIES
THE
NUMBER
OF
PSUS
THAT
ARE
ENGAGED
ENSURING
THAT
ALL
ACTIVE
SUPPLIES
OPERATE
IN
THEIR
GREEN
ZONE
WHILE
MAINTAINING
REDUNDANCY
ALTHOUGH
HP
SO
LUTION
IS
IDEAL
FOR
THE
IDLE
AND
PEAK
POWER
RANGE
OF
THE
C
CLASS
BLADES
IT
REQUIRES
EXPENSIVE
PSUS
AND
PROVIDES
INSUFFICIENT
GRANULARITY
FOR
POWERNAP
WHILE
ALL
THESE
SOLUTIONS
IMPROVE
EFFICIENCY
FOR
THEIR
TARGET
MARKETS
NONE
ACHIEVE
ALL
OUR
GOALS
OF
EFFICIENCY
FOR
POWER
NAP
REDUNDANCY
AND
LOW
COST
RAILS
DESIGN
WE
INTRODUCE
A
NEW
POWER
DELIVERY
SOLUTION
TUNED
FOR
POWERNAP
THE
REDUNDANT
ARRAY
FOR
INEXPENSIVE
LOAD
SHAR
ING
RAILS
THE
CENTRAL
IDEA
OF
OUR
SCHEME
IS
TO
LOAD
SHARE
OVER
MULTIPLE
INEXPENSIVE
SMALL
PSUS
TO
PROVIDE
THE
EFFICIENCY
AND
RELIABILITY
OF
LARGER
MORE
EXPENSIVE
UNITS
THROUGH
INTELLIGENT
SIZING
AND
LOAD
SHARING
WE
ENSURE
THAT
ACTIVE
PSUS
OPERATE
IN
THEIR
EFFICIENCY
SWEET
SPOTS
OUR
SCHEME
PROVIDES
EFFICIENCY
AND
ENTERPRISE
CLASS
REDUN
DANCY
WITH
COMMODITY
COMPONENTS
RAILS
TARGETS
THREE
KEY
OBJECTIVES
EFFICIENCY
ACROSS
THE
ENTIRE
POWERNAP
DYNAMIC
POWER
RANGE
N
RELIABILITY
AND
GRACEFUL
DEGRADATION
OF
COMPUTE
CAPACITY
UNDER
MULTIPLE
PSU
FAILURE
AND
MINIMAL
COST
FIGURE
ILLUSTRATES
RAILS
AS
IN
CONVENTIONAL
BLADE
EN
CLOSURES
POWER
IS
PROVIDED
BY
MULTIPLE
PSUS
CONNECTED
IN
LOADN
MAXIMUM
OUTPUT
W
FIGURE
RAILS
PSU
DESIGN
PARALLEL
A
CONVENTIONAL
LOAD
SHARING
CONTROL
CIRCUIT
CONTIN
UOUSLY
MONITORS
AND
CONTROLS
THE
PSUS
TO
ENSURE
LOAD
IS
DI
VIDED
EVENLY
AMONG
THEM
AS
IN
DYNAMIC
SMART
POWER
RAILS
DISABLES
AND
ELECTRICALLY
ISOLATES
PSUS
THAT
ARE
NOT
NECESSARY
TO
SUPPLY
THE
LOAD
HOWEVER
OUR
KEY
DEPARTURE
FROM
PRIOR
DESIGNS
IS
IN
THE
GRANULARITY
OF
THE
INDIVIDUAL
PSUS
WE
SELECT
PSUS
FROM
THE
ECONOMIC
SWEET
SPOT
OF
THE
HIGH
SALES
VOLUME
MARKET
FOR
LOW
WATTAGE
COMMODITY
SUP
PLIES
WE
CHOOSE
A
POWER
SUPPLY
GRANULARITY
TO
SATISFY
TWO
CRITERIA
A
SINGLE
SUPPLY
MUST
BE
OPERATING
IN
ITS
GREEN
ZONE
WHEN
ALL
BLADES
ARE
NAPPING
THIS
CRITERION
ESTABLISHES
AN
UPPER
BOUND
ON
THE
PSU
CAPACITY
BASED
ON
THE
MINIMUM
CHASSIS
POWER
DRAW
WHEN
ALL
BLADES
ARE
NAPPING
SUBJECT
TO
THIS
BOUND
WE
SIZE
PSUS
TO
MATCH
THE
INCREMENTAL
POWER
DRAW
OF
ACTIVATING
A
BLADE
THUS
AS
EACH
BLADE
AWAKENS
ONE
ADDITIONAL
PSU
IS
BROUGHT
ON
LINE
BECAUSE
OF
INTELLIGENT
SIZING
EACH
OF
THESE
PSUS
WILL
OPERATE
IN
THEIR
OPTIMAL
EFFICIENCY
REGION
WHEREAS
CURRENT
BLADE
SERVERS
USE
MULTI
KILOWATT
PSUS
A
TYPICAL
RAILS
PSU
MIGHT
SUPPLY
RAILS
MEETS
ITS
COST
GOALS
BY
INCORPORATING
HIGH
VOLUME
COMMODITY
COMPONENTS
ALTHOUGH
THE
FORM
FACTOR
OF
COM
MODITY
PSUS
MAY
PROVE
AWKWARD
FOR
RACK
MOUNT
BLADE
EN
CLOSURES
PRECLUDING
THE
USE
OF
OFF
THE
SHELF
PSUS
THE
POWER
DENSITY
OF
HIGH
SALES
VOLUME
PSUS
DIFFERS
LITTLE
FROM
HIGH
END
SERVER
SUPPLIES
HENCE
WITH
APPROPRIATE
MECHANICAL
MODIFICATIONS
IT
IS
POSSIBLE
TO
PACK
RAILS
PSUS
IN
ROUGHLY
THE
SAME
PHYSICAL
VOLUME
AS
CONVENTIONAL
BLADE
ENCLOSURE
POWER
SYSTEMS
RAILS
MEETS
ITS
RELIABILITY
GOALS
BY
PROVIDING
FINE
GRAIN
DEGRADATION
OF
THE
SYSTEM
PEAK
POWER
CAPACITY
AS
PSUS
FAIL
IN
ANY
N
DESIGN
THE
FIRST
PSU
FAILURE
DOES
NOT
AF
FECT
COMPUTE
CAPACITY
HOWEVER
IN
CONVENTIONAL
BLADE
EN
CLOSURES
A
SUBSEQUENT
FAILURE
MAY
FORCE
SHUTDOWN
OF
SEVERAL
POSSIBLY
ALL
BLADES
MULTIPLE
FAILURE
TOLERANCE
TYPICALLY
RE
QUIRES
REDUNDANCY
WHICH
IS
EXPENSIVE
IN
CONTRAST
IN
RAILS
WHERE
PSU
CAPACITY
IS
MATCHED
TO
THE
ACTIVE
POWER
FIGURE
POWER
SUPPLY
PRICING
DRAW
OF
A
SINGLE
BLADE
THE
SECOND
AND
SUBSEQUENT
FAILURES
EACH
REQUIRE
THE
SHUTDOWN
OF
ONLY
ONE
BLADE
EVALUATION
WE
EVALUATE
THE
POWER
EFFICIENCY
AND
COST
OF
POWERNAP
WITH
FOUR
POWER
SUPPLY
DESIGNS
COMMODITY
SUPPLIES
COMMOD
ITY
HIGH
EFFICIENCY
SUPPLIES
DYNAMIC
LOAD
SHARING
DYNAMIC
AND
RAILS
RAILS
WE
EVALU
ATE
ALL
FOUR
DESIGNS
IN
THE
CONTEXT
OF
A
POWERNAP
ENABLED
BLADE
SYSTEM
SIMILAR
TO
HP
BLADE
CENTER
WE
AS
SUME
A
FULLY
POPULATED
CHASSIS
WITH
HALF
HEIGHT
BLADES
EACH
BLADE
CONSUMES
AT
PEAK
AT
IDLE
WITHOUT
POWERNAP
AND
IN
POWERNAP
SEE
TABLE
WE
AS
SUME
THE
BLADE
ENCLOSURE
DRAWS
WE
NEGLECT
ANY
VARI
ATION
IN
CHASSIS
POWER
AS
A
FUNCTION
OF
THE
NUMBER
OF
ACTIVE
BLADES
THE
NON
RAILS
SYSTEMS
EMPLOY
PSUS
SUFFICIENT
TO
PROVIDE
N
REDUNDANCY
THE
RAILS
DESIGN
USES
PSUS
WE
ASSUME
THE
AVERAGE
EFFICIENCY
CHAR
ACTERISTIC
FROM
FIGURE
FOR
COMMODITY
PSUS
COST
SERVER
COMPONENTS
ARE
SOLD
IN
RELATIVELY
LOW
VOL
UMES
COMPARED
TO
DESKTOP
OR
EMBEDDED
PRODUCTS
AND
THUS
COMMAND
PREMIUM
PRICES
SOME
INTERNET
COMPANIES
E
G
GOOGLE
HAVE
ESCHEWED
ENTERPRISE
SERVERS
AND
INSTEAD
AS
SEMBLE
SYSTEMS
FROM
COMMODITY
COMPONENTS
TO
AVOID
THESE
PREMIUMS
PSUS
PRESENT
ANOTHER
OPPORTUNITY
TO
CAPITALIZE
ON
LOW
COST
COMMODITY
COMPONENTS
BECAUSE
DESKTOP
ATX
PSUS
ARE
SOLD
IN
MASSIVE
VOLUMES
THEIR
CONSTITUENT
COMPO
NENTS
ARE
CHEAP
A
MODERATELY
SIZED
SUPPLY
CAN
BE
OBTAINED
AT
EXTREMELY
LOW
COST
FIGURE
SHOWS
A
SURVEY
OF
PSU
PRICES
IN
WATTS
PER
DOLLAR
FOR
A
WIDE
RANGE
OF
PSUS
ACROSS
MARKET
SEGMENTS
PRICE
PER
WATT
INCREASES
RAPIDLY
WITH
POWER
DELIV
ERY
CAPACITY
THIS
RISE
CAN
BE
ATTRIBUTED
TO
THE
PROPORTIONAL
INCREASE
IN
REQUIRED
SIZE
FOR
POWER
COMPONENTS
SUCH
AS
IN
DUCTORS
AND
CAPACITORS
ALSO
THE
PRICE
OF
DISCRETE
POWER
COMPONENTS
GROWS
WITH
SIZE
AND
MAXIMUM
CURRENT
RATING
PRESENTLY
THE
MARKET
SWEET
SPOT
IS
AROUND
SUPPLIES
BOTH
AND
BLADE
SERVER
PSUS
ARE
SUBSTANTIALLY
MORE
EX
TABLE
RELATIVE
PSU
DENSITY
MICROATX
ATX
CUSTOM
BLADE
DENSITY
NORMALIZED
W
VOL
PENSIVE
THAN
COMMODITY
PARTS
BECAUSE
RAILS
USES
COM
MODITY
PSUS
WITH
SMALL
MAXIMUM
OUTPUTS
IT
TAKES
ADVAN
TAGE
OF
PSU
MARKET
ECONOMICS
MAKING
RAILS
FAR
CHEAPER
THAN
PROPRIETARY
BLADE
PSUS
POWER
DENSITY
IN
DATA
CENTERS
RACK
SPACE
IS
AT
A
PREMIUM
AND
HENCE
THE
PHYSICAL
VOLUME
OCCUPIED
BY
A
BLADE
EN
CLOSURE
IS
A
KEY
CONCERN
RAILS
DRASTICALLY
INCREASES
THE
NUMBER
OF
DISTINCT
PSUS
IN
THE
ENCLOSURE
BUT
EACH
PSU
IS
INDIVIDUALLY
SMALLER
TO
CONFIRM
THE
FEASIBILITY
OF
RAILS
WE
HAVE
COMPARED
THE
HIGHEST
POWER
DENSITY
AVAILABLE
IN
COMMODITY
PSUS
WHICH
CONFORM
TO
ONE
OF
SEVERAL
STAN
DARD
FORM
FACTORS
WITH
THAT
OF
PSUS
DESIGNED
FOR
BLADE
CEN
TERS
WHICH
MAY
HAVE
ARBITRARY
DIMENSIONS
TABLE
COM
PARES
THE
POWER
DENSITY
OF
TWO
COMMODITY
FORM
FACTORS
WITH
THE
POWER
DENSITY
OF
HP
PSUS
WE
REPORT
DENSITY
IN
TERMS
OF
WATTS
PER
UNIT
VOLUME
NORMALIZED
TO
THE
VOLUME
OF
ONE
ATX
POWER
SUPPLY
THE
HIGHLY
COMPACT
MICROATX
FORM
FACTOR
EXHIBITS
THE
WORST
POWER
DENSITY
THESE
UNITS
HAVE
BEEN
OPTIMIZED
FOR
SMALL
DIMENSIONS
BUT
ARE
EMPLOYED
IN
SMALL
FORM
FACTOR
DEVICES
THAT
DO
NOT
REQUIRE
HIGH
PEAK
POWER
THOUGH
THEY
ARE
NOT
DESIGNED
FOR
DENSITY
COMMOD
ITY
ATX
SUPPLIES
ARE
ONLY
LESS
DENSE
THAN
ENTERPRISE
CLASS
SUPPLIES
FURTHERMORE
AS
RAILS
REQUIRES
ONLY
A
SINGLE
OUTPUT
VOLTAGE
ELIMINATING
THE
NEED
FOR
MANY
OF
A
STANDARD
ATX
PSU
COMPONENTS
WE
CONCLUDE
THAT
RAILS
PSUS
FIT
WITHIN
BLADE
ENCLOSURE
VOLUMETRIC
CONSTRAINTS
POWER
SAVINGS
AND
ENERGY
EFFICIENCY
TO
EVALUATE
EACH
POWER
SYSTEM
WE
CALCULATE
EXPECTED
POWER
DRAW
AND
CON
VERSION
EFFICIENCY
ACROSS
BLADE
ENSEMBLE
UTILIZATIONS
AS
NOTED
IN
SECTION
LOW
AVERAGE
UTILIZATION
MANIFESTS
AS
BRIEF
BURSTS
OF
ACTIVITY
WHERE
A
SUBSET
OF
BLADES
DRAW
NEAR
PEAK
POWER
THE
EFFICIENCY
OF
EACH
POWER
DELIVERY
SOLUTION
DE
PENDS
ON
HOW
LONG
BLADES
ARE
ACTIVE
AND
HOW
MANY
ARE
SIMULTANEOUSLY
ACTIVE
FOR
EACH
UTILIZATION
WE
CONSTRUCT
A
PROBABILITY
MASS
FUNCTION
FOR
THE
NUMBER
OF
SIMULTANEOUSLY
ACTIVE
BLADES
ASSUMING
UTILIZATION
ACROSS
BLADES
IS
UNCORRE
LATED
HENCE
THE
NUMBER
OF
ACTIVE
BLADES
FOLLOWS
A
BINO
MIAL
DISTRIBUTION
FROM
THE
DISTRIBUTION
OF
ACTIVE
BLADES
WE
COMPUTE
AN
EXPECTED
POWER
DRAW
AND
DETERMINE
CONVERSION
LOSSES
FROM
THE
POWER
SUPPLY
EFFICIENCY
VERSUS
LOAD
CURVE
WE
OBTAIN
EFFICIENCY
CURVES
FROM
THE
ENERGY
STAR
BRONZE
SPECIFICATION
FOR
PSUS
AND
FOR
COMMODITY
PSUS
FIGURE
COMPARES
THE
RELATIVE
EFFICIENCY
OF
POWERNAP
UN
DER
EACH
POWER
DELIVERY
SOLUTION
USING
COMMODITY
COM
MODITY
OR
HIGH
EFFICIENCY
PSUS
RESULTS
IN
THE
LOW
EST
EFFICIENCY
AS
POWERNAP
LOW
POWER
DRAW
WILL
OPERATE
THESE
POWER
SUPPLIES
IN
THE
RED
ZONE
RAILS
RAILS
UTILIZATION
FIGURE
POWER
DELIVERY
SOLUTION
COMPARISON
AND
DYNAMIC
LOAD
SHARING
DYNAMIC
BOTH
IMPROVE
PSU
PERFORMANCE
BECAUSE
THEY
INCREASE
AVERAGE
PSU
LOAD
RAILS
OUTPERFORMS
ALL
OF
THE
OTHER
OPTIONS
BECAUSE
ITS
FINE
GRAIN
SIZING
BEST
MATCHES
POWERNAP
REQUIREMENTS
CONCLUSION
WE
PRESENTED
POWERNAP
A
METHOD
FOR
ELIMINATING
IDLE
POWER
IN
SERVERS
BY
QUICKLY
TRANSITIONING
IN
AND
OUT
OF
AN
ULTRA
LOW
POWER
STATE
WE
HAVE
CONSTRUCTED
AN
ANALYTIC
MODEL
TO
DEMONSTRATE
THAT
FOR
TYPICAL
SERVER
WORKLOADS
POWERNAP
FAR
EXCEEDS
DVFS
POWER
SAVINGS
POTENTIAL
WITH
BETTER
RESPONSE
TIME
BECAUSE
OF
POWERNAP
UNIQUE
POWER
REQUIREMENTS
WE
INTRODUCED
RAILS
A
NOVEL
POWER
DELIVERY
SYSTEM
THAT
IMPROVES
POWER
CONVERSION
EFFICIENCY
PROVIDES
GRACEFUL
DEGRADATION
IN
THE
EVENT
OF
PSU
FAILURES
AND
RE
DUCES
COSTS
TO
CONCLUDE
WE
PRESENT
A
PROJECTION
OF
THE
EFFECTIVENESS
OF
POWERNAP
WITH
RAILS
IN
REAL
COMMERCIAL
DEPLOYMENTS
WE
CONSTRUCT
OUR
PROJECTIONS
USING
THE
COMMERCIAL
HIGH
DENSITY
SERVER
UTILIZATION
TRACES
DESCRIBED
IN
TABLE
TA
BLE
PRESENTS
THE
POWER
REQUIREMENTS
ENERGY
CONVERSION
EF
FICIENCY
AND
TOTAL
POWER
COSTS
FOR
THREE
SERVER
CONFIGURATIONS
AN
UNMODIFIED
MODERN
BLADE
CENTER
SUCH
AS
THE
HP
A
POWERNAP
ENABLED
SYSTEM
WITH
LARGE
CONVENTIONAL
PSUS
POWERNAP
AND
POWERNAP
WITH
RAILS
THE
POWER
COSTS
INCLUDE
THE
ESTIMATED
PURCHASE
PRICE
OF
THE
POWER
DELIVERY
SYSTEM
CONVENTIONAL
HIGH
WATTAGE
PSUS
OR
RAILS
YEAR
POWER
COSTS
ASSUMING
CALIFORNIA
COMMERCIAL
RATE
OF
CENTS
KWH
AND
A
COOLING
BURDEN
OF
PER
OF
IT
EQUIPMENT
POWERNAP
YIELDS
A
STRIKING
REDUCTION
IN
AVERAGE
POWER
RELA
TIVE
TO
BLADE
OF
NEARLY
FOR
WEB
SERVERS
IMPROVING
THE
POWER
SYSTEM
WITH
RAILS
SHAVES
ANOTHER
OUR
TOTAL
POWER
COST
ESTIMATES
DEMONSTRATE
THE
TRUE
VALUE
OF
POWER
NAP
WITH
RAILS
OUR
SOLUTION
PROVIDES
POWER
COST
REDUC
TIONS
OF
NEARLY
FOR
WEB
SERVERS
AND
FOR
ENTER
PRISE
IT
POWER
AWARE
SPEED
SCALING
IN
PROCESSOR
SHARING
SYSTEMS
ADAM
WIERMAN
COMPUTER
SCIENCE
DEPARTMENT
CALIFORNIA
INSTITUTE
OF
TECHNOLOGY
LACHLAN
L
H
ANDREW
CENTRE
FOR
ADVANCED
INTERNET
ARCHITECTURES
SWINBURNE
UNIVERSITY
OF
TECHNOLOGY
AUSTRALIA
AO
TANG
SCHOOL
OF
ECE
CORNELL
UNIVERSITY
ABSTRACT
ENERGY
USE
OF
COMPUTER
COMMUNICATIONS
SYSTEMS
HAS
QUICKLY
BECOME
A
VITAL
DESIGN
CONSIDERATION
ONE
EFFECTIVE
METHOD
FOR
REDUCING
ENERGY
CONSUMPTION
IS
DYNAMIC
SPEED
SCALING
WHICH
ADAPTS
THE
PROCESSING
SPEED
TO
THE
CURRENT
LOAD
THIS
PAPER
STUDIES
HOW
TO
OPTIMALLY
SCALE
SPEED
TO
BALANCE
MEAN
RESPONSE
TIME
AND
MEAN
ENERGY
CONSUMPTION
UNDER
PROCESSOR
SHARING
SCHEDULING
BOTH
BOUNDS
AND
ASYMPTOTICS
FOR
THE
OPTIMAL
SPEED
SCALING
SCHEME
ARE
PROVIDED
THESE
RESULTS
SHOW
THAT
A
SIMPLE
SCHEME
THAT
HALTS
WHEN
THE
SYSTEM
IS
IDLE
AND
USES
A
STATIC
RATE
WHILE
THE
SYSTEM
IS
BUSY
PROVIDES
NEARLY
THE
SAME
PERFORMANCE
AS
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
HOWEVER
THE
RESULTS
ALSO
HIGHLIGHT
THAT
DYNAMIC
SPEED
SCALING
PROVIDE
AT
LEAST
ONE
KEY
BENEFIT
SIGNIFICANTLY
IMPROVED
ROBUSTNESS
TO
BURSTY
TRAFFIC
AND
MIS
ESTIMATION
OF
WORKLOAD
PARAMETERS
INTRODUCTION
POWER
MANAGEMENT
IS
INCREASINGLY
IMPORTANT
IN
COMPUTER
COMMUNICATIONS
SYSTEMS
NOT
ONLY
IS
THE
ENERGY
CONSUMPTION
OF
THE
INTERNET
BECOMING
A
SIGNIFICANT
FRACTION
OF
THE
ENERGY
CONSUMPTION
OF
DEVELOPED
COUNTRIES
BUT
COOLING
IS
ALSO
BECOMING
A
MAJOR
CONCERN
CONSEQUENTLY
THERE
IS
AN
IMPORTANT
TRADEOFF
IN
MODERN
SYSTEM
DESIGN
BETWEEN
REDUCING
ENERGY
USE
AND
MAINTAINING
GOOD
PERFORMANCE
THERE
IS
AN
EXTENSIVE
LITERATURE
ON
POWER
MANAGEMENT
REVIEWED
IN
A
COMMON
TECHNIQUE
WHICH
IS
THE
FOCUS
OF
THE
CURRENT
PAPER
IS
DYNAMIC
SPEED
SCALING
THIS
DYNAMICALLY
REDUCES
THE
PROCESSING
SPEED
AT
TIMES
OF
LOW
WORKLOAD
SINCE
PROCESSING
MORE
SLOWLY
USES
LESS
ENERGY
PER
OPERATION
THIS
IS
NOW
COMMON
IN
MANY
CHIP
DESIGNS
IN
PARTICULAR
SPEED
SCALING
HAS
BEEN
PROPOSED
FOR
MANY
NETWORK
DEVICES
SUCH
AS
SWITCH
FABRICS
TCP
OFFLOAD
ENGINES
AND
OFDM
MODULATION
CLOCKS
THIS
PAPER
STUDIES
THE
EFFICACY
OF
DYNAMIC
SPEED
SCALING
ANALYTICALLY
THE
GOAL
IS
TWOFOLD
I
TO
ELUCIDATE
THE
STRUCTURE
OF
THE
OPTIMAL
SPEED
SCALING
SCHEME
E
G
HOW
SHOULD
THE
SPEED
DEPEND
ON
THE
CURRENT
WORKLOAD
II
TO
COMPARE
THE
PERFORMANCE
OF
DYNAMIC
SPEED
SCALING
DESIGNS
WITH
THAT
OF
DESIGNS
THAT
USE
STATIC
PROCESSING
SPEEDS
E
G
HOW
MUCH
IMPROVEMENT
DOES
DYNAMIC
SPEED
SCALING
PROVIDE
THERE
ARE
MANY
ANALYTIC
STUDIES
OF
SPEED
SCALING
DESIGNS
BEGINNING
WITH
YAO
ET
AL
THE
FOCUS
HAS
BEEN
ON
EITHER
THE
GOAL
OF
MINIMIZING
THE
TOTAL
ENERGY
USED
IN
ORDER
TO
COMPLETE
ARRIVING
JOBS
BY
THEIR
DEADLINES
E
G
OR
THE
GOAL
OF
MINIMIZING
THE
AVERAGE
RESPONSE
TIME
OF
JOBS
I
E
THE
TIME
BETWEEN
THEIR
ARRIVAL
AND
THEIR
COMPLETION
OF
SERVICE
GIVEN
A
SET
ENERGY
HEAT
BUDGET
E
G
WEB
SETTINGS
TYPICALLY
HAVE
NEITHER
JOB
COMPLETION
DEADLINES
NOR
FIXED
ENERGY
BUDGETS
INSTEAD
THE
GOAL
IS
TO
OPTIMIZE
A
TRADEOFF
BETWEEN
ENERGY
CONSUMPTION
AND
MEAN
RESPONSE
TIME
THIS
MODEL
IS
THE
FOCUS
OF
THE
CURRENT
PAPER
IN
PARTICULAR
THE
PERFORMANCE
METRIC
CONSIDERED
IS
E
T
E
E
ΒT
WHERE
T
IS
THE
RESPONSE
TIME
OF
A
JOB
E
IS
THE
EXPECTED
ENERGY
EXPENDED
ON
THAT
JOB
AND
ΒT
CONTROLS
THE
RELATIVE
COST
OF
DELAY
THIS
PERFORMANCE
METRIC
HAS
ATTRACTED
ATTENTION
RECENTLY
THE
RELATED
ANALYTIC
WORK
FALLS
INTO
TWO
CATEGORIES
WORST
CASE
ANALYSES
AND
STOCHASTIC
ANALYSES
THE
FORMER
PROVIDE
SPECIFIC
SIMPLE
SPEED
SCALINGS
GUARANTEED
TO
BE
WITHIN
A
CONSTANT
FACTOR
OF
THE
OPTIMAL
PERFORMANCE
REGARDLESS
OF
THE
WORKLOAD
E
G
IN
CONTRAST
STOCHASTIC
RE
SULTS
HAVE
FOCUSED
ON
SERVICE
RATE
CONTROL
IN
THE
M
M
MODEL
UNDER
FIRST
COME
FIRST
SERVED
FCFS
SCHEDULING
WHICH
CAN
BE
SOLVED
NUMERICALLY
USING
DYNAMIC
PROGRAMMING
ONE
SUCH
APPROACH
IS
REVIEWED
IN
SECTION
III
C
UNFORTUNATELY
THE
STRUCTURAL
INSIGHT
OBTAINED
FROM
STOCHASTIC
MODELS
HAS
BEEN
LIMITED
OUR
WORK
EXTENDS
THE
STOCHASTIC
ANALYSIS
OF
DYNAMIC
SPEED
SCALING
WE
FOCUS
ON
THE
M
GI
QUEUE
UNDER
PROCESSOR
SHARING
PS
SCHEDULING
WHICH
SERVES
ALL
JOBS
CURRENTLY
IN
THE
SYSTEM
AT
EQUAL
RATES
WE
FOCUS
ON
PS
BECAUSE
IT
IS
A
TRACTABLE
MODEL
OF
CURRENT
SCHEDULING
POLICIES
IN
CPUS
WEB
SERVERS
ROUTERS
ETC
BASED
ON
THE
MODEL
SECTION
II
AND
THE
SPEED
SCALING
WE
CONSIDER
SECTION
III
OUR
ANALYSIS
MAKES
THREE
MAIN
CONTRIBUTIONS
WE
PROVIDE
BOUNDS
ON
THE
PERFORMANCE
OF
DYNAMIC
SPEED
SCALING
SECTION
IV
A
SURPRISINGLY
THESE
BOUNDS
SHOW
THAT
EVEN
AN
IDEALIZED
VERSION
OF
DYNAMIC
SPEED
SCALING
IM
PROVES
PERFORMANCE
ONLY
MARGINALLY
COMPARED
TO
A
SIMPLE
SCHEME
WHERE
THE
SERVER
USES
A
STATIC
SPEED
WHEN
BUSY
AND
SPEED
WHEN
IDLE
AT
MOST
A
FACTOR
OF
FOR
TYPICAL
PA
RAMETERS
AND
OFTEN
LESS
SEE
SECTION
V
COUNTERINTUITIVELY
THESE
BOUNDS
ALSO
SHOW
THAT
THE
POWER
OPTIMIZED
RESPONSE
TIME
REMAINS
BOUNDED
AS
THE
LOAD
GROWS
WE
PROVIDE
BOUNDS
AND
ASYMPTOTICS
FOR
THE
SPEEDS
USED
BY
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
SECTIONS
IV
B
AND
IV
C
THESE
RESULTS
PROVIDE
INSIGHT
INTO
HOW
THE
SPEEDS
SCALE
WITH
THE
ARRIVING
LOAD
THE
QUEUE
LENGTH
AND
THE
RELATIVE
COST
OF
ENERGY
FURTHER
THEY
UNCOVER
A
CONNECTION
BETWEEN
THE
OPTIMAL
STOCHASTIC
POLICY
AND
RESULTS
FROM
THE
WORST
CASE
COMMUNITY
SECTION
IV
WE
ILLUSTRATE
THROUGH
ANALYTIC
RESULTS
AND
NUMERICAL
EXPERI
MENTS
THAT
THOUGH
DYNAMIC
SPEED
SCALING
PROVIDES
LIMITED
PERFORMANCE
GAINS
IT
DRAMATICALLY
IMPROVES
ROBUSTNESS
TO
MIS
ESTIMATION
OF
WORKLOAD
PARAMETERS
AND
BURSTY
TRAFFIC
SECTION
VI
MODEL
AND
NOTATION
IN
ORDER
TO
STUDY
THE
PERFORMANCE
OF
DYNAMIC
SPEED
SCALING
WE
FOCUS
ON
A
SIMPLE
MODEL
AN
M
GI
PS
QUEUE
WITH
CONTROLLABLE
SERVICE
RATES
DEPENDENT
ON
THE
QUEUE
LENGTH
IN
THIS
MODEL
JOBS
ARRIVE
TO
THE
SERVER
AS
A
POISSON
PROCESS
WITH
RATE
Λ
HAVE
INTRINSIC
SIZES
WITH
MEAN
Μ
AND
DEPART
AT
RATE
SNΜ
WHEN
THERE
ARE
N
JOBS
IN
THE
SYSTEM
UNDER
STATIC
SCHEMES
THE
CONSTANT
SERVICE
RATE
IS
DENOTED
BY
DEFINE
THE
LOAD
AS
Ρ
Λ
Μ
AND
NOTE
THAT
THIS
Ρ
IS
NOT
THE
FRACTION
OF
TIME
THE
SERVER
IS
BUSY
THE
PERFORMANCE
METRIC
WE
CONSIDER
IS
E
T
E
E
ΒT
WHERE
T
IS
THE
RESPONSE
TIME
OF
A
JOB
AND
E
IS
THE
ENERGY
EXPENDED
ON
A
JOB
IT
IS
OFTEN
CONVENIENT
TO
WORK
WITH
THE
EXPECTED
COST
PER
UNIT
TIME
INSTEAD
OF
PER
JOB
BY
LITTLE
LAW
THIS
CAN
BE
WRITTEN
AS
Z
E
N
ΛE
F
ΒT
WHERE
N
IS
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
AND
F
DETERMINES
THE
POWER
USED
WHEN
RUNNING
AT
SPEED
THE
REMAINING
PIECE
OF
THE
MODEL
IS
TO
DEFINE
THE
FORM
OF
F
PRIOR
LITERATURE
WITH
THE
NOTABLE
EXCEPTION
OF
HAS
TYPICALLY
ASSUMED
THAT
F
IS
CONVEX
AND
OFTEN
THAT
F
IS
A
POLYNOMIAL
SPECIFICALLY
A
CUBIC
THAT
IS
BECAUSE
THE
DYNAMIC
POWER
OF
CMOS
IS
PROPORTIONAL
TO
V
WHERE
V
IS
THE
SUPPLY
VOLTAGE
AND
F
IS
THE
CLOCK
FREQUENCY
OPERATING
AT
A
HIGHER
FREQUENCY
REQUIRES
DYNAMIC
VOLTAGE
SCALING
DVS
TO
A
HIGHER
VOLTAGE
NOMINALLY
WITH
V
F
YIELDING
A
CUBIC
RELATIONSHIP
TO
VALIDATE
THE
POLYNOMIAL
FORM
OF
F
WE
CONSIDER
DATA
FROM
REAL
NM
CHIPS
IN
FIG
THE
VOLTAGE
VERSUS
SPEED
DATA
COMES
FROM
THE
INTEL
PXA
PENTIUM
M
PRO
CESSOR
AND
THE
TCP
OFFLOAD
ENGINE
STUDIED
IN
SPECIFICALLY
THE
NBB
TRACE
AT
C
IN
FIG
INTERESTINGLY
THE
DYNAMIC
POWER
USE
OF
REAL
CHIPS
IS
WELL
MODELED
BY
A
POLYNOMIAL
SCALING
OF
SPEED
TO
POWER
BUT
THIS
POLYNOMIAL
IS
FAR
FROM
CUBIC
IN
FACT
IT
IS
CLOSER
TO
QUADRATIC
INDICATING
THAT
THE
VOLTAGE
IS
SCALED
DOWN
LESS
AGGRESSIVELY
THAN
LINEARLY
WITH
SPEED
AS
A
RESULT
WE
WILL
MODEL
THE
POWER
USED
BY
RUNNING
LOG
FREQ
FIG
DYNAMIC
POWER
FOR
AN
INTEL
PXA
A
TCP
OFFLOAD
ENGINE
AND
A
PENTIUM
M
THE
SLOPES
OF
THE
FITTED
LINES
ARE
AND
RESPECTIVELY
THE
SCOPE
OF
THIS
PAPER
AND
WE
LEAVE
THE
QUESTION
OF
INCLUDING
BOTH
LEAKAGE
AND
DYNAMIC
POWER
FOR
FUTURE
WORK
POWER
AWARE
SPEED
SELECTION
WHEN
PROVISIONING
PROCESSING
SPEED
IN
A
POWER
AWARE
MANNER
THERE
ARE
THREE
NATURAL
THRESHOLDS
IN
THE
CAPABILITY
OF
THE
SERVER
STATIC
PROVISIONING
THE
SERVER
USES
A
CONSTANT
STATIC
SPEED
WHICH
IS
DETERMINED
BASED
ON
WORKLOAD
CHARAC
TERISTICS
SO
AS
TO
BALANCE
ENERGY
USE
AND
RESPONSE
TIME
GATED
STATIC
PROVISIONING
THE
SERVER
GATES
ITS
CLOCK
SETTING
IF
NO
JOBS
ARE
PRESENT
AND
IF
JOBS
ARE
PRESENT
IT
WORKS
AT
A
CONSTANT
RATE
CHOSEN
TO
BALANCE
ENERGY
USE
AND
RESPONSE
TIME
DYNAMIC
SPEED
SCALING
THE
SERVER
ADAPTS
ITS
SPEED
TO
THE
CURRENT
NUMBER
OF
REQUESTS
PRESENT
IN
THE
SYSTEM
THE
GOAL
OF
THIS
PAPER
IS
TO
UNDERSTAND
HOW
TO
CHOOSE
OPTIMAL
SPEEDS
IN
EACH
OF
THESE
SCENARIOS
AND
TO
CONTRAST
THE
RELATIVE
MERITS
OF
EACH
SCHEME
CLEARLY
THE
EXPECTED
COST
IS
REDUCED
EACH
TIME
THE
SERVER
IS
ALLOWED
TO
ADJUST
ITS
SPEED
MORE
DYNAMICALLY
THIS
MUST
BE
TRADED
AGAINST
THE
COSTS
OF
SWITCHING
SUCH
AS
A
DELAY
OF
UP
TO
TENS
OF
MICROSECONDS
TO
CHANGE
SPEEDS
THE
IMPORTANT
QUESTION
IS
WHAT
IS
THE
MAGNITUDE
OF
IMPROVEMENT
AT
EACH
LEVEL
FOR
OUR
COMPARISON
AT
SPEED
BY
Λ
F
ΒT
SΑ
Β
WE
WILL
USE
IDEALIZED
VERSIONS
OF
EACH
SCHEME
IN
PARTICULAR
IN
EACH
CASE
WE
WILL
ASSUME
THAT
THE
SERVER
CAN
BE
RUN
AT
ANY
DESIRED
SPEED
IN
AND
IGNORE
SWITCHING
COSTS
WHERE
Α
AND
Β
TAKES
THE
ROLE
OF
ΒT
BUT
HAS
DIMENSION
TIME
Α
THE
COST
PER
UNIT
TIME
THEN
BECOMES
SΑ
THUS
IN
PARTICULAR
THE
DYNAMIC
SPEED
SCALING
IS
A
SIGNIFICANT
IDEALIZATION
OF
WHAT
IS
POSSIBLE
IN
PRACTICE
HOWEVER
OUR
RESULTS
WILL
SUGGEST
THAT
IT
PROVIDES
VERY
LITTLE
PERFORMANCE
Z
E
N
Β
IMPROVEMENT
OVER
AN
IDEALLY
TUNED
GATED
STATIC
SCHEME
IN
THIS
SECTION
WE
WILL
DERIVE
EXPRESSIONS
FOR
THE
OPTIMAL
WE
WILL
OFTEN
FOCUS
ON
THE
CASE
OF
Α
TO
PROVIDE
INTUITION
CLEARLY
THIS
IS
AN
IDEALIZED
MODEL
SINCE
IN
REALITY
ONLY
A
FEW
DISCRETE
SPEEDS
CAN
BE
USED
THE
IMPACT
OF
THE
WORKLOAD
PARAMETERS
Ρ
Β
AND
Α
CAN
OFTEN
BE
CAPTURED
USING
ONE
SIMPLE
PARAMETER
Γ
Ρ
Α
WHICH
IS
A
DIMENSIONLESS
MEASURE
THUS
WE
WILL
STATE
OUR
RESULTS
IN
TERMS
OF
Γ
TO
SIMPLIFY
THEIR
FORM
ALSO
IT
WILL
OFTEN
BE
CONVENIENT
TO
USE
THE
THE
DIMENSIONLESS
UNIT
OF
SPEED
Α
THOUGH
WE
FOCUS
ON
DYNAMIC
POWER
IN
THIS
PAPER
IT
SHOULD
BE
NOTED
THAT
LEAKAGE
POWER
IS
INCREASINGLY
IMPORTANT
IT
REPRESENTS
OF
THE
POWER
USE
OF
CURRENT
AND
NEAR
FUTURE
CHIPS
HOWEVER
ANALYTIC
MODELS
FOR
LEAKAGE
ARE
MUCH
LESS
UNDERSTOOD
AND
SO
INCLUDING
LEAKAGE
IN
OUR
ANALYSIS
IS
BEYOND
SPEEDS
IN
CASES
I
AND
II
FOR
CASE
III
WE
WILL
DESCRIBE
A
NUMERICAL
APPROACH
FOR
CALCULATING
THE
OPTIMAL
SPEEDS
WHICH
IS
DUE
TO
GEORGE
AND
HARRISON
THOUGH
THIS
NUMERICAL
APPROACH
IS
EFFICIENT
IT
PROVIDES
LITTLE
STRUCTURAL
INSIGHT
INTO
THE
STRUCTURE
OF
THE
DYNAMIC
SPEEDS
OR
THE
OVERALL
PERFORMANCE
PROVIDING
SUCH
RESULTS
WILL
BE
THE
FOCUS
OF
SECTION
IV
THE
OPTIMAL
STATIC
SPEED
THE
SIMPLEST
SYSTEM
TO
MANAGE
POWER
IS
ONE
WHICH
SELECTS
AN
OPTIMAL
SPEED
AND
THEN
ALWAYS
RUNS
THE
PROCESSOR
AT
THAT
SPEED
THIS
CASE
WHICH
WE
CALL
PURE
STATIC
IS
THE
LEAST
POWER
AWARE
SCENARIO
WE
CONSIDER
AND
WILL
BE
USED
SIMPLY
AS
A
BENCHMARK
FOR
COMPARISON
EVEN
WHEN
THE
SPEED
IS
STATIC
THE
OPTIMAL
DESIGN
CAN
BE
POWER
AWARE
SINCE
THE
OPTIMAL
SPEED
CAN
BE
CHOSEN
SO
THAT
IT
TRADES
OFF
THE
COST
OF
RESPONSE
TIME
AND
ENERGY
APPROPRIATELY
IN
PARTICULAR
WE
CAN
WRITE
THE
COST
PER
UNIT
TIME
AS
Z
Ρ
Β
THEN
DIFFERENTIATING
AND
SOLVING
FOR
THE
MINIMIZER
GIVES
THAT
THE
OPTIMUM
OCCURS
WHEN
Ρ
AND
SΑ
Ρ
ΒΡ
Α
THE
OPTIMAL
STATIC
SPEED
FOR
A
GATED
SYSTEM
THE
NEXT
SIMPLEST
SYSTEM
IS
WHEN
THE
PROCESSOR
IS
ALLOWED
TWO
STATES
HALTED
OR
PROCESSING
WE
MODEL
THIS
SITUATION
WITH
A
SERVER
THAT
RUNS
AT
A
CONSTANT
RATE
EXCEPT
WHEN
THERE
ARE
NO
JOBS
IN
THE
SYSTEM
AT
WHICH
POINT
IT
SETS
USING
ZERO
DYNAMIC
POWER
TO
DETERMINE
THE
OPTIMAL
STATIC
SPEED
WE
PROCEED
AS
WE
DID
IN
THE
PREVIOUS
SECTION
IF
THE
SERVER
CAN
GATE
ITS
CLOCK
THE
ENERGY
COST
IS
ONLY
INCURRED
DURING
THE
FRACTION
OF
TIME
THE
SERVER
IS
BUSY
Ρ
THE
COST
PER
UNIT
TIME
THEN
BECOMES
OPTIMAL
DYNAMIC
SPEED
SCALING
A
POPULAR
ALTERNATIVE
TO
STATIC
POWER
MANAGEMENT
IS
TO
ALLOW
THE
SPEED
TO
ADJUST
DYNAMICALLY
TO
THE
NUMBER
OF
REQUESTS
IN
THE
SYSTEM
THE
TASK
OF
DESIGNING
AN
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
IN
OUR
MODEL
CAN
BE
VIEWED
AS
A
STOCHASTIC
CONTROL
PROBLEM
WE
START
WITH
THE
FOLLOWING
OBSERVATION
WHICH
SIMPLIFIES
THE
PROBLEM
DRAMATICALLY
AN
M
GI
PS
SYSTEM
IS
WELL
KNOWN
TO
BE
INSENSITIVE
TO
THE
JOB
SIZE
DISTRIBUTION
THIS
STILL
HOLDS
WHEN
THE
SERVICE
RATE
IS
QUEUE
LENGTH
DEPENDENT
SINCE
THE
POLICY
STILL
FALLS
INTO
THE
CLASS
OF
SYMMETRIC
POLICIES
INTRODUCED
BY
KELLY
AS
A
RESULT
THE
MEAN
RESPONSE
TIME
AND
ENTIRE
QUEUE
LENGTH
DISTRIBUTION
ARE
AFFECTED
BY
THE
SERVICE
DISTRIBUTION
THROUGH
ONLY
ITS
MEAN
THUS
WE
CAN
CONSIDER
AN
M
M
PS
SYSTEM
FURTHER
THE
MEAN
RESPONSE
TIME
AND
ENTIRE
QUEUE
LENGTH
DISTRIBUTION
ARE
EQUIVALENT
UNDER
ALL
NON
SIZE
BASED
SERVICE
DISTRIBUTIONS
IN
THE
M
M
QUEUE
THUS
TO
DETERMINE
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
FOR
AN
M
GI
PS
QUEUE
WE
NEED
ONLY
CONSIDER
AN
M
M
FCFS
QUEUE
THE
SERVICE
RATE
CONTROL
PROBLEM
IN
THE
M
M
FCFS
Z
Ρ
Ρ
SΑ
Β
QUEUE
HAS
BEEN
STUDIED
EXTENSIVELY
IN
PARTIC
ULAR
GEORGE
AND
HARRISON
PROVIDE
AN
ELEGANT
SOLUTION
TO
THE
PROBLEM
OF
SELECTING
THE
STATE
DEPENDENT
PROCESSING
SPEEDS
THE
OPTIMUM
OCCURS
WHEN
Ρ
AND
TO
MINIMIZE
A
WEIGHTED
SUM
OF
AN
ARBITRARY
HOLDING
COST
WITH
A
PROCESSING
SPEED
COST
SPECIFICALLY
THE
OPTIMAL
STATE
DZ
DS
Ρ
Ρ
Α
SΑ
Ρ
Β
DEPENDENT
PROCESSING
SPEEDS
CAN
BE
FRAMED
AS
THE
SOLUTION
TO
A
STOCHASTIC
DYNAMIC
PROGRAM
TO
WHICH
PROVIDES
AN
EFFICIENT
NUMERICAL
SOLUTION
IN
THE
REMAINDER
OF
THIS
SECTION
WHICH
IS
SOLVED
WHEN
Α
SΑ
Ρ
Β
THE
OPTIMAL
SPEED
CAN
BE
SOLVED
FOR
EXPLICITLY
FOR
SOME
Α
FOR
EXAMPLE
WHEN
Α
SGS
Ρ
Β
IN
GENERAL
DEFINE
G
Γ
Α
Σ
T
Σ
Γ
Α
ΣΑ
Γ
Σ
WITH
THIS
NOTATION
THE
OPTIMAL
STATIC
SPEED
FOR
A
SERVER
WHICH
GATES
ITS
CLOCK
IS
SGS
ΑG
Γ
Α
WE
CALL
THIS
POLICY
THE
WE
WILL
PROVIDE
AN
OVERVIEW
OF
THIS
NUMERICAL
APPROACH
THE
CORE
OF
THIS
APPROACH
WILL
FORM
THE
BASIS
OF
OUR
DERIVATION
OF
BOUNDS
ON
THE
OPTIMAL
SPEEDS
IN
SECTION
IV
WE
WILL
DESCRIBE
THE
ALGORITHM
OF
SPECIALIZED
TO
THE
CASE
CONSIDERED
IN
THIS
PAPER
WHERE
THE
HOLDING
COST
IN
STATE
N
IS
SIMPLY
N
FURTHER
WE
WILL
GENERALIZE
THE
DESCRIPTION
TO
ALLOW
ARBITRARY
ARRIVAL
RATES
Λ
THE
SOLUTION
STARTS
WITH
AN
ESTIMATE
Z
OF
THE
MINIMAL
COST
PER
UNIT
TIME
INCLUDING
BOTH
THE
OCCUPANCY
COST
AND
THE
ENERGY
COST
AS
IN
THE
MINIMUM
COST
OF
RETURNING
FROM
STATE
N
TO
THE
EMPTY
SYSTEM
IS
GIVEN
BY
THE
DYNAMIC
PROGRAM
GATED
STATIC
POLICY
AND
DENOTE
THE
CORRESPONDING
COST
ZGS
V
INF
Λ
F
N
ZL
THE
FOLLOWING
LEMMA
BOUNDS
G
THE
PROOF
IS
DEFERRED
TO
APPENDIX
A
LEMMA
FOR
Α
N
A
Λ
ΜS
ΒT
ΜS
Λ
ΜS
VN
Λ
Λ
ΜS
VN
I
Α
Α
WHERE
A
IS
THE
SET
OF
AVAILABLE
SPEEDS
WE
WILL
USUALLY
ASSUME
Γ
Α
G
Γ
Α
Α
Γ
Α
A
WITH
THE
SUBSTITUTION
UN
Λ
VN
VN
THIS
CAN
BE
WRITTEN
AS
AND
THE
INEQUALITIES
ARE
REVERSED
FOR
Α
NOTE
THAT
THE
FIRST
INEQUALITY
BECOMES
TIGHT
FOR
ΓΑ
AND
U
SUP
Z
N
Λ
F
SUN
THE
SECOND
BECOMES
TIGHT
FOR
ΓΑ
FURTHER
WHEN
Α
N
A
ΒT
Ρ
BOTH
BECOME
EQUALITIES
GIVING
G
Γ
Γ
AND
AND
THE
CONDITIONS
ON
Α
HERE
HAVE
BEEN
CORRECTED
SINCE
THE
VERSION
PRESENTED
AT
INFOCOM
TWO
ADDITIONAL
FUNCTIONS
ARE
DEFINED
FIRST
Φ
U
SUP
UX
Ρ
ΛF
X
ΒT
X
A
SECOND
THE
MINIMUM
VALUE
OF
X
WHICH
ACHIEVES
THIS
SUPRE
MUM
NORMALIZED
TO
BE
DIMENSIONLESS
IS
Ψ
U
Β
Α
MIN
X
UX
Ρ
ΛF
X
ΒT
Φ
U
NOTE
THAT
UNDER
IS
CONSTANT
COMPETITIVE
I
E
IN
THE
WORST
CASE
THE
TOTAL
COST
IS
WITHIN
A
CONSTANT
OF
OPTIMAL
THIS
MATCHES
THE
ASYMPTOTIC
BEHAVIOR
OF
THE
BOUNDS
FOR
Α
FOR
LARGE
N
THIS
BEHAVIOR
CAN
ALSO
BE
OBSERVED
FOR
GENERAL
Α
LEMMA
AND
THEOREM
BOUNDS
ON
COST
Φ
U
Α
U
Α
Α
ΑΓ
Ψ
U
U
Α
ΑΓ
WE
START
THE
ANALYSIS
BY
PROVIDING
BOUNDS
ON
Z
IN
THIS
SUBSECTION
AND
THEN
USING
THE
BOUNDS
ON
Z
TO
BOUND
N
ABOVE
GIVEN
THE
ESTIMATE
OF
Z
UN
SATISFY
AND
BELOW
SECTIONS
IV
B
AND
IV
C
RECALL
THAT
ZGS
IS
THE
TOTAL
COST
UNDER
GATED
STATIC
Z
UN
Φ
UN
N
Z
THEOREM
MAX
ΓΑ
ΓΑ
Α
Α
THE
OPTIMAL
VALUE
OF
Z
CAN
BE
FOUND
AS
THE
MINIMUM
VALUE
SUCH
THAT
UN
N
IS
AN
INCREASING
SEQUENCE
THIS
ALLOWS
Z
TO
Z
ZGS
Γ
G
Γ
Α
Γ
ΓG
Γ
Α
Α
BE
FOUND
BY
AN
EFFICIENT
BINARY
SEARCH
AFTER
WHICH
UN
CAN
IN
PRINCIPLE
BE
FOUND
RECURSIVELY
THE
OPTIMAL
SPEED
IN
STATE
N
IS
THEN
GIVEN
BY
PROOF
THE
OPTIMAL
COST
Z
IS
BOUNDED
ABOVE
BY
THE
COST
OF
THE
GATED
STATIC
POLICY
WHICH
IS
SIMPLY
N
Ψ
U
Α
N
ZGS
Γ
G
Γ
Α
Γ
ΓG
Γ
Α
Α
THIS
HIGHLIGHTS
THE
FACT
THAT
Γ
Ρ
Α
PROVIDES
THE
APPRO
PRIATE
SCALING
OF
THE
WORKLOAD
INFORMATION
BECAUSE
THE
COST
Z
NORMALIZED
SPEED
SΒ
Α
AND
VARIABLES
UN
DEPEND
ON
Λ
Μ
TWO
LOWER
BOUNDS
CAN
BE
OBTAINED
AS
FOLLOWS
IN
ORDER
TO
MAINTAIN
STABILITY
THE
TIME
AVERAGE
SPEED
MUST
SATISFY
E
Ρ
BUT
Z
E
SΑ
Β
E
Α
Β
BY
JENSEN
INEQUALITY
AND
THE
CONVEXITY
OF
Α
THUS
AND
Β
ONLY
THROUGH
Γ
NOTE
THAT
THIS
FORWARD
APPROACH
ADVOCATED
IN
IS
NUMERICALLY
UNSTABLE
APPENDIX
B
WE
SUGGEST
THAT
A
MORE
E
SΑ
ΡΑ
Z
Β
Β
ΓΑ
STABLE
WAY
TO
CALCULATE
UN
IS
TO
START
WITH
A
GUESS
FOR
LARGE
N
AND
WORK
BACKWARDS
ERRORS
IN
THE
INITIAL
GUESS
DECAY
EXPONENTIALLY
AS
N
DECREASES
AND
ARE
MUCH
SMALLER
THAN
THE
ACCUMULATED
ROUNDOFF
ERRORS
OF
THE
FORWARD
APPROACH
THIS
FOR
SMALL
LOADS
THIS
BOUND
IS
QUITE
LOOSE
ANOTHER
BOUND
COMES
FROM
CONSIDERING
THE
MINIMUM
COST
OF
PROCESSING
A
SINGLE
JOB
OF
SIZE
X
WITH
NO
WAITING
TIME
OR
PROCESSOR
SHARING
IT
IS
OPTIMAL
TO
SERVE
THE
JOB
AT
A
CONSTANT
RATE
THUS
BACKWARD
APPROACH
IS
MADE
POSSIBLE
BY
THE
BOUNDS
WE
DERIVE
IN
SECTION
IV
Z
EX
MIN
X
SΑ
X
L
BOUNDS
ON
OPTIMAL
DYNAMIC
SPEED
SCALING
IN
THE
PRIOR
SECTION
WE
PRESENTED
THE
OPTIMAL
DESIGNS
FOR
THE
CASES
OF
STATIC
GATED
STATIC
AND
DYNAMIC
SPEED
SCALING
IN
THE
FIRST
TWO
CASES
THE
OPTIMAL
SPEEDS
WERE
PRESENTED
MORE
OR
LESS
EXPLICITLY
HOWEVER
IN
THE
THIRD
CASE
WE
PRESENTED
ONLY
A
RECURSIVE
NUMERICAL
ALGORITHM
FOR
DETERMINING
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
EVEN
THOUGH
THIS
APPROACH
PROVIDES
AN
EFFICIENT
MEANS
TO
CALCULATE
N
IT
IS
DIFFICULT
TO
GAIN
INSIGHT
INTO
SYSTEM
DESIGN
IN
THIS
SECTION
WE
PROVIDE
RESULTS
EXHIBITING
THE
STRUCTURE
OF
THE
OPTIMAL
DYNAMIC
SPEEDS
AND
THE
PERFORMANCE
THEY
ACHIEVE
THE
MAIN
RESULTS
OF
THIS
SECTION
ARE
SUMMARIZED
IN
TABLE
I
THE
BOUNDS
ON
Z
FOR
ARBITRARY
Α
ARE
ESSENTIALLY
TIGHT
I
E
AGREE
TO
LEADING
ORDER
IN
THE
LIMITS
OF
SMALL
OR
LARGE
Γ
DUE
THE
RIGHT
HAND
SIDE
IS
MINIMIZED
FOR
Β
Α
Α
INDEPENDENT
OF
X
GIVING
Z
ΡΒ
ΑΑ
Α
Α
THUS
Z
MAX
ΓΑ
ΓΑ
Α
Α
THE
FORM
OF
THE
BOUNDS
ON
Z
ARE
COMPLICATED
SO
IT
IS
USEFUL
TO
LOOK
AT
THE
PARTICULAR
CASE
OF
Α
COROLLARY
FOR
Α
GATED
STATIC
HAS
COST
WITHIN
A
FACTOR
OF
OF
OPTIMAL
SPECIFICALLY
MAX
Z
ZGS
PROOF
FOR
Α
G
Γ
Γ
HENCE
GIVES
Γ
TO
THE
COMPLICATED
FORM
OF
THE
GENERAL
RESULTS
WE
ILLUSTRATE
THE
BOUNDS
FOR
THE
SPECIFIC
CASE
OF
Α
TO
PROVIDE
INSIGHT
IN
ZGS
Γ
Γ
Γ
Γ
PARTICULAR
IT
IS
EASY
TO
SEE
THE
BEHAVIOR
OF
SN
AND
Z
AS
A
FUNC
TION
OF
Γ
AND
N
IN
THE
CASE
OF
Α
THIS
LEADS
TO
INTERESTING
OBSERVATIONS
FOR
EXAMPLE
IT
ILLUSTRATES
A
CONNECTION
BETWEEN
THE
OPTIMAL
STOCHASTIC
POLICY
AND
POLICIES
ANALYZED
IN
THE
WORST
CASE
MODEL
IN
PARTICULAR
BANSAL
PRUHS
AND
STEIN
SHOWED
THAT
WHEN
NOTHING
IS
KNOWN
ABOUT
FUTURE
ARRIVALS
A
POLICY
THAT
GIVES
SPEEDS
OF
THE
FORM
SN
N
Α
Α
WHICH
ESTABLISHES
THE
UPPER
BOUND
THE
LOWER
BOUND
FOLLOWS
FROM
SUBSTITUTING
Α
INTO
Z
MAX
THE
RATIO
OF
ZGS
TO
THE
LOWER
BOUND
ON
Z
HAS
A
MAXIMUM
VALUE
OF
AT
Γ
AND
HENCE
GATED
STATIC
IS
WITHIN
A
FACTOR
OF
OF
THE
TRUE
OPTIMAL
SCHEME
TABLE
I
BOUNDS
ON
TOTAL
COSTS
AND
SPEED
AS
A
FUNCTION
OF
THE
NUMBER
N
OF
JOBS
IN
THE
SYSTEM
FOR
ANY
Α
MAX
ΓΑ
ΓΑ
Α
Α
Z
Γ
ΓG
Γ
Α
Α
THEOREM
G
Γ
Α
Γ
N
N
ΣΑ
ΓΑ
Γ
Α
WHERE
ΣN
SATISFIES
ΣΑ
Α
ΣN
ΑΓ
N
Γ
G
Γ
Α
Γ
ΓG
Γ
Α
Α
FOR
Α
MAX
Z
COROLLARY
P
N
Γ
Γ
FOR
Α
AND
N
A
LOWER
BOUND
ON
SN
RESULTS
FROM
LINEAR
INTERPOLATION
BETWEEN
MAX
Γ
AT
N
AND
Γ
AT
N
IT
IS
PERHAPS
SURPRISING
THAT
SUCH
AN
IDEALIZED
VERSION
OF
DYNAMIC
SPEED
SCALING
PROVIDES
SUCH
A
SMALL
MAGNITUDE
OF
IMPROVEMENT
OVER
A
SIMPLISTIC
POLICY
SUCH
AS
GATED
STATIC
IN
FACT
THE
BOUND
OF
IS
VERY
LOOSE
WHEN
Γ
IS
LARGE
OR
SMALL
FURTHER
EMPIRICALLY
THE
MAXIMUM
RATIOS
FOR
TYPICAL
Α
ARE
BELOW
SEE
FIG
THUS
THERE
IS
LITTLE
TO
BE
GAINED
BY
UNROLLING
THE
DYNAMIC
PROGRAM
GIVES
A
JOINT
MINIMIZATION
OVER
ALL
SN
U
Ρ
MIN
SΑ
Β
N
Z
SN
SN
Ρ
MIN
SΑ
Β
N
Z
U
L
DYNAMIC
SCALING
IN
TERMS
OF
MEAN
COST
HOWEVER
SECTION
VI
SHOWS
THAT
DYNAMIC
SCALING
DRAMATICALLY
IMPROVES
ROBUSTNESS
SN
SN
TTI
N
Ρ
N
REMAINS
BOUNDED
AS
THE
ARRIVAL
RATE
Λ
GROWS
SPECIFICALLY
BY
AN
UPPER
BOUND
CAN
BE
FOUND
BY
TAKING
ANY
POSSIBLY
SUBOPTIMAL
CHOICE
OF
SN
I
FOR
I
AND
BOUNDING
THE
Z
E
Β
OPTIMAL
Z
TAKING
SI
Α
FOR
ALL
I
N
GIVES
E
T
Λ
Λ
Μ
Β
MIN
N
ΣΑ
Z
Γ
L
UPPER
BOUNDS
ON
THE
OPTIMAL
DYNAMIC
SPEEDS
Γ
Σ
O
Γ
Σ
Γ
WE
NOW
MOVE
TO
PROVIDING
UPPER
BOUNDS
ON
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
THEOREM
FOR
ALL
N
AND
Α
SINCE
Z
ΓΑ
FROM
EQUATION
FOLLOWS
WITH
THIS
ESTABLISHES
FOR
N
HOLDS
SINCE
OTHERWISE
IT
FOLLOWS
FROM
THE
INEQUALITY
ΣΑ
N
ΓN
Α
Α
N
Γ
Α
AND
THE
FACT
THAT
N
Α
N
ΣΑ
ΓΑ
BY
SPECIALIZING
TO
THE
CASE
WHEN
Α
WE
CAN
PROVIDE
UN
Γ
FOR
ALL
Σ
WHENCE
O
Γ
Σ
Γ
SOME
INTUITION
FOR
THE
UPPER
BOUND
ON
THE
SPEEDS
COROLLARY
FOR
Α
N
Γ
Γ
Α
Α
Σ
Γ
O
Γ
Σ
Γ
PROOF
FACTORING
THE
DIFFERENCE
OF
SQUARES
IN
THE
FIRST
TERM
IN
PARTICULAR
FOR
Σ
Γ
Α
OF
AND
CANCELING
WITH
THE
DENOMINATOR
YIELDS
UN
N
Α
Α
Γ
Γ
Α
ΓN
UN
Σ
Γ
Γ
Σ
Γ
Σ
Γ
WHICH
IS
CONCAVE
IN
N
PROOF
AS
EXPLAINED
IN
CAN
BE
REWRITTEN
AS
ONE
TERM
OF
IS
INCREASING
IN
Σ
AND
TWO
ARE
DECREASING
MINIMIZING
PAIRS
OF
THESE
TERMS
GIVES
UPPER
BOUNDS
ON
UN
A
FIRST
BOUND
CAN
BE
OBTAINED
BY
SETTING
Σ
Γ
N
WHICH
MINIMIZES
THE
SUM
OF
THE
FIRST
TWO
TERMS
AND
GIVES
UN
MIN
SΑ
Β
N
UN
Z
L
U
N
BY
THIS
GIVES
A
BOUND
ON
THE
OPTIMAL
SPEEDS
OF
THEOREM
THE
SCALED
SPEED
ΣN
N
Α
SATISFIES
N
Γ
ΣΑ
Α
Σ
ΑΓ
N
Γ
ΓG
Γ
Α
Α
A
SECOND
BOUND
COMES
BY
MINIMIZING
THE
SUM
OF
THE
SECOND
AND
THIRD
TERMS
WHEN
Σ
Γ
THIS
GIVES
PROOF
NOTE
THAT
UN
UN
THUS
BY
Α
UN
U
N
Z
Α
Α
ΓN
UN
Γ
Γ
ΑΓ
Α
Α
N
BY
THIS
CAN
BE
EXPRESSED
IN
TERMS
OF
N
AS
WHICH
UPON
DIVISION
BY
GIVES
ΑΓ
N
Α
Α
N
Α
N
Z
N
Γ
Α
Β
THE
MINIMUM
OF
THE
RIGHT
HAND
SIDES
OF
AND
IS
A
BOUND
ON
SN
N
Α
Α
Α
N
Α
ΑΓ
N
Z
THE
RESULT
THEN
FOLLOWS
FROM
THE
FACT
THAT
AND
THE
RESULT
FOLLOWS
FROM
SINCE
Z
Z
Γ
Γ
N
FOR
Α
GS
THE
ABOVE
THEOREM
CAN
BE
EXPRESSED
MORE
N
WHICH
FOLLOWS
FROM
TAKING
THE
SQUARE
ROOT
OF
THE
FIRST
INEQUAL
EXPLICITLY
AS
FOLLOWS
COROLLARY
FOR
Α
AND
ANY
N
ITY
AND
REARRANGING
FACTORS
LOWER
BOUNDS
ON
THE
OPTIMAL
DYNAMIC
SPEEDS
N
Α
Γ
N
FINALLY
WE
PROVE
LOWER
BOUNDS
ON
THE
DYNAMIC
SPEED
SCALING
SCHEME
WE
BEGIN
BY
BOUNDING
THE
SPEED
USED
WHEN
THERE
IS
ONE
JOB
IN
THE
SYSTEM
THE
FOLLOWING
RESULT
IS
AN
IMMEDIATE
CONSEQUENCE
OF
COROLLARY
AND
PROOF
FOR
Α
CAN
BE
SOLVED
EXPLICITLY
GIVING
UN
N
Z
SINCE
UN
BY
COROLLARY
FOR
Α
N
Γ
N
Z
Γ
Γ
Α
MAX
Β
AND
SUBSTITUTING
Z
FROM
GIVES
THE
RESULT
OBSERVE
THAT
THE
BOUNDS
IN
LIKE
THOSE
IN
COROLLARY
ARE
ESSENTIALLY
TIGHT
FOR
BOTH
LARGE
AND
SMALL
Γ
BUT
LOOSE
FOR
Γ
NEAR
ESPECIALLY
THE
LOWER
BOUND
NEXT
WE
WILL
PROVE
A
BOUND
ON
N
FOR
LARGE
N
LEMMA
FOR
SUFFICIENTLY
LARGE
N
THERE
ARE
TWO
IMPORTANT
OBSERVATIONS
ABOUT
THE
ABOVE
COROLLARY
FIRST
THE
COROLLARY
ONLY
APPLIES
WHEN
Ρ
AND
HENCE
AFTER
THE
MODE
OF
THE
DISTRIBUTION
HOWEVER
IT
ALSO
PROVES
THAT
THE
MODE
OCCURS
AT
N
SECOND
THE
COROLLARY
ONLY
APPLIES
WHEN
N
IN
THIS
CASE
WE
CAN
SIMPLIFY
THE
UPPER
BOUND
ON
SN
IN
AND
COMBINE
IT
WITH
TO
OBTAIN
N
Α
PROOF
REARRANGE
AS
UN
N
Z
UN
Α
Α
N
Α
Α
WHEN
THIS
FORM
HOLDS
IT
IS
TIGHT
FOR
LARGE
N
AND
OR
LARGE
Γ
FINALLY
NOTE
THAT
IN
THE
CASE
WHEN
N
THE
ONLY
BOUNDS
WE
HAVE
ON
THE
OPTIMAL
SPEEDS
ARE
WHERE
THE
INEQUALITY
USES
THE
FACT
THAT
THE
UN
IS
NON
DECREASING
HENCE
UNBOUNDED
AS
N
IS
UNBOUNDED
WHENCE
UN
Z
FOR
LARGE
N
APPLYING
N
Α
UN
ΑΓ
Α
GIVES
THIS
RESULT
HIGHLIGHTS
THE
CONNECTION
BETWEEN
THE
OPTIMAL
STOCHASTIC
POLICY
AND
PRIOR
POLICIES
ANALYZED
IN
THE
WORST
CASE
MODEL
THAT
WE
MENTIONED
AT
THE
BEGINNING
OF
THIS
SECTION
SPECIFICALLY
COMBINING
WITH
AND
SHOWS
THAT
SPEEDS
CHOSEN
TO
PERFORM
WELL
IN
THE
WORST
CASE
ARE
ASYMPTOT
ICALLY
OPTIMAL
FOR
LARGE
N
IN
THE
STOCHASTIC
MODEL
HOWEVER
NOTE
THAT
THE
PROBABILITY
OF
N
BEING
LARGE
IS
SMALL
NEXT
WE
CAN
DERIVE
A
TIGHTER
ALBEIT
IMPLICIT
BOUND
ON
THE
OPTIMAL
SPEEDS
THAT
N
IS
INCREASING
IN
N
THE
FOLLOWING
LEMMA
PROVES
THAT
AN
IMPROVED
LOWER
BOUND
CAN
BE
ATTAINED
BY
INTERPOLATING
LINEARLY
BETWEEN
MAX
Γ
AND
Γ
LEMMA
THE
SEQUENCE
UN
IS
STRICTLY
CONCAVE
INCREASING
PROOF
LET
P
N
BE
THE
PROPOSITION
UN
UN
UN
UN
STRICT
CONCAVITY
OF
UN
IS
EQUIVALENT
TO
THERE
BEING
NO
N
FOR
WHICH
P
N
HOLDS
SINCE
UN
IS
NON
DECREASING
AND
THERE
EXISTS
AN
UPPER
BOUND
ON
UN
WITH
GRADIENT
TENDING
TO
IT
IS
SUFFICIENT
TO
SHOW
THAT
P
N
IMPLIES
P
N
IF
SO
THEN
ANY
LOCAL
NON
CONCAVITY
WOULD
IMPLY
CONVEXITY
FROM
THAT
POINT
ONWARDS
IN
WHICH
CASE
ITS
LONG
TERM
GRADIENT
WOULD
BE
POSITIVE
AND
BOUNDED
AWAY
FROM
ZERO
AND
HENCE
UN
WOULD
EVENTUALLY
VIOLATE
THE
UPPER
BOUND
BY
U
IDENTITY
N
UN
Φ
UN
Φ
U
N
WITH
THIS
P
N
IS
EQUIVALENT
TO
Φ
UN
Φ
UN
UN
UN
THIS
IMPLIES
UN
UN
AND
Φ
UN
Φ
UN
ABSOLUTE
COSTS
Α
RATIO
OF
COST
FOR
GATED
STATIC
TO
OPTIMAL
ZGS
Z
NOTE
THAT
THE
FIRST
FACTOR
IS
POSITIVE
SINCE
THE
SECOND
FACTOR
IS
POSITIVE
SINCE
Φ
IS
CONVEX
THERE
IS
A
SUBGRADIENT
G
DEFINED
AT
EACH
POINT
THIS
GIVES
SHOWS
THAT
THE
GATED
STATIC
I
E
THE
UPPER
BOUND
HAS
VERY
CLOSE
TO
THE
OPTIMAL
COST
IN
ADDITION
TO
COMPARING
THE
TOTAL
COST
OF
THE
SCHEMES
IT
IS
IMPORTANT
TO
CONTRAST
THE
MEAN
RESPONSE
TIME
AND
MEAN
Φ
UN
Φ
UN
G
U
UN
UN
Φ
UN
Φ
UN
UN
UN
ENERGY
USE
FIGURE
SHOWS
THE
BREAKDOWN
A
REFERENCE
LOAD
OF
Ρ
WITH
DELAY
AVERSION
Β
AND
POWER
SCALING
Α
THIS
AND
IMPLY
THAT
BOTH
OF
THE
FACTORS
OF
IN
CREASE
WHEN
GOING
FROM
P
N
TO
P
N
ESTABLISHING
P
N
AND
THE
STRICT
CONCAVITY
OF
UN
SINCE
IT
IS
ALSO
NON
DECREASING
THE
RESULT
FOLLOWS
COMPARING
STATIC
AND
DYNAMIC
SCHEMES
TO
THIS
POINT
WE
HAVE
ONLY
PROVIDED
ANALYTIC
RESULTS
WE
NOW
USE
NUMERICAL
EXPERIMENTS
TO
CONTRAST
STATIC
AND
DYNAMIC
SCHEMES
IN
ADDITION
THESE
EXPERIMENTS
WILL
ILLUSTRATE
THE
TIGHTNESS
OF
THE
BOUNDS
PROVEN
IN
SECTION
IV
ON
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
SCHEME
WE
WILL
START
BY
CONTRASTING
THE
OPTIMAL
SPEEDS
UNDER
EACH
OF
THE
SCHEMES
FIGURE
COMPARES
THE
OPTIMAL
DYNAMIC
SPEEDS
WITH
THE
OPTIMAL
STATIC
SPEEDS
NOTE
THAT
THE
BOUNDS
ON
THE
DYNAMIC
SPEEDS
ARE
QUITE
TIGHT
ESPECIALLY
WHEN
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
N
IS
LARGE
FOR
REFERENCE
THE
MODES
OF
THE
OCCUPANCY
DISTRIBUTIONS
ARE
ABOUT
AND
CLOSE
TO
THE
POINTS
AT
WHICH
THE
OPTIMAL
SPEED
MATCHES
THE
STATIC
SPEEDS
NOTE
ALSO
THAT
THE
OPTIMAL
RATE
GROWS
ONLY
SLOWLY
FOR
N
MUCH
LARGER
THAN
THE
TYPICAL
OCCUPANCY
THIS
IS
IMPORTANT
SINCE
THE
RANGE
OVER
WHICH
DVS
IS
POSSIBLE
IS
LIMITED
ALTHOUGH
THE
SPEED
OF
THE
OPTIMAL
SCHEME
DIFFERS
SIGNIF
ICANTLY
FROM
THAT
OF
GATED
STATIC
THE
ACTUAL
COSTS
ARE
VERY
SIMILAR
AS
PREDICTED
BY
THE
REMARK
AFTER
COROLLARY
THIS
IS
SHOWN
IN
FIG
THE
BOUNDS
ON
THE
OPTIMAL
SPEED
ARE
ALSO
VERY
TIGHT
BOTH
FOR
LARGE
AND
SMALL
Γ
PART
A
SHOWS
THAT
THE
LOWER
BOUND
IS
LOOSEST
FOR
INTERMEDIATE
Γ
WHERE
THE
WEIGHTS
GIVEN
TO
POWER
AND
RESPONSE
TIME
ARE
COMPARABLE
PART
B
WAS
COMPARED
AGAINST
CHANGING
Ρ
FOR
FIXED
Γ
CHANGING
Β
FOR
FIXED
Ρ
AND
CHANGING
Α
NOTE
Γ
WAS
CHOSEN
TO
MAXIMIZE
THE
RATIO
OF
ZGS
Z
THE
SECOND
SCENARIO
SHOWS
THAT
WHEN
Γ
IS
HELD
FIXED
BUT
THE
LOAD
Ρ
IS
REDUCED
AND
DELAY
AVERSION
IS
REDUCED
COMMENSURATELY
THE
ENERGY
CONSUMPTION
BECOMES
NEGLIGIBLE
ROBUST
POWER
AWARE
DESIGN
WE
HAVE
SEEN
BOTH
ANALYTICALLY
AND
NUMERICALLY
THAT
IDE
ALIZED
DYNAMIC
SPEED
SCALING
ONLY
MARGINALLY
REDUCES
THE
COST
COMPARED
TO
THE
SIMPLE
GATED
STATIC
THIS
RAISES
THE
QUESTION
OF
WHETHER
DYNAMIC
SCALING
IS
WORTH
THE
COMPLEXITY
THIS
SECTION
ILLUSTRATES
ONE
REASON
ROBUSTNESS
SPECIFICALLY
DYNAMIC
SCHEMES
PROVIDE
SIGNIFICANTLY
BETTER
PERFORMANCE
IN
THE
FACE
OF
BURSTY
TRAFFIC
AND
MIS
ESTIMATION
OF
WORKLOAD
WE
FOCUS
ON
ROBUSTNESS
WITH
RESPECT
TO
THE
LOAD
Ρ
THE
OPTIMAL
SPEEDS
ARE
SENSITIVE
TO
Ρ
BUT
IN
REALITY
THIS
PARAMETER
MUST
BE
ESTIMATED
AND
WILL
BE
TIME
VARYING
IT
IS
EASY
TO
SEE
THE
PROBLEMS
MIS
ESTIMATION
OF
Ρ
CAUSES
FOR
STATIC
SPEED
DESIGNS
IF
THE
LOAD
IS
NOT
KNOWN
THEN
THE
SELECTED
SPEED
MUST
BE
SATISFACTORY
FOR
ALL
POSSIBLE
ANTICIPATED
LOADS
CONSIDER
THE
CASE
THAT
IT
IS
ONLY
KNOWN
THAT
Ρ
Ρ
Ρ
LET
Z
DENOTE
THE
EXPECTED
COST
PER
UNIT
TIME
IF
THE
ARRIVAL
RATE
IS
BUT
THE
SPEED
WAS
OPTIMIZED
FOR
THEN
THE
ROBUST
DESIGN
PROBLEM
IS
TO
SELECT
THE
SPEED
ΡT
SUCH
THAT
MIN
MAX
Z
Ρ
ΡT
ΡL
Ρ
Ρ
Ρ
THE
OPTIMAL
DESIGN
IS
TO
PROVISION
FOR
THE
HIGHEST
FORESEEN
LOAD
I
E
MAXΡ
Ρ
Ρ
Z
Ρ
ΡT
Z
Ρ
ΡT
HOWEVER
THIS
IS
WASTEFUL
IN
THE
TYPICAL
CASE
THAT
THE
LOAD
IS
LESS
THAN
Ρ
OCCUPANCY
N
Γ
OCCUPANCY
N
Γ
THE
FRAGILITY
OF
STATIC
SPEED
DESIGNS
IS
ILLUSTRATED
IN
FIG
WHICH
SHOWS
THAT
WHEN
SPEED
IS
UNDERPROVISIONED
THE
SERVER
IS
UNSTABLE
AND
WHEN
IT
IS
OVERPROVISIONED
THE
DESIGN
IS
WASTEFUL
OPTIMAL
DYNAMIC
SCALING
IS
NOT
IMMUNE
TO
MIS
ESTIMATION
OF
Ρ
SINCE
N
IS
HIGHLY
DEPENDENT
ON
Ρ
HOWEVER
BECAUSE
THE
SPEED
ADAPTS
TO
THE
QUEUE
LENGTH
DYNAMIC
SCALING
IS
MORE
ROBUST
FIGURE
SHOWS
THIS
IMPROVEMENT
THIS
ROBUSTNESS
IS
IMPROVED
FURTHER
BY
THE
SPEED
SCALING
FIG
RATE
VS
N
FOR
Α
AND
DIFFERENT
ENERGY
AWARE
LOAD
Γ
SCHEME
WHICH
WE
TERM
LINEAR
THAT
SCALES
THE
SERVER
SPEED
DESIGN
FIG
BREAKDOWN
OF
E
T
AND
E
SΑ
FOR
SEVERAL
SCENARIOS
IN
PROPORTION
TO
THE
QUEUE
LENGTH
I
E
SN
Α
N
NOTE
FIG
COST
AT
LOAD
Β
Α
Ρ
WHEN
SPEEDS
ARE
DESIGNED
FOR
DESIGN
Ρ
USING
THAT
UNDER
THIS
SCALING
THE
QUEUE
IS
EQUIVALENT
TO
AN
M
GI
QUEUE
WITH
HOMOGENEOUS
SERVERS
FIGURE
SHOWS
THAT
LINEAR
SCALING
PROVIDES
SIGNIFICANTLY
BETTER
ROBUSTNESS
THAN
THE
OPTI
MAL
DYNAMIC
SCHEME
INDEED
THE
OPTIMAL
SCHEME
IS
ONLY
AND
ZGS
E
E
Β
EΡ
Ρ
Ρ
Β
Β
Β
E
OPTIMAL
FOR
DESIGNS
WITH
Ρ
EVEN
THEN
ITS
COST
IS
ONLY
SLIGHTLY
LOWER
THAN
THAT
OF
LINEAR
SCALING
THE
SIGNIFICANT
WE
CAN
FURTHER
RELATE
ZGS
TO
ZLIN
BY
EΡ
Ρ
Ρ
PRICE
THAT
LINEAR
SCALING
PAYS
IS
THAT
IT
REQUIRES
VERY
HIGH
PROCESSING
SPEED
WHEN
THE
OCCUPANCY
IS
HIGH
WHICH
MAY
NOT
BE
SUPPORTED
BY
THE
HARDWARE
WE
NOW
COMPARE
THE
ROBUSTNESS
ANALYTICALLY
IN
THE
CASE
OF
Α
FIRST
WE
WILL
SHOW
THAT
IF
Ρ
IS
KNOWN
THE
COST
OF
THE
ZGS
ZLIN
FROM
WHICH
FOLLOWS
Β
Β
E
Β
EΡ
EΡ
Β
Β
Β
E
LINEAR
SCHEME
IS
EXACTLY
THE
SAME
AS
THE
COST
OF
THE
GATED
STATIC
SCHEME
AND
THUS
WITHIN
A
FACTOR
OF
OF
OPTIMAL
THEOREM
THEN
WE
WILL
SHOW
THAT
WHEN
THE
TARGET
LOAD
DIFFERS
FROM
THE
ACTUAL
LOAD
THE
LINEAR
SCHEME
SIGNIFICANTLY
REDUCES
THE
COST
THEOREM
IN
PARTICULAR
THE
LINEAR
SCALING
SCHEME
HAS
COST
INDEPENDENT
OF
THE
DIFFERENCE
BETWEEN
THE
DESIGN
AND
ACTUAL
Ρ
IN
CONTRAST
THE
COST
OF
GATED
STATIC
GROWS
LINEARLY
IN
THIS
DIFFERENCE
AS
SEEN
IN
FIG
THEOREM
WHEN
Α
ZGS
ZLIN
THUS
ZLIN
PROOF
IF
THE
SPEED
IN
STATE
N
IS
KN
THEN
THIS
INSENSITIVITY
TO
DESIGN
LOAD
MIRRORS
WORST
CASE
ANALYSIS
THE
OPTIMUM
AVAILABLE
SCALING
WHICH
DESIGNS
FOR
Ρ
IS
O
WORST
CASE
COMPETITIVE
HOWEVER
FIG
SUGGESTS
THAT
LINEAR
SCALING
IS
MUCH
BETTER
THAN
DESIGNING
FOR
Ρ
TIGHTER
BOUNDS
ARE
KNOWN
FOR
SN
Α
BUT
THOSE
ARE
STILL
LOOSER
THAN
THEOREM
CONCLUDING
REMARKS
SPEED
SCALING
IS
AN
IMPORTANT
METHOD
FOR
REDUCING
ENERGY
CONSUMPTION
IN
COMPUTER
COMMUNICATION
SYSTEMS
INTRINSI
CALLY
IT
TRADES
OFF
THE
MEAN
RESPONSE
TIME
AND
THE
MEAN
ENERGY
N
CONSUMPTION
AND
THIS
PAPER
PROVIDES
INSIGHT
INTO
THIS
TRADEOFF
E
N
Ρ
E
KN
Ρ
K
E
Ρ
K
ΡK
USING
A
STOCHASTIC
ANALYSIS
AND
SO
THE
TOTAL
COST
IS
OPTIMIZED
FOR
K
Β
IN
THIS
CASE
E
Ρ
Ρ
ASYMPTOTICS
FOR
THE
OPTIMAL
SPEED
SCALING
SCHEME
ARE
PROVIDED
THESE
BOUNDS
ARE
TIGHT
FOR
SMALL
AND
LARGE
Γ
AND
PROVIDE
A
NUMBER
OF
INSIGHTS
E
G
THAT
THE
MEAN
RESPONSE
TIME
IS
WHICH
IS
IDENTICAL
TO
THE
COST
FOR
GATED
STATIC
BY
COROLLARY
THIS
IS
WITHIN
A
FACTOR
OF
OF
Z
THEOREM
CONSIDER
A
SYSTEM
DESIGNED
FOR
TARGET
LOAD
ΡT
THAT
IS
OPERATING
AT
LOAD
Ρ
ΡT
E
WHEN
Α
Ρ
SCALING
AND
THAT
THE
OPTIMAL
DYNAMIC
SPEEDS
IN
THE
STOCHASTIC
MODEL
MATCH
FOR
LARGE
N
DYNAMIC
SPEED
SCALINGS
THAT
HAVE
BEEN
SHOWN
TO
HAVE
GOOD
WORST
CASE
PERFORMANCE
SURPRISINGLY
THE
BOUNDS
ALSO
ILLUSTRATE
THAT
A
SIMPLE
SCHEME
WHICH
GATES
THE
CLOCK
WHEN
THE
SYSTEM
IS
IDLE
AND
USES
A
STATIC
RATE
OTHERWISE
PROVIDES
PERFORMANCE
WITHIN
A
FACTOR
OF
OF
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
HOWEVER
THE
VALUE
OF
ZLIN
Β
Β
Ρ
DYNAMIC
SPEED
SCALING
IS
ALSO
ILLUSTRATED
DYNAMIC
SPEED
SCALING
SCHEMES
PROVIDE
SIGNIFICANTLY
IMPROVED
ROBUSTNESS
TO
BURSTY
TRAFFIC
AND
MIS
ESTIMATION
OF
WORKLOAD
PARAMETERS
PROOF
THE
OPTIMAL
RATES
FOR
THE
LINEAR
POLICY
ARE
SN
N
Β
INDEPENDENT
OF
ΡT
THUS
ITS
COST
IS
ALWAYS
THE
OPTIMAL
SPEED
FOR
GATED
STATIC
IN
THIS
CASE
IS
SN
ΡT
Β
FOR
N
WHEN
OPERATED
AT
ACTUAL
LOAD
Ρ
THIS
GIVES
E
Ρ
E
ΡΡT
Ρ
Β
ΡT
Ρ
Β
Β
Β
LONGER
OPTIMAL
WHEN
ROBUSTNESS
IS
CONSIDERED
A
SCHEME
THAT
SCALES
SPEEDS
LINEARLY
WITH
N
PROVIDES
SIGNIFICANTLY
IMPROVED
ROBUSTNESS
WHILE
INCREASING
COST
ONLY
SLIGHTLY
THERE
ARE
A
NUMBER
OF
RELATED
DIRECTIONS
IN
WHICH
TO
EXTEND
THIS
WORK
FOR
EXAMPLE
WE
HAVE
ONLY
CONSIDERED
DYNAMIC
POWER
CONSUMPTION
WHICH
CAN
BE
MODELED
AS
A
POLYNOMIAL
OF
THE
SPEED
HOWEVER
THE
CONTRIBUTION
OF
LEAKAGE
POWER
IS
GROWING
AND
AN
IMPORTANT
EXTENSION
IS
TO
DEVELOP
MODELS
OF
TOTAL
POWER
USE
THAT
CAN
BE
USED
FOR
ANALYSIS
ALSO
IT
WILL
BE
VERY
INTERESTING
TO
EXTEND
THE
ANALYSIS
TO
SCHEDULING
POLICIES
BEYOND
PS
FOR
EXAMPLE
GIVEN
THAT
THE
SPEED
CAN
BE
REDUCED
IF
THERE
ARE
FEWER
JOBS
IN
THE
SYSTEM
IT
IS
NATURAL
TO
SUGGEST
SCHEDULING
ACCORDING
TO
SHORTEST
REMAINING
PROCESSING
TIME
FIRST
SRPT
W
HICH
I
K
NOWN
T
O
M
INIMIZE
T
HE
N
UMBER
O
F
JOBS
IN
THE
SYSTEM
OPTIMAL
POWER
DOWN
STRATEGIES
JOHN
AUGUSTINE
SCHOOL
OF
INFORMATION
AND
COMPUTER
SCIENCE
UNIV
OF
CALIFORNIA
AT
IRVINE
IRVINE
CA
SANDY
IRANI
SCHOOL
OF
INFORMATION
AND
COMPUTER
SCIENCE
UNIV
OF
CALIFORNIA
AT
IRVINE
IRVINE
CA
CHAITANYA
SWAMYY
CENTER
FOR
THE
MATHEMATICS
OF
INFORMATION
CALTECH
PASADENA
CA
ABSTRACT
WE
CONSIDER
THE
PROBLEM
OF
SELECTING
THRESHOLD
TIMES
TO
TRANSITION
A
DEVICE
TO
LOW
POWER
SLEEP
STATES
DURING
AN
IDLE
PERIOD
THE
TWO
STATE
CASE
IN
WHICH
THERE
IS
A
SINGLE
ACTIVE
AND
A
SINGLE
SLEEP
STATE
IS
A
CONTINUOUS
VERSION
OF
THE
SKI
RENTAL
PROBLEM
WE
CONSIDER
A
GENERALIZED
VERSION
IN
WHICH
THERE
IS
MORE
THAN
ONE
SLEEP
STATE
EACH
WITH
ITS
OWN
POWER
CONSUMPTION
RATE
AND
TRANSITION
COSTS
WE
GIVE
AN
ALGORITHM
THAT
GIVEN
A
SYSTEM
PRODUCES
A
DETERMINISTIC
STRATEGY
WHOSE
COMPETITIVE
RATIO
IS
ARBITRARILY
CLOSE
TO
OPTIMAL
WE
ALSO
GIVE
AN
ALGORITHM
TO
PRODUCE
THE
OPTIMAL
ONLINE
STRATEGY
GIVEN
A
SYSTEM
AND
A
PROBABILITY
DISTRIBUTION
THAT
GENERATES
THE
LENGTH
OF
THE
IDLE
PERIOD
WE
ALSO
GIVE
A
SIMPLE
ALGORITHM
THAT
ACHIEVES
A
COMPETITIVE
RATIO
OF
P
ANY
SYSTEM
INTRODUCTION
SUPPOSE
YOU
ARE
ABOUT
TO
GO
SKIING
FOR
THE
FIRST
TIME
IN
YOUR
LIFE
NATURALLY
YOU
ASK
YOURSELF
WHETHER
TO
RENT
SKIS
OR
TO
BUY
THEM
RENTING
SKIS
COSTS
SAY
WHEREAS
BUYING
SKIS
COSTS
IF
YOU
KNEW
HOW
MANY
TIMES
YOU
WOULD
GO
SKIING
IN
THE
FUTURE
IGNORING
COMPLICATING
FACTORS
SUCH
AS
INFLATION
AND
CHANGING
MODELS
OF
SKIS
THEN
YOUR
CHOICE
WOULD
BE
CLEAR
IF
YOU
KNEW
YOU
WOULD
GO
AT
LEAST
TIMES
YOU
WOULD
BE
FINANCIALLY
BETTER
OFF
BY
BUYING
SKIS
RIGHT
FROM
THE
BEGINNING
WHEREAS
IF
YOU
KNEW
YOU
WOULD
GO
LESS
THAN
TIMES
YOU
WOULD
BE
BETTER
OFF
RENTING
SKIS
EVERY
TIME
ALAS
THE
FUTURE
IS
UNCLEAR
AND
YOU
MUST
MAKE
A
DECISION
NONETHELESS
ALTHOUGH
THE
SKI
RENTAL
PROBLEM
IS
A
VERY
SIMPLE
ABSTRACTION
THIS
BASIC
PARADIGM
ARISES
IN
MANY
AP
PLICATIONS
IN
COMPUTER
SYSTEMS
IN
THESE
SITUATIONS
THERE
IS
A
SYSTEM
THAT
CAN
RESIDE
IN
EITHER
A
LOW
COST
OR
A
HIGH
COST
STATE
OCCASIONALLY
IT
IS
FORCED
TO
BE
IN
THE
HIGH
COST
STATE
USUALLY
TO
PERFORM
SOME
TASK
A
PERIOD
BETWEEN
ANY
TWO
SUCH
POINTS
IN
TIME
IS
CALLED
AN
IDLE
PERIOD
THE
SYSTEM
PAYS
A
PER
TIME
UNIT
COST
TO
RESIDE
IN
THE
HIGH
COST
STATE
ALTERNATIVELY
IT
CAN
TRANSITION
TO
THE
LOW
COST
STATE
AT
A
FIXED
ONE
TIME
COST
IF
THE
IDLE
PERIOD
IS
LONG
IT
IS
ADVANTAGEOUS
TO
TRANSITION
TO
THE
LOW
COST
STATE
IMMEDIATELY
IF
THE
IDLE
PERIOD
IS
SHORT
IT
IS
BETTER
TO
STAY
IN
THE
HIGH
COST
STATE
AN
ONLINE
ALGORITHM
WHICH
DOES
NOT
KNOW
THE
LENGTH
OF
THE
IDLE
PERIOD
MUST
BALANCE
THESE
TWO
POSSIBILITIES
THIS
PROBLEM
HAS
BEEN
STUDIED
IN
THE
CONTEXT
OF
SHARED
MEMORY
MULTIPROCESSORS
IN
WHICH
A
THREAD
IS
WAITING
FOR
A
LOCKED
PIECE
OF
DATA
AND
MUST
DECIDE
WHETHER
TO
SPIN
OR
BLOCK
RESEARCHERS
INVESTIGATING
RESEARCH
SUPPORTED
PARTIALLY
BY
NSF
GRANTS
CCR
AND
CCF
AND
BY
ONR
AWARD
YWORK
DONE
WHILE
THE
AUTHOR
WAS
A
STUDENT
AT
THE
DEPARTMENT
OF
COMPUTER
SCIENCE
CORNELL
UNIVERSITY
ITHACA
NY
RESEARCH
SUPPORTED
PARTIALLY
BY
NSF
GRANT
CCR
THE
INTERFACE
BETWEEN
IP
NETWORKS
AND
CONNECTION
ORIENTED
NETWORKS
HAVE
DISCOVERED
THIS
SAME
UNDERLYING
PROBLEM
IN
DECIDING
WHETHER
TO
KEEP
A
CONNECTION
OPEN
BETWEEN
BURSTS
OF
PACKETS
THAT
MUST
BE
SENT
ALONG
THE
CONNECTION
KARLIN
KENYON
AND
RANDALL
STUDY
THE
TCP
ACKNOWLEDGMENT
PROBLEM
AND
THE
RELATED
BAHNCARD
PROBLEM
BOTH
OF
WHICH
ARE
AT
HEART
SKI
RENTAL
PROBLEMS
THE
PROBLEM
ALSO
ARISES
IN
CACHE
COHERENCY
IN
DECIDING
WHETHER
TO
UPDATE
OR
INVALIDATE
DATA
THAT
HAS
BEEN
CHANGED
IN
A
PROCESSOR
LOCAL
CACHE
AN
IMPORTANT
APPLICATION
OF
THE
SKI
RENTAL
PROBLEM
IS
IN
MINIMIZING
THE
POWER
CONSUMED
BY
DEVICES
THAT
CAN
TRANSITION
TO
A
LOW
POWER
SLEEP
STATE
WHEN
IDLE
THE
SLEEP
STATE
CONSUMES
LESS
POWER
HOWEVER
ONE
INCURS
A
FIXED
START
UP
COST
IN
MAKING
THE
TRANSITION
TO
THE
HIGH
POWER
ACTIVE
STATE
IN
ORDER
TO
BEGIN
WORK
WHEN
A
NEW
JOB
ARRIVES
AT
THE
ARCHITECTURAL
LEVEL
THE
TECHNIQUE
OF
ELIMINATING
POWER
TO
A
FUNCTIONAL
COMPONENT
IS
CALLED
CLOCK
POWER
GATING
AT
A
HIGHER
LEVEL
THE
POWERED
DOWN
COMPONENT
MIGHT
BE
A
DISK
DRIVE
OR
EVEN
THE
WHOLE
SYSTEM
E
G
A
LAPTOP
THAT
HIBERNATES
THE
EMBEDDED
SYSTEMS
COMMUNITY
HAS
INVESTED
A
GREAT
DEAL
OF
EFFORT
INTO
DEVISING
POLICIES
GOVERNING
THE
SELECTION
OF
POWER
STATES
DURING
IDLE
PERIODS
TERMED
DYNAMIC
POWER
MANAGEMENT
IN
THEIR
LITERATURE
SEE
FOR
EXAMPLE
FOR
A
SURVEY
THESE
TECHNIQUES
HAVE
BEEN
CRITICAL
TO
MAXIMIZING
BATTERY
USE
IN
MOBILE
SYSTEMS
WHILE
POWER
IS
ALREADY
A
FIRST
CLASS
PARAMETER
IN
SYSTEM
DESIGN
IT
WILL
BECOME
INCREASINGLY
IMPORTANT
IN
THE
FUTURE
SINCE
BATTERY
CAPACITIES
ARE
INCREASING
AT
A
MUCH
SLOWER
RATE
THAN
POWER
REQUIREMENTS
MOST
OF
THE
PREVIOUS
WORK
ON
THIS
PROBLEM
HAS
BEEN
CONCERNED
WITH
TWO
STATE
SYSTEMS
WHICH
HAVE
AN
ACTIVE
STATE
AND
A
SINGLE
SLEEP
STATE
THIS
PAPER
FOCUSES
ON
FINDING
POWER
DOWN
THRESHOLDS
FOR
SYSTEMS
THAT
HAVE
MORE
THAN
ONE
LOW
POWER
STATE
AN
EXAMPLE
OF
SUCH
A
SYSTEM
IS
THE
ADVANCED
CONFIGURATION
AND
POWER
INTERFACE
ACPI
INCLUDED
IN
THE
BIOS
ON
MOST
NEWER
COMPUTERS
WHICH
HAS
FIVE
POWER
STATES
INCLUDING
A
HIBERNATION
STATE
AND
THREE
LEVELS
OF
STANDBY
PREVIOUS
WORK
AND
NEW
RESULTS
FOR
THE
TWO
STATE
PROBLEM
AN
ONLINE
ALGORITHM
CONSISTS
OF
A
SINGLE
THRESHOLD
TAFTER
WHICH
TIME
THE
ALGORITHM
WILL
TRANSITION
FROM
THE
ACTIVE
TO
THE
SLEEP
STATE
THE
INPUT
TO
THE
PROBLEM
IS
THE
LENGTH
OF
THE
IDLE
PERIOD
AND
THE
COST
OF
AN
ALGORITHM
IS
THE
TOTAL
AMOUNT
OF
ENERGY
IT
CONSUMES
OVER
A
SINGLE
IDLE
PERIOD
TYPICALLY
AN
ONLINE
ALGORITHM
IS
EVALUATED
IN
TERMS
OF
ITS
COMPETITIVE
RATIO
THE
RATIO
OF
THE
COST
OF
THE
ONLINE
ALGORITHM
TO
THE
COST
OF
THE
OPTIMAL
OFFLINE
ALGORITHM
MAXIMIZED
OVER
ALL
INPUTS
WHEN
RANDOMIZED
ALGORITHMS
ARE
CONSIDERED
WHERE
THE
THRESHOLD
TIS
CHOSEN
AT
RANDOM
WE
LOOK
AT
THE
RATIO
OF
THE
EXPECTED
COST
OF
THE
ONLINE
ALGORITHM
TO
THE
COST
OF
THE
OFFLINE
ALGORITHM
PREVIOUS
WORK
HAS
ALSO
ADDRESSED
THE
TWO
STATE
PROBLEM
WHEN
THE
IDLE
PERIOD
IS
GENERATED
BY
A
KNOWN
PROBABILITY
DISTRIBUTION
IN
THIS
CASE
THE
ONLINE
ALGORITHM
WILL
CHOOSE
A
THRESHOLD
WHICH
MINIMIZES
ITS
EXPECTED
COST
WHERE
THE
EXPECTATION
HERE
IS
TAKEN
OVER
THE
RANDOM
CHOICE
OF
THE
IDLE
PERIOD
WE
CALL
SUCH
ALGORITHMS
PROBABILITY
BASED
ALGORITHMS
THE
BEST
DETERMINISTIC
ONLINE
ALGORITHM
WILL
STAY
IN
THE
HIGH
POWER
STATE
UNTIL
THE
TOTAL
ENERGY
SPENT
IS
EQUAL
TO
THE
COST
TO
POWER
UP
FROM
THE
LOW
POWER
STATE
IT
IS
KNOWN
THAT
THIS
ALGORITHM
ACHIEVES
THE
OPTIMAL
DETERMINISTIC
COMPETITIVE
RATIO
OF
WHEN
ONE
CONSIDERS
RANDOMIZED
ONLINE
ALGORITHMS
THE
BEST
COMPETITIVE
RATIO
ACHIEVABLE
IMPROVES
TO
E
E
IF
THE
IDLE
PERIOD
IS
GENERATED
BY
A
KNOWN
PROBABILITY
DISTRIBUTION
THEN
THE
ALGORITHM
THAT
CHOOSES
TSO
AS
TO
MINIMIZE
THE
EXPECTED
COST
IS
ALWAYS
WITHIN
A
FACTOR
OF
E
E
OF
OPTIMAL
FURTHERMORE
THIS
BOUND
IS
TIGHT
SINCE
THERE
IS
A
DISTRIBUTION
OVER
THE
IDLE
PERIOD
LENGTHS
WHICH
WILL
FORCE
ANY
ONLINE
ALGORITHM
TO
INCUR
AN
EXPECTED
COST
THAT
IS
A
FACTOR
E
E
TIMES
LARGER
THAN
THAT
INCURRED
BY
THE
OPTIMAL
OFFLINE
ALGORITHM
NOTE
THAT
IN
THE
CONTEXT
OF
POWER
DOWN
SYSTEMS
IT
MAY
NOT
BE
THE
CASE
THAT
THE
POWER
USAGE
IN
THE
SLEEP
STATE
IS
ZERO
OR
EVEN
THAT
THE
START
UP
COST
IN
THE
ACTIVE
STATE
IS
ZERO
IN
THESE
CASES
BOTH
THE
ONLINE
AND
THE
OFFLINE
ALGORITHM
WILL
INCUR
AN
IDENTICAL
ADDITIONAL
COST
THUS
THE
RATIO
OF
THE
ONLINE
TO
THE
OFFLINE
COST
WILL
DECREASE
AND
THE
OPTIMAL
COMPETITIVE
RATIO
WILL
BE
STRICTLY
LESS
THAN
TWO
HOWEVER
THESE
ADDITIONAL
COSTS
DO
NOT
CHANGE
THE
OPTIMAL
ONLINE
OR
OFFLINE
STRATEGY
IN
EITHER
THE
DETERMINISTIC
OR
THE
PROBABILITY
BASED
CASE
AND
THE
OPTIMAL
COMPETITIVE
RATIO
THAT
CAN
BE
ACHIEVED
FOR
SUCH
SYSTEMS
CAN
EASILY
BE
DETERMINED
AS
A
FUNCTION
OF
ALL
THE
PARAMETERS
OF
THE
SYSTEM
WE
DENOTE
THE
PROBLEM
THAT
INVOLVES
POWERING
DOWN
THROUGH
KSLEEP
STATES
PD
K
A
FORMAL
DESCRIPTION
OF
THE
PROBLEM
IS
AS
FOLLOWS
WE
ARE
GIVEN
A
SEQUENCE
OF
K
STATES
SKI
THERE
IS
ALSO
A
VECTOR
OF
POWER
CONSUMPTION
RATES
K
H
KI
WHERE
IIS
THE
POWER
CONSUMPTION
RATE
OF
THE
SYSTEM
IN
STATE
SI
WE
ASSUME
AS
A
CONVENTION
THAT
THE
STATES
ARE
ORDERED
SO
THAT
I
JFOR
JK
SO
THE
ACTIVE
STATE
AND
THE
SYSTEM
MUST
TRANSITION
TO
I
E
POWER
UP
AT
THE
END
OF
THE
IDLE
PERIOD
THERE
IS
AN
ASSOCIATED
TRANSITION
COST
DI
JTO
MOVE
FROM
STATE
SITO
SJ
A
SYSTEM
IS
DESCRIBED
BY
A
PAIR
D
NOTE
THAT
THERE
CAN
BE
COSTS
TO
MOVE
FROM
HIGH
POWER
STATES
TO
LOW
POWER
STATES
AND
VICE
VERSA
HOWEVER
THE
ONLY
POWER
UP
COSTS
THAT
ARE
OF
INTEREST
ARE
THE
COSTS
TO
TRANSITION
FROM
A
PARTICULAR
STATE
SITO
THE
ACTIVE
STATE
SINCE
THE
ONLY
REASON
TO
TRANSITION
TO
A
HIGHER
POWER
STATE
IS
WHEN
A
NEW
TASK
ARRIVES
A
SCHEDULE
OR
STRATEGY
A
SA
TA
CONSISTS
OF
A
SEQUENCE
OF
NA
STATES
SATHAT
IS
A
SUBSEQUENCE
OF
AND
A
SEQUENCE
OF
TRANSITION
TIMES
TA
WHERE
OBVIOUS
WE
WILL
OMIT
THE
SUBSCRIPT
A
WE
REQUIRE
THAT
T
WE
USE
A
T
TO
DENOTE
THE
COST
OF
THE
SCHEDULE
PRODUCED
BY
STRATEGY
AFOR
AN
IDLE
PERIOD
OF
LENGTH
T
WE
ALSO
CONSIDER
A
GENERALIZATION
OF
PD
K
THAT
WE
CALL
PD
K
M
WHEREIN
WE
REQUIRE
THAT
NAM
WHERE
MKIS
SOME
LIMITING
INTEGER
CONSTANT
THIS
GENERALIZATION
WOULD
BE
ESPECIALLY
USEFUL
FOR
ENGINEERS
WHO
HAVE
A
LARGE
NUMBER
OF
SLEEP
STATE
OPTIONS
AVAILABLE
IN
THE
DESIGN
PHASE
BUT
ARE
REQUIRED
TO
IMPLEMENT
AT
MOST
A
FIXED
NUMBER
OF
STATES
IN
THE
PRODUCT
THAT
ROLLS
OUT
INTO
THE
MARKET
THE
ONLY
PREVIOUS
WORK
THAT
EXAMINES
THE
MULTIPLE
STATE
PROBLEM
PD
K
FROM
THE
PERSPECTIVE
OF
WORST
CASE
GUARANTEES
IS
WHICH
CONSIDERS
THE
SPECIAL
CASE
WHERE
THE
COST
TO
POWER
DOWN
IS
ZERO
AND
THE
ALGORITHM
ONLY
PAYS
TO
MOVE
FROM
LOW
POWER
STATES
TO
HIGHER
POWER
STATES
NOTE
THAT
THIS
ALSO
INCLUDES
THE
CASE
WHERE
THE
TRANSITION
COSTS
ARE
ADDITIVE
DI
J
DJ
K
DI
KFOR
I
J
K
SINCE
THE
COSTS
TO
POWER
DOWN
CAN
THEN
BE
FOLDED
INTO
THE
COSTS
TO
POWER
UP
GIVES
NATURAL
GENERALIZATIONS
OF
THE
ALGORITHMS
FOR
THE
TWO
STATE
CASE
BOTH
FOR
THE
CASE
WHEN
THE
IDLE
PERIOD
LENGTH
IS
UNKNOWN
AND
WHEN
IT
IS
GENERATED
BY
A
KNOWN
PROBABILITY
DISTRIBUTION
IT
IS
SHOWN
THAT
WHEN
THE
TRANSITION
COSTS
ARE
ADDITIVE
THE
GENERALIZED
DETERMINISTIC
ALGORITHM
IS
COMPETITIVE
AND
THE
PROBABILITY
BASED
ALGORITHM
IS
E
E
COMPETITIVE
THUS
MATCHING
THE
GUARANTEES
IN
THE
TWO
STATE
CASE
THERE
ARE
TWO
IMPORTANT
DIRECTIONS
LEFT
OPEN
BY
THIS
WORK
THE
FIRST
IS
BASED
ON
THE
OBSERVATION
THAT
SYSTEMS
IN
GENERAL
DO
NOT
HAVE
ADDITIVE
TRANSITION
COSTS
IN
MANY
SCENARIOS
ADDITIONAL
ENERGY
IS
SPENT
IN
TRANSITIONING
TO
LOWER
POWER
STATES
FURTHERMORE
THERE
COULD
BE
OVERHEAD
IN
STOPPING
AT
INTERMEDIATE
STATES
RESULTING
IN
NON
ADDITIVE
TRANSITION
COSTS
SEE
FOR
AN
EXAMPLE
THE
SECOND
POINT
IS
THAT
THE
KNOWN
UPPER
BOUNDS
ARE
TYPICALLY
NOT
OPTIMAL
FOR
THE
SYSTEM
UNDER
CONSIDERATION
THAT
IS
WHILE
IT
IS
TRUE
THAT
THERE
EXIST
SYSTEMS
FOR
WHICH
THE
OPTIMAL
COMPETITIVE
RATIO
THAT
CAN
BE
ACHIEVED
BY
ANY
DETERMINISTIC
ALGORITHM
IS
AND
E
E
BY
ANY
RANDOMIZED
ALGORITHM
IT
IS
POSSIBLE
TO
ACHIEVE
A
BETTER
COMPETITIVE
RATIO
FOR
MANY
SYSTEMS
FOR
MULTI
STATE
SYSTEMS
THE
OPTIMAL
COMPETITIVE
RATIO
THAT
CAN
BE
ACHIEVED
WILL
IN
GENERAL
BE
A
COMPLICATED
FUNCTION
OF
ALL
THE
PARAMETERS
OF
THE
SYSTEM
THE
POWER
CONSUMPTION
RATES
AS
WELL
AS
TRANSITION
COSTS
FOR
PROBABILITY
BASED
ALGORITHMS
THE
OPTIMAL
COMPETITIVE
RATIO
WILL
ALSO
DEPEND
ON
THE
PROBABILITY
DISTRIBUTION
GENERATING
THE
LENGTH
OF
THE
IDLE
PERIOD
WHILE
IT
MAY
NOT
BE
FEASIBLE
TO
EXPRESS
THE
OPTIMAL
COMPETITIVE
RATIO
AS
A
FUNCTION
OF
ALL
THESE
PARAMETERS
A
SYSTEM
DESIGNER
WOULD
IN
GENERAL
LIKE
TO
DESIGN
A
POWER
DOWN
STRATEGY
THAT
OBTAINS
THE
BEST
POSSIBLE
COMPETITIVE
RATIO
GIVEN
THE
CONSTRAINTS
OF
HIS
OR
HER
PARTICULAR
SYSTEM
THIS
PAPER
ESTABLISHES
THE
FOLLOWING
RESULTS
WE
GIVE
AN
ALGORITHM
THAT
TAKES
AS
INPUT
AN
INSTANCE
OF
PD
K
THAT
IS
DESCRIBED
BY
K
D
AND
AN
ERROR
PARAMETER
E
AND
PRODUCES
A
POWER
DOWN
STRATEGY
A
A
WHOSE
COMPETITIVE
RATIO
IS
WITHIN
AN
ADDITIVE
EOF
THE
BEST
COMPETITIVE
RATIO
THAT
CAN
BE
ACHIEVED
FOR
THAT
SYSTEM
THE
ALGORITHM
RUNS
IN
TIME
O
K
LOGK
LOG
E
WHERE
K
IS
THE
NUMBER
OF
STATES
IN
THE
SYSTEM
AND
ALSO
OUTPUTS
THE
COMPETITIVE
RATIO
OF
THE
ALGORITHM
WORKS
VIA
A
DECISION
PROCEDURE
WHICH
DETERMINES
FOR
A
SYSTEM
AND
A
CONSTANT
PIF
THERE
IS
A
P
COMPETITIVE
STRATEGY
FOR
THAT
SYSTEM
THIS
DECISION
PROCEDURE
ALSO
ALLOWS
US
TO
OBTAIN
LOWER
BOUNDS
ON
THE
COMPETITIVE
RATIO
ACHIEVABLE
BY
DETERMINISTIC
ALGORITHMS
FOR
SPECIFIC
SYSTEMS
WHICH
IN
TURN
PROVIDES
A
LOWER
BOUND
ON
THE
COMPETITIVE
RATIO
ACHIEVABLE
BY
DETER
MINISTIC
ALGORITHMS
IN
GENERAL
IN
PARTICULAR
WE
OBTAIN
A
LOWER
BOUND
OF
THE
COMPETITIVE
RATIO
FOR
DETERMINISTIC
ALGORITHMS
THIS
IS
THE
FIRST
LOWER
BOUND
KNOWN
THAT
IS
GREATER
THAN
INDEPENDENTLY
DAMASCHKE
HAS
GIVEN
A
LOWER
BOUND
OF
THE
ABOVE
APPROACH
CAN
BE
MODIFIED
TO
SOLVE
THE
MORE
GENERAL
VERSION
WHERE
A
BOUND
OF
MIS
SPECIFIED
ON
THE
NUMBER
OF
STATES
ALLOWED
IN
FINAL
STRATEGY
WE
SHOW
HOW
TO
EXTEND
THE
DECISION
PROCEDURE
TO
ANSWER
IF
THERE
IS
A
P
COMPETITIVE
STRATEGY
FOR
THE
SYSTEM
THAT
USES
AT
MOST
MPOWER
STATES
EXPERIMENTAL
RESULTS
SHOW
THAT
THERE
ARE
SIGNIFICANT
PERFORMANCE
GAINS
TO
BE
MADE
BY
ESTIMATING
THE
DISTRIBUTION
GOVERNING
THE
LENGTH
OF
AN
IDLE
PERIOD
BASED
ON
RECENT
HISTORY
AND
USING
THIS
ESTIMATE
TO
DRIVE
A
PROBABILITY
BASED
STRATEGY
WE
GIVE
AN
ALGORITHM
THAT
TAKES
AS
INPUT
A
DESCRIPTION
OF
A
SYSTEM
AND
A
PROBABILITY
DISTRIBUTION
GENERATING
THE
IDLE
PERIOD
LENGTH
AND
PRODUCES
THE
OPTIMAL
POWER
DOWN
STRATEGY
NATURALLY
THE
RUNNING
TIME
OF
THE
ALGORITHM
WILL
DEPEND
ON
THE
REPRESENTATION
OF
THE
DISTRIBUTION
IN
PRACTICE
THIS
IS
MOST
LIKELY
TO
BE
A
HISTOGRAM
OUR
ALGORITHM
RUNS
IN
TIME
O
K
LOGK
B
WHERE
BIS
THE
NUMBER
OF
BINS
IN
THE
HISTOGRAM
AND
K
IS
THE
NUMBER
OF
STATES
ONE
OUTCOME
OF
THE
PROOF
IS
THAT
IT
ALSO
ESTABLISHES
THE
OPTIMALITY
OF
THE
STRATEGY
GIVEN
IN
FOR
ADDITIVE
SYSTEMS
WE
THEN
GENERALIZE
THIS
TO
FIND
THE
BEST
ONLINE
ALGORITHM
SUBJECT
TO
THE
RESTRICTION
THAT
AT
MOST
MSTATES
ARE
USED
AT
THE
EXPENSE
OF
AN
EXTRA
FACTOR
OF
MIN
THE
RUNNING
TIME
WE
GIVE
A
SIMPLE
DETERMINISTIC
STRATEGY
THAT
ACHIEVES
A
COMPETITIVE
RATIO
OF
P
ALL
SYSTEMS
THIS
RESULT
GIVES
A
BOUND
ON
THE
COMPETITIVE
RATIO
ACHIEVED
BY
THE
OPTIMAL
STRATEGIES
GENERATED
BY
OUR
ALGORITHMS
NOTE
THAT
P
ALSO
SERVES
AS
A
BOUND
ON
THE
RATIO
OF
THE
EXPECTED
COSTS
OF
THE
ONLINE
AND
OFFLINE
ALGORITHMS
WHEN
THE
INPUT
IS
PROBABILISTICALLY
GENERATED
IN
THE
REMAINDER
OF
THIS
PAPER
WE
USE
THE
TERMS
SCHEDULE
OR
STRATEGY
INTERCHANGEABLY
TO
REFER
TO
THE
CHOICES
OF
STATES
AND
THRESHOLD
TIMES
FOR
POWERING
DOWN
THE
TERM
ALGORITHM
WILL
REFER
TO
A
PROCEDURE
THAT
PRODUCES
A
SCHEDULE
OR
STRATEGY
BASED
ON
A
PARTICULAR
SYSTEM
AZAR
ET
AL
IN
CONSIDER
A
RELATED
PROBLEM
WHICH
THEY
REFER
TO
AS
CAPITAL
INVESTMENT
THIS
PROBLEM
IS
A
DIFFERENT
GENERALIZATION
OF
THE
SKI
RENTAL
PROBLEM
THAN
THE
POWER
DOWN
PROBLEM
CONSIDERED
HERE
HOWEVER
A
SPECIAL
CASE
OF
THEIR
PROBLEM
COINCIDES
WITH
A
SPECIAL
CASE
OF
OUR
PROBLEM
SPECIFICALLY
THEY
GIVE
A
P
COMPETITIVE
DETERMINISTIC
ALGORITHM
FOR
THE
SPECIAL
CASE
OF
THE
POWER
DOWN
PROBLEM
IN
WHICH
THE
COST
TO
TRANSITION
TO
EACH
STATE
IS
THE
SAME
REGARDLESS
OF
THE
STATE
FROM
WHICH
ONE
IS
TRANSITIONING
LATER
DAMASCHKE
IN
IMPROVES
THE
UPPER
BOUND
ON
THE
COMPETITIVE
RATIO
FOR
THIS
SPECIAL
CASE
ALSO
IN
THE
CONTEXT
OF
CAPITAL
INVESTMENT
TO
DETERMINISTIC
ALGORITHMS
AND
RANOMIZED
ALGORITHMS
IN
ADDITION
DAMASCHKE
GIVES
A
BOUND
FOR
ANY
DETERMINISTIC
ALGORITHM
WHICH
SUBSUMES
THE
LOWER
BOUND
OF
HERE
PRELIMINARIES
FIRST
WE
WILL
ESTABLISH
THAT
WE
CAN
ASSUME
WITHOUT
LOSS
OF
GENERALITY
THAT
THE
POWER
UP
TRANSITION
COSTS
ARE
ZERO
IF
THIS
IS
NOT
THE
CASE
FOR
SOME
SYSTEM
D
WE
CAN
DEFINE
A
NEW
SYSTEM
SUCH
THAT
FOR
ANY
I
J
THE
COST
TO
TRANSITION
FROM
SITO
SJIS
DI
J
DJ
THE
COST
TO
GO
FROM
SJTO
SIIS
SINCE
THERE
IS
NEVER
ANY
REASON
TO
TRANSITION
TO
A
HIGHER
POWER
STATE
UNLESS
THE
SYSTEM
IS
TRANSITIONING
TO
THE
ACTIVE
STATE
AT
THE
ARRIVAL
OF
A
NEW
TASK
ANY
SET
OF
ACTIONS
IN
THE
ORIGINAL
SYSTEM
WILL
INCUR
THE
SAME
COST
IN
THE
NEW
SYSTEM
THUS
IN
THE
SEQUEL
WE
ASSUME
THAT
DI
ALL
I
LET
D
I
DENOTE
I
THEN
OPT
T
MINI
D
I
IT
LET
T
DENOTE
THE
STATE
WHICH
ATTAINS
THE
MINIMUM
THE
OPTIMAL
STATE
THE
OPTIMAL
STRATEGY
IS
TO
TRANSITION
TO
STATE
T
AT
TIME
AND
STAY
THERE
THROUGH
TIME
T
WE
ASSUME
THAT
THE
OPTIMAL
STRATEGY
WILL
ACTUALLY
USE
EVERY
STATE
I
E
RANGE
T
SK
NONE
OF
THE
ONLINE
STRATEGIES
WE
PRESENT
WILL
MAKE
USE
OF
A
STATE
THAT
IS
NEVER
USED
BY
THE
OPTIMAL
OFFLINE
STRATEGY
FOR
ANY
TIME
T
ENERGY
STATE
STATE
STATE
STATE
B
B
FIGURE
ENERGY
CONSUMED
BY
THE
OPTIMAL
STRATEGY
AS
A
FUNCTION
OF
IDLE
PERIOD
LENGTH
NOTE
THAT
OPT
T
IS
PIECEWISE
LINEAR
AND
T
IS
NON
DECREASING
WITH
T
AS
THE
IDLE
PERIOD
LENGTH
GETS
LONGER
IT
BECOMES
MORE
WORTHWHILE
TO
PAY
THE
EXTRA
COST
TO
TRANSITION
TO
A
LOWER
POWER
STATE
LET
BIDENOTE
THE
FIRST
TIME
INSTANT
AT
WHICH
STATE
SIBECOMES
THE
OPTIMAL
STATE
SO
B
D
I
I
BI
D
I
IBI
BI
WE
HAVE
B
B
B
K
FIGURE
SHOWS
THE
TOTAL
ENERGY
CONSUMED
K
I
K
I
BY
OPTAS
A
FUNCTION
OF
THE
LENGTH
OF
THE
IDLE
PERIOD
THERE
IS
A
LINE
FOR
EACH
STATE
THE
Y
INTERCEPT
IS
THE
TRANSITION
COST
TO
MOVE
TO
THAT
STATE
FROM
THE
ACTIVE
STATE
AND
THE
SLOPE
IS
THE
POWER
CONSUMPTION
RATE
THE
ENERGY
CONSUMED
BY
THE
OPTIMAL
STRATEGY
IS
THE
LOWER
ENVELOPE
OF
THESE
LINES
SINCE
IT
WILL
PICK
THE
SINGLE
STATE
WHICH
MINIMIZES
THE
COST
FOR
A
GIVEN
IDLE
PERIOD
LENGTH
THUS
FOR
T
BI
BI
I
OPT
T
D
I
IT
J
BJ
BJ
I
T
BI
J
WE
COMPARE
OUR
ONLINE
STRATEGY
WITH
OPT
T
AND
WANT
TO
GET
A
STRATEGY
AWHICH
MINIMIZES
THE
COM
PETITIVE
RATIO
CA
SUP
T
TOPT
T
WHERE
A
T
DENOTES
THE
TOTAL
POWER
CONSUMPTION
OF
ABY
TIME
T
A
SIMPLE
P
COMPETITIVE
STRATEGY
FIRST
WE
ESTABLISH
THAT
WE
CAN
ASSUME
THAT
FOR
ALL
I
J
DI
J
J
RECALL
THAT
WE
ARE
REALLY
USING
DI
JTO
DENOTE
DI
J
DJ
JTO
DENOTE
J
DJ
THUS
THE
ASSUMPTION
THAT
DI
J
JREALLY
AMOUNTS
TO
ASSUMING
THAT
DI
J
DI
J
IF
THIS
WERE
NOT
THE
CASE
WE
COULD
JUST
TRANSITION
FROM
STATE
SITO
STATE
SJ
BY
FIRST
GOING
TO
THEN
DOWN
TO
SJ
LET
US
FOR
THE
MOMENT
ASSUME
THAT
FOR
SOME
D
I
D
I
FOR
ALL
I
K
THIS
IS
A
NON
TRIVIAL
ASSUMPTION
THAT
WE
WILL
HAVE
TO
HANDLE
LATER
CONSIDER
THE
STRATEGY
WHICH
ALWAYS
STAYS
IN
STATE
T
THE
SAME
STATE
AS
OPT
AT
EVERY
TIME
T
THE
OPTIMAL
STRATEGY
WHICH
KNOWS
THE
LENGTH
OF
THE
IDLE
PERIOD
IN
ADVANCE
WILL
JUST
TRANSITION
TO
THE
OPTIMAL
STATE
STRATEGY
HOWEVER
MUST
FOLLOW
THE
OPTIMAL
STRATEGY
MAKING
EACH
TRANSITION
TO
A
NEW
STATE
AS
THE
IDLE
PERIOD
GETS
LONGER
THIS
IS
THE
STRATEGY
PROPOSED
IN
AND
SHOWN
TO
BE
COMPETITIVE
FOR
ADDITIVE
SYSTEMS
NOTE
THAT
THIS
STRATEGY
IS
THE
SAME
AS
THE
COMPETITIVE
BALANCE
STRATEGY
FOR
THE
TWO
STATE
CASE
FOR
T
BI
BI
THE
ONLINE
COST
IS
A
T
PJ
J
BJ
BJ
DJ
J
I
T
BI
IN
COMPARING
THIS
COST
TO
THE
OPTIMAL
COST
IN
EQUATION
OBSERVE
THAT
BOTH
TERMS
HAVE
AN
ADDITIVE
I
T
BI
WHICH
MEANS
THAT
THE
RATIO
A
T
WILL
BE
MAXIMIZED
AT
T
BI
TO
BOUND
THE
COST
OF
AIN
TERMS
OF
OPT
WE
USE
THE
FACT
I
THAT
OPT
BI
D
I
AND
OPT
BI
J
BJ
BJ
BOTH
OF
WHICH
COME
FROM
EQUATION
I
A
BI
J
BJ
BJ
DJ
J
J
I
I
X
J
BJ
BJ
XD
J
J
J
I
OPT
BI
D
I
I
J
J
OPT
B
OPT
B
I
I
THIS
HOLDS
FOR
ANY
TIMPLYING
A
COMPETITIVE
RATIO
OF
NOW
SUPPOSE
THE
ASSUMPTION
D
I
D
I
DOES
NOT
HOLD
WE
CONSIDER
A
NEW
OFFLINE
STRATEGY
OPT
THAT
ONLY
USES
A
SUBSET
OF
STATES
FOR
WHICH
THE
PROPERTY
DOES
HOLD
AND
IS
A
APPROXIMATION
OF
OPT
I
E
OPT
T
OPT
T
WE
NOW
VIEW
OUR
PROBLEM
AS
SPECIFIED
BY
JUST
THE
STATES
IN
AND
EXECUTE
STRATEGY
AAS
SPECIFIED
ABOVE
EMULATING
OPT
INSTEAD
OF
OPT
WE
GET
THAT
A
T
OPT
T
OPT
T
SETTING
P
WE
GET
A
COMPETITIVE
RATIO
OF
P
WE
DETERMINE
OPT
AS
FOLLOWS
LET
FSKGINITIALLY
CONSIDER
THE
STATES
IN
SIN
REVERSE
ORDER
LET
SIBE
THE
LAST
STATE
ADDED
TO
WE
FIND
THE
LARGEST
J
J
IS
T
D
J
D
I
WE
ADD
SJTO
AND
CONTINUE
UNTIL
NO
SUCH
JEXISTS
NOTE
THAT
SINCE
D
OPT
WILL
EXECUTE
THE
OPTIMAL
OFFLINE
STRATEGY
ASSUMING
THAT
ONLY
THE
STATES
IN
ARE
AVAILABLE
CONSIDER
I
JS
T
SI
SJ
AND
NO
IS
IN
FOR
I
R
J
WE
HAVE
OPT
T
OPT
T
FOR
T
BI
BI
AND
T
BJ
BJ
FOR
RS
T
I
R
JAND
TIME
T
B
B
OPT
T
MIN
D
I
IT
D
J
JT
AND
OPT
T
D
R
T
JWAS
CHOSEN
TO
BE
THE
LARGEST
VALUE
LESS
THAN
ISUCH
THAT
D
J
D
I
WHICH
MEANS
THAT
D
R
D
I
FURTHERMORE
SINCE
I
WE
HAVE
THAT
OPT
T
D
I
IT
D
R
T
OPT
T
AND
OPT
IS
A
APPROXIMATION
TO
OPT
THEOREM
THERE
IS
A
P
COMPETITIVE
STRATEGY
FOR
ANY
SYSTEM
A
NEAR
OPTIMAL
DETERMINISTIC
ALGORITHM
IN
THIS
SECTION
WE
TURN
OUR
ATTENTION
TO
OBTAINING
A
NEAR
OPTIMAL
SCHEDULE
FOR
A
PARTICULAR
SYSTEM
MORE
PRE
CISELY
GIVEN
A
SYSTEM
D
WITH
STATE
SEQUENCE
SFOR
WHICH
THE
OPTIMAL
ONLINE
SCHEDULE
HAS
COMPETITIVE
RA
TIO
P
WE
GIVE
AN
ALGORITHM
THAT
RETURNS
A
P
E
COMPETITIVE
ONLINE
SCHEDULE
IN
TIME
O
K
E
THE
ALGORITHM
IS
BASED
ON
A
DECISION
PROCEDURE
WHICH
DETERMINES
WHETHER
A
COMPETITIVE
SCHEDULE
EXISTS
FOR
A
GIVEN
VALUE
OF
P
THEOREM
ESTABLISHES
AN
UPPER
BOUND
OF
P
ON
THE
OPTIMAL
COMPETITIVE
RATIO
SO
WE
PERFORM
A
BISECTION
SEARCH
IN
THE
RANGE
P
TO
FIND
THE
SMALLEST
PSUCH
THAT
THERE
EXISTS
A
P
COMPETITIVE
SCHEDULE
WE
ALSO
OUTPUT
THE
RESULTING
SCHEDULE
ENERGY
ENERGY
TTIME
TITIME
FIGURE
ENERGY
CONSUMED
BY
THE
ONLINE
AND
OPTIMAL
STRATEGY
AS
A
FUNCTION
OF
IDLE
PERIOD
LENGTH
THE
SOLID
LINE
IS
POPT
T
THE
DASHED
LINE
IS
THE
ONLINE
COST
TIS
THE
FIRST
TRANSITION
TIME
THAT
IS
NOT
EAGER
T
SHOWS
THE
TRANSFORMED
STRATEGY
WHICH
NOW
HAS
AN
EAGER
TRANSITION
THE
FOLLOWING
LEMMA
SHOWS
THAT
THE
ONLINE
STRATEGY
MUST
EVENTUALLY
GET
TO
A
SUFFICIENTLY
LOW
POWER
STATE
LEMMA
ALLOWS
US
TO
LIMIT
OUR
CONCERN
TO
JUST
THE
TRANSITION
POINTS
IN
ANY
ONLINE
SCHEDULE
LEMMA
IF
A
T
IS
A
P
COMPETITIVE
STRATEGY
AND
IS
THE
LAST
STATE
IN
THEN
P
K
PROOF
FOR
THE
SAKE
OF
CONTRADICTION
ASSUME
THAT
P
K
FOR
TO
BE
P
COMPETITIVE
THE
FUNCTION
T
MUST
LIE
ENTIRELY
BELOW
POPT
T
HOWEVER
THE
LAST
LINE
OF
POPT
T
HAS
SLOPE
P
KAND
WILL
THEREFORE
INTERSECT
THE
LAST
LINE
OF
T
WHICH
HAS
A
LARGER
SLOPE
AFTER
WHICH
TIME
T
WILL
EXCEED
POPT
T
THIS
IS
A
CONTRADICTION
LEMMA
IF
A
SCHEDULE
AHAS
FINITE
COMPETITIVE
RATIO
THEN
THE
EARLIEST
TIME
T
WHICH
A
T
IS
MAXIMIZED
IS
A
TRANSITION
POINT
IN
THE
STRATEGY
A
PROOF
LET
P
MAXT
T
CONSIDER
THE
FUNCTIONS
A
T
AND
POPT
T
THE
FUNCTION
A
T
NEVER
EXCEEDS
POPT
T
AND
IS
THE
EARLIEST
POINT
AT
WHICH
THESE
TWO
FUNCTIONS
HAVE
THE
SAME
VALUE
NOT
CONSID
ERING
THE
ORIGIN
FOR
THE
SAKE
OF
CONTRADICTION
ASSUME
THAT
T
IS
NOT
A
TRANSITION
POINT
IN
A
SO
WE
CAN
FIND
SOME
SMALL
E
THAT
A
T
IS
LINEAR
IN
T
E
T
E
SINCE
A
T
IS
STRICTLY
LESS
THAN
POPT
T
IN
THE
INTERVAL
T
E
T
AND
A
T
POPT
T
IT
MUST
BE
THE
CASE
THAT
THE
SLOPE
OF
A
T
IS
LARGER
THAN
THE
SLOPE
OF
POPT
T
IN
THIS
INTERVAL
THIS
GIVES
A
CONTRADICTION
BECAUSE
A
T
HAS
CONSTANT
SLOPE
OVER
T
E
E
AND
POPT
T
IS
A
CONTINUOUS
FUNCTION
WITH
DECREASING
SLOPE
WHICH
MEANS
THAT
A
T
POPT
T
FOR
T
T
WE
NOW
EXPLORE
WAYS
TO
RESTRICT
THE
SPACE
OF
SCHEDULES
WE
NEED
TO
CONSIDER
IN
SEARCHING
FOR
A
P
COMPETITIVE
SCHEDULE
FOR
A
STRATEGY
A
T
WE
SAY
THAT
A
TRANSITION
AT
TIME
T
TIS
P
EAGER
OR
JUST
EAGER
IF
PIS
CLEAR
FROM
THE
CONTEXT
IF
A
T
POPT
T
WE
SAY
THAT
AIS
A
P
EAGER
STRATEGY
IF
A
T
POPT
T
FOR
EVERY
T
T
NOTE
THAT
BY
LEMMAS
AND
A
P
EAGER
STRATEGY
THAT
ENDS
AT
STATE
SUCH
THAT
P
KIS
P
COMPETITIVE
LEMMA
IF
IS
A
P
COMPETITIVE
STRATEGY
THEN
THERE
EXISTS
AN
EAGER
STRATEGY
THAT
IS
ALSO
P
COMPETITIVE
PROOF
FIGURE
SHOWS
A
SCHEMATIC
OF
THE
PROOF
THE
JUMPS
IN
THE
ONLINE
COST
THE
DASHED
LINE
ARE
TRANSITION
COSTS
THE
SOLID
LINE
IS
POPT
T
THE
FIGURE
SHOWS
A
TRANSITION
TIME
TAT
WHICH
THE
ONLINE
COST
IS
LESS
THAN
POPT
T
THE
IDEA
IS
THAT
WE
CAN
SLIDE
SUCH
A
TRANSITION
TIME
EARLIER
UNTIL
IT
HITS
THE
FUNCTION
POPT
T
CONSIDER
THE
EARLIEST
TRANSITION
TIME
TWHICH
IS
NOT
EAGER
SUPPOSE
THAT
ATRANSITIONS
FROM
STATE
SITO
STATE
SJAT
TIME
T
LET
T
TBE
THE
TIME
OF
THE
IMMEDIATELY
PRECEDING
TRANSITION
IF
THERE
IS
NO
SUCH
TRANSITION
TIME
THEN
SET
T
THE
FUNCTION
POPT
T
A
T
IS
CONTINUOUS
IN
THE
INTERVAL
T
T
SINCE
ADOES
NOT
HAVE
ANY
TRANSITIONS
IN
THIS
OPEN
INTERVAL
AND
POPT
T
T
IS
TIME
T
AND
IS
STRICTLY
GREATER
THAN
DI
JAT
TIME
TEFOR
A
SMALL
ENOUGH
E
LET
TBE
THE
EARLIEST
TIME
AFTER
T
SUCH
THAT
POPT
T
T
DI
J
SO
T
T
CONSIDER
THE
STRATEGY
A
THAT
IS
IDENTICAL
TO
AEXCEPT
THAT
THE
TRANSITION
FROM
SITO
SJIS
MOVED
EARLIER
FROM
TTO
T
WE
NEED
TO
ARGUE
THAT
A
IS
P
COMPETITIVE
CLEARLY
A
T
A
T
FOR
T
T
T
AND
A
T
POPT
T
ALSO
A
T
A
T
SINCE
A
TRANSITIONS
EARLIER
TO
THE
LOW
POWER
STATE
SJAND
HENCE
USES
LESS
TOTAL
ENERGY
AND
SINCE
THE
STRATEGIES
BEHAVE
THE
SAME
AFTER
TIME
T
A
WILL
CONTINUE
TO
HAVE
A
LOWER
COST
AT
ALL
TIMES
T
T
TO
SEE
THAT
A
T
POPT
T
OVER
THE
INTERVAL
T
T
NOTE
THAT
A
T
IS
LINEAR
OVER
THIS
INTERVAL
SINCE
A
REMAINS
IN
STATE
SJ
ALSO
POPT
T
IS
A
PIECEWISE
LINEAR
CONCAVE
FUNCTION
SINCE
ITS
SLOPE
IS
NON
INCREASING
OVER
TIME
THUS
SINCE
THE
POINTS
T
T
AND
T
T
BOTH
LIE
ON
OR
BELOW
THIS
CURVE
THE
STRAIGHT
LINE
CONNECTING
THEM
LIES
UNDER
THE
CURVE
POPT
T
THE
PROCEDURE
ABOVE
CAN
BE
REPEATED
UNTIL
ALL
THE
TRANSITIONS
ARE
EAGER
LEMMA
SUPPOSE
A
STRATEGY
MAKES
A
P
EAGER
TRANSITION
TO
STATE
SIAT
TIME
TIAND
NEXT
MAKES
A
TRANSITION
TO
STATE
SJ
USING
THE
FUNCTION
POPT
T
ONE
CAN
COMPUTE
THE
EARLIEST
P
EAGER
TRANSITION
TIME
T
TO
STATE
SJ
IN
TIME
O
LOGK
PROOF
DEFINE
THE
LINE
L
T
IT
POPT
TI
ITI
DI
J
T
IS
THE
SMALLEST
T
TISUCH
THAT
POPT
T
L
T
IF
THERE
IS
NO
SUCH
T
THEN
A
P
EAGER
TRANSITION
FROM
SITO
SJDOES
NOT
EXIST
SINCE
POPT
T
IS
CONCAVE
WE
HAVE
THAT
IF
L
T
POPT
T
OR
IF
L
T
POPT
T
AND
THE
SLOPE
OF
POPT
T
IS
LESS
THAN
OR
EQUAL
TO
I
THEN
T
T
OTHERWISE
T
THESE
INEQUALITIES
ALLOW
ONE
TO
DO
A
BINARY
SEARCH
USING
THE
LINE
SEGMENTS
OF
POPT
T
TO
DETERMINE
TIF
IT
EXISTS
LET
BE
THE
OPTIMAL
STATE
I
E
STATE
OF
OPT
T
AT
TIME
TI
CONSIDER
THE
LINE
SEGMENTS
OF
POPT
T
CORRESPONDING
TO
STATES
AND
SK
RECALL
THAT
B
AND
BKARE
RESPECTIVELY
THE
LEFT
END
POINTS
OF
THESE
SEGMENTS
THESE
ARE
THE
FIRST
TIME
INSTANTS
AT
WHICH
AND
SKBECOME
THE
OPTIMAL
STATES
RESPECTIVELY
USING
THE
ABOVE
INEQUALITIES
IF
WE
DETERMINE
THAT
T
BK
THEN
T
IS
SIMPLY
THE
POINT
OF
INTERSECTION
IF
IT
EXISTS
OF
L
T
WITH
THE
SEGMENT
OF
POPT
T
CORRESPONDING
TO
SK
OTHERWISE
WE
HAVE
A
LOW
SEGMENT
WITH
END
POINT
B
AND
A
HIGH
SEGMENT
WITH
END
POINT
BK
NOW
WE
REPEATEDLY
CONSIDER
THE
LEFT
END
POINT
OF
THE
SEGMENT
THAT
IS
IN
THE
MIDDLE
OF
THE
LOW
AND
HIGH
SEGMENTS
AND
USE
THE
ABOVE
INEQUALITIES
TO
UPDATE
THE
LOW
OR
HIGH
SEGMENT
AND
THE
CORRESPONDING
END
POINT
ACCORDINGLY
UNTIL
THE
END
POINTS
OF
THE
LOW
AND
HIGH
SEGMENTS
CORRESPOND
RESPECTIVELY
TO
THE
LEFT
AND
RIGHT
END
POINTS
OF
A
SEGMENT
OF
POPT
T
WHEN
THIS
HAPPENS
WE
CAN
COMPUTE
BY
FINDING
THE
INTERSECTION
POINT
IF
IT
EXISTS
OF
L
T
AND
THIS
SEGMENT
THE
BINARY
SEARCH
CAN
BE
IMPLEMENTED
IN
TIME
LOGK
WHERE
KIS
THE
NUMBER
OF
SEGMENTS
I
E
NUMBER
OF
STATES
LEMMA
IMMEDIATELY
GIVES
AN
ALGORITHM
THAT
IS
EXPONENTIAL
IN
K
THE
NUMBER
OF
STATES
AND
DETERMINES
WHETHER
A
P
COMPETITIVE
STRATEGY
EXISTS
FOR
THE
SYSTEM
THIS
ALGORITHM
ENUMERATES
ALL
SUBSEQUENCES
OF
STATES
AND
DETERMINES
THE
P
EAGER
STRATEGY
FOR
THAT
SUBSEQUENCE
BY
FINDING
THE
EAGER
TRANSITION
TO
EACH
STATE
BASED
ON
THE
EAGER
TRANSITIONS
TO
THE
PREVIOUS
STATES
AS
DESCRIBED
IN
THE
PROOF
OF
LEMMA
A
P
COMPETITIVE
STRATEGY
FOR
THE
SYSTEM
EXISTS
IF
AND
ONLY
IF
ONE
OF
THESE
P
EAGER
STRATEGIES
IS
P
COMPETITIVE
I
E
ENDS
AT
A
STATE
SWITH
SP
K
THE
REMAINDER
OF
THIS
SECTION
PRESENTS
A
WAY
TO
REMOVE
THE
EXPONENTIAL
DEPENDENCE
ON
K
LET
SKIBE
A
SEQUENCE
OF
STATES
THAT
FORM
A
SYSTEM
DEFINE
SSI
SJ
TO
BE
THE
CONTIGUOUS
SUBSEQUENCE
HSI
SJI
WHERE
SIAND
SJARE
ELEMENTS
OF
SSUCH
THAT
I
J
LET
SBE
THE
SET
OF
SUBSE
QUENCES
OF
SS
STHAT
INCLUDE
SSUCH
THAT
FOR
EACH
ONE
CAN
FIND
TRANSITION
TIMES
FOR
THE
STATE
SEQUENCE
SO
THAT
IN
THE
RESULTING
SCHEDULE
EACH
TRANSITION
UP
TO
AND
INCLUDING
THE
TRANSITION
TO
STATE
SIS
A
P
EAGER
TRANSITION
FOR
A
STATE
Q
WE
WILL
USE
QTO
DENOTE
THIS
P
EAGER
TRANSITION
TIME
TO
QFOR
THE
SEQUENCE
NOTE
THAT
UNIQUELY
DETERMINES
THE
TRANSITION
TIMES
Q
WE
DEFINE
THE
EARLIEST
TRANSITION
TIME
E
P
OF
STATE
SFOR
THE
GIVEN
SYSTEM
AS
E
P
THAT
IS
E
P
IS
THE
EARLIEST
TIME
AT
WHICH
ANY
ONLINE
STRATEGY
CAN
TRANSITION
TO
STATE
SWHILE
REMAINING
P
EAGER
OVER
ALL
ITS
TRANSITIONS
UP
TO
AND
INCLUDING
THE
TRANSITION
TO
STATE
OBSERVE
THAT
IF
THERE
IS
P
COMPETITIVE
STRATEGY
THAT
USES
STATE
THEN
BY
LEMMA
THERE
IS
SUCH
A
P
EAGER
STRATEGY
SO
AND
E
P
IS
WELL
DEFINED
WE
CALL
A
TRANSITION
TO
STATE
SP
EARLY
OR
SIMPLY
EARLY
IF
IT
HAPPENS
AT
TIME
E
P
A
STRATEGY
THAT
CONSISTS
ENTIRELY
OF
EARLY
TRANSITIONS
IS
CALLED
A
P
EARLY
STRATEGY
LEMMA
IF
THERE
IS
A
P
COMPETITIVE
STRATEGY
THEN
THERE
IS
AN
EAGER
AND
EARLY
P
COMPETITIVE
STRATEGY
PROOF
LET
SBE
THE
LAST
STATE
IN
CONSIDER
THE
SEQUENCE
SSUCH
THAT
E
P
AND
THE
STRATEGY
THAT
USES
ONLY
THE
STATES
IN
TRANSITIONING
TO
STATE
Q
AT
TIME
Q
I
E
SINCE
IS
P
COMPETITIVE
IT
MUST
BE
THAT
SP
KAND
SINCE
BY
DEFINITION
HAS
ALL
P
EAGER
TRANSITIONS
AND
ENDS
IN
STATE
IT
IS
ALSO
P
COMPETITIVE
WE
NOW
ARGUE
THAT
IS
AN
EARLY
STRATEGY
NOTE
THAT
WAS
CHOSEN
SO
THAT
THE
TRANSITION
TO
STATE
SIS
P
EARLY
WE
HAVE
TO
SHOW
THAT
THE
REMAINING
TRANSITIONS
OF
ARE
ALSO
P
EARLY
SUPPOSE
NOT
CONSIDER
THE
LATEST
TRANSITION
THAT
IS
NOT
P
EARLY
SUPPOSE
THIS
HAPPENS
FOR
STATE
R
SO
T
R
E
R
P
LET
R
BE
THE
STATE
JUST
AFTER
RIN
SEQUENCE
LET
RBE
THE
SEQUENCE
FOR
WHICH
I
R
E
R
P
T
T
THE
EARLIEST
TIME
THAT
A
P
EAGER
SCHEDULE
CAN
TRANSITION
TO
STATE
RAND
THE
SEQUENCE
OF
STATES
IN
THIS
SCHEDULE
IS
GIVEN
BY
CONSIDER
THE
HYBRID
STRATEGY
THAT
USES
THE
STATES
IN
FOLLOWED
BY
THE
STATES
IN
THAT
APPEAR
AFTER
R
WITH
THE
TRANSITION
TIMES
BEING
I
QFOR
Q
AND
QFOR
Q
RI
STRATEGY
TRANSITIONS
TO
STATE
RAT
TIME
T
AND
STRATEGY
TRANSITIONS
TO
STATE
RAT
TIME
T
T
BOTH
OF
THESE
TRANSITIONS
ARE
EAGER
TRANSITIONS
BOTH
STRATEGIES
ARE
IN
STATE
RAT
TIME
T
AND
MAKE
THE
SAME
STATE
TRANSITIONS
THEREAFTER
THUS
FOR
ANY
T
T
T
T
T
T
IN
PARTICULAR
BOTH
STRATEGIES
TRANSITION
TO
R
THE
STATE
AFTER
R
AT
TIME
RI
E
R
P
T
USING
THE
EQUATION
ABOVE
WE
HAVE
THAT
T
T
T
T
WE
WILL
SHOW
THAT
T
T
WHICH
IMPLIES
IN
PARTICULAR
THAT
T
T
SO
IN
THE
TRANSITION
TO
R
IS
NO
LONGER
P
EAGER
ARGUING
AS
IN
LEMMA
THIS
MEANS
THAT
WE
CAN
SHIFT
THE
TRANSITION
TO
R
TO
GET
AN
EAGER
TRANSITION
AT
AN
EARLIER
TIME
BUT
THIS
CONTRADICTS
THE
ASSUMPTION
THAT
THE
TRANSITION
TO
STATE
R
AT
TIME
T
WAS
AN
EARLY
TRANSITION
WE
NOW
PROVE
THAT
T
T
THE
TRANSITIONS
TO
STATE
RIN
SCHEDULES
AND
ARE
EAGER
TRANSITIONS
SO
BOTH
THE
POINTS
T
T
AND
T
T
LIE
ON
THE
POPT
T
CURVE
SINCE
T
POPT
T
FOR
ALL
T
THE
THE
SLOPE
OF
POPT
T
AT
TIME
T
IS
AT
LEAST
R
THE
SLOPE
OF
T
AT
TIME
T
AND
STRICTLY
GREATER
SINCE
THE
GAP
BETWEEN
POPT
T
AND
T
MUST
ACCOMMODATE
THE
TRANSITION
COST
FROM
STATE
RTO
R
AT
TIME
T
THE
CONCAVITY
OF
POPT
T
IMPLIES
THAT
ITS
SLOPE
IS
GREATER
THAN
ROVER
THE
INTERVAL
T
T
POPT
T
SO
T
POPT
T
POPT
T
R
T
T
T
WHERE
THE
LAST
INEQUALITY
FOLLOWS
SINCE
STAYS
IN
STATE
RIN
THE
INTERVAL
T
T
FROM
LEMMA
WE
CAN
DEDUCE
THAT
WE
ONLY
NEED
TO
CONSIDER
A
SPECIFIC
EARLY
AND
EAGER
SCHEDULE
THE
ONE
THAT
IS
DETERMINED
BY
THE
E
P
VALUES
TO
DETERMINE
IF
A
P
COMPETITIVE
STRATEGY
EXISTS
WE
CAN
NOW
DEFINE
A
DECISION
PROCEDURE
EXISTS
THAT
TAKES
A
SYSTEM
AND
A
CONSTANT
PAND
OUTPUTS
YES
IF
A
P
COMPETITIVE
STRATEGY
EXISTS
FOR
THE
SYSTEM
AND
NO
OTHERWISE
THE
PROCEDURE
CAN
BE
MODIFIED
TO
ALSO
OUTPUT
A
P
COMPETITIVE
STRATEGY
IF
IT
EXISTS
WE
EMPLOY
A
DYNAMIC
PROGRAMMING
APPROACH
TO
CALCULATE
E
SI
P
FOR
I
K
WE
ALWAYS
START
WITH
THE
HIGH
POWER
STATE
AND
HENCE
E
P
SUPPOSE
WE
HAVE
COMPUTED
E
SJ
P
FOR
ALL
J
I
LET
TJBE
THE
EARLIEST
TIME
AT
WHICH
THE
SYSTEM
P
EAGERLY
TRANSITIONS
FROM
SJTO
SIGIVEN
T
FIGURE
THE
SOLID
LINE
IS
POPT
THE
DASHED
LINE
IS
THE
SCHEDULE
FROM
LEMMA
AND
THE
DASHED
DOTTED
LINE
IS
THE
POINT
LABELED
PIS
T
T
AND
P
IS
T
T
THE
IDEA
IS
TO
SHOW
THAT
AT
TIME
T
HAS
A
LOWER
COST
THAN
THAT
THE
TRANSITION
TO
SJIS
P
EAGER
AND
OCCURS
AT
TIME
E
SJ
P
IF
SUCH
A
TRANSITION
IS
NOT
POSSIBLE
THEN
WE
ASSIGN
TJ
WE
CAN
COMPUTE
TJIN
O
LOGK
TIME
AS
DESCRIBED
IN
LEMMA
THEN
E
SI
P
MINJ
ITJ
DETERMINING
EACH
E
SI
P
REQUIRES
EXAMINING
JDIFFERENT
POSSIBILITIES
SO
FINDING
ALL
THE
EARLY
TRANSITION
TIMES
FOR
ALL
STATES
TAKES
TIME
O
K
BY
LEMMA
WE
KNOW
THAT
IF
E
SI
P
IS
FINITE
FOR
SOME
STATE
SIWHERE
IP
K
WE
KNOW
THAT
A
P
COMPETITIVE
STRATEGY
EXISTS
ONE
CAN
QUICKLY
ELICIT
THE
SCHEDULE
BY
STARTING
FROM
STATE
KAND
RETRACING
THE
STATES
THAT
MINIMIZED
THE
EARLIEST
TRANSITION
TIME
WE
USE
THE
PROCEDURE
EXISTS
TO
DO
A
BISECTION
SEARCH
IN
THE
INTERVAL
AND
FIND
A
P
COMPETITIVE
STRATEGY
WHERE
PP
E
THE
TOTAL
TIME
TAKEN
IS
O
K
E
WE
NOW
TURN
OUR
ATTENTION
TO
ADAPTING
THIS
DYNAMIC
PROGRAMMING
TECHNIQUE
TO
SOLVE
PD
K
M
WHERE
A
BOUND
OF
MIS
SPECIFIED
ON
THE
NUMBER
OF
STATES
THAT
CAN
BE
USED
BY
THE
ONLINE
ALGORITHM
WE
INTRODUCE
A
NEW
PARAMETER
BMODIFYING
OUR
FUNCTION
TO
E
SI
P
B
WHERE
I
M
THE
INTUITION
IS
THAT
FUNCTION
EIS
NOW
REQUIRED
TO
RETURN
THE
EARLIEST
TIME
WHEN
THE
SYSTEM
CAN
TRANSITION
TO
STATE
SIWHILE
STAYING
ENTIRELY
BELOW
POPT
T
AND
USING
AT
MOST
B
STATES
FROM
SI
THE
BASE
CASE
IS
E
P
B
FOR
ALL
INTUITIVELY
E
SI
P
B
IS
DETERMINED
BY
THE
BEST
STATE
SJPRIOR
TO
SISUCH
THAT
AT
MOST
B
STATES
WERE
USED
TO
REACH
SJ
NOTICE
THAT
FOR
ANY
GIVEN
STATE
SIAND
FIXED
P
E
SJ
P
B
IS
NON
INCREASING
AS
BINCREASES
THEREFORE
AS
ABOVE
WE
CAN
WRITE
E
SI
P
B
MINJ
IT
J
WHERE
T
IS
THE
EARLIEST
TIME
WHEN
THE
SYSTEM
P
EAGERLY
TRANSITIONS
FROM
SJTO
SIGIVEN
THAT
THE
TRANSITION
TO
SJWAS
P
EAGER
AND
OCCURRED
AT
E
SJ
P
B
THE
RUNNING
TIME
INCREASES
BY
A
FACTOR
OF
MNOW
AND
IS
O
K
LOGK
LOG
E
A
PROBABILITY
BASED
ALGORITHM
KARLIN
ET
AL
STUDY
THE
TWO
STATE
CASE
WHEN
THE
LENGTH
OF
THE
IDLE
PERIOD
IS
GENERATED
BY
A
KNOWN
PROBABILITY
DISTRIBUTION
P
ALTHOUGH
THEY
EXAMINED
THE
PROBLEM
IN
THE
CONTEXT
OF
THE
SPIN
BLOCK
PROBLEM
THEIR
PROBLEM
IS
IDENTICAL
TO
OUR
TWO
STATE
CASE
THEY
OBSERVED
THAT
THE
EXPECTED
COST
OF
THE
ONLINE
STRATEGY
THAT
MAKES
THE
TRANSITION
TO
THE
SLEEP
STATE
AT
TIME
TIS
OC
P
T
DT
P
T
T
T
DT
WHERE
THE
POWER
CONSUMPTION
RATE
IN
THE
ACTIVE
STATE
IS
THE
POWER
CONSUMPTION
RATE
IN
THE
SLEEP
STATE
AND
IS
THE
TRANSITION
COST
BETWEEN
THE
TWO
STATES
THE
ONLINE
STRATEGY
THEN
SHOULD
SELECT
THE
TRANSITION
TIME
TTHAT
MINIMIZES
THIS
COST
THE
MULTI
STATE
CASE
PRESENTS
TWO
DISTINCT
CHALLENGES
THE
FIRST
IS
TO
DETERMINE
THE
OPTIMAL
SEQUENCE
OF
STATES
THROUGH
WHICH
AN
ONLINE
STRATEGY
SHOULD
TRANSITION
THROUGHOUT
THE
COURSE
OF
THE
IDLE
PERIOD
THEN
ONCE
THIS
SEQUENCE
HAS
BEEN
DETERMINED
THE
OPTIMAL
TRANSITION
TIMES
NEED
TO
BE
DETERMINED
OUR
PROOF
PROCEEDS
BY
ESTABLISHING
THAT
THE
ONLY
TRANSITION
TIMES
THAT
NEED
TO
BE
CONSIDERED
ARE
THE
OPTIMAL
TRANSITION
TIMES
FOR
TWO
STATES
SYSTEMS
SUPPOSE
FOR
EXAMPLE
THAT
WE
ARE
CONSIDERING
A
SEQUENCE
OF
STATE
TRANSITIONS
IN
WHICH
STATE
SIIS
FOLLOWED
BY
STATE
SJ
LET
TI
JDENOTE
THE
OPTIMAL
TRANSITION
TIME
FROM
STATE
SITO
SJIF
THESE
WERE
THE
ONLY
TWO
STATES
IN
THE
SYSTEM
THAT
IS
IF
SIWERE
THE
ACTIVE
STATE
AND
SJWERE
THE
ONLY
SLEEP
STATE
NOTE
THAT
TI
JCAN
BE
DETERMINED
BY
THE
EXPRESSION
ABOVE
WE
ESTABLISH
THAT
REGARDLESS
OF
THE
REST
OF
THE
SEQUENCE
THE
OPTIMAL
TRANSITION
POINT
FROM
STATE
SITO
SJIS
TI
J
WE
CALL
THE
TI
J
THE
PAIRWISE
OPTIMAL
TRANSITION
TIMES
LEMMAS
AND
ESTABLISH
THAT
THE
PAIRWISE
OPTIMAL
TRANSITION
TIMES
HAPPEN
IN
THE
RIGHT
ORDER
THAT
IS
FOR
I
K
J
TI
KTK
J
IF
THIS
IS
NOT
THE
CASE
THEN
ANY
SUBSEQUENCE
THAT
HAS
SIFOLLOWED
BY
SKFOLLOWED
BY
SJCAN
NOT
POSSIBLY
BE
THE
BEST
SEQUENCE
OF
STATES
NOTE
THAT
THE
TI
J
MAY
NOT
NECESSARILY
BE
UNIQUE
IN
GENERAL
WE
WILL
SELECT
THE
EARLIEST
TRANSITION
TIME
THAT
MINIMIZES
THE
COST
FOR
THE
TWO
STATE
SYSTEM
LEMMA
THEN
SHOWS
THAT
AS
LONG
AS
THE
PAIRWISE
OPTIMAL
TRANSITION
TIMES
ARE
IN
THE
RIGHT
ORDER
THEY
GIVE
THE
GLOBALLY
OPTIMAL
SET
OF
TRANSITION
TIMES
FOR
THAT
SUBSEQUENCE
OUR
ALGORITHM
THEN
USES
THIS
FACT
TO
FIND
THE
OPTIMAL
SEQUENCE
OF
STATES
BY
DYNAMIC
PROGRAMMING
NOTE
THAT
IT
IS
NOT
NECESSARY
TO
EXHAUSTIVELY
CONSIDER
ALL
POSSIBLE
SUBSEQUENCES
OPTIMAL
TRANSITION
TIMES
CONSIDER
A
PARTICULAR
SUBSEQUENCE
OF
L
STATES
SA
SAL
IN
ORDER
TO
AVOID
THE
DOUBLE
SUBSCRIPTS
THROUGH
OUT
THIS
SUBSECTION
WE
WILL
RENAME
OUR
SUBSEQUENCE
Q
QL
SINCE
THE
STRATEGY
MUST
START
IN
STATE
WE
CAN
ASSUME
THAT
FOR
I
J
DEFINE
I
JTO
BE
THE
COST
TO
TRANSITION
FROM
STATE
QITO
STATE
QJ
THAT
IS
I
J
DAI
AJ
FURTHERMORE
WE
WILL
REFER
TO
THE
POWER
CONSUMPTION
RATE
OF
STATE
QIAS
I
THAT
IS
I
AI
WE
WILL
CONSIDER
THE
STRATEGY
THAT
TRANSITIONS
THROUGH
THE
STATES
IN
THE
SUBSEQUENCE
Q
QL
SUP
POSE
THAT
WE
USE
TRANSITION
TIME
TITO
TRANSITION
FROM
STATE
QI
TO
STATE
QI
IT
WILL
BE
CONVENIENT
FOR
NOTATION
TO
DEFINE
TL
AND
THE
COST
OF
THE
STRATEGY
THAT
USES
THESE
TRANSITION
TIMES
IS
COST
T
TL
L
TJ
X
P
T
J
T
TJ
DT
XZOC
P
T
J
TJ
TJ
J
J
DT
J
TJ
J
TJ
THE
GOAL
IS
TO
PICK
THE
T
TLSO
AS
TO
MINIMIZE
THE
ABOVE
COST
THIS
IS
THE
OPTIMAL
COST
FOR
THE
SUBSE
QUENCE
QL
FOR
EACH
I
F
LG
LET
I
I
I
I
I
LEMMA
SUPPOSE
THAT
THERE
IS
AN
I
JSUCH
THAT
I
J
THEN
THERE
IS
A
A
STRICT
SUBSEQUENCE
OF
QL
WHOSE
OPTIMAL
COST
IS
NO
GREATER
THAN
THE
OPTIMAL
COST
FOR
QL
PROOF
CONSIDER
THE
FIRST
JSUCH
THAT
J
J
LET
T
TJ
TJ
T
L
BE
THE
SEQUENCE
OF
THRESHOLDS
THAT
MINIMIZES
THE
COST
OF
THIS
SEQUENCE
OF
STATES
DEFINE
THE
FOLLOWING
QUANTITIES
FJ
J
COST
T
TJ
TJ
TJ
T
J
TL
FJ
J
COST
T
TJ
TJ
TJ
TJ
T
L
FJ
J
COST
T
T
J
T
J
TJ
T
J
T
L
WE
WILL
SHOW
THAT
FJ
JIS
GREATER
THAN
OR
EQUAL
TO
A
WEIGHTED
AVERAGE
OF
FJ
J
AND
FJ
JWHICH
MEANS
THAT
IT
MUST
BE
GREATER
THAN
OR
EQUAL
TO
AT
LEAST
ONE
OF
THESE
VALUES
THIS
MEANS
THAT
THE
STRATEGY
THAT
TRANSITIONS
FROM
STATE
QJ
STATE
QJ
AND
THEN
IMMEDIATELY
TRANSITIONS
TO
STATE
QJAT
EITHER
TIME
TJ
OR
T
JIS
AT
LEAST
AS
GOOD
AS
THE
ORIGINAL
STRATEGY
SINCE
J
J
J
J
J
J
SKIPPING
STATE
J
ALTOGETHER
CAN
ONLY
IMPROVE
THE
STRATEGY
BELOW
WE
HAVE
AN
EXPRESSION
FOR
FJ
J
FJ
JWHICH
CAN
BE
DERIVED
FROM
THE
DEFINITION
FOR
THE
COST
IN
EQUATION
UNDER
FJ
JTHE
TRANSITION
FROM
STATE
QJ
QJ
IS
MOVED
FORWARD
FROM
TIME
T
J
TO
TIME
T
J
ANY
TIME
SPENT
IN
THE
INTERVAL
T
J
TJ
HAPPENS
AT
THE
HIGHER
POWER
RATE
OF
J
OF
J
THIS
IS
ACCOUNTED
FOR
IN
THE
FIRST
TWO
TERMS
OF
THE
SUM
HOWEVER
IDLE
TIMES
ENDING
IN
THE
INTERVAL
T
J
TJ
SAVE
ON
THE
TRANSITION
COST
WHICH
IS
ACCOUNTED
FOR
IN
THE
LAST
TERM
BELOW
FJ
J
FJ
J
Z
TJ
TJ
P
T
T
TJ
J
J
DT
Z
OC
TJ
P
T
T
J
T
J
J
J
DT
T
J
J
J
P
T
DT
DIVIDING
BY
J
J
THIS
BECOMES
T
TJ
T
FJ
J
FJ
JZJ
ZOCZJ
J
J
TJ
P
T
T
TJ
DT
T
J
P
T
TJ
TJ
DT
T
J
J
P
T
DT
BELOW
WE
USE
THE
DEFINITION
OF
COST
IN
EQUATION
TO
GET
AN
EXPRESSION
FOR
FJ
J
FJ
J
NOTE
THAT
IN
FJ
J
THE
TRANSITION
FROM
STATE
QJ
TO
STATE
QIS
MOVED
BACK
FROM
TIME
T
JTO
TIME
TJ
THUS
FJ
J
WILL
SPEND
J
JMORE
POWER
THAN
FJ
J
FOR
ANY
TIME
SPENT
IN
THE
INTERVAL
T
J
TJ
FURTHERMORE
FJ
J
WILL
HAVE
AN
ADDITIONAL
TRANSITION
COST
OF
J
JFOR
THOSE
INTERVALS
THAT
END
IN
THE
PERIOD
T
J
TJ
T
ZJZOC
FJ
J
FJ
J
T
J
P
T
T
TJ
J
J
DT
T
J
P
T
TJ
TJ
J
J
DT
T
J
J
JP
T
DT
DIVIDING
BY
J
J
THIS
BECOMES
T
TJ
T
FJ
J
FJ
J
ZJ
ZOCZJ
J
J
T
J
P
T
T
TJ
DT
TJ
P
T
TJ
TJ
DT
TJ
P
T
DT
J
COMPARING
EQUATIONS
AND
THE
EXPRESSIONS
ARE
ALMOST
IDENTICAL
EXCEPT
FOR
THE
IN
THE
LAST
TERM
T
SINCE
J
JAND
R
J
T
J
P
T
DT
WE
HAVE
THAT
FJ
J
FJ
J
J
J
FJ
J
FJ
J
J
J
LET
W
J
J
AND
W
J
J
NOTE
THAT
BOTH
W
AND
W
AT
LEAST
REARRANGING
WE
GET
THAT
W
F
W
F
F
W
W
J
J
W
W
J
J
J
J
NOW
SUPPOSE
THAT
WE
CONSIDER
ONLY
THE
TWO
STATE
SYSTEM
CONSISTING
OF
STATE
QI
AND
STATE
QI
WE
WILL
LET
TIDENOTE
THE
OPTIMAL
THRESHOLD
TIME
IF
THESE
ARE
THE
ONLY
TWO
STATES
IN
THE
SYSTEM
WE
HAVE
THAT
TIIS
THE
TIME
TTHAT
MINIMIZES
P
T
I
TDT
ZOCP
T
I
T
I
T
T
I
I
DT
NOTE
THAT
THE
VALUE
OF
TTHAT
RESULTS
IN
THE
MINIMUM
ABOVE
MAY
NOT
BE
UNIQUE
IN
THIS
CASE
WE
TAKE
TTO
BE
THE
SMALLEST
VALUE
WHICH
ACHIEVES
THE
MINIMUM
ALSO
NOTE
THAT
BY
SUBTRACTING
THE
TERM
OCP
T
ITDT
WHICH
IS
INDEPENDENT
OF
T
AND
DIVIDING
BY
I
IIN
THE
ABOVE
DEFINITION
IT
CAN
BE
SEEN
THAT
TI
ARGMINTF
I
T
WHERE
T
F
T
P
T
TDT
OC
T
P
T
T
DT
NOTE
THAT
FOR
A
TWO
STATE
SYSTEM
WHOSE
ACTIVE
STATE
AND
SLEEP
STATES
HAS
POWER
CONSUMPTION
RATES
OF
AND
RESPECTIVELY
AND
WHOSE
TRANSITION
COST
IS
F
T
DENOTES
THE
EXPECTED
POWER
CONSUMED
BY
AN
ONLINE
STRATEGY
THAT
TRANSITIONS
TO
THE
SLEEP
STATE
AT
TIME
T
WE
WILL
SHOW
THAT
FOR
A
PARTICULAR
SUBSEQUENCE
OF
STATES
IF
WE
MINIMIZE
THE
COST
OVER
ALL
CHOICES
FOR
THE
THRESHOLDS
THE
RESULTING
THRESHOLDS
ARE
THOSE
OBTAINED
BY
THE
PAIR
WISE
OPTIMIZATION
ABOVE
FIRST
HOWEVER
WE
MUST
ESTABLISH
THAT
THE
TIVALUES
HAVE
THE
CORRECT
ORDERING
LEMMA
IF
I
I
THEN
TI
TI
PROOF
INTUITIVELY
IIS
THE
RATIO
OF
THE
ADDITIONAL
POWER
COST
OF
BEING
IN
STATE
QIINSTEAD
OF
STATE
QI
OVER
THE
TRANSITION
COSTS
BETWEEN
THE
TWO
STATES
IT
STANDS
TO
REASON
THAT
THE
LARGER
THIS
COST
THE
SOONER
ONE
WOULD
WANT
TO
TRANSITION
FROM
STATE
QI
TO
STATE
QI
WE
WILL
FORMALIZE
THIS
ARGUMENT
USING
A
PROOF
BY
CONTRADICTION
SUPPOSE
THAT
WE
HAVE
TI
TI
AND
I
I
THE
PROOF
WILL
MAKE
USE
OF
THE
DEFINITION
OF
F
T
GIVEN
ABOVE
TIIS
THE
SMALLEST
VALUE
FOR
TWHICH
ATTAINS
THE
MINIMUM
OF
F
I
T
SINCE
TI
TI
WE
KNOW
THAT
F
I
TI
F
I
TI
BY
THE
DEFINITION
OF
TI
WE
HAVE
THAT
F
I
TI
F
I
TI
THUS
IT
SHOULD
BE
THE
CASE
THAT
F
I
TI
F
I
TI
F
I
TI
F
I
TI
USING
THE
DEFINITION
OF
F
T
ABOVE
FOR
ANY
T
T
F
T
F
T
Z
P
T
TT
DT
OC
T
P
T
T
T
DT
Z
P
T
DT
THE
QUANTITY
INSIDE
THE
SQUARE
BRACES
ABOVE
IS
NON
NEGATIVE
THIS
IMPLIES
THAT
THE
QUANTITY
F
T
F
T
IS
NON
DECREASING
IN
THIS
HOWEVER
CONTRADICTS
INEQUALITY
AND
THE
FACT
THAT
I
I
FINALLY
WE
PROVE
THE
MAIN
LEMMA
WHICH
STATES
THAT
THE
TRANSITION
TIMES
ARE
SIMULTANEOUSLY
OPTIMIZED
AT
THE
PAIRWISE
OPTIMAL
TRANSITION
POINTS
LEMMA
FOR
A
GIVEN
SUBSEQUENCE
OF
STATES
QL
IF
TI
TIFOR
ALL
I
F
LG
THEN
THE
MINIMUM
TOTAL
COST
IS
ACHIEVED
FOR
COST
T
TL
PROOF
THE
BASIC
IDEA
IS
THAT
WE
CAN
INTERPRET
COST
T
TL
OCP
T
LTDTAS
THE
SUM
OF
THE
POWER
CONSUMED
IN
LTWO
STATE
SYSTEMS
WHERE
THE
ITHSYSTEM
FOR
I
L
HAS
STATES
WHOSE
POWER
CONSUMP
TION
RATES
ARE
I
I
AND
AND
THE
COST
TO
TRANSITION
BETWEEN
THE
TWO
IS
I
I
NOTE
THAT
OCP
T
LTDT
IS
A
CONSTANT
INDEPENDENT
OF
THE
CHOICE
OF
TI
AFTER
RESCALING
ONE
CAN
WRITE
THIS
EXPRESSION
AS
A
LINEAR
COMBINATION
OF
THE
F
I
TI
TERMS
SINCE
TIMINIMIZES
F
I
T
AND
THE
TIVALUES
HAVE
THE
RIGHT
ORDERING
THIS
IMPLIES
THAT
COST
T
TL
IS
MINIMIZED
BY
SETTING
TI
TIFOR
I
L
WE
WILL
ESTABLISH
BELOW
THAT
WE
CAN
REWRITE
AS
FOLLOWS
COST
T
T
ZOCP
T
TDT
LL
LTIOC
Z
I
P
T
I
I
TDT
Z
P
T
I
I
TI
I
I
DT
SO
BY
RESCALING
WE
GET
THAT
COST
T
TL
ZOCP
T
LTDT
X
I
IF
I
TI
WE
WANT
TO
CHOOSE
T
TLTO
MINIMIZE
THIS
EXPRESSION
SINCE
T
TLAND
EACH
TI
ARGMINTF
I
T
IT
FOLLOWS
THAT
THE
MINIMUM
IS
ATTAINED
BY
SETTING
TI
TIFOR
EACH
I
TO
COMPLETE
THE
PROOF
WE
SHOW
THE
EQUIVALENCE
OF
AND
IT
SUFFICES
TO
SHOW
THAT
AND
INTEGRATE
THE
SAME
EXPRESSION
OVER
EACH
INTERVAL
TI
TI
FOR
I
L
THE
INTEGRAND
IN
OVER
THE
INTERVAL
TI
TI
IS
I
P
T
I
T
TI
J
TJ
TJ
J
J
J
AND
THE
INTEGRAND
IN
IS
I
L
P
T
X
J
J
TJ
J
J
X
J
J
L
T
J
J
I
THE
SUMMATIONS
OVER
THE
JINDICES
IN
TELESCOPE
TO
SHOW
THAT
THE
TWO
EXPRESSIONS
ARE
IDENTICAL
THE
OPTIMAL
STATE
SEQUENCE
WE
NOW
PRESENT
A
SIMPLE
POLYNOMIAL
TIME
ALGORITHM
TO
OBTAIN
THE
OPTIMAL
STATE
SEQUENCE
FOR
A
GIVEN
SYSTEM
FIRST
FOR
EACH
PAIR
I
J
JK
LET
TI
JDENOTE
THE
OPTIMAL
TRANSITION
POINT
IF
SIAND
SJWERE
THE
ONLY
TWO
STATES
IN
THE
SYSTEM
THE
TIME
COMPLEXITY
OF
DETERMINING
A
SINGLE
TI
JDEPENDS
ON
THE
REPRESENTATION
OF
THE
PROBABILITY
DISTRIBUTION
IN
PRACTICE
THIS
IS
MOST
LIKELY
TO
BE
ESTIMATED
BY
A
FINITE
HISTOGRAM
WITH
BBINS
STARTING
AT
TIME
SAMPLED
AT
A
UNIFORM
DISCRETE
INTERVAL
OF
IT
FOLLOWS
THAT
BIN
ICORRESPONDS
TO
TIME
I
IT
IS
NOT
DIFFICULT
TO
GENERALIZE
THIS
FOR
VARIABLE
SIZED
BINS
WE
WILL
ALSO
ASSUME
THAT
ALL
TRANSITION
TIMES
OCCUR
AT
SOME
I
THE
HEIGHT
OF
BIN
IIS
H
I
AND
THIS
IMPLIES
THAT
THE
PROBABILITY
THAT
THE
IDLE
TIME
TEQUALS
H
I
IO
IIS
GIVEN
BY
PH
I
IN
ALGORITHM
WE
CALCULATE
ACC
I
AND
ACCT
I
VALUES
WHICH
ARE
T
DTAND
T
DTAND
WE
THEN
USE
THEM
TO
EVALUATE
TI
JVALUES
WE
CAN
RE
WRITE
THE
EXPRESSION
FOR
THE
COST
OF
A
TWO
STATE
SYSTEM
IN
EQUATION
AS
T
I
OC
P
T
TDT
J
T
P
T
TDT
I
J
T
I
J
Z
P
T
DT
BOBO
WE
ALSO
DENOTE
T
DTAND
T
DTAS
TOTALAND
TOTALTRESPECTIVELY
USING
THE
PRE
CALCULATED
VALUES
ABOVE
THE
COST
OF
TRANSITIONING
FROM
STATE
SITO
STATE
SJAT
TIME
LIS
I
ACCT
L
IL
JL
I
J
TOTAL
ACC
L
J
TOTALT
ACCT
L
ONCE
THE
TI
J
ARE
FOUND
WE
SWEEP
THROUGH
THEM
IN
NON
DECREASING
ORDER
KEEPING
A
RUNNING
TAB
OF
THE
BEST
SUB
SCHEDULES
THAT
WE
CAN
ACHIEVE
ENDING
IN
EACH
STATE
SIAT
EACH
POINT
IN
TIME
WHEN
WE
ENCOUNTER
A
TI
J
WE
CHECK
TO
SEE
IF
TRANSITIONING
FROM
SITO
SJCAN
IMPROVE
THE
CURRENT
BEST
SUB
SCHEDULE
ENDING
IN
SJ
AND
IF
IT
DOES
UPDATE
OUR
DATA
STRUCTURE
TO
REFLECT
IT
A
GIVEN
STRATEGY
DIVIDES
TIME
INTO
INTERVALS
WHERE
EACH
INTERVAL
IS
THE
PERIOD
OF
TIME
SPENT
IN
A
PARTICULAR
STATE
THE
EXPECTED
COST
FOR
A
STRATEGY
GIVEN
IN
EQUATION
IS
OBTAINED
BY
SUMMING
OVER
THE
EXPECTED
COST
GREEN
IT
TOWARD
A
SCIENCE
OF
POWER
MANAGEMENT
KRISHNA
KANT
INTEL
CORP
A
FORMAL
UNDERSTANDING
OF
ENERGY
AND
COMPUTATION
TRADEOFFS
WILL
LEAD
TO
SIGNIFICANTLY
MORE
ENERGY
EFFICIENT
HARDWARE
AND
SOFTWARE
DESIGNS
OMPUTERS
ORIGINALLY
EVOLVED
WITH
THE
GOAL
OF
AUTOMATING
REPETI
TIVE
CALCULATIONS
AND
EVER
SINCE
HAVE
FOCUSED
ON
DOING
THE
JOB
FASTER
AND
CHEAPER
CONSE
QUENTLY
THE
DOMINANT
RESOURCES
IN
MOST
COMPUTATIONAL
SETTINGS
HAVE
BEEN
PROCESSING
TIME
AND
MEMORY
OCCUPANCY
BEGINNING
IN
THE
RESEARCH
ERS
HAVE
EXPLOITED
COMPUTATIONAL
COMPLEXITY
THEORY
TO
ACHIEVE
REMARK
ABLE
SUCCESSES
IN
DEVISING
FASTER
ALGORITHMS
PROVING
BOUNDS
ON
COMPUTATIONAL
SPEED
AND
DEFINING
CLASSES
OF
PROBLEMS
OF
EQUIVALENT
DIF
FICULTY
TODAY
THIS
HUGE
VIBRANT
FIELD
OFFERS
BENEFITS
THAT
EXTEND
FAR
BEYOND
THEORETICAL
COMPUTER
SCIENCE
INTO
THE
DESIGN
OF
SYSTEMS
THAT
DIRECTLY
SOLVE
SOCIALLY
RELEVANT
PROBLEMS
WITH
THE
EXPLOSIVE
GROWTH
OF
INTERNET
ENABLED
GADGETS
OF
ALL
TYPES
IT
ENERGY
CONSUMPTION
AND
SUS
TAINABILITY
IMPACT
ARE
EXPECTED
TO
CONTINUE
CLIMBING
WELL
INTO
THE
FUTURE
ALTHOUGH
THE
PROBLEM
IS
WELL
RECOGNIZED
AND
AGGRESSIVE
EFFORTS
TO
ADDRESS
IT
ARE
UNDER
WAY
IN
BOTH
INDUSTRY
AND
ACADEMIA
MUCH
OF
THE
EFFORT
TENDS
TO
BE
EMPIRICALLY
DRIVEN
JUST
AS
THE
FORMAL
NOTIONS
OF
COMPUTATIONAL
COMPLEXITY
HAVE
IMMENSELY
BENEFITED
HARDWARE
AND
SOFTWARE
DESIGN
A
FORMAL
TREAT
MENT
OF
ENERGY
POWER
AND
THERMAL
ISSUES
SHOULD
ALSO
LEAD
TO
SIGNIFICANT
ADVANCES
IN
HOW
SUCH
SYSTEMS
ARE
DESIGNED
IN
THE
FUTURE
THE
VISION
IS
OF
A
WELL
DEVELOPED
BODY
OF
KNOWLEDGE
THAT
CAN
GUIDE
VARIOUS
POWER
PERFOR
MANCE
TRADEOFFS
IN
THE
DESIGN
TUNING
AND
OPERATION
OF
IT
DEVICES
RANG
ING
FROM
NANOWATT
LEVEL
EMBEDDED
DEVICES
TO
LARGE
SERVER
FARMS
CONSUM
ING
TENS
OF
MEGAWATTS
OF
POWER
ALTHOUGH
POWER
AND
ENERGY
ARE
OFTEN
USED
INTERCHANGEABLY
THERE
ARE
IMPORTANT
DISTINCTIONS
TRANSACTIONAL
APPLICATIONS
ARE
BETTER
DESCRIBED
IN
TERMS
OF
THROUGHPUT
AND
AVERAGE
POWER
WHEREAS
COMPLETION
TIME
AND
ENERGY
CONSUMED
ARE
MORE
MEANING
FUL
FOR
NONTRANSACTIONAL
APPLICATIONS
POWER
CONSUMPTION
CAN
OFTEN
BE
INCREASED
OVER
SHORT
PERIODS
TO
ACCELERATE
COMPUTATION
OR
ACCUMU
LATE
MORE
DATA
AND
THEREBY
MINIMIZE
OVERALL
ENERGY
CONSUMPTION
ENERGY
VERSUS
COMPUTATION
OUR
CURRENT
UNDERSTANDING
OF
ENERGY
ISSUES
IS
RUDIMENTARY
FOR
EXAMPLE
IS
THERE
A
NOTION
OF
ENERGY
OR
POWER
COMPLEXITY
OF
AN
ALGORITHM
DISTINCT
FROM
COMPUTATIONAL
COM
PLEXITY
CERTAINLY
A
PROGRAM
ENERGY
CONSUMPTION
STRONGLY
DEPENDS
ON
THE
NUMBER
OF
INSTRUCTIONS
EXECUTED
AND
THE
NUMBER
OF
ACCESSES
TO
THE
MEMORY
HIERARCHY
THESE
ARE
THE
SAME
FACTORS
THAT
DETERMINE
A
PRO
GRAM
COMPLETION
TIME
ALSO
AN
INSTRUCTION
THAT
TAKES
FEWER
CLOCK
CYCLES
TO
EXECUTE
GENERALLY
ALSO
CON
SUMES
LESS
ENERGY
YET
THE
CORRESPONDENCE
BETWEEN
COMPLETION
TIME
AND
ENERGY
CON
SUMPTION
IS
NOT
ONE
TO
ONE
FOR
EXAMPLE
THE
ORDER
OF
CERTAIN
OPERA
TIONS
AND
BIT
REPRESENTATION
OF
DATA
CAN
ALTER
THE
ENERGY
CONSUMED
WITHOUT
NECESSARILY
CHANGING
THE
COMPLETION
TIME
AT
A
COARSER
LEVEL
OF
DETAIL
HOW
EVER
COMPUTATION
AND
POWER
ENERGY
COMPLEXITY
MAY
DIFFER
BECAUSE
OF
VARIOUS
POWER
MANAGEMENT
ACTIONS
AVAILABLE
IN
MODERN
COMPUTING
HARD
WARE
FOR
EXAMPLE
MANY
DEVICES
PROVIDE
THE
CAPABILITY
TO
TRANSITION
TO
ONE
OR
MORE
LOWER
POWER
MODES
WHEN
IDLE
IF
THE
TRANSITION
LATENCIES
INTO
AND
OUT
OF
LOWER
POWER
MODES
ARE
NEGLIGIBLE
ENERGY
CONSUMPTION
CAN
BE
LOWERED
SIMPLY
BY
EXPLOITING
THESE
STATES
UNFORTUNATELY
THE
TRANSITION
LATEN
CIES
ARE
RARELY
NEGLIGIBLE
AND
THUS
THE
USE
OF
LOW
POWER
MODES
IMPEDES
PER
FORMANCE
TO
MINIMIZE
THIS
IMPACT
IT
IS
NECESSARY
TO
ALTER
THE
WORKLOAD
SO
THAT
MANY
SMALL
IDLE
PERIODS
CAN
IEEE
PUBLISHED
BY
THE
IEEE
COMPUTER
SOCIETY
SEPTEMBER
WHAT
IS
NEEDED
IS
A
HIERARCHY
OF
MODELS
WITH
VARYING
ABSTRACTION
LEVELS
IN
ADDITION
THE
MODELS
SHOULD
REFLECT
THE
PHYSICAL
AND
LOGICAL
STRUC
TURE
OF
COMPUTING
SYSTEMS
IN
TERMS
OF
MULTIPLE
SUBSYSTEMS
OPERATING
AT
MULTIPLE
GRANULARITIES
OF
CONTROL
COL
LECTIVELY
SUCH
MODELS
WILL
FORM
THE
BEGINNINGS
OF
THE
SCIENCE
OF
POWER
MANAGEMENT
SCIPM
A
COMPREHENSIVE
SCIPM
SHOULD
LET
RESEARCHERS
ADDRESS
MANY
OTHER
FACETS
OF
ENERGY
CONSUMPTION
AS
WELL
AS
THE
NEED
FOR
REDUCING
ENERGY
CON
SUMPTION
GROWS
SO
DOES
THE
NUMBER
OF
CONTROL
KNOBS
HARDWARE
DESIGN
ERS
ARE
DEVELOPING
NEW
MECHANISMS
FOR
REDUCING
ENERGY
CONSUMPTION
MOST
OF
WHICH
HAVE
PERFORMANCE
IMPLICATIONS
FOR
EXAMPLE
DEEPER
SLEEP
STATES
AND
MORE
SPEED
STATES
MAY
BE
ADDED
TO
VARIOUS
PLATFORM
COMPONENTS
THE
CRUCIAL
QUESTION
IS
WHETHER
FORMAL
TECHNIQUES
CAN
INDICATE
HOW
MANY
STATES
ARE
NEEDED
AND
WHAT
THEIR
ESSENTIAL
CHARACTERISTICS
SHOULD
BE
FURTHER
SINCE
A
CROSS
PRODUCT
OF
POWER
STATES
OF
ALL
COMPONENTS
QUICKLY
BECOMES
UNWIELDY
A
FORMAL
MODEL
IS
NEEDED
TO
ESTABLISH
RELATION
SHIPS
BETWEEN
VARIOUS
STATES
NOT
ONLY
IN
TERMS
OF
THEIR
NUMBER
AND
CHAR
ACTERISTICS
BUT
ALSO
IN
TERMS
OF
THEIR
SIMULTANEOUS
USAGE
POWER
MANAGEMENT
RELATES
TO
MORE
THAN
JUST
COORDINATED
POWER
STATE
MANAGEMENT
AS
SEMICONDUC
TOR
TECHNOLOGY
BEGINS
TO
REACH
LIMITS
OF
RELIABLE
OPERATION
COMPUTATION
MODELS
WILL
NEED
TO
CHANGE
TO
ACCOUNT
FOR
THIS
UNRELIABILITY
SCIPM
THUS
MUST
ADDRESS
THREE
ATTRIBUTES
TOGETHER
PER
FORMANCE
ENERGY
AND
RELIABILITY
IT
MAY
BE
NECESSARY
TO
INVENT
NEW
HARDWARE
ARCHITECTURES
THAT
SLOW
DOWN
ERROR
PROPAGATION
WHICH
WOULD
RAISE
A
NEW
SET
OF
ISSUES
IN
MODEL
ING
THIS
THREE
WAY
TRADEOFF
CAN
WE
DEVELOP
A
SCIENCE
TO
START
EXAMINING
SUCH
TRADEOFFS
BEFORE
THE
PRODUCTS
NEED
TO
DEAL
WITH
IT
ANOTHER
BENEFIT
OF
A
FORMAL
APPROACH
IS
THE
EXPLORATION
OF
MORE
COMPUTER
ENERGY
EFFICIENT
SEMANTICALLY
EQUIVA
LENT
SOFTWARE
TRANSFORMATIONS
FOR
EXAMPLE
AUTOMATED
REORDERING
OF
OPERATIONS
TO
ALLOW
OPTIMUM
USE
OF
AVAILABLE
POWER
MANAGEMENT
FUNCTIONS
FINALLY
SCIPM
SHOULD
IDEALLY
HELP
QUANTIFY
THE
CAPABILITIES
AND
LIMITATIONS
OF
ENERGY
EFFICIENCY
MECHANISMS
AT
LOWER
LEVELS
OF
DETAIL
INCLUDING
HARDWARE
ARCHITECTURE
CIR
CUIT
DESIGN
SEMICONDUCTOR
PROCESSES
AND
MATERIALS
PHYSICS
SCIPM
WORKSHOP
WITH
THESE
GOALS
IN
MIND
THE
NATIONAL
SCIENCE
FOUNDATION
SPON
SORED
A
SCIPM
WORKSHOP
IN
ARLINGTON
VIRGINIA
IN
APRIL
CS
VT
EDU
THE
WORKSHOP
GENERATED
TREMENDOUS
INTEREST
AMONG
RESEARCH
ERS
ACROSS
THE
US
FACING
VARIOUS
DATA
CENTER
RELATED
ISSUES
A
SIGNIFI
CANT
NUMBER
OF
THEORETICAL
COMPUTER
SCIENTISTS
ALSO
PARTICIPATED
THE
ATTENDEES
WERE
DIVIDED
INTO
F
IVE
GROUPS
TO
FOCUS
THE
DELIBERATIONS
THE
PHYSICALS
GROUP
EXAMINED
ISSUES
RELATED
TO
PHYSICAL
INFRA
STRUCTURE
WHICH
OFTEN
REPRESENTS
A
SUBSTANTIAL
PART
OF
DATA
CENTER
COST
AND
COULD
EASILY
CONSUME
MORE
THAN
PERCENT
OF
THE
ENERGY
THESE
INCLUDE
COOLING
FROM
ROOM
LEVEL
DOWN
TO
SERVER
FANS
ELECTRICAL
CONVERSION
AND
DISTRIBUTION
FROM
SUBSTATION
TO
BOARD
LEVEL
VOLTAGE
REGULATORS
DIS
PLAYS
AND
SO
ON
THE
HARDWARE
AND
ARCHITECTURE
GROUP
WAS
TASKED
WITH
ADDRESSING
ISSUES
RANGING
FROM
NEW
MATERIALS
AND
PROCESSES
TO
NEW
ARCHITECTURES
AND
POWER
MANAGEMENT
KNOBS
THE
SOFTWARE
AND
MIDDLEWARE
GROUP
WAS
CONCERNED
NOT
ONLY
WITH
SOPHIS
TICATED
MULTILEVEL
AND
MULTIDOMAIN
ENERGY
MANAGEMENT
BUT
ALSO
WITH
ENERGY
EFFICIENT
SOFTWARE
DESIGN
THE
STORAGE
GROUP
CONSIDERED
THE
ENERGY
IMPACT
OF
THE
INCREASINGLY
DATA
INTENSIVE
NATURE
OF
COMPUTING
AND
VARIOUS
NONVOLATILE
STORAGE
TECHNOLO
GIES
CURRENTLY
UNDER
DEVELOPMENT
FINALLY
THE
NETWORKING
GROUP
WAS
ASKED
TO
EXPLORE
NETWORK
RELATED
ENERGY
MANAGEMENT
ISSUES
RANGING
FROM
PLATFORM
LEVEL
INTERCONNECTS
TO
DATA
CENTER
FABRICS
TO
THE
INTERNET
ROUTING
INFRASTRUCTURE
EACH
GROUP
WAS
SEEDED
WITH
A
FEW
THEORETICAL
COMPUTER
SCIENTISTS
TO
BLEND
DOMAIN
KNOWLEDGE
WITH
ABSTRACTIONS
THE
SCIPM
DELIBERATIONS
SUP
PORTED
A
MORE
SCIENTIFIC
INVESTIGATION
OF
POWER
THERMAL
ISSUES
AND
DEVELOP
MENT
OF
GOOD
METRICS
AND
MODELS
AT
LEAST
TWO
GROUPS
EXPRESSED
THE
NEED
FOR
A
BIG
O
NOTATION
FOR
POWER
SEVERAL
GROUPS
EMPHASIZED
THE
IMPORTANCE
OF
MULTIDISCIPLINARY
EDUCATION
SO
THAT
FUTURE
COMPUTER
SCIENTISTS
ARE
BETTER
PREPARED
TO
DEAL
WITH
PHYSICALS
TYPE
ENERGY
ISSUES
IN
PARTICULAR
AND
MORE
BROADLY
THE
NUMEROUS
SUSTAINABILITY
CHALLENGES
INHERENT
IN
IT
EQUIPMENT
AND
INFRASTRUCTURE
DESIGN
FINALLY
WORKSHOP
PARTICIPANTS
RECOGNIZED
THE
NEED
TO
TAP
INTO
THE
ENORMOUS
POTENTIAL
OF
IT
TO
TACKLE
THE
REMAINING
PERCENT
OF
THE
PROBLEM
THAT
IS
USE
IT
SOLUTIONS
TO
SIGNIFICANTLY
REDUCE
THE
PERCENT
OF
EMISSIONS
NOT
ATTRIBUTED
TO
IT
ITSELF
ITANDENERGY
PDF
FORMAL
UNDERSTANDING
OF
ENERGY
AND
COMPUTATION
TRADEOFFS
WILL
OPEN
UP
NEW
AREAS
OF
INVESTIGATION
AND
LEAD
TO
SIGNIFICANTLY
MORE
ENERGY
EFFICIENT
HARDWARE
AND
SOFTWARE
DESIGNS
WHILE
PRESERVING
THE
CONTINUED
IMPROVEMENT
IN
PERFORMANCE
THAT
THE
EMERGING
APPLICATIONS
DEMAND
A
FORMAL
APPROACH
WILL
HOPEFULLY
LEAD
TO
ENERGY
MANAGEMENT
SOLUTIONS
THAT
ARE
NOT
ONLY
EFFECTIVE
BUT
ALSO
EASY
TO
IMPLEMENT
AND
VALIDATE
TRADITIONALLY
ENERGY
CONSUMPTION
AND
SUPPLY
SIDES
HAVE
BEEN
FAIRLY
WELL
ISOLATED
HOWEVER
WITH
A
GREATER
ROLE
PLAYED
BY
VARIABLE
ENERGY
SOURCES
SUCH
AS
RENEWABLE
AND
HARVESTED
ENERGY
IT
IS
IMPERATIVE
TO
DESIGN
SYS
TEMS
THAT
ARE
NOT
JUST
ENERGY
EFFICIENT
IN
PROCEEDINGS
OF
THE
ACM
INTERNATIONAL
SYMPOSIUM
ON
COMPUTER
ARCHITECTURE
SAN
DIEGO
CA
JUNE
POWER
PROVISIONING
FOR
A
WAREHOUSE
SIZED
COMPUTER
XIAOBO
FAN
WOLF
DIETRICH
WEBER
LUIZ
ANDRÉ
BARROSO
GOOGLE
INC
AMPHITHEATRE
PKWY
MOUNTAIN
VIEW
CA
XIAOBO
WOLF
LUIZ
GOOGLE
COM
ABSTRACT
LARGE
SCALE
INTERNET
SERVICES
REQUIRE
A
COMPUTING
INFRASTRUCTURE
THAT
CAN
BE
APPROPRIATELY
DESCRIBED
AS
A
WAREHOUSE
SIZED
COMPUTING
SYSTEM
THE
COST
OF
BUILDING
DATACENTER
FACILITIES
CAPABLE
OF
DE
LIVERING
A
GIVEN
POWER
CAPACITY
TO
SUCH
A
COMPUTER
CAN
RIVAL
THE
RE
CURRING
ENERGY
CONSUMPTION
COSTS
THEMSELVES
THEREFORE
THERE
ARE
STRONG
ECONOMIC
INCENTIVES
TO
OPERATE
FACILITIES
AS
CLOSE
AS
POSSIBLE
TO
MAXIMUM
CAPACITY
SO
THAT
THE
NON
RECURRING
FACILITY
COSTS
CAN
BE
BEST
AMORTIZED
THAT
IS
DIFFICULT
TO
ACHIEVE
IN
PRACTICE
BECAUSE
OF
UNCERTAINTIES
IN
EQUIPMENT
POWER
RATINGS
AND
BECAUSE
POWER
CON
SUMPTION
TENDS
TO
VARY
SIGNIFICANTLY
WITH
THE
ACTUAL
COMPUTING
AC
TIVITY
EFFECTIVE
POWER
PROVISIONING
STRATEGIES
ARE
NEEDED
TO
DETER
MINE
HOW
MUCH
COMPUTING
EQUIPMENT
CAN
BE
SAFELY
AND
EFFICIENTLY
HOSTED
WITHIN
A
GIVEN
POWER
BUDGET
IN
THIS
PAPER
WE
PRESENT
THE
AGGREGATE
POWER
USAGE
CHARACTER
ISTICS
OF
LARGE
COLLECTIONS
OF
SERVERS
UP
TO
THOUSAND
FOR
DIF
FERENT
CLASSES
OF
APPLICATIONS
OVER
A
PERIOD
OF
APPROXIMATELY
SIX
MONTHS
THOSE
OBSERVATIONS
ALLOW
US
TO
EVALUATE
OPPORTUNITIES
FOR
MAXIMIZING
THE
USE
OF
THE
DEPLOYED
POWER
CAPACITY
OF
DATACENTERS
AND
ASSESS
THE
RISKS
OF
OVER
SUBSCRIBING
IT
WE
FIND
THAT
EVEN
IN
WELL
TUNED
APPLICATIONS
THERE
IS
A
NOTICEABLE
GAP
BETWEEN
ACHIEVED
AND
THEORETICAL
AGGREGATE
PEAK
POWER
USAGE
AT
THE
CLUSTER
LEVEL
THOUSANDS
OF
SERVERS
THE
GAP
GROWS
TO
ALMOST
IN
WHOLE
DATACENTERS
THIS
HEADROOM
CAN
BE
USED
TO
DEPLOY
ADDITIONAL
COM
PUTE
EQUIPMENT
WITHIN
THE
SAME
POWER
BUDGET
WITH
MINIMAL
RISK
OF
EXCEEDING
IT
WE
USE
OUR
MODELING
FRAMEWORK
TO
ESTIMATE
THE
POTENTIAL
OF
POWER
MANAGEMENT
SCHEMES
TO
REDUCE
PEAK
POWER
AND
ENERGY
USAGE
WE
FIND
THAT
THE
OPPORTUNITIES
FOR
POWER
AND
ENERGY
SAVINGS
ARE
SIGNIFICANT
BUT
GREATER
AT
THE
CLUSTER
LEVEL
THOUSANDS
OF
SERVERS
THAN
AT
THE
RACK
LEVEL
TENS
FINALLY
WE
ARGUE
THAT
SYSTEMS
NEED
TO
BE
POWER
EFFICIENT
ACROSS
THE
ACTIVITY
RANGE
AND
NOT
ONLY
AT
PEAK
PERFORMANCE
LEVELS
CATEGORIES
AND
SUBJECT
DESCRIPTORS
C
COMPUTER
SYSTEMS
OR
GANIZATION
GENERAL
SYSTEM
ARCHITECTURES
C
COMPUTER
SYS
TEMS
ORGANIZATION
PERFORMANCE
OF
SYSTEMS
DESIGN
STUDIES
MEA
SUREMENT
TECHNIQUES
MODELING
TECHNIQUES
GENERAL
TERMS
MEASUREMENT
EXPERIMENTATION
KEYWORDS
POWER
MODELING
POWER
PROVISIONING
ENERGY
EFFICIENCY
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
ISCA
JUNE
SAN
DIEGO
CALIFORNIA
USA
COPYRIGHT
ACM
INTRODUCTION
WITH
THE
ONSET
OF
LARGE
SCALE
INTERNET
SERVICES
AND
THE
MASSIVELY
PARALLEL
COMPUTING
INFRASTRUCTURE
THAT
IS
REQUIRED
TO
SUPPORT
THEM
THE
JOB
OF
A
COMPUTER
ARCHITECT
HAS
EXPANDED
TO
INCLUDE
THE
DESIGN
OF
WAREHOUSE
SIZED
COMPUTING
SYSTEMS
MADE
UP
OF
THOUSANDS
OF
COMPUTING
NODES
THEIR
ASSOCIATED
STORAGE
HIERARCHY
AND
INTERCON
NECTION
INFRASTRUCTURE
POWER
AND
ENERGY
ARE
FIRST
ORDER
CON
CERNS
IN
THE
DESIGN
OF
THESE
NEW
COMPUTERS
AS
THE
COST
OF
POWERING
SERVER
SYSTEMS
HAS
BEEN
STEADILY
RISING
WITH
HIGHER
PERFORMING
SYS
TEMS
WHILE
THE
COST
OF
HARDWARE
HAS
REMAINED
RELATIVELY
STABLE
BARROSO
ARGUED
THAT
IF
THESE
TRENDS
WERE
TO
CONTINUE
THE
COST
OF
THE
ENERGY
CONSUMED
BY
A
SERVER
DURING
ITS
LIFETIME
COULD
SURPASS
THE
COST
OF
THE
EQUIPMENT
ITSELF
BY
COMPARISON
ANOTHER
ENERGY
RELATED
COST
FACTOR
HAS
YET
TO
RECEIVE
SIGNIFICANT
ATTENTION
THE
COST
OF
BUILDING
A
DATACENTER
FACILITY
CAPABLE
OF
PROVIDING
POWER
TO
A
GROUP
OF
SERVERS
TYPICAL
DATACENTER
BUILDING
COSTS
FALL
BETWEEN
AND
PER
DEPLOYED
WATT
OF
PEAK
CRITICAL
POWER
POWER
FOR
COMPUTING
EQUIP
MENT
ONLY
EXCLUDING
COOLING
AND
OTHER
ANCILLARY
LOADS
WHILE
ELECTRICITY
COSTS
IN
THE
U
ARE
APPROXIMATELY
WATT
YEAR
LESS
THAN
THAT
IN
AREAS
WHERE
LARGE
DATACENTERS
TEND
TO
BE
DEPLOYED
UN
LIKE
ENERGY
COSTS
THAT
VARY
WITH
ACTUAL
USAGE
THE
COST
OF
BUILDING
A
DATACENTER
IS
FIXED
FOR
A
GIVEN
PEAK
POWER
DELIVERY
CAPACITY
CON
SEQUENTLY
THE
MORE
UNDER
UTILIZED
A
FACILITY
THE
MORE
EXPENSIVE
IT
BECOMES
AS
A
FRACTION
THE
TOTAL
COST
OF
OWNERSHIP
FOR
EXAM
PLE
IF
A
FACILITY
OPERATES
AT
OF
ITS
PEAK
CAPACITY
ON
AVERAGE
THE
COST
OF
BUILDING
THE
FACILITY
WILL
STILL
BE
HIGHER
THAN
ALL
ELEC
TRICITY
EXPENSES
FOR
TEN
YEARS
OF
MAXIMIZING
USAGE
OF
THE
AVAILABLE
POWER
BUDGET
IS
ALSO
IMPORTANT
FOR
EXISTING
FACILITIES
SINCE
IT
CAN
ALLOW
THE
COMPUTING
INFRASTRUCTURE
TO
GROW
OR
TO
EN
ABLE
UPGRADES
WITHOUT
REQUIRING
THE
ACQUISITION
OF
NEW
DATACENTER
CAPACITY
WHICH
CAN
TAKE
YEARS
IF
IT
INVOLVES
NEW
CONSTRUCTION
THE
INCENTIVE
TO
FULLY
UTILIZE
THE
POWER
BUDGET
OF
A
DATACENTER
IS
OFFSET
BY
THE
BUSINESS
RISK
OF
EXCEEDING
ITS
MAXIMUM
CAPACITY
WHICH
COULD
RESULT
IN
OUTAGES
OR
COSTLY
VIOLATIONS
OF
SERVICE
AGREE
MENTS
DETERMINING
THE
RIGHT
DEPLOYMENT
AND
POWER
MANAGEMENT
STRATE
GIES
REQUIRES
UNDERSTANDING
THE
SIMULTANEOUS
POWER
USAGE
CHARAC
TERISTICS
OF
GROUPS
OF
HUNDREDS
OR
THOUSANDS
OF
MACHINES
OVER
TIME
THIS
IS
COMPLICATED
BY
THREE
IMPORTANT
FACTORS
THE
RATED
MAXI
MUM
POWER
OR
NAMEPLATE
VALUE
OF
COMPUTING
EQUIPMENT
IS
USU
ALLY
OVERLY
CONSERVATIVE
AND
THEREFORE
OF
LIMITED
USEFULNESS
ACTUAL
CONSUMED
POWER
OF
SERVERS
VARIES
SIGNIFICANTLY
WITH
THE
AMOUNT
OF
ACTIVITY
MAKING
IT
HARD
TO
PREDICT
DIFFERENT
APPLICATIONS
EXERCISE
LARGE
SCALE
SYSTEMS
DIFFERENTLY
CONSEQUENTLY
ONLY
THE
MONITORING
TYPICAL
TIER
DATACENTER
COSTS
OF
WATT
OF
CRIT
ICAL
POWER
AND
A
ENERGY
OVERHEAD
FOR
COOLING
AND
CONVERSION
LOSSES
OF
REAL
LARGE
SCALE
WORKLOADS
CAN
YIELD
INSIGHT
INTO
THE
AGGREGATE
LOAD
AT
THE
DATACENTER
LEVEL
IN
THIS
PAPER
WE
PRESENT
THE
POWER
USAGE
CHARACTERISTICS
OF
THREE
LARGE
SCALE
WORKLOADS
AS
WELL
AS
A
WORKLOAD
MIX
FROM
AN
ACTUAL
DATACENTER
EACH
USING
UP
TO
SEVERAL
THOUSAND
SERVERS
OVER
A
PERIOD
OF
ABOUT
SIX
MONTHS
WE
FOCUS
ON
CRITICAL
POWER
AND
EXAMINE
HOW
POWER
USAGE
VARIES
OVER
TIME
AND
OVER
DIFFERENT
AGGREGATION
LEVELS
FROM
INDIVIDUAL
RACKS
TO
AN
ENTIRE
CLUSTER
WE
USE
A
LIGHT
WEIGHT
YET
ACCURATE
POWER
ESTIMATION
METHODOLOGY
THAT
IS
BASED
ON
REAL
TIME
ACTIVITY
INFORMATION
AND
THE
BASELINE
SERVER
HARDWARE
CONFIG
URATION
THE
MODEL
LETS
US
ALSO
ESTIMATE
THE
POTENTIAL
POWER
AND
ENERGY
SAVINGS
OF
POWER
MANAGEMENT
TECHNIQUES
SUCH
AS
POWER
CAPPING
AND
CPU
VOLTAGE
FREQUENCY
SCALING
TO
OUR
KNOWLEDGE
THIS
IS
THE
FIRST
POWER
USAGE
STUDY
OF
VERY
LARGE
SCALE
SYSTEMS
RUNNING
REAL
LIVE
WORKLOADS
AND
THE
FIRST
REPORTED
USE
OF
POWER
MODELING
FOR
POWER
PROVISIONING
SOME
OF
OUR
OTHER
KEY
FINDINGS
AND
CONTRIBUTIONS
ARE
THE
GAP
BETWEEN
THE
MAXIMUM
POWER
ACTUALLY
USED
BY
LARGE
GROUPS
OF
MACHINES
AND
THEIR
AGGREGATE
THEORETICAL
PEAK
US
AGE
CAN
BE
AS
LARGE
AS
IN
DATACENTERS
SUGGESTING
A
SIG
NIFICANT
OPPORTUNITY
TO
HOST
ADDITIONAL
MACHINES
UNDER
THE
SAME
POWER
BUDGET
THIS
GAP
IS
SMALLER
BUT
STILL
SIGNIFICANT
WHEN
WELL
TUNED
LARGE
WORKLOADS
ARE
CONSIDERED
POWER
CAPPING
USING
DYNAMIC
POWER
MANAGEMENT
CAN
EN
ABLE
ADDITIONAL
MACHINES
TO
BE
HOSTED
BUT
IS
MORE
USEFUL
AS
A
SAFETY
MECHANISM
TO
PREVENT
OVERLOAD
SITUATIONS
WE
OBSERVE
TIME
INTERVALS
WHEN
LARGE
GROUPS
OF
MACHINES
ARE
OPERATING
NEAR
PEAK
POWER
LEVELS
SUGGESTING
THAT
POWER
GAPS
AND
POWER
MANAGEMENT
TECHNIQUES
MIGHT
BE
MORE
EASILY
EX
PLOITED
AT
THE
DATACENTER
LEVEL
THAN
AT
THE
RACK
LEVEL
CPU
VOLTAGE
FREQUENCY
SCALING
A
TECHNIQUE
TARGETED
AT
EN
ERGY
MANAGEMENT
HAS
THE
POTENTIAL
TO
BE
MODERATELY
EFFEC
TIVE
AT
REDUCING
PEAK
POWER
CONSUMPTION
ONCE
LARGE
GROUPS
OF
MACHINES
ARE
CONSIDERED
WE
EVALUATE
THE
BENEFITS
OF
BUILDING
SYSTEMS
THAT
ARE
POWER
EFFICIENT
ACROSS
THE
ACTIVITY
RANGE
INSTEAD
OF
SIMPLY
AT
PEAK
POWER
OR
PERFORMANCE
LEVELS
DATACENTER
POWER
PROVISIONING
IT
IS
USEFUL
TO
PRESENT
A
TYPICAL
DATACENTER
POWER
DISTRIBUTION
HIER
ARCHY
SINCE
OUR
ANALYSIS
USES
SOME
OF
THOSE
CONCEPTS
EVEN
THOUGH
THE
EXACT
POWER
DISTRIBUTION
ARCHITECTURE
CAN
VARY
SIGNIFICANTLY
FROM
SITE
TO
SITE
FIGURE
SHOWS
A
TYPICAL
TIER
DATACENTER
FACILITY
WITH
A
TOTAL
CAPACITY
OF
MW
THE
ROUGH
CAPACITY
OF
THE
DIFFERENT
COMPO
NENTS
IS
SHOWN
ON
THE
LEFT
SIDE
A
MEDIUM
VOLTAGE
FEED
TOP
FROM
A
SUBSTATION
IS
FIRST
TRANSFORMED
DOWN
TO
V
IT
IS
COMMON
TO
HAVE
AN
UNINTERRUPTIBLE
POWER
SUPPLY
UPS
AND
GENERATOR
COMBINATION
TO
PROVIDE
BACK
UP
POWER
SHOULD
THE
MAIN
POWER
FAIL
THE
UPS
IS
RESPONSIBLE
FOR
CONDITIONING
POWER
AND
PROVIDING
SHORT
TERM
BACK
UP
WHILE
THE
GENERATOR
PROVIDES
LONGER
TERM
BACK
UP
AN
AUTO
MATIC
TRANSFER
SWITCH
ATS
SWITCHES
BETWEEN
THE
GENERATOR
AND
THE
MAINS
AND
SUPPLIES
THE
REST
OF
THE
HIERARCHY
FROM
HERE
POWER
IS
SUPPLIED
VIA
TWO
INDEPENDENT
ROUTES
IN
ORDER
TO
ASSURE
A
DEGREE
OF
FAULT
TOLERANCE
EACH
SIDE
HAS
ITS
OWN
UPS
THAT
SUPPLIES
A
SE
RIES
OF
POWER
DISTRIBUTION
UNITS
PDUS
EACH
PDU
IS
PAIRED
WITH
A
STATIC
TRANSFER
SWITCH
STS
TO
ROUTE
POWER
FROM
BOTH
SIDES
AND
ASSURE
AN
UNINTERRUPTED
SUPPLY
SHOULD
ONE
SIDE
FAIL
THE
PDUS
ARE
FIGURE
SIMPLIFIED
DATACENTER
POWER
DISTRIBUTION
HIERARCHY
WHICH
INDIVIDUAL
CIRCUITS
EMERGE
CIRCUITS
POWER
A
RACK
WORTH
OF
COMPUTING
EQUIPMENT
OR
A
FRACTION
OF
A
RACK
POWER
DEPLOYMENT
DECISIONS
ARE
GENERALLY
MADE
AT
THREE
LEVELS
RACK
PDU
AND
FACILITY
OR
DATACENTER
HERE
WE
CONSIDER
A
RACK
AS
A
COLLECTION
OF
COMPUTING
EQUIPMENT
THAT
IS
HOUSED
IN
A
STANDARD
WIDE
AND
TALL
ENCLOSURE
DEPENDING
ON
THE
TYPES
OF
SERVERS
A
RACK
CAN
CONTAIN
BETWEEN
AND
COMPUTING
NODES
AND
IS
FED
BY
A
SMALL
NUMBER
OF
CIRCUITS
BETWEEN
AND
RACKS
ARE
AGGREGATED
INTO
A
PDU
ENFORCEMENT
OF
POWER
LIMITS
CAN
BE
PHYSICAL
OR
CONTRACTUAL
IN
NATURE
PHYSICAL
ENFORCEMENT
MEANS
THAT
OVERLOADING
OF
ELECTRICAL
CIRCUITS
WILL
CAUSE
CIRCUIT
BREAKERS
TO
TRIP
AND
RESULT
IN
OUTAGES
CONTRACTUAL
ENFORCEMENT
IS
IN
THE
FORM
OF
ECONOMIC
PENALTIES
FOR
EXCEEDING
THE
NEGOTIATED
LOAD
POWER
AND
OR
ENERGY
PHYSICAL
LIM
ITS
ARE
GENERALLY
USED
AT
THE
LOWER
LEVELS
OF
THE
POWER
DISTRIBUTION
SYSTEM
WHILE
CONTRACTUAL
LIMITS
SHOW
UP
AT
THE
HIGHER
LEVELS
AT
THE
CIRCUIT
LEVEL
BREAKERS
PROTECT
INDIVIDUAL
CIRCUITS
AND
THIS
LIMITS
THE
POWER
THAT
CAN
BE
DRAWN
OUT
OF
THAT
CIRCUIT
ENFORCEMENT
AT
THE
CIRCUIT
LEVEL
IS
STRAIGHTFORWARD
BECAUSE
CIRCUITS
ARE
TYPICALLY
NOT
SHARED
BETWEEN
USERS
AS
WE
MOVE
HIGHER
UP
IN
THE
POWER
DISTRIBU
TION
SYSTEM
LARGER
POWER
UNITS
ARE
MORE
LIKELY
TO
BE
SHARED
BETWEEN
MULTIPLE
DIFFERENT
USERS
THE
DATACENTER
OPERATOR
MUST
PROVIDE
THE
MAXIMUM
RATED
LOAD
FOR
EACH
BRANCH
CIRCUIT
UP
TO
THE
CONTRACTUAL
LIMITS
AND
ASSURE
THAT
THE
HIGHER
LEVELS
OF
THE
POWER
DISTRIBUTION
SYSTEM
CAN
SUSTAIN
THAT
LOAD
VIOLATING
ONE
OF
THESE
CONTRACTS
CAN
HAVE
STEEP
PENALTIES
BECAUSE
THE
USER
MAY
BE
LIABLE
FOR
THE
OUTAGE
OF
ANOTHER
USER
SHARING
THE
POWER
DISTRIBUTION
INFRASTRUCTURE
SINCE
THE
OPERATOR
TYPICALLY
DOES
NOT
KNOW
ABOUT
THE
CHARACTERISTICS
OF
THE
LOAD
AND
THE
USER
DOES
NOT
KNOW
THE
DETAILS
OF
THE
POWER
DISTRI
BUTION
INFRASTRUCTURE
BOTH
TEND
TO
BE
VERY
CONSERVATIVE
IN
ASSURING
THAT
THE
LOAD
STAYS
FAR
BELOW
THE
ACTUAL
CIRCUIT
BREAKER
LIMITS
IF
THE
OPERATOR
AND
THE
USER
ARE
THE
SAME
ENTITY
THE
MARGIN
BETWEEN
EXPECTED
LOAD
AND
ACTUAL
POWER
CAPACITY
CAN
BE
REDUCED
BECAUSE
LOAD
AND
INFRASTRUCTURE
CAN
BE
MATCHED
TO
ONE
ANOTHER
RATED
ON
THE
ORDER
OF
KW
EACH
THEY
FURTHER
TRANSFORM
THE
VOLTAGE
TO
V
IN
THE
US
AND
PROVIDE
ADDITIONAL
CONDITION
ING
AND
MONITORING
EQUIPMENT
AS
WELL
AS
DISTRIBUTION
PANELS
FROM
FACT
THE
NATIONAL
ELECTRICAL
CODE
ARTICLE
A
LIMITS
THE
LOAD
TO
OF
THE
AMPACITY
OF
THE
BRANCH
CIRCUIT
INEFFICIENT
USE
OF
THE
POWER
BUDGET
THE
POWER
BUDGET
AVAILABLE
AT
A
GIVEN
AGGREGATION
LEVEL
IS
OFTEN
UNDERUTILIZED
IN
PRACTICE
SOMETIMES
BY
LARGE
AMOUNTS
SOME
OF
THE
IMPORTANT
CONTRIBUTING
FACTORS
TO
UNDERUTILIZATION
ARE
STAGED
DEPLOYMENT
A
FACILITY
IS
RARELY
FULLY
POPULATED
UPON
INITIAL
COMMISSIONING
BUT
TENDS
TO
BE
SIZED
TO
ACCOMODATE
BUSINESS
DEMAND
GROWTH
THEREFORE
THE
GAP
BETWEEN
DE
PLOYED
AND
USED
POWER
TENDS
TO
BE
LARGER
IN
NEW
FACILITIES
FRAGMENTATION
POWER
USAGE
CAN
BE
LEFT
STRANDED
SIMPLY
BE
CAUSE
THE
ADDITION
OF
ONE
MORE
UNIT
A
SERVER
RACK
OR
PDU
MIGHT
EXCEED
THAT
LEVEL
LIMIT
FOR
EXAMPLE
A
CIR
CUIT
MAY
SUPPORT
ONLY
FOUR
SERVERS
WHICH
WOULD
GUAR
ANTEE
A
UNDERUTILIZATION
OF
THAT
CIRCUIT
IF
A
DATACENTER
IS
DESIGNED
SUCH
THAT
THE
PDU
LEVEL
PEAK
CAPACITY
EXACTLY
MATCHES
THE
SUM
OF
THE
PEAK
CAPACITIES
OF
ALL
OF
ITS
CIRCUITS
SUCH
UNDERUTILIZATION
PERCOLATES
UP
THE
POWER
DELIVERY
CHAIN
AND
BECOME
TRULY
WASTED
AT
THE
DATACENTER
LEVEL
CONSERVATIVE
EQUIPMENT
RATINGS
NAMEPLATE
RATINGS
IN
COM
PUTING
EQUIPMENT
DATASHEETS
OFTEN
REFLECT
THE
MAXIMUM
RAT
ING
OF
THE
POWER
SUPPLY
INSTEAD
OF
THE
ACTUAL
PEAK
POWER
DRAW
OF
THE
SPECIFIC
EQUIPMENT
AS
A
RESULT
NAMEPLATE
VALUES
TEND
TO
DRASTICALLY
OVERESTIMATE
ACHIEVABLE
POWER
DRAW
VARIABLE
LOAD
TYPICAL
SERVER
SYSTEMS
CONSUME
VARIABLE
LEV
ELS
OF
POWER
DEPENDING
ON
THEIR
ACTIVITY
FOR
EXAMPLE
A
TYP
ICAL
LOW
END
SERVER
SYSTEM
CONSUMES
LESS
THAN
HALF
ITS
ACTUAL
PEAK
POWER
WHEN
IT
IS
IDLE
EVEN
IN
THE
ABSENCE
OF
ANY
SO
PHISTICATED
POWER
MANAGEMENT
TECHNIQUES
SUCH
VARIABILITY
TRANSFORMS
THE
POWER
PROVISIONING
PROBLEM
INTO
AN
ACTIVITY
PREDICTION
PROBLEM
STATISTICAL
EFFECTS
IT
IS
INCREASINGLY
UNLIKELY
THAT
LARGE
GROUPS
OF
SYSTEMS
WILL
BE
AT
THEIR
PEAK
ACTIVITY
THEREFORE
POWER
LEV
ELS
SIMULTANEOUSLY
AS
THE
SIZE
OF
THE
GROUP
INCREASES
LOAD
VARIATION
AND
STATISTICAL
EFFECTS
ARE
THE
MAIN
DYNAMIC
SOURCES
OF
INEFFICIENCY
IN
POWER
DEPLOYMENT
AND
THEREFORE
WE
WILL
FOCUS
ON
THOSE
EFFECTS
FOR
THE
REMAINDER
OF
THIS
PAPER
OTHER
CONSUMERS
OF
POWER
OUR
PAPER
FOCUSES
ON
CRITICAL
POWER
AND
THEREFORE
DOES
NOT
DI
RECTLY
ACCOUNT
FOR
DATACENTER
LEVEL
POWER
CONVERSION
LOSSES
AND
THE
POWER
USED
FOR
THE
COOLING
INFRASTRUCTURE
HOWEVER
IN
MODERN
WELL
DESIGNED
FACILITIES
BOTH
CONVERSION
LOSSES
AND
COOLING
OVER
HEADS
CAN
BE
APPROXIMATELY
MODELED
AS
A
FIXED
TAX
OVER
THE
CRITICAL
POWER
LESS
MODERN
FACILITIES
MIGHT
HAVE
A
RELATIVELY
FLAT
COOLING
POWER
USAGE
THAT
DOES
NOT
REACT
TO
CHANGES
IN
THE
HEAT
LOAD
IN
EI
THER
CASE
THE
VARIATIONS
IN
THE
CRITICAL
LOAD
WILL
ACCURATELY
CAPTURE
THE
DYNAMIC
POWER
EFFECTS
IN
THE
FACILITY
AND
WITH
THE
AID
OF
SOME
CALIBRATION
CAN
BE
USED
TO
ESTIMATE
THE
TOTAL
POWER
DRAW
POWER
ESTIMATION
ONE
OF
THE
DIFFICULTIES
OF
STUDYING
POWER
PROVISIONING
STRATEGIES
IS
THE
LACK
OF
POWER
USAGE
DATA
FROM
LARGE
SCALE
DEPLOYMENTS
IN
PARTICULAR
MOST
FACILITIES
LACK
ON
LINE
POWER
MONITORING
AND
DATA
COLLECTION
SYSTEMS
THAT
ARE
NEEDED
FOR
SUCH
STUDIES
WE
CIRCUMVENT
THIS
PROBLEM
BY
DEPLOYING
AN
INDIRECT
POWER
ESTIMATION
FRAMEWORK
THAT
IS
FLEXIBLE
LOW
OVERHEAD
AND
YET
ACCURATE
IN
PREDICTING
POWER
USAGE
AT
MODERATE
TIME
INTERVALS
IN
THIS
SECTION
WE
DESCRIBE
OUR
FRAMEWORK
AND
PRESENT
SOME
VALIDATION
DATA
SUPPORTING
ITS
ACCU
RACY
WE
BEGIN
BY
LOOKING
AT
THE
POWER
USAGE
PROFILE
OF
A
TYPICAL
SERVER
AND
HOW
NAMEPLATE
RATINGS
RELATE
TO
THE
ACTUAL
POWER
DRAW
OF
MACHINES
TABLE
COMPONENT
PEAK
POWER
BREAKDOWN
FOR
A
TYPICAL
SERVER
NAMEPLATE
VS
ACTUAL
PEAK
POWER
A
SERVER
IS
TYPICALLY
TAGGED
WITH
A
NAMEPLATE
RATING
THAT
IS
MEANT
TO
INDICATE
THE
MAXIMUM
POWER
DRAW
OF
THAT
MACHINE
THE
MAIN
PURPOSE
OF
THIS
LABEL
IS
TO
INFORM
THE
USER
OF
THE
POWER
INFRASTRUC
TURE
REQUIRED
TO
SAFELY
SUPPLY
POWER
TO
THE
MACHINE
AS
SUCH
IT
IS
A
CONSERVATIVE
NUMBER
THAT
IS
GUARANTEED
NOT
TO
BE
REACHED
IT
IS
TYP
ICALLY
ESTIMATED
BY
THE
EQUIPMENT
MANUFACTURER
SIMPLY
BY
ADDING
UP
THE
WORST
CASE
POWER
DRAW
OF
ALL
COMPONENTS
IN
A
FULLY
CONFIG
URED
SYSTEM
TABLE
SHOWS
THE
POWER
DRAW
BREAKDOWN
FOR
A
SERVER
BUILT
OUT
OF
A
MOTHERBOARD
WITH
CPUS
AN
IDE
DISK
DRIVE
SLOTS
OF
DRAM
AND
PCI
EXPANSION
SLOTS
USING
THE
MAXIMUM
POWER
DRAW
TAKEN
FROM
THE
COMPONENT
DATASHEETS
WE
ARRIVE
AT
A
TOTAL
DC
DRAW
OF
W
ASSUMING
A
POWER
SUPPLY
EFFICIENCY
OF
WE
ARRIVE
AT
A
TOTAL
NAMEPLATE
POWER
OF
W
WHEN
WE
ACTUALLY
MEASURE
THE
POWER
CONSUMPTION
OF
THIS
SERVER
USING
OUR
MOST
POWER
INTENSIVE
BENCHMARKS
WE
INSTEAD
ONLY
REACH
A
MAXIMUM
OF
WHICH
IS
LESS
THAN
OF
THE
NAMEPLATE
VALUE
WE
REFER
TO
THIS
MEASURED
RATING
AS
THE
ACTUAL
PEAK
POWER
AS
THIS
EXAMPLE
ILLUSTRATES
ACTUAL
PEAK
POWER
IS
A
MUCH
MORE
ACCURATE
ES
TIMATE
OF
A
SYSTEM
PEAK
CONSUMPTION
THEREFORE
WE
CHOOSE
TO
USE
IT
INSTEAD
OF
NAMEPLATE
RATINGS
IN
OUR
SUBSEQUENT
ANALYSIS
THE
BREAKDOWN
SHOWN
IN
TABLE
DOES
NEVERTHELESS
REFLECT
THE
POWER
CONSUMPTION
BREAKDOWN
IN
A
TYPICAL
SERVER
CPUS
AND
MEM
ORY
DOMINATE
TOTAL
POWER
WITH
DISK
POWER
BECOMING
SIGNIFICANT
ONLY
IN
SYSTEMS
WITH
SEVERAL
DISK
DRIVES
MISCELLANEOUS
ITEMS
SUCH
AS
FANS
AND
THE
MOTHERBOARD
COMPONENTS
ROUND
OUT
THE
PICTURE
ESTIMATING
SERVER
POWER
USAGE
OUR
POWER
MODEL
USES
CPU
UTILIZATION
AS
THE
MAIN
SIGNAL
OF
MACHINE
LEVEL
ACTIVITY
FOR
EACH
FAMILY
OF
MACHINES
WITH
SIMILAR
HARDWARE
CONFIGURATION
WE
RUN
A
SUITE
OF
BENCHMARKS
THAT
INCLUDES
SOME
OF
OUR
MOST
REPRESENTATIVE
WORKLOADS
AS
WELL
AS
A
FEW
MICRO
BENCHMARKS
UNDER
VARIABLE
LOADS
WE
MEASURE
TOTAL
SYSTEM
POWER
AGAINST
CPU
UTILIZATION
AND
TRY
TO
FIND
A
CURVE
THAT
APPROXIMATES
THE
AGGREGATE
BEHAVIOR
FIGURE
SHOWS
OUR
MEASUREMENTS
ALONGSIDE
A
LINEAR
MODEL
AND
AN
EMPIRICAL
NON
LINEAR
MODEL
THAT
MORE
CLOSELY
FITS
OUR
OBSERVATIONS
THE
HORIZONTAL
AXIS
SHOWS
THE
CPU
UTILIZATION
REPORTED
BY
THE
OS
AS
AN
AVERAGE
ACROSS
ALL
CPUS
U
A
CALIBRA
TION
PARAMETER
R
THAT
MINIMIZES
THE
SQUARED
ERROR
IS
CHOSEN
A
VALUE
OF
IN
THIS
CASE
FOR
EACH
CLASS
OF
MACHINES
DEPLOYED
ONE
SET
OF
CALIBRATION
EXPERIMENTS
IS
NEEDED
TO
PRODUCE
THE
CORRESPONDING
MODEL
AN
APPROACH
SIMILAR
TO
MANTIS
THE
ERROR
BARS
IN
FIGURE
GIVE
A
VISUAL
INDICATION
THAT
SUCH
MOD
ELS
CAN
BE
REASONABLY
ACCURATE
IN
ESTIMATING
TOTAL
POWER
USAGE
OF
INDIVIDUAL
MACHINES
OF
GREATER
INTEREST
TO
THIS
STUDY
HOWEVER
IS
THE
ACCURACY
OF
THIS
METHODOLOGY
IN
ESTIMATING
THE
DYNAMIC
POWER
USAGE
OF
GROUPS
OF
MACHINES
FIGURE
SHOWS
HOW
THE
MODEL
COM
PARES
TO
THE
ACTUAL
MEASURED
POWER
DRAWN
AT
THE
PDU
LEVEL
A
FEW
PBUSY
PIDLE
CPU
UTILIZATION
FIGURE
MODEL
FITTING
AT
THE
MACHINE
LEVEL
HUNDRED
SERVERS
IN
ONE
OF
OUR
PRODUCTION
FACILITIES
NOTE
THAT
EX
CEPT
FOR
A
FIXED
OFFSET
THE
MODEL
TRACKS
THE
DYNAMIC
POWER
USAGE
BEHAVIOR
EXTREMELY
WELL
IN
FACT
ONCE
THE
OFFSET
IS
REMOVED
THE
ERROR
STAYS
BELOW
ACROSS
THE
USAGE
SPECTRUM
AND
OVER
A
LARGE
NUMBER
OF
PDU
LEVEL
VALIDATION
EXPERIMENTS
THE
FIXED
OFFSET
IS
DUE
TO
OTHER
LOADS
CONNECTED
TO
THE
PDUS
THAT
ARE
NOT
CAPTURED
BY
OUR
MODEL
MOST
NOTABLY
NETWORK
SWITCHING
EQUIPMENT
WE
HAVE
FOUND
THAT
NETWORKING
SWITCHES
OPERATE
ON
A
VERY
NARROW
DYNAMIC
THEREFORE
A
SIMPLE
INVENTORY
OF
SUCH
EQUIPMENT
OR
A
FACILITY
LEVEL
CALIBRATION
STEP
IS
SUFFICIENT
FOR
POWER
ESTIMATION
WE
WERE
RATHER
SURPRISED
TO
FIND
THAT
THIS
SINGLE
ACTIVITY
LEVEL
SIGNAL
CPU
UTILIZATION
PRODUCES
VERY
ACCURATE
RESULTS
ESPECIALLY
WHEN
LARGER
NUMBERS
OF
MACHINES
ARE
CONSIDERED
THE
OBSERVA
TION
CAN
BE
EXPLAINED
BY
NOTING
THAT
CPU
AND
MEMORY
ARE
IN
FACT
THE
MAIN
CONTRIBUTORS
TO
THE
DYNAMIC
POWER
AND
OTHER
COMPONENTS
EITHER
HAVE
VERY
SMALL
DYNAMIC
RANGE
OR
THEIR
ACTIVITY
LEVELS
COR
RELATE
WELL
WITH
CPU
ACTIVITY
THEREFORE
WE
FOUND
IT
UNNECESSARY
SO
FAR
TO
USE
MORE
COMPLEX
MODELS
AND
ADDITIONAL
ACTIVITY
SIGNALS
SUCH
AS
HARDWARE
PERFORMANCE
COUNTERS
THIS
MODELING
METHODOLOGY
HAS
PROVED
VERY
USEFUL
IN
INFORMING
OUR
OWN
POWER
PROVISIONING
PLANS
THE
DATA
COLLECTION
INFRASTRUCTURE
IN
ORDER
TO
GATHER
MACHINE
UTILIZATION
INFORMATION
FROM
THOU
SANDS
OF
SERVERS
WE
USE
A
DISTRIBUTED
COLLECTION
INFRASTRUCTURE
AS
SHOWN
IN
FIGURE
AT
THE
BOTTOM
LAYER
COLLECTOR
JOBS
GATHER
PE
RIODIC
DATA
ON
CPU
UTILIZATION
FROM
ALL
OUR
SERVERS
THE
COLLEC
TORS
WRITE
THE
RAW
DATA
INTO
A
CENTRAL
DATA
REPOSITORY
IN
THE
ANALY
SIS
LAYER
DIFFERENT
JOBS
COMBINE
CPU
ACTIVITY
WITH
THE
APPROPRIATE
MODELS
FOR
EACH
MACHINE
CLASS
DERIVE
THE
CORRESPONDING
POWER
ES
TIMATES
AND
STORE
THEM
IN
A
DATA
REPOSITORY
IN
TIME
SERIES
FORMAT
ANALYSIS
PROGRAMS
ARE
TYPICALLY
BUILT
USING
GOOGLE
MAPREDUCE
FRAMEWORK
POWER
USAGE
CHARACTERIZATION
HERE
WE
PRESENT
A
BASELINE
CHARACTERIZATION
OF
THE
POWER
USAGE
OF
THREE
LARGE
SCALE
WORKLOADS
AND
AN
ACTUAL
WHOLE
DATACENTER
BASED
ON
SIX
MONTHS
OF
POWER
MONITORING
OBSERVATIONS
SHOW
THAT
ETHERNET
SWITCH
POWER
CONSUMPTION
CAN
VARY
BY
LESS
THAN
ACROSS
THE
ACTIVITY
SPECTRUM
COMPONENT
MEASUREMENTS
SHOW
THAT
THE
DYNAMIC
POWER
RANGE
IS
LESS
THAN
FOR
DISKS
AND
NEGLIGIBLE
FOR
MOTHERBOARDS
00
HOUR
OF
THE
DAY
FIGURE
MODELED
VS
MEASURED
POWER
AT
THE
PDU
LEVEL
FIGURE
COLLECTION
STORAGE
AND
ANALYSIS
ARCHITECTURE
WORKLOADS
WE
HAVE
SELECTED
THREE
WORKLOADS
THAT
ARE
REPRESENTATIVE
OF
DIF
FERENT
TYPES
OF
LARGE
SCALE
SERVICES
BELOW
WE
BRIEFLY
DESCRIBE
THE
CHARACTERISTICS
OF
THESE
WORKLOADS
THAT
ARE
RELEVANT
TO
THIS
STUDY
WEBSEARCH
THIS
REPRESENTS
A
SERVICE
WITH
HIGH
REQUEST
THROUGH
PUT
AND
A
VERY
LARGE
DATA
PROCESSING
REQUIREMENTS
FOR
EACH
REQUEST
WE
MEASURE
MACHINES
THAT
ARE
DEPLOYED
IN
GOOGLE
WEB
SEARCH
SERVICES
OVERALL
ACTIVITY
LEVEL
IS
GENERALLY
STRONGLY
CORRELATED
WITH
TIME
OF
DAY
GIVEN
THE
ONLINE
NATURE
OF
THE
SYSTEM
WEBMAIL
THIS
REPRESENTS
A
MORE
DISK
I
O
INTENSIVE
INTERNET
SER
VICE
WE
MEASURE
SERVERS
RUNNING
GMAIL
A
WEB
BASED
EMAIL
PROD
UCT
WITH
SOPHISTICATED
SEARCHING
FUNCTIONALITY
MACHINES
IN
THIS
SER
VICE
TEND
TO
BE
CONFIGURED
WITH
A
LARGER
NUMBER
OF
DISK
DRIVES
AND
EACH
REQUEST
INVOLVES
A
RELATIVELY
SMALL
NUMBER
OF
SERVERS
LIKE
WEBSEARCH
ACTIVITY
LEVEL
IS
CORRELATED
WITH
TIME
OF
DAY
MAPREDUCE
THIS
IS
A
CLUSTER
THAT
IS
MOSTLY
DEDICATED
TO
RUNNING
LARGE
OFFLINE
BATCH
JOBS
OF
THE
KIND
THAT
ARE
AMENABLE
TO
THE
MAPRE
DUCE
STYLE
OF
COMPUTATION
THE
CLUSTER
IS
SHARED
BY
SEVERAL
USERS
AND
JOBS
TYPICALLY
INVOLVE
PROCESSING
TERABYTES
OF
DATA
USING
HUNDREDS
OR
THOUSANDS
OF
MACHINES
SINCE
THIS
IS
NOT
AN
ONLINE
SER
VICE
USAGE
PATTERNS
ARE
MORE
VARIED
AND
LESS
CORRELATED
WITH
TIME
OF
DAY
DATACENTER
SETUP
FOR
THE
RESULTS
IN
THIS
SECTION
WE
PICKED
A
SAMPLE
OF
APPROXI
MATELY
FIVE
THOUSAND
SERVERS
RUNNING
EACH
OF
THE
WORKLOADS
ABOVE
IN
EACH
CASE
THE
SETS
OF
SERVERS
SELECTED
ARE
RUNNING
WELL
TUNED
WORKLOADS
AND
TYPICALLY
AT
HIGH
ACTIVITY
LEVELS
THEREFORE
WE
BE
LIEVE
THEY
ARE
REPRESENTATIVE
OF
THE
MORE
EFFICIENT
DATACENTER
LEVEL
WORKLOADS
IN
TERMS
OF
USAGE
OF
THE
AVAILABLE
POWER
BUDGET
THE
MAIN
RESULTS
ARE
SHOWN
AS
CUMULATIVE
DISTRIBUTION
FUNCTIONS
CDFS
OF
THE
TIME
THAT
A
GROUP
OF
MACHINES
SPENDS
AT
OR
BELOW
A
GIVEN
FRACTION
OF
THEIR
AGGREGATE
PEAK
POWER
SEE
FOR
EXAMPLE
FIG
URE
FOR
EACH
MACHINE
WE
DERIVE
THE
AVERAGE
POWER
OVER
MINUTE
INTERVALS
USING
THE
POWER
MODEL
DESCRIBED
EARLIER
THE
AG
GREGATE
POWER
FOR
EACH
GROUP
OF
MACHINES
DURING
AN
INTERVAL
MAKES
UP
A
RACK
POWER
VALUE
WHICH
IS
NORMALIZED
TO
THEIR
ACTUAL
PEAK
I
E
THE
SUM
OF
THE
MAXIMUM
ACHIEAVABLE
PEAK
POWER
CON
SUMPTION
OF
ALL
MACHINES
IN
THE
GROUP
THE
CUMULATIVE
DISTRIBUTION
OF
THESE
RACK
POWER
VALUES
IS
THE
CURVE
LABELED
RACK
IN
THE
GRAPH
THE
PDU
CURVE
REPRESENTS
A
SIMILAR
AGGREGATION
BUT
NOW
GROUP
ING
SETS
OF
RACKS
OR
ABOUT
MACHINES
FINALLY
THE
CLUSTER
CURVE
SHOWS
THE
CDF
FOR
ALL
MACHINES
APPROXIMATELY
MA
CHINES
CDF
POWER
RESULTS
LET
TAKE
A
CLOSER
LOOK
AT
THE
POWER
CDF
FOR
WEBSEARCH
FIGURE
THE
RACK
CDF
STARTS
AT
AROUND
OF
NORMALIZED
POWER
IN
DICATING
THAT
AT
NO
TIME
DOES
ANY
ONE
RACK
CONSUME
LESS
THAN
OF
ITS
ACTUAL
PEAK
THIS
IS
LIKELY
CLOSE
TO
THE
IDLE
POWER
OF
THE
MA
CHINES
IN
THE
RACK
THE
CURVE
RISES
STEEPLY
WITH
THE
LARGEST
FRACTION
OF
THE
CDF
I
E
THE
MOST
TIME
SPENT
IN
THE
RANGE
OF
ACTUAL
PEAK
POWER
THE
CURVE
INTERCEPTS
THE
TOP
OF
THE
GRAPH
AT
OF
THE
PEAK
POWER
INDICATING
THAT
THERE
ARE
SOME
TIME
INTER
VALS
WHERE
ALL
MACHINES
IN
A
GIVEN
RACK
ARE
OPERATING
VERY
CLOSE
TO
THEIR
ACTUAL
PEAK
POWER
THE
RIGHT
GRAPH
OF
FIGURE
ZOOMS
IN
ON
THE
UPPER
PART
OF
THE
CDF
TO
MAKE
THE
INTERCEPTS
WITH
THE
TOP
OF
THE
GRAPH
CLEARER
LOOKING
AT
THE
PDU
AND
CLUSTER
CURVES
WE
SEE
THAT
THEY
TEND
TO
HAVE
PROGRESSIVELY
HIGHER
MINIMUM
POWER
AND
LOWER
MAXIMUM
POWER
THE
LARGER
THE
GROUP
OF
MACHINES
IS
THE
LESS
LIKELY
IT
IS
THAT
ALL
OF
THEM
ARE
SIMULTANEOUSLY
OPERATING
NEAR
THE
EXTREME
MINIMUM
OR
MAXIMUM
OF
POWER
DRAW
FOR
WEBSEARCH
SOME
RACKS
ARE
REACHING
OF
ACTUAL
PEAK
POWER
FOR
SOME
TIME
INTERVAL
WHEREAS
THE
ENTIRE
CLUSTER
NEVER
GOES
OVER
IT
IS
STRIK
ING
TO
SEE
THAT
GROUPS
OF
MANY
HUNDREDS
OF
MACHINES
PDU
LEVEL
CAN
SPEND
NEARLY
OF
THE
TIME
WITHIN
OF
THEIR
AGGREGATE
PEAK
POWER
THE
CORRESPONDING
CDFS
FOR
WEBMAIL
ARE
SHOWN
IN
FIGURE
THE
SHAPE
OF
THESE
IS
SIMILAR
TO
THAT
OF
WEBSEARCH
WITH
TWO
NO
TABLE
DIFFERENCES
THE
DYNAMIC
RANGE
OF
THE
POWER
DRAW
IS
MUCH
NARROWER
AND
THE
MAXIMUM
POWER
DRAW
IS
LOWER
WEBMAIL
MA
CHINES
TEND
TO
HAVE
MORE
DISKS
PER
MACHINE
AND
DISK
POWER
DRAW
DOES
NOT
VARY
SIGNIFICANTLY
WITH
CHANGES
IN
ACTIVITY
LEVELS
HENCE
A
LARGER
FRACTION
OF
THE
POWER
DRAW
OF
THESE
MACHINES
IS
FIXED
AND
THE
DYNAMIC
RANGE
IS
REDUCED
THE
MAX
POWER
DRAW
IS
ALSO
LOWER
IN
TERESTINGLY
WE
SEE
A
MAXIMUM
OF
ABOUT
OF
PEAK
ACTUAL
POWER
AT
THE
RACK
LEVEL
AND
AT
THE
CLUSTER
LEVEL
AN
EVEN
HIGHER
GAP
THAN
WEBSEARCH
THE
CURVES
FOR
MAPREDUCE
FIGURE
SHOW
A
LARGER
DIFFERENCE
BETWEEN
THE
RACK
PDU
AND
CLUSTER
GRAPHS
THAN
BOTH
WEBSEARCH
AND
WEBMAIL
THIS
INDICATES
THAT
THE
POWER
DRAW
ACROSS
DIFFERENT
RACKS
IS
MUCH
LESS
UNIFORM
LIKELY
A
RESULT
OF
ITS
LESS
TIME
DEPENDENT
ACTIVITY
CHARACTERISTICS
THIS
BEHAVIOR
LEADS
TO
A
MUCH
MORE
NOTICE
ABLE
AVERAGING
EFFECT
AT
THE
CLUSTER
LEVEL
WHILE
THE
RACKS
TOP
OUT
AT
VERY
CLOSE
TO
OF
PEAK
ACTUAL
POWER
THE
CLUSTER
NEVER
GOES
ABOVE
ABOUT
THESE
RESULTS
ARE
SIGNIFICANT
FOR
MACHINE
DEPLOYMENT
PLANNING
IF
WE
USE
THE
MAXIMUM
POWER
DRAW
OF
INDIVIDUAL
MACHINES
TO
PRO
VISION
THE
DATACENTER
WE
WILL
BE
STRANDING
SOME
CAPACITY
FOR
WEB
SEARCH
ABOUT
MORE
MACHINES
COULD
BE
SAFELY
DEPLOYED
WITHIN
THE
SAME
POWER
BUDGET
THE
CORRESPONDING
NUMBERS
FOR
WEBMAIL
AND
MAPREDUCE
ARE
EVEN
HIGHER
AT
AND
THE
IMPACT
OF
DIVERSITY
FIGURE
PRESENTS
THE
POWER
CDF
WHEN
ALL
THE
MACHINES
RUNNING
THE
THREE
WORKLOADS
ARE
DEPLOYED
IN
A
HYPOTHETICAL
COMBINED
CLUSTER
THIS
MIGHT
BE
REPRESENTATIVE
OF
A
DATACENTER
LEVEL
BEHAVIOR
WHERE
MULTIPLE
HIGH
ACTIVITY
SERVICES
ARE
HOSTED
NOTE
THAT
THE
DYNAMIC
RANGE
OF
THE
MIX
IS
NARROWER
THAN
THAT
OF
ANY
INDIVIDUAL
WORKLOAD
AND
THAT
THE
HIGHEST
POWER
VALUE
ACHIEVED
OF
ACTUAL
PEAK
IS
ALSO
LOWER
THAN
EVEN
THAT
OF
THE
LOWEST
INDIVIDUAL
WORKLOAD
WEBMAIL
AT
THIS
IS
CAUSED
BY
THE
FACT
THAT
POWER
CONSUMPTION
PEAKS
ARE
LESS
CORRELATED
ACROSS
WORKLOADS
THAN
WITHIN
THEM
IT
IS
AN
IMPORTANT
ARGUMENT
FOR
MIX
ING
DIVERSE
WORKLOADS
AT
A
DATACENTER
IN
ORDER
TO
SMOOTH
OUT
THE
PEAKS
THAT
INDIVIDUAL
WORKLOADS
MIGHT
PRESENT
USING
THE
HIGHEST
POWER
OF
THE
MIX
TO
DRIVE
DEPLOYMENT
WOULD
ALLOW
MORE
MA
CHINES
TO
BE
DEPLOYED
TO
THIS
DATACENTER
AN
ACTUAL
DATACENTER
SO
FAR
WE
HAVE
LOOKED
ONLY
AT
LARGE
WELL
TUNED
WORKLOADS
IN
A
FULLY
DEPLOYED
ENVIRONMENT
IN
A
REAL
DATACEN
TER
THERE
WILL
BE
ADDITIONAL
WORKLOADS
THAT
ARE
LESS
WELL
TUNED
STILL
IN
DEVELOPMENT
OR
SIMPLY
NOT
HIGHLY
LOADED
FOR
EXAMPLE
MA
CHINES
CAN
BE
ASSIGNED
TO
A
SERVICE
THAT
IS
NOT
YET
FULLY
DEPLOYED
OR
MIGHT
BE
IN
VARIOUS
STAGES
OF
BEING
REPAIRED
OR
UPGRADED
ETC
FIGURE
SHOWS
THE
POWER
CDF
FOR
ONE
SUCH
DATACENTER
WE
NOTE
THE
SAME
TRENDS
AS
SEEN
IN
THE
WORKLOAD
MIX
ONLY
MUCH
MORE
PRO
NOUNCED
OVERALL
DYNAMIC
RANGE
IS
VERY
NARROW
AND
THE
HIGHEST
POWER
CONSUMPTION
IS
ONLY
OF
ACTUAL
PEAK
POWER
US
ING
THIS
NUMBER
TO
GUIDE
DEPLOYMENT
WOULD
PRESENT
THE
OPPORTUNITY
TO
HOST
A
SIZABLE
MORE
MACHINES
AT
THIS
DATACENTER
VALUE
OF
POWER
CAPPING
ONE
OF
THE
FEATURES
THAT
STANDS
OUT
IN
THE
POWER
CDF
CURVES
PRE
SENTED
IN
THE
PREVIOUS
SECTION
IS
THAT
THE
CDF
CURVE
INTERCEPTS
THE
LINE
AT
A
RELATIVELY
FLAT
SLOPE
INDICATING
THAT
THERE
ARE
FEW
TIME
INTERVALS
IN
WHICH
CLOSE
TO
THE
HIGHEST
POWER
IS
DRAWN
BY
THE
MA
CHINES
IF
WE
COULD
SOMEHOW
REMOVE
THOSE
FEW
INTERVALS
WE
MIGHT
BE
ABLE
TO
FURTHER
INCREASE
THE
NUMBER
OF
MACHINES
HOSTED
WITHIN
A
GIVEN
POWER
BUDGET
POWER
CAPPING
TECHNIQUES
ACCOMPLISH
THAT
BY
SETTING
A
VALUE
BELOW
THE
ACTUAL
PEAK
POWER
AND
PREVENTING
THAT
NUMBER
FROM
BEING
EXCEEDED
THROUGH
SOME
TYPE
OF
CONTROL
LOOP
THERE
ARE
NUMEROUS
WAYS
TO
IMPLEMENT
THIS
BUT
THEY
GENERALLY
CONSIST
OF
A
POWER
MONITORING
SYSTEM
POSSIBLY
SUCH
AS
OURS
OR
ONE
BASED
ON
DIRECT
POWER
SENSING
AND
A
POWER
THROTTLING
MECH
ANISM
POWER
THROTTLING
GENERALLY
WORKS
BEST
WHEN
THERE
IS
A
SET
OF
JOBS
WITH
LOOSE
SERVICE
LEVEL
GUARANTEES
OR
LOW
PRIORITY
THAT
CAN
BE
FORCED
TO
REDUCE
CONSUMPTION
WHEN
THE
DATACENTER
IS
APPROACHING
THE
POWER
CAP
VALUE
POWER
CONSUMPTION
CAN
BE
REDUCED
SIMPLY
BY
DESCHEDULING
TASKS
OR
BY
USING
ANY
AVAILABLE
COMPONENT
LEVEL
POWER
MANAGEMENT
KNOBS
SUCH
AS
CPU
VOLTAGE
FREQUENCY
SCALING
NOTE
THAT
THE
POWER
SENSING
THROTTLING
MECHANISMS
NEEDED
FOR
POWER
CAPPING
ARE
LIKELY
NEEDED
ANYWAY
EVEN
IF
WE
DO
NOT
INTEND
TO
CAP
POWER
BUT
SIMPLY
WANT
TO
TAKE
ADVANTAGE
OF
THE
POWER
USAGE
GAPS
SHOWN
IN
THE
CDF
GRAPHS
IN
THOSE
CASES
IT
IS
REQUIRED
TO
NORMALIZED
POWER
FULL
DISTRIBUTION
NORMALIZED
POWER
ZOOMED
VIEW
FIGURE
WEBSEARCH
CDF
OF
POWER
USAGE
NORMALIZED
TO
ACTUAL
PEAK
NORMALIZED
POWER
FULL
DISTRIBUTION
NORMALIZED
POWER
ZOOMED
VIEW
FIGURE
WEBMAIL
CDF
OF
POWER
USAGE
NORMALIZED
TO
ACTUAL
PEAK
NORMALIZED
POWER
FULL
DISTRIBUTION
NORMALIZED
POWER
ZOOMED
VIEW
FIGURE
MAPREDUCE
CDF
OF
POWER
USAGE
NORMALIZED
TO
ACTUAL
PEAK
NORMALIZED
POWER
FULL
DISTRIBUTION
94
NORMALIZED
POWER
ZOOMED
VIEW
FIGURE
CDF
OF
WEBSEARCH
WEBMAIL
MAPREDUCE
AND
THE
MIXTURE
OF
ALL
AT
THE
CLUSTER
LEVEL
NORMALIZED
POWER
FULL
DISTRIBUTION
NORMALIZED
POWER
ZOOMED
VIEW
FIGURE
CDF
OF
A
REAL
DATACENTER
INSURE
AGAINST
POORLY
CHARACTERIZED
WORKLOADS
OR
UNEXPECTED
LOAD
SPIKES
TABLE
PRESENTS
THE
GAINS
THAT
COULD
BE
ACHIEVED
WITH
SUCH
A
SCHEME
FOR
EACH
WORKLOAD
WE
SHOW
THE
POTENTIAL
FOR
INCREASED
MACHINE
DEPLOYMENT
GIVEN
AN
ALLOWANCE
OF
OR
OF
TIME
SPENT
IN
POWER
CAPPING
MODE
WE
ALSO
INCLUDE
THE
NO
POWER
CAPPING
NUMBERS
FOR
COMPARISON
WE
HAVE
EXCLUDED
WEBSEARCH
AND
WEB
MAIL
BY
THEMSELVES
FROM
POWER
CAPPING
BECAUSE
GIVEN
THEIR
ON
LINE
NATURE
THEY
MIGHT
NOT
HAVE
MUCH
OPPORTUNITY
FOR
POWER
REDUC
TION
AT
PEAK
LOAD
OVERALL
THE
ADDITIONAL
GAINS
IN
MACHINE
DEPLOYMENT
ARE
NOTICE
ABLE
BUT
RELATIVELY
MODEST
GENERALLY
CAPTURES
MOST
OF
THE
BEN
EFITS
WITH
ONLY
LITTLE
ADDITIONAL
GAINS
FOR
OF
CAPPING
TIME
THE
BEST
CASE
IS
MAPREDUCE
WHICH
SHOWS
AN
INCREASE
FROM
IN
PO
TENTIAL
INCREASED
MACHINE
DEPLOYMENT
WITHOUT
POWER
CAPPING
TO
WITH
CAPPING
OF
THE
TIME
NOTABLY
MIXING
THE
WORKLOADS
DIMINISHES
THE
RELATIVE
GAINS
BECAUSE
THE
DIFFERENT
WORKLOADS
ARE
ALREADY
DECREASING
THE
LIKELIHOOD
OF
A
SIMULTANEOUS
POWER
SPIKE
IN
ALL
MACHINES
THE
TABLE
ALSO
SHOWS
THE
NUMBER
AND
LENGTH
OF
POWER
CAPPING
INTERVALS
THAT
WOULD
BE
INCURRED
FOR
EACH
WORKLOAD
THIS
INFORMA
TION
GIVES
SOME
INSIGHT
INTO
HOW
OFTEN
THE
POWER
CAPPING
SYSTEM
WOULD
BE
TRIGGERED
WHICH
IN
TURN
IS
USEFUL
FOR
DECIDING
ON
WHAT
KIND
OF
MECHANISM
TO
USE
FEWER
LONGER
INTERVALS
ARE
PROBABLY
MORE
DESIRABLE
BECAUSE
THERE
IS
ALWAYS
SOME
LOSS
UPON
ENTERING
AND
LEAVING
THE
POWER
CAPPING
INTERVAL
PERHAPS
THE
BIGGEST
ADVANTAGE
DYNAMIC
POWER
CAPPING
IS
THAT
IT
CAN
RELAX
THE
REQUIREMENT
TO
ACCURATELY
CHARACTERIZE
WORKLOADS
PRIOR
TO
DEPLOYMENT
AND
PROVIDE
A
SAFETY
VALVE
FOR
CASES
WHERE
WORKLOAD
BEHAVIOR
CHANGES
UNEXPECTEDLY
AVERAGE
VERSUS
PEAK
POWER
ANOTHER
INTERESTING
OBSERVATION
THAT
CAN
BE
DERIVED
FROM
OUR
DATA
IS
THE
DIFFERENCE
BETWEEN
THE
AVERAGE
AND
OBSERVED
PEAK
POWER
DRAW
OF
A
WORKLOAD
OR
MIX
OF
WORKLOADS
WHILE
PEAK
POWER
DRAW
IS
THE
MOST
IMPORTANT
QUANTITY
FOR
GUIDING
THE
DEPLOYMENT
OF
MA
CHINES
TO
A
DATACENTER
AVERAGE
POWER
IS
WHAT
DETERMINES
THE
POWER
BILL
WE
MENTIONED
LOAD
DEPENDENT
POWER
VARIATIONS
AS
ONE
OF
THE
FACTORS
LEADING
TO
INEFFICIENT
USE
OF
THE
POWER
BUDGET
IN
AN
EARLIER
SECTION
WE
ARE
NOW
ABLE
TO
QUANTIFY
IT
TABLE
SHOWS
THE
RATIO
OF
AVERAGE
POWER
TO
OBSERVED
PEAK
POWER
OVER
THE
HALF
YEAR
INTERVAL
FOR
THE
DIFFERENT
WORKLOADS
AND
MIXES
OF
WORKLOADS
THE
RATIOS
REFLECT
THE
DIFFERENT
DYNAMIC
RANGES
FOR
THE
DIFFERENT
WORKLOADS
WEBSEARCH
HAS
THE
HIGHEST
DYNAMIC
RANGE
AND
LOWEST
AVERAGE
TO
PEAK
RATIO
AT
MAPREDUCE
IS
SOMEWHAT
HIGHER
AND
WEBMAIL
HAS
THE
HIGHEST
RATIO
AT
CLOSE
TO
THE
TWO
TABLE
IMPACT
OF
POWER
CAPPING
TABLE
AVERAGE
AND
OBSERVED
PEAK
POWER
NORMALIZED
TO
AC
TUAL
PEAK
AT
THE
CLUSTER
LEVEL
MIXED
WORKLOADS
ALSO
SHOW
HIGHER
RATIOS
WITH
FOR
THE
MIX
OF
THE
THREE
TUNED
WORKLOADS
AND
FOR
THE
REAL
DATACENTER
WE
SEE
THAT
A
MIX
OF
DIVERSE
WORKLOADS
GENERALLY
REDUCES
THE
DIFFERENCE
BETWEEN
AVERAGE
AND
PEAK
POWER
ANOTHER
ARGUMENT
IN
FAVOR
OF
THIS
TYPE
OF
DEPLOYMENT
NOTE
THAT
EVEN
FOR
THIS
BEST
CASE
ON
THE
ORDER
OF
OF
THE
POWER
BUDGET
REMAINS
STRANDED
SIMPLY
BECAUSE
OF
THE
DIFFERENCE
BETWEEN
AVERAGE
AND
PEAK
POWER
WHICH
FURTHER
INCREASES
THE
RELATIVE
WEIGHT
OF
POWER
PROVISIONING
COSTS
OVER
THE
COST
OF
ENERGY
TWO
POWER
SAVINGS
APPROACHES
IN
THE
PREVIOUS
SECTION
WE
USED
THE
POWER
MODELING
INFRASTRUC
TURE
TO
ANALYZE
ACTUAL
POWER
CONSUMPTION
OF
VARIOUS
WORKLOADS
HERE
WE
TAKE
THE
SAME
ACTIVITY
DATA
FROM
OUR
MACHINES
OVER
THE
SIX
MONTH
TIME
PERIOD
AND
USE
THE
MODEL
TO
SIMULATE
THE
POTENTIAL
FOR
POWER
AND
ENERGY
SAVING
OF
TWO
SCHEMES
CPU
VOLTAGE
FREQUENCY
SCALING
CPU
VOLTAGE
AND
FREQUENCY
SCALING
DVS
FOR
SHORT
IS
A
USEFUL
TECHNIQUE
FOR
MANAGING
ENERGY
CONSUMPTION
THAT
HAS
RECENTLY
BEEN
MADE
AVAILABLE
TO
SERVER
CLASS
PROCESSORS
HERE
WE
ASK
OUR
POWER
MODEL
TO
PREDICT
HOW
MUCH
ENERGY
SAVINGS
AND
PEAK
POWER
REDUC
TIONS
COULD
HAVE
BEEN
ACHIEVED
HAD
WE
USED
POWER
MANAGEMENT
TECHNIQUES
BASED
ON
DVS
IN
THE
WORKLOADS
ANALYZED
IN
THE
PREVI
OUS
SECTION
FOR
SIMPLICITY
AND
FOR
THE
PURPOSE
OF
EXPLORING
THE
LIMITS
OF
THE
BENEFIT
WE
USE
AN
ORACLE
STYLE
POLICY
FOR
EACH
MACHINE
AND
EACH
DATA
COLLECTION
INTERVAL
IF
THE
CPU
UTILIZATION
IS
BELOW
A
CERTAIN
THRESHOLD
WE
SIMULATE
DVS
ACTIVATION
BY
THE
CPU
COM
ARE
VARIOUS
CPUS
IN
THE
MARKET
TODAY
THAT
ARE
CAPABLE
OF
SUCH
POWER
REDUCTIONS
THROUGH
DVS
PONENT
OF
THE
TOTAL
POWER
WHILE
LEAVING
THE
POWER
CONSUMPTION
OF
THE
REMAINING
COMPONENTS
UNCHANGED
WITHOUT
DETAILED
APPLICATION
CHARACTERIZATION
WE
CANNOT
DETER
MINE
HOW
SYSTEM
PERFORMANCE
MIGHT
BE
AFFECTED
BY
DVS
THEREFORE
WE
SIMULATE
THREE
CPU
UTILIZATION
THRESHOLDS
FOR
TRIGGERING
DVS
WE
PICK
AS
A
CONSERVATIVE
THRESHOLD
TO
EXAM
INE
HOW
MUCH
BENEFIT
CAN
BE
ACHIEVED
WITH
ALMOST
NO
PERFORMANCE
IMPACT
WE
USE
AS
A
VERY
AGGRESSIVE
THRESHOLD
FOR
THE
SCENARIO
WHERE
PERFORMANCE
CAN
BE
DEGRADED
SIGNIFICANTLY
OR
THE
APPLICATION
HAS
SUBSTANTIAL
AMOUNT
OF
PERFORMANCE
SLACK
FIGURE
SHOWS
THE
IMPACT
OF
CPU
DVS
AT
THE
CLUSTER
LEVEL
ON
OUR
THREE
WORKLOADS
AND
ON
THE
REAL
DATACENTER
DVS
HAS
A
MORE
SIGNIFICANT
POTENTIAL
IMPACT
ON
ENERGY
THAN
PEAK
POWER
WITH
SAV
INGS
OF
OVER
WHEN
USING
THE
MORE
AGGRESSIVE
THRESHOLD
IN
TWO
OUT
OF
FOUR
CASES
WE
EXPECTED
THIS
SINCE
IN
PERIODS
OF
CLUSTER
WIDE
PEAK
ACTIVITY
IT
IS
UNLIKELY
THAT
MANY
SERVERS
WILL
BE
BELOW
THE
DVS
TRIGGER
THRESHOLD
IT
IS
STILL
SURPRISING
THAT
THERE
ARE
CASES
WHERE
DVS
CAN
DELIVER
A
MODERATE
BUT
NOTICEABLE
REDUCTION
IN
MAXIMUM
OBSERVED
POWER
THIS
IS
PARTICULARLY
THE
CASE
FOR
THE
REAL
DATACENTER
WHERE
THE
WORKLOAD
MIX
ENABLES
PEAK
POWER
REDUCTIONS
BETWEEN
AMONG
THE
THREE
WORKLOADS
WEBSEARCH
HAS
THE
HIGHEST
REDUC
TION
IN
BOTH
PEAK
POWER
AND
ENERGY
WEBSEARCH
IS
THE
MOST
COMPUTE
INTENSIVE
WORKLOAD
THEREFORE
THE
CPU
CONSUMES
A
LARGER
PERCENT
AGE
OF
THE
TOTAL
MACHINE
POWER
ALLOWING
DVS
TO
PRODUCE
LARGER
RE
DUCTIONS
RELATIVE
TO
TOTAL
POWER
DVS
ACHIEVES
THE
LEAST
ENERGY
SAV
INGS
FOR
WEBMAIL
WHICH
HAS
THE
NARROWEST
DYNAMIC
POWER
RANGE
AND
RELATIVELY
HIGH
AVERAGE
ENERGY
USAGE
WEBMAIL
IS
GENERALLY
DE
PLOYED
ON
MACHINES
WITH
MORE
DISKS
AND
THEREFORE
THE
CPU
IS
A
SMALLER
CONTRIBUTOR
TO
THE
TOTAL
POWER
RESULTING
IN
A
CORRESPOND
INGLY
SMALLER
IMPACT
OF
DVS
MAPREDUCE
SHOWS
THE
LEAST
REDUC
TION
IN
PEAK
POWER
SINCE
IT
ALSO
TENDS
TO
USE
MACHINES
WITH
MORE
DISKS
WHILE
ACHIEVING
EVEN
HIGHER
PEAK
POWER
USAGE
THAN
WEBMAIL
THESE
TWO
FACTORS
CREATE
THE
MOST
DIFFICULT
SCENARIO
FOR
DVS
IT
IS
ALSO
WORTH
NOTING
THAT
DUE
TO
OUR
SOMEWHAT
COARSE
DATA
COLLECTION
INTERVAL
MIN
THE
DVS
UPSIDE
IS
SOMEWHAT
UNDER
ESTIMATED
HERE
THE
SWITCHING
TIME
OF
THE
CURRENT
DVS
TECHNOLOGY
CAN
ACCOMMODATE
A
SUB
SECOND
INTERVAL
SO
BIGGER
SAVINGS
MIGHT
BE
POSSIBLE
USING
FINER
GRAINED
TRIGGERS
IMPROVING
NON
PEAK
POWER
EFFICIENCY
POWER
EFFICIENCY
OF
COMPUTING
EQUIPMENT
IS
ALMOST
INVARIABLY
MEASURED
WHEN
RUNNING
THE
SYSTEM
UNDER
MAXIMUM
LOAD
GEN
ERALLY
WHEN
PERFORMANCE
PER
WATT
IS
PRESENTED
AS
A
RATING
IT
IS
PEAK
POWER
REDUCTION
B
ENERGY
SAVING
FIGURE
IMPACT
OF
CPU
DVS
AT
DATACENTER
LEVEL
IMPLICITLY
UNDERSTOOD
THAT
THE
SYSTEM
WAS
EXERCISED
TO
MAXIMUM
PERFORMANCE
AND
UPON
REACHING
THAT
THE
POWER
CONSUMPTION
WAS
MEASURED
HOWEVER
AS
THE
ANALYSIS
IN
THE
PREVIOUS
SECTION
SHOWED
THE
REALITY
IS
THAT
MACHINES
OPERATE
AWAY
FROM
PEAK
ACTIVITY
A
GOOD
FRACTION
OF
THE
TIME
THEREFORE
IT
IS
IMPORTANT
TO
CONSERVE
POWER
ACROSS
THE
ACTIVITY
SPECTRUM
AND
NOT
JUST
AT
PEAK
ACTIVITY
FIGURE
SHOWS
THE
POWER
CONSUMPTION
AT
IDLE
NO
ACTIVITY
AS
A
FRACTION
OF
PEAK
POWER
FROM
FIVE
OF
THE
SERVER
CONFIGURATIONS
WE
DE
PLOY
IDLE
POWER
IS
SIGNIFICANTLY
LOWER
THAN
THE
ACTUAL
PEAK
POWER
BUT
GENERALLY
NEVER
BELOW
OF
PEAK
IDEALLY
WE
WOULD
LIKE
OUR
SYSTEMS
TO
CONSUME
NO
POWER
WHEN
IDLE
AND
FOR
POWER
TO
INCREASE
ROUGHLY
PROPORTIONALLY
WITH
INCREASED
ACTIVITY
A
BEHAVIOR
SIMILAR
TO
THE
CURVES
IN
FIGURE
BUT
WHERE
PIDLE
IS
NEAR
ZERO
ARGUABLY
SYS
TEMS
WITH
THIS
BEHAVIOR
WOULD
BE
EQUALLY
POWER
EFFICIENT
REGARDLESS
OF
ACTIVITY
LEVEL
TO
ASSESS
THE
BENEFITS
OF
SUCH
BEHAVIORAL
CHANGE
WE
ALTERED
OUR
MODEL
SO
THAT
IDLE
POWER
FOR
EVERY
MACHINE
WAS
SET
TO
OF
THE
ACTUAL
PEAK
POWER
ALL
OTHER
MODEL
PARAMETERS
IN
CLUDING
ACTUAL
PEAK
POWER
REMAINED
THE
SAME
AS
BEFORE
THE
RESULTS
SHOWN
IN
FIGURE
REVEAL
THAT
THE
GAINS
CAN
BE
QUITE
SUBSTANTIAL
THE
MAXIMUM
CLUSTER
LEVEL
PEAK
POWER
WAS
RE
DUCED
BETWEEN
FOR
OUR
THREE
WORKLOADS
WITH
CORRESPOND
ING
ENERGY
SAVINGS
OF
IN
A
REAL
DATACENTER
HOWEVER
THE
OBSERVED
MAXIMUM
POWER
CONSUMPTION
DROPPED
OVER
WHILE
LESS
THAN
HALF
THE
ENERGY
WAS
USED
THE
FACT
THAT
SUCH
DRAMATIC
GAINS
ARE
POSSIBLE
WITHOUT
ANY
CHANGES
TO
PEAK
POWER
CONSUMPTION
STRONGLY
SUGGEST
THAT
SYSTEM
AND
COMPONENT
DESIGNERS
SHOULD
STRIVE
TO
ACHIEVE
SUCH
BEHAVIOR
IN
REAL
SERVERS
IT
IS
IMPORTANT
TO
NOTE
THAT
THE
MACHINES
IN
OUR
STUDY
ESPECIALLY
THE
ONES
RUNNING
THE
THREE
WORKLOADS
WERE
RARELY
FULLY
IDLE
THERE
FORE
INACTIVE
POWER
MODES
SUCH
AS
SLEEP
OR
STANDBY
MODES
ARE
UNLIKELY
TO
ACHIEVE
THE
SAME
LEVEL
OF
SAVINGS
POWER
PROVISIONING
STRATEGIES
FROM
THE
RESULTS
IN
THE
PREVIOUS
SECTIONS
WE
CAN
DRAW
SOME
CON
CLUSIONS
ABOUT
STRATEGIES
FOR
MAXIMIZING
THE
AMOUNT
OF
COMPUTE
EQUIPMENT
THAT
CAN
BE
DEPLOYED
AT
A
DATACENTER
WITH
A
GIVEN
POWER
CAPACITY
FIRST
OF
ALL
IT
IS
IMPORTANT
TO
UNDERSTAND
THE
ACTUAL
POWER
DRAW
OF
THE
MACHINES
TO
BE
DEPLOYED
NAMEPLATE
POWER
FIGURES
ARE
SO
CONSERVATIVE
AS
TO
BE
USELESS
FOR
THE
DEPLOYMENT
PROCESS
ACCURATE
POWER
MEASUREMENTS
OF
THE
MACHINES
ARE
NEEDED
IN
THE
ACTUAL
CON
FIGURATIONS
TO
BE
DEPLOYED
AND
RUNNING
BENCHMARKS
THAT
MAXIMIZE
OVERALL
POWER
DRAW
FIGURE
IDLE
POWER
AS
FRACTION
OF
PEAK
POWER
IN
SERVER
CONFIGURATIONS
FIGURE
POWER
AND
ENERGY
SAVINGS
ACHIEVABLE
BY
REDUCING
IDLE
POWER
CONSUMPTION
TO
OF
PEAK
THE
CHARACTERIZATION
OF
APPLICATION
POWER
DRAW
AT
DIFFERENT
LEV
ELS
OF
DEPLOYMENT
GRANULARITY
ALLOWS
US
TO
JUDGE
THE
POTENTIAL
FOR
SAFELY
OVER
SUBSCRIBING
PIECES
OF
THE
POWER
DISTRIBUTION
HIERARCHY
OVER
SUBSCRIPTION
AT
THE
RACK
LEVEL
IS
NOT
SAFE
IN
BOTH
WEBSEARCH
AND
MAPREDUCE
INDIVIDUAL
RACKS
APPROACH
VERY
CLOSE
TO
PEAK
ACTUAL
POWER
DURING
SOME
TIME
INTERVALS
WEBMAIL
HAS
A
LITTLE
ROOM
FOR
OVER
SUBSCRIPTION
AT
THE
RACK
LEVEL
AT
AT
THE
PDU
LEVEL
MORE
POTENTIAL
FOR
OVER
SUBSCRIPTION
EXISTS
AT
THE
CLUSTER
LEVEL
THERE
IS
A
NOTICEABLE
DIFFERENCE
BETWEEN
OBSERVED
AND
ACTUAL
PEAK
POWER
ALLOWING
FOR
THE
DEPLOYMENT
OF
BETWEEN
MORE
MACHINES
FOR
INDIVIDUAL
APPLICATIONS
THE
HEADROOM
INCREASES
WHEN
APPLI
CATIONS
ARE
MIXED
TOGETHER
INDICATING
THAT
IT
IS
DESIRABLE
TO
DO
SO
MIXING
ALSO
LEADS
TO
A
NARROWING
OF
AVERAGE
TO
PEAK
POWER
WHICH
IS
DESIRABLE
FROM
A
UTILIZATION
OF
INFRASTRUCTURE
STANDPOINT
FINALLY
WE
HAVE
SHOWN
THAT
IN
A
REAL
CLUSTER
THE
DEPLOYMENT
OF
LESS
WELL
TUNED
APPLICATIONS
AND
OTHER
CONDITIONS
LEADING
TO
POORLY
UTILIZED
MACHINES
CAN
DRIVE
THE
HEADROOM
CLOSE
TO
ONCE
AGAIN
THIS
IS
USING
PEAK
ACTUAL
POWER
TO
GUIDE
DEPLOYMENT
THE
MORE
COMMON
PRACTICE
OF
USING
NAMEPLATE
POWER
FURTHER
INFLATES
THESE
NUMBERS
LEADING
TO
HEADROOM
FOR
MORE
MACHINES
TO
BE
DEPLOYED
A
DYNAMIC
POWER
MANAGEMENT
SCHEME
TO
CAP
THE
PEAK
POWER
DRAW
AT
SOME
PRE
DETERMINED
VALUE
HAS
TWO
ADVANTAGES
FIRST
OF
ALL
IT
CAN
ACT
AS
A
SAFETY
VALVE
PROTECTING
THE
POWER
DISTRIBUTION
HIERARCHY
AGAINST
OVERDRAW
IT
THUS
ALLOWS
FOR
AGGRESSIVE
DEPLOY
MENT
OF
MACHINES
EVEN
IN
THE
FACE
OF
POORLY
CHARACTERIZED
APPLI
CATIONS
OR
UNEXPECTED
LOAD
SPIKES
SECONDLY
IT
ENABLES
ADDITIONAL
OVER
SUBSCRIPTION
OF
THE
AVAILABLE
POWER
CAPPING
POWER
FOR
EVEN
A
SMALL
FRACTION
OF
OVERALL
TIME
CAN
DELIVER
NOTICEABLE
ADDITIONAL
GAINS
IN
MACHINE
DEPLOYMENT
WHILE
DYNAMIC
VOLTAGE
FREQUENCY
SCALING
MAY
NOT
PRODUCE
MUCH
REDUCTION
OF
PEAK
POWER
DRAW
AT
THE
RACK
LEVEL
THERE
IS
A
NOTICE
ABLE
REDUCTION
AT
THE
CLUSTER
LEVEL
DEPENDING
ON
APPLICATION
PEAK
POWER
REDUCTIONS
OF
UP
TO
ARE
SEEN
FOR
AGGRESSIVE
SCHEMES
GROWING
UP
TO
FOR
THE
REAL
DATACENTER
WORKLOAD
MIX
EVEN
THE
LEAST
AGGRESSIVE
SCHEME
NETTED
AN
REDUCTION
IN
PEAK
POWER
FOR
THE
REAL
DATACENTER
MIX
RELATED
WORK
TO
OVERCOME
THE
CONSERVATIVENESS
OF
NAMEPLATE
POWER
THE
IN
DUSTRY
STARTS
TO
PROVIDE
COARSE
GRAINED
POWER
CALCULATORS
BASED
ON
CUSTOMIZED
COMPONENT
AND
ACTIVITY
LEVEL
SELECTIONS
MODELS
THAT
ESTIMATE
POWER
USAGE
BASED
ON
ACTIVITY
METRICS
HAVE
BEEN
STUD
IED
BY
A
NUMBER
OF
RESEARCHERS
CONTRERAS
AND
MARTONOSI
USE
HARDWARE
EVENT
COUNTERS
TO
DERIVE
POWER
ESTIMATES
THAT
ARE
ACCU
RATE
AT
SUB
SECOND
TIME
INTERVALS
OUR
APPROACH
IS
MORE
SIMILAR
TO
THAT
OF
ECONOMOU
ET
AL
WHICH
IS
BASED
ON
COARSER
ACTIVITY
METRICS
SUCH
AS
CPU
LOAD
AND
I
O
ACTIVITY
AN
EVEN
COARSER
MODEL
ING
SCHEME
IS
PRESENTED
BY
BOHRER
ET
AL
IN
THEIR
STUDY
OF
ENERGY
MANAGEMENT
TECHNIQUES
OUR
RESULTS
FURTHER
VALIDATE
THE
USEFULNESS
OF
RELATIVELY
SIMPLE
LOW
OVERHEAD
POWER
MODELING
TECHNIQUES
THERE
IS
GROWING
NUMBER
OF
STUDIES
OF
POWER
MANAGEMENT
TECH
NIQUES
AND
POWER
ENERGY
AWARE
SCHEDULING
POLICIES
AT
THE
SINGLE
SYSTEM
LEVEL
FELTER
ET
AL
STUDY
POWER
SHIFTING
A
TECHNIQUE
TO
REDUCE
PEAK
POWER
WITH
MINIMAL
PERFORMANCE
IMPACT
THAT
IS
BASED
ON
DYNAMICALLY
RE
ALLOCATING
POWER
TO
THE
MOST
PERFORMANCE
CRITI
CAL
COMPONENTS
CARRERA
ET
AL
INSTEAD
FOCUS
ON
THE
DISK
SUBSYS
TEM
PROPOSING
ENERGY
MANAGEMENT
STRATEGIES
THAT
INCLUDE
THE
USE
OF
MULTI
SPEED
DISKS
AND
COMBINATIONS
OF
SERVER
CLASS
AND
LAPTOP
DRIVES
AT
THE
CLUSTER
OR
DATACENTER
LEVEL
CHASE
ET
AL
TREAT
ENERGY
AS
A
RESOURCE
TO
BE
SCHEDULED
BY
A
HOSTING
CENTER
MANAGEMENT
INFRASTRUCTURE
AND
PROPOSE
A
SCHEME
THAT
CAN
REDUCE
ENERGY
US
AGE
BY
WHILE
STILL
MEETING
A
SPECIFIED
LEVEL
OF
SERVICE
MOORE
ET
AL
PRESENT
A
FRAMEWORK
FOR
MEASUREMENT
AND
ANALYSIS
OF
DATACENTER
LEVEL
WORKLOADS
WITH
A
MEASUREMENT
INFRASTRUCTURE
THAT
HAS
SOME
SIMILARITIES
TO
OURS
THEY
USE
A
SYNTHETIC
WORKLOAD
TO
EVALUATE
THEIR
FRAMEWORK
RUNNING
ON
TWO
MODERATE
SIZED
CLUSTERS
OUR
PAPER
IS
NOT
CONCERNED
WITH
ANY
SPECIFIC
POWER
MANAGEMENT
SCHEME
WE
BELIEVE
THAT
A
WIDE
RANGE
OF
TECHNIQUES
COULD
BE
EFFEC
TIVE
AT
THE
DATACENTER
LEVEL
GIVEN
THE
LARGE
TIME
CONSTANTS
INVOLVED
THE
STUDIES
OF
FEMAL
AND
FREEH
AND
OF
RANGANATHAN
ET
AL
ARE
PROBABLY
THE
MOST
CLOSELY
RELATED
TO
OURS
RANGANATHAN
ET
AL
NOTE
THAT
MORE
EFFICIENT
POWER
MANAGEMENT
SOLUTIONS
CAN
BE
REACHED
BY
MANAGING
POWER
AT
THE
RACK
OR
ENSEMBLE
LEVEL
THAN
AT
INDIVIDUAL
BLADES
WHILE
WE
AGREE
WITH
THAT
ASSERTION
OUR
RESULTS
SEEM
TO
CONTRADICT
THEIR
OBSERVATION
THAT
SYNCHRONIZED
POWER
US
AGE
SPIKES
NEVER
HAPPEN
IN
PRACTICE
OUR
DATA
INDICATES
THAT
SUCH
SPIKES
AT
THE
RACK
LEVEL
DO
HAPPEN
SUGGESTING
THAT
THE
KIND
OF
POWER
MANAGEMENT
SOLUTIONS
THEY
PROPOSED
MIGHT
BE
MORE
APPROPRIATE
FOR
MUCH
LARGER
GROUPS
OF
MACHINES
WE
SPECULATE
THAT
THE
MUCH
LARGER
SCALE
OF
OUR
WORKLOADS
AND
HOW
WELL
THEY
ARE
TUNED
ARE
PARTLY
RESPONSIBLE
FOR
THIS
DISCREPANCY
IN
OBSERVED
BEHAVIOR
FE
MAL
AND
FREEH
DEAL
DIRECTLY
WITH
THE
ISSUE
OF
POWER
OVER
SUBSCRIPTION
IN
SMALL
CLUSTERS
TENS
OF
SERVERS
AND
PROPOSE
A
DY
NAMIC
CONTROL
SCHEME
BASED
ON
DYNAMIC
CPU
DVS
TO
REDUCE
PEAK
CONSUMPTION
ALTHOUGH
OUR
ESTIMATES
APPEAR
TO
BE
MORE
MODEST
THAN
THEIRS
WE
AGREE
THAT
CPU
DVS
CAN
HAVE
AN
IMPACT
ON
CLUSTER
LEVEL
PEAK
POWER
SAVINGS
FINALLY
SOME
RESEARCHERS
FOCUS
ON
THE
COOLING
INFRASTRUCTURE
AND
TEMPERATURE
MANAGEMENT
AS
PREVIOUSLY
STATED
OUR
PAPER
DOES
NOT
DEAL
WITH
THE
ENERGY
OR
POWER
USED
FOR
THE
COOLING
INFRAS
TRUCTURE
THESE
ARE
IMPORTANT
RESEARCH
AREAS
THAT
ARE
COMPLEMEN
TARY
TO
OUR
WORK
CONCLUSIONS
SOME
OF
THE
MOST
INTERESTING
COMPUTING
SYSTEMS
BEING
BUILT
TO
DAY
LOOK
MORE
LIKE
A
WAREHOUSE
THAN
A
REFRIGERATOR
POWER
PROVI
SIONING
DECISIONS
FOR
SUCH
SYSTEMS
CAN
HAVE
A
DRAMATIC
ECONOMIC
IMPACT
AS
THE
COST
OF
BUILDING
LARGE
DATACENTERS
COULD
SURPASS
THE
COST
OF
ENERGY
FOR
THE
LIFETIME
OF
THE
FACILITY
SINCE
NEW
DATACENTER
CONSTRUCTION
CAN
TAKE
TENS
OF
MONTHS
INTELLIGENT
POWER
PROVISION
ING
ALSO
HAS
A
LARGE
STRATEGIC
IMPACT
AS
IT
MAY
ALLOW
AN
EXISTING
FACILITY
TO
ACCOMMODATE
THE
BUSINESS
GROWTH
WITHIN
A
GIVEN
POWER
BUDGET
IN
THIS
PAPER
WE
STUDY
HOW
POWER
USAGE
VARIES
OVER
TIME
AND
AS
THE
NUMBER
OF
MACHINES
INCREASES
FROM
INDIVIDUAL
RACKS
TO
CLUSTERS
OF
UP
TO
FIVE
THOUSAND
SERVERS
BY
USING
MULTIPLE
PRODUCTION
WORK
LOADS
WE
ARE
ALSO
ABLE
TO
QUANTIFY
HOW
POWER
USAGE
PATTERNS
ARE
AFFECTED
BY
WORKLOAD
CHOICE
THE
UNDERSTANDING
OF
POWER
USAGE
DYNAMICS
CAN
INFORM
THE
CHOICE
OF
POWER
MANAGEMENT
AND
PROVI
SIONING
POLICIES
AS
WELL
AS
QUANTIFY
THE
POTENTIAL
IMPACT
OF
POWER
AND
ENERGY
REDUCTION
OPPORTUNITIES
TO
OUR
KNOWLEDGE
THIS
IS
THE
FIRST
POWER
USAGE
STUDY
AT
THE
SCALE
OF
DATACENTER
WORKLOADS
AND
THE
FIRST
REPORTED
USE
OF
MODEL
BASED
POWER
MONITORING
TECHNIQUES
FOR
POWER
PROVISIONING
IN
REAL
PRODUCTION
SYSTEMS
WE
ECHO
COMMONLY
HELD
BELIEFS
THAT
NAMEPLATE
RATINGS
ARE
OF
LITTLE
USE
IN
POWER
PROVISIONING
AS
THEY
TEND
TO
GROSSLY
OVERESTI
MATE
ACTUAL
MAXIMUM
USAGE
USING
A
MORE
REALISTIC
PEAK
POWER
DEFINITION
WE
WERE
ABLE
TO
QUANTIFY
THE
GAPS
BETWEEN
MAXIMUM
ACHIEVED
AND
MAXIMUM
THEORETICAL
POWER
CONSUMPTION
OF
GROUPS
OF
MACHINES
THESE
GAPS
WOULD
ALLOW
HOSTING
BETWEEN
AND
MORE
COMPUTING
EQUIPMENT
FOR
INDIVIDUAL
WELL
TUNED
APPLI
CATIONS
AND
AS
MUCH
AS
IN
A
REAL
DATACENTER
RUNNING
A
MIX
OF
APPLICATIONS
THROUGH
CAREFUL
OVER
SUBSCRIPTION
OF
THE
DATACENTER
POWER
BUDGET
WE
FIND
THAT
POWER
CAPPING
MECHANISMS
CAN
EN
ABLE
US
TO
CAPITALIZE
ON
THOSE
OPPORTUNITIES
BY
ACTING
AS
A
SAFETY
NET
AGAINST
THE
RISKS
OF
OVER
SUBSCRIPTION
AND
ARE
THEMSELVES
ABLE
TO
PROVIDE
ADDITIONAL
ALBEIT
MODEST
POWER
SAVINGS
WE
NOTE
HOW
EVER
THAT
OVER
SUBSCRIBING
POWER
AT
THE
RACK
LEVEL
IS
QUITE
RISKY
GIVEN
THAT
LARGE
INTERNET
SERVICES
ARE
CAPABLE
OF
DRIVING
HUNDREDS
OF
SERVERS
TO
HIGH
ACTIVITY
LEVELS
SIMULTANEOUSLY
THE
MORE
EASILY
EXPLOITABLE
OVER
SUBSCRIPTION
OPPORTUNITIES
LIE
AT
THE
FACILITY
LEVEL
THOUSANDS
OF
SERVERS
WE
ALSO
FIND
THAT
CPU
DYNAMIC
VOLTAGE
FREQUENCY
SCALING
MIGHT
YIELD
MODERATE
ENERGY
SAVINGS
UP
TO
ALTHOUGH
IT
HAS
A
MORE
LIMITED
PEAK
POWER
SAVINGS
POTENTIAL
IT
IS
STILL
SURPRISING
THAT
A
TECHNIQUE
USUALLY
DISMISSED
FOR
PEAK
POWER
MANAGEMENT
CAN
HAVE
A
NOTICEABLE
IMPACT
AT
THE
DATACENTER
LEVEL
FINALLY
WE
ARGUE
THAT
COMPONENT
AND
SYSTEM
DESIGNERS
SHOULD
CONSIDER
POWER
EFFICIENCY
N
O
T
I
MPLY
A
T
P
E
AK
P
E
RFORMANCE
LEVELS
BUT
ACROSS
THE
ACTIVITY
RANGE
AS
EVEN
MACHINES
USED
IN
WELL
TUNED
LARGE
SCALE
WORKLOADS
WILL
SPEND
A
SIGNIFICANT
FRACTION
OF
THEIR
OPER
ATIONAL
LIVES
BELOW
PEAK
ACTIVITY
LEVELS
WE
SHOW
THAT
PEAK
POWER
CONSUMPTION
AT
THE
DATACENTER
LEVEL
COULD
BE
REDUCED
BY
UP
TO
AND
ENERGY
USAGE
COULD
BE
HALVED
IF
SYSTEMS
WERE
DESIGNED
SO
THAT
LOWER
ACTIVITY
LEVELS
MEANT
CORRESPONDINGLY
LOWER
POWER
USAGE
PRO
FILES
REDUCING
NETWORK
ENERGY
CONSUMPTION
VIA
SLEEPING
AND
RATE
ADAPTATION
SERGIU
NEDEVSCHI
LUCIAN
POPA
GIANLUCA
IANNACCONE
SYLVIA
RATNASAMY
DAVID
WETHERALL
ABSTRACT
WE
PRESENT
THE
DESIGN
AND
EVALUATION
OF
TWO
FORMS
OF
POWER
MANAGEMENT
SCHEMES
THAT
REDUCE
THE
ENERGY
CONSUMPTION
OF
NETWORKS
THE
FIRST
IS
BASED
ON
PUTTING
NETWORK
COMPONENTS
TO
SLEEP
DURING
IDLE
TIMES
REDUCING
ENERGY
CONSUMED
IN
THE
ABSENCE
OF
PACKETS
THE
SECOND
IS
BASED
ON
ADAPTING
THE
RATE
OF
NETWORK
OPERATION
TO
THE
OFFERED
WORKLOAD
REDUCING
THE
ENERGY
CONSUMED
WHEN
ACTIVELY
PROCESSING
PACKETS
FOR
REAL
WORLD
TRAFFIC
WORKLOADS
AND
TOPOLOGIES
AND
US
ING
POWER
CONSTANTS
DRAWN
FROM
EXISTING
NETWORK
EQUIP
MENT
WE
SHOW
THAT
EVEN
SIMPLE
SCHEMES
FOR
SLEEPING
OR
RATE
ADAPTATION
CAN
OFFER
SUBSTANTIAL
SAVINGS
FOR
IN
STANCE
OUR
PRACTICAL
ALGORITHMS
STAND
TO
HALVE
ENERGY
CONSUMPTION
FOR
LIGHTLY
UTILIZED
NETWORKS
WE
SHOW
THAT
THESE
SAVINGS
APPROACH
THE
MAXIMUM
ACHIEV
ABLE
BY
ANY
ALGORITHMS
USING
THE
SAME
POWER
MANAGE
MENT
PRIMITIVES
MOREOVER
THIS
ENERGY
CAN
BE
SAVED
WITH
OUT
NOTICEABLY
INCREASING
LOSS
AND
WITH
A
SMALL
AND
CON
TROLLED
INCREASE
IN
LATENCY
FINALLY
WE
SHOW
THAT
BOTH
SLEEPING
AND
RATE
ADAPTATION
ARE
VALUABLE
DE
PENDING
PRIMARILY
ON
THE
POWER
PROFILE
OF
NETWORK
EQUIPMENT
AND
THE
UTILIZATION
OF
THE
NETWORK
ITSELF
INTRODUCTION
IN
THIS
PAPER
WE
CONSIDER
POWER
MANAGEMENT
FOR
NETWORKS
FROM
A
PERSPECTIVE
THAT
HAS
RECENTLY
BEGUN
TO
RECEIVE
ATTENTION
THE
CONSERVATION
OF
ENERGY
FOR
OPERATING
AND
ENVIRONMENTAL
REASONS
ENERGY
CONSUMP
TION
IN
NETWORK
EXCHANGES
IS
RISING
AS
HIGHER
CAPACITY
NETWORK
EQUIPMENT
BECOMES
MORE
POWER
HUNGRY
AND
REQUIRES
GREATER
AMOUNTS
OF
COOLING
COMBINED
WITH
RISING
ENERGY
COSTS
THIS
HAS
MADE
THE
COST
OF
POWERING
NETWORK
EXCHANGES
A
SUBSTANTIAL
AND
GROWING
FRACTION
OF
THE
TOTAL
COST
OF
OWNERSHIP
UP
TO
HALF
BY
SOME
ESTIMATES
VARIOUS
STUDIES
NOW
ESTIMATE
THE
POWER
USAGE
OF
THE
US
NETWORK
INFRASTRUCTURE
AT
BETWEEN
AND
TWH
YEAR
OR
YEAR
AT
A
RATE
OF
KWH
DEPENDING
ON
WHAT
IS
INCLUDED
PUBLIC
CONCERN
ABOUT
CARBON
FOOTPRINTS
IS
ALSO
RISING
AND
STANDS
TO
AFFECT
NETWORK
EQUIPMENT
MUCH
AS
IT
HAS
COMPUTERS
UNIVERSITY
OF
CALIFORNIA
BERKELEY
INTEL
RESEARCH
BERKELEY
UNIVERSITY
OF
WASHINGON
INTEL
RESEARCH
SEATTLE
VIA
STANDARDS
SUCH
AS
ENERGYSTAR
IN
FACT
ENERGYSTAR
STANDARD
PROPOSALS
FOR
DISCUSS
SLOWER
OPERATION
OF
NETWORK
LINKS
TO
CONSERVE
ENERGY
WHEN
IDLE
A
NEW
IEEE
TASK
FORCE
WAS
LAUNCHED
IN
EARLY
TO
FOCUS
ON
THIS
ISSUE
FOR
ETHERNET
FORTUNATELY
THERE
IS
AN
OPPORTUNITY
FOR
SUBSTANTIAL
RE
DUCTIONS
IN
THE
ENERGY
CONSUMPTION
OF
EXISTING
NETWORKS
DUE
TO
TWO
FACTORS
FIRST
NETWORKS
ARE
PROVISIONED
FOR
WORST
CASE
OR
BUSY
HOUR
LOAD
AND
THIS
LOAD
TYPICALLY
EXCEEDS
THEIR
LONG
TERM
UTILIZATION
BY
A
WIDE
MARGIN
FOR
EXAMPLE
MEASUREMENTS
REVEAL
BACKBONE
UTILIZATIONS
UNDER
AND
UP
TO
HOUR
LONG
IDLE
TIMES
AT
ACCESS
POINTS
IN
ENTERPRISE
WIRELESS
NETWORKS
SECOND
THE
ENERGY
CONSUMPTION
OF
NETWORK
EQUIPMENT
REMAINS
SUB
STANTIAL
EVEN
WHEN
THE
NETWORK
IS
IDLE
THE
IMPLICATION
OF
THESE
FACTORS
IS
THAT
MOST
OF
THE
ENERGY
CONSUMED
IN
NETWORKS
IS
WASTED
OUR
WORK
IS
AN
INITIAL
EXPLORATION
OF
HOW
OVERALL
NETWORK
ENERGY
CONSUMPTION
MIGHT
BE
REDUCED
WITHOUT
ADVERSELY
AFFECTING
NETWORK
PERFORMANCE
THIS
WILL
REQUIRE
TWO
STEPS
FIRST
NETWORK
EQUIPMENT
RANGING
FROM
ROUTERS
TO
SWITCHES
AND
NICS
WILL
NEED
POWER
MAN
AGEMENT
PRIMITIVES
AT
THE
HARDWARE
LEVEL
BY
ANALOGY
POWER
MANAGEMENT
IN
COMPUTERS
HAS
EVOLVED
AROUND
HARDWARE
SUPPORT
FOR
SLEEP
AND
PERFORMANCE
STATES
THE
FORMER
E
G
C
STATES
IN
INTEL
PROCESSORS
REDUCE
IDLE
CON
SUMPTION
BY
POWERING
OFF
SUB
COMPONENTS
TO
DIFFERENT
EXTENTS
WHILE
THE
LATTER
E
G
SPEEDSTEP
P
STATES
IN
INTEL
PROCESSORS
TRADEOFF
PERFORMANCE
FOR
POWER
VIA
OPERATING
FREQUENCY
SECOND
NETWORK
PROTOCOLS
WILL
NEED
TO
MAKE
USE
OF
THE
HARDWARE
PRIMITIVES
TO
BEST
EFFECT
AGAIN
BY
ANALOGY
WITH
COMPUTERS
POWER
MANAGEMENT
PREFERENCES
CONTROL
HOW
THE
SYSTEM
SWITCHES
BETWEEN
THE
AVAILABLE
STATES
TO
SAVE
ENERGY
WITH
MINIMAL
IMPACT
ON
USERS
OF
THESE
TWO
STEPS
OUR
FOCUS
IS
ON
THE
NETWORK
PROTOCOLS
ADMITTEDLY
THESE
PROTOCOLS
BUILD
ON
HARDWARE
SUPPORT
FOR
POWER
MANAGEMENT
THAT
IS
IN
ITS
INFANCY
FOR
NETWORKING
EQUIPMENT
YET
THE
NECESSARY
SUPPORT
WILL
READILY
BE
DEPLOYED
IN
NETWORKS
WHERE
IT
PROVES
VALUABLE
WITH
FORMS
SUCH
AS
SLEEPING
AND
RAPID
RATE
SELECTION
FOR
ETHERNET
ALREADY
UNDER
DEVELOPMENT
FOR
COMPARISON
COMPUTER
POWER
MANAGEMENT
COMPAT
IBLE
WITH
THE
ACPI
STANDARD
HAS
GONE
FROM
SCARCE
TO
WIDELY
DEPLOYED
OVER
THE
PAST
FIVE
TO
TEN
YEARS
AND
IS
NOW
EXPANDING
INTO
THE
SERVER
MARKET
THUS
OUR
GOAL
IS
TO
LEARN
WHAT
MAGNITUDE
OF
ENERGY
SAVINGS
A
PROTOCOL
USING
FEASIBLE
HARDWARE
PRIMITIVES
MIGHT
OFFER
WHAT
PER
FORMANCE
TRADEOFF
COMES
WITH
THESE
SAVINGS
AND
WHICH
OF
THE
FEASIBLE
KINDS
OF
HARDWARE
PRIMITIVES
WOULD
MAX
IMIZE
BENEFITS
WE
HOPE
THAT
OUR
RESEARCH
CAN
POSITIVELY
INFLUENCE
THE
HARDWARE
SUPPORT
OFFERED
BY
INDUSTRY
THE
HARDWARE
SUPPORT
WE
ASSUME
FROM
NETWORK
EQUIP
MENT
IS
IN
THE
FORM
OF
PERFORMANCE
AND
SLEEP
STATES
PERFORMANCE
STATES
HELP
TO
SAVE
POWER
WHEN
ROUTERS
ARE
ACTIVE
WHILE
SLEEP
STATES
HELP
TO
SAVE
POWER
WHEN
ROUTERS
ARE
IDLE
THE
PERFORMANCE
STATES
WE
ASSUME
DYNAMICALLY
CHANGE
THE
RATE
OF
LINKS
AND
THEIR
ASSOCIATED
INTERFACES
THE
SLEEP
STATES
WE
ASSUME
QUICKLY
POWER
OFF
NETWORK
INTERFACES
WHEN
THEY
ARE
IDLE
WE
DEVELOP
TWO
APPROACHES
TO
SAVE
ENERGY
WITH
THESE
PRIMITIVES
THE
FIRST
PUTS
NETWORK
INTERFACES
TO
SLEEP
DURING
SHORT
IDLE
PERIODS
TO
MAKE
THIS
EFFECTIVE
WE
INTRODUCE
SMALL
AMOUNTS
OF
BUFFERING
MUCH
AS
APS
DO
FOR
SLEEPING
CLIENTS
THIS
COLLECTS
PACKETS
INTO
SMALL
BURSTS
AND
THEREBY
CREATES
GAPS
LONG
ENOUGH
TO
PROFITABLY
SLEEP
POTENTIAL
CONCERNS
ARE
THAT
BUFFERING
WILL
ADD
TOO
MUCH
DELAY
ACROSS
THE
NETWORK
AND
THAT
BURSTS
WILL
EXACERBATE
LOSS
OUR
ALGORITHMS
ARRANGE
FOR
ROUTERS
AND
SWITCHES
TO
SLEEP
IN
A
MANNER
THAT
ENSURES
THE
BUFFERING
DELAY
PENALTY
IS
PAID
ONLY
ONCE
NOT
PER
LINK
AND
THAT
ROUTERS
CLEAR
BURSTS
SO
AS
TO
NOT
AMPLIFY
LOSS
NOTICEABLY
THE
RESULT
IS
A
NOVEL
SCHEME
THAT
DIFFERS
FROM
SCHEMES
IN
THAT
ALL
NETWORK
ELEMENTS
ARE
ABLE
TO
SLEEP
WHEN
NOT
UTILIZED
YET
ADDED
DELAY
IS
BOUNDED
THE
SECOND
APPROACH
ADAPTS
THE
RATE
OF
INDIVIDUAL
LINKS
BASED
ON
THE
UTILIZATION
AND
QUEUING
DELAY
OF
THE
LINK
WE
THEN
EVALUATE
THESE
APPROACHES
USING
REAL
WORLD
NETWORK
TOPOLOGIES
AND
TRAFFIC
WORKLOADS
FROM
ABILENE
AND
INTEL
WE
FIND
THAT
RATE
ADAPTATION
AND
SLEEPING
HAVE
THE
POTENTIAL
TO
DELIVER
SUBSTANTIAL
ENERGY
SAVINGS
FOR
TYPICAL
NETWORKS
THE
SIMPLE
SCHEMES
WE
DEVELOP
ARE
ABLE
TO
CAPTURE
MOST
OF
THIS
ENERGY
SAVING
POTENTIAL
OUR
SCHEMES
DO
NOT
NOTICEABLY
DEGRADE
NETWORK
PERFORMANCE
AND
BOTH
SLEEPING
AND
RATE
ADAPTATION
ARE
VALUABLE
DEPENDING
PRIMARILY
ON
THE
UTILIZATION
OF
THE
NETWORK
AND
EQUIPMENT
POWER
PROFILES
APPROACH
THIS
SECTION
DESCRIBES
THE
HIGH
LEVEL
MODEL
FOR
POWER
CONSUMPTION
THAT
MOTIVATES
OUR
RATE
ADAPTATION
AND
SLEEPING
SOLUTIONS
AS
WELL
AS
THE
METHODOLOGY
BY
WHICH
WE
EVALUATE
THESE
SOLUTIONS
POWER
MODEL
OVERVIEW
ACTIVE
AND
IDLE
POWER
A
NETWORK
ELEMENT
IS
ACTIVE
WHEN
IT
IS
ACTIVELY
PROCESSING
INCOMING
OR
OUTGOING
TRAF
FIC
AND
IDLE
WHEN
IT
IS
POWERED
ON
BUT
DOES
NOT
PROCESS
TRAFFIC
GIVEN
THESE
MODES
THE
ENERGY
CONSUMPTION
FOR
A
NETWORK
ELEMENT
IS
E
PATA
PITI
WHERE
PA
PI
DENOTE
THE
POWER
CONSUMPTION
IN
ACTIVE
AND
IDLE
MODES
RESPECTIVELY
AND
TA
TI
THE
TIMES
SPENT
IN
EACH
MODE
REDUCING
POWER
THROUGH
SLEEP
AND
PERFORMANCE
STATES
SLEEP
STATES
LOWER
POWER
CONSUMPTION
BY
PUTTING
SUB
COMPONENTS
OF
THE
OVERALL
SYSTEM
TO
SLEEP
WHEN
THERE
IS
NO
WORK
TO
PROCESS
THUS
SLEEPING
REDUCES
THE
POWER
CONSUMED
WHEN
IDLE
I
E
IT
REDUCES
THE
PITI
TERM
OF
EQN
BY
REDUCING
THE
PI
TO
SOME
SLEEP
MODE
POWER
DRAW
PS
WHERE
PS
PI
PERFORMANCE
STATES
REDUCE
POWER
CONSUMPTION
BY
LOWERING
THE
RATE
AT
WHICH
WORK
IS
PROCESSED
AS
WE
ELABORATE
ON
IN
LATER
SECTIONS
SOME
PORTION
OF
BOTH
ACTIVE
AND
IDLE
POWER
CONSUMPTION
DEPENDS
ON
THE
FREQUENCY
AND
VOLTAGE
AT
WHICH
WORK
IS
PROCESSED
HENCE
PERFORMANCE
STATES
THAT
SCALE
FREQUENCY
AND
OR
VOLTAGE
REDUCE
BOTH
THE
PA
AND
PI
POWER
DRAWS
RESULTING
IN
AN
OVERALL
REDUCTION
IN
ENERGY
CONSUMPTION
WE
ALSO
ASSUME
A
PENALTY
FOR
TRANSITIONING
BETWEEN
POWER
STATES
FOR
SIMPLICITY
WE
MEASURE
THIS
PENALTY
IN
TIME
TYPICALLY
MILLISECONDS
TREATING
IT
AS
A
PERIOD
IN
WHICH
THE
ROUTER
CAN
DO
NO
USEFUL
WORK
WE
USE
THIS
AS
A
SIMPLE
SWITCHING
MODEL
THAT
LUMPS
ALL
PENALTIES
IGNOR
ING
OTHER
EFFECTS
THAT
MAY
BE
ASSOCIATED
WITH
SWITCHES
SUCH
AS
A
TRANSIENT
INCREASE
IN
POWER
CONSUMPTION
THUS
THERE
IS
ALSO
A
COST
FOR
SWITCHING
BETWEEN
STATES
NETWORKS
WITH
RATE
ADAPTATION
AND
SLEEPING
SUPPORT
IN
A
NETWORK
CONTEXT
THE
SLEEPING
AND
RATE
ADAPTATION
DECISIONS
ONE
ROUTER
MAKES
FUNDAMENTALLY
IMPACTS
AND
IS
IMPACTED
BY
THE
DECISIONS
OF
ITS
NEIGHBORING
ROUTERS
MOREOVER
AS
WE
SEE
LATER
IN
THE
PAPER
THE
STRATEGIES
BY
WHICH
EACH
IS
BEST
EXPLOITED
ARE
VERY
DIFFERENT
INTUITIVELY
THIS
IS
BECAUSE
SLEEP
MODE
SAVINGS
ARE
BEST
EXPLOITED
BY
MAXIMIZING
IDLE
TIMES
WHICH
IMPLIES
PROCESSING
WORK
AS
QUICKLY
AS
POSSIBLE
WHILE
PERFORMANCE
SCALING
IS
BEST
EXPLOITED
BY
PROCESSING
WORK
AS
SLOWLY
AS
POSSIBLE
WHICH
REDUCES
IDLE
TIMES
HENCE
TO
AVOID
COMPLEX
INTERACTIONS
WE
CONSIDER
THAT
THE
WHOLE
NETWORK
OR
AT
LEAST
WELL
DEFINED
COMPONENTS
OF
IT
RUN
IN
EITHER
RATE
ADAPTATION
OR
SLEEP
MODE
WE
DEVELOP
THE
SPECIFICS
OF
OUR
SLEEPING
SCHEMES
IN
SECTION
AND
OUR
RATE
ADAPTATION
SCHEMES
IN
SECTION
NOTE
THAT
OUR
SOLUTIONS
ARE
DELIBERATELY
CONSTRUCTED
TO
APPLY
BROADLY
TO
THE
NETWORKING
INFRASTRUCTURE
FROM
END
HOST
NICS
TO
SWITCHES
AND
IP
ROUTERS
ETC
SO
THAT
THEY
MAY
BE
APPLIED
WHEREVER
THEY
PROVE
TO
BE
THE
MOST
VALUABLE
THEY
ARE
NOT
TIED
TO
IP
LAYER
PROTOCOLS
METHODOLOGY
THE
OVERALL
ENERGY
SAVINGS
WE
CAN
EXPECT
WILL
DEPEND
ON
THE
EXTENT
TO
WHICH
OUR
POWER
MANAGEMENT
ALGORITHMS
CAN
SUCCESSFUL
EXPLOIT
OPPORTUNITIES
TO
SLEEP
OR
RATE
ADAPT
AS
WELL
AS
THE
POWER
PROFILE
OF
NETWORK
EQUIPMENT
I
E
RELATIVE
MAGNITUDES
OF
PA
PI
AND
PS
TO
CLEARLY
SEPARATE
THE
EFFECT
OF
EACH
WE
EVALUATE
SLEEP
SOLUTIONS
IN
TERMS
OF
THE
FRACTION
OF
TIME
FOR
WHICH
NETWORK
ELEMENTS
CAN
SLEEP
AND
RATE
ADAPTATION
SOLUTIONS
IN
TERMS
OF
THE
REDUCTION
IN
THE
AVERAGE
RATE
AT
WHICH
THE
NETWORK
OPERATES
IN
THIS
WAY
WE
ASSESS
EACH
SOLUTION
WITH
THE
APPROPRIATE
BASELINE
WE
THEN
EVALUATE
HOW
THESE
METRICS
TRANSLATE
INTO
OVERALL
NETWORK
ENERGY
SAVINGS
FOR
DIFFERENT
EQUIPMENT
POWER
PROFILES
AND
HENCE
COMPARE
THE
RELATIVE
MERITS
OF
SLEEPING
AND
RATE
ADAPTATION
SECTION
FOR
BOTH
SLEEP
AND
RATE
ADAPTATION
WE
CALIBRATE
THE
SAVINGS
ACHIEVED
BY
OUR
PRACTICAL
SOLUTIONS
BY
COMPARING
TO
THE
MAXIMUM
SAVINGS
ACHIEVABLE
BY
OPTIMAL
BUT
NOT
NECESSARILY
PRACTICAL
SOLUTIONS
IN
NETWORK
ENVIRONMENTS
WHERE
PACKET
ARRIVAL
RATES
CAN
BE
HIGHLY
NON
UNIFORM
ALLOWING
NETWORK
ELEMENTS
TO
TRANSITION
BETWEEN
OPERATING
RATES
OR
SLEEP
ACTIVE
MODES
WITH
CORRESPONDING
TRANSITION
TIMES
CAN
INTRODUCE
ADDITIONAL
PACKET
DELAY
OR
EVEN
LOSS
THAT
WOULD
HAVE
NOT
OTHERWISE
OCCURRED
OUR
GOAL
IS
TO
EXPLORE
SOLUTIONS
THAT
USEFULLY
NAVIGATE
THE
TRADEOFF
BETWEEN
POTENTIAL
ENERGY
SAVINGS
AND
PERFORMANCE
IN
TERMS
OF
PERFORMANCE
WE
MEASURE
THE
AVERAGE
AND
PERCENTILE
OF
THE
END
TO
END
PACKET
DELAY
AND
LOSS
IN
THE
ABSENCE
OF
NETWORK
EQUIPMENT
WITH
HARDWARE
SUPPORT
FOR
POWER
MANAGEMENT
WE
BASE
OUR
EVALUATIONS
ON
PACKET
LEVEL
SIMULATION
WITH
REAL
WORLD
NETWORK
TOPOLOGIES
AND
TRAFFIC
WORKLOADS
THE
KEY
FACTORS
ON
WHICH
POWER
SAVINGS
THEN
DEPEND
BEYOND
THE
DETAILS
OF
THE
SOLUTIONS
THEMSELVES
ARE
THE
TECHNOLOGY
CONSTANTS
OF
THE
SLEEP
AND
PERFORMANCE
STATES
AND
THE
CHARACTERISTICS
OF
THE
NETWORK
IN
PARTICULAR
THE
UTILIZATION
OF
LINKS
DETERMINES
THE
RELATIVE
MAGNITUDES
OF
TACTIVE
AND
TIDLE
AS
WELL
AS
THE
OPPORTUNITIES
FOR
PROFITABLY
EXPLOITING
SLEEP
AND
PERFORMANCE
STATES
WE
GIVE
SIMPLE
MODELS
FOR
TECHNOLOGY
CONSTANTS
IN
THE
FOLLOWING
SECTIONS
TO
CAP
TURE
THE
EFFECT
OF
THE
NETWORK
ON
POWER
SAVINGS
WE
DRIVE
OUR
SIMULATION
WITH
TWO
REALISTIC
NETWORK
TOPOLOGIES
AND
TRAFFIC
WORKLOADS
ABILENE
AND
INTEL
THAT
ARE
SUMMA
RIZED
BELOW
WE
USE
AS
OUR
PACKET
LEVEL
SIMULATOR
ABILENE
WE
USE
ABILENE
AS
A
TEST
CASE
BECAUSE
OF
THE
READY
AVAILABILITY
OF
DETAILED
TOPOLOGY
AND
TRAFFIC
INFOR
MATION
THE
INFORMATION
FROM
PROVIDES
US
WITH
THE
LINK
CONNECTIVITY
WEIGHTS
TO
COMPUTE
ROUTES
LATENCIES
AND
CAPACITIES
FOR
ABILENE
ROUTER
LEVEL
TOPOLOGY
WE
USE
MEASURED
ABILENE
TRAFFIC
MATRICES
TMS
AVAILABLE
IN
THE
COMMUNITY
TO
GENERATE
REALISTIC
WORKLOADS
OVER
THIS
TOPOLOGY
UNLESS
OTHERWISE
STATED
WE
USE
AS
OUR
DEFAULT
A
TRAFFIC
MATRIX
WHOSE
LINK
UTILIZATION
LEVELS
REFLECT
THE
AVERAGE
LINK
UTILIZATION
OVER
THE
ENTIRE
DAY
THIS
CORRESPONDS
TO
A
LINK
UTILIZATION
ON
AVERAGE
WITH
BOTTLENECK
LINKS
EXPERIENCING
ABOUT
UTILIZATION
WE
LINEARLY
SCALE
TMS
TO
STUDY
PERFORMANCE
WITH
INCREASING
UTILIZATION
UP
TO
A
MAXIMUM
AVERAGE
NETWORK
UTILIZATION
OF
AS
BEYOND
THIS
SOME
LINKS
REACH
VERY
HIGH
UTILIZATIONS
FINALLY
WHILE
THE
TMS
SPECIFY
THE
MINUTE
AVERAGE
RATE
OBSERVED
FOR
EACH
INGRESS
EGRESS
PAIR
WE
STILL
REQUIRE
A
PACKET
LEVEL
TRAFFIC
GENERATION
MODEL
THAT
CREATES
THIS
RATE
IN
KEEPING
WITH
PREVIOUS
STUDIES
WE
GENERATE
TRAFFIC
AS
A
MIX
OF
PARETO
FLOWS
AND
FOR
SOME
RESULTS
WE
USE
CONSTANT
BIT
RATE
CBR
TRAFFIC
AS
PER
STANDARD
PRACTICE
WE
SET
ROUTER
QUEUE
SIZES
EQUAL
TO
THE
BANDWIDTH
DELAY
PRODUCT
IN
THE
NETWORK
WE
USE
THE
BANDWIDTH
OF
THE
BOTTLENECK
LINK
AND
A
DELAY
OF
INTEL
AS
AN
ADDITIONAL
REAL
WORLD
DATASET
WE
COLLECTED
TOPOLOGY
AND
TRAFFIC
INFORMATION
FOR
THE
GLOBAL
INTEL
ENTERPRISE
NETWORK
THIS
NETWORK
CONNECTS
INTEL
SITES
WORLDWIDE
FROM
SMALL
REMOTE
OFFICES
TO
LARGE
MULTI
BUILDING
SITES
WITH
THOUSANDS
OF
USERS
IT
COMPRISES
APPROXIMATELY
ROUTERS
AND
OVER
LINKS
WITH
CAPACITIES
RANGING
FROM
TO
TO
SIMULATE
REALISTIC
TRAFFIC
WE
COLLECTED
UNSAMPLED
NETFLOW
RECORDS
FROM
THE
CORE
ROUTERS
THE
RECORDS
EXPORTED
BY
EACH
ROUTER
EVERY
MINUTE
CONTAIN
PER
FLOW
INFORMATION
THAT
ALLOWS
US
TO
RECREATE
THE
TRAFFIC
SOURCED
BY
INGRESS
NODES
PUTTING
NETWORK
ELEMENTS
TO
SLEEP
IN
THIS
SECTION
WE
DISCUSS
POWER
MANAGEMENT
ALGORITHMS
THAT
EXPLOIT
SLEEP
STATES
TO
REDUCE
POWER
CONSUMPTION
DURING
IDLE
TIMES
MODEL
AND
ASSUMPTIONS
BACKGROUND
A
WELL
ESTABLISHED
TECHNIQUE
AS
USED
BY
MICROPROCESSORS
AND
MOBILES
IS
TO
REDUCE
IDLE
POWER
BY
PUTTING
HARDWARE
SUB
COMPONENTS
TO
SLEEP
FOR
EXAMPLE
MODERN
INTEL
PROCESSORS
SUCH
AS
THE
CORE
DUO
HAVE
A
SUCCESSION
OF
SLEEP
STATES
CALLED
C
STATES
THAT
OFFER
INCREASINGLY
REDUCED
POWER
AT
THE
COST
OF
INCREASINGLY
HIGH
LATENCIES
TO
ENTER
AND
EXIT
THESE
STATES
WE
ASSUME
SIMILAR
SLEEP
STATES
MADE
AVAILABLE
FOR
NETWORK
EQUIP
MENT
FOR
THE
PURPOSE
OF
THIS
STUDY
WE
IGNORE
THE
OPTIONS
AFFORDED
BY
MULTIPLE
SLEEP
STATES
AND
ASSUME
AS
AN
INITIAL
SIMPLIFICATION
THAT
WE
HAVE
A
SINGLE
SLEEP
STATE
MODEL
WE
MODEL
A
NETWORK
SLEEP
STATE
AS
CHARACTER
IZED
BY
THREE
FEATURES
OR
PARAMETERS
THE
FIRST
IS
THE
POWER
DRAW
IN
SLEEP
MODE
PS
WHICH
WE
ASSUME
TO
BE
A
SMALL
FRACTION
OF
THE
IDLE
MODE
POWER
DRAW
PI
THE
SECOND
CHARACTERIZING
PARAMETER
OF
A
SLEEP
STATE
IS
THE
TIME
Δ
IT
TAKES
TO
TRANSITION
IN
AND
OUT
OF
SLEEP
STATES
HIGHER
VALUES
OF
Δ
RAISE
THE
BAR
ON
WHEN
THE
NETWORK
ELEMENT
CAN
PROFITABLY
ENTER
SLEEP
MODE
AND
HENCE
Δ
CRITICALLY
AFFECTS
POTENTIAL
SAVINGS
WHILE
NETWORK
INTERFACE
CARDS
CAN
MAKE
PHYSICAL
LAYER
TRANSITIONS
IN
AS
LOW
AS
TRANSITION
TIMES
THAT
INVOLVE
RESTORING
STATE
AT
HIGHER
LAYERS
MEMORY
OPERATING
SYSTEM
ARE
LIKELY
TO
BE
HIGHER
WE
THUS
EVALUATE
OUR
SOLUTIONS
OVER
A
WIDE
RANGE
VALUES
OF
TRANSITION
TIMES
FINALLY
NETWORK
EQUIPMENT
MUST
SUPPORT
A
MECHANISM
FOR
INVOKING
AND
EXITING
SLEEP
STATES
THE
OPTION
THAT
MAKES
THE
FEWEST
ASSUMPTIONS
ABOUT
THE
SOPHISTICATION
OF
HARDWARE
SUPPORT
IS
TIMER
DRIVEN
SLEEPING
IN
WHICH
THE
NETWORK
ELEMENT
ENTERS
AND
EXITS
SLEEP
AT
WELL
DEFINED
TIMES
PRIOR
TO
ENTERING
SLEEP
THE
NETWORK
ELEMENT
SPECIFIES
THE
TIME
IN
THE
FUTURE
AT
WHICH
IT
WILL
EXIT
SLEEP
AND
ALL
PACKETS
THAT
ARRIVE
AT
A
SLEEPING
INTERFACE
ARE
LOST
THE
SECOND
POSSIBILITY
DESCRIBED
IN
IS
FOR
ROUTERS
TO
WAKE
UP
AUTOMATICALLY
ON
SENSING
INCOMING
TRAFFIC
ON
THEIR
INPUT
PORTS
TO
ACHIEVE
THIS
WAKE
ON
ARRIVAL
WOA
THE
CIRCUITRY
THAT
SENSES
PACKETS
ON
A
LINE
IS
LEFT
POWERED
ON
EVEN
IN
SLEEP
MODE
WHILE
SUPPORT
FOR
WOA
IS
NOT
COMMON
IN
EITHER
COMPUTERS
OR
INTERFACES
TODAY
THIS
IS
A
FORM
OF
HARDWARE
SUPPORT
THAT
MIGHT
PROVE
DESIRABLE
FOR
FUTURE
NETWORK
EQUIPMENT
AND
IS
CURRENTLY
UNDER
DISCUSSION
IN
THE
IEEE
TASK
FORCE
NOTE
THAT
EVEN
WITH
WAKE
ON
ARRIVAL
BITS
ARRIVING
DURING
THE
TRANSITION
PERIOD
Δ
ARE
EFFECTIVELY
LOST
TO
HANDLE
THIS
THE
AUTHORS
IN
PROPOSE
THE
USE
OF
DUMMY
PACKETS
TO
ROUSE
A
SLEEPING
NEIGHBOR
A
NODE
A
THAT
WISHES
TO
WAKE
B
FIRST
SENDS
B
A
DUMMY
PACKET
AND
THEN
WAITS
FOR
TIME
Δ
BEFORE
TRANSMITTING
THE
ACTUAL
DATA
TRAFFIC
THE
SOLUTIONS
WE
DEVELOP
IN
THIS
PAPER
APPLY
SEAMLESSLY
TO
EITHER
TIMER
DRIVEN
OR
WOA
BASED
HARDWARE
MEASURING
SAVINGS
AND
PERFORMANCE
IN
THIS
SECTION
WE
MEASURE
SAVINGS
IN
TERMS
OF
THE
PERCENTAGE
OF
TIME
NETWORK
ELEMENTS
SPEND
ASLEEP
AND
PERFORMANCE
IN
TERMS
OF
THE
AVERAGE
AND
PERCENTILE
OF
THE
END
TO
END
PACKET
DELAY
AND
LOSS
WE
ASSUME
THAT
INDIVIDUAL
LINE
CARDS
IN
A
NETWORK
ELEMENT
CAN
BE
INDEPENDENTLY
PUT
TO
SLEEP
THIS
ALLOWS
FOR
MORE
OPPORTUNITIES
TO
SLEEP
THAN
IF
ONE
WERE
TO
REQUIRE
THAT
A
ROUTER
SLEEP
IN
ITS
ENTIRETY
AS
THE
LATTER
IS
ONLY
POSSIBLE
WHEN
THERE
IS
NO
INCOMING
TRAF
FIC
AT
ANY
OF
THE
INCOMING
INTERFACES
CORRESPONDINGLY
OUR
ENERGY
SAVINGS
ARE
WITH
RESPECT
TO
INTERFACE
CARDS
WHICH
TYPICALLY
REPRESENT
A
MAJOR
PORTION
OF
THE
OVERALL
CONSUMPTION
OF
A
NETWORK
DEVICE
THAT
SAID
ONE
COULD
IN
ADDITION
PUT
THE
ROUTE
PROCESSOR
AND
SWITCH
FABRIC
TO
SLEEP
AT
TIMES
WHEN
ALL
LINE
CARDS
ARE
ASLEEP
APPROACHES
AND
POTENTIAL
SAVINGS
FOR
INTERFACES
THAT
SUPPORT
WAKE
ON
ARRIVAL
ONE
AP
PROACH
TO
EXPLOITING
SLEEP
STATES
IS
THAT
OF
OPPORTUNISTIC
SLEEPING
IN
WHICH
LINK
INTERFACES
SLEEP
WHEN
IDLE
I
E
A
ROUTER
IS
AWAKENED
BY
AN
INCOMING
DUMMY
PACKET
AND
AFTER
FORWARDING
IT
ON
RETURNS
TO
SLEEP
IF
NO
SUBSEQUENT
FIGURE
PACKETS
WITHIN
A
BURST
ARE
ORGANIZED
BY
DESTINATION
PACKET
ARRIVES
FOR
SOME
TIME
WHILE
VERY
SIMPLE
SUCH
AN
APPROACH
CAN
RESULT
IN
FREQUENT
TRANSITIONS
WHICH
LIMITS
SAVINGS
FOR
HIGHER
TRANSITION
TIMES
AND
OR
HIGHER
LINK
SPEEDS
FOR
EXAMPLE
WITH
A
LINK
EVEN
UNDER
LOW
UTILIZATION
AND
PACKET
SIZES
OF
THE
AVERAGE
PACKET
INTER
ARRIVAL
TIME
IS
VERY
SMALL
THUS
WHILE
OPPORTUNISTIC
SLEEPING
MIGHT
BE
EFFECTIVE
IN
LANS
WITH
HIGH
IDLE
TIMES
FOR
FAST
LINKS
THIS
TECHNIQUE
IS
ONLY
EFFECTIVE
FOR
VERY
LOW
TRANSITION
TIMES
Δ
WE
QUANTIFY
THIS
SHORTLY
IN
ADDITION
OPPORTUNISTIC
SLEEP
IS
ONLY
POSSIBLE
WITH
THE
MORE
SOPHISTICATED
HARDWARE
SUPPORT
OF
WAKE
ON
ARRIVAL
TO
CREATE
GREATER
OPPORTUNITIES
FOR
SLEEP
WE
CONSIDER
A
NOVEL
APPROACH
THAT
ALLOWS
US
TO
EXPLICITLY
CONTROL
THE
TRADEOFF
BETWEEN
NETWORK
PERFORMANCE
AND
ENERGY
SAVINGS
OUR
APPROACH
IS
TO
SHAPE
TRAFFIC
INTO
SMALL
BURSTS
AT
THE
EDGES
OF
THE
NETWORK
EDGE
DEVICES
THEN
TRANSMIT
PACKETS
IN
BUNCHES
AND
ROUTERS
WITHIN
THE
NETWORK
WAKE
UP
TO
PROCESS
A
BURST
OF
PACKETS
AND
THEN
SLEEP
UNTIL
THE
NEXT
BURST
ARRIVES
THE
INTENT
IS
TO
PROVIDE
SUFFICIENT
BUNCHING
TO
CREATE
OPPORTUNITIES
FOR
SLEEP
IF
THE
LOAD
IS
LOW
YET
NOT
ADD
EXCESSIVE
DELAY
THIS
IS
A
RADICAL
APPROACH
IN
THE
SENSE
THAT
MUCH
OTHER
WORK
SEEKS
TO
AVOID
BURSTS
RATHER
THAN
CREATE
THEM
E
G
TOKEN
BUCKETS
FOR
QOS
CONGESTION
AVOIDANCE
BUFFERING
AT
ROUTERS
AS
OUR
MEASUREMENTS
OF
LOSS
AND
DELAY
SHOW
OUR
SCHEMES
AVOID
THE
PITFALLS
ASSOCIATED
WITH
BURSTS
BECAUSE
WE
INTRODUCE
ONLY
A
BOUNDED
AND
SMALL
AMOUNT
OF
BURSTINESS
AND
A
ROUTER
NEVER
ENTERS
SLEEP
UNTIL
IT
HAS
CLEARED
ALL
BURSTS
IT
HAS
BUILT
UP
MORE
PRECISELY
WE
INTRODUCE
A
BUFFER
INTERVAL
B
THAT
CONTROLS
THE
TRADEOFF
BETWEEN
SAVINGS
AND
PERFORMANCE
AN
INGRESS
ROUTER
BUFFERS
INCOMING
TRAFFIC
FOR
UP
TO
B
MS
AND
ONCE
EVERY
B
MS
FORWARDS
BUFFERED
TRAFFIC
IN
A
BURST
TO
ENSURE
THAT
BURSTS
CREATED
AT
THE
INGRESS
ARE
RETAINED
AS
THEY
TRAVERSE
THROUGH
THE
NETWORK
AN
INGRESS
ROUTER
ARRANGES
PACKETS
WITHIN
THE
BURST
SUCH
THAT
ALL
PACKETS
DESTINED
FOR
THE
SAME
EGRESS
ROUTER
ARE
CONTIGUOUS
WITHIN
THE
BURST
SEE
FIGURE
THE
ABOVE
BUFFER
AND
BURST
APPROACH
B
B
CREATES
ALTERNATING
PERIODS
OF
CONTIGUOUS
ACTIVITY
AND
SLEEP
LEADING
TO
FEWER
TRANSITIONS
AND
AMORTIZING
THE
TRANSITION
PENALTY
Δ
OVER
MULTIPLE
PACKETS
THIS
IMPROVEMENT
COMES
AT
THE
COST
OF
AN
ADDED
END
TO
END
DELAY
OF
UP
SJ
SI
IDLE
BOUND
WOA
PARETO
WOA
CBR
OPTB
B
SI
SJ
SI
SJ
B
FIGURE
EXAMPLES
OF
BURST
SYNCHRONIZATION
TO
B
MS
NOTE
THAT
BECAUSE
ONLY
INGRESS
ROUTERS
BUFFER
TRAFFIC
THE
ADDITIONAL
DELAY
DUE
TO
BUFFERING
IS
ONLY
INCURRED
ONCE
ALONG
THE
ENTIRE
INGRESS
TO
EGRESS
PATH
AS
IMPORTANTLY
THIS
APPROACH
UNLIKE
OPPORTUNISTIC
SLEEP
CAN
BE
USED
BY
INTERFACES
THAT
SUPPORT
ONLY
TIMER
DRIVEN
SLEEP
A
ROUTER
THAT
RECEIVES
A
BURST
FROM
UPSTREAM
ROUTER
AT
TIME
KNOWS
THAT
THE
NEXT
START
OF
BURST
WILL
ARRIVE
AT
TIME
B
AND
CAN
HENCE
SLEEP
BETWEEN
BURSTS
THE
QUESTION
THEN
IS
HOW
SIGNIFICANT
ARE
THE
SAVINGS
THIS
APPROACH
ENABLES
FOR
REASONABLE
ADDITIONAL
DELAY
WE
NOTE
THAT
THE
BEST
POSSIBLE
SAVINGS
WOULD
OCCUR
IF
A
ROUTER
RECEIVED
THE
INCOMING
BURSTS
FROM
ALL
INGRESS
ROUTERS
CLOSE
IN
TIME
SUCH
THAT
IT
PROCESSES
ALL
INCOMING
BURSTS
AND
RETURNS
TO
SLEEP
THUS
INCURRING
EXACTLY
ONE
SLEEP
WAKE
TRANSITION
PER
B
MS
THIS
MIGHT
APPEAR
POS
SIBLE
BY
HAVING
INGRESS
ROUTERS
COORDINATE
THE
TIMES
AT
WHICH
THEY
TRANSMIT
BURSTS
SUCH
THAT
BURSTS
FROM
DIFFERENT
INGRESSES
ARRIVE
CLOSE
IN
TIME
AT
INTERMEDIATE
ROUTERS
FOR
EXAMPLE
CONSIDER
THE
SCENARIO
IN
FIGURE
A
WHERE
INGRESS
ROUTERS
AND
ARE
SCHEDULED
TO
TRANSMIT
TRAFFIC
AT
TIMES
AND
RESPECTIVELY
IF
INSTEAD
WERE
TO
SCHEDULE
ITS
BURST
FOR
TIME
INSTEAD
THEN
BURSTS
FROM
AND
WOULD
ALIGN
IN
TIME
AT
THUS
REDUCING
THE
NUMBER
OF
DISTINCT
BURST
TIMES
AND
SLEEP
TO
WAKE
TRANSITIONS
AT
DOWNSTREAM
ROUTERS
AND
UNFORTUNATELY
THE
EXAMPLE
IN
FIGURE
B
SUGGESTS
THIS
IS
UNACHIEVABLE
FOR
GENERAL
TOPOLOGIES
HERE
SI
AND
SJ
REPRESENT
THE
ARRIVAL
TIMES
OF
INCOMING
BURSTS
TO
NODES
AND
RESPECTIVELY
AND
WE
SEE
THAT
THE
TOPOLOGY
MAKES
IT
IMPOSSIBLE
TO
FIND
TIMES
SI
AND
SJ
THAT
COULD
SIMULTA
NEOUSLY
ALIGN
THE
BURSTS
DOWNSTREAM
FROM
AND
WE
THUS
USE
A
BRUTE
FORCE
STRATEGY
TO
EVALUATE
THE
MAXIMUM
ACHIEVABLE
COORDINATION
FOR
A
GIVEN
TOPOLOGY
AND
TRAFFIC
WORKLOAD
WE
CONSIDER
THE
START
OF
BURST
TIME
FOR
TRAFFIC
FROM
EACH
INGRESS
I
TO
EGRESS
J
DENOTED
SIJ
AND
PERFORM
AN
EXHAUSTIVE
SEARCH
OF
ALL
SIJ
TO
FIND
A
SET
OF
START
TIMES
THAT
MINIMIZES
THE
NUMBER
OF
TRANSITIONS
ACROSS
ALL
THE
INTERFACES
IN
THE
NETWORK
WE
CALL
THIS
SCHEME
OPTB
B
CLEARLY
SUCH
AN
ALGORITHM
IS
NOT
PRACTICAL
AND
WE
USE
IT
MERELY
AS
AN
OPTIMISTIC
BOUND
ON
WHAT
MIGHT
BE
ACHIEVABLE
WERE
NODES
TO
COORDINATE
IN
SHAPING
TRAFFIC
UNDER
A
BUFFER
AND
BURST
APPROACH
WE
COMPARE
THE
SLEEP
TIME
ACHIEVED
BY
OPTB
B
TO
THE
UPPER
BOUND
ON
SLEEP
AS
GIVEN
BY
Μ
WHERE
Μ
IS
THE
AVERAGE
UTILIZATION
FIGURE
TIME
ASLEEP
USING
OPTB
B
AND
OPPORTUNISTIC
SLEEPING
AND
COMPARED
TO
THE
UPPER
BOUND
Μ
NETWORK
UTILIZATION
THIS
UPPER
BOUND
IS
NOT
ACHIEVABLE
BY
ANY
ALGORITHM
SINCE
UNLIKE
OPTB
B
IT
DOES
NOT
TAKE
INTO
ACCOUNT
THE
OVERHEAD
Δ
DUE
TO
SLEEP
WAKE
TRANSI
TIONS
NONETHELESS
IT
SERVES
TO
CAPTURE
THE
LOSS
IN
SAVINGS
DUE
TO
Δ
AND
THE
INABILITY
TO
ACHIEVE
PERFECT
COORDINATION
ANY
TRAFFIC
SHAPING
INCURS
SOME
ADDITIONAL
COMPLEXITY
AND
HENCE
A
VALID
QUESTION
IS
WHETHER
WE
NEED
ANY
TRAFFIC
SHAPING
OR
WHETHER
OPPORTUNISTIC
SLEEPING
THAT
DOES
NOT
REQUIRE
SHAPING
IS
ENOUGH
WE
THEREFORE
ALSO
COMPARE
OPTB
B
TO
OPPORTUNISTIC
SLEEPING
BASED
ON
WAKE
ON
ARRIVAL
WOA
FOR
THIS
NAIVE
WOA
WE
ASSUME
OPTIMISTICALLY
THAT
AN
INTERFACE
KNOWS
THE
PRECISE
ARRIVAL
TIME
OF
THE
SUBSEQUENT
PACKET
AND
RETURNS
TO
SLEEP
ONLY
FOR
INTER
PACKET
ARRIVAL
PERIODS
GREATER
THAN
Δ
BECAUSE
THE
PERFORMANCE
OF
OPPORTUNISTIC
WOA
DEPENDS
GREATLY
ON
THE
INTER
ARRIVAL
TIMES
OF
PACKETS
WE
EVALUATE
WOA
FOR
TWO
TYPES
OF
TRAFFIC
CONSTANT
BIT
RATE
CBR
AND
PARETO
FOR
EACH
OF
THE
ABOVE
BOUNDS
FIGURE
PLOTS
THE
PERCENTAGE
OF
TIME
ASLEEP
UNDER
INCREASING
UTILIZATION
IN
ABILENE
WE
USE
A
BUFFER
PERIOD
OF
B
AND
ASSUME
A
CONSERVATIVE
TRANSITION
TIME
Δ
OF
COMPARING
THE
SAVINGS
FROM
OPTB
B
TO
THE
UTILIZATION
BOUND
WE
SEE
THAT
A
TRAFFIC
SHAPING
APPROACH
BASED
ON
BUFFER
AND
BURST
CAN
ACHIEVE
MUCH
OF
THE
POTENTIAL
FOR
EXPLOITING
SLEEP
AS
EXPECTED
EVEN
AT
VERY
LOW
UTILIZA
TION
WOA
WITH
CBR
TRAFFIC
CAN
RARELY
SLEEP
PERHAPS
MORE
SURPRISING
IS
THAT
EVEN
WITH
BURSTY
TRAFFIC
WOA
PERFORMS
RELATIVELY
POORLY
THESE
RESULTS
SUGGEST
THAT
EVEN
ASSUMING
HARDWARE
WOA
TRAFFIC
SHAPING
OFFERS
A
SIGNIFICANT
IMPROVEMENT
OVER
OPPORTUNISTIC
SLEEP
A
PRACTICAL
ALGORITHM
WE
CONSIDER
A
VERY
SIMPLE
BUFFER
AND
BURST
SCHEME
CALLED
PRACTB
B
IN
WHICH
EACH
INGRESS
ROUTER
SENDS
ITS
BURSTS
DESTINED
FOR
THE
VARIOUS
EGRESSES
ONE
AFTER
THE
OTHER
IN
A
SINGLE
TRAIN
OF
BURSTS
AT
ROUTERS
CLOSE
TO
THE
INGRESS
THIS
APPEARS
AS
A
SINGLE
BURST
WHICH
THEN
DISPERSES
AS
IT
TRAVERSES
THROUGH
THE
NETWORK
PRACTB
B
BOUNDS
THE
NUMBER
OF
BURSTS
AND
CORRE
SPONDINGLY
THE
NUMBER
OF
TRANSITIONS
SEEN
BY
ANY
ROUTER
R
IN
AN
INTERVAL
OF
B
MS
TO
AT
MOST
IR
THE
TOTAL
NUMBER
OF
INGRESS
ROUTERS
THAT
SEND
TRAFFIC
THROUGH
R
IN
PRACTICE
OUR
RESULTS
SHOW
THAT
THE
NUMBER
OF
BURSTS
SEEN
BY
R
IN
TIME
BMS
IS
SIGNIFICANTLY
SMALLER
THAN
THIS
BOUND
AVERAGE
UTILIZATION
AVERAGE
UTILIZATION
AVERAGE
UTILIZATION
A
AVERAGE
DELAY
PRACTB
B
B
DELAY
ILE
FIGURE
TIME
ASLEEP
USING
CBR
AND
PARETO
TRAFFIC
PRACTB
B
IS
SIMPLE
IT
REQUIRES
NO
INTER
ROUTER
CO
ORDINATION
AS
THE
TIME
AT
WHICH
BURSTS
ARE
TRANSMITTED
IS
DECIDED
INDEPENDENTLY
BY
EACH
INGRESS
ROUTER
FOR
NET
WORKS
SUPPORTING
WAKE
ON
ARRIVAL
THE
IMPLEMENTATION
IS
TRIVIAL
THE
ONLY
ADDITIONAL
FEATURE
IS
THE
IMPLEMENTATION
OF
BUFFER
AND
BURST
AT
THE
INGRESS
NODES
FOR
NETWORKS
THAT
EMPLOY
TIMER
DRIVEN
SLEEPING
PACKET
BURSTS
WOULD
NEED
TO
INCLUDE
A
MARKER
DENOTING
THE
END
OF
BURST
AND
NOTIFYING
THE
ROUTER
OF
WHEN
IT
SHOULD
EXPECT
THE
NEXT
BURST
ON
THAT
INTERFACE
EVALUATION
WE
EVALUATE
THE
SAVINGS
VS
PERFORMANCE
TRADEOFF
ACHIEVED
BY
PRACTB
B
ALGORITHM
AND
THE
IMPACT
OF
EQUIPMENT
AND
NETWORK
PARAMETERS
ON
THE
SAME
SAVINGS
VS
PERFORMANCE
USING
PRACTB
B
WE
COMPARE
THE
SLEEP
TIME
ACHIEVED
BY
PRACTB
B
TO
THAT
ACHIEVABLE
BY
OPTB
B
IN
TERMS
OF
PERFORMANCE
WE
COMPARE
THE
END
TO
END
PACKET
DELAY
AND
LOSS
IN
A
NET
WORK
USING
PRACTB
B
TO
THAT
OF
A
NETWORK
THAT
NEVER
SLEEPS
AS
TODAY
AS
THIS
SHOWS
THE
OVERALL
PERFORMANCE
PENALTY
DUE
TO
OUR
SLEEP
PROTOCOLS
FIGURE
PLOTS
THE
SLEEP
TIME
WITH
INCREASING
UTILIZATION
ON
THE
ABILENE
NETWORK
USING
A
BUFFERING
INTERVAL
B
WE
PLOT
THE
PERCENTAGE
SLEEP
TIME
UNDER
BOTH
CBR
AND
PARETO
TRAFFIC
WORKLOADS
WE
SEE
THAT
EVEN
A
SCHEME
AS
SIMPLE
AS
PRACTB
B
CAN
CREATE
AND
EXPLOIT
SIGNIFICANT
OPPORTUNITIES
FOR
SLEEP
AND
APPROACHES
THE
SAVINGS
ACHIEVED
BY
THE
SIGNIFICANTLY
MORE
COMPLEX
OPTB
B
AS
WITH
OPPORTUNISTIC
SLEEPING
WE
SEE
THAT
PRACTB
B
SAVINGS
WITH
CBR
TRAFFIC
ARE
LOWER
THAN
FOR
THE
MORE
BURSTY
PARETO
WORKLOADS
BUT
THAT
THIS
REDUCTION
IS
SIGNIFICANTLY
SMALLER
IN
THE
CASE
OF
PRACTB
B
THAN
WITH
OPPORTUNISTIC
SLEEPING
RECALL
FIGURE
THAT
PARETO
TRAFFIC
IMPROVES
SAVINGS
IS
TO
BE
EXPECTED
AS
BURSTIER
TRAFFIC
ONLY
ENHANCES
OUR
BUNCHING
STRATEGY
FIGURES
A
AND
B
PLOT
THE
CORRESPONDING
AVERAGE
AND
PERCENTILE
OF
THE
END
TO
END
DELAY
AS
EXPECTED
WE
SEE
THAT
THE
ADDITIONAL
DELAY
IN
BOTH
CASES
IS
PROPOR
TIONAL
TO
THE
BUFFERING
INTERVAL
B
NOTE
THAT
THIS
IS
THE
END
TO
END
DELAY
REINFORCING
THAT
THE
BUFFERING
DELAY
B
IS
INCURRED
ONCE
FOR
THE
ENTIRE
END
TO
END
PATH
FIGURE
THE
IMPACT
ON
DELAY
OF
PRACTB
B
WE
SEE
THAT
FOR
HIGHER
B
THE
DELAY
GROWS
SLIGHTLY
FASTER
WITH
UTILIZATION
E
G
COMPARE
THE
ABSOLUTE
INCREASE
IN
DELAY
FOR
B
TO
BECAUSE
THIS
SITUATION
IS
MORE
PRONE
TO
LARGER
BURSTS
OVERLAPPING
AT
INTERMEDIATE
ROUTERS
HOWEVER
THIS
EFFECT
IS
RELATIVELY
SMALL
EVEN
IN
A
SITUATION
COMBINING
LARGER
B
AND
LARGER
UTILIZATIONS
AND
IS
NEGLIGIBLE
FOR
SMALLER
B
AND
OR
MORE
TYPICAL
UTILIZATIONS
WE
SEE
THAT
BOTH
AVERAGE
AND
MAXIMUM
DELAYS
IN
CREASE
ABRUPTLY
BEYOND
NETWORK
UTILIZATIONS
EXCEEDING
THIS
OCCURS
WHEN
CERTAIN
LINKS
APPROACH
FULL
UTILIZATION
AND
QUEUING
DELAYS
INCREASE
RECALL
THAT
THE
UTILIZATION
ON
THE
HORIZONTAL
AXIS
IS
THE
AVERAGE
NETWORK
WIDE
UTILIZATION
HOWEVER
THIS
INCREASE
OCCURS
EVEN
IN
THE
DEFAULT
NETWORK
SCENARIO
AND
IS
THUS
NOT
CAUSED
BY
PRACTB
B
TRAFFIC
SHAPING
FINALLY
OUR
MEASUREMENTS
REVEALED
THAT
PRACTB
B
INTRODUCED
NO
ADDITIONAL
PACKET
LOSS
RELA
TIVE
TO
THE
DEFAULT
NETWORK
SCENARIO
UNTIL
WE
APPROACH
UTILIZATIONS
THAT
COME
CLOSE
TO
SATURATING
SOME
LINKS
FOR
EXAMPLE
IN
A
NETWORK
SCENARIO
LOSSES
GREATER
THAN
OCCUR
AT
UTILIZATION
WITHOUT
ANY
BUFFERING
THEY
OCCUR
AT
UTILIZATION
WITH
B
AND
AT
UTILIZATION
WITH
B
AS
NETWORKS
DO
NOT
TYPICALLY
OPERATE
WITH
LINKS
CLOSE
TO
SATURATION
POINT
WE
DO
NOT
EXPECT
THIS
ADDITIONAL
LOSS
TO
BE
A
PROBLEM
IN
PRACTICE
IN
SUMMARY
THE
ABOVE
RESULTS
SUGGEST
THAT
PRACTB
B
CAN
YIELD
SIGNIFICANT
SAVINGS
WITH
A
VERY
SMALL
AND
CONTROLLABLE
IMPACT
ON
NETWORK
DELAY
AND
LOSS
FOR
EXAMPLE
AT
A
UTILIZATION
OF
A
BUFFERING
TIME
OF
JUST
B
ALLOWS
THE
NETWORK
TO
SPEND
OVER
OF
ITS
TIME
IN
SLEEP
MODE
FOR
UNDER
ADDED
DELAY
IMPACT
OF
HARDWARE
CHARACTERISTICS
WE
NOW
EVALUATE
HOW
THE
TRANSITION
TIME
Δ
AFFECTS
THE
PERFORMANCE
OF
PRACTB
B
FIGURE
A
PLOTS
THE
SLEEP
TIME
ACHIEVED
BY
PRACTB
B
FOR
A
RANGE
OF
TRANSITION
TIMES
AND
COMPARES
THIS
TO
THE
IDEAL
CASE
OF
HAVING
INSTANTANEOUS
TRANSITIONS
AS
EXPECTED
THE
ABILITY
TO
SLEEP
DEGRADES
DRASTICALLY
WITH
INCREASING
Δ
THIS
OBSERVATION
HOLDS
ACROSS
VARIOUS
BUFFER
INTERVALS
B
AS
ILLUSTRATED
IN
FIGURE
B
THAT
PLOTS
THE
SLEEP
TIME
ACHIEVED
AT
TYPICAL
UTILIZATION
FOR
IDEAL
B
FOR
EXAMPLE
ETHERNET
LINKS
DISSIPATE
BETWEEN
WHEN
OPERATING
BETWEEN
COMPARED
TO
BETWEEN
SECOND
OPERATING
AT
A
LOWER
FREQUENCY
ALSO
ALLOWS
THE
USE
OF
DYNAMIC
VOLTAGE
SCALING
DVS
THAT
REDUCES
THE
OPERATING
VOLTAGE
THIS
ALLOWS
POWER
TO
SCALE
CUBICALLY
AND
HENCE
ENERGY
CON
SUMPTION
QUADRATICALLY
WITH
OPERATING
FREQUENCY
AVERAGE
UTILIZATION
IMPACT
OF
Δ
TRANSITION
TIME
MS
IMPACT
OF
B
DVS
AND
FREQUENCY
SCALING
ARE
ALREADY
COMMON
IN
MICROPROCESSORS
FOR
THESE
REASONS
WE
ASSUME
THE
APPLICATION
OF
THESE
TECHNIQUES
TO
NETWORK
LINKS
AND
ASSOCIATED
EQUIPMENT
I
E
LINECARDS
FIGURE
THE
IMPACT
OF
HARDWARE
CONSTANTS
ON
SLEEP
TIME
LINK
UTILIZATION
FIGURE
TIME
ASLEEP
PER
LINK
INCREASING
TRANSITION
TIMES
AND
DIFFERENT
VALUES
OF
B
THESE
FINDINGS
REINFORCE
OUR
INTUITION
THAT
HARDWARE
SUPPORT
FEATURING
LOW
POWER
SLEEP
STATES
AND
QUICK
TRANSITIONS
PREFERABLY
BETWEEN
THESE
STATES
ARE
ESSENTIAL
TO
EFFECTIVELY
SAVE
ENERGY
IMPACT
OF
NETWORK
TOPOLOGY
WE
NOW
EVALUATE
PRACTB
B
FOR
THE
INTEL
ENTERPRISE
NETWORK
THE
ROUTING
STRUCTURE
OF
THE
INTEL
NETWORK
IS
STRICTLY
HIERARCHICAL
WITH
A
RELATIVELY
SMALL
NUMBER
OF
NODES
THAT
CONNECT
TO
THE
WIDE
AREA
BECAUSE
OF
THIS
WE
FIND
A
WIDE
VARIATION
IN
LINK
UTILIZATION
FAR
MORE
THAN
ON
THE
ABILENE
NETWORK
OVER
OF
LINKS
HAVE
UTILIZATIONS
BELOW
WHILE
A
SMALL
NUMBER
OF
LINKS
CAN
SEE
SIGNIFICANTLY
HIGHER
UTILIZATIONS
OF
BETWEEN
CORRESPOND
INGLY
THE
OPPORTUNITY
FOR
SLEEP
ALSO
VARIES
GREATLY
ACROSS
LINKS
THIS
IS
SHOWN
IN
FIGURE
EACH
POINT
IN
THE
SCATTER
PLOT
CORRESPONDS
TO
A
SINGLE
LINK
AND
WE
LOOK
AT
SLEEP
TIMES
FOR
TWO
TRANSITION
TIMES
AND
WE
SEE
THAT
THE
DOMINANT
TRENDS
IN
SLEEP
TIME
VS
UTILIZATION
REMAINS
AND
THAT
HIGHER
Δ
YIELDS
LOWER
SAVINGS
RATE
ADAPTATION
IN
NETWORKS
THIS
SECTION
EXPLORES
THE
USE
OF
PERFORMANCE
STATES
TO
REDUCE
NETWORK
ENERGY
CONSUMPTION
MODEL
AND
ASSUMPTIONS
BACKGROUND
IN
GENERAL
OPERATING
A
DEVICE
AT
A
LOWER
FREQUENCY
CAN
ENABLE
DRAMATIC
REDUCTIONS
IN
ENERGY
CONSUMPTION
FOR
TWO
REASONS
FIRST
SIMPLY
OPERATING
MORE
SLOWLY
OFFERS
SOME
FAIRLY
SUBSTANTIAL
SAVINGS
TRANSCEIVERS
WHILE
THE
USE
OF
DVS
HAS
BEEN
DEMON
STRATED
IN
PROTOTYPE
LINECARDS
IT
IS
NOT
CURRENTLY
SUP
PORTED
IN
COMMERCIAL
EQUIPMENT
AND
HENCE
WE
INVESTI
GATE
SAVINGS
UNDER
TWO
DIFFERENT
SCENARIOS
EQUIPMENT
THAT
SUPPORTS
ONLY
FREQUENCY
SCALING
AND
EQUIPMENT
THAT
SUPPORTS
BOTH
FREQUENCY
AND
VOLTAGE
SCALING
MODEL
WE
ASSUME
INDIVIDUAL
LINKS
CAN
SWITCH
PERFOR
MANCE
STATES
INDEPENDENTLY
AND
WITH
INDEPENDENT
RATES
FOR
TRANSMISSION
AND
RECEPTION
ON
INTERFACES
HENCE
THE
SAVINGS
WE
OBTAIN
APPLY
DIRECTLY
TO
THE
CONSUMPTION
AT
THE
LINKS
AND
INTERFACE
CARDS
OF
A
NETWORK
ELEMENT
ALTHOUGH
IN
PRACTICE
ONE
COULD
ALSO
SCALE
THE
RATE
OF
OPERATION
OF
THE
SWITCH
FABRIC
AND
OR
ROUTE
PROCESSOR
WE
ASSUME
THAT
EACH
NETWORK
INTERFACE
SUPPORTS
N
PERFORMANCE
STATES
CORRESPONDING
TO
LINK
RATES
RN
WITH
RI
RI
AND
RN
RMAX
THE
DEFAULT
MAXIMUM
LINK
RATE
AND
WE
INVESTIGATE
THE
EFFECT
THAT
THE
GRANULARITY
AND
DISTRIBUTION
LINEAR
VS
EXPONENTIAL
OF
THESE
RATES
HAS
ON
THE
POTENTIAL
ENERGY
SAVINGS
THE
FINAL
DEFINING
CHARACTERISTIC
OF
PERFORMANCE
STATES
IS
THE
TRANSITION
TIME
DENOTED
Δ
DURING
WHICH
PACKET
TRANSMISSION
IS
STALLED
AS
THE
LINK
TRANSITIONS
BETWEEN
SUCCESSIVE
RATES
WE
EXPLORE
PERFORMANCE
FOR
A
RANGE
OF
TRANSITION
TIMES
Δ
FROM
TO
MILLISECONDS
MEASURING
SAVINGS
AND
PERFORMANCE
AS
IN
THE
CASE
OF
SLEEP
WE
RE
INTERESTED
IN
SOLUTIONS
THAT
REDUCE
THE
RATE
AT
WHICH
LINKS
OPERATE
WITHOUT
SIGNIFICANTLY
AFFECTING
PERFORMANCE
IN
THIS
SECTION
WE
USE
THE
PERCENTAGE
REDUCTION
IN
AVERAGE
LINK
RATE
AS
AN
INDICATIVE
MEASURE
OF
ENERGY
SAVINGS
AND
RELATE
THIS
TO
OVERALL
ENERGY
SAVINGS
IN
SECTION
WHERE
WE
TAKE
INTO
ACCOUNT
THE
POWER
PROFILE
OF
EQUIPMENT
INCLUDING
WHETHER
IT
SUPPORTS
DVS
OR
NOT
IN
TERMS
OF
PERFORMANCE
WE
AGAIN
MEASURE
THE
AVERAGE
AND
PERCENTILE
OF
THE
END
TO
END
PACKET
DELAY
AND
PACKET
LOSS
AN
OPTIMAL
STRATEGY
OUR
INITIAL
INTEREST
IS
TO
UNDERSTAND
THE
EXTENT
TO
WHICH
PERFORMANCE
STATES
CAN
HELP
IF
USED
TO
BEST
EFFECT
FOR
A
DVS
PROCESSOR
IT
HAS
BEEN
SHOWN
THAT
THE
MOST
ENERGY
EFFICIENT
WAY
TO
EXECUTE
C
CYCLES
WITHIN
A
GIVEN
TIME
INTERVAL
T
IS
TO
MAINTAIN
A
CONSTANT
CLOCK
SPEED
OF
FIGURE
AN
ILLUSTRATION
OF
DELAY
CONSTRAINED
SERVICE
CURVES
AND
THE
SERVICE
CURVE
MINIMIZING
ENERGY
C
T
IN
THE
CONTEXT
OF
A
NETWORK
LINK
THIS
TRANSLATES
INTO
SENDING
PACKETS
AT
A
CONSTANT
RATE
EQUAL
TO
THE
AVERAGE
ARRIVAL
RATE
HOWEVER
UNDER
NON
UNIFORM
TRAFFIC
THIS
CAN
RESULT
IN
ARBITRARY
DELAYS
AND
HENCE
WE
INSTEAD
LOOK
FOR
AN
OPTIMAL
SCHEDULE
OF
RATES
I
E
THE
SET
OF
RATES
AT
WHICH
THE
LINK
SHOULD
OPERATE
AT
DIFFERENT
POINTS
IN
TIME
THAT
MINIMIZE
ENERGY
WHILE
RESPECTING
A
SPECIFIED
CONSTRAINT
ON
THE
ADDITIONAL
DELAY
INCURRED
AT
THE
LINK
MORE
PRECISELY
GIVEN
A
PACKET
ARRIVAL
CURVE
AC
WE
LOOK
FOR
A
SERVICE
CURVE
SC
THAT
MINIMIZES
ENERGY
CON
SUMPTION
WHILE
RESPECTING
A
GIVEN
UPPER
BOUND
D
ON
THE
PER
PACKET
QUEUING
DELAY
THE
DELAY
PARAMETER
D
THUS
SERVES
TO
TRADEOFF
SAVINGS
FOR
INCREASED
DELAY
FIGURE
A
SHOWS
AN
EXAMPLE
ARRIVAL
CURVE
AND
THE
ASSOCIATED
LATEST
DEPARTURE
CURVE
AC
D
WHICH
IS
SIMPLY
THE
ARRIVAL
CURVE
SHIFTED
IN
TIME
BY
THE
DELAY
BOUND
D
TO
MEET
THE
DELAY
CONSTRAINT
THE
SERVICE
CURVE
SC
MUST
LIE
WITHIN
THE
AREA
BETWEEN
THE
ARRIVAL
AND
LATEST
DEPARTURE
CURVES
IN
THE
CONTEXT
OF
WIRELESS
LINKS
PROVES
THAT
IF
THE
ENERGY
CAN
BE
EXPRESSED
AS
A
CONVEX
MONOTONICALLY
INCREASING
FUNCTION
OF
THE
TRANSMISSION
RATE
THEN
THE
MINIMAL
ENERGY
SERVICE
CURVE
IS
THE
SHORTEST
EUCLIDEAN
DISTANCE
IN
THE
ARRIVAL
SPACE
BYTES
TIME
BETWEEN
THE
ARRIVAL
AND
SHIFTED
ARRIVAL
CURVES
IN
THE
SCENARIO
WHERE
WE
ASSUME
DVS
SUPPORT
THE
ENERGY
CONSUMPTION
IS
A
CONVEX
MONOTONICALLY
INCREAS
ING
FUNCTION
OF
LINK
RATE
AND
THUS
THIS
RESULT
APPLIES
TO
OUR
CONTEXT
AS
WELL
WHERE
ONLY
FREQUENCY
SCALING
IS
TO
CALIBRATE
PRACTICAL
PROTOCOLS
WE
WILL
EVALUATE
THE
SAVINGS
ACHIEVED
BY
APPLYING
THE
ABOVE
PER
LINK
SOLUTION
AT
ALL
LINKS
IN
THE
NETWORK
AND
CALL
THIS
APPROACH
LINK
OPTRA
ONE
ISSUE
IN
DOING
SO
IS
THAT
THE
SERVICE
CURVES
AT
THE
DIFFERENT
LINKS
ARE
INTER
DEPENDENT
I
E
THE
SERVICE
CURVE
FOR
A
LINK
L
DEPENDS
IN
TURN
ON
THE
SERVICE
CURVES
AT
OTHER
LINKS
SINCE
THE
LATTER
IN
TURN
DETERMINE
THE
ARRIVAL
CURVE
AT
L
WE
ADDRESS
THIS
BY
APPLYING
THE
PER
LINK
OPTIMAL
ALGORITHM
ITERATIVELY
ACROSS
ALL
LINKS
UNTIL
THE
SERVICE
AND
ARRIVAL
CURVES
AT
THE
DIFFERENT
LINKS
CONVERGE
A
PRACTICAL
ALGORITHM
BUILDING
ON
THE
INSIGHT
OFFERED
BY
THE
PER
LINK
OPTI
MAL
ALGORITHM
WE
DEVELOP
A
SIMPLE
APPROACH
CALLED
PRACTRA
PRACTICAL
RATE
ADAPTATION
THAT
SEEKS
TO
NAV
IGATE
THE
TRADEOFF
BETWEEN
SAVINGS
AND
DELAY
CONSTRAINTS
A
PRACTICAL
APPROACH
DIFFERS
FROM
THE
OPTIMUM
IN
THAT
IT
DOES
NOT
HAVE
KNOWLEDGE
OF
FUTURE
PACKET
ARRIVALS
IT
CAN
ONLY
CHOOSE
AMONG
A
FIXED
SET
OF
AVAILABLE
RATES
RN
AND
III
AT
EVERY
RATE
SWITCH
IT
INCURS
A
PENALTY
Δ
DURING
WHICH
IT
CANNOT
SEND
PACKETS
WHILE
KNOWLEDGE
OF
THE
FUTURE
ARRIVAL
RATE
IS
UNAVAIL
ABLE
WE
CAN
USE
THE
HISTORY
OF
PACKET
ARRIVALS
TO
PREDICT
THE
FUTURE
ARRIVAL
RATE
WE
DENOTE
THIS
PREDICTED
ARRIVAL
RATE
AS
RˆF
AND
ESTIMATE
IT
WITH
AN
EXPONENTIALLY
WEIGHTED
MOVING
AVERAGE
EWMA
OF
THE
MEASURED
HISTORY
OF
PAST
ARRIVALS
SIMILARLY
WE
CAN
USE
THE
CURRENT
LINK
BUFFER
SIZE
Q
AND
RATE
RI
TO
ESTIMATE
THE
POTENTIAL
QUEUING
DELAY
SO
AS
TO
AVOID
VIOLATING
THE
DELAY
CONSTRAINT
WITH
THESE
SUBSTITUTES
WE
DEFINE
A
TECHNIQUE
INSPIRED
BY
THE
PER
LINK
OPTIMAL
ALGORITHM
IN
PRACTRA
PACKETS
ARE
SERVICED
AT
A
CONSTANT
RATE
UNTIL
WE
INTERSECT
ONE
OF
THE
TWO
BOUNDING
CURVES
PRESENTED
EARLIER
FIGURE
THE
ARRIVAL
CURVE
AC
AND
THE
LATEST
DEPARTURE
CURVE
AC
D
THUS
WE
AVOID
INCREASING
THE
OPERATING
RATE
RI
UNLESS
NOT
DOING
SO
WOULD
VIOLATE
THE
DELAY
CONSTRAINT
THIS
LEADS
TO
THE
FOLLOWING
CONDITION
FOR
RATE
INCREASES
A
LINK
OPERATING
AT
RATE
RI
WITH
CURRENT
QUEUE
SIZE
Q
SUPPORTED
ANY
SERVICE
CURVE
BETWEEN
THE
ARRIVAL
AND
SHIFTED
ARRIVAL
CURVES
WOULD
ACHIEVE
THE
SAME
ENERGY
INCREASES
ITS
RATE
TO
RI
IFF
Q
I
D
OR
ΔRˆF
Q
D
Δ
I
SAVINGS
AND
THEREFORE
THE
SERVICE
CURVE
WITH
THE
SHORTEST
EUCLIDEAN
DISTANCE
WOULD
BE
OPTIMAL
IN
THIS
CASE
TOO
IN
SUMMARY
FOR
BOTH
FREQUENCY
SCALING
AND
DVS
THE
SHORTEST
DISTANCE
SERVICE
CURVE
WOULD
ACHIEVE
THE
HIGHEST
POSSIBLE
ENERGY
SAVINGS
FIG
ILLUSTRATES
AN
EXAMPLE
OF
SUCH
A
MINIMAL
ENERGY
SERVICE
CURVE
INTUITIVELY
THIS
IS
THE
SET
OF
LOWEST
CONSTANT
RATES
OBEYING
THE
DELAY
CONSTRAINT
NOTE
THAT
THIS
PER
LINK
OPTIMAL
STRATEGY
IS
NOT
SUITED
TO
PRACTICAL
IMPLEMENTATION
SINCE
IT
ASSUMES
PERFECT
KNOWLEDGE
OF
THE
FUTURE
ARRIVAL
CURVE
LINK
RATES
OF
INFINITE
GRANULARITY
AND
IGNORES
SWITCHING
OVERHEADS
NONETHELESS
IT
IS
USEFUL
AS
AN
ESTIMATE
OF
THE
POTENTIAL
SAVINGS
BY
WHICH
THE
FIRST
TERM
CHECKS
WHETHER
THE
DELAY
BOUND
D
WOULD
BE
VIOLATED
WERE
WE
TO
MAINTAIN
THE
CURRENT
LINK
RATE
THE
SECOND
CONSTRAINT
ENSURES
THAT
THE
SERVICE
CURVE
DOES
NOT
GET
TOO
CLOSE
TO
THE
DELAY
CONSTRAINED
CURVE
WHICH
WOULD
PREVENT
US
FROM
ATTEMPTING
A
RATE
INCREASE
IN
THE
FUTURE
WITHOUT
VIOLATING
THE
DELAY
BOUND
THAT
IS
WE
NEED
TO
ALLOW
ENOUGH
TIME
FOR
A
LINK
THAT
INCREASES
ITS
RATE
TO
SUBSEQUENTLY
PROCESS
PACKETS
THAT
ARRIVED
DURING
THE
TRANSITION
TIME
ESTIMATED
BY
ΔRˆF
AND
ITS
ALREADY
ACCUMULATED
QUEUE
NOTE
THAT
WE
CANNOT
USE
DELAY
CONSTRAINTS
D
SMALLER
THAN
THE
TRANSITION
TIME
Δ
SIMILARLY
THE
CONDITION
UNDER
WHICH
WE
ALLOW
A
RATE
DECREASE
IS
AS
FOLLOWS
A
LINK
OPERATING
AT
RATE
RI
WITH
CURRENT
QUEUE
SIZE
Q
DECREASES
ITS
RATE
TO
RI
IFF
Q
AND
RˆF
RI
FIRST
WE
ONLY
ATTEMPT
TO
SWITCH
TO
A
LOWER
RATE
WHEN
THE
QUEUE
AT
THE
LINK
IS
EMPTY
Q
INTUITIVELY
THIS
CORRE
SPONDS
TO
AN
INTERSECTION
BETWEEN
THE
ARRIVAL
CURVE
AND
THE
SERVICE
CURVE
HOWEVER
WE
DON
T
ALWAYS
SWITCH
TO
A
LOWER
RATE
RI
WHEN
A
QUEUE
EMPTIES
AS
DOING
SO
WOULD
PREVENT
THE
ALGORITHM
FROM
OPERATING
IN
A
DESIRED
STEADY
STATE
MODE
WITH
ZERO
QUEUING
DELAY
INSTEAD
THE
DESIRABLE
STEADY
STATE
IS
ONE
WHERE
RI
RF
RI
AND
WE
WANT
TO
AVOID
OSCILLATING
BETWEEN
RATES
WHICH
UNIFORM
R
UNIFORM
R
AVERAGE
UTILIZATION
WOULD
LEAD
TO
LARGER
AVERAGE
DELAY
FOR
EXAMPLE
A
LINK
THAT
SEES
A
CONSTANT
ARRIVAL
RATE
OF
MIGHT
OSCILLATE
BETWEEN
AND
INCURRING
QUEUING
FIGURE
AVERAGE
RATE
OF
OPERATION
THE
AVERAGE
IS
WEIGHTED
BY
THE
TIME
SPENT
AT
A
PARTICULAR
RATE
DELAYS
AT
INSTEAD
OF
REMAINING
AT
WITH
LOW
AVERAGE
QUEUING
DELAY
WE
THUS
USE
THE
ADDITIONAL
CONDITION
RˆF
RI
TO
STEER
OUR
ALGORITHM
TOWARD
THE
DESIRED
STEADY
STATE
AND
ENSURE
THAT
SWITCHING
TO
A
LOWER
RATE
DOES
NOT
IMMEDIATELY
LEAD
TO
LARGER
QUEUES
IN
ADDITION
TO
THE
ABOVE
CONDITIONS
WE
FURTHER
DIS
COURAGE
OSCILLATIONS
BY
ENFORCING
A
MINIMUM
TIME
KΔ
BETWEEN
CONSECUTIVE
SWITCHES
INTUITIVELY
THIS
IS
BECAUSE
RATE
SWITCHING
SHOULD
NOT
OCCUR
ON
TIMESCALES
SMALLER
THAN
THE
TRANSITION
TIME
Δ
IN
OUR
EXPERIMENTS
WE
FOUND
K
TO
BE
A
REASONABLE
VALUE
FOR
THIS
PARAMETER
AVERAGE
UTILIZATION
AVERAGE
DELAY
AVERAGE
UTILIZATION
DELAY
ILE
NOTE
THAT
UNLIKE
THE
LINK
OPTRA
ALGORITHM
THE
ABOVE
DECISION
PROCESS
DOES
NOT
GUARANTEE
THAT
THE
DELAY
CONSTRAINTS
WILL
BE
MET
SINCE
IT
IS
BASED
ON
ESTIMATED
RATHER
THAN
TRUE
ARRIVAL
RATES
SIMILARLY
PRACTRA
CANNOT
GUARANTEE
THAT
THE
RATES
USED
BY
THE
LINKS
MATCH
THOSE
USED
BY
THE
LINK
OPTIMAL
ALGORITHM
IN
SECTION
AFTER
DISCUSSING
HOW
POWER
SCALES
WITH
RATE
WE
USE
SIMULATION
TO
COMPARE
OUR
APPROXIMATE
ALGORITHM
TO
THE
OPTIMAL
UNDER
REALISTIC
NETWORK
CONDITIONS
WE
LEAVE
IT
TO
FUTURE
WORK
TO
ANALYTICALLY
BOUND
THE
INACCURACY
DUE
TO
OUR
APPROXIMATIONS
FINALLY
WE
OBSERVE
THAT
THE
ABOVE
RATE
ADAPTATION
IS
SIMPLE
IN
THAT
IT
REQUIRES
NO
COORDINATION
ACROSS
DIFFERENT
NODES
AND
IS
AMENABLE
TO
IMPLEMENTATION
IN
HIGH
SPEED
EQUIPMENT
THIS
IS
BECAUSE
THE
ABOVE
DECISION
MAKING
NEED
NOT
BE
PERFORMED
ON
A
PER
PACKET
BASIS
EVALUATION
WE
EVALUATE
THE
SAVINGS
VS
PERFORMANCE
TRADEOFF
ACHIEVED
BY
OUR
PRACTICAL
RATE
ADAPTATION
ALGORITHM
AND
THE
IMPACT
OF
EQUIPMENT
AND
NETWORK
PARAMETERS
ON
THE
SAME
WE
FIRST
EVALUATE
THE
PERCENTAGE
REDUCTION
IN
AV
ERAGE
LINK
RATE
ACHIEVED
BY
PRACTRA
FOR
THE
ABILENE
NETWORK
FOR
COMPARISON
WE
CONSIDER
THE
RATE
REDUCTION
DUE
TO
LINK
OPTRA
AND
THE
UPPER
BOUND
ON
RATE
REDUC
TION
AS
DETERMINED
BY
THE
AVERAGE
LINK
UTILIZATION
IN
THIS
CASE
SINCE
WE
FOUND
THE
REDUCTION
FROM
LINK
OPTRA
WAS
VIRTUALLY
INDISTINGUISHABLE
FROM
THE
UTILIZATION
BOUND
WE
ONLY
SHOW
THE
LATTER
HERE
FOR
CLARITY
WE
RE
FIGURE
AVERAGE
AND
PERCENTILE
DELAY
ACHIEVED
BY
PRACTRA
FOR
VARIOUS
AVAILABLE
RATES
PORT
ON
THE
ENERGY
SAVINGS
DUE
TO
LINK
OPTRA
IN
THE
FOLLOWING
SECTION
TO
MEASURE
PERFORMANCE
WE
MEASURE
THE
END
TO
END
PACKET
DELAY
AND
LOSS
IN
A
NETWORK
USING
PRACTRA
TO
THAT
IN
A
NETWORK
WITH
NO
RATE
ADAPTATION
IMPACT
OF
HARDWARE
CHARACTERISTICS
THE
MAIN
CON
STANTS
AFFECTING
THE
PERFORMANCE
OF
PRACTRA
ARE
THE
GRANULARITY
AND
DISTRIBUTION
OF
AVAILABLE
RATES
AND
Δ
THE
TIME
TO
TRANSITION
BETWEEN
SUCCESSIVE
RATES
WE
INVESTIGATE
THE
REDUCTION
IN
RATE
FOR
THREE
DIFFERENT
DIS
TRIBUTIONS
OF
RATES
RN
I
RATES
UNIFORMLY
DIS
TRIBUTED
BETWEEN
TO
II
RATES
UNIFORMLY
DISTRIBUTED
BETWEEN
TO
AND
III
EXPO
NENTIALLY
DISTRIBUTED
RATES
WE
CONSIDER
THE
LATTER
CASE
SINCE
PHYSICAL
LAYER
TECHNOLOGIES
FOR
THESE
RATES
ALREADY
EXIST
MAKING
THESE
LIKELY
CANDIDATES
FOR
EARLY
RATE
ADAPTATION
TECHNOLOGIES
FIGURE
PLOTS
THE
AVERAGE
RATE
REDUCTION
UNDER
INCREAS
ING
UTILIZATIONS
WITH
A
PER
LINK
DELAY
CONSTRAINT
D
Δ
AND
A
TRANSITION
TIME
Δ
WE
SEE
THAT
FOR
UNIFORMLY
DISTRIBUTED
RATES
PRACTRA
OPERATES
LINKS
AT
A
RATE
THAT
APPROACHES
THE
AVERAGE
LINK
UTILIZATION
WITH
UNIFORMLY
DISTRIBUTED
RATES
THIS
REDUCTION
DROPS
BUT
NOT
SIGNIFICANTLY
HOWEVER
FOR
EXPONENTIALLY
DISTRIBUTED
RATES
THE
ALGORITHM
PERFORMS
POORLY
INDICATING
THAT
SUPPORT
FOR
UNIFORMLY
DISTRIBUTED
RATES
IS
ESSENTIAL
UNIFORM
TRANSITION
TIME
MS
UTILIZ
UTILIZ
TRANSITION
TIME
MS
EXPONENTIAL
UTILIZATION
BOUND
LINK
UTILIZATION
RATE
REDUCTION
TIME
MAX
DELAY
PERCENTILE
FIGURE
RATE
REDUCTION
PER
LINK
FIGURE
IMPACT
OF
SWITCH
TIME
Δ
FOR
PRACTRA
FIGURE
PLOTS
THE
CORRESPONDING
AVERAGE
AND
PERCENTILE
OF
THE
END
TO
END
DELAY
WE
SEE
THAT
FOR
ALL
THE
SCENARIOS
THE
INCREASE
IN
AVERAGE
DELAY
DUE
TO
PRACTRA
IS
VERY
SMALL
IN
THE
WORST
CASE
AND
LESS
THAN
FOR
THE
SCENARIO
USING
UNIFORM
RATES
THE
INCREASE
IN
MAXIMUM
DELAY
IS
ALSO
REASONABLE
AT
MOST
WITH
PRACTRA
RELATIVE
TO
WITH
NO
ADAPTATION
PERHAPS
SURPRISINGLY
THE
LOWEST
ADDITIONAL
DELAY
OCCURS
IN
THE
CASE
OF
UNIFORMLY
DISTRIBUTED
RATES
WE
FOUND
THIS
OCCURS
BECAUSE
THERE
ARE
FEWER
RATE
TRANSITIONS
WITH
CORRESPONDING
TRANSITION
DELAYS
IN
THIS
CASE
FI
NALLY
WE
FOUND
THAT
PRACTRA
INTRODUCED
NO
ADDITIONAL
PACKET
LOSS
FOR
THE
RANGE
OF
UTILIZATIONS
CONSIDERED
NEXT
WE
LOOK
AT
THE
IMPACT
OF
TRANSITION
TIMES
Δ
FIGURES
A
AND
B
PLOT
THE
AVERAGE
RATE
REDUCTION
AND
PERCENTILE
OF
DELAY
UNDER
INCREASING
Δ
FOR
DIFFERENT
NETWORK
UTILIZATIONS
FOR
EACH
TEST
WE
SET
THE
DELAY
CONSTRAINT
AS
D
Δ
AND
WE
ASSUME
UNIFORM
RATES
AS
WOULD
BE
EXPECTED
WE
SEE
THAT
LARGER
Δ
LEAD
TO
REDUCED
SAVINGS
AND
HIGHER
DELAY
ON
THE
WHOLE
WE
SEE
THAT
IN
THIS
SCENARIO
BOTH
SAVINGS
AND
PERFORMANCE
REMAIN
ATTRACTIVE
FOR
TRANSITION
TIMES
AS
HIGH
AS
IN
SUMMARY
THESE
RESULTS
SUGGEST
THAT
RATE
ADAPTATION
AS
IMPLEMENTED
BY
PRACTRA
HAS
THE
POTENTIAL
TO
OFFERS
SIGNIFICANT
ENERGY
SAVINGS
WITH
LITTLE
IMPACT
ON
PACKET
LOSS
OR
DELAY
IN
ALL
OUR
TESTS
WE
FOUND
PRACTRA
TO
HAVE
MINIMAL
EFFECTS
ON
THE
AVERAGE
DELAY
AND
LOSS
AND
HENCE
FROM
HERE
ON
WE
MEASURE
THE
PERFORMANCE
IMPACT
ONLY
IN
TERMS
OF
THE
PERCENTILE
IN
PACKET
DELAY
IMPACT
OF
NETWORK
TOPOLOGY
WE
NOW
EVALUATE
PRACTRA
APPLIED
TO
THE
INTEL
ENTERPRISE
NETWORK
FIG
URE
PLOTS
THE
RATE
REDUCTION
ACROSS
LINKS
EACH
POINT
IN
THE
SCATTER
PLOT
CORRESPONDS
TO
A
SINGLE
LINK
AND
WE
LOOK
AT
RATE
REDUCTION
FOR
TWO
RATE
DISTRIBUTION
POLICIES
UNI
FORMLY
DISTRIBUTED
RATES
AND
EXPONENTIALLY
DISTRIBUTED
RATES
SINCE
THESE
ARE
PER
LINK
RESULTS
WE
SEE
SIGNIFICANT
VARIATIONS
IN
RATE
REDUCTION
FOR
THE
SAME
UTILIZATION
DUE
TO
SPECIFICS
OF
TRAFFIC
ACROSS
VARIOUS
LINKS
WE
ALSO
NOTICE
THAT
THE
DOMINANT
TREND
IN
REDUCTION
REMAINS
SIMILAR
TO
THAT
SEEN
IN
THE
ABILENE
NETWORK
FIGURE
OVERALL
ENERGY
SAVINGS
IN
THE
PREVIOUS
SECTIONS
WE
EVALUATED
POWER
MANAGEMENT
SOLUTIONS
BASED
ON
THEIR
ABILITY
TO
INCREASE
SLEEP
TIMES
SECTION
OR
OPERATE
AT
REDUCED
RATES
SECTION
IN
THIS
SECTION
WE
TRANSLATE
THESE
TO
OVERALL
ENERGY
SAVINGS
AND
HENCE
COMPARE
THE
RELATIVE
MERITS
OF
RATE
ADAPTATION
VS
SLEEPING
FOR
THIS
WE
DEVELOP
AN
ANALYTICAL
MODEL
OF
POWER
CONSUMPTION
UNDER
DIFFERENT
OPERATING
MODES
OUR
MODEL
DERIVES
FROM
MEASUREMENTS
OF
EXISTING
NETWORKING
EQUIPMENT
AT
THE
SAME
TIME
WE
CONSTRUCT
THE
MODEL
TO
BE
SUFFICIENTLY
GENERAL
THAT
WE
MAY
STUDY
THE
POTENTIAL
IMPACT
OF
FUTURE
MORE
ENERGY
EFFICIENT
HARDWARE
POWER
MODEL
RECALL
FROM
SECTION
THAT
THE
TOTAL
ENERGY
CONSUMP
TION
OF
A
NETWORK
ELEMENT
OPERATING
IN
THE
ABSENCE
OF
ANY
POWER
SAVING
MODES
CAN
BE
APPROXIMATED
AS
E
PATA
PITI
WE
START
BY
CONSIDERING
THE
POWER
CONSUMPTION
WHEN
ACTIVELY
PROCESSING
PACKETS
PA
TYPICALLY
A
PORTION
OF
THIS
POWER
DRAW
IS
STATIC
IN
THE
SENSE
THAT
IT
DOES
NOT
DEPEND
ON
THE
OPERATING
FREQUENCY
E
G
REFRESH
POWER
IN
MEMORY
BLOCKS
LEAKAGE
CURRENTS
AND
SO
FORTH
WHILE
THE
DOMINANT
PORTION
OF
POWER
DRAW
DOES
SCALE
WITH
OPERATING
FREQUENCY
CORRESPONDINGLY
WE
SET
PA
R
C
F
R
INTUITIVELY
C
CAN
BE
VIEWED
AS
THAT
PORTION
OF
POWER
DRAW
THAT
CANNOT
BE
ELIMINATED
THROUGH
RATE
ADAPTATION
WHILE
F
R
REFLECTS
THE
RATE
DEPENDENT
PORTION
OF
ENERGY
CONSUMPTION
TO
REFLECT
THE
RELATIVE
PROPORTIONS
OF
C
AND
F
R
WE
SET
C
TO
BE
RELATIVELY
SMALL
BETWEEN
AND
OF
THE
MAXIMUM
ACTIVE
POWER
PA
RN
TO
STUDY
THE
EFFECT
OF
JUST
FREQUENCY
SCALING
ALONE
WE
SET
F
R
O
R
AND
SET
F
R
O
TO
EVALUATE
DYNAMIC
VOLTAGE
SCAL
ING
DVS
IN
EVALUATING
DVS
WE
NEED
TO
CONSIDER
AN
ADDITIONAL
CONSTRAINT
NAMELY
THAT
IN
PRACTICE
THERE
IS
A
MINIMUM
RATE
THRESHOLD
BELOW
WHICH
SCALING
THE
LINK
RATE
OFFERS
NO
FURTHER
REDUCTION
IN
VOLTAGE
WE
THUS
DEFINE
A
MAXIMUM
SCALING
FACTOR
Λ
AND
LIMIT
OUR
CHOICE
OF
AVAIL
ABLE
OPERATING
RATES
TO
LIE
BETWEEN
RN
Λ
RN
FOR
SCENAR
IOS
THAT
ASSUME
VOLTAGE
SCALING
WHILE
CURRENT
TRANSISTOR
TECHNOLOGY
ALLOWS
SCALING
UP
TO
FACTORS
AS
HIGH
AS
CURRENT
PROCESSORS
TYPICALLY
USE
Λ
AND
HENCE
WE
INVESTIGATE
BOTH
VALUES
AS
POTENTIAL
RATE
SCALING
LIMITS
EMPIRICAL
MEASUREMENTS
FURTHER
REVEAL
THAT
THE
IDLE
MODE
POWER
DRAW
PI
VARIES
WITH
OPERATING
FREQUENCY
IN
A
MANNER
SIMILAR
TO
THE
ACTIVE
MODE
POWER
DRAW
BUT
WITH
LOWER
ABSOLUTE
VALUE
CORRESPONDINGLY
WE
MODEL
THE
IDLE
MODE
POWER
DRAW
AS
PI
R
C
ΒF
R
INTUITIVELY
THE
PARAMETER
Β
REPRESENTS
THE
RELATIVE
MAGNITUDES
OF
ROUTINE
WORK
INCURRED
EVEN
IN
THE
ABSENCE
OF
PACKETS
TO
THE
WORK
INCURRED
WHEN
ACTIVELY
PROCESSING
PACKETS
WHILE
MEASUREMENTS
FROM
EXISTING
EQUIPMENT
SUGGEST
VALUES
OF
Β
AS
HIGH
AS
FOR
NETWORK
INTERFACE
CARDS
AND
ROUTER
LINECARDS
WE
WOULD
LIKE
TO
CAP
TURE
THE
POTENTIAL
FOR
FUTURE
ENERGY
EFFICIENT
EQUIPMENT
AND
HENCE
CONSIDER
A
WIDE
RANGE
OF
Β
BETWEEN
ENERGY
SAVINGS
FROM
RATE
ADAPTATION
WITH
THE
ABOVE
DEFINITIONS
OF
PA
AND
PI
WE
CAN
NOW
EVALUATE
THE
OVERALL
ENERGY
SAVINGS
DUE
TO
RATE
ADAPTATION
THE
TOTAL
ENERGY
CONSUMPTION
IS
NOW
GIVEN
BY
PA
RK
TA
RK
PI
RK
TI
RK
RK
OUR
EVALUATION
IN
SECTION
YIELDS
THE
VALUES
OF
TA
RK
AND
TI
RK
FOR
DIFFERENT
RK
AND
TEST
SCENARIOS
WHILE
EQNS
AND
ALLOW
US
TO
MODEL
PA
RK
AND
PI
RK
FOR
DIFFERENT
C
Β
AND
F
R
WE
FIRST
EVALUATE
THE
ENERGY
SAVINGS
USING
RATE
ADAPTATION
UNDER
FREQUENCY
SCALING
F
R
O
R
AND
DVS
F
R
O
FOR
THESE
TESTS
WE
SET
C
AND
Β
TO
MIDDLE
OF
THE
RANGE
VALUES
OF
AND
RESPECTIVELY
WE
EXAMINE
THE
EFFECT
OF
VARYING
C
AND
Β
IN
THE
NEXT
SECTION
FIGURE
A
PLOTS
THE
ENERGY
SAVINGS
FOR
OUR
PRACTICAL
PRACTRA
AND
OPTIMAL
LINK
OPTRA
RATE
ADAPTATION
ALGORITHMS
ASSUMING
ONLY
FREQUENCY
SCALING
WE
SEE
THAT
IN
THIS
CASE
THE
RELATIVE
ENERGY
SAVINGS
FOR
THE
DIFFERENT
ALGORITHMS
AS
WELL
AS
THE
IMPACT
OF
THE
DIFFERENT
RATE
DISTRIBUTIONS
IS
SIMILAR
TO
OUR
PREVIOUS
RESULTS
FIG
THAT
MEASURED
SAVINGS
IN
TERMS
OF
THE
AVERAGE
REDUCTION
IN
LINK
RATES
OVERALL
WE
SEE
THAT
SIGNIFICANT
SAVINGS
ARE
POSSIBLE
EVEN
IN
THE
CASE
OF
FREQUENCY
SCALING
ALONE
FIGURE
B
REPEATS
THE
ABOVE
TEST
ASSUMING
VOLTAGE
SCALING
FOR
TWO
DIFFERENT
VALUES
OF
Λ
THE
MAXIMUM
RATE
SCALING
FACTOR
ALLOWED
BY
DVS
IN
THIS
CASE
WE
SEE
THAT
THE
USE
OF
DVS
SIGNIFICANTLY
CHANGES
THE
SAVINGS
CURVE
THE
MORE
AGGRESSIVE
VOLTAGE
SCALING
ALLOWS
FOR
LARGER
SAVINGS
THAT
CAN
BE
MAINTAINED
OVER
A
WIDE
RANGE
OF
UTILIZATIONS
MOREOVER
WE
SEE
THAT
ONCE
AGAIN
THE
SAVINGS
FROM
OUR
PRACTICAL
ALGORITHM
PRACTRA
APPROACH
THOSE
FROM
THE
OPTIMAL
ALGORITHM
FINALLY
AS
EXPECTED
INCREASING
THE
RANGE
OF
SUPPORTED
RATES
Λ
RESULTS
IN
ADDITIONAL
ENERGY
SAVINGS
ENERGY
SAVINGS
FROM
SLEEPING
TO
MODEL
THE
ENERGY
SAVINGS
WITH
SLEEPING
WE
NEED
TO
PIN
DOWN
THE
RELATIVE
MAGNITUDES
OF
THE
SLEEP
MODE
POWER
DRAW
PS
RELATIVE
TO
THAT
WHEN
IDLE
PI
WE
DO
SO
BY
INTRODUCING
A
PARAMETER
Γ
AND
SET
PS
ΓPI
RN
WHERE
Γ
WHILE
THE
VALUE
OF
Γ
WILL
DEPEND
ON
THE
HARDWARE
CHARACTERISTICS
OF
THE
NETWORK
ELEMENT
IN
QUESTION
EMPIRICAL
DATA
SUGGEST
THAT
SLEEP
MODE
POWER
IS
TYPICALLY
A
VERY
SMALL
FRACTION
OF
THE
IDLE
MODE
POWER
CONSUMPTION
FOR
NETWORK
INTERFACES
FOR
RFM
RADIOS
FOR
PC
CARDS
AND
LESS
THAN
FOR
DRAM
MEMORY
IN
OUR
EVALUATION
WE
CONSIDER
VALUES
OF
Γ
BETWEEN
AND
WITH
THIS
THE
ENERGY
CONSUMPTION
OF
AN
ELEMENT
THAT
SPENDS
TIME
TS
IN
SLEEP
IS
GIVEN
BY
E
PA
RN
TA
PI
RN
TI
TS
PSTS
OUR
EVALUATION
FROM
SECTION
ESTIMATED
TS
FOR
DIFFERENT
SCENARIOS
FIGURE
C
PLOTS
THE
CORRESPONDING
OVERALL
ENERGY
SAVINGS
FOR
DIFFERENT
VALUES
OF
Γ
FOR
OUR
PRACTB
B
ALGORITHM
WE
ASSUME
A
TRANSITION
TIME
Δ
AND
A
BUFFERING
INTERVAL
B
AGAIN
OUR
RESULTS
CONFIRM
THAT
SLEEPING
OFFERS
GOOD
OVERALL
ENERGY
SAVINGS
AND
THAT
AS
EXPECTED
ENERGY
SAVINGS
ARE
DIRECTLY
PROPORTIONAL
TO
Γ
COMPARISON
SLEEP
VS
RATE
ADAPTATION
WE
NOW
COMPARE
THE
SAVINGS
FROM
SLEEPING
VS
RATE
ADAPTATION
BY
VARYING
THE
TWO
DEFINING
AXES
OF
OUR
POWER
MODEL
C
THE
PERCENTAGE
OF
POWER
THAT
DOES
NOT
SCALE
WITH
FREQUENCY
AND
Β
THAT
DETERMINES
THE
RELATIVE
MAGNITUDES
OF
IDLE
TO
ACTIVE
POWER
DRAWS
WE
CONSIDER
TWO
END
OF
THE
RANGE
VALUES
FOR
EACH
C
AND
C
AND
Β
AND
Β
COMBINING
THE
TWO
GIVES
US
FOUR
TEST
CASES
THAT
SPAN
THE
SPECTRUM
OF
HARDWARE
POWER
PROFILES
C
AND
Β
CAPTURES
THE
CASE
WHERE
THE
STATIC
PORTION
OF
POWER
CONSUMPTION
THAT
CANNOT
BE
RATE
SCALED
AWAY
IS
LOW
AND
IDLE
MODE
POWER
IS
SIGNIFICANTLY
LOWER
THAN
ACTIVE
MODE
POWER
C
AND
Β
THE
STATIC
PORTION
OF
POWER
CONSUMPTION
IS
LOW
AND
IDLE
MODE
POWER
IS
ALMOST
COMPARABLE
TO
ACTIVE
MODE
POWER
C
AND
Β
THE
STATIC
PORTION
OF
POWER
CONSUMPTION
IS
HIGH
IDLE
MODE
POWER
IS
SIGNIFICANTLY
LOWER
THAN
THAT
IN
ACTIVE
MODE
C
AND
Β
THE
STATIC
PORTION
OF
POWER
CONSUMPTION
IS
HIGH
IDLE
MODE
POWER
IS
ALMOST
COMPARABLE
TO
ACTIVE
MODE
POWER
AVERAGE
UTILIZATION
PRACTRA
FREQ
SCALING
PRACTRA
PRACTRA
AVERAGE
UTILIZATION
PRACTRA
DVS
AVERAGE
UTILIZATION
PRACTB
B
FIGURE
TOTAL
ENERGY
SAVING
WITH
SLEEPING
AND
RATE
ADAPTATION
WE
EVALUATE
ENERGY
SAVINGS
FOR
EACH
OF
THE
ABOVE
SCENARIOS
FOR
THE
CASE
WHERE
THE
HARDWARE
SUPPORTS
DVS
AND
WHEN
THE
HARDWARE
ONLY
SUPPORTS
FREQUENCY
SCALING
WITH
DVS
F
R
O
FIGURES
PLOTS
THE
OVERALL
ENERGY
SAVINGS
FOR
PRACTRA
AND
PRACTB
B
FOR
THE
DIFFERENT
TEST
SCENARIOS
THESE
TESTS
ASSUME
UNIFORMLY
DISTRIBUTED
RATES
AND
A
SLEEP
POWER
PS
RN
IN
EACH
CASE
FOR
BOTH
SLEEP
AND
RATE
ADAPTATION
WE
CONSIDER
HARDWARE
PARAMETERS
THAT
REFLECT
THE
BEST
AND
WORST
CASE
SAVINGS
FOR
THE
ALGORITHM
IN
QUESTION
FOR
UTILIZATION
UTILIZATION
PRACTRA
THESE
PARAMETERS
ARE
Λ
THE
RANGE
FOR
VOLTAGE
SCALING
AND
Δ
THE
TRANSITION
TIME
FOR
THE
BEST
CASE
RESULTS
THESE
ARE
Λ
AND
Δ
FOR
THE
WORST
CASE
Λ
Δ
THE
PARAMETER
FOR
PRACTB
B
IS
THE
TRANSITION
TIME
Δ
WHICH
WE
SET
AS
Δ
BEST
CASE
AND
Δ
WORST
CASE
THE
CONCLUSION
WE
DRAW
FROM
FIGURE
IS
THAT
IN
EACH
SCENARIO
THERE
IS
A
BOUNDARY
UTILIZATION
BELOW
WHICH
SLEEPING
OFFERS
GREATER
SAVINGS
AND
ABOVE
WHICH
RATE
ADAPTATION
IS
PREFERABLE
COMPARING
ACROSS
GRAPHS
WE
SEE
THAT
THE
BOUNDARY
UTILIZATION
DEPENDS
PRIMARILY
ON
THE
VALUES
OF
C
AND
Β
AND
ONLY
SECONDARILY
ON
THE
A
C
Β
UTILIZATION
C
C
Β
B
C
Β
RATE
ADAPTATION
SLEEPING
UTILIZATION
D
C
Β
TRANSITION
TIME
AND
OTHER
HARDWARE
PARAMETERS
OF
THE
ALGORITHM
FOR
EXAMPLE
THE
BOUNDARY
UTILIZATION
FOR
C
AND
Β
VARIES
BETWEEN
APPROXIMATELY
WHILE
AT
C
Β
THIS
BOUNDARY
UTILIZATION
LIES
BETWEEN
AND
WE
ALSO
EVALUATED
SAVINGS
UNDER
DIFFERENT
TRAFFIC
CHARACTERISTICS
CBR
PARETO
AND
FOUND
THAT
THE
BURSTINESS
OF
TRAFFIC
HAS
A
MORE
SECONDARY
EFFECT
ON
THE
BOUNDARY
UTILIZATION
FOR
FURTHER
INSIGHT
ON
WHAT
DETERMINES
THE
BOUNDARY
UTILIZATION
WE
CONSIDER
THE
SCENARIO
OF
A
SINGLE
IDEALIZED
LINK
THE
SLEEP
MODE
ENERGY
CONSUMPTION
OF
SUCH
AN
IDEALIZED
LINK
CAN
BE
VIEWED
AS
ESLEEP
PA
RMAX
ΜT
PS
Μ
T
SIMILARLY
THE
IDEALIZED
LINK
WITH
RATE
ADAPTATION
IS
ONE
THAT
RUNS
WITH
AN
AVERAGE
RATE
OF
ΜRMAX
FOR
AN
ENERGY
CONSUMPTION
OF
ERATE
PA
ΜRMAX
T
FIGURE
COMPARISON
OF
ENERGY
SAVINGS
BETWEEN
SLEEP
AND
RATE
ADAPTATION
SUPPORT
FOR
DYNAMIC
VOLTAGE
SCALING
FIGURE
REPRESENTS
THE
BOUNDARY
UTILIZATION
FOR
THIS
IDEALIZED
LINK
AS
A
FUNCTION
OF
C
IN
THIS
IDEALIZED
SCENARIO
THE
DOMINANT
PARAMETER
IS
C
BECAUSE
THE
LINK
IS
NEVER
IDLE
AND
THEREFORE
Β
HAS
ONLY
A
SMALL
INDIRECT
EFFECT
ON
PS
THE
GRAY
ZONE
IN
THE
FIGURE
REPRESENTS
THE
SPREAD
IN
BOUNDARY
UTILIZATION
OBTAINED
BY
VARYING
Β
BETWEEN
AND
WITH
FREQUENCY
SCALING
ALONE
F
R
O
R
FIG
URES
PLOTS
THE
OVERALL
ENERGY
SAVINGS
FOR
PRACTRA
AND
PRACTB
B
FOR
THE
DIFFERENT
TEST
SCENARIOS
IN
THE
MORE
PESSIMISTIC
SCENARIO
WHERE
VOLTAGE
SCALING
IS
NOT
SUPPORTED
DUE
TO
LACK
OF
SPACE
WE
ONLY
PLOT
THE
COM
PARISON
FOR
THE
FIRST
TWO
TEST
SCENARIOS
WHERE
C
AT
C
THE
SAVINGS
SHOW
A
SIMILAR
SCALING
TREND
BUT
WITH
SIGNIFICANTLY
POORER
PERFORMANCE
FOR
RATE
ADAPTATION
UTILIZATION
FIGURE
SLEEPING
VS
RATE
ADAPTATION
AND
HENCE
ADD
LITTLE
ADDITIONAL
INFORMATION
UTILIZATION
A
C
Β
UTILIZATION
B
C
Β
THE
PRIMARY
OBSERVATION
IS
THAT
THE
SAVINGS
FROM
RATE
ADAPTATION
ARE
SIGNIFICANTLY
LOWER
THAN
IN
THE
PREVIOUS
CASE
WITH
DVS
AND
IN
THIS
CASE
SLEEPING
OUTPERFORMS
RATE
ADAPTATION
MORE
FREQUENTLY
WE
ALSO
SEE
THAT
UNLIKE
THE
DVS
CASE
NETWORK
UTILIZATION
IMPACTS
ENERGY
SAVINGS
IN
A
SIMILAR
MANNER
FOR
BOTH
SLEEPING
AND
RATE
ADAPTATION
I
E
THE
OVERALL
SLOPE
OF
THE
SAVINGS
VS
UTILIZATION
CURVES
IS
SIMILAR
WITH
BOTH
SLEEPING
AND
RATE
ADAPTATION
WHILE
THEY
WERE
DRAMATICALLY
DIFFERENT
WITH
DVS
SEE
FIG
ONCE
AGAIN
WE
OBTAIN
INSIGHT
ON
THIS
BY
STUDYING
THE
THE
HIGHLY
SIMPLIFIED
CASE
OF
A
SINGLE
IDEALIZED
LINK
FOR
THIS
IDEALIZED
SCENARIO
WITH
F
R
O
R
WE
FIND
THAT
THE
BOUNDARY
CONDITION
THAT
DETERMINES
WHETHER
TO
USE
SLEEP
OR
RATE
ADAPTATION
IS
IN
FACT
INDEPENDENT
OF
NETWORK
UTILIZATION
INSTEAD
ONE
CAN
SHOW
THAT
SLEEP
IS
SUPERIOR
TO
RATE
ADAPTATION
IF
THE
FOLLOWING
INEQUALITY
HOLDS
FIGURE
ENERGY
SAVINGS
OF
SLEEP
VS
RATE
ADAPTATION
Β
FREQUENCY
SCALING
ALONE
AND
OS
TECHNIQUES
TO
EXTEND
BATTERY
LIFETIMES
IN
MOBILES
PERHAPS
THE
FIRST
TO
DRAW
ATTENTION
TO
THE
PROBLEM
OF
SAVING
OVERALL
ENERGY
IN
THE
NETWORK
WAS
AN
EARLY
POSITION
PAPER
BY
GUPTA
ET
AL
THEY
USE
DATA
FROM
THE
US
DEPARTMENT
OF
COMMERCE
TO
DETAIL
THE
GROWTH
IN
NETWORK
ENERGY
CONSUMPTION
AND
ARGUE
THE
CASE
FOR
ENERGY
SAVING
NETWORK
PROTOCOLS
INCLUDING
THE
POSSI
BILITY
OF
WAKE
ON
ARRIVAL
IN
WIRED
ROUTERS
IN
FOLLOW
ON
WORK
THEY
EVALUATE
THE
APPLICATION
OF
OPPORTUNISTIC
SLEEPING
IN
A
CAMPUS
LAN
ENVIRONMENT
OTHER
RECENT
WORK
LOOKS
AT
POWERING
DOWN
REDUNDANT
ACCESS
POINTS
APS
IN
ENTERPRISE
WIRELESS
NETWORKS
THE
AUTHORS
PROPOSE
THAT
A
CENTRAL
SERVER
COLLECT
AP
ΓΒ
C
Γ
Β
OTHERWISE
RATE
ADAPTATION
IS
SUPERIOR
CONNECTIVITY
AND
UTILIZATION
INFORMATION
TO
DETERMINE
WHICH
APS
CAN
BE
SAFELY
POWERED
DOWN
THIS
APPROACH
IS
LESS
APPLICABLE
TO
WIRED
NETWORKS
THAT
EXHIBIT
MUCH
LESS
REDUNDANCY
IN
PRACTICE
NETWORK
UTILIZATION
DOES
PLAY
A
ROLE
AS
OUR
RESULTS
CLEARLY
INDICATE
BECAUSE
THE
VARIOUS
PRACTICAL
CONSTRAINTS
DUE
TO
DELAY
BOUNDS
AND
TRANSITION
TIMES
PREVENT
OUR
ALGORITHMS
FROM
FULLY
EXPLOITING
ALL
OPPORTUNITIES
TO
SLEEP
OR
CHANGE
RATES
IN
SUMMARY
WE
FIND
THAT
BOTH
SLEEPING
AND
RATE
ADAPTATION
ARE
USEFUL
WITH
THE
TRADEOFF
BETWEEN
THEM
DEPENDING
PRIMARILY
ON
THE
POWER
PROFILE
OF
HARDWARE
CA
PABILITIES
AND
NETWORK
UTILIZATION
RESULTS
SUCH
AS
THOSE
PRESENTED
HERE
CAN
GUIDE
OPERATORS
IN
DECIDING
HOW
TO
BEST
RUN
THEIR
NETWORKS
FOR
EXAMPLE
AN
OPERATOR
MIGHT
CHOOSE
TO
RUN
THE
NETWORK
WITH
RATE
ADAPTATION
DURING
THE
DAY
AND
SLEEPING
AT
NIGHT
BASED
ON
WHERE
THE
BOUNDARY
UTILIZATION
INTERSECTS
DIURNAL
BEHAVIOR
OR
IDENTIFY
COM
PONENTS
OF
THE
NETWORK
WITH
CONSISTENTLY
LOW
OR
HIGH
UTILIZATION
TO
BE
RUN
WITH
SLEEPING
OR
RATE
ADAPTATION
RELATED
WORK
THERE
IS
A
LARGE
BODY
OF
WORK
ON
POWER
MANAGEMENT
IN
CONTEXTS
COMPLEMENTARY
TO
OURS
THIS
INCLUDES
POWER
PROVISIONING
AND
LOAD
BALANCING
IN
DATA
CENTERS
SLEEPING
HAS
ALSO
BEEN
EXPLORED
IN
THE
CONTEXT
OF
TO
SAVE
CLIENT
POWER
E
G
SEE
THE
STANDARD
ITSELF
INCLUDES
TWO
SCHEMES
POWER
SAVE
POLL
AND
AUTOMATIC
POWER
SAVE
DELIVERY
BY
WHICH
ACCESS
POINTS
MAY
BUFFER
PACKETS
SO
THAT
CLIENTS
MAY
SLEEP
FOR
SHORT
INTERVALS
IN
SOME
SENSE
OUR
PROPOSAL
FOR
BUNCHING
TRAFFIC
TO
IMPROVE
SLEEP
OPPORTUNITIES
CAN
BE
VIEWED
AS
EXTENDING
THIS
IDEA
DEEP
INTO
THE
NETWORK
FINALLY
THE
IEEE
ENERGY
EFFICIENT
ETHERNET
TASK
FORCE
HAS
RECENTLY
STARTED
TO
EXPLORE
BOTH
SLEEPING
AND
RATE
ADAPTATION
FOR
ENERGY
SAVINGS
SOME
INITIAL
STUDIES
CONSIDER
INDIVIDUAL
LINKS
AND
ARE
BASED
ON
SYNTHETIC
TRAFFIC
AND
INFINITE
BUFFERS
IN
THE
DOMAIN
OF
SENSOR
NETWORKS
THERE
HAVE
BEEN
NUMEROUS
EFFORTS
TO
DESIGN
ENERGY
EFFICIENT
PROTOCOLS
APPROACHES
INVESTIGATED
INCLUDE
PUTTING
NODES
TO
SLEEP
USING
TDMA
LIKE
TECHNIQUES
TO
COORDINATE
TRANSMIS
SION
AND
IDLE
TIMES
E
G
FPS
AND
DISTRIBUTED
ALGORITHMS
FOR
SLEEPING
E
G
MAC
THIS
CONTEXT
DIFFERS
FROM
OURS
IN
MANY
WAYS
CONCLUSION
WE
HAVE
ARGUED
THAT
POWER
MANAGEMENT
STATES
THAT
SLOW
DOWN
LINKS
AND
PUT
COMPONENTS
TO
SLEEP
STAND
TO
SAVE
MUCH
OF
THE
PRESENT
ENERGY
EXPENDITURE
OF
NETWORKS
AT
A
HIGH
LEVEL
THIS
IS
APPARENT
FROM
THE
FACTS
THAT
WHILE
NETWORK
ENERGY
CONSUMPTION
IS
GROWING
NETWORKS
CONTINUE
TO
OPERATE
AT
LOW
AVERAGE
UTILIZATIONS
WE
PRESENT
THE
DESIGN
AND
EVALUATION
OF
SIMPLE
POWER
MAN
AGEMENT
ALGORITHMS
THAT
EXPLOIT
THESE
STATES
FOR
ENERGY
CONSERVATION
AND
SHOW
THAT
WITH
THE
RIGHT
HARDWARE
SUPPORT
THERE
IS
THE
POTENTIAL
FOR
SAVING
MUCH
ENERGY
WITH
A
SMALL
AND
BOUNDED
IMPACT
ON
PERFORMANCE
E
G
A
FEW
MILLISECONDS
OF
DELAY
WE
HOPE
THESE
PRELIMINARY
RESULTS
WILL
ENCOURAGE
THE
DEVELOPMENT
OF
HARDWARE
SUPPORT
FOR
POWER
SAVING
AS
WELL
AS
ALGORITHMS
THAT
USE
THEM
MORE
EFFECTIVELY
TO
REALIZE
GREATER
SAVINGS
PEAK
SHAVING
THROUGH
RESOURCE
BUFFERING
AMOTZ
BAR
NOY
MATTHEW
P
JOHNSON
OU
LIU
THE
GRADUATE
CENTER
OF
THE
CITY
UNIVERSITY
OF
NEW
YORK
ABSTRACT
WE
INTRODUCE
AND
SOLVE
A
NEW
PROBLEM
INSPIRED
BY
ENERGY
PRICING
SCHEMES
IN
WHICH
A
CLIENT
IS
BILLED
FOR
PEAK
USAGE
AT
EACH
TIMESLOT
THE
SYSTEM
MEETS
AN
ENERGY
DEMAND
THROUGH
A
COMBINATION
OF
A
NEW
REQUEST
AN
UNRELIABLE
AMOUNT
OF
FREE
SOURCE
ENERGY
E
G
SOLAR
OR
WIND
POWER
AND
PREVIOUSLY
RECEIVED
ENERGY
THE
ADDED
PIECE
OF
INFRASTRUCTURE
IS
THE
BATTERY
WHICH
CAN
STORE
SUR
PLUS
ENERGY
FOR
FUTURE
USE
MORE
GENERALLY
THE
DEMANDS
COULD
REPRESENT
REQUIRED
AMOUNTS
OF
ENERGY
WATER
OR
ANY
OTHER
TENABLE
RESOURCE
WHICH
CAN
BE
OBTAINED
IN
ADVANCE
AND
HELD
UNTIL
NEEDED
IN
A
FEASIBLE
SOLUTION
EACH
DEMAND
MUST
BE
SUPPLIED
ON
TIME
THROUGH
A
COMBINATION
OF
NEWLY
REQUESTED
ENERGY
ENERGY
WITH
DRAWN
FROM
THE
BATTERY
AND
FREE
SOURCE
THE
GOAL
IS
TO
MINIMIZE
THE
MAXIMUM
REQUEST
IN
THE
ONLINE
VERSION
OF
THIS
PROBLEM
THE
ALGORITHM
MUST
DETERMINE
EACH
REQUEST
WITHOUT
KNOWLEDGE
OF
FUTURE
DEMANDS
OR
FREE
SOURCE
AVAILABILITY
WITH
THE
GOAL
OF
MAXIMIZING
THE
AMOUNT
BY
WHICH
THE
PEAK
IS
REDUCED
WE
GIVE
EFFICIENT
OPTIMAL
ALGORITHMS
FOR
THE
OFFLINE
PROBLEM
WITH
AND
WITHOUT
A
BOUNDED
BATTERY
WE
ALSO
SHOW
HOW
TO
FIND
THE
OPTIMAL
OFFLINE
BATTERY
SIZE
GIVEN
THE
REQUIREMENT
THAT
THE
FINAL
BATTERY
LEVEL
EQUALS
THE
INITIAL
BATTERY
LEVEL
FINALLY
WE
GIVE
EFFI
CIENT
HN
COMPETITIVE
ALGORITHMS
ASSUMING
THE
PEAK
EFFECTIVE
DEMAND
IS
REVEALED
IN
ADVANCE
AND
PROVIDE
MATCHING
LOWER
BOUNDS
INTRODUCTION
THERE
IS
INCREASING
INTEREST
IN
SAVING
FUEL
COSTS
BY
USE
OF
RENEWABLE
ENERGY
SOURCES
SUCH
AS
WIND
AND
SOLAR
POWER
ALTHOUGH
SUCH
SOURCES
ARE
HIGHLY
DESIRABLE
AND
THE
POWER
THEY
PROVIDE
IS
IN
A
SENSE
FREE
THE
TYPICAL
DISADVANTAGE
IS
UNRELIABILITY
AVAILABILITY
DEPENDS
E
G
ON
WEATHER
CONDITIONS
IT
IS
NOT
DISPATCHABLE
ON
DEMAND
MANY
COMPANIES
SEEK
TO
BUILD
EFFICIENT
SYSTEMS
TO
GATHER
SUCH
ENERGY
WHEN
AVAILABLE
AND
STORE
IT
PERHAPS
IN
MODIFIED
FORM
FOR
FUTURE
USE
ON
THE
OTHER
HAND
POWER
COMPANIES
CHARGE
SOME
HIGH
CONSUMPTION
CLIENTS
NOT
JUST
FOR
THE
TOTAL
AMOUNT
OF
POWER
CONSUMED
BUT
ALSO
FOR
HOW
QUICKLY
THEY
CONSUME
IT
WITHIN
THE
BILLING
PERIOD
TYPICALLY
A
MONTH
THE
CLIENT
IS
CHARGED
FOR
THE
AMOUNT
OF
ENERGY
USED
USAGE
CHARGE
IN
KWH
AND
FOR
THE
MAXIMUM
AMOUNT
REQUESTED
OVER
TIME
PEAK
CHARGE
IN
KW
IF
DEMANDS
ARE
GIVEN
AS
A
SEQUENCE
DN
THEN
THE
TOTAL
BILL
IS
OF
THE
FORM
I
DI
MAXI
DI
FOR
SOME
CONSTANTS
I
E
A
WEIGHTED
SUM
OF
THE
TOTAL
USAGE
AND
THE
MAXIMUM
USAGE
IN
PRACTICE
THE
DIS
CRETE
TIMESLOTS
MAY
BE
MINUTE
AVERAGES
THIS
MEANS
THAT
A
CLIENT
WHO
POWERS
A
THIS
WORK
WAS
SUPPORTED
BY
GRANTS
FROM
THE
NATIONAL
SCIENCE
FOUNDATION
AND
THE
NEW
YORK
STATE
OFFICE
OF
SCIENCE
TECHNOLOGY
AND
ACADEMIC
RESEARCH
PIECE
OF
MACHINERY
FOR
ONE
HOUR
AND
THEN
USES
NO
MORE
ENERGY
FOR
THE
REST
OF
THE
MONTH
WOULD
BE
CHARGED
MORE
THAN
A
CLIENT
WHO
USES
A
TOTAL
OF
SPREAD
EVENLY
OVER
THE
COURSE
OF
THE
MONTH
SINCE
THE
PER
UNIT
COST
FOR
PEAK
CHARGES
MAY
BE
ON
THE
ORDER
OF
TIMES
THE
PER
UNIT
COST
FOR
TOTAL
USAGE
THIS
DIFFERENCE
CAN
BE
SIGNIFICANT
THIS
SUGGESTS
A
SECOND
USE
FOR
THE
BATTERY
TO
STORE
PURCHASED
ENERGY
FOR
FUTURE
USE
INDEED
AT
LEAST
ONE
START
UP
COMPANY
IS
CURRENTLY
MARKETING
SUCH
A
BATTERY
BASED
SYSTEM
INTENDED
TO
REDUCE
PEAK
ENERGY
CHARGES
IN
SUCH
A
SYSTEM
A
BATTERY
IS
PLACED
BETWEEN
THE
POWER
COMPANY
AND
A
HIGH
CONSUMPTION
CLIENT
SITE
IN
ORDER
TO
SMOOTH
POWER
REQUESTS
AND
SHAVE
THE
PEAK
THE
CLIENT
SITE
WILL
CHARGE
TO
THE
BATTERY
WHEN
DEMAND
IS
LOW
AND
DISCHARGE
WHEN
DEMAND
IS
HIGH
SPIKES
IN
THE
DEMAND
CURVE
CAN
THUS
BE
RENDERED
CONSISTENT
WITH
A
RELATIVELY
FLAT
LEVEL
OF
SUPPLIED
POWER
THE
RESULT
IS
A
LOWER
COST
FOR
THE
CLIENT
AND
A
MORE
MANAGEABLE
REQUEST
CURVE
FOR
THE
PROVIDER
WE
MAY
GENERALIZE
THIS
PROBLEM
OF
MINIMAXING
THE
REQUEST
TO
ANY
RESOURCE
WHICH
IS
TENABLE
IN
THE
SENSE
THAT
IT
MAY
BE
OBTAINED
EARLY
AND
STORED
UNTIL
NEEDED
FOR
EX
AMPLE
COMPANIES
FREQUENTLY
FACE
SHORTAGES
OF
POPULAR
PRODUCTS
PLENTIFUL
SUPPLY
OF
XBOXES
WOULD
BE
POSSIBLE
ONLY
IF
MICROSOFT
MADE
MILLIONS
OF
CONSOLES
IN
ADVANCE
AND
STORED
THEM
WITHOUT
RELEASING
THEM
OR
IF
IT
BUILT
VAST
PRODUCTION
LINES
THAT
ONLY
RAN
FOR
A
FEW
WEEKS
BOTH
ECONOMICALLY
UNWISE
STRATEGIES
A
RECENT
NEWS
STORY
ASSERTED
A
PRODUCER
COULD
SMOOTH
THE
PRODUCT
PRODUCTION
CURVE
BY
INCREASING
PRODUCTION
AND
WAREHOUSING
SUPPLY
UNTIL
FUTURE
SALES
BUT
WHEN
SHOULD
THE
PRODUCER
CHARGE
AND
DIS
CHARGE
IN
SOME
DOMAINS
THERE
MAY
ALSO
BE
AN
UNPREDICTABLE
LEVEL
OF
VOLUNTEER
HELP
A
THIRD
APPLICATION
IS
THE
SCHEDULING
OF
JOBS
COMPOSED
OF
GENERIC
WORK
UNITS
THAT
MAY
BE
DONE
IN
ADVANCE
ALTHOUGH
THE
PROBLEM
IS
VERY
GENERAL
WE
WILL
USE
THE
LANGUAGE
OF
ENERGY
AND
BATTERIES
FOR
CONCRETENESS
IN
THE
ONLINE
VERSION
OF
OUR
PROBLEM
THE
ESSENTIAL
CHOICE
FACED
AT
EACH
TIMESLOT
IS
WHETHER
AND
BY
HOW
MUCH
TO
INVEST
IN
THE
FUTURE
OR
TO
CASH
IN
A
PRIOR
INVESTMENT
THE
INVESTMENT
IN
OUR
SETTING
IS
A
REQUEST
FOR
MORE
ENERGY
THAN
IS
NEEDED
AT
THE
TIME
IF
THE
ALGORITHM
ONLY
ASKS
FOR
THE
MINIMUM
REQUIRED
THEN
IT
IS
VULNERABLE
TO
SPIKES
IN
DEMAND
IF
IT
ASKS
FOR
MUCH
MORE
ENERGY
THAN
IT
NEEDS
THEN
THE
GREATER
REQUEST
COULD
ITSELF
INTRODUCE
A
NEW
HIGHER
PEAK
THE
STRICTNESS
OF
THE
PROBLEM
LIES
IN
THE
FACT
THAT
THE
COST
IS
NOT
CUMULATIVE
WE
WANT
EVERY
REQUEST
TO
BE
LOW
BACKGROUND
EXPERIMENTAL
WORK
APPLYING
VARIATIONS
OF
THESE
ONLINE
ALGORITHMS
TO
SET
TINGS
LACKING
PROVABLE
GUARANTEES
WAS
RECENTLY
PRESENTED
THE
PRESENT
PAPER
FOCUSES
ON
SETTINGS
THAT
ALLOW
GUARANTEED
COMPETITIVENESS
THERE
IS
A
WIDE
LITERATURE
ON
COMMODITY
PRODUCTION
STORAGE
WAREHOUSING
AND
SUPPLY
CHAIN
MANAGEMENT
SEE
E
G
MORE
SPECIFICALLY
THERE
ARE
A
NUM
BER
OF
INVENTORY
PROBLEMS
BASED
ON
THE
ECONOMIC
LOT
SIZING
MODEL
IN
WHICH
DE
MAND
LEVELS
FOR
A
PRODUCT
VARY
OVER
A
DISCRETE
FINITE
TIME
HORIZON
AND
ARE
KNOWN
IN
ADVANCE
A
FEASIBLE
SOLUTION
IN
THESE
PROBLEMS
MUST
OBTAIN
SUFFICIENT
SUPPLY
THROUGH
PRODUCTION
SOMETIMES
CONSTRUED
AS
ORDERING
OR
THROUGH
OTHER
METHODS
IN
ORDER
TO
MEET
EACH
OF
THE
DEMANDS
ON
TIME
WHILE
OBSERVING
CERTAIN
CONSTRAINTS
THE
NATURE
OF
SOLUTION
QUALITY
VARIES
BY
FORMULATION
ONE
SUCH
INVENTORY
PROBLEM
IS
SINGLE
ITEM
LOT
SIZING
IN
WHICH
SUFFICIENT
SUPPLIES
MUST
BE
ORDERED
TO
SATISFY
EACH
DEMAND
WHILE
MINIMIZING
THE
TOTAL
COST
OF
ORDERING
CHARGES
AND
HOLDING
CHARGES
THE
ORDERING
CHARGE
CONSISTS
OF
A
FIXED
CHARGE
PER
OR
DER
PLUS
A
CHARGE
LINEAR
IN
ORDER
SIZE
THE
HOLDING
CHARGE
FOR
INVENTORY
IS
PER
UNIT
AND
PER
TIMESLOT
THERE
IS
A
TRADEOFF
BETWEEN
THESE
INCENTIVES
SINCE
FIXED
ORDERING
CHARGES
ENCOURAGE
LARGE
ORDERS
WHILE
HOLDING
CHARGES
DISCOURAGE
THEM
WAGNER
WHITIN
SHOWED
IN
THAT
THIS
PROBLEM
CAN
BE
SOLVED
IN
POLYNOMIAL
TIME
UNDER
THE
AS
SUMPTION
OF
NON
SPECULATIVE
COSTS
IN
WHICH
CASE
ORDERS
SHOULD
ALWAYS
BE
PLACED
AS
LATE
AS
POSSIBLE
THE
PROBLEM
CAN
BE
SOLVED
IN
LINEAR
TIME
SUCH
SPECULATIVE
BEHAVIOR
HOWEVER
IS
THE
VERY
MOTIVATION
OF
OUR
PROBLEM
THERE
ARE
MANY
LOT
SIZING
VARIATIONS
INCLUDING
CONSTANT
CAPACITY
MODELS
THAT
LIMIT
THE
AMOUNT
ORDERED
PER
TIMESLOT
SEE
AND
REFERENCES
THEREIN
OUR
OFFLINE
PROBLEM
DIFFERS
IN
THAT
OUR
OBJECTIVE
IS
MINIMIZING
THIS
CONSTANT
CAPACITY
FOR
ORDERS
SUBJECT
TO
A
BOUND
ON
INVENTORY
SIZE
AND
WE
HAVE
NO
INVENTORY
CHARGE
ANOTHER
RELATED
INVENTORY
PROBLEM
IS
CAPACITY
AND
SUBCONTRACTING
WITH
INVENTORY
CSI
WHICH
INCORPORATES
TRADE
OFFS
BETWEEN
PRODUCTION
COSTS
SUBCONTRACTING
COSTS
HOLDING
COSTS
AND
THE
COST
FOR
MAXIMUM
PER
UNIT
TIMESLOT
PRODUCTION
CAPACITY
THE
GOAL
IN
THAT
PROBLEM
IS
TO
CHOOSE
A
PRODUCTION
CAPACITY
AND
A
FEASIBLE
PRODUCTION
SUB
CONTRACTING
SCHEDULE
THAT
TOGETHER
MINIMIZE
TOTAL
COST
WHEREAS
IN
OUR
PROBLEM
CHOOSING
A
PRODUCTION
CAPACITY
SUBJECT
TO
STORAGE
CONSTRAINTS
IS
THE
ESSENTIAL
TASK
IN
THE
MINIMAX
WORK
SCHEDULING
PROBLEM
THE
GOAL
IS
TO
MINIMIZE
THE
MAXI
MUM
AMOUNT
OF
WORK
DONE
IN
ANY
TIMESLOT
OVER
A
FINITE
TIME
HORIZON
OUR
ONLINE
PROB
LEM
IS
RELATED
TO
A
PREVIOUSLY
STUDIED
SPECIAL
CASE
IN
WHICH
JOBS
WITH
DEADLINES
ARE
ASSIGNED
ONLINE
IN
THAT
PROBLEM
ALL
WORK
MUST
BE
DONE
BY
DEADLINE
BUT
CANNOT
BE
BE
GUN
UNTIL
ASSIGNED
SUBJECT
TO
THESE
RESTRICTIONS
THE
GOAL
IS
TO
MINIMIZE
THE
MAXIMUM
WORK
DONE
IN
ANY
TIMESLOT
WHILE
THE
OPTIMIZATION
GOAL
IS
THE
SAME
OUR
ONLINE
PROB
LEM
DIFFERS
IN
TWO
RESPECTS
FIRST
EACH
JOB
FOR
US
IS
DUE
IMMEDIATELY
WHEN
ASSIGNED
SECOND
WE
ARE
ALLOWED
TO
DO
WORK
REQUEST
AND
STORE
ENERGY
IN
ADVANCE
ONE
ONLINE
ALGORITHM
FOR
THE
JOBS
BY
DEADLINES
PROBLEM
IS
THE
Α
POLICY
AT
EACH
TIMESLOT
THE
AMOUNT
OF
WORK
DONE
IS
Α
TIMES
THE
MAXIMUM
PER
UNIT
TIMESLOT
AMOUNT
OF
WORK
THAT
OPT
WOULD
HAVE
DONE
WHEN
RUNNING
ON
THE
PARTIAL
INPUT
RECEIVED
SO
FAR
ONE
OF
OUR
ONLINE
ALGORITHMS
ADOPTS
A
SIMILAR
STRATEGY
CONTRIBUTIONS
WE
INTRODUCE
A
NOVEL
SCHEDULING
PROBLEM
AND
SOLVE
SEVERAL
VERSIONS
OPTIMALLY
WITH
EFFICIENT
COMBINATORIAL
ALGORITHMS
WE
SOLVE
THE
OFFLINE
PROBLEM
FOR
TWO
KINDS
OF
BATTERIES
UNBOUNDED
BATTERY
IN
O
N
TIME
AND
BOUNDED
IN
O
SEPARATELY
WE
SHOW
HOW
TO
FIND
THE
OPTIMAL
OFFLINE
BATTERY
SIZE
FOR
THE
SETTING
IN
WHICH
THE
FINAL
BATTERY
LEVEL
MUST
EQUAL
THE
INITIAL
BATTERY
LEVEL
THIS
IS
THE
SMALLEST
BATTERY
SIZE
THAT
ACHIEVES
THE
OPTIMAL
PEAK
THE
ONLINE
PROBLEM
WE
STUDY
IS
VERY
STRICT
A
META
STRATEGY
IN
MANY
ONLINE
PROBLEMS
IS
TO
BALANCE
EXPENSIVE
PERIODS
WITH
CHEAP
ONES
SO
THAT
THE
OVERALL
COST
STAYS
LOW
THE
DIFFICULTY
IN
OUR
PROBLEM
LIES
IN
ITS
NON
CUMULATIVE
NATURE
WE
OPTIMIZE
FOR
THE
MAX
NOT
FOR
THE
AVERAGE
WE
SHOW
THAT
SEVERAL
VERSIONS
OF
THE
ONLINE
PROBLEM
HAVE
NO
ALGORITHM
WITH
NON
TRIVIAL
COMPETITIVE
RATIO
I
E
BETTER
THAN
N
OR
Ω
N
GIVEN
ADVANCED
KNOWLEDGE
OF
THE
PEAK
DEMAND
D
HOWEVER
WE
GIVE
HN
COMPETITIVE
ALGORITHMS
FOR
BATTERIES
BOUNDED
AND
UNBOUNDED
OUR
FASTEST
ALGORITHM
HAS
O
PER
SLOT
RUNNING
TIME
HN
IS
THE
OPTIMAL
COMPETITIVE
RATIO
FOR
BOTH
BATTERY
SETTINGS
EXAMPLES
ALTHOUGH
THERE
IS
NO
CONSTANT
RATIO
COMPETITIVE
ALGORITHM
FOR
UNBOUNDED
N
OUR
INTENDED
APPLICATION
IN
FACT
PRESUMES
A
FIXED
TIME
HORIZON
IF
THE
BILLING
PERIOD
IS
ONE
MONTH
AND
PEAK
CHARGES
ARE
COMPUTED
AS
MINUTE
AVERAGES
THEN
FOR
THIS
SETTING
HN
IS
APPROXIMATELY
IF
WE
ASSUME
THAT
THE
BATTERY
CAN
FULLY
RECHARGE
AT
NIGHT
SO
THAT
EACH
DAY
CAN
BE
TREATED
AS
A
SEPARATE
TIME
PERIOD
THEN
FOR
A
HOUR
DAYTIME
TIME
HORIZON
HN
IS
APPROXIMATELY
MODEL
AND
PRELIMINARIES
DEFINITION
AT
EACH
TIMESLOT
I
DI
IS
THE
DEMAND
RI
IS
THE
REQUEST
BI
IS
THE
BATTERY
CHARGE
LEVEL
AT
THE
START
OF
THE
TIMESLOT
AND
FI
IS
THE
AMOUNT
OF
FREE
SOURCE
AVAILABLE
BY
DˆI
WE
INDICATE
THE
EFFECTIVE
DEMAND
DI
FI
WE
SOMETIMES
REFER
TO
THE
SEQUENCE
OVER
TIME
OF
ONE
OF
THESE
VALUE
TYPES
AS
A
CURVE
E
G
THE
DEMAND
CURVE
D
IS
THE
MAXIMUM
EFFECTIVE
DEMAND
MAXI
DˆI
AND
R
IS
THE
MAXIMUM
REQUEST
MAXI
RI
THE
PROBLEM
INSTANCE
COMPRISES
THE
DEMANDS
THE
FREE
SOURCE
CURVE
BATTERY
SIZE
B
INITIAL
CHARGE
AND
REQUIRED
FINAL
CHARGE
BN
IN
THE
OFFLINE
CASE
THE
PROBLEM
SOLUTION
CONSISTS
OF
THE
REQUEST
CURVE
DEFINITION
LET
OVERFLOW
BE
THE
SITUATION
IN
WHICH
RI
FI
DI
B
BI
I
E
THERE
IS
NOT
ENOUGH
ROOM
IN
THE
BATTERY
FOR
THE
AMOUNT
WE
WANT
TO
CHARGE
LET
UNDERFLOW
BE
THE
SITUATION
IN
WHICH
DI
RI
FI
BI
I
E
THERE
IS
NOT
ENOUGH
ENERGY
IN
THE
BATTERY
FOR
THE
AMOUNT
WE
WANT
TO
DISCHARGE
CALL
AN
ALGORITHM
FEASIBLE
IF
UNDERFLOW
NEVER
OCCURS
THE
GOAL
OF
THE
PROBLEM
IS
TO
MINIMIZE
R
FOR
COMPETITIVENESS
MEASURES
THIS
IS
CONSTRUED
AS
MAXIMIZING
D
R
WHILE
MAINTAINING
FEASIBILITY
IN
THE
ABSENCE
OF
OVERFLOW
UNDERFLOW
THE
BATTERY
LEVEL
AT
TIMESLOT
I
IS
SIMPLY
BI
BI
RI
FI
DI
IT
IS
FORBIDDEN
FOR
BI
TO
EVER
FALL
BELOW
THAT
IS
THE
REQUEST
RI
THE
FREE
SOURCE
FI
AND
THE
BATTERY
LEVEL
BI
MUST
SUM
TO
AT
LEAST
THE
DEMAND
DI
AT
EACH
TIMESLOT
I
NOTICE
THAT
EFFECTIVE
DEMAND
CAN
BE
NEGATIVE
WHICH
MEANS
THAT
THE
BATTERY
MAY
BE
CHARGED
CAPACITY
ALLOWING
EVEN
IF
THE
REQUEST
IS
WE
ASSUME
D
HOWEVER
IS
STRICTLY
POSITIVE
OTHERWISE
THE
PROBLEM
INSTANCE
IS
ESSENTIALLY
TRIVIAL
WE
USE
THE
FOLLOWING
TO
SIMPLIFY
THE
PROBLEM
STATEMENT
OBSERVATION
IF
EFFECTIVE
DEMANDS
MAY
BE
NEGATIVE
THEN
FREE
SOURCE
ENERGY
NEED
NOT
BE
EXPLICITLY
CONSIDERED
AS
SUCH
WE
SET
ASIDE
THE
NOTION
OF
FREE
SOURCE
AND
FOR
THE
REMAINDER
OF
THE
PAPER
SIMPLIFYING
NOTATION
ALLOW
DEMAND
DI
TO
BE
NEGATIVE
IN
THE
ENERGY
APPLICATION
BATTERY
CAPACITY
IS
MEASURED
IN
KWH
WHILE
INSTANTANEOUS
REQUEST
IS
MEASURED
IN
KW
BY
DISCRETIZING
WE
ASSUME
WLOG
THAT
BATTERY
LEVEL
DEMAND
AND
REQUEST
VALUES
ARE
EXPRESSED
IN
COMMON
UNITS
PEAK
CHARGES
ARE
BASED
LINEARLY
ON
THE
MAX
REQUEST
WHICH
IS
WHAT
WE
OPTIMIZE
FOR
THE
BATTERY
CAN
HAVE
A
MAXIMUM
CAPAC
ITY
B
OR
BE
UNBOUNDED
THE
PROBLEM
MAY
BE
ONLINE
OFFLINE
OR
IN
BETWEEN
WE
CONSIDER
THE
SETTING
IN
WHICH
THE
PEAK
DEMAND
D
IS
REVEALED
IN
ADVANCE
PERHAPS
PREDICTED
FROM
HISTORICAL
INFORMATION
THRESHOLD
ALGORITHMS
FOR
A
PARTICULAR
SNAPSHOT
DI
RI
BI
DEMAND
DI
MUST
BE
SUP
PLIED
THROUGH
A
COMBINATION
OF
THE
REQUEST
RI
AND
A
CHANGE
IN
BATTERY
BI
BI
THIS
MEANS
THAT
THERE
ARE
ONLY
THREE
POSSIBLE
MODES
FOR
EACH
TIMESLOT
REQUEST
EXACTLY
THE
DEMAND
FREE
REQUEST
MORE
THAN
THIS
AND
CHARGE
THE
DIFFERENCE
OR
REQUEST
LESS
AND
DIS
CHARGE
THE
DIFFERENCE
WE
REFER
TO
OUR
ALGORITHMS
AS
THRESHOLD
ALGORITHMS
LET
TN
BE
A
SEQUENCE
OF
VALUES
THEN
THE
FOLLOWING
ALGORITHM
USES
THESE
AS
REQUEST
THRESHOLDS
INTUITIVELY
THE
ALGORITHM
AMOUNTS
TO
THE
RULE
AT
EACH
TIMESLOT
I
REQUEST
AN
AMOUNT
AS
NEAR
TO
TI
AS
THE
BATTERY
CONSTRAINTS
WILL
ALLOW
OUR
OFFLINE
ALGORITHMS
ARE
CONSTANT
THRESHOLD
ALGORITHMS
WITH
A
FIXED
T
OUR
ONLINE
ALGORITHMS
COMPUTE
TI
DYNAMICALLY
FOR
EACH
TIMESLOT
I
A
CONSTANT
THRESHOLD
ALGORITHM
IS
SPECIFIABLE
BY
A
SINGLE
NUMBER
IN
THE
ONLINE
SET
TING
PREDICTING
THE
EXACT
OPTIMAL
THRESHOLD
FROM
HISTORICAL
DATA
SUFFICES
TO
SOLVE
THE
ONLINE
ALGORITHM
OPTIMALLY
A
SMALL
OVERESTIMATE
OF
THE
THRESHOLD
WILL
MERELY
RAISE
THE
PEAK
COST
CORRESPONDINGLY
HIGHER
UNFORTUNATELY
HOWEVER
EXAMPLES
CAN
BE
FOUND
IN
WHICH
EVEN
A
SMALL
UNDERESTIMATE
EVENTUALLY
DEPLETES
THE
BATTERY
BEFORE
PEAK
DEMAND
AND
THUS
PRODUCE
NO
COST
SAVINGS
AT
ALL
THE
OFFLINE
PROBLEM
CAN
BE
SOLVED
APPROXIMATELY
WITHIN
ADDITIVE
ERROR
E
THROUGH
BINARY
SEARCH
FOR
THE
MINIMUM
FEASIBLE
CONSTANT
THRESHOLD
VALUE
T
SIMPLY
SEARCH
THE
RANGE
D
FOR
THE
LARGEST
VALUE
T
FOR
WHICH
THE
THRESHOLD
ALGORITHM
HAS
NO
UNDERFLOW
IN
TIME
O
N
LOG
D
IF
THE
OPTIMAL
PEAK
REDUCTION
IS
R
T
THEN
THE
ALGORITHM
PEAK
REDUCTION
WILL
BE
AT
LEAST
R
T
E
IT
IS
STRAIGHTFORWARD
TO
GIVE
A
LINEAR
PROGRAMMING
FORMULATION
OF
THE
OFFLINE
PROBLEM
IT
CAN
ALSO
BE
SOLVED
BY
GENERALIZED
PARAMETRIC
MAX
FLOW
OUR
INTEREST
HERE
HOWEVER
IS
IN
EFFICIENT
COMBINATORIAL
OPTIMAL
ALGORITHMS
INDEED
OUR
COMBINATORIAL
OFFLINE
ALGORITHMS
ARE
SIGNIFICANTLY
FASTER
THAN
THESE
GENERAL
TECHNIQUES
AND
LEAD
NATURALLY
TO
OUR
COMPETITIVE
ONLINE
ALGORITHMS
ONLINE
ALGORITHMS
BASED
ON
SUCH
GENERAL
TECHNIQUES
WOULD
BE
INTRACTABLE
FOR
FINE
GRAIN
TIMESLOTS
OFFLINE
PROBLEM
WE
NOW
FIND
OPTIMAL
ALGORITHMS
FOR
BOTH
BATTERY
SETTINGS
FOR
UNBOUNDED
WE
ASSUME
THE
BATTERY
STARTS
EMPTY
FOR
BOUNDED
WE
ASSUME
THE
BATTERY
STARTS
WITH
AMOUNT
B
FOR
BOTH
THE
FINAL
BATTERY
LEVEL
IS
UNSPECIFIED
WE
SHOW
BELOW
THAT
THESE
ASSUMPTIONS
ARE
MADE
WLOG
THE
TWO
OFFLINE
THRESHOLD
FUNCTIONS
SHOWN
IN
TABLE
USE
THE
FOLLOWING
DEFINITION
DEFINITION
LET
Μ
J
J
DT
BE
THE
MEAN
DEMAND
OF
THE
PREFIX
REGION
J
AND
LET
Μˆ
K
J
KΜ
J
BE
THE
MAXIMUM
MEAN
AMONG
OF
THE
PREFIX
REGIONS
UP
TO
K
LET
Ρ
I
J
T
I
DT
J
I
BE
THE
DENSITY
OF
THE
REGION
I
J
AND
Ρˆ
K
I
J
KΡ
I
J
BE
THE
MAXIMUM
DENSITY
AMONG
ALL
SUBREGIONS
OF
K
ALG
BATTERY
THRESHOLD
TI
RUN
TIME
UNBOUNDED
BOUNDED
Μˆ
N
O
N
Ρˆ
N
O
TABLE
THRESHOLD
FUNCTIONS
USED
FOR
OFFLINE
ALGORITHM
SETTINGS
BOUNDED
CAPACITY
CHANGES
THE
CHARACTER
OF
THE
OFFLINE
PROBLEM
IT
SUFFICES
HOWEVER
TO
FIND
THE
PEAK
REQUEST
MADE
BY
THE
OPTIMAL
ALGORITHM
ROPT
CLEARLY
ROPT
D
B
SINCE
THE
IDEAL
CASE
IS
THAT
A
WIDTH
ONE
PEAK
IS
REDUCED
BY
SIZE
B
OF
COURSE
THE
PEAK
REGION
MIGHT
BE
WIDER
THEOREM
ALGORITHM
A
THRESHOLD
TI
Μˆ
N
FOR
UNBOUNDED
BATTERY
AND
ALGO
RITHM
B
THRESHOLD
TI
Ρˆ
N
FOR
BOUNDED
BATTERY
ARE
OPTIMAL
FEASIBLE
AND
RUN
IN
TIMES
O
N
AND
O
RESPECTIVELY
PROOF
FIRST
LET
THE
BATTERY
BE
UNBOUNDED
FOR
ANY
REGION
J
THE
BEST
WE
CAN
HOPE
FOR
IS
THAT
REQUESTS
FOR
ALL
DEMANDS
DJ
CAN
BE
SPREAD
EVENLY
OVER
THE
FIRST
J
TIMESLOTS
THEREFORE
THE
OPTIMAL
THRESHOLD
CANNOT
BE
LOWER
THAN
THE
MAXIMUM
Μ
J
WHICH
IS
ALGORITHM
A
THRESHOLD
FOR
FEASIBILITY
IT
SUFFICES
TO
SHOW
THAT
AFTER
EACH
TIME
J
THE
BATTERY
LEVEL
IS
NONNEGATIVE
BUT
BY
TIME
J
THE
TOTAL
INPUT
TO
THE
SYSTEM
WILL
BE
J
Μˆ
N
J
Μ
J
J
DJ
WHICH
IS
THE
TOTAL
OUTPUT
TO
THE
SYSTEM
UP
TO
THAT
POINT
FOR
COMPLEXITY
JUST
NOTE
THAT
Μ
J
J
Μ
J
DJ
SO
THE
SEQUENCE
OF
Μ
VALUES
AND
THEIR
MAX
CAN
BE
COMPUTED
IN
LINEAR
TIME
NOW
LET
THE
BATTERY
BE
BOUNDED
OVER
THE
COURSE
OF
ANY
REGION
I
J
THE
BEST
THAT
CAN
BE
HOPED
FOR
IS
THAT
THE
PEAK
REQUEST
WILL
BE
REDUCED
TO
B
J
I
LESS
THAN
THE
AVERAGE
DI
IN
THE
REGION
I
E
Ρ
I
J
SO
NO
THRESHOLD
LOWER
THAN
ALGORITHM
B
IS
POSSIBLE
FOR
FEASIBILITY
IT
SUFFICES
TO
SHOW
THAT
THE
BATTERY
LEVEL
WILL
BE
NONNEGATIVE
AFTER
EACH
TIME
J
SUPPOSE
J
IS
THE
FIRST
TIME
UNDERFLOW
OCCURS
LET
I
BE
THE
LAST
TIMESLOT
PRIOR
TO
J
WITH
A
FULL
BATTERY
THEN
THERE
IS
NO
UNDERFLOW
OR
OVERFLOW
IN
I
J
AND
SO
FOR
EACH
T
I
J
THE
DISCHARGE
AT
T
IS
BT
BT
DT
T
POSSIBLY
NEGATIVE
MEANING
A
CHARGE
AND
THE
SO
THE
TOTAL
NET
DISCHARGE
OVER
I
J
IS
J
DT
J
I
T
TOTAL
NET
DISCHARGE
GREATER
THAN
B
IMPLIES
T
J
T
I
J
I
DT
WHICH
CONTRADICTS
THE
DEFINITION
OF
T
THE
DENSEST
REGION
CAN
BE
FOUND
IN
O
WITH
N
SEPARATE
LINEAR
TIME
PASSES
EACH
OF
WHICH
FINDS
THE
DENSEST
REGION
BEGINNING
IN
SOME
POSITION
I
SINCE
Ρ
I
J
CAN
BE
COMPUTED
IN
CONSTANT
TIME
FROM
Ρ
I
J
BATTERY
LEVEL
BOUNDARY
CONDITIONS
WE
ASSUMED
ABOVE
THAT
THE
BATTERY
STARTS
EMPTY
FOR
THE
UNBOUNDED
OFFLINE
ALGORITHM
AND
STARTS
FULL
FOR
THE
BOUNDED
OFFLINE
ALGORITHM
WITH
THE
FINAL
BATTERY
LEVEL
LEFT
INDE
TERMINATE
FOR
BOTH
SETTINGS
A
MORE
GENERAL
OFFLINE
PROBLEM
MAY
REQUIRE
THAT
AND
BN
I
E
THE
BATTERY
BEGINS
AND
ENDS
AT
SOME
CHARGE
LEVELS
SPECIFIED
BY
PARAMETERS
AND
WE
ARGUE
HERE
THAT
THESE
REQUIREMENTS
ARE
NOT
SIGNIFICANT
ALGO
RITHMICALLY
SINCE
BY
PRE
AND
POSTPROCESSING
WE
CAN
REDUCE
TO
THE
DEFAULT
CASES
FOR
BOTH
THE
UNBOUNDED
AND
BOUNDED
VERSIONS
FIRST
CONSIDER
THE
UNBOUNDED
SETTING
IN
WHICH
THE
INITIAL
BATTERY
LEVEL
IS
IN
ORDER
TO
ENFORCE
THAT
AND
BN
RUN
THE
USUAL
OPTIMAL
ALGORITHM
ON
THE
SEQUENCE
DN
DN
RECALL
THAT
NEGATIVE
DEMANDS
ARE
ALLOWED
THEN
BN
WILL
BE
AT
LEAST
LARGER
THAN
DN
TO
CORRECT
FOR
ANY
SURPLUS
MANUALLY
DELETE
A
TOTAL
OF
BN
FROM
THE
FINAL
REQUESTS
FOR
THE
BOUNDED
SETTING
THE
DEFAULT
CASE
IS
B
AND
BN
INDETERMINATE
TO
SUPPORT
B
AND
BN
MODIFY
THE
DEMAND
SEQUENCE
AS
ABOVE
EXCEPT
WITH
B
AS
THE
FIRST
DEMAND
AND
THEN
DO
SIMILAR
POSTPROCESSING
AS
IN
THE
UNBOUNDED
CASE
TO
DEAL
WITH
ANY
FINAL
SURPLUS
OPTIMAL
BATTERY
SIZE
A
LARGE
COMPONENT
OF
THE
FIXED
INITIAL
COST
OF
THE
SYSTEM
WILL
DEPEND
ON
BATTERY
CA
PACITY
A
RELATED
PROBLEM
THEREFORE
IS
FINDING
THE
OPTIMAL
BATTERY
SIZE
B
FOR
A
GIVEN
DEMAND
CURVE
DI
GIVEN
THAT
THE
BATTERY
STARTS
AND
ENDS
AT
THE
SAME
LEVEL
Β
WHICH
CAN
BE
SEEN
AS
AN
AMOUNT
BORROWED
AND
REPAID
THE
OPTIMAL
PEAK
REQUEST
POSSIBLE
WILL
BE
N
DI
Μ
N
AND
THE
GOAL
IS
TO
FIND
THE
SMALLEST
B
AND
Β
THAT
ACHIEVE
PEAK
Μ
N
A
COMPLETELY
FLAT
REQUEST
CURVE
IS
POSSIBLE
GIVEN
A
SUFFICIENTLY
LARGE
BATTERY
THIS
CAN
BE
DONE
IN
O
N
SINCE
WE
WILL
HAVE
BN
RI
Μ
FOR
ALL
I
AND
DI
RI
THERE
MUST
BE
NO
OVERFLOW
LET
DII
DI
Μ
I
E
THE
AMOUNT
BY
WHICH
DI
IS
ABOVE
AVERAGE
POSITIVE
MEANS
DISCHARGE
NEGATIVE
MEANS
CHARGE
THEN
THE
MINIMUM
POSSIBLE
Β
IS
THE
MAXIMUM
PREFIX
SUM
OF
THE
DI
CURVE
WHICH
WILL
BE
AT
LEAST
IT
COULD
HAPPEN
THAT
THE
BATTERY
LEVEL
WILL
AT
SOME
POINT
RISE
ABOVE
HOWEVER
CONSIDER
THE
EXAMPLE
D
FOR
WHICH
Μ
DI
AND
Β
THE
NEEDED
CAPACITY
B
CAN
BE
COMPUTED
AS
Β
PLUS
THE
MAXIMUM
PREFIX
SUM
OF
THE
NEGATION
OF
THE
DI
CURVE
WHICH
WILL
ALSO
BE
AT
LEAST
IN
THE
EXAMPLE
WE
HAVE
B
Β
ALTHOUGH
B
IS
COMPUTED
USING
Β
WE
EMPHASIZE
THAT
THE
COMPUTED
Β
IS
THE
MINI
MUM
POSSIBLE
REGARDLESS
OF
B
AND
THE
COMPUTED
B
IS
THE
MINIMUM
POSSIBLE
REGARDLESS
OF
Β
ONLINE
PROBLEM
WE
CONSIDER
TWO
NATURAL
CHOICES
OF
OBJECTIVE
FUNCTION
FOR
THE
ONLINE
PROBLEM
ONE
OP
TION
IS
TO
COMPARE
THE
PEAK
REQUESTS
SO
THAT
IF
ALG
IS
THE
PEAK
REQUEST
OF
THE
ON
LINE
ALGORITHM
ALG
AND
OP
T
IS
THAT
OF
THE
OPTIMAL
OFFLINE
ALGORITHM
OPT
THEN
A
C
COMPETITIVE
ALGORITHM
FOR
C
MUST
SATISFY
ALG
C
FOR
EVERY
DEMAND
SEQUENCE
ALTHOUGH
THIS
MAY
BE
THE
MOST
NATURAL
CANDIDATE
WE
ARGUE
THAT
FOR
MANY
SETTINGS
IT
IS
UNINTERESTING
IF
THE
PEAK
DEMAND
IS
A
FACTOR
K
LARGER
THAN
THE
BATTERY
CAPACITY
FOR
EXAM
PLE
THEN
THE
TRIVIAL
ONLINE
ALGORITHM
THAT
DOES
NOT
USE
THE
BATTERY
WOULD
BE
K
K
COMPETITIVE
IF
WE
DROP
THE
ASSUMPTION
OF
A
SMALL
BATTERY
THEN
NO
REASONABLE
COMPETI
TIVE
FACTOR
IS
POSSIBLE
IN
GENERAL
EVEN
IF
D
IS
REVEALED
IN
ADVANCE
PROPOSITION
WITH
PEAK
DEMAND
D
REVEALED
IN
ADVANCE
AND
FINITE
TIME
HORIZON
N
NO
ONLINE
ALGORITHM
FOR
THE
PROBLEM
OF
MINIMIZING
PEAK
REQUEST
CAN
HAVE
COMPETITIVE
RATIO
BETTER
THAN
N
IF
THE
BATTERY
BEGINS
WITH
SOME
STRICTLY
POSITIVE
CHARGE
OR
BETTER
THAN
Ω
N
IF
THE
BATTERY
BEGINS
EMPTY
PROOF
FOR
PART
SUPPOSE
THAT
THE
BATTERY
BEGINS
WITH
INITIAL
CHARGE
D
CONSIDER
THE
DEMAND
CURVE
D
WHERE
THE
LAST
DEMAND
IS
EITHER
D
OR
ALG
MUST
DIS
CHARGE
D
AT
TIME
SINCE
OP
T
WHEN
DN
THUS
ALG
BATTERY
IS
EMPTY
AT
TIME
IF
ALG
REQUESTS
NOTHING
BETWEEN
TIMES
AND
N
AND
DN
D
THEN
WE
HAVE
OP
T
D
N
AND
ALG
D
IF
ALG
REQUESTS
SOME
Α
DURING
ANY
OF
THOSE
TIMESLOTS
AND
DN
THEN
WE
HAVE
OP
T
AND
ALG
Α
THIS
YIELDS
A
LOWER
BOUND
OF
N
FOR
PART
SUPPOSE
THE
BATTERY
BEGINS
EMPTY
WHICH
IS
A
DISADVANTAGE
FOR
BOTH
ALG
AND
OPT
CONSIDER
THE
DEMAND
CURVE
D
IN
WHICH
CASE
OP
T
D
N
IF
AN
ALGORITHM
IS
C
COMPETITIVE
FOR
SOME
C
THEN
IN
EACH
OF
THE
FIRST
N
TIMESLOTS
OF
THIS
DEMAND
CURVE
ALG
CAN
CHARGE
AT
MOST
AMOUNT
CD
N
NOW
SUPPOSE
THAT
THE
ONLY
NONZERO
DEMAND
OF
VALUE
D
ARRIVES
POSSIBLY
EARLIER
AT
SOME
TIMESLOT
K
N
FOLLOWING
K
DEMANDS
OF
ZERO
DURING
WHICH
ALG
CAN
CHARGE
AT
MOST
K
CD
N
IN
THIS
CASE
WE
HAVE
OP
T
D
K
AND
ALG
D
K
CD
N
WHICH
YIELDS
THE
COMPETITIVE
RATIO
C
D
K
CD
N
K
C
N
K
N
KC
N
SOLVING
FOR
C
AND
THEN
CHOOSING
K
N
WE
HAVE
K
N
C
Ω
N
N
K
N
THUS
ESTABLISHING
THE
LOWER
BOUND
N
INSTEAD
WE
COMPARE
THE
PEAK
SHAVING
AMOUNT
OR
SAVINGS
I
E
D
R
FOR
A
GIVEN
INPUT
LET
OPT
BE
THE
PEAK
SHAVING
OF
THE
OPTIMAL
ALGORITHM
AND
LET
ALG
BE
THE
PEAK
SHAVING
OF
THE
ONLINE
ALGORITHM
THEN
AN
ONLINE
ALGORITHM
IS
C
COMPETITIVE
FOR
C
IF
C
O
P
T
FOR
EVERY
PROBLEM
INSTANCE
FOR
THIS
SETTING
WE
OBTAIN
THE
ONLINE
ALGORITHMS
DESCRIBED
BELOW
THE
ONLINE
ALGORITHMS
SEE
DEF
ARE
SHOWN
IN
TABLE
DEFINITION
LET
T
OPT
BE
THE
OPTIMAL
THRESHOLD
USED
BY
THE
APPROPRIATE
OPTIMAL
ALGO
RITHM
WHEN
RUN
ON
THE
FIRST
I
TIMESLOTS
AT
TIME
I
DURING
THE
ONLINE
COMPUTATION
LET
SI
BE
THE
INDEX
OF
THE
MOST
RECENT
TIME
PRIOR
TO
I
WITH
BSI
B
OR
IN
THE
UNBOUNDED
SETTING
IF
THE
PEAK
IS
NOT
KNOWN
A
LOWER
BOUND
OF
N
CAN
BE
OBTAINED
ALSO
FOR
THE
LATTER
CASE
ALG
BATTERY
THRESHOLD
TI
PER
SLOT
TIME
BOTH
D
T
OPT
HN
O
N
BOTH
D
D
Ρ
SI
I
HN
SI
O
TABLE
THRESHOLD
FUNCTIONS
USED
FOR
ONLINE
ALGORITHMS
LOWER
BOUNDS
FOR
D
R
SINCE
THE
COMPETITIVENESS
OF
THE
ONLINE
ALGORITHMS
HOLDS
FOR
ARBITRARY
INITIAL
BATTERY
LEVEL
IN
OBTAINING
LOWER
BOUNDS
ON
COMPETITIVENESS
WE
ASSUME
PARTICULAR
INITIAL
BATTERY
LEVELS
PROPOSITION
WITH
PEAK
DEMAND
D
UNKNOWN
AND
FINITE
TIME
HORIZON
N
THERE
IS
NO
ONLINE
ALGORITHM
WITH
ANY
CONSTANT
COMPETITIVE
RATIO
FOR
UNBOUNDED
BATTERY
EVEN
WITH
N
OR
WITH
COMPETITIVE
RATIO
BETTER
THAN
N
FOR
BOUNDED
BATTERY
PROOF
FOR
PART
ASSUME
AND
SUPPOSE
THEN
IF
ALG
REQUESTS
AND
WE
HAVE
D
THEN
OP
T
D
AND
ALG
IF
ALG
REQUESTS
A
FOR
SOME
A
AND
WE
HAVE
A
THEN
OP
T
A
AND
ALG
FOR
PART
LET
B
AND
ASSUME
ALG
IS
C
COMPETITIVE
CONSIDER
THE
DEMAND
CURVE
B
THEN
OPT
CLEARLY
DISCHARGES
B
AT
TIME
DECREASING
THE
PEAK
BY
B
FOR
ALG
TO
BE
C
COMPETITIVE
IT
MUST
DISCHARGE
AT
LEAST
B
IN
THE
FIRST
SLOT
NOW
CONSIDER
CURVE
B
AT
TIME
OPT
DISCHARGES
B
DECREASING
THE
PEAK
BY
B
SO
AT
TIME
ALG
MUST
DISCHARGE
AT
LEAST
B
AT
TIME
ALG
ALREADY
HAD
TO
DISCHARGE
B
SIMILARLY
AT
TIME
I
FOR
B
IB
NB
ALG
MUST
DISCHARGE
B
TOTAL
C
N
B
NB
B
C
THE
TRIVIAL
ALGORITHM
THAT
SIMPLY
DISCHARGES
AMOUNT
B
N
AT
EACH
OF
THE
N
TIMESLOTS
AND
NEVER
CHARGES
IS
N
COMPETITIVE
SINCE
OP
T
B
AND
SO
MATCHES
THE
LOWER
BOUND
FOR
THE
BOUNDED
CASE
PROPOSITION
WITH
PEAK
DEMAND
D
KNOWN
IN
ADVANCE
AND
FINITE
TIME
HORIZON
N
NO
ONLINE
ALGORITHM
CAN
HAVE
COMPETITIVE
RATIO
BETTER
THAN
HN
IF
THE
BATTERY
BEGINS
NONEMPTY
OR
COMPETITIVE
RATIO
BETTER
THAN
HN
IF
THE
BATTERY
BEGINS
EMPTY
REGARDLESS
OF
WHETHER
THE
BATTERY
IS
BOUNDED
OR
NOT
PROOF
FIRST
ASSUME
THE
BATTERY
HAS
INITIAL
CHARGE
B
THE
CAPACITY
IS
EITHER
AT
LEAST
B
OR
UNBOUNDED
SUPPOSE
ALG
IS
C
COMPETITIVE
CONSIDER
THE
CURVE
D
WITH
D
B
THEN
OPT
CLEARLY
DISCHARGES
B
AT
TIME
DECREASING
THE
PEAK
BY
B
FOR
ALG
TO
BE
C
COMPETITIVE
IT
MUST
DISCHARGE
AT
LEAST
B
NOW
CONSIDER
CURVE
D
D
AT
TIMES
AND
OPT
DISCHARGES
B
DECREASING
THE
PEAK
BY
B
AT
TIME
ALG
WILL
HAVE
TO
DISCHARGE
AT
LEAST
B
B
SIMILARLY
AT
TIME
I
ON
D
D
D
D
ALG
MUST
DISCHARGE
B
C
N
B
B
SINCE
WE
DISCHARGE
AT
EACH
TIMESLOT
AND
NEVER
CHARGE
WE
MUST
HAVE
B
HN
B
AND
SO
IT
FOLLOWS
THAT
C
HN
NOW
LET
THE
BATTERY
START
EMPTY
ASSUME
THE
BATTERY
CAPACITY
IS
AT
LEAST
D
OR
IS
UNBOUNDED
REPEAT
THE
ARGUMENT
AS
ABOVE
EXCEPT
NOW
WITH
A
ZERO
DEMAND
INSERTED
AT
THE
START
OF
THE
DEMAND
CURVES
WHICH
GIVES
BOTH
ALG
AND
OPT
AN
OPPORTUNITY
TO
CHARGE
THEN
FOR
EACH
TIME
I
N
ALG
MUST
DISCHARGE
AT
LEAST
D
SINCE
OPT
MAY
DISCHARGE
AND
SO
SAVE
D
IN
WHICH
CASE
IT
WOULD
HAVE
INITIALLY
CHARGED
D
I
ALG
IS
THEN
REQUIRED
TO
DISCHARGE
HN
D
DURING
THE
LAST
N
TIMESLOTS
OBVIOUSLY
IT
COULD
NOT
HAVE
CHARGED
MORE
THAN
D
DURING
THE
FIRST
TIMESLOT
IN
FACT
IT
MUST
CHARGE
LESS
THAN
THIS
ON
THE
SEQUENCE
D
OPT
CHARGES
D
AT
TIME
AND
DISCHARGES
IT
AT
TIME
SAVING
D
ALG
MUST
DISCHARGE
D
AT
TIME
IN
ORDER
TO
BE
C
COMPETITIVE
ON
THIS
SEQUENCE
AND
SO
REDUCE
THE
PEAK
D
BY
D
THEREFORE
AT
TIME
ALG
CANNOT
CHARGE
MORE
THAN
D
D
THEREFORE
WE
MUST
HAVE
D
D
HN
D
C
WHICH
IMPLIES
THAT
C
HN
BOUNDED
BATTERY
OUR
FIRST
ONLINE
ALGORITHM
BASES
ITS
THRESHOLD
AT
TIME
I
ON
A
COMPUTATION
OF
THE
OPTIMAL
OFFLINE
THRESHOLD
T
OPT
FOR
THE
DEMANDS
DI
THE
SECOND
BASES
ITS
THRESHOLD
AT
TIME
I
ON
Ρ
SI
I
SEE
DEFS
AND
ASSUMING
THE
ALGORITHMS
ARE
FEASIBLE
I
E
NO
BATTERY
UNDERFLOW
OCCURS
IT
IS
NOT
DIFFICULT
TO
SHOW
THAT
THEY
ARE
COMPETITIVE
THEOREM
ALGORITHMS
A
AND
B
ARE
HN
COMPETITIVE
IF
THEY
ARE
FEASIBLE
AND
HAVE
PER
TIMESLOT
RUNNING
TIMES
OF
O
N
AND
O
RESPECTIVELY
PROOF
FIRST
OBSERVE
THAT
Ρˆ
I
Ρ
SI
I
IMPLIES
D
Ρˆ
I
D
Ρ
SI
I
IMPLIES
T
A
T
B
FOR
ALL
I
THEREFORE
IT
SUFFICES
TO
PROVE
COMPETITIVENESS
FOR
ALGORITHM
A
SINCE
T
OPT
I
IS
THE
LOWEST
POSSIBLE
THRESHOLD
UP
TO
TIME
I
D
T
OPT
I
IS
THE
HIGHEST
POSSIBLE
PEAK
SHAVING
AS
OF
TIME
I
SINCE
THE
ALGORITHM
ALWAYS
SAVES
A
HN
FRACTION
OF
THIS
IT
IS
HN
COMPETITIVE
BY
CONSTRUCTION
SINCE
Μ
I
CAN
BE
FOUND
IN
CONSTANT
TIME
FROM
Μ
I
ALGORITHM
B
IS
CONSTANT
TIME
PER
SLOT
SIMILARLY
ALGORITHM
A
IS
RECALLING
THE
PROOF
OF
THEOREM
LINEAR
PER
SLOT
WE
NOW
SHOW
THAT
INDEED
BOTH
ALGORITHMS
ARE
FEASIBLE
USING
THE
FOLLOWING
LEMMA
WHICH
ALLOWS
US
TO
LIMIT
OUR
ATTENTION
TO
A
CERTAIN
FAMILY
OF
DEMAND
SEQUENCES
LEMMA
IF
THERE
IS
A
DEMAND
SEQUENCE
DN
IN
WHICH
UNDERFLOW
OCCURS
FOR
ALGORITHM
A
OR
B
THEN
THERE
IS
ALSO
A
DEMAND
SEQUENCE
FOR
THE
SAME
ALGORITHM
IN
WHICH
UNDERFLOW
CONTINUES
TO
THE
END
I
E
BN
AND
NO
OVERFLOW
EVER
OCCURS
I
E
ONE
IN
WHICH
THE
BATTERY
LEVEL
DECREASES
MONOTONICALLY
FROM
FULL
TO
EMPTY
PROOF
THE
BATTERY
IS
INITIALIZED
TO
FULL
B
OVER
THE
COURSE
OF
RUNNING
ONE
OF
THE
ALGORITHMS
ON
A
PARTICULAR
PROBLEM
INSTANCE
THE
BATTERY
LEVEL
WILL
FALL
AND
RISE
AND
MAY
RETURN
TO
FULL
CHARGE
MULTIPLE
TIMES
SUPPOSE
UNDERFLOW
WERE
TO
OCCUR
AT
SOME
TIME
T
I
E
BT
AND
LET
BE
THE
MOST
RECENT
TIME
BEFORE
T
WHEN
THE
BATTERY
WAS
FULL
WE
NOW
CONSTRUCT
A
DEMAND
SEQUENCE
WITH
THE
TWO
DESIRED
PROPERTIES
FOR
BOTH
ALGORITHMS
FIRST
IF
THEN
ALSO
CONSIDERING
REGION
WHEN
DEFINING
THE
THRESH
OLD
TI
FOR
ALGORITHM
A
OR
B
CAN
ONLY
RAISE
THE
THRESHOLD
OVER
WHAT
IT
WOULD
BE
IF
ONLY
REGION
T
WERE
CONSIDERED
THEREFORE
SHIFTING
THE
REGION
LEFTWARD
FROM
T
TO
TI
T
WILL
ONLY
LOWER
THE
THRESHOLDS
USED
WHICH
THEREFORE
PRESERVES
THE
UNDERFLOW
SECOND
SINCE
ANY
UNDERFLOW
THAT
OCCURS
IN
REGION
TI
CAN
BE
EXTENDED
TO
THE
END
OF
SEQUENCE
BY
SETTING
EACH
DEMAND
AFTER
TIME
TI
TO
D
WE
CAN
ASSUME
WLOG
THAT
TI
N
THEOREM
ALGORITHMS
A
AND
B
ARE
FEASIBLE
PROOF
FOR
A
PROOF
BY
CONTRADICTION
WE
CAN
RESTRICT
OURSELVES
BY
LEMMA
TO
REGIONS
THAT
BEGIN
WITH
A
FULL
BATTERY
UNDERFLOW
AT
THE
END
AND
HAVE
NO
OVERFLOW
IN
THE
MIDDLE
FOR
SUCH
A
REGION
THE
CHANGE
IN
BATTERY
LEVEL
IS
WELL
BEHAVED
BI
BI
DI
TI
WHICH
ALLOWS
US
TO
SUM
THE
NET
DISCHARGE
AND
PROVE
IT
IS
BOUNDED
BY
B
WE
NOW
SHOW
THAT
IT
IS
IMPOSSIBLE
FOR
THE
BATTERY
TO
FALL
BELOW
AT
TIME
N
BY
UPPERBOUNDING
THE
NET
DISCHARGE
OVER
THIS
REGION
LET
BI
BI
BI
DI
TI
BE
THE
AMOUNT
OF
ENERGY
THE
BATTERY
DISCHARGES
AT
STEP
I
BI
WILL
BE
NEGATIVE
WHEN
THE
BATTERY
CHARGES
WE
WILL
SHOW
THAT
BI
B
LET
BA
AND
BB
REFER
TO
THE
CHANGE
IN
BATTERY
LEVELS
FOR
THE
CORRESPONDING
ALGORITHMS
BECAUSE
AS
WE
OBSERVED
ABOVE
T
A
T
B
WE
HAVE
I
I
BA
DI
T
A
BB
DI
T
B
THEREFORE
IT
SUFFICES
TO
PROVE
THE
FEASIBILITY
RESULT
FOR
ALGORITHM
B
AND
SO
WE
DROP
THE
SUPERSCRIPTS
EXPANDING
THE
DEFINITION
OF
THAT
ALGORITHM
THRESHOLD
WE
HAVE
BI
DI
TI
DI
D
H
D
Ρ
I
DI
D
HN
I
D
I
K
DK
B
BY
SUMMING
EQ
FOR
EACH
I
WE
OBTAIN
BI
DI
D
I
DK
B
I
I
I
HN
D
I
D
I
B
I
I
DI
D
K
HN
D
I
K
D
I
H
I
THEREFORE
IT
SUFFICES
TO
SHOW
THAT
I
DI
D
I
K
HN
DK
I
WHICH
IS
EQUIVALENT
TO
D
ND
ND
D
I
K
I
N
I
K
N
IFF
HNDI
N
DK
I
ND
HN
I
WITH
THE
FOLLOWING
DERIVATION
I
K
N
I
N
N
N
N
N
N
N
K
N
N
DK
DK
DK
D
K
DK
H
D
H
D
WE
CAN
REWRITE
EQ
REPLACING
THE
PARENTHESIZED
EXPRESSION
AS
N
HI
N
HN
IN
FACT
THIS
HOLDS
WITH
EQUALITY
SEE
EQ
UNBOUNDED
BATTERY
AND
BOUNDARY
CONDITIONS
BOTH
ONLINE
ALGORITHMS
MODIFIED
TO
CALL
APPROPRIATE
SUBROUTINES
ALSO
WORK
FOR
THE
UN
BOUNDED
BATTERY
SETTING
THE
ALGORITHMS
ARE
FEASIBLE
IN
THIS
SETTING
SINCE
Ρ
I
T
OPT
I
STILL
HOLDS
WHERE
T
OPT
I
IS
NOW
THE
OPTIMAL
THRESHOLD
FOR
THE
UNBOUNDED
BATTERY
SETTING
RECALL
THAT
OFFLINE
ALGORITHM
A
CAN
GREEDILY
RUN
IN
LINEAR
TOTAL
TIME
THE
ALGORITHM
IS
HN
COMPETITIVE
BY
CONSTRUCTION
AS
BEFORE
COROLLARY
ALGORITHMS
A
AND
B
ARE
FEASIBLE
IN
THE
UNBOUNDED
BATTERY
SETTING
PROOF
THE
PROOF
IS
SIMILAR
TO
THAT
OF
THEOREM
EXCEPT
THAT
WHICH
MAY
BE
IS
PLUGGED
IN
FOR
ALL
OCCURRENCES
OF
B
RESULTING
IN
A
MODIFIED
Ρ
AND
OVERFLOW
IS
NO
LONGER
A
CONCERN
WE
ALSO
NOTE
THAT
THE
CORRECTNESS
AND
COMPETITIVENESS
OF
BOTH
ALGORITHMS
WITH
MI
NOR
PREPROCESSING
HOLDS
FOR
THE
SETTING
OF
ARBITRARY
INITIAL
BATTERY
LEVELS
IN
THE
CASE
OF
ALGORITHM
A
EACH
COMPUTATION
OF
T
OPT
IS
COMPUTED
FOR
THE
CHOSEN
AS
DESCRIBED
IN
SECTION
AND
THE
ONLINE
ALGORITHM
AGAIN
PROVIDES
A
FRACTION
HN
OF
THIS
SAVINGS
ALTHOUGH
IN
THE
BOUNDED
SETTING
INCREASING
FOR
THE
T
OPT
COMPUTATION
BY
MODIFY
ING
MAY
RAISE
THE
PEAK
DEMAND
IN
THE
OFFLINE
ALGORITHM
INPUT
THE
D
VALUE
FOR
THE
ONLINE
ALGORITHM
IS
NOT
CHANGED
INDEED
EXAMINING
THE
PROOF
OF
THEOREM
WE
NOTE
THAT
THE
UPPERBOUND
ON
THE
DI
IS
ONLY
USED
FOR
I
SEE
EQ
CONCLUSION
IN
THIS
PAPER
WE
FORMULATED
A
NOVEL
PEAK
SHAVING
PROBLEM
AND
GAVE
EFFICIENT
OPTIMAL
OFFLINE
A
LGORITHMS
A
ND
O
PTIMALLY
C
OMPETITIVE
O
NLINE
A
LGORITHMS
I
N
W
ORK
I
N
PROGRESS
WE
ARE
TESTING
OUR
ONLINE
ALGORITHMS
ON
ACTUAL
CLIENT
DATA
FROM
GAIA
SEE
FOR
PRELIMINARY
RESULTS
THERE
ARE
SEVERAL
INTERESTING
EXTENSIONS
TO
THE
THEORETICAL
PROBLEM
THAT
WE
PLAN
TO
ADDRESS
SUCH
AS
ADAPTING
THE
ALGORITHMS
TO
INEFFICIENT
BATTERIES
THAT
LOSE
A
PERCENTAGE
OF
CHARGE
INSTANTLY
OR
OVER
TIME
OR
BATTERIES
WITH
A
CHARGING
SPEED
LIMIT
WE
COULD
ALSO
OPTIMIZE
FOR
A
MOVING
AVERAGE
OF
DEMANDS
RATHER
THAN
A
SINGLE
PEAK
FINALLY
ONLINE
ALGORITHMS
COULD
ALSO
BE
GRANTED
ADDITIONAL
PREDICTIONS
ABOUT
THE
FUTURE
OPTIMAL
POWER
ALLOCATION
IN
SERVER
FARMS
ABSTRACT
ANSHUL
GANDHI
CARNEGIE
MELLON
UNIVERSITY
PITTSBURGH
PA
USA
RAJARSHI
DAS
IBM
RESEARCH
HAWTHORNE
NY
USA
MOR
HARCHOL
BALTER
CARNEGIE
MELLON
UNIVERSITY
PITTSBURGH
PA
USA
CHARLES
LEFURGY
IBM
RESEARCH
AUSTIN
TX
USA
IMPROVE
SERVER
FARM
PERFORMANCE
BY
A
FACTOR
OF
TYPICALLY
SERVER
FARMS
TODAY
CONSUME
MORE
THAN
OF
THE
TOTAL
ELECTRICITY
IN
THE
U
AT
A
COST
OF
NEARLY
BILLION
GIVEN
THE
RISING
COST
OF
ENERGY
MANY
INDUSTRIES
ARE
NOW
SEEKING
SOLUTIONS
FOR
HOW
TO
BEST
MAKE
USE
OF
THEIR
AVAILABLE
POWER
AN
IMPORTANT
QUESTION
WHICH
ARISES
IN
THIS
CONTEXT
IS
HOW
TO
DISTRIBUTE
AVAILABLE
POWER
AMONG
SERVERS
IN
A
SERVER
FARM
SO
AS
TO
GET
MAXIMUM
PERFORMANCE
BY
GIVING
MORE
POWER
TO
A
SERVER
ONE
CAN
GET
HIGHER
SERVER
FREQUENCY
SPEED
HENCE
IT
IS
COMMONLY
BELIEVED
THAT
FOR
A
GIVEN
POWER
BUDGET
PERFORMANCE
CAN
BE
MAXIMIZED
BY
OPERATING
SERVERS
AT
THEIR
HIGHEST
POWER
LEVELS
HOWEVER
IT
IS
ALSO
CONCEIVABLE
THAT
ONE
MIGHT
PREFER
TO
RUN
SERVERS
AT
THEIR
LOWEST
POWER
LEVELS
WHICH
ALLOWS
MORE
SERVERS
TO
BE
TURNED
ON
FOR
A
GIVEN
POWER
BUDGET
TO
FULLY
UNDERSTAND
THE
EFFECT
OF
POWER
ALLOCATION
ON
PERFORMANCE
IN
A
SERVER
FARM
WITH
A
FIXED
POWER
BUDGET
WE
INTRODUCE
A
QUEUEING
THEO
RETIC
MODEL
WHICH
ALLOWS
US
TO
PREDICT
THE
OPTIMAL
POWER
ALLOCATION
IN
A
VARIETY
OF
SCENARIOS
RESULTS
ARE
VERIFIED
VIA
EXTENSIVE
EXPERIMENTS
ON
AN
IBM
BLADECENTER
WE
FIND
THAT
THE
OPTIMAL
POWER
ALLOCATION
VARIES
FOR
DIFFER
ENT
SCENARIOS
IN
PARTICULAR
IT
IS
NOT
ALWAYS
OPTIMAL
TO
RUN
SERVERS
AT
THEIR
MAXIMUM
POWER
LEVELS
THERE
ARE
SCENAR
IOS
WHERE
IT
MIGHT
BE
OPTIMAL
TO
RUN
SERVERS
AT
THEIR
LOWEST
POWER
LEVELS
OR
AT
SOME
INTERMEDIATE
POWER
LEVELS
OUR
ANAL
YSIS
SHOWS
THAT
THE
OPTIMAL
POWER
ALLOCATION
IS
NON
OBVIOUS
AND
DEPENDS
ON
MANY
FACTORS
SUCH
AS
THE
POWER
TO
FREQUENCY
RELATIONSHIP
IN
THE
PROCESSORS
THE
ARRIVAL
RATE
OF
JOBS
THE
MAXIMUM
SERVER
FREQUENCY
THE
LOWEST
ATTAINABLE
SERVER
FRE
QUENCY
AND
THE
SERVER
FARM
CONFIGURATION
FURTHERMORE
OUR
THEORETICAL
MODEL
ALLOWS
US
TO
EXPLORE
MORE
GENERAL
SETTINGS
THAN
WE
CAN
IMPLEMENT
INCLUDING
ARBITRARILY
LARGE
SERVER
FARMS
AND
DIFFERENT
POWER
TO
FREQUENCY
CURVES
IMPORTANTLY
WE
SHOW
THAT
THE
OPTIMAL
POWER
ALLOCATION
CAN
SIGNIFICANTLY
RESEARCH
SUPPORTED
BY
NSF
SMA
PDOS
GRANT
CCR
AND
A
IBM
FACULTY
AWARD
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
SIGMETRICS
PERFORMANCE
JUNE
SEATTLE
WA
USA
COPYRIGHT
ACM
AND
AS
MUCH
AS
A
FACTOR
OF
IN
SOME
CASES
CATEGORIES
AND
SUBJECT
DESCRIPTORS
C
PERFORMANCE
OF
SYSTEMS
MODELING
TECHNIQUES
GENERAL
TERMS
THEORY
EXPERIMENTATION
MEASUREMENT
PERFORMANCE
INTRODUCTION
SERVERS
TODAY
CONSUME
TEN
TIMES
MORE
POWER
THAN
THEY
DID
TEN
YEARS
AGO
RECENT
ARTICLES
ESTIMATE
THAT
A
HIGH
PERFORMANCE
SERVER
REQUIRES
MORE
THAN
OF
ENERGY
COST
PER
YEAR
GIVEN
THE
LARGE
NUMBER
OF
SERVERS
IN
USE
TODAY
THE
WORLDWIDE
EXPENDITURE
ON
ENTERPRISE
POWER
AND
COOLING
OF
THESE
SERVERS
IS
ESTIMATED
TO
BE
IN
EXCESS
OF
BILLION
POWER
CONSUMPTION
IS
PARTICULARLY
PRONOUNCED
IN
CPU
INTENSIVE
SERVER
FARMS
COMPOSED
OF
TENS
TO
THOUSANDS
OF
SERVERS
ALL
SHARING
WORKLOAD
AND
POWER
SUPPLY
WE
CONSIDER
SERVER
FARMS
WHERE
EACH
INCOMING
JOB
CAN
BE
ROUTED
TO
ANY
SERVER
I
E
IT
HAS
NO
AFFINITY
FOR
A
PARTICULAR
SERVER
SERVER
FARMS
USUALLY
HAVE
A
FIXED
PEAK
POWER
BUDGET
THIS
IS
BECAUSE
LARGE
POWER
CONSUMERS
OPERATING
SERVER
FARMS
ARE
OFTEN
BILLED
BY
POWER
SUPPLIERS
IN
PART
BASED
ON
THEIR
PEAK
POWER
REQUIREMENTS
THE
PEAK
POWER
BUDGET
OF
A
SERVER
FARM
ALSO
DETERMINES
ITS
COOLING
AND
POWER
DELIVERY
INFRAS
TRUCTURE
COSTS
HENCE
COMPANIES
ARE
INTERESTED
IN
MAXI
MIZING
THE
PERFORMANCE
AT
A
SERVER
FARM
GIVEN
A
FIXED
PEAK
POWER
BUDGET
THE
POWER
ALLOCATION
PROBLEM
WE
CONSIDER
IS
HOW
TO
DIS
TRIBUTE
AVAILABLE
POWER
AMONG
SERVERS
IN
A
SERVER
FARM
SO
AS
TO
MINIMIZE
MEAN
RESPONSE
TIME
EVERY
SERVER
RUNNING
A
GIVEN
WORKLOAD
HAS
A
MINIMUM
LEVEL
OF
POWER
CONSUMPTION
B
NEEDED
TO
OPERATE
THE
PROCESSOR
AT
THE
LOWEST
ALLOWABLE
FREQUENCY
AND
A
MAXIMUM
LEVEL
OF
POWER
CONSUMPTION
C
NEEDED
TO
OPERATE
THE
PROCESSOR
AT
THE
HIGHEST
ALLOWABLE
FRE
QUENCY
BY
VARYING
THE
POWER
ALLOCATED
TO
A
SERVER
WITHIN
THE
RANGE
OF
B
TO
C
WATTS
ONE
CAN
PROPORTIONATELY
VARY
THE
SERVER
FREQUENCY
SEE
FIG
HENCE
ONE
MIGHT
EXPECT
THAT
RUNNING
SERVERS
AT
THEIR
HIGHEST
POWER
LEVELS
OF
C
WATTS
WHICH
WE
REFER
TO
AS
POWMAX
IS
THE
OPTIMAL
POWER
ALLOCATION
SCHEME
TO
MINIMIZE
RESPONSE
TIME
SINCE
WE
ARE
CONSTRAINED
BY
A
POWER
BUDGET
THERE
ARE
ONLY
A
LIMITED
NUMBER
OF
SERVERS
THAT
WE
CAN
OPERATE
AT
THE
HIGHEST
POWER
LEVEL
THE
REST
OF
THE
SERVERS
REMAIN
TURNED
OFF
THUS
POWMAX
CORRESPONDS
TO
HAVING
FEW
FAST
SERVERS
IN
SHARP
CONTRAST
IS
POWMIN
WHICH
WE
DEFINE
AS
OPERATING
SERVERS
AT
THEIR
LOWEST
POWER
LEVELS
OF
B
WATTS
SINCE
WE
SPEND
LESS
POWER
ON
EACH
SERVER
POWMIN
CORRESPONDS
TO
HAVING
MANY
SLOW
SERVERS
OF
COURSE
THERE
MIGHT
BE
SCENARIOS
WHERE
WE
NEITHER
OPERATE
OUR
SERVERS
AT
THE
HIGHEST
POWER
LEVELS
NOR
AT
THE
LOWEST
POWER
LEVELS
BUT
WE
OPERATE
THEM
AT
SOME
INTERMEDIATE
POWER
LEVELS
WE
REFER
TO
SUCH
POWER
ALLOCATION
SCHEMES
AS
POWMED
UNDERSTANDING
POWER
ALLOCATION
IN
A
SERVER
FARM
IS
INTRIN
SICALLY
DIFFICULT
FOR
MANY
REASONS
FIRST
THERE
IS
NO
SINGLE
ALLOCATION
SCHEME
WHICH
IS
OPTIMAL
IN
ALL
SCENARIOS
FOR
EX
AMPLE
IT
IS
COMMONLY
BELIEVED
THAT
POWMAX
IS
THE
OPTIMAL
POWER
ALLOCATION
SCHEME
HOWEVER
AS
WE
SHOW
LATER
POWMIN
AND
POWMED
CAN
SOMETIMES
OUTPERFORM
POWMAX
BY
ALMOST
A
FACTOR
OF
SECOND
IT
TURNS
OUT
THAT
THE
OPTIMAL
POWER
ALLOCATION
DEPENDS
ON
A
VERY
LONG
LIST
OF
EX
TERNAL
FACTORS
SUCH
AS
THE
OUTSIDE
ARRIVAL
RATE
WHETHER
AN
OPEN
OR
CLOSED
WORKLOAD
CONFIGURATION
IS
USED
THE
POWER
TO
FREQUENCY
RELATIONSHIP
HOW
POWER
TRANSLATES
TO
SERVER
FREQUENCY
INHERENT
IN
THE
TECHNOLOGY
THE
MINIMUM
POWER
CONSUMPTION
OF
A
SERVER
B
WATTS
THE
MAXIMUM
POWER
THAT
A
SERVER
CAN
USE
C
WATTS
AND
MANY
OTHER
FACTORS
IT
IS
SIM
PLY
IMPOSSIBLE
TO
EXAMINE
ALL
THESE
FACTORS
VIA
EXPERIMENTS
TO
FULLY
UNDERSTAND
THE
EFFECT
OF
POWER
ALLOCATION
ON
MEAN
RESPONSE
TIME
IN
A
SERVER
FARM
WITH
A
FIXED
POWER
BUDGET
WE
INTRODUCE
A
QUEUEING
THEORETIC
MODEL
WHICH
ALLOWS
US
TO
PREDICT
THE
OPTIMAL
POWER
ALLOCATION
IN
A
VARIETY
OF
SCENARIOS
WE
THEN
VERIFY
OUR
RESULTS
VIA
EXTENSIVE
EXPERIMENTS
ON
AN
IBM
BLADECENTER
PRIOR
WORK
IN
POWER
MANAGEMENT
HAS
BEEN
MOTIVATED
BY
THE
IDEA
OF
MANAGING
POWER
AT
THE
GLOBAL
DATA
CENTER
LEVEL
RATHER
THAN
AT
THE
MORE
LOCALIZED
SINGLE
SERVER
LEVEL
WHILE
POWER
MANAGEMENT
IN
SERVER
FARMS
OFTEN
DEALS
WITH
VARIOUS
ISSUES
SUCH
AS
REDUCING
COOLING
COSTS
MINIMIZING
IDLE
POWER
WASTAGE
AND
MINIMIZING
AVERAGE
POWER
CONSUMPTION
WE
ARE
MORE
INTERESTED
IN
THE
PROBLEM
OF
ALLOCATING
PEAK
POWER
AMONG
SERVERS
IN
A
SERVER
FARM
TO
MAXIMIZE
PERFOR
MANCE
NOTABLE
PRIOR
WORK
DEALING
WITH
PEAK
POWER
ALLOCA
TION
IN
A
SERVER
FARM
INCLUDES
RAGHAVENDRA
ET
AL
FEMAL
ET
AL
AND
CHASE
ET
AL
AMONG
OTHERS
RAGHAVENDRA
ET
AL
PRESENT
A
POWER
MANAGEMENT
SOLUTION
THAT
CO
ORDINATES
DIFFERENT
INDIVIDUAL
APPROACHES
TO
SIMULTANEOUSLY
MINIMIZE
AVERAGE
POWER
PEAK
POWER
AND
IDLE
POWER
WASTAGE
FEMAL
ET
AL
ALLOCATE
PEAK
POWER
SO
AS
TO
MAXIMIZE
THROUGHPUT
IN
A
DATA
CENTER
WHILE
SIMULTANEOUSLY
ATTEMPT
ING
TO
SATISFY
CERTAIN
OPERATING
CONSTRAINTS
SUCH
AS
LOAD
BALANCING
THE
AVAILABLE
POWER
AMONG
THE
SERVERS
CHASE
ET
AL
PRESENT
AN
AUCTION
BASED
ARCHITECTURE
FOR
IMPROVING
THE
ENERGY
EFFICIENCY
OF
DATA
CENTERS
WHILE
ACHIEVING
SOME
QUALITY
OF
SERVICE
SPECIFICATIONS
WE
DIFFER
FROM
THE
ABOVE
WORK
IN
THAT
WE
SPECIFICALLY
DEAL
WITH
MINIMIZING
MEAN
RE
SPONSE
TIME
FOR
A
GIVEN
PEAK
POWER
BUDGET
AND
UNDERSTAND
ING
ALL
THE
FACTORS
THAT
AFFECT
IT
OUR
CONTRIBUTIONS
AS
WE
HAVE
STATED
THE
OPTIMAL
POWER
ALLOCATION
SCHEME
DEPENDS
ON
MANY
FACTORS
PERHAPS
THE
MOST
IMPORTANT
OF
THESE
IS
THE
SPECIFIC
RELATIONSHIP
BETWEEN
THE
POWER
ALLOCATED
TO
A
SERVER
AND
ITS
FREQUENCY
SPEED
HENCEFORTH
REFERRED
TO
AS
THE
POWER
TO
FREQUENCY
RELATIONSHIP
THERE
ARE
SEV
ERAL
MECHANISMS
WITHIN
PROCESSORS
THAT
CONTROL
THE
POWER
TO
FREQUENCY
RELATIONSHIP
THESE
CAN
BE
CATEGORIZED
INTO
DFS
DYNAMIC
FREQUENCY
SCALING
DVFS
DYNAMIC
VOLT
AGE
AND
FREQUENCY
SCALING
AND
DVFS
DFS
SECTION
DIS
CUSSES
THESE
MECHANISMS
IN
MORE
DETAIL
THE
FUNCTIONAL
FORM
OF
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
A
SERVER
DE
PENDS
ON
MANY
FACTORS
SUCH
AS
THE
WORKLOAD
USED
MAXI
MUM
SERVER
POWER
MAXIMUM
SERVER
FREQUENCY
AND
THE
VOLT
AGE
AND
FREQUENCY
SCALING
MECHANISM
USED
DFS
DVFS
OR
DVFS
DFS
UNFORTUNATELY
THE
FUNCTIONAL
FORM
OF
THE
SERVER
LEVEL
POWER
TO
FREQUENCY
RELATIONSHIP
IS
ONLY
RECENTLY
BEGINNING
TO
BE
STUDIED
SEE
THE
PAPERS
AND
IS
STILL
NOT
WELL
UNDERSTOOD
OUR
FIRST
CONTRIBUTION
IS
THE
INVES
TIGATION
OF
HOW
POWER
ALLOCATION
AFFECTS
SERVER
FREQUENCY
IN
A
SINGLE
SERVER
USING
DFS
DVFS
AND
DVFS
DFS
FOR
VARIOUS
WORKLOADS
IN
PARTICULAR
IN
SECTION
WE
DERIVE
A
FUNCTIONAL
FORM
FOR
THE
POWER
TO
FREQUENCY
RELATIONSHIP
BASED
ON
OUR
MEASURED
VALUES
SEE
FIGS
A
AND
B
OUR
SECOND
CONTRIBUTION
IS
THE
DEVELOPMENT
OF
A
QUEUE
ING
THEORETIC
MODEL
WHICH
PREDICTS
THE
MEAN
RESPONSE
TIME
FOR
A
SERVER
FARM
AS
A
FUNCTION
OF
MANY
FACTORS
INCLUDING
THE
POWER
TO
FREQUENCY
RELATIONSHIP
ARRIVAL
RATE
PEAK
POWER
BUDGET
ETC
THE
QUEUEING
MODEL
ALSO
ALLOWS
US
TO
DETERMINE
THE
OPTIMAL
POWER
ALLOCATION
FOR
EVERY
POSSIBLE
CONFIGURATION
OF
THE
ABOVE
FACTORS
SEE
SECTION
OUR
THIRD
CONTRIBUTION
IS
THE
EXPERIMENTAL
IMPLEMENTA
TION
OF
OUR
SCHEMES
POWMAX
POWMIN
AND
POWMED
ON
AN
IBM
BLADECENTER
AND
THE
MEASUREMENT
OF
THEIR
RESPONSE
TIME
FOR
VARIOUS
WORKLOADS
AND
VOLTAGE
AND
FREQUENCY
SCAL
ING
MECHANISMS
SEE
SECTIONS
AND
IMPORTANTLY
OUR
EXPERIMENTS
SHOW
THAT
USING
THE
OPTIMAL
POWER
ALLOCATION
SCHEME
CAN
SIGNIFICANTLY
REDUCE
MEAN
RESPONSE
TIME
SOME
TIMES
BY
AS
MUCH
AS
A
FACTOR
OF
TO
BE
MORE
CONCRETE
WE
SHOW
A
SUBSET
OF
OUR
RESULTS
IN
FIG
WHICH
ASSUMES
A
CPU
BOUND
WORKLOAD
IN
AN
OPEN
LOOP
SETTING
FIG
A
DEPICTS
ONE
POSSIBLE
SCENARIO
USING
DFS
WHERE
POWMAX
IS
OPTIMAL
BY
CONTRAST
FIG
B
DEPICTS
A
SCENARIO
USING
DVFS
WHERE
POWMIN
IS
OPTIMAL
FOR
HIGH
ARRIVAL
RATES
LASTLY
FIG
C
DEPICTS
A
SCENARIO
USING
DVFS
DFS
WHERE
POWMED
IS
OP
TIMAL
FOR
HIGH
ARRIVAL
RATES
WE
EXPERIMENT
WITH
DIFFERENT
WORKLOADS
WHILE
SECTION
PRESENTS
EXPERIMENTAL
RESULTS
FOR
A
CPU
BOUND
WORKLOAD
LINPACK
SECTION
REPEATS
ALL
THE
EXPERIMENTS
UNDER
THE
STREAM
MEMORY
BOUND
WORKLOAD
THE
WEBBENCH
WEB
WORK
LOAD
AND
OTHER
WORKLOADS
IN
ALL
CASES
EXPERIMENTAL
RESULTS
ARE
IN
EXCELLENT
AGREEMENT
WITH
OUR
THEORETICAL
PREDICTIONS
SECTION
SUMMARIZES
OUR
WORK
FINALLY
SECTION
DISCUSSES
FUTURE
APPLICATIONS
OF
OUR
MODEL
TO
MORE
COMPLEX
SITUATIONS
SUCH
AS
WORKLOADS
WITH
VARYING
ARRIVAL
RATES
SERVERS
WITH
IDLE
LOW
POWER
STATES
AND
POWER
MANAGEMENT
AT
THE
SUB
SYSTEM
LEVEL
SUCH
AS
THE
STORAGE
SUBSYSTEM
EXPERIMENTAL
FRAMEWORK
EXPERIMENTAL
SETUP
OUR
EXPERIMENTAL
SETUP
CONSISTS
OF
A
SERVER
FARM
WITH
UP
TO
FOURTEEN
IBM
BLADECENTER
BLADE
SERVERS
FEATURING
TWO
GHZ
DUAL
CORE
INTEL
WOODCREST
XEON
PROCESSORS
AND
GB
MEMORY
PER
BLADE
ALL
RESIDING
IN
A
SINGLE
CHASSIS
WE
INSTALLED
AND
CONFIGURED
APACHE
AS
AN
APPLICATION
SERVER
ON
EACH
OF
THE
BLADE
SERVERS
TO
PROCESS
TRANSACTIONAL
REQUESTS
TO
GENERATE
HTTP
REQUESTS
FOR
THE
APACHE
WEB
SERVERS
WE
EMPLOY
AN
ADDITIONAL
BLADE
SERVER
ON
THE
SAME
CHASSIS
AS
THE
WORKLOAD
GENERATOR
TO
REDUCE
THE
EFFECTS
OF
NETWORK
LA
TENCY
THE
WORKLOAD
GENERATOR
USES
THE
WEB
SERVER
PERFOR
MANCE
BENCHMARKING
TOOL
HTTPERF
IN
THE
OPEN
SERVER
LOW
ARRIVAL
RATE
HIGH
ARRIVAL
RATE
LOW
ARRIVAL
RATE
HIGH
ARRIVAL
RATE
POWMAX
IS
BEST
B
POWMIN
IS
BEST
C
POWMED
IS
BEST
AT
HIGH
ARRIVAL
RATES
AT
HIGH
ARRIVAL
RATES
FIGURE
SUBSET
OF
OUR
RESULTS
SHOWING
THAT
NO
SINGLE
POWER
ALLOCATION
SCHEME
IS
OPTIMAL
FIG
A
DEPICTS
A
SCENARIO
USING
DFS
WHERE
POWMAX
IS
OPTIMAL
FIG
B
DEPICTS
A
SCENARIO
USING
DVFS
WHERE
POWMIN
IS
OPTIMAL
AT
HIGH
ARRIVAL
RATES
WHEREAS
POWMAX
IS
OPTIMAL
AT
LOW
ARRIVAL
RATES
FIG
C
DEPICTS
A
SCENARIO
USING
DVFS
DFS
WHERE
POWMED
IS
OPTIMAL
AT
HIGH
ARRIVAL
RATES
WHEREAS
POWMAX
IS
OPTIMAL
AT
LOW
ARRIVAL
RATES
FARM
CONFIGURATION
AND
WBOX
IN
THE
CLOSED
SERVER
FARM
CONFIGURATION
WE
MODIFIED
AND
EXTENDED
HTTPERF
AND
WBOX
TO
ALLOW
FOR
MULTIPLE
SERVERS
AND
TO
SPECIFY
THE
ROUTING
PROB
ABILITY
AMONG
THE
SERVERS
WE
MEASURE
AND
ALLOCATE
POWER
TO
THE
SERVERS
USING
IBM
AMESTER
SOFTWARE
AMESTER
ALONG
WITH
ADDITIONAL
SCRIPTS
COLLECTS
ALL
RELEVANT
DATA
FOR
OUR
EX
PERIMENTS
VOLTAGE
AND
FREQUENCY
SCALING
MECHANISMS
PROCESSORS
TODAY
ARE
COMMONLY
EQUIPPED
WITH
MECHA
NISMS
TO
REDUCE
POWER
CONSUMPTION
AT
THE
EXPENSE
OF
RE
DUCED
SERVER
FREQUENCY
COMMON
EXAMPLES
OF
THESE
MECHA
NISMS
ARE
INTEL
SPEEDSTEP
TECHNOLOGY
AND
AMD
COOL
N
QUIET
TECHNOLOGY
THE
POWER
TO
FREQUENCY
RELATIONSHIP
IN
SUCH
SERVERS
DEPENDS
ON
THE
SPECIFIC
VOLTAGE
AND
FREQUENCY
SCALING
MECHANISM
USED
MOST
MECHANISMS
FALL
UNDER
THE
FOLLOWING
THREE
CATEGORIES
DYNAMIC
FREQUENCY
SCALING
DFS
A
K
A
CLOCK
THROTTLING
OR
T
STATES
IS
A
TECHNIQUE
TO
MANAGE
POWER
BY
RUNNING
THE
PROCESSOR
AT
A
LESS
THAN
MAXIMUM
CLOCK
FRE
QUENCY
UNDER
DFS
THE
INTEL
GHZ
WOODCREST
XEON
PROCESSORS
WE
USE
ALLOW
FOR
OPERATING
POINTS
WHICH
CORRE
SPOND
TO
EFFECTIVE
FREQUENCIES
OF
AND
OF
THE
MAXIMUM
SERVER
FRE
QUENCY
DYNAMIC
VOLTAGE
AND
FREQUENCY
SCALING
DVFS
A
K
A
P
STATES
IS
A
MORE
EFFICIENT
POWER
SAVINGS
MECH
ANISM
THAT
REDUCES
SERVER
FREQUENCY
BY
REDUCING
THE
PRO
CESSOR
VOLTAGE
AND
FREQUENCY
UNDER
DVFS
OUR
PROCESSORS
ALLOW
FOR
OPERATING
POINTS
WHICH
CORRESPOND
TO
EFFECTIVE
FREQUENCIES
OF
AND
OF
THE
MAXI
MUM
SERVER
FREQUENCY
DVFS
DFS
ATTEMPTS
TO
LEVERAGE
BOTH
DVFS
AND
DFS
BY
APPLYING
DFS
ON
THE
LOWEST
PERFORMANCE
STATE
AVAILABLE
IN
DVFS
UNDER
DVFS
DFS
OUR
PROCESSORS
ALLOW
FOR
OPERATING
POINTS
WHICH
CORRESPOND
TO
EFFECTIVE
FREQUENCIES
OF
AND
OF
THE
MAXIMUM
SERVER
FREQUENCY
POWER
CONSUMPTION
WITHIN
A
SINGLE
SERVER
WHEN
ALLOCATING
POWER
TO
A
SERVER
THERE
IS
A
MINIMUM
LEVEL
OF
POWER
CONSUMPTION
B
NEEDED
TO
OPERATE
THE
PROCES
SOR
AT
THE
LOWEST
ALLOWABLE
FREQUENCY
AND
A
MAXIMUM
LEVEL
OF
POWER
CONSUMPTION
C
NEEDED
TO
OPERATE
THE
PROCESSOR
AT
THE
HIGHEST
ALLOWABLE
FREQUENCY
OF
COURSE
THE
SPECIFIC
VALUES
OF
B
AND
C
DEPEND
ON
THE
APPLICATION
THAT
THE
SERVER
IS
RUNNING
FORMALLY
WE
DEFINE
THE
FOLLOWING
NOTATION
BASELINE
POWER
B
WATTS
THE
MINIMUM
POWER
CONSUMED
BY
A
FULLY
UTILIZED
SERVER
OVER
THE
ALLOWABLE
RANGE
OF
PROCES
SOR
FREQUENCY
SPEED
AT
BASELINE
SB
HERTZ
THE
SPEED
OR
FREQUENCY
OF
A
FULLY
UTILIZED
SERVER
RUNNING
AT
B
WATTS
MAXIMUM
POWER
C
WATTS
THE
MAXIMUM
POWER
CON
SUMED
BY
A
FULLY
UTILIZED
SERVER
OVER
THE
ALLOWABLE
RANGE
OF
PROCESSOR
FREQUENCY
SPEED
AT
MAXIMUM
POWER
SC
HERTZ
THE
SPEED
OR
FRE
QUENCY
OF
A
FULLY
UTILIZED
SERVER
RUNNING
AT
C
WATTS
POWER
TO
FREQUENCY
AN
INTEGRAL
PART
OF
DETERMINING
THE
OPTIMAL
POWER
ALLO
CATION
IS
TO
UNDERSTAND
THE
POWER
TO
FREQUENCY
RELATIONSHIP
THIS
RELATIONSHIP
DIFFERS
FOR
DFS
DVFS
AND
DVFS
DFS
AND
ALSO
DIFFERS
BASED
ON
THE
WORKLOAD
IN
USE
UNFORTUNATELY
THE
FUNCTIONAL
FORM
OF
THE
POWER
TO
FREQUENCY
RELATIONSHIP
IS
NOT
WELL
STUDIED
IN
THE
LITERATURE
THE
SERVERS
WE
USE
SUP
PORT
ALL
THREE
VOLTAGE
AND
FREQUENCY
SCALING
MECHANISMS
AND
THEREFORE
CAN
BE
USED
TO
STUDY
THE
POWER
TO
FREQUENCY
RELA
TIONSHIPS
IN
THIS
SECTION
WE
PRESENT
OUR
MEASUREMENTS
DE
PICTED
IN
FIGS
A
AND
B
SHOWING
THE
FUNCTIONAL
RELATION
SHIP
BETWEEN
POWER
ALLOCATED
TO
A
SERVER
AND
ITS
FREQUENCY
FOR
THE
LINPACK
WORKLOAD
WE
GENERALIZE
THE
FUNC
TIONAL
FORM
OF
THE
POWER
TO
FREQUENCY
RELATIONSHIP
TO
OTHER
WORKLOADS
IN
SECTION
SEE
FIGS
AND
THROUGHOUT
WE
ASSUME
A
HOMOGENEOUS
SERVER
FARM
WE
USE
THE
TOOLS
DEVELOPED
IN
TO
LIMIT
THE
MAXI
MUM
POWER
ALLOCATED
TO
EACH
SERVER
LIMITING
THE
MAXIMUM
POWER
ALLOCATED
TO
A
SERVER
IS
USUALLY
REFERRED
TO
AS
CAPPING
THE
POWER
ALLOCATED
TO
A
SERVER
WE
RUN
LINPACK
JOBS
BACK
TO
BACK
TO
ENSURE
THAT
THE
SERVER
IS
ALWAYS
OCCUPIED
BY
THE
WORKLOAD
AND
THAT
THE
SERVER
IS
RUNNING
AT
THE
SPECIFIED
POWER
CAP
VALUE
HENCE
THE
POWER
VALUES
WE
OBSERVE
WILL
BE
THE
PEAK
POWER
VALUES
FOR
THE
SPECIFIED
WORKLOAD
RECALL
FROM
SECTION
THAT
THE
VOLTAGE
AND
FREQUENCY
SCALING
MECH
ANISMS
HAVE
CERTAIN
DISCRETE
PERFORMANCE
POINTS
IN
TERMS
OF
FREQUENCY
AT
WHICH
THE
SERVER
CAN
OPERATE
AT
EACH
OF
THESE
PERFORMANCE
POINTS
THE
SERVER
CONSUMES
A
CERTAIN
AMOUNT
OF
POWER
FOR
A
GIVEN
WORKLOAD
BY
QUICKLY
DITHERING
BETWEEN
AVAILABLE
PERFORMANCE
STATES
WE
CAN
ENSURE
THAT
THE
SERVER
NEVER
CONSUMES
MORE
THAN
THE
SET
POWER
CAP
VALUE
IN
THIS
WAY
WE
ALSO
GET
THE
BEST
PERFORMANCE
FROM
THE
SERVER
FOR
THE
GIVEN
POWER
CAP
VALUE
NOTE
THAT
WHEN
WE
SAY
POWER
WE
MEAN
THE
SYSTEM
LEVEL
POWER
WHICH
INCLUDES
THE
POWER
CONSUMED
BY
THE
PROCESSOR
AND
ALL
OTHER
COMPONENTS
WITHIN
THE
SERVER
FIG
A
ILLUSTRATES
THE
POWER
TO
FREQUENCY
CURVES
OBTAINED
FOR
LINPACK
USING
DFS
AND
DVFS
FROM
THE
FIGURE
WE
SEE
THAT
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
BOTH
DFS
AND
DVFS
IS
ALMOST
LINEAR
IT
MAY
SEEM
SURPRISING
THAT
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
DVFS
LOOKS
LIKE
A
LIN
EAR
PLOT
THIS
IS
OPPOSITE
TO
WHAT
IS
WIDELY
SUGGESTED
IN
THE
LITERATURE
FOR
THE
PROCESSOR
POWER
TO
FREQUENCY
RELATION
SHIP
WHICH
IS
CUBIC
THE
REASON
WHY
THE
SERVER
POWER
TO
FREQUENCY
RELATIONSHIP
IS
LINEAR
CAN
BE
EXPLAINED
BY
TWO
INTERRELATED
FACTORS
FIRST
MANUFACTURERS
USUALLY
SETTLE
ON
A
LIMITED
NUMBER
OF
ALLOWED
VOLTAGE
LEVELS
OR
PERFORMANCE
STATES
WHICH
RESULTS
IN
A
LESS
THAN
IDEAL
RELATIONSHIP
BE
TWEEN
POWER
AND
FREQUENCY
IN
PRACTICE
SECOND
DVFS
IS
NOT
APPLIED
ON
MANY
COMPONENTS
AT
THE
SYSTEM
LEVEL
FOR
EXAMPLE
POWER
CONSUMPTION
IN
MEMORY
REMAINS
PROPOR
TIONAL
TO
THE
NUMBER
OF
REFERENCES
TO
MEMORY
PER
UNIT
TIME
WHICH
IS
ONLY
LINEARLY
RELATED
TO
THE
FREQUENCY
OF
THE
PRO
CESSOR
THUS
THE
POWER
TO
FREQUENCY
CURVE
FOR
BOTH
DFS
AND
DVFS
CAN
BE
APPROXIMATED
AS
A
LINEAR
FUNCTION
USING
THE
TERMINOLOGY
INTRODUCED
IN
SECTION
WE
APPROXIMATE
THE
SERVER
SPEED
OR
FREQUENCY
GHZ
AS
A
FUNCTION
OF
THE
POWER
ALLOCATED
TO
IT
P
WATTS
AS
SB
Α
P
B
WHERE
THE
COEFFICIENT
Α
UNITS
OF
GHZ
PER
WATT
IS
THE
SLOPE
OF
THE
POWER
TO
FREQUENCY
CURVE
SPECIFICALLY
OUR
EXPERI
MENTS
SHOW
THAT
Α
GHZ
W
FOR
DVFS
AND
Α
GHZ
W
FOR
DFS
ALSO
FROM
OUR
EXPERIMENTS
WE
FIND
THAT
B
AND
C
FOR
BOTH
DVFS
AND
DFS
HOW
EVER
SB
GHZ
FOR
DVFS
AND
SB
GHZ
FOR
DFS
THE
MAXIMUM
SPEED
IN
BOTH
CASES
IS
SC
GHZ
WHICH
IS
SIMPLY
THE
MAXIMUM
SPEED
OF
THE
PROCESSOR
WE
USE
NOTE
THAT
THE
SPECIFIC
VALUES
OF
THESE
PARAMETERS
CHANGE
DEPEND
ING
ON
THE
WORKLOAD
IN
USE
SECTION
DISCUSSES
OUR
MEASURED
PARAMETER
VALUES
FOR
DIFFERENT
WORKLOADS
FOR
DVFS
DFS
WE
EXPECT
THE
POWER
TO
FREQUENCY
RELA
TIONSHIP
TO
BE
PIECEWISE
LINEAR
SINCE
IT
IS
A
COMBINATION
OF
DVFS
AND
DFS
EXPERIMENTALLY
WE
SEE
FROM
FIG
B
THAT
THE
POWER
TO
FREQUENCY
RELATIONSHIP
IS
IN
FACT
PIECEWISE
LIN
EAR
GHZ
GHZ
AND
THEN
GHZ
GHZ
THOUGH
WE
COULD
USE
A
PIECEWISE
LINEAR
FIT
FOR
DVFS
DFS
WE
CHOOSE
TO
APPROXIMATE
IT
USING
A
CUBIC
CURVE
FOR
THE
FOL
LOWING
REASONS
USING
A
CUBIC
FIT
DEMONSTRATES
HOW
WE
CAN
EXTEND
OUR
RESULTS
TO
NON
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIPS
POWER
TO
FREQUENCY
RELATIONSHIP
TO
BE
CUBIC
ESPECIALLY
FOR
THE
PROCESSOR
BY
USING
A
CUBIC
MODEL
FOR
DVFS
DFS
WE
WISH
TO
ANALYZE
THE
OPTIMAL
POWER
ALLOCATION
POLICY
FOR
THOSE
SETTINGS
APPROXIMATING
DVFS
DFS
USING
A
CUBIC
FIT
GIVES
THE
FOL
LOWING
RELATIONSHIP
BETWEEN
THE
SPEED
OF
A
SERVER
AND
THE
POWER
ALLOCATED
TO
IT
SB
ΑL
P
B
SPECIFICALLY
OUR
EXPERIMENTS
SHOW
THAT
ΑL
GHZ
W
ALSO
FROM
OUR
EXPERIMENTS
WE
FOUND
THAT
B
C
SB
GHZ
AND
SC
GHZ
FOR
DVFS
DFS
THEORETICAL
RESULTS
THE
OPTIMAL
POWER
ALLOCATION
DEPENDS
ON
A
LARGE
NUM
BER
OF
FACTORS
INCLUDING
THE
POWER
TO
FREQUENCY
RELATIONSHIP
JUST
DISCUSSED
IN
FIG
THE
ARRIVAL
RATE
THE
MINIMUM
AND
MAXIMUM
POWER
CONSUMPTION
LEVELS
B
AND
C
RESPECTIVELY
WHETHER
THE
SERVER
FARM
HAS
AN
OPEN
LOOP
CONFIGURATION
OR
A
CLOSED
LOOP
CONFIGURATION
ETC
IN
ORDER
TO
INVESTIGATE
THE
EFFECTS
OF
ALL
THESE
FACTORS
ON
THE
OPTIMAL
POWER
ALLOCATION
WE
DEVELOP
A
QUEUEING
THE
ORETIC
MODEL
WHICH
PREDICTS
THE
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
ALL
THE
ABOVE
FACTORS
SEE
SECTION
WE
THEN
PRODUCE
THEOREMS
THAT
DETERMINE
THE
OPTIMAL
POWER
ALLO
CATION
FOR
EVERY
POSSIBLE
CONFIGURATION
OF
THE
ABOVE
FACTORS
INCLUDING
OPEN
LOOP
CONFIGURATION
SEE
THEOREMS
AND
IN
SECTION
AND
CLOSED
LOOP
CONFIGURATION
SEE
THEOREMS
AND
IN
SECTION
QUEUEING
MODEL
FIGURE
ILLUSTRATION
OF
OUR
K
SERVER
FARM
MODEL
FIG
ILLUSTRATES
OUR
QUEUEING
MODEL
FOR
A
SERVER
FARM
WITH
K
SERVERS
WE
ASSUME
THAT
THERE
IS
A
FIXED
POWER
BUD
GET
P
WHICH
CAN
BE
SPLIT
A
MONG
THE
K
SERVERS
ALLOCATING
PI
POWER
TO
SERVER
I
WHERE
I
PI
P
THE
CORRESPOND
AS
PREVIOUSLY
MENTIONED
SEVERAL
PAPERS
CONSIDER
THE
ING
SERVER
SPEEDS
ARE
DENOTED
BY
SK
EACH
SERVER
DFS
AND
DVFS
B
DVFS
DFS
FIGURE
POWER
TO
FREQUENCY
CURVES
FOR
DFS
DVFS
AND
DVFS
DFS
FOR
THE
CPU
BOUND
LINPACK
WORKLOAD
FIG
A
ILLUSTRATES
OUR
MEASUREMENTS
FOR
DFS
AND
DVFS
IN
BOTH
THESE
MECHANISMS
WE
SEE
THAT
THE
SERVER
FREQUENCY
IS
LINEARLY
RELATED
TO
THE
POWER
ALLOCATED
TO
THE
SERVER
FIG
B
ILLUSTRATES
OUR
MEASUREMENTS
FOR
DVFS
DFS
WHERE
THE
POWER
TO
FREQUENCY
CURVE
IS
BETTER
APPROXIMATED
BY
A
CUBIC
RELATIONSHIP
I
RECEIVES
A
FRACTION
QI
OF
THE
TOTAL
WORKLOAD
COMING
IN
TO
THE
SERVER
FARM
CORRESPONDING
TO
ANY
VECTOR
OF
POWER
ALLO
CATION
K
THERE
EXISTS
AN
OPTIMAL
WORKLOAD
ALLO
CATION
VECTOR
QK
WE
DERIVE
THE
OPTIMAL
WORKLOAD
ALLOCATION
FOR
EACH
POWER
ALLOCATION
AND
USE
THAT
VECTOR
QK
BOTH
IN
THEORY
AND
IN
THE
ACTUAL
EXPERIMENTS
THE
DETAILS
OF
HOW
WE
OBTAIN
THE
OPTIMAL
QK
ARE
DEFERRED
TO
THE
APPENDIX
OUR
MODEL
ASSUMES
THAT
THE
JOBS
AT
A
SERVER
ARE
SCHEDULED
USING
THE
PROCESSOR
SHARING
PS
SCHEDULING
DISCIPLINE
UN
DER
PS
WHEN
THERE
ARE
N
JOBS
AT
A
SERVER
THEY
EACH
RECEIVE
NTH
OF
THE
SERVER
CAPACITY
PS
IS
IDENTICAL
TO
ROUND
ROBIN
WITH
QUANTUMS
AS
IN
LINUX
WHEN
THE
QUANTUM
SIZE
AP
PROACHES
ZERO
A
JOB
RESPONSE
TIME
T
IS
THE
TIME
FROM
WHEN
THE
JOB
ARRIVES
UNTIL
IT
HAS
COMPLETED
SERVICE
INCLUD
ING
WAITING
TIME
WE
AIM
TO
MINIMIZE
MEAN
RESPONSE
TIME
E
T
WE
WILL
ANALYZE
OUR
SERVER
FARM
MODEL
UNDER
BOTH
AN
OPEN
LOOP
CONFIGURATION
SEE
SECTION
AND
A
CLOSED
LOOP
CON
FIGURATION
SEE
SECTION
AN
OPEN
LOOP
CONFIGURATION
IS
ONE
IN
WHICH
JOBS
ARRIVE
FROM
OUTSIDE
THE
SYSTEM
AND
LEAVE
THE
SYSTEM
AFTER
THEY
COMPLETE
SERVICE
WE
ASSUME
THAT
THE
ARRIVAL
PROCESS
IS
POISSON
WITH
AVERAGE
RATE
Λ
JOBS
SEC
SOMETIMES
IT
WILL
BE
CONVENIENT
TO
INSTEAD
EXPRESS
Λ
IN
UNITS
OF
GHZ
THIS
CONVERSION
IS
EASILY
ACHIEVABLE
SINCE
AN
AVERAGE
JOB
HAS
SIZE
E
GIGACYCLES
IN
THE
THEOREMS
PRE
SENTED
IN
THE
PAPER
Λ
IS
IN
THE
UNITS
OF
GHZ
HOWEVER
IN
THE
TIMAL
POWER
ALLOCATION
IS
NON
TRIVIAL
COMPUTING
E
T
FOR
A
GIVEN
ALLOCATION
IS
EASY
HENCE
WE
OMIT
SHOWING
THE
MEAN
RESPONSE
TIME
IN
EACH
CASE
AND
REFER
THE
READER
TO
THE
AP
PENDIX
DUE
TO
LACK
OF
SPACE
WE
DEFER
ALL
PROOFS
TO
THE
APPENDIX
HOWEVER
WE
PRESENT
THE
INTUITION
BEHIND
THE
THEOREMS
IN
EACH
CASE
RECALL
FROM
SECTION
THAT
EACH
FULLY
UTILIZED
SERVER
HAS
A
MINIMUM
POWER
CONSUMPTION
OF
B
WATTS
AND
MAXIMUM
POWER
CONSUMPTION
OF
C
WATTS
TO
ILLUSTRATE
OUR
RESULTS
CLEARLY
WE
SHALL
ASSUME
THROUGH
OUT
THIS
SECTION
AND
THE
APPENDIX
THAT
THE
POWER
BUDGET
IS
SUCH
THAT
POWMAX
ALLOWS
US
TO
RUN
N
SERVERS
EACH
AT
POWER
C
AND
POWMIN
ALLOWS
US
TO
RUN
M
SERVERS
EACH
AT
POWER
B
THIS
IS
EQUIVALENT
TO
SAYING
P
M
B
N
C
WHERE
M
AND
N
ARE
LESS
THAN
OR
EQUAL
TO
K
OBVIOUSLY
M
N
THEOREMS
FOR
OPEN
LOOP
CONFIGURATIONS
THEOREM
DERIVES
THE
OPTIMAL
POWER
ALLOCATION
IN
AN
OPEN
LOOP
CONFIGURATION
FOR
A
LINEAR
POWER
TO
FREQUENCY
RELATION
SHIP
AS
IS
THE
CASE
FOR
DFS
AND
DVFS
IN
SUCH
CASES
THE
SERVER
FREQUENCY
VARIES
WITH
THE
POWER
ALLOCATED
TO
IT
AS
SI
SB
Α
I
B
THE
THEOREM
SAYS
THAT
IF
THE
SPEED
AT
BASELINE
SB
IS
SUFFICIENTLY
LOW
THEN
POWMAX
IS
OPTIMAL
BY
CONTRAST
IF
SB
IS
HIGH
THEN
POWMIN
IS
OPTIMAL
FOR
HIGH
ARRIVAL
RATES
AND
POWMAX
IS
OPTIMAL
FOR
LOW
ARRIVAL
RATES
IF
I
IS
THE
SPEED
OF
SERVER
I
WHEN
RUN
AT
POWER
PI
THEN
THE
TO
JOBS
SEC
LIKEWISE
WHILE
IT
IS
COMMON
FOR
US
TO
EXPRESS
THE
SPEED
OF
THE
SERVER
IN
GHZ
WE
SOMETIMES
SWITCH
TO
JOBS
SEC
IN
THE
APPENDIX
WHEN
CONVENIENT
A
CLOSED
LOOP
CONFIGURATION
IS
ONE
IN
WHICH
THERE
ARE
ALWAYS
A
FIXED
NUMBER
OF
USERS
N
ALSO
REFERRED
TO
AS
THE
MULTI
PROGRAMMING
LEVEL
THEOREM
GIVEN
AN
OPEN
K
SERVER
FARM
CONFIGURATION
WITH
A
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIP
GIVEN
BY
EQ
AND
POWER
BUDGET
THE
FOLLOWING
POWER
ALLOCATION
MINI
MIZES
E
T
IF
SB
Α
P
C
P
WHO
EACH
SUBMIT
ONE
JOB
TO
THE
SERVER
ONCE
A
USER
JOB
IS
B
N
P
N
N
K
C
P
IF
Λ
Λ
IN
ALL
OF
THE
THEOREMS
THAT
FOLLOW
WE
FIND
THE
OPTIMAL
POWER
ALLOCATION
K
FOR
A
K
SERVER
FARM
WHICH
MINIMIZES
THE
MEAN
RESPONSE
TIME
E
T
GIVEN
THE
FIXED
PEAK
POWER
BUDGET
P
K
PI
WHILE
DERIVING
THE
OP
WHERE
ΛLOW
Α
P
COROLLARY
FOR
DFS
POWMAX
IS
OPTIMAL
FOR
DVFS
POWMAX
IS
OPTIMAL
AT
LOW
ARRIVAL
RATES
AND
POWMIN
IS
OP
TIMAL
AT
HIGH
ARRIVAL
RATES
INTUITION
FOR
A
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIP
WE
HAVE
FROM
EQ
THAT
THE
SPEED
OF
A
SERVER
SI
VARIES
WITH
THE
POWER
ALLOCATED
TO
IT
PI
AS
SI
SB
Α
PI
B
BY
EQS
AND
THE
FOLLOWING
POWER
ALLOCATION
MINI
MIZES
E
T
FOR
LOW
N
BASED
ON
THE
ASYMPTOTIC
APPROXIMA
TIONS
IN
FROM
THIS
EQUATION
IT
FOLLOWS
THAT
THE
FREQUENCY
PER
WATT
FOR
A
SINGLE
SERVER
SI
CAN
BE
WRITTEN
AS
PI
SI
SB
ΑB
Α
PI
PI
HENCE
MAXIMIZING
THE
FREQUENCY
PER
WATT
DEPENDS
ON
WHETHER
SB
ΑB
OR
SB
ΑB
IF
SB
ΑB
MAXIMIZING
IS
EQUIVALENT
N
C
PN
N
K
COROLLARY
FOR
A
CLOSED
LOOP
SERVER
FARM
CONFIGURA
TION
WITH
LOW
N
POWMAX
IS
OPTIMAL
FOR
DFS
DVFS
AND
DVFS
DFS
TO
MAXIMIZING
P
PI
I
WHICH
IS
ACHIEVED
BY
POWMAX
ALTERNA
INTUITION
WHEN
N
IS
SUFFICIENTLY
LOW
THERE
ARE
VERY
FEW
TIVELY
IF
SB
ΑB
WE
WANT
TO
MINIMIZE
I
WHICH
IS
ACHIEVED
BY
POWMIN
HOWEVER
THE
ABOVE
ARGUMENT
STILL
DOES
NOT
TAKE
INTO
ACCOUNT
THE
MEAN
ARRIVAL
RATE
Λ
IF
Λ
IS
SUFFICIENTLY
LOW
THERE
ARE
VERY
FEW
JOBS
IN
THE
SERVER
FARM
HENCE
FEW
FAST
SERVERS
OR
POWMAX
IS
OPTIMAL
THE
COROLLARY
FOLLOWS
BY
SIMPLY
PLUGGING
IN
THE
VALUES
OF
SB
Α
AND
B
FOR
DFS
AND
DVFS
FROM
SECTION
THEOREM
DERIVES
THE
OPTIMAL
POWER
ALLOCATION
FOR
NON
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIPS
SUCH
AS
THE
CUBIC
RELATIONSHIP
IN
THE
CASE
OF
DVFS
DFS
IN
SUCH
CASES
THE
SERVER
FREQUENCY
VARIES
WITH
THE
POWER
ALLOCATED
TO
IT
AS
SI
SB
ΑL
I
B
THE
THEOREM
SAYS
THAT
IF
THE
ARRIVAL
RATE
IS
SUFFICIENTLY
LOW
THEN
POWMAX
IS
OPTIMAL
HOWEVER
IF
THE
ARRIVAL
RATE
IS
HIGH
POWMED
IS
OPTIMAL
ALTHOUGH
THE
OREM
SPECIFIES
A
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
WE
CONJECTURE
THAT
SIMILAR
RESULTS
HOLD
FOR
MORE
GENERAL
POWER
TO
FREQUENCY
CURVES
WHERE
SERVER
FREQUENCY
VARIES
AS
THE
N
TH
JOBS
IN
THE
SYSTEM
HENCE
FEW
FAST
SERVERS
ARE
OPTIMAL
SINCE
THERE
AREN
T
ENOUGH
JOBS
TO
UTILIZE
THE
SERVERS
LEAVING
SERVERS
IDLE
THUS
POWMAX
IS
OPTIMAL
THIS
IS
SIMILAR
TO
THE
CASE
OF
LOW
ARRIVAL
RATE
THAT
WE
CONSIDERED
FOR
AN
OPEN
LOOP
SERVER
FARM
CONFIGURATION
IN
THEOREMS
AND
WHEN
N
IS
HIGH
THE
OPTIMAL
POWER
ALLOCATION
IS
NON
TRIVIAL
FROM
HERE
ON
WE
ASSUME
N
IS
LARGE
ENOUGH
TO
KEEP
ALL
SERVERS
BUSY
SO
THAT
ASYMPTOTIC
BOUNDS
APPLY
SEE
IN
OUR
EXPERIMENTS
WE
FIND
THAT
N
SUFFICES
THEOREM
SAYS
THAT
FOR
HIGH
N
IF
THE
SPEED
AT
BASELINE
SB
IS
SUFFICIENTLY
LOW
THEN
POWMAX
IS
OPTIMAL
BY
CONTRAST
IF
SB
IS
HIGH
THEN
POWMIN
IS
OPTIMAL
THEOREM
GIVEN
A
CLOSED
K
SERVER
FARM
CONFIGURATION
WITH
A
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIP
GIVEN
BY
EQ
THE
FOLLOWING
POWER
ALLOCATION
MINIMIZES
E
T
FOR
HIGH
N
BASED
ON
THE
ASYMPTOTIC
APPROXIMATIONS
IN
IF
SB
Α
N
C
P
IF
SB
Α
M
B
P
B
THEOREM
GIVEN
AN
OPEN
K
SERVER
FARM
CONFIGURATION
M
M
K
WITH
A
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
GIVEN
BY
EQ
AND
POWER
BUDGET
THE
FOLLOWING
POWER
ALLOCATION
MINI
MIZES
E
T
N
C
PN
N
K
IF
Λ
ΛLLOW
L
PL
PL
L
K
IF
Λ
ΛLLOW
COROLLARY
FOR
DFS
POWMAX
IS
OPTIMAL
FOR
HIGH
N
FOR
DVFS
POWMIN
IS
OPTIMAL
FOR
HIGH
N
INTUITION
FOR
A
CLOSED
QUEUEING
SYSTEM
WITH
ZERO
THINK
TIME
THE
MEAN
RESPONSE
TIME
IS
INVERSELY
PROPORTIONAL
TO
I
THE
THROUGHPUT
OF
THE
SYSTEM
HENCE
TO
MINIMIZE
THE
MEAN
B
COROLLARY
FOR
DVFS
DFS
POWMAX
IS
OPTIMAL
AT
LOW
ARRIVAL
RATES
AND
POWMED
IS
OPTIMAL
AT
HIGH
ARRIVAL
RATES
INTUITION
WHEN
THE
ARRIVAL
RATE
IS
SUFFICIENTLY
LOW
THERE
ARE
VERY
FEW
JOBS
IN
THE
SYSTEM
HENCE
POWMAX
IS
OPTIMAL
HOWEVER
FOR
HIGHER
ARRIVAL
RATES
WE
ALLOCATE
TO
EACH
SERVER
THE
AMOUNT
OF
POWER
THAT
MAXIMIZES
ITS
FREQUENCY
PER
WATT
RATIO
FOR
THE
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
WHICH
HAS
A
DOWNWARDS
CONCAVE
CURVE
SEE
FIG
B
WE
FIND
THAT
THE
OPTIMAL
POWER
ALLOCATION
VALUE
FOR
EACH
SERVER
DERIVED
VIA
CALCULUS
LIES
BETWEEN
THE
MAXIMUM
C
AND
THE
MINIMUM
B
HENCE
POWMED
IS
OPTIMAL
THEOREMS
FOR
CLOSED
LOOP
CONFIGURATIONS
WE
NOW
MOVE
ON
TO
CLOSED
LOOP
CONFIGURATIONS
WHERE
THE
A
SERVER
FARM
CONFIGURATION
ALL
SERVERS
ARE
BUSY
HENCE
THE
THROUGHPUT
IS
THE
SUM
OF
THE
SERVER
SPEEDS
IT
CAN
BE
EASILY
SHOWN
THAT
THE
THROUGHPUT
OF
THE
SYSTEM
UNDER
POWMIN
SB
M
EXCEEDS
THE
THROUGHPUT
OF
THE
SYSTEM
UNDER
POWMAX
SC
N
WHEN
SB
ΑB
HENCE
THE
RESULT
THEOREM
DEALS
WITH
THE
CASE
OF
HIGH
N
FOR
A
NON
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIP
THE
THEOREM
SAYS
THAT
IF
THE
SPEED
AT
BASELINE
SB
IS
SUFFICIENTLY
LOW
THEN
POWMAX
IS
OPTIMAL
BY
CONTRAST
IF
SB
IS
HIGH
THEN
POWMED
IS
OPTIMAL
THEOREM
GIVEN
A
CLOSED
K
SERVER
FARM
CONFIGURATION
WITH
A
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
GIVEN
BY
EQ
THE
FOLLOWING
POWER
ALLOCATION
MINIMIZES
E
T
FOR
HIGH
N
BASED
ON
THE
ASYMPTOTIC
APPROXIMATIONS
IN
IF
SB
SL
N
C
PN
N
K
IF
SB
SL
L
B
X
PL
L
K
WHERE
L
P
SL
MSC
ΑL
X
AND
X
IS
THE
NON
WILL
RELY
ON
ASYMPTOTIC
OPERATIONAL
LAWS
SEE
WHICH
APPROXIMATE
THE
PERFORMANCE
OF
THE
SYSTEM
FOR
VERY
HIGH
N
AND
VERY
LOW
N
SEE
APPENDIX
THEOREM
SAYS
THAT
FOR
A
CLOSED
SERVER
FARM
CONFIGURATION
WITH
SUFFICIENTLY
LOW
VALUE
OF
N
POWMAX
IS
OPTIMAL
THEOREM
GIVEN
A
CLOSED
K
SERVER
FARM
CONFIGURATION
WITH
A
LINEAR
OR
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
GIVEN
NEGATIVE
REAL
SOLUTION
OF
THE
EQUATION
B
ΑJ
SB
COROLLARY
FOR
DVFS
DFS
FOR
HIGH
N
POWMED
IS
OPTIMAL
IF
SB
IS
HIGH
ELSE
POWMAX
IS
OPTIMAL
INTUITION
AS
IN
THE
CASE
OF
THEOREM
WE
WISH
TO
MAXIMIZE
THE
THROUGHPUT
OF
THE
SYSTEM
WHEN
WE
TURN
ON
A
NEW
DFS
B
DVFS
FIGURE
OPEN
LOOP
EXPERIMENTAL
RESULTS
FOR
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
ARRIVAL
RATE
USING
DFS
AND
DVFS
FOR
LINPACK
IN
FIG
A
POWMAX
OUTPERFORMS
POWMIN
FOR
ALL
ARRIVAL
RATES
UNDER
DFS
BY
AS
MUCH
AS
A
FACTOR
OF
BY
CONTRAST
IN
FIG
B
FOR
DVFS
AT
LOWER
ARRIVAL
RATES
POWMAX
OUTPERFORMS
POWMIN
BY
UP
TO
WHILE
AT
HIGHER
ARRIVAL
RATES
POWMIN
OUTPERFORMS
POWMAX
BY
UP
TO
SERVER
AT
B
UNITS
OF
POWER
THE
INCREASE
IN
THROUGHPUT
OF
THE
SYSTEM
IS
SB
GHZ
FOR
LOW
VALUES
OF
SB
THIS
INCREASE
IS
SMALL
HENCE
FOR
LOW
VALUES
OF
SB
WE
WISH
TO
TURN
ON
AS
FEW
SERVERS
AS
POSSIBLE
THUS
POWMAX
IS
OPTIMAL
HOWEVER
WHEN
SB
IS
HIGH
IT
PAYS
TO
TURN
SERVERS
ON
ONCE
A
SERVER
IS
ON
THE
INITIAL
STEEP
INCREASE
IN
FREQUENCY
PER
WATT
AFFORDED
BY
A
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
ADVOCATES
RUNNING
THE
SERVER
AT
MORE
THAN
THE
MINIMAL
POWER
B
THE
EXACT
OPTIMAL
POWMED
POWER
VALUE
B
X
IN
THE
THEOREM
IS
CLOSE
TO
THE
KNEE
OF
THE
CUBIC
POWER
TO
FREQUENCY
CURVE
EXPERIMENTAL
RESULTS
IN
THIS
SECTION
WE
TEST
OUR
THEORETICAL
RESULTS
FROM
SEC
TION
ON
AN
IBM
BLADECENTER
USING
THE
EXPERIMENTAL
SETUP
DISCUSSED
IN
SECTION
WE
SHALL
FIRST
PRESENT
OUR
EXPERIMEN
TAL
RESULTS
FOR
THE
OPEN
SERVER
FARM
CONFIGURATION
AND
THEN
MOVE
ON
TO
THE
CLOSED
SERVER
FARM
CONFIGURATION
FOR
THE
EXPERIMENTS
IN
THIS
SECTION
WE
USE
THE
INTEL
LINPACK
WORKLOAD
WHICH
IS
CPU
BOUND
WE
DEFER
EXPERIMENTAL
RE
SULTS
FOR
OTHER
WORKLOADS
TO
SECTION
AS
NOTED
IN
SECTION
THE
BASELINE
POWER
LEVEL
AND
THE
MAXIMUM
POWER
LEVEL
FOR
BOTH
DFS
AND
DVFS
ARE
B
AND
C
RESPECTIVELY
FOR
DVFS
DFS
B
AND
C
IN
EACH
OF
OUR
EXPERIMENTS
WE
TRY
TO
FIX
THE
POWER
BUDGET
TO
BE
AN
INTEGER
MULTIPLE
OF
B
AND
C
AS
IN
EQ
OPEN
SERVER
FARM
CONFIGURATION
FIG
A
PLOTS
THE
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
ARRIVAL
RATE
FOR
DFS
WITH
A
POWER
BUDGET
OF
IN
THIS
CASE
POWMAX
REPRESENTED
BY
THE
DASHED
LINE
DE
NOTES
RUNNING
SERVERS
AT
C
AND
TURNING
OFF
ALL
OTHER
SERVERS
POWMIN
REPRESENTED
BY
THE
SOLID
LINE
DE
NOTES
RUNNING
SERVERS
AT
B
AND
TURNING
OFF
ALL
OTHER
SERVERS
CLEARLY
POWMAX
OUTPERFORMS
POWMIN
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
THIS
IS
IN
AGREEMENT
WITH
THE
PREDICTIONS
OF
THEOREM
NOTE
FROM
FIG
A
THAT
THE
IM
PROVEMENT
IN
MEAN
RESPONSE
TIME
AFFORDED
BY
POWMAX
OVER
MEAN
ARRIVAL
RATE
JOBS
SEC
FIGURE
OPEN
LOOP
EXPERIMENTAL
RESULTS
FOR
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
ARRIVAL
RATE
USING
DVFS
DFS
FOR
LINPACK
AT
LOWER
ARRIVAL
RATES
POWMAX
OUTPERFORMS
POWMED
BY
UP
TO
WHILE
AT
HIGHER
ARRIVAL
RATES
POWMED
OUTPERFORMS
POWMAX
BY
UP
TO
NOTE
THAT
POWMIN
IS
WORSE
THAN
BOTH
POWMED
AND
POWMAX
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
POWMIN
IS
HUGE
RANGING
FROM
A
FACTOR
OF
AT
LOW
ARRIVAL
RATES
LOAD
Ρ
TO
AS
MUCH
AS
A
FACTOR
OF
AT
HIGH
ARRIVAL
RATES
LOAD
Ρ
THIS
IS
BECAUSE
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
DFS
IS
STEEP
SEE
FIG
A
HENCE
RUNNING
SERVERS
AT
MAXIMUM
POWER
LEVELS
AFFORDS
A
HUGE
GAIN
IN
SERVER
FREQUENCY
ARRIVAL
RATES
HIGHER
THAN
JOBS
SEC
CAUSE
OUR
SYSTEMS
TO
OVERLOAD
UNDER
POWMIN
BECAUSE
SB
IS
VERY
LOW
FOR
DFS
HENCE
WE
ONLY
GO
AS
HIGH
AS
JOBS
SEC
FIG
B
PLOTS
THE
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
ARRIVAL
RATE
FOR
DVFS
WITH
A
POWER
BUDGET
OF
POWMAX
REPRESENTED
BY
THE
DASHED
LINE
AGAIN
DENOTES
RUN
NING
SERVERS
AT
C
AND
TURNING
OFF
ALL
OTHER
SERVERS
MULTI
PROGRAMMING
LEVEL
N
MULTI
PROGRAMMING
LEVEL
N
DFS
B
DVFS
FIGURE
CLOSED
LOOP
EXPERIMENTAL
RESULTS
FOR
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
NUMBER
OF
JOBS
N
IN
THE
SYSTEM
USING
DFS
AND
DVFS
FOR
LINPACK
IN
FIG
A
FOR
DFS
POWMAX
OUTPERFORMS
POWMIN
FOR
ALL
VALUES
OF
N
BY
ALMOST
A
FACTOR
OF
THROUGHOUT
BY
CONTRAST
IN
FIG
B
FOR
DVFS
AT
LOWER
VALUES
OF
N
POWMAX
IS
SLIGHTLY
BETTER
THAN
POWMIN
WHILE
AT
HIGHER
VALUES
OF
N
POWMIN
OUTPERFORMS
POWMAX
BY
ALMOST
POWMIN
REPRESENTED
BY
THE
SOLID
LINE
DENOTES
RUNNING
SERVERS
AT
B
AND
TURNING
OFF
ALL
OTHER
SERVERS
WE
SEE
THAT
WHEN
THE
ARRIVAL
RATE
IS
LOW
POWMAX
PRODUCES
LOWER
MEAN
RESPONSE
TIMES
THAN
POWMIN
IN
PARTICULAR
WHEN
THE
ARRIVAL
RATE
IS
JOBS
SEC
POWMAX
AFFORDS
A
IM
PROVEMENT
IN
MEAN
RESPONSE
TIME
OVER
POWMIN
HOWEVER
AT
HIGHER
ARRIVAL
RATES
POWMIN
OUTPERFORMS
POWMAX
AS
PREDICTED
BY
THEOREM
IN
PARTICULAR
WHEN
THE
ARRIVAL
RATE
IS
JOB
SEC
POWMIN
AFFORDS
A
IMPROVEMENT
IN
MEAN
RESPONSE
TIME
OVER
POWMAX
UNDER
DVFS
WE
CAN
AFFORD
ARRIVAL
RATES
UP
TO
JOB
SEC
BEFORE
OVERLOADING
THE
SYSTEM
TO
SUMMARIZE
UNDER
DVFS
WE
SEE
THAT
POWMIN
CAN
BE
PREFERABLE
TO
POWMAX
THIS
IS
DUE
TO
THE
FLATNESS
OF
THE
POWER
TO
FREQUENCY
CURVE
FOR
DVFS
SEE
FIG
A
AND
POWMIN
X
POWMED
X
POWMAX
X
AGREES
PERFECTLY
WITH
THEOREM
FIG
PLOTS
THE
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
ARRIVAL
RATE
FOR
DVFS
DFS
WITH
A
POWER
BUDGET
OF
IN
THIS
CASE
POWMAX
REPRESENTED
BY
THE
DASHED
LINE
DENOTES
RUNNING
SERVERS
AT
C
AND
TURNING
OFF
ALL
OTHER
SERVERS
POWMED
REPRESENTED
BY
THE
SOLID
LINE
DENOTES
RUNNING
SERVERS
AT
B
C
AND
TURNING
OFF
ALL
OTHER
SERVERS
WE
SEE
THAT
WHEN
THE
ARRIVAL
RATE
IS
LOW
POWMAX
PRODUCES
LOWER
MEAN
RESPONSE
TIMES
THAN
POWMED
HOWEVER
AT
HIGHER
ARRIVAL
RATES
POWMED
OUTPERFORMS
POW
MAX
EXACTLY
AS
PREDICTED
BY
THEOREM
FOR
THE
SAKE
OF
COMPLETION
WE
ALSO
PLOT
POWMIN
DOTTED
LINE
IN
FIG
NOTE
THAT
POWMIN
IS
WORSE
THAN
BOTH
POWMED
AND
POW
MAX
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
NOTE
THAT
WE
USE
THE
VALUE
OF
B
C
AS
THE
OPTIMAL
POWER
ALLOCATED
TO
EACH
SERVER
IN
POWMED
FOR
OUR
EXPERIMENTS
AS
THIS
VALUE
IS
CLOSE
TO
THE
THEORETICAL
OPTIMUM
PREDICTED
BY
THEOREM
WHICH
IS
AROUND
FOR
THE
RANGE
OF
ARRIVAL
RATES
WE
USE
AND
ALSO
HELPS
TO
KEEP
THE
POWER
BUDGET
AT
CLOSED
SERVER
FARM
CONFIGURATION
WE
NOW
TURN
TO
OUR
EXPERIMENTAL
RESULTS
FOR
CLOSED
SERVER
FARM
CONFIGURATIONS
FIG
A
PLOTS
THE
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
MULTI
PROGRAMMING
LEVEL
MPL
N
FOR
DFS
WITH
A
POWER
BUDGET
OF
P
IN
THIS
CASE
MULTI
PROGRAMMING
LEVEL
N
FIGURE
CLOSED
LOOP
EXPERIMENTAL
RESULTS
FOR
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
NUMBER
OF
JOBS
N
IN
THE
SYSTEM
USING
DVFS
DFS
FOR
LINPACK
AT
LOWER
VALUES
OF
N
POWMED
IS
SLIGHTLY
BETTER
THAN
POWMAX
WHILE
AT
HIGHER
VALUES
OF
N
POWMED
OUT
PERFORMS
POWMAX
BY
AS
MUCH
AS
NOTE
THAT
POWMIN
IS
WORSE
THAN
BOTH
POWMED
AND
POWMAX
FOR
ALL
VALUES
OF
N
POWMAX
REPRESENTED
BY
THE
DASHED
LINE
DENOTES
RUNNING
SERVERS
AT
C
AND
TURNING
OFF
ALL
OTHER
SERVERS
POWMIN
REPRESENTED
BY
THE
SOLID
LINE
DENOTES
RUNNING
SERVERS
AT
B
AND
TURNING
OFF
ALL
OTHER
SERVERS
CLEARLY
POWMAX
OUTPERFORMS
POWMIN
THROUGHOUT
THE
RANGE
OF
N
BY
ALMOST
A
FACTOR
OF
THROUGHOUT
THE
RANGE
THIS
IS
IN
AGREEMENT
WITH
THE
PREDICTIONS
OF
THEOREM
FIG
B
PLOTS
THE
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
MULTI
PROGRAMMING
LEVEL
FOR
DVFS
WITH
A
POWER
BUD
GET
OF
POWMAX
REPRESENTED
BY
THE
DASHED
LINE
AGAIN
DENOTES
RUNNING
SERVERS
AT
C
AND
TURNING
OFF
ALL
OTHER
SERVERS
POWMIN
REPRESENTED
BY
THE
SOLID
LINE
DFS
AND
DVFS
B
DVFS
DFS
FIGURE
POWER
TO
FREQUENCY
CURVES
FOR
DFS
DVFS
AND
DVFS
DFS
FOR
THE
CPU
BOUND
DAXPY
WORKLOAD
FIG
A
ILLUSTRATES
OUR
MEASUREMENTS
FOR
DFS
AND
DVFS
IN
BOTH
THESE
MECHANISMS
WE
SEE
THAT
THE
SERVER
FREQUENCY
IS
LINEARLY
RELATED
TO
THE
POWER
ALLOCATED
TO
THE
SERVER
FIG
B
ILLUSTRATES
OUR
MEASUREMENTS
FOR
DVFS
DFS
WHERE
THE
POWER
TO
FREQUENCY
CURVE
IS
BETTER
APPROXIMATED
BY
A
CUBIC
RELATIONSHIP
MEAN
ARRIVAL
RATE
JOBS
SEC
MEAN
ARRIVAL
RATE
JOBS
SEC
MEAN
ARRIVAL
RATE
JOBS
SEC
A
DFS
B
DVFS
C
DVFS
DFS
FIGURE
OPEN
LOOP
EXPERIMENTAL
RESULTS
FOR
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
ARRIVAL
RATE
USING
DFS
DVFS
AND
DVFS
DFS
FOR
THE
CPU
BOUND
DAXPY
WORKLOAD
IN
FIG
A
FOR
DFS
POWMAX
OUTPERFORMS
POWMIN
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
BY
AS
MUCH
AS
A
FACTOR
OF
IN
FIG
B
FOR
DVFS
POWMAX
OUTPERFORMS
POWMIN
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
BY
AROUND
IN
FIG
C
FOR
DVFS
DFS
POWMAX
OUTPERFORMS
BOTH
POWMED
AND
POWMIN
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
WHILE
AT
LOWER
ARRIVAL
RATES
POWMAX
ONLY
SLIGHTLY
OUTPERFORMS
POWMED
AT
HIGHER
ARRIVAL
RATES
THE
IMPROVEMENT
IS
AROUND
DENOTES
RUNNING
SERVERS
AT
B
AND
TURNING
OFF
ALL
OTHER
SERVERS
WE
SEE
THAT
WHEN
N
IS
HIGH
POWMIN
PRO
DUCES
LOWER
MEAN
RESPONSE
TIMES
THAN
POWMAX
THIS
IS
IN
AGREEMENT
WITH
THE
PREDICTIONS
OF
THEOREM
IN
PARTICULAR
WHEN
N
POWMIN
AFFORDS
A
IMPROVEMENT
IN
MEAN
RESPONSE
TIME
OVER
POWMAX
HOWEVER
WHEN
N
IS
LOW
POW
MAX
PRODUCES
SLIGHTLY
LOWER
RESPONSE
TIMES
THAN
POWMIN
THIS
IS
IN
AGREEMENT
WITH
THEOREM
FIG
PLOTS
THE
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
MULTI
PROGRAMMING
LEVEL
FOR
DVFS
DFS
WITH
A
POWER
BUD
GET
OF
IN
THIS
CASE
POWMAX
REPRESENTED
BY
THE
DASHED
LINE
DENOTES
RUNNING
SERVERS
AT
C
AND
TURNING
OFF
ALL
OTHER
SERVERS
POWMED
REPRESENTED
BY
PREDICTIONS
OF
THEOREM
IN
PARTICULAR
WHEN
N
POWMED
AFFORDS
A
IMPROVEMENT
IN
MEAN
RESPONSE
TIME
OVER
POWMAX
HOWEVER
WHEN
N
IS
LOW
POWMED
PRODUCES
ONLY
SLIGHTLY
LOWER
RESPONSE
TIMES
THAN
POWMAX
NOTE
THAT
THROUGHOUT
THE
RANGE
OF
N
POWMIN
IS
OUTPERFORMED
BY
BOTH
POWMAX
AND
POWMED
OTHER
WORKLOADS
THUS
FAR
WE
HAVE
PRESENTED
EXPERIMENTAL
RESULTS
FOR
A
CPU
BOUND
WORKLOAD
LINPACK
IN
THIS
SECTION
WE
PRESENT
EX
PERIMENTAL
RESULTS
FOR
OTHER
WORKLOADS
OUR
EXPERIMENTAL
RESULTS
AGREE
WITH
OUR
THEORETICAL
PREDICTIONS
EVEN
IN
THE
THE
SOLID
LINE
DENOTES
RUNNING
SERVERS
AT
B
C
CASE
OF
NON
CPU
BOUND
WORKLOADS
AND
TURNING
OFF
ALL
OTHER
SERVERS
POWMIN
REPRESENTED
BY
THE
DOTTED
LINE
DENOTES
RUNNING
SERVERS
AT
WE
SEE
THAT
WHEN
N
IS
HIGH
POWMED
PRODUCES
LOWER
MEAN
RE
SPONSE
TIMES
THAN
POWMAX
THIS
IS
IN
AGREEMENT
WITH
THE
WE
FULLY
DISCUSS
EXPERIMENTAL
RESULTS
FOR
TWO
WORKLOADS
DAXPY
AND
STREAM
IN
THIS
SECTION
AND
SUMMARIZE
OUR
RESULTS
FOR
OTHER
WORKLOADS
AT
THE
END
OF
THE
SECTION
DUE
TO
LACK
OF
SPACE
WE
ONLY
SHOW
RESULTS
FOR
OPEN
LOOP
CONFIGURA
TIONS
DAXPY
DAXPY
IS
A
CPU
BOUND
WORKLOAD
WHICH
WE
HAVE
SIZED
TO
BE
CACHE
RESIDENT
THIS
MEANS
DAXPY
USES
A
LOT
OF
PROCESSOR
AND
CACHE
BUT
RARELY
USES
THE
SERVER
MEM
ORY
AND
DISK
SUBSYSTEMS
HENCE
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
DAXPY
IS
SIMILAR
TO
THAT
OF
CPU
BOUND
LINPACK
EXCEPT
THAT
DAXPY
PEAK
POWER
CONSUMPTION
TENDS
TO
BE
LOWER
THAN
THAT
OF
LINPACK
SINCE
DAXPY
DOES
NOT
USE
A
LOT
OF
MEMORY
OR
DISK
FIGS
A
AND
B
PRESENT
OUR
RESULTS
FOR
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
DAXPY
THE
FUNCTIONAL
FORM
OF
THE
POWER
TO
FREQUENCY
RELATIONSHIP
UNDER
DFS
AND
DVFS
IN
FIG
A
IS
CLEARLY
LINEAR
HOWEVER
THE
POWER
TO
FREQUENCY
RELATIONSHIP
UNDER
DVFS
DFS
IN
FIG
B
IS
BETTER
APPROX
IMATED
BY
A
CUBIC
RELATIONSHIP
THESE
TRENDS
ARE
SIMILAR
TO
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
LINPACK
SEEN
IN
FIG
FIGS
A
B
AND
C
PRESENT
OUR
POWER
ALLOCATION
RESULTS
FOR
DAXPY
UNDER
DFS
DVFS
AND
DVFS
DFS
RESPEC
TIVELY
FOR
DFS
IN
FIG
A
POWMAX
OUTPERFORMS
POWMIN
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
BY
AS
MUCH
AS
A
FAC
TOR
OF
THIS
IS
IN
AGREEMENT
WITH
THEOREM
NOTE
THAT
WE
USE
AS
THE
POWER
ALLOCATED
TO
EACH
SERVER
UNDER
POWMIN
TO
KEEP
THE
POWER
BUDGET
SAME
FOR
POWMIN
AND
POWMAX
FOR
DVFS
IN
FIG
B
POWMAX
OUTPERFORMS
POWMIN
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
BY
AROUND
THIS
IS
IN
CONTRAST
TO
LINPACK
WHERE
POWMIN
OUT
PERFORMS
POWMAX
AT
HIGH
ARRIVAL
RATES
THE
REASON
WHY
POWMAX
OUTPERFORMS
POWMIN
FOR
DAXPY
IS
THE
LOWER
VALUE
OF
SB
GHZ
FOR
DAXPY
AS
COMPARED
TO
SB
GHZ
FOR
LINPACK
SINCE
SB
Α
FOR
DAXPY
UNDER
DVFS
THEOREM
RIGHTLY
PREDICTS
POWMAX
TO
BE
OP
TIMAL
FINALLY
IN
FIG
C
FOR
DVFS
DFS
POWMAX
OUT
PERFORMS
BOTH
POWMED
AND
POWMIN
THROUGHOUT
THE
RANGE
OF
ARRIVAL
RATES
AGAIN
THIS
IS
IN
CONTRAST
TO
LINPACK
WHERE
POWMED
OUTPERFORMS
POWMAX
AT
HIGH
ARRIVAL
RATES
THE
REASON
WHY
POWMAX
OUTPERFORMS
POWMED
FOR
DAXPY
IS
THE
HIGHER
VALUE
OF
ΑL
GHZ
W
FOR
DAXPY
AS
COMPARED
TO
ΑL
GHZ
W
FOR
LINPACK
THIS
IS
IN
AGREEMENT
WITH
THE
PREDICTIONS
OF
THEOREM
FOR
HIGH
VALUES
OF
ΑL
INTUITIVELY
FOR
A
CUBIC
POWER
TO
FREQUENCY
RE
LATIONSHIP
WE
HAVE
FROM
EQ
SB
ΑL
B
AS
ΑL
INCREASES
WE
GET
MORE
SERVER
FREQUENCY
FOR
EVERY
WATT
OF
POWER
ADDED
TO
THE
SERVER
THUS
AT
HIGH
ΑL
WE
ALLOCATE
AS
MUCH
POWER
AS
POSSIBLE
TO
EVERY
SERVER
IMPLYING
POWMAX
STREAM
STREAM
IS
A
MEMORY
BOUND
WORKLOAD
WHICH
DOES
NOT
USE
A
LOT
OF
PROCESSOR
CYCLES
HENCE
THE
POWER
CONSUMPTION
AT
A
GIVEN
SERVER
FREQUENCY
FOR
STREAM
IS
USUALLY
LOWER
THAN
CPU
BOUND
LINPACK
AND
DAXPY
FIGS
A
AND
B
PRESENT
OUR
RESULTS
FOR
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
STREAM
SURPRISINGLY
THE
FUNCTIONAL
FORM
OF
THE
POWER
TO
FREQUENCY
RELATIONSHIP
UNDER
DFS
DVFS
AND
DVFS
DFS
IS
CLOSER
TO
A
CUBIC
RELATION
SHIP
THAN
TO
A
LINEAR
ONE
IN
PARTICULAR
THE
GAIN
IN
SERVER
FREQUENCY
PER
WATT
AT
HIGHER
POWER
ALLOCATIONS
IS
MUCH
LOWER
THAN
THE
GAIN
IN
FREQUENCY
PER
WATT
AT
LOWER
POWER
ALLOCA
TIONS
WE
ARGUE
THIS
OBSERVATION
AS
FOLLOWS
AT
EXTREMELY
LOW
SERVER
FREQUENCIES
THE
BOTTLENECK
FOR
STREAM
PERFOR
MANCE
IS
THE
CPU
THUS
EVERY
EXTRA
WATT
OF
POWER
ADDED
TO
THE
SYSTEM
WOULD
BE
USED
UP
BY
THE
CPU
TO
IMPROVE
ITS
FREQUENCY
HOWEVER
AT
HIGHER
SERVER
FREQUENCIES
THE
BOTTLE
NECK
FOR
STREAM
PERFORMANCE
IS
THE
MEMORY
SUBSYSTEM
SINCE
STREAM
IS
MEMORY
BOUND
THUS
EVERY
EXTRA
WATT
OF
POWER
ADDED
TO
THE
SYSTEM
WOULD
MAINLY
BE
USED
UP
BY
THE
MEMORY
SUBSYSTEM
AND
THE
IMPROVEMENT
IN
PROCESSOR
FREQUENCY
WOULD
BE
MINIMAL
FIGS
A
B
AND
C
PRESENT
OUR
POWER
ALLOCATION
RE
SULTS
FOR
STREAM
UNDER
DFS
DVFS
AND
DVFS
DFS
RE
SPECTIVELY
DUE
TO
THE
DOWNWARDS
CONCAVE
NATURE
OF
THE
POWER
TO
FREQUENCY
CURVES
FOR
STREAM
STUDIED
IN
FIG
THEOREM
SAYS
THAT
POWMAX
SHOULD
BE
OPTIMAL
AT
LOW
AR
RIVAL
RATES
AND
POWMED
SHOULD
BE
OPTIMAL
AT
HIGH
ARRIVAL
RATES
HOWEVER
FOR
THE
VALUES
OF
ΑL
IN
FIG
WE
FIND
THAT
THE
THRESHOLD
POINT
ΛLOW
BELOW
WHICH
POWMAX
IS
OPTIMAL
IS
QUITE
HIGH
HENCE
POWMAX
IS
OPTIMAL
IN
FIG
C
IN
FIGS
A
AND
B
POWMAX
AND
POWMED
PRODUCE
SIMILAR
RESPONSE
TIMES
GZIP
AND
GZIP
AND
ARE
COMMON
SOFTWARE
APPLICATIONS
USED
FOR
DATA
COMPRESSION
IN
UNIX
SYSTEMS
THESE
CPU
BOUND
COM
PRESSION
APPLICATIONS
USE
SOPHISTICATED
ALGORITHMS
TO
REDUCE
THE
SIZE
OF
A
GIVEN
FILE
WE
USE
GZIP
AND
TO
COMPRESS
A
FILE
OF
UNCOMPRESSED
SIZE
MB
FOR
GZIP
WE
FIND
THAT
POWMAX
IS
OPTIMAL
FOR
ALL
OF
DFS
DVFS
AND
DVFS
DFS
THESE
RESULTS
ARE
SIMILAR
TO
THE
RESULTS
FOR
DAXPY
FOR
THE
RESULTS
ARE
SIMILAR
TO
THOSE
OF
LINPACK
IN
PARTICULAR
AT
LOW
ARRIVAL
RATES
POWMAX
IS
OPTIMAL
FOR
HIGH
ARRIVAL
RATES
POWMAX
IS
OPTIMAL
FOR
DFS
POWMIN
IS
OPTIMAL
FOR
DVFS
AND
POWMED
IS
OPTIMAL
FOR
DVFS
DFS
WEBBENCH
WEBBENCH
IS
A
BENCHMARK
PROGRAM
USED
TO
MEASURE
WEB
SERVER
PERFORMANCE
BY
SENDING
MULTIPLE
FILE
REQUESTS
TO
A
SERVER
FOR
WEBBENCH
WE
FIND
THE
POWER
TO
FREQUENCY
RELATIONSHIP
FOR
DFS
DVFS
AND
DVFS
DFS
TO
BE
CUBIC
THIS
IS
SIMILAR
TO
THE
POWER
TO
FREQUENCY
RELATIONSHIPS
OB
SERVED
FOR
STREAM
SINCE
WEBBENCH
IS
MORE
MEMORY
AND
DISK
INTENSIVE
AS
THEORY
PREDICTS
SEE
THEOREM
WE
FIND
POWMAX
TO
BE
OPTIMAL
AT
LOW
ARRIVAL
RATES
AND
POWMED
TO
BE
OPTIMAL
AT
HIGH
ARRIVAL
RATES
FOR
DFS
DVFS
AND
DVFS
DFS
SUMMARY
IN
THIS
PAPER
WE
CONSIDER
THE
PROBLEM
OF
ALLOCATING
AN
AVAILABLE
POWER
BUDGET
AMONG
SERVERS
IN
A
SERVER
FARM
TO
MINIMIZE
MEAN
RESPONSE
TIME
THE
AMOUNT
OF
POWER
ALLO
CATED
TO
A
SERVER
DETERMINES
ITS
SPEED
IN
ACCORDANCE
TO
SOME
POWER
TO
FREQUENCY
RELATIONSHIP
HENCE
WE
BEGIN
BY
MEA
SURING
THE
POWER
TO
FREQUENCY
RELATIONSHIP
WITHIN
A
SINGLE
SERVER
WE
EXPERIMENTALLY
FIND
THAT
THE
POWER
TO
FREQUENCY
RELATIONSHIP
WITHIN
A
SERVER
FOR
A
GIVEN
WORKLOAD
CAN
BE
EI
THER
LINEAR
OR
CUBIC
INTERESTINGLY
WE
SEE
THAT
THE
RELATION
SHIP
IS
LINEAR
FOR
DFS
AND
DVFS
WHEN
THE
WORKLOAD
IS
CPU
BOUND
BUT
CUBIC
WHEN
IT
IS
MORE
MEMORY
BOUND
BY
CON
TRAST
THE
RELATIONSHIP
FOR
DVFS
DFS
IS
ALWAYS
CUBIC
IN
OUR
EXPERIMENTS
GIVEN
THE
POWER
TO
FREQUENCY
RELATIONSHIP
WE
CAN
VIEW
THE
PROBLEM
OF
FINDING
THE
OPTIMAL
POWER
ALLOCATION
IN
TERMS
OF
DETERMINING
THE
OPTIMAL
FREQUENCIES
OF
SERVERS
IN
THE
SERVER
FARM
TO
MINIMIZE
MEAN
RESPONSE
TIME
HOWEVER
THERE
ARE
SEVERAL
FACTORS
APART
FROM
THE
SERVER
FREQUENCIES
THAT
AFFECT
THE
MEAN
RESPONSE
TIME
FOR
A
SERVER
FARM
THESE
INCLUDE
THE
ARRIVAL
RATE
THE
MAXIMUM
SPEED
OF
A
SERVER
THE
TOTAL
POWER
BUDGET
WHETHER
THE
SERVER
FARM
HAS
AN
OPEN
OR
CLOSED
CONFIGURATION
ETC
TO
FULLY
UNDERSTAND
THE
EFFECTS
OF
THESE
A
DFS
AND
DVFS
B
DVFS
DFS
FIGURE
POWER
TO
FREQUENCY
CURVES
FOR
DFS
DVFS
AND
DVFS
DFS
FOR
THE
MEMORY
BOUND
STREAM
WORKLOAD
FIG
A
ILLUSTRATES
OUR
MEASUREMENTS
FOR
DFS
AND
DVFS
WHILE
FIG
B
ILLUSTRATES
OUR
MEASUREMENTS
FOR
DVFS
DFS
IN
ALL
THE
THREE
MECHANISMS
THE
POWER
TO
FREQUENCY
CURVES
ARE
DOWNWARDS
CONCAVE
DEPICTING
A
CUBIC
RELATIONSHIP
BETWEEN
POWER
ALLOCATED
TO
A
SERVER
AND
ITS
FREQUENCY
A
DFS
B
DVFS
C
DVFS
DFS
FIGURE
OPEN
LOOP
EXPERIMENTAL
RESULTS
FOR
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
ARRIVAL
RATE
USING
DFS
DVFS
AND
DVFS
DFS
FOR
THE
MEMORY
BOUND
STREAM
WORKLOAD
IN
FIGS
A
AND
B
FOR
DFS
AND
DVFS
RESPECTIVELY
POWMED
AND
POWMAX
PRODUCE
SIMILAR
RESPONSE
TIMES
IN
FIG
C
HOWEVER
FOR
DVFS
DFS
POWMAX
OUTPERFORMS
POWMED
BY
AS
MUCH
AS
AT
HIGH
ARRIVAL
RATES
IN
ALL
THREE
CASES
POWMIN
IS
WORSE
THAN
BOTH
POWMED
AND
POWMAX
FACTORS
ON
MEAN
RESPONSE
TIME
WE
DEVELOP
A
QUEUEING
THEO
RETIC
MODEL
SEE
SECTION
THAT
ALLOWS
US
TO
PREDICT
MEAN
RESPONSE
TIME
AS
A
FUNCTION
OF
THE
ABOVE
FACTORS
WE
THEN
PRODUCE
THEOREMS
SEE
SECTIONS
AND
THAT
DETERMINE
THE
OPTIMAL
POWER
ALLOCATION
FOR
EVERY
POSSIBLE
CONFIGURATION
OF
THE
ABOVE
FACTORS
TO
VERIFY
OUR
THEORETICAL
PREDICTIONS
WE
CONDUCT
EXTENSIVE
EXPERIMENTS
ON
AN
IBM
BLADECENTER
FOR
A
RANGE
OF
WORK
LOADS
USING
DFS
DVFS
AND
DVFS
DFS
SEE
SECTION
AND
IN
EVERY
CASE
WE
FIND
THAT
THE
EXPERIMENTAL
RESULTS
ARE
IN
EXCELLENT
AGREEMENT
WITH
OUR
THEORETICAL
PREDICTIONS
DISCUSSION
AND
FUTURE
WORK
THERE
ARE
MANY
EXTENSIONS
TO
THIS
WORK
THAT
WE
ARE
EX
PLORING
BUT
ARE
BEYOND
THE
SCOPE
OF
THIS
PAPER
FIRST
OF
ALL
THE
ARRIVAL
RATE
INTO
OUR
SERVER
FARM
MAY
VARY
DYNAMICALLY
OVER
TIME
IN
ORDER
TO
ADJUST
TO
A
DYNAMICALLY
VARYING
ARRIVAL
RATE
WE
MAY
NEED
TO
ADJUST
THE
POWER
ALLOCA
TION
ACCORDINGLY
THE
THEOREMS
IN
THIS
PAPER
ALREADY
TELL
US
THE
OPTIMAL
POWER
ALLOCATION
FOR
ANY
GIVEN
ARRIVAL
RATE
WE
ARE
NOW
WORKING
ON
INCORPORATING
THE
EFFECTS
OF
SWITCHING
COSTS
INTO
OUR
MODEL
SECOND
WHILE
WE
HAVE
CONSIDERED
TURNING
SERVERS
ON
OR
OFF
TODAY
TECHNOLOGY
ALLOWS
FOR
SERVERS
WHICH
ARE
SLEEP
ING
HALT
STATE
OR
DEEP
C
STATES
THESE
SLEEPING
SERVERS
CONSUME
LESS
POWER
THAN
SERVERS
THAT
ARE
ON
AND
CAN
MORE
QUICKLY
BE
MOVED
INTO
THE
ON
STATE
THAN
SERVERS
THAT
ARE
TURNED
OFF
WE
ARE
LOOKING
AT
WAYS
TO
EXTEND
OUR
THEOREMS
TO
ALLOW
FOR
SERVERS
WITH
SLEEP
STATES
THIRD
WHILE
THIS
PAPER
DEALS
WITH
POWER
MANAGEMENT
AT
THE
SERVER
LEVEL
MEASURING
AND
ALLOCATING
POWER
TO
THE
SERVER
AS
A
WHOLE
OUR
TECHNIQUES
CAN
BE
EXTENDED
TO
DEAL
WITH
INDIVIDUAL
SUBSYSTEMS
WITHIN
A
SERVER
SUCH
AS
POWER
ALLOCATION
WITHIN
THE
STORAGE
SUBSYSTEM
WE
ARE
LOOKING
AT
EXTENDING
OUR
IMPLEMENTATION
TO
INDIVIDUAL
COMPONENTS
WITHIN
A
SERVER
REFERENCES
LESSWATTS
ORG
RACE
TO
IDLE
POWER
MANAGEMENT
RACE
TO
IDLE
PHP
INTEL
NEHALEM
PDF
U
ENVIRONMENTAL
PROTECTION
AGENCY
EPA
REPORT
ON
SERVER
AND
DATA
CENTER
ENERGY
EFFICIENCY
NATIONAL
ELECTRICAL
CONTRACTORS
ASSOCIATION
DATA
CENTERS
MEETING
TODAY
DEMAND
JEFFREY
CHASE
DARRELL
C
ANDERSON
PRACHI
N
THAKAR
AND
AMIN
M
VAHDAT
MANAGING
ENERGY
AND
SERVER
RESOURCES
IN
HOSTING
CENTERS
IN
IN
PROCEEDINGS
OF
THE
EIGHTEENTH
ACM
SYMPOSIUM
ON
OPERATING
SYSTEMS
PRINCIPLES
SOSP
PAGES
INTEL
CORP
INTEL
DUO
MOBILE
PROCESSOR
DATASHEET
TABLE
PDF
M
ELNOZAHY
M
KISTLER
AND
R
RAJAMONY
ENERGY
CONSERVATION
POLICIES
FOR
WEB
SERVERS
IN
USITS
XIAOBO
FAN
WOLF
DIETRICH
WEBER
AND
LUIZ
ANDRE
BARROSO
POWER
PROVISIONING
FOR
A
WAREHOUSE
SIZED
COMPUTER
PAGES
WES
FELTER
KARTHICK
RAJAMANI
TOM
KELLER
AND
COSMIN
RUSU
A
PERFORMANCE
CONSERVING
APPROACH
FOR
REDUCING
PEAK
POWER
CONSUMPTION
IN
SERVER
SYSTEMS
IN
ICS
PROCEEDINGS
OF
THE
ANNUAL
INTERNATIONAL
CONFERENCE
ON
SUPERCOMPUTING
PAGES
NEW
YORK
NY
USA
ACM
MARK
E
FEMAL
AND
VINCENT
W
FREEH
BOOSTING
DATA
CENTER
PERFORMANCE
THROUGH
NON
UNIFORM
POWER
ALLOCATION
IN
ICAC
PROCEEDINGS
OF
THE
SECOND
INTERNATIONAL
CONFERENCE
ON
AUTOMATIC
COMPUTING
PAGES
WASHINGTON
DC
M
FLOYD
GHIASI
T
W
KELLER
K
RAJAMANI
F
L
RAWSON
J
C
RUBIO
AND
M
WARE
SYSTEM
POWER
MANAGEMENT
SUPPORT
IN
THE
IBM
MICROPROCESSOR
IBM
JOURNAL
OF
RESEARCH
AND
DEVELOPMENT
ANSHUL
GANDHI
MOR
HARCHOL
BALTER
RAJARSHI
DAS
AND
CHARLES
LEFURGY
OPTIMAL
POWER
ALLOCATION
IN
SERVER
FARMS
TECHNICAL
REPORT
CMU
CS
INTEL
CORP
INTEL
MATH
KERNEL
LIBRARY
DAVID
MOSBERGER
AND
TAI
JIN
HTTPERF
A
TOOL
FOR
MEASURING
WEB
SERVER
PERFORMANCE
ACM
SIGMETRICS
PERFORMANCE
EVALUATION
REVIEW
VIVEK
PANDEY
W
JIANG
Y
ZHOU
AND
R
BIANCHINI
DMA
AWARE
MEMORY
ENERGY
MANAGEMENT
HPCA
THE
INTERNATIONAL
SYMPOSIUM
ON
HIGH
PERFORMANCE
COMPUTER
ARCHITECTURE
PAGES
FEB
RAMYA
RAGHAVENDRA
PARTHASARATHY
RANGANATHAN
VANISH
TALWAR
ZHIKUI
WANG
AND
XIAOYUN
ZHU
NO
POWER
STRUGGLES
COORDINATED
MULTI
LEVEL
POWER
MANAGEMENT
FOR
THE
DATA
CENTER
IN
ASPLOS
XIII
PROCEEDINGS
OF
THE
INTERNATIONAL
CONFERENCE
ON
ARCHITECTURAL
SUPPORT
FOR
PROGRAMMING
LANGUAGES
AND
OPERATING
SYSTEMS
PAGES
K
RAJAMANI
H
HANSON
J
C
RUBIO
GHIASI
AND
F
L
RAWSON
ONLINE
POWER
AND
PERFORMANCE
ESTIMATION
FOR
DYNAMIC
POWER
MANAGEMENT
RESEARCH
REPORT
RC
JULY
SALVATORE
SANFILIPPO
WBOX
HTTP
TESTING
TOOL
VERSION
X
WANG
AND
M
CHEN
CLUSTER
LEVEL
FEEDBACK
POWER
CONTROL
FOR
PERFORMANCE
OPTIMIZATION
IEEE
INTERNATIONAL
SYMPOSIUM
ON
HIGH
PERFORMANCE
COMPUTER
ARCHITECTURE
HPCA
FEBRUARY
ZHIKUI
WANG
XIAOYUN
ZHU
CLIFF
MCCARTHY
PARTHA
RANGANATHAN
AND
VANISH
TALWAR
FEEDBACK
CONTROL
ALGORITHMS
FOR
POWER
MANAGEMENT
OF
SERVERS
IN
THIRD
INTERNATIONAL
WORKSHOP
ON
FEEDBACK
CONTROL
IMPLEMENTATION
AND
DESIGN
IN
COMPUTING
SYSTEMS
AND
NETWORKS
FEBID
ANNAPOLIS
MD
JUNE
APPENDIX
PROOFS
OF
OPEN
LOOP
CONFIGURA
TION
THEOREMS
SECTION
IN
THIS
APPENDIX
WE
PROVIDE
BRIEF
SKETCHES
OF
THE
PROOFS
OF
EACH
THEOREM
FROM
THE
PAPER
ALL
THEOREMS
AGAIN
ASSUME
EQ
THEOREM
GIVEN
AN
OPEN
K
SERVER
FARM
CONFIGURATION
WITH
A
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIP
AND
POWER
BUD
GET
P
THE
FOLLOWING
POWER
ALLOCATION
SETTING
MINIMIZES
E
T
IF
SB
Α
N
C
P
IF
SB
Α
N
C
PN
N
K
IF
Λ
ΛLOW
NA
ENG
HTM
RAJ
JAIN
THE
ART
OF
COMPUTER
SYSTEMS
PERFORMANCE
ANALYSIS
TECHNIQUES
FOR
EXPERIMENTAL
DESIGN
MEASUREMENT
SIMULATION
AND
MODELING
PAGES
WILEY
RADIM
KOLAR
WEB
BENCH
HTTP
HOME
TISCALI
CZ
WEBBENCH
HTML
KLEINROCK
L
QUEUEING
SYSTEMS
VOLUME
WHERE
ΛLOW
Α
P
PROOF
WE
PROVIDE
A
COMPLETE
PROOF
FOR
THE
CASE
OF
K
IN
LEMMA
WE
THEN
USE
THIS
RESULT
TO
PROVE
THE
THEOREM
FOR
THE
CASE
OF
ARBITRARY
K
BY
CONTRADICTION
LEMMA
GIVEN
AN
OPEN
SERVER
FARM
CONFIGURATION
WITH
LINEAR
POWER
TO
SPEED
RELATIONSHIP
AND
POWER
BUDGET
THE
FOLLOWING
POWER
ALLOCATION
MINIMIZES
E
T
IF
SB
Α
MIN
C
P
P
WILEY
INTERSCIENCE
NEW
YORK
MIN
C
P
P
IF
Λ
ΛLOW
CHARLES
LEFURGY
XIAORUI
WANG
AND
MALCOLM
WARE
POWER
CAPPING
A
PRELUDE
TO
POWER
SHIFTING
CLUSTER
IF
SB
Α
B
B
IF
B
C
Λ
Λ
MIN
C
P
MIN
C
P
OTHERWISE
COMPUTING
NOVEMBER
J
D
MCCALPIN
STREAM
SUSTAINABLE
MEMORY
BANDWIDTH
IN
HIGH
PERFORMANCE
COMPUTERS
WHERE
ΛLOW
SC
Α
C
B
C
B
P
SB
SB
Α
P
PROOF
LEMMA
THE
PROOF
IS
TRIVIAL
FOR
THE
CASES
P
AND
THUS
ASSUME
LET
BE
SPLIT
AMONG
THE
TWO
SERVERS
AS
THE
GOAL
IS
TO
FIND
THE
OPTIMAL
POWER
ALLOCATION
THAT
MIN
IMIZES
E
T
GIVEN
THE
CONSTRAINT
TWO
SERVERS
SAY
SERVERS
I
AND
J
SUCH
THAT
B
PI
C
AND
B
PJ
C
WE
INVOKE
LEMMA
ON
SERVERS
I
AND
J
WITH
TOTAL
POWER
PL
PI
PJ
LEMMA
TELLS
US
THAT
THE
OPTIMAL
POWER
ALLOCATION
FOR
SERVERS
I
AND
J
IS
PI
MIN
C
PL
AND
P
J
L
I
IT
CAN
ALSO
BE
SHOWN
SEE
THAT
REDUCING
THE
MEAN
RESPONSE
TIME
OF
THE
SERVER
SYSTEM
CONSISTING
ASSUMING
A
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIP
WE
HAVE
THE
FOLLOWING
RELATIONSHIP
FOR
THE
SPEED
OF
SERVER
I
SI
AS
A
FUNCTION
OF
THE
POWER
ALLOCATED
TO
IT
PI
SI
SB
Α
PI
B
WHERE
Α
SB
AND
B
ARE
AS
DEFINED
IN
SECTIONS
AND
WE
NOW
WISH
TO
DERIVE
E
T
FOR
A
SERVER
FARM
WITH
SPEEDS
AND
RECALL
FROM
SECTION
THAT
WE
HAVE
A
PS
SCHEDUL
ING
DISCIPLINE
AND
A
POISSON
ARRIVAL
PROCESS
WITH
SOME
MEAN
Λ
IT
IS
WELL
KNOWN
THAT
FOR
A
SINGLE
M
G
PS
QUEUE
WITH
SPEED
POISSON
ARRIVAL
RATE
Λ
AND
GENERAL
JOB
SIZE
E
T
IS
AS
FOLLOWS
E
T
M
G
PS
Λ
BY
EXPLOITING
POISSON
SPLITTING
IN
OUR
MODEL
WE
HAVE
THAT
THE
MEAN
RESPONSE
TIME
OF
JOBS
IN
THE
SERVER
FARM
WITH
SPLIT
TING
PARAMETER
Q
WHERE
Q
IS
THE
FRACTION
OF
JOBS
SENT
TO
THE
SERVER
AND
SERVER
SPEEDS
AND
IS
OF
SERVERS
I
AND
J
REDUCES
THE
MEAN
RESPONSE
TIME
OF
THE
WHOLE
SYSTEM
THUS
WE
HAVE
SHOWN
THAT
THE
MEAN
RESPONSE
TIME
OF
THE
SYSTEM
UNDER
Π
CAN
REDUCED
THUS
POWMAX
IS
OPTIMAL
THE
CASE
OF
SB
Α
IS
MORE
COMPLEX
WE
WANT
TO
SHOW
THAT
THE
SERVERS
THAT
ARE
TURNED
ON
SHOULD
EITHER
ALL
RUN
AT
POWER
C
OR
ALL
RUN
AT
POWER
B
DEPENDING
ON
Λ
WE
START
WITH
AN
ARBITRARY
POWER
ALLOCATION
AND
SHOW
THAT
WE
CAN
REPEATEDLY
APPLY
LEMMA
TO
TWO
SERVERS
AT
A
TIME
TO
END
UP
IN
A
POWER
ALLOCATION
WHERE
WE
HAVE
SOME
SERVERS
AT
POWER
B
SOME
SERVERS
AT
POWER
C
AND
AT
MOST
ONE
SERVER
AT
SOME
INTERMEDIATE
POWER
AT
THIS
POINT
WE
OPTIMIZE
OVER
ALL
POWER
ALLOCATION
SETTINGS
OF
THE
ABOVE
TYPE
AND
FIND
THAT
E
T
IS
MINIMIZED
IN
THE
PARTICULAR
POWER
ALLOCATION
WHERE
EITHER
N
SERVERS
ARE
AT
POWER
C
FOR
LOW
Λ
OR
M
SERVERS
ARE
AT
POWER
B
FOR
HIGH
Λ
FOR
DETAILS
SEE
THEOREM
GIVEN
AN
OPEN
K
SERVER
FARM
CONFIGURATION
WITH
A
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
AS
IN
EQ
AND
POWER
BUDGET
P
THE
FOLLOWING
POWER
ALLOCATION
SETTING
E
T
Q
Q
ΛQ
Λ
Q
MINIMIZES
E
T
N
C
PN
N
K
IF
Λ
ΛLLOW
L
PL
PL
L
K
IF
Λ
ΛLLOW
GIVEN
THE
STABILITY
CONSTRAINTS
I
FROM
EQ
WE
SEE
THAT
E
T
IS
A
FUNCTION
OF
AND
Q
HOWEVER
USING
EQS
AND
WE
CAN
EXPRESS
IN
TERMS
OF
AS
Α
P
WE
NOW
DERIVE
THE
OPTIMAL
VALUE
OF
Q
WHICH
WE
CALL
Q
TO
DO
THIS
WE
FIX
THE
SPEEDS
AND
IN
EQ
AND
SET
THE
DERIVATIVE
OF
E
T
W
R
T
Q
TO
BE
THIS
YIELDS
B
PROOF
WE
FOLLOW
THE
SAME
PROCESS
AS
IN
THE
PROOF
OF
THM
STARING
WITH
THE
CASE
OF
TWO
SERVERS
WE
NOTE
THAT
EQ
IS
STILL
VALID
IN
THE
CASE
OF
A
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
HOWEVER
EQ
NOW
TAKES
THE
FORM
SB
P
SB
WHERE
ΑL
SB
AND
B
ARE
AS
DEFINED
IN
SECTION
WE
CAN
Q
Λ
NOW
EXPRESS
E
T
FROM
EQ
IN
TERMS
OF
JUST
BY
SUBSTITUTING
FOR
Q
AND
FROM
EQS
AND
HENCE
Λ
WE
CAN
DIFFERENTIATE
E
T
W
R
T
TO
YIELD
THE
OPTIMAL
USING
Q
IN
THE
EXPRESSION
FOR
E
T
FROM
EQ
GIVES
US
WHICH
TRANSLATES
TO
WE
FIND
THAT
FOR
THE
SERVER
CASE
POWMAX
IS
OPTIMAL
AT
LOW
ARRIVAL
RATES
WHEREAS
POWMED
E
T
Λ
IS
OPTIMAL
AT
HIGH
ARRIVAL
RATES
NOTE
THAT
POWMED
FOR
TWO
AT
THIS
POINT
WE
CAN
EXPRESS
E
T
IN
TERMS
OF
JUST
BY
SUBSTITUTING
FOR
FROM
EQ
HENCE
WE
CAN
DIFFERENTIATE
E
T
W
R
T
TO
YIELD
THE
OPTIMAL
WHICH
TRANSLATES
VIA
EQ
TO
WE
FIND
THAT
FOR
SB
Α
POWMAX
MINIMIZES
OTHERWISE
WE
NOW
RETURN
TO
THE
PROOF
OF
THM
WHEN
THERE
ARE
K
SERVERS
WE
WANT
TO
SHOW
THAT
WE
SHOULD
EITHER
RUN
N
SERVERS
AT
POWER
C
OR
RUN
L
SERVERS
AT
POWER
PL
DEPENDING
B
ON
Λ
WE
START
WITH
AN
ARBITRARY
POWER
ALLOCATION
AND
SHOW
E
T
WHERE
POWMAX
FOR
SERVERS
REFERS
TO
MIN
C
P
AND
P
FOR
SB
Α
WE
FIND
THAT
POWMAX
MINIMIZES
E
T
FOR
Λ
ΛLOW
WHEREAS
POWMIN
MINIMIZES
E
T
FOR
Λ
Λ
LOW
POWMIN
FOR
SERVERS
REFERS
TO
B
P
B
IF
P
B
C
AND
MIN
C
P
MIN
C
P
OTHERWISE
WE
NOW
RETURN
TO
THE
PROOF
OF
THM
WE
FIRST
CONSIDER
THE
CASE
SB
Α
THE
PROOF
PROCEEDS
BY
CONTRADICTION
WE
CLAIM
THAT
POWMAX
IS
OPTIMAL
THUS
ASSUME
WE
HAVE
A
POWER
ALLOCATION
Π
K
WHICH
IS
NOT
THE
SAME
AS
POWMAX
SINCE
Π
IS
NOT
POWMAX
THERE
MUST
EXIST
AT
LEAST
THAT
WE
CAN
REPEATEDLY
APPLY
THE
TWO
SERVER
RESULT
DISCUSSED
ABOVE
TO
PAIRS
OF
SERVERS
AT
A
TIME
TO
END
UP
IN
A
POWER
ALLOCATION
WHERE
WE
HAVE
SOME
V
SERVERS
AT
POWER
C
AND
SOME
W
SERVERS
AT
POWER
P
VC
AT
THIS
POINT
WE
OPTIMIZE
OVER
ALL
POWER
ALLOCATION
SETTINGS
OF
THE
ABOVE
TYPE
AND
FIND
THAT
E
T
IS
MINIMIZED
IN
THE
PARTICULAR
POWER
ALLOCATION
WHERE
EITHER
N
SERVERS
ARE
AT
POWER
C
FOR
LOW
Λ
OR
L
SERVERS
ARE
AT
POWER
PL
FOR
HIGH
Λ
FOR
DETAILS
SEE
PROOFS
OF
CLOSED
LOOP
CONFIG
URATION
THEOREMS
SECTION
FOR
CLOSED
LOOP
CONFIGURATIONS
PROVIDES
WELL
KNOWN
WHERE
Α
SB
AND
B
ARE
AS
DEFINED
IN
SECTION
THUS
WE
HAVE
ASYMPTOTIC
BOUNDS
FOR
THE
MEAN
RESPONSE
TIME
BOTH
IN
THE
CASE
WHERE
N
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
IS
VERY
HIGH
AND
IN
THE
CASE
WHERE
N
IS
VERY
LOW
THESE
BOUNDS
PROVIDE
EXCELLENT
APPROXIMATIONS
FOR
E
T
HERE
N
IS
CONSIDERED
J
SI
I
J
SB
Α
PI
B
I
J
HIGH
IF
IT
SIGNIFICANTLY
EXCEEDS
THE
NUMBER
OF
SERVERS
THAT
ARE
ON
AND
N
IS
CONSIDERED
LOW
IF
IT
IS
CLOSE
TO
WE
WILL
USE
THESE
APPROXIMATIONS
FROM
TO
DERIVE
THE
OPTIMAL
POWER
ALLOCATIONS
IN
THEOREM
LOW
N
AND
IN
THEOREMS
AND
HIGH
N
J
SB
ΑB
Α
PI
I
J
SB
ΑB
ΑP
HENCE
IF
SB
Α
E
T
IS
MINIMIZED
BY
MAXIMIZING
J
THUS
THEOREM
GIVEN
A
CLOSED
K
SERVER
FARM
CONFIGURATION
B
POWMIN
IS
OPTIMAL
FOR
SB
Α
WHEN
SB
Α
E
T
IS
WITH
A
LINEAR
OR
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
THE
FOLLOWING
POWER
ALLOCATION
MINIMIZES
E
T
FOR
LOW
N
BASED
ON
THE
ASYMPTOTIC
APPROXIMATIONS
IN
N
C
PN
N
K
PROOF
WE
START
BY
ASSUMING
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
N
IS
LOW
FROM
WE
HAVE
THAT
MINIMIZED
BY
MINIMIZING
J
THUS
POWMAX
IS
OPTIMAL
FOR
SB
Α
THEOREM
GIVEN
A
CLOSED
K
SERVER
FARM
CONFIGURATION
WITH
A
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
THE
FOLLOWING
POWER
ALLOCATION
MINIMIZES
E
T
FOR
HIGH
N
BASED
ON
THE
ASYMPTOTIC
APPROXIMATIONS
IN
IF
SB
SL
N
C
PN
N
K
IF
SB
SL
L
B
X
PL
L
K
Q
WHERE
L
P
SL
MSC
ΑL
X
AND
X
IS
THE
NON
I
WITHOUT
LOSS
OF
GENERALITY
ASSUME
SK
ΑJ
SB
PROOF
AS
IN
THE
PROOF
FOR
THEOREM
MINIMIZING
E
T
THIS
IMPLIES
SK
THUS
E
T
GIVEN
BY
IS
EQUIVALENT
TO
MAXIMIZING
J
SI
WHERE
J
K
IS
THE
EQ
IS
MINIMIZED
BY
SETTING
IF
OR
QA
IF
SA
FOR
SOME
INTEGER
A
IF
WE
WANT
TO
MINIMIZE
E
T
THUS
POWMAX
IS
OPTIMAL
NUMBER
OF
SERVERS
THAT
ARE
TURNED
ON
FOR
A
CUBIC
POWER
TO
FREQUENCY
RELATIONSHIP
GIVEN
BY
EQ
WE
HAVE
L
IN
THIS
CASE
FOR
THE
CASE
OF
SA
FOR
SOME
INTEGER
SI
SB
Α
PI
B
A
WE
WANT
TO
MINIMIZE
E
T
FOR
I
A
THIS
I
IS
ACHIEVED
BY
MAXIMIZING
SI
CLEARLY
BY
SETTING
A
PC
SI
WHERE
ΑL
SB
AND
B
ARE
AS
DEFINED
IN
SECTION
THUS
WE
HAVE
IS
MAXIMIZED
BY
I
C
FOR
I
A
HENCE
FOR
LOW
N
POWMAX
IS
OPTIMAL
THEOREM
GIVEN
A
CLOSED
K
SERVER
FARM
CONFIGURATION
I
SI
I
SB
ΑL
PI
B
J
WITH
A
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIP
THE
FOLLOWING
POWER
ALLOCATION
MINIMIZES
E
T
FOR
HIGH
N
BASED
ON
THE
ASYMPTOTIC
APPROXIMATIONS
IN
IF
SB
Α
N
C
P
JSB
ΑL
PI
B
I
SINCE
THE
SUM
OF
PI
IS
A
CONSTANT
THE
SUM
OF
THEIR
CUBE
IF
SB
Α
M
B
P
ROOTS
ATTAINS
ITS
MAXIMUM
VALUE
WHEN
ALL
THE
PI
ARE
EQUAL
B
M
M
K
THIS
FOLLOWS
FROM
THE
GENERALIZED
MEAN
INEQUALITY
THUS
PROOF
ASSUMING
THAT
N
IS
HIGH
FROM
WE
HAVE
THAT
WE
WISH
TO
MAXIMIZE
J
SB
PJ
B
BY
LOOKING
AT
WITHOUT
LOSS
OF
GENERALITY
ASSUME
SJ
RESPONSE
TIME
WE
FIND
THAT
POWMAX
IS
OPTIMAL
FOR
LOW
VALUES
OF
SB
WHEREAS
POWMED
IS
OPTIMAL
FOR
HIGH
VALUES
OF
WHERE
J
K
DENOTES
THE
NUMBER
OF
SERVERS
THAT
ARE
TURNED
SB
HENCE
THE
RESULT
ON
THUS
E
T
N
FROM
EQ
SINCE
N
IS
A
CONSTANT
MINIMIZING
E
T
IS
NOW
EQUIVALENT
TO
MINIMIZING
WHICH
ATTAINS
THE
LOWEST
VALUE
OF
WHEN
HOWEVER
ITSELF
ATTAINS
THE
LOWEST
VALUE
OF
WHEN
AND
SO
ON
THUS
TO
MINIMIZE
E
T
WE
MUST
SET
QJ
R
SAY
SINCE
J
Q
WE
HAVE
E
T
N
N
R
N
THUS
MINIMIZING
E
T
IS
EQUIVALENT
TO
MAXIMIZING
J
SI
FOR
A
LINEAR
POWER
TO
FREQUENCY
RELATIONSHIP
GIVEN
BY
EQ
WE
HAVE
SI
SB
Α
PI
B
SOMNILOQUY
AUGMENTING
NETWORK
INTERFACES
TO
REDUCE
PC
ENERGY
USAGE
YUVRAJ
AGARWAL
STEVE
HODGES
RANVEER
CHANDRA
JAMES
SCOTT
PARAMVIR
BAHL
RAJESH
GUPTA
MICROSOFT
RESEARCH
UNIVERSITY
OF
CALIFORNIA
SAN
DIEGO
SHODGES
RANVEER
JWS
BAHL
MICROSOFT
COM
ABSTRACT
REDUCING
THE
ENERGY
CONSUMPTION
OF
PCS
IS
BECOMING
IN
CREASINGLY
IMPORTANT
WITH
RISING
ENERGY
COSTS
AND
ENVIRONMEN
TAL
CONCERNS
SLEEP
STATES
SUCH
AS
SUSPEND
TO
RAM
SAVE
ENERGY
BUT
ARE
OFTEN
NOT
APPROPRIATE
BECAUSE
ONGOING
NETWORK
ING
TASKS
SUCH
AS
ACCEPTING
REMOTE
DESKTOP
LOGINS
OR
PERFORM
ING
BACKGROUND
FILE
TRANSFERS
MUST
BE
SUPPORTED
IN
THIS
PAPER
WE
PRESENT
SOMNILOQUY
AN
ARCHITECTURE
THAT
AUGMENTS
NETWORK
INTERFACES
TO
ALLOW
PCS
IN
TO
BE
RESPONSIVE
TO
NETWORK
TRAF
FIC
WE
SHOW
THAT
MANY
APPLICATIONS
SUCH
AS
REMOTE
DESKTOP
AND
VOIP
CAN
BE
SUPPORTED
WITHOUT
APPLICATION
SPECIFIC
CODE
IN
THE
AUGMENTED
NETWORK
INTERFACE
BY
USING
APPLICATION
LEVEL
WAKEUP
TRIGGERS
A
FURTHER
CLASS
OF
APPLICATIONS
SUCH
AS
IN
STANT
MESSAGING
AND
PEER
TO
PEER
FILE
SHARING
CAN
BE
SUPPORTED
WITH
MODEST
PROCESSING
AND
MEMORY
RESOURCES
IN
THE
NETWORK
INTERFACE
EXPERIMENTS
USING
OUR
PROTOTYPE
SOMNILOQUY
IMPLE
MENTATION
A
USB
BASED
NETWORK
INTERFACE
DEMONSTRATES
EN
ERGY
SAVINGS
OF
TO
IN
MOST
COMMONLY
OCCURING
SCE
NARIOS
THIS
TRANSLATES
TO
SIGNIFICANT
COST
SAVINGS
FOR
PC
USERS
INTRODUCTION
MANY
PERSONAL
COMPUTERS
PCS
REMAIN
SWITCHED
ON
FOR
MUCH
OR
ALL
OF
THE
TIME
EVEN
WHEN
A
USER
IS
NOT
PRESENT
DESPITE
THE
EXISTENCE
OF
LOW
POWER
MODES
SUCH
AS
SLEEP
OR
SUSPEND
TO
RAM
ACPI
STATE
AND
HIBERNATE
ACPI
STATE
THE
RESULTING
ELECTRICITY
USAGE
WASTES
MONEY
AND
HAS
A
NEGATIVE
IMPACT
ON
THE
ENVIRONMENT
PCS
ARE
LEFT
ON
FOR
A
VARIETY
OF
REASONS
SEE
SECTION
INCLUDING
ENSURING
REMOTE
ACCESS
TO
LOCAL
FILES
MAIN
TAINING
THE
REACHABILITY
OF
USERS
VIA
INCOMING
EMAIL
IN
STANT
MESSAGING
IM
OR
VOICE
OVER
IP
VOIP
CLIENTS
FILE
SHARING
AND
CONTENT
DISTRIBUTION
AND
SO
ON
UNFORTU
NATELY
THESE
ARE
ALL
INCOMPATIBLE
WITH
CURRENT
POWER
SAVING
SCHEMES
SUCH
AS
AND
IN
WHICH
THE
PC
DOES
NOT
RESPOND
TO
REMOTE
NETWORK
EVENTS
EXISTING
SOLUTIONS
FOR
SLEEP
MODE
RESPONSIVENESS
SUCH
AS
WAKE
ON
LAN
TION
SERVERS
OR
CONFIGURE
NETWORK
HARDWARE
A
FEW
INITIAL
PROPOSALS
SUGGEST
THE
USE
OF
NETWORK
PROXIES
TO
PERFORM
LIGHTWEIGHT
PROTOCOL
FUNCTIONALITY
SUCH
AS
RE
SPONDING
TO
ARPS
HOWEVER
SUCH
A
SYSTEM
TOO
REQUIRES
SIGNIFICANT
MODIFICATIONS
TO
THE
NETWORK
INFRASTRUCTURE
AND
TO
THE
BEST
OF
OUR
KNOWLEDGE
SUCH
A
PROTOTYPE
HAS
NOT
BEEN
DESCRIBED
IN
PUBLISHED
FORM
SEE
SECTION
FOR
A
FULL
DISCUSSION
IN
THIS
PAPER
WE
PRESENT
A
SYSTEM
CALLED
SOM
THAT
SUPPORTS
CONTINUOUS
OPERATION
OF
MANY
NETWORK
FACING
APPLICATIONS
EVEN
WHILE
A
PC
IS
ASLEEP
SOMNILOQUY
PROVIDES
FUNCTIONALITY
THAT
IS
NOT
PRESENT
IN
EXISTING
WAKE
UP
SYSTEMS
IN
PARTICULAR
IT
ALLOWS
A
PC
TO
SLEEP
WHILE
CONTINUING
TO
RUN
SOME
APPLICATIONS
SUCH
AS
BITTORRENT
AND
LARGE
WEB
DOWNLOADS
IN
THE
BACKGROUND
IN
EXISTING
SYSTEMS
THESE
APPLICATIONS
WOULD
STOP
WHEN
THE
PC
SLEEPS
SOMNILOQUY
ACHIEVES
THE
ABOVE
FUNCTIONALITY
BY
EM
BEDDING
A
LOW
POWER
SECONDARY
PROCESSOR
IN
THE
PC
NETWORK
INTERFACE
THIS
PROCESSOR
RUNS
AN
EMBEDDED
OP
ERATING
SYSTEM
AND
IMPERSONATES
THE
SLEEPING
PC
TO
OTHER
HOSTS
ON
THE
NETWORK
MANY
APPLICATIONS
CAN
BE
SUP
PORTED
EITHER
WITH
OR
WITHOUT
APPLICATION
SPECIFIC
CODE
STUBS
ON
THE
SECONDARY
PROCESSOR
APPLICATIONS
SIM
PLY
REQUIRING
THE
PC
TO
BE
WOKEN
UP
ON
AN
EVENT
CAN
BE
SUPPORTED
WITHOUT
STUBS
WHILE
OTHER
APPLICATIONS
REQUIRE
STUBS
BUT
IN
RETURN
SUPPORT
GREATER
LEVELS
OF
FUNCTIONALITY
DURING
THE
SLEEP
STATE
WE
HAVE
PROTOTYPED
SOMNILOQUY
USING
A
USB
BASED
LOW
POWER
NETWORK
INTERFACE
OUR
SYSTEM
WORKS
FOR
DESKTOPS
AND
LAPTOPS
OVER
WIRED
AND
WIRELESS
NETWORKS
AND
IS
INCREMENTALLY
DEPLOYABLE
ON
SYSTEMS
WITH
AN
EXISTING
NETWORK
INTERFACE
IT
DOES
NOT
REQUIRE
ANY
CHANGES
TO
THE
OPERATING
SYSTEM
TO
NETWORK
HARDWARE
E
G
ROUTERS
OR
TO
REMOTE
APPLICATION
SERVERS
WE
HAVE
IMPLEMENTED
SUPPORT
FOR
APPLICATIONS
INCLUDING
REMOTE
DESKTOP
ACCESS
SSH
TELNET
VOIP
IM
WEB
DOWNLOADS
WOL
HAVE
NOT
PROVEN
SUCCESSFUL
IN
THE
WILD
FOR
A
NUMBER
OF
REASONS
SUCH
AS
THE
NEED
TO
MODIFY
APPLICA
THE
ACT
OR
HABIT
OF
TALKING
IN
ONE
SLEEP
AND
BITTORRENT
OUR
SYSTEM
CAN
ALSO
BE
EXTENDED
TO
SUP
PORT
OTHER
APPLICATIONS
WE
HAVE
EVALUATED
SOMNILOQUY
IN
VARIOUS
SETTINGS
AND
IN
OUR
TESTBED
SECTION
A
PC
IN
SOMNILOQUY
MODE
CONSUMES
TO
LESS
POWER
THAN
A
PC
IN
IDLE
STATE
FOR
COMMONLY
OCCURRING
SCENARIOS
THIS
TRANSLATES
TO
ENERGY
SAVINGS
OF
TO
WE
MAKE
THE
FOLLOWING
CONTRIBUTIONS
IN
THIS
PAPER
WE
PRESENT
A
NEW
ARCHITECTURE
TO
SIGNIFICANTLY
RE
DUCE
THE
ENERGY
CONSUMPTION
OF
A
PC
WHILE
MAIN
TAINING
NETWORK
PRESENCE
THIS
IS
ACCOMPLISHED
WITHOUT
CHANGES
IN
THE
NETWORK
INFRASTRUCTURE
WE
SHOW
THAT
SEVERAL
APPLICATIONS
BITTORRENT
WEB
DOWNLOADS
IM
REMOTE
DESKTOP
ETC
CAN
CONSUME
MUCH
LESS
ENERGY
THIS
IS
ACHIEVED
WITH
OUT
MODIFYING
THE
REMOTE
APPLICATION
SERVERS
WE
PRESENT
AND
EMPIRICALLY
VALIDATE
A
MODEL
TO
PRE
DICT
THE
ENERGY
SAVINGS
OF
SOMNILOQUY
FOR
VARIOUS
APPLICATIONS
WE
DEMONSTRATE
THE
FEASIBILITY
OF
SOMNILOQUY
VIA
A
PROTOTYPE
USING
COMMODITY
HARDWARE
THIS
PROTO
TYPE
IS
INCREMENTALLY
DEPLOYABLE
AND
SAVES
SIGNIFI
CANT
ENERGY
IN
A
NUMBER
OF
SCENARIOS
MOTIVATION
PRIOR
STUDIES
HAVE
SHOWN
THAT
THAT
USERS
OFTEN
LEAVE
THEIR
COMPUTER
POWERED
ON
EVEN
WHEN
THEY
ARE
LARGELY
IDLE
A
STUDY
BY
ROBERSON
ET
AL
SHOWS
THAT
IN
OFFICES
OF
DESKTOP
PCS
REMAIN
POWERED
ON
OUTSIDE
WORK
HOURS
AND
ONLY
USE
SLEEP
MODE
IN
HOME
ENVI
RONMENTS
ROTH
ET
AL
SHOW
THAT
AVERAGE
RESIDENTIAL
COMPUTER
IS
ON
OF
THE
TIME
BUT
IS
NOT
BEING
ACTIVELY
USED
FOR
MORE
THAN
HALF
THE
TIME
TO
UNCOVER
THE
REASONS
WHY
PEOPLE
DO
NOT
USE
SLEEP
MODE
WE
CONDUCTED
AN
INFORMAL
SURVEY
WE
PASSED
IT
AMONG
OUR
CONTACTS
WHO
IN
TURN
CIRCULATED
IT
FURTHER
WE
HAD
RESPONDENTS
FROM
VARIOUS
PARTS
OF
THE
WORLD
OF
WHICH
WORKED
IN
THE
IT
SECTOR
OF
THE
RESPON
DENTS
LEFT
AT
LEAST
ONE
MACHINE
AT
HOME
ON
ALL
OF
THE
TIME
AND
OF
THE
RESPONDENTS
LEFT
AT
LEAST
ONE
WORK
MA
CHINE
ON
EVEN
WHEN
NO
ONE
WAS
USING
IT
AMONG
THE
PEOPLE
WHO
LEFT
THEIR
HOME
MACHINE
POW
ERED
ON
DID
SO
FOR
REMOTE
ACCESS
FOR
QUICK
AVAILABILITY
AND
FOR
APPLICATIONS
RUNNING
IN
THE
BACK
GROUND
OF
WHICH
FILE
SHARING
DOWNLOADING
AND
IM
E
MAIL
WERE
MOST
POPULAR
IN
THE
OFFICE
ENVI
RONMENT
OF
RESPONDENTS
LEFT
THEIR
MACHINES
ON
FOR
REMOTE
ACCESS
AND
DID
SO
TO
SUPPORT
APPLICATIONS
RUNNING
IN
THE
BACKGROUND
OF
WHICH
E
MAIL
AND
IM
WERE
MOST
POPULAR
ALTHOUGH
THIS
SURVEY
SHOULD
NOT
BE
REGARDED
AS
REPRE
FIGURE
SOMNILOQUY
AUGMENTS
THE
PC
NETWORK
INTER
FACE
WITH
A
LOW
POWER
SECONDARY
PROCESSOR
THAT
RUNS
AN
EMBEDDED
OS
AND
NETWORKING
STACK
NETWORK
PORT
FILTERS
AND
LIGHTWEIGHT
VERSIONS
OF
CERTAIN
APPLICATIONS
STUBS
SHADING
INDICATES
ELEMENTS
INTRODUCED
BY
SOMNILOQUY
PCS
DON
T
GO
TO
SLEEP
EVEN
WHEN
THEY
ARE
UNUSED
SEC
OND
SIGNIFICANT
ENERGY
SAVINGS
CAN
BE
ACHIEVED
IF
ONLY
A
FEW
APPLICATIONS
REMOTE
REACHABILITY
FILE
SHARING
FILE
DOWNLOADS
INSTANT
MESSAGING
E
MAIL
CAN
BE
HANDLED
WHEN
THE
PC
IS
ASLEEP
THE
SOMNILOQUY
ARCHITECTURE
OUR
PRIMARY
AIMS
DURING
THE
DEVELOPMENT
OF
SOMNILO
QUY
WERE
TO
ALLOW
AN
UNATTENDED
PC
TO
BE
IN
LOW
POWER
STATE
WHILE
STILL
BEING
AVAILABLE
AND
ACTIVE
FOR
NETWORK
FACING
APPLICATIONS
AS
IF
THE
PC
WERE
FULLY
ON
TO
DO
SO
WITHOUT
CHANGING
THE
USER
EXPERIENCE
OF
THE
PC
OR
REQUIRING
MODIFICATION
TO
THE
NETWORK
INFRAS
TRUCTURE
OR
REMOTE
APPLICATION
SERVERS
WE
ACCOMPLISH
THESE
GOALS
BY
AUGMENTING
THE
PC
NETWORK
INTERFACE
HARDWARE
WITH
AN
ALWAYS
ON
LOW
POWER
EMBEDDED
CPU
AS
SHOWN
IN
FIGURE
THIS
SEC
ONDARY
PROCESSOR
HAS
A
RELATIVELY
SMALL
AMOUNT
OF
MEM
ORY
AND
FLASH
STORAGE
WHICH
CONSUMES
MUCH
LESS
POWER
THAN
IF
IT
WERE
SHARING
THE
LARGER
DISK
AND
MEMORY
OF
THE
HOST
PROCESSOR
IT
RUNS
AN
EMBEDDED
OPERATING
SYSTEM
WITH
A
FULL
TCP
IP
NETWORKING
STACK
SUCH
AS
EMBEDDED
LINUX
OR
WINDOWS
CE
THE
FLASH
STORAGE
IS
USED
AS
A
TEMPORARY
BUFFER
TO
STORE
DATA
BEFORE
THE
DATA
IS
TRANS
FERRED
IN
A
LARGER
CHUNK
TO
THE
PC
A
LARGER
FLASH
ON
THE
SECONDARY
PROCESSOR
ALLOWS
THE
PC
TO
SLEEP
LONGER
SEC
TION
THIS
ARCHITECTURE
HAS
A
COUPLE
OF
USEFUL
PROP
ERTIES
FIRST
IT
DOES
NOT
REQUIRE
ANY
CHANGES
TO
THE
HOST
OPERATING
SYSTEM
AND
SECOND
IT
CAN
BE
INCREMENTALLY
DE
PLOYED
ON
EXISTING
PCS
USING
A
PERIPHERAL
NETWORK
INTER
FACE
SECTION
SENTATIVE
OF
ALL
USERS
AND
IS
NOT
STATISTICALLY
SIGNIFICANT
IT
DOES
HIGHLIGHT
TWO
IMPORTANT
POINTS
FIRST
A
NUMBER
OF
PROTOTYPE
HAD
MB
DRAM
AND
GB
OF
FLASH
THE
SOFTWARE
COMPONENTS
OF
SOMNILOQUY
AND
THEIR
IN
TERACTIONS
ARE
ILLUSTRATED
IN
FIGURE
THE
HIGH
LEVEL
OPER
ATION
OF
SOMNILOQUY
IS
AS
FOLLOWS
WHEN
THE
HOST
PC
IS
POWERED
ON
THE
SECONDARY
PROCESSOR
DOES
NOTHING
THE
NETWORK
STACK
ON
THE
HOST
PROCESSOR
COMMUNICATES
DI
RECTLY
WITH
THE
NETWORK
INTERFACE
HARDWARE
WHEN
THE
PC
INITIATES
SLEEP
THE
SOMNILOQUY
DAEMON
ON
THE
HOST
PRO
CESSOR
CAPTURES
THE
SLEEP
EVENT
AND
TRANSFERS
THE
NETWORK
STATE
TO
THE
SECONDARY
PROCESSOR
THIS
STATE
INCLUDES
THE
ARP
TABLE
ENTRIES
IP
ADDRESS
DHCP
LEASE
DETAILS
AND
ASSOCIATED
SSID
FOR
WIRELESS
NETWORKS
I
E
MAC
AND
IP
LAYER
INFORMATION
IT
ALSO
INCLUDES
DETAILS
OF
WHAT
EVENTS
THE
HOST
SHOULD
BE
WOKEN
ON
AND
APPLICATION
SPECIFIC
DE
TAILS
SUCH
AS
ONGOING
FILE
DOWNLOADS
THAT
SHOULD
CONTINUE
DURING
SLEEP
FOLLOWING
THE
TRANSFER
OF
THIS
INFORMATION
TO
THE
SECONDARY
PROCESSOR
THE
HOST
PC
ENTERS
SLEEP
ALTHOUGH
THE
HOST
PROCESSOR
IS
ASLEEP
POWER
TO
THE
NETWORK
INTERFACE
AND
THE
SECONDARY
PROCESSOR
IS
MAIN
TAINED
TO
MAINTAIN
TRANSPARENT
REACHABILITY
TO
THE
HOST
WHILE
IT
IS
ASLEEP
THE
SECONDARY
PROCESSOR
IMPER
SONATES
THE
HOST
BY
USING
THE
SAME
MAC
AND
IP
AD
DRESSES
HOST
NAME
DHCP
DETAILS
AND
FOR
WIRELESS
THE
SAME
SSID
IT
ALSO
HANDLES
TRAFFIC
AT
THE
LINK
AND
NETWORK
LAYERS
SUCH
AS
ARP
REQUESTS
AND
PINGS
THEREBY
MAIN
TAINING
BASIC
PRESENCE
ON
THE
NETWORK
NEW
INCOMING
CONNECTION
REQUESTS
FOR
THE
HOST
PROCESSOR
ARE
NOW
RE
CEIVED
AND
HANDLED
BY
THE
NETWORK
STACK
RUNNING
ON
THE
SECONDARY
PROCESSOR
IN
THIS
WAY
THE
PC
TRANSITION
INTO
SLEEP
IS
TRANSPARENT
TO
REMOTE
HOSTS
ON
THE
NETWORK
TO
ENSURE
THAT
THE
HOST
PC
IS
REACHABLE
BY
VARIOUS
AP
PLICATIONS
A
PROCESS
ON
THE
SECONDARY
PROCESSOR
MON
ITORS
INCOMING
PACKETS
THIS
PROCESS
WATCHES
FOR
PAT
TERNS
SUCH
AS
REQUESTS
ON
SPECIFIC
PORT
NUMBERS
WHICH
SHOULD
TRIGGER
WAKE
UP
OF
THE
HOST
PROCESSOR
ALTHOUGH
THIS
SIMPLE
ARCHITECTURE
SUPPORTS
SEVERAL
AP
PLICATIONS
WITH
MINIMAL
COMPLEXITY
SOMNILOQUY
CAN
GET
MUCH
GREATER
ENERGY
SAVINGS
FOR
SOME
APPLICATIONS
BY
NOT
WAKING
UP
THE
HOST
PROCESSOR
FOR
SIMPLE
TASKS
FOR
EXAMPLE
TO
SEND
INSTANT
MESSENGER
PRESENCE
UPDATES
TO
PERFORM
THESE
TASKS
ON
THE
SECONDARY
PROCESSOR
WE
RE
QUIRE
THE
APPLICATION
WRITER
TO
ADD
A
SMALL
AMOUNT
OF
APPLICATION
SPECIFIC
CODE
STUBS
ON
THE
HOST
AND
SEC
ONDARY
PROCESSOR
IN
THE
REST
OF
THIS
SECTION
WE
DESCRIBE
IN
MORE
DETAIL
HOW
WE
HANDLE
VARIOUS
APPLICATIONS
WITH
AND
WITHOUT
APPLICATION
STUBS
SOMNILOQUY
WITHOUT
APPLICATION
STUBS
THE
SOMNILOQUY
DAEMON
ON
THE
HOST
PROCESSOR
SPECI
FIES
PACKET
FILTERS
I
E
PATTERNS
ON
INCOMING
PACKETS
ON
WHICH
THE
SECONDARY
PROCESSOR
SHOULD
WAKE
UP
THE
HOST
PROCESSOR
FROM
SLEEP
STATE
THE
SOMNILOQUY
DAEMON
CRE
ATES
FILTERS
AT
VARIOUS
LAYERS
OF
THE
NETWORK
STACK
AT
THE
LINK
LAYER
AND
NETWORK
LAYER
THE
SECONDARY
PROCESSOR
CAN
FIGURE
SOMNILOQUY
SOFTWARE
COMPONENTS
ON
THE
HOST
PC
AND
THE
SECONDARY
PROCESSOR
AND
THEIR
INTERACTIONS
BE
TOLD
TO
WAKE
THE
COMPUTER
WHEN
IT
DETECTS
A
PARTICULAR
PACKET
ANALOGOUSLY
TO
THE
MAGIC
PACKETS
USED
BY
WAKE
ON
LAN
THOUGH
NOT
REQUIRING
THE
MAC
ADDRESS
TO
BE
KNOWN
BY
THE
REMOTE
HOST
SEE
FURTHER
DISCUSSION
IN
SEC
TION
TRIGGER
CONDITIONS
AT
THE
TRANSPORT
LAYER
MAY
ALSO
BE
SPECIFIED
FOR
EXAMPLE
WAKE
ON
TCP
PORT
FOR
TELNET
REQUESTS
SIMILARLY
SOMNILOQUY
ALSO
SUPPORTS
WAKE
UPS
ON
PATTERNS
IN
THE
APPLICATION
PAYLOAD
ALTHOUGH
THE
HOST
PC
WILL
WAKE
UP
WITHIN
A
FEW
SEC
ONDS
IT
WILL
NOT
RECEIVE
THE
PACKET
THAT
TRIGGERED
THE
WAKE
UP
ONE
WAY
TO
SOLVE
THIS
PROBLEM
IS
TO
BUFFER
THE
PACKET
ON
THE
SECONDARY
PROCESSOR
AND
REPLAY
IT
ON
THE
NETWORK
STACK
OF
THE
HOST
PROCESSOR
ONCE
IT
HAS
WOKEN
UP
HOWEVER
SINCE
THE
TIME
TO
WAKE
UP
IS
JUST
A
FEW
SEC
ONDS
MOST
SOURCES
CAN
BE
RELIED
UPON
TO
RETRY
THE
CON
NECTION
REQUEST
FOR
EXAMPLE
ANY
PROTOCOL
USING
TCP
AS
THE
TRANSPORT
LAYER
WILL
AUTOMATICALLY
RETRANSMIT
THE
INITIAL
SYN
PACKET
EVEN
UDP
BASED
APPLICATIONS
THAT
ARE
DESIGNED
FOR
INTERNET
USE
ARE
DESIGNED
TO
COPE
WITH
PACKET
LOSS
USING
AUTOMATIC
RETRANSMISSIONS
THIS
SIMPLE
PACKET
FILTER
BASED
APPROACH
TO
TRIGGER
ING
WAKE
UPS
HAS
THE
ADVANTAGE
THAT
APPLICATION
SPECIFIC
CODE
DOES
NOT
NEED
TO
BE
EXECUTED
ON
THE
SECONDARY
PRO
CESSOR
NONETHELESS
IT
IS
SUFFICIENT
TO
SUPPORT
MANY
AP
PLICATIONS
THAT
GET
TRIGGERED
ON
REMOTE
CONNECTION
RE
QUESTS
SUCH
AS
REMOTE
FILE
ACCESS
REMOTE
DESKTOP
ACCESS
TELNET
AND
SSH
REQUESTS
TO
NAME
A
FEW
APPLICATION
SPECIFIC
EXTENSIONS
SEVERAL
APPLICATIONS
MAINTAIN
ACTIVE
STATE
ON
THE
PC
EVEN
WHEN
IT
IS
IDLE
AND
HENCE
PREVENT
A
PC
FROM
GOING
TO
SLEEP
FOR
EXAMPLE
A
MOVIE
DOWNLOAD
CLIENT
ON
A
HOME
PC
E
G
FROM
NETFLIX
WILL
REQUIRE
THE
HOST
PC
TO
BE
AWAKE
FOR
A
FEW
HOURS
WHILE
DOWNLOADING
THE
MOVIE
AN
INSTANT
MESSENGER
IM
CLIENT
WILL
REQUIRE
THE
PC
TO
BE
ON
IN
ORDER
FOR
THE
USER
TO
STAY
ONLINE
REACHABLE
TO
THEIR
CONTACTS
SOMNILOQUY
PROVIDES
A
WAY
FOR
THESE
APPLICATIONS
TO
CONSUME
SIGNIFICANTLY
LESS
POWER
BY
PERFORMING
LIGHTWEIGHT
OPERATIONS
ON
THE
SECONDARY
PROCESSOR
IT
CAN
OPPORTUNISTICALLY
PUT
THE
HOST
PROCESSOR
TO
SLEEP
FOR
EXAMPLE
THE
SECONDARY
PROCESSOR
CAN
SEND
AND
RE
CEIVE
PRESENCE
UPDATES
TO
FROM
THE
IM
SERVER
WHILE
THE
HOST
PROCESSOR
IS
ASLEEP
DURING
A
LARGE
DOWNLOAD
THE
SECONDARY
PROCESSOR
CAN
DOWNLOAD
PORTIONS
OF
THE
FILE
PUTTING
THE
HOST
PROCESSOR
TO
SLEEP
IN
THE
MEANTIME
THE
KEY
TO
SUPPORTING
THESE
APPLICATIONS
IS
THE
USE
OF
STUBS
THAT
RUN
ON
THE
HOST
AND
THE
SECONDARY
PROCES
SOR
WE
HAVE
IMPLEMENTED
STUBS
FOR
THREE
POPULAR
AP
PLICATIONS
IM
MSN
AOL
ICQ
BITTORRENT
AND
WEB
DOWNLOAD
HERE
WE
WILL
DESCRIBE
THE
GENERAL
GUIDELINES
FOR
WRITING
THESE
STUBS
AND
DESCRIBE
THE
SPECIFIC
IMPLE
MENTATIONS
FOR
THE
THREE
APPLICATIONS
IN
SECTION
WRITING
APPLICATION
STUBS
WHEN
DESIGNING
AN
APPLI
CATION
STUB
THE
FIRST
STEP
IS
TO
UNDERSTAND
THE
SUBSET
OF
THE
APPLICATION
FUNCTIONALITY
THAT
NEEDS
TO
RUN
WHEN
THE
PC
IS
ASLEEP
THIS
IS
IMPLEMENTED
AS
A
STUB
ON
THE
SECONDARY
PROCESSOR
FOR
EXAMPLE
FOR
AN
IM
STUB
THE
FUNCTIONALITY
TO
SEND
AND
RECEIVE
PRESENCE
UPDATES
IS
ESSENTIAL
TO
MAIN
TAIN
IM
REACHABILITY
HOWEVER
THE
STUB
NEED
NOT
INCLUDE
ANY
UI
RELATED
CODE
SUCH
AS
OPENING
A
CHAT
WINDOW
WE
NOTE
THAT
IT
IS
NOT
FEASIBLE
FOR
THE
STUB
TO
REUSE
THE
ENTIRE
ORIGINAL
APPLICATION
CODE
FROM
THE
HOST
PC
THE
APPLICATION
CODE
MIGHT
DEPEND
ON
DRIVERS
DISPLAY
DISK
ETC
THAT
ARE
ABSENT
ON
THE
SECONDARY
PROCESSOR
FURTHER
MORE
RUNNING
THE
ENTIRE
APPLICATION
MIGHT
OVERLOAD
THE
SECONDARY
PROCESSOR
THEREFORE
ONLY
THE
ESSENTIAL
COM
PONENTS
OF
THE
APPLICATION
ARE
IMPLEMENTED
AS
PART
OF
THE
APPLICATION
STUB
ANOTHER
STEP
IN
DESIGNING
APPLICATION
STUBS
IS
TO
DE
CIDE
WHEN
TO
WAKE
UP
THE
HOST
PROCESSOR
TRIGGERS
CAN
BE
USER
DEFINED
FOR
EXAMPLE
WAKING
UP
ON
AN
INCOMING
CALL
FROM
A
SPECIFIC
IM
CONTACT
TRIGGERS
MAY
ALSO
OCCUR
WHEN
THE
SECONDARY
PROCESSOR
RESOURCES
ARE
INSUFFI
CIENT
FOR
EXAMPLE
WHEN
THE
FLASH
IS
FULL
OR
MORE
CPU
RE
SOURCES
ARE
NEEDED
IN
ALL
OF
THESE
CASES
THE
STUB
WAKES
UP
THE
HOST
PROCESSOR
TO
INTERFACE
WITH
THE
APPLICATION
ON
THE
HOST
PC
AND
THE
SOMNILOQUY
DAEMON
THE
APPLICATION
STUB
NEEDS
TO
HAVE
A
COMPONENT
ON
THE
HOST
PROCESSOR
THIS
COMPO
NENT
REGISTERS
TWO
CALLBACK
FUNCTIONS
WITH
THE
SOMNILO
QUY
DAEMON
ONE
THAT
IS
CALLED
JUST
BEFORE
THE
PC
GOES
TO
SLEEP
AND
THE
OTHER
JUST
AFTER
IT
HAS
WOKEN
UP
THE
FIRST
FUNCTION
TRANSFERS
THE
APPLICATION
STATE
TO
THE
STUB
ON
THE
SECONDARY
PROCESSOR
AND
ALSO
SETS
THE
TRIGGER
CONDI
TIONS
ON
WHICH
TO
WAKE
THE
HOST
PROCESSOR
THESE
VAL
UES
DEPEND
ON
THE
APPLICATION
BEING
HANDLED
BY
THE
STUB
THE
SECOND
CALLBACK
FUNCTION
WHICH
IS
CALLED
WHEN
THE
HOST
RESUMES
FROM
SLEEP
CHECKS
THE
EVENT
THAT
CAUSED
THE
WAKEUP
WHETHER
IT
WAS
CAUSED
BY
A
TRIGGER
CON
DITION
ON
THE
SECONDARY
PROCESSOR
OR
DUE
TO
USER
ACTIV
ITY
IT
HANDLES
THESE
EVENTS
DIFFERENTLY
IF
THE
WAKEUP
WAS
CAUSED
BY
USER
ACTIVITY
THE
STUB
TRANSFERS
STATE
FROM
THE
SECONDARY
PROCESSOR
AND
DISABLES
IT
HOWEVER
IF
THE
WAKEUP
WAS
CAUSED
BY
A
TRIGGER
CONDITION
ON
THE
SEC
ONDARY
PROCESSOR
THE
APPLICATION
STUB
HANDLES
IT
AS
DE
FINED
BY
THE
USER
FOR
EXAMPLE
FOR
AN
INCOMING
VOIP
CALL
THE
STUB
ENGAGES
THE
INCOMING
CALL
FUNCTIONALITY
OF
THE
VOIP
APPLICATION
HAVING
DETERMINED
WHAT
FUNCTIONALITY
NEEDS
TO
BE
SUP
PORTED
BY
THE
APPLICATION
STUB
AND
HOST
BASED
CALLBACKS
AND
WHAT
STATE
MUST
PASS
BETWEEN
THEM
THE
FINAL
STEP
IS
TO
IMPLEMENT
THIS
WE
HAVE
USED
TWO
MANUAL
APPROACHES
TO
DOING
THIS
FOR
THE
DOWNLOAD
STUB
WE
BUILT
ALL
THE
FUNCTIONALITY
OURSELVES
BASED
ON
DETAILED
KNOWLEDGE
OF
THE
APPLICATION
PROTOCOLS
AND
FOR
THE
BITTORRENT
AND
IM
STUBS
WE
TRIMMED
DOWN
EXISTING
APPLICATION
CODE
TO
RE
DUCE
MEMORY
AND
CPU
FOOTPRINT
AN
ALTERNATIVE
COULD
BE
TO
AUTOMATICALLY
LEARN
PROTOCOL
BEHAVIOR
TO
BUILD
THESE
APPLICATION
STUBS
HOWEVER
WE
BELIEVE
THAT
THIS
IS
AN
EXTREMELY
DIFFICULT
PROBLEM
THERE
ARE
PARTS
OF
THE
AP
PLICATION
THAT
ARE
DIFFICULT
TO
INFER
AND
ANY
INACCURACY
IN
THE
APPLICATION
STUB
WILL
MAKE
IT
UNUSABLE
FOR
EXAM
PLE
KNOWLEDGE
OF
HOW
BITTORRENT
HASHES
THE
FILE
BLOCKS
IS
NECESSARY
FOR
THE
STUB
TO
SUCCESSFULLY
SHARE
A
FILE
WITH
PEERS
WE
ARE
UNAWARE
OF
ANY
AUTOMATIC
TOOL
THAT
CAN
LEARN
SUCH
APPLICATION
BEHAVIOR
THEREFORE
WE
BELIEVE
THAT
THE
BEST
ALTHOUGH
PERHAPS
NOT
THE
MOST
ELEGANT
APPROACH
TO
BUILDING
THESE
STUBS
IS
TO
MODIFY
APPLICA
TION
SOURCE
CODE
AND
REMOVE
FUNCTIONALITY
THAT
IS
NOT
RE
QUIRED
BY
THE
SECONDARY
PROCESSOR
IN
THE
FUTURE
WITH
A
GREATER
INCENTIVE
TO
SAVE
ENERGY
WE
EXPECT
THAT
APPLI
CATION
DEVELOPERS
WILL
COMPETE
FOR
ENERGY
CONSUMPTION
AND
HENCE
PROVIDE
STUBS
FOR
THEIR
APPLICATIONS
USING
THE
GUIDELINES
DESCRIBED
IN
THIS
SECTION
WE
REALIZE
THAT
PARTIAL
APPLICATION
STUBS
MIGHT
BE
CRE
ATED
USING
TOOLS
SUCH
AS
THE
GENERIC
APPLICATION
LEVEL
PROTOCOL
ANALYZER
AND
DISCOVERER
WHICH
AUTO
MATICALLY
LEARN
THE
BEHAVIOR
AND
MESSAGE
FORMATS
FOR
A
RANGE
OF
PROTOCOLS
AS
PART
OF
FUTURE
WORK
WE
PLAN
TO
EXPLORE
HOW
THE
KNOWLEDGE
OF
THE
PROTOCOL
CAN
BE
AUG
MENTED
WITH
APPLICATION
SPECIFIC
BEHAVIOR
TO
EASE
THE
DE
VELOPMENT
OF
APPLICATION
STUBS
WHEN
TO
USE
APPLICATION
STUBS
NOT
ALL
APPLICATIONS
ARE
CONDUCIVE
TO
LOW
POWER
OPERATION
VIA
APPLICATION
STUBS
A
CPU
INTENSIVE
APPLICATION
SUCH
AS
A
COMPI
LATION
JOB
WILL
BE
VERY
SLOW
ON
THE
SECONDARY
PROCESSOR
SINCE
IT
HAS
A
LESS
POWERFUL
CPU
AND
LOW
MEMORY
SIMI
LARLY
AN
I
O
INTENSIVE
APPLICATION
SUCH
AS
A
DISK
INDEXER
WILL
NEED
TO
READ
THE
DISK
VERY
OFTEN
AND
WILL
THEREFORE
NEED
THE
PC
TO
BE
AWAKE
DOWNLOAD
AND
FILE
SHARING
AP
PLICATIONS
ARE
AN
INTERESTING
EXCEPTION
BECAUSE
PORTIONS
OF
A
FILE
CAN
BE
TRANSFERRED
BY
THE
SECONDARY
PROCESSOR
WHILST
THE
HOST
SLEEPS
WE
WILL
DISCUSS
THIS
APPROACH
IN
MORE
DETAIL
IN
SECTION
EVEN
FOR
AN
APPLICATION
STUB
THAT
SAVES
ENERGY
FOR
A
GIVEN
APPLICATION
IT
IS
NOT
ALWAYS
USEFUL
TO
OFFLOAD
THE
AP
PLICATION
TO
THE
SECONDARY
PROCESSOR
WHEN
THE
HOST
PC
IS
GOING
TO
SLEEP
SEVERAL
OTHER
APPLICATIONS
MAY
ALSO
WANT
TO
RUN
THEIR
APPLICATION
STUBS
ON
THE
SECONDARY
PROCESSOR
THIS
MIGHT
OVERLOAD
THE
CPU
OF
THE
WEAKER
LOW
POWER
SECONDARY
PROCESSOR
IN
THIS
CASE
IT
MIGHT
BE
BENEFICIAL
TO
KEEP
THE
HOST
PC
AWAKE
ONE
WAY
TO
SOLVE
THIS
PROBLEM
IS
TO
MODIFY
THE
SOM
NILOQUY
DAEMON
TO
PREDICT
THE
CPU
UTILIZATION
OF
THE
STUBS
FOR
ALL
APPLICATIONS
THAT
ARE
WILLING
TO
BE
OFFLOADED
TO
THE
SECONDARY
PROCESSOR
HOWEVER
MAKING
THIS
PRE
DICTION
IS
EXTREMELY
DIFFICULT
THERE
MIGHT
BE
LITTLE
COR
RELATION
BETWEEN
THE
CPU
UTILIZATION
OF
THE
APPLICATION
ON
THE
HOST
PC
AND
THE
STUB
ON
THE
SECONDARY
PROCES
SOR
BECAUSE
OF
DIFFERENT
PROCESSOR
ARCHITECTURES
AND
VARYING
APPLICATION
DEMANDS
INSTEAD
WE
TAKE
A
SYS
TEMS
APPROACH
WE
MONITOR
THE
CPU
UTILIZATION
OF
THE
SECONDARY
PROCESSOR
IF
IT
REMAINS
AT
MORE
THAN
CONTINUOUSLY
SECONDS
WE
WAKE
UP
THE
PC
AND
RE
SUME
ALL
APPLICATIONS
ON
THE
HOST
PROCESSOR
IF
THE
CPU
UTILIZATION
OF
THESE
APPLICATIONS
DECREASES
BY
MORE
THAN
ON
THE
HOST
PROCESSOR
WE
REPEAT
THE
SAME
PROCEDURE
OFFLOAD
TO
THE
SECONDARY
PROCESSOR
AND
STAY
THERE
IF
CPU
UTILIZATION
IS
LESS
THAN
IN
OUR
SOMNILOQUY
DE
PLOYMENT
THE
NEED
TO
MOVE
APPLICATIONS
AROSE
WHEN
RUN
NING
MULTIPLE
APPLICATION
STUBS
ON
THE
SECONDARY
PROCES
SOR
SUCH
AS
TWO
CONCURRENT
MBPS
WEB
DOWNLOADS
AND
TWO
CONCURRENT
BITTORRENT
DOWNLOADS
OF
SECTION
INCREMENTAL
DEPLOYMENT
WE
REALIZE
THAT
SOMNIL
OQUY
MAY
NEVER
BE
UNIVERSALLY
DEPLOYED
AND
THAT
GET
TING
SOFTWARE
VENDORS
TO
TRY
FOR
INCREMENTAL
DEPLOYMENT
WHEN
IT
WOULD
PREVIOUSLY
HAVE
BEEN
AWAKE
FOR
APPLICA
TIONS
WITHOUT
STUBS
THIS
PROPORTION
IS
LARGELY
DEPENDENT
ON
THE
ACTIONS
OF
A
REMOTE
USER
HOW
FREQUENTLY
A
RE
MOTE
SSH
SESSION
IS
INITIATED
FOR
EXAMPLE
AND
FOR
HOW
LONG
ON
THE
OTHER
HAND
FOR
APPLICATIONS
WITH
STUBS
THE
SECONDARY
PROCESSOR
MAY
REGULARLY
WAKE
UP
THE
HOST
TO
PERFORM
SOME
TASK
OR
OTHER
WE
QUANTIFY
THE
ENERGY
SAV
INGS
FOR
AN
APPLICATION
WITH
DIFFERENT
WAKE
UP
INTERVALS
IN
SECTION
MORE
FORMALLY
SUPPOSE
THE
HOST
IS
WOKEN
UP
ONCE
EV
ERY
TSLEEP
SECONDS
WHEREUPON
IT
STAYS
AWAKE
FOR
TAWAKE
SECONDS
TAWAKE
INCLUDES
THE
TIME
IT
TAKES
TO
TRANSFER
DATA
BETWEEN
THE
PC
AND
THE
SECONDARY
PROCESSOR
ALSO
ASSUME
THAT
D
IS
SUM
OF
THE
TIME
TO
WAKE
UP
THE
HOST
PLUS
THE
TIME
TO
TRANSITION
TO
SLEEP
SUPPOSE
PA
IS
THE
POWER
CONSUMPTION
OF
THE
PC
WHEN
IT
IS
AWAKE
IN
W
PS
IS
POWER
CONSUMED
IN
SLEEP
MODE
IN
W
AND
PE
IS
POWER
CONSUMED
BY
THE
SECONDARY
EMBED
DED
PROCESSOR
IN
W
THE
ENERGY
E
CONSUMED
DURING
SOMNILOQUY
OPERATION
IS
GIVEN
BY
ESOMNILOQUY
EPCINSLEEPMODE
EPCINAWAKEMODE
ESECONDARYPROCESSOR
TSLEEP
PS
TAWAKE
D
PA
TAWAKE
D
TSLEEP
PE
JOULES
IN
THE
ABSENCE
OF
SOMNILOQUY
THE
AMOUNT
OF
ENERGY
CONSUMED
BY
THE
HOST
PC
IN
THE
SAME
TIME
IS
EHOST
PA
TAWAKE
TSLEEP
JOULES
THEREFORE
THE
RATIO
OF
ENERGY
CONSUMED
BY
SOMNILOQUY
COMPARED
TO
THE
HOST
PC
BEING
ALWAYS
ON
IS
GIVEN
BY
ESOMNILOQUY
TSLEEP
PE
PS
TAWAKE
PA
PE
D
PA
PS
REQUIRES
A
LOW
EFFORT
MECHANISM
TO
ENSURE
THAT
THEIR
EHOST
PA
TAWAKE
TSLEEP
SOMNILOQUY
ENHANCED
SOFTWARE
IS
COMPATIBLE
WITH
MA
CHINES
AND
PLATFORMS
THAT
DO
NOT
HAVE
SOMNILOQUY
SUP
PORT
THE
SOMNILOQUY
DAEMON
QUERIES
THE
OS
TO
DE
TERMINE
THE
PRESENCE
OF
A
SECONDARY
PROCESSOR
AND
THE
SUPPORTED
APPLICATION
STUBS
APPLICATIONS
THEN
NEED
TO
QUERY
THE
SOMNILOQUY
DAEMON
AND
INVOKE
THE
APPLICA
TION
STUBS
ONLY
IF
THE
OS
SUPPORTS
SOMNILOQUY
AND
THE
CORRESPONDING
STUBS
ARE
IMPLEMENTED
ON
THE
SECONDARY
PROCESSOR
QUANTIFYING
ENERGY
SAVINGS
THE
AMOUNT
OF
ENERGY
SAVED
THROUGH
ADOPTION
OF
SOM
NILOQUY
IS
QUITE
EASY
TO
PREDICT
IT
DEPENDS
ON
THE
RELATIVE
POWER
CONSUMPTION
OF
THE
AWAKE
AND
SLEEP
STATES
AND
THE
PROPORTION
OF
TIME
THAT
A
MACHINE
CAN
BE
KEPT
ASLEEP
TYPICALLY
AS
WE
SHOW
IN
SECTION
PE
AND
PS
ARE
TWO
ORDERS
OF
MAGNITUDE
LESS
THAN
PA
FOR
A
DESKTOP
COMPUTER
AND
D
IS
AROUND
SECONDS
TO
WAKE
UP
THE
HOST
AND
PUT
IT
BACK
TO
SLEEP
THEREFORE
FOR
MOST
ENERGY
SAVINGS
WE
WOULD
WANT
TAWAKE
TO
BE
MUCH
LESS
THAN
TSLEEP
I
E
IF
TAWAKE
TSLEEP
THEN
THE
RATIO
ESOMNILOQUY
EHOST
IS
APPROXIMATELY
PE
PS
PA
WE
WILL
PRESENT
THE
APPROXIMATE
ENERGY
SAVINGS
FOR
DIFFERENT
APPLICATIONS
IN
SECTION
OF
COURSE
SOMNILOQUY
COULD
SAVE
MORE
ENERGY
BY
DIS
ABLING
THE
SECONDARY
PROCESSOR
WHEN
THE
PC
IS
AWAKE
THIS
WOULD
REQUIRE
THE
PC
TO
ENABLE
THE
SECONDARY
PRO
CESSOR
BEFORE
GOING
TO
SLEEP
AND
DISABLE
IT
WHEN
THE
PC
HAS
WOKEN
UP
WE
WERE
UNABLE
TO
FULLY
IMPLEMENT
THIS
FUNCTIONALITY
IN
OUR
PROTOTYPE
BUT
WE
EXPECT
THIS
TO
BE
A
MINOR
FIX
IN
A
PRODUCTION
SYSTEM
DISCUSSION
SECURITY
A
COMMON
REQUIREMENT
OF
CORPORATE
IT
DE
PARTMENTS
IS
THAT
ALL
PCS
SHOULD
BE
UP
TO
DATE
WITH
THE
LATEST
OS
AND
APPLICATION
PATCHES
SOMNILOQUY
CAN
EN
SURE
THAT
THIS
CONSTRAINT
IS
MET
EVEN
WHEN
PCS
ARE
ASLEEP
THIS
IS
ACHIEVED
USING
A
PORT
BASED
TRIGGER
TO
WAKE
UP
THE
HOST
PC
WHEN
THE
SMS
SYSTEMS
MANAGEMENT
SERVER
CONTACTS
THE
HOST
PC
TO
INSTALL
UPDATES
SOMNILOQUY
ENSURES
THAT
THE
SECONDARY
PROCESSOR
IS
SECURE
BY
PATCHING
ITS
OS
WHENEVER
SECURITY
UPDATES
BECOME
AVAILABLE
ALSO
IT
PREVENTS
ATTACKERS
FROM
RE
PLACING
THE
SECONDARY
PROCESSOR
BY
REQUIRING
THAT
IT
BE
A
PHYSICALLY
PART
OF
THE
PC
AS
PART
OF
THE
NETWORK
IN
TERFACE
IN
SOME
CASES
HOWEVER
THE
FUNCTIONALITY
THAT
SOMNILOQUY
PROVIDES
COULD
BE
MISUSED
TO
CONDUCT
AT
TACKS
THAT
SPURIOUSLY
WAKE
UP
THE
PC
AND
WASTE
ENERGY
THIS
KIND
OF
DENIAL
OF
SERVICE
ATTACK
WOULD
BE
PARTICU
LARLY
EFFECTIVE
FOR
MOBILE
DEVICES
WHERE
A
DRAINED
BAT
TERY
MIGHT
RESULT
ONE
WAY
TO
ADDRESS
THIS
ISSUE
IS
TO
DISABLE
PORT
TRIGGERS
AND
INSTEAD
EXCLUSIVELY
USE
APPLI
CATION
STUBS
WHICH
ENSURE
THAT
ONLY
AUTHENTICATED
REMOTE
HOSTS
ARE
ALLOWED
TO
TRIGGER
WAKEUP
ANOTHER
CONCERN
IS
THAT
APPLICATION
STUBS
AND
HENCE
THE
USE
OF
EXTRA
CODE
INCREASES
THE
PC
ATTACK
SURFACE
TO
MITIGATE
THE
IMPACT
OF
THIS
VULNERABILITY
WE
USE
A
FEW
TECHNIQUES
FIRST
THE
SECONDARY
PROCESSOR
ONLY
LISTENS
ON
PORTS
THAT
HAVE
BEEN
OPENED
BY
APPLICATIONS
ON
THE
HOST
PC
SECOND
WE
REQUIRE
THE
PC
AND
THE
SECONDARY
PROCESSOR
TO
BE
ON
THE
SAME
ADMINISTRATIVE
DOMAIN
WE
ALSO
NOTE
THAT
MODERN
PROCESSORS
HAVE
ADDITIONAL
SECURITY
FEATURES
BUILT
IN
FOR
EXAMPLE
AN
EXECUTE
DISABLE
BIT
USED
BY
SOME
APPLICATIONS
TO
PREVENT
EXECUTING
AR
BITRARY
CODE
AND
PREVENTING
BUFFER
OVERFLOWS
WE
REALIZE
THAT
A
LOW
POWER
PROCESSOR
MAY
NOT
CURRENTLY
SUPPORT
THIS
ADVANCED
FUNCTIONALITY
ALTHOUGH
WE
EXPECT
THAT
IN
THE
FUTURE
LOW
POWER
CHIPS
WILL
ALSO
BE
AVAILABLE
WITH
THESE
FEATURES
ALTERNATIVE
DESIGN
WITH
THE
INCREASING
PREVALENCE
OF
MULTI
CORE
PCS
ONE
IDEA
TO
ALLEVIATE
THE
NEED
FOR
THE
ADDITIONAL
SECONDARY
PROCESSOR
INTRODUCED
BY
SOMNILO
QUY
WOULD
BE
TO
USE
ONE
OF
THE
CORES
OF
THE
HOST
CPU
IN
STEAD
RUNNING
JUST
ONE
CORE
AT
THE
LOWEST
POSSIBLE
CLOCK
FREQUENCY
WOULD
MINIMIZE
ENERGY
CONSUMPTION
AND
OB
VIATE
THE
NEED
FOR
A
SEPARATE
LOW
POWER
PROCESSOR
IN
THE
NIC
HOWEVER
IT
TURNS
OUT
THAT
SUCH
AN
APPROACH
IS
NOT
USE
FUL
WITHOUT
SIGNIFICANT
MODIFICATION
TO
TODAY
PC
ARCHI
TECTURE
OUR
MEASUREMENTS
SEE
SECTION
SHOW
THAT
THE
POWER
CONSUMPTION
OF
A
MULTI
CORE
PC
WITH
ONLY
ONE
CORE
ACTIVE
RUNNING
AT
THE
LOWEST
PERMISSIBLE
CLOCK
SPEED
IS
STILL
APPROXIMATELY
TIMES
THAT
OF
OUR
LOW
POWER
SEC
ONDARY
PROCESSOR
EVEN
WITH
ALL
OTHER
PERIPHERALS
IN
THEIR
LOWEST
POWER
MODES
E
G
DISK
SPUN
DOWN
THIS
IS
BE
CAUSE
OF
THE
LACK
OF
TRULY
FINE
GRAINED
POWER
CONTROL
OF
PC
COMPONENTS
SUCH
AS
THE
NORTHBRIDGE
SOUTHBRIDGE
MEMORY
BUSES
PARTS
OF
THE
STORAGE
HIERARCHY
AND
VARIOUS
PERIPHERALS
EVEN
IF
FINE
GRAINED
CONTROL
WERE
AVAILABLE
THE
BASE
POWER
CONSUMPTION
OF
INDIVIDUAL
COMPONENTS
NIC
HARD
DRIVE
IS
SIGNIFICANT
SEE
TABLE
ONE
WAY
TO
REDUCE
THIS
BASE
POWER
DRAW
WOULD
BE
TO
HAVE
A
SEP
ARATE
AND
RELATIVELY
SIMPLE
CORE
WITH
A
SMALL
AMOUNT
OF
ASSOCIATED
MEMORY
RUNNING
FROM
A
SEPARATE
POWER
DO
MAIN
SO
THAT
IT
CAN
FUNCTION
WITHOUT
POWERING
ON
OTHER
COMPONENTS
SUCH
AN
ARCHITECTURE
IS
VERY
SIMILAR
TO
SOM
NILOQUY
AND
MOST
OF
OUR
DESIGN
PRINCIPLES
CAN
EASILY
BE
ADOPTED
PROTOTYPE
IMPLEMENTATION
WE
HAVE
PROTOTYPED
SOMNILOQUY
USING
GUMSTIX
A
LOW
POWER
MODULAR
EMBEDDED
PROCESSOR
PLATFORM
MANUFAC
TURED
BY
GUMSTIX
INC
THAT
SUPPORT
A
WIDE
VARIETY
OF
PE
RIPHERALS
HARDWARE
AND
SOFTWARE
OVERVIEW
AN
IMPORTANT
GOAL
WHEN
PROTOTYPING
SOMNILOQUY
WAS
TO
HAVE
IT
WORK
WITH
EXISTING
UNMODIFIED
DESKTOPS
AND
LAP
TOPS
AND
FOR
BOTH
WIRED
AND
WIRELESS
NETWORKS
FURTHER
MORE
WE
REQUIRED
THE
PLATFORM
TO
BE
LOW
POWER
HAVE
A
SMALL
FORM
FACTOR
AND
BE
WELL
SUPPORTED
FOR
DEVELOP
MENT
THE
GUMSTIX
PLATFORM
SERVED
ALL
THESE
DESIGN
RE
QUIREMENTS
WELL
THE
SPECIFIC
COMPONENTS
WE
USE
FOR
SOMNILOQUY
INCLUDE
A
CONNEX
PROCESSOR
BOARD
AN
ETHERSTIX
NETWORK
INTERFACE
CARD
NIC
FOR
WIRED
ETH
ERNET
A
WIFISTIX
NIC
FOR
WI
FI
AND
A
THUMBSTIX
COM
BINED
USB
INTERFACE
BREAKOUT
BOARD
THE
CONNEX
EMPLOYS
A
LOW
POWER
MHZ
XSCALE
PRO
CESSOR
WITH
MB
OF
NON
VOLATILE
FLASH
AND
MB
OF
RAM
THE
ETHERSTIX
PROVIDES
A
WIRED
ETH
ERNET
INTERFACE
PLUS
AN
SD
MEMORY
SLOT
TO
WHICH
WE
HAVE
ATTACHED
A
SD
CARD
THE
THUMBSTIX
PROVIDES
A
USB
CONNECTOR
SERIAL
CONNECTIONS
AND
GENERAL
PURPOSE
INPUT
AND
OUTPUT
GPIO
CONNECTIONS
FROM
THE
XSCALE
TO
ENABLE
SOMNILOQUY
WE
NEEDED
MECHANISMS
TO
WAKE
UP
THE
HOST
PC
AND
ALSO
TO
DETECT
ITS
STATE
AWAKE
OR
IN
TO
ACHIEVE
THIS
WE
ADDED
A
CUSTOM
DE
SIGNED
CIRCUIT
BOARD
THAT
INCORPORATES
A
SINGLE
CHIP
THE
FROM
FTDI
THE
IS
A
USB
TO
SERIAL
CONVERTER
CHIP
SUPPORTING
FUNCTIONALITY
SUCH
AS
SENDING
A
RESUME
SIGNAL
TO
THE
HOST
AND
DETECTING
THE
STATE
OF
THE
HOST
BOTH
OVER
THE
USB
BUS
THIS
BOARD
IS
ATTACHED
TO
THE
COMPUTER
VIA
A
SECOND
USB
PORT
AND
TO
THE
THUMB
STIX
MODULE
AND
THENCE
TO
THE
XSCALE
PROCESSOR
VIA
A
TWO
WIRE
SERIAL
INTERFACE
PLUS
TWO
GPIO
LINES
ONE
GPIO
LINE
IS
CONNECTED
TO
THE
RING
INDI
CATOR
INPUT
TO
WAKE
UP
THE
COMPUTER
THE
SECOND
GPIO
FIGURE
BLOCK
DIAGRAM
OF
THE
SOMNILOQUY
PROTOTYPE
SYSTEM
WIRED
VERSION
THE
FIGURE
SHOWS
VARIOUS
COMPONENTS
OF
THE
GUMSTIX
AND
THE
USB
INTERFACES
TO
THE
HOST
LAPTOP
LINE
IS
CONNECTED
TO
THE
SLEEP
OUTPUT
WHICH
CAN
BE
POLLED
BY
THE
GUMSTIX
TO
DETECT
WHETHER
THE
HOST
PC
IS
ACTIVE
OR
IN
AS
MENTIONED
ABOVE
AND
SHOWN
IN
FIGURE
THE
COM
PUTER
IS
CONNECTED
TO
THE
SECONDARY
PROCESSOR
VIA
TWO
USB
CONNECTIONS
ONE
OF
THESE
PROVIDES
POWER
AND
TWO
WAY
COMMUNICATIONS
BETWEEN
THE
TWO
PROCESSORS
IT
IS
CONFIGURED
TO
APPEAR
AS
A
POINT
TO
POINT
NETWORK
INTER
FACE
USBNET
OVER
WHICH
THE
GUMSTIX
AND
THE
HOST
COMPUTER
COMMUNICATE
USING
TCP
IP
THE
SECOND
USB
INTERFACE
PROVIDES
SLEEP
AND
WAKE
UP
SIGNALING
AND
A
SE
RIAL
PORT
FOR
DEBUGGING
PURPOSES
THE
USE
OF
TWO
USB
INTERFACES
IS
NOT
A
FUNDAMENTAL
REQUIREMENT
IT
IS
SIMPLY
FOR
EASE
OF
PROTOTYPING
SINCE
WE
USE
STANDARD
USB
PORTS
FOR
INTERFACING
WITH
THE
HOST
AND
FOR
SLEEP
SIGNALING
OUR
PROTOTYPE
WORKS
ON
ANY
RECENT
DESKTOP
OR
LAPTOP
THAT
SUPPORTS
USB
WE
RUN
AN
EMBEDDED
DISTRIBUTION
OF
LINUX
ON
THE
GUMSTIX
THAT
SUPPORTS
A
FULL
TCP
IP
STACK
DHCP
CONFIGURABLE
ROUTING
TABLES
A
CONFIGURABLE
FIREWALL
SSH
AND
SERIAL
PORT
COM
MUNICATION
THIS
PROVIDES
A
FLEXIBLE
PROTOTYPING
PLAT
FORM
FOR
SOMNILOQUY
WITH
VERY
LOW
POWER
OPERATION
WE
HAVE
IMPLEMENTED
THE
SOMNILOQUY
HOST
SOFTWARE
ON
WINDOWS
VISTA
THE
SOMNILOQUY
DAEMON
DETECTS
TRANSITION
TO
SLEEP
STATE
AND
BEFORE
THIS
IS
ALLOWED
TO
OCCUR
WE
TRANSFER
THE
NETWORK
STATE
MAC
ADDRESS
IP
ADDRESS
AND
IN
THE
CASE
OF
THE
WIRELESS
PROTOTYPE
THE
SSID
OF
THE
AP
AND
OTHER
INFORMATION
ABOUT
THE
WAKEUP
TRIGGERS
AS
DISCUSSED
IN
SECTION
FIGURE
PHOTOGRAPH
OF
THE
GUMSTIX
BASED
SOMNILOQUY
PROTOTYPE
WIRED
VERSION
THREE
DIFFERENT
PROTOTYPES
WE
HAVE
PROTOTYPED
THREE
DIFFERENT
SOMNILOQUY
DESIGNS
TO
EXPLORE
DIFFERENT
ASPECTS
OF
OPERATION
THE
FIRST
USES
THE
GUMSTIX
AS
AN
AUGMENTED
ETHERNET
INTERFACE
AS
DE
SCRIBED
IN
SECTION
HOWEVER
IN
OUR
PROTOTYPE
THIS
HAS
SOME
PERFORMANCE
LIMITATIONS
SO
WE
HAVE
ALSO
IMPLE
MENTED
A
SECOND
DESIGN
WHICH
USES
THE
GUMSTIX
IN
CO
OPERATION
WITH
AN
EXISTING
HIGH
SPEED
ETHERNET
INTERFACE
FINALLY
WE
HAVE
A
WI
FI
VERSION
ALL
THREE
PROTOTYPES
ARE
DESCRIBED
IN
FURTHER
DETAIL
BELOW
AUGMENTED
NETWORK
INTERFACE
WE
CALL
THIS
IMPLE
MENTATION
THE
WIRED
VERSION
THE
ARCHITECTURE
IS
SHOWN
IN
FIGURE
WITH
A
PHOTOGRAPH
OF
THE
PROTOTYPE
SHOWN
IN
FIGURE
IN
THIS
PROTOTYPE
WE
DISABLE
THE
NIC
OF
THE
HOST
AND
CONFIGURE
THE
PC
TO
USE
THE
USBNET
IN
TERFACE
USB
CONNECTION
BETWEEN
THE
GUMSTIX
AND
THE
HOST
AS
ITS
ONLY
NIC
THE
GUMSTIX
IS
CONNECTED
TO
THE
NETWORK
USING
ITS
ETHERNET
CONNECTION
TO
ENABLE
THE
HOST
PC
TO
BE
ON
THE
NETWORK
WE
SET
UP
A
TRANSPARENT
LAYER
SOFTWARE
BRIDGE
BETWEEN
THE
USBNET
INTERFACE
TO
THE
HOST
AND
THE
ETHERNET
INTERFACE
OF
THE
GUMSTIX
THIS
BRIDGE
IS
ACTIVE
WHEN
THE
HOST
IS
AWAKE
WHEN
THE
HOST
TRANSITIONS
TO
SLEEP
THE
GUMSTIX
DISABLES
THE
BRIDGE
AND
RESETS
THE
MAC
ADDRESS
OF
ITS
ETHERNET
INTERFACE
TO
THAT
OF
THE
US
BNET
INTERFACE
OF
THE
HOST
THE
GUMSTIX
THUS
APPEARS
TO
THE
REST
OF
THE
NETWORK
AS
THE
HOST
ITSELF
SINCE
IT
HAS
THE
SAME
NETWORK
PARAMETERS
IP
MAC
ADDRESS
WHEN
THE
HOST
WAKES
UP
THE
GUMSTIX
RESETS
ITS
MAC
ADDRESS
TO
ITS
ORIGINAL
VALUE
AND
STARTS
BRIDGING
TRAFFIC
TO
THE
HOST
AGAIN
ALTHOUGH
OUR
WIRED
PROTOTYPE
HARDWARE
SUP
PORTS
A
MBPS
ETHERNET
INTERFACE
WE
ARE
LIMITED
TO
A
THROUGHPUT
OF
MBPS
DUE
TO
THE
BANDWIDTH
SUPPORTED
BY
THE
USBNET
INTERFACE
DRIVER
THERE
IS
ALSO
A
SLIGHT
OVER
HEAD
OF
BRIDGING
TRAFFIC
ON
THE
GUMSTIX
ALTHOUGH
THIS
LIMITS
BANDWIDTH
TO
THE
HOST
SIGNIFICANTLY
IN
OUR
PROTO
TYPE
WE
NOTE
THAT
IN
A
FINAL
INTEGRATED
VERSION
THIS
OVER
HEAD
OF
BRIDGING
CAN
BE
AVOIDED
BY
ALLOWING
BOTH
THE
HOST
AND
THE
LOW
POWER
SECONDARY
PROCESSOR
TO
ACCESS
THE
NIC
DIRECTLY
USING
EXISTING
NETWORK
INTERFACE
SOMNILOQUY
CAN
COEXIST
WITH
AN
EXISTING
NIC
ON
SUCH
SYSTEMS
THE
OVER
HEAD
OF
BRIDGING
IS
AVOIDED
BY
USING
THE
EXISTING
ETHER
NET
INTERFACE
ON
THE
HOST
PC
FOR
DATA
TRANSFER
WHEN
IT
IS
AWAKE
WITH
THE
GUMSTIX
USING
ITS
OWN
ETHERNET
INTERFACE
WHILE
STILL
IMPERSONATING
THE
HOST
PC
WHEN
THE
HOST
IS
ASLEEP
WE
HAVE
BUILT
THIS
VERSION
WHERE
THE
GUMSTIX
DOES
NOT
PERFORM
LAYER
BRIDGING
AND
CALL
IT
THE
WIRED
PROTOTYPE
USING
WI
FI
WE
HAVE
ALSO
IMPLEMENTED
A
WIRELESS
VERSION
OF
SOMNILOQUY
WE
WERE
UNABLE
TO
IMPLEMENT
A
ONE
NIC
VERSION
SINCE
THE
MARVELL
B
G
CHIPSET
PRESENT
ON
THE
WIFISTIX
DOES
NOT
CURRENTLY
SUP
PORT
LAYER
BRIDGING
WE
HAVE
HOWEVER
IMPLEMENTED
A
WIRELESS
VERSION
APPLICATIONS
WITHOUT
STUBS
WE
HAVE
IMPLEMENTED
A
FLEXIBLE
PACKET
FILTER
ON
THE
GUM
STIX
USING
THE
BSD
RAW
SOCKET
INTERFACE
TO
SUPPORT
APPLI
CATIONS
THAT
DO
NOT
REQUIRE
STUBS
E
G
RDP
SSH
TELNET
AND
SMB
CONNECTIONS
EVERY
APPLICATION
IN
THIS
CLASS
PROVIDES
A
REGULAR
EXPRESSION
MATCHED
AGAINST
INCOMING
PACKETS
TO
DECIDE
WHETHER
TO
TRIGGER
HOST
WAKEUP
FOR
EXAMPLE
HANDLING
INCOMING
REMOTE
DESKTOP
REQUESTS
RE
QUIRES
THE
HOST
TO
BE
WOKEN
UP
WHEN
THE
GUMSTIX
RECEIVES
A
TCP
PACKET
WITH
DESTINATION
PORT
WE
NOTE
THAT
WAKING
UP
THE
HOST
COMPUTER
IS
NOT
ENOUGH
THE
INCOMING
CONNECTION
REQUEST
MUST
SOMEHOW
BE
CONVEYED
TO
THE
HOST
WE
ACCOMPLISH
THIS
BY
USING
THE
IPTABLES
FIREWALL
ON
THE
GUMSTIX
TO
FILTER
ANY
RE
SPONSE
TO
TCP
OR
UDP
PACKETS
THAT
THE
GUMSTIX
DOES
NOT
HANDLE
ITSELF
THUS
TRIGGER
PACKETS
ARE
NOT
ACKNOWLEDGED
BY
THE
GUMSTIX
AND
THE
REMOTE
CLIENT
SENDS
RETRIES
AF
TER
THE
HOST
HAS
RESUMED
ONE
OF
THE
RETRIES
WILL
REACH
IT
SINCE
IT
IS
STILL
USING
THE
SAME
IP
AND
MAC
ADDRESSES
AND
IT
WILL
RESPOND
DIRECTLY
USING
PORT
BASED
FILTERING
WE
HAVE
IMPLEMENTED
WAKE
UP
TRIGGERS
FOR
FOUR
APPLI
CATIONS
REMOTE
DESKTOP
REQUESTS
RDP
REMOTE
SECURE
SHELL
SSH
FILE
ACCESS
REQUESTS
SMB
AND
VOICE
OVER
IP
CALLS
SIP
VOIP
APPLICATIONS
USING
STUBS
TO
DEMONSTRATE
HOW
MODEST
APPLICATION
STUBS
CAN
ENABLE
SIGNIFICANT
SLEEP
MODE
OPERATION
IN
SOMNILOQUY
WE
HAVE
ALSO
IMPLEMENTED
APPLICATION
STUBS
FOR
THREE
APPLICATIONS
THAT
WERE
POPULAR
IN
OUR
INFORMAL
SURVEY
BACKGROUND
WEB
DOWNLOAD
PEER
TO
PEER
CONTENT
DISTRIBUTION
USING
BITTORRENT
AND
INSTANT
MESSAGING
FOR
ALL
THESE
APPLI
CATIONS
WE
DID
NOT
HAVE
TO
MODIFY
THE
OPERATING
SYSTEM
OR
THE
EXISTING
APPLICATIONS
ON
THE
PC
WHICH
WERE
ONLY
AVAILABLE
TO
US
IN
BINARIES
TO
CAPTURE
THE
STATE
OF
THE
APPLICATION
FOR
THE
RESPECTIVE
STUB
WE
WROTE
WRAPPERS
AROUND
THE
BINARIES
BACKGROUND
WEB
DOWNLOADS
WE
DEVELOPED
THE
WEB
DOWNLOAD
STUB
FOR
WGET
WHICH
WORKS
AS
FOLLOWS
WHEN
THE
HOST
PC
TRANSITIONS
TO
SLEEP
THE
STATUS
OF
AC
TIVE
DOWNLOADS
IS
SENT
TO
THE
STUB
RUNNING
ON
THE
GUM
STIX
THE
STATUS
INCLUDES
THE
DOWNLOAD
URL
THE
OFFSET
OF
HOW
MUCH
DOWNLOAD
HAS
TAKEN
PLACE
THE
BUFFER
SPACE
AVAILABLE
AND
THE
CREDENTIALS
IF
REQUIRED
FOR
THE
DOWN
LOAD
MOST
POPULAR
WEB
SERVERS
E
G
IIS
AND
APACHE
ALLOW
THESE
BYTE
RANGES
TO
BE
SPECIFIED
USING
THE
HTTP
ACCEPT
RANGES
PRIMITIVES
THE
WEB
DOWNLOAD
STUB
THEN
RESUMES
THE
DOWNLOADS
FROM
THE
RESPECTIVE
OFF
SETS
OF
THE
FILES
AND
STORES
THE
DATA
ON
THE
FLASH
STORAGE
OF
THE
GUMSTIX
IF
THE
FLASH
MEMORY
FILLS
UP
BEFORE
THE
DOWNLOADS
COMPLETE
THE
STUB
WAKES
UP
THE
HOST
PC
AND
TRANSFERS
THE
DOWNLOADED
FILES
FROM
FLASH
STORAGE
TO
THE
HOST
PC
THEREBY
FREEING
UP
SPACE
THE
HOST
PC
THEN
GOES
BACK
TO
SLEEP
WHILE
THE
STUB
CONTINUES
THE
DOWNLOADS
AT
THE
END
OF
A
DOWNLOAD
THE
GUMSTIX
WAKES
UP
THE
HOST
PC
AND
TRANSFERS
THE
REMAINING
PART
OF
THE
FILE
THE
DOWNLOAD
STUB
CONSUMES
SIGNIFICANTLY
LESS
ENERGY
TO
DOWNLOAD
A
FILE
THAN
KEEPING
THE
PC
AWAKE
TO
DOWN
LOAD
IT
THE
OVERHEAD
IS
A
SLIGHT
INCREASE
IN
LATENCY
WE
CAN
QUANTIFY
THE
SAVINGS
AND
OVERHEAD
USING
THE
MODEL
DESCRIBED
IN
SECTION
IF
FLASH
STORAGE
IS
F
MB
AND
THE
DOWNLOAD
BANDWIDTH
IS
B
MBPS
THEN
THE
HOST
PC
IS
WOKEN
UP
EVERY
F
B
SECONDS
AND
IT
IS
AWAKE
FOR
F
T
SECONDS
WHERE
T
IS
THE
TRANSFER
RATE
BETWEEN
THE
HOST
AND
THE
GUMSTIX
THEREFORE
USING
THE
FORMULA
IN
SEC
TION
SOMNILOQUY
GIVES
MOST
ENERGY
SAVINGS
AT
LOW
B
AND
HIGH
T
WE
EMPIRICALLY
VALIDATE
THIS
OBSERVATION
IN
SECTION
WHEN
T
IS
OF
THE
SAME
ORDER
AS
B
SOMNILOQUY
MIGHT
NOT
SAVE
MUCH
ENERGY
THIS
CAN
HAP
PEN
IF
THE
NIC
SUPPORTS
VERY
HIGH
RATES
E
G
GBPS
WHILE
THE
SECONDARY
PROCESSOR
CAN
ONLY
SUPPORT
LOWER
DATA
RATES
UP
TO
MBPS
OR
IF
THE
TRANSFER
RATE
T
IS
LIMITED
HOWEVER
WE
ANTICIPATE
THE
DOWNLOAD
STUB
TO
BE
PRIMARILY
USED
IN
SCENARIOS
WHERE
THE
DOWNLOAD
SPEEDS
ARE
LIMITED
BY
THE
LAST
MILE
CONNECTION
OF
AT
MOST
A
FEW
TENS
OF
MBPS
HERE
THIS
STUB
IS
NEARLY
ALWAYS
BENEFICIAL
BITTORRENT
FOR
THE
BITTORRENT
STUB
WE
CUSTOMIZED
A
CONSOLE
BASED
CLIENT
CTORRENT
TO
RUN
ON
THE
GUMSTIX
WITH
A
LOW
CPU
UTILIZATION
AND
MEMORY
FOOTPRINT
PRIOR
TO
SUSPENDING
TO
THE
HOST
COMPUTER
TRANSFERS
THE
TOR
RENT
FILE
AND
THE
PORTION
OF
THE
FILE
THAT
HAS
ALREADY
BEEN
DOWNLOADED
TO
THE
GUMSTIX
THE
BITTORRENT
STUB
ON
THE
GUMSTIX
THEN
RESUMES
DOWNLOAD
OF
THE
TORRENT
FILE
AND
STORES
IT
TEMPORARILY
ON
THE
SD
FLASH
MEMORY
OF
THE
GUM
STIX
WHEN
THE
DOWNLOAD
COMPLETES
THE
STUB
WAKES
UP
THE
HOST
AND
TRANSFERS
THE
FILE
WHEN
ONLY
DOWNLOADING
CONTENT
THE
ENERGY
SAVED
BY
USING
THIS
STUB
IS
SIMILAR
TO
THAT
OF
THE
WEB
DOWNLOAD
STUB
I
E
FREQUENCY
OF
WAKING
UP
THE
PC
AND
THE
DURATION
FOR
WHICH
IT
IS
WOKEN
UP
DEPENDS
ON
THE
DOWNLOAD
BAND
WIDTH
B
THE
TRANSFER
SPEED
T
AND
THE
FLASH
SIZE
F
HOW
EVER
WHEN
UPLOADING
SHARING
WHICH
IS
KEY
TO
ALTRUIS
TIC
APPLICATIONS
THE
ENERGY
SAVINGS
ARE
MUCH
MORE
THE
SAME
FILE
CHUNK
CAN
BE
UPLOADED
TO
MANY
PEERS
AND
HENCE
THE
PC
CAN
SLEEP
FOR
MUCH
LONGER
IMPLYING
MORE
ENERGY
SAVINGS
USING
THE
FORMULA
IN
SECTION
INSTANT
MESSAGING
FOR
THE
IM
STUB
WE
USED
A
CONSOLE
ONLY
IM
CLIENT
CALLED
FINCH
THAT
SUPPORTS
MANY
IM
PROTOCOLS
SUCH
AS
MSN
AOL
ICQ
ETC
ON
THE
PC
WE
USED
THE
CORRESPONDING
GUI
VERSION
OF
THE
IM
CLIENT
TO
ENSURE
OUR
GOAL
OF
A
LOW
MEMORY
AND
CPU
FOOTPRINT
WE
CUSTOMIZED
FINCH
TO
INCLUDE
ONLY
THE
FEATURES
SALIENT
TO
OUR
AIM
OF
WAKING
UP
THE
HOST
PROCESSOR
WHEN
AN
IN
COMING
CHAT
MESSAGE
ARRIVES
THIS
ONLY
REQUIRES
AUTHEN
TICATION
PRESENCE
UPDATES
AND
NOTIFICATIONS
WE
DISABLED
OTHER
FUNCTIONALITY
THE
HOST
PROCESSOR
TRANSFERS
OVER
THE
AUTHENTICATION
CREDENTIALS
FOR
RELEVANT
IM
ACCOUNTS
BE
FORE
GOING
TO
THE
GUMSTIX
THEN
LOGS
INTO
THE
RELE
VANT
IM
SERVERS
AND
WHEN
AN
INCOMING
MESSAGE
ARRIVES
IT
TRIGGERS
WAKEUP
THE
ENERGY
SAVED
BY
THE
IM
STUB
IS
THUS
SIMILAR
TO
APPLICATIONS
THAT
ARE
HANDLED
USING
PACKET
FILTERS
E
G
SSH
RDP
WHERE
THE
DURATION
FOR
WHICH
A
HOST
CAN
SLEEP
DEPENDS
ON
THE
FREQUENCY
OF
OCCURRENCE
OF
WAKE
UP
TRIGGERS
SYSTEM
EVALUATION
WE
PRESENT
THE
BENEFITS
OF
SOMNILOQUY
IN
FOUR
STEPS
FIRST
WE
SHOW
THAT
GUMSTIX
CONSUMES
MUCH
LESS
POWER
THAN
A
PC
BY
PROFILING
STANDALONE
DESKTOPS
LAPTOPS
AND
THE
GUMSTIX
IN
DIFFERENT
POWER
STATES
SECOND
WE
MEA
SURE
THE
ENERGY
SAVED
AND
LATENCY
INTRODUCED
BY
SOM
NILOQUY
WHEN
USED
ON
AN
IDLE
HOST
PROCESSOR
THIRD
WE
SHOW
HOW
SOMNILOQUY
AFFECTS
THE
PERFORMANCE
OF
VARI
OUS
APPLICATIONS
WITH
AND
WITHOUT
APPLICATION
STUBS
FI
NALLY
WE
QUANTIFY
SOMNILOQUY
ENERGY
SAVINGS
MON
ETARY
AND
ENVIRONMENTAL
COST
FOR
AN
ENTERPRISE
AND
BAT
TERY
LIFETIME
INCREASE
FOR
LAPTOPS
METHODOLOGY
TO
MEASURE
THE
POWER
CONSUMPTION
OF
LAPTOPS
AND
DESKTOP
PCS
WE
USED
A
COMMERCIALLY
AVAIL
ABLE
MAINS
POWER
METER
WATTS
UP
TO
MEASURE
THE
POWER
CONSUMPTION
OF
THE
STANDALONE
GUMSTIX
WE
BUILT
A
USB
EXTENSION
CABLE
WITH
A
MΩ
SENSE
RESIS
TOR
WHICH
WAS
INSERTED
IN
SERIES
WITH
THE
V
SUPPLY
LINE
AND
WE
USED
THIS
CABLE
TO
CONNECT
THE
GUMSTIX
TO
THE
COMPUTER
WE
CALCULATED
THE
POWER
DRAW
OF
THE
GUMSTIX
BY
MEASURING
THE
VOLTAGE
DROP
ACROSS
THE
SENSE
RESISTOR
ALL
POWER
NUMBERS
PRESENTED
IN
THIS
SECTION
ARE
AVERAGED
ACROSS
AT
LEAST
FIVE
RUNS
TABLE
POWER
CONSUMPTION
AND
SUSPEND
RESUME
TIME
FOR
TWO
DESKTOPS
UNDER
VARIOUS
OPERATING
CONDI
TIONS
IN
ALL
CASES
THE
PROCESSOR
IS
IDLE
AND
THE
HARD
DISK
IS
SPUN
DOWN
THE
POWER
CONSUMED
BY
OTHER
PERIPHERALS
SUCH
AS
DISPLAYS
IS
NOT
INCLUDED
TABLE
POWER
CONSUMPTION
AND
BATTERY
LIFETIME
OF
THREE
LAPTOPS
UNDER
VARIOUS
OPERATING
CONDITIONS
AND
THE
TIME
TO
CHANGE
POWER
STATES
MICROBENCHMARKS
POWER
LATENCY
DESKTOPS
TABLE
PRESENTS
THE
AVERAGE
POWER
CONSUMP
TION
FOR
TWO
DELL
DESKTOP
MACHINES
AN
INTEL
DUAL
CORE
GHZ
OPTIPLEX
WITH
GB
RAM
RUN
NING
WINDOWS
VISTA
AND
A
GHZ
PENTIUM
DIMEN
SION
WITH
MB
RAM
RUNNING
WINDOWS
XP
THE
DISPLAY
IS
TURNED
OFF
IN
THESE
EXPERIMENTS
AND
ONLY
THE
ESSENTIAL
SYSTEM
PROCESSES
ARE
LEFT
RUNNING
THE
POWER
CONSUMPTION
OF
THE
DESKTOP
IN
IS
TWO
ORDERS
OF
MAG
NITUDE
LESS
THAN
WHEN
IT
IS
AWAKE
THIS
IS
CONSISTENT
WITH
PRIOR
PUBLISHED
DATA
ON
THE
POWER
CONSUMPTION
OF
MOD
ERN
PCS
WE
USE
THE
TERM
BASE
POWER
TO
INDICATE
THE
LOWEST
POWER
MODE
THAT
A
PC
CAN
BE
IN
AND
STILL
BE
RE
SPONSIVE
TO
NETWORK
TRAFFIC
WITHOUT
USING
SOMNILOQUY
TO
GET
THIS
NUMBER
WE
FURTHER
SCALED
DOWN
THE
CPU
TO
THE
LOWEST
PERMISSIBLE
FREQUENCY
ON
THESE
DESKTOPS
FUR
THERMORE
WE
DISABLED
THE
MULTI
CORE
FUNCTIONALITY
USING
THE
SYSTEM
BIOS
TO
EFFECTIVELY
USE
ONLY
ONE
CORE
AND
VERIFIED
THAT
THE
SYSTEM
WAS
ACTUALLY
DOING
SO
BY
USING
A
PROCESSOR
ID
UTILITY
SUPPLIED
BY
INTEL
THE
TIME
TAKEN
FOR
THE
DESKTOPS
TO
RESUME
FROM
AND
RECONNECT
TO
THE
NETWORK
IS
OF
THE
ORDER
OF
A
FEW
SECONDS
TABLE
TABLE
POWER
CONSUMPTION
FOR
THE
GUMSTIX
PLATFORM
IN
VARIOUS
STATES
OF
OPERATION
LAPTOPS
TABLE
PRESENTS
THE
AVERAGE
POWER
CON
SUMPTION
OF
THREE
POPULAR
LAPTOPS
A
LENOVO
TABLET
PC
WITH
GB
RAM
RUNNING
WINDOWS
VISTA
A
TOSHIBA
LAPTOP
WITH
GB
RAM
RUNNING
WINDOWS
XP
AND
A
LENOVO
LAPTOP
WITH
GB
RAM
RUNNING
WINDOWS
VISTA
FOR
ALL
POWER
MEASUREMENTS
THE
PROCESSOR
IS
SET
TO
THE
LOWEST
SPEED
AND
IS
IDLE
THE
HARD
DISK
IS
SPUN
DOWN
AND
THE
WIRELESS
NETWORK
INTERFACE
IS
POWERED
ON
THE
BASE
POWER
IS
BETWEEN
W
AND
W
RESULTING
IN
A
BAT
TERY
LIFETIME
OF
AROUND
TO
HOURS
WITH
THE
BATTERIES
THAT
ARE
PRESENT
ON
THESE
LAPTOPS
USING
THE
SLEEP
STATE
CAN
DRAMATICALLY
EXTEND
THE
BATTERY
LIFETIME
TO
BETWEEN
AND
HOURS
FOR
THE
LAPTOPS
WE
TESTED
ALTHOUGH
THE
LAPTOP
IS
UNREACHABLE
IN
THIS
STATE
GUMSTIX
TABLE
SHOWS
THE
AVERAGE
POWER
CON
SUMED
BY
THE
GUMSTIX
WITH
BOTH
ETHERSTIX
AND
WIFISTIX
IN
VARIOUS
STATES
OF
OPERATION
THE
GUMSTIX
HAS
A
BASE
POWER
OF
APPROXIMATELY
MW
WHEN
NO
NETWORK
IN
TERFACE
IS
PRESENT
ROW
A
GUMSTIX
WITH
AN
ACTIVE
NET
WORK
INTERFACE
TYPICALLY
CONSUMES
APPROXIMATELY
MW
ROWS
AND
HOWEVER
WITH
AN
ASSOCIATED
WI
FI
INTERFACE
IN
POWER
SAVE
MODE
IT
CONSUMES
ONLY
MW
ROW
THE
POWER
CONSUMPTION
OF
THE
GUMSTIX
WHEN
ITS
NETWORK
INTERFACE
IS
ACTIVE
AND
THE
DOWNLOADED
DATA
IS
BEING
WRITTEN
TO
FLASH
IS
AROUND
MW
ROW
BROADCAST
AND
UNICAST
STORMS
CONTINUOUS
TRAFFIC
INCREASE
THE
POWER
CONSUMPTION
BY
A
FEW
HUNDRED
MILLI
IMPORTANTLY
THE
POWER
CONSUMPTION
OF
THE
GUM
STIX
IS
APPROXIMATELY
ONE
TENTH
THAT
OF
AN
AWAKE
LAPTOP
IN
THE
LOWEST
POWER
STATE
AND
APPROXIMATELY
TIMES
LESS
THAN
AN
IDLE
DESKTOP
FI
BROADCASTS
ARE
SENT
AT
MBPS
WHILE
UNICASTS
ARE
SENT
AT
MBPS
IN
OUR
SETUP
CONSEQUENTLY
A
UNICAST
STORM
CONSUMES
MORE
POWER
THAN
A
BROADCAST
STORM
FIGURE
POWER
CONSUMPTION
AND
STATE
TRANSITIONS
FOR
OUR
DESKTOP
TESTBED
SOMNILOQUY
IN
OPERATION
WE
NOW
REPORT
THE
POWER
CONSUMPTION
OF
SOMNILOQUY
IN
OPERATION
FOR
THESE
MEASUREMENTS
WE
USE
TWO
TESTBED
SYSTEMS
A
DESKTOP
DELL
OPTIPLEX
WITH
GB
RAM
RUNNING
WINDOWS
VISTA
WITH
THE
WIRED
PROTOTYPE
OF
SOMNILOQUY
AND
A
LAPTOP
LENOVO
TABLET
PC
RUN
NING
WINDOWS
VISTA
WITH
THE
WIRELESS
VERSION
OF
SOMNILOQUY
THUS
OUR
TESTS
SPAN
BOTH
ETHERNET
AND
WI
FI
NETWORKS
AND
BOTH
THE
INTEGRATED
SINGLE
NETWORK
IN
TERFACE
AND
THE
HIGHER
PERFORMANCE
VERSIONS
WHICH
USES
THE
EXISTING
INTERNAL
NETWORK
INTERFACE
THE
TEST
TRAFFIC
IS
GENERATED
USING
A
STANDARD
DESKTOP
MACHINE
RUNNING
ON
THE
SAME
WIRELESS
OR
WIRED
LAN
SUBNET
AS
THE
TESTBED
MACHINE
FIGURE
SHOWS
THE
POWER
CONSUMPTION
OF
OUR
DESKTOP
TESTBED
INITIALLY
THE
DESKTOP
HOST
PROCESSOR
IS
AWAKE
AND
USES
THE
GUMSTIX
FOR
BRIDGING
AND
THE
WHOLE
SYS
TEM
DRAWS
W
OF
POWER
AT
TIME
A
A
STATE
CHANGE
TO
IS
INITIATED
BY
THE
USER
THIS
REQUEST
COMPLETES
AT
TIME
B
AFTER
WHICH
THE
POWER
DRAW
OF
THE
SYSTEM
IS
APPROXIMATELY
W
I
E
LESS
THIS
POWER
IS
SPLIT
BETWEEN
THE
GUMSTIX
THE
DRAM
OF
THE
PC
AND
OTHER
POWER
CHAIN
ELEMENTS
IN
THE
PC
SUBSEQUENTLY
AT
TIME
C
THE
GUMSTIX
WHICH
HAS
BEEN
ACTIVELY
MONITORING
THE
NETWORK
INTERFACE
WAKES
UP
THE
HOST
IN
RESPONSE
TO
A
NET
WORK
EVENT
THIS
REQUEST
COMPLETES
AT
TIME
D
WHEN
THE
HOST
SYSTEM
HAS
FULLY
RESUMED
AS
THE
FIGURES
ILLUSTRATE
THIS
RESUME
EVENT
TAKES
ABOUT
SECONDS
WE
DO
NOT
SHOW
THE
LAPTOP
FIGURE
FOR
SPACE
REASONS
THE
TRACE
LOOKS
VERY
SIMILAR
WITH
A
STARTING
POWER
OF
W
WITH
THE
SCREEN
ON
WHICH
DROPS
TO
W
IF
THE
SCREEN
IS
TURNED
OFF
A
POWER
DRAW
OF
W
WHEN
USING
SOMNILOQUY
LESS
THAN
THE
SCREEN
OFF
CASE
AND
A
RESUME
TIME
OF
SECONDS
APPLICATION
PERFORMANCE
AS
DESCRIBED
EARLIER
THERE
ARE
TWO
CLASSES
OF
APPLICATIONS
THAT
ARE
SUPPORTED
BY
SOMNILOQUY
FIRST
A
LARGE
CLASS
OF
APPLICATIONS
THAT
DO
NOT
REQUIRE
APPLICATION
STUBS
AND
SECOND
A
SMALLER
CLASS
OF
APPLICATIONS
THAT
CAN
BE
SUP
FIGURE
APPLICATION
LAYER
LATENCY
FOR
THREE
SOMNILOQUY
TESTBEDS
AND
FOUR
APPLICATION
TYPES
PORTED
USING
APPLICATION
STUBS
RUNNING
ON
THE
GUMSTIX
WE
PERFORMED
A
NUMBER
OF
EXPERIMENTS
TO
EVALUATE
THE
PERFORMANCE
OF
BOTH
THESE
CLASSES
OF
APPLICATIONS
APPLICATIONS
WITHOUT
STUBS
WE
NOW
QUANTIFY
THE
END
TO
END
LATENCY
AS
PERCEIVED
BY
USERS
INCURRED
BY
THE
APPLICATIONS
THAT
ARE
HANDLED
BY
SOMNILOQUY
WITHOUT
USING
APPLICATION
STUBS
FOR
THESE
EXPERIMENTS
WE
USE
THE
SAME
TWO
TESTBEDS
AS
ABOVE
WITH
THE
ADDITION
OF
A
THIRD
TESTBED
BASED
ON
THE
WIRED
PROTOTYPE
USING
SAME
DESKTOP
MACHINE
AS
THE
WIRED
CASE
PROVIDING
A
DIRECT
COMPARISON
BETWEEN
THE
AND
CASES
IN
EACH
CASE
THE
LATENCY
REPORTED
IS
THE
MEAN
OVER
FIVE
TEST
RUNS
FIGURE
REPORTS
THE
TIME
TAKEN
TO
SATISFY
AN
INCOMING
APPLICATION
LAYER
REQUEST
FOR
FOUR
SAMPLE
APPLICATIONS
FOR
EACH
APPLICATION
WE
SHOW
THE
LATENCY
FOR
AWAKE
OPERATION
I
E
WHEN
THE
HOST
IS
ON
AND
DIRECTLY
RESPONDS
TO
THE
REQUEST
AND
WHEN
THE
HOST
IS
IN
AND
SOMNILO
QUY
PROTOTYPE
RECEIVES
THE
INCOMING
PACKET
AND
TRIGGERS
WAKE
UP
OF
THE
HOST
THE
FOUR
APPLICATIONS
WE
TESTED
WERE
REMOTE
DESKTOP
ACCESS
RDP
HERE
WE
USED
A
STOP
WATCH
TO
MEASURE
THE
LATENCY
BETWEEN
INITIATING
A
REMOTE
DESKTOP
SESSION
TO
THE
HOST
AND
THE
REMOTE
DESKTOP
BE
ING
DISPLAYED
A
STOPWATCH
WAS
USED
TO
ENSURE
THAT
TRUE
USER
PERCEIVED
LATENCY
WAS
MEASURED
THE
GUMSTIX
WAS
CONFIGURED
TO
WAKEUP
THE
MAIN
PROCESSOR
ON
DETECTING
TCP
TRAFFIC
ON
PORT
THE
RDP
PORT
REMOTE
DIRECTORY
LISTING
SMB
A
DIRECTORY
LISTING
FROM
THE
SOMNILOQUY
TESTBED
WAS
REQUESTED
BY
THE
TESTER
MACHINE
VIA
WINDOWS
FILE
SHARING
WHICH
IS
BASED
ON
THE
SMB
PROTOCOL
THE
TIME
BETWEEN
THE
REQUEST
BEING
INI
TIATED
AND
THE
LISTING
BEING
RETURNED
WAS
MEASURED
USING
A
SIMPLE
SCRIPT
THE
SECONDARY
PROCESSOR
WAS
CONFIGURED
TO
INITIATE
WAKE
UP
ON
DETECTION
OF
TRAFFIC
ON
EITHER
OF
THE
TCP
PORTS
USED
BY
SMB
I
E
PORTS
AND
REMOTE
FILE
COPY
SMB
THE
SMB
PROTOCOL
WAS
USED
AGAIN
BUT
THIS
TIME
TO
TRANSFER
A
MB
FILE
FROM
THE
SOMNILOQUY
TESTBED
TO
THE
TESTER
MACHINE
VOIP
CALL
SIP
A
VOICE
OVER
IP
CALL
WAS
PLACED
TO
A
USER
WHO
HAD
BEEN
RUNNING
A
SIP
CLIENT
ON
THE
SOM
NILOQUY
LAPTOP
BEFORE
IT
HAD
ENTERED
ON
RECEIPT
OF
THE
INCOMING
CALL
THE
SIP
SERVER
RESPONDED
WITH
A
TCP
CONNECTION
TO
THE
TESTBED
CAUSING
THE
GUMSTIX
TO
TRIG
GER
WAKEUP
A
SIMILAR
PROCEDURE
WAS
USED
IN
ONCE
AGAIN
THE
LATENCIES
WERE
MEASURED
USING
A
STOPWATCH
TO
MEASURE
TRUE
USER
PERCEIVED
DELAY
AS
FIGURE
SHOWS
SOMNILOQUY
ADDS
BETWEEN
LATENCY
IN
ALL
CASES
AS
DESCRIBED
IN
SECTION
EARLIER
PART
OF
THIS
LATENCY
IS
ATTRIBUTED
TO
RESUMING
FROM
I
E
FOR
THE
DESKTOP
AND
FOR
THE
LAPTOP
AND
IS
IN
DEPENDENT
OF
SOMNILOQUY
FURTHER
LATENCY
IS
DUE
TO
THE
DELAY
FOR
TCP
TO
RETRANSMIT
THE
REQUEST
AND
FOR
THE
HOST
TO
RESPOND
TO
THE
REQUEST
WHICH
MAY
TAKE
LONGER
SINCE
IT
HAS
JUST
RESUMED
NOTE
THAT
THE
WIRED
PROTO
TYPE
SHOWS
HIGHER
LATENCY
THAN
THE
WIRED
PROTO
TYPE
THIS
IS
PURELY
AN
ARTIFACT
OF
OUR
PROTOTYPE
CAUSED
BY
THE
OVERHEAD
OF
MAC
BRIDGING
AND
LARGELY
THE
SLOWER
SPEED
OF
THE
USBNET
IP
LINK
BETWEEN
THE
GUMSTIX
AND
THE
HOST
THE
LATTER
IS
PARTICULARLY
OBVIOUS
IN
THE
FILE
COPY
TEST
WHERE
THE
FILE
COPY
TIME
WITH
THE
WIRED
CASE
IS
MUCH
FASTER
THAN
FOR
WIRED
ALTHOUGH
THE
WIRED
SPEED
IS
STILL
FASTER
THAN
WIRELESS
WHILE
SOMNILOQUY
DOES
RESULT
IN
ADDITIONAL
APPLICATION
LAYER
LATENCY
THESE
DELAYS
ARE
ACCEPTABLE
FOR
REAL
USAGE
INCLUDING
VOIP
IN
EXCHANGE
FOR
THE
SUBSTANTIAL
BEN
EFIT
OF
POWER
SAVINGS
APPLICATIONS
REQUIRING
STUBS
IN
THIS
SECTION
WE
PRESENT
EVALUATIONS
FOR
APPLICATIONS
THAT
REQUIRE
STUB
SUPPORT
ON
THE
GUMSTIX
PRIMARILY
LOOK
ING
AT
THE
OVERHEAD
IN
TERMS
OF
MEMORY
CONSUMPTION
AND
PROCESSING
CAPABILITIES
THAT
THEY
IMPOSE
ON
THE
GUM
STIX
WE
HAVE
IMPLEMENTED
APPLICATION
STUBS
FOR
THREE
COMMON
APPLICATIONS
BACKGROUND
DOWNLOADS
USING
THE
HTTP
PROTOCOL
FILE
SHARING
USING
BITTORRENT
AND
MAINTAINING
PRESENCE
ON
IM
NETWORKS
AS
DESCRIBED
IN
SECTION
TO
STUDY
THE
OVERHEAD
OF
IM
CLIENTS
WE
RUN
THE
COR
RESPONDING
APPLICATION
STUB
USING
UP
TO
THREE
DIFFERENT
IM
PROTOCOLS
SIMULTANEOUSLY
MSN
MESSENGER
AOL
MESSENGER
AND
ICQ
CHAT
TABLE
SHOWS
THE
PROCESSOR
UTILIZATION
AND
MEMORY
FOOTPRINT
OF
THE
WIRED
PRO
TOTYPE
WHEN
RUNNING
THESE
IM
CLIENTS
SINCE
THE
BEHAV
IOR
OF
THE
IM
STUB
IS
SUCH
THAT
IT
MAINTAINS
PRESENCE
OF
THE
USER
ON
VARIOUS
NETWORKS
AND
ON
RECEIPT
OF
AN
APPRO
PRIATE
TRIGGER
IM
FROM
SOMEONE
WAKES
UP
THE
HOST
THE
LATENCY
VALUES
ARE
SIMILAR
TO
THOSE
OF
THE
VOIP
APPLICATION
TABLE
PROCESSOR
AND
MEMORY
UTILIZATION
FOR
THE
IM
STUB
FOR
VARIOUS
CONFIGURATIONS
TOTAL
MEMORY
FOR
THE
GUMSTIX
IS
MB
TABLE
PROCESSOR
AND
MEMORY
UTILIZATION
FOR
THE
BIT
TORENT
STUB
FOR
VARIOUS
CONFIGURATIONS
TOTAL
MEMORY
FOR
THE
GUMSTIX
IS
MB
AS
REPORTED
IN
FIGURE
FOR
OUR
WIRED
PROTOTYPE
THE
ADDITIONAL
LATENCY
FOR
THE
IM
STUB
WHEN
USING
SOM
NILOQUY
IS
AROUND
SEVEN
SECONDS
TO
EVALUATE
THE
OVERHEAD
OF
FILE
SHARING
USING
THE
BITTORRENT
STUB
ON
THE
GUMSTIX
WE
INITIATED
DOWNLOADS
USING
A
TORRENT
FROM
A
REMOTE
INTO
THE
GB
SD
CARD
OF
THE
WIRED
GUMSTIX
WE
VARIED
THE
MEM
ORY
CACHE
AVAILABLE
TO
THE
STUB
WHILE
CONDUCTING
A
SINGLE
DOWNLOAD
AND
THEN
TESTED
TWO
SIMULTANEOUS
DOWNLOADS
THE
RESULTS
IN
TABLE
SHOW
THAT
THE
MEMORY
FOOTPRINT
OF
THE
STUB
INCREASES
PROPORTIONALLY
TO
THE
CACHE
SIZE
AS
EX
PECTED
WHILE
THE
PROCESSOR
UTILIZATION
REMAINS
CONSTANT
WHEN
THERE
ARE
TWO
SIMULTANEOUS
DOWNLOADS
EACH
IN
STANCE
OF
THE
STUB
USES
MEMORY
PROPORTIONAL
TO
ITS
SPECI
FIED
MB
CACHE
FINALLY
TO
EVALUATE
THE
WEB
DOWNLOAD
STUB
ON
THE
GUMSTIX
WE
INITIATE
DOWNLOAD
OF
A
LARGE
MB
FILE
FROM
A
LOCAL
WEB
SERVER
WE
VARIED
THE
THROUGHPUT
OF
THE
DOWNLOADS
AND
MEASURED
THE
PROCESSOR
UTILIZATION
AND
THE
MEMORY
CONSUMPTION
OF
THE
GUMSTIX
AND
EXPER
IMENTED
WITH
TWO
SIMULTANEOUS
DOWNLOADS
AS
SHOWN
IN
TABLE
THE
PROCESSOR
UTILIZATION
INCREASES
AS
THE
DOWN
LOAD
RATE
INCREASES
ALTHOUGH
THE
MEMORY
FOOTPRINT
FOR
EACH
DOWNLOAD
REMAINS
CONSTANT
THE
ABOVE
RESULTS
SHOW
THAT
USING
APPLICATION
STUBS
WE
CAN
SUPPORT
FAIRLY
COMPLEX
TASKS
AND
APPLICATIONS
IN
CLUDING
BACKGROUND
WEB
DOWNLOADS
AND
FILE
SHAR
TABLE
PROCESSOR
AND
MEMORY
UTILIZATION
FOR
THE
WEB
DOWNLOAD
STUB
FOR
VARIOUS
CONFIGURATIONS
TOTAL
MEMORY
FOR
THE
GUMSTIX
IS
MB
ING
USING
RELATIVELY
MODEST
RESOURCES
ON
THE
GUMSTIX
IT
IS
IMPORTANT
TO
NOTE
THAT
THE
POWER
CONSUMPTION
OF
THE
GUMSTIX
DID
NOT
EXCEED
W
IN
ALL
OF
THESE
EXPERIMENTS
ENERGY
SAVINGS
USING
SOMNILOQUY
IN
ADDITION
TO
EVALUATING
THE
OPERATING
PERFORMANCE
OF
OUR
SOMNILOQUY
PROTOTYPES
IT
ALSO
IMPORTANT
TO
ASSESS
THE
HIGHER
LEVEL
GOAL
OF
THIS
WORK
NAMELY
THE
IMPACT
ON
PC
ENERGY
CONSUMPTION
IN
THIS
SECTION
WE
PRESENT
SOME
DATA
WHICH
DEMONSTRATES
THE
POTENTIAL
OF
SOMNILOQUY
TO
REDUCE
BOTH
DESKTOP
AND
LAPTOP
ENERGY
USAGE
IN
GENERAL
TERMS
WE
ALSO
VERIFY
THE
ENERGY
SAVING
MODEL
PRESENTED
IN
SECTION
WHICH
ALLOWS
THE
SPECIFIC
SAVINGS
IN
A
GIVEN
APPLICATION
SCENARIO
TO
BE
CALCULATED
UNLESS
OTHER
WISE
NOTED
WE
ARE
USING
THE
WIRED
VERSION
OF
OUR
PROTOTYPE
FOR
THE
DESKTOP
ENERGY
MEASUREMENTS
AND
THE
WIRELESS
VERSION
FOR
THE
LAPTOP
ENERGY
MEASURE
MENTS
REDUCING
DESKTOP
ENERGY
CONSUMPTION
OUR
TESTBED
DESKTOP
PC
CONSUMES
W
IN
NORMAL
OP
ERATION
AND
W
IN
WITH
SOMNILOQUY
SOMNILOQUY
THEREFORE
SAVES
AROUND
W
ON
THIS
BASIS
IF
SOMNILO
QUY
WERE
TO
BE
DEPLOYED
IN
AN
ENVIRONMENT
WHERE
A
PC
IS
ACTIVELY
USED
FOR
AN
AVERAGE
OF
HOURS
EACH
WEEK
I
E
OF
THE
TIME
THIS
WOULD
RESULT
IN
KWH
OF
SAVINGS
PER
COMPUTER
IN
A
YEAR
ASSUMING
KG
AND
US
THIS
MEANS
AN
ANNUAL
SAVING
OF
KG
OF
TO
PUT
IT
IN
PERSPECTIVE
THE
AV
ERAGE
US
RESIDENTS
ANNUAL
EMISSIONS
ARE
METRIC
TONNES
AS
COMPARED
TO
A
WORLDWIDE
AVERAGE
OF
MET
RIC
TONNES
PER
AND
US
PER
COMPUTER
WE
PAGE
HTML
EPA
HTML
HTM
FIGURE
POWER
CONSUMPTION
AND
THE
RESULTING
ESTIMATED
BATTERY
LIFETIME
OF
A
LENOVO
USING
SOMNILOQUY
THE
LIFETIME
IS
CALCULATED
USING
THE
STANDARD
WATT
HOUR
BATTERY
OF
THE
LAPTOP
BELIEVE
THIS
IS
SIGNIFICANTLY
HIGHER
THAN
THE
BILL
OF
MA
TERIALS
COST
OF
THE
COMPONENTS
REQUIRED
TO
IMPLEMENT
A
COMMODITIZED
SOMNILOQUY
ENABLED
NETWORK
CARD
IN
THIS
CASE
DEPLOYMENTS
OF
SOMNILOQUY
ENABLED
DEVICES
WOULD
PAY
FOR
THEMSELVES
WITHIN
A
YEAR
DESKTOP
ENERGY
SAVINGS
FOR
REAL
WORKLOADS
WE
NOW
ESTIMATE
THE
ENERGY
SAVINGS
ENABLED
BY
SOMNIL
OQUY
UNDER
REALISTIC
WORKLOADS
WE
USE
THE
DATA
PROVIDED
BY
RELATING
TO
THE
USE
PATTERNS
OF
TWENTY
TWO
DISTINCT
DESKTOP
PCS
EACH
OF
WHICH
IS
CLASSIFIED
AS
BEING
EITHER
IDLE
ACTIVE
SLEEP
OR
TURNED
OFF
WE
THEN
COMPUTE
THE
ENERGY
CONSUMED
BY
EACH
OF
THE
PCS
WITH
AND
WITHOUT
SOMNILOQUY
USING
THE
FORMULA
OF
SECTION
FOR
EASE
OF
EXPOSITION
WE
BIN
THE
DATA
INTO
THREE
DIFFERENT
CATEGORIES
PCS
THAT
ARE
IDLE
FOR
OF
THE
TIME
MACHINES
IDLE
FOR
OF
THE
TIME
MACHINES
AND
FINALLY
THOSE
THAT
ARE
IDLE
FOR
OF
THE
TIME
MACHINES
THE
AVERAGE
ENERGY
SAVINGS
FOR
THESE
TWENTY
TWO
PCS
WHEN
USING
SOMNILOQUY
IS
AS
COMPARED
TO
NORMAL
OPER
ATION
WITHOUT
SOMNILOQUY
THE
AVERAGE
ENERGY
SAVINGS
FOR
THE
PCS
IN
THE
INDIVIDUAL
CATEGORIES
ARE
AND
RESPECTIVELY
AS
EXPECTED
THE
MOST
ENERGY
SAV
INGS
ARE
FOR
THE
PCS
WITH
LARGER
IDLE
TIMES
SINCE
THEY
HAVE
MORE
OPPORTUNITY
TO
USE
SOMNILOQUY
INCREASING
LAPTOP
BATTERY
LIFETIME
FIGURE
SHOWS
THE
AVERAGE
POWER
CONSUMPTION
OF
THE
LAPTOP
TESTBED
WHEN
OPERATING
NORMALLY
I
E
NO
POWER
SAVING
MECHANISMS
WITH
STANDARD
POWER
SAVING
MECH
ANISMS
IN
PLACE
THE
BASELINE
POWER
WHEN
SOMNILOQUY
WIRELESS
IS
OPERATIONAL
AND
IN
THE
STANDARD
MODE
WITHOUT
THE
GUMSTIX
ATTACHED
SOMNILOQUY
ADDS
A
RELATIVELY
LOW
OVERHEAD
OF
MW
TO
MODE
RESULT
ING
IN
A
TOTAL
POWER
CONSUMPTION
WHICH
IS
CLOSE
TO
JUST
FIGURE
COMPARING
THE
ANALYTICAL
RESULTS
WITH
THE
MEA
SURED
VALUES
FOR
THE
WEB
DOWNLOAD
STUB
THE
FLASH
STOR
AGE
AVAILABLE
ON
THE
GUMSTIX
IS
SET
TO
MB
UNLESS
STATED
OTHERWISE
W
AS
COMPARED
TO
THE
W
OF
THE
IDLE
LAPTOP
THIS
MEANS
THAT
WHEN
THE
LAPTOP
NEEDS
TO
BE
ATTACHED
TO
THE
NETWORK
AND
AVAILABLE
FOR
REMOTE
APPLICATIONS
BUT
IS
OTH
ERWISE
IDLE
IT
CAN
BE
PUT
INTO
SOMNILOQUY
MODE
TO
ENABLE
AN
ORDER
OF
MAGNITUDE
DECREASE
IN
POWER
CONSUMPTION
AND
A
RESULTING
INCREASE
IN
BATTERY
LIFETIME
FROM
HOURS
TO
HOURS
USING
THE
STANDARD
WATT
HOUR
BATTERY
ENERGY
SAVINGS
FOR
SPECIFIC
APPLICATIONS
THE
BASIC
ANALYSIS
OF
ENERGY
CONSUMPTION
AND
BATTERY
LIFETIME
PRESENTED
ABOVE
IS
VERY
GENERIC
FOR
A
GIVEN
US
AGE
SCENARIO
IT
SHOULD
BE
POSSIBLE
TO
USE
THE
ENERGY
SAV
ING
MODEL
PRESENTED
IN
SECTION
TO
PREDICT
SAVINGS
MUCH
MORE
ACCURATELY
IN
ORDER
TO
VALIDATE
THIS
MODEL
WE
RAN
EXPERIMENTS
DOWNLOADING
CONTENT
FROM
A
REMOTE
WEB
SERVER
AND
MEASURED
BOTH
ENERGY
CONSUMPTION
AND
LATENCY
SO
AS
TO
COMPARE
THEM
WITH
THEIR
CORRESPONDING
ANALYTICAL
VALUES
NOTE
THAT
WE
ONLY
MEASURE
THE
ENERGY
CONSUMPTION
FOR
THE
DURATION
OF
THE
APPLICATION
THE
WEB
DOWNLOAD
STUB
WAS
CHOSEN
SINCE
IT
WAS
RELA
TIVELY
EASY
TO
CHANGE
THE
DUTY
CYCLE
OF
THE
HOST
I
E
THE
DURATION
FOR
WHICH
THE
HOST
CAN
SLEEP
TSLEEP
AFTER
WHICH
IT
NEEDS
TO
BE
WOKEN
UP
TO
TRANSFER
DATA
FROM
THE
GUMSTIX
TAWAKE
AS
DISCUSSED
IN
SECTION
TSLEEP
DEPENDS
ON
THE
DOWNLOAD
BANDWIDTH
AND
THE
AMOUNT
OF
FLASH
STORAGE
ON
THE
GUMSTIX
WHILE
TAWAKE
DEPENDS
ON
THE
AMOUNT
OF
FLASH
STORAGE
ON
THE
GUMSTIX
AND
THE
TRANSFER
RATE
BETWEEN
THE
GUMSTIX
AND
THE
HOST
WE
DOWNLOADED
A
MB
FILE
AT
VARIOUS
LINK
BANDWIDTHS
RANGING
FROM
KBPS
TO
MBPS
AND
USED
TWO
DIFFERENT
FLASH
STORAGE
SIZES
AT
THE
GUMSTIX
MB
AND
MB
EFFECTIVELY
VARYING
TSLEEP
FROM
APPROXIMATELY
SECONDS
DOWN
TO
SECONDS
WE
MEASURED
THE
POWER
CONSUMED
DURING
THE
DOWNLOAD
USING
THE
METHODOLOGY
DESCRIBED
IN
THE
BEGIN
NING
OF
THIS
SECTION
IN
FIGURE
WE
PRESENT
THE
MEASURED
ENERGY
SAVINGS
AND
THE
CORRESPONDING
PREDICTED
VALUES
USING
OUR
MODEL
FOR
FOUR
DIFFERENT
DATA
POINTS
AS
WE
CAN
SEE
FROM
THE
FIGURE
THE
PREDICTED
ENERGY
SAVINGS
AND
THE
INCREASED
LATENCY
CLOSELY
MATCH
THE
MEASURED
VALUES
WITHIN
THE
VALUES
DO
NOT
EXACTLY
MATCH
SINCE
THE
ACTUAL
MEASURED
POWER
VALUES
VARY
OVER
TIME
AND
THE
TIME
TAKEN
TO
SUSPEND
AND
RESUME
ALSO
VARIES
ACROSS
RUNS
WE
USED
A
FIXED
VALUE
FOR
THESE
IN
THE
FORMULA
FIGURE
ALSO
ILLUSTRATES
THAT
INCREASING
THE
BANDWIDTH
FROM
KBPS
TO
MBPS
REDUCES
THE
ENERGY
SAVINGS
FROM
TO
AND
INCREASES
THE
LATENCY
FROM
TO
ALTHOUGH
A
LARGER
AMOUNT
OF
FLASH
STORAGE
IMPROVES
THE
ENERGY
SAVING
AND
LATENCY
AS
EXPLAINED
EARLIER
THIS
IS
DUE
TO
THE
LIMITED
TRANSFER
SPEED
OF
THE
USBNET
INTER
FACE
IN
OUR
PROTOTYPE
MBPS
BECAUSE
OF
WHICH
THE
PC
IS
AWAKE
FOR
LONGER
PERIODS
OF
TIME
WHILE
TRANSFER
RING
THE
DATA
FROM
THE
GUMSTIX
TAWAKE
SECONDS
TO
TRANSFER
MB
OF
DATA
IN
FIGURE
WE
HAVE
ALSO
PLOT
TED
AN
IDEAL
CASE
MBPS
IDEAL
WHERE
THE
HOST
CAN
READ
THE
FLASH
STORAGE
OF
THE
GUMSTIX
DIRECTLY
FOR
THE
IDEAL
CASE
THE
DURATION
FOR
WHICH
THE
HOST
NEEDS
TO
STAY
AWAKE
TO
TRANSFER
DATA
FROM
THE
GUMSTIX
REDUCES
CONSIDERABLY
TAWAKE
SECONDS
THIS
IMPROVES
ENERGY
SAVINGS
TO
AND
LIMITS
THE
INCREASE
IN
LATENCY
WHEN
USING
SOM
NILOQUY
TO
LESS
THAN
RELATED
WORK
THERE
HAVE
BEEN
SEVERAL
PROPOSALS
TO
REDUCE
THE
EN
ERGY
CONSUMPTION
OF
DESKTOP
PCS
AND
LAPTOPS
PRIOR
WORK
CAN
LARGELY
BE
GROUPED
IN
THREE
CATEGORIES
RE
DUCING
THE
ACTIVE
POWER
CONSUMPTION
OF
DEVICES
WHEN
AWAKE
REDUCING
THE
POWER
CON
SUMPTION
OF
THE
NETWORK
INFRASTRUCTURE
E
G
ROUTERS
AND
SWITCHES
AND
OPPORTUNISTICALLY
PUTTING
THE
DEVICES
TO
SLEEP
SOMNILOQUY
FALLS
IN
THE
THIRD
CATEGORY
SINCE
A
MACHINE
IN
SLEEP
STATE
CONSUMES
SIGNIFICANTLY
LESS
POWER
THAN
IN
LOWEST
POWER
ACTIVE
STATE
VER
IFIED
BY
US
IN
SECTION
SIGNIFICANT
ENERGY
SAVINGS
ARE
POSSIBLE
BY
PUTTING
THE
MACHINE
TO
SLEEP
WHENEVER
POS
SIBLE
FOR
OPPORTUNISTIC
SLEEP
SYSTEMS
THE
BIGGEST
CHALLENGE
IS
TO
ENSURE
CONNECTIVITY
WHEN
THE
HOST
IS
ASLEEP
PRIOR
TECHNIQUES
TO
SOLVE
THIS
PROBLEM
EITHER
USE
ADVANCED
FUNCTIONALITY
IN
THE
NIC
OR
USE
EXTRA
NETWORK
IN
TERFACES
WE
NOW
COMPARE
AND
CONTRAST
SOMNIL
OQUY
TO
BOTH
THESE
CLASSES
OF
WORK
AMONG
SCHEMES
THAT
DO
NOT
USE
AN
EXTRA
NET
WORK
INTERFACE
THE
MOST
WELL
KNOWN
ARE
WAKE
ON
LAN
WOL
AND
ITS
WIRELESS
EQUIVALENT
WAKE
ON
WLAN
WOWLAN
IN
BOTH
THESE
SCHEMES
THE
NIC
PARSES
IN
COMING
PACKETS
WHEN
THE
HOST
IS
ASLEEP
IT
WAKES
UP
THE
HOST
PC
WHENEVER
AN
INCOMING
MAGIC
PACKET
IS
RE
CEIVED
ACCORDING
TO
THE
SPECIFICATION
THE
MAGIC
PACKET
PAYLOAD
MUST
INCLUDE
CHARACTERS
OF
A
WAKEUP
PATTERN
THAT
IS
SET
BY
THE
HOST
PC
FOLLOWED
BY
COPIES
OF
THE
NIC
MAC
ADDRESS
IN
WOWLAN
THE
ONLY
DIF
FERENCE
IS
THAT
THIS
PACKET
IS
SENT
OVER
THE
WIRELESS
LAN
ALTHOUGH
MOST
MODERN
NICS
IMPLEMENT
WOL
FUNCTION
ALITY
FEW
DEPLOYED
SYSTEMS
ACTUALLY
USE
THIS
FUNCTION
ALITY
DUE
TO
FOUR
MAIN
REASONS
FIRST
THE
REMOTE
HOST
MUST
KNOW
THAT
THE
PC
IS
ASLEEP
AND
THAT
IT
MUST
WAKE
IT
UP
BEFORE
PURSUING
APPLICATION
FUNCTIONALITY
SECOND
THE
REMOTE
HOST
MUST
HAVE
A
WAY
OF
SENDING
A
PACKET
TO
THE
SLEEPING
PC
THROUGH
ANY
FIREWALLS
NAT
BOXES
WHICH
TYPICALLY
DO
NOT
ALLOW
INCOMING
CONNECTIONS
WITHOUT
SPE
CIAL
CONFIGURATION
THIRD
THE
REMOTE
HOST
MUST
KNOW
THE
MAC
ADDRESS
OF
THE
SLEEPING
PC
FOURTH
WOWLAN
DOES
NOT
WORK
WHEN
LAPTOPS
CHANGE
THEIR
SUBNET
BECAUSE
OF
MOBILITY
IN
CONTRAST
SOMNILOQUY
DOES
NOT
REQUIRE
THE
EXTRA
CONFIGURATION
OF
FIREWALLS
NAT
BOXES
AND
IS
TRANS
PARENT
TO
REMOTE
APPLICATION
SERVERS
IT
CAN
HANDLE
MO
BILITY
ACROSS
SUBNETS
SINCE
THE
SECONDARY
PROCESSOR
CAN
RE
ASSOCIATE
WITH
SERVICES
SUCH
AS
DYNAMIC
DNS
TO
REDI
RECT
A
PERMANENT
HOST
NAME
TO
THE
PC
NEW
IP
ADDRESS
AND
RE
LOG
IN
TO
SERVERS
SUCH
AS
IM
SERVERS
IN
ADDITION
TO
THESE
DIFFERENCES
SOMNILOQUY
ALSO
ALLOWS
APPLICATIONS
TO
BE
OFFLOADED
TO
THE
LOW
POWER
PROCESSOR
THERE
IS
NO
SUCH
CONCEPT
IN
WOL
WHICH
INSTEAD
WAKES
UP
THE
HOST
WHEN
ANY
PATTERN
IS
MATCHED
INTEL
RECENTLY
ANNOUNCED
ITS
REMOTE
WAKE
CHIPSET
TECHNOLOGY
RWT
THAT
CLAIMS
TO
EXTEND
WOL
ON
NEW
MOTHERBOARDS
BY
ALLOWING
VOIP
CALLS
TO
WAKE
UP
A
SYSTEM
ALTHOUGH
ITS
GENERAL
APPLICABILITY
TO
OTHER
APPLI
CATIONS
IS
NOT
KNOWN
THE
DETAILS
OF
THIS
TECHNOLOGY
ARE
NOT
PUBLISHED
IN
CONTRAST
SOMNILOQUY
GOES
BEYOND
JUST
WOL
OR
RWT
IT
ALLOWS
LOW
POWER
OPERATION
FOR
VARIOUS
APPLICATIONS
OTHER
THAN
VOIP
FURTHERMORE
SOMNILOQUY
DOES
NOT
REQUIRE
MODIFICATIONS
TO
APPLICATION
END
POINTS
OR
SERVERS
RWT
REQUIRES
APPLICATIONS
TO
FIRST
CONTACT
A
SERVER
WHICH
THEN
SENDS
A
SPECIAL
PACKET
TO
THE
PC
TO
SIGNAL
A
WAKE
UP
ANOTHER
APPROACH
IS
TO
USE
ADDITIONAL
LOW
POWER
NETWORK
INTERFACES
TO
MAINTAIN
CONNECTIVITY
TO
THE
PC
THAT
IS
ASLEEP
THIS
APPROACH
HAS
BEEN
PROPOSED
FOR
USE
WITH
MOBILE
DEVICES
FOR
EXAMPLE
WAKE
ON
WIRELESS
WAKES
UP
THE
HOST
PC
ON
RECEIVING
A
SPECIAL
PACKET
ON
THE
LOW
POWER
NETWORK
INTERFACE
TURDUCKEN
USES
SEVERAL
TIERS
OF
NETWORK
INTERFACES
AND
PROCESSORS
WITH
DIFFERENT
POWER
CHARACTERISTICS
AND
WAKES
UP
THE
UPPER
TIER
WHEN
THE
LOWER
TIER
CANNOT
HANDLE
A
TASK
IN
CON
TRAST
TO
THESE
SCHEMES
SOMNILOQUY
REQUIRES
ONLY
A
SINGLE
NETWORK
INTERFACE
AND
PRESENTS
THE
PARADIGM
OF
A
SINGLE
PC
TO
USERS
RATHER
THAN
A
MULTI
TIERED
SYSTEM
PRESERV
ING
THE
CURRENT
USER
EXPERIENCE
AND
THEREFORE
REQUIRING
LESS
TRAINING
TO
USE
SOMNILOQUY
ALSO
GIVES
THE
IMPRES
SION
TO
REMOTE
APPLICATION
SERVERS
THAT
A
DEVICE
REMAINS
AWAKE
ALL
THE
TIME
EVEN
THOUGH
IT
IS
ACTUALLY
ASLEEP
SINCE
THE
SAME
MAC
AND
IP
ADDRESSES
ARE
USED
THIS
LEVEL
OF
TRANSPARENCY
IS
NOT
PROVIDED
EITHER
BY
WAKE
ON
WIRELESS
OR
TURDUCKEN
FINALLY
WE
HAVE
GONE
INTO
MORE
DETAIL
THAN
PREVIOUS
WORK
ON
WAYS
OF
SUPPORTING
APPLICATIONS
THAT
REQUIRE
INTERACTIONS
AMONG
THE
SECONDARY
AND
THE
HOST
PROCESSOR
TO
PERFORM
OFFLOAD
SUCH
AS
IM
BITTOR
RENT
AND
WEB
DOWNLOADS
TO
REDUCE
THE
POWER
CONSUMED
BY
DESKTOP
PCS
SOME
EARLY
PROPOSALS
HAVE
SUGGESTED
THE
USE
OF
PROXIES
ON
THE
SUBNET
THAT
FUNCTION
ON
BEHALF
OF
THE
DESKTOP
PC
WHEN
IT
IS
ASLEEP
THE
PROXY
MONITORS
INCOMING
PACK
ETS
FOR
THE
PC
AND
WAKES
IT
UP
USING
WOL
WHEN
THE
PC
NEEDS
TO
HANDLE
THE
PACKET
WE
ARE
NOT
AWARE
OF
ANY
PUB
LISHED
PROTOTYPE
IMPLEMENTATIONS
OF
SUCH
SYSTEMS
RE
CENTLY
SABHANATARAJAN
ET
AL
PROPOSE
A
SMART
NIC
THAT
CAN
ACT
AS
PROXY
FOR
A
HOST
TO
SAVE
POWER
HOW
EVER
THE
AUTHORS
FOCUS
PRIMARILY
ON
THE
DESIGN
OF
A
HIGH
SPEED
PACKET
CLASSIFIER
FOR
SUCH
AN
INTERFACE
IN
COMPAR
ISON
SOMNILOQUY
HAS
MUCH
WIDER
APPLICABILITY
THAN
THE
ABOVE
SCHEMES
IT
CAN
BE
USED
IN
HOMES
AND
SMALL
OFFICES
WHERE
IT
MIGHT
BE
INFEASIBLE
TO
DEPLOY
A
DEDICATED
SERVER
TO
HANDLE
PROCESSING
FOR
ANOTHER
PC
A
CONTEMPORANEOUS
EFFORT
TO
SOMNILOQUY
IS
THE
IDEA
OF
A
NETWORK
CONNECTION
PROXY
NCP
WHICH
IS
A
NETWORK
ENTITY
THAT
MAINTAINS
THE
PRESENCE
OF
A
SLEEP
ING
PC
IN
THE
AUTHORS
DEFINE
THE
REQUIREMENTS
OF
AN
NCP
AND
PROPOSE
MODIFICATIONS
TO
THE
SOCKET
LAYER
SIMILAR
TO
SPLIT
TCP
FOR
KEEPING
TCP
CONNECTIONS
ALIVE
THROUGH
A
PC
SLEEP
TRANSITIONS
IN
THE
AUTHORS
EX
TEND
THESE
APIS
TO
SUPPORT
OTHER
PROTOCOLS
AS
WELL
SOM
NILOQUY
IS
SIMILAR
IN
SPIRIT
TO
NCP
AND
NCP
SOCKET
APIS
CAN
REDUCE
SOMNILOQUY
OVERHEAD
WHEN
WAKING
UP
FROM
SLEEP
SECTION
FURTHERMORE
TO
THE
BEST
OF
OUR
KNOWLEDGE
SOMNILOQUY
IS
THE
FIRST
PUBLISHED
PROTO
TYPE
OF
ANY
PROXYING
SYSTEM
WE
NOTE
THAT
THE
CONCEPT
OF
ADDING
MORE
PROCESS
ING
TO
THE
NETWORK
INTERFACE
IS
NOT
NEW
EXISTING
PROD
UCTS
OFFLOAD
PROCESSING
TO
THE
NIC
TO
IMPROVE
PERFOR
MANCE
TCP
OFFLOAD
AND
REMOTE
MANAGEABILITY
IN
TEL
AMT
SOMNILOQUY
USES
A
SIMILAR
OFFLOADING
PARADIGM
BUT
TO
CONSERVE
ENERGY
INSTEAD
OF
IMPROVING
PERFORMANCE
OR
MANAGEABILITY
CONCLUSIONS
WE
HAVE
PRESENTED
SOMNILOQUY
A
SYSTEM
THAT
AUGMENTS
NETWORK
INTERFACES
TO
ALLOW
PCS
TO
BE
PUT
INTO
LOW
POWER
SLEEP
STATES
OPPORTUNISTICALLY
WITHOUT
SACRIFICING
FUNC
TIONALITY
SOMNILOQUY
ENABLES
SEVERAL
NEW
ENERGY
SAV
ING
OPPORTUNITIES
FIRST
PCS
CAN
BE
PUT
TO
SLEEP
WHILE
MAINTAINING
NETWORK
REACHABILITY
WITHOUT
SPECIAL
NET
WORK
INFRASTRUCTURE
AS
NEEDED
BY
PREVIOUS
SOLUTIONS
E
G
WOL
SECOND
SOME
APPLICATIONS
CAN
BE
RUN
IN
SLEEP
MODE
THEREBY
REQUIRING
MUCH
LESS
POWER
IN
THIS
PAPER
WE
HAVE
SHOWN
THE
FEASIBILITY
FOR
THREE
SUCH
APPLICATIONS
TO
BE
RUN
IN
SLEEP
MODE
BITTORRENT
INSTANT
MESSAGING
AND
WEB
DOWNLOADS
SOMNILOQUY
ACHIEVES
THESE
ENERGY
SAVINGS
WITHOUT
RE
QUIRING
ANY
MODIFICATIONS
TO
NETWORK
TO
REMOTE
APPLI
CATION
SERVERS
OR
TO
THE
USER
EXPERIENCE
OF
THE
PC
FUR
THERMORE
SOMNILOQUY
CAN
BE
INCREMENTALLY
DEPLOYED
ON
LEGACY
NETWORK
INTERFACES
AND
DOES
NOT
RELY
ON
CHANGES
TO
THE
CPU
SCHEDULER
OR
THE
MEMORY
MANAGER
TO
IMPLE
MENT
THIS
FUNCTIONALITY
THUS
IT
IS
COMPATIBLE
WITH
A
WIDE
CLASS
OF
MACHINES
AND
OPERATING
SYSTEMS
OUR
PROTOTYPE
IMPLEMENTATION
BASED
ON
A
USB
PE
RIPHERAL
INCLUDES
SUPPORT
FOR
WAKING
UP
THE
PC
ON
NET
WORK
EVENTS
SUCH
AS
INCOMING
FILE
COPY
REQUESTS
VOIP
CALLS
INSTANT
MESSAGES
AND
REMOTE
DESKTOP
CONNECTIONS
AND
WE
HAVE
ALSO
DEMONSTRATED
THAT
FILE
SHARING
CONTENT
DISTRIBUTION
SYSTEMS
E
G
BITTORRENT
WEB
DOWNLOADS
CAN
RUN
IN
THE
AUGMENTED
NETWORK
INTERFACE
ALLOWING
FOR
FILE
DOWNLOADS
TO
PROGRESS
WITHOUT
THE
PC
BEING
AWAKE
OUR
TESTS
SHOW
POWER
SAVINGS
OF
ARE
POSSIBLE
FOR
DESKTOP
PCS
LEFT
ON
WHEN
IDLE
OR
FOR
LAPTOPS
FOR
PCS
THAT
ARE
LEFT
IDLE
MOST
OF
THE
TIME
THIS
TRANSLATES
TO
ENERGY
SAVINGS
OF
TO
THE
ELECTRICITY
SAVINGS
MADE
ARE
SUCH
THAT
DEPLOYING
A
PRODUCTIZED
VERSION
OF
SOMNILOQUY
COULD
PAY
FOR
ITSELF
WITHIN
A
YEAR
OPTIMALITY
FAIRNESS
AND
ROBUSTNESS
IN
SPEED
SCALING
DESIGNS
LACHLAN
L
H
ANDREW
CENTRE
FOR
ADVANCED
INTERNET
ARCHITECTURES
SWINBURNE
UNIVERSITY
OF
TECHNOLOGY
AUSTRALIA
MINGHONG
LIN
ADAM
WIERMAN
COMPUTER
SCIENCE
DEPARTMENT
CALIFORNIA
INSTITUTE
OF
TECHNOLOGY
ABSTRACT
SYSTEM
DESIGN
MUST
STRIKE
A
BALANCE
BETWEEN
ENERGY
AND
PER
FORMANCE
BY
CAREFULLY
SELECTING
THE
SPEED
AT
WHICH
THE
SYS
TEM
WILL
RUN
IN
THIS
WORK
WE
EXAMINE
FUNDAMENTAL
TRADE
OFFS
INCURRED
WHEN
DESIGNING
A
SPEED
SCALER
TO
MINIMIZE
A
WEIGHTED
SUM
OF
EXPECTED
RESPONSE
TIME
AND
ENERGY
USE
PER
JOB
WE
PROVE
THAT
A
POPULAR
DYNAMIC
SPEED
SCALING
ALGO
RITHM
IS
COMPETITIVE
FOR
THIS
OBJECTIVE
AND
THAT
NO
NAT
URAL
SPEED
SCALER
CAN
IMPROVE
ON
THIS
FURTHER
WE
PROVE
THAT
ENERGY
PROPORTIONAL
SPEED
SCALING
WORKS
WELL
ACROSS
TWO
COMMON
SCHEDULING
POLICIES
SHORTEST
REMAINING
PROCESS
ING
TIME
SRPT
AND
PROCESSOR
SHARING
PS
THIRD
WE
SHOW
THAT
UNDER
SRPT
AND
PS
GATED
STATIC
SPEED
SCALING
IS
NEARLY
OPTIMAL
WHEN
THE
MEAN
WORKLOAD
IS
KNOWN
BUT
THAT
DYNAMIC
SPEED
SCALING
PROVIDES
ROBUSTNESS
AGAINST
UNCERTAIN
WORKLOADS
FINALLY
WE
PROVE
THAT
SPEED
SCALING
MAGNIFIES
UNFAIRNESS
NOTABLY
SRPT
BIAS
AGAINST
LARGE
JOBS
AND
THE
BIAS
AGAINST
SHORT
JOBS
IN
NON
PREEMPTIVE
POLICIES
HOWEVER
PS
REMAINS
FAIR
UNDER
SPEED
SCALING
TOGETHER
THESE
RESULTS
SHOW
THAT
THE
SPEED
SCALERS
STUDIED
HERE
CAN
ACHIEVE
ANY
TWO
BUT
ONLY
TWO
OF
OPTIMALITY
FAIRNESS
AND
ROBUSTNESS
INTRODUCTION
COMPUTER
SYSTEMS
MUST
MAKE
A
FUNDAMENTAL
TRADEOFF
BE
TWEEN
PERFORMANCE
AND
ENERGY
USAGE
THE
DAYS
OF
FASTER
IS
BETTER
ARE
GONE
ENERGY
USAGE
CAN
NO
LONGER
BE
IGNORED
IN
DESIGNS
ALL
THE
WAY
FROM
CHIPS
TO
MOBILE
DEVICES
TO
DATA
CENTERS
THE
IMPORTANCE
OF
ENERGY
HAS
LED
DESIGNS
AT
ALL
LEVELS
OF
SYSTEMS
TO
MOVE
TOWARD
SPEED
SCALING
ONCE
A
TECHNIQUE
USED
PRIMARILY
AT
THE
CHIP
LEVEL
SPEED
SCALING
DESIGNS
ADAPT
THE
SPEED
OF
THE
SYSTEM
SO
AS
TO
BALANCE
ENERGY
AND
PERFOR
MANCE
MEASURES
SPEED
SCALING
DESIGNS
CAN
BE
HIGHLY
SO
PHISTICATED
ADAPTING
THE
SPEED
AT
ALL
TIMES
TO
THE
CURRENT
STATE
DYNAMIC
SPEED
SCALING
OR
VERY
SIMPLE
RUNNING
AT
A
STATIC
SPEED
THAT
IS
CHOSEN
TO
BALANCE
ENERGY
AND
PER
FORMANCE
EXCEPT
WHEN
IDLE
GATED
STATIC
SPEED
SCALING
THE
GROWING
ADOPTION
OF
SPEED
SCALING
DESIGNS
FOR
SYSTEMS
FROM
CHIPS
TO
DISKS
TO
DATA
CENTERS
HAS
SPURRED
ANALYTIC
RE
SEARCH
INTO
THE
TOPIC
THE
ANALYTIC
STUDY
OF
THE
SPEED
SCAL
ING
PROBLEM
BEGAN
WITH
YAO
ET
AL
IN
SINCE
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
COPYRIGHT
ACM
X
XXXXX
XX
X
XX
XX
00
THREE
MAIN
PERFORMANCE
OBJECTIVES
BALANCING
ENERGY
AND
DE
LAY
HAVE
BEEN
CONSIDERED
I
MINIMIZE
THE
TOTAL
ENERGY
USED
IN
ORDER
TO
MEET
JOB
DEADLINES
E
G
II
MINIMIZE
THE
AVERAGE
RESPONSE
TIME
GIVEN
AN
ENERGY
POWER
BUDGET
E
G
AND
III
MINIMIZE
A
LINEAR
COMBINATION
OF
EXPECTED
RESPONSE
TIME
AND
ENERGY
USAGE
PER
JOB
IN
THIS
WORK
WE
FOCUS
ON
THE
THIRD
OBJECTIVE
THIS
OBJECTIVE
CAPTURES
HOW
MUCH
REDUCTION
IN
RESPONSE
TIME
IS
NECESSARY
TO
JUSTIFY
USING
AN
EXTRA
JOULE
OF
ENERGY
AND
NATURALLY
APPLIES
TO
SETTINGS
WHERE
THERE
IS
A
KNOWN
MONETARY
COST
TO
EXTRA
DELAY
E
G
MANY
WEB
APPLICATIONS
FUNDAMENTALLY
A
SPEED
SCALING
ALGORITHM
MUST
MAKE
TWO
DECISIONS
AT
EACH
TIME
I
A
SCHEDULING
POLICY
MUST
DECIDE
WHICH
JOB
TO
SERVICE
AND
II
A
SPEED
SCALER
MUST
DECIDE
HOW
FAST
TO
RUN
THE
SERVER
IT
HAS
BEEN
NOTED
BY
PRIOR
WORK
E
G
THAT
AN
OPTIMAL
SPEED
SCALING
ALGORITHM
WILL
USE
SHORTEST
REMAINING
PROCESSING
TIME
SRPT
SCHEDULING
HOWEVER
IN
REAL
SYSTEMS
IT
IS
OFTEN
IMPOSSIBLE
TO
IMPLE
MENT
SRPT
SINCE
IT
REQUIRES
EXACT
KNOWLEDGE
OF
REMAINING
SIZES
INSTEAD
TYPICAL
SYSTEM
DESIGNS
OFTEN
USE
SCHEDULING
THAT
IS
CLOSER
TO
PROCESSOR
SHARING
PS
E
G
WEB
SERVERS
OPERATING
SYSTEMS
AND
ROUTERS
IN
THIS
WORK
WE
FOCUS
ON
THE
DESIGN
OF
SPEED
SCALERS
FOR
BOTH
SRPT
AND
PS
THE
STUDY
OF
SPEED
SCALING
ALGORITHMS
FOR
THESE
TWO
POLICIES
IS
NOT
NEW
THERE
HAS
BEEN
SIGNIFICANT
PRIOR
WORK
WHICH
WE
DISCUSS
IN
SECTIONS
AND
STUDYING
SPEED
SCALING
FOR
SRPT
AND
FOR
PS
INTERESTINGLY
THE
PRIOR
WORK
FOR
SRPT
IS
ENTIRELY
DONE
USING
A
WORST
CASE
FRAMEWORK
WHILE
THE
PRIOR
WORK
FOR
PS
IS
DONE
IN
A
STOCHASTIC
ENVIRONMENT
THE
M
GI
QUEUE
DESPITE
THE
CONSIDERABLE
LITERATURE
STUDYING
SPEED
SCALING
THERE
ARE
MANY
FUNDAMENTAL
ISSUES
IN
THE
DESIGN
OF
SPEED
SCALING
ALGORITHMS
THAT
ARE
NOT
YET
UNDERSTOOD
THIS
PAPER
PROVIDES
NEW
INSIGHTS
INTO
FOUR
OF
THESE
ISSUES
CAN
A
SPEED
SCALING
ALGORITHM
BE
OPTIMAL
WHAT
STRUC
TURE
DO
NEAR
OPTIMAL
ALGORITHMS
HAVE
HOW
DOES
SPEED
SCALING
INTERACT
WITH
SCHEDULING
HOW
IMPORTANT
IS
THE
SOPHISTICATION
OF
THE
SPEED
SCALER
WHAT
ARE
THE
DRAWBACKS
OF
SPEED
SCALING
TO
ADDRESS
THESE
QUESTIONS
WE
STUDY
BOTH
PS
AND
SRPT
SCHEDULING
UNDER
BOTH
DYNAMIC
AND
GATED
STATIC
SPEED
SCAL
ING
ALGORITHMS
OUR
WORK
PROVIDES
I
NEW
RESULTS
FOR
DY
NAMIC
SPEED
SCALING
WITH
SRPT
SCHEDULING
IN
THE
WORST
CASE
MODEL
II
THE
FIRST
RESULTS
FOR
DYNAMIC
SPEED
SCALING
WITH
PS
SCHEDULING
IN
THE
WORST
CASE
MODEL
III
THE
FIRST
RESULTS
FOR
DYNAMIC
SPEED
SCALING
WITH
SRPT
SCHEDULING
IN
THE
STOCHAS
TIC
MODEL
IV
THE
FIRST
RESULTS
FOR
GATED
STATIC
SPEED
SCALING
WITH
SRPT
IN
THE
STOCHASTIC
MODEL
AND
V
THE
FIRST
RESULTS
IDENTIFYING
UNFAIRNESS
IN
SPEED
SCALING
DESIGNS
TABLE
SUM
MARIZES
THESE
THESE
RESULTS
LEAD
TO
IMPORTANT
NEW
INSIGHTS
INTO
ISSUES
I
IV
ABOVE
WE
DESCRIBE
THESE
INSIGHTS
INFORMALLY
HERE
AND
PROVIDE
POINTERS
TO
THE
RESULTS
IN
THE
BODY
OF
THE
PAPER
WITH
RESPECT
TO
ISSUE
I
OUR
RESULTS
SHOW
THAT
ENERGY
PROPORTIONAL
SPEED
SCALING
PROVIDES
NEAR
OPTIMAL
PERFOR
MANCE
SPECIFICALLY
WE
CONSIDER
THE
ALGORITHM
WHICH
USES
SRPT
SCHEDULING
AND
CHOOSES
SN
THE
SPEED
TO
RUN
AT
GIVEN
N
JOBS
TO
SATISFY
P
SN
NΒ
WHERE
P
IS
THE
POWER
NEEDED
TO
RUN
AT
SPEED
AND
Β
IS
THE
COST
OF
ENERGY
WE
PROVE
THAT
THIS
ALGORITHM
IS
Ε
COMPETITIVE
UNDER
GENERAL
P
COROLLARY
THIS
PROVIDES
A
TIGHT
ANALYSIS
OF
AN
ALGORITHM
WITH
A
CONSIDERABLE
LITERATURE
E
G
SEE
SEC
TION
FOR
A
DISCUSSION
IT
ALSO
GIVES
ANALYTIC
JUSTIFICATION
FOR
A
COMMON
HEURISTIC
APPLIED
BY
SYSTEM
DESIGNERS
E
G
FURTHER
WE
SHOW
THAT
NO
NATURAL
SPEED
SCALING
ALGORITHM
DEFINITION
CAN
BE
BETTER
THAN
COMPETITIVE
THEOREM
WHICH
IMPLIES
THAT
NO
ONLINE
ENERGY
PROPORTIONAL
SPEED
SCALER
CAN
MATCH
THE
OFFLINE
OPTIMAL
WITH
RESPECT
TO
ISSUE
II
OUR
RESULTS
UNCOVER
TWO
NEW
INSIGHTS
FIRST
WE
PROVE
THAT
AT
LEAST
WITH
RESPECT
TO
PS
AND
SRPT
SPEED
SCALING
CAN
BE
DECOUPLED
FROM
THE
SCHEDULER
THAT
IS
ENERGY
PROPORTIONAL
SPEED
SCALING
PERFORMS
WELL
FOR
BOTH
SRPT
AND
PS
AND
ANOTHER
POLICY
LAPS
STUDIED
IN
SPECIFICALLY
WE
SHOW
THAT
PS
SCHEDULING
WITH
SPEEDS
SUCH
THAT
P
SN
N
WHICH
ARE
OPTIMALLY
COMPETITIVE
UNDER
SRPT
IS
AGAIN
O
COMPETITIVE
THEOREM
FURTHER
WE
SHOW
THAT
USING
THE
SPEEDS
OPTIMAL
FOR
AN
M
GI
PS
QUEUE
TO
CONTROL
INSTEAD
AN
M
GI
SRPT
QUEUE
LEADS
TO
NEARLY
OPTIMAL
PERFORMANCE
SECTION
SECOND
OUR
RESULTS
SHOW
THAT
SCHEDULING
IS
NOT
AS
IMPORTANT
ONCE
ENERGY
IS
CONSIDERED
SPECIFICALLY
PS
IS
O
COMPETITIVE
FOR
THE
LINEAR
COMBINA
TION
OF
ENERGY
AND
RESPONSE
TIME
HOWEVER
WHEN
JUST
MEAN
RESPONSE
TIME
IS
CONSIDERED
PS
IS
Ω
COMPETITIVE
FOR
IN
STANCES
WITH
Ν
JOBS
SIMILARLY
WE
SEE
IN
THE
STOCHASTIC
ENVIRONMENT
THAT
THE
PERFORMANCE
UNDER
SRPT
AND
PS
IS
ALMOST
INDISTINGUISHABLE
E
G
FIGURE
TOGETHER
THE
IN
SIGHTS
INTO
ISSUE
II
PROVIDE
A
SIGNIFICANT
SIMPLIFICATION
OF
THE
DESIGN
OF
SPEED
SCALING
SYSTEMS
THEY
SUGGEST
THAT
PRACTI
TIONERS
CAN
SEPARATE
TWO
SEEMINGLY
COUPLED
DESIGN
DECISIONS
AND
DEAL
WITH
EACH
INDIVIDUALLY
WITH
RESPECT
TO
ISSUE
III
OUR
RESULTS
ADD
SUPPORT
TO
AN
INSIGHT
SUGGESTED
BY
PRIOR
WORK
PRIOR
WORK
HAS
SHOWN
THAT
THE
OPTIMAL
GATED
STATIC
SPEED
SCALING
ALGORITHM
PER
FORMS
NEARLY
AS
WELL
AS
THE
OPTIMAL
DYNAMIC
SPEED
SCALING
AL
GORITHM
IN
THE
M
GI
PS
SETTING
OUR
RESULTS
SHOW
THAT
THE
SAME
HOLDS
FOR
SRPT
SECTION
THUS
SOPHISTICATION
DOES
NOT
PROVIDE
SIGNIFICANT
PERFORMANCE
IMPROVEMENTS
IN
SPEED
SCALING
DESIGNS
HOWEVER
SOPHISTICATION
PROVIDES
IMPROVED
ROBUSTNESS
SECTION
TO
SUPPORT
THIS
ANALYTICALLY
WE
PRO
VIDE
WORST
CASE
GUARANTEES
ON
THE
NEAR
OPTIMAL
STOCHASTIC
SPEED
SCALERS
FOR
PS
AND
SRPT
COROLLARY
NOTE
THAT
IT
IS
RARE
TO
BE
ABLE
TO
PROVIDE
SUCH
GUARANTEES
FOR
STOCHASTIC
CONTROL
POLICIES
THE
INSIGHTS
RELATED
TO
ISSUE
III
HAVE
AN
INTERESTING
PRACTICAL
IMPLICATION
INSTEAD
OF
DESIGNING
OPTI
MAL
SPEEDS
IT
MAY
BE
BETTER
TO
DESIGN
OPTIMALLY
ROBUST
SPEEDS
SINCE
THE
MAIN
FUNCTION
OF
DYNAMIC
SPEED
SCALING
IS
TO
PROVIDE
ROBUSTNESS
THIS
REPRESENTS
A
SIGNIFICANT
SHIFT
IN
APPROACH
FOR
STOCHASTIC
SPEED
SCALING
DESIGN
WITH
RESPECT
TO
ISSUE
IV
OUR
RESULTS
UNCOVER
ONE
UNIN
TENDED
DRAWBACK
OF
DYNAMIC
SPEED
SCALING
SPEED
SCALING
CAN
MAGNIFY
UNFAIRNESS
UNFAIRNESS
IN
SPEED
SCALING
DESIGNS
HAS
NOT
BEEN
IDENTIFIED
PREVIOUSLY
BUT
IN
RETROSPECT
THE
INTUITION
BEHIND
IT
IS
CLEAR
IF
A
JOB
SIZE
IS
CORRELATED
WITH
THE
OC
CUPANCY
OF
THE
SYSTEM
WHILE
IT
IS
IN
SERVICE
THEN
DYNAMIC
SPEED
SCALING
WILL
LEAD
TO
DIFFERENTIAL
SERVICE
RATES
ACROSS
JOB
SIZES
AND
THUS
UNFAIRNESS
WE
PROVE
THAT
SPEED
SCALING
MAG
NIFIES
UNFAIRNESS
UNDER
SRPT
THEOREM
AND
ALL
NON
PREEMPTIVE
POLICIES
E
G
FCFS
PROPOSITION
IN
CONTRAST
PS
IS
FAIR
EVEN
WITH
DYNAMIC
SPEED
SCALING
PROPOSITION
COMBINING
THESE
RESULTS
WITH
OUR
INSIGHTS
RELATED
TO
ISSUE
II
WE
SEE
THAT
DESIGNERS
CAN
DECOUPLE
THE
SCHEDULER
AND
THE
SPEED
SCALER
WHEN
CONSIDERING
PERFORMANCE
BUT
SHOULD
BE
WARY
ABOUT
THE
INTERACTION
WHEN
CONSIDERING
FAIRNESS
OUR
RESULTS
HIGHLIGHT
THE
BALANCING
ACT
A
SPEED
SCALING
AL
GORITHM
MUST
PERFORM
IN
ORDER
TO
ACHIEVE
THE
THREE
DESIRABLE
PROPERTIES
WE
HAVE
DISCUSSED
NEAR
OPTIMAL
PERFORMANCE
RO
BUSTNESS
AND
FAIRNESS
IT
IS
POSSIBLE
TO
BE
NEAR
OPTIMAL
AND
ROBUST
USING
SRPT
SCHEDULING
AND
DYNAMIC
SPEED
SCALING
BUT
THIS
CREATES
UNFAIRNESS
SRPT
CAN
BE
FAIR
AND
STILL
NEAR
OPTIMAL
IF
GATED
STATIC
SPEED
SCALING
IS
USED
BUT
THIS
IS
NOT
ROBUST
ON
THE
OTHER
HAND
DYNAMIC
SPEED
SCALING
WITH
PS
CAN
BE
FAIR
AND
ROBUST
BUT
IN
THE
WORST
CASE
PAYS
A
SIGNIF
ICANT
PERFORMANCE
PENALTY
THOUGH
IN
STOCHASTIC
SETTINGS
IS
NEAR
OPTIMAL
THUS
THE
POLICIES
CONSIDERED
IN
THIS
PAPER
CAN
ACHIEVE
ANY
TWO
OF
NEAR
OPTIMAL
FAIR
AND
ROBUST
BUT
NOT
ALL
THREE
FINALLY
IT
IS
IMPORTANT
TO
NOTE
THAT
THE
ANALYTIC
APPROACH
OF
THIS
PAPER
IS
DISTINCTIVE
IT
IS
UNUSUAL
TO
TREAT
BOTH
STOCHASTIC
AND
WORST
CASE
MODELS
IN
ONE
PAPER
AND
FURTHER
MANY
RESULTS
DEPEND
ON
A
COMBINATION
OF
WORST
CASE
AND
STOCHASTIC
TECHNIQUES
WHICH
LEADS
TO
INSIGHTS
THAT
COULD
NOT
HAVE
BEEN
ATTAINED
BY
FOCUSING
ON
ONE
MODEL
ALONE
MODEL
AND
NOTATION
WE
CONSIDER
THE
JOINT
PROBLEM
OF
SPEED
SCALING
AND
SCHEDULING
IN
A
SINGLE
SERVER
QUEUE
TO
MINIMIZE
A
LINEAR
COM
BINATION
OF
EXPECTED
RESPONSE
TIME
ALSO
CALLED
SOJOURN
TIME
OR
FLOW
TIME
DENOTED
BY
T
AND
ENERGY
USAGE
PER
JOB
DE
NOTED
E
Z
E
T
E
E
Β
BY
LITTLE
LAW
THIS
MAY
BE
MORE
CONVENIENTLY
EXPRESSED
AS
ΛZ
E
N
E
P
Β
WHERE
N
IS
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
AND
P
Λ
IS
THE
POWER
EXPENDED
BEFORE
DEFINING
THE
SPEED
SCALING
ALGORITHMS
WE
NEED
SOME
NOTATION
LET
N
T
BE
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
AT
TIME
T
AND
T
BE
THE
SPEED
THAT
THE
SYSTEM
IS
RUNNING
AT
AT
TIME
T
FURTHER
DEFINE
P
AS
THE
POWER
NEEDED
TO
RUN
AT
SPEED
THEN
THE
ENERGY
USED
BY
TIME
T
IS
T
MEASUREMENTS
HAVE
SHOWN
THAT
P
CAN
TAKE
ON
A
VARIETY
OF
FORMS
DEPENDING
ON
THE
SYSTEM
BEING
STUDIED
HOWEVER
IN
MANY
APPLICATIONS
A
LOW
ORDER
POLYNOMIAL
FORM
PROVIDES
A
GOOD
APPROXIMATION
I
E
P
KSΑ
WITH
Α
FOR
EXAMPLE
FOR
DYNAMIC
POWER
IN
CMOS
CHIPS
Α
IS
A
GOOD
APPROXIMATION
HOWEVER
THIS
POLYNOMIAL
FORM
IS
NOT
ALWAYS
APPROPRIATE
WIRELESS
AND
OTHER
COMMUNICATION
OVER
AN
ADDITIVE
WHITE
GAUSSIAN
NOISE
CHANNEL
HAVE
AN
EXPO
NENTIAL
POWER
FUNCTION
WHILE
INTERFERENCE
LIMITED
COM
MUNICATIONS
HAS
UNBOUNDED
POWER
AT
FINITE
RATE
SOME
OF
OUR
RESULTS
ASSUME
A
POLYNOMIAL
FORM
TO
MAKE
THE
ANALYSIS
TRACTABLE
AND
PARTICULARLY
Α
PROVIDES
A
SIMPLE
EXAMPLE
WHICH
WE
USE
FOR
MANY
OF
OUR
NUMERICAL
EXPERIMENTS
OTHER
RESULTS
HOLD
FOR
GENERAL
EVEN
NON
CONVEX
AND
DISCONTINUOUS
POWER
FUNCTIONS
ADDITIONALLY
WE
OCCASIONALLY
LIMIT
OUR
RE
SULTS
TO
REGULAR
POWER
FUNCTIONS
WHICH
ARE
DIFFERENTIABLE
ON
STRICTLY
CONCAVE
NON
NEGATIVE
AND
AT
SPEED
TABLE
SUMMARY
OF
THE
SPEED
SCALING
SCHEMES
IN
THIS
PAPER
NOW
WE
CAN
DEFINE
A
SPEED
SCALING
ALGORITHM
A
SPEED
SCALING
ALGORITHM
Π
Σ
IS
A
PAIR
OF
A
SCHEDULING
DIS
CIPLINE
Π
THAT
DEFINES
THE
ORDER
IN
WHICH
JOBS
ARE
PROCESSED
I
UNDER
A
GIVEN
ALGORITHM
A
AS
ZA
I
TJ
E
I
AND
A
SPEED
SCALING
RULE
Σ
THAT
DEFINES
THE
SPEED
AS
A
FUNC
TION
OF
SYSTEM
STATE
IN
TERMS
OF
THE
POWER
FUNCTION
P
IN
THIS
PAPER
WE
CONSIDER
SPEED
SCALING
RULES
WHERE
THE
SPEED
IS
A
FUNCTION
OF
THE
NUMBER
OF
JOBS
IN
THE
SYSTEM
I
E
SN
IS
THE
SPEED
WHEN
THE
OCCUPANCY
IS
N
THE
SCHEDULING
ALGORITHMS
Π
WE
CONSIDER
ARE
ONLINE
AND
SO
ARE
NOT
AWARE
OF
A
JOB
J
UNTIL
IT
ARRIVES
AT
TIME
R
J
AT
WHICH
POINT
Π
LEARNS
THE
SIZE
OF
THE
JOB
XJ
WE
CON
SIDER
A
PREEMPT
RESUME
MODEL
THAT
IS
THE
SCHEDULER
MAY
PREEMPT
A
JOB
AND
LATER
RESTART
IT
FROM
THE
POINT
IT
WAS
IN
TERRUPTED
WITHOUT
ANY
OVERHEAD
THE
POLICIES
THAT
WE
FOCUS
ON
ARE
SHORTEST
REMAINING
PROCESSING
TIME
SRPT
WHICH
PREEMPTIVELY
SERVES
THE
JOB
WITH
THE
LEAST
REMAINING
WORK
PROCESSOR
SHARING
PS
WHICH
SHARES
THE
SERVICE
RATE
EVENLY
AMONG
THE
JOBS
IN
THE
SYSTEM
AT
ALL
TIMES
AND
FIRST
COME
FIRST
SERVED
FCFS
WHICH
SERVES
JOBS
IN
ORDER
OF
ARRIVAL
THE
SPEED
SCALING
RULES
SN
WE
CONSIDER
CAN
BE
GATED
STATIC
WHICH
RUNS
AT
A
CONSTANT
SPEED
WHILE
THE
SYSTEM
IS
NON
IDLE
AND
SLEEPS
WHILE
THE
SYSTEM
IS
IDLE
I
E
SN
OR
MORE
GENERALLY
DYNAMIC
SN
G
N
FOR
SOME
FUNC
TION
G
N
NOTE
THAT
THE
SPEED
IS
SIMPLY
THE
RATE
AT
WHICH
WORK
IS
COMPLETED
I
E
A
JOB
OF
SIZE
X
SERVED
AT
SPEED
WILL
COMPLETE
IN
TIME
X
TO
AVOID
CONFUSION
WE
OCCASIONALLY
WRITE
SΠ
AS
THE
SPEED
UNDER
POLICY
Π
WHEN
THE
OCCUPANCY
IS
N
WE
ANALYZE
THE
PERFORMANCE
OF
SPEED
SCALING
ALGORITHMS
IN
TWO
DIFFERENT
MODELS
ONE
WORST
CASE
AND
ONE
STOCHASTIC
NOTATION
FOR
THE
WORST
CASE
MODEL
IN
THE
WORST
CASE
MODEL
WE
CONSIDER
FINITE
ARBITRARY
MAYBE
ADVERSARIAL
INSTANCES
OF
ARRIVING
JOBS
A
PROBLEM
INSTANCE
CONSISTS
OF
Ν
JOBS
WITH
THE
JTH
JOB
HAVING
ARRIVAL
TIME
RE
LEASE
TIME
R
J
AND
SIZE
WORK
XJ
OUR
OBJECTIVE
IS
AGAIN
A
LINEAR
COMBINATION
OF
RESPONSE
TIME
AND
ENERGY
USAGE
LET
I
BE
THE
TOTAL
ENERGY
USED
TO
COMPLETE
INSTANCE
I
AND
TJ
BE
THE
RESPONSE
TIME
OF
JOB
J
THE
COMPLETION
TIME
MINUS
THE
RELEASE
TIME
THE
ANALOG
OF
IS
TO
REPLACE
THE
ENSEMBLE
AVERAGE
BY
THE
SAMPLE
AVERAGE
GIVING
THE
COST
OF
AN
INSTANCE
THAT
FOR
SOME
OTHER
OBJECTIVES
IT
IS
BETTER
TO
BASE
THE
SPEED
ON
THE
UNFINISHED
WORK
INSTEAD
Ν
J
Β
IN
THIS
MODEL
WE
COMPARE
THE
COST
OF
SPEED
SCALING
ALGO
RITHMS
TO
THE
COST
OF
THE
OPTIMAL
OFFLINE
ALGORITHM
OPT
IN
PARTICULAR
WE
STUDY
THE
COMPETITIVE
RATIO
DEFINED
AS
CR
SUP
ZA
I
ZO
I
I
WHERE
ZO
I
IS
THE
OPTIMAL
COST
ACHIEVABLE
ON
I
NOTATION
FOR
THE
STOCHASTIC
MODEL
IN
THE
STOCHASTIC
MODEL
WE
CONSIDER
AN
M
GI
OR
SOME
TIMES
GI
GI
QUEUE
WITH
ARRIVAL
RATE
Λ
LET
X
DENOTE
A
RANDOM
JOB
SIZE
WITH
C
D
F
F
X
C
C
D
F
F
X
AND
CONTINU
OUS
P
D
F
F
X
LET
Ρ
ΛE
X
DENOTE
THE
LOAD
OF
ARRIVING
JOBS
NOTE
THAT
Ρ
IS
NOT
THE
UTILIZATION
OF
THE
SYSTEM
AND
THAT
MANY
DYNAMIC
SPEED
SCALING
ALGORITHMS
ARE
STABLE
FOR
ALL
Ρ
WHEN
THE
POWER
FUNCTION
IS
P
SΑ
IT
IS
NATURAL
TO
USE
A
SCALED
LOAD
Γ
Ρ
ΒΑ
WHICH
JOINTLY
CHARACTERIZES
THE
IMPACT
OF
Ρ
AND
Β
SEE
DENOTE
THE
RESPONSE
TIME
OF
A
JOB
OF
SIZE
X
BY
T
X
WE
CONSIDER
THE
PERFORMANCE
METRIC
WHERE
THE
EXPECTATIONS
ARE
AVERAGES
PER
JOB
IN
THIS
MODEL
THE
GOAL
IS
TO
OPTIMIZE
THIS
COST
FOR
A
SPECIFIC
WORKLOAD
Ρ
DEFINE
THE
COMPETITIVE
RATIO
IN
THE
M
GI
MODEL
AS
CR
SUP
ZA
ZO
F
Λ
WHERE
ZO
IS
THE
AVERAGE
COST
OF
THE
OPTIMAL
OFFLINE
ALGORITHM
DYNAMIC
SPEED
SCALING
WE
START
BY
STUDYING
THE
MOST
SOPHISTICATED
SPEED
SCALING
ALGORITHMS
THOSE
THAT
DYNAMICALLY
ADJUST
THE
SPEED
AS
A
FUNCTION
OF
THE
QUEUE
LENGTH
IN
THIS
SECTION
WE
INVESTIGATE
THE
STRUCTURE
OF
THE
OPTIMAL
SPEED
SCALING
ALGORITHM
IN
TWO
WAYS
I
WE
STUDY
NEAR
OPTIMAL
SPEED
SCALING
RULES
IN
THE
CASE
OF
BOTH
SRPT
AND
PS
SCHEDULING
II
WE
STUDY
EACH
OF
THESE
ALGORITHMS
IN
BOTH
THE
WORST
CASE
MODEL
AND
THE
STOCHASTIC
MODEL
WORST
CASE
ANALYSIS
THERE
HAS
BEEN
SIGNIFICANT
WORK
STUDYING
SPEED
SCALING
IN
THE
WORST
CASE
MODEL
FOLLOWING
YAO
ET
AL
SEMINAL
PA
PER
MOST
OF
IT
FOCUSING
ON
SRPT
A
PROMISING
ALGORITHM
THAT
HAS
EMERGED
IS
SRPT
P
N
AND
THERE
HAS
BEEN
A
SIGNIFICANT
STREAM
OF
PAPERS
PROVIDING
UPPER
BOUNDS
ON
THE
COMPETITIVE
RATIO
OF
THIS
ALGORITHM
FOR
OBJECTIVE
FOR
UNIT
SIZE
JOBS
IN
AND
FOR
GENERAL
JOBS
WITH
P
SΑ
IN
A
MAJOR
BREAKTHROUGH
WAS
MADE
IN
WHICH
SHOWS
THE
COMPETITIVENESS
OF
SRP
T
P
N
FOR
GENERAL
P
OUR
CONTRIBUTION
TO
THIS
LITERATURE
IS
TWOFOLD
FIRST
WE
TIGHTLY
CHARACTERIZE
THE
COMPETITIVE
RATIO
OF
SRPT
P
NΒ
SPECIFICALLY
WE
PROVE
THAT
SRPT
P
NΒ
IS
EXACTLY
COMPETITIVE
UNDER
GENERAL
POWER
FUNCTIONS
SEE
THEOREM
AND
COROLLARY
SECOND
WE
PROVE
THAT
NO
NATURAL
SPEED
SCALING
ALGORITHM
CAN
BE
BETTER
THAN
COMPETITIVE
NATURAL
SPEED
SCALING
ALGORITHMS
INCLUDE
AL
GORITHMS
WHICH
HAVE
SPEEDS
THAT
GROW
FASTER
SLOWER
OR
PRO
PORTIONAL
TO
P
NΒ
OR
THAT
USE
A
SCHEDULER
THAT
WORKS
ON
EXACTLY
ONE
JOB
BETWEEN
ARRIVAL
DEPARTURE
EVENTS
SEE
DEFINI
TION
THUS
THE
CLASS
OF
NATURAL
ALGORITHMS
INCLUDES
ENERGY
PROPORTIONAL
DESIGNS
FOR
ALL
SCHEDULERS
AND
SRPT
SCHEDULING
FOR
ANY
SN
WE
CONJECTURE
THAT
THIS
RESULT
CAN
BE
EXTENDED
TIABLE
IT
DOES
NOT
INCREASE
RUNNING
CONDITION
WHEN
Φ
IS
DIFFERENTIABLE
ZA
T
DΦ
CZO
T
DT
WHERE
ZA
T
AND
ZO
T
ARE
THE
COST
Z
T
UNDER
AND
OP
T
RESPECTIVELY
GIVEN
THESE
CONDITIONS
THE
COMPETITIVENESS
FOLLOWS
FROM
IN
TEGRATING
WHICH
GIVES
ZA
ZA
Φ
Φ
CZO
SRPT
ANALYSIS
WE
NOW
STATE
AND
PROVE
OUR
RESULTS
FOR
SRPT
THEOREM
FOR
ANY
REGULAR
POWER
FUNCTION
P
SRPT
P
NΒ
HAS
A
COMPETITIVE
RATIO
OF
EXACTLY
THE
PROOF
OF
THE
UPPER
BOUND
IS
A
MODIFICATION
OF
THE
ANAL
YSIS
IN
THAT
ACCOUNTS
MORE
CAREFULLY
FOR
SOME
BOUNDARY
CASES
IT
USES
THE
POTENTIAL
FUNCTION
N
Q
T
IN
CONTRAST
TO
THIS
STREAM
OF
WORK
STUDYING
SRPT
THERE
HAS
BEEN
NO
ANALYSIS
OF
SPEED
SCALING
UNDER
PS
WE
PROVE
THAT
PS
P
NΒ
IS
O
COMPETITIVE
AND
IN
PARTICULAR
IS
COMPETITIVE
FOR
TYPICAL
Α
I
E
Α
THIS
BUILDS
ON
WHICH
STUDIES
LAPS
ANOTHER
POLICY
BLIND
TO
JOB
SIZES
LAPS
P
NΒ
IS
ALSO
O
COMPETITIVE
FOR
P
SΑ
WITH
FIXED
Α
HOWEVER
FOR
BOTH
PS
AND
LAPS
THE
COMPETITIVE
RATIO
IS
UNBOUNDED
FOR
LARGE
Α
WHICH
PROVES
HOLDS
FOR
ALL
BLIND
POLICIES
BUT
NOTE
THAT
Α
IN
MOST
COMPUTER
SYSTEMS
TODAY
E
G
DISKS
CHIPS
AND
SERVERS
THUS
ASYMPTOTICS
IN
Α
ARE
LESS
IMPORTANT
THAN
THE
PERFOR
MANCE
FOR
SMALL
Α
THE
RESULTS
IN
THIS
SECTION
HIGHLIGHT
IMPORTANT
INSIGHTS
ABOUT
FUNDAMENTAL
ISSUES
IN
SPEED
SCALING
DESIGN
FIRST
THE
COMPETITIVE
RATIO
RESULTS
HIGHLIGHT
THAT
ENERGY
PROPORTIONAL
SPEED
SCALING
P
SN
NΒ
IS
NEARLY
OPTIMAL
WHICH
PROVIDES
ANALYTIC
JUSTIFICATION
OF
A
COMMON
DESIGN
HEURISTIC
E
G
SECOND
NOTE
THAT
ENERGY
PROPORTIONAL
SPEED
SCALING
WORKS
WELL
FOR
PS
AND
SRPT
AND
LAPS
THIS
SUGGESTS
A
DESIGNER
MAY
DECOUPLE
THE
CHOICE
OF
A
SPEED
SCALER
FROM
THE
CHOICE
OF
A
SCHEDULER
CHOICES
THAT
INITIALLY
SEEM
VERY
INTERTWINED
THOUGH
WE
HAVE
SEEN
THIS
DECOUPLING
ONLY
FOR
PS
SRPT
AND
LAPS
WE
CONJECTURE
THAT
IT
HOLDS
MORE
GENERALLY
THIRD
SCHEDULING
SEEMS
MUCH
LESS
IMPORTANT
IN
THE
SPEED
SCALING
MODEL
THAN
IN
THE
STANDARD
CONSTANT
SPEED
MODEL
FOR
AN
INSTANCE
OF
Ν
JOBS
PS
IS
Ω
COMPETITIVE
FOR
MEAN
RE
SPONSE
TIME
IN
THE
CONSTANT
SPEED
MODEL
BUT
IS
O
COMPETITIVE
IN
THE
SPEED
SCALING
MODEL
AGAIN
WE
CONJECTURE
THAT
THIS
HOLDS
MORE
GENERALLY
THAN
FOR
JUST
PS
AMORTIZED
COMPETITIVE
ANALYSIS
THE
PROOFS
OF
THE
RESULTS
DESCRIBED
ABOVE
USE
A
TECHNIQUE
TERMED
AMORTIZED
LOCAL
COMPETITIVE
ANALYSIS
THE
TECHNIQUE
WORKS
AS
FOLLOWS
TO
SHOW
THAT
AN
ALGORITHM
IS
C
COMPETITIVE
WITH
AN
OP
TIMAL
ALGORITHM
OP
T
FOR
A
PERFORMANCE
METRIC
Z
Z
T
DT
IT
IS
SUFFICIENT
TO
FIND
A
POTENTIAL
FUNCTION
Φ
R
R
SUCH
THAT
FOR
ANY
INSTANCE
OF
THE
PROBLEM
BOUNDARY
CONDITION
Φ
BEFORE
THE
FIRST
JOB
IS
RE
LEASED
AND
Φ
AFTER
THE
LAST
JOB
IS
FINISHED
JUMP
CONDITION
AT
ANY
POINT
WHERE
Φ
IS
NOT
DIFFEREN
FOR
SOME
NON
DECREASING
WITH
I
FOR
I
WHERE
N
Q
T
MAX
NA
Q
T
NO
Q
T
WITH
NA
Q
T
AND
NO
Q
T
THE
NUMBER
OF
UNFINISHED
JOBS
AT
TIME
T
WITH
REMAINING
SIZE
AT
LEAST
Q
UNDER
THE
SCHEME
UNDER
INVESTIGATION
AND
THE
OPTIMAL
OFFLINE
SCHEME
RESPECTIVELY
THE
FOLLOWING
TECHNICAL
LEMMA
IS
THE
KEY
STEP
OF
THE
PROOF
AND
IS
PROVEN
IN
APPENDIX
A
LEMMA
LET
Η
AND
Φ
BE
GIVEN
BY
WITH
I
Η
P
P
IΒ
LET
SRP
T
SN
WITH
SN
P
NΒ
P
ΗNΒ
THEN
AT
POINTS
WHERE
Φ
IS
DIFFERENTIABLE
NA
P
SA
Β
DΦ
Η
NO
P
SO
Β
DT
USING
THE
ABOVE
LEMMA
WE
CAN
NOW
PROVE
THEOREM
PROOF
OF
THEOREM
TO
SHOW
THAT
THE
COMPETITIVE
RA
TIO
OF
SRPT
P
NΒ
IS
AT
MOST
WE
SHOW
THAT
Φ
GIVEN
BY
AND
IS
A
VALID
POTENTIAL
FUNCTION
THE
BOUNDARY
CONDITIONS
ARE
SATISFIED
SINCE
Φ
WHEN
THERE
ARE
NO
JOBS
IN
THE
SYSTEM
ALSO
Φ
IS
DIFFERENTIABLE
EXCEPT
WHEN
A
JOB
ARRIVES
OR
DEPARTS
WHEN
A
JOB
ARRIVES
THE
CHANGE
IN
NA
Q
EQUALS
THAT
IN
NO
Q
FOR
ALL
Q
AND
SO
Φ
IS
UNCHANGED
WHEN
A
JOB
IS
COMPLETED
N
Q
IS
UNCHANGED
FOR
ALL
Q
AND
SO
Φ
IS
AGAIN
UNCHANGED
THE
RUNNING
CONDITION
IS
ESTABLISHED
BY
LEMMA
WITH
Η
TO
PROVE
THE
LOWER
BOUND
ON
THE
COMPETITIVE
RATIO
CON
SIDER
PERIODIC
UNIT
WORK
ARRIVALS
AT
RATE
Λ
SN
FOR
SOME
N
AS
THE
NUMBER
OF
JOBS
THAT
ARRIVE
GROWS
LARGE
THE
OPTIMAL
SCHEDULE
RUNS
AT
RATE
Λ
AND
MAINTAINS
A
QUEUE
OF
AT
MOST
ONE
PACKET
THE
ONE
IN
SERVICE
GIVING
A
COST
PER
JOB
OF
AT
MOST
P
Λ
Β
Λ
IN
ORDER
TO
RUN
AT
SPEED
Λ
THE
SCHEDULE
SRPT
P
NΒ
REQUIRES
N
P
Λ
Β
JOBS
IN
THE
QUEUE
GIVING
A
COST
PER
JOB
OF
P
Λ
P
Λ
ΛΒ
THE
COMPETITIVE
RATIO
IS
THUS
AT
LEAST
Λ
AS
Λ
BECOMES
LARGE
THIS
TENDS
TO
SINCE
A
REGULAR
P
IS
UNBOUNDED
THEOREM
CAN
EASILY
BE
EXTENDED
TO
NON
NEGATIVE
POWER
FUNCTIONS
BY
APPLYING
THE
SAME
ARGUMENT
AS
USED
IN
COROLLARY
LET
Ε
FOR
ANY
NON
NEGATIVE
AND
UN
BOUNDED
P
THERE
EXISTS
A
P
SUCH
THAT
EMULATING
SRPT
P
NΒ
YIELDS
A
Ε
COMPETITIVE
ALGORITHM
THIS
EMULATION
INVOLVES
AVOIDING
SPEEDS
WHERE
P
IS
NOT
CONVEX
INSTEAD
EMULATING
SUCH
SPEEDS
BY
SWITCHING
BETWEEN
A
HIGHER
AND
LOWER
SPEED
ON
THE
CONVEX
HULL
OF
P
COROLLARY
SHOWS
THAT
SRPT
P
NΒ
DOES
NOT
MATCH
THE
PERFORMANCE
OF
THE
OFFLINE
OPTIMAL
THIS
MOTIVATES
CON
SIDERING
OTHER
ALGORITHMS
HOWEVER
WE
NOW
SHOW
THAT
NO
NATURAL
ALGORITHM
CAN
DO
BETTER
CONSIDER
A
TYPE
B
NATURAL
ON
IR
N
SN
ALSO
SAT
ISFIES
LET
TO
DENOTE
THE
TIME
AVERAGE
SPEED
FOR
ALL
Φ
FOR
SUFFICIENTLY
LONG
INSTANCES
WE
NEED
ΦSN
TO
PREVENT
AN
UNBOUNDED
QUEUE
FORMING
BY
JENSEN
INEQUAL
ITY
THE
AVERAGE
COST
PER
JOB
SATISFIES
Z
G
P
Β
G
ΦSN
P
ΦSN
Β
SINCE
Φ
CAN
BE
ARBITRARILY
CLOSE
TO
THE
COST
CAN
BE
ARBITRARILY
CLOSE
TO
N
P
SN
Β
WHENCE
HOLDS
FOR
A
TYPE
C
NATURAL
A
P
SN
N
FOR
LARGE
N
THUS
FOR
TYPES
A
C
SUCH
THAT
FOR
ALL
N
DEFINITION
A
SPEED
SCALING
ALGORITHM
A
IS
NATURAL
IF
P
SN
X
IT
RUNS
AT
SPEED
SN
WHEN
IT
HAS
N
UNFINISHED
JOBS
AND
FOR
N
NΒ
Ε
CONVEX
P
ONE
OF
THE
FOLLOWING
HOLDS
THE
SCHEDULER
IS
WORK
CONSERVING
AND
WORKS
ON
A
SINGLE
JOB
BETWEEN
ARRIVAL
DEPARTURE
EVENTS
OR
G
P
Β
IS
CONVEX
FOR
SOME
G
WITH
G
SN
N
OR
THE
SPEEDS
SN
SATISFY
P
SN
Ω
N
OR
THE
SPEEDS
SN
SATISFY
P
SN
O
N
WE
NOW
SHOW
THAT
THIS
CONDITION
PRECLUDES
HAVING
A
COM
PETITIVE
RATIO
OF
Ε
IN
THE
CASE
OF
BATCH
ARRIVALS
IB
Ν
FOR
IB
Ν
THE
COST
OF
ANY
ALGORITHM
WHICH
DOES
NOT
SERVE
THE
JOBS
ONE
AT
A
TIME
CAN
BE
LOWERED
BY
REASSIGNING
THE
SERVICE
INSTANTS
SO
THAT
IT
DOES
SO
WITHOUT
LOSS
OF
GENERALITY
Ν
NOTE
THAT
NATURAL
ALGORITHMS
INCLUDE
ALL
ALGORITHMS
THAT
USE
THE
OPTIMAL
SCHEDULER
SRPT
AND
ALL
ALGORITHMS
WHOSE
Z
I
B
Ν
N
N
P
SN
ΒSN
SPEEDS
GROW
FASTER
THAN
SLOWER
THAN
OR
PROPORTIONAL
TO
N
Α
Α
F
NΒ
Α
SΑ
Α
P
N
THEOREM
FOR
ANY
Ε
THERE
IS
A
REGULAR
POWER
FUNC
N
Α
Α
NΒ
TION
PΕ
SUCH
THAT
ANY
NATURAL
ALGORITHM
A
ON
PΕ
HAS
COM
PETITIVE
RATIO
LARGER
THAN
Ε
THE
UNIQUE
LOCAL
MINIMUM
OF
Α
Α
Α
OCCURS
AT
Α
THIS
GIVES
A
MINIMUM
COST
OF
THIS
THEOREM
HIGHLIGHTS
THAT
IF
AN
ALGORITHM
DOES
HAVE
A
SMALLER
COMPETITIVE
RATIO
THAN
SRPT
P
NΒ
IT
WILL
NOT
ZO
IB
Ν
Ν
N
N
Α
Α
USE
NATURAL
SCHEDULING
OR
SPEED
SCALING
THOUGH
THE
RESULT
ONLY
APPLIES
TO
NATURAL
ALGORITHMS
WE
CONJECTURE
THAT
IN
FACT
IT
HOLDS
FOR
ALL
SPEED
SCALING
ALGORITHMS
AND
THUS
THE
COMPETITIVE
RATIO
OF
SRPT
P
NΒ
IS
MINIMAL
PROOF
CONSIDER
THE
CASE
WHEN
P
SΑ
WITH
Α
YET
TO
BE
DETERMINED
WE
SHOW
THAT
FOR
LARGE
Α
THE
COMPETITIVE
RATIO
IS
AT
LEAST
Ε
BY
CONSIDERING
TWO
CASES
INSTANCE
IB
Ν
IS
A
BATCH
ARRIVAL
OF
Ν
JOBS
OF
SIZE
AT
TIME
WITH
NO
FUTURE
Α
Α
Α
Α
FOR
SN
NΒ
Α
Α
MORE
GENERALLY
THE
OPTIMUM
IS
ΒN
SNP
SN
P
SN
MOREOVER
FOR
Α
Ε
THE
MINIMUM
SUBJECT
TO
OCCURS
WHEN
XN
Ε
HENCE
THE
COMPETITIVE
RATIO
FOR
THE
BATCH
CASE
SUBJECT
TO
SATISFIES
ARRIVALS
AND
INSTANCE
IR
B
Λ
IS
A
BATCH
OF
B
JOBS
AT
TIME
FOLLOWED
BY
A
LONG
TRAIN
OF
PERIODIC
ARRIVALS
OF
JOBS
OF
SIZE
AT
TIMES
K
Λ
FOR
K
N
CRBATCH
Ν
N
Ν
N
N
Α
Α
Α
Α
Α
N
Α
Α
Α
FIX
AN
Ε
AND
CONSIDER
A
SPEED
SCALING
WHICH
CAN
ATTAIN
A
COMPETITIVE
RATIO
OF
Ε
FOR
ALL
INSTANCES
IR
FOR
IR
Λ
WITH
LARGE
Λ
THE
OPTIMAL
ALGORITHM
WILL
RUN
AT
SPEED
EXCEEDING
Λ
FOR
A
FINITE
TIME
UNTIL
THE
OCCUPANCY
IS
ONE
AFTER
Α
Α
Ε
Α
Ε
THAT
IT
WILL
RUN
AT
SPEED
Λ
SO
THAT
NO
QUEUE
FORMS
FOR
LONG
TRAINS
THIS
LEADS
TO
A
COST
PER
JOB
OF
P
Λ
Β
Λ
FIRST
CONSIDER
A
TYPE
D
NATURAL
FOR
SUFFICIENTLY
LARGE
Λ
N
KSΑ
FOR
ALL
SN
Λ
WHERE
K
Β
BETWEEN
ARRIVALS
AT
LEAST
UNIT
OF
WORK
MUST
BE
DONE
AT
SPEED
AT
LEAST
Λ
IN
ORDER
FOR
NOT
TO
FALL
BEHIND
THE
COST
PER
UNIT
WORK
IS
AT
LEAST
KSΑ
SΑ
Β
AND
SO
THE
TOTAL
COST
OF
PERFORMING
THIS
UNIT
IS
AT
LEAST
K
Β
ΛΑ
Β
FOR
LARGE
Λ
THIS
IS
AT
LEAST
TWICE
THE
COST
PER
JOB
UNDER
THE
OPTIMAL
SCHEME
P
Λ
Β
Λ
Β
IT
REMAINS
TO
CONSIDER
NATURAL
ALGORITHMS
OF
TYPES
A
C
CONSIDER
A
TYPE
A
NATURAL
ON
THE
INSTANCE
IR
N
SN
FOR
SOME
N
IT
WILL
INITIALLY
PROCESS
EXACTLY
ONE
JOB
AT
SPEED
SN
WHICH
IT
WILL
FINISH
AT
TIME
SN
FROM
THIS
TIME
A
NEW
ARRIVAL
WILL
OCCUR
WHENEVER
A
JOB
COMPLETES
AND
SO
THE
ALGO
RITHM
RUNS
AT
SPEED
SN
WITH
OCCUPANCY
N
UNTIL
THE
LAST
AR
RIVAL
SO
THE
AVERAGE
COST
PER
JOB
TENDS
TO
N
P
SN
Β
SN
ON
LARGE
INSTANCES
LEADING
TO
A
COMPETITIVE
RATIO
OF
N
FOR
ANY
Ε
THE
PRODUCT
OF
THE
LAST
TWO
FACTORS
TENDS
TO
Ε
AS
Α
AND
HENCE
THERE
IS
AN
Α
Α
Ε
FOR
WHICH
THEIR
PRODUCT
EXCEEDS
Ε
SIMILARLY
FOR
ALL
Α
THERE
IS
A
SUFFICIENTLY
LARGE
Ν
THAT
THE
FIRST
FACTOR
EXCEEDS
Ε
FOR
THIS
Α
AND
Ν
CRBATCH
SO
FOR
P
SΑ
Ε
IF
THE
COMPETITIVE
RATIO
IS
SMALLER
THAN
Ε
IN
THE
PERIODIC
CASE
IT
MUST
BE
LARGER
THAN
IN
THE
BATCH
CASE
THEOREM
IS
PESSIMISTIC
BUT
NOTE
THAT
THE
PROOF
FOCUSES
ON
P
SΑ
FOR
LARGE
Α
THUS
IT
IS
LIKELY
POSSIBLE
TO
DESIGN
NATURAL
ALGORITHMS
THAT
CAN
OUTPERFORM
SRPT
P
NΒ
FOR
Α
WHICH
IS
TYPICAL
FOR
COMPUTER
SYSTEMS
TODAY
THIS
IS
AN
INTERESTING
TOPIC
FOR
FUTURE
RESEARCH
PS
ANALYSIS
WE
NOW
STATE
AND
PROVE
OUR
BOUND
ON
THE
COMPETITIVE
RATIO
OF
PS
THEOREM
IF
P
SΑ
THEN
PS
P
NΒ
IS
P
SN
Β
CRPERIODIC
Ε
MAX
Α
Α
COMPETITIVE
IN
PARTICULAR
PS
IS
COMPETITIVE
FOR
Α
IN
THE
TYPICAL
RANGE
OF
FOR
GENERAL
Α
THEY
THEOREM
IS
PROVEN
USING
AMORTIZED
LOCAL
COMPETITIVE
Α
SN
Α
MIN
N
ΣΑ
ΓΑ
Γ
Α
NESS
LET
Η
AND
Γ
Η
Β
FUNCTION
IS
THEN
DEFINED
AS
THE
POTENTIAL
Β
SN
Α
Σ
Γ
N
Α
Σ
Γ
Σ
Γ
NA
T
Α
Α
Φ
Γ
Α
MAX
QA
JI
T
QO
JI
T
I
WHERE
QΠ
J
T
IS
THE
REMAINING
WORK
ON
JOB
J
AT
TIME
T
UNDER
SCHEME
Π
AND
J
NA
T
IS
AN
ORDERING
OF
THE
JOBS
IN
INCREAS
PROOF
BOUNDS
AND
ARE
SHOWN
IN
ADDI
TIONALLY
THE
CONCAVITY
OF
SN
FOLLOWS
FROM
RESULTS
IN
TO
PROVE
NOTE
THAT
WHEN
Ρ
THE
OPTIMAL
SPEEDS
ARE
THOSE
OPTIMAL
FOR
BATCH
ARRIVALS
WHICH
SATISFY
BY
THEN
IT
IS
STRAIGHTFORWARD
FROM
THE
DP
THAT
INCREASES
ING
ORDER
OF
RELEASE
TIME
R
R
R
JNA
T
NOTE
THAT
THIS
IS
A
SCALING
OF
THE
POTENTIAL
FUNCTION
THAT
WAS
USED
IN
TO
ANALYZE
LAPS
AS
A
RESULT
TO
PROVE
THEO
REM
WE
CAN
USE
THE
CORRESPONDING
RESULTS
IN
TO
VERIFY
THE
BOUNDARY
AND
JUMP
CONDITIONS
ALL
THAT
REMAINS
IS
THE
RUNNING
CONDITION
WHICH
FOLLOWS
FROM
THE
TECHNICAL
LEMMA
BELOW
THE
PROOF
IS
PROVIDED
IN
APPENDIX
B
LEMMA
LET
Φ
BE
GIVEN
BY
AND
A
BE
THE
DISCIPLINE
PS
SN
WITH
SN
NΒ
ΗNΒ
THEN
UNDER
A
AT
POINTS
WHERE
Φ
IS
DIFFERENTIABLE
MONOTONICALLY
WITH
LOAD
Ρ
WHICH
GIVES
INTERESTINGLY
THE
BOUNDS
IN
PROPOSITION
ARE
TIGHT
FOR
LARGE
N
AND
HAVE
A
FORM
SIMILAR
TO
THE
FORM
OF
THE
WORST
CASE
SPEEDS
FOR
SRPT
AND
PS
IN
THEOREMS
AND
IN
CONTRAST
TO
THE
LARGE
BODY
OF
WORK
STUDYING
THE
OPTI
MAL
SPEEDS
UNDER
PS
SCHEDULING
THERE
IS
NO
WORK
CHARAC
TERIZING
THE
OPTIMAL
SPEEDS
UNDER
SRPT
SCHEDULING
THIS
IS
NOT
UNEXPECTED
SINCE
THE
ANALYSIS
OF
SRPT
IN
THE
STATIC
SPEED
SETTING
IS
SIGNIFICANTLY
MORE
INVOLVED
THAN
THAT
OF
PS
THUS
INSTEAD
OF
ANALYTICALLY
DETERMINING
THE
OPTIMAL
SPEEDS
FOR
SRPT
WE
ARE
LEFT
TO
USE
A
HEURISTIC
APPROACH
A
A
Α
DΦ
O
O
Α
NOTE
THAT
THE
SPEEDS
SUGGESTED
BY
THE
WORST
CASE
RESULTS
N
Β
C
N
DT
Β
FOR
SRPT
AND
PS
THEOREMS
AND
ARE
THE
SAME
AND
THE
OPTIMAL
SPEEDS
FOR
A
BATCH
ARRIVAL
ARE
GIVEN
BY
FOR
BOTH
WHERE
C
Η
MAX
Α
Α
STOCHASTIC
ANALYSIS
WE
NOW
STUDY
OPTIMAL
DYNAMIC
SPEED
SCALING
IN
THE
STOCHASTIC
SETTING
IN
CONTRAST
TO
THE
WORST
CASE
RESULTS
IN
THE
STOCHASTIC
SETTING
IT
IS
POSSIBLE
TO
OPTIMIZE
THE
ALGORITHM
FOR
THE
EXPECTED
WORKLOAD
IN
A
REAL
APPLICATION
IT
IS
CLEAR
THAT
INCORPORATING
KNOWLEDGE
ABOUT
THE
WORKLOAD
INTO
THE
DESIGN
CAN
LEAD
TO
IMPROVED
PERFORMANCE
OF
COURSE
THE
POLICIES
MOTIVATED
BY
THIS
AND
THE
FACT
THAT
MATCHES
THE
ASYMPTOTIC
FORM
OF
THE
STOCHASTIC
RESULTS
FOR
PS
IN
PROPO
SITION
WE
PROPOSE
TO
USE
THE
OPTIMAL
PS
SPEEDS
IN
THE
CASE
OF
SRPT
TO
EVALUATE
THE
PERFORMANCE
OF
THIS
HEURISTIC
WE
USE
SIMU
LATION
EXPERIMENTS
FIGURE
THAT
COMPARE
THE
PERFORMANCE
OF
THIS
SPEED
SCALING
ALGORITHM
TO
THE
FOLLOWING
LOWER
BOUND
PROPOSITION
IN
A
GI
GI
QUEUE
WITH
P
SΑ
DRAWBACK
IS
THAT
THERE
IS
ALWAYS
UNCERTAINTY
ABOUT
WORKLOAD
INFORMATION
EITHER
DUE
TO
TIME
VARYING
WORKLOADS
MEASURE
ZO
MAX
ΓΑ
ΓΑ
Α
Α
Λ
MENT
NOISE
OR
SIMPLY
MODEL
INACCURACIES
WE
DISCUSS
RO
BUSTNESS
TO
THESE
FACTORS
IN
SECTION
AND
IN
THE
CURRENT
SECTION
ASSUME
THAT
EXACT
WORKLOAD
INFORMATION
IS
KNOWN
TO
THE
SPEED
SCALER
AND
THAT
THE
MODEL
IS
ACCURATE
IN
THIS
SETTING
THERE
HAS
BEEN
A
SUBSTANTIAL
AMOUNT
OF
WORK
STUDYING
THE
M
GI
PS
MODEL
THIS
WORK
IS
IN
THE
CONTEXT
OF
OPERATIONS
MANAGEMENT
AND
SO
FOCUSES
ON
OPERATING
COSTS
RATHER
THAN
ENERGY
BUT
THE
MODEL
STRUC
TURE
IS
EQUIVALENT
THIS
SERIES
OF
WORK
FORMULATES
THE
DETER
MINATION
OF
THE
OPTIMAL
SPEEDS
AS
A
STOCHASTIC
DYNAMIC
PRO
GRAMMING
DP
PROBLEM
AND
PROVIDES
NUMERIC
TECHNIQUES
FOR
DETERMINING
THE
OPTIMAL
SPEEDS
AS
WELL
AS
PROVING
THAT
THE
OPTIMAL
SPEEDS
ARE
MONOTONIC
IN
THE
QUEUE
LENGTH
THE
OPTIMAL
SPEEDS
HAVE
BEEN
CHARACTERIZED
AS
FOLLOWS
RE
CALL
THAT
Γ
Ρ
Α
PROPOSITION
CONSIDER
AN
M
GI
PS
QUEUE
WITH
CONTROLLABLE
SERVICE
RATES
SN
LET
P
SΑ
THE
OPTIMAL
DYNAMIC
SPEEDS
ARE
CONCAVE
AND
SATISFY
THE
DYNAMIC
PROGRAM
GIVEN
IN
FOR
Α
AND
ANY
N
THEY
SATISFY
Γ
N
Γ
N
MIN
Γ
WORK
ACTUALLY
STUDIES
THE
M
M
FCFS
QUEUE
BUT
SINCE
THE
M
GI
PS
QUEUE
WITH
CONTROLLABLE
SERVICE
RATES
IS
THIS
WAS
PROVEN
IN
IN
THE
CONTEXT
OF
THE
M
GI
PS
BUT
THE
PROOF
CAN
EASILY
BE
SEEN
TO
HOLD
MORE
GENERALLY
SIMULATION
EXPERIMENTS
ALSO
ALLOW
US
TO
STUDY
OTHER
INTER
ESTING
TOPICS
SUCH
AS
I
A
COMPARISON
OF
THE
PERFORMANCE
OF
THE
WORST
CASE
SCHEMES
FOR
SRPT
AND
PS
WITH
THE
STOCHASTIC
SCHEMES
AND
II
A
COMPARISON
OF
THE
PERFORMANCE
OF
SRPT
AND
PS
IN
THE
SPEED
SCALING
MODEL
IN
THESE
EXPERIMENTS
THE
OPTIMAL
SPEEDS
FOR
PS
IN
THE
STOCHASTIC
MODEL
ARE
FOUND
USING
THE
NUMERIC
ALGORITHM
FOR
SOLVING
THE
DP
DESCRIBED
IN
AND
THEN
THESE
SPEEDS
ARE
ALSO
USED
FOR
SRPT
DUE
TO
LIMITED
SPACE
WE
DESCRIBE
THE
RESULTS
FROM
ONLY
ONE
OF
MANY
SETTINGS
WE
INVESTIGATED
FIGURE
SHOWS
THAT
THE
OPTIMAL
SPEEDS
FROM
THE
DP
DP
HAVE
A
SIMILAR
FORM
TO
THE
SPEEDS
MOTIVATED
BY
THE
WORST
CASE
RESULTS
P
NΒ
INV
DIFFERING
BY
Γ
FOR
HIGH
QUEUE
OCCUPANCIES
FIGURE
SHOWS
HOW
THE
TOTAL
COST
DE
PENDS
ON
THE
CHOICE
OF
SPEEDS
AND
SCHEDULER
AT
LOW
LOADS
ALL
SCHEMES
ARE
INDISTINGUISHABLE
AT
HIGHER
LOADS
THE
PER
FORMANCE
OF
THE
PS
INV
SCHEME
DEGRADES
SIGNIFICANTLY
BUT
THE
SRPT
INV
SCHEME
MAINTAINS
FAIRLY
GOOD
PERFORMANCE
NOTE
THOUGH
THAT
IF
P
SΑ
FOR
Α
THE
PERFORMANCE
OF
SRPT
INV
DEGRADES
SIGNIFICANTLY
TOO
IN
CONTRAST
THE
DP
BASED
SCHEMES
BENEFIT
SIGNIFICANTLY
FROM
HAVING
THE
SLIGHTLY
HIGHER
SPEEDS
CHOSEN
TO
OPTIMIZE
RATHER
THAN
MINIMIZE
THE
COMPETITIVE
RATIO
FINALLY
THE
SRPT
DP
SCHEME
PER
FORMS
NEARLY
OPTIMALLY
WHICH
JUSTIFIES
THE
HEURISTIC
OF
USING
A
SYMMETRIC
DISCIPLINE
IT
HAS
THE
SAME
OCCUPANCY
DISTRI
BUTION
AND
MEAN
DELAY
AS
AN
M
M
FCFS
QUEUE
THE
RANGE
OF
MINIMIZATION
WAS
MISSTATED
AS
Σ
A
B
N
FIGURE
COMPARISON
OF
SRPT
AND
PS
SCHEDULING
UNDER
BOTH
SN
P
NΒ
AND
SPEEDS
OPTIMIZED
FOR
AN
M
GI
PS
SYSTEM
USING
PARETO
JOB
SIZES
AND
P
THE
OPTIMAL
SPEEDS
FOR
PS
IN
THE
CASE
OF
HOWEVER
THE
PS
DP
SCHEME
PERFORMS
NEARLY
AS
WELL
AS
SRPT
DP
TO
GETHER
THESE
OBSERVATIONS
SUGGEST
THAT
IT
IS
IMPORTANT
TO
FIGURE
COMPARISON
OF
SN
P
NΒ
WITH
SPEEDS
DP
OPTIMIZED
FOR
AN
M
GI
SYSTEM
WITH
Γ
AND
P
GATED
STATIC
SPEED
SATISFIES
Β
DE
N
R
DS
FIGURE
VALIDATION
OF
THE
HEAVY
TRAFFIC
APPROX
IMATION
BY
SIMULA
TION
USING
PARETO
JOB
SIZES
WITH
E
X
P
OPTIMIZE
THE
SPEED
SCALER
BUT
NOT
NECESSARILY
THE
SCHEDULER
GATED
STATIC
SPEED
SCALING
SECTION
STUDIED
A
SOPHISTICATED
FORM
OF
SPEED
SCALING
WHERE
THE
SPEED
CAN
DEPEND
ON
THE
CURRENT
OCCUPANCY
THIS
SCHEME
CAN
PERFORM
NEARLY
OPTIMALLY
HOWEVER
ITS
COMPLEX
ITY
AND
OVERHEADS
MAY
BE
PROHIBITIVE
THIS
IS
IN
CONTRAST
TO
THE
SIMPLEST
NON
TRIVIAL
FORM
GATED
STATIC
SPEED
SCALING
WHERE
SN
FOR
SOME
CONSTANT
SPEED
SGS
THIS
RE
QUIRES
MINIMAL
HARDWARE
TO
SUPPORT
E
G
A
CMOS
CHIP
MAY
HAVE
A
CONSTANT
CLOCK
SPEED
BUT
AND
IT
WITH
THE
GATING
SIG
WHERE
R
Ρ
IS
THE
UTILIZATION
AND
P
SP
P
NOTE
THAT
IF
P
IS
CONVEX
THEN
P
IS
INCREASING
AND
IF
P
IS
BOUNDED
AWAY
FROM
THEN
P
IS
UNBOUNDED
UNDER
PS
E
N
Ρ
Ρ
AND
SO
DE
N
DS
N
Ρ
BY
THE
OPTIMAL
SPEEDS
SATISFY
ΒE
N
R
RP
UNFORTUNATELY
IN
THE
CASE
OF
SRPT
THINGS
ARE
NOT
AS
EASY
FOR
IT
IS
WELL
KNOWN
E
G
THAT
NAL
TO
SET
THE
SPEED
TO
R
R
X
DT
Λ
R
X
Τ
DF
Τ
X
GATED
STATIC
SPEED
SCALING
CAN
BE
ARBITRARILY
BAD
IN
THE
WORST
CASE
SINCE
JOBS
CAN
ARRIVE
FASTER
THAN
SGS
THUS
WE
E
T
X
T
Λ
R
T
Τ
DF
Τ
Λ
R
X
Τ
DF
Τ
DF
X
STUDY
GATED
STATIC
SPEED
SCALING
ONLY
IN
THE
STOCHASTIC
MODEL
WHERE
THE
CONSTANT
SPEED
SGS
CAN
DEPEND
ON
THE
LOAD
WE
STUDY
THE
GATED
STATIC
SPEED
SCALING
UNDER
SRPT
AND
PS
SCHEDULING
THE
OPTIMAL
GATED
STATIC
SPEED
UNDER
PS
HAS
BEEN
DERIVED
IN
BUT
THE
OPTIMAL
SPEED
UNDER
SRPT
HAS
NOT
BEEN
STUDIED
PREVIOUSLY
OUR
RESULTS
HIGHLIGHT
TWO
PRACTICAL
INSIGHTS
FIRST
WE
SHOW
THAT
GATED
STATIC
SPEED
SCALING
CAN
PROVIDE
NEARLY
THE
SAME
COST
AS
THE
OPTIMAL
DYNAMIC
POLICY
IN
THE
STOCHASTIC
MODEL
THUS
THE
SIMPLEST
POLICY
CAN
NEARLY
MATCH
THE
PERFORMANCE
OF
THE
MOST
SOPHISTICATED
POLICY
SECOND
WE
SHOW
THAT
THE
PERFORMANCE
OF
GATED
STATIC
UNDER
PS
AND
SRPT
IS
NOT
TOO
DIFFERENT
THUS
SCHEDULING
IS
MUCH
LESS
IMPORTANT
TO
OPTIMIZE
THAN
IN
SYSTEMS
IN
WHICH
THE
SPEED
IS
FIXED
IN
ADVANCE
THIS
REINFORCES
WHAT
WE
OBSERVED
FOR
DYNAMIC
SPEED
SCALING
OPTIMAL
GATED
STATIC
SPEEDS
WE
NOW
DERIVE
THE
OPTIMAL
SPEED
SGS
WHICH
MINIMIZES
THE
EXPECTED
COST
OF
GATED
STATIC
IN
THE
STOCHASTIC
MODEL
UNDER
BOTH
SRPT
AND
PS
FIRST
NOTE
THAT
SINCE
THE
POWER
COST
IS
CONSTANT
AT
P
SGS
WHENEVER
THE
SERVER
IS
RUNNING
THE
OPTIMAL
SPEED
IS
ARG
MIN
ΒE
T
P
PR
N
Λ
THE
COMPLEXITY
OF
THIS
EQUATION
RULES
OUT
CALCULATING
THE
SPEEDS
ANALYTICALLY
SO
INSTEAD
WE
USE
SIMPLER
FORMS
FOR
E
N
THAT
ARE
EXACT
IN
ASYMPTOTICALLY
HEAVY
OR
LIGHT
TRAFFIC
A
HEAVY
TRAFFIC
APPROXIMATION
WE
STATE
THE
HEAVY
TRAFFIC
RESULTS
FOR
DISTRIBUTIONS
WHOSE
C
C
D
F
F
HAS
LOWER
AND
UPPER
MATUSZEWSKA
INDICES
M
AND
M
INTUITIVELY
F
X
AS
X
FOR
SOME
SO
THE
MATUSZEWSKA
INDEX
CAN
BE
THOUGHT
OF
X
BE
THE
FRACTION
OF
WORK
COMING
FROM
JOBS
OF
SIZE
AT
MOST
X
THE
FOLLOWING
WAS
PROVEN
IN
PROPOSITION
FOR
AN
M
GI
UNDER
SRPT
WITH
SPEED
E
N
Θ
H
Ρ
AS
Ρ
WHERE
E
Ρ
G
Ρ
IF
M
PROPOSITION
MOTIVATES
THE
FOLLOWING
HEAVY
TRAFFIC
APPROXI
MATION
FOR
THE
CASE
WHEN
THE
SPEED
IS
E
N
CH
Ρ
WHERE
C
IS
A
CONSTANT
DEPENDENT
ON
THE
JOB
SIZE
DISTRIBU
TION
FOR
JOB
SIZES
WHICH
ARE
PARETO
A
OR
MORE
GENER
ALLY
REGULARLY
VARYING
WITH
A
IT
IS
KNOWN
THAT
C
Π
A
SIN
Π
A
FIGURE
SHOWS
IN
THE
SECOND
TERM
PR
N
Ρ
AND
SO
MULTIPLYING
THAT
IN
THIS
CASE
THE
HEAVY
TRAFFIC
RESULTS
ARE
ACCURATE
EVEN
BY
Λ
AND
SETTING
THE
DERIVATIVE
TO
GIVES
THAT
THE
OPTIMAL
THAT
THE
PEAK
AROUND
Γ
IN
FIG
B
IS
MOST
LIKELY
DUE
TO
THE
LOOSENESS
OF
THE
LOWER
BOUND
FOR
QUITE
LOW
LOADS
GIVEN
APPROXIMATION
WE
CAN
NOW
RETURN
TO
EQUATION
AND
CALCULATE
THE
OPTIMAL
SPEED
FOR
GATED
STATIC
SRPT
DEFINE
H
R
G
R
G
R
A
A
B
A
B
FIGURE
COMPARISON
FOR
GATED
STATIC
PS
USING
AND
SRPT
USING
WITH
P
A
UTILIZATION
GIVEN
PARETO
JOB
SIZES
B
DEPENDENCE
OF
SPEED
ON
THE
JOB
SIZE
DISTRIBUTION
FOR
PARETO
A
THEOREM
SUPPOSE
APPROXIMATION
HOLDS
WITH
EQUALITY
IF
M
THEN
FOR
THE
OPTIMAL
GATED
STATIC
SPEED
ΒE
N
R
RH
R
RP
R
IF
M
THEN
FOR
THE
OPTIMAL
GATED
STATIC
SPEED
FIGURE
COMPARISON
OF
PS
AND
SRPT
WITH
GATED
STATIC
SPEEDS
AND
VERSUS
THE
DYNAMIC
SPEEDS
OPTIMAL
FOR
AN
M
GI
PS
JOB
SIZES
ARE
DIS
TRIBUTED
AS
PARETO
AND
P
BEYOND
HEAVY
TRAFFIC
LET
US
NEXT
BRIEFLY
CONSIDER
THE
LIGHT
TRAFFIC
REGIME
AS
Ρ
THERE
IS
SELDOM
MORE
THAN
ONE
JOB
IN
THE
SYSTEM
AND
SRPT
AND
PS
HAVE
NEARLY
INDISTINGUISHABLE
E
N
SO
IN
THIS
CASE
IT
IS
APPROPRIATE
TO
USE
SPEEDS
GIVEN
BY
GIVEN
THE
LIGHT
AND
HEAVY
TRAFFIC
APPROXIMATIONS
WE
HAVE
JUST
DESCRIBED
IT
REMAINS
TO
DECIDE
THE
SPEED
IN
THE
INTER
MEDIATE
REGIME
WE
PROPOSE
SETTING
SRP
T
P
SRP
T
HT
ΒE
N
R
LOG
R
P
SGS
MIN
SGS
SGS
WHERE
SP
SATISFIES
AND
SSRP
T
HT
IS
GIVEN
BY
WITH
PROOF
FOR
BREVITY
WE
ONLY
PROVE
THE
SECOND
CLAIM
IF
M
THEN
THERE
IS
A
C
CE
X
SUCH
THAT
GS
GS
E
N
ESTIMATED
BY
TO
SEE
WHY
IS
REASONABLE
WE
FIRST
SHOW
THAT
OFTEN
TENDS
TO
THE
OPTIMAL
SPEED
AS
Ρ
E
N
C
LOG
FOR
SPEED
NOW
DE
N
C
Ρ
C
Ρ
PROPOSITION
IF
M
OR
BOTH
M
AND
ARBI
TRARILY
SMALL
JOBS
ARE
POSSIBLE
I
E
FOR
ALL
X
THERE
IS
A
Y
X
WITH
F
Y
THEN
PRODUCES
THE
OPTIMAL
SCALING
AS
Ρ
PROOF
FOR
Ρ
ALSO
R
AND
E
N
R
BY
E
N
Ρ
G
AND
RH
R
BY
L
HOSPITAL
RULE
WHENCE
Ρ
LOG
Ρ
AND
THE
FACTOR
IN
BRACKETS
IS
DOMINATED
BY
ITS
SECOND
TERM
IN
HEAVY
TRAFFIC
SUBSTITUTING
THIS
INTO
GIVES
THE
RESULT
TO
EVALUATE
THE
SPEEDS
DERIVED
FOR
HEAVY
TRAFFIC
FIG
URE
B
ILLUSTRATES
THE
GATED
STATIC
SPEEDS
DERIVED
FOR
SRPT
AND
PS
FOR
P
AND
Ρ
AND
VARYING
JOB
SIZE
DIS
TRIBUTION
THIS
SUGGESTS
THAT
THE
SRPT
SPEEDS
ARE
NEARLY
INDEPENDENT
OF
THE
JOB
SIZE
DISTRIBUTION
NOTE
THAT
THE
VER
TICAL
AXIS
DOES
NOT
START
FROM
MOREOVER
THE
SPEEDS
OF
SRPT
AND
PS
DIFFER
SIGNIFICANTLY
IN
THIS
SETTING
SINCE
THE
ALSO
BECOMES
Β
P
FROM
THIS
IS
THE
OPTIMAL
SPEED
AT
WHICH
TO
SERVER
A
BATCH
OF
A
SINGLE
JOB
SINCE
AS
Ρ
THE
SYSTEM
ALMOST
CERTAINLY
HAS
A
SINGLE
JOB
WHEN
IT
IS
NON
EMPTY
THIS
IS
AN
APPROPRIATE
SPEED
ALTHOUGH
TENDS
TO
THE
OPTIMAL
SPEEDS
OVER
ESTIMATES
E
N
FOR
SMALL
Ρ
AND
SO
SSRP
T
HT
IS
HIGHER
THAN
OPTIMAL
FOR
SMALL
LOADS
CONVERSELY
FOR
A
GIVEN
SPEED
THE
DELAY
IS
LESS
UNDER
SRPT
THAN
PS
AND
SO
THE
OPTIMAL
SPEED
UNDER
SRPT
WILL
BE
LOWER
THAN
THAT
UNDER
PS
HENCE
SSRP
T
HT
SP
IN
THE
LARGE
Ρ
REGIME
WHERE
THE
FORMER
GS
GS
SPEEDS
UNDER
SRPT
ARE
APPROXIMATELY
MINIMAL
THE
SPEEDS
MUST
BE
LARGER
THAN
Γ
WHILE
THE
PS
SPEEDS
ARE
Γ
THEOREM
ASSUMES
THAT
THE
SYSTEM
IS
IN
HEAVY
TRAFFIC
TO
UNDERSTAND
WHEN
THIS
HOLDS
FIRST
NOTE
THAT
IF
THERE
IS
A
MAXIMUM
ALLOWABLE
SPEED
SMAX
THEN
THE
HEAVY
TRAFFIC
REGIME
IS
VALID
AS
Ρ
SMAX
IN
THE
CASE
WHEN
THERE
IS
NO
MAXIMUM
ALLOWABLE
SPEED
THE
FOLLOWING
APPLIES
PROPOSITION
IF
P
IS
UNBOUNDED
AS
AND
M
M
THEN
AS
Ρ
INDUCES
THE
HEAVY
TRAFFIC
REGIME
Ρ
WE
OMIT
THE
PROOF
OF
THIS
RESULT
DUE
TO
SPACE
CONCERNS
FIG
URE
A
ILLUSTRATES
THE
EFFECT
OF
RAISING
THE
LOAD
ON
THE
UTI
LIZATION
IN
THE
CASE
OF
P
AND
PARETO
JOB
SIZES
BECOMES
ACCURATE
THUS
THE
MIN
OPERATION
IN
SELECTS
THE
APPROPRIATE
FORM
IN
EACH
REGIME
GATED
STATIC
VS
DYNAMIC
SPEED
SCALING
NOW
THAT
WE
HAVE
DERIVED
THE
OPTIMAL
GATED
STATIC
SPEEDS
WE
CAN
CONTRAST
THE
PERFORMANCE
OF
GATED
STATIC
WITH
THAT
OF
DYNAMIC
SPEED
SCALING
THIS
IS
A
COMPARISON
OF
THE
MOST
AND
LEAST
SOPHISTICATED
FORMS
OF
SPEED
SCALING
AS
FIGURE
SHOWS
THE
PERFORMANCE
IN
TERMS
OF
MEAN
DELAY
PLUS
MEAN
ENERGY
OF
A
WELL
TUNED
GATED
STATIC
SYSTEM
IS
ALMOST
INDISTINGUISHABLE
FROM
THAT
OF
THE
OPTIMAL
DYNAMIC
SPEEDS
MOREOVER
THERE
IS
LITTLE
DIFFERENCE
BETWEEN
THE
COST
UNDER
PS
GATED
AND
SRPT
GATED
AGAIN
HIGHLIGHTING
THAT
THE
IMPORTANCE
OF
SCHEDULING
IN
THE
SPEED
SCALING
MODEL
IS
CONSIDERABLY
LESS
THAN
IN
STANDARD
QUEUEING
MODELS
DESIGN
DESIGN
SRPT
PS
A
B
C
FIGURE
IMPACT
OF
MISESTIMATION
OF
Γ
UNDER
PS
AND
SRPT
COST
WHEN
Γ
BUT
SN
WERE
CHOSEN
FOR
DESIGNED
Γ
JOB
SIZES
ARE
PARETO
AND
P
IN
ADDITION
TO
OBSERVING
NUMERICALLY
THAT
THE
GATED
STATIC
SCHEMES
ARE
NEAR
OPTIMAL
IT
IS
POSSIBLE
TO
PROVIDE
SOME
ANA
LYTIC
SUPPORT
FOR
THIS
FACT
AS
WELL
IN
IT
WAS
PROVEN
THAT
PS
GATED
IS
WITHIN
A
FACTOR
OF
OF
PS
DP
WHEN
P
COMBINING
THIS
RESULT
WITH
THE
COMPETITIVE
RATIO
RESULTS
IN
THIS
PAPER
WE
HAVE
COROLLARY
CONSIDER
P
THE
OPTIMAL
PS
AND
SRPT
GATED
STATIC
DESIGNS
ARE
O
COMPETITIVE
IN
AN
FIGURE
COMPARISON
OF
PS
AND
SRPT
WITH
LINEAR
SPEEDS
SN
N
Β
AND
WITH
DYNAMIC
SPEEDS
OPTIMAL
FOR
PS
JOB
SIZES
ARE
PARETO
AND
P
BY
PROPOSITION
SN
NΒ
Α
Α
SINCE
Α
THIS
IMPLIES
SN
P
NΒ
FURTHER
IMPLIES
THAT
SDP
O
Α
FOR
ANY
FIXED
Ρ
AND
Β
AND
IS
BOUNDED
FOR
FINITE
N
HENCE
THE
SPEEDS
SDP
ARE
OF
THE
FORM
GIVEN
IN
LEMMAS
AND
FOR
SOME
FINITE
Η
WHICH
MAY
DEPEND
ON
Π
AND
THE
CONSTANT
Ρ
FROM
WHICH
IT
FOLLOWS
THAT
IS
CONSTANT
COM
PETITIVE
FOR
Α
PROPOSITION
IMPLIES
SDP
P
NΒ
PROOF
LET
Π
P
SRP
T
AND
SΠ
BE
THE
OPTIMAL
WHENCE
SRPT
SN
IS
COMPETITIVE
GS
GATED
STATIC
SPEED
FOR
Π
AND
SDP
BE
THE
OPTIMAL
SPEEDS
COROLLARY
HIGHLIGHTS
THAT
SDP
DESIGNED
FOR
A
GIVEN
Ρ
N
WHICH
SOLVE
THE
DP
FOR
THE
M
GI
PS
QUEUE
THEN
LEADS
TO
A
SPEED
SCALER
THAT
IS
ROBUST
HOWEVER
THE
COST
STILL
DEGRADES
SIGNIFICANTLY
WHEN
Ρ
IS
MISPREDICTED
BADLY
AS
Z
Π
SΠ
P
SP
P
SDP
SHOWN
IN
FIGURE
WE
NOW
CONSIDER
A
DIFFERENT
FORM
OF
ROBUSTNESS
IF
THE
AR
THE
LAST
TWO
INEQUALITIES
FOLLOW
FROM
AND
THEOREM
AS
Z
P
SDP
Z
P
P
NΒ
IN
AN
M
GI
WITH
KNOWN
Ρ
RIVALS
ARE
KNOWN
TO
BE
WELL
APPROXIMATED
BY
A
POISSON
PRO
CESS
BUT
Ρ
IS
UNKNOWN
IS
IT
POSSIBLE
TO
CHOOSE
SPEEDS
THAT
ARE
CLOSE
TO
OPTIMAL
FOR
ALL
Ρ
IT
WAS
SHOWN
IN
THAT
USING
LINEAR
SPEEDS
SN
N
Β
GIVES
NEAR
OPTIMAL
PERFORMANCE
ROBUSTNESS
AND
SPEED
SCALING
SECTION
SHOWS
THAT
NEAR
OPTIMAL
PERFORMANCE
CAN
BE
OB
TAINED
USING
THE
SIMPLEST
FORM
OF
SPEED
SCALING
RUNNING
AT
A
STATIC
SPEED
WHEN
NOT
IDLE
WHY
THEN
DO
CPU
MANUFACTUR
ERS
DESIGN
CHIPS
WITH
MULTIPLE
SPEEDS
THE
REASON
IS
THAT
THE
OPTIMAL
GATED
STATIC
DESIGN
DEPENDS
INTIMATELY
ON
THE
LOAD
Ρ
THIS
CANNOT
BE
KNOWN
EXACTLY
IN
ADVANCE
ESPECIALLY
SINCE
WORKLOADS
TYPICALLY
VARY
OVER
TIME
SO
AN
IMPORTANT
PROP
ERTY
OF
A
SPEED
SCALING
DESIGN
IS
ROBUSTNESS
TO
UNCERTAINTY
IN
THE
WORKLOAD
Ρ
AND
F
AND
TO
MODEL
INACCURACIES
FIGURE
ILLUSTRATES
THAT
IF
A
GATED
STATIC
DESIGN
IS
USED
PERFORMANCE
DEGRADES
DRAMATICALLY
WHEN
Ρ
IS
MISPREDICTED
IF
THE
STATIC
SPEED
IS
CHOSEN
AND
THE
LOAD
IS
LOWER
THAN
EX
PECTED
EXCESS
ENERGY
WILL
BE
USED
UNDERESTIMATING
THE
LOAD
IS
EVEN
WORSE
IF
THE
SYSTEM
HAS
STATIC
SPEED
AND
Ρ
THEN
THE
COST
IS
UNBOUNDED
IN
CONTRAST
FIGURE
ILLUSTRATES
SIMULATION
EXPERIMENTS
WHICH
SHOW
THAT
DYNAMIC
SPEED
SCALING
SRPT
DP
IS
SIG
NIFICANTLY
MORE
ROBUST
TO
MISPREDICTION
OF
THE
WORKLOAD
IN
FACT
WE
CAN
PROVE
THIS
ANALYTICALLY
BY
PROVIDING
WORST
CASE
GUARANTEES
FOR
THE
SRPT
DP
AND
PS
DP
LET
SDP
DENOTE
THE
SPEEDS
USED
FOR
SRPT
DP
AND
PS
DP
NOTE
THAT
THE
COROLLARY
BELOW
IS
DISTINCTIVE
IN
THAT
IT
PROVIDES
WORST
CASE
GUARANTEES
FOR
A
STOCHASTIC
CONTROL
POLICY
COROLLARY
CONSIDER
P
SΑ
WITH
Α
AND
ALGORITHM
WHICH
CHOOSES
SPEEDS
OPTIMAL
FOR
PS
SCHEDULING
IN
AN
M
GI
QUEUE
WITH
LOAD
Ρ
IF
USES
EITHER
PS
OR
SRPT
SCHEDULING
THEN
IS
O
COMPETITIVE
IN
THE
WORST
CASE
MODEL
PROOF
THE
PROOF
APPLIES
LEMMAS
AND
FROM
THE
WORST
CASE
MODEL
TO
THE
SPEEDS
FROM
THE
STOCHASTIC
MODEL
WHEN
P
AND
PS
SCHEDULING
IS
USED
THIS
SCHEME
PER
FORMS
CONSIDERABLY
BETTER
THAN
USING
SN
P
NΒ
DESPITE
THE
FACT
THAT
IT
ALSO
USES
NO
KNOWLEDGE
OF
THE
WORKLOAD
GIVEN
THE
DECOUPLING
OF
SCHEDULING
AND
SPEED
SCALING
SUGGESTED
BY
THE
RESULTS
IN
SECTION
THIS
MOTIVATES
USING
THE
SAME
LINEAR
SPEED
SCALING
FOR
SRPT
FIGURE
ILLUSTRATES
THAT
THIS
LIN
EAR
SPEED
SCALING
PROVIDES
NEAR
OPTIMAL
PERFORMANCE
UNDER
SRPT
TOO
THE
ROBUSTNESS
OF
THIS
SPEED
SCALING
IS
ILLUSTRATED
IN
FIGURE
HOWEVER
DESPITE
BEING
MORE
ROBUST
IN
THE
SENSE
OF
THIS
PARAGRAPH
THE
LINEAR
SCALING
IS
NOT
ROBUST
TO
INACCU
RACIES
IN
THE
MODEL
SPECIFICALLY
IT
IS
NOT
O
COMPETITIVE
IN
GENERAL
NOR
EVEN
FOR
THE
CASE
OF
BATCH
ARRIVALS
FAIRNESS
AND
SPEED
SCALING
TO
THIS
POINT
WE
HAVE
SEEN
THAT
SPEED
SCALING
HAS
MANY
BENEFITS
HOWEVER
WE
SHOW
IN
THIS
SECTION
THAT
DYNAMIC
SPEED
SCALING
HAS
AN
UNDESIRABLE
CONSEQUENCE
MAGNIFYING
UNFAIR
NESS
FAIRNESS
IS
AN
IMPORTANT
CONCERN
FOR
SYSTEM
DESIGN
IN
MANY
APPLICATIONS
AND
THE
IMPORTANCE
OF
FAIRNESS
WHEN
CON
SIDERING
ENERGY
EFFICIENCY
WAS
RECENTLY
RAISED
IN
HOW
EVER
UNFAIRNESS
UNDER
SPEED
SCALING
DESIGNS
HAS
NOT
PREVI
OUSLY
BEEN
IDENTIFIED
IN
RETROSPECT
THOUGH
IT
IS
NOT
A
SUR
PRISING
BYPRODUCT
OF
SPEED
SCALING
IF
THERE
IS
SOME
JOB
TYPE
THAT
IS
ALWAYS
SERVED
WHEN
THE
QUEUE
LENGTH
IS
LONG
SHORT
IT
WILL
RECEIVE
BETTER
WORSE
PERFORMANCE
THAN
IT
WOULD
HAVE
IN
A
SYSTEM
WITH
A
STATIC
SPEED
TO
SEE
THAT
THIS
MAGNIFIES
UN
FAIRNESS
RATHER
THAN
BEING
INDEPENDENT
OF
OTHER
BIASES
NOTE
THAT
THE
SCHEDULER
HAS
GREATEST
FLEXIBILITY
TO
SELECT
WHICH
JOB
TO
SERVE
WHEN
THE
QUEUE
IS
LONG
AND
SO
JOBS
SERVED
AT
THAT
TIME
ARE
LIKELY
TO
BE
THOSE
THAT
ALREADY
GET
BETTER
SERVICE
IN
THIS
SECTION
WE
PROVE
THAT
THIS
SERVICE
RATE
DIFFEREN
TIAL
CAN
LEAD
TO
UNFAIRNESS
IN
A
RIGOROUS
SENSE
UNDER
SRPT
AND
NON
PREEMPTIVE
POLICIES
E
G
FCFS
HOWEVER
UNDER
PS
SPEED
SCALING
DOES
NOT
LEAD
TO
UNFAIRNESS
DEFINING
FAIRNESS
THE
FAIRNESS
OF
SCHEDULING
POLICIES
HAS
RECENTLY
RECEIVED
A
LOT
OF
ATTENTION
IN
COMPUTER
SYSTEMS
MODELING
WHICH
HAS
LED
TO
A
VARIETY
OF
FAIRNESS
MEASURES
E
G
AND
THE
ANALYSIS
OF
NEARLY
ALL
COMMON
SCHEDULING
POLICIES
E
G
X
F
X
REFER
TO
THE
SURVEY
FOR
MORE
DETAILS
HERE
WE
COMPARE
FAIRNESS
NOT
BETWEEN
INDIVIDUAL
JOBS
BUT
VS
JOB
SIZE
VS
CDF
OF
JOB
SIZE
BETWEEN
CLASSES
OF
JOBS
WHERE
A
CLASS
CONSISTS
OF
ALL
JOBS
OF
A
GIVEN
SIZE
SINCE
THIS
PAPER
FOCUSES
ON
DELAY
WE
COMPARE
E
T
X
ACROSS
X
FOR
THIS
PURPOSE
FAIRNESS
WHEN
HAS
BEEN
DEFINED
IN
PRIOR
WORK
AS
FOLLOWS
DEFINITION
A
POLICY
Π
IS
FAIR
IF
FOR
ALL
X
FIGURE
SLOWDOWN
OF
LARGE
JOBS
UNDER
PS
AND
SRPT
UNDER
PARETO
JOB
SIZES
Γ
SN
P
N
AND
P
THE
INTUITION
BEHIND
THEOREM
IS
THE
FOLLOWING
AN
INFINITELY
SIZED
JOB
UNDER
SRPT
WILL
RECEIVE
ALMOST
ALL
OF
E
T
Π
X
X
T
P
X
X
ITS
SERVICE
WHILE
THE
SYSTEM
IS
EMPTY
OF
SMALLER
JOBS
THUS
IT
RECEIVES
SERVICE
DURING
THE
IDLE
PERIODS
OF
THE
REST
OF
THE
SYSTEM
FURTHER
IF
SSRP
T
SP
THEN
THE
BUSY
PERIODS
WILL
THIS
METRIC
IS
MOTIVATED
BY
THE
FACT
THAT
I
PS
IS
INTUITIVELY
FAIR
SINCE
IT
SHARES
THE
SERVER
EVENLY
AMONG
ALL
JOBS
AT
ALL
TIMES
II
THE
SLOWDOWN
A
K
A
STRETCH
OF
PS
IS
CONSTANT
I
E
E
T
X
X
Ρ
III
E
T
X
Θ
X
SO
NOR
MALIZING
BY
X
WHEN
COMPARING
THE
PERFORMANCE
OF
DIFFERENT
BE
LONGER
UNDER
SRPT
AND
SO
THE
SLOWDOWN
OF
THE
LARGEST
JOB
WILL
BE
STRICTLY
GREATER
UNDER
SRPT
THIS
INTUITION
ALSO
PROVIDES
AN
OUTLINE
OF
THE
PROOF
PROOF
BY
LEMMA
IN
APPENDIX
C
T
Π
X
X
Π
Ρ
A
IN
EACH
CASE
JOB
SIZES
IS
APPROPRIATE
ADDITIONAL
SUPPORT
IS
PROVIDED
BY
LEMMA
COMPLETES
THE
PROOF
BY
SHOWING
P
THE
FACT
THAT
MINΠ
MAXX
E
T
Π
X
X
Ρ
USING
THIS
DEFINITION
IT
IS
INTERESTING
TO
NOTE
THAT
THE
CLASS
OF
LARGE
JOBS
IS
ALWAYS
TREATED
FAIRLY
UNDER
ALL
WORK
CONSERVING
POLICIES
I
E
LIMX
E
T
X
X
Ρ
EVEN
UNDER
POLICIES
SUCH
AS
SRPT
THAT
SEEM
BIASED
AGAINST
LARGE
JOBS
IN
CONTRAST
ALL
NON
PREEMPTIVE
POLICIES
E
G
FCFS
HAVE
BEEN
SHOWN
TO
BE
UNFAIR
TO
SMALL
JOBS
THE
FOREGOING
APPLIES
WHEN
THE
FOLLOWING
PROPO
SITION
SHOWS
THAT
PS
STILL
MAINTAINS
A
CONSTANT
SLOWDOWN
IN
THE
SPEED
SCALING
ENVIRONMENT
AND
SO
DEFINITION
IS
STILL
A
NATURAL
NOTION
OF
FAIRNESS
PROPOSITION
CONSIDER
AN
M
GI
QUEUE
WITH
A
SYMMETRIC
SCHEDULING
DISCIPLINE
E
G
PS
WITH
CONTROLLABLE
SERVICE
RATES
THEN
E
T
X
X
E
T
E
X
THE
PROOF
FOLLOWS
FROM
USING
LITTLE
LAW
FOR
JOBS
WITH
SIZE
IN
X
X
E
BUT
IS
OMITTED
FOR
BREVITY
SPEED
SCALING
MAGNIFIES
UNFAIRNESS
NOW
THAT
WE
HAVE
A
NATURAL
CRITERION
FOR
FAIRNESS
WE
PROVE
THAT
SPEED
SCALING
CREATES
MAGNIFIES
UNFAIRNESS
UNDER
SRPT
AND
NON
PREEMPTIVE
POLICIES
SUCH
AS
FCFS
SRPT
WE
FIRST
PROVE
THAT
SRPT
TREATS
THE
LARGEST
JOBS
UNFAIRLY
IN
A
SPEED
SCALING
SYSTEM
RECALL
THAT
THE
LARGEST
JOBS
ARE
ALWAYS
TREATED
FAIRLY
IN
THE
CASE
OF
A
STATIC
SPEED
LET
Π
BE
THE
TIME
AVERAGE
SPEED
UNDER
POLICY
Π
AND
LET
Π
DENOTE
RUNNING
POLICY
Π
ON
A
SYSTEM
WITH
A
PERMANENT
CUSTOMER
IN
ADDITION
TO
THE
STOCHASTIC
LOAD
THEOREM
CONSIDER
A
GI
GI
QUEUE
WITH
CONTROL
LABLE
SERVICE
RATES
AND
UNBOUNDED
INTERARRIVAL
TIMES
LET
SRP
T
IT
CONSIDERS
THE
AVERAGE
SPEED
BETWEEN
RENEWAL
INSTANTS
IN
WHICH
BOTH
QUEUES
ARE
EMPTY
WHICH
IT
MAPS
TO
RENEWAL
PERIODS
IT
THEN
USES
LEMMA
WHICH
SHOWS
THAT
A
BUSY
PERIOD
IS
LONGER
UNDER
SRPT
THAN
PS
TO
SHOW
THAT
LESS
WORK
IS
DONE
ON
THE
PERMANENT
CUSTOMER
IN
THE
RENEWAL
PERIOD
UNDER
SRPT
THAN
UNDER
PS
FIGURE
SHOWS
THAT
UNFAIRNESS
UNDER
SRPT
CAN
BE
CONSID
ERABLE
WITH
LARGE
JOBS
SUFFERING
A
SIGNIFICANT
INCREASE
IN
SLOW
DOWN
AS
COMPARED
TO
PS
HOWEVER
IN
THIS
CASE
ONLY
AROUND
OF
THE
JOBS
ARE
WORSE
OFF
THAN
UNDER
PS
NOTE
THAT
THIS
SETTING
HAS
A
MODERATE
LOAD
WHICH
MEANS
THAT
SRPT
WITH
STATIC
SPEEDS
WOULD
BE
FAIR
TO
ALL
JOB
SIZES
FIGURE
WAS
GENERATED
BY
RUNNING
A
SIMULATION
TO
STEADY
STATE
AND
THEN
INJECTING
A
JOB
OF
SIZE
X
INTO
THE
SYSTEM
AND
MEASURING
ITS
RESPONSE
TIME
THIS
WAS
REPEATED
UNTIL
THE
CONFIDENCE
INTERVALS
SHOWN
ON
FIGURE
A
FOR
SRPT
WERE
TIGHT
AROUND
THE
ESTIMATE
THEOREM
PROVES
THAT
SRPT
CANNOT
USE
DYNAMIC
SPEEDS
AND
PROVIDE
FAIRNESS
TO
LARGE
JOBS
HOWEVER
BY
USING
GATED
STATIC
SPEED
SCALING
SRPT
CAN
PROVIDE
FAIRNESS
E
G
FURTHER
AS
FIGURE
ILLUSTRATES
GATED
STATIC
SPEED
SCALING
PROVIDES
NEARLY
OPTIMAL
COST
SO
IT
IS
POSSIBLE
TO
BE
FAIR
AND
NEAR
OPTIMAL
USING
SRPT
SCHEDULING
BUT
TO
BE
FAIR
ROBUST
NESS
MUST
BE
SACRIFICED
NON
PREEMPTIVE
POLICIES
THE
MAGNIFICATION
OF
UNFAIRNESS
BY
SPEED
SCALING
ALSO
OC
CURS
FOR
ALL
NON
PREEMPTIVE
POLICIES
IN
THE
STATIC
SPEED
SETTING
ALL
NON
PREEMPTIVE
POLICIES
ARE
UNFAIR
TO
SMALL
JOBS
SINCE
THE
RESPONSE
TIME
MUST
INCLUDE
AT
LEAST
THE
RESIDUAL
OF
THE
JOB
SIZE
DISTRIBUTION
IF
THE
SERVER
SRP
T
N
SN
BE
WEAKLY
MONOTONE
INCREASING
AND
SATISFY
IS
BUSY
I
E
P
Ρ
AND
SRP
T
Ρ
THEN
T
P
X
T
SRP
T
X
E
T
X
X
ΡE
X
X
WHICH
GROWS
UNBOUNDEDLY
AS
X
HOWEVER
IF
WE
CONDI
LIM
X
X
A
LIM
X
X
TION
ON
THE
ARRIVAL
OF
A
JOB
TO
AN
EMPTY
SYSTEM
I
E
THE
WORK
IN
SYSTEM
AT
ARRIVAL
W
THEN
NON
PREEMPTIVE
THAT
THE
CONDITIONS
P
Ρ
AND
SRP
T
Ρ
ARE
EQUIVALENT
TO
THE
STABILITY
CONDITIONS
FOR
SSRP
T
AND
SP
POLICIES
ARE
FAIR
IN
THE
SENSE
THAT
THE
SLOWDOWN
IS
CON
STANT
T
X
W
X
SPEED
SCALING
MAGNIFIES
UNFAIR
NESS
UNDER
NON
PREEMPTIVE
POLICIES
IN
THE
FOLLOWING
SENSE
T
X
W
X
CAN
NOW
DIFFER
DRAMATICALLY
ACROSS
JOB
SIZES
PROPOSITION
CONSIDER
A
NON
PREEMPTIVE
GI
GI
SPEED
SCALING
QUEUE
WITH
MEAN
INTER
ARRIVAL
TIME
Λ
AND
SPEEDS
SN
MONOTONICALLY
APPROACHING
AS
N
THEN
WITH
PROBABILITY
MAIN
RESULTS
OF
THE
WORK
COROLLARY
PROVIDING
WORST
CASE
GUARANTEES
FOR
POLICIES
DESIGNED
IN
THE
STOCHASTIC
MODEL
AND
THEOREM
IDENTIFYING
UNFAIRNESS
IN
EXPECTED
PERFORMANCE
UNDER
DYNAMIC
SPEED
SCALING
WITH
SRPT
FURTHER
THE
COM
BINATION
OF
STOCHASTIC
AND
WORST
CASE
ANALYSIS
ADDS
SUPPORT
TO
MANY
OF
THE
OTHER
INSIGHTS
OF
THE
PAPER
E
G
THE
DECOU
PLING
OF
SCHEDULING
AND
SPEED
SCALING
LIM
T
X
W
AND
LIM
T
X
W
THE
RESULTS
IN
THIS
PAPER
MOTIVATE
MANY
INTERESTING
TOPICS
X
X
X
X
FOR
FUTURE
WORK
FOREMOST
IT
WILL
BE
INTERESTING
TO
SEE
IF
THE
LOWER
BOUND
OF
COMPETITIVE
FOR
NATURAL
SPEED
SCALING
ALGO
THE
INTUITION
BEHIND
THIS
RESULT
IS
THAT
SMALL
JOBS
RECEIVE
THEIR
WHOLE
SERVICE
WHILE
ALONE
IN
THE
SYSTEM
WHEREAS
LARGE
JOBS
HAVE
A
LARGE
QUEUE
BUILD
UP
BEHIND
THEM
AND
THERE
FORE
GET
SERVED
AT
A
FASTER
SPEED
THUS
THE
SERVICE
RATE
OF
LARGE
AND
SMALL
JOBS
DIFFERS
MAGNIFYING
THE
UNFAIRNESS
OF
NON
PREEMPTIVE
POLICIES
PROOF
FIRST
THE
LIMIT
AS
X
FOLLOWS
IMMEDIATELY
FROM
NOTING
THAT
AS
X
SHRINKS
THE
PROBABILITY
OF
ANOTHER
AR
RIVAL
BEFORE
COMPLETION
GOES
TO
TO
PROVE
THE
LIMIT
AS
X
LET
A
X
BE
SUCH
THAT
RITHMS
CAN
BE
EXTENDED
TO
ALL
ALGORITHMS
ADDITIONALLY
IT
IS
IMPORTANT
TO
UNDERSTAND
THE
RANGE
OF
APPLICABILITY
OF
THE
INSIGHTS
THAT
SPEED
SCALING
CAN
BE
DECOUPLED
FROM
SCHEDUL
ING
WITH
LITTLE
PERFORMANCE
LOSS
AND
THAT
SCHEDULING
IS
LESS
IMPORTANT
WHEN
ENERGY
IS
ADDED
TO
THE
OBJECTIVE
FURTHER
THE
STUDY
OF
FAIRNESS
IN
THE
CONTEXT
OF
SPEED
SCALING
WAS
ONLY
TOUCHED
ON
BRIEFLY
IN
THIS
PAPER
AND
THERE
ARE
MANY
QUES
TIONS
LEFT
TO
ANSWER
FINALLY
IT
IS
IMPORTANT
TO
ADDRESS
ALL
OF
THE
ISSUES
STUDIED
IN
THIS
PAPER
IN
THE
CONTEXT
OF
OTHER
PER
FORMANCE
OBJECTIVES
E
G
WHEN
TEMPERATURE
IS
CONSIDERED
OR
A
X
A
X
SI
X
SI
WHEN
MORE
GENERAL
COMBINATIONS
OF
ENERGY
AND
RESPONSE
TIME
ARE
CONSIDERED
AND
LET
E
BE
ARBITRARY
THIS
A
X
CAN
BE
THOUGHT
OF
AS
THE
NUMBER
OF
ARRIVALS
BEFORE
X
WORK
IS
COMPLETED
IF
JOBS
ARRIVED
PERIODICALLY
WITH
INTERARRIVAL
TIME
Λ
SINCE
SPEEDS
ARE
NON
DECREASING
THE
TIME
TO
FINISH
THE
JOB
CAN
BE
BOUNDED
ABOVE
BY
THE
TIME
TO
REACH
SPEED
SI
PLUS
THE
TIME
IT
WOULD
TAKE
TO
FINISH
THE
WHOLE
JOB
AT
SPEED
SI
FURTHER
WE
CAN
USE
THE
LAW
OF
LARGE
NUMBERS
TO
BOUND
THE
TIME
TO
REACH
SPEED
SI
AS
X
THIS
GIVES
T
X
W
A
X
E
SINCE
SI
ARE
NON
DECREASING
AND
A
X
Θ
X
IT
FOLLOWS
THAT
THE
RIGHT
HAND
SIDE
INSIDE
THE
BRACKETS
APPROACHES
AS
X
CONVERSELY
A
LOWER
BOUND
ON
THE
TIME
TO
FINISH
THE
JOB
IS
GIVEN
BY
THE
TIME
TO
FINISH
IT
AT
MAXIMUM
SPEED
P
R
T
X
W
W
P
TOGETHER
AND
ESTABLISH
THE
RESULT
IN
GENERAL
SPEED
SCALING
BASED
ON
THE
OCCUPANCY
N
MAY
MAGNIFY
UNFAIRNESS
IN
ANY
POLICY
FOR
WHICH
N
T
IS
CORRELATED
WITH
THE
SIZE
OF
THE
JOB
BEING
PROCESSED
AT
TIME
T
NOTE
THAT
GATED
STATIC
SCALING
DOES
NOT
MAGNIFY
UNFAIRNESS
REGARD
LESS
OF
THE
SCHEDULING
DISCIPLINE
SINCE
ALL
JOBS
ARE
PROCESSED
AT
THE
SAME
SPEED
CONCLUDING
REMARKS
THIS
PAPER
HAS
STUDIED
SEVERAL
FUNDAMENTAL
QUESTIONS
ABOUT
THE
DESIGN
OF
SPEED
SCALING
ALGORITHMS
THE
FOCUS
HAS
BEEN
ON
UNDERSTANDING
THE
STRUCTURE
OF
THE
OPTIMAL
ALGO
RITHM
THE
INTERACTION
BETWEEN
SPEED
SCALING
AND
SCHEDULING
AND
THE
IMPACT
OF
THE
SOPHISTICATION
OF
THE
SPEED
SCALER
THIS
HAS
LED
TO
A
NUMBER
OF
NEW
INSIGHTS
WHICH
ARE
SUMMARIZED
IN
THE
INTRODUCTION
THE
ANALYTIC
APPROACH
OF
THIS
PAPER
IS
DISTINCTIVE
IN
THAT
IT
CONSIDERS
BOTH
WORST
CASE
AND
STOCHASTIC
MODELS
THIS
COM
BINATION
OF
TECHNIQUES
IS
FUNDAMENTAL
IN
OBTAINING
TWO
OF
THE
SPEED
SCALING
WITH
AN
ARBITRARY
POWER
FUNCTION
NIKHIL
BANSAL
HO
LEUNG
CHAN
KIRK
PRUHS
WHAT
MATTERS
MOST
TO
THE
COMPUTER
DESIGN
ERS
AT
GOOGLE
IS
NOT
SPEED
BUT
POWER
LOW
POWER
BECAUSE
DATA
CENTERS
CAN
CONSUME
AS
MUCH
ELECTRICITY
AS
A
CITY
DR
ERIC
SCHMIDT
CEO
OF
GOOGLE
ABSTRACT
ALL
OF
THE
THEORETICAL
SPEED
SCALING
RESEARCH
TO
DATE
HAS
ASSUMED
THAT
THE
POWER
FUNCTION
WHICH
EXPRESSES
THE
POWER
CONSUMPTION
P
AS
A
FUNCTION
OF
THE
PROCESSOR
SPEED
IS
OF
THE
FORM
P
SΑ
WHERE
Α
IS
SOME
CONSTANT
MOTIVATED
IN
PART
BY
TECHNOLOGICAL
ADVANCES
WE
INITIATE
A
STUDY
OF
SPEED
SCALING
WITH
ARBITRARY
POWER
FUNCTIONS
WE
CONSIDER
THE
PROBLEM
OF
MINIMIZING
THE
TOTAL
FLOW
PLUS
ENERGY
OUR
MAIN
RESULT
IS
A
Ǫ
COMPETITIVE
ALGORITHM
FOR
THIS
PROBLEM
THAT
HOLDS
FOR
ESSENTIALLY
ANY
POWER
FUNCTION
WE
ALSO
GIVE
A
Ǫ
COMPETITIVE
ALGORITHM
FOR
THE
OBJECTIVE
OF
FRACTIONAL
WEIGHTED
FLOW
PLUS
ENERGY
EVEN
FOR
POWER
FUNCTIONS
OF
THE
FORM
SΑ
IT
WAS
NOT
PREVIOUSLY
KNOWN
HOW
TO
OBTAIN
COMPETITIVENESS
INDEPENDENT
OF
Α
FOR
THESE
PROBLEMS
WE
ALSO
INTRODUCE
A
MODEL
OF
ALLOWABLE
SPEEDS
THAT
GENERALIZES
ALL
KNOWN
MODELS
IN
THE
LITERATURE
INTRODUCTION
ENERGY
CONSUMPTION
HAS
BECOME
A
KEY
ISSUE
IN
THE
DESIGN
OF
MICROPROCESSORS
MAJOR
CHIP
MANUFACTURERS
SUCH
AS
INTEL
AMD
AND
IBM
NOW
PRODUCE
CHIPS
WITH
DYNAMICALLY
SCALABLE
SPEEDS
AND
PRODUCE
ASSOCIATED
SOFTWARE
THAT
ENABLES
AN
OPERATING
SYSTEM
TO
MANAGE
POWER
BY
SCALING
PROCESSOR
SPEED
WITHIN
THE
LAST
FEW
YEARS
THERE
HAS
BEEN
A
SIGNIFICANT
AMOUNT
OF
RESEARCH
ON
THE
SCHEDULING
PROBLEMS
THAT
ARISE
IN
THIS
SETTING
GENERALLY
THESE
PROBLEMS
HAVE
DUAL
OBJECTIVES
AS
ONE
WANTS
BOTH
TO
OPTIMIZE
SOME
SCHEDULE
QUALITY
OF
SERVICE
OBJECTIVE
FOR
EXAMPLE
TOTAL
FLOW
AND
SOME
POWER
RELATED
OBJECTIVE
FOR
EXAMPLE
THE
TOTAL
ENERGY
USED
IBM
T
J
WATSON
RESEARCH
P
O
BOX
YORKTOWN
HEIGHTS
NY
MAX
PLANCK
INSTITUT
FU
R
INFORMATIK
THIS
PAPER
WAS
DONE
WHEN
THE
AUTHOR
WAS
IN
UNIVERSITY
OF
PITTSBURGH
COMPUTER
SCIENCE
DEPARTMENT
UNIVERSITY
OF
PITTSBURGH
SUPPORTED
IN
PART
BY
AN
IBM
FACULTY
AWARD
AND
BY
NSF
GRANTS
CNS
CCF
IIS
AND
CCF
SCHEDULING
ALGORITHMS
FOR
THESE
PROBLEMS
HAVE
TWO
COMPONENTS
A
JOB
SELECTION
POLICY
THAT
DETERMINES
WHICH
JOB
TO
RUN
AND
A
SPEED
SCALING
POLICY
TO
DETERMINE
THE
SPEED
AT
WHICH
THE
PROCESSOR
IS
RUN
ALL
OF
THE
THEORETICAL
SPEED
SCALING
RESEARCH
TO
DATE
HAS
ASSUMED
THAT
THE
POWER
FUNCTION
WHICH
EXPRESSES
THE
POWER
CONSUMPTION
P
AS
A
FUNCTION
OF
THE
PROCESSOR
SPEED
IS
OF
THE
FORM
P
SΑ
WHERE
Α
IS
SOME
CONSTANT
LET
US
CALL
THIS
THE
TRADITIONAL
MODEL
THE
TRADITIONAL
MODEL
WAS
MOTIVED
BY
THE
FACT
THAT
IN
CMOS
BASED
PROCESSORS
THE
WELL
KNOWN
CUBE
ROOT
RULE
STATES
THAT
THE
SPEED
IS
APPROXIMATELY
THE
CUBE
ROOT
OF
THE
POWER
SO
HISTORICALLY
P
WAS
A
REASONABLE
ASSUMPTION
IN
THE
LITERATURE
ONE
FINDS
DIFFERENT
VARIATIONS
ON
THIS
TRADITIONAL
MODEL
BASED
ON
WHICH
SPEEDS
ARE
ALLOWABLE
MOST
OF
THE
LITERATURE
ASSUMES
THE
UNBOUNDED
SPEED
MODEL
IN
WHICH
A
PROCESSOR
CAN
BE
RUN
AT
ANY
REAL
SPEED
IN
THE
RANGE
SOME
OF
THE
LITERATURE
ASSUMES
THE
BOUNDED
SPEED
MODEL
IN
WHICH
THE
ALLOWABLE
SPEEDS
LIE
IN
SOME
REAL
INTERVAL
T
SOME
OF
THE
LITERATURE
ON
OFFLINE
ALGORITHMS
ASSUMES
THE
DISCRETE
SPEEDS
MODEL
IN
WHICH
THERE
ARE
A
FINITE
NUMBER
OF
ALLOWABLE
SPEEDS
OUR
MAIN
CONTRIBUTION
IN
THIS
PAPER
IS
TO
INITIATE
THEORETICAL
INVESTIGATIONS
INTO
SPEED
SCALING
PROBLEMS
WITH
MORE
GENERAL
POWER
FUNCTIONS
AND
DEVELOP
ALGO
RITHMIC
ANALYSIS
TECHNIQUES
FOR
THIS
SETTING
FOR
AN
EX
PLANATION
OF
THE
HISTORICAL
TECHNOLOGICAL
MOTIVATION
FOR
THE
TRADITIONAL
MODEL
AND
THE
CURRENT
TECHNOLOGICAL
MO
TIVATIONS
FOR
CONSIDERING
MORE
GENERAL
POWER
FUNCTIONS
SEE
SECTION
A
SECONDARY
CONTRIBUTION
IS
TO
INTRO
DUCE
A
MODEL
FOR
ALLOWABLE
SPEEDS
THAT
GENERALIZES
ALL
OF
VARIOUS
MODELS
FOUND
IN
THE
LITERATURE
WE
WILL
CONSIDER
THE
OBJECTIVE
OF
MINIMIZING
A
LINEAR
COMBINATION
OF
TOTAL
POSSIBLY
WEIGHTED
FLOW
AND
TOTAL
ENERGY
USED
OUR
THIRD
CONTRIBUTION
IS
TO
IMPROVE
ON
THE
KNOWN
RESULTS
FOR
THIS
IMPORTANT
FUNDAMENTAL
PROBLEM
OPTIMIZING
A
LINEAR
COMBINATION
OF
ENERGY
AND
TOTAL
FLOW
HAS
THE
FOLLOWING
NATURAL
INTERPRETATION
SUPPOSE
THAT
THE
USER
SPECIFIES
HOW
MUCH
IMPROVEMENT
IN
FLOW
CALL
THIS
AMOUNT
Ρ
IS
NECESSARY
TO
JUSTIFY
SPENDING
ONE
UNIT
OF
ENERGY
FOR
EXAMPLE
THE
USER
MIGHT
SPECIFY
THAT
HE
IS
WILLING
TO
SPEND
ERG
OF
ENERGY
FROM
THE
BATTERY
FOR
A
DECREASE
OF
MICRO
SECONDS
IN
FLOW
THEN
THE
OPTIMAL
SCHEDULE
FROM
THIS
USER
PERSPECTIVE
IS
THE
SCHEDULE
THAT
OPTIMIZES
Ρ
TIMES
THE
ENERGY
USED
PLUS
THE
TOTAL
FLOW
BY
CHANGING
THE
UNITS
OF
EITHER
ENERGY
OR
TIME
ONE
MAY
ASSUME
WITHOUT
LOSS
OF
GENERALITY
THAT
Ρ
WEIGHTED
FLOW
GENERALIZES
BOTH
TOTAL
FLOW
AND
TOTAL
AVERAGE
STRETCH
WHICH
IS
ANOTHER
COMMON
QOS
MEASURE
THE
STRETCH
SLOWDOWN
OF
A
JOB
IS
THE
FLOW
DIVIDED
BY
THE
WORK
OF
THE
JOB
WHEN
THE
USER
IS
AWARE
OF
THE
SIZE
OF
A
JOB
SAY
IF
THE
USER
KNOWS
THAT
HE
SHE
IS
DOWNLOADING
A
VIDEO
FILE
INSTEAD
OF
A
TEXT
FILE
THEN
PERHAPS
SLOWDOWN
IS
A
MORE
APPROPRIATE
MEASURE
OF
THE
HAPPINESS
OF
A
USER
THAN
FLOW
MANY
SERVER
IS
SAY
COMPLETED
AT
TIME
T
ONLY
CONTRIBUTES
TO
THE
INCREASE
IN
FRACTIONAL
FLOW
AT
TIME
T
THEY
THEN
SHOWED
THAT
THE
NATURAL
ALGORITHM
PROPOSED
IN
IS
COMPETITIVE
FOR
TOTAL
FLOW
PLUS
ENERGY
FOR
UNIT
WORK
JOBS
FOR
THE
MORE
GENERAL
SETTING
WHERE
JOBS
HAVE
ARBI
TRARY
SIZES
AND
ARBITRARY
WEIGHTS
AND
THE
OBJECTIVE
IS
WEIGHTED
FLOW
PLUS
ENERGY
BANSAL
PRUHS
AND
STEIN
CONSIDERED
THE
ALGORITHM
THAT
USES
HIGHEST
DENSITY
FIRST
HDF
FOR
JOB
SELECTION
AND
ALWAYS
RUNS
AT
A
POWER
EQUAL
TO
THE
FRACTIONAL
WEIGHT
OF
THE
UNFINISHED
JOBS
SYSTEMS
SUCH
AS
OPERATING
SYSTEMS
AND
DATABASES
HAVE
THEY
SHOWED
THAT
THIS
ALGORITHM
IS
O
Α
COMPETITIVE
MECHANISMS
THAT
ALLOW
THE
USER
OR
THE
SYSTEM
TO
GIVE
DIFFERENT
PRIORITIES
TO
DIFFERENT
JOBS
FOR
EXAMPLE
UNIX
HAS
THE
NICE
COMMAND
IN
A
SPEED
SCALING
SETTING
THE
FOR
FRACTIONAL
WEIGHTED
FLOW
PLUS
ENERGY
USING
AN
AMOR
TIZED
LOCAL
COMPETITIVENESS
ARGUMENT
USING
THE
KNOWN
RESOURCE
AUGMENTATION
ANALYSIS
OF
HDF
THEY
THEN
WEIGHT
OF
A
JOB
IS
INDICATIVE
OF
THE
FLOW
VERSUS
ENERGY
SHOWED
HOW
TO
OBTAIN
AN
O
COMPETITIVE
ALGO
TRADE
OFF
FOR
THIS
JOB
THE
USER
MAY
BE
WILLING
TO
SPEND
MORE
ENERGY
TO
REDUCE
THE
FLOW
OF
A
HIGHER
PRIORITY
JOB
THAN
FOR
A
LOWER
PRIORITY
JOB
THE
LITERATURE
ON
FLOW
ENERGY
IN
THE
TRADITIONAL
MODEL
LET
US
START
WITH
RESULTS
IN
THE
UNBOUNDED
SPEED
MODEL
PRUHS
UTHAISOMBUT
AND
WOEGINGER
GAVE
AN
EFFICIENT
OFFLINE
ALGORITHM
TO
FIND
THE
SCHEDULE
THAT
MINIMIZES
AVERAGE
FLOW
SUBJECT
TO
A
CONSTRAINT
ON
THE
AMOUNT
OF
ENERGY
USED
IN
THE
CASE
THAT
JOBS
HAVE
UNIT
WORK
THIS
ALGORITHM
CAN
ALSO
BE
USED
TO
FIND
OPTIMAL
SCHEDULES
WHEN
THE
OBJECTIVE
IS
A
LINEAR
COMBINATION
OF
TOTAL
FLOW
AND
ENERGY
USED
THEY
OBSERVED
THAT
IN
ANY
LOCALLY
OPTIMAL
SCHEDULE
ESSENTIALLY
EACH
JOB
I
IS
RUN
AT
A
POWER
PROPORTIONAL
TO
THE
NUMBER
OF
JOBS
THAT
WOULD
BE
DELAYED
IF
JOB
I
WAS
DELAYED
ALBERS
AND
FUJIWARA
PROPOSED
THE
NATURAL
ONLINE
SPEED
SCALING
ALGORITHM
THAT
ALWAYS
RUNS
AT
A
POWER
EQUAL
TO
THE
NUMBER
OF
UNFINISHED
JOBS
WHICH
IS
LOWER
BOUND
TO
THE
NUMBER
OF
JOBS
THAT
WOULD
BE
DELAYED
IF
THE
SELECTED
JOB
WAS
DELAYED
THEY
DID
NOT
ACTUALLY
ANALYZE
THIS
NATURAL
ALGORITHM
BUT
RATHER
ANALYZED
A
BATCHED
VARIATION
IN
WHICH
JOBS
THAT
ARE
RELEASED
WHILE
THE
CURRENT
BATCH
IS
RUNNING
ARE
IGNORED
UNTIL
THE
CURRENT
BATCH
FINISHES
THEY
SHOWED
THAT
FOR
UNIT
WORK
JOBS
THAT
THIS
BATCHED
ALGORITHM
IS
O
COMPETITIVE
BY
REASONING
DIRECTLY
ABOUT
THE
OPTIMAL
SCHEDULE
THIS
GAVE
A
COMPETITIVE
RATIO
OF
ABOUT
WHEN
THE
CUBE
ROOT
RULE
HOLDS
THEY
ALSO
GAVE
AN
EFFICIENT
OFFLINE
DYNAMIC
PROGRAMMING
ALGORITHM
BANSAL
PRUHS
AND
STEIN
CONSIDERED
THE
ALGORITHM
THAT
RUNS
AT
A
POWER
EQUAL
TO
THE
UNFINISHED
WORK
WHICH
IS
IN
GENERAL
A
BIT
LESS
THAN
THE
NUMBER
OF
UNFINISHED
JOBS
FOR
UNIT
WORK
JOBS
THEY
SHOWED
THAT
FOR
UNIT
WORK
JOBS
THIS
ALGORITHM
IS
COMPETITIVE
WITH
RESPECT
TO
THE
OBJECTIVE
OF
FRACTIONAL
FLOW
PLUS
ENERGY
USING
AN
AMORTIZED
LOCAL
COMPETITIVENESS
ARGUMENT
A
JOB
THAT
RITHM
FOR
INTEGRAL
WEIGHTED
FLOW
PLUS
ENERGY
THE
COM
PETITIVE
RATIO
WAS
A
BIT
LESS
THAN
WHEN
THE
CUBE
ROOT
RULE
HOLDS
RECENTLY
LAM
ET
AL
IMPROVED
THE
COMPETITIVE
RATIO
FOR
TOTAL
FLOW
PLUS
ENERGY
FOR
ARBITRARY
WORK
AND
UNIT
WEIGHT
JOBS
THEY
CONSIDERED
THE
JOB
SELECTION
ALGO
RITHM
SHORTEST
REMAINING
PROCESSING
TIME
SRPT
AND
THE
SPEED
SCALING
ALGORITHM
OF
RUNNING
AT
A
POWER
PRO
PORTIONAL
TO
THE
NUMBER
OF
UNFINISHED
JOBS
AND
PROVED
THAT
THIS
IS
O
Α
COMPETITIVE
FOR
FLOW
TIME
PLUS
EN
ERGY
WHEN
THE
CUBE
ROOT
RULE
HOLDS
THIS
COMPETITIVE
RATIO
WAS
ABOUT
THEIR
IMPROVEMENT
CAME
BY
REA
SONING
DIRECTLY
ABOUT
INTEGRAL
FLOW
TIME
INSTEAD
OF
ARGU
ING
FIRST
ABOUT
FRACTIONAL
FLOW
TIME
SPEED
SCALING
PA
PERS
PRIOR
TO
USED
POTENTIAL
FUNCTIONS
RELATED
TO
THE
FRACTIONAL
AMOUNT
OF
UNFINISHED
WORK
OR
WEIGHT
THE
MAIN
REASON
FOR
THIS
WAS
THAT
SUCH
A
POTENTIAL
FUNC
TION
VARIES
CONTINUOUSLY
WITH
TIME
WHICH
SUBSTANTIALLY
SIMPLIFIES
THE
PROOFS
AS
ONE
ONLY
NEEDS
TO
CONSIDER
IN
STANTANEOUS
STATES
OF
THE
ONLINE
AND
OFFLINE
ALGORITHMS
THE
MAIN
TECHNICAL
CONTRIBUTION
OF
WAS
THE
INTRO
DUCTION
OF
A
DIFFERENT
FORM
OF
CONTINUOUSLY
VARYING
PO
TENTIAL
FUNCTION
THAT
DEPENDS
ON
THE
INTEGRAL
NUMBER
OF
UNFINISHED
JOBS
BANSAL
ET
AL
EXTENDED
THE
RESULTS
OF
FOR
THE
UNBOUNDED
SPEED
MODEL
TO
THE
BOUNDED
SPEED
MODEL
THE
SPEED
SCALING
ALGORITHM
WAS
TO
RUN
AT
THE
MINIMUM
OF
THE
SPEED
RECOMMENDED
BY
THE
SPEED
SCALING
ALGORITHM
IN
THE
UNBOUNDED
SPEED
MODEL
AND
THE
MAXIMUM
SPEED
OF
THE
PROCESSOR
THE
CONTRIBUTION
THERE
WAS
TO
DEVELOP
THE
ALGORITHMIC
ANALYSIS
TECHNIQUES
NECESSARY
TO
ANALYZE
THIS
ALGORITHM
THE
RESULTS
FOR
THE
BOUNDED
SPEED
MODEL
IN
WERE
IMPROVED
IN
AGAIN
BY
REASONING
DIRECTLY
ABOUT
INTEGRAL
FLOW
AGAIN
SHOWED
COMPETITIVE
RATIOS
OF
O
Α
WHEN
THE
CUBE
ROOT
RULE
HOLDS
THE
OBTAINED
COMPETITIVE
RATIO
WAS
OUR
RESULTS
WE
ASSUME
THAT
THE
ALLOWABLE
SΑ
OUR
NEW
POTENTIAL
FUNCTION
NOT
ONLY
ALLOWS
US
TO
SPEEDS
ARE
A
COUNTABLE
COLLECTION
OF
DISJOINT
SUBINTERVALS
BREAK
THE
BARRIER
OF
Α
AND
IT
ALLOWS
US
TO
OBTAIN
O
OF
WE
ASSUME
THAT
ALL
THE
INTERVALS
EXCEPT
POSSI
BLY
THE
RIGHTMOST
INTERVAL
ARE
CLOSED
ON
BOTH
ENDS
THE
RIGHTMOST
INTERVAL
MAY
BE
OPEN
ON
THE
RIGHT
IF
THE
POWER
P
APPROACHES
INFINITY
AS
THE
SPEED
APPROACHES
THE
RIGHTMOST
ENDPOINT
OF
THAT
INTERVAL
WE
ASSUME
THAT
P
IS
NON
NEGATIVE
AND
P
IS
CONTINUOUS
AND
DIFFERENTIABLE
ON
ALL
BUT
COUNTABLY
MANY
POINTS
WE
ASSUME
THAT
EI
THER
THERE
IS
A
MAXIMUM
ALLOWABLE
SPEED
T
OR
THAT
THE
LIMIT
INFERIOR
OF
P
AS
APPROACHES
INFINITY
IS
NOT
ZERO
IF
THIS
CONDITION
DOESN
T
HOLD
THEN
THEN
THE
OPTI
MAL
SPEED
SCALING
POLICY
IS
TO
RUN
AT
INFINITE
SPEED
LET
US
CALL
THIS
THE
GENERAL
MODEL
WE
GIVE
TWO
MAIN
RESULTS
IN
THE
GENERAL
MODEL
THEOREM
CONSIDER
THE
SCHEDULING
ALGORITHM
THAT
USES
SHORTEST
REMAINING
PROCESSING
TIME
SRPT
FOR
JOB
SELECTION
AND
POWER
EQUAL
TO
ONE
MORE
THAN
THE
NUMBER
OF
UNFINISHED
JOBS
FOR
SPEED
SCALING
IN
THE
GENERAL
MODEL
THIS
SCHEDULING
ALGORITHM
IS
Ǫ
COMPETITIVE
FOR
THE
OBJECTIVE
OF
TOTAL
FLOW
PLUS
ENERGY
ON
ARBITRARY
WORK
UNIT
WEIGHT
JOBS
THEOREM
CONSIDER
THE
SCHEDULING
ALGORITHM
THAT
USES
HIGHEST
DENSITY
FIRST
HDF
FOR
JOB
SELECTION
AND
POWER
EQUAL
TO
THE
FRACTIONAL
WEIGHT
OF
THE
UNFINISHED
JOBS
FOR
SPEED
SCALING
IN
THE
GENERAL
MODEL
THIS
SCHEDUL
ING
ALGORITHM
IS
Ǫ
COMPETITIVE
FOR
THE
OBJECTIVE
OF
FRACTIONAL
WEIGHTED
FLOW
PLUS
ENERGY
ON
ARBITRARY
WORK
ARBITRARY
WEIGHT
JOBS
WE
ESTABLISH
THESE
RESULTS
THROUGH
AN
AMORTIZED
LOCAL
COMPETITIVENESS
ARGUMENT
AS
IN
OUR
POTENTIAL
FUNCTION
IS
BASED
ON
THE
INTEGRAL
NUMBER
OF
UNFINISHED
JOBS
HOWEVER
OUR
POTENTIAL
FUNCTION
IS
QUITE
DIFFERENT
AND
MORE
GENERAL
THAN
THE
POTENTIAL
FUNCTION
CONSIDERED
IN
WHILE
THEOREM
DEALS
WITH
INTEGRAL
FLOW
THEOREM
ONLY
HOLDS
FOR
FRACTIONAL
WEIGHTED
FLOW
OBTAINING
A
COMPETITIVE
RATIO
INDEPENDENT
OF
Α
FOR
THE
OBJECTIVE
OF
INTEGRAL
WEIGHTED
FLOW
PLUS
ENERGY
IS
RULED
OUT
SINCE
RESOURCE
AUGMENTATION
IS
REQUIRED
TO
ACHIEVE
O
COMPETITIVENESS
FOR
THE
OBJECTIVE
OF
WEIGHTED
FLOW
ON
A
FIXED
SPEED
PROCESSOR
LET
US
CONSIDER
WHAT
THESE
THEOREMS
SAY
IN
THE
TRADITIONAL
MODEL
THEOREM
SLIGHTLY
IMPROVES
THE
BEST
KNOWN
COMPETITIVE
RATIOS
WHEN
THE
CUBE
ROOT
RULE
HOLDS
IN
BOTH
THE
UNBOUNDED
AND
BOUNDED
SPEED
MODELS
THE
POTENTIAL
FUNCTIONS
USED
IN
ALL
PREVIOUS
PAPERS
ARE
SPECIFICALLY
TAILORED
TOWARD
THE
FUNCTION
P
SΑ
MOREOVER
THESE
POTENTIAL
FUNCTIONS
CAN
COMPETITIVENESS
FOR
GENERAL
POWER
FUNCTIONS
TECHNOLOGICAL
MOTIVATIONS
THE
POWER
USED
BY
A
PROCESSOR
CAN
BE
PARTITIONED
INTO
DYNAMIC
POWER
THE
POWER
USED
BY
SWITCHING
WHEN
DOING
COMPUTATIONS
AND
STATIC
LEAKAGE
POWER
WHICH
IS
THE
POWER
LOST
IN
ABSENCE
OF
ANY
SWITCHING
HISTORICALLY
SAY
UP
UNTIL
YEARS
AGO
THE
STATIC
POWER
USED
BY
A
PROCESSOR
WAS
NEGLIGIBLE
COM
PARED
TO
THE
DYNAMIC
POWER
DYNAMIC
POWER
IS
ROUGHLY
PROPORTIONAL
TO
SV
WHERE
V
IS
THE
VOLTAGE
HOWEVER
AS
THE
MINIMUM
VOLTAGE
REQUIRED
TO
DRIVE
THE
MICROPRO
CESSOR
AT
A
DESIRED
SPEED
IS
APPROXIMATELY
PROPORTIONAL
TO
THE
FREQUENCY
THIS
LEADS
TO
THE
WELL
KNOWN
CUBE
ROOT
RULE
THAT
THE
SPEED
IS
ROUGHLY
PROPORTIONAL
TO
THE
CUBE
ROOT
OF
THE
POWER
P
OR
EQUIVALENTLY
P
THE
POWER
IS
PROPORTIONAL
TO
THE
SPEED
CUBED
HOWEVER
CURRENTLY
THE
STATIC
POWER
USED
BY
THE
COMMON
PROCESSORS
MANUFACTURED
BY
INTEL
AND
AMD
IS
NOW
COMPARABLE
TO
THE
DYNAMIC
POWER
THE
REASON
FOR
THIS
IS
THAT
TO
INCREASE
PROCESSOR
SPEEDS
TRANSISTOR
SIZES
AND
DESIRED
TRANSISTOR
SWITCHING
DELAYS
MUST
DE
CREASE
TO
OBTAIN
A
SMALLER
SWITCHING
DELAY
THE
THRESH
OLD
VOLTAGE
FOR
THE
TRANSISTOR
MUST
BE
DECREASED
UN
FORTUNATELY
SUBTHRESHOLD
LEAKAGE
CURRENT
INCREASES
EX
PONENTIALLY
AS
THRESHOLD
VOLTAGE
DECREASES
THIS
SUGGESTS
MODELING
THE
SPEED
TO
POWER
FUNCTION
AS
SOME
THING
LIKE
P
SΑ
C
WHERE
THERE
IS
SOME
RANGE
OF
SPEEDS
SMIN
SMAX
THAT
THE
PROCESSOR
CAN
RUN
AT
HERE
Α
COMES
FROM
THE
CUBE
ROOT
RULE
FOR
DYNAMIC
POWER
AND
C
IS
A
CONSTANT
SPECIFYING
THE
STATIC
POWER
LOSS
THERE
IS
HOWEVER
GOOD
MOTIVATION
FOR
CONSIDERING
MORE
GENERAL
SPEED
TO
POWER
FUNCTIONS
WE
WILL
STATE
THREE
MORE
EXAMPLES
HERE
EXAMPLE
IN
GENERAL
THE
HIGHER
SPEED
THAT
ONE
CAN
SCALE
A
CORE
OR
A
PROCESSOR
THE
GREATER
THE
STATIC
POWER
BECAUSE
THE
THRESHOLD
VOLTAGE
MUST
BE
LESS
IT
HAS
BEEN
PROPOSED
THAT
IT
MIGHT
BE
ADVANTAGEOUS
TO
BUILD
A
PROCESSOR
CONSISTING
OF
DIFFERENT
CIRCUITRY
FOR
RUNNING
JOBS
AT
DIFFERENT
SPEEDS
SPEED
RANGES
EACH
CIRCUITRY
WOULD
HAVE
DIFFERENT
SPEED
RANGES
DIFFERENT
STATIC
POWERS
DUE
TO
THE
DIFFERENT
THRESHOLD
VOLTAGES
AND
DIFFERENT
POWER
FUNCTIONS
THUS
THE
SPEED
SCALING
ALGORITHM
MIGHT
BE
ABLE
TO
CHOOSE
AMONG
A
COLLECTION
OF
POWER
FUNCTIONS
OF
THE
FORM
PI
AISΑ
CI
WHERE
EACH
PI
IS
APPLICABLE
OVER
A
COLLECTION
SI
SI
SI
KI
OF
SPEEDS
NOTE
THAT
IN
THIS
CONTEXT
THAT
WE
CAN
NOT
SCALE
OUR
UNITS
SO
THAT
THE
MULTIPLICATIVE
NOT
BE
USED
TO
SHOW
COMPETITIVE
RATIOS
OF
O
Α
FOR
CONSTANT
IS
ALWAYS
IN
THIS
SETTING
THE
POWER
FUNCTION
ARBITRARY
WORK
JOBS
SO
WHILE
THE
COMPETITIVE
RATIOS
WERE
O
COMPETITIVE
FOR
A
FIXED
Α
THEY
WERE
NOT
O
COMPETITIVE
FOR
A
GENERAL
POWER
FUNCTION
OF
THE
FORM
OF
MIGHT
BE
OF
THE
FORM
P
MIN
AISΑ
CI
I
SI
EXAMPLE
SUBTHRESHOLD
LEAKAGE
CURRENT
POWER
IS
NOT
INDEPENDENT
OF
TEMPERATURE
THE
TEMPERATURE
IN
TURN
DEPENDS
ON
THE
SPEED
THAT
THE
PROCESSOR
IS
RUN
SOME
DISCUSSION
OF
HOW
LEAKAGE
CURRENT
POWER
IS
RELATED
TO
TEMPERATURE
CAN
BE
FOUND
IN
BUT
IT
SEEMS
THAT
THIS
ISSUE
IS
PERHAPS
NOT
SO
WELL
UNDERSTOOD
BUT
IN
ANY
CASE
THERE
APPEARS
TO
BE
NO
REASON
TO
PRESUME
THAT
THE
RESULTING
POWER
FUNCTION
WOULD
BE
OF
THE
FORM
SΑ
EXAMPLE
ANOTHER
MOTIVATION
FOR
CONSIDERING
GEN
ERAL
SPEED
TO
POWER
FUNCTIONS
IS
AT
THE
DATA
CENTER
LEVEL
THE
OPENING
QUOTE
INDICATES
THE
IMPORTANCE
OF
POWER
MANAGEMENT
AT
THE
DATA
CENTER
LEVEL
A
NICE
INVESTI
GATION
BY
GOOGLE
RESEARCHERS
INTO
THE
ENERGY
THAT
A
DATA
CENTER
COULD
SAVE
FROM
SPEED
SCALING
CAN
BE
FOUND
IN
ULTIMATELY
WHAT
A
DATA
CENTER
OPERATOR
MIGHT
CARE
ABOUT
IS
THE
COST
OF
THE
ENERGY
USED
NOT
THE
ACTUAL
AMOUNT
OF
ENERGY
USED
THESE
CAN
BE
QUITE
DIFFERENT
BECAUSE
NORMALLY
THE
CONTRACT
THAT
THE
DATA
CENTER
HAS
WITH
THE
ELECTRICAL
SUPPLIERS
CAN
HAVE
DRASTIC
INCREASES
IN
COST
IF
VARIOUS
POWER
THRESHOLDS
ARE
EXCEEDED
SO
HERE
THE
RELEVANT
FUNCTION
THAT
ONE
WOULD
CARE
ABOUT
IS
THE
SPEED
TO
COST
SAY
MEASURED
IN
DOLLARS
PER
SECOND
FUNCTION
NOT
THE
SPEED
TO
POWER
FUNCTION
THERE
IS
NO
REASON
WHY
THESE
CONTRACTS
NEED
TO
HAVE
A
COST
THAT
RISES
AS
SΑ
PRELIMINARIES
AN
INSTANCE
CONSISTS
OF
N
JOBS
WHERE
JOB
I
HAS
A
RELEASE
TIME
RI
AND
A
POSITIVE
WORK
YI
IN
SOME
CASES
EACH
JOB
MAY
HAVE
A
POSITIVE
INTEGER
WEIGHT
WI
AN
ONLINE
SCHEDULER
IS
NOT
AWARE
OF
JOB
I
UNTIL
TIME
RI
AND
AT
TIME
RI
LEARNS
YI
AND
WEIGHT
WI
FOR
EACH
TIME
A
SCHEDULER
SPECIFIES
A
JOB
TO
BE
RUN
AND
A
SPEED
AT
WHICH
THE
PROCESSOR
IS
RUN
WE
ASSUME
THAT
PREEMPTION
IS
ALLOWED
THAT
IS
A
JOB
MAY
BE
SUSPENDED
AND
LATER
RESTARTED
FROM
THE
POINT
OF
SUSPENSION
A
JOB
I
COMPLETES
ONCE
YI
UNITS
OF
WORK
HAVE
BEEN
PERFORMED
ON
I
THE
SPEED
IS
THE
RATE
AT
WHICH
WORK
IS
COMPLETED
A
JOB
WITH
WORK
Y
RUN
AT
A
CONSTANT
SPEED
COMPLETES
IN
Y
SECONDS
THE
FLOW
FI
OF
A
JOB
I
IS
ITS
COMPLETION
TIME
MINUS
ITS
RELEASE
TIME
THE
Φ
T
SUCH
THAT
THE
FOLLOWING
TWO
CONDITIONS
HOLD
BOUNDARY
CONDITION
Φ
IS
ZERO
BEFORE
ANY
JOB
IS
RELEASED
AND
Φ
IS
NON
NEGATIVE
AFTER
ALL
JOBS
ARE
FINISHED
GENERAL
CONDITION
AT
ANY
TIME
T
DΦ
T
GA
T
DT
C
GOPT
T
TO
PROVE
THE
GENERAL
CONDITION
IT
SUFFICES
TO
SHOW
THAT
Φ
DOES
NOT
INCREASE
WHEN
A
JOB
ARRIVES
OR
IS
COMPLETED
BY
A
OR
OPT
AND
EQUATION
IS
TRUE
DURING
ANY
PERIOD
OF
TIME
WITHOUT
JOB
ARRIVAL
OR
COMPLETION
BY
INTEGRATING
EQUATION
WE
CAN
SEE
THAT
A
IS
C
COMPETITIVE
FOR
H
FOR
MORE
INFORMATION
SEE
FLOW
PLUS
ENERGY
OUR
GOAL
IN
THIS
SECTION
IS
TO
PROVE
THEOREM
THAT
IN
THE
GENERAL
MODEL
SRPT
PLUS
THE
NATURAL
SPEED
SCALING
ALGORITHM
IS
Ǫ
COMPETITIVE
FOR
THE
OBJECTIVE
OF
TOTAL
FLOW
PLUS
ENERGY
WE
START
BY
SHOWING
THAT
THIS
ALGORITHM
IS
COMPETITIVE
IF
P
ALSO
SATISFIES
THE
FOLLOWING
ADDITIONAL
SPECIAL
CONDITIONS
P
IS
DEFINED
AND
CONTINUOUS
AND
DIFFERENTIABLE
AT
ALL
SPEEDS
IN
P
P
IS
STRICTLY
INCREASING
P
IS
STRICTLY
CONVEX
THAT
IS
FOR
A
B
AND
FOR
ALL
P
IS
THE
CASE
THAT
PP
A
P
P
B
P
PA
P
B
P
IS
UNBOUNDED
THAT
IS
FOR
ALL
C
THE
EXISTS
A
SPEED
SUCH
THAT
P
C
THEN
IN
SECTION
WE
SHOW
HOW
TO
EXTEND
THE
ANALYSIS
TO
THE
CASE
WHEN
THESE
SPECIAL
CONDITIONS
DO
NOT
HOLD
AT
THE
COST
OF
AN
ARBITRARILY
SMALL
INCREASE
IN
THE
COMPETITIVE
RATIO
DEFINITION
OF
ONLINE
ALGORITHM
A
ALGORITHM
A
N
N
TOTAL
FLOW
IS
I
FI
THE
WEIGHTED
FLOW
IS
I
WI
FI
LET
US
QUICKLY
REVIEW
AMORTIZED
LOCAL
COMPETITIVE
NESS
ANALYSIS
CONSIDER
AN
OBJECTIVE
G
LET
GA
T
BE
SCHEDULES
THE
UNFINISHED
JOB
WITH
THE
LEAST
REMAINING
UNFINISHED
WORK
AND
RUNS
AT
SPEED
ST
WHERE
T
T
THE
INCREASE
IN
THE
OBJECTIVE
IN
THE
SCHEDULE
FOR
ALGO
RITHM
A
AT
TIME
T
SO
WHEN
G
IS
TOTAL
FLOW
PLUS
ENERGY
GA
T
IS
P
ST
NT
WHERE
ST
IS
THE
SPEED
FOR
A
AT
T
P
NA
IF
NA
A
IF
NT
A
A
A
TIME
T
AND
NT
IS
THE
NUMBER
OF
UNFINISHED
JOBS
FOR
A
AT
WHERE
NT
IS
THE
NUMBER
OF
UNFINISHED
JOBS
FOR
A
AT
TIME
T
THIS
IS
BECAUSE
ENERGY
IS
POWER
INTEGRATED
OVER
TIME
AND
TOTAL
FLOW
IS
THE
NUMBER
OF
UNFINISHED
JOBS
INTEGRATED
OVER
TIME
LET
OPT
BE
THE
OFFLINE
ADVERSARY
THAT
OPTIMIZES
G
AND
A
BE
SOME
ALGORITHM
TO
PROVE
A
IS
C
COMPETITIVE
IT
SUFFICES
TO
GIVE
A
POTENTIAL
FUNCTION
TIME
T
AS
P
AND
P
IS
STRICTLY
INCREASING
CONTINUOUS
AND
UNBOUNDED
P
I
EXISTS
AND
IS
UNIQUE
FOR
ALL
INTEGERS
I
THUS
A
IS
WELL
DEFINED
DEFINITION
OF
POTENTIAL
FUNCTION
Φ
LET
OPT
BE
THE
OFFLINE
ADVERSARY
THAT
MINIMIZES
TOTAL
FLOW
PLUS
ENERGY
WE
CAN
ASSUME
WITHOUT
LOSS
OF
GENERALITY
THAT
OPT
RUNS
SRPT
AT
ANY
TIME
T
LET
NT
BE
THE
NUMBER
THE
FIRST
EQUALITY
IS
OBTAINED
BY
INTEGRATION
BY
PARTS
AND
OF
UNFINISHED
JOBS
IN
OPT
LET
NT
Q
AND
NT
Q
BE
THE
SECOND
EQUALITY
IS
OBTAINED
BY
LETTING
Y
G
X
A
O
THE
NUMBER
OF
UNFINISHED
JOBS
WITH
REMAINING
SIZE
AT
LEAST
Q
IN
A
AND
OPT
RESPECTIVELY
AT
TIME
T
LET
NT
Q
MAX
NT
Q
NT
Q
WE
DEFINE
THE
THE
POTENTIAL
FUNCTION
Φ
T
F
NT
Q
DQ
WHERE
F
IS
DEFINED
BY
F
I
F
I
F
I
P
P
I
AND
P
X
IS
THE
DERIVATIVE
OF
P
X
NOTE
THAT
P
P
I
SIMPLY
MEANS
SUBSTITUTING
X
P
I
INTO
P
X
SINCE
P
X
IS
DIFFERENTIABLE
P
X
IS
WELL
DEFINED
HENCE
F
IS
WELL
DEFINED
FOR
ALL
INTEGER
I
AS
BOTH
P
X
AND
P
I
ARE
INCREASING
WE
HAVE
F
I
F
I
F
J
F
J
IF
I
J
WE
DENOTE
I
F
I
F
I
FOR
SIMPLICITY
WE
NOW
WISH
TO
ESTABLISH
A
CRUCIAL
TECHNICAL
LEMMA
LEMMA
WHICH
RELIES
ON
THE
WELL
KNOWN
YOUNG
INEQUALITY
THEOREM
YOUNG
INEQUALITY
LET
G
BE
A
REAL
VALUED
CONTINUOUS
AND
STRICTLY
INCREASING
FUNCTION
ON
C
WITH
C
IF
G
AND
A
B
SUCH
THAT
A
C
AND
B
G
G
C
THEN
G
X
DX
G
X
DX
AB
G
WHERE
G
IS
THE
INVERSE
FUNCTION
OF
G
LEMMA
LET
P
BE
A
STRICTLY
INCREASING
STRICTLY
CONVEX
CONTINUOUS
AND
DIFFERENTIABLE
FUNCTION
LET
I
SA
SO
BE
ANY
REAL
THEN
P
P
I
SA
SO
SA
P
I
P
P
I
P
SO
I
THE
LEMMA
FOLLOWS
BY
ADDING
SAP
P
I
TO
BOTH
SIDES
WE
ARE
NOW
READY
TO
PROCEED
WITH
OUR
AMORTIZED
LOCAL
COMPETITIVENESS
ARGUMENT
FOR
THE
BOUNDARY
CONDITION
WE
OBSERVE
THAT
BEFORE
ANY
JOB
IS
RELEASED
AND
AFTER
ALL
JOBS
ARE
FINISHED
NT
Q
FOR
ALL
Q
SO
Φ
FOR
THE
GENERAL
CONDITION
WE
OBSERVE
THAT
WHEN
A
JOB
IS
RELEASED
NT
Q
IS
NOT
CHANGED
FOR
ALL
Q
SO
Φ
IS
UNCHANGED
WHEN
A
JOB
IS
COMPLETED
BY
A
OR
OPT
NT
Q
IS
CHANGED
ONLY
AT
THE
SINGLE
POINT
OF
Q
WHICH
DOES
NOT
AFFECT
THE
INTEGRATION
SO
Φ
IS
ALSO
UNCHANGED
IT
REMAINS
TO
SHOW
AT
ANY
TIME
T
DURING
A
TIME
INTERVAL
WITHOUT
JOB
ARRIVAL
OR
COMPLETION
NT
P
ST
D
Φ
T
NT
P
ST
A
A
DT
O
O
THE
REST
OF
THIS
PROOF
CONSIDERS
ANY
SUCH
TIME
T
AND
PROVES
EQUATION
WE
OMIT
T
FROM
THE
SUPERSCRIPT
AND
THE
PARAMETER
FOR
CONVENIENCE
FIRST
OBSERVE
THAT
IF
NA
THEN
N
Q
FOR
ALL
Q
AND
D
Φ
THUS
EQUATION
TRIVIALLY
HOLDS
HENCEFORTH
WE
ASSUME
NA
LET
QA
BE
THE
REMAINING
SIZE
OF
THE
JOB
WITH
THE
SHORTEST
REMAINING
SIZE
IN
A
QA
EXISTS
AS
NA
NOTE
THAT
A
IS
PROCESSING
A
JOB
OF
SIZE
QA
DEFINE
QO
SIMILARLY
FOR
OPT
QO
IF
NO
CONSIDER
AN
INFINITESIMAL
INTERVAL
OF
TIME
T
T
DT
THE
PROCESSING
OF
A
CAUSES
NA
Q
TO
DECREASE
BY
FOR
Q
QA
SADT
QA
SIMILARLY
THE
PROCESSING
OF
OPT
CAUSES
NO
Q
TO
DECREASE
BY
FOR
Q
QO
SODT
QO
DENOTE
THE
CHANGE
IN
Φ
DURING
THIS
TIME
INTERVAL
AS
DΦ
WE
CONSIDER
THREE
CASES
DEPENDING
ON
WHETHER
NO
NA
NO
NA
OR
NO
NA
CASE
NO
NA
WE
SHOW
THAT
DΦ
AS
FOLLOWS
AT
TIME
T
NO
Q
IS
GREATER
THAN
NA
Q
FOR
Q
IN
QO
SODT
QO
THUS
AT
TIME
T
DT
N
Q
IS
STILL
AT
LEAST
N
Q
FOR
Q
IN
O
A
PROOF
SINCE
P
IS
STRICTLY
INCREASING
AND
STRICTLY
CONVEX
P
AND
P
X
IS
STRICTLY
INCREASING
THUS
BY
YOUNG
INEQUALITY
WITH
G
X
P
X
A
SO
AND
B
P
P
I
WE
HAVE
SOP
P
I
SO
P
P
I
QO
SODT
QO
IT
MEANS
THAT
N
Q
REMAINS
ZERO
IN
THIS
INTERVAL
AND
Φ
IS
NOT
INCREASED
DUE
TO
THE
PROCESSING
OF
OPT
THE
PROCESSING
OF
A
ONLY
DECREASES
Φ
OR
LEAVES
Φ
UNCHANGED
HENCE
DΦ
THUS
IMPLIES
THAT
D
Φ
AND
NA
P
SA
D
Φ
NA
NA
SO
HOLDS
G
X
DX
P
G
X
DX
G
P
I
CASE
NO
NA
WE
SHOW
THAT
EITHER
DΦ
NA
NO
SA
SO
DT
OR
DΦ
NA
NO
SA
SO
DT
AS
FOLLOWS
CONSIDER
SUBCASES
DEPENDING
ON
WHETHER
P
SO
XG
X
LG
I
G
P
I
XD
G
X
QA
IS
SMALLER
THAN
EQUAL
TO
OR
BIGGER
THAN
QO
ASSUME
QA
QO
AS
NA
Q
DECREASES
BY
FOR
Q
IN
P
SO
G
P
W
P
I
P
SO
G
P
I
P
I
I
G
Y
DY
QA
SADT
QA
BY
THE
DEFINITION
OF
Φ
THE
CHANGE
IN
Φ
DUE
TO
A
IS
F
NA
QA
NO
QA
F
NA
QA
NO
QA
SADT
NA
QA
NO
QA
SADT
SINCE
QA
QO
WE
HAVE
NO
QA
NO
QO
NO
SO
THE
CHANGE
IN
Φ
DUE
TO
A
IS
NA
NO
SADT
WHERE
THE
LAST
INEQUALITY
FOLLOWS
FROM
SA
P
NA
P
NA
NO
SIMILARLY
NA
NO
SA
SO
AS
N
Q
DECREASES
BY
FOR
Q
IN
Q
DT
Q
BY
P
P
NA
NO
SA
SO
SETTING
I
NA
NO
O
O
O
O
THE
DEFINITION
OF
Φ
THE
CHANGE
IN
Φ
DUE
TO
OPT
IN
LEMMA
WE
HAVE
THAT
IS
AT
MOST
F
NA
QO
NO
QO
F
NA
QO
NO
QO
SODT
NA
QO
NO
QO
SODT
SINCE
P
P
NA
NO
SA
SO
Q
Q
WE
HAVE
N
Q
N
Q
THUS
SA
P
NA
NO
P
P
NA
NO
A
O
A
O
A
A
NA
QO
NO
QO
NA
QA
NO
QO
AS
I
J
FOR
I
J
THE
CHANGE
IN
Φ
DUE
TO
OPT
IS
AT
MOST
NA
QA
NO
QO
SODT
NA
NO
SODT
ADDING
UP
THE
CHANGE
IN
Φ
DUE
TO
A
AND
OPT
WE
OBTAIN
THAT
DΦ
NA
NO
SA
SO
DT
P
SO
NA
NO
P
SO
NA
NO
WHERE
THE
LAST
INEQUALITY
FOLLOWS
FROM
SA
P
NA
P
NA
NO
THUS
D
Φ
SO
ASSUME
QA
QO
IF
SA
SO
N
Q
NA
Q
NO
Q
DECREASES
BY
FOR
Q
IN
QA
SADT
QA
SODT
HENCE
THE
CHANGE
IN
Φ
IS
F
NA
QA
NO
QA
F
NA
QA
NO
QA
SA
SO
DT
NA
NO
SA
SO
DT
ELSE
IF
SA
SO
N
Q
NA
Q
NO
Q
INCREASES
BY
FOR
Q
IN
QA
SODT
QA
SADT
HENCE
THE
CHANGE
IN
Φ
IS
F
NA
QA
NO
QA
F
NA
QA
NO
QA
SO
SA
DT
NA
SA
SO
DT
ASSUME
QA
QO
AS
NA
Q
DECREASES
BY
FOR
Q
IN
QA
SADT
QA
THE
CHANGE
IN
Φ
DUE
TO
A
IS
F
NA
QA
NO
QA
F
NA
QA
NO
QA
SADT
NA
QA
NO
QA
SADT
SINCE
QA
QO
WE
HAVE
NO
QA
NO
QO
THUS
NA
QA
NO
QA
NA
QA
NO
QO
AS
I
J
FOR
I
J
THE
CHANGE
IN
Φ
DUE
TO
A
IS
AT
MOST
NA
QA
NO
QO
SADT
NA
NO
SADT
ON
THE
OTHER
HAND
NO
Q
DECREASES
BY
FOR
Q
IN
QO
SODT
QO
SO
THE
CHANGE
IN
Φ
DUE
TO
OPT
IS
AT
MOST
F
NA
QO
NO
QO
F
NA
QO
NO
QO
SODT
NA
QO
NO
QO
SODT
SINCE
QA
QO
NA
QO
NA
QA
NA
AND
THE
CHANGE
IN
Φ
DUE
TO
OPT
IS
AT
MOST
NA
NO
SODT
SUMMING
THE
CHANGE
IN
Φ
DUE
TO
A
AND
OPT
DΦ
NA
NO
SA
SO
DT
COMBINING
THE
ABOVE
SUBCASES
IT
IMPLIES
THAT
IF
NO
NA
THEN
EITHER
D
Φ
NA
SA
SO
OR
DT
D
Φ
NA
SA
SO
DT
NOTE
THAT
NA
NO
SA
SO
P
P
NA
NO
SA
SO
SETTING
I
NA
NO
IN
LEMMA
WE
HAVE
THAT
P
P
NA
NO
SA
SO
SA
P
NA
NO
P
P
NA
NO
P
SO
NA
NO
P
SO
NA
NO
IN
BOTH
CASES
IT
FOLLOWS
THAT
D
NA
P
SA
DT
Φ
NA
NA
SO
P
SO
NO
SO
EQUATION
IS
TRUE
IF
NO
NA
CASE
NO
NA
WE
SHOW
THAT
DΦ
OR
DΦ
NA
NO
SA
SO
DT
AS
FOLLOWS
AGAIN
WE
CONSIDER
SUBCASES
DEPENDING
WHETHER
QA
QO
QA
QO
OR
QA
QO
ASSUME
QA
QO
THEN
N
Q
REMAINS
ZERO
FOR
Q
QA
SADT
QA
AND
Q
QO
SODT
QO
THUS
N
Q
IS
NOT
CHANGED
FOR
ANY
Q
AND
D
Φ
ASSUME
QA
QO
IF
SA
SO
N
Q
REMAINS
ZERO
FOR
Q
QA
SADT
QA
WHICH
ALSO
CONTAINS
QO
SODT
QO
THUS
N
Q
IS
UNCHANGED
FOR
ANY
Q
AND
D
Φ
ELSE
IF
SA
SO
N
Q
INCREASES
BY
FOR
Q
QA
SODT
QA
SADT
THE
CHANGE
IN
Φ
IS
F
NA
QA
NO
QO
F
NA
QA
NO
QO
SA
SO
DT
NA
NO
SA
SO
DT
IF
QA
QO
IT
IS
IDENTICAL
TO
SUBCASE
OF
THE
CASE
NO
NA
SO
DΦ
NA
NO
SA
SO
DT
IT
IMPLIES
THAT
D
Φ
OR
D
Φ
NA
NO
SA
SO
DT
SIMILAR
ARGUMENT
AS
BEFORE
SHOWS
THAT
EQUATION
IS
TRUE
IF
NO
NA
THIS
COMPLETES
THE
ANALYSIS
WHEN
P
SATISFIES
THE
SPECIAL
CONDITIONS
THAT
WE
IMPOSED
AT
THE
START
OF
THIS
SECTION
REMOVING
THE
SPECIAL
CONDITIONS
ON
P
CON
SIDER
AN
ARBITRARY
POWER
FUNCTION
P
IN
THE
GENERAL
MODEL
WE
EXPLAIN
HOW
TO
MODIFY
P
SO
THAT
IT
MEETS
THE
SPECIAL
CONDITIONS
WITHOUT
SIGNIFICANTLY
RAISING
THE
COM
PETITIVE
RATIO
IF
P
IS
NOT
INCREASING
ONE
CAN
MAKE
P
UNDEFINED
ON
THOSE
SPEEDS
WHERE
THERE
IS
A
GREATER
SPEED
THAT
CONSUMES
LESS
POWER
SIMILARLY
WE
CAN
MAKE
P
CONVEX
BY
ELIMINATING
ANY
POINTS
IN
A
SUBINTERVAL
A
B
TIME
T
IS
ST
P
WT
A
A
OF
SPEEDS
WHERE
THE
LINE
SEGMENT
BETWEEN
A
P
A
AND
B
P
B
LIES
BELOW
THE
CURVE
P
WE
NOW
ARGUE
THAT
THE
ONLINE
ALGORITHM
A
WITH
THE
NEW
POWER
FUNCTION
CAN
SIMULATE
RUNNING
A
SPEED
A
B
IN
THE
OLD
POWER
FUNCTION
ALGORITHM
A
CAN
SIMULATE
ANY
SPEED
A
B
BY
ALTERNATELY
RUNNING
AT
SPEEDS
A
AND
B
SPECIFICALLY
ASSUME
PA
P
B
FOR
SOME
P
WE
CAN
RUN
AT
SPEED
A
FOR
A
P
FRACTION
OF
THE
TIME
AND
RUN
AT
SPEED
B
FOR
A
P
FRACTION
OF
THE
TIME
IT
GIVES
AN
EFFECTIVE
SPEED
OF
AND
THE
ENERGY
USAGE
IS
PP
A
P
P
B
IF
THE
SLOWEST
SPEED
ON
WHICH
P
IS
DEFINED
IS
NOT
THEN
CAN
SHIFT
P
DOWN
BY
REDEFINING
WHERE
WT
IS
THE
TOTAL
FRACTIONAL
WEIGHT
OF
ALL
UNFINISHED
JOBS
FOR
A
AT
TIME
T
DEFINITION
OF
THE
POTENTIAL
FUNCTION
Γ
LET
OPT
BE
THE
OFFLINE
ADVERSARY
THAT
MINIMIZES
FRACTIONAL
WEIGHTED
FLOW
PLUS
ENERGY
AT
ANY
TIME
T
LET
WT
BE
THE
TOTAL
FRACTIONAL
WEIGHT
OF
ALL
UNFINISHED
JOBS
IN
OPT
FOR
A
JOB
J
ITS
INVERSE
DENSITY
IS
DEFINED
TO
BE
THE
RATIO
OF
ITS
ORIGINAL
SIZE
DIVIDED
BY
ITS
ORIGINAL
WEIGHT
AT
ANY
TIME
T
LET
WT
M
DENOTE
THE
TOTAL
FRACTIONAL
WEIGHT
OF
ALL
UNFINISHED
JOBS
WITH
INVERSE
DENSITY
AT
LEAST
M
IN
A
AT
TIME
T
DEFINE
WT
M
SIMILARLY
FOR
THAT
OF
OPT
LET
WT
M
MAX
WT
M
WT
M
WE
DEFINE
P
AS
P
P
P
AND
P
FOR
THEN
IT
IS
EASY
TO
SEE
THAT
IF
A
IS
C
COMPETITIVE
WITH
RESPECT
TO
THIS
NEW
P
THEN
A
IS
ALSO
BE
C
COMPETITIVE
WITH
RESPECT
TO
THE
ORIGINAL
P
IF
P
IS
DEFINED
AT
A
AND
A
Γ
T
O
H
WT
M
DM
B
BUT
NOT
AT
ANY
POINT
IN
THE
INTERVAL
A
B
THEN
IN
TERPOLATE
P
LINEARLY
BETWEEN
A
AND
B
ONCE
AGAIN
THE
ONLINE
ALGORITHM
CAN
SIMULATE
RUNNING
A
SPEED
A
B
IN
THE
NEW
POWER
FUNCTION
BY
ALTERNATELY
RUNNING
AT
SPEEDS
A
AND
B
P
STAYS
CONVEX
BY
THE
CONVEXITY
OF
THE
ORIGINAL
P
ONE
CAN
FIND
A
POWER
FUNCTION
BETWEEN
P
WHERE
H
IS
THE
FUNCTION
SUCH
THAT
FOR
ANY
REAL
W
D
H
W
P
P
W
DW
SINCE
BOTH
P
X
AND
P
ARE
INCREASING
P
P
W
IS
AN
INCREASING
OF
FUNCTION
IN
W
FURTHERMORE
H
W
AND
P
MAX
Ǫ
ǪP
THAT
IS
DEFINED
ON
THE
SAME
DOMAIN
W
OF
SPEEDS
IS
CONTINUOUS
DIFFERENTIABLE
STRICTLY
INCREAS
ING
AND
STRICTLY
CONVEX
THE
COMPETITIVE
RATIO
OF
A
FOR
THE
OLD
POWER
FUNCTION
WILL
BE
WITHIN
Ǫ
OF
THE
COM
PETITIVE
RATIO
OF
A
FOR
THE
NEW
POWER
FUNCTION
IF
P
IS
BOUNDED
WHERE
T
IS
THE
MAXIMUM
SPEED
THEN
THE
NAT
URAL
INTERPRETATION
OF
THE
ALGORITHM
IS
TO
RUN
AT
SPEED
MIN
P
NA
T
WE
CAN
FOLLOW
THE
LINE
OF
REASON
ING
FROM
WE
FIRST
NOTE
THAT
AT
ALL
TIMES
IT
MUST
BE
THE
CASE
THAT
NA
NO
P
T
THEN
THE
ANAL
YSIS
ESSENTIALLY
THEN
GOES
THROUGH
AS
IN
THE
UNBOUNDED
CASE
IN
PARTICULAR
IN
THE
CASE
THAT
NO
NA
WE
ARRIVE
AT
THE
SAME
INEQUALITY
OF
DΦ
SA
P
NA
NO
P
P
NA
NO
NA
NO
IF
SA
P
NA
THEN
WE
HAVE
THAT
SA
P
NA
NO
AS
BEFORE
ELSE
IF
SA
T
WE
STILL
HAVE
THAT
SA
P
NA
NO
FRACTIONAL
WEIGHTED
FLOW
PLUS
ENERGY
OUR
GOAL
IN
THIS
SECTION
IS
TO
PROVE
THEOREM
THAT
CAN
BE
WRITTEN
AS
H
W
P
P
Y
DY
WE
NOW
START
THE
AMORTIZED
LOCAL
COMPETITIVENESS
ANALYSIS
FOR
THE
BOUNDARY
CONDITION
WE
OBSERVE
THAT
BEFORE
ANY
JOB
IS
RELEASED
AND
AFTER
ALL
JOBS
ARE
COMPLETED
WT
M
FOR
ALL
M
SO
Γ
FOR
THE
GENERAL
CONDITION
WHEN
A
JOB
IS
RELEASED
WT
M
IS
UNCHANGED
FOR
ALL
M
SO
Γ
IS
UNCHANGED
WHEN
A
JOB
IS
PROCESSED
BY
A
OR
OPT
THE
FRACTIONAL
WEIGHT
OF
THE
JOB
DECREASES
CONTINUOUSLY
TO
ZERO
SO
Γ
IS
CONTINUOUS
AND
DOES
NOT
DECREASE
DUE
TO
THE
COMPLETION
OF
A
JOB
IT
REMAINS
TO
SHOW
THAT
AT
ANY
TIME
T
DURING
A
TIME
INTERVAL
WITHOUT
JOB
ARRIVAL
OR
COMPLETION
WT
P
ST
D
Γ
T
WT
P
ST
A
A
DT
O
O
THE
REST
OF
THIS
PROOF
CONSIDERS
ANY
SUCH
TIME
T
AND
PROVES
EQUATION
WE
OMIT
T
FROM
THE
SUPERSCRIPT
AND
THE
PARAMETER
FOR
CONVENIENCE
THEN
IN
THE
GENERAL
MODEL
HDF
PLUS
THE
NATURAL
SPEED
SCALING
ALGORITHM
IS
Ǫ
COMPETITIVE
FOR
THE
OBJECTIVE
OF
FRACTIONAL
WEIGHTED
FLOW
PLUS
ENERGY
WE
START
BY
SHOWING
USING
AN
AMORTIZED
LOCAL
COMPETITIVENESS
ARGUMENT
THAT
THIS
ALGORITHM
IS
COMPETITIVE
IF
P
ALSO
SATISFIES
THE
SPECIAL
CONDITIONS
FROM
THE
LAST
SECTION
DEFINITION
OF
ONLINE
ALGORITHM
A
THE
ALGORITHM
ALWAYS
RUNS
THE
JOB
OF
HIGHEST
DENSITY
THE
DENSITY
OF
A
JOB
IS
ITS
WEIGHT
DIVIDED
BY
ITS
WORK
THE
SPEED
ST
AT
D
Γ
D
H
W
M
DM
DT
DT
D
H
W
M
D
W
M
DM
D
W
M
DT
P
P
W
M
D
W
M
DM
DT
WHERE
THE
SECOND
EQUALITY
FOLLOWS
FROM
CHAIN
RULE
LET
MA
AND
MO
DENOTE
THE
MINIMUM
INVERSE
DENSITY
OF
AN
UNFINISHED
JOB
IN
A
AND
OPT
RESPECTIVELY
LET
MA
RESP
MO
BE
IF
A
RESP
OPT
HAS
NO
UNFINISHED
JOB
NOTE
THAT
MA
AND
MO
ARE
THE
DENSITY
OF
THE
HIGHEST
DENSITY
JOB
IN
A
AND
OPT
RESPECTIVELY
SINCE
CASE
WO
WA
FOR
M
MO
WO
M
IS
DECREAS
ING
AT
A
RATE
OF
SO
MO
WHICH
CAUSES
Γ
TO
INCREASE
AT
A
RATE
AT
MOST
MO
P
P
W
M
SO
DM
FOR
MO
A
IS
RUNNING
HDF
AT
SPEED
SA
WA
M
IS
DECREASING
AT
THE
RATE
OF
SA
MA
FOR
ALL
M
MA
AND
WA
M
REMAINS
UNCHANGED
FOR
M
M
SIMILARLY
W
M
IS
M
MO
W
M
MAX
WA
M
WO
M
WA
WO
SO
P
P
W
M
P
P
AND
MO
P
P
W
M
SO
DM
P
P
BY
A
O
MO
O
DECREASING
AT
THE
RATE
OF
SO
MO
FOR
M
MO
AND
REMAINS
UNCHANGED
FOR
M
MO
WE
CONSIDER
THREE
CASES
DEPENDING
ON
WO
WA
WO
WA
OR
WO
WA
CASE
WO
WA
WE
SHOW
THAT
D
Γ
IN
THIS
CASE
NOTE
THAT
FOR
ANY
M
MO
WO
M
WO
WA
WA
M
THUS
FOR
ANY
M
MO
W
M
MAX
WA
M
WO
M
REMAINS
ZERO
AND
DOES
NOT
CHANGE
IT
MEANS
THAT
D
W
M
FOR
M
MO
FOR
ANY
M
MO
WO
M
REMAINS
UNCHANGED
SO
D
W
M
FOR
M
MO
THEREFORE
D
Γ
LEMMA
WITH
I
SA
WE
OBTAIN
THAT
D
Γ
P
SO
SO
WE
HAVE
WA
P
SA
D
Γ
SO
WO
P
SO
HENCE
PROVING
EQUA
TION
THIS
COMPLETES
THE
ANALYSIS
WHEN
THE
SPECIAL
CON
DITIONS
HOLD
THE
PROOF
CAN
BE
EXTENDED
TO
THE
GENERAL
MODEL
IN
THE
SAME
WAY
AS
WAS
DONE
IN
THE
PROOF
OF
THE
OREM
DT
DT
R
P
P
W
M
D
W
M
DM
WE
HAVE
WA
P
SA
D
Γ
WO
P
SO
SO
EQUATION
IS
TRUE
CASE
WO
WA
THE
DECREASE
IN
WA
M
CAUSES
A
DE
CREASE
IN
Γ
AND
THE
DECREASE
IN
WO
M
CAUSES
AN
IN
CREASE
IN
Γ
WE
ANALYZE
THESE
TWO
EFFECTS
SEPARATELY
AND
BOUND
THEM
APPROPRIATELY
FOR
ANY
M
MA
WA
M
IS
DECREASING
AT
A
RATE
OF
SA
MA
WHICH
CAUSES
Γ
TO
MA
DECREASE
AT
THE
RATE
OF
P
P
W
M
SA
DM
MA
FOR
M
MA
W
M
WA
M
WO
M
WA
WO
SO
P
P
W
M
P
P
WA
WO
THUS
MA
P
P
W
M
SA
DM
MA
MA
R
P
P
W
W
SA
DM
A
O
MA
P
WA
WO
SA
FOR
M
MO
WO
M
IS
DECREASING
AT
A
RATE
OF
SO
MO
WHICH
CAUSES
Γ
TO
INCREASE
AT
A
RATE
AT
MOST
MO
P
P
W
M
SO
DM
FOR
M
M
MO
O
W
M
MAX
WA
M
WO
M
WA
WO
SO
P
P
W
M
P
P
WA
WO
THUS
MO
P
P
W
M
SO
DM
MO
MO
R
P
P
W
W
SO
DM
A
O
MO
P
WA
WO
SO
SUMMING
THE
ABOVE
TWO
TERMS
WE
HAVE
D
Γ
P
WA
WO
SA
SO
BY
LEMMA
IT
IS
AT
MOST
SA
P
WA
WO
P
P
WA
WO
P
SO
WA
WO
SINCE
SA
P
WA
P
WA
WO
D
Γ
P
SO
WA
WO
WE
HAVE
WA
P
SA
D
Γ
P
SO
WA
WO
P
SO
WO
HENCE
PROVING
EQUATION
SPEED
SCALING
TO
MANAGE
ENERGY
AND
TEMPERATURE
NIKHIL
BANSAL
AND
TRACY
KIMBREL
IBM
T
J
WATSON
RESEARCH
CENTER
AND
KIRK
PRUHS
UNIVERSITY
OF
PITTSBURGH
ABSTRACT
SPEED
SCALING
IS
A
POWER
MANAGEMENT
TECHNIQUE
THAT
INVOLVES
DYNAMICALLY
CHANGING
THE
SPEED
OF
A
PROCESSOR
WE
STUDY
POLICIES
FOR
SETTING
THE
SPEED
OF
THE
PROCESSOR
FOR
BOTH
OF
THE
GOALS
OF
MINIMIZING
THE
ENERGY
USED
AND
THE
MAXIMUM
TEMPERATURE
ATTAINED
THE
THEORETICAL
STUDY
OF
SPEED
SCALING
POLICIES
TO
MANAGE
ENERGY
WAS
INITIATED
IN
A
SEMINAL
PAPER
BY
YAO
ET
AL
AND
WE
ADOPT
THEIR
SETTING
WE
ASSUME
THAT
THE
POWER
REQUIRED
TO
RUN
AT
SPEED
IS
P
SΑ
FOR
SOME
CONSTANT
Α
WE
ASSUME
A
COLLECTION
OF
TASKS
EACH
WITH
A
RELEASE
TIME
A
DEADLINE
AND
AN
ARBITRARY
AMOUNT
OF
WORK
THAT
MUST
BE
DONE
BETWEEN
THE
RELEASE
TIME
AND
THE
DEADLINE
YAO
ET
AL
GAVE
AN
OFFLINE
GREEDY
ALGORITHM
YDS
TO
COMPUTE
THE
MINIMUM
ENERGY
SCHEDULE
THEY
FURTHER
PROPOSED
TWO
ONLINE
ALGORITHMS
AVERAGE
RATE
AVR
AND
OPTIMAL
AVAILABLE
OA
AND
SHOWED
THAT
AVR
IS
COMPETITIVE
WITH
RESPECT
TO
ENERGY
WE
PROVIDE
A
TIGHT
ΑΑ
BOUND
ON
THE
COMPETITIVE
RATIO
OF
OA
WITH
RESPECT
TO
ENERGY
WE
INITIATE
THE
STUDY
OF
SPEED
SCALING
TO
MANAGE
TEMPERATURE
WE
ASSUME
THAT
THE
ENVIRONMENT
HAS
A
FIXED
AMBIENT
TEMPERATURE
AND
THAT
THE
DEVICE
COOLS
ACCORDING
TO
NEWTON
LAW
OF
COOLING
WE
OBSERVE
THAT
THE
MAXIMUM
TEMPERATURE
CAN
BE
APPROXIMATED
WITHIN
A
FACTOR
OF
TWO
BY
THE
MAXIMUM
ENERGY
USED
OVER
ANY
INTERVAL
OF
LENGTH
B
WHERE
B
IS
THE
COOLING
PARAMETER
OF
THE
DEVICE
WE
DEFINE
A
SPEED
SCALING
POLICY
TO
BE
COOLING
OBLIVIOUS
IF
IT
IS
SIMULTANEOUSLY
CONSTANT
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
FOR
ALL
COOLING
PARAMETERS
WE
THEN
OBSERVE
THAT
COOLING
OBLIVIOUS
ALGORITHMS
ARE
ALSO
CONSTANT
COMPETITIVE
WITH
RESPECT
TO
ENERGY
MAXIMUM
SPEED
AND
MAXIMUM
POWER
WE
SHOW
THAT
YDS
IS
A
COOLING
OBLIVIOUS
ALGORITHM
IN
CONTRAST
WE
SHOW
THAT
THE
ONLINE
ALGORITHMS
OA
AND
AVR
ARE
NOT
COOLING
OBLIVIOUS
WE
THEN
PROPOSE
A
NEW
ONLINE
AL
GORITHM
THAT
WE
CALL
BKP
WE
SHOW
THAT
BKP
IS
COOLING
OBLIVIOUS
WE
FURTHER
SHOW
THAT
BKP
IS
E
COMPETITIVE
WITH
RESPECT
TO
THE
MAXIMUM
SPEED
AND
THAT
NO
DETERMINISTIC
ONLINE
ALGORITHM
CAN
HAVE
A
BETTER
COMPETITIVE
RATIO
BKP
ALSO
HAS
A
LOWER
COMPETITIVE
RATIO
FOR
ENERGY
THAN
OA
FOR
Α
K
PRUHS
WAS
SUPPORTED
IN
PART
BY
NSF
GRANTS
CCR
CNS
CNS
CCF
CCF
AND
IIS
AUTHORS
ADDRESSES
N
BANSAL
AND
T
KIMBREL
IBM
T
J
WATSON
RESEARCH
CENTER
PO
BOX
YORK
TOWN
HEIGHTS
NY
E
MAIL
NIKHIL
KIMBREL
US
IBM
COM
K
PRUHS
DEPARTMENT
OF
COMPUTER
SCIENCE
SOUTH
BOUQUET
STREET
SENNOTT
SQUARE
BUILDING
ROOM
UNIVERSITY
OF
PITTSBURGH
PITTSBURGH
PA
E
MAIL
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
PART
OR
ALL
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
DIRECT
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
SHOW
THIS
NOTICE
ON
THE
FIRST
PAGE
OR
INITIAL
SCREEN
OF
A
DISPLAY
ALONG
WITH
THE
FULL
CITATION
COPYRIGHTS
FOR
COMPONENTS
OF
THIS
WORK
OWNED
BY
OTHERS
THAN
ACM
MUST
BE
HONORED
ABSTRACTING
WITH
CREDIT
IS
PERMITTED
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
TO
REDISTRIBUTE
TO
LISTS
OR
TO
USE
ANY
COMPONENT
OF
THIS
WORK
IN
OTHER
WORKS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
PERMISSIONS
MAY
BE
REQUESTED
FROM
PUBLICATIONS
DEPT
ACM
INC
PENN
PLAZA
SUITE
NEW
YORK
NY
USA
FAX
OR
QC
ACM
DOI
FINALLY
WE
SHOW
THAT
THE
OPTIMAL
TEMPERATURE
SCHEDULE
CAN
BE
COMPUTED
OFFLINE
IN
POLYNOMIAL
TIME
USING
THE
ELLIPSOID
ALGORITHM
CATEGORIES
AND
SUBJECT
DESCRIPTORS
C
PERFORMANCE
OF
SYSTEMS
PERFORMANCE
ATTRIBUTES
F
ANALYSIS
OF
ALGORITHMS
AND
PROBLEM
COMPLEXITY
NONNUMERICAL
ALGORITHMS
AND
PROBLEMS
SEQUENCING
AND
SCHEDULING
GENERAL
TERMS
ALGORITHMS
PERFORMANCE
ADDITIONAL
KEY
WORDS
AND
PHRASES
SPEED
SCALING
VOLTAGE
SCALING
POWER
MANAGEMENT
ACM
REFERENCE
FORMAT
BANSAL
N
KIMBREL
T
AND
PRUHS
K
SPEED
SCALING
TO
MANAGE
ENERGY
AND
TEMPER
ATURE
J
ACM
ARTICLE
MARCH
PAGES
DOI
INTRODUCTION
MOTIVATION
THE
ENERGY
CONSUMPTION
RATE
OF
COMPUTING
DEVICES
HAS
IN
CREASED
EXPONENTIALLY
FOR
SEVERAL
DECADES
SINCE
THE
EARLY
POWER
DENSITIES
IN
MICROPROCESSORS
HAVE
DOUBLED
EVERY
THREE
YEARS
SKADRON
ET
AL
THIS
INCREASED
POWER
USAGE
POSES
TWO
TYPES
OF
DIFFICULTIES
ENERGY
CONSUMPTION
AS
ENERGY
IS
POWER
INTEGRATED
OVER
TIME
SUPPLYING
THE
REQUIRED
ENERGY
MAY
BECOME
PROHIBITIVELY
EXPENSIVE
OR
EVEN
TECHNOLOGICALLY
IN
FEASIBLE
THIS
IS
A
PARTICULAR
DIFFICULTY
IN
DEVICES
THAT
RELY
ON
BATTERIES
FOR
ENERGY
AND
WILL
WILL
BECOME
EVEN
MORE
CRITICAL
SINCE
BATTERY
CAPACITIES
ARE
INCREASING
AT
A
MUCH
SLOWER
RATE
THAN
ENERGY
CONSUMPTION
TEMPERATURE
THE
ENERGY
USED
IN
COMPUTING
DEVICES
IS
IN
LARGE
PART
CONVERTED
INTO
HEAT
COOLING
COSTS
ARE
RISING
EXPONENTIALLY
ALONG
WITH
ENERGY
CONSUMPTION
AND
THREATEN
THE
COMPUTER
INDUSTRY
ABILITY
TO
DEPLOY
NEW
SYSTEMS
SKADRON
ET
AL
IN
FACT
IN
MAY
INTEL
PUBLICLY
ACKNOWLEDGED
THAT
IT
HAD
HIT
A
THERMAL
WALL
ON
ITS
MICROPROCESSOR
LINE
INTEL
SCRAPPED
THE
DEVELOPMENT
OF
ITS
TEJAS
AND
JAYHAWK
CHIPS
IN
ORDER
TO
RUSH
TO
THE
MARKETPLACE
A
MORE
POWER
EFFICIENT
CHIP
TECHNOLOGY
DESIGNERS
SAID
THE
ESCALATING
HEAT
PROBLEMS
WERE
SO
SEVERE
THAT
THEY
THREATENED
TO
CAUSE
CHIPS
TO
FRACTURE
MARKOFF
APPLE
WAS
UNABLE
TO
DEVELOP
A
LAPTOP
DUE
TO
INADEQUATE
HEAT
MANAGEMENT
IN
THE
IBM
POWERPC
CHIPS
AND
IN
THE
SUMMER
OF
APPLE
ANNOUNCED
THAT
IT
WAS
SWITCHING
TO
COOLER
INTEL
CHIPS
APPLE
THESE
FACTORS
HAVE
RESULTED
IN
POWER
BECOMING
A
FIRST
CLASS
DESIGN
CONSTRAINT
FOR
MODERN
COMPUTING
DEVICES
MUDGE
THERE
IS
AN
EXTENSIVE
LITERATURE
ON
POWER
MANAGEMENT
IN
COMPUTING
DEVICES
OVERVIEWS
HAVE
BEEN
GIVEN
BY
BROOKS
ET
AL
MUDGE
AND
TIWARI
ET
AL
BOTH
IN
ACADEMIC
RESEARCH
AND
PRACTICE
VOLTAGE
FREQUENCY
SPEED
SCALING
IS
THE
DOMINANT
TECHNIQUE
FOR
POWER
MANAGEMENT
SPEED
SCALING
INVOLVES
DYNAMICALLY
CHANGING
THE
SPEED
OF
THE
PROCESSOR
CURRENT
MICROPROCESSORS
FROM
AMD
INTEL
AND
TRANSMETA
ALLOW
THE
SPEED
OF
THE
MICROPROCESSOR
TO
BE
SET
DYNAMICALLY
SOME
MODERN
PROCESSORS
ARE
ABLE
TO
SENSE
THEIR
OWN
TEMPERATURES
SUCH
A
DEVICE
CAN
BE
SLOWED
DOWN
OR
SHUT
DOWN
SO
THAT
ITS
TEMPERATURE
WILL
STAY
BELOW
ITS
THERMAL
THRESHOLD
SKADRON
ET
AL
THERE
IS
AN
INHERENT
TRADEOFF
BETWEEN
POWER
REDUCTION
AND
PERFORMANCE
IN
GEN
ERAL
WHEN
MORE
POWER
IS
AVAILABLE
BETTER
PERFORMANCE
CAN
BE
ACHIEVED
AS
A
RESULT
IT
IS
GENERALLY
PROPOSED
THAT
POWER
REDUCTION
TECHNIQUES
BE
PREFERENTIALLY
APPLIED
DURING
TIMES
WHEN
PERFORMANCE
IS
LESS
CRITICAL
IT
IS
LIKELY
THAT
IN
THE
FUTURE
THESE
POLICIES
WILL
NECESSARILY
INCORPORATE
INFORMATION
PROVIDED
BY
APPLICATIONS
AND
HIGH
LEVEL
RESOURCE
MANAGERS
IN
OPERATING
SYSTEMS
ELLIS
THIS
WILL
REQUIRE
POLICIES
TO
DETERMINE
HOW
ESSENTIAL
PERFORMANCE
IS
AT
ANY
GIVEN
TIME
AND
HOW
TO
APPLY
A
PARTICULAR
POWER
REDUCTION
TECHNIQUE
OUR
GOAL
HERE
IS
TO
MAKE
A
FORMAL
STUDY
OF
A
PARTICULAR
POWER
REDUCTION
TECHNIQUE
NAMELY
SPEED
SCALING
IN
A
SPECIFIC
SETTING
NAMELY
SCHEDULING
TASKS
WITH
DEADLINES
TO
MANAGE
EITHER
TEMPERATURE
OR
ENERGY
BACKGROUND
THE
STARTING
POINT
FOR
OUR
INVESTIGATIONS
IS
THE
SEMINAL
PAPER
BY
YAO
ET
AL
IN
WHICH
THE
AUTHORS
PROPOSED
FORMULATING
SPEED
SCALING
PROBLEMS
AS
SCHEDULING
PROBLEMS
THAT
IS
THE
SETTING
IS
A
COLLECTION
OF
TASKS
AND
A
SCHEDULE
SPECIFIES
NOT
ONLY
WHICH
TASK
TO
RUN
AT
EACH
TIME
BUT
ALSO
THE
SPEED
AT
WHICH
TO
RUN
THE
SELECTED
TASK
EACH
TASK
I
HAS
A
RELEASE
TIME
RI
AT
WHICH
IT
ENTERS
THE
SYSTEM
A
DEADLINE
DI
AND
AN
AMOUNT
OF
WORK
WI
THAT
MUST
BE
PERFORMED
BETWEEN
TIMES
RI
AND
DI
TO
COMPLETE
THE
TASK
IN
SOME
SETTINGS
FOR
EXAMPLE
THE
PLAYING
OF
A
VIDEO
OR
OTHER
MULTIMEDIA
PRESEN
TATION
THERE
MAY
BE
NATURAL
DEADLINES
FOR
THE
VARIOUS
TASKS
IMPOSED
BY
THE
APPLICA
TION
IN
OTHER
SETTINGS
THE
SYSTEM
MAY
IMPOSE
DEADLINES
TO
BETTER
MANAGE
TASKS
OR
INSURE
A
CERTAIN
QUALITY
OF
SERVICE
TO
EACH
TASK
BUTTAZZO
YAO
ET
AL
ASSUMED
THAT
TASKS
CAN
BE
PREEMPTED
THAT
IS
THE
DEVICE
CAN
SUSPEND
THE
EXECUTION
OF
A
TASK
AND
LATER
RESUME
THE
TASK
FROM
THE
POINT
OF
SUSPENSION
PREEMPTION
IS
A
NECESSARY
FEATURE
TO
OBTAIN
REASONABLE
PERFORMANCE
IN
A
SYSTEM
WITH
TASKS
WITH
WIDELY
VARYING
WORK
YAO
ET
AL
ASSUMED
THE
EXISTENCE
OF
A
FUNCTION
P
THAT
SPECIFIES
THE
POWER
USED
WHEN
THE
DEVICE
IS
RUN
AT
SPEED
THEY
ASSUMED
THAT
P
SΑ
FOR
SOME
Α
THE
KEY
FACT
ABOUT
SUCH
A
FUNCTION
IS
THAT
IT
IS
STRICTLY
CONVEX
THAT
IS
THE
SLOWER
A
TASK
IS
RUN
THE
LESS
ENERGY
IS
USED
TO
COMPLETE
THAT
TASK
THIS
IS
A
GENERALIZATION
OF
THE
WELL
KNOWN
CUBE
ROOT
RULE
FOR
CMOS
DEVICES
WHICH
STATES
THAT
THE
SPEED
IS
ROUGHLY
PROPORTIONAL
TO
THE
CUBE
ROOT
OF
THE
POWER
P
OR
EQUIVALENTLY
P
CMOS
IS
LIKELY
TO
REMAIN
THE
DOMINANT
TECHNOLOGY
FOR
THE
NEAR
TERM
FUTURE
POWER
IN
CMOS
DEVICES
HAS
THREE
COMPONENTS
SWITCHING
LOSS
LEAKAGE
LOSS
AND
SHORT
CIRCUIT
LOSS
BROOKS
ET
AL
MUDGE
SWITCHING
LOSS
IS
THE
ENERGY
CONSUMPTION
DUE
TO
CHARGING
AND
DISCHARGING
GATES
THE
SWITCHING
LOSS
IS
ROUGHLY
PROPORTIONAL
TO
SV
WHERE
IS
THE
SPEED
CLOCK
FREQUENCY
AND
V
IS
THE
VOLTAGE
V
AND
ARE
NOT
INDEPENDENT
THERE
IS
A
MINIMUM
VOLTAGE
REQUIRED
TO
DRIVE
THE
MICROPROCESSOR
AT
A
DESIRED
FREQUENCY
AND
THIS
MINIMUM
VOLTAGE
IS
APPROXIMATELY
PROPORTIONAL
TO
THE
FREQUENCY
BROOKS
ET
AL
HENCE
ONE
CAN
CONCLUDE
THAT
SWITCHING
LOSS
IS
ROUGHLY
PROPORTIONAL
TO
THE
CUBE
OF
THE
SPEED
CURRENTLY
SWITCHING
LOSS
IS
RESPONSIBLE
FOR
THE
MAJORITY
OF
THE
ENERGY
USED
BY
COMPUTING
DEVICES
MUDGE
YAO
ET
AL
THEN
STUDIED
THE
PROBLEM
OF
MINIMIZING
THE
TOTAL
ENERGY
USED
SUBJECT
TO
THE
DEADLINE
FEASIBILITY
CONSTRAINTS
THIS
IS
ALWAYS
POSSIBLE
UNDER
THE
ASSUMPTION
THAT
THE
PROCESSOR
CAN
RUN
AT
ANY
SPEED
THEY
GAVE
AN
OPTIMAL
OFFLINE
GREEDY
POLYNOMIAL
TIME
ALGORITHM
WHICH
WE
CALL
YDS
THE
YDS
SCHEDULE
IS
SI
MULTANEOUSLY
OPTIMAL
FOR
ALL
STRICTLY
CONVEX
SPEED
TO
POWER
FUNCTIONS
THE
YDS
SCHEDULE
CAN
ALSO
BE
SEEN
TO
MINIMIZE
THE
MAXIMUM
SPEED
AND
HENCE
THE
MAXI
MUM
POWER
YAO
ET
AL
ALSO
PROPOSED
TWO
SIMPLE
ONLINE
ALGORITHMS
IN
THE
ONLINE
VERSION
OF
THE
PROBLEM
THE
SCHEDULER
LEARNS
ABOUT
A
TASK
ONLY
AT
ITS
RELEASE
TIME
AT
THIS
TIME
THE
SCHEDULER
ALSO
LEARNS
THE
EXACT
WORK
REQUIREMENT
AND
THE
DEADLINE
OF
THE
TASK
THE
ONLINE
ALGORITHM
AVERAGE
RATE
AVR
RUNS
EACH
TASK
I
AT
SPEED
WI
DI
RI
THE
ONLINE
ALGORITHM
OPTIMAL
AVAILABLE
OA
SCHEDULES
THE
UNFINISHED
WORK
OPTIMALLY
SAY
USING
YDS
UNDER
THE
ASSUMPTION
THAT
NO
MORE
TASKS
WILL
ARRIVE
YAO
ET
AL
STATE
A
LOWER
BOUND
OF
ΑΑ
ON
THE
COMPETITIVE
RATIO
FOR
AVR
AND
OA
THEY
PROVE
USING
A
RATHER
COMPLICATED
SPECTRAL
ANALYSIS
THAT
THE
COMPETITIVE
RATIO
OF
AVR
IS
AT
MOST
THEY
ALSO
SHOW
THAT
AN
ON
LINE
ALGORITHM
CANNOT
IN
GENERAL
CONSTRUCT
AN
OPTIMAL
ENERGY
SCHEDULE
EVEN
FOR
AN
INSTANCE
THAT
CONTAINS
ONLY
TWO
TASKS
OUR
CONTRIBUTIONS
YAO
ET
AL
DID
NOT
EXPLICITLY
PROVE
THAT
THE
YDS
ALGORITHM
PRODUCES
THE
MOST
ENERGY
EFFICIENT
FEASIBLE
SCHEDULE
TO
THE
BEST
OF
OUR
KNOWLEDGE
NO
SUCH
PROOF
HAS
APPEARED
IN
THE
LITERATURE
WE
SHOW
IN
SECTION
THAT
THE
CORRECTNESS
OF
YDS
IS
AN
ELEGANT
CONSEQUENCE
OF
THE
WELL
KNOWN
KKT
OPTIMALITY
CONDITIONS
FOR
CONVEX
PROGRAMS
THIS
ILLUSTRATES
THE
UTILITY
OF
THE
KKT
OPTIMALITY
CONDITIONS
IN
POWER
MANAGEMENT
PROBLEMS
IN
SECTION
WE
EXTEND
THE
RESULTS
YAO
ET
AL
ON
ONLINE
ALGORITHMS
FOR
ENERGY
MINIMIZATION
WE
GIVE
EXPLICIT
INSTANCES
THAT
SHOW
THAT
THE
COMPETITIVE
RATIOS
OF
AVR
AND
OA
ARE
AT
LEAST
ΑΑ
WE
THEN
PROVIDE
A
TIGHT
ΑΑ
BOUND
ON
THE
COMPETITIVE
RATIO
OF
OA
USING
A
POTENTIAL
FUNCTION
ARGUMENT
WE
THEN
TURN
OUR
ATTENTION
TO
SPEED
SCALING
TO
MANAGE
TEMPERATURE
TO
OUR
KNOWL
EDGE
THIS
IS
THE
FIRST
THEORETICAL
INVESTIGATION
OF
THIS
AREA
WE
FIRST
NEED
TO
MODEL
THE
COOLING
BEHAVIOR
OF
A
DEVICE
COOLING
IS
A
COMPLEX
PHENOMENON
THAT
CANNOT
BE
CAPTURED
COMPLETELY
ACCURATELY
BY
ANY
SIMPLE
MODEL
SERGENT
AND
KRUM
FOR
TRACTABILITY
WE
REQUIRE
A
SIMPLE
FIRST
ORDER
APPROXIMATION
OUR
KEY
ASSUMPTIONS
ARE
THAT
HEAT
IS
LOST
VIA
CONDUCTION
AND
THE
AMBIENT
TEMPERATURE
OF
THE
ENVIRONMENT
SURROUNDING
THE
DEVICE
IS
CONSTANT
THIS
IS
LIKELY
A
REASONABLE
FIRST
ORDER
APPROXI
MATION
IN
SOME
BUT
CERTAINLY
NOT
ALL
SETTINGS
THEN
WE
APPEAL
TO
NEWTON
LAW
OF
COOLING
WHICH
STATES
THAT
THE
RATE
OF
COOLING
IS
PROPORTIONAL
TO
THE
DIFFERENCE
IN
TEM
PERATURE
BETWEEN
THE
OBJECT
AND
THE
AMBIENT
ENVIRONMENTAL
TEMPERATURE
WITHOUT
LOSS
OF
GENERALITY
WE
MAY
ASSUME
THAT
THE
TEMPERATURE
SCALE
IS
TRANSLATED
SO
THAT
THE
AMBIENT
TEMPERATURE
IS
ZERO
IF
WE
ASSUME
THAT
THE
NET
CHANGE
IN
TEMPERATURE
IS
THE
SUM
OF
THE
DECREASE
DUE
TO
COOLING
AS
DESCRIBED
ABOVE
AND
AN
INCREASE
PROPORTIONAL
TO
THE
ELECTRICAL
POWER
APPLIED
TO
THE
DEVICE
A
FIRST
ORDER
APPROXIMATION
FOR
RATE
OF
CHANGE
T
T
T
OF
THE
TEMPERATURE
T
T
AT
TIME
T
IS
THEN
GIVEN
BY
THE
EQUATION
T
T
T
AP
T
BT
T
WHERE
P
T
IS
THE
SUPPLIED
POWER
AT
TIME
T
AND
A
AND
B
ARE
CONSTANTS
SERGENT
AND
KRUM
CRUSOE
WE
CALL
B
THE
COOLING
PARAMETER
OF
THE
DEVICE
WE
THEN
CONSIDER
THE
RELATIONSHIP
BETWEEN
TEMPERATURE
AND
ENERGY
TEMPERATURE
AND
ENERGY
ARE
PHYSICAL
VARIABLES
WITH
QUITE
DIFFERENT
PROPERTIES
IF
THE
PROCESSOR
IN
A
MOBILE
DEVICE
EXCEEDS
ITS
ENERGY
BOUND
THEN
THE
BATTERY
IS
EXHAUSTED
IF
A
PROCES
SOR
EXCEEDS
IT
THERMAL
THRESHOLD
IT
IS
DESTROYED
POWER
MANAGEMENT
SCHEMES
FOR
CONSERVING
ENERGY
FOCUS
ON
REDUCING
CUMULATIVE
POWER
WHILE
POWER
MANAGEMENT
SCHEMES
FOR
REDUCING
TEMPERATURE
MUST
FOCUS
MORE
ON
INSTANTANEOUS
POWER
POWER
MANAGEMENT
SCHEMES
DESIGNED
TO
CONSERVE
ENERGY
MAY
NOT
PERFORM
WELL
WHEN
THE
GOAL
IS
TO
REDUCE
TEMPERATURE
IN
FACT
MANY
LOW
POWER
TECHNIQUES
ARE
REPORTED
TO
HAVE
LITTLE
OR
NO
EFFECT
ON
TEMPERATURE
SKADRON
ET
AL
TEMPERATURE
AWARE
DESIGN
IS
THEREFORE
AN
AREA
OF
STUDY
DISTINCT
FROM
ALBEIT
RELATED
TO
ENERGY
AWARE
DESIGN
SKADRON
ET
AL
IN
SECTION
WE
CONSIDER
THE
RELATIONSHIP
BETWEEN
TEMPERATURE
AND
ENERGY
ONE
CONSEQUENCE
OF
NEWTON
LAW
IS
THAT
AN
UN
POWERED
DEVICE
COOLS
BY
A
CONSTANT
FRACTION
EVERY
TIME
UNITS
THIS
LEADS
US
TO
OBSERVE
THAT
THE
MAXIMUM
TEMPERATURE
IS
WITHIN
A
FACTOR
OF
TWO
OF
A
TIMES
THE
MAXIMUM
ENERGY
USED
OVER
ANY
INTERVAL
OF
LENGTH
IF
B
THEN
NO
ENERGY
IS
EVER
DISSIPATED
FROM
THE
DEVICE
AND
THE
MAXIMUM
TEMPERATURE
IS
THE
FINAL
TEMPERATURE
WHICH
IS
A
TIMES
THE
ENERGY
USED
THUS
WHEN
B
THE
TEMPERATURE
MINIMIZATION
PROBLEM
IS
EQUIVALENT
TO
THE
ENERGY
MINIMIZATION
PROBLEM
IN
THE
LIMIT
AS
THE
COOLING
PARAMETER
B
APPROACHES
THE
MAXIMUM
TEMPERATURE
IS
ESSENTIALLY
DETERMINED
BY
THE
MAXIMUM
ENERGY
OVER
AN
INFINITESIMAL
INTERVAL
AND
THUS
THE
MINIMIZATION
PROBLEM
INTUITIVELY
BECOMES
EQUIVALENT
TO
THE
PROBLEM
OF
MINIMIZING
THE
MAXIMUM
POWER
OR
EQUIVALENTLY
MINIMIZING
THE
MAXIMUM
SPEED
THE
ENERGY
MINIMIZATION
PROBLEM
WHEN
THE
SPEED
TO
POWER
PARAMETER
Α
IS
IS
ALSO
EQUIVALENT
TO
MINIMIZING
THE
MAXIMUM
POWER
BECAUSE
OF
THIS
EXPONENTIAL
COOLING
IT
SEEMS
DIFFICULT
TO
REASON
ABOUT
TEMPERA
TURE
HOWEVER
THE
ABOVE
OBSERVATION
ABOUT
APPROXIMATING
TEMPERATURE
BY
ENERGY
USED
OVER
SOME
INTERVAL
MAKES
REASONING
ABOUT
APPROXIMATE
TEMPERATURE
MUCH
EASIER
THAN
REASONING
ABOUT
EXACT
TEMPERATURE
THIS
OBSERVATION
ALSO
MOTIVATES
US
TO
DEFINE
WHAT
WE
CALL
A
COOLING
OBLIVIOUS
SPEED
SCALING
ALGORITHM
WHICH
IS
AN
ALGORITHM
THAT
IS
SIMULTANEOUSLY
O
APPROXIMATE
FOR
MINIMIZING
THE
MAXIMUM
TEMPERATURE
FOR
ALL
COOLING
PARAMETERS
B
THUS
A
COOLING
OBLIVIOUS
ALGORITHM
IS
O
APPROXIMATE
FOR
TOTAL
ENERGY
FURTHER
IF
THE
SCHEDULE
PRODUCED
BY
A
COOLING
OBLIVIOUS
ALGORITHM
DOES
NOT
DEPEND
ON
THE
VALUE
OF
Α
AND
THE
COOLING
PARAMETER
B
AS
IS
THE
CASE
FOR
ALL
THE
ALGORITHMS
THAT
WE
CONSIDER
THEN
THE
ALGORITHM
IS
ALSO
O
APPROXIMATE
FOR
MINIMIZING
THE
MAXIMUM
SPEED
IN
SECTION
WE
SHOW
THAT
WHILE
THE
YDS
SCHEDULE
MAY
NOT
BE
OPTIMAL
FOR
TEMPER
ATURE
IT
IS
COOLING
OBLIVIOUS
MORE
PRECISELY
WE
SHOW
THAT
YDS
IS
APPROXIMATE
WITH
RESPECT
TO
TEMPERATURE
FOR
ALL
COOLING
PARAMETERS
B
THIS
CONSTRUCTIVELY
SHOWS
THAT
THERE
ARE
SCHEDULES
THAT
ARE
O
APPROXIMATE
WITH
RESPECT
TO
BOTH
OF
THE
DUAL
CRITERIA
OF
TEMPERATURE
AND
ENERGY
WE
THEN
TURN
TO
ONLINE
SPEED
SCALING
TO
MINIMIZE
THE
MAXIMUM
TEMPERATURE
THAT
A
DEVICE
EVER
REACHES
AGAIN
SUBJECT
TO
THE
CONSTRAINT
THAT
ALL
TASKS
FINISH
BY
THEIR
DEADLINES
IN
SECTION
WE
SHOW
THAT
ONLINE
ALGORITHMS
OA
AND
AVR
PROPOSED
BY
YAO
ET
AL
IN
THE
CONTEXT
OF
ENERGY
MANAGEMENT
ARE
NOT
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
THAT
IS
THESE
ALGORITHMS
ARE
NOT
COOLING
OBLIVIOUS
RECALL
THAT
BOTH
OA
AND
AVR
ARE
O
COMPETITIVE
WITH
RESPECT
TO
ENERGY
THIS
DEMONSTRATIVELY
ILLUSTRATES
THE
OBSERVATION
FROM
PRACTICE
THAT
POWER
MANAGEMENT
TECHNIQUES
THAT
ARE
EFFECTIVE
FOR
MANAGING
ENERGY
MAY
NOT
BE
EFFECTIVE
FOR
TEM
PERATURE
ONE
INTUITIVE
SPEED
SCALING
ALGORITHM
TO
MANAGE
TEMPERATURE
IS
TO
RUN
AT
THE
MINIMUM
CONSTANT
SPEED
THAT
WILL
ALLOW
ALL
TASKS
TO
FINISH
BY
THEIR
DEADLINE
SURPRISINGLY
WE
SHOW
THAT
THIS
ALGORITHM
IS
ALSO
NOT
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
WE
PROPOSE
A
NEW
ONLINE
SPEED
SCALING
ALGORITHM
THAT
WE
CALL
BKP
IN
SECTION
WE
SHOW
THAT
BKP
IS
COOLING
OBLIVIOUS
THAT
IS
BKP
IS
SIMULTANE
OUSLY
O
COMPETITIVE
FOR
TOTAL
ENERGY
MAXIMUM
TEMPERATURE
MAXIMUM
POWER
AND
MAXIMUM
SPEED
WE
SHOW
THAT
THE
COMPETITIVE
RATIO
FOR
BKP
WITH
RESPECT
TO
ENERGY
IS
AT
MOST
Α
Α
Α
EXP
Α
NOTE
THAT
FOR
Α
THIS
COMPETITIVE
RATIO
IS
AT
MOST
EXP
Α
THE
COMPETITIVE
RATIO
OF
BKP
IS
BETTER
THAN
THE
COMPETITIVE
RATIOS
OF
OA
AND
AVR
FOR
Α
WE
SHOW
THAT
THE
COMPETITIVE
RATIO
OF
BKP
WITH
RESPECT
TO
TEMPERATURE
IS
AT
MOST
EXP
Α
Α
Α
Α
WE
SHOW
THAT
BKP
IS
E
COMPETITIVE
WITH
RESPECT
TO
MAXIMUM
SPEED
OR
EQUIVALENTLY
THAT
BKP
IS
EXP
Α
COMPETITIVE
WITH
RESPECT
TO
MAXIMUM
POWER
WE
FURTHER
SHOW
THAT
BKP
IS
OPTIMALLY
COMPETITIVE
WITH
RESPECT
TO
BOTH
MAXIMUM
SPEED
AND
MAXIMUM
POWER
THAT
IS
NO
DETERMINISTIC
ALGORITHM
CAN
HAVE
BETTER
COMPETITIVE
RATIOS
AS
A
CONSEQUENCE
OF
THIS
ONE
CAN
CONCLUDE
THAT
IF
A
DETERMINISTIC
ONLINE
ALGORITHM
IS
O
KΑ
COMPETITIVE
WITH
RESPECT
TO
ENERGY
THEN
THE
VALUE
OF
THE
CONSTANT
K
HAS
TO
BE
AT
LEAST
E
THUS
THE
COMPETITIVE
RATIO
OF
BKP
WITH
RESPECT
TO
ENERGY
IS
OPTIMAL
UP
TO
A
MULTIPLICATIVE
CONSTANT
FOR
DETERMINISTIC
ONLINE
ALGORITHMS
WE
FINALLY
TURN
OUR
ATTENTION
TO
OFFLINE
TEMPERATURE
MANAGEMENT
WE
SHOW
IN
SECTION
THAT
THIS
PROBLEM
CAN
BE
POSED
AS
A
CONVEX
OPTIMIZATION
PROBLEM
CONVEX
OPTIMIZATION
PROBLEMS
CAN
BE
SOLVED
ARBITRARILY
PRECISELY
IN
POLYNOMIAL
TIME
USING
THE
ELLIPSOID
ALGORITHM
IF
ONE
CAN
COMPUTE
A
SEPARATING
HYPERPLANE
FOR
A
VIOLATED
CONSTRAINT
IN
POLYNOMIAL
TIME
TO
ACCOMPLISH
THIS
FOR
OUR
TEMPERATURE
PROBLEM
WE
SHOW
THAT
THE
KEY
SUBPROBLEM
IS
DETERMINING
THE
MAXIMUM
WORK
THAT
CAN
BE
ACCOMPLISHED
DURING
A
FIXED
TIME
PERIOD
WITH
A
FIXED
STARTING
AND
A
FIXED
ENDING
TEMPERATURE
WE
SHOW
HOW
TO
USE
TECHNIQUES
FROM
CALCULUS
OF
VARIATIONS
TO
SOLVE
THIS
SUBPROBLEM
AS
A
CONSEQUENCE
OF
THIS
WE
REVEAL
SOME
STRUCTURE
OF
THE
OPTIMAL
TEMPERATURE
SCHEDULE
DURING
ANY
MAXIMAL
TIME
PERIOD
WHICH
CONTAINS
NO
RELEASE
TIME
OR
DEADLINE
THE
TEMPERATURE
CURVE
IS
EITHER
AN
EULER
LAGRANGE
CURVE
OR
RISES
TO
THE
THERMAL
THRESHOLD
ALONG
AN
EULER
LAGRANGE
CURVE
STAYS
AT
THE
THERMAL
THRESHOLD
FOR
SOME
AMOUNT
OF
TIME
AND
THEN
FALLS
ALONG
AN
EULER
LAGRANGE
CURVE
RELATED
RESEARCH
A
NAIVE
IMPLEMENTATION
OF
YDS
RUNS
IN
TIME
O
THIS
CAN
BE
IMPROVED
TO
O
IF
THE
INTERVALS
HAVE
A
TREE
STRUCTURE
LI
ET
AL
RECENTLY
LI
ET
AL
GAVE
AN
O
LOG
N
IMPLEMENTATION
FOR
THE
GENERAL
CASE
FOR
HARD
REAL
TIME
TASKS
WITH
FIXED
PRIORITIES
YUN
AND
KIM
SHOW
THAT
IT
IS
NP
HARD
TO
COMPUTE
A
MINIMUM
ENERGY
SCHEDULE
THEY
ALSO
GIVE
A
FULLY
POLYNOMIAL
TIME
APPROXIMATION
SCHEME
FOR
THE
PROBLEM
KWON
AND
KIM
GIVE
A
POLYNOMIAL
TIME
ALGORITHM
FOR
THE
CASE
OF
A
PROCESSOR
WITH
DISCRETE
SPEEDS
LI
AND
YAO
GIVE
AN
ALGORITHM
WITH
RUNNING
TIME
O
D
N
LOG
N
WHERE
D
IS
THE
NUMBER
OF
SPEEDS
IRANI
ET
AL
STUDY
ONLINE
SPEED
SCALING
ALGORITHMS
TO
MINIMIZE
ENERGY
USAGE
FOR
A
DEVICE
THAT
ALSO
HAS
A
SLEEP
STATE
THEY
GIVE
AN
OFFLINE
POLYNOMIAL
TIME
APPROXIMATE
ALGORITHM
IRANI
ET
AL
ALSO
GIVE
AN
ONLINE
ALGORITHM
A
THAT
USES
AS
A
SUBROUTINE
AN
ALGORITHM
B
FOR
PURE
SPEED
SCALING
IF
B
IS
ADDITIVE
AND
MONOTONE
AS
AVR
OA
AND
BKP
ARE
THEN
A
IS
MAX
Α
R
COMPETITIVE
WHERE
R
IS
THE
COMPETITIVE
RATIO
OF
B
THUS
OUR
ANALYSIS
OF
OA
AND
BKP
IMPROVE
THE
BEST
KNOWN
COMPETITIVE
RATIO
FOR
THIS
PROBLEM
A
SURVEY
ON
ALGORITHMIC
PROBLEMS
IN
POWER
MANAGEMENT
WAS
GIVEN
BY
IRANI
AND
PRUHS
DEFINITIONS
IN
THIS
SECTION
WE
RECAP
THE
DEFINITIONS
INTRODUCED
SO
FAR
AND
INTRODUCE
SOME
DEFINITIONS
THAT
WE
WILL
USE
THROUGHOUT
THE
ARTICLE
WE
ALSO
MAKE
SOME
OBSERVATIONS
ABOUT
THESE
DEFINITIONS
A
PROBLEM
INSTANCE
CONSISTS
OF
N
TASKS
TASK
I
HAS
A
RELEASE
TIME
RI
A
DEADLINE
DI
RI
AND
WORK
WI
IN
THE
ONLINE
VERSION
OF
THE
PROBLEM
THE
SCHEDULER
LEARNS
ABOUT
A
TASK
ONLY
AT
ITS
RELEASE
TIME
AT
THIS
TIME
THE
SCHEDULER
ALSO
LEARNS
THE
EXACT
WORK
REQUIREMENT
AND
THE
DEADLINE
OF
THE
TASK
WE
ASSUME
THAT
TIME
IS
CONTINUOUS
A
SCHEDULE
SPECIFIES
FOR
EACH
TIME
A
TASK
TO
BE
RUN
AND
A
SPEED
AT
WHICH
TO
RUN
THE
TASK
THE
SPEED
IS
THE
AMOUNT
OF
WORK
PERFORMED
ON
THE
TASK
PER
UNIT
TIME
A
TASK
WITH
WORK
W
RUN
AT
A
CONSTANT
SPEED
THUS
TAKES
W
TIME
TO
COMPLETE
MORE
GENERALLY
THE
WORK
DONE
ON
A
TASK
DURING
A
TIME
PERIOD
IS
THE
INTEGRAL
OVER
THAT
TIME
PERIOD
OF
THE
SPEED
AT
WHICH
THE
TASK
IS
RUN
A
SCHEDULE
IS
FEASIBLE
IF
FOR
EACH
TASK
I
WORK
AT
LEAST
WI
IS
DONE
ON
TASK
I
DURING
RI
DI
NOTE
THAT
THE
TIMES
AT
WHICH
WORK
IS
PERFORMED
ON
TASK
I
DO
NOT
HAVE
TO
BE
CONTIGUOUS
IF
A
TASK
IS
RUN
AT
SPEED
THEN
THE
POWER
IS
P
SΑ
FOR
SOME
CONSTANT
Α
THE
ENERGY
USED
DURING
A
TIME
PERIOD
IS
THE
INTEGRAL
OF
THE
POWER
OVER
THAT
TIME
PERIOD
IN
THE
ENERGY
VERSION
OF
OUR
PROBLEM
THE
OBJECTIVE
IS
TO
MINIMIZE
THE
TOTAL
ENERGY
E
USED
BY
THE
SCHEDULE
WE
NOW
TURN
TO
TEMPERATURE
WE
ASSUME
WITHOUT
LOSS
OF
GENERALITY
THAT
THE
INITIAL
TEMPERATURE
IS
WE
ASSUME
THAT
THE
TEMPERATURE
T
T
AT
TIME
T
IS
THEN
GIVEN
BY
THE
COOLING
EQUATION
T
T
T
AP
T
BT
T
WHERE
P
T
IS
THE
POWER
AT
TIME
T
AND
A
AND
B
ARE
NONNEGATIVE
CONSTANTS
IF
T
IS
A
TEMPERATURE
FUNCTION
THEN
T
T
WILL
ALWAYS
REFER
TO
THE
DERIVATIVE
OF
T
WITH
RESPECT
TO
TIME
WE
MAKE
OBSERVATIONS
ABOUT
THIS
COOLING
EQUATION
NOTE
THAT
BY
RESCALING
TEMPERATURE
OR
ENERGY
ONE
COULD
ASSUME
THAT
A
THUS
A
WILL
NOT
PLAY
MUCH
OF
A
ROLE
IN
OUR
ANALYSIS
AND
THE
KEY
PARAMETER
IS
THE
COOLING
PARAMETER
B
NOTE
THAT
THE
TEMPERATURE
FUNCTION
WILL
BE
CONTINUOUS
EVEN
IF
THE
POWER
FUNCTION
IS
NOT
IF
WE
WANT
TO
MAINTAIN
A
CONSTANT
TEMPERATURE
TZ
THEN
AS
T
T
WILL
EQUAL
IT
IS
SUFFICIENT
TO
RUN
AT
POWER
BTZ
A
ONCE
TEMPERATURE
TZ
IS
REACHED
SOLVING
THE
COOLING
EQUATION
FOR
P
T
YIELDS
P
T
T
T
T
BT
T
A
THUS
ONE
CAN
SPECIFY
A
POWER
FUNCTION
AND
HENCE
A
SPEED
FUNCTION
BY
SPECIFYING
A
TEMPERATURE
FUNCTION
BY
EQ
THE
ENERGY
USED
DURING
AN
INTERVAL
X
Y
IS
Y
Y
T
T
T
BT
T
DT
X
X
A
R
Y
T
B
R
Y
T
Y
T
X
B
R
Y
SINCE
P
SΑ
THE
WORK
DONE
DURING
A
TIME
INTERVAL
X
Y
IS
R
Y
F
T
T
T
BT
T
Α
IN
THE
TEMPERATURE
VERSION
OF
OUR
PROBLEM
THE
OBJECTIVE
IS
TO
MINIMIZE
THE
MAXIMUM
TEMPERATURE
T
REACHED
DURING
THE
SCHEDULE
LET
C
LN
CALL
A
TIME
INTERVAL
OF
LENGTH
C
A
C
INTERVAL
AS
WE
WILL
SEE
THE
PROBLEM
OF
MINIMIZING
THE
MAXIMUM
TEMPERATURE
IS
RELATED
TO
THE
PROBLEM
OF
MINIMIZING
THE
MAXIMUM
ENERGY
C
USED
IN
ANY
C
INTERVAL
DURING
THE
SCHEDULE
IN
THE
MAXIMUM
SPEED
VERSION
OF
OUR
PROBLEM
THE
OBJECTIVE
FUNCTION
IS
TO
MIN
IMIZE
THE
MAXIMUM
SPEED
REACHED
DURING
THE
SCHEDULE
IN
THE
MAXIMUM
POWER
VERSION
OF
OUR
PROBLEM
THE
OBJECTIVE
FUNCTION
IS
TO
MINIMIZE
THE
MAXIMUM
POWER
REACHED
DURING
THE
SCHEDULE
IF
A
IS
A
SCHEDULING
ALGORITHM
THEN
A
I
DENOTES
THE
SCHEDULE
OUTPUT
BY
A
ON
INPUT
I
E
A
I
WILL
DENOTE
THE
ENERGY
OF
A
I
AND
T
A
I
THE
MAXIMUM
TEMPERATURE
FOR
CONVENIENCE
WE
WILL
USE
OPT
I
TO
REPRESENT
AN
OPTIMAL
SCHEDULE
FOR
THE
OBJECTIVE
UNDER
CONSIDERATION
E
OPT
I
WILL
DENOTE
THE
OPTIMAL
ENERGY
AND
T
OPT
I
THE
OPTIMAL
TEMPERATURE
WHEN
I
IS
CLEARLY
UNDERSTOOD
FROM
CONTEXT
WE
MAY
DROP
IT
FROM
THE
NOTATION
A
SCHEDULE
IS
R
COMPETITIVE
OR
R
APPROXIMATE
FOR
A
PARTICULAR
OBJECTIVE
FUNC
TION
IF
THE
VALUE
OF
THAT
OBJECTIVE
FUNCTION
ON
THE
SCHEDULE
IS
AT
MOST
R
TIMES
THE
VALUE
OF
THE
OBJECTIVE
FUNCTION
ON
AN
OPTIMAL
SCHEDULE
AN
ONLINE
SCHEDULING
ALGORITHM
A
IS
R
COMPETITIVE
OR
HAS
COMPETITIVE
RATIO
R
IF
A
I
IS
R
COMPETITIVE
FOR
ALL
IN
STANCES
AN
OFFLINE
SCHEDULING
ALGORITHM
A
IS
R
APPROXIMATE
OR
HAS
APPROXIMATION
RATIO
R
IF
A
I
IS
R
APPROXIMATE
FOR
ALL
INSTANCES
AN
ONLINE
ALGORITHM
A
IS
COOLING
OBLIVIOUS
IF
A
IS
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
FOR
ALL
COOLING
PARAMETERS
B
WE
NOW
DEFINE
THE
ALGORITHMS
THAT
WE
CONSIDER
IN
THIS
PAPER
ALONG
WITH
RELATED
CONCEPTS
WE
START
WITH
THE
OFFLINE
SPEED
SCALING
ALGORITHM
YDS
PROPOSED
BY
YAO
ET
AL
LET
W
DENOTE
THE
WORK
THAT
HAS
RELEASE
TIME
AT
LEAST
AND
HAS
DEADLINE
AT
MOST
THE
INTENSITY
I
OF
THE
TIME
INTERVAL
IS
DEFINED
TO
BE
W
ALGORITHM
YDS
THE
ALGORITHM
REPEATS
THE
FOLLOWING
STEPS
UNTIL
ALL
JOBS
ARE
SCHEDULED
LET
BE
THE
MAXIMUM
INTENSITY
TIME
INTERVAL
THE
PROCESSOR
WILL
RUN
AT
SPEED
I
DURING
AND
SCHEDULE
ALL
THE
JOBS
COMPRISING
W
ALWAYS
RUNNING
THE
RELEASED
UNFINISHED
TASK
WITH
THE
EARLIEST
DEADLINE
THEN
THE
INSTANCE
IS
MODIFIED
AS
IF
THE
TIMES
DIDN
T
EXIST
THAT
IS
ALL
DEADLINES
DI
ARE
REDUCED
TO
MAX
DI
AND
ALL
RELEASE
TIMES
RI
ARE
REDUCED
TO
MAX
RI
IT
IS
EASY
TO
SEE
THAT
THE
YDS
ALGORITHM
IS
OPTIMAL
WITH
RESPECT
TO
MAXIMUM
SPEED
AND
HENCE
MAXIMUM
POWER
BY
NOTING
THAT
THE
MAXIMUM
SPEED
OF
ANY
SCHEDULE
MUST
BE
AT
LEAST
THE
INTENSITY
OF
THE
MAXIMUM
INTENSITY
INTERVAL
FOUND
BY
YDS
AT
THE
VERY
BEGINNING
I
E
WHEN
NO
JOBS
HAVE
BEEN
SCHEDULED
AND
THE
INTENSITY
OF
THE
MAXIMUM
INTENSITY
INTERVAL
AND
HENCE
THE
SPEED
AT
WHICH
YDS
SCHEDULES
JOBS
ONLY
DECREASES
AS
YDS
PROCEEDS
NOTE
THAT
THE
YDS
SCHEDULE
HAS
THE
PROPERTY
THAT
EACH
TASK
IS
RUN
AT
A
FIXED
SPEED
HOWEVER
THIS
SPEED
MAY
BE
DIFFERENT
FOR
DIFFERENT
TASKS
LET
Y
T
BE
THE
SPEED
OF
YDS
AT
TIME
T
WE
NOW
DEFINE
THE
ONLINE
SPEED
SCALING
ALGORITHMS
OA
AND
AVR
PROPOSED
BY
YAO
ET
AL
ALGORITHM
OA
MAINTAIN
THE
INVARIANT
THAT
AT
ALL
TIMES
T
THE
TASK
WITH
THE
EARLIEST
DEADLINE
IS
RUN
AT
SPEED
MAXT
W
T
T
WHERE
W
T
IS
THE
UNFINISHED
WORK
THAT
HAS
DEADLINE
WITHIN
THE
NEXT
T
UNITS
OF
TIME
AN
ALTERNATIVE
DESCRIPTION
IS
THAT
OA
SCHEDULE
FOR
THE
FUTURE
IS
ALWAYS
THE
OPTIMAL
ENERGY
YDS
SCHEDULE
BASED
ON
THE
CURRENT
STATE
ALGORITHM
AVR
MAINTAIN
THE
INVARIANT
THAT
AT
ALL
TIMES
T
THE
EARLIEST
DEADLINE
TASK
IS
RUN
AT
SPEED
I
J
T
WI
WHERE
J
T
IS
THE
COLLECTION
OF
TASKS
I
WITH
RI
T
DI
INTUITIVELY
AVR
RUNS
EACH
TASK
AT
THE
OPTIMAL
SPEED
UNDER
THE
ASSUMPTION
THAT
IT
IS
THE
ONLY
TASK
IN
THE
SYSTEM
WE
NOW
TURN
TO
DEFINING
OUR
PROPOSED
ONLINE
SPEED
SCALING
ALGORITHM
BANSAL
ET
AL
FOR
T
LET
W
T
DENOTE
AMOUNT
OF
WORK
THAT
HAS
RELEASE
TIME
AT
LEAST
AND
DEADLINE
AT
MOST
AND
THAT
HAS
ALREADY
ARRIVED
BY
TIME
T
WE
DEFINE
THREE
MORE
TERMS
Q
T
P
T
AND
V
T
WHICH
WILL
BE
USEFUL
IN
THE
DESCRIPTION
AND
ANALYSIS
OF
OUR
NEW
ONLINE
ALGORITHM
BKP
LET
Q
T
BE
THE
MAXIMUM
INTENSITY
OF
AN
INTERVAL
CONTAINING
T
THAT
IS
Q
T
MAX
I
SUCH
THAT
T
INTUITIVELY
Q
T
CAN
BE
VIEWED
AS
YDS
SPEED
AT
TIME
T
NOTE
THAT
THIS
STATEMENT
IS
NOT
EXACTLY
TRUE
BUT
Q
T
IS
AN
UPPER
BOUND
ON
YDS
SPEED
AT
TIME
T
LET
P
T
BE
DEFINED
BY
P
T
MAX
W
T
SUCH
THAT
T
T
T
INTUITIVELY
P
T
IS
THE
ONLINE
ALGORITHM
ESTIMATE
OF
THE
SPEED
AT
WHICH
YDS
WOULD
WORK
AT
TIME
T
BASED
ON
THE
KNOWLEDGE
OF
TASKS
THAT
HAVE
ARRIVED
BY
TIME
T
LET
V
T
BE
DEFINED
BY
V
T
MAX
W
T
ET
E
T
T
T
T
T
T
T
E
T
T
T
WE
ARE
NOW
READY
TO
DEFINE
THE
ONLINE
ALGORITHM
BKP
ALGORITHM
BKP
AT
TIME
T
WORK
AT
SPEED
E
V
T
ON
THE
UNFINISHED
TASK
WITH
THE
EARLIEST
DEADLINE
NOTE
THAT
W
T
P
T
AND
V
T
MAY
BE
COMPUTED
BY
AN
ONLINE
ALGORITHM
AT
TIME
T
IT
IS
A
MATTER
OF
TASTE
WHETHER
WE
DEFINE
BKP
TO
RUN
AT
SPEED
E
V
T
OR
E
P
T
ALL
OF
OUR
RESULTS
HOLD
FOR
BOTH
VARIATIONS
THE
FOLLOWING
LEMMA
WHICH
WE
USE
FREQUENTLY
RELATES
V
T
P
T
AND
Q
T
LEMMA
FOR
ALL
INSTANCES
I
AND
FOR
ALL
TIMES
T
V
T
P
T
Q
T
PROOF
THE
SPEED
V
T
IS
EQUIVALENT
TO
A
RESTRICTED
VARIANT
OF
P
T
WHERE
INSTEAD
OF
CONSIDERING
THE
MAXIMUM
OVER
ALL
SUCH
THAT
T
WE
REQUIRE
THAT
AND
TO
BE
RELATED
SUCH
THAT
T
E
T
THUS
V
T
P
T
FINALLY
IT
IS
OBVIOUS
THAT
P
T
Q
T
SINCE
W
T
W
FOR
ANY
T
PROPERTIES
OF
THE
YDS
SCHEDULE
ENERGY
WE
SHOW
THAT
THE
ENERGY
OPTIMALITY
OF
THE
YDS
SCHEDULE
FOLLOWS
AS
A
DIRECT
CONSEQUENCE
OF
THE
WELL
KNOWN
KKT
OPTIMALITY
CONDITIONS
FOR
CONVEX
PROGRAMS
THEOREM
YDS
IS
OPTIMAL
WITH
RESPECT
TO
ENERGY
PROOF
WE
START
BY
STATING
THE
KKT
CONDITIONS
NEXT
WE
SHOW
HOW
TO
EXPRESS
THE
ENERGY
PROBLEM
AS
A
CONVEX
PROGRAM
AND
THEN
SHOW
THE
RESULT
OF
APPLYING
THE
KKT
CONDITIONS
TO
THIS
CONVEX
PROGRAM
CONSIDER
A
CONVEX
PROGRAM
MIN
X
FI
X
I
N
ASSUME
THAT
THIS
PROGRAM
IS
STRICTLY
FEASIBLE
THAT
IS
THERE
IS
SOME
POINT
X
WHERE
FI
X
FOR
I
N
ASSUME
THAT
THE
FI
ARE
ALL
DIFFERENTIABLE
LET
ΛI
I
N
BE
A
VARIABLE
LAGRANGIAN
MULTIPLIER
ASSOCIATED
WITH
THE
FUNCTION
FI
X
THEN
THE
NECESSARY
AND
SUFFICIENT
KKT
CONDITIONS
FOR
SOLUTIONS
X
AND
Λ
TO
BE
FEASIBLE
PRIMAL
AND
DUAL
SOLUTIONS
ARE
BOYD
AND
VANDENBERGHE
FI
X
I
N
ΛI
I
N
ΛI
FI
X
I
N
N
X
ΛI
FI
X
I
TO
STATE
THE
ENERGY
MINIMIZATION
PROBLEM
AS
A
CONVEX
PROGRAM
WE
BREAK
TIME
INTO
INTERVALS
TM
AT
RELEASE
TIMES
AND
DEADLINES
OF
THE
TASKS
NOTE
THAT
BECAUSE
THE
SET
OF
AVAILABLE
JOBS
DOES
NOT
CHANGE
OVER
ANY
SUCH
INTERVAL
AND
BECAUSE
OF
THE
CONVEXITY
OF
THE
SPEED
TO
POWER
FUNCTION
WE
MAY
ASSUME
THAT
THE
PROCESSOR
RUNS
AT
CONSTANT
SPEED
THROUGHOUT
ANY
SUCH
INTERVAL
LET
J
I
BE
THE
TASKS
THAT
CAN
FEASIBLY
BE
EXECUTED
DURING
THE
TIME
INTERVAL
II
TI
TI
AND
J
J
BE
INTERVALS
DURING
WHICH
TASK
J
CAN
BE
FEASIBLY
EXECUTED
WE
INTRODUCE
A
VARIABLE
WI
J
FOR
J
J
I
THAT
REPRESENTS
THE
WORK
DONE
ON
TASK
J
DURING
TIME
TI
TI
OUR
INTERVAL
INDEXED
PROGRAM
IS
THEN
MIN
E
WI
J
Α
W
J
I
J
J
WI
J
J
N
I
J
J
I
TI
TI
TI
TI
E
WI
J
I
M
J
J
I
IT
IS
EASY
TO
VERIFY
THAT
THE
PROGRAM
IS
CONVEX
WE
NOW
APPLY
THE
KKT
CONDITIONS
TO
THIS
PROGRAM
WE
ASSOCIATE
A
DUAL
VARIABLE
Δ
J
WITH
INEQUALITY
J
IN
LINE
A
DUAL
VARIABLE
Β
WITH
THE
INEQUALITY
IN
LINE
AND
A
DUAL
VARIABLE
ΓI
J
WITH
INEQUALITY
I
J
IN
LINE
WE
NOW
EVALUATE
LINE
OF
THE
KKT
CONDITIONS
FOR
OUR
CONVEX
PROGRAM
WE
HAVE
E
J
Δ
J
W
J
I
J
J
WI
J
Β
M
J
J
I
WI
J
Α
T
E
I
M
N
TI
TI
I
I
ΓI
J
WI
J
CONSIDERING
THE
COMPONENT
OF
THIS
EQUATION
CORRESPONDING
TO
THE
VARIABLE
E
WE
HAVE
Β
OR
EQUIVALENTLY
Β
CONSIDERING
THE
COMPONENT
CORRESPONDING
TO
THE
VARIABLE
WI
J
WE
HAVE
ΔJ
ΒΑ
F
K
J
I
WI
K
Α
T
ΓI
J
CONSIDER
A
WI
J
SUCH
THAT
WI
J
WE
KNOW
THAT
BY
COMPLEMENTARY
SLACKNESS
EQ
THAT
IT
MUST
BE
THE
CASE
THAT
ΓI
J
HENCE
Δ
J
Α
F
K
J
I
WI
K
Α
T
HENCE
THE
INTERPRETATION
OF
THE
DUAL
VARIABLE
Δ
J
IS
Α
TIMES
THE
SPEED
AT
WHICH
THE
PROCESSOR
RUNS
DURING
INTERVAL
I
RAISED
TO
THE
POWER
OF
Α
THIS
QUANTITY
AND
HENCE
THE
SPEED
OF
THE
PROCESSOR
MUST
BE
THE
SAME
FOR
EACH
I
SUCH
THAT
WI
J
THAT
IS
DURING
EACH
INTERVAL
I
IN
WHICH
TASK
J
IS
RUN
NOW
CONSIDER
A
WI
J
SUCH
THAT
WI
J
REARRANGING
EQ
WE
FIND
THAT
ΓI
J
Α
F
K
J
I
WI
K
Α
T
Δ
J
SINCE
ΓI
J
IS
NONNEGATIVE
THE
PROCESSOR
RUNS
AT
LEAST
AS
FAST
DURING
INTERVAL
I
AS
DURING
THE
INTERVALS
WHERE
TASK
J
IS
RUN
THUS
WE
CAN
CONCLUDE
THAT
NECESSARY
CONDITIONS
FOR
A
PRIMAL
FEASIBLE
SOLUTION
TO
BE
OPTIMAL
ARE
FOR
EACH
TASK
J
THE
PROCESSOR
RUNS
AT
THE
SAME
SPEED
SAY
J
IN
ALL
INTERVALS
I
IN
WHICH
TASK
J
IS
RUN
THE
PROCESSOR
RUNS
AT
SPEED
NO
LESS
THAN
J
DURING
INTERVALS
I
SUCH
THAT
J
J
I
AND
TASK
J
IS
NOT
RUN
THE
YDS
SCHEDULE
CLEARLY
HAS
THESE
PROPERTIES
WE
CAN
USE
EQS
AND
TO
FIND
A
DUAL
FEASIBLE
SOLUTION
SATISFYING
THE
KKT
CONDITIONS
AND
THUS
WE
HAVE
FOUND
OPTIMAL
PRIMAL
AND
DUAL
SOLUTIONS
THE
YDS
SCHEDULE
CORRESPONDING
TO
THE
PRIMAL
SOLUTION
IS
THUS
OPTIMAL
THE
RELATIONSHIP
BETWEEN
ENERGY
AND
TEMPERATURE
WE
SHOW
IN
THEOREM
THAT
THE
MAXIMUM
TEMPERATURE
T
OF
A
SCHEDULE
IS
WITHIN
A
FACTOR
OF
TWO
OF
A
C
RECALL
THE
PARAMETER
A
IN
THE
COOLING
EQUATION
AND
OUR
DEFINITION
OF
A
C
INTERVAL
WHERE
C
LN
AND
THAT
C
IS
THE
MAXIMUM
ENERGY
EXPENDED
OVER
ANY
C
INTERVAL
FURTHER
RECALL
THAT
WE
ASSUME
THAT
THE
INITIAL
TEMPERATURE
IS
ZERO
WE
SHOW
IN
THEOREM
THAT
THE
ENERGY
OPTIMAL
YDS
SCHEDULE
IS
APPROXIMATE
WITH
RESPECT
TO
TEMPERATURE
FOR
ALL
COOLING
PARAMETERS
B
THAT
IS
YDS
IS
COOLING
OBLIVIOUS
GIVEN
THEOREM
TO
PROVE
THEOREM
IT
IS
SUFFICIENT
TO
SHOW
THAT
YDS
IS
APPROXIMATE
WITH
RESPECT
TO
THE
OBJECTIVE
OF
THE
MAXIMUM
ENERGY
EXPENDED
OVER
ANY
C
INTERVAL
THEOREM
FOR
ANY
SCHEDULE
AND
FOR
ANY
COOLING
PARAMETER
B
A
C
T
C
PROOF
IF
B
THEN
A
C
T
A
E
AND
THE
RESULT
HOLDS
SO
ASSUME
FROM
NOW
ON
THAT
B
WE
REWRITE
OUR
COOLING
EQUATION
DT
T
AP
T
BT
T
AS
D
EXP
BT
T
T
DT
A
EXP
BT
P
T
INTEGRATING
THIS
EQUATION
OVER
A
C
INTERVAL
THAT
ENDS
AT
SOME
TIME
WE
GET
T
T
BT
T
T
C
BT
CB
A
R
BT
P
T
DT
NOTE
T
X
FOR
X
WE
FIRST
SHOW
THAT
T
C
SUPPOSE
THAT
THE
TEMPERATURE
T
IS
ACHIEVED
AT
TIME
WE
SIMPLIFY
EQ
AS
FOLLOWS
SINCE
EXP
BT
IS
INCREASING
IN
T
WE
HAVE
THAT
C
EXP
BT
P
T
DT
EXP
BT
C
P
T
DT
THUS
T
T
T
T
C
CB
A
R
P
T
DT
AS
T
C
T
T
IT
FOLLOWS
THAT
T
CB
A
R
P
T
DT
A
C
AND
AS
CB
LN
IT
FOLLOWS
THAT
T
A
C
C
EXP
CB
WE
NOW
SHOW
THAT
T
A
C
LET
C
BEA
C
INTERVAL
WHERE
C
ENERGY
IS
USED
AGAIN
WE
START
WITH
EQ
USING
THE
FACT
THAT
THE
TEMPERATURE
AT
ANY
TIME
IS
NONNEGATIVE
AS
THE
ENVIRONMENTAL
TEMPERATURE
IS
AND
HENCE
IN
PARTICULAR
THAT
T
C
AND
THAT
EXP
BT
IS
AN
INCREASING
FUNCTION
OF
T
IT
FOLLOWS
THAT
T
T
BT
A
R
BT
P
T
DT
A
BT
CB
R
P
T
DT
THUS
EXP
CB
A
C
T
T
T
EXP
CB
A
C
A
C
NOTE
THAT
YDS
IS
NOT
OPTIMAL
FOR
MINIMIZING
THE
MAXIMUM
TEMPERATURE
NOR
FOR
MINIMIZING
THE
MAXIMUM
TOTAL
ENERGY
IN
ANY
C
INTERVAL
THE
FACT
THAT
YDS
IS
NOT
OPTIMAL
FOR
TEMPERATURE
CAN
BE
SEEN
ON
SINGLE
TASK
INSTANCES
WHERE
THE
OPTIMAL
TEMPERATURE
SCHEDULE
MUST
RUN
AT
A
SPEED
THAT
FOLLOWS
SOME
NON
CONSTANT
EULER
LAGRANGE
TEMPERATURE
CURVE
SEE
THEOREM
FOR
MORE
DETAILS
THAT
YDS
IS
NOT
OPTIMAL
FOR
MINIMIZING
THE
MAXIMUM
ENERGY
USED
IN
ANY
C
INTERVAL
CAN
BE
SEEN
FROM
THE
FOLLOWING
INSTANCE
WITHOUT
LOSS
OF
GENERALITY
WE
CAN
NORMALIZE
SO
THAT
C
THERE
ARE
TWO
TASKS
WITH
WORK
EACH
BOTH
ARRIVING
AT
TIME
WITH
DEADLINES
AND
RESPECTIVELY
IF
THE
FIRST
TASK
IS
RUN
AT
SPEED
FROM
TIME
TO
AND
THE
SECOND
AT
SPEED
FROM
TO
THE
MAXIMUM
ENERGY
IN
ANY
INTERVAL
OF
LENGTH
IS
YDS
RUNS
THE
FIRST
TASK
AT
SPEED
FROM
TIME
TO
AND
THE
SECOND
AT
SPEED
FROM
TO
THE
ENERGY
USED
FROM
TIME
TO
FOR
INSTANCE
IS
LARGER
THAN
NOTE
THAT
THIS
HOLDS
FOR
ANY
SPEED
TO
POWER
FUNCTION
OF
THE
FORM
P
SΑ
WITH
Α
ENERGY
IN
A
C
INTERVAL
FOR
THE
REST
OF
THIS
SECTION
WE
ONLY
CONSIDER
THE
OBJECTIVE
OF
MINIMIZING
THE
MAXIMUM
ENERGY
IN
ANY
C
INTERVAL
THIS
WILL
CULMINATE
IN
LEMMA
WHICH
STATES
THAT
THE
YDS
SCHEDULE
IS
APPROXIMATE
WITH
RESPECT
TO
THIS
OBJECTIVE
WE
FIRST
REQUIRE
SOME
PRELIMINARY
DEFINITIONS
AND
OBSERVATIONS
LET
CY
I
DENOTE
A
C
INTERVAL
IN
YDS
I
THAT
USES
ENERGY
C
YDS
I
A
MAXIMUM
ENERGY
C
INTERVAL
YDS
I
LET
E
BE
A
SMALL
POSITIVE
CONSTANT
THAT
WE
WILL
DEFINE
PRECISELY
LATER
LET
THE
SPEED
BE
DEFINED
AS
EC
YDS
I
C
Α
WE
CALL
A
TASK
IN
I
SLOW
IF
IT
RUNS
AT
A
SPEED
STRICTLY
LESS
THAN
IN
YDS
I
THIS
NOTION
IS
WELL
DEFINED
BECAUSE
EACH
TASK
RUNS
AT
CONSTANT
SPEED
IN
THE
YDS
SCHEDULE
THE
REST
OF
THE
TASKS
ARE
CALLED
FAST
LET
T
DENOTE
THE
SPEED
AT
TIME
T
IN
YDS
I
DEFINE
AN
ISLAND
TO
BE
A
MAXIMAL
INTERVAL
OF
TIME
WHERE
T
WE
NOW
GIVE
SOME
SIMPLE
BUT
USEFUL
PROPERTIES
OF
THE
SCHEDULE
YDS
I
CLAIM
LET
G
BE
AN
ISLAND
YDS
SCHEDULES
WITHIN
G
EXACTLY
THOSE
TASKS
K
SUCH
THAT
RK
DK
PROOF
IT
IS
TRIVIAL
THAT
ALL
SUCH
TASKS
MUST
BE
EXECUTED
WHOLLY
IN
G
TO
SEE
THAT
ONLY
SUCH
TASKS
ARE
EXECUTED
IN
G
OBSERVE
THAT
IF
A
TASK
CAN
FEASIBLY
BE
EXECUTED
OUTSIDE
G
AND
IS
PARTIALLY
OR
WHOLLY
EXECUTED
IN
G
WE
WOULD
HAVE
A
CONTRADICTION
TO
THE
ENERGY
OPTIMALITY
OF
YDS
I
SINCE
WORK
COULD
BE
SHIFTED
FROM
AN
INTERVAL
OF
HIGHER
SPEED
TO
AN
INTERVAL
OF
LOWER
SPEED
CLAIM
FOR
ANY
ISLAND
G
OF
LENGTH
NO
MORE
THAN
C
IN
ANY
INSTANCE
I
C
OPT
I
IS
AT
LEAST
THE
ENERGY
CONSUMED
IN
G
BY
YDS
PROOF
FOLLOWS
FROM
CLAIM
AND
THE
ENERGY
OPTIMALITY
OF
YDS
SINCE
OPT
MUST
EXECUTE
IN
G
AT
LEAST
THE
TASKS
EXECUTED
BY
YDS
IN
G
WE
NOW
SHOW
THAT
MOST
OF
THE
ENERGY
IN
CY
I
IS
CONTAINED
IN
FAST
TASKS
LEMMA
LET
H
I
DENOTE
THE
SET
OF
ISLANDS
THAT
INTERSECT
CY
I
AND
LET
E
H
I
DENOTE
THE
ENERGY
CONSUMED
UNDER
YDS
IN
THE
ISLANDS
H
I
THEN
WE
HAVE
THAT
E
H
I
E
C
YDS
I
PROOF
CONSIDER
THE
TIMES
IN
CY
I
WHERE
T
THOSE
PERIODS
ARE
CONTAINED
IN
H
I
SO
C
YDS
I
E
H
I
IS
AT
MOST
THE
ENERGY
USED
BY
THE
YDS
I
DURING
THE
TIMES
T
IN
THE
C
INTERVAL
CY
I
WHEN
T
THUS
C
YDS
I
E
H
I
CSΑ
WHICH
IS
AT
MOST
EC
YDS
I
BY
THE
DEFINITION
OF
THE
CLAIMED
INEQUALITY
THEN
FOLLOWS
WE
NOW
SHOW
THAT
YDS
IS
APPROXIMATE
WITH
RESPECT
TO
MINIMIZING
THE
MAXI
MUM
ENERGY
USED
OVER
ANY
C
INTERVAL
LEMMA
FOR
ANY
INSTANCE
I
C
OPT
I
MIN
F
E
C
YDS
I
E
C
YDS
I
CHOOSING
E
IT
FOLLOWS
THAT
C
OPT
I
C
YDS
I
PROOF
CONSIDER
AN
ISLAND
G
OF
I
AND
LET
G
BE
THE
LENGTH
OF
G
AS
THE
YDS
SCHEDULE
FOR
I
RUNS
AT
SPEED
AT
LEAST
DURING
G
THE
TOTAL
ENERGY
CONSUMED
BY
YDS
IS
AT
LEAST
G
SΑ
BY
CLAIM
ALL
THE
TASKS
IN
I
THAT
YDS
RUNS
IN
G
MUST
ALSO
BE
RUN
IN
G
IN
ANY
FEASIBLE
SCHEDULE
SO
IT
MUST
BE
THE
CASE
THAT
THE
TOTAL
ENERGY
CONSUMED
BY
ANY
FEASIBLE
SCHEDULE
FOR
I
DURING
G
HAS
TO
ALSO
BE
AT
LEAST
G
SΑ
IF
G
C
THEN
BY
A
SIMPLE
AVERAGING
ARGUMENT
FOR
ANY
FEASIBLE
SCHEDULE
THERE
IS
SOME
C
INTERVAL
THAT
IS
TOTALLY
CONTAINED
IN
G
WITH
THE
PROPERTY
THAT
THE
ENERGY
USED
DURING
THIS
C
INTERVAL
IS
AT
LEAST
G
SΑ
I
G
CL
IN
TURN
THIS
IS
AT
LEAST
CSΑ
WHICH
BY
THE
DEFINITION
OF
Α
EQUALS
E
C
YDS
I
THUS
C
OPT
I
E
C
YDS
I
IF
G
C
IF
ALL
THE
ISLANDS
HAVE
LENGTH
NO
MORE
THAN
C
THEN
CONSIDER
THE
ISLANDS
THAT
INTERSECT
CY
I
IF
SOME
SUCH
ISLAND
G
HAS
ENERGY
AT
LEAST
E
C
YDS
I
THE
RESULT
FOLLOWS
BY
CLAIM
NOW
SUPPOSE
THAT
ALL
ISLANDS
THAT
INTERSECT
CY
I
HAVE
ENERGY
LESS
THAN
E
C
YDS
I
BY
LEMMA
WE
KNOW
THAT
IN
YDS
I
THE
TOTAL
ENERGY
DURING
THE
ISLANDS
INTERSECTING
CY
I
IS
AT
LEAST
E
C
YDS
I
AS
AT
MOST
TWO
ISLANDS
CAN
LIE
PARTIALLY
IN
CY
I
AT
LEAST
E
C
YDS
I
ENERGY
IS
IN
ISLANDS
THAT
ARE
TOTALLY
CONTAINED
INSIDE
CY
I
AND
HENCE
THE
RESULT
FOLLOWS
BY
CLAIM
WE
CAN
NOW
CONCLUDE
THAT
YDS
IS
COOLING
OBLIVIOUS
THEOREM
THE
ENERGY
OPTIMAL
ALGORITHM
YDS
IS
A
APPROXIMATION
WITH
RESPECT
TO
MAXIMUM
TEMPERATURE
PROOF
BY
LEMMA
WE
KNOW
THAT
C
OPT
I
C
YDS
I
BY
THE
OREM
WE
KNOW
THAT
T
OPT
I
C
OPT
I
AND
C
YDS
I
COMBINING
THESE
THREE
INEQUALITIES
GIVES
THAT
T
OPT
I
T
YDS
I
THE
OA
AVR
AND
CONSTANT
TEMPERATURE
ALGORITHMS
IN
THIS
SECTION
WE
CONSIDER
THE
ONLINE
ALGORITHMS
AVR
AND
OA
PROPOSED
BY
YAO
ET
AL
AND
THE
CLASS
OF
CONSTANT
TEMPERATURE
ALGORITHMS
RECALL
THE
DEFINITIONS
OF
AVR
AND
OA
IN
SECTION
WE
SHOW
THAT
BOTH
AVR
AND
OA
HAVE
A
COMPETITIVE
RATIO
OF
AT
LEAST
ΑΑ
WITH
RESPECT
TO
ENERGY
WE
THEN
SHOW
THAT
OA
IS
IN
FACT
EXACTLY
ΑΑ
COMPETITIVE
WE
SHOW
THAT
BOTH
AVR
AND
OA
ARE
NOT
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
WE
ALSO
SHOW
THAT
ANOTHER
NATURAL
ALGORITHM
THAT
WE
CALL
THE
CONSTANT
TEMPERATURE
ALGORITHM
IS
NOT
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
UNDER
A
NATURAL
DEFINITION
OF
COMPETITIVENESS
FOR
THIS
CLASS
OF
ALGORITHMS
ENERGY
BEFORE
GIVING
AN
EXPLICIT
LOWER
BOUND
INSTANCE
FOR
THE
COMPETI
TIVE
RATIO
OF
OA
AND
AVR
WITH
RESPECT
TO
ENERGY
WE
NEED
THE
FOLLOWING
TECHNICAL
LEMMA
LEMMA
IF
Α
AND
X
Y
THEN
X
Y
Α
XΑ
ΑXΑ
PROOF
SETTING
T
Y
X
AND
DIVIDING
THROUGH
BY
XΑ
THE
INEQUALITY
ABOVE
IS
EQUIVALENT
TO
T
Α
ΑT
OR
T
Α
ΑT
WHICH
WE
NEED
TO
SHOW
HOLDS
WHEN
T
NOTE
THAT
THE
LEFT
HAND
SIDE
IS
WHEN
T
NOW
DIFFERENTIATING
THE
LEFT
HAND
SIDE
WITH
RESPECT
TO
T
GIVES
Α
T
Α
Α
WHICH
IS
ALWAYS
POSITIVE
WHEN
T
THUS
T
Α
ΑT
IS
INCREASING
AND
THUS
POSITIVE
WHEN
T
LEMMA
THE
COMPETITIVE
RATIO
OF
AVR
AND
OA
WITH
RESPECT
TO
ENERGY
IS
AT
LEAST
ΑΑ
PROOF
THE
INSTANCE
IS
DEFINED
AS
FOLLOWS
ALL
TASKS
HAVE
THE
SAME
DEADLINE
N
FOR
I
N
A
TASK
OF
WORK
N
I
Α
ARRIVES
AT
TIME
I
OBSERVE
THAT
FOR
INSTANCES
WITH
A
COMMON
DEADLINE
AS
IS
THE
CASE
HERE
AVR
AND
OA
BEHAVE
IDENTICALLY
THE
OPTIMAL
ENERGY
ALGORITHM
YDS
COMPLETES
THE
TASK
THAT
ARRIVES
AT
TIME
I
BY
TIME
I
RUNNING
AT
SPEED
N
I
Α
DURING
THE
TIME
INTERVAL
I
I
THE
RESULTING
ENERGY
USAGE
FOR
YDS
IS
THEN
N
N
I
Α
Α
N
N
I
WE
NOW
ANALYZE
THE
ENERGY
USAGE
OF
OA
AND
AVR
LET
I
BE
THE
SPEED
OF
AVR
DURING
THE
TIME
INTERVAL
I
I
THEN
FOR
I
N
I
Α
J
I
N
J
J
N
J
Α
R
J
I
Α
Α
N
I
Α
ΑN
Α
THE
FIRST
EQUALITY
ABOVE
IS
BY
THE
DEFINITION
OF
AVR
THEN
THE
ENERGY
USED
BY
AVR
IS
N
E
AVR
I
I
I
N
Α
N
I
Α
ΑN
Α
Α
I
N
Α
N
I
N
I
N
Α
Α
Α
Α
N
Α
I
N
I
N
I
N
Α
ΑN
Α
N
F
Α
Α
I
N
I
I
N
I
ΑΑ
HN
ΑG
Α
Α
HN
G
THE
FIRST
EQUALITY
ABOVE
IS
BY
THE
DEFINITION
OF
YDS
THE
FIRST
INEQUALITY
COMES
FROM
THE
LOWER
BOUND
ON
I
FROM
EQ
THE
SECOND
INEQUALITY
COMES
FROM
APPLYING
LEMMA
WITH
X
N
I
Α
AND
Y
N
Α
CHOOSING
N
LARGE
ENOUGH
THE
COMPETITIVE
RATIO
CAN
BE
MADE
ARBITRARILY
CLOSE
TO
ΑΑ
WE
NOW
TURN
TO
THE
MAIN
RESULT
OF
THIS
SECTION
THAT
THE
COMPETITIVE
RATIO
OF
OA
WITH
RESPECT
TO
ENERGY
IS
EXACTLY
ΑΑ
BEFORE
WE
BEGIN
WE
NEED
THE
FOLLOWING
ALGEBRAIC
FACT
LEMMA
LET
Q
R
Δ
AND
Α
THEN
Q
Δ
Α
Q
ΑR
Α
Δ
QΑ
Q
ΑR
PROOF
WE
NEED
TO
SHOW
THAT
Q
Δ
Α
Q
ΑR
Q
Δ
Α
Α
Δ
QΑ
Q
ΑR
OR
EQUIVALENTLY
THAT
Q
ΑR
Q
Δ
Α
QΑ
Q
Δ
Α
Α
Δ
SINCE
Q
Δ
Α
QΑ
IT
SUFFICES
TO
SHOW
THAT
Q
Q
Δ
Α
QΑ
Q
Δ
Α
Α
Δ
SUBSTITUTING
Δ
ZQ
THE
LEFT
HAND
SIDE
OF
THE
ABOVE
CAN
BE
WRITTEN
QΑ
Z
Α
QΑ
Z
Α
Α
Z
THUS
IT
WILL
BE
ENOUGH
TO
SHOW
THAT
FOR
Z
Z
Α
Z
Α
Α
Z
DIFFERENTIATING
THIS
WITH
RESPECT
TO
Z
WE
GET
Α
Z
Α
Α
Z
Z
Α
Α
Α
Z
Α
Α
Z
Z
Α
Α
Z
Z
Α
WHERE
THE
LAST
INEQUALITY
HOLDS
SINCE
Α
AND
Z
THUS
THE
MAXIMUM
OF
THIS
EXPRESSION
IS
ATTAINED
AT
Z
WHERE
IT
HAS
VALUE
THIS
IMPLIES
THE
RESULT
THEOREM
THE
ALGORITHM
OPTIMUM
AVAILABLE
IS
ΑΑ
COMPETITIVE
WITH
RE
SPECT
TO
ENERGY
PROOF
LET
SOA
T
DENOTE
THE
SPEED
AT
WHICH
OA
WORKS
AT
TIME
T
AND
SOPT
T
DENOTE
THE
SPEED
THAT
THE
OPTIMAL
ALGORITHM
YDS
WORKS
AT
TIME
T
AT
ANY
TIME
T
EITHER
A
TASK
ARRIVES
OR
FINISHES
OR
ELSE
AN
INFINITESIMAL
INTERVAL
OF
TIME
DT
ELAPSES
AND
OA
CONSUMES
SOA
T
ΑDT
UNITS
OF
ENERGY
WE
WILL
DEFINE
A
POTENTIAL
FUNCTION
Φ
T
THAT
SATISFIES
THE
FOLLOWING
PROPERTIES
THE
POTENTIAL
FUNCTION
Φ
T
DOES
NOT
INCREASE
AS
A
RESULT
OF
ANY
OF
THE
FOLLOWING
EVENTS
THE
ARRIVAL
OF
A
TASK
THE
COMPLETION
OF
A
TASK
BY
OA
THE
COMPLETION
OF
A
TASK
BY
OPT
AT
ANY
TIME
T
BETWEEN
ARRIVALS
T
Α
DΦ
T
ΑΑS
T
Α
OA
DT
OPT
THE
POTENTIAL
FUNCTION
Φ
T
HAS
VALUE
BEFORE
ANY
TASKS
ARRIVE
AND
ALSO
HAS
VALUE
AFTER
THE
LAST
DEADLINE
INTEGRATING
EQ
OVER
TIME
AND
USING
THE
OTHER
TWO
STATED
PROPERTIES
WE
CAN
CONCLUDE
THAT
E
OA
I
ΑΑ
E
OPT
I
FOR
MORE
INFORMATION
ON
THE
POTENTIAL
FUNCTION
METHOD
SEE
CORMEN
ET
AL
BEFORE
WE
CAN
DEFINE
THE
POTENTIAL
FUNCTION
WE
NEED
TO
INTRODUCE
SOME
NOTATION
LET
T
DENOTE
THE
SPEED
AT
WHICH
OA
WOULD
BE
WORKING
AT
TIME
T
IF
NO
NEW
TASKS
WERE
TO
ARRIVE
AFTER
THE
CURRENT
TIME
SINCE
OA
SIMPLY
COMPUTES
THE
YDS
SCHEDULE
BASED
ON
THE
CURRENT
KNOWLEDGE
OF
TASKS
THE
SPEEDS
T
ARE
COMPUTED
AS
FOLLOWS
LET
WOA
T
T
T
DENOTE
THE
UNFINISHED
WORK
UNDER
OA
THAT
IS
CURRENTLY
AVAILABLE
WITH
DEADLINES
IN
T
T
T
WE
WILL
REFER
TO
WOA
T
T
T
T
T
T
AS
THE
DENSITY
OF
INTERVAL
T
T
T
CONSIDER
A
SEQUENCE
OF
TIMES
DEFINED
INDUCTIVELY
AS
FOLLOWS
LET
ALWAYS
DENOTE
THE
CURRENT
TIME
LET
TI
I
DENOTE
THE
SMALLEST
TIME
SUCH
THAT
WOA
TI
TI
TI
TI
MAX
WOA
TI
T
T
T
T
TI
T
T
TI
THUS
IS
THE
SMALLEST
TIME
WHEN
THE
DENSITY
OF
TASKS
WITH
DEADLINES
BETWEEN
AND
IS
MAXIMIZED
AND
SO
ON
IT
IS
EASY
TO
SEE
THAT
FOR
ALL
I
T
TI
FOR
TI
T
TI
WE
WILL
CALL
THE
INTERVAL
TI
TI
A
CRITICAL
INTERVAL
AND
DENOTE
IT
BY
II
NOTE
THAT
THESE
INTERVALS
ARE
THOSE
SCHEDULED
IN
SUCCESSIVE
STEPS
OF
THE
YDS
ALGORITHM
ASSUMING
THAT
TIES
ARE
BROKEN
BY
CHOOSING
THE
SMALLEST
INTERVAL
WITH
MAXIMUM
INTENSITY
WE
NOW
NOTE
THAT
TI
IS
A
NONINCREASING
SEQUENCE
IF
TI
TI
THEN
THIS
CONTRADICTS
THAT
TI
IS
THE
CRITICAL
DENSITY
FOR
TIME
TI
SINCE
IN
THIS
CASE
WOA
TI
TI
TI
TI
WOA
TI
TI
TI
TI
TI
ANALOGOUSLY
LET
WOPT
T
T
T
DENOTE
THE
UNFINISHED
WORK
UNDER
THE
OPTIMAL
OFFLINE
ALGORITHM
AT
THE
CURRENT
TIME
THAT
HAS
DEADLINE
IN
T
T
T
WE
DEFINE
THE
POTENTIAL
FUNCTION
Φ
T
AS
FOLLOWS
Φ
T
Α
TI
Α
WOA
TI
TI
ΑWOPT
TI
TI
I
NOTE
THAT
BY
THE
DEFINITION
OF
OA
IS
ALWAYS
WORKING
AT
SPEED
IF
NO
NEW
TASK
ARRIVES
THE
ALGORITHM
CONTINUES
TO
WORK
AT
THE
SAME
SPEED
UNTIL
THE
CURRENT
CRITICAL
INTERVAL
FINISHES
WHEN
THE
CURRENT
CRITICAL
INTERVAL
FINISHES
THE
ALGORITHM
ENTERS
THE
NEXT
CRITICAL
INTERVAL
THE
INDICES
OF
THE
CRITICAL
DEADLINES
SHIFT
BY
ONE
AND
THE
NEW
SPEED
IS
THAT
WHICH
WAS
PREVIOUSLY
ALSO
NOTE
THAT
THE
POTENTIAL
IS
CONTINUOUS
AS
A
CRITICAL
INTERVAL
FINISHES
AND
WE
MOVE
TO
THE
NEXT
ONE
THIS
FOLLOWS
BECAUSE
AS
THE
CURRENT
CRITICAL
INTERVAL
FINISHES
APPROACHES
AND
BOTH
WOA
AND
WOPT
APPROACH
SINCE
BOTH
ALGORITHMS
HAVE
TO
FINISH
THIS
WORK
BY
TIME
THUS
THE
CONTRIBUTION
OF
THE
FIRST
TERM
APPROACHES
AS
THE
CURRENT
INTERVAL
IS
ABOUT
TO
FINISH
WE
NOW
CONSIDER
THE
VARIOUS
CASES
AS
TIME
PROGRESSES
AND
PROVE
THAT
THE
POTENTIAL
FUNCTION
SATISFIES
THE
PROPERTIES
CLAIMED
ABOVE
IT
IS
EASY
TO
SEE
THAT
AT
ANY
TIME
OA
IS
RUNNING
SOME
TASK
IF
AND
ONLY
IF
THE
OPTIMAL
ENERGY
ALGORITHM
YDS
IS
RUNNING
SOME
TASK
AND
THUS
WE
MAY
ASSUME
THAT
ALL
TIMES
IN
OUR
ARGUMENTS
THAT
EACH
OF
OA
AND
YDS
IS
RUNNING
SOME
TASK
WORKING
CASE
WE
FIRST
CONSIDER
THE
CASE
THAT
NO
TASK
ARRIVES
AND
NO
TASK
IS
COMPLETED
DURING
THE
NEXT
DT
UNITS
OF
TIME
THUS
EACH
TI
REMAINS
FIXED
INCLUDING
DURING
THE
DT
TIME
UNITS
WE
HAVE
TO
SHOW
THAT
OR
EQUIVALENTLY
T
Α
ΑΑS
OPT
T
Α
DΦ
T
DT
Α
ΑΑSOPT
T
Α
D
Α
T
Α
W
DT
I
TI
TI
ΑW
OPT
TI
TI
AS
OA
WORKS
WOA
IS
DECREASING
AT
RATE
AND
WOA
TI
TI
REMAINS
FIXED
FOR
ALL
I
LET
K
BE
THE
SMALLEST
INDEX
SUCH
THAT
WOPT
TK
TK
ASSUMING
THE
OPTIMAL
ENERGY
SCHEDULE
YDS
ALWAYS
EXECUTES
THE
TASK
WITH
THE
EARLIEST
DEADLINE
WE
HAVE
THAT
WOPT
TK
TK
DECREASES
AT
RATE
SOPT
AND
WOPT
TI
TI
IS
FIXED
FOR
I
K
THUS
EVALUATING
THE
LEFT
HAND
SIDE
OF
EQ
WE
SEE
THAT
IT
IS
EQUIVALENT
TO
Α
ΑΑSOPT
Α
ΑS
Α
TK
Α
SINCE
TK
EQ
WOULD
BE
IMPLIED
BY
Α
Α
Α
ΑΑSOPT
Α
LET
Z
SOPT
BY
SUBSTITUTION
EQ
IS
EQUIVALENT
TO
Α
ZΑ
ΑΑ
FOR
Z
LET
U
Z
BE
THE
POLYNOMIAL
ON
THE
LEFT
HAND
SIDE
OF
INEQUALITY
NOTE
THAT
U
ΑΑ
AND
U
DIFFERENTIATING
U
Z
WITH
RESPECT
TO
Z
WE
UT
Z
Α
Α
ZΑ
Α
ZΑ
SOLVING
FOR
UT
Z
WE
GET
THE
UNIQUE
VALUE
Z
Α
BECAUSE
Α
UT
Z
FOR
Z
Α
AND
UT
Z
FOR
Z
Α
U
Z
IS
MAXIMIZED
AT
Z
Α
AND
U
Α
HENCE
U
Z
IS
NONPOSITIVE
FOR
NONNEGATIVE
Z
THUS
WE
HAVE
ESTABLISHED
INEQUALITY
ARRIVAL
CASE
CONSIDER
THE
ARRIVAL
OF
A
TASK
OF
WORK
X
WITH
DEADLINE
T
LET
I
BE
SUCH
THAT
TI
T
TI
WE
MUST
SHOW
THAT
THE
CHANGE
IN
POTENTIAL
CAUSED
BY
THIS
ARRIVAL
IS
NONPOSITIVE
FIRST
WE
CONSIDER
THE
SIMPLEST
CASE
WHEN
THE
CRITICAL
INTERVALS
ARE
UNCHANGED
THAT
IS
ONLY
THE
VALUES
OF
THE
CRITICAL
DENSITIES
CHANGE
HENCE
THE
ONLY
EFFECT
THE
NEW
TASK
HAS
IS
TO
INCREASE
THE
DENSITY
TI
OF
THE
INTERVAL
II
TI
TI
TO
WOA
TI
TI
X
TI
TI
AND
THE
QUANTITY
WOA
TI
TI
ΑWOPT
TI
TI
DECREASES
BY
Α
X
THUS
THE
CHANGE
IN
THE
POTENTIAL
FUNCTION
IS
THEN
F
WOA
TI
TI
X
Α
F
WOA
TI
TI
Α
SUBSTITUTING
Q
WOA
TI
TI
Δ
X
AND
R
WOPT
TI
TI
AND
REARRANGING
WE
CAN
WRITE
Α
Q
Δ
Α
Q
ΑR
Α
Δ
QΑ
Q
ΑR
Φ
WHICH
IS
NONPOSITIVE
BY
LEMMA
TI
TI
Α
WE
NOW
CONSIDER
THE
MORE
INTERESTING
CASE
WHEN
THE
ARRIVAL
OF
A
TASK
MIGHT
CHANGE
THE
CRITICAL
INTERVALS
WHILE
THIS
NEW
TASK
MAY
RADICALLY
CHANGE
THE
STRUCTURE
OF
THE
CRITICAL
INTERVALS
WE
SHOW
THAT
WE
CAN
THINK
OF
THIS
CHANGE
AS
A
SEQUENCE
OF
SMALLER
CHANGES
WHERE
EACH
SMALLER
CHANGE
AFFECTS
ONLY
TWO
CRITICAL
INTERVALS
MOREOVER
EACH
CHANGE
IS
ESSENTIALLY
EQUIVALENT
TO
THAT
IN
THE
PREVIOUS
CASE
WHERE
THE
STRUCTURE
OF
THE
CRITICAL
INTERVALS
REMAINS
UNCHANGED
TO
EXPLAIN
HOW
TO
ACCOMPLISH
THIS
IMAGINE
THE
WORK
OF
THE
NEW
TASK
INCREASING
STARTING
FROM
FOR
SOME
AMOUNT
OF
WORK
X
T
X
ONE
OF
THE
FOLLOWING
THREE
EVENTS
MUST
OCCUR
THE
INTERVAL
II
REMAINS
A
CRITICAL
INTERVAL
AND
ITS
DENSITY
BECOMES
EQUAL
TO
THAT
OF
II
IN
PARTICULAR
X
T
IS
SUCH
THAT
TI
WOA
TI
TI
X
T
TI
TI
THE
INTERVAL
II
SPLITS
INTO
TWO
CRITICAL
INTERVALS
IIT
TI
T
T
AND
IITT
T
T
TI
FOR
SOME
TI
T
T
TI
SINCE
X
T
IS
THE
SMALLEST
SUCH
WORK
THE
DENSITIES
OF
IIT
AND
IITT
ARE
IDENTICAL
AND
EQUAL
TO
WOA
TI
TI
X
T
TI
TI
A
SEQUENCE
II
OF
CRITICAL
INTERVALS
MERGE
INTO
ONE
NEW
CRITICAL
INTERVAL
WE
CAN
THINK
OF
THIS
EVENT
AS
A
SEQUENCE
OF
PAIRWISE
MERGES
EACH
OF
WHICH
COMBINES
WITH
TO
FORM
A
NEW
INTERVAL
IN
THIS
CASE
IT
MUST
BE
THAT
X
T
FOR
EACH
OF
THESE
EVENTS
WE
CAN
IMAGINE
THE
ORIGINAL
TASK
OF
WORK
X
AS
CONSISTING
OF
TWO
TASKS
SUCH
THAT
BOTH
ARRIVE
AT
THE
SAME
TIME
AND
HAVE
THE
SAME
DEADLINE
T
BUT
ONE
HAS
WORK
X
T
AND
THE
OTHER
HAS
WORK
X
X
T
WE
CAN
THEN
FIRST
ANALYZE
THE
CHANGE
IN
POTENTIAL
AS
THE
RESULT
OF
THE
ARRIVAL
OF
THE
WORK
X
AND
THEN
REPEAT
THIS
PROCEDURE
RECURSIVELY
FOR
X
POTENTIAL
FOR
A
TASK
OF
WORK
X
X
T
THUS
WE
ONLY
NEED
TO
CONSIDER
THE
CHANGE
IN
THAT
CAUSES
ONE
OF
THE
THREE
EVENTS
DESCRIBED
ABOVE
FOR
THE
FIRST
TYPE
OF
EVENT
THE
ONLY
CHANGE
IN
THE
POTENTIAL
FUNCTION
IS
DUE
TO
THE
CHANGE
OF
DENSITY
OF
II
THIS
CHANGE
IS
IDENTICAL
TO
EQ
WITH
X
REPLACED
BY
X
T
AND
AGAIN
THE
NON
POSITIVITY
OF
Φ
FOLLOWS
BY
LEMMA
FOR
THE
SECOND
TYPE
OF
EVENT
THE
CHANGE
IN
THE
POTENTIAL
FUNCTION
IS
ONLY
DUE
TO
II
BEING
REPLACED
BY
IIT
AND
IITT
SINCE
THE
DENSITIES
OF
IIT
AND
IITT
ARE
IDENTICAL
THESE
INTERVALS
CAN
STILL
BE
CONSIDERED
TOGETHER
AS
FAR
AS
THE
POTENTIAL
FUNCTION
IS
CONCERNED
HENCE
THE
CHANGE
IN
THE
POTENTIAL
FUNCTION
IS
AGAIN
GIVEN
BY
EQ
WITH
X
REPLACED
BY
X
T
AND
AGAIN
THE
NON
POSITIVITY
OF
Φ
FOLLOWS
BY
LEMMA
FOR
THE
THIRD
TYPE
OF
EVENT
THE
CHANGE
IN
THE
POTENTIAL
FUNCTION
IS
ONLY
DUE
TO
AND
BEING
REPLACED
BY
SINCE
THE
DENSITIES
OF
AND
ARE
ALL
IDENTICAL
THESE
INTERVALS
CAN
STILL
BE
CONSIDERED
TOGETHER
AS
FAR
AS
THE
POTENTIAL
FUNCTION
IS
CONCERNED
HENCE
THE
CHANGE
IN
THE
POTENTIAL
FUNCTION
IS
AGAIN
GIVEN
BY
EQ
WITH
X
REPLACED
BY
X
T
AND
AGAIN
THE
NONPOSITIVITY
OF
Φ
FOLLOWS
BY
LEMMA
WE
CAN
REPEAT
THIS
PROCESS
UNTIL
WE
HAVE
USED
UP
ALL
OF
THE
X
WORK
IN
THE
ARRIVING
TASK
TEMPERATURE
WE
START
THIS
SUBSECTION
BY
SHOWING
THAT
AVR
AND
OA
ARE
NOT
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
IN
THE
REASONABLE
CASE
THAT
THE
THERMAL
THRESHOLD
TMAX
OF
THE
DEVICE
IS
KNOWN
THE
MOST
OBVIOUS
TEMPERATURE
MANAGEMENT
STRATEGY
IS
TO
RUN
AT
A
SPEED
THAT
LEAVES
THE
TEMPERATURE
FIXED
AT
TMAX
IN
PARTICULAR
WHENEVER
THERE
IS
WORK
TO
DO
THE
ALGORITHM
WORKS
AT
A
SPEED
THAT
MAINTAINS
THE
TEMPERATURE
TMAX
AND
OTHERWISE
IT
COOLS
ACCORDING
TO
NEWTON
LAW
OF
COOLING
WE
CALL
SUCH
A
STRATEGY
O
COMPETITIVE
IF
ON
ANY
INSTANCE
I
ON
WHICH
THIS
CONSTANT
TEMPERATURE
ALGORITHM
MISSES
A
DEADLINE
EVERY
FEASIBLE
SCHEDULE
REACHES
A
TEMPERATURE
OF
Q
TMAX
AT
SOME
POINT
IN
TIME
WE
THEN
SHOW
THAT
STAYING
AT
THE
THERMAL
THRESHOLD
IS
NOT
AN
O
COMPETITIVE
STRATEGY
LEMMA
THE
ONLINE
ALGORITHMS
AVR
AND
OA
ARE
NOT
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
MORE
PRECISELY
THE
COMPETITIVE
RATIOS
OF
THESE
ALGORITHMS
MUST
DEPEND
ON
EITHER
THE
NUMBER
OF
TASKS
OR
THE
COOLING
RATE
B
PROOF
WE
USE
A
VARIATION
OF
AN
INSTANCE
FROM
YAO
ET
AL
CHOOSE
AN
ARBITRARILY
LARGE
INTEGER
N
AND
CONSIDER
AN
INSTANCE
WITH
N
TASKS
WHERE
TASK
I
IS
RELEASED
AT
TIME
RI
IC
HAS
WORK
WI
C
AND
DEADLINE
DI
NC
FOR
I
N
AGAIN
NOTE
THAT
SINCE
ALL
TASKS
HAVE
A
COMMON
DEADLINE
AVR
AND
OA
BEHAVE
IDENTICALLY
THE
YDS
SCHEDULE
RUNS
TASKS
AT
A
CONSTANT
SPEED
OF
AND
THUS
USES
TOTAL
ENERGY
N
AND
ENERGY
C
IN
ANY
C
INTERVAL
USING
THEOREM
IT
IS
SUFFICIENT
TO
SHOW
THAT
THERE
IS
SOME
C
INTERVAL
WHERE
THE
ENERGY
USED
BY
OA
AND
AVR
IS
Ω
C
AS
AVR
RUNS
TASK
I
AT
SPEED
N
I
DURING
THE
INTERVAL
IC
NC
DURING
THE
C
INTERVAL
C
N
CN
AVR
AND
OA
RUN
AT
A
SPEED
OF
HN
G
LOG
N
WHERE
HN
IS
THE
NTH
HARMONIC
NUMBER
AND
THUS
THE
ENERGY
USED
DURING
THIS
C
INTERVAL
IS
Q
C
LOGΑ
N
THEOREM
THE
SPEED
SCALING
ALGORITHM
THAT
RUNS
AT
SUCH
A
SPEED
THAT
THE
TEMPERATURE
REMAINS
CONSTANT
AT
THE
THERMAL
THRESHOLD
TMAX
IS
NOT
O
COMPETITIVE
PROOF
SUPPOSE
AT
TIME
A
TASK
WITH
WORK
X
WHICH
WILL
BE
SPECIFIED
LATER
AND
DEADLINE
E
ARRIVES
WE
WILL
CONSIDER
THE
BEHAVIOR
AS
E
GOES
TO
SUPPOSE
THE
TEMPERATURE
AT
TIME
IS
WE
CHOOSE
X
SUCH
THAT
IT
IS
EQUAL
TO
THE
MAXIMUM
WORK
THAT
THE
ADVERSARY
CAN
GET
DONE
BY
TIME
E
WHILE
KEEPING
THE
TEMPERATURE
BELOW
TMAX
K
FOR
SOME
CONSTANT
K
USING
EQ
FROM
SECTION
FOR
THIS
MAXIMUM
WORK
AND
SUBSTITUTING
Α
WE
GET
X
G
BTMAX
THE
CRUCIAL
FACT
IS
THAT
THE
MAXIMUM
WORK
THAT
THE
ADVERSARY
CAN
DO
DEPENDS
ON
E
AS
ON
THE
OTHER
HAND
THE
CONSTANT
TEMPERATURE
ALGORITHM
AT
TEMPERATURE
TMAX
HAS
POWER
P
BTMAX
A
AND
HENCE
SPEED
BTMAX
A
AND
WORK
G
BTMAX
A
WHICH
DEPENDS
LINEARLY
ON
E
THUS
FOR
ANY
CONSTANT
K
THE
RATIO
OF
THE
WORK
COMPLETED
BY
THE
ADVERSARY
TO
THE
WORK
COMPLETED
BY
THE
CONSTANT
TEMPERATURE
ALGORITHM
GOES
TO
INFINITY
AS
E
GOES
TO
THE
BKP
ALGORITHM
RECALL
THE
DEFINITION
OF
OUR
NEWLY
INTRODUCED
ALGORITHM
BKP
IN
SECTION
WE
WILL
SHOW
THAT
IT
IS
COOLING
OBLIVIOUS
THAT
IS
BKP
IS
O
COMPETITIVE
WITH
RESPECT
TO
ENERGY
MAXIMUM
POWER
SPEED
AND
TEMPERATURE
WE
ANALYZE
BKP
SEPARATELY
FOR
EACH
OF
THESE
OBJECTIVES
PRELIMINARIES
RECALL
THAT
AT
ANY
TIME
T
BKP
WORKS
AT
SPEED
E
V
T
ON
THE
UNFINISHED
JOB
WITH
THE
EARLIEST
DEADLINE
WHERE
V
T
IS
DEFINED
BY
EQ
IN
THIS
SECTION
WE
FIRST
PROVE
THAT
BKP
ALWAYS
PRODUCES
A
FEASIBLE
SCHEDULE
WE
THEN
GIVE
FOUR
INEQUALITIES
THAT
WILL
ALLOW
US
TO
RELATE
BKP
TO
THE
OPTIMAL
SCHEDULE
THEOREM
THE
BKP
ALGORITHM
ALWAYS
OUTPUTS
A
FEASIBLE
SCHEDULE
PROOF
ASSUME
FOR
THE
SAKE
OF
CONTRADICTION
THAT
BKP
MISSES
SOME
DEADLINE
FOR
SOME
PROBLEM
INSTANCE
OF
THESE
INFEASIBLE
INSTANCES
CONSIDER
ONE
WITH
THE
FEWEST
NUMBER
OF
TASKS
AND
LET
D
DENOTE
THE
FIRST
DEADLINE
THAT
IS
MISSED
IN
THIS
INSTANCE
WE
CLAIM
THAT
ON
THIS
INSTANCE
BKP
ALWAYS
WORKS
ON
TASKS
WITH
DEADLINE
NO
MORE
THAN
D
DURING
THE
INTERVAL
D
TO
SEE
THIS
FIRST
OBSERVE
THAT
IF
BKP
IS
IDLE
AT
SOME
TIME
T
DURING
D
THEN
WE
CAN
REPLACE
THE
INSTANCE
BY
A
SMALLER
ONE
BY
REMOVING
ALL
JOBS
THAT
FINISH
BEFORE
T
SIMILARLY
AS
BKP
ALWAYS
WORKS
ON
THE
JOB
WITH
THE
EARLIEST
DEADLINE
IF
IT
WORKED
ON
A
TASK
WITH
DEADLINE
GREATER
THAN
D
AT
SOME
TIME
T
D
THEN
BKP
MUST
HAVE
FINISHED
ALL
WORK
WITH
DEADLINE
NO
MORE
THAN
THAN
D
THAT
ARRIVED
BY
TIME
T
THUS
ONE
COULD
OBTAIN
ANOTHER
INFEASIBLE
INSTANCE
WITH
FEWER
TASKS
BY
CONSIDERING
ONLY
THOSE
TASKS
RELEASED
AFTER
THE
TIME
T
THIS
IMPLIES
THAT
IF
BKP
MISSES
THE
DEADLINE
AT
TIME
D
IT
MUST
BE
THAT
THE
TOTAL
WORK
DONE
BY
BKP
DURING
D
IS
STRICTLY
LESS
THAN
W
D
THE
TOTAL
WORK
IN
THE
INSTANCE
WITH
DEADLINE
NO
MORE
THAN
D
OUR
PROOF
WILL
BE
TO
SHOW
THAT
THIS
CANNOT
HAPPEN
BY
CHOOSING
T
T
D
IN
THE
DEFINITION
OF
V
T
IT
FOLLOWS
TRIVIALLY
THAT
V
T
W
T
ET
E
D
D
E
D
T
THUS
THE
WORK
DONE
BY
BKP
DURING
THE
TIME
PERIOD
D
IS
R
D
E
V
T
DT
R
D
W
T
ET
E
D
D
DT
WE
NOW
EXPAND
THE
RIGHT
HAND
SIDE
OF
THIS
INEQUALITY
LET
B
X
DENOTE
THE
RATE
AT
WHICH
WORK
ARRIVES
AT
TIME
X
THUS
IF
NO
WORK
ARRIVES
AT
TIME
X
THEN
B
X
IF
W
UNITS
OF
WORK
ARRIVES
AT
TIME
X
THEN
B
X
IS
W
TIMES
THE
DIRAC
DELTA
FUNCTION
THUS
FOR
EXAMPLE
R
B
B
X
DX
IS
JUST
THE
WORK
THAT
ARRIVES
DURING
THE
INTERVAL
A
B
R
D
W
T
ET
E
D
D
DT
R
D
FR
T
B
X
DX
DT
R
D
FR
X
E
D
E
B
X
R
D
FR
X
E
D
E
D
B
X
LN
R
D
D
X
DX
D
X
E
D
E
W
D
THE
SECOND
EQUALITY
FOLLOWS
BY
INTERCHANGING
THE
INTEGRALS
AND
OBSERVING
THAT
FOR
EACH
X
D
THE
WORK
B
X
CONTRIBUTES
TO
W
T
ET
E
D
D
D
T
IF
AND
ONLY
IF
X
ET
E
D
T
OR
EQUIVALENTLY
THAT
T
X
X
E
D
E
THUS
BKP
DOES
AT
LEAST
W
D
WORK
DURING
THE
INTERVAL
D
WHICH
IS
THE
CONTRADICTION
WE
NEED
WE
NOW
STATE
TWO
INEQUALITIES
FROM
HARDY
ET
AL
THAT
ARE
CRITICAL
IN
OUR
FURTHER
ANALYSIS
OF
BKP
FACT
HARDY
INEQUALITY
THEOREM
HARDY
ET
AL
IF
IT
IS
THE
CASE
THAT
Α
F
X
AND
F
X
R
X
F
T
DT
THEN
R
F
F
X
Α
DX
F
Α
Α
R
F
Α
X
DX
X
Α
THE
FOLLOWING
FACT
WAS
FIRST
PROVED
BY
HARDY
AND
LITTLEWOOD
AND
LATER
SIMPLIFIED
BY
GABRIEL
IT
CAN
ALSO
BE
FOUND
IN
HARDY
ET
AL
THEOREM
AND
FACT
SUPPOSE
THAT
F
X
IS
NONNEGATIVE
AND
INTEGRABLE
IN
A
FINITE
INTERVAL
A
AND
THAT
F
X
IS
THE
REARRANGEMENT
OF
F
X
IN
DECREASING
ORDER
LET
M
X
M
X
F
R
X
F
T
DT
MAX
Y
X
X
Y
Y
SUPPOSE
Y
IS
ANY
INCREASING
FUNCTION
OF
Y
DEFINED
FOR
Y
THEN
R
A
M
X
DX
R
A
F
R
X
F
T
DT
DX
ROUGHLY
IF
WE
THINK
OF
F
X
AS
THE
WORK
ARRIVING
AT
TIME
X
THEN
THE
DEFINITION
OF
M
X
RESEMBLES
THE
WAY
WE
DEFINE
R
X
IN
OUR
ALGORITHM
THIS
ALLOWS
US
TO
ARGUE
ABOUT
THE
FUNCTION
M
IN
TERMS
OF
F
WE
USE
THESE
TWO
FACTS
TO
PROVE
THE
FOLLOWING
TWO
LEMMAS
LEMMA
SHOWS
HOW
TO
RELATE
THE
WORK
IN
AN
ARBITRARY
SCHEDULE
WHICH
WILL
BE
THE
OPTIMAL
SCHEDULE
IN
OUR
ARGUMENTS
TO
THE
SPEED
Q
T
THAT
UPPER
BOUNDS
THE
SPEED
V
T
USED
IN
THE
DEFINITION
OF
BKP
LEMMA
THEN
SHOWS
THAT
THE
ENERGY
USED
BY
RUNNING
AT
SPEED
Q
T
IS
LESS
THAN
SOME
CONSTANT
TIMES
THE
ENERGY
USED
BY
AN
ARBITRARY
SCHEDULE
LEMMA
LET
Z
T
BE
THE
SPEED
AT
TIME
T
FOR
SOME
FEASIBLE
SCHEDULE
Z
THEN
Q
T
R
Z
T
DT
MAX
T
PROOF
SINCE
Z
IS
FEASIBLE
FOR
ANY
TIMES
AND
WE
HAVE
THAT
R
Z
T
DT
W
THUS
Q
T
I
T
T
R
Z
T
DT
MAX
T
MAX
T
LEMMA
LET
Q
T
AND
Y
T
BE
FUNCTIONS
SUCH
THAT
Q
T
R
Y
T
DT
MAX
T
THEN
IT
MUST
BE
THE
CASE
THAT
R
Q
T
ΑDT
Α
Α
Α
Y
T
ΑDT
T
PROOF
WE
SPLIT
Q
T
INTO
TWO
PARTS
LET
L
T
R
T
Y
X
DX
T
T
SIMILARLY
LET
MAX
T
SUCH
THAT
V
T
R
Y
X
DX
T
T
MAX
T
T
SUCH
THAT
Q
T
ΑDT
IT
SUFFICES
TO
SHOW
THAT
BOTH
BY
THE
QUANTITY
Α
Α
RRT
Y
T
ΑDT
T
L
T
ΑDT
AND
T
V
T
ΑDT
ARE
UPPER
BOUNDED
DENOTE
THE
REARRANGEMENT
OF
Y
T
IN
NONINCREASING
ORDER
THEN
BY
FACT
IT
FOLLOWS
THAT
R
L
T
ΑDT
R
F
R
T
Α
Y
X
DX
DT
T
T
T
X
NOW
USING
FACT
WITH
F
X
Y
X
R
F
R
T
Α
Y
X
DX
DT
F
Α
Α
R
Y
X
ΑDX
T
T
X
Α
X
THE
DESIRED
BOUND
ON
RT
L
T
ΑDT
FOLLOWS
BY
EQS
AND
AND
OBSERVING
THAT
Α
Α
Α
R
Y
X
ΑDX
Α
Α
Α
Y
X
ΑDX
X
AS
Y
IS
A
REARRANGEMENT
OF
Y
THE
ANALYSIS
OF
T
V
T
ΑDT
IS
SIMILAR
ENERGY
IN
THIS
SECTION
WE
SHOW
THAT
THE
BKP
ALGORITHM
IS
O
Α
COMPETITIVE
WITH
RESPECT
TO
ENERGY
THEOREM
THE
BKP
ALGORITHM
IS
Α
Α
EXP
Α
COMPETITIVE
WITH
RESPECT
TO
ENERGY
Α
PROOF
WE
FIRST
NOTE
THAT
E
BKP
R
E
V
T
ΑDT
R
E
Q
T
ΑDT
EXP
Α
R
Q
T
ΑDT
THE
FIRST
INEQUALITY
FOLLOWS
BY
THE
DEFINITION
OF
BKP
THE
SECOND
INEQUALITY
FOLLOWS
BY
LEMMA
SETTING
Y
T
TO
BE
THE
SPEED
AT
WHICH
YDS
WORKS
AT
TIME
T
BY
LEMMA
IT
IS
THE
CASE
THAT
Q
T
R
Y
T
DT
FINALLY
MAX
T
R
Q
T
ΑDT
Α
Α
Α
Y
T
ΑDT
T
Α
Α
Α
E
OPT
THE
FIRST
INEQUALITY
IS
BY
LEMMA
THE
LAST
EQUALITY
FOLLOWS
SINCE
YDS
IS
THE
OPTIMAL
ENERGY
SCHEDULE
MAXIMUM
SPEED
AND
MAXIMUM
POWER
IT
IS
NOT
HARD
TO
SEE
THAT
OUR
ONLINE
ALGORITHM
BKP
IS
E
COMPETITIVE
WITH
RESPECT
TO
THE
MAXIMUM
SPEED
OR
EQUIVALENTLY
EXP
Α
COMPETITIVE
FOR
MAXIMUM
POWER
WE
SHOW
THIS
FORMALLY
IN
LEMMA
WE
THEN
SHOW
IN
LEMMA
THAT
THIS
IS
THE
BEST
POSSIBLE
COMPETITIVE
RATIO
FOR
THE
MAXIMUM
SPEED
CONSIDER
AN
ONLINE
DETERMINISTIC
ALGORITHM
A
WITH
THE
PROPERTY
THAT
THE
SCHEDULE
PRODUCED
BY
A
IS
IDENTICAL
FOR
ALL
VALUES
OF
Α
WE
CALL
SUCH
AN
ALGORITHM
Α
INDEPENDENT
NOTE
ALL
ONLINE
ALGORITHMS
CONSIDERED
IN
THIS
ARTICLE
BKP
OA
AND
AVR
ARE
Α
INDEPENDENT
GIVEN
ANY
FIXED
SCHEDULE
PRODUCED
BY
AN
Α
INDEPENDENT
ALGORITHM
A
WE
CAN
CHOOSE
Α
LARGE
ENOUGH
SUCH
THAT
THE
TOTAL
ENERGY
FOR
THIS
SCHEDULE
IS
ESSENTIALLY
DETERMINED
BY
THE
MAXIMUM
POWER
USED
BY
A
THUS
THE
LOWER
BOUND
OF
E
FOR
MAXIMUM
SPEED
IMPLIES
THAT
FOR
ANY
ARBITRARILY
SMALL
CONSTANT
E
THERE
IS
SOME
Α
LARGE
ENOUGH
SUCH
THAT
A
CANNOT
BE
E
E
Α
COMPETITIVE
WITH
RESPECT
TO
TOTAL
ENERGY
WHEN
POWER
VARIES
AS
SPEED
RAISED
TO
Α
THIS
IMPLIES
THAT
RESTRICTED
TO
THE
CLASS
OF
Α
INDEPENDENT
ALGORITHMS
THE
BASE
OF
THE
EXPONENT
IN
THE
COMPETITIVE
RATIO
OF
BKP
CANNOT
BE
IMPROVED
LEMMA
THE
ONLINE
ALGORITHM
BKP
IS
E
COMPETITIVE
WITH
RESPECT
TO
MAXI
MUM
SPEED
PROOF
YDS
IS
THE
OPTIMUM
OFFLINE
ALGORITHM
WITH
RESPECT
TO
MAXIMUM
SPEED
THE
MAXIMUM
SPEED
AT
WHICH
YDS
EVER
WORKS
IS
EXACTLY
EQUAL
TO
MAXT
Q
T
AT
ANY
TIME
THE
BKP
ALGORITHM
WORKS
AT
SPEED
AT
MOST
E
V
T
AT
TIME
T
WHICH
IS
AT
MOST
E
Q
T
BY
LEMMA
LEMMA
FOR
EVERY
DETERMINISTIC
ONLINE
ALGORITHM
A
THAT
MAINTAINS
DEAD
LINE
FEASIBILITY
THERE
IS
SOME
INPUT
THAT
CAUSES
A
AT
SOME
TIME
TO
RUN
E
TIMES
FASTER
THAN
THE
MAXIMUM
SPEED
OF
YDS
PROOF
ASSUME
A
HAS
A
COMPETITIVE
RATIO
R
WE
SHOW
THAT
IT
MUST
BE
THE
CASE
THAT
R
E
LET
E
BE
AN
ARBITRARILY
SMALL
CONSTANT
LET
A
X
LN
X
THE
ADVERSARY
ADOPTS
THE
FOLLOWING
STRATEGY
IT
RELEASES
WORK
AT
THE
RATE
OF
A
X
UNTIL
SOME
TIME
T
E
AND
THEN
AFTER
TIME
T
NO
MORE
WORK
IS
RELEASED
THE
VALUE
OF
T
DEPENDS
ON
THE
BEHAVIOR
OF
THE
ONLINE
ALGORITHM
ALL
WORK
HAS
DEADLINE
THE
ADVERSARY
WILL
MAKE
T
E
UNLESS
A
WORKS
AT
TOO
GREAT
A
SPEED
AT
SOME
TIME
BEFORE
E
SO
ASSUME
FOR
THE
MOMENT
THAT
T
E
THE
WORK
THAT
IS
RELEASED
BETWEEN
TIME
F
AND
TIME
G
IS
R
G
DX
G
LET
I
T
W
T
DENOTE
THE
INTENSITY
OF
INTERVAL
RESTRICTED
TO
THE
WORK
THAT
HAS
ARRIVED
BY
TIME
T
NOTE
THAT
FOR
T
E
W
T
LN
E
LN
T
IN
PARTICULAR
IF
T
E
THEN
W
T
FOR
SOME
TIME
T
E
AND
SOME
K
T
BY
EQ
I
T
K
K
LN
E
LN
T
K
GIVEN
A
FIXED
T
WE
WILL
BE
INTERESTED
IN
THE
VALUE
OF
K
T
THAT
MAXIMIZES
T
K
LET
K
T
DENOTE
THIS
VALUE
OF
K
TO
DETERMINE
THE
VALUE
OF
K
T
WE
DIFFERENTIATE
I
T
K
WITH
RESPECT
TO
K
WE
GET
D
I
T
K
LN
T
DK
K
LN
E
K
K
LN
E
SETTING
I
T
T
K
AND
SOLVING
FOR
K
WE
HAVE
K
ET
E
ALSO
NOTE
THAT
I
T
T
K
IS
POSITIVE
WHEN
K
ET
E
AND
NEGATIVE
WHEN
K
ET
E
E
ALSO
NOTE
THAT
ET
E
IS
ALWAYS
LESS
THAN
T
AND
IS
NONNEGATIVE
WHEN
T
E
WE
CAN
THEN
CONCLUDE
THAT
IF
T
E
E
THEN
K
T
ET
E
AND
IF
T
E
E
THEN
K
T
RECALL
THAT
THE
YDS
SCHEDULE
IS
OPTIMAL
WITH
RESPECT
TO
MAXIMUM
SPEED
SUPPOSE
THE
ADVERSARY
STOPS
BRINGING
IN
MORE
WORK
AT
TIME
T
THEN
THE
FIRST
INTERVAL
CHOSEN
BY
YDS
ON
THIS
INSTANCE
WILL
BE
K
T
AND
HENCE
T
K
T
IS
THE
MAXIMUM
SPEED
THAT
YDS
WILL
RUN
IF
T
E
E
THEN
K
T
AND
YDS
WILL
RUN
AT
A
CONSTANT
SPEED
OF
I
T
K
T
I
T
LN
T
LN
E
DURING
THE
TIME
INTERVAL
THUS
FOR
EACH
TIME
X
E
E
A
CANNOT
WORK
AT
A
GREATER
SPEED
THAN
R
LN
X
IF
IT
DID
SO
THEN
A
WOULD
NOT
BE
R
COMPETITIVE
IN
THE
CASE
THAT
ADVERSARY
STOPS
BRINGING
IN
WORK
AT
TIME
X
IF
T
E
E
E
THEN
K
T
ET
E
AND
YDS
WILL
RUN
AT
MAXIMUM
SPEED
OF
I
T
K
T
I
T
ET
E
E
T
LN
E
THUS
FOR
EACH
TIME
X
E
E
E
A
CANNOT
WORK
AT
A
GREATER
SPEED
THAN
R
X
LN
E
IF
IT
DID
SO
THEN
A
WOULD
NOT
BE
R
COMPETITIVE
IN
THE
CASE
THAT
T
X
IF
T
E
THEN
YDS
WILL
RUN
AT
A
MAXIMUM
SPEED
OF
I
E
K
E
EE
LN
E
THUS
DURING
THE
TIME
PERIOD
E
A
CANNOT
WORK
FASTER
THAN
R
E
IF
IT
DID
SO
THEN
A
WOULD
NOT
BE
R
COMPETITIVE
IN
THE
CASE
THAT
T
E
NOW
CONSIDER
THE
CASE
THAT
T
E
BY
THE
ARGUMENTS
ABOVE
THE
MOST
WORK
THAT
A
CAN
GET
DONE
IS
E
E
LN
T
R
LN
E
DT
R
E
E
E
DT
R
E
T
LN
E
E
DT
EE
LN
E
R
F
R
LN
E
R
R
F
E
F
R
NOW
AS
E
APPROACHES
THE
TERM
E
E
R
APPROACHES
THUS
THE
MAXIMUM
WORK
THAT
A
CAN
GET
DONE
APPROACHES
R
WHICH
MUST
BE
AT
LEAST
W
THUS
WE
CONCLUDE
THAT
R
CANNOT
BE
LESS
THAN
E
TEMPERATURE
WE
SHOW
IN
THEOREM
THAT
BKP
IS
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
THEOREM
THE
ONLINE
ALGORITHM
BKP
IS
EXP
Α
Α
Α
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
FOR
ALL
COOLING
PARAMETERS
B
SATISFYING
B
PROOF
LET
X
BE
AN
ARBITRARY
C
INTERVAL
AS
X
IS
ARBITRARY
BY
THEOREM
IT
IS
SUFFICIENT
TO
SHOW
THAT
BKP
USES
AT
MOST
A
FACTOR
OF
EXP
Α
Α
TIMES
AS
MUCH
ENERGY
AS
C
OPT
DURING
THE
INTERVAL
X
HERE
WE
USE
ZΑT
SPEED
AT
TIME
T
OF
A
FIXED
ARBITRARY
SCHEDULE
OPT
THAT
USES
ENERGY
AT
MOST
C
OPT
IN
EVERY
C
INTERVAL
LET
XK
RESPECTIVELY
XK
DENOTE
THE
KTH
C
INTERVAL
IMMEDIATELY
TO
THE
LEFT
RESPECTIVELY
R
IGHT
OF
X
THAT
I
THE
LEFT
ENDPOINT
OF
XK
THE
LEFTMOST
POINT
OF
X
LET
THE
INTERVAL
Z
BE
DEFINED
TO
BE
X
X
IS
KC
UNITS
TO
X
AS
IN
THE
PROOF
OF
THEOREM
WE
WILL
ASSUME
THAT
BKP
RUNS
A
T
SPE
ED
E
Q
T
EVEN
IF
THERE
IS
NO
WORK
TO
DO
THUS
WE
ARE
LEFT
TO
SHOW
THAT
RT
X
Q
T
ΑDT
Α
Α
Α
C
OPT
SINCE
OPT
IS
FEASIBLE
WE
HAVE
BY
LEMMA
Q
T
R
Z
T
DT
MAX
T
WE
DECOMPOSE
Z
T
AS
FOLLOWS
LET
T
Z
T
IF
T
Z
AND
AT
ALL
OTHER
TIMES
LET
T
Z
T
T
FOR
ALL
T
LET
T
T
MAX
R
X
DX
T
X
AND
Q
T
R
Z
X
DX
MAX
T
X
NOTE
THAT
Q
T
T
T
SINCE
Z
T
T
T
FOR
ALL
EACH
T
BY
CONVEXITY
OF
THE
SPEED
TO
POWER
FUNCTION
P
IT
FOLLOWS
THAT
Q
T
Α
T
T
Α
T
Α
T
Α
AND
THUS
RT
X
Q
T
ΑDT
T
X
T
ΑDT
T
X
T
ΑDT
WE
FIRST
UPPER
BOUND
Q
T
ΑDT
IN
FACT
WE
WILL
UPPER
BOUND
Q
T
ΑDT
NOTE
THAT
T
IS
IDENTICALLY
AT
ALL
POINTS
NOT
IN
Z
MOREOVER
AS
Z
IS
AN
INTERVAL
OF
LENGTH
BY
THE
DEFINITION
OF
C
OPT
IT
FOLLOWS
THAT
NOW
WE
HAVE
RT
Z
R
T
ΑDT
OPT
F
Α
Α
R
T
T
ΑDT
Α
T
ΑDT
F
Α
Α
R
F
Α
Α
T
Z
T
ΑDT
Α
C
OPT
THE
FIRST
INEQUALITY
FOLLOWS
FROM
LEMMA
THE
EQUALITY
FOLLOWS
FROM
THE
DEFI
NITION
OF
THE
FINAL
INEQUALITY
FOLLOWS
FROM
EQ
WE
NOW
BOUND
THE
TERM
T
X
T
ΑDT
BY
THE
DEFINITION
OF
C
OPT
AND
THE
CON
VEXITY
OF
THE
FUNCTION
SΑ
FOR
Α
ANY
C
INTERVAL
CONTAINS
AT
MOST
C
C
OPT
C
Α
AMOUNT
OF
WORK
IN
OPT
OUR
NEXT
STEP
IS
TO
UPPER
BOUND
T
FOR
ANY
T
X
IN
PARTICULAR
WE
CLAIM
THAT
FOR
ANY
AND
SUCH
THAT
T
X
AND
T
Q
T
R
Z
X
DX
C
C
Α
MAX
T
OPT
X
TO
SEE
THIS
THE
CRUCIAL
OBSERVATION
IS
THAT
T
FOR
T
Z
AS
Z
X
X
IF
XK
FOR
K
THEN
THE
INTERVAL
T
CAN
CONTAIN
AT
MOST
K
Α
K
C
C
OPT
C
WORK
FROM
SIMILARLY
IF
X
THEN
THE
INTERVAL
T
CAN
CONTAIN
AT
MOST
K
C
C
OPT
C
Α
WORK
FROM
THUS
ANY
INTERVAL
CONTAINING
T
AND
WITH
K
C
KC
CAN
CONTAIN
AT
MOST
K
C
C
OPT
C
Α
WORK
FROM
THE
BOUND
ON
T
FOLLOWS
AS
THE
INTEGRAL
IS
JUST
THE
WORK
FROM
OVER
THE
TIME
INTERVAL
THUS
RT
X
T
ΑDT
RT
X
F
C
OPT
Α
Α
DT
RT
X
C
OPT
C
DT
C
OPT
COMBINING
EQS
AND
WE
HAVE
THAT
FOR
ANY
C
INTERVAL
X
RT
X
Q
T
ΑDT
RT
X
T
Α
T
Α
DT
Α
Α
Α
C
OPT
THIS
ACCOMPLISHES
OUR
GOAL
COMPUTING
THE
OPTIMAL
OFFLINE
TEMPERATURE
SCHEDULE
IN
THIS
SECTION
WE
CONSIDER
THE
OFFLINE
PROBLEM
OF
SPEED
SCALING
TO
MINIMIZE
THE
MAXIMUM
TEMPERATURE
WE
SHOW
HOW
TO
SOLVE
THIS
PROBLEM
IN
POLYNOMIAL
TIME
WITH
ARBITRARY
PRECISION
USING
THE
ELLIPSOID
ALGORITHM
FOR
BASIC
INFORMATION
ON
THE
USE
OF
THE
ELLIPSOID
ALGORITHM
TO
SOLVE
CONVEX
PROBLEMS
SEE
FOR
EXAMPLE
NESTOROV
WE
ASSUME
A
CONSTANT
TMAX
THAT
IS
THE
THERMAL
THRESHOLD
FOR
THE
DEVICE
THE
PROBLEM
IS
THEN
TO
DETERMINE
WHETHER
THERE
IS
A
SCHEDULE
THAT
IS
FEASIBLE
AND
MAINTAINS
THE
INVARIANT
THAT
THE
TEMPERATURE
STAYS
BELOW
TMAX
BY
BINARY
SEARCH
WE
CAN
THEN
SOLVE
THE
PROBLEM
OF
MINIMIZING
TMAX
BEFORE
GIVING
THE
CONVEX
PROGRAM
WE
NEED
TO
MAKE
A
FEW
DEFINITIONS
LET
MAXW
TX
TY
TX
TY
BE
THE
MAXIMUM
WORK
THAT
CAN
BE
DONE
STARTING
AT
TIME
TX
AT
TEMPERATURE
TX
AND
ENDING
AT
TIME
TY
AT
TEMPERATURE
TY
SUBJECT
TO
THE
TEMPERATURE
CONSTRAINT
T
TMAX
THROUGHOUT
THE
INTERVAL
TX
TY
STRICTLY
SPEAKING
MAXW
IS
A
FUNCTION
OF
TMAX
BUT
WE
SUPPRESS
THIS
IN
THE
NOTATION
AS
WE
ASSUME
THROUGHOUT
THAT
TMAX
IS
FIXED
AND
GIVEN
A
PRIORI
IN
ORDER
TO
UNDERSTAND
MAXW
WE
WILL
FIRST
NEED
TO
UNDERSTAND
THE
UNCONSTRAINED
PROBLEM
UMAXW
TX
TY
TX
TY
DEFINED
AS
THE
MAXIMUM
POSSIBLE
WORK
THAT
CAN
BE
DONE
DURING
THE
INTERVAL
TX
TY
SUBJECT
TO
THE
BOUNDARY
CONSTRAINTS
THAT
T
TX
TX
AND
T
TY
TY
IN
PARTICULAR
THE
TEMPER
ATURE
AT
ANY
TIME
IS
ALLOWED
TO
EXCEED
TMAX
IN
LEMMA
WE
PROVE
THE
INTUITIVE
FACT
THAT
MAXW
AND
UMAXW
ARE
WELL
DEFINED
IF
AND
ONLY
IF
IT
IS
POSSIBLE
TO
COOL
AS
QUICKLY
AS
SPECIFIED
LEMMA
SUPPOSE
THAT
TX
AND
TY
ARE
AT
MOST
TMAX
EACH
OF
THE
QUANTIES
MAXW
TX
TY
TX
TY
AND
UMAXW
TX
TY
TX
TY
ARE
WELL
DEFINED
IF
AND
ONLY
IF
TY
TX
EXP
B
TX
TY
PROOF
WE
WISH
TO
SHOW
THAT
IF
TY
TX
EXP
B
TX
TY
THEN
THERE
IS
NO
FEASI
BLE
SOLUTION
CONSIDER
THE
CASE
THAT
THE
POWER
IS
ZERO
THROUGHOUT
THE
INTERVAL
TX
TY
THEN
THROUGHOUT
THIS
INTERVAL
IT
IS
THE
CASE
THAT
T
T
T
BT
T
SOLVING
THIS
DIFFER
ENTIAL
EQUATION
WE
GET
T
T
TX
EXP
B
TX
T
FOR
T
TX
TY
SO
IN
PARTICULAR
IF
TY
TX
EXP
B
TX
TY
THEN
THERE
IS
NO
FEASIBLE
SOLUTION
TO
EITHER
THE
CONSTRAINED
PROBLEM
OR
UNCONSTRAINED
PROBLEM
SINCE
THE
POWER
MUST
BE
NONNEGATIVE
WE
NOW
WISH
TO
SHOW
THAT
IF
TY
TX
EXP
B
TX
TY
THEN
THERE
IS
A
FEASIBLE
SOLUTION
FOR
THE
CONSTRAINED
PROBLEM
AND
HENCE
TRIVIALLY
FOR
THE
UNCONSTRAINED
PROBLEM
IF
TY
TX
EXP
B
TX
TY
THEN
RUNNING
WITH
POWER
EQUAL
TO
ZERO
IS
A
FEASIBLE
SOLUTION
SO
ASSUME
THAT
TY
TX
EXP
B
TX
TY
LET
TZ
BE
SOME
TIME
JUST
AFTER
TX
ONE
FEASIBLE
SOLUTION
FOR
THE
CONSTRAINED
PROBLEM
IS
FOR
THE
TEMPERATURE
TO
RISE
TO
TMAX
AT
TIME
TZ
THEN
STAY
AT
TEMPERATURE
TMAX
UNTIL
THE
TIME
TV
THAT
SOLVES
TY
TMAX
EXP
B
TV
TY
AND
THEN
RUN
WITH
POWER
EQUAL
TO
ZERO
UNTIL
TIME
TY
WE
ARE
NOW
READY
TO
GIVE
THE
CONVEX
PROGRAM
WE
DIVIDE
TIME
INTO
INTERVALS
DEMARCATED
BY
THE
LIST
TM
OF
ALL
RELEASE
TIMES
AND
DEADLINES
WE
INTRODUCE
A
VARIABLE
TI
THAT
REPRESENTS
T
TI
THE
TEMPERATURE
AT
TIME
TI
LET
J
I
BE
THE
SET
OF
TASKS
J
THAT
CAN
FEASIBLY
BE
EXECUTED
DURING
THE
TIME
INTERVAL
TI
TI
THAT
IS
R
J
TI
AND
D
J
TI
WE
INTRODUCE
A
VARIABLE
WI
J
FOR
J
J
I
THAT
REPRESENTS
THE
WORK
DONE
ON
TASK
J
DURING
TI
TI
WE
CAN
THEN
EXPRESS
OUR
PROBLEM
AS
A
MATHEMATICAL
PROGRAM
CP
IN
A
RELATIVELY
STRAIGHTFORWARD
WAY
P
J
I
J
J
I
WI
J
J
N
J
J
I
WI
J
MAXW
TI
TI
TI
TI
I
M
TI
EXP
B
TI
TI
TI
I
M
TI
TMAX
I
M
TI
I
M
WI
J
I
M
J
N
CONSTRAINT
ENSURES
THAT
ENOUGH
WORK
IS
DONE
TO
FINISH
EACH
JOB
CONSTRAINT
ENSURES
THAT
IT
IS
FEASIBLE
TO
COMPLETE
THE
CLAIMED
WORK
WITHIN
AN
INTERVAL
BY
LEMMA
CONSTRAINT
ENSURES
THAT
THE
QUANTITY
MAXW
TI
TI
TI
TI
IS
WELL
DEFINED
LEMMA
THE
MATHEMATICAL
PROGRAM
C
P
IS
CONVEX
THAT
IS
THE
FEASIBLE
REGION
DESCRIBED
IN
C
P
IS
CONVEX
PROOF
TO
SEE
THAT
THE
FEASIBLE
REGION
IS
CONVEX
LET
T
AND
Tˆ
BE
THE
TEMPERATURE
CURVES
CORRESPONDING
TO
TWO
FEASIBLE
SOLUTIONS
TO
THIS
PROBLEM
LET
T
T
Tˆ
THE
SPEED
CURVE
CORRESPONDING
TO
T
IS
T
T
BT
A
Α
T
T
Tˆ
T
BT
BTˆ
Α
THEN
SINCE
X
Α
IS
A
CONCAVE
FUNCTION
FOR
Α
Sˆ
THAT
IS
THE
AVERAGE
OF
THE
TWO
UNDERLYING
FEASIBLE
SOLUTIONS
IS
FEASIBLE
TO
APPLY
THE
ELLIPSOID
ALGORITHM
ONE
NEEDS
TO
GIVE
A
PROCEDURE
TO
DETERMINE
WHETHER
AN
ARBITRARY
POINT
IS
FEASIBLE
AND
IF
NOT
TO
DETERMINE
A
SEPARATING
HYPER
PLANE
IN
PARTICULAR
IF
G
IS
A
VIOLATED
CONSTRAINT
THEN
THE
SEPARATING
HYPERPLANE
FOR
US
WILL
BE
THE
HYPERPLANE
WHOSE
NORMAL
IS
THE
GRADIENT
OF
G
EVALUATED
AT
THE
CUR
RENT
POINT
THE
ONLY
CONSTRAINTS
FOR
WHICH
THIS
IS
NOT
STRAIGHTFORWARD
ARE
THE
MAXW
CONSTRAINTS
SO
WE
ASSUME
FOR
THE
REST
OF
THIS
SECTION
THAT
ALL
CONSTRAINTS
OTHER
THAN
THE
MAXW
CONSTRAINTS
ARE
NOT
VIOLATED
BY
THE
CURRENT
POINT
AS
THIS
IS
THE
ONLY
INTERESTING
CASE
OUR
GOAL
IS
THEN
TO
EXPLAIN
HOW
TO
TAKE
THE
GRADIENT
OF
THE
MAXW
FUNCTION
SO
THAT
WE
MAY
COMPUTE
A
SEPARATING
HYPERPLANE
TO
BETTER
UNDERSTAND
MAXW
WE
FIRST
NEED
TO
UNDERSTAND
THE
UNCONSTRAINED
FUNCTION
UMAXW
THE
UNCONSTRAINED
MAXIMUM
WORK
PROBLEM
WE
CONSIDER
THE
UN
CONSTRAINED
PROBLEM
UMAXW
TI
TI
TI
TI
WHERE
THE
TIMES
AND
TEMPERATURES
ARE
ARBITRARY
OTHER
THAN
THAT
WE
REQUIRE
THAT
TI
TI
EXP
B
TI
TI
SO
THAT
A
FEASIBLE
SOLUTION
EXISTS
LET
UMAXT
T
UMAXT
TI
TI
TI
TI
T
DENOTE
THE
TEMPERATURE
AS
A
FUNCTION
OF
THE
TIME
T
THAT
SOLVES
UMAXW
TI
TI
TI
TI
THAT
IS
UMAXT
T
IS
THE
TEMPERATURE
CURVE
T
THAT
MAXIMIZES
THE
QUANTITY
R
TI
P
T
ΑDT
R
TI
F
T
T
T
BT
T
Α
DT
TI
TI
A
SUBJECT
TO
THE
CONSTRAINTS
THAT
P
T
T
TI
TI
AND
T
TI
TI
THIS
PROBLEM
FALLS
UNDER
THE
RUBRIC
OF
CALCULUS
OF
VARIATIONS
WE
REFER
THE
READER
TO
SMITH
FOR
THE
BASICS
ON
CALCULUS
OF
VARIATIONS
FOR
NOTATIONAL
SIMPLICITY
WE
USUALLY
TRANSLATE
TIME
SO
THAT
SINCE
TEMPERATURE
IS
ALWAYS
A
FUNCTION
OF
TIME
WE
WILL
DROP
T
IN
FUTURE
REFERENCES
TO
TEMPERATURE
FUNCTIONS
LET
F
BE
THE
FUNCTIONAL
F
T
T
BT
AND
LET
F
A
F
B
T
T
BT
T
Α
BE
THE
PARTIAL
DERIVATIVE
OF
F
WITH
RESPECT
TO
T
AND
LET
FT
T
Α
T
T
BT
Α
BE
THE
PARTIAL
DERIVATIVE
OF
F
WITH
RESPECT
TO
T
T
ANY
WEAK
EXTREMUM
T
MUST
SATISFY
THE
EULER
LAGRANGE
EQUATION
SEE
E
G
SMITH
PAGE
D
FT
DT
FT
T
WE
CALL
A
TEMPERATURE
FUNCTION
T
THAT
SATISFIES
THE
EULER
LAGRANGE
EQUATION
AN
EULER
LAGRANGE
CURVE
WE
HAVE
THAT
D
F
Α
T
T
BT
T
TT
BT
T
DT
T
T
Α
Α
THUS
THE
EULER
LAGRANGE
EQUATION
GIVES
THAT
B
T
T
BT
Α
T
T
BT
T
TT
BT
T
Α
Α
Α
Α
ELIMINATING
COMMON
FACTORS
AND
MULTIPLYING
BY
T
T
BT
Α
GIVES
BT
T
T
TT
USING
THE
STANDARD
LAPLACE
TRANSFORM
TECHNIQUE
WE
FIND
THAT
THE
SOLUTION
TO
THE
ABOVE
DIFFERENTIAL
EQUATION
IS
UMAXT
T
C
EXP
BT
D
EXP
BTΑ
Α
WHERE
THE
CONSTANTS
C
AND
D
ARE
DETERMINED
BY
THE
BOUNDARY
CONDITIONS
ALTERNA
TIVELY
ONE
CAN
VERIFY
THE
CORRECTNESS
OF
THIS
SOLUTION
BY
PLUGGING
IT
BACK
INTO
THE
DIFFERENTIAL
EQUATION
WE
NOW
COMPUTE
THE
VALUES
OF
C
AND
D
SETTING
T
AND
T
IN
EQ
WE
GET
C
D
SETTING
T
AND
T
IN
EQ
WE
GET
C
EXP
D
EXP
Α
USING
C
D
WE
HAVE
D
EXP
D
EXP
Α
OR
D
EXP
EXP
EXP
Α
AND
THEREFORE
C
EXP
EXP
EXP
Α
THIS
GIVES
A
COMPLETE
DESCRIPTION
OF
THE
CURVE
UMAXT
IN
PARTICULAR
THE
TEM
PERATURE
VALUES
AT
TWO
TIMES
COMPLETELY
DETERMINE
THE
CURVE
UMAXT
THAT
PASSES
THROUGH
THESE
POINTS
SINCE
EXP
WE
CAN
CONCLUDE
THAT
D
AND
HENCE
C
D
WE
MUST
CHECK
THAT
OUR
CURVE
UMAXT
IS
INDEED
A
MAXIMUM
AND
NOT
A
MINIMUM
THIS
IS
EASILY
SEEN
BY
SUBSTITUTING
D
FOR
C
IN
EQ
WE
HAVE
UMAXT
EXP
BT
D
EXP
BTΑ
Α
EXP
BT
SINCE
D
AND
THE
TERM
IN
PARENTHESES
ARE
BOTH
NON
POSITIVE
THEIR
PRODUCT
IS
NON
NEGATIVE
AND
THUS
UMAXT
IS
AT
LEAST
AS
GREAT
AS
THE
NOPOWER
CURVE
T
EXP
BT
NOTE
THAT
UMAXT
IS
WELL
DEFINED
FOR
ALL
TIMES
T
NOT
JUST
FOR
T
WE
WILL
ARGUE
ABOUT
PROPERTIES
OF
UMAXT
ON
THIS
LARGER
DOMAIN
WE
NOW
TURN
OUR
ATTENTION
TO
EVALUATING
THE
WORK
UMAXW
DONE
BY
THE
CURVE
UMAXT
DIFFERENTIATING
EQ
GIVES
UMAXTT
T
T
BC
EXP
BT
BDΑ
EXP
BTΑ
Α
ADDING
EQ
TO
B
TIMES
EQ
THE
TERM
BC
EXP
BT
CANCELS
OUT
LEAVING
JUST
T
T
BT
BD
Α
EXP
BTΑ
Α
BY
EQ
THE
POWER
FUNCTION
CORRESPONDING
TO
THE
TEMPERATURE
CURVE
UMAXT
IS
THEN
HENCE
BY
EQ
T
T
BT
A
BD
A
Α
EXP
BTΑ
Α
UMAXW
R
F
T
T
BT
Α
R
F
BD
Α
A
Α
EXP
Α
WE
NOW
STATE
SEVERAL
INTUITIVE
BUT
TECHNICAL
PROPERTIES
OF
THE
UMAXT
WHEN
THE
PROPERTY
IS
NOT
OBVIOUS
WE
WILL
GIVE
SOME
EXPLANATION
WHY
THE
PROPERTY
HOLDS
IN
FACT
WE
OBSERVE
THAT
IT
IS
OBVIOUS
FROM
EQ
FOR
UMAXT
THAT
THE
TEMPERATURE
APPROACHES
ZERO
AS
TIME
GOES
TO
INFINITY
LEMMA
OBSERVES
THAT
UMAXT
HAS
A
UNIQUE
MAXIMUM
AND
CHARACTERIZES
WHERE
THIS
MAXIMUM
OCCURS
LEMMA
OBSERVES
THAT
IF
THEN
THE
TEMPERATURE
WILL
ALWAYS
STAY
ABOVE
LEMMA
OBSERVES
THAT
THE
MAXIMUM
TEMPERATURE
IS
A
NONDECREASING
FUNCTION
OF
AND
FINALLY
LEMMA
OBSERVES
THAT
THE
MAXIMUM
TEMPERATURE
WILL
EXCEED
TMAX
FOR
SUFFICIENTLY
LARGE
FACT
FOR
ANY
AND
THE
CURVE
UMAXT
T
APPROACHES
AS
T
APPROACHES
INFINITY
LEMMA
LET
UMAXT
AND
UMAXT
BE
THE
TWO
CURVES
THAT
INTERSECT
AT
DISTINCT
POINTS
TA
TA
AND
TB
TB
WHERE
TA
TB
MAX
MIN
LET
UMAXT
TA
TA
TB
TB
THEN
T
T
T
FOR
ALL
T
TA
PROOF
FOLLOWS
IMMEDIATELY
FROM
THE
UNIQUENESS
OF
THE
MAXIMUM
DETERMINED
BY
TWO
POINTS
LEMMA
CONSIDER
THE
CURVE
UMAXT
UMAXT
TI
TI
TI
TI
THE
CURVE
UMAXT
HAS
AT
MOST
ONE
POINT
WHERE
UMAXTT
THE
DERIVATIVE
OF
UMAXT
WITH
RESPECT
TO
TIME
T
IS
IF
UMAXTT
X
FOR
SOME
TIME
X
THEN
UMAXTT
T
FOR
ALL
T
X
IF
UMAXTT
THEN
THE
MAXIMUM
OF
UMAXT
IS
AT
T
IF
UMAXTT
THEN
THE
MAXIMUM
OF
UMAXT
IS
AT
T
IF
UMAXTT
AND
UMAXTT
THEN
THE
MAXIMUM
OF
UMAXT
IS
AT
THE
UNIQUE
POINT
TX
WHERE
UMAXTT
TX
PROOF
IF
D
IT
FOLLOWS
FROM
EQ
THAT
T
T
BC
EXP
BT
EXP
BT
AND
HENCE
THAT
UMAXTT
EVERYWHERE
AND
IT
IS
EASY
TO
SEE
THAT
THE
ABOVE
CLAIMS
HOLD
NOW
ASSUME
THAT
D
USING
C
D
AND
MULTIPLYING
BY
EXP
BT
B
EQ
CAN
BE
REWRITTEN
AS
EXP
BT
UMAXTT
DΑ
B
D
Α
EXP
BT
Α
NOTE
THAT
SINCE
EXP
BT
B
FOR
ALL
T
THE
SIGN
OF
UMAXTT
IS
THE
SAME
AS
THE
SIGN
OF
THE
RIGHT
HAND
SIDE
OF
THE
ABOVE
EQUATION
WHICH
IS
A
STRICTLY
DECREASING
FUNCTION
OF
T
IN
PARTICULAR
OBSERVE
THAT
THIS
IMPLIES
THAT
UMAXTT
CANNOT
GO
FROM
NEGATIVE
TO
POSITIVE
ALL
THE
ABOVE
CLAIMS
ARE
THEN
SIMPLE
CONSEQUENCES
OF
THIS
OBSERVATION
LEMMA
CONSIDER
TWO
POINTS
AND
LET
DENOTE
THE
CONSTANT
TEMPERATURE
CURVE
BETWEEN
AND
LET
DENOTE
A
TEMPERATURE
CURVE
SUCH
THAT
AND
T
T
FOR
T
THEN
AT
LEAST
AS
MUCH
WORK
IS
COMPLETED
BY
FOLLOWING
AS
BY
FOLLOWING
PROOF
CONSIDER
A
TEMPERATURE
CURVE
T
SUCH
THAT
T
T
APPLY
ING
EQ
THE
ENERGY
USED
DURING
BY
FOLLOWING
T
IS
B
T
A
T
DT
UNDER
THE
TEMPERATURE
CONSTRAINT
T
T
THIS
INTEGRAL
IS
MAXIMIZED
WITH
T
T
THROUGHOUT
AND
THUS
THE
TOTAL
ENERGY
OF
IS
AT
MOST
THE
TOTAL
ENERGY
OF
BY
CONVEXITY
FOR
A
GIVEN
ENERGY
BUDGET
WORKING
AT
CONSTANT
POWER
MAXIMIZES
THE
WORK
DONE
THAT
IS
THE
MAXIMUM
WORK
THAT
CAN
BE
DONE
BY
ANY
CURVE
L
WITH
ENERGY
AT
MOST
E
IS
E
Α
AND
IS
ACHIEVED
BY
STAYING
AT
CONSTANT
POWER
AS
STAYS
AT
CONSTANT
TEMPERATURE
AND
HENCE
CONSTANT
POWER
IT
FOLLOWS
THAT
THE
WORK
COMPLETED
BY
FOLLOWING
THE
CURVE
IS
NO
MORE
THAN
THAT
OF
LEMMA
LET
AND
BE
FIXED
AND
CONSIDER
THE
CLASS
OF
UNCONSTRAINED
CURVES
UMAXT
FOR
IN
PARTICULAR
FOR
LET
AND
DENOTE
THE
CURVES
UMAXT
T
AND
U
MAX
T
T
RESPECTIVELY
THEN
IF
WE
HAVE
THAT
FOR
ALL
TIMES
T
T
T
IF
THEN
FOR
ALL
T
SUCH
THAT
T
T
T
IN
PARTICULAR
THIS
IMPLIES
THE
FOLLOWING
THE
MAXIMUM
TEMPERATURE
REACHED
BY
THE
CURVE
UMAXT
IN
THE
INTERVAL
IS
A
NONDECREASING
FUNCTION
OF
IF
THEN
UMAXTT
IS
A
NONDECREASING
FUNCTION
OF
IF
THEN
UMAXTT
IS
A
NONINCREASING
FUNCTION
OF
PROOF
WE
FIRST
CONSIDER
THE
CASE
THAT
AS
IT
MUST
BE
TRUE
THAT
EITHER
T
T
FOR
ALL
T
OR
ONE
OF
THESE
TEMPERATURE
CURVES
IS
ALWAYS
LARGER
THAN
THE
OTHER
SINCE
BY
LEMMA
IF
THEY
INTERSECT
AT
TWO
POINTS
THEY
ARE
THE
SAME
BY
THE
MEAN
VALUE
THEOREM
THERE
MUST
BE
A
TIME
T
WHERE
T
BY
LEMMA
THIS
IMPLIES
THAT
X
FOR
ALL
X
T
IN
PARTICULAR
SINCE
T
THIS
IMPLIES
THAT
HENCE
T
T
IT
IS
THEN
OBVIOUS
THAT
THE
MAXIMUM
TEMPERATURE
MUST
BE
A
NONDECREASING
FUNCTION
OF
THE
ENDING
TIME
AND
THE
DERIVATIVE
AT
TIME
MUST
BE
A
NONDECREASING
FUNCTION
OF
THE
ENDING
TIME
WE
NOW
CONSIDER
THE
CASE
THAT
INSTEAD
OF
IT
WILL
BE
MORE
CONVENIENT
TO
WORK
WITH
THE
CURVE
UMAXT
WHICH
IS
JUST
THE
CURVE
TRANSLATED
TO
THE
RIGHT
BY
TIME
UNITS
NOW
SINCE
EITHER
AND
ARE
IDENTICAL
OR
ONE
THESE
TEMPERATURE
CURVES
IS
ALWAYS
LARGER
THAN
THE
OTHER
IF
T
T
THEN
THIS
WOULD
IN
PARTICULAR
MEAN
THAT
BUT
BY
LEMMA
IT
IS
CLEAR
THAT
T
FOR
ALL
T
THUS
IT
MUST
BE
THE
CASE
THAT
T
T
FOR
ALL
T
IT
IS
THEN
OBVIOUS
THAT
THE
MAXIMUM
TEMPERATURE
MUST
BE
A
NONDECREASING
FUNCTION
OF
THE
ENDING
TIME
AND
THE
DERIVATIVE
AT
THE
ENDING
TIME
MUST
BE
A
NONINCREASING
FUNCTION
OF
THE
ENDING
TIME
LEMMA
CONSIDER
THE
TEMPERATURE
CURVE
UMAXT
AS
TMAX
AND
TMAX
ARE
FIXED
AND
IS
VARIED
THEN
THERE
IS
A
FINITE
TIME
T
SUCH
THAT
THE
MAXIMUM
TEMPERATURE
REACHED
BY
THE
CURVE
UMAXT
IS
AT
LEAST
TMAX
FOR
ALL
T
PROOF
CONSIDER
THE
CURVE
U
UMAXT
TMAX
AS
U
T
APPROACHES
AS
T
APPROACHES
INFINITY
THERE
EXISTS
A
FINITE
TIME
Tˆ
SUCH
THAT
U
Tˆ
CONSIDER
THE
CURVE
UMAXT
Tˆ
AS
U
AND
SHARE
THE
POINTS
AND
Tˆ
U
AND
MUST
BE
IDENTICAL
THUS
UMAXT
Tˆ
ATTAINS
A
MAXIMUM
TEMPERATURE
OF
AT
LEAST
TMAX
AND
HENCE
BY
LEMMA
THE
MAXIMUM
TEMPERATURE
REACHED
BY
UMAXT
IS
AT
LEAST
TMAX
FOR
ALL
Tˆ
THE
TEMPERATURE
CONSTRAINED
MAXIMUM
WORK
PROBLEM
WE
NOW
TURN
OUR
ATTENTION
TO
MAXW
AND
MAXT
AGAIN
REQUIRING
THAT
EXP
SO
THAT
A
FEASIBLE
SOLUTION
EXISTS
ALSO
WE
NOW
REQUIRE
THAT
BOTH
AND
ARE
AT
MOST
TMAX
THAT
IS
WE
ASSUME
THE
EXISTENCE
OF
A
TEMPERATURE
CONSTRAINT
T
TMAX
IT
IS
KNOWN
THAT
WHEN
SUCH
A
GLOBAL
CONSTRAINT
IS
ADDED
THE
SOLUTION
CAN
BE
DECOMPOSED
INTO
SUBCURVES
WHERE
EACH
SUBCURVE
IS
EITHER
AN
EULER
LAGRANGE
CURVE
CORRESPONDING
TO
SOME
UNCONSTRAINED
PROBLEM
OR
ELSE
FOLLOWS
THE
BOUNDARY
SMITH
PAGE
WE
ARE
FORTUNATE
IN
OUR
CASE
THAT
THE
PORTION
OF
MAXT
FOR
WHICH
MAXT
TMAX
IS
A
SINGLE
LINE
SEGMENT
THIS
IS
AN
IMMEDIATE
CONSEQUENCE
OF
LEMMA
WE
NOW
KNOW
THAT
EITHER
MAXT
UMAXT
OR
MAXT
CONSISTS
OF
THREE
PARTS
AN
EULER
LAGRANGE
CURVE
UP
TO
TMAX
A
LINE
SEGMENT
AT
TMAX
AND
AN
EULER
LAGRANGE
CURVE
DOWN
TO
NOTE
THAT
EITHER
OR
BOTH
OF
THE
EULER
LAGRANGE
CURVES
MAY
HAVE
LENGTH
ZERO
THE
EULER
LAGRANGE
CURVE
UP
TO
TMAX
IS
OF
LENGTH
ZERO
IF
AND
ONLY
IF
TMAX
THE
EULER
LAGRANGE
CURVE
DOWN
FROM
TMAX
IS
OF
LENGTH
ZERO
IF
AND
ONLY
IF
TMAX
BEFORE
WE
CHARACTERIZE
THE
CURVE
MAXT
IN
LEMMA
WE
DEFINE
SOME
NOTATION
LET
T
DENOTE
THE
SUPREMUM
OF
VALUES
OF
FOR
WHICH
THE
CURVE
UMAXT
DOES
NOT
EXCEED
TEMPERATURE
TMAX
AT
ANY
TIME
THE
TIME
T
IS
WELL
DEFINED
AND
FINITE
BY
LEMMA
NOTE
THAT
T
IS
A
FUNCTION
OF
AND
DEFINE
Γ
TO
BE
THE
TIME
UNIQUE
BY
LEMMA
AT
WHICH
THE
MAXIMUM
TEMPERATURE
IS
ATTAINED
ON
THE
CURVE
UMAXT
T
DEFINE
Β
T
Γ
WE
NOW
PROVIDE
ALTERNATE
CHARACTERIZATIONS
OF
Γ
AND
Β
AND
SHOW
THAT
THESE
POINTS
OCCUR
WHERE
THE
DERIVATIVE
OF
UMAXT
IS
ZERO
LEMMA
Γ
IS
THE
LARGEST
VALUE
OF
FOR
WHICH
THE
MAXIMUM
TEMPERATURE
ATTAINED
BY
THE
CURVE
UMAXT
TMAX
DURING
THE
INTERVAL
IS
NO
MORE
THAN
TMAX
SIMILARLY
Β
IS
THE
LARGEST
VALUE
OF
FOR
WHICH
THE
MAXIMUM
TEMPERATURE
ATTAINED
BY
THE
CURVE
UMAXT
TMAX
IN
IS
NO
MORE
THAN
TMAX
PROOF
BY
LEMMA
UMAXT
T
UMAXT
Γ
TMAX
CALL
THIS
CURVE
T
T
LET
Γ
T
Γ
AND
CONSIDER
THE
CURVE
T
UMAXT
Γ
TMAX
WE
KNOW
THAT
Γ
T
Γ
T
TMAX
SO
USING
LEMMA
AGAIN
WE
KNOW
THAT
Γ
Γ
TMAX
THE
PROOF
FOR
Β
IS
SIMILAR
LEMMA
UMAXT
Γ
TMAX
T
Γ
AND
UMAXT
TMAX
Β
T
PROOF
AGAIN
USING
UMAXT
T
UMAXT
Γ
TMAX
WE
SEE
THAT
THE
DERIVATIVE
MUST
BE
AT
T
Γ
SINCE
THE
MAXIMUM
IS
ATTAINED
THERE
THE
ARGUMENT
FOR
Β
IS
SIMILAR
LEMMA
CONSIDER
THE
CURVE
MAXT
LET
Γ
AND
Β
BE
DEFINED
AS
ABOVE
IF
Γ
Β
THEN
MAXT
UMAXT
IF
Γ
Β
THEN
THE
CURVE
MAXT
TRAVELS
ALONG
THE
CURVE
UMAXT
Γ
TMAX
THEN
STAYS
AT
TMAX
UNTIL
TIME
Β
AND
FINALLY
TRAVELS
ALONG
THE
CURVE
UMAXT
Β
TMAX
PROOF
ASSUME
FIRST
THAT
T
Γ
Β
BY
LEMMA
THE
MAXIMUM
TEM
PERATURE
REACHED
BY
UMAXT
DURING
IS
A
NONDECREASING
FUNCTION
OF
HENCE
BY
THE
DEFINITION
OF
T
AT
NO
TIME
CAN
THE
FUNCTION
UMAXT
EXCEED
TMAX
THEREFORE
MAXT
UMAXT
NOW
CONSIDER
THE
CASE
THAT
Γ
Β
LET
TX
AND
TY
DENOTE
THE
FIRST
AND
LAST
TIMES
RESPECTIVELY
AT
WHICH
MAXT
EQUALS
TMAX
WE
WILL
SHOW
THAT
TX
Γ
AND
TY
Β
BY
DEFINITION
OF
TX
AND
TY
THE
CURVE
MAXT
RESTRICTED
TO
TX
IS
IDENTICAL
TO
THE
CURVE
UMAXT
TX
TMAX
AND
THE
CURVE
MAXT
RESTRICTED
TO
TY
IS
IDENTICAL
TO
THE
CURVE
UMAXT
TMAX
TY
IF
TX
Γ
THEN
BY
LEMMA
AND
LEMMA
THIS
CONTRADICTS
THE
DEFINITION
OF
Γ
A
SIMILAR
ARGUMENT
SHOWS
THAT
TY
Β
NOW
SUPPOSE
FOR
CONTRADICTION
THAT
TX
Γ
SINCE
TY
Β
Γ
IT
FOLLOWS
THAT
THE
POINT
Γ
TMAX
LIES
ON
MAXT
HENCE
FOR
THE
NON
ZERO
LENGTH
TIME
INTERVAL
TX
Γ
IT
MUST
BE
THE
CASE
THAT
MAXT
TMAX
THIS
CONTRADICTS
THE
WORK
OPTIMALITY
OF
UMAXT
Γ
TMAX
AND
HENCE
THE
WORK
OPTIMALITY
OF
MAXT
AGAIN
IN
A
SIMILAR
MANNER
WE
CAN
OBTAIN
A
CONTRADICTION
FROM
THE
ASSUMPTION
THAT
TY
Β
WE
NOW
EXPLAIN
HOW
TO
EXPLICITLY
COMPUTE
Γ
AND
Β
CONSIDER
THE
CURVE
U
UMAXT
Γ
TMAX
RECALL
FROM
LEMMA
THAT
U
T
Γ
SETTING
U
T
Γ
IN
EQ
GIVES
C
DΑ
EXP
BΓ
Α
PLUGGING
IN
THE
VALUES
OF
C
AND
D
FROM
EQS
AND
WE
HAVE
THAT
Γ
IS
THE
UNIQUE
SOLUTION
OF
THE
EQUATION
EXP
EXP
EXP
Α
Α
EXP
BΓ
Α
F
EXP
Α
EXP
EXP
Α
BY
MULTIPLYING
THROUGH
BY
EXP
EXP
Α
AND
AGGREGATING
LIKE
TERMS
THIS
IS
EQUIVALENT
TO
EXP
BΓ
Α
Α
Α
TMAX
ΑTMAX
EXP
BΓ
Α
SIMILARLY
THE
CURVE
U
UMAXT
TMAX
Β
SATISFIES
U
T
WE
CAN
SEE
FROM
EQ
THAT
U
T
IS
EQUIVALENT
TO
C
DΑ
Α
PLUGGING
IN
THE
VALUES
OF
C
AND
D
BY
EQS
AND
WE
HAVE
THAT
Β
IS
THE
UNIQUE
SOLUTION
OF
THE
EQUATION
EXP
EXP
EXP
Α
F
EXP
F
Α
EXP
EXP
Α
Α
BY
MULTIPLYING
THROUGH
BY
EXP
EXP
Α
AND
AGGREGATING
LIKE
TERMS
THIS
IS
EQUIVALENT
TO
TMAX
EXP
BΒΑ
Α
Α
EXP
BΒ
Α
COMPUTING
A
SEPARATING
HYPERPLANE
WE
ARE
NOW
FINALLY
READY
TO
EXPLAIN
HOW
TO
COMPUTE
A
SEPARATING
HYPERPLANE
FOR
A
VIOLATED
MAXW
CONSTRAINT
CONSIDER
AN
ARBITRARY
POINT
WHERE
EACH
TI
TAKES
THE
VALUE
TˆI
AND
EACH
WI
J
TAKES
THE
VALUE
Wˆ
I
J
ASSUME
THAT
THE
I
TH
MAXW
CONSTRAINT
IS
VIOLATED
GIVEN
THE
VALUES
OF
TI
TI
TˆI
AND
TˆI
WE
CAN
COMPUTE
THE
VALUES
Γ
AND
Β
BY
BINARY
SEARCH
USING
THE
EQS
AND
NOTE
THAT
THE
LEFT
HAND
SIDES
OF
THESE
EQUATIONS
ARE
MONOTONE
FUNCTIONS
OF
Γ
AND
Β
RESPECTIVELY
FIRST
CONSIDER
THE
CASE
THAT
TI
TI
Γ
Β
THEN
WE
KNOW
THE
MAXIMUM
TEMPERATURE
CONSTRAINT
IS
NOT
RELEVANT
HERE
NOW
EQ
GIVES
THAT
MAXW
D
A
Α
WHERE
BY
EQ
B
Α
Α
EXP
B
TI
TI
Α
D
TI
EXP
B
TI
TI
TI
EXP
B
TI
TI
EXP
B
TI
TI
Α
Α
WE
CAN
THEN
DETERMINE
WHETHER
THE
I
TH
MAXW
CONSTRAINT
IS
VIOLATED
IF
THIS
CONSTRAINT
IS
VIOLATED
TO
COMPUTE
A
SEPARATING
HYPERPLANE
LET
G
BE
A
FUNCTION
OF
TI
TI
AND
WI
J
FOR
J
J
I
DEFINED
AS
G
J
J
I
WI
J
UMAXW
TI
TI
TI
TI
THE
I
TH
MAXW
CONSTRAINT
IS
THEN
EQUIVALENT
TO
G
A
SEPARATING
HYPERPLANE
IS
THEN
THE
PLANE
WHOSE
NORMAL
IS
THE
GRADIENT
OF
G
EVALUATED
AT
THE
CURRENT
POINT
NOTE
THAT
ONE
CAN
EASILY
DIFFERENTIATE
G
WITH
RESPECT
TO
ALL
VARIABLES
NOW
CONSIDER
THE
CASE
THAT
TI
TI
Γ
Β
MAXW
IS
GIVEN
BY
THE
WORK
DONE
BY
THE
EULER
LAGRANGE
CURVE
BETWEEN
TI
TI
AND
TI
Γ
TMAX
PLUS
THE
WORK
DONE
ON
THE
CONSTANT
TEMPERATURE
CURVE
AT
TMAX
BETWEEN
TIME
TI
Γ
AND
TIME
TI
Β
PLUS
THE
WORK
DONE
BY
THE
EULER
LAGRANGE
CURVE
FROM
TI
Β
TMAX
TO
TI
TI
USING
EQ
TO
COMPUTE
THE
WORK
DONE
BY
A
CONSTANT
TEMPERATURE
CURVE
AND
EQ
TO
COMPUTE
THE
WORK
DONE
BY
THE
TWO
EULER
LAGRANGE
CURVES
WE
HAVE
MAXW
A
Α
B
Α
Α
EXP
BΓ
Α
TI
TI
Γ
Β
F
BTMAX
Α
A
WHERE
BY
EQ
Α
EXP
BΒ
Α
AND
D
TI
EXP
BΓ
TMAX
EXP
BΓ
EXP
BΓ
Α
Α
D
TMAX
EXP
BΒ
TI
EXP
BΒ
EXP
BΒΑ
Α
WE
CAN
DETERMINE
WHETHER
THIS
MAXW
CONSTRAINT
IS
VIOLATED
IF
THIS
CONSTRAINT
IS
VIOLATED
TO
COMPUTE
A
SEPARATING
HYPERPLANE
LET
G
BE
A
FUNCTION
OF
TI
TI
AND
WI
J
FOR
J
J
I
DEFINED
AS
G
J
J
I
WI
J
MAXW
TI
TI
TI
TI
THE
I
TH
MAXW
CONSTRAINT
IS
THEN
EQUIVALENT
TO
G
A
SEPARATING
HYPERPLANE
IS
THEN
THE
PLANE
WHOSE
NORMAL
IS
THE
GRADIENT
OF
G
EVALUATED
AT
THE
CURRENT
POINT
COMPUTING
THE
GRADIENT
OF
G
THOUGH
TEDIOUS
IS
STRAIGHTFORWARD
WITH
THE
POSSIBLE
EXCEPTIONS
OF
DIFFERENTIATING
Γ
WITH
RESPECT
TO
TI
AND
DIFFERENTIATING
Β
WITH
RESPECT
TO
TI
THESE
PARTIAL
DERIVATIVES
CAN
BE
COMPUTED
USING
THE
EQS
AND
THAT
DEFINE
Γ
AND
Β
RESPECTIVELY
CONSIDER
EQ
WHERE
IS
REPLACED
BY
TI
NAMELY
TI
EXP
BΓ
Α
Α
Α
TMAX
ΑTMAX
EXP
BΓ
Α
DIFFERENTIATING
THIS
EQUATION
WITH
RESPECT
TO
TI
YIELDS
EXP
F
BΑΓ
TI
F
BΑ
EXP
F
BΑΓ
DΓ
Α
Α
Α
D
TI
ΑTMAXB
EXP
F
BΓ
DΓ
SOLVING
FOR
DΓ
I
YIELDS
Α
Α
D
TI
DΓ
D
TI
Α
EXP
BΓ
ΑB
TMAX
TI
EXP
BΓ
CONSIDER
EQ
WHERE
IS
REPLACED
BY
TI
NAMELY
TMAX
EXP
BΒΑ
Α
Α
TI
ΑTI
EXP
BΒ
Α
DIFFERENTIATING
EQ
WITH
RESPECT
TO
TI
YIELDS
TMAX
BΑ
EXP
F
BΒΑ
DΒ
Α
Α
Α
D
TI
Α
EXP
F
BΒ
ΑBTI
EXP
F
BΒ
DΒ
SOLVING
FOR
DΒ
YIELDS
D
TI
Α
Α
Α
D
TI
DΒ
Α
Α
Α
EXP
BΒ
Α
D
TI
ΑB
TMAX
EXP
ΑBΒ
Α
TI
EXP
BΒ
Α
CONCLUSION
IN
THIS
ARTICLE
WE
HAVE
INITIATED
THE
THEORETICAL
STUDY
OF
SPEED
SCALING
TO
MANAGE
TEMPERATURE
WE
ASSUMED
A
FIXED
AMBIENT
TEMPERATURE
AND
THAT
THE
DEVICE
COOLS
ACCORDING
TO
NEWTON
LAW
OF
COOLING
WE
HAVE
OBSERVED
THAT
THE
MAXIMUM
TEM
PERATURE
IS
WITHIN
A
FACTOR
OF
TWO
OF
THE
ENERGY
USED
OVER
AN
INTERVAL
WITH
LENGTH
INVERSELY
PROPORTIONAL
TO
THE
COOLING
PARAMETER
IN
NEWTON
LAW
WE
HAVE
IDENTI
FIED
THE
CONCEPT
OF
A
COOLING
OBLIVIOUS
ALGORITHM
AS
AN
ALGORITHM
THAT
IS
SIMULTA
NEOUSLY
O
COMPETITIVE
WITH
RESPECT
TO
TEMPERATURE
FOR
ALL
COOLING
PARAMETERS
AND
OBSERVED
THAT
COOLING
OBLIVIOUS
ALGORITHMS
ARE
ALSO
O
COMPETITIVE
WITH
RE
SPECT
TO
ENERGY
AND
MAXIMUM
POWER
WE
SHOWED
THAT
THE
OPTIMAL
ENERGY
SCHEDULE
YDS
IS
COOLING
OBLIVIOUS
AND
INTRODUCED
THE
FIRST
KNOWN
ONLINE
COOLING
OBLIVIOUS
ALGORITHM
BKP
FURTHER
WE
HAVE
SHOWN
THAT
BKP
IS
OPTIMALLY
COMPETITIVE
WITH
RESPECT
TO
MAXIMUM
POWER
WE
BELIEVE
THAT
SPEED
SCALING
TO
MANAGE
ENERGY
AND
TEMPERATURE
IS
AN
AREA
DESERVING
FURTHER
RESEARCH
ATTENTION
THIS
AREA
IS
BOTH
ACADEMICALLY
INTERESTING
AND
HAS
PRACTICAL
APPLICATIONS
THE
MOST
OBVIOUS
WAY
TO
PROCEED
IS
TO
CONSIDER
SPEED
SCALING
VERSIONS
OF
OTHER
SCHEDULING
PROBLEMS
WE
CAN
TAKE
ESSENTIALLY
ANY
SCHEDULING
PROBLEM
AND
CONSIDER
AN
ENERGY
MANAGEMENT
VERSION
OR
A
TEMPERATURE
MANAGEMENT
VERSION
WE
WOULD
THEN
GET
A
DUAL
CRITERIA
OPTIMIZATION
PROBLEM
IN
WHICH
THE
FIRST
OBJECTIVE
IS
THE
ORIGINAL
QUALITY
OF
SERVICE
OBJECTIVE
FOR
THE
SCHEDULING
PROBLEM
AND
THE
SECOND
OBJECTIVE
IS
EITHER
ENERGY
OR
TEMPERATURE
BY
SOLVING
MANY
SUCH
PROBLEMS
THE
HOPE
WOULD
BE
THAT
IT
WOULD
BE
POSSIBLE
TO
BUILD
UP
A
BETTER
UNDERSTANDING
OF
SPEED
SCALING
AND
TO
BUILD
AN
ALGORITHMIC
TOOLKIT
OF
ANALYSIS
AND
DESIGN
TECHNIQUES
THAT
ARE
USEFUL
FOR
SPEED
SCALING
PROBLEMS
SRCMAP
ENERGY
PROPORTIONAL
STORAGE
USING
DYNAMIC
CONSOLIDATION
AKSHAT
VERMA
RICARDO
KOLLER
LUIS
USECHE
RAJU
RANGASWAMI
IBM
RESEARCH
INDIA
FLORIDA
INTERNATIONAL
UNIVERSITY
LUIS
RAJU
CS
FIU
EDU
ABSTRACT
WE
INVESTIGATE
THE
PROBLEM
OF
CREATING
AN
ENERGY
PRO
PORTIONAL
STORAGE
SYSTEM
THROUGH
POWER
AWARE
DYNAMIC
STORAGE
CONSOLIDATION
OUR
PROPOSAL
SAMPLE
REPLICATE
CONSOLIDATE
MAPPING
SRCMAP
IS
A
STORAGE
VIRTUAL
IZATION
LAYER
OPTIMIZATION
THAT
ENABLES
ENERGY
PROPOR
TIONALITY
FOR
DYNAMIC
I
O
WORKLOADS
BY
CONSOLIDATING
THE
CUMULATIVE
WORKLOAD
ON
A
SUBSET
OF
PHYSICAL
VOL
UMES
PROPORTIONAL
TO
THE
I
O
WORKLOAD
INTENSITY
INSTEAD
OF
MIGRATING
DATA
ACROSS
PHYSICAL
VOLUMES
DYNAMICALLY
OR
REPLICATING
ENTIRE
VOLUMES
BOTH
OF
WHICH
ARE
PRO
HIBITIVELY
EXPENSIVE
SRCMAP
SAMPLES
A
SUBSET
OF
BLOCKS
FROM
EACH
DATA
VOLUME
THAT
CONSTITUTES
ITS
WORKING
SET
AND
REPLICATES
THESE
ON
OTHER
PHYSICAL
VOLUMES
DUR
ING
A
GIVEN
CONSOLIDATION
INTERVAL
SRCMAP
ACTIVATES
A
MINIMAL
SET
OF
PHYSICAL
VOLUMES
TO
SERVE
THE
WORKLOAD
AND
SPINS
DOWN
THE
REMAINING
VOLUMES
REDIRECTING
THEIR
WORKLOAD
TO
REPLICAS
ON
ACTIVE
VOLUMES
WE
PRESENT
BOTH
THEORETICAL
AND
EXPERIMENTAL
EVIDENCE
TO
ESTABLISH
THE
EFFECTIVENESS
OF
SRCMAP
IN
MINIMIZING
THE
POWER
CON
SUMPTION
OF
ENTERPRISE
STORAGE
SYSTEMS
INTRODUCTION
ENERGY
MANAGEMENT
HAS
EMERGED
AS
ONE
OF
THE
MOST
SIGNIFICANT
CHALLENGES
FACED
BY
DATA
CENTER
OPERATORS
THE
CURRENT
POWER
DENSITY
OF
DATA
CENTERS
IS
ESTIMATED
TO
BE
IN
THE
RANGE
OF
W
SQ
FT
AND
GROWING
AT
THE
RATE
OF
PER
YEAR
BARROSO
AND
HO
LZLE
HAVE
MADE
THE
CASE
FOR
ENERGY
PROPORTIONAL
COMPUTING
BASED
ON
THE
OBSERVATION
THAT
SERVERS
IN
DATA
CENTERS
TO
DAY
OPERATE
AT
WELL
BELOW
PEAK
LOAD
LEVELS
ON
AN
AVER
AGE
A
POPULAR
TECHNIQUE
FOR
DELIVERING
ENERGY
PRO
PORTIONAL
BEHAVIOR
IN
SERVERS
IS
CONSOLIDATION
USING
VIR
TUALIZATION
THESE
TECHNIQUES
A
UTILIZE
HETEROGENEITY
TO
SELECT
THE
MOST
POWER
EFFICIENT
SERVERS
AT
ANY
GIVEN
TIME
B
UTILIZE
LOW
OVERHEAD
LIVE
VIRTUAL
MACHINE
VM
MIGRATION
TO
VARY
THE
NUMBER
OF
ACTIVE
SERVERS
IN
RESPONSE
TO
WORKLOAD
VARIATION
AND
C
PRO
VIDE
FINE
GRAINED
CONTROL
OVER
POWER
CONSUMPTION
BY
AL
LOWING
THE
NUMBER
OF
ACTIVE
SERVERS
TO
BE
INCREASED
OR
DECREASED
ONE
AT
A
TIME
STORAGE
CONSUMES
ROUGHLY
OF
THE
POWER
WITHIN
COMPUTING
EQUIPMENT
AT
DATA
CENTERS
DEPENDING
ON
THE
LOAD
LEVEL
CONSUMING
A
GREATER
FRACTION
OF
THE
POWER
WHEN
SERVER
LOAD
IS
LOWER
ENERGY
PROPORTION
ALITY
FOR
THE
STORAGE
SUBSYSTEM
THUS
REPRESENTS
A
CRITICAL
GAP
IN
THE
ENERGY
EFFICIENCY
OF
FUTURE
DATA
CENTERS
IN
THIS
PAPER
WE
THE
INVESTIGATE
THE
FOLLOWING
FUNDAMENTAL
QUESTION
CAN
WE
USE
A
STORAGE
VIRTUALIZATION
LAYER
TO
DESIGN
A
PRACTICAL
ENERGY
PROPORTIONAL
STORAGE
SYSTEM
STORAGE
VIRTUALIZATION
SOLUTIONS
E
G
EMC
INVISTA
HP
SVSP
IBM
SVC
NETAPP
V
SERIES
PROVIDE
A
UNIFIED
VIEW
OF
DISPARATE
STORAGE
CONTROLLERS
THUS
SIMPLIFYING
MANAGEMENT
SIMILAR
TO
SERVER
VIR
TUALIZATION
STORAGE
VIRTUALIZATION
PROVIDES
A
TRANSPARENT
I
O
REDIRECTION
LAYER
THAT
CAN
BE
USED
TO
CONSOLIDATE
FRAG
MENTED
STORAGE
RESOURCE
UTILIZATION
SIMILAR
TO
SERVER
WORKLOADS
STORAGE
WORKLOADS
EXHIBIT
SIGNIFICANT
VARIA
TION
IN
WORKLOAD
INTENSITY
MOTIVATING
DYNAMIC
CONSOLI
DATION
HOWEVER
UNLIKE
THE
RELATIVELY
INEXPENSIVE
VM
MIGRATION
MIGRATING
A
LOGICAL
VOLUME
FROM
ONE
DE
VICE
TO
ANOTHER
CAN
BE
PROHIBITIVELY
EXPENSIVE
A
KEY
FAC
TOR
DISRUPTING
STORAGE
CONSOLIDATION
SOLUTIONS
OUR
PROPOSAL
SAMPLE
REPLICATE
CONSOLIDATE
MAP
PING
SRCMAP
IS
A
STORAGE
VIRTUALIZATION
LAYER
OP
TIMIZATION
THAT
MAKES
STORAGE
SYSTEMS
ENERGY
PROPOR
TIONAL
THE
SRCMAP
ARCHITECTURE
LEVERAGES
STORAGE
VIR
TUALIZATION
TO
REDIRECT
THE
I
O
WORKLOAD
WITHOUT
ANY
CHANGES
IN
THE
HOSTS
OR
STORAGE
CONTROLLERS
SRCMAP
TIES
TOGETHER
DISPARATE
IDEAS
FROM
SERVER
AND
STORAGE
POWER
MANAGEMENT
NAMELY
CACHING
REPLICATION
TRANSPARENT
LIVE
MIGRATION
AND
WRITE
OFF
LOADING
TO
MINIMIZE
THE
POWER
DRAWN
BY
STORAGE
DEVICES
IN
A
DATA
CENTER
IT
CON
TINUOUSLY
TARGETS
ENERGY
PROPORTIONALITY
BY
DYNAMICALLY
INCREASING
OR
DECREASING
THE
NUMBER
OF
ACTIVE
PHYSICAL
VOLUMES
IN
A
DATA
CENTER
IN
RESPONSE
TO
VARIATION
IN
I
O
WORKLOAD
INTENSITY
SRCMAP
IS
BASED
ON
THE
FOLLOWING
OBSERVATIONS
IN
PRODUCTION
WORKLOADS
DETAILED
IN
I
THE
ACTIVE
DATA
SET
IN
STORAGE
VOLUMES
IS
SMALL
II
THIS
ACTIVE
DATA
SET
IS
STABLE
AND
III
THERE
IS
SUBSTANTIAL
VARIATION
IN
WORK
LOAD
INTENSITY
BOTH
WITHIN
AND
ACROSS
STORAGE
VOLUMES
THUS
INSTEAD
OF
CREATING
FULL
REPLICAS
OF
DATA
VOLUMES
SRCMAP
CREATES
PARTIAL
REPLICAS
THAT
CONTAIN
THE
WORKING
SETS
OF
DATA
VOLUMES
THE
SMALL
REPLICA
SIZE
ALLOWS
CRE
ATING
MULTIPLE
COPIES
ON
ONE
OR
MORE
TARGET
VOLUMES
OR
ANALOGOUSLY
ALLOWING
ONE
TARGET
VOLUME
TO
HOST
REPLICAS
OF
MULTIPLE
SOURCE
VOLUMES
ADDITIONAL
SPACE
IS
RESERVED
ON
EACH
PARTIAL
REPLICA
TO
OFFLOAD
WRITES
TO
VOLUMES
THAT
ARE
SPUN
DOWN
SRCMAP
ENABLES
A
HIGH
DEGREE
OF
FLEXIBILITY
IN
SPIN
NING
DOWN
VOLUMES
BECAUSE
IT
ACTIVATES
EITHER
THE
PRI
MARY
VOLUME
OR
EXACTLY
ONE
WORKING
SET
REPLICA
OF
EACH
VOLUME
AT
ANY
TIME
BASED
ON
THE
AGGREGATE
WORKLOAD
INTENSITY
SRCMAP
CHANGES
THE
SET
OF
ACTIVE
VOLUMES
IN
THE
GRANULARITY
OF
HOURS
RATHER
THAN
MINUTES
TO
ADDRESS
THE
RELIABILITY
CONCERNS
RELATED
TO
THE
LIMITED
NUMBER
OF
DISK
SPIN
UP
CYCLES
IT
SELECTS
ACTIVE
REPLICA
TARGETS
THAT
ALLOW
SPINNING
DOWN
THE
MAXIMUM
NUMBER
OF
VOLUMES
WHILE
SERVING
THE
AGGREGATE
STORAGE
WORKLOAD
THE
VIR
TUALIZATION
LAYER
REMAPS
THE
VIRTUAL
TO
PHYSICAL
VOLUME
MAPPING
AS
REQUIRED
THEREBY
REPLACING
EXPENSIVE
DATA
MIGRATION
OPERATIONS
WITH
BACKGROUND
DATA
SYNCHRONIZA
TION
OPERATIONS
SRCMAP
IS
ABLE
TO
CREATE
CLOSE
TO
N
POWER
PERFORMANCE
LEVELS
ON
A
STORAGE
SUBSYSTEM
WITH
N
VOLUMES
ENABLING
STORAGE
ENERGY
CONSUMPTION
PRO
PORTIONAL
TO
THE
I
O
WORKLOAD
INTENSITY
IN
THE
REST
OF
THIS
PAPER
WE
PROPOSE
DESIGN
GOALS
FOR
ENERGY
PROPORTIONAL
STORAGE
SYSTEMS
AND
EXAMINE
EXIST
ING
SOLUTIONS
ANALYZE
STORAGE
WORKLOAD
CHARACTERIS
TICS
THAT
MOTIVATE
DESIGN
CHOICES
PROVIDE
DE
TAILED
SYSTEM
DESIGN
ALGORITHMS
AND
OPTIMIZATIONS
AND
AND
EVALUATE
FOR
ENERGY
PROPORTIONALITY
WE
CONCLUDE
WITH
A
FAIRLY
POSITIVE
VIEW
ON
SRCMAP
MEET
ING
ITS
ENERGY
PROPORTIONALITY
GOALS
AND
SOME
DIRECTIONS
FOR
FUTURE
WORK
ON
ENERGY
PROPORTIONAL
STORAGE
IN
THIS
SECTION
WE
IDENTIFY
THE
GOALS
FOR
A
PRACTICAL
AND
EFFECTIVE
ENERGY
PROPORTIONAL
STORAGE
SYSTEM
WE
ALSO
EXAMINE
EXISTING
WORK
ON
ENERGY
AWARE
STORAGE
AND
THE
EXTENT
TO
WHICH
THEY
DELIVER
ON
THESE
GOALS
DESIGN
GOALS
FINE
GRAINED
ENERGY
PROPORTIONALITY
ENERGY
PRO
PORTIONAL
STORAGE
SYSTEMS
ARE
UNIQUELY
CHARACTERIZED
BY
MULTIPLE
PERFORMANCE
POWER
LEVELS
TRUE
ENERGY
PROPOR
TIONALITY
REQUIRES
THAT
FOR
A
SYSTEM
WITH
A
PEAK
POWER
OF
PPEAK
FOR
A
WORKLOAD
INTENSITY
ΡMAX
THE
POWER
DRAWN
FOR
A
WORKLOAD
INTENSITY
ΡI
WOULD
BE
PPEAK
ΡI
LOW
SPACE
OVERHEAD
REPLICATION
BASED
STRATEGIES
COULD
ACHIEVE
ENERGY
PROPORTIONALITY
TRIVIALLY
BY
REPLI
CATING
EACH
VOLUME
ON
ALL
THE
OTHER
N
VOLUMES
THIS
WOULD
REQUIRE
N
COPIES
OF
EACH
VOLUME
REPRESENTING
AN
UNACCEPTABLE
SPACE
OVERHEAD
A
PRACTICAL
ENERGY
PROPOR
TABLE
COMPARISON
OF
POWER
MANAGEMENT
TECH
NIQUES
INDICATES
THE
GOAL
IS
PARTIALLY
ADDRESSED
TIONAL
SYSTEM
SHOULD
INCUR
MINIMUM
SPACE
OVERHEAD
FOR
EXAMPLE
ADDITIONAL
SPACE
IS
OFTEN
AVAILABLE
RELIABILITY
DISK
DRIVES
ARE
DESIGNED
TO
SURVIVE
A
LIM
ITED
NUMBER
OF
SPIN
UP
CYCLES
ENERGY
CONSERVATION
BASED
ON
SPINNING
DOWN
THE
DISK
MUST
ENSURE
THAT
THE
ADDITIONAL
NUMBER
OF
SPIN
UP
CYCLES
INDUCED
DURING
THE
DISKS
EXPECTED
LIFETIME
IS
SIGNIFICANTLY
LESSER
THAN
THE
MANUFACTURER
SPECIFIED
MAXIMUM
SPIN
UP
CYCLES
WORKLOAD
SHIFT
ADAPTATION
THE
POPULARITY
OF
DATA
CHANGES
EVEN
IF
SLOWLY
OVER
TIME
POWER
MANAGEMENT
FOR
STORAGE
SYSTEMS
THAT
RELY
ON
CACHING
POPULAR
DATA
OVER
LONG
INTERVALS
SHOULD
ADDRESS
ANY
SHIFT
IN
POPULAR
ITY
WHILE
ENSURING
ENERGY
PROPORTIONALITY
HETEROGENEITY
SUPPORT
A
DATA
CENTER
IS
TYPICALLY
COMPOSED
OF
SEVERAL
SUBSTANTIALLY
DIFFERENT
STORAGE
SYS
TEMS
E
G
WITH
VARIABLE
NUMBERS
AND
TYPES
OF
DRIVES
AN
IDEAL
ENERGY
PROPORTIONAL
STORAGE
SYSTEM
SHOULD
AC
COUNT
FOR
THE
DIFFERENCES
IN
THEIR
PERFORMANCE
POWER
RA
TIOS
TO
PROVIDE
THE
BEST
PERFORMANCE
AT
EACH
HOST
LEVEL
EXAMINING
EXISTING
SOLUTIONS
IT
HAS
BEEN
SHOWN
THAT
THE
IDLENESS
IN
STORAGE
WORKLOAD
IS
QUITE
LOW
FOR
TYPICAL
SERVER
WORKLOADS
WE
EX
AMINE
SEVERAL
CLASSES
OF
RELATED
WORK
THAT
REPRESENT
AP
PROACHES
TO
INCREASE
THIS
IDLENESS
FOR
POWER
MINIMIZATION
AND
EVALUATE
THE
EXTENT
TO
WHICH
THEY
ADDRESS
OUR
DESIGN
GOALS
WE
NEXT
DISCUSS
EACH
OF
THEM
AND
SUMMARIZE
THEIR
RELATIVE
STRENGTHS
IN
TABLE
SINGLY
REDUNDANT
SCHEMES
THE
CENTRAL
IDEA
USED
BY
THESE
SCHEMES
IS
SPINNING
DOWN
DISKS
WITH
REDUNDANT
DATA
DURING
PERIODS
OF
LOW
I
O
LOAD
RI
MAC
USES
MEMORY
LEVEL
AND
ON
DISK
REDUNDANCY
TO
REDUCE
PASSIVE
SPIN
UPS
IN
SYSTEMS
ENABLING
THE
SPINNING
DOWN
OF
ONE
OUT
OF
THE
N
DISKS
IN
THE
ARRAY
THE
DIVERTED
ACCESSES
TECHNIQUE
GENERALIZES
THIS
APPROACH
TO
FIND
THE
BEST
REDUNDANCY
CONFIGURATION
FOR
ENERGY
PERFORMANCE
AND
RELIABILITY
FOR
ALL
RAID
LEVELS
GREENAN
ET
AL
PROPOSE
GENERIC
TECHNIQUES
FOR
MANAG
ING
POWER
AWARE
ERASURE
CODED
STORAGE
SYSTEMS
THE
ABOVE
TECHNIQUES
AIM
TO
SUPPORT
TWO
ENERGY
LEVELS
AND
DO
NOT
ADDRESS
FINE
GRAINED
ENERGY
PROPORTIONALITY
GEARED
RAIDS
PARAID
IS
A
GEAR
SHIFTING
MECH
ANISM
EACH
DISK
SPUN
DOWN
REPRESENTS
A
GEAR
SHIFT
FOR
A
PARITY
BASED
RAID
TO
IMPLEMENT
N
GEARS
IN
A
N
DISK
ARRAY
WITH
USED
STORAGE
X
PARAID
REQUIRES
O
X
LOG
N
SPACE
EVEN
IF
WE
IGNORE
THE
SPACE
REQUIRED
FOR
STORING
PARITY
INFORMATION
DISKGROUP
IS
A
MOD
IFICATION
OF
RAID
THAT
ENABLES
A
SUBSET
OF
THE
DISKS
IN
A
MIRROR
GROUP
TO
BE
ACTIVATED
AS
NECESSARY
BOTH
TECHNIQUES
INCUR
LARGE
SPACE
OVERHEAD
FURTHER
THEY
DO
NOT
ADDRESS
HETEROGENEOUS
STORAGE
SYSTEMS
COMPOSED
OF
MULTIPLE
VOLUMES
WITH
VARYING
I
O
WORKLOAD
INTENSITIES
CACHING
SYSTEMS
THIS
CLASS
OF
WORK
IS
MOSTLY
BASED
ON
CACHING
POPULAR
DATA
ON
ADDITIONAL
STORAGE
TO
SPIN
DOWN
PRIMARY
DATA
DRIVES
MAID
AN
ARCHIVAL
STORAGE
SYSTEM
OPTIONALLY
USES
ADDITIONAL
CACHE
DISKS
FOR
REPLICATING
POPULAR
DATA
TO
INCREASE
IDLE
PERIODS
ON
THE
REMAINING
DISKS
PDC
DOES
NOT
USE
ADDITIONAL
DISKS
BUT
RATHER
SUGGESTS
MIGRATING
DATA
BETWEEN
DISKS
ACCORD
ING
TO
POPULARITY
ALWAYS
KEEPING
THE
MOST
POPULAR
DATA
ON
A
FEW
ACTIVE
DISKS
EXCES
USES
A
LOW
END
FLASH
DEVICE
FOR
CACHING
POPULAR
DATA
AND
BUFFERING
WRITES
TO
INCREASE
IDLE
PERIODS
OF
DISK
DRIVES
LEE
ET
AL
SUG
GEST
AUGMENTING
RAID
SYSTEMS
WITH
AN
SSD
FOR
A
SIMI
LAR
PURPOSE
A
DEDICATED
STORAGE
CACHE
DOES
NOT
PROVIDE
FINE
GRAINED
ENERGY
PROPORTIONALITY
THE
STORAGE
SYSTEM
IS
ABLE
TO
SAVE
ENERGY
ONLY
WHEN
THE
I
O
LOAD
IS
LOW
AND
CAN
BE
SERVED
FROM
THE
CACHE
FURTHER
THESE
TECHNIQUES
DO
NOT
ACCOUNT
FOR
THE
RELIABILITY
IMPACT
OF
FREQUENT
DISK
SPIN
UP
OPERATIONS
WRITE
OFFLOADING
WRITE
OFF
LOADING
IS
AN
ENERGY
SAV
ING
TECHNIQUE
BASED
ON
REDIRECTING
WRITES
TO
ALTERNATE
LOCATIONS
THE
AUTHORS
OF
WRITE
OFFLOADING
DEMONSTRATE
THAT
IDLE
PERIODS
AT
A
ONE
MINUTE
GRANULARITY
CAN
BE
SIG
NIFICANTLY
INCREASED
BY
OFF
LOADING
WRITES
TO
A
DIFFERENT
VOLUME
THE
RELIABILITY
IMPACT
DUE
TO
FREQUENT
SPIN
UP
CYCLES
ON
A
DISK
IS
A
POTENTIAL
CONCERN
WHICH
THE
AU
THORS
ACKNOWLEDGE
BUT
LEAVE
AS
AN
OPEN
PROBLEM
IN
CON
TRAST
SRCMAP
INCREASES
THE
IDLE
PERIODS
SUBSTANTIALLY
BY
OFF
LOADING
POPULAR
DATA
READS
IN
ADDITION
TO
THE
WRITES
AND
THUS
MORE
COMPREHENSIVELY
ADDRESSING
THIS
IMPOR
TANT
CONCERN
ANOTHER
IMPORTANT
QUESTION
NOT
ADDRESSED
IN
THE
WRITE
OFF
LOADING
WORK
IS
WITH
MULTIPLE
VOLUMES
WHICH
ACTIVE
VOLUME
SHOULD
BE
TREATED
AS
A
WRITE
OFF
LOADING
TARGET
FOR
EACH
SPUN
DOWN
VOLUME
SRCMAP
ADDRESSES
THIS
QUESTION
CLEARLY
WITH
A
FORMAL
PROCESS
FOR
IDENTIFYING
THE
SET
OF
ACTIVE
DISKS
DURING
EACH
INTERVAL
OTHER
TECHNIQUES
THERE
ARE
ORTHOGONAL
CLASSES
OF
WORK
THAT
CAN
EITHER
BE
USED
IN
CONJUNCTION
WITH
SR
CMAP
OR
THAT
ADDRESS
OTHER
TARGET
ENVIRONMENTS
HIBER
NATOR
USES
DRPM
TO
CREATE
A
MULTI
TIER
HIERAR
CHY
OF
FUTURISTIC
MULTI
SPEED
DISKS
THE
SPEED
FOR
EACH
DISK
IS
SET
AND
DATA
MIGRATED
ACROSS
TIERS
AS
THE
WORKLOAD
CHANGES
PERGAMUM
IS
AN
ARCHIVAL
STORAGE
SYSTEM
DE
SIGNED
TO
BE
ENERGY
EFFICIENT
WITH
TECHNIQUES
FOR
REDUC
ING
INTER
DISK
DEPENDENCIES
AND
STAGGERING
REBUILD
OPER
ATIONS
GURUMURTHI
ET
AL
PROPOSE
INTRA
DISK
PAR
ALLELISM
ON
HIGH
CAPACITY
DRIVES
TO
IMPROVE
DISK
BAND
TABLE
SUMMARY
STATISTICS
OF
ONE
WEEK
I
O
WORK
LOAD
TRACES
OBTAINED
FROM
THREE
DIFFERENT
VOLUMES
MAIL
WEB
VM
HOMES
HOUR
FIGURE
VARIABILITY
IN
I
O
WORKLOAD
INTENSITY
WIDTH
WITHOUT
INCREASING
POWER
CONSUMPTION
FI
NALLY
GANESH
ET
AL
PROPOSE
LOG
STRUCTURED
STRIPED
WRIT
ING
ON
A
DISK
ARRAY
TO
INCREASE
THE
PREDICTABILITY
OF
AC
TIVE
INACTIVE
SPINDLES
STORAGE
WORKLOAD
CHARACTERISTICS
IN
THIS
SECTION
WE
CHARACTERIZE
THE
NATURE
OF
I
O
ACCESS
ON
SERVERS
USING
WORKLOADS
FROM
THREE
PRODUCTION
SYS
TEMS
SPECIFICALLY
LOOKING
FOR
PROPERTIES
THAT
HELP
US
IN
OUR
GOAL
OF
ENERGY
PROPORTIONAL
STORAGE
THE
SYSTEMS
IN
CLUDE
AN
EMAIL
SERVER
MAIL
WORKLOAD
A
VIRTUAL
MACHINE
MONITOR
RUNNING
TWO
WEB
SERVERS
WEB
VM
WORKLOAD
AND
A
FILE
SERVER
HOMES
WORKLOAD
THE
MAIL
WORKLOAD
SERVES
USER
INBOXES
FOR
THE
ENTIRE
COMPUTER
SCIENCE
DEPARTMENT
AT
FIU
THE
HOMES
WORKLOAD
IS
THAT
OF
A
NFS
SERVER
THAT
SERVES
THE
HOME
DIRECTORIES
FOR
OUR
RE
SEARCH
GROUP
AT
FIU
ACTIVITIES
REPRESENT
THOSE
OF
A
TYPICAL
RESEARCHER
CONSISTING
OF
SOFTWARE
DEVELOPMENT
TESTING
AND
EXPERIMENTATION
THE
USE
OF
GRAPH
PLOTTING
SOFTWARE
AND
TECHNICAL
DOCUMENT
PREPARATION
FINALLY
THE
WEB
VM
WORKLOAD
IS
COLLECTED
FROM
A
VIRTUALIZED
SYSTEM
THAT
HOSTS
TWO
CS
DEPARTMENT
WEB
SERVERS
ONE
HOSTING
THE
DEPART
MENT
ONLINE
COURSE
MANAGEMENT
SYSTEM
AND
THE
OTHER
HOSTING
THE
DEPARTMENT
WEB
BASED
EMAIL
ACCESS
PORTAL
IN
EACH
SYSTEM
WE
COLLECTED
I
O
TRACES
DOWNSTREAM
OF
AN
ACTIVE
PAGE
CACHE
FOR
A
DURATION
OF
THREE
WEEKS
AVERAGE
WEEKLY
STATISTICS
RELATED
TO
THESE
WORKLOADS
ARE
SUMMARIZED
IN
TABLE
THE
FIRST
THING
TO
NOTE
IS
THAT
THE
WEEKLY
WORKING
SETS
UNIQUE
ACCESSES
DURING
A
WEEK
IS
A
SMALL
PERCENTAGE
OF
THE
TOTAL
VOLUME
SIZE
THIS
TREND
IS
CONSISTENT
ACROSS
ALL
VOLUMES
AND
LEADS
TO
OUR
FIRST
OBSERVATION
OBSERVATION
THE
ACTIVE
DATA
SET
FOR
STORAGE
VOLUMES
IS
TYPICALLY
A
SMALL
FRACTION
OF
TOTAL
USED
STORAGE
DYNAMIC
CONSOLIDATION
UTILIZES
VARIABILITY
IN
I
O
WORKLOAD
INTENSITY
TO
INCREASE
OR
DECREASE
THE
NUMBER
OF
I
M
II
M
III
M
I
H
II
H
III
H
I
W
II
W
III
W
DAYS
DAYS
DAYS
DAYS
DAYS
DAYS
FIGURE
OVERLAP
IN
DAILY
WORKING
SETS
FOR
THE
MAIL
M
HOMES
H
AND
WEB
VM
W
WORKLOADS
I
READS
AND
WRITES
AGAINST
WORKING
SET
II
READS
AGAINST
WORK
ING
SET
AND
III
READS
AGAINST
WORKING
SET
RECENTLY
OF
FLOADED
WRITES
AND
RECENT
MISSED
READS
ACTIVE
DEVICES
FIGURE
DEPICTS
LARGE
VARIABILITY
IN
I
O
WORKLOAD
INTENSITY
FOR
EACH
OF
THE
THREE
WORKLOADS
OVER
TIME
WITH
AS
MUCH
AS
ORDERS
OF
MAGNITUDE
BETWEEN
THE
LOWEST
AND
HIGHEST
WORKLOAD
INTENSITY
LEVELS
ACROSS
TIME
THIS
HIGHLIGHTS
THE
POTENTIAL
OF
ENERGY
SAVINGS
IF
THE
STORAGE
SYSTEMS
CAN
BE
MADE
ENERGY
PROPORTIONAL
OBSERVATION
THERE
IS
A
SIGNIFICANT
VARIABILITY
IN
I
O
WORKLOAD
INTENSITY
ON
STORAGE
VOLUMES
BASED
ON
OUR
FIRST
TWO
OBSERVATIONS
WE
HYPOTHE
SIZE
THAT
THERE
IS
ROOM
FOR
POWERING
DOWN
PHYSICAL
VOL
UMES
THAT
ARE
SUBSTANTIALLY
UNDER
UTILIZED
BY
REPLICATING
A
SMALL
ACTIVE
WORKING
SET
ON
OTHER
VOLUMES
WHICH
HAVE
THE
SPARE
BANDWIDTH
TO
SERVE
ACCESSES
TO
THE
POWERED
DOWN
VOLUMES
THIS
MOTIVATES
SAMPLE
AND
REPLICATE
IN
SRCMAP
ENERGY
CONSERVATION
IS
POSSIBLE
PROVIDED
THE
CORRESPONDING
WORKING
SET
REPLICAS
CAN
SERVE
MOST
RE
QUESTS
TO
EACH
POWERED
DOWN
VOLUME
THIS
WOULD
BE
TRUE
IF
WORKING
SETS
ARE
LARGELY
STABLE
WE
INVESTIGATE
THE
STABILITY
OF
THE
VOLUME
WORKING
SETS
IN
FIG
FOR
THREE
PROGRESSIVE
DEFINITIONS
OF
THE
WORKING
SET
IN
THE
FIRST
SCENARIO
WE
COMPUTE
THE
CLASSICAL
WORK
ING
SET
BASED
ON
THE
LAST
FEW
DAYS
OF
ACCESS
HISTORY
IN
THE
SECOND
SCENARIO
WE
ADDITIONALLY
ASSUME
THAT
WRITES
CAN
BE
OFFLOADED
AND
MARK
ALL
WRITES
AS
HITS
IN
THE
THIRD
SCENARIO
WE
FURTHER
EXPAND
THE
WORKING
SET
TO
INCLUDE
RE
CENT
WRITES
AND
PAST
MISSED
READS
FOR
EACH
SCENARIO
WE
COMPUTE
THE
WORKING
SET
HITS
AND
MISSES
FOR
THE
FOLLOW
ING
DAY
WORKLOAD
AND
STUDY
THE
HIT
RATIO
WITH
CHANGE
IN
THE
LENGTH
OF
HISTORY
USED
TO
COMPUTE
THE
WORKING
SET
WE
OBSERVE
THAT
THE
HIT
RATIO
PROGRESSIVELY
INCREASES
BOTH
ACROSS
THE
SCENARIOS
AND
AS
WE
INCREASE
THE
HISTORY
LENGTH
LEADING
US
TO
CONCLUDE
THAT
DATA
USAGE
EXHIBITS
HIGH
TEM
PORAL
LOCALITY
AND
THAT
THE
WORKING
SET
AFTER
INCLUDING
RE
CENT
ACCESSES
IS
FAIRLY
STABLE
THIS
LEADS
TO
OUR
THIRD
OB
SERVATION
ALSO
OBSERVED
EARLIER
BY
LEUNG
ET
AL
OBSERVATION
DATA
USAGE
IS
HIGHLY
SKEWED
WITH
MORE
THAN
OF
THE
WORKING
SET
CONSISTING
OF
SOME
REALLY
POPULAR
DATA
AND
RECENTLY
ACCESSED
DATA
THE
FIRST
THREE
OBSERVATIONS
ARE
THE
PILLARS
BEHIND
THE
SAMPLE
REPLICATE
AND
CONSOLIDATE
APPROACH
WHEREBY
WE
SAMPLE
EACH
VOLUME
FOR
ITS
WORKING
SET
REPLICATE
INTERVAL
LENGTH
FIGURE
DISTRIBUTION
OF
READ
IDLE
TIMES
THESE
WORKING
SETS
ON
OTHER
VOLUMES
AND
CONSOLIDATE
I
O
WORKLOADS
ON
PROPORTIONATELY
FEWER
VOLUMES
DUR
ING
PERIODS
OF
LOW
LOAD
BEFORE
DESIGNING
A
NEW
SYSTEM
BASED
ON
THE
ABOVE
OBSERVATIONS
WE
STUDY
THE
SUITABIL
ITY
OF
A
SIMPLER
WRITE
OFFLOADING
TECHNIQUE
FOR
BUILDING
ENERGY
PROPORTIONAL
STORAGE
SYSTEMS
WRITE
OFF
LOADING
IS
BASED
ON
THE
OBSERVATION
THAT
I
O
WORKLOADS
ARE
WRITE
DOMINATED
AND
SIMPLY
OFF
LOADING
WRITES
TO
A
DIFFERENT
VOLUME
CAN
CAUSE
VOLUMES
TO
BE
IDLE
FOR
A
SUBSTANTIAL
FRACTION
FOR
WORKLOADS
IN
THE
ORIGINAL
STUDY
OF
TIME
WHILE
WRITE
OFF
LOADING
INCREASES
THE
FRACTION
OF
IDLE
TIME
OF
VOLUMES
THE
DISTRIBUTION
OF
IDLE
TIME
DU
RATIONS
DUE
TO
WRITE
OFF
LOADING
RAISES
AN
ORTHOGONAL
BUT
IMPORTANT
CONCERN
IF
THESE
IDLE
TIME
DURATIONS
ARE
SHORT
SAVING
POWER
REQUIRES
FREQUENT
SPINNING
DOWN
UP
OF
THE
VOLUMES
WHICH
DEGRADES
RELIABILITY
OF
THE
DISK
DRIVES
FIGURE
DEPICTS
THE
READ
IDLE
TIME
DISTRIBUTIONS
OF
THE
THREE
WORKLOADS
IT
IS
INTERESTING
TO
NOTE
THAT
IDLE
TIME
DURATIONS
FOR
THE
HOMES
AND
MAIL
WORKLOADS
ARE
ALL
LESS
THAN
OR
EQUAL
TO
MINUTES
AND
FOR
THE
WEB
VM
THE
MA
JORITY
ARE
LESS
THAN
OR
EQUAL
TO
MINUTES
ARE
ALL
ARE
LESS
THAN
MINUTES
OBSERVATION
THE
READ
IDLE
TIME
DISTRIBUTION
PERIODS
OF
WRITES
ALONE
WITH
NO
INTERVENING
READ
OPERATIONS
OF
I
O
WORKLOADS
IS
DOMINATED
BY
SMALL
DURATIONS
TYPICALLY
LESS
THAN
FIVE
MINUTES
THIS
OBSERVATION
IMPLIES
THAT
EXPLOITING
ALL
READ
IDLENESS
FOR
SAVING
POWER
WILL
NECESSITATE
SPINNING
UP
THE
DISK
AT
LEAST
TIMES
A
DAY
IN
THE
CASE
OF
HOMES
AND
MAIL
AND
AT
LEAST
TIMES
IN
THE
CASE
OF
WEB
VM
THIS
CAN
BE
A
SIGNIFICANT
HURDLE
TO
RELIABILITY
OF
THE
DISK
DRIVES
WHICH
TYPICALLY
HAVE
LIMITED
SPIN
UP
CYCLES
IT
IS
THEREFORE
IMPORTANT
TO
DEVELOP
NEW
TECHNIQUES
THAT
CAN
SUBSTANTIALLY
INCREASE
AVERAGE
READ
IDLE
TIME
DURATIONS
BACKGROUND
AND
RATIONALE
STORAGE
VIRTUALIZATION
MANAGERS
SIMPLIFY
STORAGE
MAN
AGEMENT
BY
ENABLING
A
UNIFORM
VIEW
OF
DISPARATE
STOR
AGE
RESOURCES
IN
A
DATA
CENTER
THEY
EXPORT
A
STORAGE
CONTROLLER
INTERFACE
ALLOWING
USERS
TO
CREATE
LOGICAL
VOL
UMES
OR
VIRTUAL
DISKS
VDISKS
AND
MOUNT
THESE
ON
HOSTS
THE
PHYSICAL
VOLUMES
MANAGED
BY
THE
PHYSICAL
STORAGE
CONTROLLERS
ARE
AVAILABLE
TO
THE
VIRTUALIZATION
MANAGER
AS
MANAGED
DISKS
MDISKS
ENTIRELY
TRANSPARENTLY
TO
THE
HOSTS
WHICH
ONLY
VIEW
THE
LOGICAL
VDISK
VOLUMES
A
USE
FUL
PROPERTY
OF
THE
VIRTUALIZATION
LAYER
IS
THE
COMPLETE
FLEXIBILITY
IN
ALLOCATION
OF
MDISK
EXTENTS
TO
VDISKS
APPLYING
SERVER
CONSOLIDATION
PRINCIPLES
TO
STORAGE
CONSOLIDATION
USING
VIRTUALIZATION
WOULD
ACTIVATE
ONLY
THE
MOST
ENERGY
EFFICIENT
MDISKS
REQUIRED
TO
SERVE
THE
AG
GREGATE
WORKLOAD
DURING
ANY
PERIOD
T
DATA
FROM
THE
OTHER
MDISKS
CHOSEN
TO
BE
SPUN
DOWN
WOULD
FIRST
NEED
TO
BE
MIGRATED
TO
ACTIVE
MDISKS
TO
EFFECT
THE
CHANGE
WHILE
DATA
MIGRATION
IS
AN
EXPENSIVE
OPERATION
THE
EASE
WITH
WHICH
VIRTUAL
TO
PHYSICAL
MAPPINGS
CAN
BE
RECONFIGURED
PROVIDES
AN
ALTERNATIVE
APPROACH
A
NA
IVE
STRATEGY
FOL
LOWING
THIS
APPROACH
COULD
REPLICATE
DATA
FOR
EACH
VDISK
ON
ALL
THE
MDISKS
AND
ADAPT
TO
WORKLOAD
VARIATIONS
BY
DYNAMICALLY
CHANGING
THE
VIRTUAL
TO
PHYSICAL
MAPPINGS
TO
USE
ONLY
THE
SELECTED
MDISKS
DURING
T
UNFORTUNATELY
THIS
STRATEGY
REQUIRES
N
TIMES
ADDITIONAL
SPACE
FOR
A
N
VDISK
STORAGE
SYSTEM
AN
UNACCEPTABLE
SPACE
OVERHEAD
SRCMAP
INTELLIGENTLY
USES
THE
STORAGE
VIRTUALIZATION
LAYER
AS
AN
I
O
INDIRECTION
MECHANISM
TO
DELIVER
A
PRACTI
CALLY
FEASIBLE
ENERGY
PROPORTIONAL
SOLUTION
SINCE
IT
OP
ERATES
AT
THE
STORAGE
VIRTUALIZATION
MANAGER
IT
DOES
NOT
ALTER
THE
BASIC
REDUNDANCY
BASED
RELIABILITY
PROPERTIES
OF
THE
UNDERLYING
PHYSICAL
VOLUMES
WHICH
IS
DETERMINED
BY
THE
RESPECTIVE
PHYSICAL
VOLUME
E
G
RAID
CONTROLLERS
TO
MAINTAIN
THE
REDUNDANCY
LEVEL
SRCMAP
ENSURES
THAT
A
VOLUME
IS
REPLICATED
ON
TARGET
VOLUMES
AT
THE
SAME
RAID
LEVEL
WHILE
WE
DETAIL
SRCMAP
DESIGN
AND
AL
GORITHMS
IN
SUBSEQUENT
SECTIONS
AND
HERE
WE
LIST
THE
RATIONALE
BEHIND
SRCMAP
DESIGN
DECISIONS
THESE
DESIGN
DECISIONS
TOGETHER
HELP
TO
SATISFY
THE
DESIGN
GOALS
FOR
AN
IDEAL
ENERGY
PROPORTIONAL
STORAGE
SYSTEM
MULTIPLE
REPLICA
TARGETS
FINE
GRAINED
ENERGY
PROPOR
TIONALITY
REQUIRES
THE
FLEXIBILITY
TO
INCREASE
OR
DECREASE
THE
NUMBER
OF
ACTIVE
PHYSICAL
VOLUMES
ONE
AT
A
TIME
TECHNIQUES
THAT
ACTIVATE
A
FIXED
SECONDARY
DEVICE
FOR
EACH
DATA
VOLUME
DURING
PERIODS
OF
LOW
ACTIVITY
CANNOT
PROVIDE
THE
FLEXIBILITY
NECESSARY
TO
DEACTIVATE
AN
ARBI
TRARY
FRACTION
OF
THE
PHYSICAL
VOLUMES
IN
SRCMAP
WE
ACHIEVE
THIS
FINE
GRAINED
CONTROL
BY
CREATING
A
PRIMARY
MDISK
FOR
EACH
VDISK
AND
REPLICATING
ONLY
THE
WORKING
SET
OF
EACH
VDISK
ON
MULTIPLE
SECONDARY
MDISKS
THIS
ENSURES
THAT
A
EVERY
VOLUME
CAN
BE
OFFLOADED
TO
ONE
OF
MULTIPLE
TARGETS
AND
B
EACH
TARGET
CAN
SERVE
THE
I
O
WORKLOAD
FOR
MULTIPLE
VDISKS
DURING
PEAK
LOAD
EACH
VDISK
MAPS
TO
ITS
PRIMARY
MDISK
AND
ALL
MDISKS
ARE
AC
TIVE
HOWEVER
DURING
PERIODS
OF
LOW
ACTIVITY
SRCMAP
SELECTS
A
PROPORTIONATELY
SMALL
SUBSET
OF
MDISKS
THAT
CAN
SUPPORT
THE
AGGREGATE
I
O
WORKLOAD
FOR
ALL
VDISKS
SAMPLING
CREATING
MULTIPLE
FULL
REPLICAS
OF
VDISKS
IS
IMPRACTICAL
DRAWING
FROM
OBSERVATION
SR
CMAP
SUBSTANTIALLY
REDUCES
THE
SPACE
OVERHEAD
OF
MAIN
TAINING
MULTIPLE
REPLICAS
BY
SAMPLING
ONLY
THE
WORKING
SET
FOR
EACH
VDISK
AND
REPLICATING
IT
SINCE
THE
WORKING
SET
IS
TYPICALLY
SMALL
THE
SPACE
OVERHEAD
IS
LOW
ORDERED
REPLICA
PLACEMENT
WHILE
SAMPLING
HELPS
TO
REDUCE
REPLICA
SIZES
SUBSTANTIALLY
CREATING
MULTIPLE
REPLICAS
FOR
EACH
SAMPLE
STILL
INDUCES
SPACE
OVERHEAD
IN
SRCMAP
WE
OBSERVE
THAT
ALL
REPLICAS
ARE
NOT
CREATED
EQUAL
FOR
INSTANCE
IT
IS
MORE
BENEFICIAL
TO
REPLICATE
A
LIGHTLY
LOADED
VOLUME
THAN
A
HEAVILY
LOADED
ONE
WHICH
IS
LIKELY
TO
BE
ACTIVE
ANYWAY
SIMILARLY
A
LARGE
WORKING
SET
HAS
GREATER
SPACE
OVERHEAD
SRCMAP
CHOOSES
TO
CREATE
FEWER
REPLICAS
AIMING
TO
KEEP
IT
ACTIVE
IF
POSSIBLE
AS
WE
SHALL
FORMALLY
DEMONSTRATE
CAREFULLY
ORDERING
THE
REPLICA
PLACEMENT
HELPS
TO
MINIMIZE
THE
NUMBER
OF
ACTIVE
DISKS
FOR
FINE
GRAINED
ENERGY
PROPORTIONALITY
DYNAMIC
SOURCE
TO
TARGET
MAPPING
AND
DUAL
DATA
SYNCHRONIZATION
FROM
OBSERVATION
WE
KNOW
THAT
WORKLOADS
CAN
VARY
SUBSTANTIALLY
OVER
A
PERIOD
OF
TIME
HENCE
IT
IS
NOT
POSSIBLE
TO
PRE
DETERMINE
WHICH
VOLUMES
NEED
TO
BE
ACTIVE
TARGET
REPLICA
SELECTION
FOR
ANY
VOLUME
BEING
POWERED
DOWN
THEREFORE
NEEDS
TO
BE
A
DYNAMIC
DECISION
AND
ALSO
NEEDS
TO
TAKE
INTO
ACCOUNT
THAT
SOME
VOLUMES
HAVE
MORE
REPLICAS
OR
TARGET
CHOICES
THAN
OTHERS
WE
USE
TWO
DISTINCT
MECHANISMS
FOR
UPDAT
ING
THE
REPLICA
WORKING
SETS
THE
ACTIVE
REPLICA
LIES
IN
THE
DATA
PATH
AND
IS
IMMEDIATELY
SYNCHRONIZED
IN
THE
CASE
OF
A
READ
MISS
THIS
ENSURES
THAT
THE
ACTIVE
REPLICA
CONTIN
UOUSLY
ADAPTS
WITH
CHANGE
IN
WORKLOAD
POPULARITY
THE
SECONDARY
REPLICAS
ON
THE
OTHER
HAND
USE
A
LAZY
INCRE
MENTAL
DATA
SYNCHRONIZATION
IN
THE
BACKGROUND
BETWEEN
THE
PRIMARY
REPLICA
AND
ANY
SECONDARY
REPLICAS
PRESENT
ON
ACTIVE
MDISKS
THIS
ENSURES
THAT
SWITCHING
BETWEEN
REPLICAS
REQUIRES
MINIMAL
DATA
COPYING
AND
CAN
BE
PER
FORMED
FAIRLY
QUICKLY
COARSE
GRAINED
POWER
CYCLING
IN
CONTRAST
TO
MOST
EXISTING
SOLUTIONS
THAT
RELY
ON
FINE
GRAINED
DISK
POWER
MODE
SWITCHING
SRCMAP
IMPLEMENTS
COARSE
GRAINED
CONSOLIDATION
INTERVALS
OF
THE
ORDER
OF
HOURS
DURING
EACH
OF
WHICH
THE
SET
OF
ACTIVE
MDISKS
CHOSEN
BY
SR
CMAP
DOES
NOT
CHANGE
THIS
ENSURES
NORMAL
DISK
LIFE
TIMES
ARE
REALIZED
BY
ADHERING
TO
THE
DISK
POWER
CYCLE
SPECIFICATION
CONTAINED
WITHIN
MANUFACTURER
DATA
SHEETS
DESIGN
OVERVIEW
SRCMAP
IS
BUILT
IN
A
MODULAR
FASHION
TO
DIRECTLY
INTER
FACE
WITH
STORAGE
VIRTUALIZATION
MANAGERS
OR
BE
INTEGRATED
INTO
ONE
AS
SHOWN
IN
FIGURE
THE
OVERALL
ARCHITECTURE
SUPPORTS
THE
FOLLOWING
DISTINCT
FLOWS
OF
CONTROL
THE
REPLICA
GENERATION
FLOW
FLOW
A
IDENTIFIES
THE
WORKING
SET
FOR
EACH
VDISK
AND
REPLICATES
IT
ON
MULTIPLE
MDISKS
THIS
FLOW
IS
ORCHESTRATED
BY
THE
REPLICA
PLACE
MENT
CONTROLLER
AND
IS
TRIGGERED
ONCE
WHEN
SRCMAP
INITIALIZATION
RECONFIGURATION
TIME
TRIGGER
LOAD
MONITOR
THE
LOAD
MONITOR
RESIDES
IN
THE
STORAGE
VIRTUALIZATION
MANAGER
AND
RECORDS
ACCESS
TO
DATA
ON
ANY
OF
THE
VDISKS
EXPORTED
BY
THE
VIRTUALIZATION
LAYER
IT
PROVIDES
TWO
INTER
FACES
FOR
USE
BY
SRCMAP
LONG
TERM
WORKLOAD
DATA
IN
TERFACE
INVOKED
BY
THE
REPLICA
PLACEMENT
CONTROLLER
AND
PREDICTED
SHORT
TERM
WORKLOAD
DATA
INTERFACE
INVOKED
BY
THE
ACTIVE
DISK
MANAGER
FIGURE
SRCMAP
INTEGRATED
INTO
A
STORAGE
VIR
TUALIZATION
MANAGER
ARROWS
DEPICT
CONTROL
FLOW
DASHED
SOLID
BOXES
DENOTE
EXISTING
NEW
COMPONENTS
IS
INITIALIZED
AND
WHENEVER
A
CONFIGURATION
CHANGE
E
G
ADDITION
OF
A
NEW
WORKLOAD
OR
NEW
DISKS
TAKES
PLACE
ONCE
A
TRIGGER
IS
GENERATED
THE
REPLICA
PLACEMENT
CON
TROLLER
OBTAINS
A
HISTORICAL
WORKLOAD
TRACE
FROM
THE
LOAD
MONITOR
AND
COMPUTES
THE
WORKING
SET
AND
THE
LONG
TERM
WORKLOAD
INTENSITY
FOR
EACH
VOLUME
VDISK
THE
WORK
ING
SET
IS
THEN
REPLICATED
ON
ONE
OR
MORE
PHYSICAL
VOL
UMES
MDISKS
THE
BLOCKS
THAT
CONSTITUTE
THE
WORKING
SET
FOR
THE
VDISK
AND
THE
TARGET
PHYSICAL
VOLUMES
WHERE
THESE
ARE
REPLICATED
ARE
MANAGED
USING
A
COMMON
DATA
STRUCTURE
CALLED
THE
REPLICA
DISK
MAP
RDM
THE
ACTIVE
DISK
IDENTIFICATION
FLOW
FLOW
B
IDENTIFIES
FOR
A
PERIOD
T
THE
ACTIVE
MDISKS
AND
ACTIVATED
REPLI
CAS
FOR
EACH
INACTIVE
MDISK
THE
FLOW
IS
TRIGGERED
AT
THE
BEGINNING
OF
THE
CONSOLIDATION
INTERVAL
T
E
G
EVERY
HOURS
AND
ORCHESTRATED
BY
THE
ACTIVE
DISK
MANAGER
IN
THIS
FLOW
THE
ACTIVE
DISK
MANAGER
QUERIES
THE
LOAD
MONITOR
FOR
EXPECTED
WORKLOAD
INTENSITY
OF
EACH
VDISK
IN
THE
PERIOD
T
IT
THEN
USES
THE
WORKLOAD
INFORMATION
ALONG
WITH
THE
PLACEMENT
OF
WORKING
SET
REPLICAS
ON
TAR
GET
MDISKS
TO
COMPUTE
THE
SET
OF
ACTIVE
PRIMARY
MDISKS
AND
A
ACTIVE
SECONDARY
REPLICA
MDISK
FOR
EACH
INACTIVE
PRIMARY
MDISK
IT
THEN
DIRECTS
THE
CONSISTENCY
MANAGER
TO
ENSURE
THAT
THE
DATA
ON
ANY
SELECTED
ACTIVE
PRIMARY
OR
ACTIVE
SECONDARY
REPLICA
IS
CURRENT
ONCE
CONSISTENCY
CHECKS
ARE
MADE
IT
UPDATES
THE
VIRTUAL
TO
PHYSICAL
MAP
PING
TO
REDIRECT
THE
WORKLOAD
TO
THE
APPROPRIATE
MDISK
THE
I
O
REDIRECTION
FLOW
FLOW
C
IS
AN
EXTENSION
OF
THE
I
O
PROCESSING
IN
THE
STORAGE
VIRTUALIZATION
MANAGER
AND
UTILIZES
THE
BUILT
IN
VIRTUAL
TO
PHYSICAL
RE
MAPPING
SUPPORT
TO
DIRECT
REQUESTS
TO
PRIMARIES
OR
ACTIVE
REPLI
CAS
FURTHER
THIS
FLOW
ENSURES
THAT
THE
WORKING
SET
OF
EACH
VDISK
IS
KEPT
UP
TO
DATE
TO
ENSURE
THIS
WHENEVER
A
REQUEST
TO
A
BLOCK
NOT
AVAILABLE
IN
THE
ACTIVE
REPLICA
IS
MADE
A
REPLICA
MISS
EVENT
IS
GENERATED
ON
A
REPLICA
MISS
THE
REPLICA
MANAGER
SPIN
UPS
THE
PRIMARY
MDISK
TO
FETCH
THE
REQUIRED
BLOCK
FURTHER
IT
ADDS
THIS
NEW
BLOCK
TO
THE
WORKING
SET
OF
THE
VDISK
IN
THE
RDM
WE
NEXT
DESCRIBE
THE
KEY
COMPONENTS
OF
SRCMAP
REPLICA
PLACEMENT
CONTROLLER
THE
REPLICA
PLACEMENT
CONTROLLER
ORCHESTRATES
THE
PRO
CESS
OF
SAMPLING
IDENTIFYING
WORKING
SETS
FOR
EACH
VDISK
AND
REPLICATING
ON
ONE
OR
MORE
TARGET
MDISKS
WE
USE
A
CONSERVATIVE
DEFINITION
OF
WORKING
SET
THAT
IN
CLUDES
ALL
THE
BLOCKS
THAT
WERE
ACCESSED
DURING
A
FIXED
DURATION
CONFIGURED
AS
THE
MINIMUM
DURATION
BEYOND
WHICH
THE
HIT
RATIO
ON
THE
WORKING
SET
SATURATES
CONSE
QUENTLY
WE
USE
DAYS
FOR
MAIL
DAYS
FOR
HOMES
AND
DAYS
FOR
WEB
VM
WORKLOAD
FIG
THE
BLOCKS
THAT
CAPTURE
THE
WORKING
SET
FOR
EACH
VDISK
AND
THE
MDISKS
WHERE
IT
IS
REPLICATED
ARE
STORED
IN
THE
RDM
THE
DETAILS
OF
THE
PARAMETERS
AND
METHODOLOGY
USED
WITHIN
REPLICA
PLACEMENT
ARE
DESCRIBED
IN
SECTION
ACTIVE
DISK
MANAGER
THE
ACTIVE
DISK
MANAGER
ORCHESTRATES
THE
CONSOLIDATE
STEP
IN
SRCMAP
THE
MODULE
TAKES
AS
INPUT
THE
WORK
LOAD
INTENSITY
FOR
EACH
VDISK
AND
IDENTIFIES
IF
THE
PRIMARY
MDISK
CAN
BE
SPUN
DOWN
BY
REDIRECTING
THE
WORKLOAD
TO
ONE
OF
THE
SECONDARY
MDISKS
HOSTING
ITS
REPLICA
ONCE
THE
TARGET
SET
OF
ACTIVE
MDISKS
AND
REPLICAS
ARE
IDENTIFIED
THE
ACTIVE
DISK
MANAGER
SYNCHRONIZES
THE
IDENTIFIED
AC
TIVE
PRIMARIES
OR
ACTIVE
SECONDARY
REPLICAS
AND
UPDATES
THE
VIRTUAL
TO
PHYSICAL
MAPPING
OF
THE
STORAGE
VIRTUALIZA
TION
MANAGER
SO
THAT
I
O
REQUESTS
TO
A
VDISK
COULD
BE
REDIRECTED
ACCORDINGLY
THE
ACTIVE
DISK
MANAGER
USES
A
CONSISTENCY
MANAGER
FOR
THE
SYNCHRONIZATION
OPERATION
DETAILS
OF
THE
ALGORITHM
USED
BY
ACTIVE
DISK
MANAGER
FOR
SELECTING
ACTIVE
MDISKS
ARE
DESCRIBED
IN
SECTION
CONSISTENCY
MANAGER
THE
CONSISTENCY
MANAGER
ENSURES
THAT
THE
PRIMARY
MDISK
AND
THE
REPLICAS
ARE
CONSISTENT
BEFORE
AN
MDISK
IS
SPUN
DOWN
AND
A
NEW
REPLICA
ACTIVATED
THE
NEW
ACTIVE
REPLICA
IS
MADE
CONSISTENT
WITH
THE
PREVIOUS
ONE
IN
ORDER
TO
ENSURE
THAT
THE
OVERHEAD
DURING
THE
RE
SYNCHRONIZATION
IS
MINIMAL
AN
INCREMENTAL
POINT
IN
TIME
PIT
RELATION
SHIP
E
G
FLASH
COPY
IN
IBM
SVC
IS
MAINTAINED
BETWEEN
THE
ACTIVE
DATA
EITHER
THE
PRIMARY
MDISK
OR
ONE
OF
THE
ACTIVE
REPLICAS
AND
ALL
OTHER
COPIES
OF
THE
SAME
DATA
A
GO
TO
SYNC
OPERATION
IS
PERFORMED
PERIODI
CALLY
BETWEEN
THE
ACTIVE
DATA
AND
ALL
ITS
COPIES
ON
ACTIVE
MDISKS
THIS
ENSURES
THAT
WHEN
AN
MDISK
IS
SPUN
UP
OR
DOWN
THE
AMOUNT
OF
DATA
TO
BE
SYNCHRONIZED
IS
SMALL
REPLICA
MANAGER
THE
REPLICA
MANAGER
ENSURES
THAT
THE
REPLICA
DATA
SET
FOR
A
VDISK
IS
ABLE
TO
MIMIC
THE
WORKING
SET
OF
THE
VDISK
OVER
TIME
IF
A
DATA
BLOCK
UNAVAILABLE
AT
THE
ACTIVE
REPLICA
OF
THE
VDISK
IS
READ
CAUSING
A
REPLICA
MISS
THE
REPLICA
MANAGER
COPIES
THE
BLOCK
TO
THE
REPLICA
SPACE
ASSIGNED
TO
V
PI
WORKINGSETN
V
N
MN
W
N
PRIMARY
DATA
REPLICA
SPACE
THE
ACTIVE
REPLICA
AND
ADDS
THE
BLOCK
TO
THE
REPLICA
META
VDISKS
TARGET
MDISKS
DATA
ACCORDINGLY
FINALLY
THE
REPLICA
MANAGER
USES
A
LEAST
RECENTLY
USED
LRU
POLICY
TO
EVICT
AN
OLDER
BLOCK
IN
CASE
THE
REPLICA
SPACE
ASSIGNED
TO
A
REPLICA
IS
FILLED
UP
IF
THE
ACTIVE
DATA
SET
CHANGES
DRASTICALLY
THERE
MAY
BE
A
LARGE
NUMBER
OF
REPLICA
MISSES
ALL
THESE
REPLICA
MISSES
CAN
BE
HANDLED
BY
A
SINGLE
SPIN
UP
OF
THE
PRI
MARY
MDISK
ONCE
ALL
THE
DATA
IN
THE
NEW
WORKING
SET
IS
TOUCHED
THE
PRIMARY
MDISK
CAN
BE
SPUN
DOWN
AS
THE
ACTIVE
REPLICA
IS
NOW
UP
TO
DATE
THE
CONTINUOUS
UPDAT
ING
OF
THE
REPLICA
METADATA
ENABLES
SRCMAP
TO
MEET
THE
GOAL
OF
WORKLOAD
SHIFT
ADAPTATION
WITHOUT
RE
RUNNING
THE
EXPENSIVE
REPLICA
GENERATION
FLOW
THE
REPLICA
GENER
ATION
FLOW
NEEDS
TO
RE
RUN
ONLY
WHEN
A
DISRUPTIVE
CHANGE
OCCURS
SUCH
AS
ADDITION
OF
A
NEW
WORKLOAD
OR
A
NEW
VOL
UME
OR
NEW
DISKS
TO
A
VOLUME
ALGORITHMS
AND
OPTIMIZATIONS
FIGURE
REPLICA
PLACEMENT
MODEL
ACTIVE
MDISKS
AT
TIME
T
THE
REPLICA
PLACEMENT
ALGORITHM
CONSISTS
OF
I
CREAT
ING
AN
INITIAL
ORDERING
OF
VDISKS
IN
TERMS
OF
COST
BENEFIT
TRADEOFF
II
A
BIPARTITE
GRAPH
CREATION
THAT
REFLECTS
THIS
ORDERING
III
ITERATIVELY
CREATING
ONE
SOURCE
TARGET
MAP
PING
RESPECTING
THE
CURRENT
ORDER
AND
IV
RE
CALIBRATION
OF
EDGE
WEIGHTS
TO
ENSURE
THE
ORDERING
PROPERTY
HOLDS
FOR
THE
NEXT
ITERATION
OF
SOURCE
TARGET
MAPPING
INITIAL
VDISK
ORDERING
THE
INITIAL
VDISK
ORDERING
CREATES
A
SORTED
ORDER
AMONGST
VDISKS
BASED
ON
THEIR
COST
BENEFIT
TRADEOFF
FOR
EACH
VDISK
VI
WE
COMPUTE
THE
PROBABILITY
PI
THAT
ITS
PRIMARY
MDISK
MI
WOULD
BE
SPUN
DOWN
AS
SMIN
P
RMIN
W
F
MMIN
P
IN
THIS
SECTION
WE
PRESENT
DETAILS
ABOUT
THE
ALGORITHMS
I
EMPLOYED
BY
SRCMAP
WE
FIRST
PRESENT
THE
LONG
TERM
W
SI
P
P
RI
ΡI
MI
REPLICA
PLACEMENT
METHODOLOGY
AND
SUBSEQUENTLY
THE
SHORT
TERM
ACTIVE
DISK
IDENTIFICATION
METHOD
REPLICA
PLACEMENT
ALGORITHM
THE
REPLICA
PLACEMENT
CONTROLLER
CREATES
ONE
OR
MORE
REPLICAS
OF
THE
WORKING
SET
OF
EACH
VDISK
ON
THE
AVAILABLE
REPLICA
SPACE
ON
THE
TARGET
MDISKS
WE
USE
THE
INSIGHT
THAT
ALL
REPLICAS
ARE
NOT
CREATED
EQUAL
AND
HAVE
DISTINCT
ASSOCIATED
COSTS
AND
BENEFITS
THE
SPACE
COST
OF
CREATING
THE
REPLICA
IS
LOWER
IF
THE
VDISK
HAS
A
SMALLER
WORKING
SET
SIMILARLY
THE
BENEFIT
OF
CREATING
A
REPLICA
IS
HIGHER
IF
THE
VDISK
I
HAS
A
STABLE
WORKING
SET
LOWER
MISSES
IF
THE
PRIMARY
MDISK
IS
SWITCHED
OFF
II
HAS
A
SMALL
AVERAGE
LOAD
MAKING
IT
EASY
TO
FIND
SPARE
BANDWIDTH
FOR
IT
ON
ANY
TARGET
MDISK
AND
III
IS
HOSTED
ON
A
LESS
POWER
EFFICIENT
PRIMARY
MDISK
HENCE
THE
GOAL
OF
BOTH
REPLICA
PLACEMENT
AND
ACTIVE
DISK
IDENTIFICATION
IS
TO
ENSURE
THAT
WE
CREATE
MORE
REPLICAS
FOR
VDISKS
THAT
HAVE
A
FAVORABLE
COST
BENEFIT
RATIO
THE
GOAL
OF
THE
REPLICA
PLACEMENT
IS
TO
ENSURE
THAT
IF
THE
ACTIVE
DISK
MANAGER
DECIDES
TO
SPIN
DOWN
THE
PRIMARY
MDISK
OF
A
VDISK
IT
SHOULD
BE
ABLE
TO
FIND
AT
LEAST
ONE
ACTIVE
TARGET
MDISK
THAT
HOSTS
ITS
REPLICA
CAPTURED
IN
THE
FOLLOWING
ORDERING
PROPERTY
DEFINITION
ORDERING
PROPERTY
FOR
ANY
TWO
VDISKS
VI
AND
VJ
IF
VI
IS
MORE
LIKELY
TO
REQUIRE
A
REPLICA
TARGET
THAN
VJ
AT
ANY
TIME
T
DURING
ACTIVE
DISK
IDENTIFICATION
THEN
VI
IS
MORE
LIKELY
THAN
VJ
TO
FIND
A
REPLICA
TARGET
AMONGST
WHERE
THE
WK
ARE
TUNABLE
WEIGHTS
W
SI
IS
THE
SIZE
OF
THE
WORKING
SET
OF
VI
P
P
RI
IS
THE
PERFORMANCE
POWER
RATIO
RATIO
BETWEEN
THE
PEAK
IO
BANDWIDTH
AND
PEAK
POWER
FOR
THE
PRIMARY
MDISK
MI
OF
VI
ΡI
IS
THE
AVERAGE
LONG
TERM
I
O
WORKLOAD
INTENSITY
MEASURED
IN
IOPS
FOR
VI
AND
MI
IS
THE
NUMBER
OF
READ
MISSES
IN
THE
WORKING
SET
OF
VI
NORMALIZED
BY
THE
NUMBER
OF
SPINDLES
USED
BY
ITS
PRIMARY
MDISK
MI
THE
CORRESPONDING
MIN
SUBSCRIPT
TERMS
REPRESENT
THE
MINIMUM
VALUES
ACROSS
ALL
THE
VDISKS
AND
PROVIDE
NORMALIZATION
THE
PROBABILITY
FORMULATION
IS
BASED
ON
THE
DUAL
RATIONALE
THAT
IT
IS
RELATIVELY
EASIER
TO
FIND
A
TARGET
MDISK
FOR
A
SMALLER
WORKLOAD
AND
SWITCH
ING
OFF
RELATIVELY
MORE
POWER
HUNGRY
DISKS
SAVES
MORE
POWER
FURTHER
WE
ASSIGN
A
HIGHER
PROBABILITY
FOR
SPIN
NING
DOWN
MDISKS
THAT
HOST
MORE
STABLE
WORKING
SETS
BY
ACCOUNTING
FOR
THE
NUMBER
OF
TIMES
A
READ
REQUEST
CAN
NOT
BE
SERVED
FROM
THE
REPLICATED
WORKING
SET
THEREBY
NECESSITATING
THE
SPINNING
UP
OF
THE
PRIMARY
MDISK
BIPARTITE
GRAPH
CREATION
REPLICA
PLACEMENT
CREATES
A
BIPARTITE
GRAPH
G
V
M
WITH
EACH
VDISK
AS
A
SOURCE
NODE
VI
ITS
PRIMARY
MDISK
AS
A
TARGET
NODE
MI
AND
THE
EDGE
WEIGHTS
E
VI
MJ
REP
RESENTING
THE
COST
BENEFIT
TRADE
OFF
OF
PLACING
A
REPLICA
OF
VI
ON
MJ
FIG
THE
NODES
IN
THE
BIPARTITE
GRAPH
ARE
SORTED
USING
PI
DISKS
WITH
LARGER
PI
ARE
AT
THE
TOP
WE
INITIALIZE
THE
EDGE
WEIGHTS
WI
J
PI
FOR
EACH
EDGE
E
VI
MJ
SOURCE
TARGET
PAIR
INITIALLY
THERE
ARE
NO
PI
VK
VK
INACTIVE
MDISKS
WORKLOAD
REDIRECTION
MK
MK
ACTIVE
MDISKS
VN
MN
FIGURE
ACTIVE
DISK
IDENTIFICATION
REPLICA
ASSIGNMENTS
MADE
TO
ANY
TARGET
MDISK
THE
REPLICA
PLACEMENT
ALGORITHM
ITERATES
THROUGH
THE
FOLLOW
ING
TWO
STEPS
UNTIL
ALL
THE
AVAILABLE
REPLICA
SPACE
ON
THE
TARGET
MDISKS
HAVE
BEEN
ASSIGNED
TO
SOURCE
VDISK
REPLI
CAS
IN
EACH
ITERATION
EXACTLY
ONE
TARGET
MDISK
REPLICA
SPACE
IS
ASSIGNED
SOURCE
TARGET
MAPPING
THE
GOAL
OF
THE
REPLICA
PLACEMENT
METHOD
IS
TO
ACHIEVE
A
SOURCE
TARGET
MAPPING
THAT
ACHIEVES
THE
ORDERING
PROP
ERTY
TO
ACHIEVE
THIS
GOAL
THE
ALGORITHM
TAKES
THE
TOP
MOST
TARGET
MDISK
MI
WHOSE
REPLICA
SPACE
IS
NOT
YET
ASSIGNED
AND
SELECTS
THE
SET
OF
HIGHEST
WEIGHT
INCIDENT
EDGES
SUCH
THAT
THE
COMBINED
REPLICA
SIZE
OF
THE
SOURCE
NODES
IN
THIS
SET
FILLS
UP
THE
REPLICA
SPACE
AVAILABLE
IN
MI
E
G
THE
WORKING
SETS
OF
AND
VN
ARE
REPLICATED
IN
THE
REPLICA
SPACE
OF
IN
FIG
WHEN
THE
REPLICA
SPACE
ON
A
TARGET
MDISK
IS
FILLED
UP
WE
MARK
THE
TARGET
MDISK
AS
ASSIGNED
ONE
MAY
OBSERVE
THAT
THIS
PROCEDURE
ALWAYS
GIVES
PREFERENCE
TO
SOURCE
NODES
WITH
A
LARGER
PI
ONCE
AN
MDISK
FINDS
A
REPLICA
THE
LIKELIHOOD
OF
IT
REQUIRING
ANOTHER
REPLICA
DECREASES
AND
WE
FACTOR
THIS
USING
A
RE
CALIBRATION
OF
EDGE
WEIGHTS
WHICH
IS
DETAILED
NEXT
RE
CALIBRATION
OF
EDGE
WEIGHTS
WE
OBSERVE
THAT
THE
INITIAL
ASSIGNMENTS
OF
WEIGHTS
EN
SURE
THE
ORDERING
PROPERTY
HOWEVER
ONCE
THE
WORK
ING
SET
OF
A
VDISK
VI
HAS
BEEN
REPLICATED
ON
A
SET
OF
TAR
GET
MDISKS
TI
MLEAST
MLEAST
IS
THE
MDISK
WITH
THE
LEAST
PI
IN
TI
T
PI
PLEAST
THE
PROBABILITY
THAT
VI
WOULD
REQUIRE
A
NEW
TARGET
MDISK
DURING
ACTIVE
DISK
IDENTIFICATION
IS
THE
PROBABILITY
THAT
BOTH
MI
AND
MLEAST
WOULD
BE
SPUN
DOWN
HENCE
TO
PRESERVE
THE
OR
DERING
PROPERTY
WE
RE
CALIBRATE
THE
EDGE
WEIGHTS
OF
ALL
OUTGOING
EDGES
OF
ANY
PRIMARY
MDISKS
SI
ASSIGNED
TO
TARGET
MDISKS
TJ
AS
K
WI
K
PJPI
ONCE
THE
WEIGHTS
ARE
RECOMPUTED
WE
ITERATE
FROM
THE
SOURCE
TARGET
MAPPING
STEP
UNTIL
ALL
THE
REPLICAS
HAVE
BEEN
ASSIGNED
TO
TARGET
MDISKS
ONE
MAY
OBSERVE
THAT
THE
RE
CALIBRATION
SUCCEEDS
IN
ACHIEVING
THE
ORDERING
PROPERTY
BECAUSE
WE
START
ASSIGNING
THE
REPLICA
SPACE
FOR
THE
TOP
MOST
TARGET
MDISKS
FIRST
THIS
ALLOWS
US
TO
IN
CREASE
THE
WEIGHTS
OF
SOURCE
NODES
MONOTONICALLY
AS
WE
FIGURE
ACTIVE
REPLICA
IDENTIFICATION
ALGORITHM
PLACE
MORE
REPLICAS
OF
ITS
WORKING
SET
WE
FORMALLY
PROVE
THE
FOLLOWING
RESULT
IN
THE
APPENDIX
THEOREM
THE
REPLICA
PLACEMENT
ALGORITHM
ENSURES
ORDERING
PROPERTY
ACTIVE
DISK
IDENTIFICATION
WE
NOW
DESCRIBE
THE
METHODOLOGY
EMPLOYED
TO
IDENTIFY
THE
SET
OF
ACTIVE
MDISKS
AND
REPLICAS
AT
ANY
GIVEN
TIME
FOR
EASE
OF
EXPOSITION
WE
DEFINE
THE
PROBABILITY
PI
OF
A
PRIMARY
MDISK
MI
EQUAL
TO
THE
PROBABILITY
PI
OF
ITS
VDISK
VI
ACTIVE
DISK
IDENTIFICATION
CONSISTS
OF
I
ACTIVE
MDISK
SELECTION
WE
FIRST
ESTIMATE
THE
EXPECTED
AGGREGATE
WORKLOAD
TO
THE
STORAGE
SUBSYSTEM
IN
THE
NEXT
INTERVAL
WE
USE
THE
WORKLOAD
TO
A
VDISK
IN
THE
PREVI
OUS
INTERVAL
AS
THE
PREDICTED
WORKLOAD
IN
THE
NEXT
INTERVAL
FOR
THE
VDISK
THE
AGGREGATE
WORKLOAD
IS
THEN
ESTIMATED
AS
SUM
OF
THE
PREDICTED
WORKLOADS
FOR
ALL
VDISKS
IN
THE
STORAGE
SYSTEM
THIS
AGGREGATE
WORKLOAD
IS
THEN
USED
TO
IDENTIFY
THE
MINIMUM
SUBSET
OF
MDISKS
ORDERED
BY
RE
VERSE
OF
PI
SUCH
THAT
THE
AGGREGATE
BANDWIDTH
OF
THESE
MDISKS
EXCEEDS
THE
EXPECTED
AGGREGATE
LOAD
II
ACTIVE
REPLICA
IDENTIFICATION
THIS
STEP
ELABORATED
SHORTLY
IDENTIFIES
ONE
OF
THE
MANY
POSSIBLE
REPLICAS
ON
AN
ACTIVE
MDISK
FOR
EACH
INACTIVE
MDISK
TO
SERVE
THE
WORKLOAD
REDIRECTED
FROM
THE
INACTIVE
MDISK
III
ITERATE
IF
THE
ACTIVE
REPLICA
IDENTIFICATION
STEP
SUC
CEEDS
IN
FINDING
AN
ACTIVE
REPLICA
FOR
ALL
THE
INACTIVE
MDISKS
THE
ALGORITHM
TERMINATES
ELSE
THE
NUMBER
OF
ACTIVE
MDISKS
ARE
INCREASED
BY
AND
THE
ALGORITHM
RE
PEATS
THE
ACTIVE
REPLICA
IDENTIFICATION
STEP
ONE
MAY
NOTE
THAT
SINCE
THE
NUMBER
OF
ACTIVE
DISKS
ARE
BASED
ON
THE
MAXIMUM
PREDICTED
LOAD
IN
A
CONSOLI
DATION
INTERVAL
A
SUDDEN
INCREASE
IN
LOAD
MAY
LEAD
TO
AN
INCREASE
IN
RESPONSE
TIMES
IF
PERFORMANCE
DEGRADATION
BEYOND
USER
DEFINED
ACCEPTABLE
LEVELS
PERSISTS
BEYOND
A
USER
DEFINED
INTERVAL
E
G
MINS
THE
ACTIVE
DISK
IDEN
TIFICATION
IS
REPEATED
FOR
THE
NEW
LOAD
ACTIVE
REPLICA
IDENTIFICATION
FIG
DEPICTS
THE
HIGH
LEVEL
GOAL
OF
ACTIVE
REPLICA
IDENTIFICATION
WHICH
IS
TO
HAVE
THE
PRIMARY
MDISKS
FOR
VDISKS
WITH
LARGER
PI
SPUN
DOWN
AND
THEIR
WORKLOAD
DIRECTED
TO
FEW
MDISKS
WITH
SMALLER
PI
TO
DO
SO
IT
MUST
IDENTIFY
AN
ACTIVE
REPLICA
FOR
EACH
INACTIVE
PRIMARY
MDISK
ON
ONE
OF
THE
ACTIVE
MDISKS
THE
ALGORITHM
USES
TWO
INSIGHTS
I
THE
REPLICA
PLACEMENT
PROCESS
CREATES
MORE
REPLICAS
FOR
VDISKS
WITH
A
HIGHER
PROBABILITY
OF
BE
ING
SPUN
DOWN
PI
AND
II
PRIMARY
MDISKS
WITH
LARGER
PI
ARE
LIKELY
TO
BE
SPUN
DOWN
FOR
A
LONGER
TIME
TO
UTILIZE
THE
FIRST
INSIGHT
WE
FIRST
ALLOW
PRIMARY
MDISKS
WITH
SMALL
PI
WHICH
ARE
MARKED
AS
INACTIVE
TO
FIND
AN
ACTIVE
REPLICA
AS
THEY
HAVE
FEWER
CHOICES
AVAIL
ABLE
TO
UTILIZE
THE
SECOND
INSIGHT
WE
FORCE
INACTIVE
PRI
MARY
MDISKS
WITH
LARGE
PI
TO
USE
A
REPLICA
ON
ACTIVE
MDISKS
WITH
SMALL
PI
FOR
EXAMPLE
IN
FIG
VDISK
VK
HAS
THE
FIRST
CHOICE
OF
FINDING
AN
ACTIVE
MDISK
THAT
HOSTS
ITS
REPLICA
AND
IN
THIS
CASE
IT
IS
ABLE
TO
SELECT
THE
FIRST
ACTIVE
MDISK
MK
AS
A
RESULT
INACTIVE
MDISKS
WITH
LARGER
PI
ARE
MAPPED
TO
ACTIVE
MDISKS
WITH
THE
SMALLER
PI
E
G
IS
MAPPED
TO
MN
SINCE
AN
MDISK
WITH
THE
SMALLEST
PI
IS
LIKELY
TO
REMAIN
ACTIVE
MOST
OF
THE
TIME
THIS
ENSURES
THAT
THERE
IS
LITTLE
TO
NO
NEED
TO
SWITCH
ACTIVE
REPLICAS
FREQUENTLY
FOR
THE
INACTIVE
DISKS
THE
DETAILS
OF
THIS
METHODOLOGY
ARE
DESCRIBED
IN
FIG
KEY
OPTIMIZATIONS
TO
BASIC
SRCMAP
WE
AUGMENT
THE
BASIC
SRCMAP
ALGORITHM
TO
INCREASE
ITS
PRACTICAL
USABILITY
AND
EFFECTIVENESS
AS
FOLLOWS
SUB
VOLUME
CREATION
SRCMAP
REDIRECTS
THE
WORKLOAD
FOR
ANY
PRIMARY
MDISK
THAT
IS
SPUN
DOWN
TO
EXACTLY
ONE
TARGET
MDISK
HENCE
A
TARGET
MDISK
MJ
FOR
A
PRIMARY
MDISK
MI
NEEDS
TO
SUPPORT
THE
COMBINED
LOAD
OF
THE
VDISKS
VI
AND
VJ
IN
ORDER
TO
BE
SELECTED
WITH
THIS
REQUIREMENT
THE
SR
CMAP
CONSOLIDATION
PROCESS
MAY
INCUR
A
FRAGMENTATION
OF
THE
AVAILABLE
I
O
BANDWIDTH
ACROSS
ALL
VOLUMES
TO
ELABORATE
CONSIDER
AN
EXAMPLE
SCENARIO
WITH
IDEN
TICAL
MDISKS
EACH
WITH
CAPACITY
C
AND
INPUT
LOAD
OF
C
Δ
NOTE
THAT
EVEN
THOUGH
THIS
LOAD
CAN
BE
SERVED
USING
MDISKS
THERE
IS
NO
SINGLE
MDISK
CAN
SUPPORT
THE
INPUT
LOAD
OF
VDISKS
TO
AVOID
SUCH
A
SCENARIO
SRCMAP
SUB
DIVIDES
EACH
MDISK
INTO
NSV
SUB
VOLUMES
AND
IDENTIFIES
THE
WORKING
SET
FOR
EACH
SUB
VOLUME
SEPARATELY
THE
SUB
REPLICAS
WORKING
SETS
OF
A
SUB
VOLUME
ARE
THEN
PLACED
INDEPENDENTLY
OF
EACH
OTHER
ON
TARGET
MDISKS
WITH
THIS
OPTIMIZATION
SRCMAP
IS
ABLE
TO
SUBDIVIDE
THE
LEAST
AMOUNT
OF
LOAD
THAT
CAN
BE
MI
GRATED
THEREBY
DEALING
WITH
THE
FRAGMENTATION
PROBLEM
IN
A
STRAIGHTFORWARD
MANNER
THIS
OPTIMIZATION
REQUIRES
A
COMPLEMENTARY
MODIFI
CATION
TO
THEREPLICA
PLACEMENT
ALGORITHM
THE
SOURCE
TARGET
MAPPING
STEP
IS
MODIFIED
TO
ENSURE
THAT
SUB
REPLICAS
BELONGING
TO
THE
SAME
SOURCE
VDISK
ARE
NOT
CO
LOCATED
ON
A
TARGET
MDISK
SCRATCH
SPACE
FOR
WRITES
AND
MISSED
READS
SRCMAP
INCORPORATES
THE
BASIC
WRITE
OFF
LOADING
MECH
ANISM
AS
PROPOSED
BY
NARAYANAN
ET
AL
THE
CURRENT
IMPLEMENTATION
OF
SRCMAP
USES
AN
ADDITIONAL
ALLOCA
TION
OF
WRITE
SCRATCH
SPACE
WITH
EACH
SUB
REPLICA
TO
AB
SORB
NEW
WRITES
TO
THE
CORRESPONDING
PORTION
OF
THE
DATA
VOLUME
A
FUTURE
OPTIMIZATION
IS
TO
USE
A
SINGLE
WRITE
SCRATCH
SPACE
WITHIN
EACH
TARGET
MDISK
RATHER
THAN
ONE
PER
SUB
REPLICA
WITHIN
THE
TARGET
MDISK
SO
THAT
THE
OVER
HEAD
FOR
ABSORBING
WRITES
CAN
BE
MINIMIZED
A
KEY
DIFFERENCE
FROM
WRITE
OFF
LOADING
HOWEVER
IS
THAT
ON
A
READ
MISS
FOR
A
SPUN
DOWN
VOLUME
SRCMAP
ADDITIONALLY
OFFLOADS
THE
DATA
READ
TO
DYNAMICALLY
LEARN
THE
WORKING
SET
THIS
HELPS
SRCMAP
ACHIEVE
THE
GOAL
OF
WORKLOAD
SHIFT
ADAPTATION
WITH
CHANGE
IN
WORKING
SET
WHILE
WRITE
OFF
LOADING
USES
THE
INTER
READ
MISS
DURA
TIONS
EXCLUSIVELY
FOR
SPIN
DOWN
OPERATIONS
SRCMAP
TAR
GETS
CAPTURING
ENTIRE
WORKING
SETS
INCLUDING
BOTH
READS
AND
WRITES
IN
REPLICA
LOCATIONS
TO
PROLONG
READ
MISS
DU
RATIONS
TO
THE
ORDER
OF
HOURS
AND
THUS
PLACES
MORE
IMPOR
TANCE
ON
LEARNING
CHANGES
IN
THE
WORKING
SET
EVALUATION
IN
THIS
SECTION
WE
EVALUATE
SRCMAP
USING
A
PROTOTYPE
IMPLEMENTATION
OF
SRCMAP
BASED
STORAGE
VIRTUALIZATION
MANAGER
AND
AN
ENERGY
SIMULATOR
SEEDED
BY
THE
PROTO
TYPE
WE
INVESTIGATE
THE
FOLLOWING
QUESTIONS
WHAT
DEGREE
OF
PROPORTIONALITY
IN
ENERGY
CONSUMP
TION
AND
I
O
LOAD
CAN
BE
ACHIEVED
USING
SRCMAP
HOW
DOES
SRCMAP
IMPACT
RELIABILITY
WHAT
IS
THE
IMPACT
OF
STORAGE
CONSOLIDATION
ON
THE
I
O
PERFORMANCE
HOW
SENSITIVE
ARE
THE
ENERGY
SAVINGS
TO
THE
AMOUNT
OF
OVER
PROVISIONED
SPACE
WHAT
IS
THE
OVERHEAD
ASSOCIATED
WITH
IMPLEMENTING
AN
SRCMAP
INDIRECTION
OPTIMIZATION
WORKLOAD
THE
WORKLOADS
USED
CONSIST
OF
I
O
REQUESTS
TO
EIGHT
INDEPENDENT
DATA
VOLUMES
EACH
MAPPED
TO
AN
INDEPENDENT
DISK
DRIVE
IN
PRACTICE
VOLUMES
WILL
LIKELY
COMPRISE
OF
MORE
THAN
ONE
DISK
BUT
RESOURCE
RESTRICTIONS
DID
NOT
ALLOW
US
TO
CREATE
A
MORE
EXPANSIVE
TESTBED
WE
ARGUE
THAT
RELATIVE
ENERGY
CONSUMPTION
RESULTS
STILL
HOLD
DESPITE
THIS
APPROXIMATION
THESE
VOLUMES
SUPPORT
A
MIX
OF
PRODUCTION
WEB
SERVERS
FROM
THE
FIU
CS
DEPARTMENT
DATA
CENTER
END
USER
HOMES
DATA
AND
OUR
LAB
SUBVER
SION
SVN
AND
WIKI
SERVERS
AS
DETAILED
IN
TABLE
WORKLOAD
I
O
STATISTICS
WERE
OBTAINED
BY
RUNNING
BLK
TRACE
ON
EACH
VOLUME
OBSERVE
THAT
THERE
IS
A
WIDE
VARIANCE
IN
THEIR
LOAD
INTENSITY
VALUES
CREATING
OPPORTU
NITIES
FOR
CONSOLIDATION
ACROSS
VOLUMES
STORAGE
TESTBED
FOR
EXPERIMENTAL
EVALUATION
WE
SET
UP
A
SINGLE
MACHINE
INTEL
PENTIUM
HT
MEM
A
TABLE
WORKLOAD
AND
STORAGE
SYSTEM
DETAILS
TRACES
FIGURE
LOGICAL
VIEW
OF
EXPERIMENTAL
SETUP
ORY
CONNECTED
TO
DISKS
VIA
TWO
SATA
II
CONTROLLERS
A
AND
B
THE
CUMULATIVE
MERGED
WORKLOAD
TRACE
IS
PLAYED
BACK
USING
BTREPLAY
WITH
EACH
VOLUME
TRACE
PLAYED
BACK
TO
THE
CORRESPONDING
DISK
ALL
THE
DISKS
SHARE
ONE
POWER
SUPPLY
P
THAT
IS
DEDICATED
ONLY
FOR
THE
EXPERIMENTAL
DRIVES
THE
MACHINE
CONNECTS
TO
ANOTHER
POWER
SUPPLY
THE
POWER
SUPPLY
P
IS
CONNECTED
TO
A
WATTS
UP
PRO
POWER
METER
WHICH
ALLOWS
US
TO
MEASURE
POWER
CONSUMPTION
AT
A
ONE
SECOND
GRANULARITY
WITH
A
RESOLUTION
OF
AN
OVERHEAD
OF
IS
INTRO
DUCED
BY
THE
POWER
SUPPLY
ITSELF
WHICH
WE
DEDUCT
FROM
ALL
OUR
POWER
MEASUREMENTS
EXPERIMENTAL
SETUP
WE
DESCRIBE
THE
EXPERIMENTAL
SETUP
USED
IN
OUR
EVALUATION
STUDY
IN
FIG
WE
IM
PLEMENTED
AN
SRCMAP
MODULE
WITH
ITS
ALGORITHMS
FOR
REPLICA
PLACEMENT
AND
ACTIVE
DISK
IDENTIFICATION
DURING
ANY
CONSOLIDATION
INTERVAL
AN
OVERALL
EXPERIMENTAL
RUN
CONSISTS
OF
USING
THE
MONITORED
DATA
TO
IDENTIFY
THE
CONSOLIDATION
CANDIDATES
FOR
EACH
INTERVAL
AND
CREATE
THE
VIRTUAL
TO
PHYSICAL
MAPPING
MODIFY
THE
ORIGINAL
TRACES
TO
REFLECT
THE
MAPPING
AND
REPLAYING
IT
AND
POWER
AND
RESPONSE
TIME
REPORTING
AT
EACH
CONSOLIDA
TION
EVENT
THE
WORKLOAD
MODIFIER
GENERATES
THE
NECES
SARY
ADDITIONAL
I
O
TO
SYNCHRONIZE
DATA
ACROSS
THE
SUB
VOLUMES
AFFECTED
DUE
TO
ACTIVE
REPLICA
CHANGES
WE
EVALUATE
SRCMAP
USING
TWO
DIFFERENT
SETS
OF
EX
PERIMENTS
I
PROTOTYPE
RUNS
AND
II
SIMULATED
RUNS
THE
PROTOTYPE
RUNS
EVALUATE
SRCMAP
AGAINST
A
REAL
STORAGE
SYSTEM
AND
ENABLE
REALISTIC
MEASUREMENTS
OF
POWER
CON
SUMPTION
AND
IMPACT
TO
I
O
PERFORMANCE
VIA
THE
REPORT
ING
MODULE
IN
A
PROTOTYPE
RUN
THE
MODIFIED
I
O
WORK
B
TABLE
EXPERIMENTAL
SETTINGS
A
ESTIMATED
DISK
IOPS
CAPACITY
LEVELS
B
STORAGE
SYSTEM
POWER
CON
SUMPTION
IN
WATTS
AS
THE
NUMBER
OF
DISKS
IN
ACTIVE
MODE
ARE
VARIED
FROM
TO
ALL
DISKS
CONSUMED
AP
PROXIMATELY
THE
SAME
POWER
WHEN
ACTIVE
THE
DISKS
NOT
IN
ACTIVE
MODE
CONSUME
STANDBY
POWER
WHICH
WAS
FOUND
TO
BE
THE
SAME
ACROSS
ALL
DISKS
LOAD
IS
REPLAYED
ON
THE
ACTUAL
TESTBED
USING
BTREPLAY
THE
SIMULATOR
RUNS
OPERATE
SIMILARLY
ON
A
SIMULATED
TESTBED
WHEREIN
A
POWER
MODEL
INSTANTIATED
WITH
POWER
MEASUREMENTS
FROM
THE
TESTBED
IS
USED
FOR
REPORTING
THE
POWER
NUMBERS
THE
ADVANTAGE
WITH
THE
SIMULATOR
IS
THE
ABILITY
TO
CARRY
OUT
LONGER
DURATION
EXPERIMENTS
IN
SIM
ULATED
TIME
AS
OPPOSED
TO
REAL
TIME
ALLOWING
US
TO
EX
PLORE
THE
PARAMETER
SPACE
EFFICIENTLY
FURTHER
ONE
MAY
USE
IT
TO
SIMULATE
VARIOUS
TYPES
OF
STORAGE
TESTBEDS
TO
STUDY
THE
PERFORMANCE
UNDER
VARIOUS
LOAD
CONDITIONS
IN
PARTICULAR
WE
USE
THE
SIMULATOR
RUNS
TO
EVALUATE
ENERGY
PROPORTIONALITY
BY
SIMULATING
THE
TESTBED
WITH
DIFFERENT
VALUES
OF
DISK
IOPS
CAPACITY
ESTIMATES
WE
ALSO
SIMULATE
ALTERNATE
POWER
MANAGEMENT
TECHNIQUES
E
G
CACHING
REPLICATION
FOR
A
COMPARATIVE
EVALUATION
ALL
EXPERIMENTS
WITH
THE
PROTOTYPE
AND
THE
SIMULA
TOR
WERE
PERFORMED
WITH
THE
FOLLOWING
CONFIGURATION
PA
RAMETERS
THE
CONSOLIDATION
INTERVAL
WAS
CHOSEN
TO
BE
HOURS
FOR
ALL
EXPERIMENTS
TO
RESTRICT
THE
WORST
CASE
SPIN
UP
CYCLES
FOR
THE
DISK
DRIVES
TO
AN
ACCEPTABLE
VALUE
TWO
MINUTE
DISK
TIMEOUTS
WERE
USED
FOR
INACTIVE
DISKS
ACTIVE
DISKS
WITHIN
A
CONSOLIDATION
INTERVAL
REMAIN
CONTINUOUSLY
ACTIVE
WORKING
SETS
AND
REPLICAS
WERE
CREATED
BASED
ON
A
THREE
WEEK
WORKLOAD
HISTORY
AND
WE
REPORT
RESULTS
FOR
A
SUBSEQUENT
HOUR
DURATION
FOR
BREVITY
THE
CONSOLI
DATION
IS
BASED
ON
AN
ESTIMATE
OF
THE
DISK
IOPS
CAPACITY
WHICH
VARIES
FOR
EACH
VOLUME
WE
COMPUTED
AN
ESTIMATE
OF
THE
DISK
IOPS
USING
A
SYNTHETIC
RANDOM
I
O
WORKLOAD
FOR
EACH
VOLUME
SEPARATELY
LEVEL
WE
USE
IOPS
ESTIMATION
LEVELS
THROUGH
TO
A
SIMULATE
STORAGE
TESTBEDS
AT
DIFFERENT
LOAD
FACTORS
AND
B
STUDY
THE
SEN
SITIVITY
OF
SRCMAP
WITH
THE
VOLUME
IOPS
ESTIMATION
THE
PER
VOLUME
SUSTAINABLE
IOPS
AT
EACH
OF
THESE
LOAD
LEVELS
IS
PROVIDED
IN
TABLE
A
THE
POWER
CONSUMPTION
OF
THE
STORAGE
SYSTEM
WITH
VARYING
NUMBER
OF
DISKS
IN
ACTIVE
MODE
IS
PRESENTED
IN
TABLE
B
PROTOTYPE
RESULTS
FOR
THE
PROTOTYPE
EVALUATION
WE
TOOK
THE
MOST
DY
NAMIC
HOUR
PERIOD
CONSOLIDATION
INTERVALS
FROM
THE
HOUR
FIGURE
POWER
AND
ACTIVE
DISKS
TIME
LINE
HOURS
AND
PLAYED
BACK
I
O
TRACES
FOR
THE
WORK
LOADS
DESCRIBED
EARLIER
IN
REAL
TIME
WE
REPORT
ACTUAL
POWER
CONSUMPTION
AND
THE
I
O
RESPONSE
TIME
WHICH
INCLUDES
QUEUING
AND
SERVICE
TIME
DISTRIBUTION
FOR
SR
CMAP
WHEN
COMPARED
TO
A
BASELINE
CONFIGURATION
WHERE
RESPONSE
TIME
MSEC
ALL
DISKS
ARE
CONTINUOUSLY
ACTIVE
POWER
CONSUMPTION
WAS
MEASURED
EVERY
SECOND
AND
DISK
ACTIVE
STANDBY
STATE
INFORMATION
WAS
POLLED
EVERY
SECONDS
WE
USED
DIF
FERENT
IOPS
LEVELS
WHEN
A
VERY
CONSERVATIVE
LOW
ESTIMATE
OF
THE
DISK
IOPS
CAPACITY
IS
MADE
AND
WHEN
A
REASONABLY
AGGRESSIVE
HIGH
ESTIMATE
IS
MADE
WE
STUDY
THE
POWER
SAVINGS
DUE
TO
SRCMAP
IN
FIG
URE
EVEN
USING
A
CONSERVATIVE
ESTIMATE
OF
DISK
IOPS
WE
ARE
ABLE
TO
SPIN
DOWN
APPROXIMATELY
DISKS
ON
AN
AVERAGE
LEADING
TO
AN
AVERAGE
SAVINGS
OF
USING
AN
AGGRESSIVE
ESTIMATE
OF
DISK
IOPS
SR
CMAP
IS
ABLE
TO
SPIN
DOWN
DISKS
SAVING
FOR
ALL
PERIODS
OTHER
THAN
THE
PERIOD
IN
THE
HR
PERIOD
IT
USES
DISKS
LEADING
TO
A
POWER
SAVINGS
OF
THE
SPIKES
IN
THE
POWER
CONSUMPTION
RE
LATE
TO
PLANNED
AND
UNPLANNED
DUE
TO
READ
MISSES
VOL
UME
ACTIVATIONS
WHICH
ARE
FEW
IN
NUMBER
IT
IS
IMPOR
TANT
TO
NOTE
THAT
SUBSTANTIAL
POWER
IS
USED
IN
MAINTAINING
STANDBY
STATES
AND
WITHIN
THE
DYNAMIC
RANGE
THE
POWER
SAVINGS
DUE
TO
SRCMAP
ARE
EVEN
HIGHER
WE
NEXT
INVESTIGATE
ANY
PERFORMANCE
PENALTY
INCURRED
DUE
TO
CONSOLIDATION
FIG
UPPER
DEPICTS
THE
CUMULA
TIVE
PROBABILITY
DENSITY
FUNCTION
CDF
OF
RESPONSE
TIMES
FOR
THREE
DIFFERENT
CONFIGURATIONS
BASELINE
ON
NO
CONSOLIDATION
AND
ALL
DISKS
ALWAYS
ACTIVE
SRCMAP
US
ING
AND
THE
ACCURACY
OF
THE
CDFS
FOR
AND
SUFFER
FROM
A
REPORTING
ARTIFACT
THAT
THE
CDFS
INCLUDE
THE
LATENCIES
FOR
THE
SYNCHRONIZATION
I
OS
THEMSELVES
WHICH
WE
WERE
NOT
ABLE
TO
FILTER
OUT
WE
THROTTLE
THE
SYNCHRO
NIZATION
I
OS
TO
ONE
EVERY
TO
REDUCE
THEIR
INTERFER
ENCE
WITH
FOREGROUND
OPERATIONS
FIRST
WE
OBSERVED
THAT
LESS
THAN
OF
THE
RE
QUESTS
INCURRED
A
SPIN
UP
HIT
DUE
TO
READ
MISSES
RESULT
ING
IN
LATENCIES
OF
GREATER
THAN
SECONDS
IN
BOTH
THE
AND
CONFIGURATIONS
NOT
SHOWN
THIS
IMPLIES
THAT
THE
WORKING
SET
DYNAMICALLY
UPDATED
WITH
MISSED
READS
AND
OFFLOADED
WRITES
IS
A
FAIRLY
AT
CAPTURING
THE
ACTIVE
DATA
FOR
THESE
WORKLOADS
SECOND
WE
OBSERVE
THAT
FOR
RE
SPONSE
TIMES
GREATER
THAN
BASELINE
ON
DEMON
FIGURE
IMPACT
OF
CONSOLIDATION
ON
RESPONSE
TIME
STRATES
BETTER
PERFORMANCE
THAN
AND
UPPER
PLOT
FOR
BOTH
AND
LESS
THAN
OF
REQUESTS
INCUR
LA
TENCIES
GREATER
THAN
LESS
THAN
OF
REQUESTS
IN
CUR
LATENCIES
GREATER
THAN
HAVING
MORE
DISKS
AT
ITS
DISPOSAL
SHOWS
SLIGHTLY
BETTER
RESPONSE
TIMES
THAN
FOR
RESPONSE
TIMES
LOWER
THAN
A
REVERSE
TREND
IS
OBSERVED
WHEREIN
THE
SRCMAP
CONFIGURATIONS
DO
BETTER
THAN
BASELINE
ON
WE
CONJECTURED
THAT
THIS
IS
DUE
TO
THE
INFLUENCE
OF
THE
LOW
LATENCY
WRITES
DURING
SYNCHRO
NIZATION
OPERATIONS
TO
FURTHER
DELINEATE
THE
INFLUENCE
OF
SYNCHRONIZATION
I
OS
WE
PERFORMED
TWO
ADDITIONAL
RUNS
IN
THE
FIRST
RUN
WE
DISABLE
ALL
SYNCHRONIZATION
I
OS
AND
IN
THE
SECOND
WE
DISABLE
ALL
FOREGROUND
I
OS
LOWER
PLOT
THE
CDFS
OF
ONLY
THE
SYNCHRONIZATION
OPERATIONS
WHICH
SHOW
A
BI
MODAL
DISTRIBUTION
WITH
LOW
LATENCY
WRITES
ABSORBED
BY
THE
DISK
BUFFER
AND
READS
WITH
LATENCIES
GREATER
THAN
INDICATE
THAT
SYNCHRONIZATION
READS
ARE
CON
TRIBUTING
TOWARDS
THE
INCREASED
LATENCIES
IN
AND
FOR
THE
UPPER
PLOT
THE
CDF
WITHOUT
SYNCHRONIZATION
W
O
SYNCH
IS
MUCH
CLOSER
TO
BASELINE
ON
WITH
A
DECREASE
OF
APPROXIMATELY
IN
THE
NUMBER
OF
REQUEST
WITH
LA
TENCIES
GREATER
THAN
INTELLIGENT
SCHEDULING
OF
SYN
CHRONIZATION
I
OS
IS
AN
IMPORTANT
AREA
OF
FUTURE
WORK
TO
FURTHER
REDUCE
THE
IMPACT
ON
FOREGROUND
I
O
OPERATIONS
SIMULATOR
RESULTS
WE
CONDUCTED
SEVERAL
EXPERIMENTS
WITH
SIMULATED
TESTBEDS
HOSTING
DISKS
OF
CAPACITIES
TO
FOR
BREVITY
WE
REPORT
OUR
OBSERVATIONS
FOR
DISK
CAPACITY
LEVELS
AND
EXPANDING
TO
OTHER
LEVELS
ONLY
WHEN
REQUIRED
COMPARATIVE
EVALUATION
WE
FIRST
DEMONSTRATE
THE
BASIC
ENERGY
PROPORTIONALITY
ACHIEVED
BY
SRCMAP
IN
ITS
MOST
CONSERVATIVE
CONFIG
URATION
AND
THREE
ALTERNATE
SOLUTIONS
CACHING
CACHING
AND
REPLICATION
CACHING
IS
A
SCHEME
THAT
USES
ADDITIONAL
PHYSICAL
VOLUME
AS
A
CACHE
IF
THE
AG
GREGATE
LOAD
OBSERVED
IS
LESS
THAN
THE
IOPS
CAPACITY
OF
SRCMAP
BASELINE
ON
LOAD
IOPS
MODIFIED
LOAD
IOPS
POWER
WATTS
HOUR
FIGURE
POWER
CONSUMPTION
REMAP
OPERATIONS
AND
AGGREGATE
LOAD
ACROSS
TIME
FOR
A
SINGLE
DAY
THE
CACHE
VOLUME
THE
WORKLOAD
IS
REDIRECTED
TO
THE
CACHE
VOLUME
IF
THE
LOAD
IS
HIGHER
THE
ORIGINAL
PHYSICAL
VOL
UMES
ARE
USED
CACHING
USES
CACHE
VOLUMES
IN
A
SIM
ILAR
MANNER
REPLICATION
IDENTIFIES
PAIRS
OF
PHYSICAL
VOL
UMES
WITH
SIMILAR
BANDWIDTHS
AND
CREATES
REPLICA
PAIRS
WHERE
ALL
THE
DATA
ON
ONE
VOLUME
IS
REPLICATED
ON
THE
OTHER
IF
THE
AGGREGATE
LOAD
TO
A
PAIR
IS
LESS
THAN
THE
IOPS
CAPACITY
OF
ONE
VOLUME
ONLY
ONE
IN
THE
PAIR
IS
KEPT
AC
TIVE
ELSE
BOTH
VOLUMES
ARE
KEPT
ACTIVE
FIGURE
EVALUATES
POWER
CONSUMPTION
OF
ALL
FOUR
SO
LUTIONS
BY
SIMULATING
THE
POWER
CONSUMED
AS
VOLUMES
ARE
SPUN
UP
DOWN
OVER
HOUR
CONSOLIDATION
INTERVALS
IT
ALSO
PRESENTS
THE
AVERAGE
LOAD
MEASURED
IN
IOPS
WITHIN
EACH
CONSOLIDATION
INTERVAL
IN
THE
CASE
OF
SR
CMAP
READ
MISSES
ARE
INDICATED
BY
INSTANTANEOUS
POWER
SPIKES
WHICH
REQUIRE
ACTIVATING
AN
ADDITIONAL
DISK
DRIVE
TO
AVOID
CLUTTER
WE
DO
NOT
SHOW
THE
SPIKES
DUE
TO
READ
MISSES
FOR
THE
CACHE
CONFIGURATIONS
WE
OBSERVE
THAT
EACH
OF
SOLUTIONS
DEMONSTRATE
VARYING
DEGREES
OF
ENERGY
PROPORTIONALITY
ACROSS
THE
INTERVALS
SRCMAP
UNI
FORMLY
CONSUMES
THE
LEAST
AMOUNT
OF
POWER
ACROSS
ALL
IN
TERVALS
AND
ITS
POWER
CONSUMPTION
IS
PROPORTIONAL
TO
LOAD
REPLICATION
ALSO
DEMONSTRATES
GOOD
ENERGY
PROPORTIONAL
ITY
BUT
AT
A
HIGHER
POWER
CONSUMPTION
ON
AN
AVERAGE
THE
CACHING
CONFIGURATIONS
ARE
THE
LEAST
ENERGY
PROPORTIONAL
WITH
ONLY
TWO
EFFECTIVE
ENERGY
LEVELS
TO
WORK
WITH
WE
ALSO
OBSERVE
THAT
SRCMAP
REMAPS
I
E
CHANGES
THE
ACTIVE
REPLICA
FOR
A
MINIMAL
NUMBER
OF
VOLUMES
EI
THER
OR
DURING
EACH
CONSOLIDATION
INTERVAL
IN
FACT
WE
FOUND
THAT
FOR
ALL
DURATIONS
THE
NUMBER
OF
VOLUMES
BE
ING
REMAPPED
EQUALED
THE
CHANGE
IN
THE
NUMBER
OF
ACTIVE
PHYSICAL
VOLUMES
INDICATING
THAT
THE
NUMBER
OF
SYNCHRO
NIZATION
OPERATIONS
ARE
KEPT
TO
THE
MINIMUM
FINALLY
IN
OUR
SYSTEM
WITH
EIGHT
VOLUMES
CACHING
CACHING
AND
REPLICATION
USE
AND
ADDITIONAL
SPACE
RESPECTIVELY
WHILE
AS
WE
SHALL
SHOW
LATER
SR
CMAP
IS
ABLE
TO
DELIVER
ALMOST
ALL
ITS
ENERGY
SAVINGS
WITH
JUST
ADDITIONAL
SPACE
NEXT
WE
INVESTIGATE
HOW
SRCMAP
MODIFIES
PER
VOLUME
ACTIVITY
AND
POWER
CONSUMPTION
WITH
AN
AGGRES
SIVE
CONFIGURATION
A
CONFIGURATION
THAT
DEMONSTRATED
FIGURE
LOAD
AND
POWER
CONSUMPTION
FOR
EACH
DISK
Y
RANGES
FOR
ALL
LOADS
IS
IOPS
IN
LOG
ARITHMIC
SCALE
Y
RANGES
FOR
POWER
IS
W
INTERESTING
CONSOLIDATION
DYNAMICS
OVER
THE
HOUR
CONSOLIDATION
INTERVALS
EACH
ROW
IN
FIGURE
IS
SPECIFIC
TO
ONE
OF
THE
EIGHT
VOLUMES
THROUGH
THE
LEFT
AND
CENTER
COLUMNS
SHOW
THE
ORIGINAL
AND
SRCMAP
MODIFIED
LOAD
IOPS
FOR
EACH
VOLUME
THE
MODIFIED
LOAD
WERE
CONSOLIDATED
ON
DISKS
AND
BY
SRCMAP
NOTE
THAT
DISKS
AND
ARE
CONTINUOUSLY
IN
STANDBY
MODE
IS
CONTINUOUSLY
IN
ACTIVE
MODE
THROUGHOUT
THE
HOUR
DURATION
WHILE
THE
REMAINING
DISKS
SWITCHED
STATES
MORE
THAN
ONCE
OF
THESE
AND
WERE
MAINTAINED
IN
STANDBY
MODE
BY
SRCMAP
BUT
WERE
SPUN
UP
ONE
OR
MORE
TIMES
DUE
TO
READ
MISSES
TO
THEIR
REPLICA
VOLUMES
WHILE
WAS
MADE
ACTIVE
BY
SRCMAP
FOR
TWO
OF
THE
CONSOLIDATION
INTERVALS
ONLY
WE
NOTE
THAT
THE
NUMBER
OF
SPIN
UP
CYCLES
DID
NOT
EX
CEED
FOR
ANY
PHYSICAL
VOLUME
DURING
THE
HOUR
PE
RIOD
THUS
NOT
SACRIFICING
RELIABILITY
DUE
TO
THE
RELIABILITY
AWARE
DESIGN
OF
SRCMAP
VOLUMES
MARKED
AS
ACTIVE
CONSUME
POWER
EVEN
WHEN
THERE
IS
IDLENESS
OVER
SHORTER
SUB
INTERVAL
DURATIONS
FOR
THE
RIGHT
COLUMN
POWER
CON
SUMPTION
FOR
EACH
DISK
IN
EITHER
ACTIVE
MODE
OR
SPUN
DOWN
IS
SHOWN
WITH
SPIKES
REPRESENTING
SPIN
UPS
DUE
TO
READ
MISSES
IN
THE
VOLUME
ACTIVE
REPLICA
FURTHER
EVEN
IF
THE
WORKING
SET
CHANGES
DRASTICALLY
DURING
AN
INTERVAL
IT
ONLY
LEADS
TO
A
SINGLE
SPIN
UP
THAT
SERVICES
A
LARGE
NUM
BER
OF
MISSES
FOR
EXAMPLE
SERVED
APPROXIMATELY
MISSES
IN
THE
SINGLE
SPIN
UP
IT
HAD
TO
INCUR
FIGURE
OMITTED
DUE
TO
LACK
OF
SPACE
WE
ALSO
NOTE
THAT
SUMMING
UP
POWER
CONSUMPTION
OF
INDIVIDUAL
VOLUMES
CANNOT
BE
USED
TO
COMPUTE
TOTAL
POWER
AS
PER
TABLE
B
SENSITIVITY
WITH
SPACE
OVERHEAD
WE
EVALUATED
THE
SENSITIVITY
OF
SRCMAP
ENERGY
SAVINGS
WITH
THE
AMOUNT
OF
OVER
PROVISIONED
SPACE
TO
STORE
VOL
UME
WORKING
SETS
FIGURE
DEPICTS
THE
AVERAGE
POWER
CONSUMPTION
OF
THE
ENTIRE
STORAGE
SYSTEM
I
E
ALL
EIGHT
VOLUMES
ACROSS
A
HOUR
INTERVAL
AS
THE
AMOUNT
OF
OVER
PROVISIONED
SPACE
IS
VARIED
AS
A
PERCENTAGE
OF
THE
TOTAL
OVERPROVISIONED
SPACE
50
LOAD
FACTOR
FIGURE
SENSITIVITY
TO
OVER
PROVISIONED
SPACE
FIGURE
ENERGY
PROPORTIONALITY
WITH
LOAD
STORAGE
SPACE
FOR
THE
LOAD
LEVEL
WE
OBSERVE
THAT
SR
CMAP
IS
ABLE
TO
DELIVER
MOST
OF
ITS
ENERGY
SAVINGS
WITH
PRESSED
AS
N
R
FOR
A
STORAGE
VIRTUALIZATION
MAN
SPACE
OVER
PROVISIONING
AND
ALL
SAVINGS
WITH
HENCE
WE
CONCLUDE
THAT
SRCMAP
CAN
DELIVER
POWER
SAV
INGS
WITH
MINIMAL
REPLICA
SPACE
ENERGY
PROPORTIONALITY
OUR
NEXT
EXPERIMENT
EVALUATES
THE
DEGREE
OF
ENERGY
PRO
PORTIONALITY
TO
THE
TOTAL
LOAD
ON
THE
STORAGE
SYSTEM
DE
LIVERED
BY
SRCMAP
FOR
THIS
EXPERIMENT
WE
EXAMINED
THE
POWER
CONSUMPTION
WITHIN
EACH
HOUR
CONSOLIDA
TION
INTERVAL
ACROSS
THE
HOUR
DURATION
FOR
EACH
OF
THE
FIVE
LOAD
ESTIMATION
LEVELS
THROUGH
GIVING
US
DATA
POINTS
FURTHER
WE
CREATED
A
FEW
HIGHER
LOAD
LEV
ELS
BELOW
TO
STUDY
ENERGY
PROPORTIONALITY
AT
HIGH
LOAD
AS
WELL
EACH
DATA
POINT
IS
CHARACTERIZED
BY
AN
AVERAGE
POWER
CONSUMPTION
VALUE
AND
A
LOAD
FACTOR
VALUE
WHICH
IS
THE
OBSERVED
AVERAGE
IOPS
LOAD
AS
A
PERCENTAGE
OF
THE
ESTIMATED
IOPS
CAPACITY
BASED
ON
THE
LOAD
ESTIMA
TION
LEVEL
ACROSS
ALL
THE
VOLUMES
FIGURE
PRESENTS
THE
POWER
CONSUMPTION
AT
EACH
LOAD
FACTOR
EVEN
THOUGH
THE
LOAD
FACTOR
IS
A
CONTINUOUS
VARIABLE
POWER
CONSUMPTION
LEVELS
IN
SRCMAP
ARE
DISCRETE
ONE
MAY
NOTE
THAT
SR
CMAP
CAN
ONLY
VARY
ONE
VOLUME
AT
A
TIME
AND
HENCE
THE
DIFFERENT
POWER
PERFORMANCE
LEVELS
IN
SRCMAP
DIFFER
BY
ONE
PHYSICAL
VOLUME
WE
DO
OBSERVE
THAT
SRCMAP
IS
ABLE
TO
ACHIEVE
CLOSE
TO
N
LEVEL
PROPORTIONALITY
FOR
A
SYSTEM
WITH
N
VOLUMES
DEMONSTRATING
A
STEP
WISE
LIN
EAR
INCREASE
IN
POWER
LEVELS
WITH
INCREASING
LOAD
RESOURCE
OVERHEAD
OF
SRCMAP
THE
PRIMARY
RESOURCE
OVERHEAD
IN
SRCMAP
IS
THE
MEM
ORY
USED
BY
THE
REPLICA
METADATA
MAP
OF
THE
REPLICA
MANAGER
THIS
MEMORY
OVERHEAD
DEPENDS
ON
THE
SIZE
OF
THE
REPLICA
SPACE
MAINTAINED
ON
EACH
VOLUME
FOR
STORING
BOTH
WORKING
SETS
AND
OFF
LOADED
WRITES
WE
MAINTAIN
A
PER
BLOCK
MAP
ENTRY
WHICH
CONSISTS
OF
BYTES
TO
POINT
TO
THE
CURRENT
ACTIVE
REPLICA
ADDITIONAL
BYTES
KEEP
WHAT
REPLICAS
CONTAIN
THE
LAST
DATA
VERSION
AND
MORE
BYTES
ARE
USED
TO
HANDLE
THE
I
OS
ABSORBED
IN
THE
REPLICA
SPACE
WRITE
BUFFER
MAKING
A
TOTAL
OF
BYTES
FOR
EACH
ENTRY
IN
THE
MAP
IF
N
IS
THE
NUMBER
OF
VOLUMES
OF
SIZE
WITH
R
SPACE
TO
STORE
REPLICAS
THEN
THE
WORST
CASE
MEMORY
CONSUMPTION
IS
APPROXIMATELY
EQUAL
TO
THE
MAP
SIZE
EX
AGER
THAT
MANAGES
VOLUMES
OF
TOTAL
SIZE
EACH
WITH
A
REPLICA
SPACE
ALLOCATION
OF
OVER
PROVISIONING
THE
MEMORY
OVERHEAD
IS
ONLY
EAS
ILY
AFFORDABLE
FOR
A
HIGH
END
STORAGE
VIRTUALIZATION
MAN
AGER
CONCLUSIONS
AND
FUTURE
WORK
IN
THIS
WORK
WE
HAVE
PROPOSED
AND
EVALUATED
SRCMAP
A
STORAGE
VIRTUALIZATION
SOLUTION
FOR
ENERGY
PROPORTIONAL
STORAGE
SRCMAP
ESTABLISHES
THE
FEASIBILITY
OF
AN
ENERGY
PROPORTIONAL
STORAGE
SYSTEM
WITH
FULLY
FLEXIBLE
DYNAMIC
STORAGE
CONSOLIDATION
ALONG
THE
LINES
OF
SERVER
CONSOLI
DATION
WHERE
ANY
VIRTUAL
MACHINE
CAN
BE
MIGRATED
TO
ANY
PHYSICAL
SERVER
IN
THE
CLUSTER
SRCMAP
IS
ABLE
TO
MEET
ALL
THE
DESIRED
GOALS
OF
FINE
GRAINED
ENERGY
PROPORTIONALITY
LOW
SPACE
OVERHEAD
RELIABILITY
WORKLOAD
SHIFT
ADAPTA
TION
AND
HETEROGENEITY
SUPPORT
OUR
WORK
OPENS
UP
SEVERAL
NEW
DIRECTIONS
FOR
FURTHER
RESEARCH
SOME
OF
THE
MOST
IMPORTANT
MODELING
AND
OP
TIMIZATION
SOLUTIONS
THAT
WILL
IMPROVE
A
SYSTEM
LIKE
SR
CMAP
ARE
I
NEW
MODELS
THAT
CAPTURE
THE
PERFORMANCE
IMPACT
OF
STORAGE
CONSOLIDATION
II
INVESTIGATING
THE
USE
OF
WORKLOAD
CORRELATION
BETWEEN
LOGICAL
VOLUMES
DUR
ING
CONSOLIDATION
AND
III
OPTIMIZING
THE
SCHEDULING
OF
REPLICA
SYNCHRONIZATION
TO
MINIMIZE
IMPACT
ON
FORE
GROUND
I
O
CHAPTER
POINTERS
AND
ARRAYS
A
POINTER
IS
A
VARIABLE
THAT
CONTAINS
THE
ADDRESS
OF
A
VARIABLE
POINTERS
ARE
MUCH
USED
IN
C
PARTLY
BECAUSE
THEY
ARE
SOMETIMES
THE
ONLY
WAY
TO
EXPRESS
A
COMPUTATION
AND
PARTLY
BECAUSE
THEY
USUALLY
LEAD
TO
MORE
COMPACT
AND
EFFICIENT
CODE
THAN
CAN
BE
OBTAINED
IN
OTHER
WAYS
POINTERS
AND
ARRAYS
ARE
CLOSELY
RELATED
THIS
CHAPTER
ALSO
EXPLORES
THIS
RELATIONSHIP
AND
SHOWS
HOW
TO
EXPLOIT
IT
POINTERS
HAVE
BEEN
LUMPED
WITH
THE
GOTO
STATEMENT
AS
A
MARVELOUS
WAY
TO
CREATE
IMPOSSIBLE
TO
UNDERSTAND
PROGRAMS
THIS
IS
CERTAINLY
TRUE
WHEN
THEY
ARE
USED
CARELESSLY
AND
IT
IS
EASY
TO
CREATE
POINTERS
THAT
POINT
SOMEWHERE
UNEXPECTED
WITH
DISCIPLINE
HOWEVER
POINTERS
CAN
ALSO
BE
USED
TO
ACHIEVE
CLARITY
AND
SIMPLICITY
THIS
IS
THE
ASPECT
THAT
WE
WILL
TRY
TO
ILLUSTRATE
THE
MAIN
CHANGE
IN
ANSI
C
IS
TO
MAKE
EXPLICIT
THE
RULES
ABOUT
HOW
POINTERS
CAN
BE
MANIPULATED
IN
EFFECT
MANDATING
WHAT
GOOD
PROGRAMMERS
ALREADY
PRACTICE
AND
GOOD
COMPILERS
ALREADY
ENFORCE
IN
ADDITION
THE
TYPE
VOID
POINTER
TO
VOID
REPLACES
CHAR
AS
THE
PROPER
TYPE
FOR
A
GENERIC
POINTER
POINTERS
AND
ADDRESSES
LET
US
BEGIN
WITH
A
SIMPLIFIED
PICTURE
OF
HOW
MEMORY
IS
ORGANIZED
A
TYPICAL
MACHINE
HAS
AN
ARRAY
OF
CONSECUTIVELY
NUMBERED
OR
ADDRESSED
MEMORY
CELLS
THAT
MAY
BE
MANIPULATED
INDIVIDUALLY
OR
IN
CONTIGUOUS
GROUPS
ONE
COMMON
SITUATION
IS
THAT
ANY
BYTE
CAN
BE
A
CHAR
A
PAIR
OF
ONE
BYTE
CELLS
CAN
BE
TREATED
AS
A
SHORT
INTEGER
AND
FOUR
ADJACENT
BYTES
FORM
A
LONG
A
POINTER
IS
A
GROUP
OF
CELLS
OFTEN
TWO
OR
FOUR
THAT
CAN
HOLD
AN
ADDRESS
SO
IF
C
IS
A
CHAR
AND
P
IS
A
POINTER
THAT
POINTS
TO
IT
WE
COULD
REPRESENT
THE
SITUATION
THIS
WAY
THE
UNARY
OPERATOR
GIVES
THE
ADDRESS
OF
AN
OBJECT
SO
THE
STATEMENT
P
C
ASSIGNS
THE
ADDRESS
OF
C
TO
THE
VARIABLE
P
AND
P
IS
SAID
TO
POINT
TO
C
THE
OPERATOR
ONLY
APPLIES
TO
OBJECTS
IN
MEMORY
VARIABLES
AND
ARRAY
ELEMENTS
IT
CANNOT
BE
APPLIED
TO
EXPRESSIONS
CONSTANTS
OR
REGISTER
VARIABLES
THE
UNARY
OPERATOR
IS
THE
INDIRECTION
OR
DEREFERENCING
OPERATOR
WHEN
APPLIED
TO
A
POINTER
IT
ACCESSES
THE
OBJECT
THE
POINTER
POINTS
TO
SUPPOSE
THAT
X
AND
Y
ARE
INTEGERS
AND
IP
IS
A
POINTER
TO
INT
THIS
ARTIFICIAL
SEQUENCE
SHOWS
HOW
TO
DECLARE
A
POINTER
AND
HOW
TO
USE
AND
INT
X
Y
Z
INT
IP
IP
IS
A
POINTER
TO
INT
IP
X
IP
NOW
POINTS
TO
X
Y
IP
Y
IS
NOW
IP
X
IS
NOW
IP
Z
IP
NOW
POINTS
TO
Z
THE
DECLARATION
OF
X
Y
AND
Z
ARE
WHAT
WE
VE
SEEN
ALL
ALONG
THE
DECLARATION
OF
THE
POINTER
IP
INT
IP
IS
INTENDED
AS
A
MNEMONIC
IT
SAYS
THAT
THE
EXPRESSION
IP
IS
AN
INT
THE
SYNTAX
OF
THE
DECLARATION
FOR
A
VARIABLE
MIMICS
THE
SYNTAX
OF
EXPRESSIONS
IN
WHICH
THE
VARIABLE
MIGHT
APPEAR
THIS
REASONING
APPLIES
TO
FUNCTION
DECLARATIONS
AS
WELL
FOR
EXAMPLE
DOUBLE
DP
ATOF
CHAR
SAYS
THAT
IN
AN
EXPRESSION
DP
AND
ATOF
HAVE
VALUES
OF
DOUBLE
AND
THAT
THE
ARGUMENT
OF
ATOF
IS
A
POINTER
TO
CHAR
YOU
SHOULD
ALSO
NOTE
THE
IMPLICATION
THAT
A
POINTER
IS
CONSTRAINED
TO
POINT
TO
A
PARTICULAR
KIND
OF
OBJECT
EVERY
POINTER
POINTS
TO
A
SPECIFIC
DATA
TYPE
THERE
IS
ONE
EXCEPTION
A
POINTER
TO
VOID
IS
USED
TO
HOLD
ANY
TYPE
OF
POINTER
BUT
CANNOT
BE
DEREFERENCED
ITSELF
WE
LL
COME
BACK
TO
IT
IN
SECTION
IF
IP
POINTS
TO
THE
INTEGER
X
THEN
IP
CAN
OCCUR
IN
ANY
CONTEXT
WHERE
X
COULD
SO
IP
IP
INCREMENTS
IP
BY
THE
UNARY
OPERATORS
AND
BIND
MORE
TIGHTLY
THAN
ARITHMETIC
OPERATORS
SO
THE
ASSIGNMENT
Y
IP
TAKES
WHATEVER
IP
POINTS
AT
ADDS
AND
ASSIGNS
THE
RESULT
TO
Y
WHILE
IP
INCREMENTS
WHAT
IP
POINTS
TO
AS
DO
IP
AND
IP
THE
PARENTHESES
ARE
NECESSARY
IN
THIS
LAST
EXAMPLE
WITHOUT
THEM
THE
EXPRESSION
WOULD
INCREMENT
IP
INSTEAD
OF
WHAT
IT
POINTS
TO
BECAUSE
UNARY
OPERATORS
LIKE
AND
ASSOCIATE
RIGHT
TO
LEFT
FINALLY
SINCE
POINTERS
ARE
VARIABLES
THEY
CAN
BE
USED
WITHOUT
DEREFERENCING
FOR
EXAMPLE
IF
IQ
IS
ANOTHER
POINTER
TO
INT
IQ
IP
COPIES
THE
CONTENTS
OF
IP
INTO
IQ
THUS
MAKING
IQ
POINT
TO
WHATEVER
IP
POINTED
TO
POINTERS
AND
FUNCTION
ARGUMENTS
SINCE
C
PASSES
ARGUMENTS
TO
FUNCTIONS
BY
VALUE
THERE
IS
NO
DIRECT
WAY
FOR
THE
CALLED
FUNCTION
TO
ALTER
A
VARIABLE
IN
THE
CALLING
FUNCTION
FOR
INSTANCE
A
SORTING
ROUTINE
MIGHT
EXCHANGE
TWO
OUT
OF
ORDER
ARGUMENTS
WITH
A
FUNCTION
CALLED
SWAP
IT
IS
NOT
ENOUGH
TO
WRITE
SWAP
A
B
WHERE
THE
SWAP
FUNCTION
IS
DEFINED
AS
VOID
SWAP
INT
X
INT
Y
WRONG
INT
TEMP
TEMP
X
X
Y
Y
TEMP
BECAUSE
OF
CALL
BY
VALUE
SWAP
CAN
T
AFFECT
THE
ARGUMENTS
A
AND
B
IN
THE
ROUTINE
THAT
CALLED
IT
THE
FUNCTION
ABOVE
SWAPS
COPIES
OF
A
AND
B
THE
WAY
TO
OBTAIN
THE
DESIRED
EFFECT
IS
FOR
THE
CALLING
PROGRAM
TO
PASS
POINTERS
TO
THE
VALUES
TO
BE
CHANGED
SWAP
A
B
SINCE
THE
OPERATOR
PRODUCES
THE
ADDRESS
OF
A
VARIABLE
A
IS
A
POINTER
TO
A
IN
SWAP
ITSELF
THE
PARAMETERS
ARE
DECLARED
AS
POINTERS
AND
THE
OPERANDS
ARE
ACCESSED
INDIRECTLY
THROUGH
THEM
VOID
SWAP
INT
PX
INT
PY
INTERCHANGE
PX
AND
PY
INT
TEMP
TEMP
PX
PX
PY
PY
TEMP
PICTORIALLY
POINTER
ARGUMENTS
ENABLE
A
FUNCTION
TO
ACCESS
AND
CHANGE
OBJECTS
IN
THE
FUNCTION
THAT
CALLED
IT
AS
AN
EXAMPLE
CONSIDER
A
FUNCTION
GETINT
THAT
PERFORMS
FREE
FORMAT
INPUT
CONVERSION
BY
BREAKING
A
STREAM
OF
CHARACTERS
INTO
INTEGER
VALUES
ONE
INTEGER
PER
CALL
GETINT
HAS
TO
RETURN
THE
VALUE
IT
FOUND
AND
ALSO
SIGNAL
END
OF
FILE
WHEN
THERE
IS
NO
MORE
INPUT
THESE
VALUES
HAVE
TO
BE
PASSED
BACK
BY
SEPARATE
PATHS
FOR
NO
MATTER
WHAT
VALUE
IS
USED
FOR
EOF
THAT
COULD
ALSO
BE
THE
VALUE
OF
AN
INPUT
INTEGER
ONE
SOLUTION
IS
TO
HAVE
GETINT
RETURN
THE
END
OF
FILE
STATUS
AS
ITS
FUNCTION
VALUE
WHILE
USING
A
POINTER
ARGUMENT
TO
STORE
THE
CONVERTED
INTEGER
BACK
IN
THE
CALLING
FUNCTION
THIS
IS
THE
SCHEME
USED
BY
SCANF
AS
WELL
SEE
SECTION
THE
FOLLOWING
LOOP
FILLS
AN
ARRAY
WITH
INTEGERS
BY
CALLS
TO
GETINT
INT
N
ARRAY
SIZE
GETINT
INT
FOR
N
N
SIZE
GETINT
ARRAY
N
EOF
N
EACH
CALL
SETS
ARRAY
N
TO
THE
NEXT
INTEGER
FOUND
IN
THE
INPUT
AND
INCREMENTS
N
NOTICE
THAT
IT
IS
ESSENTIAL
TO
PASS
THE
ADDRESS
OF
ARRAY
N
TO
GETINT
OTHERWISE
THERE
IS
NO
WAY
FOR
GETINT
TO
COMMUNICATE
THE
CONVERTED
INTEGER
BACK
TO
THE
CALLER
OUR
VERSION
OF
GETINT
RETURNS
EOF
FOR
END
OF
FILE
ZERO
IF
THE
NEXT
INPUT
IS
NOT
A
NUMBER
AND
A
POSITIVE
VALUE
IF
THE
INPUT
CONTAINS
A
VALID
NUMBER
INCLUDE
CTYPE
H
INT
GETCH
VOID
VOID
UNGETCH
INT
GETINT
GET
NEXT
INTEGER
FROM
INPUT
INTO
PN
INT
GETINT
INT
PN
INT
C
SIGN
WHILE
ISSPACE
C
GETCH
SKIP
WHITE
SPACE
IF
ISDIGIT
C
C
EOF
C
C
UNGETCH
C
IT
IS
NOT
A
NUMBER
RETURN
SIGN
C
IF
C
C
C
GETCH
FOR
PN
ISDIGIT
C
C
GETCH
PN
PN
C
PN
SIGN
IF
C
EOF
UNGETCH
C
RETURN
C
THROUGHOUT
GETINT
PN
IS
USED
AS
AN
ORDINARY
INT
VARIABLE
WE
HAVE
ALSO
USED
GETCH
AND
UNGETCH
DESCRIBED
IN
SECTION
SO
THE
ONE
EXTRA
CHARACTER
THAT
MUST
BE
READ
CAN
BE
PUSHED
BACK
ONTO
THE
INPUT
EXERCISE
AS
WRITTEN
GETINT
TREATS
A
OR
NOT
FOLLOWED
BY
A
DIGIT
AS
A
VALID
REPRESENTATION
OF
ZERO
FIX
IT
TO
PUSH
SUCH
A
CHARACTER
BACK
ON
THE
INPUT
EXERCISE
WRITE
GETFLOAT
THE
FLOATING
POINT
ANALOG
OF
GETINT
WHAT
TYPE
DOES
GETFLOAT
RETURN
AS
ITS
FUNCTION
VALUE
POINTERS
AND
ARRAYS
IN
C
THERE
IS
A
STRONG
RELATIONSHIP
BETWEEN
POINTERS
AND
ARRAYS
STRONG
ENOUGH
THAT
POINTERS
AND
ARRAYS
SHOULD
BE
DISCUSSED
SIMULTANEOUSLY
ANY
OPERATION
THAT
CAN
BE
ACHIEVED
BY
ARRAY
SUBSCRIPTING
CAN
ALSO
BE
DONE
WITH
POINTERS
THE
POINTER
VERSION
WILL
IN
GENERAL
BE
FASTER
BUT
AT
LEAST
TO
THE
UNINITIATED
SOMEWHAT
HARDER
TO
UNDERSTAND
THE
DECLARATION
INT
A
DEFINES
AN
ARRAY
OF
SIZE
THAT
IS
A
BLOCK
OF
CONSECUTIVE
OBJECTS
NAMED
A
A
A
THE
NOTATION
A
I
REFERS
TO
THE
I
TH
ELEMENT
OF
THE
ARRAY
IF
PA
IS
A
POINTER
TO
AN
INTEGER
DECLARED
AS
INT
PA
THEN
THE
ASSIGNMENT
PA
A
SETS
PA
TO
POINT
TO
ELEMENT
ZERO
OF
A
THAT
IS
PA
CONTAINS
THE
ADDRESS
OF
A
NOW
THE
ASSIGNMENT
X
PA
WILL
COPY
THE
CONTENTS
OF
A
INTO
X
IF
PA
POINTS
TO
A
PARTICULAR
ELEMENT
OF
AN
ARRAY
THEN
BY
DEFINITION
PA
POINTS
TO
THE
NEXT
ELEMENT
PA
I
POINTS
I
ELEMENTS
AFTER
PA
AND
PA
I
POINTS
I
ELEMENTS
BEFORE
THUS
IF
PA
POINTS
TO
A
PA
REFERS
TO
THE
CONTENTS
OF
A
PA
I
IS
THE
ADDRESS
OF
A
I
AND
PA
I
IS
THE
CONTENTS
OF
A
I
THESE
REMARKS
ARE
TRUE
REGARDLESS
OF
THE
TYPE
OR
SIZE
OF
THE
VARIABLES
IN
THE
ARRAY
A
THE
MEANING
OF
ADDING
TO
A
POINTER
AND
BY
EXTENSION
ALL
POINTER
ARITHMETIC
IS
THAT
PA
POINTS
TO
THE
NEXT
OBJECT
AND
PA
I
POINTS
TO
THE
I
TH
OBJECT
BEYOND
PA
THE
CORRESPONDENCE
BETWEEN
INDEXING
AND
POINTER
ARITHMETIC
IS
VERY
CLOSE
BY
DEFINITION
THE
VALUE
OF
A
VARIABLE
OR
EXPRESSION
OF
TYPE
ARRAY
IS
THE
ADDRESS
OF
ELEMENT
ZERO
OF
THE
ARRAY
THUS
AFTER
THE
ASSIGNMENT
PA
A
PA
AND
A
HAVE
IDENTICAL
VALUES
SINCE
THE
NAME
OF
AN
ARRAY
IS
A
SYNONYM
FOR
THE
LOCATION
OF
THE
INITIAL
ELEMENT
THE
ASSIGNMENT
PA
A
CAN
ALSO
BE
WRITTEN
AS
PA
A
RATHER
MORE
SURPRISING
AT
FIRST
SIGHT
IS
THE
FACT
THAT
A
REFERENCE
TO
A
I
CAN
ALSO
BE
WRITTEN
AS
A
I
IN
EVALUATING
A
I
C
CONVERTS
IT
TO
A
I
IMMEDIATELY
THE
TWO
FORMS
ARE
EQUIVALENT
APPLYING
THE
OPERATOR
TO
BOTH
PARTS
OF
THIS
EQUIVALENCE
IT
FOLLOWS
THAT
A
I
AND
A
I
ARE
ALSO
IDENTICAL
A
I
IS
THE
ADDRESS
OF
THE
I
TH
ELEMENT
BEYOND
A
AS
THE
OTHER
SIDE
OF
THIS
COIN
IF
PA
IS
A
POINTER
EXPRESSIONS
MIGHT
USE
IT
WITH
A
SUBSCRIPT
PA
I
IS
IDENTICAL
TO
PA
I
IN
SHORT
AN
ARRAY
AND
INDEX
EXPRESSION
IS
EQUIVALENT
TO
ONE
WRITTEN
AS
A
POINTER
AND
OFFSET
THERE
IS
ONE
DIFFERENCE
BETWEEN
AN
ARRAY
NAME
AND
A
POINTER
THAT
MUST
BE
KEPT
IN
MIND
A
POINTER
IS
A
VARIABLE
SO
PA
A
AND
PA
ARE
LEGAL
BUT
AN
ARRAY
NAME
IS
NOT
A
VARIABLE
CONSTRUCTIONS
LIKE
A
PA
AND
A
ARE
ILLEGAL
WHEN
AN
ARRAY
NAME
IS
PASSED
TO
A
FUNCTION
WHAT
IS
PASSED
IS
THE
LOCATION
OF
THE
INITIAL
ELEMENT
WITHIN
THE
CALLED
FUNCTION
THIS
ARGUMENT
IS
A
LOCAL
VARIABLE
AND
SO
AN
ARRAY
NAME
PARAMETER
IS
A
POINTER
THAT
IS
A
VARIABLE
CONTAINING
AN
ADDRESS
WE
CAN
USE
THIS
FACT
TO
WRITE
ANOTHER
VERSION
OF
STRLEN
WHICH
COMPUTES
THE
LENGTH
OF
A
STRING
STRLEN
RETURN
LENGTH
OF
STRING
INT
STRLEN
CHAR
INT
N
FOR
N
N
RETURN
N
SINCE
IS
A
POINTER
INCREMENTING
IT
IS
PERFECTLY
LEGAL
HAS
NO
EFFECT
ON
THE
CHARACTER
STRING
IN
THE
FUNCTION
THAT
CALLED
STRLEN
BUT
MERELY
INCREMENTS
STRLEN
PRIVATE
COPY
OF
THE
POINTER
THAT
MEANS
THAT
CALLS
LIKE
STRLEN
HELLO
WORLD
STRING
CONSTANT
STRLEN
ARRAY
CHAR
ARRAY
STRLEN
PTR
CHAR
PTR
ALL
WORK
AS
FORMAL
PARAMETERS
IN
A
FUNCTION
DEFINITION
CHAR
AND
CHAR
ARE
EQUIVALENT
WE
PREFER
THE
LATTER
BECAUSE
IT
SAYS
MORE
EXPLICITLY
THAT
THE
VARIABLE
IS
A
POINTER
WHEN
AN
ARRAY
NAME
IS
PASSED
TO
A
FUNCTION
THE
FUNCTION
CAN
AT
ITS
CONVENIENCE
BELIEVE
THAT
IT
HAS
BEEN
HANDED
EITHER
AN
ARRAY
OR
A
POINTER
AND
MANIPULATE
IT
ACCORDINGLY
IT
CAN
EVEN
USE
BOTH
NOTATIONS
IF
IT
SEEMS
APPROPRIATE
AND
CLEAR
IT
IS
POSSIBLE
TO
PASS
PART
OF
AN
ARRAY
TO
A
FUNCTION
BY
PASSING
A
POINTER
TO
THE
BEGINNING
OF
THE
SUBARRAY
FOR
EXAMPLE
IF
A
IS
AN
ARRAY
F
A
AND
F
A
BOTH
PASS
TO
THE
FUNCTION
F
THE
ADDRESS
OF
THE
SUBARRAY
THAT
STARTS
AT
A
WITHIN
F
THE
PARAMETER
DECLARATION
CAN
READ
F
INT
ARR
OR
F
INT
ARR
SO
AS
FAR
AS
F
IS
CONCERNED
THE
FACT
THAT
THE
PARAMETER
REFERS
TO
PART
OF
A
LARGER
ARRAY
IS
OF
NO
CONSEQUENCE
IF
ONE
IS
SURE
THAT
THE
ELEMENTS
EXIST
IT
IS
ALSO
POSSIBLE
TO
INDEX
BACKWARDS
IN
AN
ARRAY
P
P
AND
SO
ON
ARE
SYNTACTICALLY
LEGAL
AND
REFER
TO
THE
ELEMENTS
THAT
IMMEDIATELY
PRECEDE
P
OF
COURSE
IT
IS
ILLEGAL
TO
REFER
TO
OBJECTS
THAT
ARE
NOT
WITHIN
THE
ARRAY
BOUNDS
ADDRESS
ARITHMETIC
IF
P
IS
A
POINTER
TO
SOME
ELEMENT
OF
AN
ARRAY
THEN
P
INCREMENTS
P
TO
POINT
TO
THE
NEXT
ELEMENT
AND
P
I
INCREMENTS
IT
TO
POINT
I
ELEMENTS
BEYOND
WHERE
IT
CURRENTLY
DOES
THESE
AND
SIMILAR
CONSTRUCTIONS
ARE
THE
SIMPLES
FORMS
OF
POINTER
OR
ADDRESS
ARITHMETIC
C
IS
CONSISTENT
AND
REGULAR
IN
ITS
APPROACH
TO
ADDRESS
ARITHMETIC
ITS
INTEGRATION
OF
POINTERS
ARRAYS
AND
ADDRESS
ARITHMETIC
IS
ONE
OF
THE
STRENGTHS
OF
THE
LANGUAGE
LET
US
ILLUSTRATE
BY
WRITING
A
RUDIMENTARY
STORAGE
ALLOCATOR
THERE
ARE
TWO
ROUTINES
THE
FIRST
ALLOC
N
RETURNS
A
POINTER
TO
N
CONSECUTIVE
CHARACTER
POSITIONS
WHICH
CAN
BE
USED
BY
THE
CALLER
OF
ALLOC
FOR
STORING
CHARACTERS
THE
SECOND
AFREE
P
RELEASES
THE
STORAGE
THUS
ACQUIRED
SO
IT
CAN
BE
RE
USED
LATER
THE
ROUTINES
ARE
RUDIMENTARY
BECAUSE
THE
CALLS
TO
AFREE
MUST
BE
MADE
IN
THE
OPPOSITE
ORDER
TO
THE
CALLS
MADE
ON
ALLOC
THAT
IS
THE
STORAGE
MANAGED
BY
ALLOC
AND
AFREE
IS
A
STACK
OR
LAST
IN
FIRST
OUT
THE
STANDARD
LIBRARY
PROVIDES
ANALOGOUS
FUNCTIONS
CALLED
MALLOC
AND
FREE
THAT
HAVE
NO
SUCH
RESTRICTIONS
IN
SECTION
WE
WILL
SHOW
HOW
THEY
CAN
BE
IMPLEMENTED
THE
EASIEST
IMPLEMENTATION
IS
TO
HAVE
ALLOC
HAND
OUT
PIECES
OF
A
LARGE
CHARACTER
ARRAY
THAT
WE
WILL
CALL
ALLOCBUF
THIS
ARRAY
IS
PRIVATE
TO
ALLOC
AND
AFREE
SINCE
THEY
DEAL
IN
POINTERS
NOT
ARRAY
INDICES
NO
OTHER
ROUTINE
NEED
KNOW
THE
NAME
OF
THE
ARRAY
WHICH
CAN
BE
DECLARED
STATIC
IN
THE
SOURCE
FILE
CONTAINING
ALLOC
AND
AFREE
AND
THUS
BE
INVISIBLE
OUTSIDE
IT
IN
PRACTICAL
IMPLEMENTATIONS
THE
ARRAY
MAY
WELL
NOT
EVEN
HAVE
A
NAME
IT
MIGHT
INSTEAD
BE
OBTAINED
BY
CALLING
MALLOC
OR
BY
ASKING
THE
OPERATING
SYSTEM
FOR
A
POINTER
TO
SOME
UNNAMED
BLOCK
OF
STORAGE
THE
OTHER
INFORMATION
NEEDED
IS
HOW
MUCH
OF
ALLOCBUF
HAS
BEEN
USED
WE
USE
A
POINTER
CALLED
ALLOCP
THAT
POINTS
TO
THE
NEXT
FREE
ELEMENT
WHEN
ALLOC
IS
ASKED
FOR
N
CHARACTERS
IT
CHECKS
TO
SEE
IF
THERE
IS
ENOUGH
ROOM
LEFT
IN
ALLOCBUF
IF
SO
ALLOC
RETURNS
THE
CURRENT
VALUE
OF
ALLOCP
I
E
THE
BEGINNING
OF
THE
FREE
BLOCK
THEN
INCREMENTS
IT
BY
N
TO
POINT
TO
THE
NEXT
FREE
AREA
IF
THERE
IS
NO
ROOM
ALLOC
RETURNS
ZERO
AFREE
P
MERELY
SETS
ALLOCP
TO
P
IF
P
IS
INSIDE
ALLOCBUF
DEFINE
ALLOCSIZE
SIZE
OF
AVAILABLE
SPACE
STATIC
CHAR
ALLOCBUF
ALLOCSIZE
STORAGE
FOR
ALLOC
STATIC
CHAR
ALLOCP
ALLOCBUF
NEXT
FREE
POSITION
CHAR
ALLOC
INT
N
RETURN
POINTER
TO
N
CHARACTERS
IF
ALLOCBUF
ALLOCSIZE
ALLOCP
N
IT
FITS
ALLOCP
N
RETURN
ALLOCP
N
OLD
P
ELSE
NOT
ENOUGH
ROOM
RETURN
VOID
AFREE
CHAR
P
FREE
STORAGE
POINTED
TO
BY
P
IF
P
ALLOCBUF
P
ALLOCBUF
ALLOCSIZE
ALLOCP
P
IN
GENERAL
A
POINTER
CAN
BE
INITIALIZED
JUST
AS
ANY
OTHER
VARIABLE
CAN
THOUGH
NORMALLY
THE
ONLY
MEANINGFUL
VALUES
ARE
ZERO
OR
AN
EXPRESSION
INVOLVING
THE
ADDRESS
OF
PREVIOUSLY
DEFINED
DATA
OF
APPROPRIATE
TYPE
THE
DECLARATION
STATIC
CHAR
ALLOCP
ALLOCBUF
DEFINES
ALLOCP
TO
BE
A
CHARACTER
POINTER
AND
INITIALIZES
IT
TO
POINT
TO
THE
BEGINNING
OF
ALLOCBUF
WHICH
IS
THE
NEXT
FREE
POSITION
WHEN
THE
PROGRAM
STARTS
THIS
COULD
ALSO
HAVE
BEEN
WRITTEN
STATIC
CHAR
ALLOCP
ALLOCBUF
SINCE
THE
ARRAY
NAME
IS
THE
ADDRESS
OF
THE
ZEROTH
ELEMENT
THE
TEST
IF
ALLOCBUF
ALLOCSIZE
ALLOCP
N
IT
FITS
CHECKS
IF
THERE
ENOUGH
ROOM
TO
SATISFY
A
REQUEST
FOR
N
CHARACTERS
IF
THERE
IS
THE
NEW
VALUE
OF
ALLOCP
WOULD
BE
AT
MOST
ONE
BEYOND
THE
END
OF
ALLOCBUF
IF
THE
REQUEST
CAN
BE
SATISFIED
ALLOC
RETURNS
A
POINTER
TO
THE
BEGINNING
OF
A
BLOCK
OF
CHARACTERS
NOTICE
THE
DECLARATION
OF
THE
FUNCTION
ITSELF
IF
NOT
ALLOC
MUST
RETURN
SOME
SIGNAL
THAT
THERE
IS
NO
SPACE
LEFT
C
GUARANTEES
THAT
ZERO
IS
NEVER
A
VALID
ADDRESS
FOR
DATA
SO
A
RETURN
VALUE
OF
ZERO
CAN
BE
USED
TO
SIGNAL
AN
ABNORMAL
EVENT
IN
THIS
CASE
NO
SPACE
POINTERS
AND
INTEGERS
ARE
NOT
INTERCHANGEABLE
ZERO
IS
THE
SOLE
EXCEPTION
THE
CONSTANT
ZERO
MAY
BE
ASSIGNED
TO
A
POINTER
AND
A
POINTER
MAY
BE
COMPARED
WITH
THE
CONSTANT
ZERO
THE
SYMBOLIC
CONSTANT
NULL
IS
OFTEN
USED
IN
PLACE
OF
ZERO
AS
A
MNEMONIC
TO
INDICATE
MORE
CLEARLY
THAT
THIS
IS
A
SPECIAL
VALUE
FOR
A
POINTER
NULL
IS
DEFINED
IN
STDIO
H
WE
WILL
USE
NULL
HENCEFORTH
TESTS
LIKE
IF
ALLOCBUF
ALLOCSIZE
ALLOCP
N
IT
FITS
AND
IF
P
ALLOCBUF
P
ALLOCBUF
ALLOCSIZE
SHOW
SEVERAL
IMPORTANT
FACETS
OF
POINTER
ARITHMETIC
FIRST
POINTERS
MAY
BE
COMPARED
UNDER
CERTAIN
CIRCUMSTANCES
IF
P
AND
Q
POINT
TO
MEMBERS
OF
THE
SAME
ARRAY
THEN
RELATIONS
LIKE
ETC
WORK
PROPERLY
FOR
EXAMPLE
P
Q
IS
TRUE
IF
P
POINTS
TO
AN
EARLIER
ELEMENT
OF
THE
ARRAY
THAN
Q
DOES
ANY
POINTER
CAN
BE
MEANINGFULLY
COMPARED
FOR
EQUALITY
OR
INEQUALITY
WITH
ZERO
BUT
THE
BEHAVIOR
IS
UNDEFINED
FOR
ARITHMETIC
OR
COMPARISONS
WITH
POINTERS
THAT
DO
NOT
POINT
TO
MEMBERS
OF
THE
SAME
ARRAY
THERE
IS
ONE
EXCEPTION
THE
ADDRESS
OF
THE
FIRST
ELEMENT
PAST
THE
END
OF
AN
ARRAY
CAN
BE
USED
IN
POINTER
ARITHMETIC
SECOND
WE
HAVE
ALREADY
OBSERVED
THAT
A
POINTER
AND
AN
INTEGER
MAY
BE
ADDED
OR
SUBTRACTED
THE
CONSTRUCTION
P
N
MEANS
THE
ADDRESS
OF
THE
N
TH
OBJECT
BEYOND
THE
ONE
P
CURRENTLY
POINTS
TO
THIS
IS
TRUE
REGARDLESS
OF
THE
KIND
OF
OBJECT
P
POINTS
TO
N
IS
SCALED
ACCORDING
TO
THE
SIZE
OF
THE
OBJECTS
P
POINTS
TO
WHICH
IS
DETERMINED
BY
THE
DECLARATION
OF
P
IF
AN
INT
IS
FOUR
BYTES
FOR
EXAMPLE
THE
INT
WILL
BE
SCALED
BY
FOUR
POINTER
SUBTRACTION
IS
ALSO
VALID
IF
P
AND
Q
POINT
TO
ELEMENTS
OF
THE
SAME
ARRAY
AND
P
Q
THEN
Q
P
IS
THE
NUMBER
OF
ELEMENTS
FROM
P
TO
Q
INCLUSIVE
THIS
FACT
CAN
BE
USED
TO
WRITE
YET
ANOTHER
VERSION
OF
STRLEN
STRLEN
RETURN
LENGTH
OF
STRING
INT
STRLEN
CHAR
CHAR
P
WHILE
P
P
RETURN
P
IN
ITS
DECLARATION
P
IS
INITIALIZED
TO
THAT
IS
TO
POINT
TO
THE
FIRST
CHARACTER
OF
THE
STRING
IN
THE
WHILE
LOOP
EACH
CHARACTER
IN
TURN
IS
EXAMINED
UNTIL
THE
AT
THE
END
IS
SEEN
BECAUSE
P
POINTS
TO
CHARACTERS
P
ADVANCES
P
TO
THE
NEXT
CHARACTER
EACH
TIME
AND
P
GIVES
THE
NUMBER
OF
CHARACTERS
ADVANCED
OVER
THAT
IS
THE
STRING
LENGTH
THE
NUMBER
OF
CHARACTERS
IN
THE
STRING
COULD
BE
TOO
LARGE
TO
STORE
IN
AN
INT
THE
HEADER
STDDEF
H
DEFINES
A
TYPE
THAT
IS
LARGE
ENOUGH
TO
HOLD
THE
SIGNED
DIFFERENCE
OF
TWO
POINTER
VALUES
IF
WE
WERE
BEING
CAUTIOUS
HOWEVER
WE
WOULD
USE
FOR
THE
RETURN
VALUE
OF
STRLEN
TO
MATCH
THE
STANDARD
LIBRARY
VERSION
IS
THE
UNSIGNED
INTEGER
TYPE
RETURNED
BY
THE
SIZEOF
OPERATOR
POINTER
ARITHMETIC
IS
CONSISTENT
IF
WE
HAD
BEEN
DEALING
WITH
FLOATS
WHICH
OCCUPY
MORE
STORAGE
THAT
CHARS
AND
IF
P
WERE
A
POINTER
TO
FLOAT
P
WOULD
ADVANCE
TO
THE
NEXT
FLOAT
THUS
WE
COULD
WRITE
ANOTHER
VERSION
OF
ALLOC
THAT
MAINTAINS
FLOATS
INSTEAD
OF
CHARS
MERELY
BY
CHANGING
CHAR
TO
FLOAT
THROUGHOUT
ALLOC
AND
AFREE
ALL
THE
POINTER
MANIPULATIONS
AUTOMATICALLY
TAKE
INTO
ACCOUNT
THE
SIZE
OF
THE
OBJECTS
POINTED
TO
THE
VALID
POINTER
OPERATIONS
ARE
ASSIGNMENT
OF
POINTERS
OF
THE
SAME
TYPE
ADDING
OR
SUBTRACTING
A
POINTER
AND
AN
INTEGER
SUBTRACTING
OR
COMPARING
TWO
POINTERS
TO
MEMBERS
OF
THE
SAME
ARRAY
AND
ASSIGNING
OR
COMPARING
TO
ZERO
ALL
OTHER
POINTER
ARITHMETIC
IS
ILLEGAL
IT
IS
NOT
LEGAL
TO
ADD
TWO
POINTERS
OR
TO
MULTIPLY
OR
DIVIDE
OR
SHIFT
OR
MASK
THEM
OR
TO
ADD
FLOAT
OR
DOUBLE
TO
THEM
OR
EVEN
EXCEPT
FOR
VOID
TO
ASSIGN
A
POINTER
OF
ONE
TYPE
TO
A
POINTER
OF
ANOTHER
TYPE
WITHOUT
A
CAST
CHARACTER
POINTERS
AND
FUNCTIONS
A
STRING
CONSTANT
WRITTEN
AS
I
AM
A
STRING
IS
AN
ARRAY
OF
CHARACTERS
IN
THE
INTERNAL
REPRESENTATION
THE
ARRAY
IS
TERMINATED
WITH
THE
NULL
CHARACTER
SO
THAT
PROGRAMS
CAN
FIND
THE
END
THE
LENGTH
IN
STORAGE
IS
THUS
ONE
MORE
THAN
THE
NUMBER
OF
CHARACTERS
BETWEEN
THE
DOUBLE
QUOTES
PERHAPS
THE
MOST
COMMON
OCCURRENCE
OF
STRING
CONSTANTS
IS
AS
ARGUMENTS
TO
FUNCTIONS
AS
IN
PRINTF
HELLO
WORLD
N
WHEN
A
CHARACTER
STRING
LIKE
THIS
APPEARS
IN
A
PROGRAM
ACCESS
TO
IT
IS
THROUGH
A
CHARACTER
POINTER
PRINTF
RECEIVES
A
POINTER
TO
THE
BEGINNING
OF
THE
CHARACTER
ARRAY
THAT
IS
A
STRING
CONSTANT
IS
ACCESSED
BY
A
POINTER
TO
ITS
FIRST
ELEMENT
STRING
CONSTANTS
NEED
NOT
BE
FUNCTION
ARGUMENTS
IF
PMESSAGE
IS
DECLARED
AS
CHAR
PMESSAGE
THEN
THE
STATEMENT
PMESSAGE
NOW
IS
THE
TIME
ASSIGNS
TO
PMESSAGE
A
POINTER
TO
THE
CHARACTER
ARRAY
THIS
IS
NOT
A
STRING
COPY
ONLY
POINTERS
ARE
INVOLVED
C
DOES
NOT
PROVIDE
ANY
OPERATORS
FOR
PROCESSING
AN
ENTIRE
STRING
OF
CHARACTERS
AS
A
UNIT
THERE
IS
AN
IMPORTANT
DIFFERENCE
BETWEEN
THESE
DEFINITIONS
CHAR
AMESSAGE
NOW
IS
THE
TIME
AN
ARRAY
CHAR
PMESSAGE
NOW
IS
THE
TIME
A
POINTER
AMESSAGE
IS
AN
ARRAY
JUST
BIG
ENOUGH
TO
HOLD
THE
SEQUENCE
OF
CHARACTERS
AND
THAT
INITIALIZES
IT
INDIVIDUAL
CHARACTERS
WITHIN
THE
ARRAY
MAY
BE
CHANGED
BUT
AMESSAGE
WILL
ALWAYS
REFER
TO
THE
SAME
STORAGE
ON
THE
OTHER
HAND
PMESSAGE
IS
A
POINTER
INITIALIZED
TO
POINT
TO
A
STRING
CONSTANT
THE
POINTER
MAY
SUBSEQUENTLY
BE
MODIFIED
TO
POINT
ELSEWHERE
BUT
THE
RESULT
IS
UNDEFINED
IF
YOU
TRY
TO
MODIFY
THE
STRING
CONTENTS
WE
WILL
ILLUSTRATE
MORE
ASPECTS
OF
POINTERS
AND
ARRAYS
BY
STUDYING
VERSIONS
OF
TWO
USEFUL
FUNCTIONS
ADAPTED
FROM
THE
STANDARD
LIBRARY
THE
FIRST
FUNCTION
IS
STRCPY
T
WHICH
COPIES
THE
STRING
T
TO
THE
STRING
IT
WOULD
BE
NICE
JUST
TO
SAY
T
BUT
THIS
COPIES
THE
POINTER
NOT
THE
CHARACTERS
TO
COPY
THE
CHARACTERS
WE
NEED
A
LOOP
THE
ARRAY
VERSION
FIRST
STRCPY
COPY
T
TO
ARRAY
SUBSCRIPT
VERSION
VOID
STRCPY
CHAR
CHAR
T
INT
I
I
WHILE
I
T
I
I
FOR
CONTRAST
HERE
IS
A
VERSION
OF
STRCPY
WITH
POINTERS
STRCPY
COPY
T
TO
POINTER
VERSION
VOID
STRCPY
CHAR
CHAR
T
INT
I
I
WHILE
T
T
BECAUSE
ARGUMENTS
ARE
PASSED
BY
VALUE
STRCPY
CAN
USE
THE
PARAMETERS
AND
T
IN
ANY
WAY
IT
PLEASES
HERE
THEY
ARE
CONVENIENTLY
INITIALIZED
POINTERS
WHICH
ARE
MARCHED
ALONG
THE
ARRAYS
A
CHARACTER
AT
A
TIME
UNTIL
THE
THAT
TERMINATES
T
HAS
BEEN
COPIED
INTO
IN
PRACTICE
STRCPY
WOULD
NOT
BE
WRITTEN
AS
WE
SHOWED
IT
ABOVE
EXPERIENCED
C
PROGRAMMERS
WOULD
PREFER
STRCPY
COPY
T
TO
POINTER
VERSION
VOID
STRCPY
CHAR
CHAR
T
WHILE
T
THIS
MOVES
THE
INCREMENT
OF
AND
T
INTO
THE
TEST
PART
OF
THE
LOOP
THE
VALUE
OF
T
IS
THE
CHARACTER
THAT
T
POINTED
TO
BEFORE
T
WAS
INCREMENTED
THE
POSTFIX
DOESN
T
CHANGE
T
UNTIL
AFTER
THIS
CHARACTER
HAS
BEEN
FETCHED
IN
THE
SAME
WAY
THE
CHARACTER
IS
STORED
INTO
THE
OLD
POSITION
BEFORE
IS
INCREMENTED
THIS
CHARACTER
IS
ALSO
THE
VALUE
THAT
IS
COMPARED
AGAINST
TO
CONTROL
THE
LOOP
THE
NET
EFFECT
IS
THAT
CHARACTERS
ARE
COPIED
FROM
T
TO
UP
AND
INCLUDING
THE
TERMINATING
AS
THE
FINAL
ABBREVIATION
OBSERVE
THAT
A
COMPARISON
AGAINST
IS
REDUNDANT
SINCE
THE
QUESTION
IS
MERELY
WHETHER
THE
EXPRESSION
IS
ZERO
SO
THE
FUNCTION
WOULD
LIKELY
BE
WRITTEN
AS
STRCPY
COPY
T
TO
POINTER
VERSION
VOID
STRCPY
CHAR
CHAR
T
WHILE
T
ALTHOUGH
THIS
MAY
SEEM
CRYPTIC
AT
FIRST
SIGHT
THE
NOTATIONAL
CONVENIENCE
IS
CONSIDERABLE
AND
THE
IDIOM
SHOULD
BE
MASTERED
BECAUSE
YOU
WILL
SEE
IT
FREQUENTLY
IN
C
PROGRAMS
THE
STRCPY
IN
THE
STANDARD
LIBRARY
STRING
H
RETURNS
THE
TARGET
STRING
AS
ITS
FUNCTION
VALUE
THE
SECOND
ROUTINE
THAT
WE
WILL
EXAMINE
IS
STRCMP
T
WHICH
COMPARES
THE
CHARACTER
STRINGS
AND
T
AND
RETURNS
NEGATIVE
ZERO
OR
POSITIVE
IF
IS
LEXICOGRAPHICALLY
LESS
THAN
EQUAL
TO
OR
GREATER
THAN
T
THE
VALUE
IS
OBTAINED
BY
SUBTRACTING
THE
CHARACTERS
AT
THE
FIRST
POSITION
WHERE
AND
T
DISAGREE
STRCMP
RETURN
IF
T
IF
T
IF
T
INT
STRCMP
CHAR
CHAR
T
INT
I
FOR
I
I
T
I
I
IF
I
RETURN
RETURN
I
T
I
THE
POINTER
VERSION
OF
STRCMP
STRCMP
RETURN
IF
T
IF
T
IF
T
INT
STRCMP
CHAR
CHAR
T
FOR
T
T
IF
RETURN
RETURN
T
SINCE
AND
ARE
EITHER
PREFIX
OR
POSTFIX
OPERATORS
OTHER
COMBINATIONS
OF
AND
AND
OCCUR
ALTHOUGH
LESS
FREQUENTLY
FOR
EXAMPLE
P
DECREMENTS
P
BEFORE
FETCHING
THE
CHARACTER
THAT
P
POINTS
TO
IN
FACT
THE
PAIR
OF
EXPRESSIONS
P
VAL
PUSH
VAL
ONTO
STACK
VAL
P
POP
TOP
OF
STACK
INTO
VAL
ARE
THE
STANDARD
IDIOM
FOR
PUSHING
AND
POPPING
A
STACK
SEE
SECTION
THE
HEADER
STRING
H
CONTAINS
DECLARATIONS
FOR
THE
FUNCTIONS
MENTIONED
IN
THIS
SECTION
PLUS
A
VARIETY
OF
OTHER
STRING
HANDLING
FUNCTIONS
FROM
THE
STANDARD
LIBRARY
EXERCISE
WRITE
A
POINTER
VERSION
OF
THE
FUNCTION
STRCAT
THAT
WE
SHOWED
IN
CHAPTER
STRCAT
T
COPIES
THE
STRING
T
TO
THE
END
OF
EXERCISE
WRITE
THE
FUNCTION
STREND
T
WHICH
RETURNS
IF
THE
STRING
T
OCCURS
AT
THE
END
OF
THE
STRING
AND
ZERO
OTHERWISE
EXERCISE
WRITE
VERSIONS
OF
THE
LIBRARY
FUNCTIONS
STRNCPY
STRNCAT
AND
STRNCMP
WHICH
OPERATE
ON
AT
MOST
THE
FIRST
N
CHARACTERS
OF
THEIR
ARGUMENT
STRINGS
FOR
EXAMPLE
STRNCPY
T
N
COPIES
AT
MOST
N
CHARACTERS
OF
T
TO
FULL
DESCRIPTIONS
ARE
IN
APPENDIX
B
EXERCISE
REWRITE
APPROPRIATE
PROGRAMS
FROM
EARLIER
CHAPTERS
AND
EXERCISES
WITH
POINTERS
INSTEAD
OF
ARRAY
INDEXING
GOOD
POSSIBILITIES
INCLUDE
GETLINE
CHAPTERS
AND
ATOI
ITOA
AND
THEIR
VARIANTS
CHAPTERS
AND
REVERSE
CHAPTER
AND
STRINDEX
AND
GETOP
CHAPTER
POINTER
ARRAYS
POINTERS
TO
POINTERS
SINCE
POINTERS
ARE
VARIABLES
THEMSELVES
THEY
CAN
BE
STORED
IN
ARRAYS
JUST
AS
OTHER
VARIABLES
CAN
LET
US
ILLUSTRATE
BY
WRITING
A
PROGRAM
THAT
WILL
SORT
A
SET
OF
TEXT
LINES
INTO
ALPHABETIC
ORDER
A
STRIPPED
DOWN
VERSION
OF
THE
UNIX
PROGRAM
SORT
IN
CHAPTER
WE
PRESENTED
A
SHELL
SORT
FUNCTION
THAT
WOULD
SORT
AN
ARRAY
OF
INTEGERS
AND
IN
CHAPTER
WE
IMPROVED
ON
IT
WITH
A
QUICKSORT
THE
SAME
ALGORITHMS
WILL
WORK
EXCEPT
THAT
NOW
WE
HAVE
TO
DEAL
WITH
LINES
OF
TEXT
WHICH
ARE
OF
DIFFERENT
LENGTHS
AND
WHICH
UNLIKE
INTEGERS
CAN
T
BE
COMPARED
OR
MOVED
IN
A
SINGLE
OPERATION
WE
NEED
A
DATA
REPRESENTATION
THAT
WILL
COPE
EFFICIENTLY
AND
CONVENIENTLY
WITH
VARIABLE
LENGTH
TEXT
LINES
THIS
IS
WHERE
THE
ARRAY
OF
POINTERS
ENTERS
IF
THE
LINES
TO
BE
SORTED
ARE
STORED
END
TO
END
IN
ONE
LONG
CHARACTER
ARRAY
THEN
EACH
LINE
CAN
BE
ACCESSED
BY
A
POINTER
TO
ITS
FIRST
CHARACTER
THE
POINTERS
THEMSELVES
CAN
BEE
STORED
IN
AN
ARRAY
TWO
LINES
CAN
BE
COMPARED
BY
PASSING
THEIR
POINTERS
TO
STRCMP
WHEN
TWO
OUT
OF
ORDER
LINES
HAVE
TO
BE
EXCHANGED
THE
POINTERS
IN
THE
POINTER
ARRAY
ARE
EXCHANGED
NOT
THE
TEXT
LINES
THEMSELVES
THIS
ELIMINATES
THE
TWIN
PROBLEMS
OF
COMPLICATED
STORAGE
MANAGEMENT
AND
HIGH
OVERHEAD
THAT
WOULD
GO
WITH
MOVING
THE
LINES
THEMSELVES
THE
SORTING
PROCESS
HAS
THREE
STEPS
READ
ALL
THE
LINES
OF
INPUT
SORT
THEM
PRINT
THEM
IN
ORDER
AS
USUAL
IT
BEST
TO
DIVIDE
THE
PROGRAM
INTO
FUNCTIONS
THAT
MATCH
THIS
NATURAL
DIVISION
WITH
THE
MAIN
ROUTINE
CONTROLLING
THE
OTHER
FUNCTIONS
LET
US
DEFER
THE
SORTING
STEP
FOR
A
MOMENT
AND
CONCENTRATE
ON
THE
DATA
STRUCTURE
AND
THE
INPUT
AND
OUTPUT
THE
INPUT
ROUTINE
HAS
TO
COLLECT
AND
SAVE
THE
CHARACTERS
OF
EACH
LINE
AND
BUILD
AN
ARRAY
OF
POINTERS
TO
THE
LINES
IT
WILL
ALSO
HAVE
TO
COUNT
THE
NUMBER
OF
INPUT
LINES
SINCE
THAT
INFORMATION
IS
NEEDED
FOR
SORTING
AND
PRINTING
SINCE
THE
INPUT
FUNCTION
CAN
ONLY
COPE
WITH
A
FINITE
NUMBER
OF
INPUT
LINES
IT
CAN
RETURN
SOME
ILLEGAL
COUNT
LIKE
IF
TOO
MUCH
INPUT
IS
PRESENTED
THE
OUTPUT
ROUTINE
ONLY
HAS
TO
PRINT
THE
LINES
IN
THE
ORDER
IN
WHICH
THEY
APPEAR
IN
THE
ARRAY
OF
POINTERS
INCLUDE
STDIO
H
INCLUDE
STRING
H
DEFINE
MAXLINES
MAX
LINES
TO
BE
SORTED
CHAR
LINEPTR
MAXLINES
POINTERS
TO
TEXT
LINES
INT
READLINES
CHAR
LINEPTR
INT
NLINES
VOID
WRITELINES
CHAR
LINEPTR
INT
NLINES
VOID
QSORT
CHAR
LINEPTR
INT
LEFT
INT
RIGHT
SORT
INPUT
LINES
MAIN
INT
NLINES
NUMBER
OF
INPUT
LINES
READ
IF
NLINES
READLINES
LINEPTR
MAXLINES
QSORT
LINEPTR
NLINES
WRITELINES
LINEPTR
NLINES
RETURN
ELSE
PRINTF
ERROR
INPUT
TOO
BIG
TO
SORT
N
RETURN
DEFINE
MAXLEN
MAX
LENGTH
OF
ANY
INPUT
LINE
INT
GETLINE
CHAR
INT
CHAR
ALLOC
INT
READLINES
READ
INPUT
LINES
INT
READLINES
CHAR
LINEPTR
INT
MAXLINES
INT
LEN
NLINES
CHAR
P
LINE
MAXLEN
NLINES
WHILE
LEN
GETLINE
LINE
MAXLEN
IF
NLINES
MAXLINES
P
ALLOC
LEN
NULL
RETURN
ELSE
LINE
LEN
DELETE
NEWLINE
STRCPY
P
LINE
LINEPTR
NLINES
P
RETURN
NLINES
WRITELINES
WRITE
OUTPUT
LINES
VOID
WRITELINES
CHAR
LINEPTR
INT
NLINES
INT
I
FOR
I
I
NLINES
I
PRINTF
N
LINEPTR
I
THE
FUNCTION
GETLINE
IS
FROM
SECTION
THE
MAIN
NEW
THING
IS
THE
DECLARATION
FOR
LINEPTR
CHAR
LINEPTR
MAXLINES
SAYS
THAT
LINEPTR
IS
AN
ARRAY
OF
MAXLINES
ELEMENTS
EACH
ELEMENT
OF
WHICH
IS
A
POINTER
TO
A
CHAR
THAT
IS
LINEPTR
I
IS
A
CHARACTER
POINTER
AND
LINEPTR
I
IS
THE
CHARACTER
IT
POINTS
TO
THE
FIRST
CHARACTER
OF
THE
I
TH
SAVED
TEXT
LINE
SINCE
LINEPTR
IS
ITSELF
THE
NAME
OF
AN
ARRAY
IT
CAN
BE
TREATED
AS
A
POINTER
IN
THE
SAME
MANNER
AS
IN
OUR
EARLIER
EXAMPLES
AND
WRITELINES
CAN
BE
WRITTEN
INSTEAD
AS
WRITELINES
WRITE
OUTPUT
LINES
VOID
WRITELINES
CHAR
LINEPTR
INT
NLINES
WHILE
NLINES
PRINTF
N
LINEPTR
INITIALLY
LINEPTR
POINTS
TO
THE
FIRST
LINE
EACH
ELEMENT
ADVANCES
IT
TO
THE
NEXT
LINE
POINTER
WHILE
NLINES
IS
COUNTED
DOWN
WITH
INPUT
AND
OUTPUT
UNDER
CONTROL
WE
CAN
PROCEED
TO
SORTING
THE
QUICKSORT
FROM
CHAPTER
NEEDS
MINOR
CHANGES
THE
DECLARATIONS
HAVE
TO
BE
MODIFIED
AND
THE
COMPARISON
OPERATION
MUST
BE
DONE
BY
CALLING
STRCMP
THE
ALGORITHM
REMAINS
THE
SAME
WHICH
GIVES
US
SOME
CONFIDENCE
THAT
IT
WILL
STILL
WORK
QSORT
SORT
V
LEFT
V
RIGHT
INTO
INCREASING
ORDER
VOID
QSORT
CHAR
V
INT
LEFT
INT
RIGHT
INT
I
LAST
VOID
SWAP
CHAR
V
INT
I
INT
J
IF
LEFT
RIGHT
DO
NOTHING
IF
ARRAY
CONTAINS
RETURN
FEWER
THAN
TWO
ELEMENTS
SWAP
V
LEFT
LEFT
RIGHT
LAST
LEFT
FOR
I
LEFT
I
RIGHT
I
IF
STRCMP
V
I
V
LEFT
SWAP
V
LAST
I
SWAP
V
LEFT
LAST
QSORT
V
LEFT
LAST
QSORT
V
LAST
RIGHT
SIMILARLY
THE
SWAP
ROUTINE
NEEDS
ONLY
TRIVIAL
CHANGES
SWAP
INTERCHANGE
V
I
AND
V
J
VOID
SWAP
CHAR
V
INT
I
INT
J
CHAR
TEMP
TEMP
V
I
V
I
V
J
V
J
TEMP
SINCE
ANY
INDIVIDUAL
ELEMENT
OF
V
ALIAS
LINEPTR
IS
A
CHARACTER
POINTER
TEMP
MUST
BE
ALSO
SO
ONE
CAN
BE
COPIED
TO
THE
OTHER
EXERCISE
REWRITE
READLINES
TO
STORE
LINES
IN
AN
ARRAY
SUPPLIED
BY
MAIN
RATHER
THAN
CALLING
ALLOC
TO
MAINTAIN
STORAGE
HOW
MUCH
FASTER
IS
THE
PROGRAM
MULTI
DIMENSIONAL
ARRAYS
C
PROVIDES
RECTANGULAR
MULTI
DIMENSIONAL
ARRAYS
ALTHOUGH
IN
PRACTICE
THEY
ARE
MUCH
LESS
USED
THAN
ARRAYS
OF
POINTERS
IN
THIS
SECTION
WE
WILL
SHOW
SOME
OF
THEIR
PROPERTIES
CONSIDER
THE
PROBLEM
OF
DATE
CONVERSION
FROM
DAY
OF
THE
MONTH
TO
DAY
OF
THE
YEAR
AND
VICE
VERSA
FOR
EXAMPLE
MARCH
IS
THE
DAY
OF
A
NON
LEAP
YEAR
AND
THE
DAY
OF
A
LEAP
YEAR
LET
US
DEFINE
TWO
FUNCTIONS
TO
DO
THE
CONVERSIONS
CONVERTS
THE
MONTH
AND
DAY
INTO
THE
DAY
OF
THE
YEAR
AND
CONVERTS
THE
DAY
OF
THE
YEAR
INTO
THE
MONTH
AND
DAY
SINCE
THIS
LATTER
FUNCTION
COMPUTES
TWO
VALUES
THE
MONTH
AND
DAY
ARGUMENTS
WILL
BE
POINTERS
M
D
SETS
M
TO
AND
D
TO
FEBRUARY
THESE
FUNCTIONS
BOTH
NEED
THE
SAME
INFORMATION
A
TABLE
OF
THE
NUMBER
OF
DAYS
IN
EACH
MONTH
THIRTY
DAYS
HATH
SEPTEMBER
SINCE
THE
NUMBER
OF
DAYS
PER
MONTH
DIFFERS
FOR
LEAP
YEARS
AND
NON
LEAP
YEARS
IT
EASIER
TO
SEPARATE
THEM
INTO
TWO
ROWS
OF
A
TWO
DIMENSIONAL
ARRAY
THAN
TO
KEEP
TRACK
OF
WHAT
HAPPENS
TO
FEBRUARY
DURING
COMPUTATION
THE
ARRAY
AND
THE
FUNCTIONS
FOR
PERFORMING
THE
TRANSFORMATIONS
ARE
AS
FOLLOWS
STATIC
CHAR
DAYTAB
SET
DAY
OF
YEAR
FROM
MONTH
DAY
INT
INT
YEAR
INT
MONTH
INT
DAY
INT
I
LEAP
LEAP
YEAR
YEAR
YEAR
FOR
I
I
MONTH
I
DAY
DAYTAB
LEAP
I
RETURN
DAY
SET
MONTH
DAY
FROM
DAY
OF
YEAR
VOID
INT
YEAR
INT
YEARDAY
INT
PMONTH
INT
PDAY
INT
I
LEAP
LEAP
YEAR
YEAR
YEAR
FOR
I
YEARDAY
DAYTAB
LEAP
I
I
YEARDAY
DAYTAB
LEAP
I
PMONTH
I
PDAY
YEARDAY
RECALL
THAT
THE
ARITHMETIC
VALUE
OF
A
LOGICAL
EXPRESSION
SUCH
AS
THE
ONE
FOR
LEAP
IS
EITHER
ZERO
FALSE
OR
ONE
TRUE
SO
IT
CAN
BE
USED
AS
A
SUBSCRIPT
OF
THE
ARRAY
DAYTAB
THE
ARRAY
DAYTAB
HAS
TO
BE
EXTERNAL
TO
BOTH
AND
SO
THEY
CAN
BOTH
USE
IT
WE
MADE
IT
CHAR
TO
ILLUSTRATE
A
LEGITIMATE
USE
OF
CHAR
FOR
STORING
SMALL
NON
CHARACTER
INTEGERS
DAYTAB
IS
THE
FIRST
TWO
DIMENSIONAL
ARRAY
WE
HAVE
DEALT
WITH
IN
C
A
TWO
DIMENSIONAL
ARRAY
IS
REALLY
A
ONE
DIMENSIONAL
ARRAY
EACH
OF
WHOSE
ELEMENTS
IS
AN
ARRAY
HENCE
SUBSCRIPTS
ARE
WRITTEN
AS
DAYTAB
I
J
ROW
COL
RATHER
THAN
DAYTAB
I
J
WRONG
OTHER
THAN
THIS
NOTATIONAL
DISTINCTION
A
TWO
DIMENSIONAL
ARRAY
CAN
BE
TREATED
IN
MUCH
THE
SAME
WAY
AS
IN
OTHER
LANGUAGES
ELEMENTS
ARE
STORED
BY
ROWS
SO
THE
RIGHTMOST
SUBSCRIPT
OR
COLUMN
VARIES
FASTEST
AS
ELEMENTS
ARE
ACCESSED
IN
STORAGE
ORDER
AN
ARRAY
IS
INITIALIZED
BY
A
LIST
OF
INITIALIZERS
IN
BRACES
EACH
ROW
OF
A
TWO
DIMENSIONAL
ARRAY
IS
INITIALIZED
BY
A
CORRESPONDING
SUB
LIST
WE
STARTED
THE
ARRAY
DAYTAB
WITH
A
COLUMN
OF
ZERO
SO
THAT
MONTH
NUMBERS
CAN
RUN
FROM
THE
NATURAL
TO
INSTEAD
OF
TO
SINCE
SPACE
IS
NOT
AT
A
PREMIUM
HERE
THIS
IS
CLEARER
THAN
ADJUSTING
THE
INDICES
IF
A
TWO
DIMENSIONAL
ARRAY
IS
TO
BE
PASSED
TO
A
FUNCTION
THE
PARAMETER
DECLARATION
IN
THE
FUNCTION
MUST
INCLUDE
THE
NUMBER
OF
COLUMNS
THE
NUMBER
OF
ROWS
IS
IRRELEVANT
SINCE
WHAT
IS
PASSED
IS
AS
BEFORE
A
POINTER
TO
AN
ARRAY
OF
ROWS
WHERE
EACH
ROW
IS
AN
ARRAY
OF
INTS
IN
THIS
PARTICULAR
CASE
IT
IS
A
POINTER
TO
OBJECTS
THAT
ARE
ARRAYS
OF
INTS
THUS
IF
THE
ARRAY
DAYTAB
IS
TO
BE
PASSED
TO
A
FUNCTION
F
THE
DECLARATION
OF
F
WOULD
BE
F
INT
DAYTAB
IT
COULD
ALSO
BE
F
INT
DAYTAB
SINCE
THE
NUMBER
OF
ROWS
IS
IRRELEVANT
OR
IT
COULD
BE
F
INT
DAYTAB
WHICH
SAYS
THAT
THE
PARAMETER
IS
A
POINTER
TO
AN
ARRAY
OF
INTEGERS
THE
PARENTHESES
ARE
NECESSARY
SINCE
BRACKETS
HAVE
HIGHER
PRECEDENCE
THAN
WITHOUT
PARENTHESES
THE
DECLARATION
INT
DAYTAB
IS
AN
ARRAY
OF
POINTERS
TO
INTEGERS
MORE
GENERALLY
ONLY
THE
FIRST
DIMENSION
SUBSCRIPT
OF
AN
ARRAY
IS
FREE
ALL
THE
OTHERS
HAVE
TO
BE
SPECIFIED
SECTION
HAS
A
FURTHER
DISCUSSION
OF
COMPLICATED
DECLARATIONS
EXERCISE
THERE
IS
NO
ERROR
CHECKING
IN
OR
REMEDY
THIS
DEFECT
INITIALIZATION
OF
POINTER
ARRAYS
CONSIDER
THE
PROBLEM
OF
WRITING
A
FUNCTION
N
WHICH
RETURNS
A
POINTER
TO
A
CHARACTER
STRING
CONTAINING
THE
NAME
OF
THE
N
TH
MONTH
THIS
IS
AN
IDEAL
APPLICATION
FOR
AN
INTERNAL
STATIC
ARRAY
CONTAINS
A
PRIVATE
ARRAY
OF
CHARACTER
STRINGS
AND
RETURNS
A
POINTER
TO
THE
PROPER
ONE
WHEN
CALLED
THIS
SECTION
SHOWS
HOW
THAT
ARRAY
OF
NAMES
IS
INITIALIZED
THE
SYNTAX
IS
SIMILAR
TO
PREVIOUS
INITIALIZATIONS
RETURN
NAME
OF
N
TH
MONTH
CHAR
INT
N
STATIC
CHAR
NAME
ILLEGAL
MONTH
JANUARY
FEBRUARY
MARCH
APRIL
MAY
JUNE
JULY
AUGUST
SEPTEMBER
OCTOBER
NOVEMBER
DECEMBER
RETURN
N
N
NAME
NAME
N
THE
DECLARATION
OF
NAME
WHICH
IS
AN
ARRAY
OF
CHARACTER
POINTERS
IS
THE
SAME
AS
LINEPTR
IN
THE
SORTING
EXAMPLE
THE
INITIALIZER
IS
A
LIST
OF
CHARACTER
STRINGS
EACH
IS
ASSIGNED
TO
THE
CORRESPONDING
POSITION
IN
THE
ARRAY
THE
CHARACTERS
OF
THE
I
TH
STRING
ARE
PLACED
SOMEWHERE
AND
A
POINTER
TO
THEM
IS
STORED
IN
NAME
I
SINCE
THE
SIZE
OF
THE
ARRAY
NAME
IS
NOT
SPECIFIED
THE
COMPILER
COUNTS
THE
INITIALIZERS
AND
FILLS
IN
THE
CORRECT
NUMBER
POINTERS
VS
MULTI
DIMENSIONAL
ARRAYS
NEWCOMERS
TO
C
ARE
SOMETIMES
CONFUSED
ABOUT
THE
DIFFERENCE
BETWEEN
A
TWO
DIMENSIONAL
ARRAY
AND
AN
ARRAY
OF
POINTERS
SUCH
AS
NAME
IN
THE
EXAMPLE
ABOVE
GIVEN
THE
DEFINITIONS
INT
A
INT
B
THEN
A
AND
B
ARE
BOTH
SYNTACTICALLY
LEGAL
REFERENCES
TO
A
SINGLE
INT
BUT
A
IS
A
TRUE
TWO
DIMENSIONAL
ARRAY
INT
SIZED
LOCATIONS
HAVE
BEEN
SET
ASIDE
AND
THE
CONVENTIONAL
RECTANGULAR
SUBSCRIPT
CALCULATION
ROW
COL
IS
USED
TO
FIND
THE
ELEMENT
A
ROW
COL
FOR
B
HOWEVER
THE
DEFINITION
ONLY
ALLOCATES
POINTERS
AND
DOES
NOT
INITIALIZE
THEM
INITIALIZATION
MUST
BE
DONE
EXPLICITLY
EITHER
STATICALLY
OR
WITH
CODE
ASSUMING
THAT
EACH
ELEMENT
OF
B
DOES
POINT
TO
A
TWENTY
ELEMENT
ARRAY
THEN
THERE
WILL
BE
INTS
SET
ASIDE
PLUS
TEN
CELLS
FOR
THE
POINTERS
THE
IMPORTANT
ADVANTAGE
OF
THE
POINTER
ARRAY
IS
THAT
THE
ROWS
OF
THE
ARRAY
MAY
BE
OF
DIFFERENT
LENGTHS
THAT
IS
EACH
ELEMENT
OF
B
NEED
NOT
POINT
TO
A
TWENTY
ELEMENT
VECTOR
SOME
MAY
POINT
TO
TWO
ELEMENTS
SOME
TO
FIFTY
AND
SOME
TO
NONE
AT
ALL
ALTHOUGH
WE
HAVE
PHRASED
THIS
DISCUSSION
IN
TERMS
OF
INTEGERS
BY
FAR
THE
MOST
FREQUENT
USE
OF
ARRAYS
OF
POINTERS
IS
TO
STORE
CHARACTER
STRINGS
OF
DIVERSE
LENGTHS
AS
IN
THE
FUNCTION
COMPARE
THE
DECLARATION
AND
PICTURE
FOR
AN
ARRAY
OF
POINTERS
CHAR
NAME
ILLEGAL
MONTH
JAN
FEB
MAR
WITH
THOSE
FOR
A
TWO
DIMENSIONAL
ARRAY
CHAR
ANAME
ILLEGAL
MONTH
JAN
FEB
MAR
EXERCISE
REWRITE
THE
ROUTINES
AND
WITH
POINTERS
INSTEAD
OF
INDEXING
COMMAND
LINE
ARGUMENTS
IN
ENVIRONMENTS
THAT
SUPPORT
C
THERE
IS
A
WAY
TO
PASS
COMMAND
LINE
ARGUMENTS
OR
PARAMETERS
TO
A
PROGRAM
WHEN
IT
BEGINS
EXECUTING
WHEN
MAIN
IS
CALLED
IT
IS
CALLED
WITH
TWO
ARGUMENTS
THE
FIRST
CONVENTIONALLY
CALLED
ARGC
FOR
ARGUMENT
COUNT
IS
THE
NUMBER
OF
COMMAND
LINE
ARGUMENTS
THE
PROGRAM
WAS
INVOKED
WITH
THE
SECOND
ARGV
FOR
ARGUMENT
VECTOR
IS
A
POINTER
TO
AN
ARRAY
OF
CHARACTER
STRINGS
THAT
CONTAIN
THE
ARGUMENTS
ONE
PER
STRING
WE
CUSTOMARILY
USE
MULTIPLE
LEVELS
OF
POINTERS
TO
MANIPULATE
THESE
CHARACTER
STRINGS
THE
SIMPLEST
ILLUSTRATION
IS
THE
PROGRAM
ECHO
WHICH
ECHOES
ITS
COMMAND
LINE
ARGUMENTS
ON
A
SINGLE
LINE
SEPARATED
BY
BLANKS
THAT
IS
THE
COMMAND
ECHO
HELLO
WORLD
PRINTS
THE
OUTPUT
HELLO
WORLD
BY
CONVENTION
ARGV
IS
THE
NAME
BY
WHICH
THE
PROGRAM
WAS
INVOKED
SO
ARGC
IS
AT
LEAST
IF
ARGC
IS
THERE
ARE
NO
COMMAND
LINE
ARGUMENTS
AFTER
THE
PROGRAM
NAME
IN
THE
EXAMPLE
ABOVE
ARGC
IS
AND
ARGV
ARGV
AND
ARGV
ARE
ECHO
HELLO
AND
WORLD
RESPECTIVELY
THE
FIRST
OPTIONAL
ARGUMENT
IS
ARGV
AND
THE
LAST
IS
ARGV
ARGC
ADDITIONALLY
THE
STANDARD
REQUIRES
THAT
ARGV
ARGC
BE
A
NULL
POINTER
THE
FIRST
VERSION
OF
ECHO
TREATS
ARGV
AS
AN
ARRAY
OF
CHARACTER
POINTERS
INCLUDE
STDIO
H
ECHO
COMMAND
LINE
ARGUMENTS
VERSION
MAIN
INT
ARGC
CHAR
ARGV
INT
I
FOR
I
I
ARGC
I
PRINTF
ARGV
I
I
ARGC
PRINTF
N
RETURN
SINCE
ARGV
IS
A
POINTER
TO
AN
ARRAY
OF
POINTERS
WE
CAN
MANIPULATE
THE
POINTER
RATHER
THAN
INDEX
THE
ARRAY
THIS
NEXT
VARIANT
IS
BASED
ON
INCREMENTING
ARGV
WHICH
IS
A
POINTER
TO
POINTER
TO
CHAR
WHILE
ARGC
IS
COUNTED
DOWN
INCLUDE
STDIO
H
ECHO
COMMAND
LINE
ARGUMENTS
VERSION
MAIN
INT
ARGC
CHAR
ARGV
WHILE
ARGC
PRINTF
ARGV
ARGC
PRINTF
N
RETURN
SINCE
ARGV
IS
A
POINTER
TO
THE
BEGINNING
OF
THE
ARRAY
OF
ARGUMENT
STRINGS
INCREMENTING
IT
BY
ARGV
MAKES
IT
POINT
AT
THE
ORIGINAL
ARGV
INSTEAD
OF
ARGV
EACH
SUCCESSIVE
INCREMENT
MOVES
IT
ALONG
TO
THE
NEXT
ARGUMENT
ARGV
IS
THEN
THE
POINTER
TO
THAT
ARGUMENT
AT
THE
SAME
TIME
ARGC
IS
DECREMENTED
WHEN
IT
BECOMES
ZERO
THERE
ARE
NO
ARGUMENTS
LEFT
TO
PRINT
ALTERNATIVELY
WE
COULD
WRITE
THE
PRINTF
STATEMENT
AS
PRINTF
ARGC
ARGV
THIS
SHOWS
THAT
THE
FORMAT
ARGUMENT
OF
PRINTF
CAN
BE
AN
EXPRESSION
TOO
AS
A
SECOND
EXAMPLE
LET
US
MAKE
SOME
ENHANCEMENTS
TO
THE
PATTERN
FINDING
PROGRAM
FROM
SECTION
IF
YOU
RECALL
WE
WIRED
THE
SEARCH
PATTERN
DEEP
INTO
THE
PROGRAM
AN
OBVIOUSLY
UNSATISFACTORY
ARRANGEMENT
FOLLOWING
THE
LEAD
OF
THE
UNIX
PROGRAM
GREP
LET
US
ENHANCE
THE
PROGRAM
SO
THE
PATTERN
TO
BE
MATCHED
IS
SPECIFIED
BY
THE
FIRST
ARGUMENT
ON
THE
COMMAND
LINE
INCLUDE
STDIO
H
INCLUDE
STRING
H
DEFINE
MAXLINE
INT
GETLINE
CHAR
LINE
INT
MAX
FIND
PRINT
LINES
THAT
MATCH
PATTERN
FROM
ARG
MAIN
INT
ARGC
CHAR
ARGV
CHAR
LINE
MAXLINE
INT
FOUND
IF
ARGC
PRINTF
USAGE
FIND
PATTERN
N
ELSE
WHILE
GETLINE
LINE
MAXLINE
IF
STRSTR
LINE
ARGV
NULL
PRINTF
LINE
FOUND
RETURN
FOUND
THE
STANDARD
LIBRARY
FUNCTION
STRSTR
T
RETURNS
A
POINTER
TO
THE
FIRST
OCCURRENCE
OF
THE
STRING
T
IN
THE
STRING
OR
NULL
IF
THERE
IS
NONE
IT
IS
DECLARED
IN
STRING
H
THE
MODEL
CAN
NOW
BE
ELABORATED
TO
ILLUSTRATE
FURTHER
POINTER
CONSTRUCTIONS
SUPPOSE
WE
WANT
TO
ALLOW
TWO
OPTIONAL
ARGUMENTS
ONE
SAYS
PRINT
ALL
THE
LINES
EXCEPT
THOSE
THAT
MATCH
THE
PATTERN
THE
SECOND
SAYS
PRECEDE
EACH
PRINTED
LINE
BY
ITS
LINE
NUMBER
A
COMMON
CONVENTION
FOR
C
PROGRAMS
ON
UNIX
SYSTEMS
IS
THAT
AN
ARGUMENT
THAT
BEGINS
WITH
A
MINUS
SIGN
INTRODUCES
AN
OPTIONAL
FLAG
OR
PARAMETER
IF
WE
CHOOSE
X
FOR
EXCEPT
TO
SIGNAL
THE
INVERSION
AND
N
NUMBER
TO
REQUEST
LINE
NUMBERING
THEN
THE
COMMAND
FIND
X
NPATTERN
WILL
PRINT
EACH
LINE
THAT
DOESN
T
MATCH
THE
PATTERN
PRECEDED
BY
ITS
LINE
NUMBER
OPTIONAL
ARGUMENTS
SHOULD
BE
PERMITTED
IN
ANY
ORDER
AND
THE
REST
OF
THE
PROGRAM
SHOULD
BE
INDEPENDENT
OF
THE
NUMBER
OF
ARGUMENTS
THAT
WE
PRESENT
FURTHERMORE
IT
IS
CONVENIENT
FOR
USERS
IF
OPTION
ARGUMENTS
CAN
BE
COMBINED
AS
IN
FIND
NX
PATTERN
HERE
IS
THE
PROGRAM
INCLUDE
STDIO
H
INCLUDE
STRING
H
DEFINE
MAXLINE
INT
GETLINE
CHAR
LINE
INT
MAX
FIND
PRINT
LINES
THAT
MATCH
PATTERN
FROM
ARG
MAIN
INT
ARGC
CHAR
ARGV
CHAR
LINE
MAXLINE
LONG
LINENO
INT
C
EXCEPT
NUMBER
FOUND
WHILE
ARGC
ARGV
WHILE
C
ARGV
SWITCH
C
CASE
X
EXCEPT
BREAK
CASE
N
NUMBER
BREAK
DEFAULT
PRINTF
FIND
ILLEGAL
OPTION
C
N
C
ARGC
FOUND
BREAK
IF
ARGC
PRINTF
USAGE
FIND
X
N
PATTERN
N
ELSE
WHILE
GETLINE
LINE
MAXLINE
LINENO
IF
STRSTR
LINE
ARGV
NULL
EXCEPT
IF
NUMBER
PRINTF
LD
LINENO
PRINTF
LINE
FOUND
RETURN
FOUND
ARGC
IS
DECREMENTED
AND
ARGV
IS
INCREMENTED
BEFORE
EACH
OPTIONAL
ARGUMENT
AT
THE
END
OF
THE
LOOP
IF
THERE
ARE
NO
ERRORS
ARGC
TELLS
HOW
MANY
ARGUMENTS
REMAIN
UNPROCESSED
AND
ARGV
POINTS
TO
THE
FIRST
OF
THESE
THUS
ARGC
SHOULD
BE
AND
ARGV
SHOULD
POINT
AT
THE
PATTERN
NOTICE
THAT
ARGV
IS
A
POINTER
TO
AN
ARGUMENT
STRING
SO
ARGV
IS
ITS
FIRST
CHARACTER
AN
ALTERNATE
VALID
FORM
WOULD
BE
ARGV
BECAUSE
BINDS
TIGHTER
THAN
AND
THE
PARENTHESES
ARE
NECESSARY
WITHOUT
THEM
THE
EXPRESSION
WOULD
BE
TAKEN
AS
ARGV
IN
FACT
THAT
IS
WHAT
WE
HAVE
USED
IN
THE
INNER
LOOP
WHERE
THE
TASK
IS
TO
WALK
ALONG
A
SPECIFIC
ARGUMENT
STRING
IN
THE
INNER
LOOP
THE
EXPRESSION
ARGV
INCREMENTS
THE
POINTER
ARGV
IT
IS
RARE
THAT
ONE
USES
POINTER
EXPRESSIONS
MORE
COMPLICATED
THAN
THESE
IN
SUCH
CASES
BREAKING
THEM
INTO
TWO
OR
THREE
STEPS
WILL
BE
MORE
INTUITIVE
EXERCISE
WRITE
THE
PROGRAM
EXPR
WHICH
EVALUATES
A
REVERSE
POLISH
EXPRESSION
FROM
THE
COMMAND
LINE
WHERE
EACH
OPERATOR
OR
OPERAND
IS
A
SEPARATE
ARGUMENT
FOR
EXAMPLE
EXPR
EVALUATES
EXERCISE
MODIFY
THE
PROGRAM
ENTAB
AND
DETAB
WRITTEN
AS
EXERCISES
IN
CHAPTER
TO
ACCEPT
A
LIST
OF
TAB
STOPS
AS
ARGUMENTS
USE
THE
DEFAULT
TAB
SETTINGS
IF
THERE
ARE
NO
ARGUMENTS
EXERCISE
EXTEND
ENTAB
AND
DETAB
TO
ACCEPT
THE
SHORTHAND
ENTAB
M
N
TO
MEAN
TAB
STOPS
EVERY
N
COLUMNS
STARTING
AT
COLUMN
M
CHOOSE
CONVENIENT
FOR
THE
USER
DEFAULT
BEHAVIOR
EXERCISE
WRITE
THE
PROGRAM
TAIL
WHICH
PRINTS
THE
LAST
N
LINES
OF
ITS
INPUT
BY
DEFAULT
N
IS
SET
TO
LET
US
SAY
BUT
IT
CAN
BE
CHANGED
BY
AN
OPTIONAL
ARGUMENT
SO
THAT
TAIL
N
PRINTS
THE
LAST
N
LINES
THE
PROGRAM
SHOULD
BEHAVE
RATIONALLY
NO
MATTER
HOW
UNREASONABLE
THE
INPUT
OR
THE
VALUE
OF
N
WRITE
THE
PROGRAM
SO
IT
MAKES
THE
BEST
USE
OF
AVAILABLE
STORAGE
LINES
SHOULD
BE
STORED
AS
IN
THE
SORTING
PROGRAM
OF
SECTION
NOT
IN
A
TWO
DIMENSIONAL
ARRAY
OF
FIXED
SIZE
POINTERS
TO
FUNCTIONS
IN
C
A
FUNCTION
ITSELF
IS
NOT
A
VARIABLE
BUT
IT
IS
POSSIBLE
TO
DEFINE
POINTERS
TO
FUNCTIONS
WHICH
CAN
BE
ASSIGNED
PLACED
IN
ARRAYS
PASSED
TO
FUNCTIONS
RETURNED
BY
FUNCTIONS
AND
SO
ON
WE
WILL
ILLUSTRATE
THIS
BY
MODIFYING
THE
SORTING
PROCEDURE
WRITTEN
EARLIER
IN
THIS
CHAPTER
SO
THAT
IF
THE
OPTIONAL
ARGUMENT
N
IS
GIVEN
IT
WILL
SORT
THE
INPUT
LINES
NUMERICALLY
INSTEAD
OF
LEXICOGRAPHICALLY
A
SORT
OFTEN
CONSISTS
OF
THREE
PARTS
A
COMPARISON
THAT
DETERMINES
THE
ORDERING
OF
ANY
PAIR
OF
OBJECTS
AN
EXCHANGE
THAT
REVERSES
THEIR
ORDER
AND
A
SORTING
ALGORITHM
THAT
MAKES
COMPARISONS
AND
EXCHANGES
UNTIL
THE
OBJECTS
ARE
IN
ORDER
THE
SORTING
ALGORITHM
IS
INDEPENDENT
OF
THE
COMPARISON
AND
EXCHANGE
OPERATIONS
SO
BY
PASSING
DIFFERENT
COMPARISON
AND
EXCHANGE
FUNCTIONS
TO
IT
WE
CAN
ARRANGE
TO
SORT
BY
DIFFERENT
CRITERIA
THIS
IS
THE
APPROACH
TAKEN
IN
OUR
NEW
SORT
LEXICOGRAPHIC
COMPARISON
OF
TWO
LINES
IS
DONE
BY
STRCMP
AS
BEFORE
WE
WILL
ALSO
NEED
A
ROUTINE
NUMCMP
THAT
COMPARES
TWO
LINES
ON
THE
BASIS
OF
NUMERIC
VALUE
AND
RETURNS
THE
SAME
KIND
OF
CONDITION
INDICATION
AS
STRCMP
DOES
THESE
FUNCTIONS
ARE
DECLARED
AHEAD
OF
MAIN
AND
A
POINTER
TO
THE
APPROPRIATE
ONE
IS
PASSED
TO
QSORT
WE
HAVE
SKIMPED
ON
ERROR
PROCESSING
FOR
ARGUMENTS
SO
AS
TO
CONCENTRATE
ON
THE
MAIN
ISSUES
INCLUDE
STDIO
H
INCLUDE
STRING
H
DEFINE
MAXLINES
MAX
LINES
TO
BE
SORTED
CHAR
LINEPTR
MAXLINES
POINTERS
TO
TEXT
LINES
INT
READLINES
CHAR
LINEPTR
INT
NLINES
VOID
WRITELINES
CHAR
LINEPTR
INT
NLINES
VOID
QSORT
VOID
LINEPTR
INT
LEFT
INT
RIGHT
INT
COMP
VOID
VOID
INT
NUMCMP
CHAR
CHAR
SORT
INPUT
LINES
MAIN
INT
ARGC
CHAR
ARGV
INT
NLINES
NUMBER
OF
INPUT
LINES
READ
INT
NUMERIC
IF
NUMERIC
SORT
IF
ARGC
STRCMP
ARGV
N
NUMERIC
IF
NLINES
READLINES
LINEPTR
MAXLINES
QSORT
VOID
LINEPTR
NLINES
INT
VOID
VOID
NUMERIC
NUMCMP
STRCMP
WRITELINES
LINEPTR
NLINES
RETURN
ELSE
PRINTF
INPUT
TOO
BIG
TO
SORT
N
RETURN
IN
THE
CALL
TO
QSORT
STRCMP
AND
NUMCMP
ARE
ADDRESSES
OF
FUNCTIONS
SINCE
THEY
ARE
KNOWN
TO
BE
FUNCTIONS
THE
IS
NOT
NECESSARY
IN
THE
SAME
WAY
THAT
IT
IS
NOT
NEEDED
BEFORE
AN
ARRAY
NAME
WE
HAVE
WRITTEN
QSORT
SO
IT
CAN
PROCESS
ANY
DATA
TYPE
NOT
JUST
CHARACTER
STRINGS
AS
INDICATED
BY
THE
FUNCTION
PROTOTYPE
QSORT
EXPECTS
AN
ARRAY
OF
POINTERS
TWO
INTEGERS
AND
A
FUNCTION
WITH
TWO
POINTER
ARGUMENTS
THE
GENERIC
POINTER
TYPE
VOID
IS
USED
FOR
THE
POINTER
ARGUMENTS
ANY
POINTER
CAN
BE
CAST
TO
VOID
AND
BACK
AGAIN
WITHOUT
LOSS
OF
INFORMATION
SO
WE
CAN
CALL
QSORT
BY
CASTING
ARGUMENTS
TO
VOID
THE
ELABORATE
CAST
OF
THE
FUNCTION
ARGUMENT
CASTS
THE
ARGUMENTS
OF
THE
COMPARISON
FUNCTION
THESE
WILL
GENERALLY
HAVE
NO
EFFECT
ON
ACTUAL
REPRESENTATION
BUT
ASSURE
THE
COMPILER
THAT
ALL
IS
WELL
QSORT
SORT
V
LEFT
V
RIGHT
INTO
INCREASING
ORDER
VOID
QSORT
VOID
V
INT
LEFT
INT
RIGHT
INT
COMP
VOID
VOID
INT
I
LAST
VOID
SWAP
VOID
V
INT
INT
IF
LEFT
RIGHT
DO
NOTHING
IF
ARRAY
CONTAINS
RETURN
FEWER
THAN
TWO
ELEMENTS
SWAP
V
LEFT
LEFT
RIGHT
LAST
LEFT
FOR
I
LEFT
I
RIGHT
I
IF
COMP
V
I
V
LEFT
SWAP
V
LAST
I
SWAP
V
LEFT
LAST
QSORT
V
LEFT
LAST
COMP
QSORT
V
LAST
RIGHT
COMP
THE
DECLARATIONS
SHOULD
BE
STUDIED
WITH
SOME
CARE
THE
FOURTH
PARAMETER
OF
QSORT
IS
INT
COMP
VOID
VOID
WHICH
SAYS
THAT
COMP
IS
A
POINTER
TO
A
FUNCTION
THAT
HAS
TWO
VOID
ARGUMENTS
AND
RETURNS
AN
INT
THE
USE
OF
COMP
IN
THE
LINE
IF
COMP
V
I
V
LEFT
IS
CONSISTENT
WITH
THE
DECLARATION
COMP
IS
A
POINTER
TO
A
FUNCTION
COMP
IS
THE
FUNCTION
AND
COMP
V
I
V
LEFT
IS
THE
CALL
TO
IT
THE
PARENTHESES
ARE
NEEDED
SO
THE
COMPONENTS
ARE
CORRECTLY
ASSOCIATED
WITHOUT
THEM
INT
COMP
VOID
VOID
WRONG
SAYS
THAT
COMP
IS
A
FUNCTION
RETURNING
A
POINTER
TO
AN
INT
WHICH
IS
VERY
DIFFERENT
WE
HAVE
ALREADY
SHOWN
STRCMP
WHICH
COMPARES
TWO
STRINGS
HERE
IS
NUMCMP
WHICH
COMPARES
TWO
STRINGS
ON
A
LEADING
NUMERIC
VALUE
COMPUTED
BY
CALLING
ATOF
INCLUDE
STDLIB
H
NUMCMP
COMPARE
AND
NUMERICALLY
INT
NUMCMP
CHAR
CHAR
DOUBLE
ATOF
ATOF
IF
RETURN
ELSE
IF
RETURN
ELSE
RETURN
THE
SWAP
FUNCTION
WHICH
EXCHANGES
TWO
POINTERS
IS
IDENTICAL
TO
WHAT
WE
PRESENTED
EARLIER
IN
THE
CHAPTER
EXCEPT
THAT
THE
DECLARATIONS
ARE
CHANGED
TO
VOID
VOID
SWAP
VOID
V
INT
I
INT
J
VOID
TEMP
TEMP
V
I
V
I
V
J
V
J
TEMP
A
VARIETY
OF
OTHER
OPTIONS
CAN
BE
ADDED
TO
THE
SORTING
PROGRAM
SOME
MAKE
CHALLENGING
EXERCISES
EXERCISE
MODIFY
THE
SORT
PROGRAM
TO
HANDLE
A
R
FLAG
WHICH
INDICATES
SORTING
IN
REVERSE
DECREASING
ORDER
BE
SURE
THAT
R
WORKS
WITH
N
EXERCISE
ADD
THE
OPTION
F
TO
FOLD
UPPER
AND
LOWER
CASE
TOGETHER
SO
THAT
CASE
DISTINCTIONS
ARE
NOT
MADE
DURING
SORTING
FOR
EXAMPLE
A
AND
A
COMPARE
EQUAL
EXERCISE
ADD
THE
D
DIRECTORY
ORDER
OPTION
WHICH
MAKES
COMPARISONS
ONLY
ON
LETTERS
NUMBERS
AND
BLANKS
MAKE
SURE
IT
WORKS
IN
CONJUNCTION
WITH
F
EXERCISE
ADD
A
FIELD
SEARCHING
CAPABILITY
SO
SORTING
MAY
BEE
DONE
ON
FIELDS
WITHIN
LINES
EACH
FIELD
SORTED
ACCORDING
TO
AN
INDEPENDENT
SET
OF
OPTIONS
THE
INDEX
FOR
THIS
BOOK
WAS
SORTED
WITH
DF
FOR
THE
INDEX
CATEGORY
AND
N
FOR
THE
PAGE
NUMBERS
COMPLICATED
DECLARATIONS
C
IS
SOMETIMES
CASTIGATED
FOR
THE
SYNTAX
OF
ITS
DECLARATIONS
PARTICULARLY
ONES
THAT
INVOLVE
POINTERS
TO
FUNCTIONS
THE
SYNTAX
IS
AN
ATTEMPT
TO
MAKE
THE
DECLARATION
AND
THE
USE
AGREE
IT
WORKS
WELL
FOR
SIMPLE
CASES
BUT
IT
CAN
BE
CONFUSING
FOR
THE
HARDER
ONES
BECAUSE
DECLARATIONS
CANNOT
BE
READ
LEFT
TO
RIGHT
AND
BECAUSE
PARENTHESES
ARE
OVER
USED
THE
DIFFERENCE
BETWEEN
INT
F
F
FUNCTION
RETURNING
POINTER
TO
INT
AND
INT
PF
PF
POINTER
TO
FUNCTION
RETURNING
INT
ILLUSTRATES
THE
PROBLEM
IS
A
PREFIX
OPERATOR
AND
IT
HAS
LOWER
PRECEDENCE
THAN
SO
PARENTHESES
ARE
NECESSARY
TO
FORCE
THE
PROPER
ASSOCIATION
ALTHOUGH
TRULY
COMPLICATED
DECLARATIONS
RARELY
ARISE
IN
PRACTICE
IT
IS
IMPORTANT
TO
KNOW
HOW
TO
UNDERSTAND
THEM
AND
IF
NECESSARY
HOW
TO
CREATE
THEM
ONE
GOOD
WAY
TO
SYNTHESIZE
DECLARATIONS
IS
IN
SMALL
STEPS
WITH
TYPEDEF
WHICH
IS
DISCUSSED
IN
SECTION
AS
AN
ALTERNATIVE
IN
THIS
SECTION
WE
WILL
PRESENT
A
PAIR
OF
PROGRAMS
THAT
CONVERT
FROM
VALID
C
TO
A
WORD
DESCRIPTION
AND
BACK
AGAIN
THE
WORD
DESCRIPTION
READS
LEFT
TO
RIGHT
THE
FIRST
DCL
IS
THE
MORE
COMPLEX
IT
CONVERTS
A
C
DECLARATION
INTO
A
WORD
DESCRIPTION
AS
IN
THESE
EXAMPLES
CHAR
ARGV
ARGV
POINTER
TO
CHAR
INT
DAYTAB
DAYTAB
POINTER
TO
ARRAY
OF
INT
INT
DAYTAB
DAYTAB
ARRAY
OF
POINTER
TO
INT
VOID
COMP
COMP
FUNCTION
RETURNING
POINTER
TO
VOID
VOID
COMP
COMP
POINTER
TO
FUNCTION
RETURNING
VOID
CHAR
X
X
FUNCTION
RETURNING
POINTER
TO
ARRAY
OF
POINTER
TO
FUNCTION
RETURNING
CHAR
CHAR
X
X
ARRAY
OF
POINTER
TO
FUNCTION
RETURNING
POINTER
TO
ARRAY
OF
CHAR
DCL
IS
BASED
ON
THE
GRAMMAR
THAT
SPECIFIES
A
DECLARATOR
WHICH
IS
SPELLED
OUT
PRECISELY
IN
APPENDIX
A
SECTION
THIS
IS
A
SIMPLIFIED
FORM
DCL
OPTIONAL
DIRECT
DCL
DIRECT
DCL
NAME
DCL
DIRECT
DCL
DIRECT
DCL
OPTIONAL
SIZE
IN
WORDS
A
DCL
IS
A
DIRECT
DCL
PERHAPS
PRECEDED
BY
A
DIRECT
DCL
IS
A
NAME
OR
A
PARENTHESIZED
DCL
OR
A
DIRECT
DCL
FOLLOWED
BY
PARENTHESES
OR
A
DIRECT
DCL
FOLLOWED
BY
BRACKETS
WITH
AN
OPTIONAL
SIZE
THIS
GRAMMAR
CAN
BE
USED
TO
PARSE
FUNCTIONS
FOR
INSTANCE
CONSIDER
THIS
DECLARATOR
PFA
PFA
WILL
BE
IDENTIFIED
AS
A
NAME
AND
THUS
AS
A
DIRECT
DCL
THEN
PFA
IS
ALSO
A
DIRECT
DCL
THEN
PFA
IS
RECOGNIZED
AS
A
DCL
SO
PFA
IS
A
DIRECT
DCL
THEN
PFA
IS
A
DIRECT
DCL
AND
THUS
A
DCL
WE
CAN
ALSO
ILLUSTRATE
THE
PARSE
WITH
A
TREE
LIKE
THIS
WHERE
DIRECT
DCL
HAS
BEEN
ABBREVIATED
TO
DIR
DCL
THE
HEART
OF
THE
DCL
PROGRAM
IS
A
PAIR
OF
FUNCTIONS
DCL
AND
DIRDCL
THAT
PARSE
A
DECLARATION
ACCORDING
TO
THIS
GRAMMAR
BECAUSE
THE
GRAMMAR
IS
RECURSIVELY
DEFINED
THE
FUNCTIONS
CALL
EACH
OTHER
RECURSIVELY
AS
THEY
RECOGNIZE
PIECES
OF
A
DECLARATION
THE
PROGRAM
IS
CALLED
A
RECURSIVE
DESCENT
PARSER
DCL
PARSE
A
DECLARATOR
VOID
DCL
VOID
INT
NS
FOR
NS
GETTOKEN
COUNT
NS
DIRDCL
WHILE
NS
STRCAT
OUT
POINTER
TO
DIRDCL
PARSE
A
DIRECT
DECLARATOR
VOID
DIRDCL
VOID
INT
TYPE
IF
TOKENTYPE
DCL
DCL
IF
TOKENTYPE
PRINTF
ERROR
MISSING
N
ELSE
IF
TOKENTYPE
NAME
VARIABLE
NAME
STRCPY
NAME
TOKEN
ELSE
PRINTF
ERROR
EXPECTED
NAME
OR
DCL
N
WHILE
TYPE
GETTOKEN
PARENS
TYPE
BRACKETS
IF
TYPE
PARENS
STRCAT
OUT
FUNCTION
RETURNING
ELSE
STRCAT
OUT
ARRAY
STRCAT
OUT
TOKEN
STRCAT
OUT
OF
SINCE
THE
PROGRAMS
ARE
INTENDED
TO
BE
ILLUSTRATIVE
NOT
BULLET
PROOF
THERE
ARE
SIGNIFICANT
RESTRICTIONS
ON
DCL
IT
CAN
ONLY
HANDLE
A
SIMPLE
DATA
TYPE
LINE
CHAR
OR
INT
IT
DOES
NOT
HANDLE
ARGUMENT
TYPES
IN
FUNCTIONS
OR
QUALIFIERS
LIKE
CONST
SPURIOUS
BLANKS
CONFUSE
IT
IT
DOESN
T
DO
MUCH
ERROR
RECOVERY
SO
INVALID
DECLARATIONS
WILL
ALSO
CONFUSE
IT
THESE
IMPROVEMENTS
ARE
LEFT
AS
EXERCISES
HERE
ARE
THE
GLOBAL
VARIABLES
AND
THE
MAIN
ROUTINE
INCLUDE
STDIO
H
INCLUDE
STRING
H
INCLUDE
CTYPE
H
DEFINE
MAXTOKEN
ENUM
NAME
PARENS
BRACKETS
VOID
DCL
VOID
VOID
DIRDCL
VOID
INT
GETTOKEN
VOID
INT
TOKENTYPE
TYPE
OF
LAST
TOKEN
CHAR
TOKEN
MAXTOKEN
LAST
TOKEN
STRING
CHAR
NAME
MAXTOKEN
IDENTIFIER
NAME
CHAR
DATATYPE
MAXTOKEN
DATA
TYPE
CHAR
INT
ETC
CHAR
OUT
MAIN
CONVERT
DECLARATION
TO
WORDS
WHILE
GETTOKEN
EOF
TOKEN
ON
LINE
STRCPY
DATATYPE
TOKEN
IS
THE
DATATYPE
OUT
DCL
PARSE
REST
OF
LINE
IF
TOKENTYPE
N
PRINTF
SYNTAX
ERROR
N
PRINTF
N
NAME
OUT
DATATYPE
RETURN
THE
FUNCTION
GETTOKEN
SKIPS
BLANKS
AND
TABS
THEN
FINDS
THE
NEXT
TOKEN
IN
THE
INPUT
A
TOKEN
IS
A
NAME
A
PAIR
OF
PARENTHESES
A
PAIR
OF
BRACKETS
PERHAPS
INCLUDING
A
NUMBER
OR
ANY
OTHER
SINGLE
CHARACTER
INT
GETTOKEN
VOID
RETURN
NEXT
TOKEN
INT
C
GETCH
VOID
VOID
UNGETCH
INT
CHAR
P
TOKEN
WHILE
C
GETCH
C
T
IF
C
IF
C
GETCH
STRCPY
TOKEN
RETURN
TOKENTYPE
PARENS
ELSE
UNGETCH
C
RETURN
TOKENTYPE
ELSE
IF
C
FOR
P
C
P
GETCH
P
RETURN
TOKENTYPE
BRACKETS
ELSE
IF
ISALPHA
C
FOR
P
C
ISALNUM
C
GETCH
P
C
P
UNGETCH
C
RETURN
TOKENTYPE
NAME
ELSE
RETURN
TOKENTYPE
C
GETCH
AND
UNGETCH
ARE
DISCUSSED
IN
CHAPTER
GOING
IN
THE
OTHER
DIRECTION
IS
EASIER
ESPECIALLY
IF
WE
DO
NOT
WORRY
ABOUT
GENERATING
REDUNDANT
PARENTHESES
THE
PROGRAM
UNDCL
CONVERTS
A
WORD
DESCRIPTION
LIKE
X
IS
A
FUNCTION
RETURNING
A
POINTER
TO
AN
ARRAY
OF
POINTERS
TO
FUNCTIONS
RETURNING
CHAR
WHICH
WE
WILL
EXPRESS
AS
X
CHAR
TO
CHAR
X
THE
ABBREVIATED
INPUT
SYNTAX
LETS
US
REUSE
THE
GETTOKEN
FUNCTION
UNDCL
ALSO
USES
THE
SAME
EXTERNAL
VARIABLES
AS
DCL
DOES
UNDCL
CONVERT
WORD
DESCRIPTIONS
TO
DECLARATIONS
MAIN
INT
TYPE
CHAR
TEMP
MAXTOKEN
WHILE
GETTOKEN
EOF
STRCPY
OUT
TOKEN
WHILE
TYPE
GETTOKEN
N
IF
TYPE
PARENS
TYPE
BRACKETS
STRCAT
OUT
TOKEN
ELSE
IF
TYPE
SPRINTF
TEMP
OUT
STRCPY
OUT
TEMP
ELSE
IF
TYPE
NAME
SPRINTF
TEMP
TOKEN
OUT
STRCPY
OUT
TEMP
ELSE
PRINTF
INVALID
INPUT
AT
N
TOKEN
RETURN
EXERCISE
MAKE
DCL
RECOVER
FROM
INPUT
ERRORS
EXERCISE
MODIFY
UNDCL
SO
THAT
IT
DOES
NOT
ADD
REDUNDANT
PARENTHESES
TO
DECLARATIONS
EXERCISE
EXPAND
DCL
TO
HANDLE
DECLARATIONS
WITH
FUNCTION
ARGUMENT
TYPES
QUALIFIERS
LIKE
CONST
AND
SO
ON
BACK
TO
CHAPTER
INDEX
CHAPTER
BACK
TO
CHAPTER
INDEX
CHAPTER
CHAPTER
STRUCTURES
A
STRUCTURE
IS
A
COLLECTION
OF
ONE
OR
MORE
VARIABLES
POSSIBLY
OF
DIFFERENT
TYPES
GROUPED
TOGETHER
UNDER
A
SINGLE
NAME
FOR
CONVENIENT
HANDLING
STRUCTURES
ARE
CALLED
RECORDS
IN
SOME
LANGUAGES
NOTABLY
PASCAL
STRUCTURES
HELP
TO
ORGANIZE
COMPLICATED
DATA
PARTICULARLY
IN
LARGE
PROGRAMS
BECAUSE
THEY
PERMIT
A
GROUP
OF
RELATED
VARIABLES
TO
BE
TREATED
AS
A
UNIT
INSTEAD
OF
AS
SEPARATE
ENTITIES
ONE
TRADITIONAL
EXAMPLE
OF
A
STRUCTURE
IS
THE
PAYROLL
RECORD
AN
EMPLOYEE
IS
DESCRIBED
BY
A
SET
OF
ATTRIBUTES
SUCH
AS
NAME
ADDRESS
SOCIAL
SECURITY
NUMBER
SALARY
ETC
SOME
OF
THESE
IN
TURN
COULD
BE
STRUCTURES
A
NAME
HAS
SEVERAL
COMPONENTS
AS
DOES
AN
ADDRESS
AND
EVEN
A
SALARY
ANOTHER
EXAMPLE
MORE
TYPICAL
FOR
C
COMES
FROM
GRAPHICS
A
POINT
IS
A
PAIR
OF
COORDINATE
A
RECTANGLE
IS
A
PAIR
OF
POINTS
AND
SO
ON
THE
MAIN
CHANGE
MADE
BY
THE
ANSI
STANDARD
IS
TO
DEFINE
STRUCTURE
ASSIGNMENT
STRUCTURES
MAY
BE
COPIED
AND
ASSIGNED
TO
PASSED
TO
FUNCTIONS
AND
RETURNED
BY
FUNCTIONS
THIS
HAS
BEEN
SUPPORTED
BY
MOST
COMPILERS
FOR
MANY
YEARS
BUT
THE
PROPERTIES
ARE
NOW
PRECISELY
DEFINED
AUTOMATIC
STRUCTURES
AND
ARRAYS
MAY
NOW
ALSO
BE
INITIALIZED
BASICS
OF
STRUCTURES
LET
US
CREATE
A
FEW
STRUCTURES
SUITABLE
FOR
GRAPHICS
THE
BASIC
OBJECT
IS
A
POINT
WHICH
WE
WILL
ASSUME
HAS
AN
X
COORDINATE
AND
A
Y
COORDINATE
BOTH
INTEGERS
THE
TWO
COMPONENTS
CAN
BE
PLACED
IN
A
STRUCTURE
DECLARED
LIKE
THIS
STRUCT
POINT
INT
X
INT
Y
THE
KEYWORD
STRUCT
INTRODUCES
A
STRUCTURE
DECLARATION
WHICH
IS
A
LIST
OF
DECLARATIONS
ENCLOSED
IN
BRACES
AN
OPTIONAL
NAME
CALLED
A
STRUCTURE
TAG
MAY
FOLLOW
THE
WORD
STRUCT
AS
WITH
POINT
HERE
THE
TAG
NAMES
THIS
KIND
OF
STRUCTURE
AND
CAN
BE
USED
SUBSEQUENTLY
AS
A
SHORTHAND
FOR
THE
PART
OF
THE
DECLARATION
IN
BRACES
THE
VARIABLES
NAMED
IN
A
STRUCTURE
ARE
CALLED
MEMBERS
A
STRUCTURE
MEMBER
OR
TAG
AND
AN
ORDINARY
I
E
NON
MEMBER
VARIABLE
CAN
HAVE
THE
SAME
NAME
WITHOUT
CONFLICT
SINCE
THEY
CAN
ALWAYS
BE
DISTINGUISHED
BY
CONTEXT
FURTHERMORE
THE
SAME
MEMBER
NAMES
MAY
OCCUR
IN
DIFFERENT
STRUCTURES
ALTHOUGH
AS
A
MATTER
OF
STYLE
ONE
WOULD
NORMALLY
USE
THE
SAME
NAMES
ONLY
FOR
CLOSELY
RELATED
OBJECTS
A
STRUCT
DECLARATION
DEFINES
A
TYPE
THE
RIGHT
BRACE
THAT
TERMINATES
THE
LIST
OF
MEMBERS
MAY
BE
FOLLOWED
BY
A
LIST
OF
VARIABLES
JUST
AS
FOR
ANY
BASIC
TYPE
THAT
IS
STRUCT
X
Y
Z
IS
SYNTACTICALLY
ANALOGOUS
TO
INT
X
Y
Z
IN
THE
SENSE
THAT
EACH
STATEMENT
DECLARES
X
Y
AND
Z
TO
BE
VARIABLES
OF
THE
NAMED
TYPE
AND
CAUSES
SPACE
TO
BE
SET
ASIDE
FOR
THEM
A
STRUCTURE
DECLARATION
THAT
IS
NOT
FOLLOWED
BY
A
LIST
OF
VARIABLES
RESERVES
NO
STORAGE
IT
MERELY
DESCRIBES
A
TEMPLATE
OR
SHAPE
OF
A
STRUCTURE
IF
THE
DECLARATION
IS
TAGGED
HOWEVER
THE
TAG
CAN
BE
USED
LATER
IN
DEFINITIONS
OF
INSTANCES
OF
THE
STRUCTURE
FOR
EXAMPLE
GIVEN
THE
DECLARATION
OF
POINT
ABOVE
STRUCT
POINT
PT
DEFINES
A
VARIABLE
PT
WHICH
IS
A
STRUCTURE
OF
TYPE
STRUCT
POINT
A
STRUCTURE
CAN
BE
INITIALIZED
BY
FOLLOWING
ITS
DEFINITION
WITH
A
LIST
OF
INITIALIZERS
EACH
A
CONSTANT
EXPRESSION
FOR
THE
MEMBERS
STRUCT
MAXPT
AN
AUTOMATIC
STRUCTURE
MAY
ALSO
BE
INITIALIZED
BY
ASSIGNMENT
OR
BY
CALLING
A
FUNCTION
THAT
RETURNS
A
STRUCTURE
OF
THE
RIGHT
TYPE
A
MEMBER
OF
A
PARTICULAR
STRUCTURE
IS
REFERRED
TO
IN
AN
EXPRESSION
BY
A
CONSTRUCTION
OF
THE
FORM
STRUCTURE
NAME
MEMBER
THE
STRUCTURE
MEMBER
OPERATOR
CONNECTS
THE
STRUCTURE
NAME
AND
THE
MEMBER
NAME
TO
PRINT
THE
COORDINATES
OF
THE
POINT
PT
FOR
INSTANCE
PRINTF
D
D
PT
X
PT
Y
OR
TO
COMPUTE
THE
DISTANCE
FROM
THE
ORIGIN
TO
PT
DOUBLE
DIST
SQRT
DOUBLE
DIST
SQRT
DOUBLE
PT
X
PT
X
DOUBLE
PT
Y
PT
Y
STRUCTURES
CAN
BE
NESTED
ONE
REPRESENTATION
OF
A
RECTANGLE
IS
A
PAIR
OF
POINTS
THAT
DENOTE
THE
DIAGONALLY
OPPOSITE
CORNERS
STRUCT
RECT
STRUCT
POINT
STRUCT
POINT
THE
RECT
STRUCTURE
CONTAINS
TWO
POINT
STRUCTURES
IF
WE
DECLARE
SCREEN
AS
STRUCT
RECT
SCREEN
THEN
SCREEN
X
REFERS
TO
THE
X
COORDINATE
OF
THE
MEMBER
OF
SCREEN
STRUCTURES
AND
FUNCTIONS
THE
ONLY
LEGAL
OPERATIONS
ON
A
STRUCTURE
ARE
COPYING
IT
OR
ASSIGNING
TO
IT
AS
A
UNIT
TAKING
ITS
ADDRESS
WITH
AND
ACCESSING
ITS
MEMBERS
COPY
AND
ASSIGNMENT
INCLUDE
PASSING
ARGUMENTS
TO
FUNCTIONS
AND
RETURNING
VALUES
FROM
FUNCTIONS
AS
WELL
STRUCTURES
MAY
NOT
BE
COMPARED
A
STRUCTURE
MAY
BE
INITIALIZED
BY
A
LIST
OF
CONSTANT
MEMBER
VALUES
AN
AUTOMATIC
STRUCTURE
MAY
ALSO
BE
INITIALIZED
BY
AN
ASSIGNMENT
LET
US
INVESTIGATE
STRUCTURES
BY
WRITING
SOME
FUNCTIONS
TO
MANIPULATE
POINTS
AND
RECTANGLES
THERE
ARE
AT
LEAST
THREE
POSSIBLE
APPROACHES
PASS
COMPONENTS
SEPARATELY
PASS
AN
ENTIRE
STRUCTURE
OR
PASS
A
POINTER
TO
IT
EACH
HAS
ITS
GOOD
POINTS
AND
BAD
POINTS
THE
FIRST
FUNCTION
MAKEPOINT
WILL
TAKE
TWO
INTEGERS
AND
RETURN
A
POINT
STRUCTURE
MAKEPOINT
MAKE
A
POINT
FROM
X
AND
Y
COMPONENTS
STRUCT
POINT
MAKEPOINT
INT
X
INT
Y
STRUCT
POINT
TEMP
TEMP
X
X
TEMP
Y
Y
RETURN
TEMP
NOTICE
THAT
THERE
IS
NO
CONFLICT
BETWEEN
THE
ARGUMENT
NAME
AND
THE
MEMBER
WITH
THE
SAME
NAME
INDEED
THE
RE
USE
OF
THE
NAMES
STRESSES
THE
RELATIONSHIP
MAKEPOINT
CAN
NOW
BE
USED
TO
INITIALIZE
ANY
STRUCTURE
DYNAMICALLY
OR
TO
PROVIDE
STRUCTURE
ARGUMENTS
TO
A
FUNCTION
STRUCT
RECT
SCREEN
STRUCT
POINT
MIDDLE
STRUCT
POINT
MAKEPOINT
INT
INT
SCREEN
MAKEPOINT
SCREEN
MAKEPOINT
XMAX
YMAX
MIDDLE
MAKEPOINT
SCREEN
X
SCREEN
X
SCREEN
Y
SCREEN
Y
THE
NEXT
STEP
IS
A
SET
OF
FUNCTIONS
TO
DO
ARITHMETIC
ON
POINTS
FOR
INSTANCE
ADDPOINTS
ADD
TWO
POINTS
STRUCT
ADDPOINT
STRUCT
POINT
STRUCT
POINT
X
X
Y
Y
RETURN
HERE
BOTH
THE
ARGUMENTS
AND
THE
RETURN
VALUE
ARE
STRUCTURES
WE
INCREMENTED
THE
COMPONENTS
IN
RATHER
THAN
USING
AN
EXPLICIT
TEMPORARY
VARIABLE
TO
EMPHASIZE
THAT
STRUCTURE
PARAMETERS
ARE
PASSED
BY
VALUE
LIKE
ANY
OTHERS
AS
ANOTHER
EXAMPLE
THE
FUNCTION
PTINRECT
TESTS
WHETHER
A
POINT
IS
INSIDE
A
RECTANGLE
WHERE
WE
HAVE
ADOPTED
THE
CONVENTION
THAT
A
RECTANGLE
INCLUDES
ITS
LEFT
AND
BOTTOM
SIDES
BUT
NOT
ITS
TOP
AND
RIGHT
SIDES
PTINRECT
RETURN
IF
P
IN
R
IF
NOT
INT
PTINRECT
STRUCT
POINT
P
STRUCT
RECT
R
RETURN
P
X
R
X
P
X
R
X
P
Y
R
Y
P
Y
R
Y
THIS
ASSUMES
THAT
THE
RECTANGLE
IS
PRESENTED
IN
A
STANDARD
FORM
WHERE
THE
COORDINATES
ARE
LESS
THAN
THE
COORDINATES
THE
FOLLOWING
FUNCTION
RETURNS
A
RECTANGLE
GUARANTEED
TO
BE
IN
CANONICAL
FORM
DEFINE
MIN
A
B
A
B
A
B
DEFINE
MAX
A
B
A
B
A
B
CANONRECT
CANONICALIZE
COORDINATES
OF
RECTANGLE
STRUCT
RECT
CANONRECT
STRUCT
RECT
R
STRUCT
RECT
TEMP
TEMP
X
MIN
R
X
R
X
TEMP
Y
MIN
R
Y
R
Y
TEMP
X
MAX
R
X
R
X
TEMP
Y
MAX
R
Y
R
Y
RETURN
TEMP
IF
A
LARGE
STRUCTURE
IS
TO
BE
PASSED
TO
A
FUNCTION
IT
IS
GENERALLY
MORE
EFFICIENT
TO
PASS
A
POINTER
THAN
TO
COPY
THE
WHOLE
STRUCTURE
STRUCTURE
POINTERS
ARE
JUST
LIKE
POINTERS
TO
ORDINARY
VARIABLES
THE
DECLARATION
STRUCT
POINT
PP
SAYS
THAT
PP
IS
A
POINTER
TO
A
STRUCTURE
OF
TYPE
STRUCT
POINT
IF
PP
POINTS
TO
A
POINT
STRUCTURE
PP
IS
THE
STRUCTURE
AND
PP
X
AND
PP
Y
ARE
THE
MEMBERS
TO
USE
PP
WE
MIGHT
WRITE
FOR
EXAMPLE
STRUCT
POINT
ORIGIN
PP
PP
ORIGIN
PRINTF
ORIGIN
IS
D
D
N
PP
X
PP
Y
THE
PARENTHESES
ARE
NECESSARY
IN
PP
X
BECAUSE
THE
PRECEDENCE
OF
THE
STRUCTURE
MEMBER
OPERATOR
IS
HIGHER
THEN
THE
EXPRESSION
PP
X
MEANS
PP
X
WHICH
IS
ILLEGAL
HERE
BECAUSE
X
IS
NOT
A
POINTER
POINTERS
TO
STRUCTURES
ARE
SO
FREQUENTLY
USED
THAT
AN
ALTERNATIVE
NOTATION
IS
PROVIDED
AS
A
SHORTHAND
IF
P
IS
A
POINTER
TO
A
STRUCTURE
THEN
P
MEMBER
OF
STRUCTURE
REFERS
TO
THE
PARTICULAR
MEMBER
SO
WE
COULD
WRITE
INSTEAD
PRINTF
ORIGIN
IS
D
D
N
PP
X
PP
Y
BOTH
AND
ASSOCIATE
FROM
LEFT
TO
RIGHT
SO
IF
WE
HAVE
STRUCT
RECT
R
RP
R
THEN
THESE
FOUR
EXPRESSIONS
ARE
EQUIVALENT
R
X
RP
X
R
X
RP
X
THE
STRUCTURE
OPERATORS
AND
TOGETHER
WITH
FOR
FUNCTION
CALLS
AND
FOR
SUBSCRIPTS
ARE
AT
THE
TOP
OF
THE
PRECEDENCE
HIERARCHY
AND
THUS
BIND
VERY
TIGHTLY
FOR
EXAMPLE
GIVEN
THE
DECLARATION
STRUCT
INT
LEN
CHAR
STR
P
THEN
P
LEN
INCREMENTS
LEN
NOT
P
BECAUSE
THE
IMPLIED
PARENTHESIZATION
IS
P
LEN
PARENTHESES
CAN
BE
USED
TO
ALTER
BINDING
P
LEN
INCREMENTS
P
BEFORE
ACCESSING
LEN
AND
P
LEN
INCREMENTS
P
AFTERWARD
THIS
LAST
SET
OF
PARENTHESES
IS
UNNECESSARY
IN
THE
SAME
WAY
P
STR
FETCHES
WHATEVER
STR
POINTS
TO
P
STR
INCREMENTS
STR
AFTER
ACCESSING
WHATEVER
IT
POINTS
TO
JUST
LIKE
P
STR
INCREMENTS
WHATEVER
STR
POINTS
TO
AND
P
STR
INCREMENTS
P
AFTER
ACCESSING
WHATEVER
STR
POINTS
TO
ARRAYS
OF
STRUCTURES
CONSIDER
WRITING
A
PROGRAM
TO
COUNT
THE
OCCURRENCES
OF
EACH
C
KEYWORD
WE
NEED
AN
ARRAY
OF
CHARACTER
STRINGS
TO
HOLD
THE
NAMES
AND
AN
ARRAY
OF
INTEGERS
FOR
THE
COUNTS
ONE
POSSIBILITY
IS
TO
USE
TWO
PARALLEL
ARRAYS
KEYWORD
AND
KEYCOUNT
AS
IN
CHAR
KEYWORD
NKEYS
INT
KEYCOUNT
NKEYS
BUT
THE
VERY
FACT
THAT
THE
ARRAYS
ARE
PARALLEL
SUGGESTS
A
DIFFERENT
ORGANIZATION
AN
ARRAY
OF
STRUCTURES
EACH
KEYWORD
IS
A
PAIR
CHAR
WORD
INT
COUT
AND
THERE
IS
AN
ARRAY
OF
PAIRS
THE
STRUCTURE
DECLARATION
STRUCT
KEY
CHAR
WORD
INT
COUNT
KEYTAB
NKEYS
DECLARES
A
STRUCTURE
TYPE
KEY
DEFINES
AN
ARRAY
KEYTAB
OF
STRUCTURES
OF
THIS
TYPE
AND
SETS
ASIDE
STORAGE
FOR
THEM
EACH
ELEMENT
OF
THE
ARRAY
IS
A
STRUCTURE
THIS
COULD
ALSO
BE
WRITTEN
STRUCT
KEY
CHAR
WORD
INT
COUNT
STRUCT
KEY
KEYTAB
NKEYS
SINCE
THE
STRUCTURE
KEYTAB
CONTAINS
A
CONSTANT
SET
OF
NAMES
IT
IS
EASIEST
TO
MAKE
IT
AN
EXTERNAL
VARIABLE
AND
INITIALIZE
IT
ONCE
AND
FOR
ALL
WHEN
IT
IS
DEFINED
THE
STRUCTURE
INITIALIZATION
IS
ANALOGOUS
TO
EARLIER
ONES
THE
DEFINITION
IS
FOLLOWED
BY
A
LIST
OF
INITIALIZERS
ENCLOSED
IN
BRACES
STRUCT
KEY
CHAR
WORD
INT
COUNT
KEYTAB
AUTO
BREAK
CASE
CHAR
CONST
CONTINUE
DEFAULT
UNSIGNED
VOID
VOLATILE
WHILE
THE
INITIALIZERS
ARE
LISTED
IN
PAIRS
CORRESPONDING
TO
THE
STRUCTURE
MEMBERS
IT
WOULD
BE
MORE
PRECISE
TO
ENCLOSE
THE
INITIALIZERS
FOR
EACH
ROW
OR
STRUCTURE
IN
BRACES
AS
IN
AUTO
BREAK
CASE
BUT
INNER
BRACES
ARE
NOT
NECESSARY
WHEN
THE
INITIALIZERS
ARE
SIMPLE
VARIABLES
OR
CHARACTER
STRINGS
AND
WHEN
ALL
ARE
PRESENT
AS
USUAL
THE
NUMBER
OF
ENTRIES
IN
THE
ARRAY
KEYTAB
WILL
BE
COMPUTED
IF
THE
INITIALIZERS
ARE
PRESENT
AND
THE
IS
LEFT
EMPTY
THE
KEYWORD
COUNTING
PROGRAM
BEGINS
WITH
THE
DEFINITION
OF
KEYTAB
THE
MAIN
ROUTINE
READS
THE
INPUT
BY
REPEATEDLY
CALLING
A
FUNCTION
GETWORD
THAT
FETCHES
ONE
WORD
AT
A
TIME
EACH
WORD
IS
LOOKED
UP
IN
KEYTAB
WITH
A
VERSION
OF
THE
BINARY
SEARCH
FUNCTION
THAT
WE
WROTE
IN
CHAPTER
THE
LIST
OF
KEYWORDS
MUST
BE
SORTED
IN
INCREASING
ORDER
IN
THE
TABLE
INCLUDE
STDIO
H
INCLUDE
CTYPE
H
INCLUDE
STRING
H
DEFINE
MAXWORD
INT
GETWORD
CHAR
INT
INT
BINSEARCH
CHAR
STRUCT
KEY
INT
COUNT
C
KEYWORDS
MAIN
INT
N
CHAR
WORD
MAXWORD
WHILE
GETWORD
WORD
MAXWORD
EOF
IF
ISALPHA
WORD
IF
N
BINSEARCH
WORD
KEYTAB
NKEYS
KEYTAB
N
COUNT
FOR
N
N
NKEYS
N
IF
KEYTAB
N
COUNT
PRINTF
N
KEYTAB
N
COUNT
KEYTAB
N
WORD
RETURN
BINSEARCH
FIND
WORD
IN
TAB
TAB
N
INT
BINSEARCH
CHAR
WORD
STRUCT
KEY
TAB
INT
N
INT
COND
INT
LOW
HIGH
MID
LOW
HIGH
N
WHILE
LOW
HIGH
MID
LOW
HIGH
IF
COND
STRCMP
WORD
TAB
MID
WORD
HIGH
MID
ELSE
IF
COND
LOW
MID
ELSE
RETURN
MID
RETURN
WE
WILL
SHOW
THE
FUNCTION
GETWORD
IN
A
MOMENT
FOR
NOW
IT
SUFFICES
TO
SAY
THAT
EACH
CALL
TO
GETWORD
FINDS
A
WORD
WHICH
IS
COPIED
INTO
THE
ARRAY
NAMED
AS
ITS
FIRST
ARGUMENT
THE
QUANTITY
NKEYS
IS
THE
NUMBER
OF
KEYWORDS
IN
KEYTAB
ALTHOUGH
WE
COULD
COUNT
THIS
BY
HAND
IT
A
LOT
EASIER
AND
SAFER
TO
DO
IT
BY
MACHINE
ESPECIALLY
IF
THE
LIST
IS
SUBJECT
TO
CHANGE
ONE
POSSIBILITY
WOULD
BE
TO
TERMINATE
THE
LIST
OF
INITIALIZERS
WITH
A
NULL
POINTER
THEN
LOOP
ALONG
KEYTAB
UNTIL
THE
END
IS
FOUND
BUT
THIS
IS
MORE
THAN
IS
NEEDED
SINCE
THE
SIZE
OF
THE
ARRAY
IS
COMPLETELY
DETERMINED
AT
COMPILE
TIME
THE
SIZE
OF
THE
ARRAY
IS
THE
SIZE
OF
ONE
ENTRY
TIMES
THE
NUMBER
OF
ENTRIES
SO
THE
NUMBER
OF
ENTRIES
IS
JUST
SIZE
OF
KEYTAB
SIZE
OF
STRUCT
KEY
C
PROVIDES
A
COMPILE
TIME
UNARY
OPERATOR
CALLED
SIZEOF
THAT
CAN
BE
USED
TO
COMPUTE
THE
SIZE
OF
ANY
OBJECT
THE
EXPRESSIONS
SIZEOF
OBJECT
AND
SIZEOF
TYPE
NAME
YIELD
AN
INTEGER
EQUAL
TO
THE
SIZE
OF
THE
SPECIFIED
OBJECT
OR
TYPE
IN
BYTES
STRICTLY
SIZEOF
PRODUCES
AN
UNSIGNED
INTEGER
VALUE
WHOSE
TYPE
IS
DEFINED
IN
THE
HEADER
STDDEF
H
AN
OBJECT
CAN
BE
A
VARIABLE
OR
ARRAY
OR
STRUCTURE
A
TYPE
NAME
CAN
BE
THE
NAME
OF
A
BASIC
TYPE
LIKE
INT
OR
DOUBLE
OR
A
DERIVED
TYPE
LIKE
A
STRUCTURE
OR
A
POINTER
IN
OUR
CASE
THE
NUMBER
OF
KEYWORDS
IS
THE
SIZE
OF
THE
ARRAY
DIVIDED
BY
THE
SIZE
OF
ONE
ELEMENT
THIS
COMPUTATION
IS
USED
IN
A
DEFINE
STATEMENT
TO
SET
THE
VALUE
OF
NKEYS
DEFINE
NKEYS
SIZEOF
KEYTAB
SIZEOF
STRUCT
KEY
ANOTHER
WAY
TO
WRITE
THIS
IS
TO
DIVIDE
THE
ARRAY
SIZE
BY
THE
SIZE
OF
A
SPECIFIC
ELEMENT
DEFINE
NKEYS
SIZEOF
KEYTAB
SIZEOF
KEYTAB
THIS
HAS
THE
ADVANTAGE
THAT
IT
DOES
NOT
NEED
TO
BE
CHANGED
IF
THE
TYPE
CHANGES
A
SIZEOF
CAN
NOT
BE
USED
IN
A
IF
LINE
BECAUSE
THE
PREPROCESSOR
DOES
NOT
PARSE
TYPE
NAMES
BUT
THE
EXPRESSION
IN
THE
DEFINE
IS
NOT
EVALUATED
BY
THE
PREPROCESSOR
SO
THE
CODE
HERE
IS
LEGAL
NOW
FOR
THE
FUNCTION
GETWORD
WE
HAVE
WRITTEN
A
MORE
GENERAL
GETWORD
THAN
IS
NECESSARY
FOR
THIS
PROGRAM
BUT
IT
IS
NOT
COMPLICATED
GETWORD
FETCHES
THE
NEXT
WORD
FROM
THE
INPUT
WHERE
A
WORD
IS
EITHER
A
STRING
OF
LETTERS
AND
DIGITS
BEGINNING
WITH
A
LETTER
OR
A
SINGLE
NON
WHITE
SPACE
CHARACTER
THE
FUNCTION
VALUE
IS
THE
FIRST
CHARACTER
OF
THE
WORD
OR
EOF
FOR
END
OF
FILE
OR
THE
CHARACTER
ITSELF
IF
IT
IS
NOT
ALPHABETIC
GETWORD
GET
NEXT
WORD
OR
CHARACTER
FROM
INPUT
INT
GETWORD
CHAR
WORD
INT
LIM
INT
C
GETCH
VOID
VOID
UNGETCH
INT
CHAR
W
WORD
WHILE
ISSPACE
C
GETCH
IF
C
EOF
W
C
IF
ISALPHA
C
W
RETURN
C
FOR
LIM
W
IF
ISALNUM
W
GETCH
UNGETCH
W
BREAK
W
RETURN
WORD
GETWORD
USES
THE
GETCH
AND
UNGETCH
THAT
WE
WROTE
IN
CHAPTER
WHEN
THE
COLLECTION
OF
AN
ALPHANUMERIC
TOKEN
STOPS
GETWORD
HAS
GONE
ONE
CHARACTER
TOO
FAR
THE
CALL
TO
UNGETCH
PUSHES
THAT
CHARACTER
BACK
ON
THE
INPUT
FOR
THE
NEXT
CALL
GETWORD
ALSO
USES
ISSPACE
TO
SKIP
WHITESPACE
ISALPHA
TO
IDENTIFY
LETTERS
AND
ISALNUM
TO
IDENTIFY
LETTERS
AND
DIGITS
ALL
ARE
FROM
THE
STANDARD
HEADER
CTYPE
H
EXERCISE
OUR
VERSION
OF
GETWORD
DOES
NOT
PROPERLY
HANDLE
UNDERSCORES
STRING
CONSTANTS
COMMENTS
OR
PREPROCESSOR
CONTROL
LINES
WRITE
A
BETTER
VERSION
POINTERS
TO
STRUCTURES
TO
ILLUSTRATE
SOME
OF
THE
CONSIDERATIONS
INVOLVED
WITH
POINTERS
TO
AND
ARRAYS
OF
STRUCTURES
LET
US
WRITE
THE
KEYWORD
COUNTING
PROGRAM
AGAIN
THIS
TIME
USING
POINTERS
INSTEAD
OF
ARRAY
INDICES
THE
EXTERNAL
DECLARATION
OF
KEYTAB
NEED
NOT
CHANGE
BUT
MAIN
AND
BINSEARCH
DO
NEED
MODIFICATION
INCLUDE
STDIO
H
INCLUDE
CTYPE
H
INCLUDE
STRING
H
DEFINE
MAXWORD
INT
GETWORD
CHAR
INT
STRUCT
KEY
BINSEARCH
CHAR
STRUCT
KEY
INT
COUNT
C
KEYWORDS
POINTER
VERSION
MAIN
CHAR
WORD
MAXWORD
STRUCT
KEY
P
WHILE
GETWORD
WORD
MAXWORD
EOF
IF
ISALPHA
WORD
IF
P
BINSEARCH
WORD
KEYTAB
NKEYS
NULL
P
COUNT
FOR
P
KEYTAB
P
KEYTAB
NKEYS
P
IF
P
COUNT
PRINTF
N
P
COUNT
P
WORD
RETURN
BINSEARCH
FIND
WORD
IN
TAB
TAB
N
STRUCT
KEY
BINSEARCH
CHAR
WORD
STRUCK
KEY
TAB
INT
N
INT
COND
STRUCT
KEY
LOW
TAB
STRUCT
KEY
HIGH
TAB
N
STRUCT
KEY
MID
WHILE
LOW
HIGH
MID
LOW
HIGH
LOW
IF
COND
STRCMP
WORD
MID
WORD
HIGH
MID
ELSE
IF
COND
LOW
MID
ELSE
RETURN
MID
RETURN
NULL
THERE
ARE
SEVERAL
THINGS
WORTHY
OF
NOTE
HERE
FIRST
THE
DECLARATION
OF
BINSEARCH
MUST
INDICATE
THAT
IT
RETURNS
A
POINTER
TO
STRUCT
KEY
INSTEAD
OF
AN
INTEGER
THIS
IS
DECLARED
BOTH
IN
THE
FUNCTION
PROTOTYPE
AND
IN
BINSEARCH
IF
BINSEARCH
FINDS
THE
WORD
IT
RETURNS
A
POINTER
TO
IT
IF
IT
FAILS
IT
RETURNS
NULL
SECOND
THE
ELEMENTS
OF
KEYTAB
ARE
NOW
ACCESSED
BY
POINTERS
THIS
REQUIRES
SIGNIFICANT
CHANGES
IN
BINSEARCH
THE
INITIALIZERS
FOR
LOW
AND
HIGH
ARE
NOW
POINTERS
TO
THE
BEGINNING
AND
JUST
PAST
THE
END
OF
THE
TABLE
THE
COMPUTATION
OF
THE
MIDDLE
ELEMENT
CAN
NO
LONGER
BE
SIMPLY
MID
LOW
HIGH
WRONG
BECAUSE
THE
ADDITION
OF
POINTERS
IS
ILLEGAL
SUBTRACTION
IS
LEGAL
HOWEVER
SO
HIGH
LOW
IS
THE
NUMBER
OF
ELEMENTS
AND
THUS
MID
LOW
HIGH
LOW
SETS
MID
TO
THE
ELEMENT
HALFWAY
BETWEEN
LOW
AND
HIGH
THE
MOST
IMPORTANT
CHANGE
IS
TO
ADJUST
THE
ALGORITHM
TO
MAKE
SURE
THAT
IT
DOES
NOT
GENERATE
AN
ILLEGAL
POINTER
OR
ATTEMPT
TO
ACCESS
AN
ELEMENT
OUTSIDE
THE
ARRAY
THE
PROBLEM
IS
THAT
TAB
AND
TAB
N
ARE
BOTH
OUTSIDE
THE
LIMITS
OF
THE
ARRAY
TAB
THE
FORMER
IS
STRICTLY
ILLEGAL
AND
IT
IS
ILLEGAL
TO
DEREFERENCE
THE
LATTER
THE
LANGUAGE
DEFINITION
DOES
GUARANTEE
HOWEVER
THAT
POINTER
ARITHMETIC
THAT
INVOLVES
THE
FIRST
ELEMENT
BEYOND
THE
END
OF
AN
ARRAY
THAT
IS
TAB
N
WILL
WORK
CORRECTLY
IN
MAIN
WE
WROTE
FOR
P
KEYTAB
P
KEYTAB
NKEYS
P
IF
P
IS
A
POINTER
TO
A
STRUCTURE
ARITHMETIC
ON
P
TAKES
INTO
ACCOUNT
THE
SIZE
OF
THE
STRUCTURE
SO
P
INCREMENTS
P
BY
THE
CORRECT
AMOUNT
TO
GET
THE
NEXT
ELEMENT
OF
THE
ARRAY
OF
STRUCTURES
AND
THE
TEST
STOPS
THE
LOOP
AT
THE
RIGHT
TIME
DON
T
ASSUME
HOWEVER
THAT
THE
SIZE
OF
A
STRUCTURE
IS
THE
SUM
OF
THE
SIZES
OF
ITS
MEMBERS
BECAUSE
OF
ALIGNMENT
REQUIREMENTS
FOR
DIFFERENT
OBJECTS
THERE
MAY
BE
UNNAMED
HOLES
IN
A
STRUCTURE
THUS
FOR
INSTANCE
IF
A
CHAR
IS
ONE
BYTE
AND
AN
INT
FOUR
BYTES
THE
STRUCTURE
STRUCT
CHAR
C
INT
I
MIGHT
WELL
REQUIRE
EIGHT
BYTES
NOT
FIVE
THE
SIZEOF
OPERATOR
RETURNS
THE
PROPER
VALUE
FINALLY
AN
ASIDE
ON
PROGRAM
FORMAT
WHEN
A
FUNCTION
RETURNS
A
COMPLICATED
TYPE
LIKE
A
STRUCTURE
POINTER
AS
IN
STRUCT
KEY
BINSEARCH
CHAR
WORD
STRUCT
KEY
TAB
INT
N
THE
FUNCTION
NAME
CAN
BE
HARD
TO
SEE
AND
TO
FIND
WITH
A
TEXT
EDITOR
ACCORDINGLY
AN
ALTERNATE
STYLE
IS
SOMETIMES
USED
STRUCT
KEY
BINSEARCH
CHAR
WORD
STRUCT
KEY
TAB
INT
N
THIS
IS
A
MATTER
OF
PERSONAL
TASTE
PICK
THE
FORM
YOU
LIKE
AND
HOLD
TO
IT
SELF
REFERENTIAL
STRUCTURES
SUPPOSE
WE
WANT
TO
HANDLE
THE
MORE
GENERAL
PROBLEM
OF
COUNTING
THE
OCCURRENCES
OF
ALL
THE
WORDS
IN
SOME
INPUT
SINCE
THE
LIST
OF
WORDS
ISN
T
KNOWN
IN
ADVANCE
WE
CAN
T
CONVENIENTLY
SORT
IT
AND
USE
A
BINARY
SEARCH
YET
WE
CAN
T
DO
A
LINEAR
SEARCH
FOR
EACH
WORD
AS
IT
ARRIVES
TO
SEE
IF
IT
ALREADY
BEEN
SEEN
THE
PROGRAM
WOULD
TAKE
TOO
LONG
MORE
PRECISELY
ITS
RUNNING
TIME
IS
LIKELY
TO
GROW
QUADRATICALLY
WITH
THE
NUMBER
OF
INPUT
WORDS
HOW
CAN
WE
ORGANIZE
THE
DATA
TO
COPY
EFFICIENTLY
WITH
A
LIST
OR
ARBITRARY
WORDS
ONE
SOLUTION
IS
TO
KEEP
THE
SET
OF
WORDS
SEEN
SO
FAR
SORTED
AT
ALL
TIMES
BY
PLACING
EACH
WORD
INTO
ITS
PROPER
POSITION
IN
THE
ORDER
AS
IT
ARRIVES
THIS
SHOULDN
T
BE
DONE
BY
SHIFTING
WORDS
IN
A
LINEAR
ARRAY
THOUGH
THAT
ALSO
TAKES
TOO
LONG
INSTEAD
WE
WILL
USE
A
DATA
STRUCTURE
CALLED
A
BINARY
TREE
THE
TREE
CONTAINS
ONE
NODE
PER
DISTINCT
WORD
EACH
NODE
CONTAINS
A
POINTER
TO
THE
TEXT
OF
THE
WORD
A
COUNT
OF
THE
NUMBER
OF
OCCURRENCES
A
POINTER
TO
THE
LEFT
CHILD
NODE
A
POINTER
TO
THE
RIGHT
CHILD
NODE
NO
NODE
MAY
HAVE
MORE
THAN
TWO
CHILDREN
IT
MIGHT
HAVE
ONLY
ZERO
OR
ONE
THE
NODES
ARE
MAINTAINED
SO
THAT
AT
ANY
NODE
THE
LEFT
SUBTREE
CONTAINS
ONLY
WORDS
THAT
ARE
LEXICOGRAPHICALLY
LESS
THAN
THE
WORD
AT
THE
NODE
AND
THE
RIGHT
SUBTREE
CONTAINS
ONLY
WORDS
THAT
ARE
GREATER
THIS
IS
THE
TREE
FOR
THE
SENTENCE
NOW
IS
THE
TIME
FOR
ALL
GOOD
MEN
TO
COME
TO
THE
AID
OF
THEIR
PARTY
AS
BUILT
BY
INSERTING
EACH
WORD
AS
IT
IS
ENCOUNTERED
TO
FIND
OUT
WHETHER
A
NEW
WORD
IS
ALREADY
IN
THE
TREE
START
AT
THE
ROOT
AND
COMPARE
THE
NEW
WORD
TO
THE
WORD
STORED
AT
THAT
NODE
IF
THEY
MATCH
THE
QUESTION
IS
ANSWERED
AFFIRMATIVELY
IF
THE
NEW
RECORD
IS
LESS
THAN
THE
TREE
WORD
CONTINUE
SEARCHING
AT
THE
LEFT
CHILD
OTHERWISE
AT
THE
RIGHT
CHILD
IF
THERE
IS
NO
CHILD
IN
THE
REQUIRED
DIRECTION
THE
NEW
WORD
IS
NOT
IN
THE
TREE
AND
IN
FACT
THE
EMPTY
SLOT
IS
THE
PROPER
PLACE
TO
ADD
THE
NEW
WORD
THIS
PROCESS
IS
RECURSIVE
SINCE
THE
SEARCH
FROM
ANY
NODE
USES
A
SEARCH
FROM
ONE
OF
ITS
CHILDREN
ACCORDINGLY
RECURSIVE
ROUTINES
FOR
INSERTION
AND
PRINTING
WILL
BE
MOST
NATURAL
GOING
BACK
TO
THE
DESCRIPTION
OF
A
NODE
IT
IS
MOST
CONVENIENTLY
REPRESENTED
AS
A
STRUCTURE
WITH
FOUR
COMPONENTS
STRUCT
TNODE
THE
TREE
NODE
CHAR
WORD
POINTS
TO
THE
TEXT
INT
COUNT
NUMBER
OF
OCCURRENCES
STRUCT
TNODE
LEFT
LEFT
CHILD
STRUCT
TNODE
RIGHT
RIGHT
CHILD
THIS
RECURSIVE
DECLARATION
OF
A
NODE
MIGHT
LOOK
CHANCY
BUT
IT
CORRECT
IT
IS
ILLEGAL
FOR
A
STRUCTURE
TO
CONTAIN
AN
INSTANCE
OF
ITSELF
BUT
STRUCT
TNODE
LEFT
DECLARES
LEFT
TO
BE
A
POINTER
TO
A
TNODE
NOT
A
TNODE
ITSELF
OCCASIONALLY
ONE
NEEDS
A
VARIATION
OF
SELF
REFERENTIAL
STRUCTURES
TWO
STRUCTURES
THAT
REFER
TO
EACH
OTHER
THE
WAY
TO
HANDLE
THIS
IS
STRUCT
T
STRUCT
P
P
POINTS
TO
AN
STRUCT
STRUCT
T
Q
Q
POINTS
TO
A
T
THE
CODE
FOR
THE
WHOLE
PROGRAM
IS
SURPRISINGLY
SMALL
GIVEN
A
HANDFUL
OF
SUPPORTING
ROUTINES
LIKE
GETWORD
THAT
WE
HAVE
ALREADY
WRITTEN
THE
MAIN
ROUTINE
READS
WORDS
WITH
GETWORD
AND
INSTALLS
THEM
IN
THE
TREE
WITH
ADDTREE
INCLUDE
STDIO
H
INCLUDE
CTYPE
H
INCLUDE
STRING
H
DEFINE
MAXWORD
STRUCT
TNODE
ADDTREE
STRUCT
TNODE
CHAR
VOID
TREEPRINT
STRUCT
TNODE
INT
GETWORD
CHAR
INT
WORD
FREQUENCY
COUNT
MAIN
STRUCT
TNODE
ROOT
CHAR
WORD
MAXWORD
ROOT
NULL
WHILE
GETWORD
WORD
MAXWORD
EOF
IF
ISALPHA
WORD
ROOT
ADDTREE
ROOT
WORD
TREEPRINT
ROOT
RETURN
THE
FUNCTION
ADDTREE
IS
RECURSIVE
A
WORD
IS
PRESENTED
BY
MAIN
TO
THE
TOP
LEVEL
THE
ROOT
OF
THE
TREE
AT
EACH
STAGE
THAT
WORD
IS
COMPARED
TO
THE
WORD
ALREADY
STORED
AT
THE
NODE
AND
IS
PERCOLATED
DOWN
TO
EITHER
THE
LEFT
OR
RIGHT
SUBTREE
BY
A
RECURSIVE
CALL
TO
ADTREE
EVENTUALLY
THE
WORD
EITHER
MATCHES
SOMETHING
ALREADY
IN
THE
TREE
IN
WHICH
CASE
THE
COUNT
IS
INCREMENTED
OR
A
NULL
POINTER
IS
ENCOUNTERED
INDICATING
THAT
A
NODE
MUST
BE
CREATED
AND
ADDED
TO
THE
TREE
IF
A
NEW
NODE
IS
CREATED
ADDTREE
RETURNS
A
POINTER
TO
IT
WHICH
IS
INSTALLED
IN
THE
PARENT
NODE
STRUCT
TNODE
TALLOC
VOID
CHAR
STRDUP
CHAR
ADDTREE
ADD
A
NODE
WITH
W
AT
OR
BELOW
P
STRUCT
TREENODE
ADDTREE
STRUCT
TNODE
P
CHAR
W
INT
COND
IF
P
NULL
A
NEW
WORD
HAS
ARRIVED
P
TALLOC
MAKE
A
NEW
NODE
P
WORD
STRDUP
W
P
COUNT
P
LEFT
P
RIGHT
NULL
ELSE
IF
COND
STRCMP
W
P
WORD
P
COUNT
REPEATED
WORD
ELSE
IF
COND
LESS
THAN
INTO
LEFT
SUBTREE
P
LEFT
ADDTREE
P
LEFT
W
ELSE
GREATER
THAN
INTO
RIGHT
SUBTREE
P
RIGHT
ADDTREE
P
RIGHT
W
RETURN
P
STORAGE
FOR
THE
NEW
NODE
IS
FETCHED
BY
A
ROUTINE
TALLOC
WHICH
RETURNS
A
POINTER
TO
A
FREE
SPACE
SUITABLE
FOR
HOLDING
A
TREE
NODE
AND
THE
NEW
WORD
IS
COPIED
INTO
A
HIDDEN
SPACE
BY
STRDUP
WE
WILL
DISCUSS
THESE
ROUTINES
IN
A
MOMENT
THE
COUNT
IS
INITIALIZED
AND
THE
TWO
CHILDREN
ARE
MADE
NULL
THIS
PART
OF
THE
CODE
IS
EXECUTED
ONLY
AT
THE
LEAVES
OF
THE
TREE
WHEN
A
NEW
NODE
IS
BEING
ADDED
WE
HAVE
UNWISELY
OMITTED
ERROR
CHECKING
ON
THE
VALUES
RETURNED
BY
STRDUP
AND
TALLOC
TREEPRINT
PRINTS
THE
TREE
IN
SORTED
ORDER
AT
EACH
NODE
IT
PRINTS
THE
LEFT
SUBTREE
ALL
THE
WORDS
LESS
THAN
THIS
WORD
THEN
THE
WORD
ITSELF
THEN
THE
RIGHT
SUBTREE
ALL
THE
WORDS
GREATER
IF
YOU
FEEL
SHAKY
ABOUT
HOW
RECURSION
WORKS
SIMULATE
TREEPRINT
AS
IT
OPERATES
ON
THE
TREE
SHOWN
ABOVE
TREEPRINT
IN
ORDER
PRINT
OF
TREE
P
VOID
TREEPRINT
STRUCT
TNODE
P
IF
P
NULL
TREEPRINT
P
LEFT
PRINTF
N
P
COUNT
P
WORD
TREEPRINT
P
RIGHT
A
PRACTICAL
NOTE
IF
THE
TREE
BECOMES
UNBALANCED
BECAUSE
THE
WORDS
DON
T
ARRIVE
IN
RANDOM
ORDER
THE
RUNNING
TIME
OF
THE
PROGRAM
CAN
GROW
TOO
MUCH
AS
A
WORST
CASE
IF
THE
WORDS
ARE
ALREADY
IN
ORDER
THIS
PROGRAM
DOES
AN
EXPENSIVE
SIMULATION
OF
LINEAR
SEARCH
THERE
ARE
GENERALIZATIONS
OF
THE
BINARY
TREE
THAT
DO
NOT
SUFFER
FROM
THIS
WORST
CASE
BEHAVIOR
BUT
WE
WILL
NOT
DESCRIBE
THEM
HERE
BEFORE
LEAVING
THIS
EXAMPLE
IT
IS
ALSO
WORTH
A
BRIEF
DIGRESSION
ON
A
PROBLEM
RELATED
TO
STORAGE
ALLOCATORS
CLEARLY
IT
DESIRABLE
THAT
THERE
BE
ONLY
ONE
STORAGE
ALLOCATOR
IN
A
PROGRAM
EVEN
THOUGH
IT
ALLOCATES
DIFFERENT
KINDS
OF
OBJECTS
BUT
IF
ONE
ALLOCATOR
IS
TO
PROCESS
REQUESTS
FOR
SAY
POINTERS
TO
CHARS
AND
POINTERS
TO
STRUCT
TNODES
TWO
QUESTIONS
ARISE
FIRST
HOW
DOES
IT
MEET
THE
REQUIREMENT
OF
MOST
REAL
MACHINES
THAT
OBJECTS
OF
CERTAIN
TYPES
MUST
SATISFY
ALIGNMENT
RESTRICTIONS
FOR
EXAMPLE
INTEGERS
OFTEN
MUST
BE
LOCATED
AT
EVEN
ADDRESSES
SECOND
WHAT
DECLARATIONS
CAN
COPE
WITH
THE
FACT
THAT
AN
ALLOCATOR
MUST
NECESSARILY
RETURN
DIFFERENT
KINDS
OF
POINTERS
ALIGNMENT
REQUIREMENTS
CAN
GENERALLY
BE
SATISFIED
EASILY
AT
THE
COST
OF
SOME
WASTED
SPACE
BY
ENSURING
THAT
THE
ALLOCATOR
ALWAYS
RETURNS
A
POINTER
THAT
MEETS
ALL
ALIGNMENT
RESTRICTIONS
THE
ALLOC
OF
CHAPTER
DOES
NOT
GUARANTEE
ANY
PARTICULAR
ALIGNMENT
SO
WE
WILL
USE
THE
STANDARD
LIBRARY
FUNCTION
MALLOC
WHICH
DOES
IN
CHAPTER
WE
WILL
SHOW
ONE
WAY
TO
IMPLEMENT
MALLOC
THE
QUESTION
OF
THE
TYPE
DECLARATION
FOR
A
FUNCTION
LIKE
MALLOC
IS
A
VEXING
ONE
FOR
ANY
LANGUAGE
THAT
TAKES
ITS
TYPE
CHECKING
SERIOUSLY
IN
C
THE
PROPER
METHOD
IS
TO
DECLARE
THAT
MALLOC
RETURNS
A
POINTER
TO
VOID
THEN
EXPLICITLY
COERCE
THE
POINTER
INTO
THE
DESIRED
TYPE
WITH
A
CAST
MALLOC
AND
RELATED
ROUTINES
ARE
DECLARED
IN
THE
STANDARD
HEADER
STDLIB
H
THUS
TALLOC
CAN
BE
WRITTEN
AS
INCLUDE
STDLIB
H
TALLOC
MAKE
A
TNODE
STRUCT
TNODE
TALLOC
VOID
RETURN
STRUCT
TNODE
MALLOC
SIZEOF
STRUCT
TNODE
STRDUP
MERELY
COPIES
THE
STRING
GIVEN
BY
ITS
ARGUMENT
INTO
A
SAFE
PLACE
OBTAINED
BY
A
CALL
ON
MALLOC
CHAR
STRDUP
CHAR
MAKE
A
DUPLICATE
OF
CHAR
P
P
CHAR
MALLOC
STRLEN
FOR
IF
P
NULL
STRCPY
P
RETURN
P
MALLOC
RETURNS
NULL
IF
NO
SPACE
IS
AVAILABLE
STRDUP
PASSES
THAT
VALUE
ON
LEAVING
ERROR
HANDLING
TO
ITS
CALLER
STORAGE
OBTAINED
BY
CALLING
MALLOC
MAY
BE
FREED
FOR
RE
USE
BY
CALLING
FREE
SEE
CHAPTERS
AND
EXERCISE
WRITE
A
PROGRAM
THAT
READS
A
C
PROGRAM
AND
PRINTS
IN
ALPHABETICAL
ORDER
EACH
GROUP
OF
VARIABLE
NAMES
THAT
ARE
IDENTICAL
IN
THE
FIRST
CHARACTERS
BUT
DIFFERENT
SOMEWHERE
THEREAFTER
DON
T
COUNT
WORDS
WITHIN
STRINGS
AND
COMMENTS
MAKE
A
PARAMETER
THAT
CAN
BE
SET
FROM
THE
COMMAND
LINE
EXERCISE
WRITE
A
CROSS
REFERENCER
THAT
PRINTS
A
LIST
OF
ALL
WORDS
IN
A
DOCUMENT
AND
FOR
EACH
WORD
A
LIST
OF
THE
LINE
NUMBERS
ON
WHICH
IT
OCCURS
REMOVE
NOISE
WORDS
LIKE
THE
AND
AND
SO
ON
EXERCISE
WRITE
A
PROGRAM
THAT
PRINTS
THE
DISTINCT
WORDS
IN
ITS
INPUT
SORTED
INTO
DECREASING
ORDER
OF
FREQUENCY
OF
OCCURRENCE
PRECEDE
EACH
WORD
BY
ITS
COUNT
TABLE
LOOKUP
IN
THIS
SECTION
WE
WILL
WRITE
THE
INNARDS
OF
A
TABLE
LOOKUP
PACKAGE
TO
ILLUSTRATE
MORE
ASPECTS
OF
STRUCTURES
THIS
CODE
IS
TYPICAL
OF
WHAT
MIGHT
BE
FOUND
IN
THE
SYMBOL
TABLE
MANAGEMENT
ROUTINES
OF
A
MACRO
PROCESSOR
OR
A
COMPILER
FOR
EXAMPLE
CONSIDER
THE
DEFINE
STATEMENT
WHEN
A
LINE
LIKE
DEFINE
IN
IS
ENCOUNTERED
THE
NAME
IN
AND
THE
REPLACEMENT
TEXT
ARE
STORED
IN
A
TABLE
LATER
WHEN
THE
NAME
IN
APPEARS
IN
A
STATEMENT
LIKE
STATE
IN
IT
MUST
BE
REPLACED
BY
THERE
ARE
TWO
ROUTINES
THAT
MANIPULATE
THE
NAMES
AND
REPLACEMENT
TEXTS
INSTALL
T
RECORDS
THE
NAME
AND
THE
REPLACEMENT
TEXT
T
IN
A
TABLE
AND
T
ARE
JUST
CHARACTER
STRINGS
LOOKUP
SEARCHES
FOR
IN
THE
TABLE
AND
RETURNS
A
POINTER
TO
THE
PLACE
WHERE
IT
WAS
FOUND
OR
NULL
IF
IT
WASN
T
THERE
THE
ALGORITHM
IS
A
HASH
SEARCH
THE
INCOMING
NAME
IS
CONVERTED
INTO
A
SMALL
NON
NEGATIVE
INTEGER
WHICH
IS
THEN
USED
TO
INDEX
INTO
AN
ARRAY
OF
POINTERS
AN
ARRAY
ELEMENT
POINTS
TO
THE
BEGINNING
OF
A
LINKED
LIST
OF
BLOCKS
DESCRIBING
NAMES
THAT
HAVE
THAT
HASH
VALUE
IT
IS
NULL
IF
NO
NAMES
HAVE
HASHED
TO
THAT
VALUE
A
BLOCK
IN
THE
LIST
IS
A
STRUCTURE
CONTAINING
POINTERS
TO
THE
NAME
THE
REPLACEMENT
TEXT
AND
THE
NEXT
BLOCK
IN
THE
LIST
A
NULL
NEXT
POINTER
MARKS
THE
END
OF
THE
LIST
STRUCT
NLIST
TABLE
ENTRY
STRUCT
NLIST
NEXT
NEXT
ENTRY
IN
CHAIN
CHAR
NAME
DEFINED
NAME
CHAR
DEFN
REPLACEMENT
TEXT
THE
POINTER
ARRAY
IS
JUST
DEFINE
HASHSIZE
STATIC
STRUCT
NLIST
HASHTAB
HASHSIZE
POINTER
TABLE
THE
HASHING
FUNCTION
WHICH
IS
USED
BY
BOTH
LOOKUP
AND
INSTALL
ADDS
EACH
CHARACTER
VALUE
IN
THE
STRING
TO
A
SCRAMBLED
COMBINATION
OF
THE
PREVIOUS
ONES
AND
RETURNS
THE
REMAINDER
MODULO
THE
ARRAY
SIZE
THIS
IS
NOT
THE
BEST
POSSIBLE
HASH
FUNCTION
BUT
IT
IS
SHORT
AND
EFFECTIVE
HASH
FORM
HASH
VALUE
FOR
STRING
UNSIGNED
HASH
CHAR
UNSIGNED
HASHVAL
FOR
HASHVAL
HASHVAL
HASHVAL
RETURN
HASHVAL
HASHSIZE
UNSIGNED
ARITHMETIC
ENSURES
THAT
THE
HASH
VALUE
IS
NON
NEGATIVE
THE
HASHING
PROCESS
PRODUCES
A
STARTING
INDEX
IN
THE
ARRAY
HASHTAB
IF
THE
STRING
IS
TO
BE
FOUND
ANYWHERE
IT
WILL
BE
IN
THE
LIST
OF
BLOCKS
BEGINNING
THERE
THE
SEARCH
IS
PERFORMED
BY
LOOKUP
IF
LOOKUP
FINDS
THE
ENTRY
ALREADY
PRESENT
IT
RETURNS
A
POINTER
TO
IT
IF
NOT
IT
RETURNS
NULL
LOOKUP
LOOK
FOR
IN
HASHTAB
STRUCT
NLIST
LOOKUP
CHAR
STRUCT
NLIST
NP
FOR
NP
HASHTAB
HASH
NP
NULL
NP
NP
NEXT
IF
STRCMP
NP
NAME
RETURN
NP
FOUND
RETURN
NULL
NOT
FOUND
THE
FOR
LOOP
IN
LOOKUP
IS
THE
STANDARD
IDIOM
FOR
WALKING
ALONG
A
LINKED
LIST
FOR
PTR
HEAD
PTR
NULL
PTR
PTR
NEXT
INSTALL
USES
LOOKUP
TO
DETERMINE
WHETHER
THE
NAME
BEING
INSTALLED
IS
ALREADY
PRESENT
IF
SO
THE
NEW
DEFINITION
WILL
SUPERSEDE
THE
OLD
ONE
OTHERWISE
A
NEW
ENTRY
IS
CREATED
INSTALL
RETURNS
NULL
IF
FOR
ANY
REASON
THERE
IS
NO
ROOM
FOR
A
NEW
ENTRY
STRUCT
NLIST
LOOKUP
CHAR
CHAR
STRDUP
CHAR
INSTALL
PUT
NAME
DEFN
IN
HASHTAB
STRUCT
NLIST
INSTALL
CHAR
NAME
CHAR
DEFN
STRUCT
NLIST
NP
UNSIGNED
HASHVAL
IF
NP
LOOKUP
NAME
NULL
NOT
FOUND
NP
STRUCT
NLIST
MALLOC
SIZEOF
NP
IF
NP
NULL
NP
NAME
STRDUP
NAME
NULL
RETURN
NULL
HASHVAL
HASH
NAME
NP
NEXT
HASHTAB
HASHVAL
HASHTAB
HASHVAL
NP
ELSE
ALREADY
THERE
FREE
VOID
NP
DEFN
FREE
PREVIOUS
DEFN
IF
NP
DEFN
STRDUP
DEFN
NULL
RETURN
NULL
RETURN
NP
EXERCISE
WRITE
A
FUNCTION
UNDEF
THAT
WILL
REMOVE
A
NAME
AND
DEFINITION
FROM
THE
TABLE
MAINTAINED
BY
LOOKUP
AND
INSTALL
EXERCISE
IMPLEMENT
A
SIMPLE
VERSION
OF
THE
DEFINE
PROCESSOR
I
E
NO
ARGUMENTS
SUITABLE
FOR
USE
WITH
C
PROGRAMS
BASED
ON
THE
ROUTINES
OF
THIS
SECTION
YOU
MAY
ALSO
FIND
GETCH
AND
UNGETCH
HELPFUL
TYPEDEF
C
PROVIDES
A
FACILITY
CALLED
TYPEDEF
FOR
CREATING
NEW
DATA
TYPE
NAMES
FOR
EXAMPLE
THE
DECLARATION
TYPEDEF
INT
LENGTH
MAKES
THE
NAME
LENGTH
A
SYNONYM
FOR
INT
THE
TYPE
LENGTH
CAN
BE
USED
IN
DECLARATIONS
CASTS
ETC
IN
EXACTLY
THE
SAME
WAYS
THAT
THE
INT
TYPE
CAN
BE
LENGTH
LEN
MAXLEN
LENGTH
LENGTHS
SIMILARLY
THE
DECLARATION
TYPEDEF
CHAR
STRING
MAKES
STRING
A
SYNONYM
FOR
CHAR
OR
CHARACTER
POINTER
WHICH
MAY
THEN
BE
USED
IN
DECLARATIONS
AND
CASTS
STRING
P
LINEPTR
MAXLINES
ALLOC
INT
INT
STRCMP
STRING
STRING
P
STRING
MALLOC
NOTICE
THAT
THE
TYPE
BEING
DECLARED
IN
A
TYPEDEF
APPEARS
IN
THE
POSITION
OF
A
VARIABLE
NAME
NOT
RIGHT
AFTER
THE
WORD
TYPEDEF
SYNTACTICALLY
TYPEDEF
IS
LIKE
THE
STORAGE
CLASSES
EXTERN
STATIC
ETC
WE
HAVE
USED
CAPITALIZED
NAMES
FOR
TYPEDEFS
TO
MAKE
THEM
STAND
OUT
AS
A
MORE
COMPLICATED
EXAMPLE
WE
COULD
MAKE
TYPEDEFS
FOR
THE
TREE
NODES
SHOWN
EARLIER
IN
THIS
CHAPTER
TYPEDEF
STRUCT
TNODE
TREEPTR
TYPEDEF
STRUCT
TNODE
THE
TREE
NODE
CHAR
WORD
POINTS
TO
THE
TEXT
INT
COUNT
NUMBER
OF
OCCURRENCES
STRUCT
TNODE
LEFT
LEFT
CHILD
STRUCT
TNODE
RIGHT
RIGHT
CHILD
TREENODE
THIS
CREATES
TWO
NEW
TYPE
KEYWORDS
CALLED
TREENODE
A
STRUCTURE
AND
TREEPTR
A
POINTER
TO
THE
STRUCTURE
THEN
THE
ROUTINE
TALLOC
COULD
BECOME
TREEPTR
TALLOC
VOID
RETURN
TREEPTR
MALLOC
SIZEOF
TREENODE
IT
MUST
BE
EMPHASIZED
THAT
A
TYPEDEF
DECLARATION
DOES
NOT
CREATE
A
NEW
TYPE
IN
ANY
SENSE
IT
MERELY
ADDS
A
NEW
NAME
FOR
SOME
EXISTING
TYPE
NOR
ARE
THERE
ANY
NEW
SEMANTICS
VARIABLES
DECLARED
THIS
WAY
HAVE
EXACTLY
THE
SAME
PROPERTIES
AS
VARIABLES
WHOSE
DECLARATIONS
ARE
SPELLED
OUT
EXPLICITLY
IN
EFFECT
TYPEDEF
IS
LIKE
DEFINE
EXCEPT
THAT
SINCE
IT
IS
INTERPRETED
BY
THE
COMPILER
IT
CAN
COPE
WITH
TEXTUAL
SUBSTITUTIONS
THAT
ARE
BEYOND
THE
CAPABILITIES
OF
THE
PREPROCESSOR
FOR
EXAMPLE
TYPEDEF
INT
PFI
CHAR
CHAR
CREATES
THE
TYPE
PFI
FOR
POINTER
TO
FUNCTION
OF
TWO
CHAR
ARGUMENTS
RETURNING
INT
WHICH
CAN
BE
USED
IN
CONTEXTS
LIKE
PFI
STRCMP
NUMCMP
IN
THE
SORT
PROGRAM
OF
CHAPTER
BESIDES
PURELY
AESTHETIC
ISSUES
THERE
ARE
TWO
MAIN
REASONS
FOR
USING
TYPEDEFS
THE
FIRST
IS
TO
PARAMETERIZE
A
PROGRAM
AGAINST
PORTABILITY
PROBLEMS
IF
TYPEDEFS
ARE
USED
FOR
DATA
TYPES
THAT
MAY
BE
MACHINE
DEPENDENT
ONLY
THE
TYPEDEFS
NEED
CHANGE
WHEN
THE
PROGRAM
IS
MOVED
ONE
COMMON
SITUATION
IS
TO
USE
TYPEDEF
NAMES
FOR
VARIOUS
INTEGER
QUANTITIES
THEN
MAKE
AN
APPROPRIATE
SET
OF
CHOICES
OF
SHORT
INT
AND
LONG
FOR
EACH
HOST
MACHINE
TYPES
LIKE
AND
FROM
THE
STANDARD
LIBRARY
ARE
EXAMPLES
THE
SECOND
PURPOSE
OF
TYPEDEFS
IS
TO
PROVIDE
BETTER
DOCUMENTATION
FOR
A
PROGRAM
A
TYPE
CALLED
TREEPTR
MAY
BE
EASIER
TO
UNDERSTAND
THAN
ONE
DECLARED
ONLY
AS
A
POINTER
TO
A
COMPLICATED
STRUCTURE
UNIONS
A
UNION
IS
A
VARIABLE
THAT
MAY
HOLD
AT
DIFFERENT
TIMES
OBJECTS
OF
DIFFERENT
TYPES
AND
SIZES
WITH
THE
COMPILER
KEEPING
TRACK
OF
SIZE
AND
ALIGNMENT
REQUIREMENTS
UNIONS
PROVIDE
A
WAY
TO
MANIPULATE
DIFFERENT
KINDS
OF
DATA
IN
A
SINGLE
AREA
OF
STORAGE
WITHOUT
EMBEDDING
ANY
MACHINE
DEPENDENT
INFORMATION
IN
THE
PROGRAM
THEY
ARE
ANALOGOUS
TO
VARIANT
RECORDS
IN
PASCAL
AS
AN
EXAMPLE
SUCH
AS
MIGHT
BE
FOUND
IN
A
COMPILER
SYMBOL
TABLE
MANAGER
SUPPOSE
THAT
A
CONSTANT
MAY
BE
AN
INT
A
FLOAT
OR
A
CHARACTER
POINTER
THE
VALUE
OF
A
PARTICULAR
CONSTANT
MUST
BE
STORED
IN
A
VARIABLE
OF
THE
PROPER
TYPE
YET
IT
IS
MOST
CONVENIENT
FOR
TABLE
MANAGEMENT
IF
THE
VALUE
OCCUPIES
THE
SAME
AMOUNT
OF
STORAGE
AND
IS
STORED
IN
THE
SAME
PLACE
REGARDLESS
OF
ITS
TYPE
THIS
IS
THE
PURPOSE
OF
A
UNION
A
SINGLE
VARIABLE
THAT
CAN
LEGITIMATELY
HOLD
ANY
OF
ONE
OF
SEVERAL
TYPES
THE
SYNTAX
IS
BASED
ON
STRUCTURES
UNION
INT
IVAL
FLOAT
FVAL
CHAR
SVAL
U
THE
VARIABLE
U
WILL
BE
LARGE
ENOUGH
TO
HOLD
THE
LARGEST
OF
THE
THREE
TYPES
THE
SPECIFIC
SIZE
IS
IMPLEMENTATION
DEPENDENT
ANY
OF
THESE
TYPES
MAY
BE
ASSIGNED
TO
U
AND
THEN
USED
IN
EXPRESSIONS
SO
LONG
AS
THE
USAGE
IS
CONSISTENT
THE
TYPE
RETRIEVED
MUST
BE
THE
TYPE
MOST
RECENTLY
STORED
IT
IS
THE
PROGRAMMER
RESPONSIBILITY
TO
KEEP
TRACK
OF
WHICH
TYPE
IS
CURRENTLY
STORED
IN
A
UNION
THE
RESULTS
ARE
IMPLEMENTATION
DEPENDENT
IF
SOMETHING
IS
STORED
AS
ONE
TYPE
AND
EXTRACTED
AS
ANOTHER
SYNTACTICALLY
MEMBERS
OF
A
UNION
ARE
ACCESSED
AS
UNION
NAME
MEMBER
OR
UNION
POINTER
MEMBER
JUST
AS
FOR
STRUCTURES
IF
THE
VARIABLE
UTYPE
IS
USED
TO
KEEP
TRACK
OF
THE
CURRENT
TYPE
STORED
IN
U
THEN
ONE
MIGHT
SEE
CODE
SUCH
AS
IF
UTYPE
INT
PRINTF
D
N
U
IVAL
IF
UTYPE
FLOAT
PRINTF
F
N
U
FVAL
IF
UTYPE
STRING
PRINTF
N
U
SVAL
ELSE
PRINTF
BAD
TYPE
D
IN
UTYPE
N
UTYPE
UNIONS
MAY
OCCUR
WITHIN
STRUCTURES
AND
ARRAYS
AND
VICE
VERSA
THE
NOTATION
FOR
ACCESSING
A
MEMBER
OF
A
UNION
IN
A
STRUCTURE
OR
VICE
VERSA
IS
IDENTICAL
TO
THAT
FOR
NESTED
STRUCTURES
FOR
EXAMPLE
IN
THE
STRUCTURE
ARRAY
DEFINED
BY
STRUCT
CHAR
NAME
INT
FLAGS
INT
UTYPE
UNION
INT
IVAL
FLOAT
FVAL
CHAR
SVAL
U
SYMTAB
NSYM
THE
MEMBER
IVAL
IS
REFERRED
TO
AS
SYMTAB
I
U
IVAL
AND
THE
FIRST
CHARACTER
OF
THE
STRING
SVAL
BY
EITHER
OF
SYMTAB
I
U
SVAL
SYMTAB
I
U
SVAL
IN
EFFECT
A
UNION
IS
A
STRUCTURE
IN
WHICH
ALL
MEMBERS
HAVE
OFFSET
ZERO
FROM
THE
BASE
THE
STRUCTURE
IS
BIG
ENOUGH
TO
HOLD
THE
WIDEST
MEMBER
AND
THE
ALIGNMENT
IS
APPROPRIATE
FOR
ALL
OF
THE
TYPES
IN
THE
UNION
THE
SAME
OPERATIONS
ARE
PERMITTED
ON
UNIONS
AS
ON
STRUCTURES
ASSIGNMENT
TO
OR
COPYING
AS
A
UNIT
TAKING
THE
ADDRESS
AND
ACCESSING
A
MEMBER
A
UNION
MAY
ONLY
BE
INITIALIZED
WITH
A
VALUE
OF
THE
TYPE
OF
ITS
FIRST
MEMBER
THUS
UNION
U
DESCRIBED
ABOVE
CAN
ONLY
BE
INITIALIZED
WITH
AN
INTEGER
VALUE
THE
STORAGE
ALLOCATOR
IN
CHAPTER
SHOWS
HOW
A
UNION
CAN
BE
USED
TO
FORCE
A
VARIABLE
TO
BE
ALIGNED
ON
A
PARTICULAR
KIND
OF
STORAGE
BOUNDARY
BIT
FIELDS
WHEN
STORAGE
SPACE
IS
AT
A
PREMIUM
IT
MAY
BE
NECESSARY
TO
PACK
SEVERAL
OBJECTS
INTO
A
SINGLE
MACHINE
WORD
ONE
COMMON
USE
IS
A
SET
OF
SINGLE
BIT
FLAGS
IN
APPLICATIONS
LIKE
COMPILER
SYMBOL
TABLES
EXTERNALLY
IMPOSED
DATA
FORMATS
SUCH
AS
INTERFACES
TO
HARDWARE
DEVICES
ALSO
OFTEN
REQUIRE
THE
ABILITY
TO
GET
AT
PIECES
OF
A
WORD
IMAGINE
A
FRAGMENT
OF
A
COMPILER
THAT
MANIPULATES
A
SYMBOL
TABLE
EACH
IDENTIFIER
IN
A
PROGRAM
HAS
CERTAIN
INFORMATION
ASSOCIATED
WITH
IT
FOR
EXAMPLE
WHETHER
OR
NOT
IT
IS
A
KEYWORD
WHETHER
OR
NOT
IT
IS
EXTERNAL
AND
OR
STATIC
AND
SO
ON
THE
MOST
COMPACT
WAY
TO
ENCODE
SUCH
INFORMATION
IS
A
SET
OF
ONE
BIT
FLAGS
IN
A
SINGLE
CHAR
OR
INT
THE
USUAL
WAY
THIS
IS
DONE
IS
TO
DEFINE
A
SET
OF
MASKS
CORRESPONDING
TO
THE
RELEVANT
BIT
POSITIONS
AS
IN
DEFINE
KEYWORD
DEFINE
EXTRENAL
OR
DEFINE
STATIC
ENUM
KEYWORD
EXTERNAL
STATIC
THE
NUMBERS
MUST
BE
POWERS
OF
TWO
THEN
ACCESSING
THE
BITS
BECOMES
A
MATTER
OF
BIT
FIDDLING
WITH
THE
SHIFTING
MASKING
AND
COMPLEMENTING
OPERATORS
THAT
WERE
DESCRIBED
IN
CHAPTER
CERTAIN
IDIOMS
APPEAR
FREQUENTLY
FLAGS
EXTERNAL
STATIC
TURNS
ON
THE
EXTERNAL
AND
STATIC
BITS
IN
FLAGS
WHILE
FLAGS
EXTERNAL
STATIC
TURNS
THEM
OFF
AND
IF
FLAGS
EXTERNAL
STATIC
IS
TRUE
IF
BOTH
BITS
ARE
OFF
ALTHOUGH
THESE
IDIOMS
ARE
READILY
MASTERED
AS
AN
ALTERNATIVE
C
OFFERS
THE
CAPABILITY
OF
DEFINING
AND
ACCESSING
FIELDS
WITHIN
A
WORD
DIRECTLY
RATHER
THAN
BY
BITWISE
LOGICAL
OPERATORS
A
BIT
FIELD
OR
FIELD
FOR
SHORT
IS
A
SET
OF
ADJACENT
BITS
WITHIN
A
SINGLE
IMPLEMENTATION
DEFINED
STORAGE
UNIT
THAT
WE
WILL
CALL
A
WORD
FOR
EXAMPLE
THE
SYMBOL
TABLE
DEFINES
ABOVE
COULD
BE
REPLACED
BY
THE
DEFINITION
OF
THREE
FIELDS
STRUCT
UNSIGNED
INT
UNSIGNED
INT
UNSIGNED
INT
FLAGS
THIS
DEFINES
A
VARIABLE
TABLE
CALLED
FLAGS
THAT
CONTAINS
THREE
BIT
FIELDS
THE
NUMBER
FOLLOWING
THE
COLON
REPRESENTS
THE
FIELD
WIDTH
IN
BITS
THE
FIELDS
ARE
DECLARED
UNSIGNED
INT
TO
ENSURE
THAT
THEY
ARE
UNSIGNED
QUANTITIES
INDIVIDUAL
FIELDS
ARE
REFERENCED
IN
THE
SAME
WAY
AS
OTHER
STRUCTURE
MEMBERS
FLAGS
FLAGS
ETC
FIELDS
BEHAVE
LIKE
SMALL
INTEGERS
AND
MAY
PARTICIPATE
IN
ARITHMETIC
EXPRESSIONS
JUST
LIKE
OTHER
INTEGERS
THUS
THE
PREVIOUS
EXAMPLES
MAY
BE
WRITTEN
MORE
NATURALLY
AS
FLAGS
FLAGS
TO
TURN
THE
BITS
ON
FLAGS
FLAGS
TO
TURN
THEM
OFF
AND
IF
FLAGS
FLAGS
TO
TEST
THEM
ALMOST
EVERYTHING
ABOUT
FIELDS
IS
IMPLEMENTATION
DEPENDENT
WHETHER
A
FIELD
MAY
OVERLAP
A
WORD
BOUNDARY
IS
IMPLEMENTATION
DEFINED
FIELDS
NEED
NOT
BE
NAMES
UNNAMED
FIELDS
A
COLON
AND
WIDTH
ONLY
ARE
USED
FOR
PADDING
THE
SPECIAL
WIDTH
MAY
BE
USED
TO
FORCE
ALIGNMENT
AT
THE
NEXT
WORD
BOUNDARY
FIELDS
ARE
ASSIGNED
LEFT
TO
RIGHT
ON
SOME
MACHINES
AND
RIGHT
TO
LEFT
ON
OTHERS
THIS
MEANS
THAT
ALTHOUGH
FIELDS
ARE
USEFUL
FOR
MAINTAINING
INTERNALLY
DEFINED
DATA
STRUCTURES
THE
QUESTION
OF
WHICH
END
COMES
FIRST
HAS
TO
BE
CAREFULLY
CONSIDERED
WHEN
PICKING
APART
EXTERNALLY
DEFINED
DATA
PROGRAMS
THAT
DEPEND
ON
SUCH
THINGS
ARE
NOT
PORTABLE
FIELDS
MAY
BE
DECLARED
ONLY
AS
INTS
FOR
PORTABILITY
SPECIFY
SIGNED
OR
UNSIGNED
EXPLICITLY
THEY
ARE
NOT
ARRAYS
AND
THEY
DO
NOT
HAVE
ADDRESSES
SO
THE
OPERATOR
CANNOT
BE
APPLIED
ON
THEM
WHITEPAPER
NVIDIA
NEXT
GENERATION
CUDATM
COMPUTE
ARCHITECTURE
FERMITM
TABLE
OF
CONTENTS
A
BRIEF
HISTORY
OF
GPU
COMPUTING
THE
ARCHITECTURE
NVIDIA
NEXT
GENERATION
CUDA
COMPUTE
AND
GRAPHICS
ARCHITECTURE
CODE
NAMED
FERMI
A
QUICK
REFRESHER
ON
CUDA
HARDWARE
EXECUTION
AN
OVERVIEW
OF
THE
FERMI
ARCHITECTURE
THIRD
GENERATION
STREAMING
MULTIPROCESSOR
HIGH
PERFORMANCE
CUDA
CORES
LOAD
STORE
UNITS
FOUR
SPECIAL
FUNCTION
UNITS
DESIGNED
FOR
DOUBLE
PRECISION
DUAL
WARP
SCHEDULER
KB
CONFIGURABLE
SHARED
MEMORY
AND
CACHE
SUMMARY
TABLE
SECOND
GENERATION
PARALLEL
THREAD
EXECUTION
ISA
UNIFIED
ADDRESS
SPACE
ENABLES
FULL
C
SUPPORT
OPTIMIZED
FOR
OPENCL
AND
DIRECTCOMPUTE
IEEE
BIT
FLOATING
POINT
PRECISION
IMPROVED
CONDITIONAL
PERFORMANCE
THROUGH
PREDICATION
MEMORY
SUBSYSTEM
INNOVATIONS
NVIDIA
PARALLEL
DATACACHETM
WITH
CONFIGURABLE
AND
UNIFIED
CACHE
FIRST
GPU
WITH
ECC
MEMORY
SUPPORT
FAST
ATOMIC
MEMORY
OPERATIONS
GIGATHREADTM
THREAD
SCHEDULER
FASTER
APPLICATION
CONTEXT
SWITCHING
CONCURRENT
KERNEL
EXECUTION
INTRODUCING
NVIDIA
NEXUS
CONCLUSION
A
BRIEF
HISTORY
OF
GPU
COMPUTING
THE
GRAPHICS
PROCESSING
UNIT
GPU
FIRST
INVENTED
BY
NVIDIA
IN
IS
THE
MOST
PERVASIVE
PARALLEL
PROCESSOR
TO
DATE
FUELED
BY
THE
INSATIABLE
DESIRE
FOR
LIFE
LIKE
REAL
TIME
GRAPHICS
THE
GPU
HAS
EVOLVED
INTO
A
PROCESSOR
WITH
UNPRECEDENTED
FLOATING
POINT
PERFORMANCE
AND
PROGRAMMABILITY
TODAY
GPUS
GREATLY
OUTPACE
CPUS
IN
ARITHMETIC
THROUGHPUT
AND
MEMORY
BANDWIDTH
MAKING
THEM
THE
IDEAL
PROCESSOR
TO
ACCELERATE
A
VARIETY
OF
DATA
PARALLEL
APPLICATIONS
EFFORTS
TO
EXPLOIT
THE
GPU
FOR
NON
GRAPHICAL
APPLICATIONS
HAVE
BEEN
UNDERWAY
SINCE
BY
USING
HIGH
LEVEL
SHADING
LANGUAGES
SUCH
AS
DIRECTX
OPENGL
AND
CG
VARIOUS
DATA
PARALLEL
ALGORITHMS
HAVE
BEEN
PORTED
TO
THE
GPU
PROBLEMS
SUCH
AS
PROTEIN
FOLDING
STOCK
OPTIONS
PRICING
SQL
QUERIES
AND
MRI
RECONSTRUCTION
ACHIEVED
REMARKABLE
PERFORMANCE
SPEEDUPS
ON
THE
GPU
THESE
EARLY
EFFORTS
THAT
USED
GRAPHICS
APIS
FOR
GENERAL
PURPOSE
COMPUTING
WERE
KNOWN
AS
GPGPU
PROGRAMS
WHILE
THE
GPGPU
MODEL
DEMONSTRATED
GREAT
SPEEDUPS
IT
FACED
SEVERAL
DRAWBACKS
FIRST
IT
REQUIRED
THE
PROGRAMMER
TO
POSSESS
INTIMATE
KNOWLEDGE
OF
GRAPHICS
APIS
AND
GPU
ARCHITECTURE
SECOND
PROBLEMS
HAD
TO
BE
EXPRESSED
IN
TERMS
OF
VERTEX
COORDINATES
TEXTURES
AND
SHADER
PROGRAMS
GREATLY
INCREASING
PROGRAM
COMPLEXITY
THIRD
BASIC
PROGRAMMING
FEATURES
SUCH
AS
RANDOM
READS
AND
WRITES
TO
MEMORY
WERE
NOT
SUPPORTED
GREATLY
RESTRICTING
THE
PROGRAMMING
MODEL
LASTLY
THE
LACK
OF
DOUBLE
PRECISION
SUPPORT
UNTIL
RECENTLY
MEANT
SOME
SCIENTIFIC
APPLICATIONS
COULD
NOT
BE
RUN
ON
THE
GPU
TO
ADDRESS
THESE
PROBLEMS
NVIDIA
INTRODUCED
TWO
KEY
TECHNOLOGIES
THE
UNIFIED
GRAPHICS
AND
COMPUTE
ARCHITECTURE
FIRST
INTRODUCED
IN
GEFORCE
QUADRO
FX
AND
TESLA
GPUS
AND
CUDA
A
SOFTWARE
AND
HARDWARE
ARCHITECTURE
THAT
ENABLED
THE
GPU
TO
BE
PROGRAMMED
WITH
A
VARIETY
OF
HIGH
LEVEL
PROGRAMMING
LANGUAGES
TOGETHER
THESE
TWO
TECHNOLOGIES
REPRESENTED
A
NEW
WAY
OF
USING
THE
GPU
INSTEAD
OF
PROGRAMMING
DEDICATED
GRAPHICS
UNITS
WITH
GRAPHICS
APIS
THE
PROGRAMMER
COULD
NOW
WRITE
C
PROGRAMS
WITH
CUDA
EXTENSIONS
AND
TARGET
A
GENERAL
PURPOSE
MASSIVELY
PARALLEL
PROCESSOR
WE
CALLED
THIS
NEW
WAY
OF
GPU
PROGRAMMING
GPU
COMPUTING
IT
SIGNIFIED
BROADER
APPLICATION
SUPPORT
WIDER
PROGRAMMING
LANGUAGE
SUPPORT
AND
A
CLEAR
SEPARATION
FROM
THE
EARLY
GPGPU
MODEL
OF
PROGRAMMING
THE
ARCHITECTURE
NVIDIA
GEFORCE
WAS
THE
PRODUCT
THAT
GAVE
BIRTH
TO
THE
NEW
GPU
COMPUTING
MODEL
INTRODUCED
IN
NOVEMBER
THE
BASED
GEFORCE
BROUGHT
SEVERAL
KEY
INNOVATIONS
TO
GPU
COMPUTING
WAS
THE
FIRST
GPU
TO
SUPPORT
C
ALLOWING
PROGRAMMERS
TO
USE
THE
POWER
OF
THE
GPU
WITHOUT
HAVING
TO
LEARN
A
NEW
PROGRAMMING
LANGUAGE
WAS
THE
FIRST
GPU
TO
REPLACE
THE
SEPARATE
VERTEX
AND
PIXEL
PIPELINES
WITH
A
SINGLE
UNIFIED
PROCESSOR
THAT
EXECUTED
VERTEX
GEOMETRY
PIXEL
AND
COMPUTING
PROGRAMS
WAS
THE
FIRST
GPU
TO
UTILIZE
A
SCALAR
THREAD
PROCESSOR
ELIMINATING
THE
NEED
FOR
PROGRAMMERS
TO
MANUALLY
MANAGE
VECTOR
REGISTERS
INTRODUCED
THE
SINGLE
INSTRUCTION
MULTIPLE
THREAD
SIMT
EXECUTION
MODEL
WHERE
MULTIPLE
INDEPENDENT
THREADS
EXECUTE
CONCURRENTLY
USING
A
SINGLE
INSTRUCTION
INTRODUCED
SHARED
MEMORY
AND
BARRIER
SYNCHRONIZATION
FOR
INTER
THREAD
COMMUNICATION
IN
JUNE
NVIDIA
INTRODUCED
A
MAJOR
REVISION
TO
THE
ARCHITECTURE
THE
SECOND
GENERATION
UNIFIED
ARCHITECTURE
FIRST
INTRODUCED
IN
THE
GEFORCE
GTX
QUADRO
FX
AND
TESLA
GPUS
INCREASED
THE
NUMBER
OF
STREAMING
PROCESSOR
CORES
SUBSEQUENTLY
REFERRED
TO
AS
CUDA
CORES
FROM
TO
EACH
PROCESSOR
REGISTER
FILE
WAS
DOUBLED
IN
SIZE
ALLOWING
A
GREATER
NUMBER
OF
THREADS
TO
EXECUTE
ON
CHIP
AT
ANY
GIVEN
TIME
HARDWARE
MEMORY
ACCESS
COALESCING
WAS
ADDED
TO
IMPROVE
MEMORY
ACCESS
EFFICIENCY
DOUBLE
PRECISION
FLOATING
POINT
SUPPORT
WAS
ALSO
ADDED
TO
ADDRESS
THE
NEEDS
OF
SCIENTIFIC
AND
HIGH
PERFORMANCE
COMPUTING
HPC
APPLICATIONS
WHEN
DESIGNING
EACH
NEW
GENERATION
GPU
IT
HAS
ALWAYS
BEEN
THE
PHILOSOPHY
AT
NVIDIA
TO
IMPROVE
BOTH
EXISTING
APPLICATION
PERFORMANCE
AND
GPU
PROGRAMMABILITY
WHILE
FASTER
APPLICATION
PERFORMANCE
BRINGS
IMMEDIATE
BENEFITS
IT
IS
THE
GPU
RELENTLESS
ADVANCEMENT
IN
PROGRAMMABILITY
THAT
HAS
ALLOWED
IT
TO
EVOLVE
INTO
THE
MOST
VERSATILE
PARALLEL
PROCESSOR
OF
OUR
TIME
IT
WAS
WITH
THIS
MINDSET
THAT
WE
SET
OUT
TO
DEVELOP
THE
SUCCESSOR
TO
THE
ARCHITECTURE
NVIDIA
NEXT
GENERATION
CUDA
COMPUTE
AND
GRAPHICS
ARCHITECTURE
CODE
NAMED
FERMI
THE
FERMI
ARCHITECTURE
IS
THE
MOST
SIGNIFICANT
LEAP
FORWARD
IN
GPU
ARCHITECTURE
SINCE
THE
ORIGINAL
WAS
OUR
INITIAL
VISION
OF
WHAT
A
UNIFIED
GRAPHICS
AND
COMPUTING
PARALLEL
PROCESSOR
SHOULD
LOOK
LIKE
EXTENDED
THE
PERFORMANCE
AND
FUNCTIONALITY
OF
WITH
FERMI
WE
HAVE
TAKEN
ALL
WE
HAVE
LEARNED
FROM
THE
TWO
PRIOR
PROCESSORS
AND
ALL
THE
APPLICATIONS
THAT
WERE
WRITTEN
FOR
THEM
AND
EMPLOYED
A
COMPLETELY
NEW
APPROACH
TO
DESIGN
TO
CREATE
THE
WORLD
FIRST
COMPUTATIONAL
GPU
WHEN
WE
STARTED
LAYING
THE
GROUNDWORK
FOR
FERMI
WE
GATHERED
EXTENSIVE
USER
FEEDBACK
ON
GPU
COMPUTING
SINCE
THE
INTRODUCTION
OF
AND
AND
FOCUSED
ON
THE
FOLLOWING
KEY
AREAS
FOR
IMPROVEMENT
IMPROVE
DOUBLE
PRECISION
PERFORMANCE
WHILE
SINGLE
PRECISION
FLOATING
POINT
PERFORMANCE
WAS
ON
THE
ORDER
OF
TEN
TIMES
THE
PERFORMANCE
OF
DESKTOP
CPUS
SOME
GPU
COMPUTING
APPLICATIONS
DESIRED
MORE
DOUBLE
PRECISION
PERFORMANCE
AS
WELL
ECC
SUPPORT
ECC
ALLOWS
GPU
COMPUTING
USERS
TO
SAFELY
DEPLOY
LARGE
NUMBERS
OF
GPUS
IN
DATACENTER
INSTALLATIONS
AND
ALSO
ENSURE
DATA
SENSITIVE
APPLICATIONS
LIKE
MEDICAL
IMAGING
AND
FINANCIAL
OPTIONS
PRICING
ARE
PROTECTED
FROM
MEMORY
ERRORS
TRUE
CACHE
HIERARCHY
SOME
PARALLEL
ALGORITHMS
WERE
UNABLE
TO
USE
THE
GPU
SHARED
MEMORY
AND
USERS
REQUESTED
A
TRUE
CACHE
ARCHITECTURE
TO
AID
THEM
MORE
SHARED
MEMORY
MANY
CUDA
PROGRAMMERS
REQUESTED
MORE
THAN
KB
OF
SM
SHARED
MEMORY
TO
SPEED
UP
THEIR
APPLICATIONS
FASTER
CONTEXT
SWITCHING
USERS
REQUESTED
FASTER
CONTEXT
SWITCHES
BETWEEN
APPLICATION
PROGRAMS
AND
FASTER
GRAPHICS
AND
COMPUTE
INTEROPERATION
FASTER
ATOMIC
OPERATIONS
USERS
REQUESTED
FASTER
READ
MODIFY
WRITE
ATOMIC
OPERATIONS
FOR
THEIR
PARALLEL
ALGORITHMS
WITH
THESE
REQUESTS
IN
MIND
THE
FERMI
TEAM
DESIGNED
A
PROCESSOR
THAT
GREATLY
INCREASES
RAW
COMPUTE
HORSEPOWER
AND
THROUGH
ARCHITECTURAL
INNOVATIONS
ALSO
OFFERS
DRAMATICALLY
INCREASED
PROGRAMMABILITY
AND
COMPUTE
EFFICIENCY
THE
KEY
ARCHITECTURAL
HIGHLIGHTS
OF
FERMI
ARE
THIRD
GENERATION
STREAMING
MULTIPROCESSOR
SM
O
CUDA
CORES
PER
SM
OVER
O
THE
PEAK
DOUBLE
PRECISION
FLOATING
POINT
PERFORMANCE
OVER
O
DUAL
WARP
SCHEDULER
SIMULTANEOUSLY
SCHEDULES
AND
DISPATCHES
INSTRUCTIONS
FROM
TWO
INDEPENDENT
WARPS
O
KB
OF
RAM
WITH
A
CONFIGURABLE
PARTITIONING
OF
SHARED
MEMORY
AND
CACHE
SECOND
GENERATION
PARALLEL
THREAD
EXECUTION
ISA
O
UNIFIED
ADDRESS
SPACE
WITH
FULL
C
SUPPORT
O
OPTIMIZED
FOR
OPENCL
AND
DIRECTCOMPUTE
O
FULL
IEEE
BIT
AND
BIT
PRECISION
O
FULL
BIT
INTEGER
PATH
WITH
BIT
EXTENSIONS
O
MEMORY
ACCESS
INSTRUCTIONS
TO
SUPPORT
TRANSITION
TO
BIT
ADDRESSING
O
IMPROVED
PERFORMANCE
THROUGH
PREDICATION
IMPROVED
MEMORY
SUBSYSTEM
O
NVIDIA
PARALLEL
DATACACHETM
HIERARCHY
WITH
CONFIGURABLE
AND
UNIFIED
CACHES
O
FIRST
GPU
WITH
ECC
MEMORY
SUPPORT
O
GREATLY
IMPROVED
ATOMIC
MEMORY
OPERATION
PERFORMANCE
NVIDIA
GIGATHREADTM
ENGINE
O
FASTER
APPLICATION
CONTEXT
SWITCHING
O
CONCURRENT
KERNEL
EXECUTION
O
OUT
OF
ORDER
THREAD
BLOCK
EXECUTION
O
DUAL
OVERLAPPED
MEMORY
TRANSFER
ENGINES
A
QUICK
REFRESHER
ON
CUDA
CUDA
IS
THE
HARDWARE
AND
SOFTWARE
ARCHITECTURE
THAT
ENABLES
NVIDIA
GPUS
TO
EXECUTE
PROGRAMS
WRITTEN
WITH
C
C
FORTRAN
OPENCL
DIRECTCOMPUTE
AND
OTHER
LANGUAGES
A
CUDA
PROGRAM
CALLS
PARALLEL
KERNELS
A
KERNEL
EXECUTES
IN
PARALLEL
ACROSS
A
SET
OF
PARALLEL
THREADS
THE
PROGRAMMER
OR
COMPILER
ORGANIZES
THESE
THREADS
IN
THREAD
BLOCKS
AND
GRIDS
OF
THREAD
BLOCKS
THE
GPU
INSTANTIATES
A
KERNEL
PROGRAM
ON
A
GRID
OF
PARALLEL
THREAD
BLOCKS
EACH
THREAD
WITHIN
A
THREAD
BLOCK
EXECUTES
AN
INSTANCE
OF
THE
KERNEL
AND
HAS
A
THREAD
ID
WITHIN
ITS
THREAD
BLOCK
PROGRAM
COUNTER
REGISTERS
PER
THREAD
PRIVATE
MEMORY
INPUTS
AND
OUTPUT
RESULTS
A
THREAD
BLOCK
IS
A
SET
OF
CONCURRENTLY
EXECUTING
THREADS
THAT
CAN
COOPERATE
AMONG
THEMSELVES
THROUGH
BARRIER
SYNCHRONIZATION
AND
SHARED
MEMORY
A
THREAD
BLOCK
HAS
A
BLOCK
ID
WITHIN
ITS
GRID
CUDA
HIERARCHY
OF
THREADS
BLOCKS
AND
GRIDS
WITH
CORRESPONDING
PER
THREAD
PRIVATE
PER
BLOCK
SHARED
AND
PER
APPLICATION
GLOBAL
MEMORY
SPACES
A
GRID
IS
AN
ARRAY
OF
THREAD
BLOCKS
THAT
EXECUTE
THE
SAME
KERNEL
READ
INPUTS
FROM
GLOBAL
MEMORY
WRITE
RESULTS
TO
GLOBAL
MEMORY
AND
SYNCHRONIZE
BETWEEN
DEPENDENT
KERNEL
CALLS
IN
THE
CUDA
PARALLEL
PROGRAMMING
MODEL
EACH
THREAD
HAS
A
PER
THREAD
PRIVATE
MEMORY
SPACE
USED
FOR
REGISTER
SPILLS
FUNCTION
CALLS
AND
C
AUTOMATIC
ARRAY
VARIABLES
EACH
THREAD
BLOCK
HAS
A
PER
BLOCK
SHARED
MEMORY
SPACE
USED
FOR
INTER
THREAD
COMMUNICATION
DATA
SHARING
AND
RESULT
SHARING
IN
PARALLEL
ALGORITHMS
GRIDS
OF
THREAD
BLOCKS
SHARE
RESULTS
IN
GLOBAL
MEMORY
SPACE
AFTER
KERNEL
WIDE
GLOBAL
SYNCHRONIZATION
HARDWARE
EXECUTION
CUDA
HIERARCHY
OF
THREADS
MAPS
TO
A
HIERARCHY
OF
PROCESSORS
ON
THE
GPU
A
GPU
EXECUTES
ONE
OR
MORE
KERNEL
GRIDS
A
STREAMING
MULTIPROCESSOR
SM
EXECUTES
ONE
OR
MORE
THREAD
BLOCKS
AND
CUDA
CORES
AND
OTHER
EXECUTION
UNITS
IN
THE
SM
EXECUTE
THREADS
THE
SM
EXECUTES
THREADS
IN
GROUPS
OF
THREADS
CALLED
A
WARP
WHILE
PROGRAMMERS
CAN
GENERALLY
IGNORE
WARP
EXECUTION
FOR
FUNCTIONAL
CORRECTNESS
AND
THINK
OF
PROGRAMMING
ONE
THREAD
THEY
CAN
GREATLY
IMPROVE
PERFORMANCE
BY
HAVING
THREADS
IN
A
WARP
EXECUTE
THE
SAME
CODE
PATH
AND
ACCESS
MEMORY
IN
NEARBY
ADDRESSES
AN
OVERVIEW
OF
THE
FERMI
ARCHITECTURE
THE
FIRST
FERMI
BASED
GPU
IMPLEMENTED
WITH
BILLION
TRANSISTORS
FEATURES
UP
TO
CUDA
CORES
A
CUDA
CORE
EXECUTES
A
FLOATING
POINT
OR
INTEGER
INSTRUCTION
PER
CLOCK
FOR
A
THREAD
THE
CUDA
CORES
ARE
ORGANIZED
IN
SMS
OF
CORES
EACH
THE
GPU
HAS
SIX
BIT
MEMORY
PARTITIONS
FOR
A
BIT
MEMORY
INTERFACE
SUPPORTING
UP
TO
A
TOTAL
OF
GB
OF
DRAM
MEMORY
A
HOST
INTERFACE
CONNECTS
THE
GPU
TO
THE
CPU
VIA
PCI
EXPRESS
THE
GIGATHREAD
GLOBAL
SCHEDULER
DISTRIBUTES
THREAD
BLOCKS
TO
SM
THREAD
SCHEDULERS
FERMI
SM
ARE
POSITIONED
AROUND
A
COMMON
CACHE
EACH
SM
IS
A
VERTICAL
RECTANGULAR
STRIP
THAT
CONTAIN
AN
ORANGE
PORTION
SCHEDULER
AND
DISPATCH
A
GREEN
PORTION
EXECUTION
UNITS
AND
LIGHT
BLUE
PORTIONS
REGISTER
FILE
AND
CACHE
THIRD
GENERATION
STREAMING
MULTIPROCESSOR
THE
THIRD
GENERATION
SM
INTRODUCES
SEVERAL
ARCHITECTURAL
INNOVATIONS
THAT
MAKE
IT
NOT
ONLY
THE
MOST
POWERFUL
SM
YET
BUILT
BUT
ALSO
THE
MOST
PROGRAMMABLE
AND
EFFICIENT
HIGH
PERFORMANCE
CUDA
CORES
EACH
SM
FEATURES
CUDA
PROCESSORS
A
FOURFOLD
INCREASE
OVER
PRIOR
SM
DESIGNS
EACH
CUDA
PROCESSOR
HAS
A
FULLY
PIPELINED
INTEGER
ARITHMETIC
LOGIC
UNIT
ALU
AND
FLOATING
RESULT
QUEUE
POINT
UNIT
FPU
PRIOR
GPUS
USED
IEEE
FLOATING
POINT
ARITHMETIC
THE
FERMI
ARCHITECTURE
IMPLEMENTS
THE
NEW
IEEE
FLOATING
POINT
STANDARD
PROVIDING
THE
FUSED
MULTIPLY
ADD
FMA
INSTRUCTION
FOR
BOTH
SINGLE
AND
DOUBLE
PRECISION
ARITHMETIC
FMA
IMPROVES
OVER
A
MULTIPLY
ADD
MAD
INSTRUCTION
BY
DOING
THE
MULTIPLICATION
AND
ADDITION
WITH
A
SINGLE
FINAL
ROUNDING
STEP
WITH
NO
LOSS
OF
PRECISION
IN
THE
ADDITION
FMA
IS
MORE
ACCURATE
THAN
PERFORMING
THE
OPERATIONS
SEPARATELY
IMPLEMENTED
DOUBLE
PRECISION
FMA
INTERCONNECT
NETWORK
FERMI
STREAMING
MULTIPROCESSOR
SM
IN
THE
INTEGER
ALU
WAS
LIMITED
TO
BIT
PRECISION
FOR
MULTIPLY
OPERATIONS
AS
A
RESULT
MULTI
INSTRUCTION
EMULATION
SEQUENCES
WERE
REQUIRED
FOR
INTEGER
ARITHMETIC
IN
FERMI
THE
NEWLY
DESIGNED
INTEGER
ALU
SUPPORTS
FULL
BIT
PRECISION
FOR
ALL
INSTRUCTIONS
CONSISTENT
WITH
STANDARD
PROGRAMMING
LANGUAGE
REQUIREMENTS
THE
INTEGER
ALU
IS
ALSO
OPTIMIZED
TO
EFFICIENTLY
SUPPORT
BIT
AND
EXTENDED
PRECISION
OPERATIONS
VARIOUS
INSTRUCTIONS
ARE
SUPPORTED
INCLUDING
BOOLEAN
SHIFT
MOVE
COMPARE
CONVERT
BIT
FIELD
EXTRACT
BIT
REVERSE
INSERT
AND
POPULATION
COUNT
LOAD
STORE
UNITS
EACH
SM
HAS
LOAD
STORE
UNITS
ALLOWING
SOURCE
AND
DESTINATION
ADDRESSES
TO
BE
CALCULATED
FOR
SIXTEEN
THREADS
PER
CLOCK
SUPPORTING
UNITS
LOAD
AND
STORE
THE
DATA
AT
EACH
ADDRESS
TO
CACHE
OR
DRAM
FOUR
SPECIAL
FUNCTION
UNITS
SPECIAL
FUNCTION
UNITS
SFUS
EXECUTE
TRANSCENDENTAL
INSTRUCTIONS
SUCH
AS
SIN
COSINE
RECIPROCAL
AND
SQUARE
ROOT
EACH
SFU
EXECUTES
ONE
INSTRUCTION
PER
THREAD
PER
CLOCK
A
WARP
EXECUTES
OVER
EIGHT
CLOCKS
THE
SFU
PIPELINE
IS
DECOUPLED
FROM
THE
DISPATCH
UNIT
ALLOWING
THE
DISPATCH
UNIT
TO
ISSUE
TO
OTHER
EXECUTION
UNITS
WHILE
THE
SFU
IS
OCCUPIED
DESIGNED
FOR
DOUBLE
PRECISION
DOUBLE
PRECISION
ARITHMETIC
IS
AT
THE
HEART
OF
HPC
APPLICATIONS
SUCH
AS
LINEAR
ALGEBRA
NUMERICAL
SIMULATION
AND
QUANTUM
CHEMISTRY
THE
FERMI
ARCHITECTURE
HAS
BEEN
SPECIFICALLY
DESIGNED
TO
OFFER
UNPRECEDENTED
PERFORMANCE
IN
DOUBLE
PRECISION
UP
TO
DOUBLE
PRECISION
FUSED
MULTIPLY
ADD
OPERATIONS
CAN
BE
PERFORMED
PER
SM
PER
CLOCK
A
DRAMATIC
IMPROVEMENT
OVER
THE
ARCHITECTURE
DOUBLE
PRECISION
APPLICATION
PERFORMANCE
ARCHITECTURE
FERMI
ARCHITECTURE
DOUBLE
PRECISION
MATRIX
MULTIPLY
DOUBLE
PRECISION
TRI
DIAGONAL
SOLVER
EARLY
PERFORMANCE
EVALUATIONS
SHOW
FERMI
PERFORMING
UP
TO
FASTER
THAN
IN
DOUBLE
PRECISION
APPLICATIONS
DUAL
WARP
SCHEDULER
THE
SM
SCHEDULES
THREADS
IN
GROUPS
OF
PARALLEL
THREADS
CALLED
WARPS
EACH
SM
FEATURES
TWO
WARP
SCHEDULERS
AND
TWO
INSTRUCTION
DISPATCH
UNITS
ALLOWING
TWO
WARPS
TO
BE
ISSUED
AND
EXECUTED
CONCURRENTLY
FERMI
DUAL
WARP
SCHEDULER
SELECTS
TWO
WARPS
AND
ISSUES
ONE
INSTRUCTION
FROM
EACH
WARP
TO
A
GROUP
OF
SIXTEEN
CORES
SIXTEEN
LOAD
STORE
UNITS
OR
FOUR
SFUS
BECAUSE
WARPS
EXECUTE
INDEPENDENTLY
FERMI
SCHEDULER
DOES
NOT
NEED
TO
CHECK
FOR
DEPENDENCIES
FROM
WITHIN
THE
INSTRUCTION
STREAM
USING
THIS
ELEGANT
MODEL
OF
DUAL
ISSUE
FERMI
ACHIEVES
NEAR
PEAK
HARDWARE
PERFORMANCE
MOST
INSTRUCTIONS
CAN
BE
DUAL
ISSUED
TWO
INTEGER
INSTRUCTIONS
TWO
FLOATING
INSTRUCTIONS
OR
A
MIX
OF
INTEGER
FLOATING
POINT
LOAD
STORE
AND
SFU
INSTRUCTIONS
CAN
BE
ISSUED
CONCURRENTLY
DOUBLE
PRECISION
INSTRUCTIONS
DO
NOT
SUPPORT
DUAL
DISPATCH
WITH
ANY
OTHER
OPERATION
KB
CONFIGURABLE
SHARED
MEMORY
AND
CACHE
ONE
OF
THE
KEY
ARCHITECTURAL
INNOVATIONS
THAT
GREATLY
IMPROVED
BOTH
THE
PROGRAMMABILITY
AND
PERFORMANCE
OF
GPU
APPLICATIONS
IS
ON
CHIP
SHARED
MEMORY
SHARED
MEMORY
ENABLES
THREADS
WITHIN
THE
SAME
THREAD
BLOCK
TO
COOPERATE
FACILITATES
EXTENSIVE
REUSE
OF
ON
CHIP
DATA
AND
GREATLY
REDUCES
OFF
CHIP
TRAFFIC
SHARED
MEMORY
IS
A
KEY
ENABLER
FOR
MANY
HIGH
PERFORMANCE
CUDA
APPLICATIONS
AND
HAVE
KB
OF
SHARED
MEMORY
PER
SM
IN
THE
FERMI
ARCHITECTURE
EACH
SM
HAS
KB
OF
ON
CHIP
MEMORY
THAT
CAN
BE
CONFIGURED
AS
KB
OF
SHARED
MEMORY
WITH
KB
OF
CACHE
OR
AS
KB
OF
SHARED
MEMORY
WITH
KB
OF
CACHE
FOR
EXISTING
APPLICATIONS
THAT
MAKE
EXTENSIVE
USE
OF
SHARED
MEMORY
TRIPLING
THE
AMOUNT
OF
SHARED
MEMORY
YIELDS
SIGNIFICANT
PERFORMANCE
IMPROVEMENTS
ESPECIALLY
FOR
PROBLEMS
THAT
ARE
BANDWIDTH
CONSTRAINED
FOR
EXISTING
APPLICATIONS
THAT
USE
SHARED
MEMORY
AS
SOFTWARE
MANAGED
CACHE
CODE
CAN
BE
STREAMLINED
TO
TAKE
ADVANTAGE
OF
THE
HARDWARE
CACHING
SYSTEM
WHILE
STILL
HAVING
ACCESS
TO
AT
LEAST
KB
OF
SHARED
MEMORY
FOR
EXPLICIT
THREAD
COOPERATION
BEST
OF
ALL
APPLICATIONS
THAT
DO
NOT
USE
SHARED
MEMORY
AUTOMATICALLY
BENEFIT
FROM
THE
CACHE
ALLOWING
HIGH
PERFORMANCE
CUDA
PROGRAMS
TO
BE
BUILT
WITH
MINIMUM
TIME
AND
EFFORT
SUMMARY
TABLE
GPU
FERMI
TRANSISTORS
MILLION
BILLION
BILLION
CUDA
CORES
DOUBLE
PRECISION
FLOATING
POINT
CAPABILITY
NONE
FMA
OPS
CLOCK
FMA
OPS
CLOCK
SINGLE
PRECISION
FLOATING
POINT
CAPABILITY
MAD
OPS
CLOCK
MAD
OPS
CLOCK
FMA
OPS
CLOCK
WARP
SCHEDULERS
PER
SM
SPECIAL
FUNCTION
UNITS
SFUS
SM
SHARED
MEMORY
PER
SM
KB
KB
CONFIGURABLE
KB
OR
KB
CACHE
PER
SM
NONE
NONE
CONFIGURABLE
KB
OR
KB
CACHE
PER
SM
NONE
NONE
KB
ECC
MEMORY
SUPPORT
NO
NO
YES
CONCURRENT
KERNELS
NO
NO
UP
TO
LOAD
STORE
ADDRESS
WIDTH
BIT
BIT
BIT
SECOND
GENERATION
PARALLEL
THREAD
EXECUTION
ISA
FERMI
IS
THE
FIRST
ARCHITECTURE
TO
SUPPORT
THE
NEW
PARALLEL
THREAD
EXECUTION
PTX
INSTRUCTION
SET
PTX
IS
A
LOW
LEVEL
VIRTUAL
MACHINE
AND
ISA
DESIGNED
TO
SUPPORT
THE
OPERATIONS
OF
A
PARALLEL
THREAD
PROCESSOR
AT
PROGRAM
INSTALL
TIME
PTX
INSTRUCTIONS
ARE
TRANSLATED
TO
MACHINE
INSTRUCTIONS
BY
THE
GPU
DRIVER
THE
PRIMARY
GOALS
OF
PTX
ARE
D
PROVIDE
A
STABLE
ISA
THAT
SPANS
MULTIPLE
GPU
GENERATIONS
D
ACHIEVE
FULL
GPU
PERFORMANCE
IN
COMPILED
APPLICATIONS
D
PROVIDE
A
MACHINE
INDEPENDENT
ISA
FOR
C
C
FORTRAN
AND
OTHER
COMPILER
TARGETS
D
PROVIDE
A
CODE
DISTRIBUTION
ISA
FOR
APPLICATION
AND
MIDDLEWARE
DEVELOPERS
D
PROVIDE
A
COMMON
ISA
FOR
OPTIMIZING
CODE
GENERATORS
AND
TRANSLATORS
WHICH
MAP
PTX
TO
SPECIFIC
TARGET
MACHINES
D
FACILITATE
HAND
CODING
OF
LIBRARIES
AND
PERFORMANCE
KERNELS
D
PROVIDE
A
SCALABLE
PROGRAMMING
MODEL
THAT
SPANS
GPU
SIZES
FROM
A
FEW
CORES
TO
MANY
PARALLEL
CORES
PTX
INTRODUCES
SEVERAL
NEW
FEATURES
THAT
GREATLY
IMPROVE
GPU
PROGRAMMABILITY
ACCURACY
AND
PERFORMANCE
THESE
INCLUDE
FULL
IEEE
BIT
FLOATING
POINT
PRECISION
UNIFIED
ADDRESS
SPACE
FOR
ALL
VARIABLES
AND
POINTERS
BIT
ADDRESSING
AND
NEW
INSTRUCTIONS
FOR
OPENCL
AND
DIRECTCOMPUTE
MOST
IMPORTANTLY
PTX
WAS
SPECIFICALLY
DESIGNED
TO
PROVIDE
FULL
SUPPORT
FOR
THE
C
PROGRAMMING
LANGUAGE
UNIFIED
ADDRESS
SPACE
ENABLES
FULL
C
SUPPORT
FERMI
AND
THE
PTX
ISA
IMPLEMENT
A
UNIFIED
ADDRESS
SPACE
THAT
UNIFIES
THE
THREE
SEPARATE
ADDRESS
SPACES
THREAD
PRIVATE
LOCAL
BLOCK
SHARED
AND
GLOBAL
FOR
LOAD
AND
STORE
OPERATIONS
IN
PTX
LOAD
STORE
INSTRUCTIONS
WERE
SPECIFIC
TO
ONE
OF
THE
THREE
ADDRESS
SPACES
PROGRAMS
COULD
LOAD
OR
STORE
VALUES
IN
A
SPECIFIC
TARGET
ADDRESS
SPACE
KNOWN
AT
COMPILE
TIME
IT
WAS
DIFFICULT
TO
FULLY
IMPLEMENT
C
AND
C
POINTERS
SINCE
A
POINTER
TARGET
ADDRESS
SPACE
MAY
NOT
BE
KNOWN
AT
COMPILE
TIME
AND
MAY
ONLY
BE
DETERMINED
DYNAMICALLY
AT
RUN
TIME
WITH
PTX
A
UNIFIED
ADDRESS
SPACE
UNIFIES
ALL
THREE
ADDRESS
SPACES
INTO
A
SINGLE
CONTINUOUS
ADDRESS
SPACE
A
SINGLE
SET
OF
UNIFIED
LOAD
STORE
INSTRUCTIONS
OPERATE
ON
THIS
ADDRESS
SPACE
AUGMENTING
THE
THREE
SEPARATE
SETS
OF
LOAD
STORE
INSTRUCTIONS
FOR
LOCAL
SHARED
AND
GLOBAL
MEMORY
THE
BIT
UNIFIED
ADDRESS
SPACE
SUPPORTS
A
TERABYTE
OF
ADDRESSABLE
MEMORY
AND
THE
LOAD
STORE
ISA
SUPPORTS
BIT
ADDRESSING
FOR
FUTURE
GROWTH
THE
IMPLEMENTATION
OF
A
UNIFIED
ADDRESS
SPACE
ENABLES
FERMI
TO
SUPPORT
TRUE
C
PROGRAMS
IN
C
ALL
VARIABLES
AND
FUNCTIONS
RESIDE
IN
OBJECTS
WHICH
ARE
PASSED
VIA
POINTERS
PTX
MAKES
IT
POSSIBLE
TO
USE
UNIFIED
POINTERS
TO
PASS
OBJECTS
IN
ANY
MEMORY
SPACE
AND
FERMI
HARDWARE
ADDRESS
TRANSLATION
UNIT
AUTOMATICALLY
MAPS
POINTER
REFERENCES
TO
THE
CORRECT
MEMORY
SPACE
FERMI
AND
THE
PTX
ISA
ALSO
ADD
SUPPORT
FOR
C
VIRTUAL
FUNCTIONS
FUNCTION
POINTERS
AND
NEW
AND
DELETE
OPERATORS
FOR
DYNAMIC
OBJECT
ALLOCATION
AND
DE
ALLOCATION
C
EXCEPTION
HANDLING
OPERATIONS
TRY
AND
CATCH
ARE
ALSO
SUPPORTED
OPTIMIZED
FOR
OPENCL
AND
DIRECTCOMPUTE
OPENCL
AND
DIRECTCOMPUTE
ARE
CLOSELY
RELATED
TO
THE
CUDA
PROGRAMMING
MODEL
SHARING
THE
KEY
ABSTRACTIONS
OF
THREADS
THREAD
BLOCKS
GRIDS
OF
THREAD
BLOCKS
BARRIER
SYNCHRONIZATION
PER
BLOCK
SHARED
MEMORY
GLOBAL
MEMORY
AND
ATOMIC
OPERATIONS
FERMI
A
THIRD
GENERATION
CUDA
ARCHITECTURE
IS
BY
NATURE
WELL
OPTIMIZED
FOR
THESE
APIS
IN
ADDITION
FERMI
OFFERS
HARDWARE
SUPPORT
FOR
OPENCL
AND
DIRECTCOMPUTE
SURFACE
INSTRUCTIONS
WITH
FORMAT
CONVERSION
ALLOWING
GRAPHICS
AND
COMPUTE
PROGRAMS
TO
EASILY
OPERATE
ON
THE
SAME
DATA
THE
PTX
ISA
ALSO
ADDS
SUPPORT
FOR
THE
DIRECTCOMPUTE
INSTRUCTIONS
POPULATION
COUNT
APPEND
AND
BIT
REVERSE
IEEE
BIT
FLOATING
POINT
PRECISION
SINGLE
PRECISION
FLOATING
POINT
INSTRUCTIONS
NOW
SUPPORT
SUBNORMAL
NUMBERS
BY
DEFAULT
IN
HARDWARE
AS
WELL
AS
ALL
FOUR
IEEE
ROUNDING
MODES
NEAREST
ZERO
POSITIVE
INFINITY
AND
NEGATIVE
INFINITY
SUBNORMAL
NUMBERS
ARE
SMALL
NUMBERS
THAT
LIE
BETWEEN
ZERO
AND
THE
SMALLEST
NORMALIZED
NUMBER
OF
A
GIVEN
FLOATING
POINT
NUMBER
SYSTEM
PRIOR
GENERATION
GPUS
FLUSHED
SUBNORMAL
OPERANDS
AND
RESULTS
TO
ZERO
INCURRING
A
LOSS
OF
ACCURACY
CPUS
TYPICALLY
PERFORM
SUBNORMAL
CALCULATIONS
IN
EXCEPTION
HANDLING
SOFTWARE
TAKING
THOUSANDS
OF
CYCLES
FERMI
FLOATING
POINT
UNITS
HANDLE
SUBNORMAL
NUMBERS
IN
HARDWARE
ALLOWING
VALUES
TO
GRADUALLY
UNDERFLOW
TO
ZERO
WITH
NO
PERFORMANCE
PENALTY
A
FREQUENTLY
USED
SEQUENCE
OF
OPERATIONS
IN
COMPUTER
GRAPHICS
LINEAR
ALGEBRA
AND
SCIENTIFIC
APPLICATIONS
IS
TO
MULTIPLY
TWO
NUMBERS
ADDING
THE
PRODUCT
TO
A
THIRD
NUMBER
FOR
EXAMPLE
D
A
B
C
PRIOR
GENERATION
GPUS
ACCELERATED
THIS
FUNCTION
WITH
THE
MULTIPLY
ADD
MAD
INSTRUCTION
THAT
ALLOWED
BOTH
OPERATIONS
TO
BE
PERFORMED
IN
A
SINGLE
CLOCK
THE
MAD
INSTRUCTION
PERFORMS
A
MULTIPLICATION
WITH
TRUNCATION
FOLLOWED
BY
AN
ADDITION
WITH
ROUND
TO
NEAREST
EVEN
FERMI
IMPLEMENTS
THE
NEW
FUSED
MULTIPLY
ADD
FMA
INSTRUCTION
FOR
BOTH
BIT
SINGLE
PRECISION
AND
BIT
DOUBLE
PRECISION
FLOATING
POINT
NUMBERS
SUPPORTED
FMA
ONLY
IN
DOUBLE
PRECISION
THAT
IMPROVES
UPON
MULTIPLY
ADD
BY
RETAINING
FULL
PRECISION
IN
THE
INTERMEDIATE
STAGE
THE
INCREASE
IN
PRECISION
BENEFITS
A
NUMBER
OF
ALGORITHMS
SUCH
AS
RENDERING
FINE
INTERSECTING
GEOMETRY
GREATER
PRECISION
IN
ITERATIVE
MATHEMATICAL
CALCULATIONS
AND
FAST
EXACTLY
ROUNDED
DIVISION
AND
SQUARE
ROOT
OPERATIONS
IMPROVED
CONDITIONAL
PERFORMANCE
THROUGH
PREDICATION
IN
THE
FERMI
ISA
THE
NATIVE
HARDWARE
PREDICATION
SUPPORT
USED
FOR
DIVERGENT
THREAD
MANAGEMENT
IS
NOW
AVAILABLE
AT
THE
INSTRUCTION
LEVEL
PREDICATION
ENABLES
SHORT
CONDITIONAL
CODE
SEGMENTS
TO
EXECUTE
EFFICIENTLY
WITH
NO
BRANCH
INSTRUCTION
OVERHEAD
MEMORY
SUBSYSTEM
INNOVATIONS
NVIDIA
PARALLEL
DATACACHETM
WITH
CONFIGURABLE
AND
UNIFIED
CACHE
WORKING
WITH
HUNDREDS
OF
GPU
COMPUTING
APPLICATIONS
FROM
VARIOUS
INDUSTRIES
WE
LEARNED
THAT
WHILE
SHARED
MEMORY
BENEFITS
MANY
PROBLEMS
IT
IS
NOT
APPROPRIATE
FOR
ALL
PROBLEMS
SOME
ALGORITHMS
MAP
NATURALLY
TO
SHARED
MEMORY
OTHERS
REQUIRE
A
CACHE
WHILE
OTHERS
REQUIRE
A
COMBINATION
OF
BOTH
THE
OPTIMAL
MEMORY
HIERARCHY
SHOULD
OFFER
THE
BENEFITS
OF
BOTH
SHARED
MEMORY
AND
CACHE
AND
ALLOW
THE
PROGRAMMER
A
CHOICE
OVER
ITS
PARTITIONING
THE
FERMI
MEMORY
HIERARCHY
ADAPTS
TO
BOTH
TYPES
OF
PROGRAM
BEHAVIOR
ADDING
A
TRUE
CACHE
HIERARCHY
FOR
LOAD
STORE
OPERATIONS
PRESENTED
SIGNIFICANT
CHALLENGES
TRADITIONAL
GPU
ARCHITECTURES
SUPPORT
A
READ
ONLY
LOAD
PATH
FOR
TEXTURE
OPERATIONS
AND
A
WRITE
ONLY
EXPORT
PATH
FOR
PIXEL
DATA
OUTPUT
HOWEVER
THIS
APPROACH
IS
POORLY
SUITED
TO
EXECUTING
GENERAL
PURPOSE
C
OR
C
THREAD
PROGRAMS
THAT
EXPECT
READS
AND
WRITES
TO
BE
ORDERED
AS
ONE
EXAMPLE
SPILLING
A
REGISTER
OPERAND
TO
MEMORY
AND
THEN
READING
IT
BACK
CREATES
A
READ
AFTER
WRITE
HAZARD
IF
THE
READ
AND
WRITE
PATHS
ARE
SEPARATE
IT
MAY
BE
NECESSARY
TO
EXPLICITLY
FLUSH
THE
ENTIRE
WRITE
EXPORT
PATH
BEFORE
IT
IS
SAFE
TO
ISSUE
THE
READ
AND
ANY
CACHES
ON
THE
READ
PATH
WOULD
NOT
BE
COHERENT
WITH
RESPECT
TO
THE
WRITE
DATA
THE
FERMI
ARCHITECTURE
ADDRESSES
THIS
CHALLENGE
BY
IMPLEMENTING
A
SINGLE
UNIFIED
MEMORY
REQUEST
PATH
FOR
LOADS
AND
STORES
WITH
AN
CACHE
PER
SM
MULTIPROCESSOR
AND
UNIFIED
CACHE
THAT
SERVICES
ALL
OPERATIONS
LOAD
STORE
AND
TEXTURE
THE
PER
SM
CACHE
IS
CONFIGURABLE
TO
SUPPORT
BOTH
SHARED
MEMORY
AND
CACHING
OF
LOCAL
AND
GLOBAL
MEMORY
OPERATIONS
THE
KB
MEMORY
CAN
BE
CONFIGURED
AS
EITHER
KB
OF
SHARED
MEMORY
WITH
KB
OF
CACHE
OR
KB
OF
SHARED
MEMORY
WITH
KB
OF
CACHE
WHEN
CONFIGURED
WITH
KB
OF
SHARED
MEMORY
PROGRAMS
THAT
MAKE
EXTENSIVE
USE
OF
SHARED
MEMORY
SUCH
AS
ELECTRODYNAMIC
SIMULATIONS
CAN
PERFORM
UP
TO
THREE
TIMES
FASTER
FOR
PROGRAMS
WHOSE
MEMORY
ACCESSES
ARE
NOT
KNOWN
BEFOREHAND
THE
KB
CACHE
CONFIGURATION
OFFERS
GREATLY
IMPROVED
PERFORMANCE
OVER
DIRECT
ACCESS
TO
DRAM
RADIX
SORT
USING
SHARED
MEMORY
ARCHITECTURE
FERMI
ARCHITECTURE
IN
EITHER
CONFIGURATION
THE
CACHE
ALSO
HELPS
BY
CACHING
TEMPORARY
REGISTER
SPILLS
OF
COMPLEX
PROGRAMS
PRIOR
GENERATION
GPUS
SPILLED
REGISTERS
DIRECTLY
TO
DRAM
INCREASING
ACCESS
LATENCY
WITH
THE
CACHE
PERFORMANCE
SCALES
GRACEFULLY
WITH
INCREASED
TEMPORARY
REGISTER
USAGE
FERMI
FEATURES
A
KB
UNIFIED
CACHE
THAT
WHEN
USING
KB
OF
SHARED
MEMORY
ON
FERMI
RADIX
SORT
EXECUTES
FASTER
THAN
PHYSX
FLUID
COLLISION
FOR
CONVEX
SHAPES
150
SERVICES
ALL
LOAD
STORE
AND
TEXTURE
REQUESTS
THE
PROVIDES
EFFICIENT
HIGH
SPEED
DATA
SHARING
ACROSS
THE
GPU
ALGORITHMS
FOR
WHICH
DATA
ADDRESSES
ARE
NOT
KNOWN
BEFOREHAND
SUCH
AS
PHYSICS
SOLVERS
RAYTRACING
AND
SPARSE
MATRIX
MULTIPLICATION
ESPECIALLY
BENEFIT
FROM
THE
CACHE
HIERARCHY
FILTER
AND
CONVOLUTION
KERNELS
THAT
REQUIRE
MULTIPLE
SMS
TO
READ
THE
SAME
DATA
ALSO
BENEFIT
ARCHITECTURE
FERMI
ARCHITECTURE
PHYSICS
ALGORITHMS
SUCH
AS
FLUID
SIMULATIONS
ESPECIALLY
BENEFIT
FROM
FERMI
CACHES
FOR
CONVEX
SHAPE
COLLISIONS
FERMI
IS
FASTER
THAN
FIRST
GPU
WITH
ECC
MEMORY
SUPPORT
FERMI
IS
THE
FIRST
GPU
TO
SUPPORT
ERROR
CORRECTING
CODE
ECC
BASED
PROTECTION
OF
DATA
IN
MEMORY
ECC
WAS
REQUESTED
BY
GPU
COMPUTING
USERS
TO
ENHANCE
DATA
INTEGRITY
IN
HIGH
PERFORMANCE
COMPUTING
ENVIRONMENTS
ECC
IS
A
HIGHLY
DESIRED
FEATURE
IN
AREAS
SUCH
AS
MEDICAL
IMAGING
AND
LARGE
SCALE
CLUSTER
COMPUTING
NATURALLY
OCCURRING
RADIATION
CAN
CAUSE
A
BIT
STORED
IN
MEMORY
TO
BE
ALTERED
RESULTING
IN
A
SOFT
ERROR
ECC
TECHNOLOGY
DETECTS
AND
CORRECTS
SINGLE
BIT
SOFT
ERRORS
BEFORE
THEY
AFFECT
THE
SYSTEM
BECAUSE
THE
PROBABILITY
OF
SUCH
RADIATION
INDUCED
ERRORS
INCREASE
LINEARLY
WITH
THE
NUMBER
OF
INSTALLED
SYSTEMS
ECC
IS
AN
ESSENTIAL
REQUIREMENT
IN
LARGE
CLUSTER
INSTALLATIONS
FERMI
SUPPORTS
SINGLE
ERROR
CORRECT
DOUBLE
ERROR
DETECT
SECDED
ECC
CODES
THAT
CORRECT
ANY
SINGLE
BIT
ERROR
IN
HARDWARE
AS
THE
DATA
IS
ACCESSED
IN
ADDITION
SECDED
ECC
ENSURES
THAT
ALL
DOUBLE
BIT
ERRORS
AND
MANY
MULTI
BIT
ERRORS
ARE
ALSO
BE
DETECTED
AND
REPORTED
SO
THAT
THE
PROGRAM
CAN
BE
RE
RUN
RATHER
THAN
BEING
ALLOWED
TO
CONTINUE
EXECUTING
WITH
BAD
DATA
FERMI
REGISTER
FILES
SHARED
MEMORIES
CACHES
CACHE
AND
DRAM
MEMORY
ARE
ECC
PROTECTED
MAKING
IT
NOT
ONLY
THE
MOST
POWERFUL
GPU
FOR
HPC
APPLICATIONS
BUT
ALSO
THE
MOST
RELIABLE
IN
ADDITION
FERMI
SUPPORTS
INDUSTRY
STANDARDS
FOR
CHECKING
OF
DATA
DURING
TRANSMISSION
FROM
CHIP
TO
CHIP
ALL
NVIDIA
GPUS
INCLUDE
SUPPORT
FOR
THE
PCI
EXPRESS
STANDARD
FOR
CRC
CHECK
WITH
RETRY
AT
THE
DATA
LINK
LAYER
FERMI
ALSO
SUPPORTS
THE
SIMILAR
STANDARD
FOR
CRC
CHECK
WITH
RETRY
AKA
EDC
DURING
TRANSMISSION
OF
DATA
ACROSS
THE
MEMORY
BUS
FAST
ATOMIC
MEMORY
OPERATIONS
ATOMIC
MEMORY
OPERATIONS
ARE
IMPORTANT
IN
PARALLEL
PROGRAMMING
ALLOWING
CONCURRENT
THREADS
TO
CORRECTLY
PERFORM
READ
MODIFY
WRITE
OPERATIONS
ON
SHARED
DATA
STRUCTURES
ATOMIC
OPERATIONS
SUCH
AS
ADD
MIN
MAX
AND
COMPARE
AND
SWAP
ARE
ATOMIC
IN
THE
SENSE
THAT
THE
READ
MODIFY
AND
WRITE
OPERATIONS
ARE
PERFORMED
WITHOUT
INTERRUPTION
BY
OTHER
THREADS
ATOMIC
MEMORY
OPERATIONS
ARE
WIDELY
USED
FOR
PARALLEL
SORTING
REDUCTION
OPERATIONS
AND
BUILDING
DATA
STRUCTURES
IN
PARALLEL
WITHOUT
LOCKS
THAT
SERIALIZE
THREAD
EXECUTION
THANKS
TO
A
COMBINATION
OF
MORE
ATOMIC
UNITS
IN
HARDWARE
AND
THE
ADDITION
OF
THE
CACHE
ATOMIC
OPERATIONS
PERFORMANCE
IS
UP
TO
FASTER
IN
FERMI
COMPARED
TO
THE
GENERATION
GIGATHREADTM
THREAD
SCHEDULER
ONE
OF
THE
MOST
IMPORTANT
TECHNOLOGIES
OF
THE
FERMI
ARCHITECTURE
IS
ITS
TWO
LEVEL
DISTRIBUTED
THREAD
SCHEDULER
AT
THE
CHIP
LEVEL
A
GLOBAL
WORK
DISTRIBUTION
ENGINE
SCHEDULES
THREAD
BLOCKS
TO
VARIOUS
SMS
WHILE
AT
THE
SM
LEVEL
EACH
WARP
SCHEDULER
DISTRIBUTES
WARPS
OF
THREADS
TO
ITS
EXECUTION
UNITS
THE
FIRST
GENERATION
GIGATHREAD
ENGINE
INTRODUCED
IN
MANAGED
UP
TO
THREADS
IN
REALTIME
THE
FERMI
ARCHITECTURE
IMPROVES
ON
THIS
FOUNDATION
BY
PROVIDING
NOT
ONLY
GREATER
THREAD
THROUGHPUT
BUT
DRAMATICALLY
FASTER
CONTEXT
SWITCHING
CONCURRENT
KERNEL
EXECUTION
AND
IMPROVED
THREAD
BLOCK
SCHEDULING
FASTER
APPLICATION
CONTEXT
SWITCHING
LIKE
CPUS
GPUS
SUPPORT
MULTITASKING
THROUGH
THE
USE
OF
CONTEXT
SWITCHING
WHERE
EACH
PROGRAM
RECEIVES
A
TIME
SLICE
OF
THE
PROCESSOR
RESOURCES
THE
FERMI
PIPELINE
IS
OPTIMIZED
TO
REDUCE
THE
COST
OF
AN
APPLICATION
CONTEXT
SWITCH
TO
BELOW
MICROSECONDS
A
SIGNIFICANT
IMPROVEMENT
OVER
LAST
GENERATION
GPUS
BESIDES
IMPROVED
PERFORMANCE
THIS
ALLOWS
DEVELOPERS
TO
CREATE
APPLICATIONS
THAT
TAKE
GREATER
ADVANTAGE
OF
FREQUENT
KERNEL
TO
KERNEL
COMMUNICATION
SUCH
AS
FINE
GRAINED
INTEROPERATION
BETWEEN
GRAPHICS
AND
PHYSX
APPLICATIONS
CONCURRENT
KERNEL
EXECUTION
FERMI
SUPPORTS
CONCURRENT
KERNEL
EXECUTION
WHERE
DIFFERENT
KERNELS
OF
THE
SAME
APPLICATION
CONTEXT
CAN
EXECUTE
ON
THE
GPU
AT
THE
SAME
TIME
CONCURRENT
KERNEL
EXECUTION
ALLOWS
PROGRAMS
THAT
EXECUTE
A
NUMBER
OF
SMALL
KERNELS
TO
UTILIZE
THE
WHOLE
GPU
FOR
EXAMPLE
A
PHYSX
PROGRAM
MAY
INVOKE
A
FLUIDS
SOLVER
AND
A
RIGID
BODY
SOLVER
WHICH
IF
EXECUTED
SEQUENTIALLY
WOULD
USE
ONLY
HALF
OF
THE
AVAILABLE
THREAD
PROCESSORS
ON
THE
FERMI
ARCHITECTURE
DIFFERENT
KERNELS
OF
THE
SAME
CUDA
CONTEXT
CAN
EXECUTE
CONCURRENTLY
ALLOWING
MAXIMUM
UTILIZATION
OF
GPU
RESOURCES
KERNELS
FROM
DIFFERENT
APPLICATION
CONTEXTS
CAN
STILL
RUN
SEQUENTIALLY
WITH
GREAT
EFFICIENCY
THANKS
TO
THE
IMPROVED
CONTEXT
SWITCHING
PERFORMANCE
SERIAL
KERNEL
EXECUTION
CONCURRENT
KERNEL
EXECUTION
INTRODUCING
NVIDIA
NEXUS
NVIDIA
NEXUS
IS
THE
FIRST
DEVELOPMENT
ENVIRONMENT
DESIGNED
SPECIFICALLY
TO
SUPPORT
MASSIVELY
PARALLEL
CUDA
C
OPENCL
AND
DIRECTCOMPUTE
APPLICATIONS
IT
BRIDGES
THE
PRODUCTIVITY
GAP
BETWEEN
CPU
AND
GPU
CODE
BY
BRINGING
PARALLEL
AWARE
HARDWARE
SOURCE
CODE
DEBUGGING
AND
PERFORMANCE
ANALYSIS
DIRECTLY
INTO
MICROSOFT
VISUAL
STUDIO
THE
MOST
WIDELY
USED
INTEGRATED
APPLICATION
DEVELOPMENT
ENVIRONMENT
UNDER
MICROSOFT
WINDOWS
NEXUS
ALLOWS
VISUAL
STUDIO
DEVELOPERS
TO
WRITE
AND
DEBUG
GPU
SOURCE
CODE
USING
EXACTLY
THE
SAME
TOOLS
AND
INTERFACES
THAT
ARE
USED
WHEN
WRITING
AND
DEBUGGING
CPU
CODE
INCLUDING
SOURCE
AND
DATA
BREAKPOINTS
AND
MEMORY
INSPECTION
FURTHERMORE
NEXUS
EXTENDS
VISUAL
STUDIO
FUNCTIONALITY
BY
OFFERING
TOOLS
TO
MANAGE
MASSIVE
PARALLELISM
SUCH
AS
THE
ABILITY
TO
FOCUS
AND
DEBUG
ON
A
SINGLE
THREAD
OUT
OF
THE
THOUSANDS
OF
THREADS
RUNNING
PARALLEL
AND
THE
ABILITY
TO
SIMPLY
AND
EFFICIENTLY
VISUALIZE
THE
RESULTS
COMPUTED
BY
ALL
PARALLEL
THREADS
NEXUS
IS
THE
PERFECT
ENVIRONMENT
TO
DEVELOP
CO
PROCESSING
APPLICATIONS
THAT
TAKE
ADVANTAGE
OF
BOTH
THE
CPU
AND
GPU
IT
CAPTURES
PERFORMANCE
EVENTS
AND
INFORMATION
ACROSS
BOTH
PROCESSORS
AND
PRESENTS
THE
INFORMATION
TO
THE
DEVELOPER
ON
A
SINGLE
CORRELATED
TIMELINE
THIS
ALLOWS
DEVELOPERS
TO
SEE
HOW
THEIR
APPLICATION
BEHAVES
AND
PERFORMS
ON
THE
ENTIRE
SYSTEM
RATHER
THAN
THROUGH
A
NARROW
VIEW
THAT
IS
FOCUSED
ON
A
PARTICULAR
SUBSYSTEM
OR
PROCESSOR
NVIDIA
NEXUS
INTEGRATED
DEVELOPMENT
ENVIRONMENT
CONCLUSION
FOR
SIXTEEN
YEARS
NVIDIA
HAS
DEDICATED
ITSELF
TO
BUILDING
THE
WORLD
FASTEST
GRAPHICS
PROCESSORS
WHILE
WAS
A
PIONEERING
ARCHITECTURE
IN
GPU
COMPUTING
AND
A
MAJOR
REFINEMENT
THEIR
DESIGNS
WERE
NEVERTHELESS
DEEPLY
ROOTED
IN
THE
WORLD
OF
GRAPHICS
THE
FERMI
ARCHITECTURE
REPRESENTS
A
NEW
DIRECTION
FOR
NVIDIA
FAR
FROM
BEING
MERELY
THE
SUCCESSOR
TO
FERMI
IS
THE
OUTCOME
OF
A
RADICAL
RETHINKING
OF
THE
ROLE
PURPOSE
AND
CAPABILITY
OF
THE
GPU
RATHER
THAN
TAKING
THE
SIMPLE
ROUTE
OF
ADDING
EXECUTION
UNITS
THE
FERMI
TEAM
HAS
TACKLED
SOME
OF
THE
TOUGHEST
PROBLEMS
OF
GPU
COMPUTING
THE
IMPORTANCE
OF
DATA
LOCALITY
IS
RECOGNIZED
THROUGH
FERMI
TWO
LEVEL
CACHE
HIERARCHY
AND
ITS
COMBINED
LOAD
STORE
MEMORY
PATH
DOUBLE
PRECISION
PERFORMANCE
IS
ELEVATED
TO
SUPERCOMPUTING
LEVELS
WHILE
ATOMIC
OPERATIONS
EXECUTE
UP
TO
TWENTY
TIMES
FASTER
LASTLY
FERMI
COMPREHENSIVE
ECC
SUPPORT
STRONGLY
DEMONSTRATES
OUR
COMMITMENT
TO
THE
HIGH
PERFORMANCE
COMPUTING
MARKET
ON
THE
SOFTWARE
SIDE
THE
ARCHITECTURE
BRINGS
FORWARD
SUPPORT
FOR
C
THE
WORLD
MOST
UBIQUITOUS
OBJECT
ORIENTATED
PROGRAMMING
LANGUAGE
AND
NEXUS
THE
WORLD
FIRST
INTEGRATED
DEVELOPMENT
ENVIRONMENT
DESIGNED
FOR
MASSIVELY
PARALLEL
GPU
COMPUTING
APPLICATIONS
WITH
ITS
COMBINATION
OF
GROUND
BREAKING
PERFORMANCE
FUNCTIONALITY
AND
PROGRAMMABILITY
THE
FERMI
ARCHITECTURE
REPRESENTS
THE
NEXT
REVOLUTION
IN
GPU
COMPUTING
NOTICE
ALL
INFORMATION
PROVIDED
IN
THIS
WHITE
PAPER
INCLUDING
COMMENTARY
OPINION
NVIDIA
DESIGN
SPECIFICATIONS
REFERENCE
BOARDS
FILES
DRAWINGS
DIAGNOSTICS
LISTS
AND
OTHER
DOCUMENTS
TOGETHER
AND
SEPARATELY
MATERIALS
ARE
BEING
PROVIDED
AS
IS
NVIDIA
MAKES
NO
WARRANTIES
EXPRESSED
IMPLIED
STATUTORY
OR
OTHERWISE
WITH
RESPECT
TO
MATERIALS
AND
EXPRESSLY
DISCLAIMS
ALL
IMPLIED
WARRANTIES
OF
NONINFRINGEMENT
MERCHANTABILITY
AND
FITNESS
FOR
A
PARTICULAR
PURPOSE
INFORMATION
FURNISHED
IS
BELIEVED
TO
BE
ACCURATE
AND
RELIABLE
HOWEVER
NVIDIA
CORPORATION
ASSUMES
NO
RESPONSIBILITY
FOR
THE
CONSEQUENCES
OF
USE
OF
SUCH
INFORMATION
OR
FOR
ANY
INFRINGEMENT
OF
PATENTS
OR
OTHER
RIGHTS
OF
THIRD
PARTIES
THAT
MAY
RESULT
FROM
ITS
USE
NO
LICENSE
IS
GRANTED
BY
IMPLICATION
OR
OTHERWISE
UNDER
ANY
PATENT
OR
PATENT
RIGHTS
OF
NVIDIA
CORPORATION
SPECIFICATIONS
MENTIONED
IN
THIS
PUBLICATION
ARE
SUBJECT
TO
CHANGE
WITHOUT
NOTICE
THIS
PUBLICATION
SUPERSEDES
AND
REPLACES
ALL
INFORMATION
PREVIOUSLY
SUPPLIED
NVIDIA
CORPORATION
PRODUCTS
ARE
NOT
AUTHORIZED
FOR
USE
AS
CRITICAL
COMPONENTS
IN
LIFE
SUPPORT
DEVICES
OR
SYSTEMS
WITHOUT
EXPRESS
WRITTEN
APPROVAL
OF
NVIDIA
CORPORATION
TRADEMARKS
NVIDIA
THE
NVIDIA
LOGO
CUDA
FERMI
AND
GEFORCE
ARE
TRADEMARKS
OR
REGISTERED
TRADEMARKS
OF
NVIDIA
CORPORATION
IN
THE
UNITED
STATES
AND
OTHER
COUNTRIES
OTHER
COMPANY
AND
PRODUCT
NAMES
MAY
BE
TRADEMARKS
OF
THE
RESPECTIVE
COMPANIES
WITH
WHICH
THEY
ARE
ASSOCIATED
COPYRIGHT
NVIDIA
CORPORATION
ALL
RIGHTS
RESERVED
TWO
OR
MORE
COMMON
TAKEN
TARGETS
AND
AT
LEAST
ONE
OF
THOSE
TARGETS
IS
CORRELATED
WITH
BRANCH
HISTORY
LEADING
UP
TO
THE
BRANCH
THEN
CONVERT
THE
INDIRECT
BRANCH
TO
A
TREE
WHERE
ONE
OR
MORE
INDIRECT
BRANCHES
ARE
PRECEDED
BY
CONDITIONAL
BRANCHES
TO
THOSE
TARGETS
APPLY
THIS
PEELING
PROCEDURE
TO
THE
COMMON
TARGET
OF
AN
INDIRECT
BRANCH
THAT
CORRELATES
TO
BRANCH
HISTORY
THE
PURPOSE
OF
THIS
RULE
IS
TO
REDUCE
THE
TOTAL
NUMBER
OF
MISPREDICTIONS
BY
ENHANCING
THE
PREDICTABILITY
OF
BRANCHES
EVEN
AT
THE
EXPENSE
OF
ADDING
MORE
BRANCHES
THE
ADDED
BRANCHES
MUST
BE
PREDICTABLE
FOR
THIS
TO
BE
WORTHWHILE
ONE
REASON
FOR
SUCH
PREDICTABILITY
IS
A
STRONG
CORRELATION
WITH
PRECEDING
BRANCH
HISTORY
THAT
IS
THE
DIREC
TIONS
TAKEN
ON
PRECEDING
BRANCHES
ARE
A
GOOD
INDICATOR
OF
THE
DIRECTION
OF
THE
BRANCH
UNDER
CONSIDERATION
EXAMPLE
SHOWS
A
SIMPLE
EXAMPLE
OF
THE
CORRELATION
BETWEEN
A
TARGET
OF
A
PRECEDING
CONDITIONAL
BRANCH
AND
A
TARGET
OF
AN
INDIRECT
BRANCH
EXAMPLE
INDIRECT
BRANCH
WITH
TWO
FAVORED
TARGETS
FUNCTION
INT
N
RAND
RANDOM
INTEGER
TO
IF
N
N
WILL
BE
HALF
THE
TIMES
N
UPDATES
BRANCH
HISTORY
TO
PREDICT
TAKEN
INDIRECT
BRANCHES
WITH
MULTIPLE
TAKEN
TARGETS
MAY
HAVE
LOWER
PREDICTION
RATES
SWITCH
N
CASE
BREAK
COMMON
TARGET
CORRELATED
WITH
BRANCH
HISTORY
THAT
IS
FORWARD
TAKEN
CASE
BREAK
UNCOMMON
CASE
BREAK
UNCOMMON
DEFAULT
COMMON
TARGET
CORRELATION
CAN
BE
DIFFICULT
TO
DETERMINE
ANALYTICALLY
FOR
A
COMPILER
AND
FOR
AN
ASSEMBLY
LANGUAGE
PROGRAMMER
IT
MAY
BE
FRUITFUL
TO
EVALUATE
PERFORMANCE
WITH
AND
WITHOUT
PEELING
TO
GET
THE
BEST
PERFORMANCE
FROM
A
CODING
EFFORT
AN
EXAMPLE
OF
PEELING
OUT
THE
MOST
FAVORED
TARGET
OF
AN
INDIRECT
BRANCH
WITH
CORRE
LATED
BRANCH
HISTORY
IS
SHOWN
IN
EXAMPLE
EXAMPLE
A
PEELING
TECHNIQUE
TO
REDUCE
INDIRECT
BRANCH
MISPREDICTION
FUNCTION
INT
N
RAND
RANDOM
INTEGER
TO
IF
N
THEN
N
N
WILL
BE
HALF
THE
TIMES
IF
N
THEN
PEEL
OUT
THE
MOST
COMMON
TARGET
WITH
CORRELATED
BRANCH
HISTORY
SWITCH
N
CASE
BREAK
UNCOMMON
CASE
BREAK
UNCOMMON
DEFAULT
MAKE
THE
FAVORED
TARGET
IN
THE
FALL
THROUGH
PATH
LOOP
UNROLLING
BENEFITS
OF
UNROLLING
LOOPS
ARE
UNROLLING
AMORTIZES
THE
BRANCH
OVERHEAD
SINCE
IT
ELIMINATES
BRANCHES
AND
SOME
OF
THE
CODE
TO
MANAGE
INDUCTION
VARIABLES
UNROLLING
ALLOWS
ONE
TO
AGGRESSIVELY
SCHEDULE
OR
PIPELINE
THE
LOOP
TO
HIDE
LATENCIES
THIS
IS
USEFUL
IF
YOU
HAVE
ENOUGH
FREE
REGISTERS
TO
KEEP
VARIABLES
LIVE
AS
YOU
STRETCH
OUT
THE
DEPENDENCE
CHAIN
TO
EXPOSE
THE
CRITICAL
PATH
UNROLLING
EXPOSES
THE
CODE
TO
VARIOUS
OTHER
OPTIMIZATIONS
SUCH
AS
REMOVAL
OF
REDUNDANT
LOADS
COMMON
SUBEXPRESSION
ELIMINATION
AND
SO
ON
THE
PENTIUM
PROCESSOR
CAN
CORRECTLY
PREDICT
THE
EXIT
BRANCH
FOR
AN
INNER
LOOP
THAT
HAS
OR
FEWER
ITERATIONS
IF
THAT
NUMBER
OF
ITERATIONS
IS
PREDICTABLE
AND
THERE
ARE
NO
CONDITIONAL
BRANCHES
IN
THE
LOOP
SO
IF
THE
LOOP
BODY
SIZE
IS
NOT
EXCESSIVE
AND
THE
PROBABLE
NUMBER
OF
ITERATIONS
IS
KNOWN
UNROLL
INNER
LOOPS
UNTIL
THEY
HAVE
A
MAXIMUM
OF
ITERATIONS
WITH
THE
PENTIUM
M
PROCESSOR
DO
NOT
UNROLL
LOOPS
HAVING
MORE
THAN
ITERATIONS
THE
POTENTIAL
COSTS
OF
UNROLLING
LOOPS
ARE
EXCESSIVE
UNROLLING
OR
UNROLLING
OF
VERY
LARGE
LOOPS
CAN
LEAD
TO
INCREASED
CODE
SIZE
THIS
CAN
BE
HARMFUL
IF
THE
UNROLLED
LOOP
NO
LONGER
FITS
IN
THE
TRACE
CACHE
TC
UNROLLING
LOOPS
WHOSE
BODIES
CONTAIN
BRANCHES
INCREASES
DEMAND
ON
BTB
CAPACITY
IF
THE
NUMBER
OF
ITERATIONS
OF
THE
UNROLLED
LOOP
IS
OR
FEWER
THE
BRANCH
PREDICTOR
SHOULD
BE
ABLE
TO
CORRECTLY
PREDICT
BRANCHES
IN
THE
LOOP
BODY
THAT
ALTERNATE
DIRECTION
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
UNROLL
SMALL
LOOPS
UNTIL
THE
OVERHEAD
OF
THE
BRANCH
AND
INDUCTION
VARIABLE
ACCOUNTS
GENERALLY
FOR
LESS
THAN
OF
THE
EXECUTION
TIME
OF
THE
LOOP
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
AVOID
UNROLLING
LOOPS
EXCESSIVELY
THIS
MAY
THRASH
THE
TRACE
CACHE
OR
INSTRUCTION
CACHE
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
M
GENERALITY
UNROLL
LOOPS
THAT
ARE
FREQUENTLY
EXECUTED
AND
HAVE
A
PREDICTABLE
NUMBER
OF
ITERATIONS
TO
REDUCE
THE
NUMBER
OF
ITERATIONS
TO
OR
FEWER
DO
THIS
UNLESS
IT
INCREASES
CODE
SIZE
SO
THAT
THE
WORKING
SET
NO
LONGER
FITS
IN
THE
TRACE
OR
INSTRUCTION
CACHE
IF
THE
LOOP
BODY
CONTAINS
MORE
THAN
ONE
CONDITIONAL
BRANCH
THEN
UNROLL
SO
THAT
THE
NUMBER
OF
ITERATIONS
IS
CONDITIONAL
BRANCHES
EXAMPLE
SHOWS
HOW
UNROLLING
ENABLES
OTHER
OPTIMIZATIONS
EXAMPLE
LOOP
UNROLLING
IN
THIS
EXAMPLE
THE
LOOP
THAT
EXECUTES
TIMES
ASSIGNS
X
TO
EVERY
EVEN
NUMBERED
ELEMENT
AND
Y
TO
EVERY
ODD
NUMBERED
ELEMENT
BY
UNROLLING
THE
LOOP
YOU
CAN
MAKE
ASSIGNMENTS
MORE
EFFICIENTLY
REMOVING
ONE
BRANCH
IN
THE
LOOP
BODY
COMPILER
SUPPORT
FOR
BRANCH
PREDICTION
COMPILERS
GENERATE
CODE
THAT
IMPROVES
THE
EFFICIENCY
OF
BRANCH
PREDICTION
IN
THE
PENTIUM
PENTIUM
M
INTEL
CORE
DUO
PROCESSORS
AND
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
THE
INTEL
C
COMPILER
ACCOMPLISHES
THIS
BY
KEEPING
CODE
AND
DATA
ON
SEPARATE
PAGES
USING
CONDITIONAL
MOVE
INSTRUCTIONS
TO
ELIMINATE
BRANCHES
GENERATING
CODE
CONSISTENT
WITH
THE
STATIC
BRANCH
PREDICTION
ALGORITHM
INLINING
WHERE
APPROPRIATE
UNROLLING
IF
THE
NUMBER
OF
ITERATIONS
IS
PREDICTABLE
WITH
PROFILE
GUIDED
OPTIMIZATION
THE
COMPILER
CAN
LAY
OUT
BASIC
BLOCKS
TO
ELIMINATE
BRANCHES
FOR
THE
MOST
FREQUENTLY
EXECUTED
PATHS
OF
A
FUNCTION
OR
AT
LEAST
IMPROVE
THEIR
PREDICTABILITY
BRANCH
PREDICTION
NEED
NOT
BE
A
CONCERN
AT
THE
SOURCE
LEVEL
FOR
MORE
INFORMATION
SEE
INTEL
C
COMPILER
DOCUMENTATION
FETCH
AND
DECODE
OPTIMIZATION
INTEL
CORE
MICROARCHITECTURE
PROVIDES
SEVERAL
MECHANISMS
TO
INCREASE
FRONT
END
THROUGHPUT
TECHNIQUES
TO
TAKE
ADVANTAGE
OF
SOME
OF
THESE
FEATURES
ARE
DISCUSSED
BELOW
OPTIMIZING
FOR
MICRO
FUSION
AN
INSTRUCTION
THAT
OPERATES
ON
A
REGISTER
AND
A
MEMORY
OPERAND
DECODES
INTO
MORE
OPS
THAN
ITS
CORRESPONDING
REGISTER
REGISTER
VERSION
REPLACING
THE
EQUIVALENT
WORK
OF
THE
FORMER
INSTRUCTION
USING
THE
REGISTER
REGISTER
VERSION
USUALLY
REQUIRE
A
SEQUENCE
OF
TWO
INSTRUCTIONS
THE
LATTER
SEQUENCE
IS
LIKELY
TO
RESULT
IN
REDUCED
FETCH
BANDWIDTH
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
M
GENERALITY
FOR
IMPROVING
FETCH
DECODE
THROUGHPUT
GIVE
PREFERENCE
TO
MEMORY
FLAVOR
OF
AN
INSTRUCTION
OVER
THE
REGISTER
ONLY
FLAVOR
OF
THE
SAME
INSTRUCTION
IF
SUCH
INSTRUCTION
CAN
BENEFIT
FROM
MICRO
FUSION
THE
FOLLOWING
EXAMPLES
ARE
SOME
OF
THE
TYPES
OF
MICRO
FUSIONS
THAT
CAN
BE
HANDLED
BY
ALL
DECODERS
ALL
STORES
TO
MEMORY
INCLUDING
STORE
IMMEDIATE
STORES
EXECUTE
INTERNALLY
AS
TWO
SEPARATE
OPS
STORE
ADDRESS
AND
STORE
DATA
ALL
READ
MODIFY
LOAD
OP
INSTRUCTIONS
BETWEEN
REGISTER
AND
MEMORY
FOR
EXAMPLE
ADDPS
OWORD
PTR
RSP
FADD
DOUBLE
PTR
RDI
RSI
XOR
RAX
QWORD
PTR
RBP
ALL
INSTRUCTIONS
OF
THE
FORM
LOAD
AND
JUMP
FOR
EXAMPLE
JMP
RDI
RET
CMP
AND
TEST
WITH
IMMEDIATE
OPERAND
AND
MEMORY
AN
INTEL
INSTRUCTION
WITH
RIP
RELATIVE
ADDRESSING
IS
NOT
MICRO
FUSED
IN
THE
FOLLOWING
CASES
WHEN
AN
ADDITIONAL
IMMEDIATE
IS
NEEDED
FOR
EXAMPLE
CMP
RIP
MOV
RIP
WHEN
AN
RIP
IS
NEEDED
FOR
CONTROL
FLOW
PURPOSES
FOR
EXAMPLE
JMP
RIP
IN
THESE
CASES
INTEL
CORE
MICROARCHITECTURE
PROVIDES
A
OP
FLOW
FROM
DECODER
RESULTING
IN
A
SLIGHT
LOSS
OF
DECODE
BANDWIDTH
SINCE
OP
FLOW
MUST
BE
STEERED
TO
DECODER
FROM
THE
DECODER
WITH
WHICH
IT
WAS
ALIGNED
RIP
ADDRESSING
MAY
BE
COMMON
IN
ACCESSING
GLOBAL
DATA
SINCE
IT
WILL
NOT
BENEFIT
FROM
MICRO
FUSION
COMPILER
MAY
CONSIDER
ACCESSING
GLOBAL
DATA
WITH
OTHER
MEANS
OF
MEMORY
ADDRESSING
OPTIMIZING
FOR
MACRO
FUSION
MACRO
FUSION
MERGES
TWO
INSTRUCTIONS
TO
A
SINGLE
OP
INTEL
CORE
MICROARCHITECTURE
PERFORMS
THIS
HARDWARE
OPTIMIZATION
UNDER
LIMITED
CIRCUMSTANCES
THE
FIRST
INSTRUCTION
OF
THE
MACRO
FUSED
PAIR
MUST
BE
A
CMP
OR
TEST
INSTRUCTION
THIS
INSTRUCTION
CAN
BE
REG
REG
REG
IMM
OR
A
MICRO
FUSED
REG
MEM
COMPARISON
THE
SECOND
INSTRUCTION
ADJACENT
IN
THE
INSTRUCTION
STREAM
SHOULD
BE
A
CONDITIONAL
BRANCH
SINCE
THESE
PAIRS
ARE
COMMON
INGREDIENT
IN
BASIC
ITERATIVE
PROGRAMMING
SEQUENCES
MACRO
FUSION
IMPROVES
PERFORMANCE
EVEN
ON
UN
RECOMPILED
BINARIES
ALL
OF
THE
DECODERS
CAN
DECODE
ONE
MACRO
FUSED
PAIR
PER
CYCLE
WITH
UP
TO
THREE
OTHER
INSTRUC
TIONS
RESULTING
IN
A
PEAK
DECODE
BANDWIDTH
OF
INSTRUCTIONS
PER
CYCLE
EACH
MACRO
FUSED
INSTRUCTION
EXECUTES
WITH
A
SINGLE
DISPATCH
THIS
PROCESS
REDUCES
LATENCY
WHICH
IN
THIS
CASE
SHOWS
UP
AS
A
CYCLE
REMOVED
FROM
BRANCH
MISPREDICT
PENALTY
SOFTWARE
ALSO
GAIN
ALL
OTHER
FUSION
BENEFITS
INCREASED
RENAME
AND
RETIRE
BANDWIDTH
MORE
STORAGE
FOR
INSTRUCTIONS
IN
FLIGHT
AND
POWER
SAVINGS
FROM
REPRE
SENTING
MORE
WORK
IN
FEWER
BITS
THE
FOLLOWING
LIST
DETAILS
WHEN
YOU
CAN
USE
MACRO
FUSION
CMP
OR
TEST
CAN
BE
FUSED
WHEN
COMPARING
REG
REG
FOR
EXAMPLE
CMP
EAX
ECX
JZ
LABEL
REG
IMM
FOR
EXAMPLE
CMP
EAX
JZ
LABEL
REG
MEM
FOR
EXAMPLE
CMP
EAX
ECX
JZ
LABEL
MEM
REG
FOR
EXAMPLE
CMP
EAX
ECX
JZ
LABEL
TEST
CAN
FUSED
WITH
ALL
CONDITIONAL
JUMPS
CMP
CAN
BE
FUSED
WITH
ONLY
THE
FOLLOWING
CONDITIONAL
JUMPS
IN
INTEL
CORE
MICROAR
CHITECTURE
THESE
CONDITIONAL
JUMPS
CHECK
CARRY
FLAG
CF
OR
ZERO
FLAG
ZF
JUMP
THE
LIST
OF
MACRO
FUSION
CAPABLE
CONDITIONAL
JUMPS
ARE
JA
OR
JNBE
JAE
OR
JNB
OR
JNC
JE
OR
JZ
JNA
OR
JBE
JNAE
OR
JC
OR
JB
JNE
OR
JNZ
CMP
AND
TEST
CAN
NOT
BE
FUSED
WHEN
COMPARING
MEM
IMM
E
G
CMP
EAX
JZ
LABEL
MACRO
FUSION
IS
NOT
SUPPORTED
IN
BIT
MODE
FOR
INTEL
CORE
MICROARCHITECTURE
INTEL
MICROARCHITECTURE
NEHALEM
SUPPORTS
THE
FOLLOWING
ENHANCEMENTS
IN
MACROFUSION
CMP
CAN
BE
FUSED
WITH
THE
FOLLOWING
CONDITIONAL
JUMPS
THAT
WAS
NOT
SUPPORTED
IN
INTEL
CORE
MICROARCHITECTURE
JL
OR
JNGE
JGE
OR
JNL
JLE
OR
JNG
JG
OR
JNLE
MACRO
FUSION
IS
SUPPORT
IN
BIT
MODE
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
ML
GENERALITY
EMPLOY
MACRO
FUSION
WHERE
POSSIBLE
USING
INSTRUCTION
PAIRS
THAT
SUPPORT
MACRO
FUSION
PREFER
TEST
OVER
CMP
IF
POSSIBLE
USE
UNSIGNED
VARIABLES
AND
UNSIGNED
JUMPS
WHEN
POSSIBLE
TRY
TO
LOGICALLY
VERIFY
THAT
A
VARIABLE
IS
NON
NEGATIVE
AT
THE
TIME
OF
COMPARISON
AVOID
CMP
OR
TEST
OF
MEM
IMM
FLAVOR
WHEN
POSSIBLE
HOWEVER
DO
NOT
ADD
OTHER
INSTRUCTIONS
TO
AVOID
USING
THE
MEM
IMM
FLAVOR
EXAMPLE
MACRO
FUSION
UNSIGNED
ITERATION
COUNT
WITHOUT
MACRO
FUSION
WITH
MACRO
FUSION
C
CODE
FOR
I
I
I
A
FOR
UNSIGNED
I
I
I
A
DISASSEMBLY
FOR
INT
I
I
I
MOV
DWORD
PTR
I
JMP
FIRST
LOOP
MOV
EAX
DWORD
PTR
I
ADD
EAX
MOV
DWORD
PTR
I
EAX
FIRST
CMP
DWORD
PTR
I
JGE
END
A
MOV
EAX
DWORD
PTR
A
ADDQQ
EAX
MOV
DWORD
PTR
A
EAX
JMP
LOOP
END
FOR
UNSIGNED
INT
I
I
I
MOV
DWORD
PTR
I
JMP
FIRST
LOOP
MOV
EAX
DWORD
PTR
I
ADD
EAX
MOV
DWORD
PTR
I
EAX
FIRST
CMP
EAX
JAE
END
A
MOV
EAX
DWORD
PTR
A
ADD
EAX
MOV
DWORD
PTR
A
EAX
JMP
LOOP
END
NOTES
SIGNED
ITERATION
COUNT
INHIBITS
MACRO
FUSION
UNSIGNED
ITERATION
COUNT
IS
COMPATIBLE
WITH
MACRO
FUSION
CMP
MEM
IMM
JGE
INHIBIT
MACRO
FUSION
CMP
REG
IMM
JAE
PERMITS
MACRO
FUSION
EXAMPLE
MACRO
FUSION
IF
STATEMENT
WITHOUT
MACRO
FUSION
WITH
MACRO
FUSION
C
CODE
A
IF
A
A
ELSE
A
UNSIGNED
A
IF
A
A
ELSE
A
DISASSEMBLY
INT
A
MOV
DWORD
PTR
A
IF
A
CMP
DWORD
PTR
A
JGE
DEC
A
MOV
EAX
DWORD
PTR
A
ADD
EAX
MOV
DWORD
PTR
A
EAX
ELSE
JMP
END
A
DEC
MOV
EAX
DWORD
PTR
A
SUB
EAX
MOV
DWORD
PTR
A
EAX
END
UNSIGNED
INT
A
MOV
DWORD
PTR
A
IF
A
MOV
EAX
DWORD
PTR
A
CMP
EAX
JAE
DEC
A
ADD
EAX
MOV
DWORD
PTR
A
EAX
ELSE
JMP
END
A
DEC
SUB
EAX
MOV
DWORD
PTR
A
EAX
END
NOTES
SIGNED
ITERATION
COUNT
INHIBITS
MACRO
FUSION
UNSIGNED
ITERATION
COUNT
IS
COMPATIBLE
WITH
MACRO
FUSION
CMP
MEM
IMM
JGE
INHIBIT
MACRO
FUSION
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
ML
GENERALITY
SOFTWARE
CAN
ENABLE
MACRO
FUSION
WHEN
IT
CAN
BE
LOGICALLY
DETERMINED
THAT
A
VARIABLE
IS
NON
NEGATIVE
AT
THE
TIME
OF
COMPARISON
USE
TEST
APPROPRIATELY
TO
ENABLE
MACRO
FUSION
WHEN
COMPARING
A
VARIABLE
WITH
EXAMPLE
MACRO
FUSION
SIGNED
VARIABLE
WITHOUT
MACRO
FUSION
WITH
MACRO
FUSION
TEST
ECX
ECX
TEST
ECX
ECX
JLE
OUTSIDETHEIF
JLE
OUTSIDETHEIF
CMP
ECX
CMP
ECX
JGE
OUTSIDETHEIF
JAE
OUTSIDETHEIF
IF
BLOCK
CODE
IF
BLOCK
CODE
OUTSIDETHEIF
OUTSIDETHEIF
FOR
EITHER
SIGNED
OR
UNSIGNED
VARIABLE
A
CMP
A
AND
TEST
A
A
PRODUCE
THE
SAME
RESULT
AS
FAR
AS
THE
FLAGS
ARE
CONCERNED
SINCE
TEST
CAN
BE
MACRO
FUSED
MORE
OFTEN
SOFTWARE
CAN
USE
TEST
A
A
TO
REPLACE
CMP
A
FOR
THE
PURPOSE
OF
ENABLING
MACRO
FUSION
EXAMPLE
MACRO
FUSION
SIGNED
COMPARISON
C
CODE
WITHOUT
MACRO
FUSION
WITH
MACRO
FUSION
IF
A
CMP
A
TEST
A
A
JNE
LBL
JNE
LBL
LBL
LBL
IF
A
CMP
A
TEST
A
A
JL
LBL
JL
LBL
LBL
LBL
LENGTH
CHANGING
PREFIXES
LCP
THE
LENGTH
OF
AN
INSTRUCTION
CAN
BE
UP
TO
BYTES
IN
LENGTH
SOME
PREFIXES
CAN
DYNAMICALLY
CHANGE
THE
LENGTH
OF
AN
INSTRUCTION
THAT
THE
DECODER
MUST
RECOGNIZE
TYPICALLY
THE
PRE
DECODE
UNIT
WILL
ESTIMATE
THE
LENGTH
OF
AN
INSTRUCTION
IN
THE
BYTE
STREAM
ASSUMING
THE
ABSENCE
OF
LCP
WHEN
THE
PREDECODER
ENCOUNTERS
AN
LCP
IN
THE
FETCH
LINE
IT
MUST
USE
A
SLOWER
LENGTH
DECODING
ALGORITHM
WITH
THE
SLOWER
LENGTH
DECODING
ALGORITHM
THE
PREDECODER
DECODES
THE
FETCH
IN
CYCLES
INSTEAD
OF
THE
USUAL
CYCLE
NORMAL
QUEUING
THROUGHOUT
OF
THE
MACHINE
PIPELINE
GENERALLY
CANNOT
HIDE
LCP
PENALTIES
THE
PREFIXES
THAT
CAN
DYNAMICALLY
CHANGE
THE
LENGTH
OF
A
INSTRUCTION
INCLUDE
OPERAND
SIZE
PREFIX
ADDRESS
SIZE
PREFIX
THE
INSTRUCTION
MOV
DX
IS
SUBJECT
TO
LCP
STALLS
IN
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
AND
IN
INTEL
CORE
DUO
AND
INTEL
CORE
SOLO
PROCESSORS
INSTRUCTIONS
THAT
CONTAIN
AS
PART
OF
THEIR
FIXED
ENCODING
BUT
DO
NOT
REQUIRE
LCP
TO
CHANGE
THE
IMMEDIATE
SIZE
ARE
NOT
SUBJECT
TO
LCP
STALLS
THE
REX
PREFIX
IN
BIT
MODE
CAN
CHANGE
THE
SIZE
OF
TWO
CLASSES
OF
INSTRUCTION
BUT
DOES
NOT
CAUSE
AN
LCP
PENALTY
IF
THE
LCP
STALL
HAPPENS
IN
A
TIGHT
LOOP
IT
CAN
CAUSE
SIGNIFICANT
PERFORMANCE
DEGRADA
TION
WHEN
DECODING
IS
NOT
A
BOTTLENECK
AS
IN
FLOATING
POINT
HEAVY
CODE
ISOLATED
LCP
STALLS
USUALLY
DO
NOT
CAUSE
PERFORMANCE
DEGRADATION
ASSEMBLY
COMPILER
CODING
RULE
MH
IMPACT
MH
GENERALITY
FAVOR
GENERATING
CODE
USING
OR
VALUES
INSTEAD
OF
VALUES
IF
IS
NEEDED
LOAD
EQUIVALENT
INTO
A
REGISTER
AND
USE
THE
WORD
VALUE
IN
THE
REGISTER
INSTEAD
DOUBLE
LCP
STALLS
INSTRUCTIONS
THAT
ARE
SUBJECT
TO
LCP
STALLS
AND
CROSS
A
BYTE
FETCH
LINE
BOUNDARY
CAN
CAUSE
THE
LCP
STALL
TO
TRIGGER
TWICE
THE
FOLLOWING
ALIGNMENT
SITUATIONS
CAN
CAUSE
LCP
STALLS
TO
TRIGGER
TWICE
AN
INSTRUCTION
IS
ENCODED
WITH
A
MODR
M
AND
SIB
BYTE
AND
THE
FETCH
LINE
BOUNDARY
CROSSING
IS
BETWEEN
THE
MODR
M
AND
THE
SIB
BYTES
AN
INSTRUCTION
STARTS
AT
OFFSET
OF
A
FETCH
LINE
REFERENCES
A
MEMORY
LOCATION
USING
REGISTER
AND
IMMEDIATE
BYTE
OFFSET
ADDRESSING
MODE
THE
FIRST
STALL
IS
FOR
THE
FETCH
LINE
AND
THE
STALL
IS
FOR
THE
FETCH
LINE
A
DOUBLE
LCP
STALL
CAUSES
A
DECODE
PENALTY
OF
CYCLES
THE
FOLLOWING
EXAMPLES
CAUSE
LCP
STALL
ONCE
REGARDLESS
OF
THEIR
FETCH
LINE
LOCATION
OF
THE
FIRST
BYTE
OF
THE
INSTRUCTION
ADD
DX
ADD
WORD
PTR
EDX
ADD
WORD
PTR
EDX
ADD
WORD
PTR
THE
FOLLOWING
INSTRUCTIONS
CAUSE
A
DOUBLE
LCP
STALL
WHEN
STARTING
AT
OFFSET
OF
A
FETCH
LINE
ADD
WORD
PTR
EDX
ESI
ADD
WORD
PTR
EDX
ADD
WORD
PTR
EDX
ESI
TO
AVOID
DOUBLE
LCP
STALLS
DO
NOT
USE
INSTRUCTIONS
SUBJECT
TO
LCP
STALLS
THAT
USE
SIB
BYTE
ENCODING
OR
ADDRESSING
MODE
WITH
BYTE
DISPLACEMENT
FALSE
LCP
STALLS
FALSE
LCP
STALLS
HAVE
THE
SAME
CHARACTERISTICS
AS
LCP
STALLS
BUT
OCCUR
ON
INSTRUCTIONS
THAT
DO
NOT
HAVE
ANY
VALUE
FALSE
LCP
STALLS
OCCUR
WHEN
A
INSTRUCTIONS
WITH
LCP
THAT
ARE
ENCODED
USING
THE
OPCODES
AND
B
ARE
LOCATED
AT
OFFSET
OF
A
FETCH
LINE
THESE
INSTRUCTIONS
ARE
NOT
NEG
DIV
IDIV
MUL
AND
IMUL
FALSE
LCP
EXPERIENCES
DELAY
BECAUSE
THE
INSTRUCTION
LENGTH
DECODER
CAN
NOT
DETERMINE
THE
LENGTH
OF
THE
INSTRUCTION
BEFORE
THE
NEXT
FETCH
LINE
WHICH
HOLDS
THE
EXACT
OPCODE
OF
THE
INSTRUCTION
IN
ITS
MODR
M
BYTE
THE
FOLLOWING
TECHNIQUES
CAN
HELP
AVOID
FALSE
LCP
STALLS
UPCAST
ALL
SHORT
OPERATIONS
FROM
THE
GROUP
OF
INSTRUCTIONS
TO
LONG
USING
THE
FULL
BIT
VERSION
ENSURE
THAT
THE
OPCODE
NEVER
STARTS
AT
OFFSET
OF
A
FETCH
LINE
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
ML
GENERALITY
ENSURE
INSTRUCTIONS
USING
OPCODE
BYTE
DOES
NOT
START
AT
OFFSET
OF
A
FETCH
LINE
AND
AVOID
USING
THESE
INSTRUCTION
TO
OPERATE
ON
BIT
DATA
UPCAST
SHORT
DATA
TO
BITS
EXAMPLE
AVOIDING
FALSE
LCP
DELAYS
WITH
GROUP
INSTRUCTIONS
A
SEQUENCE
CAUSING
DELAY
IN
THE
DECODER
ALTERNATE
SEQUENCE
TO
AVOID
DELAY
NEG
WORD
PTR
A
MOVSX
EAX
WORD
PTR
A
NEG
EAX
MOV
WORD
PTR
A
AX
OPTIMIZING
THE
LOOP
STREAM
DETECTOR
LSD
LOOPS
THAT
FIT
THE
FOLLOWING
CRITERIA
ARE
DETECTED
BY
THE
LSD
AND
REPLAYED
FROM
THE
INSTRUCTION
QUEUE
TO
FEED
THE
DECODER
IN
INTEL
CORE
MICROARCHITECTURE
MUST
BE
LESS
THAN
OR
EQUAL
TO
FOUR
BYTE
FETCHES
MUST
BE
LESS
THAN
OR
EQUAL
TO
INSTRUCTIONS
CAN
CONTAIN
NO
MORE
THAN
FOUR
TAKEN
BRANCHES
AND
NONE
OF
THEM
CAN
BE
A
RET
SHOULD
USUALLY
HAVE
MORE
THAN
ITERATIONS
LOOP
STREAM
DETECTOR
IN
INTEL
MICROARCHITECTURE
NEHALEM
IS
IMPROVED
BY
CACHING
DECODED
MICRO
OPERATIONS
IN
THE
INSTRUCTION
DECODER
QUEUE
IDQ
SEE
SECTION
TO
FEED
THE
RENAME
ALLOC
STAGE
THE
SIZE
OF
THE
LSD
IS
INCREASED
TO
MICRO
OPS
MANY
CALCULATION
INTENSIVE
LOOPS
SEARCHES
AND
SOFTWARE
STRING
MOVES
MATCH
THESE
CHARACTERISTICS
THESE
LOOPS
EXCEED
THE
BPU
PREDICTION
CAPACITY
AND
ALWAYS
TERMI
NATE
IN
A
BRANCH
MISPREDICTION
ASSEMBLY
COMPILER
CODING
RULE
MH
IMPACT
MH
GENERALITY
BREAK
UP
A
LOOP
LONG
SEQUENCE
OF
INSTRUCTIONS
INTO
LOOPS
OF
SHORTER
INSTRUCTION
BLOCKS
OF
NO
MORE
THAN
THE
SIZE
OF
LSD
ASSEMBLY
COMPILER
CODING
RULE
MH
IMPACT
M
GENERALITY
AVOID
UNROLLING
LOOPS
CONTAINING
LCP
STALLS
IF
THE
UNROLLED
BLOCK
EXCEEDS
THE
SIZE
OF
LSD
SCHEDULING
RULES
FOR
THE
PENTIUM
PROCESSOR
DECODER
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
HAVE
A
SINGLE
DECODER
THAT
CAN
DECODE
INSTRUCTIONS
AT
THE
MAXIMUM
RATE
OF
ONE
INSTRUCTION
PER
CLOCK
COMPLEX
INSTRUCTIONS
MUST
ENLIST
THE
HELP
OF
THE
MICROCODE
ROM
BECAUSE
OPS
ARE
DELIVERED
FROM
THE
TRACE
CACHE
IN
THE
COMMON
CASES
DECODING
RULES
AND
CODE
ALIGNMENT
ARE
NOT
REQUIRED
SCHEDULING
RULES
FOR
THE
PENTIUM
M
PROCESSOR
DECODER
THE
PENTIUM
M
PROCESSOR
HAS
THREE
DECODERS
BUT
THE
DECODING
RULES
TO
SUPPLY
OPS
AT
HIGH
BANDWIDTH
ARE
LESS
STRINGENT
THAN
THOSE
OF
THE
PENTIUM
III
PROCESSOR
THIS
PROVIDES
AN
OPPORTUNITY
TO
BUILD
A
FRONT
END
TRACKER
IN
THE
COMPILER
AND
TRY
TO
SCHEDULE
INSTRUCTIONS
CORRECTLY
THE
DECODER
LIMITATIONS
ARE
THE
FIRST
DECODER
IS
CAPABLE
OF
DECODING
ONE
MACROINSTRUCTION
MADE
UP
OF
FOUR
OR
FEWER
OPS
IN
EACH
CLOCK
CYCLE
IT
CAN
HANDLE
ANY
NUMBER
OF
BYTES
UP
TO
THE
MAXIMUM
OF
MULTIPLE
PREFIX
INSTRUCTIONS
REQUIRE
ADDITIONAL
CYCLES
THE
TWO
ADDITIONAL
DECODERS
CAN
EACH
DECODE
ONE
MACROINSTRUCTION
PER
CLOCK
CYCLE
ASSUMING
THE
INSTRUCTION
IS
ONE
OP
UP
TO
SEVEN
BYTES
IN
LENGTH
INSTRUCTIONS
COMPOSED
OF
MORE
THAN
FOUR
OPS
TAKE
MULTIPLE
CYCLES
TO
DECODE
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
M
GENERALITY
AVOID
PUTTING
EXPLICIT
REFERENCES
TO
ESP
IN
A
SEQUENCE
OF
STACK
OPERATIONS
POP
PUSH
CALL
RET
OTHER
DECODING
GUIDELINES
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
L
GENERALITY
USE
SIMPLE
INSTRUCTIONS
THAT
ARE
LESS
THAN
EIGHT
BYTES
IN
LENGTH
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
MH
GENERALITY
AVOID
USING
PREFIXES
TO
CHANGE
THE
SIZE
OF
IMMEDIATE
AND
DISPLACEMENT
LONG
INSTRUCTIONS
MORE
THAN
SEVEN
BYTES
LIMIT
THE
NUMBER
OF
DECODED
INSTRUCTIONS
PER
CYCLE
ON
THE
PENTIUM
M
PROCESSOR
EACH
PREFIX
ADDS
ONE
BYTE
TO
THE
LENGTH
OF
INSTRUCTION
POSSIBLY
LIMITING
THE
DECODER
THROUGHPUT
IN
ADDITION
MULTIPLE
PREFIXES
CAN
ONLY
BE
DECODED
BY
THE
FIRST
DECODER
THESE
PREFIXES
ALSO
INCUR
A
DELAY
WHEN
DECODED
IF
MULTIPLE
PREFIXES
OR
A
PREFIX
THAT
CHANGES
THE
SIZE
OF
AN
IMMEDIATE
OR
DISPLACEMENT
CANNOT
BE
AVOIDED
SCHEDULE
THEM
BEHIND
INSTRUCTIONS
THAT
STALL
THE
PIPE
FOR
SOME
OTHER
REASON
OPTIMIZING
THE
EXECUTION
CORE
THE
SUPERSCALAR
OUT
OF
ORDER
EXECUTION
CORE
IN
RECENT
GENERATIONS
OF
MICROARCHI
TECTURES
CONTAIN
MULTIPLE
EXECUTION
HARDWARE
RESOURCES
THAT
CAN
EXECUTE
MULTIPLE
OPS
IN
PARALLEL
THESE
RESOURCES
GENERALLY
ENSURE
THAT
OPS
EXECUTE
EFFICIENTLY
AND
PROCEED
WITH
FIXED
LATENCIES
GENERAL
GUIDELINES
TO
MAKE
USE
OF
THE
AVAILABLE
PARAL
LELISM
ARE
FOLLOW
THE
RULES
SEE
SECTION
TO
MAXIMIZE
USEFUL
DECODE
BANDWIDTH
AND
FRONT
END
THROUGHPUT
THESE
RULES
INCLUDE
FAVOURING
SINGLE
OP
INSTRUCTIONS
AND
TAKING
ADVANTAGE
OF
MICRO
FUSION
STACK
POINTER
TRACKER
AND
MACRO
FUSION
MAXIMIZE
RENAME
BANDWIDTH
GUIDELINES
ARE
DISCUSSED
IN
THIS
SECTION
AND
INCLUDE
PROPERLY
DEALING
WITH
PARTIAL
REGISTERS
ROB
READ
PORTS
AND
INSTRUCTIONS
WHICH
CAUSES
SIDE
EFFECTS
ON
FLAGS
SCHEDULING
RECOMMENDATIONS
ON
SEQUENCES
OF
INSTRUCTIONS
SO
THAT
MULTIPLE
DEPENDENCY
CHAINS
ARE
ALIVE
IN
THE
RESERVATION
STATION
RS
SIMULTANEOUSLY
THUS
ENSURING
THAT
YOUR
CODE
UTILIZES
MAXIMUM
PARALLELISM
AVOID
HAZARDS
MINIMIZE
DELAYS
THAT
MAY
OCCUR
IN
THE
EXECUTION
CORE
ALLOWING
THE
DISPATCHED
OPS
TO
MAKE
PROGRESS
AND
BE
READY
FOR
RETIREMENT
QUICKLY
INSTRUCTION
SELECTION
SOME
EXECUTION
UNITS
ARE
NOT
PIPELINED
THIS
MEANS
THAT
OPS
CANNOT
BE
DISPATCHED
IN
CONSECUTIVE
CYCLES
AND
THE
THROUGHPUT
IS
LESS
THAN
ONE
PER
CYCLE
IT
IS
GENERALLY
A
GOOD
STARTING
POINT
TO
SELECT
INSTRUCTIONS
BY
CONSIDERING
THE
NUMBER
OF
OPS
ASSOCIATED
WITH
EACH
INSTRUCTION
FAVORING
IN
THE
ORDER
OF
SINGLE
OP
INSTRUC
TIONS
SIMPLE
INSTRUCTION
WITH
LESS
THEN
OPS
AND
LAST
INSTRUCTION
REQUIRING
MICROSE
QUENCER
ROM
OPS
WHICH
ARE
EXECUTED
OUT
OF
THE
MICROSEQUENCER
INVOLVE
EXTRA
OVERHEAD
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
H
GENERALITY
FAVOR
SINGLE
MICRO
OPERATION
INSTRUCTIONS
ALSO
FAVOR
INSTRUCTION
WITH
SHORTER
LATENCIES
A
COMPILER
MAY
BE
ALREADY
DOING
A
GOOD
JOB
ON
INSTRUCTION
SELECTION
IF
SO
USER
INTER
VENTION
USUALLY
IS
NOT
NECESSARY
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
L
GENERALITY
AVOID
PREFIXES
ESPECIALLY
MULTIPLE
NON
PREFIXED
OPCODES
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
L
GENERALITY
DO
NOT
USE
MANY
SEGMENT
REGISTERS
ON
THE
PENTIUM
M
PROCESSOR
THERE
IS
ONLY
ONE
LEVEL
OF
RENAMING
OF
SEGMENT
REGIS
TERS
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
M
GENERALITY
AVOID
USING
COMPLEX
INSTRUCTIONS
FOR
EXAMPLE
ENTER
LEAVE
OR
LOOP
THAT
HAVE
MORE
THAN
FOUR
ΜOPS
AND
REQUIRE
MULTIPLE
CYCLES
TO
DECODE
USE
SEQUENCES
OF
SIMPLE
INSTRUCTIONS
INSTEAD
COMPLEX
INSTRUCTIONS
MAY
SAVE
ARCHITECTURAL
REGISTERS
BUT
INCUR
A
PENALTY
OF
ΜOPS
TO
SET
UP
PARAMETERS
FOR
THE
MICROSEQUENCER
ROM
IN
INTEL
NETBURST
MICROARCHITECTURE
THEORETICALLY
ARRANGING
INSTRUCTIONS
SEQUENCE
TO
MATCH
THE
TEMPLATE
APPLIES
TO
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
HOWEVER
WITH
MACRO
FUSION
AND
MICRO
FUSION
CAPABILITIES
IN
THE
FRONT
END
ATTEMPTS
TO
SCHEDULE
INSTRUCTION
SEQUENCES
USING
THE
TEMPLATE
WILL
LIKELY
PROVIDE
DIMINISHING
RETURNS
INSTEAD
SOFTWARE
SHOULD
FOLLOW
THESE
ADDITIONAL
DECODER
GUIDELINES
IF
YOU
NEED
TO
USE
MULTIPLE
OP
NON
MICROSEQUENCED
INSTRUCTIONS
TRY
TO
SEPARATE
BY
A
FEW
SINGLE
OP
INSTRUCTIONS
THE
FOLLOWING
INSTRUCTIONS
ARE
EXAMPLES
OF
MULTIPLE
OP
INSTRUCTION
NOT
REQUIRING
MICRO
SEQUENCER
ADC
SBB
CMOVCC
READ
MODIFY
WRITE
INSTRUCTIONS
IF
A
SERIES
OF
MULTIPLE
OP
INSTRUCTIONS
CANNOT
BE
SEPARATED
TRY
BREAKING
THE
SERIES
INTO
A
DIFFERENT
EQUIVALENT
INSTRUCTION
SEQUENCE
FOR
EXAMPLE
A
SERIES
OF
READ
MODIFY
WRITE
INSTRUCTIONS
MAY
GO
FASTER
IF
SEQUENCED
AS
A
SERIES
OF
READ
MODIFY
STORE
INSTRUCTIONS
THIS
STRATEGY
COULD
IMPROVE
PERFORMANCE
EVEN
IF
THE
NEW
CODE
SEQUENCE
IS
LARGER
THAN
THE
ORIGINAL
ONE
USE
OF
THE
INC
AND
DEC
INSTRUCTIONS
THE
INC
AND
DEC
INSTRUCTIONS
MODIFY
ONLY
A
SUBSET
OF
THE
BITS
IN
THE
FLAG
REGISTER
THIS
CREATES
A
DEPENDENCE
ON
ALL
PREVIOUS
WRITES
OF
THE
FLAG
REGISTER
THIS
IS
ESPECIALLY
PROBLEMATIC
WHEN
THESE
INSTRUCTIONS
ARE
ON
THE
CRITICAL
PATH
BECAUSE
THEY
ARE
USED
TO
CHANGE
AN
ADDRESS
FOR
A
LOAD
ON
WHICH
MANY
OTHER
INSTRUCTIONS
DEPEND
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
H
GENERALITY
INC
AND
DEC
INSTRUCTIONS
SHOULD
BE
REPLACED
WITH
ADD
OR
SUB
INSTRUCTIONS
BECAUSE
ADD
AND
SUB
OVERWRITE
ALL
FLAGS
WHEREAS
INC
AND
DEC
DO
NOT
THEREFORE
CREATING
FALSE
DEPENDENCIES
ON
EARLIER
INSTRUCTIONS
THAT
SET
THE
FLAGS
INTEGER
DIVIDE
TYPICALLY
AN
INTEGER
DIVIDE
IS
PRECEDED
BY
A
CWD
OR
CDQ
INSTRUCTION
DEPENDING
ON
THE
OPERAND
SIZE
DIVIDE
INSTRUCTIONS
USE
DX
AX
OR
EDX
EAX
FOR
THE
DIVIDEND
THE
CWD
OR
CDQ
INSTRUCTIONS
SIGN
EXTEND
AX
OR
EAX
INTO
DX
OR
EDX
RESPECTIVELY
THESE
INSTRUCTIONS
HAVE
DENSER
ENCODING
THAN
A
SHIFT
AND
MOVE
WOULD
BE
BUT
THEY
GENERATE
THE
SAME
NUMBER
OF
MICRO
OPS
IF
AX
OR
EAX
IS
KNOWN
TO
BE
POSITIVE
REPLACE
THESE
INSTRUCTIONS
WITH
XOR
DX
DX
OR
XOR
EDX
EDX
USING
LEA
IN
SOME
CASES
WITH
PROCESSOR
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
THE
LEA
INSTRUCTION
OR
A
SEQUENCE
OF
LEA
ADD
SUB
AND
SHIFT
INSTRUCTIONS
CAN
REPLACE
CONSTANT
MULTIPLY
INSTRUCTIONS
THE
LEA
INSTRUCTION
CAN
ALSO
BE
USED
AS
A
MULTIPLE
OPERAND
ADDITION
INSTRUCTION
FOR
EXAMPLE
LEA
ECX
EAX
EBX
A
USING
LEA
IN
THIS
WAY
MAY
AVOID
REGISTER
USAGE
BY
NOT
TYING
UP
REGISTERS
FOR
OPERANDS
OF
ARITHMETIC
INSTRUCTIONS
THIS
USE
MAY
ALSO
SAVE
CODE
SPACE
IF
THE
LEA
INSTRUCTION
USES
A
SHIFT
BY
A
CONSTANT
AMOUNT
THEN
THE
LATENCY
OF
THE
SEQUENCE
OF
ΜOPS
IS
SHORTER
IF
ADDS
ARE
USED
INSTEAD
OF
A
SHIFT
AND
THE
LEA
INSTRUCTION
MAY
BE
REPLACED
WITH
AN
APPROPRIATE
SEQUENCE
OF
ΜOPS
THIS
HOWEVER
INCREASES
THE
TOTAL
NUMBER
OF
ΜOPS
LEADING
TO
A
TRADE
OFF
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
L
GENERALITY
IF
AN
LEA
INSTRUCTION
USING
THE
SCALED
INDEX
IS
ON
THE
CRITICAL
PATH
A
SEQUENCE
WITH
ADDS
MAY
BE
BETTER
IF
CODE
DENSITY
AND
BANDWIDTH
OUT
OF
THE
TRACE
CACHE
ARE
THE
CRITICAL
FACTOR
THEN
USE
THE
LEA
INSTRUCTION
USING
SHIFT
AND
ROTATE
THE
SHIFT
AND
ROTATE
INSTRUCTIONS
HAVE
A
LONGER
LATENCY
ON
PROCESSOR
WITH
A
CPUID
SIGNATURE
CORRESPONDING
TO
FAMILY
AND
MODEL
ENCODING
OF
OR
THE
LATENCY
OF
A
SEQUENCE
OF
ADDS
WILL
BE
SHORTER
FOR
LEFT
SHIFTS
OF
THREE
OR
LESS
FIXED
AND
VARIABLE
SHIFTS
HAVE
THE
SAME
LATENCY
THE
ROTATE
BY
IMMEDIATE
AND
ROTATE
BY
REGISTER
INSTRUCTIONS
ARE
MORE
EXPENSIVE
THAN
A
SHIFT
THE
ROTATE
BY
INSTRUCTION
HAS
THE
SAME
LATENCY
AS
A
SHIFT
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
L
GENERALITY
AVOID
ROTATE
BY
REGISTER
OR
ROTATE
BY
IMMEDIATE
INSTRUCTIONS
IF
POSSIBLE
REPLACE
WITH
A
ROTATE
BY
INSTRUCTION
ADDRESS
CALCULATIONS
FOR
COMPUTING
ADDRESSES
USE
THE
ADDRESSING
MODES
RATHER
THAN
GENERAL
PURPOSE
COMPUTATIONS
INTERNALLY
MEMORY
REFERENCE
INSTRUCTIONS
CAN
HAVE
FOUR
OPERANDS
RELOCATABLE
LOAD
TIME
CONSTANT
IMMEDIATE
CONSTANT
BASE
REGISTER
SCALED
INDEX
REGISTER
IN
THE
SEGMENTED
MODEL
A
SEGMENT
REGISTER
MAY
CONSTITUTE
AN
ADDITIONAL
OPERAND
IN
THE
LINEAR
ADDRESS
CALCULATION
IN
MANY
CASES
SEVERAL
INTEGER
INSTRUCTIONS
CAN
BE
ELIMINATED
BY
FULLY
USING
THE
OPERANDS
OF
MEMORY
REFERENCES
CLEARING
REGISTERS
AND
DEPENDENCY
BREAKING
IDIOMS
CODE
SEQUENCES
THAT
MODIFIES
PARTIAL
REGISTER
CAN
EXPERIENCE
SOME
DELAY
IN
ITS
DEPENDENCY
CHAIN
BUT
CAN
BE
AVOIDED
BY
USING
DEPENDENCY
BREAKING
IDIOMS
IN
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
A
NUMBER
OF
INSTRUCTIONS
CAN
HELP
CLEAR
EXECUTION
DEPENDENCY
WHEN
SOFTWARE
USES
THESE
INSTRUCTION
TO
CLEAR
REGISTER
CONTENT
TO
ZERO
THE
INSTRUCTIONS
INCLUDE
XOR
REG
REG
SUB
REG
REG
XORPS
PD
XMMREG
XMMREG
PXOR
XMMREG
XMMREG
SUBPS
PD
XMMREG
XMMREG
PSUBB
W
D
Q
XMMREG
XMMREG
IN
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
THE
XOR
SUB
XORPS
OR
PXOR
INSTRUCTIONS
CAN
BE
USED
TO
CLEAR
EXECUTION
DEPENDENCIES
ON
THE
ZERO
EVALUATION
OF
THE
DESTINATION
REGISTER
THE
PENTIUM
PROCESSOR
PROVIDES
SPECIAL
SUPPORT
FOR
XOR
SUB
AND
PXOR
OPERA
TIONS
WHEN
EXECUTED
WITHIN
THE
SAME
REGISTER
THIS
RECOGNIZES
THAT
CLEARING
A
REGISTER
DOES
NOT
DEPEND
ON
THE
OLD
VALUE
OF
THE
REGISTER
THE
XORPS
AND
XORPD
INSTRUCTIONS
DO
NOT
HAVE
THIS
SPECIAL
SUPPORT
THEY
CANNOT
BE
USED
TO
BREAK
DEPENDENCE
CHAINS
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
ML
GENERALITY
USE
DEPENDENCY
BREAKING
IDIOM
INSTRUCTIONS
TO
SET
A
REGISTER
TO
OR
TO
BREAK
A
FALSE
DEPENDENCE
CHAIN
RESULTING
FROM
RE
USE
OF
REGISTERS
IN
CONTEXTS
WHERE
THE
CONDITION
CODES
MUST
BE
PRESERVED
MOVE
INTO
THE
REGISTER
INSTEAD
THIS
REQUIRES
MORE
CODE
SPACE
THAN
USING
XOR
AND
SUB
BUT
AVOIDS
SETTING
THE
CONDITION
CODES
EXAMPLE
OF
USING
PXOR
TO
BREAK
DEPENDENCY
IDIOM
ON
A
XMM
REGISTER
WHEN
PERFORMING
NEGATION
ON
THE
ELEMENTS
OF
AN
ARRAY
INT
A
B
C
FOR
INT
I
I
I
C
I
A
I
B
I
EXAMPLE
CLEARING
REGISTER
TO
BREAK
DEPENDENCY
WHILE
NEGATING
ARRAY
ELEMENTS
NEGATION
X
X
XOR
WITHOUT
BREAKING
DEPENDENCY
NEGATION
X
X
USING
PXOR
REG
REG
BREAKS
DEPENDENCY
LEA
EAX
A
LEA
EAX
A
LEA
ECX
B
LEA
ECX
B
LEA
EDI
C
LEA
EDI
C
XOR
EDX
EDX
XOR
EDX
EDX
MOVDQA
ALLONE
LP
LP
MOVDQA
EAX
EDX
MOVDQA
EAX
EDX
PADDD
ECX
EDX
PADDD
ECX
EDX
PXOR
PXOR
PSUBD
PSUBD
MOVDQA
EDI
EDX
MOVDQA
EDI
EDX
ADD
EDX
ADD
EDX
CMP
EDX
CMP
EDX
JL
LP
JL
LP
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
MH
GENERALITY
BREAK
DEPENDENCES
ON
PORTIONS
OF
REGISTERS
BETWEEN
INSTRUCTIONS
BY
OPERATING
ON
BIT
REGISTERS
INSTEAD
OF
PARTIAL
REGISTERS
FOR
MOVES
THIS
CAN
BE
ACCOMPLISHED
WITH
BIT
MOVES
OR
BY
USING
MOVZX
ON
PENTIUM
M
PROCESSORS
THE
MOVSX
AND
MOVZX
INSTRUCTIONS
BOTH
TAKE
A
SINGLE
OP
WHETHER
THEY
MOVE
FROM
A
REGISTER
OR
MEMORY
ON
PENTIUM
PROCESSORS
THE
MOVSX
TAKES
AN
ADDITIONAL
OP
THIS
IS
LIKELY
TO
CAUSE
LESS
DELAY
THAN
THE
PARTIAL
REGISTER
UPDATE
PROBLEM
MENTIONED
ABOVE
BUT
THE
PERFORMANCE
GAIN
MAY
VARY
IF
THE
ADDITIONAL
OP
IS
A
CRITICAL
PROBLEM
MOVSX
CAN
SOMETIMES
BE
USED
AS
ALTERNATIVE
SOMETIMES
SIGN
EXTENDED
SEMANTICS
CAN
BE
MAINTAINED
BY
ZERO
EXTENDING
OPER
ANDS
FOR
EXAMPLE
THE
C
CODE
IN
THE
FOLLOWING
STATEMENTS
DOES
NOT
NEED
SIGN
EXTEN
SION
NOR
DOES
IT
NEED
PREFIXES
FOR
OPERAND
SIZE
OVERRIDES
STATIC
SHORT
INT
A
B
IF
A
B
CODE
FOR
COMPARING
THESE
BIT
OPERANDS
MIGHT
BE
MOVZW
EAX
A
MOVZW
EBX
B
CMP
EAX
EBX
THESE
CIRCUMSTANCES
TEND
TO
BE
COMMON
HOWEVER
THE
TECHNIQUE
WILL
NOT
WORK
IF
THE
COMPARE
IS
FOR
GREATER
THAN
LESS
THAN
GREATER
THAN
OR
EQUAL
AND
SO
ON
OR
IF
THE
VALUES
IN
EAX
OR
EBX
ARE
TO
BE
USED
IN
ANOTHER
OPERATION
WHERE
SIGN
EXTENSION
IS
REQUIRED
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
M
GENERALITY
TRY
TO
USE
ZERO
EXTENSION
OR
OPERATE
ON
BIT
OPERANDS
INSTEAD
OF
USING
MOVES
WITH
SIGN
EXTENSION
THE
TRACE
CACHE
CAN
BE
PACKED
MORE
TIGHTLY
WHEN
INSTRUCTIONS
WITH
OPERANDS
THAT
CAN
ONLY
BE
REPRESENTED
AS
BITS
ARE
NOT
ADJACENT
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
L
GENERALITY
AVOID
PLACING
INSTRUCTIONS
THAT
USE
BIT
IMMEDIATES
WHICH
CANNOT
BE
ENCODED
AS
SIGN
EXTENDED
BIT
IMMEDIATES
NEAR
EACH
OTHER
TRY
TO
SCHEDULE
ΜOPS
THAT
HAVE
NO
IMMEDIATE
IMMEDIATELY
BEFORE
OR
AFTER
ΜOPS
WITH
BIT
IMMEDIATES
COMPARES
USE
TEST
WHEN
COMPARING
A
VALUE
IN
A
REGISTER
WITH
ZERO
TEST
ESSENTIALLY
ANDS
OPERANDS
TOGETHER
WITHOUT
WRITING
TO
A
DESTINATION
REGISTER
TEST
IS
PREFERRED
OVER
AND
BECAUSE
AND
PRODUCES
AN
EXTRA
RESULT
REGISTER
TEST
IS
BETTER
THAN
CMP
BECAUSE
THE
INSTRUCTION
SIZE
IS
SMALLER
USE
TEST
WHEN
COMPARING
THE
RESULT
OF
A
LOGICAL
AND
WITH
AN
IMMEDIATE
CONSTANT
FOR
EQUALITY
OR
INEQUALITY
IF
THE
REGISTER
IS
EAX
FOR
CASES
SUCH
AS
IF
AVAR
THE
TEST
INSTRUCTION
CAN
ALSO
BE
USED
TO
DETECT
ROLLOVER
OF
MODULO
OF
A
POWER
OF
FOR
EXAMPLE
THE
C
CODE
IF
AVAR
CAN
BE
IMPLEMENTED
USING
TEST
EAX
JNZ
AFTERIF
USING
THE
TEST
INSTRUCTION
BETWEEN
THE
INSTRUCTION
THAT
MAY
MODIFY
PART
OF
THE
FLAG
REGISTER
AND
THE
INSTRUCTION
THAT
USES
THE
FLAG
REGISTER
CAN
ALSO
HELP
PREVENT
PARTIAL
FLAG
REGISTER
STALL
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
M
GENERALITY
USE
THE
TEST
INSTRUCTION
INSTEAD
OF
AND
WHEN
THE
RESULT
OF
THE
LOGICAL
AND
IS
NOT
USED
THIS
SAVES
ΜOPS
IN
EXECUTION
USE
A
TEST
IF
A
REGISTER
WITH
ITSELF
INSTEAD
OF
A
CMP
OF
THE
REGISTER
TO
ZERO
THIS
SAVES
THE
NEED
TO
ENCODE
THE
ZERO
AND
SAVES
ENCODING
SPACE
AVOID
COMPARING
A
CONSTANT
TO
A
MEMORY
OPERAND
IT
IS
PREFERABLE
TO
LOAD
THE
MEMORY
OPERAND
AND
COMPARE
THE
CONSTANT
TO
A
REGISTER
OFTEN
A
PRODUCED
VALUE
MUST
BE
COMPARED
WITH
ZERO
AND
THEN
USED
IN
A
BRANCH
BECAUSE
MOST
INTEL
ARCHITECTURE
INSTRUCTIONS
SET
THE
CONDITION
CODES
AS
PART
OF
THEIR
EXECUTION
THE
COMPARE
INSTRUCTION
MAY
BE
ELIMINATED
THUS
THE
OPERATION
CAN
BE
TESTED
DIRECTLY
BY
A
JCC
INSTRUCTION
THE
NOTABLE
EXCEPTIONS
ARE
MOV
AND
LEA
IN
THESE
CASES
USE
TEST
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
M
GENERALITY
ELIMINATE
UNNECESSARY
COMPARE
WITH
ZERO
INSTRUCTIONS
BY
USING
THE
APPROPRIATE
CONDITIONAL
JUMP
INSTRUCTION
WHEN
THE
FLAGS
ARE
ALREADY
SET
BY
A
PRECEDING
ARITHMETIC
INSTRUCTION
IF
NECESSARY
USE
A
TEST
INSTRUCTION
INSTEAD
OF
A
COMPARE
BE
CERTAIN
THAT
ANY
CODE
TRANSFORMATIONS
MADE
DO
NOT
INTRODUCE
PROBLEMS
WITH
OVERFLOW
USING
NOPS
CODE
GENERATORS
GENERATE
A
NO
OPERATION
NOP
TO
ALIGN
INSTRUCTIONS
EXAMPLES
OF
NOPS
OF
DIFFERENT
LENGTHS
IN
BIT
MODE
ARE
SHOWN
BELOW
BYTE
XCHG
EAX
EAX
BYTE
NOP
BYTE
LEA
REG
REG
BIT
DISPLACEMENT
BYTE
NOP
DWORD
PTR
EAX
BIT
DISPLACEMENT
BYTE
NOP
DWORD
PTR
EAX
EAX
BIT
DISPLACEMENT
BYTE
LEA
REG
REG
BIT
DISPLACEMENT
BYTE
NOP
DWORD
PTR
EAX
BIT
DISPLACEMENT
BYTE
NOP
DWORD
PTR
EAX
EAX
BIT
DISPLACEMENT
BYTE
NOP
WORD
PTR
EAX
EAX
BIT
DISPLACEMENT
THESE
ARE
ALL
TRUE
NOPS
HAVING
NO
EFFECT
ON
THE
STATE
OF
THE
MACHINE
EXCEPT
TO
ADVANCE
THE
EIP
BECAUSE
NOPS
REQUIRE
HARDWARE
RESOURCES
TO
DECODE
AND
EXECUTE
USE
THE
FEWEST
NUMBER
TO
ACHIEVE
THE
DESIRED
PADDING
THE
ONE
BYTE
NOP
XCHG
EAX
EAX
HAS
SPECIAL
HARDWARE
SUPPORT
ALTHOUGH
IT
STILL
CONSUMES
A
ΜOP
AND
ITS
ACCOMPANYING
RESOURCES
THE
DEPENDENCE
UPON
THE
OLD
VALUE
OF
EAX
IS
REMOVED
THIS
ΜOP
CAN
BE
EXECUTED
AT
THE
EARLIEST
POSSIBLE
OPPORTUNITY
REDUCING
THE
NUMBER
OF
OUTSTANDING
INSTRUCTIONS
AND
IS
THE
LOWEST
COST
NOP
THE
OTHER
NOPS
HAVE
NO
SPECIAL
HARDWARE
SUPPORT
THEIR
INPUT
AND
OUTPUT
REGISTERS
ARE
INTERPRETED
BY
THE
HARDWARE
THEREFORE
A
CODE
GENERATOR
SHOULD
ARRANGE
TO
USE
THE
REGISTER
CONTAINING
THE
OLDEST
VALUE
AS
INPUT
SO
THAT
THE
NOP
WILL
DISPATCH
AND
RELEASE
RS
RESOURCES
AT
THE
EARLIEST
POSSIBLE
OPPORTUNITY
TRY
TO
OBSERVE
THE
FOLLOWING
NOP
GENERATION
PRIORITY
SELECT
THE
SMALLEST
NUMBER
OF
NOPS
AND
PSEUDO
NOPS
TO
PROVIDE
THE
DESIRED
PADDING
SELECT
NOPS
THAT
ARE
LEAST
LIKELY
TO
EXECUTE
ON
SLOWER
EXECUTION
UNIT
CLUSTERS
SELECT
THE
REGISTER
ARGUMENTS
OF
NOPS
TO
REDUCE
DEPENDENCIES
MIXING
SIMD
DATA
TYPES
PREVIOUS
MICROARCHITECTURES
BEFORE
INTEL
CORE
MICROARCHITECTURE
DO
NOT
HAVE
EXPLICIT
RESTRICTIONS
ON
MIXING
INTEGER
AND
FLOATING
POINT
FP
OPERATIONS
ON
XMM
REGISTERS
FOR
INTEL
CORE
MICROARCHITECTURE
MIXING
INTEGER
AND
FLOATING
POINT
OPERA
TIONS
ON
THE
CONTENT
OF
AN
XMM
REGISTER
CAN
DEGRADE
PERFORMANCE
SOFTWARE
SHOULD
AVOID
MIXED
USE
OF
INTEGER
FP
OPERATION
ON
XMM
REGISTERS
SPECIFICALLY
USE
SIMD
INTEGER
OPERATIONS
TO
FEED
SIMD
INTEGER
OPERATIONS
USE
PXOR
FOR
IDIOM
USE
SIMD
FLOATING
POINT
OPERATIONS
TO
FEED
SIMD
FLOATING
POINT
OPERATIONS
USE
XORPS
FOR
IDIOM
WHEN
FLOATING
POINT
OPERATIONS
ARE
BITWISE
EQUIVALENT
USE
PS
DATA
TYPE
INSTEAD
OF
PD
DATA
TYPE
MOVAPS
AND
MOVAPD
DO
THE
SAME
THING
BUT
MOVAPS
TAKES
ONE
LESS
BYTE
TO
ENCODE
THE
INSTRUCTION
SPILL
SCHEDULING
THE
SPILL
SCHEDULING
ALGORITHM
USED
BY
A
CODE
GENERATOR
WILL
BE
IMPACTED
BY
THE
MEMORY
SUBSYSTEM
A
SPILL
SCHEDULING
ALGORITHM
IS
AN
ALGORITHM
THAT
SELECTS
WHAT
VALUES
TO
SPILL
TO
MEMORY
WHEN
THERE
ARE
TOO
MANY
LIVE
VALUES
TO
FIT
IN
REGISTERS
CONSIDER
THE
CODE
IN
EXAMPLE
WHERE
IT
IS
NECESSARY
TO
SPILL
EITHER
A
B
OR
C
EXAMPLE
SPILL
SCHEDULING
CODE
LOOP
C
B
A
A
FOR
MODERN
MICROARCHITECTURES
USING
DEPENDENCE
DEPTH
INFORMATION
IN
SPILL
SCHED
ULING
IS
EVEN
MORE
IMPORTANT
THAN
IN
PREVIOUS
PROCESSORS
THE
LOOP
CARRIED
DEPEN
DENCE
IN
A
MAKES
IT
ESPECIALLY
IMPORTANT
THAT
A
NOT
BE
SPILLED
NOT
ONLY
WOULD
A
STORE
LOAD
BE
PLACED
IN
THE
DEPENDENCE
CHAIN
BUT
THERE
WOULD
ALSO
BE
A
DATA
NOT
READY
STALL
OF
THE
LOAD
COSTING
FURTHER
CYCLES
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
MH
GENERALITY
FOR
SMALL
LOOPS
PLACING
LOOP
INVARIANTS
IN
MEMORY
IS
BETTER
THAN
SPILLING
LOOP
CARRIED
DEPENDENCIES
A
POSSIBLY
COUNTER
INTUITIVE
RESULT
IS
THAT
IN
SUCH
A
SITUATION
IT
IS
BETTER
TO
PUT
LOOP
INVARIANTS
IN
MEMORY
THAN
IN
REGISTERS
SINCE
LOOP
INVARIANTS
NEVER
HAVE
A
LOAD
BLOCKED
BY
STORE
DATA
THAT
IS
NOT
READY
AVOIDING
STALLS
IN
EXECUTION
CORE
ALTHOUGH
THE
DESIGN
OF
THE
EXECUTION
CORE
IS
OPTIMIZED
TO
MAKE
COMMON
CASES
EXECUTES
QUICKLY
A
OP
MAY
ENCOUNTER
VARIOUS
HAZARDS
DELAYS
OR
STALLS
WHILE
MAKING
FORWARD
PROGRESS
FROM
THE
FRONT
END
TO
THE
ROB
AND
RS
THE
SIGNIFICANT
CASES
ARE
ROB
READ
PORT
STALLS
PARTIAL
REGISTER
REFERENCE
STALLS
PARTIAL
UPDATES
TO
XMM
REGISTER
STALLS
PARTIAL
FLAG
REGISTER
REFERENCE
STALLS
ROB
READ
PORT
STALLS
AS
A
OP
IS
RENAMED
IT
DETERMINES
WHETHER
ITS
SOURCE
OPERANDS
HAVE
EXECUTED
AND
BEEN
WRITTEN
TO
THE
REORDER
BUFFER
ROB
OR
WHETHER
THEY
WILL
BE
CAPTURED
IN
FLIGHT
IN
THE
RS
OR
IN
THE
BYPASS
NETWORK
TYPICALLY
THE
GREAT
MAJORITY
OF
SOURCE
OPERANDS
ARE
FOUND
TO
BE
IN
FLIGHT
DURING
RENAMING
THOSE
THAT
HAVE
BEEN
WRITTEN
BACK
TO
THE
ROB
ARE
READ
THROUGH
A
SET
OF
READ
PORTS
SINCE
THE
INTEL
CORE
MICROARCHITECTURE
IS
OPTIMIZED
FOR
THE
COMMON
CASE
WHERE
THE
OPERANDS
ARE
IN
FLIGHT
IT
DOES
NOT
PROVIDE
A
FULL
SET
OF
READ
PORTS
TO
ENABLE
ALL
RENAMED
OPS
TO
READ
ALL
SOURCES
FROM
THE
ROB
IN
THE
SAME
CYCLE
WHEN
NOT
ALL
SOURCES
CAN
BE
READ
A
OP
CAN
STALL
IN
THE
RENAME
STAGE
UNTIL
IT
CAN
GET
ACCESS
TO
ENOUGH
ROB
READ
PORTS
TO
COMPLETE
RENAMING
THE
OP
THIS
STALL
IS
USUALLY
SHORT
LIVED
TYPICALLY
A
OP
WILL
COMPLETE
RENAMING
IN
THE
NEXT
CYCLE
BUT
IT
APPEARS
TO
THE
APPLICATION
AS
A
LOSS
OF
RENAME
BANDWIDTH
SOME
OF
THE
SOFTWARE
VISIBLE
SITUATIONS
THAT
CAN
CAUSE
ROB
READ
PORT
STALLS
INCLUDE
REGISTERS
THAT
HAVE
BECOME
COLD
AND
REQUIRE
A
ROB
READ
PORT
BECAUSE
EXECUTION
UNITS
ARE
DOING
OTHER
INDEPENDENT
CALCULATIONS
CONSTANTS
INSIDE
REGISTERS
POINTER
AND
INDEX
REGISTERS
IN
RARE
CASES
ROB
READ
PORT
STALLS
MAY
LEAD
TO
MORE
SIGNIFICANT
PERFORMANCE
DEGRA
DATIONS
THERE
ARE
A
COUPLE
OF
HEURISTICS
THAT
CAN
HELP
PREVENT
OVER
SUBSCRIBING
THE
ROB
READ
PORTS
KEEP
COMMON
REGISTER
USAGE
CLUSTERED
TOGETHER
MULTIPLE
REFERENCES
TO
THE
SAME
WRITTEN
BACK
REGISTER
CAN
BE
FOLDED
INSIDE
THE
OUT
OF
ORDER
EXECUTION
CORE
KEEP
DEPENDENCY
CHAINS
INTACT
THIS
PRACTICE
ENSURES
THAT
THE
REGISTERS
WILL
NOT
HAVE
BEEN
WRITTEN
BACK
WHEN
THE
NEW
MICRO
OPS
ARE
WRITTEN
TO
THE
RS
THESE
TWO
SCHEDULING
HEURISTICS
MAY
CONFLICT
WITH
OTHER
MORE
COMMON
SCHEDULING
HEURISTICS
TO
REDUCE
DEMAND
ON
THE
ROB
READ
PORT
USE
THESE
TWO
HEURISTICS
ONLY
IF
BOTH
THE
FOLLOWING
SITUATIONS
ARE
MET
SHORT
LATENCY
OPERATIONS
INDICATIONS
OF
ACTUAL
ROB
READ
PORT
STALLS
CAN
BE
CONFIRMED
BY
MEASUREMENTS
OF
THE
PERFORMANCE
EVENT
THE
RELEVANT
EVENT
IS
SEE
APPENDIX
A
OF
THE
INTEL
AND
IA
ARCHITECTURES
SOFTWARE
DEVELOPER
MANUAL
VOLUME
IF
THE
CODE
HAS
A
LONG
DEPENDENCY
CHAIN
THESE
TWO
HEURISTICS
SHOULD
NOT
BE
USED
BECAUSE
THEY
CAN
CAUSE
THE
RS
TO
FILL
CAUSING
DAMAGE
THAT
OUTWEIGHS
THE
POSITIVE
EFFECTS
OF
REDUCING
DEMANDS
ON
THE
ROB
READ
PORT
BYPASS
BETWEEN
EXECUTION
DOMAINS
FLOATING
POINT
FP
LOADS
HAVE
AN
EXTRA
CYCLE
OF
LATENCY
MOVES
BETWEEN
FP
AND
SIMD
STACKS
HAVE
ANOTHER
ADDITIONAL
CYCLE
OF
LATENCY
EXAMPLE
ADDPS
PAND
ADDPS
THE
OVERALL
LATENCY
FOR
THE
ABOVE
CALCULATION
IS
CYCLES
CYCLES
FOR
EACH
ADDPS
INSTRUCTION
CYCLE
FOR
THE
PAND
INSTRUCTION
CYCLE
TO
BYPASS
BETWEEN
THE
ADDPS
FLOATING
POINT
DOMAIN
TO
THE
PAND
INTEGER
DOMAIN
CYCLE
TO
MOVE
THE
DATA
FROM
THE
PAND
INTEGER
TO
THE
SECOND
FLOATING
POINT
ADDPS
DOMAIN
TO
AVOID
THIS
PENALTY
YOU
SHOULD
ORGANIZE
CODE
TO
MINIMIZE
DOMAIN
CHANGES
SOME
TIMES
YOU
CANNOT
AVOID
BYPASSES
ACCOUNT
FOR
BYPASS
CYCLES
WHEN
COUNTING
THE
OVERALL
LATENCY
OF
YOUR
CODE
IF
YOUR
CALCULATION
IS
LATENCY
BOUND
YOU
CAN
EXECUTE
MORE
INSTRUCTIONS
IN
PARALLEL
OR
BREAK
DEPENDENCY
CHAINS
TO
REDUCE
TOTAL
LATENCY
CODE
THAT
HAS
MANY
BYPASS
DOMAINS
AND
IS
COMPLETELY
LATENCY
BOUND
MAY
RUN
SLOWER
ON
THE
INTEL
CORE
MICROARCHITECTURE
THAN
IT
DID
ON
PREVIOUS
MICROARCHITECTURES
PARTIAL
REGISTER
STALLS
GENERAL
PURPOSE
REGISTERS
CAN
BE
ACCESSED
IN
GRANULARITIES
OF
BYTES
WORDS
DOUBLE
WORDS
BIT
MODE
ALSO
SUPPORTS
QUADWORD
GRANULARITY
REFERENCING
A
PORTION
OF
A
REGISTER
IS
REFERRED
TO
AS
A
PARTIAL
REGISTER
REFERENCE
A
PARTIAL
REGISTER
STALL
HAPPENS
WHEN
AN
INSTRUCTION
REFERS
TO
A
REGISTER
PORTIONS
OF
WHICH
WERE
PREVIOUSLY
MODIFIED
BY
OTHER
INSTRUCTIONS
FOR
EXAMPLE
PARTIAL
REGISTER
STALLS
OCCURS
WITH
A
READ
TO
AX
WHILE
PREVIOUS
INSTRUCTIONS
STORED
AL
AND
AH
OR
A
READ
TO
EAX
WHILE
PREVIOUS
INSTRUCTION
MODIFIED
AX
THE
DELAY
OF
A
PARTIAL
REGISTER
STALL
IS
SMALL
IN
PROCESSORS
BASED
ON
INTEL
CORE
AND
NETBURST
MICROARCHITECTURES
AND
IN
PENTIUM
M
PROCESSOR
WITH
CPUID
SIGNATURE
FAMILY
MODEL
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
PENTIUM
M
PROCESSORS
CPUID
SIGNATURE
WITH
FAMILY
MODEL
AND
THE
FAMILY
INCUR
A
LARGE
PENALTY
NOTE
THAT
IN
INTEL
ARCHITECTURE
AN
UPDATE
TO
THE
LOWER
BITS
OF
A
BIT
INTEGER
REGISTER
IS
ARCHITECTURALLY
DEFINED
TO
ZERO
EXTEND
THE
UPPER
BITS
WHILE
THIS
ACTION
MAY
BE
LOGICALLY
VIEWED
AS
A
BIT
UPDATE
IT
IS
REALLY
A
BIT
UPDATE
AND
THEREFORE
DOES
NOT
CAUSE
A
PARTIAL
STALL
REFERENCING
PARTIAL
REGISTERS
FREQUENTLY
PRODUCES
CODE
SEQUENCES
WITH
EITHER
FALSE
OR
REAL
DEPENDENCIES
EXAMPLE
DEMONSTRATES
A
SERIES
OF
FALSE
AND
REAL
DEPENDEN
CIES
CAUSED
BY
REFERENCING
PARTIAL
REGISTERS
IF
INSTRUCTIONS
AND
IN
EXAMPLE
ARE
CHANGED
TO
USE
A
MOVZX
INSTRUCTION
INSTEAD
OF
A
MOV
THEN
THE
DEPENDENCES
OF
INSTRUCTION
ON
AND
TRANSITIVELY
BEFORE
IT
AND
INSTRUCTION
ON
ARE
BROKEN
THIS
CREATES
TWO
INDEPENDENT
CHAINS
OF
COMPUTATION
INSTEAD
OF
ONE
SERIAL
ONE
EXAMPLE
DEPENDENCIES
CAUSED
BY
REFERENCING
PARTIAL
REGISTERS
ADD
AH
BH
ADD
AL
INSTRUCTION
HAS
A
FALSE
DEPENDENCY
ON
MOV
BL
AL
DEPENDS
ON
BUT
THE
DEPENDENCE
IS
REAL
MOV
AH
CH
INSTRUCTION
HAS
A
FALSE
DEPENDENCY
ON
SAR
EAX
THIS
WIPES
OUT
THE
AL
AH
AX
PART
SO
THE
RESULT
REALLY
DOESN
T
DEPEND
ON
THEM
PROGRAMATICALLY
BUT
THE
PROCESSOR
MUST
DEAL
WITH
REAL
DEPENDENCY
ON
AL
AH
AX
MOV
AL
BL
INSTRUCTION
HAS
A
REAL
DEPENDENCY
ON
ADD
AH
INSTRUCTION
HAS
A
FALSE
DEPENDENCY
ON
IMUL
DL
INSTRUCTION
HAS
A
FALSE
DEPENDENCY
ON
BECAUSE
AL
IS
IMPLICITLY
USED
MOV
AL
INSTRUCTION
HAS
A
FALSE
DEPENDENCY
ON
AND
A
REAL
DEPENDENCY
ON
IMUL
CX
IMPLICITLY
USES
AX
AND
WRITES
TO
DX
HENCE
A
REAL
DEPENDENCY
EXAMPLE
ILLUSTRATES
THE
USE
OF
MOVZX
TO
AVOID
A
PARTIAL
REGISTER
STALL
WHEN
PACKING
THREE
BYTE
VALUES
INTO
A
REGISTER
EXAMPLE
AVOIDING
PARTIAL
REGISTER
STALLS
IN
INTEGER
CODE
A
SEQUENCE
CAUSING
PARTIAL
REGISTER
STALL
ALTERNATE
SEQUENCE
USING
MOVZX
TO
AVOID
DELAY
MOV
AL
BYTE
PTR
A
MOVZX
EAX
BYTE
PTR
A
SHL
EAX
SHL
EAX
MOV
AX
WORD
PTR
A
MOVZX
ECX
WORD
PTR
A
MOVD
EAX
OR
EAX
ECX
RET
MOVD
EAX
RET
PARTIAL
XMM
REGISTER
STALLS
PARTIAL
REGISTER
STALLS
CAN
ALSO
APPLY
TO
XMM
REGISTERS
THE
FOLLOWING
SSE
AND
INSTRUCTIONS
UPDATE
ONLY
PART
OF
THE
DESTINATION
REGISTER
MOVL
HPD
XMM
MOVL
HPS
XMM
MOVSS
SD
BETWEEN
REGISTERS
USING
THESE
INSTRUCTIONS
CREATES
A
DEPENDENCY
CHAIN
BETWEEN
THE
UNMODIFIED
PART
OF
THE
REGISTER
AND
THE
MODIFIED
PART
OF
THE
REGISTER
THIS
DEPENDENCY
CHAIN
CAN
CAUSE
PERFORMANCE
LOSS
EXAMPLE
ILLUSTRATES
THE
USE
OF
MOVZX
TO
AVOID
A
PARTIAL
REGISTER
STALL
WHEN
PACKING
THREE
BYTE
VALUES
INTO
A
REGISTER
FOLLOW
THESE
RECOMMENDATIONS
TO
AVOID
STALLS
FROM
PARTIAL
UPDATES
TO
XMM
REGISTERS
AVOID
USING
INSTRUCTIONS
WHICH
UPDATE
ONLY
PART
OF
THE
XMM
REGISTER
IF
A
BIT
LOAD
IS
NEEDED
USE
THE
MOVSD
OR
MOVQ
INSTRUCTION
IF
BIT
LOADS
ARE
REQUIRED
TO
THE
SAME
REGISTER
FROM
NON
CONTINUOUS
LOCATIONS
USE
MOVSD
MOVHPD
INSTEAD
OF
MOVLPD
MOVHPD
WHEN
COPYING
THE
XMM
REGISTER
USE
THE
FOLLOWING
INSTRUCTIONS
FOR
FULL
REGISTER
COPY
EVEN
IF
YOU
ONLY
WANT
TO
COPY
SOME
OF
THE
SOURCE
REGISTER
DATA
MOVAPS
MOVAPD
MOVDQA
EXAMPLE
AVOIDING
PARTIAL
REGISTER
STALLS
IN
SIMD
CODE
USING
MOVLPD
FOR
MEMORY
TRANSACTIONS
AND
MOVSD
BETWEEN
REGISTER
COPIES
CAUSING
PARTIAL
REGISTER
STALL
USING
MOVSD
FOR
MEMORY
AND
MOVAPD
BETWEEN
REGISTER
COPIES
AVOID
DELAY
MOV
EDX
X
MOV
EDX
X
MOV
ECX
COUNT
MOV
ECX
COUNT
MOVLPD
MOVSD
MOVLPD
MOVSD
ALIGN
ALIGN
EXAMPLE
AVOIDING
PARTIAL
REGISTER
STALLS
IN
SIMD
CODE
CONTD
USING
MOVLPD
FOR
MEMORY
TRANSACTIONS
AND
MOVSD
BETWEEN
REGISTER
COPIES
CAUSING
PARTIAL
REGISTER
STALL
USING
MOVSD
FOR
MEMORY
AND
MOVAPD
BETWEEN
REGISTER
COPIES
AVOID
DELAY
LP
MOVLPD
EDX
ADDSD
MOVSD
SUBSD
EDX
MULSD
MOVSD
EDX
ADD
EDX
DEC
ECX
JNZ
LP
LP
MOVSD
EDX
ADDSD
MOVAPD
SUBSD
EDX
MULSD
MOVSD
EDX
ADD
EDX
DEC
ECX
JNZ
LP
PARTIAL
FLAG
REGISTER
STALLS
A
PARTIAL
FLAG
REGISTER
STALL
OCCURS
WHEN
AN
INSTRUCTION
MODIFIES
A
PART
OF
THE
FLAG
REGISTER
AND
THE
FOLLOWING
INSTRUCTION
IS
DEPENDENT
ON
THE
OUTCOME
OF
THE
FLAGS
THIS
HAPPENS
MOST
OFTEN
WITH
SHIFT
INSTRUCTIONS
SAR
SAL
SHR
SHL
THE
FLAGS
ARE
NOT
MODIFIED
IN
THE
CASE
OF
A
ZERO
SHIFT
COUNT
BUT
THE
SHIFT
COUNT
IS
USUALLY
KNOWN
ONLY
AT
EXECUTION
TIME
THE
FRONT
END
STALLS
UNTIL
THE
INSTRUCTION
IS
RETIRED
OTHER
INSTRUCTIONS
THAT
CAN
MODIFY
SOME
PART
OF
THE
FLAG
REGISTER
INCLUDE
VARIOUS
ROTATE
INSTRUCTIONS
STC
AND
STD
AN
EXAMPLE
OF
ASSEMBLY
WITH
A
PARTIAL
FLAG
REGISTER
STALL
AND
ALTERNATIVE
CODE
WITHOUT
THE
STALL
IS
SHOWN
IN
EXAMPLE
IN
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
SHIFT
IMMEDIATE
BY
IS
HANDLED
BY
SPECIAL
HARDWARE
SUCH
THAT
IT
DOES
NOT
EXPERIENCE
PARTIAL
FLAG
STALL
EXAMPLE
AVOIDING
PARTIAL
FLAG
REGISTER
STALLS
A
SEQUENCE
WITH
PARTIAL
FLAG
REGISTER
STALL
ALTERNATE
SEQUENCE
WITHOUT
PARTIAL
FLAG
REGISTER
STALL
XOR
EAX
EAX
OR
EAX
EAX
MOV
ECX
A
MOV
ECX
A
SAR
ECX
SAR
ECX
SETZ
AL
TEST
ECX
ECX
NO
PARTIAL
REGISTER
STALL
SETZ
AL
BUT
FLAG
STALL
AS
SAR
MAY
NO
PARTIAL
REG
OR
FLAG
STALL
CHANGE
THE
FLAGS
TEST
ALWAYS
UPDATES
ALL
THE
FLAGS
FLOATING
POINT
SIMD
OPERANDS
IN
INTEL
NETBURST
MICROARCHITECTURE
IN
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
THE
LATENCY
OF
MMX
OR
SIMD
FLOATING
POINT
REGISTER
TO
REGISTER
MOVES
IS
SIGNIFICANT
THIS
CAN
HAVE
IMPLICATIONS
FOR
REGISTER
ALLOCATION
MOVES
THAT
WRITE
A
PORTION
OF
A
REGISTER
CAN
INTRODUCE
UNWANTED
DEPENDENCES
THE
MOVSD
REG
REG
INSTRUCTION
WRITES
ONLY
THE
BOTTOM
BITS
OF
A
REGISTER
NOT
ALL
BITS
THIS
INTRODUCES
A
DEPENDENCE
ON
THE
PRECEDING
INSTRUCTION
THAT
PRODUCES
THE
UPPER
BITS
EVEN
IF
THOSE
BITS
ARE
NOT
LONGER
WANTED
THE
DEPENDENCE
INHIBITS
REGISTER
RENAMING
AND
THEREBY
REDUCES
PARALLELISM
USE
MOVAPD
AS
AN
ALTERNATIVE
IT
WRITES
ALL
BITS
EVEN
THOUGH
THIS
INSTRUCTION
HAS
A
LONGER
LATENCY
THE
OPS
FOR
MOVAPD
USE
A
DIFFERENT
EXECUTION
PORT
AND
THIS
PORT
IS
MORE
LIKELY
TO
BE
FREE
THE
CHANGE
CAN
IMPACT
PERFORMANCE
THERE
MAY
BE
EXCEP
TIONAL
CASES
WHERE
THE
LATENCY
MATTERS
MORE
THAN
THE
DEPENDENCE
OR
THE
EXECUTION
PORT
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
ML
GENERALITY
AVOID
INTRODUCING
DEPENDENCES
WITH
PARTIAL
FLOATING
POINT
REGISTER
WRITES
E
G
FROM
THE
MOVSD
INSTRUCTION
USE
THE
MOVAPD
INSTRUCTION
INSTEAD
THE
MOVSD
XMMREG
MEM
INSTRUCTION
WRITES
ALL
BITS
AND
BREAKS
A
DEPENDENCE
THE
MOVUPD
FROM
MEMORY
INSTRUCTION
PERFORMS
TWO
BIT
LOADS
BUT
REQUIRES
ADDI
TIONAL
ΜOPS
TO
ADJUST
THE
ADDRESS
AND
COMBINE
THE
LOADS
INTO
A
SINGLE
REGISTER
THIS
SAME
FUNCTIONALITY
CAN
BE
OBTAINED
USING
MOVSD
MEM
MOVSD
MEM
UNPCKLPD
WHICH
USES
FEWER
ΜOPS
AND
CAN
BE
PACKED
INTO
THE
TRACE
CACHE
MORE
EFFECTIVELY
THE
LATTER
ALTERNATIVE
HAS
BEEN
FOUND
TO
PROVIDE
A
SEVERAL
PERCENT
PERFORMANCE
IMPROVEMENT
IN
SOME
CASES
ITS
ENCODING
REQUIRES
MORE
INSTRUCTION
BYTES
BUT
THIS
IS
SELDOM
AN
ISSUE
FOR
THE
PENTIUM
PROCESSOR
THE
STORE
VERSION
OF
MOVUPD
IS
COMPLEX
AND
SLOW
SO
MUCH
SO
THAT
THE
SEQUENCE
WITH
TWO
MOVSD
AND
A
UNPCKHPD
SHOULD
ALWAYS
BE
USED
ASSEMBLY
COMPILER
CODING
RULE
ML
IMPACT
L
GENERALITY
INSTEAD
OF
USING
MOVUPD
MEM
FOR
A
UNALIGNED
BIT
LOAD
USE
MOVSD
MEM
MOVSD
MEM
UNPCKLPD
IF
THE
ADDITIONAL
REGISTER
IS
NOT
AVAILABLE
THEN
USE
MOVSD
MEM
MOVHPD
MEM
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
ML
GENERALITY
INSTEAD
OF
USING
MOVUPD
MEM
FOR
A
STORE
USE
MOVSD
MEM
UNPCKHPD
MOVSD
MEM
INSTEAD
VECTORIZATION
THIS
SECTION
PROVIDES
A
BRIEF
SUMMARY
OF
OPTIMIZATION
ISSUES
RELATED
TO
VECTORIZATION
THERE
IS
MORE
DETAIL
IN
THE
CHAPTERS
THAT
FOLLOW
VECTORIZATION
IS
A
PROGRAM
TRANSFORMATION
THAT
ALLOWS
SPECIAL
HARDWARE
TO
PERFORM
THE
SAME
OPERATION
ON
MULTIPLE
DATA
ELEMENTS
AT
THE
SAME
TIME
SUCCESSIVE
PROCESSOR
GENERATIONS
HAVE
PROVIDED
VECTOR
SUPPORT
THROUGH
THE
MMX
TECHNOLOGY
STREAMING
SIMD
EXTENSIONS
SSE
STREAMING
SIMD
EXTENSIONS
STREAMING
SIMD
EXTENSIONS
AND
SUPPLEMENTAL
STREAMING
SIMD
EXTENSIONS
VECTORIZATION
IS
A
SPECIAL
CASE
OF
SIMD
A
TERM
DEFINED
IN
FLYNN
ARCHITECTURE
TAXONOMY
TO
DENOTE
A
SINGLE
INSTRUCTION
STREAM
CAPABLE
OF
OPERATING
ON
MULTIPLE
DATA
ELEMENTS
IN
PARALLEL
THE
NUMBER
OF
ELEMENTS
WHICH
CAN
BE
OPERATED
ON
IN
PARALLEL
RANGE
FROM
FOUR
SINGLE
PRECISION
FLOATING
POINT
DATA
ELEMENTS
IN
STREAMING
SIMD
EXTENSIONS
AND
TWO
DOUBLE
PRECISION
FLOATING
POINT
DATA
ELEMENTS
IN
STREAMING
SIMD
EXTENSIONS
TO
SIXTEEN
BYTE
OPERATIONS
IN
A
BIT
REGISTER
IN
STREAMING
SIMD
EXTENSIONS
THUS
VECTOR
LENGTH
RANGES
FROM
TO
DEPENDING
ON
THE
INSTRUCTION
EXTENSIONS
USED
AND
ON
THE
DATA
TYPE
THE
INTEL
C
COMPILER
SUPPORTS
VECTORIZATION
IN
THREE
WAYS
THE
COMPILER
MAY
BE
ABLE
TO
GENERATE
SIMD
CODE
WITHOUT
INTERVENTION
FROM
THE
USER
THE
CAN
USER
INSERT
PRAGMAS
TO
HELP
THE
COMPILER
REALIZE
THAT
IT
CAN
VECTORIZE
THE
CODE
THE
USER
CAN
WRITE
SIMD
CODE
EXPLICITLY
USING
INTRINSICS
AND
C
CLASSES
TO
HELP
ENABLE
THE
COMPILER
TO
GENERATE
SIMD
CODE
AVOID
GLOBAL
POINTERS
AND
GLOBAL
VARIABLES
THESE
ISSUES
MAY
BE
LESS
TROUBLESOME
IF
ALL
MODULES
ARE
COMPILED
SIMULTA
NEOUSLY
AND
WHOLE
PROGRAM
OPTIMIZATION
IS
USED
USER
SOURCE
CODING
RULE
H
IMPACT
M
GENERALITY
USE
THE
SMALLEST
POSSIBLE
FLOATING
POINT
OR
SIMD
DATA
TYPE
TO
ENABLE
MORE
PARALLELISM
WITH
THE
USE
OF
A
LONGER
SIMD
VECTOR
FOR
EXAMPLE
USE
SINGLE
PRECISION
INSTEAD
OF
DOUBLE
PRECISION
WHERE
POSSIBLE
USER
SOURCE
CODING
RULE
M
IMPACT
ML
GENERALITY
ARRANGE
THE
NESTING
OF
LOOPS
SO
THAT
THE
INNERMOST
NESTING
LEVEL
IS
FREE
OF
INTER
ITERATION
DEPENDENCIES
ESPECIALLY
AVOID
THE
CASE
WHERE
THE
STORE
OF
DATA
IN
AN
EARLIER
ITERATION
HAPPENS
LEXICALLY
AFTER
THE
LOAD
OF
THAT
DATA
IN
A
FUTURE
ITERATION
SOMETHING
WHICH
IS
CALLED
A
LEXICALLY
BACKWARD
DEPENDENCE
THE
INTEGER
PART
OF
THE
SIMD
INSTRUCTION
SET
EXTENSIONS
COVER
BIT
BIT
AND
BIT
OPERANDS
NOT
ALL
SIMD
OPERATIONS
ARE
SUPPORTED
FOR
BITS
MEANING
THAT
SOME
SOURCE
CODE
WILL
NOT
BE
ABLE
TO
BE
VECTORIZED
AT
ALL
UNLESS
SMALLER
OPERANDS
ARE
USED
USER
SOURCE
CODING
RULE
M
IMPACT
ML
GENERALITY
AVOID
THE
USE
OF
CONDITIONAL
BRANCHES
INSIDE
LOOPS
AND
CONSIDER
USING
SSE
INSTRUCTIONS
TO
ELIMINATE
BRANCHES
USER
SOURCE
CODING
RULE
M
IMPACT
ML
GENERALITY
KEEP
INDUCTION
LOOP
VARIABLE
EXPRESSIONS
SIMPLE
OPTIMIZATION
OF
PARTIALLY
VECTORIZABLE
CODE
FREQUENTLY
A
PROGRAM
CONTAINS
A
MIXTURE
OF
VECTORIZABLE
CODE
AND
SOME
ROUTINES
THAT
ARE
NON
VECTORIZABLE
A
COMMON
SITUATION
OF
PARTIALLY
VECTORIZABLE
CODE
INVOLVES
A
LOOP
STRUCTURE
WHICH
INCLUDE
MIXTURES
OF
VECTORIZED
CODE
AND
UNVECTORIZABLE
CODE
THIS
SITUATION
IS
DEPICTED
IN
FIGURE
PACKED
SIMD
INSTRUCTION
UNPACKING
UNVECTORIZABLE
CODE
SERIAL
ROUTINE
PACKING
PACKED
SIMD
INSTRUCTION
FIGURE
GENERIC
PROGRAM
FLOW
OF
PARTIALLY
VECTORIZED
CODE
IT
GENERALLY
CONSISTS
OF
FIVE
STAGES
WITHIN
THE
LOOP
PROLOG
UNPACKING
VECTORIZED
DATA
STRUCTURE
INTO
INDIVIDUAL
ELEMENTS
CALLING
A
NON
VECTORIZABLE
ROUTINE
TO
PROCESS
EACH
ELEMENT
SERIALLY
PACKING
INDIVIDUAL
RESULT
INTO
VECTORIZED
DATA
STRUCTURE
EPILOG
THIS
SECTION
DISCUSSES
TECHNIQUES
THAT
CAN
REDUCE
THE
COST
AND
BOTTLENECK
ASSOCIATED
WITH
THE
PACKING
UNPACKING
STAGES
IN
THESE
PARTIALLY
VECTORIZE
CODE
EXAMPLE
SHOWS
A
REFERENCE
CODE
TEMPLATE
THAT
IS
REPRESENTATIVE
OF
PARTIALLY
VECTORIZABLE
CODING
SITUATIONS
THAT
ALSO
EXPERIENCE
PERFORMANCE
ISSUES
THE
UNVEC
TORIZABLE
PORTION
OF
CODE
IS
REPRESENTED
GENERICALLY
BY
A
SEQUENCE
OF
CALLING
A
SERIAL
FUNCTION
NAMED
FOO
MULTIPLE
TIMES
THIS
GENERIC
EXAMPLE
IS
REFERRED
TO
AS
SHUFFLE
WITH
STORE
FORWARDING
BECAUSE
THE
PROBLEM
GENERALLY
INVOLVES
AN
UNPACKING
STAGE
THAT
SHUFFLES
DATA
ELEMENTS
BETWEEN
REGISTER
AND
MEMORY
FOLLOWED
BY
A
PACKING
STAGE
THAT
CAN
EXPERIENCE
STORE
FORWARDING
ISSUE
THERE
ARE
MORE
THAN
ONE
USEFUL
TECHNIQUES
THAT
CAN
REDUCE
THE
STORE
FORWARDING
BOTTLENECK
BETWEEN
THE
SERIALIZED
PORTION
AND
THE
PACKING
STAGE
THE
FOLLOWING
SUB
SECTIONS
PRESENTS
ALTERNATE
TECHNIQUES
TO
DEAL
WITH
THE
PACKING
UNPACKING
AND
PARAMETER
PASSING
TO
SERIALIZED
FUNCTION
CALLS
EXAMPLE
REFERENCE
CODE
TEMPLATE
FOR
PARTIALLY
VECTORIZABLE
PROGRAM
EXAMPLE
REFERENCE
CODE
TEMPLATE
FOR
PARTIALLY
VECTORIZABLE
PROGRAM
CONTD
ALTERNATE
PACKING
TECHNIQUES
THE
PACKING
METHOD
IMPLEMENTED
IN
THE
REFERENCE
CODE
OF
EXAMPLE
WILL
EXPERI
ENCE
DELAY
AS
IT
ASSEMBLES
DOUBLEWORD
RESULT
FROM
MEMORY
INTO
AN
XMM
REGISTER
DUE
TO
STORE
FORWARDING
RESTRICTIONS
THREE
ALTERNATE
TECHNIQUES
FOR
PACKING
USING
DIFFERENT
SIMD
INSTRUCTION
TO
ASSEMBLE
CONTENTS
IN
XMM
REGISTERS
ARE
SHOWN
IN
EXAMPLE
ALL
THREE
TECHNIQUES
AVOID
STORE
FORWARDING
DELAY
BY
SATISFYING
THE
RESTRICTIONS
ON
DATA
SIZES
BETWEEN
A
PRECEDING
STORE
AND
SUBSEQUENT
LOAD
OPERATIONS
EXAMPLE
THREE
ALTERNATE
PACKING
METHODS
FOR
AVOIDING
STORE
FORWARDING
DIFFICULTY
PACKING
METHOD
PACKING
METHOD
PACKING
METHOD
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
MOVD
EBP
PUNPCKLDQ
PSLLQ
MOVLHPS
PUNPCKLDQ
ORPS
PSLLQ
PUNPCKLDQ
PSLLQ
MOVLHPS
ORPS
ORPS
SIMPLIFYING
RESULT
PASSING
IN
EXAMPLE
INDIVIDUAL
RESULTS
WERE
PASSED
TO
THE
PACKING
STAGE
BY
STORING
TO
CONTIGUOUS
MEMORY
LOCATIONS
INSTEAD
OF
USING
MEMORY
SPILLS
TO
PASS
FOUR
RESULTS
RESULT
PASSING
MAY
BE
ACCOMPLISHED
BY
USING
EITHER
ONE
OR
MORE
REGISTERS
USING
REGISTERS
TO
SIMPLIFY
RESULT
PASSING
AND
REDUCE
MEMORY
SPILLS
CAN
IMPROVE
PERFOR
MANCE
BY
VARYING
DEGREES
DEPENDING
ON
THE
REGISTER
PRESSURE
AT
RUNTIME
EXAMPLE
SHOWS
THE
CODING
SEQUENCE
THAT
USES
FOUR
EXTRA
XMM
REGISTERS
TO
REDUCE
ALL
MEMORY
SPILLS
OF
PASSING
RESULTS
BACK
TO
THE
PARENT
ROUTINE
HOWEVER
SOFT
WARE
MUST
OBSERVE
THE
FOLLOWING
CONDITIONS
WHEN
USING
THIS
TECHNIQUE
THERE
IS
NO
REGISTER
SHORTAGE
IF
THE
LOOP
DOES
NOT
HAVE
MANY
STORES
OR
LOADS
BUT
HAS
MANY
COMPUTATIONS
THIS
TECHNIQUE
DOES
NOT
HELP
PERFORMANCE
THIS
TECHNIQUE
ADDS
WORK
TO
THE
COMPUTA
TIONAL
UNITS
WHILE
THE
STORE
AND
LOADS
PORTS
ARE
IDLE
EXAMPLE
USING
FOUR
REGISTERS
TO
REDUCE
MEMORY
SPILLS
AND
SIMPLIFY
RESULT
PASSING
MOV
EAX
EBP
MOV
EBP
EAX
CALL
FOO
MOVD
EAX
MOV
EAX
EBP
MOV
EBP
EAX
CALL
FOO
MOVD
EAX
MOV
EAX
EBP
MOV
EBP
EAX
CALL
FOO
MOVD
EAX
MOV
EAX
EBP
MOV
EBP
EAX
CALL
FOO
MOVD
EAX
STACK
OPTIMIZATION
IN
EXAMPLE
AN
INPUT
PARAMETER
WAS
COPIED
IN
TURN
ONTO
THE
STACK
AND
PASSED
TO
THE
NON
VECTORIZABLE
ROUTINE
FOR
PROCESSING
THE
PARAMETER
PASSING
FROM
CONSECU
TIVE
MEMORY
LOCATIONS
CAN
BE
SIMPLIFIED
BY
A
TECHNIQUE
SHOWN
IN
EXAMPLE
EXAMPLE
STACK
OPTIMIZATION
TECHNIQUE
TO
SIMPLIFY
PARAMETER
PASSING
CALL
FOO
MOV
EBP
EAX
ADD
EBP
CALL
FOO
MOV
EBP
EAX
EXAMPLE
STACK
OPTIMIZATION
TECHNIQUE
TO
SIMPLIFY
PARAMETER
PASSING
CONTD
ADD
EBP
CALL
FOO
MOV
EBP
EAX
ADD
EBP
CALL
FOO
STACK
OPTIMIZATION
CAN
ONLY
BE
USED
WHEN
THE
SERIAL
OPERATIONS
ARE
FUNCTION
CALLS
THE
FUNCTION
FOO
IS
DECLARED
AS
INT
FOO
INT
A
THE
PARAMETER
IS
PASSED
ON
THE
STACK
THE
ORDER
OF
OPERATION
ON
THE
COMPONENTS
IS
FROM
LAST
TO
FIRST
NOTE
THE
CALL
TO
FOO
AND
THE
ADVANCE
OF
EDP
WHEN
PASSING
THE
VECTOR
ELEMENTS
TO
FOO
ONE
BY
ONE
FROM
LAST
TO
FIRST
TUNING
CONSIDERATIONS
TUNING
CONSIDERATIONS
FOR
SITUATIONS
REPRESENTED
BY
LOOPING
OF
EXAMPLE
INCLUDE
APPLYING
ONE
OF
MORE
OF
THE
FOLLOWING
COMBINATIONS
CHOOSE
AN
ALTERNATE
PACKING
TECHNIQUE
CONSIDER
A
TECHNIQUE
TO
SIMPLY
RESULT
PASSING
CONSIDER
THE
STACK
OPTIMIZATION
TECHNIQUE
TO
SIMPLIFY
PARAMETER
PASSING
MINIMIZING
THE
AVERAGE
NUMBER
OF
CYCLES
TO
EXECUTE
ONE
ITERATION
OF
THE
LOOP
MINIMIZING
THE
PER
ITERATION
COST
OF
THE
UNPACKING
AND
PACKING
OPERATIONS
THE
SPEED
IMPROVEMENT
BY
USING
THE
TECHNIQUES
DISCUSSED
IN
THIS
SECTION
WILL
VARY
DEPENDING
ON
THE
CHOICE
OF
COMBINATIONS
IMPLEMENTED
AND
CHARACTERISTICS
OF
THE
NON
VECTORIZABLE
ROUTINE
FOR
EXAMPLE
IF
THE
ROUTINE
FOO
IS
SHORT
REPRESENTATIVE
OF
TIGHT
SHORT
LOOPS
THE
PER
ITERATION
COST
OF
UNPACKING
PACKING
TEND
TO
BE
SMALLER
THAN
SITUATIONS
WHERE
THE
NON
VECTORIZABLE
CODE
CONTAIN
LONGER
OPERATION
OR
MANY
DEPENDENCIES
THIS
IS
BECAUSE
MANY
ITERATIONS
OF
SHORT
TIGHT
LOOP
CAN
BE
IN
FLIGHT
IN
THE
EXECUTION
CORE
SO
THE
PER
ITERATION
COST
OF
PACKING
AND
UNPACKING
IS
ONLY
PARTIALLY
EXPOSED
AND
APPEAR
TO
CAUSE
VERY
LITTLE
PERFORMANCE
DEGRADATION
EVALUATION
OF
THE
PER
ITERATION
COST
OF
PACKING
UNPACKING
SHOULD
BE
CARRIED
OUT
IN
A
METHODICAL
MANNER
OVER
A
SELECTED
NUMBER
OF
TEST
CASES
WHERE
EACH
CASE
MAY
IMPLEMENT
SOME
COMBINATION
OF
THE
TECHNIQUES
DISCUSSED
IN
THIS
SECTION
THE
PER
ITERATION
COST
CAN
BE
ESTIMATED
BY
EVALUATING
THE
AVERAGE
CYCLES
TO
EXECUTE
ONE
ITERATION
OF
THE
TEST
CASE
EVALUATING
THE
AVERAGE
CYCLES
TO
EXECUTE
ONE
ITERATION
OF
A
BASE
LINE
LOOP
SEQUENCE
OF
NON
VECTORIZABLE
CODE
EXAMPLE
SHOWS
THE
BASE
LINE
CODE
SEQUENCE
THAT
CAN
BE
USED
TO
ESTIMATE
THE
AVERAGE
COST
OF
A
LOOP
THAT
EXECUTES
NON
VECTORIZABLE
ROUTINES
EXAMPLE
BASE
LINE
CODE
SEQUENCE
TO
ESTIMATE
LOOP
OVERHEAD
PUSH
EBP
MOV
EBP
ESP
SUB
EBP
MOV
EBP
EDI
CALL
FOO
MOV
EBP
EDI
CALL
FOO
MOV
EBP
EDI
CALL
FOO
MOV
EBP
EDI
CALL
FOO
ADD
EBP
POP
EBP
RET
THE
AVERAGE
PER
ITERATION
COST
OF
PACKING
UNPACKING
CAN
BE
DERIVED
FROM
MEASURING
THE
EXECUTION
TIMES
OF
A
LARGE
NUMBER
OF
ITERATIONS
BY
CYCLES
TO
RUN
TESTCASE
CYCLES
TO
RUN
EQUIVALENT
BASELINE
SEQUENCE
ITERATION
COUNT
FOR
EXAMPLE
USING
A
SIMPLE
FUNCTION
THAT
RETURNS
AN
INPUT
PARAMETER
REPRESENTATIVE
OF
TIGHT
SHORT
LOOPS
THE
PER
ITERATION
COST
OF
PACKING
UNPACKING
MAY
RANGE
FROM
SLIGHTLY
MORE
THAN
CYCLES
THE
SHUFFLE
WITH
STORE
FORWARDING
CASE
EXAMPLE
TO
CYCLES
ACCOMPLISHED
BY
SEVERAL
TEST
CASES
ACROSS
TEST
CASES
CONSISTING
OF
ONE
OF
THE
ALTERNATE
PACKING
METHODS
NO
RESULT
SIMPLIFICATION
SIMPLIFICATION
OF
EITHER
OR
RESULTS
NO
STACK
OPTIMIZATION
OR
WITH
STACK
OPTIMIZATION
THE
AVERAGE
PER
ITER
ATION
COST
OF
PACKING
UNPACKING
IS
ABOUT
CYCLES
GENERALLY
SPEAKING
PACKING
METHOD
AND
SEE
EXAMPLE
TEND
TO
BE
MORE
ROBUST
THAN
PACKING
METHOD
THE
OPTIMAL
CHOICE
OF
SIMPLIFYING
OR
RESULTS
WILL
BE
AFFECTED
BY
REGISTER
PRESSURE
OF
THE
RUNTIME
AND
OTHER
RELEVANT
MICROARCHITECTURAL
CONDITIONS
NOTE
THAT
THE
NUMERIC
DISCUSSION
OF
PER
ITERATION
COST
OF
PACKING
PACKING
IS
ILLUSTRA
TIVE
ONLY
IT
WILL
VARY
WITH
TEST
CASES
USING
A
DIFFERENT
BASE
LINE
CODE
SEQUENCE
AND
WILL
GENERALLY
INCREASE
IF
THE
NON
VECTORIZABLE
ROUTINE
REQUIRES
LONGER
TIME
TO
EXECUTE
BECAUSE
THE
NUMBER
OF
LOOP
ITERATIONS
THAT
CAN
RESIDE
IN
FLIGHT
IN
THE
EXECUTION
CORE
DECREASES
OPTIMIZING
MEMORY
ACCESSES
THIS
SECTION
DISCUSSES
GUIDELINES
FOR
OPTIMIZING
CODE
AND
DATA
MEMORY
ACCESSES
THE
MOST
IMPORTANT
RECOMMENDATIONS
ARE
EXECUTE
LOAD
AND
STORE
OPERATIONS
WITHIN
AVAILABLE
EXECUTION
BANDWIDTH
ENABLE
FORWARD
PROGRESS
OF
SPECULATIVE
EXECUTION
ENABLE
STORE
FORWARDING
TO
PROCEED
ALIGN
DATA
PAYING
ATTENTION
TO
DATA
LAYOUT
AND
STACK
ALIGNMENT
PLACE
CODE
AND
DATA
ON
SEPARATE
PAGES
ENHANCE
DATA
LOCALITY
USE
PREFETCHING
AND
CACHEABILITY
CONTROL
INSTRUCTIONS
ENHANCE
CODE
LOCALITY
AND
ALIGN
BRANCH
TARGETS
TAKE
ADVANTAGE
OF
WRITE
COMBINING
ALIGNMENT
AND
FORWARDING
PROBLEMS
ARE
AMONG
THE
MOST
COMMON
SOURCES
OF
LARGE
DELAYS
ON
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
LOAD
AND
STORE
EXECUTION
BANDWIDTH
TYPICALLY
LOADS
AND
STORES
ARE
THE
MOST
FREQUENT
OPERATIONS
IN
A
WORKLOAD
UP
TO
OF
THE
INSTRUCTIONS
IN
A
WORKLOAD
CARRYING
LOAD
OR
STORE
INTENT
ARE
NOT
UNCOMMON
EACH
GENERATION
OF
MICROARCHITECTURE
PROVIDES
MULTIPLE
BUFFERS
TO
SUPPORT
EXECUTING
LOAD
AND
STORE
OPERATIONS
WHILE
THERE
ARE
INSTRUCTIONS
IN
FLIGHT
SOFTWARE
CAN
MAXIMIZE
MEMORY
PERFORMANCE
BY
NOT
EXCEEDING
THE
ISSUE
OR
BUFFERING
LIMITATIONS
OF
THE
MACHINE
IN
THE
INTEL
CORE
MICROARCHITECTURE
ONLY
STORES
AND
LOADS
MAY
BE
IN
FLIGHT
AT
ONCE
SINCE
ONLY
ONE
LOAD
CAN
ISSUE
PER
CYCLE
ALGORITHMS
WHICH
OPERATE
ON
TWO
ARRAYS
ARE
CONSTRAINED
TO
ONE
OPERATION
EVERY
OTHER
CYCLE
UNLESS
YOU
USE
PROGRAMMING
TRICKS
TO
REDUCE
THE
AMOUNT
OF
MEMORY
USAGE
INTEL
NETBURST
MICROARCHITECTURE
HAS
THE
SAME
NUMBER
OF
STORE
BUFFERS
SLIGHTLY
MORE
LOAD
BUFFERS
AND
SIMILAR
THROUGHPUT
OF
ISSUING
LOAD
OPERATIONS
INTEL
CORE
DUO
AND
INTEL
CORE
SOLO
PROCESSORS
HAVE
LESS
BUFFERS
NEVERTHELESS
THE
GENERAL
HEURISTIC
APPLIES
TO
ALL
OF
THEM
ENHANCE
SPECULATIVE
EXECUTION
AND
MEMORY
DISAMBIGUATION
PRIOR
TO
INTEL
CORE
MICROARCHITECTURE
WHEN
CODE
CONTAINS
BOTH
STORES
AND
LOADS
THE
LOADS
CANNOT
BE
ISSUED
BEFORE
THE
ADDRESS
OF
THE
STORE
IS
RESOLVED
THIS
RULE
ENSURES
CORRECT
HANDLING
OF
LOAD
DEPENDENCIES
ON
PRECEDING
STORES
THE
INTEL
CORE
MICROARCHITECTURE
CONTAINS
A
MECHANISM
THAT
ALLOWS
SOME
LOADS
TO
BE
ISSUED
EARLY
SPECULATIVELY
THE
PROCESSOR
LATER
CHECKS
IF
THE
LOAD
ADDRESS
OVERLAPS
WITH
A
STORE
IF
THE
ADDRESSES
DO
OVERLAP
THEN
THE
PROCESSOR
RE
EXECUTES
THE
INSTRUC
TIONS
EXAMPLE
ILLUSTRATES
A
SITUATION
THAT
THE
COMPILER
CANNOT
BE
SURE
THAT
PTR
ARRAY
DOES
NOT
CHANGE
DURING
THE
LOOP
THEREFORE
THE
COMPILER
CANNOT
KEEP
PTR
ARRAY
IN
A
REGISTER
AS
AN
INVARIANT
AND
MUST
READ
IT
AGAIN
IN
EVERY
ITERATION
ALTHOUGH
THIS
SITUATION
CAN
BE
FIXED
IN
SOFTWARE
BY
A
REWRITING
THE
CODE
TO
REQUIRE
THE
ADDRESS
OF
THE
POINTER
IS
INVARIANT
MEMORY
DISAMBIGUATION
PROVIDES
PERFORMANCE
GAIN
WITHOUT
REWRITING
THE
CODE
EXAMPLE
LOADS
BLOCKED
BY
STORES
OF
UNKNOWN
ADDRESS
C
CODE
ASSEMBLY
SEQUENCE
STRUCT
AA
AA
ARRAY
VOID
AA
PTR
DWORD
INDEX
AA
THISPTR
WHILE
PTR
ARRAY
INDEX
THISPTR
PTR
ARRAY
INDEX
NULL
MOV
DWORD
PTR
EAX
MOV
EDX
DWORD
PTR
EDI
SUB
ECX
CMP
DWORD
PTR
ECX
EDX
ESI
LEA
EAX
ECX
EDX
JNE
ALIGNMENT
ALIGNMENT
OF
DATA
CONCERNS
ALL
KINDS
OF
VARIABLES
DYNAMICALLY
ALLOCATED
VARIABLES
MEMBERS
OF
A
DATA
STRUCTURE
GLOBAL
OR
LOCAL
VARIABLES
PARAMETERS
PASSED
ON
THE
STACK
MISALIGNED
DATA
ACCESS
CAN
INCUR
SIGNIFICANT
PERFORMANCE
PENALTIES
THIS
IS
PARTICU
LARLY
TRUE
FOR
CACHE
LINE
SPLITS
THE
SIZE
OF
A
CACHE
LINE
IS
BYTES
IN
THE
PENTIUM
AND
OTHER
RECENT
INTEL
PROCESSORS
INCLUDING
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHI
TECTURE
AN
ACCESS
TO
DATA
UNALIGNED
ON
BYTE
BOUNDARY
LEADS
TO
TWO
MEMORY
ACCESSES
AND
REQUIRES
SEVERAL
ΜOPS
TO
BE
EXECUTED
INSTEAD
OF
ONE
ACCESSES
THAT
SPAN
BYTE
BOUNDARIES
ARE
LIKELY
TO
INCUR
A
LARGE
PERFORMANCE
PENALTY
THE
COST
OF
EACH
STALL
GENERALLY
ARE
GREATER
ON
MACHINES
WITH
LONGER
PIPELINES
DOUBLE
PRECISION
FLOATING
POINT
OPERANDS
THAT
ARE
EIGHT
BYTE
ALIGNED
HAVE
BETTER
PERFORMANCE
THAN
OPERANDS
THAT
ARE
NOT
EIGHT
BYTE
ALIGNED
SINCE
THEY
ARE
LESS
LIKELY
TO
INCUR
PENALTIES
FOR
CACHE
AND
MOB
SPLITS
FLOATING
POINT
OPERATION
ON
A
MEMORY
OPERANDS
REQUIRE
THAT
THE
OPERAND
BE
LOADED
FROM
MEMORY
THIS
INCURS
AN
ADDITIONAL
ΜOP
WHICH
CAN
HAVE
A
MINOR
NEGATIVE
IMPACT
ON
FRONT
END
BANDWIDTH
ADDITIONALLY
MEMORY
OPERANDS
MAY
CAUSE
A
DATA
CACHE
MISS
CAUSING
A
PENALTY
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
H
GENERALITY
ALIGN
DATA
ON
NATURAL
OPERAND
SIZE
ADDRESS
BOUNDARIES
IF
THE
DATA
WILL
BE
ACCESSED
WITH
VECTOR
INSTRUCTION
LOADS
AND
STORES
ALIGN
THE
DATA
ON
BYTE
BOUNDARIES
FOR
BEST
PERFORMANCE
ALIGN
DATA
AS
FOLLOWS
ALIGN
BIT
DATA
AT
ANY
ADDRESS
ALIGN
BIT
DATA
TO
BE
CONTAINED
WITHIN
AN
ALIGNED
BYTE
WORD
ALIGN
BIT
DATA
SO
THAT
ITS
BASE
ADDRESS
IS
A
MULTIPLE
OF
FOUR
ALIGN
BIT
DATA
SO
THAT
ITS
BASE
ADDRESS
IS
A
MULTIPLE
OF
EIGHT
ALIGN
BIT
DATA
SO
THAT
ITS
BASE
ADDRESS
IS
A
MULTIPLE
OF
SIXTEEN
ALIGN
BIT
DATA
SO
THAT
ITS
BASE
ADDRESS
IS
A
MULTIPLE
OF
SIXTEEN
A
BYTE
OR
GREATER
DATA
STRUCTURE
OR
ARRAY
SHOULD
BE
ALIGNED
SO
THAT
ITS
BASE
ADDRESS
IS
A
MULTIPLE
OF
SORTING
DATA
IN
DECREASING
SIZE
ORDER
IS
ONE
HEURISTIC
FOR
ASSISTING
WITH
NATURAL
ALIGNMENT
AS
LONG
AS
BYTE
BOUNDARIES
AND
CACHE
LINES
ARE
NEVER
CROSSED
NATURAL
ALIGNMENT
IS
NOT
STRICTLY
NECESSARY
THOUGH
IT
IS
AN
EASY
WAY
TO
ENFORCE
THIS
EXAMPLE
SHOWS
THE
TYPE
OF
CODE
THAT
CAN
CAUSE
A
CACHE
LINE
SPLIT
THE
CODE
LOADS
THE
ADDRESSES
OF
TWO
DWORD
ARRAYS
IS
NOT
A
BYTE
ALIGNED
ADDRESS
SO
A
BYTE
ACCESS
AT
THIS
ADDRESS
WILL
GET
BYTES
FROM
THE
CACHE
LINE
THIS
ADDRESS
IS
CONTAINED
IN
AND
BYTES
FROM
THE
CACHE
LINE
THAT
STARTS
AT
ON
PROCESSORS
WITH
BYTE
CACHE
LINES
A
SIMILAR
CACHE
LINE
SPLIT
WILL
OCCUR
EVERY
ITER
ATIONS
EXAMPLE
CODE
THAT
CAUSES
CACHE
LINE
SPLIT
MOV
ESI
MOV
EDI
BLOCKMOVE
MOV
EAX
DWORD
PTR
ESI
MOV
EBX
DWORD
PTR
ESI
MOV
DWORD
PTR
EDI
EAX
MOV
DWORD
PTR
EDI
EBX
ADD
ESI
ADD
EDI
SUB
EDX
JNZ
BLOCKMOVE
FIGURE
ILLUSTRATES
THE
SITUATION
OF
ACCESSING
A
DATA
ELEMENT
THAT
SPAN
ACROSS
CACHE
LINE
BOUNDARIES
FIGURE
CACHE
LINE
SPLIT
IN
ACCESSING
ELEMENTS
IN
A
ARRAY
ALIGNMENT
OF
CODE
IS
LESS
IMPORTANT
FOR
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROAR
CHITECTURE
ALIGNMENT
OF
BRANCH
TARGETS
TO
MAXIMIZE
BANDWIDTH
OF
FETCHING
CACHED
INSTRUCTIONS
IS
AN
ISSUE
ONLY
WHEN
NOT
EXECUTING
OUT
OF
THE
TRACE
CACHE
ALIGNMENT
OF
CODE
CAN
BE
AN
ISSUE
FOR
THE
PENTIUM
M
INTEL
CORE
DUO
AND
INTEL
CORE
DUO
PROCESSORS
ALIGNMENT
OF
BRANCH
TARGETS
WILL
IMPROVE
DECODER
THROUGHPUT
STORE
FORWARDING
THE
PROCESSOR
MEMORY
SYSTEM
ONLY
SENDS
STORES
TO
MEMORY
INCLUDING
CACHE
AFTER
STORE
RETIREMENT
HOWEVER
STORE
DATA
CAN
BE
FORWARDED
FROM
A
STORE
TO
A
SUBSEQUENT
LOAD
FROM
THE
SAME
ADDRESS
TO
GIVE
A
MUCH
SHORTER
STORE
LOAD
LATENCY
THERE
ARE
TWO
KINDS
OF
REQUIREMENTS
FOR
STORE
FORWARDING
IF
THESE
REQUIREMENTS
ARE
VIOLATED
STORE
FORWARDING
CANNOT
OCCUR
AND
THE
LOAD
MUST
GET
ITS
DATA
FROM
THE
CACHE
SO
THE
STORE
MUST
WRITE
ITS
DATA
BACK
TO
THE
CACHE
FIRST
THIS
INCURS
A
PENALTY
THAT
IS
LARGELY
RELATED
TO
PIPELINE
DEPTH
OF
THE
UNDERLYING
MICRO
ARCHITECTURE
THE
FIRST
REQUIREMENT
PERTAINS
TO
THE
SIZE
AND
ALIGNMENT
OF
THE
STORE
FORWARDING
DATA
THIS
RESTRICTION
IS
LIKELY
TO
HAVE
HIGH
IMPACT
ON
OVERALL
APPLICATION
PERFORMANCE
TYPI
CALLY
A
PERFORMANCE
PENALTY
DUE
TO
VIOLATING
THIS
RESTRICTION
CAN
BE
PREVENTED
THE
STORE
TO
LOAD
FORWARDING
RESTRICTIONS
VARY
FROM
ONE
MICROARCHITECTURE
TO
ANOTHER
SEVERAL
EXAMPLES
OF
CODING
PITFALLS
THAT
CAUSE
STORE
FORWARDING
STALLS
AND
SOLUTIONS
TO
THESE
PITFALLS
ARE
DISCUSSED
IN
DETAIL
IN
SECTION
STORE
TO
LOAD
FORWARDING
RESTRICTION
ON
SIZE
AND
ALIGNMENT
THE
SECOND
REQUIREMENT
IS
THE
AVAILABILITY
OF
DATA
DISCUSSED
IN
SECTION
STORE
FORWARDING
RESTRICTION
ON
DATA
AVAIL
ABILITY
A
GOOD
PRACTICE
IS
TO
ELIMINATE
REDUNDANT
LOAD
OPERATIONS
IT
MAY
BE
POSSIBLE
TO
KEEP
A
TEMPORARY
SCALAR
VARIABLE
IN
A
REGISTER
AND
NEVER
WRITE
IT
TO
MEMORY
GENERALLY
SUCH
A
VARIABLE
MUST
NOT
BE
ACCESSIBLE
USING
INDIRECT
POINTERS
MOVING
A
VARIABLE
TO
A
REGISTER
ELIMINATES
ALL
LOADS
AND
STORES
OF
THAT
VARIABLE
AND
ELIMINATES
POTENTIAL
PROBLEMS
ASSOCIATED
WITH
STORE
FORWARDING
HOWEVER
IT
ALSO
INCREASES
REGISTER
PRESSURE
LOAD
INSTRUCTIONS
TEND
TO
START
CHAINS
OF
COMPUTATION
SINCE
THE
OUT
OF
ORDER
ENGINE
IS
BASED
ON
DATA
DEPENDENCE
LOAD
INSTRUCTIONS
PLAY
A
SIGNIFICANT
ROLE
IN
THE
ENGINE
ABILITY
TO
EXECUTE
AT
A
HIGH
RATE
ELIMINATING
LOADS
SHOULD
BE
GIVEN
A
HIGH
PRIORITY
IF
A
VARIABLE
DOES
NOT
CHANGE
BETWEEN
THE
TIME
WHEN
IT
IS
STORED
AND
THE
TIME
WHEN
IT
IS
USED
AGAIN
THE
REGISTER
THAT
WAS
STORED
CAN
BE
COPIED
OR
USED
DIRECTLY
IF
REGISTER
PRESSURE
IS
TOO
HIGH
OR
AN
UNSEEN
FUNCTION
IS
CALLED
BEFORE
THE
STORE
AND
THE
SECOND
LOAD
IT
MAY
NOT
BE
POSSIBLE
TO
ELIMINATE
THE
SECOND
LOAD
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
PASS
PARAMETERS
IN
REGISTERS
INSTEAD
OF
ON
THE
STACK
WHERE
POSSIBLE
PASSING
ARGUMENTS
ON
THE
STACK
REQUIRES
A
STORE
FOLLOWED
BY
A
RELOAD
WHILE
THIS
SEQUENCE
IS
OPTIMIZED
IN
HARDWARE
BY
PROVIDING
THE
VALUE
TO
THE
LOAD
DIRECTLY
FROM
THE
MEMORY
ORDER
BUFFER
WITHOUT
THE
NEED
TO
ACCESS
THE
DATA
CACHE
IF
PERMITTED
BY
STORE
FORWARDING
RESTRICTIONS
FLOATING
POINT
VALUES
INCUR
A
SIGNIFICANT
LATENCY
IN
FORWARDING
PASSING
FLOATING
POINT
ARGUMENTS
IN
PREFERABLY
XMM
REGISTERS
SHOULD
SAVE
THIS
LONG
LATENCY
OPERATION
PARAMETER
PASSING
CONVENTIONS
MAY
LIMIT
THE
CHOICE
OF
WHICH
PARAMETERS
ARE
PASSED
IN
REGISTERS
WHICH
ARE
PASSED
ON
THE
STACK
HOWEVER
THESE
LIMITATIONS
MAY
BE
OVER
COME
IF
THE
COMPILER
HAS
CONTROL
OF
THE
COMPILATION
OF
THE
WHOLE
BINARY
USING
WHOLE
PROGRAM
OPTIMIZATION
STORE
TO
LOAD
FORWARDING
RESTRICTION
ON
SIZE
AND
ALIGNMENT
DATA
SIZE
AND
ALIGNMENT
RESTRICTIONS
FOR
STORE
FORWARDING
APPLY
TO
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
INTEL
CORE
MICROARCHITECTURE
INTEL
CORE
DUO
INTEL
CORE
SOLO
AND
PENTIUM
M
PROCESSORS
THE
PERFORMANCE
PENALTY
FOR
VIOLATING
STORE
FORWARDING
RESTRICTIONS
IS
LESS
FOR
SHORTER
PIPELINED
MACHINES
THAN
FOR
INTEL
NETBURST
MICROARCHITECTURE
STORE
FORWARDING
RESTRICTIONS
VARY
WITH
EACH
MICROARCHITECTURE
INTEL
NETBURST
MICROARCHITECTURE
PLACES
MORE
CONSTRAINTS
THAN
INTEL
CORE
MICROARCHITECTURE
ON
CODE
GENERATION
TO
ENABLE
STORE
FORWARDING
TO
MAKE
PROGRESS
INSTEAD
OF
EXPERIENCING
STALLS
FIXING
STORE
FORWARDING
PROBLEMS
FOR
INTEL
NETBURST
MICROARCHITECTURE
GENER
ALLY
ALSO
AVOIDS
PROBLEMS
ON
PENTIUM
M
INTEL
CORE
DUO
AND
INTEL
CORE
DUO
PROCES
SORS
THE
SIZE
AND
ALIGNMENT
RESTRICTIONS
FOR
STORE
FORWARDING
IN
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
ARE
ILLUSTRATED
IN
FIGURE
FIGURE
SIZE
AND
ALIGNMENT
RESTRICTIONS
IN
STORE
FORWARDING
THE
FOLLOWING
RULES
HELP
SATISFY
SIZE
AND
ALIGNMENT
RESTRICTIONS
FOR
STORE
FORWARDING
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
A
LOAD
THAT
FORWARDS
FROM
A
STORE
MUST
HAVE
THE
SAME
ADDRESS
START
POINT
AND
THEREFORE
THE
SAME
ALIGNMENT
AS
THE
STORE
DATA
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
THE
DATA
OF
A
LOAD
WHICH
IS
FORWARDED
FROM
A
STORE
MUST
BE
COMPLETELY
CONTAINED
WITHIN
THE
STORE
DATA
A
LOAD
THAT
FORWARDS
FROM
A
STORE
MUST
WAIT
FOR
THE
STORE
DATA
TO
BE
WRITTEN
TO
THE
STORE
BUFFER
BEFORE
PROCEEDING
BUT
OTHER
UNRELATED
LOADS
NEED
NOT
WAIT
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
ML
GENERALITY
IF
IT
IS
NECESSARY
TO
EXTRACT
A
NON
ALIGNED
PORTION
OF
STORED
DATA
READ
OUT
THE
SMALLEST
ALIGNED
PORTION
THAT
COMPLETELY
CONTAINS
THE
DATA
AND
SHIFT
MASK
THE
DATA
AS
NECESSARY
THIS
IS
BETTER
THAN
INCURRING
THE
PENALTIES
OF
A
FAILED
STORE
FORWARD
ASSEMBLY
COMPILER
CODING
RULE
MH
IMPACT
ML
GENERALITY
AVOID
SEVERAL
SMALL
LOADS
AFTER
LARGE
STORES
TO
THE
SAME
AREA
OF
MEMORY
BY
USING
A
SINGLE
LARGE
READ
AND
REGISTER
COPIES
AS
NEEDED
EXAMPLE
DEPICTS
SEVERAL
STORE
FORWARDING
SITUATIONS
IN
WHICH
SMALL
LOADS
FOLLOW
LARGE
STORES
THE
FIRST
THREE
LOAD
OPERATIONS
ILLUSTRATE
THE
SITUATIONS
DESCRIBED
IN
RULE
HOWEVER
THE
LAST
LOAD
OPERATION
GETS
DATA
FROM
STORE
FORWARDING
WITHOUT
PROBLEM
EXAMPLE
SITUATIONS
SHOWING
SMALL
LOADS
AFTER
LARGE
STORE
EXAMPLE
ILLUSTRATES
A
STORE
FORWARDING
SITUATION
IN
WHICH
A
LARGE
LOAD
FOLLOWS
SEVERAL
SMALL
STORES
THE
DATA
NEEDED
BY
THE
LOAD
OPERATION
CANNOT
BE
FORWARDED
BECAUSE
ALL
OF
THE
DATA
THAT
NEEDS
TO
BE
FORWARDED
IS
NOT
CONTAINED
IN
THE
STORE
BUFFER
AVOID
LARGE
LOADS
AFTER
SMALL
STORES
TO
THE
SAME
AREA
OF
MEMORY
EXAMPLE
NON
FORWARDING
EXAMPLE
OF
LARGE
LOAD
AFTER
SMALL
STORE
MOV
EBP
A
MOV
EBP
B
MOV
EBP
C
MOV
EBP
D
MOV
EAX
EBP
BLOCKED
THE
FIRST
SMALL
STORE
CAN
BE
CONSOLIDATED
INTO
A
SINGLE
DWORD
STORE
TO
PREVENT
THIS
NON
FORWARDING
SITUATION
EXAMPLE
ILLUSTRATES
A
STALLED
STORE
FORWARDING
SITUATION
THAT
MAY
APPEAR
IN
COMPILER
GENERATED
CODE
SOMETIMES
A
COMPILER
GENERATES
CODE
SIMILAR
TO
THAT
SHOWN
IN
EXAMPLE
TO
HANDLE
A
SPILLED
BYTE
TO
THE
STACK
AND
CONVERT
THE
BYTE
TO
AN
INTEGER
VALUE
EXAMPLE
A
NON
FORWARDING
SITUATION
IN
COMPILER
GENERATED
CODE
MOV
DWORD
PTR
ESP
MOV
BYTE
PTR
ESP
BL
MOV
EAX
DWORD
PTR
ESP
STALL
AND
EAX
CONVERTING
BACK
TO
BYTE
VALUE
EXAMPLE
OFFERS
TWO
ALTERNATIVES
TO
AVOID
THE
NON
FORWARDING
SITUATION
SHOWN
IN
EXAMPLE
EXAMPLE
TWO
WAYS
TO
AVOID
NON
FORWARDING
SITUATION
IN
EXAMPLE
A
USE
MOVZ
INSTRUCTION
TO
AVOID
LARGE
LOAD
AFTER
SMALL
STORE
WHEN
SPILLS
ARE
IGNORED
MOVZ
EAX
BL
REPLACES
THE
LAST
THREE
INSTRUCTIONS
B
USE
MOVZ
INSTRUCTION
AND
HANDLE
SPILLS
TO
THE
STACK
MOV
DWORD
PTR
ESP
MOV
BYTE
PTR
ESP
BL
MOVZ
EAX
BYTE
PTR
ESP
NOT
BLOCKED
WHEN
MOVING
DATA
THAT
IS
SMALLER
THAN
BITS
BETWEEN
MEMORY
LOCATIONS
BIT
OR
BIT
SIMD
REGISTER
MOVES
ARE
MORE
EFFICIENT
IF
ALIGNED
AND
CAN
BE
USED
TO
AVOID
UNALIGNED
LOADS
ALTHOUGH
FLOATING
POINT
REGISTERS
ALLOW
THE
MOVEMENT
OF
BITS
AT
A
TIME
FLOATING
POINT
INSTRUCTIONS
SHOULD
NOT
BE
USED
FOR
THIS
PURPOSE
AS
DATA
MAY
BE
INADVERTENTLY
MODIFIED
AS
AN
ADDITIONAL
EXAMPLE
CONSIDER
THE
CASES
IN
EXAMPLE
EXAMPLE
LARGE
AND
SMALL
LOAD
STALLS
A
LARGE
LOAD
STALL
MOV
MEM
EAX
STORE
DWORD
TO
ADDRESS
MEM
MOV
MEM
EBX
STORE
DWORD
TO
ADDRESS
MEM
FLD
MEM
LOAD
QWORD
AT
ADDRESS
MEM
STALLS
B
SMALL
LOAD
STALL
FSTP
MEM
STORE
QWORD
TO
ADDRESS
MEM
MOV
BX
MEM
LOAD
WORD
AT
ADDRESS
MEM
STALLS
MOV
CX
MEM
LOAD
WORD
AT
ADDRESS
MEM
STALLS
IN
THE
FIRST
CASE
A
THERE
IS
A
LARGE
LOAD
AFTER
A
SERIES
OF
SMALL
STORES
TO
THE
SAME
AREA
OF
MEMORY
BEGINNING
AT
MEMORY
ADDRESS
MEM
THE
LARGE
LOAD
WILL
STALL
THE
FLD
MUST
WAIT
FOR
THE
STORES
TO
WRITE
TO
MEMORY
BEFORE
IT
CAN
ACCESS
ALL
THE
DATA
IT
REQUIRES
THIS
STALL
CAN
ALSO
OCCUR
WITH
OTHER
DATA
TYPES
FOR
EXAMPLE
WHEN
BYTES
OR
WORDS
ARE
STORED
AND
THEN
WORDS
OR
DOUBLEWORDS
ARE
READ
FROM
THE
SAME
AREA
OF
MEMORY
IN
THE
SECOND
CASE
B
THERE
IS
A
SERIES
OF
SMALL
LOADS
AFTER
A
LARGE
STORE
TO
THE
SAME
AREA
OF
MEMORY
BEGINNING
AT
MEMORY
ADDRESS
MEM
THE
SMALL
LOADS
WILL
STALL
THE
WORD
LOADS
MUST
WAIT
FOR
THE
QUADWORD
STORE
TO
WRITE
TO
MEMORY
BEFORE
THEY
CAN
ACCESS
THE
DATA
THEY
REQUIRE
THIS
STALL
CAN
ALSO
OCCUR
WITH
OTHER
DATA
TYPES
FOR
EXAMPLE
WHEN
DOUBLEWORDS
OR
WORDS
ARE
STORED
AND
THEN
WORDS
OR
BYTES
ARE
READ
FROM
THE
SAME
AREA
OF
MEMORY
THIS
CAN
BE
AVOIDED
BY
MOVING
THE
STORE
AS
FAR
FROM
THE
LOADS
AS
POSSIBLE
STORE
FORWARDING
RESTRICTIONS
FOR
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
IS
LISTED
IN
TABLE
TABLE
STORE
FORWARDING
RESTRICTIONS
OF
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
STORE
ALIGNMENT
WIDTH
OF
STORE
BITS
LOAD
ALIGNMENT
BYTE
WIDTH
OF
LOAD
BITS
STORE
FORWARDING
RESTRICTION
TO
NATURAL
SIZE
WORD
ALIGNED
NOT
STALLED
TO
NATURAL
SIZE
NOT
WORD
ALIGNED
STALLED
TO
NATURAL
SIZE
DWORD
ALIGNED
NOT
STALLED
TO
NATURAL
SIZE
NOT
DWORD
ALIGNED
STALLED
TO
NATURAL
SIZE
WORD
ALIGNED
NOT
STALLED
TO
NATURAL
SIZE
NOT
WORD
ALIGNED
STALLED
TO
NATURAL
SIZE
QWORD
ALIGNED
NOT
STALLED
TABLE
STORE
FORWARDING
RESTRICTIONS
OF
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
CONTD
STORE
ALIGNMENT
WIDTH
OF
STORE
BITS
LOAD
ALIGNMENT
BYTE
WIDTH
OF
LOAD
BITS
STORE
FORWARDING
RESTRICTION
TO
NATURAL
SIZE
NOT
QWORD
ALIGNED
STALLED
TO
NATURAL
SIZE
DWORD
ALIGNED
NOT
STALLED
TO
NATURAL
SIZE
NOT
DWORD
ALIGNED
STALLED
TO
NATURAL
SIZE
DQWORD
ALIGNED
NOT
STALLED
TO
NATURAL
SIZE
NOT
DQWORD
ALIGNED
STALLED
TO
NATURAL
SIZE
DWORD
ALIGNED
NOT
STALLED
TO
NATURAL
SIZE
NOT
DWORD
ALIGNED
STALLED
TO
NATURAL
SIZE
QWORD
ALIGNED
NOT
STALLED
TO
NATURAL
SIZE
NOT
QWORD
ALIGNED
STALLED
UNALIGNED
START
BYTE
BYTE
OF
STORE
NOT
STALLED
UNALIGNED
START
BYTE
NOT
BYTE
OF
STORE
STALLED
UNALIGNED
START
BYTE
BYTE
OF
STORE
NOT
STALLED
UNALIGNED
START
BYTE
NOT
BYTE
OF
STORE
STALLED
UNALIGNED
START
BYTE
BYTE
OF
STORE
STALLED
UNALIGNED
START
BYTE
BYTE
OF
STORE
NOT
STALLED
UNALIGNED
START
BYTE
NOT
BYTE
OF
STORE
NOT
STALLED
UNALIGNED
START
BYTE
DON
T
CARE
STALLED
UNALIGNED
START
BYTE
DON
T
CARE
STALLED
STORE
FORWARDING
RESTRICTION
ON
DATA
AVAILABILITY
THE
VALUE
TO
BE
STORED
MUST
BE
AVAILABLE
BEFORE
THE
LOAD
OPERATION
CAN
BE
COMPLETED
IF
THIS
RESTRICTION
IS
VIOLATED
THE
EXECUTION
OF
THE
LOAD
WILL
BE
DELAYED
UNTIL
THE
DATA
IS
AVAILABLE
THIS
DELAY
CAUSES
SOME
EXECUTION
RESOURCES
TO
BE
USED
UNNECESSARILY
AND
THAT
CAN
LEAD
TO
SIZABLE
BUT
NON
DETERMINISTIC
DELAYS
HOWEVER
THE
OVERALL
IMPACT
OF
THIS
PROBLEM
IS
MUCH
SMALLER
THAN
THAT
FROM
VIOLATING
SIZE
AND
ALIGNMENT
REQUIRE
MENTS
IN
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
HARDWARE
PREDICTS
WHEN
LOADS
ARE
DEPENDENT
ON
AND
GET
THEIR
DATA
FORWARDED
FROM
PRECEDING
STORES
THESE
PREDICTIONS
CAN
SIGNIFICANTLY
IMPROVE
PERFORMANCE
HOWEVER
IF
A
LOAD
IS
SCHEDULED
TOO
SOON
AFTER
THE
STORE
IT
DEPENDS
ON
OR
IF
THE
GENERATION
OF
THE
DATA
TO
BE
STORED
IS
DELAYED
THERE
CAN
BE
A
SIGNIFICANT
PENALTY
THERE
ARE
SEVERAL
CASES
IN
WHICH
DATA
IS
PASSED
THROUGH
MEMORY
AND
THE
STORE
MAY
NEED
TO
BE
SEPARATED
FROM
THE
LOAD
SPILLS
SAVE
AND
RESTORE
REGISTERS
IN
A
STACK
FRAME
PARAMETER
PASSING
GLOBAL
AND
VOLATILE
VARIABLES
TYPE
CONVERSION
BETWEEN
INTEGER
AND
FLOATING
POINT
WHEN
COMPILERS
DO
NOT
ANALYZE
CODE
THAT
IS
INLINED
FORCING
VARIABLES
THAT
ARE
INVOLVED
IN
THE
INTERFACE
WITH
INLINED
CODE
TO
BE
IN
MEMORY
CREATING
MORE
MEMORY
VARIABLES
AND
PREVENTING
THE
ELIMINATION
OF
REDUNDANT
LOADS
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
MH
GENERALITY
WHERE
IT
IS
POSSIBLE
TO
DO
SO
WITHOUT
INCURRING
OTHER
PENALTIES
PRIORITIZE
THE
ALLOCATION
OF
VARIABLES
TO
REGISTERS
AS
IN
REGISTER
ALLOCATION
AND
FOR
PARAMETER
PASSING
TO
MINIMIZE
THE
LIKELIHOOD
AND
IMPACT
OF
STORE
FORWARDING
PROBLEMS
TRY
NOT
TO
STORE
FORWARD
DATA
GENERATED
FROM
A
LONG
LATENCY
INSTRUCTION
FOR
EXAMPLE
MUL
OR
DIV
AVOID
STORE
FORWARDING
DATA
FOR
VARIABLES
WITH
THE
SHORTEST
STORE
LOAD
DISTANCE
AVOID
STORE
FORWARDING
DATA
FOR
VARIABLES
WITH
MANY
AND
OR
LONG
DEPENDENCE
CHAINS
AND
ESPECIALLY
AVOID
INCLUDING
A
STORE
FORWARD
ON
A
LOOP
CARRIED
DEPENDENCE
CHAIN
SHOWS
AN
EXAMPLE
OF
A
LOOP
CARRIED
DEPENDENCE
CHAIN
EXAMPLE
LOOP
CARRIED
DEPENDENCE
CHAIN
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
MH
GENERALITY
CALCULATE
STORE
ADDRESSES
AS
EARLY
AS
POSSIBLE
TO
AVOID
HAVING
STORES
BLOCK
LOADS
DATA
LAYOUT
OPTIMIZATIONS
USER
SOURCE
CODING
RULE
H
IMPACT
M
GENERALITY
PAD
DATA
STRUCTURES
DEFINED
IN
THE
SOURCE
CODE
SO
THAT
EVERY
DATA
ELEMENT
IS
ALIGNED
TO
A
NATURAL
OPERAND
SIZE
ADDRESS
BOUNDARY
IF
THE
OPERANDS
ARE
PACKED
IN
A
SIMD
INSTRUCTION
ALIGN
TO
THE
PACKED
ELEMENT
SIZE
BIT
OR
BIT
ALIGN
DATA
BY
PROVIDING
PADDING
INSIDE
STRUCTURES
AND
ARRAYS
PROGRAMMERS
CAN
REOR
GANIZE
STRUCTURES
AND
ARRAYS
TO
MINIMIZE
THE
AMOUNT
OF
MEMORY
WASTED
BY
PADDING
HOWEVER
COMPILERS
MIGHT
NOT
HAVE
THIS
FREEDOM
THE
C
PROGRAMMING
LANGUAGE
FOR
EXAMPLE
SPECIFIES
THE
ORDER
IN
WHICH
STRUCTURE
ELEMENTS
ARE
ALLOCATED
IN
MEMORY
FOR
MORE
INFORMATION
SEE
SECTION
STACK
AND
DATA
ALIGNMENT
AND
APPENDIX
D
STACK
ALIGNMENT
EXAMPLE
SHOWS
HOW
A
DATA
STRUCTURE
COULD
BE
REARRANGED
TO
REDUCE
ITS
SIZE
EXAMPLE
REARRANGING
A
DATA
STRUCTURE
STRUCT
UNPACKED
FITS
IN
BYTES
DUE
TO
PADDING
INT
A
CHAR
B
INT
C
CHAR
D
INT
E
STRUCT
PACKED
FITS
IN
BYTES
INT
A
INT
C
INT
E
CHAR
B
CHAR
D
CACHE
LINE
SIZE
OF
BYTES
CAN
IMPACT
STREAMING
APPLICATIONS
FOR
EXAMPLE
MULTI
MEDIA
THESE
REFERENCE
AND
USE
DATA
ONLY
ONCE
BEFORE
DISCARDING
IT
DATA
ACCESSES
WHICH
SPARSELY
UTILIZE
THE
DATA
WITHIN
A
CACHE
LINE
CAN
RESULT
IN
LESS
EFFICIENT
UTILIZATION
OF
SYSTEM
MEMORY
BANDWIDTH
FOR
EXAMPLE
ARRAYS
OF
STRUCTURES
CAN
BE
DECOMPOSED
INTO
SEVERAL
ARRAYS
TO
ACHIEVE
BETTER
PACKING
AS
SHOWN
IN
EXAMPLE
EXAMPLE
DECOMPOSING
AN
ARRAY
STRUCT
BYTES
INT
A
C
E
CHAR
B
D
STRUCT
BYTES
INT
A
C
E
CHAR
B
D
STRUCT
BYTES
INT
A
C
E
EXAMPLE
DECOMPOSING
AN
ARRAY
CONTD
STRUCT
BYTES
CHAR
B
D
THE
EFFICIENCY
OF
SUCH
OPTIMIZATIONS
DEPENDS
ON
USAGE
PATTERNS
IF
THE
ELEMENTS
OF
THE
STRUCTURE
ARE
ALL
ACCESSED
TOGETHER
BUT
THE
ACCESS
PATTERN
OF
THE
ARRAY
IS
RANDOM
THEN
AVOIDS
UNNECESSARY
PREFETCH
EVEN
THOUGH
IT
WASTES
MEMORY
HOWEVER
IF
THE
ACCESS
PATTERN
OF
THE
ARRAY
EXHIBITS
LOCALITY
FOR
EXAMPLE
IF
THE
ARRAY
INDEX
IS
BEING
SWEPT
THROUGH
THEN
PROCESSORS
WITH
HARDWARE
PREFETCHERS
WILL
PREFETCH
DATA
FROM
EVEN
IF
THE
ELEMENTS
OF
THE
STRUCTURE
ARE
ACCESSED
TOGETHER
WHEN
THE
ELEMENTS
OF
THE
STRUCTURE
ARE
NOT
ACCESSED
WITH
EQUAL
FREQUENCY
SUCH
AS
WHEN
ELEMENT
A
IS
ACCESSED
TEN
TIMES
MORE
OFTEN
THAN
THE
OTHER
ENTRIES
THEN
NOT
ONLY
SAVES
MEMORY
BUT
IT
ALSO
PREVENTS
FETCHING
UNNECES
SARY
DATA
ITEMS
B
C
D
AND
E
USING
ALSO
ENABLES
THE
USE
OF
THE
SIMD
DATA
TYPES
BY
THE
PROGRAMMER
AND
THE
COMPILER
NOTE
THAT
CAN
HAVE
THE
DISADVANTAGE
OF
REQUIRING
MORE
INDEPEN
DENT
MEMORY
STREAM
REFERENCES
THIS
CAN
REQUIRE
THE
USE
OF
MORE
PREFETCHES
AND
ADDITIONAL
ADDRESS
GENERATION
CALCULATIONS
IT
CAN
ALSO
HAVE
AN
IMPACT
ON
DRAM
PAGE
ACCESS
EFFICIENCY
AN
ALTERNATIVE
BLENDS
THE
TWO
APPROACHES
IN
THIS
CASE
ONLY
SEPARATE
ADDRESS
STREAMS
ARE
GENERATED
AND
REFER
ENCED
FOR
AND
FOR
THE
SECOND
ALTERATIVE
ALSO
PREVENTS
FETCHING
UNNECESSARY
DATA
ASSUMING
THAT
THE
VARIABLES
A
C
AND
E
ARE
ALWAYS
USED
TOGETHER
AND
THE
VARIABLES
B
AND
D
ARE
ALWAYS
USED
TOGETHER
BUT
NOT
AT
THE
SAME
TIME
AS
A
C
AND
E
THE
HYBRID
APPROACH
ENSURES
SIMPLER
FEWER
ADDRESS
GENERATIONS
THAN
FEWER
STREAMS
WHICH
REDUCES
DRAM
PAGE
MISSES
FEWER
PREFETCHES
DUE
TO
FEWER
STREAMS
EFFICIENT
CACHE
LINE
PACKING
OF
DATA
ELEMENTS
THAT
ARE
USED
CONCURRENTLY
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
TRY
TO
ARRANGE
DATA
STRUCTURES
SUCH
THAT
THEY
PERMIT
SEQUENTIAL
ACCESS
IF
THE
DATA
IS
ARRANGED
INTO
A
SET
OF
STREAMS
THE
AUTOMATIC
HARDWARE
PREFETCHER
CAN
PREFETCH
DATA
THAT
WILL
BE
NEEDED
BY
THE
APPLICATION
REDUCING
THE
EFFECTIVE
MEMORY
LATENCY
IF
THE
DATA
IS
ACCESSED
IN
A
NON
SEQUENTIAL
MANNER
THE
AUTOMATIC
HARDWARE
PREFETCHER
CANNOT
PREFETCH
THE
DATA
THE
PREFETCHER
CAN
RECOGNIZE
UP
TO
EIGHT
CONCURRENT
STREAMS
SEE
CHAPTER
OPTIMIZING
CACHE
USAGE
FOR
MORE
INFORMATION
ON
THE
HARDWARE
PREFETCHER
ON
INTEL
CORE
DUO
INTEL
CORE
DUO
INTEL
CORE
SOLO
PENTIUM
INTEL
XEON
AND
PENTIUM
M
PROCESSORS
MEMORY
COHERENCE
IS
MAINTAINED
ON
BYTE
CACHE
LINES
RATHER
THAN
BYTE
CACHE
LINES
AS
IN
EARLIER
PROCESSORS
THIS
CAN
INCREASE
THE
OPPORTUNITY
FOR
FALSE
SHARING
USER
SOURCE
CODING
RULE
M
IMPACT
L
GENERALITY
BEWARE
OF
FALSE
SHARING
WITHIN
A
CACHE
LINE
BYTES
AND
WITHIN
A
SECTOR
OF
BYTES
ON
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
STACK
ALIGNMENT
THE
EASIEST
WAY
TO
AVOID
STACK
ALIGNMENT
PROBLEMS
IS
TO
KEEP
THE
STACK
ALIGNED
AT
ALL
TIMES
FOR
EXAMPLE
A
LANGUAGE
THAT
SUPPORTS
BIT
BIT
BIT
AND
BIT
DATA
QUANTITIES
BUT
NEVER
USES
BIT
DATA
QUANTITIES
CAN
REQUIRE
THE
STACK
TO
ALWAYS
BE
ALIGNED
ON
A
BIT
BOUNDARY
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
IF
BIT
DATA
IS
EVER
PASSED
AS
A
PARAMETER
OR
ALLOCATED
ON
THE
STACK
MAKE
SURE
THAT
THE
STACK
IS
ALIGNED
TO
AN
BYTE
BOUNDARY
DOING
THIS
WILL
REQUIRE
USING
A
GENERAL
PURPOSE
REGISTER
SUCH
AS
EBP
AS
A
FRAME
POINTER
THE
TRADE
OFF
IS
BETWEEN
CAUSING
UNALIGNED
BIT
REFERENCES
IF
THE
STACK
IS
NOT
ALIGNED
AND
CAUSING
EXTRA
GENERAL
PURPOSE
REGISTER
SPILLS
IF
THE
STACK
IS
ALIGNED
NOTE
THAT
A
PERFORMANCE
PENALTY
IS
CAUSED
ONLY
WHEN
AN
UNALIGNED
ACCESS
SPLITS
A
CACHE
LINE
THIS
MEANS
THAT
ONE
OUT
OF
EIGHT
SPATIALLY
CONSECUTIVE
UNALIGNED
ACCESSES
IS
ALWAYS
PENALIZED
A
ROUTINE
THAT
MAKES
FREQUENT
USE
OF
BIT
DATA
CAN
AVOID
STACK
MISALIGNMENT
BY
PLACING
THE
CODE
DESCRIBED
IN
EXAMPLE
IN
THE
FUNCTION
PROLOGUE
AND
EPILOGUE
EXAMPLE
DYNAMIC
STACK
ALIGNMENT
PROLOGUE
SUBL
ESP
SAVE
FRAME
PTR
MOVL
ESP
EBP
MOVL
EBP
ESP
NEW
FRAME
POINTER
ANDL
EBP
ALIGNED
TO
BITS
MOVL
EBP
ESP
SAVE
OLD
STACK
PTR
SUBL
ESP
FRAMESIZE
ALLOCATE
SPACE
CALLEE
SAVES
ETC
EXAMPLE
DYNAMIC
STACK
ALIGNMENT
CONTD
EPILOGUE
CALLEE
RESTORES
ETC
MOVL
ESP
EBP
RESTORE
STACK
PTR
MOVL
EBP
ESP
RESTORE
FRAME
PTR
ADDL
ESP
RET
IF
FOR
SOME
REASON
IT
IS
NOT
POSSIBLE
TO
ALIGN
THE
STACK
FOR
BITS
THE
ROUTINE
SHOULD
ACCESS
THE
PARAMETER
AND
SAVE
IT
INTO
A
REGISTER
OR
KNOWN
ALIGNED
STORAGE
THUS
INCUR
RING
THE
PENALTY
ONLY
ONCE
CAPACITY
LIMITS
AND
ALIASING
IN
CACHES
THERE
ARE
CASES
IN
WHICH
ADDRESSES
WITH
A
GIVEN
STRIDE
WILL
COMPETE
FOR
SOME
RESOURCE
IN
THE
MEMORY
HIERARCHY
TYPICALLY
CACHES
ARE
IMPLEMENTED
TO
HAVE
MULTIPLE
WAYS
OF
SET
ASSOCIATIVITY
WITH
EACH
WAY
CONSISTING
OF
MULTIPLE
SETS
OF
CACHE
LINES
OR
SECTORS
IN
SOME
CASES
MULTIPLE
MEMORY
REFERENCES
THAT
COMPETE
FOR
THE
SAME
SET
OF
EACH
WAY
IN
A
CACHE
CAN
CAUSE
A
CAPACITY
ISSUE
THERE
ARE
ALIASING
CONDITIONS
THAT
APPLY
TO
SPECIFIC
MICROARCHITECTURES
NOTE
THAT
FIRST
LEVEL
CACHE
LINES
ARE
BYTES
THUS
THE
LEAST
SIGNIFICANT
BITS
ARE
NOT
CONSIDERED
IN
ALIAS
COMPARISONS
FOR
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
DATA
IS
LOADED
INTO
THE
SECOND
LEVEL
CACHE
IN
A
SECTOR
OF
BYTES
SO
THE
LEAST
SIGNIFICANT
BITS
ARE
NOT
CONSIDERED
IN
ALIAS
COMPAR
ISONS
CAPACITY
LIMITS
IN
SET
ASSOCIATIVE
CACHES
CAPACITY
LIMITS
MAY
BE
REACHED
IF
THE
NUMBER
OF
OUTSTANDING
MEMORY
REFERENCES
THAT
ARE
MAPPED
TO
THE
SAME
SET
IN
EACH
WAY
OF
A
GIVEN
CACHE
EXCEEDS
THE
NUMBER
OF
WAYS
OF
THAT
CACHE
THE
CONDITIONS
THAT
APPLY
TO
THE
FIRST
LEVEL
DATA
CACHE
AND
SECOND
LEVEL
CACHE
ARE
LISTED
BELOW
SET
CONFLICTS
MULTIPLE
REFERENCES
MAP
TO
THE
SAME
FIRST
LEVEL
CACHE
SET
THE
CONFLICTING
CONDITION
IS
A
STRIDE
DETERMINED
BY
THE
SIZE
OF
THE
CACHE
IN
BYTES
DIVIDED
BY
THE
NUMBER
OF
WAYS
THESE
COMPETING
MEMORY
REFERENCES
CAN
CAUSE
EXCESSIVE
CACHE
MISSES
ONLY
IF
THE
NUMBER
OF
OUTSTANDING
MEMORY
REFERENCES
EXCEEDS
THE
NUMBER
OF
WAYS
IN
THE
WORKING
SET
ON
PENTIUM
AND
INTEL
XEON
PROCESSORS
WITH
A
CPUID
SIGNATURE
OF
FAMILY
ENCODING
MODEL
ENCODING
OF
OR
THERE
WILL
BE
AN
EXCESS
OF
FIRST
LEVEL
CACHE
MISSES
FOR
MORE
THAN
SIMULTANEOUS
COMPETING
MEMORY
REFERENCES
TO
ADDRESSES
WITH
KBYTE
MODULUS
ON
PENTIUM
AND
INTEL
XEON
PROCESSORS
WITH
A
CPUID
SIGNATURE
OF
FAMILY
ENCODING
MODEL
ENCODING
THERE
WILL
BE
AN
EXCESS
OF
FIRST
LEVEL
CACHE
MISSES
FOR
MORE
THAN
SIMULTANEOUS
COMPETING
REFERENCES
TO
ADDRESSES
THAT
ARE
APART
BY
KBYTE
MODULUS
ON
INTEL
CORE
DUO
INTEL
CORE
DUO
INTEL
CORE
SOLO
AND
PENTIUM
M
PROCESSORS
THERE
WILL
BE
AN
EXCESS
OF
FIRST
LEVEL
CACHE
MISSES
FOR
MORE
THAN
SIMULTANEOUS
REFERENCES
TO
ADDRESSES
THAT
ARE
APART
BY
KBYTE
MODULUS
SET
CONFLICTS
MULTIPLE
REFERENCES
MAP
TO
THE
SAME
SECOND
LEVEL
CACHE
SET
THE
CONFLICTING
CONDITION
IS
ALSO
DETERMINED
BY
THE
SIZE
OF
THE
CACHE
OR
THE
NUMBER
OF
WAYS
ON
PENTIUM
AND
INTEL
XEON
PROCESSORS
THERE
WILL
BE
AN
EXCESS
OF
SECOND
LEVEL
CACHE
MISSES
FOR
MORE
THAN
SIMULTANEOUS
COMPETING
REFERENCES
THE
STRIDE
SIZES
THAT
CAN
CAUSE
CAPACITY
ISSUES
ARE
KBYTES
KBYTES
OR
KBYTES
DEPENDING
OF
THE
SIZE
OF
THE
SECOND
LEVEL
CACHE
ON
PENTIUM
M
PROCESSORS
THE
STRIDE
SIZES
THAT
CAN
CAUSE
CAPACITY
ISSUES
ARE
KBYTES
OR
KBYTES
DEPENDING
OF
THE
SIZE
OF
THE
SECOND
LEVEL
CACHE
ON
INTEL
CORE
DUO
INTEL
CORE
DUO
INTEL
CORE
SOLO
PROCESSORS
STRIDE
SIZE
OF
KBYTES
CAN
CAUSE
CAPACITY
ISSUE
IF
THE
NUMBER
OF
SIMULTANEOUS
ACCESSES
EXCEEDED
THE
WAY
ASSOCIATIVITY
OF
THE
CACHE
ALIASING
CASES
IN
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
ALIASING
CONDITIONS
THAT
ARE
SPECIFIC
TO
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROAR
CHITECTURE
ARE
KBYTES
FOR
CODE
THERE
CAN
ONLY
BE
ONE
OF
THESE
IN
THE
TRACE
CACHE
AT
A
TIME
IF
TWO
TRACES
WHOSE
STARTING
ADDRESSES
ARE
KBYTES
APART
ARE
IN
THE
SAME
WORKING
SET
THE
SYMPTOM
WILL
BE
A
HIGH
TRACE
CACHE
MISS
RATE
SOLVE
THIS
BY
OFFSETTING
ONE
OF
THE
ADDRESSES
BY
ONE
OR
MORE
BYTES
DATA
CONFLICT
THERE
CAN
ONLY
BE
ONE
INSTANCE
OF
THE
DATA
IN
THE
FIRST
LEVEL
CACHE
AT
A
TIME
IF
A
REFERENCE
LOAD
OR
STORE
OCCURS
AND
ITS
LINEAR
ADDRESS
MATCHES
A
DATA
CONFLICT
CONDITION
WITH
ANOTHER
REFERENCE
LOAD
OR
STORE
THAT
IS
UNDER
WAY
THEN
THE
SECOND
REFERENCE
CANNOT
BEGIN
UNTIL
THE
FIRST
ONE
IS
KICKED
OUT
OF
THE
CACHE
ON
PENTIUM
AND
INTEL
XEON
PROCESSORS
WITH
A
CPUID
SIGNATURE
OF
FAMILY
ENCODING
MODEL
ENCODING
OF
OR
THE
DATA
CONFLICT
CONDITION
APPLIES
TO
ADDRESSES
HAVING
IDENTICAL
VALUES
IN
BITS
THIS
IS
ALSO
REFERRED
TO
AS
A
KBYTE
ALIASING
CONFLICT
IF
YOU
AVOID
THIS
KIND
OF
ALIASING
YOU
CAN
SPEED
UP
PROGRAMS
BY
A
FACTOR
OF
THREE
IF
THEY
LOAD
FREQUENTLY
FROM
PRECEDING
STORES
WITH
ALIASED
ADDRESSES
AND
LITTLE
OTHER
INSTRUCTION
LEVEL
PARALLELISM
IS
AVAILABLE
THE
GAIN
IS
SMALLER
WHEN
LOADS
ALIAS
WITH
OTHER
LOADS
WHICH
CAUSES
THRASHING
IN
THE
FIRST
LEVEL
CACHE
ON
PENTIUM
AND
INTEL
XEON
PROCESSORS
WITH
A
CPUID
SIGNATURE
OF
FAMILY
ENCODING
MODEL
ENCODING
THE
DATA
CONFLICT
CONDITION
APPLIES
TO
ADDRESSES
HAVING
IDENTICAL
VALUES
IN
BITS
ALIASING
CASES
IN
THE
PENTIUM
M
INTEL
CORE
SOLO
INTEL
CORE
DUO
AND
INTEL
CORE
DUO
PROCESSORS
PENTIUM
M
INTEL
CORE
SOLO
INTEL
CORE
DUO
AND
INTEL
CORE
DUO
PROCESSORS
HAVE
THE
FOLLOWING
ALIASING
CASE
STORE
FORWARDING
IF
A
STORE
TO
AN
ADDRESS
IS
FOLLOWED
BY
A
LOAD
FROM
THE
SAME
ADDRESS
THE
LOAD
WILL
NOT
PROCEED
UNTIL
THE
STORE
DATA
IS
AVAILABLE
IF
A
STORE
IS
FOLLOWED
BY
A
LOAD
AND
THEIR
ADDRESSES
DIFFER
BY
A
MULTIPLE
OF
KBYTES
THE
LOAD
STALLS
UNTIL
THE
STORE
OPERATION
COMPLETES
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
AVOID
HAVING
A
STORE
FOLLOWED
BY
A
NON
DEPENDENT
LOAD
WITH
ADDRESSES
THAT
DIFFER
BY
A
MULTIPLE
OF
KBYTES
ALSO
LAY
OUT
DATA
OR
ORDER
COMPUTATION
TO
AVOID
HAVING
CACHE
LINES
THAT
HAVE
LINEAR
ADDRESSES
THAT
ARE
A
MULTIPLE
OF
KBYTES
APART
IN
THE
SAME
WORKING
SET
AVOID
HAVING
MORE
THAN
CACHE
LINES
THAT
ARE
SOME
MULTIPLE
OF
KBYTES
APART
IN
THE
SAME
FIRST
LEVEL
CACHE
WORKING
SET
AND
AVOID
HAVING
MORE
THAN
CACHE
LINES
THAT
ARE
SOME
MULTIPLE
OF
KBYTES
APART
IN
THE
SAME
FIRST
LEVEL
CACHE
WORKING
SET
WHEN
DECLARING
MULTIPLE
ARRAYS
THAT
ARE
REFERENCED
WITH
THE
SAME
INDEX
AND
ARE
EACH
A
MULTIPLE
OF
KBYTES
AS
CAN
HAPPEN
WITH
DATA
LAYOUTS
PAD
THEM
TO
AVOID
DECLARING
THEM
CONTIGUOUSLY
PADDING
CAN
BE
ACCOMPLISHED
BY
EITHER
INTERVENING
DECLARATIONS
OF
OTHER
VARIABLES
OR
BY
ARTIFICIALLY
INCREASING
THE
DIMENSION
USER
SOURCE
CODING
RULE
H
IMPACT
ML
GENERALITY
CONSIDER
USING
A
SPECIAL
MEMORY
ALLOCATION
LIBRARY
WITH
ADDRESS
OFFSET
CAPABILITY
TO
AVOID
ALIASING
ONE
WAY
TO
IMPLEMENT
A
MEMORY
ALLOCATOR
TO
AVOID
ALIASING
IS
TO
ALLOCATE
MORE
THAN
ENOUGH
SPACE
AND
PAD
FOR
EXAMPLE
ALLOCATE
STRUCTURES
THAT
ARE
KB
INSTEAD
OF
KBYTES
TO
AVOID
THE
KBYTE
ALIASING
OR
HAVE
THE
ALLOCATOR
PAD
AND
RETURN
RANDOM
OFFSETS
THAT
ARE
A
MULTIPLE
OF
BYTES
THE
SIZE
OF
A
CACHE
LINE
USER
SOURCE
CODING
RULE
M
IMPACT
M
GENERALITY
WHEN
PADDING
VARIABLE
DECLARATIONS
TO
AVOID
ALIASING
THE
GREATEST
BENEFIT
COMES
FROM
AVOIDING
ALIASING
ON
SECOND
LEVEL
CACHE
LINES
SUGGESTING
AN
OFFSET
OF
BYTES
OR
MORE
KBYTE
MEMORY
ALIASING
OCCURS
WHEN
THE
CODE
ACCESSES
TWO
DIFFERENT
MEMORY
LOCA
TIONS
WITH
A
KBYTE
OFFSET
BETWEEN
THEM
THE
KBYTE
ALIASING
SITUATION
CAN
MANI
FEST
IN
A
MEMORY
COPY
ROUTINE
WHERE
THE
ADDRESSES
OF
THE
SOURCE
BUFFER
AND
DESTINATION
BUFFER
MAINTAIN
A
CONSTANT
OFFSET
AND
THE
CONSTANT
OFFSET
HAPPENS
TO
BE
A
MULTIPLE
OF
THE
BYTE
INCREMENT
FROM
ONE
ITERATION
TO
THE
NEXT
EXAMPLE
SHOWS
A
ROUTINE
THAT
COPIES
BYTES
OF
MEMORY
IN
EACH
ITERATION
OF
A
LOOP
IF
THE
OFFSETS
MODULAR
BETWEEN
SOURCE
BUFFER
EAX
AND
DESTINATION
BUFFER
EDX
DIFFER
BY
LOADS
HAVE
TO
WAIT
UNTIL
STORES
HAVE
BEEN
RETIRED
BEFORE
THEY
CAN
CONTINUE
FOR
EXAMPLE
AT
OFFSET
THE
LOAD
OF
THE
NEXT
ITERA
TION
IS
KBYTE
ALIASED
CURRENT
ITERATION
STORE
THEREFORE
THE
LOOP
MUST
WAIT
UNTIL
THE
STORE
OPERATION
COMPLETES
MAKING
THE
ENTIRE
LOOP
SERIALIZED
THE
AMOUNT
OF
TIME
NEEDED
TO
WAIT
DECREASES
WITH
LARGER
OFFSET
UNTIL
OFFSET
OF
RESOLVES
THE
ISSUE
AS
THERE
IS
NO
PENDING
STORES
BY
THE
TIME
OF
THE
LOAD
WITH
SAME
ADDRESS
THE
INTEL
CORE
MICROARCHITECTURE
PROVIDES
A
PERFORMANCE
MONITORING
EVENT
SEE
IN
INTEL
AND
IA
ARCHITECTURES
SOFTWARE
DEVELOPER
MANUAL
VOLUME
THAT
ALLOWS
SOFTWARE
TUNING
EFFORT
TO
DETECT
THE
OCCURRENCE
OF
ALIASING
CONDITIONS
EXAMPLE
ALIASING
BETWEEN
LOADS
AND
STORES
ACROSS
LOOP
ITERATIONS
LP
MOVAPS
EAX
ECX
MOVAPS
EDX
ECX
ADD
ECX
JNZ
LP
MIXING
CODE
AND
DATA
THE
AGGRESSIVE
PREFETCHING
AND
PRE
DECODING
OF
INSTRUCTIONS
BY
INTEL
PROCESSORS
HAVE
TWO
RELATED
EFFECTS
SELF
MODIFYING
CODE
WORKS
CORRECTLY
ACCORDING
TO
THE
INTEL
ARCHITECTURE
PROCESSOR
REQUIREMENTS
BUT
INCURS
A
SIGNIFICANT
PERFORMANCE
PENALTY
AVOID
SELF
MODIFYING
CODE
IF
POSSIBLE
PLACING
WRITABLE
DATA
IN
THE
CODE
SEGMENT
MIGHT
BE
IMPOSSIBLE
TO
DISTINGUISH
FROM
SELF
MODIFYING
CODE
WRITABLE
DATA
IN
THE
CODE
SEGMENT
MIGHT
SUFFER
THE
SAME
PERFORMANCE
PENALTY
AS
SELF
MODIFYING
CODE
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
L
GENERALITY
IF
HOPEFULLY
READ
ONLY
DATA
MUST
OCCUR
ON
THE
SAME
PAGE
AS
CODE
AVOID
PLACING
IT
IMMEDIATELY
AFTER
AN
INDIRECT
JUMP
FOR
EXAMPLE
FOLLOW
AN
INDIRECT
JUMP
WITH
ITS
MOSTLY
LIKELY
TARGET
AND
PLACE
THE
DATA
AFTER
AN
UNCONDITIONAL
BRANCH
TUNING
SUGGESTION
IN
RARE
CASES
A
PERFORMANCE
PROBLEM
MAY
BE
CAUSED
BY
EXECUTING
DATA
ON
A
CODE
PAGE
AS
INSTRUCTIONS
THIS
IS
VERY
LIKELY
TO
HAPPEN
WHEN
EXECUTION
IS
FOLLOWING
AN
INDIRECT
BRANCH
THAT
IS
NOT
RESIDENT
IN
THE
TRACE
CACHE
IF
THIS
IS
CLEARLY
CAUSING
A
PERFORMANCE
PROBLEM
TRY
MOVING
THE
DATA
ELSEWHERE
OR
INSERTING
AN
ILLEGAL
OPCODE
OR
A
PAUSE
INSTRUCTION
IMMEDIATELY
AFTER
THE
INDIRECT
BRANCH
NOTE
THAT
THE
LATTER
TWO
ALTERNATIVES
MAY
DEGRADE
PERFORMANCE
IN
SOME
CIRCUMSTANCES
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
L
GENERALITY
ALWAYS
PUT
CODE
AND
DATA
ON
SEPARATE
PAGES
AVOID
SELF
MODIFYING
CODE
WHEREVER
POSSIBLE
IF
CODE
IS
TO
BE
MODIFIED
TRY
TO
DO
IT
ALL
AT
ONCE
AND
MAKE
SURE
THE
CODE
THAT
PERFORMS
THE
MODIFICATIONS
AND
THE
CODE
BEING
MODIFIED
ARE
ON
SEPARATE
KBYTE
PAGES
OR
ON
SEPARATE
ALIGNED
KBYTE
SUBPAGES
SELF
MODIFYING
CODE
SELF
MODIFYING
CODE
SMC
THAT
RAN
CORRECTLY
ON
PENTIUM
III
PROCESSORS
AND
PRIOR
IMPLEMENTATIONS
WILL
RUN
CORRECTLY
ON
SUBSEQUENT
IMPLEMENTATIONS
SMC
AND
CROSS
MODIFYING
CODE
WHEN
MULTIPLE
PROCESSORS
IN
A
MULTIPROCESSOR
SYSTEM
ARE
WRITING
TO
A
CODE
PAGE
SHOULD
BE
AVOIDED
WHEN
HIGH
PERFORMANCE
IS
DESIRED
SOFTWARE
SHOULD
AVOID
WRITING
TO
A
CODE
PAGE
IN
THE
SAME
KBYTE
SUBPAGE
THAT
IS
BEING
EXECUTED
OR
FETCHING
CODE
IN
THE
SAME
KBYTE
SUBPAGE
OF
THAT
IS
BEING
WRITTEN
IN
ADDITION
SHARING
A
PAGE
CONTAINING
DIRECTLY
OR
SPECULATIVELY
EXECUTED
CODE
WITH
ANOTHER
PROCESSOR
AS
A
DATA
PAGE
CAN
TRIGGER
AN
SMC
CONDITION
THAT
CAUSES
THE
ENTIRE
PIPELINE
OF
THE
MACHINE
AND
THE
TRACE
CACHE
TO
BE
CLEARED
THIS
IS
DUE
TO
THE
SELF
MODIFYING
CODE
CONDITION
DYNAMIC
CODE
NEED
NOT
CAUSE
THE
SMC
CONDITION
IF
THE
CODE
WRITTEN
FILLS
UP
A
DATA
PAGE
BEFORE
THAT
PAGE
IS
ACCESSED
AS
CODE
DYNAMICALLY
MODIFIED
CODE
FOR
EXAMPLE
FROM
TARGET
FIX
UPS
IS
LIKELY
TO
SUFFER
FROM
THE
SMC
CONDITION
AND
SHOULD
BE
AVOIDED
WHERE
POSSIBLE
AVOID
THE
CONDITION
BY
INTRODUCING
INDIRECT
BRANCHES
AND
USING
DATA
TABLES
ON
DATA
PAGES
NOT
CODE
PAGES
USING
REGISTER
INDIRECT
CALLS
WRITE
COMBINING
WRITE
COMBINING
WC
IMPROVES
PERFORMANCE
IN
TWO
WAYS
ON
A
WRITE
MISS
TO
THE
FIRST
LEVEL
CACHE
IT
ALLOWS
MULTIPLE
STORES
TO
THE
SAME
CACHE
LINE
TO
OCCUR
BEFORE
THAT
CACHE
LINE
IS
READ
FOR
OWNERSHIP
RFO
FROM
FURTHER
OUT
IN
THE
CACHE
MEMORY
HIERARCHY
THEN
THE
REST
OF
LINE
IS
READ
AND
THE
BYTES
THAT
HAVE
NOT
BEEN
WRITTEN
ARE
COMBINED
WITH
THE
UNMODIFIED
BYTES
IN
THE
RETURNED
LINE
WRITE
COMBINING
ALLOWS
MULTIPLE
WRITES
TO
BE
ASSEMBLED
AND
WRITTEN
FURTHER
OUT
IN
THE
CACHE
HIERARCHY
AS
A
UNIT
THIS
SAVES
PORT
AND
BUS
TRAFFIC
SAVING
TRAFFIC
IS
PARTICULARLY
IMPORTANT
FOR
AVOIDING
PARTIAL
WRITES
TO
UNCACHED
MEMORY
THERE
ARE
SIX
WRITE
COMBINING
BUFFERS
ON
PENTIUM
AND
INTEL
XEON
PROCESSORS
WITH
A
CPUID
SIGNATURE
OF
FAMILY
ENCODING
MODEL
ENCODING
THERE
ARE
WRITE
COMBINING
BUFFERS
TWO
OF
THESE
BUFFERS
MAY
BE
WRITTEN
OUT
TO
HIGHER
CACHE
LEVELS
AND
FREED
UP
FOR
USE
ON
OTHER
WRITE
MISSES
ONLY
FOUR
WRITE
COMBINING
BUFFERS
ARE
GUARANTEED
TO
BE
AVAILABLE
FOR
SIMULTANEOUS
USE
WRITE
COMBINING
APPLIES
TO
MEMORY
TYPE
WC
IT
DOES
NOT
APPLY
TO
MEMORY
TYPE
UC
THERE
ARE
SIX
WRITE
COMBINING
BUFFERS
IN
EACH
PROCESSOR
CORE
IN
INTEL
CORE
DUO
AND
INTEL
CORE
SOLO
PROCESSORS
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
HAVE
EIGHT
WRITE
COMBINING
BUFFERS
IN
EACH
CORE
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
L
GENERALITY
IF
AN
INNER
LOOP
WRITES
TO
MORE
THAN
FOUR
ARRAYS
FOUR
DISTINCT
CACHE
LINES
APPLY
LOOP
FISSION
TO
BREAK
UP
THE
BODY
OF
THE
LOOP
SUCH
THAT
ONLY
FOUR
ARRAYS
ARE
BEING
WRITTEN
TO
IN
EACH
ITERATION
OF
EACH
OF
THE
RESULTING
LOOPS
WRITE
COMBINING
BUFFERS
ARE
USED
FOR
STORES
OF
ALL
MEMORY
TYPES
THEY
ARE
PARTICU
LARLY
IMPORTANT
FOR
WRITES
TO
UNCACHED
MEMORY
WRITES
TO
DIFFERENT
PARTS
OF
THE
SAME
CACHE
LINE
CAN
BE
GROUPED
INTO
A
SINGLE
FULL
CACHE
LINE
BUS
TRANSACTION
INSTEAD
OF
GOING
ACROSS
THE
BUS
SINCE
THEY
ARE
NOT
CACHED
AS
SEVERAL
PARTIAL
WRITES
AVOIDING
PARTIAL
WRITES
CAN
HAVE
A
SIGNIFICANT
IMPACT
ON
BUS
BANDWIDTH
BOUND
GRAPHICS
APPLI
CATIONS
WHERE
GRAPHICS
BUFFERS
ARE
IN
UNCACHED
MEMORY
SEPARATING
WRITES
TO
UNCACHED
MEMORY
AND
WRITES
TO
WRITEBACK
MEMORY
INTO
SEPARATE
PHASES
CAN
ASSURE
THAT
THE
WRITE
COMBINING
BUFFERS
CAN
FILL
BEFORE
GETTING
EVICTED
BY
OTHER
WRITE
TRAFFIC
ELIMINATING
PARTIAL
WRITE
TRANSACTIONS
HAS
BEEN
FOUND
TO
HAVE
PERFORMANCE
IMPACT
ON
THE
ORDER
OF
FOR
SOME
APPLICATIONS
BECAUSE
THE
CACHE
LINES
ARE
BYTES
A
WRITE
TO
THE
BUS
FOR
BYTES
WILL
RESULT
IN
PARTIAL
BUS
TRANSACTIONS
WHEN
CODING
FUNCTIONS
THAT
EXECUTE
SIMULTANEOUSLY
ON
TWO
THREADS
REDUCING
THE
NUMBER
OF
WRITES
THAT
ARE
ALLOWED
IN
AN
INNER
LOOP
WILL
HELP
TAKE
FULL
ADVANTAGE
OF
WRITE
COMBINING
STORE
BUFFERS
FOR
WRITE
COMBINING
BUFFER
RECOMMENDATIONS
FOR
HYPER
THREADING
TECHNOLOGY
SEE
CHAPTER
MULTICORE
AND
HYPER
THREADING
TECH
NOLOGY
STORE
ORDERING
AND
VISIBILITY
ARE
ALSO
IMPORTANT
ISSUES
FOR
WRITE
COMBINING
WHEN
A
WRITE
TO
A
WRITE
COMBINING
BUFFER
FOR
A
PREVIOUSLY
UNWRITTEN
CACHE
LINE
OCCURS
THERE
WILL
BE
A
READ
FOR
OWNERSHIP
RFO
IF
A
SUBSEQUENT
WRITE
HAPPENS
TO
ANOTHER
WRITE
COMBINING
BUFFER
A
SEPARATE
RFO
MAY
BE
CAUSED
FOR
THAT
CACHE
LINE
SUBSEQUENT
WRITES
TO
THE
FIRST
CACHE
LINE
AND
WRITE
COMBINING
BUFFER
WILL
BE
DELAYED
UNTIL
THE
SECOND
RFO
HAS
BEEN
SERVICED
TO
GUARANTEE
PROPERLY
ORDERED
VISIBILITY
OF
THE
WRITES
IF
THE
MEMORY
TYPE
FOR
THE
WRITES
IS
WRITE
COMBINING
THERE
WILL
BE
NO
RFO
SINCE
THE
LINE
IS
NOT
CACHED
AND
THERE
IS
NO
SUCH
DELAY
FOR
DETAILS
ON
WRITE
COMBINING
SEE
CHAPTER
POWER
OPTIMIZATION
FOR
MOBILE
USAGES
OF
INTEL
AND
IA
ARCHI
TECTURES
SOFTWARE
DEVELOPER
MANUAL
VOLUME
LOCALITY
ENHANCEMENT
LOCALITY
ENHANCEMENT
CAN
REDUCE
DATA
TRAFFIC
ORIGINATING
FROM
AN
OUTER
LEVEL
SUB
SYSTEM
IN
THE
CACHE
MEMORY
HIERARCHY
THIS
IS
TO
ADDRESS
THE
FACT
THAT
THE
ACCESS
COST
IN
TERMS
OF
CYCLE
COUNT
FROM
AN
OUTER
LEVEL
WILL
BE
MORE
EXPENSIVE
THAN
FROM
AN
INNER
LEVEL
TYPICALLY
THE
CYCLE
COST
OF
ACCESSING
A
GIVEN
CACHE
LEVEL
OR
MEMORY
SYSTEM
VARIES
ACROSS
DIFFERENT
MICROARCHITECTURES
PROCESSOR
IMPLEMENTATIONS
AND
PLATFORM
COMPONENTS
IT
MAY
BE
SUFFICIENT
TO
RECOGNIZE
THE
RELATIVE
DATA
ACCESS
COST
TREND
BY
LOCALITY
RATHER
THAN
TO
FOLLOW
A
LARGE
TABLE
OF
NUMERIC
VALUES
OF
CYCLE
COSTS
LISTED
PER
LOCALITY
PER
PROCESSOR
PLATFORM
IMPLEMENTATIONS
ETC
THE
GENERAL
TREND
IS
TYPICALLY
THAT
ACCESS
COST
FROM
AN
OUTER
SUB
SYSTEM
MAY
BE
APPROXIMATELY
MORE
EXPENSIVE
THAN
ACCESSING
DATA
FROM
THE
IMMEDIATE
INNER
LEVEL
IN
THE
CACHE
MEMORY
HIERARCHY
ASSUMING
SIMILAR
DEGREES
OF
DATA
ACCESS
PARALLELISM
THUS
LOCALITY
ENHANCEMENT
SHOULD
START
WITH
CHARACTERIZING
THE
DOMINANT
DATA
TRAFFIC
LOCALITY
SECTION
A
APPLICATION
PERFORMANCE
TOOLS
DESCRIBES
SOME
TECHNIQUES
THAT
CAN
BE
USED
TO
DETERMINE
THE
DOMINANT
DATA
TRAFFIC
LOCALITY
FOR
ANY
WORKLOAD
EVEN
IF
CACHE
MISS
RATES
OF
THE
LAST
LEVEL
CACHE
MAY
BE
LOW
RELATIVE
TO
THE
NUMBER
OF
CACHE
REFERENCES
PROCESSORS
TYPICALLY
SPEND
A
SIZABLE
PORTION
OF
THEIR
EXECUTION
TIME
WAITING
FOR
CACHE
MISSES
TO
BE
SERVICED
REDUCING
CACHE
MISSES
BY
ENHANCING
A
PROGRAM
LOCALITY
IS
A
KEY
OPTIMIZATION
THIS
CAN
TAKE
SEVERAL
FORMS
BLOCKING
TO
ITERATE
OVER
A
PORTION
OF
AN
ARRAY
THAT
WILL
FIT
IN
THE
CACHE
WITH
THE
PURPOSE
THAT
SUBSEQUENT
REFERENCES
TO
THE
DATA
BLOCK
OR
TILE
WILL
BE
CACHE
HIT
REFERENCES
LOOP
INTERCHANGE
TO
AVOID
CROSSING
CACHE
LINES
OR
PAGE
BOUNDARIES
LOOP
SKEWING
TO
MAKE
ACCESSES
CONTIGUOUS
LOCALITY
ENHANCEMENT
TO
THE
LAST
LEVEL
CACHE
CAN
BE
ACCOMPLISHED
WITH
SEQUENCING
THE
DATA
ACCESS
PATTERN
TO
TAKE
ADVANTAGE
OF
HARDWARE
PREFETCHING
THIS
CAN
ALSO
TAKE
SEVERAL
FORMS
TRANSFORMATION
OF
A
SPARSELY
POPULATED
MULTI
DIMENSIONAL
ARRAY
INTO
A
ONE
DIMENSION
ARRAY
SUCH
THAT
MEMORY
REFERENCES
OCCUR
IN
A
SEQUENTIAL
SMALL
STRIDE
PATTERN
THAT
IS
FRIENDLY
TO
THE
HARDWARE
PREFETCH
SEE
SECTION
DATA
PREFETCH
OPTIMAL
TILE
SIZE
AND
SHAPE
SELECTION
CAN
FURTHER
IMPROVE
TEMPORAL
DATA
LOCALITY
BY
INCREASING
HIT
RATES
INTO
THE
LAST
LEVEL
CACHE
AND
REDUCE
MEMORY
TRAFFIC
RESULTING
FROM
THE
ACTIONS
OF
HARDWARE
PREFETCHING
SEE
SECTION
HARDWARE
PREFETCHING
AND
CACHE
BLOCKING
TECHNIQUES
IT
IS
IMPORTANT
TO
AVOID
OPERATIONS
THAT
WORK
AGAINST
LOCALITY
ENHANCING
TECHNIQUES
USING
THE
LOCK
PREFIX
HEAVILY
CAN
INCUR
LARGE
DELAYS
WHEN
ACCESSING
MEMORY
REGARD
LESS
OF
WHETHER
THE
DATA
IS
IN
THE
CACHE
OR
IN
SYSTEM
MEMORY
USER
SOURCE
CODING
RULE
H
IMPACT
H
GENERALITY
OPTIMIZATION
TECHNIQUES
SUCH
AS
BLOCKING
LOOP
INTERCHANGE
LOOP
SKEWING
AND
PACKING
ARE
BEST
DONE
BY
THE
COMPILER
OPTIMIZE
DATA
STRUCTURES
EITHER
TO
FIT
IN
ONE
HALF
OF
THE
FIRST
LEVEL
CACHE
OR
IN
THE
SECOND
LEVEL
CACHE
TURN
ON
LOOP
OPTIMIZATIONS
IN
THE
COMPILER
TO
ENHANCE
LOCALITY
FOR
NESTED
LOOPS
OPTIMIZING
FOR
ONE
HALF
OF
THE
FIRST
LEVEL
CACHE
WILL
BRING
THE
GREATEST
PERFORMANCE
BENEFIT
IN
TERMS
OF
CYCLE
COST
PER
DATA
ACCESS
IF
ONE
HALF
OF
THE
FIRST
LEVEL
CACHE
IS
TOO
SMALL
TO
BE
PRACTICAL
OPTIMIZE
FOR
THE
SECOND
LEVEL
CACHE
OPTIMIZING
FOR
A
POINT
IN
BETWEEN
FOR
EXAMPLE
FOR
THE
ENTIRE
FIRST
LEVEL
CACHE
WILL
LIKELY
NOT
BRING
A
SUBSTANTIAL
IMPROVEMENT
OVER
OPTIMIZING
FOR
THE
SECOND
LEVEL
CACHE
MINIMIZING
BUS
LATENCY
EACH
BUS
TRANSACTION
INCLUDES
THE
OVERHEAD
OF
MAKING
REQUESTS
AND
ARBITRATIONS
THE
AVERAGE
LATENCY
OF
BUS
READ
AND
BUS
WRITE
TRANSACTIONS
WILL
BE
LONGER
IF
READS
AND
WRITES
ALTERNATE
SEGMENTING
READS
AND
WRITES
INTO
PHASES
CAN
REDUCE
THE
AVERAGE
LATENCY
OF
BUS
TRANSACTIONS
THIS
IS
BECAUSE
THE
NUMBER
OF
INCIDENCES
OF
SUCCESSIVE
TRANSACTIONS
INVOLVING
A
READ
FOLLOWING
A
WRITE
OR
A
WRITE
FOLLOWING
A
READ
ARE
REDUCED
USER
SOURCE
CODING
RULE
M
IMPACT
ML
GENERALITY
IF
THERE
IS
A
BLEND
OF
READS
AND
WRITES
ON
THE
BUS
CHANGING
THE
CODE
TO
SEPARATE
THESE
BUS
TRANSACTIONS
INTO
READ
PHASES
AND
WRITE
PHASES
CAN
HELP
PERFORMANCE
NOTE
HOWEVER
THAT
THE
ORDER
OF
READ
AND
WRITE
OPERATIONS
ON
THE
BUS
IS
NOT
THE
SAME
AS
IT
APPEARS
IN
THE
PROGRAM
BUS
LATENCY
FOR
FETCHING
A
CACHE
LINE
OF
DATA
CAN
VARY
AS
A
FUNCTION
OF
THE
ACCESS
STRIDE
OF
DATA
REFERENCES
IN
GENERAL
BUS
LATENCY
WILL
INCREASE
IN
RESPONSE
TO
INCREASING
VALUES
OF
THE
STRIDE
OF
SUCCESSIVE
CACHE
MISSES
INDEPENDENTLY
BUS
LATENCY
WILL
ALSO
INCREASE
AS
A
FUNCTION
OF
INCREASING
BUS
QUEUE
DEPTHS
THE
NUMBER
OF
OUTSTANDING
BUS
REQUESTS
OF
A
GIVEN
TRANSACTION
TYPE
THE
COMBINATION
OF
THESE
TWO
TRENDS
CAN
BE
HIGHLY
NON
LINEAR
IN
THAT
BUS
LATENCY
OF
LARGE
STRIDE
BANDWIDTH
SENSITIVE
SITUATIONS
ARE
SUCH
THAT
EFFECTIVE
THROUGHPUT
OF
THE
BUS
SYSTEM
FOR
DATA
PARALLEL
ACCESSES
CAN
BE
SIGNIFICANTLY
LESS
THAN
THE
EFFECTIVE
THROUGHPUT
OF
SMALL
STRIDE
BANDWIDTH
SENSITIVE
SITUATIONS
TO
MINIMIZE
THE
PER
ACCESS
COST
OF
MEMORY
TRAFFIC
OR
AMORTIZE
RAW
MEMORY
LATENCY
EFFECTIVELY
SOFTWARE
SHOULD
CONTROL
ITS
CACHE
MISS
PATTERN
TO
FAVOR
HIGHER
CONCENTRA
TION
OF
SMALLER
STRIDE
CACHE
MISSES
USER
SOURCE
CODING
RULE
H
IMPACT
H
GENERALITY
TO
ACHIEVE
EFFECTIVE
AMORTIZATION
OF
BUS
LATENCY
SOFTWARE
SHOULD
FAVOR
DATA
ACCESS
PATTERNS
THAT
RESULT
IN
HIGHER
CONCENTRATIONS
OF
CACHE
MISS
PATTERNS
WITH
CACHE
MISS
STRIDES
THAT
ARE
SIGNIFICANTLY
SMALLER
THAN
HALF
THE
HARDWARE
PREFETCH
TRIGGER
THRESHOLD
NON
TEMPORAL
STORE
BUS
TRAFFIC
PEAK
SYSTEM
BUS
BANDWIDTH
IS
SHARED
BY
SEVERAL
TYPES
OF
BUS
ACTIVITIES
INCLUDING
READS
FROM
MEMORY
READS
FOR
OWNERSHIP
OF
A
CACHE
LINE
AND
WRITES
THE
DATA
TRANSFER
RATE
FOR
BUS
WRITE
TRANSACTIONS
IS
HIGHER
IF
BYTES
ARE
WRITTEN
OUT
TO
THE
BUS
AT
A
TIME
TYPICALLY
BUS
WRITES
TO
WRITEBACK
WB
MEMORY
MUST
SHARE
THE
SYSTEM
BUS
BAND
WIDTH
WITH
READ
FOR
OWNERSHIP
RFO
TRAFFIC
NON
TEMPORAL
STORES
DO
NOT
REQUIRE
RFO
TRAFFIC
THEY
DO
REQUIRE
CARE
IN
MANAGING
THE
ACCESS
PATTERNS
IN
ORDER
TO
ENSURE
BYTES
ARE
EVICTED
AT
ONCE
RATHER
THAN
EVICTING
SEVERAL
BYTE
CHUNKS
ALTHOUGH
THE
DATA
BANDWIDTH
OF
FULL
BYTE
BUS
WRITES
DUE
TO
NON
TEMPORAL
STORES
IS
TWICE
THAT
OF
BUS
WRITES
TO
WB
MEMORY
TRANSFERRING
BYTE
CHUNKS
WASTES
BUS
REQUEST
BANDWIDTH
AND
DELIVERS
SIGNIFICANTLY
LOWER
DATA
BANDWIDTH
THIS
DIFFERENCE
IS
DEPICTED
IN
EXAMPLES
AND
EXAMPLE
USING
NON
TEMPORAL
STORES
AND
BYTE
BUS
WRITE
TRANSACTIONS
DEFINE
STRIDESIZE
LEA
ECX
MOV
EDX
XOR
EAX
EAX
SLLOOP
MOVNTPS
XMMWORD
PTR
ECX
EAX
MOVNTPS
XMMWORD
PTR
ECX
EAX
MOVNTPS
XMMWORD
PTR
ECX
EAX
MOVNTPS
XMMWORD
PTR
ECX
EAX
BYTES
IS
WRITTEN
IN
ONE
BUS
TRANSACTION
ADD
EAX
STRIDESIZE
CMP
EAX
EDX
JL
SLLOOP
EXAMPLE
ON
TEMPORAL
STORES
AND
PARTIAL
BUS
WRITE
TRANSACTIONS
DEFINE
STRIDESIZE
LEA
ECX
MOV
EDX
XOR
EAX
EAX
SLLOOP
MOVNTPS
XMMWORD
PTR
ECX
EAX
MOVNTPS
XMMWORD
PTR
ECX
EAX
MOVNTPS
XMMWORD
PTR
ECX
EAX
STORING
BYTES
RESULTS
IN
BUS
PARTIAL
TRANSACTIONS
ADD
EAX
STRIDESIZE
CMP
EAX
EDX
PREFETCHING
RECENT
INTEL
PROCESSOR
FAMILIES
EMPLOY
SEVERAL
PREFETCHING
MECHANISMS
TO
ACCELERATE
THE
MOVEMENT
OF
DATA
OR
CODE
AND
IMPROVE
PERFORMANCE
HARDWARE
INSTRUCTION
PREFETCHER
SOFTWARE
PREFETCH
FOR
DATA
HARDWARE
PREFETCH
FOR
CACHE
LINES
OF
DATA
OR
INSTRUCTIONS
HARDWARE
INSTRUCTION
FETCHING
AND
SOFTWARE
PREFETCHING
IN
PROCESSOR
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
THE
HARDWARE
INSTRUCTION
FETCHER
READS
INSTRUCTIONS
BYTES
AT
A
TIME
INTO
THE
BYTE
INSTRUCTION
STREAMING
BUFFERS
INSTRUCTION
FETCHING
FOR
INTEL
CORE
MICROARCHITECTURE
IS
DISCUSSED
IN
SECTION
SOFTWARE
PREFETCHING
REQUIRES
A
PROGRAMMER
TO
USE
PREFETCH
HINT
INSTRUCTIONS
AND
ANTICIPATE
SOME
SUITABLE
TIMING
AND
LOCATION
OF
CACHE
MISSES
IN
INTEL
CORE
MICROARCHITECTURE
SOFTWARE
PREFETCH
INSTRUCTIONS
CAN
PREFETCH
BEYOND
PAGE
BOUNDARIES
AND
CAN
PERFORM
ONE
TO
FOUR
PAGE
WALKS
SOFTWARE
PREFETCH
INSTRUCTIONS
ISSUED
ON
FILL
BUFFER
ALLOCATIONS
RETIRE
AFTER
THE
PAGE
WALK
COMPLETES
AND
THE
DCU
MISS
IS
DETECTED
SOFTWARE
PREFETCH
INSTRUCTIONS
CAN
TRIGGER
ALL
HARDWARE
PREFETCHERS
IN
THE
SAME
MANNER
AS
DO
REGULAR
LOADS
SOFTWARE
PREFETCH
OPERATIONS
WORK
THE
SAME
WAY
AS
DO
LOAD
FROM
MEMORY
OPERA
TIONS
WITH
THE
FOLLOWING
EXCEPTIONS
SOFTWARE
PREFETCH
INSTRUCTIONS
RETIRE
AFTER
VIRTUAL
TO
PHYSICAL
ADDRESS
TRANSLATION
IS
COMPLETED
IF
AN
EXCEPTION
SUCH
AS
PAGE
FAULT
IS
REQUIRED
TO
PREFETCH
THE
DATA
THEN
THE
SOFTWARE
PREFETCH
INSTRUCTION
RETIRES
WITHOUT
PREFETCHING
DATA
SOFTWARE
AND
HARDWARE
PREFETCHING
IN
PRIOR
MICROARCHITECTURES
PENTIUM
AND
INTEL
XEON
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
INTRO
DUCED
HARDWARE
PREFETCHING
IN
ADDITION
TO
SOFTWARE
PREFETCHING
THE
HARDWARE
PREFETCHER
OPERATES
TRANSPARENTLY
TO
FETCH
DATA
AND
INSTRUCTION
STREAMS
FROM
MEMORY
WITHOUT
REQUIRING
PROGRAMMER
INTERVENTION
SUBSEQUENT
MICROARCHITECTURES
CONTINUE
TO
IMPROVE
AND
ADD
FEATURES
TO
THE
HARDWARE
PREFETCHING
MECHANISMS
EARLIER
IMPLEMENTATIONS
OF
HARDWARE
PREFETCHING
MECHANISMS
FOCUS
ON
PREFETCHING
DATA
AND
INSTRUCTION
FROM
MEMORY
TO
MORE
RECENT
IMPLEMENTATIONS
PROVIDE
ADDI
TIONAL
FEATURES
TO
PREFETCH
DATA
FROM
TO
IN
INTEL
NETBURST
MICROARCHITECTURE
THE
HARDWARE
PREFETCHER
CAN
TRACK
INDEPEN
DENT
STREAMS
THE
PENTIUM
M
PROCESSOR
ALSO
PROVIDES
A
HARDWARE
PREFETCHER
FOR
DATA
IT
CAN
TRACK
SEPARATE
STREAMS
IN
THE
FORWARD
DIRECTION
AND
STREAMS
IN
THE
BACKWARD
DIREC
TION
THE
PROCESSOR
PREFETCHNTA
INSTRUCTION
ALSO
FETCHES
BYTES
INTO
THE
FIRST
LEVEL
DATA
CACHE
WITHOUT
POLLUTING
THE
SECOND
LEVEL
CACHE
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
PROVIDE
MORE
ADVANCED
HARDWARE
PREFETCHERS
FOR
DATA
THAN
PENTIUM
M
PROCESSORS
KEY
DIFFERENCES
ARE
SUMMARIZED
IN
TABLE
ALTHOUGH
THE
HARDWARE
PREFETCHER
OPERATES
TRANSPARENTLY
REQUIRING
NO
INTERVENTION
BY
THE
PROGRAMMER
IT
OPERATES
MOST
EFFICIENTLY
IF
THE
PROGRAMMER
SPECIFICALLY
TAILORS
DATA
ACCESS
PATTERNS
TO
SUIT
ITS
CHARACTERISTICS
IT
FAVORS
SMALL
STRIDE
CACHE
MISS
PATTERNS
OPTIMIZING
DATA
ACCESS
PATTERNS
TO
SUIT
THE
HARDWARE
PREFETCHER
IS
HIGHLY
RECOMMENDED
AND
SHOULD
BE
A
HIGHER
PRIORITY
CONSIDERATION
THAN
USING
SOFT
WARE
PREFETCH
INSTRUCTIONS
THE
HARDWARE
PREFETCHER
IS
BEST
FOR
SMALL
STRIDE
DATA
ACCESS
PATTERNS
IN
EITHER
DIREC
TION
WITH
A
CACHE
MISS
STRIDE
NOT
FAR
FROM
BYTES
THIS
IS
TRUE
FOR
DATA
ACCESSES
TO
ADDRESSES
THAT
ARE
EITHER
KNOWN
OR
UNKNOWN
AT
THE
TIME
OF
ISSUING
THE
LOAD
OPERA
TIONS
SOFTWARE
PREFETCH
CAN
COMPLEMENT
THE
HARDWARE
PREFETCHER
IF
USED
CAREFULLY
THERE
IS
A
TRADE
OFF
TO
MAKE
BETWEEN
HARDWARE
AND
SOFTWARE
PREFETCHING
THIS
PERTAINS
TO
APPLICATION
CHARACTERISTICS
SUCH
AS
REGULARITY
AND
STRIDE
OF
ACCESSES
BUS
BANDWIDTH
ISSUE
BANDWIDTH
THE
LATENCY
OF
LOADS
ON
THE
CRITICAL
PATH
AND
WHETHER
ACCESS
PATTERNS
ARE
SUITABLE
FOR
NON
TEMPORAL
PREFETCH
WILL
ALSO
HAVE
AN
IMPACT
FOR
A
DETAILED
DESCRIPTION
OF
HOW
TO
USE
PREFETCHING
SEE
CHAPTER
OPTIMIZING
CACHE
USAGE
CHAPTER
OPTIMIZING
FOR
SIMD
INTEGER
APPLICATIONS
CONTAINS
AN
EXAMPLE
THAT
USES
SOFTWARE
PREFETCH
TO
IMPLEMENT
A
MEMORY
COPY
ALGORITHM
TUNING
SUGGESTION
IF
A
LOAD
IS
FOUND
TO
MISS
FREQUENTLY
EITHER
INSERT
A
PREFETCH
BEFORE
IT
OR
IF
ISSUE
BANDWIDTH
IS
A
CONCERN
MOVE
THE
LOAD
UP
TO
EXECUTE
EARLIER
HARDWARE
PREFETCHING
FOR
FIRST
LEVEL
DATA
CACHE
THE
HARDWARE
PREFETCHING
MECHANISM
FOR
IN
INTEL
CORE
MICROARCHITECTURE
IS
DISCUSSED
IN
SECTION
A
SIMILAR
PREFETCH
MECHANISM
IS
ALSO
AVAILABLE
TO
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
WITH
CPUID
SIGNATURE
OF
FAMILY
AND
MODEL
EXAMPLE
DEPICTS
A
TECHNIQUE
TO
TRIGGER
HARDWARE
PREFETCH
THE
CODE
DEMON
STRATES
TRAVERSING
A
LINKED
LIST
AND
PERFORMING
SOME
COMPUTATIONAL
WORK
ON
MEMBERS
OF
EACH
ELEMENT
THAT
RESIDE
IN
DIFFERENT
CACHE
LINES
EACH
ELEMENT
IS
OF
SIZE
BYTES
THE
TOTAL
SIZE
OF
ALL
ELEMENTS
IS
LARGER
THAN
CAN
BE
FITTED
IN
THE
CACHE
EXAMPLE
USING
DCU
HARDWARE
PREFETCH
ORIGINAL
CODE
MODIFIED
SEQUENCE
BENEFIT
FROM
PREFETCH
MOV
EBX
DWORD
PTR
FIRST
MOV
EBX
DWORD
PTR
FIRST
XOR
EAX
EAX
XOR
EAX
EAX
MOV
EAX
EBX
MOV
EAX
EBX
MOV
ECX
MOV
EAX
EBX
MOV
EAX
EBX
ADD
EAX
EAX
MOV
ECX
AND
EAX
SUB
ECX
ADD
EAX
EAX
JNZ
AND
EAX
SUB
ECX
JNZ
MOV
EAX
EBX
MOV
ECX
ADD
EAX
EAX
AND
EAX
SUB
ECX
JNZ
MOV
EAX
EBX
MOV
ECX
ADD
EAX
EAX
AND
EAX
SUB
ECX
JNZ
MOV
EBX
EBX
TEST
EBX
EBX
JNZ
MOV
EBX
EBX
TEST
EBX
EBX
JNZ
THE
ADDITIONAL
INSTRUCTIONS
TO
LOAD
DATA
FROM
ONE
MEMBER
IN
THE
MODIFIED
SEQUENCE
CAN
TRIGGER
THE
DCU
HARDWARE
PREFETCH
MECHANISMS
TO
PREFETCH
DATA
IN
THE
NEXT
CACHE
LINE
ENABLING
THE
WORK
ON
THE
SECOND
MEMBER
TO
COMPLETE
SOONER
SOFTWARE
CAN
GAIN
FROM
THE
FIRST
LEVEL
DATA
CACHE
PREFETCHERS
IN
TWO
CASES
IF
DATA
IS
NOT
IN
THE
SECOND
LEVEL
CACHE
THE
FIRST
LEVEL
DATA
CACHE
PREFETCHER
ENABLES
EARLY
TRIGGER
OF
THE
SECOND
LEVEL
CACHE
PREFETCHER
IF
DATA
IS
IN
THE
SECOND
LEVEL
CACHE
AND
NOT
IN
THE
FIRST
LEVEL
DATA
CACHE
THEN
THE
FIRST
LEVEL
DATA
CACHE
PREFETCHER
TRIGGERS
EARLIER
DATA
BRING
UP
OF
SEQUENTIAL
CACHE
LINE
TO
THE
FIRST
LEVEL
DATA
CACHE
THERE
ARE
SITUATIONS
THAT
SOFTWARE
SHOULD
PAY
ATTENTION
TO
A
POTENTIAL
SIDE
EFFECT
OF
TRIGGERING
UNNECESSARY
DCU
HARDWARE
PREFETCHES
IF
A
LARGE
DATA
STRUCTURE
WITH
MANY
MEMBERS
SPANNING
MANY
CACHE
LINES
IS
ACCESSED
IN
WAYS
THAT
ONLY
A
FEW
OF
ITS
MEMBERS
ARE
ACTUALLY
REFERENCED
BUT
THERE
ARE
MULTIPLE
PAIR
ACCESSES
TO
THE
SAME
CACHE
LINE
THE
DCU
HARDWARE
PREFETCHER
CAN
TRIGGER
FETCHING
OF
CACHE
LINES
THAT
ARE
NOT
NEEDED
IN
EXAMPLE
REFERENCES
TO
THE
PTS
ARRAY
AND
ALTPTS
WILL
TRIGGER
DCU
PREFETCH
TO
FETCH
ADDITIONAL
CACHE
LINES
THAT
WON
T
BE
NEEDED
IF
SIGNIFICANT
NEGATIVE
PERFORMANCE
IMPACT
IS
DETECTED
DUE
TO
DCU
HARDWARE
PREFETCH
ON
A
PORTION
OF
THE
CODE
SOFTWARE
CAN
TRY
TO
REDUCE
THE
SIZE
OF
THAT
CONTEMPORANEOUS
WORKING
SET
TO
BE
LESS
THAN
HALF
OF
THE
CACHE
EXAMPLE
AVOID
CAUSING
DCU
HARDWARE
PREFETCH
TO
FETCH
UN
NEEDED
LINES
WHILE
CURRBOND
NULL
MYATOM
CURRBOND
MYATOM
CURRBOND
IF
CURRSTEP
LASTSTEP
CURRSTEP
LASTSTEP
CURRSTEP
CURRSTEP
DOUBLE
UX
PTS
X
PTS
X
DOUBLE
UY
PTS
Y
PTS
Y
DOUBLE
UZ
PTS
Z
PTS
Z
AUXPTS
X
UX
AUXPTS
Y
UY
AUXPTS
Z
UZ
AUXPTS
X
UX
AUXPTS
Y
UY
AUXPTS
Z
UZ
CURRBOND
CURRBOND
NEXT
TO
FULLY
BENEFIT
FROM
THESE
PREFETCHERS
ORGANIZE
AND
ACCESS
THE
DATA
USING
ONE
OF
THE
FOLLOWING
METHODS
METHOD
ORGANIZE
THE
DATA
SO
CONSECUTIVE
ACCESSES
CAN
USUALLY
BE
FOUND
IN
THE
SAME
KBYTE
PAGE
ACCESS
THE
DATA
IN
CONSTANT
STRIDES
FORWARD
OR
BACKWARD
IP
PREFETCHER
METHOD
ORGANIZE
THE
DATA
IN
CONSECUTIVE
LINES
ACCESS
THE
DATA
IN
INCREASING
ADDRESSES
IN
SEQUENTIAL
CACHE
LINES
EXAMPLE
DEMONSTRATES
ACCESSES
TO
SEQUENTIAL
CACHE
LINES
THAT
CAN
BENEFIT
FROM
THE
FIRST
LEVEL
CACHE
PREFETCHER
EXAMPLE
TECHNIQUE
FOR
USING
HARDWARE
PREFETCH
UNSIGNED
INT
J
A
B
FOR
J
J
NUM
J
A
J
B
J
USE
THESE
TWO
VALUES
BY
ELEVATING
THE
LOAD
OPERATIONS
FROM
MEMORY
TO
THE
BEGINNING
OF
EACH
ITERATION
IT
IS
LIKELY
THAT
A
SIGNIFICANT
PART
OF
THE
LATENCY
OF
THE
PAIR
CACHE
LINE
TRANSFER
FROM
MEMORY
TO
THE
SECOND
LEVEL
CACHE
WILL
BE
IN
PARALLEL
WITH
THE
TRANSFER
OF
THE
FIRST
CACHE
LINE
THE
IP
PREFETCHER
USES
ONLY
THE
LOWER
BITS
OF
THE
ADDRESS
TO
DISTINGUISH
A
SPECIFIC
ADDRESS
IF
THE
CODE
SIZE
OF
A
LOOP
IS
BIGGER
THAN
BYTES
TWO
LOADS
MAY
APPEAR
SIMILAR
IN
THE
LOWEST
BITS
AND
THE
IP
PREFETCHER
WILL
BE
RESTRICTED
THEREFORE
IF
YOU
HAVE
A
LOOP
BIGGER
THAN
BYTES
MAKE
SURE
THAT
NO
TWO
LOADS
HAVE
THE
SAME
LOWEST
BITS
IN
ORDER
TO
USE
THE
IP
PREFETCHER
HARDWARE
PREFETCHING
FOR
SECOND
LEVEL
CACHE
THE
INTEL
CORE
MICROARCHITECTURE
CONTAINS
TWO
SECOND
LEVEL
CACHE
PREFETCHERS
STREAMER
LOADS
DATA
OR
INSTRUCTIONS
FROM
MEMORY
TO
THE
SECOND
LEVEL
CACHE
TO
USE
THE
STREAMER
ORGANIZE
THE
DATA
OR
INSTRUCTIONS
IN
BLOCKS
OF
BYTES
ALIGNED
ON
BYTES
THE
FIRST
ACCESS
TO
ONE
OF
THE
TWO
CACHE
LINES
IN
THIS
BLOCK
WHILE
IT
IS
IN
MEMORY
TRIGGERS
THE
STREAMER
TO
PREFETCH
THE
PAIR
LINE
TO
SOFTWARE
THE
STREAMER
FUNCTIONALITY
IS
SIMILAR
TO
THE
ADJACENT
CACHE
LINE
PREFETCH
MECHANISM
FOUND
IN
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
DATA
PREFETCH
LOGIC
DPL
DPL
AND
STREAMER
ARE
TRIGGERED
ONLY
BY
WRITEBACK
MEMORY
TYPE
THEY
PREFETCH
ONLY
INSIDE
PAGE
BOUNDARY
KBYTES
BOTH
PREFETCHERS
CAN
BE
TRIGGERED
BY
SOFTWARE
PREFETCH
INSTRUCTIONS
AND
BY
PREFETCH
REQUEST
FROM
DCU
PREFETCHERS
DPL
CAN
ALSO
BE
TRIGGERED
BY
READ
FOR
OWNERSHIP
RFO
OPERATIONS
THE
STREAMER
CAN
ALSO
BE
TRIGGERED
BY
DPL
REQUESTS
FOR
CACHE
MISSES
SOFTWARE
CAN
GAIN
FROM
ORGANIZING
DATA
BOTH
ACCORDING
TO
THE
INSTRUCTION
POINTER
AND
ACCORDING
TO
LINE
STRIDES
FOR
EXAMPLE
FOR
MATRIX
CALCULATIONS
COLUMNS
CAN
BE
PREFETCHED
BY
IP
BASED
PREFETCHES
AND
ROWS
CAN
BE
PREFETCHED
BY
DPL
AND
THE
STREAMER
CACHEABILITY
INSTRUCTIONS
PROVIDES
ADDITIONAL
CACHEABILITY
INSTRUCTIONS
THAT
EXTEND
THOSE
PROVIDED
IN
SSE
THE
NEW
CACHEABILITY
INSTRUCTIONS
INCLUDE
NEW
STREAMING
STORE
INSTRUCTIONS
NEW
CACHE
LINE
FLUSH
INSTRUCTION
NEW
MEMORY
FENCING
INSTRUCTIONS
FOR
MORE
INFORMATION
SEE
CHAPTER
OPTIMIZING
CACHE
USAGE
REP
PREFIX
AND
DATA
MOVEMENT
THE
REP
PREFIX
IS
COMMONLY
USED
WITH
STRING
MOVE
INSTRUCTIONS
FOR
MEMORY
RELATED
LIBRARY
FUNCTIONS
SUCH
AS
MEMCPY
USING
REP
MOVSD
OR
MEMSET
USING
REP
STOS
THESE
STRING
MOV
INSTRUCTIONS
WITH
THE
REP
PREFIXES
ARE
IMPLEMENTED
IN
MS
ROM
AND
HAVE
SEVERAL
IMPLEMENTATION
VARIANTS
WITH
DIFFERENT
PERFORMANCE
LEVELS
THE
SPECIFIC
VARIANT
OF
THE
IMPLEMENTATION
IS
CHOSEN
AT
EXECUTION
TIME
BASED
ON
DATA
LAYOUT
ALIGNMENT
AND
THE
COUNTER
ECX
VALUE
FOR
EXAMPLE
MOVSB
STOSB
WITH
THE
REP
PREFIX
SHOULD
BE
USED
WITH
COUNTER
VALUE
LESS
THAN
OR
EQUAL
TO
THREE
FOR
BEST
PERFORMANCE
STRING
MOVE
STORE
INSTRUCTIONS
HAVE
MULTIPLE
DATA
GRANULARITIES
FOR
EFFICIENT
DATA
MOVEMENT
LARGER
DATA
GRANULARITIES
ARE
PREFERABLE
THIS
MEANS
BETTER
EFFICIENCY
CAN
BE
ACHIEVED
BY
DECOMPOSING
AN
ARBITRARY
COUNTER
VALUE
INTO
A
NUMBER
OF
DOUBLE
WORDS
PLUS
SINGLE
BYTE
MOVES
WITH
A
COUNT
VALUE
LESS
THAN
OR
EQUAL
TO
BECAUSE
SOFTWARE
CAN
USE
SIMD
DATA
MOVEMENT
INSTRUCTIONS
TO
MOVE
BYTES
AT
A
TIME
THE
FOLLOWING
PARAGRAPHS
DISCUSS
GENERAL
GUIDELINES
FOR
DESIGNING
AND
IMPLE
MENTING
HIGH
PERFORMANCE
LIBRARY
FUNCTIONS
SUCH
AS
MEMCPY
MEMSET
AND
MEMMOVE
FOUR
FACTORS
ARE
TO
BE
CONSIDERED
THROUGHPUT
PER
ITERATION
IF
TWO
PIECES
OF
CODE
HAVE
APPROXIMATELY
IDENTICAL
PATH
LENGTHS
EFFICIENCY
FAVORS
CHOOSING
THE
INSTRUCTION
THAT
MOVES
LARGER
PIECES
OF
DATA
PER
ITERATION
ALSO
SMALLER
CODE
SIZE
PER
ITERATION
WILL
IN
GENERAL
REDUCE
OVERHEAD
AND
IMPROVE
THROUGHPUT
SOMETIMES
THIS
MAY
INVOLVE
A
COMPARISON
OF
THE
RELATIVE
OVERHEAD
OF
AN
ITERATIVE
LOOP
STRUCTURE
VERSUS
USING
REP
PREFIX
FOR
ITERATION
ADDRESS
ALIGNMENT
DATA
MOVEMENT
INSTRUCTIONS
WITH
HIGHEST
THROUGHPUT
USUALLY
HAVE
ALIGNMENT
RESTRICTIONS
OR
THEY
OPERATE
MORE
EFFICIENTLY
IF
THE
DESTINATION
ADDRESS
IS
ALIGNED
TO
ITS
NATURAL
DATA
SIZE
SPECIFICALLY
BYTE
MOVES
NEED
TO
ENSURE
THE
DESTINATION
ADDRESS
IS
ALIGNED
TO
BYTE
BOUNDARIES
AND
BYTES
MOVES
PERFORM
BETTER
IF
THE
DESTINATION
ADDRESS
IS
ALIGNED
TO
BYTE
BOUNDARIES
FREQUENTLY
MOVING
AT
DOUBLEWORD
GRANULARITY
PERFORMS
BETTER
WITH
ADDRESSES
THAT
ARE
BYTE
ALIGNED
REP
STRING
MOVE
VS
SIMD
MOVE
IMPLEMENTING
GENERAL
PURPOSE
MEMORY
FUNCTIONS
USING
SIMD
EXTENSIONS
USUALLY
REQUIRES
ADDING
SOME
PROLOG
CODE
TO
ENSURE
THE
AVAILABILITY
OF
SIMD
INSTRUCTIONS
PREAMBLE
CODE
TO
FACILITATE
ALIGNED
DATA
MOVEMENT
REQUIREMENTS
AT
RUNTIME
THROUGHPUT
COMPARISON
MUST
ALSO
TAKE
INTO
CONSIDERATION
THE
OVERHEAD
OF
THE
PROLOG
WHEN
CONSIDERING
A
REP
STRING
IMPLEMENTATION
VERSUS
A
SIMD
APPROACH
CACHE
EVICTION
IF
THE
AMOUNT
OF
DATA
TO
BE
PROCESSED
BY
A
MEMORY
ROUTINE
APPROACHES
HALF
THE
SIZE
OF
THE
LAST
LEVEL
ON
DIE
CACHE
TEMPORAL
LOCALITY
OF
THE
CACHE
MAY
SUFFER
USING
STREAMING
STORE
INSTRUCTIONS
FOR
EXAMPLE
MOVNTQ
MOVNTDQ
CAN
MINIMIZE
THE
EFFECT
OF
FLUSHING
THE
CACHE
THE
THRESHOLD
TO
START
USING
A
STREAMING
STORE
DEPENDS
ON
THE
SIZE
OF
THE
LAST
LEVEL
CACHE
DETERMINE
THE
SIZE
USING
THE
DETERMINISTIC
CACHE
PARAMETER
LEAF
OF
CPUID
TECHNIQUES
FOR
USING
STREAMING
STORES
FOR
IMPLEMENTING
A
MEMSET
TYPE
LIBRARY
MUST
ALSO
CONSIDER
THAT
THE
APPLICATION
CAN
BENEFIT
FROM
THIS
TECHNIQUE
ONLY
IF
IT
HAS
NO
IMMEDIATE
NEED
TO
REFERENCE
THE
TARGET
ADDRESSES
THIS
ASSUMPTION
IS
EASILY
UPHELD
WHEN
TESTING
A
STREAMING
STORE
IMPLEMENTATION
ON
A
MICRO
BENCHMARK
CONFIGURATION
BUT
VIOLATED
IN
A
FULL
SCALE
APPLICATION
SITUATION
WHEN
APPLYING
GENERAL
HEURISTICS
TO
THE
DESIGN
OF
GENERAL
PURPOSE
HIGH
PERFOR
MANCE
LIBRARY
ROUTINES
THE
FOLLOWING
GUIDELINES
CAN
ARE
USEFUL
WHEN
OPTIMIZING
AN
ARBITRARY
COUNTER
VALUE
N
AND
ADDRESS
ALIGNMENT
DIFFERENT
TECHNIQUES
MAY
BE
NECES
SARY
FOR
OPTIMAL
PERFORMANCE
DEPENDING
ON
THE
MAGNITUDE
OF
N
WHEN
N
IS
LESS
THAN
SOME
SMALL
COUNT
WHERE
THE
SMALL
COUNT
THRESHOLD
WILL
VARY
BETWEEN
MICROARCHITECTURES
EMPIRICALLY
MAY
BE
A
GOOD
VALUE
WHEN
OPTIMIZING
FOR
INTEL
NETBURST
MICROARCHITECTURE
EACH
CASE
CAN
BE
CODED
DIRECTLY
WITHOUT
THE
OVERHEAD
OF
A
LOOPING
STRUCTURE
FOR
EXAMPLE
BYTES
CAN
BE
PROCESSED
USING
TWO
MOVSD
INSTRUCTIONS
EXPLICITLY
AND
A
MOVSB
WITH
REP
COUNTER
EQUALING
WHEN
N
IS
NOT
SMALL
BUT
STILL
LESS
THAN
SOME
THRESHOLD
VALUE
WHICH
MAY
VARY
FOR
DIFFERENT
MICRO
ARCHITECTURES
BUT
CAN
BE
DETERMINED
EMPIRICALLY
AN
SIMD
IMPLEMENTATION
USING
RUN
TIME
CPUID
AND
ALIGNMENT
PROLOG
WILL
LIKELY
DELIVER
LESS
THROUGHPUT
DUE
TO
THE
OVERHEAD
OF
THE
PROLOG
A
REP
STRING
IMPLEMENTATION
SHOULD
FAVOR
USING
A
REP
STRING
OF
DOUBLEWORDS
TO
IMPROVE
ADDRESS
ALIGNMENT
A
SMALL
PIECE
OF
PROLOG
CODE
USING
MOVSB
STOSB
WITH
A
COUNT
LESS
THAN
CAN
BE
USED
TO
PEEL
OFF
THE
NON
ALIGNED
DATA
MOVES
BEFORE
STARTING
TO
USE
MOVSD
STOSD
WHEN
N
IS
LESS
THAN
HALF
THE
SIZE
OF
LAST
LEVEL
CACHE
THROUGHPUT
CONSIDERATION
MAY
FAVOR
EITHER
AN
APPROACH
USING
A
REP
STRING
WITH
THE
LARGEST
DATA
GRANULARITY
BECAUSE
A
REP
STRING
HAS
LITTLE
OVERHEAD
FOR
LOOP
ITERATION
AND
THE
BRANCH
MISPREDICTION
OVERHEAD
IN
THE
PROLOG
EPILOGUE
CODE
TO
HANDLE
ADDRESS
ALIGNMENT
IS
AMORTIZED
OVER
MANY
ITERATIONS
AN
ITERATIVE
APPROACH
USING
THE
INSTRUCTION
WITH
LARGEST
DATA
GRANULARITY
WHERE
THE
OVERHEAD
FOR
SIMD
FEATURE
DETECTION
ITERATION
OVERHEAD
AND
PROLOG
EPILOGUE
FOR
ALIGNMENT
CONTROL
CAN
BE
MINIMIZED
THE
TRADE
OFF
BETWEEN
THESE
APPROACHES
MAY
DEPEND
ON
THE
MICROARCHITECTURE
AN
EXAMPLE
OF
MEMSET
IMPLEMENTED
USING
STOSD
FOR
ARBITRARY
COUNTER
VALUE
WITH
THE
DESTINATION
ADDRESS
ALIGNED
TO
DOUBLEWORD
BOUNDARY
IN
BIT
MODE
IS
SHOWN
IN
EXAMPLE
WHEN
N
IS
LARGER
THAN
HALF
THE
SIZE
OF
THE
LAST
LEVEL
CACHE
USING
BYTE
GRANULARITY
STREAMING
STORES
WITH
PROLOG
EPILOG
FOR
ADDRESS
ALIGNMENT
WILL
LIKELY
BE
MORE
EFFICIENT
IF
THE
DESTINATION
ADDRESSES
WILL
NOT
BE
REFERENCED
IMMEDIATELY
AFTERWARDS
EXAMPLE
REP
STOSD
WITH
ARBITRARY
COUNT
SIZE
AND
BYTE
ALIGNED
DESTINATION
A
C
EXAMPLE
OF
MEMSET
EQUIVALENT
IMPLEMENTATION
USING
REP
STOSD
VOID
MEMSET
VOID
DST
INT
C
SIZE
CHAR
D
CHAR
DST
I
FOR
I
I
SIZE
I
D
CHAR
C
PUSH
EDI
MOVZX
EAX
BYTE
PTR
ESP
MOV
ECX
EAX
SHL
ECX
OR
ECX
EAX
MOV
ECX
EAX
SHL
ECX
OR
EAX
ECX
MOV
EDI
ESP
BYTE
ALIGNED
MOV
ECX
ESP
BYTE
COUNT
SHR
ECX
DO
DWORD
CMP
ECX
JLE
TEST
EDI
JZ
STOSD
PEEL
OFF
ONE
DWORD
DEC
ECX
BYTE
ALIGNED
REP
STOSD
MOV
ECX
ESP
AND
ECX
DO
COUNT
REP
STOSB
OPTIMAL
WITH
POP
EDI
RET
MEMORY
ROUTINES
IN
THE
RUNTIME
LIBRARY
GENERATED
BY
INTEL
COMPILERS
ARE
OPTIMIZED
ACROSS
A
WIDE
RANGE
OF
ADDRESS
ALIGNMENTS
COUNTER
VALUES
AND
MICROARCHITECTURES
IN
MOST
CASES
APPLICATIONS
SHOULD
TAKE
ADVANTAGE
OF
THE
DEFAULT
MEMORY
ROUTINES
PROVIDED
BY
INTEL
COMPILERS
IN
SOME
SITUATIONS
THE
BYTE
COUNT
OF
THE
DATA
IS
KNOWN
BY
THE
CONTEXT
AS
OPPOSED
TO
BEING
KNOWN
BY
A
PARAMETER
PASSED
FROM
A
CALL
AND
ONE
CAN
TAKE
A
SIMPLER
APPROACH
THAN
THOSE
REQUIRED
FOR
A
GENERAL
PURPOSE
LIBRARY
ROUTINE
FOR
EXAMPLE
IF
THE
BYTE
COUNT
IS
ALSO
SMALL
USING
REP
MOVSB
STOSB
WITH
A
COUNT
LESS
THAN
FOUR
CAN
ENSURE
GOOD
ADDRESS
ALIGNMENT
AND
LOOP
UNROLLING
TO
FINISH
THE
REMAINING
DATA
USING
MOVSD
STOSD
CAN
REDUCE
THE
OVERHEAD
ASSOCIATED
WITH
ITERATION
USING
A
REP
PREFIX
WITH
STRING
MOVE
INSTRUCTIONS
CAN
PROVIDE
HIGH
PERFORMANCE
IN
THE
SITUATIONS
DESCRIBED
ABOVE
HOWEVER
USING
A
REP
PREFIX
WITH
STRING
SCAN
INSTRUCTIONS
SCASB
SCASW
SCASD
SCASQ
OR
COMPARE
INSTRUCTIONS
CMPSB
CMPSW
SMPSD
SMPSQ
IS
NOT
RECOMMENDED
FOR
HIGH
PERFORMANCE
CONSIDER
USING
SIMD
INSTRUCTIONS
INSTEAD
FLOATING
POINT
CONSIDERATIONS
WHEN
PROGRAMMING
FLOATING
POINT
APPLICATIONS
IT
IS
BEST
TO
START
WITH
A
HIGH
LEVEL
PROGRAMMING
LANGUAGE
SUCH
AS
C
C
OR
FORTRAN
MANY
COMPILERS
PERFORM
FLOATING
POINT
SCHEDULING
AND
OPTIMIZATION
WHEN
IT
IS
POSSIBLE
HOWEVER
IN
ORDER
TO
PRODUCE
OPTIMAL
CODE
THE
COMPILER
MAY
NEED
SOME
ASSISTANCE
GUIDELINES
FOR
OPTIMIZING
FLOATING
POINT
CODE
USER
SOURCE
CODING
RULE
M
IMPACT
M
GENERALITY
ENABLE
THE
COMPILER
USE
OF
SSE
OR
INSTRUCTIONS
WITH
APPROPRIATE
SWITCHES
FOLLOW
THIS
PROCEDURE
TO
INVESTIGATE
THE
PERFORMANCE
OF
YOUR
FLOATING
POINT
APPLICA
TION
UNDERSTAND
HOW
THE
COMPILER
HANDLES
FLOATING
POINT
CODE
LOOK
AT
THE
ASSEMBLY
DUMP
AND
SEE
WHAT
TRANSFORMS
ARE
ALREADY
PERFORMED
ON
THE
PROGRAM
STUDY
THE
LOOP
NESTS
IN
THE
APPLICATION
THAT
DOMINATE
THE
EXECUTION
TIME
DETERMINE
WHY
THE
COMPILER
IS
NOT
CREATING
THE
FASTEST
CODE
SEE
IF
THERE
IS
A
DEPENDENCE
THAT
CAN
BE
RESOLVED
DETERMINE
THE
PROBLEM
AREA
BUS
BANDWIDTH
CACHE
LOCALITY
TRACE
CACHE
BANDWIDTH
OR
INSTRUCTION
LATENCY
FOCUS
ON
OPTIMIZING
THE
PROBLEM
AREA
FOR
EXAMPLE
ADDING
PREFETCH
INSTRUCTIONS
WILL
NOT
HELP
IF
THE
BUS
IS
ALREADY
SATURATED
IF
TRACE
CACHE
BANDWIDTH
IS
THE
PROBLEM
ADDED
PREFETCH
ΜOPS
MAY
DEGRADE
PERFORMANCE
ALSO
IN
GENERAL
FOLLOW
THE
GENERAL
CODING
RECOMMENDATIONS
DISCUSSED
IN
THIS
CHAPTER
INCLUDING
BLOCKING
THE
CACHE
USING
PREFETCH
ENABLING
VECTORIZATION
UNROLLING
LOOPS
USER
SOURCE
CODING
RULE
H
IMPACT
ML
GENERALITY
MAKE
SURE
YOUR
APPLICATION
STAYS
IN
RANGE
TO
AVOID
DENORMAL
VALUES
UNDERFLOWS
OUT
OF
RANGE
NUMBERS
CAUSE
VERY
HIGH
OVERHEAD
USER
SOURCE
CODING
RULE
M
IMPACT
ML
GENERALITY
DO
NOT
USE
DOUBLE
PRECISION
UNLESS
NECESSARY
SET
THE
PRECISION
CONTROL
PC
FIELD
IN
THE
FPU
CONTROL
WORD
TO
SINGLE
PRECISION
THIS
ALLOWS
SINGLE
PRECISION
BIT
COMPUTATION
TO
COMPLETE
FASTER
ON
SOME
OPERATIONS
FOR
EXAMPLE
DIVIDES
DUE
TO
EARLY
OUT
HOWEVER
BE
CAREFUL
OF
INTRODUCING
MORE
THAN
A
TOTAL
OF
TWO
VALUES
FOR
THE
FLOATING
POINT
CONTROL
WORD
OR
THERE
WILL
BE
A
LARGE
PERFORMANCE
PENALTY
SEE
SECTION
USER
SOURCE
CODING
RULE
H
IMPACT
ML
GENERALITY
USE
FAST
FLOAT
TO
INT
ROUTINES
FISTTP
OR
INSTRUCTIONS
IF
CODING
THESE
ROUTINES
USE
THE
FISTTP
INSTRUCTION
IF
IS
AVAILABLE
OR
THE
AND
INSTRUCTIONS
IF
CODING
WITH
STREAMING
SIMD
EXTENSIONS
MANY
LIBRARIES
GENERATE
CODE
THAT
DOES
MORE
WORK
THAN
IS
NECESSARY
THE
FISTTP
INSTRUCTION
IN
CAN
CONVERT
FLOATING
POINT
VALUES
TO
BIT
BIT
OR
BIT
INTE
GERS
USING
TRUNCATION
WITHOUT
ACCESSING
THE
FLOATING
POINT
CONTROL
WORD
FCW
THE
INSTRUCTIONS
AND
SAVE
MANY
ΜOPS
AND
SOME
STORE
FORWARDING
DELAYS
OVER
SOME
COMPILER
IMPLEMENTATIONS
THIS
AVOIDS
CHANGING
THE
ROUNDING
MODE
USER
SOURCE
CODING
RULE
M
IMPACT
ML
GENERALITY
REMOVING
DATA
DEPENDENCE
ENABLES
THE
OUT
OF
ORDER
ENGINE
TO
EXTRACT
MORE
ILP
FROM
THE
CODE
WHEN
SUMMING
UP
THE
ELEMENTS
OF
AN
ARRAY
USE
PARTIAL
SUMS
INSTEAD
OF
A
SINGLE
ACCUMULATOR
FOR
EXAMPLE
TO
CALCULATE
Z
A
B
C
D
INSTEAD
OF
X
A
B
Y
X
C
Z
Y
D
USE
X
A
B
Y
C
D
Z
X
Y
USER
SOURCE
CODING
RULE
M
IMPACT
ML
GENERALITY
USUALLY
MATH
LIBRARIES
TAKE
ADVANTAGE
OF
THE
TRANSCENDENTAL
INSTRUCTIONS
FOR
EXAMPLE
FSIN
WHEN
EVALUATING
ELEMENTARY
FUNCTIONS
IF
THERE
IS
NO
CRITICAL
NEED
TO
EVALUATE
THE
TRANSCENDENTAL
FUNCTIONS
USING
THE
EXTENDED
PRECISION
OF
BITS
APPLICATIONS
SHOULD
CONSIDER
AN
ALTERNATE
SOFTWARE
BASED
APPROACH
SUCH
AS
A
LOOK
UP
TABLE
BASED
ALGORITHM
USING
INTERPOLATION
TECHNIQUES
IT
IS
POSSIBLE
TO
IMPROVE
TRANSCENDENTAL
PERFORMANCE
WITH
THESE
TECHNIQUES
BY
CHOOSING
THE
DESIRED
NUMERIC
PRECISION
AND
THE
SIZE
OF
THE
LOOK
UP
TABLE
AND
BY
TAKING
ADVANTAGE
OF
THE
PARALLELISM
OF
THE
SSE
AND
THE
INSTRUCTIONS
FLOATING
POINT
MODES
AND
EXCEPTIONS
WHEN
WORKING
WITH
FLOATING
POINT
NUMBERS
HIGH
SPEED
MICROPROCESSORS
FREQUENTLY
MUST
DEAL
WITH
SITUATIONS
THAT
NEED
SPECIAL
HANDLING
IN
HARDWARE
OR
CODE
FLOATING
POINT
EXCEPTIONS
THE
MOST
FREQUENT
CAUSE
OF
PERFORMANCE
DEGRADATION
IS
THE
USE
OF
MASKED
FLOATING
POINT
EXCEPTION
CONDITIONS
SUCH
AS
ARITHMETIC
OVERFLOW
ARITHMETIC
UNDERFLOW
DENORMALIZED
OPERAND
REFER
TO
CHAPTER
OF
INTEL
AND
IA
ARCHITECTURES
SOFTWARE
DEVELOPER
MANUAL
VOLUME
FOR
DEFINITIONS
OF
OVERFLOW
UNDERFLOW
AND
DENORMAL
EXCEPTIONS
DENORMALIZED
FLOATING
POINT
NUMBERS
IMPACT
PERFORMANCE
IN
TWO
WAYS
DIRECTLY
WHEN
ARE
USED
AS
OPERANDS
INDIRECTLY
WHEN
ARE
PRODUCED
AS
A
RESULT
OF
AN
UNDERFLOW
SITUATION
IF
A
FLOATING
POINT
APPLICATION
NEVER
UNDERFLOWS
THE
DENORMALS
CAN
ONLY
COME
FROM
FLOATING
POINT
CONSTANTS
USER
SOURCE
CODING
RULE
H
IMPACT
ML
GENERALITY
DENORMALIZED
FLOATING
POINT
CONSTANTS
SHOULD
BE
AVOIDED
AS
MUCH
AS
POSSIBLE
DENORMAL
AND
ARITHMETIC
UNDERFLOW
EXCEPTIONS
CAN
OCCUR
DURING
THE
EXECUTION
OF
INSTRUCTIONS
OR
SSE
INSTRUCTIONS
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
HANDLE
THESE
EXCEPTIONS
MORE
EFFICIENTLY
WHEN
EXECUTING
SSE
INSTRUCTIONS
AND
WHEN
SPEED
IS
MORE
IMPORTANT
THAN
COMPLYING
WITH
THE
IEEE
STANDARD
THE
FOLLOWING
PARAGRAPHS
GIVE
RECOMMENDATIONS
ON
HOW
TO
OPTI
MIZE
YOUR
CODE
TO
REDUCE
PERFORMANCE
DEGRADATIONS
RELATED
TO
FLOATING
POINT
EXCEP
TIONS
DEALING
WITH
FLOATING
POINT
EXCEPTIONS
IN
FPU
CODE
EVERY
SPECIAL
SITUATION
LISTED
IN
SECTION
FLOATING
POINT
EXCEPTIONS
IS
COSTLY
IN
TERMS
OF
PERFORMANCE
FOR
THAT
REASON
FPU
CODE
SHOULD
BE
WRITTEN
TO
AVOID
THESE
SITUATIONS
THERE
ARE
BASICALLY
THREE
WAYS
TO
REDUCE
THE
IMPACT
OF
OVERFLOW
UNDERFLOW
SITUATIONS
WITH
FPU
CODE
CHOOSE
FLOATING
POINT
DATA
TYPES
THAT
ARE
LARGE
ENOUGH
TO
ACCOMMODATE
RESULTS
WITHOUT
GENERATING
ARITHMETIC
OVERFLOW
AND
UNDERFLOW
EXCEPTIONS
SCALE
THE
RANGE
OF
OPERANDS
RESULTS
TO
REDUCE
AS
MUCH
AS
POSSIBLE
THE
NUMBER
OF
ARITHMETIC
OVERFLOW
UNDERFLOW
SITUATIONS
KEEP
INTERMEDIATE
RESULTS
ON
THE
FPU
REGISTER
STACK
UNTIL
THE
FINAL
RESULTS
HAVE
BEEN
COMPUTED
AND
STORED
IN
MEMORY
OVERFLOW
OR
UNDERFLOW
IS
LESS
LIKELY
TO
HAPPEN
WHEN
INTERMEDIATE
RESULTS
ARE
KEPT
IN
THE
FPU
STACK
THIS
IS
BECAUSE
DATA
ON
THE
STACK
IS
STORED
IN
DOUBLE
EXTENDED
PRECISION
FORMAT
AND
OVERFLOW
UNDERFLOW
CONDITIONS
ARE
DETECTED
ACCORDINGLY
DENORMALIZED
FLOATING
POINT
CONSTANTS
WHICH
ARE
READ
ONLY
AND
HENCE
NEVER
CHANGE
SHOULD
BE
AVOIDED
AND
REPLACED
IF
POSSIBLE
WITH
ZEROS
OF
THE
SAME
SIGN
FLOATING
POINT
EXCEPTIONS
IN
SSE
CODE
MOST
SPECIAL
SITUATIONS
THAT
INVOLVE
MASKED
FLOATING
POINT
EXCEPTIONS
ARE
HANDLED
EFFICIENTLY
IN
HARDWARE
WHEN
A
MASKED
OVERFLOW
EXCEPTION
OCCURS
WHILE
EXECUTING
SSE
CODE
PROCESSOR
HARDWARE
CAN
HANDLES
IT
WITHOUT
PERFORMANCE
PENALTY
UNDERFLOW
EXCEPTIONS
AND
DENORMALIZED
SOURCE
OPERANDS
ARE
USUALLY
TREATED
ACCORDING
TO
THE
IEEE
SPECIFICATION
BUT
THIS
CAN
INCUR
SIGNIFICANT
PERFORMANCE
DELAY
IF
A
PROGRAMMER
IS
WILLING
TO
TRADE
PURE
IEEE
COMPLIANCE
FOR
SPEED
TWO
NON
IEEE
COMPLIANT
MODES
ARE
PROVIDED
TO
SPEED
SITUATIONS
WHERE
UNDERFLOWS
AND
INPUT
ARE
FREQUENT
FTZ
MODE
AND
DAZ
MODE
WHEN
THE
FTZ
MODE
IS
ENABLED
AN
UNDERFLOW
RESULT
IS
AUTOMATICALLY
CONVERTED
TO
A
ZERO
WITH
THE
CORRECT
SIGN
ALTHOUGH
THIS
BEHAVIOR
IS
NOT
COMPLIANT
WITH
IEEE
IT
IS
PROVIDED
FOR
USE
IN
APPLICATIONS
WHERE
PERFORMANCE
IS
MORE
IMPORTANT
THAN
IEEE
COMPLIANCE
SINCE
DENORMAL
RESULTS
ARE
NOT
PRODUCED
WHEN
THE
FTZ
MODE
IS
ENABLED
THE
ONLY
DENORMAL
FLOATING
POINT
NUMBERS
THAT
CAN
BE
ENCOUNTERED
IN
FTZ
MODE
ARE
THE
ONES
SPECIFIED
AS
CONSTANTS
READ
ONLY
THE
DAZ
MODE
IS
PROVIDED
TO
HANDLE
DENORMAL
SOURCE
OPERANDS
EFFICIENTLY
WHEN
RUNNING
A
SIMD
FLOATING
POINT
APPLICATION
WHEN
THE
DAZ
MODE
IS
ENABLED
INPUT
DENORMALS
ARE
TREATED
AS
ZEROS
WITH
THE
SAME
SIGN
ENABLING
THE
DAZ
MODE
IS
THE
WAY
TO
DEAL
WITH
DENORMAL
FLOATING
POINT
CONSTANTS
WHEN
PERFORMANCE
IS
THE
OBJEC
TIVE
IF
DEPARTING
FROM
THE
IEEE
SPECIFICATION
IS
ACCEPTABLE
AND
PERFORMANCE
IS
CRITICAL
RUN
SSE
APPLICATIONS
WITH
FTZ
AND
DAZ
MODES
ENABLED
NOTE
THE
DAZ
MODE
IS
AVAILABLE
WITH
BOTH
THE
SSE
AND
EXTENSIONS
ALTHOUGH
THE
SPEED
IMPROVEMENT
EXPECTED
FROM
THIS
MODE
IS
FULLY
REALIZED
ONLY
IN
SSE
CODE
FLOATING
POINT
MODES
ON
THE
PENTIUM
III
PROCESSOR
THE
FLDCW
INSTRUCTION
IS
AN
EXPENSIVE
OPERATION
ON
EARLY
GENERATIONS
OF
PENTIUM
PROCESSORS
FLDCW
IS
IMPROVED
ONLY
FOR
SITUATIONS
WHERE
AN
APPLICATION
ALTERNATES
BETWEEN
TWO
CONSTANT
VALUES
OF
THE
FPU
CONTROL
WORD
FCW
SUCH
AS
WHEN
PERFORMING
CONVERSIONS
TO
INTEGERS
ON
PENTIUM
M
INTEL
CORE
SOLO
INTEL
CORE
DUO
AND
INTEL
CORE
DUO
PROCESSORS
FLDCW
IS
IMPROVED
OVER
PREVIOUS
GENERATIONS
SPECIFICALLY
THE
OPTIMIZATION
FOR
FLDCW
IN
THE
FIRST
TWO
GENERATIONS
OF
PENTIUM
PROCESSORS
ALLOW
PROGRAMMERS
TO
ALTERNATE
BETWEEN
TWO
CONSTANT
VALUES
EFFICIENTLY
FOR
THE
FLDCW
OPTIMIZATION
TO
BE
EFFECTIVE
THE
TWO
CONSTANT
FCW
VALUES
ARE
ONLY
ALLOWED
TO
DIFFER
ON
THE
FOLLOWING
BITS
IN
THE
FCW
FCW
PRECISION
CONTROL
FCW
ROUNDING
CONTROL
FCW
INFINITY
CONTROL
IF
PROGRAMMERS
NEED
TO
MODIFY
OTHER
BITS
FOR
EXAMPLE
MASK
BITS
IN
THE
FCW
THE
FLDCW
INSTRUCTION
IS
STILL
AN
EXPENSIVE
OPERATION
IN
SITUATIONS
WHERE
AN
APPLICATION
CYCLES
BETWEEN
THREE
OR
MORE
CONSTANT
VALUES
FLDCW
OPTIMIZATION
DOES
NOT
APPLY
AND
THE
PERFORMANCE
DEGRADATION
OCCURS
FOR
EACH
FLDCW
INSTRUCTION
ONE
SOLUTION
TO
THIS
PROBLEM
IS
TO
CHOOSE
TWO
CONSTANT
FCW
VALUES
TAKE
ADVANTAGE
OF
THE
OPTIMIZATION
OF
THE
FLDCW
INSTRUCTION
TO
ALTERNATE
BETWEEN
ONLY
THESE
TWO
CONSTANT
FCW
VALUES
AND
DEVISE
SOME
MEANS
TO
ACCOMPLISH
THE
TASK
THAT
REQUIRES
THE
FCW
VALUE
WITHOUT
ACTUALLY
CHANGING
THE
FCW
TO
A
THIRD
CONSTANT
VALUE
AN
ALTERNATIVE
SOLUTION
IS
TO
STRUCTURE
THE
CODE
SO
THAT
FOR
PERIODS
OF
TIME
THE
APPLICA
TION
ALTERNATES
BETWEEN
ONLY
TWO
CONSTANT
FCW
VALUES
WHEN
THE
APPLICATION
LATER
ALTERNATES
BETWEEN
A
PAIR
OF
DIFFERENT
FCW
VALUES
THE
PERFORMANCE
DEGRADATION
OCCURS
ONLY
DURING
THE
TRANSITION
IT
IS
EXPECTED
THAT
SIMD
APPLICATIONS
ARE
UNLIKELY
TO
ALTERNATE
BETWEEN
FTZ
AND
DAZ
MODE
VALUES
CONSEQUENTLY
THE
SIMD
CONTROL
WORD
DOES
NOT
HAVE
THE
SHORT
LATENCIES
THAT
THE
FLOATING
POINT
CONTROL
REGISTER
DOES
A
READ
OF
THE
MXCSR
REGISTER
HAS
A
FAIRLY
LONG
LATENCY
AND
A
WRITE
TO
THE
REGISTER
IS
A
SERIALIZING
INSTRUCTION
THERE
IS
NO
SEPARATE
CONTROL
WORD
FOR
SINGLE
AND
DOUBLE
PRECISION
BOTH
USE
THE
SAME
MODES
NOTABLY
THIS
APPLIES
TO
BOTH
FTZ
AND
DAZ
MODES
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
M
GENERALITY
MINIMIZE
CHANGES
TO
BITS
OF
THE
FLOATING
POINT
CONTROL
WORD
CHANGES
FOR
MORE
THAN
TWO
VALUES
EACH
VALUE
BEING
A
COMBINATION
OF
THE
FOLLOWING
BITS
PRECISION
ROUNDING
AND
INFINITY
CONTROL
AND
THE
REST
OF
BITS
IN
FCW
LEADS
TO
DELAYS
THAT
ARE
ON
THE
ORDER
OF
THE
PIPELINE
DEPTH
ROUNDING
MODE
MANY
LIBRARIES
PROVIDE
FLOAT
TO
INTEGER
LIBRARY
ROUTINES
THAT
CONVERT
FLOATING
POINT
VALUES
TO
INTEGER
MANY
OF
THESE
LIBRARIES
CONFORM
TO
ANSI
C
CODING
STANDARDS
WHICH
STATE
THAT
THE
ROUNDING
MODE
SHOULD
BE
TRUNCATION
WITH
THE
PENTIUM
PROCESSOR
ONE
CAN
USE
THE
AND
INSTRUCTIONS
TO
CONVERT
OPERANDS
WITH
TRUNCATION
WITHOUT
EVER
NEEDING
TO
CHANGE
ROUNDING
MODES
THE
COST
SAVINGS
OF
USING
THESE
INSTRUCTIONS
OVER
THE
METHODS
BELOW
IS
ENOUGH
TO
JUSTIFY
USING
SSE
AND
WHEREVER
POSSIBLE
WHEN
TRUNCATION
IS
INVOLVED
FOR
FLOATING
POINT
THE
FIST
INSTRUCTION
USES
THE
ROUNDING
MODE
REPRESENTED
IN
THE
FLOATING
POINT
CONTROL
WORD
FCW
THE
ROUNDING
MODE
IS
GENERALLY
ROUND
TO
NEAREST
SO
MANY
COMPILER
WRITERS
IMPLEMENT
A
CHANGE
IN
THE
ROUNDING
MODE
IN
THE
PROCESSOR
IN
ORDER
TO
CONFORM
TO
THE
C
AND
FORTRAN
STANDARDS
THIS
IMPLEMENTATION
REQUIRES
CHANGING
THE
CONTROL
WORD
ON
THE
PROCESSOR
USING
THE
FLDCW
INSTRUCTION
FOR
A
CHANGE
IN
THE
ROUNDING
PRECISION
AND
INFINITY
BITS
USE
THE
FSTCW
INSTRUCTION
TO
STORE
THE
FLOATING
POINT
CONTROL
WORD
THEN
USE
THE
FLDCW
INSTRUCTION
TO
CHANGE
THE
ROUNDING
MODE
TO
TRUNCATION
IN
A
TYPICAL
CODE
SEQUENCE
THAT
CHANGES
THE
ROUNDING
MODE
IN
THE
FCW
A
FSTCW
INSTRUCTION
IS
USUALLY
FOLLOWED
BY
A
LOAD
OPERATION
THE
LOAD
OPERATION
FROM
MEMORY
SHOULD
BE
A
BIT
OPERAND
TO
PREVENT
STORE
FORWARDING
PROBLEM
IF
THE
LOAD
OPERA
TION
ON
THE
PREVIOUSLY
STORED
FCW
WORD
INVOLVES
EITHER
AN
BIT
OR
A
BIT
OPERAND
THIS
WILL
CAUSE
A
STORE
FORWARDING
PROBLEM
DUE
TO
MISMATCH
OF
THE
SIZE
OF
THE
DATA
BETWEEN
THE
STORE
OPERATION
AND
THE
LOAD
OPERATION
TO
AVOID
STORE
FORWARDING
PROBLEMS
MAKE
SURE
THAT
THE
WRITE
AND
READ
TO
THE
FCW
ARE
BOTH
BIT
OPERATIONS
IF
THERE
IS
MORE
THAN
ONE
CHANGE
TO
THE
ROUNDING
PRECISION
AND
INFINITY
BITS
AND
THE
ROUNDING
MODE
IS
NOT
IMPORTANT
TO
THE
RESULT
USE
THE
ALGORITHM
IN
EXAMPLE
TO
AVOID
SYNCHRONIZATION
ISSUES
THE
OVERHEAD
OF
THE
FLDCW
INSTRUCTION
AND
HAVING
TO
CHANGE
THE
ROUNDING
MODE
NOTE
THAT
THE
EXAMPLE
SUFFERS
FROM
A
STORE
FORWARDING
PROBLEM
WHICH
WILL
LEAD
TO
A
PERFORMANCE
PENALTY
HOWEVER
ITS
PERFORMANCE
IS
STILL
BETTER
THAN
CHANGING
THE
ROUNDING
PRECISION
AND
INFINITY
BITS
AMONG
MORE
THAN
TWO
VALUES
EXAMPLE
ALGORITHM
TO
AVOID
CHANGING
ROUNDING
MODE
LEA
ECX
ESP
SUB
ESP
ALLOCATE
FRAME
AND
ECX
ALIGN
POINTER
ON
BOUNDARY
OF
FLD
ST
DUPLICATE
FPU
STACK
TOP
FISTP
QWORD
PTR
ECX
FILD
QWORD
PTR
ECX
MOV
EDX
ECX
HIGH
DWORD
OF
INTEGER
MOV
EAX
ECX
LOW
DWIRD
OF
INTEGER
TEST
EAX
EAX
JE
EXAMPLE
ALGORITHM
TO
AVOID
CHANGING
ROUNDING
MODE
CONTD
FSUBP
ST
ST
TOS
D
ROUND
D
ST
ST
ST
POP
ST
TEST
EDX
EDX
WHAT
SIGN
OF
INTEGER
JNS
POSITIVE
NUMBER
IS
NEGATIVE
FSTP
DWORD
PTR
ECX
RESULT
OF
SUBTRACTION
MOV
ECX
ECX
DWORD
OF
DIFF
SINGLE
PRECISION
ADD
ESP
XOR
ECX
ADD
ECX
IF
DIFF
THEN
DECREMENT
INTEGER
ADC
EAX
INC
EAX
ADD
CARRY
FLAG
RET
POSITIVE
POSITIVE
FSTP
DWORD
PTR
ECX
RESULT
OF
SUBTRACTION
MOV
ECX
ECX
DWORD
OF
DIFF
SINGLE
PRECISION
ADD
ESP
ADD
ECX
IF
DIFF
THEN
DECREMENT
INTEGER
SBB
EAX
DEC
EAX
SUBTRACT
CARRY
FLAG
RET
TEST
EDX
JNZ
ADD
ESP
RET
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
L
GENERALITY
MINIMIZE
THE
NUMBER
OF
CHANGES
TO
THE
ROUNDING
MODE
DO
NOT
USE
CHANGES
IN
THE
ROUNDING
MODE
TO
IMPLEMENT
THE
FLOOR
AND
CEILING
FUNCTIONS
IF
THIS
INVOLVES
A
TOTAL
OF
MORE
THAN
TWO
VALUES
OF
THE
SET
OF
ROUNDING
PRECISION
AND
INFINITY
BITS
PRECISION
IF
SINGLE
PRECISION
IS
ADEQUATE
USE
IT
INSTEAD
OF
DOUBLE
PRECISION
THIS
IS
TRUE
BECAUSE
SINGLE
PRECISION
OPERATIONS
ALLOW
THE
USE
OF
LONGER
SIMD
VECTORS
SINCE
MORE
SINGLE
PRECISION
DATA
ELEMENTS
CAN
FIT
IN
A
REGISTER
IF
THE
PRECISION
CONTROL
PC
FIELD
IN
THE
FPU
CONTROL
WORD
IS
SET
TO
SINGLE
PRECISION
THE
FLOATING
POINT
DIVIDER
CAN
COMPLETE
A
SINGLE
PRECISION
COMPUTATION
MUCH
FASTER
THAN
EITHER
A
DOUBLE
PRECISION
COMPUTATION
OR
AN
EXTENDED
DOUBLE
PRECISION
COMPUTATION
IF
THE
PC
FIELD
IS
SET
TO
DOUBLE
PRECISION
THIS
WILL
ENABLE
THOSE
FPU
OPERATIONS
ON
DOUBLE
PRECISION
DATA
TO
COMPLETE
FASTER
THAN
EXTENDED
DOUBLE
PRECISION
COMPUTATION
THESE
CHARACTERISTICS
AFFECT
COMPUTA
TIONS
INCLUDING
FLOATING
POINT
DIVIDE
AND
SQUARE
ROOT
ASSEMBLY
COMPILER
CODING
RULE
H
IMPACT
L
GENERALITY
MINIMIZE
THE
NUMBER
OF
CHANGES
TO
THE
PRECISION
MODE
IMPROVING
PARALLELISM
AND
THE
USE
OF
FXCH
THE
INSTRUCTION
SET
RELIES
ON
THE
FLOATING
POINT
STACK
FOR
ONE
OF
ITS
OPERANDS
IF
THE
DEPENDENCE
GRAPH
IS
A
TREE
WHICH
MEANS
EACH
INTERMEDIATE
RESULT
IS
USED
ONLY
ONCE
AND
CODE
IS
SCHEDULED
CAREFULLY
IT
IS
OFTEN
POSSIBLE
TO
USE
ONLY
OPERANDS
THAT
ARE
ON
THE
TOP
OF
THE
STACK
OR
IN
MEMORY
AND
TO
AVOID
USING
OPERANDS
THAT
ARE
BURIED
UNDER
THE
TOP
OF
THE
STACK
WHEN
OPERANDS
NEED
TO
BE
PULLED
FROM
THE
MIDDLE
OF
THE
STACK
AN
FXCH
INSTRUCTION
CAN
BE
USED
TO
SWAP
THE
OPERAND
ON
THE
TOP
OF
THE
STACK
WITH
ANOTHER
ENTRY
IN
THE
STACK
THE
FXCH
INSTRUCTION
CAN
ALSO
BE
USED
TO
ENHANCE
PARALLELISM
DEPENDENT
CHAINS
CAN
BE
OVERLAPPED
TO
EXPOSE
MORE
INDEPENDENT
INSTRUCTIONS
TO
THE
HARDWARE
SCHEDULER
AN
FXCH
INSTRUCTION
MAY
BE
REQUIRED
TO
EFFECTIVELY
INCREASE
THE
REGISTER
NAME
SPACE
SO
THAT
MORE
OPERANDS
CAN
BE
SIMULTANEOUSLY
LIVE
IN
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
HOWEVER
THAT
FXCH
INHIBITS
ISSUE
BANDWIDTH
IN
THE
TRACE
CACHE
IT
DOES
THIS
NOT
ONLY
BECAUSE
IT
CONSUMES
A
SLOT
BUT
ALSO
BECAUSE
OF
ISSUE
SLOT
RESTRICTIONS
IMPOSED
ON
FXCH
IF
THE
APPLICATION
IS
NOT
BOUND
BY
ISSUE
OR
RETIREMENT
BANDWIDTH
FXCH
WILL
HAVE
NO
IMPACT
THE
EFFECTIVE
INSTRUCTION
WINDOW
SIZE
IN
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROAR
CHITECTURE
IS
LARGE
ENOUGH
TO
PERMIT
INSTRUCTIONS
THAT
ARE
AS
FAR
AWAY
AS
THE
NEXT
ITER
ATION
TO
BE
OVERLAPPED
THIS
OFTEN
OBVIATES
THE
NEED
TO
USE
FXCH
TO
ENHANCE
PARALLELISM
THE
FXCH
INSTRUCTION
SHOULD
BE
USED
ONLY
WHEN
IT
NEEDED
TO
EXPRESS
AN
ALGORITHM
OR
TO
ENHANCE
PARALLELISM
IF
THE
SIZE
OF
REGISTER
NAME
SPACE
IS
A
PROBLEM
THE
USE
OF
XMM
REGISTERS
IS
RECOMMENDED
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
M
GENERALITY
USE
FXCH
ONLY
WHERE
NECESSARY
TO
INCREASE
THE
EFFECTIVE
NAME
SPACE
THIS
IN
TURN
ALLOWS
INSTRUCTIONS
TO
BE
REORDERED
AND
MADE
AVAILABLE
FOR
EXECUTION
IN
PARALLEL
OUT
OF
ORDER
EXECUTION
PRECLUDES
THE
NEED
FOR
USING
FXCH
TO
MOVE
INSTRUC
TIONS
FOR
VERY
SHORT
DISTANCES
VS
SCALAR
SIMD
FLOATING
POINT
TRADE
OFFS
THERE
ARE
A
NUMBER
OF
DIFFERENCES
BETWEEN
FLOATING
POINT
CODE
AND
SCALAR
FLOATING
POINT
CODE
USING
SSE
AND
THE
FOLLOWING
DIFFERENCES
SHOULD
DRIVE
DECISIONS
ABOUT
WHICH
REGISTERS
AND
INSTRUCTIONS
TO
USE
WHEN
AN
INPUT
OPERAND
FOR
A
SIMD
FLOATING
POINT
INSTRUCTION
CONTAINS
VALUES
THAT
ARE
LESS
THAN
THE
REPRESENTABLE
RANGE
OF
THE
DATA
TYPE
A
DENORMAL
EXCEPTION
OCCURS
THIS
CAUSES
A
SIGNIFICANT
PERFORMANCE
PENALTY
AN
SIMD
FLOATING
POINT
OPERATION
HAS
A
FLUSH
TO
ZERO
MODE
IN
WHICH
THE
RESULTS
WILL
NOT
UNDERFLOW
THEREFORE
SUBSEQUENT
COMPUTATION
WILL
NOT
FACE
THE
PERFORMANCE
PENALTY
OF
HANDLING
DENORMAL
INPUT
OPERANDS
FOR
EXAMPLE
IN
THE
CASE
OF
APPLICATIONS
WITH
LOW
LIGHTING
LEVELS
USING
FLUSH
TO
ZERO
MODE
CAN
IMPROVE
PERFORMANCE
BY
AS
MUCH
AS
FOR
APPLICATIONS
WITH
LARGE
NUMBERS
OF
UNDERFLOWS
SCALAR
FLOATING
POINT
SIMD
INSTRUCTIONS
HAVE
LOWER
LATENCIES
THAN
EQUIVALENT
INSTRUCTIONS
SCALAR
SIMD
FLOATING
POINT
MULTIPLY
INSTRUCTION
MAY
BE
PIPELINED
WHILE
MULTIPLY
INSTRUCTION
IS
NOT
ONLY
SUPPORTS
TRANSCENDENTAL
INSTRUCTIONS
SUPPORTS
BIT
PRECISION
DOUBLE
EXTENDED
FLOATING
POINT
SSE
SUPPORT
A
MAXIMUM
OF
BIT
PRECISION
SUPPORTS
A
MAXIMUM
OF
BIT
PRECISION
SCALAR
FLOATING
POINT
REGISTERS
MAY
BE
ACCESSED
DIRECTLY
AVOIDING
FXCH
AND
TOP
OF
STACK
RESTRICTIONS
THE
COST
OF
CONVERTING
FROM
FLOATING
POINT
TO
INTEGER
WITH
TRUNCATION
IS
SIGNIFI
CANTLY
LOWER
WITH
STREAMING
SIMD
EXTENSIONS
AND
STREAMING
SIMD
EXTENSIONS
IN
THE
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
THAN
WITH
EITHER
CHANGES
TO
THE
ROUNDING
MODE
OR
THE
SEQUENCE
PRESCRIBED
IN
THE
EXAMPLE
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
M
GENERALITY
USE
STREAMING
SIMD
EXTENSIONS
OR
STREAMING
SIMD
EXTENSIONS
UNLESS
YOU
NEED
AN
FEATURE
MOST
ARITHMETIC
OPERATIONS
HAVE
SHORTER
LATENCY
THEN
THEIR
COUNTERPART
AND
THEY
ELIMINATE
THE
OVERHEAD
ASSOCIATED
WITH
THE
MANAGEMENT
OF
THE
REGISTER
STACK
SCALAR
SSE
PERFORMANCE
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
THE
COMBINATION
OF
IMPROVED
DECODING
AND
OP
FUSION
ALLOWS
INSTRUCTIONS
WHICH
WERE
FORMERLY
TWO
THREE
AND
FOUR
ΜOPS
TO
GO
THROUGH
ALL
DECODERS
AS
A
RESULT
SCALAR
SSE
CODE
CAN
MATCH
THE
PERFORMANCE
OF
CODE
EXECUTING
THROUGH
TWO
FLOATING
POINT
UNITS
ON
PENTIUM
M
PROCESSORS
SCALAR
SSE
CODE
CAN
EXPERIENCE
APPROXIMATELY
PERFORMANCE
DEGRADATION
RELATIVE
TO
CODE
EXECUTING
THROUGH
TWO
FLOATING
POINT
UNITS
IN
CODE
SEQUENCES
THAT
HAVE
CONVERSIONS
FROM
FLOATING
POINT
TO
INTEGER
DIVIDE
SINGLE
PRECISION
INSTRUCTIONS
OR
ANY
PRECISION
CHANGE
CODE
GENERATION
FROM
A
COMPILER
TYPICALLY
WRITES
DATA
TO
MEMORY
IN
SINGLE
PRECISION
AND
READS
IT
AGAIN
IN
ORDER
TO
REDUCE
PRECISION
USING
SSE
SCALAR
CODE
INSTEAD
OF
CODE
CAN
GENERATE
A
LARGE
PERFORMANCE
BENEFIT
USING
INTEL
NETBURST
MICROARCHITECTURE
AND
A
MODEST
BENEFIT
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
RECOMMENDATION
USE
THE
COMPILER
SWITCH
TO
GENERATE
SCALAR
FLOATING
POINT
CODE
RATHER
THAN
CODE
WHEN
WORKING
WITH
SCALAR
SSE
CODE
PAY
ATTENTION
TO
THE
NEED
FOR
CLEARING
THE
CONTENT
OF
UNUSED
SLOTS
IN
AN
XMM
REGISTER
AND
THE
ASSOCIATED
PERFORMANCE
IMPACT
FOR
EXAMPLE
LOADING
DATA
FROM
MEMORY
WITH
MOVSS
OR
MOVSD
CAUSES
AN
EXTRA
MICRO
OP
FOR
ZEROING
THE
UPPER
PART
OF
THE
XMM
REGISTER
ON
PENTIUM
M
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
THIS
PENALTY
CAN
BE
AVOIDED
BY
USING
MOVLPD
HOWEVER
USING
MOVLPD
CAUSES
A
PERFORMANCE
PENALTY
ON
PENTIUM
PROCESSORS
ANOTHER
SITUATION
OCCURS
WHEN
MIXING
SINGLE
PRECISION
AND
DOUBLE
PRECISION
CODE
ON
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
USING
HAS
PERFOR
MANCE
PENALTY
RELATIVE
TO
THE
ALTERNATIVE
SEQUENCE
XORPS
MOVSS
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
USING
IS
MORE
DESIRABLE
THAN
THE
ALTERNATIVE
SEQUENCE
FLOATING
POINT
OPERATIONS
WITH
INTEGER
OPERANDS
FOR
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
SPLITTING
FLOATING
POINT
OPERATIONS
FIADD
FISUB
FIMUL
AND
FIDIV
THAT
TAKE
BIT
INTEGER
OPERANDS
INTO
TWO
INSTRUCTIONS
FILD
AND
A
FLOATING
POINT
OPERATION
IS
MORE
EFFICIENT
HOWEVER
FOR
FLOATING
POINT
OPERATIONS
WITH
BIT
INTEGER
OPERANDS
USING
FIADD
FISUB
FIMUL
AND
FIDIV
IS
EQUALLY
EFFICIENT
COMPARED
WITH
USING
SEPARATE
INSTRUCTIONS
ASSEMBLY
COMPILER
CODING
RULE
M
IMPACT
L
GENERALITY
TRY
TO
USE
BIT
OPERANDS
RATHER
THAN
BIT
OPERANDS
FOR
FILD
HOWEVER
DO
NOT
DO
SO
AT
THE
EXPENSE
OF
INTRODUCING
A
STORE
FORWARDING
PROBLEM
BY
WRITING
THE
TWO
HALVES
OF
THE
BIT
MEMORY
OPERAND
SEPARATELY
FLOATING
POINT
COMPARISON
INSTRUCTIONS
THE
FCOMI
AND
FCMOV
INSTRUCTIONS
SHOULD
BE
USED
WHEN
PERFORMING
FLOATING
POINT
COMPARISONS
USING
THE
FCOM
FCOMP
AND
FCOMPP
INSTRUCTIONS
TYPICALLY
REQUIRES
ADDITIONAL
INSTRUCTION
LIKE
FSTSW
THE
LATTER
ALTERNATIVE
CAUSES
MORE
OPS
TO
BE
DECODED
AND
SHOULD
BE
AVOIDED
TRANSCENDENTAL
FUNCTIONS
IF
AN
APPLICATION
NEEDS
TO
EMULATE
MATH
FUNCTIONS
IN
SOFTWARE
FOR
PERFORMANCE
OR
OTHER
REASONS
SEE
SECTION
GUIDELINES
FOR
OPTIMIZING
FLOATING
POINT
CODE
IT
MAY
BE
WORTHWHILE
TO
INLINE
MATH
LIBRARY
CALLS
BECAUSE
THE
CALL
AND
THE
PROLOGUE
EPILOGUE
INVOLVED
WITH
SUCH
CALLS
CAN
SIGNIFICANTLY
AFFECT
THE
LATENCY
OF
OPERATIONS
NOTE
THAT
TRANSCENDENTAL
FUNCTIONS
ARE
SUPPORTED
ONLY
IN
FLOATING
POINT
NOT
IN
STREAMING
SIMD
EXTENSIONS
OR
STREAMING
SIMD
EXTENSIONS
MAXIMIZING
PCIE
PERFORMANCE
PCIE
PERFORMANCE
CAN
BE
DRAMATICALLY
IMPACTED
BY
THE
SIZE
AND
ALIGNMENT
OF
UPSTREAM
READS
AND
WRITES
READ
AND
WRITE
TRANSACTIONS
ISSUED
FROM
A
PCIE
AGENT
TO
THE
HOST
MEMORY
AS
A
GENERAL
RULE
THE
BEST
PERFORMANCE
IN
TERMS
OF
BOTH
BAND
WIDTH
AND
LATENCY
IS
OBTAINED
BY
ALIGNING
THE
START
ADDRESSES
OF
UPSTREAM
READS
AND
WRITES
ON
BYTE
BOUNDARIES
AND
ENSURING
THAT
THE
REQUEST
SIZE
IS
A
MULTIPLE
OF
BYTES
WITH
MODEST
FURTHER
INCREASES
IN
BANDWIDTH
WHEN
LARGER
MULTIPLES
BYTES
ARE
EMPLOYED
IN
PARTICULAR
A
PARTIAL
WRITE
WILL
CAUSE
A
DELAY
FOR
THE
FOLLOWING
REQUEST
READ
OR
WRITE
A
SECOND
RULE
IS
TO
AVOID
MULTIPLE
CONCURRENTLY
OUTSTANDING
ACCESSES
TO
A
SINGLE
CACHE
LINE
THIS
CAN
RESULT
IN
A
CONFLICT
WHICH
IN
TURN
CAN
CAUSE
SERIALIZATION
OF
ACCESSES
THAT
WOULD
OTHERWISE
BE
PIPELINED
RESULTING
IN
HIGHER
LATENCY
AND
OR
LOWER
BANDWIDTH
PATTERNS
THAT
VIOLATE
THIS
RULE
INCLUDE
SEQUENTIAL
ACCESSES
READS
OR
WRITES
THAT
ARE
NOT
A
MULTIPLE
OF
BYTES
AS
WELL
AS
EXPLICIT
ACCESSES
TO
THE
SAME
CACHE
LINE
ADDRESS
OVERLAPPING
REQUESTS
THOSE
WITH
DIFFERENT
START
ADDRESSES
BUT
WITH
REQUEST
LENGTHS
THAT
RESULT
IN
OVERLAP
OF
THE
REQUESTS
CAN
HAVE
THE
SAME
EFFECT
FOR
EXAMPLE
A
BYTE
READ
OF
ADDRESS
FOLLOWED
BY
A
BYTE
READ
OF
ADDRESS
WILL
CAUSE
A
CONFLICT
AND
A
LIKELY
DELAY
FOR
THE
SECOND
READ
UPSTREAM
WRITES
THAT
ARE
A
MULTIPLE
OF
BYTE
BUT
ARE
NON
ALIGNED
WILL
HAVE
THE
PERFORMANCE
OF
A
SERIES
OF
PARTIAL
AND
FULL
SEQUENTIAL
WRITES
FOR
EXAMPLE
A
WRITE
OF
LENGTH
BYTE
TO
ADDRESS
WILL
PERFORM
SIMILARLY
TO
SEQUENTIAL
WRITES
OF
LENGTHS
AND
TO
ADDRESSES
AND
RESPECTIVELY
FOR
PCIE
CARDS
IMPLEMENTING
MULTI
FUNCTION
DEVICES
SUCH
AS
DUAL
OR
QUAD
PORT
NETWORK
INTERFACE
CARDS
NICS
OR
DUAL
GPU
GRAPHICS
CARDS
IT
IS
IMPORTANT
TO
NOTE
THAT
NON
OPTIMAL
BEHAVIOR
BY
ONE
OF
THOSE
DEVICES
CAN
IMPACT
THE
BANDWIDTH
AND
OR
LATENCY
OBSERVED
BY
THE
OTHER
DEVICES
ON
THAT
CARD
WITH
RESPECT
TO
THE
BEHAVIOR
DESCRIBED
IN
THIS
SECTION
ALL
TRAFFIC
ON
A
GIVEN
PCIE
PORT
IS
TREATED
AS
IF
IT
ORIGINATED
FROM
A
SINGLE
DEVICE
AND
FUNCTION
FOR
THE
BEST
PCIE
BANDWIDTH
ALIGN
START
ADDRESSES
OF
UPSTREAM
READS
AND
WRITES
ON
BYTE
BOUNDARIES
USE
READ
AND
WRITE
REQUESTS
THAT
ARE
A
MULTIPLE
OF
BYTES
ELIMINATE
OR
AVOID
SEQUENTIAL
AND
RANDOM
PARTIAL
LINE
UPSTREAM
WRITES
ELIMINATE
OR
AVOID
CONFLICTING
UPSTREAM
READS
INCLUDING
SEQUENTIAL
PARTIAL
LINE
READS
TECHNIQUES
FOR
AVOIDING
PERFORMANCE
PITFALLS
INCLUDE
CACHE
LINE
ALIGNING
ALL
DESCRIP
TORS
AND
DATA
BUFFERS
PADDING
DESCRIPTORS
THAT
ARE
WRITTEN
UPSTREAM
TO
BYTE
ALIGNMENT
BUFFERING
INCOMING
DATA
TO
ACHIEVE
LARGER
UPSTREAM
WRITE
PAYLOADS
ALLO
CATING
DATA
STRUCTURES
INTENDED
FOR
SEQUENTIAL
READING
BY
THE
PCIE
DEVICE
IN
SUCH
A
WAY
AS
TO
ENABLE
USE
OF
MULTIPLE
OF
BYTE
READS
THE
NEGATIVE
IMPACT
OF
UNOPTI
MIZED
READS
AND
WRITES
DEPENDS
ON
THE
SPECIFIC
WORKLOAD
AND
THE
MICROARCHITECTURE
ON
WHICH
THE
PRODUCT
IS
BASED
CHAPTER
CODING
FOR
SIMD
ARCHITECTURES
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
SUPPORTS
MMX
SSE
AND
PROCESSORS
BASED
ON
ENHANCED
INTEL
CORE
MICROARCHITECTURE
SUPPORTS
MMX
SSE
AND
PROCESSORS
BASED
ON
INTEL
MICROARCHI
TECTURE
NEHALEM
SUPPORTS
MMX
SSE
AND
INTEL
PENTIUM
INTEL
XEON
AND
PENTIUM
M
PROCESSORS
INCLUDE
SUPPORT
FOR
SSE
AND
MMX
TECHNOLOGY
WERE
INTRODUCED
WITH
THE
PENTIUM
PROCESSOR
SUPPORTING
HYPER
THREADING
TECHNOLOGY
AT
NM
TECHNOLOGY
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
SUPPORT
SSE
AND
MMX
SINGLE
INSTRUCTION
MULTIPLE
DATA
SIMD
TECHNOLOGIES
ENABLE
THE
DEVELOPMENT
OF
ADVANCED
MULTIMEDIA
SIGNAL
PROCESSING
AND
MODELING
APPLICATIONS
SINGLE
INSTRUCTION
MULTIPLE
DATA
TECHNIQUES
CAN
BE
APPLIED
TO
TEXT
STRING
PROCESSING
LEXING
AND
PARSER
APPLICATIONS
THIS
IS
COVERED
IN
CHAPTER
AND
SIMD
PROGRAMMING
FOR
TEXT
PROCESSING
LEXING
PARSING
TO
TAKE
ADVANTAGE
OF
THE
PERFORMANCE
OPPORTUNITIES
PRESENTED
BY
THESE
CAPABILITIES
DO
THE
FOLLOWING
ENSURE
THAT
THE
PROCESSOR
SUPPORTS
MMX
TECHNOLOGY
SSE
AND
ENSURE
THAT
THE
OPERATING
SYSTEM
SUPPORTS
MMX
TECHNOLOGY
AND
SSE
OS
SUPPORT
FOR
AND
IS
THE
SAME
AS
OS
SUPPORT
FOR
SSE
EMPLOY
THE
OPTIMIZATION
AND
SCHEDULING
STRATEGIES
DESCRIBED
IN
THIS
BOOK
USE
STACK
AND
DATA
ALIGNMENT
TECHNIQUES
TO
KEEP
DATA
PROPERLY
ALIGNED
FOR
EFFICIENT
MEMORY
USE
UTILIZE
THE
CACHEABILITY
INSTRUCTIONS
OFFERED
BY
SSE
AND
WHERE
APPROPRIATE
CHECKING
FOR
PROCESSOR
SUPPORT
OF
SIMD
TECHNOLOGIES
THIS
SECTION
SHOWS
HOW
TO
CHECK
WHETHER
A
PROCESSOR
SUPPORTS
MMX
TECHNOLOGY
SSE
AND
SIMD
TECHNOLOGY
CAN
BE
INCLUDED
IN
YOUR
APPLICATION
IN
THREE
WAYS
CHECK
FOR
THE
SIMD
TECHNOLOGY
DURING
INSTALLATION
IF
THE
DESIRED
SIMD
TECHNOLOGY
IS
AVAILABLE
THE
APPROPRIATE
DLLS
CAN
BE
INSTALLED
CHECK
FOR
THE
SIMD
TECHNOLOGY
DURING
PROGRAM
EXECUTION
AND
INSTALL
THE
PROPER
DLLS
AT
RUNTIME
THIS
IS
EFFECTIVE
FOR
PROGRAMS
THAT
MAY
BE
EXECUTED
ON
DIFFERENT
MACHINES
CREATE
A
FAT
BINARY
THAT
INCLUDES
MULTIPLE
VERSIONS
OF
ROUTINES
VERSIONS
THAT
USE
SIMD
TECHNOLOGY
AND
VERSIONS
THAT
DO
NOT
CHECK
FOR
SIMD
TECHNOLOGY
DURING
PROGRAM
EXECUTION
AND
RUN
THE
APPROPRIATE
VERSIONS
OF
THE
ROUTINES
THIS
IS
ESPECIALLY
EFFECTIVE
FOR
PROGRAMS
THAT
MAY
BE
EXECUTED
ON
DIFFERENT
MACHINES
CHECKING
FOR
MMX
TECHNOLOGY
SUPPORT
IF
MMX
TECHNOLOGY
IS
AVAILABLE
THEN
CPUID
EDX
BIT
USE
THE
CODE
SEGMENT
IN
EXAMPLE
TO
TEST
FOR
MMX
TECHNOLOGY
EXAMPLE
IDENTIFICATION
OF
MMX
TECHNOLOGY
WITH
CPUID
IDENTIFY
EXISTENCE
OF
CPUID
INSTRUCTION
IDENTIFY
SIGNATURE
IS
GENUINE
INTEL
MOV
EAX
REQUEST
FOR
FEATURE
FLAGS
CPUID
CPUID
INSTRUCTION
TEST
EDX
IS
MMX
TECHNOLOGY
BIT
BIT
IN
FEATURE
FLAGS
EQUAL
TO
JNZ
FOUND
FOR
MORE
INFORMATION
ON
CPUID
SEE
INTEL
PROCESSOR
IDENTIFICATION
WITH
CPUID
INSTRUCTION
ORDER
NUMBER
CHECKING
FOR
STREAMING
SIMD
EXTENSIONS
SUPPORT
CHECKING
FOR
PROCESSOR
SUPPORT
OF
STREAMING
SIMD
EXTENSIONS
SSE
ON
YOUR
PROCESSOR
IS
SIMILAR
TO
CHECKING
FOR
MMX
TECHNOLOGY
HOWEVER
OPERATING
SYSTEM
OS
MUST
PROVIDE
SUPPORT
FOR
SSE
STATES
SAVE
AND
RESTORE
ON
CONTEXT
SWITCHES
TO
ENSURE
CONSISTENT
APPLICATION
BEHAVIOR
WHEN
USING
SSE
INSTRUCTIONS
TO
CHECK
WHETHER
YOUR
SYSTEM
SUPPORTS
SSE
FOLLOW
THESE
STEPS
CHECK
THAT
YOUR
PROCESSOR
SUPPORTS
THE
CPUID
INSTRUCTION
CHECK
THE
FEATURE
BITS
OF
CPUID
FOR
SSE
EXISTENCE
EXAMPLE
SHOWS
HOW
TO
FIND
THE
SSE
FEATURE
BIT
BIT
IN
CPUID
FEATURE
FLAGS
EXAMPLE
IDENTIFICATION
OF
SSE
WITH
CPUID
CHECKING
FOR
STREAMING
SIMD
EXTENSIONS
SUPPORT
CHECKING
FOR
SUPPORT
OF
IS
LIKE
CHECKING
FOR
SSE
SUPPORT
THE
OS
REQUIREMENTS
FOR
SUPPORT
ARE
THE
SAME
AS
THE
OS
REQUIREMENTS
FOR
SSE
TO
CHECK
WHETHER
YOUR
SYSTEM
SUPPORTS
FOLLOW
THESE
STEPS
CHECK
THAT
YOUR
PROCESSOR
HAS
THE
CPUID
INSTRUCTION
CHECK
THE
FEATURE
BITS
OF
CPUID
FOR
TECHNOLOGY
EXISTENCE
EXAMPLE
SHOWS
HOW
TO
FIND
THE
FEATURE
BIT
BIT
IN
THE
CPUID
FEATURE
FLAGS
EXAMPLE
IDENTIFICATION
OF
WITH
CPUID
IDENTIFY
EXISTENCE
OF
CPUID
INSTRUCTION
IDENTIFY
SIGNATURE
IS
GENUINE
INTEL
MOV
EAX
REQUEST
FOR
FEATURE
FLAGS
CPUID
CPUID
INSTRUCTION
TEST
EDX
BIT
IN
FEATURE
FLAGS
EQUAL
TO
JNZ
FOUND
CHECKING
FOR
STREAMING
SIMD
EXTENSIONS
SUPPORT
INCLUDES
INSTRUCTIONS
OF
THOSE
ARE
SUITED
FOR
SIMD
OR
STYLE
PROGRAM
MING
CHECKING
FOR
SUPPORT
OF
INSTRUCTIONS
IS
SIMILAR
TO
CHECKING
FOR
SSE
SUPPORT
THE
OS
REQUIREMENTS
FOR
SUPPORT
ARE
THE
SAME
AS
THE
REQUIREMENTS
FOR
SSE
TO
CHECK
WHETHER
YOUR
SYSTEM
SUPPORTS
THE
AND
SIMD
INSTRUCTIONS
OF
FOLLOW
THESE
STEPS
CHECK
THAT
YOUR
PROCESSOR
HAS
THE
CPUID
INSTRUCTION
CHECK
THE
ECX
FEATURE
BIT
OF
CPUID
FOR
TECHNOLOGY
EXISTENCE
EXAMPLE
SHOWS
HOW
TO
FIND
THE
FEATURE
BIT
BIT
OF
ECX
IN
THE
CPUID
FEATURE
FLAGS
EXAMPLE
IDENTIFICATION
OF
WITH
CPUID
IDENTIFY
EXISTENCE
OF
CPUID
INSTRUCTION
IDENTIFY
SIGNATURE
IS
GENUINE
INTEL
MOV
EAX
REQUEST
FOR
FEATURE
FLAGS
CPUID
CPUID
INSTRUCTION
TEST
ECX
BIT
IN
FEATURE
FLAGS
EQUAL
TO
JNZ
FOUND
SOFTWARE
MUST
CHECK
FOR
SUPPORT
OF
MONITOR
AND
MWAIT
BEFORE
ATTEMPTING
TO
USE
MONITOR
AND
MWAIT
DETECTING
THE
AVAILABILITY
OF
MONITOR
AND
MWAIT
CAN
BE
DONE
USING
A
CODE
SEQUENCE
SIMILAR
TO
EXAMPLE
THE
AVAILABILITY
OF
MONITOR
AND
MWAIT
IS
INDICATED
BY
BIT
OF
THE
RETURNED
VALUE
IN
ECX
CHECKING
FOR
SUPPLEMENTAL
STREAMING
SIMD
EXTENSIONS
SUPPORT
CHECKING
FOR
SUPPORT
OF
IS
SIMILAR
TO
CHECKING
FOR
SSE
SUPPORT
THE
OS
REQUIRE
MENTS
FOR
SUPPORT
ARE
THE
SAME
AS
THE
REQUIREMENTS
FOR
SSE
TO
CHECK
WHETHER
YOUR
SYSTEM
SUPPORTS
FOLLOW
THESE
STEPS
CHECK
THAT
YOUR
PROCESSOR
HAS
THE
CPUID
INSTRUCTION
CHECK
THE
FEATURE
BITS
OF
CPUID
FOR
TECHNOLOGY
EXISTENCE
EXAMPLE
SHOWS
HOW
TO
FIND
THE
FEATURE
BIT
IN
THE
CPUID
FEATURE
FLAGS
EXAMPLE
IDENTIFICATION
OF
WITH
CPUID
IDENTIFY
EXISTENCE
OF
CPUID
INSTRUCTION
IDENTIFY
SIGNATURE
IS
GENUINE
INTEL
MOV
EAX
REQUEST
FOR
FEATURE
FLAGS
CPUID
CPUID
INSTRUCTION
TEST
ECX
ECX
BIT
JNZ
FOUND
CHECKING
FOR
SUPPORT
CHECKING
FOR
SUPPORT
OF
IS
SIMILAR
TO
CHECKING
FOR
SSE
SUPPORT
THE
OS
REQUIREMENTS
FOR
SUPPORT
ARE
THE
SAME
AS
THE
REQUIREMENTS
FOR
SSE
TO
CHECK
WHETHER
YOUR
SYSTEM
SUPPORTS
FOLLOW
THESE
STEPS
CHECK
THAT
YOUR
PROCESSOR
HAS
THE
CPUID
INSTRUCTION
CHECK
THE
FEATURE
BITS
OF
CPUID
FOR
EXAMPLE
SHOWS
HOW
TO
FIND
THE
FEATURE
BIT
IN
THE
CPUID
FEATURE
FLAGS
EXAMPLE
IDENTIFICATION
OF
WITH
CPUID
CONSIDERATIONS
FOR
CODE
CONVERSION
TO
SIMD
PROGRAMMING
THE
VTUNE
PERFORMANCE
ENHANCEMENT
ENVIRONMENT
CD
PROVIDES
TOOLS
TO
AID
IN
THE
EVALUATION
AND
TUNING
BEFORE
IMPLEMENTING
THEM
YOU
NEED
ANSWERS
TO
THE
FOLLOWING
QUESTIONS
WILL
THE
CURRENT
CODE
BENEFIT
BY
USING
MMX
TECHNOLOGY
STREAMING
SIMD
EXTENSIONS
STREAMING
SIMD
EXTENSIONS
STREAMING
SIMD
EXTENSIONS
OR
SUPPLEMENTAL
STREAMING
SIMD
EXTENSIONS
IS
THIS
CODE
INTEGER
OR
FLOATING
POINT
WHAT
INTEGER
WORD
SIZE
OR
FLOATING
POINT
PRECISION
IS
NEEDED
WHAT
CODING
TECHNIQUES
SHOULD
I
USE
WHAT
GUIDELINES
DO
I
NEED
TO
FOLLOW
HOW
SHOULD
I
ARRANGE
AND
ALIGN
THE
DATATYPES
FIGURE
PROVIDES
A
FLOWCHART
FOR
THE
PROCESS
OF
CONVERTING
CODE
TO
MMX
TECH
NOLOGY
SSE
OR
NO
CODE
BENEFITS
FROM
SIMD
YES
FLOATING
POINT
INTEGER
OR
FLOATING
POINT
INTEGER
WHY
FP
PERFORMANCE
RANGE
OR
PRECISION
CAN
CONVERT
TO
INTEGER
YES
NO
CAN
CONVERT
TO
SINGLE
PRECISION
YES
NO
STOP
FIGURE
CONVERTING
TO
STREAMING
SIMD
EXTENSIONS
CHART
TO
USE
ANY
OF
THE
SIMD
TECHNOLOGIES
OPTIMALLY
YOU
MUST
EVALUATE
THE
FOLLOWING
SITU
ATIONS
IN
YOUR
CODE
FRAGMENTS
THAT
ARE
COMPUTATIONALLY
INTENSIVE
FRAGMENTS
THAT
ARE
EXECUTED
OFTEN
ENOUGH
TO
HAVE
AN
IMPACT
ON
PERFORMANCE
FRAGMENTS
THAT
WITH
LITTLE
DATA
DEPENDENT
CONTROL
FLOW
FRAGMENTS
THAT
REQUIRE
FLOATING
POINT
COMPUTATIONS
FRAGMENTS
THAT
CAN
BENEFIT
FROM
MOVING
DATA
BYTES
AT
A
TIME
FRAGMENTS
OF
COMPUTATION
THAT
CAN
CODED
USING
FEWER
INSTRUCTIONS
FRAGMENTS
THAT
REQUIRE
HELP
IN
USING
THE
CACHE
HIERARCHY
EFFICIENTLY
IDENTIFYING
HOT
SPOTS
TO
OPTIMIZE
PERFORMANCE
USE
THE
VTUNE
PERFORMANCE
ANALYZER
TO
FIND
SECTIONS
OF
CODE
THAT
OCCUPY
MOST
OF
THE
COMPUTATION
TIME
SUCH
SECTIONS
ARE
CALLED
THE
HOTSPOTS
SEE
APPENDIX
A
APPLICATION
PERFORMANCE
TOOLS
THE
VTUNE
ANALYZER
PROVIDES
A
HOTSPOTS
VIEW
OF
A
SPECIFIC
MODULE
TO
HELP
YOU
IDENTIFY
SECTIONS
IN
YOUR
CODE
THAT
TAKE
THE
MOST
CPU
TIME
AND
THAT
HAVE
POTENTIAL
PERFOR
MANCE
PROBLEMS
THE
HOTSPOTS
VIEW
HELPS
YOU
IDENTIFY
SECTIONS
IN
YOUR
CODE
THAT
TAKE
THE
MOST
CPU
TIME
AND
THAT
HAVE
POTENTIAL
PERFORMANCE
PROBLEMS
THE
VTUNE
ANALYZER
ENABLES
YOU
TO
CHANGE
THE
VIEW
TO
SHOW
HOTSPOTS
BY
MEMORY
LOCATION
FUNCTIONS
CLASSES
OR
SOURCE
FILES
YOU
CAN
DOUBLE
CLICK
ON
A
HOTSPOT
AND
OPEN
THE
SOURCE
OR
ASSEMBLY
VIEW
FOR
THE
HOTSPOT
AND
SEE
MORE
DETAILED
INFORMATION
ABOUT
THE
PERFORMANCE
OF
EACH
INSTRUCTION
IN
THE
HOTSPOT
THE
VTUNE
ANALYZER
OFFERS
FOCUSED
ANALYSIS
AND
PERFORMANCE
DATA
AT
ALL
LEVELS
OF
YOUR
SOURCE
CODE
AND
CAN
ALSO
PROVIDE
ADVICE
AT
THE
ASSEMBLY
LANGUAGE
LEVEL
THE
CODE
COACH
ANALYZES
AND
IDENTIFIES
OPPORTUNITIES
FOR
BETTER
PERFORMANCE
OF
C
C
FORTRAN
AND
JAVA
PROGRAMS
AND
SUGGESTS
SPECIFIC
OPTIMIZATIONS
WHERE
APPROPRIATE
THE
COACH
DISPLAYS
PSEUDO
CODE
TO
SUGGEST
THE
USE
OF
HIGHLY
OPTIMIZED
INTRINSICS
AND
FUNCTIONS
IN
THE
INTEL
PERFORMANCE
LIBRARY
SUITE
BECAUSE
VTUNE
ANALYZER
IS
DESIGNED
SPECIFICALLY
FOR
INTEL
ARCHITECTURE
IA
BASED
PROCESSORS
INCLUDING
THE
PENTIUM
PROCESSOR
IT
CAN
OFFER
DETAILED
APPROACHES
TO
WORKING
WITH
IA
SEE
APPENDIX
A
RECOMMENDED
OPTIMIZATION
SETTINGS
FOR
INTEL
AND
IA
PROCES
SORS
FOR
DETAILS
DETERMINE
IF
CODE
BENEFITS
BY
CONVERSION
TO
SIMD
EXECUTION
IDENTIFYING
CODE
THAT
BENEFITS
BY
USING
SIMD
TECHNOLOGIES
CAN
BE
TIME
CONSUMING
AND
DIFFICULT
LIKELY
CANDIDATES
FOR
CONVERSION
ARE
APPLICATIONS
THAT
ARE
HIGHLY
COMPU
TATION
INTENSIVE
SUCH
AS
THE
FOLLOWING
SPEECH
COMPRESSION
ALGORITHMS
AND
FILTERS
SPEECH
RECOGNITION
ALGORITHMS
VIDEO
DISPLAY
AND
CAPTURE
ROUTINES
RENDERING
ROUTINES
GRAPHICS
GEOMETRY
IMAGE
AND
VIDEO
PROCESSING
ALGORITHMS
SPATIAL
AUDIO
PHYSICAL
MODELING
GRAPHICS
CAD
WORKSTATION
APPLICATIONS
ENCRYPTION
ALGORITHMS
COMPLEX
ARITHMETICS
GENERALLY
GOOD
CANDIDATE
CODE
IS
CODE
THAT
CONTAINS
SMALL
SIZED
REPETITIVE
LOOPS
THAT
OPERATE
ON
SEQUENTIAL
ARRAYS
OF
INTEGERS
OF
OR
BITS
SINGLE
PRECISION
BIT
FLOATING
POINT
DATA
DOUBLE
PRECISION
BIT
FLOATING
POINT
DATA
INTEGER
AND
FLOATING
POINT
DATA
ITEMS
SHOULD
BE
SEQUENTIAL
IN
MEMORY
THE
REPETITIVENESS
OF
THESE
LOOPS
INCURS
COSTLY
APPLICATION
PROCESSING
TIME
HOWEVER
THESE
ROUTINES
HAVE
POTENTIAL
FOR
INCREASED
PERFORMANCE
WHEN
YOU
CONVERT
THEM
TO
USE
ONE
OF
THE
SIMD
TECHNOLOGIES
ONCE
YOU
IDENTIFY
YOUR
OPPORTUNITIES
FOR
USING
A
SIMD
TECHNOLOGY
YOU
MUST
EVALUATE
WHAT
SHOULD
BE
DONE
TO
DETERMINE
WHETHER
THE
CURRENT
ALGORITHM
OR
A
MODIFIED
ONE
WILL
ENSURE
THE
BEST
PERFORMANCE
CODING
TECHNIQUES
THE
SIMD
FEATURES
OF
SSE
AND
MMX
TECHNOLOGY
REQUIRE
NEW
METHODS
OF
CODING
ALGORITHMS
ONE
OF
THEM
IS
VECTORIZATION
VECTORIZATION
IS
THE
PROCESS
OF
TRANS
FORMING
SEQUENTIALLY
EXECUTING
OR
SCALAR
CODE
INTO
CODE
THAT
CAN
EXECUTE
IN
PARALLEL
TAKING
ADVANTAGE
OF
THE
SIMD
ARCHITECTURE
PARALLELISM
THIS
SECTION
DISCUSSES
THE
CODING
TECHNIQUES
AVAILABLE
FOR
AN
APPLICATION
TO
MAKE
USE
OF
THE
SIMD
ARCHITECTURE
TO
VECTORIZE
YOUR
CODE
AND
THUS
TAKE
ADVANTAGE
OF
THE
SIMD
ARCHITECTURE
DO
THE
FOLLOWING
DETERMINE
IF
THE
MEMORY
ACCESSES
HAVE
DEPENDENCIES
THAT
WOULD
PREVENT
PARALLEL
EXECUTION
STRIP
MINE
THE
INNER
LOOP
TO
REDUCE
THE
ITERATION
COUNT
BY
THE
LENGTH
OF
THE
SIMD
OPERATIONS
FOR
EXAMPLE
FOUR
FOR
SINGLE
PRECISION
FLOATING
POINT
SIMD
EIGHT
FOR
BIT
INTEGER
SIMD
ON
THE
XMM
REGISTERS
RE
CODE
THE
LOOP
WITH
THE
SIMD
INSTRUCTIONS
EACH
OF
THESE
ACTIONS
IS
DISCUSSED
IN
DETAIL
IN
THE
SUBSEQUENT
SECTIONS
OF
THIS
CHAPTER
THESE
SECTIONS
ALSO
DISCUSS
ENABLING
AUTOMATIC
VECTORIZATION
USING
THE
INTEL
C
COMPILER
CODING
METHODOLOGIES
SOFTWARE
DEVELOPERS
NEED
TO
COMPARE
THE
PERFORMANCE
IMPROVEMENT
THAT
CAN
BE
OBTAINED
FROM
ASSEMBLY
CODE
VERSUS
THE
COST
OF
THOSE
IMPROVEMENTS
PROGRAMMING
DIRECTLY
IN
ASSEMBLY
LANGUAGE
FOR
A
TARGET
PLATFORM
MAY
PRODUCE
THE
REQUIRED
PERFOR
MANCE
GAIN
HOWEVER
ASSEMBLY
CODE
IS
NOT
PORTABLE
BETWEEN
PROCESSOR
ARCHITEC
TURES
AND
IS
EXPENSIVE
TO
WRITE
AND
MAINTAIN
PERFORMANCE
OBJECTIVES
CAN
BE
MET
BY
TAKING
ADVANTAGE
OF
THE
DIFFERENT
SIMD
TECH
NOLOGIES
USING
HIGH
LEVEL
LANGUAGES
AS
WELL
AS
ASSEMBLY
THE
NEW
C
C
LANGUAGE
EXTENSIONS
DESIGNED
SPECIFICALLY
FOR
SSE
AND
MMX
TECHNOLOGY
HELP
MAKE
THIS
POSSIBLE
FIGURE
ILLUSTRATES
THE
TRADE
OFFS
INVOLVED
IN
THE
PERFORMANCE
OF
HAND
CODED
ASSEMBLY
VERSUS
THE
EASE
OF
PROGRAMMING
AND
PORTABILITY
FIGURE
HAND
CODED
ASSEMBLY
AND
HIGH
LEVEL
COMPILER
PERFORMANCE
TRADE
OFFS
THE
EXAMPLES
THAT
FOLLOW
ILLUSTRATE
THE
USE
OF
CODING
ADJUSTMENTS
TO
ENABLE
THE
ALGO
RITHM
TO
BENEFIT
FROM
THE
SSE
THE
SAME
TECHNIQUES
MAY
BE
USED
FOR
SINGLE
PRECISION
FLOATING
POINT
DOUBLE
PRECISION
FLOATING
POINT
AND
INTEGER
DATA
UNDER
SSE
AND
MMX
TECHNOLOGY
AS
A
BASIS
FOR
THE
USAGE
MODEL
DISCUSSED
IN
THIS
SECTION
CONSIDER
A
SIMPLE
LOOP
SHOWN
IN
EXAMPLE
EXAMPLE
SIMPLE
FOUR
ITERATION
LOOP
VOID
ADD
FLOAT
A
FLOAT
B
FLOAT
C
INT
I
FOR
I
I
I
C
I
A
I
B
I
NOTE
THAT
THE
LOOP
RUNS
FOR
ONLY
FOUR
ITERATIONS
THIS
ALLOWS
A
SIMPLE
REPLACEMENT
OF
THE
CODE
WITH
STREAMING
SIMD
EXTENSIONS
FOR
THE
OPTIMAL
USE
OF
THE
STREAMING
SIMD
EXTENSIONS
THAT
NEED
DATA
ALIGNMENT
ON
THE
BYTE
BOUNDARY
ALL
EXAMPLES
IN
THIS
CHAPTER
ASSUME
THAT
THE
ARRAYS
PASSED
TO
THE
ROUTINE
A
B
C
ARE
ALIGNED
TO
BYTE
BOUNDARIES
BY
A
CALLING
ROUTINE
FOR
THE
METHODS
TO
ENSURE
THIS
ALIGNMENT
PLEASE
REFER
TO
THE
APPLICATION
NOTES
FOR
THE
PENTIUM
PROCESSOR
THE
SECTIONS
THAT
FOLLOW
PROVIDE
DETAILS
ON
THE
CODING
METHODOLOGIES
INLINED
ASSEMBLY
INTRINSICS
C
VECTOR
CLASSES
AND
AUTOMATIC
VECTORIZATION
ASSEMBLY
KEY
LOOPS
CAN
BE
CODED
DIRECTLY
IN
ASSEMBLY
LANGUAGE
USING
AN
ASSEMBLER
OR
BY
USING
INLINED
ASSEMBLY
C
ASM
IN
C
C
CODE
THE
INTEL
COMPILER
OR
ASSEMBLER
RECOGNIZE
THE
NEW
INSTRUCTIONS
AND
REGISTERS
THEN
DIRECTLY
GENERATE
THE
CORRESPONDING
CODE
THIS
MODEL
OFFERS
THE
OPPORTUNITY
FOR
ATTAINING
GREATEST
PERFORMANCE
BUT
THIS
PERFOR
MANCE
IS
NOT
PORTABLE
ACROSS
THE
DIFFERENT
PROCESSOR
ARCHITECTURES
EXAMPLE
SHOWS
THE
STREAMING
SIMD
EXTENSIONS
INLINED
ASSEMBLY
ENCODING
EXAMPLE
STREAMING
SIMD
EXTENSIONS
USING
INLINED
ASSEMBLY
ENCODING
VOID
ADD
FLOAT
A
FLOAT
B
FLOAT
C
ASM
MOV
EAX
A
MOV
EDX
B
MOV
ECX
C
MOVAPS
XMMWORD
PTR
EAX
ADDPS
XMMWORD
PTR
EDX
MOVAPS
XMMWORD
PTR
ECX
INTRINSICS
INTRINSICS
PROVIDE
THE
ACCESS
TO
THE
ISA
FUNCTIONALITY
USING
C
C
STYLE
CODING
INSTEAD
OF
ASSEMBLY
LANGUAGE
INTEL
HAS
DEFINED
THREE
SETS
OF
INTRINSIC
FUNCTIONS
THAT
ARE
IMPLEMENTED
IN
THE
INTEL
C
COMPILER
TO
SUPPORT
THE
MMX
TECHNOLOGY
STREAMING
SIMD
EXTENSIONS
AND
STREAMING
SIMD
EXTENSIONS
FOUR
NEW
C
DATA
TYPES
REPRESENTING
BIT
AND
BIT
OBJECTS
ARE
USED
AS
THE
OPERANDS
OF
THESE
INTRINSIC
FUNCTIONS
IS
USED
FOR
MMX
INTEGER
SIMD
IS
USED
FOR
SINGLE
PRECISION
FLOATING
POINT
SIMD
IS
USED
FOR
STREAMING
SIMD
EXTENSIONS
INTEGER
SIMD
AND
IS
USED
FOR
DOUBLE
PRECISION
FLOATING
POINT
SIMD
THESE
TYPES
ENABLE
THE
PROGRAMMER
TO
CHOOSE
THE
IMPLEMENTATION
OF
AN
ALGORITHM
DIRECTLY
WHILE
ALLOWING
THE
COMPILER
TO
PERFORM
REGISTER
ALLOCATION
AND
INSTRUCTION
SCHEDULING
WHERE
POSSIBLE
THE
INTRINSICS
ARE
PORTABLE
AMONG
ALL
INTEL
ARCHITECTURE
BASED
PROCESSORS
SUPPORTED
BY
A
COMPILER
THE
USE
OF
INTRINSICS
ALLOWS
YOU
TO
OBTAIN
PERFORMANCE
CLOSE
TO
THE
LEVELS
ACHIEVABLE
WITH
ASSEMBLY
THE
COST
OF
WRITING
AND
MAINTAINING
PROGRAMS
WITH
INTRINSICS
IS
CONSID
ERABLY
LESS
FOR
A
DETAILED
DESCRIPTION
OF
THE
INTRINSICS
AND
THEIR
USE
REFER
TO
THE
INTEL
C
COMPILER
DOCUMENTATION
EXAMPLE
SHOWS
THE
LOOP
FROM
EXAMPLE
USING
INTRINSICS
EXAMPLE
SIMPLE
FOUR
ITERATION
LOOP
CODED
WITH
INTRINSICS
INCLUDE
XMMINTRIN
H
VOID
ADD
FLOAT
A
FLOAT
B
FLOAT
C
A
B
C
THE
INTRINSICS
MAP
ONE
TO
ONE
WITH
ACTUAL
STREAMING
SIMD
EXTENSIONS
ASSEMBLY
CODE
THE
XMMINTRIN
H
HEADER
FILE
IN
WHICH
THE
PROTOTYPES
FOR
THE
INTRINSICS
ARE
DEFINED
IS
PART
OF
THE
INTEL
C
COMPILER
INCLUDED
WITH
THE
VTUNE
PERFORMANCE
ENHANCEMENT
ENVIRONMENT
CD
INTRINSICS
ARE
ALSO
DEFINED
FOR
THE
MMX
TECHNOLOGY
ISA
THESE
ARE
BASED
ON
THE
DATA
TYPE
TO
REPRESENT
THE
CONTENTS
OF
AN
MM
REGISTER
YOU
CAN
SPECIFY
VALUES
IN
BYTES
SHORT
INTEGERS
BIT
VALUES
OR
AS
A
BIT
OBJECT
THE
INTRINSIC
DATA
TYPES
HOWEVER
ARE
NOT
A
BASIC
ANSI
C
DATA
TYPE
AND
THEREFORE
YOU
MUST
OBSERVE
THE
FOLLOWING
USAGE
RESTRICTIONS
USE
INTRINSIC
DATA
TYPES
ONLY
ON
THE
LEFT
HAND
SIDE
OF
AN
ASSIGNMENT
AS
A
RETURN
VALUE
OR
AS
A
PARAMETER
YOU
CANNOT
USE
IT
WITH
OTHER
ARITHMETIC
EXPRESSIONS
FOR
EXAMPLE
USE
INTRINSIC
DATA
TYPE
OBJECTS
IN
AGGREGATES
SUCH
AS
UNIONS
TO
ACCESS
THE
BYTE
ELEMENTS
AND
STRUCTURES
THE
ADDRESS
OF
AN
OBJECT
MAY
BE
ALSO
USED
USE
INTRINSIC
DATA
TYPE
DATA
ONLY
WITH
THE
MMX
TECHNOLOGY
INTRINSICS
DESCRIBED
IN
THIS
GUIDE
FOR
COMPLETE
DETAILS
OF
THE
HARDWARE
INSTRUCTIONS
SEE
THE
INTEL
ARCHITECTURE
MMX
TECHNOLOGY
PROGRAMMER
REFERENCE
MANUAL
FOR
A
DESCRIPTION
OF
DATA
TYPES
SEE
THE
INTEL
AND
IA
ARCHITECTURES
SOFTWARE
DEVELOPER
MANUAL
CLASSES
A
SET
OF
C
CLASSES
HAS
BEEN
DEFINED
AND
AVAILABLE
IN
INTEL
C
COMPILER
TO
PROVIDE
BOTH
A
HIGHER
LEVEL
ABSTRACTION
AND
MORE
FLEXIBILITY
FOR
PROGRAMMING
WITH
MMX
TECH
NOLOGY
STREAMING
SIMD
EXTENSIONS
AND
STREAMING
SIMD
EXTENSIONS
THESE
CLASSES
PROVIDE
AN
EASY
TO
USE
AND
FLEXIBLE
INTERFACE
TO
THE
INTRINSIC
FUNCTIONS
ALLOWING
DEVELOPERS
TO
WRITE
MORE
NATURAL
C
CODE
WITHOUT
WORRYING
ABOUT
WHICH
INTRINSIC
OR
ASSEMBLY
LANGUAGE
INSTRUCTION
TO
USE
FOR
A
GIVEN
OPERATION
SINCE
THE
INTRINSIC
FUNCTIONS
UNDERLIE
THE
IMPLEMENTATION
OF
THESE
C
CLASSES
THE
PERFOR
MANCE
OF
APPLICATIONS
USING
THIS
METHODOLOGY
CAN
APPROACH
THAT
OF
ONE
USING
THE
INTRINSICS
FURTHER
DETAILS
ON
THE
USE
OF
THESE
CLASSES
CAN
BE
FOUND
IN
THE
INTEL
C
CLASS
LIBRARIES
FOR
SIMD
OPERATIONS
USER
GUIDE
ORDER
NUMBER
EXAMPLE
SHOWS
THE
C
CODE
USING
A
VECTOR
CLASS
LIBRARY
THE
EXAMPLE
ASSUMES
THE
ARRAYS
PASSED
TO
THE
ROUTINE
ARE
ALREADY
ALIGNED
TO
BYTE
BOUNDARIES
EXAMPLE
C
CODE
USING
THE
VECTOR
CLASSES
HERE
FVEC
H
IS
THE
CLASS
DEFINITION
FILE
AND
IS
THE
CLASS
REPRESENTING
AN
ARRAY
OF
FOUR
FLOATS
THE
AND
OPERATORS
ARE
OVERLOADED
SO
THAT
THE
ACTUAL
STREAMING
SIMD
EXTENSIONS
IMPLEMENTATION
IN
THE
PREVIOUS
EXAMPLE
IS
ABSTRACTED
OUT
OR
HIDDEN
FROM
THE
DEVELOPER
NOTE
HOW
MUCH
MORE
THIS
RESEMBLES
THE
ORIGINAL
CODE
ALLOWING
FOR
SIMPLER
AND
FASTER
PROGRAMMING
AGAIN
THE
EXAMPLE
IS
ASSUMING
THE
ARRAYS
PASSED
TO
THE
ROUTINE
ARE
ALREADY
ALIGNED
TO
BYTE
BOUNDARY
AUTOMATIC
VECTORIZATION
THE
INTEL
C
COMPILER
PROVIDES
AN
OPTIMIZATION
MECHANISM
BY
WHICH
LOOPS
SUCH
AS
IN
EXAMPLE
CAN
BE
AUTOMATICALLY
VECTORIZED
OR
CONVERTED
INTO
STREAMING
SIMD
EXTENSIONS
CODE
THE
COMPILER
USES
SIMILAR
TECHNIQUES
TO
THOSE
USED
BY
A
PROGRAMMER
TO
IDENTIFY
WHETHER
A
LOOP
IS
SUITABLE
FOR
CONVERSION
TO
SIMD
THIS
INVOLVES
DETERMINING
WHETHER
THE
FOLLOWING
MIGHT
PREVENT
VECTORIZATION
THE
LAYOUT
OF
THE
LOOP
AND
THE
DATA
STRUCTURES
USED
DEPENDENCIES
AMONGST
THE
DATA
ACCESSES
IN
EACH
ITERATION
AND
ACROSS
ITERATIONS
ONCE
THE
COMPILER
HAS
MADE
SUCH
A
DETERMINATION
IT
CAN
GENERATE
VECTORIZED
CODE
FOR
THE
LOOP
ALLOWING
THE
APPLICATION
TO
USE
THE
SIMD
INSTRUCTIONS
THE
CAVEAT
TO
THIS
IS
THAT
ONLY
CERTAIN
TYPES
OF
LOOPS
CAN
BE
AUTOMATICALLY
VECTORIZED
AND
IN
MOST
CASES
USER
INTERACTION
WITH
THE
COMPILER
IS
NEEDED
TO
FULLY
ENABLE
THIS
EXAMPLE
SHOWS
THE
CODE
FOR
AUTOMATIC
VECTORIZATION
FOR
THE
SIMPLE
FOUR
ITERA
TION
LOOP
FROM
EXAMPLE
EXAMPLE
AUTOMATIC
VECTORIZATION
FOR
A
SIMPLE
LOOP
VOID
ADD
FLOAT
RESTRICT
A
FLOAT
RESTRICT
B
FLOAT
RESTRICT
C
INT
I
FOR
I
I
I
C
I
A
I
B
I
COMPILE
THIS
CODE
USING
THE
QAX
AND
QRESTRICT
SWITCHES
OF
THE
INTEL
C
COMPILER
VERSION
OR
LATER
THE
RESTRICT
QUALIFIER
IN
THE
ARGUMENT
LIST
IS
NECESSARY
TO
LET
THE
COMPILER
KNOW
THAT
THERE
ARE
NO
OTHER
ALIASES
TO
THE
MEMORY
TO
WHICH
THE
POINTERS
POINT
IN
OTHER
WORDS
THE
POINTER
FOR
WHICH
IT
IS
USED
PROVIDES
THE
ONLY
MEANS
OF
ACCESSING
THE
MEMORY
IN
QUESTION
IN
THE
SCOPE
IN
WHICH
THE
POINTERS
LIVE
WITHOUT
THE
RESTRICT
QUALIFIER
THE
COMPILER
WILL
STILL
VECTORIZE
THIS
LOOP
USING
RUNTIME
DATA
DEPENDENCE
TESTING
WHERE
THE
GENERATED
CODE
DYNAMICALLY
SELECTS
BETWEEN
SEQUENTIAL
OR
VECTOR
EXECUTION
OF
THE
LOOP
BASED
ON
OVERLAP
OF
THE
PARAMETERS
SEE
DOCUMENTATION
FOR
THE
INTEL
C
COMPILER
THE
RESTRICT
KEYWORD
AVOIDS
THE
ASSOCIATED
OVERHEAD
ALTOGETHER
SEE
INTEL
C
COMPILER
DOCUMENTATION
FOR
DETAILS
STACK
AND
DATA
ALIGNMENT
TO
GET
THE
MOST
PERFORMANCE
OUT
OF
CODE
WRITTEN
FOR
SIMD
TECHNOLOGIES
DATA
SHOULD
BE
FORMATTED
IN
MEMORY
ACCORDING
TO
THE
GUIDELINES
DESCRIBED
IN
THIS
SECTION
ASSEMBLY
CODE
WITH
AN
UNALIGNED
ACCESSES
IS
A
LOT
SLOWER
THAN
AN
ALIGNED
ACCESS
ALIGNMENT
AND
CONTIGUITY
OF
DATA
ACCESS
PATTERNS
THE
BIT
PACKED
DATA
TYPES
DEFINED
BY
MMX
TECHNOLOGY
AND
THE
BIT
PACKED
DATA
TYPES
FOR
STREAMING
SIMD
EXTENSIONS
AND
STREAMING
SIMD
EXTENSIONS
CREATE
MORE
POTENTIAL
FOR
MISALIGNED
DATA
ACCESSES
THE
DATA
ACCESS
PATTERNS
OF
MANY
ALGO
RITHMS
ARE
INHERENTLY
MISALIGNED
WHEN
USING
MMX
TECHNOLOGY
AND
STREAMING
SIMD
EXTENSIONS
SEVERAL
TECHNIQUES
FOR
IMPROVING
DATA
ACCESS
SUCH
AS
PADDING
ORGA
NIZING
DATA
ELEMENTS
INTO
ARRAYS
ETC
ARE
DESCRIBED
BELOW
PROVIDES
A
SPECIAL
PURPOSE
INSTRUCTION
LDDQU
THAT
CAN
AVOID
CACHE
LINE
SPLITS
IS
DISCUSSED
IN
SECTION
SUPPLEMENTAL
TECHNIQUES
FOR
AVOIDING
CACHE
LINE
SPLITS
USING
PADDING
TO
ALIGN
DATA
HOWEVER
WHEN
ACCESSING
SIMD
DATA
USING
SIMD
OPERATIONS
ACCESS
TO
DATA
CAN
BE
IMPROVED
SIMPLY
BY
A
CHANGE
IN
THE
DECLARATION
FOR
EXAMPLE
CONSIDER
A
DECLARATION
OF
A
STRUCTURE
WHICH
REPRESENTS
A
POINT
IN
SPACE
PLUS
AN
ATTRIBUTE
TYPEDEF
STRUCT
SHORT
X
Y
Z
CHAR
A
POINT
POINT
PT
N
ASSUME
WE
WILL
BE
PERFORMING
A
NUMBER
OF
COMPUTATIONS
ON
X
Y
Z
IN
THREE
OF
THE
FOUR
ELEMENTS
OF
A
SIMD
WORD
SEE
SECTION
DATA
STRUCTURE
LAYOUT
FOR
AN
EXAMPLE
EVEN
IF
THE
FIRST
ELEMENT
IN
ARRAY
PT
IS
ALIGNED
THE
SECOND
ELEMENT
WILL
START
BYTES
LATER
AND
NOT
BE
ALIGNED
SHORTS
AT
TWO
BYTES
EACH
PLUS
A
SINGLE
BYTE
BYTES
BY
ADDING
THE
PADDING
VARIABLE
PAD
THE
STRUCTURE
IS
NOW
BYTES
AND
IF
THE
FIRST
ELEMENT
IS
ALIGNED
TO
BYTES
BITS
ALL
FOLLOWING
ELEMENTS
WILL
ALSO
BE
ALIGNED
THE
SAMPLE
DECLARATION
FOLLOWS
TYPEDEF
STRUCT
SHORT
X
Y
Z
CHAR
A
CHAR
PAD
POINT
POINT
PT
N
USING
ARRAYS
TO
MAKE
DATA
CONTIGUOUS
IN
THE
FOLLOWING
CODE
FOR
I
I
N
I
PT
I
Y
SCALE
THE
SECOND
DIMENSION
Y
NEEDS
TO
BE
MULTIPLIED
BY
A
SCALING
VALUE
HERE
THE
FOR
LOOP
ACCESSES
EACH
Y
DIMENSION
IN
THE
ARRAY
PT
THUS
DISALLOWING
THE
ACCESS
TO
CONTIGUOUS
DATA
THIS
CAN
DEGRADE
THE
PERFORMANCE
OF
THE
APPLICATION
BY
INCREASING
CACHE
MISSES
BY
POOR
UTILIZATION
OF
EACH
CACHE
LINE
THAT
IS
FETCHED
AND
BY
INCREASING
THE
CHANCE
FOR
ACCESSES
WHICH
SPAN
MULTIPLE
CACHE
LINES
THE
FOLLOWING
DECLARATION
ALLOWS
YOU
TO
VECTORIZE
THE
SCALING
OPERATION
AND
FURTHER
IMPROVE
THE
ALIGNMENT
OF
THE
DATA
ACCESS
PATTERNS
SHORT
PTX
N
PTY
N
PTZ
N
FOR
I
I
N
I
PTY
I
SCALE
WITH
THE
SIMD
TECHNOLOGY
CHOICE
OF
DATA
ORGANIZATION
BECOMES
MORE
IMPORTANT
AND
SHOULD
BE
MADE
CAREFULLY
BASED
ON
THE
OPERATIONS
THAT
WILL
BE
PERFORMED
ON
THE
DATA
IN
SOME
APPLICATIONS
TRADITIONAL
DATA
ARRANGEMENTS
MAY
NOT
LEAD
TO
THE
MAXIMUM
PERFORMANCE
A
SIMPLE
EXAMPLE
OF
THIS
IS
AN
FIR
FILTER
AN
FIR
FILTER
IS
EFFECTIVELY
A
VECTOR
DOT
PRODUCT
IN
THE
LENGTH
OF
THE
NUMBER
OF
COEFFICIENT
TAPS
CONSIDER
THE
FOLLOWING
CODE
DATA
J
COEFF
DATA
J
COEFF
DATA
J
NUM
OF
TAPS
COEFF
NUM
OF
TAPS
IF
IN
THE
CODE
ABOVE
THE
FILTER
OPERATION
OF
DATA
ELEMENT
I
IS
THE
VECTOR
DOT
PRODUCT
THAT
BEGINS
AT
DATA
ELEMENT
J
THEN
THE
FILTER
OPERATION
OF
DATA
ELEMENT
I
BEGINS
AT
DATA
ELEMENT
J
ASSUMING
YOU
HAVE
A
BIT
ALIGNED
DATA
VECTOR
AND
A
BIT
ALIGNED
COEFFICIENTS
VECTOR
THE
FILTER
OPERATION
ON
THE
FIRST
DATA
ELEMENT
WILL
BE
FULLY
ALIGNED
FOR
THE
SECOND
DATA
ELEMENT
HOWEVER
ACCESS
TO
THE
DATA
VECTOR
WILL
BE
MISALIGNED
FOR
AN
EXAMPLE
OF
HOW
TO
AVOID
THE
MISALIGNMENT
PROBLEM
IN
THE
FIR
FILTER
REFER
TO
INTEL
APPLICATION
NOTES
ON
STREAMING
SIMD
EXTENSIONS
AND
FILTERS
DUPLICATION
AND
PADDING
OF
DATA
STRUCTURES
CAN
BE
USED
TO
AVOID
THE
PROBLEM
OF
DATA
ACCESSES
IN
ALGORITHMS
WHICH
ARE
INHERENTLY
MISALIGNED
SECTION
DATA
STRUC
TURE
LAYOUT
DISCUSSES
TRADE
OFFS
FOR
ORGANIZING
DATA
STRUCTURES
NOTE
THE
DUPLICATION
AND
PADDING
TECHNIQUE
OVERCOMES
THE
MISALIGNMENT
PROBLEM
THUS
AVOIDING
THE
EXPENSIVE
PENALTY
FOR
MISALIGNED
DATA
ACCESS
AT
THE
COST
OF
INCREASING
THE
DATA
SIZE
WHEN
DEVELOPING
YOUR
CODE
YOU
SHOULD
CONSIDER
THIS
TRADEOFF
AND
USE
THE
OPTION
WHICH
GIVES
THE
BEST
PERFORMANCE
STACK
ALIGNMENT
FOR
BIT
SIMD
TECHNOLOGIES
FOR
BEST
PERFORMANCE
THE
STREAMING
SIMD
EXTENSIONS
AND
STREAMING
SIMD
EXTEN
SIONS
REQUIRE
THEIR
MEMORY
OPERANDS
TO
BE
ALIGNED
TO
BYTE
BOUNDARIES
UNALIGNED
DATA
CAN
CAUSE
SIGNIFICANT
PERFORMANCE
PENALTIES
COMPARED
TO
ALIGNED
DATA
HOWEVER
THE
EXISTING
SOFTWARE
CONVENTIONS
FOR
IA
STDCALL
CDECL
FAST
CALL
AS
IMPLEMENTED
IN
MOST
COMPILERS
DO
NOT
PROVIDE
ANY
MECHANISM
FOR
ENSURING
THAT
CERTAIN
LOCAL
DATA
AND
CERTAIN
PARAMETERS
ARE
BYTE
ALIGNED
THERE
FORE
INTEL
HAS
DEFINED
A
NEW
SET
OF
IA
SOFTWARE
CONVENTIONS
FOR
ALIGNMENT
TO
SUPPORT
THE
NEW
DATATYPES
AND
THESE
MEET
THE
FOLLOWING
CONDITIONS
FUNCTIONS
THAT
USE
STREAMING
SIMD
EXTENSIONS
OR
STREAMING
SIMD
EXTENSIONS
DATA
NEED
TO
PROVIDE
A
BYTE
ALIGNED
STACK
FRAME
PARAMETERS
NEED
TO
BE
ALIGNED
TO
BYTE
BOUNDARIES
POSSIBLY
CREATING
HOLES
DUE
TO
PADDING
IN
THE
ARGUMENT
BLOCK
THE
NEW
CONVENTIONS
PRESENTED
IN
THIS
SECTION
AS
IMPLEMENTED
BY
THE
INTEL
C
COMPILER
CAN
BE
USED
AS
A
GUIDELINE
FOR
AN
ASSEMBLY
LANGUAGE
CODE
AS
WELL
IN
MANY
CASES
THIS
SECTION
ASSUMES
THE
USE
OF
THE
DATA
TYPES
AS
DEFINED
BY
THE
INTEL
C
COMPILER
WHICH
REPRESENTS
AN
ARRAY
OF
FOUR
BIT
FLOATS
FOR
MORE
DETAILS
ON
THE
STACK
ALIGNMENT
FOR
STREAMING
SIMD
EXTENSIONS
AND
SEE
APPENDIX
D
STACK
ALIGNMENT
DATA
ALIGNMENT
FOR
MMX
TECHNOLOGY
MANY
COMPILERS
ENABLE
ALIGNMENT
OF
VARIABLES
USING
CONTROLS
THIS
ALIGNS
VARIABLE
BIT
LENGTHS
TO
THE
APPROPRIATE
BOUNDARIES
IF
SOME
OF
THE
VARIABLES
ARE
NOT
APPROPRIATELY
ALIGNED
AS
SPECIFIED
YOU
CAN
ALIGN
THEM
USING
THE
C
ALGORITHM
IN
EXAMPLE
EXAMPLE
C
ALGORITHM
FOR
BIT
DATA
ALIGNMENT
MAKE
NEWP
A
POINTER
TO
A
BIT
ALIGNED
ARRAY
OF
BIT
ELEMENTS
DOUBLE
P
NEWP
P
DOUBLE
MALLOC
SIZEOF
DOUBLE
NEWP
P
THE
ALGORITHM
IN
EXAMPLE
ALIGNS
AN
ARRAY
OF
BIT
ELEMENTS
ON
A
BIT
BOUNDARY
THE
CONSTANT
OF
IS
DERIVED
FROM
ONE
LESS
THAN
THE
NUMBER
OF
BYTES
IN
A
BIT
ELEMENT
OR
ALIGNING
DATA
IN
THIS
MANNER
AVOIDS
THE
SIGNIFICANT
PERFOR
MANCE
PENALTIES
THAT
CAN
OCCUR
WHEN
AN
ACCESS
CROSSES
A
CACHE
LINE
BOUNDARY
ANOTHER
WAY
TO
IMPROVE
DATA
ALIGNMENT
IS
TO
COPY
THE
DATA
INTO
LOCATIONS
THAT
ARE
ALIGNED
ON
BIT
BOUNDARIES
WHEN
THE
DATA
IS
ACCESSED
FREQUENTLY
THIS
CAN
PROVIDE
A
SIGNIFICANT
PERFORMANCE
IMPROVEMENT
DATA
ALIGNMENT
FOR
BIT
DATA
DATA
MUST
BE
BYTE
ALIGNED
WHEN
LOADING
TO
AND
STORING
FROM
THE
BIT
XMM
REGISTERS
USED
BY
SSE
THIS
MUST
BE
DONE
TO
AVOID
SEVERE
PERFOR
MANCE
PENALTIES
AND
AT
WORST
EXECUTION
FAULTS
THERE
ARE
MOVE
INSTRUCTIONS
AND
INTRINSICS
THAT
ALLOW
UNALIGNED
DATA
TO
BE
COPIED
TO
AND
OUT
OF
XMM
REGISTERS
WHEN
NOT
USING
ALIGNED
DATA
BUT
SUCH
OPERATIONS
ARE
MUCH
SLOWER
THAN
ALIGNED
ACCESSES
IF
DATA
IS
NOT
BYTE
ALIGNED
AND
THE
PROGRAMMER
OR
THE
COMPILER
DOES
NOT
DETECT
THIS
AND
USES
THE
ALIGNED
INSTRUCTIONS
A
FAULT
OCCURS
SO
KEEP
DATA
BYTE
ALIGNED
SUCH
ALIGNMENT
ALSO
WORKS
FOR
MMX
TECHNOLOGY
CODE
EVEN
THOUGH
MMX
TECHNOLOGY
ONLY
REQUIRES
BYTE
ALIGNMENT
THE
FOLLOWING
DESCRIBES
ALIGNMENT
TECHNIQUES
FOR
PENTIUM
PROCESSOR
AS
IMPLE
MENTED
WITH
THE
INTEL
C
COMPILER
COMPILER
SUPPORTED
ALIGNMENT
THE
INTEL
C
COMPILER
PROVIDES
THE
FOLLOWING
METHODS
TO
ENSURE
THAT
THE
DATA
IS
ALIGNED
ALIGNMENT
BY
OR
DATA
TYPES
WHEN
THE
COMPILER
DETECTS
OR
DATA
DECLARATIONS
OR
PARAMETERS
IT
FORCES
ALIGNMENT
OF
THE
OBJECT
TO
A
BYTE
BOUNDARY
FOR
BOTH
GLOBAL
AND
LOCAL
DATA
AS
WELL
AS
PARAMETERS
IF
THE
DECLARATION
IS
WITHIN
A
FUNCTION
THE
COMPILER
ALSO
ALIGNS
THE
FUNCTION
STACK
FRAME
TO
ENSURE
THAT
LOCAL
DATA
AND
PARAMETERS
ARE
BYTE
ALIGNED
FOR
DETAILS
ON
THE
STACK
FRAME
LAYOUT
THAT
THE
COMPILER
GENERATES
FOR
BOTH
DEBUG
AND
OPTIMIZED
RELEASE
MODE
COMPILATIONS
REFER
TO
INTEL
COMPILER
DOCU
MENTATION
DECLSPEC
ALIGN
SPECIFICATIONS
THESE
CAN
BE
PLACED
BEFORE
DATA
DECLARATIONS
TO
FORCE
BYTE
ALIGNMENT
THIS
IS
USEFUL
FOR
LOCAL
OR
GLOBAL
DATA
DECLARATIONS
THAT
ARE
ASSIGNED
TO
BIT
DATA
TYPES
THE
SYNTAX
FOR
IT
IS
DECLSPEC
ALIGN
INTEGER
CONSTANT
WHERE
THE
INTEGER
CONSTANT
IS
AN
INTEGRAL
POWER
OF
TWO
BUT
NO
GREATER
THAN
FOR
EXAMPLE
THE
FOLLOWING
INCREASES
THE
ALIGNMENT
TO
BYTES
DECLSPEC
ALIGN
FLOAT
BUFFER
THE
VARIABLE
BUFFER
COULD
THEN
BE
USED
AS
IF
IT
CONTAINED
OBJECTS
OF
TYPE
OR
IN
THE
CODE
BELOW
THE
CONSTRUCTION
OF
THE
OBJECT
X
WILL
OCCUR
WITH
ALIGNED
DATA
VOID
FOO
X
BUFFER
WITHOUT
THE
DECLARATION
OF
DECLSPEC
ALIGN
A
FAULT
MAY
OCCUR
ALIGNMENT
BY
USING
A
UNION
STRUCTURE
WHEN
FEASIBLE
A
UNION
CAN
BE
USED
WITH
BIT
DATA
TYPES
TO
ALLOW
THE
COMPILER
TO
ALIGN
THE
DATA
STRUCTURE
BY
DEFAULT
THIS
IS
PREFERRED
TO
FORCING
ALIGNMENT
WITH
DECLSPEC
ALIGN
BECAUSE
IT
EXPOSES
THE
TRUE
PROGRAM
INTENT
TO
THE
COMPILER
IN
THAT
DATA
IS
BEING
USED
FOR
EXAMPLE
UNION
FLOAT
F
M
BUFFER
NOW
BYTE
ALIGNMENT
IS
USED
BY
DEFAULT
DUE
TO
THE
TYPE
IN
THE
UNION
IT
IS
NOT
NECESSARY
TO
USE
DECLSPEC
ALIGN
TO
FORCE
THE
RESULT
IN
C
BUT
NOT
IN
C
IT
IS
ALSO
POSSIBLE
TO
FORCE
THE
ALIGNMENT
OF
A
CLASS
STRUCT
UNION
TYPE
AS
IN
THE
CODE
THAT
FOLLOWS
STRUCT
DECLSPEC
ALIGN
FLOAT
F
IF
THE
DATA
IN
SUCH
A
CLASS
IS
GOING
TO
BE
USED
WITH
THE
STREAMING
SIMD
EXTENSIONS
OR
STREAMING
SIMD
EXTENSIONS
IT
IS
PREFERABLE
TO
USE
A
UNION
TO
MAKE
THIS
EXPLICIT
IN
C
AN
ANONYMOUS
UNION
CAN
BE
USED
TO
MAKE
THIS
MORE
CONVENIENT
CLASS
UNION
M
FLOAT
F
BECAUSE
THE
UNION
IS
ANONYMOUS
THE
NAMES
M
AND
F
CAN
BE
USED
AS
IMMEDIATE
MEMBER
NAMES
OF
MY
NOTE
THAT
DECLSPEC
ALIGN
HAS
NO
EFFECT
WHEN
APPLIED
TO
A
CLASS
STRUCT
OR
UNION
MEMBER
IN
EITHER
C
OR
C
ALIGNMENT
BY
USING
OR
DOUBLE
DATA
IN
SOME
CASES
THE
COMPILER
ALIGNS
ROUTINES
WITH
OR
DOUBLE
DATA
TO
BYTES
BY
DEFAULT
THE
COMMAND
LINE
SWITCH
LIMITS
THE
COMPILER
SO
THAT
IT
ONLY
PERFORMS
THIS
ALIGNMENT
ON
ROUTINES
THAT
CONTAIN
BIT
DATA
THE
DEFAULT
BEHAVIOR
IS
TO
USE
THIS
SWITCH
INSTRUCTS
THE
COMPLIER
TO
ALIGN
ROUTINES
WITH
OR
BYTE
DATA
TYPES
TO
BYTES
FOR
MORE
SEE
THE
INTEL
C
COMPILER
DOCUMENTATION
IMPROVING
MEMORY
UTILIZATION
MEMORY
PERFORMANCE
CAN
BE
IMPROVED
BY
REARRANGING
DATA
AND
ALGORITHMS
FOR
SSE
AND
MMX
TECHNOLOGY
INTRINSICS
METHODS
FOR
IMPROVING
MEMORY
PERFORMANCE
INVOLVE
WORKING
WITH
THE
FOLLOWING
DATA
STRUCTURE
LAYOUT
STRIP
MINING
FOR
VECTORIZATION
AND
MEMORY
UTILIZATION
LOOP
BLOCKING
USING
THE
CACHEABILITY
INSTRUCTIONS
PREFETCH
AND
STREAMING
STORE
ALSO
GREATLY
ENHANCE
MEMORY
UTILIZATION
SEE
ALSO
CHAPTER
OPTIMIZING
CACHE
USAGE
DATA
STRUCTURE
LAYOUT
FOR
CERTAIN
ALGORITHMS
LIKE
TRANSFORMATIONS
AND
LIGHTING
THERE
ARE
TWO
BASIC
WAYS
TO
ARRANGE
VERTEX
DATA
THE
TRADITIONAL
METHOD
IS
THE
ARRAY
OF
STRUCTURES
AOS
ARRANGEMENT
WITH
A
STRUCTURE
FOR
EACH
VERTEX
EXAMPLE
HOWEVER
THIS
METHOD
DOES
NOT
TAKE
FULL
ADVANTAGE
OF
SIMD
TECHNOLOGY
CAPABILITIES
EXAMPLE
AOS
DATA
STRUCTURE
TYPEDEF
STRUCT
FLOAT
X
Y
Z
INT
A
B
C
VERTEX
VERTEX
VERTICES
NUMOFVERTICES
THE
BEST
PROCESSING
METHOD
FOR
CODE
USING
SIMD
TECHNOLOGY
IS
TO
ARRANGE
THE
DATA
IN
AN
ARRAY
FOR
EACH
COORDINATE
EXAMPLE
THIS
DATA
ARRANGEMENT
IS
CALLED
STRUC
TURE
OF
ARRAYS
SOA
EXAMPLE
SOA
DATA
STRUCTURE
THERE
ARE
TWO
OPTIONS
FOR
COMPUTING
DATA
IN
AOS
FORMAT
PERFORM
OPERATION
ON
THE
DATA
AS
IT
STANDS
IN
AOS
FORMAT
OR
RE
ARRANGE
IT
SWIZZLE
IT
INTO
SOA
FORMAT
DYNAMI
CALLY
SEE
EXAMPLE
FOR
CODE
SAMPLES
OF
EACH
OPTION
BASED
ON
A
DOT
PRODUCT
COMPUTATION
EXAMPLE
AOS
AND
SOA
CODE
SAMPLES
THE
DOT
PRODUCT
OF
AN
ARRAY
OF
VECTORS
ARRAY
AND
A
FIXED
VECTOR
FIXED
IS
A
COMMON
OPERATION
IN
LIGHTING
OPERATIONS
WHERE
ARRAY
AND
FIXED
XF
YF
ZF
A
DOT
PRODUCT
IS
DEFINED
AS
THE
SCALAR
QUANTITY
XF
YF
ZF
AOS
CODE
ALL
VALUES
MARKED
DC
ARE
DON
T
CARE
EXAMPLE
AOS
AND
SOA
CODE
SAMPLES
CONTD
IN
THE
AOS
MODEL
THE
VERTICES
ARE
STORED
IN
THE
XYZ
FORMAT
MOVAPS
ARRAY
DC
MOVAPS
FIXED
DC
XF
YF
ZF
MULPS
DC
XF
YF
ZF
MOVHLPS
XMM
XMM
DC
DC
DC
XF
ADDPS
DC
DC
DC
XF
ZFMOVAPS
SHUFPS
DC
DC
DC
YF
ADDPS
DC
DC
DC
XF
YF
ZF
SOA
CODE
X
Y
Z
A
XF
XF
XF
XF
B
YF
YF
YF
YF
C
ZF
ZF
ZF
ZF
MOVAPS
X
MOVAPS
Y
MOVAPS
Z
MULPS
A
XF
XF
XF
XF
MULPS
B
YF
YF
YF
XF
MULPS
C
ZF
ZF
ZF
ZF
ADDPS
ADDPS
XF
YF
ZF
PERFORMING
SIMD
OPERATIONS
ON
THE
ORIGINAL
AOS
FORMAT
CAN
REQUIRE
MORE
CALCULATIONS
AND
SOME
OPERATIONS
DO
NOT
TAKE
ADVANTAGE
OF
ALL
SIMD
ELEMENTS
AVAILABLE
THERE
FORE
THIS
OPTION
IS
GENERALLY
LESS
EFFICIENT
THE
RECOMMENDED
WAY
FOR
COMPUTING
DATA
IN
AOS
FORMAT
IS
TO
SWIZZLE
EACH
SET
OF
ELEMENTS
TO
SOA
FORMAT
BEFORE
PROCESSING
IT
USING
SIMD
TECHNOLOGIES
SWIZZLING
CAN
EITHER
BE
DONE
DYNAMICALLY
DURING
PROGRAM
EXECUTION
OR
STATICALLY
WHEN
THE
DATA
STRUCTURES
ARE
GENERATED
SEE
CHAPTER
AND
CHAPTER
FOR
EXAMPLES
PERFORMING
THE
SWIZZLE
DYNAMICALLY
IS
USUALLY
BETTER
THAN
USING
AOS
BUT
CAN
BE
SOMEWHAT
INEFFICIENT
BECAUSE
THERE
ARE
EXTRA
INSTRUCTIONS
DURING
COMPUTATION
PERFORMING
THE
SWIZZLE
STATICALLY
WHEN
DATA
STRUCTURES
ARE
BEING
LAID
OUT
IS
BEST
AS
THERE
IS
NO
RUNTIME
OVER
HEAD
AS
MENTIONED
EARLIER
THE
SOA
ARRANGEMENT
ALLOWS
MORE
EFFICIENT
USE
OF
THE
PARAL
LELISM
OF
SIMD
TECHNOLOGIES
BECAUSE
THE
DATA
IS
READY
FOR
COMPUTATION
IN
A
MORE
OPTIMAL
VERTICAL
MANNER
MULTIPLYING
COMPONENTS
BY
XF
XF
XF
XF
USING
SIMD
EXECUTION
SLOTS
TO
PRODUCE
UNIQUE
RESULTS
IN
CONTRAST
COMPUTING
DIRECTLY
ON
AOS
DATA
CAN
LEAD
TO
HORIZONTAL
OPERATIONS
THAT
CONSUME
SIMD
EXECUTION
SLOTS
BUT
PRODUCE
ONLY
A
SINGLE
SCALAR
RESULT
AS
SHOWN
BY
THE
MANY
DON
T
CARE
DC
SLOTS
IN
EXAMPLE
USE
OF
THE
SOA
FORMAT
FOR
DATA
STRUCTURES
CAN
LEAD
TO
MORE
EFFICIENT
USE
OF
CACHES
AND
BANDWIDTH
WHEN
THE
ELEMENTS
OF
THE
STRUCTURE
ARE
NOT
ACCESSED
WITH
EQUAL
FREQUENCY
SUCH
AS
WHEN
ELEMENT
X
Y
Z
ARE
ACCESSED
TEN
TIMES
MORE
OFTEN
THAN
THE
OTHER
ENTRIES
THEN
SOA
SAVES
MEMORY
AND
PREVENTS
FETCHING
UNNECESSARY
DATA
ITEMS
A
B
AND
C
EXAMPLE
HYBRID
SOA
DATA
STRUCTURE
NUMOFGROUPS
NUMOFVERTICES
SIMDWIDTH
TYPEDEF
STRUCT
FLOAT
X
SIMDWIDTH
FLOAT
Y
SIMDWIDTH
FLOAT
Z
SIMDWIDTH
VERTICESCOORDLIST
TYPEDEF
STRUCT
INT
A
SIMDWIDTH
INT
B
SIMDWIDTH
INT
C
SIMDWIDTH
VERTICESCOLORLIST
VERTICESCOORDLIST
VERTICESCOORD
NUMOFGROUPS
VERTICESCOLORLIST
VERTICESCOLOR
NUMOFGROUPS
NOTE
THAT
SOA
CAN
HAVE
THE
DISADVANTAGE
OF
REQUIRING
MORE
INDEPENDENT
MEMORY
STREAM
REFERENCES
A
COMPUTATION
THAT
USES
ARRAYS
X
Y
AND
Z
SEE
EXAMPLE
WOULD
REQUIRE
THREE
SEPARATE
DATA
STREAMS
THIS
CAN
REQUIRE
THE
USE
OF
MORE
PREFETCHES
ADDITIONAL
ADDRESS
GENERATION
CALCULATIONS
AS
WELL
AS
HAVING
A
GREATER
IMPACT
ON
DRAM
PAGE
ACCESS
EFFICIENCY
THERE
IS
AN
ALTERNATIVE
A
HYBRID
SOA
APPROACH
BLENDS
THE
TWO
ALTERNATIVES
SEE
EXAMPLE
IN
THIS
CASE
ONLY
SEPARATE
ADDRESS
STREAMS
ARE
GENERATED
AND
REFERENCED
ONE
CONTAINS
XXXX
YYYY
ZZZZ
ZZZZ
AND
THE
OTHER
AAAA
BBBB
CCCC
AAAA
DDDD
THE
APPROACH
PREVENTS
FETCHING
UNNECESSARY
DATA
ASSUMING
THE
VARIABLES
X
Y
Z
ARE
ALWAYS
USED
TOGETHER
WHEREAS
THE
VARIABLES
A
B
C
WOULD
ALSO
BE
USED
TOGETHER
BUT
NOT
AT
THE
SAME
TIME
AS
X
Y
Z
THE
HYBRID
SOA
APPROACH
ENSURES
DATA
IS
ORGANIZED
TO
ENABLE
MORE
EFFICIENT
VERTICAL
SIMD
COMPUTATION
SIMPLER
LESS
ADDRESS
GENERATION
THAN
AOS
FEWER
STREAMS
WHICH
REDUCES
DRAM
PAGE
MISSES
USE
OF
FEWER
PREFETCHES
DUE
TO
FEWER
STREAMS
EFFICIENT
CACHE
LINE
PACKING
OF
DATA
ELEMENTS
THAT
ARE
USED
CONCURRENTLY
WITH
THE
ADVENT
OF
THE
SIMD
TECHNOLOGIES
THE
CHOICE
OF
DATA
ORGANIZATION
BECOMES
MORE
IMPORTANT
AND
SHOULD
BE
CAREFULLY
BASED
ON
THE
OPERATIONS
TO
BE
PERFORMED
ON
THE
DATA
THIS
WILL
BECOME
INCREASINGLY
IMPORTANT
IN
THE
PENTIUM
PROCESSOR
AND
FUTURE
PROCESSORS
IN
SOME
APPLICATIONS
TRADITIONAL
DATA
ARRANGEMENTS
MAY
NOT
LEAD
TO
THE
MAXIMUM
PERFORMANCE
APPLICATION
DEVELOPERS
ARE
ENCOURAGED
TO
EXPLORE
DIFFERENT
DATA
ARRANGEMENTS
AND
DATA
SEGMENTATION
POLICIES
FOR
EFFICIENT
COMPUTA
TION
THIS
MAY
MEAN
USING
A
COMBINATION
OF
AOS
SOA
AND
HYBRID
SOA
IN
A
GIVEN
APPLICATION
STRIP
MINING
STRIP
MINING
ALSO
KNOWN
AS
LOOP
SECTIONING
IS
A
LOOP
TRANSFORMATION
TECHNIQUE
FOR
ENABLING
SIMD
ENCODINGS
OF
LOOPS
AS
WELL
AS
PROVIDING
A
MEANS
OF
IMPROVING
MEMORY
PERFORMANCE
FIRST
INTRODUCED
FOR
VECTORIZERS
THIS
TECHNIQUE
CONSISTS
OF
THE
GENERATION
OF
CODE
WHEN
EACH
VECTOR
OPERATION
IS
DONE
FOR
A
SIZE
LESS
THAN
OR
EQUAL
TO
THE
MAXIMUM
VECTOR
LENGTH
ON
A
GIVEN
VECTOR
MACHINE
BY
FRAGMENTING
A
LARGE
LOOP
INTO
SMALLER
SEGMENTS
OR
STRIPS
THIS
TECHNIQUE
TRANSFORMS
THE
LOOP
STRUCTURE
BY
INCREASING
THE
TEMPORAL
AND
SPATIAL
LOCALITY
IN
THE
DATA
CACHE
IF
THE
DATA
ARE
REUSABLE
IN
DIFFERENT
PASSES
OF
AN
ALGORITHM
REDUCING
THE
NUMBER
OF
ITERATIONS
OF
THE
LOOP
BY
A
FACTOR
OF
THE
LENGTH
OF
EACH
VECTOR
OR
NUMBER
OF
OPERATIONS
BEING
PERFORMED
PER
SIMD
OPERATION
IN
THE
CASE
OF
STREAMING
SIMD
EXTENSIONS
THIS
VECTOR
OR
STRIP
LENGTH
IS
REDUCED
BY
TIMES
FOUR
FLOATING
POINT
DATA
ITEMS
PER
SINGLE
STREAMING
SIMD
EXTENSIONS
SINGLE
PRECISION
FLOATING
POINT
SIMD
OPERATION
ARE
PROCESSED
CONSIDER
EXAMPLE
EXAMPLE
PSEUDO
CODE
BEFORE
STRIP
MINING
TYPEDEF
STRUCT
FLOAT
X
Y
Z
NX
NY
NZ
U
V
MAIN
V
NUM
FOR
I
I
NUM
I
TRANSFORM
V
I
EXAMPLE
PSEUDO
CODE
BEFORE
STRIP
MINING
CONTD
FOR
I
I
NUM
I
LIGHTING
V
I
THE
MAIN
LOOP
CONSISTS
OF
TWO
FUNCTIONS
TRANSFORMATION
AND
LIGHTING
FOR
EACH
OBJECT
THE
MAIN
LOOP
CALLS
A
TRANSFORMATION
ROUTINE
TO
UPDATE
SOME
DATA
THEN
CALLS
THE
LIGHTING
ROUTINE
TO
FURTHER
WORK
ON
THE
DATA
IF
THE
SIZE
OF
ARRAY
V
NUM
IS
LARGER
THAN
THE
CACHE
THEN
THE
COORDINATES
FOR
V
I
THAT
WERE
CACHED
DURING
TRANSFORM
V
I
WILL
BE
EVICTED
FROM
THE
CACHE
BY
THE
TIME
WE
DO
LIGHTING
V
I
THIS
MEANS
THAT
V
I
WILL
HAVE
TO
BE
FETCHED
FROM
MAIN
MEMORY
A
SECOND
TIME
REDUCING
PERFORMANCE
IN
EXAMPLE
THE
COMPUTATION
HAS
BEEN
STRIP
MINED
TO
A
SIZE
THE
VALUE
IS
CHOSEN
SUCH
THAT
ELEMENTS
OF
ARRAY
V
NUM
FIT
INTO
THE
CACHE
HIERARCHY
BY
DOING
THIS
A
GIVEN
ELEMENT
V
I
BROUGHT
INTO
THE
CACHE
BY
TRANSFORM
V
I
WILL
STILL
BE
IN
THE
CACHE
WHEN
WE
PERFORM
LIGHTING
V
I
AND
THUS
IMPROVE
PERFORMANCE
OVER
THE
NON
STRIP
MINED
CODE
EXAMPLE
STRIP
MINED
CODE
MAIN
V
NUM
FOR
I
I
NUM
I
FOR
J
I
J
MIN
NUM
I
J
TRANSFORM
V
J
FOR
J
I
J
MIN
NUM
I
J
LIGHTING
V
J
LOOP
BLOCKING
LOOP
BLOCKING
IS
ANOTHER
USEFUL
TECHNIQUE
FOR
MEMORY
PERFORMANCE
OPTIMIZATION
THE
MAIN
PURPOSE
OF
LOOP
BLOCKING
IS
ALSO
TO
ELIMINATE
AS
MANY
CACHE
MISSES
AS
POSSIBLE
THIS
TECHNIQUE
TRANSFORMS
THE
MEMORY
DOMAIN
OF
A
GIVEN
PROBLEM
INTO
SMALLER
CHUNKS
RATHER
THAN
SEQUENTIALLY
TRAVERSING
THROUGH
THE
ENTIRE
MEMORY
DOMAIN
EACH
CHUNK
SHOULD
BE
SMALL
ENOUGH
TO
FIT
ALL
THE
DATA
FOR
A
GIVEN
COMPUTATION
INTO
THE
CACHE
THEREBY
MAXIMIZING
DATA
REUSE
IN
FACT
ONE
CAN
TREAT
LOOP
BLOCKING
AS
STRIP
MINING
IN
TWO
OR
MORE
DIMENSIONS
CONSIDER
THE
CODE
IN
EXAMPLE
AND
ACCESS
PATTERN
IN
FIGURE
THE
TWO
DIMENSIONAL
ARRAY
A
IS
REFERENCED
IN
THE
J
COLUMN
DIRECTION
AND
THEN
REFERENCED
IN
THE
I
ROW
DIRECTION
COLUMN
MAJOR
ORDER
WHEREAS
ARRAY
B
IS
REFERENCED
IN
THE
OPPOSITE
MANNER
ROW
MAJOR
ORDER
ASSUME
THE
MEMORY
LAYOUT
IS
IN
COLUMN
MAJOR
ORDER
THEREFORE
THE
ACCESS
STRIDES
OF
ARRAY
A
AND
B
FOR
THE
CODE
IN
EXAMPLE
WOULD
BE
AND
MAX
RESPECTIVELY
EXAMPLE
LOOP
BLOCKING
A
ORIGINAL
LOOP
FLOAT
A
MAX
MAX
B
MAX
MAX
FOR
I
I
MAX
I
FOR
J
J
MAX
J
A
I
J
A
I
J
B
J
I
B
TRANSFORMED
LOOP
AFTER
BLOCKING
FLOAT
A
MAX
MAX
B
MAX
MAX
FOR
I
I
MAX
I
FOR
J
J
MAX
J
FOR
II
I
II
I
II
FOR
JJ
J
JJ
J
JJ
A
II
JJ
A
II
JJ
B
JJ
II
FOR
THE
FIRST
ITERATION
OF
THE
INNER
LOOP
EACH
ACCESS
TO
ARRAY
B
WILL
GENERATE
A
CACHE
MISS
IF
THE
SIZE
OF
ONE
ROW
OF
ARRAY
A
THAT
IS
A
MAX
IS
LARGE
ENOUGH
BY
THE
TIME
THE
SECOND
ITERATION
STARTS
EACH
ACCESS
TO
ARRAY
B
WILL
ALWAYS
GENERATE
A
CACHE
MISS
FOR
INSTANCE
ON
THE
FIRST
ITERATION
THE
CACHE
LINE
CONTAINING
B
WILL
BE
BROUGHT
IN
WHEN
B
IS
REFERENCED
BECAUSE
THE
FLOAT
TYPE
VARIABLE
IS
FOUR
BYTES
AND
EACH
CACHE
LINE
IS
BYTES
DUE
TO
THE
LIMITATION
OF
CACHE
CAPACITY
THIS
LINE
WILL
BE
EVICTED
DUE
TO
CONFLICT
MISSES
BEFORE
THE
INNER
LOOP
REACHES
THE
END
FOR
THE
NEXT
ITERATION
OF
THE
OUTER
LOOP
ANOTHER
CACHE
MISS
WILL
BE
GENERATED
WHILE
REFERENCING
B
IN
THIS
MANNER
A
CACHE
MISS
OCCURS
WHEN
EACH
ELEMENT
OF
ARRAY
B
IS
REFER
ENCED
THAT
IS
THERE
IS
NO
DATA
REUSE
IN
THE
CACHE
AT
ALL
FOR
ARRAY
B
THIS
SITUATION
CAN
BE
AVOIDED
IF
THE
LOOP
IS
BLOCKED
WITH
RESPECT
TO
THE
CACHE
SIZE
IN
FIGURE
A
IS
SELECTED
AS
THE
LOOP
BLOCKING
FACTOR
SUPPOSE
THAT
IS
THEN
THE
BLOCKED
CHUNK
OF
EACH
ARRAY
WILL
BE
EIGHT
CACHE
LINES
BYTES
EACH
IN
THE
FIRST
ITERATION
OF
THE
INNER
LOOP
A
AND
B
WILL
BE
BROUGHT
INTO
THE
CACHE
B
WILL
BE
COMPLETELY
CONSUMED
BY
THE
FIRST
ITERATION
OF
THE
OUTER
LOOP
CONSEQUENTLY
B
WILL
ONLY
EXPERIENCE
ONE
CACHE
MISS
AFTER
APPLYING
LOOP
BLOCKING
OPTIMIZATION
IN
LIEU
OF
EIGHT
MISSES
FOR
THE
ORIGINAL
ALGORITHM
AS
ILLUSTRATED
IN
FIGURE
ARRAYS
A
AND
B
ARE
BLOCKED
INTO
SMALLER
RECTANGULAR
CHUNKS
SO
THAT
THE
TOTAL
SIZE
OF
TWO
BLOCKED
A
AND
B
CHUNKS
IS
SMALLER
THAN
THE
CACHE
SIZE
THIS
ALLOWS
MAXIMUM
DATA
REUSE
FIGURE
LOOP
BLOCKING
ACCESS
PATTERN
AS
ONE
CAN
SEE
ALL
THE
REDUNDANT
CACHE
MISSES
CAN
BE
ELIMINATED
BY
APPLYING
THIS
LOOP
BLOCKING
TECHNIQUE
IF
MAX
IS
HUGE
LOOP
BLOCKING
CAN
ALSO
HELP
REDUCE
THE
PENALTY
FROM
DTLB
DATA
TRANSLATION
LOOK
ASIDE
BUFFER
MISSES
IN
ADDITION
TO
IMPROVING
THE
CACHE
MEMORY
PERFORMANCE
THIS
OPTIMIZATION
TECHNIQUE
ALSO
SAVES
EXTERNAL
BUS
BANDWIDTH
INSTRUCTION
SELECTION
THE
FOLLOWING
SECTION
GIVES
SOME
GUIDELINES
FOR
CHOOSING
INSTRUCTIONS
TO
COMPLETE
A
TASK
ONE
BARRIER
TO
SIMD
COMPUTATION
CAN
BE
THE
EXISTENCE
OF
DATA
DEPENDENT
BRANCHES
CONDITIONAL
MOVES
CAN
BE
USED
TO
ELIMINATE
DATA
DEPENDENT
BRANCHES
CONDITIONAL
MOVES
CAN
BE
EMULATED
IN
SIMD
COMPUTATION
BY
USING
MASKED
COMPARES
AND
LOGI
CALS
AS
SHOWN
IN
EXAMPLE
PROVIDES
PACKED
BLEND
INSTRUCTION
THAT
CAN
VECTORIZE
DATA
DEPENDENT
BRANCHES
IN
A
LOOP
EXAMPLE
EMULATION
OF
CONDITIONAL
MOVES
HIGH
LEVEL
CODE
DECLSPEC
ALIGN
SHORT
A
B
C
D
E
FOR
I
I
I
IF
A
I
B
I
C
I
D
I
ELSE
C
I
E
I
MMX
ASSEMBLY
CODE
PROCESSES
SHORT
VALUES
PER
ITERATION
XOR
EAX
EAX
MOVQ
A
EAX
B
EAX
CREATE
COMPARE
MASK
MOVQ
D
EAX
PAND
DROP
ELEMENTS
WHERE
A
B
PANDN
E
EAX
DROP
ELEMENTS
WHERE
A
B
POR
CRETE
SINGLE
WORD
MOVQ
C
EAX
ADD
EAX
CMP
EAX
JLE
ASSEMBLY
PROCESSES
SHORT
VALUES
PER
ITERATION
XOR
EAX
EAX
MOVDQQ
A
EAX
B
EAX
CREATE
COMPARE
MASK
MOVDQA
E
EAX
PBLENDV
D
EAX
MOVDQA
C
EAX
ADD
EAX
CMP
EAX
JLE
IF
THERE
ARE
MULTIPLE
CONSUMERS
OF
AN
INSTANCE
OF
A
REGISTER
GROUP
THE
CONSUMERS
TOGETHER
AS
CLOSELY
AS
POSSIBLE
HOWEVER
THE
CONSUMERS
SHOULD
NOT
BE
SCHEDULED
NEAR
THE
PRODUCER
SIMD
OPTIMIZATIONS
AND
MICROARCHITECTURES
PENTIUM
M
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
HAVE
A
DIFFERENT
MICROAR
CHITECTURE
THAN
INTEL
NETBURST
MICROARCHITECTURE
THE
FOLLOWING
SUB
SECTION
DISCUSSES
OPTIMIZING
SIMD
CODE
TARGETING
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
THE
REGISTER
REGISTER
VARIANT
OF
THE
FOLLOWING
INSTRUCTIONS
HAS
IMPROVED
PERFORMANCE
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSOR
RELATIVE
TO
PENTIUM
M
PROCESSORS
THIS
IS
BECAUSE
THE
INSTRUCTIONS
CONSIST
OF
TWO
MICRO
OPS
INSTEAD
OF
THREE
RELEVANT
INSTRUCTIONS
ARE
UNPCKLPS
UNPCKHPS
PACKSSWB
PACKUSWB
PACKSSDW
PSHUFD
SHUFFPS
AND
SHUFFPD
RECOMMENDATION
WHEN
TARGETING
CODE
GENERATION
FOR
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
FAVOR
INSTRUCTIONS
CONSISTING
OF
TWO
OPS
OVER
THOSE
WITH
MORE
THAN
TWO
OPS
INTEL
CORE
MICROARCHITECTURE
GENERALLY
EXECUTES
SIMD
INSTRUCTIONS
MORE
EFFICIENTLY
THAN
PREVIOUS
MICROARCHITECTURES
IN
TERMS
OF
LATENCY
AND
THROUGHPUT
MOST
BIT
SIMD
OPERATIONS
HAVE
CYCLE
THROUGHPUT
EXCEPT
SHUFFLE
PACK
UNPACK
OPERATIONS
MANY
OF
THE
RESTRICTIONS
SPECIFIC
TO
INTEL
CORE
DUO
INTEL
CORE
SOLO
PROCESSORS
SUCH
AS
BIT
SIMD
OPERATIONS
HAVING
CYCLE
THROUGHPUT
AT
A
MINIMUM
DO
NOT
APPLY
TO
INTEL
CORE
MICROARCHITECTURE
THE
SAME
IS
TRUE
OF
INTEL
CORE
MICROARCHITECTURE
RELA
TIVE
TO
INTEL
NETBURST
MICROARCHITECTURES
ENHANCED
INTEL
CORE
MICROARCHITECTURE
PROVIDES
DEDICATED
BIT
SHUFFLER
AND
RADIX
DIVIDER
HARDWARE
THESE
CAPABILITIES
AND
INSTRUCTIONS
WILL
MAKE
VECTORIZA
TION
USING
BIT
SIMD
INSTRUCTIONS
EVEN
MORE
EFFICIENT
AND
EFFECTIVE
RECOMMENDATION
WITH
THE
PROLIFERATION
OF
BIT
SIMD
HARDWARE
IN
INTEL
CORE
MICROARCHITECTURE
AND
ENHANCED
INTEL
CORE
MICROARCHITECTURE
INTEGER
SIMD
CODE
WRITTEN
USING
MMX
INSTRUCTIONS
SHOULD
CONSIDER
MORE
EFFICIENT
IMPLEMENTATIONS
USING
BIT
SIMD
INSTRUCTIONS
TUNING
THE
FINAL
APPLICATION
THE
BEST
WAY
TO
TUNE
YOUR
APPLICATION
ONCE
IT
IS
FUNCTIONING
CORRECTLY
IS
TO
USE
A
PROFILER
THAT
MEASURES
THE
APPLICATION
WHILE
IT
IS
RUNNING
ON
A
SYSTEM
VTUNE
ANALYZER
CAN
HELP
YOU
DETERMINE
WHERE
TO
MAKE
CHANGES
IN
YOUR
APPLICATION
TO
IMPROVE
PERFORMANCE
USING
THE
VTUNE
ANALYZER
CAN
HELP
YOU
WITH
VARIOUS
PHASES
REQUIRED
FOR
OPTIMIZED
PERFORMANCE
SEE
APPENDIX
A
INTEL
VTUNE
PERFORMANCE
ANALYZER
FOR
DETAILS
AFTER
EVERY
EFFORT
TO
OPTIMIZE
YOU
SHOULD
CHECK
THE
PERFORMANCE
GAINS
TO
SEE
WHERE
YOU
ARE
MAKING
YOUR
MAJOR
OPTIMIZATION
GAINS
CHAPTER
OPTIMIZING
FOR
SIMD
INTEGER
APPLICATIONS
SIMD
INTEGER
INSTRUCTIONS
PROVIDE
PERFORMANCE
IMPROVEMENTS
IN
APPLICATIONS
THAT
ARE
INTEGER
INTENSIVE
AND
CAN
TAKE
ADVANTAGE
OF
SIMD
ARCHITECTURE
GUIDELINES
IN
THIS
CHAPTER
FOR
USING
SIMD
INTEGER
INSTRUCTIONS
IN
ADDITION
TO
THOSE
DESCRIBED
IN
CHAPTER
MAY
BE
USED
TO
DEVELOP
FAST
AND
EFFICIENT
CODE
THAT
SCALES
ACROSS
PROCESSOR
GENERATIONS
THE
COLLECTION
OF
BIT
AND
BIT
SIMD
INTEGER
INSTRUCTIONS
SUPPORTED
BY
MMX
TECHNOLOGY
SSE
AND
PCMPEQQ
IN
ARE
REFERRED
TO
AS
SIMD
INTEGER
INSTRUCTIONS
CODE
SEQUENCES
IN
THIS
CHAPTER
DEMONSTRATES
THE
USE
OF
BASIC
BIT
SIMD
INTEGER
INSTRUCTIONS
AND
MORE
EFFICIENT
BIT
SIMD
INTEGER
INSTRUCTIONS
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
SUPPORT
MMX
SSE
AND
PROCESSORS
BASED
ON
ENHANCED
INTEL
CORE
MICROARCHITECTURE
SUPPORT
AND
ALL
PREVIOUS
GENERATIONS
OF
SIMD
INTEGER
INSTRUCTIONS
PROCESSORS
BASED
ON
INTEL
MICROARCHITECTURE
NEHALEM
SUPPORTS
MMX
SSE
AND
SINGLE
INSTRUCTION
MULTIPLE
DATA
TECHNIQUES
CAN
BE
APPLIED
TO
TEXT
STRING
PROCESSING
LEXING
AND
PARSER
APPLICATIONS
SIMD
PROGRAMMING
IN
STRING
TEXT
PROCESSING
AND
LEXING
APPLICATIONS
OFTEN
REQUIRE
SOPHISTICATED
TECHNIQUES
BEYOND
THOSE
COMMONLY
USED
IN
SIMD
INTEGER
PROGRAMMING
THIS
IS
COVERED
IN
CHAPTER
AND
SIMD
PROGRAMMING
FOR
TEXT
PROCESSING
LEXING
PARSING
EXECUTION
OF
BIT
SIMD
INTEGER
INSTRUCTIONS
IN
INTEL
CORE
MICROARCHITECTURE
AND
ENHANCED
INTEL
CORE
MICROARCHITECTURE
ARE
SUBSTANTIALLY
MORE
EFFICIENT
THAN
ON
PREVIOUS
MICROARCHITECTURES
THUS
NEWER
SIMD
CAPABILITIES
INTRODUCED
IN
OPERATE
ON
BIT
OPERANDS
AND
DO
NOT
INTRODUCE
EQUIVALENT
BIT
SIMD
CAPABILI
TIES
CONVERSION
FROM
BIT
SIMD
INTEGER
CODE
TO
BIT
SIMD
INTEGER
CODE
IS
HIGHLY
RECOMMENDED
THIS
CHAPTER
CONTAINS
EXAMPLES
THAT
WILL
HELP
YOU
TO
GET
STARTED
WITH
CODING
YOUR
APPLICATION
THE
GOAL
IS
TO
PROVIDE
SIMPLE
LOW
LEVEL
OPERATIONS
THAT
ARE
FREQUENTLY
USED
THE
EXAMPLES
USE
A
MINIMUM
NUMBER
OF
INSTRUCTIONS
NECESSARY
TO
ACHIEVE
BEST
PERFORMANCE
ON
THE
CURRENT
GENERATION
OF
INTEL
AND
IA
PROCESSORS
EACH
EXAMPLE
INCLUDES
A
SHORT
DESCRIPTION
SAMPLE
CODE
AND
NOTES
IF
NECESSARY
THESE
EXAMPLES
DO
NOT
ADDRESS
SCHEDULING
AS
IT
IS
ASSUMED
THE
EXAMPLES
WILL
BE
INCORPORATED
IN
LONGER
CODE
SEQUENCES
FOR
PLANNING
CONSIDERATIONS
OF
USING
THE
SIMD
INTEGER
INSTRUCTIONS
REFER
TO
SECTION
GENERAL
RULES
ON
SIMD
INTEGER
CODE
GENERAL
RULES
AND
SUGGESTIONS
ARE
DO
NOT
INTERMIX
BIT
SIMD
INTEGER
INSTRUCTIONS
WITH
FLOATING
POINT
INSTRUC
TIONS
SEE
SECTION
USING
SIMD
INTEGER
WITH
FLOATING
POINT
NOTE
THAT
ALL
SIMD
INTEGER
INSTRUCTIONS
CAN
BE
INTERMIXED
WITHOUT
PENALTY
FAVOR
BIT
SIMD
INTEGER
CODE
OVER
BIT
SIMD
INTEGER
CODE
ON
MICROARCHI
TECTURES
PRIOR
TO
INTEL
CORE
MICROARCHITECTURE
MOST
BIT
SIMD
INSTRUCTIONS
HAVE
TWO
CYCLE
THROUGHPUT
RESTRICTIONS
DUE
TO
THE
UNDERLYING
BIT
DATA
PATH
IN
THE
EXECUTION
ENGINE
INTEL
CORE
MICROARCHITECTURE
EXECUTES
MOST
SIMD
INSTRUC
TIONS
EXCEPT
SHUFFLE
PACK
UNPACK
OPERATIONS
WITH
ONE
CYCLE
THROUGHPUT
AND
PROVIDES
THREE
PORTS
TO
EXECUTE
MULTIPLE
SIMD
INSTRUCTIONS
IN
PARALLEL
ENHANCED
INTEL
CORE
MICROARCHITECTURE
SPEEDS
UP
BIT
SHUFFLE
PACK
UNPACK
OPERATIONS
WITH
CYCLE
THROUGHPUT
WHEN
WRITING
SIMD
CODE
THAT
WORKS
FOR
BOTH
INTEGER
AND
FLOATING
POINT
DATA
USE
THE
SUBSET
OF
SIMD
CONVERT
INSTRUCTIONS
OR
LOAD
STORE
INSTRUCTIONS
TO
ENSURE
THAT
THE
INPUT
OPERANDS
IN
XMM
REGISTERS
CONTAIN
DATA
TYPES
THAT
ARE
PROPERLY
DEFINED
TO
MATCH
THE
INSTRUCTION
CODE
SEQUENCES
CONTAINING
CROSS
TYPED
USAGE
PRODUCE
THE
SAME
RESULT
ACROSS
DIFFERENT
IMPLEMENTATIONS
BUT
INCUR
A
SIGNIFICANT
PERFORMANCE
PENALTY
USING
SSE
INSTRUCTIONS
TO
OPERATE
ON
TYPE
MISMATCHED
SIMD
DATA
IN
THE
XMM
REGISTER
IS
STRONGLY
DISCOURAGED
USE
THE
OPTIMIZATION
RULES
AND
GUIDELINES
DESCRIBED
IN
CHAPTER
AND
CHAPTER
TAKE
ADVANTAGE
OF
HARDWARE
PREFETCHER
WHERE
POSSIBLE
USE
THE
PREFETCH
INSTRUCTION
ONLY
WHEN
DATA
ACCESS
PATTERNS
ARE
IRREGULAR
AND
PREFETCH
DISTANCE
CAN
BE
PRE
DETERMINED
SEE
CHAPTER
OPTIMIZING
CACHE
USAGE
EMULATE
CONDITIONAL
MOVES
BY
USING
BLEND
MASKED
COMPARES
AND
LOGICALS
INSTEAD
OF
USING
CONDITIONAL
BRANCHES
USING
SIMD
INTEGER
WITH
FLOATING
POINT
ALL
BIT
SIMD
INTEGER
INSTRUCTIONS
USE
MMX
REGISTERS
WHICH
SHARE
REGISTER
STATE
WITH
THE
FLOATING
POINT
STACK
BECAUSE
OF
THIS
SHARING
CERTAIN
RULES
AND
CONSIDER
ATIONS
APPLY
INSTRUCTIONS
USING
MMX
REGISTERS
CANNOT
BE
FREELY
INTERMIXED
WITH
FLOATING
POINT
REGISTERS
TAKE
CARE
WHEN
SWITCHING
BETWEEN
BIT
SIMD
INTEGER
INSTRUCTIONS
AND
FLOATING
POINT
INSTRUCTIONS
TO
ENSURE
FUNCTIONAL
CORRECTNESS
SEE
SECTION
BOTH
SECTION
AND
SECTION
APPLY
ONLY
TO
SOFTWARE
THAT
EMPLOYS
MMX
INSTRUCTIONS
AS
NOTED
BEFORE
BIT
SIMD
INTEGER
INSTRUCTIONS
SHOULD
BE
FAVORED
TO
REPLACE
MMX
CODE
AND
ACHIEVE
HIGHER
PERFORMANCE
THAT
ALSO
OBVIATES
THE
NEED
TO
USE
EMMS
AND
THE
PERFORMANCE
PENALTY
OF
USING
EMMS
WHEN
INTERMIXING
MMX
AND
INSTRUCTIONS
FOR
PERFORMANCE
CONSIDERATIONS
THERE
IS
NO
PENALTY
OF
INTERMIXING
SIMD
FLOATING
POINT
OPERATIONS
AND
BIT
SIMD
INTEGER
OPERATIONS
AND
FLOATING
POINT
OPERA
TIONS
USING
THE
EMMS
INSTRUCTION
WHEN
GENERATING
BIT
SIMD
INTEGER
CODE
KEEP
IN
MIND
THAT
THE
EIGHT
MMX
REGIS
TERS
ARE
ALIASED
TO
FLOATING
POINT
REGISTERS
SWITCHING
FROM
MMX
INSTRUCTIONS
TO
FLOATING
POINT
INSTRUCTIONS
INCURS
A
FINITE
DELAY
SO
IT
IS
THE
BEST
TO
MINIMIZE
SWITCHING
BETWEEN
THESE
INSTRUCTION
TYPES
BUT
WHEN
SWITCHING
THE
EMMS
INSTRUCTION
PROVIDES
AN
EFFICIENT
MEANS
TO
CLEAR
THE
STACK
SO
THAT
SUBSEQUENT
CODE
CAN
OPERATE
PROPERLY
AS
SOON
AS
AN
INSTRUCTION
MAKES
REFERENCE
TO
AN
MMX
REGISTER
ALL
VALID
BITS
IN
THE
FLOATING
POINT
TAG
WORD
ARE
SET
WHICH
IMPLIES
THAT
ALL
REGISTERS
CONTAIN
VALID
VALUES
IN
ORDER
FOR
SOFTWARE
TO
OPERATE
CORRECTLY
THE
FLOATING
POINT
STACK
SHOULD
BE
EMPTIED
WHEN
STARTING
A
SERIES
OF
FLOATING
POINT
CALCULATIONS
AFTER
OPERATING
ON
THE
MMX
REGISTERS
USING
EMMS
CLEARS
ALL
VALID
BITS
EFFECTIVELY
EMPTYING
THE
FLOATING
POINT
STACK
AND
MAKING
IT
READY
FOR
NEW
FLOATING
POINT
OPERATIONS
THE
EMMS
INSTRUCTION
ENSURES
A
CLEAN
TRANSITION
BETWEEN
USING
OPERATIONS
ON
THE
MMX
REGISTERS
AND
USING
OPERA
TIONS
ON
THE
FLOATING
POINT
STACK
ON
THE
PENTIUM
PROCESSOR
THERE
IS
A
FINITE
OVERHEAD
FOR
USING
THE
EMMS
INSTRUCTION
FAILURE
TO
USE
THE
EMMS
INSTRUCTION
OR
THE
INTRINSIC
BETWEEN
OPERA
TIONS
ON
THE
MMX
REGISTERS
AND
FLOATING
POINT
REGISTERS
MAY
LEAD
TO
UNEXPECTED
RESULTS
NOTE
FAILURE
TO
RESET
THE
TAG
WORD
FOR
FP
INSTRUCTIONS
AFTER
USING
AN
MMX
INSTRUCTION
CAN
RESULT
IN
FAULTY
EXECUTION
OR
POOR
PERFORMANCE
GUIDELINES
FOR
USING
EMMS
INSTRUCTION
WHEN
DEVELOPING
CODE
WITH
BOTH
FLOATING
POINT
AND
BIT
SIMD
INTEGER
INSTRUC
TIONS
FOLLOW
THESE
STEPS
ALWAYS
CALL
THE
EMMS
INSTRUCTION
AT
THE
END
OF
BIT
SIMD
INTEGER
CODE
WHEN
THE
CODE
TRANSITIONS
TO
FLOATING
POINT
CODE
INSERT
THE
EMMS
INSTRUCTION
AT
THE
END
OF
ALL
BIT
SIMD
INTEGER
CODE
SEGMENTS
TO
AVOID
AN
FLOATING
POINT
STACK
OVERFLOW
EXCEPTION
WHEN
AN
FLOATING
POINT
INSTRUCTION
IS
EXECUTED
WHEN
WRITING
AN
APPLICATION
THAT
USES
BOTH
FLOATING
POINT
AND
BIT
SIMD
INTEGER
INSTRUCTIONS
USE
THE
FOLLOWING
GUIDELINES
TO
HELP
YOU
DETERMINE
WHEN
TO
USE
EMMS
IF
NEXT
INSTRUCTION
IS
FP
USE
AFTER
A
BIT
SIMD
INTEGER
INSTRUCTION
IF
THE
NEXT
INSTRUCTION
IS
AN
FP
INSTRUCTION
FOR
EXAMPLE
BEFORE
DOING
CALCULATIONS
ON
FLOATS
DOUBLES
OR
LONG
DOUBLES
DON
T
EMPTY
WHEN
ALREADY
EMPTY
IF
THE
NEXT
INSTRUCTION
USES
AN
MMX
REGISTER
INCURS
A
COST
WITH
NO
BENEFIT
GROUP
INSTRUCTIONS
TRY
TO
PARTITION
REGIONS
THAT
USE
FP
INSTRUCTIONS
FROM
THOSE
THAT
USE
BIT
SIMD
INTEGER
INSTRUCTIONS
THIS
ELIMINATES
THE
NEED
FOR
AN
EMMS
INSTRUCTION
WITHIN
THE
BODY
OF
A
CRITICAL
LOOP
RUNTIME
INITIALIZATION
USE
DURING
RUNTIME
INITIALIZATION
OF
AND
FP
DATA
TYPES
THIS
ENSURES
RESETTING
THE
REGISTER
BETWEEN
DATA
TYPE
TRANSITIONS
SEE
EXAMPLE
FOR
CODING
USAGE
EXAMPLE
RESETTING
REGISTER
BETWEEN
AND
FP
DATA
TYPES
CODE
INCORRECT
USAGE
CORRECT
USAGE
X
Y
Z
X
Y
Z
FLOAT
F
INIT
FLOAT
F
INIT
YOU
MUST
BE
AWARE
THAT
YOUR
CODE
GENERATES
AN
MMX
INSTRUCTION
WHICH
USES
MMX
REGISTERS
WITH
THE
INTEL
C
COMPILER
IN
THE
FOLLOWING
SITUATIONS
WHEN
USING
A
BIT
SIMD
INTEGER
INTRINSIC
FROM
MMX
TECHNOLOGY
SSE
WHEN
USING
A
BIT
SIMD
INTEGER
INSTRUCTION
FROM
MMX
TECHNOLOGY
SSE
THROUGH
INLINE
ASSEMBLY
WHEN
REFERENCING
THE
DATA
TYPE
VARIABLE
ADDITIONAL
INFORMATION
ON
THE
FLOATING
POINT
PROGRAMMING
MODEL
CAN
BE
FOUND
IN
THE
INTEL
AND
IA
ARCHITECTURES
SOFTWARE
DEVELOPER
MANUAL
VOLUME
FOR
MORE
ON
EMMS
VISIT
HTTP
DEVELOPER
INTEL
COM
DATA
ALIGNMENT
MAKE
SURE
THAT
BIT
SIMD
INTEGER
DATA
IS
BYTE
ALIGNED
AND
THAT
BIT
SIMD
INTEGER
DATA
IS
BYTE
ALIGNED
REFERENCING
UNALIGNED
BIT
SIMD
INTEGER
DATA
CAN
INCUR
A
PERFORMANCE
PENALTY
DUE
TO
ACCESSES
THAT
SPAN
CACHE
LINES
REFERENCING
UNALIGNED
BIT
SIMD
INTEGER
DATA
RESULTS
IN
AN
EXCEPTION
UNLESS
THE
MOVDQU
MOVE
DOUBLE
QUADWORD
UNALIGNED
INSTRUCTION
IS
USED
USING
THE
MOVDQU
INSTRUC
TION
ON
UNALIGNED
DATA
CAN
RESULT
IN
LOWER
PERFORMANCE
THAN
USING
BYTE
ALIGNED
REFERENCES
REFER
TO
SECTION
STACK
AND
DATA
ALIGNMENT
FOR
MORE
INFORMATION
LOADING
BYTES
OF
SIMD
DATA
EFFICIENTLY
REQUIRES
DATA
ALIGNMENT
ON
BYTE
BOUND
ARIES
PROVIDES
THE
PALIGNR
INSTRUCTION
IT
REDUCES
OVERHEAD
IN
SITUATIONS
THAT
REQUIRES
SOFTWARE
TO
PROCESSING
DATA
ELEMENTS
FROM
NON
ALIGNED
ADDRESS
THE
PALIGNR
INSTRUCTION
IS
MOST
VALUABLE
WHEN
LOADING
OR
STORING
UNALIGNED
DATA
WITH
THE
ADDRESS
SHIFTS
BY
A
FEW
BYTES
YOU
CAN
REPLACE
A
SET
OF
UNALIGNED
LOADS
WITH
ALIGNED
LOADS
FOLLOWED
BY
USING
PALIGNR
INSTRUCTIONS
AND
SIMPLE
REGISTER
TO
REGISTER
COPIES
USING
PALIGNRS
TO
REPLACE
UNALIGNED
LOADS
IMPROVES
PERFORMANCE
BY
ELIMINATING
CACHE
LINE
SPLITS
AND
OTHER
PENALTIES
IN
ROUTINES
LIKE
MEMCPY
PALIGNR
CAN
BOOST
THE
PERFORMANCE
OF
MISALIGNED
CASES
EXAMPLE
SHOWS
A
SITUATION
THAT
BENEFITS
BY
USING
PALIGNR
EXAMPLE
FIR
PROCESSING
EXAMPLE
IN
C
LANGUAGE
CODE
VOID
FIR
FLOAT
IN
FLOAT
OUT
FLOAT
COEFF
INT
COUNT
INT
I
J
FOR
I
I
COUNT
TAP
I
FLOAT
SUM
FOR
J
J
TAP
J
SUM
IN
J
COEFF
J
OUT
SUM
IN
EXAMPLE
COMPARES
AN
OPTIMAL
SEQUENCE
OF
THE
FIR
LOOP
AND
AN
EQUIVALENT
IMPLEMENTATION
BOTH
IMPLEMENTATIONS
UNROLL
ITERATION
OF
THE
FIR
INNER
LOOP
TO
ENABLE
SIMD
CODING
TECHNIQUES
THE
CODE
CAN
NOT
AVOID
EXPERIENCING
CACHE
LINE
SPLIT
ONCE
EVERY
FOUR
ITERATIONS
PALGNR
ALLOWS
THE
CODE
TO
AVOID
THE
DELAYS
ASSOCIATED
WITH
CACHE
LINE
SPLITS
EXAMPLE
AND
IMPLEMENTATION
OF
FIR
PROCESSING
CODE
OPTIMIZED
FOR
OPTIMIZED
FOR
PXOR
XOR
ECX
ECX
MOV
EAX
DWORD
PTR
INPUT
MOV
EBX
DWORD
PTR
PXOR
XOR
ECX
ECX
MOV
EAX
DWORD
PTR
INPUT
MOV
EBX
DWORD
PTR
MOVAPS
XMMWORD
PTR
EAX
ECX
MULPS
XMMWORD
PTR
EBX
ECX
ADDPS
MOVAPS
XMMWORD
PTR
EAX
ECX
MOVAPS
MULPS
XMMWORD
PTR
EBX
ECX
ADDPS
MOVUPS
XMMWORD
PTR
EAX
ECX
MULPS
XMMWORD
PTR
EBX
ECX
ADDPS
MOVAPS
XMMWORD
PTR
EAX
ECX
MOVAPS
PALIGNR
MULPS
XMMWORD
PTR
EBX
ECX
ADDPS
EXAMPLE
AND
IMPLEMENTATION
OF
FIR
PROCESSING
CODE
CONTD
OPTIMIZED
FOR
OPTIMIZED
FOR
MOVUPS
XMMWORD
PTR
EAX
ECX
MOVAPS
MULPS
XMMWORD
PALIGNR
PTR
EBX
ECX
ADDPS
MULPS
XMMWORD
PTR
EBX
ECX
ADDPS
MOVUPS
XMMWORD
PTR
EAX
ECX
MULPS
XMMWORD
PTR
EBX
ECX
ADDPS
MOVAPS
PALIGNR
MULPS
XMMWORD
PTR
EBX
ECX
ADDPS
ADD
ECX
CMP
ECX
TAP
JL
ADD
ECX
CMP
ECX
TAP
JL
MOV
EAX
DWORD
PTR
OUTPUT
MOVAPS
XMMWORD
PTR
EAX
MOV
EAX
DWORD
PTR
OUTPUT
MOVAPS
XMMWORD
PTR
EAX
DATA
MOVEMENT
CODING
TECHNIQUES
IN
GENERAL
BETTER
PERFORMANCE
CAN
BE
ACHIEVED
IF
DATA
IS
PRE
ARRANGED
FOR
SIMD
COMPUTATION
SEE
SECTION
IMPROVING
MEMORY
UTILIZATION
THIS
MAY
NOT
ALWAYS
BE
POSSIBLE
THIS
SECTION
COVERS
TECHNIQUES
FOR
GATHERING
AND
ARRANGING
DATA
FOR
MORE
EFFICIENT
SIMD
COMPUTATION
UNSIGNED
UNPACK
MMX
TECHNOLOGY
PROVIDES
SEVERAL
INSTRUCTIONS
THAT
ARE
USED
TO
PACK
AND
UNPACK
DATA
IN
THE
MMX
REGISTERS
EXTENDS
THESE
INSTRUCTIONS
SO
THAT
THEY
OPERATE
ON
BIT
SOURCE
AND
DESTINATIONS
THE
UNPACK
INSTRUCTIONS
CAN
BE
USED
TO
ZERO
EXTEND
AN
UNSIGNED
NUMBER
EXAMPLE
ASSUMES
THE
SOURCE
IS
A
PACKED
WORD
BIT
DATA
TYPE
EXAMPLE
ZERO
EXTEND
BIT
VALUES
INTO
BITS
USING
UNSIGNED
UNPACK
INSTRUC
TIONS
CODE
INPUT
BIT
VALUES
IN
SOURCE
A
LOCAL
VARIABLE
CAN
BE
USED
INSTEAD
OF
THE
REGISTER
IF
DESIRED
OUTPUT
FOUR
ZERO
EXTENDED
BIT
DOUBLEWORDS
FROM
FOUR
LOW
END
WORDS
FOUR
ZERO
EXTENDED
BIT
DOUBLEWORDS
FROM
FOUR
HIGH
END
WORDS
MOVDQA
COPY
SOURCE
PUNPCKLWD
UNPACK
THE
LOW
END
WORDS
INTO
BIT
DOUBLEWORD
PUNPCKHWD
UNPACK
THE
HIGH
END
WORDS
INTO
BIT
DOUBLEWORDS
SIGNED
UNPACK
SIGNED
NUMBERS
SHOULD
BE
SIGN
EXTENDED
WHEN
UNPACKING
VALUES
THIS
IS
SIMILAR
TO
THE
ZERO
EXTEND
SHOWN
ABOVE
EXCEPT
THAT
THE
PSRAD
INSTRUCTION
PACKED
SHIFT
RIGHT
ARITHMETIC
IS
USED
TO
SIGN
EXTEND
THE
VALUES
EXAMPLE
ASSUMES
THE
SOURCE
IS
A
PACKED
WORD
BIT
DATA
TYPE
EXAMPLE
SIGNED
UNPACK
CODE
INPUT
SOURCE
VALUE
OUTPUT
FOUR
SIGN
EXTENDED
BIT
DOUBLEWORDS
FROM
FOUR
LOW
END
WORDS
FOUR
SIGN
EXTENDED
BIT
DOUBLEWORDS
FROM
FOUR
HIGH
END
WORDS
EXAMPLE
SIGNED
UNPACK
CODE
CONTD
MOVDQA
COPY
SOURCE
PUNPCKLWD
UNPACK
FOUR
LOW
END
WORDS
OF
THE
SOURCE
INTO
THE
UPPER
BITS
OF
EACH
DOUBLEWORD
IN
THE
DESTINATION
PUNPCKHWD
UNPACK
HIGH
END
WORDS
OF
THE
SOURCE
INTO
THE
UPPER
BITS
OF
EACH
DOUBLEWORD
IN
THE
DESTINATION
PSRAD
SIGN
EXTEND
THE
LOW
END
WORDS
OF
THE
SOURCE
INTO
FOUR
BIT
SIGNED
DOUBLEWORDS
PSRAD
SIGN
EXTEND
THE
HIGH
END
WORDS
OF
THE
SOURCE
INTO
FOUR
BIT
SIGNED
DOUBLEWORDS
INTERLEAVED
PACK
WITH
SATURATION
PACK
INSTRUCTIONS
PACK
TWO
VALUES
INTO
A
DESTINATION
REGISTER
IN
A
PREDETERMINED
ORDER
PACKSSDW
SATURATES
TWO
SIGNED
DOUBLEWORDS
FROM
A
SOURCE
OPERAND
AND
TWO
SIGNED
DOUBLEWORDS
FROM
A
DESTINATION
OPERAND
INTO
FOUR
SIGNED
WORDS
AND
IT
PACKS
THE
FOUR
SIGNED
WORDS
INTO
A
DESTINATION
REGISTER
SEE
FIGURE
EXTENDS
PACKSSDW
SO
THAT
IT
SATURATES
FOUR
SIGNED
DOUBLEWORDS
FROM
A
SOURCE
OPERAND
AND
FOUR
SIGNED
DOUBLEWORDS
FROM
A
DESTINATION
OPERAND
INTO
EIGHT
SIGNED
WORDS
THE
EIGHT
SIGNED
WORDS
ARE
PACKED
INTO
THE
DESTINATION
FIGURE
PACKSSDW
MM
MM
INSTRUCTION
FIGURE
ILLUSTRATES
WHERE
TWO
PAIRS
OF
VALUES
ARE
INTERLEAVED
IN
A
DESTINATION
REGISTER
EXAMPLE
SHOWS
MMX
CODE
THAT
ACCOMPLISHES
THE
OPERATION
TWO
SIGNED
DOUBLEWORDS
ARE
USED
AS
SOURCE
OPERANDS
AND
THE
RESULT
IS
INTERLEAVED
SIGNED
WORDS
THE
SEQUENCE
IN
EXAMPLE
CAN
BE
EXTENDED
IN
TO
INTERLEAVE
EIGHT
SIGNED
WORDS
USING
XMM
REGISTERS
D
FIGURE
INTERLEAVED
PACK
WITH
SATURATION
EXAMPLE
INTERLEAVED
PACK
WITH
SATURATION
CODE
INPUT
SIGNED
VALUE
SIGNED
VALUE
OUTPUT
THE
FIRST
AND
THIRD
WORDS
CONTAIN
THE
SIGNED
SATURATED
DOUBLEWORDS
FROM
THE
SECOND
AND
FOURTH
WORDS
CONTAIN
SIGNED
SATURATED
DOUBLEWORDS
FROM
PACKSSDW
PACK
AND
SIGN
SATURATE
PACKSSDW
PACK
AND
SIGN
SATURATE
PUNPCKLWD
INTERLEAVE
THE
LOW
END
BIT
VALUES
OF
THE
OPERANDS
PACK
INSTRUCTIONS
ALWAYS
ASSUME
THAT
SOURCE
OPERANDS
ARE
SIGNED
NUMBERS
THE
RESULT
IN
THE
DESTINATION
REGISTER
IS
ALWAYS
DEFINED
BY
THE
PACK
INSTRUCTION
THAT
PERFORMS
THE
OPERATION
FOR
EXAMPLE
PACKSSDW
PACKS
EACH
OF
TWO
SIGNED
BIT
VALUES
OF
TWO
SOURCES
INTO
FOUR
SATURATED
BIT
SIGNED
VALUES
IN
A
DESTINATION
REGISTER
PACKUSWB
ON
THE
OTHER
HAND
PACKS
THE
FOUR
SIGNED
BIT
VALUES
OF
TWO
SOURCES
INTO
EIGHT
SATURATED
EIGHT
BIT
UNSIGNED
VALUES
IN
THE
DESTINATION
INTERLEAVED
PACK
WITHOUT
SATURATION
EXAMPLE
IS
SIMILAR
TO
EXAMPLE
EXCEPT
THAT
THE
RESULTING
WORDS
ARE
NOT
SATU
RATED
IN
ADDITION
IN
ORDER
TO
PROTECT
AGAINST
OVERFLOW
ONLY
THE
LOW
ORDER
BITS
OF
EACH
DOUBLEWORD
ARE
USED
AGAIN
EXAMPLE
CAN
BE
EXTENDED
IN
TO
ACCOM
PLISH
INTERLEAVING
EIGHT
WORDS
WITHOUT
SATURATION
EXAMPLE
INTERLEAVED
PACK
WITHOUT
SATURATION
CODE
INPUT
SIGNED
SOURCE
VALUE
SIGNED
SOURCE
VALUE
OUTPUT
THE
FIRST
AND
THIRD
WORDS
CONTAIN
THE
LOW
BITS
OF
THE
DOUBLEWORDS
IN
THE
SECOND
AND
FOURTH
WORDS
CONTAIN
THE
LOW
BITS
OF
THE
DOUBLEWORDS
IN
PSLLD
SHIFT
THE
LSB
FROM
EACH
OF
THE
DOUBLEWORD
VALUES
TO
THE
MSB
POSITION
PAND
FFFF
FFFF
MASK
TO
ZERO
THE
MSB
OF
EACH
DOUBLEWORD
VALUE
POR
MERGE
THE
TWO
OPERANDS
NON
INTERLEAVED
UNPACK
UNPACK
INSTRUCTIONS
PERFORM
AN
INTERLEAVE
MERGE
OF
THE
DATA
ELEMENTS
OF
THE
DESTI
NATION
AND
SOURCE
OPERANDS
INTO
THE
DESTINATION
REGISTER
THE
FOLLOWING
EXAMPLE
MERGES
THE
TWO
OPERANDS
INTO
DESTINATION
REGISTERS
WITHOUT
INTERLEAVING
FOR
EXAMPLE
TAKE
TWO
ADJACENT
ELEMENTS
OF
A
PACKED
WORD
DATA
TYPE
IN
AND
PLACE
THIS
VALUE
IN
THE
LOW
BITS
OF
THE
RESULTS
THEN
TAKE
TWO
ADJA
CENT
ELEMENTS
OF
A
PACKED
WORD
DATA
TYPE
IN
AND
PLACE
THIS
VALUE
IN
THE
HIGH
BITS
OF
THE
RESULTS
ONE
OF
THE
DESTINATION
REGISTERS
WILL
HAVE
THE
COMBINATION
ILLUSTRATED
IN
FIGURE
FIGURE
RESULT
OF
NON
INTERLEAVED
UNPACK
LOW
IN
THE
OTHER
DESTINATION
REGISTER
WILL
CONTAIN
THE
OPPOSITE
COMBINATION
ILLUSTRATED
IN
FIGURE
FIGURE
RESULT
OF
NON
INTERLEAVED
UNPACK
HIGH
IN
CODE
IN
THE
EXAMPLE
UNPACKS
TWO
PACKED
WORD
SOURCES
IN
A
NON
INTERLEAVED
WAY
THE
GOAL
IS
TO
USE
THE
INSTRUCTION
WHICH
UNPACKS
DOUBLEWORDS
TO
A
QUADWORD
INSTEAD
OF
USING
THE
INSTRUCTION
WHICH
UNPACKS
WORDS
TO
DOUBLEWORDS
EXAMPLE
UNPACKING
TWO
PACKED
WORD
SOURCES
IN
NON
INTERLEAVED
WAY
CODE
INPUT
PACKED
WORD
SOURCE
VALUE
PACKED
WORD
SOURCE
VALUE
OUTPUT
CONTAINS
THE
TWO
LOW
END
WORDS
OF
THE
ORIGINAL
SOURCES
NON
INTERLEAVED
CONTAINS
THE
TWO
HIGH
END
WORDS
OF
THE
ORIGINAL
SOURCES
NON
INTERLEAVED
MOVQ
COPY
PUNPCKLDQ
REPLACE
THE
TWO
HIGH
END
WORDS
OF
MMO
WITH
TWO
LOW
END
WORDS
OF
LEAVE
THE
TWO
LOW
END
WORDS
OF
IN
PLACE
PUNPCKHDQ
MOVE
TWO
HIGH
END
WORDS
OF
TO
THE
TWO
LOW
END
WORDS
OF
PLACE
THE
TWO
HIGH
END
WORDS
OF
IN
TWO
HIGH
END
WORDS
OF
EXTRACT
DATA
ELEMENT
THE
PEXTRW
INSTRUCTION
IN
SSE
TAKES
THE
WORD
IN
THE
DESIGNATED
MMX
REGISTER
SELECTED
BY
THE
TWO
LEAST
SIGNIFICANT
BITS
OF
THE
IMMEDIATE
VALUE
AND
MOVES
IT
TO
THE
LOWER
HALF
OF
A
BIT
INTEGER
REGISTER
SEE
FIGURE
AND
EXAMPLE
WITH
PEXTRW
CAN
EXTRACT
A
WORD
FROM
AN
XMM
REGISTER
TO
THE
LOWER
BITS
OF
AN
INTEGER
REGISTER
PROVIDES
EXTRACTION
OF
A
BYTE
WORD
DWORD
AND
QWORD
FROM
AN
XMM
REGISTER
INTO
EITHER
A
MEMORY
LOCATION
OR
INTEGER
REGISTER
FIGURE
PEXTRW
INSTRUCTION
EXAMPLE
PEXTRW
INSTRUCTION
CODE
INPUT
EAX
SOURCE
VALUE
IMMEDIATE
VALUE
OUTPUT
EDX
BIT
INTEGER
REGISTER
CONTAINING
THE
EXTRACTED
WORD
IN
THE
LOW
ORDER
BITS
THE
HIGH
ORDER
BITS
ZERO
EXTENDED
MOVQ
EAX
PEXTRW
EDX
INSERT
DATA
ELEMENT
THE
PINSRW
INSTRUCTION
IN
SSE
LOADS
A
WORD
FROM
THE
LOWER
HALF
OF
A
BIT
INTEGER
REGISTER
OR
FROM
MEMORY
AND
INSERTS
IT
IN
AN
MMX
TECHNOLOGY
DESTINATION
REGISTER
AT
A
POSITION
DEFINED
BY
THE
TWO
LEAST
SIGNIFICANT
BITS
OF
THE
IMMEDIATE
CONSTANT
INSER
TION
IS
DONE
IN
SUCH
A
WAY
THAT
THREE
OTHER
WORDS
FROM
THE
DESTINATION
REGISTER
ARE
LEFT
UNTOUCHED
SEE
FIGURE
AND
EXAMPLE
WITH
PINSRW
CAN
INSERT
A
WORD
FROM
THE
LOWER
BITS
OF
AN
INTEGER
REGISTER
OR
MEMORY
INTO
AN
XMM
REGISTER
PROVIDES
INSERTION
OF
A
BYTE
DWORD
AND
QWORD
FROM
EITHER
A
MEMORY
LOCATION
OR
INTEGER
REGISTER
INTO
AN
XMM
REGISTER
FIGURE
PINSRW
INSTRUCTION
EXAMPLE
PINSRW
INSTRUCTION
CODE
INPUT
EDX
POINTER
TO
SOURCE
VALUE
OUTPUT
REGISTER
WITH
NEW
BIT
VALUE
INSERTED
MOV
EAX
EDX
PINSRW
EAX
IF
ALL
OF
THE
OPERANDS
IN
A
REGISTER
ARE
BEING
REPLACED
BY
A
SERIES
OF
PINSRW
INSTRUC
TIONS
IT
CAN
BE
USEFUL
TO
CLEAR
THE
CONTENT
AND
BREAK
THE
DEPENDENCE
CHAIN
BY
EITHER
USING
THE
PXOR
INSTRUCTION
OR
LOADING
THE
REGISTER
SEE
EXAMPLE
AND
SECTION
CLEARING
REGISTERS
AND
DEPENDENCY
BREAKING
IDIOMS
EXAMPLE
REPEATED
PINSRW
INSTRUCTION
CODE
INPUT
EDX
POINTER
TO
STRUCTURE
CONTAINING
SOURCE
VALUES
AT
OFFSETS
OF
AND
IMMEDIATE
VALUE
OUTPUT
MMX
REGISTER
WITH
NEW
BIT
VALUE
INSERTED
PXOR
BREAKS
DEPENDENCY
ON
PREVIOUS
VALUE
OF
MOV
EAX
EDX
PINSRW
EAX
MOV
EAX
EDX
PINSRW
EAX
MOV
EAX
EDX
PINSRW
EAX
MOV
EAX
EDX
PINSRW
EAX
NON
UNIT
STRIDE
DATA
MOVEMENT
PROVIDES
INSTRUCTIONS
TO
INSERT
A
DATA
ELEMENT
FROM
MEMORY
INTO
AN
XMM
REGISTER
AND
TO
EXTRACT
A
DATA
ELEMENT
FROM
AN
XMM
REGISTER
INTO
MEMORY
DIRECTLY
SEPARATE
INSTRUCTIONS
ARE
PROVIDED
TO
HANDLE
FLOATING
POINT
DATA
AND
INTEGER
BYTE
WORD
OR
DWORD
THESE
INSTRUCTIONS
ARE
SUITED
FOR
VECTORIZING
CODE
THAT
LOADS
STORES
NON
UNIT
STRIDE
DATA
FROM
MEMORY
SEE
EXAMPLE
EXAMPLE
NON
UNIT
STRIDE
LOAD
STORE
USING
INSTRUCTIONS
EXAMPLE
PROVIDES
TWO
EXAMPLES
USING
INSERTPS
AND
PEXTRD
TO
PERFORM
GATHER
OPERATIONS
ON
FLOATING
POINT
DATA
USING
EXTRACTPS
AND
PEXTRD
TO
PERFORM
SCATTER
OPERATIONS
ON
FLOATING
POINT
DATA
EXAMPLE
SCATTER
AND
GATHER
OPERATIONS
USING
INSTRUCTIONS
MOVE
BYTE
MASK
TO
INTEGER
THE
PMOVMSKB
INSTRUCTION
RETURNS
A
BIT
MASK
FORMED
FROM
THE
MOST
SIGNIFICANT
BITS
OF
EACH
BYTE
OF
ITS
SOURCE
OPERAND
WHEN
USED
WITH
BIT
MMX
REGISTERS
THIS
PRODUCES
AN
BIT
MASK
ZEROING
OUT
THE
UPPER
BITS
IN
THE
DESTINATION
REGISTER
WHEN
USED
WITH
BIT
XMM
REGISTERS
IT
PRODUCES
A
BIT
MASK
ZEROING
OUT
THE
UPPER
BITS
IN
THE
DESTINATION
REGISTER
THE
BIT
VERSION
OF
THIS
INSTRUCTION
IS
SHOWN
IN
FIGURE
AND
EXAMPLE
FIGURE
PMOVSMKB
INSTRUCTION
EXAMPLE
PMOVMSKB
INSTRUCTION
CODE
INPUT
SOURCE
VALUE
OUTPUT
BIT
REGISTER
CONTAINING
THE
BYTE
MASK
IN
THE
LOWER
EIGHT
BITS
MOVQ
EDI
PMOVMSKB
EAX
PACKED
SHUFFLE
WORD
FOR
BIT
REGISTERS
THE
PSHUFW
INSTRUCTION
USES
THE
IMMEDIATE
OPERAND
TO
SELECT
BETWEEN
THE
FOUR
WORDS
IN
EITHER
TWO
MMX
REGISTERS
OR
ONE
MMX
REGISTER
AND
A
BIT
MEMORY
LOCATION
PROVIDES
PSHUFLW
TO
SHUFFLE
THE
LOWER
FOUR
WORDS
INTO
AN
XMM
REGISTER
IN
ADDITION
TO
THE
EQUIVALENT
TO
THE
PSHUFW
ALSO
PROVIDES
PSHUFHW
TO
SHUFFLE
THE
HIGHER
FOUR
WORDS
FURTHERMORE
OFFERS
PSHUFD
TO
SHUFFLE
FOUR
DWORDS
INTO
AN
XMM
REGISTER
ALL
OF
THESE
FOUR
PSHUF
INSTRUCTIONS
USE
AN
IMMEDIATE
BYTE
TO
ENCODE
THE
DATA
PATH
OF
INDIVIDUAL
WORDS
WITHIN
THE
CORRESPONDING
BYTES
FROM
SOURCE
TO
DESTINATION
SHOWN
IN
TABLE
TABLE
PSHUF
ENCODING
BITS
WORDS
PACKED
SHUFFLE
WORD
FOR
BIT
REGISTERS
THE
PSHUFLW
PSHUFHW
INSTRUCTION
PERFORMS
A
FULL
SHUFFLE
OF
ANY
SOURCE
WORD
FIELD
WITHIN
THE
LOW
HIGH
BITS
TO
ANY
RESULT
WORD
FIELD
IN
THE
LOW
HIGH
BITS
USING
AN
BIT
IMMEDIATE
OPERAND
OTHER
HIGH
LOW
BITS
ARE
PASSED
THROUGH
FROM
THE
SOURCE
OPERAND
PSHUFD
PERFORMS
A
FULL
SHUFFLE
OF
ANY
DOUBLE
WORD
FIELD
WITHIN
THE
BIT
SOURCE
TO
ANY
DOUBLE
WORD
FIELD
IN
THE
BIT
RESULT
USING
AN
BIT
IMMEDIATE
OPERAND
NO
MORE
THAN
INSTRUCTIONS
USING
PSHUFLW
PSHUFHW
PSHUFD
ARE
REQUIRED
TO
IMPLEMENT
MANY
COMMON
DATA
SHUFFLING
OPERATIONS
BROADCAST
SWAP
AND
REVERSE
ARE
ILLUSTRATED
IN
EXAMPLE
AND
EXAMPLE
EXAMPLE
BROADCAST
A
WORD
ACROSS
XMM
USING
INSTRUCTIONS
EXAMPLE
SWAP
REVERSE
WORDS
IN
AN
XMM
USING
INSTRUCTIONS
SHUFFLE
BYTES
PROVIDES
PSHUFB
THIS
INSTRUCTION
CARRIES
OUT
BYTE
MANIPULATION
WITHIN
A
BYTE
RANGE
PSHUFB
CAN
REPLACE
UP
TO
OTHER
INSTRUCTIONS
INCLUDING
SHIFT
OR
AND
AND
MOV
USE
PSHUFB
IF
THE
ALTERNATIVE
USES
OR
MORE
INSTRUCTIONS
CONDITIONAL
DATA
MOVEMENT
PROVIDES
TWO
PACKED
BLEND
INSTRUCTIONS
ON
BYTE
AND
WORD
DATA
ELEMENTS
IN
BIT
OPERANDS
PACKED
BLEND
INSTRUCTIONS
CONDITIONALLY
COPIES
DATA
ELEMENTS
FROM
SELECTED
POSITIONS
IN
THE
SOURCE
TO
THE
CORRESPONDING
DATA
ELEMENT
USING
A
MASK
SPECIFIED
BY
AN
IMMEDIATE
CONTROL
BYTE
OR
AN
IMPLIED
XMM
REGISTER
THE
MASK
CAN
BE
GENERATED
BY
A
PACKED
COMPARE
INSTRUCTION
FOR
EXAMPLE
THUS
PACKED
BLEND
INSTRUCTIONS
ARE
MOST
USEFUL
FOR
VECTORIZING
CONDITIONAL
FLOWS
WITHIN
A
LOOP
AND
CAN
BE
MORE
EFFICIENT
THAN
INSERTING
SINGLE
ELEMENT
ONE
AT
A
TIME
FOR
SOME
SITUATIONS
UNPACKING
INTERLEAVING
BIT
DATA
IN
BIT
REGISTERS
THE
PUNPCKLQDQ
PUNPCHQDQ
INSTRUCTIONS
INTERLEAVE
THE
LOW
HIGH
ORDER
BITS
OF
THE
SOURCE
OPERAND
AND
THE
LOW
HIGH
ORDER
BITS
OF
THE
DESTINATION
OPERAND
IT
THEN
WRITES
THE
RESULTS
TO
THE
DESTINATION
REGISTER
THE
HIGH
LOW
ORDER
BITS
OF
THE
SOURCE
OPERANDS
ARE
IGNORED
DATA
MOVEMENT
THERE
ARE
TWO
ADDITIONAL
INSTRUCTIONS
TO
ENABLE
DATA
MOVEMENT
FROM
BIT
SIMD
INTEGER
REGISTERS
TO
BIT
SIMD
REGISTERS
THE
INSTRUCTION
MOVES
THE
BIT
INTEGER
DATA
FROM
AN
MMX
REGISTER
SOURCE
TO
A
BIT
DESTINATION
REGISTER
THE
HIGH
ORDER
BITS
OF
THE
DESTINATION
REGISTER
ARE
ZEROED
OUT
THE
INSTRUCTION
MOVES
THE
LOW
ORDER
BITS
OF
INTEGER
DATA
FROM
A
BIT
SOURCE
REGISTER
TO
AN
MMX
REGISTER
DESTINATION
CONVERSION
INSTRUCTIONS
SSE
PROVIDES
INSTRUCTIONS
TO
SUPPORT
WIDE
CONVERSION
OF
SINGLE
PRECISION
DATA
TO
FROM
DOUBLE
WORD
INTEGER
DATA
CONVERSIONS
BETWEEN
DOUBLE
PRECISION
DATA
TO
DOUBLE
WORD
INTEGER
DATA
HAVE
BEEN
ADDED
IN
PROVIDES
ROUNDING
INSTRUCTIONS
TO
CONVERT
FLOATING
POINT
VALUES
TO
INTEGER
VALUES
WITH
ROUNDING
CONTROL
SPECIFIED
IN
A
MORE
FLEXIBLE
MANNER
AND
INDEPENDENT
OF
THE
ROUNDING
CONTROL
IN
MXCSR
THE
INTEGER
VALUES
PRODUCED
BY
ROUNDXX
INSTRUC
TIONS
ARE
MAINTAINED
AS
FLOATING
POINT
DATA
ALSO
PROVIDES
INSTRUCTIONS
TO
CONVERT
INTEGER
DATA
FROM
PACKED
BYTES
TO
PACKED
WORD
DWORD
QWORD
FORMAT
USING
EITHER
SIGN
EXTENSION
OR
ZERO
EXTENSION
PACKED
WORDS
TO
PACKED
DWORD
QWORD
FORMAT
USING
EITHER
SIGN
EXTENSION
OR
ZERO
EXTENSION
PACKED
DWORD
TO
PACKED
QWORD
FORMAT
USING
EITHER
SIGN
EXTENSION
OR
ZERO
EXTENSION
GENERATING
CONSTANTS
SIMD
INTEGER
INSTRUCTION
SETS
DO
NOT
HAVE
INSTRUCTIONS
THAT
WILL
LOAD
IMMEDIATE
CONSTANTS
TO
THE
SIMD
REGISTERS
THE
FOLLOWING
CODE
SEGMENTS
GENERATE
FREQUENTLY
USED
CONSTANTS
IN
THE
SIMD
REGISTER
THESE
EXAMPLES
CAN
ALSO
BE
EXTENDED
IN
BY
SUBSTITUTING
MMX
WITH
XMM
REGISTERS
SEE
EXAMPLE
EXAMPLE
GENERATING
CONSTANTS
EXAMPLE
GENERATING
CONSTANTS
CONTD
NOTE
BECAUSE
SIMD
INTEGER
INSTRUCTION
SETS
DO
NOT
SUPPORT
SHIFT
INSTRUC
TIONS
FOR
BYTES
AND
ARE
RELEVANT
ONLY
FOR
PACKED
WORDS
AND
PACKED
DOUBLEWORDS
BUILDING
BLOCKS
THIS
SECTION
DESCRIBES
INSTRUCTIONS
AND
ALGORITHMS
WHICH
IMPLEMENT
COMMON
CODE
BUILDING
BLOCKS
ABSOLUTE
DIFFERENCE
OF
UNSIGNED
NUMBERS
EXAMPLE
COMPUTES
THE
ABSOLUTE
DIFFERENCE
OF
TWO
UNSIGNED
NUMBERS
IT
ASSUMES
AN
UNSIGNED
PACKED
BYTE
DATA
TYPE
HERE
WE
MAKE
USE
OF
THE
SUBTRACT
INSTRUCTION
WITH
UNSIGNED
SATURATION
THIS
INSTRUC
TION
RECEIVES
UNSIGNED
OPERANDS
AND
SUBTRACTS
THEM
WITH
UNSIGNED
SATURATION
THIS
SUPPORT
EXISTS
ONLY
FOR
PACKED
BYTES
AND
PACKED
WORDS
NOT
FOR
PACKED
DOUBLE
WORDS
EXAMPLE
ABSOLUTE
DIFFERENCE
OF
TWO
UNSIGNED
NUMBERS
INPUT
SOURCE
OPERAND
SOURCE
OPERAND
OUTPUT
ABSOLUTE
DIFFERENCE
OF
THE
UNSIGNED
OPERANDS
MOVQ
MAKE
A
COPY
OF
COMPUTE
DIFFERENCE
ONE
WAY
COMPUTE
DIFFERENCE
THE
OTHER
WAY
POR
OR
THEM
TOGETHER
THIS
EXAMPLE
WILL
NOT
WORK
IF
THE
OPERANDS
ARE
SIGNED
NOTE
THAT
PSADBW
MAY
ALSO
BE
USED
IN
SOME
SITUATIONS
SEE
SECTION
FOR
DETAILS
ABSOLUTE
DIFFERENCE
OF
SIGNED
NUMBERS
EXAMPLE
COMPUTES
THE
ABSOLUTE
DIFFERENCE
OF
TWO
SIGNED
NUMBERS
USING
INSTRUCTION
PABSW
THIS
SEQUENCE
IS
MORE
EFFICIENT
THAN
USING
PREVIOUS
GENERATION
OF
SIMD
INSTRUCTION
EXTENSIONS
EXAMPLE
ABSOLUTE
DIFFERENCE
OF
SIGNED
NUMBERS
INPUT
SIGNED
SOURCE
OPERAND
SIGNED
SOURCE
OPERAND
OUTPUT
DIFFERENCE
OF
THE
UNSIGNED
OPERANDS
PSUBW
SUBTRACT
WORDS
PABSW
RESULTS
IN
ABSOLUTE
VALUE
EXAMPLE
SHOW
AN
MMX
CODE
SEQUENCE
TO
COMPUTE
X
WHERE
X
IS
SIGNED
THIS
EXAMPLE
ASSUMES
SIGNED
WORDS
TO
BE
THE
OPERANDS
WITH
THIS
SEQUENCE
OF
THREE
INSTRUCTIONS
CAN
BE
REPLACED
BY
THE
PABSW
INSTRUCTION
ADDITIONALLY
PROVIDES
A
BIT
VERSION
USING
XMM
REGISTERS
AND
SUPPORTS
BYTE
WORD
AND
DOUBLEWORD
GRANULARITY
EXAMPLE
COMPUTING
ABSOLUTE
VALUE
INPUT
SIGNED
SOURCE
OPERAND
OUTPUT
ABS
MMO
PXOR
SET
TO
ALL
ZEROS
PSUBW
MAKE
EACH
WORD
CONTAIN
THE
NEGATIVE
OF
EACH
WORD
WILL
CONTAIN
ONLY
THE
POSITIVE
LARGER
VALUES
THE
ABSOLUTE
VALUE
NOTE
THE
ABSOLUTE
VALUE
OF
THE
MOST
NEGATIVE
NUMBER
THAT
IS
FOR
BIT
CANNOT
BE
REPRESENTED
USING
POSITIVE
NUMBERS
THIS
ALGORITHM
WILL
RETURN
THE
ORIGINAL
VALUE
FOR
THE
ABSOLUTE
VALUE
PIXEL
FORMAT
CONVERSION
PROVIDES
THE
PSHUFB
INSTRUCTION
TO
CARRY
OUT
BYTE
MANIPULATION
WITHIN
A
BYTE
RANGE
PSHUFB
CAN
REPLACE
A
SET
OF
UP
TO
OTHER
INSTRUCTION
INCLUDING
SHIFT
OR
AND
AND
MOV
USE
PSHUFB
IF
THE
ALTERNATIVE
CODE
USES
OR
MORE
INSTRUCTIONS
EXAMPLE
SHOWS
THE
BASIC
FORM
OF
CONVERSION
OF
COLOR
PIXEL
FORMATS
EXAMPLE
BASIC
C
IMPLEMENTATION
OF
RGBA
TO
BGRA
CONVERSION
STANDARD
C
CODE
STRUCT
RGBA
BYTE
R
G
B
A
STRUCT
BGRA
BYTE
B
G
R
A
VOID
BGRA
SOURCE
RGBA
DEST
INT
FOR
INT
I
I
I
DEST
I
R
SOURCE
I
R
DEST
I
G
SOURCE
I
G
DEST
I
B
SOURCE
I
B
DEST
I
A
SOURCE
I
A
EXAMPLE
AND
EXAMPLE
SHOW
CODE
AND
CODE
FOR
PIXEL
FORMAT
CONVERSION
IN
THE
EXAMPLE
PSHUFB
REPLACES
SIX
INSTRUCTIONS
EXAMPLE
COLOR
PIXEL
FORMAT
CONVERSION
USING
OPTIMIZED
FOR
MOV
ESI
SRC
MOV
EDI
DEST
MOV
ECX
ITERATIONS
MOVDQA
FF
FF
FF
FF
FF
FF
FF
FF
MOVDQA
FF
FF
FF
FF
FF
FF
FF
FF
MOV
EAX
REMAINDER
PIXELS
BYTE
PER
ITERATION
MOVDQA
ESI
MOVDQA
MOVDQA
ABGR
PSRLD
PSLLD
POR
GRAB
PAND
PAND
POR
ARGB
MOVDQA
EDI
REPEATS
FOR
ANOTHER
BYTES
ADD
ESI
ADD
EDI
SUB
ECX
JNZ
EXAMPLE
COLOR
PIXEL
FORMAT
CONVERSION
USING
OPTIMIZED
FOR
MOV
ESI
SRC
MOV
EDI
DEST
MOV
ECX
ITERATIONS
MOVDQA
MOV
EAX
REMAINDER
PIXELS
BYTE
PER
ITERATION
MOVDQA
ESI
MOVDQA
ESI
PSHUFB
MOVDQA
EDI
REPEATS
FOR
ANOTHER
BYTES
ADD
ESI
ADD
EDI
SUB
ECX
JNZ
ENDIAN
CONVERSION
THE
PSHUFB
INSTRUCTION
CAN
ALSO
BE
USED
TO
REVERSE
BYTE
ORDERING
WITHIN
A
DOUBLE
WORD
IT
IS
MORE
EFFICIENT
THAN
TRADITIONAL
TECHNIQUES
SUCH
AS
BSWAP
EXAMPLE
A
SHOWS
THE
TRADITIONAL
TECHNIQUE
USING
FOUR
BSWAP
INSTRUCTIONS
TO
REVERSE
THE
BYTES
WITHIN
A
DWORD
EACH
BSWAP
REQUIRES
EXECUTING
TWO
OPS
IN
ADDITION
THE
CODE
REQUIRES
LOADS
AND
STORES
FOR
PROCESSING
DWORDS
OF
DATA
EXAMPLE
B
SHOWS
AN
IMPLEMENTATION
OF
ENDIAN
CONVERSION
USING
PSHUFB
THE
REVERSING
OF
FOUR
DWORDS
REQUIRES
ONE
LOAD
ONE
STORE
AND
PSHUFB
ON
INTEL
CORE
MICROARCHITECTURE
REVERSING
DWORDS
USING
PSHUFB
CAN
BE
APPROX
IMATELY
TWICE
AS
FAST
AS
USING
BSWAP
EXAMPLE
BIG
ENDIAN
TO
LITTLE
ENDIAN
CONVERSION
A
USING
BSWAP
LEA
EAX
SRC
LEA
ECX
DST
MOV
EDX
ELCOUNT
START
MOV
EDI
EAX
MOV
ESI
EAX
BSWAP
EDI
MOV
EBX
EAX
BSWAP
ESI
MOV
EBP
EAX
MOV
ECX
EDI
MOV
ECX
ESI
BSWAP
EBX
MOV
ECX
EBX
BSWAP
EBP
MOV
ECX
EBP
ADD
EAX
ADD
ECX
SUB
EDX
JNZ
START
B
USING
PSHUFB
DECLSPEC
ALIGN
BYTE
BSWAPMASK
LEA
EAX
SRC
LEA
ECX
DST
MOV
EDX
ELCOUNT
MOVAPS
BSWAPMASK
START
MOVDQA
EAX
PSHUFB
MOVDQA
ECX
ADD
EAX
ADD
ECX
SUB
EDX
JNZ
START
CLIPPING
TO
AN
ARBITRARY
RANGE
HIGH
LOW
THIS
SECTION
EXPLAINS
HOW
TO
CLIP
A
VALUES
TO
A
RANGE
HIGH
LOW
SPECIFICALLY
IF
THE
VALUE
IS
LESS
THAN
LOW
OR
GREATER
THAN
HIGH
THEN
CLIP
TO
LOW
OR
HIGH
RESPECTIVELY
THIS
TECHNIQUE
USES
THE
PACKED
ADD
AND
PACKED
SUBTRACT
INSTRUCTIONS
WITH
SATURATION
SIGNED
OR
UNSIGNED
WHICH
MEANS
THAT
THIS
TECHNIQUE
CAN
ONLY
BE
USED
ON
PACKED
BYTE
AND
PACKED
WORD
DATA
TYPES
THE
EXAMPLES
IN
THIS
SECTION
USE
THE
CONSTANTS
AND
AND
SHOW
OPERATIONS
ON
WORD
VALUES
FOR
SIMPLICITY
WE
USE
THE
FOLLOWING
CONSTANTS
CORRESPONDING
CONSTANTS
ARE
USED
IN
CASE
THE
OPERATION
IS
DONE
ON
BYTE
VALUES
EQUALS
EQUALS
CONTAINS
THE
VALUE
LOW
IN
ALL
FOUR
WORDS
OF
THE
PACKED
WORDS
DATA
TYPE
CONTAINS
THE
VALUE
HIGH
IN
ALL
FOUR
WORDS
OF
THE
PACKED
WORDS
DATA
TYPE
ALL
VALUES
EQUAL
ADDS
THE
HIGH
VALUE
TO
ALL
DATA
ELEMENTS
WORDS
OF
ADDS
THE
LOW
VALUE
TO
ALL
DATA
ELEMENTS
WORDS
OF
HIGHLY
EFFICIENT
CLIPPING
FOR
CLIPPING
SIGNED
WORDS
TO
AN
ARBITRARY
RANGE
THE
PMAXSW
AND
PMINSW
INSTRUC
TIONS
MAY
BE
USED
FOR
CLIPPING
UNSIGNED
BYTES
TO
AN
ARBITRARY
RANGE
THE
PMAXUB
AND
PMINUB
INSTRUCTIONS
MAY
BE
USED
EXAMPLE
SHOWS
HOW
TO
CLIP
SIGNED
WORDS
TO
AN
ARBITRARY
RANGE
THE
CODE
FOR
CLIPPING
UNSIGNED
BYTES
IS
SIMILAR
EXAMPLE
CLIPPING
TO
A
SIGNED
RANGE
OF
WORDS
HIGH
LOW
INPUT
SIGNED
SOURCE
OPERANDS
OUTPUT
SIGNED
WORDS
CLIPPED
TO
THE
SIGNED
RANGE
HIGH
LOW
PMINSW
WITH
EXAMPLE
CAN
BE
EASILY
EXTENDED
TO
CLIP
SIGNED
BYTES
UNSIGNED
WORDS
SIGNED
AND
UNSIGNED
DWORDS
EXAMPLE
CLIPPING
TO
AN
ARBITRARY
SIGNED
RANGE
HIGH
LOW
INPUT
SIGNED
SOURCE
OPERANDS
OUTPUT
SIGNED
OPERANDS
CLIPPED
TO
THE
UNSIGNED
RANGE
HIGH
LOW
PADDW
ADD
WITH
NO
SATURATION
TO
CONVERT
TO
UNSIGNED
IN
EFFECT
THIS
CLIPS
TO
HIGH
IN
EFFECT
THIS
CLIPS
TO
LOW
PADDW
UNDO
THE
PREVIOUS
TWO
OFFSETS
THE
CODE
ABOVE
CONVERTS
VALUES
TO
UNSIGNED
NUMBERS
FIRST
AND
THEN
CLIPS
THEM
TO
AN
UNSIGNED
RANGE
THE
LAST
INSTRUCTION
CONVERTS
THE
DATA
BACK
TO
SIGNED
DATA
AND
PLACES
THE
DATA
WITHIN
THE
SIGNED
RANGE
CONVERSION
TO
UNSIGNED
DATA
IS
REQUIRED
FOR
CORRECT
RESULTS
WHEN
HIGH
LOW
IF
HIGH
LOW
SIMPLIFY
THE
ALGORITHM
AS
IN
EXAMPLE
EXAMPLE
SIMPLIFIED
CLIPPING
TO
AN
ARBITRARY
SIGNED
RANGE
INPUT
SIGNED
SOURCE
OPERANDS
OUTPUT
SIGNED
OPERANDS
CLIPPED
TO
THE
UNSIGNED
RANGE
HIGH
LOW
PADDSSW
IN
EFFECT
THIS
CLIPS
TO
HIGH
PSUBSSW
CLIPS
TO
LOW
PADDW
LOW
UNDO
THE
PREVIOUS
TWO
OFFSETS
THIS
ALGORITHM
SAVES
A
CYCLE
WHEN
IT
IS
KNOWN
THAT
HIGH
LOW
THE
THREE
INSTRUCTION
ALGORITHM
DOES
NOT
WORK
WHEN
HIGH
LOW
BECAUSE
MINUS
ANY
NUMBER
WILL
YIELD
A
NUMBER
GREATER
IN
MAGNITUDE
THAN
WHICH
IS
A
NEGATIVE
NUMBER
WHEN
THE
SECOND
INSTRUCTION
PSUBSSW
HIGH
LOW
IN
THE
THREE
STEP
ALGORITHM
EXAMPLE
IS
EXECUTED
A
NEGATIVE
NUMBER
IS
SUBTRACTED
THE
RESULT
OF
THIS
SUBTRACTION
CAUSES
THE
VALUES
IN
TO
BE
INCREASED
INSTEAD
OF
DECREASED
AS
SHOULD
BE
THE
CASE
AND
AN
INCORRECT
ANSWER
IS
GENERATED
CLIPPING
TO
AN
ARBITRARY
UNSIGNED
RANGE
HIGH
LOW
EXAMPLE
CLIPS
AN
UNSIGNED
VALUE
TO
THE
UNSIGNED
RANGE
HIGH
LOW
IF
THE
VALUE
IS
LESS
THAN
LOW
OR
GREATER
THAN
HIGH
THEN
CLIP
TO
LOW
OR
HIGH
RESPECTIVELY
THIS
TECH
NIQUE
USES
THE
PACKED
ADD
AND
PACKED
SUBTRACT
INSTRUCTIONS
WITH
UNSIGNED
SATURA
TION
THUS
THE
TECHNIQUE
CAN
ONLY
BE
USED
ON
PACKED
BYTES
AND
PACKED
WORDS
DATA
TYPES
FIGURE
ILLUSTRATES
OPERATION
ON
WORD
VALUES
EXAMPLE
CLIPPING
TO
AN
ARBITRARY
UNSIGNED
RANGE
HIGH
LOW
INPUT
UNSIGNED
SOURCE
OPERANDS
OUTPUT
UNSIGNED
OPERANDS
CLIPPED
TO
THE
UNSIGNED
RANGE
HIGH
LOW
PADDUSW
HIGH
IN
EFFECT
THIS
CLIPS
TO
HIGH
PSUBUSW
HIGH
LOW
IN
EFFECT
THIS
CLIPS
TO
LOW
PADDW
LOW
UNDO
THE
PREVIOUS
TWO
OFFSETS
PACKED
MAX
MIN
OF
BYTE
WORD
AND
DWORD
THE
PMAXSW
INSTRUCTION
RETURNS
THE
MAXIMUM
BETWEEN
FOUR
SIGNED
WORDS
IN
EITHER
OF
TWO
SIMD
REGISTERS
OR
ONE
SIMD
REGISTER
AND
A
MEMORY
LOCATION
THE
PMINSW
INSTRUCTION
RETURNS
THE
MINIMUM
BETWEEN
THE
FOUR
SIGNED
WORDS
IN
EITHER
OF
TWO
SIMD
REGISTERS
OR
ONE
SIMD
REGISTER
AND
A
MEMORY
LOCATION
THE
PMAXUB
INSTRUCTION
RETURNS
THE
MAXIMUM
BETWEEN
THE
EIGHT
UNSIGNED
BYTES
IN
EITHER
OF
TWO
SIMD
REGISTERS
OR
ONE
SIMD
REGISTER
AND
A
MEMORY
LOCATION
THE
PMINUB
INSTRUCTION
RETURNS
THE
MINIMUM
BETWEEN
THE
EIGHT
UNSIGNED
BYTES
IN
EITHER
OF
TWO
SIMD
REGISTERS
OR
ONE
SIMD
REGISTER
AND
A
MEMORY
LOCATION
EXTENDED
PMAXSW
PMAXUB
PMINSW
PMINUB
TO
BIT
OPERATIONS
ADDS
BIT
OPERATIONS
FOR
SIGNED
BYTES
UNSIGNED
WORD
SIGNED
AND
UNSIGNED
DWORD
PACKED
MULTIPLY
INTEGERS
THE
PMULHUW
PMULHW
INSTRUCTION
MULTIPLIES
THE
UNSIGNED
SIGNED
WORDS
IN
THE
DESTINATION
OPERAND
WITH
THE
UNSIGNED
SIGNED
WORDS
IN
THE
SOURCE
OPERAND
THE
HIGH
ORDER
BITS
OF
THE
BIT
INTERMEDIATE
RESULTS
ARE
WRITTEN
TO
THE
DESTINATION
OPERAND
THE
PMULLW
INSTRUCTION
MULTIPLIES
THE
SIGNED
WORDS
IN
THE
DESTINATION
OPERAND
WITH
THE
SIGNED
WORDS
IN
THE
SOURCE
OPERAND
THE
LOW
ORDER
BITS
OF
THE
BIT
INTERMEDIATE
RESULTS
ARE
WRITTEN
TO
THE
DESTINATION
OPERAND
EXTENDED
PMULHUW
PMULHW
PMULLW
TO
BIT
OPERATIONS
AND
ADDS
PMULUDQ
THE
PMULUDQ
INSTRUCTION
PERFORMS
AN
UNSIGNED
MULTIPLY
ON
THE
LOWER
PAIR
OF
DOUBLE
WORD
OPERANDS
WITHIN
BIT
CHUNKS
FROM
THE
TWO
SOURCES
THE
FULL
BIT
RESULT
FROM
EACH
MULTIPLICATION
IS
RETURNED
TO
THE
DESTINATION
REGISTER
THIS
INSTRUCTION
IS
ADDED
IN
BOTH
A
BIT
AND
BIT
VERSION
THE
LATTER
PERFORMS
INDEPENDENT
OPERATIONS
ON
THE
LOW
AND
HIGH
HALVES
OF
A
BIT
REGISTER
ADDS
BIT
OPERATIONS
OF
PMULDQ
AND
PMULLD
THE
PMULLD
INSTRUCTION
MULTIPLIES
THE
SIGNED
DWORDS
IN
THE
DESTINATION
OPERAND
WITH
THE
SIGNED
DWORDS
IN
THE
SOURCE
OPERAND
THE
LOW
ORDER
BITS
OF
THE
BIT
INTERMEDIATE
RESULTS
ARE
WRITTEN
TO
THE
DESTINATION
OPERAND
THE
PMULDQ
INSTRUCTION
MULTIPLIES
THE
TWO
LOW
ORDER
SIGNED
DWORDS
IN
THE
DESTINATION
OPERAND
WITH
THE
TWO
LOW
ORDER
SIGNED
DWORDS
IN
THE
SOURCE
OPERAND
AND
STORES
TWO
BIT
RESULTS
IN
THE
DESTINATION
OPERAND
PACKED
SUM
OF
ABSOLUTE
DIFFERENCES
THE
PSADBW
INSTRUCTION
COMPUTES
THE
ABSOLUTE
VALUE
OF
THE
DIFFERENCE
OF
UNSIGNED
BYTES
FOR
EITHER
TWO
SIMD
REGISTERS
OR
ONE
SIMD
REGISTER
AND
A
MEMORY
LOCATION
THE
DIFFERENCES
OF
PAIRS
OF
UNSIGNED
BYTES
ARE
THEN
SUMMED
TO
PRODUCE
A
WORD
RESULT
IN
THE
LOWER
BIT
FIELD
AND
THE
UPPER
THREE
WORDS
ARE
SET
TO
ZERO
WITH
PSADBW
IS
EXTENDED
TO
COMPUTE
TWO
WORD
RESULTS
THE
SUBTRACTION
OPERATION
PRESENTED
ABOVE
IS
AN
ABSOLUTE
DIFFERENCE
THAT
IS
T
ABS
X
Y
BYTE
VALUES
ARE
STORED
IN
TEMPORARY
SPACE
ALL
VALUES
ARE
SUMMED
TOGETHER
AND
THE
RESULT
IS
WRITTEN
TO
THE
LOWER
WORD
OF
THE
DESTINATION
REGISTER
MOTION
ESTIMATION
INVOLVES
SEARCHING
REFERENCE
FRAMES
FOR
BEST
MATCHES
SUM
ABSO
LUTE
DIFFERENCE
SAD
ON
TWO
BLOCKS
OF
PIXELS
IS
A
COMMON
INGREDIENT
IN
VIDEO
PROCESSING
ALGORITHMS
TO
LOCATE
MATCHING
BLOCKS
OF
PIXELS
PSADBW
CAN
BE
USED
AS
BUILDING
BLOCKS
FOR
FINDING
BEST
MATCHES
BY
WAY
OF
CALCULATING
SAD
RESULTS
ON
BLOCKS
OF
PIXELS
MPSADBW
AND
PHMINPOSUW
THE
MPSADBW
INSTRUCTION
IN
PERFORMS
EIGHT
SAD
OPERATIONS
EACH
SAD
OPER
ATION
PRODUCES
A
WORD
RESULT
FROM
PAIRS
OF
UNSIGNED
BYTES
WITH
SAD
RESULT
IN
AN
XMM
REGISTER
PHMINPOSUM
CAN
HELP
SEARCH
FOR
THE
BEST
MATCH
BETWEEN
EIGHT
PIXEL
BLOCKS
FOR
MOTION
ESTIMATION
ALGORITHMS
MPSADBW
IS
LIKELY
TO
IMPROVE
OVER
PSADBW
IN
SEVERAL
WAYS
SIMPLIFIED
DATA
MOVEMENT
TO
CONSTRUCT
PACKED
DATA
FORMAT
FOR
SAD
COMPUTATION
ON
PIXEL
BLOCKS
HIGHER
THROUGHPUT
IN
TERMS
OF
SAD
RESULTS
PER
ITERATION
LESS
ITERATION
REQUIRED
PER
FRAME
MPSADBW
RESULTS
ARE
AMENABLE
TO
EFFICIENT
SEARCH
USING
PHMINPOSUW
EXAMPLES
OF
MPSADBW
VS
PSADBW
FOR
AND
BLOCK
SEARCH
CAN
BE
FOUND
IN
THE
WHITE
PAPER
LISTED
IN
THE
REFERENCE
SECTION
OF
CHAPTER
PACKED
AVERAGE
BYTE
WORD
THE
PAVGB
AND
PAVGW
INSTRUCTIONS
ADD
THE
UNSIGNED
DATA
ELEMENTS
OF
THE
SOURCE
OPERAND
TO
THE
UNSIGNED
DATA
ELEMENTS
OF
THE
DESTINATION
REGISTER
ALONG
WITH
A
CARRY
IN
THE
RESULTS
OF
THE
ADDITION
ARE
THEN
INDEPENDENTLY
SHIFTED
TO
THE
RIGHT
BY
ONE
BIT
POSITION
THE
HIGH
ORDER
BITS
OF
EACH
ELEMENT
ARE
FILLED
WITH
THE
CARRY
BITS
OF
THE
CORRE
SPONDING
SUM
THE
DESTINATION
OPERAND
IS
AN
SIMD
REGISTER
THE
SOURCE
OPERAND
CAN
EITHER
BE
AN
SIMD
REGISTER
OR
A
MEMORY
OPERAND
THE
PAVGB
INSTRUCTION
OPERATES
ON
PACKED
UNSIGNED
BYTES
AND
THE
PAVGW
INSTRUC
TION
OPERATES
ON
PACKED
UNSIGNED
WORDS
COMPLEX
MULTIPLY
BY
A
CONSTANT
COMPLEX
MULTIPLICATION
IS
AN
OPERATION
WHICH
REQUIRES
FOUR
MULTIPLICATIONS
AND
TWO
ADDITIONS
THIS
IS
EXACTLY
HOW
THE
PMADDWD
INSTRUCTION
OPERATES
IN
ORDER
TO
USE
THIS
INSTRUCTION
YOU
NEED
TO
FORMAT
THE
DATA
INTO
MULTIPLE
BIT
VALUES
THE
REAL
AND
IMAGINARY
COMPONENTS
SHOULD
BE
BITS
EACH
CONSIDER
EXAMPLE
WHICH
ASSUMES
THAT
THE
BIT
MMX
REGISTERS
ARE
BEING
USED
LET
THE
INPUT
DATA
BE
DR
AND
DI
WHERE
DR
IS
REAL
COMPONENT
OF
THE
DATA
AND
DI
IS
IMAGINARY
COMPONENT
OF
THE
DATA
FORMAT
THE
CONSTANT
COMPLEX
COEFFICIENTS
IN
MEMORY
AS
FOUR
BIT
VALUES
CR
CI
CI
CR
REMEMBER
TO
LOAD
THE
VALUES
INTO
THE
MMX
REGISTER
USING
MOVQ
THE
REAL
COMPONENT
OF
THE
COMPLEX
PRODUCT
IS
PR
DR
CR
DI
CI
AND
THE
IMAGINARY
COMPONENT
OF
THE
COMPLEX
PRODUCT
IS
PI
DR
CI
DI
CR
THE
OUTPUT
IS
A
PACKED
DOUBLEWORD
IF
NEEDED
A
PACK
INSTRUCTION
CAN
BE
USED
TO
CONVERT
THE
RESULT
TO
BIT
THEREBY
MATCHING
THE
FORMAT
OF
THE
INPUT
EXAMPLE
COMPLEX
MULTIPLY
BY
A
CONSTANT
INPUT
COMPLEX
VALUE
DR
DI
CONSTANT
COMPLEX
COEFFICIENT
IN
THE
FORM
CR
CI
CI
CR
OUTPUT
TWO
BIT
DWORDS
CONTAINING
PR
PI
PUNPCKLDQ
MAKES
DR
DI
DR
DI
PMADDWD
DONE
THE
RESULT
IS
DR
CR
DI
CI
DR
CI
DI
CR
PACKED
BIT
ADD
SUBTRACT
THE
PADDQ
PSUBQ
INSTRUCTIONS
ADD
SUBTRACT
QUAD
WORD
OPERANDS
WITHIN
EACH
BIT
CHUNK
FROM
THE
TWO
SOURCES
THE
BIT
RESULT
FROM
EACH
COMPUTATION
IS
WRITTEN
TO
THE
DESTINATION
REGISTER
LIKE
THE
INTEGER
ADD
SUB
INSTRUCTION
PADDQ
PSUBQ
CAN
OPERATE
ON
EITHER
UNSIGNED
OR
SIGNED
TWO
COMPLEMENT
NOTATION
INTEGER
OPERANDS
WHEN
AN
INDIVIDUAL
RESULT
IS
TOO
LARGE
TO
BE
REPRESENTED
IN
BITS
THE
LOWER
BITS
OF
THE
RESULT
ARE
WRITTEN
TO
THE
DESTINATION
OPERAND
AND
THEREFORE
THE
RESULT
WRAPS
AROUND
THESE
INSTRUCTIONS
ARE
ADDED
IN
BOTH
A
BIT
AND
BIT
VERSION
THE
LATTER
PERFORMS
INDEPENDENT
OPERATIONS
ON
THE
LOW
AND
HIGH
HALVES
OF
A
BIT
REGISTER
BIT
SHIFTS
THE
PSLLDQ
PSRLDQ
INSTRUCTIONS
SHIFT
THE
FIRST
OPERAND
TO
THE
LEFT
RIGHT
BY
THE
NUMBER
OF
BYTES
SPECIFIED
BY
THE
IMMEDIATE
OPERAND
THE
EMPTY
LOW
HIGH
ORDER
BYTES
ARE
CLEARED
SET
TO
ZERO
IF
THE
VALUE
SPECIFIED
BY
THE
IMMEDIATE
OPERAND
IS
GREATER
THAN
THEN
THE
DESTINA
TION
IS
SET
TO
ALL
ZEROS
PTEST
AND
CONDITIONAL
BRANCH
OFFERS
PTEST
INSTRUCTION
THAT
CAN
BE
USED
IN
VECTORIZING
LOOPS
WITH
CONDITIONAL
BRANCHES
PTEST
IS
AN
BIT
VERSION
OF
THE
GENERAL
PURPOSE
INSTRUCTION
TEST
THE
ZF
OR
CF
FIELD
OF
THE
EFLAGS
REGISTER
ARE
MODIFIED
AS
A
RESULT
OF
PTEST
EXAMPLE
A
DEPICTS
A
LOOP
THAT
REQUIRES
A
CONDITIONAL
BRANCH
TO
HANDLE
THE
SPECIAL
CASE
OF
DIVIDE
BY
ZERO
IN
ORDER
TO
VECTORIZE
SUCH
LOOP
ANY
ITERATION
THAT
MAY
ENCOUNTER
DIVIDE
BY
ZERO
MUST
BE
TREATED
OUTSIDE
THE
VECTORIZABLE
ITERATIONS
EXAMPLE
USING
PTEST
TO
SEPARATE
VECTORIZABLE
AND
NON
VECTORIZABLE
LOOP
ITERATIONS
A
LOOPS
REQUIRING
INFREQUENT
EXCEPTION
HANDLING
FLOAT
A
CNT
UNSIGNED
INT
I
B
PTEST
ENABLES
EARLY
OUT
TO
HANDLE
INFREQUENT
NON
VECTORIZABLE
PORTION
XOR
EAX
EAX
MOVAPS
XORPS
FOR
I
I
CNT
I
IF
A
I
LP
MOVAPS
A
EAX
CMPEQPS
CONVERT
EACH
NON
ZERO
TO
A
I
A
I
ELSE
CALL
DIVEXCEPTION
ONES
PTEST
JNC
CARRY
WILL
BE
SET
IF
ALL
WERE
NON
ZERO
MOVAPS
DIVPS
MOVAPS
A
EAX
ADD
EAX
CMP
EAX
CNT
JNZ
LP
JMP
END
EXECUTE
ONE
BY
ONE
CALL
EXCEPTION
WHEN
VALUE
IS
ZERO
EXAMPLE
B
SHOWS
AN
ASSEMBLY
SEQUENCE
THAT
USES
PTEST
TO
CAUSE
AN
EARLY
OUT
BRANCH
WHENEVER
ANY
ONE
OF
THE
FOUR
FLOATING
POINT
VALUES
IN
IS
ZERO
THE
FALL
THROUGH
PATH
ENABLES
THE
REST
OF
THE
FLOATING
POINT
CALCULATIONS
TO
BE
VECTORIZED
BECAUSE
NONE
OF
THE
FOUR
VALUES
ARE
ZERO
VECTORIZATION
OF
HETEROGENEOUS
COMPUTATIONS
ACROSS
LOOP
ITERATIONS
VECTORIZATION
TECHNIQUES
ON
UN
ROLLED
LOOPS
GENERALLY
RELY
ON
REPETITIVE
HOMOGE
NEOUS
OPERATIONS
BETWEEN
EACH
LOOP
ITERATION
USING
PTEST
AND
VARIABLE
BLEND
INSTRUCTIONS
VECTORIZATION
OF
HETEROGENEOUS
OPERATIONS
ACROSS
LOOP
ITERATIONS
MAY
BE
POSSIBLE
EXAMPLE
A
DEPICTS
A
SIMPLE
HETEROGENEOUS
LOOP
THE
HETEROGENEOUS
OPERATION
AND
CONDITIONAL
BRANCH
MAKES
SIMPLE
LOOP
UNROLLING
TECHNIQUE
INFEASIBLE
FOR
VECTOR
IZATION
EXAMPLE
USING
PTEST
AND
VARIABLE
BLEND
TO
VECTORIZE
HETEROGENEOUS
LOOPS
A
LOOPS
WITH
HETEROGENEOUS
OPERATION
ACROSS
ITERATIONS
FLOAT
A
CNT
UNSIGNED
INT
I
FOR
I
I
CNT
I
IF
A
I
B
I
A
I
B
I
ELSE
A
I
B
I
B
VECTORIZE
CONDITION
FLOW
WITH
PTEST
BLENDVPS
XOR
EAX
EAX
LP
MOVAPS
A
EAX
MOVAPS
B
EAX
MOVAPS
COMPARE
A
AND
B
VALUES
CMPGTPS
WILL
HOLD
B
MOVAPS
XORPS
SELECT
VALUES
FOR
THE
ADD
OPERATION
TRUE
CONDITION
PRODUCE
A
B
FALSE
WILL
BECOME
A
B
BLEND
MASK
IS
BLENDVPS
ADDPS
MOVAPS
A
EAX
ADD
EAX
CMP
EAX
CNT
JNZ
LP
EXAMPLE
B
DEPICTS
AN
ASSEMBLY
SEQUENCE
THAT
USES
BLENDVPS
AND
PTEST
TO
VECTORIZE
THE
HANDLING
OF
HETEROGENEOUS
COMPUTATIONS
OCCURRING
ACROSS
FOUR
CONSEC
UTIVE
LOOP
ITERATIONS
VECTORIZATION
OF
CONTROL
FLOWS
IN
NESTED
LOOPS
THE
PTEST
AND
BLENDVPX
INSTRUCTIONS
CAN
BE
USED
AS
BUILDING
BLOCKS
TO
VECTORIZE
MORE
COMPLEX
CONTROL
FLOW
STATEMENTS
WHERE
EACH
CONTROL
FLOW
STATEMENT
IS
CREATING
A
WORKING
MASK
USED
AS
A
PREDICATE
OF
WHICH
THE
CONDITIONAL
CODE
UNDER
THE
MASK
WILL
OPERATE
THE
MANDELBROT
SET
MAP
EVALUATION
IS
USEFUL
TO
ILLUSTRATE
A
SITUATION
WITH
MORE
COMPLEX
CONTROL
FLOWS
IN
NESTED
LOOPS
THE
MANDELBROT
SET
IS
A
SET
OF
HEIGHT
VALUES
MAPPED
TO
A
D
GRID
THE
HEIGHT
VALUE
IS
THE
NUMBER
OF
MANDELBROT
ITERATIONS
DEFINED
OVER
THE
COMPLEX
NUMBER
SPACE
AS
IN
IN
NEEDED
TO
GET
IN
IT
IS
COMMON
TO
LIMIT
THE
MAP
GENERATION
BY
SETTING
SOME
MAXIMUM
THRESHOLD
VALUE
OF
THE
HEIGHT
ALL
OTHER
POINTS
ARE
ASSIGNED
WITH
A
HEIGHT
EQUAL
TO
THE
THRESHOLD
EXAMPLE
SHOWS
AN
EXAMPLE
OF
MANDELBROT
MAP
EVALUATION
IMPLEMENTED
IN
C
EXAMPLE
BASELINE
C
CODE
FOR
MANDELBROT
SET
MAP
EVALUATION
DEFINE
DIMX
DEFINE
DIMY
DEFINE
DIMX
DEFINE
DIMY
INT
MAP
DIMX
DIMY
VOID
INT
I
J
FLOAT
X
Y
FOR
I
X
I
DIMX
I
X
FOR
J
Y
J
DIMY
J
Y
FLOAT
SX
SY
INT
ITER
SX
X
SY
Y
WHILE
ITER
IF
SX
SX
SY
SY
BREAK
FLOAT
SX
SX
X
SX
SX
SY
SY
SY
Y
SY
ITER
MAP
I
J
ITER
EXAMPLE
SHOWS
A
VECTORIZED
IMPLEMENTATION
OF
MANDELBROT
MAP
EVALUATION
VECTORIZATION
IS
NOT
DONE
ON
THE
INNER
MOST
LOOP
BECAUSE
THE
PRESENCE
OF
THE
BREAK
STATEMENT
IMPLIES
THE
ITERATION
COUNT
WILL
VARY
FROM
ONE
PIXEL
TO
THE
NEXT
THE
VECTOR
IZED
VERSION
TAKE
INTO
ACCOUNT
THE
PARALLEL
NATURE
OF
D
VECTORIZE
OVER
FOUR
ITERA
TIONS
OF
Y
VALUES
OF
CONSECUTIVE
PIXELS
AND
CONDITIONALLY
HANDLES
THREE
SCENARIOS
IN
THE
INNER
MOST
ITERATION
WHEN
ALL
PIXELS
DO
NOT
REACH
BREAK
CONDITION
VECTORIZE
PIXELS
WHEN
ONE
OR
MORE
PIXELS
REACHED
BREAK
CONDITION
USE
BLEND
INTRINSICS
TO
ACCUMULATE
THE
COMPLEX
HEIGHT
VECTOR
FOR
THE
REMAINING
PIXELS
NOT
REACHING
THE
BREAK
CONDITION
AND
CONTINUE
THE
INNER
ITERATION
OF
THE
COMPLEX
HEIGHT
VECTOR
WHEN
ALL
FOUR
PIXELS
REACHED
BREAK
CONDITION
EXIT
THE
INNER
LOOP
EXAMPLE
VECTORIZED
MANDELBROT
SET
MAP
EVALUATION
USING
INTRINSICS
DECLSPEC
ALIGN
FLOAT
VOID
INT
I
J
X
Y
FOR
I
X
I
DIMX
I
X
FOR
J
DIMY
Y
J
DIMY
J
Y
SX
SY
ITER
INT
SX
X
SY
Y
WHILE
INT
MASK
SX
VMASK
SX
SX
SY
SY
IF
ALL
DATA
POINTS
IN
OUR
VECTOR
ARE
HITTING
THE
EXIT
CONDITION
THE
VECTORIZED
LOOP
CAN
EXIT
IF
VMASK
BREAK
CONTINUE
EXAMPLE
VECTORIZED
MANDELBROT
SET
MAP
EVALUATION
USING
INTRINSICS
IF
NON
OF
THE
DATA
POINTS
ARE
OUT
WE
DON
T
NEED
THE
EXTRA
CODE
WHICH
BLENDS
THE
RESULTS
IF
VMASK
VMASK
SX
X
SX
SX
SY
SY
SY
Y
SY
ITER
ELSE
BLENDED
FLAVOUR
OF
THE
CODE
THIS
CODE
BLENDS
VALUES
FROM
PREVIOUS
ITERATION
WITH
THE
VALUES
FROM
CURRENT
ITERATION
ONLY
VALUES
WHICH
DID
NOT
HIT
THE
EXIT
CONDITION
ARE
BEING
STORED
VALUES
WHICH
ARE
ALREADY
OUT
ARE
MAINTAINING
THEIR
VALUE
SX
X
SX
SX
SY
SY
SX
VMASK
SY
Y
SY
SY
VMASK
ITER
ITER
ITER
VMASK
MAP
I
J
ITER
MEMORY
OPTIMIZATIONS
YOU
CAN
IMPROVE
MEMORY
ACCESS
USING
THE
FOLLOWING
TECHNIQUES
AVOIDING
PARTIAL
MEMORY
ACCESSES
INCREASING
THE
BANDWIDTH
OF
MEMORY
FILLS
AND
VIDEO
FILLS
PREFETCHING
DATA
WITH
STREAMING
SIMD
EXTENSIONS
SEE
CHAPTER
OPTIMIZING
CACHE
USAGE
MMX
REGISTERS
AND
XMM
REGISTERS
ALLOW
YOU
TO
MOVE
LARGE
QUANTITIES
OF
DATA
WITHOUT
STALLING
THE
PROCESSOR
INSTEAD
OF
LOADING
SINGLE
ARRAY
VALUES
THAT
ARE
OR
BITS
LONG
CONSIDER
LOADING
THE
VALUES
IN
A
SINGLE
QUADWORD
OR
DOUBLE
QUADWORD
AND
THEN
INCREMENTING
THE
STRUCTURE
OR
ARRAY
POINTER
ACCORDINGLY
ANY
DATA
THAT
WILL
BE
MANIPULATED
BY
SIMD
INTEGER
INSTRUCTIONS
SHOULD
BE
LOADED
USING
EITHER
AN
SIMD
INTEGER
INSTRUCTION
THAT
LOADS
A
BIT
OR
BIT
OPERAND
FOR
EXAMPLE
MOVQ
THE
REGISTER
MEMORY
FORM
OF
ANY
SIMD
INTEGER
INSTRUCTION
THAT
OPERATES
ON
A
QUADWORD
OR
DOUBLE
QUADWORD
MEMORY
OPERAND
FOR
EXAMPLE
PMADDW
ALL
SIMD
DATA
SHOULD
BE
STORED
USING
AN
SIMD
INTEGER
INSTRUCTION
THAT
STORES
A
BIT
OR
BIT
OPERAND
FOR
EXAMPLE
MOVQ
THE
GOAL
OF
THE
ABOVE
RECOMMENDATIONS
IS
TWOFOLD
FIRST
THE
LOADING
AND
STORING
OF
SIMD
DATA
IS
MORE
EFFICIENT
USING
THE
LARGER
BLOCK
SIZES
SECOND
FOLLOWING
THE
ABOVE
RECOMMENDATIONS
HELPS
TO
AVOID
MIXING
OF
OR
BIT
LOAD
AND
STORE
OPERA
TIONS
WITH
SIMD
INTEGER
TECHNOLOGY
LOAD
AND
STORE
OPERATIONS
TO
THE
SAME
SIMD
DATA
THIS
PREVENTS
SITUATIONS
IN
WHICH
SMALL
LOADS
FOLLOW
LARGE
STORES
TO
THE
SAME
AREA
OF
MEMORY
OR
LARGE
LOADS
FOLLOW
SMALL
STORES
TO
THE
SAME
AREA
OF
MEMORY
THE
PENTIUM
II
PENTIUM
III
AND
PENTIUM
PROCESSORS
MAY
STALL
IN
SUCH
SITUATIONS
SEE
CHAPTER
FOR
DETAILS
PARTIAL
MEMORY
ACCESSES
CONSIDER
A
CASE
WITH
A
LARGE
LOAD
AFTER
A
SERIES
OF
SMALL
STORES
TO
THE
SAME
AREA
OF
MEMORY
BEGINNING
AT
MEMORY
ADDRESS
MEM
THE
LARGE
LOAD
STALLS
IN
THE
CASE
SHOWN
IN
EXAMPLE
EXAMPLE
A
LARGE
LOAD
AFTER
A
SERIES
OF
SMALL
STORES
PENALTY
MOVQ
MUST
WAIT
FOR
THE
STORES
TO
WRITE
MEMORY
BEFORE
IT
CAN
ACCESS
ALL
DATA
IT
REQUIRES
THIS
STALL
CAN
ALSO
OCCUR
WITH
OTHER
DATA
TYPES
FOR
EXAMPLE
WHEN
BYTES
OR
WORDS
ARE
STORED
AND
THEN
WORDS
OR
DOUBLEWORDS
ARE
READ
FROM
THE
SAME
AREA
OF
MEMORY
WHEN
YOU
CHANGE
THE
CODE
SEQUENCE
AS
SHOWN
IN
EXAMPLE
THE
PROCESSOR
CAN
ACCESS
THE
DATA
WITHOUT
DELAY
EXAMPLE
ACCESSING
DATA
WITHOUT
DELAY
MOVD
EBX
BUILD
DATA
INTO
A
QWORD
FIRST
BEFORE
STORING
IT
TO
MEMORY
MOVD
EAX
PSLLQ
POR
MOVQ
MEM
STORE
SIMD
VARIABLE
TO
MEM
AS
A
QWORD
MOVQ
MEM
LOAD
QWORD
SIMD
MEM
NO
STALL
CONSIDER
A
CASE
WITH
A
SERIES
OF
SMALL
LOADS
AFTER
A
LARGE
STORE
TO
THE
SAME
AREA
OF
MEMORY
BEGINNING
AT
MEMORY
ADDRESS
MEM
AS
SHOWN
IN
EXAMPLE
MOST
OF
THE
SMALL
LOADS
STALL
BECAUSE
THEY
ARE
NOT
ALIGNED
WITH
THE
STORE
SEE
SECTION
STORE
FORWARDING
FOR
DETAILS
EXAMPLE
A
SERIES
OF
SMALL
LOADS
AFTER
A
LARGE
STORE
MOVQ
MEM
STORE
QWORD
TO
ADDRESS
MEM
MOV
BX
MEM
LOAD
WORD
AT
MEM
STALLS
MOV
CX
MEM
LOAD
WORD
AT
MEM
STALLS
THE
WORD
LOADS
MUST
WAIT
FOR
THE
QUADWORD
STORE
TO
WRITE
TO
MEMORY
BEFORE
THEY
CAN
ACCESS
THE
DATA
THEY
REQUIRE
THIS
STALL
CAN
ALSO
OCCUR
WITH
OTHER
DATA
TYPES
FOR
EXAMPLE
WHEN
DOUBLEWORDS
OR
WORDS
ARE
STORED
AND
THEN
WORDS
OR
BYTES
ARE
READ
FROM
THE
SAME
AREA
OF
MEMORY
WHEN
YOU
CHANGE
THE
CODE
SEQUENCE
AS
SHOWN
IN
EXAMPLE
THE
PROCESSOR
CAN
ACCESS
THE
DATA
WITHOUT
DELAY
EXAMPLE
ELIMINATING
DELAY
FOR
A
SERIES
OF
SMALL
LOADS
AFTER
A
LARGE
STORE
MOVQ
MEM
STORE
QWORD
TO
ADDRESS
MEM
MOVQ
MEM
LOAD
QWORD
AT
ADDRESS
MEM
MOVD
EAX
TRANSFER
MEM
TO
EAX
FROM
MMX
REGISTER
NOT
MEMORY
EXAMPLE
ELIMINATING
DELAY
FOR
A
SERIES
OF
SMALL
LOADS
AFTER
A
LARGE
STORE
PSRLQ
SHR
EAX
MOVD
EBX
TRANSFER
MEM
TO
BX
FROM
MMX
REGISTER
NOT
MEMORY
AND
EBX
THESE
TRANSFORMATIONS
IN
GENERAL
INCREASE
THE
NUMBER
OF
INSTRUCTIONS
REQUIRED
TO
PERFORM
THE
DESIRED
OPERATION
FOR
PENTIUM
II
PENTIUM
III
AND
PENTIUM
PROCESSORS
THE
BENEFIT
OF
AVOIDING
FORWARDING
PROBLEMS
OUTWEIGHS
THE
PERFORMANCE
PENALTY
DUE
TO
THE
INCREASED
NUMBER
OF
INSTRUCTIONS
SUPPLEMENTAL
TECHNIQUES
FOR
AVOIDING
CACHE
LINE
SPLITS
VIDEO
PROCESSING
APPLICATIONS
SOMETIMES
CANNOT
AVOID
LOADING
DATA
FROM
MEMORY
ADDRESSES
THAT
ARE
NOT
ALIGNED
TO
BYTE
BOUNDARIES
AN
EXAMPLE
OF
THIS
SITUATION
IS
WHEN
EACH
LINE
IN
A
VIDEO
FRAME
IS
AVERAGED
BY
SHIFTING
HORIZONTALLY
HALF
A
PIXEL
EXAMPLE
SHOWS
A
COMMON
OPERATION
IN
VIDEO
PROCESSING
THAT
LOADS
DATA
FROM
MEMORY
ADDRESS
NOT
ALIGNED
TO
A
BYTE
BOUNDARY
AS
VIDEO
PROCESSING
TRAVERSES
EACH
LINE
IN
THE
VIDEO
FRAME
IT
EXPERIENCES
A
CACHE
LINE
SPLIT
FOR
EACH
BYTE
CHUNK
LOADED
FROM
MEMORY
EXAMPLE
AN
EXAMPLE
OF
VIDEO
PROCESSING
WITH
CACHE
LINE
SPLITS
AVERAGE
HALF
PELS
HORIZONTALLY
ON
THE
X
AXIS
FROM
ONE
REFERENCE
FRAME
ONLY
NEXTLINESLOOP
MOVDQU
XMMWORD
PTR
EDX
MAY
NOT
BE
ALIGNED
MOVDQU
XMMWORD
PTR
EDX
MOVDQU
XMMWORD
PTR
EDX
EAX
MOVDQU
XMMWORD
PTR
EDX
EAX
MOVDQAXMMWORD
PTR
ECX
MOVDQAXMMWORD
PTR
ECX
EAX
REPEAT
PROVIDES
AN
INSTRUCTION
LDDQU
FOR
LOADING
FROM
MEMORY
ADDRESS
THAT
ARE
NOT
BYTE
ALIGNED
LDDQU
IS
A
SPECIAL
BIT
UNALIGNED
LOAD
DESIGNED
TO
AVOID
CACHE
LINE
SPLITS
IF
THE
ADDRESS
OF
THE
LOAD
IS
ALIGNED
ON
A
BYTE
BOUNDARY
LDQQU
LOADS
THE
BYTES
REQUESTED
IF
THE
ADDRESS
OF
THE
LOAD
IS
NOT
ALIGNED
ON
A
BYTE
BOUNDARY
LDDQU
LOADS
A
BYTE
BLOCK
STARTING
AT
THE
BYTE
ALIGNED
ADDRESS
IMMEDIATELY
BELOW
THE
ADDRESS
OF
THE
LOAD
REQUEST
IT
THEN
PROVIDES
THE
REQUESTED
BYTES
IF
THE
ADDRESS
IS
ALIGNED
ON
A
BYTE
BOUNDARY
THE
EFFECTIVE
NUMBER
OF
MEMORY
REQUESTS
IS
IMPLEMENTATION
DEPENDENT
ONE
OR
MORE
LDDQU
IS
DESIGNED
FOR
PROGRAMMING
USAGE
OF
LOADING
DATA
FROM
MEMORY
WITHOUT
STORING
MODIFIED
DATA
BACK
TO
THE
SAME
ADDRESS
THUS
THE
USAGE
OF
LDDQU
SHOULD
BE
RESTRICTED
TO
SITUATIONS
WHERE
NO
STORE
TO
LOAD
FORWARDING
IS
EXPECTED
FOR
SITUATIONS
WHERE
STORE
TO
LOAD
FORWARDING
IS
EXPECTED
USE
REGULAR
STORE
LOAD
PAIRS
EITHER
ALIGNED
OR
UNALIGNED
BASED
ON
THE
ALIGNMENT
OF
THE
DATA
ACCESSED
EXAMPLE
VIDEO
PROCESSING
USING
LDDQU
TO
AVOID
CACHE
LINE
SPLITS
AVERAGE
HALF
PELS
HORIZONTALLY
ON
THE
X
AXIS
FROM
ONE
REFERENCE
FRAME
ONLY
NEXTLINESLOOP
LDDQU
XMMWORD
PTR
EDX
MAY
NOT
BE
ALIGNED
LDDQU
XMMWORD
PTR
EDX
LDDQU
XMMWORD
PTR
EDX
EAX
LDDQU
XMMWORD
PTR
EDX
EAX
MOVDQAXMMWORD
PTR
ECX
RESULTS
STORED
ELSEWHERE
MOVDQAXMMWORD
PTR
ECX
EAX
REPEAT
INCREASING
BANDWIDTH
OF
MEMORY
FILLS
AND
VIDEO
FILLS
IT
IS
BENEFICIAL
TO
UNDERSTAND
HOW
MEMORY
IS
ACCESSED
AND
FILLED
A
MEMORY
TO
MEMORY
FILL
FOR
EXAMPLE
A
MEMORY
TO
VIDEO
FILL
IS
DEFINED
AS
A
BYTE
CACHE
LINE
LOAD
FROM
MEMORY
WHICH
IS
IMMEDIATELY
STORED
BACK
TO
MEMORY
SUCH
AS
A
VIDEO
FRAME
BUFFER
THE
FOLLOWING
ARE
GUIDELINES
FOR
OBTAINING
HIGHER
BANDWIDTH
AND
SHORTER
LATENCIES
FOR
SEQUENTIAL
MEMORY
FILLS
VIDEO
FILLS
THESE
RECOMMENDATIONS
ARE
RELEVANT
FOR
ALL
INTEL
ARCHITECTURE
PROCESSORS
WITH
MMX
TECHNOLOGY
AND
REFER
TO
CASES
IN
WHICH
THE
LOADS
AND
STORES
DO
NOT
HIT
IN
THE
FIRST
OR
SECOND
LEVEL
CACHE
INCREASING
MEMORY
BANDWIDTH
USING
THE
MOVDQ
INSTRUCTION
LOADING
ANY
SIZE
DATA
OPERAND
WILL
CAUSE
AN
ENTIRE
CACHE
LINE
TO
BE
LOADED
INTO
THE
CACHE
HIERARCHY
THUS
ANY
SIZE
LOAD
LOOKS
MORE
OR
LESS
THE
SAME
FROM
A
MEMORY
BANDWIDTH
PERSPECTIVE
HOWEVER
USING
MANY
SMALLER
LOADS
CONSUMES
MORE
MICROAR
CHITECTURAL
RESOURCES
THAN
FEWER
LARGER
STORES
CONSUMING
TOO
MANY
RESOURCES
CAN
CAUSE
THE
PROCESSOR
TO
STALL
AND
REDUCE
THE
BANDWIDTH
THAT
THE
PROCESSOR
CAN
REQUEST
OF
THE
MEMORY
SUBSYSTEM
USING
MOVDQ
TO
STORE
THE
DATA
BACK
TO
UC
MEMORY
OR
WC
MEMORY
IN
SOME
CASES
INSTEAD
OF
USING
BIT
STORES
FOR
EXAMPLE
MOVD
WILL
REDUCE
BY
THREE
QUARTERS
THE
NUMBER
OF
STORES
PER
MEMORY
FILL
CYCLE
AS
A
RESULT
USING
THE
MOVDQ
IN
MEMORY
FILL
CYCLES
CAN
ACHIEVE
SIGNIFICANTLY
HIGHER
EFFECTIVE
BANDWIDTH
THAN
USING
MOVD
INCREASING
MEMORY
BANDWIDTH
BY
LOADING
AND
STORING
TO
AND
FROM
THE
SAME
DRAM
PAGE
DRAM
IS
DIVIDED
INTO
PAGES
WHICH
ARE
NOT
THE
SAME
AS
OPERATING
SYSTEM
OS
PAGES
THE
SIZE
OF
A
DRAM
PAGE
IS
A
FUNCTION
OF
THE
TOTAL
SIZE
OF
THE
DRAM
AND
THE
ORGANIZA
TION
OF
THE
DRAM
PAGE
SIZES
OF
SEVERAL
KILOBYTES
ARE
COMMON
LIKE
OS
PAGES
DRAM
PAGES
ARE
CONSTRUCTED
OF
SEQUENTIAL
ADDRESSES
SEQUENTIAL
MEMORY
ACCESSES
TO
THE
SAME
DRAM
PAGE
HAVE
SHORTER
LATENCIES
THAN
SEQUENTIAL
ACCESSES
TO
DIFFERENT
DRAM
PAGES
IN
MANY
SYSTEMS
THE
LATENCY
FOR
A
PAGE
MISS
THAT
IS
AN
ACCESS
TO
A
DIFFERENT
PAGE
INSTEAD
OF
THE
PAGE
PREVIOUSLY
ACCESSED
CAN
BE
TWICE
AS
LARGE
AS
THE
LATENCY
OF
A
MEMORY
PAGE
HIT
ACCESS
TO
THE
SAME
PAGE
AS
THE
PREVIOUS
ACCESS
THEREFORE
IF
THE
LOADS
AND
STORES
OF
THE
MEMORY
FILL
CYCLE
ARE
TO
THE
SAME
DRAM
PAGE
A
SIGNIFICANT
INCREASE
IN
THE
BANDWIDTH
OF
THE
MEMORY
FILL
CYCLES
CAN
BE
ACHIEVED
INCREASING
UC
AND
WC
STORE
BANDWIDTH
BY
USING
ALIGNED
STORES
USING
ALIGNED
STORES
TO
FILL
UC
OR
WC
MEMORY
WILL
YIELD
HIGHER
BANDWIDTH
THAN
USING
UNALIGNED
STORES
IF
A
UC
STORE
OR
SOME
WC
STORES
CROSS
A
CACHE
LINE
BOUNDARY
A
SINGLE
STORE
WILL
RESULT
IN
TWO
TRANSACTION
ON
THE
BUS
REDUCING
THE
EFFICIENCY
OF
THE
BUS
TRANSACTIONS
BY
ALIGNING
THE
STORES
TO
THE
SIZE
OF
THE
STORES
YOU
ELIMINATE
THE
POSSIBILITY
OF
CROSSING
A
CACHE
LINE
BOUNDARY
AND
THE
STORES
WILL
NOT
BE
SPLIT
INTO
SEPA
RATE
TRANSACTIONS
REVERSE
MEMORY
COPY
COPYING
BLOCKS
OF
MEMORY
FROM
A
SOURCE
LOCATION
TO
A
DESTINATION
LOCATION
IN
REVERSE
ORDER
PRESENTS
A
CHALLENGE
FOR
SOFTWARE
TO
MAKE
THE
MOST
OUT
OF
THE
MACHINES
CAPA
BILITIES
WHILE
AVOIDING
MICROARCHITECTURAL
HAZARDS
THE
BASIC
UN
OPTIMIZED
C
CODE
IS
SHOWN
IN
EXAMPLE
THE
SIMPLE
C
CODE
IN
EXAMPLE
IS
SUB
OPTIMAL
BECAUSE
IT
LOADS
AND
STORES
ONE
BYTE
AT
A
TIME
EVEN
IN
SITUATIONS
THAT
HARDWARE
PREFETCHER
MIGHT
HAVE
BROUGHT
DATA
IN
FROM
SYSTEM
MEMORY
TO
CACHE
EXAMPLE
UN
OPTIMIZED
REVERSE
MEMORY
COPY
IN
C
UNSIGNED
CHAR
SRC
UNSIGNED
CHAR
DST
WHILE
LEN
DST
SRC
LEN
USING
MOVDQA
OR
MOVDQU
SOFTWARE
CAN
LOAD
AND
STORE
UP
TO
BYTES
AT
A
TIME
BUT
MUST
EITHER
ENSURE
BYTE
ALIGNMENT
REQUIREMENT
IF
USING
MOVDQA
OR
MINIMIZE
THE
DELAYS
MOVDQU
MAY
ENCOUNTER
IF
DATA
SPAN
ACROSS
CACHE
LINE
BOUNDARY
FIGURE
DATA
ALIGNMENT
OF
LOADS
AND
STORES
IN
REVERSE
MEMORY
COPY
GIVEN
THE
GENERAL
PROBLEM
OF
ARBITRARY
BYTE
COUNT
TO
COPY
ARBITRARY
OFFSETS
OF
LEADING
SOURCE
BYTE
AND
DESTINATION
BYTES
ADDRESS
ALIGNMENT
RELATIVE
TO
BYTE
AND
CACHE
LINE
BOUNDARIES
THESE
ALIGNMENT
SITUATIONS
CAN
BE
A
BIT
COMPLICATED
FIGURE
A
AND
B
DEPICT
THE
ALIGNMENT
SITUATIONS
OF
REVERSE
MEMORY
COPY
OF
N
BYTES
THE
GENERAL
GUIDELINES
FOR
DEALING
WITH
UNALIGNED
LOADS
AND
STORES
ARE
IN
ORDER
OF
IMPORTANCE
AVOID
STORES
THAT
SPAN
CACHE
LINE
BOUNDARIES
MINIMIZE
THE
NUMBER
OF
LOADS
THAT
SPAN
CACHELINE
BOUNDARIES
FAVOR
BYTE
ALIGNED
LOADS
AND
STORES
OVER
UNALIGNED
VERSIONS
IN
FIGURE
A
THE
GUIDELINES
ABOVE
CAN
BE
APPLIED
TO
THE
REVERSE
MEMORY
COPY
PROBLEM
AS
FOLLOWS
PEEL
OFF
SEVERAL
LEADING
DESTINATION
BYTES
UNTIL
IT
ALIGNS
ON
BYTE
BOUNDARY
THEN
THE
ENSUING
DESTINATION
BYTES
CAN
BE
WRITTEN
TO
USING
MOVAPS
UNTIL
THE
REMAINING
BYTE
COUNT
FALLS
BELOW
BYTES
AFTER
THE
LEADING
SOURCE
BYTES
HAVE
BEEN
PEELED
CORRESPONDING
TO
STEP
ABOVE
THE
SOURCE
ALIGNMENT
IN
FIGURE
A
ALLOWS
LOADING
BYTES
AT
A
TIME
USING
MOVAPS
UNTIL
THE
REMAINING
BYTE
COUNT
FALLS
BELOW
BYTES
SWITCHING
THE
BYTE
ORDERING
OF
EACH
BYTES
OF
DATA
CAN
BE
ACCOMPLISHED
BY
A
BYTE
MASK
WITH
PSHUFB
THE
PERTINENT
CODE
SEQUENCE
IS
SHOWN
IN
EXAMPLE
EXAMPLE
USING
PSHUFB
TO
REVERSE
BYTE
ORDERING
BYTES
AT
A
TIME
DECLSPEC
ALIGN
STATIC
CONST
UNSIGNED
CHAR
BSWAPMASK
MOV
ESI
SRC
MOV
EDI
DST
MOV
ECX
LEN
MOVAPS
BSWAPMASK
START
MOVDQA
ESI
PSHUFB
MOVDQA
EDI
SUB
EDI
ADD
ESI
SUB
ECX
CMP
ECX
JAE
START
HANDLE
LEFT
OVERS
IN
FIGURE
B
WE
ALSO
START
WITH
PEELING
THE
DESTINATION
BYTES
PEEL
OFF
SEVERAL
LEADING
DESTINATION
BYTES
UNTIL
IT
ALIGNS
ON
BYTE
BOUNDARY
THEN
THE
ENSUING
DESTINATION
BYTES
CAN
BE
WRITTEN
TO
USING
MOVAPS
UNTIL
THE
REMAINING
BYTE
COUNT
FALLS
BELOW
BYTES
HOWEVER
THE
REMAINING
SOURCE
BYTES
ARE
NOT
ALIGNED
ON
BYTE
BOUNDARIES
REPLACING
MOVDQA
WITH
MOVDQU
FOR
LOADS
WILL
INEVITABLY
RUN
INTO
CACHE
LINE
SPLITS
TO
ACHIEVE
HIGHER
DATA
THROUGHPUT
THAN
LOADING
UNALIGNED
BYTES
WITH
MOVDQU
THE
BYTES
OF
DATA
TARGETED
TO
EACH
OF
BYTES
OF
ALIGNED
DESTINATION
ADDRESSES
CAN
BE
ASSEMBLED
USING
TWO
ALIGNED
LOADS
THIS
TECHNIQUE
IS
ILLUSTRATED
IN
FIGURE
FIGURE
A
TECHNIQUE
TO
AVOID
CACHELINE
SPLIT
LOADS
IN
REVERSE
MEMORY
COPY
USING
TWO
ALIGNED
LOADS
CONVERTING
FROM
BIT
TO
BIT
SIMD
INTEGERS
DEFINES
A
SUPERSET
OF
BIT
INTEGER
INSTRUCTIONS
CURRENTLY
AVAILABLE
IN
MMX
TECHNOLOGY
THE
OPERATION
OF
THE
EXTENDED
INSTRUCTIONS
REMAINS
THE
SUPERSET
SIMPLY
OPERATES
ON
DATA
THAT
IS
TWICE
AS
WIDE
THIS
SIMPLIFIES
PORTING
OF
BIT
INTEGER
APPLI
CATIONS
HOWEVER
THERE
ARE
FEW
CONSIDERATIONS
COMPUTATION
INSTRUCTIONS
WHICH
USE
A
MEMORY
OPERAND
THAT
MAY
NOT
BE
ALIGNED
TO
A
BYTE
BOUNDARY
MUST
BE
REPLACED
WITH
AN
UNALIGNED
BIT
LOAD
MOVDQU
FOLLOWED
BY
THE
SAME
COMPUTATION
OPERATION
THAT
USES
INSTEAD
REGISTER
OPERANDS
USE
OF
BIT
INTEGER
COMPUTATION
INSTRUCTIONS
WITH
MEMORY
OPERANDS
THAT
ARE
NOT
BYTE
ALIGNED
WILL
RESULT
IN
A
GP
UNALIGNED
BIT
LOADS
AND
STORES
ARE
NOT
AS
EFFICIENT
AS
CORRESPONDING
ALIGNED
VERSIONS
THIS
FACT
CAN
REDUCE
THE
PERFORMANCE
GAINS
WHEN
USING
THE
BIT
SIMD
INTEGER
EXTENSIONS
GENERAL
GUIDELINES
ON
THE
ALIGNMENT
OF
MEMORY
OPERANDS
ARE
THE
GREATEST
PERFORMANCE
GAINS
CAN
BE
ACHIEVED
WHEN
ALL
MEMORY
STREAMS
ARE
BYTE
ALIGNED
REASONABLE
PERFORMANCE
GAINS
ARE
POSSIBLE
IF
ROUGHLY
HALF
OF
ALL
MEMORY
STREAMS
ARE
BYTE
ALIGNED
AND
THE
OTHER
HALF
ARE
NOT
LITTLE
OR
NO
PERFORMANCE
GAIN
MAY
RESULT
IF
ALL
MEMORY
STREAMS
ARE
NOT
ALIGNED
TO
BYTES
IN
THIS
CASE
USE
OF
THE
BIT
SIMD
INTEGER
INSTRUCTIONS
MAY
BE
PREFERABLE
LOOP
COUNTERS
NEED
TO
BE
UPDATED
BECAUSE
EACH
BIT
INTEGER
INSTRUCTION
OPERATES
ON
TWICE
THE
AMOUNT
OF
DATA
AS
ITS
BIT
INTEGER
COUNTERPART
EXTENSION
OF
THE
PSHUFW
INSTRUCTION
SHUFFLE
WORD
ACROSS
BIT
INTEGER
OPERAND
ACROSS
A
FULL
BIT
OPERAND
IS
EMULATED
BY
A
COMBINATION
OF
THE
FOLLOWING
INSTRUCTIONS
PSHUFHW
PSHUFLW
AND
PSHUFD
USE
OF
THE
BIT
SHIFT
BY
BIT
INSTRUCTIONS
PSRLQ
PSLLQ
ARE
EXTENDED
TO
BITS
BY
USE
OF
PSRLQ
AND
PSLLQ
ALONG
WITH
MASKING
LOGIC
OPERATIONS
A
CODE
SEQUENCE
REWRITTEN
TO
USE
THE
PSRLDQ
AND
PSLLDQ
INSTRUCTIONS
SHIFT
DOUBLE
QUAD
WORD
OPERAND
BY
BYTES
SIMD
OPTIMIZATIONS
AND
MICROARCHITECTURES
PENTIUM
M
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
HAVE
A
DIFFERENT
MICROAR
CHITECTURE
THAN
INTEL
NETBURST
MICROARCHITECTURE
THE
FOLLOWING
SECTIONS
DISCUSS
OPTI
MIZING
SIMD
CODE
THAT
TARGETS
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
LDDQU
BEHAVES
IDENTICALLY
TO
MOVDQU
BY
LOADING
BYTES
OF
DATA
IRRESPECTIVE
OF
ADDRESS
ALIGNMENT
PACKED
INTEGER
VERSUS
MMX
INSTRUCTIONS
IN
GENERAL
BIT
SIMD
INTEGER
INSTRUCTIONS
SHOULD
BE
FAVORED
OVER
BIT
MMX
INSTRUCTIONS
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
THIS
IS
BECAUSE
IMPROVED
DECODER
BANDWIDTH
AND
MORE
EFFICIENT
OP
FLOWS
RELATIVE
TO
THE
PENTIUM
M
PROCESSOR
WIDER
WIDTH
OF
THE
XMM
REGISTERS
CAN
BENEFIT
CODE
THAT
IS
LIMITED
BY
EITHER
DECODER
BANDWIDTH
OR
EXECUTION
LATENCY
XMM
REGISTERS
CAN
PROVIDE
TWICE
THE
SPACE
TO
STORE
DATA
FOR
IN
FLIGHT
EXECUTION
WIDER
XMM
REGISTERS
CAN
FACILITATE
LOOP
UNROLLING
OR
IN
REDUCING
LOOP
OVERHEAD
BY
HALVING
THE
NUMBER
OF
LOOP
ITERATIONS
IN
MICROARCHITECTURES
PRIOR
TO
INTEL
CORE
MICROARCHITECTURE
EXECUTION
THROUGHPUT
OF
BIT
SIMD
INTEGRATION
OPERATIONS
IS
BASICALLY
THE
SAME
AS
BIT
MMX
OPERATIONS
SOME
SHUFFLE
UNPACK
SHIFT
OPERATIONS
DO
NOT
BENEFIT
FROM
THE
FRONT
END
IMPROVE
MENTS
THE
NET
IMPACT
OF
USING
BIT
SIMD
INTEGER
INSTRUCTION
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
IS
LIKELY
TO
BE
SLIGHTLY
POSITIVE
OVERALL
BUT
THERE
MAY
BE
A
FEW
SITUATIONS
WHERE
THEIR
USE
WILL
GENERATE
AN
UNFAVORABLE
PERFORMANCE
IMPACT
INTEL
CORE
MICROARCHITECTURE
GENERALLY
EXECUTES
BIT
SIMD
INSTRUCTIONS
MORE
EFFI
CIENTLY
THAN
PREVIOUS
MICROARCHITECTURES
IN
TERMS
OF
LATENCY
AND
THROUGHPUT
MANY
OF
THE
LIMITATIONS
SPECIFIC
TO
INTEL
CORE
DUO
INTEL
CORE
SOLO
PROCESSORS
DO
NOT
APPLY
THE
SAME
IS
TRUE
OF
INTEL
CORE
MICROARCHITECTURE
RELATIVE
TO
INTEL
NETBURST
MICROARCHITEC
TURES
ENHANCED
INTEL
CORE
MICROARCHITECTURE
PROVIDES
EVEN
MORE
POWERFUL
BIT
SIMD
EXECUTION
CAPABILITIES
AND
MORE
COMPREHENSIVE
SETS
OF
SIMD
INSTRUCTION
EXTENSIONS
THAN
INTEL
CORE
MICROARCHITECTURE
THE
INTEGER
SIMD
INSTRUCTIONS
OFFERED
BY
OPERATES
ON
BIT
XMM
REGISTER
ONLY
ALL
OF
THESE
HIGHLY
ENCOURAGES
SOFTWARE
TO
FAVOR
BIT
VECTORIZABLE
CODE
TO
TAKE
ADVANTAGE
OF
PROCESSORS
BASED
ON
ENHANCED
INTEL
CORE
MICROARCHITECTURE
AND
INTEL
CORE
MICROARCHITECTURE
WORK
AROUND
FOR
FALSE
DEPENDENCY
ISSUE
IN
PROCESSOR
BASED
ON
INTEL
MICROARCHITECTURE
NEHALEM
USING
PMOVSX
AND
PMOVZX
INSTRUCTIONS
TO
COMBINE
DATA
TYPE
CONVERSION
AND
DATA
MOVEMENT
IN
THE
SAME
INSTRUCTION
WILL
CREATE
A
FALSE
DEPENDENCY
DUE
TO
HARDWARE
CAUSES
A
SIMPLE
WORK
AROUND
TO
AVOID
THE
FALSE
DEPENDENCY
ISSUE
IS
TO
USE
PMOVSX
PMOVZX
INSTRUC
TION
SOLELY
FOR
DATA
TYPE
CONVERSION
AND
ISSUE
SEPARATE
INSTRUCTION
TO
MOVE
DATA
TO
DESTINATION
OR
FROM
ORIGIN
EXAMPLE
PMOVSX
PMOVZX
WORK
AROUND
TO
AVOID
FALSE
DEPENDENCY
TUNING
PARTIALLY
VECTORIZABLE
CODE
SOME
LOOP
STRUCTURED
CODE
ARE
MORE
DIFFICULT
TO
VECTORIZE
THAN
OTHERS
EXAMPLE
DEPICTS
A
LOOP
CARRYING
OUT
TABLE
LOOK
UP
OPERATION
AND
SOME
ARITHMETIC
COMPUTATION
EXAMPLE
TABLE
LOOK
UP
OPERATIONS
IN
C
CODE
INTEGER
INPUT
ARRAYS
POUT
INTEGER
OUTPUT
ARRAY
COUNT
SIZE
OF
ARRAY
LOOKUPTABLE
INTEGER
VALUES
SIZE
OF
THE
LOOK
UP
TABLE
FOR
UNSIGNED
I
I
COUNT
I
POUT
I
LOOKUPTABLE
I
I
ALTHOUGH
SOME
OF
THE
ARITHMETIC
COMPUTATIONS
AND
INPUT
OUTPUT
TO
DATA
ARRAY
IN
EACH
ITERATION
CAN
BE
EASILY
VECTORIZABLE
BUT
THE
TABLE
LOOK
UP
VIA
AN
INDEX
ARRAY
IS
NOT
THIS
CREATES
DIFFERENT
APPROACHES
TO
TUNING
A
COMPILER
CAN
TAKE
A
SCALAR
APPROACH
TO
EXECUTE
EACH
ITERATION
SEQUENTIALLY
HAND
TUNING
OF
SUCH
LOOPS
MAY
USE
A
COUPLE
OF
DIFFERENT
TECHNIQUES
TO
HANDLE
THE
NON
VECTORIZABLE
TABLE
LOOK
UP
OPERATION
ONE
VECTORIZATION
TECHNIQUE
IS
TO
LOAD
THE
INPUT
DATA
FOR
FOUR
ITERATION
AT
ONCE
THEN
USE
INSTRUCTION
TO
SHIFT
OUT
INDIVIDUAL
INDEX
OUT
OF
AN
XMM
REGISTER
TO
CARRY
OUT
TABLE
LOOK
UP
SEQUENTIALLY
THE
SHIFT
TECHNIQUE
IS
DEPICTED
BY
EXAMPLE
ANOTHER
TECH
NIQUE
IS
TO
USE
PEXTRD
IN
TO
EXTRACT
THE
INDEX
FROM
AN
XMM
DIRECTLY
AND
THEN
CARRY
OUT
TABLE
LOOK
UP
SEQUENTIALLY
THE
PEXTRD
TECHNIQUE
IS
DEPICTED
BY
EXAMPLE
EXAMPLE
SHIFT
TECHNIQUES
ON
NON
VECTORIZABLE
TABLE
LOOK
UP
INT
MODULO
INT
C
MOV
ESI
MOV
EBX
POUT
MOV
ECX
COUNT
MOV
EDX
PLOOKUPTABLEPTR
MOVAPS
MODULO
MOVAPS
C
LLOOP
VECTORIZABLE
MULTIPLE
CONSECUTIVE
DATA
ACCESSES
MOVAPS
ESI
READ
INDICES
FROM
MOVAPS
PAND
TABLESIZE
TABLE
LOOK
UP
IS
NOT
VECTORIZABLE
SHIFT
OUT
ONE
DATA
ELEMENT
TO
LOOK
UP
TABLE
ONE
BY
ONE
MOVD
EAX
GET
FIRST
INDEX
MOVD
WORD
PTR
EDX
EAX
PSRLDQ
MOVD
EAX
GET
INDEX
MOVD
WORD
PTR
EDX
EAX
PSRLDQ
MOVD
EAX
GET
MOVD
WORD
PTR
EDX
EAX
PSRLDQ
MOVD
EAX
GET
FOURTH
INDEX
MOVD
WORD
PTR
EDX
EAX
END
OF
SCALAR
PART
PACKING
MOVLHPS
PSLLQ
MOVLHPS
ORPS
END
OF
PACKING
CONTINUE
EXAMPLE
SHIFT
TECHNIQUES
ON
NON
VECTORIZABLE
TABLE
LOOK
UP
CONTD
VECTORIZABLE
COMPUTATION
OPERATIONS
PADDD
PADDD
POR
ANDPS
MOD
MOVAPS
EBX
END
OF
VECTORIZABLE
OPERATION
ADD
EBX
ADD
ESI
ADD
EDI
SUB
ECX
TEST
ECX
ECX
JNE
LLOOP
EXAMPLE
PEXTRD
TECHNIQUES
ON
NON
VECTORIZABLE
TABLE
LOOK
UP
INT
MODULO
INT
C
MOV
ESI
MOV
EBX
POUT
MOV
ECX
COUNT
MOV
EDX
PLOOKUPTABLEPTR
MOVAPS
MODULO
MOVAPS
C
LLOOP
VECTORIZABLE
MULTIPLE
CONSECUTIVE
DATA
ACCESSES
MOVAPS
ESI
READ
INDICES
FROM
MOVAPS
PAND
TABLESIZE
TABLE
LOOK
UP
IS
NOT
VECTORIZABLE
EXTRACT
ONE
DATA
ELEMENT
TO
LOOK
UP
TABLE
ONE
BY
ONE
MOVD
EAX
GET
FIRST
INDEX
MOV
EAX
EDX
EAX
MOVD
EAX
CONTINUE
EXAMPLE
PEXTRD
TECHNIQUES
ON
NON
VECTORIZABLE
TABLE
LOOK
UP
CONTD
PEXTRD
EAX
EXTRACT
INDEX
MOV
EAX
EDX
EAX
PINSRD
EAX
PEXTRD
EAX
EXTRACT
INDEX
MOV
EAX
EDX
EAX
PINSRD
EAX
PEXTRD
EAX
EXTRACT
INDEX
MOV
EAX
EDX
EAX
PINSRD
EAX
END
OF
SCALAR
PART
PACKING
NOT
NEEDED
VECTORIZABLE
OPERATIONS
PADDD
PADDD
POR
ANDPS
MOD
MOVAPS
EBX
ADD
EBX
ADD
ESI
ADD
EDI
SUB
ECX
TEST
ECX
ECX
JNE
LLOOP
THE
EFFECTIVENESS
OF
THESE
TWO
HAND
TUNING
TECHNIQUES
ON
PARTIALLY
VECTORIZABLE
CODE
DEPENDS
ON
THE
RELATIVE
COST
OF
TRANSFORMING
DATA
LAYOUT
FORMAT
USING
VARIOUS
FORMS
OF
PACK
AND
UNPACK
INSTRUCTIONS
THE
SHIFT
TECHNIQUE
REQUIRES
ADDITIONAL
INSTRUCTIONS
TO
PACK
SCALAR
TABLE
VALUES
INTO
AN
XMM
TO
TRANSITION
INTO
VECTORIZED
ARITHMETIC
COMPUTATIONS
THE
NET
PERFORMANCE
GAIN
OR
LOSS
OF
THIS
TECHNIQUE
WILL
VARY
WITH
THE
CHARACTERISTICS
OF
DIFFERENT
MICROARCHITEC
TURES
THE
ALTERNATE
PEXTRD
TECHNIQUE
USES
LESS
INSTRUCTION
TO
EXTRACT
EACH
INDEX
DOES
NOT
REQUIRE
EXTRANEOUS
PACKING
OF
SCALAR
DATA
INTO
PACKED
SIMD
DATA
FORMAT
TO
BEGIN
VECTORIZED
ARITHMETIC
COMPUTATION
CHAPTER
OPTIMIZING
FOR
SIMD
FLOATING
POINT
APPLICATIONS
THIS
CHAPTER
DISCUSSES
RULES
FOR
OPTIMIZING
FOR
THE
SINGLE
INSTRUCTION
MULTIPLE
DATA
SIMD
FLOATING
POINT
INSTRUCTIONS
AVAILABLE
IN
SSE
AND
THE
CHAPTER
ALSO
PROVIDES
EXAMPLES
THAT
ILLUSTRATE
THE
OPTIMIZATION
TECHNIQUES
FOR
SINGLE
PRECISION
AND
DOUBLE
PRECISION
SIMD
FLOATING
POINT
APPLICATIONS
GENERAL
RULES
FOR
SIMD
FLOATING
POINT
CODE
THE
RULES
AND
SUGGESTIONS
IN
THIS
SECTION
HELP
OPTIMIZE
FLOATING
POINT
CODE
CONTAINING
SIMD
FLOATING
POINT
INSTRUCTIONS
GENERALLY
IT
IS
IMPORTANT
TO
UNDERSTAND
AND
BALANCE
PORT
UTILIZATION
TO
CREATE
EFFICIENT
SIMD
FLOATING
POINT
CODE
BASIC
RULES
AND
SUGGES
TIONS
INCLUDE
THE
FOLLOWING
FOLLOW
ALL
GUIDELINES
IN
CHAPTER
AND
CHAPTER
MASK
EXCEPTIONS
TO
ACHIEVE
HIGHER
PERFORMANCE
WHEN
EXCEPTIONS
ARE
UNMASKED
SOFTWARE
PERFORMANCE
IS
SLOWER
UTILIZE
THE
FLUSH
TO
ZERO
AND
DENORMALS
ARE
ZERO
MODES
FOR
HIGHER
PERFORMANCE
TO
AVOID
THE
PENALTY
OF
DEALING
WITH
DENORMALS
AND
UNDERFLOWS
USE
THE
RECIPROCAL
INSTRUCTIONS
FOLLOWED
BY
ITERATION
FOR
INCREASED
ACCURACY
THESE
INSTRUCTIONS
YIELD
REDUCED
ACCURACY
BUT
EXECUTE
MUCH
FASTER
NOTE
THE
FOLLOWING
IF
REDUCED
ACCURACY
IS
ACCEPTABLE
USE
THEM
WITH
NO
ITERATION
IF
NEAR
FULL
ACCURACY
IS
NEEDED
USE
A
NEWTON
RAPHSON
ITERATION
IF
FULL
ACCURACY
IS
NEEDED
THEN
USE
DIVIDE
AND
SQUARE
ROOT
WHICH
PROVIDE
MORE
ACCURACY
BUT
SLOW
DOWN
PERFORMANCE
PLANNING
CONSIDERATIONS
WHETHER
ADAPTING
AN
EXISTING
APPLICATION
OR
CREATING
A
NEW
ONE
USING
SIMD
FLOATING
POINT
INSTRUCTIONS
TO
ACHIEVE
OPTIMUM
PERFORMANCE
GAIN
REQUIRES
PROGRAMMERS
TO
CONSIDER
SEVERAL
ISSUES
IN
GENERAL
WHEN
CHOOSING
CANDIDATES
FOR
OPTIMIZATION
LOOK
FOR
CODE
SEGMENTS
THAT
ARE
COMPUTATIONALLY
INTENSIVE
AND
FLOATING
POINT
INTENSIVE
ALSO
CONSIDER
EFFICIENT
USE
OF
THE
CACHE
ARCHITECTURE
THE
SECTIONS
THAT
FOLLOW
ANSWER
THE
QUESTIONS
THAT
SHOULD
BE
RAISED
BEFORE
IMPLE
MENTATION
CAN
DATA
LAYOUT
BE
ARRANGED
TO
INCREASE
PARALLELISM
OR
CACHE
UTILIZATION
WHICH
PART
OF
THE
CODE
BENEFITS
FROM
SIMD
FLOATING
POINT
INSTRUCTIONS
IS
THE
CURRENT
ALGORITHM
THE
MOST
APPROPRIATE
FOR
SIMD
FLOATING
POINT
INSTRUC
TIONS
IS
THE
CODE
FLOATING
POINT
INTENSIVE
DO
EITHER
SINGLE
PRECISION
FLOATING
POINT
OR
DOUBLE
PRECISION
FLOATING
POINT
COMPUTATIONS
PROVIDE
ENOUGH
RANGE
AND
PRECISION
DOES
THE
RESULT
OF
COMPUTATION
AFFECTED
BY
ENABLING
FLUSH
TO
ZERO
OR
DENORMALS
TO
ZERO
MODES
IS
THE
DATA
ARRANGED
FOR
EFFICIENT
UTILIZATION
OF
THE
SIMD
FLOATING
POINT
REGISTERS
IS
THIS
APPLICATION
TARGETED
FOR
PROCESSORS
WITHOUT
SIMD
FLOATING
POINT
INSTRUC
TIONS
SEE
ALSO
SECTION
CONSIDERATIONS
FOR
CODE
CONVERSION
TO
SIMD
PROGRAMMING
USING
SIMD
FLOATING
POINT
WITH
FLOATING
POINT
BECAUSE
THE
XMM
REGISTERS
USED
FOR
SIMD
FLOATING
POINT
COMPUTATIONS
ARE
SEPARATE
REGISTERS
AND
ARE
NOT
MAPPED
TO
THE
EXISTING
FLOATING
POINT
STACK
SIMD
FLOATING
POINT
CODE
CAN
BE
MIXED
WITH
FLOATING
POINT
OR
BIT
SIMD
INTEGER
CODE
WITH
INTEL
CORE
MICROARCHITECTURE
BIT
SIMD
INTEGER
INSTRUCTIONS
PROVIDES
SUBSTANTIALLY
HIGHER
EFFICIENCY
THAN
BIT
SIMD
INTEGER
INSTRUCTIONS
SOFTWARE
SHOULD
FAVOR
USING
SIMD
FLOATING
POINT
AND
INTEGER
SIMD
INSTRUCTIONS
WITH
XMM
REGISTERS
WHERE
POSSIBLE
SCALAR
FLOATING
POINT
CODE
THERE
ARE
SIMD
FLOATING
POINT
INSTRUCTIONS
THAT
OPERATE
ONLY
ON
THE
LOWEST
ORDER
ELEMENT
IN
THE
SIMD
REGISTER
THESE
INSTRUCTIONS
ARE
KNOWN
AS
SCALAR
INSTRUCTIONS
THEY
ALLOW
THE
XMM
REGISTERS
TO
BE
USED
FOR
GENERAL
PURPOSE
FLOATING
POINT
COMPUTA
TIONS
IN
TERMS
OF
PERFORMANCE
SCALAR
FLOATING
POINT
CODE
CAN
BE
EQUIVALENT
TO
OR
EXCEED
FLOATING
POINT
CODE
AND
HAS
THE
FOLLOWING
ADVANTAGES
SIMD
FLOATING
POINT
CODE
USES
A
FLAT
REGISTER
MODEL
WHEREAS
FLOATING
POINT
CODE
USES
A
STACK
MODEL
USING
SCALAR
FLOATING
POINT
CODE
ELIMINATES
THE
NEED
TO
USE
FXCH
INSTRUCTIONS
THESE
HAVE
PERFORMANCE
LIMITS
ON
THE
INTEL
PENTIUM
PROCESSOR
MIXING
WITH
MMX
TECHNOLOGY
CODE
WITHOUT
PENALTY
FLUSH
TO
ZERO
MODE
SHORTER
LATENCIES
THAN
FLOATING
POINT
WHEN
USING
SCALAR
FLOATING
POINT
INSTRUCTIONS
IT
IS
NOT
NECESSARY
TO
ENSURE
THAT
THE
DATA
APPEARS
IN
VECTOR
FORM
HOWEVER
THE
OPTIMIZATIONS
REGARDING
ALIGNMENT
SCHED
ULING
INSTRUCTION
SELECTION
AND
OTHER
OPTIMIZATIONS
COVERED
IN
CHAPTER
AND
CHAPTER
SHOULD
BE
OBSERVED
DATA
ALIGNMENT
SIMD
FLOATING
POINT
DATA
IS
BYTE
ALIGNED
REFERENCING
UNALIGNED
BIT
SIMD
FLOATING
POINT
DATA
WILL
RESULT
IN
AN
EXCEPTION
UNLESS
MOVUPS
OR
MOVUPD
MOVE
UNALIGNED
PACKED
SINGLE
OR
UNALIGNED
PACKED
DOUBLE
IS
USED
THE
UNALIGNED
INSTRUC
TIONS
USED
ON
ALIGNED
OR
UNALIGNED
DATA
WILL
ALSO
SUFFER
A
PERFORMANCE
PENALTY
RELATIVE
TO
ALIGNED
ACCESSES
SEE
ALSO
SECTION
STACK
AND
DATA
ALIGNMENT
DATA
ARRANGEMENT
BECAUSE
SSE
AND
INCORPORATE
SIMD
ARCHITECTURE
ARRANGING
DATA
TO
FULLY
USE
THE
SIMD
REGISTERS
PRODUCES
OPTIMUM
PERFORMANCE
THIS
IMPLIES
CONTIGUOUS
DATA
FOR
PROCESSING
WHICH
LEADS
TO
FEWER
CACHE
MISSES
CORRECT
DATA
ARRANGEMENT
CAN
POTEN
TIALLY
QUADRUPLE
DATA
THROUGHPUT
WHEN
USING
SSE
OR
DOUBLE
THROUGHPUT
WHEN
USING
PERFORMANCE
GAINS
CAN
OCCUR
BECAUSE
FOUR
DATA
ELEMENTS
CAN
BE
LOADED
WITH
BIT
LOAD
INSTRUCTIONS
INTO
XMM
REGISTERS
USING
SSE
MOVAPS
SIMILARLY
TWO
DATA
ELEMENTS
CAN
LOADED
WITH
BIT
LOAD
INSTRUCTIONS
INTO
XMM
REGISTERS
USING
MOVAPD
REFER
TO
THE
SECTION
STACK
AND
DATA
ALIGNMENT
FOR
DATA
ARRANGEMENT
RECOM
MENDATIONS
DUPLICATING
AND
PADDING
TECHNIQUES
OVERCOME
MISALIGNMENT
PROBLEMS
THAT
OCCUR
IN
SOME
DATA
STRUCTURES
AND
ARRANGEMENTS
THIS
INCREASES
THE
DATA
SPACE
BUT
AVOIDS
PENALTIES
FOR
MISALIGNED
DATA
ACCESS
FOR
SOME
APPLICATIONS
FOR
EXAMPLE
GEOMETRY
TRADITIONAL
DATA
ARRANGEMENT
REQUIRES
SOME
CHANGES
TO
FULLY
UTILIZE
THE
SIMD
REGISTERS
AND
PARALLEL
TECHNIQUES
TRADITIONALLY
THE
DATA
LAYOUT
HAS
BEEN
AN
ARRAY
OF
STRUCTURES
AOS
TO
FULLY
UTILIZE
THE
SIMD
REGISTERS
IN
SUCH
APPLICATIONS
A
NEW
DATA
LAYOUT
HAS
BEEN
PROPOSED
A
STRUC
TURE
OF
ARRAYS
SOA
RESULTING
IN
MORE
OPTIMIZED
PERFORMANCE
VERTICAL
VERSUS
HORIZONTAL
COMPUTATION
THE
MAJORITY
OF
THE
FLOATING
POINT
ARITHMETIC
INSTRUCTIONS
IN
SSE
PROVIDE
GREATER
PERFORMANCE
GAIN
ON
VERTICAL
DATA
PROCESSING
FOR
PARALLEL
DATA
ELEMENTS
THIS
MEANS
EACH
ELEMENT
OF
THE
DESTINATION
IS
THE
RESULT
OF
AN
ARITHMETIC
OPERATION
PERFORMED
FROM
THE
SOURCE
ELEMENTS
IN
THE
SAME
VERTICAL
POSITION
FIGURE
TO
SUPPLEMENT
THESE
HOMOGENEOUS
ARITHMETIC
OPERATIONS
ON
PARALLEL
DATA
ELEMENTS
SSE
AND
PROVIDES
DATA
MOVEMENT
INSTRUCTIONS
E
G
SHUFPS
UNPCKLPS
UNPCKHPS
MOVLHPS
MOVHLPS
ETC
THAT
FACILITATE
MOVING
DATA
ELEMENTS
HORIZONTALLY
OP
OP
X
OP
FIGURE
HOMOGENEOUS
OPERATION
ON
PARALLEL
DATA
ELEMENTS
THE
ORGANIZATION
OF
STRUCTURED
DATA
HAVE
A
SIGNIFICANT
IMPACT
ON
SIMD
PROGRAMMING
EFFICIENCY
AND
PERFORMANCE
THIS
CAN
BE
ILLUSTRATED
USING
TWO
COMMON
TYPE
OF
DATA
STRUCTURE
ORGANIZATIONS
ARRAY
OF
STRUCTURE
THIS
REFERS
TO
THE
ARRANGEMENT
OF
AN
ARRAY
OF
DATA
STRUCTURES
WITHIN
THE
DATA
STRUCTURE
EACH
MEMBER
IS
A
SCALAR
THIS
IS
SHOWN
IN
FIGURE
TYPICALLY
A
REPETITIVE
SEQUENCE
OF
COMPUTATION
IS
APPLIED
TO
EACH
ELEMENT
OF
AN
ARRAY
I
E
A
DATA
STRUCTURE
COMPUTATIONAL
SEQUENCE
FOR
THE
SCALAR
MEMBERS
OF
THE
STRUCTURE
IS
LIKELY
TO
BE
NON
HOMOGENEOUS
WITHIN
EACH
ITERATION
AOS
IS
GENERALLY
ASSOCIATED
WITH
A
HORIZONTAL
COMPUTATION
MODEL
X
Y
Z
W
FIGURE
HORIZONTAL
COMPUTATION
MODEL
STRUCTURE
OF
ARRAY
HERE
EACH
MEMBER
OF
THE
DATA
STRUCTURE
IS
AN
ARRAY
EACH
ELEMENT
OF
THE
ARRAY
IS
A
SCALAR
THIS
IS
SHOWN
TABLE
REPETITIVE
COMPUTA
TIONAL
SEQUENCE
IS
APPLIED
TO
SCALAR
ELEMENTS
AND
HOMOGENEOUS
OPERATION
CAN
BE
EASILY
ACHIEVED
ACROSS
CONSECUTIVE
ITERATIONS
WITHIN
THE
SAME
STRUCTURAL
MEMBER
CONSEQUENTLY
SOA
IS
GENERALLY
AMENABLE
TO
THE
VERTICAL
COMPUTATION
MODEL
TABLE
SOA
FORM
OF
REPRESENTING
VERTICES
DATA
VX
ARRAY
XN
VY
ARRAY
YN
VZ
ARRAY
ZN
VW
ARRAY
WN
USING
SIMD
INSTRUCTIONS
WITH
VERTICAL
COMPUTATION
ON
SOA
ARRANGEMENT
CAN
ACHIEVE
HIGHER
EFFICIENCY
AND
PERFORMANCE
THAN
AOS
AND
HORIZONTAL
COMPUTATION
THIS
CAN
BE
SEEN
WITH
DOT
PRODUCT
OPERATION
ON
VECTORS
THE
DOT
PRODUCT
OPERATION
ON
SOA
ARRANGEMENT
IS
SHOWN
IN
FIGURE
X4
FX
FX
FX
FX
FY
FY
FY
FY
FZ
FZ
FZ
FZ
FW
FW
FW
FW
FIGURE
DOT
PRODUCT
OPERATION
EXAMPLE
SHOWS
HOW
ONE
RESULT
WOULD
BE
COMPUTED
FOR
SEVEN
INSTRUCTIONS
IF
THE
DATA
WERE
ORGANIZED
AS
AOS
AND
USING
SSE
ALONE
FOUR
RESULTS
WOULD
REQUIRE
INSTRUCTIONS
EXAMPLE
PSEUDOCODE
FOR
HORIZONTAL
XYZ
AOS
COMPUTATION
MULPS
X
X
Y
Y
Z
Z
MOVAPS
REG
REG
MOVE
SINCE
NEXT
STEPS
OVERWRITE
SHUFPS
GET
B
A
D
C
FROM
A
B
C
D
ADDPS
GET
A
B
A
B
C
D
C
D
MOVAPS
REG
REG
MOVE
SHUFPS
GET
C
D
C
D
A
B
A
B
FROM
PRIOR
ADDPS
ADDPS
GET
A
B
C
D
A
B
C
D
A
B
C
D
A
B
C
D
NOW
CONSIDER
THE
CASE
WHEN
THE
DATA
IS
ORGANIZED
AS
SOA
EXAMPLE
DEMON
STRATES
HOW
FOUR
RESULTS
ARE
COMPUTED
FOR
FIVE
INSTRUCTIONS
EXAMPLE
PSEUDOCODE
FOR
VERTICAL
XXXX
YYYY
ZZZZ
SOA
COMPUTATION
MULPS
X
X
FOR
ALL
X
COMPONENTS
OF
VERTICES
MULPS
Y
Y
FOR
ALL
Y
COMPONENTS
OF
VERTICES
MULPS
Z
Z
FOR
ALL
Z
COMPONENTS
OF
VERTICES
ADDPS
X
X
Y
Y
ADDPS
X
X
Y
Y
Z
Z
FOR
THE
MOST
EFFICIENT
USE
OF
THE
FOUR
COMPONENT
WIDE
REGISTERS
REORGANIZING
THE
DATA
INTO
THE
SOA
FORMAT
YIELDS
INCREASED
THROUGHPUT
AND
HENCE
MUCH
BETTER
PERFOR
MANCE
FOR
THE
INSTRUCTIONS
USED
AS
SEEN
FROM
THIS
SIMPLE
EXAMPLE
VERTICAL
COMPUTATION
CAN
YIELD
USE
OF
THE
AVAILABLE
SIMD
REGISTERS
TO
PRODUCE
FOUR
RESULTS
THE
RESULTS
MAY
VARY
FOR
OTHER
SITU
ATIONS
IF
THE
DATA
STRUCTURES
ARE
REPRESENTED
IN
A
FORMAT
THAT
IS
NOT
FRIENDLY
TO
VERTICAL
COMPUTATION
IT
CAN
BE
REARRANGED
ON
THE
FLY
TO
FACILITATE
BETTER
UTILIZATION
OF
THE
SIMD
REGISTERS
THIS
OPERATION
IS
REFERRED
TO
AS
SWIZZLING
OPERATION
AND
THE
REVERSE
OPERATION
IS
REFERRED
TO
AS
DESWIZZLING
DATA
SWIZZLING
SWIZZLING
DATA
FROM
SOA
TO
AOS
FORMAT
CAN
APPLY
TO
A
NUMBER
OF
APPLICATION
DOMAINS
INCLUDING
GEOMETRY
VIDEO
AND
IMAGING
TWO
DIFFERENT
SWIZZLING
TECHNIQUES
CAN
BE
ADAPTED
TO
HANDLE
FLOATING
POINT
AND
INTEGER
DATA
EXAMPLE
ILLUSTRATES
A
SWIZZLE
FUNCTION
THAT
USES
SHUFPS
MOVLHPS
MOVHLPS
INSTRUCTIONS
EXAMPLE
SWIZZLING
DATA
USING
SHUFPS
MOVLHPS
MOVHLPS
EXAMPLE
SHOWS
A
SIMILAR
DATA
SWIZZLING
ALGORITHM
USING
SIMD
INSTRUCTIONS
IN
THE
INTEGER
DOMAIN
EXAMPLE
SWIZZLING
DATA
USING
UNPCKXXX
INSTRUCTIONS
THE
TECHNIQUE
IN
EXAMPLE
LOADING
BYTES
USING
SHUFPS
AND
COPYING
HALVES
OF
XMM
REGISTERS
IS
PREFERABLE
OVER
AN
ALTERNATE
APPROACH
OF
LOADING
HALVES
OF
EACH
VECTOR
USING
MOVLPS
MOVHPS
ON
NEWER
MICROARCHITECTURES
THIS
IS
BECAUSE
LOADING
BYTES
USING
MOVLPS
MOVHPS
CAN
CREATE
CODE
DEPENDENCY
AND
REDUCE
THE
THROUGHPUT
OF
THE
EXECUTION
ENGINE
THE
PERFORMANCE
CONSIDERATIONS
OF
EXAMPLE
AND
EXAMPLE
OFTEN
DEPENDS
ON
THE
CHARACTERISTICS
OF
EACH
MICROARCHITECTURE
FOR
EXAMPLE
IN
INTEL
CORE
MICROARCHI
TECTURE
EXECUTING
A
SHUFPS
TEND
TO
BE
SLOWER
THAN
A
PUNPCKXXX
INSTRUCTION
IN
ENHANCED
INTEL
CORE
MICROARCHITECTURE
SHUFPS
AND
PUNPCKXXX
INSTRUCTION
ALL
EXECUTES
WITH
CYCLE
THROUGHPUT
DUE
TO
THE
BIT
SHUFFLE
EXECUTION
UNIT
THEN
THE
NEXT
IMPORTANT
CONSIDERATION
IS
THAT
THERE
IS
ONLY
ONE
PORT
THAT
CAN
EXECUTE
PUNPCKXXX
VS
MOVLHPS
MOVHLPS
CAN
EXECUTE
ON
MULTIPLE
PORTS
THE
PERFORMANCE
OF
BOTH
TECHNIQUES
IMPROVES
ON
INTEL
CORE
MICROARCHITECTURE
OVER
PREVIOUS
MICROAR
CHITECTURES
DUE
TO
PORTS
FOR
EXECUTING
SIMD
INSTRUCTIONS
BOTH
TECHNIQUES
IMPROVES
FURTHER
ON
ENHANCED
INTEL
CORE
MICROARCHITECTURE
DUE
TO
THE
BIT
SHUFFLE
UNIT
DATA
DESWIZZLING
IN
THE
DESWIZZLE
OPERATION
WE
WANT
TO
ARRANGE
THE
SOA
FORMAT
BACK
INTO
AOS
FORMAT
SO
THE
XXXX
YYYY
ZZZZ
ARE
REARRANGED
AND
STORED
IN
MEMORY
AS
XYZ
EXAMPLE
ILLUSTRATES
ONE
DESWIZZLE
FUNCTION
FOR
FLOATING
POINT
DATA
EXAMPLE
DESWIZZLING
SINGLE
PRECISION
SIMD
DATA
VOID
IN
OUT
ASM
MOV
ECX
IN
LOAD
STRUCTURE
ADDRESSES
MOV
EDX
OUT
MOVAPS
ECX
MOVAPS
ECX
MOVAPS
ECX
MOVAPS
ECX
MOVAPS
MOVAPS
UNPCKLPS
UNPCKLPS
MOVDQA
MOVLHPS
MOVHLPS
UNPCKHPS
UNPCKHPS
MOVDQA
MOVLHPS
MOVHLPS
MOVAPS
EDX
MOVAPS
EDX
MOVAPS
EDX
MOVAPS
EDX
EXAMPLE
SHOWS
A
SIMILAR
DESWIZZLE
FUNCTION
USING
SIMD
INTEGER
INSTRUCTIONS
BOTH
OF
THESE
TECHNIQUES
DEMONSTRATE
LOADING
BYTES
AND
PERFORMING
HORIZONTAL
DATA
MOVEMENT
IN
REGISTERS
THIS
APPROACH
IS
LIKELY
TO
BE
MORE
EFFICIENT
THAN
ALTERNA
TIVE
TECHNIQUES
OF
STORING
BYTE
HALVES
OF
XMM
REGISTERS
USING
MOVLPS
AND
MOVHPS
EXAMPLE
DESWIZZLING
DATA
USING
SIMD
INTEGER
INSTRUCTIONS
MOV
MOV
MOVDQA
ECX
IN
EDX
OUT
ECX
LOAD
STRUCTURE
ADDRESSES
LOAD
MOVDQA
ECX
LOAD
MOVDQA
ECX
LOAD
MOVDQA
ECX
LOAD
START
DESWIZZLI
MOVDQA
NG
HERE
MOVDQA
PUNPCKLDQ
PUNPCKLDQ
MOVDQA
PUNPCKLQDQ
PUNPCKHQDQ
PUNPCKHDQ
PUNPCKHDQ
MOVDQA
PUNPCKLQDQ
PUNPCKHQDQ
MOVDQA
EDX
MOVDQA
EDX
MOVDQA
EDX
MOVDQA
EDX
DESWIZZLING
ENDS
HERE
HORIZONTAL
ADD
USING
SSE
ALTHOUGH
VERTICAL
COMPUTATIONS
GENERALLY
MAKE
USE
OF
SIMD
PERFORMANCE
BETTER
THAN
HORIZONTAL
COMPUTATIONS
IN
SOME
CASES
CODE
MUST
USE
A
HORIZONTAL
OPERATION
MOVLHPS
MOVHLPS
AND
SHUFFLE
CAN
BE
USED
TO
SUM
DATA
HORIZONTALLY
FOR
EXAMPLE
STARTING
WITH
FOUR
BIT
REGISTERS
TO
SUM
UP
EACH
REGISTER
HORIZONTALLY
WHILE
HAVING
THE
FINAL
RESULTS
IN
ONE
REGISTER
USE
THE
MOVLHPS
MOVHLPS
TO
ALIGN
THE
UPPER
AND
LOWER
PARTS
OF
EACH
REGISTER
THIS
ALLOWS
YOU
TO
USE
A
VERTICAL
ADD
WITH
THE
RESULTING
PARTIAL
HORIZONTAL
SUMMATION
FULL
SUMMATION
FOLLOWS
EASILY
FIGURE
PRESENTS
A
HORIZONTAL
ADD
USING
MOVHLPS
MOVLHPS
EXAMPLE
AND
EXAMPLE
PROVIDE
THE
CODE
FOR
THIS
OPERATION
A
FIGURE
HORIZONTAL
ADD
USING
MOVHLPS
MOVLHPS
EXAMPLE
HORIZONTAL
ADD
USING
MOVHLPS
MOVLHPS
VOID
IN
FLOAT
OUT
ASM
MOV
ECX
IN
LOAD
STRUCTURE
ADDRESSES
MOV
EDX
OUT
MOVAPS
ECX
LOAD
MOVAPS
ECX
LOAD
MOVAPS
ECX
LOAD
MOVAPS
ECX
LOAD
START
HORIZONTAL
ADD
MOVAPS
MOVLHPS
MOVHLPS
ADDPS
MOVAPS
MOVLHPS
MOVHLPS
ADDPS
MOVAPS
SHUFPS
SHUFPS
ADDPS
D
C
B
A
END
HORIZONTAL
ADD
MOVAPS
EDX
EXAMPLE
HORIZONTAL
ADD
USING
INTRINSICS
WITH
MOVHLPS
MOVLHPS
EXAMPLE
HORIZONTAL
ADD
USING
INTRINSICS
WITH
MOVHLPS
MOVLHPS
CONTD
IN
Y
IN
Z
IN
W
TMM0
TMM2
TMM2
C2
D4
A2
C2
C4
D2
D3
D4
OUT
USE
OF
INSTRUCTIONS
THE
AND
INSTRUCTIONS
ENCODE
THE
TRUNCATE
CHOP
ROUNDING
MODE
IMPLICITLY
IN
THE
INSTRUCTION
THEY
TAKE
PRECEDENCE
OVER
THE
ROUNDING
MODE
SPECIFIED
IN
THE
MXCSR
REGISTER
THIS
BEHAVIOR
CAN
ELIMINATE
THE
NEED
TO
CHANGE
THE
ROUNDING
MODE
FROM
ROUND
NEAREST
TO
TRUNCATE
CHOP
AND
THEN
BACK
TO
ROUND
NEAREST
TO
RESUME
COMPUTATION
AVOID
FREQUENT
CHANGES
TO
THE
MXCSR
REGISTER
SINCE
THERE
IS
A
PENALTY
ASSOCIATED
WITH
WRITING
THIS
REGISTER
TYPICALLY
WHEN
USING
ROUNDING
CONTROL
IN
MXCSR
CAN
ALWAYS
BE
SET
TO
ROUND
NEAREST
FLUSH
TO
ZERO
AND
DENORMALS
ARE
ZERO
MODES
THE
FLUSH
TO
ZERO
FTZ
AND
DENORMALS
ARE
ZERO
DAZ
MODES
ARE
NOT
COMPATIBLE
WITH
THE
IEEE
STANDARD
THEY
ARE
PROVIDED
TO
IMPROVE
PERFORMANCE
FOR
APPLICA
TIONS
WHERE
UNDERFLOW
IS
COMMON
AND
WHERE
THE
GENERATION
OF
A
DENORMALIZED
RESULT
IS
NOT
NECESSARY
SEE
ALSO
SECTION
FLOATING
POINT
MODES
AND
EXCEPTIONS
SIMD
OPTIMIZATIONS
AND
MICROARCHITECTURES
PENTIUM
M
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
HAVE
A
DIFFERENT
MICROAR
CHITECTURE
THAN
INTEL
NETBURST
MICROARCHITECTURE
INTEL
CORE
MICROARCHITECTURE
OFFERS
SIGNIFICANTLY
MORE
EFFICIENT
SIMD
FLOATING
POINT
CAPABILITY
THAN
PREVIOUS
MICROARCHI
TECTURES
IN
ADDITION
INSTRUCTION
LATENCY
AND
THROUGHPUT
OF
INSTRUCTIONS
ARE
SIGNIFICANTLY
IMPROVED
IN
INTEL
CORE
MICROARCHITECTURE
OVER
PREVIOUS
MICROARCHITEC
TURES
SIMD
FLOATING
POINT
PROGRAMMING
USING
ENHANCES
SSE
AND
WITH
NINE
INSTRUCTIONS
TARGETED
FOR
SIMD
FLOATING
POINT
PROGRAMMING
IN
CONTRAST
TO
MANY
SSE
INSTRUCTIONS
OFFERING
HOMOGENEOUS
ARITHMETIC
OPERATIONS
ON
PARALLEL
DATA
ELEMENTS
AND
FAVORING
THE
VERTICAL
COMPUTATION
MODEL
OFFERS
INSTRUCTIONS
THAT
PERFORMS
ASYMMETRIC
ARITHMETIC
OPERATION
AND
ARITHMETIC
OPERATION
ON
HORIZONTAL
DATA
ELEMENTS
ADDSUBPS
AND
ADDSUBPD
ARE
TWO
INSTRUCTIONS
WITH
ASYMMETRIC
ARITHMETIC
PROCESSING
CAPABILITY
SEE
FIGURE
HADDPS
HADDPD
HSUBPS
AND
HSUBPD
OFFERS
HORIZONTAL
ARITHMETIC
PROCESSING
CAPABILITY
SEE
FIGURE
IN
ADDITION
MOVSLDUP
MOVSHDUP
AND
MOVDDUP
LOAD
DATA
FROM
MEMORY
OR
XMM
REGISTER
AND
REPLICATE
DATA
ELEMENTS
AT
ONCE
FIGURE
ASYMMETRIC
ARITHMETIC
OPERATION
OF
THE
INSTRUCTION
FIGURE
HORIZONTAL
ARITHMETIC
OPERATION
OF
THE
INSTRUCTION
HADDPD
AND
COMPLEX
ARITHMETICS
THE
FLEXIBILITY
OF
IN
DEALING
WITH
AOS
TYPE
OF
DATA
STRUCTURE
CAN
BE
DEMON
STRATED
BY
THE
EXAMPLE
OF
MULTIPLICATION
AND
DIVISION
OF
COMPLEX
NUMBERS
FOR
EXAMPLE
A
COMPLEX
NUMBER
CAN
BE
STORED
IN
A
STRUCTURE
CONSISTING
OF
ITS
REAL
AND
IMAGINARY
PART
THIS
NATURALLY
LEADS
TO
THE
USE
OF
AN
ARRAY
OF
STRUCTURE
EXAMPLE
DEMONSTRATES
USING
INSTRUCTIONS
TO
PERFORM
MULTIPLICATIONS
OF
SINGLE
PRECISION
COMPLEX
NUMBERS
EXAMPLE
DEMONSTRATES
USING
INSTRUCTIONS
TO
PERFORM
DIVISION
OF
COMPLEX
NUMBERS
EXAMPLE
MULTIPLICATION
OF
TWO
PAIR
OF
SINGLE
PRECISION
COMPLEX
NUMBER
MULTIPLICATION
OF
AK
I
BK
CK
I
DK
A
I
B
CAN
BE
STORED
AS
A
DATA
STRUCTURE
MOVSLDUP
LOAD
REAL
PARTS
INTO
THE
DESTINATION
MOVAPS
LOAD
THE
PAIR
OF
COMPLEX
VALUES
I
E
MULPS
TEMPORARY
RESULTS
SHUFPS
REORDER
THE
REAL
AND
IMAGINARY
PARTS
MOVSHDUP
LOAD
THE
IMAGINARY
PARTS
INTO
THE
DESTINATION
EXAMPLE
MULTIPLICATION
OF
TWO
PAIR
OF
SINGLE
PRECISION
COMPLEX
NUMBER
MULPS
TEMPORARY
RESULTS
ADDSUBPS
EXAMPLE
DIVISION
OF
TWO
PAIR
OF
SINGLE
PRECISION
COMPLEX
NUMBERS
DIVISION
OF
AK
I
BK
CK
I
DK
MOVSHDUP
LOAD
IMAGINARY
PARTS
INTO
THE
DESTINATION
MOVAPS
LOAD
THE
PAIR
OF
COMPLEX
VALUES
I
E
MULPS
TEMPORARY
RESULTS
SHUFPS
REORDER
THE
REAL
AND
IMAGINARY
PARTS
D1
MOVSLDUP
LOAD
THE
REAL
PARTS
INTO
THE
DESTINATION
MULPS
TEMP
RESULTS
ADDSUBPS
MULPS
MOVPS
SHUFPS
ADDPS
DIVPS
SHUFPS
D1D1
D1D1
A0C0
IN
BOTH
EXAMPLES
THE
COMPLEX
NUMBERS
ARE
STORE
IN
ARRAYS
OF
STRUCTURES
MOVSLDUP
MOVSHDUP
AND
THE
ASYMMETRIC
ADDSUBPS
ALLOW
PERFORMING
COMPLEX
ARITHMETICS
ON
TWO
PAIR
OF
SINGLE
PRECISION
COMPLEX
NUMBER
SIMULTANEOUSLY
AND
WITHOUT
ANY
UNNECESSARY
SWIZZLING
BETWEEN
DATA
ELEMENTS
DUE
TO
MICROARCHITECTURAL
DIFFERENCES
SOFTWARE
SHOULD
IMPLEMENT
MULTIPLICATION
OF
COMPLEX
DOUBLE
PRECISION
NUMBERS
USING
INSTRUCTIONS
ON
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
IN
INTEL
CORE
DUO
AND
INTEL
CORE
SOLO
PROCESSORS
SOFT
WARE
SHOULD
USE
SCALAR
INSTRUCTIONS
TO
IMPLEMENT
DOUBLE
PRECISION
COMPLEX
MULTIPLICATION
THIS
IS
BECAUSE
THE
DATA
PATH
BETWEEN
SIMD
EXECUTION
UNITS
IS
BITS
IN
INTEL
CORE
MICROARCHITECTURE
AND
ONLY
BITS
IN
PREVIOUS
MICROARCHITECTURES
PROCESSORS
BASED
ON
THE
ENHANCED
INTEL
CORE
MICROARCHITECTURE
GENERALLY
EXECUTES
INSTRUCTION
MORE
EFFICIENTLY
THAN
PREVIOUS
MICROARCHITECTURES
THEY
ALSO
HAVE
A
BIT
SHUFFLE
UNIT
THAT
WILL
BENEFIT
COMPLEX
ARITHMETIC
OPERATIONS
FURTHER
THAN
INTEL
CORE
MICROARCHITECTURE
DID
EXAMPLE
SHOWS
TWO
EQUIVALENT
IMPLEMENTATIONS
OF
DOUBLE
PRECISION
COMPLEX
MULTIPLY
OF
TWO
PAIR
OF
COMPLEX
NUMBERS
USING
VECTOR
VERSUS
INSTRUCTIONS
EXAMPLE
SHOWS
THE
EQUIVALENT
SCALAR
IMPLEMENTATION
EXAMPLE
DOUBLE
PRECISION
COMPLEX
MULTIPLICATION
OF
TWO
PAIRS
VECTOR
IMPLEMENTATION
VECTOR
IMPLEMENTATION
MOVAPD
EAX
Y
X
MOVAPD
EAX
Y
X
MOVAPD
EAX
W
Z
MOVAPD
EAX
Z
Z
UNPCKLPD
Z
Z
MOVAPD
MOVAPD
EAX
W
Z
UNPCKLPD
UNPCKHPD
W
W
UNPCKHPD
MULPD
Z
Y
Z
X
MULPD
Z
Y
Z
X
MULPD
W
Y
W
X
MULPD
W
Y
W
X
XORPD
W
Y
W
X
SHUFPD
W
X
W
Y
SHUFPD
W
X
W
Y
ADDSUBPD
W
X
Z
Y
Z
X
W
Y
ADDPD
Z
Y
W
X
Z
X
W
Y
MOVAPD
ECX
MOVAPD
ECX
EXAMPLE
DOUBLE
PRECISION
COMPLEX
MULTIPLICATION
USING
SCALAR
EXAMPLE
DOUBLE
PRECISION
COMPLEX
MULTIPLICATION
USING
SCALAR
CONTD
MULSD
W
Y
SUBSD
Z
X
W
Y
ADDSD
Z
Y
W
X
MOVSD
ECX
MOVSD
ECX
PACKED
FLOATING
POINT
PERFORMANCE
IN
INTEL
CORE
DUO
PROCESSOR
MOST
PACKED
SIMD
FLOATING
POINT
CODE
WILL
SPEED
UP
ON
INTEL
CORE
SOLO
PROCESSORS
RELATIVE
TO
PENTIUM
M
PROCESSORS
THIS
IS
DUE
TO
IMPROVEMENT
IN
DECODING
PACKED
SIMD
INSTRUCTIONS
THE
IMPROVEMENT
OF
PACKED
FLOATING
POINT
PERFORMANCE
ON
THE
INTEL
CORE
SOLO
PROCESSOR
OVER
PENTIUM
M
PROCESSOR
DEPENDS
ON
SEVERAL
FACTORS
GENERALLY
CODE
THAT
IS
DECODER
BOUND
AND
OR
HAS
A
MIXTURE
OF
INTEGER
AND
PACKED
FLOATING
POINT
INSTRUC
TIONS
CAN
EXPECT
SIGNIFICANT
GAIN
CODE
THAT
IS
LIMITED
BY
EXECUTION
LATENCY
AND
HAS
A
CYCLES
PER
INSTRUCTIONS
RATIO
GREATER
THAN
ONE
WILL
NOT
BENEFIT
FROM
DECODER
IMPROVEMENT
WHEN
TARGETING
COMPLEX
ARITHMETICS
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCES
SORS
USING
SINGLE
PRECISION
INSTRUCTIONS
CAN
DELIVER
HIGHER
PERFORMANCE
THAN
ALTERNATIVES
ON
THE
OTHER
HAND
TASKS
REQUIRING
DOUBLE
PRECISION
COMPLEX
ARITH
METICS
MAY
PERFORM
BETTER
USING
SCALAR
INSTRUCTIONS
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
THIS
IS
BECAUSE
SCALAR
INSTRUCTIONS
CAN
BE
DISPATCHED
THROUGH
TWO
PORTS
AND
EXECUTED
USING
TWO
SEPARATE
FLOATING
POINT
UNITS
PACKED
HORIZONTAL
INSTRUCTIONS
HADDPS
AND
HSUBPS
CAN
SIMPLIFY
THE
CODE
SEQUENCE
FOR
SOME
TASKS
HOWEVER
THESE
INSTRUCTION
CONSIST
OF
MORE
THAN
FIVE
MICRO
OPS
ON
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
CARE
MUST
BE
TAKEN
TO
ENSURE
THE
LATENCY
AND
DECODING
PENALTY
OF
THE
HORIZONTAL
INSTRUCTION
DOES
NOT
OFFSET
ANY
ALGORITHMIC
BENEFITS
DOT
PRODUCT
AND
HORIZONTAL
SIMD
INSTRUCTIONS
SOMETIMES
THE
AOS
TYPE
OF
DATA
ORGANIZATION
ARE
MORE
NATURAL
IN
MANY
ALGEBRAIC
FORMULA
ONE
COMMON
EXAMPLE
IS
THE
DOT
PRODUCT
OPERATION
DOT
PRODUCT
OPERATION
CAN
BE
IMPLEMENTED
USING
SSE
INSTRUCTION
SETS
ADDED
A
FEW
HORIZONTAL
ADD
SUBTRACT
INSTRUCTIONS
FOR
APPLICATIONS
THAT
RELY
ON
THE
HORIZONTAL
COMPUTATION
MODEL
PROVIDES
ADDITIONAL
ENHANCEMENT
WITH
INSTRUCTIONS
THAT
ARE
CAPABLE
OF
DIRECTLY
EVALUATING
DOT
PRODUCT
OPERATIONS
OF
VECTORS
OF
OR
COMPONENTS
EXAMPLE
DOT
PRODUCT
OF
VECTOR
LENGTH
USING
SSE
USING
SSE
TO
COMPUTE
ONE
DOT
PRODUCT
MOVAPS
EAX
MULPS
EAX
MOVHLPS
X
X
UPPER
HALF
NOT
NEEDED
ADDPS
X
X
PSHUFD
X
X
X
ADDSS
MOVSS
ECX
EXAMPLE
DOT
PRODUCT
OF
VECTOR
LENGTH
USING
USING
TO
COMPUTE
ONE
DOT
PRODUCT
MOVAPS
EAX
MULPS
EAX
HADDPS
MOVAPS
PSRLQ
ADDSS
A3
B2
MOVSS
EAX
EXAMPLE
DOT
PRODUCT
OF
VECTOR
LENGTH
USING
USING
TO
COMPUTE
ONE
DOT
PRODUCT
MOVAPS
EAX
DPPS
EAX
B1
A3
A2
B2
A4
B4
MOVSS
EAX
EXAMPLE
EXAMPLE
AND
EXAMPLE
COMPARE
THE
BASIC
CODE
SEQUENCE
TO
COMPUTE
ONE
DOT
PRODUCT
RESULT
FOR
A
PAIR
OF
VECTORS
THE
SELECTION
OF
AN
OPTIMAL
SEQUENCE
IN
CONJUNCTION
WITH
AN
APPLICATION
MEMORY
ACCESS
PATTERNS
MAY
FAVOR
DIFFERENT
APPROACHES
FOR
EXAMPLE
IF
EACH
DOT
PRODUCT
RESULT
IS
IMMEDIATELY
CONSUMED
BY
ADDITIONAL
COMPUTATIONAL
SEQUENCES
IT
MAY
BE
MORE
OPTIMAL
TO
COMPARE
THE
RELATIVE
SPEED
OF
THESE
DIFFERENT
APPROACHES
IF
DOT
PRODUCTS
CAN
BE
COMPUTED
FOR
AN
ARRAY
OF
VECTORS
AND
KEPT
IN
THE
CACHE
FOR
SUBSE
QUENT
COMPUTATIONS
THEN
MORE
OPTIMAL
CHOICE
MAY
DEPEND
ON
THE
RELATIVE
THROUGHPUT
OF
THE
SEQUENCE
OF
INSTRUCTIONS
IN
INTEL
CORE
MICROARCHITECTURE
EXAMPLE
HAS
HIGHER
THROUGHPUT
THAN
EXAMPLE
DUE
TO
THE
RELATIVELY
LONGER
LATENCY
OF
HADDPS
THE
SPEED
OF
EXAMPLE
IS
SLIGHTLY
SLOWER
THAN
EXAMPLE
IN
ENHANCED
INTEL
CORE
MICROARCHITECTURE
EXAMPLE
IS
FASTER
IN
BOTH
SPEED
AND
THROUGHPUT
THAN
EXAMPLE
AND
EXAMPLE
ALTHOUGH
THE
LATENCY
OF
DPPS
IS
ALSO
RELATIVELY
LONG
IT
IS
COMPENSATED
BY
THE
REDUCTION
OF
NUMBER
OF
INSTRUCTIONS
IN
EXAMPLE
TO
DO
THE
SAME
AMOUNT
OF
WORK
UNROLLING
CAN
FURTHER
IMPROVE
THE
THROUGHPUT
OF
EACH
OF
THREE
DOT
PRODUCT
IMPLEMEN
TATIONS
EXAMPLE
SHOWS
TWO
UNROLLED
VERSIONS
USING
THE
BASIC
AND
SEQUENCES
THE
VERSION
CAN
ALSO
BE
UNROLLED
AND
USING
INSERTPS
TO
PACK
DOT
PRODUCT
RESULTS
EXAMPLE
UNROLLED
IMPLEMENTATION
OF
FOUR
DOT
PRODUCTS
IMPLEMENTATION
IMPLEMENTATION
MOVAPS
EAX
MOVAPS
EAX
MULPS
EAX
MULPS
EAX
Y0
MOVAPS
EAX
MOVAPS
EAX
MULPS
EAX
MULPS
EAX
MOVAPS
EAX
X3
MULPS
EAX
MOVAPS
EAX
MOVAPS
EAX
MULPS
EAX
MULPS
EAX
HADDPS
MOVAPS
EAX
HADDPS
MULPS
EAX
96
HADDPS
MOVAPS
ECX
EXAMPLE
UNROLLED
IMPLEMENTATION
OF
FOUR
DOT
PRODUCTS
CONTD
IMPLEMENTATION
IMPLEMENTATION
MOVAPS
UNPCKLPS
Y0
X3
UNPCKHPS
Z2
MOVAPS
UNPCKLPS
UNPCKHPS
ADDPS
ADDPS
MOVAPS
MOVHLPS
MOVLHPS
ADDPS
MOVAPS
ECX
VECTOR
NORMALIZATION
NORMALIZING
VECTORS
IS
A
COMMON
OPERATION
IN
MANY
FLOATING
POINT
APPLICATIONS
EXAMPLE
SHOWS
AN
EXAMPLE
IN
C
OF
NORMALIZING
AN
ARRAY
OF
X
Y
Z
VECTORS
EXAMPLE
NORMALIZATION
OF
AN
ARRAY
OF
VECTORS
FOR
I
I
CNT
I
FLOAT
SIZE
NODES
I
VEC
DOT
IF
SIZE
SIZE
SQRTF
SIZE
ELSE
SIZE
NODES
I
VEC
X
SIZE
NODES
I
VEC
Y
SIZE
NODES
I
VEC
Z
SIZE
EXAMPLE
SHOWS
AN
ASSEMBLY
SEQUENCE
THAT
NORMALIZES
THE
X
Y
Z
COMPONENTS
OF
A
VECTOR
EXAMPLE
NORMALIZE
X
Y
Z
COMPONENTS
OF
AN
ARRAY
OF
VECTORS
USING
P
NODES
I
VEC
ASM
MOV
EAX
P
XORPS
MOVUPS
EAX
LOADS
THE
X
Y
Z
OF
INPUT
VECTOR
PLUS
X
OF
NEXT
VECTOR
MOVAPS
SAVE
A
COPY
OF
DATA
FROM
MEMORY
TO
RESTORE
THE
UNNORMALIZED
VALUE
MOVAPS
MASK
TO
SELECT
X
Y
Z
VALUES
FROM
AN
XMM
REGISTER
TO
NORMALIZE
ANDPS
MASK
ELEMENTS
MOVAPS
SAVE
A
COPY
OF
X
Y
Z
TO
COMPUTE
NORMALIZED
VECTOR
LATER
MULPS
Z
Z
Y
Y
X
X
PSHUFD
X
X
Y
Y
Z
Z
ADDPS
X
X
Z
Z
Y
Y
Z
Z
Y
Y
X
X
PSHUFD
Z
Z
Y
Y
X
X
X
X
Z
Z
Y
Y
ADDPS
X
X
Y
Y
Z
Z
X
X
Y
Y
Z
Z
X
X
Y
Y
Z
Z
X
X
Y
Y
Z
Z
COMISD
COMPARE
SIZE
TO
JZ
ZERO
MOVAPS
PRELOADED
UNITARY
VECTOR
SQRTPS
DIVPS
JMP
STORE
ZERO
MOVAPS
STORE
MULPS
NORMALIZE
THE
VECTOR
IN
THE
LOWER
ELEMENTS
ANDNPS
MASK
OFF
THE
LOWER
ELEMENTS
TO
KEEP
THE
UN
NORMALIZED
VALUE
ORPS
ORDER
THE
UN
NORMALIZED
COMPONENT
AFTER
THE
NORMALIZED
VECTOR
MOVAPS
EAX
WRITES
NORMALIZED
X
Y
Z
FOLLOWED
BY
UNMODIFIED
VALUE
EXAMPLE
SHOWS
AN
ASSEMBLY
SEQUENCE
USING
TO
NORMALIZES
THE
X
Y
Z
COMPONENTS
OF
A
VECTOR
EXAMPLE
NORMALIZE
X
Y
Z
COMPONENTS
OF
AN
ARRAY
OF
VECTORS
USING
P
NODES
I
VEC
ASM
MOV
EAX
P
XORPS
MOVUPS
EAX
LOADS
THE
X
Y
Z
OF
INPUT
VECTOR
PLUS
X
OF
NEXT
VECTOR
MOVAPS
SAVE
A
COPY
OF
DATA
FROM
MEMORY
DPPS
X
X
Y
Y
Z
Z
X
X
Y
Y
Z
Z
X
X
Y
Y
Z
Z
X
X
Y
Y
Z
Z
COMISD
COMPARE
SIZE
TO
JZ
ZERO
MOVAPS
PRELOADED
UNITARY
VECTOR
SQRTPS
DIVPS
JMP
STORE
ZERO
MOVAPS
STORE
MULPS
NORMALIZE
THE
VECTOR
IN
THE
LOWER
ELEMENTS
BLENDPS
COPY
THE
UN
NORMALIZED
COMPONENT
NEXT
TO
THE
NORMALIZED
VECTOR
MOVAPS
EAX
IN
EXAMPLE
AND
EXAMPLE
THE
THROUGHPUT
OF
THESE
INSTRUCTION
SEQUENCES
ARE
BASICALLY
LIMITED
BY
THE
LONG
LATENCY
INSTRUCTIONS
OF
DIVPS
AND
SQRTPS
IN
EXAMPLE
THE
USE
OF
DPPS
REPLACES
EIGHT
INSTRUCTIONS
TO
EVALUATE
AND
BROADCAST
THE
DOT
PRODUCT
RESULT
TO
FOUR
ELEMENTS
OF
AN
XMM
REGISTER
THIS
COULD
RESULT
IN
IMPROVEMENT
OF
THE
RELATIVE
SPEED
OF
EXAMPLE
OVER
EXAMPLE
USING
HORIZONTAL
SIMD
INSTRUCTION
SETS
AND
DATA
LAYOUT
SSE
AND
PROVIDE
PACKED
ADD
SUBTRACT
MULTIPLY
DIVIDE
INSTRUCTIONS
THAT
ARE
IDEAL
FOR
SITUATIONS
THAT
CAN
TAKE
ADVANTAGE
OF
VERTICAL
COMPUTATION
MODEL
SUCH
AS
SOA
DATA
LAYOUT
AND
ADDED
HORIZONTAL
SIMD
INSTRUCTIONS
INCLUDING
HORIZONTAL
ADD
SUBTRACT
DOT
PRODUCT
OPERATIONS
THESE
MORE
RECENT
SIMD
EXTENSIONS
PROVIDE
TOOLS
TO
SOLVE
PROBLEMS
INVOLVING
DATA
LAYOUTS
OR
OPERATIONS
THAT
DO
NOT
CONFORM
TO
THE
VERTICAL
SIMD
COMPUTATION
MODEL
IN
THIS
SECTION
WE
CONSIDER
A
VECTOR
MATRIX
MULTIPLICATION
PROBLEM
AND
DISCUSS
THE
RELEVANT
FACTORS
FOR
CHOOSING
VARIOUS
HORIZONTAL
SIMD
INSTRUCTIONS
EXAMPLE
SHOWS
THE
VECTOR
MATRIX
DATA
LAYOUT
IN
AOS
WHERE
THE
INPUT
AND
OUT
VECTORS
ARE
STORED
AS
AN
ARRAY
OF
STRUCTURE
EXAMPLE
20
DATA
ORGANIZATION
IN
MEMORY
FOR
AOS
VECTOR
MATRIX
MULTIPLICATION
MATRIX
PMAT
INPUT
VERTICES
PVERT
OUPUT
VERTICES
POUTVERT
EXAMPLE
SHOWS
AN
EXAMPLE
USING
HADDPS
AND
MULPS
TO
PERFORM
VECTOR
MATRIX
MULTIPLICATION
WITH
DATA
LAYOUT
IN
AOS
AFTER
THREE
HADDPS
COMPLETING
THE
SUMMATIONS
OF
EACH
OUTPUT
VECTOR
COMPONENT
THE
OUTPUT
COMPONENTS
ARE
ARRANGED
IN
AOS
EXAMPLE
AOS
VECTOR
MATRIX
MULTIPLICATION
WITH
HADDPS
MOV
EAX
PMAT
MOV
EBX
PVERT
MOV
ECX
POUTVERT
XOR
EDX
EDX
MOVAPS
EAX
LOAD
ROW
MOVAPS
EAX
LOAD
ROW
MOVAPS
EAX
LOAD
ROW
LLOOP
MOVAPS
EBX
EDX
LOAD
INPUT
VECTOR
MOVAPS
MULPS
EAX
VW
VZ
VY
VX
MOVAPS
MULPS
VW
VZ
VY
VX
EXAMPLE
AOS
VECTOR
MATRIX
MULTIPLICATION
WITH
HADDPS
MOVAPS
MULPS
VW
VZ
VY
VX
MOVAPS
MULPS
VW
VZ
VY
VX
HADDPS
HADDPS
HADDPS
MOVAPS
ECX
EDX
STORE
A
VECTOR
OF
LENGTH
ADD
EDX
CMP
EDX
TOP
JB
LLOOP
EXAMPLE
SHOWS
AN
EXAMPLE
USING
DPPS
TO
PERFORM
VECTOR
MATRIX
MULTIPLICATION
IN
AOS
EXAMPLE
AOS
VECTOR
MATRIX
MULTIPLICATION
WITH
DPPS
MOV
EAX
PMAT
MOV
EBX
PVERT
MOV
ECX
POUTVERT
XOR
EDX
EDX
MOVAPS
EAX
LOAD
ROW
MOVAPS
EAX
LOAD
ROW
MOVAPS
EAX
LOAD
ROW
LLOOP
MOVAPS
EBX
EDX
LOAD
INPUT
VECTOR
MOVAPS
DPPS
EAX
CALCULATE
DOT
PRODUCT
OF
LENGTH
STORE
TO
LOWEST
DWORD
MOVAPS
DPPS
MOVAPS
DPPS
MOVAPS
DPPS
MOVSS
ECX
EDX
STORE
ONE
ELEMENT
OF
VECTOR
LENGTH
MOVSS
ECX
EDX
MOVSS
ECX
EDX
MOVSS
ECX
EDX
ADD
EDX
CMP
EDX
TOP
JB
LLOOP
EXAMPLE
AND
EXAMPLE
BOTH
WORK
WITH
AOS
DATA
LAYOUT
USING
DIFFERENT
HORIZONTAL
PROCESSING
TECHNIQUES
PROVIDED
BY
AND
THE
EFFECTIVENESS
OF
EITHER
TECHNIQUES
WILL
VARY
DEPENDING
ON
THE
DEGREE
OF
EXPOSURES
OF
LONG
LATENCY
INSTRUCTION
IN
THE
INNER
LOOP
THE
OVERHEAD
EFFICIENCY
OF
DATA
MOVEMENT
AND
THE
LATENCY
OF
HADDPS
VS
DPPS
ON
PROCESSORS
THAT
SUPPORT
BOTH
HADDPS
AND
DPPS
THE
CHOICE
BETWEEN
EITHER
TECH
NIQUE
MAY
DEPEND
ON
APPLICATION
SPECIFIC
CONSIDERATIONS
IF
THE
OUTPUT
VECTORS
ARE
WRITTEN
BACK
TO
MEMORY
DIRECTLY
IN
A
BATCH
SITUATION
EXAMPLE
MAY
BE
PREFER
ABLE
OVER
EXAMPLE
BECAUSE
THE
LATENCY
OF
DPPS
IS
LONG
AND
STORING
EACH
OUTPUT
VECTOR
COMPONENT
INDIVIDUALLY
IS
LESS
THAN
IDEAL
FOR
STORING
AN
ARRAY
OF
VECTORS
THERE
MAY
BE
PARTIALLY
VECTORIZABLE
SITUATIONS
THAT
THE
INDIVIDUAL
OUTPUT
VECTOR
COMPONENT
IS
CONSUMED
IMMEDIATELY
BY
OTHER
NON
VECTORIZABLE
COMPUTATIONS
THEN
USING
DPPS
PRODUCING
INDIVIDUAL
COMPONENT
MAY
BE
MORE
SUITABLE
THAN
DISPERSING
THE
PACKED
OUTPUT
VECTOR
PRODUCED
BY
THREE
HADDPS
AS
IN
EXAMPLE
SOA
AND
VECTOR
MATRIX
MULTIPLICATION
IF
THE
NATIVE
DATA
LAYOUT
OF
A
PROBLEM
CONFORMS
TO
SOA
THEN
VECTOR
MATRIX
MULTIPLY
CAN
BE
CODED
USING
MULPS
ADDPS
WITHOUT
USING
THE
LONGER
LATENCY
HORIZONTAL
ARITH
METIC
INSTRUCTIONS
OR
PACKING
SCALAR
COMPONENTS
INTO
PACKED
FORMAT
EXAMPLE
TO
ACHIEVE
HIGHER
THROUGHPUT
WITH
SOA
DATA
LAYOUT
THERE
ARE
EITHER
PRE
REQUISITE
DATA
PREPARATION
OR
SWIZZLING
DESWIZZLING
ON
THE
FLY
THAT
MUST
BE
COMPREHENDED
FOR
EXAMPLE
AN
SOA
DATA
LAYOUT
FOR
VECTOR
MATRIX
MULTIPLICATION
IS
SHOWN
IN
EXAMPLE
EACH
MATRIX
ELEMENT
IS
REPLICATED
FOUR
TIMES
TO
MINIMIZE
DATA
MOVEMENT
OVERHEAD
FOR
PRODUCING
PACKED
RESULTS
EXAMPLE
DATA
ORGANIZATION
IN
MEMORY
FOR
SOA
VECTOR
MATRIX
MULTIPLICATION
MATRIX
PMAT
M03
M12
M21
M30
M33
INPUT
VERTICES
PVERT
OUPUT
VERTICES
POUTVERT
O3Z
THE
CORRESPONDING
VECTOR
MATRIX
MULTIPLY
EXAMPLE
IN
SOA
UNROLLED
FOR
FOUR
ITERATION
OF
VECTORS
IS
SHOWN
IN
EXAMPLE
EXAMPLE
VECTOR
MATRIX
MULTIPLICATION
WITH
NATIVE
SOA
DATA
LAYOUT
MOV
EBX
PVERT
MOV
ECX
POUTVERT
XOR
EDX
EDX
MOVAPS
EAX
LOAD
ROW
MOVAPS
EAX
LOAD
ROW
MOVAPS
EAX
LOAD
ROW
MOV
EAX
PMAT
XOR
EDI
EDI
MOVAPS
EBX
LOAD
MOVAPS
EBX
LOAD
MOVAPS
EBX
LOAD
MOVAPS
EBX
LOAD
MOVAPS
EAX
MULPS
V1X
MOVAPS
EAX
MULPS
V2Y
V0Y
ADDPS
MOVAPS
EAX
MULPS
V1Z
ADDPS
MOVAPS
EAX
MULPS
V2W
V0W
ADDPS
MOVAPS
ECX
EDX
ADD
EAX
ADD
EDX
ADD
EDI
CMP
EDI
JB
ADD
EBX
CMP
EDX
TOP
JB
CHAPTER
OPTIMIZING
CACHE
USAGE
OVER
THE
PAST
DECADE
PROCESSOR
SPEED
HAS
INCREASED
MEMORY
ACCESS
SPEED
HAS
INCREASED
AT
A
SLOWER
PACE
THE
RESULTING
DISPARITY
HAS
MADE
IT
IMPORTANT
TO
TUNE
APPLICATIONS
IN
ONE
OF
TWO
WAYS
EITHER
A
A
MAJORITY
OF
DATA
ACCESSES
ARE
FULFILLED
FROM
PROCESSOR
CACHES
OR
B
EFFECTIVELY
MASKING
MEMORY
LATENCY
TO
UTILIZE
PEAK
MEMORY
BANDWIDTH
AS
MUCH
AS
POSSIBLE
HARDWARE
PREFETCHING
MECHANISMS
ARE
ENHANCEMENTS
IN
MICROARCHITECTURE
TO
FACILI
TATE
THE
LATTER
ASPECT
AND
WILL
BE
MOST
EFFECTIVE
WHEN
COMBINED
WITH
SOFTWARE
TUNING
THE
PERFORMANCE
OF
MOST
APPLICATIONS
CAN
BE
CONSIDERABLY
IMPROVED
IF
THE
DATA
REQUIRED
CAN
BE
FETCHED
FROM
THE
PROCESSOR
CACHES
OR
IF
MEMORY
TRAFFIC
CAN
TAKE
ADVANTAGE
OF
HARDWARE
PREFETCHING
EFFECTIVELY
STANDARD
TECHNIQUES
TO
BRING
DATA
INTO
THE
PROCESSOR
BEFORE
IT
IS
NEEDED
INVOLVE
ADDI
TIONAL
PROGRAMMING
WHICH
CAN
BE
DIFFICULT
TO
IMPLEMENT
AND
MAY
REQUIRE
SPECIAL
STEPS
TO
PREVENT
PERFORMANCE
DEGRADATION
STREAMING
SIMD
EXTENSIONS
ADDRESSED
THIS
ISSUE
BY
PROVIDING
VARIOUS
PREFETCH
INSTRUCTIONS
STREAMING
SIMD
EXTENSIONS
INTRODUCED
THE
VARIOUS
NON
TEMPORAL
STORE
INSTRUCTIONS
EXTENDS
THIS
SUPPORT
TO
NEW
DATA
TYPES
AND
ALSO
INTRODUCE
NON
TEMPORAL
STORE
SUPPORT
FOR
THE
BIT
INTEGER
REGISTERS
THIS
CHAPTER
FOCUSES
ON
HARDWARE
PREFETCH
MECHANISM
SOFTWARE
PREFETCH
AND
CACHEABILITY
INSTRUCTIONS
DISCUSSES
MICROARCHITECTURAL
FEATURE
AND
INSTRUCTIONS
THAT
ALLOW
YOU
TO
AFFECT
DATA
CACHING
IN
AN
APPLICATION
MEMORY
OPTIMIZATION
USING
HARDWARE
PREFETCHING
SOFTWARE
PREFETCH
AND
CACHE
ABILITY
INSTRUCTIONS
DISCUSSES
TECHNIQUES
FOR
IMPLEMENTING
MEMORY
OPTIMIZA
TIONS
USING
THE
ABOVE
INSTRUCTIONS
NOTE
IN
A
NUMBER
OF
CASES
PRESENTED
THE
PREFETCHING
AND
CACHE
UTILIZATION
DESCRIBED
ARE
SPECIFIC
TO
CURRENT
IMPLEMENTATIONS
OF
INTEL
NETBURST
MICROARCHITECTURE
BUT
ARE
LARGELY
APPLICABLE
FOR
THE
FUTURE
PROCESSORS
USING
DETERMINISTIC
CACHE
PARAMETERS
TO
MANAGE
CACHE
HIERARCHY
GENERAL
PREFETCH
CODING
GUIDELINES
THE
FOLLOWING
GUIDELINES
WILL
HELP
YOU
TO
REDUCE
MEMORY
TRAFFIC
AND
UTILIZE
PEAK
MEMORY
SYSTEM
BANDWIDTH
MORE
EFFECTIVELY
WHEN
LARGE
AMOUNTS
OF
DATA
MOVEMENT
MUST
ORIGINATE
FROM
THE
MEMORY
SYSTEM
TAKE
ADVANTAGE
OF
THE
HARDWARE
PREFETCHER
ABILITY
TO
PREFETCH
DATA
THAT
ARE
ACCESSED
IN
LINEAR
PATTERNS
IN
EITHER
A
FORWARD
OR
BACKWARD
DIRECTION
TAKE
ADVANTAGE
OF
THE
HARDWARE
PREFETCHER
ABILITY
TO
PREFETCH
DATA
THAT
ARE
ACCESSED
IN
A
REGULAR
PATTERN
WITH
ACCESS
STRIDES
THAT
ARE
SUBSTANTIALLY
SMALLER
THAN
HALF
OF
THE
TRIGGER
DISTANCE
OF
THE
HARDWARE
PREFETCH
SEE
TABLE
USE
A
CURRENT
GENERATION
COMPILER
SUCH
AS
THE
INTEL
C
COMPILER
THAT
SUPPORTS
C
LANGUAGE
LEVEL
FEATURES
FOR
STREAMING
SIMD
EXTENSIONS
STREAMING
SIMD
EXTENSIONS
AND
MMX
TECHNOLOGY
INSTRUCTIONS
PROVIDE
INTRINSICS
THAT
ALLOW
YOU
TO
OPTIMIZE
CACHE
UTILIZATION
EXAMPLES
OF
INTEL
COMPILER
INTRINSICS
INCLUDE
AND
FOR
DETAILS
REFER
TO
INTEL
C
COMPILER
USER
GUIDE
DOCUMENTATION
FACILITATE
COMPILER
OPTIMIZATION
BY
MINIMIZE
USE
OF
GLOBAL
VARIABLES
AND
POINTERS
MINIMIZE
USE
OF
COMPLEX
CONTROL
FLOW
USE
THE
CONST
MODIFIER
AVOID
REGISTER
MODIFIER
CHOOSE
DATA
TYPES
CAREFULLY
SEE
BELOW
AND
AVOID
TYPE
CASTING
USE
CACHE
BLOCKING
TECHNIQUES
FOR
EXAMPLE
STRIP
MINING
AS
FOLLOWS
IMPROVE
CACHE
HIT
RATE
BY
USING
CACHE
BLOCKING
TECHNIQUES
SUCH
AS
STRIP
MINING
ONE
DIMENSIONAL
ARRAYS
OR
LOOP
BLOCKING
TWO
DIMENSIONAL
ARRAYS
EXPLORE
USING
HARDWARE
PREFETCHING
MECHANISM
IF
YOUR
DATA
ACCESS
PATTERN
HAS
SUFFICIENT
REGULARITY
TO
ALLOW
ALTERNATE
SEQUENCING
OF
DATA
ACCESSES
FOR
EXAMPLE
TILING
FOR
IMPROVED
SPATIAL
LOCALITY
OTHERWISE
USE
PREFETCHNTA
BALANCE
SINGLE
PASS
VERSUS
MULTI
PASS
EXECUTION
SINGLE
PASS
OR
UNLAYERED
EXECUTION
PASSES
A
SINGLE
DATA
ELEMENT
THROUGH
AN
ENTIRE
COMPUTATION
PIPELINE
MULTI
PASS
OR
LAYERED
EXECUTION
PERFORMS
A
SINGLE
STAGE
OF
THE
PIPELINE
ON
A
BATCH
OF
DATA
ELEMENTS
BEFORE
PASSING
THE
ENTIRE
BATCH
ON
TO
THE
NEXT
STAGE
IF
YOUR
ALGORITHM
IS
SINGLE
PASS
USE
PREFETCHNTA
IF
YOUR
ALGORITHM
IS
MULTI
PASS
USE
RESOLVE
MEMORY
BANK
CONFLICT
ISSUES
MINIMIZE
MEMORY
BANK
CONFLICTS
BY
APPLYING
ARRAY
GROUPING
TO
GROUP
CONTIGUOUSLY
USED
DATA
TOGETHER
OR
BY
ALLOCATING
DATA
WITHIN
KBYTE
MEMORY
PAGES
RESOLVE
CACHE
MANAGEMENT
ISSUES
MINIMIZE
THE
DISTURBANCE
OF
TEMPORAL
DATA
HELD
WITHIN
PROCESSOR
CACHES
BY
USING
STREAMING
STORE
INSTRUCTIONS
OPTIMIZE
SOFTWARE
PREFETCH
SCHEDULING
DISTANCE
FAR
AHEAD
ENOUGH
TO
ALLOW
INTERIM
COMPUTATIONS
TO
OVERLAP
MEMORY
ACCESS
TIME
NEAR
ENOUGH
THAT
PREFETCHED
DATA
IS
NOT
REPLACED
FROM
THE
DATA
CACHE
USE
SOFTWARE
PREFETCH
CONCATENATION
ARRANGE
PREFETCHES
TO
AVOID
UNNECESSARY
PREFETCHES
AT
THE
END
OF
AN
INNER
LOOP
AND
TO
PREFETCH
THE
FIRST
FEW
ITERATIONS
OF
THE
INNER
LOOP
INSIDE
THE
NEXT
OUTER
LOOP
MINIMIZE
THE
NUMBER
OF
SOFTWARE
PREFETCHES
PREFETCH
INSTRUCTIONS
ARE
NOT
COMPLETELY
FREE
IN
TERMS
OF
BUS
CYCLES
MACHINE
CYCLES
AND
RESOURCES
EXCESSIVE
USAGE
OF
PREFETCHES
CAN
ADVERSELY
IMPACT
APPLICATION
PERFORMANCE
INTERLEAVE
PREFETCHES
WITH
COMPUTATION
INSTRUCTIONS
FOR
BEST
PERFORMANCE
SOFTWARE
PREFETCH
INSTRUCTIONS
MUST
BE
INTERSPERSED
WITH
COMPUTATIONAL
INSTRUC
TIONS
IN
THE
INSTRUCTION
SEQUENCE
RATHER
THAN
CLUSTERED
TOGETHER
HARDWARE
PREFETCHING
OF
DATA
PENTIUM
M
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
AND
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
AND
INTEL
NETBURST
MICROARCHITECTURE
PROVIDE
HARDWARE
DATA
PREFETCH
MECHANISMS
WHICH
MONITOR
APPLICATION
DATA
ACCESS
PATTERNS
AND
PREFETCHES
DATA
AUTOMATICALLY
THIS
BEHAVIOR
IS
AUTOMATIC
AND
DOES
NOT
REQUIRE
PROGRAMMER
INTERVENTION
FOR
PROCESSORS
BASED
ON
INTEL
NETBURST
MICROARCHITECTURE
CHARACTERISTICS
OF
THE
HARDWARE
DATA
PREFETCHER
ARE
IT
REQUIRES
TWO
SUCCESSIVE
CACHE
MISSES
IN
THE
LAST
LEVEL
CACHE
TO
TRIGGER
THE
MECHANISM
THESE
TWO
CACHE
MISSES
MUST
SATISFY
THE
CONDITION
THAT
STRIDES
OF
THE
CACHE
MISSES
ARE
LESS
THAN
THE
TRIGGER
DISTANCE
OF
THE
HARDWARE
PREFETCH
MECHANISM
SEE
TABLE
ATTEMPTS
TO
STAY
BYTES
AHEAD
OF
CURRENT
DATA
ACCESS
LOCATIONS
FOLLOWS
ONLY
ONE
STREAM
PER
KBYTE
PAGE
LOAD
OR
STORE
CAN
PREFETCH
UP
TO
SIMULTANEOUS
INDEPENDENT
STREAMS
FROM
EIGHT
DIFFERENT
KBYTE
REGIONS
DOES
NOT
PREFETCH
ACROSS
KBYTE
BOUNDARY
THIS
IS
INDEPENDENT
OF
PAGING
MODES
FETCHES
DATA
INTO
SECOND
THIRD
LEVEL
CACHE
DOES
NOT
PREFETCH
UC
OR
WC
MEMORY
TYPES
FOLLOWS
LOAD
AND
STORE
STREAMS
ISSUES
READ
FOR
OWNERSHIP
RFO
TRANSACTIONS
FOR
STORE
STREAMS
AND
DATA
READS
FOR
LOAD
STREAMS
OTHER
THAN
ITEMS
AND
DISCUSSED
ABOVE
MOST
OTHER
CHARACTERISTICS
ALSO
APPLY
TO
PENTIUM
M
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
THE
HARDWARE
PREFETCHER
IMPLEMENTED
IN
THE
PENTIUM
M
PROCESSOR
FETCHES
DATA
TO
A
SECOND
LEVEL
CACHE
IT
CAN
TRACK
INDEPENDENT
STREAMS
IN
THE
FORWARD
DIRECTION
AND
INDEPENDENT
STREAMS
IN
THE
BACKWARD
DIRECTION
THE
HARDWARE
PREFETCHER
OF
INTEL
CORE
SOLO
PROCESSOR
CAN
TRACK
FORWARD
STREAMS
AND
BACKWARD
STREAMS
ON
THE
INTEL
CORE
DUO
PROCESSOR
THE
HARDWARE
PREFETCHER
IN
EACH
CORE
FETCHES
DATA
INDEPENDENTLY
HARDWARE
PREFETCH
MECHANISMS
OF
PROCESSORS
BASED
ON
INTEL
CORE
MICROARCHITECTURE
ARE
DISCUSSED
IN
SECTION
AND
SECTION
DESPITE
DIFFERENCES
IN
HARDWARE
IMPLEMENTATION
TECHNIQUE
THE
OVERALL
BENEFIT
OF
HARDWARE
PREFETCHING
TO
SOFTWARE
ARE
SIMILAR
BETWEEN
INTEL
CORE
MICROARCHITECTURE
AND
PRIOR
MICROARCHITECTURES
PREFETCH
AND
CACHEABILITY
INSTRUCTIONS
THE
PREFETCH
INSTRUCTION
INSERTED
BY
THE
PROGRAMMERS
OR
COMPILERS
ACCESSES
A
MINIMUM
OF
TWO
CACHE
LINES
OF
DATA
ON
THE
PENTIUM
PROCESSOR
PRIOR
TO
THE
DATA
ACTUALLY
BEING
NEEDED
ONE
CACHE
LINE
OF
DATA
ON
THE
PENTIUM
M
PROCESSOR
THIS
HIDES
THE
LATENCY
FOR
DATA
ACCESS
IN
THE
TIME
REQUIRED
TO
PROCESS
DATA
ALREADY
RESI
DENT
IN
THE
CACHE
MANY
ALGORITHMS
CAN
PROVIDE
INFORMATION
IN
ADVANCE
ABOUT
THE
DATA
THAT
IS
TO
BE
REQUIRED
IN
CASES
WHERE
MEMORY
ACCESSES
ARE
IN
LONG
REGULAR
DATA
PATTERNS
THE
AUTOMATIC
HARDWARE
PREFETCHER
SHOULD
BE
FAVORED
OVER
SOFTWARE
PREFETCHES
THE
CACHEABILITY
CONTROL
INSTRUCTIONS
ALLOW
YOU
TO
CONTROL
DATA
CACHING
STRATEGY
IN
ORDER
TO
INCREASE
CACHE
EFFICIENCY
AND
MINIMIZE
CACHE
POLLUTION
DATA
REFERENCE
PATTERNS
CAN
BE
CLASSIFIED
AS
FOLLOWS
TEMPORAL
DATA
WILL
BE
USED
AGAIN
SOON
SPATIAL
DATA
WILL
BE
USED
IN
ADJACENT
LOCATIONS
FOR
EXAMPLE
ON
THE
SAME
CACHE
LINE
NON
TEMPORAL
DATA
WHICH
IS
REFERENCED
ONCE
AND
NOT
REUSED
IN
THE
IMMEDIATE
FUTURE
FOR
EXAMPLE
FOR
SOME
MULTIMEDIA
DATA
TYPES
AS
THE
VERTEX
BUFFER
IN
A
GRAPHICS
APPLICATION
THESE
DATA
CHARACTERISTICS
ARE
USED
IN
THE
DISCUSSIONS
THAT
FOLLOW
PREFETCH
THIS
SECTION
DISCUSSES
THE
MECHANICS
OF
THE
SOFTWARE
PREFETCH
INSTRUCTIONS
IN
GENERAL
SOFTWARE
PREFETCH
INSTRUCTIONS
SHOULD
BE
USED
TO
SUPPLEMENT
THE
PRACTICE
OF
TUNING
AN
ACCESS
PATTERN
TO
SUIT
THE
AUTOMATIC
HARDWARE
PREFETCH
MECHANISM
SOFTWARE
DATA
PREFETCH
THE
PREFETCH
INSTRUCTION
CAN
HIDE
THE
LATENCY
OF
DATA
ACCESS
IN
PERFORMANCE
CRITICAL
SECTIONS
OF
APPLICATION
CODE
BY
ALLOWING
DATA
TO
BE
FETCHED
IN
ADVANCE
OF
ACTUAL
USAGE
PREFETCH
INSTRUCTIONS
DO
NOT
CHANGE
THE
USER
VISIBLE
SEMANTICS
OF
A
PROGRAM
ALTHOUGH
THEY
MAY
IMPACT
PROGRAM
PERFORMANCE
PREFETCH
MERELY
PROVIDES
A
HINT
TO
THE
HARDWARE
AND
GENERALLY
DOES
NOT
GENERATE
EXCEPTIONS
OR
FAULTS
PREFETCH
LOADS
EITHER
NON
TEMPORAL
DATA
OR
TEMPORAL
DATA
IN
THE
SPECIFIED
CACHE
LEVEL
THIS
DATA
ACCESS
TYPE
AND
THE
CACHE
LEVEL
ARE
SPECIFIED
AS
A
HINT
DEPENDING
ON
THE
IMPLEMENTATION
THE
INSTRUCTION
FETCHES
OR
MORE
ALIGNED
BYTES
INCLUDING
THE
SPECIFIED
ADDRESS
BYTE
INTO
THE
INSTRUCTION
SPECIFIED
CACHE
LEVELS
PREFETCH
IS
IMPLEMENTATION
SPECIFIC
APPLICATIONS
NEED
TO
BE
TUNED
TO
EACH
IMPLE
MENTATION
TO
MAXIMIZE
PERFORMANCE
NOTE
USING
THE
PREFETCH
INSTRUCTION
IS
RECOMMENDED
ONLY
IF
DATA
DOES
NOT
FIT
IN
CACHE
PREFETCH
PROVIDES
A
HINT
TO
THE
HARDWARE
IT
DOES
NOT
GENERATE
EXCEPTIONS
OR
FAULTS
EXCEPT
FOR
A
FEW
SPECIAL
CASES
SEE
SECTION
PREFETCH
AND
LOAD
INSTRUCTIONS
HOWEVER
EXCESSIVE
USE
OF
PREFETCH
INSTRUCTIONS
MAY
WASTE
MEMORY
BANDWIDTH
AND
RESULT
IN
A
PERFORMANCE
PENALTY
DUE
TO
RESOURCE
CONSTRAINTS
NEVERTHELESS
PREFETCH
CAN
LESSEN
THE
OVERHEAD
OF
MEMORY
TRANSACTIONS
BY
PREVENTING
CACHE
POLLUTION
AND
BY
USING
CACHES
AND
MEMORY
EFFICIENTLY
THIS
IS
PARTIC
ULARLY
IMPORTANT
FOR
APPLICATIONS
THAT
SHARE
CRITICAL
SYSTEM
RESOURCES
SUCH
AS
THE
MEMORY
BUS
SEE
AN
EXAMPLE
IN
SECTION
VIDEO
ENCODER
PREFETCH
IS
MAINLY
DESIGNED
TO
IMPROVE
APPLICATION
PERFORMANCE
BY
HIDING
MEMORY
LATENCY
IN
THE
BACKGROUND
IF
SEGMENTS
OF
AN
APPLICATION
ACCESS
DATA
IN
A
PREDICTABLE
MANNER
FOR
EXAMPLE
USING
ARRAYS
WITH
KNOWN
STRIDES
THEY
ARE
GOOD
CANDIDATES
FOR
USING
PREFETCH
TO
IMPROVE
PERFORMANCE
USE
THE
PREFETCH
INSTRUCTIONS
IN
PREDICTABLE
MEMORY
ACCESS
PATTERNS
TIME
CONSUMING
INNERMOST
LOOPS
LOCATIONS
WHERE
THE
EXECUTION
PIPELINE
MAY
STALL
IF
DATA
IS
NOT
AVAILABLE
PREFETCH
INSTRUCTIONS
PENTIUM
PROCESSOR
IMPLEMENTATION
STREAMING
SIMD
EXTENSIONS
INCLUDE
FOUR
PREFETCH
INSTRUCTIONS
VARIANTS
ONE
NON
TEMPORAL
AND
THREE
TEMPORAL
THEY
CORRESPOND
TO
TWO
TYPES
OF
OPERATIONS
TEMPORAL
AND
NON
TEMPORAL
NOTE
AT
THE
TIME
OF
PREFETCH
IF
DATA
IS
ALREADY
FOUND
IN
A
CACHE
LEVEL
THAT
IS
CLOSER
TO
THE
PROCESSOR
THAN
THE
CACHE
LEVEL
SPECIFIED
BY
THE
INSTRUCTION
NO
DATA
MOVEMENT
OCCURS
THE
NON
TEMPORAL
INSTRUCTION
IS
PREFETCHNTA
FETCH
THE
DATA
INTO
THE
SECOND
LEVEL
CACHE
MINIMIZING
CACHE
POLLUTION
TEMPORAL
INSTRUCTIONS
ARE
FETCH
THE
DATA
INTO
ALL
CACHE
LEVELS
THAT
IS
TO
THE
SECOND
LEVEL
CACHE
FOR
THE
PENTIUM
PROCESSOR
THIS
INSTRUCTION
IS
IDENTICAL
TO
THIS
INSTRUCTION
IS
IDENTICAL
TO
PREFETCH
AND
LOAD
INSTRUCTIONS
THE
PENTIUM
PROCESSOR
HAS
A
DECOUPLED
EXECUTION
AND
MEMORY
ARCHITECTURE
THAT
ALLOWS
INSTRUCTIONS
TO
BE
EXECUTED
INDEPENDENTLY
WITH
MEMORY
ACCESSES
IF
THERE
ARE
NO
DATA
AND
RESOURCE
DEPENDENCIES
PROGRAMS
OR
COMPILERS
CAN
USE
DUMMY
LOAD
INSTRUCTIONS
TO
IMITATE
PREFETCH
FUNCTIONALITY
BUT
PRELOADING
IS
NOT
COMPLETELY
EQUIVALENT
TO
USING
PREFETCH
INSTRUCTIONS
PREFETCH
PROVIDES
GREATER
PERFORMANCE
THAN
PRELOADING
CURRENTLY
PREFETCH
PROVIDES
GREATER
PERFORMANCE
THAN
PRELOADING
BECAUSE
HAS
NO
DESTINATION
REGISTER
IT
ONLY
UPDATES
CACHE
LINES
DOES
NOT
STALL
THE
NORMAL
INSTRUCTION
RETIREMENT
DOES
NOT
AFFECT
THE
FUNCTIONAL
BEHAVIOR
OF
THE
PROGRAM
HAS
NO
CACHE
LINE
SPLIT
ACCESSES
DOES
NOT
CAUSE
EXCEPTIONS
EXCEPT
WHEN
THE
LOCK
PREFIX
IS
USED
THE
LOCK
PREFIX
IS
NOT
A
VALID
PREFIX
FOR
USE
WITH
PREFETCH
DOES
NOT
COMPLETE
ITS
OWN
EXECUTION
IF
THAT
WOULD
CAUSE
A
FAULT
CURRENTLY
THE
ADVANTAGE
OF
PREFETCH
OVER
PRELOADING
INSTRUCTIONS
ARE
PROCESSOR
SPECIFIC
THIS
MAY
CHANGE
IN
THE
FUTURE
THERE
ARE
CASES
WHERE
A
PREFETCH
WILL
NOT
PERFORM
THE
DATA
PREFETCH
THESE
INCLUDE
PREFETCH
CAUSES
A
DTLB
DATA
TRANSLATION
LOOKASIDE
BUFFER
MISS
THIS
APPLIES
TO
PENTIUM
PROCESSORS
WITH
CPUID
SIGNATURE
CORRESPONDING
TO
FAMILY
MODEL
OR
PREFETCH
RESOLVES
DTLB
MISSES
AND
FETCHES
DATA
ON
PENTIUM
PROCESSORS
WITH
CPUID
SIGNATURE
CORRESPONDING
TO
FAMILY
MODEL
AN
ACCESS
TO
THE
SPECIFIED
ADDRESS
THAT
CAUSES
A
FAULT
EXCEPTION
IF
THE
MEMORY
SUBSYSTEM
RUNS
OUT
OF
REQUEST
BUFFERS
BETWEEN
THE
FIRST
LEVEL
CACHE
AND
THE
SECOND
LEVEL
CACHE
PREFETCH
TARGETS
AN
UNCACHEABLE
MEMORY
REGION
FOR
EXAMPLE
USWC
AND
UC
THE
LOCK
PREFIX
IS
USED
THIS
CAUSES
AN
INVALID
OPCODE
EXCEPTION
CACHEABILITY
CONTROL
THIS
SECTION
COVERS
THE
MECHANICS
OF
CACHEABILITY
CONTROL
INSTRUCTIONS
THE
NON
TEMPORAL
STORE
INSTRUCTIONS
THIS
SECTION
DESCRIBES
THE
BEHAVIOR
OF
STREAMING
STORES
AND
REITERATES
SOME
OF
THE
INFORMATION
PRESENTED
IN
THE
PREVIOUS
SECTION
IN
STREAMING
SIMD
EXTENSIONS
THE
MOVNTPS
MOVNTPD
MOVNTQ
MOVNTDQ
MOVNTI
MASKMOVQ
AND
MASKMOVDQU
INSTRUCTIONS
ARE
STREAMING
NON
TEMPORAL
STORES
WITH
REGARD
TO
MEMORY
CHARACTERISTICS
AND
ORDERING
THEY
ARE
SIMILAR
TO
THE
WRITE
COMBINING
WC
MEMORY
TYPE
WRITE
COMBINING
SUCCESSIVE
WRITES
TO
THE
SAME
CACHE
LINE
ARE
COMBINED
WRITE
COLLAPSING
SUCCESSIVE
WRITES
TO
THE
SAME
BYTE
RESULT
IN
ONLY
THE
LAST
WRITE
BEING
VISIBLE
WEAKLY
ORDERED
NO
ORDERING
IS
PRESERVED
BETWEEN
WC
STORES
OR
BETWEEN
WC
STORES
AND
OTHER
LOADS
OR
STORES
UNCACHEABLE
AND
NOT
WRITE
ALLOCATING
STORED
DATA
IS
WRITTEN
AROUND
THE
CACHE
AND
WILL
NOT
GENERATE
A
READ
FOR
OWNERSHIP
BUS
REQUEST
FOR
THE
CORRE
SPONDING
CACHE
LINE
FENCING
BECAUSE
STREAMING
STORES
ARE
WEAKLY
ORDERED
A
FENCING
OPERATION
IS
REQUIRED
TO
ENSURE
THAT
THE
STORED
DATA
IS
FLUSHED
FROM
THE
PROCESSOR
TO
MEMORY
FAILURE
TO
USE
AN
APPROPRIATE
FENCE
MAY
RESULT
IN
DATA
BEING
TRAPPED
WITHIN
THE
PROCESSOR
AND
WILL
PREVENT
VISIBILITY
OF
THIS
DATA
BY
OTHER
PROCESSORS
OR
SYSTEM
AGENTS
WC
STORES
REQUIRE
SOFTWARE
TO
ENSURE
COHERENCE
OF
DATA
BY
PERFORMING
THE
FENCING
OPERATION
SEE
SECTION
FENCE
INSTRUCTIONS
STREAMING
NON
TEMPORAL
STORES
STREAMING
STORES
CAN
IMPROVE
PERFORMANCE
BY
INCREASING
STORE
BANDWIDTH
IF
THE
BYTES
THAT
FIT
WITHIN
A
CACHE
LINE
ARE
WRITTEN
CONSECUTIVELY
SINCE
THEY
DO
NOT
REQUIRE
READ
FOR
OWNERSHIP
BUS
REQUESTS
AND
BYTES
ARE
COMBINED
INTO
A
SINGLE
BUS
WRITE
TRANSACTION
REDUCING
DISTURBANCE
OF
FREQUENTLY
USED
CACHED
TEMPORAL
DATA
SINCE
THEY
WRITE
AROUND
THE
PROCESSOR
CACHES
STREAMING
STORES
ALLOW
CROSS
ALIASING
OF
MEMORY
TYPES
FOR
A
GIVEN
MEMORY
REGION
FOR
INSTANCE
A
REGION
MAY
BE
MAPPED
AS
WRITE
BACK
WB
USING
PAGE
ATTRIBUTE
TABLES
PAT
OR
MEMORY
TYPE
RANGE
REGISTERS
MTRRS
AND
YET
IS
WRITTEN
USING
A
STREAMING
STORE
MEMORY
TYPE
AND
NON
TEMPORAL
STORES
MEMORY
TYPE
CAN
TAKE
PRECEDENCE
OVER
A
NON
TEMPORAL
HINT
LEADING
TO
THE
FOLLOWING
CONSIDERATIONS
IF
THE
PROGRAMMER
SPECIFIES
A
NON
TEMPORAL
STORE
TO
STRONGLY
ORDERED
UNCACHEABLE
MEMORY
FOR
EXAMPLE
UNCACHEABLE
UC
OR
WRITE
PROTECT
WP
MEMORY
TYPES
THEN
THE
STORE
BEHAVES
LIKE
AN
UNCACHEABLE
STORE
THE
NON
TEMPORAL
HINT
IS
IGNORED
AND
THE
MEMORY
TYPE
FOR
THE
REGION
IS
RETAINED
IF
THE
PROGRAMMER
SPECIFIES
THE
WEAKLY
ORDERED
UNCACHEABLE
MEMORY
TYPE
OF
WRITE
COMBINING
WC
THEN
THE
NON
TEMPORAL
STORE
AND
THE
REGION
HAVE
THE
SAME
SEMANTICS
AND
THERE
IS
NO
CONFLICT
IF
THE
PROGRAMMER
SPECIFIES
A
NON
TEMPORAL
STORE
TO
CACHEABLE
MEMORY
FOR
EXAMPLE
WRITE
BACK
WB
OR
WRITE
THROUGH
WT
MEMORY
TYPES
TWO
CASES
MAY
RESULT
CASE
IF
THE
DATA
IS
PRESENT
IN
THE
CACHE
HIERARCHY
THE
INSTRUCTION
WILL
ENSURE
CONSISTENCY
A
PARTICULAR
PROCESSOR
MAY
CHOOSE
DIFFERENT
WAYS
TO
IMPLEMENT
THIS
THE
FOLLOWING
APPROACHES
ARE
PROBABLE
A
UPDATING
DATA
IN
PLACE
IN
THE
CACHE
HIERARCHY
WHILE
PRESERVING
THE
MEMORY
TYPE
SEMANTICS
ASSIGNED
TO
THAT
REGION
OR
B
EVICTING
THE
DATA
FROM
THE
CACHES
AND
WRITING
THE
NEW
NON
TEMPORAL
DATA
TO
MEMORY
WITH
WC
SEMANTICS
THE
APPROACHES
SEPARATE
OR
COMBINED
CAN
BE
DIFFERENT
FOR
FUTURE
PROCESSORS
PENTIUM
INTEL
CORE
SOLO
AND
INTEL
CORE
DUO
PROCESSORS
IMPLEMENT
THE
LATTER
POLICY
OF
EVICTING
DATA
FROM
ALL
PROCESSOR
CACHES
THE
PENTIUM
M
PROCESSOR
IMPLEMENTS
A
COMBINATION
OF
BOTH
APPROACHES
IF
THE
STREAMING
STORE
HITS
A
LINE
THAT
IS
PRESENT
IN
THE
FIRST
LEVEL
CACHE
THE
STORE
DATA
IS
COMBINED
IN
PLACE
WITHIN
THE
FIRST
LEVEL
CACHE
IF
THE
STREAMING
STORE
HITS
A
LINE
PRESENT
IN
THE
SECOND
LEVEL
THE
LINE
AND
STORED
DATA
IS
FLUSHED
FROM
THE
SECOND
LEVEL
TO
SYSTEM
MEMORY
CASE
IF
THE
DATA
IS
NOT
PRESENT
IN
THE
CACHE
HIERARCHY
AND
THE
DESTINATION
REGION
IS
MAPPED
AS
WB
OR
WT
THE
TRANSACTION
WILL
BE
WEAKLY
ORDERED
AND
IS
SUBJECT
TO
ALL
WC
MEMORY
SEMANTICS
THIS
NON
TEMPORAL
STORE
WILL
NOT
WRITE
ALLOCATE
DIFFERENT
IMPLEMENTATIONS
MAY
CHOOSE
TO
COLLAPSE
AND
COMBINE
SUCH
STORES
WRITE
COMBINING
GENERALLY
WC
SEMANTICS
REQUIRE
SOFTWARE
TO
ENSURE
COHERENCE
WITH
RESPECT
TO
OTHER
PROCESSORS
AND
OTHER
SYSTEM
AGENTS
SUCH
AS
GRAPHICS
CARDS
APPROPRIATE
USE
OF
SYNCHRONIZATION
AND
A
FENCING
OPERATION
MUST
BE
PERFORMED
FOR
PRODUCER
CONSUMER
USAGE
MODELS
SEE
SECTION
FENCE
INSTRUCTIONS
FENCING
ENSURES
THAT
ALL
SYSTEM
AGENTS
HAVE
GLOBAL
VISIBILITY
OF
THE
STORED
DATA
FOR
INSTANCE
FAILURE
TO
FENCE
MAY
RESULT
IN
A
WRITTEN
CACHE
LINE
STAYING
WITHIN
A
PROCESSOR
AND
THE
LINE
WOULD
NOT
BE
VISIBLE
TO
OTHER
AGENTS
FOR
PROCESSORS
WHICH
IMPLEMENT
NON
TEMPORAL
STORES
BY
UPDATING
DATA
IN
PLACE
THAT
ALREADY
RESIDES
IN
THE
CACHE
HIERARCHY
THE
DESTINATION
REGION
SHOULD
ALSO
BE
MAPPED
AS
WC
OTHERWISE
IF
MAPPED
AS
WB
OR
WT
THERE
IS
A
POTENTIAL
FOR
SPECULATIVE
PROCESSOR
READS
TO
BRING
THE
DATA
INTO
THE
CACHES
IN
SUCH
A
CASE
NON
TEMPORAL
STORES
WOULD
THEN
UPDATE
IN
PLACE
AND
DATA
WOULD
NOT
BE
FLUSHED
FROM
THE
PROCESSOR
BY
A
SUBSEQUENT
FENCING
OPERATION
THE
MEMORY
TYPE
VISIBLE
ON
THE
BUS
IN
THE
PRESENCE
OF
MEMORY
TYPE
ALIASING
IS
IMPLE
MENTATION
SPECIFIC
AS
ONE
EXAMPLE
THE
MEMORY
TYPE
WRITTEN
TO
THE
BUS
MAY
REFLECT
THE
MEMORY
TYPE
FOR
THE
FIRST
STORE
TO
THE
LINE
AS
SEEN
IN
PROGRAM
ORDER
OTHER
ALTER
NATIVES
ARE
POSSIBLE
THIS
BEHAVIOR
SHOULD
BE
CONSIDERED
RESERVED
AND
DEPENDENCE
ON
THE
BEHAVIOR
OF
ANY
PARTICULAR
IMPLEMENTATION
RISKS
FUTURE
INCOMPATIBILITY
STREAMING
STORE
USAGE
MODELS
THE
TWO
PRIMARY
USAGE
DOMAINS
FOR
STREAMING
STORE
ARE
COHERENT
REQUESTS
AND
NON
COHERENT
REQUESTS
COHERENT
REQUESTS
COHERENT
REQUESTS
ARE
NORMAL
LOADS
AND
STORES
TO
SYSTEM
MEMORY
WHICH
MAY
ALSO
HIT
CACHE
LINES
PRESENT
IN
ANOTHER
PROCESSOR
IN
A
MULTIPROCESSOR
ENVIRONMENT
WITH
COHERENT
REQUESTS
A
STREAMING
STORE
CAN
BE
USED
IN
THE
SAME
WAY
AS
A
REGULAR
STORE
THAT
HAS
BEEN
MAPPED
WITH
A
WC
MEMORY
TYPE
PAT
OR
MTRR
AN
SFENCE
INSTRUC
TION
MUST
BE
USED
WITHIN
A
PRODUCER
CONSUMER
USAGE
MODEL
IN
ORDER
TO
ENSURE
COHER
ENCY
AND
VISIBILITY
OF
DATA
BETWEEN
PROCESSORS
WITHIN
A
SINGLE
PROCESSOR
SYSTEM
THE
CPU
CAN
ALSO
RE
READ
THE
SAME
MEMORY
LOCA
TION
AND
BE
ASSURED
OF
COHERENCE
THAT
IS
A
SINGLE
CONSISTENT
VIEW
OF
THIS
MEMORY
LOCATION
THE
SAME
IS
TRUE
FOR
A
MULTIPROCESSOR
MP
SYSTEM
ASSUMING
AN
ACCEPTED
MP
SOFTWARE
PRODUCER
CONSUMER
SYNCHRONIZATION
POLICY
IS
EMPLOYED
NON
COHERENT
REQUESTS
NON
COHERENT
REQUESTS
ARISE
FROM
AN
I
O
DEVICE
SUCH
AS
AN
AGP
GRAPHICS
CARD
THAT
READS
OR
WRITES
SYSTEM
MEMORY
USING
NON
COHERENT
REQUESTS
WHICH
ARE
NOT
REFLECTED
ON
THE
PROCESSOR
BUS
AND
THUS
WILL
NOT
QUERY
THE
PROCESSOR
CACHES
AN
SFENCE
INSTRUCTION
MUST
BE
USED
WITHIN
A
PRODUCER
CONSUMER
USAGE
MODEL
IN
ORDER
TO
ENSURE
COHERENCY
AND
VISIBILITY
OF
DATA
BETWEEN
PROCESSORS
IN
THIS
CASE
IF
THE
PROCESSOR
IS
WRITING
DATA
TO
THE
I
O
DEVICE
A
STREAMING
STORE
CAN
BE
USED
WITH
A
PROCESSOR
WITH
ANY
BEHAVIOR
OF
CASE
SECTION
ONLY
IF
THE
REGION
HAS
ALSO
BEEN
MAPPED
WITH
A
WC
MEMORY
TYPE
PAT
MTRR
NOTE
FAILURE
TO
MAP
THE
REGION
AS
WC
MAY
ALLOW
THE
LINE
TO
BE
SPECULATIVELY
READ
INTO
THE
PROCESSOR
CACHES
VIA
THE
WRONG
PATH
OF
A
MISPREDICTED
BRANCH
IN
CASE
THE
REGION
IS
NOT
MAPPED
AS
WC
THE
STREAMING
MIGHT
UPDATE
IN
PLACE
IN
THE
CACHE
AND
A
SUBSEQUENT
SFENCE
WOULD
NOT
RESULT
IN
THE
DATA
BEING
WRITTEN
TO
SYSTEM
MEMORY
EXPLICITLY
MAPPING
THE
REGION
AS
WC
IN
THIS
CASE
ENSURES
THAT
ANY
DATA
READ
FROM
THIS
REGION
WILL
NOT
BE
PLACED
IN
THE
PROCESSOR
CACHES
A
READ
OF
THIS
MEMORY
LOCATION
BY
A
NON
COHERENT
I
O
DEVICE
WOULD
RETURN
INCORRECT
OUT
OF
DATE
RESULTS
FOR
A
PROCESSOR
WHICH
SOLELY
IMPLEMENTS
CASE
SECTION
A
STREAMING
STORE
CAN
BE
USED
IN
THIS
NON
COHERENT
DOMAIN
WITHOUT
REQUIRING
THE
MEMORY
REGION
TO
ALSO
BE
MAPPED
AS
WB
SINCE
ANY
CACHED
DATA
WILL
BE
FLUSHED
TO
MEMORY
BY
THE
STREAMING
STORE
STREAMING
STORE
INSTRUCTION
DESCRIPTIONS
MOVNTQ
MOVNTDQ
NON
TEMPORAL
STORE
OF
PACKED
INTEGER
IN
AN
MMX
TECHNOLOGY
OR
STREAMING
SIMD
EXTENSIONS
REGISTER
STORE
DATA
FROM
A
REGISTER
TO
MEMORY
THEY
ARE
IMPLICITLY
WEAKLY
ORDERED
DO
NO
WRITE
ALLOCATE
AND
SO
MINIMIZE
CACHE
POLLUTION
MOVNTPS
NON
TEMPORAL
STORE
OF
PACKED
SINGLE
PRECISION
FLOATING
POINT
IS
SIMILAR
TO
MOVNTQ
IT
STORES
DATA
FROM
A
STREAMING
SIMD
EXTENSIONS
REGISTER
TO
MEMORY
IN
BYTE
GRANULARITY
UNLIKE
MOVNTQ
THE
MEMORY
ADDRESS
MUST
BE
ALIGNED
TO
A
BYTE
BOUNDARY
OR
A
GENERAL
PROTECTION
EXCEPTION
WILL
OCCUR
THE
INSTRUCTION
IS
IMPLICITLY
WEAKLY
ORDERED
DOES
NOT
WRITE
ALLOCATE
AND
THUS
MINIMIZES
CACHE
POLLU
TION
MASKMOVQ
MASKMOVDQU
NON
TEMPORAL
BYTE
MASK
STORE
OF
PACKED
INTEGER
IN
AN
MMX
TECHNOLOGY
OR
STREAMING
SIMD
EXTENSIONS
REGISTER
STORE
DATA
FROM
A
REGISTER
TO
THE
LOCATION
SPECIFIED
BY
THE
EDI
REGISTER
THE
MOST
SIGNIFICANT
BIT
IN
EACH
BYTE
OF
THE
SECOND
MASK
REGISTER
IS
USED
TO
SELECTIVELY
WRITE
THE
DATA
OF
THE
FIRST
REGISTER
ON
A
PER
BYTE
BASIS
THE
INSTRUCTIONS
ARE
IMPLICITLY
WEAKLY
ORDERED
THAT
IS
SUCCESSIVE
STORES
MAY
NOT
WRITE
MEMORY
IN
ORIGINAL
PROGRAM
ORDER
DO
NOT
WRITE
ALLOCATE
AND
THUS
MINIMIZE
CACHE
POLLUTION
THE
STREAMING
LOAD
INSTRUCTION
INTRODUCES
THE
MOVNTDQA
INSTRUCTION
MOVNTDQA
LOADS
BYTES
FROM
MEMORY
USING
A
NON
TEMPORAL
HINT
IF
THE
MEMORY
SOURCE
IS
WC
TYPE
FOR
WC
MEMORY
TYPE
THE
NON
TEMPORAL
HINT
MAY
BE
IMPLEMENTED
BY
LOADING
INTO
A
TEMPORARY
INTERNAL
BUFFER
WITH
THE
EQUIVALENT
OF
AN
ALIGNED
CACHE
LINE
WITHOUT
FILLING
THIS
DATA
TO
THE
CACHE
SUBSEQUENT
MOVNTDQA
READS
TO
UNREAD
PORTIONS
OF
THE
BUFFERED
WC
DATA
WILL
CAUSE
BYTES
OF
DATA
TRANSFERRED
FROM
THE
TEMPORARY
INTERNAL
BUFFER
TO
AN
XMM
REGISTER
IF
DATA
IS
AVAILABLE
IF
USED
APPROPRIATELY
MOVNTDQA
CAN
HELP
SOFTWARE
ACHIEVE
SIGNIFICANTLY
HIGHER
THROUGHPUT
WHEN
LOADING
DATA
IN
WC
MEMORY
REGION
INTO
THE
PROCESSOR
THAN
OTHER
MEANS
CHAPTER
PROVIDES
A
REFERENCE
TO
AN
APPLICATION
NOTE
ON
USING
MOVNTDQA
ADDI
TIONAL
INFORMATION
AND
REQUIREMENTS
TO
USE
MOVNTDQA
APPROPRIATELY
CAN
BE
FOUND
IN
CHAPTER
PROGRAMMING
WITH
AND
OF
INTEL
AND
IA
ARCHITECTURES
SOFTWARE
DEVELOPER
MANUAL
VOLUME
AND
THE
INSTRUCTION
REFERENCE
PAGES
OF
MOVNTDQA
IN
INTEL
AND
IA
ARCHITECTURES
SOFTWARE
DEVELOPER
MANUAL
VOLUME
FENCE
INSTRUCTIONS
THE
FOLLOWING
FENCE
INSTRUCTIONS
ARE
AVAILABLE
SFENCE
LFENCE
AND
MFENCE
SFENCE
INSTRUCTION
THE
SFENCE
STORE
FENCE
INSTRUCTION
MAKES
IT
POSSIBLE
FOR
EVERY
STORE
INSTRUC
TION
THAT
PRECEDES
AN
SFENCE
IN
PROGRAM
ORDER
TO
BE
GLOBALLY
VISIBLE
BEFORE
ANY
STORE
THAT
FOLLOWS
THE
SFENCE
SFENCE
PROVIDES
AN
EFFICIENT
WAY
OF
ENSURING
ORDERING
BETWEEN
ROUTINES
THAT
PRODUCE
WEAKLY
ORDERED
RESULTS
THE
USE
OF
WEAKLY
ORDERED
MEMORY
TYPES
CAN
BE
IMPORTANT
UNDER
CERTAIN
DATA
SHARING
RELATIONSHIPS
SUCH
AS
A
PRODUCER
CONSUMER
RELATIONSHIP
USING
WEAKLY
ORDERED
MEMORY
CAN
MAKE
ASSEMBLING
THE
DATA
MORE
EFFICIENT
BUT
CARE
MUST
BE
TAKEN
TO
ENSURE
THAT
THE
CONSUMER
OBTAINS
THE
DATA
THAT
THE
PRODUCER
INTENDED
TO
SEE
SOME
COMMON
USAGE
MODELS
MAY
BE
AFFECTED
BY
WEAKLY
ORDERED
STORES
EXAMPLES
ARE
LIBRARY
FUNCTIONS
WHICH
USE
WEAKLY
ORDERED
MEMORY
TO
WRITE
RESULTS
COMPILER
GENERATED
CODE
WHICH
ALSO
BENEFITS
FROM
WRITING
WEAKLY
ORDERED
RESULTS
HAND
CRAFTED
CODE
THE
DEGREE
TO
WHICH
A
CONSUMER
OF
DATA
KNOWS
THAT
THE
DATA
IS
WEAKLY
ORDERED
CAN
VARY
FOR
DIFFERENT
CASES
AS
A
RESULT
SFENCE
SHOULD
BE
USED
TO
ENSURE
ORDERING
BETWEEN
ROUTINES
THAT
PRODUCE
WEAKLY
ORDERED
DATA
AND
ROUTINES
THAT
CONSUME
THIS
DATA
LFENCE
INSTRUCTION
THE
LFENCE
LOAD
FENCE
INSTRUCTION
MAKES
IT
POSSIBLE
FOR
EVERY
LOAD
INSTRUCTION
THAT
PRECEDES
THE
LFENCE
INSTRUCTION
IN
PROGRAM
ORDER
TO
BE
GLOBALLY
VISIBLE
BEFORE
ANY
LOAD
INSTRUCTION
THAT
FOLLOWS
THE
LFENCE
THE
LFENCE
INSTRUCTION
PROVIDES
A
MEANS
OF
SEGREGATING
LOAD
INSTRUCTIONS
FROM
OTHER
LOADS
MFENCE
INSTRUCTION
THE
MFENCE
MEMORY
FENCE
INSTRUCTION
MAKES
IT
POSSIBLE
FOR
EVERY
LOAD
STORE
INSTRUCTION
PRECEDING
MFENCE
IN
PROGRAM
ORDER
TO
BE
GLOBALLY
VISIBLE
BEFORE
ANY
LOAD
STORE
FOLLOWING
MFENCE
MFENCE
PROVIDES
A
MEANS
OF
SEGREGATING
CERTAIN
MEMORY
INSTRUCTIONS
FROM
OTHER
MEMORY
REFERENCES
THE
USE
OF
A
LFENCE
AND
SFENCE
IS
NOT
EQUIVALENT
TO
THE
USE
OF
A
MFENCE
SINCE
THE
LOAD
AND
STORE
FENCES
ARE
NOT
ORDERED
WITH
RESPECT
TO
EACH
OTHER
IN
OTHER
WORDS
THE
LOAD
FENCE
CAN
BE
EXECUTED
BEFORE
PRIOR
STORES
AND
THE
STORE
FENCE
CAN
BE
EXECUTED
BEFORE
PRIOR
LOADS
MFENCE
SHOULD
BE
USED
WHENEVER
THE
CACHE
LINE
FLUSH
INSTRUCTION
CLFLUSH
IS
USED
TO
ENSURE
THAT
SPECULATIVE
MEMORY
REFERENCES
GENERATED
BY
THE
PROCESSOR
DO
NOT
INTERFERE
WITH
THE
FLUSH
SEE
SECTION
CLFLUSH
INSTRUCTION
CLFLUSH
INSTRUCTION
THE
CLFLUSH
INSTRUCTION
INVALIDATES
THE
CACHE
LINE
ASSOCIATED
WITH
THE
LINEAR
ADDRESS
THAT
CONTAIN
THE
BYTE
ADDRESS
OF
THE
MEMORY
LOCATION
IN
ALL
LEVELS
OF
THE
PROCESSOR
CACHE
HIERARCHY
DATA
AND
INSTRUCTION
THIS
INVALIDATION
IS
BROADCAST
THROUGHOUT
THE
COHERENCE
DOMAIN
IF
AT
ANY
LEVEL
OF
THE
CACHE
HIERARCHY
A
LINE
IS
INCONSISTENT
WITH
MEMORY
DIRTY
IT
IS
WRITTEN
TO
MEMORY
BEFORE
INVALIDATION
OTHER
CHARACTERISTICS
INCLUDE
THE
DATA
SIZE
AFFECTED
IS
THE
CACHE
COHERENCY
SIZE
WHICH
IS
BYTES
ON
PENTIUM
PROCESSOR
THE
MEMORY
ATTRIBUTE
OF
THE
PAGE
CONTAINING
THE
AFFECTED
LINE
HAS
NO
EFFECT
ON
THE
BEHAVIOR
OF
THIS
INSTRUCTION
THE
CLFLUSH
INSTRUCTION
CAN
BE
USED
AT
ALL
PRIVILEGE
LEVELS
AND
IS
SUBJECT
TO
ALL
PERMISSION
CHECKING
AND
FAULTS
ASSOCIATED
WITH
A
BYTE
LOAD
CLFLUSH
IS
AN
UNORDERED
OPERATION
WITH
RESPECT
TO
OTHER
MEMORY
TRAFFIC
INCLUDING
OTHER
CLFLUSH
INSTRUCTIONS
SOFTWARE
SHOULD
USE
A
MEMORY
FENCE
FOR
CASES
WHERE
ORDERING
IS
A
CONCERN
AS
AN
EXAMPLE
CONSIDER
A
VIDEO
USAGE
MODEL
WHERE
A
VIDEO
CAPTURE
DEVICE
IS
USING
NON
COHERENT
AGP
ACCESSES
TO
WRITE
A
CAPTURE
STREAM
DIRECTLY
TO
SYSTEM
MEMORY
SINCE
THESE
NON
COHERENT
WRITES
ARE
NOT
BROADCAST
ON
THE
PROCESSOR
BUS
THEY
WILL
NOT
FLUSH
COPIES
OF
THE
SAME
LOCATIONS
THAT
RESIDE
IN
THE
PROCESSOR
CACHES
AS
A
RESULT
BEFORE
THE
PROCESSOR
RE
READS
THE
CAPTURE
BUFFER
IT
SHOULD
USE
CLFLUSH
TO
ENSURE
THAT
STALE
COPIES
OF
THE
CAPTURE
BUFFER
ARE
FLUSHED
FROM
THE
PROCESSOR
CACHES
DUE
TO
SPECULATIVE
READS
THAT
MAY
BE
GENERATED
BY
THE
PROCESSOR
IT
IS
IMPORTANT
TO
OBSERVE
APPROPRIATE
FENCING
USING
MFENCE
EXAMPLE
PROVIDES
PSEUDO
CODE
FOR
CLFLUSH
USAGE
EXAMPLE
PSEUDO
CODE
USING
CLFLUSH
MEMORY
OPTIMIZATION
USING
PREFETCH
THE
PENTIUM
PROCESSOR
HAS
TWO
MECHANISMS
FOR
DATA
PREFETCH
SOFTWARE
CONTROLLED
PREFETCH
AND
AN
AUTOMATIC
HARDWARE
PREFETCH
SOFTWARE
CONTROLLED
PREFETCH
THE
SOFTWARE
CONTROLLED
PREFETCH
IS
ENABLED
USING
THE
FOUR
PREFETCH
INSTRUCTIONS
INTRODUCED
WITH
STREAMING
SIMD
EXTENSIONS
INSTRUCTIONS
THESE
INSTRUCTIONS
ARE
HINTS
TO
BRING
A
CACHE
LINE
OF
DATA
IN
TO
VARIOUS
LEVELS
AND
MODES
IN
THE
CACHE
HIER
ARCHY
THE
SOFTWARE
CONTROLLED
PREFETCH
IS
NOT
INTENDED
FOR
PREFETCHING
CODE
USING
IT
CAN
INCUR
SIGNIFICANT
PENALTIES
ON
A
MULTIPROCESSOR
SYSTEM
WHEN
CODE
IS
SHARED
SOFTWARE
PREFETCHING
HAS
THE
FOLLOWING
CHARACTERISTICS
CAN
HANDLE
IRREGULAR
ACCESS
PATTERNS
WHICH
DO
NOT
TRIGGER
THE
HARDWARE
PREFETCHER
CAN
USE
LESS
BUS
BANDWIDTH
THAN
HARDWARE
PREFETCHING
SEE
BELOW
SOFTWARE
PREFETCHES
MUST
BE
ADDED
TO
NEW
CODE
AND
DO
NOT
BENEFIT
EXISTING
APPLICATIONS
HARDWARE
PREFETCH
AUTOMATIC
HARDWARE
PREFETCH
CAN
BRING
CACHE
LINES
INTO
THE
UNIFIED
LAST
LEVEL
CACHE
BASED
ON
PRIOR
DATA
MISSES
IT
WILL
ATTEMPT
TO
PREFETCH
TWO
CACHE
LINES
AHEAD
OF
THE
PREFETCH
STREAM
CHARACTERISTICS
OF
THE
HARDWARE
PREFETCHER
ARE
IT
REQUIRES
SOME
REGULARITY
IN
THE
DATA
ACCESS
PATTERNS
IF
A
DATA
ACCESS
PATTERN
HAS
CONSTANT
STRIDE
HARDWARE
PREFETCHING
IS
EFFECTIVE
IF
THE
ACCESS
STRIDE
IS
LESS
THAN
HALF
OF
THE
TRIGGER
DISTANCE
OF
HARDWARE
PREFETCHER
SEE
TABLE
IF
THE
ACCESS
STRIDE
IS
NOT
CONSTANT
THE
AUTOMATIC
HARDWARE
PREFETCHER
CAN
MASK
MEMORY
LATENCY
IF
THE
STRIDES
OF
TWO
SUCCESSIVE
CACHE
MISSES
ARE
LESS
THAN
THE
TRIGGER
THRESHOLD
DISTANCE
SMALL
STRIDE
MEMORY
TRAFFIC
THE
AUTOMATIC
HARDWARE
PREFETCHER
IS
MOST
EFFECTIVE
IF
THE
STRIDES
OF
TWO
SUCCESSIVE
CACHE
MISSES
REMAIN
LESS
THAN
THE
TRIGGER
THRESHOLD
DISTANCE
AND
CLOSE
TO
BYTES
THERE
IS
A
START
UP
PENALTY
BEFORE
THE
PREFETCHER
TRIGGERS
AND
THERE
MAY
BE
FETCHES
AN
ARRAY
FINISHES
FOR
SHORT
ARRAYS
OVERHEAD
CAN
REDUCE
EFFECTIVENESS
THE
HARDWARE
PREFETCHER
REQUIRES
A
COUPLE
MISSES
BEFORE
IT
STARTS
OPERATING
HARDWARE
PREFETCHING
GENERATES
A
REQUEST
FOR
DATA
BEYOND
THE
END
OF
AN
ARRAY
WHICH
IS
NOT
BE
UTILIZED
THIS
BEHAVIOR
WASTES
BUS
BANDWIDTH
IN
ADDITION
THIS
BEHAVIOR
RESULTS
IN
A
START
UP
PENALTY
WHEN
FETCHING
THE
BEGINNING
OF
THE
NEXT
ARRAY
SOFTWARE
PREFETCHING
MAY
RECOGNIZE
AND
HANDLE
THESE
CASES
IT
WILL
NOT
PREFETCH
ACROSS
A
KBYTE
PAGE
BOUNDARY
A
PROGRAM
HAS
TO
INITIATE
DEMAND
LOADS
FOR
THE
NEW
PAGE
BEFORE
THE
HARDWARE
PREFETCHER
STARTS
PREFETCHING
FROM
THE
NEW
PAGE
THE
HARDWARE
PREFETCHER
MAY
CONSUME
EXTRA
SYSTEM
BANDWIDTH
IF
THE
APPLI
CATION
MEMORY
TRAFFIC
HAS
SIGNIFICANT
PORTIONS
WITH
STRIDES
OF
CACHE
MISSES
GREATER
THAN
THE
TRIGGER
DISTANCE
THRESHOLD
OF
HARDWARE
PREFETCH
LARGE
STRIDE
MEMORY
TRAFFIC
THE
EFFECTIVENESS
WITH
EXISTING
APPLICATIONS
DEPENDS
ON
THE
PROPORTIONS
OF
SMALL
STRIDE
VERSUS
LARGE
STRIDE
ACCESSES
IN
THE
APPLICATION
MEMORY
TRAFFIC
AN
APPLICATION
WITH
A
PREPONDERANCE
OF
SMALL
STRIDE
MEMORY
TRAFFIC
WITH
GOOD
TEMPORAL
LOCALITY
WILL
BENEFIT
GREATLY
FROM
THE
AUTOMATIC
HARDWARE
PREFETCHER
IN
SOME
SITUATIONS
MEMORY
TRAFFIC
CONSISTING
OF
A
PREPONDERANCE
OF
LARGE
STRIDE
CACHE
MISSES
CAN
BE
TRANSFORMED
BY
RE
ARRANGEMENT
OF
DATA
ACCESS
SEQUENCES
TO
ALTER
THE
CONCENTRATION
OF
SMALL
STRIDE
CACHE
MISSES
AT
THE
EXPENSE
OF
LARGE
STRIDE
CACHE
MISSES
TO
TAKE
ADVANTAGE
OF
THE
AUTOMATIC
HARDWARE
PREFETCHER
EXAMPLE
OF
EFFECTIVE
LATENCY
REDUCTION
WITH
HARDWARE
PREFETCH
CONSIDER
THE
SITUATION
THAT
AN
ARRAY
IS
POPULATED
WITH
DATA
CORRESPONDING
TO
A
CONSTANT
ACCESS
STRIDE
CIRCULAR
POINTER
CHASING
SEQUENCE
SEE
EXAMPLE
THE
POTENTIAL
OF
EMPLOYING
THE
AUTOMATIC
HARDWARE
PREFETCHING
MECHANISM
TO
REDUCE
THE
EFFECTIVE
LATENCY
OF
FETCHING
A
CACHE
LINE
FROM
MEMORY
CAN
BE
ILLUSTRATED
BY
VARYING
THE
ACCESS
STRIDE
BETWEEN
BYTES
AND
THE
TRIGGER
THRESHOLD
DISTANCE
OF
HARDWARE
PREFETCH
WHEN
POPULATING
THE
ARRAY
FOR
CIRCULAR
POINTER
CHASING
EXAMPLE
POPULATING
AN
ARRAY
FOR
CIRCULAR
POINTER
CHASING
WITH
CONSTANT
STRIDE
REGISTER
CHAR
P
CHAR
NEXT
POPULATING
PARRAY
FOR
CIRCULAR
POINTER
CHASING
WITH
CONSTANT
ACCESS
STRIDE
P
CHAR
P
LOADS
A
VALUE
POINTING
TO
NEXT
LOAD
P
CHAR
PARRAY
FOR
I
I
APERTURE
I
STRIDE
P
CHAR
PARRAY
I
IF
I
STRIDE
NEXT
PARRAY
ELSE
NEXT
PARRAY
I
STRIDE
P
NEXT
POPULATE
THE
ADDRESS
OF
THE
NEXT
NODE
THE
EFFECTIVE
LATENCY
REDUCTION
FOR
SEVERAL
MICROARCHITECTURE
IMPLEMENTATIONS
IS
SHOWN
IN
FIGURE
FOR
A
CONSTANT
STRIDE
ACCESS
PATTERN
THE
BENEFIT
OF
THE
AUTO
MATIC
HARDWARE
PREFETCHER
BEGINS
AT
HALF
THE
TRIGGER
THRESHOLD
DISTANCE
AND
REACHES
MAXIMUM
BENEFIT
WHEN
THE
CACHE
MISS
STRIDE
IS
BYTES
FIGURE
EFFECTIVE
LATENCY
REDUCTION
AS
A
FUNCTION
OF
ACCESS
STRIDE
EXAMPLE
OF
LATENCY
HIDING
WITH
W
PREFETCH
INSTRUCTION
ACHIEVING
THE
HIGHEST
LEVEL
OF
MEMORY
OPTIMIZATION
USING
PREFETCH
INSTRUCTIONS
REQUIRES
AN
UNDERSTANDING
OF
THE
ARCHITECTURE
OF
A
GIVEN
MACHINE
THIS
SECTION
TRANS
LATES
THE
KEY
ARCHITECTURAL
IMPLICATIONS
INTO
SEVERAL
SIMPLE
GUIDELINES
FOR
PROGRAM
MERS
TO
USE
FIGURE
AND
FIGURE
SHOW
TWO
SCENARIOS
OF
A
SIMPLIFIED
GEOMETRY
PIPELINE
AS
AN
EXAMPLE
A
GEOMETRY
PIPELINE
TYPICALLY
FETCHES
ONE
VERTEX
RECORD
AT
A
TIME
AND
THEN
PERFORMS
TRANSFORMATION
AND
LIGHTING
FUNCTIONS
ON
IT
BOTH
FIGURES
SHOW
TWO
SEPARATE
PIPELINES
AN
EXECUTION
PIPELINE
AND
A
MEMORY
PIPELINE
FRONT
SIDE
BUS
SINCE
THE
PENTIUM
PROCESSOR
SIMILAR
TO
THE
PENTIUM
II
AND
PENTIUM
III
PROCESSORS
COMPLETELY
DECOUPLES
THE
FUNCTIONALITY
OF
EXECUTION
AND
MEMORY
ACCESS
THE
TWO
PIPELINES
CAN
FUNCTION
CONCURRENTLY
FIGURE
SHOWS
BUBBLES
IN
BOTH
THE
EXECUTION
AND
MEMORY
PIPELINES
WHEN
LOADS
ARE
ISSUED
FOR
ACCESSING
VERTEX
DATA
THE
EXECU
TION
UNITS
SIT
IDLE
AND
WAIT
UNTIL
DATA
IS
RETURNED
ON
THE
OTHER
HAND
THE
MEMORY
BUS
SITS
IDLE
WHILE
THE
EXECUTION
UNITS
ARE
PROCESSING
VERTICES
THIS
SCENARIO
SEVERELY
DECREASES
THE
ADVANTAGE
OF
HAVING
A
DECOUPLED
ARCHITECTURE
FIGURE
MEMORY
ACCESS
LATENCY
AND
EXECUTION
WITHOUT
PREFETCH
FIGURE
MEMORY
ACCESS
LATENCY
AND
EXECUTION
WITH
PREFETCH
THE
PERFORMANCE
LOSS
CAUSED
BY
POOR
UTILIZATION
OF
RESOURCES
CAN
BE
COMPLETELY
ELIM
INATED
BY
CORRECTLY
SCHEDULING
THE
PREFETCH
INSTRUCTIONS
AS
SHOWN
IN
FIGURE
PREFETCH
INSTRUCTIONS
ARE
ISSUED
TWO
VERTEX
ITERATIONS
AHEAD
THIS
ASSUMES
THAT
ONLY
ONE
VERTEX
GETS
PROCESSED
IN
ONE
ITERATION
AND
A
NEW
DATA
CACHE
LINE
IS
NEEDED
FOR
EACH
ITERATION
AS
A
RESULT
WHEN
ITERATION
N
VERTEX
VN
IS
BEING
PROCESSED
THE
REQUESTED
DATA
IS
ALREADY
BROUGHT
INTO
CACHE
IN
THE
MEANTIME
THE
FRONT
SIDE
BUS
IS
TRANSFERRING
THE
DATA
NEEDED
FOR
ITERATION
N
VERTEX
VN
BECAUSE
THERE
IS
NO
DEPENDENCE
BETWEEN
VN
DATA
AND
THE
EXECUTION
OF
VN
THE
LATENCY
FOR
DATA
ACCESS
OF
VN
CAN
BE
ENTIRELY
HIDDEN
BEHIND
THE
EXECUTION
OF
VN
UNDER
SUCH
CIRCUMSTANCES
NO
BUBBLES
ARE
PRESENT
IN
THE
PIPELINES
AND
THUS
THE
BEST
POSSIBLE
PERFORMANCE
CAN
BE
ACHIEVED
PREFETCHING
IS
USEFUL
FOR
INNER
LOOPS
THAT
HAVE
HEAVY
COMPUTATIONS
OR
ARE
CLOSE
TO
THE
BOUNDARY
BETWEEN
BEING
COMPUTE
BOUND
AND
MEMORY
BANDWIDTH
BOUND
IT
IS
PROB
ABLY
NOT
VERY
USEFUL
FOR
LOOPS
WHICH
ARE
PREDOMINATELY
MEMORY
BANDWIDTH
BOUND
WHEN
DATA
IS
ALREADY
LOCATED
IN
THE
FIRST
LEVEL
CACHE
PREFETCHING
CAN
BE
USELESS
AND
COULD
EVEN
SLOW
DOWN
THE
PERFORMANCE
BECAUSE
THE
EXTRA
ΜOPS
EITHER
BACK
UP
WAITING
FOR
OUTSTANDING
MEMORY
ACCESSES
OR
MAY
BE
DROPPED
ALTOGETHER
THIS
BEHAVIOR
IS
PLATFORM
SPECIFIC
AND
MAY
CHANGE
IN
THE
FUTURE
SOFTWARE
PREFETCHING
USAGE
CHECKLIST
THE
FOLLOWING
CHECKLIST
COVERS
ISSUES
THAT
NEED
TO
BE
ADDRESSED
AND
OR
RESOLVED
TO
USE
THE
SOFTWARE
PREFETCH
INSTRUCTION
PROPERLY
DETERMINE
SOFTWARE
PREFETCH
SCHEDULING
DISTANCE
USE
SOFTWARE
PREFETCH
CONCATENATION
MINIMIZE
THE
NUMBER
OF
SOFTWARE
PREFETCHES
MIX
SOFTWARE
PREFETCH
WITH
COMPUTATION
INSTRUCTIONS
USE
CACHE
BLOCKING
TECHNIQUES
FOR
EXAMPLE
STRIP
MINING
BALANCE
SINGLE
PASS
VERSUS
MULTI
PASS
EXECUTION
RESOLVE
MEMORY
BANK
CONFLICT
ISSUES
RESOLVE
CACHE
MANAGEMENT
ISSUES
SUBSEQUENT
SECTIONS
DISCUSS
THE
ABOVE
ITEMS
SOFTWARE
PREFETCH
SCHEDULING
DISTANCE
DETERMINING
THE
IDEAL
PREFETCH
PLACEMENT
IN
THE
CODE
DEPENDS
ON
MANY
ARCHITECTURAL
PARAMETERS
INCLUDING
THE
AMOUNT
OF
MEMORY
TO
BE
PREFETCHED
CACHE
LOOKUP
LATENCY
SYSTEM
MEMORY
LATENCY
AND
ESTIMATE
OF
COMPUTATION
CYCLE
THE
IDEAL
DISTANCE
FOR
PREFETCHING
DATA
IS
PROCESSOR
AND
PLATFORM
DEPENDENT
IF
THE
DISTANCE
IS
TOO
SHORT
THE
PREFETCH
WILL
NOT
HIDE
THE
LATENCY
OF
THE
FETCH
BEHIND
COMPUTATION
IF
THE
PREFETCH
IS
TOO
FAR
AHEAD
PREFETCHED
DATA
MAY
BE
FLUSHED
OUT
OF
THE
CACHE
BY
THE
TIME
IT
IS
REQUIRED
SINCE
PREFETCH
DISTANCE
IS
NOT
A
WELL
DEFINED
METRIC
FOR
THIS
DISCUSSION
WE
DEFINE
A
NEW
TERM
PREFETCH
SCHEDULING
DISTANCE
PSD
WHICH
IS
REPRESENTED
BY
THE
NUMBER
OF
ITERATIONS
FOR
LARGE
LOOPS
PREFETCH
SCHEDULING
DISTANCE
CAN
BE
SET
TO
THAT
IS
SCHEDULE
PREFETCH
INSTRUCTIONS
ONE
ITERATION
AHEAD
FOR
SMALL
LOOP
BODIES
THAT
IS
LOOP
ITERATIONS
WITH
LITTLE
COMPUTATION
THE
PREFETCH
SCHEDULING
DISTANCE
MUST
BE
MORE
THAN
ONE
ITERATION
A
SIMPLIFIED
EQUATION
TO
COMPUTE
PSD
IS
DEDUCED
FROM
THE
MATHEMATICAL
MODEL
FOR
A
SIMPLIFIED
EQUATION
COMPLETE
MATHEMATICAL
MODEL
AND
METHODOLOGY
OF
PREFETCH
DISTANCE
DETERMINATION
SEE
APPENDIX
E
SUMMARY
OF
RULES
AND
SUGGESTIONS
EXAMPLE
ILLUSTRATES
THE
USE
OF
A
PREFETCH
WITHIN
THE
LOOP
BODY
THE
PREFETCH
SCHEDULING
DISTANCE
IS
SET
TO
ESI
IS
EFFECTIVELY
THE
POINTER
TO
A
LINE
EDX
IS
THE
ADDRESS
OF
THE
DATA
BEING
REFERENCED
AND
ARE
THE
DATA
USED
IN
COMPU
TATION
EXAMPLE
USES
TWO
INDEPENDENT
CACHE
LINES
OF
DATA
PER
ITERATION
THE
PSD
WOULD
NEED
TO
BE
INCREASED
DECREASED
IF
MORE
LESS
THAN
TWO
CACHE
LINES
ARE
USED
PER
ITERATION
EXAMPLE
PREFETCH
SCHEDULING
DISTANCE
PREFETCHNTA
EDX
ESI
PREFETCHNTA
EDX
ESI
EXAMPLE
PREFETCH
SCHEDULING
DISTANCE
CONTD
MOVAPS
EDX
ESI
MOVAPS
EDX
ESI
MOVAPS
EDX
ESI
MOVAPS
EDX
ESI
ADD
ESI
CMP
ESI
ECX
JL
SOFTWARE
PREFETCH
CONCATENATION
MAXIMUM
PERFORMANCE
CAN
BE
ACHIEVED
WHEN
THE
EXECUTION
PIPELINE
IS
AT
MAXIMUM
THROUGHPUT
WITHOUT
INCURRING
ANY
MEMORY
LATENCY
PENALTIES
THIS
CAN
BE
ACHIEVED
BY
PREFETCHING
DATA
TO
BE
USED
IN
SUCCESSIVE
ITERATIONS
IN
A
LOOP
DE
PIPELINING
MEMORY
GENERATES
BUBBLES
IN
THE
EXECUTION
PIPELINE
TO
EXPLAIN
THIS
PERFORMANCE
ISSUE
A
GEOMETRY
PIPELINE
THAT
PROCESSES
VERTICES
IN
STRIP
FORMAT
IS
USED
AS
AN
EXAMPLE
A
STRIP
CONTAINS
A
LIST
OF
VERTICES
WHOSE
PREDEFINED
VERTEX
ORDER
FORMS
CONTIGUOUS
TRIANGLES
IT
CAN
BE
EASILY
OBSERVED
THAT
THE
MEMORY
PIPE
IS
DE
PIPELINED
ON
THE
STRIP
BOUNDARY
DUE
TO
INEFFECTIVE
PREFETCH
ARRANGEMENT
THE
EXECUTION
PIPELINE
IS
STALLED
FOR
THE
FIRST
TWO
ITERATIONS
FOR
EACH
STRIP
AS
A
RESULT
THE
AVERAGE
LATENCY
FOR
COMPLETING
AN
ITERATION
WILL
BE
FIX
CLOCKS
SEE
APPENDIX
E
SUMMARY
OF
RULES
AND
SUGGESTIONS
FOR
A
DETAILED
DESCRIPTION
THIS
MEMORY
DE
PIPELINING
CREATES
INEFFICIENCY
IN
BOTH
THE
MEMORY
PIPELINE
AND
EXECUTION
PIPELINE
THIS
DE
PIPELINING
EFFECT
CAN
BE
REMOVED
BY
APPLYING
A
TECHNIQUE
CALLED
PREFETCH
CONCATENATION
WITH
THIS
TECHNIQUE
THE
MEMORY
ACCESS
AND
EXECU
TION
CAN
BE
FULLY
PIPELINED
AND
FULLY
UTILIZED
FOR
NESTED
LOOPS
MEMORY
DE
PIPELINING
COULD
OCCUR
DURING
THE
INTERVAL
BETWEEN
THE
LAST
ITERATION
OF
AN
INNER
LOOP
AND
THE
NEXT
ITERATION
OF
ITS
ASSOCIATED
OUTER
LOOP
WITHOUT
PAYING
SPECIAL
ATTENTION
TO
PREFETCH
INSERTION
LOADS
FROM
THE
FIRST
ITERATION
OF
AN
INNER
LOOP
CAN
MISS
THE
CACHE
AND
STALL
THE
EXECUTION
PIPELINE
WAITING
FOR
DATA
RETURNED
THUS
DEGRADING
THE
PERFORMANCE
IN
EXAMPLE
THE
CACHE
LINE
CONTAINING
A
II
IS
NOT
PREFETCHED
AT
ALL
AND
ALWAYS
MISSES
THE
CACHE
THIS
ASSUMES
THAT
NO
ARRAY
A
FOOTPRINT
RESIDES
IN
THE
CACHE
THE
PENALTY
OF
MEMORY
DE
PIPELINING
STALLS
CAN
BE
AMORTIZED
ACROSS
THE
INNER
LOOP
ITERATIONS
HOWEVER
IT
MAY
BECOME
VERY
HARMFUL
WHEN
THE
INNER
LOOP
IS
SHORT
IN
ADDITION
THE
LAST
PREFETCH
IN
THE
LAST
PSD
ITERATIONS
ARE
WASTED
AND
CONSUME
MACHINE
RESOURCES
PREFETCH
CONCATENATION
IS
INTRODUCED
HERE
IN
ORDER
TO
ELIMINATE
THE
PERFORMANCE
ISSUE
OF
MEMORY
DE
PIPELINING
EXAMPLE
USING
PREFETCH
CONCATENATION
FOR
II
II
II
FOR
JJ
JJ
JJ
PREFETCH
A
II
JJ
COMPUTATION
A
II
JJ
PREFETCH
CONCATENATION
CAN
BRIDGE
THE
EXECUTION
PIPELINE
BUBBLES
BETWEEN
THE
BOUNDARY
OF
AN
INNER
LOOP
AND
ITS
ASSOCIATED
OUTER
LOOP
SIMPLY
BY
UNROLLING
THE
LAST
ITERATION
OUT
OF
THE
INNER
LOOP
AND
SPECIFYING
THE
EFFECTIVE
PREFETCH
ADDRESS
FOR
DATA
USED
IN
THE
FOLLOWING
ITERATION
THE
PERFORMANCE
LOSS
OF
MEMORY
DE
PIPELINING
CAN
BE
COMPLETELY
REMOVED
EXAMPLE
GIVES
THE
REWRITTEN
CODE
EXAMPLE
CONCATENATION
AND
UNROLLING
THE
LAST
ITERATION
OF
INNER
LOOP
FOR
II
II
II
FOR
JJ
JJ
JJ
N
ITERATIONS
PREFETCH
A
II
JJ
COMPUTATION
A
II
JJ
PREFETCH
A
II
COMPUTATION
A
II
JJ
LAST
ITERATION
THIS
CODE
SEGMENT
FOR
DATA
PREFETCHING
IS
IMPROVED
AND
ONLY
THE
FIRST
ITERATION
OF
THE
OUTER
LOOP
SUFFERS
ANY
MEMORY
ACCESS
LATENCY
PENALTY
ASSUMING
THE
COMPUTATION
TIME
IS
LARGER
THAN
THE
MEMORY
LATENCY
INSERTING
A
PREFETCH
OF
THE
FIRST
DATA
ELEMENT
NEEDED
PRIOR
TO
ENTERING
THE
NESTED
LOOP
COMPUTATION
WOULD
ELIMINATE
OR
REDUCE
THE
START
UP
PENALTY
FOR
THE
VERY
FIRST
ITERATION
OF
THE
OUTER
LOOP
THIS
UNCOMPLICATED
HIGH
LEVEL
CODE
OPTIMIZATION
CAN
IMPROVE
MEMORY
PERFORMANCE
SIGNIFICANTLY
MINIMIZE
NUMBER
OF
SOFTWARE
PREFETCHES
PREFETCH
INSTRUCTIONS
ARE
NOT
COMPLETELY
FREE
IN
TERMS
OF
BUS
CYCLES
MACHINE
CYCLES
AND
RESOURCES
EVEN
THOUGH
THEY
REQUIRE
MINIMAL
CLOCK
AND
MEMORY
BANDWIDTH
EXCESSIVE
PREFETCHING
MAY
LEAD
TO
PERFORMANCE
PENALTIES
BECAUSE
OF
ISSUE
PENALTIES
IN
THE
FRONT
END
OF
THE
MACHINE
AND
OR
RESOURCE
CONTENTION
IN
THE
MEMORY
SUB
SYSTEM
THIS
EFFECT
MAY
BE
SEVERE
IN
CASES
WHERE
THE
TARGET
LOOPS
ARE
SMALL
AND
OR
CASES
WHERE
THE
TARGET
LOOP
IS
ISSUE
BOUND
ONE
APPROACH
TO
SOLVE
THE
EXCESSIVE
PREFETCHING
ISSUE
IS
TO
UNROLL
AND
OR
SOFTWARE
PIPELINE
LOOPS
TO
REDUCE
THE
NUMBER
OF
PREFETCHES
REQUIRED
FIGURE
PRESENTS
A
CODE
EXAMPLE
WHICH
IMPLEMENTS
PREFETCH
AND
UNROLLS
THE
LOOP
TO
REMOVE
THE
REDUN
DANT
PREFETCH
INSTRUCTIONS
WHOSE
PREFETCH
ADDRESSES
HIT
THE
PREVIOUSLY
ISSUED
PREFETCH
INSTRUCTIONS
IN
THIS
PARTICULAR
EXAMPLE
UNROLLING
THE
ORIGINAL
LOOP
ONCE
SAVES
SIX
PREFETCH
INSTRUCTIONS
AND
NINE
INSTRUCTIONS
FOR
CONDITIONAL
JUMPS
IN
EVERY
OTHER
ITERATION
FIGURE
PREFETCH
AND
LOOP
UNROLLING
FIGURE
DEMONSTRATES
THE
EFFECTIVENESS
OF
SOFTWARE
PREFETCHES
IN
LATENCY
HIDING
FIGURE
MEMORY
ACCESS
LATENCY
AND
EXECUTION
WITH
PREFETCH
THE
X
AXIS
IN
FIGURE
INDICATES
THE
NUMBER
OF
COMPUTATION
CLOCKS
PER
LOOP
EACH
ITERATION
IS
INDEPENDENT
THE
Y
AXIS
INDICATES
THE
EXECUTION
TIME
MEASURED
IN
CLOCKS
PER
LOOP
THE
SECONDARY
Y
AXIS
INDICATES
THE
PERCENTAGE
OF
BUS
BANDWIDTH
UTILIZATION
THE
TESTS
VARY
BY
THE
FOLLOWING
PARAMETERS
NUMBER
OF
LOAD
STORE
STREAMS
EACH
LOAD
AND
STORE
STREAM
ACCESSES
ONE
BYTE
CACHE
LINE
EACH
PER
ITERATION
AMOUNT
OF
COMPUTATION
PER
LOOP
THIS
IS
VARIED
BY
INCREASING
THE
NUMBER
OF
DEPENDENT
ARITHMETIC
OPERATIONS
EXECUTED
NUMBER
OF
THE
SOFTWARE
PREFETCHES
PER
LOOP
FOR
EXAMPLE
ONE
EVERY
BYTES
BYTES
BYTES
BYTES
AS
EXPECTED
THE
LEFTMOST
PORTION
OF
EACH
OF
THE
GRAPHS
IN
FIGURE
SHOWS
THAT
WHEN
THERE
IS
NOT
ENOUGH
COMPUTATION
TO
OVERLAP
THE
LATENCY
OF
MEMORY
ACCESS
PREFETCH
DOES
NOT
HELP
AND
THAT
THE
EXECUTION
IS
ESSENTIALLY
MEMORY
BOUND
THE
GRAPHS
ALSO
ILLUSTRATE
THAT
REDUNDANT
PREFETCHES
DO
NOT
INCREASE
PERFORMANCE
THE
VIRTUAL
RUNTIME
THE
VRUNTIME
VARIABLE
STORES
THE
VIRTUAL
RUNTIME
OF
A
PROCESS
WHICH
IS
THE
ACTUAL
RUNTIME
THE
AMOUNT
OF
TIME
SPENT
RUNNING
NORMALIZED
OR
WEIGHTED
BY
THE
NUMBER
OF
RUNNABLE
PROCESSES
THE
VIRTUAL
RUNTIME
UNITS
ARE
NANOSECONDS
AND
THEREFORE
VRUNTIME
IS
DECOU
PLED
FROM
THE
TIMER
TICK
THE
VIRTUAL
RUNTIME
IS
USED
TO
HELP
US
APPROXIMATE
THE
IDEAL
MULTITASKING
PROCESSOR
THAT
CFS
IS
MODELING
WITH
SUCH
AN
IDEAL
PROCESSOR
WE
WOULDN
T
NEED
VRUNTIME
BECAUSE
ALL
RUNNABLE
PROCESSES
WOULD
PERFECTLY
MULTITASK
THAT
IS
ON
AN
IDEAL
PROCESSOR
THE
VIRTUAL
RUNTIME
OF
ALL
PROCESSES
OF
THE
SAME
PRIORITY
WOULD
BE
IDENTI
CAL
ALL
TASKS
WOULD
HAVE
RECEIVED
AN
EQUAL
FAIR
SHARE
OF
THE
PROCESSOR
BECAUSE
PROCESSORS
ARE
NOT
CAPABLE
OF
PERFECT
MULTITASKING
AND
WE
MUST
RUN
EACH
PROCESS
IN
SUCCESSION
CFS
USES
VRUNTIME
TO
ACCOUNT
FOR
HOW
LONG
A
PROCESS
HAS
RUN
AND
THUS
HOW
MUCH
LONGER
IT
OUGHT
TO
RUN
THE
FUNCTION
DEFINED
IN
KERNEL
C
MANAGES
THIS
ACCOUNTING
STATIC
VOID
STRUCT
STRUCT
CURR
CURR
NOW
CLOCK
UNSIGNED
LONG
IF
UNLIKELY
CURR
RETURN
GET
THE
AMOUNT
OF
TIME
THE
CURRENT
TASK
WAS
RUNNING
SINCE
THE
LAST
TIME
WE
CHANGED
LOAD
THIS
CANNOT
OVERFLOW
ON
BITS
UNSIGNED
LONG
NOW
CURR
IF
RETURN
CURR
CURR
NOW
IF
CURR
STRUCT
CURTASK
CURR
CURTASK
CURR
VRUNTIME
CURTASK
CURTASK
CALCULATES
THE
EXECUTION
TIME
OF
THE
CURRENT
PROCESS
AND
STORES
THAT
VALUE
IN
IT
THEN
PASSES
THAT
RUNTIME
TO
WHICH
WEIGHTS
THE
TIME
BY
THE
NUMBER
OF
RUNNABLE
PROCESSES
THE
CURRENT
PROCESS
VRUNTIME
IS
THEN
INCRE
MENTED
BY
THE
WEIGHTED
VALUE
UPDATE
THE
CURRENT
TASK
RUNTIME
STATISTICS
SKIP
CURRENT
TASKS
THAT
ARE
NOT
IN
OUR
SCHEDULING
CLASS
STATIC
INLINE
VOID
STRUCT
STRUCT
CURR
UNSIGNED
LONG
UNSIGNED
LONG
CURR
MAX
CURR
CURR
CURR
CURR
VRUNTIME
IS
INVOKED
PERIODICALLY
BY
THE
SYSTEM
TIMER
AND
ALSO
WHENEVER
A
PROCESS
BECOMES
RUNNABLE
OR
BLOCKS
BECOMING
UNRUNNABLE
IN
THIS
MANNER
VRUNTIME
IS
AN
ACCURATE
MEASURE
OF
THE
RUNTIME
OF
A
GIVEN
PROCESS
AND
AN
INDICATOR
OF
WHAT
PROCESS
SHOULD
RUN
NEXT
PROCESS
SELECTION
IN
THE
LAST
SECTION
WE
DISCUSSED
HOW
VRUNTIME
ON
AN
IDEAL
PERFECTLY
MULTITASKING
PROCES
SOR
WOULD
BE
IDENTICAL
AMONG
ALL
RUNNABLE
PROCESSES
IN
REALITY
WE
CANNOT
PERFECTLY
MULTI
TASK
SO
CFS
ATTEMPTS
TO
BALANCE
A
PROCESS
VIRTUAL
RUNTIME
WITH
A
SIMPLE
RULE
WHEN
CFS
IS
DECIDING
WHAT
PROCESS
TO
RUN
NEXT
IT
PICKS
THE
PROCESS
WITH
THE
SMALLEST
VRUNTIME
THIS
IS
IN
FACT
THE
CORE
OF
CFS
SCHEDULING
ALGORITHM
PICK
THE
TASK
WITH
THE
SMALLEST
VRUNTIME
THAT
IT
THE
REST
OF
THIS
SUBSECTION
DESCRIBES
HOW
THE
SELECTION
OF
THE
PROCESS
WITH
THE
SMALLEST
VRUNTIME
IS
IMPLEMENTED
CFS
USES
A
RED
BLACK
TREE
TO
MANAGE
THE
LIST
OF
RUNNABLE
PROCESSES
AND
EFFICIENTLY
FIND
THE
PROCESS
WITH
THE
SMALLEST
VRUNTIME
A
RED
BLACK
TREE
CALLED
AN
RBTREE
IN
LINUX
IS
A
TYPE
OF
SELF
BALANCING
BINARY
SEARCH
TREE
WE
DISCUSS
SELF
BALANCING
BINARY
SEARCH
TREES
IN
GENERAL
AND
RED
BLACK
TREES
IN
PARTICULAR
IN
CHAPTER
FOR
NOW
IF
YOU
ARE
UNFAMILIAR
YOU
NEED
TO
KNOW
ONLY
THAT
RED
BLACK
TREES
ARE
A
DATA
STRUCTURE
THAT
STORE
NODES
OF
ARBITRARY
DATA
IDEN
TIFIED
BY
A
SPECIFIC
KEY
AND
THAT
THEY
ENABLE
EFFICIENT
SEARCH
FOR
A
GIVEN
KEY
SPECIFICALLY
OBTAINING
A
NODE
IDENTIFIED
BY
A
GIVEN
KEY
IS
LOGARITHMIC
IN
TIME
AS
A
FUNCTION
OF
TOTAL
NODES
IN
THE
TREE
PICKING
THE
NEXT
TASK
LET
START
WITH
THE
ASSUMPTION
THAT
WE
HAVE
A
RED
BLACK
TREE
POPULATED
WITH
EVERY
RUNNABLE
PROCESS
IN
THE
SYSTEM
WHERE
THE
KEY
FOR
EACH
NODE
IS
THE
RUNNABLE
PROCESS
VIR
TUAL
RUNTIME
WE
LL
LOOK
AT
HOW
WE
BUILD
THAT
TREE
IN
A
MOMENT
BUT
FOR
NOW
LET
ASSUME
WE
HAVE
IT
GIVEN
THIS
TREE
THE
PROCESS
THAT
CFS
WANTS
TO
RUN
NEXT
WHICH
IS
THE
PROCESS
WITH
THE
SMALLEST
VRUNTIME
IS
THE
LEFTMOST
NODE
IN
THE
TREE
THAT
IS
IF
YOU
FOLLOW
THE
TREE
FROM
THE
ROOT
DOWN
THROUGH
THE
LEFT
CHILD
AND
CONTINUE
MOVING
TO
THE
LEFT
UNTIL
YOU
REACH
A
LEAF
NODE
YOU
FIND
THE
PROCESS
WITH
THE
SMALLEST
VRUNTIME
AGAIN
IF
YOU
ARE
UNFAMILIAR
WITH
BINARY
SEARCH
TREES
DON
T
WORRY
JUST
KNOW
THAT
THIS
PROCESS
IS
EFFICIENT
CFS
PROCESS
SELECTION
ALGORITHM
IS
THUS
SUMMED
UP
AS
RUN
THE
PROCESS
REPRESENTED
BY
THE
LEFTMOST
NODE
IN
THE
RBTREE
THE
FUNCTION
THAT
PERFORMS
THIS
SELECTION
IS
DEFINED
IN
KERNEL
C
STATIC
STRUCT
STRUCT
STRUCT
LEFT
IF
LEFT
RETURN
NULL
RETURN
LEFT
STRUCT
NOTE
THAT
DOES
NOT
ACTUALLY
TRAVERSE
THE
TREE
TO
FIND
THE
LEFT
MOST
NODE
BECAUSE
THE
VALUE
IS
CACHED
BY
ALTHOUGH
IT
IS
EFFICIENT
TO
WALK
THE
TREE
TO
FIND
THE
LEFTMOST
NODE
O
HEIGHT
OF
TREE
WHICH
IS
O
LOG
N
FOR
N
NODES
IF
THE
TREE
IS
BALANCED
IT
IS
EVEN
EASIER
TO
CACHE
THE
LEFTMOST
NODE
THE
RETURN
VALUE
FROM
THIS
FUNCTION
IS
THE
PROCESS
THAT
CFS
NEXT
RUNS
IF
THE
FUNCTION
RETURNS
NULL
THERE
IS
NO
LEFTMOST
NODE
AND
THUS
NO
NODES
IN
THE
TREE
IN
THAT
CASE
THERE
ARE
NO
RUNNABLE
PROCESSES
AND
CFS
SCHEDULES
THE
IDLE
TASK
ADDING
PROCESSES
TO
THE
TREE
NOW
LET
LOOK
AT
HOW
CFS
ADDS
PROCESSES
TO
THE
RBTREE
AND
CACHES
THE
LEFTMOST
NODE
THIS
WOULD
OCCUR
WHEN
A
PROCESS
BECOMES
RUNNABLE
WAKES
UP
OR
IS
FIRST
CREATED
VIA
FORK
AS
DISCUSSED
IN
CHAPTER
ADDING
PROCESSES
TO
THE
TREE
IS
PERFORMED
BY
STATIC
VOID
STRUCT
STRUCT
SE
INT
FLAGS
UPDATE
THE
NORMALIZED
VRUNTIME
BEFORE
UPDATING
THROUGH
CALLIG
IF
FLAGS
FLAGS
SE
VRUNTIME
UPDATE
RUN
TIME
STATISTICS
OF
THE
CURRENT
SE
IF
FLAGS
SE
SE
SE
SE
IF
SE
CURR
SE
THIS
FUNCTION
UPDATES
THE
RUNTIME
AND
OTHER
STATISTICS
AND
THEN
INVOKES
TO
PERFORM
THE
ACTUAL
HEAVY
LIFTING
OF
INSERTING
THE
ENTRY
INTO
THE
RED
BLACK
TREE
ENQUEUE
AN
ENTITY
INTO
THE
RB
TREE
STATIC
VOID
STRUCT
STRUCT
SE
STRUCT
LINK
STRUCT
PARENT
NULL
STRUCT
ENTRY
KEY
SE
INT
LEFTMOST
FIND
THE
RIGHT
PLACE
IN
THE
RBTREE
WHILE
LINK
PARENT
LINK
ENTRY
PARENT
STRUCT
WE
DONT
CARE
ABOUT
COLLISIONS
NODES
WITH
THE
SAME
KEY
STAY
TOGETHER
IF
KEY
ENTRY
LINK
PARENT
ELSE
LINK
PARENT
LEFTMOST
MAINTAIN
A
CACHE
OF
LEFTMOST
TREE
ENTRIES
IT
IS
FREQUENTLY
USED
IF
LEFTMOST
SE
SE
PARENT
LINK
SE
LET
LOOK
AT
THIS
FUNCTION
THE
BODY
OF
THE
WHILE
LOOP
TRAVERSES
THE
TREE
IN
SEARCH
OF
A
MATCHING
KEY
WHICH
IS
THE
INSERTED
PROCESS
VRUNTIME
PER
THE
RULES
OF
THE
BALANCED
TREE
IT
MOVES
TO
THE
LEFT
CHILD
IF
THE
KEY
IS
SMALLER
THAN
THE
CURRENT
NODE
KEY
AND
TO
THE
RIGHT
CHILD
IF
THE
KEY
IS
LARGER
IF
IT
EVER
MOVES
TO
THE
RIGHT
EVEN
ONCE
IT
KNOWS
THE
INSERTED
PROCESS
CANNOT
BE
THE
NEW
LEFTMOST
NODE
AND
IT
SETS
LEFTMOST
TO
ZERO
IF
IT
MOVES
ONLY
TO
THE
LEFT
LEFTMOST
REMAINS
ONE
AND
WE
HAVE
A
NEW
LEFTMOST
NODE
AND
CAN
UPDATE
THE
CACHE
BY
SETTING
TO
THE
INSERTED
PROCESS
THE
LOOP
TERMINATES
WHEN
WE
COMPARE
OURSELVES
TO
A
NODE
THAT
HAS
NO
CHILD
IN
THE
DIRECTION
WE
MOVE
LINK
IS
THEN
NULL
AND
THE
LOOP
TERMINATES
WHEN
OUT
OF
THE
LOOP
THE
FUNCTION
CALLS
ON
THE
PARENT
NODE
MAKING
THE
INSERTED
PROCESS
THE
NEW
CHILD
THE
FUNCTION
UPDATES
THE
SELF
BALANCING
PROPERTIES
OF
THE
TREE
WE
DISCUSS
THE
COL
ORING
IN
CHAPTER
REMOVING
PROCESSES
FROM
THE
TREE
FINALLY
LET
LOOK
AT
HOW
CFS
REMOVES
PROCESSES
FROM
THE
RED
BLACK
TREE
THIS
HAPPENS
WHEN
A
PROCESS
BLOCKS
BECOMES
UNRUNNABLE
OR
TERMINATES
CEASES
TO
EXIST
STATIC
VOID
STRUCT
STRUCT
SE
INT
SLEEP
UPDATE
RUN
TIME
STATISTICS
OF
THE
CURRENT
SE
SE
IF
SE
CURR
SE
SE
NORMALIZE
THE
ENTITY
AFTER
UPDATING
THE
BECAUSE
THE
UPDATE
CAN
REFER
TO
THE
CURR
ITEM
AND
WE
NEED
TO
REFLECT
THIS
MOVEMENT
IN
OUR
NORMALIZED
POSITION
IF
SLEEP
SE
VRUNTIME
AS
WITH
ADDING
A
PROCESS
TO
THE
RED
BLACK
TREE
THE
REAL
WORK
IS
PERFORMED
BY
A
HELPER
FUNCTION
STATIC
VOID
STRUCT
STRUCT
SE
IF
SE
STRUCT
SE
SE
REMOVING
A
PROCESS
FROM
THE
TREE
IS
MUCH
SIMPLER
BECAUSE
THE
RBTREE
IMPLEMENTATION
PROVIDES
THE
FUNCTION
THAT
PERFORMS
ALL
THE
WORK
THE
REST
OF
THIS
FUNCTION
UPDATES
THE
CACHE
IF
THE
PROCESS
TO
REMOVE
IS
THE
LEFTMOST
NODE
THE
FUNC
TION
INVOKES
TO
FIND
WHAT
WOULD
BE
THE
NEXT
NODE
IN
AN
IN
ORDER
TRAVERSAL
THIS
IS
WHAT
WILL
BE
THE
LEFTMOST
NODE
WHEN
THE
CURRENT
LEFTMOST
NODE
IS
REMOVED
THE
SCHEDULER
ENTRY
POINT
THE
MAIN
ENTRY
POINT
INTO
THE
PROCESS
SCHEDULE
IS
THE
FUNCTION
SCHEDULE
DEFINED
IN
KERNEL
SCHED
C
THIS
IS
THE
FUNCTION
THAT
THE
REST
OF
THE
KERNEL
USES
TO
INVOKE
THE
PROCESS
SCHEDULER
DECIDING
WHICH
PROCESS
TO
RUN
AND
THEN
RUNNING
IT
SCHEDULE
IS
GENERIC
WITH
RESPECT
TO
SCHEDULER
CLASSES
THAT
IS
IT
FINDS
THE
HIGHEST
PRIORITY
SCHEDULER
CLASS
WITH
A
RUNNABLE
PROCESS
AND
ASKS
IT
WHAT
TO
RUN
NEXT
GIVEN
THAT
IT
SHOULD
BE
NO
SURPRISE
THAT
SCHEDULE
IS
SIMPLE
THE
ONLY
IMPORTANT
PART
OF
THE
FUNCTION
WHICH
IS
OTHERWISE
TOO
UNINTERESTING
TO
REPRODUCE
HERE
IS
ITS
INVOCATION
OF
ALSO
DEFINED
IN
KERNEL
SCHED
C
THE
FUNCTION
GOES
THROUGH
EACH
SCHEDULER
CLASS
STARTING
WITH
THE
HIGHEST
PRIORITY
AND
SELECTS
THE
HIGHEST
PRIORITY
PROCESS
IN
THE
HIGHEST
PRIORITY
CLASS
PICK
UP
THE
HIGHEST
PRIO
TASK
STATIC
INLINE
STRUCT
STRUCT
RQ
RQ
CONST
STRUCT
CLASS
STRUCT
P
OPTIMIZATION
WE
KNOW
THAT
IF
ALL
TASKS
ARE
IN
THE
FAIR
CLASS
WE
CAN
CALL
THAT
FUNCTION
DIRECTLY
IF
LIKELY
RQ
RQ
CFS
P
RQ
IF
LIKELY
P
RETURN
P
CLASS
FOR
P
CLASS
RQ
IF
P
RETURN
P
WILL
NEVER
BE
NULL
AS
THE
IDLE
CLASS
ALWAYS
RETURNS
A
NON
NULL
P
CLASS
CLASS
NEXT
NOTE
THE
OPTIMIZATION
AT
THE
BEGINNING
OF
THE
FUNCTION
BECAUSE
CFS
IS
THE
SCHEDULER
CLASS
FOR
NORMAL
PROCESSES
AND
MOST
SYSTEMS
RUN
MOSTLY
NORMAL
PROCESSES
THERE
IS
A
SMALL
HACK
TO
QUICKLY
SELECT
THE
NEXT
CFS
PROVIDED
PROCESS
IF
THE
NUMBER
OF
RUNNABLE
PROCESSES
IS
EQUAL
TO
THE
NUMBER
OF
CFS
RUNNABLE
PROCESSES
WHICH
SUGGESTS
THAT
ALL
RUNNABLE
PROCESSES
ARE
PROVIDED
BY
CFS
THE
CORE
OF
THE
FUNCTION
IS
THE
FOR
LOOP
WHICH
ITERATES
OVER
EACH
CLASS
IN
PRIORITY
ORDER
STARTING
WITH
THE
HIGHEST
PRIORITY
CLASS
EACH
CLASS
IMPLEMENTS
THE
FUNCTION
WHICH
RETURNS
A
POINTER
TO
ITS
NEXT
RUNNABLE
PROCESS
OR
IF
THERE
IS
NOT
ONE
NULL
THE
FIRST
CLASS
TO
RETURN
A
NON
NULL
VALUE
HAS
SELECTED
THE
NEXT
RUNNABLE
PROCESS
CFS
IMPLEMENTATION
OF
CALLS
WHICH
IN
TURN
CALLS
THE
FUNCTION
THAT
WE
DISCUSSED
IN
THE
PREVIOUS
SECTION
SLEEPING
AND
WAKING
UP
TASKS
THAT
ARE
SLEEPING
BLOCKED
ARE
IN
A
SPECIAL
NONRUNNABLE
STATE
THIS
IS
IMPORTANT
BECAUSE
WITHOUT
THIS
SPECIAL
STATE
THE
SCHEDULER
WOULD
SELECT
TASKS
THAT
DID
NOT
WANT
TO
RUN
OR
WORSE
SLEEPING
WOULD
HAVE
TO
BE
IMPLEMENTED
AS
BUSY
LOOPING
A
TASK
SLEEPS
FOR
A
NUMBER
OF
REASONS
BUT
ALWAYS
WHILE
IT
IS
WAITING
FOR
SOME
EVENT
THE
EVENT
CAN
BE
A
SPEC
IFIED
AMOUNT
OF
TIME
MORE
DATA
FROM
A
FILE
I
O
OR
ANOTHER
HARDWARE
EVENT
A
TASK
CAN
ALSO
INVOLUNTARILY
GO
TO
SLEEP
WHEN
IT
TRIES
TO
OBTAIN
A
CONTENDED
SEMAPHORE
IN
THE
KERNEL
THIS
IS
COVERED
IN
CHAPTER
AN
INTRODUCTION
TO
KERNEL
SYNCHRONIZATION
A
COMMON
REASON
TO
SLEEP
IS
FILE
I
O
FOR
EXAMPLE
THE
TASK
ISSUED
A
READ
REQUEST
ON
A
FILE
WHICH
NEEDS
TO
BE
READ
IN
FROM
DISK
AS
ANOTHER
EXAMPLE
THE
TASK
COULD
BE
WAITING
FOR
KEYBOARD
INPUT
WHATEVER
THE
CASE
THE
KERNEL
BEHAVIOR
IS
THE
SAME
THE
TASK
MARKS
ITSELF
AS
SLEEPING
PUTS
ITSELF
ON
A
WAIT
QUEUE
REMOVES
ITSELF
FROM
THE
RED
BLACK
TREE
OF
RUNNABLE
AND
CALLS
SCHEDULE
TO
SELECT
A
NEW
PROCESS
TO
EXECUTE
WAKING
BACK
UP
IS
THE
INVERSE
THE
TASK
IS
SET
AS
RUNNABLE
REMOVED
FROM
THE
WAIT
QUEUE
AND
ADDED
BACK
TO
THE
RED
BLACK
TREE
AS
DISCUSSED
IN
THE
PREVIOUS
CHAPTER
TWO
STATES
ARE
ASSOCIATED
WITH
SLEEPING
AND
THEY
DIFFER
ONLY
IN
THAT
TASKS
IN
THE
STATE
IGNORE
SIGNALS
WHEREAS
TASKS
IN
THE
STATE
WAKE
UP
PREMATURELY
AND
RESPOND
TO
A
SIGNAL
IF
ONE
IS
ISSUED
BOTH
TYPES
OF
SLEEPING
TASKS
SIT
ON
A
WAIT
QUEUE
WAITING
FOR
AN
EVENT
TO
OCCUR
AND
ARE
NOT
RUNNABLE
WAIT
QUEUES
SLEEPING
IS
HANDLED
VIA
WAIT
QUEUES
A
WAIT
QUEUE
IS
A
SIMPLE
LIST
OF
PROCESSES
WAITING
FOR
AN
EVENT
TO
OCCUR
WAIT
QUEUES
ARE
REPRESENTED
IN
THE
KERNEL
BY
WAIT
QUEUES
ARE
CREATED
STATICALLY
VIA
OR
DYNAMICALLY
VIA
PROCESSES
PUT
THEMSELVES
ON
A
WAIT
QUEUE
AND
MARK
THEMSELVES
NOT
RUNNABLE
WHEN
THE
EVENT
ASSOCIATED
WITH
THE
WAIT
QUEUE
OCCURS
THE
PROCESSES
ON
THE
QUEUE
ARE
AWAKENED
IT
IS
IMPORTANT
TO
IMPLEMENT
SLEEPING
AND
WAKING
CORRECTLY
TO
AVOID
RACE
CONDITIONS
SOME
SIMPLE
INTERFACES
FOR
SLEEPING
USED
TO
BE
IN
WIDE
USE
THESE
INTERFACES
HOWEVER
HAVE
RACES
IT
IS
POSSIBLE
TO
GO
TO
SLEEP
AFTER
THE
CONDITION
BECOMES
TRUE
IN
THAT
CASE
THE
TASK
MIGHT
SLEEP
INDEFINITELY
THEREFORE
THE
RECOMMENDED
METHOD
FOR
SLEEPING
IN
THE
KER
NEL
IS
A
BIT
MORE
COMPLICATED
Q
IS
THE
WAIT
QUEUE
WE
WISH
TO
SLEEP
ON
WAIT
Q
WAIT
WHILE
CONDITION
CONDITION
IS
THE
EVENT
THAT
WE
ARE
WAITING
FOR
Q
WAIT
IF
CURRENT
HANDLE
SIGNAL
SCHEDULE
Q
WAIT
THE
TASK
PERFORMS
THE
FOLLOWING
STEPS
TO
ADD
ITSELF
TO
A
WAIT
QUEUE
CREATES
A
WAIT
QUEUE
ENTRY
VIA
THE
MACRO
ADDS
ITSELF
TO
A
WAIT
QUEUE
VIA
THIS
WAIT
QUEUE
AWAKENS
THE
PROCESS
WHEN
THE
CONDITION
FOR
WHICH
IT
IS
WAITING
OCCURS
OF
COURSE
THERE
NEEDS
TO
BE
CODE
ELSEWHERE
THAT
CALLS
ON
THE
QUEUE
WHEN
THE
EVENT
ACTUALLY
DOES
OCCUR
CALLS
TO
CHANGE
THE
PROCESS
STATE
TO
EITHER
OR
THIS
FUNCTION
ALSO
ADDS
THE
TASK
BACK
TO
THE
WAIT
QUEUE
IF
NECESSARY
WHICH
IS
NEEDED
ON
SUBSEQUENT
ITERATIONS
OF
THE
LOOP
IF
THE
STATE
IS
SET
TO
A
SIGNAL
WAKES
THE
PROCESS
UP
THIS
IS
CALLED
A
SPURIOUS
WAKE
UP
A
WAKE
UP
NOT
CAUSED
BY
THE
OCCURRENCE
OF
THE
EVENT
SO
CHECK
AND
HANDLE
SIGNALS
WHEN
THE
TASK
AWAKENS
IT
AGAIN
CHECKS
WHETHER
THE
CONDITION
IS
TRUE
IF
IT
IS
IT
EXITS
THE
LOOP
OTHERWISE
IT
AGAIN
CALLS
SCHEDULE
AND
REPEATS
NOW
THAT
THE
CONDITION
IS
TRUE
THE
TASK
SETS
ITSELF
TO
AND
REMOVES
ITSELF
FROM
THE
WAIT
QUEUE
VIA
IF
THE
CONDITION
OCCURS
BEFORE
THE
TASK
GOES
TO
SLEEP
THE
LOOP
TERMINATES
AND
THE
TASK
DOES
NOT
ERRONEOUSLY
GO
TO
SLEEP
NOTE
THAT
KERNEL
CODE
OFTEN
HAS
TO
PERFORM
VARIOUS
OTHER
TASKS
IN
THE
BODY
OF
THE
LOOP
FOR
EXAMPLE
IT
MIGHT
NEED
TO
RELEASE
LOCKS
BEFORE
CALLING
SCHEDULE
AND
REACQUIRE
THEM
AFTER
OR
REACT
TO
OTHER
EVENTS
THE
FUNCTION
IN
FS
NOTIFY
INOTIFY
C
WHICH
HANDLES
READING
FROM
THE
INOTIFY
FILE
DESCRIPTOR
IS
A
STRAIGHTFORWARD
EXAMPLE
OF
USING
WAIT
QUEUES
STATIC
STRUCT
FILE
FILE
CHAR
USER
BUF
COUNT
POS
STRUCT
GROUP
STRUCT
KEVENT
CHAR
USER
START
INT
RET
WAIT
START
BUF
GROUP
FILE
WHILE
GROUP
WAIT
GROUP
KEVENT
GROUP
COUNT
GROUP
IF
KEVENT
RET
KEVENT
IF
KEVENT
BREAK
RET
GROUP
KEVENT
BUF
KEVENT
IF
RET
BREAK
BUF
RET
COUNT
RET
CONTINUE
RET
EAGAIN
IF
FILE
BREAK
RET
EINTR
IF
CURRENT
BREAK
IF
START
BUF
BREAK
SCHEDULE
GROUP
WAIT
IF
START
BUF
RET
EFAULT
RET
BUF
START
RETURN
RET
THIS
FUNCTION
FOLLOWS
THE
PATTERN
LAID
OUT
IN
OUR
EXAMPLE
THE
MAIN
DIFFERENCE
IS
THAT
IT
CHECKS
FOR
THE
CONDITION
IN
THE
BODY
OF
THE
WHILE
LOOP
INSTEAD
OF
IN
THE
WHILE
STATEMENT
ITSELF
THIS
IS
BECAUSE
CHECKING
THE
CONDITION
IS
COMPLICATED
AND
REQUIRES
GRAB
BING
LOCKS
THE
LOOP
IS
TERMINATED
VIA
BREAK
WAKING
UP
WAKING
IS
HANDLED
VIA
WHICH
WAKES
UP
ALL
THE
TASKS
WAITING
ON
THE
GIVEN
WAIT
QUEUE
IT
CALLS
WHICH
SETS
THE
TASK
STATE
TO
CALLS
TO
ADD
THE
TASK
TO
THE
RED
BLACK
TREE
AND
SETS
IF
THE
AWAKENED
TASK
PRIORITY
IS
HIGHER
THAN
THE
PRIORITY
OF
THE
CURRENT
TASK
THE
CODE
THAT
CAUSES
THE
EVENT
TO
OCCUR
TYPICALLY
CALLS
ITSELF
FOR
EXAMPLE
WHEN
DATA
ARRIVES
FROM
THE
HARD
DISK
THE
VFS
CALLS
ON
THE
WAIT
QUEUE
THAT
HOLDS
THE
PROCESSES
WAITING
FOR
THE
DATA
AN
IMPORTANT
NOTE
ABOUT
SLEEPING
IS
THAT
THERE
ARE
SPURIOUS
WAKE
UPS
JUST
BECAUSE
A
TASK
IS
AWAKENED
DOES
NOT
MEAN
THAT
THE
EVENT
FOR
WHICH
THE
TASK
IS
WAITING
HAS
OCCURRED
SLEEPING
SHOULD
ALWAYS
BE
HANDLED
IN
A
LOOP
THAT
ENSURES
THAT
THE
CONDITION
FOR
WHICH
THE
TASK
IS
WAITING
HAS
INDEED
OCCURRED
FIGURE
DEPICTS
THE
RELATIONSHIP
BETWEEN
EACH
SCHEDULER
STATE
ADDS
TASK
TO
A
WAIT
QUEUE
SETS
THE
TASK
STATE
TO
AND
CALLS
SCHEDULE
SCHEDULE
CALLS
WHICH
REMOVES
THE
TASK
FROM
THE
RUNQUEUE
TASK
IS
RUNNABLE
TASK
IS
NOT
RUNNABLE
RECEIVES
A
SIGNAL
TASK
STATE
IS
SET
TO
AND
TASK
EXECUTES
SIGNAL
HANDLER
EVENT
THE
TASK
IS
WAITING
FOR
OCCURS
AND
SETS
THE
TASK
TO
CALLS
TO
ADD
THE
TASK
TO
A
RUNQUEUE
AND
CALLS
SCHEDULE
REMOVES
THE
TASK
FROM
THE
WAIT
QUEUE
FIGURE
SLEEPING
AND
WAKING
UP
PREEMPTION
AND
CONTEXT
SWITCHING
CONTEXT
SWITCHING
THE
SWITCHING
FROM
ONE
RUNNABLE
TASK
TO
ANOTHER
IS
HANDLED
BY
THE
FUNCTION
DEFINED
IN
KERNEL
SCHED
C
IT
IS
CALLED
BY
SCHEDULE
WHEN
A
NEW
PROCESS
HAS
BEEN
SELECTED
TO
RUN
IT
DOES
TWO
BASIC
JOBS
N
CALLS
WHICH
IS
DECLARED
IN
ASM
H
TO
SWITCH
THE
VIR
TUAL
MEMORY
MAPPING
FROM
THE
PREVIOUS
PROCESS
TO
THAT
OF
THE
NEW
PROCESS
N
CALLS
DECLARED
IN
ASM
SYSTEM
H
TO
SWITCH
THE
PROCESSOR
STATE
FROM
THE
PREVIOUS
PROCESS
TO
THE
CURRENT
THIS
INVOLVES
SAVING
AND
RESTORING
STACK
INFOR
MATION
AND
THE
PROCESSOR
REGISTERS
AND
ANY
OTHER
ARCHITECTURE
SPECIFIC
STATE
THAT
MUST
BE
MANAGED
AND
RESTORED
ON
A
PER
PROCESS
BASIS
THE
KERNEL
HOWEVER
MUST
KNOW
WHEN
TO
CALL
SCHEDULE
IF
IT
CALLED
SCHEDULE
ONLY
WHEN
CODE
EXPLICITLY
DID
SO
USER
SPACE
PROGRAMS
COULD
RUN
INDEFINITELY
INSTEAD
THE
KERNEL
PROVIDES
THE
FLAG
TO
SIGNIFY
WHETHER
A
RESCHEDULE
SHOULD
BE
PERFORMED
SEE
TABLE
THIS
FLAG
IS
SET
BY
WHEN
A
PROCESS
SHOULD
BE
PREEMPTED
AND
BY
WHEN
A
PROCESS
THAT
HAS
A
HIGHER
PRIORITY
THAN
THE
CURRENTLY
RUN
NING
PROCESS
IS
AWAKENED
THE
KERNEL
CHECKS
THE
FLAG
SEES
THAT
IT
IS
SET
AND
CALLS
SCHEDULE
TO
SWITCH
TO
A
NEW
PROCESS
THE
FLAG
IS
A
MESSAGE
TO
THE
KERNEL
THAT
THE
SCHEDULER
SHOULD
BE
INVOKED
AS
SOON
AS
POSSIBLE
BECAUSE
ANOTHER
PROCESS
DESERVES
TO
RUN
TABLE
FUNCTIONS
FOR
ACCESSING
AND
MANIPULATING
FUNCTION
PURPOSE
SET
THE
FLAG
IN
THE
GIVEN
PROCESS
CLEAR
THE
FLAG
IN
THE
GIVEN
PROCESS
TEST
THE
VALUE
OF
THE
FLAG
RETURN
TRUE
IF
SET
AND
FALSE
OTHERWISE
UPON
RETURNING
TO
USER
SPACE
OR
RETURNING
FROM
AN
INTERRUPT
THE
FLAG
IS
CHECKED
IF
IT
IS
SET
THE
KERNEL
INVOKES
THE
SCHEDULER
BEFORE
CONTINUING
THE
FLAG
IS
PER
PROCESS
AND
NOT
SIMPLY
GLOBAL
BECAUSE
IT
IS
FASTER
TO
ACCESS
A
VALUE
IN
THE
PROCESS
DESCRIPTOR
BECAUSE
OF
THE
SPEED
OF
CURRENT
AND
HIGH
PROBABILITY
OF
IT
BEING
CACHE
HOT
THAN
A
GLOBAL
VARIABLE
HISTORICALLY
THE
FLAG
WAS
GLOBAL
BEFORE
THE
KERNEL
IN
AND
THE
FLAG
WAS
AN
INT
INSIDE
THE
IN
IT
WAS
MOVED
INTO
A
SIN
GLE
BIT
OF
A
SPECIAL
FLAG
VARIABLE
INSIDE
THE
STRUCTURE
USER
PREEMPTION
USER
PREEMPTION
OCCURS
WHEN
THE
KERNEL
IS
ABOUT
TO
RETURN
TO
USER
SPACE
IS
SET
AND
THEREFORE
THE
SCHEDULER
IS
INVOKED
IF
THE
KERNEL
IS
RETURNING
TO
USER
SPACE
IT
KNOWS
IT
IS
IN
A
SAFE
QUIESCENT
STATE
IN
OTHER
WORDS
IF
IT
IS
SAFE
TO
CONTINUE
EXECUTING
THE
CURRENT
TASK
IT
IS
ALSO
SAFE
TO
PICK
A
NEW
TASK
TO
EXECUTE
CONSEQUENTLY
WHENEVER
THE
KER
NEL
IS
PREPARING
TO
RETURN
TO
USER
SPACE
EITHER
ON
RETURN
FROM
AN
INTERRUPT
OR
AFTER
A
SYS
TEM
CALL
THE
VALUE
OF
IS
CHECKED
IF
IT
IS
SET
THE
SCHEDULER
IS
INVOKED
TO
SELECT
A
NEW
MORE
FIT
PROCESS
TO
EXECUTE
BOTH
THE
RETURN
PATHS
FOR
RETURN
FROM
INTERRUPT
AND
RETURN
FROM
SYSTEM
CALL
ARE
ARCHITECTURE
DEPENDENT
AND
TYPICALLY
IMPLEMENTED
IN
ASSEMBLY
IN
ENTRY
WHICH
ASIDE
FROM
KERNEL
ENTRY
CODE
ALSO
CONTAINS
KERNEL
EXIT
CODE
IN
SHORT
USER
PREEMPTION
CAN
OCCUR
N
WHEN
RETURNING
TO
USER
SPACE
FROM
A
SYSTEM
CALL
N
WHEN
RETURNING
TO
USER
SPACE
FROM
AN
INTERRUPT
HANDLER
KERNEL
PREEMPTION
THE
LINUX
KERNEL
UNLIKE
MOST
OTHER
UNIX
VARIANTS
AND
MANY
OTHER
OPERATING
SYSTEMS
IS
A
FULLY
PREEMPTIVE
KERNEL
IN
NONPREEMPTIVE
KERNELS
KERNEL
CODE
RUNS
UNTIL
COMPLETION
THAT
IS
THE
SCHEDULER
CANNOT
RESCHEDULE
A
TASK
WHILE
IT
IS
IN
THE
KERNEL
KERNEL
CODE
IS
SCHEDULED
COOPERATIVELY
NOT
PREEMPTIVELY
KERNEL
CODE
RUNS
UNTIL
IT
FINISHES
RETURNS
TO
USER
SPACE
OR
EXPLICITLY
BLOCKS
IN
THE
KERNEL
HOWEVER
THE
LINUX
KERNEL
BECAME
PRE
EMPTIVE
IT
IS
NOW
POSSIBLE
TO
PREEMPT
A
TASK
AT
ANY
POINT
SO
LONG
AS
THE
KERNEL
IS
IN
A
STATE
IN
WHICH
IT
IS
SAFE
TO
RESCHEDULE
SO
WHEN
IS
IT
SAFE
TO
RESCHEDULE
THE
KERNEL
CAN
PREEMPT
A
TASK
RUNNING
IN
THE
KERNEL
SO
LONG
AS
IT
DOES
NOT
HOLD
A
LOCK
THAT
IS
LOCKS
ARE
USED
AS
MARKERS
OF
REGIONS
OF
NONPRE
EMPTIBILITY
BECAUSE
THE
KERNEL
IS
SMP
SAFE
IF
A
LOCK
IS
NOT
HELD
THE
CURRENT
CODE
IS
REEN
TRANT
AND
CAPABLE
OF
BEING
PREEMPTED
THE
FIRST
CHANGE
IN
SUPPORTING
KERNEL
PREEMPTION
WAS
THE
ADDITION
OF
A
PREEMPTION
COUNTER
TO
EACH
PROCESS
THIS
COUNTER
BEGINS
AT
ZERO
AND
INCREMENTS
ONCE
FOR
EACH
LOCK
THAT
IS
ACQUIRED
AND
DECREMENTS
ONCE
FOR
EACH
LOCK
THAT
IS
RELEASED
WHEN
THE
COUNTER
IS
ZERO
THE
KERNEL
IS
PREEMPTIBLE
UPON
RETURN
FROM
INTERRUPT
IF
RETURNING
TO
KERNEL
SPACE
THE
KERNEL
CHECKS
THE
VALUES
OF
AND
IF
IS
SET
AND
IS
ZERO
THEN
A
MORE
IMPOR
TANT
TASK
IS
RUNNABLE
AND
IT
IS
SAFE
TO
PREEMPT
THUS
THE
SCHEDULER
IS
INVOKED
IF
IS
NONZERO
A
LOCK
IS
HELD
AND
IT
IS
UNSAFE
TO
RESCHEDULE
IN
THAT
CASE
THE
INTERRUPT
RETURNS
AS
USUAL
TO
THE
CURRENTLY
EXECUTING
TASK
WHEN
ALL
THE
LOCKS
THAT
THE
CUR
RENT
TASK
IS
HOLDING
ARE
RELEASED
RETURNS
TO
ZERO
AT
THAT
TIME
THE
UNLOCK
CODE
CHECKS
WHETHER
IS
SET
IF
SO
THE
SCHEDULER
IS
INVOKED
ENABLING
AND
DISABLING
KERNEL
PREEMPTION
IS
SOMETIMES
REQUIRED
IN
KERNEL
CODE
AND
IS
DISCUSSED
IN
CHAPTER
KERNEL
PREEMPTION
CAN
ALSO
OCCUR
EXPLICITLY
WHEN
A
TASK
IN
THE
KERNEL
BLOCKS
OR
EXPLICITLY
CALLS
SCHEDULE
THIS
FORM
OF
KERNEL
PREEMPTION
HAS
ALWAYS
BEEN
SUPPORTED
BECAUSE
NO
ADDITIONAL
LOGIC
IS
REQUIRED
TO
ENSURE
THAT
THE
KERNEL
IS
IN
A
STATE
THAT
IS
SAFE
TO
PREEMPT
IT
IS
ASSUMED
THAT
THE
CODE
THAT
EXPLICITLY
CALLS
SCHEDULE
KNOWS
IT
IS
SAFE
TO
RESCHEDULE
KERNEL
PREEMPTION
CAN
OCCUR
N
WHEN
AN
INTERRUPT
HANDLER
EXITS
BEFORE
RETURNING
TO
KERNEL
SPACE
N
WHEN
KERNEL
CODE
BECOMES
PREEMPTIBLE
AGAIN
N
IF
A
TASK
IN
THE
KERNEL
EXPLICITLY
CALLS
SCHEDULE
N
IF
A
TASK
IN
THE
KERNEL
BLOCKS
WHICH
RESULTS
IN
A
CALL
TO
SCHEDULE
REAL
TIME
SCHEDULING
POLICIES
LINUX
PROVIDES
TWO
REAL
TIME
SCHEDULING
POLICIES
AND
THE
NOR
MAL
NOT
REAL
TIME
SCHEDULING
POLICY
IS
VIA
THE
SCHEDULING
CLASSES
FRAMEWORK
THESE
REAL
TIME
POLICIES
ARE
MANAGED
NOT
BY
THE
COMPLETELY
FAIR
SCHEDULER
BUT
BY
A
SPE
CIAL
REAL
TIME
SCHEDULER
DEFINED
IN
KERNEL
C
THE
REST
OF
THIS
SECTION
DISCUSSES
THE
REAL
TIME
SCHEDULING
POLICIES
AND
ALGORITHM
IMPLEMENTS
A
SIMPLE
FIRST
IN
FIRST
OUT
SCHEDULING
ALGORITHM
WITHOUT
TIMESLICES
A
RUNNABLE
TASK
IS
ALWAYS
SCHEDULED
OVER
ANY
TASKS
WHEN
A
TASK
BECOMES
RUNNABLE
IT
CONTINUES
TO
RUN
UNTIL
IT
BLOCKS
OR
EXPLIC
ITLY
YIELDS
THE
PROCESSOR
IT
HAS
NO
TIMESLICE
AND
CAN
RUN
INDEFINITELY
ONLY
A
HIGHER
PRIOR
ITY
OR
TASK
CAN
PREEMPT
A
TASK
TWO
OR
MORE
TASKS
AT
THE
SAME
PRIORITY
RUN
ROUND
ROBIN
BUT
AGAIN
ONLY
YIELDING
THE
PROCESSOR
WHEN
THEY
EXPLICITLY
CHOOSE
TO
DO
SO
IF
A
TASK
IS
RUNNABLE
ALL
TASKS
AT
A
LOWER
PRIORITY
CANNOT
RUN
UNTIL
IT
BECOMES
UNRUNNABLE
IS
IDENTICAL
TO
EXCEPT
THAT
EACH
PROCESS
CAN
RUN
ONLY
UNTIL
IT
EXHAUSTS
A
PREDETERMINED
TIMESLICE
THAT
IS
IS
WITH
TIMESLICES
IT
IS
A
REAL
TIME
ROUND
ROBIN
SCHEDULING
ALGORITHM
WHEN
A
TASK
EXHAUSTS
ITS
TIMES
LICE
ANY
OTHER
REAL
TIME
PROCESSES
AT
ITS
PRIORITY
ARE
SCHEDULED
ROUND
ROBIN
THE
TIMESLICE
IS
USED
TO
ALLOW
ONLY
RESCHEDULING
OF
SAME
PRIORITY
PROCESSES
AS
WITH
A
HIGHER
PRIORITY
PROCESS
ALWAYS
IMMEDIATELY
PREEMPTS
A
LOWER
PRIORITY
ONE
AND
A
LOWER
PRIORITY
PROCESS
CAN
NEVER
PREEMPT
A
TASK
EVEN
IF
ITS
TIMESLICE
IS
EXHAUSTED
BOTH
REAL
TIME
SCHEDULING
POLICIES
IMPLEMENT
STATIC
PRIORITIES
THE
KERNEL
DOES
NOT
CAL
CULATE
DYNAMIC
PRIORITY
VALUES
FOR
REAL
TIME
TASKS
THIS
ENSURES
THAT
A
REAL
TIME
PROCESS
AT
A
GIVEN
PRIORITY
ALWAYS
PREEMPTS
A
PROCESS
AT
A
LOWER
PRIORITY
THE
REAL
TIME
SCHEDULING
POLICIES
IN
LINUX
PROVIDE
SOFT
REAL
TIME
BEHAVIOR
SOFT
REAL
TIME
REFERS
TO
THE
NOTION
THAT
THE
KERNEL
TRIES
TO
SCHEDULE
APPLICATIONS
WITHIN
TIMING
DEADLINES
BUT
THE
KERNEL
DOES
NOT
PROMISE
TO
ALWAYS
ACHIEVE
THESE
GOALS
CONVERSELY
HARD
REAL
TIME
SYSTEMS
ARE
GUARANTEED
TO
MEET
ANY
SCHEDULING
REQUIREMENTS
WITHIN
CERTAIN
LIM
ITS
LINUX
MAKES
NO
GUARANTEES
ON
THE
CAPABILITY
TO
SCHEDULE
REAL
TIME
TASKS
DESPITE
NOT
HAVING
A
DESIGN
THAT
GUARANTEES
HARD
REAL
TIME
BEHAVIOR
THE
REAL
TIME
SCHEDULING
PER
FORMANCE
IN
LINUX
IS
QUITE
GOOD
THE
LINUX
KERNEL
IS
CAPABLE
OF
MEETING
STRINGENT
TIMING
REQUIREMENTS
REAL
TIME
PRIORITIES
RANGE
INCLUSIVELY
FROM
ZERO
TO
MINUS
BY
DEFAULT
IS
THEREFORE
THE
DEFAULT
REAL
TIME
PRIORITY
RANGE
IS
ZERO
TO
THIS
PRIORITY
SPACE
IS
SHARED
WITH
THE
NICE
VALUES
OF
TASKS
THEY
USE
THE
SPACE
FROM
TO
BY
DEFAULT
THIS
MEANS
THE
TO
NICE
RANGE
MAPS
DIRECTLY
ONTO
THE
PRIORITY
SPACE
FROM
TO
SCHEDULER
RELATED
SYSTEM
CALLS
LINUX
PROVIDES
A
FAMILY
OF
SYSTEM
CALLS
FOR
THE
MANAGEMENT
OF
SCHEDULER
PARAMETERS
THESE
SYSTEM
CALLS
ALLOW
MANIPULATION
OF
PROCESS
PRIORITY
SCHEDULING
POLICY
AND
PROCESSOR
AFFINITY
AS
WELL
AS
PROVIDE
AN
EXPLICIT
MECHANISM
TO
YIELD
THE
PROCESSOR
TO
OTHER
TASKS
VARIOUS
BOOKS
AND
YOUR
FRIENDLY
SYSTEM
MAN
PAGES
PROVIDE
REFERENCE
TO
THESE
SYS
TEM
CALLS
WHICH
ARE
ALL
IMPLEMENTED
IN
THE
C
LIBRARY
WITHOUT
MUCH
WRAPPER
THEY
JUST
INVOKE
THE
SYSTEM
CALL
TABLE
LISTS
THE
SYSTEM
CALLS
AND
PROVIDES
A
BRIEF
DESCRIPTION
HOW
SYSTEM
CALLS
ARE
IMPLEMENTED
IN
THE
KERNEL
IS
DISCUSSED
IN
CHAPTER
SYSTEM
CALLS
TABLE
SCHEDULER
RELATED
SYSTEM
CALLS
SYSTEM
CALL
DESCRIPTION
NICE
SETS
A
PROCESS
NICE
VALUE
SETS
A
PROCESS
SCHEDULING
POLICY
GETS
A
PROCESS
SCHEDULING
POLICY
SETS
A
PROCESS
REAL
TIME
PRIORITY
GETS
A
PROCESS
REAL
TIME
PRIORITY
GETS
THE
MAXIMUM
REAL
TIME
PRIORITY
GETS
THE
MINIMUM
REAL
TIME
PRIORITY
GETS
A
PROCESS
TIMESLICE
VALUE
SETS
A
PROCESS
PROCESSOR
AFFINITY
GETS
A
PROCESS
PROCESSOR
AFFINITY
TEMPORARILY
YIELDS
THE
PROCESSOR
SCHEDULING
POLICY
AND
PRIORITY
RELATED
SYSTEM
CALLS
THE
AND
SYSTEM
CALLS
SET
AND
GET
A
GIVEN
PROCESS
SCHEDULING
POLICY
AND
REAL
TIME
PRIORITY
RESPECTIVELY
THEIR
IMPLEMENTATION
LIKE
MOST
SYSTEM
CALLS
INVOLVES
A
LOT
OF
ARGUMENT
CHECKING
SETUP
AND
CLEANUP
THE
IMPORTANT
WORK
HOWEVER
IS
MERELY
TO
READ
OR
WRITE
THE
POLICY
AND
VALUES
IN
THE
PROCESS
THE
AND
SYSTEM
CALLS
SET
AND
GET
A
PROCESS
REAL
TIME
PRIORITY
THESE
CALLS
MERELY
ENCODE
IN
A
SPECIAL
STRUCTURE
THE
CALLS
AND
RETURN
THE
MAXIMUM
AND
MINIMUM
PRIORITIES
RESPECTIVELY
FOR
A
GIVEN
SCHEDULING
POLICY
THE
MAXIMUM
PRIORITY
FOR
THE
REAL
TIME
POLICIES
IS
MINUS
ONE
THE
MINI
MUM
IS
ONE
FOR
NORMAL
TASKS
THE
NICE
FUNCTION
INCREMENTS
THE
GIVEN
PROCESS
STATIC
PRIORITY
BY
THE
GIVEN
AMOUNT
ONLY
ROOT
CAN
PROVIDE
A
NEGATIVE
VALUE
THEREBY
LOWERING
THE
NICE
VALUE
AND
INCREASING
THE
PRIORITY
THE
NICE
FUNCTION
CALLS
THE
KERNEL
FUNCTION
WHICH
SETS
THE
AND
PRIO
VALUES
IN
THE
TASK
AS
APPROPRIATE
PROCESSOR
AFFINITY
SYSTEM
CALLS
THE
LINUX
SCHEDULER
ENFORCES
HARD
PROCESSOR
AFFINITY
THAT
IS
ALTHOUGH
IT
TRIES
TO
PROVIDE
SOFT
OR
NATURAL
AFFINITY
BY
ATTEMPTING
TO
KEEP
PROCESSES
ON
THE
SAME
PROCESSOR
THE
SCHED
ULER
ALSO
ENABLES
A
USER
TO
SAY
THIS
TASK
MUST
REMAIN
ON
THIS
SUBSET
OF
THE
AVAILABLE
PROCESSORS
NO
MATTER
WHAT
THIS
HARD
AFFINITY
IS
STORED
AS
A
BITMASK
IN
THE
TASK
AS
THE
BITMASK
CONTAINS
ONE
BIT
PER
POSSIBLE
PROCESSOR
ON
THE
SYSTEM
BY
DEFAULT
ALL
BITS
ARE
SET
AND
THEREFORE
A
PROCESS
IS
POTENTIALLY
RUNNABLE
ON
ANY
PROCESSOR
THE
USER
HOWEVER
VIA
CAN
PROVIDE
A
DIFFERENT
BIT
MASK
OF
ANY
COMBINATION
OF
ONE
OR
MORE
BITS
LIKEWISE
THE
CALL
RETURNS
THE
CURRENT
BITMASK
THE
KERNEL
ENFORCES
HARD
AFFINITY
IN
A
SIMPLE
MANNER
FIRST
WHEN
A
PROCESS
IS
INITIALLY
CREATED
IT
INHERITS
ITS
PARENT
AFFINITY
MASK
BECAUSE
THE
PARENT
IS
RUNNING
ON
AN
ALLOWED
PROCESSOR
THE
CHILD
THUS
RUNS
ON
AN
ALLOWED
PROCESSOR
SECOND
WHEN
A
PROCESSOR
AFFINITY
IS
CHANGED
THE
KERNEL
USES
THE
MIGRATION
THREADS
TO
PUSH
THE
TASK
ONTO
A
LEGAL
PROCESSOR
FINALLY
THE
LOAD
BALANCER
PULLS
TASKS
TO
ONLY
AN
ALLOWED
PROCESSOR
THEREFORE
A
PROCESS
ONLY
EVER
RUNS
ON
A
PROCESSOR
WHOSE
BIT
IS
SET
IN
THE
FIELD
OF
ITS
PROCESS
DESCRIPTOR
YIELDING
PROCESSOR
TIME
LINUX
PROVIDES
THE
SYSTEM
CALL
AS
A
MECHANISM
FOR
A
PROCESS
TO
EXPLICITLY
YIELD
THE
PROCESSOR
TO
OTHER
WAITING
PROCESSES
IT
WORKS
BY
REMOVING
THE
PROCESS
FROM
THE
ACTIVE
ARRAY
WHERE
IT
CURRENTLY
IS
BECAUSE
IT
IS
RUNNING
AND
INSERTING
IT
INTO
THE
EXPIRED
ARRAY
THIS
HAS
THE
EFFECT
OF
NOT
ONLY
PREEMPTING
THE
PROCESS
AND
PUTTING
IT
AT
THE
END
OF
ITS
PRIORITY
LIST
BUT
ALSO
PUTTING
IT
ON
THE
EXPIRED
LIST
GUARANTEEING
IT
WILL
NOT
RUN
FOR
A
WHILE
BECAUSE
REAL
TIME
TASKS
NEVER
EXPIRE
THEY
ARE
A
SPECIAL
CASE
THEREFORE
THEY
ARE
MERELY
MOVED
TO
THE
END
OF
THEIR
PRIORITY
LIST
AND
NOT
INSERTED
INTO
THE
EXPIRED
ARRAY
IN
EARLIER
VERSIONS
OF
LINUX
THE
SEMANTICS
OF
THE
CALL
WERE
QUITE
DIFFERENT
AT
BEST
THE
TASK
WAS
MOVED
ONLY
TO
THE
END
OF
ITS
PRIORITY
LIST
THE
YIELDING
WAS
OFTEN
NOT
FOR
A
LONG
TIME
NOWADAYS
APPLICATIONS
AND
EVEN
KERNEL
CODE
SHOULD
BE
CERTAIN
THEY
TRULY
WANT
TO
GIVE
UP
THE
PROCESSOR
BEFORE
CALLING
KERNEL
CODE
AS
A
CONVENIENCE
CAN
CALL
YIELD
WHICH
ENSURES
THAT
THE
TASK
STATE
IS
AND
THEN
CALL
USER
SPACE
APPLICATIONS
USE
THE
SYSTEM
CALL
CONCLUSION
THE
PROCESS
SCHEDULER
IS
AN
IMPORTANT
PART
OF
ANY
KERNEL
BECAUSE
RUNNING
PROCESSES
IS
FOR
MOST
OF
US
AT
LEAST
THE
POINT
OF
USING
THE
COMPUTER
IN
THE
FIRST
PLACE
JUGGLING
THE
DEMANDS
OF
PROCESS
SCHEDULING
IS
NONTRIVIAL
HOWEVER
A
LARGE
NUMBER
OF
RUNNABLE
PROCESSES
SCALABILITY
CONCERNS
TRADE
OFFS
BETWEEN
LATENCY
AND
THROUGHPUT
AND
THE
DEMANDS
OF
VARIOUS
WORKLOADS
MAKE
A
ONE
SIZE
FITS
ALL
ALGORITHM
HARD
TO
ACHIEVE
THE
LINUX
KERNEL
NEW
CFS
PROCESS
SCHEDULER
HOWEVER
COMES
CLOSE
TO
APPEASING
ALL
PARTIES
AND
PROVIDING
AN
OPTIMAL
SOLUTION
FOR
MOST
USE
CASES
WITH
GOOD
SCALABILITY
THROUGH
A
NOVEL
INTERESTING
APPROACH
THE
PREVIOUS
CHAPTER
COVERED
PROCESS
MANAGEMENT
THIS
CHAPTER
RUMINATED
ON
THE
THEORY
BEHIND
PROCESS
SCHEDULING
AND
THE
SPECIFIC
IMPLEMENTATION
ALGORITHMS
AND
INTER
FACES
USED
BY
THE
CURRENT
LINUX
KERNEL
THE
NEXT
CHAPTER
COVERS
THE
PRIMARY
INTERFACE
THAT
THE
KERNEL
PROVIDES
TO
RUNNING
PROCESSES
SYSTEM
CALLS
SYSTEM
CALLS
IN
ANY
MODERN
OPERATING
SYSTEM
THE
KERNEL
PROVIDES
A
SET
OF
INTERFACES
BY
WHICH
PROCESSES
RUNNING
IN
USER
SPACE
CAN
INTERACT
WITH
THE
SYSTEM
THESE
INTERFACES
GIVE
APPLI
CATIONS
CONTROLLED
ACCESS
TO
HARDWARE
A
MECHANISM
WITH
WHICH
TO
CREATE
NEW
PROCESSES
AND
COMMUNICATE
WITH
EXISTING
ONES
AND
THE
CAPABILITY
TO
REQUEST
OTHER
OPERATING
SYSTEM
RESOURCES
THE
INTERFACES
ACT
AS
THE
MESSENGERS
BETWEEN
APPLICATIONS
AND
THE
KERNEL
WITH
THE
APPLICATIONS
ISSUING
VARIOUS
REQUESTS
AND
THE
KERNEL
FULFILLING
THEM
OR
RETURNING
AN
ERROR
THE
EXISTENCE
OF
THESE
INTERFACES
AND
THE
FACT
THAT
APPLICATIONS
ARE
NOT
FREE
TO
DIRECTLY
DO
WHATEVER
THEY
WANT
IS
KEY
TO
PROVIDING
A
STABLE
SYSTEM
COMMUNICATING
WITH
THE
KERNEL
SYSTEM
CALLS
PROVIDE
A
LAYER
BETWEEN
THE
HARDWARE
AND
USER
SPACE
PROCESSES
THIS
LAYER
SERVES
THREE
PRIMARY
PURPOSES
FIRST
IT
PROVIDES
AN
ABSTRACTED
HARDWARE
INTERFACE
FOR
USER
SPACE
WHEN
READING
OR
WRITING
FROM
A
FILE
FOR
EXAMPLE
APPLICATIONS
ARE
NOT
CONCERNED
WITH
THE
TYPE
OF
DISK
MEDIA
OR
EVEN
THE
TYPE
OF
FILESYSTEM
ON
WHICH
THE
FILE
RESIDES
SEC
OND
SYSTEM
CALLS
ENSURE
SYSTEM
SECURITY
AND
STABILITY
WITH
THE
KERNEL
ACTING
AS
A
MIDDLE
MAN
BETWEEN
SYSTEM
RESOURCES
AND
USER
SPACE
THE
KERNEL
CAN
ARBITRATE
ACCESS
BASED
ON
PERMISSIONS
USERS
AND
OTHER
CRITERIA
FOR
EXAMPLE
THIS
ARBITRATION
PREVENTS
APPLICATIONS
FROM
INCORRECTLY
USING
HARDWARE
STEALING
OTHER
PROCESSES
RESOURCES
OR
OTHERWISE
DOING
HARM
TO
THE
SYSTEM
FINALLY
A
SINGLE
COMMON
LAYER
BETWEEN
USER
SPACE
AND
THE
REST
OF
THE
SYSTEM
ALLOWS
FOR
THE
VIRTUALIZED
SYSTEM
PROVIDED
TO
PROCESSES
DISCUSSED
IN
CHAPTER
PROCESS
MANAGEMENT
IF
APPLICATIONS
WERE
FREE
TO
ACCESS
SYSTEM
RESOURCES
WITHOUT
THE
KERNEL
KNOWLEDGE
IT
WOULD
BE
NEARLY
IMPOSSIBLE
TO
IMPLEMENT
MULTITASKING
AND
VIRTUAL
MEMORY
AND
CERTAINLY
IMPOSSIBLE
TO
DO
SO
WITH
STABILITY
AND
SECURITY
IN
LINUX
SYSTEM
CALLS
ARE
THE
ONLY
MEANS
USER
SPACE
HAS
OF
INTERFACING
WITH
THE
KERNEL
THEY
ARE
THE
ONLY
LEGAL
ENTRY
POINT
INTO
THE
KERNEL
OTHER
THAN
EXCEPTIONS
AND
TRAPS
INDEED
OTHER
INTERFACES
SUCH
AS
DEVICE
FILES
OR
PROC
ARE
ULTIMATELY
ACCESSED
VIA
SYSTEM
CALLS
INTERESTINGLY
LINUX
IMPLEMENTS
FAR
FEWER
SYSTEM
CALLS
THAN
MOST
SYSTEMS
CHAPTER
ADDRESSES
THE
ROLE
AND
IMPLEMENTATION
OF
SYSTEM
CALLS
IN
LINUX
APIS
POSIX
AND
THE
C
LIBRARY
TYPICALLY
APPLICATIONS
ARE
PROGRAMMED
AGAINST
AN
APPLICATION
PROGRAMMING
INTERFACE
API
IMPLEMENTED
IN
USER
SPACE
NOT
DIRECTLY
TO
SYSTEM
CALLS
THIS
IS
IMPORTANT
BECAUSE
NO
DIRECT
CORRELATION
IS
NEEDED
BETWEEN
THE
INTERFACES
THAT
APPLICATIONS
MAKE
USE
OF
AND
THE
ACTUAL
INTERFACE
PROVIDED
BY
THE
KERNEL
AN
API
DEFINES
A
SET
OF
PROGRAMMING
INTER
FACES
USED
BY
APPLICATIONS
THOSE
INTERFACES
CAN
BE
IMPLEMENTED
AS
A
SYSTEM
CALL
IMPLE
MENTED
THROUGH
MULTIPLE
SYSTEM
CALLS
OR
IMPLEMENTED
WITHOUT
THE
USE
OF
SYSTEM
CALLS
AT
ALL
THE
SAME
API
CAN
EXIST
ON
MULTIPLE
SYSTEMS
AND
PROVIDE
THE
SAME
INTERFACE
TO
APPLICA
TIONS
WHILE
THE
IMPLEMENTATION
OF
THE
API
ITSELF
CAN
DIFFER
GREATLY
FROM
SYSTEM
TO
SYSTEM
SEE
FIGURE
FOR
AN
EXAMPLE
OF
THE
RELATIONSHIP
BETWEEN
A
POSIX
API
THE
C
LIBRARY
AND
SYSTEM
CALLS
APPLICATION
C
LIBRARY
KERNEL
FIGURE
THE
RELATIONSHIP
BETWEEN
APPLICATIONS
THE
C
LIBRARY
AND
THE
KERNEL
WITH
A
CALL
TO
PRINTF
ONE
OF
THE
MORE
COMMON
APPLICATION
PROGRAMMING
INTERFACES
IN
THE
UNIX
WORLD
IS
BASED
ON
THE
POSIX
STANDARD
TECHNICALLY
POSIX
IS
COMPOSED
OF
A
SERIES
OF
STANDARDS
FROM
THE
THAT
AIM
TO
PROVIDE
A
PORTABLE
OPERATING
SYSTEM
STANDARD
ROUGHLY
BASED
ON
UNIX
LINUX
STRIVES
TO
BE
POSIX
AND
COMPLIANT
WHERE
APPLICABLE
POSIX
IS
AN
EXCELLENT
EXAMPLE
OF
THE
RELATIONSHIP
BETWEEN
APIS
AND
SYSTEM
CALLS
ON
MOST
UNIX
SYSTEMS
THE
POSIX
DEFINED
API
CALLS
HAVE
A
STRONG
CORRELATION
TO
THE
SYSTEM
CALLS
INDEED
THE
POSIX
STANDARD
WAS
CREATED
TO
RESEMBLE
THE
INTERFACES
PROVIDED
BY
EAR
LIER
UNIX
SYSTEMS
ON
THE
OTHER
HAND
SOME
SYSTEMS
THAT
ARE
RATHER
UN
UNIX
SUCH
AS
MICROSOFT
WINDOWS
OFFER
POSIX
COMPATIBLE
LIBRARIES
THERE
ARE
ABOUT
SYSTEM
CALLS
ARE
ON
EACH
ARCHITECTURE
IS
ALLOWED
TO
DEFINE
UNIQUE
SYSTEM
CALLS
ALTHOUGH
NOT
ALL
OPERATING
SYSTEMS
PUBLISH
THEIR
EXACT
SYSTEM
CALLS
SOME
OPERATING
SYSTEMS
ARE
ESTIMATED
TO
HAVE
MORE
THAN
ONE
THOUSAND
IN
THE
PREVIOUS
EDITION
OF
THIS
BOOK
HAD
ONLY
SYS
TEM
CALLS
IEEE
EYE
TRIPLE
E
IS
THE
INSTITUTE
OF
ELECTRICAL
AND
ELECTRONICS
ENGINEERS
IT
IS
A
NONPROFIT
PROFESSIONAL
ASSOCIATION
INVOLVED
IN
NUMEROUS
TECHNICAL
AREAS
AND
RESPONSIBLE
FOR
MANY
IMPORTANT
STANDARDS
SUCH
AS
POSIX
FOR
MORE
INFORMATION
VISIT
HTTP
WWW
IEEE
ORG
SYSCALLS
THE
SYSTEM
CALL
INTERFACE
IN
LINUX
AS
WITH
MOST
UNIX
SYSTEMS
IS
PROVIDED
IN
PART
BY
THE
C
LIBRARY
THE
C
LIBRARY
IMPLEMENTS
THE
MAIN
API
ON
UNIX
SYSTEMS
INCLUDING
THE
STANDARD
C
LIBRARY
AND
THE
SYSTEM
CALL
INTERFACE
THE
C
LIBRARY
IS
USED
BY
ALL
C
PROGRAMS
AND
BECAUSE
OF
C
NATURE
IS
EASILY
WRAPPED
BY
OTHER
PROGRAMMING
LANGUAGES
FOR
USE
IN
THEIR
PROGRAMS
THE
C
LIBRARY
ADDITIONALLY
PROVIDES
THE
MAJORITY
OF
THE
POSIX
API
FROM
THE
APPLICATION
PROGRAMMER
POINT
OF
VIEW
SYSTEM
CALLS
ARE
IRRELEVANT
ALL
THE
PROGRAMMER
IS
CONCERNED
WITH
IS
THE
API
CONVERSELY
THE
KERNEL
IS
CONCERNED
ONLY
WITH
THE
SYSTEM
CALLS
WHAT
LIBRARY
CALLS
AND
APPLICATIONS
MAKE
USE
OF
THE
SYSTEM
CALLS
IS
NOT
OF
THE
KERNEL
CONCERN
NONETHELESS
IT
IS
IMPORTANT
FOR
THE
KERNEL
TO
KEEP
TRACK
OF
THE
POTENTIAL
USES
OF
A
SYSTEM
CALL
AND
KEEP
THE
SYSTEM
CALL
AS
GENERAL
AND
FLEXIBLE
AS
POSSIBLE
A
MEME
RELATED
TO
INTERFACES
IN
UNIX
IS
PROVIDE
MECHANISM
NOT
POLICY
IN
OTHER
WORDS
UNIX
SYSTEM
CALLS
EXIST
TO
PROVIDE
A
SPECIFIC
FUNCTION
IN
AN
ABSTRACT
SENSE
THE
MANNER
IN
WHICH
THE
FUNCTION
IS
USED
IS
NOT
ANY
OF
THE
KERNEL
BUSINESS
SYSCALLS
SYSTEM
CALLS
OFTEN
CALLED
SYSCALLS
IN
LINUX
ARE
TYPICALLY
ACCESSED
VIA
FUNCTION
CALLS
DEFINED
IN
THE
C
LIBRARY
THEY
CAN
DEFINE
ZERO
ONE
OR
MORE
ARGUMENTS
INPUTS
AND
MIGHT
RESULT
IN
ONE
OR
MORE
SIDE
EFFECTS
FOR
EXAMPLE
WRITING
TO
A
FILE
OR
COPYING
SOME
DATA
INTO
A
PROVIDED
POINTER
SYSTEM
CALLS
ALSO
PROVIDE
A
RETURN
VALUE
OF
TYPE
THAT
SIGNIFIES
SUC
CESS
OR
ERROR
USUALLY
ALTHOUGH
NOT
ALWAYS
A
NEGATIVE
RETURN
VALUE
DENOTES
AN
ERROR
A
RETURN
VALUE
OF
ZERO
IS
USUALLY
BUT
AGAIN
NOT
ALWAYS
A
SIGN
OF
SUCCESS
THE
C
LIBRARY
WHEN
A
SYSTEM
CALL
RETURNS
AN
ERROR
WRITES
A
SPECIAL
ERROR
CODE
INTO
THE
GLOBAL
ERRNO
VARIABLE
THIS
VARIABLE
CAN
BE
TRANSLATED
INTO
HUMAN
READABLE
ERRORS
VIA
LIBRARY
FUNCTIONS
SUCH
AS
PERROR
FINALLY
SYSTEM
CALLS
HAVE
A
DEFINED
BEHAVIOR
FOR
EXAMPLE
THE
SYSTEM
CALL
GETPID
IS
DEFINED
TO
RETURN
AN
INTEGER
THAT
IS
THE
CURRENT
PROCESS
PID
THE
IMPLEMENTATION
OF
THIS
SYSCALL
IN
THE
KERNEL
IS
SIMPLE
GETPID
RETURN
CURRENT
RETURNS
CURRENT
TGID
NOTE
THAT
THE
DEFINITION
SAYS
NOTHING
OF
THE
IMPLEMENTATION
THE
KERNEL
MUST
PROVIDE
THE
INTENDED
BEHAVIOR
OF
THE
SYSTEM
CALL
BUT
IS
FREE
TO
DO
SO
WITH
WHATEVER
IMPLEMENTATION
NOTE
THE
MIGHT
HERE
ALTHOUGH
NEARLY
ALL
SYSTEM
CALLS
HAVE
A
SIDE
EFFECT
THAT
IS
THEY
RESULT
IN
SOME
CHANGE
OF
THE
SYSTEM
STATE
A
FEW
SYSCALLS
SUCH
AS
GETPID
MERELY
RETURN
SOME
DATA
FROM
THE
KERNEL
THE
USE
OF
TYPE
LONG
IS
FOR
COMPATIBILITY
WITH
BIT
ARCHITECTURES
IT
WANTS
AS
LONG
AS
THE
RESULT
IS
CORRECT
OF
COURSE
THIS
SYSTEM
CALL
IS
AS
SIMPLE
AS
THEY
COME
AND
THERE
ARE
NOT
TOO
MANY
OTHER
WAYS
TO
IMPLEMENT
IT
IS
SIMPLY
A
MACRO
THAT
DEFINES
A
SYSTEM
CALL
WITH
NO
PARAMETERS
HENCE
THE
THE
EXPANDED
CODE
LOOKS
LIKE
THIS
ASMLINKAGE
LONG
VOID
LET
LOOK
AT
HOW
SYSTEM
CALLS
ARE
DEFINED
FIRST
NOTE
THE
ASMLINKAGE
MODIFIER
ON
THE
FUNCTION
DEFINITION
THIS
IS
A
DIRECTIVE
TO
TELL
THE
COMPILER
TO
LOOK
ONLY
ON
THE
STACK
FOR
THIS
FUNCTION
ARGUMENTS
THIS
IS
A
REQUIRED
MODIFIER
FOR
ALL
SYSTEM
CALLS
SECOND
THE
FUNC
TION
RETURNS
A
LONG
FOR
COMPATIBILITY
BETWEEN
AND
BIT
SYSTEMS
SYSTEM
CALLS
DEFINED
TO
RETURN
AN
INT
IN
USER
SPACE
RETURN
A
LONG
IN
THE
KERNEL
THIRD
NOTE
THAT
THE
GETPID
SYSTEM
CALL
IS
DEFINED
AS
IN
THE
KERNEL
THIS
IS
THE
NAMING
CON
VENTION
TAKEN
WITH
ALL
SYSTEM
CALLS
IN
LINUX
SYSTEM
CALL
BAR
IS
IMPLEMENTED
IN
THE
KER
NEL
AS
FUNCTION
SYSTEM
CALL
NUMBERS
IN
LINUX
EACH
SYSTEM
CALL
IS
ASSIGNED
A
SYSCALL
NUMBER
THIS
IS
A
UNIQUE
NUMBER
THAT
IS
USED
TO
REFERENCE
A
SPECIFIC
SYSTEM
CALL
WHEN
A
USER
SPACE
PROCESS
EXECUTES
A
SYSTEM
CALL
THE
SYSCALL
NUMBER
IDENTIFIES
WHICH
SYSCALL
WAS
EXECUTED
THE
PROCESS
DOES
NOT
REFER
TO
THE
SYSCALL
BY
NAME
THE
SYSCALL
NUMBER
IS
IMPORTANT
WHEN
ASSIGNED
IT
CANNOT
CHANGE
OR
COMPILED
APPLI
CATIONS
WILL
BREAK
LIKEWISE
IF
A
SYSTEM
CALL
IS
REMOVED
ITS
SYSTEM
CALL
NUMBER
CANNOT
BE
RECYCLED
OR
PREVIOUSLY
COMPILED
CODE
WOULD
AIM
TO
INVOKE
ONE
SYSTEM
CALL
BUT
WOULD
IN
REALITY
INVOKE
ANOTHER
LINUX
PROVIDES
A
NOT
IMPLEMENTED
SYSTEM
CALL
WHICH
DOES
NOTHING
EXCEPT
RETURN
ENOSYS
THE
ERROR
CORRESPONDING
TO
AN
INVALID
SYSTEM
CALL
THIS
FUNCTION
IS
USED
TO
PLUG
THE
HOLE
IN
THE
RARE
EVENT
THAT
A
SYSCALL
IS
REMOVED
OR
OTHERWISE
MADE
UNAVAILABLE
THE
KERNEL
KEEPS
A
LIST
OF
ALL
REGISTERED
SYSTEM
CALLS
IN
THE
SYSTEM
CALL
TABLE
STORED
IN
THIS
TABLE
IS
ARCHITECTURE
ON
IT
IS
DEFINED
IN
ARCH
KERNEL
C
THIS
TABLE
ASSIGNS
EACH
VALID
SYSCALL
TO
A
UNIQUE
SYSCALL
NUMBER
SYSTEM
CALL
PERFORMANCE
SYSTEM
CALLS
IN
LINUX
ARE
FASTER
THAN
IN
MANY
OTHER
OPERATING
SYSTEMS
THIS
IS
PARTLY
BECAUSE
OF
LINUX
FAST
CONTEXT
SWITCH
TIMES
ENTERING
AND
EXITING
THE
KERNEL
IS
A
STREAM
LINED
AND
SIMPLE
AFFAIR
THE
OTHER
FACTOR
IS
THE
SIMPLICITY
OF
THE
SYSTEM
CALL
HANDLER
AND
THE
INDIVIDUAL
SYSTEM
CALLS
THEMSELVES
YOU
MIGHT
BE
WONDERING
WHY
DOES
GETPID
RETURN
TGID
THE
THREAD
GROUP
ID
IN
NORMAL
PROCESS
ES
THE
TGID
IS
EQUAL
TO
THE
PID
WITH
THREADS
THE
TGID
IS
THE
SAME
FOR
ALL
THREADS
IN
A
THREAD
GROUP
THIS
ENABLES
THE
THREADS
TO
CALL
GETPID
AND
GET
THE
SAME
PID
SYSTEM
CALL
HANDLER
SYSTEM
CALL
HANDLER
IT
IS
NOT
POSSIBLE
FOR
USER
SPACE
APPLICATIONS
TO
EXECUTE
KERNEL
CODE
DIRECTLY
THEY
CANNOT
SIMPLY
MAKE
A
FUNCTION
CALL
TO
A
METHOD
EXISTING
IN
KERNEL
SPACE
BECAUSE
THE
KERNEL
EXISTS
IN
A
PROTECTED
MEMORY
SPACE
IF
APPLICATIONS
COULD
DIRECTLY
READ
AND
WRITE
TO
THE
KERNEL
ADDRESS
SPACE
SYSTEM
SECURITY
AND
STABILITY
WOULD
BE
NONEXISTENT
INSTEAD
USER
SPACE
APPLICATIONS
MUST
SOMEHOW
SIGNAL
TO
THE
KERNEL
THAT
THEY
WANT
TO
EXECUTE
A
SYSTEM
CALL
AND
HAVE
THE
SYSTEM
SWITCH
TO
KERNEL
MODE
WHERE
THE
SYSTEM
CALL
CAN
BE
EXECUTED
IN
KERNEL
SPACE
BY
THE
KERNEL
ON
BEHALF
OF
THE
APPLICATION
THE
MECHANISM
TO
SIGNAL
THE
KERNEL
IS
A
SOFTWARE
INTERRUPT
INCUR
AN
EXCEPTION
AND
THE
SYSTEM
WILL
SWITCH
TO
KERNEL
MODE
AND
EXECUTE
THE
EXCEPTION
HANDLER
THE
EXCEPTION
HANDLER
IN
THIS
CASE
IS
ACTUALLY
THE
SYSTEM
CALL
HANDLER
THE
DEFINED
SOFTWARE
INTERRUPT
ON
IS
INTERRUPT
NUMBER
WHICH
IS
INCURRED
VIA
THE
INT
INSTRUCTION
IT
TRIGGERS
A
SWITCH
TO
KERNEL
MODE
AND
THE
EXECUTION
OF
EXCEPTION
VECTOR
WHICH
IS
THE
SYSTEM
CALL
HANDLER
THE
SYSTEM
CALL
HANDLER
IS
THE
APTLY
NAMED
FUNCTION
IT
IS
ARCHITECTURE
DEPENDENT
ON
IT
IS
IMPLEMENTED
IN
ASSEMBLY
IN
RECENTLY
PROCESSORS
ADDED
A
FEATURE
KNOWN
AS
SYSENTER
THIS
FEATURE
PROVIDES
A
FASTER
MORE
SPECIALIZED
WAY
OF
TRAPPING
INTO
A
KERNEL
TO
EXECUTE
A
SYSTEM
CALL
THAN
USING
THE
INT
INTERRUPT
INSTRUCTION
SUPPORT
FOR
THIS
FEATURE
WAS
QUICKLY
ADDED
TO
THE
KERNEL
REGARDLESS
OF
HOW
THE
SYSTEM
CALL
HANDLER
IS
INVOKED
HOWEVER
THE
IMPORTANT
NOTION
IS
THAT
SOMEHOW
USER
SPACE
CAUSES
AN
EXCEPTION
OR
TRAP
TO
ENTER
THE
KERNEL
DENOTING
THE
CORRECT
SYSTEM
CALL
SIMPLY
ENTERING
KERNEL
SPACE
ALONE
IS
NOT
SUFFICIENT
BECAUSE
MULTIPLE
SYSTEM
CALLS
EXIST
ALL
OF
WHICH
ENTER
THE
KERNEL
IN
THE
SAME
MANNER
THUS
THE
SYSTEM
CALL
NUMBER
MUST
BE
PASSED
INTO
THE
KERNEL
ON
THE
SYSCALL
NUMBER
IS
FED
TO
THE
KERNEL
VIA
THE
EAX
REGIS
TER
BEFORE
CAUSING
THE
TRAP
INTO
THE
KERNEL
USER
SPACE
STICKS
IN
EAX
THE
NUMBER
CORRE
SPONDING
TO
THE
DESIRED
SYSTEM
CALL
THE
SYSTEM
CALL
HANDLER
THEN
READS
THE
VALUE
FROM
EAX
OTHER
ARCHITECTURES
DO
SOMETHING
SIMILAR
THE
FUNCTION
CHECKS
THE
VALIDITY
OF
THE
GIVEN
SYSTEM
CALL
NUMBER
BY
COMPARING
IT
TO
IF
IT
IS
LARGER
THAN
OR
EQUAL
TO
THE
FUNCTION
RETURNS
ENOSYS
OTHERWISE
THE
SPECIFIED
SYSTEM
CALL
IS
INVOKED
CALL
RAX
BECAUSE
EACH
ELEMENT
IN
THE
SYSTEM
CALL
TABLE
IS
BITS
BYTES
THE
KERNEL
MULTIPLIES
THE
GIVEN
SYSTEM
CALL
NUMBER
BY
FOUR
TO
ARRIVE
AT
ITS
LOCATION
IN
THE
SYSTEM
CALL
TABLE
ON
THE
CODE
IS
SIMILAR
WITH
THE
REPLACED
BY
SEE
FIGURE
MUCH
OF
THE
FOLLOWING
DESCRIPTION
OF
THE
SYSTEM
CALL
HANDLER
IS
BASED
ON
THE
VERSION
THEY
ARE
ALL
SIMILAR
FIGURE
INVOKING
THE
SYSTEM
CALL
HANDLER
AND
EXECUTING
A
SYSTEM
CALL
PARAMETER
PASSING
IN
ADDITION
TO
THE
SYSTEM
CALL
NUMBER
MOST
SYSCALLS
REQUIRE
THAT
ONE
OR
MORE
PARAMETERS
BE
PASSED
TO
THEM
SOMEHOW
USER
SPACE
MUST
RELAY
THE
PARAMETERS
TO
THE
KERNEL
DURING
THE
TRAP
THE
EASIEST
WAY
TO
DO
THIS
IS
VIA
THE
SAME
MEANS
THAT
THE
SYSCALL
NUMBER
IS
PASSED
THE
PARAMETERS
ARE
STORED
IN
REGISTERS
ON
THE
REGISTERS
EBX
ECX
EDX
ESI
AND
EDI
CONTAIN
IN
ORDER
THE
FIRST
FIVE
ARGUMENTS
IN
THE
UNLIKELY
CASE
OF
SIX
OR
MORE
ARGU
MENTS
A
SINGLE
REGISTER
IS
USED
TO
HOLD
A
POINTER
TO
USER
SPACE
WHERE
ALL
THE
PARAMETERS
ARE
STORED
THE
RETURN
VALUE
IS
SENT
TO
USER
SPACE
ALSO
VIA
REGISTER
ON
IT
IS
WRITTEN
INTO
THE
EAX
REGISTER
SYSTEM
CALL
IMPLEMENTATION
THE
ACTUAL
IMPLEMENTATION
OF
A
SYSTEM
CALL
IN
LINUX
DOES
NOT
NEED
TO
BE
CONCERNED
WITH
THE
BEHAVIOR
OF
THE
SYSTEM
CALL
HANDLER
THUS
ADDING
A
NEW
SYSTEM
CALL
TO
LINUX
IS
RELA
TIVELY
EASY
THE
HARD
WORK
LIES
IN
DESIGNING
AND
IMPLEMENTING
THE
SYSTEM
CALL
REGISTERING
IT
WITH
THE
KERNEL
IS
SIMPLE
LET
LOOK
AT
THE
STEPS
INVOLVED
IN
WRITING
A
NEW
SYSTEM
CALL
FOR
LINUX
IMPLEMENTING
SYSTEM
CALLS
THE
FIRST
STEP
IN
IMPLEMENTING
A
SYSTEM
CALL
IS
DEFINING
ITS
PURPOSE
WHAT
WILL
IT
DO
THE
SYSCALL
SHOULD
HAVE
EXACTLY
ONE
PURPOSE
MULTIPLEXING
SYSCALLS
A
SINGLE
SYSTEM
CALL
THAT
DOES
WILDLY
DIFFERENT
THINGS
DEPENDING
ON
A
FLAG
ARGUMENT
IS
DISCOURAGED
IN
LINUX
LOOK
AT
IOCTL
AS
AN
EXAMPLE
OF
WHAT
NOT
TO
DO
WHAT
ARE
THE
NEW
SYSTEM
CALL
ARGUMENTS
RETURN
VALUE
AND
ERROR
CODES
THE
SYSTEM
CALL
SHOULD
HAVE
A
CLEAN
AND
SIMPLE
INTERFACE
WITH
THE
SMALLEST
NUMBER
OF
ARGUMENTS
POSSI
BLE
THE
SEMANTICS
AND
BEHAVIOR
OF
A
SYSTEM
CALL
ARE
IMPORTANT
THEY
MUST
NOT
CHANGE
BECAUSE
EXISTING
APPLICATIONS
WILL
COME
TO
RELY
ON
THEM
BE
FORWARD
THINKING
CONSIDER
HOW
THE
FUNCTION
MIGHT
CHANGE
OVER
TIME
CAN
NEW
FUNCTIONALITY
BE
ADDED
TO
YOUR
SYSTEM
CALL
OR
WILL
ANY
CHANGE
REQUIRE
AN
ENTIRELY
NEW
FUNCTION
CAN
YOU
EASILY
FIX
BUGS
WITHOUT
BREAKING
BACKWARD
COMPATIBILITY
MANY
SYSTEM
CALLS
PROVIDE
A
FLAG
ARGUMENT
TO
ADDRESS
FORWARD
COMPATIBILITY
THE
FLAG
IS
NOT
USED
TO
MULTIPLEX
DIFFERENT
BEHAVIOR
ACROSS
A
SINGLE
SYSTEM
CALL
AS
MENTIONED
THAT
IS
NOT
ACCEPTABLE
BUT
TO
ENABLE
NEW
FUNCTIONALITY
AND
OPTIONS
WITHOUT
BREAKING
BACKWARD
COMPATIBILITY
OR
NEEDING
TO
ADD
A
NEW
SYSTEM
CALL
DESIGNING
THE
INTERFACE
WITH
AN
EYE
TOWARD
THE
FUTURE
IS
IMPORTANT
ARE
YOU
NEEDLESSLY
LIMITING
THE
FUNCTION
DESIGN
THE
SYSTEM
CALL
TO
BE
AS
GENERAL
AS
POSSIBLE
DO
NOT
ASSUME
ITS
USE
TODAY
WILL
BE
THE
SAME
AS
ITS
USE
TOMORROW
THE
PURPOSE
OF
THE
SYSTEM
CALL
WILL
REMAIN
CONSTANT
BUT
ITS
USES
MAY
CHANGE
IS
THE
SYSTEM
CALL
PORTABLE
DO
NOT
MAKE
ASSUMPTIONS
ABOUT
AN
ARCHITECTURE
WORD
SIZE
OR
ENDIANNESS
CHAPTER
PORTABILITY
DIS
CUSSES
THESE
ISSUES
MAKE
SURE
YOU
ARE
NOT
MAKING
POOR
ASSUMPTIONS
THAT
WILL
BREAK
THE
SYSTEM
CALL
IN
THE
FUTURE
REMEMBER
THE
UNIX
MOTTO
PROVIDE
MECHANISM
NOT
POLICY
WHEN
YOU
WRITE
A
SYSTEM
CALL
YOU
NEED
TO
REALIZE
THE
NEED
FOR
PORTABILITY
AND
ROBUST
NESS
NOT
JUST
TODAY
BUT
IN
THE
FUTURE
THE
BASIC
UNIX
SYSTEM
CALLS
HAVE
SURVIVED
THIS
TEST
OF
TIME
MOST
OF
THEM
ARE
JUST
AS
USEFUL
AND
APPLICABLE
TODAY
AS
THEY
WERE
YEARS
AGO
VERIFYING
THE
PARAMETERS
SYSTEM
CALLS
MUST
CAREFULLY
VERIFY
ALL
THEIR
PARAMETERS
TO
ENSURE
THAT
THEY
ARE
VALID
AND
LEGAL
THE
SYSTEM
CALL
RUNS
IN
KERNEL
SPACE
AND
IF
THE
USER
CAN
PASS
INVALID
INPUT
INTO
THE
KERNEL
WITHOUT
RESTRAINT
THE
SYSTEM
SECURITY
AND
STABILITY
CAN
SUFFER
FOR
EXAMPLE
FILE
I
O
SYSCALLS
MUST
CHECK
WHETHER
THE
FILE
DESCRIPTOR
IS
VALID
PROCESS
RELATED
FUNCTIONS
MUST
CHECK
WHETHER
THE
PROVIDED
PID
IS
VALID
EVERY
PARAMETER
MUST
BE
CHECKED
TO
ENSURE
IT
IS
NOT
JUST
VALID
AND
LEGAL
BUT
CORRECT
PROCESSES
MUST
NOT
ASK
THE
KER
NEL
TO
ACCESS
RESOURCES
TO
WHICH
THE
PROCESS
DOES
NOT
HAVE
ACCESS
ONE
OF
THE
MOST
IMPORTANT
CHECKS
IS
THE
VALIDITY
OF
ANY
POINTERS
THAT
THE
USER
PRO
VIDES
IMAGINE
IF
A
PROCESS
COULD
PASS
ANY
POINTER
INTO
THE
KERNEL
UNCHECKED
WITH
WARTS
AND
ALL
EVEN
PASSING
A
POINTER
TO
WHICH
IT
DID
NOT
HAVE
READ
ACCESS
PROCESSES
COULD
THEN
TRICK
THE
KERNEL
INTO
COPYING
DATA
FOR
WHICH
THEY
DID
NOT
HAVE
ACCESS
PERMISSION
SUCH
AS
DATA
BELONGING
TO
ANOTHER
PROCESS
OR
DATA
MAPPED
UNREADABLE
BEFORE
FOLLOWING
A
POINTER
INTO
USER
SPACE
THE
SYSTEM
MUST
ENSURE
THAT
N
THE
POINTER
POINTS
TO
A
REGION
OF
MEMORY
IN
USER
SPACE
PROCESSES
MUST
NOT
BE
ABLE
TO
TRICK
THE
KERNEL
INTO
READING
DATA
IN
KERNEL
SPACE
ON
THEIR
BEHALF
N
THE
POINTER
POINTS
TO
A
REGION
OF
MEMORY
IN
THE
PROCESS
ADDRESS
SPACE
THE
PROCESS
MUST
NOT
BE
ABLE
TO
TRICK
THE
KERNEL
INTO
READING
SOMEONE
ELSE
DATA
N
IF
READING
THE
MEMORY
IS
MARKED
READABLE
IF
WRITING
THE
MEMORY
IS
MARKED
WRITABLE
IF
EXECUTING
THE
MEMORY
IS
MARKED
EXECUTABLE
THE
PROCESS
MUST
NOT
BE
ABLE
TO
BYPASS
MEMORY
ACCESS
RESTRICTIONS
THE
KERNEL
PROVIDES
TWO
METHODS
FOR
PERFORMING
THE
REQUISITE
CHECKS
AND
THE
DESIRED
COPY
TO
AND
FROM
USER
SPACE
NOTE
KERNEL
CODE
MUST
NEVER
BLINDLY
FOLLOW
A
POINTER
INTO
USER
SPACE
ONE
OF
THESE
TWO
METHODS
MUST
ALWAYS
BE
USED
FOR
WRITING
INTO
USER
SPACE
THE
METHOD
IS
PROVIDED
IT
TAKES
THREE
PARAMETERS
THE
FIRST
IS
THE
DESTINATION
MEMORY
ADDRESS
IN
THE
PROCESS
ADDRESS
SPACE
THE
SECOND
IS
THE
SOURCE
POINTER
IN
KERNEL
SPACE
FINALLY
THE
THIRD
ARGUMENT
IS
THE
SIZE
IN
BYTES
OF
THE
DATA
TO
COPY
FOR
READING
FROM
USER
SPACE
THE
METHOD
IS
ANALOGOUS
TO
THE
FUNCTION
READS
FROM
THE
SECOND
PARAMETER
INTO
THE
FIRST
PARAMETER
THE
NUMBER
OF
BYTES
SPECIFIED
IN
THE
THIRD
PARAMETER
BOTH
OF
THESE
FUNCTIONS
RETURN
THE
NUMBER
OF
BYTES
THEY
FAILED
TO
COPY
ON
ERROR
ON
SUCCESS
THEY
RETURN
ZERO
IT
IS
STANDARD
FOR
THE
SYSCALL
TO
RETURN
EFAULT
IN
THE
CASE
OF
SUCH
AN
ERROR
LET
CONSIDER
AN
EXAMPLE
SYSTEM
CALL
THAT
USES
BOTH
AND
THIS
SYSCALL
IS
UTTERLY
WORTHLESS
IT
COPIES
DATA
FROM
ITS
FIRST
PARAMETER
INTO
ITS
SECOND
THIS
IS
SUBOPTIMAL
IN
THAT
IT
INVOLVES
AN
INTERMEDIATE
AND
EXTRANEOUS
COPY
INTO
KERNEL
SPACE
FOR
NO
GAIN
BUT
IT
HELPS
ILLUSTRATE
THE
POINT
POINTLESS
SYSCALL
THAT
COPIES
THE
LEN
BYTES
FROM
SRC
TO
DST
USING
THE
KERNEL
AS
AN
INTERMEDIARY
IN
THE
COPY
INTENDED
AS
AN
EXAMPLE
OF
COPYING
TO
AND
FROM
THE
KERNEL
UNSIGNED
LONG
SRC
UNSIGNED
LONG
DST
UNSIGNED
LONG
LEN
UNSIGNED
LONG
BUF
COPY
SRC
WHICH
IS
IN
THE
USER
ADDRESS
SPACE
INTO
BUF
IF
BUF
SRC
LEN
RETURN
EFAULT
COPY
BUF
INTO
DST
WHICH
IS
IN
THE
USER
ADDRESS
SPACE
IF
DST
BUF
LEN
RETURN
EFAULT
RETURN
AMOUNT
OF
DATA
COPIED
RETURN
LEN
BOTH
AND
MAY
BLOCK
THIS
OCCURS
FOR
EXAMPLE
IF
THE
PAGE
CONTAINING
THE
USER
DATA
IS
NOT
IN
PHYSICAL
MEMORY
BUT
IS
SWAPPED
TO
DISK
IN
THAT
CASE
THE
PROCESS
SLEEPS
UNTIL
THE
PAGE
FAULT
HANDLER
CAN
BRING
THE
PAGE
FROM
THE
SWAP
FILE
ON
DISK
INTO
PHYSICAL
MEMORY
A
FINAL
POSSIBLE
CHECK
IS
FOR
VALID
PERMISSION
IN
OLDER
VERSIONS
OF
LINUX
IT
WAS
STAN
DARD
FOR
SYSCALLS
THAT
REQUIRE
ROOT
PRIVILEGE
TO
USE
SUSER
THIS
FUNCTION
MERELY
CHECKED
WHETHER
A
USER
WAS
ROOT
THIS
IS
NOW
REMOVED
AND
A
FINER
GRAINED
CAPABILITIES
SYSTEM
IS
IN
PLACE
THE
NEW
SYSTEM
ENABLES
SPECIFIC
ACCESS
CHECKS
ON
SPECIFIC
RESOURCES
A
CALL
TO
CAPABLE
WITH
A
VALID
CAPABILITIES
FLAG
RETURNS
NONZERO
IF
THE
CALLER
HOLDS
THE
SPECIFIED
CAPABILITY
AND
ZERO
OTHERWISE
FOR
EXAMPLE
CAPABLE
CHECKS
WHETHER
THE
CALLER
HAS
THE
ABILITY
TO
MODIFY
NICE
VALUES
OF
OTHER
PROCESSES
BY
DEFAULT
THE
SUPERUSER
POSSESSES
ALL
CAPABILITIES
AND
NONROOT
POSSESSES
NONE
FOR
EXAMPLE
HERE
IS
THE
REBOOT
SYSTEM
CALL
NOTE
HOW
ITS
FIRST
STEP
IS
ENSURING
THAT
THE
CALLING
PROCESS
HAS
THE
IF
THAT
ONE
CONDITIONAL
STATEMENT
WERE
REMOVED
ANY
PROCESS
COULD
REBOOT
THE
SYSTEM
REBOOT
INT
INT
UNSIGNED
INT
CMD
VOID
USER
ARG
CHAR
BUFFER
WE
ONLY
TRUST
THE
SUPERUSER
WITH
REBOOTING
THE
SYSTEM
IF
CAPABLE
RETURN
EPERM
FOR
SAFETY
WE
REQUIRE
MAGIC
ARGUMENTS
IF
RETURN
EINVAL
INSTEAD
OF
TRYING
TO
MAKE
THE
CODE
LOOK
LIKE
HALT
WHEN
IS
NOT
SET
DO
IT
THE
EASY
WAY
IF
CMD
CMD
SWITCH
CMD
CASE
NULL
BREAK
CASE
BREAK
CASE
BREAK
CASE
BREAK
CASE
BREAK
CASE
IF
BUFFER
ARG
SIZEOF
BUFFER
RETURN
EFAULT
BUFFER
SIZEOF
BUFFER
BUFFER
BREAK
DEFAULT
RETURN
EINVAL
RETURN
SEE
LINUX
CAPABILITY
H
FOR
A
LIST
OF
ALL
CAPABILITIES
AND
WHAT
RIGHTS
THEY
ENTAIL
SYSTEM
CALL
CONTEXT
AS
DISCUSSED
IN
CHAPTER
THE
KERNEL
IS
IN
PROCESS
CONTEXT
DURING
THE
EXECUTION
OF
A
SYS
TEM
CALL
THE
CURRENT
POINTER
POINTS
TO
THE
CURRENT
TASK
WHICH
IS
THE
PROCESS
THAT
ISSUED
THE
SYSCALL
IN
PROCESS
CONTEXT
THE
KERNEL
IS
CAPABLE
OF
SLEEPING
FOR
EXAMPLE
IF
THE
SYSTEM
CALL
BLOCKS
ON
A
CALL
OR
EXPLICITLY
CALLS
SCHEDULE
AND
IS
FULLY
PREEMPTIBLE
THESE
TWO
POINTS
ARE
IMPORTANT
FIRST
THE
CAPABILITY
TO
SLEEP
MEANS
THAT
SYSTEM
CALLS
CAN
MAKE
USE
OF
THE
MAJORITY
OF
THE
KERNEL
FUNCTIONALITY
AS
WE
WILL
SEE
IN
CHAPTER
INTERRUPTS
AND
INTERRUPT
HANDLERS
THE
CAPABILITY
TO
SLEEP
GREATLY
SIMPLIFIES
KERNEL
PROGRAMMING
THE
FACT
THAT
PROCESS
CONTEXT
IS
PREEMPTIBLE
IMPLIES
THAT
LIKE
USER
SPACE
THE
CURRENT
TASK
MAY
BE
PREEMPTED
BY
ANOTHER
TASK
BECAUSE
THE
NEW
TASK
MAY
THEN
EXECUTE
THE
SAME
SYSTEM
CALL
CARE
MUST
BE
EXERCISED
TO
ENSURE
THAT
SYSTEM
CALLS
ARE
REENTRANT
OF
COURSE
THIS
IS
THE
SAME
CONCERN
THAT
SYMMETRICAL
MULTIPROCESSING
INTRODUCES
SYNCHRONIZING
REENTRANCY
IS
COVERED
IN
CHAPTER
AN
INTRODUCTION
TO
KERNEL
SYNCHRONIZATION
AND
CHAPTER
KERNEL
SYNCHRONIZATION
METHODS
WHEN
THE
SYSTEM
CALL
RETURNS
CONTROL
CONTINUES
IN
WHICH
ULTIMATELY
SWITCHES
TO
USER
SPACE
AND
CONTINUES
THE
EXECUTION
OF
THE
USER
PROCESS
FINAL
STEPS
IN
BINDING
A
SYSTEM
CALL
AFTER
THE
SYSTEM
CALL
IS
WRITTEN
IT
IS
TRIVIAL
TO
REGISTER
IT
AS
AN
OFFICIAL
SYSTEM
CALL
ADD
AN
ENTRY
TO
THE
END
OF
THE
SYSTEM
CALL
TABLE
THIS
NEEDS
TO
BE
DONE
FOR
EACH
ARCHITECTURE
THAT
SUPPORTS
THE
SYSTEM
CALL
WHICH
FOR
MOST
CALLS
IS
ALL
THE
ARCHITEC
TURES
THE
POSITION
OF
THE
SYSCALL
IN
THE
TABLE
STARTING
AT
ZERO
IS
ITS
SYSTEM
CALL
NUM
BER
FOR
EXAMPLE
THE
TENTH
ENTRY
IN
THE
LIST
IS
ASSIGNED
SYSCALL
NUMBER
NINE
FOR
EACH
SUPPORTED
ARCHITECTURE
DEFINE
THE
SYSCALL
NUMBER
IN
ASM
UNISTD
H
COMPILE
THE
SYSCALL
INTO
THE
KERNEL
IMAGE
AS
OPPOSED
TO
COMPILING
AS
A
MODULE
THIS
CAN
BE
AS
SIMPLE
AS
PUTTING
THE
SYSTEM
CALL
IN
A
RELEVANT
FILE
IN
KERNEL
SUCH
AS
SYS
C
WHICH
IS
HOME
TO
MISCELLANEOUS
SYSTEM
CALLS
LOOK
AT
THESE
STEPS
IN
MORE
DETAIL
WITH
A
FICTIONAL
SYSTEM
CALL
FOO
FIRST
WE
WANT
TO
ADD
TO
THE
SYSTEM
CALL
TABLE
FOR
MOST
ARCHITECTURES
THE
TABLE
IS
LOCATED
IN
ENTRY
AND
LOOKS
LIKE
THIS
ENTRY
LONG
LONG
LONG
LONG
LONG
LONG
LONG
LONG
LONG
INTERRUPT
HANDLERS
CANNOT
SLEEP
AND
THUS
ARE
MUCH
MORE
LIMITED
IN
WHAT
THEY
CAN
DO
THAN
SYSTEM
CALLS
RUNNING
IN
PROCESS
CONTEXT
LONG
LONG
LONG
LONG
LONG
LONG
LONG
THE
NEW
SYSTEM
CALL
IS
THEN
APPENDED
TO
THE
TAIL
OF
THIS
LIST
LONG
ALTHOUGH
IT
IS
NOT
EXPLICITLY
SPECIFIED
THE
SYSTEM
CALL
IS
THEN
GIVEN
THE
NEXT
SUBSEQUENT
SYSCALL
NUMBER
IN
THIS
CASE
FOR
EACH
ARCHITECTURE
YOU
WANT
TO
SUPPORT
THE
SYSTEM
CALL
MUST
BE
ADDED
TO
THE
ARCHITECTURE
SYSTEM
CALL
TABLE
THE
SYSTEM
CALL
DOES
NOT
NEED
TO
RECEIVE
THE
SAME
SYSCALL
NUMBER
UNDER
EACH
ARCHITECTURE
AS
THE
SYSTEM
CALL
NUMBER
IS
PART
OF
THE
ARCHITECTURE
UNIQUE
ABI
USUALLY
YOU
WOULD
WANT
TO
MAKE
THE
SYSTEM
CALL
AVAIL
ABLE
TO
EACH
ARCHITECTURE
NOTE
THE
CONVENTION
OF
PLACING
THE
NUMBER
IN
A
COMMENT
EVERY
FIVE
ENTRIES
THIS
MAKES
IT
EASY
TO
FIND
OUT
WHICH
SYSCALL
IS
ASSIGNED
WHICH
NUMBER
NEXT
THE
SYSTEM
CALL
NUMBER
IS
ADDED
TO
ASM
UNISTD
H
WHICH
CURRENTLY
LOOKS
SOMEWHAT
LIKE
THIS
THIS
FILE
CONTAINS
THE
SYSTEM
CALL
NUMBERS
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
DEFINE
THE
FOLLOWING
IS
THEN
ADDED
TO
THE
END
OF
THE
LIST
DEFINE
FINALLY
THE
ACTUAL
FOO
SYSTEM
CALL
IS
IMPLEMENTED
BECAUSE
THE
SYSTEM
CALL
MUST
BE
COMPILED
INTO
THE
CORE
KERNEL
IMAGE
IN
ALL
CONFIGURATIONS
IN
THIS
EXAMPLE
WE
DEFINE
IT
IN
KERNEL
SYS
C
YOU
SHOULD
PUT
IT
WHEREVER
THE
FUNCTION
IS
MOST
RELEVANT
FOR
EXAMPLE
IF
THE
FUNCTION
IS
RELATED
TO
SCHEDULING
YOU
COULD
DEFINE
IT
IN
KERNEL
SCHED
C
INCLUDE
ASM
PAGE
H
EVERYONE
FAVORITE
SYSTEM
CALL
RETURNS
THE
SIZE
OF
THE
PER
PROCESS
KERNEL
STACK
ASMLINKAGE
LONG
VOID
RETURN
THAT
IS
IT
BOOT
THIS
KERNEL
AND
USER
SPACE
CAN
INVOKE
THE
FOO
SYSTEM
CALL
ACCESSING
THE
SYSTEM
CALL
FROM
USER
SPACE
GENERALLY
THE
C
LIBRARY
PROVIDES
SUPPORT
FOR
SYSTEM
CALLS
USER
APPLICATIONS
CAN
PULL
IN
FUNCTION
PROTOTYPES
FROM
THE
STANDARD
HEADERS
AND
LINK
WITH
THE
C
LIBRARY
TO
USE
YOUR
SYSTEM
CALL
OR
THE
LIBRARY
ROUTINE
THAT
IN
TURN
USES
YOUR
SYSCALL
CALL
IF
YOU
JUST
WROTE
THE
SYSTEM
CALL
HOWEVER
IT
IS
DOUBTFUL
THAT
GLIBC
ALREADY
SUPPORTS
IT
THANKFULLY
LINUX
PROVIDES
A
SET
OF
MACROS
FOR
WRAPPING
ACCESS
TO
SYSTEM
CALLS
IT
SETS
UP
THE
REGISTER
CONTENTS
AND
ISSUES
THE
TRAP
INSTRUCTIONS
THESE
MACROS
ARE
NAMED
WHERE
N
IS
BETWEEN
AND
THE
NUMBER
CORRESPONDS
TO
THE
NUMBER
OF
PARAMETERS
PASSED
INTO
THE
SYSCALL
BECAUSE
THE
MACRO
NEEDS
TO
KNOW
HOW
MANY
PARAME
TERS
TO
EXPECT
AND
CONSEQUENTLY
PUSH
INTO
REGISTERS
FOR
EXAMPLE
CONSIDER
THE
SYSTEM
CALL
OPEN
DEFINED
AS
LONG
OPEN
CONST
CHAR
FILENAME
INT
FLAGS
INT
MODE
THE
SYSCALL
MACRO
TO
USE
THIS
SYSTEM
CALL
WITHOUT
EXPLICIT
LIBRARY
SUPPORT
WOULD
BE
DEFINE
LONG
OPEN
CONST
CHAR
FILENAME
INT
FLAGS
INT
MODE
THEN
THE
APPLICATION
CAN
SIMPLY
CALL
OPEN
FOR
EACH
MACRO
THERE
ARE
N
PARAMETERS
THE
FIRST
PARAMETER
CORRESPONDS
TO
THE
RETURN
TYPE
OF
THE
SYSCALL
THE
SECOND
IS
THE
NAME
OF
THE
SYSTEM
CALL
NEXT
FOLLOWS
THE
TYPE
AND
NAME
FOR
EACH
PARAMETER
IN
ORDER
OF
THE
SYSTEM
CALL
THE
DEFINE
IS
IN
ASM
UNISTD
H
IT
IS
THE
SYSTEM
CALL
NUMBER
THE
MACRO
EXPANDS
INTO
A
C
FUNCTION
WITH
INLINE
ASSEMBLY
THE
ASSEMBLY
PERFORMS
THE
STEPS
DISCUSSED
IN
THE
PREVIOUS
SECTION
TO
PUSH
THE
SYSTEM
CALL
NUMBER
AND
PARAMETERS
INTO
THE
CORRECT
REGISTERS
AND
ISSUE
THE
SOFTWARE
INTERRUPT
TO
TRAP
INTO
THE
KERNEL
PLACING
THIS
MACRO
IN
AN
APPLICATION
IS
ALL
THAT
IS
REQUIRED
TO
USE
THE
OPEN
SYSTEM
CALL
LET
WRITE
THE
MACRO
TO
USE
OUR
SPLENDID
NEW
FOO
SYSTEM
CALL
AND
THEN
WRITE
SOME
TEST
CODE
TO
SHOW
OFF
OUR
EFFORTS
DEFINE
LONG
FOO
INT
MAIN
LONG
FOO
PRINTF
THE
KERNEL
STACK
SIZE
IS
LD
N
RETURN
WHY
NOT
TO
IMPLEMENT
A
SYSTEM
CALL
THE
PREVIOUS
SECTIONS
HAVE
SHOWN
THAT
IT
IS
EASY
TO
IMPLEMENT
A
NEW
SYSTEM
CALL
BUT
THAT
IN
NO
WAY
SHOULD
ENCOURAGE
YOU
TO
DO
SO
INDEED
YOU
SHOULD
EXERCISE
CAUTION
AND
RESTRAINT
IN
ADDING
NEW
SYSCALLS
OFTEN
MUCH
MORE
VIABLE
ALTERNATIVES
TO
PROVIDING
A
NEW
SYSTEM
CALL
ARE
AVAILABLE
LET
LOOK
AT
THE
PROS
CONS
AND
ALTERNATIVES
THE
PROS
OF
IMPLEMENTING
A
NEW
INTERFACE
AS
A
SYSCALL
ARE
AS
FOLLOWS
N
SYSTEM
CALLS
ARE
SIMPLE
TO
IMPLEMENT
AND
EASY
TO
USE
N
SYSTEM
CALL
PERFORMANCE
ON
LINUX
IS
FAST
THE
CONS
N
YOU
NEED
A
SYSCALL
NUMBER
WHICH
NEEDS
TO
BE
OFFICIALLY
ASSIGNED
TO
YOU
N
AFTER
THE
SYSTEM
CALL
IS
IN
A
STABLE
SERIES
KERNEL
IT
IS
WRITTEN
IN
STONE
THE
INTERFACE
CANNOT
CHANGE
WITHOUT
BREAKING
USER
SPACE
APPLICATIONS
N
EACH
ARCHITECTURE
NEEDS
TO
SEPARATELY
REGISTER
THE
SYSTEM
CALL
AND
SUPPORT
IT
N
SYSTEM
CALLS
ARE
NOT
EASILY
USED
FROM
SCRIPTS
AND
CANNOT
BE
ACCESSED
DIRECTLY
FROM
THE
FILESYSTEM
N
BECAUSE
YOU
NEED
AN
ASSIGNED
SYSCALL
NUMBER
IT
IS
HARD
TO
MAINTAIN
AND
USE
A
SYS
TEM
CALL
OUTSIDE
OF
THE
MASTER
KERNEL
TREE
N
FOR
SIMPLE
EXCHANGES
OF
INFORMATION
A
SYSTEM
CALL
IS
OVERKILL
THE
ALTERNATIVES
N
IMPLEMENT
A
DEVICE
NODE
AND
READ
AND
WRITE
TO
IT
USE
IOCTL
TO
MANIPU
LATE
SPECIFIC
SETTINGS
OR
RETRIEVE
SPECIFIC
INFORMATION
N
CERTAIN
INTERFACES
SUCH
AS
SEMAPHORES
CAN
BE
REPRESENTED
AS
FILE
DESCRIPTORS
AND
MANIPULATED
AS
SUCH
N
ADD
THE
INFORMATION
AS
A
FILE
TO
THE
APPROPRIATE
LOCATION
IN
SYSFS
FOR
MANY
INTERFACES
SYSTEM
CALLS
ARE
THE
CORRECT
ANSWER
LINUX
HOWEVER
HAS
TRIED
TO
AVOID
SIMPLY
ADDING
A
SYSTEM
CALL
TO
SUPPORT
EACH
NEW
ABSTRACTION
THAT
COMES
ALONG
THE
RESULT
HAS
BEEN
AN
INCREDIBLY
CLEAN
SYSTEM
CALL
LAYER
WITH
FEW
REGRETS
OR
DEPRECATIONS
INTERFACES
NO
LONGER
USED
OR
SUPPORTED
THE
SLOW
RATE
OF
ADDITION
OF
NEW
SYSTEM
CALLS
IS
A
SIGN
THAT
LINUX
IS
A
RELATIVELY
STABLE
AND
FEATURE
COMPLETE
OPERATING
SYSTEM
CONCLUSION
IN
THIS
CHAPTER
WE
DISCUSSED
WHAT
SYSTEM
CALLS
ARE
AND
HOW
THEY
RELATE
TO
LIBRARY
CALLS
AND
THE
APPLICATION
PROGRAMMING
INTERFACE
API
WE
THEN
LOOKED
AT
HOW
THE
LINUX
KERNEL
IMPLEMENTS
SYSTEM
CALLS
AND
THE
CHAIN
OF
EVENTS
REQUIRED
TO
EXECUTE
A
SYSTEM
CALL
TRAPPING
INTO
THE
KERNEL
TRANSMITTING
THE
SYSCALL
NUMBER
AND
ANY
ARGUMENTS
EXECUTING
THE
CORRECT
SYSTEM
CALL
FUNCTION
AND
RETURNING
TO
USER
SPACE
WITH
THE
SYSCALL
RETURN
VALUE
WE
THEN
WENT
OVER
HOW
TO
ADD
SYSTEM
CALLS
AND
PROVIDED
A
SIMPLE
EXAMPLE
OF
USING
A
NEW
SYSTEM
CALL
FROM
USER
SPACE
THE
WHOLE
PROCESS
WAS
QUITE
EASY
AS
THE
SIMPLICITY
OF
ADDING
A
NEW
SYSTEM
CALL
DEMONSTRATES
THE
WORK
IS
ALL
IN
THE
SYSCALL
IMPLEMENTATION
THE
REST
OF
THIS
BOOK
DISCUSSES
CONCEPTS
AND
KERNEL
INTERFACES
NEEDED
TO
WRITE
WELL
BEHAVED
OPTIMAL
AND
SAFE
SYSTEM
CALLS
FINALLY
WE
WRAPPED
UP
THE
CHAPTER
WITH
A
DISCUSSION
ON
THE
PROS
AND
CONS
OF
IMPLE
MENTING
SYSTEM
CALLS
AND
A
BRIEF
LIST
OF
THE
ALTERNATIVES
TO
ADDING
NEW
ONES
KERNEL
DATA
STRUCTURES
THIS
CHAPTER
INTRODUCES
SEVERAL
BUILT
IN
DATA
STRUCTURES
FOR
USE
IN
LINUX
KERNEL
CODE
AS
WITH
ANY
LARGE
SOFTWARE
PROJECT
THE
LINUX
KERNEL
PROVIDES
THESE
GENERIC
DATA
STRUCTURES
AND
PRIMITIVES
TO
ENCOURAGE
CODE
REUSE
KERNEL
DEVELOPERS
SHOULD
USE
THESE
DATA
STRUC
TURES
WHENEVER
POSSIBLE
AND
NOT
ROLL
YOUR
OWN
SOLUTIONS
IN
THE
FOLLOWING
SECTIONS
WE
COVER
THE
MOST
USEFUL
OF
THESE
GENERIC
DATA
STRUCTURES
WHICH
ARE
THE
FOLLOWING
N
LINKED
LISTS
N
QUEUES
N
MAPS
N
BINARY
TREES
WE
CONCLUDE
THE
CHAPTER
WITH
A
DISCUSSION
ON
ALGORITHMIC
COMPLEXITY
THE
EASE
WITH
WHICH
ALGORITHMS
AND
DATA
STRUCTURES
SCALE
TO
SUPPORT
EVER
LARGER
INPUTS
LINKED
LISTS
THE
LINKED
LIST
IS
THE
SIMPLEST
AND
MOST
COMMON
DATA
STRUCTURE
IN
THE
LINUX
KERNEL
A
LINKED
LIST
IS
A
DATA
STRUCTURE
THAT
ALLOWS
THE
STORAGE
AND
MANIPULATION
OF
A
VARIABLE
NUMBER
OF
ELEMENTS
CALLED
THE
NODES
OF
THE
LIST
UNLIKE
IN
A
STATIC
ARRAY
THE
ELEMENTS
IN
A
LINKED
LIST
ARE
DYNAMICALLY
CREATED
AND
INSERTED
INTO
THE
LIST
THIS
ENABLES
THE
MANAGEMENT
OF
A
VARY
ING
NUMBER
OF
ELEMENTS
UNKNOWN
AT
COMPILE
TIME
BECAUSE
THE
ELEMENTS
ARE
CREATED
AT
DIFFERENT
TIMES
THEY
DO
NOT
NECESSARILY
OCCUPY
CONTIGUOUS
REGIONS
IN
MEMORY
THEREFORE
THE
ELEMENTS
NEED
TO
BE
LINKED
TOGETHER
THUS
EACH
ELEMENT
IN
THE
LIST
CONTAINS
A
POINTER
TO
THE
NEXT
ELEMENT
AS
ELEMENTS
ARE
ADDED
TO
OR
REMOVED
FROM
THE
LIST
THE
POINTER
TO
THE
NEXT
NODE
IS
SIMPLY
ADJUSTED
SINGLY
AND
DOUBLY
LINKED
LISTS
THE
SIMPLEST
DATA
STRUCTURE
REPRESENTING
SUCH
A
LINKED
LIST
MIGHT
LOOK
SIMILAR
TO
THE
FOLLOWING
AN
ELEMENT
IN
A
LINKED
LIST
STRUCT
VOID
DATA
THE
PAYLOAD
STRUCT
NEXT
POINTER
TO
THE
NEXT
ELEMENT
FIGURE
IS
A
LINKED
LIST
FIGURE
A
SINGLY
LINKED
LIST
IN
SOME
LINKED
LISTS
EACH
ELEMENT
ALSO
CONTAINS
A
POINTER
TO
THE
PREVIOUS
ELEMENT
THESE
LISTS
ARE
CALLED
DOUBLY
LINKED
LISTS
BECAUSE
THEY
ARE
LINKED
BOTH
FORWARD
AND
BACKWARD
LINKED
LISTS
SUCH
AS
THE
LIST
IN
FIGURE
THAT
DO
NOT
HAVE
A
POINTER
TO
THE
PREVIOUS
ELE
MENT
ARE
CALLED
SINGLY
LINKED
LISTS
A
DATA
STRUCTURE
REPRESENTING
A
DOUBLY
LINKED
LIST
WOULD
LOOK
SIMILAR
TO
THIS
AN
ELEMENT
IN
A
LINKED
LIST
STRUCT
VOID
DATA
THE
PAYLOAD
STRUCT
NEXT
POINTER
TO
THE
NEXT
ELEMENT
STRUCT
PREV
POINTER
TO
THE
PREVIOUS
ELEMENT
FIGURE
IS
A
DOUBLY
LINKED
LIST
NULL
PREV
NEXT
PREV
NEXT
PREV
NEXT
FIGURE
A
DOUBLY
LINKED
LIST
CIRCULAR
LINKED
LISTS
NORMALLY
BECAUSE
THE
LAST
ELEMENT
IN
A
LINKED
LIST
HAS
NO
NEXT
ELEMENT
IT
IS
SET
TO
POINT
TO
A
SPECIAL
VALUE
SUCH
AS
NULL
TO
INDICATE
IT
IS
THE
LAST
ELEMENT
IN
THE
LIST
IN
SOME
LINKED
LISTS
THE
LAST
ELEMENT
DOES
NOT
POINT
TO
A
SPECIAL
VALUE
INSTEAD
IT
POINTS
BACK
TO
THE
FIRST
VALUE
THIS
LINKED
LIST
IS
CALLED
A
CIRCULAR
LINKED
LIST
BECAUSE
THE
LIST
IS
CYCLIC
CIRCULAR
LINKED
LISTS
CAN
COME
IN
BOTH
DOUBLY
AND
SINGLY
LINKED
VERSIONS
IN
A
CIRCULAR
DOUBLY
LINKED
LIST
THE
FIRST
NODE
PREVIOUS
POINTER
POINTS
AT
THE
LAST
NODE
FIGURES
AND
ARE
SINGLY
AND
DOUBLY
CIRCULAR
LINKED
LISTS
RESPECTIVELY
FIGURE
A
CIRCULAR
SINGLY
LINKED
LIST
PREV
NEXT
PREV
NEXT
PREV
NEXT
FIGURE
A
CIRCULAR
DOUBLY
LINKED
LIST
ALTHOUGH
THE
LINUX
KERNEL
LINKED
LIST
IMPLEMENTATION
IS
UNIQUE
IT
IS
FUNDAMENTALLY
A
CIRCULAR
DOUBLY
LINKED
LIST
USING
THIS
TYPE
OF
LINKED
LIST
PROVIDES
THE
GREATEST
FLEXIBILITY
MOVING
THROUGH
A
LINKED
LIST
MOVEMENT
THROUGH
A
LINKED
LIST
OCCURS
LINEARLY
YOU
VISIT
ONE
ELEMENT
FOLLOW
THE
NEXT
POINTER
AND
VISIT
THE
NEXT
ELEMENT
RINSE
AND
REPEAT
THIS
IS
THE
EASIEST
METHOD
OF
MOVING
THROUGH
A
LINKED
LIST
AND
THE
ONE
FOR
WHICH
LINKED
LISTS
ARE
BEST
SUITED
LINKED
LISTS
ARE
ILL
SUITED
FOR
USE
CASES
WHERE
RANDOM
ACCESS
IS
AN
IMPORTANT
OPERATION
INSTEAD
YOU
USE
LINKED
LISTS
WHEN
ITERATING
OVER
THE
WHOLE
LIST
IS
IMPORTANT
AND
THE
DYNAMIC
ADDITION
AND
REMOVAL
OF
ELEMENTS
IS
REQUIRED
IN
LINKED
LIST
IMPLEMENTATIONS
THE
FIRST
ELEMENT
IS
OFTEN
REPRESENTED
BY
A
SPECIAL
POINTER
CALLED
THE
HEAD
THAT
ENABLES
EASY
ACCESS
TO
THE
START
OF
THE
LIST
IN
A
NONCIRCU
LAR
LINKED
LIST
THE
LAST
ELEMENT
IS
DELINEATED
BY
ITS
NEXT
POINTER
BEING
NULL
IN
A
CIRCULAR
LINKED
LIST
THE
LAST
ELEMENT
IS
DELINEATED
BECAUSE
IT
POINTS
TO
THE
HEAD
ELEMENT
TRAVERSING
THE
LIST
THEREFORE
OCCURS
LINEARLY
THROUGH
EACH
ELEMENT
FROM
THE
FIRST
TO
THE
LAST
IN
A
DOU
BLY
LINKED
LIST
MOVEMENT
CAN
ALSO
OCCUR
BACKWARD
LINEARLY
FROM
THE
LAST
ELEMENT
TO
THE
FIRST
OF
COURSE
GIVEN
A
SPECIFIC
ELEMENT
IN
THE
LIST
YOU
CAN
ITERATE
BACKWARD
AND
FORWARD
ANY
NUMBER
OF
ELEMENTS
TOO
YOU
NEED
NOT
TRAVERSE
THE
WHOLE
LIST
THE
LINUX
KERNEL
IMPLEMENTATION
IN
COMPARISON
TO
MOST
LINKED
LIST
IMPLEMENTATIONS
INCLUDING
THE
GENERIC
APPROACH
DESCRIBED
IN
THE
PREVIOUS
SECTIONS
THE
LINUX
KERNEL
IMPLEMENTATION
IS
UNIQUE
RECALL
FROM
THE
EARLIER
DISCUSSION
THAT
DATA
OR
A
GROUPING
OF
DATA
SUCH
AS
A
STRUCT
IS
MAINTAINED
IN
A
LINKED
LIST
BY
ADDING
A
NEXT
AND
PERHAPS
A
PREVIOUS
NODE
POINTER
TO
THE
DATA
FOR
EXAMPLE
ASSUME
WE
HAD
A
FOX
STRUCTURE
TO
DESCRIBE
THAT
MEMBER
OF
THE
CANIDAE
FAMILY
STRUCT
FOX
UNSIGNED
LONG
LENGTH
IN
CENTIMETERS
OF
TAIL
UNSIGNED
LONG
WEIGHT
WEIGHT
IN
KILOGRAMS
BOOL
IS
THIS
FOX
FANTASTIC
THE
COMMON
PATTERN
FOR
STORING
THIS
STRUCTURE
IN
A
LINKED
LIST
IS
TO
EMBED
THE
LIST
POINTER
IN
THE
STRUCTURE
FOR
EXAMPLE
STRUCT
FOX
UNSIGNED
LONG
LENGTH
IN
CENTIMETERS
OF
TAIL
UNSIGNED
LONG
WEIGHT
WEIGHT
IN
KILOGRAMS
BOOL
IS
THIS
FOX
FANTASTIC
STRUCT
FOX
NEXT
NEXT
FOX
IN
LINKED
LIST
STRUCT
FOX
PREV
PREVIOUS
FOX
IN
LINKED
LIST
THE
LINUX
KERNEL
APPROACH
IS
DIFFERENT
INSTEAD
OF
TURNING
THE
STRUCTURE
INTO
A
LINKED
LIST
THE
LINUX
APPROACH
IS
TO
EMBED
A
LINKED
LIST
NODE
IN
THE
STRUCTURE
THE
LINKED
LIST
STRUCTURE
IN
THE
OLD
DAYS
THERE
WERE
MULTIPLE
IMPLEMENTATIONS
OF
LINKED
LISTS
IN
THE
KERNEL
A
SINGLE
POWERFUL
LINKED
LIST
IMPLEMENTATION
WAS
NEEDED
TO
REMOVE
DUPLICATE
CODE
DURING
THE
KERNEL
DEVELOPMENT
SERIES
THE
OFFICIAL
KERNEL
LINKED
LIST
IMPLEMENTATION
WAS
INTRO
DUCED
ALL
EXISTING
USES
OF
LINKED
LISTS
NOW
USE
THE
OFFICIAL
IMPLEMENTATION
DO
NOT
REIN
VENT
THE
WHEEL
THE
LINKED
LIST
CODE
IS
DECLARED
IN
THE
HEADER
FILE
LINUX
LIST
H
AND
THE
DATA
STRUC
TURE
IS
SIMPLE
STRUCT
STRUCT
NEXT
STRUCT
PREV
THE
NEXT
POINTER
POINTS
TO
THE
NEXT
LIST
NODE
AND
THE
PREV
POINTER
POINTS
TO
THE
PRE
VIOUS
LIST
NODE
YET
SEEMINGLY
THIS
IS
NOT
PARTICULARLY
USEFUL
WHAT
VALUE
IS
A
GIANT
LINKED
LIST
OF
LINKED
LIST
NODES
THE
UTILITY
IS
IN
HOW
THE
STRUCTURE
IS
USED
STRUCT
FOX
UNSIGNED
LONG
LENGTH
IN
CENTIMETERS
OF
TAIL
UNSIGNED
LONG
WEIGHT
WEIGHT
IN
KILOGRAMS
BOOL
IS
THIS
FOX
FANTASTIC
STRUCT
LIST
LIST
OF
ALL
FOX
STRUCTURES
WITH
THIS
LIST
NEXT
IN
FOX
POINTS
TO
THE
NEXT
ELEMENT
AND
LIST
PREV
IN
FOX
POINTS
TO
THE
PREVIOUS
NOW
THIS
IS
BECOMING
USEFUL
BUT
IT
GETS
BETTER
THE
KERNEL
PROVIDES
A
FAMILY
OF
ROUTINES
TO
MANIPULATE
LINKED
LISTS
FOR
EXAMPLE
THE
METHOD
ADDS
A
NEW
NODE
TO
AN
EXISTING
LINKED
LIST
THESE
METHODS
HOWEVER
ARE
GENERIC
THEY
ACCEPT
ONLY
STRUCTURES
USING
THE
MACRO
WE
CAN
EASILY
FIND
THE
PAR
ENT
STRUCTURE
CONTAINING
ANY
GIVEN
MEMBER
VARIABLE
THIS
IS
BECAUSE
IN
C
THE
OFFSET
OF
A
GIVEN
VARIABLE
INTO
A
STRUCTURE
IS
FIXED
BY
THE
ABI
AT
COMPILE
TIME
DEFINE
PTR
TYPE
MEMBER
CONST
TYPEOF
TYPE
MEMBER
MPTR
PTR
TYPE
CHAR
MPTR
OFFSETOF
TYPE
MEMBER
USING
WE
CAN
DEFINE
A
SIMPLE
FUNCTION
TO
RETURN
THE
PARENT
STRUCTURE
CONTAINING
ANY
DEFINE
PTR
TYPE
MEMBER
PTR
TYPE
MEMBER
ARMED
WITH
THE
KERNEL
PROVIDES
ROUTINES
TO
CREATE
MANIPULATE
AND
OTHERWISE
MANAGE
LINKED
LISTS
ALL
WITHOUT
KNOWING
ANYTHING
ABOUT
THE
STRUCTURES
THAT
THE
RESIDES
WITHIN
DEFINING
A
LINKED
LIST
AS
SHOWN
A
BY
ITSELF
IS
WORTHLESS
IT
IS
NORMALLY
EMBEDDED
INSIDE
YOUR
OWN
STRUCTURE
STRUCT
FOX
UNSIGNED
LONG
LENGTH
IN
CENTIMETERS
OF
TAIL
UNSIGNED
LONG
WEIGHT
WEIGHT
IN
KILOGRAMS
BOOL
IS
THIS
FOX
FANTASTIC
STRUCT
LIST
LIST
OF
ALL
FOX
STRUCTURES
THE
LIST
NEEDS
TO
BE
INITIALIZED
BEFORE
IT
CAN
BE
USED
BECAUSE
MOST
OF
THE
ELEMENTS
ARE
CREATED
DYNAMICALLY
PROBABLY
WHY
YOU
NEED
A
LINKED
LIST
THE
MOST
COMMON
WAY
OF
INI
TIALIZING
THE
LINKED
LIST
IS
AT
RUNTIME
STRUCT
FOX
KMALLOC
SIZEOF
WEIGHT
FALSE
LIST
IF
THE
STRUCTURE
IS
STATICALLY
CREATED
AT
COMPILE
TIME
AND
YOU
HAVE
A
DIRECT
REFERENCE
TO
IT
YOU
CAN
SIMPLY
DO
THIS
STRUCT
FOX
WEIGHT
LIST
LIST
LIST
HEADS
THE
PREVIOUS
SECTION
SHOWS
HOW
EASY
IT
IS
TO
TAKE
AN
EXISTING
STRUCTURE
SUCH
AS
OUR
STRUCT
FOX
EXAMPLE
AND
TURN
IT
INTO
A
LINKED
LIST
WITH
SIMPLE
CODE
CHANGES
OUR
STRUC
TURE
IS
NOW
MANAGEABLE
BY
THE
KERNEL
LINKED
LIST
ROUTINES
BUT
BEFORE
WE
CAN
USE
THOSE
ROUTINES
WE
NEED
A
CANONICAL
POINTER
TO
REFER
TO
THE
LIST
AS
A
WHOLE
A
HEAD
POINTER
ONE
NICE
ASPECT
OF
THE
KERNEL
LINKED
LIST
IMPLEMENTATION
IS
THAT
OUR
FOX
NODES
ARE
INDISTINGUISHABLE
EACH
CONTAINS
A
AND
WE
CAN
ITERATE
FROM
ANY
ONE
NODE
TO
THE
NEXT
UNTIL
WE
HAVE
SEEN
EVERY
NODE
THIS
APPROACH
IS
ELEGANT
BUT
YOU
WILL
GENERALLY
WANT
A
SPECIAL
POINTER
THAT
REFERS
TO
YOUR
LINKED
LIST
WITHOUT
BEING
A
LIST
NODE
ITSELF
INTER
ESTINGLY
THIS
SPECIAL
NODE
IS
IN
FACT
A
NORMAL
STATIC
THIS
DEFINES
AND
INITIALIZES
A
NAMED
THE
MAJORITY
OF
THE
LINKED
LIST
ROUTINES
ACCEPT
ONE
OR
TWO
PARAMETERS
THE
HEAD
NODE
OR
THE
HEAD
NODE
PLUS
AN
ACTUAL
LIST
NODE
LET
LOOK
AT
THOSE
ROUTINES
MANIPULATING
LINKED
LISTS
THE
KERNEL
PROVIDES
A
FAMILY
OF
FUNCTIONS
TO
MANIPULATE
LINKED
LISTS
THEY
ALL
TAKE
POINT
ERS
TO
ONE
OR
MORE
STRUCTURES
THE
FUNCTIONS
ARE
IMPLEMENTED
AS
INLINE
FUNC
TIONS
IN
GENERIC
C
AND
CAN
BE
FOUND
IN
LINUX
LIST
H
INTERESTINGLY
ALL
THESE
FUNCTIONS
ARE
O
THIS
MEANS
THEY
EXECUTE
IN
CONSTANT
TIME
REGARDLESS
OF
THE
SIZE
OF
THE
LIST
OR
ANY
OTHER
INPUTS
FOR
EXAMPLE
IT
TAKES
THE
SAME
AMOUNT
OF
TIME
TO
ADD
OR
REMOVE
AN
ENTRY
TO
OR
FROM
A
LIST
WHETHER
THAT
LIST
HAS
OR
ENTRIES
THIS
IS
PERHAPS
NOT
SURPRISING
BUT
STILL
GOOD
TO
KNOW
ADDING
A
NODE
TO
A
LINKED
LIST
TO
ADD
A
NODE
TO
A
LINKED
LIST
STRUCT
NEW
STRUCT
HEAD
SEE
THE
SECTION
ALGORITHMIC
COMPLEXITY
LATER
IN
THIS
CHAPTER
FOR
A
DISCUSSION
ON
O
THIS
FUNCTION
ADDS
THE
NEW
NODE
TO
THE
GIVEN
LIST
IMMEDIATELY
AFTER
THE
HEAD
NODE
BECAUSE
THE
LIST
IS
CIRCULAR
AND
GENERALLY
HAS
NO
CONCEPT
OF
FIRST
OR
LAST
NODES
YOU
CAN
PASS
ANY
ELEMENT
FOR
HEAD
IF
YOU
DO
PASS
THE
LAST
ELEMENT
HOWEVER
THIS
FUNCTION
CAN
BE
USED
TO
IMPLEMENT
A
STACK
RETURNING
TO
OUR
FOX
EXAMPLE
ASSUME
WE
HAD
A
NEW
STRUCT
FOX
THAT
WE
WANTED
TO
ADD
TO
THE
LIST
WE
D
DO
THIS
F
LIST
TO
ADD
A
NODE
TO
THE
END
OF
A
LINKED
LIST
STRUCT
NEW
STRUCT
HEAD
THIS
FUNCTION
ADDS
THE
NEW
NODE
TO
THE
GIVEN
LIST
IMMEDIATELY
BEFORE
THE
HEAD
NODE
AS
WITH
BECAUSE
THE
LISTS
ARE
CIRCULAR
YOU
CAN
GENERALLY
PASS
ANY
ELEMENT
FOR
HEAD
THIS
FUNCTION
CAN
BE
USED
TO
IMPLEMENT
A
QUEUE
HOWEVER
IF
YOU
PASS
THE
FIRST
ELEMENT
DELETING
A
NODE
FROM
A
LINKED
LIST
AFTER
ADDING
A
NODE
TO
A
LINKED
LIST
DELETING
A
NODE
FROM
A
LIST
IS
THE
NEXT
MOST
IMPORTANT
OPERATION
TO
DELETE
A
NODE
FROM
A
LINKED
LIST
USE
STRUCT
ENTRY
THIS
FUNCTION
REMOVES
THE
ELEMENT
ENTRY
FROM
THE
LIST
NOTE
THAT
IT
DOES
NOT
FREE
ANY
MEMORY
BELONGING
TO
ENTRY
OR
THE
DATA
STRUCTURE
IN
WHICH
IT
IS
EMBEDDED
THIS
FUNCTION
MERELY
REMOVES
THE
ELEMENT
FROM
THE
LIST
AFTER
CALLING
THIS
YOU
WOULD
TYPICALLY
DESTROY
YOUR
DATA
STRUCTURE
AND
THE
INSIDE
IT
FOR
EXAMPLE
TO
DELETE
THE
FOX
NODE
WE
PREVIOUS
ADDED
TO
F
LIST
NOTE
THE
FUNCTION
DOES
NOT
RECEIVE
AS
INPUT
IT
SIMPLY
RECEIVES
A
SPECIFIC
NODE
AND
MODIFIES
THE
POINTERS
OF
THE
PREVIOUS
AND
SUBSEQUENT
NODES
SUCH
THAT
THE
GIVEN
NODE
IS
NO
LONGER
PART
OF
THE
LIST
THE
IMPLEMENTATION
IS
INSTRUCTIVE
STATIC
INLINE
VOID
STRUCT
PREV
STRUCT
NEXT
NEXT
PREV
PREV
PREV
NEXT
NEXT
STATIC
INLINE
VOID
STRUCT
ENTRY
ENTRY
PREV
ENTRY
NEXT
TO
DELETE
A
NODE
FROM
A
LINKED
LIST
AND
REINITIALIZE
IT
THE
KERNEL
PROVIDES
STRUCT
ENTRY
THIS
FUNCTION
BEHAVES
THE
SAME
AS
EXCEPT
IT
ALSO
REINITIALIZES
THE
GIVEN
WITH
THE
RATIONALE
THAT
YOU
NO
LONGER
WANT
THE
ENTRY
IN
THE
LIST
BUT
YOU
CAN
REUSE
THE
DATA
STRUCTURE
ITSELF
MOVING
AND
SPLICING
LINKED
LIST
NODES
TO
MOVE
A
NODE
FROM
ONE
LIST
TO
ANOTHER
STRUCT
LIST
STRUCT
HEAD
THIS
FUNCTION
REMOVES
THE
LIST
ENTRY
FROM
ITS
LINKED
LIST
AND
ADDS
IT
TO
THE
GIVEN
LIST
AFTER
THE
HEAD
ELEMENT
TO
MOVE
A
NODE
FROM
ONE
LIST
TO
THE
END
OF
ANOTHER
STRUCT
LIST
STRUCT
HEAD
THIS
FUNCTION
DOES
THE
SAME
AS
BUT
INSERTS
THE
LIST
ELEMENT
BEFORE
THE
HEAD
ENTRY
TO
CHECK
WHETHER
A
LIST
IS
EMPTY
STRUCT
HEAD
THIS
RETURNS
NONZERO
IF
THE
GIVEN
LIST
IS
EMPTY
OTHERWISE
IT
RETURNS
ZERO
TO
SPLICE
TWO
UNCONNECTED
LISTS
TOGETHER
STRUCT
LIST
STRUCT
HEAD
THIS
FUNCTION
SPLICES
TOGETHER
TWO
LISTS
BY
INSERTING
THE
LIST
POINTED
TO
BY
LIST
TO
THE
GIVEN
LIST
AFTER
THE
ELEMENT
HEAD
TO
SPLICE
TWO
UNCONNECTED
LISTS
TOGETHER
AND
REINITIALIZE
THE
OLD
LIST
STRUCT
LIST
STRUCT
HEAD
THIS
FUNCTION
WORKS
THE
SAME
AS
EXCEPT
THAT
THE
EMPTIED
LIST
POINTED
TO
BY
LIST
IS
REINITIALIZED
SAVING
A
COUPLE
DEREFERENCES
IF
YOU
HAPPEN
TO
ALREADY
HAVE
THE
NEXT
AND
PREV
POINTERS
AVAILABLE
YOU
CAN
SAVE
A
COUPLE
CYCLES
SPECIFICALLY
THE
DEREFERENCES
TO
GET
THE
POINTERS
BY
CALLING
THE
INTERNAL
LIST
FUNCTIONS
DIRECTLY
EVERY
PREVIOUSLY
DISCUSSED
FUNCTION
ACTUALLY
DOES
NOTHING
EXCEPT
FIND
THE
NEXT
AND
PREV
POINTERS
AND
THEN
CALL
THE
INTERNAL
FUNCTIONS
THE
INTERNAL
FUNCTIONS
GENERALLY
HAVE
THE
SAME
NAME
AS
THEIR
WRAPPERS
EXCEPT
THEY
ARE
PREFIXED
BY
DOUBLE
UNDERSCORES
FOR
EXAM
PLE
RATHER
THAN
CALL
LIST
YOU
CAN
CALL
PREV
NEXT
THIS
IS
USEFUL
ONLY
IF
THE
NEXT
AND
PREVIOUS
POINTERS
ARE
ALREADY
DEREFERENCED
OTHERWISE
YOU
ARE
JUST
WRITING
UGLY
CODE
SEE
THE
HEADER
LINUX
LIST
H
FOR
THE
EXACT
INTERFACES
TRAVERSING
LINKED
LISTS
NOW
YOU
KNOW
HOW
TO
DECLARE
INITIALIZE
AND
MANIPULATE
A
LINKED
LIST
IN
THE
KERNEL
THIS
IS
ALL
VERY
WELL
AND
GOOD
BUT
IT
IS
MEANINGLESS
IF
YOU
HAVE
NO
WAY
TO
ACCESS
YOUR
DATA
THE
LINKED
LISTS
ARE
JUST
CONTAINERS
THAT
HOLD
YOUR
IMPORTANT
DATA
YOU
NEED
A
WAY
TO
USE
LISTS
TO
MOVE
AROUND
AND
ACCESS
THE
ACTUAL
STRUCTURES
THAT
CONTAIN
THE
DATA
THE
KERNEL
THANK
GOODNESS
PROVIDES
A
NICE
SET
OF
INTERFACES
FOR
TRAVERSING
LINKED
LISTS
AND
REFERENCING
THE
DATA
STRUCTURES
THAT
INCLUDE
THEM
NOTE
THAT
UNLIKE
THE
LIST
MANIPULATION
ROUTINES
ITERATING
OVER
A
LINKED
LIST
IN
ITS
ENTIRETY
IS
CLEARLY
AN
O
N
OPERATION
FOR
N
ENTRIES
IN
THE
LIST
THE
BASIC
APPROACH
THE
MOST
BASIC
WAY
TO
ITERATE
OVER
A
LIST
IS
WITH
THE
MACRO
THE
MACRO
TAKES
TWO
PARAMETERS
BOTH
STRUCTURES
THE
FIRST
IS
A
POINTER
USED
TO
POINT
TO
THE
CURRENT
ENTRY
IT
IS
A
TEMPORARY
VARIABLE
THAT
YOU
MUST
PROVIDE
THE
SECOND
IS
THE
ACTING
AS
THE
HEAD
NODE
OF
THE
LIST
YOU
WANT
TO
TRAVERSE
SEE
THE
EARLIER
SECTION
LIST
HEADS
ON
EACH
ITERATION
OF
THE
LOOP
THE
FIRST
PARAMETER
POINTS
TO
THE
NEXT
ENTRY
IN
THE
LIST
UNTIL
EACH
ENTRY
HAS
BEEN
VISITED
USAGE
IS
AS
FOLLOWS
STRUCT
P
P
P
POINTS
TO
AN
ENTRY
IN
THE
LIST
WELL
THAT
IS
STILL
WORTHLESS
A
POINTER
TO
THE
LIST
STRUCTURE
IS
USUALLY
NO
GOOD
WHAT
WE
NEED
IS
A
POINTER
TO
THE
STRUCTURE
THAT
CONTAINS
THE
FOR
EXAMPLE
WITH
THE
PRE
VIOUS
FOX
STRUCTURE
EXAMPLE
WE
WANT
A
POINTER
TO
EACH
FOX
NOT
A
POINTER
TO
THE
LIST
MEMBER
IN
THE
STRUCTURE
WE
CAN
USE
THE
MACRO
WHICH
WE
DISCUSSED
EAR
LIER
TO
RETRIEVE
THE
STRUCTURE
THAT
CONTAINS
A
GIVEN
FOR
EXAMPLE
STRUCT
P
STRUCT
FOX
F
P
F
POINTS
TO
THE
STRUCTURE
IN
WHICH
THE
LIST
IS
EMBEDDED
F
P
STRUCT
FOX
LIST
THE
USABLE
APPROACH
THE
PREVIOUS
APPROACH
DOES
NOT
MAKE
FOR
PARTICULARLY
INTUITIVE
OR
ELEGANT
CODE
ALTHOUGH
IT
DOES
ILLUSTRATE
HOW
NODES
FUNCTION
CONSEQUENTLY
MOST
KERNEL
CODE
USES
THE
MACRO
TO
ITERATE
OVER
A
LINKED
LIST
THIS
MACRO
HANDLES
THE
WORK
PERFORMED
BY
MAKING
LIST
ITERATION
SIMPLE
POS
HEAD
MEMBER
HERE
POS
IS
A
POINTER
TO
THE
OBJECT
CONTAINING
THE
NODES
THINK
OF
IT
AS
THE
RETURN
VALUE
FROM
HEAD
IS
A
POINTER
TO
THE
HEAD
NODE
FROM
WHICH
YOU
WANT
TO
START
ITERATING
IN
OUR
PREVIOUS
EXAMPLE
MEMBER
IS
THE
VARI
ABLE
NAME
OF
THE
STRUCTURE
IN
POS
LIST
IN
OUR
EXAMPLE
THIS
SOUNDS
CONFUS
ING
BUT
IT
IS
EASY
TO
USE
HERE
IS
HOW
WE
WOULD
REWRITE
THE
PREVIOUS
TO
ITERATE
OVER
EVERY
FOX
NODE
STRUCT
FOX
F
F
LIST
ON
EACH
ITERATION
F
POINTS
TO
THE
NEXT
FOX
STRUCTURE
NOW
LET
LOOK
AT
A
REAL
EXAMPLE
FROM
INOTIFY
THE
KERNEL
FILESYSTEM
NOTIFICATION
SYSTEM
STATIC
STRUCT
STRUCT
INODE
INODE
STRUCT
IH
STRUCT
WATCH
WATCH
INODE
IF
WATCH
IH
IH
RETURN
WATCH
RETURN
NULL
THIS
FUNCTION
ITERATES
OVER
ALL
THE
ENTRIES
IN
THE
INODE
LIST
EACH
ENTRY
IS
OF
TYPE
STRUCT
AND
THE
IN
THAT
STRUCTURE
IS
NAMED
WITH
EACH
ITERATION
OF
THE
LOOP
WATCH
POINTS
AT
A
NEW
NODE
IN
THE
LIST
THE
PURPOSE
OF
THIS
SIMPLE
FUNCTION
IS
TO
SEARCH
THE
LIST
IN
THE
PROVIDED
INODE
STRUC
TURE
TO
FIND
AN
ENTRY
WHOSE
MATCHES
THE
PROVIDED
HANDLE
ITERATING
THROUGH
A
LIST
BACKWARD
THE
MACRO
WORKS
JUST
LIKE
EXCEPT
THAT
IT
MOVES
THROUGH
THE
LIST
IN
REVERSE
THAT
IS
INSTEAD
OF
FOLLOWING
THE
NEXT
POINTERS
FORWARD
THROUGH
THE
LIST
IT
FOLLOWS
THE
PREV
POINTERS
BACKWARD
USAGE
IS
THE
SAME
AS
WITH
POS
HEAD
MEMBER
THERE
ARE
ONLY
A
HANDFUL
OF
REASONS
TO
FAVOR
MOVING
THROUGH
A
LIST
IN
REVERSE
ONE
IS
PERFORMANCE
IF
YOU
KNOW
THE
ITEM
YOU
ARE
SEARCHING
FOR
IS
LIKELY
BEHIND
THE
NODE
YOU
ARE
STARTING
YOUR
SEARCH
FROM
YOU
CAN
MOVE
BACKWARD
IN
HOPES
OF
FINDING
IT
SOONER
A
SECOND
REASON
IS
IF
ORDERING
IS
IMPORTANT
FOR
EXAMPLE
IF
YOU
USE
A
LINKED
LIST
AS
A
STACK
YOU
CAN
WALK
THE
LIST
FROM
THE
TAIL
BACKWARD
TO
ACHIEVE
LAST
IN
FIRST
OUT
LIFO
ORDERING
IF
YOU
DO
NOT
HAVE
AN
EXPLICIT
REASON
TO
MOVE
THROUGH
THE
LIST
IN
REVERSE
DON
T
JUST
USE
ITERATING
WHILE
REMOVING
THE
STANDARD
LIST
ITERATION
METHODS
ARE
NOT
APPROPRIATE
IF
YOU
ARE
REMOVING
ENTRIES
FROM
THE
LIST
AS
YOU
ITERATE
THE
STANDARD
METHODS
RELY
ON
THE
FACT
THAT
THE
LIST
ENTRIES
ARE
NOT
CHANGING
OUT
FROM
UNDER
THEM
AND
THUS
IF
THE
CURRENT
ENTRY
IS
REMOVED
IN
THE
BODY
OF
THE
LOOP
THE
SUBSEQUENT
ITERATION
CANNOT
ADVANCE
TO
THE
NEXT
OR
PREVIOUS
POINTER
THIS
IS
A
COMMON
PATTERN
IN
LOOPS
AND
PROGRAMMERS
SOLVE
IT
BY
STORING
THE
NEXT
OR
PREVIOUS
POINTER
IN
A
TEMPORARY
VARIABLE
PRIOR
TO
A
POTENTIAL
REMOVAL
OPERATION
THE
LINUX
KERNEL
PROVIDES
A
ROUTINE
TO
HANDLE
THIS
SITUATION
FOR
YOU
POS
NEXT
HEAD
MEMBER
YOU
USE
THIS
VERSION
IN
THE
SAME
MANNER
AS
EXCEPT
THAT
YOU
PROVIDE
THE
NEXT
POINTER
WHICH
IS
OF
THE
SAME
TYPE
AS
POS
THE
NEXT
POINTER
IS
USED
BY
THE
MACRO
TO
STORE
THE
NEXT
ENTRY
IN
THE
LIST
MAKING
IT
SAFE
TO
REMOVE
THE
CURRENT
ENTRY
LET
CONSIDER
AN
EXAMPLE
AGAIN
IN
INOTIFY
VOID
STRUCT
INODE
INODE
STRUCT
WATCH
NEXT
INODE
WATCH
NEXT
INODE
STRUCT
IH
WATCH
IH
IH
MUTEX
IH
WATCH
DELETES
WATCH
IH
MUTEX
INODE
THIS
FUNCTION
ITERATES
OVER
AND
REMOVES
ALL
THE
ENTRIES
IN
THE
LIST
IF
THE
STANDARD
WERE
USED
THIS
CODE
WOULD
INTRODUCE
A
USE
AFTER
FREE
BUG
AS
MOVING
TO
THE
NEXT
ITEM
IN
THE
LIST
WOULD
REQUIRE
ACCESSING
WATCH
WHICH
WAS
DESTROYED
IF
YOU
NEED
TO
ITERATE
OVER
A
LINKED
LIST
IN
REVERSE
AND
POTENTIALLY
REMOVE
ELEMENTS
THE
KERNEL
PROVIDES
POS
N
HEAD
MEMBER
USAGE
IS
THE
SAME
AS
WITH
YOU
MAY
STILL
NEED
LOCKING
THE
SAFE
VARIANTS
OF
PROTECT
YOU
ONLY
FROM
REMOVALS
FROM
THE
LIST
WITHIN
THE
BODY
OF
THE
LOOP
IF
THERE
IS
A
CHANCE
OF
CONCURRENT
REMOVALS
FROM
OTHER
CODE
OR
ANY
OTHER
FORM
OF
CONCURRENT
LIST
MANIPULATION
YOU
NEED
TO
PROPERLY
LOCK
ACCESS
TO
THE
LIST
SEE
CHAPTERS
AN
INTRODUCTION
TO
KERNEL
SYNCHRONIZATION
AND
CHAPTER
KERNEL
SYN
CHRONIZATION
METHODS
FOR
A
DISCUSSION
ON
SYNCHRONIZATION
AND
LOCKING
OTHER
LINKED
LIST
METHODS
LINUX
PROVIDES
MYRIAD
OTHER
LIST
METHODS
ENABLING
SEEMINGLY
EVERY
CONCEIVABLE
WAY
TO
ACCESS
AND
MANIPULATE
A
LINKED
LIST
ALL
THESE
METHODS
ARE
DEFINED
IN
THE
HEADER
FILE
LINUX
LIST
H
QUEUES
A
COMMON
PROGRAMMING
PATTERN
IN
ANY
OPERATING
SYSTEM
KERNEL
IS
PRODUCER
AND
CONSUMER
IN
THIS
PATTERN
A
PRODUCER
CREATES
DATA
SAY
ERROR
MESSAGES
TO
BE
READ
OR
NETWORKING
PACKETS
TO
BE
PROCESSED
WHILE
A
CONSUMER
IN
TURN
READS
PROCESSES
OR
OTHERWISE
CONSUMES
THE
DATA
OFTEN
THE
EASIEST
WAY
TO
IMPLEMENT
THIS
PATTERN
IS
WITH
A
QUEUE
THE
PRODUCER
PUSHES
DATA
ONTO
THE
QUEUE
AND
THE
CONSUMER
PULLS
DATA
OFF
THE
QUEUE
THE
CONSUMER
RETRIEVES
THE
DATA
IN
THE
ORDER
IT
WAS
ENQUEUED
THAT
IS
THE
FIRST
DATA
ON
THE
QUEUE
IS
THE
FIRST
DATA
OFF
THE
QUEUE
FOR
THIS
REASON
QUEUES
ARE
ALSO
CALLED
FIFOS
SHORT
FOR
FIRST
IN
FIRST
OUT
SEE
FIGURE
FOR
AN
EXAMPLE
OF
A
STANDARD
QUEUE
FIGURE
A
QUEUE
FIFO
THE
LINUX
KERNEL
GENERIC
QUEUE
IMPLEMENTATION
IS
CALLED
KFIFO
AND
IS
IMPLEMENTED
IN
KERNEL
KFIFO
C
AND
DECLARED
IN
LINUX
KFIFO
H
THIS
SECTION
DISCUSSES
THE
API
AFTER
AN
UPDATE
IN
USAGE
IS
SLIGHTLY
DIFFERENT
IN
KERNEL
VERSIONS
PRIOR
TO
DOUBLE
CHECK
LINUX
KFIFO
H
BEFORE
WRITING
CODE
KFIFO
LINUX
KFIFO
WORKS
LIKE
MOST
OTHER
QUEUE
ABSTRACTIONS
PROVIDING
TWO
PRIMARY
OPERATIONS
ENQUEUE
UNFORTUNATELY
NAMED
IN
AND
DEQUEUE
OUT
THE
KFIFO
OBJECT
MAINTAINS
TWO
OFF
SETS
INTO
THE
QUEUE
AN
IN
OFFSET
AND
AN
OUT
OFFSET
THE
IN
OFFSET
IS
THE
LOCATION
IN
THE
QUEUE
TO
WHICH
THE
NEXT
ENQUEUE
WILL
OCCUR
THE
OUT
OFFSET
IS
THE
LOCATION
IN
THE
QUEUE
FROM
WHICH
THE
NEXT
DEQUEUE
WILL
OCCUR
THE
OUT
OFFSET
IS
ALWAYS
LESS
THAN
OR
EQUAL
TO
THE
IN
OFFSET
IT
WOULDN
T
MAKE
SENSE
FOR
IT
TO
BE
GREATER
OTHERWISE
YOU
COULD
DEQUEUE
DATA
THAT
HAD
NOT
YET
BEEN
ENQUEUED
THE
ENQUEUE
IN
OPERATION
COPIES
DATA
INTO
THE
QUEUE
STARTING
AT
THE
IN
OFFSET
WHEN
COMPLETE
THE
IN
OFFSET
IS
INCREMENTED
BY
THE
AMOUNT
OF
DATA
ENQUEUED
THE
DEQUEUE
OUT
OPERATION
COPIES
DATA
OUT
OF
THE
QUEUE
STARTING
FROM
THE
OUT
OFFSET
WHEN
COMPLETE
THE
OUT
OFFSET
IS
INCREMENTED
BY
THE
AMOUNT
OF
DATA
ENQUEUED
WHEN
THE
OUT
OFFSET
IS
EQUAL
TO
THE
IN
OFFSET
THE
QUEUE
IS
EMPTY
NO
MORE
DATA
CAN
BE
DEQUEUED
UNTIL
MORE
DATA
IS
ENQUEUED
WHEN
THE
IN
OFFSET
IS
EQUAL
TO
THE
LENGTH
OF
THE
QUEUE
NO
MORE
DATA
CAN
BE
ENQUEUED
UNTIL
THE
QUEUE
IS
RESET
CREATING
A
QUEUE
TO
USE
A
KFIFO
YOU
MUST
FIRST
DEFINE
AND
INITIALIZE
IT
AS
WITH
MOST
KERNEL
OBJECTS
YOU
CAN
DO
THIS
DYNAMICALLY
OR
STATICALLY
THE
MOST
COMMON
METHOD
IS
DYNAMIC
INT
STRUCT
KFIFO
FIFO
UNSIGNED
INT
SIZE
THIS
FUNCTION
CREATES
AND
INITIALIZES
A
KFIFO
WITH
A
QUEUE
OF
SIZE
BYTES
THE
KERNEL
USES
THE
GFP
MASK
TO
ALLOCATE
THE
QUEUE
WE
DISCUSS
MEMORY
ALLOCATIONS
IN
CHAPTER
MEMORY
MANAGEMENT
UPON
SUCCESS
RETURNS
ZERO
ON
ERROR
IT
RETURNS
A
NEGATIVE
ERROR
CODE
FOLLOWING
IS
A
SIMPLE
EXAMPLE
STRUCT
KFIFO
FIFO
INT
RET
RET
KIFO
IF
RET
RETURN
RET
FIFO
NOW
REPRESENTS
A
SIZED
QUEUE
IF
YOU
WANT
TO
ALLOCATE
THE
BUFFER
YOURSELF
YOU
CAN
VOID
STRUCT
KFIFO
FIFO
VOID
BUFFER
UNSIGNED
INT
SIZE
THIS
FUNCTION
CREATES
AND
INITIALIZES
A
KFIFO
THAT
WILL
USE
THE
SIZE
BYTES
OF
MEMORY
POINTED
AT
BY
BUFFER
FOR
ITS
QUEUE
WITH
BOTH
AND
SIZE
MUST
BE
A
POWER
OF
TWO
STATICALLY
DECLARING
A
KFIFO
IS
SIMPLER
BUT
LESS
COMMON
NAME
SIZE
NAME
THIS
CREATES
A
STATIC
KFIFO
NAMED
NAME
WITH
A
QUEUE
OF
SIZE
BYTES
AS
BEFORE
SIZE
MUST
BE
A
POWER
OF
ENQUEUING
DATA
WHEN
YOUR
KFIFO
IS
CREATED
AND
INITIALIZED
ENQUEUING
DATA
INTO
THE
QUEUE
IS
PERFORMED
VIA
THE
FUNCTION
UNSIGNED
INT
STRUCT
KFIFO
FIFO
CONST
VOID
FROM
UNSIGNED
INT
LEN
THIS
FUNCTION
COPIES
THE
LEN
BYTES
STARTING
AT
FROM
INTO
THE
QUEUE
REPRESENTED
BY
FIFO
ON
SUCCESS
IT
RETURNS
THE
NUMBER
OF
BYTES
ENQUEUED
IF
LESS
THAN
LEN
BYTES
ARE
FREE
IN
THE
QUEUE
THE
FUNCTION
COPIES
ONLY
UP
TO
THE
AMOUNT
OF
AVAILABLE
BYTES
THUS
THE
RETURN
VALUE
CAN
BE
LESS
THAN
LEN
OR
EVEN
ZERO
IF
NOTHING
WAS
COPIED
DEQUEUING
DATA
WHEN
YOU
ADD
DATA
TO
A
QUEUE
WITH
YOU
CAN
REMOVE
IT
WITH
UNSIGNED
INT
STRUCT
KFIFO
FIFO
VOID
TO
UNSIGNED
INT
LEN
THIS
FUNCTION
COPIES
AT
MOST
LEN
BYTES
FROM
THE
QUEUE
POINTED
AT
BY
FIFO
TO
THE
BUFFER
POINTED
AT
BY
TO
ON
SUCCESS
THE
FUNCTION
RETURNS
THE
NUMBER
OF
BYTES
COPIED
IF
LESS
THAN
LEN
BYTES
ARE
IN
THE
QUEUE
THE
FUNCTION
COPIES
LESS
THAN
REQUESTED
WHEN
DEQUEUED
DATA
IS
NO
LONGER
ACCESSIBLE
FROM
THE
QUEUE
THIS
IS
THE
NORMAL
USAGE
OF
A
QUEUE
BUT
IF
YOU
WANT
TO
PEEK
AT
DATA
WITHIN
THE
QUEUE
WITHOUT
REMOVING
IT
YOU
CAN
USE
UNSIGNED
INT
STRUCT
KFIFO
FIFO
VOID
TO
UNSIGNED
INT
LEN
UNSIGNED
OFFSET
THIS
WORKS
THE
SAME
AS
EXCEPT
THAT
THE
OUT
OFFSET
IS
NOT
INCREMENTED
AND
THUS
THE
DEQUEUED
DATA
IS
AVAILABLE
TO
READ
ON
A
SUBSEQUENT
CALL
TO
THE
PARAMETER
OFFSET
SPECIFIES
AN
INDEX
INTO
THE
QUEUE
SPECIFY
ZERO
TO
READ
FROM
THE
HEAD
OF
THE
QUEUE
AS
DOES
OBTAINING
THE
SIZE
OF
A
QUEUE
TO
OBTAIN
THE
TOTAL
SIZE
IN
BYTES
OF
THE
BUFFER
USED
TO
STORE
A
KFIFO
QUEUE
CALL
STATIC
INLINE
UNSIGNED
INT
STRUCT
KFIFO
FIFO
IN
ANOTHER
EXAMPLE
OF
HORRIBLE
KERNEL
NAMING
USE
TO
OBTAIN
THE
NUMBER
OF
BYTES
ENQUEUED
IN
A
KFIFO
STATIC
INLINE
UNSIGNED
INT
STRUCT
KFIFO
FIFO
TO
FIND
OUT
THE
NUMBER
OF
BYTES
AVAILABLE
TO
WRITE
INTO
A
KFIFO
CALL
STATIC
INLINE
UNSIGNED
INT
STRUCT
KFIFO
FIFO
FINALLY
AND
RETURN
NONZERO
IF
THE
GIVEN
KFIFO
IS
EMPTY
OR
FULL
RESPECTIVELY
AND
ZERO
IF
NOT
STATIC
INLINE
INT
STRUCT
KFIFO
FIFO
STATIC
INLINE
INT
STRUCT
KFIFO
FIFO
RESETTING
AND
DESTROYING
THE
QUEUE
TO
RESET
A
KFIFO
JETTISONING
ALL
THE
CONTENTS
OF
THE
QUEUE
CALL
STATIC
INLINE
VOID
STRUCT
KFIFO
FIFO
TO
DESTROY
A
KFIFO
ALLOCATED
WITH
CALL
VOID
STRUCT
KFIFO
FIFO
IF
YOU
CREATED
YOUR
KFIFO
WITH
IT
IS
YOUR
RESPONSIBILITY
TO
FREE
THE
ASSO
CIATED
BUFFER
HOW
YOU
DO
SO
DEPENDS
ON
HOW
YOU
CREATED
IT
SEE
CHAPTER
FOR
A
DIS
CUSSION
ON
ALLOCATING
AND
FREEING
DYNAMIC
MEMORY
EXAMPLE
QUEUE
USAGE
WITH
THESE
INTERFACES
UNDER
OUR
BELT
LET
TAKE
A
LOOK
AT
A
SIMPLE
EXAMPLE
OF
USING
A
KFIFO
ASSUME
WE
CREATED
A
KFIFO
POINTED
AT
BY
FIFO
WITH
A
QUEUE
SIZE
OF
WE
CAN
NOW
ENQUEUE
DATA
ONTO
THE
QUEUE
IN
THIS
EXAMPLE
WE
ENQUEUE
SIMPLE
INTEGERS
IN
YOUR
OWN
CODE
YOU
WILL
LIKELY
ENQUEUE
MORE
COMPLICATED
TASK
SPECIFIC
STRUCTURES
USING
INTEGERS
IN
THIS
EXAMPLE
LET
SEE
EXACTLY
HOW
THE
KFIFO
WORKS
UNSIGNED
INT
I
ENQUEUE
TO
THE
KFIFO
NAMED
FIFO
FOR
I
I
I
FIFO
I
SIZEOF
I
THE
KFIFO
NAMED
FIFO
NOW
CONTAINS
THROUGH
INCLUSIVE
WE
CAN
TAKE
A
PEEK
AT
THE
FIRST
ITEM
IN
THE
QUEUE
AND
VERIFY
IT
IS
UNSIGNED
INT
VAL
INT
RET
RET
FIFO
VAL
SIZEOF
VAL
IF
RET
SIZEOF
VAL
RETURN
EINVAL
PRINTK
U
N
VAL
SHOULD
PRINT
TO
DEQUEUE
AND
PRINT
ALL
THE
ITEMS
IN
THE
KFIFO
WE
CAN
USE
WHILE
THERE
IS
DATA
IN
THE
QUEUE
WHILE
FIFO
UNSIGNED
INT
VAL
INT
RET
READ
IT
ONE
INTEGER
AT
A
TIME
RET
FIFO
VAL
SIZEOF
VAL
IF
RET
SIZEOF
VAL
RETURN
EINVAL
PRINTK
U
N
VAL
THIS
PRINTS
THROUGH
INCLUSIVE
AND
IN
THAT
ORDER
IF
THIS
CODE
SNIPPET
PRINTED
THE
NUMBERS
BACKWARD
FROM
TO
WE
WOULD
HAVE
A
STACK
NOT
A
QUEUE
MAPS
A
MAP
ALSO
KNOWN
AS
AN
ASSOCIATIVE
ARRAY
IS
A
COLLECTION
OF
UNIQUE
KEYS
WHERE
EACH
KEY
IS
ASSOCIATED
WITH
A
SPECIFIC
VALUE
THE
RELATIONSHIP
BETWEEN
A
KEY
AND
ITS
VALUE
IS
CALLED
A
MAPPING
MAPS
SUPPORT
AT
LEAST
THREE
OPERATIONS
N
ADD
KEY
VALUE
N
REMOVE
KEY
N
VALUE
LOOKUP
KEY
ALTHOUGH
A
HASH
TABLE
IS
A
TYPE
OF
MAP
NOT
ALL
MAPS
ARE
IMPLEMENTED
VIA
HASHES
INSTEAD
OF
A
HASH
TABLE
MAPS
CAN
ALSO
USE
A
SELF
BALANCING
BINARY
SEARCH
TREE
TO
STORE
THEIR
DATA
ALTHOUGH
A
HASH
OFFERS
BETTER
AVERAGE
CASE
ASYMPTOTIC
COMPLEXITY
SEE
THE
SECTION
ALGORITHMIC
COMPLEXITY
LATER
IN
THIS
CHAPTER
A
BINARY
SEARCH
TREE
HAS
BETTER
WORST
CASE
BEHAVIOR
LOGARITHMIC
VERSUS
LINEAR
A
BINARY
SEARCH
TREE
ALSO
ENABLES
ORDER
PRESERVATION
ENABLING
USERS
TO
EFFICIENTLY
ITERATE
OVER
THE
ENTIRE
COLLECTION
IN
A
SORTED
ORDER
FINALLY
A
BINARY
SEARCH
TREE
DOES
NOT
REQUIRE
A
HASH
FUNCTION
INSTEAD
ANY
KEY
TYPE
IS
SUITABLE
SO
LONG
AS
IT
CAN
DEFINE
THE
OPERATOR
ALTHOUGH
THE
GENERAL
TERM
FOR
ALL
COLLECTIONS
MAPPING
A
KEY
TO
A
VALUE
THE
NAME
MAPS
OFTEN
REFERS
SPECIFICALLY
TO
AN
ASSOCIATED
ARRAY
IMPLEMENTED
USING
A
BINARY
SEARCH
TREE
AS
OPPOSED
TO
A
HASH
TABLE
FOR
EXAMPLE
THE
C
STL
CONTAINER
STD
MAP
IS
IMPLEMENTED
USING
A
SELF
BALANCING
BINARY
SEARCH
TREE
OR
SIMILAR
DATA
STRUCTURE
BECAUSE
IT
PROVIDES
THE
ABILITY
TO
IN
ORDER
TRAVERSE
THE
COLLECTION
THE
LINUX
KERNEL
PROVIDES
A
SIMPLE
AND
EFFICIENT
MAP
DATA
STRUCTURE
BUT
IT
IS
NOT
A
GENERAL
PURPOSE
MAP
INSTEAD
IT
IS
DESIGNED
FOR
ONE
SPECIFIC
USE
CASE
MAPPING
A
UNIQUE
IDENTIFICATION
NUMBER
UID
TO
A
POINTER
IN
ADDITION
TO
PROVIDING
THE
THREE
MAIN
MAP
OPERATIONS
LINUX
IMPLEMENTATION
ALSO
PIGGYBACKS
AN
ALLOCATE
OPERATION
ON
TOP
OF
THE
ADD
OPERATION
THIS
ALLOCATE
OPERATION
NOT
ONLY
ADDS
A
UID
VALUE
PAIR
TO
THE
MAP
BUT
ALSO
GENERATES
THE
UID
THE
IDR
DATA
STRUCTURE
IS
USED
FOR
MAPPING
USER
SPACE
UIDS
SUCH
AS
INOTIFY
WATCH
DESCRIPTORS
OR
POSIX
TIMER
IDS
TO
THEIR
ASSOCIATED
KERNEL
DATA
STRUCTURE
SUCH
AS
THE
OR
STRUCTURES
RESPECTIVELY
FOLLOWING
THE
LINUX
KERNEL
SCHEME
OF
OBFUSCATED
CONFUSING
NAMES
THIS
MAP
IS
CALLED
IDR
INITIALIZING
AN
IDR
SETTING
UP
AN
IDR
IS
EASY
FIRST
YOU
STATICALLY
DEFINE
OR
DYNAMICALLY
ALLOCATE
AN
IDR
STRUC
TURE
THEN
YOU
CALL
VOID
STRUCT
IDR
IDP
FOR
EXAMPLE
STRUCT
IDR
STATICALLY
DEFINE
IDR
STRUCTURE
INITIALIZE
PROVIDED
IDR
STRUCTURE
ALLOCATING
A
NEW
UID
ONCE
YOU
HAVE
AN
IDR
SET
UP
YOU
CAN
ALLOCATE
A
NEW
UID
WHICH
IS
A
TWO
STEP
PROCESS
FIRST
YOU
TELL
THE
IDR
THAT
YOU
WANT
TO
ALLOCATE
A
NEW
UID
ALLOWING
IT
TO
RESIZE
THE
BACK
ING
TREE
AS
NECESSARY
THEN
WITH
A
SECOND
CALL
YOU
ACTUALLY
REQUEST
THE
NEW
UID
THIS
COMPLICATION
EXISTS
TO
ALLOW
YOU
TO
PERFORM
THE
INITIAL
RESIZING
WHICH
MAY
REQUIRE
A
MEMORY
ALLOCATION
WITHOUT
A
LOCK
WE
DISCUSS
MEMORY
ALLOCATIONS
IN
CHAPTER
AND
LOCKING
IN
CHAPTERS
AND
FOR
NOW
LET
CONCENTRATE
ON
USING
IDR
WITHOUT
CONCERN
TO
HOW
WE
HANDLE
LOCKING
THE
FIRST
FUNCTION
TO
RESIZE
THE
BACKING
TREE
IS
INT
STRUCT
IDR
IDP
THIS
FUNCTION
WILL
IF
NEEDED
TO
FULFILL
A
NEW
UID
ALLOCATION
RESIZE
THE
IDR
POINTED
AT
BY
IDP
IF
A
RESIZE
IS
NEEDED
THE
MEMORY
ALLOCATION
WILL
USE
THE
GFP
FLAGS
GFP
FLAGS
ARE
DISCUSSED
IN
CHAPTER
YOU
DO
NOT
NEED
TO
SYNCHRONIZE
CONCURRENT
ACCESS
TO
THIS
CALL
INVERTED
FROM
NEARLY
EVERY
OTHER
FUNCTION
IN
THE
KERNEL
RETURNS
ONE
ON
SUCCESS
AND
ZERO
ON
ERROR
BE
CAREFUL
THE
SECOND
FUNCTION
TO
ACTUALLY
OBTAIN
A
NEW
UID
AND
ADD
IT
TO
THE
IDR
IS
INT
STRUCT
IDR
IDP
VOID
PTR
INT
ID
THIS
FUNCTION
USES
THE
IDR
POINTED
AT
BY
IDP
TO
ALLOCATE
A
NEW
UID
AND
ASSOCIATE
IT
WITH
THE
POINTER
PTR
ON
SUCCESS
THE
FUNCTION
RETURNS
ZERO
AND
STORES
THE
NEW
UID
IN
ID
ON
ERROR
IT
RETURNS
A
NONZERO
ERROR
CODE
EAGAIN
IF
YOU
NEED
TO
AGAIN
CALL
AND
ENOSPC
IF
THE
IDR
IS
FULL
LET
LOOK
AT
A
FULL
EXAMPLE
INT
ID
DO
IF
RETURN
ENOSPC
RET
PTR
ID
WHILE
RET
EAGAIN
IF
SUCCESSFUL
THIS
SNIPPET
OBTAINS
A
NEW
UID
WHICH
IS
STORED
IN
THE
INTEGER
ID
AND
MAPS
THAT
UID
TO
PTR
WHICH
WE
DON
T
DEFINE
IN
THE
SNIPPET
THE
FUNCTION
ENABLES
THE
CALLER
TO
SPECIFY
A
MINIMUM
UID
VALUE
TO
RETURN
INT
STRUCT
IDR
IDP
VOID
PTR
INT
INT
ID
THIS
WORKS
THE
SAME
AS
EXCEPT
THAT
THE
NEW
UID
IS
GUARANTEED
TO
BE
EQUAL
TO
OR
GREATER
THAN
USING
THIS
VARIANT
OF
THE
FUNCTION
ALLOWS
IDR
USERS
TO
ENSURE
THAT
A
UID
IS
NEVER
REUSED
ALLOWING
THE
VALUE
TO
BE
UNIQUE
NOT
ONLY
AMONG
CURRENTLY
ALLOCATED
IDS
BUT
ACROSS
THE
ENTIRETY
OF
A
SYSTEM
UPTIME
THIS
CODE
SNIPPET
IS
THE
SAME
AS
OUR
PREVIOUS
EXAMPLE
EXCEPT
THAT
WE
REQUEST
STRICTLY
INCREASING
UID
VALUES
INT
ID
DO
IF
RETURN
ENOSPC
RET
PTR
ID
WHILE
RET
EAGAIN
IF
RET
ID
LOOKING
UP
A
UID
WHEN
WE
HAVE
ALLOCATED
SOME
NUMBER
OF
UIDS
IN
AN
IDR
WE
CAN
LOOK
THEM
UP
THE
CALLER
PROVIDES
THE
UID
AND
THE
IDR
RETURNS
THE
ASSOCIATED
POINTER
THIS
IS
ACCOMPLISHED
IN
A
MUCH
SIMPLER
MANNER
THAN
ALLOCATING
A
NEW
UID
WITH
THE
FUNCTION
VOID
STRUCT
IDR
IDP
INT
ID
A
SUCCESSFUL
CALL
TO
THIS
FUNCTION
RETURNS
THE
POINTER
ASSOCIATED
WITH
THE
UID
ID
IN
THE
IDR
POINTED
AT
BY
IDP
ON
ERROR
THE
FUNCTION
RETURNS
NULL
NOTE
IF
YOU
MAPPED
NULL
TO
A
UID
WITH
OR
THIS
FUNCTION
SUCCESSFULLY
RETURNS
NULL
GIVING
YOU
NO
WAY
TO
DISTINGUISH
SUCCESS
FROM
FAILURE
CONSEQUENTLY
YOU
SHOULD
NOT
MAP
UIDS
TO
NULL
USAGE
IS
SIMPLE
STRUCT
PTR
ID
IF
PTR
RETURN
EINVAL
ERROR
REMOVING
A
UID
TO
REMOVE
A
UID
FROM
AN
IDR
USE
VOID
STRUCT
IDR
IDP
INT
ID
A
SUCCESSFUL
CALL
TO
REMOVES
THE
UID
ID
FROM
THE
IDR
POINTED
AT
BY
IDP
UNFORTUNATELY
HAS
NO
WAY
TO
SIGNIFY
ERROR
FOR
EXAMPLE
IF
ID
IS
NOT
IN
IDP
DESTROYING
AN
IDR
DESTROYING
AN
IDR
IS
A
SIMPLE
AFFAIR
ACCOMPLISHED
WITH
THE
FUNCTION
VOID
STRUCT
IDR
IDP
A
SUCCESSFUL
CALL
TO
DEALLOCATES
ONLY
UNUSED
MEMORY
ASSOCIATED
WITH
THE
IDR
POINTED
AT
BY
IDP
IT
DOES
NOT
FREE
ANY
MEMORY
CURRENTLY
IN
USE
BY
ALLOCATED
UIDS
GENERALLY
KERNEL
CODE
WOULDN
T
DESTROY
ITS
IDR
FACILITY
UNTIL
IT
WAS
SHUTTING
DOWN
OR
UNLOADING
AND
IT
WOULDN
T
UNLOAD
UNTIL
IT
HAD
NO
MORE
USERS
AND
THUS
NO
MORE
UIDS
BUT
TO
FORCE
THE
REMOVAL
OF
ALL
UIDS
YOU
CAN
CALL
VOID
STRUCT
IDR
IDP
YOU
WOULD
CALL
ON
THE
IDR
POINTED
AT
BY
IDP
BEFORE
CALLING
ENSURING
THAT
ALL
IDR
MEMORY
WAS
FREED
BINARY
TREES
A
TREE
IS
A
DATA
STRUCTURE
THAT
PROVIDES
A
HIERARCHICAL
TREE
LIKE
STRUCTURE
OF
DATA
MATHEMATI
CALLY
IT
IS
AN
ACYCLIC
CONNECTED
DIRECTED
GRAPH
IN
WHICH
EACH
VERTEX
CALLED
A
NODE
HAS
ZERO
OR
MORE
OUTGOING
EDGES
AND
ZERO
OR
ONE
INCOMING
EDGES
A
BINARY
TREE
IS
A
TREE
IN
WHICH
NODES
HAVE
AT
MOST
TWO
OUTGOING
EDGES
THAT
IS
A
TREE
IN
WHICH
NODES
HAVE
ZERO
ONE
OR
TWO
CHILDREN
SEE
FIGURE
FOR
A
SAMPLE
BINARY
TREE
FIGURE
A
BINARY
TREE
BINARY
SEARCH
TREES
A
BINARY
SEARCH
TREE
OFTEN
ABBREVIATED
BST
IS
A
BINARY
TREE
WITH
A
SPECIFIC
ORDERING
IMPOSED
ON
ITS
NODES
THE
ORDERING
IS
OFTEN
DEFINED
VIA
THE
FOLLOWING
INDUCTION
N
THE
LEFT
SUBTREE
OF
THE
ROOT
CONTAINS
ONLY
NODES
WITH
VALUES
LESS
THAN
THE
ROOT
N
THE
RIGHT
SUBTREE
OF
THE
ROOT
CONTAINS
ONLY
NODES
WITH
VALUES
GREATER
THAN
THE
ROOT
N
ALL
SUBTREES
ARE
ALSO
BINARY
SEARCH
TREES
A
BINARY
SEARCH
TREE
IS
THUS
A
BINARY
TREE
IN
WHICH
ALL
NODES
ARE
ORDERED
SUCH
THAT
LEFT
CHILDREN
ARE
LESS
THAN
THEIR
PARENT
IN
VALUE
AND
RIGHT
CHILDREN
ARE
GREATER
THAN
THEIR
PARENT
CONSEQUENTLY
BOTH
SEARCHING
FOR
A
GIVEN
NODE
AND
IN
ORDER
TRAVERSAL
ARE
EFFICIENT
LOGA
RITHMIC
AND
LINEAR
RESPECTIVELY
SEE
FIGURE
FOR
A
SAMPLE
BINARY
SEARCH
TREE
FIGURE
A
BINARY
SEARCH
TREE
BST
SELF
BALANCING
BINARY
SEARCH
TREES
THE
DEPTH
OF
A
NODE
IS
MEASURED
BY
HOW
MANY
PARENT
NODES
IT
IS
FROM
THE
ROOT
NODES
AT
THE
BOTTOM
OF
THE
TREE
THOSE
WITH
NO
CHILDREN
ARE
CALLED
LEAVES
THE
HEIGHT
OF
A
TREE
IS
THE
DEPTH
OF
THE
DEEPEST
NODE
IN
THE
TREE
A
BALANCED
BINARY
SEARCH
TREE
IS
A
BINARY
SEARCH
TREE
IN
WHICH
THE
DEPTH
OF
ALL
LEAVES
DIFFERS
BY
AT
MOST
ONE
SEE
FIGURE
A
SELF
BALANCING
BINARY
SEARCH
TREE
IS
A
BINARY
SEARCH
TREE
THAT
ATTEMPTS
AS
PART
OF
ITS
NORMAL
OPERATIONS
TO
REMAIN
SEMI
BALANCED
FIGURE
A
BALANCED
BINARY
SEARCH
TREE
RED
BLACK
TREES
A
RED
BLACK
TREE
IS
A
TYPE
OF
SELF
BALANCING
BINARY
SEARCH
TREE
LINUX
PRIMARY
BINARY
TREE
DATA
STRUCTURE
IS
THE
RED
BLACK
TREE
RED
BLACK
TREES
HAVE
A
SPECIAL
COLOR
ATTRIBUTE
WHICH
IS
EITHER
RED
OR
BLACK
RED
BLACK
TREES
REMAIN
SEMI
BALANCED
BY
ENFORCING
THAT
THE
FOLLOWING
SIX
PROPERTIES
REMAIN
TRUE
ALL
NODES
ARE
EITHER
RED
OR
BLACK
LEAF
NODES
ARE
BLACK
LEAF
NODES
DO
NOT
CONTAIN
DATA
ALL
NON
LEAF
NODES
HAVE
TWO
CHILDREN
IF
A
NODE
IS
RED
BOTH
OF
ITS
CHILDREN
ARE
BLACK
THE
PATH
FROM
A
NODE
TO
ONE
OF
ITS
LEAVES
CONTAINS
THE
SAME
NUMBER
OF
BLACK
NODES
AS
THE
SHORTEST
PATH
TO
ANY
OF
ITS
OTHER
LEAVES
TAKEN
TOGETHER
THESE
PROPERTIES
ENSURE
THAT
THE
DEEPEST
LEAF
HAS
A
DEPTH
OF
NO
MORE
THAN
DOUBLE
THAT
OF
THE
SHALLOWEST
LEAF
CONSEQUENTLY
THE
TREE
IS
ALWAYS
SEMI
BALANCED
WHY
THIS
IS
TRUE
IS
SURPRISINGLY
SIMPLE
FIRST
BY
PROPERTY
FIVE
A
RED
NODE
CANNOT
BE
THE
CHILD
OR
PARENT
OF
ANOTHER
RED
NODE
BY
PROPERTY
SIX
ALL
PATHS
THROUGH
THE
TREE
TO
ITS
LEAVES
HAVE
THE
SAME
NUMBER
OF
BLACK
NODES
THE
LONGEST
PATH
THROUGH
THE
TREE
ALTERNATES
RED
AND
BLACK
NODES
THUS
THE
SHORTEST
PATH
WHICH
MUST
HAVE
THE
SAME
NUMBER
OF
BLACK
NODES
CONTAINS
ONLY
BLACK
NODES
THEREFORE
THE
LONGEST
PATH
FROM
THE
ROOT
TO
A
LEAF
IS
NO
MORE
THAN
DOUBLE
THE
SHORTEST
PATH
FROM
THE
ROOT
TO
ANY
OTHER
LEAF
IF
THE
INSERTION
AND
REMOVAL
OPERATIONS
ENFORCE
THESE
SIX
PROPERTIES
THE
TREE
REMAINS
SEMI
BALANCED
NOW
IT
MIGHT
SEEM
ODD
TO
REQUIRE
INSERT
AND
REMOVE
TO
MAINTAIN
THESE
PARTICULAR
PROPERTIES
WHY
NOT
IMPLEMENT
THE
OPERATIONS
SUCH
THAT
THEY
ENFORCE
OTHER
SIMPLER
RULES
THAT
RESULT
IN
A
BALANCED
TREE
IT
TURNS
OUT
THAT
THESE
PROPERTIES
ARE
RELATIVELY
EASY
TO
ENFORCE
ALTHOUGH
COMPLEX
TO
IMPLEMENT
ALLOWING
INSERT
AND
REMOVE
TO
GUARAN
TEE
A
SEMI
BALANCED
TREE
WITHOUT
BURDENSOME
EXTRA
OVERHEAD
DESCRIBING
HOW
INSERT
AND
REMOVE
ENFORCE
THESE
RULES
IS
BEYOND
THE
SCOPE
OF
THIS
BOOK
ALTHOUGH
SIMPLE
RULES
THE
IMPLEMENTATION
IS
COMPLEX
ANY
GOOD
UNDERGRADUATE
LEVEL
DATA
STRUCTURES
TEXTBOOK
OUGHT
TO
GIVE
A
FULL
TREATMENT
RBTREES
THE
LINUX
IMPLEMENTATION
OF
RED
BLACK
TREES
IS
CALLED
RBTREES
THEY
ARE
DEFINED
IN
LIB
RBTREE
C
AND
DECLARED
IN
LINUX
RBTREE
H
ASIDE
FROM
OPTIMIZATIONS
LINUX
RBTREES
RESEMBLE
THE
CLASSIC
RED
BLACK
TREE
AS
DESCRIBED
IN
THE
PREVIOUS
SECTION
THEY
REMAIN
BALANCED
SUCH
THAT
INSERTS
ARE
ALWAYS
LOGARITHMIC
WITH
RESPECT
TO
THE
NUMBER
OF
NODES
IN
THE
TREE
THE
ROOT
OF
AN
RBTREE
IS
REPRESENTED
BY
THE
STRUCTURE
TO
CREATE
A
NEW
TREE
WE
ALLOCATE
A
NEW
AND
INITIALIZE
IT
TO
THE
SPECIAL
VALUE
STRUCT
ROOT
INDIVIDUAL
NODES
IN
AN
RBTREE
ARE
REPRESENTED
BY
THE
STRUCTURE
GIVEN
AN
WE
CAN
MOVE
TO
ITS
LEFT
OR
RIGHT
CHILD
BY
FOLLOWING
POINTERS
OFF
THE
NODE
OF
THE
SAME
NAME
THE
RBTREE
IMPLEMENTATION
DOES
NOT
PROVIDE
SEARCH
AND
INSERT
ROUTINES
USERS
OF
RBTREES
ARE
EXPECTED
TO
DEFINE
THEIR
OWN
THIS
IS
BECAUSE
C
DOES
NOT
MAKE
GENERIC
PRO
GRAMMING
EASY
AND
THE
LINUX
KERNEL
DEVELOPERS
BELIEVED
THE
MOST
EFFICIENT
WAY
TO
IMPLE
MENT
SEARCH
AND
INSERT
WAS
TO
REQUIRE
EACH
USER
TO
DO
SO
MANUALLY
USING
PROVIDED
RBTREE
HELPER
FUNCTIONS
BUT
THEIR
OWN
COMPARISON
OPERATORS
THE
BEST
WAY
TO
DEMONSTRATE
SEARCH
AND
INSERT
IS
TO
SHOW
A
REAL
WORLD
EXAMPLE
FIRST
LET
LOOK
AT
SEARCH
THE
FOLLOWING
FUNCTION
IMPLEMENTS
A
SEARCH
OF
LINUX
PAGE
CACHE
FOR
A
CHUNK
OF
A
FILE
REPRESENTED
BY
AN
INODE
AND
OFFSET
PAIR
EACH
INODE
HAS
ITS
OWN
RBTREE
KEYED
OFF
OF
PAGE
OFFSETS
INTO
FILE
THIS
FUNCTION
THUS
SEARCHES
THE
GIVEN
INODE
RBTREE
FOR
A
MATCHING
OFFSET
VALUE
STRUCT
PAGE
STRUCT
INODE
INODE
UNSIGNED
LONG
OFFSET
STRUCT
N
INODE
WHILE
N
STRUCT
PAGE
PAGE
N
STRUCT
PAGE
IF
OFFSET
PAGE
OFFSET
N
N
ELSE
IF
OFFSET
PAGE
OFFSET
N
N
ELSE
RETURN
PAGE
RETURN
NULL
IN
THIS
EXAMPLE
THE
WHILE
LOOP
ITERATES
OVER
THE
RBTREE
TRAVERSING
AS
NEEDED
TO
THE
LEFT
OR
RIGHT
CHILD
IN
THE
DIRECTION
OF
THE
GIVEN
OFFSET
THE
IF
AND
ELSE
STATEMENTS
IMPLEMENT
THE
RBTREE
COMPARISON
FUNCTION
THUS
ENFORCING
THE
TREE
ORDERING
IF
THE
LOOP
FINDS
A
NODE
WITH
A
MATCHING
OFFSET
THE
SEARCH
IS
COMPLETE
AND
THE
FUNCTION
RETURNS
THE
ASSOCI
ATED
PAGE
STRUCTURE
IF
THE
LOOP
REACHES
THE
END
OF
THE
RBTREE
WITHOUT
FINDING
A
MATCH
ONE
DOES
NOT
EXIST
IN
THE
TREE
AND
THE
FUNCTION
RETURNS
NULL
INSERT
IS
EVEN
MORE
COMPLICATED
BECAUSE
IT
IMPLEMENTS
BOTH
SEARCH
AND
INSERTION
LOGIC
THE
FOLLOWING
ISN
T
A
TRIVIAL
FUNCTION
BUT
IF
YOU
NEED
TO
IMPLEMENT
YOUR
OWN
INSERT
ROU
TINE
THIS
IS
A
GOOD
GUIDE
STRUCT
PAGE
STRUCT
INODE
INODE
UNSIGNED
LONG
OFFSET
STRUCT
NODE
STRUCT
P
INODE
STRUCT
PARENT
NULL
STRUCT
PAGE
PAGE
WHILE
P
PARENT
P
PAGE
PARENT
STRUCT
PAGE
IF
OFFSET
PAGE
OFFSET
P
P
ELSE
IF
OFFSET
PAGE
OFFSET
P
P
ELSE
RETURN
PAGE
NODE
PARENT
P
NODE
INODE
RETURN
NULL
AS
WITH
OUR
SEARCH
FUNCTION
THE
WHILE
LOOP
IS
ITERATING
OVER
THE
TREE
MOVING
IN
THE
DIRECTION
OF
THE
PROVIDED
OFFSET
UNLIKE
WITH
SEARCH
HOWEVER
THE
FUNCTION
IS
HOPING
NOT
TO
FIND
A
MATCHING
OFFSET
BUT
INSTEAD
REACH
THE
LEAF
NODE
THAT
IS
THE
CORRECT
INSERTION
POINT
FOR
THE
NEW
OFFSET
WHEN
THE
INSERTION
POINT
IS
FOUND
IS
CALLED
TO
INSERT
THE
NEW
NODE
AT
THE
GIVEN
SPOT
IS
THEN
CALLED
TO
PERFORM
THE
COMPLICATED
REBALANCING
DANCE
THE
FUNCTION
RETURNS
NULL
IF
THE
PAGE
WAS
ADDED
TO
THE
PAGE
CACHE
AND
THE
ADDRESS
OF
AN
EXISTING
PAGE
STRUCTURE
IF
THE
PAGE
IS
ALREADY
IN
THE
CACHE
WHAT
DATA
STRUCTURE
TO
USE
WHEN
THUS
FAR
WE
VE
DISCUSSED
FOUR
OF
LINUX
MOST
IMPORTANT
DATA
STRUCTURES
LINKED
LISTS
QUEUES
MAPS
AND
RED
BLACK
TREES
IN
THIS
SECTION
WE
COVER
SOME
TIPS
TO
HELP
YOU
DECIDE
WHICH
DATA
STRUCTURE
TO
USE
IN
YOUR
OWN
CODE
IF
YOUR
PRIMARY
ACCESS
METHOD
IS
ITERATING
OVER
ALL
YOUR
DATA
USE
A
LINKED
LIST
INTUITIVELY
NO
DATA
STRUCTURE
CAN
PROVIDE
BETTER
THAN
LINEAR
COMPLEXITY
WHEN
VISITING
EVERY
ELEMENT
SO
YOU
SHOULD
FAVOR
THE
SIMPLEST
DATA
STRUCTURE
FOR
THAT
SIMPLE
JOB
ALSO
CONSIDER
LINKED
LISTS
WHEN
PERFORMANCE
IS
NOT
IMPORTANT
WHEN
YOU
NEED
TO
STORE
A
RELATIVELY
SMALL
NUMBER
OF
ITEMS
OR
WHEN
YOU
NEED
TO
INTERFACE
WITH
OTHER
KERNEL
CODE
THAT
USES
LINKED
LISTS
IF
YOUR
CODE
FOLLOWS
THE
PRODUCER
CONSUMER
PATTERN
USE
A
QUEUE
PARTICULARLY
IF
YOU
WANT
OR
CAN
COPE
WITH
A
FIXED
SIZE
BUFFER
QUEUES
MAKE
ADDING
AND
REMOVING
ITEMS
SIMPLE
AND
EFFICIENT
AND
THEY
PROVIDE
FIRST
IN
FIRST
OUT
FIFO
SEMANTICS
WHICH
IS
WHAT
MOST
PRODUCER
CONSUMER
USE
CASES
DEMAND
ON
THE
OTHER
HAND
IF
YOU
NEED
TO
STORE
AN
UNKNOWN
POTENTIALLY
LARGE
NUMBER
OF
ITEMS
A
LINKED
LIST
MAY
MAKE
MORE
SENSE
BECAUSE
YOU
CAN
DYNAMICALLY
ADD
ANY
NUMBER
OF
ITEMS
TO
THE
LIST
IF
YOU
NEED
TO
MAP
A
UID
TO
AN
OBJECT
USE
A
MAP
MAPS
MAKE
SUCH
MAPPINGS
EASY
AND
EFFICIENT
AND
THEY
ALSO
MAINTAIN
AND
ALLOCATE
THE
UID
FOR
YOU
LINUX
MAP
INTERFACE
BEING
SPECIFIC
TO
UID
TO
POINTER
MAPPINGS
ISN
T
GOOD
FOR
MUCH
ELSE
HOWEVER
IF
YOU
ARE
DEALING
WITH
DESCRIPTORS
HANDED
OUT
TO
USER
SPACE
CONSIDER
THIS
OPTION
IF
YOU
NEED
TO
STORE
A
LARGE
AMOUNT
OF
DATA
AND
LOOK
IT
UP
EFFICIENTLY
CONSIDER
A
RED
BLACK
TREE
RED
BLACK
TREES
ENABLE
THE
SEARCHING
IN
LOGARITHMIC
TIME
WHILE
STILL
PROVIDING
AN
EFFICIENT
LINEAR
TIME
IN
ORDER
TRAVERSAL
ALTHOUGH
MORE
COMPLICATED
TO
IMPLEMENT
THAN
THE
OTHER
DATA
STRUCTURES
THEIR
IN
MEMORY
FOOTPRINT
ISN
T
SIGNIFICANTLY
WORSE
IF
YOU
ARE
NOT
PERFORMING
MANY
TIME
CRITICAL
LOOK
UP
OPERATIONS
A
RED
BLACK
TREE
PROBABLY
ISN
T
YOUR
BEST
BET
IN
THAT
CASE
FAVOR
A
LINKED
LIST
NONE
OF
THESE
DATA
STRUCTURES
FIT
YOUR
NEEDS
THE
KERNEL
IMPLEMENTS
OTHER
SELDOM
USED
DATA
STRUCTURES
THAT
MIGHT
MEET
YOUR
NEEDS
SUCH
AS
RADIX
TREES
A
TYPE
OF
TRIE
AND
BITMAPS
ONLY
AFTER
EXHAUSTING
ALL
KERNEL
PROVIDED
SOLUTIONS
SHOULD
YOU
CONSIDER
ROLLING
YOUR
OWN
DATA
STRUCTURE
ONE
COMMON
DATA
STRUCTURE
OFTEN
IMPLEMENTED
IN
INDIVIDUAL
SOURCE
FILES
IS
THE
HASH
TABLE
BECAUSE
A
HASH
TABLE
IS
LITTLE
MORE
THAN
SOME
BUCKETS
AND
A
HASH
FUNCTION
AND
THE
HASH
FUNCTION
IS
SO
SPECIFIC
TO
EACH
USE
CASE
THERE
IS
LITTLE
VALUE
IN
PROVIDING
A
KERNELWIDE
SOLUTION
IN
A
NONGENERIC
PROGRAMMING
LANGUAGE
SUCH
AS
C
ALGORITHMIC
COMPLEXITY
ALGORITHMIC
COMPLEXITY
OFTEN
IN
COMPUTER
SCIENCE
AND
RELATED
DISCIPLINES
IT
IS
USEFUL
TO
EXPRESS
THE
ALGORITHMIC
COMPLEXITY
OR
SCALABILITY
OF
ALGORITHMS
QUANTITATIVELY
VARIOUS
METHODS
EXIST
FOR
REPRE
SENTING
SCALABILITY
ONE
COMMON
TECHNIQUE
IS
TO
STUDY
THE
ASYMPTOTIC
BEHAVIOR
OF
THE
ALGO
RITHM
THIS
IS
THE
BEHAVIOR
OF
THE
ALGORITHM
BECAUSE
ITS
INPUTS
GROW
EXCEEDINGLY
LARGE
AND
APPROACH
INFINITY
ASYMPTOTIC
BEHAVIOR
SHOWS
HOW
WELL
AN
ALGORITHM
SCALES
AS
ITS
INPUT
GROWS
LARGER
AND
LARGER
STUDYING
AN
ALGORITHM
SCALABILITY
HOW
IT
PERFORMS
AS
THE
SIZE
OF
ITS
INPUT
INCREASES
ENABLES
US
TO
MODEL
THE
ALGORITHM
AGAINST
A
BENCHMARK
AND
BETTER
UNDERSTAND
ITS
BEHAVIOR
ALGORITHMS
AN
ALGORITHM
IS
A
SERIES
OF
INSTRUCTIONS
POSSIBLY
ONE
OR
MORE
INPUTS
AND
ULTIMATELY
A
RESULT
OR
OUTPUT
FOR
EXAMPLE
THE
STEPS
CARRIED
OUT
TO
COUNT
THE
NUMBER
OF
PEOPLE
IN
A
ROOM
ARE
AN
ALGORITHM
WITH
THE
PEOPLE
BEING
THE
INPUT
AND
THE
COUNT
BEING
THE
OUTPUT
IN
THE
LINUX
KERNEL
BOTH
PAGE
EVICTION
AND
THE
PROCESS
SCHEDULER
ARE
EXAMPLES
OF
ALGO
RITHMS
MATHEMATICALLY
AN
ALGORITHM
IS
LIKE
A
FUNCTION
OR
AT
LEAST
YOU
CAN
MODEL
IT
AS
ONE
FOR
EXAMPLE
IF
YOU
CALL
THE
PEOPLE
COUNTING
ALGORITHM
F
AND
THE
NUMBER
OF
PEOPLE
TO
COUNT
X
YOU
CAN
WRITE
Y
F
X
PEOPLE
COUNTING
FUNCTION
WHERE
Y
IS
THE
TIME
REQUIRED
TO
COUNT
THE
X
PEOPLE
BIG
O
NOTATION
ONE
USEFUL
ASYMPTOTIC
NOTATION
IS
THE
UPPER
BOUND
WHICH
IS
A
FUNCTION
WHOSE
VALUE
AFTER
AN
INITIAL
POINT
IS
ALWAYS
GREATER
THAN
THE
VALUE
OF
THE
FUNCTION
THAT
YOU
ARE
STUDYING
IT
IS
SAID
THAT
THE
UPPER
BOUND
GROWS
AS
FAST
OR
FASTER
THAN
THE
FUNCTION
IN
QUESTION
A
SPECIAL
NOTATION
BIG
O
PRONOUNCED
BIG
OH
NOTATION
IS
USED
TO
DESCRIBE
THIS
GROWTH
IT
IS
WRITTEN
F
X
IS
O
G
X
AND
IS
READ
AS
F
IS
BIG
OH
OF
G
THE
FORMAL
MATHEMATICAL
DEFINITION
IS
IF
F
X
IS
O
G
X
THEN
C
X
SUCH
THAT
F
X
C
G
X
X
X
IN
ENGLISH
THE
TIME
TO
COMPLETE
F
X
IS
ALWAYS
LESS
THAN
OR
EQUAL
TO
THE
TIME
TO
COM
PLETE
G
X
MULTIPLIED
BY
SOME
ARBITRARY
CONSTANT
SO
LONG
AS
THE
INPUT
X
IS
LARGER
THAN
SOME
INITIAL
VALUE
X
ESSENTIALLY
YOU
ARE
LOOKING
FOR
A
FUNCTION
WHOSE
BEHAVIOR
IS
AS
BAD
AS
OR
WORSE
THAN
THE
ALGORITHM
YOU
CAN
THEN
LOOK
AT
THE
RESULT
OF
LARGE
INPUTS
TO
THIS
FUNCTION
AND
OBTAIN
AN
UNDERSTANDING
OF
THE
BOUND
OF
YOUR
ALGORITHM
BIG
THETA
NOTATION
WHEN
MOST
PEOPLE
TALK
ABOUT
BIG
O
NOTATION
THEY
ARE
MORE
ACCURATELY
REFERRING
TO
WHAT
DONALD
KNUTH
DESCRIBES
AS
BIG
THETA
NOTATION
TECHNICALLY
BIG
O
NOTATION
REFERS
TO
AN
UPPER
BOUND
FOR
EXAMPLE
IS
AN
UPPER
BOUND
OF
SO
ARE
AND
SUBSEQUENTLY
WHEN
MOST
PEOPLE
DISCUSS
FUNCTION
GROWTH
THEY
TALK
ABOUT
THE
LEAST
UPPER
BOUND
OR
A
FUNCTION
THAT
MODELS
BOTH
THE
UPPER
AND
LOWER
BOUNDS
PROFESSOR
KNUTH
THE
FATHER
OF
THE
FIELD
OF
ALGORITHMIC
ANALYSIS
DESCRIBES
THIS
AS
BIG
THETA
NOTATION
AND
GIVES
THE
FOLLOWING
DEFINITION
IF
F
X
IS
BIG
THETA
OF
G
X
THEN
G
X
IS
BOTH
AN
UPPER
BOUND
AND
A
LOWER
BOUND
FOR
F
X
THEN
YOU
CAN
SAY
THAT
F
X
IS
OF
ORDER
G
X
THE
ORDER
OR
BIG
THETA
OF
AN
ALGORITHM
IS
ONE
OF
THE
MOST
IMPORTANT
MATHEMATICAL
TOOLS
FOR
UNDERSTANDING
ALGORITHMS
IN
THE
KERNEL
CONSEQUENTLY
WHEN
PEOPLE
REFER
TO
BIG
O
NOTATION
THEY
ARE
MORE
OFTEN
TALKING
ABOUT
THE
LEAST
SUCH
BIG
O
THE
BIG
THETA
YOU
REALLY
DO
NOT
HAVE
TO
WORRY
ABOUT
THIS
UNLESS
YOU
WANT
TO
MAKE
PROFESSOR
KNUTH
REALLY
HAPPY
TIME
COMPLEXITY
CONSIDER
THE
ORIGINAL
EXAMPLE
OF
HAVING
TO
COUNT
THE
NUMBER
OF
PEOPLE
IN
A
ROOM
PRE
TEND
YOU
CAN
COUNT
ONE
PERSON
PER
SECOND
THEN
IF
THERE
ARE
PEOPLE
IN
THE
ROOM
IT
WILL
TAKE
SECONDS
TO
COUNT
THEM
MORE
GENERALLY
GIVEN
N
PEOPLE
IT
WILL
TAKE
N
SECONDS
TO
COUNT
EVERYONE
THUS
YOU
CAN
SAY
THIS
ALGORITHM
IS
O
N
WHAT
IF
THE
TASK
WAS
TO
DANCE
IN
FRONT
OF
EVERYONE
IN
THE
ROOM
BECAUSE
IT
WOULD
TAKE
THE
SAME
AMOUNT
OF
TIME
TO
DANCE
WHETHER
THERE
WERE
OR
PEOPLE
IN
THE
ROOM
THIS
TASK
IS
O
SEE
TABLE
FOR
OTHER
COMMON
COMPLEXITIES
TABLE
TABLE
OF
COMMON
TIME
COMPLEXITY
VALUES
O
G
X
NAME
CONSTANT
PERFECT
SCALABILITY
LOG
N
LOGARITHMIC
N
LINEAR
QUADRATIC
CUBIC
EXPONENTIAL
N
FACTORIAL
IF
YOU
RE
CURIOUS
THE
LOWER
BOUND
IS
MODELED
BY
BIG
OMEGA
NOTATION
THE
DEFINITION
IS
THE
SAME
AS
BIG
O
EXCEPT
G
X
IS
ALWAYS
LESS
THAN
OR
EQUAL
TO
F
X
NOT
GREATER
THAN
OR
EQUAL
TO
BIG
OMEGA
NOTATION
IS
LESS
USEFUL
THAN
BIG
O
BECAUSE
FINDING
FUNCTIONS
SMALLER
THAN
YOUR
FUNCTION
IS
RARELY
INDICATIVE
OF
BEHAVIOR
CONCLUSION
WHAT
IS
THE
COMPLEXITY
OF
INTRODUCING
EVERYONE
IN
THE
ROOM
TO
EVERYONE
ELSE
WHAT
IS
A
POSSIBLE
FUNCTION
THAT
MODELS
THIS
ALGORITHM
IF
IT
TOOK
SECONDS
TO
INTRODUCE
EACH
PERSON
HOW
LONG
WOULD
IT
TAKE
TO
INTRODUCE
PEOPLE
TO
EACH
OTHER
WHAT
ABOUT
PEOPLE
TO
EACH
OTHER
UNDERSTANDING
HOW
AN
ALGORITHM
PERFORMS
AS
IT
HAS
EVER
MORE
WORK
TO
DO
IS
A
CRUCIAL
COMPONENT
IN
DETERMINING
THE
BEST
ALGORITHM
FOR
A
GIVEN
JOB
OF
COURSE
IT
IS
WISE
TO
AVOID
COMPLEXITIES
SUCH
AS
O
N
OR
O
LIKEWISE
IT
IS
USU
ALLY
AN
IMPROVEMENT
TO
REPLACE
AN
O
N
ALGORITHM
WITH
A
FUNCTIONALLY
EQUIVALENT
O
LOG
N
ALGORITHM
THIS
IS
NOT
ALWAYS
THE
CASE
HOWEVER
AND
A
BLIND
ASSUMPTION
SHOULD
NOT
BE
MADE
BASED
SOLELY
ON
BIG
O
NOTATION
RECALL
THAT
GIVEN
O
G
X
THERE
IS
A
CONSTANT
C
MULTIPLIED
BY
G
X
THEREFORE
IT
IS
POSSIBLE
THAT
AN
O
ALGORITHM
TAKES
HOURS
TO
COM
PLETE
SURE
IT
IS
ALWAYS
HOURS
REGARDLESS
OF
HOW
LARGE
THE
INPUT
BUT
THAT
CAN
STILL
BE
A
LONG
TIME
COMPARED
TO
AN
O
N
ALGORITHM
WITH
FEW
INPUTS
THE
TYPICAL
INPUT
SIZE
SHOULD
ALWAYS
BE
TAKEN
INTO
ACCOUNT
WHEN
COMPARING
ALGORITHMS
FAVOR
LESS
COMPLEX
ALGORITHMS
BUT
KEEP
IN
MIND
THE
OVERHEAD
OF
THE
ALGORITHM
IN
RELATION
TO
THE
TYPICAL
INPUT
SIZE
DO
NOT
BLINDLY
OPTIMIZE
TO
A
LEVEL
OF
SCALABILITY
YOU
WILL
NEVER
NEED
TO
SUPPORT
CONCLUSION
IN
THIS
CHAPTER
WE
DISCUSSED
MANY
OF
THE
GENERIC
DATA
STRUCTURES
THAT
LINUX
KERNEL
DEVELOPERS
USE
TO
IMPLEMENT
EVERYTHING
FROM
THE
PROCESS
SCHEDULER
TO
DEVICE
DRIVERS
YOU
WILL
FIND
THESE
DATA
STRUCTURES
USEFUL
AS
WE
CONTINUE
OUR
STUDY
OF
THE
LINUX
KERNEL
WHEN
WRITING
YOUR
OWN
KERNEL
CODE
ALWAYS
REUSE
EXISTING
KERNEL
INFRASTRUCTURE
AND
DON
T
REINVENT
THE
WHEEL
WE
ALSO
COVERED
ALGORITHMIC
COMPLEXITY
AND
TOOLS
FOR
MEASURING
AND
EXPRESSING
IT
THE
MOST
NOTABLE
BEING
BIG
O
NOTATION
THROUGHOUT
THIS
BOOK
AND
THE
LINUX
KERNEL
BIG
O
NOTATION
IS
AN
IMPORTANT
NOTION
OF
HOW
WELL
ALGORITHMS
AND
KERNEL
COMPONENTS
SCALE
IN
LIGHT
OF
MANY
USERS
PROCESSES
PROCESSORS
NETWORK
CONNECTIONS
AND
OTHER
EVER
EXPANDING
INPUTS
INTERRUPTS
AND
INTERRUPT
HANDLERS
A
CORE
RESPONSIBILITY
OF
ANY
OPERATING
SYSTEM
KERNEL
IS
MANAGING
THE
HARDWARE
CON
NECTED
TO
THE
MACHINE
HARD
DRIVES
AND
BLU
RAY
DISCS
KEYBOARDS
AND
MICE
PROCESSORS
AND
WIRELESS
RADIOS
TO
MEET
THIS
RESPONSIBILITY
THE
KERNEL
NEEDS
TO
COMMUNICATE
WITH
THE
MACHINE
INDIVIDUAL
DEVICES
GIVEN
THAT
PROCESSORS
CAN
BE
ORDERS
OF
MAGNITUDES
FASTER
THAN
THE
HARDWARE
THEY
TALK
TO
IT
IS
NOT
IDEAL
FOR
THE
KERNEL
TO
ISSUE
A
REQUEST
AND
WAIT
FOR
A
RESPONSE
FROM
THE
SIGNIFICANTLY
SLOWER
HARDWARE
INSTEAD
BECAUSE
THE
HARDWARE
IS
COM
PARATIVELY
SLOW
TO
RESPOND
THE
KERNEL
MUST
BE
FREE
TO
GO
AND
HANDLE
OTHER
WORK
DEALING
WITH
THE
HARDWARE
ONLY
AFTER
THAT
HARDWARE
HAS
ACTUALLY
COMPLETED
ITS
WORK
HOW
CAN
THE
PROCESSOR
WORK
WITH
HARDWARE
WITHOUT
IMPACTING
THE
MACHINE
OVERALL
PERFORMANCE
ONE
ANSWER
TO
THIS
QUESTION
IS
POLLING
PERIODICALLY
THE
KERNEL
CAN
CHECK
THE
STATUS
OF
THE
HARDWARE
IN
THE
SYSTEM
AND
RESPOND
ACCORDINGLY
POLLING
INCURS
OVERHEAD
HOWEVER
BECAUSE
IT
MUST
OCCUR
REPEATEDLY
REGARDLESS
OF
WHETHER
THE
HARDWARE
IS
ACTIVE
OR
READY
A
BETTER
SOLUTION
IS
TO
PROVIDE
A
MECHANISM
FOR
THE
HARDWARE
TO
SIGNAL
TO
THE
KERNEL
WHEN
ATTENTION
IS
NEEDED
THIS
MECHANISM
IS
CALLED
AN
INTERRUPT
IN
THIS
CHAPTER
WE
DISCUSS
INTERRUPTS
AND
HOW
THE
KERNEL
RESPONDS
TO
THEM
WITH
SPECIAL
FUNCTIONS
CALLED
INTERRUPT
HANDLERS
INTERRUPTS
INTERRUPTS
ENABLE
HARDWARE
TO
SIGNAL
TO
THE
PROCESSOR
FOR
EXAMPLE
AS
YOU
TYPE
THE
KEY
BOARD
CONTROLLER
THE
HARDWARE
DEVICE
THAT
MANAGES
THE
KEYBOARD
ISSUES
AN
ELECTRICAL
SIG
NAL
TO
THE
PROCESSOR
TO
ALERT
THE
OPERATING
SYSTEM
TO
NEWLY
AVAILABLE
KEY
PRESSES
THESE
ELECTRICAL
SIGNALS
ARE
INTERRUPTS
THE
PROCESSOR
RECEIVES
THE
INTERRUPT
AND
SIGNALS
THE
OPER
ATING
SYSTEM
TO
ENABLE
THE
OPERATING
SYSTEM
TO
RESPOND
TO
THE
NEW
DATA
HARDWARE
DEVICES
GENERATE
INTERRUPTS
ASYNCHRONOUSLY
WITH
RESPECT
TO
THE
PROCESSOR
CLOCK
THEY
CAN
OCCUR
AT
ANY
TIME
CONSEQUENTLY
THE
KERNEL
CAN
BE
INTERRUPTED
AT
ANY
TIME
TO
PROCESS
INTERRUPTS
AN
INTERRUPT
IS
PHYSICALLY
PRODUCED
BY
ELECTRONIC
SIGNALS
ORIGINATING
FROM
HARDWARE
DEVICES
AND
DIRECTED
INTO
INPUT
PINS
ON
AN
INTERRUPT
CONTROLLER
A
SIMPLE
CHIP
THAT
MULTI
PLEXES
MULTIPLE
INTERRUPT
LINES
INTO
A
SINGLE
LINE
TO
THE
PROCESSOR
UPON
RECEIVING
AN
INTER
RUPT
THE
INTERRUPT
CONTROLLER
SENDS
A
SIGNAL
TO
THE
PROCESSOR
THE
PROCESSOR
DETECTS
THIS
SIG
NAL
AND
INTERRUPTS
ITS
CURRENT
EXECUTION
TO
HANDLE
THE
INTERRUPT
THE
PROCESSOR
CAN
THEN
NOTIFY
THE
OPERATING
SYSTEM
THAT
AN
INTERRUPT
HAS
OCCURRED
AND
THE
OPERATING
SYSTEM
CAN
HANDLE
THE
INTERRUPT
APPROPRIATELY
DIFFERENT
DEVICES
CAN
BE
ASSOCIATED
WITH
DIFFERENT
INTERRUPTS
BY
MEANS
OF
A
UNIQUE
VALUE
ASSOCIATED
WITH
EACH
INTERRUPT
THIS
WAY
INTERRUPTS
FROM
THE
KEYBOARD
ARE
DISTINCT
FROM
INTERRUPTS
FROM
THE
HARD
DRIVE
THIS
ENABLES
THE
OPERATING
SYSTEM
TO
DIFFERENTIATE
BETWEEN
INTERRUPTS
AND
TO
KNOW
WHICH
HARDWARE
DEVICE
CAUSED
WHICH
INTERRUPT
IN
TURN
THE
OPERATING
SYSTEM
CAN
SERVICE
EACH
INTERRUPT
WITH
ITS
CORRESPONDING
HANDLER
THESE
INTERRUPT
VALUES
ARE
OFTEN
CALLED
INTERRUPT
REQUEST
IRQ
LINES
EACH
IRQ
LINE
IS
ASSIGNED
A
NUMERIC
VALUE
FOR
EXAMPLE
ON
THE
CLASSIC
PC
IRQ
ZERO
IS
THE
TIMER
INTER
RUPT
AND
IRQ
ONE
IS
THE
KEYBOARD
INTERRUPT
NOT
ALL
INTERRUPT
NUMBERS
HOWEVER
ARE
SO
RIGIDLY
DEFINED
INTERRUPTS
ASSOCIATED
WITH
DEVICES
ON
THE
PCI
BUS
FOR
EXAMPLE
GENERALLY
ARE
DYNAMICALLY
ASSIGNED
OTHER
NON
PC
ARCHITECTURES
HAVE
SIMILAR
DYNAMIC
ASSIGNMENTS
FOR
INTERRUPT
VALUES
THE
IMPORTANT
NOTION
IS
THAT
A
SPECIFIC
INTERRUPT
IS
ASSOCIATED
WITH
A
SPECIFIC
DEVICE
AND
THE
KERNEL
KNOWS
THIS
THE
HARDWARE
THEN
ISSUES
INTERRUPTS
TO
GET
THE
KERNEL
ATTENTION
HEY
I
HAVE
NEW
KEY
PRESSES
WAITING
READ
AND
PROCESS
THESE
BAD
BOYS
EXCEPTIONS
IN
OS
TEXTS
EXCEPTIONS
ARE
OFTEN
DISCUSSED
AT
THE
SAME
TIME
AS
INTERRUPTS
UNLIKE
INTER
RUPTS
EXCEPTIONS
OCCUR
SYNCHRONOUSLY
WITH
RESPECT
TO
THE
PROCESSOR
CLOCK
INDEED
THEY
ARE
OFTEN
CALLED
SYNCHRONOUS
INTERRUPTS
EXCEPTIONS
ARE
PRODUCED
BY
THE
PROCESSOR
WHILE
EXECUT
ING
INSTRUCTIONS
EITHER
IN
RESPONSE
TO
A
PROGRAMMING
ERROR
FOR
EXAMPLE
DIVIDE
BY
ZERO
OR
ABNORMAL
CONDITIONS
THAT
MUST
BE
HANDLED
BY
THE
KERNEL
FOR
EXAMPLE
A
PAGE
FAULT
BECAUSE
MANY
PROCESSOR
ARCHITECTURES
HANDLE
EXCEPTIONS
IN
A
SIMILAR
MANNER
TO
INTERRUPTS
THE
KERNEL
INFRASTRUCTURE
FOR
HANDLING
THE
TWO
IS
SIMILAR
MUCH
OF
THE
DISCUSSION
OF
INTERRUPTS
ASYNCHRONOUS
INTERRUPTS
GENERATED
BY
HARDWARE
IN
THIS
CHAPTER
ALSO
PERTAINS
TO
EXCEPTIONS
SYNCHRONOUS
INTERRUPTS
GENERATED
BY
THE
PROCESSOR
YOU
ARE
ALREADY
FAMILIAR
WITH
ONE
EXCEPTION
IN
THE
PREVIOUS
CHAPTER
YOU
SAW
HOW
SYSTEM
CALLS
ON
THE
ARCHITECTURE
ARE
IMPLEMENTED
BY
THE
ISSUANCE
OF
A
SOFTWARE
INTERRUPT
WHICH
TRAPS
INTO
THE
KERNEL
AND
CAUSES
EXECUTION
OF
A
SPECIAL
SYSTEM
CALL
HANDLER
INTER
RUPTS
WORK
IN
A
SIMILAR
WAY
YOU
WILL
SEE
EXCEPT
HARDWARE
NOT
SOFTWARE
ISSUES
INTERRUPTS
INTERRUPT
HANDLERS
THE
FUNCTION
THE
KERNEL
RUNS
IN
RESPONSE
TO
A
SPECIFIC
INTERRUPT
IS
CALLED
AN
INTERRUPT
HANDLER
OR
INTERRUPT
SERVICE
ROUTINE
ISR
EACH
DEVICE
THAT
GENERATES
INTERRUPTS
HAS
AN
ASSOCIATED
INTERRUPT
HANDLER
FOR
EXAMPLE
ONE
FUNCTION
HANDLES
INTERRUPTS
FROM
THE
SYSTEM
TIMER
WHEREAS
ANOTHER
FUNCTION
HANDLES
INTERRUPTS
GENERATED
BY
THE
KEYBOARD
THE
INTERRUPT
HANDLER
FOR
A
DEVICE
IS
PART
OF
THE
DEVICE
DRIVER
THE
KERNEL
CODE
THAT
MANAGES
THE
DEVICE
IN
LINUX
INTERRUPT
HANDLERS
ARE
NORMAL
C
FUNCTIONS
THEY
MATCH
A
SPECIFIC
PROTOTYPE
WHICH
ENABLES
THE
KERNEL
TO
PASS
THE
HANDLER
INFORMATION
IN
A
STANDARD
WAY
BUT
OTHERWISE
TOP
HALVES
VERSUS
BOTTOM
HALVES
THEY
ARE
ORDINARY
FUNCTIONS
WHAT
DIFFERENTIATES
INTERRUPT
HANDLERS
FROM
OTHER
KERNEL
FUNC
TIONS
IS
THAT
THE
KERNEL
INVOKES
THEM
IN
RESPONSE
TO
INTERRUPTS
AND
THAT
THEY
RUN
IN
A
SPE
CIAL
CONTEXT
DISCUSSED
LATER
IN
THIS
CHAPTER
CALLED
INTERRUPT
CONTEXT
THIS
SPECIAL
CONTEXT
IS
OCCASIONALLY
CALLED
ATOMIC
CONTEXT
BECAUSE
AS
WE
SHALL
SEE
CODE
EXECUTING
IN
THIS
CONTEXT
IS
UNABLE
TO
BLOCK
IN
THIS
BOOK
WE
WILL
USE
THE
TERM
INTERRUPT
CONTEXT
BECAUSE
AN
INTERRUPT
CAN
OCCUR
AT
ANY
TIME
AN
INTERRUPT
HANDLER
CAN
IN
TURN
BE
EXE
CUTED
AT
ANY
TIME
IT
IS
IMPERATIVE
THAT
THE
HANDLER
RUNS
QUICKLY
TO
RESUME
EXECUTION
OF
THE
INTERRUPTED
CODE
AS
SOON
AS
POSSIBLE
THEREFORE
WHILE
IT
IS
IMPORTANT
TO
THE
HARDWARE
THAT
THE
OPERATING
SYSTEM
SERVICES
THE
INTERRUPT
WITHOUT
DELAY
IT
IS
ALSO
IMPORTANT
TO
THE
REST
OF
THE
SYSTEM
THAT
THE
INTERRUPT
HANDLER
EXECUTES
IN
AS
SHORT
A
PERIOD
AS
POSSIBLE
AT
THE
VERY
LEAST
AN
INTERRUPT
HANDLER
JOB
IS
TO
ACKNOWLEDGE
THE
INTERRUPT
RECEIPT
TO
THE
HARDWARE
HEY
HARDWARE
I
HEAR
YA
NOW
GET
BACK
TO
WORK
OFTEN
HOWEVER
INTERRUPT
HAN
DLERS
HAVE
A
LARGE
AMOUNT
OF
WORK
TO
PERFORM
FOR
EXAMPLE
CONSIDER
THE
INTERRUPT
HANDLER
FOR
A
NETWORK
DEVICE
ON
TOP
OF
RESPONDING
TO
THE
HARDWARE
THE
INTERRUPT
HANDLER
NEEDS
TO
COPY
NETWORKING
PACKETS
FROM
THE
HARDWARE
INTO
MEMORY
PROCESS
THEM
AND
PUSH
THE
PACKETS
DOWN
TO
THE
APPROPRIATE
PROTOCOL
STACK
OR
APPLICATION
OBVIOUSLY
THIS
CAN
BE
A
LOT
OF
WORK
ESPECIALLY
WITH
TODAY
GIGABIT
AND
GIGABIT
ETHERNET
CARDS
TOP
HALVES
VERSUS
BOTTOM
HALVES
THESE
TWO
GOALS
THAT
AN
INTERRUPT
HANDLER
EXECUTE
QUICKLY
AND
PERFORM
A
LARGE
AMOUNT
OF
WORK
CLEARLY
CONFLICT
WITH
ONE
ANOTHER
BECAUSE
OF
THESE
COMPETING
GOALS
THE
PRO
CESSING
OF
INTERRUPTS
IS
SPLIT
INTO
TWO
PARTS
OR
HALVES
THE
INTERRUPT
HANDLER
IS
THE
TOP
HALF
THE
TOP
HALF
IS
RUN
IMMEDIATELY
UPON
RECEIPT
OF
THE
INTERRUPT
AND
PERFORMS
ONLY
THE
WORK
THAT
IS
TIME
CRITICAL
SUCH
AS
ACKNOWLEDGING
RECEIPT
OF
THE
INTERRUPT
OR
RESETTING
THE
HARDWARE
WORK
THAT
CAN
BE
PERFORMED
LATER
IS
DEFERRED
UNTIL
THE
BOTTOM
HALF
THE
BOTTOM
HALF
RUNS
IN
THE
FUTURE
AT
A
MORE
CONVENIENT
TIME
WITH
ALL
INTERRUPTS
ENABLED
LINUX
PRO
VIDES
VARIOUS
MECHANISMS
FOR
IMPLEMENTING
BOTTOM
HALVES
AND
THEY
ARE
ALL
DISCUSSED
IN
CHAPTER
BOTTOM
HALVES
AND
DEFERRING
WORK
LET
LOOK
AT
AN
EXAMPLE
OF
THE
TOP
HALF
BOTTOM
HALF
DICHOTOMY
USING
OUR
OLD
FRIEND
THE
NETWORK
CARD
WHEN
NETWORK
CARDS
RECEIVE
PACKETS
FROM
THE
NETWORK
THEY
NEED
TO
ALERT
THE
KERNEL
OF
THEIR
AVAILABILITY
THEY
WANT
AND
NEED
TO
DO
THIS
IMMEDIATELY
TO
OPTI
MIZE
NETWORK
THROUGHPUT
AND
LATENCY
AND
AVOID
TIMEOUTS
THUS
THEY
IMMEDIATELY
ISSUE
AN
INTERRUPT
HEY
KERNEL
I
HAVE
SOME
FRESH
PACKETS
HERE
THE
KERNEL
RESPONDS
BY
EXECUTING
THE
NETWORK
CARD
REGISTERED
INTERRUPT
THE
INTERRUPT
RUNS
ACKNOWLEDGES
THE
HARDWARE
COPIES
THE
NEW
NETWORKING
PACKETS
INTO
MAIN
MEMORY
AND
READIES
THE
NETWORK
CARD
FOR
MORE
PACKETS
THESE
JOBS
ARE
THE
IMPORTANT
TIME
CRITICAL
AND
HARDWARE
SPECIFIC
WORK
THE
KERNEL
GENERALLY
NEEDS
TO
QUICKLY
COPY
THE
NETWORKING
PACKET
INTO
MAIN
MEMORY
BECAUSE
THE
NETWORK
DATA
BUFFER
ON
THE
NETWORKING
CARD
IS
FIXED
AND
MINISCULE
IN
SIZE
PARTICULARLY
COMPARED
TO
MAIN
MEMORY
DELAYS
IN
COPYING
THE
PACKETS
CAN
RESULT
IN
A
BUFFER
OVERRUN
WITH
INCOMING
PACKETS
OVERWHELMING
THE
NETWORKING
CARD
BUFFER
AND
THUS
PACKETS
BEING
DROPPED
AFTER
THE
NETWORKING
DATA
IS
SAFELY
IN
THE
MAIN
MEMORY
THE
INTERRUPT
JOB
IS
DONE
AND
IT
CAN
RETURN
CONTROL
OF
THE
SYSTEM
TO
WHATEVER
CODE
WAS
INTERRUPTED
WHEN
THE
INTERRUPT
WAS
GENERATED
THE
REST
OF
THE
PROCESSING
AND
HANDLING
OF
THE
PACKETS
OCCURS
LATER
IN
THE
BOT
TOM
HALF
IN
THIS
CHAPTER
WE
LOOK
AT
THE
TOP
HALF
IN
THE
NEXT
CHAPTER
WE
STUDY
THE
BOTTOM
REGISTERING
AN
INTERRUPT
HANDLER
INTERRUPT
HANDLERS
ARE
THE
RESPONSIBILITY
OF
THE
DRIVER
MANAGING
THE
HARDWARE
EACH
DEVICE
HAS
ONE
ASSOCIATED
DRIVER
AND
IF
THAT
DEVICE
USES
INTERRUPTS
AND
MOST
DO
THEN
THAT
DRIVER
MUST
REGISTER
ONE
INTERRUPT
HANDLER
DRIVERS
CAN
REGISTER
AN
INTERRUPT
HANDLER
AND
ENABLE
A
GIVEN
INTERRUPT
LINE
FOR
HANDLING
WITH
THE
FUNCTION
WHICH
IS
DECLARED
IN
LINUX
INTERRUPT
H
ALLOCATE
A
GIVEN
INTERRUPT
LINE
INT
UNSIGNED
INT
IRQ
HANDLER
UNSIGNED
LONG
FLAGS
CONST
CHAR
NAME
VOID
DEV
THE
FIRST
PARAMETER
IRQ
SPECIFIES
THE
INTERRUPT
NUMBER
TO
ALLOCATE
FOR
SOME
DEVICES
FOR
EXAMPLE
LEGACY
PC
DEVICES
SUCH
AS
THE
SYSTEM
TIMER
OR
KEYBOARD
THIS
VALUE
IS
TYPICALLY
HARD
CODED
FOR
MOST
OTHER
DEVICES
IT
IS
PROBED
OR
OTHERWISE
DETERMINED
PROGRAMMATI
CALLY
AND
DYNAMICALLY
THE
SECOND
PARAMETER
HANDLER
IS
A
FUNCTION
POINTER
TO
THE
ACTUAL
INTERRUPT
HANDLER
THAT
SERVICES
THIS
INTERRUPT
THIS
FUNCTION
IS
INVOKED
WHENEVER
THE
OPERATING
SYSTEM
RECEIVES
THE
INTERRUPT
TYPEDEF
INT
VOID
NOTE
THE
SPECIFIC
PROTOTYPE
OF
THE
HANDLER
FUNCTION
IT
TAKES
TWO
PARAMETERS
AND
HAS
A
RETURN
VALUE
OF
THIS
FUNCTION
IS
DISCUSSED
LATER
IN
THIS
CHAPTER
INTERRUPT
HANDLER
FLAGS
THE
THIRD
PARAMETER
FLAGS
CAN
BE
EITHER
ZERO
OR
A
BIT
MASK
OF
ONE
OR
MORE
OF
THE
FLAGS
DEFINED
IN
LINUX
INTERRUPT
H
AMONG
THESE
FLAGS
THE
MOST
IMPORTANT
ARE
N
WHEN
SET
THIS
FLAG
INSTRUCTS
THE
KERNEL
TO
DISABLE
ALL
INTERRUPTS
WHEN
EXECUTING
THIS
INTERRUPT
HANDLER
WHEN
UNSET
INTERRUPT
HANDLERS
RUN
WITH
ALL
INTERRUPTS
EXCEPT
THEIR
OWN
ENABLED
MOST
INTERRUPT
HANDLERS
DO
NOT
SET
THIS
FLAG
AS
DISABLING
ALL
INTERRUPTS
IS
BAD
FORM
ITS
USE
IS
RESERVED
FOR
PERFORMANCE
SENSITIVE
INTER
RUPTS
THAT
EXECUTE
QUICKLY
THIS
FLAG
IS
THE
CURRENT
MANIFESTATION
OF
THE
FLAG
WHICH
IN
THE
PAST
DISTINGUISHED
BETWEEN
FAST
AND
SLOW
INTERRUPTS
N
THIS
FLAG
SPECIFIES
THAT
INTERRUPTS
GENERATED
BY
THIS
DEVICE
SHOULD
CONTRIBUTE
TO
THE
KERNEL
ENTROPY
POOL
THE
KERNEL
ENTROPY
POOL
PROVIDES
TRULY
RANDOM
NUMBERS
DERIVED
FROM
VARIOUS
RANDOM
EVENTS
IF
THIS
FLAG
IS
SPECIFIED
THE
TIMING
OF
INTERRUPTS
FROM
THIS
DEVICE
ARE
FED
TO
THE
POOL
AS
ENTROPY
DO
NOT
SET
REGISTERING
AN
INTERRUPT
HANDLER
THIS
IF
YOUR
DEVICE
ISSUES
INTERRUPTS
AT
A
PREDICTABLE
RATE
FOR
EXAMPLE
THE
SYSTEM
TIMER
OR
CAN
BE
INFLUENCED
BY
EXTERNAL
ATTACKERS
FOR
EXAMPLE
A
NETWORKING
DEVICE
ON
THE
OTHER
HAND
MOST
OTHER
HARDWARE
GENERATES
INTERRUPTS
AT
NONDETER
MINISTIC
TIMES
AND
IS
THEREFORE
A
GOOD
SOURCE
OF
ENTROPY
N
THIS
FLAG
SPECIFIES
THAT
THIS
HANDLER
PROCESSES
INTERRUPTS
FOR
THE
SYS
TEM
TIMER
N
THIS
FLAG
SPECIFIES
THAT
THE
INTERRUPT
LINE
CAN
BE
SHARED
AMONG
MUL
TIPLE
INTERRUPT
HANDLERS
EACH
HANDLER
REGISTERED
ON
A
GIVEN
LINE
MUST
SPECIFY
THIS
FLAG
OTHERWISE
ONLY
ONE
HANDLER
CAN
EXIST
PER
LINE
MORE
INFORMATION
ON
SHARED
HANDLERS
IS
PROVIDED
IN
A
FOLLOWING
SECTION
THE
FOURTH
PARAMETER
NAME
IS
AN
ASCII
TEXT
REPRESENTATION
OF
THE
DEVICE
ASSOCIATED
WITH
THE
INTERRUPT
FOR
EXAMPLE
THIS
VALUE
FOR
THE
KEYBOARD
INTERRUPT
ON
A
PC
IS
KEY
BOARD
THESE
TEXT
NAMES
ARE
USED
BY
PROC
IRQ
AND
PROC
INTERRUPTS
FOR
COMMUNICA
TION
WITH
THE
USER
WHICH
IS
DISCUSSED
SHORTLY
THE
FIFTH
PARAMETER
DEV
IS
USED
FOR
SHARED
INTERRUPT
LINES
WHEN
AN
INTERRUPT
HANDLER
IS
FREED
DISCUSSED
LATER
DEV
PROVIDES
A
UNIQUE
COOKIE
TO
ENABLE
THE
REMOVAL
OF
ONLY
THE
DESIRED
INTERRUPT
HANDLER
FROM
THE
INTERRUPT
LINE
WITHOUT
THIS
PARAMETER
IT
WOULD
BE
IMPOSSIBLE
FOR
THE
KERNEL
TO
KNOW
WHICH
HANDLER
TO
REMOVE
ON
A
GIVEN
INTERRUPT
LINE
YOU
CAN
PASS
NULL
HERE
IF
THE
LINE
IS
NOT
SHARED
BUT
YOU
MUST
PASS
A
UNIQUE
COOKIE
IF
YOUR
INTERRUPT
LINE
IS
SHARED
AND
UNLESS
YOUR
DEVICE
IS
OLD
AND
CRUSTY
AND
LIVES
ON
THE
ISA
BUS
THERE
IS
A
GOOD
CHANCE
IT
MUST
SUPPORT
SHARING
THIS
POINTER
IS
ALSO
PASSED
INTO
THE
INTER
RUPT
HANDLER
ON
EACH
INVOCATION
A
COMMON
PRACTICE
IS
TO
PASS
THE
DRIVER
DEVICE
STRUC
TURE
THIS
POINTER
IS
UNIQUE
AND
MIGHT
BE
USEFUL
TO
HAVE
WITHIN
THE
HANDLERS
ON
SUCCESS
RETURNS
ZERO
A
NONZERO
VALUE
INDICATES
AN
ERROR
IN
WHICH
CASE
THE
SPECIFIED
INTERRUPT
HANDLER
WAS
NOT
REGISTERED
A
COMMON
ERROR
IS
EBUSY
WHICH
DENOTES
THAT
THE
GIVEN
INTERRUPT
LINE
IS
ALREADY
IN
USE
AND
EITHER
THE
CURRENT
USER
OR
YOU
DID
NOT
SPECIFY
NOTE
THAT
CAN
SLEEP
AND
THEREFORE
CANNOT
BE
CALLED
FROM
INTERRUPT
CONTEXT
OR
OTHER
SITUATIONS
WHERE
CODE
CANNOT
BLOCK
IT
IS
A
COMMON
MISTAKE
TO
CALL
WHEN
IT
IS
UNSAFE
TO
SLEEP
THIS
IS
PARTLY
BECAUSE
OF
WHY
CAN
BLOCK
IT
IS
INDEED
UNCLEAR
ON
REGISTRATION
AN
ENTRY
CORRESPONDING
TO
THE
INTERRUPT
IS
CREATED
IN
PROC
IRQ
THE
FUNCTION
CREATES
NEW
PROCFS
ENTRIES
THIS
FUNC
TION
CALLS
TO
SET
UP
THE
NEW
PROCFS
ENTRIES
WHICH
IN
TURN
CALLS
KMALLOC
TO
ALLOCATE
MEMORY
AS
YOU
WILL
SEE
IN
CHAPTER
MEMORY
MANAGEMENT
KMALLOC
CAN
SLEEP
SO
THERE
YOU
GO
AN
INTERRUPT
EXAMPLE
IN
A
DRIVER
REQUESTING
AN
INTERRUPT
LINE
AND
INSTALLING
A
HANDLER
IS
DONE
VIA
IF
IRQN
PRINTK
CANNOT
REGISTER
IRQ
D
N
IRQN
RETURN
EIO
IN
THIS
EXAMPLE
IRQN
IS
THE
REQUESTED
INTERRUPT
LINE
IS
THE
HANDLER
WE
SPECIFY
VIA
FLAGS
THAT
THE
LINE
CAN
BE
SHARED
THE
DEVICE
IS
NAMED
AND
WE
PASSED
FOR
DEV
ON
FAILURE
THE
CODE
PRINTS
AN
ERROR
AND
RETURNS
IF
THE
CALL
RETURNS
ZERO
THE
HANDLER
HAS
BEEN
SUCCESSFULLY
INSTALLED
FROM
THAT
POINT
FORWARD
THE
HANDLER
IS
INVOKED
IN
RESPONSE
TO
AN
INTERRUPT
IT
IS
IMPORTANT
TO
INITIALIZE
HARDWARE
AND
REGISTER
AN
INTERRUPT
HANDLER
IN
THE
PROPER
ORDER
TO
PREVENT
THE
INTERRUPT
HANDLER
FROM
RUNNING
BEFORE
THE
DEVICE
IS
FULLY
INITIALIZED
FREEING
AN
INTERRUPT
HANDLER
WHEN
YOUR
DRIVER
UNLOADS
YOU
NEED
TO
UNREGISTER
YOUR
INTERRUPT
HANDLER
AND
POTENTIALLY
DISABLE
THE
INTERRUPT
LINE
TO
DO
THIS
CALL
VOID
UNSIGNED
INT
IRQ
VOID
DEV
IF
THE
SPECIFIED
INTERRUPT
LINE
IS
NOT
SHARED
THIS
FUNCTION
REMOVES
THE
HANDLER
AND
DIS
ABLES
THE
LINE
IF
THE
INTERRUPT
LINE
IS
SHARED
THE
HANDLER
IDENTIFIED
VIA
DEV
IS
REMOVED
BUT
THE
INTERRUPT
LINE
IS
DISABLED
ONLY
WHEN
THE
LAST
HANDLER
IS
REMOVED
NOW
YOU
CAN
SEE
WHY
A
UNIQUE
DEV
IS
IMPORTANT
WITH
SHARED
INTERRUPT
LINES
A
UNIQUE
COOKIE
IS
REQUIRED
TO
DIF
FERENTIATE
BETWEEN
THE
MULTIPLE
HANDLERS
THAT
CAN
EXIST
ON
A
SINGLE
LINE
AND
ENABLE
TO
REMOVE
ONLY
THE
CORRECT
HANDLER
IN
EITHER
CASE
SHARED
OR
UNSHARED
IF
DEV
IS
NON
NULL
IT
MUST
MATCH
THE
DESIRED
HANDLER
A
CALL
TO
MUST
BE
MADE
FROM
PROCESS
CONTEXT
TABLE
REVIEWS
THE
FUNCTIONS
FOR
REGISTERING
AND
DEREGISTERING
AN
INTERRUPT
HANDLER
TABLE
INTERRUPT
REGISTRATION
METHODS
FUNCTION
DESCRIPTION
REGISTER
A
GIVEN
INTERRUPT
HANDLER
ON
A
GIVEN
INTERRUPT
LINE
UNREGISTER
A
GIVEN
INTERRUPT
HANDLER
IF
NO
HANDLERS
REMAIN
ON
THE
LINE
THE
GIVEN
INTERRUPT
LINE
IS
DISABLED
WRITING
AN
INTERRUPT
HANDLER
THE
FOLLOWING
IS
A
DECLARATION
OF
AN
INTERRUPT
HANDLER
STATIC
INT
IRQ
VOID
DEV
NOTE
THAT
THIS
DECLARATION
MATCHES
THE
PROTOTYPE
OF
THE
HANDLER
ARGUMENT
GIVEN
TO
THE
FIRST
PARAMETER
IRQ
IS
THE
NUMERIC
VALUE
OF
THE
INTERRUPT
LINE
THE
HANDLER
IS
SERVICING
THIS
VALUE
IS
PASSED
INTO
THE
HANDLER
BUT
IT
IS
NOT
USED
VERY
OFTEN
EXCEPT
IN
PRINTING
LOG
MESSAGES
BEFORE
VERSION
OF
THE
LINUX
KERNEL
THERE
WAS
NOT
A
DEV
PARAMETER
AND
THUS
IRQ
WAS
USED
TO
DIFFERENTIATE
BETWEEN
MULTIPLE
DEVICES
USING
THE
SAME
DRIVER
AND
THEREFORE
THE
SAME
INTERRUPT
HANDLER
AS
AN
EXAMPLE
OF
THIS
CONSIDER
A
COMPUTER
WITH
MULTIPLE
HARD
DRIVE
CONTROLLERS
OF
THE
SAME
TYPE
THE
SECOND
PARAMETER
DEV
IS
A
GENERIC
POINTER
TO
THE
SAME
DEV
THAT
WAS
GIVEN
TO
WHEN
THE
INTERRUPT
HANDLER
WAS
REGISTERED
IF
THIS
VALUE
IS
UNIQUE
WHICH
IS
REQUIRED
TO
SUPPORT
SHARING
IT
CAN
ACT
AS
A
COOKIE
TO
DIFFERENTIATE
BETWEEN
MULTIPLE
DEVICES
POTENTIALLY
USING
THE
SAME
INTERRUPT
HANDLER
DEV
MIGHT
ALSO
POINT
TO
A
STRUCTURE
OF
USE
TO
THE
INTERRUPT
HANDLER
BECAUSE
THE
DEVICE
STRUCTURE
IS
BOTH
UNIQUE
TO
EACH
DEVICE
AND
POTENTIALLY
USEFUL
TO
HAVE
WITHIN
THE
HANDLER
IT
IS
TYPICALLY
PASSED
FOR
DEV
THE
RETURN
VALUE
OF
AN
INTERRUPT
HANDLER
IS
THE
SPECIAL
TYPE
AN
INTERRUPT
HANDLER
CAN
RETURN
TWO
SPECIAL
VALUES
OR
THE
FORMER
IS
RETURNED
WHEN
THE
INTERRUPT
HANDLER
DETECTS
AN
INTERRUPT
FOR
WHICH
ITS
DEVICE
WAS
NOT
THE
ORIGINA
TOR
THE
LATTER
IS
RETURNED
IF
THE
INTERRUPT
HANDLER
WAS
CORRECTLY
INVOKED
AND
ITS
DEVICE
DID
INDEED
CAUSE
THE
INTERRUPT
ALTERNATIVELY
VAL
MAY
BE
USED
IF
VAL
IS
NONZERO
THIS
MACRO
RETURNS
OTHERWISE
THE
MACRO
RETURNS
THESE
SPECIAL
VALUES
ARE
USED
TO
LET
THE
KERNEL
KNOW
WHETHER
DEVICES
ARE
ISSUING
SPURIOUS
THAT
IS
UNREQUESTED
INTERRUPTS
IF
ALL
THE
INTERRUPT
HANDLERS
ON
A
GIVEN
INTERRUPT
LINE
RETURN
THEN
THE
KERNEL
CAN
DETECT
THE
PROBLEM
NOTE
THE
CURIOUS
RETURN
TYPE
WHICH
IS
SIMPLY
AN
INT
THIS
VALUE
PROVIDES
BACKWARD
COMPATIBILITY
WITH
EARLIER
KERNELS
WHICH
DID
NOT
HAVE
THIS
FEATURE
BEFORE
INTERRUPT
HANDLERS
RETURNED
VOID
DRIVERS
MAY
SIMPLY
TYPEDEF
TO
VOID
AND
DEFINE
THE
DIFFERENT
RETURN
VALUES
TO
NO
OPS
AND
THEN
WORK
IN
WITHOUT
FURTHER
MODIFICATION
THE
INTERRUPT
HAN
DLER
IS
NORMALLY
MARKED
STATIC
BECAUSE
IT
IS
NEVER
CALLED
DIRECTLY
FROM
ANOTHER
FILE
THE
ROLE
OF
THE
INTERRUPT
HANDLER
DEPENDS
ENTIRELY
ON
THE
DEVICE
AND
ITS
REASONS
FOR
ISSUING
THE
INTERRUPT
AT
A
MINIMUM
MOST
INTERRUPT
HANDLERS
NEED
TO
PROVIDE
ACKNOWL
EDGMENT
TO
THE
DEVICE
THAT
THEY
RECEIVED
THE
INTERRUPT
DEVICES
THAT
ARE
MORE
COMPLEX
NEED
TO
ADDITIONALLY
SEND
AND
RECEIVE
DATA
AND
PERFORM
EXTENDED
WORK
IN
THE
INTERRUPT
HANDLER
AS
MENTIONED
THE
EXTENDED
WORK
IS
PUSHED
AS
MUCH
AS
POSSIBLE
INTO
THE
BOTTOM
HALF
HANDLER
WHICH
IS
DISCUSSED
IN
THE
NEXT
CHAPTER
REENTRANCY
AND
INTERRUPT
HANDLERS
INTERRUPT
HANDLERS
IN
LINUX
NEED
NOT
BE
REENTRANT
WHEN
A
GIVEN
INTERRUPT
HANDLER
IS
EXECUT
ING
THE
CORRESPONDING
INTERRUPT
LINE
IS
MASKED
OUT
ON
ALL
PROCESSORS
PREVENTING
ANOTHER
INTERRUPT
ON
THE
SAME
LINE
FROM
BEING
RECEIVED
NORMALLY
ALL
OTHER
INTERRUPTS
ARE
ENABLED
SO
OTHER
INTERRUPTS
ARE
SERVICED
BUT
THE
CURRENT
LINE
IS
ALWAYS
DISABLED
CONSEQUENTLY
THE
SAME
INTERRUPT
HANDLER
IS
NEVER
INVOKED
CONCURRENTLY
TO
SERVICE
A
NESTED
INTERRUPT
THIS
GREATLY
SIMPLIFIES
WRITING
YOUR
INTERRUPT
HANDLER
SHARED
HANDLERS
A
SHARED
HANDLER
IS
REGISTERED
AND
EXECUTED
MUCH
LIKE
A
NONSHARED
HANDLER
FOLLOWING
ARE
THREE
MAIN
DIFFERENCES
N
THE
FLAG
MUST
BE
SET
IN
THE
FLAGS
ARGUMENT
TO
N
THE
DEV
ARGUMENT
MUST
BE
UNIQUE
TO
EACH
REGISTERED
HANDLER
A
POINTER
TO
ANY
PER
DEVICE
STRUCTURE
IS
SUFFICIENT
A
COMMON
CHOICE
IS
THE
DEVICE
STRUCTURE
AS
IT
IS
BOTH
UNIQUE
AND
POTENTIALLY
USEFUL
TO
THE
HANDLER
YOU
CANNOT
PASS
NULL
FOR
A
SHARED
HANDLER
N
THE
INTERRUPT
HANDLER
MUST
BE
CAPABLE
OF
DISTINGUISHING
WHETHER
ITS
DEVICE
ACTUALLY
GENERATED
AN
INTERRUPT
THIS
REQUIRES
BOTH
HARDWARE
SUPPORT
AND
ASSOCIATED
LOGIC
IN
THE
INTERRUPT
HANDLER
IF
THE
HARDWARE
DID
NOT
OFFER
THIS
CAPABILITY
THERE
WOULD
BE
NO
WAY
FOR
THE
INTERRUPT
HANDLER
TO
KNOW
WHETHER
ITS
ASSOCIATED
DEVICE
OR
SOME
OTHER
DEVICE
SHARING
THE
LINE
CAUSED
THE
INTERRUPT
ALL
DRIVERS
SHARING
THE
INTERRUPT
LINE
MUST
MEET
THE
PREVIOUS
REQUIREMENTS
IF
ANY
ONE
DEVICE
DOES
NOT
SHARE
FAIRLY
NONE
CAN
SHARE
THE
LINE
WHEN
IS
CALLED
WITH
SPECIFIED
THE
CALL
SUCCEEDS
ONLY
IF
THE
INTERRUPT
LINE
IS
CURRENTLY
NOT
REGIS
TERED
OR
IF
ALL
REGISTERED
HANDLERS
ON
THE
LINE
ALSO
SPECIFIED
SHARED
HANDLERS
HOWEVER
CAN
MIX
USAGE
OF
WHEN
THE
KERNEL
RECEIVES
AN
INTERRUPT
IT
INVOKES
SEQUENTIALLY
EACH
REGISTERED
HANDLER
ON
THE
LINE
THEREFORE
IT
IS
IMPORTANT
THAT
THE
HANDLER
BE
CAPABLE
OF
DISTINGUISHING
WHETHER
IT
GENERATED
A
GIVEN
INTERRUPT
THE
HANDLER
MUST
QUICKLY
EXIT
IF
ITS
ASSOCIATED
DEVICE
DID
NOT
GENERATE
THE
INTERRUPT
THIS
REQUIRES
THE
HARDWARE
DEVICE
TO
HAVE
A
STATUS
REGISTER
OR
SIMILAR
MECHANISM
THAT
THE
HANDLER
CAN
CHECK
MOST
HARDWARE
DOES
INDEED
HAVE
SUCH
A
FEATURE
A
REAL
LIFE
INTERRUPT
HANDLER
LET
LOOK
AT
A
REAL
INTERRUPT
HANDLER
FROM
THE
REAL
TIME
CLOCK
RTC
DRIVER
FOUND
IN
DRIVERS
CHAR
RTC
C
AN
RTC
IS
FOUND
IN
MANY
MACHINES
INCLUDING
PCS
IT
IS
A
DEVICE
SEPARATE
FROM
THE
SYSTEM
TIMER
WHICH
SETS
THE
SYSTEM
CLOCK
PROVIDES
AN
ALARM
OR
SUPPLIES
A
PERIODIC
TIMER
ON
MOST
ARCHITECTURES
THE
SYSTEM
CLOCK
IS
SET
BY
WRITING
THE
DESIRED
TIME
INTO
A
SPECIFIC
REGISTER
OR
I
O
RANGE
ANY
ALARM
OR
PERIODIC
TIMER
FUNCTIONALITY
IS
NORMALLY
IMPLEMENTED
VIA
INTERRUPT
THE
INTERRUPT
IS
EQUIVALENT
TO
A
REAL
WORLD
CLOCK
ALARM
THE
RECEIPT
OF
THE
INTERRUPT
IS
ANALOGOUS
TO
A
BUZZING
ALARM
WHEN
THE
RTC
DRIVER
LOADS
THE
FUNCTION
IS
INVOKED
TO
INITIALIZE
THE
DRIVER
ONE
OF
ITS
DUTIES
IS
TO
REGISTER
THE
INTERRUPT
HANDLER
REGISTER
ON
IF
RTC
VOID
PRINTK
RTC
CANNOT
REGISTER
IRQ
D
N
RETURN
EIO
IN
THIS
EXAMPLE
THE
INTERRUPT
LINE
IS
STORED
IN
THIS
VARIABLE
IS
SET
TO
THE
RTC
INTERRUPT
FOR
A
GIVEN
ARCHITECTURE
ON
A
PC
THE
RTC
IS
LOCATED
AT
IRQ
THE
SECOND
PARAMETER
IS
THE
INTERRUPT
HANDLER
WHICH
IS
WILLING
TO
SHARE
THE
INTERRUPT
LINE
WITH
OTHER
HANDLERS
THANKS
TO
THE
FLAG
FROM
THE
FOURTH
PARAMETER
YOU
CAN
SEE
THAT
THE
DRIVER
NAME
IS
RTC
BECAUSE
THIS
DEVICE
SHARES
THE
INTERRUPT
LINE
IT
PASSES
A
UNIQUE
PER
DEVICE
VALUE
FOR
DEV
FINALLY
THE
HANDLER
ITSELF
STATIC
INT
IRQ
VOID
DEV
CAN
BE
AN
ALARM
INTERRUPT
UPDATE
COMPLETE
INTERRUPT
OR
A
PERIODIC
INTERRUPT
WE
STORE
THE
STATUS
IN
THE
LOW
BYTE
AND
THE
NUMBER
OF
INTERRUPTS
RECEIVED
SINCE
THE
LAST
READ
IN
THE
REMAINDER
OF
IF
JIFFIES
HZ
HZ
NOW
DO
THE
REST
OF
THE
ACTIONS
IF
FUNC
SIGIO
RETURN
THIS
FUNCTION
IS
INVOKED
WHENEVER
THE
MACHINE
RECEIVES
THE
RTC
INTERRUPT
FIRST
NOTE
THE
SPIN
LOCK
CALLS
THE
FIRST
SET
ENSURES
THAT
IS
NOT
ACCESSED
CONCURRENTLY
BY
ANOTHER
PROCESSOR
ON
AN
SMP
MACHINE
AND
THE
SECOND
SET
PROTECTS
FROM
THE
SAME
LOCKS
ARE
DISCUSSED
IN
CHAPTER
KERNEL
SYNCHRONIZATION
METHODS
THE
VARIABLE
IS
AN
UNSIGNED
LONG
THAT
STORES
INFORMATION
ABOUT
THE
RTC
AND
IS
UPDATED
ON
EACH
INTERRUPT
TO
REFLECT
THE
STATUS
OF
THE
INTERRUPT
NEXT
IF
AN
RTC
PERIODIC
TIMER
IS
SET
IT
IS
UPDATED
VIA
TIMERS
ARE
DIS
CUSSED
IN
CHAPTER
TIMERS
AND
TIME
MANAGEMENT
THE
FINAL
BUNCH
OF
CODE
UNDER
THE
COMMENT
NOW
DO
THE
REST
OF
THE
ACTIONS
EXECUTES
A
POSSIBLE
PRESET
CALLBACK
FUNCTION
THE
RTC
DRIVER
ENABLES
A
CALLBACK
FUNCTION
TO
BE
REG
ISTERED
AND
EXECUTED
ON
EACH
RTC
INTERRUPT
FINALLY
THIS
FUNCTION
RETURNS
TO
SIGNIFY
THAT
IT
PROPERLY
HANDLED
THIS
DEVICE
BECAUSE
THE
INTERRUPT
HANDLER
DOES
NOT
SUPPORT
SHARING
AND
THERE
IS
NO
MECHA
NISM
FOR
THE
RTC
TO
DETECT
A
SPURIOUS
INTERRUPT
THIS
HANDLER
ALWAYS
RETURNS
INTERRUPT
CONTEXT
WHEN
EXECUTING
AN
INTERRUPT
HANDLER
THE
KERNEL
IS
IN
INTERRUPT
CONTEXT
RECALL
THAT
PROCESS
CONTEXT
IS
THE
MODE
OF
OPERATION
THE
KERNEL
IS
IN
WHILE
IT
IS
EXECUTING
ON
BEHALF
OF
A
PROCESS
FOR
EXAMPLE
EXECUTING
A
SYSTEM
CALL
OR
RUNNING
A
KERNEL
THREAD
IN
PROCESS
CON
TEXT
THE
CURRENT
MACRO
POINTS
TO
THE
ASSOCIATED
TASK
FURTHERMORE
BECAUSE
A
PROCESS
IS
COUPLED
TO
THE
KERNEL
IN
PROCESS
CONTEXT
PROCESS
CONTEXT
CAN
SLEEP
OR
OTHERWISE
INVOKE
THE
SCHEDULER
INTERRUPT
CONTEXT
ON
THE
OTHER
HAND
IS
NOT
ASSOCIATED
WITH
A
PROCESS
THE
CURRENT
MACRO
IS
NOT
RELEVANT
ALTHOUGH
IT
POINTS
TO
THE
INTERRUPTED
PROCESS
WITHOUT
A
BACKING
PROCESS
INTERRUPT
CONTEXT
CANNOT
SLEEP
HOW
WOULD
IT
EVER
RESCHEDULE
THEREFORE
YOU
CANNOT
CALL
CERTAIN
FUNCTIONS
FROM
INTERRUPT
CONTEXT
IF
A
FUNCTION
SLEEPS
YOU
CANNOT
USE
IT
FROM
YOUR
INTERRUPT
HANDLER
THIS
LIMITS
THE
FUNCTIONS
THAT
ONE
CAN
CALL
FROM
AN
INTERRUPT
HANDLER
INTERRUPT
CONTEXT
IS
TIME
CRITICAL
BECAUSE
THE
INTERRUPT
HANDLER
INTERRUPTS
OTHER
CODE
CODE
SHOULD
BE
QUICK
AND
SIMPLE
BUSY
LOOPING
IS
POSSIBLE
BUT
DISCOURAGED
THIS
IS
AN
IMPORTANT
POINT
ALWAYS
KEEP
IN
MIND
THAT
YOUR
INTERRUPT
HANDLER
HAS
INTERRUPTED
OTHER
CODE
POSSIBLY
EVEN
ANOTHER
INTERRUPT
HANDLER
ON
A
DIFFERENT
LINE
BECAUSE
OF
THIS
ASYN
CHRONOUS
NATURE
IT
IS
IMPERATIVE
THAT
ALL
INTERRUPT
HANDLERS
BE
AS
QUICK
AND
AS
SIMPLE
AS
POSSIBLE
AS
MUCH
AS
POSSIBLE
WORK
SHOULD
BE
PUSHED
OUT
FROM
THE
INTERRUPT
HANDLER
AND
PERFORMED
IN
A
BOTTOM
HALF
WHICH
RUNS
AT
A
MORE
CONVENIENT
TIME
THE
SETUP
OF
AN
INTERRUPT
HANDLER
STACKS
IS
A
CONFIGURATION
OPTION
HISTORICALLY
INTER
RUPT
HANDLERS
DID
NOT
RECEIVE
THEIR
OWN
STACKS
INSTEAD
THEY
WOULD
SHARE
THE
STACK
OF
THE
PROCESS
THAT
THEY
INTERRUPTED
THE
KERNEL
STACK
IS
TWO
PAGES
IN
SIZE
TYPICALLY
THAT
IS
ON
BIT
ARCHITECTURES
AND
ON
BIT
ARCHITECTURES
BECAUSE
IN
THIS
SETUP
INTERRUPT
HANDLERS
SHARE
THE
STACK
THEY
MUST
BE
EXCEPTIONALLY
FRUGAL
WITH
WHAT
DATA
THEY
ALLOCATE
THERE
OF
COURSE
THE
KERNEL
STACK
IS
LIMITED
TO
BEGIN
WITH
SO
ALL
KERNEL
CODE
SHOULD
BE
CAUTIOUS
EARLY
IN
THE
KERNEL
PROCESS
AN
OPTION
WAS
ADDED
TO
REDUCE
THE
STACK
SIZE
FROM
TWO
PAGES
DOWN
TO
ONE
PROVIDING
ONLY
A
STACK
ON
BIT
SYSTEMS
THIS
REDUCED
MEMORY
PRESSURE
BECAUSE
EVERY
PROCESS
ON
THE
SYSTEM
PREVIOUSLY
NEEDED
TWO
PAGES
OF
CONTIGUOUS
NONSWAPPABLE
KERNEL
MEMORY
TO
COPE
WITH
THE
REDUCED
STACK
SIZE
INTERRUPT
HANDLERS
WERE
GIVEN
THEIR
OWN
STACK
ONE
STACK
PER
PROCESSOR
ONE
PAGE
IN
SIZE
THIS
STACK
IS
REFERRED
TO
AS
THE
INTERRUPT
STACK
ALTHOUGH
THE
TOTAL
SIZE
OF
THE
INTERRUPT
STACK
IS
HALF
THAT
OF
THE
ORIGINAL
SHARED
STACK
THE
AVERAGE
STACK
SPACE
AVAILABLE
IS
GREATER
BECAUSE
INTERRUPT
HANDLERS
GET
THE
FULL
PAGE
OF
MEMORY
TO
THEMSELVES
A
PROCESS
IS
ALWAYS
RUNNING
WHEN
NOTHING
ELSE
IS
SCHEDULABLE
THE
IDLE
TASK
RUNS
YOUR
INTERRUPT
HANDLER
SHOULD
NOT
CARE
WHAT
STACK
SETUP
IS
IN
USE
OR
WHAT
THE
SIZE
OF
THE
KERNEL
STACK
IS
ALWAYS
USE
AN
ABSOLUTE
MINIMUM
AMOUNT
OF
STACK
SPACE
IMPLEMENTING
INTERRUPT
HANDLERS
PERHAPS
NOT
SURPRISING
THE
IMPLEMENTATION
OF
THE
INTERRUPT
HANDLING
SYSTEM
IN
LINUX
IS
ARCHITECTURE
DEPENDENT
THE
IMPLEMENTATION
DEPENDS
ON
THE
PROCESSOR
THE
TYPE
OF
INTER
RUPT
CONTROLLER
USED
AND
THE
DESIGN
OF
THE
ARCHITECTURE
AND
MACHINE
FIGURE
IS
A
DIAGRAM
OF
THE
PATH
AN
INTERRUPT
TAKES
THROUGH
HARDWARE
AND
THE
KERNEL
HARDWARE
GENERATES
AN
INTERRUPT
YES
INTERRUPT
CONTROLLER
PROCESSOR
INTERRUPTS
THE
KERNEL
IS
THERE
AN
INTERRUPT
HANDLER
ON
THIS
LINE
NO
RUN
ALL
INTERRUPT
HANDLERS
ON
THIS
LINE
RETURN
TO
THE
KERNEL
CODE
THAT
WAS
INTERRUPTED
PROCESSOR
FIGURE
THE
PATH
THAT
AN
INTERRUPT
TAKES
FROM
HARDWARE
AND
ON
THROUGH
THE
KERNEL
A
DEVICE
ISSUES
AN
INTERRUPT
BY
SENDING
AN
ELECTRIC
SIGNAL
OVER
ITS
BUS
TO
THE
INTERRUPT
CONTROLLER
IF
THE
INTERRUPT
LINE
IS
ENABLED
THEY
CAN
BE
MASKED
OUT
THE
INTERRUPT
CON
TROLLER
SENDS
THE
INTERRUPT
TO
THE
PROCESSOR
IN
MOST
ARCHITECTURES
THIS
IS
ACCOMPLISHED
BY
AN
ELECTRICAL
SIGNAL
SENT
OVER
A
SPECIAL
PIN
TO
THE
PROCESSOR
UNLESS
INTERRUPTS
ARE
DISABLED
IN
THE
PROCESSOR
WHICH
CAN
ALSO
HAPPEN
THE
PROCESSOR
IMMEDIATELY
STOPS
WHAT
IT
IS
DOING
DISABLES
THE
INTERRUPT
SYSTEM
AND
JUMPS
TO
A
PREDEFINED
LOCATION
IN
MEMORY
AND
EXECUTES
THE
CODE
LOCATED
THERE
THIS
PREDEFINED
POINT
IS
SET
UP
BY
THE
KERNEL
AND
IS
THE
ENTRY
POINT
FOR
INTERRUPT
HANDLERS
THE
INTERRUPT
JOURNEY
IN
THE
KERNEL
BEGINS
AT
THIS
PREDEFINED
ENTRY
POINT
JUST
AS
SYSTEM
CALLS
ENTER
THE
KERNEL
THROUGH
A
PREDEFINED
EXCEPTION
HANDLER
FOR
EACH
INTERRUPT
LINE
THE
PROCESSOR
JUMPS
TO
A
UNIQUE
LOCATION
IN
MEMORY
AND
EXECUTES
THE
CODE
LOCATED
THERE
IN
THIS
MANNER
THE
KERNEL
KNOWS
THE
IRQ
NUMBER
OF
THE
INCOMING
INTERRUPT
THE
INITIAL
ENTRY
POINT
SIMPLY
SAVES
THIS
VALUE
AND
STORES
THE
CURRENT
REGISTER
VALUES
WHICH
BELONG
TO
THE
INTERRUPTED
TASK
ON
THE
STACK
THEN
THE
KERNEL
CALLS
FROM
HERE
ONWARD
MOST
OF
THE
INTERRUPT
HANDLING
CODE
IS
WRITTEN
IN
C
HOWEVER
IT
IS
STILL
ARCHITECTURE
DEPENDENT
THE
FUNCTION
IS
DECLARED
AS
UNSIGNED
INT
STRUCT
REGS
BECAUSE
THE
C
CALLING
CONVENTION
PLACES
FUNCTION
ARGUMENTS
AT
THE
TOP
OF
THE
STACK
THE
STRUCTURE
CONTAINS
THE
INITIAL
REGISTER
VALUES
THAT
WERE
PREVIOUSLY
SAVED
IN
THE
ASSEMBLY
ENTRY
ROUTINE
BECAUSE
THE
INTERRUPT
VALUE
WAS
ALSO
SAVED
CAN
EXTRACT
IT
AFTER
THE
INTERRUPT
LINE
IS
CALCULATED
ACKNOWLEDGES
THE
RECEIPT
OF
THE
INTER
RUPT
AND
DISABLES
INTERRUPT
DELIVERY
ON
THE
LINE
ON
NORMAL
PC
MACHINES
THESE
OPERATIONS
ARE
HANDLED
BY
NEXT
ENSURES
THAT
A
VALID
HANDLER
IS
REGISTERED
ON
THE
LINE
AND
THAT
IT
IS
ENABLED
AND
NOT
CURRENTLY
EXECUTING
IF
SO
IT
CALLS
DEFINED
IN
KERNEL
IRQ
HANDLER
C
TO
RUN
THE
INSTALLED
INTERRUPT
HANDLERS
FOR
THE
LINE
IRQ
ACTION
CHAIN
HANDLER
IRQ
THE
INTERRUPT
NUMBER
ACTION
THE
INTERRUPT
ACTION
CHAIN
FOR
THIS
IRQ
HANDLES
THE
ACTION
CHAIN
OF
AN
IRQ
EVENT
UNSIGNED
INT
IRQ
STRUCT
IRQACTION
ACTION
RET
RETVAL
UNSIGNED
INT
STATUS
IF
ACTION
FLAGS
DO
IRQ
ACTION
RET
ACTION
HANDLER
IRQ
ACTION
IRQ
ACTION
RET
SWITCH
RET
CASE
SET
RESULT
TO
HANDLED
SO
THE
SPURIOUS
CHECK
DOES
NOT
TRIGGER
RET
CATCH
DRIVERS
WHICH
RETURN
BUT
DID
NOT
SET
UP
A
THREAD
FUNCTION
IF
UNLIKELY
ACTION
IRQ
ACTION
BREAK
WAKE
UP
THE
HANDLER
THREAD
FOR
THIS
ACTION
IN
CASE
THE
THREAD
CRASHED
AND
WAS
KILLED
WE
JUST
PRETEND
THAT
WE
HANDLED
THE
INTERRUPT
THE
HARDIRQ
HANDLER
ABOVE
HAS
DISABLED
THE
DEVICE
INTERRUPT
SO
NO
IRQ
STORM
IS
LURKING
IF
LIKELY
ACTION
ACTION
ACTION
THREAD
FALL
THROUGH
TO
ADD
TO
RANDOMNESS
CASE
STATUS
ACTION
FLAGS
BREAK
DEFAULT
BREAK
RETVAL
RET
ACTION
ACTION
NEXT
WHILE
ACTION
IF
STATUS
IRQ
RETURN
RETVAL
FIRST
BECAUSE
THE
PROCESSOR
DISABLED
INTERRUPTS
THEY
ARE
TURNED
BACK
ON
UNLESS
WAS
SPECIFIED
DURING
THE
HANDLER
REGISTRATION
RECALL
THAT
SPECIFIES
THAT
THE
HANDLER
MUST
BE
RUN
WITH
INTERRUPTS
DISABLED
NEXT
EACH
POTENTIAL
HANDLER
IS
EXECUTED
IN
A
LOOP
IF
THIS
LINE
IS
NOT
SHARED
THE
LOOP
TERMINATES
AFTER
THE
FIRST
ITERATION
OTHERWISE
ALL
HANDLERS
ARE
EXECUTED
AFTER
THAT
IS
CALLED
IF
WAS
SPECIFIED
DURING
REGISTRATION
THIS
FUNCTION
USES
THE
TIMING
OF
THE
INTERRUPT
TO
GENERATE
ENTROPY
FOR
THE
RAN
DOM
NUMBER
GENERATOR
FINALLY
INTERRUPTS
ARE
AGAIN
DISABLED
EXPECTS
THEM
STILL
TO
BE
OFF
AND
THE
FUNCTION
RETURNS
BACK
IN
THE
FUNCTION
CLEANS
UP
AND
RETURNS
TO
THE
INITIAL
ENTRY
POINT
WHICH
THEN
JUMPS
TO
THE
ROUTINE
IS
AS
WITH
THE
INITIAL
ENTRY
CODE
WRITTEN
IN
ASSEMBLY
THIS
ROUTINE
CHECKS
WHETHER
A
RESCHEDULE
IS
PENDING
RECALL
FROM
CHAPTER
PROCESS
SCHEDULING
THAT
THIS
IMPLIES
THAT
IS
SET
IF
A
RESCHEDULE
IS
PENDING
AND
THE
KERNEL
IS
RETURNING
TO
USER
SPACE
THAT
IS
THE
INTERRUPT
INTERRUPTED
A
USER
PROCESS
SCHEDULE
IS
CALLED
IF
THE
KERNEL
IS
RETURNING
TO
KERNEL
SPACE
THAT
IS
THE
INTERRUPT
INTER
RUPTED
THE
KERNEL
ITSELF
SCHEDULE
IS
CALLED
ONLY
IF
THE
IS
ZERO
OTHER
WISE
IT
IS
NOT
SAFE
TO
PREEMPT
THE
KERNEL
AFTER
SCHEDULE
RETURNS
OR
IF
THERE
IS
NO
WORK
PENDING
THE
INITIAL
REGISTERS
ARE
RESTORED
AND
THE
KERNEL
RESUMES
WHATEVER
WAS
INTERRUPTED
ON
THE
INITIAL
ASSEMBLY
ROUTINES
ARE
LOCATED
IN
ARCH
KERNEL
FOR
BIT
AND
THE
C
METHODS
ARE
LOCATED
IN
ARCH
KERNEL
IRQ
C
OTHER
SUPPORTED
ARCHITECTURES
ARE
SIMILAR
PROC
INTERRUPTS
PROCFS
IS
A
VIRTUAL
FILESYSTEM
THAT
EXISTS
ONLY
IN
KERNEL
MEMORY
AND
IS
TYPICALLY
MOUNTED
AT
PROC
READING
OR
WRITING
FILES
IN
PROCFS
INVOKES
KERNEL
FUNCTIONS
THAT
SIMULATE
READING
OR
WRITING
FROM
A
REAL
FILE
A
RELEVANT
EXAMPLE
IS
THE
PROC
INTERRUPTS
FILE
WHICH
IS
POPU
LATED
WITH
STATISTICS
RELATED
TO
INTERRUPTS
ON
THE
SYSTEM
HERE
IS
SAMPLE
OUTPUT
FROM
A
UNIPROCESSOR
PC
XT
PIC
TIMER
XT
PIC
XT
PIC
CASCADE
XT
PIC
UHCI
HCD
XT
PIC
XT
PIC
UHCI
HCD
XT
PIC
NMI
LOC
ERR
THE
FIRST
COLUMN
IS
THE
INTERRUPT
LINE
ON
THIS
SYSTEM
INTERRUPTS
NUMBERED
AND
ARE
PRESENT
HANDLERS
ARE
NOT
INSTALLED
ON
LINES
NOT
DISPLAYED
THE
SECOND
COLUMN
IS
A
COUNTER
OF
THE
NUMBER
OF
INTERRUPTS
RECEIVED
A
COLUMN
IS
PRESENT
FOR
EACH
PROCESSOR
ON
THE
SYSTEM
BUT
THIS
MACHINE
HAS
ONLY
ONE
PROCESSOR
AS
YOU
CAN
SEE
THE
TIMER
INTERRUPT
HAS
RECEIVED
INTERRUPTS
WHEREAS
THE
SOUND
CARD
HAS
RECEIVED
NONE
WHICH
IS
AN
INDICATION
THAT
IT
HAS
NOT
BEEN
USED
SINCE
THE
MACHINE
BOOTED
THE
THIRD
COL
UMN
IS
THE
INTERRUPT
CONTROLLER
HANDLING
THIS
INTERRUPT
XT
PIC
CORRESPONDS
TO
THE
STANDARD
AS
AN
EXERCISE
AFTER
READING
CHAPTER
CAN
YOU
TELL
HOW
LONG
THE
SYSTEM
HAS
BEEN
UP
IN
TERMS
OF
HZ
KNOWING
THE
NUMBER
OF
TIMER
INTERRUPTS
THAT
HAVE
OCCURRED
PC
PROGRAMMABLE
INTERRUPT
CONTROLLER
ON
SYSTEMS
WITH
AN
I
O
APIC
MOST
INTERRUPTS
WOULD
LIST
IO
APIC
LEVEL
OR
IO
APIC
EDGE
AS
THEIR
INTERRUPT
CONTROLLER
FINALLY
THE
LAST
COLUMN
IS
THE
DEVICE
ASSOCIATED
WITH
THIS
INTERRUPT
THIS
NAME
IS
SUPPLIED
BY
THE
DEVNAME
PARAMETER
TO
AS
DISCUSSED
PREVIOUSLY
IF
THE
INTERRUPT
IS
SHARED
AS
IS
THE
CASE
WITH
INTERRUPT
NUMBER
IN
THIS
EXAMPLE
ALL
THE
DEVICES
REGISTERED
ON
THE
INTERRUPT
LINE
ARE
LISTED
FOR
THE
CURIOUS
PROCFS
CODE
IS
LOCATED
PRIMARILY
IN
FS
PROC
THE
FUNCTION
THAT
PROVIDES
PROC
INTERRUPTS
IS
NOT
SURPRISINGLY
ARCHITECTURE
DEPENDENT
AND
NAMED
INTERRUPT
CONTROL
THE
LINUX
KERNEL
IMPLEMENTS
A
FAMILY
OF
INTERFACES
FOR
MANIPULATING
THE
STATE
OF
INTER
RUPTS
ON
A
MACHINE
THESE
INTERFACES
ENABLE
YOU
TO
DISABLE
THE
INTERRUPT
SYSTEM
FOR
THE
CURRENT
PROCESSOR
OR
MASK
OUT
AN
INTERRUPT
LINE
FOR
THE
ENTIRE
MACHINE
THESE
ROUTINES
ARE
ALL
ARCHITECTURE
DEPENDENT
AND
CAN
BE
FOUND
IN
ASM
SYSTEM
H
AND
ASM
IRQ
H
SEE
TABLE
LATER
IN
THIS
CHAPTER
FOR
A
COMPLETE
LISTING
OF
THE
INTERFACES
REASONS
TO
CONTROL
THE
INTERRUPT
SYSTEM
GENERALLY
BOIL
DOWN
TO
NEEDING
TO
PROVIDE
SYNCHRONIZATION
BY
DISABLING
INTERRUPTS
YOU
CAN
GUARANTEE
THAT
AN
INTERRUPT
HANDLER
WILL
NOT
PREEMPT
YOUR
CURRENT
CODE
MOREOVER
DISABLING
INTERRUPTS
ALSO
DISABLES
KERNEL
PRE
EMPTION
NEITHER
DISABLING
INTERRUPT
DELIVERY
NOR
DISABLING
KERNEL
PREEMPTION
PROVIDES
ANY
PROTECTION
FROM
CONCURRENT
ACCESS
FROM
ANOTHER
PROCESSOR
HOWEVER
BECAUSE
LINUX
SUPPORTS
MULTIPLE
PROCESSORS
KERNEL
CODE
MORE
GENERALLY
NEEDS
TO
OBTAIN
SOME
SORT
OF
LOCK
TO
PREVENT
ANOTHER
PROCESSOR
FROM
ACCESSING
SHARED
DATA
SIMULTANEOUSLY
THESE
LOCKS
ARE
OFTEN
OBTAINED
IN
CONJUNCTION
WITH
DISABLING
LOCAL
INTERRUPTS
THE
LOCK
PROVIDES
PRO
TECTION
AGAINST
CONCURRENT
ACCESS
FROM
ANOTHER
PROCESSOR
WHEREAS
DISABLING
INTERRUPTS
PROVIDES
PROTECTION
AGAINST
CONCURRENT
ACCESS
FROM
A
POSSIBLE
INTERRUPT
HANDLER
CHAPTERS
AND
DISCUSS
THE
VARIOUS
PROBLEMS
OF
SYNCHRONIZATION
AND
THEIR
SOLUTIONS
NEVERTHE
LESS
UNDERSTANDING
THE
KERNEL
INTERRUPT
CONTROL
INTERFACES
IS
IMPORTANT
DISABLING
AND
ENABLING
INTERRUPTS
TO
DISABLE
INTERRUPTS
LOCALLY
FOR
THE
CURRENT
PROCESSOR
AND
ONLY
THE
CURRENT
PROCESSOR
AND
THEN
LATER
REENABLE
THEM
DO
THE
FOLLOWING
INTERRUPTS
ARE
DISABLED
THESE
FUNCTIONS
ARE
USUALLY
IMPLEMENTED
AS
A
SINGLE
ASSEMBLY
OPERATION
OF
COURSE
THIS
DEPENDS
ON
THE
ARCHITECTURE
INDEED
ON
IS
A
SIMPLE
CLI
AND
IS
A
SIMPLE
STI
INSTRUCTION
CLI
AND
STI
ARE
THE
ASSEMBLY
CALLS
TO
CLEAR
AND
SET
THE
ALLOW
INTERRUPTS
FLAG
RESPECTIVELY
IN
OTHER
WORDS
THEY
DISABLE
AND
ENABLE
INTERRUPT
DELIVERY
ON
THE
ISSUING
PROCESSOR
THE
ROUTINE
IS
DANGEROUS
IF
INTERRUPTS
WERE
ALREADY
DISABLED
PRIOR
TO
ITS
INVOCATION
THE
CORRESPONDING
CALL
TO
UNCONDITIONALLY
ENABLES
INTERRUPTS
DESPITE
THE
FACT
THAT
THEY
WERE
OFF
TO
BEGIN
WITH
INSTEAD
A
MECHANISM
IS
NEEDED
TO
RESTORE
INTERRUPTS
TO
A
PREVIOUS
STATE
THIS
IS
A
COMMON
CONCERN
BECAUSE
A
GIVEN
CODE
PATH
IN
THE
KERNEL
CAN
BE
REACHED
BOTH
WITH
AND
WITHOUT
INTERRUPTS
ENABLED
DEPENDING
ON
THE
CALL
CHAIN
FOR
EXAMPLE
IMAGINE
THE
PREVIOUS
CODE
SNIPPET
IS
PART
OF
A
LARGER
FUNCTION
IMAGINE
THAT
THIS
FUNCTION
IS
CALLED
BY
TWO
OTHER
FUNCTIONS
ONE
THAT
DIS
ABLES
INTERRUPTS
AND
ONE
THAT
DOES
NOT
BECAUSE
IT
IS
BECOMING
HARDER
AS
THE
KERNEL
GROWS
IN
SIZE
AND
COMPLEXITY
TO
KNOW
ALL
THE
CODE
PATHS
LEADING
UP
TO
A
FUNCTION
IT
IS
MUCH
SAFER
TO
SAVE
THE
STATE
OF
THE
INTERRUPT
SYSTEM
BEFORE
DISABLING
IT
THEN
WHEN
YOU
ARE
READY
TO
REENABLE
INTERRUPTS
YOU
SIMPLY
RESTORE
THEM
TO
THEIR
ORIGINAL
STATE
UNSIGNED
LONG
FLAGS
FLAGS
INTERRUPTS
ARE
NOW
DISABLED
FLAGS
INTERRUPTS
ARE
RESTORED
TO
THEIR
PREVIOUS
STATE
NOTE
THAT
THESE
METHODS
ARE
IMPLEMENTED
AT
LEAST
IN
PART
AS
MACROS
SO
THE
FLAGS
PARAMETER
WHICH
MUST
BE
DEFINED
AS
AN
UNSIGNED
LONG
IS
SEEMINGLY
PASSED
BY
VALUE
THIS
PARAMETER
CONTAINS
ARCHITECTURE
SPECIFIC
DATA
CONTAINING
THE
STATE
OF
THE
INTERRUPT
SYS
TEMS
BECAUSE
AT
LEAST
ONE
SUPPORTED
ARCHITECTURE
INCORPORATES
STACK
INFORMATION
INTO
THE
VALUE
AHEM
SPARC
FLAGS
CANNOT
BE
PASSED
TO
ANOTHER
FUNCTION
SPECIFICALLY
IT
MUST
REMAIN
ON
THE
SAME
STACK
FRAME
FOR
THIS
REASON
THE
CALL
TO
SAVE
AND
THE
CALL
TO
RESTORE
INTERRUPTS
MUST
OCCUR
IN
THE
SAME
FUNCTION
ALL
THE
PREVIOUS
FUNCTIONS
CAN
BE
CALLED
FROM
BOTH
INTERRUPT
AND
PROCESS
CONTEXT
NO
MORE
GLOBAL
CLI
THE
KERNEL
FORMERLY
PROVIDED
A
METHOD
TO
DISABLE
INTERRUPTS
ON
ALL
PROCESSORS
IN
THE
SYSTEM
FURTHERMORE
IF
ANOTHER
PROCESSOR
CALLED
THIS
METHOD
IT
WOULD
HAVE
TO
WAIT
UNTIL
INTERRUPTS
WERE
ENABLED
BEFORE
CONTINUING
THIS
FUNCTION
WAS
NAMED
CLI
AND
THE
CORRESPONDING
ENABLE
CALL
WAS
NAMED
STI
VERY
CENTRIC
DESPITE
EXISTING
FOR
ALL
ARCHITECTURES
THESE
INTERFACES
WERE
DEPRECATED
DURING
AND
CONSEQUENTLY
ALL
INTERRUPT
SYNCHRONIZATION
MUST
NOW
USE
A
COMBINATION
OF
LOCAL
INTERRUPT
CONTROL
AND
SPIN
LOCKS
DISCUSSED
IN
CHAPTER
AN
INTRODUCTION
TO
KERNEL
SYNCHRONIZATION
THIS
MEANS
THAT
CODE
THAT
PREVIOUSLY
ONLY
HAD
TO
DISABLE
INTERRUPTS
GLOBALLY
TO
ENSURE
MUTUAL
EXCLUSIVE
ACCESS
TO
SHARED
DATA
NOW
NEEDS
TO
DO
A
BIT
MORE
WORK
PREVIOUSLY
DRIVER
WRITERS
COULD
ASSUME
A
CLI
USED
IN
THEIR
INTERRUPT
HANDLERS
AND
ANY
WHERE
ELSE
THE
SHARED
DATA
WAS
ACCESSED
WOULD
PROVIDE
MUTUAL
EXCLUSION
THE
CLI
CALL
WOULD
ENSURE
THAT
NO
OTHER
INTERRUPT
HANDLERS
AND
THUS
THEIR
SPECIFIC
HANDLER
WOULD
RUN
FURTHERMORE
IF
ANOTHER
PROCESSOR
ENTERED
A
CLI
PROTECTED
REGION
IT
WOULD
NOT
CONTINUE
UNTIL
THE
ORIGINAL
PROCESSOR
EXITED
ITS
CLI
PROTECTED
REGION
WITH
A
CALL
TO
STI
REMOVING
THE
GLOBAL
CLI
HAS
A
HANDFUL
OF
ADVANTAGES
FIRST
IT
FORCES
DRIVER
WRITERS
TO
IMPLE
MENT
REAL
LOCKING
A
FINE
GRAINED
LOCK
WITH
A
SPECIFIC
PURPOSE
IS
FASTER
THAN
A
GLOBAL
LOCK
WHICH
IS
EFFECTIVELY
WHAT
CLI
IS
SECOND
THE
REMOVAL
STREAMLINED
A
LOT
OF
CODE
IN
THE
INTERRUPT
SYS
TEM
AND
REMOVED
A
BUNCH
MORE
THE
RESULT
IS
SIMPLER
AND
EASIER
TO
COMPREHEND
DISABLING
A
SPECIFIC
INTERRUPT
LINE
IN
THE
PREVIOUS
SECTION
WE
LOOKED
AT
FUNCTIONS
THAT
DISABLE
ALL
INTERRUPT
DELIVERY
FOR
AN
ENTIRE
PROCESSOR
IN
SOME
CASES
IT
IS
USEFUL
TO
DISABLE
ONLY
A
SPECIFIC
INTERRUPT
LINE
FOR
THE
ENTIRE
SYSTEM
THIS
IS
CALLED
MASKING
OUT
AN
INTERRUPT
LINE
AS
AN
EXAMPLE
YOU
MIGHT
WANT
TO
DISABLE
DELIVERY
OF
A
DEVICE
INTERRUPTS
BEFORE
MANIPULATING
ITS
STATE
LINUX
PROVIDES
FOUR
INTERFACES
FOR
THIS
TASK
VOID
UNSIGNED
INT
IRQ
VOID
UNSIGNED
INT
IRQ
VOID
UNSIGNED
INT
IRQ
VOID
UNSIGNED
INT
IRQ
THE
FIRST
TWO
FUNCTIONS
DISABLE
A
GIVEN
INTERRUPT
LINE
IN
THE
INTERRUPT
CONTROLLER
THIS
DISABLES
DELIVERY
OF
THE
GIVEN
INTERRUPT
TO
ALL
PROCESSORS
IN
THE
SYSTEM
ADDITIONALLY
THE
FUNCTION
DOES
NOT
RETURN
UNTIL
ANY
CURRENTLY
EXECUTING
HANDLER
COMPLETES
THUS
CALLERS
ARE
ASSURED
NOT
ONLY
THAT
NEW
INTERRUPTS
WILL
NOT
BE
DELIVERED
ON
THE
GIVEN
LINE
BUT
ALSO
THAT
ANY
ALREADY
EXECUTING
HANDLERS
HAVE
EXITED
THE
FUNCTION
DOES
NOT
WAIT
FOR
CURRENT
HANDLERS
TO
COMPLETE
THE
FUNCTION
WAITS
FOR
A
SPECIFIC
INTERRUPT
HANDLER
TO
EXIT
IF
IT
IS
EXECUTING
BEFORE
RETURNING
CALLS
TO
THESE
FUNCTIONS
NEST
FOR
EACH
CALL
TO
OR
ON
A
GIVEN
INTERRUPT
LINE
A
CORRESPONDING
CALL
TO
IS
REQUIRED
ONLY
ON
THE
LAST
CALL
TO
IS
THE
INTERRUPT
LINE
ACTUALLY
ENABLED
FOR
EXAMPLE
IF
IS
CALLED
TWICE
THE
INTERRUPT
LINE
IS
NOT
ACTUALLY
REENABLED
UNTIL
THE
SECOND
CALL
TO
ALL
THREE
OF
THESE
FUNCTIONS
CAN
BE
CALLED
FROM
INTERRUPT
OR
PROCESS
CONTEXT
AND
DO
NOT
SLEEP
IF
CALLING
FROM
INTERRUPT
CONTEXT
BE
CAREFUL
YOU
DO
NOT
WANT
FOR
EXAMPLE
TO
ENABLE
AN
INTERRUPT
LINE
WHILE
YOU
ARE
HANDLING
IT
RECALL
THAT
THE
INTERRUPT
LINE
OF
A
HANDLER
IS
MASKED
OUT
WHILE
IT
IS
SERVICED
IT
WOULD
BE
RATHER
RUDE
TO
DISABLE
AN
INTERRUPT
LINE
SHARED
AMONG
MULTIPLE
INTERRUPT
HANDLERS
DISABLING
THE
LINE
DISABLES
INTERRUPT
DELIVERY
FOR
ALL
DEVICES
ON
THE
LINE
THERE
FORE
DRIVERS
FOR
NEWER
DEVICES
TEND
NOT
TO
USE
THESE
INTERFACES
BECAUSE
PCI
DEVICES
HAVE
TO
SUPPORT
INTERRUPT
LINE
SHARING
BY
SPECIFICATION
THEY
SHOULD
NOT
USE
THESE
INTERFACES
AT
ALL
THUS
AND
FRIENDS
ARE
FOUND
MORE
OFTEN
IN
DRIVERS
FOR
OLDER
LEGACY
DEVICES
SUCH
AS
THE
PC
PARALLEL
PORT
MANY
OLDER
DEVICES
PARTICULARLY
ISA
DEVICES
DO
NOT
PROVIDE
A
METHOD
OF
OBTAINING
WHETHER
THEY
GENER
ATED
AN
INTERRUPT
THEREFORE
OFTEN
INTERRUPT
LINES
FOR
ISA
DEVICES
CANNOT
BE
SHARED
BECAUSE
THE
PCI
SPECIFICATION
MANDATES
THE
SHARING
OF
INTERRUPTS
MODERN
PCI
BASED
DEVICES
SUPPORT
INTERRUPT
SHARING
IN
CONTEMPORARY
COMPUTERS
NEARLY
ALL
INTERRUPT
LINES
CAN
BE
SHARED
STATUS
OF
THE
INTERRUPT
SYSTEM
IT
IS
OFTEN
USEFUL
TO
KNOW
THE
STATE
OF
THE
INTERRUPT
SYSTEM
FOR
EXAMPLE
WHETHER
INTER
RUPTS
ARE
ENABLED
OR
DISABLED
OR
WHETHER
YOU
ARE
CURRENTLY
EXECUTING
IN
INTERRUPT
CONTEXT
THE
MACRO
DEFINED
IN
ASM
SYSTEM
H
RETURNS
NONZERO
IF
THE
INTERRUPT
SYSTEM
ON
THE
LOCAL
PROCESSOR
IS
DISABLED
OTHERWISE
IT
RETURNS
ZERO
TWO
MACROS
DEFINED
IN
LINUX
HARDIRQ
H
PROVIDE
AN
INTERFACE
TO
CHECK
THE
KER
NEL
CURRENT
CONTEXT
THEY
ARE
THE
MOST
USEFUL
IS
THE
FIRST
IT
RETURNS
NONZERO
IF
THE
KERNEL
IS
PERFORMING
ANY
TYPE
OF
INTERRUPT
HANDLING
THIS
INCLUDES
EITHER
EXECUTING
AN
INTERRUPT
HANDLER
OR
A
BOTTOM
HALF
HANDLER
THE
MACRO
RETURNS
NONZERO
ONLY
IF
THE
KERNEL
IS
SPECIFICALLY
EXECUTING
AN
INTERRUPT
HANDLER
MORE
OFTEN
YOU
WANT
TO
CHECK
WHETHER
YOU
ARE
IN
PROCESS
CONTEXT
THAT
IS
YOU
WANT
TO
ENSURE
YOU
ARE
NOT
IN
INTERRUPT
CONTEXT
THIS
IS
OFTEN
THE
CASE
BECAUSE
CODE
WANTS
TO
DO
SOMETHING
THAT
CAN
ONLY
BE
DONE
FROM
PROCESS
CONTEXT
SUCH
AS
SLEEP
IF
RETURNS
ZERO
THE
KERNEL
IS
IN
PROCESS
CONTEXT
YES
THE
NAMES
ARE
CONFUSING
AND
DO
LITTLE
TO
IMPART
THEIR
MEANING
TABLE
IS
A
SUM
MARY
OF
THE
INTERRUPT
CONTROL
METHODS
AND
THEIR
DESCRIPTION
TABLE
INTERRUPT
CONTROL
METHODS
FUNCTION
DESCRIPTION
DISABLES
LOCAL
INTERRUPT
DELIVERY
ENABLES
LOCAL
INTERRUPT
DELIVERY
SAVES
THE
CURRENT
STATE
OF
LOCAL
INTERRUPT
DELIVERY
AND
THEN
DISABLES
IT
RESTORES
LOCAL
INTERRUPT
DELIVERY
TO
THE
GIVEN
STATE
DISABLES
THE
GIVEN
INTERRUPT
LINE
AND
ENSURES
NO
HANDLER
ON
THE
LINE
IS
EXECUTING
BEFORE
RETURNING
DISABLES
THE
GIVEN
INTERRUPT
LINE
ENABLES
THE
GIVEN
INTERRUPT
LINE
RETURNS
NONZERO
IF
LOCAL
INTERRUPT
DELIVERY
IS
DISABLED
OTHER
WISE
RETURNS
ZERO
RETURNS
NONZERO
IF
IN
INTERRUPT
CONTEXT
AND
ZERO
IF
IN
PROCESS
CONTEXT
RETURNS
NONZERO
IF
CURRENTLY
EXECUTING
AN
INTERRUPT
HANDLER
AND
ZERO
OTHERWISE
CONCLUSION
CONCLUSION
THIS
CHAPTER
LOOKED
AT
INTERRUPTS
A
HARDWARE
RESOURCE
USED
BY
DEVICES
TO
ASYNCHRO
NOUSLY
SIGNAL
THE
PROCESSOR
INTERRUPTS
IN
EFFECT
ARE
USED
BY
HARDWARE
TO
INTERRUPT
THE
OPERATING
SYSTEM
MOST
MODERN
HARDWARE
USES
INTERRUPTS
TO
COMMUNICATE
WITH
OPERATING
SYSTEMS
THE
DEVICE
DRIVER
THAT
MANAGES
A
GIVEN
PIECE
OF
HARDWARE
REGISTERS
AN
INTERRUPT
HANDLER
TO
RESPOND
TO
AND
PROCESS
INTERRUPTS
ISSUED
FROM
THEIR
ASSOCIATED
HARDWARE
WORK
PERFORMED
IN
INTERRUPTS
INCLUDES
ACKNOWLEDGING
AND
RESETTING
HARDWARE
COPYING
DATA
FROM
THE
DEVICE
TO
MAIN
MEMORY
AND
VICE
VERSA
PROCESSING
HARDWARE
REQUESTS
AND
SENDING
OUT
NEW
HARDWARE
REQUESTS
THE
KERNEL
PROVIDES
INTERFACES
FOR
REGISTERING
AND
UNREGISTERING
INTERRUPT
HANDLERS
DIS
ABLING
INTERRUPTS
MASKING
OUT
INTERRUPT
LINES
AND
CHECKING
THE
STATUS
OF
THE
INTERRUPT
SYS
TEM
TABLE
PROVIDED
AN
OVERVIEW
OF
MANY
OF
THESE
FUNCTIONS
BECAUSE
INTERRUPTS
INTERRUPT
OTHER
EXECUTING
CODE
PROCESSES
THE
KERNEL
ITSELF
AND
EVEN
OTHER
INTERRUPT
HANDLERS
THEY
MUST
EXECUTE
QUICKLY
OFTEN
HOWEVER
THERE
IS
A
LOT
OF
WORK
TO
DO
TO
BALANCE
THE
LARGE
AMOUNT
OF
WORK
WITH
THE
NEED
FOR
QUICK
EXECUTION
THE
KERNEL
DIVIDES
THE
WORK
OF
PROCESSING
INTERRUPTS
INTO
TWO
HALVES
THE
INTERRUPT
HANDLER
THE
TOP
HALF
WAS
DISCUSSED
IN
THIS
CHAPTER
THE
NEXT
CHAPTER
LOOKS
AT
THE
BOTTOM
HALF
BOTTOM
HALVES
AND
DEFERRING
WORK
THE
PREVIOUS
CHAPTER
DISCUSSED
INTERRUPT
HANDLERS
THE
KERNEL
MECHANISM
FOR
DEALING
WITH
HARDWARE
INTERRUPTS
INTERRUPT
HANDLERS
ARE
AN
IMPORTANT
INDEED
REQUIRED
PART
OF
ANY
OPERATING
SYSTEM
DUE
TO
VARIOUS
LIMITATIONS
HOWEVER
INTERRUPT
HANDLERS
CAN
FORM
ONLY
THE
FIRST
HALF
OF
ANY
INTERRUPT
PROCESSING
SOLUTION
THESE
LIMITATIONS
INCLUDE
N
INTERRUPT
HANDLERS
RUN
ASYNCHRONOUSLY
AND
THUS
INTERRUPT
OTHER
POTENTIALLY
IMPOR
TANT
CODE
INCLUDING
OTHER
INTERRUPT
HANDLERS
THEREFORE
TO
AVOID
STALLING
THE
INTER
RUPTED
CODE
FOR
TOO
LONG
INTERRUPT
HANDLERS
NEED
TO
RUN
AS
QUICKLY
AS
POSSIBLE
N
INTERRUPT
HANDLERS
RUN
WITH
THE
CURRENT
INTERRUPT
LEVEL
DISABLED
AT
BEST
IF
IS
UNSET
AND
AT
WORST
IF
IS
SET
WITH
ALL
INTERRUPTS
ON
THE
CURRENT
PROCESSOR
DISABLED
AS
DISABLING
INTERRUPTS
PREVENTS
HARDWARE
FROM
COMMUNICATING
WITH
THE
OPERATING
SYSTEMS
INTERRUPT
HANDLERS
NEED
TO
RUN
AS
QUICKLY
AS
POSSIBLE
N
INTERRUPT
HANDLERS
ARE
OFTEN
TIMING
CRITICAL
BECAUSE
THEY
DEAL
WITH
HARDWARE
N
INTERRUPT
HANDLERS
DO
NOT
RUN
IN
PROCESS
CONTEXT
THEREFORE
THEY
CANNOT
BLOCK
THIS
LIMITS
WHAT
THEY
CAN
DO
IT
SHOULD
NOW
BE
EVIDENT
THAT
INTERRUPT
HANDLERS
ARE
ONLY
A
PIECE
OF
THE
SOLUTION
TO
MANAGING
HARDWARE
INTERRUPTS
OPERATING
SYSTEMS
CERTAINLY
NEED
A
QUICK
ASYNCHRONOUS
SIMPLE
MECHANISM
FOR
IMMEDIATELY
RESPONDING
TO
HARDWARE
AND
PERFORMING
ANY
TIME
CRITICAL
ACTIONS
INTERRUPT
HANDLERS
SERVE
THIS
FUNCTION
WELL
BUT
OTHER
LESS
CRITICAL
WORK
CAN
AND
SHOULD
BE
DEFERRED
TO
A
LATER
POINT
WHEN
INTERRUPTS
ARE
ENABLED
CONSEQUENTLY
MANAGING
INTERRUPTS
IS
DIVIDED
INTO
TWO
PARTS
OR
HALVES
THE
FIRST
PART
INTERRUPT
HANDLERS
TOP
HALVES
ARE
EXECUTED
BY
THE
KERNEL
ASYNCHRONOUSLY
IN
IMMEDIATE
RESPONSE
TO
A
HARDWARE
INTERRUPT
AS
DISCUSSED
IN
THE
PREVIOUS
CHAPTER
THIS
CHAPTER
LOOKS
AT
THE
SECOND
PART
OF
THE
INTERRUPT
SOLUTION
BOTTOM
HALVES
BOTTOM
HALVES
THE
JOB
OF
BOTTOM
HALVES
IS
TO
PERFORM
ANY
INTERRUPT
RELATED
WORK
NOT
PERFORMED
BY
THE
INTERRUPT
HANDLER
IN
AN
IDEAL
WORLD
THIS
IS
NEARLY
ALL
THE
WORK
BECAUSE
YOU
WANT
THE
INTERRUPT
HANDLER
TO
PERFORM
AS
LITTLE
WORK
AND
IN
TURN
BE
AS
FAST
AS
POSSIBLE
BY
OFFLOAD
ING
AS
MUCH
WORK
AS
POSSIBLE
TO
THE
BOTTOM
HALF
THE
INTERRUPT
HANDLER
CAN
RETURN
CONTROL
OF
THE
SYSTEM
TO
WHATEVER
IT
INTERRUPTED
AS
QUICKLY
AS
POSSIBLE
NONETHELESS
THE
INTERRUPT
HANDLER
MUST
PERFORM
SOME
OF
THE
WORK
FOR
EXAMPLE
THE
INTERRUPT
HANDLER
ALMOST
ASSUREDLY
NEEDS
TO
ACKNOWLEDGE
TO
THE
HARDWARE
THE
RECEIPT
OF
THE
INTERRUPT
IT
MAY
NEED
TO
COPY
DATA
TO
OR
FROM
THE
HARDWARE
THIS
WORK
IS
TIMING
SENSITIVE
SO
IT
MAKES
SENSE
TO
PERFORM
IT
IN
THE
INTERRUPT
HANDLER
ALMOST
ANYTHING
ELSE
IS
FAIR
GAME
FOR
PERFORMING
IN
THE
BOTTOM
HALF
FOR
EXAMPLE
IF
YOU
COPY
DATA
FROM
THE
HARDWARE
INTO
MEMORY
IN
THE
TOP
HALF
IT
CERTAINLY
MAKES
SENSE
TO
PROCESS
IT
IN
THE
BOTTOM
HALF
UNFORTUNATELY
NO
HARD
AND
FAST
RULES
EXIST
ABOUT
WHAT
WORK
TO
PERFORM
WHERE
THE
DECISION
IS
LEFT
ENTIRELY
UP
TO
THE
DEVICE
DRIVER
AUTHOR
ALTHOUGH
NO
ARRANGEMENT
IS
ILLEGAL
AN
ARRANGEMENT
CAN
CERTAINLY
BE
SUBOPTIMAL
REMEMBER
INTER
RUPT
HANDLERS
RUN
ASYNCHRONOUSLY
WITH
AT
LEAST
THE
CURRENT
INTERRUPT
LINE
DISABLED
MINI
MIZING
THEIR
DURATION
IS
IMPORTANT
ALTHOUGH
IT
IS
NOT
ALWAYS
CLEAR
HOW
TO
DIVIDE
THE
WORK
BETWEEN
THE
TOP
AND
BOTTOM
HALF
A
COUPLE
OF
USEFUL
TIPS
HELP
N
IF
THE
WORK
IS
TIME
SENSITIVE
PERFORM
IT
IN
THE
INTERRUPT
HANDLER
N
IF
THE
WORK
IS
RELATED
TO
THE
HARDWARE
PERFORM
IT
IN
THE
INTERRUPT
HANDLER
N
IF
THE
WORK
NEEDS
TO
ENSURE
THAT
ANOTHER
INTERRUPT
PARTICULARLY
THE
SAME
INTERRUPT
DOES
NOT
INTERRUPT
IT
PERFORM
IT
IN
THE
INTERRUPT
HANDLER
N
FOR
EVERYTHING
ELSE
CONSIDER
PERFORMING
THE
WORK
IN
THE
BOTTOM
HALF
WHEN
ATTEMPTING
TO
WRITE
YOUR
OWN
DEVICE
DRIVER
LOOKING
AT
OTHER
INTERRUPT
HANDLERS
AND
THEIR
CORRESPONDING
BOTTOM
HALVES
CAN
HELP
WHEN
DECIDING
HOW
TO
DIVIDE
YOUR
INTERRUPT
PROCESSING
WORK
BETWEEN
THE
TOP
AND
BOTTOM
HALF
ASK
YOURSELF
WHAT
MUST
BE
IN
THE
TOP
HALF
AND
WHAT
CAN
BE
IN
THE
BOTTOM
HALF
GENERALLY
THE
QUICKER
THE
INTERRUPT
HAN
DLER
EXECUTES
THE
BETTER
WHY
BOTTOM
HALVES
IT
IS
CRUCIAL
TO
UNDERSTAND
WHY
TO
DEFER
WORK
AND
WHEN
EXACTLY
TO
DEFER
IT
YOU
WANT
TO
LIMIT
THE
AMOUNT
OF
WORK
YOU
PERFORM
IN
AN
INTERRUPT
HANDLER
BECAUSE
INTERRUPT
HANDLERS
RUN
WITH
THE
CURRENT
INTERRUPT
LINE
DISABLED
ON
ALL
PROCESSORS
WORSE
HANDLERS
THAT
REGISTER
WITH
RUN
WITH
ALL
INTERRUPT
LINES
DISABLED
ON
THE
LOCAL
PROCESSOR
PLUS
THE
CURRENT
INTERRUPT
LINE
DISABLED
ON
ALL
PROCESSORS
MINIMIZING
THE
TIME
SPENT
WITH
INTER
RUPTS
DISABLED
IS
IMPORTANT
FOR
SYSTEM
RESPONSE
AND
PERFORMANCE
ADD
TO
THIS
THE
FACT
THAT
INTERRUPT
HANDLERS
RUN
ASYNCHRONOUSLY
WITH
RESPECT
TO
OTHER
CODE
EVEN
OTHER
INTERRUPT
HANDLERS
AND
IT
IS
CLEAR
THAT
YOU
SHOULD
WORK
TO
MINIMIZE
HOW
LONG
INTERRUPT
HANDLERS
RUN
PROCESSING
INCOMING
NETWORK
TRAFFIC
SHOULD
NOT
PREVENT
THE
KERNEL
RECEIPT
OF
KEY
STROKES
THE
SOLUTION
IS
TO
DEFER
SOME
OF
THE
WORK
UNTIL
LATER
BUT
WHEN
IS
LATER
THE
IMPORTANT
THING
TO
REALIZE
IS
THAT
LATER
IS
OFTEN
SIMPLY
NOT
NOW
THE
POINT
OF
A
BOTTOM
HALF
IS
NOT
TO
DO
WORK
AT
SOME
SPECIFIC
POINT
IN
THE
FUTURE
BUT
SIM
PLY
TO
DEFER
WORK
UNTIL
ANY
POINT
IN
THE
FUTURE
WHEN
THE
SYSTEM
IS
LESS
BUSY
AND
INTERRUPTS
ARE
AGAIN
ENABLED
OFTEN
BOTTOM
HALVES
RUN
IMMEDIATELY
AFTER
THE
INTERRUPT
RETURNS
THE
KEY
IS
THAT
THEY
RUN
WITH
ALL
INTERRUPTS
ENABLED
LINUX
IS
NOT
ALONE
IN
SEPARATING
THE
PROCESSING
OF
HARDWARE
INTERRUPTS
INTO
TWO
PARTS
MOST
OPERATING
SYSTEMS
DO
SO
THE
TOP
HALF
IS
QUICK
AND
SIMPLE
AND
RUNS
WITH
SOME
OR
ALL
INTERRUPTS
DISABLED
THE
BOTTOM
HALF
HOWEVER
IT
IS
IMPLEMENTED
RUNS
LATER
WITH
ALL
INTER
RUPTS
ENABLED
THIS
DESIGN
KEEPS
SYSTEM
LATENCY
LOW
BY
RUNNING
WITH
INTERRUPTS
DISABLED
FOR
AS
LITTLE
TIME
AS
NECESSARY
A
WORLD
OF
BOTTOM
HALVES
UNLIKE
THE
TOP
HALF
WHICH
IS
IMPLEMENTED
ENTIRELY
VIA
THE
INTERRUPT
HANDLER
MULTIPLE
MECHANISMS
ARE
AVAILABLE
FOR
IMPLEMENTING
A
BOTTOM
HALF
THESE
MECHANISMS
ARE
DIFFERENT
INTERFACES
AND
SUBSYSTEMS
THAT
ENABLE
YOU
TO
IMPLEMENT
BOTTOM
HALVES
WHEREAS
THE
PRE
VIOUS
CHAPTER
LOOKED
AT
JUST
A
SINGLE
WAY
OF
IMPLEMENTING
INTERRUPT
HANDLERS
THIS
CHAPTER
LOOKS
AT
MULTIPLE
METHODS
OF
IMPLEMENTING
BOTTOM
HALVES
OVER
THE
COURSE
OF
LINUX
HISTORY
THERE
HAVE
BEEN
MANY
BOTTOM
HALF
MECHANISMS
CONFUSINGLY
SOME
OF
THESE
MECHANISMS
HAVE
SIMILAR
OR
EVEN
DUMB
NAMES
IT
REQUIRES
A
SPECIAL
TYPE
OF
PROGRAMMER
TO
NAME
BOTTOM
HALVES
THIS
CHAPTER
DISCUSSES
BOTH
THE
DESIGN
AND
IMPLEMENTATION
OF
THE
BOTTOM
HALF
MECHA
NISMS
THAT
EXIST
IN
WE
ALSO
DISCUSS
HOW
TO
USE
THEM
IN
THE
KERNEL
CODE
YOU
WRITE
THE
OLD
BUT
LONG
SINCE
REMOVED
BOTTOM
HALF
MECHANISMS
ARE
HISTORICALLY
SIGNIFICANT
AND
SO
THEY
ARE
MENTIONED
WHEN
RELEVANT
THE
ORIGINAL
BOTTOM
HALF
IN
THE
BEGINNING
LINUX
PROVIDED
ONLY
THE
BOTTOM
HALF
FOR
IMPLEMENTING
BOTTOM
HALVES
THIS
NAME
WAS
LOGICAL
BECAUSE
AT
THE
TIME
THAT
WAS
THE
ONLY
MEANS
AVAILABLE
FOR
DEFERRING
WORK
THE
INFRASTRUCTURE
WAS
ALSO
KNOWN
AS
BH
WHICH
IS
WHAT
WE
WILL
CALL
IT
TO
AVOID
CONFUSION
WITH
THE
GENERIC
TERM
BOTTOM
HALF
THE
BH
INTERFACE
WAS
SIMPLE
LIKE
MOST
THINGS
IN
THOSE
GOOD
OLD
DAYS
IT
PROVIDED
A
STATICALLY
CREATED
LIST
OF
BOTTOM
HALVES
FOR
THE
ENTIRE
SYSTEM
THE
TOP
HALF
COULD
MARK
WHETHER
THE
BOTTOM
HALF
WOULD
RUN
BY
SETTING
A
BIT
IN
A
BIT
INTEGER
EACH
BH
WAS
GLOBALLY
SYNCHRONIZED
NO
TWO
COULD
RUN
AT
THE
SAME
TIME
EVEN
ON
DIFFERENT
PROCESSORS
THIS
WAS
EASY
TO
USE
YET
INFLEXIBLE
A
SIMPLE
APPROACH
YET
A
BOTTLENECK
TASK
QUEUES
LATER
ON
THE
KERNEL
DEVELOPERS
INTRODUCED
TASK
QUEUES
BOTH
AS
A
METHOD
OF
DEFERRING
WORK
AND
AS
A
REPLACEMENT
FOR
THE
BH
MECHANISM
THE
KERNEL
DEFINED
A
FAMILY
OF
QUEUES
EACH
QUEUE
CONTAINED
A
LINKED
LIST
OF
FUNCTIONS
TO
CALL
THE
QUEUED
FUNCTIONS
WERE
RUN
AT
CERTAIN
TIMES
DEPENDING
ON
WHICH
QUEUE
THEY
WERE
IN
DRIVERS
COULD
REGISTER
THEIR
BOT
TOM
HALVES
IN
THE
APPROPRIATE
QUEUE
THIS
WORKED
FAIRLY
WELL
BUT
IT
WAS
STILL
TOO
INFLEXIBLE
TO
REPLACE
THE
BH
INTERFACE
ENTIRELY
IT
ALSO
WAS
NOT
LIGHTWEIGHT
ENOUGH
FOR
PERFORMANCE
CRITICAL
SUBSYSTEMS
SUCH
AS
NETWORKING
SOFTIRQS
AND
TASKLETS
DURING
THE
DEVELOPMENT
SERIES
THE
KERNEL
DEVELOPERS
INTRODUCED
SOFTIRQS
AND
TASKLETS
WITH
THE
EXCEPTION
OF
COMPATIBILITY
WITH
EXISTING
DRIVERS
SOFTIRQS
AND
TASKLETS
COULD
COM
PLETELY
REPLACE
THE
BH
INTERFACE
SOFTIRQS
ARE
A
SET
OF
STATICALLY
DEFINED
BOTTOM
HALVES
THAT
CAN
RUN
SIMULTANEOUSLY
ON
ANY
PROCESSOR
EVEN
TWO
OF
THE
SAME
TYPE
CAN
RUN
CONCUR
RENTLY
TASKLETS
WHICH
HAVE
AN
AWFUL
AND
CONFUSING
NAME
ARE
FLEXIBLE
DYNAMICALLY
CRE
ATED
BOTTOM
HALVES
BUILT
ON
TOP
OF
SOFTIRQS
TWO
DIFFERENT
TASKLETS
CAN
RUN
CONCURRENTLY
ON
DIFFERENT
PROCESSORS
BUT
TWO
OF
THE
SAME
TYPE
OF
TASKLET
CANNOT
RUN
SIMULTANEOUSLY
THUS
TASKLETS
ARE
A
GOOD
TRADE
OFF
BETWEEN
PERFORMANCE
AND
EASE
OF
USE
FOR
MOST
BOTTOM
HALF
PROCESSING
THE
TASKLET
IS
SUFFICIENT
SOFTIRQS
ARE
USEFUL
WHEN
PERFORMANCE
IS
CRITICAL
SUCH
AS
WITH
NETWORKING
USING
SOFTIRQS
REQUIRES
MORE
CARE
HOWEVER
BECAUSE
TWO
OF
THE
SAME
SOFTIRQ
CAN
RUN
AT
THE
SAME
TIME
IN
ADDITION
SOFTIRQS
MUST
BE
REGISTERED
STATICALLY
AT
COM
PILE
TIME
CONVERSELY
CODE
CAN
DYNAMICALLY
REGISTER
TASKLETS
TO
FURTHER
CONFOUND
THE
ISSUE
SOME
PEOPLE
REFER
TO
ALL
BOTTOM
HALVES
AS
SOFTWARE
INTER
RUPTS
OR
SOFTIRQS
IN
OTHER
WORDS
THEY
CALL
BOTH
THE
SOFTIRQ
MECHANISM
AND
BOTTOM
HALVES
IN
GENERAL
SOFTIRQS
IGNORE
THOSE
PEOPLE
THEY
RUN
WITH
THE
SAME
CROWD
THAT
NAMED
THE
BH
AND
TASKLET
MECHANISMS
WHILE
DEVELOPING
THE
KERNEL
THE
BH
INTERFACE
WAS
FINALLY
TOSSED
TO
THE
CURB
BECAUSE
ALL
BH
USERS
WERE
CONVERTED
TO
THE
OTHER
BOTTOM
HALF
INTERFACES
ADDITIONALLY
THE
TASK
QUEUE
INTERFACE
WAS
REPLACED
BY
THE
WORK
QUEUE
INTERFACE
WORK
QUEUES
ARE
A
SIMPLE
YET
USEFUL
METHOD
OF
QUEUING
WORK
TO
LATER
BE
PERFORMED
IN
PROCESS
CONTEXT
WE
GET
TO
THEM
LATER
CONSEQUENTLY
TODAY
HAS
THREE
BOTTOM
HALF
MECHANISMS
IN
THE
KERNEL
SOFTIRQS
TASKLETS
AND
WORK
QUEUES
THE
OLD
BH
AND
TASK
QUEUE
INTERFACES
ARE
BUT
MERE
MEMORIES
KERNEL
TIMERS
ANOTHER
MECHANISM
FOR
DEFERRING
WORK
IS
KERNEL
TIMERS
UNLIKE
THE
MECHANISMS
DISCUSSED
IN
THE
CHAPTER
THUS
FAR
TIMERS
DEFER
WORK
FOR
A
SPECIFIED
AMOUNT
OF
TIME
THAT
IS
ALTHOUGH
THE
TOOLS
DISCUSSED
IN
THIS
CHAPTER
ARE
USEFUL
TO
DEFER
WORK
TO
ANY
TIME
BUT
NOW
YOU
USE
TIMERS
TO
DEFER
WORK
UNTIL
AT
LEAST
A
SPECIFIC
TIME
HAS
ELAPSED
THEREFORE
TIMERS
HAVE
DIFFERENT
USES
THAN
THE
GENERAL
MECHANISMS
DISCUSSED
IN
THIS
CHAP
TER
A
FULL
DISCUSSION
OF
TIMERS
IS
GIVEN
IN
CHAPTER
TIMERS
AND
TIME
MANAGEMENT
IT
IS
NONTRIVIAL
TO
CONVERT
BHS
TO
SOFTIRQS
OR
TASKLETS
BECAUSE
BHS
ARE
GLOBALLY
SYNCHRONIZED
AND
THERE
FORE
ASSUME
THAT
NO
OTHER
BH
IS
RUNNING
DURING
THEIR
EXECUTION
THE
CONVERSION
DID
EVENTUALLY
HAPPEN
HOWEVER
IN
THEY
HAVE
NOTHING
TO
DO
WITH
TASKS
THINK
OF
A
TASKLET
AS
A
SIMPLE
AND
EASY
TO
USE
SOFTIRQ
DISPELLING
THE
CONFUSION
THIS
IS
SOME
SERIOUSLY
CONFUSING
STUFF
BUT
ACTUALLY
IT
INVOLVES
JUST
NAMING
ISSUES
LET
GO
OVER
IT
AGAIN
BOTTOM
HALF
IS
A
GENERIC
OPERATING
SYSTEM
TERM
REFERRING
TO
THE
DEFERRED
PORTION
OF
INTERRUPT
PROCESSING
SO
NAMED
BECAUSE
IT
REPRESENTS
THE
SECOND
OR
BOTTOM
HALF
OF
THE
INTERRUPT
PROCESSING
SOLUTION
IN
LINUX
THE
TERM
CURRENTLY
HAS
THIS
MEANING
TOO
ALL
THE
KERNEL
MECHANISMS
FOR
DEFERRING
WORK
ARE
BOTTOM
HALVES
SOME
PEOPLE
ALSO
CONFUS
INGLY
CALL
ALL
BOTTOM
HALVES
SOFTIRQS
BOTTOM
HALF
ALSO
REFERS
TO
THE
ORIGINAL
DEFERRED
WORK
MECHANISM
IN
LINUX
THIS
MECH
ANISM
IS
ALSO
KNOWN
AS
A
BH
SO
WE
CALL
IT
BY
THAT
NAME
NOW
AND
LEAVE
THE
FORMER
AS
A
GENERIC
DESCRIPTION
THE
BH
MECHANISM
WAS
DEPRECATED
A
WHILE
BACK
AND
FULLY
REMOVED
IN
THE
DEVELOPMENT
KERNEL
SERIES
CURRENTLY
THREE
METHODS
EXIST
FOR
DEFERRING
WORK
SOFTIRQS
TASKLETS
AND
WORK
QUEUES
TASKLETS
ARE
BUILT
ON
SOFTIRQS
AND
WORK
QUEUES
ARE
THEIR
OWN
SUBSYSTEM
TABLE
PRESENTS
A
HISTORY
OF
BOTTOM
HALVES
TABLE
BOTTOM
HALF
STATUS
BOTTOM
HALF
STATUS
BH
REMOVED
IN
TASK
QUEUES
REMOVED
IN
SOFTIRQ
AVAILABLE
SINCE
TASKLET
AVAILABLE
SINCE
WORK
QUEUES
AVAILABLE
SINCE
WITH
THIS
NAMING
CONFUSION
SETTLED
LET
LOOK
AT
THE
INDIVIDUAL
MECHANISMS
SOFTIRQS
THE
PLACE
TO
START
THIS
DISCUSSION
OF
THE
ACTUAL
BOTTOM
HALF
METHODS
IS
WITH
SOFTIRQS
SOFTIRQS
ARE
RARELY
USED
DIRECTLY
TASKLETS
ARE
A
MUCH
MORE
COMMON
FORM
OF
BOTTOM
HALF
NONETHELESS
BECAUSE
TASKLETS
ARE
BUILT
ON
SOFTIRQS
WE
COVER
THEM
FIRST
THE
SOFTIRQ
CODE
LIVES
IN
THE
FILE
KERNEL
SOFTIRQ
C
IN
THE
KERNEL
SOURCE
TREE
IMPLEMENTING
SOFTIRQS
SOFTIRQS
ARE
STATICALLY
ALLOCATED
AT
COMPILE
TIME
UNLIKE
TASKLETS
YOU
CANNOT
DYNAMICALLY
REGISTER
AND
DESTROY
SOFTIRQS
SOFTIRQS
ARE
REPRESENTED
BY
THE
STRUCTURE
WHICH
IS
DEFINED
IN
LINUX
INTERRUPT
H
STRUCT
VOID
ACTION
STRUCT
A
ENTRY
ARRAY
OF
THIS
STRUCTURE
IS
DECLARED
IN
KERNEL
SOFTIRQ
C
STATIC
STRUCT
EACH
REGISTERED
SOFTIRQ
CONSUMES
ONE
ENTRY
IN
THE
ARRAY
CONSEQUENTLY
THERE
ARE
REGISTERED
SOFTIRQS
THE
NUMBER
OF
REGISTERED
SOFTIRQS
IS
STATICALLY
DETERMINED
AT
COMPILE
TIME
AND
CANNOT
BE
CHANGED
DYNAMICALLY
THE
KERNEL
ENFORCES
A
LIMIT
OF
REGISTERED
SOFTIRQS
IN
THE
CURRENT
KERNEL
HOWEVER
ONLY
NINE
EXIST
THE
SOFTIRQ
HANDLER
THE
PROTOTYPE
OF
A
SOFTIRQ
HANDLER
ACTION
LOOKS
LIKE
VOID
STRUCT
WHEN
THE
KERNEL
RUNS
A
SOFTIRQ
HANDLER
IT
EXECUTES
THIS
ACTION
FUNCTION
WITH
A
POINTER
TO
THE
CORRESPONDING
STRUCTURE
AS
ITS
LONE
ARGUMENT
FOR
EXAMPLE
IF
POINTED
TO
AN
ENTRY
IN
THE
ARRAY
THE
KERNEL
WOULD
INVOKE
THE
SOFTIRQ
HANDLER
FUNCTION
AS
ACTION
IT
SEEMS
A
BIT
ODD
THAT
THE
KERNEL
PASSES
THE
ENTIRE
STRUCTURE
TO
THE
SOFTIRQ
HANDLER
THIS
TRICK
ENABLES
FUTURE
ADDITIONS
TO
THE
STRUCTURE
WITHOUT
REQUIRING
A
CHANGE
IN
EVERY
SOFTIRQ
HANDLER
A
SOFTIRQ
NEVER
PREEMPTS
ANOTHER
SOFTIRQ
THE
ONLY
EVENT
THAT
CAN
PREEMPT
A
SOFTIRQ
IS
AN
INTERRUPT
HANDLER
ANOTHER
SOFTIRQ
EVEN
THE
SAME
ONE
CAN
RUN
ON
ANOTHER
PROCESSOR
HOWEVER
EXECUTING
SOFTIRQS
A
REGISTERED
SOFTIRQ
MUST
BE
MARKED
BEFORE
IT
WILL
EXECUTE
THIS
IS
CALLED
RAISING
THE
SOFTIRQ
USUALLY
AN
INTERRUPT
HANDLER
MARKS
ITS
SOFTIRQ
FOR
EXECUTION
BEFORE
RETURNING
THEN
AT
A
SUITABLE
TIME
THE
SOFTIRQ
RUNS
PENDING
SOFTIRQS
ARE
CHECKED
FOR
AND
EXECUTED
IN
THE
FOL
LOWING
PLACES
N
IN
THE
RETURN
FROM
HARDWARE
INTERRUPT
CODE
PATH
N
IN
THE
KSOFTIRQD
KERNEL
THREAD
N
IN
ANY
CODE
THAT
EXPLICITLY
CHECKS
FOR
AND
EXECUTES
PENDING
SOFTIRQS
SUCH
AS
THE
NET
WORKING
SUBSYSTEM
REGARDLESS
OF
THE
METHOD
OF
INVOCATION
SOFTIRQ
EXECUTION
OCCURS
IN
WHICH
IS
INVOKED
BY
THE
FUNCTION
IS
QUITE
SIMPLE
IF
THERE
ARE
PENDING
MOST
DRIVERS
USE
TASKLETS
OR
WORK
QUEUES
FOR
THEIR
BOTTOM
HALF
TASKLETS
ARE
BUILT
OFF
SOFTIRQS
AS
THE
NEXT
SECTION
EXPLAINS
SOFTIRQS
LOOPS
OVER
EACH
ONE
INVOKING
ITS
HANDLER
LET
LOOK
AT
A
SIMPLI
FIED
VARIANT
OF
THE
IMPORTANT
PART
OF
PENDING
PENDING
IF
PENDING
STRUCT
H
RESET
THE
PENDING
BITMASK
H
DO
IF
PENDING
H
ACTION
H
H
PENDING
WHILE
PENDING
THIS
SNIPPET
IS
THE
HEART
OF
SOFTIRQ
PROCESSING
IT
CHECKS
FOR
AND
EXECUTES
ANY
PENDING
SOFTIRQS
SPECIFICALLY
IT
SETS
THE
PENDING
LOCAL
VARIABLE
TO
THE
VALUE
RETURNED
BY
THE
MACRO
THIS
IS
A
BIT
MASK
OF
PENDING
SOFTIRQS
IF
BIT
N
IS
SET
THE
NTH
SOFTIRQ
IS
PENDING
NOW
THAT
THE
PENDING
BITMASK
OF
SOFTIRQS
IS
SAVED
IT
CLEARS
THE
ACTUAL
BITMASK
THE
POINTER
H
IS
SET
TO
THE
FIRST
ENTRY
IN
THE
IF
THE
FIRST
BIT
IN
PENDING
IS
SET
H
ACTION
H
IS
CALLED
THE
POINTER
H
IS
INCREMENTED
BY
ONE
SO
THAT
IT
NOW
POINTS
TO
THE
SECOND
ENTRY
IN
THE
ARRAY
THE
BITMASK
PENDING
IS
RIGHT
SHIFTED
BY
ONE
THIS
TOSSES
THE
FIRST
BIT
AWAY
AND
MOVES
ALL
OTHER
BITS
ONE
PLACE
TO
THE
RIGHT
CONSEQUENTLY
THE
SECOND
BIT
IS
NOW
THE
FIRST
AND
SO
ON
THE
POINTER
H
NOW
POINTS
TO
THE
SECOND
ENTRY
IN
THE
ARRAY
AND
THE
PENDING
BIT
MASK
NOW
HAS
THE
SECOND
BIT
AS
THE
FIRST
REPEAT
THE
PREVIOUS
STEPS
THIS
ACTUALLY
OCCURS
WITH
LOCAL
INTERRUPTS
DISABLED
BUT
THAT
IS
OMITTED
IN
THIS
SIMPLIFIED
EXAMPLE
IF
INTERRUPTS
WERE
NOT
DISABLED
A
SOFTIRQ
COULD
HAVE
BEEN
RAISED
AND
THUS
BE
PENDING
IN
THE
INTERVENING
TIME
BETWEEN
SAVING
THE
MASK
AND
CLEARING
IT
THIS
WOULD
RESULT
IN
INCORRECTLY
CLEARING
A
PENDING
BIT
CONTINUE
REPEATING
UNTIL
PENDING
IS
ZERO
AT
WHICH
POINT
THERE
ARE
NO
MORE
PEND
ING
SOFTIRQS
AND
THE
WORK
IS
DONE
NOTE
THIS
CHECK
IS
SUFFICIENT
TO
ENSURE
H
ALWAYS
POINTS
TO
A
VALID
ENTRY
IN
BECAUSE
PENDING
HAS
AT
MOST
SET
BITS
AND
THUS
THIS
LOOP
EXECUTES
AT
MOST
TIMES
USING
SOFTIRQS
SOFTIRQS
ARE
RESERVED
FOR
THE
MOST
TIMING
CRITICAL
AND
IMPORTANT
BOTTOM
HALF
PROCESSING
ON
THE
SYSTEM
CURRENTLY
ONLY
TWO
SUBSYSTEMS
NETWORKING
AND
BLOCK
DEVICES
DIRECTLY
USE
SOFTIRQS
ADDITIONALLY
KERNEL
TIMERS
AND
TASKLETS
ARE
BUILT
ON
TOP
OF
SOFTIRQS
IF
YOU
ADD
A
NEW
SOFTIRQ
YOU
NORMALLY
WANT
TO
ASK
YOURSELF
WHY
USING
A
TASKLET
IS
INSUFFICIENT
TASKLETS
ARE
DYNAMICALLY
CREATED
AND
ARE
SIMPLER
TO
USE
BECAUSE
OF
THEIR
WEAKER
LOCKING
REQUIRE
MENTS
AND
THEY
STILL
PERFORM
QUITE
WELL
NONETHELESS
FOR
TIMING
CRITICAL
APPLICATIONS
THAT
CAN
DO
THEIR
OWN
LOCKING
IN
AN
EFFICIENT
WAY
SOFTIRQS
MIGHT
BE
THE
CORRECT
SOLUTION
ASSIGNING
AN
INDEX
YOU
DECLARE
SOFTIRQS
STATICALLY
AT
COMPILE
TIME
VIA
AN
ENUM
IN
LINUX
INTERRUPT
H
THE
KERNEL
USES
THIS
INDEX
WHICH
STARTS
AT
ZERO
AS
A
RELATIVE
PRIORITY
SOFTIRQS
WITH
THE
LOWEST
NUMERICAL
PRIORITY
EXECUTE
BEFORE
THOSE
WITH
A
HIGHER
NUMERICAL
PRIORITY
CREATING
A
NEW
SOFTIRQ
INCLUDES
ADDING
A
NEW
ENTRY
TO
THIS
ENUM
WHEN
ADDING
A
NEW
SOFTIRQ
YOU
MIGHT
NOT
WANT
TO
SIMPLY
ADD
YOUR
ENTRY
TO
THE
END
OF
THE
LIST
AS
YOU
WOULD
ELSEWHERE
INSTEAD
YOU
NEED
TO
INSERT
THE
NEW
ENTRY
DEPENDING
ON
THE
PRIORITY
YOU
WANT
TO
GIVE
IT
BY
CONVENTION
IS
ALWAYS
THE
FIRST
AND
IS
ALWAYS
THE
LAST
ENTRY
A
NEW
ENTRY
LIKELY
BELONGS
IN
BETWEEN
AND
TABLE
CONTAINS
A
LIST
OF
THE
EXISTING
TASKLET
TYPES
TABLE
SOFTIRQ
TYPES
TASKLET
PRIORITY
SOFTIRQ
DESCRIPTION
HIGH
PRIORITY
TASKLETS
TIMERS
SEND
NETWORK
PACKETS
RECEIVE
NETWORK
PACKETS
BLOCK
DEVICES
NORMAL
PRIORITY
TASKLETS
SCHEDULER
HIGH
RESOLUTION
TIMERS
RCU
LOCKING
REGISTERING
YOUR
HANDLER
NEXT
THE
SOFTIRQ
HANDLER
IS
REGISTERED
AT
RUN
TIME
VIA
WHICH
TAKES
TWO
PARAMETERS
THE
SOFTIRQ
INDEX
AND
ITS
HANDLER
FUNCTION
THE
NETWORKING
SUBSYSTEM
FOR
EXAMPLE
REGISTERS
ITS
SOFTIRQS
LIKE
THIS
IN
NET
CORE
DEV
C
THE
SOFTIRQ
HANDLERS
RUN
WITH
INTERRUPTS
ENABLED
AND
CANNOT
SLEEP
WHILE
A
HANDLER
RUNS
SOFTIRQS
ON
THE
CURRENT
PROCESSOR
ARE
DISABLED
ANOTHER
PROCESSOR
HOWEVER
CAN
EXE
CUTE
OTHER
SOFTIRQS
IF
THE
SAME
SOFTIRQ
IS
RAISED
AGAIN
WHILE
IT
IS
EXECUTING
ANOTHER
PROCES
SOR
CAN
RUN
IT
SIMULTANEOUSLY
THIS
MEANS
THAT
ANY
SHARED
DATA
EVEN
GLOBAL
DATA
USED
ONLY
WITHIN
THE
SOFTIRQ
HANDLER
NEEDS
PROPER
LOCKING
AS
DISCUSSED
IN
THE
NEXT
TWO
CHAPTERS
THIS
IS
AN
IMPORTANT
POINT
AND
IT
IS
THE
REASON
TASKLETS
ARE
USUALLY
PREFERRED
SIMPLY
PRE
VENTING
YOUR
SOFTIRQS
FROM
RUNNING
CONCURRENTLY
IS
NOT
IDEAL
IF
A
SOFTIRQ
OBTAINED
A
LOCK
TO
PREVENT
ANOTHER
INSTANCE
OF
ITSELF
FROM
RUNNING
SIMULTANEOUSLY
THERE
WOULD
BE
NO
REASON
TO
USE
A
SOFTIRQ
CONSEQUENTLY
MOST
SOFTIRQ
HANDLERS
RESORT
TO
PER
PROCESSOR
DATA
DATA
UNIQUE
TO
EACH
PROCESSOR
AND
THUS
NOT
REQUIRING
LOCKING
AND
OTHER
TRICKS
TO
AVOID
EXPLICIT
LOCKING
AND
PROVIDE
EXCELLENT
SCALABILITY
THE
RAISON
D
ÊTRE
TO
SOFTIRQS
IS
SCALABILITY
IF
YOU
DO
NOT
NEED
TO
SCALE
TO
INFINITELY
MANY
PROCESSORS
THEN
USE
A
TASKLET
TASKLETS
ARE
ESSENTIALLY
SOFTIRQS
IN
WHICH
MULTIPLE
INSTANCES
OF
THE
SAME
HANDLER
CANNOT
RUN
CONCURRENTLY
ON
MULTIPLE
PROCESSORS
RAISING
YOUR
SOFTIRQ
AFTER
A
HANDLER
IS
ADDED
TO
THE
ENUM
LIST
AND
REGISTERED
VIA
IT
IS
READY
TO
RUN
TO
MARK
IT
PENDING
SO
IT
IS
RUN
AT
THE
NEXT
INVOCATION
OF
CALL
FOR
EXAMPLE
THE
NETWORKING
SUBSYSTEM
WOULD
CALL
THIS
RAISES
THE
SOFTIRQ
ITS
HANDLER
RUNS
THE
NEXT
TIME
THE
KERNEL
EXECUTES
SOFTIRQS
THIS
FUNCTION
DISABLES
INTERRUPTS
PRIOR
TO
ACTUALLY
RAISING
THE
SOFTIRQ
AND
THEN
RESTORES
THEM
TO
THEIR
PREVIOUS
STATE
IF
INTERRUPTS
ARE
ALREADY
OFF
THE
FUNCTION
CAN
BE
USED
AS
A
SMALL
OPTIMIZATION
FOR
EXAMPLE
INTERRUPTS
MUST
ALREADY
BE
OFF
SOFTIRQS
ARE
MOST
OFTEN
RAISED
FROM
WITHIN
INTERRUPT
HANDLERS
IN
THE
CASE
OF
INTERRUPT
HANDLERS
THE
INTERRUPT
HANDLER
PERFORMS
THE
BASIC
HARDWARE
RELATED
WORK
RAISES
THE
SOFTIRQ
AND
THEN
EXITS
WHEN
PROCESSING
INTERRUPTS
THE
KERNEL
INVOKES
THE
SOFTIRQ
THEN
RUNS
AND
PICKS
UP
WHERE
THE
INTERRUPT
HANDLER
LEFT
OFF
IN
THIS
EXAMPLE
THE
TOP
HALF
AND
BOTTOM
HALF
NAMING
SHOULD
MAKE
SENSE
TASKLETS
TASKLETS
ARE
A
BOTTOM
HALF
MECHANISM
BUILT
ON
TOP
OF
SOFTIRQS
AS
MENTIONED
THEY
HAVE
NOTHING
TO
DO
WITH
TASKS
TASKLETS
ARE
SIMILAR
IN
NATURE
AND
BEHAVIOR
TO
SOFTIRQS
HOWEVER
THEY
HAVE
A
SIMPLER
INTERFACE
AND
RELAXED
LOCKING
RULES
AS
A
DEVICE
DRIVER
AUTHOR
THE
DECISION
WHETHER
TO
USE
SOFTIRQS
VERSUS
TASKLETS
IS
SIMPLE
YOU
ALMOST
ALWAYS
WANT
TO
USE
TASKLETS
AS
WE
SAW
IN
THE
PREVIOUS
SECTION
YOU
CAN
ALMOST
COUNT
ON
ONE
HAND
THE
USERS
OF
SOFTIRQS
SOFTIRQS
ARE
REQUIRED
ONLY
FOR
HIGH
FREQUENCY
AND
HIGHLY
THREADED
USES
TASKLETS
ON
THE
OTHER
HAND
SEE
MUCH
GREATER
USE
TASKLETS
WORK
JUST
FINE
FOR
THE
VAST
MAJORITY
OF
CASES
AND
ARE
VERY
EASY
TO
USE
IMPLEMENTING
TASKLETS
BECAUSE
TASKLETS
ARE
IMPLEMENTED
ON
TOP
OF
SOFTIRQS
THEY
ARE
SOFTIRQS
AS
DISCUSSED
TASKLETS
ARE
REPRESENTED
BY
TWO
SOFTIRQS
AND
THE
ONLY
DIFFERENCE
IN
THESE
TYPES
IS
THAT
THE
BASED
TASKLETS
RUN
PRIOR
TO
THE
BASED
TASKLETS
THE
TASKLET
STRUCTURE
TASKLETS
ARE
REPRESENTED
BY
THE
STRUCTURE
EACH
STRUCTURE
REPRESENTS
A
UNIQUE
TASKLET
THE
STRUCTURE
IS
DECLARED
IN
LINUX
INTERRUPT
H
STRUCT
STRUCT
NEXT
NEXT
TASKLET
IN
THE
LIST
UNSIGNED
LONG
STATE
STATE
OF
THE
TASKLET
COUNT
REFERENCE
COUNTER
VOID
FUNC
UNSIGNED
LONG
TASKLET
HANDLER
FUNCTION
UNSIGNED
LONG
DATA
ARGUMENT
TO
THE
TASKLET
FUNCTION
THE
FUNC
MEMBER
IS
THE
TASKLET
HANDLER
THE
EQUIVALENT
OF
ACTION
TO
A
SOFTIRQ
AND
RECEIVES
DATA
AS
ITS
SOLE
ARGUMENT
THE
STATE
MEMBER
IS
EXACTLY
ZERO
OR
DENOTES
A
TASKLET
THAT
IS
SCHEDULED
TO
RUN
AND
DENOTES
A
TASKLET
THAT
IS
RUNNING
AS
AN
OPTIMIZATION
IS
USED
ONLY
ON
MULTIPROCESSOR
MACHINES
BECAUSE
A
UNIPROCESSOR
MACHINE
ALWAYS
KNOWS
WHETHER
THE
TASKLET
IS
RUNNING
IT
IS
EITHER
THE
CURRENTLY
EXECUTING
CODE
OR
NOT
THE
COUNT
FIELD
IS
USED
AS
A
REFERENCE
COUNT
FOR
THE
TASKLET
IF
IT
IS
NONZERO
THE
TASKLET
IS
DISABLED
AND
CANNOT
RUN
IF
IT
IS
ZERO
THE
TASKLET
IS
ENABLED
AND
CAN
RUN
IF
MARKED
PENDING
SCHEDULING
TASKLETS
SCHEDULED
TASKLETS
THE
EQUIVALENT
OF
RAISED
SOFTIRQS
ARE
STORED
IN
TWO
PER
PROCESSOR
STRUC
TURES
FOR
REGULAR
TASKLETS
AND
FOR
HIGH
PRIORITY
TASKLETS
BOTH
OF
THESE
STRUCTURES
ARE
LINKED
LISTS
OF
STRUCTURES
EACH
STRUCTURE
IN
THE
LIST
REPRESENTS
A
DIFFERENT
TASKLET
TASKLETS
ARE
SCHEDULED
VIA
THE
AND
FUNCTIONS
WHICH
RECEIVE
A
POINTER
TO
THE
TASKLET
AS
THEIR
LONE
ARGU
MENT
EACH
FUNCTION
ENSURES
THAT
THE
PROVIDED
TASKLET
IS
NOT
YET
SCHEDULED
AND
THEN
CALLS
AND
AS
APPROPRIATE
THE
TWO
FUNC
TIONS
ARE
SIMILAR
THE
DIFFERENCE
IS
THAT
ONE
USES
AND
ONE
USES
WRITING
AND
USING
TASKLETS
IS
COVERED
IN
THE
NEXT
SECTION
NOW
LET
LOOK
AT
THE
STEPS
UNDERTAKES
CHECK
WHETHER
THE
TASKLET
STATE
IS
IF
IT
IS
THE
TASKLET
IS
ALREADY
SCHEDULED
TO
RUN
AND
THE
FUNCTION
CAN
IMMEDIATELY
RETURN
CALL
SAVE
THE
STATE
OF
THE
INTERRUPT
SYSTEM
AND
THEN
DISABLE
LOCAL
INTERRUPTS
THIS
ENSURES
THAT
NOTHING
ON
THIS
PROCESSOR
WILL
MESS
WITH
THE
TASKLET
CODE
WHILE
IS
MANIPULATING
THE
TASKLETS
ADD
THE
TASKLET
TO
BE
SCHEDULED
TO
THE
HEAD
OF
THE
OR
LINKED
LIST
WHICH
IS
UNIQUE
TO
EACH
PROCESSOR
IN
THE
SYSTEM
RAISE
THE
OR
SOFTIRQ
SO
EXECUTES
THIS
TASKLET
IN
THE
NEAR
FUTURE
RESTORE
INTERRUPTS
TO
THEIR
PREVIOUS
STATE
AND
RETURN
AT
THE
NEXT
EARLIEST
CONVENIENCE
IS
RUN
AS
DISCUSSED
IN
THE
PREVIOUS
SEC
TION
BECAUSE
MOST
TASKLETS
AND
SOFTIRQS
ARE
MARKED
PENDING
IN
INTERRUPT
HANDLERS
MOST
LIKELY
RUNS
WHEN
THE
LAST
INTERRUPT
RETURNS
BECAUSE
OR
IS
NOW
RAISED
EXECUTES
THE
ASSOCIATED
HANDLERS
THESE
HANDLERS
AND
ARE
THE
HEART
OF
TASKLET
PROCESSING
LET
LOOK
AT
THE
STEPS
THESE
HANDLERS
PERFORM
DISABLE
LOCAL
INTERRUPT
DELIVERY
THERE
IS
NO
NEED
TO
FIRST
SAVE
THEIR
STATE
BECAUSE
THE
CODE
HERE
IS
ALWAYS
CALLED
AS
A
SOFTIRQ
HANDLER
AND
INTERRUPTS
ARE
ALWAYS
ENABLED
AND
RETRIEVE
THE
OR
LIST
FOR
THIS
PROCESSOR
CLEAR
THE
LIST
FOR
THIS
PROCESSOR
BY
SETTING
IT
EQUAL
TO
NULL
YET
ANOTHER
EXAMPLE
OF
THE
CONFUSING
NAMING
SCHEMES
AT
WORK
HERE
WHY
ARE
SOFTIRQS
RAISED
BUT
TASKLETS
SCHEDULED
WHO
KNOWS
BOTH
TERMS
MEAN
TO
MARK
THAT
BOTTOM
HALF
PENDING
SO
THAT
IT
IS
EXE
CUTED
SOON
ENABLE
LOCAL
INTERRUPT
DELIVERY
AGAIN
THERE
IS
NO
NEED
TO
RESTORE
THEM
TO
THEIR
PRE
VIOUS
STATE
BECAUSE
THIS
FUNCTION
KNOWS
THAT
THEY
WERE
ALWAYS
ORIGINALLY
ENABLED
LOOP
OVER
EACH
PENDING
TASKLET
IN
THE
RETRIEVED
LIST
IF
THIS
IS
A
MULTIPROCESSING
MACHINE
CHECK
WHETHER
THE
TASKLET
IS
RUNNING
ON
ANOTHER
PROCESSOR
BY
CHECKING
THE
FLAG
IF
IT
IS
CURRENTLY
RUN
NING
DO
NOT
EXECUTE
IT
NOW
AND
SKIP
TO
THE
NEXT
PENDING
TASKLET
RECALL
THAT
ONLY
ONE
TASKLET
OF
A
GIVEN
TYPE
MAY
RUN
CONCURRENTLY
IF
THE
TASKLET
IS
NOT
CURRENTLY
RUNNING
SET
THE
FLAG
SO
ANOTHER
PROCESSOR
WILL
NOT
RUN
IT
CHECK
FOR
A
ZERO
COUNT
VALUE
TO
ENSURE
THAT
THE
TASKLET
IS
NOT
DISABLED
IF
THE
TASKLET
IS
DISABLED
SKIP
IT
AND
GO
TO
THE
NEXT
PENDING
TASKLET
WE
NOW
KNOW
THAT
THE
TASKLET
IS
NOT
RUNNING
ELSEWHERE
IS
MARKED
AS
RUNNING
SO
IT
WILL
NOT
START
RUNNING
ELSEWHERE
AND
HAS
A
ZERO
COUNT
VALUE
RUN
THE
TASKLET
HANDLER
AFTER
THE
TASKLET
RUNS
CLEAR
THE
FLAG
IN
THE
TASKLET
STATE
FIELD
REPEAT
FOR
THE
NEXT
PENDING
TASKLET
UNTIL
THERE
ARE
NO
MORE
SCHEDULED
TASKLETS
WAITING
TO
RUN
THE
IMPLEMENTATION
OF
TASKLETS
IS
SIMPLE
BUT
RATHER
CLEVER
AS
YOU
SAW
ALL
TASKLETS
ARE
MULTIPLEXED
ON
TOP
OF
TWO
SOFTIRQS
AND
WHEN
A
TASKLET
IS
SCHEDULED
THE
KERNEL
RAISES
ONE
OF
THESE
SOFTIRQS
THESE
SOFTIRQS
IN
TURN
ARE
HANDLED
BY
SPECIAL
FUNCTIONS
THAT
THEN
RUN
ANY
SCHEDULED
TASKLETS
THE
SPECIAL
FUNCTIONS
ENSURE
THAT
ONLY
ONE
TASKLET
OF
A
GIVEN
TYPE
RUNS
AT
THE
SAME
TIME
BUT
OTHER
TASKLETS
CAN
RUN
SIMULTA
NEOUSLY
ALL
THIS
COMPLEXITY
IS
THEN
HIDDEN
BEHIND
A
CLEAN
AND
SIMPLE
INTERFACE
USING
TASKLETS
IN
MOST
CASES
TASKLETS
ARE
THE
PREFERRED
MECHANISM
WITH
WHICH
TO
IMPLEMENT
YOUR
BOT
TOM
HALF
FOR
A
NORMAL
HARDWARE
DEVICE
TASKLETS
ARE
DYNAMICALLY
CREATED
EASY
TO
USE
AND
QUICK
MOREOVER
ALTHOUGH
THEIR
NAME
IS
MIND
NUMBINGLY
CONFUSING
IT
GROWS
ON
YOU
IT
IS
CUTE
DECLARING
YOUR
TASKLET
YOU
CAN
CREATE
TASKLETS
STATICALLY
OR
DYNAMICALLY
WHAT
OPTION
YOU
CHOOSE
DEPENDS
ON
WHETHER
YOU
HAVE
OR
WANT
A
DIRECT
OR
INDIRECT
REFERENCE
TO
THE
TASKLET
IF
YOU
ARE
GOING
TO
STATICALLY
CREATE
THE
TASKLET
AND
THUS
HAVE
A
DIRECT
REFERENCE
TO
IT
USE
ONE
OF
TWO
MACROS
IN
LINUX
INTERRUPT
H
NAME
FUNC
DATA
NAME
FUNC
DATA
BOTH
THESE
MACROS
STATICALLY
CREATE
A
STRUCT
WITH
THE
GIVEN
NAME
WHEN
THE
TASKLET
IS
SCHEDULED
THE
GIVEN
FUNCTION
FUNC
IS
EXECUTED
AND
PASSED
THE
ARGU
MENT
DATA
THE
DIFFERENCE
BETWEEN
THE
TWO
MACROS
IS
THE
INITIAL
REFERENCE
COUNT
THE
FIRST
MACRO
CREATES
THE
TASKLET
WITH
A
COUNT
OF
ZERO
AND
THE
TASKLET
IS
ENABLED
THE
SECOND
MACRO
SETS
COUNT
TO
ONE
AND
THE
TASKLET
IS
DISABLED
HERE
IS
AN
EXAMPLE
DEV
THIS
LINE
IS
EQUIVALENT
TO
STRUCT
NULL
DEV
THIS
CREATES
A
TASKLET
NAMED
ENABLED
WITH
AS
ITS
HAN
DLER
THE
VALUE
OF
DEV
IS
PASSED
TO
THE
HANDLER
WHEN
IT
IS
EXECUTED
TO
INITIALIZE
A
TASKLET
GIVEN
AN
INDIRECT
REFERENCE
A
POINTER
TO
A
DYNAMICALLY
CREATED
STRUCT
T
CALL
T
DEV
DYNAMICALLY
AS
OPPOSED
TO
STATICALLY
WRITING
YOUR
TASKLET
HANDLER
THE
TASKLET
HANDLER
MUST
MATCH
THE
CORRECT
PROTOTYPE
VOID
UNSIGNED
LONG
DATA
AS
WITH
SOFTIRQS
TASKLETS
CANNOT
SLEEP
THIS
MEANS
YOU
CANNOT
USE
SEMAPHORES
OR
OTHER
BLOCKING
FUNCTIONS
IN
A
TASKLET
TASKLETS
ALSO
RUN
WITH
ALL
INTERRUPTS
ENABLED
SO
YOU
MUST
TAKE
PRECAUTIONS
FOR
EXAMPLE
DISABLE
INTERRUPTS
AND
OBTAIN
A
LOCK
IF
YOUR
TASKLET
SHARES
DATA
WITH
AN
INTERRUPT
HANDLER
UNLIKE
SOFTIRQS
HOWEVER
TWO
OF
THE
SAME
TASKLETS
NEVER
RUN
CONCURRENTLY
ALTHOUGH
TWO
DIFFERENT
TASKLETS
CAN
RUN
AT
THE
SAME
TIME
ON
TWO
DIF
FERENT
PROCESSORS
IF
YOUR
TASKLET
SHARES
DATA
WITH
ANOTHER
TASKLET
OR
SOFTIRQ
YOU
NEED
TO
USE
PROPER
LOCKING
SEE
CHAPTER
AN
INTRODUCTION
TO
KERNEL
SYNCHRONIZATION
AND
CHAPTER
KERNEL
SYNCHRONIZATION
METHODS
SCHEDULING
YOUR
TASKLET
TO
SCHEDULE
A
TASKLET
FOR
EXECUTION
IS
CALLED
AND
PASSED
A
POINTER
TO
THE
RELEVANT
MARK
AS
PENDING
AFTER
A
TASKLET
IS
SCHEDULED
IT
RUNS
ONCE
AT
SOME
TIME
IN
THE
NEAR
FUTURE
IF
THE
SAME
TASKLET
IS
SCHEDULED
AGAIN
BEFORE
IT
HAS
HAD
A
CHANCE
TO
RUN
IT
STILL
RUNS
ONLY
ONCE
IF
IT
IS
ALREADY
RUNNING
FOR
EXAMPLE
ON
ANOTHER
PROCESSOR
THE
TASKLET
IS
RESCHEDULED
AND
RUNS
AGAIN
AS
AN
OPTIMIZATION
A
TASKLET
ALWAYS
RUNS
ON
THE
PROCESSOR
THAT
SCHEDULED
IT
MAK
ING
BETTER
USE
OF
THE
PROCESSOR
CACHE
YOU
HOPE
YOU
CAN
DISABLE
A
TASKLET
VIA
A
CALL
TO
WHICH
DISABLES
THE
GIVEN
TASKLET
IF
THE
TASKLET
IS
CURRENTLY
RUNNING
THE
FUNCTION
WILL
NOT
RETURN
UNTIL
IT
FINISHES
EXE
CUTING
ALTERNATIVELY
YOU
CAN
USE
WHICH
DISABLES
THE
GIVEN
TASKLET
BUT
DOES
NOT
WAIT
FOR
THE
TASKLET
TO
COMPLETE
PRIOR
TO
RETURNING
THIS
IS
USUALLY
NOT
SAFE
BECAUSE
YOU
CANNOT
ASSUME
THE
TASKLET
IS
NOT
STILL
RUNNING
A
CALL
TO
ENABLES
THE
TASKLET
THIS
FUNCTION
ALSO
MUST
BE
CALLED
BEFORE
A
TASKLET
CREATED
WITH
IS
USABLE
FOR
EXAMPLE
TASKLET
IS
NOW
DISABLED
WE
CAN
NOW
DO
STUFF
KNOWING
THAT
THE
TASKLET
CANNOT
RUN
TASKLET
IS
NOW
ENABLED
YOU
CAN
REMOVE
A
TASKLET
FROM
THE
PENDING
QUEUE
VIA
THIS
FUNCTION
RECEIVES
A
POINTER
AS
A
LONE
ARGUMENT
TO
THE
TASKLET
REMOVING
A
SCHEDULED
TASKLET
FROM
THE
QUEUE
IS
USEFUL
WHEN
DEALING
WITH
A
TASKLET
THAT
OFTEN
RESCHED
ULES
ITSELF
THIS
FUNCTION
FIRST
WAITS
FOR
THE
TASKLET
TO
FINISH
EXECUTING
AND
THEN
IT
REMOVES
THE
TASKLET
FROM
THE
QUEUE
NOTHING
STOPS
SOME
OTHER
CODE
FROM
RESCHEDULING
THE
TASKLET
OF
COURSE
THIS
FUNCTION
MUST
NOT
BE
USED
FROM
INTERRUPT
CONTEXT
BECAUSE
IT
SLEEPS
KSOFTIRQD
SOFTIRQ
AND
THUS
TASKLET
PROCESSING
IS
AIDED
BY
A
SET
OF
PER
PROCESSOR
KERNEL
THREADS
THESE
KERNEL
THREADS
HELP
IN
THE
PROCESSING
OF
SOFTIRQS
WHEN
THE
SYSTEM
IS
OVERWHELMED
WITH
SOFTIRQS
BECAUSE
TASKLETS
ARE
IMPLEMENTED
USING
SOFTIRQS
THE
FOLLOWING
DISCUSSION
APPLIES
EQUALLY
TO
SOFTIRQS
AND
TASKLETS
FOR
BREVITY
WE
WILL
REFER
MAINLY
TO
SOFTIRQS
AS
ALREADY
DESCRIBED
THE
KERNEL
PROCESSES
SOFTIRQS
IN
A
NUMBER
OF
PLACES
MOST
COM
MONLY
ON
RETURN
FROM
HANDLING
AN
INTERRUPT
SOFTIRQS
MIGHT
BE
RAISED
AT
HIGH
RATES
SUCH
AS
DURING
HEAVY
NETWORK
TRAFFIC
FURTHER
SOFTIRQ
FUNCTIONS
CAN
REACTIVATE
THEMSELVES
THAT
IS
WHILE
RUNNING
A
SOFTIRQ
CAN
RAISE
ITSELF
SO
THAT
IT
RUNS
AGAIN
FOR
EXAMPLE
THE
NETWORK
ING
SUBSYSTEM
SOFTIRQ
RAISES
ITSELF
THE
POSSIBILITY
OF
A
HIGH
FREQUENCY
OF
SOFTIRQS
IN
CON
JUNCTION
WITH
THEIR
CAPABILITY
TO
REMARK
THEMSELVES
ACTIVE
CAN
RESULT
IN
USER
SPACE
PROGRAMS
BEING
STARVED
OF
PROCESSOR
TIME
NOT
PROCESSING
THE
REACTIVATED
SOFTIRQS
IN
A
TIMELY
MANNER
HOWEVER
IS
UNACCEPTABLE
WHEN
SOFTIRQS
WERE
FIRST
DESIGNED
THIS
CAUSED
A
DILEMMA
THAT
NEEDED
FIXING
AND
NEITHER
OBVIOUS
SOLUTION
WAS
A
GOOD
ONE
FIRST
LET
LOOK
AT
EACH
OF
THE
TWO
OBVIOUS
SOLUTIONS
THE
FIRST
SOLUTION
IS
SIMPLY
TO
KEEP
PROCESSING
SOFTIRQS
AS
THEY
COME
IN
AND
TO
RECHECK
AND
REPROCESS
ANY
PENDING
SOFTIRQS
BEFORE
RETURNING
THIS
ENSURES
THAT
THE
KERNEL
PROCESSES
SOFTIRQS
IN
A
TIMELY
MANNER
AND
MOST
IMPORTANT
THAT
ANY
REACTIVATED
SOFTIRQS
ARE
ALSO
IMMEDIATELY
PROCESSED
THE
PROBLEM
LIES
IN
HIGH
LOAD
ENVIRONMENTS
IN
WHICH
MANY
SOFTIRQS
OCCUR
THAT
CONTINUALLY
REACTIVATE
THEMSELVES
THE
KERNEL
MIGHT
CONTINUALLY
SERVICE
SOFTIRQS
WITHOUT
ACCOMPLISHING
MUCH
ELSE
USER
SPACE
IS
NEGLECTED
INDEED
NOTHING
BUT
SOFTIRQS
AND
INTERRUPT
HANDLERS
RUN
AND
IN
TURN
THE
SYSTEM
USERS
GET
MAD
THIS
APPROACH
MIGHT
WORK
FINE
IF
THE
SYSTEM
IS
NEVER
UNDER
INTENSE
LOAD
IF
THE
SYSTEM
EXPERIENCES
MOD
ERATE
INTERRUPT
LEVELS
THIS
SOLUTION
IS
NOT
ACCEPTABLE
USER
SPACE
CANNOT
BE
STARVED
FOR
SIG
NIFICANT
PERIODS
THE
SECOND
SOLUTION
IS
NOT
TO
HANDLE
REACTIVATED
SOFTIRQS
ON
RETURN
FROM
INTERRUPT
THE
KERNEL
MERELY
LOOKS
AT
ALL
PENDING
SOFTIRQS
AND
EXECUTES
THEM
AS
NORMAL
IF
ANY
SOFTIRQS
REACTIVATE
THEMSELVES
HOWEVER
THEY
WILL
NOT
RUN
UNTIL
THE
NEXT
TIME
THE
KERNEL
HANDLES
PENDING
SOFTIRQS
THIS
IS
MOST
LIKELY
NOT
UNTIL
THE
NEXT
INTERRUPT
OCCURS
WHICH
CAN
EQUATE
TO
A
LENGTHY
AMOUNT
OF
TIME
BEFORE
ANY
NEW
OR
REACTIVATED
SOFTIRQS
ARE
EXECUTED
WORSE
ON
AN
OTHERWISE
IDLE
SYSTEM
IT
IS
BENEFICIAL
TO
PROCESS
THE
SOFTIRQS
RIGHT
AWAY
UNFORTU
NATELY
THIS
APPROACH
IS
OBLIVIOUS
TO
WHICH
PROCESSES
ARE
RUNNABLE
THEREFORE
ALTHOUGH
THIS
METHOD
PREVENTS
STARVING
USER
SPACE
IT
DOES
STARVE
THE
SOFTIRQS
AND
DOES
NOT
TAKE
GOOD
ADVANTAGE
OF
AN
IDLE
SYSTEM
IN
DESIGNING
SOFTIRQS
THE
KERNEL
DEVELOPERS
REALIZED
THAT
SOME
SORT
OF
COMPROMISE
WAS
NEEDED
THE
SOLUTION
ULTIMATELY
IMPLEMENTED
IN
THE
KERNEL
IS
TO
NOT
IMMEDIATELY
PROCESS
REACTIVATED
SOFTIRQS
INSTEAD
IF
THE
NUMBER
OF
SOFTIRQS
GROWS
EXCESSIVE
THE
KERNEL
WAKES
UP
A
FAMILY
OF
KERNEL
THREADS
TO
HANDLE
THE
LOAD
THE
KERNEL
THREADS
RUN
WITH
THE
LOWEST
POS
SIBLE
PRIORITY
NICE
VALUE
OF
WHICH
ENSURES
THEY
DO
NOT
RUN
IN
LIEU
OF
ANYTHING
IMPOR
TANT
THIS
CONCESSION
PREVENTS
HEAVY
SOFTIRQ
ACTIVITY
FROM
COMPLETELY
STARVING
USER
SPACE
OF
PROCESSOR
TIME
CONVERSELY
IT
ALSO
ENSURES
THAT
EXCESS
SOFTIRQS
DO
RUN
EVENTUALLY
FINALLY
THIS
SOLUTION
HAS
THE
ADDED
PROPERTY
THAT
ON
AN
IDLE
SYSTEM
THE
SOFTIRQS
ARE
HANDLED
RATHER
QUICKLY
BECAUSE
THE
KERNEL
THREADS
WILL
SCHEDULE
IMMEDIATELY
THERE
IS
ONE
THREAD
PER
PROCESSOR
THE
THREADS
ARE
EACH
NAMED
KSOFTIRQD
N
WHERE
N
IS
THE
PROCESSOR
NUMBER
ON
A
TWO
PROCESSOR
SYSTEM
YOU
WOULD
HAVE
KSOFTIRQD
AND
KSOFTIRQD
HAVING
A
THREAD
ON
EACH
PROCESSOR
ENSURES
AN
IDLE
PROCESSOR
IF
AVAILABLE
CAN
ALWAYS
SERVICE
SOFTIRQS
AFTER
THE
THREADS
ARE
INITIALIZED
THEY
RUN
A
TIGHT
LOOP
SIMILAR
TO
THIS
FOR
IF
CPU
SCHEDULE
WHILE
CPU
IF
SCHEDULE
IF
ANY
SOFTIRQS
ARE
PENDING
AS
REPORTED
BY
KSOFTIRQD
CALLS
TO
HANDLE
THEM
NOTE
THAT
IT
DOES
THIS
REPEATEDLY
TO
HANDLE
ANY
REACTIVATED
SOFTIRQS
TOO
AFTER
EACH
ITERATION
SCHEDULE
IS
CALLED
IF
NEEDED
TO
ENABLE
MORE
IMPOR
TANT
PROCESSES
TO
RUN
AFTER
ALL
PROCESSING
IS
COMPLETE
THE
KERNEL
THREAD
SETS
ITSELF
AND
INVOKES
THE
SCHEDULER
TO
SELECT
A
NEW
RUNNABLE
PROCESS
THE
SOFTIRQ
KERNEL
THREADS
ARE
AWAKENED
WHENEVER
DETECTS
AN
EXECUTED
KERNEL
THREAD
REACTIVATING
ITSELF
THE
OLD
BH
MECHANISM
ALTHOUGH
THE
OLD
BH
INTERFACE
THANKFULLY
IS
NO
LONGER
PRESENT
IN
IT
WAS
AROUND
FOR
A
LONG
TIME
SINCE
THE
EARLIEST
VERSIONS
OF
THE
KERNEL
BECAUSE
IT
HAD
IMMENSE
STAYING
POWER
IT
CERTAINLY
CARRIES
SOME
HISTORICAL
SIGNIFICANCE
THAT
REQUIRES
MORE
THAN
A
PASSING
LOOK
NOTHING
IN
THIS
BRIEF
SECTION
ACTUALLY
PERTAINS
TO
BUT
THE
HISTORY
IS
IMPORTANT
THE
BH
INTERFACE
IS
ANCIENT
AND
IT
SHOWED
EACH
BH
MUST
BE
STATICALLY
DEFINED
AND
THERE
ARE
A
MAXIMUM
OF
BECAUSE
THE
HANDLERS
MUST
ALL
BE
DEFINED
AT
COMPILE
TIME
MODULES
COULD
NOT
DIRECTLY
USE
THE
BH
INTERFACE
THEY
COULD
PIGGYBACK
OFF
AN
EXISTING
BH
HOWEVER
OVER
TIME
THIS
STATIC
REQUIREMENT
AND
THE
MAXIMUM
OF
BOTTOM
HALVES
BECAME
A
MAJOR
HINDRANCE
TO
THEIR
USE
ALL
BH
HANDLERS
ARE
STRICTLY
SERIALIZED
NO
TWO
BH
HANDLERS
EVEN
OF
DIFFERENT
TYPES
CAN
RUN
CONCURRENTLY
THIS
MADE
SYNCHRONIZATION
EASY
BUT
IT
WASN
T
BENEFICIAL
TO
MULTI
PROCESSOR
SCALABILITY
PERFORMANCE
ON
LARGE
SMP
MACHINES
WAS
SUB
PAR
A
DRIVER
USING
THE
BH
INTERFACE
DID
NOT
SCALE
WELL
TO
MULTIPLE
PROCESSORS
THE
NETWORKING
LAYER
IN
PARTICULAR
SUFFERED
OTHER
THAN
THESE
ATTRIBUTES
THE
BH
MECHANISM
IS
SIMILAR
TO
TASKLETS
IN
FACT
THE
BH
INTERFACE
WAS
IMPLEMENTED
ON
TOP
OF
TASKLETS
IN
THE
POSSIBLE
BOTTOM
HALVES
WERE
REPRESENTED
BY
CONSTANTS
DEFINED
IN
LINUX
INTERRUPT
H
TO
MARK
A
BH
AS
PENDING
THE
FUNCTION
WAS
CALLED
AND
PASSED
THE
NUMBER
OF
THE
BH
IN
THIS
IN
TURN
SCHEDULED
THE
BH
TASKLET
TO
RUN
BEFORE
THE
KERNEL
THE
BH
MECHANISM
WAS
INDEPENDENTLY
IMPLEMENTED
AND
DID
NOT
RELY
ON
ANY
LOWER
LEVEL
BOTTOM
HALF
MECHA
NISM
MUCH
AS
SOFTIRQS
ARE
IMPLEMENTED
TODAY
BECAUSE
OF
THE
SHORTCOMINGS
OF
THIS
FORM
OF
BOTTOM
HALF
KERNEL
DEVELOPERS
INTRODUCED
TASK
QUEUES
TO
REPLACE
BOTTOM
HALVES
TASK
QUEUES
NEVER
ACCOMPLISHED
THIS
GOAL
ALTHOUGH
THEY
DID
WIN
MANY
NEW
USERS
IN
THE
SOFTIRQ
AND
TASKLET
MECHANISMS
WERE
INTRODUCED
TO
PUT
AN
END
TO
THE
BH
THE
BH
MECHANISM
WAS
REIMPLEMENTED
ON
TOP
OF
TASKLETS
UNFORTUNATELY
IT
WAS
COMPLICATED
TO
PORT
BOTTOM
HALVES
FROM
THE
BH
INTERFACE
TO
TASKLETS
OR
SOFTIRQS
BECAUSE
OF
THE
WEAKER
INHERENT
SERIALIZATION
OF
THE
NEW
INTERFACES
DURING
HOWEVER
THE
CONVERSION
DID
OCCUR
WHEN
TIMERS
AND
SCSI
THE
REMAINING
BH
USERS
FINALLY
MOVED
OVER
TO
SOFTIRQS
THE
KERNEL
DEVELOPERS
SUMMARILY
REMOVED
THE
BH
INTER
FACE
GOOD
RIDDANCE
BH
THAT
IS
THE
WEAKER
SERIALIZATION
WAS
BENEFICIAL
TO
PERFORMANCE
BUT
ALSO
HARDER
TO
PROGRAM
CONVERTING
A
BH
TO
A
TASKLET
FOR
EXAMPLE
REQUIRED
CAREFUL
THINKING
IS
THIS
CODE
SAFE
RUNNING
AT
THE
SAME
TIME
AS
ANY
OTHER
TASKLET
WHEN
FINALLY
CONVERTED
HOWEVER
THE
PERFORMANCE
WAS
WORTH
IT
WORK
QUEUES
WORK
QUEUES
ARE
A
DIFFERENT
FORM
OF
DEFERRING
WORK
FROM
WHAT
WE
HAVE
LOOKED
AT
SO
FAR
WORK
QUEUES
DEFER
WORK
INTO
A
KERNEL
THREAD
THIS
BOTTOM
HALF
ALWAYS
RUNS
IN
PROCESS
CONTEXT
THUS
CODE
DEFERRED
TO
A
WORK
QUEUE
HAS
ALL
THE
USUAL
BENEFITS
OF
PROCESS
CONTEXT
MOST
IMPORTANT
WORK
QUEUES
ARE
SCHEDULABLE
AND
CAN
THEREFORE
SLEEP
NORMALLY
IT
IS
EASY
TO
DECIDE
BETWEEN
USING
WORK
QUEUES
AND
SOFTIRQS
TASKLETS
IF
THE
DEFERRED
WORK
NEEDS
TO
SLEEP
WORK
QUEUES
ARE
USED
IF
THE
DEFERRED
WORK
NEED
NOT
SLEEP
SOFTIRQS
OR
TASKLETS
ARE
USED
INDEED
THE
USUAL
ALTERNATIVE
TO
WORK
QUEUES
IS
KERNEL
THREADS
BECAUSE
THE
KERNEL
DEVELOPERS
FROWN
UPON
CREATING
A
NEW
KERNEL
THREAD
AND
IN
SOME
LOCALES
IT
IS
A
PUNISHABLE
OFFENSE
WORK
QUEUES
ARE
STRONGLY
PREFERRED
THEY
ARE
REALLY
EASY
TO
USE
TOO
IF
YOU
NEED
A
SCHEDULABLE
ENTITY
TO
PERFORM
YOUR
BOTTOM
HALF
PROCESSING
YOU
NEED
WORK
QUEUES
THEY
ARE
THE
ONLY
BOTTOM
HALF
MECHANISMS
THAT
RUN
IN
PROCESS
CONTEXT
AND
THUS
THE
ONLY
ONES
THAT
CAN
SLEEP
THIS
MEANS
THEY
ARE
USEFUL
FOR
SITUATIONS
IN
WHICH
YOU
NEED
TO
ALLOCATE
A
LOT
OF
MEMORY
OBTAIN
A
SEMAPHORE
OR
PERFORM
BLOCK
I
O
IF
YOU
DO
NOT
NEED
A
KERNEL
THREAD
TO
HANDLE
YOUR
DEFERRED
WORK
CONSIDER
A
TASKLET
INSTEAD
IMPLEMENTING
WORK
QUEUES
IN
ITS
MOST
BASIC
FORM
THE
WORK
QUEUE
SUBSYSTEM
IS
AN
INTERFACE
FOR
CREATING
KERNEL
THREADS
TO
HANDLE
WORK
QUEUED
FROM
ELSEWHERE
THESE
KERNEL
THREADS
ARE
CALLED
WORKER
THREADS
WORK
QUEUES
LET
YOUR
DRIVER
CREATE
A
SPECIAL
WORKER
THREAD
TO
HANDLE
DEFERRED
WORK
THE
WORK
QUEUE
SUBSYSTEM
HOWEVER
IMPLEMENTS
AND
PROVIDES
A
DEFAULT
WORKER
THREAD
FOR
HANDLING
WORK
THEREFORE
IN
ITS
MOST
COMMON
FORM
A
WORK
QUEUE
IS
A
SIMPLE
INTERFACE
FOR
DEFERRING
WORK
TO
A
GENERIC
KERNEL
THREAD
THE
DEFAULT
WORKER
THREADS
ARE
CALLED
EVENTS
N
WHERE
N
IS
THE
PROCESSOR
NUMBER
THERE
IS
ONE
PER
PROCESSOR
FOR
EXAMPLE
ON
A
UNIPROCESSOR
SYSTEM
THERE
IS
ONE
THREAD
EVENTS
A
DUAL
PROCESSOR
SYSTEM
WOULD
ADDITIONALLY
HAVE
AN
EVENTS
THREAD
THE
DEFAULT
WORKER
THREAD
HANDLES
DEFERRED
WORK
FROM
MULTIPLE
LOCATIONS
MANY
DRIVERS
IN
THE
KERNEL
DEFER
THEIR
BOTTOM
HALF
WORK
TO
THE
DEFAULT
THREAD
UNLESS
A
DRIVER
OR
SUBSYSTEM
HAS
A
STRONG
REQUIREMENT
FOR
CREATING
ITS
OWN
THREAD
THE
DEFAULT
THREAD
IS
PREFERRED
NOTHING
STOPS
CODE
FROM
CREATING
ITS
OWN
WORKER
THREAD
HOWEVER
THIS
MIGHT
BE
ADVANTAGEOUS
IF
YOU
PERFORM
LARGE
AMOUNTS
OF
PROCESSING
IN
THE
WORKER
THREAD
PROCESSOR
INTENSE
AND
PERFORMANCE
CRITICAL
WORK
MIGHT
BENEFIT
FROM
ITS
OWN
THREAD
THIS
ALSO
LIGHT
ENS
THE
LOAD
ON
THE
DEFAULT
THREADS
WHICH
PREVENTS
STARVING
THE
REST
OF
THE
QUEUED
WORK
DATA
STRUCTURES
REPRESENTING
THE
THREADS
THE
WORKER
THREADS
ARE
REPRESENTED
BY
THE
STRUCTURE
THE
EXTERNALLY
VISIBLE
WORKQUEUE
ABSTRACTION
IS
AN
ARRAY
OF
PER
CPU
WORKQUEUES
STRUCT
STRUCT
STRUCT
LIST
CONST
CHAR
NAME
INT
SINGLETHREAD
INT
FREEZEABLE
INT
RT
THIS
STRUCTURE
DEFINED
IN
KERNEL
WORKQUEUE
C
CONTAINS
AN
ARRAY
OF
STRUCT
ONE
PER
POSSIBLE
PROCESSOR
ON
THE
SYSTEM
BECAUSE
THE
WORKER
THREADS
EXIST
ON
EACH
PROCESSOR
IN
THE
SYSTEM
THERE
IS
ONE
OF
THESE
STRUCTURES
PER
WORKER
THREAD
PER
PROCESSOR
ON
A
GIVEN
MACHINE
THE
IS
THE
CORE
DATA
STRUCTURE
AND
IS
ALSO
DEFINED
IN
KERNEL
WORKQUEUE
C
STRUCT
LOCK
LOCK
PROTECTING
THIS
STRUCTURE
STRUCT
WORKLIST
LIST
OF
WORK
STRUCT
STRUCT
WQ
ASSOCIATED
THREAD
ASSOCIATED
THREAD
NOTE
THAT
EACH
TYPE
OF
WORKER
THREAD
HAS
ONE
ASSOCIATED
TO
IT
INSIDE
THERE
IS
ONE
FOR
EVERY
THREAD
AND
THUS
EVERY
PROCESSOR
BECAUSE
THERE
IS
ONE
WORKER
THREAD
ON
EACH
PROCESSOR
DATA
STRUCTURES
REPRESENTING
THE
WORK
ALL
WORKER
THREADS
ARE
IMPLEMENTED
AS
NORMAL
KERNEL
THREADS
RUNNING
THE
FUNCTION
AFTER
INITIAL
SETUP
THIS
FUNCTION
ENTERS
AN
INFINITE
LOOP
AND
GOES
TO
SLEEP
WHEN
WORK
IS
QUEUED
THE
THREAD
IS
AWAKENED
AND
PROCESSES
THE
WORK
WHEN
THERE
IS
NO
WORK
LEFT
TO
PROCESS
IT
GOES
BACK
TO
SLEEP
THE
WORK
IS
REPRESENTED
BY
THE
STRUCTURE
DEFINED
IN
LINUX
WORKQUEUE
H
STRUCT
DATA
STRUCT
ENTRY
FUNC
THESE
STRUCTURES
ARE
STRUNG
INTO
A
LINKED
LIST
ONE
FOR
EACH
TYPE
OF
QUEUE
ON
EACH
PROCESSOR
FOR
EXAMPLE
THERE
IS
ONE
LIST
OF
DEFERRED
WORK
FOR
THE
GENERIC
THREAD
PER
PROCESSOR
WHEN
A
WORKER
THREAD
WAKES
UP
IT
RUNS
ANY
WORK
IN
ITS
LIST
AS
IT
COMPLETES
WORK
IT
REMOVES
THE
CORRESPONDING
ENTRIES
FROM
THE
LINKED
LIST
WHEN
THE
LIST
IS
EMPTY
IT
GOES
BACK
TO
SLEEP
LET
LOOK
AT
THE
HEART
OF
SIMPLIFIED
FOR
CWQ
WAIT
IF
CWQ
WORKLIST
SCHEDULE
CWQ
WAIT
CWQ
THIS
FUNCTION
PERFORMS
THE
FOLLOWING
FUNCTIONS
IN
AN
INFINITE
LOOP
THE
THREAD
MARKS
ITSELF
SLEEPING
THE
TASK
STATE
IS
SET
TO
AND
ADDS
ITSELF
TO
A
WAIT
QUEUE
IF
THE
LINKED
LIST
OF
WORK
IS
EMPTY
THE
THREAD
CALLS
SCHEDULE
AND
GOES
TO
SLEEP
IF
THE
LIST
IS
NOT
EMPTY
THE
THREAD
DOES
NOT
GO
TO
SLEEP
INSTEAD
IT
MARKS
ITSELF
AND
REMOVES
ITSELF
FROM
THE
WAIT
QUEUE
IF
THE
LIST
IS
NONEMPTY
THE
THREAD
CALLS
TO
PERFORM
THE
DEFERRED
WORK
THE
FUNCTION
IN
TURN
ACTUALLY
PERFORMS
THE
DEFERRED
WORK
WHILE
CWQ
WORKLIST
STRUCT
WORK
F
VOID
DATA
WORK
CWQ
WORKLIST
NEXT
STRUCT
ENTRY
F
WORK
FUNC
CWQ
WORKLIST
NEXT
WORK
F
WORK
THIS
FUNCTION
LOOPS
OVER
EACH
ENTRY
IN
THE
LINKED
LIST
OF
PENDING
WORK
AND
EXECUTES
THE
FUNC
MEMBER
OF
THE
FOR
EACH
ENTRY
IN
THE
LINKED
LIST
WHILE
THE
LIST
IS
NOT
EMPTY
IT
GRABS
THE
NEXT
ENTRY
IN
THE
LIST
IT
RETRIEVES
THE
FUNCTION
THAT
SHOULD
BE
CALLED
FUNC
AND
ITS
ARGUMENT
DATA
IT
REMOVES
THIS
ENTRY
FROM
THE
LIST
AND
CLEARS
THE
PENDING
BIT
IN
THE
STRUCTURE
ITSELF
IT
INVOKES
THE
FUNCTION
REPEAT
WORK
QUEUE
IMPLEMENTATION
SUMMARY
THE
RELATIONSHIP
BETWEEN
THE
DIFFERENT
DATA
STRUCTURES
IS
ADMITTEDLY
A
BIT
CONVOLUTED
FIGURE
PROVIDES
A
GRAPHICAL
EXAMPLE
WHICH
SHOULD
BRING
IT
ALL
TOGETHER
WOR
STRUC
TURE
UCT
FIGURE
THE
RELATIONSHIP
BETWEEN
WORK
WORK
QUEUES
AND
THE
WORKER
THREADS
AT
THE
HIGHEST
LEVEL
THERE
ARE
WORKER
THREADS
THERE
CAN
BE
MULTIPLE
TYPES
OF
WORKER
THREADS
THERE
IS
ONE
WORKER
THREAD
PER
PROCESSOR
OF
A
GIVEN
TYPE
PARTS
OF
THE
KERNEL
CAN
CREATE
WORKER
THREADS
AS
NEEDED
BY
DEFAULT
THERE
IS
THE
EVENTS
WORKER
THREAD
EACH
WORKER
THREAD
IS
REPRESENTED
BY
THE
STRUCTURE
THE
STRUCTURE
REPRESENTS
ALL
THE
WORKER
THREADS
OF
A
GIVEN
TYPE
FOR
EXAMPLE
ASSUME
THAT
IN
ADDITION
TO
THE
GENERIC
EVENTS
WORKER
TYPE
YOU
ALSO
CREATE
A
FALCON
WORKER
TYPE
ALSO
ASSUME
YOU
HAVE
A
FOUR
PROCESSOR
COMPUTER
THEN
THERE
ARE
FOUR
EVENTS
THREADS
AND
THUS
FOUR
STRUCTURES
AND
FOUR
FALCON
THREADS
AND
THUS
ANOTHER
FOUR
STRUCTURES
THERE
IS
ONE
FOR
THE
EVENTS
TYPE
AND
ONE
FOR
THE
FALCON
TYPE
NOW
LET
APPROACH
FROM
THE
LOWEST
LEVEL
WHICH
STARTS
WITH
WORK
YOUR
DRIVER
CREATES
WORK
WHICH
IT
WANTS
TO
DEFER
TO
LATER
THE
STRUCTURE
REPRESENTS
THIS
WORK
AMONG
OTHER
THINGS
THIS
STRUCTURE
CONTAINS
A
POINTER
TO
THE
FUNCTION
THAT
HANDLES
THE
DEFERRED
WORK
THE
WORK
IS
SUBMITTED
TO
A
SPECIFIC
WORKER
THREAD
IN
THIS
CASE
A
SPECIFIC
FALCON
THREAD
THE
WORKER
THREAD
THEN
WAKES
UP
AND
PERFORMS
THE
QUEUED
WORK
MOST
DRIVERS
USE
THE
EXISTING
DEFAULT
WORKER
THREADS
NAMED
EVENTS
THEY
ARE
EASY
AND
SIMPLE
SOME
MORE
SERIOUS
SITUATIONS
HOWEVER
DEMAND
THEIR
OWN
WORKER
THREADS
THE
XFS
FILESYSTEM
FOR
EXAMPLE
CREATES
TWO
NEW
TYPES
OF
WORKER
THREADS
USING
WORK
QUEUES
USING
WORK
QUEUES
IS
EASY
WE
COVER
THE
DEFAULT
EVENTS
QUEUE
FIRST
AND
THEN
LOOK
AT
CREAT
ING
NEW
WORKER
THREADS
CREATING
WORK
THE
FIRST
STEP
IS
ACTUALLY
CREATING
SOME
WORK
TO
DEFER
TO
CREATE
THE
STRUCTURE
STATICALLY
AT
RUNTIME
USE
NAME
VOID
FUNC
VOID
VOID
DATA
THIS
STATICALLY
CREATES
A
STRUCTURE
NAMED
NAME
WITH
HANDLER
FUNCTION
FUNC
AND
ARGUMENT
DATA
ALTERNATIVELY
YOU
CAN
CREATE
WORK
AT
RUNTIME
VIA
A
POINTER
STRUCT
WORK
VOID
FUNC
VOID
VOID
DATA
THIS
DYNAMICALLY
INITIALIZES
THE
WORK
QUEUE
POINTED
TO
BY
WORK
WITH
HANDLER
FUNCTION
FUNC
AND
ARGUMENT
DATA
YOUR
WORK
QUEUE
HANDLER
THE
PROTOTYPE
FOR
THE
WORK
QUEUE
HANDLER
IS
VOID
VOID
DATA
A
WORKER
THREAD
EXECUTES
THIS
FUNCTION
AND
THUS
THE
FUNCTION
RUNS
IN
PROCESS
CONTEXT
BY
DEFAULT
INTERRUPTS
ARE
ENABLED
AND
NO
LOCKS
ARE
HELD
IF
NEEDED
THE
FUNCTION
CAN
SLEEP
NOTE
THAT
DESPITE
RUNNING
IN
PROCESS
CONTEXT
THE
WORK
HANDLERS
CANNOT
ACCESS
USER
SPACE
MEMORY
BECAUSE
THERE
IS
NO
ASSOCIATED
USER
SPACE
MEMORY
MAP
FOR
KERNEL
THREADS
THE
KERNEL
CAN
ACCESS
USER
MEMORY
ONLY
WHEN
RUNNING
ON
BEHALF
OF
A
USER
SPACE
PROCESS
SUCH
AS
WHEN
EXECUTING
A
SYSTEM
CALL
ONLY
THEN
IS
USER
MEMORY
MAPPED
IN
LOCKING
BETWEEN
WORK
QUEUES
OR
OTHER
PARTS
OF
THE
KERNEL
IS
HANDLED
JUST
AS
WITH
ANY
OTHER
PROCESS
CONTEXT
CODE
THIS
MAKES
WRITING
WORK
HANDLERS
MUCH
EASIER
THE
NEXT
TWO
CHAPTERS
COVER
LOCKING
SCHEDULING
WORK
NOW
THAT
THE
WORK
IS
CREATED
WE
CAN
SCHEDULE
IT
TO
QUEUE
A
GIVEN
WORK
HANDLER
FUNC
TION
WITH
THE
DEFAULT
EVENTS
WORKER
THREADS
SIMPLY
CALL
WORK
THE
WORK
IS
SCHEDULED
IMMEDIATELY
AND
IS
RUN
AS
SOON
AS
THE
EVENTS
WORKER
THREAD
ON
THE
CURRENT
PROCESSOR
WAKES
UP
SOMETIMES
YOU
DO
NOT
WANT
THE
WORK
TO
EXECUTE
IMMEDIATELY
BUT
INSTEAD
AFTER
SOME
DELAY
IN
THOSE
CASES
YOU
CAN
SCHEDULE
WORK
TO
EXECUTE
AT
A
GIVEN
TIME
IN
THE
FUTURE
WORK
DELAY
IN
THIS
CASE
THE
REPRESENTED
BY
WORK
WILL
NOT
EXECUTE
FOR
AT
LEAST
DELAY
TIMER
TICKS
INTO
THE
FUTURE
USING
TICKS
AS
A
UNIT
OF
TIME
IS
COVERED
IN
CHAPTER
FLUSHING
WORK
QUEUED
WORK
IS
EXECUTED
WHEN
THE
WORKER
THREAD
NEXT
WAKES
UP
SOMETIMES
YOU
NEED
TO
ENSURE
THAT
A
GIVEN
BATCH
OF
WORK
HAS
COMPLETED
BEFORE
CONTINUING
THIS
IS
ESPECIALLY
IMPORTANT
FOR
MODULES
WHICH
ALMOST
CERTAINLY
WANT
TO
CALL
THIS
FUNCTION
BEFORE
UNLOAD
ING
OTHER
PLACES
IN
THE
KERNEL
ALSO
MIGHT
NEED
TO
MAKE
CERTAIN
NO
WORK
IS
PENDING
TO
PREVENT
RACE
CONDITIONS
FOR
THESE
NEEDS
THERE
IS
A
FUNCTION
TO
FLUSH
A
GIVEN
WORK
QUEUE
VOID
VOID
THIS
FUNCTION
WAITS
UNTIL
ALL
ENTRIES
IN
THE
QUEUE
ARE
EXECUTED
BEFORE
RETURNING
WHILE
WAITING
FOR
ANY
PENDING
WORK
TO
EXECUTE
THE
FUNCTION
SLEEPS
THEREFORE
YOU
CAN
CALL
IT
ONLY
FROM
PROCESS
CONTEXT
NOTE
THAT
THIS
FUNCTION
DOES
NOT
CANCEL
ANY
DELAYED
WORK
THAT
IS
ANY
WORK
THAT
WAS
SCHEDULED
VIA
AND
WHOSE
DELAY
IS
NOT
YET
UP
IS
NOT
FLUSHED
VIA
TO
CANCEL
DELAYED
WORK
CALL
INT
STRUCT
WORK
THIS
FUNCTION
CANCELS
THE
PENDING
WORK
IF
ANY
ASSOCIATED
WITH
THE
GIVEN
CREATING
NEW
WORK
QUEUES
IF
THE
DEFAULT
QUEUE
IS
INSUFFICIENT
FOR
YOUR
NEEDS
YOU
CAN
CREATE
A
NEW
WORK
QUEUE
AND
CORRESPONDING
WORKER
THREADS
BECAUSE
THIS
CREATES
ONE
WORKER
THREAD
PER
PROCESSOR
YOU
SHOULD
CREATE
UNIQUE
WORK
QUEUES
ONLY
IF
YOUR
CODE
NEEDS
THE
PERFORMANCE
OF
A
UNIQUE
SET
OF
THREADS
YOU
CREATE
A
NEW
WORK
QUEUE
AND
THE
ASSOCIATED
WORKER
THREADS
VIA
A
SIMPLE
FUNCTION
STRUCT
CONST
CHAR
NAME
THE
PARAMETER
NAME
IS
USED
TO
NAME
THE
KERNEL
THREADS
FOR
EXAMPLE
THE
DEFAULT
EVENTS
QUEUE
IS
CREATED
VIA
STRUCT
EVENTS
THIS
FUNCTION
CREATES
ALL
THE
WORKER
THREADS
ONE
FOR
EACH
PROCESSOR
IN
THE
SYSTEM
AND
PREPARES
THEM
TO
HANDLE
WORK
CREATING
WORK
IS
HANDLED
IN
THE
SAME
MANNER
REGARDLESS
OF
THE
QUEUE
TYPE
AFTER
THE
WORK
IS
CREATED
THE
FOLLOWING
FUNCTIONS
ARE
ANALOGOUS
TO
AND
EXCEPT
THAT
THEY
WORK
ON
THE
GIVEN
WORK
QUEUE
AND
NOT
THE
DEFAULT
EVENTS
QUEUE
INT
STRUCT
WQ
STRUCT
WORK
INT
STRUCT
WQ
STRUCT
WORK
UNSIGNED
LONG
DELAY
FINALLY
YOU
CAN
FLUSH
A
WAIT
QUEUE
VIA
A
CALL
TO
THE
FUNCTION
STRUCT
WQ
AS
PREVIOUSLY
DISCUSSED
THIS
FUNCTION
WORKS
IDENTICALLY
TO
EXCEPT
THAT
IT
WAITS
FOR
THE
GIVEN
QUEUE
TO
EMPTY
BEFORE
RETURNING
THE
OLD
TASK
QUEUE
MECHANISM
LIKE
THE
BH
INTERFACE
WHICH
GAVE
WAY
TO
SOFTIRQS
AND
TASKLETS
THE
WORK
QUEUE
INTERFACE
GREW
OUT
OF
SHORTCOMINGS
IN
THE
TASK
QUEUE
INTERFACE
THE
TASK
QUEUE
INTERFACE
OFTEN
CALLED
SIMPLY
TQ
IN
THE
KERNEL
LIKE
TASKLETS
ALSO
HAS
NOTHING
TO
DO
WITH
TASKS
IN
THE
PROCESS
SENSE
THE
USERS
OF
THE
TASK
QUEUE
INTERFACE
WERE
RIPPED
IN
HALF
DURING
THE
DEVELOPMENT
KERNEL
HALF
OF
THE
USERS
WERE
CONVERTED
TO
TASKLETS
WHEREAS
THE
OTHER
HALF
CONTINUED
USING
THE
TASK
QUEUE
INTERFACE
WHAT
WAS
LEFT
OF
THE
TASK
QUEUE
INTERFACE
THEN
BECAME
THE
WORK
QUEUE
INTERFACE
BRIEFLY
LOOKING
AT
TASK
QUEUES
WHICH
WERE
AROUND
FOR
SOME
TIME
IS
A
USEFUL
HISTORICAL
EXERCISE
TASK
QUEUES
WORK
BY
DEFINING
A
BUNCH
OF
QUEUES
THE
QUEUES
HAVE
NAMES
SUCH
AS
THE
SCHEDULER
QUEUE
THE
IMMEDIATE
QUEUE
OR
THE
TIMER
QUEUE
EACH
QUEUE
IS
RUN
AT
A
SPECIFIC
POINT
IN
THE
KERNEL
A
KERNEL
THREAD
KEVENTD
RAN
THE
WORK
ASSOCIATED
WITH
THE
SCHEDULER
QUEUE
THIS
WAS
THE
PRECURSOR
TO
THE
FULL
WORK
QUEUE
INTERFACE
THE
TIMER
QUEUE
WAS
RUN
AT
EACH
TICK
OF
THE
SYSTEM
TIMER
AND
THE
IMMEDIATE
QUEUE
WAS
RUN
IN
A
HANDFUL
OF
DIFFER
ENT
PLACES
TO
ENSURE
IT
WAS
RUN
IMMEDIATELY
HACK
THERE
WERE
OTHER
QUEUES
TOO
ADDI
TIONALLY
YOU
COULD
DYNAMICALLY
CREATE
NEW
QUEUES
ALL
THIS
MIGHT
SOUND
USEFUL
BUT
THE
REALITY
IS
THAT
THE
TASK
QUEUE
INTERFACE
WAS
A
MESS
ALL
THE
QUEUES
WERE
ESSENTIALLY
ARBITRARY
ABSTRACTIONS
SCATTERED
ABOUT
THE
KERNEL
AS
IF
THROWN
IN
THE
AIR
AND
KEPT
WHERE
THEY
LANDED
THE
ONLY
MEANINGFUL
QUEUE
WAS
THE
SCHED
ULER
QUEUE
WHICH
PROVIDED
THE
ONLY
WAY
TO
DEFER
WORK
TO
PROCESS
CONTEXT
THE
OTHER
GOOD
THING
ABOUT
TASK
QUEUES
WAS
THE
BRAIN
DEAD
SIMPLE
INTERFACE
DESPITE
THE
MYRIAD
OF
QUEUES
AND
THE
ARBITRARY
RULES
ABOUT
WHEN
THEY
RAN
THE
INTERFACE
WAS
AS
SIMPLE
AS
POSSIBLE
BUT
THAT
ABOUT
IT
THE
REST
OF
TASK
QUEUES
NEEDED
TO
GO
BOTTOM
HALF
NAMES
ARE
APPARENTLY
A
CONSPIRACY
TO
CONFUSE
NEW
KERNEL
DEVELOPERS
SERIOUSLY
THESE
NAMES
ARE
AWFUL
THE
VARIOUS
TASK
QUEUE
USERS
WERE
CONVERTED
TO
OTHER
BOTTOM
HALF
MECHANISMS
MOST
OF
THEM
SWITCHED
TO
TASKLETS
THE
SCHEDULER
QUEUE
USERS
STUCK
AROUND
FINALLY
THE
KEVENTD
CODE
WAS
GENERALIZED
INTO
THE
EXCELLENT
WORK
QUEUE
MECHANISM
WE
HAVE
TODAY
AND
TASK
QUEUES
WERE
FINALLY
RIPPED
OUT
OF
THE
KERNEL
WHICH
BOTTOM
HALF
SHOULD
I
USE
THE
DECISION
OVER
WHICH
BOTTOM
HALF
TO
USE
IS
IMPORTANT
IN
THE
CURRENT
KERNEL
YOU
HAVE
THREE
CHOICES
SOFTIRQS
TASKLETS
AND
WORK
QUEUES
TASKLETS
ARE
BUILT
ON
SOFTIRQS
AND
THEREFORE
BOTH
ARE
SIMILAR
THE
WORK
QUEUE
MECHANISM
IS
AN
ENTIRELY
DIFFERENT
CREATURE
AND
IS
BUILT
ON
KERNEL
THREADS
SOFTIRQS
BY
DESIGN
PROVIDE
THE
LEAST
SERIALIZATION
THIS
REQUIRES
SOFTIRQ
HANDLERS
TO
GO
THROUGH
EXTRA
STEPS
TO
ENSURE
THAT
SHARED
DATA
IS
SAFE
BECAUSE
TWO
OR
MORE
SOFTIRQS
OF
THE
SAME
TYPE
MAY
RUN
CONCURRENTLY
ON
DIFFERENT
PROCESSORS
IF
THE
CODE
IN
QUESTION
IS
ALREADY
HIGHLY
THREADED
SUCH
AS
IN
A
NETWORKING
SUBSYSTEM
THAT
IS
CHEST
DEEP
IN
PER
PROCESSOR
VARIABLES
SOFTIRQS
MAKE
A
GOOD
CHOICE
THEY
ARE
CERTAINLY
THE
FASTEST
ALTERNATIVE
FOR
TIMING
CRITICAL
AND
HIGH
FREQUENCY
USES
TASKLETS
MAKE
MORE
SENSE
IF
THE
CODE
IS
NOT
FINELY
THREADED
THEY
HAVE
A
SIMPLER
INTER
FACE
AND
BECAUSE
TWO
TASKLETS
OF
THE
SAME
TYPE
MIGHT
NOT
RUN
CONCURRENTLY
THEY
ARE
EASIER
TO
IMPLEMENT
TASKLETS
ARE
EFFECTIVELY
SOFTIRQS
THAT
DO
NOT
RUN
CONCURRENTLY
A
DRIVER
DEVEL
OPER
SHOULD
ALWAYS
CHOOSE
TASKLETS
OVER
SOFTIRQS
UNLESS
PREPARED
TO
UTILIZE
PER
PROCESSOR
VARIABLES
OR
SIMILAR
MAGIC
TO
ENSURE
THAT
THE
SOFTIRQ
CAN
SAFELY
RUN
CONCURRENTLY
ON
MULTI
PLE
PROCESSORS
IF
YOUR
DEFERRED
WORK
NEEDS
TO
RUN
IN
PROCESS
CONTEXT
YOUR
ONLY
CHOICE
OF
THE
THREE
IS
WORK
QUEUES
IF
PROCESS
CONTEXT
IS
NOT
A
REQUIREMENT
SPECIFICALLY
IF
YOU
HAVE
NO
NEED
TO
SLEEP
SOFTIRQS
OR
TASKLETS
ARE
PERHAPS
BETTER
SUITED
WORK
QUEUES
INVOLVE
THE
HIGHEST
OVER
HEAD
BECAUSE
THEY
INVOLVE
KERNEL
THREADS
AND
THEREFORE
CONTEXT
SWITCHING
THIS
IS
NOT
TO
SAY
THAT
THEY
ARE
INEFFICIENT
BUT
IN
LIGHT
OF
THOUSANDS
OF
INTERRUPTS
HITTING
PER
SECOND
AS
THE
NETWORKING
SUBSYSTEM
MIGHT
EXPERIENCE
OTHER
METHODS
MAKE
MORE
SENSE
FOR
MOST
SITUATIONS
HOWEVER
WORK
QUEUES
ARE
SUFFICIENT
IN
TERMS
OF
EASE
OF
USE
WORK
QUEUES
TAKE
THE
CROWN
USING
THE
DEFAULT
EVENTS
QUEUE
IS
CHILD
PLAY
NEXT
COME
TASKLETS
WHICH
ALSO
HAVE
A
SIMPLE
INTERFACE
COMING
IN
LAST
ARE
SOFTIRQS
WHICH
NEED
TO
BE
STATICALLY
CREATED
AND
REQUIRE
CAREFUL
THINKING
WITH
THEIR
IMPLEMENTATION
TABLE
IS
A
COMPARISON
BETWEEN
THE
THREE
BOTTOM
HALF
INTERFACES
TABLE
BOTTOM
HALF
COMPARISON
BOTTOM
HALF
CONTEXT
INHERENT
SERIALIZATION
SOFTIRQ
INTERRUPT
NONE
TASKLET
INTERRUPT
AGAINST
THE
SAME
TASKLET
WORK
QUEUES
PROCESS
NONE
SCHEDULED
AS
PROCESS
CONTEXT
IN
SHORT
NORMAL
DRIVER
WRITERS
HAVE
TWO
CHOICES
FIRST
DO
YOU
NEED
A
SCHEDULABLE
ENTITY
TO
PERFORM
YOUR
DEFERRED
WORK
FUNDAMENTALLY
DO
YOU
NEED
TO
SLEEP
FOR
ANY
REA
SON
THEN
WORK
QUEUES
ARE
YOUR
ONLY
OPTION
OTHERWISE
TASKLETS
ARE
PREFERRED
ONLY
IF
SCALABILITY
BECOMES
A
CONCERN
DO
YOU
INVESTIGATE
SOFTIRQS
LOCKING
BETWEEN
THE
BOTTOM
HALVES
WE
HAVE
NOT
DISCUSSED
LOCKING
YET
WHICH
IS
SUCH
A
FUN
AND
EXPANSIVE
TOPIC
THAT
WE
DEVOTE
THE
NEXT
TWO
CHAPTERS
TO
IT
NONETHELESS
YOU
NEED
TO
UNDERSTAND
THAT
IT
IS
CRUCIAL
TO
PROTECT
SHARED
DATA
FROM
CONCURRENT
ACCESS
WHILE
USING
BOTTOM
HALVES
EVEN
ON
A
SINGLE
PROCESSOR
MACHINE
REMEMBER
A
BOTTOM
HALF
CAN
RUN
AT
VIRTUALLY
ANY
MOMENT
YOU
MIGHT
WANT
TO
COME
BACK
TO
THIS
SECTION
AFTER
READING
THE
NEXT
TWO
CHAPTERS
IF
THE
CONCEPT
OF
LOCKING
IS
FOREIGN
TO
YOU
ONE
OF
THE
BENEFITS
OF
TASKLETS
IS
THAT
THEY
ARE
SERIALIZED
WITH
RESPECT
TO
THEMSELVES
THE
SAME
TASKLET
WILL
NOT
RUN
CONCURRENTLY
EVEN
ON
TWO
DIFFERENT
PROCESSORS
THIS
MEANS
YOU
DO
NOT
HAVE
TO
WORRY
ABOUT
INTRA
TASKLET
CONCURRENCY
ISSUES
INTER
TASKLET
CONCUR
RENCY
THAT
IS
WHEN
TWO
DIFFERENT
TASKLETS
SHARE
THE
SAME
DATA
REQUIRES
PROPER
LOCKING
BECAUSE
SOFTIRQS
PROVIDE
NO
SERIALIZATION
EVEN
TWO
INSTANCES
OF
THE
SAME
SOFTIRQ
MIGHT
RUN
SIMULTANEOUSLY
ALL
SHARED
DATA
NEEDS
AN
APPROPRIATE
LOCK
IF
PROCESS
CONTEXT
CODE
AND
A
BOTTOM
HALF
SHARE
DATA
YOU
NEED
TO
DISABLE
BOTTOM
HALF
PROCESSING
AND
OBTAIN
A
LOCK
BEFORE
ACCESSING
THE
DATA
DOING
BOTH
ENSURES
LOCAL
AND
SMP
PROTECTION
AND
PREVENTS
A
DEADLOCK
IF
INTERRUPT
CONTEXT
CODE
AND
A
BOTTOM
HALF
SHARE
DATA
YOU
NEED
TO
DISABLE
INTERRUPTS
AND
OBTAIN
A
LOCK
BEFORE
ACCESSING
THE
DATA
THIS
ALSO
ENSURES
BOTH
LOCAL
AND
SMP
PROTEC
TION
AND
PREVENTS
A
DEADLOCK
ANY
SHARED
DATA
IN
A
WORK
QUEUE
REQUIRES
LOCKING
TOO
THE
LOCKING
ISSUES
ARE
NO
DIF
FERENT
FROM
NORMAL
KERNEL
CODE
BECAUSE
WORK
QUEUES
RUN
IN
PROCESS
CONTEXT
CHAPTER
AN
INTRODUCTION
TO
KERNEL
SYNCHRONIZATION
PROVIDES
A
BACKGROUND
ON
THE
ISSUES
SURROUNDING
CONCURRENCY
AND
CHAPTER
COVERS
THE
KERNEL
LOCKING
PRIMITIVES
THESE
CHAPTERS
COVER
HOW
TO
PROTECT
DATA
THAT
BOTTOM
HALVES
USE
DISABLING
BOTTOM
HALVES
NORMALLY
IT
IS
NOT
SUFFICIENT
TO
ONLY
DISABLE
BOTTOM
HALVES
MORE
OFTEN
TO
SAFELY
PROTECT
SHARED
DATA
YOU
NEED
TO
OBTAIN
A
LOCK
AND
DISABLE
BOTTOM
HALVES
SUCH
METHODS
WHICH
YOU
MIGHT
USE
IN
A
DRIVER
ARE
COVERED
IN
CHAPTER
IF
YOU
ARE
WRITING
CORE
KERNEL
CODE
HOWEVER
YOU
MIGHT
NEED
TO
DISABLE
JUST
THE
BOTTOM
HALVES
TO
DISABLE
ALL
BOTTOM
HALF
PROCESSING
SPECIFICALLY
ALL
SOFTIRQS
AND
THUS
ALL
TASKLETS
CALL
TO
ENABLE
BOTTOM
HALF
PROCESSING
CALL
YES
THE
FUNCTION
IS
MISNAMED
NO
ONE
BOTHERED
TO
CHANGE
THE
NAME
WHEN
THE
BH
INTERFACE
GAVE
WAY
TO
SOFTIRQS
TABLE
IS
A
SUMMARY
OF
THESE
FUNCTIONS
TABLE
BOTTOM
HALF
CONTROL
METHODS
METHOD
DESCRIPTION
VOID
DISABLES
SOFTIRQ
AND
TASKLET
PROCESSING
ON
THE
LOCAL
PROCESSOR
VOID
ENABLES
SOFTIRQ
AND
TASKLET
PROCESSING
ON
THE
LOCAL
PROCESSOR
THE
CALLS
CAN
BE
NESTED
ONLY
THE
FINAL
CALL
TO
ACTUALLY
ENABLES
BOTTOM
HALVES
FOR
EXAMPLE
THE
FIRST
TIME
IS
CALLED
LOCAL
SOFTIRQ
PROCESSING
IS
DISABLED
IF
IS
CALLED
THREE
MORE
TIMES
LOCAL
PROCESSING
REMAINS
DISABLED
PROCESSING
IS
NOT
REENABLED
UNTIL
THE
FOURTH
CALL
TO
THE
FUNCTIONS
ACCOMPLISH
THIS
BY
MAINTAINING
A
PER
TASK
COUNTER
VIA
THE
INTERESTINGLY
THE
SAME
COUNTER
USED
BY
KERNEL
PREEMPTION
WHEN
THE
COUNTER
REACHES
ZERO
BOTTOM
HALF
PROCESSING
IS
POSSIBLE
BECAUSE
BOTTOM
HALVES
WERE
DIS
ABLED
ALSO
CHECKS
FOR
ANY
PENDING
BOTTOM
HALVES
AND
EXECUTES
THEM
THE
FUNCTIONS
ARE
UNIQUE
TO
EACH
SUPPORTED
ARCHITECTURE
AND
ARE
USUALLY
WRITTEN
AS
COMPLICATED
MACROS
IN
ASM
SOFTIRQ
H
THE
FOLLOWING
ARE
CLOSE
C
REPRESENTATIONS
FOR
THE
CURIOUS
DISABLE
LOCAL
BOTTOM
HALVES
BY
INCREMENTING
THE
VOID
VOID
STRUCT
T
T
DECREMENT
THE
THIS
WILL
AUTOMATICALLY
ENABLE
BOTTOM
HALVES
IF
THE
COUNT
RETURNS
TO
ZERO
OPTIONALLY
RUN
ANY
BOTTOM
HALVES
THAT
ARE
PENDING
VOID
VOID
THIS
COUNTER
IS
USED
BOTH
BY
THE
INTERRUPT
AND
BOTTOM
HALF
SUBSYSTEMS
THUS
IN
LINUX
A
SINGLE
PER
TASK
COUNTER
REPRESENTS
THE
ATOMICITY
OF
A
TASK
THIS
HAS
PROVEN
USEFUL
FOR
WORK
SUCH
AS
DEBUGGING
SLEEPING
WHILE
ATOMIC
BUGS
STRUCT
T
T
IS
ZERO
AND
ARE
ANY
BOTTOM
HALVES
PENDING
IF
SO
RUN
THEM
IF
UNLIKELY
T
THESE
CALLS
DO
NOT
DISABLE
THE
EXECUTION
OF
WORK
QUEUES
BECAUSE
WORK
QUEUES
RUN
IN
PROCESS
CONTEXT
THERE
ARE
NO
ISSUES
WITH
ASYNCHRONOUS
EXECUTION
AND
THUS
THERE
IS
NO
NEED
TO
DISABLE
THEM
BECAUSE
SOFTIRQS
AND
TASKLETS
CAN
OCCUR
ASYNCHRONOUSLY
SAY
ON
RETURN
FROM
HANDLING
AN
INTERRUPT
HOWEVER
KERNEL
CODE
MAY
NEED
TO
DISABLE
THEM
WITH
WORK
QUEUES
ON
THE
OTHER
HAND
PROTECTING
SHARED
DATA
IS
THE
SAME
AS
IN
ANY
PROCESS
CON
TEXT
CHAPTERS
AND
GIVE
THE
DETAILS
CONCLUSION
IN
THIS
CHAPTER
WE
COVERED
THE
THREE
MECHANISMS
USED
TO
DEFER
WORK
IN
THE
LINUX
KERNEL
SOFTIRQS
TASKLETS
AND
WORK
QUEUES
WE
WENT
OVER
THEIR
DESIGN
AND
IMPLEMENTATION
WE
DIS
CUSSED
HOW
TO
USE
THEM
IN
YOUR
OWN
CODE
AND
WE
INSULTED
THEIR
POORLY
CONCEIVED
NAMES
FOR
HISTORICAL
COMPLETENESS
WE
ALSO
LOOKED
AT
THE
BOTTOM
HALF
MECHANISMS
THAT
EXISTED
IN
PREVIOUS
VERSIONS
OF
THE
LINUX
KERNEL
BH
AND
TASK
QUEUES
WE
TALKED
A
LOT
IN
THIS
CHAPTER
ABOUT
SYNCHRONIZATION
AND
CONCURRENCY
BECAUSE
SUCH
TOPICS
APPLY
QUITE
A
BIT
TO
BOTTOM
HALVES
WE
EVEN
WRAPPED
UP
THE
CHAPTER
WITH
A
DISCUS
SION
ON
DISABLING
BOTTOM
HALVES
FOR
REASONS
OF
CONCURRENCY
PROTECTION
IT
IS
NOW
TIME
TO
DIVE
HEAD
FIRST
INTO
THESE
TOPICS
CHAPTER
DISCUSSES
KERNEL
SYNCHRONIZATION
AND
CONCUR
RENCY
IN
THE
ABSTRACT
PROVIDING
A
FOUNDATION
FOR
UNDERSTANDING
THE
ISSUES
AT
THE
HEART
OF
THE
PROBLEM
CHAPTER
DISCUSSES
THE
SPECIFIC
INTERFACES
PROVIDED
BY
OUR
BELOVED
KERNEL
TO
SOLVE
THESE
PROBLEMS
ARMED
WITH
THE
NEXT
TWO
CHAPTERS
THE
WORLD
IS
YOUR
OYSTER
AN
INTRODUCTION
TO
KERNEL
SYNCHRONIZATION
IN
A
SHARED
MEMORY
APPLICATION
DEVELOPERS
MUST
ENSURE
THAT
SHARED
RESOURCES
ARE
PRO
TECTED
FROM
CONCURRENT
ACCESS
THE
KERNEL
IS
NO
EXCEPTION
SHARED
RESOURCES
REQUIRE
PRO
TECTION
FROM
CONCURRENT
ACCESS
BECAUSE
IF
MULTIPLE
THREADS
OF
ACCESS
AND
MANIPULATE
THE
DATA
AT
THE
SAME
TIME
THE
THREADS
MAY
OVERWRITE
EACH
OTHER
CHANGES
OR
ACCESS
DATA
WHILE
IT
IS
IN
AN
INCONSISTENT
STATE
CONCURRENT
ACCESS
OF
SHARED
DATA
IS
A
RECIPE
FOR
INSTABILITY
THAT
OFTEN
PROVES
HARD
TO
TRACK
DOWN
AND
DEBUG
GETTING
IT
RIGHT
AT
THE
START
IS
IMPORTANT
PROPERLY
PROTECTING
SHARED
RESOURCES
CAN
BE
TOUGH
YEARS
AGO
BEFORE
LINUX
SUPPORTED
SYMMETRICAL
MULTIPROCESSING
PREVENTING
CONCURRENT
ACCESS
OF
DATA
WAS
SIMPLE
BECAUSE
ONLY
A
SINGLE
PROCESSOR
WAS
SUPPORTED
THE
ONLY
WAY
DATA
COULD
BE
CONCURRENTLY
ACCESSED
WAS
IF
AN
INTERRUPT
OCCURRED
OR
IF
KERNEL
CODE
EXPLICITLY
RESCHEDULED
AND
ENABLED
ANOTHER
TASK
TO
RUN
WITH
EARLIER
KERNELS
DEVELOPMENT
WAS
SIMPLE
THOSE
HALCYON
DAYS
ARE
OVER
SYMMETRICAL
MULTIPROCESSING
SUPPORT
WAS
INTRODUCED
IN
THE
KERNEL
AND
HAS
BEEN
CONTINUALLY
ENHANCED
EVER
SINCE
MULTIPROCESSING
SUPPORT
IMPLIES
THAT
KERNEL
CODE
CAN
SIMULTANEOUSLY
RUN
ON
TWO
OR
MORE
PROCESSORS
CONSE
QUENTLY
WITHOUT
PROTECTION
CODE
IN
THE
KERNEL
RUNNING
ON
TWO
DIFFERENT
PROCESSORS
CAN
SIMULTANEOUSLY
ACCESS
SHARED
DATA
AT
EXACTLY
THE
SAME
TIME
WITH
THE
INTRODUCTION
OF
THE
KERNEL
THE
LINUX
KERNEL
IS
PREEMPTIVE
THIS
IMPLIES
THAT
AGAIN
IN
THE
ABSENCE
OF
PRO
TECTION
THE
SCHEDULER
CAN
PREEMPT
KERNEL
CODE
AT
VIRTUALLY
ANY
POINT
AND
RESCHEDULE
ANOTHER
TASK
TODAY
A
NUMBER
OF
SCENARIOS
ENABLE
FOR
CONCURRENCY
INSIDE
THE
KERNEL
AND
THEY
ALL
REQUIRE
PROTECTION
THE
TERM
THREADS
OF
EXECUTION
IMPLIES
ANY
INSTANCE
OF
EXECUTING
CODE
THIS
INCLUDES
FOR
EXAMPLE
A
TASK
IN
THE
KERNEL
AN
INTERRUPT
HANDLER
A
BOTTOM
HALF
OR
A
KERNEL
THREAD
THIS
CHAPTER
MAY
SHORTEN
THREADS
OF
EXECUTION
TO
SIMPLY
THREADS
KEEP
IN
MIND
THAT
THIS
TERM
DESCRIBES
ANY
EXECUTING
CODE
THIS
CHAPTER
DISCUSSES
THE
ISSUES
OF
CONCURRENCY
AND
SYNCHRONIZATION
IN
THE
ABSTRACT
AS
THEY
EXIST
IN
ANY
OPERATING
SYSTEM
KERNEL
THE
NEXT
CHAPTER
DETAILS
THE
SPECIFIC
MECHA
NISMS
AND
INTERFACES
THAT
THE
LINUX
KERNEL
PROVIDES
TO
SOLVE
SYNCHRONIZATION
ISSUES
AND
PREVENT
RACE
CONDITIONS
CRITICAL
REGIONS
AND
RACE
CONDITIONS
CODE
PATHS
THAT
ACCESS
AND
MANIPULATE
SHARED
DATA
ARE
CALLED
CRITICAL
REGIONS
ALSO
CALLED
CRITICAL
SECTIONS
IT
IS
USUALLY
UNSAFE
FOR
MULTIPLE
THREADS
OF
EXECUTION
TO
ACCESS
THE
SAME
RESOURCE
SIMULTANEOUSLY
TO
PREVENT
CONCURRENT
ACCESS
DURING
CRITICAL
REGIONS
THE
PRO
GRAMMER
MUST
ENSURE
THAT
CODE
EXECUTES
ATOMICALLY
THAT
IS
OPERATIONS
COMPLETE
WITHOUT
INTERRUPTION
AS
IF
THE
ENTIRE
CRITICAL
REGION
WERE
ONE
INDIVISIBLE
INSTRUCTION
IT
IS
A
BUG
IF
IT
IS
POSSIBLE
FOR
TWO
THREADS
OF
EXECUTION
TO
BE
SIMULTANEOUSLY
EXECUTING
WITHIN
THE
SAME
CRITICAL
REGION
WHEN
THIS
DOES
OCCUR
WE
CALL
IT
A
RACE
CONDITION
SO
NAMED
BECAUSE
THE
THREADS
RACED
TO
GET
THERE
FIRST
NOTE
HOW
RARE
A
RACE
CONDITION
IN
YOUR
CODE
MIGHT
MANI
FEST
ITSELF
DEBUGGING
RACE
CONDITIONS
IS
OFTEN
DIFFICULT
BECAUSE
THEY
ARE
NOT
EASILY
REPRO
DUCIBLE
ENSURING
THAT
UNSAFE
CONCURRENCY
IS
PREVENTED
AND
THAT
RACE
CONDITIONS
DO
NOT
OCCUR
IS
CALLED
SYNCHRONIZATION
WHY
DO
WE
NEED
PROTECTION
TO
BEST
UNDERSTAND
THE
NEED
FOR
SYNCHRONIZATION
LET
LOOK
AT
THE
UBIQUITY
OF
RACE
CONDI
TIONS
FOR
A
FIRST
EXAMPLE
LET
CONSIDER
A
REAL
WORLD
CASE
AN
ATM
AUTOMATED
TELLER
MACHINE
CALLED
A
CASH
MACHINE
CASHPOINT
OR
ABM
OUTSIDE
OF
THE
UNITED
STATES
ONE
OF
THE
MOST
COMMON
FUNCTIONS
PERFORMED
BY
CASH
MACHINES
IS
WITHDRAWING
MONEY
FROM
AN
INDIVIDUAL
PERSONAL
BANK
ACCOUNT
A
PERSON
WALKS
UP
TO
THE
MACHINE
INSERTS
AN
ATM
CARD
TYPES
IN
A
PIN
SELECTS
WITHDRAWAL
INPUTS
A
PECUNIARY
AMOUNT
HITS
OK
TAKES
THE
MONEY
AND
MAILS
IT
TO
ME
AFTER
THE
USER
HAS
ASKED
FOR
A
SPECIFIC
AMOUNT
OF
MONEY
THE
CASH
MACHINE
NEEDS
TO
ENSURE
THAT
THE
MONEY
ACTUALLY
EXISTS
IN
THAT
USER
ACCOUNT
IF
THE
MONEY
EXISTS
IT
THEN
NEEDS
TO
DEDUCT
THE
WITHDRAWAL
FROM
THE
TOTAL
FUNDS
AVAILABLE
THE
CODE
TO
IMPLEMENT
THIS
WOULD
LOOK
SOMETHING
LIKE
INT
TOTAL
TOTAL
FUNDS
IN
ACCOUNT
INT
WITHDRAWAL
AMOUNT
USER
ASKED
TO
WITHDRAWAL
CHECK
WHETHER
THE
USER
HAS
ENOUGH
FUNDS
IN
HER
ACCOUNT
IF
TOTAL
WITHDRAWAL
ERROR
YOU
DO
NOT
HAVE
THAT
MUCH
MONEY
RETURN
OK
THE
USER
HAS
ENOUGH
MONEY
DEDUCT
THE
WITHDRAWAL
AMOUNT
FROM
HER
TOTAL
TOTAL
WITHDRAWAL
TOTAL
GIVE
THE
USER
THEIR
MONEY
WITHDRAWAL
NOW
LET
PRESUME
THAT
ANOTHER
DEDUCTION
IN
THE
USER
FUNDS
IS
HAPPENING
AT
THE
SAME
TIME
IT
DOES
NOT
MATTER
HOW
THE
SIMULTANEOUS
DEDUCTION
IS
HAPPENING
ASSUME
THAT
THE
USER
SPOUSE
IS
INITIATING
ANOTHER
WITHDRAWAL
AT
ANOTHER
ATM
A
PAYEE
IS
ELECTRONICALLY
TRANS
FERRING
FUNDS
OUT
OF
THE
ACCOUNT
OR
THE
BANK
IS
DEDUCTING
A
FEE
FROM
THE
ACCOUNT
AS
BANKS
THESE
DAYS
ARE
SO
WONT
TO
DO
ANY
OF
THESE
SCENARIOS
FITS
OUR
EXAMPLE
BOTH
SYSTEMS
PERFORMING
THE
WITHDRAWAL
WOULD
HAVE
CODE
SIMILAR
TO
WHAT
WE
JUST
LOOKED
AT
FIRST
CHECK
WHETHER
THE
DEDUCTION
IS
POSSIBLE
THEN
COMPUTE
THE
NEW
TOTAL
FUNDS
AND
FINALLY
EXECUTE
THE
PHYSICAL
DEDUCTION
NOW
LET
MAKE
UP
SOME
NUMBERS
PRE
SUME
THAT
THE
FIRST
DEDUCTION
IS
A
WITHDRAWAL
FROM
AN
ATM
FOR
AND
THAT
THE
SECOND
DEDUCTION
IS
THE
BANK
APPLYING
A
FEE
OF
BECAUSE
THE
CUSTOMER
WALKED
INTO
THE
BANK
ASSUME
THE
CUSTOMER
HAS
A
TOTAL
OF
IN
THE
BANK
OBVIOUSLY
ONE
OF
THESE
TRANSACTIONS
CANNOT
CORRECTLY
COMPLETE
WITHOUT
SENDING
THE
ACCOUNT
INTO
THE
RED
WHAT
YOU
WOULD
EXPECT
IS
SOMETHING
LIKE
THIS
THE
FEE
TRANSACTION
HAPPENS
FIRST
TEN
DOLLARS
IS
LESS
THAN
SO
IS
SUBTRACTED
FROM
TO
GET
A
NEW
TOTAL
OF
AND
IS
POCKETED
BY
THE
BANK
THEN
THE
ATM
WITHDRAWAL
COMES
ALONG
AND
FAILS
BECAUSE
IS
LESS
THAN
WITH
RACE
CONDITIONS
LIFE
CAN
BE
MUCH
MORE
INTERESTING
ASSUME
THAT
THE
TWO
TRANSAC
TIONS
ARE
INITIATED
AT
ROUGHLY
THE
SAME
TIME
BOTH
TRANSACTIONS
VERIFY
THAT
SUFFICIENT
FUNDS
EXIST
IS
MORE
THAN
BOTH
AND
SO
ALL
IS
GOOD
THEN
THE
WITHDRAWAL
PROCESS
SUBTRACTS
FROM
YIELDING
THE
FEE
TRANSACTION
THEN
DOES
THE
SAME
SUBTRACTING
FROM
AND
GETTING
THE
WITHDRAWAL
PROCESS
THEN
UPDATES
THE
USER
NEW
TOTAL
AVAILABLE
FUNDS
TO
NOW
THE
FEE
TRANSACTION
ALSO
UPDATES
THE
NEW
TOTAL
RESULTING
IN
FREE
MONEY
CLEARLY
FINANCIAL
INSTITUTIONS
MUST
ENSURE
THAT
THIS
CAN
NEVER
HAPPEN
THEY
MUST
LOCK
THE
ACCOUNT
DURING
CERTAIN
OPERATIONS
MAKING
EACH
TRANSACTION
ATOMIC
WITH
RESPECT
TO
ANY
OTHER
TRANSACTION
SUCH
TRANSACTIONS
MUST
OCCUR
IN
THEIR
ENTIRETY
WITHOUT
INTERRUP
TION
OR
NOT
OCCUR
AT
ALL
THE
SINGLE
VARIABLE
NOW
LET
LOOK
AT
A
SPECIFIC
COMPUTING
EXAMPLE
CONSIDER
A
SIMPLE
SHARED
RESOURCE
A
SIN
GLE
GLOBAL
INTEGER
AND
A
SIMPLE
CRITICAL
REGION
THE
OPERATION
OF
MERELY
INCREMENTING
IT
I
THIS
MIGHT
TRANSLATE
INTO
MACHINE
INSTRUCTIONS
TO
THE
COMPUTER
PROCESSOR
THAT
RESEM
BLE
THE
FOLLOWING
GET
THE
CURRENT
VALUE
OF
I
AND
COPY
IT
INTO
A
REGISTER
ADD
ONE
TO
THE
VALUE
STORED
IN
THE
REGISTER
WRITE
BACK
TO
MEMORY
THE
NEW
VALUE
OF
I
NOW
ASSUME
THAT
THERE
ARE
TWO
THREADS
OF
EXECUTION
BOTH
ENTER
THIS
CRITICAL
REGION
AND
THE
INITIAL
VALUE
OF
I
IS
THE
DESIRED
OUTCOME
IS
THEN
SIMILAR
TO
THE
FOLLOWING
WITH
EACH
ROW
REPRESENTING
A
UNIT
OF
TIME
THREAD
THREAD
GET
I
INCREMENT
I
WRITE
BACK
I
GET
I
INCREMENT
I
WRITE
BACK
I
AS
EXPECTED
INCREMENTED
TWICE
IS
A
POSSIBLE
OUTCOME
HOWEVER
IS
THE
FOLLOWING
THREAD
THREAD
GET
I
GET
I
INCREMENT
I
INCREMENT
I
WRITE
BACK
I
WRITE
BACK
I
IF
BOTH
THREADS
OF
EXECUTION
READ
THE
INITIAL
VALUE
OF
I
BEFORE
IT
IS
INCREMENTED
BOTH
THREADS
INCREMENT
AND
SAVE
THE
SAME
VALUE
AS
A
RESULT
THE
VARIABLE
I
CONTAINS
THE
VALUE
WHEN
IN
FACT
IT
SHOULD
NOW
CONTAIN
THIS
IS
ONE
OF
THE
SIMPLEST
EXAMPLES
OF
A
CRITICAL
REGION
THANKFULLY
THE
SOLUTION
IS
EQUALLY
AS
SIMPLE
WE
MERELY
NEED
A
WAY
TO
PERFORM
THESE
OPERATIONS
IN
ONE
INDIVISIBLE
STEP
MOST
PROCESSORS
PROVIDE
AN
INSTRUCTION
TO
ATOMI
CALLY
READ
INCREMENT
AND
WRITE
BACK
A
SINGLE
VARIABLE
USING
THIS
ATOMIC
INSTRUCTION
THE
ONLY
POSSIBLE
OUTCOME
IS
THREAD
THREAD
INCREMENT
STORE
I
INCREMENT
STORE
I
OR
CONVERSELY
THREAD
THREAD
INCREMENT
STORE
INCREMENT
STORE
IT
WOULD
NEVER
BE
POSSIBLE
FOR
THE
TWO
ATOMIC
OPERATIONS
TO
INTERLEAVE
THE
PROCESSOR
WOULD
PHYSICALLY
ENSURE
THAT
IT
WAS
IMPOSSIBLE
USING
SUCH
AN
INSTRUCTION
WOULD
ALLEVIATE
THE
PROBLEM
THE
KERNEL
PROVIDES
A
SET
OF
INTERFACES
THAT
IMPLEMENT
THESE
ATOMIC
INSTRUC
TIONS
THEY
ARE
DISCUSSED
IN
THE
NEXT
CHAPTER
LOCKING
NOW
LET
CONSIDER
A
MORE
COMPLICATED
RACE
CONDITION
THAT
REQUIRES
A
MORE
COMPLICATED
SOLUTION
ASSUME
YOU
HAVE
A
QUEUE
OF
REQUESTS
THAT
NEEDS
TO
BE
SERVICED
FOR
THIS
EXERCISE
LET
ASSUME
THE
IMPLEMENTATION
IS
A
LINKED
LIST
IN
WHICH
EACH
NODE
REPRESENTS
A
REQUEST
TWO
FUNCTIONS
MANIPULATE
THE
QUEUE
ONE
FUNCTION
ADDS
A
NEW
REQUEST
TO
THE
TAIL
OF
THE
QUEUE
ANOTHER
FUNCTION
REMOVES
A
REQUEST
FROM
THE
HEAD
OF
THE
QUEUE
AND
DOES
SOME
THING
USEFUL
WITH
THE
REQUEST
VARIOUS
PARTS
OF
THE
KERNEL
INVOKE
THESE
TWO
FUNCTIONS
THUS
REQUESTS
ARE
CONTINUALLY
BEING
ADDED
REMOVED
AND
SERVICED
MANIPULATING
THE
REQUEST
QUEUES
CERTAINLY
REQUIRES
MULTIPLE
INSTRUCTIONS
IF
ONE
THREAD
ATTEMPTS
TO
READ
FROM
THE
QUEUE
WHILE
ANOTHER
IS
IN
THE
MIDDLE
OF
MANIPULATING
IT
THE
READING
THREAD
WILL
FIND
THE
QUEUE
IN
AN
INCONSISTENT
STATE
IT
SHOULD
BE
APPARENT
THE
SORT
OF
DAMAGE
THAT
COULD
OCCUR
IF
ACCESS
TO
THE
QUEUE
COULD
OCCUR
CONCURRENTLY
OFTEN
WHEN
THE
SHARED
RESOURCE
IS
A
COMPLEX
DATA
STRUCTURE
THE
RESULT
OF
A
RACE
CONDITION
IS
CORRUPTION
OF
THE
DATA
STRUCTURE
THE
PREVIOUS
SCENARIO
AT
FIRST
MIGHT
NOT
HAVE
A
CLEAR
SOLUTION
HOW
CAN
YOU
PREVENT
ONE
PROCESSOR
FROM
READING
FROM
THE
QUEUE
WHILE
ANOTHER
PROCESSOR
IS
UPDATING
IT
ALTHOUGH
IT
IS
FEASIBLE
FOR
A
PARTICULAR
ARCHITECTURE
TO
IMPLEMENT
SIMPLE
INSTRUCTIONS
SUCH
AS
ARITHMETIC
AND
COMPARISON
ATOMICALLY
IT
IS
LUDICROUS
FOR
ARCHITECTURES
TO
PROVIDE
INSTRUCTIONS
TO
SUPPORT
THE
INDEFINITELY
SIZED
CRITICAL
REGIONS
THAT
WOULD
EXIST
IN
THE
PREVI
OUS
EXAMPLE
WHAT
IS
NEEDED
IS
A
WAY
OF
MAKING
SURE
THAT
ONLY
ONE
THREAD
MANIPULATES
THE
DATA
STRUCTURE
AT
A
TIME
A
MECHANISM
FOR
PREVENTING
ACCESS
TO
A
RESOURCE
WHILE
ANOTHER
THREAD
OF
EXECUTION
IS
IN
THE
MARKED
REGION
A
LOCK
PROVIDES
SUCH
A
MECHANISM
IT
WORKS
MUCH
LIKE
A
LOCK
ON
A
DOOR
IMAGINE
THE
ROOM
BEYOND
THE
DOOR
AS
THE
CRITICAL
REGION
INSIDE
THE
ROOM
ONLY
ONE
THREAD
OF
EXECU
TION
CAN
BE
PRESENT
AT
A
GIVEN
TIME
WHEN
A
THREAD
ENTERS
THE
ROOM
IT
LOCKS
THE
DOOR
BEHIND
IT
WHEN
THE
THREAD
IS
FINISHED
MANIPULATING
THE
SHARED
DATA
IT
LEAVES
THE
ROOM
AND
UNLOCKS
THE
DOOR
IF
ANOTHER
THREAD
REACHES
THE
DOOR
WHILE
IT
IS
LOCKED
IT
MUST
WAIT
FOR
THE
THREAD
INSIDE
TO
EXIT
THE
ROOM
AND
UNLOCK
THE
DOOR
BEFORE
IT
CAN
ENTER
THREADS
HOLD
LOCKS
LOCKS
PROTECT
DATA
IN
THE
PREVIOUS
REQUEST
QUEUE
EXAMPLE
A
SINGLE
LOCK
COULD
HAVE
BEEN
USED
TO
PROTECT
THE
QUEUE
WHENEVER
THERE
WAS
A
NEW
REQUEST
TO
ADD
TO
THE
QUEUE
THE
THREAD
WOULD
FIRST
OBTAIN
THE
LOCK
THEN
IT
COULD
SAFELY
ADD
THE
REQUEST
TO
THE
QUEUE
AND
ULTIMATELY
RELEASE
THE
LOCK
WHEN
A
THREAD
WANTED
TO
REMOVE
A
REQUEST
FROM
THE
QUEUE
IT
TOO
WOULD
OBTAIN
THE
LOCK
THEN
IT
COULD
READ
THE
REQUEST
AND
REMOVE
IT
FROM
THE
QUEUE
FINALLY
IT
WOULD
RELEASE
THE
LOCK
ANY
OTHER
ACCESS
TO
THE
QUEUE
WOULD
SIMILARLY
NEED
TO
OBTAIN
THE
LOCK
BECAUSE
THE
LOCK
CAN
BE
HELD
BY
ONLY
ONE
THREAD
AT
A
TIME
ONLY
A
SINGLE
THREAD
CAN
MANIPULATE
THE
QUEUE
AT
A
TIME
IF
A
THREAD
COMES
ALONG
WHILE
ANOTHER
THREAD
IS
ALREADY
UPDATING
IT
THE
SECOND
THREAD
HAS
TO
WAIT
FOR
THE
FIRST
TO
RELEASE
THE
LOCK
BEFORE
IT
CAN
CONTINUE
THE
LOCK
PREVENTS
CONCURRENCY
AND
PROTECTS
THE
QUEUE
FROM
RACE
CONDITIONS
ANY
CODE
THAT
ACCESSES
THE
QUEUE
FIRST
NEEDS
TO
OBTAIN
THE
RELEVANT
LOCK
IF
ANOTHER
THREAD
OF
EXECUTION
COMES
ALONG
THE
LOCK
PREVENTS
CONCURRENCY
THREAD
THREAD
TRY
TO
LOCK
THE
QUEUE
TRY
TO
LOCK
THE
QUEUE
SUCCEEDED
ACQUIRED
LOCK
FAILED
WAITING
ACCESS
QUEUE
WAITING
UNLOCK
THE
QUEUE
WAITING
SUCCEEDED
ACQUIRED
LOCK
ACCESS
QUEUE
UNLOCK
THE
QUEUE
NOTICE
THAT
LOCKS
ARE
ADVISORY
AND
VOLUNTARY
LOCKS
ARE
ENTIRELY
A
PROGRAMMING
CON
STRUCT
THAT
THE
PROGRAMMER
MUST
TAKE
ADVANTAGE
OF
NOTHING
PREVENTS
YOU
FROM
WRITING
CODE
THAT
MANIPULATES
THE
FICTIONAL
QUEUE
WITHOUT
THE
APPROPRIATE
LOCK
SUCH
A
PRACTICE
OF
COURSE
WOULD
EVENTUALLY
RESULT
IN
A
RACE
CONDITION
AND
CORRUPTION
LOCKS
COME
IN
VARIOUS
SHAPES
AND
SIZES
LINUX
ALONE
IMPLEMENTS
A
HANDFUL
OF
DIFFER
ENT
LOCKING
MECHANISMS
THE
MOST
SIGNIFICANT
DIFFERENCE
BETWEEN
THE
VARIOUS
MECHANISMS
IS
THE
BEHAVIOR
WHEN
THE
LOCK
IS
UNAVAILABLE
BECAUSE
ANOTHER
THREAD
ALREADY
HOLDS
IT
SOME
LOCK
VARIANTS
BUSY
WAIT
WHEREAS
OTHER
LOCKS
PUT
THE
CURRENT
TASK
TO
SLEEP
UNTIL
THE
LOCK
BECOMES
AVAILABLE
THE
NEXT
CHAPTER
DISCUSSES
THE
BEHAVIOR
OF
THE
DIFFERENT
LOCKS
IN
LINUX
AND
THEIR
INTERFACES
ASTUTE
READERS
ARE
NOW
SCREAMING
THE
LOCK
DOES
NOT
SOLVE
THE
PROBLEM
IT
SIMPLY
SHRINKS
THE
CRITICAL
REGION
DOWN
TO
JUST
THE
LOCK
AND
UNLOCK
CODE
PROBABLY
MUCH
SMALLER
SURE
BUT
STILL
A
POTENTIAL
RACE
FORTUNATELY
LOCKS
ARE
IMPLEMENTED
USING
ATOMIC
OPERATIONS
THAT
ENSURE
NO
RACE
EXISTS
A
SINGLE
INSTRUCTION
CAN
VERIFY
WHETHER
THE
KEY
IS
TAKEN
AND
IF
NOT
SEIZE
IT
HOW
THIS
IS
DONE
IS
ARCHITECTURE
SPECIFIC
BUT
ALMOST
ALL
PROCESSORS
IMPLEMENT
AN
ATOMIC
TEST
AND
SET
INSTRUCTION
THAT
TESTS
THE
VALUE
OF
AN
INTEGER
AND
SETS
IT
TO
A
NEW
VALUE
ONLY
IF
IT
IS
ZERO
A
VALUE
OF
ZERO
MEANS
UNLOCKED
ON
THE
POPULAR
ARCHITECTURE
LOCKS
ARE
IMPLEMENTED
USING
SUCH
A
SIMILAR
INSTRUCTION
CALLED
COMPARE
AND
EXCHANGE
THAT
IS
SPIN
IN
A
TIGHT
LOOP
CHECKING
THE
STATUS
OF
THE
LOCK
OVER
AND
OVER
WAITING
FOR
THE
LOCK
TO
BECOME
AVAILABLE
CAUSES
OF
CONCURRENCY
IN
USER
SPACE
THE
NEED
FOR
SYNCHRONIZATION
STEMS
FROM
THE
FACT
THAT
PROGRAMS
ARE
SCHED
ULED
PREEMPTIVELY
AT
THE
WILL
OF
THE
SCHEDULER
BECAUSE
A
PROCESS
CAN
BE
PREEMPTED
AT
ANY
TIME
AND
ANOTHER
PROCESS
CAN
BE
SCHEDULED
ONTO
THE
PROCESSOR
A
PROCESS
CAN
BE
INVOLUN
TARILY
PREEMPTED
IN
THE
MIDDLE
OF
ACCESSING
A
CRITICAL
REGION
IF
THE
NEWLY
SCHEDULED
PROCESS
THEN
ENTERS
THE
SAME
CRITICAL
REGION
SAY
IF
THE
TWO
PROCESSES
MANIPULATE
THE
SAME
SHARED
MEMORY
OR
WRITE
TO
THE
SAME
FILE
DESCRIPTOR
A
RACE
CAN
OCCUR
THE
SAME
PROBLEM
CAN
OCCUR
WITH
MULTIPLE
SINGLE
THREADED
PROCESSES
SHARING
FILES
OR
WITHIN
A
SINGLE
PROGRAM
WITH
SIGNALS
BECAUSE
SIGNALS
CAN
OCCUR
ASYNCHRONOUSLY
THIS
TYPE
OF
CONCURRENCY
IN
WHICH
TWO
THINGS
DO
NOT
ACTUALLY
HAPPEN
AT
THE
SAME
TIME
BUT
INTERLEAVE
WITH
EACH
OTHER
SUCH
THAT
THEY
MIGHT
AS
WELL
IS
CALLED
PSEUDO
CONCURRENCY
IF
YOU
HAVE
A
SYMMETRICAL
MULTIPROCESSING
MACHINE
TWO
PROCESSES
CAN
ACTUALLY
BE
EXE
CUTED
IN
A
CRITICAL
REGION
AT
THE
EXACT
SAME
TIME
THAT
IS
CALLED
TRUE
CONCURRENCY
ALTHOUGH
THE
CAUSES
AND
SEMANTICS
OF
TRUE
VERSUS
PSEUDO
CONCURRENCY
ARE
DIFFERENT
THEY
BOTH
RESULT
IN
THE
SAME
RACE
CONDITIONS
AND
REQUIRE
THE
SAME
SORT
OF
PROTECTION
THE
KERNEL
HAS
SIMILAR
CAUSES
OF
CONCURRENCY
N
INTERRUPTS
AN
INTERRUPT
CAN
OCCUR
ASYNCHRONOUSLY
AT
ALMOST
ANY
TIME
INTER
RUPTING
THE
CURRENTLY
EXECUTING
CODE
N
SOFTIRQS
AND
TASKLETS
THE
KERNEL
CAN
RAISE
OR
SCHEDULE
A
SOFTIRQ
OR
TASKLET
AT
ALMOST
ANY
TIME
INTERRUPTING
THE
CURRENTLY
EXECUTING
CODE
N
KERNEL
PREEMPTION
BECAUSE
THE
KERNEL
IS
PREEMPTIVE
ONE
TASK
IN
THE
KERNEL
CAN
PREEMPT
ANOTHER
N
SLEEPING
AND
SYNCHRONIZATION
WITH
USER
SPACE
A
TASK
IN
THE
KERNEL
CAN
SLEEP
AND
THUS
INVOKE
THE
SCHEDULER
RESULTING
IN
THE
RUNNING
OF
A
NEW
PROCESS
N
SYMMETRICAL
MULTIPROCESSING
TWO
OR
MORE
PROCESSORS
CAN
EXECUTE
KERNEL
CODE
AT
EXACTLY
THE
SAME
TIME
KERNEL
DEVELOPERS
NEED
TO
UNDERSTAND
AND
PREPARE
FOR
THESE
CAUSES
OF
CONCURRENCY
IT
IS
A
MAJOR
BUG
IF
AN
INTERRUPT
OCCURS
IN
THE
MIDDLE
OF
CODE
THAT
IS
MANIPULATING
A
RESOURCE
AND
THE
INTERRUPT
HANDLER
CAN
ACCESS
THE
SAME
RESOURCE
SIMILARLY
IT
IS
A
BUG
IF
KERNEL
CODE
IS
PREEMPTIVE
WHILE
IT
IS
ACCESSING
A
SHARED
RESOURCE
LIKEWISE
IT
IS
A
BUG
IF
CODE
IN
THE
KERNEL
SLEEPS
WHILE
IN
THE
MIDDLE
OF
A
CRITICAL
SECTION
FINALLY
TWO
PROCESSORS
SHOULD
NEVER
SIMULTANEOUSLY
ACCESS
THE
SAME
PIECE
OF
DATA
WITH
A
CLEAR
PICTURE
OF
WHAT
DATA
NEEDS
PRO
TECTION
IT
IS
NOT
HARD
TO
PROVIDE
THE
LOCKING
TO
KEEP
THE
SYSTEM
STABLE
RATHER
THE
HARD
PART
IS
IDENTIFYING
THESE
CONDITIONS
AND
REALIZING
THAT
TO
PREVENT
CONCURRENCY
YOU
NEED
SOME
FORM
OF
PROTECTION
LET
US
REITERATE
THIS
POINT
BECAUSE
IT
IS
IMPORTANT
IMPLEMENTING
THE
ACTUAL
LOCKING
IN
YOUR
CODE
TO
PROTECT
SHARED
DATA
IS
NOT
DIFFICULT
ESPECIALLY
WHEN
DONE
EARLY
ON
DURING
THE
DESIGN
PHASE
OF
DEVELOPMENT
THE
TRICKY
PART
IS
IDENTIFYING
THE
ACTUAL
SHARED
DATA
AND
THE
CORRESPONDING
CRITICAL
SECTIONS
THIS
IS
WHY
DESIGNING
LOCKING
INTO
YOUR
CODE
FROM
THE
GET
GO
AND
NOT
AS
AN
AFTERTHOUGHT
IS
OF
PARAMOUNT
IMPORTANCE
IT
CAN
BE
DIFFICULT
TO
GO
IN
EX
POST
AND
IDENTIFY
CRITICAL
REGIONS
AND
RETROFIT
LOCKING
INTO
THE
EXISTING
CODE
THE
RESULTING
CODE
IS
OFTEN
NOT
PRETTY
EITHER
THE
TAKEAWAY
FROM
THIS
IS
TO
ALWAYS
DESIGN
PROPER
LOCKING
INTO
YOUR
CODE
FROM
THE
BEGINNING
CODE
THAT
IS
SAFE
FROM
CONCURRENT
ACCESS
FROM
AN
INTERRUPT
HANDLER
IS
SAID
TO
BE
INTERRUPT
SAFE
CODE
THAT
IS
SAFE
FROM
CONCURRENCY
ON
SYMMETRICAL
MULTIPROCESSING
MACHINES
IS
SMP
SAFE
CODE
THAT
IS
SAFE
FROM
CONCURRENCY
WITH
KERNEL
PREEMPTION
IS
PREEMPT
SAFE
THE
ACTUAL
MECHANISMS
USED
TO
PROVIDE
SYNCHRONIZATION
AND
PROTECT
AGAINST
RACE
CONDITIONS
IN
ALL
THESE
CASES
IS
COVERED
IN
THE
NEXT
CHAPTER
KNOWING
WHAT
TO
PROTECT
IDENTIFYING
WHAT
DATA
SPECIFICALLY
NEEDS
PROTECTION
IS
VITAL
BECAUSE
ANY
DATA
THAT
CAN
BE
ACCESSED
CONCURRENTLY
ALMOST
ASSUREDLY
NEEDS
PROTECTION
IT
IS
OFTEN
EASIER
TO
IDENTIFY
WHAT
DATA
DOES
NOT
NEED
PROTECTION
AND
WORK
FROM
THERE
OBVIOUSLY
ANY
DATA
THAT
IS
LOCAL
TO
ONE
PARTICULAR
THREAD
OF
EXECUTION
DOES
NOT
NEED
PROTECTION
BECAUSE
ONLY
THAT
THREAD
CAN
ACCESS
THE
DATA
FOR
EXAMPLE
LOCAL
AUTOMATIC
VARIABLES
AND
DYNAMICALLY
ALLOCATED
DATA
STRUCTURES
WHOSE
ADDRESS
IS
STORED
ONLY
ON
THE
STACK
DO
NOT
NEED
ANY
SORT
OF
LOCKING
BECAUSE
THEY
EXIST
SOLELY
ON
THE
STACK
OF
THE
EXECUTING
THREAD
LIKEWISE
DATA
THAT
IS
ACCESSED
BY
ONLY
A
SPECIFIC
TASK
DOES
NOT
REQUIRE
LOCKING
BECAUSE
A
PROCESS
CAN
EXECUTE
ON
ONLY
ONE
PROCESSOR
AT
A
TIME
WHAT
DOES
NEED
LOCKING
MOST
GLOBAL
KERNEL
DATA
STRUCTURES
DO
A
GOOD
RULE
OF
THUMB
IS
THAT
IF
ANOTHER
THREAD
OF
EXECUTION
CAN
ACCESS
THE
DATA
THE
DATA
NEEDS
SOME
SORT
OF
LOCKING
IF
ANYONE
ELSE
CAN
SEE
IT
LOCK
IT
REMEMBER
TO
LOCK
DATA
NOT
CODE
CONFIG
OPTIONS
SMP
VERSUS
UP
BECAUSE
THE
LINUX
KERNEL
IS
CONFIGURABLE
AT
COMPILE
TIME
IT
MAKES
SENSE
THAT
YOU
CAN
TAILOR
THE
KERNEL
SPECIFICALLY
FOR
A
GIVEN
MACHINE
MOST
IMPORTANT
THE
CONFIGURE
OPTION
CONTROLS
WHETHER
THE
KERNEL
SUPPORTS
SMP
MANY
LOCKING
ISSUES
DISAPPEAR
ON
UNIPROCESSOR
MACHINES
CONSEQUENTLY
WHEN
IS
UNSET
UNNECESSARY
CODE
IS
NOT
COMPILED
INTO
THE
KERNEL
IMAGE
FOR
EXAMPLE
SUCH
CONFIGURATION
ENABLES
UNIPROCESSOR
MACHINES
TO
FOREGO
THE
OVERHEAD
OF
SPIN
LOCKS
THE
SAME
TRICK
APPLIES
TO
THE
CONFIGURE
OPTION
ENABLING
KERNEL
PREEMPTION
THIS
WAS
AN
EXCELLENT
DESIGN
DECISION
THE
KERNEL
MAINTAINS
ONE
CLEAN
SOURCE
BASE
AND
THE
VARIOUS
LOCKING
MECHANISMS
ARE
USED
AS
NEEDED
DIFFERENT
COMBINATIONS
OF
AND
ON
DIFFERENT
ARCHI
TECTURES
COMPILE
IN
VARYING
LOCK
SUPPORT
IN
YOUR
CODE
PROVIDE
APPROPRIATE
PROTECTION
FOR
THE
MOST
PESSIMISTIC
CASE
SMP
WITH
KERNEL
PREEMPTION
AND
ALL
SCENARIOS
WILL
BE
COVERED
YOU
WILL
ALSO
SEE
THAT
BARRING
A
FEW
EXCEPTIONS
BEING
SMP
SAFE
IMPLIES
BEING
PREEMPT
SAFE
WHENEVER
YOU
WRITE
KERNEL
CODE
YOU
SHOULD
ASK
YOURSELF
THESE
QUESTIONS
N
IS
THE
DATA
GLOBAL
CAN
A
THREAD
OF
EXECUTION
OTHER
THAN
THE
CURRENT
ONE
ACCESS
IT
N
IS
THE
DATA
SHARED
BETWEEN
PROCESS
CONTEXT
AND
INTERRUPT
CONTEXT
IS
IT
SHARED
BETWEEN
TWO
DIFFERENT
INTERRUPT
HANDLERS
N
IF
A
PROCESS
IS
PREEMPTED
WHILE
ACCESSING
THIS
DATA
CAN
THE
NEWLY
SCHEDULED
PROCESS
ACCESS
THE
SAME
DATA
N
CAN
THE
CURRENT
PROCESS
SLEEP
BLOCK
ON
ANYTHING
IF
IT
DOES
IN
WHAT
STATE
DOES
THAT
LEAVE
ANY
SHARED
DATA
N
WHAT
PREVENTS
THE
DATA
FROM
BEING
FREED
OUT
FROM
UNDER
ME
N
WHAT
HAPPENS
IF
THIS
FUNCTION
IS
CALLED
AGAIN
ON
ANOTHER
PROCESSOR
N
GIVEN
THE
PROCEEDING
POINTS
HOW
AM
I
GOING
TO
ENSURE
THAT
MY
CODE
IS
SAFE
FROM
CONCURRENCY
IN
SHORT
NEARLY
ALL
GLOBAL
AND
SHARED
DATA
IN
THE
KERNEL
REQUIRES
SOME
FORM
OF
THE
SYNCHRONIZATION
METHODS
DISCUSSED
IN
THE
NEXT
CHAPTER
DEADLOCKS
A
DEADLOCK
IS
A
CONDITION
INVOLVING
ONE
OR
MORE
THREADS
OF
EXECUTION
AND
ONE
OR
MORE
RESOURCES
SUCH
THAT
EACH
THREAD
WAITS
FOR
ONE
OF
THE
RESOURCES
BUT
ALL
THE
RESOURCES
ARE
ALREADY
HELD
THE
THREADS
ALL
WAIT
FOR
EACH
OTHER
BUT
THEY
NEVER
MAKE
ANY
PROGRESS
TOWARD
RELEASING
THE
RESOURCES
THAT
THEY
ALREADY
HOLD
THEREFORE
NONE
OF
THE
THREADS
CAN
CON
TINUE
WHICH
RESULTS
IN
A
DEADLOCK
A
GOOD
ANALOGY
IS
A
FOUR
WAY
TRAFFIC
STOP
IF
EACH
CAR
AT
THE
STOP
DECIDES
TO
WAIT
FOR
THE
OTHER
CARS
BEFORE
GOING
NO
CAR
WILL
EVER
PROCEED
AND
WE
HAVE
A
TRAFFIC
DEADLOCK
THE
SIMPLEST
EXAMPLE
OF
A
DEADLOCK
IS
THE
SELF
DEADLOCK
IF
A
THREAD
OF
EXECUTION
ATTEMPTS
TO
ACQUIRE
A
LOCK
IT
ALREADY
HOLDS
IT
HAS
TO
WAIT
FOR
THE
LOCK
TO
BE
RELEASED
BUT
IT
WILL
NEVER
RELEASE
THE
LOCK
BECAUSE
IT
IS
BUSY
WAITING
FOR
THE
LOCK
AND
THE
RESULT
IS
DEADLOCK
ACQUIRE
LOCK
ACQUIRE
LOCK
AGAIN
WAIT
FOR
LOCK
TO
BECOME
AVAILABLE
SOME
KERNELS
PREVENT
THIS
TYPE
OF
DEADLOCK
BY
PROVIDING
RECURSIVE
LOCKS
THESE
ARE
LOCKS
THAT
A
SINGLE
THREAD
OF
EXECUTION
MAY
ACQUIRE
MULTIPLE
TIMES
LINUX
THANKFULLY
DOES
NOT
PROVIDE
RECURSIVE
LOCKS
THIS
IS
WIDELY
CONSIDERED
A
GOOD
THING
ALTHOUGH
RECURSIVE
LOCKS
MIGHT
ALLEVIATE
THE
SELF
DEADLOCK
PROBLEM
THEY
VERY
READILY
LEAD
TO
SLOPPY
LOCKING
SEMANTICS
SIMILARLY
CONSIDER
N
THREADS
AND
N
LOCKS
IF
EACH
THREAD
HOLDS
A
LOCK
THAT
THE
OTHER
THREAD
WANTS
ALL
THREADS
BLOCK
WHILE
WAITING
FOR
THEIR
RESPECTIVE
LOCKS
TO
BECOME
AVAIL
ABLE
THE
MOST
COMMON
EXAMPLE
IS
WITH
TWO
THREADS
AND
TWO
LOCKS
WHICH
IS
OFTEN
CALLED
THE
DEADLY
EMBRACE
OR
THE
ABBA
DEADLOCK
THREAD
THREAD
ACQUIRE
LOCK
A
ACQUIRE
LOCK
B
TRY
TO
ACQUIRE
LOCK
B
TRY
TO
ACQUIRE
LOCK
A
WAIT
FOR
LOCK
B
WAIT
FOR
LOCK
A
EACH
THREAD
IS
WAITING
FOR
THE
OTHER
AND
NEITHER
THREAD
WILL
EVER
RELEASE
ITS
ORIGINAL
LOCK
THEREFORE
NEITHER
LOCK
WILL
BECOME
AVAILABLE
PREVENTION
OF
DEADLOCK
SCENARIOS
IS
IMPORTANT
ALTHOUGH
IT
IS
DIFFICULT
TO
PROVE
THAT
CODE
IS
FREE
OF
DEADLOCKS
YOU
CAN
WRITE
DEADLOCK
FREE
CODE
A
FEW
SIMPLE
RULES
GO
A
LONG
WAY
N
IMPLEMENT
LOCK
ORDERING
NESTED
LOCKS
MUST
ALWAYS
BE
OBTAINED
IN
THE
SAME
ORDER
THIS
PREVENTS
THE
DEADLY
EMBRACE
DEADLOCK
DOCUMENT
THE
LOCK
ORDERING
SO
OTHERS
WILL
FOLLOW
IT
N
PREVENT
STARVATION
ASK
YOURSELF
DOES
THIS
CODE
ALWAYS
FINISH
IF
FOO
DOES
NOT
OCCUR
WILL
BAR
WAIT
FOREVER
N
DO
NOT
DOUBLE
ACQUIRE
THE
SAME
LOCK
N
DESIGN
FOR
SIMPLICITY
COMPLEXITY
IN
YOUR
LOCKING
SCHEME
INVITES
DEADLOCKS
THE
FIRST
POINT
IS
MOST
IMPORTANT
AND
WORTH
STRESSING
IF
TWO
OR
MORE
LOCKS
ARE
ACQUIRED
AT
THE
SAME
TIME
THEY
MUST
ALWAYS
BE
ACQUIRED
IN
THE
SAME
ORDER
LET
ASSUME
YOU
HAVE
THE
CAT
DOG
AND
FOX
LOCKS
THAT
PROTECT
DATA
STRUCTURES
OF
THE
SAME
NAME
NOW
ASSUME
YOU
HAVE
A
FUNCTION
THAT
NEEDS
TO
WORK
ON
ALL
THREE
OF
THESE
DATA
STRUCTURES
SIMUL
TANEOUSLY
PERHAPS
TO
COPY
DATA
BETWEEN
THEM
WHATEVER
THE
CASE
THE
DATA
STRUCTURES
REQUIRE
LOCKING
TO
ENSURE
SAFE
ACCESS
IF
ONE
FUNCTION
ACQUIRES
THE
LOCKS
IN
THE
ORDER
CAT
DOG
AND
THEN
FOX
THEN
EVERY
OTHER
FUNCTION
MUST
OBTAIN
THESE
LOCKS
OR
A
SUBSET
OF
THEM
IN
THIS
SAME
ORDER
FOR
EXAMPLE
IT
IS
A
POTENTIAL
DEADLOCK
AND
HENCE
A
BUG
TO
FIRST
OBTAIN
THE
FOX
LOCK
AND
THEN
OBTAIN
THE
DOG
LOCK
BECAUSE
THE
DOG
LOCK
MUST
ALWAYS
BE
ACQUIRED
PRIOR
TO
THE
FOX
LOCK
HERE
IS
AN
EXAMPLE
IN
WHICH
THIS
WOULD
CAUSE
A
DEADLOCK
THREAD
THREAD
ACQUIRE
LOCK
CAT
ACQUIRE
LOCK
FOX
ACQUIRE
LOCK
DOG
TRY
TO
ACQUIRE
LOCK
DOG
TRY
TO
ACQUIRE
LOCK
FOX
WAIT
FOR
LOCK
DOG
WAIT
FOR
LOCK
FOX
THREAD
ONE
IS
WAITING
FOR
THE
FOX
LOCK
WHICH
THREAD
TWO
HOLDS
WHILE
THREAD
TWO
IS
WAITING
FOR
THE
DOG
LOCK
WHICH
THREAD
ONE
HOLDS
NEITHER
EVER
RELEASES
ITS
LOCK
AND
HENCE
BOTH
WAIT
FOREVER
BAM
DEADLOCK
IF
THE
LOCKS
WERE
ALWAYS
OBTAINED
IN
THE
SAME
ORDER
A
DEADLOCK
IN
THIS
MANNER
WOULD
NOT
BE
POSSIBLE
WHENEVER
LOCKS
ARE
NESTED
WITHIN
OTHER
LOCKS
A
SPECIFIC
ORDERING
MUST
BE
OBEYED
IT
IS
GOOD
PRACTICE
TO
PLACE
THE
ORDERING
IN
A
COMMENT
ABOVE
THE
LOCK
SOMETHING
LIKE
THE
FOL
LOWING
IS
A
GOOD
IDEA
LOCKS
ACCESS
TO
THE
CAT
STRUCTURE
ALWAYS
OBTAIN
BEFORE
THE
DOG
LOCK
THE
ORDER
OF
UNLOCK
DOES
NOT
MATTER
WITH
RESPECT
TO
DEADLOCK
ALTHOUGH
IT
IS
COMMON
PRACTICE
TO
RELEASE
THE
LOCKS
IN
AN
ORDER
INVERSE
TO
THAT
IN
WHICH
THEY
WERE
ACQUIRED
PREVENTING
DEADLOCKS
IS
IMPORTANT
THE
LINUX
KERNEL
HAS
SOME
BASIC
DEBUGGING
FACILI
TIES
FOR
DETECTING
DEADLOCK
SCENARIOS
IN
A
RUNNING
KERNEL
THESE
FEATURES
ARE
DISCUSSED
IN
THE
NEXT
CHAPTER
CONTENTION
AND
SCALABILITY
THE
TERM
LOCK
CONTENTION
OR
SIMPLY
CONTENTION
DESCRIBES
A
LOCK
CURRENTLY
IN
USE
BUT
THAT
ANOTHER
THREAD
IS
TRYING
TO
ACQUIRE
A
LOCK
THAT
IS
HIGHLY
CONTENDED
OFTEN
HAS
THREADS
WAITING
TO
ACQUIRE
IT
HIGH
CONTENTION
CAN
OCCUR
BECAUSE
A
LOCK
IS
FREQUENTLY
OBTAINED
HELD
FOR
A
LONG
TIME
AFTER
IT
IS
OBTAINED
OR
BOTH
BECAUSE
A
LOCK
JOB
IS
TO
SERIALIZE
ACCESS
TO
A
RESOURCE
IT
COMES
AS
NO
SURPRISE
THAT
LOCKS
CAN
SLOW
DOWN
A
SYSTEM
PERFORMANCE
A
HIGHLY
CONTENDED
LOCK
CAN
BECOME
A
BOTTLENECK
IN
THE
SYSTEM
QUICKLY
LIMITING
ITS
PER
FORMANCE
OF
COURSE
THE
LOCKS
ARE
ALSO
REQUIRED
TO
PREVENT
THE
SYSTEM
FROM
TEARING
ITSELF
TO
SHREDS
SO
A
SOLUTION
TO
HIGH
CONTENTION
MUST
CONTINUE
TO
PROVIDE
THE
NECESSARY
CONCURRENCY
PROTECTION
SCALABILITY
IS
A
MEASUREMENT
OF
HOW
WELL
A
SYSTEM
CAN
BE
EXPANDED
IN
OPERATING
SYS
TEMS
WE
TALK
OF
THE
SCALABILITY
WITH
A
LARGE
NUMBER
OF
PROCESSES
A
LARGE
NUMBER
OF
PROCESSORS
OR
LARGE
AMOUNTS
OF
MEMORY
WE
CAN
DISCUSS
SCALABILITY
IN
RELATION
TO
VIRTUALLY
ANY
COMPONENT
OF
A
COMPUTER
TO
WHICH
WE
CAN
ATTACH
A
QUANTITY
IDEALLY
DOUBLING
THE
NUMBER
OF
PROCESSORS
SHOULD
RESULT
IN
A
DOUBLING
OF
THE
SYSTEM
PROCESSOR
PERFORMANCE
THIS
OF
COURSE
IS
NEVER
THE
CASE
THE
SCALABILITY
OF
LINUX
ON
A
LARGE
NUMBER
OF
PROCESSORS
HAS
INCREASED
DRAMATICALLY
IN
THE
TIME
SINCE
MULTIPROCESSING
SUPPORT
WAS
INTRODUCED
IN
THE
KERNEL
IN
THE
EARLY
DAYS
OF
LINUX
MULTIPROCESSING
SUPPORT
ONLY
ONE
TASK
COULD
EXECUTE
IN
THE
KERNEL
AT
A
TIME
DURING
THIS
LIMITATION
WAS
REMOVED
AS
THE
LOCKING
MECHANISMS
GREW
MORE
FINE
GRAINED
THROUGH
AND
ONWARD
KERNEL
LOCKING
BECAME
EVEN
FINER
GRAINED
TODAY
IN
THE
LINUX
KERNEL
KERNEL
LOCKING
IS
VERY
FINE
GRAINED
AND
SCALABILITY
IS
GOOD
THE
GRANULARITY
OF
LOCKING
IS
A
DESCRIPTION
OF
THE
SIZE
OR
AMOUNT
OF
DATA
THAT
A
LOCK
PROTECTS
A
VERY
COARSE
LOCK
PROTECTS
A
LARGE
AMOUNT
OF
DATA
FOR
EXAMPLE
AN
ENTIRE
SUB
SYSTEM
SET
OF
DATA
STRUCTURES
ON
THE
OTHER
HAND
A
VERY
FINE
GRAINED
LOCK
PROTECTS
A
SMALL
AMOUNT
OF
DATA
SAY
ONLY
A
SINGLE
ELEMENT
IN
A
LARGER
STRUCTURE
IN
REALITY
MOST
LOCKS
FALL
SOMEWHERE
IN
BETWEEN
THESE
TWO
EXTREMES
PROTECTING
NEITHER
AN
ENTIRE
SUBSYSTEM
NOR
AN
INDIVIDUAL
ELEMENT
BUT
PERHAPS
A
SINGLE
STRUCTURE
OR
LIST
OF
STRUCTURES
MOST
LOCKS
START
OFF
FAIRLY
COARSE
AND
ARE
MADE
MORE
FINE
GRAINED
AS
LOCK
CONTENTION
PROVES
TO
BE
A
PROBLEM
ONE
EXAMPLE
OF
EVOLVING
TO
FINER
GRAINED
LOCKING
IS
THE
SCHEDULER
RUNQUEUES
DIS
CUSSED
IN
CHAPTER
PROCESS
SCHEDULING
IN
AND
PRIOR
KERNELS
THE
SCHEDULER
HAD
A
SINGLE
RUNQUEUE
RECALL
THAT
A
RUNQUEUE
IS
THE
LIST
OF
RUNNABLE
PROCESSES
EARLY
IN
THE
SERIES
THE
O
SCHEDULER
INTRODUCED
PER
PROCESSOR
RUNQUEUES
EACH
WITH
A
UNIQUE
LOCK
THE
LOCKING
EVOLVED
FROM
A
SINGLE
GLOBAL
LOCK
TO
SEPARATE
LOCKS
FOR
EACH
PROCESSOR
THIS
WAS
AN
IMPORTANT
OPTIMIZATION
BECAUSE
THE
RUNQUEUE
LOCK
WAS
HIGHLY
CONTENDED
ON
LARGE
MACHINES
ESSENTIALLY
SERIALIZING
THE
ENTIRE
SCHEDULING
PROCESS
DOWN
TO
A
SINGLE
PROCESSOR
EXECUTING
IN
THE
SCHEDULER
AT
A
TIME
LATER
IN
THE
SERIES
THE
CFS
SCHEDULER
IMPROVED
SCALABILITY
FURTHER
GENERALLY
THIS
SCALABILITY
IMPROVEMENT
IS
A
GOOD
THING
BECAUSE
IT
IMPROVES
LINUX
PER
FORMANCE
ON
LARGER
AND
MORE
POWERFUL
SYSTEMS
RAMPANT
SCALABILITY
IMPROVEMENTS
CAN
LEAD
TO
A
DECREASE
IN
PERFORMANCE
ON
SMALLER
SMP
AND
UP
MACHINES
HOWEVER
BECAUSE
SMALLER
MACHINES
MAY
NOT
NEED
SUCH
FINE
GRAINED
LOCKING
BUT
WILL
NONETHELESS
NEED
TO
PUT
UP
WITH
THE
INCREASED
COMPLEXITY
AND
OVERHEAD
CONSIDER
A
LINKED
LIST
AN
INITIAL
LOCKING
SCHEME
WOULD
PROVIDE
A
SINGLE
LOCK
FOR
THE
ENTIRE
LIST
IN
TIME
THIS
SINGLE
LOCK
MIGHT
PROVE
TO
BE
A
SCALABILITY
BOTTLENECK
ON
LARGE
MULTIPROCESSOR
MACHINES
THAT
FRE
QUENTLY
ACCESS
THIS
LINKED
LIST
IN
RESPONSE
THE
SINGLE
LOCK
COULD
BE
BROKEN
UP
INTO
ONE
LOCK
PER
NODE
IN
THE
LINKED
LIST
FOR
EACH
NODE
THAT
YOU
WANTED
TO
READ
OR
WRITE
YOU
OBTAINED
THE
NODE
UNIQUE
LOCK
NOW
THERE
IS
ONLY
LOCK
CONTENTION
WHEN
MULTIPLE
PROCESSORS
ARE
ACCESSING
THE
SAME
EXACT
NODE
WHAT
IF
THERE
IS
STILL
LOCK
CONTENTION
HOW
EVER
DO
YOU
PROVIDE
A
LOCK
FOR
EACH
ELEMENT
IN
EACH
NODE
EACH
BIT
OF
EACH
ELEMENT
THE
ANSWER
IS
NO
EVEN
THOUGH
THIS
FINE
GRAINED
LOCKING
MIGHT
ENSURE
EXCELLENT
SCALABILITY
ON
LARGE
SMP
MACHINES
HOW
DOES
IT
PERFORM
ON
DUAL
PROCESSOR
MACHINES
THE
OVERHEAD
OF
ALL
THOSE
EXTRA
LOCKS
IS
WASTED
IF
A
DUAL
PROCESSOR
MACHINE
DOES
NOT
SEE
SIGNIFICANT
LOCK
CONTENTION
TO
BEGIN
WITH
NONETHELESS
SCALABILITY
IS
AN
IMPORTANT
CONSIDERATION
DESIGNING
YOUR
LOCKING
FROM
THE
BEGINNING
TO
SCALE
WELL
IS
IMPORTANT
COARSE
LOCKING
OF
MAJOR
RESOURCES
CAN
EASILY
BECOME
A
BOTTLENECK
ON
EVEN
SMALL
MACHINES
THERE
IS
A
THIN
LINE
BETWEEN
TOO
COARSE
LOCKING
AND
TOO
FINE
LOCKING
LOCKING
THAT
IS
TOO
COARSE
RESULTS
IN
POOR
SCALABILITY
IF
THERE
IS
HIGH
LOCK
CONTENTION
WHEREAS
LOCKING
THAT
IS
TOO
FINE
RESULTS
IN
WASTEFUL
OVERHEAD
IF
THERE
IS
LITTLE
LOCK
CONTENTION
BOTH
SCENARIOS
EQUATE
TO
POOR
PERFORMANCE
START
SIMPLE
AND
GROW
IN
COM
PLEXITY
ONLY
AS
NEEDED
SIMPLICITY
IS
KEY
CONCLUSION
MAKING
YOUR
CODE
SMP
SAFE
IS
NOT
SOMETHING
THAT
CAN
BE
ADDED
AS
AN
AFTERTHOUGHT
PROPER
SYNCHRONIZATION
LOCKING
THAT
IS
FREE
OF
DEADLOCKS
SCALABLE
AND
CLEAN
REQUIRES
DESIGN
DECISIONS
FROM
START
THROUGH
FINISH
WHENEVER
YOU
WRITE
KERNEL
CODE
WHETHER
IT
IS
A
NEW
SYSTEM
CALL
OR
A
REWRITTEN
DRIVER
PROTECTING
DATA
FROM
CONCURRENT
ACCESS
NEEDS
TO
BE
A
PRIMARY
CONCERN
PROVIDE
SUFFICIENT
PROTECTION
FOR
EVERY
SCENARIO
SMP
KERNEL
PREEMPTION
AND
SO
ON
AND
REST
ASSURED
THE
DATA
WILL
BE
SAFE
ON
ANY
GIVEN
MACHINE
AND
CONFIGURATION
THE
NEXT
CHAPTER
DISCUSSES
JUST
HOW
TO
DO
THIS
WITH
THE
FUNDAMENTALS
AND
THE
THEORIES
OF
SYNCHRONIZATION
CONCURRENCY
AND
LOCKING
BEHIND
US
LET
NOW
DIVE
INTO
THE
ACTUAL
TOOLS
THAT
THE
LINUX
KERNEL
PROVIDES
TO
ENSURE
THAT
YOUR
CODE
IS
RACE
AND
DEADLOCK
FREE
KERNEL
SYNCHRONIZATION
METHODS
THE
PREVIOUS
CHAPTER
DISCUSSED
THE
SOURCES
OF
AND
SOLUTIONS
TO
RACE
CONDITIONS
THANK
FULLY
THE
LINUX
KERNEL
PROVIDES
A
FAMILY
OF
SYNCHRONIZATION
METHODS
THE
LINUX
KERNEL
SYNCHRONIZATION
METHODS
ENABLE
DEVELOPERS
TO
WRITE
EFFICIENT
AND
RACE
FREE
CODE
THIS
CHAPTER
DISCUSSES
THESE
METHODS
AND
THEIR
INTERFACES
BEHAVIOR
AND
USE
ATOMIC
OPERATIONS
WE
START
OUR
DISCUSSION
OF
SYNCHRONIZATION
METHODS
WITH
ATOMIC
OPERATIONS
BECAUSE
THEY
ARE
THE
FOUNDATION
ON
WHICH
OTHER
SYNCHRONIZATION
METHODS
ARE
BUILT
ATOMIC
OPERATIONS
PROVIDE
INSTRUCTIONS
THAT
EXECUTE
ATOMICALLY
WITHOUT
INTERRUPTION
JUST
AS
THE
ATOM
WAS
ORIGINALLY
THOUGHT
TO
BE
AN
INDIVISIBLE
PARTICLE
ATOMIC
OPERATORS
ARE
INDIVISIBLE
INSTRUC
TIONS
FOR
EXAMPLE
AS
DISCUSSED
IN
THE
PREVIOUS
CHAPTER
AN
ATOMIC
INCREMENT
CAN
READ
AND
INCREMENT
A
VARIABLE
BY
ONE
IN
A
SINGLE
INDIVISIBLE
AND
UNINTERRUPTIBLE
STEP
RECALL
THE
SIMPLE
RACE
IN
INCREMENTING
AN
INTEGER
THAT
WE
DISCUSSED
IN
THE
PREVIOUS
CHAPTER
THREAD
THREAD
GET
I
GET
I
INCREMENT
I
INCREMENT
I
WRITE
BACK
I
WRITE
BACK
I
WITH
ATOMIC
OPERATORS
THIS
RACE
DOES
NOT
INDEED
CANNOT
OCCUR
INSTEAD
THE
OUT
COME
IS
ALWAYS
ONE
OF
THE
FOLLOWING
THREAD
THREAD
GET
INCREMENT
AND
STORE
I
GET
INCREMENT
AND
STORE
I
OR
THREAD
THREAD
GET
INCREMENT
AND
STORE
I
GET
INCREMENT
AND
STORE
I
THE
ULTIMATE
VALUE
ALWAYS
NINE
IS
CORRECT
IT
IS
NEVER
POSSIBLE
FOR
THE
TWO
ATOMIC
OPER
ATIONS
TO
OCCUR
ON
THE
SAME
VARIABLE
CONCURRENTLY
THEREFORE
IT
IS
NOT
POSSIBLE
FOR
THE
IN
CREMENTS
TO
RACE
THE
KERNEL
PROVIDES
TWO
SETS
OF
INTERFACES
FOR
ATOMIC
OPERATIONS
ONE
THAT
OPERATES
ON
INTEGERS
AND
ANOTHER
THAT
OPERATES
ON
INDIVIDUAL
BITS
THESE
INTERFACES
ARE
IMPLEMENTED
ON
EVERY
ARCHITECTURE
THAT
LINUX
SUPPORTS
MOST
ARCHITECTURES
CONTAIN
INSTRUCTIONS
THAT
PRO
VIDE
ATOMIC
VERSIONS
OF
SIMPLE
ARITHMETIC
OPERATIONS
OTHER
ARCHITECTURES
LACKING
DIRECT
ATOMIC
OPERATIONS
PROVIDE
AN
OPERATION
TO
LOCK
THE
MEMORY
BUS
FOR
A
SINGLE
OPERATION
THUS
GUARANTEEING
THAT
ANOTHER
MEMORY
AFFECTING
OPERATION
CANNOT
OCCUR
SIMULTANEOUSLY
ATOMIC
INTEGER
OPERATIONS
THE
ATOMIC
INTEGER
METHODS
OPERATE
ON
A
SPECIAL
DATA
TYPE
THIS
SPECIAL
TYPE
IS
USED
AS
OPPOSED
TO
HAVING
THE
FUNCTIONS
WORK
DIRECTLY
ON
THE
C
INT
TYPE
FOR
SEVERAL
REA
SONS
FIRST
HAVING
THE
ATOMIC
FUNCTIONS
ACCEPT
ONLY
THE
TYPE
ENSURES
THAT
THE
ATOMIC
OPERATIONS
ARE
USED
ONLY
WITH
THESE
SPECIAL
TYPES
LIKEWISE
IT
ALSO
ENSURES
THAT
THE
DATA
TYPES
ARE
NOT
PASSED
TO
ANY
NONATOMIC
FUNCTIONS
INDEED
WHAT
GOOD
WOULD
ATOMIC
OPERATIONS
BE
IF
THEY
WERE
NOT
CONSISTENTLY
USED
ON
THE
DATA
NEXT
THE
USE
OF
ENSURES
THE
COMPILER
DOES
NOT
ERRONEOUSLY
BUT
CLEVERLY
OPTIMIZE
ACCESS
TO
THE
VALUE
IT
IS
IMPORTANT
THE
ATOMIC
OPERATIONS
RECEIVE
THE
CORRECT
MEMORY
ADDRESS
AND
NOT
AN
ALIAS
FINALLY
USE
OF
CAN
HIDE
ANY
ARCHITECTURE
SPECIFIC
DIFFERENCES
IN
ITS
IMPLEMENTA
TION
THE
TYPE
IS
DEFINED
IN
LINUX
TYPES
H
TYPEDEF
STRUCT
VOLATILE
INT
COUNTER
DESPITE
BEING
AN
INTEGER
AND
THUS
BITS
ON
ALL
THE
MACHINES
THAT
LINUX
SUPPORTS
DE
VELOPERS
AND
THEIR
CODE
ONCE
HAD
TO
ASSUME
THAT
AN
WAS
NO
LARGER
THAN
BITS
IN
SIZE
THE
SPARC
PORT
IN
LINUX
HAS
AN
ODD
IMPLEMENTATION
OF
ATOMIC
OPERATIONS
A
LOCK
WAS
EMBEDDED
IN
THE
LOWER
BITS
OF
THE
BIT
INT
IT
LOOKED
LIKE
FIGURE
THE
LOCK
WAS
USED
TO
PROTECT
CONCURRENT
ACCESS
TO
THE
ATOMIC
TYPE
BECAUSE
THE
SPARC
ARCHI
TECTURE
LACKS
APPROPRIATE
SUPPORT
AT
THE
INSTRUCTION
LEVEL
CONSEQUENTLY
ONLY
USABLE
BITS
WERE
AVAILABLE
ON
SPARC
MACHINES
ALTHOUGH
CODE
THAT
ASSUMED
THAT
THE
FULL
BIT
RANGE
EXISTED
WOULD
WORK
ON
OTHER
MACHINES
IT
WOULD
HAVE
FAILED
IN
STRANGE
AND
SUBTLE
WAYS
ON
SPARC
MACHINES
AND
THAT
IS
JUST
RUDE
RECENTLY
CLEVER
HACKS
HAVE
ALLOWED
SPARC
TO
PROVIDE
A
FULLY
USABLE
BIT
AND
THIS
LIMITATION
IS
NO
MORE
BIT
BIT
FIGURE
OLD
LAYOUT
OF
THE
BIT
ON
SPARC
THE
DECLARATIONS
NEEDED
TO
USE
THE
ATOMIC
INTEGER
OPERATIONS
ARE
IN
ASM
ATOMIC
H
SOME
ARCHITECTURES
PROVIDE
ADDITIONAL
METHODS
THAT
ARE
UNIQUE
TO
THAT
ARCHITECTURE
BUT
ALL
ARCHITECTURES
PROVIDE
AT
LEAST
A
MINIMUM
SET
OF
OPERATIONS
THAT
ARE
USED
THROUGHOUT
THE
KERNEL
WHEN
YOU
WRITE
KERNEL
CODE
YOU
CAN
ENSURE
THAT
THESE
OPERATIONS
ARE
CORRECTLY
IMPLEMENTED
ON
ALL
ARCHITECTURES
DEFINING
AN
IS
DONE
IN
THE
USUAL
MANNER
OPTIONALLY
YOU
CAN
SET
IT
TO
AN
INI
TIAL
VALUE
V
DEFINE
V
U
DEFINE
U
AND
INITIALIZE
IT
TO
ZERO
OPERATIONS
ARE
ALL
SIMPLE
V
V
ATOMICALLY
V
V
V
ATOMICALLY
V
V
V
ATOMICALLY
IF
YOU
EVER
NEED
TO
CONVERT
AN
TO
AN
INT
USE
PRINTK
D
N
V
WILL
PRINT
A
COMMON
USE
OF
THE
ATOMIC
INTEGER
OPERATIONS
IS
TO
IMPLEMENT
COUNTERS
PROTECTING
A
SOLE
COUNTER
WITH
A
COMPLEX
LOCKING
SCHEME
IS
OVERKILL
SO
INSTEAD
DEVELOPERS
USE
AND
WHICH
ARE
MUCH
LIGHTER
IN
WEIGHT
ANOTHER
USE
OF
THE
ATOMIC
INTEGER
OPERATORS
IS
ATOMICALLY
PERFORMING
AN
OPERATION
AND
TESTING
THE
RESULT
A
COMMON
EXAMPLE
IS
THE
ATOMIC
DECREMENT
AND
TEST
INT
V
THIS
FUNCTION
DECREMENTS
BY
ONE
THE
GIVEN
ATOMIC
VALUE
IF
THE
RESULT
IS
ZERO
IT
RETURNS
TRUE
OTHERWISE
IT
RETURNS
FALSE
A
FULL
LISTING
OF
THE
STANDARD
ATOMIC
INTEGER
OPERATIONS
THOSE
FOUND
ON
ALL
ARCHITECTURES
IS
IN
TABLE
ALL
THE
OPERATIONS
IMPLEMENTED
ON
A
SPECIFIC
ARCHITECTURE
CAN
BE
FOUND
IN
ASM
ATOMIC
H
TABLE
ATOMIC
INTEGER
METHODS
ATOMIC
INTEGER
OPERATION
DESCRIPTION
INT
I
AT
DECLARATION
INITIALIZE
TO
I
INT
V
ATOMICALLY
READ
THE
INTEGER
VALUE
OF
V
VOID
V
INT
I
ATOMICALLY
SET
V
EQUAL
TO
I
VOID
INT
I
V
ATOMICALLY
ADD
I
TO
V
VOID
INT
I
V
ATOMICALLY
SUBTRACT
I
FROM
V
VOID
V
ATOMICALLY
ADD
ONE
TO
V
VOID
V
ATOMICALLY
SUBTRACT
ONE
FROM
V
INT
INT
I
V
ATOMICALLY
SUBTRACT
I
FROM
V
AND
RETURN
TRUE
IF
THE
RESULT
IS
ZERO
OTHERWISE
FALSE
INT
INT
I
V
ATOMICALLY
ADD
I
TO
V
AND
RETURN
TRUE
IF
THE
RESULT
IS
NEGATIVE
OTHERWISE
FALSE
INT
INT
I
V
ATOMICALLY
ADD
I
TO
V
AND
RETURN
THE
RESULT
INT
INT
I
V
ATOMICALLY
SUBTRACT
I
FROM
V
AND
RETURN
THE
RESULT
INT
INT
I
V
ATOMICALLY
INCREMENT
V
BY
ONE
AND
RETURN
THE
RESULT
INT
INT
I
V
ATOMICALLY
DECREMENT
V
BY
ONE
AND
RETURN
THE
RESULT
INT
V
ATOMICALLY
DECREMENT
V
BY
ONE
AND
RETURN
TRUE
IF
ZERO
FALSE
OTHERWISE
INT
V
ATOMICALLY
INCREMENT
V
BY
ONE
AND
RETURN
TRUE
IF
THE
RESULT
IS
ZERO
FALSE
OTHERWISE
THE
ATOMIC
OPERATIONS
ARE
TYPICALLY
IMPLEMENTED
AS
INLINE
FUNCTIONS
WITH
INLINE
AS
SEMBLY
IN
THE
CASE
WHERE
A
SPECIFIC
FUNCTION
IS
INHERENTLY
ATOMIC
THE
GIVEN
FUNCTION
IS
USUALLY
JUST
A
MACRO
FOR
EXAMPLE
ON
MOST
ARCHITECTURES
A
WORD
SIZED
READ
IS
ALWAYS
ATOMIC
THAT
IS
A
READ
OF
A
SINGLE
WORD
CANNOT
COMPLETE
IN
THE
MIDDLE
OF
A
WRITE
TO
THAT
WORD
THE
READ
ALWAYS
RETURNS
THE
WORD
IN
A
CONSISTENT
STATE
EITHER
BEFORE
OR
AFTER
THE
WRITE
COMPLETES
BUT
NEVER
IN
THE
MIDDLE
CONSEQUENTLY
IS
USUALLY
JUST
A
MACRO
RETURNING
THE
INTEGER
VALUE
OF
THE
READ
ATOMIC
VARIABLE
V
POINTER
OF
TYPE
ATOMICALLY
READS
THE
VALUE
OF
V
STATIC
INLINE
INT
CONST
V
RETURN
V
COUNTER
ATOMICITY
VERSUS
ORDERING
THE
PRECEDING
DISCUSSION
ON
ATOMIC
READING
BEGS
A
DISCUSSION
ON
THE
DIFFERENCES
BETWEEN
ATOMICITY
AND
ORDERING
AS
DISCUSSED
A
WORD
SIZED
READ
ALWAYS
OCCURS
ATOMICALLY
IT
NEVER
IN
TERLEAVES
WITH
A
WRITE
TO
THE
SAME
WORD
THE
READ
ALWAYS
RETURNS
THE
WORD
IN
A
CONSISTENT
STATE
PERHAPS
BEFORE
THE
WRITE
COMPLETES
PERHAPS
AFTER
BUT
NEVER
DURING
FOR
EXAMPLE
IF
AN
INTEGER
IS
INITIALLY
AND
THEN
SET
TO
A
READ
ON
THE
INTEGER
ALWAYS
RETURNS
OR
AND
NEVER
SOME
COMMINGLING
OF
THE
TWO
VALUES
WE
CALL
THIS
ATOMICITY
YOUR
CODE
HOWEVER
MIGHT
HAVE
MORE
STRINGENT
REQUIREMENTS
THAN
THIS
PERHAPS
YOU
REQUIRE
THAT
THE
READ
ALWAYS
OCCURS
BEFORE
THE
PENDING
WRITE
THIS
TYPE
OF
REQUIREMENT
IS
NOT
ATOMIC
ITY
BUT
ORDERING
ATOMICITY
ENSURES
THAT
INSTRUCTIONS
OCCUR
WITHOUT
INTERRUPTION
AND
THAT
THEY
COMPLETE
EITHER
IN
THEIR
ENTIRETY
OR
NOT
AT
ALL
ORDERING
ON
THE
OTHER
HAND
ENSURES
THAT
THE
DESIRED
RELATIVE
ORDERING
OF
TWO
OR
MORE
INSTRUCTIONS
EVEN
IF
THEY
ARE
TO
OCCUR
IN
SEPARATE
THREADS
OF
EXECUTION
OR
EVEN
SEPARATE
PROCESSORS
IS
PRESERVED
THE
ATOMIC
OPERATIONS
DISCUSSED
IN
THIS
SECTION
GUARANTEE
ONLY
ATOMICITY
ORDERING
IS
EN
FORCED
VIA
BARRIER
OPERATIONS
WHICH
WE
DISCUSS
LATER
IN
THIS
CHAPTER
IN
YOUR
CODE
IT
IS
USUALLY
PREFERRED
TO
CHOOSE
ATOMIC
OPERATIONS
OVER
MORE
COMPLI
CATED
LOCKING
MECHANISMS
ON
MOST
ARCHITECTURES
ONE
OR
TWO
ATOMIC
OPERATIONS
INCUR
LESS
OVERHEAD
AND
LESS
CACHE
LINE
THRASHING
THAN
A
MORE
COMPLICATED
SYNCHRONIZATION
METHOD
AS
WITH
ANY
PERFORMANCE
SENSITIVE
CODE
HOWEVER
TESTING
MULTIPLE
APPROACHES
IS
ALWAYS
SMART
BIT
ATOMIC
OPERATIONS
WITH
THE
RISING
PREVALENCE
OF
BIT
ARCHITECTURES
IT
IS
NO
SURPRISE
THAT
THE
LINUX
KERNEL
DEVELOPERS
AUGMENTED
THE
BIT
TYPE
WITH
A
BIT
VARIANT
FOR
PORTABILITY
THE
SIZE
OF
CANNOT
CHANGE
BETWEEN
ARCHITECTURES
SO
IS
BIT
EVEN
ON
BIT
ARCHITECTURES
INSTEAD
THE
TYPE
PROVIDES
A
BIT
ATOMIC
INTEGER
THAT
FUNCTIONS
OTHERWISE
IDENTICAL
TO
ITS
BIT
BROTHER
USAGE
IS
EXACTLY
THE
SAME
EXCEPT
THAT
THE
USABLE
RANGE
OF
THE
INTEGER
IS
RATHER
THAN
BITS
NEARLY
ALL
THE
CLASSIC
BIT
ATOMIC
OPERATIONS
ARE
IMPLEMENTED
IN
BIT
VARIANTS
THEY
ARE
PREFIXED
WITH
IN
LIEU
OF
ATOMIC
TABLE
IS
A
FULL
LISTING
OF
THE
STANDARD
OPERATIONS
SOME
ARCHI
TECTURES
IMPLEMENT
MORE
BUT
THEY
ARE
NOT
PORTABLE
AS
WITH
THE
TYPE
IS
JUST
A
SIMPLE
WRAPPER
AROUND
AN
INTEGER
THIS
TYPE
A
LONG
TYPEDEF
STRUCT
VOLATILE
LONG
COUNTER
TABLE
ATOMIC
INTEGER
METHODS
ATOMIC
INTEGER
OPERATION
DESCRIPTION
LONG
I
AT
DECLARATION
INITIALIZE
TO
I
LONG
V
ATOMICALLY
READ
THE
INTEGER
VALUE
OF
V
VOID
V
INT
I
ATOMICALLY
SET
V
EQUAL
TO
I
VOID
INT
I
V
ATOMICALLY
ADD
I
TO
V
VOID
INT
I
V
ATOMICALLY
SUBTRACT
I
FROM
V
VOID
V
ATOMICALLY
ADD
ONE
TO
V
VOID
V
ATOMICALLY
SUBTRACT
ONE
FROM
V
INT
INT
I
V
ATOMICALLY
SUBTRACT
I
FROM
V
AND
RETURN
TRUE
IF
THE
RESULT
IS
ZERO
OTHERWISE
FALSE
INT
INT
I
V
ATOMICALLY
ADD
I
TO
V
AND
RETURN
TRUE
IF
THE
RESULT
IS
NEGATIVE
OTHERWISE
FALSE
LONG
INT
I
V
ATOMICALLY
ADD
I
TO
V
AND
RETURN
THE
RESULT
LONG
INT
I
V
ATOMICALLY
SUBTRACT
I
FROM
V
AND
RETURN
THE
RESULT
LONG
INT
I
V
ATOMICALLY
INCREMENT
V
BY
ONE
AND
RETURN
THE
RESULT
LONG
INT
I
V
ATOMICALLY
DECREMENT
V
BY
ONE
AND
RETURN
THE
RESULT
INT
V
ATOMICALLY
DECREMENT
V
BY
ONE
AND
RETURN
TRUE
IF
ZERO
FALSE
OTHERWISE
INT
V
ATOMICALLY
INCREMENT
V
BY
ONE
AND
RETURN
TRUE
IF
THE
RESULT
IS
ZERO
FALSE
OTHERWISE
ALL
BIT
ARCHITECTURES
PROVIDE
AND
A
FAMILY
OF
ARITHMETIC
FUNCTIONS
TO
OPERATE
ON
IT
MOST
BIT
ARCHITECTURES
DO
NOT
HOWEVER
SUPPORT
IS
A
NOTABLE
EXCEPTION
FOR
PORTABILITY
BETWEEN
ALL
LINUX
SUPPORTED
ARCHITECTURES
DEVELOP
ERS
SHOULD
USE
THE
BIT
TYPE
THE
BIT
IS
RESERVED
FOR
CODE
THAT
IS
BOTH
ARCHITECTURE
SPECIFIC
AND
THAT
REQUIRES
BITS
ATOMIC
BITWISE
OPERATIONS
IN
ADDITION
TO
ATOMIC
INTEGER
OPERATIONS
THE
KERNEL
ALSO
PROVIDES
A
FAMILY
OF
FUNCTIONS
THAT
OPERATE
AT
THE
BIT
LEVEL
NOT
SURPRISINGLY
THEY
ARE
ARCHITECTURE
SPECIFIC
AND
DEFINED
IN
ASM
BITOPS
H
WHAT
MIGHT
BE
SURPRISING
IS
THAT
THE
BITWISE
FUNCTIONS
OPERATE
ON
GENERIC
MEMORY
AD
DRESSES
THE
ARGUMENTS
ARE
A
POINTER
AND
A
BIT
NUMBER
BIT
ZERO
IS
THE
LEAST
SIGNIFICANT
BIT
OF
THE
GIVEN
ADDRESS
ON
BIT
MACHINES
BIT
IS
THE
MOST
SIGNIFICANT
BIT
AND
BIT
IS
THE
LEAST
SIGNIFICANT
BIT
OF
THE
FOLLOWING
WORD
THERE
ARE
NO
LIMITATIONS
ON
THE
BIT
NUMBER
SUPPLIED
ALTHOUGH
MOST
USES
OF
THE
FUNCTIONS
PROVIDE
A
WORD
AND
CONSEQUENTLY
A
BIT
NUMBER
BETWEEN
AND
ON
BIT
MACHINES
AND
AND
ON
BIT
MACHINES
BECAUSE
THE
FUNCTIONS
OPERATE
ON
A
GENERIC
POINTER
THERE
IS
NO
EQUIVALENT
OF
THE
ATOMIC
INTEGER
TYPE
INSTEAD
YOU
CAN
WORK
WITH
A
POINTER
TO
WHATEVER
DATA
YOU
WANT
CONSIDER
AN
EXAMPLE
UNSIGNED
LONG
WORD
WORD
BIT
ZERO
IS
NOW
SET
ATOMICALLY
WORD
BIT
ONE
IS
NOW
SET
ATOMICALLY
PRINTK
UL
N
WORD
WILL
PRINT
WORD
BIT
ONE
IS
NOW
UNSET
ATOMICALLY
WORD
BIT
ZERO
IS
FLIPPED
NOW
IT
IS
UNSET
ATOMICALLY
ATOMICALLY
SETS
BIT
ZERO
AND
RETURNS
THE
PREVIOUS
VALUE
ZERO
IF
WORD
NEVER
TRUE
THE
FOLLOWING
IS
LEGAL
YOU
CAN
MIX
ATOMIC
BIT
INSTRUCTIONS
WITH
NORMAL
C
WORD
A
LISTING
OF
THE
STANDARD
ATOMIC
BIT
OPERATIONS
IS
IN
TABLE
TABLE
ATOMIC
BITWISE
METHODS
ATOMIC
BITWISE
OPERATION
DESCRIPTION
VOID
INT
NR
VOID
ADDR
ATOMICALLY
SET
THE
NR
TH
BIT
STARTING
FROM
ADDR
VOID
INT
NR
VOID
ADDR
ATOMICALLY
CLEAR
THE
NR
TH
BIT
STARTING
FROM
ADDR
VOID
INT
NR
VOID
ADDR
ATOMICALLY
FLIP
THE
VALUE
OF
THE
NR
TH
BIT
STARTING
FROM
ADDR
INT
INT
NR
VOID
ADDR
ATOMICALLY
SET
THE
NR
TH
BIT
STARTING
FROM
ADDR
AND
RETURN
THE
PREVIOUS
VALUE
INT
INT
NR
VOID
ADDR
ATOMICALLY
CLEAR
THE
NR
TH
BIT
STARTING
FROM
ADDR
AND
RETURN
THE
PREVIOUS
VALUE
INT
INT
NR
VOID
ADDR
ATOMICALLY
FLIP
THE
NR
TH
BIT
STARTING
FROM
ADDR
AND
RETURN
THE
PREVIOUS
VALUE
INT
INT
NR
VOID
ADDR
ATOMICALLY
RETURN
THE
VALUE
OF
THE
NR
TH
BIT
STARTING
FROM
ADDR
CONVENIENTLY
NONATOMIC
VERSIONS
OF
ALL
THE
BITWISE
FUNCTIONS
ARE
ALSO
PROVIDED
THEY
BEHAVE
IDENTICALLY
TO
THEIR
ATOMIC
SIBLINGS
EXCEPT
THEY
DO
NOT
GUARANTEE
ATOMICITY
AND
THEIR
NAMES
ARE
PREFIXED
WITH
DOUBLE
UNDERSCORES
FOR
EXAMPLE
THE
NONATOMIC
FORM
OF
IS
IF
YOU
DO
NOT
REQUIRE
ATOMICITY
SAY
FOR
EXAMPLE
BECAUSE
A
LOCK
ALREADY
PROTECTS
YOUR
DATA
THESE
VARIANTS
OF
THE
BITWISE
FUNCTIONS
MIGHT
BE
FASTER
WHAT
THE
HECK
IS
A
NONATOMIC
BIT
OPERATION
ON
FIRST
GLANCE
THE
CONCEPT
OF
A
NONATOMIC
BIT
OPERATION
MIGHT
NOT
MAKE
ANY
SENSE
ONLY
A
SINGLE
BIT
IS
INVOLVED
THUS
THERE
IS
NO
POSSIBILITY
OF
INCONSISTENCY
IF
ONE
OF
THE
OPERATIONS
SUCCEEDS
WHAT
ELSE
COULD
MATTER
SURE
ORDERING
MIGHT
BE
IMPORTANT
BUT
WE
ARE
TALKING
ABOUT
ATOMICITY
HERE
AT
THE
END
OF
THE
DAY
IF
THE
BIT
HAS
A
VALUE
PROVIDED
BY
ANY
OF
THE
IN
STRUCTIONS
WE
SHOULD
BE
GOOD
TO
GO
RIGHT
LET
JUMP
BACK
TO
JUST
WHAT
ATOMICITY
MEANS
ATOMICITY
REQUIRES
THAT
EITHER
INSTRUCTIONS
SUCCEED
IN
THEIR
ENTIRETY
UNINTERRUPTED
OR
INSTRUCTIONS
FAIL
TO
EXECUTE
AT
ALL
THEREFORE
IF
YOU
ISSUE
TWO
ATOMIC
BIT
OPERATIONS
YOU
EXPECT
TWO
OPERATIONS
TO
SUCCEED
AFTER
BOTH
OPERA
TIONS
COMPLETE
THE
BIT
NEEDS
TO
HAVE
THE
VALUE
AS
SPECIFIED
BY
THE
SECOND
OPERATION
MORE
OVER
HOWEVER
AT
SOME
POINT
IN
TIME
PRIOR
TO
THE
FINAL
OPERATION
THE
BIT
NEEDS
TO
HOLD
THE
VALUE
AS
SPECIFIED
BY
THE
FIRST
OPERATION
PUT
MORE
GENERALLY
REAL
ATOMICITY
REQUIRES
THAT
ALL
INTERMEDIATE
STATES
BE
CORRECTLY
REALIZED
FOR
EXAMPLE
ASSUME
YOU
ISSUE
TWO
ATOMIC
BIT
OPERATIONS
INITIALLY
SET
THE
BIT
AND
THEN
CLEAR
THE
BIT
WITHOUT
ATOMIC
OPERATIONS
THE
BIT
MIGHT
END
UP
CLEARED
BUT
IT
MIGHT
NEVER
HAVE
BEEN
SET
THE
SET
OPERATION
COULD
OCCUR
SIMULTANEOUSLY
WITH
THE
CLEAR
OPERATION
AND
FAIL
THE
CLEAR
OPERATION
WOULD
SUCCEED
AND
THE
BIT
WOULD
EMERGE
CLEARED
AS
INTENDED
WITH
ATOMIC
OPERATIONS
HOWEVER
THE
SET
WOULD
ACTUALLY
OCCUR
THERE
WOULD
BE
A
MOMENT
IN
TIME
WHEN
A
READ
WOULD
SHOW
THE
BIT
AS
SET
AND
THEN
THE
CLEAR
WOULD
EXECUTE
AND
THE
BIT
WOULD
BE
ZERO
THIS
BEHAVIOR
CAN
BE
IMPORTANT
ESPECIALLY
WHEN
ORDERING
COMES
INTO
PLAY
OR
WHEN
DEALING
WITH
HARDWARE
REGISTERS
THE
KERNEL
ALSO
PROVIDES
ROUTINES
TO
FIND
THE
FIRST
SET
OR
UNSET
BIT
STARTING
AT
A
GIVEN
ADDRESS
INT
UNSIGNED
LONG
ADDR
UNSIGNED
INT
SIZE
INT
UNSIGNED
LONG
ADDR
UNSIGNED
INT
SIZE
BOTH
FUNCTIONS
TAKE
A
POINTER
AS
THEIR
FIRST
ARGUMENT
AND
THE
NUMBER
OF
BITS
IN
TOTAL
TO
SEARCH
AS
THEIR
SECOND
THEY
RETURN
THE
BIT
NUMBER
OF
THE
FIRST
SET
OR
FIRST
UNSET
BIT
RESPEC
TIVELY
IF
YOUR
CODE
IS
SEARCHING
ONLY
A
WORD
THE
ROUTINES
FFS
AND
FFZ
WHICH
TAKE
A
SINGLE
PARAMETER
OF
THE
WORD
IN
WHICH
TO
SEARCH
ARE
OPTIMAL
UNLIKE
THE
ATOMIC
INTEGER
OPERATIONS
CODE
TYPICALLY
HAS
NO
CHOICE
WHETHER
TO
USE
THE
BITWISE
OPERATIONS
THEY
ARE
THE
ONLY
PORTABLE
WAY
TO
SET
A
SPECIFIC
BIT
THE
ONLY
QUESTION
IS
WHETHER
TO
USE
THE
ATOMIC
OR
NONATOMIC
VARIANTS
IF
YOUR
CODE
IS
INHERENTLY
SAFE
FROM
RACE
CONDITIONS
YOU
CAN
USE
THE
NONATOMIC
VERSIONS
WHICH
MIGHT
BE
FASTER
DEPENDING
ON
THE
ARCHITECTURE
SPIN
LOCKS
ALTHOUGH
IT
WOULD
BE
NICE
IF
EVERY
CRITICAL
REGION
CONSISTED
OF
CODE
THAT
DID
NOTHING
MORE
COMPLICATED
THAN
INCREMENTING
A
VARIABLE
REALITY
IS
MUCH
CRUELER
IN
REAL
LIFE
CRITICAL
RE
GIONS
CAN
SPAN
MULTIPLE
FUNCTIONS
FOR
EXAMPLE
IT
IS
OFTEN
THE
CASE
THAT
DATA
MUST
BE
RE
MOVED
FROM
ONE
STRUCTURE
FORMATTED
AND
PARSED
AND
ADDED
TO
ANOTHER
STRUCTURE
THIS
ENTIRE
OPERATION
MUST
OCCUR
ATOMICALLY
IT
MUST
NOT
BE
POSSIBLE
FOR
OTHER
CODE
TO
READ
FROM
OR
WRITE
TO
EITHER
STRUCTURE
BEFORE
THE
UPDATE
IS
COMPLETED
BECAUSE
SIMPLE
ATOMIC
OPERATIONS
ARE
CLEARLY
INCAPABLE
OF
PROVIDING
THE
NEEDED
PROTECTION
IN
SUCH
A
COMPLEX
SCE
NARIO
A
MORE
GENERAL
METHOD
OF
SYNCHRONIZATION
IS
NEEDED
LOCKS
THE
MOST
COMMON
LOCK
IN
THE
LINUX
KERNEL
IS
THE
SPIN
LOCK
A
SPIN
LOCK
IS
A
LOCK
THAT
CAN
BE
HELD
BY
AT
MOST
ONE
THREAD
OF
EXECUTION
IF
A
THREAD
OF
EXECUTION
ATTEMPTS
TO
AC
QUIRE
A
SPIN
LOCK
WHILE
IT
IS
ALREADY
HELD
WHICH
IS
CALLED
CONTENDED
THE
THREAD
BUSY
LOOPS
SPINS
WAITING
FOR
THE
LOCK
TO
BECOME
AVAILABLE
IF
THE
LOCK
IS
NOT
CONTENDED
THE
THREAD
CAN
IMMEDIATELY
ACQUIRE
THE
LOCK
AND
CONTINUE
THE
SPINNING
PREVENTS
MORE
THAN
ONE
THREAD
OF
EXECUTION
FROM
ENTERING
THE
CRITICAL
REGION
AT
ANY
ONE
TIME
THE
SAME
LOCK
CAN
BE
USED
IN
MULTIPLE
LOCATIONS
SO
ALL
ACCESS
TO
A
GIVEN
DATA
STRUCTURE
FOR
EXAMPLE
CAN
BE
PROTECTED
AND
SYNCHRONIZED
GOING
BACK
TO
THE
DOOR
AND
KEY
ANALOGY
FROM
THE
LAST
CHAPTER
SPIN
LOCKS
ARE
AKIN
TO
SITTING
OUTSIDE
THE
DOOR
WAITING
FOR
THE
FELLOW
INSIDE
TO
COME
OUT
AND
HAND
YOU
THE
KEY
IF
YOU
REACH
THE
DOOR
AND
NO
ONE
IS
INSIDE
YOU
CAN
GRAB
THE
KEY
AND
ENTER
THE
ROOM
IF
YOU
REACH
THE
DOOR
AND
SOMEONE
IS
CURRENTLY
INSIDE
YOU
MUST
WAIT
OUTSIDE
FOR
THE
KEY
EFFEC
TIVELY
CHECKING
FOR
ITS
PRESENCE
REPEATEDLY
WHEN
THE
ROOM
IS
VACATED
YOU
CAN
GRAB
THE
KEY
AND
GO
INSIDE
THANKS
TO
THE
KEY
READ
SPIN
LOCK
ONLY
ONE
PERSON
READ
THREAD
OF
EX
ECUTION
IS
ALLOWED
INSIDE
THE
ROOM
READ
CRITICAL
REGION
AT
THE
SAME
TIME
THE
FACT
THAT
A
CONTENDED
SPIN
LOCK
CAUSES
THREADS
TO
SPIN
ESSENTIALLY
WASTING
PROCESSOR
TIME
WHILE
WAITING
FOR
THE
LOCK
TO
BECOME
AVAILABLE
IS
SALIENT
THIS
BEHAVIOR
IS
THE
POINT
OF
THE
SPIN
LOCK
IT
IS
NOT
WISE
TO
HOLD
A
SPIN
LOCK
FOR
A
LONG
TIME
THIS
IS
THE
NATURE
OF
THE
SPIN
LOCK
A
LIGHTWEIGHT
SINGLE
HOLDER
LOCK
THAT
SHOULD
BE
HELD
FOR
SHORT
DURATIONS
AN
AL
TERNATIVE
BEHAVIOR
WHEN
THE
LOCK
IS
CONTENDED
IS
TO
PUT
THE
CURRENT
THREAD
TO
SLEEP
AND
WAKE
IT
UP
WHEN
IT
BECOMES
AVAILABLE
THEN
THE
PROCESSOR
CAN
GO
OFF
AND
EXECUTE
OTHER
CODE
THIS
INCURS
A
BIT
OF
OVERHEAD
MOST
NOTABLY
THE
TWO
CONTEXT
SWITCHES
REQUIRED
TO
SWITCH
OUT
OF
AND
BACK
INTO
THE
BLOCKING
THREAD
WHICH
IS
CERTAINLY
A
LOT
MORE
CODE
THAN
THE
HANDFUL
OF
LINES
USED
TO
IMPLEMENT
A
SPIN
LOCK
THEREFORE
IT
IS
WISE
TO
HOLD
SPIN
LOCKS
FOR
LESS
THAN
THE
DURATION
OF
TWO
CONTEXT
SWITCHES
BECAUSE
MOST
OF
US
HAVE
BETTER
THINGS
TO
DO
THAN
MEASURE
CONTEXT
SWITCHES
JUST
TRY
TO
HOLD
THE
LOCK
FOR
AS
LITTLE
TIME
AS
POSSIBLE
LATER
IN
THIS
CHAPTER
WE
DISCUSS
SEMAPHORES
WHICH
PROVIDE
A
LOCK
THAT
MAKES
THE
WAITING
THREAD
SLEEP
RATHER
THAN
SPIN
WHEN
CONTENDED
SPIN
LOCK
METHODS
SPIN
LOCKS
ARE
ARCHITECTURE
DEPENDENT
AND
IMPLEMENTED
IN
ASSEMBLY
THE
ARCHITECTURE
DEPENDENT
CODE
IS
DEFINED
IN
ASM
SPINLOCK
H
THE
ACTUAL
USABLE
INTERFACES
ARE
DEFINED
IN
LINUX
SPINLOCK
H
THE
BASIC
USE
OF
A
SPIN
LOCK
IS
THIS
IS
ESPECIALLY
IMPORTANT
NOW
THAT
THE
KERNEL
IS
PREEMPTIVE
THE
DURATION
THAT
LOCKS
ARE
HELD
IS
EQUIVALENT
TO
THE
SCHEDULING
LATENCY
OF
THE
SYSTEM
CRITICAL
REGION
THE
LOCK
CAN
BE
HELD
SIMULTANEOUSLY
BY
AT
MOST
ONLY
ONE
THREAD
OF
EXECUTION
CONSE
QUENTLY
ONLY
ONE
THREAD
IS
ALLOWED
IN
THE
CRITICAL
REGION
AT
A
TIME
THIS
PROVIDES
THE
NEEDED
PROTECTION
FROM
CONCURRENCY
ON
MULTIPROCESSING
MACHINES
ON
UNIPROCESSOR
MA
CHINES
THE
LOCKS
COMPILE
AWAY
AND
DO
NOT
EXIST
THEY
SIMPLY
ACT
AS
MARKERS
TO
DISABLE
AND
ENABLE
KERNEL
PREEMPTION
IF
KERNEL
PREEMPT
IS
TURNED
OFF
THE
LOCKS
COMPILE
AWAY
ENTIRELY
WARNING
SPIN
LOCKS
ARE
NOT
RECURSIVE
UNLIKE
SPIN
LOCK
IMPLEMENTATIONS
IN
OTHER
OPERATING
SYSTEMS
AND
THREADING
LIBRARIES
THE
LINUX
KERNEL
SPIN
LOCKS
ARE
NOT
RECURSIVE
THIS
MEANS
THAT
IF
YOU
ATTEMPT
TO
ACQUIRE
A
LOCK
YOU
ALREADY
HOLD
YOU
WILL
SPIN
WAITING
FOR
YOURSELF
TO
RELEASE
THE
LOCK
BUT
BECAUSE
YOU
ARE
BUSY
SPINNING
YOU
WILL
NEVER
RELEASE
THE
LOCK
AND
YOU
WILL
DEADLOCK
BE
CAREFUL
SPIN
LOCKS
CAN
BE
USED
IN
INTERRUPT
HANDLERS
WHEREAS
SEMAPHORES
CANNOT
BE
USED
BE
CAUSE
THEY
SLEEP
IF
A
LOCK
IS
USED
IN
AN
INTERRUPT
HANDLER
YOU
MUST
ALSO
DISABLE
LOCAL
INTER
RUPTS
INTERRUPT
REQUESTS
ON
THE
CURRENT
PROCESSOR
BEFORE
OBTAINING
THE
LOCK
OTHERWISE
IT
IS
POSSIBLE
FOR
AN
INTERRUPT
HANDLER
TO
INTERRUPT
KERNEL
CODE
WHILE
THE
LOCK
IS
HELD
AND
AT
TEMPT
TO
REACQUIRE
THE
LOCK
THE
INTERRUPT
HANDLER
SPINS
WAITING
FOR
THE
LOCK
TO
BECOME
AVAILABLE
THE
LOCK
HOLDER
HOWEVER
DOES
NOT
RUN
UNTIL
THE
INTERRUPT
HANDLER
COMPLETES
THIS
IS
AN
EXAMPLE
OF
THE
DOUBLE
ACQUIRE
DEADLOCK
DISCUSSED
IN
THE
PREVIOUS
CHAPTER
NOTE
THAT
YOU
NEED
TO
DISABLE
INTERRUPTS
ONLY
ON
THE
CURRENT
PROCESSOR
IF
AN
INTERRUPT
OCCURS
ON
A
DIFFERENT
PROCESSOR
AND
IT
SPINS
ON
THE
SAME
LOCK
IT
DOES
NOT
PREVENT
THE
LOCK
HOLDER
WHICH
IS
ON
A
DIFFERENT
PROCESSOR
FROM
EVENTUALLY
RELEASING
THE
LOCK
THE
KERNEL
PROVIDES
AN
INTERFACE
THAT
CONVENIENTLY
DISABLES
INTERRUPTS
AND
ACQUIRES
THE
LOCK
USAGE
IS
UNSIGNED
LONG
FLAGS
FLAGS
CRITICAL
REGION
FLAGS
THE
ROUTINE
SAVES
THE
CURRENT
STATE
OF
INTERRUPTS
DISABLES
THEM
LOCALLY
AND
THEN
OBTAINS
THE
GIVEN
LOCK
CONVERSELY
UNLOCKS
THE
GIVEN
LOCK
AND
RETURNS
INTERRUPTS
TO
THEIR
PREVIOUS
STATE
THIS
WAY
IF
INTERRUPTS
WERE
INITIALLY
DISABLED
YOUR
CODE
WOULD
NOT
ERRONEOUSLY
ENABLE
THEM
BUT
INSTEAD
KEEP
THEM
DISABLED
NOTE
THAT
THE
FLAGS
VARIABLE
IS
SEEMINGLY
PASSED
BY
VALUE
THIS
IS
BECAUSE
THE
LOCK
ROUTINES
ARE
IMPLEMENTED
PARTIALLY
AS
MACROS
ON
UNIPROCESSOR
SYSTEMS
THE
PREVIOUS
EXAMPLE
MUST
STILL
DISABLE
INTERRUPTS
TO
PREVENT
AN
INTERRUPT
HANDLER
FROM
ACCESSING
THE
SHARED
DATA
BUT
THE
LOCK
MECHANISM
IS
COMPILED
AWAY
THE
LOCK
AND
UNLOCK
ALSO
DISABLE
AND
ENABLE
KERNEL
PREEMPTION
RESPECTIVELY
WHAT
DO
I
LOCK
IT
IS
IMPORTANT
THAT
EACH
LOCK
IS
CLEARLY
ASSOCIATED
WITH
WHAT
IT
IS
LOCKING
MORE
IMPORTANT
YOU
SHOULD
PROTECT
DATA
AND
NOT
CODE
DESPITE
THE
EXAMPLES
IN
THIS
CHAPTER
EXPLAINING
THE
IMPORTANCE
OF
PROTECTING
THE
CRITICAL
SECTIONS
IT
IS
THE
ACTUAL
DATA
INSIDE
THAT
NEEDS
PROTEC
TION
AND
NOT
THE
CODE
BIG
FAT
RULE
LOCKS
THAT
SIMPLY
WRAP
CODE
REGIONS
ARE
HARD
TO
UNDERSTAND
AND
PRONE
TO
RACE
CONDITIONS
LOCK
DATA
NOT
CODE
RATHER
THAN
LOCK
CODE
ALWAYS
ASSOCIATE
YOUR
SHARED
DATA
WITH
A
SPECIFIC
LOCK
FOR
EXAMPLE
THE
STRUCT
FOO
IS
LOCKED
BY
WHENEVER
YOU
ACCESS
SHARED
DATA
MAKE
SURE
IT
IS
SAFE
MOST
LIKELY
THIS
MEANS
OBTAINING
THE
APPROPRIATE
LOCK
BEFORE
MANIPULATING
THE
DATA
AND
RELEASING
THE
LOCK
WHEN
FINISHED
IF
YOU
ALWAYS
KNOW
BEFORE
THE
FACT
THAT
INTERRUPTS
ARE
INITIALLY
ENABLED
THERE
IS
NO
NEED
TO
RESTORE
THEIR
PREVIOUS
STATE
YOU
CAN
UNCONDITIONALLY
ENABLE
THEM
ON
UNLOCK
IN
THOSE
CASES
AND
ARE
OPTIMAL
CRITICAL
SECTION
AS
THE
KERNEL
GROWS
IN
SIZE
AND
COMPLEXITY
IT
IS
INCREASINGLY
HARD
TO
ENSURE
THAT
INTERRUPTS
ARE
ALWAYS
ENABLED
IN
ANY
GIVEN
CODE
PATH
IN
THE
KERNEL
USE
OF
THEREFORE
IS
NOT
RECOMMENDED
IF
YOU
DO
USE
IT
YOU
HAD
BETTER
BE
POSI
TIVE
THAT
INTERRUPTS
WERE
ORIGINALLY
ON
OR
PEOPLE
WILL
BE
UPSET
WHEN
THEY
EXPECT
INTERRUPTS
TO
BE
OFF
BUT
FIND
THEM
ON
DEBUGGING
SPIN
LOCKS
THE
CONFIGURE
OPTION
ENABLES
A
HANDFUL
OF
DEBUGGING
CHECKS
IN
THE
SPIN
LOCK
CODE
FOR
EXAMPLE
WITH
THIS
OPTION
THE
SPIN
LOCK
CODE
CHECKS
FOR
THE
USE
OF
UNINITIALIZED
SPIN
LOCKS
AND
UNLOCKING
A
LOCK
THAT
IS
NOT
YET
LOCKED
WHEN
TESTING
YOUR
CODE
YOU
SHOULD
ALWAYS
RUN
WITH
SPIN
LOCK
DEBUGGING
ENABLED
FOR
ADDITIONAL
DEBUGGING
OF
LOCK
LIFECYCLES
ENABLE
OTHER
SPIN
LOCK
METHODS
YOU
CAN
USE
THE
METHOD
TO
INITIALIZE
A
DYNAMICALLY
CREATED
SPIN
LOCK
A
THAT
YOU
DO
NOT
HAVE
A
DIRECT
REFERENCE
TO
JUST
A
POINTER
THE
METHOD
ATTEMPTS
TO
OBTAIN
THE
GIVEN
SPIN
LOCK
IF
THE
LOCK
IS
CONTENDED
RATHER
THAN
SPIN
AND
WAIT
FOR
THE
LOCK
TO
BE
RELEASED
THE
FUNCTION
IMMEDIATELY
RETURNS
ZERO
IF
IT
SUCCEEDS
IN
OBTAINING
THE
LOCK
IT
RETURNS
NONZERO
SIMILARLY
RETURNS
NONZERO
IF
THE
GIVEN
LOCK
IS
CURRENTLY
ACQUIRED
OTHERWISE
IT
RETURNS
ZERO
IN
NEITHER
CASE
DOES
ACTUALLY
OBTAIN
THE
LOCK
TABLE
SHOWS
A
COMPLETE
LIST
OF
THE
STANDARD
SPIN
LOCK
METHODS
TABLE
SPIN
LOCK
METHODS
METHOD
DESCRIPTION
ACQUIRES
GIVEN
LOCK
DISABLES
LOCAL
INTERRUPTS
AND
ACQUIRES
GIVEN
LOCK
SAVES
CURRENT
STATE
OF
LOCAL
INTERRUPTS
DISABLES
LOCAL
INTER
RUPTS
AND
ACQUIRES
GIVEN
LOCK
RELEASES
GIVEN
LOCK
RELEASES
GIVEN
LOCK
AND
ENABLES
LOCAL
INTERRUPTS
RELEASES
GIVEN
LOCK
AND
RESTORES
LOCAL
INTERRUPTS
TO
GIVEN
PRE
VIOUS
STATE
DYNAMICALLY
INITIALIZES
GIVEN
TRIES
TO
ACQUIRE
GIVEN
LOCK
IF
UNAVAILABLE
RETURNS
NONZERO
RETURNS
NONZERO
IF
THE
GIVEN
LOCK
IS
CURRENTLY
ACQUIRED
OTHER
WISE
IT
RETURNS
ZERO
SPIN
LOCKS
AND
BOTTOM
HALVES
AS
DISCUSSED
IN
CHAPTER
BOTTOM
HALVES
AND
DEFERRING
WORK
CERTAIN
LOCKING
PRECAU
TIONS
MUST
BE
TAKEN
WHEN
WORKING
WITH
BOTTOM
HALVES
THE
FUNCTION
OBTAINS
THE
GIVEN
LOCK
AND
DISABLES
ALL
BOTTOM
HALVES
THE
FUNCTION
PERFORMS
THE
INVERSE
BECAUSE
A
BOTTOM
HALF
MIGHT
PREEMPT
PROCESS
CONTEXT
CODE
IF
DATA
IS
SHARED
BETWEEN
A
BOTTOM
HALF
PROCESS
CONTEXT
YOU
MUST
PROTECT
THE
DATA
IN
PROCESS
CONTEXT
WITH
BOTH
A
LOCK
AND
THE
DISABLING
OF
BOTTOM
HALVES
LIKEWISE
BECAUSE
AN
INTERRUPT
HANDLER
MIGHT
PREEMPT
A
BOTTOM
HALF
IF
DATA
IS
SHARED
BETWEEN
AN
INTERRUPT
HANDLER
AND
A
BOTTOM
HALF
YOU
MUST
BOTH
OBTAIN
THE
APPROPRIATE
LOCK
AND
DISABLE
INTERRUPTS
USE
OF
THESE
TWO
FUNCTIONS
CAN
LEAD
TO
CONVOLUTED
CODE
YOU
SHOULD
NOT
FREQUENTLY
HAVE
TO
CHECK
THE
VALUES
OF
SPIN
LOCKS
YOUR
CODE
SHOULD
EITHER
ALWAYS
ACQUIRE
THE
LOCK
ITSELF
OR
ALWAYS
BE
CALLED
WHILE
THE
LOCK
IS
ALREADY
HELD
SOME
LEGITIMATE
USES
DO
EXIST
HOWEVER
SO
THESE
INTERFACES
ARE
PROVIDED
RECALL
THAT
TWO
TASKLETS
OF
THE
SAME
TYPE
DO
NOT
EVER
RUN
SIMULTANEOUSLY
THUS
THERE
IS
NO
NEED
TO
PROTECT
DATA
USED
ONLY
WITHIN
A
SINGLE
TYPE
OF
TASKLET
IF
THE
DATA
IS
SHARED
BE
TWEEN
TWO
DIFFERENT
TASKLETS
HOWEVER
YOU
MUST
OBTAIN
A
NORMAL
SPIN
LOCK
BEFORE
ACCESS
ING
THE
DATA
IN
THE
BOTTOM
HALF
YOU
DO
NOT
NEED
TO
DISABLE
BOTTOM
HALVES
BECAUSE
A
TASKLET
NEVER
PREEMPTS
ANOTHER
RUNNING
TASKLET
ON
THE
SAME
PROCESSOR
WITH
SOFTIRQS
REGARDLESS
OF
WHETHER
IT
IS
THE
SAME
SOFTIRQ
TYPE
IF
DATA
IS
SHARED
BY
SOFTIRQS
IT
MUST
BE
PROTECTED
WITH
A
LOCK
RECALL
THAT
SOFTIRQS
EVEN
TWO
OF
THE
SAME
TYPE
MIGHT
RUN
SIMULTANEOUSLY
ON
MULTIPLE
PROCESSORS
IN
THE
SYSTEM
A
SOFTIRQ
NEVER
PREEMPTS
ANOTHER
SOFTIRQ
RUNNING
ON
THE
SAME
PROCESSOR
HOWEVER
SO
DISABLING
BOTTOM
HALVES
IS
NOT
NEEDED
READER
WRITER
SPIN
LOCKS
SOMETIMES
LOCK
USAGE
CAN
BE
CLEARLY
DIVIDED
INTO
READER
AND
WRITER
PATHS
FOR
EXAMPLE
CONSIDER
A
LIST
THAT
IS
BOTH
UPDATED
AND
SEARCHED
WHEN
THE
LIST
IS
UPDATED
WRITTEN
TO
IT
IS
IMPORTANT
THAT
NO
OTHER
THREADS
OF
EXECUTION
CONCURRENTLY
WRITE
TO
OR
READ
FROM
THE
LIST
WRITING
DEMANDS
MUTUAL
EXCLUSION
ON
THE
OTHER
HAND
WHEN
THE
LIST
IS
SEARCHED
READ
FROM
IT
IS
ONLY
IMPORTANT
THAT
NOTHING
ELSE
WRITES
TO
THE
LIST
MULTIPLE
CONCURRENT
READERS
ARE
SAFE
SO
LONG
AS
THERE
ARE
NO
WRITERS
THE
TASK
LIST
ACCESS
PATTERNS
DISCUSSED
IN
CHAPTER
PROCESS
MANAGEMENT
FIT
THIS
DESCRIPTION
NOT
SURPRISINGLY
A
READER
WRITER
SPIN
LOCK
PROTECTS
THE
TASK
LIST
WHEN
A
DATA
STRUCTURE
IS
NEATLY
SPLIT
INTO
READER
WRITER
OR
CONSUMER
PRODUCER
USAGE
PATTERNS
IT
MAKES
SENSE
TO
USE
A
LOCKING
MECHANISM
THAT
PROVIDES
SIMILAR
SEMANTICS
TO
SATISFY
THIS
USE
THE
LINUX
KERNEL
PROVIDES
READER
WRITER
SPIN
LOCKS
READER
WRITER
SPIN
LOCKS
PROVIDE
SEPARATE
READER
AND
WRITER
VARIANTS
OF
THE
LOCK
ONE
OR
MORE
READERS
CAN
CONCURRENTLY
HOLD
THE
READER
LOCK
THE
WRITER
LOCK
CONVERSELY
CAN
BE
HELD
BY
AT
MOST
ONE
WRITER
WITH
NO
CONCURRENT
READERS
READER
WRITER
LOCKS
ARE
SOMETIMES
CALLED
SHARED
EXCLUSIVE
OR
CONCURRENT
EXCLUSIVE
LOCKS
BECAUSE
THE
LOCK
IS
AVAILABLE
IN
A
SHARED
FOR
READERS
AND
AN
EXCLUSIVE
FOR
WRITERS
FORM
USAGE
IS
SIMILAR
TO
SPIN
LOCKS
THE
READER
WRITER
SPIN
LOCK
IS
INITIALIZED
VIA
THEN
IN
THE
READER
CODE
PATH
CRITICAL
SECTION
READ
ONLY
FINALLY
IN
THE
WRITER
CODE
PATH
CRITICAL
SECTION
READ
AND
WRITE
NORMALLY
THE
READERS
AND
WRITERS
ARE
IN
ENTIRELY
SEPARATE
CODE
PATHS
SUCH
AS
IN
THIS
EXAMPLE
NOTE
THAT
YOU
CANNOT
UPGRADE
A
READ
LOCK
TO
A
WRITE
LOCK
FOR
EXAMPLE
CONSIDER
THIS
CODE
SNIPPET
EXECUTING
THESE
TWO
FUNCTIONS
AS
SHOWN
WILL
DEADLOCK
AS
THE
WRITE
LOCK
SPINS
WAITING
FOR
ALL
READERS
TO
RELEASE
THE
SHARED
LOCK
INCLUDING
YOURSELF
IF
YOU
EVER
NEED
TO
WRITE
OBTAIN
THE
WRITE
LOCK
FROM
THE
START
IF
THE
LINE
BETWEEN
YOUR
READERS
AND
WRITERS
IS
MUD
DLED
IT
MIGHT
BE
AN
INDICATION
THAT
YOU
DO
NOT
NEED
TO
USE
READER
WRITER
LOCKS
IN
THAT
CASE
A
NORMAL
SPIN
LOCK
IS
OPTIMAL
IT
IS
SAFE
FOR
MULTIPLE
READERS
TO
OBTAIN
THE
SAME
LOCK
IN
FACT
IT
IS
SAFE
FOR
THE
SAME
THREAD
TO
RECURSIVELY
OBTAIN
THE
SAME
READ
LOCK
THIS
LENDS
ITSELF
TO
A
USEFUL
AND
COMMON
OPTIMIZATION
IF
YOU
HAVE
ONLY
READERS
IN
INTERRUPT
HANDLERS
BUT
NO
WRITERS
YOU
CAN
MIX
THE
USE
OF
THE
INTERRUPT
DISABLING
LOCKS
YOU
CAN
USE
INSTEAD
OF
FOR
READER
PROTECTION
YOU
STILL
NEED
TO
DISABLE
INTERRUPTS
FOR
WRITE
ACCESS
À
LA
OTHERWISE
A
READER
IN
AN
INTERRUPT
COULD
DEADLOCK
ON
THE
HELD
WRITE
LOCK
SEE
TABLE
FOR
A
FULL
LISTING
OF
THE
READER
WRITER
SPIN
LOCK
METHODS
TABLE
READER
WRITER
SPIN
LOCK
METHODS
METHOD
DESCRIPTION
ACQUIRES
GIVEN
LOCK
FOR
READING
DISABLES
LOCAL
INTERRUPTS
AND
ACQUIRES
GIVEN
LOCK
FOR
READING
SAVES
THE
CURRENT
STATE
OF
LOCAL
INTERRUPTS
DISABLES
LOCAL
IN
TERRUPTS
AND
ACQUIRES
THE
GIVEN
LOCK
FOR
READING
RELEASES
GIVEN
LOCK
FOR
READING
RELEASES
GIVEN
LOCK
AND
ENABLES
LOCAL
INTERRUPTS
IRQRESTORE
RELEASES
GIVEN
LOCK
AND
RESTORES
LOCAL
INTERRUPTS
TO
THE
GIVEN
PREVIOUS
STATE
ACQUIRES
GIVEN
LOCK
FOR
WRITING
DISABLES
LOCAL
INTERRUPTS
AND
ACQUIRES
THE
GIVEN
LOCK
FOR
WRITING
SAVES
CURRENT
STATE
OF
LOCAL
INTERRUPTS
DISABLES
LOCAL
INTER
RUPTS
AND
ACQUIRES
THE
GIVEN
LOCK
FOR
WRITING
RELEASES
GIVEN
LOCK
RELEASES
GIVEN
LOCK
AND
ENABLES
LOCAL
INTERRUPTS
TABLE
READER
WRITER
SPIN
LOCK
METHODS
METHOD
DESCRIPTION
CONTINUED
RELEASES
GIVEN
LOCK
AND
RESTORES
LOCAL
INTERRUPTS
TO
GIVEN
PREVIOUS
STATE
TRIES
TO
ACQUIRE
GIVEN
LOCK
FOR
WRITING
IF
UNAVAILABLE
RETURNS
NONZERO
INITIALIZES
GIVEN
A
FINAL
IMPORTANT
CONSIDERATION
IN
USING
THE
LINUX
READER
WRITER
SPIN
LOCKS
IS
THAT
THEY
FAVOR
READERS
OVER
WRITERS
IF
THE
READ
LOCK
IS
HELD
AND
A
WRITER
IS
WAITING
FOR
EXCLUSIVE
AC
CESS
READERS
THAT
ATTEMPT
TO
ACQUIRE
THE
LOCK
CONTINUE
TO
SUCCEED
THE
SPINNING
WRITER
DOES
NOT
ACQUIRE
THE
LOCK
UNTIL
ALL
READERS
RELEASE
THE
LOCK
THEREFORE
A
SUFFICIENT
NUMBER
OF
READERS
CAN
STARVE
PENDING
WRITERS
THIS
IS
IMPORTANT
TO
KEEP
IN
MIND
WHEN
DESIGNING
YOUR
LOCKING
SOMETIMES
THIS
BEHAVIOR
IS
BENEFICIAL
SOMETIMES
IT
IS
CATASTROPHIC
SPIN
LOCKS
PROVIDE
A
QUICK
AND
SIMPLE
LOCK
THE
SPINNING
BEHAVIOR
IS
OPTIMAL
FOR
SHORT
HOLD
TIMES
AND
CODE
THAT
CANNOT
SLEEP
INTERRUPT
HANDLERS
FOR
EXAMPLE
IN
CASES
WHERE
THE
SLEEP
TIME
MIGHT
BE
LONG
OR
YOU
POTENTIALLY
NEED
TO
SLEEP
WHILE
HOLDING
THE
LOCK
THE
SEMAPHORE
IS
A
SOLUTION
SEMAPHORES
SEMAPHORES
IN
LINUX
ARE
SLEEPING
LOCKS
WHEN
A
TASK
ATTEMPTS
TO
ACQUIRE
A
SEMAPHORE
THAT
IS
UNAVAILABLE
THE
SEMAPHORE
PLACES
THE
TASK
ONTO
A
WAIT
QUEUE
AND
PUTS
THE
TASK
TO
SLEEP
THE
PROCESSOR
IS
THEN
FREE
TO
EXECUTE
OTHER
CODE
WHEN
THE
SEMAPHORE
BECOMES
AVAILABLE
ONE
OF
THE
TASKS
ON
THE
WAIT
QUEUE
IS
AWAKENED
SO
THAT
IT
CAN
THEN
ACQUIRE
THE
SEMAPHORE
LET
JUMP
BACK
TO
THE
DOOR
AND
KEY
ANALOGY
WHEN
A
PERSON
REACHES
THE
DOOR
HE
CAN
GRAB
THE
KEY
AND
ENTER
THE
ROOM
THE
BIG
DIFFERENCE
LIES
IN
WHAT
HAPPENS
WHEN
ANOTHER
DUDE
REACHES
THE
DOOR
AND
THE
KEY
IS
NOT
AVAILABLE
IN
THIS
CASE
INSTEAD
OF
SPINNING
THE
FEL
LOW
PUTS
HIS
NAME
ON
A
LIST
AND
TAKES
A
NUMBER
WHEN
THE
PERSON
INSIDE
THE
ROOM
LEAVES
HE
CHECKS
THE
LIST
AT
THE
DOOR
IF
ANYONE
NAME
IS
ON
THE
LIST
HE
GOES
OVER
TO
THE
FIRST
NAME
AND
GIVES
HIM
A
PLAYFUL
JAB
IN
THE
CHEST
WAKING
HIM
UP
AND
ALLOWING
HIM
TO
ENTER
THE
ROOM
IN
THIS
MANNER
THE
KEY
READ
SEMAPHORE
CONTINUES
TO
ENSURE
THAT
THERE
IS
ONLY
ONE
PERSON
READ
THREAD
OF
EXECUTION
INSIDE
THE
ROOM
READ
CRITICAL
REGION
AT
ONE
TIME
THIS
PROVIDES
BETTER
PROCESSOR
UTILIZATION
THAN
SPIN
LOCKS
BECAUSE
THERE
IS
NO
TIME
SPENT
BUSY
LOOPING
BUT
SEMAPHORES
HAVE
MUCH
GREATER
OVERHEAD
THAN
SPIN
LOCKS
LIFE
IS
ALWAYS
A
TRADE
OFF
YOU
CAN
DRAW
SOME
INTERESTING
CONCLUSIONS
FROM
THE
SLEEPING
BEHAVIOR
OF
SEMAPHORES
N
BECAUSE
THE
CONTENDING
TASKS
SLEEP
WHILE
WAITING
FOR
THE
LOCK
TO
BECOME
AVAILABLE
SEMAPHORES
ARE
WELL
SUITED
TO
LOCKS
THAT
ARE
HELD
FOR
A
LONG
TIME
N
CONVERSELY
SEMAPHORES
ARE
NOT
OPTIMAL
FOR
LOCKS
THAT
ARE
HELD
FOR
SHORT
PERIODS
BE
CAUSE
THE
OVERHEAD
OF
SLEEPING
MAINTAINING
THE
WAIT
QUEUE
AND
WAKING
BACK
UP
CAN
EASILY
OUTWEIGH
THE
TOTAL
LOCK
HOLD
TIME
N
BECAUSE
A
THREAD
OF
EXECUTION
SLEEPS
ON
LOCK
CONTENTION
SEMAPHORES
MUST
BE
OB
TAINED
ONLY
IN
PROCESS
CONTEXT
BECAUSE
INTERRUPT
CONTEXT
IS
NOT
SCHEDULABLE
N
YOU
CAN
ALTHOUGH
YOU
MIGHT
NOT
WANT
TO
SLEEP
WHILE
HOLDING
A
SEMAPHORE
BE
CAUSE
YOU
WILL
NOT
DEADLOCK
WHEN
ANOTHER
PROCESS
ACQUIRES
THE
SAME
SEMAPHORE
IT
WILL
JUST
GO
TO
SLEEP
AND
EVENTUALLY
LET
YOU
CONTINUE
N
YOU
CANNOT
HOLD
A
SPIN
LOCK
WHILE
YOU
ACQUIRE
A
SEMAPHORE
BECAUSE
YOU
MIGHT
HAVE
TO
SLEEP
WHILE
WAITING
FOR
THE
SEMAPHORE
AND
YOU
CANNOT
SLEEP
WHILE
HOLDING
A
SPIN
LOCK
THESE
FACTS
HIGHLIGHT
THE
USES
OF
SEMAPHORES
VERSUS
SPIN
LOCKS
IN
MOST
USES
OF
SEMA
PHORES
THERE
IS
LITTLE
CHOICE
AS
TO
WHAT
LOCK
TO
USE
IF
YOUR
CODE
NEEDS
TO
SLEEP
WHICH
IS
OFTEN
THE
CASE
WHEN
SYNCHRONIZING
WITH
USER
SPACE
SEMAPHORES
ARE
THE
SOLE
SOLUTION
IT
IS
OFTEN
EASIER
IF
NOT
NECESSARY
TO
USE
SEMAPHORES
BECAUSE
THEY
ALLOW
YOU
THE
FLEXIBILITY
OF
SLEEPING
WHEN
YOU
DO
HAVE
A
CHOICE
THE
DECISION
BETWEEN
SEMAPHORE
AND
SPIN
LOCK
SHOULD
BE
BASED
ON
LOCK
HOLD
TIME
IDEALLY
ALL
YOUR
LOCKS
SHOULD
BE
HELD
AS
BRIEFLY
AS
POS
SIBLE
WITH
SEMAPHORES
HOWEVER
LONGER
LOCK
HOLD
TIMES
ARE
MORE
ACCEPTABLE
ADDITIONALLY
UNLIKE
SPIN
LOCKS
SEMAPHORES
DO
NOT
DISABLE
KERNEL
PREEMPTION
AND
CONSEQUENTLY
CODE
HOLDING
A
SEMAPHORE
CAN
BE
PREEMPTED
THIS
MEANS
SEMAPHORES
DO
NOT
ADVERSELY
AFFECT
SCHEDULING
LATENCY
COUNTING
AND
BINARY
SEMAPHORES
A
FINAL
USEFUL
FEATURE
OF
SEMAPHORES
IS
THAT
THEY
CAN
ALLOW
FOR
AN
ARBITRARY
NUMBER
OF
SI
MULTANEOUS
LOCK
HOLDERS
WHEREAS
SPIN
LOCKS
PERMIT
AT
MOST
ONE
TASK
TO
HOLD
THE
LOCK
AT
A
TIME
THE
NUMBER
OF
PERMISSIBLE
SIMULTANEOUS
HOLDERS
OF
SEMAPHORES
CAN
BE
SET
AT
DECLARA
TION
TIME
THIS
VALUE
IS
CALLED
THE
USAGE
COUNT
OR
SIMPLY
THE
COUNT
THE
MOST
COMMON
VALUE
IS
TO
ALLOW
LIKE
SPIN
LOCKS
ONLY
ONE
LOCK
HOLDER
AT
A
TIME
IN
THIS
CASE
THE
COUNT
IS
EQUAL
TO
ONE
AND
THE
SEMAPHORE
IS
CALLED
EITHER
A
BINARY
SEMAPHORE
BECAUSE
IT
IS
EITHER
HELD
BY
ONE
TASK
OR
NOT
HELD
AT
ALL
OR
A
MUTEX
BECAUSE
IT
ENFORCES
MUTUAL
EXCLUSION
ALTERNA
TIVELY
THE
COUNT
CAN
BE
INITIALIZED
TO
A
NONZERO
VALUE
GREATER
THAN
ONE
IN
THIS
CASE
THE
SEMAPHORE
IS
CALLED
A
COUNTING
SEMAPHORE
AND
IT
ENABLES
AT
MOST
COUNT
HOLDERS
OF
THE
LOCK
AT
A
TIME
COUNTING
SEMAPHORES
ARE
NOT
USED
TO
ENFORCE
MUTUAL
EXCLUSION
BECAUSE
THEY
EN
ABLE
MULTIPLE
THREADS
OF
EXECUTION
IN
THE
CRITICAL
REGION
AT
ONCE
INSTEAD
THEY
ARE
USED
TO
ENFORCE
LIMITS
IN
CERTAIN
CODE
THEY
ARE
NOT
USED
MUCH
IN
THE
KERNEL
IF
YOU
USE
A
SEMA
PHORE
YOU
ALMOST
ASSUREDLY
WANT
TO
USE
A
MUTEX
A
SEMAPHORE
WITH
A
COUNT
OF
ONE
SEMAPHORES
WERE
FORMALIZED
BY
EDSGER
WYBE
IN
AS
A
GENERALIZED
LOCK
ING
MECHANISM
A
SEMAPHORE
SUPPORTS
TWO
ATOMIC
OPERATIONS
P
AND
V
NAMED
AFTER
THE
DUTCH
WORD
PROBEREN
TO
TEST
LITERALLY
TO
PROBE
AND
THE
DUTCH
WORD
VERHOGEN
TO
IN
CREMENT
LATER
SYSTEMS
CALLED
THESE
METHODS
DOWN
AND
UP
RESPECTIVELY
AND
SO
DOES
LINUX
THE
DOWN
METHOD
IS
USED
TO
ACQUIRE
A
SEMAPHORE
BY
DECREMENTING
THE
COUNT
BY
ONE
IF
THE
NEW
COUNT
IS
ZERO
OR
GREATER
THE
LOCK
IS
ACQUIRED
AND
THE
TASK
CAN
ENTER
THE
CRITICAL
REGION
IF
THE
COUNT
IS
NEGATIVE
THE
TASK
IS
PLACED
ON
A
WAIT
QUEUE
AND
THE
PROCES
SOR
MOVES
ON
TO
SOMETHING
ELSE
THESE
NAMES
ARE
USED
AS
VERBS
YOU
DOWN
A
SEMAPHORE
TO
ACQUIRE
IT
THE
UP
METHOD
IS
USED
TO
RELEASE
A
SEMAPHORE
UPON
COMPLETION
OF
A
CRITICAL
REGION
THIS
IS
CALLED
UPPING
THE
SEMAPHORE
THE
METHOD
INCREMENTS
THE
COUNT
VALUE
IF
THE
SEMAPHORE
WAIT
QUEUE
IS
NOT
EMPTY
ONE
OF
THE
WAITING
TASKS
IS
AWAKENED
AND
ALLOWED
TO
ACQUIRE
THE
SEMAPHORE
MEMORY
MANAGEMENT
MAIN
MEMORY
RAM
IS
AN
IMPORTANT
RESOURCE
THAT
MUST
BE
CAREFULLY
MAN
AGED
WHILE
THE
AVERAGE
HOME
COMPUTER
NOWADAYS
HAS
TIMES
MORE
MEMO
RY
AS
THE
IBM
THE
LARGEST
COMPUTER
IN
THE
WORLD
IN
THE
EARLY
PRO
GRAMS
ARE
GETTING
BIGGER
FASTER
THAN
MEMORIES
TO
PARAPHRASE
PARKINSON
LAW
PROGRAMS
EXPAND
TO
FILL
THE
MEMORY
AVAILABLE
TO
HOLD
THEM
IN
THIS
CHAPTER
WE
WILL
STUDY
HOW
OPERATING
SYSTEMS
CREATE
ABSTRACTIONS
FROM
MEMORY
AND
HOW
THEY
MANAGE
THEM
WHAT
EVERY
PROGRAMMER
WOULD
LIKE
IS
A
PRIVATE
INFINITELY
LARGE
INFINITELY
FAST
MEMORY
THAT
IS
ALSO
NONVOLATILE
THAT
IS
DOES
NOT
LOSE
ITS
CONTENTS
WHEN
THE
ELEC
TRIC
POWER
IS
SWITCHED
OFF
WHILE
WE
ARE
AT
IT
WHY
NOT
MAKE
IT
INEXPENSIVE
TOO
UNFORTUNATELY
TECHNOLOGY
DOES
NOT
PROVIDE
SUCH
MEMORIES
AT
PRESENT
MAYBE
YOU
WILL
DISCOVER
HOW
TO
DO
IT
WHAT
IS
THE
SECOND
CHOICE
OVER
THE
YEARS
PEOPLE
DISCOVERED
THE
CONCEPT
OF
A
MEMORY
HIERARCHY
IN
WHICH
COMPUTERS
HAVE
A
FEW
MEGABYTES
OF
VERY
FAST
EX
PENSIVE
VOLATILE
CACHE
MEMORY
A
FEW
GIGABYTES
OF
MEDIUM
SPEED
MEDIUM
PRICED
VOLATILE
MAIN
MEMORY
AND
A
FEW
TERABYTES
OF
SLOW
CHEAP
NONVOLATILE
DISK
STORAGE
NOT
TO
MENTION
REMOVABLE
STORAGE
SUCH
AS
DVDS
AND
USB
STICKS
IT
IS
THE
JOB
OF
THE
OPERATING
SYSTEM
TO
ABSTRACT
THIS
HIERARCHY
INTO
A
USEFUL
MODEL
AND
THEN
MANAGE
THE
ABSTRACTION
THE
PART
OF
THE
OPERATING
SYSTEM
THAT
MANAGES
PART
OF
THE
MEMORY
HIERARCHY
IS
CALLED
THE
MEMORY
MANAGER
ITS
JOB
IS
TO
EFFICIENTLY
MANAGE
MEMORY
KEEP
TRACK
OF
WHICH
PARTS
OF
MEMORY
ARE
IN
USE
ALLOCATE
MEMORY
TO
PROCESSES
WHEN
THEY
NEED
IT
AND
DEALLOCATE
IT
WHEN
THEY
ARE
DONE
MEMORY
MANAGEMENT
CHAP
IN
THIS
CHAPTER
WE
WILL
INVESTIGATE
SEVERAL
DIFFERENT
MEMORY
MANAGEMENT
SCHEMES
RANGING
FROM
VERY
SIMPLE
TO
HIGHLY
SOPHISTICATED
SINCE
MANAGING
THE
LOWEST
LEVEL
OF
CACHE
MEMORY
IS
NORMALLY
DONE
BY
THE
HARDWARE
THE
FOCUS
OF
THIS
CHAPTER
WILL
BE
ON
THE
PROGRAMMER
MODEL
OF
MAIN
MEMORY
AND
HOW
IT
CAN
BE
MANAGED
WELL
THE
ABSTRACTIONS
FOR
AND
THE
MANAGEMENT
OF
PERMANENT
STOR
AGE
THE
DISK
ARE
THE
SUBJECT
OF
THE
NEXT
CHAPTER
WE
WILL
START
AT
THE
BEGINNING
AND
LOOK
FIRST
AT
THE
SIMPLEST
POSSIBLE
SCHEMES
AND
THEN
GRADUALLY
PROGRESS
TO
MORE
AND
MORE
ELABORATE
ONES
SEC
NO
MEMORY
ABSTRACTION
OXFFF
NO
MEMORY
ABSTRACTION
THE
SIMPLEST
MEMORY
ABSTRACTION
IS
NO
ABSTRACTION
AT
ALL
EARLY
MAINFRAME
COMPUTERS
BEFORE
EARLY
MINICOMPUTERS
BEFORE
AND
EARLY
PERSONAL
COMPUTERS
BEFORE
HAD
NO
MEMORY
ABSTRACTION
EVERY
PROGRAM
SIMPLY
SAW
THE
PHYSICAL
MEMORY
WHEN
A
PROGRAM
EXECUTED
AN
INSTRUCTION
LIKE
MOV
REGISTERS
THE
COMPUTER
JUST
MOVED
THE
CONTENTS
OF
PHYSICAL
MEMORY
LOCATION
TO
REGISTER
THUS
THE
MODEL
OF
MEMORY
PRESENTED
TO
THE
PROGRAMMER
WAS
SIM
PLY
PHYSICAL
MEMORY
A
SET
OF
ADDRESSES
FROM
TO
SOME
MAXIMUM
EACH
ADDRESS
CORRESPONDING
TO
A
CELL
CONTAINING
SOME
NUMBER
OF
BITS
COMMONLY
EIGHT
UNDER
THESE
CONDITIONS
IT
WAS
NOT
POSSIBLE
TO
HAVE
TWO
RUNNING
PROGRAMS
IN
MEMORY
AT
THE
SAME
TIME
IF
THE
FIRST
PROGRAM
WROTE
A
NEW
VALUE
TO
SAY
LOCATION
THIS
WOULD
ERASE
WHATEVER
VALUE
THE
SECOND
PROGRAM
WAS
STORING
THERE
NOUIING
WOULD
WORK
AND
BOTH
PROGRAMS
WOULD
CRASH
ALMOST
IMMEDIATELY
EVEN
WITH
THE
MODEL
OF
MEMORY
BEING
JUST
PHYSICAL
MEMORY
SEVERAL
OPTIONS
ARE
POSSIBLE
THREE
VARIATIONS
ARE
SHOWN
IN
FIG
THE
OPERATING
SYSTEM
MAY
BE
AT
THE
BOTTOM
OF
MEMORY
IN
RAM
RANDOM
ACCESS
MEMORY
AS
SHOWN
IN
FIG
A
OR
IT
MAY
BE
IN
ROM
READ
ONLY
MEMORY
AT
THE
TOP
OF
MEMORY
AS
SHOWN
IN
FIG
B
OR
THE
DEVICE
DRIVERS
MAY
BE
AT
THE
TOP
OF
MEMORY
IN
A
ROM
AND
THE
REST
OF
THE
SYSTEM
IN
RAM
DOWN
BELOW
AS
SHOWN
IN
FIG
C
THE
FIRST
MODEL
WAS
FORMERLY
USED
ON
MAINFRAMES
AND
MINICOMPUTERS
BUT
IS
RARELY
USED
ANY
MORE
THE
SECOND
MODEL
IS
USED
ON
SOME
HANDHELD
COMPUTERS
AND
EMBEDDED
SYSTEMS
THE
THIRD
MODEL
WAS
USED
BY
EARLY
PERSONAL
COMPUTERS
E
G
RUNNING
MS
DOS
WHERE
THE
PORTION
OF
THE
SYSTEM
IN
THE
ROM
IS
CALLED
THE
BIOS
BASIC
INPUT
OUTPUT
SYSTEM
MODELS
A
AND
C
HAVE
THE
DISADVANTAGE
THAT
A
BUG
IN
THE
USER
PROGRAM
CAN
WIPE
OUT
THE
OPERATING
SYSTEM
POSSIBLY
WITH
DISASTROUS
RESULTS
SUCH
AS
GARBLING
THE
DISK
WHEN
THE
SYSTEM
IS
ORGANIZED
IN
THIS
WAY
GENERALLY
ONLY
ONE
PROCESS
AT
A
TIME
CAN
BE
RUNNING
AS
SOON
AS
THE
USER
TYPES
A
COMMAND
THE
OPERATING
SYSTEM
COPIES
THE
REQUESTED
PROGRAM
FROM
DISK
TO
MEMORY
AND
EXECUTES
IT
WHEN
THE
PROCESS
FINISHES
THE
OPERATING
SYSTEM
DISPLAYS
A
PROMPT
CHARACTER
AND
WAITS
FOR
A
A
B
C
FIGURE
THREE
SIMPLE
WAYS
OF
ORGANIZING
MEMORY
WITH
AN
OPERATING
SYSTEM
AND
ONE
USER
PROCESS
OTHER
POSSIBILITIES
ALSO
EXIST
NEW
COMMAND
WHEN
IT
RECEIVES
THE
COMMAND
IT
LOADS
A
NEW
PROGRAM
INTO
MEM
ORY
OVERWRITING
THE
FIRST
ONE
ONE
WAY
TO
GET
SOME
PARALLELISM
IN
A
SYSTEM
WITH
NO
MEMORY
ABSTRACTION
IS
TO
PROGRAM
WITH
MULTIPLE
THREADS
SINCE
ALL
THREADS
IN
A
PROCESS
ARE
SUPPOSED
TO
SEE
THE
SAME
MEMORY
IMAGE
THE
FACT
THAT
THEY
ARE
FORCED
TO
IS
NOT
A
PROBLEM
WHILE
THIS
IDEA
WORKS
IT
IS
OF
LIMITED
USE
SINCE
WHAT
PEOPLE
OFTEN
WANT
IS
UNRELATED
PRO
GRAMS
TO
BE
RUNNING
AT
THE
SAME
TIME
SOMETHING
THE
THREADS
ABSTRACTION
DOES
NOT
PROVIDE
FURTHERMORE
ANY
SYSTEM
THAT
IS
SO
PRIMITIVE
AS
TO
PROVIDE
NO
MEMORY
ABSTRACTION
IS
UNLIKELY
TO
PROVIDE
A
THREADS
ABSTRACTION
RUNNING
MULTIPLE
PROGRAMS
WITHOUT
A
MEMORY
ABSTRACTION
HOWEVER
EVEN
WITH
NO
MEMORY
ABSTRACTION
IT
IS
POSSIBLE
TO
RUN
MULTIPLE
PRO
GRAMS
AT
THE
SAME
TIME
WHAT
THE
OPERATING
SYSTEM
HAS
TO
DO
IS
SAVE
THE
ENTIRE
CONTENTS
OF
MEMORY
TO
A
DISK
FILE
THEN
BRING
IN
AND
RUN
THE
NEXT
PROGRAM
AS
LONG
AS
THERE
IS
ONLY
ONE
PROGRAM
AT
A
TIME
IN
MEMORY
THERE
ARE
NO
CONFLICTS
THIS
CONCEPT
SWAPPING
WILL
BE
DISCUSSED
BELOW
WITH
THE
ADDITION
OF
SOME
SPECIAL
HARDWARE
IT
IS
POSSIBLE
TO
RUN
MULTIPLE
PRO
GRAMS
CONCURRENTLY
EVEN
WITHOUT
SWAPPING
THE
EARLY
MODELS
OF
THE
IBM
SOLVED
THE
PROBLEM
AS
FOLLOWS
MEMORY
WAS
DIVIDED
INTO
KB
BLOCKS
AND
EACH
ONE
WAS
ASSIGNED
A
BIT
PROTECTION
KEY
HELD
IN
SPECIAL
REGISTERS
INSIDE
THE
CPU
A
MACHINE
WITH
A
I
MB
MEMORY
NEEDED
ONLY
OF
THESE
BIT
REGISTERS
FOR
A
TOTAL
OF
BYTES
OF
KEY
STORAGE
THE
PSW
PROGRAM
STATUS
WORD
ALSO
CONTAINED
A
BIT
KEY
THE
HARDWARE
TRAPPED
ANY
ATTEMPT
BY
A
RUNNING
PROCESS
TO
ACCESS
MEMORY
WITH
A
PROTECTION
CODE
DIFFERENT
FROM
THE
PSW
KEY
SINCE
ONLY
THE
OPER
ATING
SYSTEM
COULD
CHANGE
THE
PROTECTION
KEYS
USER
PROCESSES
WERE
PREVENTED
FROM
INTERFERING
WITH
ONE
ANOTHER
AND
WITH
THE
OPERATING
SYSTEM
ITSELF
MEMORY
MANAGEMENT
CHAP
NEVERTHELESS
THIS
SOLUTION
HAD
A
MAJOR
DRAWBACK
DEPICTED
IN
FIG
HERE
WE
HAVE
TWO
PROGRAMS
EACH
KB
IN
SIZE
AS
SHOWN
IN
FIG
A
AND
B
THE
FORMER
IS
SHADED
TO
INDICATE
THAT
IT
HAS
A
DIFFERENT
MEMORY
KEY
THAN
THE
LATTER
THE
FIRST
PROGRAM
STARTS
OUT
BY
JUMPING
TO
ADDRESS
WHICH
CONTAINS
A
MOV
INSTRUC
TION
THE
SECOND
PROGRAM
STARTS
OUT
BY
JUMPING
TO
ADDRESS
WHICH
CONTAINS
A
CMP
INSTRUCTION
THE
INSTRUCTIONS
THAT
ARE
NOT
RELEVANT
TO
THIS
DISCUSSION
ARE
NOT
SHOWN
WHEN
THE
TWO
PROGRAMS
ARE
LOADED
CONSECUTIVELY
IN
MEMORY
STARTING
AT
ADDRESS
WE
HAVE
THE
SITUATION
OF
FIG
C
FOR
THIS
EXAMPLE
WE
ASSUME
THE
OPERATING
SYSTEM
IS
IN
HIGH
MEMORY
AND
THUS
NOT
SHOWN
K
O
J
O
R
J
A
O
B
C
FIGURE
ILLUSTRATION
OF
THE
RELOCATION
PROBLEM
A
A
KB
PROGRAM
B
ANOTHER
KB
PROGRAM
C
THE
TWO
PROGRAMS
LOADED
CONSECUTIVELY
INTO
MEM
ORY
AFTER
THE
PROGRAMS
ARE
LOADED
THEY
CAN
BE
RUN
SINCE
THEY
HAVE
DIFFERENT
MEMORY
KEYS
NEITHER
ONE
CAN
DAMAGE
THE
OTHER
BUT
THE
PROBLEM
IS
OF
A
DIFFERENT
NATURE
WHEN
THE
FIRST
PROGRAM
STARTS
IT
EXECUTES
THE
JMP
INSTRUCTION
WHICH
JUMPS
TO
THE
INSTRUCTION
AS
EXPECTED
THIS
PROGRAM
FUNCTIONS
NORMALLY
HOWEVER
AFTER
THE
FIRST
PROGRAM
HAS
RUN
LONG
ENOUGH
THE
OPERATING
SYSTEM
MAY
DECIDE
TO
RUN
THE
SECOND
PROGRAM
WHICH
HAS
BEEN
LOADED
ABOVE
THE
FIRST
ONE
AT
ADDRESS
THE
FIRST
INSTRUCTION
EXECUTED
IS
JMP
WHICH
JUMPS
TO
THE
ADD
INSTRUCTION
IN
THE
FIRST
PROGRAM
INSTEAD
OF
THE
CMP
INSTRUCTION
IT
IS
SUPPOSED
TO
JUMP
TO
THE
PROGRAM
WILL
MOST
LIKELY
CRASH
IN
WELL
UNDER
SEC
SEC
NO
MEMORY
ABSTRACTION
THE
CORE
PROBLEM
HERE
IS
THAT
THE
TWO
PROGRAMS
BOTH
REFERENCE
ABSOLUTE
PHYSI
CAL
MEMORY
THAT
IS
NOT
WHAT
WE
WANT
AT
ALL
WE
WANT
EACH
PROGRAM
TO
REFERENCE
A
PRIVATE
SET
OF
ADDRESSES
LOCAL
TO
IT
WE
WILL
SHOW
HOW
THIS
IS
ACHIEVED
SHORTLY
WHAT
THE
IBM
DID
AS
A
STOP
GAP
SOLUTION
WAS
MODIFY
THE
SECOND
PROGRAM
ON
THE
FLY
AS
IT
LOADED
IT
INTO
MEMORY
USING
A
TECHNIQUE
KNOWN
AS
STATIC
RELOCATION
IT
WORKED
LIKE
THIS
WHEN
A
PROGRAM
WAS
LOADED
AT
ADDRESS
THE
CONSTANT
WAS
ADDED
TO
EVERY
PROGRAM
ADDRESS
DURING
THE
LOAD
PROCESS
WHILE
THIS
MECHANISM
WORKS
IF
DONE
RIGHT
IT
IS
NOT
A
VERY
GENERAL
SOLUTION
AND
SLOWS
DOWN
LOADING
FURTHERMORE
IT
REQUIRES
EXTRA
INFORMATION
IN
ALL
EXECUTABLE
PROGRAMS
TO
INDICATE
WHICH
WORDS
CONTAIN
RELOCATABLE
ADDRESSES
AND
WHICH
DO
NOT
AFTER
ALL
THE
IN
FIG
B
HAS
TO
BE
RELOCATED
BUT
AN
INSTRUCTION
LIKE
WHICH
MOVES
THE
NUMBER
TO
REGISTERL
MUST
NOT
BE
RELOCATED
THE
LOADER
NEEDS
SOME
WAY
TO
TELL
WHAT
IS
AN
ADDRESS
AND
WHAT
IS
A
CONSTANT
FINALLY
AS
WE
POINTED
OUT
IN
CHAP
HISTORY
TENDS
TO
REPEAT
ITSELF
IN
THE
COM
PUTER
WORLD
WHILE
DIRECT
ADDRESSING
OF
PHYSICAL
MEMORY
IS
BUT
A
DISTANT
MEMORY
SORRY
ON
MAINFRAMES
MINICOMPUTERS
DESKTOP
COMPUTERS
AND
NOTEBOOKS
THE
LACK
OF
A
MEMORY
ABSTRACTION
IS
STILL
COMMON
IN
EMBEDDED
AND
SMART
CARD
SYS
TEMS
DEVICES
SUCH
AS
RADIOS
WASHING
MACHINES
AND
MICROWAVE
OVENS
ARE
ALL
FULL
OF
SOFTWARE
IN
ROM
THESE
DAYS
AND
IN
MOST
CASES
THE
SOFTWARE
ADDRESSES
ABSO
LUTE
MEMORY
THIS
WORKS
BECAUSE
ALL
THE
PROGRAMS
ARE
KNOWN
IN
ADVANCE
AND
USERS
ARE
NOT
FREE
TO
RUN
THEIR
OWN
SOFTWARE
ON
THEIR
TOASTER
WHILE
HIGH
END
EMBEDDED
SYSTEMS
SUCH
AS
CELL
PHONES
HAVE
ELABORATE
OPER
ATING
SYSTEMS
SIMPLER
ONES
DO
NOT
IN
SOME
CASES
THERE
IS
AN
OPERATING
SYSTEM
BUT
IT
IS
JUST
A
LIBRARY
THAT
IS
LINKED
WITH
THE
APPLICATION
PROGRAM
AND
PROVIDES
SYS
TEM
CALLS
FOR
PERFORMING
I
O
AND
OTHER
COMMON
TASKS
THE
POPULAR
E
COS
OPERAT
ING
SYSTEM
IS
A
COMMON
EXAMPLE
OF
AN
OPERATING
SYSTEM
AS
LIBRARY
A
MEMORY
ABSTRACTION
ADDRESS
SPACES
AH
IN
ALL
EXPOSING
PHYSICAL
MEMORY
TO
PROCESSES
HAS
SEVERAL
MAJOR
DRAW
BACKS
FIRST
IF
USER
PROGRAMS
CAN
ADDRESS
EVERY
BYTE
OF
MEMORY
THEY
CAN
EASILY
TRASH
THE
OPERATING
SYSTEM
INTENTIONALLY
OR
BY
ACCIDENT
BRINGING
THE
SYSTEM
TO
A
GRINDING
HALT
UNLESS
THERE
IS
SPECIAL
HARDWARE
LIKE
THE
IBM
LOCK
AND
KEY
SCHEME
THIS
PROBLEM
EXISTS
EVEN
IF
ONLY
ONE
USER
PROGRAM
APPLICATION
IS
RUN
NING
SECOND
WITH
THIS
MODEL
IT
IS
DIFFICULT
TO
HAVE
MULTIPLE
PROGRAMS
RUNNING
AT
ONCE
TAKING
TURNS
IF
THERE
IS
ONLY
ONE
CPU
ON
PERSONAL
COMPUTERS
IT
IS
COM
MON
TO
HAVE
SEVERAL
PROGRAMS
OPEN
AT
ONCE
A
WORD
PROCESSOR
AN
E
MAIL
PROGRAM
AND
A
WEB
BROWSER
WITH
ONE
OF
THEM
HAVING
THE
CURRENT
FOCUS
BUT
THE
OTHERS
BEING
REACTIVATED
AT
THE
CLICK
OF
A
MOUSE
SINCE
THIS
SITUATION
IS
DIFFICULT
TO
ACHIEVE
WHEN
THERE
IS
NO
ABSTRACTION
FROM
PHYSICAL
MEMORY
SOMETHING
HAD
TO
BE
DONE
MEMORY
MANAGEMENT
CHAP
THE
NOTION
OF
AN
ADDRESS
SPACE
TWO
PROBLEMS
HAVE
TO
BE
SOLVED
TO
ALLOW
MULTIPLE
APPLICATIONS
TO
BE
IN
MEM
ORY
AT
THE
SAME
TIME
WITHOUT
THEIR
INTERFERING
WITH
EACH
OTHER
PROTECTION
AND
RELO
CATION
WE
LOOKED
AT
A
PRIMITIVE
SOLUTION
TO
THE
FORMER
USED
ON
THE
IBM
LABEL
CHUNKS
OF
MEMORY
WITH
A
PROTECTION
KEY
AND
COMPARE
THE
KEY
OF
THE
EXECUT
ING
PROCESS
TO
THAT
OF
EVERY
MEMORY
WORD
FETCHED
HOWEVER
THIS
APPROACH
BY
IT
SELF
DOES
NOT
SOLVE
THE
LATTER
PROBLEM
ALTHOUGH
IT
CAN
BE
SOLVED
BY
RELOCATING
PRO
GRAMS
AS
THEY
ARE
LOADED
BUT
THIS
IS
A
SLOW
AND
COMPLICATED
SOLUTION
A
BETTER
SOLUTION
IS
TO
INVENT
A
NEW
ABSTRACTION
FOR
MEMORY
THE
ADDRESS
SPACE
LUST
AS
THE
PROCESS
CONCEPT
CREATES
A
KIND
OF
ABSTRACT
CPU
TO
RUN
PROGRAMS
THE
AD
DRESS
SPACE
CREATES
A
KIND
OF
ABSTRACT
MEMORY
FOR
PROGRAMS
TO
LIVE
IN
AN
AD
DRESS
SPACE
IS
THE
SET
OF
ADDRESSES
THAT
A
PROCESS
CAN
USE
TO
ADDRESS
MEMORY
EACH
PROCESS
HAS
ITS
OWN
ADDRESS
SPACE
INDEPENDENT
OF
THOSE
BELONGING
TO
OTHER
PROC
ESSES
EXCEPT
IN
SOME
SPECIAL
CIRCUMSTANCES
WHERE
PROCESSES
WANT
TO
SHARE
THEIR
ADDRESS
SPACES
THE
CONCEPT
OF
AN
ADDRESS
SPACE
IS
VERY
GENERAL
AND
OCCURS
IN
MANY
CONTEXTS
CONSIDER
TELEPHONE
NUMBERS
IN
THE
U
AND
MANY
OTHER
COUNTRIES
A
LOCAL
TELE
PHONE
NUMBER
IS
USUALLY
A
DIGIT
NUMBER
THE
ADDRESS
SPACE
FOR
TELEPHONE
NUM
BERS
THUS
RUNS
FROM
TO
ALTHOUGH
SOME
NUMBERS
SUCH
AS
THOSE
BEGINNING
WITH
NOT
USED
WITH
THE
GROWTH
OF
CELL
PHONES
MODEMS
AND
FAX
MACHINES
THIS
SPACE
IS
BECOMING
TOO
SMALL
IN
WHICH
CASE
MORE
DIGITS
HAVE
TO
BE
USED
THE
ADDRESS
SPACE
FOR
I
O
PORTS
ON
THE
PENTIUM
RUNS
FROM
TO
ADDRESSES
ARE
BIT
NUMBERS
SO
THEIR
ADDRESS
SPACE
RUNS
FROM
TO
AGAIN
WITH
SOME
RESERVED
NUMBERS
ADDRESS
SPACES
DO
NOT
HAVE
TO
BE
NUMERIC
THE
SET
OF
COM
INTERNET
DOMAINS
IS
ALSO
AN
ADDRESS
SPACE
THIS
ADDRESS
SPACE
CONSISTS
OF
ALL
THE
STRINGS
OF
LENGTH
TO
CHARACTERS
THAT
CAN
BE
MADE
USING
LETTERS
NUMBERS
AND
HYPHENS
FOLLOWED
BY
COM
BY
NOW
YOU
SHOULD
GET
THE
IDEA
IT
IS
FAIRLY
SIMPLE
SOMEWHAT
HARDER
IS
HOW
TO
GIVE
EACH
PROGRAM
ITS
OWN
ADDRESS
SPACE
SO
AD
DRESS
IN
ONE
PROGRAM
MEANS
A
DIFFERENT
PHYSICAL
LOCATION
THAN
ADDRESS
IN
AN
OTHER
PROGRAM
BELOW
WE
WILL
DISCUSS
A
SIMPLE
WAY
THAT
USED
TO
BE
COMMON
BUT
HAS
FALLEN
INTO
DISUSE
DUE
TO
THE
ABILITY
TO
PUT
MUCH
MORE
COMPLICATED
AND
BETTER
SCHEMES
ON
MODERN
CPU
CHIPS
BASE
AND
LIMIT
REGISTERS
THIS
SIMPLE
SOLUTION
USES
A
PARTICULARLY
SIMPLE
VERSION
OF
DYNAMIC
RELOCA
TION
WHAT
IT
DOES
IS
MAP
EACH
PROCESS
ADDRESS
SPACE
ONTO
A
DIFFERENT
PART
OF
PHYSICAL
MEMORY
IN
A
SIMPLE
WAY
THE
CLASSICAL
SOLUTION
WHICH
WAS
USED
ON
MA
CHINES
RANGING
FROM
THE
CDC
THE
WORLD
FIRST
SUPERCOMPUTER
TO
THE
INTEL
THE
HEART
OF
THE
ORIGINAL
IBM
PC
IS
TO
EQUIP
EACH
CPU
WITH
TWO
SPECIAL
HARDWARE
REGISTERS
USUALLY
CALLED
THE
BASE
AND
LIMIT
REGISTERS
WHEN
BASE
AND
SEC
A
MEMORY
ABSTRACTION
ADDRESS
SPACES
LIMIT
REGISTERS
ARE
USED
PROGRAMS
ARE
LOADED
INTO
CONSECUTIVE
MEMORY
LOCATIONS
WHEREVER
THERE
IS
ROOM
AND
WITHOUT
RELOCATION
DURING
LOADING
AS
SHOWN
IN
FIG
C
WHEN
A
PROCESS
IS
RUN
THE
BASE
REGISTER
IS
LOADED
WITH
THE
PHYSICAL
ADDRESS
WHERE
ITS
PROGRAM
BEGINS
IN
MEMORY
AND
THE
LIMIT
REGISTER
IS
LOADED
WITH
THE
LENGTH
OF
THE
PROGRAM
IN
FIG
C
THE
BASE
AND
LIMIT
VALUES
THAT
WOULD
BE
LOADED
INTO
THESE
HARDWARE
REGISTERS
WHEN
THE
FIRST
PROGRAM
IS
RUN
ARE
AND
RESPECTIVELY
THE
VALUES
USED
WHEN
THE
SECOND
PROGRAM
IS
RUN
ARE
AND
RESPECTIVELY
IF
A
THIRD
KB
PROGRAM
WERE
LOADED
DIRECTLY
ABOVE
THE
SECOND
ONE
AND
RUN
THE
BASE
AND
LIMIT
REGISTERS
WOULD
BE
AND
EVERY
TIME
A
PROCESS
REFERENCES
MEMORY
EITHER
TO
FETCH
AN
INSTRUCTION
OR
READ
OR
WRITE
A
DATA
WORD
THE
CPU
HARDWARE
AUTOMATICALLY
ADDS
THE
BASE
VALUE
TO
THE
ADDRESS
GENERATED
BY
THE
PROCESS
BEFORE
SENDING
THE
ADDRESS
OUT
ON
THE
MEMORY
BUS
SIMULTANEOUSLY
IT
CHECKS
IF
THE
ADDRESS
OFFERED
IS
EQUAL
TO
OR
GREATER
THAN
THE
VALUE
IN
THE
LIMIT
REGISTER
IN
WHICH
CASE
A
FAULT
IS
GENERATED
AND
THE
ACCESS
IS
ABORTED
THUS
IN
THE
CASE
OF
THE
FIRST
INSTRUCTION
OF
THE
SECOND
PROGRAM
IN
FIG
C
THE
PROCESS
EXECUTES
A
INSTRUCTION
BUT
THE
HARDWARE
TREATS
IT
AS
THOUGH
IT
WERE
JMP
SO
IT
LANDS
ON
THE
CMP
INSTRUCTION
AS
EXPECTED
THE
SETTINGS
OF
THE
BASE
AND
LIMIT
REGISTERS
DURING
THE
EXECUTION
OF
THE
SECOND
PROGRAM
OF
FIG
C
ARE
SHOWN
IN
FIG
USING
BASE
AND
LIMIT
REGISTERS
IS
AN
EASY
WAY
TO
GIVE
EACH
PROCESS
ITS
OWN
PRI
VATE
ADDRESS
SPACE
BECAUSE
EVERY
MEMORY
ADDRESS
GENERATED
AUTOMATICALLY
HAS
THE
BASE
REGISTER
CONTENTS
ADDED
TO
IT
BEFORE
BEING
SENT
TO
MEMORY
IN
MANY
IMPLE
MENTATIONS
THE
BASE
AND
LIMIT
REGISTERS
ARE
PROTECTED
IN
SUCH
A
WAY
THAT
ONLY
THE
OPERATING
SYSTEM
CAN
MODIFY
THEM
THIS
WAS
THE
CASE
ON
THE
CDC
BUT
NOT
ON
THE
INTEL
WHICH
DID
NOT
EVEN
HAVE
THE
LIMIT
REGISTER
IT
DID
HOWEVER
HAVE
MULTIPLE
BASE
REGISTERS
ALLOWING
PROGRAM
TEXT
AND
DATA
FOR
EXAMPLE
TO
BE
INDEPENDENTLY
RELOCATED
BUT
OFFERED
NO
PROTECTION
FROM
OUT
OF
RANGE
MEMORY
REF
ERENCES
A
DISADVANTAGE
OF
RELOCATION
USING
BASE
AND
LIMIT
REGISTERS
IS
THE
NEED
TO
PER
FORM
AN
ADDITION
AND
A
COMPARISON
ON
EVERY
MEMORY
REFERENCE
COMPARISONS
CAN
BE
DONE
FAST
BUT
ADDITIONS
ARE
SLOW
DUE
TO
CARRY
PROPAGATION
TIME
UNLESS
SPECIAL
ADDITION
CIRCUITS
ARE
USED
SWAPPING
IF
THE
PHYSICAL
MEMORY
OF
THE
COMPUTER
IS
LARGE
ENOUGH
TO
HOLD
ALL
THE
PROC
ESSES
THE
SCHEMES
DESCRIBED
SO
FAR
WILL
MORE
OR
LESS
DO
BUT
IN
PRACTICE
THE
TOTAL
AMOUNT
OF
RAM
NEEDED
BY
ALL
THE
PROCESSES
IS
OFTEN
MUCH
MORE
THAN
CAN
FIT
IN
MEMORY
ON
A
TYPICAL
WINDOWS
OR
LINUX
SYSTEM
SOMETHING
LIKE
PROCESSES
MEMORY
MANAGEMENT
CHAP
LIMIT
REGISTER
SEC
A
MEMORY
ABSTRACTION
ADDRESS
SPACES
THE
OPERATION
OF
A
SWAPPING
SYSTEM
IS
ILLUSTRATED
IN
FIG
INITIALLY
ONLY
PROCESS
A
IS
IN
MEMORY
THEN
PROCESSES
B
AND
C
ARE
CREATED
OR
SWAPPED
IN
FROM
DISK
IN
FIG
D
A
IS
SWAPPED
OUT
TO
DISK
THEN
D
COMES
IN
AND
B
GOES
OUT
FINALLY
A
COMES
IN
AGAIN
SINCE
A
IS
NOW
AT
A
DIFFERENT
LOCATION
ADDRESSES
CON
TAINED
IN
IT
MUST
BE
RELOCATED
EITHER
BY
SOFTWARE
WHEN
IT
IS
SWAPPED
IN
OR
MORE
LIKELY
BY
HARDWARE
DURING
PROGRAM
EXECUTION
FOR
EXAMPLE
BASE
AND
LIMIT
REGIS
TERS
WOULD
WORK
FINE
HERE
BASE
REGISTER
AD
D
JMR
C
FIGURE
BASE
AND
LIMIT
REGISTERS
CAN
BE
USED
TO
GIVE
EACH
PROCESS
A
SEPARATE
ADDRESS
SPACE
OR
MORE
MAY
BE
STARTED
UP
WHEN
THE
COMPUTER
IS
BOOTED
FOR
EXAMPLE
WHEN
A
WINDOWS
APPLICATION
IS
INSTALLED
IT
OFTEN
ISSUES
COMMANDS
SO
THAT
ON
SUBSEQUENT
SYSTEM
BOOTS
A
PROCESS
WILL
BE
STARTED
THAT
DOES
NOTHING
EXCEPT
CHECK
FOR
UPDATES
TO
THE
APPLICATION
SUCH
A
PROCESS
CAN
EASILY
OCCUPY
MB
OF
MEMORY
OTHER
BACKGROUND
PROCESSES
CHECK
FOR
INCOMING
MAIL
INCOMING
NETWORK
CONNECTIONS
AND
MANY
OTHER
THINGS
AND
ALL
THIS
IS
BEFORE
THE
FIRST
USER
PROGRAM
IS
STARTED
SERIOUS
USER
APPLICATION
PROGRAMS
NOWADAYS
CAN
EASILY
RUN
FROM
TO
MB
AND
MORE
CONSEQUENTLY
KEEPING
ALL
PROCESSES
IN
MEMORY
ALL
THE
TIME
REQUIRES
A
HUGE
AMOUNT
OF
MEMORY
AND
CANNOT
BE
DONE
IF
THERE
IS
INSUFFICIENT
MEMORY
TWO
GENERAL
APPROACHES
TO
DEALING
WITH
MEMORY
OVERLOAD
HAVE
BEEN
DEVEL
OPED
OVER
THE
YEARS
THE
SIMPLEST
STRATEGY
CALLED
SWAPPING
CONSISTS
OF
BRINGING
IN
EACH
PROCESS
IN
ITS
ENTIRETY
RUNNING
IT
FOR
A
WHILE
THEN
PUTTING
IT
BACK
ON
THE
DISK
IDLE
PROCESSES
ARE
MOSTLY
STORED
ON
DISK
SO
THEY
DO
NOT
TAKE
UP
ANY
MEMORY
WHEN
THEY
ARE
NOT
RUNNING
ALTHOUGH
SOME
OF
THEM
WAKE
UP
PERIODICALLY
TO
DO
THEIR
WORK
THEN
GO
TO
SLEEP
AGAIN
THE
OTHER
STRATEGY
CALLED
VIRTUAL
MEMORY
ALLOWS
PROGRAMS
TO
RUN
EVEN
WHEN
THEY
ARE
ONLY
PARTIALLY
IN
MAIN
MEMORY
BELOW
WE
WILL
STUDY
SWAPPING
IN
SEC
WE
WILL
EXAMINE
VIRTUAL
MEMORY
WHEN
SWAPPING
CREATES
MULTIPLE
HOLES
IN
MEMORY
IT
IS
POSSIBLE
TO
COMBINE
THEM
ALL
INTO
ONE
BIG
ONE
BY
MOVING
ALL
THE
PROCESSES
DOWNWARD
AS
FAR
AS
POS
SIBLE
THIS
TECHNIQUE
IS
KNOWN
AS
MEMORY
COMPACTION
IT
IS
USUALLY
NOT
DONE
BE
CAUSE
IT
REQUIRES
A
LOT
OF
CPU
TIME
FOR
EXAMPLE
ON
A
GB
MACHINE
THAT
CAN
COPY
BYTES
IN
NSEC
IT
WOULD
TAKE
ABOUT
SEC
TO
COMPACT
ALL
OF
MEMORY
A
POINT
THAT
IS
WORTH
MAKING
CONCERNS
HOW
MUCH
MEMORY
SHOULD
BE
ALLOCATED
FOR
A
PROCESS
WHEN
IT
IS
CREATED
OR
SWAPPED
IN
IF
PROCESSES
ARE
CREATED
WITH
A
FIX
ED
SIZE
THAT
NEVER
CHANGES
THEN
THE
ALLOCATION
IS
SIMPLE
THE
OPERATING
SYSTEM
AL
LOCATES
EXACTLY
WHAT
IS
NEEDED
NO
MORE
AND
NO
LESS
IF
HOWEVER
PROCESSES
DATA
SEGMENTS
CAN
GROW
FOR
EXAMPLE
BY
DYNAMICALLY
ALLOCATING
MEMORY
FROM
A
HEAP
AS
IN
MANY
PROGRAMMING
LANGUAGES
A
PROBLEM
OCCURS
WHENEVER
A
PROCESS
TRIES
TO
GROW
IF
A
HOLE
IS
ADJACENT
TO
THE
PROCESS
IT
CAN
BE
ALLOCATED
AND
THE
PROCESS
ALLOWED
TO
GROW
INTO
THE
HOLE
ON
THE
OTHER
HAND
IF
THE
PROCESS
IS
ADJACENT
TO
ANOTHER
PROCESS
THE
GROWING
PROCESS
WILL
EITHER
HAVE
TO
BE
MOVED
TO
A
HOLE
IN
MEMORY
LARGE
ENOUGH
FOR
IT
OR
ONE
OR
MORE
PROC
ESSES
WILL
HAVE
TO
BE
SWAPPED
OUT
TO
CREATE
A
LARGE
ENOUGH
HOLE
IF
A
PROCESS
CAN
NOT
GROW
IN
MEMORY
AND
THE
SWAP
AREA
ON
THE
DISK
IS
FULL
THE
PROCESS
WILL
HAVE
TO
SUSPENDED
UNTIL
SOME
SPACE
IS
FREED
UP
OR
IT
CAN
BE
KILLED
MEMORY
MANAGEMENT
CHAP
IF
IT
IS
EXPECTED
THAT
MOST
PROCESSES
WILL
GROW
AS
THEY
RUN
IT
IS
PROBABLY
A
GOOD
IDEA
TO
ALLOCATE
A
LITTLE
EXTRA
MEMORY
WHENEVER
A
PROCESS
IS
SWAPPED
IN
OR
MOVED
TO
REDUCE
THE
OVERHEAD
ASSOCIATED
WITH
MOVING
OR
SWAPPING
PROCESSES
THAT
NO
LONGER
FIT
IN
THEIR
ALLOCATED
MEMORY
HOWEVER
WHEN
SWAPPING
PROCESSES
TO
DISK
ONLY
THE
MEMORY
ACTUALLY
IN
USE
SHOULD
BE
SWAPPED
IT
IS
WASTEFUL
TO
SWAP
THE
EXTRA
MEMORY
AS
WELL
IN
FIG
A
WE
SEE
A
MEMORY
CONFIGURATION
IN
WHICH
SPACE
FOR
GROWTH
HAS
BEEN
ALLOCATED
TO
TWO
PROCESSES
SEC
A
MEMORY
ABSTRACTION
ADDRESS
SPACES
MEMORY
MANAGEMENT
WITH
BITMAPS
WITH
A
BITMAP
MEMORY
IS
DIVIDED
INTO
ALLOCATION
UNITS
AS
SMALL
AS
A
FEW
WORDS
AND
AS
LARGE
AS
SEVERAL
KILOBYTES
CORRESPONDING
TO
EACH
ALLOCATION
UNIT
IS
A
BIT
IN
THE
BITMAP
WHICH
IS
IF
THE
UNIT
IS
FREE
AND
IF
IT
IS
OCCUPIED
OR
VICE
VERSA
FIGURE
SHOWS
PART
OF
MEMORY
AND
THE
CORRESPONDING
BITMAP
ROOM
FOR
GROWTH
ACTUALLY
IN
USE
ROOM
FOR
GROWTH
B
STACK
I
B
DATA
B
PROGRAM
A
STACK
ROOM
FOR
GROWTH
F
T
ACTUALLY
IN
USE
A
DATA
ROOM
FOR
GROWTH
B
HOLE
STARTS
LENGTH
AT
PROCESS
C
OPERATING
SYSTEM
A
A
PROGRAM
OPERATING
SYSTEM
B
FIGURE
A
A
PART
OF
MEMORY
WITH
FIVE
PROCESSES
AND
THREE
HOLES
THE
TICK
MARKS
SHOW
THE
MEMORY
ALLOCATION
UNITS
THE
SHADED
REGIONS
IN
THE
BITMAP
ARE
FREE
B
THE
CORRESPONDING
BITMAP
C
THE
SAME
INFORMATION
AS
A
LIST
THE
SIZE
OF
THE
ALLOCATION
UNIT
IS
AN
IMPORTANT
DESIGN
ISSUE
THE
SMALLER
THE
AL
LOCATION
UNIT
THE
LARGER
THE
BITMAP
HOWEVER
EVEN
WITH
AN
ALLOCATION
UNIT
AS
SMALL
AS
BYTES
BITS
OF
MEMORY
WILL
REQUIRE
ONLY
BIT
OF
THE
MAP
A
MEMORY
FIGURE
A
ALLOCATING
SPACE
FOR
A
GROWING
DATA
SEGMENT
B
ALLOCATING
SPACE
FOR
A
GROWING
STACK
AND
A
GROWING
DATA
SEGMENT
IF
PROCESSES
CAN
HAVE
TWO
GROWING
SEGMENTS
FOR
EXAMPLE
THE
DATA
SEGMENT
BEING
USED
AS
A
HEAP
FOR
VARIABLES
THAT
ARE
DYNAMICALLY
ALLOCATED
AND
RELEASED
AND
A
STACK
SEGMENT
FOR
THE
NORMAL
LOCAL
VARIABLES
AND
RETURN
ADDRESSES
AN
ALTERNA
TIVE
ARRANGEMENT
SUGGESTS
ITSELF
NAMELY
THAT
OF
FIG
B
IN
THIS
FIGURE
WE
SEE
THAT
EACH
PROCESS
ILLUSTRATED
HAS
A
STACK
AT
THE
TOP
OF
ITS
ALLOCATED
MEMORY
THAT
IS
GROWING
DOWNWARD
AND
A
DATA
SEGMENT
JUST
BEYOND
THE
PROGRAM
TEXT
THAT
IS
GROW
ING
UPWARD
THE
MEMORY
BETWEEN
THEM
CAN
BE
USED
FOR
EITHER
SEGMENT
IF
IT
RUNS
OUT
THE
PROCESS
WILL
EITHER
HAVE
TO
BE
MOVED
TO
A
HOLE
WITH
SUFFICIENT
SPACE
SWAPPED
OUT
OF
MEMORY
UNTIL
A
LARGE
ENOUGH
HOLE
CAN
BE
CREATED
OR
KILLED
MANAGING
FREE
MEMORY
WHEN
MEMORY
IS
ASSIGNED
DYNAMICALLY
THE
OPERATING
SYSTEM
MUST
MANAGE
IT
IN
GENERAL
TERMS
THERE
ARE
TWO
WAYS
TO
KEEP
TRACK
OF
MEMORY
USAGE
BITMAPS
AND
FREE
LISTS
IN
THIS
SECTION
AND
THE
NEXT
ONE
WE
WILL
LOOK
AT
THESE
TWO
METHODS
OF
BITS
WILL
USE
N
MAP
BITS
SO
THE
BITMAP
WILL
TAKE
UP
ONLY
OF
MEMORY
IF
THE
ALLOCATION
UNIT
IS
CHOSEN
LARGE
THE
BITMAP
WILL
BE
SMALLER
BUT
APPRECIABLE
MEMORY
MAY
BE
WASTED
IN
THE
LAST
UNIT
OF
THE
PROCESS
IF
THE
PROCESS
SIZE
IS
NOT
AN
EXACT
MULTIPLE
OF
THE
ALLOCATION
UNIT
A
BITMAP
PROVIDES
A
SIMPLE
WAY
TO
KEEP
TRACK
OF
MEMORY
WORDS
IN
A
FIXED
AMOUNT
OF
MEMORY
BECAUSE
THE
SIZE
OF
THE
BITMAP
DEPENDS
ONLY
ON
THE
SIZE
OF
MEMORY
AND
THE
SIZE
OF
THE
ALLOCATION
UNIT
THE
MAIN
PROBLEM
IS
THAT
WHEN
IT
HAS
BEEN
DECIDED
TO
BRING
A
K
UNIT
PROCESS
INTO
MEMORY
THE
MEMORY
MANAGER
MUST
SEARCH
THE
BITMAP
TO
FIND
A
RUN
OF
K
CONSECUTIVE
BITS
IN
THE
MAP
SEARCHING
A
BIT
MAP
FOR
A
RUN
OF
A
GIVEN
LENGTH
IS
A
SLOW
OPERATION
BECAUSE
THE
RUN
MAY
STRADDLE
WORD
BOUNDARIES
IN
THE
MAP
THIS
IS
AN
ARGUMENT
AGAINST
BITMAPS
MEMORY
MANAGEMENT
WITH
LINKED
LISTS
ANOTHER
WAY
OF
KEEPING
TRACK
OF
MEMORY
IS
TO
MAINTAIN
A
LINKED
LIST
OF
ALLO
CATED
AND
FREE
MEMORY
SEGMENTS
WHERE
A
SEGMENT
EITHER
CONTAINS
A
PROCESS
OR
IS
AN
EMPTY
HOLE
BETWEEN
TWO
PROCESSES
THE
MEMORY
OF
FIG
A
IS
REPRESENTED
MEMORY
MANAGEMENT
CHAP
IN
FIG
C
AS
A
LINKED
LIST
OF
SEGMENTS
EACH
ENTRY
IN
THE
LIST
SPECIFIES
A
HOLE
H
OR
PROCESS
P
THE
ADDRESS
AT
WHICH
IT
STARTS
THE
LENGTH
AND
A
POINTER
TO
THE
NEXT
ENTRY
IN
THIS
EXAMPLE
THE
SEGMENT
LIST
IS
KEPT
SORTED
BY
ADDRESS
SORTING
THIS
WAY
HAS
THE
ADVANTAGE
THAT
WHEN
A
PROCESS
TERMINATES
OR
IS
SWAPPED
OUT
UPDATING
THE
LIST
IS
STRAIGHTFORWARD
A
TERMINATING
PROCESS
NORMALLY
HAS
TWO
NEIGHBORS
EXCEPT
WHEN
IT
IS
AT
THE
VERY
TOP
OR
BOTTOM
OF
MEMORY
THESE
MAY
BE
EITHER
PROCESSES
OR
HOLES
LEADING
TO
THE
FOUR
COMBINATIONS
OF
FIG
IN
FIG
A
UPDATING
THE
LIST
REQUIRES
REPLACING
A
P
BY
AN
H
IN
FIG
B
AND
FIG
C
TWO
ENTRIES
ARE
COA
LESCED
INTO
ONE
AND
THE
LIST
BECOMES
ONE
ENTRY
SHORTER
IN
FIG
D
THREE
EN
TRIES
ARE
MERGED
AND
TWO
ITEMS
ARE
REMOVED
FROM
THE
LIST
SINCE
THE
PROCESS
TABLE
SLOT
FOR
THE
TERMINATING
PROCESS
WILL
NORMALLY
POINT
TO
THE
LIST
ENTRY
FOR
THE
PROCESS
ITSELF
IT
MAY
BE
MORE
CONVENIENT
TO
HAVE
THE
LIST
AS
A
DOUBLE
LINKED
LIST
RATHER
THAN
THE
SINGLE
LINKED
LIST
OF
FIG
C
THIS
STRUCTURE
MAKES
IT
EASIER
TO
FIND
THE
PREVIOUS
ENTRY
AND
TO
SEE
IF
A
MERGE
IS
POSSIBLE
BEFOR
E
X
TERMINATE
AFTER
X
TERMINATE
SEC
A
MEMORY
ABSTRACTION
ADDRESS
SPACES
RATHER
THAN
BREAKING
UP
A
BIG
HOLE
THAT
MIGHT
BE
NEEDED
LATER
BEST
FIT
TRIES
TO
FIND
A
HOLE
THAT
IS
CLOSE
TO
THE
ACTUAL
SIZE
NEEDED
TO
BEST
MATCH
THE
REQUEST
AND
THE
AVAILABLE
HOLES
AS
AN
EXAMPLE
OF
FIRST
FIT
AND
BEST
FIT
CONSIDER
FIG
AGAIN
IF
A
BLOCK
OF
SIZE
IS
NEEDED
FIRST
FIT
WILL
ALLOCATE
THE
HOLE
AT
BUT
BEST
FIT
WILL
ALLOCATE
THE
HOLE
AT
BEST
FIT
IS
SLOWER
THAN
FIRST
FIT
BECAUSE
IT
MUST
SEARCH
THE
ENTIRE
LIST
EVERY
TIME
IT
IS
CALLED
SOMEWHAT
SURPRISINGLY
IT
ALSO
RESULTS
IN
MORE
WASTED
MEMORY
THAN
FIRST
FIT
OR
NEXT
FIT
BECAUSE
IT
TENDS
TO
FILL
UP
MEMORY
WITH
TINY
USELESS
HOLES
FIRST
FIT
GENERATES
LARGER
HOLES
ON
THE
AVERAGE
TO
GET
AROUND
THE
PROBLEM
OF
BREAKING
UP
NEARLY
EXACT
MATCHES
INTO
A
PROCESS
AND
A
TINY
HOLE
ONE
COULD
THINK
ABOUT
WORST
FIT
THAT
IS
ALWAYS
TAKE
THE
LARGEST
AVAILABLE
HOLE
SO
THAT
THE
NEW
HOLE
WILL
BE
BIG
ENOUGH
TO
BE
USEFUL
SIMULATION
HAS
SHOWN
THAT
WORST
FIT
IS
NOT
A
VERY
GOOD
IDEA
EITHER
ALL
FOUR
ALGORITHMS
CAN
BE
SPEEDED
UP
BY
MAINTAINING
SEPARATE
LISTS
FOR
PROC
ESSES
AND
HOLES
IN
THIS
WAY
ALL
OF
THEM
DEVOTE
THEIR
FULL
ENERGY
TO
INSPECTING
HOLES
NOT
PROCESSES
THE
INEVITABLE
PRICE
THAT
IS
PAID
FOR
THIS
SPEEDUP
ON
ALLOCA
TION
IS
THE
ADDITIONAL
COMPLEXITY
AND
SLOWDOWN
WHEN
DEALLOCATING
MEMORY
SINCE
B
X
BECOME
BECOME
BECOME
BECOME
A
FREED
SEGMENT
HAS
TO
BE
REMOVED
FROM
THE
PROCESS
LIST
AND
INSERTED
INTO
THE
HOLE
LIST
IF
DISTINCT
LISTS
ARE
MAINTAINED
FOR
PROCESSES
AND
HOLES
THE
HOLE
LIST
MAY
BE
KEPT
SORTED
ON
SIZE
TO
MAKE
BEST
FIT
FASTER
WHEN
BEST
FIT
SEARCHES
A
LIST
OF
HOLES
FROM
SMALLEST
TO
LARGEST
AS
SOON
AS
IT
FINDS
A
HOLE
THAT
FITS
IT
KNOWS
THAT
THE
HOLE
IS
THE
SMALLEST
ONE
THAT
WILL
DO
THE
JOB
HENCE
THE
BEST
FIT
NO
FURTHER
SEARCHING
IS
NEEDED
AS
IT
IS
WITH
THE
SINGLE
LIST
SCHEME
WITH
A
HOLE
LIST
SORTED
BY
SIZE
FIRST
FIT
AND
BEST
FIT
ARE
EQUALLY
FAST
AND
NEXT
FIT
IS
POINTLESS
FIGUR
E
FOU
R
NEIGHBO
R
COMBINATION
FO
R
TH
E
TERMINATIN
G
PROCESS
X
WHEN
THE
PROCESSES
AND
HOLES
ARE
KEPT
ON
A
LIST
SORTED
BY
ADDRESS
SEVERAL
AL
GORITHMS
CAN
BE
USED
TO
ALLOCATE
MEMORY
FOR
A
CREATED
PROCESS
OR
AN
EXISTING
PROCESS
BEING
SWAPPED
IN
FROM
DISK
WE
ASSUME
THAT
THE
MEMORY
MANAGER
KNOWS
HOW
MUCH
MEMORY
TO
ALLOCATE
THE
SIMPLEST
ALGORITHM
IS
FIRST
FIT
THE
MEMORY
MANAGER
SCANS
ALONG
THE
LIST
OF
SEGMENTS
UNTIL
IT
FINDS
A
HOLE
THAT
IS
BIG
ENOUGH
THE
HOLE
IS
THEN
BROKEN
UP
INTO
TWO
PIECES
ONE
FOR
THE
PROCESS
AND
ONE
FOR
THE
UNUSED
MEMORY
EXCEPT
IN
THE
STATISTICALLY
UNLIKELY
CASE
OF
AN
EXACT
FIT
FIRST
FIT
IS
A
FAST
ALGORITHM
BECAUSE
IT
SEARCHES
AS
LITTLE
AS
POSSIBLE
A
MINOR
VARIATION
OF
FIRST
FIT
IS
NEXT
FIT
IT
WORKS
THE
SAME
WAY
AS
FIRST
FIT
EX
CEPT
THAT
IT
KEEPS
TRACK
OF
WHERE
IT
IS
WHENEVER
IT
FINDS
A
SUITABLE
HOLE
THE
NEXT
TIME
IT
IS
CALLED
TO
FIND
A
HOLE
IT
STARTS
SEARCHING
THE
LIST
FROM
THE
PLACE
WHERE
IT
LEFT
OFF
LAST
TIME
INSTEAD
OF
ALWAYS
AT
THE
BEGINNING
AS
FIRST
FIT
DOES
SIMULATIONS
BY
BAYS
SHOW
THAT
NEXT
FIT
GIVES
SLIGHTLY
WORSE
PERFORMANCE
THAN
FIRST
FIT
ANOTHER
WELL
KNOWN
AND
WIDELY
USED
ALGORITHM
IS
BEST
FIT
BEST
FIT
SEARCHES
THE
ENTIRE
LIST
FROM
BEGINNING
TO
END
AND
TAKES
THE
SMALLEST
HOLE
THAT
IS
ADEQUATE
WHEN
THE
HOLES
ARE
KEPT
ON
SEPARATE
LISTS
FROM
THE
PROCESSES
A
SMALL
OPTIMI
ZATION
IS
POSSIBLE
INSTEAD
OF
HAVING
A
SEPARATE
SET
OF
DATA
STRUCTURES
FOR
MAINTAIN
ING
THE
HOLE
LIST
AS
IS
DONE
IN
FIG
C
THE
INFORMATION
CAN
BE
STORED
IN
THE
HOLES
THE
FIRST
WORD
OF
EACH
HOLE
COULD
BE
THE
HOLE
SIZE
AND
THE
SECOND
WORD
A
POINTER
TO
THE
FOLLOWING
ENTRY
THE
NODES
OF
THE
LIST
OF
FIG
C
WHICH
REQUIRE
THREE
WORDS
AND
ONE
BIT
P
H
ARE
NO
LONGER
NEEDED
YET
ANOTHER
ALLOCATION
ALGORITHM
IS
QUICK
FIT
WHICH
MAINTAINS
SEPARATE
LISTS
FOR
SOME
OF
THE
MORE
COMMON
SIZES
REQUESTED
FOR
EXAMPLE
IT
MIGHT
HAVE
A
TABLE
WITH
N
ENTRIES
IN
WHICH
THE
FIRST
ENTRY
IS
A
POINTER
TO
THE
HEAD
OF
A
LIST
OF
KB
HOLES
THE
SECOND
ENTRY
IS
A
POINTER
TO
A
LIST
OF
KB
HOLES
THE
THIRD
ENTRY
A
POINTER
TO
KB
HOLES
AND
SO
ON
HOLES
OF
SAY
KB
COULD
BE
PUT
ON
EITHER
THE
KB
LIST
OR
ON
A
SPECIAL
LIST
OF
ODD
SIZED
HOLES
WITH
QUICK
FIT
FINDING
A
HOLE
OF
THE
REQUIRED
SIZE
IS
EXTREMELY
FAST
BUT
IT
HAS
THE
SAME
DISADVANTAGE
AS
ALL
SCHEMES
THAT
SORT
BY
HOLE
SIZE
NAMELY
WHEN
A
PROC
ESS
TERMINATES
OR
IS
SWAPPED
OUT
FINDING
ITS
NEIGHBORS
TO
SEE
IF
A
MERGE
IS
POSSIBLE
IS
EXPENSIVE
IF
MERGING
IS
NOT
DONE
MEMORY
WILL
QUICKLY
FRAGMENT
INTO
A
LARGE
NUMBER
OF
SMJFOFO
INTO
WHICH
NO
PROCESSES
FIT
MEMORY
MANAGEMENT
CHAP
VIRTUA
L
MEMOR
Y
WHILE
BASE
AND
LIMIT
REGISTERS
CAN
BE
USED
TO
CREATE
THE
ABSTRACTION
OF
ADDRESS
SPACES
THERE
IS
ANOTHER
PROBLEM
THAT
HAS
TO
BE
SOLVED
MANAGING
BLOATWARE
WHILE
MEMORY
SIZES
ARE
INCREASING
RAPIDLY
SOFTWARE
SIZES
ARE
INCREASING
MUCH
FASTER
IN
THE
MANY
UNIVERSITIES
RAN
A
TIMESHARING
SYSTEM
WITH
DOZENS
OF
MORE
OR
LESS
SATISFIED
USERS
RUNNING
SIMULTANEOUSLY
ON
A
MB
VAX
NOW
MICROSOFT
RECOMMENDS
HAVING
AT
LEAST
MB
FOR
A
SINGLE
USER
VISTA
SYSTEM
TO
RUN
SIMPLE
APPLICATIONS
AND
GB
IF
YOU
ARE
DOING
ANYTHING
SERIOUS
THE
TREND
TOWARD
MULTI
MEDIA
PUTS
EVEN
MORE
DEMANDS
ON
MEMORY
AS
A
CONSEQUENCE
OF
THESE
DEVELOPMENTS
THERE
IS
A
NEED
TO
RUN
PROGRAMS
THAT
ARE
TOO
LARGE
TO
FIT
IN
MEMORY
AND
THERE
IS
CERTAINLY
A
NEED
TO
HAVE
SYSTEMS
THAT
CAN
SUPPORT
MULTIPLE
PROGRAMS
RUNNING
SIMULTANEOUSLY
EACH
OF
WHICH
FITS
IN
MEMORY
BUT
WHICH
COLLECTIVELY
EXCEED
MEMORY
SWAPPING
IS
NOT
AN
ATTRACTIVE
OPTION
SINCE
A
TYPICAL
SATA
DISK
HAS
A
PEAK
TRANSFER
RATE
OF
AT
MOST
MB
SEC
WHICH
MEANS
IT
TAKES
AT
LEAST
SEC
TO
SWAP
OUT
A
GB
PROGRAM
AND
ANOTHER
SEC
TO
SWAP
IN
A
GB
PROGRAM
THE
PROBLEM
OF
PROGRAMS
LARGER
THAN
MEMORY
HAS
BEEN
AROUND
SINCE
THE
BEGINNING
OF
COMPUTING
ALBEIT
IN
LIMITED
AREAS
SUCH
AS
SCIENCE
AND
ENGINEERING
SIMULATING
THE
CREATION
OF
THE
UNIVERSE
OR
EVEN
SIMULATING
A
NEW
AIRCRAFT
TAKES
A
LOT
OF
MEMORY
A
SOLUTION
ADOPTED
IN
THE
WAS
TO
SPLIT
PROGRAMS
INTO
LITTLE
PIECES
CALLED
OVERLAYS
WHEN
A
PROGRAM
STARTED
ALL
THAT
WAS
LOADED
INTO
MEMORY
SEC
VIRTUAL
MEMORY
IN
A
SENSE
VIRTUAL
MEMORY
IS
A
GENERALIZATION
OF
THE
BASE
AND
LIMIT
REGISTER
IDEA
THE
HAD
SEPARATE
BASE
REGISTERS
BUT
NO
LIMIT
REGISTERS
FOR
TEXT
AND
DATA
WITH
VIRTUAL
MEMORY
INSTEAD
OF
HAVING
SEPARATE
RELOCATION
FOR
JUST
THE
TEXT
AND
DATA
SEGMENTS
THE
ENTIRE
ADDRESS
SPACE
CAN
BE
MAPPED
ONTO
PHYSICAL
MEMORY
IN
FAIRLY
SMALL
UNITS
WE
WILL
SHOW
HOW
VIRTUAL
MEMORY
IS
IMPLEMENTED
BELOW
VIRTUAL
MEMORY
WORKS
JUST
FINE
IN
A
MULTIPROGRAMMING
SYSTEM
WITH
BITS
AND
PIECES
OF
MANY
PROGRAMS
IN
MEMORY
AT
ONCE
WHILE
A
PROGRAM
IS
WAITING
FOR
PIECE
OF
ITSELF
TO
BE
READ
IN
THE
CPU
CAN
BE
GIVEN
TO
ANOTHER
PROCESS
PAGIN
G
MOST
VIRTUAL
MEMORY
SYSTEMS
USE
A
TECHNIQUE
CALLED
PAGING
WHICH
WE
WILL
NOW
DESCRIBE
ON
ANY
COMPUTER
PROGRAMS
REFERENCE
A
SET
OF
MEMORY
ADDRESSES
WHEN
A
PROGRAM
EXECUTES
AN
INSTRUCTION
LIKE
MOV
REG
IT
DOES
SO
TO
COPY
THE
CONTENTS
OF
MEMORY
ADDRESS
TO
REG
OR
VICE
VERSA
DE
PENDING
ON
THE
COMPUTER
ADDRESSES
CAN
BE
GENERATED
USING
INDEXING
BASE
REG
ISTERS
SEGMENT
REGISTERS
AND
OTHER
WAYS
WAS
THE
OVERLAY
MANAGER
WHICH
IMMEDIATELY
LOADED
AND
RAN
OVERLAY
WHEN
IT
WAS
DONE
IT
WOULD
TELL
THE
OVERLAY
MANAGER
TO
LOAD
OVERLAY
EITHER
ABOVE
OVER
LAY
IN
MEMORY
IF
THERE
WAS
SPACE
FOR
IT
OR
ON
TOP
OF
OVERLAY
IF
THERE
WAS
NO
SPACE
SOME
OVERLAY
SYSTEMS
WERE
HIGHLY
COMPLEX
ALLOWING
MANY
OVERLAYS
IN
MEMORY
AT
ONCE
THE
OVERLAYS
WERE
KEPT
ON
THE
DISK
AND
SWAPPED
IN
AND
OUT
OF
MEMORY
BY
THE
OVERLAY
MANAGER
ALTHOUGH
THE
ACTUAL
WORK
OF
SWAPPING
OVERLAYS
IN
AND
OUT
WAS
DONE
BY
THE
OPERATING
SYSTEM
THE
WORK
OF
SPLITTING
THE
PROGRAM
INTO
PIECES
HAD
TO
BE
DONE
MANUALLY
BY
THE
PROGRAMMER
SPLITTING
LARGE
PROGRAMS
UP
INTO
SMALL
MODULAR
CPU
CPU
PACKAGE
THE
CPU
SENDS
VIRTUAL
ADDRESSES
TO
THE
MMU
BUS
PIECES
WAS
TIME
CONSUMING
BORING
AND
ERROR
PRONE
FEW
PROGRAMMERS
WERE
GOOD
AT
THIS
IT
DID
NOT
TAKE
LONG
BEFORE
SOMEONE
THOUGHT
OF
A
WAY
TO
TURN
THE
WHOLE
JOB
OVER
TO
THE
COMPUTER
THE
METHOD
THAT
WAS
DEVISED
FOTHERINGHAM
HAS
COME
TO
BE
KNOWN
AS
VIRTUAL
MEMORY
THE
BASIC
IDEA
BEHIND
VIRTUAL
MEMORY
IS
THAT
EACH
PROGRAM
HAS
ITS
OWN
ADDRESS
SPACE
WHICH
IS
BROKEN
UP
INTO
CHUNKS
CALLED
PAGES
EACH
PAGE
IS
A
CONTIGUOUS
RANGE
OF
ADDRESSES
THESE
PAGES
ARE
MAPPED
ONTO
PHYSICAL
MEMORY
BUT
NOT
ALL
PAGES
HAVE
TO
BE
IN
PHYSICAL
MEMORY
TO
RUN
THE
PROGRAM
WHEN
THE
PROGRAM
REFERENCES
A
PART
OF
ITS
ADDRESS
SPACE
THAT
IS
IN
PHYSICAL
MEMORY
THE
HARDWARE
PERFORMS
THE
NECESSARY
MAPPING
ON
THE
FLY
WHEN
THE
PROGRAM
REFER
ENCES
A
PART
OF
ITS
ADDRESS
SPACE
THAT
IS
NOT
IN
PHYSICAL
MEMORY
THE
OPERATING
SYS
TEM
IS
ALERTED
TO
GO
GET
THE
MISSING
PIECE
AND
RE
EXECUTE
THE
INSTRUCTION
THAT
FAILED
THE
MMU
SENDS
PHYSICAL
ADDRESSES
TO
THE
MEMORY
FIGURE
THE
POSITION
AND
FUNCTION
OF
THE
MMU
HERE
THE
MMU
IS
SHOWN
AS
BEING
A
PART
OF
THE
CPU
CHIP
BECAUSE
IT
COMMONLY
IS
NOWADAYS
HOWEVER
LOGICALLY
IT
COULD
BE
A
SEPARATE
CHIP
AND
WAS
IN
YEARS
GONE
BY
THESE
PROGRAM
GENERATED
ADDRESSES
ARE
CALLED
VIRTUAL
ADDRESSES
AND
FORM
THE
VIRTUAL
ADDRESS
SPACE
ON
COMPUTERS
WITHOUT
VIRTUAL
MEMORY
THE
VIRTUAL
ADDRESS
IS
PUT
DIRECTLY
ONTO
THE
MEMORY
BUS
AND
CAUSES
THE
PHYSICAL
MEMORY
WORD
WITH
THE
SAME
ADDRESS
TO
BE
READ
OR
WRITTEN
WHEN
VIRTUAL
MEMORY
IS
USED
THE
VIRTUAL
ADDRESSES
DO
NOT
GO
DIRECTLY
TO
THE
MEMORY
BUS
INSTEAD
THEY
GO
TO
AN
MMU
MEMORY
MANAGEMENT
CHAP
MEMORY
MANAGEMENT
UNIT
THAT
MAPS
THE
VIRTUAL
ADDRESSES
ONTO
THE
PHYSICAL
MEMORY
ADDRESSES
AS
ILLUSTRATED
IN
FIG
A
VERY
SIMPLE
EXAMPLE
OF
HOW
THIS
MAPPING
WORKS
IS
SHOWN
IN
FIG
IN
THIS
EXAMPLE
WE
HAVE
A
COMPUTER
THAT
GENERATES
BIT
ADDRESSES
FROM
UP
TO
THESE
ARE
THE
VIRTUAL
ADDRESSES
THIS
COMPUTER
HOWEVER
HAS
ONLY
KB
OF
PHYSICAL
MEMORY
SO
ALTHOUGH
KB
PROGRAMS
CAN
BE
WRITTEN
THEY
CANNOT
BE
LOADED
INTO
MEMORY
IN
THEIR
ENTIRETY
AND
RUN
A
COMPLETE
COPY
OF
A
PROGRAM
CORE
IMAGE
UP
TO
KB
MUST
BE
PRESENT
ON
THE
DISK
HOWEVER
SO
THAT
PIECES
CAN
BE
BROUGHT
IN
AS
NEEDED
THE
VIRTUAL
ADDRESS
SPACE
IS
DIVIDED
INTO
FIXED
SIZE
UNITS
CALLED
PAGES
THE
CORRESPONDING
UNITS
IN
THE
PHYSICAL
MEMORY
ARE
CALLED
PAGE
FRAMES
THE
PAGES
AND
PAGE
FRAMES
ARE
GENERALLY
THE
SAME
SIZE
IN
THIS
EXAMPLE
THEY
ARE
KB
BUT
PAGE
SIZES
FROM
BYTES
TO
KB
HAVE
BEEN
USED
IN
REAL
SYSTEMS
WITH
KB
OF
VIRTUAL
ADDRESS
SPACE
AND
KB
OF
PHYSICAL
MEMORY
WE
GET
VIRTUAL
PAGES
AND
PAGE
FRAMES
TRANSFERS
BETWEEN
RAM
AND
DISK
ARE
ALWAYS
IN
WHOLE
PAGES
VIRTUAL
ADDRESS
SPACE
SEC
VIRTUAL
MEMORY
REFERS
TO
ADDRESSES
TO
AND
SO
ON
EACH
PAGE
CONTAINS
EXACTLY
AD
DRESSES
STARTING
AT
A
MULTIPLE
OF
AND
ENDING
ONE
SHY
OF
A
MULTIPLE
OF
WHEN
THE
PROGRAM
TRIES
TO
ACCESS
ADDRESS
FOR
EXAMPLE
USING
THE
INSTRUCTION
MOV
REG
VIRTUAL
ADDRESS
IS
SENT
TO
THE
MMU
THE
MMU
SEES
THAT
THIS
VIRTUAL
ADDRESS
FALLS
IN
PAGE
TO
WHICH
ACCORDING
TO
ITS
MAPPING
IS
PAGE
FRAME
TO
IT
THUS
TRANSFORMS
THE
ADDRESS
TO
AND
OUTPUTS
ADDRESS
ONTO
THE
BUS
THE
MEMORY
KNOWS
NOTHING
AT
ALL
ABOUT
THE
MMU
AND
JUST
SEES
A
REQUEST
FOR
READING
OR
WRITING
ADDRESS
WHICH
IT
HONORS
THUS
THE
MMU
HAS
EFFECTIVELY
MAPPED
ALL
VIRTUAL
ADDRESSES
BETWEEN
AND
ONTO
PHYSICAL
ADDRESSES
TO
SIMILARLY
THE
INSTRUCTION
MOV
REG
IS
EFFECTIVELY
TRANSFORMED
INTO
MOV
REG
BECAUSE
VIRTUAL
ADDRESS
IN
VIRTUAL
PAGE
IS
MAPPED
ONTO
IN
PHYSICAL
VIRTUAL
PAGE
PHYSICAL
MEMORY
ADDRESS
PAGE
FRAME
PAGE
FRAME
AS
A
THIRD
EXAMPLE
VIRTUAL
ADDRESS
IS
BYTES
FROM
THE
START
OF
VIRTUAL
PAGE
VIRTUAL
ADDRESSES
TO
AND
MAPS
ONTO
PHYSICAL
ADDRESS
BY
ITSELF
THIS
ABILITY
TO
MAP
THE
VIRTUAL
PAGES
ONTO
ANY
OF
THE
EIGHT
PAGE
FRAMES
BY
SETTING
THE
MMU
MAP
APPROPRIATELY
DOES
NOT
SOLVE
THE
PROBLEM
THAT
THE
VIRTUAL
ADDRESS
SPACE
IS
LARGER
THAN
THE
PHYSICAL
MEMORY
SINCE
WE
HAVE
ONLY
EIGHT
PHYSICAL
PAGE
FRAMES
ONLY
EIGHT
OF
THE
VIRTUAL
PAGES
IN
FIG
ARE
MAPPED
ONTO
PHYSICAL
MEMORY
THE
OTHERS
SHOWN
AS
A
CROSS
IN
THE
FIGURE
ARE
NOT
MAPPED
IN
THE
ACTUAL
HARDWARE
A
PRESENT
ABSENT
BIT
KEEPS
TRACK
OF
WHICH
PAGES
ARE
PHYS
ICALLY
PRESENT
IN
MEMORY
WHAT
HAPPENS
IF
THE
PROGRAM
REFERENCES
AN
UNMAPPED
ADDRESSES
FOR
EXAMPLE
BY
USING
THE
INSTRUCTION
MOV
REG
WHICH
IS
BYTE
WITHIN
VIRTUAL
PAGE
STARTING
AT
THE
MMU
NOTICES
THAT
THE
PAGE
IS
UNMAPPED
INDICATED
BY
A
CROSS
IN
THE
FIGURE
AND
CAUSES
THE
CPU
TO
TRAP
TO
THE
OPERATING
SYSTEM
THIS
TRAP
IS
CALLED
A
PAGE
FAULT
THE
OPERATING
SYS
TEM
PICKS
A
LITTLE
USED
PAGE
FRAME
AND
WRITES
ITS
CONTENTS
BACK
TO
THE
DISK
IF
IT
IS
FIGURE
THE
RELATION
BETWEEN
VIRTUAL
ADDRESSES
AND
PHYSICAL
MEMORY
AD
DRESSES
IS
GIVEN
BY
THE
PAGE
TABLE
EVERY
PAGE
BEGINS
ON
A
MULTIPLE
OF
AND
ENDS
ADDRESSES
HIGHER
SO
REALLY
MEANS
AND
TO
MEANS
THE
NOTATION
IN
FIG
IS
AS
FOLLOWS
THE
RANGE
MARKED
OK
K
MEANS
THAT
THE
VIRTUAL
OR
PHYSICAL
ADDRESSES
IN
THAT
PAGE
ARE
TO
THE
RANGE
NOT
ALREADY
THERE
IT
THEN
FETCHES
THE
PAGE
JUST
REFERENCED
INTO
THE
PAGE
FRAME
JUST
FREED
CHANGES
THE
MAP
AND
RESTARTS
THE
TRAPPED
INSTRUCTION
FOR
EXAMPLE
IF
THE
OPERATING
SYSTEM
DECIDED
TO
EVICT
PAGE
FRAME
I
IT
WOULD
LOAD
VIRTUAL
PAGE
AT
PHYSICAL
ADDRESS
AND
MAKE
TWO
CHANGES
TO
THE
MMU
MAP
FIRST
IT
WOULD
MARK
VIRTUAL
PAGE
L
ENTRY
AS
UNMAPPED
TO
TRAP
ANY
FUTURE
ACCESSES
TO
VIRTUAL
ADDRESSES
BETWEEN
AND
THEN
IT
WOULD
REPLACE
THE
MEMORY
MANAGEMENT
CHAP
CROSS
IN
VIRTUAL
PAGE
ENTRY
WITH
A
SO
THAT
WHEN
THE
TRAPPED
INSTRUCTION
IS
RE
EXECUTED
IT
WILL
MAP
VIRTUAL
ADDRESS
TO
PHYSICAL
ADDRESS
NOW
LET
US
LOOK
INSIDE
THE
MMU
TO
SEE
HOW
IT
WORKS
AND
WHY
WE
HAVE
CHO
SEN
TO
USE
A
PAGE
SIZE
THAT
IS
A
POWER
OF
IN
FIG
WE
SEE
AN
EXAMPLE
OF
A
VIRTUAL
ADDRESS
IN
BINARY
BEING
MAPPED
USING
THE
MMU
MAP
OF
FIG
THE
INCOMING
BIT
VIRTUAL
ADDRESS
IS
SPLIT
INTO
A
BIT
PAGE
NUMBER
AND
A
BIT
OFFSET
WITH
BITS
FOR
THE
PAGE
NUMBER
WE
CAN
HAVE
PAGES
AND
WITH
BITS
FOR
THE
OFFSET
WE
CAN
ADDRESS
ALL
BYTES
WITHIN
A
PAGE
OUTGOING
SEC
VIRTUAL
MEMORY
PAGE
TABLES
IN
A
SIMPLE
IMPLEMENTATION
THE
MAPPING
OF
VIRTUAL
ADDRESSES
ONTO
PHYSICAL
ADDRESSES
CAN
BE
SUMMARIZED
AS
FOLLOWS
THE
VIRTUAL
ADDRESS
IS
SPLIT
INTO
A
VIRTUAL
PAGE
NUMBER
HIGH
ORDER
BITS
AND
AN
OFFSET
LOW
ORDER
BITS
FOR
EXAMPLE
WITH
A
BIT
ADDRESS
AND
A
KB
PAGE
SIZE
THE
UPPER
BITS
COULD
SPECIFY
ONE
OF
THE
VIRTUAL
PAGES
AND
THE
LOWER
BITS
WOULD
THEN
SPECIFY
THE
BYTE
OFFSET
TO
WITHIN
THE
SELECTED
PAGE
HOWEVER
A
SPLIT
WITH
OR
OR
SOME
OTHER
NUMBER
OF
BITS
FOR
THE
PAGE
IS
ALSO
POSSIBLE
DIFFERENT
SPLITS
IMPLY
DIFFERENT
PAGE
SIZES
THE
VIRTUAL
PAGE
NUMBER
IS
USED
AS
AN
INDEX
INTO
THE
PAGE
TABLE
TO
FIND
THE
TABLE
I
O
O
J
O
O
O
O
J
O
VJOJOL
I
BIT
OFFSET
COPIED
DIRECTLY
FROM
INPUT
TO
OUTPUT
PRESENT
ABSENT
BIT
VIRTUAL
PAGE
IS
USED
AS
AN
INDEX
INTO
THE
PAGE
TABLE
PHYSICAL
ADDRESS
INCOMING
VIRTUAL
ENTRY
FOR
THAT
VIRTUAL
PAGE
FROM
THE
PAGE
TABLE
ENTRY
THE
PAGE
FRAME
NUMBER
IF
ANY
IS
FOUND
THE
PAGE
FRAME
NUMBER
IS
ATTACHED
TO
THE
HIGH
ORDER
END
OF
THE
OFFSET
REPLACING
THE
VIRTUAL
PAGE
NUMBER
TO
FORM
A
PHYSICAL
ADDRESS
THAT
CAN
BE
SENT
TO
THE
MEMORY
THUS
THE
PURPOSE
OF
THE
PAGE
TABLE
IS
TO
MAP
VIRTUAL
PAGES
ONTO
PAGE
FRAMES
MATHEMATICALLY
SPEAKING
THE
PAGE
TABLE
IS
A
FUNCTION
WITH
THE
VIRTUAL
PAGE
NUM
BER
AS
ARGUMENT
AND
THE
PHYSICAL
FRAME
NUMBER
AS
RESULT
USING
THE
RESULT
OF
THIS
FUNCTION
THE
VIRTUAL
PAGE
FIELD
IN
A
VIRTUAL
ADDRESS
CAN
BE
REPLACED
BY
A
PAGE
FRAME
FIELD
THUS
FORMING
A
PHYSICAL
MEMORY
ADDRESS
STRUCTURE
OF
A
PAGE
TABLE
ENTRY
LET
US
NOW
TURN
FROM
THE
STRUCTURE
OF
THE
PAGE
TABLES
IN
THE
LARGE
TO
THE
DETAILS
OF
A
SINGLE
PAGE
TABLE
ENTRY
THE
EXACT
LAYOUT
OF
AN
ENTRY
IS
HIGHLY
MACHINE
DEPEN
DENT
BUT
THE
KIND
OF
INFORMATION
PRESENT
IS
ROUGHLY
THE
SAME
FROM
MACHINE
TO
MA
CHINE
IN
FIG
WE
GIVE
A
SAMPLE
PAGE
TABLE
ENTRY
THE
SIZE
VARIES
FROM
COM
PUTER
TO
COMPUTER
BUT
BITS
IS
A
COMMON
SIZE
THE
MOST
IMPORTANT
FIELD
IS
THE
PAGE
FRAME
NUMBER
AFTER
ALL
THE
GOAL
OF
THE
PAGE
MAPPING
IS
TO
OUTPUT
THIS
VAL
UE
NEXT
TO
IT
WE
HAVE
THE
PRESENT
ABSENT
BIT
IF
THIS
BIT
IS
THE
ENTRY
IS
VALID
AND
CAN
BE
USED
IF
IT
IS
THE
VIRTUAL
PAGE
TO
WHICH
THE
ENTRY
BELONGS
IS
NOT
CURRENTLY
FOLOH
IOLO
OIO
OJO
O
OIOLOH
ADDRESS
IN
MEMORY
ACCESSING
A
PAGE
TABLE
ENTRY
WITH
THIS
BIT
SET
TO
CAUSES
A
PAGE
FAULT
CACHING
DISABLED
MODIFIED
PRESENT
ABSENT
FIGURE
THE
INTERNAL
OPERATION
OF
THE
MMU
WITH
KB
PAGES
THE
PAGE
NUMBER
IS
USED
AS
AN
INDEX
INTO
THE
PAGE
TABLE
YIELDING
THE
NUMBER
OF
THE
PAGE
FRAME
CORRESPONDING
TO
THAT
VIRTUAL
PAGE
IF
THE
PRESENT
ABSENT
BIT
IS
A
TRAP
TO
THE
OPERATING
SYSTEM
IS
CAUSED
IF
THE
BIT
IS
THE
PAGE
FRAME
NUMBER
FOUND
IN
THE
PAGE
TABLE
IS
COPIED
TO
THE
HIGH
ORDER
BITS
OF
THE
OUTPUT
REGISTER
ALONG
WITH
THE
BIT
OFFSET
WHICH
IS
COPIED
UNMODIFIED
FROM
THE
INCOMING
VIRTUAL
ADDRESS
TOGETHER
THEY
FORM
A
BIT
PHYSICAL
ADDRESS
THE
OUTPUT
REGISTER
IS
THEN
PUT
ONTO
THE
MEMORY
BUS
AS
THE
PHYSICAL
MEMORY
ADDRESS
PAGE
FRAME
NUMBER
Y
REFERENCED
PROTECTION
FIGURE
A
TYPICAL
PAGE
TABLE
ENTRY
MEMORY
MANAGEMENT
CHAP
THE
PROTECTION
BITS
TELL
WHAT
KINDS
OF
ACCESS
ARE
PERMITTED
IN
THE
SIMPLEST
FORM
THIS
FIELD
CONTAINS
BIT
WITH
FOR
READ
WRITE
AND
FOR
READ
ONLY
A
MORE
SOPHISTICATED
ARRANGEMENT
IS
HAVING
BITS
ONE
BIT
EACH
FOR
ENABLING
READING
WRIT
ING
AND
EXECUTING
THE
PAGE
THE
MODIFIED
AND
REFERENCED
BITS
KEEP
TRACK
OF
PAGE
USAGE
WHEN
A
PAGE
IS
WRITTEN
TO
THE
HARDWARE
AUTOMATICALLY
SETS
THE
MODIFIED
BIT
THIS
BIT
IS
OF
VALUE
WHEN
THE
OPERATING
SYSTEM
DECIDES
TO
RECLAIM
A
PAGE
FRAME
IF
THE
PAGE
IN
IT
HAS
BEEN
MODIFIED
I
E
IS
DIRTY
IT
MUST
BE
WRITTEN
BACK
TO
THE
DISK
IF
IT
HAS
NOT
BEEN
MODIFIED
I
E
IS
CLEAN
IT
CAN
JUST
BE
ABANDONED
SINCE
THE
DISK
COPY
IS
STILL
VALID
THE
BIT
IS
SOMETIMES
CALLED
THE
DIRTY
BIT
SINCE
IT
REFLECTS
THE
PAGE
STATE
THE
REFERENCED
BIT
IS
SET
WHENEVER
A
PAGE
IS
REFERENCED
EITHER
FOR
READING
OR
WRITING
ITS
VALUE
IS
TO
HELP
THE
OPERATING
SYSTEM
CHOOSE
A
PAGE
TO
EVICT
WHEN
A
PAGE
FAULT
OCCURS
PAGES
THAT
ARE
NOT
BEING
USED
ARE
BETTER
CANDIDATES
THAN
PAGES
THAT
ARE
AND
THIS
BIT
PLAYS
AN
IMPORTANT
ROLE
IN
SEVERAL
OF
THE
PAGE
REPLACEMENT
AL
GORITHMS
THAT
WE
WILL
STUDY
LATER
IN
THIS
CHAPTER
FINALLY
THE
LAST
BIT
ALLOWS
CACHING
TO
BE
DISABLED
FOR
THE
PAGE
THIS
FEATURE
IS
IMPORTANT
FOR
PAGES
THAT
MAP
ONTO
DEVICE
REGISTERS
RATHER
THAN
MEMORY
IF
THE
OP
ERATING
SYSTEM
IS
SITTING
IN
A
TIGHT
LOOP
WAITING
FOR
SOME
I
O
DEVICE
TO
RESPOND
TO
A
COMMAND
IT
WAS
JUST
GIVEN
IT
IS
ESSENTIAL
THAT
THE
HARDWARE
KEEP
FETCHING
THE
WORD
FROM
THE
DEVICE
AND
NOT
USE
AN
OLD
CACHED
COPY
WITH
THIS
BIT
CACHING
CAN
BE
TURNED
OFF
MACHINES
THAT
HAVE
A
SEPARATE
I
O
SPACE
AND
DO
NOT
USE
MEMORY
MAP
PED
I
O
DO
NOT
NEED
THIS
BIT
NOTE
THAT
THE
DISK
ADDRESS
USED
TO
HOLD
THE
PAGE
WHEN
IT
IS
NOT
IN
MEMORY
IS
NOT
PART
OF
THE
PAGE
TABLE
THE
REASON
IS
SIMPLE
THE
PAGE
TABLE
HOLDS
ONLY
THAT
INFORMATION
THE
HARDWARE
NEEDS
TO
TRANSLATE
A
VIRTUAL
ADDRESS
TO
A
PHYSICAL
ADDRESS
INFORMATION
THE
OPERATING
SYSTEM
NEEDS
TO
HANDLE
PAGE
FAULTS
IS
KEPT
IN
SOFTWARE
TABLES
INSIDE
THE
OPERATING
SYSTEM
THE
HARDWARE
DOES
NOT
NEED
IT
BEFORE
GETTING
INTO
MORE
IMPLEMENTATION
ISSUES
IT
IS
WORTH
POINTING
OUT
AGAIN
THAT
WHAT
VIRTUAL
MEMORY
FUNDAMENTALLY
DOES
IS
CREATE
A
NEW
ABSTRACTION
THE
AD
DRESS
SPACE
WHICH
IS
AN
ABSTRACTION
OF
PHYSICAL
MEMORY
JUST
AS
A
PROCESS
IS
AN
ABSTRACTION
OF
THE
PHYSICAL
PROCESSOR
CPU
VIRTUAL
MEMORY
CAN
BE
IMPLEMENTED
BY
BREAKING
THE
VIRTUAL
ADDRESS
SPACE
UP
INTO
PAGES
AND
MAPPING
EACH
ONE
ONTO
SOME
PAGE
FRAME
OF
PHYSICAL
MEMORY
OR
HAVING
IT
TEMPORARILY
UNMAPPED
THUS
THIS
CHAPTER
IS
BASICALLY
ABOUT
AN
ABSTRACTION
CREATED
BY
THE
OPERATING
SYSTEM
AND
HOW
THAT
ABSTRACTION
IS
MANAGED
SPEEDING
UP
PAGING
WE
HAVE
JUST
SEEN
THE
BASICS
OF
VIRTUAL
MEMORY
AND
PAGING
IT
IS
NOW
TIME
TO
GO
INTO
MORE
DETAIL
ABOUT
POSSIBLE
IMPLEMENTATIONS
IN
ANY
PAGING
SYSTEM
TWO
MAJOR
ISSUES
MUST
BE
FACED
SEC
VIRTUAL
MEMORY
THE
MAPPING
FROM
VIRTUAL
ADDRESS
TO
PHYSICAL
ADDRESS
MUST
BE
FAST
IF
THE
VIRTUAL
ADDRESS
SPACE
IS
LARGE
THE
PAGE
TABLE
WILL
BE
LARGE
THE
FIRST
POINT
IS
A
CONSEQUENCE
OF
THE
FACT
THAT
THE
VIRTUAL
TO
PHYSICAL
MAP
PING
MUST
BE
DONE
ON
EVERY
MEMORY
REFERENCE
ALL
INSTRUCTIONS
MUST
ULTIMATELY
COME
FROM
MEMORY
AND
MANY
OF
THEM
REFERENCE
OPERANDS
IN
MEMORY
AS
WELL
CONSEQUENTLY
IT
IS
NECESSARY
TO
MAKE
ONE
TWO
OR
SOMETIMES
MORE
PAGE
TABLE
REF
ERENCES
PER
INSTRUCTION
IF
AN
INSTRUCTION
EXECUTION
TAKES
SAY
NSEC
THE
PAGE
TABLE
LOOKUP
MUST
BE
DONE
IN
UNDER
NSEC
TO
AVOID
HAVING
THE
MAPPING
BECOME
A
MAJOR
BOTTLENECK
THE
SECOND
POINT
FOLLOWS
FROM
THE
FACT
THAT
ALL
MODERN
COMPUTERS
USE
VIRTUAL
ADDRESSES
OF
AT
LEAST
BITS
WITH
BITS
BECOMING
INCREASINGLY
COMMON
WITH
SAY
A
KB
PAGE
SIZE
A
BIT
ADDRESS
SPACE
HAS
MILLION
PAGES
AND
A
BIT
AD
DRESS
SPACE
HAS
MORE
THAN
YOU
WANT
TO
CONTEMPLATE
WITH
I
MILLION
PAGES
IN
THE
VIRTUAL
ADDRESS
SPACE
THE
PAGE
TABLE
MUST
HAVE
MILLION
ENTRIES
AND
REMEMBER
THAT
EACH
PROCESS
NEEDS
ITS
OWN
PAGE
TABLE
BECAUSE
IT
HAS
ITS
OWN
VIRTUAL
ADDRESS
SPACE
THE
NEED
FOR
LARGE
FAST
PAGE
MAPPING
IS
A
SIGNIFICANT
CONSTRAINT
ON
THE
WAY
COMPUTERS
ARE
BUILT
THE
SIMPLEST
DESIGN
AT
LEAST
CONCEPTUALLY
IS
TO
HAVE
A
SINGLE
PAGE
TABLE
CONSISTING
OF
AN
ARRAY
OF
FAST
HARDWARE
REGISTERS
WITH
ONE
ENTRY
FOR
EACH
VIRTUAL
PAGE
INDEXED
BY
VIRTUAL
PAGE
NUMBER
AS
SHOWN
IN
FIG
WHEN
A
PROCESS
IS
STARTED
UP
THE
OPERATING
SYSTEM
LOADS
THE
REGISTERS
WITH
THE
PROCESS
PAGE
TABLE
TAKEN
FROM
A
COPY
KEPT
IN
MAIN
MEMORY
DURING
PROCESS
EXECUTION
NO
MORE
MEMORY
REFERENCES
ARE
NEEDED
FOR
THE
PAGE
TABLE
THE
ADVANTAGES
OF
THIS
METHOD
ARE
THAT
IT
IS
STRAIGHTFORWARD
AND
REQUIRES
NO
MEMORY
REFERENCES
DURING
MAPPING
A
DISADVANTAGE
IS
THAT
IT
IS
UNBEARABLY
EXPENSIVE
IF
THE
PAGE
TABLE
IS
LARGE
ANOTHER
IS
THAT
HAVING
TO
LOAD
THE
FULL
PAGE
TABLE
AT
EVERY
CONTEXT
SWITCH
HURTS
PERFORMANCE
AT
THE
OTHER
EXTREME
THE
PAGE
TABLE
CAN
BE
ENTIRELY
IN
MAIN
MEMORY
ALL
THE
HARDWARE
NEEDS
THEN
IS
A
SINGLE
REGISTER
THAT
POINTS
TO
THE
START
OF
THE
PAGE
TABLE
THIS
DESIGN
ALLOWS
THE
VIRTUAL
TO
PHYSICAL
MAP
TO
BE
CHANGED
AT
A
CONTEXT
SWITCH
BY
RELOADING
ONE
REGISTER
OF
COURSE
IT
HAS
THE
DISADVANTAGE
OF
REQUIRING
ONE
OR
MORE
MEMORY
REFERENCES
TO
READ
PAGE
TABLE
ENTRIES
DURING
THE
EXECUTION
OF
EACH
INSTRUCTION
MAKING
IT
VERY
SLOW
TRANSLATION
LOOKASIDE
BUFFERS
LET
US
NOW
LOOK
AT
WIDELY
IMPLEMENTED
SCHEMES
FOR
SPEEDING
UP
PAGING
AND
FOR
HANDLING
LARGE
VIRTUAL
ADDRESS
SPACES
STARTING
WITH
THE
FORMER
THE
STARTING
POINT
OF
MOST
OPTIMIZATION
TECHNIQUES
IS
THAT
THE
PAGE
TABLE
IS
IN
MEMORY
POTEN
TIALLY
THIS
DESIGN
HAS
AN
ENORMOUS
IMPACT
ON
PERFORMANCE
CONSIDER
FOR
EXAMPLE
A
BYTE
INSTRUCTION
THAT
COPIES
ONE
REGISTER
TO
ANOTHER
IN
THE
ABSENCE
OF
PAGING
THIS
INSTRUCTION
MAKES
ONLY
ONE
MEMORY
REFERENCE
TO
FETCH
THE
INSTRUCTION
WITH
MEMORY
MANAGEMENT
CHAP
PAGING
AT
LEAST
ONE
ADDITIONAL
MEMORY
REFERENCE
WILL
BE
NEEDED
TO
ACCESS
THE
PAGE
TABLE
SINCE
EXECUTION
SPEED
IS
GENERALLY
LIMITED
BY
THE
RATE
AT
WHICH
THE
CPU
CAN
GET
INSTRUCTIONS
AND
DATA
OUT
OF
THE
MEMORY
HAVING
TO
MAKE
TWO
MEMO
RY
REFERENCES
PER
MEMORY
REFERENCE
REDUCES
PERFORMANCE
BY
HALF
UNDER
THESE
CONDITIONS
NO
ONE
WOULD
USE
PAGING
COMPUTER
DESIGNERS
HAVE
KNOWN
ABOUT
THIS
PROBLEM
FOR
YEARS
AND
HAVE
COME
UP
WITH
A
SOLUTION
THEIR
SOLUTION
IS
BASED
ON
THE
OBSERVATION
THAT
MOST
PROGRAMS
TEND
TO
MAKE
A
LARGE
NUMBER
OF
REFERENCES
TO
A
SMALL
NUMBER
OF
PAGES
AND
NOT
THE
OTHER
WAY
AROUND
THUS
ONLY
A
SMALL
FRACTION
OF
THE
PAGE
TABLE
ENTRIES
ARE
HEAVILY
READ
THE
REST
ARE
BARELY
USED
AT
ALL
THE
SOLUTION
THAT
HAS
BEEN
DEVISED
IS
TO
EQUIP
COMPUTERS
WITH
A
SMALL
HARD
WARE
DEVICE
FOR
MAPPING
VIRTUAL
ADDRESSES
TO
PHYSICAL
ADDRESSES
WITHOUT
GOING
THROUGH
THE
PAGE
TABLE
THE
DEVICE
CALLED
A
TLB
TRANSLATION
LOOKASIDE
BUFF
ER
OR
SOMETIMES
AN
ASSOCIATIVE
MEMORY
IS
ILLUSTRATED
IN
FIG
IT
IS
USUALLY
INSIDE
THE
MMU
AND
CONSISTS
OF
A
SMALL
NUMBER
OF
ENTRIES
EIGHT
IN
THIS
EXAMPLE
BUT
RARELY
MORE
THAN
EACH
ENTRY
CONTAINS
INFORMATION
ABOUT
ONE
PAGE
INCLUD
ING
THE
VIRTUAL
PAGE
NUMBER
A
BIT
THAT
IS
SET
WHEN
THE
PAGE
IS
MODIFIED
THE
PROTEC
TION
CODE
READ
WRITE
EXECUTE
PERMISSIONS
AND
THE
PHYSICAL
PAGE
FRAME
IN
WHICH
THE
PAGE
IS
LOCATED
THESE
FIELDS
HAVE
A
ONE
TO
ONE
CORRESPONDENCE
WITH
THE
FIELDS
IN
THE
PAGE
TABLE
EXCEPT
FOR
THE
VIRTUAL
PAGE
NUMBER
WHICH
IS
NOT
NEEDED
IN
THE
PAGE
TABLE
ANOTHER
BIT
INDICATES
WHETHER
THE
ENTRY
IS
VALID
I
E
IN
USE
OR
NOT
VALID
VIRTUAL
PAGE
MODIFIED
PROTECTION
PAG
E
FRAME
RW
R
X
RW
RW
R
X
R
X
RW
RW
FIGURE
A
TLB
TO
SPEED
UP
PAGING
AN
EXAMPLE
THAT
MIGHT
GENERATE
THE
TLB
OF
FIG
IS
A
PROCESS
IN
A
LOOP
THAT
SPANS
VIRTUAL
PAGES
AND
SO
THAT
THESE
TLB
ENTRIES
HAVE
PROTECTION
CODES
FOR
READING
AND
EXECUTING
THE
MAIN
DATA
CURRENTLY
BEING
USED
SAY
AN
ARRAY
BEING
PROCESSED
ARE
ON
PAGES
AND
PAGE
CONTAINS
THE
INDICES
USED
IN
THE
ARRAY
CALCULATIONS
FINALLY
THE
STACK
IS
ON
PAGES
AND
LET
US
NOW
SEE
HOW
THE
TLB
FUNCTIONS
WHEN
A
VIRTUAL
ADDRESS
IS
PRESENTED
TO
THE
MMU
FOR
TRANSLATION
THE
HARDWARE
FIRST
CHECKS
TO
SEE
IF
ITS
VIRTUAL
PAGE
NUM
BER
IS
PRESENT
IN
THE
TLB
BY
COMPARING
IT
TO
ALL
THE
ENTRIES
SIMULTANEOUSLY
I
E
IN
SEC
VIRTUAL
MEMORY
PARALLEL
IF
A
VALID
MATCH
IS
FOUND
AND
THE
ACCESS
DOES
NOT
VIOLATE
THE
PROTECTION
BITS
THE
PAGE
FRAME
IS
TAKEN
DIRECTLY
FROM
THE
TLB
WITHOUT
GOING
TO
THE
PAGE
TABLE
IF
THE
VIRTUAL
PAGE
NUMBER
IS
PRESENT
IN
THE
TLB
BUT
THE
INSTRUCTION
IS
TRYING
TO
WRITE
ON
A
READ
ONLY
PAGE
A
PROTECTION
FAULT
IS
GENERATED
THE
INTERESTING
CASE
IS
WHAT
HAPPENS
WHEN
THE
VIRTUAL
PAGE
NUMBER
IS
NOT
IN
THE
TLB
THE
MMU
DETECTS
THE
MISS
AND
DOES
AN
ORDINARY
PAGE
TABLE
LOOKUP
IT
THEN
EVICTS
ONE
OF
THE
ENTRIES
FROM
THE
TLB
AND
REPLACES
IT
WITH
THE
PAGE
TABLE
ENTRY
JUST
LOOKED
UP
THUS
IF
THAT
PAGE
IS
USED
AGAIN
SOON
THE
SECOND
TIME
IT
WILL
RESULT
IN
A
TLB
HIT
RATHER
THAN
A
MISS
WHEN
AN
ENTRY
IS
PURGED
FROM
THE
TLB
THE
MODIFIED
BIT
IS
COPIED
BACK
INTO
THE
PAGE
TABLE
ENTRY
IN
MEMORY
THE
OTHER
VALUES
ARE
ALREADY
THERE
EXCEPT
THE
REFERENCE
BIT
WHEN
THE
TLB
IS
LOADED
FROM
THE
PAGE
TABLE
ALL
THE
FIELDS
ARE
TAKEN
FROM
MEMORY
SOFTWARE
TLB
MANAGEMENT
UP
UNTIL
NOW
WE
HAVE
ASSUMED
THAT
EVERY
MACHINE
WITH
PAGED
VIRTUAL
MEMO
RY
HAS
PAGE
TABLES
RECOGNIZED
BY
THE
HARDWARE
PLUS
A
TLB
IN
THIS
DESIGN
TLB
MANAGEMENT
AND
HANDLING
TLB
FAULTS
ARE
DONE
ENTIRELY
BY
THE
MMU
HARDWARE
TRAPS
TO
THE
OPERATING
SYSTEM
OCCUR
ONLY
WHEN
A
PAGE
IS
NOT
IN
MEMORY
IN
THE
PAST
THIS
ASSUMPTION
WAS
TRUE
HOWEVER
MANY
MODERN
RISC
MA
CHINES
INCLUDING
THE
SPARC
MIPS
AND
HP
PA
DO
NEARLY
ALL
OF
THIS
PAGE
MAN
AGEMENT
IN
SOFTWARE
ON
THESE
MACHINES
THE
TLB
ENTRIES
ARE
EXPLICITLY
LOADED
BY
THE
OPERATING
SYSTEM
WHEN
A
TLB
MISS
OCCURS
INSTEAD
OF
THE
MMU
JUST
GOING
TO
THE
PAGE
TABLES
TO
FIND
AND
FETCH
THE
NEEDED
PAGE
REFERENCE
IT
JUST
GENERATES
A
TLB
FAULT
AND
TOSSES
THE
PROBLEM
INTO
THE
LAP
OF
THE
OPERATING
SYSTEM
THE
SYSTEM
MUST
FIND
THE
PAGE
REMOVE
AN
ENTRY
FROM
THE
TLB
ENTER
THE
NEW
ONE
AND
RESTART
THE
INSTRUCTION
THAT
FAULTED
AND
OF
COURSE
ALL
OF
THIS
MUST
BE
DONE
IN
A
HANDFUL
OF
INSTRUCTIONS
BECAUSE
TLB
MISSES
OCCUR
MUCH
MORE
FREQUENTLY
THAN
PAGE
FAULTS
SURPRISINGLY
ENOUGH
IF
THE
TLB
IS
REASONABLY
LARGE
SAY
ENTRIES
TO
REDUCE
THE
MISS
RATE
SOFTWARE
MANAGEMENT
OF
THE
TLB
TURNS
OUT
TO
BE
ACCEPTABLY
EFFI
CIENT
THE
MAIN
GAIN
HERE
IS
A
MUCH
SIMPLER
MMU
WHICH
FREES
UP
A
CONSIDERABLE
AMOUNT
OF
AREA
ON
THE
CPU
CHIP
FOR
CACHES
AND
OTHER
FEATURES
THAT
CAN
IMPROVE
PERFORMANCE
SOFTWARE
TLB
MANAGEMENT
IS
DISCUSSED
BY
UHLIG
ET
AL
VARIOUS
STRATEGIES
HAVE
BEEN
DEVELOPED
TO
IMPROVE
PERFORMANCE
ON
MACHINES
THAT
DO
TLB
MANAGEMENT
IN
SOFTWARE
ONE
APPROACH
ATTACKS
BOTH
REDUCING
TLB
MISSES
AND
REDUCING
THE
COST
OF
A
TLB
MISS
WHEN
IT
DOES
OCCUR
BALA
ET
AL
TO
REDUCE
TLB
MISSES
SOMETIMES
THE
OPERATING
SYSTEM
CAN
USE
ITS
INTUITION
TO
FIGURE
OUT
WHICH
PAGES
ARE
LIKELY
TO
BE
USED
NEXT
AND
TO
PRELOAD
ENTRIES
FOR
THEM
IN
THE
TLB
FOR
EXAMPLE
WHEN
A
CLIENT
PROCESS
SENDS
A
MESSAGE
TO
A
SERVER
PROCESS
ON
THE
SAME
MACHINE
IT
IS
VERY
LIKELY
THAT
THE
SERVER
WILL
HAVE
TO
RUN
SOON
KNOW
ING
THIS
WHILE
PROCESSING
THE
TRAP
TO
DO
THE
SEND
THE
SYSTEM
CAN
ALSO
CHECK
TO
SEE
WHERE
THE
SERVER
CODE
DATA
AND
STACK
PAGES
ARE
AND
MAP
THEM
IN
BEFORE
THEY
GET
A
CHANCE
TO
CAUSE
TLB
FAULTS
MEMORY
MANAGEMENT
CHAP
THE
NORMAL
WAY
TO
PROCESS
A
TLB
MISS
WHETHER
IN
HARDWARE
OR
IN
SOFTWARE
IS
TO
GO
TO
THE
PAGE
TABLE
AND
PERFORM
THE
INDEXING
OPERATIONS
TO
LOCATE
THE
PAGE
REFERENCED
THE
PROBLEM
WITH
DOING
THIS
SEARCH
IN
SOFTWARE
IS
THAT
THE
PAGES
HOLD
ING
THE
PAGE
TABLE
MAY
NOT
BE
IN
THE
TLB
WHICH
WILL
CAUSE
ADDITIONAL
TLB
FAULTS
DURING
THE
PROCESSING
THESE
FAULTS
CAN
BE
REDUCED
BY
MAINTAINING
A
LARGE
E
G
KB
SOFTWARE
CACHE
OF
TLB
ENTRIES
IN
A
FIXED
LOCATION
WHOSE
PAGE
IS
ALWAYS
KEPT
IN
THE
TLB
BY
FIRST
CHECKING
THE
SOFTWARE
CACHE
THE
OPERATING
SYSTEM
CAN
SUBSTANTIALLY
REDUCE
TLB
MISSES
WHEN
SOFTWARE
TLB
MANAGEMENT
IS
USED
IT
IS
ESSENTIAL
TO
UNDERSTAND
THE
DIF
FERENCE
BETWEEN
TWO
KINDS
OF
MISSES
A
SOFT
MISS
OCCURS
WHEN
THE
PAGE
REFER
ENCED
IS
NOT
IN
THE
TLB
BUT
IS
IN
MEMORY
ALL
THAT
IS
NEEDED
HERE
IS
FOR
THE
TLB
TO
BE
UPDATED
NO
DISK
I
O
IS
NEEDED
TYPICALLY
A
SOFT
MISS
TAKES
MACHINE
INSTRUCTIONS
TO
HANDLE
AND
CAN
BE
COMPLETED
IN
A
FEW
NANOSECONDS
IN
CONTRAST
A
HARD
MISS
OCCURS
WHEN
THE
PAGE
ITSELF
IS
NOT
IN
MEMORY
AND
OF
COURSE
ALSO
NOT
IN
THE
TLB
A
DISK
ACCESS
IS
REQUIRED
TO
BRING
IN
THE
PAGE
WHICH
TAKES
SEVERAL
MILLISECONDS
A
HARD
MISS
IS
EASILY
A
MILLION
TIMES
SLOWER
THAN
A
SOFT
MISS
PAGE
TABLES
FOR
LARGE
MEMORIES
TLBS
CAN
BE
USED
TO
SPEED
UP
VIRTUAL
ADDRESS
TO
PHYSICAL
ADDRESS
TRANSLATION
OVER
THE
ORIGINAL
PAGE
TABLE
IN
MEMORY
SCHEME
BUT
THAT
IS
NOT
THE
ONLY
PROBLEM
WE
HAVE
TO
TACKLE
ANOTHER
PROBLEM
IS
HOW
TO
DEAL
WITH
VERY
LARGE
VIRTUAL
ADDRESS
SPACES
BELOW
WE
WILL
DISCUSS
TWO
WAYS
OF
DEALING
WITH
THEM
MULTILEVEL
PAGE
TABLES
AS
A
FIRST
APPROACH
CONSIDER
THE
USE
OF
A
MULTILEVEL
PAGE
TABLE
A
SIMPLE
EX
AMPLE
IS
SHOWN
IN
FIG
IN
FIG
A
WE
HAVE
A
BIT
VIRTUAL
ADDRESS
THAT
IS
PARTITIONED
INTO
A
BIT
FIELD
A
BIT
FIELD
AND
A
BIT
OFFSET
FIELD
SINCE
OFFSETS
ARE
BITS
PAGES
ARE
KB
AND
THERE
ARE
A
TOTAL
OF
OF
THEM
THE
SECRET
TO
THE
MULTILEVEL
PAGE
TABLE
METHOD
IS
TO
AVOID
KEEPING
ALL
THE
PAGE
TABLES
IN
MEMORY
ALL
THE
TIME
IN
PARTICULAR
THOSE
THAT
ARE
NOT
NEEDED
SHOULD
NOT
BE
KEPT
AROUND
SUPPOSE
FOR
EXAMPLE
THAT
A
PROCESS
NEEDS
MEGABYTES
THE
BOTTOM
MEGABYTES
OF
MEMORY
FOR
PROGRAM
TEXT
THE
NEXT
MEGABYTES
FOR
DATA
AND
THE
TOP
MEGABYTES
FOR
THE
STACK
IN
BETWEEN
THE
TOP
OF
THE
DATA
AND
THE
BOT
TOM
OF
THE
STACK
IS
A
GIGANTIC
HOLE
THAT
IS
NOT
USED
IN
FIG
B
WE
SEE
HOW
THE
TWO
LEVEL
PAGE
TABLE
WORKS
IN
THIS
EXAMPLE
ON
THE
LEFT
WE
HAVE
THE
TOP
LEVEL
PAGE
TABLE
WITH
ENTRIES
CORRESPONDING
TO
THE
BIT
FIELD
WHEN
A
VIRTUAL
ADDRESS
IS
PRESENTED
TO
THE
MMU
IT
FIRST
EXTRACTS
THE
FIELD
AND
USES
THIS
VALUE
AS
AN
INDEX
INTO
THE
TOP
LEVEL
PAGE
TABLE
EACH
OF
THESE
ENTRIES
REPRESENTS
BECAUSE
THE
ENTIRE
GIGABYTE
I
E
BIT
VIRTUAL
ADDRESS
SPACE
HAS
BEEN
CHOPPED
INTO
CHUNKS
OF
BYTES
SEC
VIRTUAL
MEMORY
BITS
PT
OFFSET
A
B
FIGURE
A
A
BIT
ADDRESS
WITH
TWO
PAGE
TABLE
FIELDS
B
TWO
LEVEL
PAGE
TABLES
THE
ENTRY
LOCATED
BY
INDEXING
INTO
THE
TOP
LEVEL
PAGE
TABLE
YIELDS
THE
ADDRESS
OR
THE
PAGE
FRAME
NUMBER
OF
A
SECOND
LEVEL
PAGE
TABLE
ENTRY
OF
THE
TOP
LEVEL
PAGE
TABLE
POINTS
TO
THE
PAGE
TABLE
FOR
THE
PROGRAM
TEXT
ENTRY
POINTS
TO
THE
PAGE
TABLE
FOR
THE
DATA
AND
ENTRY
POINTS
TO
THE
PAGE
TABLE
FOR
THE
STACK
THE
OTHER
SHADED
ENTRIES
ARE
NOT
USED
THE
FIELD
IS
NOW
USED
AS
AN
INDEX
INTO
THE
SELECTED
SECOND
LEVEL
PAGE
TABLE
TO
FIND
THE
PAGE
FRAME
NUMBER
FOR
THE
PAGE
ITSELF
AS
AN
EXAMPLE
CONSIDER
THE
BIT
VIRTUAL
ADDRESS
DECIMAL
WHICH
IS
BYTES
INTO
THE
DATA
THIS
VIRTUAL
ADDRESS
CORRESPONDS
TO
MEMORY
MANAGEMENT
CHAP
PT
AND
OFFSET
THE
MMU
FIRST
USES
TO
INDEX
INTO
THE
TOP
LEVEL
PAGE
TABLE
AND
OBTAIN
ENTRY
WHICH
CORRESPONDS
TO
ADDRESSES
TO
IT
THEN
USES
TO
INDEX
INTO
THE
SECOND
LEVEL
PAGE
TABLE
JUST
FOUND
AND
EXTRACT
ENTRY
WHICH
CORRESPONDS
TO
ADDRESSES
TO
WITHIN
ITS
CHUNK
I
E
ABSOLUTE
ADDRESSES
TO
THIS
ENTRY
CONTAINS
THE
PAGE
FRAME
NUMBER
OF
THE
PAGE
CONTAINING
VIRTUAL
ADDRESS
IF
THAT
PAGE
IS
NOT
IN
MEMORY
THE
PRESENT
ABSENT
BIT
IN
THE
PAGE
TABLE
ENTRY
WILL
BE
ZERO
CAUSING
A
PAGE
FAULT
IF
THE
PAGE
IS
IN
MEMORY
THE
PAGE
FRAME
NUMBER
TAKEN
FROM
THE
SEC
OND
LEVEL
PAGE
TABLE
IS
COMBINED
WITH
THE
OFFSET
TO
CONSTRUCT
THE
PHYSICAL
AD
DRESS
THIS
ADDRESS
IS
PUT
ON
THE
BUS
AND
SENT
TO
MEMORY
THE
INTERESTING
THING
TO
NOTE
ABOUT
FIG
IS
THAT
ALTHOUGH
THE
ADDRESS
SPACE
CONTAINS
OVER
A
MILLION
PAGES
ONLY
FOUR
PAGE
TABLES
ARE
ACTUALLY
NEEDED
THE
TOP
LEVEL
TABLE
AND
THE
SECOND
LEVEL
TABLES
FOR
TO
FOR
THE
PROGRAM
TEXT
TO
FOR
THE
DATA
AND
THE
TOP
FOR
THE
STACK
THE
PRESENT
ABSENT
BITS
IN
ENTRIES
OF
THE
TOP
LEVEL
PAGE
TABLE
ARE
SET
TO
FORCING
A
PAGE
FAULT
IF
THEY
ARE
EVER
ACCESSED
SHOULD
THIS
OCCUR
THE
OPERATING
SYSTEM
WILL
NOTICE
THAT
THE
PROCESS
IS
TRYING
TO
REFERENCE
MEMORY
THAT
IT
IS
NOT
SUPPOSED
TO
AND
WILL
TAKE
APPROPRIATE
AC
TION
SUCH
AS
SENDING
IT
A
SIGNAL
OR
KILLING
IT
IN
THIS
EXAMPLE
WE
HAVE
CHOSEN
ROUND
NUMBERS
FOR
THE
VARIOUS
SIZES
AND
HAVE
PICKED
EQUAL
TO
BUT
IN
AC
TUAL
PRACTICE
OTHER
VALUES
ARE
ALSO
POSSIBLE
OF
COURSE
SEC
VIRTUAL
MEMORY
PAGE
TABLE
FOR
AN
ENTRY
N
P
FURTHERMORE
THIS
SEARCH
MUST
BE
DONE
ON
EVERY
MEMORY
REFERENCE
NOT
JUST
ON
PAGE
FAULTS
SEARCHING
A
TABLE
ON
EVERY
MEM
ORY
REFERENCE
IS
NOT
THE
WAY
TO
MAKE
YOUR
MACHINE
BLINDINGLY
FAST
THE
WAY
OUT
OF
THIS
DILEMMA
IS
TO
USE
THE
TLB
IF
THE
TLB
CAN
HOLD
ALL
OF
THE
HEAVILY
USED
PAGES
TRANSLATION
CAN
HAPPEN
JUST
AS
FAST
AS
WITH
REGULAR
PAGE
TABLES
ON
A
TLB
MISS
HOWEVER
THE
INVERTED
PAGE
TABLE
HAS
TO
BE
SEARCHED
IN
SOFTWARE
ONE
FEASIBLE
WAY
TO
ACCOMPLISH
THIS
SEARCH
IS
TO
HAVE
A
HASH
TABLE
HASHED
ON
THE
VIRTUAL
ADDRESS
ALL
THE
VIRTUAL
PAGES
CURRENTLY
IN
MEMORY
THAT
HAVE
THE
SAME
HASH
VALUE
ARE
CHAINED
TOGETHER
AS
SHOWN
IN
FIG
IF
THE
HASH
TABLE
HAS
AS
MANY
SLOTS
AS
THE
MACHINE
HAS
PHYSICAL
PAGES
THE
AVERAGE
CHAIN
WILL
BE
ONLY
ONE
ENTRY
LONG
GREATLY
SPEEDING
UP
THE
MAPPING
ONCE
THE
PAGE
FRAME
NUMBER
HAS
BEEN
FOUND
THE
NEW
VIRTUAL
PHYSICAL
PAIR
IS
ENTERED
INTO
THE
TLB
TRADITIONAL
PAGE
TABLE
WITH
AN
ENTRY
FOR
EACH
OF
THE
PAGES
GB
PHYSICAL
MEMORY
HAS
KB
PAGE
FRAMES
HASH
TABLE
THE
TWO
LEVEL
PAGE
TABLE
SYSTEM
OF
FIG
CAN
BE
EXPANDED
TO
THREE
FOUR
OR
MORE
LEVELS
ADDITIONAL
LEVELS
GIVE
MORE
FLEXIBILITY
BUT
IT
IS
DOUBTFUL
THAT
THE
ADDITIONAL
COMPLEXITY
IS
WORTH
IT
BEYOND
THREE
LEVELS
INVERTED
PAGE
TABLES
FOR
BIT
VIRTUAL
ADDRESS
SPACES
THE
MULTILEVEL
PAGE
TABLE
WORKS
REASONABLY
WELL
HOWEVER
AS
BIT
COMPUTERS
BECOME
MORE
COMMON
THE
SITUATION
CHANGES
DRASTICALLY
IF
THE
ADDRESS
SPACE
IS
NOW
BYTES
WITH
KB
PAGES
WE
NEED
A
PAGE
TABLE
WITH
ENTRIES
IF
EACH
ENTRY
IS
BYTES
THE
TABLE
IS
OVER
MILLION
GIGABYTES
PB
TYING
UP
MILLION
GIGABYTES
JUST
FOR
THE
PAGE
TABLE
IS
NOT
A
GOOD
IDEA
NOT
NOW
AND
PROBABLY
NOT
NEXT
YEAR
EITHER
CONSEQUENTLY
A
DIFFERENT
SOLUTION
IS
NEEDED
FOR
BIT
PAGED
VIRTUAL
ADDRESS
SPACES
ONE
SUCH
SOLUTION
IS
THE
INVERTED
PAGE
TABLE
IN
THIS
DESIGN
THERE
IS
ONE
ENTRY
PER
PAGE
FRAME
IN
REAL
MEMORY
RATHER
THAN
ONE
ENTRY
PER
PAGE
OF
VIRTUAL
AD
DRESS
SPACE
FOR
EXAMPLE
WITH
BIT
VIRTUAL
ADDRESSES
A
KB
PAGE
AND
GB
OF
RAM
AN
INVERTED
PAGE
TABLE
ONLY
REQUIRES
ENTRIES
THE
ENTRY
KEEPS
TRACK
OF
WHICH
PROCESS
VIRTUAL
PAGE
IS
LOCATED
IN
THE
PAGE
FRAME
ALTHOUGH
INVERTED
PAGE
TABLES
SAVE
VAST
AMOUNTS
OF
SPACE
AT
LEAST
WHEN
THE
VIRTUAL
ADDRESS
SPACE
IS
MUCH
LARGER
THAN
THE
PHYSICAL
MEMORY
THEY
HAVE
A
SERI
OUS
DOWNSIDE
VIRTUAL
TO
PHYSICAL
TRANSLATION
BECOMES
MUCH
HARDER
WHEN
PROCESS
N
REFERENCES
VIRTUAL
PAGE
P
THE
HARDWARE
CAN
NO
LONGER
FIND
THE
PHYSICAL
PAGE
BY
USING
P
AS
AN
INDEX
INTO
THE
PAGE
TABLE
INSTEAD
IT
MUST
SEARCH
THE
ENTIRE
INVERTED
INDEXED
INDEXED
BY
VIRTUAL
BY
HASH
ON
VIRTUAL
PAGE
PAGE
VIRTUAL
PAGE
PAGE
FRAME
FIGURE
COMPARISON
OF
A
TRADITIONAL
PAGE
TABLE
WITH
AN
INVERTED
PAGE
TABLE
INVERTED
PAGE
TABLES
ARE
COMMON
ON
BIT
MACHINES
BECAUSE
EVEN
WITH
A
VERY
LARGE
PAGE
SIZE
THE
NUMBER
OF
PAGE
TABLE
ENTRIES
IS
ENORMOUS
FOR
EXAMPLE
WITH
MB
PAGES
AND
BIT
VIRTUAL
ADDRESSES
PAGE
TABLE
ENTRIES
ARE
NEEDED
OTHER
APPROACHES
TO
HANDLING
LARGE
VIRTUAL
MEMORIES
CAN
BE
FOUND
IN
TALLURI
ET
AL
PAGE
REPLACEMENT
ALGORITHMS
WHEN
A
PAGE
FAULT
OCCURS
THE
OPERATING
SYSTEM
HAS
TO
CHOOSE
A
PAGE
TO
EVICT
REMOVE
FROM
MEMORY
TO
MAKE
ROOM
FOR
THE
INCOMING
PAGE
IF
THE
PAGE
TO
BE
RE
MOVED
HAS
BEEN
MODIFIED
WHILE
IN
MEMORY
IT
MUST
BE
REWRITTEN
TO
THE
DISK
TO
BRING
THE
DISK
COPY
UP
TO
DATE
IF
HOWEVER
THE
PAGE
HAS
NOT
BEEN
CHANGED
E
G
IT
MEMORY
MANAGEMEN
T
CHAP
CONTAINS
PROGRAM
TEXT
THE
DISK
COPY
IS
ALREADY
UP
TO
DATE
SO
NO
REWRITE
IS
NEED
ED
THE
PAGE
TO
BE
READ
IN
JUST
OVERWRITES
THE
PAGE
BEING
EVICTED
WHILE
IT
WOULD
BE
POSSIBLE
TO
PICK
A
RANDOM
PAGE
TO
EVICT
AT
EACH
PAGE
FAULT
SYSTEM
PERFORMANCE
IS
MUCH
BETTER
IF
A
PAGE
THAT
IS
NOT
HEAVILY
USED
IS
CHOSEN
IF
A
HEAVILY
USED
PAGE
IS
REMOVED
IT
WILL
PROBABLY
HAVE
TO
BE
BROUGHT
BACK
IN
QUICK
LY
RESULTING
IN
EXTRA
OVERHEAD
MUCH
WORK
HAS
BEEN
DONE
ON
THE
SUBJECT
OF
PAGE
REPLACEMENT
ALGORITHMS
BOTH
THEORETICAL
AND
EXPERIMENTAL
BELOW
WE
WILL
DESCRIBE
SOME
OF
THE
MOST
IMPORTANT
ALGORITHMS
IT
IS
WORTH
NOTING
THAT
THE
PROBLEM
OF
PAGE
REPLACEMENT
OCCURS
IN
OTHER
AREAS
OF
COMPUTER
DESIGN
AS
WELL
FOR
EXAMPLE
MOST
COMPUTERS
HAVE
ONE
OR
MORE
MEMORY
CACHES
CONSISTING
OF
RECENTLY
USED
BYTE
OR
BYTE
MEMORY
BLOCKS
WHEN
THE
CACHE
IS
FULL
SOME
BLOCK
HAS
TO
BE
CHOSEN
FOR
REMOVAL
THIS
PROBLEM
IS
PRECISELY
THE
SAME
AS
PAGE
REPLACEMENT
EXCEPT
ON
A
SHORTER
TIME
SCALE
IT
HAS
TO
BE
DONE
IN
A
FEW
NANOSECONDS
NOT
MILLISECONDS
AS
WITH
PAGE
REPLACEMENT
THE
REA
SON
FOR
THE
SHORTER
TIME
SCALE
IS
THAT
CACHE
BLOCK
MISSES
ARE
SATISFIED
FROM
MAIN
MEMORY
WHICH
HAS
NO
SEEK
TIME
AND
NO
ROTATIONAL
LATENCY
A
SECOND
EXAMPLE
IS
IN
A
WEB
SERVER
THE
SERVER
CAN
KEEP
A
CERTAIN
NUMBER
OF
HEAVILY
USED
WEB
PAGES
IN
ITS
MEMORY
CACHE
HOWEVER
WHEN
THE
MEMORY
CACHE
IS
FULL
AND
A
NEW
PAGE
IS
REFERENCED
A
DECISION
HAS
TO
BE
MADE
WHICH
WEB
PAGE
TO
EVICT
THE
CONSIDERATIONS
ARE
SIMILAR
TO
PAGES
OF
VIRTUAL
MEMORY
EXCEPT
FOR
THE
FACT
THAT
THE
WEB
PAGES
ARE
NEVER
MODIFIED
IN
THE
CACHE
SO
THERE
IS
ALWAYS
A
FRESH
COPY
ON
DISK
IN
A
VIRTUAL
MEMORY
SYSTEM
PAGES
IN
MAIN
MEMORY
MAY
BE
EITHER
CLEAN
OR
DIRTY
IN
ALL
THE
PAGE
REPLACEMENT
ALGORITHMS
TO
BE
STUDIED
BELOW
A
CERTAIN
ISSUE
ARISES
WHEN
A
PAGE
IS
TO
BE
EVICTED
FROM
MEMORY
DOES
IT
HAVE
TO
BE
ONE
OF
THE
FAULTING
PROCESS
OWN
PAGES
OR
CAN
IT
BE
A
PAGE
BELONGING
TO
ANOTHER
PROCESS
IN
THE
FORMER
CASE
WE
ARE
EFFECTIVELY
LIMITING
EACH
PROCESS
TO
A
FIXED
NUMBER
OF
PAGES
IN
THE
LATTER
CASE
WE
ARE
NOT
BOTH
ARE
POSSIBILITIES
WE
WILL
COME
BACK
TO
THIS
POINT
IN
SEC
THE
OPTIMAL
PAGE
REPLACEMENT
ALGORITHM
THE
BEST
POSSIBLE
PAGE
REPLACEMENT
ALGORITHM
IS
EASY
TO
DESCRIBE
BUT
IMPOSSI
BLE
TO
IMPLEMENT
IT
GOES
LIKE
THIS
AT
THE
MOMENT
THAT
A
PAGE
FAULT
OCCURS
SOME
SET
OF
PAGES
IS
IN
MEMORY
ONE
OF
THESE
PAGES
WILL
BE
REFERENCED
ON
THE
VERY
NEXT
INSTRUCTION
THE
PAGE
CONTAINING
THAT
INSTRUCTION
OTHER
PAGES
MAY
NOT
BE
REFER
ENCED
UNTIL
OR
PERHAPS
INSTRUCTIONS
LATER
EACH
PAGE
CAN
BE
LABELED
WITH
THE
NUMBER
OF
INSTRUCTIONS
THAT
WILL
BE
EXECUTED
BEFORE
THAT
PAGE
IS
FIRST
REF
ERENCED
THE
OPTIMAL
PAGE
REPLACEMENT
ALGORITHM
SAYS
THAT
THE
PAGE
WITH
THE
HIGHEST
LABEL
SHOULD
BE
REMOVED
IF
ONE
PAGE
WILL
NOT
BE
USED
FOR
MILLION
INSTRUCTIONS
AND
ANOTHER
PAGE
WILL
NOT
BE
USED
FOR
MILLION
INSTRUCTIONS
REMOVING
THE
FORMER
SEC
PAGE
REPLACEMENT
ALGORITHMS
PUSHES
THE
PAGE
FAULT
THAT
WILL
FETCH
IT
BACK
AS
FAR
INTO
THE
FUTURE
AS
POSSIBLE
COM
PUTERS
LIKE
PEOPLE
TRY
TO
PUT
OFF
UNPLEASANT
EVENTS
FOR
AS
LONG
AS
THEY
CAN
THE
ONLY
PROBLEM
WITH
THIS
ALGORITHM
IS
THAT
IT
IS
UNREALIZABLE
AT
THE
TIME
OF
THE
PAGE
FAULT
THE
OPERATING
SYSTEM
HAS
NO
WAY
OF
KNOWING
WHEN
EACH
OF
THE
PAGES
WILL
BE
REFERENCED
NEXT
WE
SAW
A
SIMILAR
SITUATION
EARLIER
WITH
THE
SHOR
TEST
JOB
FIRST
SCHEDULING
ALGORITHM
HOW
CAN
THE
SYSTEM
TELL
WHICH
JOB
IS
SHOR
TEST
STILL
BY
RUNNING
A
PROGRAM
ON
A
SIMULATOR
AND
KEEPING
TRACK
OF
ALL
PAGE
REF
ERENCES
IT
IS
POSSIBLE
TO
IMPLEMENT
OPTIMAL
PAGE
REPLACEMENT
ON
THE
SECOND
RUN
BY
USING
THE
PAGE
REFERENCE
INFORMATION
COLLECTED
DURING
THE
FIRST
RUN
IN
THIS
WAY
IT
IS
POSSIBLE
TO
COMPARE
THE
PERFORMANCE
OF
REALIZABLE
ALGORITHMS
WITH
THE
BEST
POSSIBLE
ONE
IF
AN
OPERATING
SYSTEM
ACHIEVES
A
PERFORMANCE
OF
SAY
ONLY
WORSE
THAN
THE
OPTIMAL
ALGORITHM
EFFORT
SPENT
IN
LOOKING
FOR
A
BETTER
AL
GORITHM
WILL
YIELD
AT
MOST
A
IMPROVEMENT
TO
AVOID
ANY
POSSIBLE
CONFUSION
IT
SHOULD
BE
MADE
CLEAR
THAT
THIS
LOG
OF
PAGE
REFERENCES
REFERS
ONLY
TO
THE
ONE
PROGRAM
JUST
MEASURED
AND
THEN
WITH
ONLY
ONE
SPECIFIC
INPUT
THE
PAGE
REPLACEMENT
ALGORITHM
DERIVED
FROM
IT
IS
THUS
SPECIFIC
TO
THAT
ONE
PROGRAM
AND
INPUT
DATA
ALTHOUGH
THIS
METHOD
IS
USEFUL
FOR
EVALUATING
PAGE
REPLACEMENT
ALGORITHMS
IT
IS
OF
NO
USE
IN
PRACTICAL
SYSTEMS
BELOW
WE
WILL
STUDY
ALGORITHMS
THAT
ARE
USEFUL
ON
REAL
SYSTEMS
THE
NOT
RECENTLY
USED
PAGE
REPLACEMENT
ALGORITHM
IN
ORDER
TO
ALLOW
THE
OPERATING
SYSTEM
TO
COLLECT
USEFUL
PAGE
USAGE
STATISTICS
MOST
COMPUTERS
WITH
VIRTUAL
MEMORY
HAVE
TWO
STATUS
BITS
ASSOCIATED
WITH
EACH
PAGE
R
IS
SET
WHENEVER
THE
PAGE
IS
REFERENCED
READ
OR
WRITTEN
M
IS
SET
WHEN
THE
PAGE
IS
WRITTEN
TO
I
E
MODIFIED
THE
BITS
ARE
CONTAINED
IN
EACH
PAGE
TABLE
ENTRY
AS
SHOWN
IN
FIG
IT
IS
IMPORTANT
TO
REALIZE
THAT
THESE
BITS
MUST
BE
UPDATED
ON
EVERY
MEMORY
REFERENCE
SO
IT
IS
ESSENTIAL
THAT
THEY
BE
SET
BY
THE
HARD
WARE
ONCE
A
BIT
HAS
BEEN
SET
TO
IT
STAYS
UNTIL
THE
OPERATING
SYSTEM
RESETS
IT
IF
THE
HARDWARE
DOES
NOT
HAVE
THESE
BITS
THEY
CAN
BE
SIMULATED
AS
FOLLOWS
WHEN
A
PROCESS
IS
STARTED
UP
ALL
OF
ITS
PAGE
TABLE
ENTRIES
ARE
MARKED
AS
NOT
IN
MEMORY
AS
SOON
AS
ANY
PAGE
IS
REFERENCED
A
PAGE
FAULT
WILL
OCCUR
THE
OPERAT
ING
SYSTEM
THEN
SETS
THE
R
BIT
IN
ITS
INTERNAL
TABLES
CHANGES
THE
PAGE
TABLE
ENTRY
TO
POINT
TO
THE
CORRECT
PAGE
WITH
MODE
READ
ONLY
AND
RESTARTS
THE
INSTRUCTION
IF
THE
PAGE
IS
SUBSEQUENTLY
MODIFIED
ANOTHER
PAGE
FAULT
WILL
OCCUR
ALLOWING
THE
OPERATING
SYSTEM
TO
SET
THE
M
BIT
AND
CHANGE
THE
PAGE
MODE
TO
READAVRITE
THE
R
AND
M
BITS
CAN
BE
USED
TO
BUILD
A
SIMPLE
PAGING
ALGORITHM
AS
FOLLOWS
WHEN
A
PROCESS
IS
STARTED
UP
BOTH
PAGE
BITS
FOR
ALL
ITS
PAGES
ARE
SET
TO
BY
THE
OP
ERATING
SYSTEM
PERIODICALLY
E
G
ON
EACH
CLOCK
INTERRUPT
THE
R
BIT
IS
CLEARED
TO
DISTINGUISH
PAGES
THAT
HAVE
NOT
BEEN
REFERENCED
RECENTLY
FROM
THOSE
THAT
HAVE
BEEN
WHEN
A
PAGE
FAULT
OCCURS
THE
OPERATING
SYSTEM
INSPECTS
ALL
THE
PAGES
AND
DIVIDES
THEM
INTO
CATEGORIES
BASED
ON
THE
CURRENT
VALUES
OF
THEIR
R
AND
M
BITS
MEMORY
MANAGEMENT
CHAP
CLASS
NOT
REFERENCED
NOT
MODIFIED
CLASS
NOT
REFERENCED
MODIFIED
CLASS
REFERENCED
NOT
MODIFIED
CLASS
REFERENCED
MODIFIED
SEC
PAGE
REPLACEMENT
ALGORITHMS
THE
OPERATION
OF
THIS
ALGORITHM
CALLED
SECOND
CHANCE
IS
SHOWN
IN
FIG
IN
FIG
A
WE
SEE
PAGES
A
THROUGH
H
KEPT
ON
A
LINKED
LIST
AND
SORTED
BY
THE
TIME
THEY
ARRIVED
IN
MEMORY
PAGE
LOADED
FIRST
ALTHOUGH
CLASS
PAGES
SEEM
AT
FIRST
GLANCE
IMPOSSIBLE
THEY
OCCUR
WHEN
A
CLASS
PAGE
HAS
ITS
R
BIT
CLEARED
BY
A
CLOCK
INTERRUPT
CLOCK
INTERRUPTS
DO
NOT
CLEAR
THE
M
BIT
BECAUSE
THIS
INFORMATION
IS
NEEDED
TO
KNOW
WHETHER
THE
PAGE
HAS
TO
BE
REWRITTEN
TO
DISK
OR
NOT
CLEARING
R
BUT
NOT
M
LEADS
TO
A
CLASS
PAGE
THE
NRU
NOT
RECENTLY
USED
ALGORITHM
REMOVES
A
PAGE
AT
RANDOM
FROM
THE
LOWEST
NUMBERED
NONEMPTY
CLASS
IMPLICIT
IN
THIS
ALGORITHM
IS
THE
IDEA
THAT
IT
IS
BETTER
TO
REMOVE
A
MODIFIED
PAGE
THAT
HAS
NOT
BEEN
REFERENCED
IN
AT
LEAST
ONE
CLOCK
TICK
TYPICALLY
ABOUT
MSEC
THAN
A
CLEAN
PAGE
THAT
IS
IN
HEAVY
USE
THE
MAIN
ATTRACTION
OF
NRU
IS
THAT
IT
IS
EASY
TO
UNDERSTAND
MODERATELY
EFFICIENT
TO
IMPLE
MENT
AND
GIVES
A
PERFORMANCE
THAT
WHILE
CERTAINLY
NOT
OPTIMAL
MAY
BE
ADEQUATE
H
C
H
D
A
B
MOST
RECENTLY
LOADED
PAGE
A
IS
TREATED
LIKE
A
NEWLY
LOADED
PAGE
THE
FIRST
IN
FIRST
OUT
FIFO
PAGE
REPLACEMENT
ALGORITHM
ANOTHER
LOW
OVERHEAD
PAGING
ALGORITHM
IS
THE
FIFO
FIRST
IN
FIRST
OUT
AL
GORITHM
TO
ILLUSTRATE
HOW
THIS
WORKS
CONSIDER
A
SUPERMARKET
THAT
HAS
ENOUGH
SHELVES
TO
DISPLAY
EXACTLY
K
DIFFERENT
PRODUCTS
ONE
DAY
SOME
COMPANY
INTRO
DUCES
A
NEW
CONVENIENCE
FOOD
INSTANT
FREEZE
DRIED
ORGANIC
YOGURT
THAT
CAN
BE
RECONSTITUTED
IN
A
MICROWAVE
OVEN
IT
IS
AN
IMMEDIATE
SUCCESS
SO
OUR
FINITE
SU
PERMARKET
HAS
TO
GET
RID
OF
ONE
OLD
PRODUCT
IN
ORDER
TO
STOCK
IT
ONE
POSSIBILITY
IS
TO
FIND
THE
PRODUCT
THAT
THE
SUPERMARKET
HAS
BEEN
STOCKING
THE
LONGEST
I
E
SOMETHING
IT
BEGAN
SELLING
YEARS
AGO
AND
GET
RID
OF
IT
ON
THE
GROUNDS
THAT
NO
ONE
IS
INTERESTED
ANY
MORE
IN
EFFECT
THE
SUPERMARKET
MAINTAINS
A
LINKED
LIST
OF
ALL
THE
PRODUCTS
IT
CURRENTLY
SELLS
IN
THE
ORDER
THEY
WERE
INTRODUCED
THE
NEW
ONE
GOES
ON
THE
BACK
OF
THE
LIST
THE
ONE
AT
THE
FRONT
OF
THE
LIST
IS
DROPPED
AS
A
PAGE
REPLACEMENT
ALGORITHM
THE
SAME
IDEA
IS
APPLICABLE
THE
OPERATING
SYSTEM
MAINTAINS
A
LIST
OF
ALL
PAGES
CURRENTLY
IN
MEMORY
WITH
THE
MOST
RECENT
ARRIVAL
AT
THE
TAIL
AND
THE
LEAST
RECENT
ARRIVAL
AT
THE
HEAD
ON
A
PAGE
FAULT
THE
PAGE
AT
THE
HEAD
IS
REMOVED
AND
THE
NEW
PAGE
ADDED
TO
THE
TAIL
OF
THE
LIST
WHEN
APPLIED
TO
STORES
FIFO
MIGHT
REMOVE
MUSTACHE
WAX
BUT
IT
MIGHT
ALSO
REMOVE
FLOUR
SALT
OR
BUTTER
WHEN
APPLIED
TO
COMPUTERS
THE
SAME
PROBLEM
ARISES
FOR
THIS
REASON
FIFO
IN
ITS
PURE
FORM
IS
RARELY
USED
THE
SECOND
CHANCE
PAGE
REPLACEMENT
ALGORITHM
A
SIMPLE
MODIFICATION
TO
FIFO
THAT
AVOIDS
THE
PROBLEM
OF
THROWING
OUT
A
HEAVILY
USED
PAGE
IS
TO
INSPECT
THE
R
BIT
OF
THE
OLDEST
PAGE
IF
IT
IS
THE
PAGE
IS
BOTH
OLD
AND
UNUSED
SO
IT
IS
REPLACED
IMMEDIATELY
IF
THE
R
BIT
IS
THE
BIT
IS
CLEARED
THE
PAGE
IS
PUT
ONTO
THE
END
OF
THE
LIST
OF
PAGES
AND
ITS
LOAD
TIME
IS
UPDATED
AS
THOUGH
IT
HAD
JUST
ARRIVED
IN
MEMORY
THEN
THE
SEARCH
CONTINUES
FIGURE
OPERATION
OF
SECOND
CHANCE
A
PAGES
SORTED
IN
FIFO
ORDER
B
PAGE
LIST
IF
A
PAGE
FAULT
OCCURS
AT
TIME
AND
A
HAS
ITS
R
BIT
SET
THE
NUMBERS
ABOVE
THE
PAGES
ARE
THEIR
LOAD
TIMES
SUPPOSE
THAT
A
PAGE
FAULT
OCCURS
AT
TIME
THE
OLDEST
PAGE
IS
A
WHICH
ARRIVED
AT
TIME
WHEN
THE
PROCESS
STARTED
IF
A
HAS
THE
R
BIT
CLEARED
IT
IS
EVICTED
FROM
MEMORY
EITHER
BY
BEING
WRITTEN
TO
THE
DISK
IF
IT
IS
DIRTY
OR
JUST
ABANDONED
IF
IT
IS
CLEAN
ON
THE
OTHER
HAND
IF
THE
R
BIT
IS
SET
A
IS
PUT
ONTO
THE
END
OF
THE
LIST
AND
ITS
LOAD
TIME
IS
RESET
TO
THE
CURRENT
TIME
THE
R
BIT
IS
ALSO
CLEARED
THE
SEARCH
FOR
A
SUITABLE
PAGE
CONTINUES
WITH
B
WHAT
SECOND
CHANCE
IS
LOOKING
FOR
IS
AN
OLD
PAGE
THAT
HAS
NOT
BEEN
REFERENCED
IN
THE
MOST
RECENT
CLOCK
INTERVAL
IF
ALL
THE
PAGES
HAVE
BEEN
REFERENCED
SECOND
CHANCE
DEGENERATES
INTO
PURE
FIFO
SPECIFICALLY
IMAGINE
THAT
ALL
THE
PAGES
IN
FIG
A
HAVE
THEIR
R
BITS
SET
ONE
BY
ONE
THE
OPERATING
SYSTEM
MOVES
THE
PAGES
TO
THE
END
OF
THE
LIST
CLEARING
THE
R
BIT
EACH
TIME
IT
APPENDS
A
PAGE
TO
THE
END
OF
THE
LIST
EVENTUALLY
IT
COMES
BACK
TO
PAGE
A
WHICH
NOW
HAS
ITS
R
BIT
CLEARED
AT
THIS
POINT
A
IS
EVICTED
THUS
THE
ALGORITHM
ALWAYS
TERMINATES
THE
CLOCK
PAGE
REPLACEMENT
ALGORITHM
ALTHOUGH
SECOND
CHANCE
IS
A
REASONABLE
ALGORITHM
IT
IS
UNNECESSARILY
INEFFI
CIENT
BECAUSE
IT
IS
CONSTANTLY
MOVING
PAGES
AROUND
ON
ITS
LIST
A
BETTER
APPROACH
IS
TO
KEEP
ALL
THE
PAGE
FRAMES
ON
A
CIRCULAR
LIST
IN
THE
FORM
OF
A
CLOCK
AS
SHOWN
IN
FIG
THE
HAND
POINTS
TO
THE
OLDEST
PAGE
WHEN
A
PAGE
FAULT
OCCURS
THE
PAGE
BEING
POINTED
TO
BY
THE
HAND
IS
INSPECTED
IF
ITS
R
BIT
IS
THE
PAGE
IS
EVICTED
THE
NEW
PAGE
IS
INSERTED
INTO
THE
CLOCK
IN
ITS
PLACE
AND
THE
HAND
IS
ADVANCED
ONE
POSITION
IF
R
IS
IT
IS
CLEARED
AND
THE
HAND
IS
ADVANCED
TO
THE
NEXT
PAGE
THIS
PROCESS
IS
REPEATED
UNTIL
A
PAGE
IS
FOUND
WITH
R
NOT
SURPRISINGLY
THIS
ALGORITHM
IS
CALLED
CLOCK
MEMORY
MANAGEMEN
T
CHAP
B
WHE
N
A
PAG
E
FAULT
OCCURS
TH
E
PAG
E
THE
HAN
D
I
POINTING
TO
IS
INSPECTED
TH
E
ACTIO
N
TAKE
N
DEPEND
O
N
THE
R
BIT
R
M
EVICT
TH
E
PAG
E
R
DEA
R
R
AN
D
ADVANC
E
HAN
D
FIGURE
THE
CLOCK
PAGE
REPLACEMENT
ALGORITHM
THE
LEAST
RECENTLY
USED
LRU
PAGE
REPLACEMENT
ALGORITHM
A
GOOD
APPROXIMATION
TO
THE
OPTIMAL
ALGORITHM
IS
BASED
ON
THE
OBSERVATION
THAT
PAGES
THAT
HAVE
BEEN
HEAVILY
USED
IN
THE
LAST
FEW
INSTRUCTIONS
WILL
PROBABLY
BE
HEAVILY
USED
AGAIN
IN
THE
NEXT
FEW
CONVERSELY
PAGES
THAT
HAVE
NOT
BEEN
USED
FOR
AGES
WILL
PROBABLY
REMAIN
UNUSED
FOR
A
LONG
TIME
THIS
IDEA
SUGGESTS
A
REALIZABLE
SEC
PAGE
REPLACEMENT
ALGORITHMS
NEXT
LOWEST
IS
NEXT
LEAST
RECENTLY
USED
AND
SO
FORTH
THE
WORKINGS
OF
THIS
ALGO
RITHM
ARE
GIVEN
IN
FIG
FOR
FOUR
PAGE
FRAMES
AND
PAGE
REFERENCES
IN
THE
ORDER
AFTER
PAGE
IS
REFERENCED
WE
HAVE
THE
SITUATION
OF
FIG
A
AFTER
PAGE
IS
REFERENCED
WE
HAVE
THE
SITUATION
OF
FIG
B
AND
SO
FORTH
PAG
E
A
B
C
ALGORITHM
WHEN
A
PAGE
FAULT
OCCURS
THROW
OUT
THE
PAGE
THAT
HAS
BEEN
UNUSED
FOR
F
H
I
I
THE
LONGEST
TIME
THIS
STRATEGY
IS
CALLED
LRU
LEAST
RECENTLY
USED
PAGING
ALTHOUGH
LRU
IS
THEORETICALLY
REALIZABLE
IT
IS
NOT
CHEAP
TO
FULLY
IMPLEMENT
LRU
IT
IS
NECESSARY
TO
MAINTAIN
A
LINKED
LIST
OF
ALL
PAGES
IN
MEMORY
WITH
THE
MOST
RECENTLY
USED
PAGE
AT
THE
FRONT
AND
THE
LEAST
RECENTLY
USED
PAGE
AT
THE
REAR
THE
DIFFICULTY
IS
THAT
THE
LIST
MUST
BE
UPDATED
ON
EVERY
MEMORY
REFERENCE
FIND
ING
A
PAGE
IN
THE
LIST
DELETING
IT
AND
THEN
MOVING
IT
TO
THE
FRONT
IS
A
VERY
TIME
CON
SUMING
OPERATION
EVEN
IN
HARDWARE
ASSUMING
THAT
SUCH
HARDWARE
COULD
BE
BUILT
HOWEVER
THERE
ARE
OTHER
WAYS
TO
IMPLEMENT
LRU
WITH
SPECIAL
HARDWARE
LET
US
CONSIDER
THE
SIMPLEST
WAY
FIRST
THIS
METHOD
REQUIRES
EQUIPPING
THE
HARDWARE
WITH
A
BIT
COUNTER
C
THAT
IS
AUTOMATICALLY
INCREMENTED
AFTER
EACH
INSTRUCTION
FURTHERMORE
EACH
PAGE
TABLE
ENTRY
MUST
ALSO
HAVE
A
FIELD
LARGE
ENOUGH
TO
CONTAIN
THE
COUNTER
AFTER
EACH
MEMORY
REFERENCE
THE
CURRENT
VALUE
OF
C
IS
STORED
IN
THE
PAGE
TABLE
ENTRY
FOR
THE
PAGE
JUST
REFERENCED
WHEN
A
PAGE
FAULT
OCCURS
THE
OPER
ATING
SYSTEM
EXAMINES
ALL
THE
COUNTERS
IN
THE
PAGE
TABLE
TO
FIND
THE
LOWEST
ONE
THAT
PAGE
IS
THE
LEAST
RECENTLY
USED
NOW
LET
US
LOOK
AT
A
SECOND
HARDWARE
LRU
ALGORITHM
FOR
A
MACHINE
WITH
N
PAGE
FRAMES
THE
LRU
HARDWARE
CAN
MAINTAIN
A
MATRIX
OF
N
X
N
BITS
INITIALLY
ALL
ZERO
WHENEVER
PAGE
FRAME
K
IS
REFERENCED
THE
HARDWARE
FIRST
SETS
ALL
THE
BITS
OF
ROW
K
TO
THEN
SETS
ALL
THE
BITS
OF
COLUMN
K
TO
AT
ANY
INSTANT
OF
TIME
THE
ROW
WHOSE
BINARY
VALUE
IS
LOWEST
IS
THE
LEAST
RECENTLY
USED
THE
ROW
WHOSE
VALUE
IS
FIGURE
LRU
USING
A
MATRIX
WHEN
PAGES
ARE
REFERENCED
IN
THE
ORDER
SIMULATING
LRU
IN
SOFTWARE
ALTHOUGH
BOTH
OF
THE
PREVIOUS
LRU
ALGORITHMS
ARE
IN
PRINCIPLE
REALIZABLE
FEW
IF
ANY
MACHINES
HAVE
THE
REQUIRED
HARDWARE
INSTEAD
A
SOLUTION
THAT
CAN
BE
IMPLEMENTED
IN
SOFTWARE
IS
NEEDED
ONE
POSSIBILITY
IS
CALLED
THE
NFU
NOT
FRE
QUENTLY
USED
ALGORITHM
IT
REQUIRES
A
SOFTWARE
COUNTER
ASSOCIATED
WITH
EACH
PAGE
INITIALLY
ZERO
AT
EACH
CLOCK
INTERRUPT
THE
OPERATING
SYSTEM
SCANS
ALL
THE
PAGES
IN
MEMORY
FOR
EACH
PAGE
THE
R
BIT
WHICH
IS
OR
IS
ADDED
TO
THE
COUNT
ER
THE
COUNTERS
ROUGHLY
KEEP
TRACK
OF
HOW
OFTEN
EACH
PAGE
HAS
BEEN
REFERENCED
WHEN
A
PAGE
FAULT
OCCURS
THE
PAGE
WITH
THE
LOWEST
COUNTER
IS
CHOSEN
FOR
REPLACE
MENT
THE
MAIN
PROBLEM
WITH
NFU
IS
THAT
IT
NEVER
FORGETS
ANYTHING
FOR
EXAMPLE
IN
A
MULTIPASS
COMPILER
PAGES
THAT
WERE
HEAVILY
USED
DURING
PASS
MAY
STILL
HAVE
A
HIGH
COUNT
WELL
INTO
LATER
PASSES
IN
FACT
IF
PASS
HAPPENS
TO
HAVE
THE
LONGEST
EX
ECUTION
TIME
OF
ALL
THE
PASSES
THE
PAGES
CONTAINING
THE
CODE
FOR
SUBSEQUENT
PASSES
MEMORY
MANAGEMENT
CHAP
MAY
ALWAYS
HAVE
LOWER
COUNTS
THAN
THE
PASS
PAGES
CONSEQUENTLY
THE
OPERATING
SYSTEM
WILL
REMOVE
USEFUL
PAGES
INSTEAD
OF
PAGES
NO
LONGER
IN
USE
FORTUNATELY
A
SMALL
MODIFICATION
TO
NFU
MAKES
IT
ABLE
TO
SIMULATE
LRU
QUITE
WELL
THE
MODIFICATION
HAS
TWO
PARTS
FIRST
THE
COUNTERS
ARE
EACH
SHIFTED
RIGHT
BIT
BEFORE
THE
R
BIT
IS
ADDED
IN
SECOND
THE
R
BIT
IS
ADDED
TO
THE
LEFTMOST
RATHER
THAN
THE
RIGHTMOST
BIT
FIGURE
ILLUSTRATES
HOW
THE
MODIFIED
ALGORITHM
KNOWN
AS
AGING
WORKS
SUPPOSE
THAT
AFTER
THE
FIRST
CLOCK
TICK
THE
R
BITS
FOR
PAGES
TO
HAVE
THE
VALUES
AND
RESPECTIVELY
PAGE
IS
PAGE
IS
PAGE
IS
ETC
IN
OTHER
WORDS
BETWEEN
TICK
AND
TICK
PAGES
AND
WERE
REFERENCED
SETTING
THEIR
R
BITS
TO
WHILE
THE
OTHER
ONES
REMAIN
AFTER
THE
SIX
CORRESPONDING
COUNT
ERS
HAVE
BEEN
SHIFTED
AND
THE
R
BIT
INSERTED
AT
THE
LEFT
THEY
HAVE
THE
VALUES
SHOWN
IN
FIG
A
THE
FOUR
REMAINING
COLUMNS
SHOW
THE
SIX
COUNTERS
AFTER
THE
NEXT
FOUR
CLOCK
TICKS
SEC
PAGE
REPLACEMENT
ALGORITHMS
SHOULD
CHOOSE
ONE
OF
THESE
TWO
THE
TROUBLE
IS
WE
DO
NOT
KNOW
WHICH
OF
THEM
WAS
REFERENCED
LAST
IN
THE
INTERVAL
BETWEEN
TICK
AND
TICK
BY
RECORDING
ONLY
ONE
BIT
PER
TIME
INTERVAL
WE
HAVE
LOST
THE
ABILITY
TO
DISTINGUISH
REFERENCES
EARLY
IN
THE
CLOCK
INTERVAL
FROM
THOSE
OCCURRING
LATER
ALL
WE
CAN
DO
IS
REMOVE
PAGE
BE
CAUSE
PAGE
WAS
ALSO
REFERENCED
TWO
TICKS
EARLIER
AND
PAGE
WAS
NOT
THE
SECOND
DIFFERENCE
BETWEEN
LRU
AND
AGING
IS
THAT
IN
AGING
THE
COUNTERS
HAVE
A
FINITE
NUMBER
OF
BITS
BITS
IN
THIS
EXAMPLE
WHICH
LIMITS
ITS
PAST
HORIZON
SUPPOSE
THAT
TWO
PAGES
EACH
HAVE
A
COUNTER
VALUE
OF
ALL
WE
CAN
DO
IS
PICK
ONE
OF
THEM
AT
RANDOM
IN
REALITY
IT
MAY
WELL
BE
THAT
ONE
OF
THE
PAGES
WAS
LAST
REFER
ENCED
NINE
TICKS
AGO
AND
THE
OTHER
WAS
LAST
REFERENCED
TICKS
AGO
WE
HAVE
NO
WAY
OF
SEEING
THAT
IN
PRACTICE
HOWEVER
BITS
IS
GENERALLY
ENOUGH
IF
A
CLOCK
TICK
IS
AROUND
MSEC
IF
A
PAGE
HAS
NOT
BEEN
REFERENCED
IN
MSEC
IT
PROBABLY
IS
NOT
THAT
IMPORTANT
TH
E
WORKIN
G
SET
PAG
E
REPLACEMEN
T
ALGORITH
M
PAGE
R
BITS
FOR
PAGES
CLOCK
TICK
R
BITS
FOR
PAGES
CLOCK
TICK
J
IN
THE
PUREST
FORM
OF
PAGING
PROCESSES
ARE
STARTED
UP
WITH
NONE
OF
THEIR
PAGES
IN
MEMORY
AS
SOON
AS
THE
CPU
TRIES
TO
FETCH
THE
FIRST
INSTRUCTION
IT
GETS
A
PAGE
FAULT
CAUSING
THE
OPERATING
SYSTEM
TO
BRING
IN
THE
PAGE
CONTAINING
THE
FIRST
INSTRUC
TION
OTHER
PAGE
FAULTS
FOR
GLOBAL
VARIABLES
AND
THE
STACK
USUALLY
FOLLOW
QUICKLY
AFTER
A
WHILE
THE
PROCESS
HAS
MOST
OF
THE
PAGES
IT
NEEDS
AND
SETDES
DOWN
TO
RUN
A
B
C
D
E
WITH
RELATIVELY
FEW
PAGE
FAULTS
THIS
STRATEGY
IS
CALLED
DEMAND
PAGING
BECAUSE
PAGES
ARE
LOADED
ONLY
ON
DEMAND
NOT
IN
ADVANCE
OF
COURSE
IT
IS
EASY
ENOUGH
TO
WRITE
A
TEST
PROGRAM
THAT
SYSTEMATICALLY
READS
ALL
THE
PAGES
IN
A
LARGE
ADDRESS
SPACE
CAUSING
SO
MANY
PAGE
FAULTS
THAT
THERE
IS
NOT
ENOUGH
MEMORY
TO
HOLD
THEM
ALL
FORTUNATELY
MOST
PROCESSES
DO
NOT
WORK
THIS
WAY
THEY
EXHIBIT
A
LOCALITY
OF
REFERENCE
MEANING
THAT
DURING
ANY
PHASE
OF
EX
ECUTION
THE
PROCESS
REFERENCES
ONLY
A
RELATIVELY
SMALL
FRACTION
OF
ITS
PAGES
EACH
PASS
OF
A
MULTIPASS
COMPILER
FOR
EXAMPLE
REFERENCES
ONLY
A
FRACTION
OF
ALL
THE
PAGES
AND
A
DIFFERENT
FRACTION
AT
THAT
THE
SET
OF
PAGES
THAT
A
PROCESS
IS
CURRENDY
USING
IS
KNOWN
AS
ITS
WORKING
SET
DENNING
DENNING
IF
THE
ENTIRE
WORKING
SET
IS
IN
MEMORY
THE
PROCESS
WILL
RUN
WITHOUT
CAUSING
MANY
FAULTS
UNTIL
IT
MOVES
INTO
ANOTHER
EXECUTION
FIGURE
THE
AGING
ALGORITHM
SIMULATES
LRU
IN
SOFTWARE
SHOWN
ARE
SIX
PAGES
FOR
FIVE
CLOCK
TICKS
THE
FIVE
CLOCK
TICKS
ARE
REPRESENTED
BY
A
TO
E
WHEN
A
PAGE
FAULT
OCCURS
THE
PAGE
WHOSE
COUNTER
IS
THE
LOWEST
IS
REMOVED
IT
IS
CLEAR
THAT
A
PAGE
THAT
HAS
NOT
BEEN
REFERENCED
FOR
SAY
FOUR
CLOCK
TICKS
WILL
HAVE
FOUR
LEADING
ZEROS
IN
ITS
COUNTER
AND
THUS
WILL
HAVE
A
LOWER
VALUE
THAN
A
COUNTER
THAT
HAS
NOT
BEEN
REFERENCED
FOR
THREE
CLOCK
TICKS
THIS
ALGORITHM
DIFFERS
FROM
LRU
IN
TWO
WAYS
CONSIDER
PAGES
AND
IN
FIG
E
NEITHER
HAS
BEEN
REFERENCED
FOR
TWO
CLOCK
TICKS
BOTH
WERE
REFER
ENCED
IN
THE
TICK
PRIOR
TO
THAT
ACCORDING
TO
LRU
IF
A
PAGE
MUST
BE
REPLACED
WE
PHASE
E
G
THE
NEXT
PASS
OF
THE
COMPILER
IF
THE
AVAILABLE
MEMORY
IS
TOO
SMALL
TO
HOLD
THE
ENTIRE
WORKING
SET
THE
PROCESS
WILL
CAUSE
MANY
PAGE
FAULTS
AND
RUN
SLOWLY
SINCE
EXECUTING
AN
INSTRUCTION
TAKES
A
FEW
NANOSECONDS
AND
READING
IN
A
PAGE
FROM
THE
DISK
TYPICALLY
TAKES
MILLISECONDS
AT
A
RATE
OF
ONE
OR
TWO
IN
STRUCTIONS
PER
MILLISECONDS
IT
WILL
TAKE
AGES
TO
FINISH
A
PROGRAM
CAUSING
PAGE
FAULTS
EVERY
FEW
INSTRUCTIONS
IS
SAID
TO
BE
THRASHING
DENNING
IN
A
MULTIPROGRAMMING
SYSTEM
PROCESSES
ARE
FREQUENTLY
MOVED
TO
DISK
I
E
ALL
THEIR
PAGES
ARE
REMOVED
FROM
MEMORY
TO
LET
OTHER
PROCESSES
HAVE
A
TURN
AT
THE
CPU
THE
QUESTION
ARISES
OF
WHAT
TO
DO
WHEN
A
PROCESS
IS
BROUGHT
BACK
IN
AGAIN
TECHNICALLY
NOTHING
NEED
BE
DONE
THE
PROCESS
WILL
JUST
CAUSE
PAGE
FAULTS
UNTIL
MEMORY
MANAGEMEN
T
CHAP
ITS
WORKING
SET
HAS
BEEN
LOADED
THE
PROBLEM
IS
THAT
HAVING
OR
EVEN
PAGE
FAULTS
EVERY
TIME
A
PROCESS
IS
LOADED
IS
SLOW
AND
IT
ALSO
WASTES
CONSIDERABLE
CPU
TIME
SINCE
IT
TAKES
THE
OPERATING
SYSTEM
A
FEW
MILLISECONDS
OF
CPU
TIME
TO
PROCESS
A
PAGE
FAULT
THEREFORE
MANY
PAGING
SYSTEMS
TRY
TO
KEEP
TRACK
OF
EACH
PROCESS
WORKING
SET
AND
MAKE
SURE
THAT
IT
IS
IN
MEMORY
BEFORE
LETTING
THE
PROCESS
RUN
THIS
AP
PROACH
IS
CALLED
THE
WORKING
SET
MODEL
DENNING
IT
IS
DESIGNED
TO
GREATLY
REDUCE
THE
PAGE
FAULT
RATE
LOADING
THE
PAGES
BEFORE
LETTING
PROCESSES
RUN
IS
ALSO
CALLED
PREPAGING
NOTE
THAT
THE
WORKING
SET
CHANGES
OVER
TIME
IT
HAS
LONG
BEEN
KNOWN
THAT
MOST
PROGRAMS
DO
NOT
REFERENCE
THEIR
ADDRESS
SPACE
UNIFORMLY
BUT
THAT
THE
REFERENCES
TEND
TO
CLUSTER
ON
A
SMALL
NUMBER
OF
PAGES
A
MEMORY
REFERENCE
MAY
FETCH
AN
INSTRUCTION
IT
MAY
FETCH
DATA
OR
IT
MAY
STORE
DATA
AT
ANY
INSTANT
OF
TIME
THERE
EXISTS
A
SET
CONSISTING
OF
ALL
THE
PAGES
USED
BY
THE
K
MOST
RECENT
MEMORY
REFERENCES
THIS
SET
W
K
T
IS
THE
WORKING
SET
BECAUSE
THE
K
MOST
RECENT
REFERENCES
MUST
HAVE
USED
ALL
THE
PAGES
USED
BY
THE
K
MOST
RECENT
REFERENCES
AND
POSSIBLY
OTHERS
W
K
T
IS
A
MONOTONICALLY
NON
DECREASING
FUNCTION
OF
K
THE
LIMIT
OF
W
K
T
AS
K
BECOMES
LARGE
IS
FINITE
BECAUSE
A
PROGRAM
CANNOT
REFERENCE
MORE
PAGES
THAN
ITS
ADDRESS
SPACE
CONTAINS
AND
FEW
PROGRAMS
WILL
USE
EVERY
SINGLE
PAGE
FIGURE
DEPICTS
THE
SIZE
OF
THE
WORKING
SET
AS
A
FUNCTION
OF
K
W
K
T
K
FIGURE
THE
WORKING
SET
IS
THE
SET
OF
PAGES
USED
BY
THE
K
MOST
RECENT
MEMORY
REFERENCES
THE
FUNCTION
W
K
T
IS
THE
SIZE
OF
THE
WORKING
SET
AT
TIME
T
THE
FACT
THAT
MOST
PROGRAMS
RANDOMLY
ACCESS
A
SMALL
NUMBER
OF
PAGES
BUT
THAT
THIS
SET
CHANGES
SLOWLY
IN
TIME
EXPLAINS
THE
INITIAL
RAPID
RISE
OF
THE
CURVE
AND
THEN
THE
SLOW
RISE
FOR
LARGE
K
FOR
EXAMPLE
A
PROGRAM
THAT
IS
EXECUTING
A
LOOP
OCCUPYING
TWO
PAGES
USING
DATA
ON
FOUR
PAGES
MAY
REFERENCE
ALL
SIX
PAGES
EVERY
INSTRUCTIONS
BUT
THE
MOST
RECENT
REFERENCE
TO
SOME
OTHER
PAGE
MAY
BE
A
MIL
LION
INSTRUCTIONS
EARLIER
DURING
THE
INITIALIZATION
PHASE
DUE
TO
THIS
ASYMPTOTIC
BE
HAVIOR
THE
CONTENTS
OF
THE
WORKING
SET
IS
NOT
SENSITIVE
TO
THE
VALUE
OF
K
CHOSEN
SEC
PAGE
REPLACEMENT
ALGORITHMS
TO
PUT
IT
DIFFERENTLY
THERE
EXISTS
A
WIDE
RANGE
OF
K
VALUES
FOR
WHICH
THE
WORKING
SET
IS
UNCHANGED
BECAUSE
THE
WORKING
SET
VARIES
SLOWLY
WITH
TIME
IT
IS
POSSIBLE
TO
MAKE
A
REASONABLE
GUESS
AS
TO
WHICH
PAGES
WILL
BE
NEEDED
WHEN
THE
PROGRAM
IS
RESTARTED
ON
THE
BASIS
OF
ITS
WORKING
SET
WHEN
IT
WAS
LAST
STOPPED
PREPAGING
CON
SISTS
OF
LOADING
THESE
PAGES
BEFORE
RESUMING
THE
PROCESS
TO
IMPLEMENT
THE
WORKING
SET
MODEL
IT
IS
NECESSARY
FOR
THE
OPERATING
SYSTEM
TO
KEEP
TRACK
OF
WHICH
PAGES
ARE
IN
THE
WORKING
SET
HAVING
THIS
INFORMATION
ALSO
IMMEDIATELY
LEADS
TO
A
POSSIBLE
PAGE
REPLACEMENT
ALGORITHM
WHEN
A
PAGE
FAULT
OCCURS
FIND
A
PAGE
NOT
IN
THE
WORKING
SET
AND
EVICT
IT
TO
IMPLEMENT
SUCH
AN
AL
GORITHM
WE
NEED
A
PRECISE
WAY
OF
DETERMINING
WHICH
PAGES
ARE
IN
THE
WORKING
SET
BY
DEFINITION
THE
WORKING
SET
IS
THE
SET
OF
PAGES
USED
IN
THE
K
MOST
RECENT
MEMORY
REFERENCES
SOME
AUTHORS
USE
THE
K
MOST
RECENT
PAGE
REFERENCES
BUT
THE
CHOICE
IS
ARBITRARY
TO
IMPLEMENT
ANY
WORKING
SET
ALGORITHM
SOME
VALUE
OF
K
MUST
BE
CHOSEN
IN
ADVANCE
ONCE
SOME
VALUE
HAS
BEEN
SELECTED
AFTER
EVERY
MEM
ORY
REFERENCE
THE
SET
OF
PAGES
USED
BY
THE
MOST
RECENT
K
MEMORY
REFERENCES
IS
UNIQUELY
DETERMINED
OF
COURSE
HAVING
AN
OPERATIONAL
DEFINITION
OF
THE
WORKING
SET
DOES
NOT
MEAN
THAT
THERE
IS
AN
EFFICIENT
WAY
TO
COMPUTE
IT
DURING
PROGRAM
EXECUTION
ONE
COULD
IMAGINE
A
SHIFT
REGISTER
OF
LENGTH
K
WITH
EVERY
MEMORY
REFERENCE
SHIFTING
THE
REG
ISTER
LEFT
ONE
POSITION
AND
INSERTING
THE
MOST
RECENTLY
REFERENCED
PAGE
NUMBER
ON
THE
RIGHT
THE
SET
OF
ALL
K
PAGE
NUMBERS
IN
THE
SHIFT
REGISTER
WOULD
BE
THE
WORKING
SET
IN
THEORY
AT
A
PAGE
FAULT
THE
CONTENTS
OF
THE
SHIFT
REGISTER
COULD
BE
READ
OUT
AND
SORTED
DUPLICATE
PAGES
COULD
THEN
BE
REMOVED
THE
RESULT
WOULD
BE
THE
WORKING
SET
HOWEVER
MAINTAINING
THE
SHIFT
REGISTER
AND
PROCESSING
IT
AT
A
PAGE
FAULT
WOULD
BOTH
BE
PROHIBITIVELY
EXPENSIVE
SO
THIS
TECHNIQUE
IS
NEVER
USED
INSTEAD
VARIOUS
APPROXIMATIONS
ARE
USED
ONE
COMMONLY
USED
APPROXIMATION
IS
TO
DROP
THE
IDEA
OF
COUNTING
BACK
K
MEMORY
REFERENCES
AND
USE
EXECUTION
TIME
INSTEAD
FOR
EXAMPLE
INSTEAD
OF
DEFINING
THE
WORKING
SET
AS
THOSE
PAGES
USED
DUR
ING
THE
PREVIOUS
MILLION
MEMORY
REFERENCES
WE
CAN
DEFINE
IT
AS
THE
SET
OF
PAGES
USED
DURING
THE
PAST
MSEC
OF
EXECUTION
TIME
IN
PRACTICE
SUCH
A
DEFINI
TION
IS
JUST
AS
GOOD
AND
MUCH
EASIER
TO
WORK
WITH
NOTE
THAT
FOR
EACH
PROCESS
ONLY
ITS
OWN
EXECUTION
TIME
COUNTS
THUS
IF
A
PROCESS
STARTS
RUNNING
AT
TIME
T
AND
HAS
HAD
MSEC
OF
CPU
TIME
AT
REAL
TIME
T
MSEC
FOR
WORKING
SET
PURPOSES
ITS
TIME
IS
MSEC
THE
AMOUNT
OF
CPU
TIME
A
PROCESS
HAS
ACTUALLY
USED
SINCE
IT
STARTED
IS
OFTEN
CALLED
ITS
CURRENT
VIRTUAL
TIME
WITH
THIS
APPROXIMATION
THE
WORKING
SET
OF
A
PROCESS
IS
THE
SET
OF
PAGES
IT
HAS
REFERENCED
DURING
THE
PAST
X
SEC
ONDS
OF
VIRTUAL
TIME
NOW
LET
US
LOOK
AT
A
PAGE
REPLACEMENT
ALGORITHM
BASED
ON
THE
WORKING
SET
THE
BASIC
IDEA
IS
TO
FIND
A
PAGE
THAT
IS
NOT
IN
THE
WORKING
SET
AND
EVICT
IT
IN
FIG
WE
SEE
A
PORTION
OF
A
PAGE
TABLE
FOR
SOME
MACHINE
BECAUSE
ONLY
PAGES
THAT
ARE
IN
MEMORY
ARE
CONSIDERED
AS
CANDIDATES
FOR
EVICTION
PAGES
THAT
ARE
ABSENT
FROM
MEMORY
ARE
IGNORED
BY
THIS
ALGORITHM
EACH
ENTRY
CONTAINS
AT
LEAST
TWO
KEY
ITEMS
OF
INFORMATION
THE
APPROXIMATE
TIME
THE
PAGE
WAS
LAST
USED
AND
THE
R
MEMORY
MANAGEMENT
CHAP
SEC
PAGE
REPLACEMEN
T
ALGORITHM
RENT
CLOCK
TICK
AND
THUS
ALL
HAVE
R
SO
ONE
IS
CHOSEN
AT
RANDOM
FOR
REMOVAL
REFERENCED
BIT
THE
EMPTY
WHITE
RECTANGLE
SYMBOLIZES
THE
OTHER
FIELDS
NOT
NEED
ED
FOR
TH
L
ALGORITHM
SUCH
AS
THE
PAGE
FRAME
NUMBER
THE
PROTECTION
BITS
AND
THE
U
MODIFIED
BIT
J
I
CURRENT
VIRTUAL
TIME
INFORMATION
ABOUT
R
REFERENCED
BIT
PREFERABLY
A
CLEAN
PAGE
IF
ONE
EXISTS
THE
WSCIOCK
PAGE
REPLACEMENT
ALGORITHM
THE
BASIC
WORKING
SET
ALGORITHM
IS
CUMBERSOME
SINCE
THE
ENTIRE
PAGE
TABLE
HAS
TO
BE
SCANNED
AT
EACH
PAGE
FAULT
UNTIL
A
SUITABLE
CANDIDATE
IS
LOCATED
AN
IMPROVED
ALGORITHM
THAT
IS
BASED
ON
THE
CLOCK
ALGORITHM
BUT
ALSO
USES
THE
WORK
ING
SET
INFORMATION
IS
CALLED
WSCIOCK
CARR
AND
HENNESSEY
DUE
TO
ITS
ONE
PAGE
TIME
OF
LAST
USE
PAGE
REFERENCED
DURING
THIS
TICK
PAGE
NOT
REFERENCED
DURING
THIS
TICK
J
I
J
PAGE
TABLE
SCAN
AIL
PAGES
EXAMINING
R
BIT
IF
R
SET
TIME
OF
LAST
USE
TO
CURRENT
VIRTUAL
TIME
IF
R
C
REMOVE
THIS
PAGE
IF
R
AND
AGE
T
REMEMBER
THE
SMALLEST
TIME
SIMPLICITY
OF
IMPLEMENTATION
AND
GOOD
PERFORMANCE
IT
IS
WIDELY
USED
IN
PRACTICE
THE
DATA
STRUCTURE
NEEDED
IS
A
CIRCULAR
LIST
OF
PAGE
FRAMES
AS
IN
THE
CLOCK
AL
GORITHM
AND
AS
SHOWN
IN
FIG
A
INITIALLY
THIS
LIST
IS
EMPTY
WHEN
THE
FIRST
PAGE
IS
LOADED
IT
IS
ADDED
TO
THE
LIST
AS
MORE
PAGES
ARE
ADDED
THEY
GO
INTO
THE
LIST
TO
FORM
A
RING
EACH
ENTRY
CONTAINS
THE
TIME
OF
LAST
USE
FIELD
FROM
THE
BASIC
WORKING
SET
ALGORITHM
AS
WELL
AS
THE
R
BIT
SHOWN
AND
THE
M
BIT
NOT
SHOWN
AS
WITH
THE
CLOCK
ALGORITHM
AT
EACH
PAGE
FAULT
THE
PAGE
POINTED
TO
BY
THE
HAND
IS
EXAMINED
FIRST
IF
THE
R
BIT
IS
SET
TO
THE
PAGE
HAS
BEEN
USED
DURING
THE
CURRENT
TICK
SO
IT
IS
NOT
AN
IDEAL
CANDIDATE
TO
REMOVE
THE
R
BIT
IS
THEN
SET
TO
THE
HAND
ADVANCED
TO
THE
NEXT
PAGE
AND
THE
ALGORITHM
REPEATED
FOR
THAT
PAGE
THE
STATE
AFTER
THIS
SEQUENCE
OF
EVENTS
IS
SHOWN
IN
FIG
B
NOW
CONSIDER
WHAT
HAPPENS
IF
THE
PAGE
POINTED
TO
HAS
R
AS
SHOWN
IN
FIGURE
THE
WORKING
SET
ALGORITHM
THE
ALGORITHM
WORKS
AS
FOLLOWS
THE
HARDWARE
IS
ASSUMED
TO
SET
THE
R
AND
M
BITS
AS
DISCUSSED
EARLIER
SIMILARLY
A
PERIODIC
CLOCK
INTERRUPT
IS
ASSUMED
TO
CAUSE
SOFTWARE
TO
RUN
THAT
CLEARS
THE
REFERENCED
BIT
ON
EVERY
CLOCK
TICK
ON
EVERY
PAGE
FAULT
THE
PAGE
TABLE
IS
SCANNED
TO
LOOK
FOR
A
SUITABLE
PAGE
TO
EVICT
AS
EACH
ENTRY
IS
PROCESSED
THE
R
BIT
IS
EXAMINED
IF
IT
IS
THE
CURRENT
VIRTUAL
TIME
IS
WRITTEN
INTO
THE
TIME
OF
LAST
USE
FIELD
IN
THE
PAGE
TABLE
INDICATING
THAT
THE
PAGE
WAS
IN
USE
AT
THE
TIME
THE
FAULT
OCCURRED
SINCE
THE
PAGE
HAS
BEEN
REFERENCED
DURING
THE
CURRENT
CLOCK
TICK
IT
IS
CLEARLY
IN
THE
WORKING
SET
AND
IS
NOT
A
CANDIDATE
FOR
REMOVAL
IS
ASSUMED
TO
SPAN
MULTIPLE
CLOCK
TICKS
IF
R
IS
THE
PAGE
HAS
NOT
BEEN
REFERENCED
DURING
THE
CURRENT
CLOCK
TICK
AND
MAY
BE
A
CANDIDATE
FOR
REMOVAL
TO
SEE
WHETHER
OR
NOT
IT
SHOULD
BE
REMOVED
ITS
AGE
THE
CURRENT
VIRTUAL
TIME
MINUS
ITS
TIME
OF
LAST
USE
IS
COMPUTED
AND
COMPARED
TO
X
IF
THE
AGE
IS
GREATER
THAN
X
THE
PAGE
IS
NO
LONGER
IN
THE
WORKING
SET
AND
THE
NEW
PAGE
REPLACES
IT
THE
SCAN
CONTINUES
UPDATING
THE
REMAINING
ENTRIES
HOWEVER
IF
R
IS
BUT
THE
AGE
IS
LESS
THAN
OR
EQUAL
TO
X
THE
PAGE
IS
STILL
IN
THE
WORKING
SET
THE
PAGE
IS
TEMPORARILY
SPARED
BUT
THE
PAGE
WITH
THE
GREATEST
AGE
SMALLEST
VALUE
OF
TIME
OF
LAST
USE
IS
NOTED
IF
THE
ENTIRE
TABLE
IS
SCANNED
WITHOUT
FINDING
A
CANDIDATE
TO
EVICT
THAT
MEANS
THAT
ALL
PAGES
ARE
IN
THE
WORKING
SET
IN
THAT
CASE
IF
ONE
OR
MORE
PAGES
WITH
R
WERE
FOUND
THE
ONE
WITH
THE
GREATEST
AGE
IS
EVICTED
IN
THE
WORST
CASE
ALL
PAGES
HAVE
BEEN
REFERENCED
DURING
THE
CUR
FIG
C
IF
THE
AGE
IS
GREATER
THAN
X
AND
THE
PAGE
IS
CLEAN
IT
IS
NOT
IN
THE
WORKING
SET
AND
A
VALID
COPY
EXISTS
ON
THE
DISK
THE
PAGE
FRAME
IS
SIMPLY
CLAIMED
AND
THE
NEW
PAGE
PUT
THERE
AS
SHOWN
IN
FIG
D
ON
THE
OTHER
HAND
IF
THE
PAGE
IS
DIRTY
IT
CANNOT
BE
CLAIMED
IMMEDIATELY
SINCE
NO
VALID
COPY
IS
PRESENT
ON
DISK
TO
AVOID
A
PROCESS
SWITCH
THE
WRITE
TO
DISK
IS
SCHEDULED
BUT
THE
HAND
IS
AD
VANCED
AND
THE
ALGORITHM
CONTINUES
WITH
THE
NEXT
PAGE
AFTER
ALL
THERE
MIGHT
BE
AN
OLD
CLEAN
PAGE
FURTHER
DOWN
THE
LINE
THAT
CAN
BE
USED
IMMEDIATELY
IN
PRINCIPLE
ALL
PAGES
MIGHT
BE
SCHEDULED
FOR
DISK
I
O
ON
ONE
CYCLE
AROUND
THE
CLOCK
TO
REDUCE
DISK
TRAFFIC
A
LIMIT
MIGHT
BE
SET
ALLOWING
A
MAXIMUM
OF
N
PAGES
TO
BE
WRITTEN
BACK
ONCE
THIS
LIMIT
HAS
BEEN
REACHED
NO
NEW
WRITES
ARE
SCHEDULED
WHAT
HAPPENS
IF
THE
HAND
COMES
ALL
THE
WAY
AROUND
TO
ITS
STARTING
POINT
THERE
ARE
TWO
CASES
TO
CONSIDER
AT
LEAST
ONE
WRITE
HAS
BEEN
SCHEDULED
NO
WRITES
HAVE
BEEN
SCHEDULED
IN
THE
FIRST
CASE
THE
HAND
JUST
KEEPS
MOVING
LOOKING
FOR
A
CLEAN
PAGE
SINCE
ONE
OR
MORE
WRITES
HAVE
BEEN
SCHEDULED
EVENTUALLY
SOME
WRITE
WILL
COMPLETE
AND
ITS
PAGE
WILL
BE
MARKED
AS
CLEAN
THE
FIRST
CLEAN
PAGE
ENCOUNTERED
IS
EVICTED
THIS
PAGE
IS
NOT
NECESSARILY
THE
FIRST
WRITE
SCHEDULED
BECAUSE
THE
DISK
DRIVER
MAY
REORDER
WRITES
IN
ORDER
TO
OPTIMIZE
DISK
PERFORMANCE
MEMORY
MANAGEMENT
CHAP
SEC
PAGE
REPLACEMENT
ALGORITHMS
SUMMAR
Y
OF
PAGE
REPLACEMENT
ALGORITHMS
J
CURRENT
VIRTUAL
TIME
WE
HAVE
NOW
LOOKED
AT
A
VARIETY
OF
PAGE
REPLACEMENT
ALGORITHMS
IN
THIS
SEC
TION
WE
WILL
BRIEFLY
SUMMARIZE
THEM
THE
LIST
OF
ALGORITHMS
DISCUSSED
IS
GIVEN
IN
FIG
A
T
V
TIME
OF
LAST
USE
RBIT
B
FIGURE
PAGE
REPLACEMENT
ALGORITHMS
DISCUSSED
IN
THE
TEXT
THE
OPTIMAL
ALGORITHM
EVICTS
THE
PAGE
THAT
WILL
BE
REFERENCED
FURTHEST
IN
THE
FUTURE
UNFORTUNATELY
THERE
IS
NO
WAY
TO
DETERMINE
WHICH
PAGE
THIS
IS
SO
IN
PRAC
C
TICE
THIS
ALGORITHM
CANNOT
BE
USED
IT
IS
USEFUL
AS
A
BENCHMARK
AGAINST
WHICH
OTHER
ALGORITHMS
CAN
BE
MEASURED
HOWEVER
THE
NRU
ALGORITHM
DIVIDES
PAGES
INTO
FOUR
CLASSES
DEPENDING
ON
THE
STATE
OF
THE
R
AND
M
BITS
A
RANDOM
PAGE
FROM
THE
LOWEST
NUMBERED
CLASS
IS
CHOSEN
THIS
ALGORITHM
IS
EASY
TO
IMPLEMENT
BUT
IT
IS
VERY
CRUDE
BETTER
ONES
EXIST
FIFO
KEEPS
TRACK
OF
THE
ORDER
IN
WHICH
PAGES
WERE
LOADED
INTO
MEMORY
BY
KEEPING
THEM
IN
A
LINKED
LIST
REMOVING
THE
OLDEST
PAGE
THEN
BECOMES
TRIVIAL
BUT
THAT
PAGE
MIGHT
STILL
BE
IN
USE
SO
FIFO
IS
A
BAD
CHOICE
SECOND
CHANCE
IS
A
MODIFICATION
TO
FIFO
THAT
CHECKS
IF
A
PAGE
IS
IN
USE
BEFORE
REMOVING
IT
IF
IT
IS
THE
PAGE
IS
SPARED
THIS
MODIFICATION
GREATLY
IMPROVES
THE
PERFORMANCE
CLOCK
IS
SIMPLY
A
DIFFERENT
IMPLEMENTATION
OF
SECOND
CHANCE
IT
HAS
THE
SAME
PERFORMANCE
PROPERTIES
BUT
TAKES
A
LITTLE
LESS
TIME
TO
EXECUTE
THE
ALGO
FIGURE
OPERATION
OF
THE
WSCLOCK
ALGORITHM
A
AND
B
GIVE
AN
EXAMPLE
OF
WHAT
HAPPENS
WHEN
R
C
AND
D
GIVE
AN
EXAMPLE
OF
R
IN
THE
SECOND
CASE
ALL
PAGES
ARE
IN
THE
WORKING
SET
OTHERWISE
AT
LEAST
ONE
WRITE
WOULD
HAVE
BEEN
SCHEDULED
LACKING
ADDITIONAL
INFORMATION
THE
SIMPLEST
THING
TO
DO
IS
CLAIM
ANY
CLEAN
PAGE
AND
USE
IT
THE
LOCATION
OF
A
CLEAN
PAGE
COULD
BE
KEPT
TRACK
OF
DURING
THE
SWEEP
IF
NO
CLEAN
PAGES
EXIST
THEN
THE
CURRENT
PAGE
IS
CHOSEN
AS
THE
VICTIM
AND
WRITTEN
BACK
TO
DISK
RITHM
LRU
IS
AN
EXCELLENT
ALGORITHM
BUT
IT
CANNOT
BE
IMPLEMENTED
WITHOUT
SPECIAL
HARDWARE
IF
THIS
HARDWARE
IS
NOT
AVAILABLE
IT
CANNOT
BE
USED
NFU
IS
A
CRUDE
AT
TEMPT
TO
APPROXIMATE
LRU
IT
IS
NOT
VERY
GOOD
HOWEVER
AGING
IS
A
MUCH
BETTER
APPROXIMATION
TO
LRU
AND
CAN
BE
IMPLEMENTED
EFFICIENTLY
IT
IS
A
GOOD
CHOICE
THE
LAST
TWO
ALGORITHMS
USE
THE
WORKING
SET
THE
WORKING
SET
ALGORITHM
GIVES
REASONABLE
PERFORMANCE
BUT
IT
IS
SOMEWHAT
EXPENSIVE
TO
IMPLEMENT
WSCLOCK
IS
A
VARIANT
THAT
NOT
ONLY
GIVES
GOOD
PERFORMANCE
BUT
IS
ALSO
EFFICIENT
TO
IMPLEMENT
MEMORY
MANAGEMENT
CHAP
ALL
IN
ALL
THE
TWO
BEST
ALGORITHMS
ARE
AGING
AND
WSCLOCK
THEY
ARE
BASED
ON
LRU
AND
THE
WORKING
SET
RESPECTIVELY
BOTH
GIVE
GOOD
PAGING
PERFORMANCE
AND
CAN
BE
IMPLEMENTED
EFFICIENTLY
A
FEW
OTHER
ALGORITHMS
EXIST
BUT
THESE
TWO
ARE
PROBABLY
THE
MOST
IMPORTANT
IN
PRACTICE
D
E
I
G
N
I
U
E
F
O
R
P
A
G
I
N
G
Y
T
E
M
IN
THE
PREVIOUS
SECTIONS
WE
HAVE
EXPLAINED
HOW
PAGING
WORKS
AND
HAVE
GIVEN
A
FEW
OF
THE
BASIC
PAGE
REPLACEMENT
ALGORITHMS
AND
SHOWN
HOW
TO
MODEL
THEM
BUT
KNOWING
THE
BARE
MECHANICS
IS
NOT
ENOUGH
TO
DESIGN
A
SYSTEM
YOU
HAVE
TO
KNOW
A
LOT
MORE
TO
MAKE
IT
WORK
WELL
IT
IS
LIKE
THE
DIFFERENCE
BETWEEN
KNOWING
HOW
TO
MOVE
THE
ROOK
KNIGHT
BISHOP
AND
OTHER
PIECES
IN
CHESS
AND
BEING
A
GOOD
PLAYER
IN
THE
FOLLOWING
SECTIONS
WE
WILL
LOOK
AT
OTHER
ISSUES
THAT
OPERATING
SYS
TEM
DESIGNERS
MUST
CONSIDER
CAREFULLY
IN
ORDER
TO
GET
GOOD
PERFORMANCE
FROM
A
SEC
DESIGN
ISSUES
FOR
PAGING
SYSTEMS
AO
AO
AO
A
A
A
A
A
A
A
A
A
A
A
A
A
A
BO
B
B
B
B
B
B
B
B
B
B
B
B
C
C
C
C
C
C
C
C
PAGING
SYSTEM
A
LB
C
LOCA
L
VERSU
GLOBA
L
ALLOCATIO
N
POLICIE
IN
THE
PRECEDING
SECTIONS
WE
HAVE
DISCUSSED
SEVERAL
ALGORITHMS
FOR
CHOOSING
A
PAGE
TO
REPLACE
WHEN
A
FAULT
OCCURS
A
MAJOR
ISSUE
ASSOCIATED
WITH
THIS
CHOICE
WHICH
WE
HAVE
CAREFULLY
SWEPT
UNDER
THE
RUG
UNTIL
NOW
IS
HOW
MEMORY
SHOULD
BE
ALLOCATED
AMONG
THE
COMPETING
RUNNABLE
PROCESSES
TAKE
A
LOOK
AT
FIG
A
IN
THIS
FIGURE
THREE
PROCESSES
A
B
AND
C
MAKE
UP
THE
SET
OF
RUNNABLE
PROCESSES
SUPPOSE
A
GETS
A
PAGE
FAULT
SHOULD
THE
PAGE
REPLACEMENT
ALGORITHM
TRY
TO
FIND
THE
LEAST
RECENTLY
USED
PAGE
CONSIDERING
ONLY
THE
SIX
PAGES
CURRENTLY
ALLOCATED
TO
A
OR
SHOULD
IT
CONSIDER
ALL
THE
PAGES
IN
MEMORY
IF
IT
LOOKS
ONLY
AT
A
PAGES
THE
PAGE
WITH
THE
LOWEST
AGE
VALUE
IS
SO
WE
GET
THE
SITUATION
OF
FIG
B
ON
THE
OTHER
HAND
IF
THE
PAGE
WITH
THE
LOWEST
AGE
VALUE
IS
REMOVED
WITHOUT
REGARD
TO
WHOSE
PAGE
IT
IS
PAGE
WILL
BE
CHOSEN
AND
WE
WILL
GET
THE
SITUATION
OF
FIG
C
THE
ALGORITHM
OF
FIG
B
IS
SAID
TO
BE
A
LOCAL
PAGE
REPLACEMENT
ALGORITHM
WHEREAS
THAT
OF
FIG
C
IS
SAID
TO
BE
A
GLOBAL
ALGORITHM
LOCAL
AL
GORITHMS
EFFECTIVELY
CORRESPOND
TO
ALLOCATING
EVERY
PROCESS
A
FIXED
FRACTION
OF
THE
MEMORY
GLOBAL
ALGORITHMS
DYNAMICALLY
ALLOCATE
PAGE
FRAMES
AMONG
THE
RUNNABLE
PROCESSES
THUS
THE
NUMBER
OF
PAGE
FRAMES
ASSIGNED
TO
EACH
PROCESS
VARIES
IN
TIME
IN
GENERAL
GLOBAL
ALGORITHMS
WORK
BETTER
ESPECIALLY
WHEN
THE
WORKING
SET
SIZE
CAN
VARY
OVER
THE
LIFETIME
OF
A
PROCESS
IF
A
LOCAL
ALGORITHM
IS
USED
AND
THE
WORKING
SET
GROWS
THRASHING
WILL
RESULT
EVEN
IF
THERE
ARE
PLENTY
OF
FREE
PAGE
FRAMES
IF
THE
WORKING
SET
SHRINKS
LOCAL
ALGORITHMS
WASTE
MEMORY
IF
A
GLOBAL
AL
GORITHM
IS
USED
THE
SYSTEM
MUST
CONTINUALLY
DECIDE
HOW
MANY
PAGE
FRAMES
TO
ASSIGN
TO
EACH
PROCESS
ONE
WAY
IS
TO
MONITOR
THE
WORKING
SET
SIZE
AS
INDICATED
BY
FIGURE
LOCAL
VERSUS
GLOBAL
PAGE
REPLACEMENT
A
ORIGINAL
CONFIGURATION
B
LOCAL
PAGE
REPLACEMENT
C
GLOBAL
PAGE
REPLACEMENT
THE
AGING
BITS
BUT
THIS
APPROACH
DOES
NOT
NECESSARILY
PREVENT
THRASHING
THE
WORK
ING
SET
MAY
CHANGE
SIZE
IN
MICROSECONDS
WHEREAS
THE
AGING
BITS
ARE
A
CRUDE
MEAS
URE
SPREAD
OVER
A
NUMBER
OF
CLOCK
TICKS
ANOTHER
APPROACH
IS
TO
HAVE
AN
ALGORITHM
FOR
ALLOCATING
PAGE
FRAMES
TO
PROC
ESSES
ONE
WAY
IS
TO
PERIODICALLY
DETERMINE
THE
NUMBER
OF
RUNNING
PROCESSES
AND
ALLOCATE
EACH
PROCESS
AN
EQUAL
SHARE
THUS
WITH
AVAILABLE
I
E
NON
OPERAT
ING
SYSTEM
PAGE
FRAMES
AND
PROCESSES
EACH
PROCESS
GETS
FRAMES
THE
REMAINING
SIX
GO
INTO
A
POOL
TO
BE
USED
WHEN
PAGE
FAULTS
OCCUR
ALTHOUGH
THIS
METHOD
SEEMS
FAIR
IT
MAKES
LITTLE
SENSE
TO
GIVE
EQUAL
SHARES
OF
THE
MEMORY
TO
A
KB
PROCESS
AND
A
KB
PROCESS
INSTEAD
PAGES
CAN
BE
ALLO
CATED
IN
PROPORTION
TO
EACH
PROCESS
TOTAL
SIZE
WITH
A
KB
PROCESS
GETTING
TIMES
THE
ALLOTMENT
OF
A
KB
PROCESS
IT
IS
PROBABLY
WISE
TO
GIVE
EACH
PROCESS
SOME
MINIMUM
NUMBER
SO
THAT
IT
CAN
RUN
NO
MATTER
HOW
SMALL
IT
IS
ON
SOME
MA
CHINES
FOR
EXAMPLE
A
SINGLE
TWO
OPERAND
INSTRUCTION
MAY
NEED
AS
MANY
AS
SIX
PAGES
BECAUSE
THE
INSTRUCTION
ITSELF
THE
SOURCE
OPERAND
AND
THE
DESTINATION
OPER
AND
MAY
ALL
STRADDLE
PAGE
BOUNDARIES
WITH
AN
ALLOCATION
OF
ONLY
FIVE
PAGES
PRO
GRAMS
CONTAINING
SUCH
INSTRUCTIONS
CANNOT
EXECUTE
AT
ALL
IF
A
GLOBAL
ALGORITHM
IS
USED
IT
MAY
BE
POSSIBLE
TO
START
EACH
PROCESS
UP
WITH
SOME
NUMBER
OF
PAGES
PROPORTIONAL
TO
THE
PROCESS
SIZE
BUT
THE
ALLOCATION
HAS
TO
BE
UPDATED
DYNAMICALLY
AS
THE
PROCESSES
RUN
ONE
WAY
TO
MANAGE
THE
ALLOCATION
IS
TO
USE
THE
PFF
PAGE
FAULT
FREQUENCY
ALGORITHM
IT
TELLS
WHEN
TO
INCREASE
OR
DECREASE
A
PROCESS
PAGE
ALLOCATION
BUT
SAYS
NOTHING
ABOUT
WHICH
PAGE
TO
REPLACE
ON
A
FAULT
IT
JUST
CONTROLS
THE
SIZE
OF
THE
ALLOCATION
SET
MEMORY
MANAGEMEN
T
CHAP
FOR
A
LARGE
CLASS
OF
PAGE
REPLACEMENT
ALGORITHMS
INCLUDING
LRU
IT
IS
KNOWN
THAT
THE
FAULT
RATE
DECREASES
AS
MORE
PAGES
ARE
ASSIGNED
AS
WE
DISCUSSED
ABOVE
THIS
IS
THE
ASSUMPTION
BEHIND
PFF
THIS
PROPERTY
IS
ILLUSTRATED
IN
FIG
NUMBER
OF
PAGE
FRAMES
ASSIGNED
FIGURE
PAGE
FAULT
RATE
AS
A
FUNCTION
OF
THE
NUMBER
OF
PAGE
FRAMES
ASSIGNED
MEASURING
THE
PAGE
FAULT
RATE
IS
STRAIGHTFORWARD
JUST
COUNT
THE
NUMBER
OF
FAULTS
PER
SECOND
POSSIBLY
TAKING
A
RUNNING
MEAN
OVER
PAST
SECONDS
AS
WELL
ONE
EASY
WAY
TO
DO
THIS
IS
TO
ADD
THE
NUMBER
OF
PAGE
FAULTS
DURING
THE
IMMEDIATELY
PRECEDING
SECOND
TO
THE
CURRENT
RUNNING
MEAN
AND
DIVIDE
BY
TWO
THE
DASHED
LINE
MARKED
A
CORRESPONDS
TO
A
PAGE
FAULT
RATE
THAT
IS
UNACCEPTABLY
HIGH
SO
THE
FAULT
ING
PROCESS
IS
GIVEN
MORE
PAGE
FRAMES
TO
REDUCE
THE
FAULT
RATE
THE
DASHED
LINE
MARKED
B
CORRESPONDS
TO
A
PAGE
FAULT
RATE
SO
LOW
THAT
WE
CAN
ASSUME
THE
PROCESS
HAS
TOO
MUCH
MEMORY
IN
THIS
CASE
PAGE
FRAMES
MAY
BE
TAKEN
AWAY
FROM
IT
THUS
PFF
TRIES
TO
KEEP
THE
PAGING
RATE
FOR
EACH
PROCESS
WITHIN
ACCEPTABLE
BOUNDS
IT
IS
IMPORTANT
TO
NOTE
THAT
SOME
PAGE
REPLACEMENT
ALGORITHMS
CAN
WORK
WITH
EITHER
A
LOCAL
REPLACEMENT
POLICY
OR
A
GLOBAL
ONE
FOR
EXAMPLE
FIFO
CAN
REPLACE
THE
OLDEST
PAGE
IN
ALL
OF
MEMORY
GLOBAL
ALGORITHM
OR
THE
OLDEST
PAGE
OWNED
BY
THE
CURRENT
PROCESS
LOCAL
ALGORITHM
SIMILARLY
LRU
OR
SOME
APPROXIMATION
TO
IT
CAN
REPLACE
THE
LEAST
RECENTLY
USED
PAGE
IN
ALL
OF
MEMORY
GLOBAL
ALGORITHM
OR
THE
LEAST
RECENTLY
USED
PAGE
OWNED
BY
THE
CURRENT
PROCESS
LOCAL
ALGORITHM
THE
CHOICE
OF
LOCAL
VERSUS
GLOBAL
IS
INDEPENDENT
OF
THE
ALGORITHM
IN
SOME
CASES
ON
THE
OTHER
HAND
FOR
OTHER
PAGE
REPLACEMENT
ALGORITHMS
ONLY
A
LOCAL
STRATE
GY
MAKES
SENSE
IN
PARTICULAR
THE
WORKING
SET
AND
WSCLOCK
ALGORITHMS
REFER
TO
SOME
SPECIFIC
PROCESS
AND
MUST
BE
APPLIED
IN
THAT
CONTEXT
THERE
REALLY
IS
NO
WORKING
SET
FOR
THE
MACHINE
AS
A
WHOLE
AND
TRYING
TO
USE
THE
UNION
OF
ALL
THE
WORKING
SETS
WOULD
LOSE
THE
LOCALITY
PROPERTY
AND
NOT
WORK
WELL
LOAD
CONTROL
EVEN
WITH
THE
BEST
PAGE
REPLACEMENT
ALGORITHM
AND
OPTIMAL
GLOBAL
ALLOCATION
OF
PAGE
FRAMES
TO
PROCESSES
IT
CAN
HAPPEN
THAT
THE
SYSTEM
THRASHES
IN
FACT
WHEN
EVER
THE
COMBINED
WORKING
SETS
OF
ALL
PROCESSES
EXCEED
THE
CAPACITY
OF
MEMORY
SEC
DESIGN
ISSUES
FOR
PAGING
SYSTEMS
THRASHING
CAN
BE
EXPECTED
ONE
SYMPTOM
OF
THIS
SITUATION
IS
THAT
THE
PFF
ALGO
RITHM
INDICATES
THAT
SOME
PROCESSES
NEED
MORE
MEMORY
BUT
NO
PROCESSES
NEED
LESS
MEMORY
IN
THIS
CASE
THERE
IS
NO
WAY
TO
GIVE
MORE
MEMORY
TO
THOSE
PROCESSES
NEEDING
IT
WITHOUT
HURTING
SOME
OTHER
PROCESSES
THE
ONLY
REAL
SOLUTION
IS
TO
TEM
PORARILY
GET
RID
OF
SOME
PROCESSES
A
GOOD
WAY
TO
REDUCE
THE
NUMBER
OF
PROCESSES
COMPETING
FOR
MEMORY
IS
TO
SWAP
SOME
OF
THEM
TO
THE
DISK
AND
FREE
UP
ALL
THE
PAGES
THEY
ARE
HOLDING
FOR
EX
AMPLE
ONE
PROCESS
CAN
BE
SWAPPED
TO
DISK
AND
ITS
PAGE
FRAMES
DIVIDED
UP
AMONG
OTHER
PROCESSES
THAT
ARE
THRASHING
IF
THE
THRASHING
STOPS
THE
SYSTEM
CAN
RUN
FOR
A
WHILE
THIS
WAY
IF
IT
DOES
NOT
STOP
ANOTHER
PROCESS
HAS
TO
BE
SWAPPED
OUT
AND
SO
ON
UNTIL
THE
THRASHING
STOPS
THUS
EVEN
WITH
PAGING
SWAPPING
IS
STILL
NEEDED
ONLY
NOW
SWAPPING
IS
USED
TO
REDUCE
POTENTIAL
DEMAND
FOR
MEMORY
RATHER
THAN
TO
RECLAIM
PAGES
SWAPPING
PROCESSES
OUT
TO
RELIEVE
THE
LOAD
ON
MEMORY
IS
REMINISCENT
OF
TWO
LEVEL
SCHEDULING
IN
WHICH
SOME
PROCESSES
ARE
PUT
ON
DISK
AND
A
SHORT
TERM
SCHED
ULER
IS
USED
TO
SCHEDULE
THE
REMAINING
PROCESSES
CLEARLY
THE
TWO
IDEAS
CAN
BE
COMBINED
WITH
JUST
ENOUGH
PROCESSES
SWAPPED
OUT
TO
MAKE
THE
PAGE
FAULT
RATE
AC
CEPTABLE
PERIODICALLY
SOME
PROCESSES
ARE
BROUGHT
IN
FROM
DISK
AND
OTHER
ONES
ARE
SWAPPED
OUT
HOWEVER
ANOTHER
FACTOR
TO
CONSIDER
IS
THE
DEGREE
OF
MULTIPROGRAMMING
WHEN
THE
NUMBER
OF
PROCESSES
IN
MAIN
MEMORY
IS
TOO
LOW
THE
CPU
MAY
BE
IDLE
FOR
SUBSTANTIAL
PERIODS
OF
TIME
THIS
CONSIDERATION
ARGUES
FOR
CONSIDERING
NOT
ONLY
PROCESS
SIZE
AND
PAGING
RATE
WHEN
DECIDING
WHICH
PROCESS
TO
SWAP
OUT
BUT
ALSO
ITS
CHARACTERISTICS
SUCH
AS
WHETHER
IT
IS
CPU
BOUND
OR
I
O
BOUND
AND
WHAT
CHARAC
TERISTICS
THE
REMAINING
PROCESSES
HAVE
PAG
E
SIZE
THE
PAGE
SIZE
IS
OFTEN
A
PARAMETER
THAT
CAN
BE
CHOSEN
BY
THE
OPERATING
SYSTEM
EVEN
IF
THE
HARDWARE
HAS
BEEN
DESIGNED
WITH
FOR
EXAMPLE
BYTE
PAGES
THE
OPERATING
SYSTEM
CAN
EASILY
REGARD
PAGE
PAIRS
AND
AND
AND
AND
SO
ON
AS
KB
PAGES
BY
ALWAYS
ALLOCATING
TWO
CONSECUTIVE
BYTE
PAGE
FRAMES
FOR
THEM
DETERMINING
THE
BEST
PAGE
SIZE
REQUIRES
BALANCING
SEVERAL
COMPETING
FACTORS
AS
A
RESULT
THERE
IS
NO
OVERALL
OPTIMUM
TO
START
WITH
THERE
ARE
TWO
FACTORS
THAT
ARGUE
FOR
A
SMALL
PAGE
SIZE
A
RANDOMLY
CHOSEN
TEXT
DATA
OR
STACK
SEGMENT
WILL
NOT
FILL
AN
INTEGRAL
NUMBER
OF
PAGES
ON
THE
AVERAGE
HALF
OF
THE
FINAL
PAGE
WILL
BE
EMPTY
THE
EXTRA
SPACE
IN
THAT
PAGE
IS
WASTED
THIS
WASTAGE
IS
CALLED
INTERNAL
FRAGMENTATION
WITH
N
SEGMENTS
IN
MEMORY
AND
A
PAGE
SIZE
OF
P
BYTES
NP
BYTES
WILL
BE
WASTED
ON
INTERNAL
FRAGMENTATION
THIS
REASONING
ARGUES
FOR
A
SMALL
PAGE
SIZE
ANOTHER
ARGUMENT
FOR
A
SMALL
PAGE
SIZE
BECOMES
APPARENT
IF
WE
THINK
ABOUT
A
PROGRAM
CONSISTING
OF
EIGHT
SEQUENTIAL
PHASES
OF
KB
EACH
WITH
A
KB
PAGE
MEMORY
MANAGEMENT
CHAP
SIZE
THE
PROGRAM
MUST
BE
ALLOCATED
KB
AIL
THE
TIME
WITH
A
KB
PAGE
SIZE
IT
NEEDS
ONLY
KB
WITH
A
PAGE
SIZE
OF
KB
OR
SMALLER
IT
REQUIRES
ONLY
KB
AT
ANY
INSTANT
IN
GENERAL
A
LARGE
PAGE
SIZE
WILL
CAUSE
MORE
UNUSED
PROGRAM
TO
BE
IN
MEMORY
THAN
A
SMALL
PAGE
SIZE
ON
THE
OTHER
HAND
SMALL
PAGES
MEAN
THAT
PROGRAMS
WILL
NEED
MANY
PAGES
HENCE
A
LARGE
PAGE
TABLE
A
KB
PROGRAM
NEEDS
ONLY
FOUR
KB
PAGES
BUT
BYTE
PAGES
TRANSFERS
TO
AND
FROM
THE
DISK
ARE
GENERALLY
A
PAGE
AT
A
TIME
WITH
MOST
OF
THE
TIME
BEING
FOR
THE
SEEK
AND
ROTATIONAL
DELAY
SO
THAT
TRANSFERRING
A
SMALL
PAGE
TAKES
ALMOST
AS
MUCH
TIME
AS
TRANSFERRING
A
LARGE
PAGE
IT
MIGHT
TAKE
X
MSEC
TO
LOAD
BYTE
PAGES
BUT
ONLY
X
MSEC
TO
LOAD
FOUR
KB
PAGES
ON
SOME
MACHINES
THE
PAGE
TABLE
MUST
BE
LOADED
INTO
HARDWARE
REGISTERS
EV
ERY
TIME
THE
CPU
SWITCHES
FROM
ONE
PROCESS
TO
ANOTHER
ON
THESE
MACHINES
HAV
ING
A
SMALL
PAGE
SIZE
MEANS
THAT
THE
TIME
REQUIRED
TO
LOAD
THE
PAGE
REGISTERS
GETS
LONGER
AS
THE
PAGE
SIZE
GETS
SMALLER
FURTHERMORE
THE
SPACE
OCCUPIED
BY
THE
PAGE
TABLE
INCREASES
AS
THE
PAGE
SIZE
DECREASES
THIS
LAST
POINT
CAN
BE
ANALYZED
MATHEMATICALLY
LET
THE
AVERAGE
PROCESS
SIZE
BE
BYTES
AND
THE
PAGE
SIZE
BE
P
BYTES
FURTHERMORE
ASSUME
THAT
EACH
PAGE
ENTRY
REQUIRES
E
BYTES
THE
APPROXIMATE
NUMBER
OF
PAGES
NEEDED
PER
PROCESS
IS
THEN
P
OCCUPYING
SE
P
BYTES
OF
PAGE
TABLE
SPACE
THE
WASTED
MEMORY
IN
THE
LAST
PAGE
OF
THE
PROCESS
DUE
TO
INTERNAL
FRAGMENTATION
IS
P
THUS
THE
TOTAL
OVERHEAD
DUE
TO
THE
PAGE
TABLE
AND
THE
INTERNAL
FRAGMENTADON
LOSS
IS
GIVEN
BY
THE
SUM
OF
THESE
TWO
TERMS
OVERHEAD
SE
P
P
THE
FIRST
TERM
PAGE
TABLE
SIZE
IS
LARGE
WHEN
THE
PAGE
SIZE
IS
SMALL
THE
SEC
OND
TERM
INTERNAL
FRAGMENTATION
IS
LARGE
WHEN
THE
PAGE
SIZE
IS
LARGE
THE
OPTIMUM
MUST
LIE
SOMEWHERE
IN
BETWEEN
BY
TAKING
THE
FIRST
DERIVATIVE
WITH
RESPECT
TO
P
AND
EQUATING
IT
TO
ZERO
WE
GET
THE
EQUATION
SE
FROM
THIS
EQUATION
WE
CAN
DERIVE
A
FORMULA
THAT
GIVES
THE
OPTIMUM
PAGE
SIZE
CONSIDERING
ONLY
MEMORY
WASTED
IN
FRAGMENTATION
AND
PAGE
TABLE
SIZE
THE
RE
SULT
IS
P
FOR
AND
E
BYTES
PER
PAGE
TABLE
ENTRY
THE
OPTIMUM
PAGE
SIZE
IS
KB
COMMERCIALLY
AVAILABLE
COMPUTERS
HAVE
USED
PAGE
SIZES
RANGING
FROM
BYTES
TO
KB
A
TYPICAL
VALUE
USED
TO
BE
KB
BUT
NOWADAYS
KB
OR
KB
IS
MORE
COMMON
AS
MEMORIES
GET
LARGER
THE
PAGE
SIZE
TENDS
TO
GET
LARGER
AS
WELL
BUT
NOT
LINEARLY
QUADRUPLING
THE
RAM
SIZE
RARELY
EVEN
DOUBLES
THE
PAGE
SIZE
SEC
DESIGN
ISSUES
FOR
PAGING
SYSTEMS
SEPARAT
E
INSTRUCTION
AN
D
DAT
A
SPACES
MOST
COMPUTERS
HAVE
A
SINGLE
ADDRESS
SPACE
THAT
HOLDS
BOTH
PROGRAMS
AND
DATA
AS
SHOWN
IN
FIG
A
IF
THIS
ADDRESS
SPACE
IS
LARGE
ENOUGH
EVERYTHING
WORKS
FINE
HOWEVER
IT
IS
OFTEN
TOO
SMALL
FORCING
PROGRAMMERS
TO
STAND
ON
THEIR
HEADS
TO
FIT
EVERYTHING
INTO
THE
ADDRESS
SPACE
SINGLE
ADDRESS
SPACE
I
SPACE
D
SPACE
UNUSED
PAGE
DATA
A
B
FIGURE
A
ONE
ADDRESS
SPACE
B
SEPARATE
I
AND
D
SPACES
ONE
SOLUTION
PIONEERED
ON
THE
BIT
PDP
IS
TO
HAVE
SEPARATE
ADDRESS
SPACES
FOR
INSTRUCTIONS
PROGRAM
TEXT
AND
DATA
CALLED
I
SPACE
AND
D
SPACE
RE
SPECTIVELY
AS
ILLUSTRATED
IN
FIG
B
EACH
ADDRESS
SPACE
RUNS
FROM
TO
SOME
MAXIMUM
TYPICALLY
OR
THE
LINKER
MUST
KNOW
WHEN
SEPARATE
I
AND
D
SPACES
ARE
BEING
USED
BECAUSE
WHEN
THEY
ARE
THE
DATA
ARE
RELOCATED
TO
VIR
TUAL
ADDRESS
INSTEAD
OF
STARTING
AFTER
THE
PROGRAM
IN
A
COMPUTER
WITH
THIS
DESIGN
BOTH
ADDRESS
SPACES
CAN
BE
PAGED
INDEPEN
DENTLY
FROM
ONE
ANOTHER
EACH
ONE
HAS
ITS
OWN
PAGE
TABLE
WITH
ITS
OWN
MAPPING
OF
VIRTUAL
PAGES
TO
PHYSICAL
PAGE
FRAMES
WHEN
THE
HARDWARE
WANTS
TO
FETCH
AN
IN
STRUCTION
IT
KNOWS
THAT
IT
MUST
USE
I
SPACE
AND
THE
I
SPACE
PAGE
TABLE
SIMILARLY
REFERENCES
TO
DATA
MUST
GO
THROUGH
THE
D
SPACE
PAGE
TABLE
OTHER
THAN
THIS
DIS
TINCTION
HAVING
SEPARATE
I
AND
D
SPACES
DOES
NOT
INTRODUCE
ANY
SPECIAL
COMPLICA
TIONS
AND
IT
DOES
DOUBLE
THE
AVAILABLE
ADDRESS
SPACE
SHARE
D
PAGES
ANOTHER
DESIGN
ISSUE
IS
SHARING
IN
A
LARGE
MULTIPROGRAMMING
SYSTEM
IT
IS
COMMON
FOR
SEVERAL
USERS
TO
BE
RUNNING
THE
SAME
PROGRAM
AT
THE
SAME
TIME
IT
IS
CLEARLY
MORE
EFFICIENT
TO
SHARE
THE
PAGES
TO
AVOID
HAVING
TWO
COPIES
OF
THE
SAME
PAGE
IN
MEMORY
AT
THE
SAME
TIME
ONE
PROBLEM
IS
THAT
NOT
ALL
PAGES
ARE
SHARABLE
IN
PARTICULAR
PAGES
THAT
ARE
READ
ONLY
SUCH
AS
PROGRAM
TEXT
CAN
BE
SHARED
BUT
DATA
PAGES
CANNOT
IF
SEPARATE
I
AND
D
SPACES
ARE
SUPPORTED
IT
IS
RELATIVELY
STRAIGHTFORWARD
TO
SHARE
PROGRAMS
BY
HAVING
TWO
OR
MORE
PROCESSES
USE
THE
SAME
PAGE
TABLE
FOR
THEIR
MEMORY
MANAGEMENT
CHAP
SEC
DESIGN
ISSUES
FOR
PAGING
SYSTEMS
I
SPACE
BUT
DIFFERENT
PAGE
TABLES
FOR
THEIR
D
SPACES
TYPICALLY
IN
AN
IMPLEMENTA
TION
THAT
SUPPORTS
SHARING
IN
THIS
WAY
PAGE
TABLES
ARE
DATA
STRUCTURES
INDEPENDENT
OF
THE
PROCESS
TABLE
EACH
PROCESS
THEN
HAS
TWO
POINTERS
IN
ITS
PROCESS
TABLE
ONE
TO
THE
I
SPACE
PAGE
TABLE
AND
ONE
TO
THE
D
SPACE
PAGE
TABLE
AS
SHOWN
IN
FIG
WHEN
THE
SCHEDULER
CHOOSES
A
PROCESS
TO
RUN
IT
USES
THESE
POINTERS
TO
LOCATE
THE
APPROPRIATE
PAGE
TABLES
AND
SETS
UP
THE
MMU
USING
THEM
EVEN
WITHOUT
SEPARATE
I
AND
D
SPACES
PROCESSES
CAN
SHARE
PROGRAMS
OR
SOMETIMES
LIBRARIES
BUT
THE
MECHANISM
IS
MORE
COMPLICATED
PROGRAM
DATA
DATA
PAGE
TABLES
FIGURE
TW
O
PROCESSES
SHARIN
G
THE
SAM
E
PROGRA
M
SHARIN
G
ITS
PAG
E
TABLE
WHEN
TWO
OR
MORE
PROCESSES
SHARE
SOME
CODE
A
PROBLEM
OCCURS
WITH
THE
SHARED
PAGES
SUPPOSE
THAT
PROCESSES
A
AND
B
ARE
BOTH
RUNNING
THE
EDITOR
AND
SHARING
ITS
PAGES
IF
THE
SCHEDULER
DECIDES
TO
REMOVE
A
FROM
MEMORY
EVICTING
ALL
ITS
PAGES
AND
FILLING
THE
EMPTY
PAGE
FRAMES
WITH
SOME
OTHER
PROGRAM
WILL
CAUSE
B
TO
GENERATE
A
LARGE
NUMBER
OF
PAGE
FAULTS
TO
BRING
THEM
BACK
IN
AGAIN
SIMILARLY
WHEN
A
TERMINATES
IT
IS
ESSENTIAL
TO
BE
ABLE
TO
DISCOVER
THAT
THE
PAGES
ARE
STILL
IN
USE
SO
THAT
THEIR
DISK
SPACE
WILL
NOT
BE
FREED
BY
ACCIDENT
SEARCH
ING
ALL
THE
PAGE
TABLES
TO
SEE
IF
A
PAGE
IS
SHARED
IS
USUALLY
TOO
EXPENSIVE
SO
SPECIAL
DATA
STRUCTURES
ARE
NEEDED
TO
KEEP
TRACK
OF
SHARED
PAGES
ESPECIALLY
IF
THE
UNIT
OF
SHARING
IS
THE
INDIVIDUAL
PAGE
OR
RUN
OF
PAGES
RATHER
THAN
AN
ENTIRE
PAGE
TABLE
SHARING
DATA
IS
TRICKIER
THAN
SHARING
CODE
BUT
IT
IS
NOT
IMPOSSIBLE
IN
PARTICU
LAR
IN
UNIX
AFTER
A
FORK
SYSTEM
CALL
THE
PARENT
AND
CHILD
ARE
REQUIRED
TO
SHARE
BOTH
PROGRAM
TEXT
AND
DATA
IN
A
PAGED
SYSTEM
WHAT
IS
OFTEN
DONE
IS
TO
GIVE
EACH
OF
THESE
PROCESSES
ITS
OWN
PAGE
TABLE
AND
HAVE
BOTH
OF
THEM
POINT
TO
THE
SAME
SET
OF
PAGES
THUS
NO
COPYING
OF
PAGES
IS
DONE
AT
FORK
TIME
HOWEVER
ALL
THE
DATA
PAGES
ARE
MAPPED
INTO
BOTH
PROCESSES
AS
READ
ONLY
AS
LONG
AS
BOTH
PROCESSES
JUST
READ
THEIR
DATA
WITHOUT
MODIFYING
IT
THIS
SITUA
TION
CAN
CONTINUE
AS
SOON
AS
EITHER
PROCESS
UPDATES
A
MEMORY
WORD
THE
VIOLA
TION
OF
THE
READ
ONLY
PROTECTION
CAUSES
A
TRAP
TO
THE
OPERATING
SYSTEM
A
COPY
IS
THEN
MADE
OF
THE
OFFENDING
PAGE
SO
THAT
EACH
PROCESS
NOW
HAS
ITS
OWN
PRIVATE
COPY
BOTH
COPIES
ARE
NOW
SET
TO
READ
WRITE
SO
SUBSEQUENT
WRITES
TO
EITHER
COPY
PROCEED
WITHOUT
TRAPPING
THIS
STRATEGY
MEANS
THAT
THOSE
PAGES
THAT
ARE
NEVER
MODIFIED
INCLUDING
ALL
THE
PROGRAM
PAGES
NEED
NOT
BE
COPIED
ONLY
THE
DATA
PAGES
THAT
ARE
ACTUALLY
MODIFIED
NEED
TO
BE
COPIED
THIS
APPROACH
CALLED
COPY
ON
WRITE
IMPROVES
PERFORMANCE
BY
REDUCING
COPYING
SHARED
LIBRARIES
SHARING
CAN
BE
DONE
AT
OTHER
GRANULARITIES
THAN
INDIVIDUAL
PAGES
IF
A
PROGRAM
IS
STARTED
UP
TWICE
MOST
OPERATING
SYSTEMS
WILL
AUTOMATICALLY
SHARE
ALL
THE
TEXT
PAGES
SO
THAT
ONLY
ONE
COPY
IS
IN
MEMORY
TEXT
PAGES
ARE
ALWAYS
READ
ONLY
SO
THERE
IS
NO
PROBLEM
HERE
DEPENDING
ON
THE
OPERATING
SYSTEM
EACH
PROCESS
MAY
GET
ITS
OWN
PRIVATE
COPY
OF
THE
DATA
PAGES
OR
THEY
MAY
BE
SHARED
AND
MARKED
READ
ONLY
IF
ANY
PROCESS
MODIFIES
A
DATA
PAGE
A
PRIVATE
COPY
WILL
BE
MADE
FOR
IT
THAT
IS
COPY
ON
WRITE
WILL
BE
APPLIED
IN
MODERN
SYSTEMS
THERE
ARE
MANY
LARGE
LIBRARIES
USED
BY
MANY
PROCESSES
FOR
EXAMPLE
THE
LIBRARY
THAT
HANDLES
THE
DIALOG
FOR
BROWSING
FOR
FILES
TO
OPEN
AND
MULTIPLE
GRAPHICS
LIBRARIES
STATICALLY
BINDING
ALL
THESE
LIBRARIES
TO
EVERY
EX
ECUTABLE
PROGRAM
ON
THE
DISK
WOULD
MAKE
THEM
EVEN
MORE
BLOATED
THAN
THEY
AL
READY
ARE
INSTEAD
A
COMMON
TECHNIQUE
IS
TO
USE
SHARED
LIBRARIES
WHICH
ARE
CALLED
DLLS
OR
DYNAMIC
LINK
LIBRARIES
ON
WINDOWS
TO
MAKE
THE
IDEA
OF
A
SHARED
LIBRARY
CLEAR
FIRST
CONSIDER
TRADITIONAL
LINKING
WHEN
A
PROGRAM
IS
LINKED
ONE
OR
MORE
OBJECT
FILES
AND
POSSIBLY
SOME
LIBRARIES
ARE
NAMED
IN
THE
COMMAND
TO
THE
LINKER
SUCH
AS
THE
UNIX
COMMAND
ID
O
IC
IM
WHICH
LINKS
ALL
THE
O
OBJECT
FILES
IN
THE
CURRENT
DIRECTORY
AND
THEN
SCANS
TWO
LI
BRARIES
USR
LIB
LIBC
A
AND
USR
LIB
LIBM
A
ANY
FUNCTIONS
CALLED
IN
THE
OBJECT
FILES
BUT
NOT
PRESENT
THERE
E
G
PRINT
ARE
CALLED
UNDEFINED
EXTERNALS
AND
ARE
SOUGHT
IN
THE
LIBRARIES
IF
THEY
ARE
FOUND
THEY
ARE
INCLUDED
IN
THE
EXECUTABLE
BINARY
ANY
FUNCTIONS
THEY
CALL
BUT
ARE
NOT
YET
PRESENT
ALSO
BECOME
UNDEFINED
EXTERNALS
FOR
EXAMPLE
PRINTF
NEEDS
WRITE
SO
IF
WRITE
IS
NOT
ALREADY
INCLUDED
THE
LINKER
WILL
LOOK
FOR
IT
AND
INCLUDE
IT
WHEN
FOUND
WHEN
THE
LINKER
IS
DONE
AN
EXECUTABLE
BINARY
MEMORY
MANAGEMENT
CHAP
FILE
IS
WRITTEN
TO
THE
DISK
CONTAINING
ALL
THE
FUNCTIONS
NEEDED
FUNCTIONS
PRESENT
IN
THE
LIBRARIES
BUT
NOT
CALLED
ARE
NOT
INCLUDED
WHEN
THE
PROGRAM
IS
LOADED
INTO
MEMORY
AND
EXECUTED
ALL
THE
FUNCTIONS
IT
NEEDS
ARE
THERE
NOW
SUPPOSE
COMMON
PROGRAMS
USE
MB
WORTH
OF
GRAPHICS
AND
USER
INTERFACE
FUNCTIONS
STATICALLY
LINKING
HUNDREDS
OF
PROGRAMS
WITH
ALL
THESE
LIBRARIES
WOULD
WASTE
A
TREMENDOUS
AMOUNT
OF
SPACE
ON
THE
DISK
AS
WELL
AS
WASTING
SPACE
IN
RAM
WHEN
THEY
WERE
LOADED
SINCE
THE
SYSTEM
WOULD
HAVE
NO
WAY
OF
KNOWING
IT
COULD
SHARE
THEM
THIS
IS
WHERE
SHARED
LIBRARIES
COME
IN
WHEN
A
PROGRAM
IS
LINKED
WITH
SHARED
LIBRARIES
WHICH
ARE
SLIGHTLY
DIFFERENT
THAN
STATIC
ONES
INSTEAD
OF
INCLUDING
THE
ACTUAL
FUNCTION
CALLED
THE
LINKER
INCLUDES
A
SMALL
STUB
ROUTINE
THAT
BINDS
TO
THE
CALLED
FUNCTION
AT
RUN
TIME
DEPENDING
ON
THE
SYSTEM
AND
THE
CONFI
GURATION
DETAILS
SHARED
LIBRARIES
ARE
LOADED
EITHER
WHEN
THE
PROGRAM
IS
LOADED
OR
WHEN
FUNCTIONS
IN
THEM
ARE
CALLED
FOR
THE
FIRST
TIME
OF
COURSE
IF
ANOTHER
PROGRAM
HAS
ALREADY
LOADED
THE
SHARED
LIBRARY
THERE
IS
NO
NEED
TO
LOAD
IT
AGAIN
THAT
IS
THE
WHOLE
POINT
OF
IT
NOTE
THAT
WHEN
A
SHARED
LIBRARY
IS
LOADED
OR
USED
THE
ENTIRE
LI
BRARY
IS
NOT
READ
INTO
MEMORY
IN
A
SINGLE
BLOW
IT
IS
PAGED
IN
PAGE
BY
PAGE
AS
NEEDED
SO
FUNCTIONS
THAT
ARE
NOT
CALLED
WILL
NOT
BE
BROUGHT
INTO
RAM
IN
ADDITION
TO
MAKING
EXECUTABLE
FILES
SMALLER
AND
SAVING
SPACE
IN
MEMORY
SHARED
LIBRARIES
HAVE
ANOTHER
ADVANTAGE
IF
A
FUNCTION
IN
A
SHARED
LIBRARY
IS
UP
DATED
TO
REMOVE
A
BUG
IT
IS
NOT
NECESSARY
TO
RECOMPILE
THE
PROGRAMS
THAT
CALL
IT
THE
OLD
BINARIES
CONTINUE
TO
WORK
THIS
FEATURE
IS
ESPECIALLY
IMPORTANT
FOR
COM
MERCIAL
SOFTWARE
WHERE
THE
SOURCE
CODE
IS
NOT
DISTRIBUTED
TO
THE
CUSTOMER
FOR
EX
AMPLE
IF
MICROSOFT
FINDS
AND
FIXES
A
SECURITY
ERROR
IN
SOME
STANDARD
DLL
WIN
DOWS
UPDATE
WILL
DOWNLOAD
THE
NEW
DLL
AND
REPLACE
THE
OLD
ONE
AND
ALL
PRO
GRAMS
THAT
USE
THE
DLL
WILL
AUTOMATICALLY
USE
THE
NEW
VERSION
THE
NEXT
TIME
THEY
ARE
LAUNCHED
SHARED
LIBRARIES
COJNE
WITH
ONE
LITTLE
PROBLEM
THAT
HAS
TO
BE
SOLVED
HOWEVER
THE
PROBLEM
IS
ILLUSTRATED
IN
FIG
HERE
WE
SEE
TWO
PROCESSES
SHARING
A
LI
BRARY
OF
SIZE
KB
ASSUMING
EACH
BOX
IS
KB
HOWEVER
THE
LIBRARY
IS
LOCATED
AT
A
DIFFERENT
ADDRESS
IN
EACH
PROCESS
PRESUMABLY
BECAUSE
THE
PROGRAMS
THEM
SELVES
ARE
NOT
THE
SAME
SIZE
IN
PROCESS
THE
LIBRARY
STARTS
AT
ADDRESS
IN
PROCESS
IT
STARTS
AT
SUPPOSE
THAT
THE
FIRST
THING
THE
FIRST
FUNCTION
IN
THE
LI
BRARY
HAS
TO
DO
IS
JUMP
TO
ADDRESS
IN
THE
LIBRARY
IF
THE
LIBRARY
WERE
NOT
SHARED
IT
COULD
BE
RELOCATED
ON
THE
FLY
AS
IT
WAS
LOADED
SO
THAT
THE
JUMP
IN
PROCESS
COULD
BE
TO
VIRTUAL
ADDRESS
NOTE
THAT
THE
PHYSICAL
ADDRESS
IN
THE
RAM
WHERE
THE
LIBRARY
IS
LOCATED
DOES
NOT
MATTER
SINCE
ALL
THE
PAGES
ARE
MAPPED
FROM
VIRTUAL
TO
PHYSICAL
ADDRESSES
BY
THE
MMU
HARDWARE
HOWEVER
SINCE
THE
LIBRARY
IS
SHARED
RELOCATION
ON
THE
FLY
WILL
NOT
WORK
AFTER
ALL
WHEN
THE
FIRST
FUNCTION
IS
CALLED
BY
PROCESS
AT
ADDRESS
THE
JUMP
IN
STRUCTION
HAS
TO
GO
TO
NOT
THIS
IS
THE
LITTLE
PROBLEM
ONE
WAY
TO
SOLVE
IT
IS
TO
USE
COPY
ON
WRITE
AND
CREATE
NEW
PAGES
FOR
EACH
PROCESS
SHARING
THE
LIBRARY
RELOCATING
THEM
ON
THE
FLY
AS
THEY
ARE
CREATED
BUT
THIS
SCHEME
DEFEATS
THE
PURPOSE
OF
SHARING
THE
LIBRARY
OF
COURSE
SEC
DESIGN
ISSUES
FOR
PAGING
SYSTEMS
PROCESS
RAM
PROCESS
FIGURE
A
SHARED
LIBRARY
BEING
USED
BY
TWO
PROCESSES
A
BETTER
SOLUTION
IS
TO
COMPILE
SHARED
LIBRARIES
WITH
A
SPECIAL
COMPILER
FLAG
TELLING
THE
COMPILER
NOT
TO
PRODUCE
ANY
INSTRUCTIONS
THAT
USE
ABSOLUTE
ADDRESSES
INSTEAD
ONLY
INSTRUCTIONS
USING
RELATIVE
ADDRESSES
ARE
USED
FOR
EXAMPLE
THERE
IS
ALMOST
ALWAYS
AN
INSTRUCTION
THAT
SAYS
JUMP
FORWARD
OR
BACKWARD
BY
N
BYTES
AS
OPPOSED
TO
AN
INSTRUCTION
THAT
GIVES
A
SPECIFIC
ADDRESS
TO
JUMP
TO
THIS
INSTRUC
TION
WORKS
CORRECTLY
NO
MATTER
WHERE
THE
SHARED
LIBRARY
IS
PLACED
IN
THE
VIRTUAL
AD
DRESS
SPACE
BY
AVOIDING
ABSOLUTE
ADDRESSES
THE
PROBLEM
CAN
BE
SOLVED
CODE
THAT
USES
ONLY
RELATIVE
OFFSETS
IS
CALLED
POSITION
INDEPENDENT
CODE
MAPPED
FILES
SHARED
LIBRARIES
ARE
REALLY
A
SPECIAL
CASE
OF
A
MORE
GENERAL
FACILITY
CALLED
MEMORY
MAPPED
FILES
THE
IDEA
HERE
IS
THAT
A
PROCESS
CAN
ISSUE
A
SYSTEM
CALL
TO
MAP
A
FILE
ONTO
A
PORTION
OF
ITS
VIRTUAL
ADDRESS
SPACE
IN
MOST
IMPLEMENTATIONS
NO
PAGES
ARE
BROUGHT
IN
AT
THE
TIME
OF
THE
MAPPING
BUT
AS
PAGES
ARE
TOUCHED
THEY
ARE
DEMAND
PAGED
IN
ONE
AT
A
TIME
USING
THE
DISK
FILE
AS
THE
BACKING
STORE
WHEN
THE
PROCESS
EXITS
OR
EXPLICITLY
UNMAPS
THE
FILE
ALL
THE
MODIFIED
PAGES
ARE
WRITTEN
BACK
TO
THE
FILE
MAPPED
FILES
PROVIDE
AN
ALTERNATIVE
MODEL
FOR
I
O
INSTEAD
OF
DOING
READS
AND
WRITES
THE
FILE
CAN
BE
ACCESSED
AS
A
BIG
CHARACTER
ARRAY
IN
MEMORY
IN
SOME
SITUA
TIONS
PROGRAMMERS
FIND
THIS
MODEL
MORE
CONVENIENT
IF
TWO
OR
MORE
PROCESSES
MAP
ONTO
THE
SAME
FILE
AT
THE
SAME
TIME
THEY
CAN
COMMUNICATE
OVER
SHARED
MEMORY
WRITES
DONE
BY
ONE
PROCESS
TO
THE
SHARED
MEMORY
ARE
IMMEDIATELY
VISIBLE
WHEN
THE
OTHER
ONE
READS
FROM
THE
PART
OF
ITS
VIR
TUAL
ADDRESS
SPACED
MAPPED
ONTO
THE
FILE
THIS
MECHANISM
THUS
PROVIDES
A
HIGH
BANDWIDTH
CHANNEL
BETWEEN
PROCESSES
AND
IS
OFTEN
USED
AS
SUCH
EVEN
TO
THE
EXTENT
OF
MAPPING
A
SCRATCH
FILE
NOW
IT
SHOULD
BE
CLEAR
THAT
IF
MEMORY
MAPPED
FILES
ARE
AVAILABLE
SHARED
LIBRARIES
CAN
USE
THIS
MECHANISM
MEMORY
MANAGEMENT
CHAP
CLEANING
POLICY
PAGING
WORKS
BEST
WHEN
THERE
ARE
PLENTY
OF
FREE
PAGE
FRAMES
THAT
CAN
BE
CLAIMED
AS
PAGE
FAULTS
OCCUR
IF
EVERY
PAGE
FRAME
IS
FULL
AND
FURTHERMORE
MODI
FIED
BEFORE
A
NEW
PAGE
CAN
BE
BROUGHT
IN
AN
OLD
PAGE
MUST
FIRST
BE
WRITTEN
TO
DISK
TO
ENSURE
A
PLENTIFUL
SUPPLY
OF
FREE
PAGE
FRAMES
MANY
PAGING
SYSTEMS
HAVE
A
BACKGROUND
PROCESS
CALLED
THE
PAGING
DAEMON
THAT
SLEEPS
MOST
OF
THE
TIME
BUT
IS
AWAKENED
PERIODICALLY
TO
INSPECT
THE
STATE
OF
MEMORY
IF
TOO
FEW
PAGE
FRAMES
ARE
FREE
THE
PAGING
DAEMON
BEGINS
SELECTING
PAGES
TO
EVICT
USING
SOME
PAGE
REPLACEMENT
ALGORITHM
IF
THESE
PAGES
HAVE
BEEN
MODIFIED
SINCE
BEING
LOADED
THEY
ARE
WRITTEN
TO
DISK
IN
ANY
EVENT
THE
PREVIOUS
CONTENTS
OF
THE
PAGE
ARE
REMEMBERED
IN
THE
EVENT
ONE
OF
THE
EVICTED
PAGES
IS
NEEDED
AGAIN
BEFORE
ITS
FRAME
HAS
BEEN
OVERWRITTEN
IT
CAN
BE
RECLAIMED
BY
REMOVING
IT
FROM
THE
POOL
OF
FREE
PAGE
FRAMES
KEEPING
A
SUPPLY
OF
PAGE
FRAMES
AROUND
YIELDS
BETTER
PERFORMANCE
THAN
USING
ALL
OF
MEMORY
AND
THEN
TRYING
TO
FIND
A
FRAME
AT
THE
MOMENT
IT
IS
NEEDED
AT
THE
VERY
LEAST
THE
PAGING
DAEMON
ENSURES
THAT
ALL
THE
FREE
FRAMES
ARE
CLEAN
SO
THEY
NEED
NOT
BE
WRIT
TEN
TO
DISK
IN
A
BIG
HURRY
WHEN
THEY
ARE
REQUIRED
ONE
WAY
TO
IMPLEMENT
THIS
CLEANING
POLICY
IS
WITH
A
TWO
HANDED
CLOCK
THE
FRONT
HAND
IS
CONTROLLED
BY
THE
PAGING
DAEMON
WHEN
IT
POINTS
TO
A
DIRTY
PAGE
THAT
PAGE
IS
WRITTEN
BACK
TO
DISK
AND
THE
FRONT
HAND
IS
ADVANCED
WHEN
IT
POINTS
TO
A
CLEAN
PAGE
IT
IS
JUST
ADVANCED
THE
BACK
HAND
IS
USED
FOR
PAGE
REPLACEMENT
AS
IN
THE
STANDARD
CLOCK
ALGORITHM
ONLY
NOW
THE
PROBABILITY
OF
THE
BACK
HAND
HITTING
A
CLEAN
PAGE
IS
INCREASED
DUE
TO
THE
WORK
OF
THE
PAGING
DAEMON
VIRTUAL
MEMORY
INTERFACE
UP
UNTIL
NOW
OUR
WHOLE
DISCUSSION
HAS
ASSUMED
THAT
VIRTUAL
MEMORY
IS
TRANS
PARENT
TO
PROCESSES
AND
PROGRAMMERS
THAT
IS
ALL
THEY
SEE
IS
A
LARGE
VIRTUAL
ADDRESS
SPACE
ON
A
COMPUTER
WITH
A
SMALL
ER
PHYSICAL
MEMORY
WITH
MANY
SYSTEMS
THAT
IS
TRUE
BUT
IN
SOME
ADVANCED
SYSTEMS
PROGRAMMERS
HAVE
SOME
CONTROL
OVER
THE
MEMORY
MAP
AND
CAN
USE
IT
IN
NONTRADITIONAL
WAYS
TO
ENHANCE
PROGRAM
BEHAVIOR
IN
THIS
SECTION
WE
WILL
BRIEFLY
LOOK
AT
A
FEW
OF
THESE
ONE
REASON
FOR
GIVING
PROGRAMMERS
CONTROL
OVER
THEIR
MEMORY
MAP
IS
TO
ALLOW
TWO
OR
MORE
PROCESSES
TO
SHARE
THE
SAME
MEMORY
IF
PROGRAMMERS
CAN
NAME
REGIONS
OF
THEIR
MEMORY
IT
MAY
BE
POSSIBLE
FOR
ONE
PROCESS
TO
GIVE
ANOTHER
PROCESS
THE
NAME
OF
A
MEMORY
REGION
SO
THAT
PROCESS
CAN
ALSO
MAP
IT
IN
WITH
TWO
OR
MORE
PROCESSES
SHARING
THE
SAME
PAGES
HIGH
BANDWIDTH
SHARING
BECOMES
POS
SIBLE
ONE
PROCESS
WRITES
INTO
THE
SHARED
MEMORY
AND
ANOTHER
ONE
READS
FROM
IT
SHARING
OF
PAGES
CAN
ALSO
BE
USED
TO
IMPLEMENT
A
HIGH
PERFORMANCE
MES
SAGE
PASSING
SYSTEM
NORMALLY
WHEN
MESSAGES
ARE
PASSED
THE
DATA
ARE
COPIED
FROM
ONE
ADDRESS
SPACE
TO
ANOTHER
AT
CONSIDERABLE
COST
IF
PROCESSES
CAN
CONTROL
THEIR
PAGE
MAP
A
MESSAGE
CAN
BE
PASSED
BY
HAVING
THE
SENDING
PROCESS
UNMAP
THE
SEC
DESIGN
ISSUES
FOR
PAGING
SYSTEMS
PAGE
CONTAINING
THE
MESSAGE
AND
THE
RECEIVING
PROCESS
MAPPING
THEM
IN
HERE
ONLY
THE
PAGE
NAMES
HAVE
TO
BE
COPIED
INSTEAD
OF
ALL
THE
DATA
YET
ANOTHER
ADVANCED
MEMORY
MANAGEMENT
TECHNIQUE
IS
DISTRIBUTED
SHARED
MEMORY
FEELEY
ET
AL
LI
LI
AND
HUDAK
AND
ZEKAUSKAS
ET
AL
THE
IDEA
HERE
IS
TO
ALLOW
MULTIPLE
PROCESSES
OVER
A
NETWORK
TO
SHARE
A
SET
OF
PAGES
POSSIBLY
BUT
NOT
NECESSARILY
AS
A
SINGLE
SHARED
LINEAR
ADDRESS
SPACE
WHEN
A
PROCESS
REFERENCES
A
PAGE
THAT
IS
NOT
CURRENTLY
MAPPED
IN
IT
GETS
A
PAGE
FAULT
THE
PAGE
FAULT
HANDLER
WHICH
MAY
BE
IN
THE
KERNEL
OR
IN
USER
SPACE
THEN
LOCATES
THE
MACHINE
HOLDING
THE
PAGE
AND
SENDS
IT
A
MESSAGE
ASKING
IT
TO
UNMAP
THE
PAGE
AND
SEND
IT
OVER
THE
NETWORK
WHEN
THE
PAGE
ARRIVES
IT
IS
MAPPED
IN
AND
THE
FAULTING
INSTRUCTION
IS
RESTARTED
WE
WILL
EXAMINE
DISTRIBUTED
SHARED
MEMORY
IN
MORE
DETAIL
IN
CHAP
IMPLEMENTATIO
N
ISSUE
IMPLEMENTED
OF
VIRTUAL
MEMORY
SYSTEMS
HAVE
TO
MAKE
CHOICES
AMONG
THE
MAJOR
THEORETICAL
ALGORITHMS
SUCH
AS
SECOND
CHANCE
VERSUS
AGING
LOCAL
VERSUS
GLOBAL
PAGE
ALLOCATION
AND
DEMAND
PAGING
VERSUS
PREPAGING
BUT
THEY
ALSO
HAVE
TO
BE
AWARE
OF
A
NUMBER
OF
PRACTICAL
IMPLEMENTATION
ISSUES
AS
WELL
IN
THIS
SEC
TION
WE
WILL
TAKE
A
LOOK
AT
A
FEW
OF
THE
COMMON
PROBLEMS
AND
SOME
SOLUTIONS
OPERATING
SYSTEM
INVOLVEMENT
WITH
PAGING
THERE
ARE
FOUR
TIMES
WHEN
THE
OPERATING
SYSTEM
HAS
PAGING
RELATED
WORK
TO
DO
PROCESS
CREATION
TIME
PROCESS
EXECUTION
TIME
PAGE
FAULT
TIME
AND
PROCESS
TERMINATION
TIME
WE
WILL
NOW
BRIEFLY
EXAMINE
EACH
OF
THESE
TO
SEE
WHAT
HAS
TO
BE
DONE
WHEN
A
NEW
PROCESS
IS
CREATED
IN
A
PAGING
SYSTEM
THE
OPERATING
SYSTEM
HAS
TO
DETERMINE
HOW
LARGE
THE
PROGRAM
AND
DATA
WILL
BE
INITIALLY
AND
CREATE
A
PAGE
TABLE
FOR
THEM
SPACE
HAS
TO
BE
ALLOCATED
IN
MEMORY
FOR
THE
PAGE
TABLE
AND
IT
HAS
TO
BE
INITIALIZED
THE
PAGE
TABLE
NEED
NOT
BE
RESIDENT
WHEN
THE
PROCESS
IS
SWAPPED
OUT
BUT
HAS
TO
BE
IN
MEMORY
WHEN
THE
PROCESS
IS
RUNNING
IN
ADDITION
SPACE
HAS
TO
BE
ALLOCATED
IN
THE
SWAP
AREA
ON
DISK
SO
THAT
WHEN
A
PAGE
IS
SWAPPED
OUT
IT
HAS
SOMEWHERE
TO
GO
THE
SWAP
AREA
ALSO
HAS
TO
BE
INITIALIZED
WITH
PROGRAM
TEXT
AND
DATA
SO
THAT
WHEN
THE
NEW
PROCESS
STARTS
GETTING
PAGE
FAULTS
THE
PAGES
CAN
BE
BROUGHT
IN
SOME
SYSTEMS
PAGE
THE
PROGRAM
TEXT
DIRECTIY
FROM
THE
EXECUTABLE
FILE
THUS
SAVING
DISK
SPACE
AND
INITIALIZATION
TIME
FINALLY
INFORMATION
ABOUT
THE
PAGE
TABLE
AND
SWAP
AREA
ON
DISK
MUST
BE
RECORDED
IN
THE
PROCESS
TABLE
WHEN
A
PROCESS
IS
SCHEDULED
FOR
EXECUTION
THE
MMU
HAS
TO
BE
RESET
FOR
THE
NEW
PROCESS
AND
THE
TLB
FLUSHED
TO
GET
RID
OF
TRACES
OF
THE
PREVIOUSLY
EXECUTING
PROCESS
THE
NEW
PROCESS
PAGE
TABLE
HAS
TO
BE
MADE
CURRENT
USUALLY
BY
COPYING
IT
OR
A
POINTER
TO
IT
TO
SOME
HARDWARE
REGISTER
OPTIONALLY
SOME
OR
ALL
OF
THE
MEMORY
MANAGEMENT
CHAP
PROCESS
PAGES
CAN
BE
BROUGHT
INTO
MEMORY
TO
REDUCE
THE
NUMBER
OF
PAGE
FAULTS
INITIALLY
E
G
IT
IS
CERTAIN
THAT
THE
PAGE
POINTED
TO
BY
THE
PC
WILL
BE
NEEDED
WHEN
A
PAGE
FAULT
OCCURS
THE
OPERATING
SYSTEM
HAS
TO
READ
OUT
HARDWARE
REG
ISTERS
TO
DETERMINE
WHICH
VIRTUAL
ADDRESS
CAUSED
THE
FAULT
FROM
THIS
INFORMATION
IT
MUST
COMPUTE
WHICH
PAGE
IS
NEEDED
AND
LOCATE
THAT
PAGE
ON
DISK
IT
MUST
THEN
FIND
AN
AVAILABLE
PAGE
FRAME
TO
PUT
THE
NEW
PAGE
EVICTING
SOME
OLD
PAGE
IF
NEED
BE
THEN
IT
MUST
READ
THE
NEEDED
PAGE
INTO
THE
PAGE
FRAME
FINALLY
IT
MUST
BACK
UP
THE
PROGRAM
COUNTER
TO
HAVE
IT
POINT
TO
THE
FAULTING
INSTRUCTION
AND
LET
THAT
IN
STRUCTION
EXECUTE
AGAIN
WHEN
A
PROCESS
EXITS
THE
OPERATING
SYSTEM
MUST
RELEASE
ITS
PAGE
TABLE
ITS
PAGES
AND
THE
DISK
SPACE
THAT
THE
PAGES
OCCUPY
WHEN
THEY
ARE
ON
DISK
IF
SOME
OF
THE
PAGES
ARE
SHARED
WITH
OTHER
PROCESSES
THE
PAGES
IN
MEMORY
AND
ON
DISK
CAN
ONLY
BE
RELEASED
WHEN
THE
LAST
PROCESS
USING
THEM
HAS
TERMINATED
PAGE
FAULT
HANDLING
WE
ARE
FINALLY
IN
A
POSITION
TO
DESCRIBE
IN
DETAIL
WHAT
HAPPENS
ON
A
PAGE
FAULT
THE
SEQUENCE
OF
EVENTS
IS
AS
FOLLOWS
THE
HARDWARE
TRAPS
TO
THE
KERNEL
SAVING
THE
PROGRAM
COUNTER
ON
THE
STACK
ON
MOST
MACHINES
SOME
INFORMATION
ABOUT
THE
STATE
OF
THE
CURRENT
INSTRUCTION
IS
SAVED
IN
SPECIAL
CPU
REGISTERS
AN
ASSEMBLY
CODE
ROUTINE
IS
STARTED
TO
SAVE
THE
GENERAL
REGISTERS
AND
OTHER
VOLATILE
INFORMATION
TO
KEEP
THE
OPERATING
SYSTEM
FROM
DES
TROYING
IT
THIS
ROUTINE
CALLS
THE
OPERATING
SYSTEM
AS
A
PROCEDURE
THE
OPERATING
SYSTEM
DISCOVERS
THAT
A
PAGE
FAULT
HAS
OCCURRED
AND
TRIES
TO
DISCOVER
WHICH
VIRTUAL
PAGE
IS
NEEDED
OFTEN
ONE
OF
THE
HARD
WARE
REGISTERS
CONTAINS
THIS
INFORMATION
IF
NOT
THE
OPERATING
SYSTEM
MUST
RETRIEVE
THE
PROGRAM
COUNTER
FETCH
THE
INSTRUCTION
AND
PARSE
IT
IN
SOFTWARE
TO
FIGURE
OUT
WHAT
IT
WAS
DOING
WHEN
THE
FAULT
HIT
ONCE
THE
VIRTUAL
ADDRESS
THAT
CAUSED
THE
FAULT
IS
KNOWN
THE
SYSTEM
CHECKS
TO
SEE
IF
THIS
ADDRESS
IS
VALID
AND
THE
PROTECTION
CONSISTENT
WITH
THE
ACCESS
IF
NOT
THE
PROCESS
IS
SENT
A
SIGNAL
OR
KILLED
IF
THE
ADDRESS
IS
VALID
AND
NO
PROTECTION
FAULT
HAS
OCCURRED
THE
SYSTEM
SEC
IMPLEMENTATION
ISSUES
AS
SOON
AS
THE
PAGE
FRAME
IS
CLEAN
EITHER
IMMEDIATELY
OR
AFTER
IT
IS
WRITTEN
TO
DISK
THE
OPERATING
SYSTEM
LOOKS
UP
THE
DISK
ADDRESS
WHERE
THE
NEEDED
PAGE
IS
AND
SCHEDULES
A
DISK
OPERATION
TO
BRING
IT
IN
WHILE
THE
PAGE
IS
BEING
LOADED
THE
FAULTING
PROCESS
IS
STILL
SUSPENDED
AND
ANOTHER
USER
PROCESS
IS
RUN
IF
ONE
IS
AVAILABLE
WHEN
THE
DISK
INTERRUPT
INDICATES
THAT
THE
PAGE
HAS
ARRIVED
THE
PAGE
TABLES
ARE
UPDATED
TO
REFLECT
ITS
POSITION
AND
THE
FRAME
IS
MARKED
AS
BEING
IN
NORMAL
STATE
THE
FAULTING
INSTRUCTION
IS
BACKED
UP
TO
THE
STATE
IT
HAD
WHEN
IT
BEGAN
AND
THE
PROGRAM
COUNTER
IS
RESET
TO
POINT
TO
THAT
INSTRUCTION
THE
FAULTING
PROCESS
IS
SCHEDULED
AND
THE
OPERATING
SYSTEM
RETURNS
TO
THE
ASSEMBLY
LANGUAGE
ROUTINE
THAT
CALLED
IT
THIS
ROUTINE
RELOADS
THE
REGISTERS
AND
OTHER
STATE
INFORMATION
AND
RE
TURNS
TO
USER
SPACE
TO
CONTINUE
EXECUTION
AS
IF
NO
FAULT
HAD
OCCURRED
INSTRUCTION
BACKUP
WHEN
A
PROGRAM
REFERENCES
A
PAGE
THAT
IS
NOT
IN
MEMORY
THE
INSTRUCTION
CAUS
ING
THE
FAULT
IS
STOPPED
PARTWAY
THROUGH
AND
A
TRAP
TO
THE
OPERATING
SYSTEM
OCCURS
AFTER
THE
OPERATING
SYSTEM
HAS
FETCHED
THE
PAGE
NEEDED
IT
MUST
RESTART
THE
INSTRUC
TION
CAUSING
THE
TRAP
THIS
IS
EASIER
SAID
THAN
DONE
TO
SEE
THE
NATURE
OF
THIS
PROBLEM
AT
ITS
WORST
CONSIDER
A
CPU
THAT
HAS
INSTRUC
TIONS
WITH
TWO
ADDRESSES
SUCH
AS
THE
MOTOROLA
WIDELY
USED
IN
EMBEDDED
SYSTEMS
THE
INSTRUCTION
MOV
L
IS
BYTES
FOR
EXAMPLE
SEE
FIG
IN
ORDER
TO
RESTART
THE
INSTRUCTION
THE
OPER
ATING
SYSTEM
MUST
DETERMINE
WHERE
THE
FIRST
BYTE
OF
THE
INSTRUCTION
IS
THE
VALUE
OF
THE
PROGRAM
COUNTER
AT
THE
TIME
OF
THE
TRAP
DEPENDS
ON
WHICH
OPERAND
FAULTED
AND
HOW
THE
CPU
MICROCODE
HAS
BEEN
IMPLEMENTED
MOVE
L
BITS
CHECKS
TO
SEE
IF
A
PAGE
FRAME
IS
FREE
IF
NO
FRAMES
ARE
FREE
THE
PAGE
REPLACEMENT
ALGORITHM
IS
RUN
TO
SELECT
A
VICTIM
IF
THE
PAGE
FRAME
SELECTED
IS
DIRTY
THE
PAGE
IS
SCHEDULED
FOR
TRANSFER
TO
THE
DISK
AND
A
CONTEXT
SWITCH
TAKES
PLACE
SUSPENDING
THE
FAULTING
MOVE
OPCODE
FIRST
OPERAND
SECOND
OPERAND
PROCESS
AND
LETTING
ANOTHER
ONE
RUN
UNTIL
THE
DISK
TRANSFER
HAS
COM
PLETED
IN
ANY
EVENT
THE
FRAME
IS
MARKED
AS
BUSY
TO
PREVENT
IT
FROM
BEING
USED
FOR
ANOTHER
PURPOSE
FIGURE
AN
INSTRUCTION
CAUSING
A
PAGE
FAULT
IN
FIG
WE
HAVE
AN
INSTRUCTION
STARTING
AT
ADDRESS
THAT
MAKES
THREE
MEMORY
REFERENCES
THE
INSTRUCTION
WORD
ITSELF
AND
TWO
OFFSETS
FOR
THE
OPERANDS
MEMORY
MANAGEMENT
CHAP
DEPENDING
ON
WHICH
OF
THESE
THREE
MEMORY
REFERENCES
CAUSED
THE
PAGE
FAULT
THE
PROGRAM
COUNTER
MIGHT
BE
OR
AT
THE
TIME
OF
THE
FAULT
IT
IS
FRE
QUENTLY
IMPOSSIBLE
FOR
THE
OPERATING
SYSTEM
TO
DETERMINE
UNAMBIGUOUSLY
WHERE
THE
INSTRUCTION
BEGAN
IF
THE
PROGRAM
COUNTER
IS
AT
THE
TIME
OF
THE
FAULT
THE
OPERATING
SYSTEM
HAS
NO
WAY
OF
TELLING
WHETHER
THE
WORD
IN
IS
A
MEMORY
AD
DRESS
ASSOCIATED
WITH
AN
INSTRUCTION
AT
E
G
THE
LOCATION
OF
AN
OPERAND
OR
AN
INSTRUCTION
OPCODE
BAD
AS
THIS
PROBLEM
MAY
BE
IT
COULD
HAVE
BEEN
WORSE
SOME
AD
DRESSING
MODES
USE
AUTOINCREMENTING
WHICH
MEANS
THAT
A
SIDE
EFFECT
OF
EXECUTING
THE
INSTRUCTION
IS
TO
INCREMENT
ONE
OR
MORE
REGISTERS
INSTRUCTIONS
THAT
USE
AUTOIN
CREMENT
MODE
CAN
ALSO
FAULT
DEPENDING
ON
THE
DETAILS
OF
THE
MICROCODE
THE
INCREMENT
MAY
BE
DONE
BEFORE
THE
MEMORY
REFERENCE
IN
WHICH
CASE
THE
OPERATING
SYSTEM
MUST
DECREMENT
THE
REGISTER
IN
SOFTWARE
BEFORE
RESTARTING
THE
INSTRUCTION
OR
THE
AUTOINCREMENT
MAY
BE
DONE
AFTER
THE
MEMORY
REFERENCE
IN
WHICH
CASE
IT
WILL
NOT
HAVE
BEEN
DONE
AT
THE
TIME
OF
THE
TRAP
AND
MUST
NOT
BE
UNDONE
BY
THE
OP
ERATING
SYSTEM
AUTODECREMENT
MODE
ALSO
EXISTS
AND
CAUSES
A
SIMILAR
PROBLEM
THE
PRECISE
DETAILS
OF
WHETHER
AUTOINCREMENTS
AND
AUTODECREMENTS
HAVE
OR
HAVE
NOT
BEEN
DONE
BEFORE
THE
CORRESPONDING
MEMORY
REFERENCES
MAY
DIFFER
FROM
IN
STRUCTION
TO
INSTRUCTION
AND
FROM
CPU
MODEL
TO
CPU
MODEL
FORTUNATELY
ON
SOME
MACHINES
THE
CPU
DESIGNERS
PROVIDE
A
SOLUTION
USUALLY
IN
THE
FORM
OF
A
HIDDEN
INTERNAL
REGISTER
INTO
WHICH
THE
PROGRAM
COUNTER
IS
COPIED
JUST
BEFORE
EACH
INSTRUCTION
IS
EXECUTED
THESE
MACHINES
MAY
ALSO
HAVE
A
SECOND
REGISTER
TELLING
WHICH
REGISTERS
HAVE
ALREADY
BEEN
AUTOINCREMENTED
OR
AUTODECRE
MENTED
AND
BY
HOW
MUCH
GIVEN
THIS
INFORMATION
THE
OPERATING
SYSTEM
CAN
UNAMBIGUOUSLY
UNDO
ALL
THE
EFFECTS
OF
THE
FAULTING
INSTRUCTION
SO
THAT
IT
CAN
BE
RE
STARTED
IF
THIS
INFORMATION
IS
NOT
AVAILABLE
THE
OPERATING
SYSTEM
HAS
TO
JUMP
THROUGH
HOOPS
TO
FIGURE
OUT
WHAT
HAPPENED
AND
HOW
TO
REPAIR
IT
IT
IS
AS
THOUGH
THE
HARDWARE
DESIGNERS
WERE
UNABLE
TO
SOLVE
THE
PROBLEM
SO
THEY
THREW
UP
THEIR
HANDS
AND
TOLD
THE
OPERATING
SYSTEM
WRITERS
TO
DEAL
WITH
IT
NICE
GUYS
LOCKING
PAGES
IN
MEMORY
ALTHOUGH
WE
HAVE
NOT
DISCUSSED
I
O
MUCH
IN
THIS
CHAPTER
THE
FACT
THAT
A
COM
PUTER
HAS
VIRTUAL
MEMORY
DOES
NOT
MEAN
THAT
I
O
IS
ABSENT
VIRTUAL
MEMORY
AND
I
O
INTERACT
IN
SUBTLE
WAYS
CONSIDER
A
PROCESS
THAT
HAS
JUST
ISSUED
A
SYSTEM
CALL
TO
READ
FROM
SOME
FILE
OR
DEVICE
INTO
A
BUFFER
WITHIN
ITS
ADDRESS
SPACE
WHILE
WAIT
ING
FOR
THE
I
O
TO
COMPLETE
THE
PROCESS
IS
SUSPENDED
AND
ANOTHER
PROCESS
IS
ALLOW
ED
TO
RUN
THIS
OTHER
PROCESS
GETS
A
PAGE
FAULT
IF
THE
PAGING
ALGORITHM
IS
GLOBAL
THERE
IS
A
SMALL
BUT
NONZERO
CHANCE
THAT
THE
PAGE
CONTAINING
THE
I
O
BUFFER
WILL
BE
CHOSEN
TO
BE
REMOVED
FROM
MEMORY
IF
AN
I
O
DEVICE
IS
CURRENTLY
IN
THE
PROCESS
OF
DOING
A
DMA
TRANSFER
TO
THAT
PAGE
REMOVING
IT
WILL
CAUSE
PART
OF
THE
DATA
TO
BE
WRITTEN
IN
THE
BUFFER
WHERE
THEY
BE
LONG
AND
PART
OF
THE
DATA
TO
BE
WRITTEN
OVER
THE
JUST
LOADED
PAGE
ONE
SOLUTION
TO
SEC
IMPLEMENTATION
ISSUES
THIS
PROBLEM
IS
TO
LOCK
PAGES
ENGAGED
IN
I
O
IN
MEMORY
SO
THAT
THEY
WILL
NOT
BE
RE
MOVED
LOCKING
A
PAGE
IS
OFTEN
CALLED
PINNING
IT
IN
MEMORY
ANOTHER
SOLUTION
IS
TO
DO
ALL
I
O
TO
KERNEL
BUFFERS
AND
THEN
COPY
THE
DATA
TO
USER
PAGES
LATER
BACKING
STORE
IN
OUR
DISCUSSION
OF
PAGE
REPLACEMENT
ALGORITHMS
WE
SAW
HOW
A
PAGE
IS
SELECTED
FOR
REMOVAL
WE
HAVE
NOT
SAID
MUCH
ABOUT
WHERE
ON
THE
DISK
IT
IS
PUT
WHEN
IT
IS
PAGED
OUT
LET
US
NOW
DESCRIBE
SOME
OF
THE
ISSUES
RELATED
TO
DISK
MAN
AGEMENT
THE
SIMPLEST
ALGORITHM
FOR
ALLOCATING
PAGE
SPACE
ON
THE
DISK
IS
TO
HAVE
A
SPE
CIAL
SWAP
PARTITION
ON
THE
DISK
OR
EVEN
BETTER
ON
A
SEPARATE
DISK
FROM
THE
FDE
SYS
TEM
TO
BALANCE
THE
I
O
LOAD
MOST
UNIX
SYSTEMS
WORK
LIKE
THIS
THIS
PARTITION
DOES
NOT
HAVE
A
NORMAL
FDE
SYSTEM
ON
IT
WHICH
ELIMINATES
ALL
THE
OVERHEAD
OF
CON
VERTING
OFFSETS
IN
FILES
TO
BLOCK
ADDRESSES
INSTEAD
BLOCK
NUMBERS
RELATIVE
TO
THE
START
OF
THE
PARTITION
ARE
USED
THROUGHOUT
WHEN
THE
SYSTEM
IS
BOOTED
THIS
SWAP
PARTITION
IS
EMPTY
AND
IS
REPRESENTED
IN
MEMORY
AS
A
SINGLE
ENTRY
GIVING
ITS
ORIGIN
AND
SIZE
IN
THE
SIMPLEST
SCHEME
WHEN
THE
FIRST
PROCESS
IS
STARTED
A
CHUNK
OF
THE
PARTITION
AREA
THE
SIZE
OF
THE
FIRST
PROCESS
IS
RESERVED
AND
THE
REMAINING
AREA
REDUCED
BY
THAT
AMOUNT
AS
NEW
PROCESSES
ARE
STARTED
THEY
ARE
ASSIGNED
CHUNKS
OF
THE
SWAP
PARTITION
EQUAL
IN
SIZE
TO
THEIR
CORE
IMAGES
AS
THEY
FINISH
THEIR
DISK
SPACE
IS
FREED
THE
SWAP
PARTITION
IS
MANAGED
AS
A
LIST
OF
FREE
CHUNKS
BETTER
ALGORITHMS
WILL
BE
DISCUSSED
IN
CHAP
ASSOCIATED
WITH
EACH
PROCESS
IS
THE
DISK
ADDRESS
OF
ITS
SWAP
AREA
THAT
IS
WHERE
ON
THE
SWAP
PARTITION
ITS
IMAGE
IS
KEPT
THIS
INFORMATION
IS
KEPT
IN
THE
PROC
ESS
TABLE
CALCULATING
THE
ADDRESS
TO
WRITE
A
PAGE
TO
BECOMES
SIMPLE
JUST
ADD
THE
OFFSET
OF
THE
PAGE
WITHIN
THE
VIRTUAL
ADDRESS
SPACE
TO
THE
START
OF
THE
SWAP
AREA
HOWEVER
BEFORE
A
PROCESS
CAN
START
THE
SWAP
AREA
MUST
BE
INITIALIZED
ONE
WAY
IS
TO
COPY
THE
ENTIRE
PROCESS
IMAGE
TO
THE
SWAP
AREA
SO
THAT
IT
CAN
BE
BROUGHT
IN
AS
NEEDED
THE
OTHER
IS
TO
LOAD
THE
ENTIRE
PROCESS
IN
MEMORY
AND
LET
IT
BE
PAGED
OUT
AS
NEEDED
HOWEVER
THIS
SIMPLE
MODEL
HAS
A
PROBLEM
PROCESSES
CAN
INCREASE
IN
SIZE
AFTER
STARTING
ALTHOUGH
THE
PROGRAM
TEXT
IS
USUALLY
FIXED
THE
DATA
AREA
CAN
SOME
TIMES
GROW
AND
THE
STACK
CAN
ALWAYS
GROW
CONSEQUENTLY
IT
MAY
BE
BETTER
TO
RESERVE
SEPARATE
SWAP
AREAS
FOR
THE
TEXT
DATA
AND
STACK
AND
ALLOW
EACH
OF
THESE
AREAS
TO
CONSIST
OF
MORE
THAN
ONE
CHUNK
ON
THE
DISK
THE
OTHER
EXTREME
IS
TO
ALLOCATE
NOTHING
IN
ADVANCE
AND
ALLOCATE
DISK
SPACE
FOR
EACH
PAGE
WHEN
IT
IS
SWAPPED
OUT
AND
DEALLOCATE
IT
WHEN
IT
IS
SWAPPED
BACK
IN
IN
THIS
WAY
PROCESSES
IN
MEMORY
DO
NOT
TIE
UP
ANY
SWAP
SPACE
THE
DISADVANTAGE
IS
THAT
A
DISK
ADDRESS
IS
NEEDED
IN
MEMORY
TO
KEEP
TRACK
OF
EACH
PAGE
ON
DISK
IN
OTHER
WORDS
THERE
MUST
A
TABLE
PER
PROCESS
TELLING
FOR
EACH
PAGE
ON
DISK
WHERE
IT
IS
THE
TWO
ALTERNATIVES
ARE
SHOWN
IN
FIG
MEMORY
MANAGEMENT
CHAP
MAIN
MEMORY
L
MAIN
MEMORY
A
FIGURE
A
PAGING
TO
A
STATIC
SWAP
AREA
B
BACKING
UP
PAGES
DYNAMICALLY
IN
FIG
A
A
PAGE
TABLE
WITH
EIGHT
PAGES
IS
ILLUSTRATED
PAGES
AND
ARE
IN
MAIN
MEMORY
PAGES
AND
ARE
ON
DISK
THE
SWAP
AREA
ON
DISK
IS
AS
LARGE
AS
THE
PROCESS
VIRTUAL
ADDRESS
SPACE
EIGHT
PAGES
WITH
EACH
PAGE
HAVING
A
FIXED
LOCATION
TO
WHICH
IT
IS
WRITTEN
WHEN
IT
IS
EVICTED
FROM
MAIN
MEMORY
CALCU
LATING
THIS
ADDRESS
REQUIRES
KNOWING
ONLY
WHERE
THE
PROCESS
PAGING
AREA
BEGINS
SINCE
PAGES
ARE
STORED
IN
IT
CONTIGUOUSLY
IN
ORDER
OF
THEIR
VIRTUAL
PAGE
NUMBER
A
PAGE
THAT
IS
IN
MEMORY
ALWAYS
HAS
A
SHADOW
COPY
ON
DISK
BUT
THIS
COPY
MAY
BE
OUT
OF
DATE
IF
THE
PAGE
HAS
BEEN
MODIFIED
SINCE
BEING
LOADED
THE
SHADED
PAGES
IN
MEMORY
INDICATE
PAGES
NOT
PRESENT
IN
MEMORY
THE
SHADED
PAGES
ON
THE
DISK
ARE
IN
PRINCIPLE
SUPERSEDED
BY
THE
COPIES
IN
MEMORY
ALTHOUGH
IF
A
MEMORY
PAGE
HAS
TO
BE
SWAPPED
BACK
TO
DISK
AND
IT
HAS
NOT
BEEN
MODIFIED
SINCE
IT
WAS
LOADED
THE
SHADED
DISK
COPY
WILL
BE
USED
IN
FIG
B
PAGES
DO
NOT
HAVE
FIXED
ADDRESSES
ON
DISK
WHEN
A
PAGE
IS
SWAPPED
OUT
AN
EMPTY
DISK
PAGE
IS
CHOSEN
ON
THE
FLY
AND
THE
DISK
MAP
WHICH
HAS
ROOM
FOR
ONE
DISK
ADDRESS
PER
VIRTUAL
PAGE
IS
UPDATED
ACCORDINGLY
A
PAGE
IN
MEMORY
HAS
NO
COPY
ON
DISK
THEIR
ENTRIES
IN
THE
DISK
MAP
CONTAIN
AN
INVALID
DISK
ADDRESS
OR
A
BIT
MARKING
THEM
AS
NOT
IN
USE
HAVING
A
FIXED
SWAP
PARTITION
IS
NOT
ALWAYS
POSSIBLE
FOR
EXAMPLE
NO
DISK
PARTITIONS
MAY
BE
AVAILABLE
IN
THIS
CASE
ONE
OR
MORE
LARGE
PREALLOCATED
FILES
WITHIN
THE
NORMAL
FILE
SYSTEM
CAN
BE
USED
WINDOWS
USES
THIS
APPROACH
HOWEV
ER
AN
OPTIMIZATION
CAN
BE
USED
HERE
TO
REDUCE
THE
AMOUNT
OF
DISK
SPACE
NEEDED
SINCE
THE
PROGRAM
TEXT
OF
EVERY
PROCESS
CAME
FROM
SOME
EXECUTABLE
FILE
IN
THE
FILE
SYSTEM
THE
EXECUTABLE
FILE
CAN
BE
USED
AS
THE
SWAP
AREA
BETTER
YET
SINCE
THE
PROGRAM
TEXT
IS
GENERALLY
READ
ONLY
WHEN
MEMORY
IS
TIGHT
AND
PROGRAM
PAGES
HAVE
TO
BE
EVICTED
FROM
MEMORY
THEY
ARE
JUST
DISCARDED
AND
READ
IN
AGAIN
FROM
THE
EXECUTABLE
FILE
WHEN
NEEDED
SHARED
LIBRARIES
CAN
ALSO
WORK
THIS
WAY
SEC
IMPLEMENTATION
ISSUES
SEPARATION
OF
POLICY
AND
MECHANISM
AN
IMPORTANT
TOOL
FOR
MANAGING
THE
COMPLEXITY
OF
ANY
SYSTEM
IS
TO
SEPARATE
POLICY
FROM
MECHANISM
THIS
PRINCIPLE
CAN
BE
APPLIED
TO
MEMORY
MANAGEMENT
BY
HAVING
MOST
OF
THE
MEMORY
MANAGER
RUN
AS
A
USER
LEVEL
PROCESS
SUCH
A
SEPARA
TION
WAS
FIRST
DONE
IN
MACH
YOUNG
ET
AL
THE
DISCUSSION
BELOW
IS
LOOSELY
BASED
ON
MACH
A
SIMPLE
EXAMPLE
OF
HOW
POLICY
AND
MECHANISM
CAN
BE
SEPARATED
IS
SHOWN
IN
FIG
HERE
THE
MEMORY
MANAGEMENT
SYSTEM
IS
DIVIDED
INTO
THREE
PARTS
A
LOW
LEVEL
MMU
HANDLER
A
PAGE
FAULT
HANDLER
THAT
IS
PART
OF
THE
KERNEL
AN
EXTERNAL
PAGER
RUNNING
IN
USER
SPACE
ALL
THE
DETAILS
OF
HOW
THE
MMU
WORKS
ARE
ENCAPSULATED
IN
THE
MMU
HANDLER
WHICH
IS
MACHINE
DEPENDENT
CODE
AND
HAS
TO
BE
REWRITTEN
FOR
EACH
NEW
PLATFORM
THE
OPERATING
SYSTEM
IS
PORTED
TO
THE
PAGE
FAULT
HANDLER
IS
MACHINE
INDEPENDENT
CODE
AND
CONTAINS
MOST
OF
THE
MECHANISM
FOR
PAGING
THE
POLICY
IS
LARGELY
DETER
MINED
BY
THE
EXTERNAL
PAGER
WHICH
RUNS
AS
A
USER
PROCESS
FIGURE
PAGE
FAULT
HANDLING
WITH
AN
EXTERNAL
PAGER
WHEN
A
PROCESS
STARTS
UP
THE
EXTERNAL
PAGER
IS
NOTIFIED
IN
ORDER
TO
SET
UP
THE
PROCESS
PAGE
MAP
AND
ALLOCATE
BACKING
STORE
ON
THE
DISK
IF
NEED
BE
AS
THE
PROC
ESS
RUNS
IT
MAY
MAP
NEW
OBJECTS
INTO
ITS
ADDRESS
SPACE
SO
THE
EXTERNAL
PAGER
IS
AGAIN
NOTIFIED
ONCE
THE
PROCESS
STARTS
RUNNING
IT
MAY
GET
A
PAGE
FAULT
THE
FAULT
HANDLER
FIG
URES
OUT
WHICH
VIRTUAL
PAGE
IS
NEEDED
AND
SENDS
A
MESSAGE
TO
THE
EXTERNAL
PAGER
TELLING
IT
THE
PROBLEM
THE
EXTERNAL
PAGER
THEN
READS
THE
NEEDED
PAGE
IN
FROM
THE
MEMORY
MANAGEMENT
CHAP
DISK
AND
COPIES
IT
TO
A
PORTION
OF
ITS
OWN
ADDRESS
SPACE
THEN
IT
TELLS
THE
FAULT
HANDLER
WHERE
THE
PAGE
IS
THE
FAULT
HANDLER
THEN
UNMAPS
THE
PAGE
FROM
THE
EX
TERNAL
PAGER
ADDRESS
SPACE
AND
ASKS
THE
MMU
HANDLER
TO
PUT
IT
INTO
THE
USER
ADDRESS
SPACE
AT
THE
RIGHT
PLACE
THEN
THE
USER
PROCESS
CAN
BE
RESTARTED
THIS
IMPLEMENTATION
LEAVES
OPEN
WHERE
THE
PAGE
REPLACEMENT
ALGORITHM
IS
PUT
IT
WOULD
BE
CLEANEST
TO
HAVE
IT
IN
THE
EXTERNAL
PAGER
BUT
THERE
ARE
SOME
PROB
LEMS
WITH
THIS
APPROACH
PRINCIPAL
AMONG
THESE
IS
THAT
THE
EXTERNAL
PAGER
DOES
NOT
HAVE
ACCESS
TO
THE
R
AND
M
BITS
OF
ALL
THE
PAGES
THESE
BITS
PLAY
A
ROLE
IN
MANY
OF
SEC
SEGMENTATION
VIRTUAL
ADDRES
SPAC
E
FRE
E
ADDRES
SPAC
E
THE
PAGING
ALGORITHMS
THUS
EITHER
SOME
MECHANISM
IS
NEEDED
TO
PASS
THIS
INFOR
MATION
UP
TO
THE
EXTERNAL
PAGER
OR
THE
PAGE
REPLACEMENT
ALGORITHM
MUST
GO
IN
THE
KERNEL
IN
THE
LATTER
CASE
THE
FAULT
HANDLER
TELLS
THE
EXTERNAL
PAGER
WHICH
PAGE
IT
HAS
SELECTED
FOR
EVICTION
AND
PROVIDES
THE
DATA
EITHER
BY
MAPPING
IT
INTO
THE
EXTER
NAL
PAGER
ADDRESS
SPACE
OR
INCLUDING
IT
IN
A
MESSAGE
EITHER
WAY
THE
EXTERNAL
PAGER
WRITES
THE
DATA
TO
DISK
THE
MAIN
ADVANTAGE
OF
THIS
IMPLEMENTATION
IS
MORE
MODULAR
CODE
AND
GREATER
FLEXIBILITY
THE
MAIN
DISADVANTAGE
IS
THE
EXTRA
OVERHEAD
OF
CROSSING
THE
USER
KERNEL
BOUNDARY
SEVERAL
TIMES
AND
THE
OVERHEAD
OF
THE
VARIOUS
MESSAGES
BEING
SENT
BETWEEN
THE
PIECES
OF
THE
SYSTEM
AT
THE
MOMENT
THE
SUBJECT
IS
HIGHLY
CON
TROVERSIAL
BUT
AS
COMPUTERS
GET
FASTER
AND
FASTER
AND
THE
SOFTWARE
GETS
MORE
AND
MORE
COMPLEX
IN
THE
LONG
RUN
SACRIFICING
SOME
PERFORMANCE
FOR
MORE
RELIABLE
SOFTWARE
WILL
PROBABLY
BE
ACCEPTABLE
TO
MOST
IMPLEMENTERS
ALLOCATE
D
TO
THE
PARS
E
TRE
E
CONSTAN
T
TABLSE
I
SPAC
E
CURRENTLY
BEIN
G
USE
D
B
Y
TH
E
PARS
E
TRE
E
SYMBO
L
TABL
E
HA
BUMPE
D
INTO
TH
E
SOURC
E
TEXT
TABLE
SEGMENTATIO
N
THE
VIRTUAL
MEMORY
DISCUSSED
SO
FAR
IS
ONE
DIMENSIONAL
BECAUSE
THE
VIRTUAL
ADDRESSES
GO
FROM
TO
SOME
MAXIMUM
ADDRESS
ONE
ADDRESS
AFTER
ANOTHER
FOR
MANY
PROBLEMS
HAVING
TWO
OR
MORE
SEPARATE
VIRTUAL
ADDRESS
SPACES
MAY
BE
MUCH
BETTER
THAN
HAVING
ONLY
ONE
FOR
EXAMPLE
A
COMPILER
HAS
MANY
TABLES
THAT
ARE
BUILT
UP
AS
COMPILATION
PROCEEDS
POSSIBLY
INCLUDING
THE
SOURCE
TEXT
BEING
SAVED
FOR
THE
PRINTED
LISTING
ON
BATCH
SYSTEMS
THE
SYMBOL
TABLE
CONTAINING
THE
NAMES
AND
ATTRIBUTES
OF
VARIABLES
THE
TABLE
CONTAINING
ALL
THE
INTEGER
AND
FLOATING
POINT
CONSTANTS
USED
THE
PARSE
TREE
CONTAINING
THE
SYNTACTIC
ANALYSIS
OF
THE
PROGRAM
THE
STACK
USED
FOR
PROCEDURE
CALLS
WITHIN
THE
COMPILER
EACH
OF
THE
FIRST
FOUR
TABLES
GROWS
CONTINUOUSLY
AS
COMPILATION
PROCEEDS
THE
LAST
ONE
GROWS
AND
SHRINKS
IN
UNPREDICTABLE
WAYS
DURING
COMPILATION
IN
A
ONE
DIMENSIONAL
MEMORY
THESE
FIVE
TABLES
WOULD
HAVE
TO
BE
ALLOCATED
CONTIGUOUS
CHUNKS
OF
VIRTUAL
ADDRESS
SPACE
AS
IN
FIG
CONSIDER
WHAT
HAPPENS
IF
A
PROGRAM
HAS
A
MUCH
LARGER
THAN
USUAL
NUMBER
OF
VARIABLES
BUT
A
NORMAL
AMOUNT
OF
EVERYTHING
ELSE
THE
CHUNK
OF
ADDRESS
SPACE
ALLOCATED
FOR
THE
SYMBOL
TABLE
MAY
FILL
UP
BUT
THERE
MAY
BE
LOTS
OF
ROOM
IN
THE
OTHER
TABLES
THE
COMPILER
COULD
OF
COURSE
SIMPLY
ISSUE
A
MESSAGE
SAYING
THAT
THE
COMPILATION
CANNOT
CONTINUE
DUE
TO
TOO
MANY
VARIABLES
BUT
DOING
SO
DOES
NOT
SEEM
VERY
SPORTING
WHEN
UNUSED
SPACE
IS
LEFT
IN
THE
OTHER
TABLES
ANOTHER
POSSIBILITY
IS
TO
PLAY
ROBIN
HOOD
TAKING
SPACE
FROM
THE
TABLES
WITH
AN
EXCESS
OF
ROOM
AND
GIVING
IT
TO
THE
TABLES
WITH
LITTLE
ROOM
THIS
SHUFFLING
CAN
BE
DONE
BUT
IT
IS
ANALOGOUS
TO
MANAGING
ONE
OWN
OVERLAYS
A
NUISANCE
AT
BEST
AND
A
GREAT
DEAL
OF
TEDIOUS
UNREWARDING
WORK
AT
WORST
WHAT
IS
REALLY
NEEDED
IS
A
WAY
OF
FREEING
THE
PROGRAMMER
FROM
HAVING
TO
MANAGE
THE
EXPANDING
AND
CONTRACTING
TABLES
IN
THE
SAME
WAY
THAT
VIRTUAL
MEMO
RY
ELIMINATES
THE
WORRY
OF
ORGANIZING
THE
PROGRAM
INTO
OVERLAYS
A
STRAIGHTFORWARD
AND
EXTREMELY
GENERAL
SOLUTION
IS
TO
PROVIDE
THE
MACHINE
WITH
MANY
COMPLETELY
INDEPENDENT
ADDRESS
SPACES
CALLED
SEGMENTS
EACH
SEG
MENT
CONSISTS
OF
A
LINEAR
SEQUENCE
OF
ADDRESSES
FROM
TO
SOME
MAXIMUM
THE
LENGTH
OF
EACH
SEGMENT
MAY
BE
ANYTHING
FROM
TO
THE
MAXIMUM
ALLOWED
DIF
FERENT
SEGMENTS
MAY
AND
USUALLY
DO
HAVE
DIFFERENT
LENGTHS
MOREOVER
SEGMENT
LENGTHS
MAY
CHANGE
DURING
EXECUTION
THE
LENGTH
OF
A
STACK
SEGMENT
MAY
BE
IN
CREASED
WHENEVER
SOMETHING
IS
PUSHED
ONTO
THE
STACK
AND
DECREASED
WHENEVER
SOMETHING
IS
POPPED
OFF
THE
STACK
BECAUSE
EACH
SEGMENT
CONSTITUTES
A
SEPARATE
ADDRESS
SPACE
DIFFERENT
SEG
MENTS
CAN
GROW
OR
SHRINK
INDEPENDENTLY
WITHOUT
AFFECTING
EACH
OTHER
IF
A
STACK
IN
MEMORY
MANAGEMENT
CHAP
SEC
SEGMENTATION
ALL
PROCEDURES
THAT
CALL
ANY
OF
THE
MOVED
PROCEDURES
IN
ORDER
TO
INCORPORATE
THEIR
A
CERTAIN
SEGMENT
NEEDS
MORE
ADDRESS
SPACE
TO
GROW
IT
CAN
HAVE
IT
BECAUSE
THERE
IS
NOTHING
ELSE
IN
ITS
ADDRESS
SPACE
TO
BUMP
INTO
OF
COURSE
A
SEGMENT
CAN
FILL
UP
BUT
SEGMENTS
ARE
USUALLY
VERY
LARGE
SO
THIS
OCCURRENCE
IS
RARE
TO
SPECIFY
AN
AD
DRESS
IN
THIS
SEGMENTED
OR
TWO
DIMENSIONAL
MEMORY
THE
PROGRAM
MUST
SUPPLY
A
TWO
PART
ADDRESS
A
SEGMENT
NUMBER
AND
AN
ADDRESS
WITHIN
THE
SEGMENT
FIGURE
ILLUSTRATES
A
SEGMENTED
MEMORY
BEING
USED
FOR
THE
COMPILER
TABLES
DISCUSSED
EARLIER
FIVE
INDEPENDENT
SEGMENTS
ARE
SHOWN
HERE
SYMBOL
TABLE
NEW
STARTING
ADDRESSES
IF
A
PROGRAM
CONTAINS
HUNDREDS
OF
PROCEDURES
THIS
PROC
ESS
CAN
BE
COSTLY
SEGMENTATION
ALSO
FACILITATES
SHARING
PROCEDURES
OR
DATA
BETWEEN
SEVERAL
PROC
ESSES
A
COMMON
EXAMPLE
IS
THE
SHARED
LIBRARY
MODERN
WORKSTATIONS
THAT
RUN
ADVANCED
WINDOW
SYSTEMS
OFTEN
HAVE
EXTREMELY
LARGE
GRAPHICAL
LIBRARIES
COM
PILED
INTO
NEARLY
EVERY
PROGRAM
IN
A
SEGMENTED
SYSTEM
THE
GRAPHICAL
LIBRARY
CAN
BE
PUT
IN
A
SEGMENT
AND
SHARED
BY
MULTIPLE
PROCESSES
ELIMINATING
THE
NEED
FOR
HAVING
IT
IN
EVERY
PROCESS
ADDRESS
SPACE
WHILE
IT
IS
ALSO
POSSIBLE
TO
HAVE
SHARED
LIBRARIES
IN
PURE
PAGING
SYSTEMS
IT
IS
MORE
COMPLICATED
IN
EFFECT
THESE
SYSTEMS
DO
IT
BY
SIMULATING
SEGMENTATION
SINCE
EACH
SEGMENT
FORMS
A
LOGICAL
ENTITY
OF
WHICH
THE
PROGRAMMER
IS
AWARE
SUCH
AS
A
PROCEDURE
OR
AN
ARRAY
OR
A
STACK
DIFFERENT
SEGMENTS
CAN
HAVE
DIFFERENT
KINDS
OF
PROTECTION
A
PROCEDURE
SEGMENT
CAN
BE
SPECIFIED
AS
EXECUTE
ONLY
PROHI
BITING
ATTEMPTS
TO
READ
FROM
IT
OR
STORE
INTO
IT
A
FLOATING
POINT
ARRAY
CAN
BE
SPECI
FIED
AS
READ
WRITE
BUT
NOT
EXECUTE
AND
ATTEMPTS
TO
JUMP
TO
IT
WILL
BE
CAUGHT
SUCH
PROTECTION
IS
HELPFUL
IN
CATCHING
PROGRAMMING
ERRORS
OK
SEGMEN
T
CONSTANTS
OK
SEGMEN
T
SEGMEN
T
SEGMEN
T
YOU
SHOULD
TRY
TO
UNDERSTAND
WHY
PROTECTION
IS
SENSIBLE
IN
A
SEGMENTED
MEM
ORY
BUT
NOT
IN
A
ONE
DIMENSIONAL
PAGED
MEMORY
IN
A
SEGMENTED
MEMORY
THE
USER
IS
AWARE
OF
WHAT
IS
IN
EACH
SEGMENT
NORMALLY
A
SEGMENT
WOULD
NOT
CONTAIN
A
PROCEDURE
AND
A
STACK
FOR
EXAMPLE
BUT
ONLY
ONE
OR
THE
OTHER
NOT
BOTH
SINCE
EACH
SEGMENT
CONTAINS
ONLY
A
SINGLE
TYPE
OF
OBJECT
THE
SEGMENT
CAN
HAVE
THE
PROTECTION
APPROPRIATE
FOR
THAT
PARTICULAR
TYPE
PAGING
AND
SEGMENTATION
ARE
COMPARED
IN
FIG
THE
CONTENTS
OF
A
PAGE
ARE
IN
A
SENSE
ACCIDENTAL
THE
PROGRAMMER
IS
UNAWARE
FIGUR
E
A
SEGMENTE
D
MEMOR
Y
ALLOWS
EAC
H
TABLE
T
O
GRO
W
O
R
SHRIN
K
INDE
PENDENTL
Y
O
F
THE
OTHER
TABLES
WE
EMPHASIZE
THAT
A
SEGMENT
IS
A
LOGICAL
ENTITY
WHICH
THE
PROGRAMMER
IS
AWARE
OF
AND
USES
AS
A
LOGICAL
ENTITY
A
SEGMENT
MIGHT
CONTAIN
A
PROCEDURE
OR
AN
ARRAY
OR
A
STACK
OR
A
COLLECTION
OF
SCALAR
VARIABLES
BUT
USUALLY
IT
DOES
NOT
CONTAIN
A
MIXTURE
OF
DIFFERENT
TYPES
A
SEGMENTED
MEMORY
HAS
OTHER
ADVANTAGES
BESIDES
SIMPLIFYING
THE
HANDLING
OF
DATA
STRUCTURES
THAT
ARE
GROWING
OR
SHRINKING
IF
EACH
PROCEDURE
OCCUPIES
A
SEP
ARATE
SEGMENT
WITH
ADDRESS
AS
ITS
STARTING
ADDRESS
THE
LINKING
OF
PROCEDURES
COMPILED
SEPARATELY
IS
GREATLY
SIMPLIFIED
AFTER
ALL
THE
PROCEDURES
THAT
CONSTITUTE
A
PROGRAM
HAVE
BEEN
COMPILED
AND
LINKED
UP
A
PROCEDURE
CALL
TO
THE
PROCEDURE
IN
SEGMENT
N
WILL
USE
THE
TWO
PART
ADDRESS
N
TO
ADDRESS
WORD
THE
ENTRY
POINT
IF
THE
PROCEDURE
IN
SEGMENT
N
IS
SUBSEQUENTLY
MODIFIED
AND
RECOMPILED
NO
OTHER
PROCEDURES
NEED
BE
CHANGED
BECAUSE
NO
STARTING
ADDRESSES
HAVE
BEEN
MODI
FIED
EVEN
IF
THE
NEW
VERSION
IS
LARGER
THAN
THE
OLD
ONE
WITH
A
ONE
DIMENSIONAL
MEMORY
THE
PROCEDURES
ARE
PACKED
TIGHTLY
NEXT
TO
EACH
OTHER
WITH
NO
ADDRESS
SPACE
BETWEEN
THEM
CONSEQUENTLY
CHANGING
ONE
PROCEDURE
SIZE
CAN
AFFECT
THE
STARTING
ADDRESS
OF
OTHER
UNRELATED
PROCEDURES
THIS
IN
TURN
REQUIRES
MODIFYING
OF
THE
FACT
THAT
PAGING
IS
EVEN
OCCURRING
ALTHOUGH
PUTTING
A
FEW
BITS
IN
EACH
ENTRY
OF
THE
PAGE
TABLE
TO
SPECIFY
THE
ACCESS
ALLOWED
WOULD
BE
POSSIBLE
TO
UTILIZE
THIS
FEATURE
THE
PROGRAMMER
WOULD
HAVE
TO
KEEP
TRACK
OF
WHERE
IN
HIS
ADDRESS
SPACE
THE
PAGE
BOUNDARIES
WERE
THAT
IS
PRECISELY
THE
SORT
OF
ADMINISTRATION
THAT
PAGING
WAS
INVENTED
TO
ELIMINATE
BECAUSE
THE
USER
OF
A
SEGMENTED
MEMORY
HAS
THE
ILLUSION
THAT
ALL
SEGMENTS
ARE
IN
MAIN
MEMORY
ALL
THE
TIME
THAT
IS
HE
CAN
AD
DRESS
THEM
AS
THOUGH
THEY
WERE
HE
CAN
PROTECT
EACH
SEGMENT
SEPARATELY
WITHOUT
HAVING
TO
BE
CONCERNED
WITH
THE
ADMINISTRATION
OF
OVERLAYING
THEM
IMPLEMENTATION
OF
PURE
SEGMENTATION
THE
IMPLEMENTATION
OF
SEGMENTATION
DIFFERS
FROM
PAGING
IN
AN
ESSENTIAL
WAY
PAGES
ARE
FIXED
SIZE
AND
SEGMENTS
ARE
NOT
FIGURE
A
SHOWS
AN
EXAMPLE
OF
PHYSICAL
MEMORY
INITIALLY
CONTAINING
FIVE
SEGMENTS
NOW
CONSIDER
WHAT
HAPPENS
IF
SEGMENT
IS
EVICTED
AND
SEGMENT
WHICH
IS
SMALLER
IS
PUT
IN
ITS
PLACE
WE
ARRIVE
AT
THE
MEMORY
CONFIGURATION
OF
FIG
B
BETWEEN
SEGMENT
AND
SEG
MENT
IS
AN
UNUSED
AREA
THAT
IS
A
HOLE
THEN
SEGMENT
IS
REPLACED
BY
SEGMENT
AS
IN
FIG
C
AND
SEGMENT
IS
REPLACED
BY
SEGMENT
AS
IN
FIG
D
MEMORY
MANAGEMEN
T
CHAP
CONSIDERATIO
N
PAGIN
G
SEGMENTATIO
N
E
C
SEGMENTATION
WARN
MM
A
A
I
D
T
J
FIGURE
A
D
DEVELOPMENT
OF
CHECKERBOARDING
E
REMOVAL
OF
THE
CHECKERBOARDING
BY
COMPACTION
FIGURE
COMPARISON
OF
PAGING
AND
SEGMENTATION
AFTER
THE
SYSTEM
HAS
BEEN
RUNNING
FOR
A
WHILE
MEMORY
WILL
BE
DIVIDED
UP
INTO
A
NUMBER
OF
CHUNKS
SOME
CONTAINING
SEGMENTS
AND
SOME
CONTAINING
HOLES
THIS
PHENOMENON
CALLED
CHECKERBOARDING
OR
EXTERNAL
FRAGMENTATION
WASTES
MEMO
RY
IN
THE
HOLES
IT
CAN
BE
DEALT
WITH
BY
COMPACTION
AS
SHOWN
IN
FIG
E
SEGMENTATION
WITH
PAGING
MULTICS
IF
THE
SEGMENTS
ARE
LARGE
IT
MAY
BE
INCONVENIENT
OR
EVEN
IMPOSSIBLE
TO
KEEP
THEM
IN
MAIN
MEMORY
IN
THEIR
ENTIRETY
THIS
LEADS
TO
THE
IDEA
OF
PAGING
THEM
SO
THAT
ONLY
THOSE
PAGES
THAT
ARE
ACTUALLY
NEEDED
HAVE
TO
BE
AROUND
SEVERAL
SIGNIFI
CANT
SYSTEMS
HAVE
SUPPORTED
PAGED
SEGMENTS
IN
THIS
SECTION
WE
WILL
DESCRIBE
THE
FIRST
ONE
MULTICS
IN
THE
NEXT
ONE
WE
WILL
DISCUSS
A
MORE
RECENT
ONE
THE
INTEL
PENTIUM
MULTICS
RAN
ON
THE
HONEYWELL
MACHINES
AND
THEIR
DESCENDANTS
AND
PROVIDED
EACH
PROGRAM
WITH
A
VIRTUAL
MEMORY
OF
UP
TO
SEGMENTS
MORE
THAN
EACH
OF
WHICH
COULD
BE
UP
TO
BIT
WORDS
LONG
TO
IMPLEMENT
THIS
THE
MULTICS
DESIGNERS
CHOSE
TO
TREAT
EACH
SEGMENT
AS
A
VIRTUAL
MEMORY
AND
TO
PAGE
IT
COMBINING
THE
ADVANTAGES
OF
PAGING
UNIFORM
PAGE
SIZE
AND
NOT
HAVING
TO
KEEP
THE
WHOLE
SEGMENT
IN
MEMORY
IF
ONLY
PART
OF
IT
IS
BEING
USED
WITH
THE
AD
VANTAGES
OF
SEGMENTATION
EASE
OF
PROGRAMMING
MODULARITY
PROTECTION
SHARING
EACH
MULTICS
PROGRAM
HAS
A
SEGMENT
TABLE
WITH
ONE
DESCRIPTOR
PER
SEG
MENT
SINCE
THERE
ARE
POTENTIALLY
MORE
THAN
A
QUARTER
OF
A
MILLION
ENTRIES
IN
THE
TABLE
THE
SEGMENT
TABLE
IS
ITSELF
A
SEGMENT
AND
IS
PAGED
A
SEGMENT
DESCRIPTOR
CONTAINS
AN
INDICATION
OF
WHETHER
THE
SEGMENT
IS
IN
MAIN
MEMORY
OR
NOT
IF
ANY
PART
OF
THE
SEGMENT
IS
IN
MEMORY
THE
SEGMENT
IS
CONSIDERED
TO
BE
IN
MEMORY
AND
ITS
PAGE
TABLE
WILL
BE
IN
MEMORY
IF
THE
SEGMENT
IS
IN
MEMORY
ITS
DESCRIPTOR
CON
TAINS
AN
BIT
POINTER
TO
ITS
PAGE
TABLE
AS
IN
FIG
A
BECAUSE
PHYSICAL
AD
DRESSES
ARE
BITS
AND
PAGES
ARE
ALIGNED
ON
BYTE
BOUNDARIES
IMPLYING
THAT
THE
LOW
ORDER
BITS
OF
PAGE
ADDRESSES
ARE
ONLY
BITS
ARE
NEEDED
IN
THE
DE
SCRIPTOR
TO
STORE
A
PAGE
TABLE
ADDRESS
THE
DESCRIPTOR
ALSO
CONTAINS
THE
SEGMENT
SIZE
THE
PROTECTION
BITS
AND
A
FEW
OTHER
ITEMS
FIGURE
B
ILLUSTRATES
A
MUL
TICS
SEGMENT
DESCRIPTOR
THE
ADDRESS
OF
THE
SEGMENT
IN
SECONDARY
MEMORY
IS
NOT
IN
THE
SEGMENT
DESCRIPTOR
BUT
IN
ANOTHER
TABLE
USED
BY
THE
SEGMENT
FAULT
HANDLER
EACH
SEGMENT
IS
AN
ORDINARY
VIRTUAL
ADDRESS
SPACE
AND
IS
PAGED
IN
THE
SAME
WAY
AS
THE
NONSEGMENTED
PAGED
MEMORY
DESCRIBED
EARLIER
IN
THIS
CHAPTER
THE
NORMAL
PAGE
SIZE
IS
WORDS
ALTHOUGH
A
FEW
SMALL
SEGMENTS
USED
BY
MUL
TICS
ITSELF
ARE
NOT
PAGED
OR
ARE
PAGED
IN
UNITS
OF
WORDS
TO
SAVE
PHYSICAL
MEM
ORY
AN
ADDRESS
IN
MULTICS
CONSISTS
OF
TWO
PARTS
THE
SEGMENT
AND
THE
ADDRESS
WITHIN
THE
SEGMENT
THE
ADDRESS
WITHIN
THE
SEGMENT
IS
FURTHER
DIVIDED
INTO
A
PAGE
NUMBER
AND
A
WORD
WITHIN
THE
PAGE
AS
SHOWN
IN
FIG
WHEN
A
MEMORY
REF
ERENCE
OCCURS
THE
FOLLOWING
ALGORITHM
IS
CARRIED
OUT
MEMORY
MANAGEMENT
CHAP
BITS
PAGE
ENTRY
PAGE
ENTRY
SEC
SEGMENTATION
MEMORY
THE
MAIN
MEMORY
ADDRESS
OF
THE
START
OF
THE
PAGE
IS
EX
TRACTED
FROM
THE
PAGE
TABLE
ENTRY
THE
OFFSET
IS
ADDED
TO
THE
PAGE
ORIGIN
TO
GIVE
THE
MAIN
MEMORY
AD
DRESS
WHERE
THE
WORD
IS
LOCATED
THE
READ
OR
STORE
FINALLY
TAKES
PLACE
SEGMENT
DESCRIPTOR
SEGMENT
DESCRIPTOR
SEGMENT
DESCRIPTOR
SEGMENT
DESCRIPTOR
SEGMENT
DESCRIPTOR
PAGE
ENTRY
PAGE
TABLE
FOR
SEGMENT
SEGMENT
NUMBER
ADDRESS
WITHIN
THE
SEGMENT
SEGMENT
DESCRIPTOR
SEGMENT
DESCRIPTOR
DESCRIPTOR
SEGMENT
FA
PAGE
ENTRY
PAGE
ENTRY
PAGE
ENTRY
PAGE
TABLE
FOR
SEGMENT
FIGURE
A
BIT
MULTICS
VIRTUAL
ADDRESS
THIS
PROCESS
IS
ILLUSTRATED
IN
FIG
FOR
SIMPLICITY
THE
FACT
THAT
THE
DE
SCRIPTOR
SEGMENT
IS
ITSELF
PAGED
HAS
BEEN
OMITTED
WHAT
REALLY
HAPPENS
IS
THAT
A
REGISTER
THE
DESCRIPTOR
BASE
REGISTER
IS
USED
TO
LOCATE
THE
DESCRIPTOR
SEGMENT
PAGE
TABLE
WHICH
IN
TURN
POINTS
TO
THE
PAGES
OF
THE
DESCRIPTOR
SEGMENT
ONCE
THE
DESCRIPTOR
FOR
THE
NEEDED
SEGMENT
HAS
BEEN
FOUND
THE
ADDRESSING
PROCEEDS
AS
SHOWN
IN
FIG
MAIN
MEMORY
ADDRESS
OF
THE
PAGE
TABLE
SEGMENT
LENGTH
IN
PAGES
PAGE
SIZE
WORDS
WORDS
SEGMENT
IS
PAGED
SEGMENT
IS
NOT
PAGED
MISCELLANEOUS
BITS
PROTECTION
BITS
B
MULTICS
VIRTUAL
ADDRESS
SEGMENT
NUMBER
PAGE
OFFSET
NUMBER
FIGURE
THE
MULTICS
VIRTUAL
MEMORY
A
THE
DESCRIPTOR
SEGMENT
POINTS
TO
THE
PAGE
TABLES
B
A
SEGMENT
DESCRIPTOR
THE
NUMBERS
ARE
THE
FIELD
LENGTHS
THE
SEGMENT
NUMBER
IS
USED
TO
FIND
THE
SEGMENT
DESCRIPTOR
A
CHECK
IS
MADE
TO
SEE
IF
THE
SEGMENT
PAGE
TABLE
IS
IN
MEMORY
IF
SEGMENT
NUMBER
DESCRIPTOR
SEGMENT
PAGE
TABLE
PAGE
OFFSET
THE
PAGE
TABLE
IS
IN
MEMORY
IT
IS
LOCATED
IF
IT
IS
NOT
A
SEGMENT
FAULT
OCCURS
IF
THERE
IS
A
PROTECTION
VIOLATION
A
FAULT
TRAP
OCCURS
THE
PAGE
TABLE
ENTRY
FOR
THE
REQUESTED
VIRTUAL
PAGE
IS
EXAMINED
IF
THE
PAGE
ITSELF
IS
NOT
IN
MEMORY
A
PAGE
FAULT
IS
TRIGGERED
IF
IT
IS
IN
FIGURE
CONVERSION
OF
A
TWO
PART
MULTICS
ADDRESS
INTO
A
MAIN
MEMORY
ADDRESS
AS
YOU
HAVE
NO
DOUBT
GUESSED
BY
NOW
IF
THE
PRECEDING
ALGORITHM
WERE
AC
TUALLY
CARRIED
OUT
BY
THE
OPERATING
SYSTEM
ON
EVERY
INSTRUCTION
PROGRAMS
WOULD
MEMORY
MANAGEMENT
CHAP
NOT
RUN
VERY
FAST
IN
REALITY
THE
MULTICS
HARDWARE
CONTAINS
A
WORD
HIGH
SPEED
TLB
THAT
CAN
SEARCH
ALL
ITS
ENTRIES
IN
PARALLEL
FOR
A
GIVEN
KEY
IT
IS
ILLUSTRAT
ED
IN
FIG
WHEN
AN
ADDRESS
IS
PRESENTED
TO
THE
COMPUTER
THE
ADDRESSING
HARDWARE
FIRST
CHECKS
TO
SEE
IF
THE
VIRTUAL
ADDRESS
IS
IN
THE
TLB
IF
SO
IT
GETS
THE
PAGE
FRAME
NUMBER
DIRECTLY
FROM
THE
TLB
AND
FORMS
THE
ACTUAL
ADDRESS
OF
THE
REF
ERENCED
WORD
WITHOUT
HAVING
TO
LOOK
IN
THE
DESCRIPTOR
SEGMENT
OR
PAGE
TABLE
COMPARISON
N
ENTRY
SEGMENT
VIRTUAL
PAGE
NUMBER
PAGE
FRAME
PROTECTION
AGE
READ
WRITE
READ
ONLY
READ
WRITE
EXECUTE
ONLY
EXECUTE
ONLY
FIGURE
A
SIMPLIFIED
VERSION
OF
THE
MULTICS
TLB
THE
EXISTENCE
OF
TWO
PAGE
SIZES
MAKES
THE
ACTUAL
TLB
MORE
COMPLICATED
THE
ADDRESSES
OF
THE
MOST
RECENTLY
REFERENCED
PAGES
ARE
KEPT
IN
THE
TLB
PROGRAMS
WHOSE
WORKING
SET
IS
SMALLER
THAN
THE
TLB
SIZE
WILL
COME
TO
EQUILI
BRIUM
WITH
THE
ADDRESSES
OF
THE
ENTIRE
WORKING
SET
IN
THE
TLB
AND
THEREFORE
WILL
RUN
EFFICIENTLY
IF
THE
PAGE
IS
NOT
IN
THE
TLB
THE
DESCRIPTOR
AND
PAGE
TABLES
ARE
ACTUALLY
REFERENCED
TO
FIND
THE
PAGE
FRAME
ADDRESS
AND
THE
TLB
IS
UPDATED
TO
IN
CLUDE
THIS
PAGE
THE
LEAST
RECENTLY
USED
PAGE
BEING
THROWN
OUT
THE
AGE
FIELD
KEEPS
TRACK
OF
WHICH
ENTRY
IS
THE
LEAST
RECENTLY
USED
THE
REASON
THAT
A
TLB
IS
USED
IS
FOR
COMPARING
THE
SEGMENT
AND
PAGE
NUMBERS
OF
ALL
THE
ENTRIES
IN
PARALLEL
SEGMENTATION
WITH
PAGING
THE
INTEL
PENTIUM
IN
MANY
WAYS
THE
VIRTUAL
MEMORY
ON
THE
PENTIUM
RESEMBLES
THAT
OF
MUL
TICS
INCLUDING
THE
PRESENCE
OF
BOTH
SEGMENTATION
AND
PAGING
WHEREAS
MUL
TICS
HAS
INDEPENDENT
SEGMENTS
EACH
UP
TO
BIT
WORDS
THE
PENTIUM
HAS
INDEPENDENT
SEGMENTS
EACH
HOLDING
UP
TO
BILLION
BIT
WORDS
AL
THOUGH
THERE
ARE
FEWER
SEGMENTS
THE
LARGER
SEGMENT
SIZE
IS
FAR
MORE
IMPORTANT
AS
FEW
PROGRAMS
NEED
MORE
THAN
SEGMENTS
BUT
MANY
PROGRAMS
NEED
LARGE
SEG
MENTS
THE
HEART
OF
THE
PENTIUM
VIRTUAL
MEMORY
CONSISTS
OF
TWO
TABLES
CALLED
THE
LDT
LOCAL
DESCRIPTOR
TABLE
AND
THE
GDT
GLOBAL
DESCRIPTOR
TABLE
EACH
SEC
SEGMENTATION
PROGRAM
HAS
ITS
OWN
LDT
BUT
THERE
IS
A
SINGLE
GDT
SHARED
BY
ALL
THE
PROGRAMS
ON
THE
COMPUTER
THE
LDT
DESCRIBES
SEGMENTS
LOCAL
TO
EACH
PROGRAM
INCLUDING
ITS
CODE
DATA
STACK
AND
SO
ON
WHEREAS
THE
GDT
DESCRIBES
SYSTEM
SEGMENTS
IN
CLUDING
THE
OPERATING
SYSTEM
ITSELF
TO
ACCESS
A
SEGMENT
A
PENTIUM
PROGRAM
FIRST
LOADS
A
SELECTOR
FOR
THAT
SEGMENT
INTO
ONE
OF
THE
MACHINE
SIX
SEGMENT
REGISTERS
DURING
EXECUTION
THE
CS
REGISTER
HOLDS
THE
SELECTOR
FOR
THE
CODE
SEGMENT
AND
THE
DS
REGISTER
HOLDS
THE
SELECTOR
FOR
THE
DATA
SEGMENT
THE
OTHER
SEGMENT
REGISTERS
ARE
LESS
IMPORTANT
EACH
SELECTOR
IS
A
BIT
NUMBER
AS
SHOWN
IN
FIG
BITS
T
INDEX
GDT
LDT
PRIVILEGE
LEVEL
FIGURE
A
PENTIUM
SELECTOR
ONE
OF
THE
SELECTOR
BITS
TELLS
WHETHER
THE
SEGMENT
IS
LOCAL
OR
GLOBAL
I
E
WHETHER
IT
IS
IN
THE
LDT
OR
GDT
THIRTEEN
OTHER
BITS
SPECIFY
THE
LDT
OR
GDT
ENTRY
NUMBER
SO
THESE
TABLES
ARE
EACH
RESTRICTED
TO
HOLDING
SEGMENT
DESCRIP
TORS
THE
OTHER
BITS
RELATE
TO
PROTECTION
AND
WILL
BE
DESCRIBED
LATER
DESCRIPTOR
IS
FORBIDDEN
IT
MAY
BE
SAFELY
LOADED
INTO
A
SEGMENT
REGISTER
TO
INDICATE
THAT
THE
SEGMENT
REGISTER
IS
NOT
CURRENTLY
AVAILABLE
IT
CAUSES
A
TRAP
IF
USED
AT
THE
TIME
A
SELECTOR
IS
LOADED
INTO
A
SEGMENT
REGISTER
THE
CORRESPONDING
DE
SCRIPTOR
IS
FETCHED
FROM
THE
LDT
OR
GDT
AND
STORED
IN
MICROPROGRAM
REGISTERS
SO
IT
CAN
BE
ACCESSED
QUICKLY
AS
DEPICTED
IN
FIG
A
DESCRIPTOR
CONSISTS
OF
BYTES
INCLUDING
THE
SEGMENT
BASE
ADDRESS
SIZE
AND
OTHER
INFORMATION
THE
FORMAT
OF
THE
SELECTOR
HAS
BEEN
CLEVERLY
CHOSEN
TO
MAKE
LOCATING
THE
DE
SCRIPTOR
EASY
FIRST
EITHER
THE
LDT
OR
GDT
IS
SELECTED
BASED
ON
SELECTOR
BIT
THEN
THE
SELECTOR
IS
COPIED
TO
AN
INTERNAL
SCRATCH
REGISTER
AND
THE
LOW
ORDER
BITS
SET
TO
FINALLY
THE
ADDRESS
OF
EITHER
THE
LDT
OR
GDT
TABLE
IS
ADDED
TO
IT
TO
GIVE
A
DIRECT
POINTER
TO
THE
DESCRIPTOR
FOR
EXAMPLE
SELECTOR
REFERS
TO
ENTRY
IN
THE
GDT
WHICH
IS
LOCATED
AT
ADDRESS
GDT
LET
US
TRACE
THE
STEPS
BY
WHICH
A
SELECTOR
OFFSET
PAIR
IS
CONVERTED
TO
A
PHYSI
CAL
ADDRESS
AS
SOON
AS
THE
MICROPROGRAM
KNOWS
WHICH
SEGMENT
REGISTER
IS
BEING
USED
IT
CAN
FIND
THE
COMPLETE
DESCRIPTOR
CORRESPONDING
TO
THAT
SELECTOR
IN
ITS
INTER
NAL
REGISTERS
IF
THE
SEGMENT
DOES
NOT
EXIST
SELECTOR
OR
IS
CURRENTLY
PAGED
OUT
A
TRAP
OCCURS
THE
HARDWARE
THEN
USES
THE
LIMIT
FIELD
TO
CHECK
IF
THE
OFFSET
IS
BEYOND
THE
END
OF
THE
SEGMENT
IN
WHICH
CASE
A
TRAP
ALSO
OCCURS
LOGICALLY
THERE
SHOULD
BE
A
BIT
FIELD
IN
THE
DESCRIPTOR
GIVING
THE
SIZE
OF
THE
SEGMENT
BUT
THERE
ARE
ONLY
BITS
MEMORY
MANAGEMENT
CHAP
SEC
SEGMENTATION
OVERLAPPING
PROBABLY
BECAUSE
IT
WOULD
BE
TOO
MUCH
TROUBLE
AND
TAKE
TOO
MUCH
BIT
SEGMENT
BIT
SEGMENT
LI
IS
IN
BYTES
LI
IS
IN
PAGE
BITS
SEGMEN
T
I
ABSEN
T
FRO
M
MEMOR
Y
SEGMEN
T
IS
PRESEN
T
IN
MEMOR
Y
PRIVILEGE
LEVEL
F
SYSTE
M
APPLICATION
SEGMEN
T
TYP
E
AN
D
PROTECTION
RELATIV
E
ADDRES
TIME
TO
VERIFY
THAT
THEY
WERE
ALL
DISJOINT
ON
THE
OTHER
HAND
IF
PAGING
IS
ENABLED
THE
LINEAR
ADDRESS
IS
INTERPRETED
AS
A
VIRTUAL
ADDRESS
AND
MAPPED
ONTO
THE
PHYSICAL
ADDRESS
USING
PAGE
TABLES
PRETTY
MUCH
AS
IN
OUR
EARLIER
EXAMPLES
THE
ONLY
REAL
COMPLICATION
IS
THAT
WITH
A
BIT
VIRTUAL
ADDRESS
AND
A
KB
PAGE
A
SEGMENT
MIGHT
CONTAIN
MILLION
PAGES
SO
A
TWO
LEVEL
MAPPING
IS
USED
TO
REDUCE
THE
PAGE
TABLE
SIZE
FOR
SMALL
SEGMENTS
EACH
RUNNING
PROGRAM
HAS
A
PAGE
DIRECTORY
CONSISTING
OF
BIT
ENTRIES
IT
IS
LOCATED
AT
AN
ADDRESS
POINTED
TO
BY
A
GLOBAL
REGISTER
EACH
ENTRY
IN
THIS
DIREC
TORY
POINTS
TO
A
PAGE
TABLE
ALSO
CONTAINING
BIT
ENTRIES
THE
PAGE
TABLE
EN
TRIES
POINT
TO
PAGE
FRAMES
THE
SCHEME
IS
SHOWN
IN
FIG
LINEA
R
ADDRES
FIGURE
PENTIUM
CODE
SEGMENT
DESCRIPTOR
DATA
SEGMENTS
DIFFER
SLIGHTLY
BITS
AVAILABLE
SO
A
DIFFERENT
SCHEME
IS
USED
IF
THE
GBIT
GRANULARITY
FIELD
IS
THE
LIMIT
FIELD
IS
THE
EXACT
SEGMENT
SIZE
UP
TO
MB
IF
IT
IS
THE
LIMIT
FIELD
GIVES
THE
SEGMENT
SIZE
IN
PAGES
INSTEAD
OF
BYTES
THE
PENTIUM
PAGE
SIZE
IS
FIXED
AT
KB
SO
BITS
ARE
ENOUGH
FOR
SEGMENTS
UP
TO
BYTES
ASSUMING
THAT
THE
SEGMENT
IS
IN
MEMORY
AND
THE
OFFSET
IS
IN
RANGE
THE
PEN
TIUM
THEN
ADDS
THE
BIT
BASE
FIELD
IN
THE
DESCRIPTOR
TO
THE
OFFSET
TO
FORM
WHAT
IS
CALLED
A
LINEAR
ADDRESS
AS
SHOWN
IN
FIG
THE
BASE
FIELD
IS
BROKEN
UP
INTO
THREE
PIECES
AND
SPREAD
ALL
OVER
THE
DESCRIPTOR
FOR
COMPATIBILITY
WITH
THE
IN
WHICH
THE
BASE
IS
ONLY
BITS
IN
EFFECT
THE
BASE
FIELD
ALLOWS
EACH
SEGMENT
TO
START
AT
AN
ARBITRARY
PLACE
WITHIN
THE
BIT
LINEAR
ADDRESS
SPACE
OFFSE
T
DESCRIPTOR
BI
T
LINEAR
ADDRES
FIGURE
CONVERSION
OF
A
SELECTOR
OFFSET
PAIR
TO
A
LINEAR
ADDRESS
IF
PAGING
IS
DISABLED
BY
A
BIT
IN
A
GLOBAL
CONTROL
REGISTER
THE
LINEAR
ADDRESS
IS
INTERPRETED
AS
THE
PHYSICAL
ADDRESS
AND
SENT
TO
THE
MEMORY
FOR
THE
READ
OR
WRITE
THUS
WITH
PAGING
DISABLED
WE
HAVE
A
PURE
SEGMENTATION
SCHEME
WITH
EACH
SEG
MENT
BASE
ADDRESS
GIVEN
IN
ITS
DESCRIPTOR
SEGMENTS
ARE
NOT
PREVENTED
FROM
DIR
PAG
E
OFFSET
A
FIGURE
MAPPING
OF
A
LINEAR
ADDRESS
ONTO
A
PHYSICAL
ADDRESS
IN
FIG
A
WE
SEE
A
LINEAR
ADDRESS
DIVIDED
INTO
THREE
FIELDS
DIR
PAGE
AND
OFFSET
THE
DIR
FIELD
IS
USED
TO
INDEX
INTO
THE
PAGE
DIRECTORY
TO
LOCATE
A
POINT
ER
TO
THE
PROPER
PAGE
TABLE
THEN
THE
PAGE
FIELD
IS
USED
AS
AN
INDEX
INTO
THE
PAGE
TABLE
TO
FIND
THE
PHYSICAL
ADDRESS
OF
THE
PAGE
FRAME
FINALLY
OFFSET
IS
ADDED
TO
THE
ADDRESS
OF
THE
PAGE
FRAME
TO
GET
THE
PHYSICAL
ADDRESS
OF
THE
BYTE
OR
WORD
NEEDED
THE
PAGE
TABLE
ENTRIES
ARE
BITS
EACH
OF
WHICH
CONTAIN
A
PAGE
FRAME
NUMBER
THE
REMAINING
BITS
CONTAIN
ACCESS
AND
DIRTY
BITS
SET
BY
THE
HARDWARE
FOR
THE
BENEFIT
OF
THE
OPERATING
SYSTEM
PROTECTION
BITS
AND
OTHER
UTILITY
BITS
EACH
PAGE
TABLE
HAS
ENTRIES
FOR
KB
PAGE
FRAMES
SO
A
SINGLE
PAGE
TABLE
HANDLES
MEGABYTES
OF
MEMORY
A
SEGMENT
SHORTER
THAN
WILL
HAVE
A
PAGE
MEMORY
MANAGEMEN
T
CHAP
DIRECTORY
WITH
A
SINGLE
ENTRY
A
POINTER
TO
ITS
ONE
AND
ONLY
PAGE
TABLE
IN
THIS
WAY
THE
OVERHEAD
FOR
SHORT
SEGMENTS
IS
ONLY
TWO
PAGES
INSTEAD
OF
THE
MILLION
PAGES
THAT
WOULD
BE
NEEDED
IN
A
ONE
LEVEL
PAGE
TABLE
TO
AVOID
MAKING
REPEATED
REFERENCES
TO
MEMORY
THE
PENTIUM
LIKE
MUL
TICS
HAS
A
SMALL
TLB
THAT
DIRECTLY
MAPS
THE
MOST
RECENTLY
USED
DIR
PAGE
COM
BINATIONS
ONTO
THE
PHYSICAL
ADDRESS
OF
THE
PAGE
FRAME
ONLY
WHEN
THE
CURRENT
COM
BINATION
IS
NOT
PRESENT
IN
THE
TLB
IS
THE
MECHANISM
OF
FIG
ACTUALLY
CARRIED
OUT
AND
THE
TLB
UPDATED
AS
LONG
AS
TLB
MISSES
ARE
RARE
PERFORMANCE
IS
GOOD
IT
IS
ALSO
WORTH
NOTING
THAT
IF
SOME
APPLICATION
DOES
NOT
NEED
SEGMENTATION
BUT
IS
CONTENT
WITH
A
SINGLE
PAGED
BIT
ADDRESS
SPACE
THAT
MODEL
IS
POSSIBLE
ALL
THE
SEGMENT
REGISTERS
CAN
BE
SET
UP
WITH
THE
SAME
SELECTOR
WHOSE
DESCRIPTOR
HAS
BASE
AND
LIMIT
SET
TO
THE
MAXIMUM
THE
INSTRUCTION
OFFSET
WILL
THEN
BE
THE
LIN
EAR
ADDRESS
WITH
ONLY
A
SINGLE
ADDRESS
SPACE
USED
IN
EFFECT
NORMAL
PAGING
IN
FACT
ALL
CURRENT
OPERATING
SYSTEMS
FOR
THE
PENTIUM
WORK
THIS
WAY
OS
WAS
THE
ONLY
ONE
THAT
USED
THE
FULL
POWER
OF
THE
INTEL
MMU
ARCHITECTURE
ALL
IN
ALL
ONE
HAS
TO
GIVE
CREDIT
TO
THE
PENTIUM
DESIGNERS
GIVEN
THE
CONFLICT
ING
GOALS
OF
IMPLEMENTING
PURE
PAGING
PURE
SEGMENTATION
AND
PAGED
SEGMENTS
WHILE
AT
THE
SAME
TIME
BEING
COMPATIBLE
WITH
THE
AND
DOING
ALL
OF
THIS
EFFI
CIENTLY
THE
RESULTING
DESIGN
IS
SURPRISINGLY
SIMPLE
AND
CLEAN
ALTHOUGH
WE
HAVE
COVERED
THE
COMPLETE
ARCHITECTURE
OF
THE
PENTIUM
VIRTUAL
MEMORY
ALBEIT
BRIEFLY
IT
IS
WORTH
SAYING
A
FEW
WORDS
ABOUT
PROTECTION
SINCE
THIS
SUBJECT
IS
INTIMATELY
RELATED
TO
THE
VIRTUAL
MEMORY
JUST
AS
THE
VIRTUAL
MEMORY
SCHEME
IS
CLOSELY
MODELED
ON
MULTICS
SO
IS
THE
PROTECTION
SYSTEM
THE
PEN
TIUM
SUPPORTS
FOUR
PROTECTION
LEVELS
WITH
LEVEL
BEING
THE
MOST
PRIVILEGED
AND
LEVEL
THE
LEAST
THESE
ARE
SHOWN
IN
FIG
AT
EACH
INSTANT
A
RUNNING
PRO
GRAM
IS
AT
A
CERTAIN
LEVEL
INDICATED
BY
A
BIT
FIELD
IN
ITS
PSW
EACH
SEGMENT
IN
THE
SYSTEM
ALSO
HAS
A
LEVEL
AS
LONG
AS
A
PROGRAM
RESTRICTS
ITSELF
TO
USING
SEGMENTS
AT
ITS
OWN
LEVEL
EVERY
THING
WORKS
FINE
ATTEMPTS
TO
ACCESS
DATA
AT
A
HIGHER
LEVEL
ARE
PERMITTED
AT
TEMPTS
TO
ACCESS
DATA
AT
A
LOWER
LEVEL
ARE
ILLEGAL
AND
CAUSE
TRAPS
ATTEMPTS
TO
CALL
PROCEDURES
AT
A
DIFFERENT
LEVEL
HIGHER
OR
LOWER
ARE
ALLOWED
BUT
IN
A
CAREFULLY
CONTROLLED
WAY
TO
MAKE
AN
INTERLEVEL
CALL
THE
CALL
INSTRUCTION
MUST
CONTAIN
A
SELECTOR
INSTEAD
OF
AN
ADDRESS
THIS
SELECTOR
DESIGNATES
A
DESCRIPTOR
CALLED
A
CALL
GATE
WHICH
GIVES
THE
ADDRESS
OF
THE
PROCEDURE
TO
BE
CALLED
THUS
IT
IS
NOT
POSSIBLE
TO
JUMP
INTO
THE
MIDDLE
OF
AN
ARBITRARY
CODE
SEGMENT
AT
A
DIFFERENT
LEVEL
ONLY
OFFICIAL
ENTRY
POINTS
MAY
BE
USED
THE
CONCEPTS
OF
PROTECTION
LEVELS
AND
CALL
GATES
WERE
PIONEERED
IN
MULTICS
WHERE
THEY
WERE
VIEWED
AS
PROTECTION
RINGS
A
TYPICAL
USE
FOR
THIS
MECHANISM
IS
SUGGESTED
IN
FIG
AT
LEVEL
WE
FIND
THE
KERNEL
OF
THE
OPERATING
SYSTEM
WHICH
HANDLES
I
O
MEMORY
MANAGEMENT
AND
OTHER
CRITICAL
MATTERS
AT
LEVEL
THE
SYSTEM
CALL
HANDLER
IS
PRESENT
USER
PRO
GRAMS
MAY
CALL
PROCEDURES
HERE
TO
HAVE
SYSTEM
CALLS
CARRIED
OUT
BUT
ONLY
A
SPE
CIFIC
AND
PROTECTED
LIST
OF
PROCEDURES
MAY
BE
CALLED
LEVEL
CONTAINS
LIBRARY
PRO
CEDURES
POSSIBLY
SHARED
AMONG
MANY
RUNNING
PROGRAMS
USER
PROGRAMS
MAY
CALL
SEC
SEGMENTATION
FIGURE
PROTECTION
ON
THE
PENTIUM
THESE
PROCEDURES
AND
READ
THEIR
DATA
BUT
THEY
MAY
NOT
MODIFY
THEM
FINALLY
USER
PROGRAMS
RUN
AT
LEVEL
WHICH
HAS
THE
LEAST
PROTECTION
TRAPS
AND
INTERRUPTS
USE
A
MECHANISM
SIMILAR
TO
THE
CALL
GATES
THEY
TOO
REF
ERENCE
DESCRIPTORS
RATHER
THAN
ABSOLUTE
ADDRESSES
AND
THESE
DESCRIPTORS
POINT
TO
SPECIFIC
PROCEDURES
TO
BE
EXECUTED
THE
TYPE
FIELD
IN
FIG
DISTINGUISHES
BE
TWEEN
CODE
SEGMENTS
DATA
SEGMENTS
AND
THE
VARIOUS
KINDS
OF
GATES
RESEARCH
ON
MEMORY
MANAGEMENT
MEMORY
MANAGEMENT
ESPECIALLY
PAGING
ALGORITHMS
WAS
ONCE
A
FRUITFUL
AREA
FOR
RESEARCH
BUT
MOST
OF
THAT
SEEMS
TO
HAVE
LARGELY
DIED
OFF
AT
LEAST
FOR
GENERAL
PURPOSE
SYSTEMS
MOST
REAL
SYSTEMS
TEND
TO
USE
SOME
VARIATION
ON
CLOCK
BECAUSE
IT
IS
EASY
TO
IMPLEMENT
AND
RELATIVELY
EFFECTIVE
ONE
RECENT
EXCEPTION
HOWEVER
IS
A
REDESIGN
OF
THE
BSD
VIRTUAL
MEMORY
SYSTEM
CRANOR
AND
PARULKAR
THERE
IS
STILL
RESEARCH
GOING
ON
CONCERNING
PAGING
IN
NEWER
KINDS
OF
SYSTEMS
THOUGH
FOR
EXAMPLE
CELL
PHONES
AND
PDAS
HAVE
BECOME
SMALL
PCS
AND
MANY
OF
THEM
PAGE
RAM
TO
DISK
ONLY
DISK
ON
A
CELL
PHONE
IS
FLASH
MEMORY
WHICH
HAS
DIFFERENT
PROPERTIES
THAN
A
ROTATING
MAGNETIC
DISK
SOME
RECENT
WORK
IS
REPORTED
BY
IN
ET
AL
JOO
ET
AL
AND
PARK
ET
AL
PARK
ET
AL
HAVE
ALSO
LOOKED
AT
ENERGY
AWARE
DEMAND
PAGING
IN
MOBILE
DEVICES
RESEARCH
IS
ALSO
TAKING
PLACE
ON
MODELING
PAGING
PERFORMANCE
ALBERS
ET
AL
BURTON
AND
KELLY
CASCAVAL
ET
AL
PANAGIOTOU
AND
SOUZA
AND
PESERICO
ALSO
OF
INTEREST
IS
MEMORY
MANAGEMENT
FOR
MULTIMEDIA
SYS
TEMS
DASIGENIS
ET
AL
HAND
AND
REAL
TIME
SYSTEMS
PIZLO
AND
VITEK
MEMORY
MANAGEMENT
CHAP
SUMMAR
Y
IN
THIS
CHAPTER
WE
HAVE
EXAMINED
MEMORY
MANAGEMENT
WE
SAW
THAT
THE
SIMPLEST
SYSTEMS
DO
NOT
SWAP
OR
PAGE
AT
ALL
ONCE
A
PROGRAM
IS
LOADED
INTO
MEM
ORY
IT
REMAINS
THERE
IN
PLACE
UNTIL
IT
FINISHES
SOME
OPERATING
SYSTEMS
ALLOW
ONLY
ONE
PROCESS
AT
A
TIME
IN
MEMORY
WHILE
OTHERS
SUPPORT
MULTIPROGRAMMING
THE
NEXT
STEP
UP
IS
SWAPPING
WHEN
SWAPPING
IS
USED
THE
SYSTEM
CAN
HANDLE
MORE
PROCESSES
THAN
IT
HAS
ROOM
FOR
IN
MEMORY
PROCESSES
FOR
WHICH
THERE
IS
NO
ROOM
ARE
SWAPPED
OUT
TO
THE
DISK
FREE
SPACE
IN
MEMORY
AND
ON
DISK
CAN
BE
KEPT
TRACK
OF
WITH
A
BITMAP
OR
A
HOLE
LIST
MODERN
COMPUTERS
OFTEN
HAVE
SOME
FORM
OF
VIRTUAL
MEMORY
IN
THE
SIMPLEST
FORM
EACH
PROCESS
ADDRESS
SPACE
IS
DIVIDED
UP
INTO
UNIFORM
SIZED
BLOCKS
CALLED
PAGES
WHICH
CAN
BE
PLACED
INTO
ANY
AVAILABLE
PAGE
FRAME
IN
MEMORY
THERE
ARE
MANY
PAGE
REPLACEMENT
ALGORITHMS
TWO
OF
THE
BETTER
ALGORITHMS
ARE
AGING
AND
WSCLOCK
PAGING
SYSTEMS
CAN
BE
MODELED
BY
ABSTRACTING
THE
PAGE
REFERENCE
STRING
FROM
THE
PROGRAM
AND
USING
THE
SAME
REFERENCE
STRING
WITH
DIFFERENT
ALGORITHMS
THESE
MODELS
CAN
BE
USED
TO
MAKE
SOME
PREDICTIONS
ABOUT
PAGING
BEHAVIOR
TO
MAKE
PAGING
SYSTEMS
WORK
WELL
CHOOSING
AN
ALGORITHM
IS
NOT
ENOUGH
ATTENTION
TO
SUCH
ISSUES
AS
DETERMINING
THE
WORKING
SET
MEMORY
ALLOCATION
POLICY
AND
PAGE
SIZE
IS
REQUIRED
SEGMENTATION
HELPS
IN
HANDLING
DATA
STRUCTURES
THAT
CHANGE
SIZE
DURING
EXECU
TION
AND
SIMPLIFIES
LINKING
AND
SHARING
IT
ALSO
FACILITATES
PROVIDING
DIFFERENT
PRO
TECTION
FOR
DIFFERENT
SEGMENTS
SOMETIMES
SEGMENTATION
AND
PAGING
ARE
COMBINED
TO
PROVIDE
A
TWO
DIMENSIONAL
VIRTUAL
MEMORY
THE
MULTICS
SYSTEM
AND
THE
INTEL
PENTIUM
SUPPORT
SEGMENTATION
AND
PAGING
PROBLEMS
THE
IBM
HAD
A
SCHEME
OF
LOCKING
KB
BLOCKS
BY
ASSIGNING
EACH
ONE
A
BIT
KEY
AND
HAVING
THE
CPU
COMPARE
THE
KEY
ON
EVERY
MEMORY
REFERENCE
TO
THE
BIT
KEY
IN
THE
PSW
NAME
TWO
DRAWBACKS
OF
THIS
SCHEME
NOT
MENTIONED
IN
THE
TEXT
IN
FIG
THE
BASE
AND
LIMIT
REGISTERS
CONTAIN
THE
SAME
VALUE
IS
THIS
JUST
AN
ACCIDENT
OR
ARE
THEY
ALWAYS
THE
SAME
IF
THIS
IS
JUST
AN
ACCIDENT
WHY
ARE
THEY
THE
SAME
IN
THIS
EXAMPLE
A
SWAPPING
SYSTEM
ELIMINATES
HOLES
BY
COMPACTION
ASSUMING
A
RANDOM
DISTRIBUTION
CHAP
PROBLEMS
CONSIDER
A
SWAPPING
SYSTEM
IN
WHICH
MEMORY
CONSISTS
OF
THE
FOLLOWING
HOLE
SIZES
IN
MEMORY
ORDER
KB
KB
KB
KB
KB
KB
KB
AND
KB
WHICH
HOLE
IS
TAKEN
FOR
SUCCESSIVE
SEGMENT
REQUESTS
OF
A
KB
B
KB
C
KB
FOR
FIRST
FIT
NOW
REPEAT
THE
QUESTION
FOR
BEST
FIT
WORST
FIT
AND
NEXT
FIT
WHAT
IS
THE
DIFFERENCE
BETWEEN
A
PHYSICAL
ADDRESS
AND
A
VIRTUAL
ADDRESS
USING
THE
PAGE
TABLE
OF
FIG
GIVE
THE
PHYSICAL
ADDRESS
CORRESPONDING
TO
EACH
OF
THE
FOLLOWING
VIRTUAL
ADDRESSES
A
B
C
THE
AMOUNT
OF
DISK
SPACE
THAT
MUST
BE
AVAILABLE
FOR
PAGE
STORAGE
IS
RELATED
TO
THE
MAX
IMUM
NUMBER
OF
PROCESSES
N
THE
NUMBER
OF
BYTES
IN
THE
VIRTUAL
ADDRESS
SPACE
V
AND
THE
NUMBER
OF
BYTES
OF
RAM
R
GIVE
AN
EXPRESSION
FOR
THE
WORST
CASE
DISK
SPACE
RE
QUIREMENTS
HOW
REALISTIC
IS
THIS
AMOUNT
IF
AN
INSTRUCTION
TAKES
NSEC
AND
A
PAGE
FAULT
TAKES
AN
ADDITIONAL
N
NSEC
GIVE
A
FOR
MULA
FOR
THE
EFFECTIVE
INSTRUCTION
TIME
IF
PAGE
FAULTS
OCCUR
EVERY
K
INSTRUCTIONS
A
MACHINE
HAS
A
BIT
ADDRESS
SPACE
AND
AN
KB
PAGE
THE
PAGE
TABLE
IS
ENTIRELY
IN
HARDWARE
WITH
ONE
BIT
WORD
PER
ENTRY
WHEN
A
PROCESS
STARTS
THE
PAGE
TABLE
IS
COPIED
TO
THE
HARDWARE
FROM
MEMORY
AT
ONE
WORD
EVERY
NSEC
IF
EACH
PROCESS
RUNS
FOR
MSEC
INCLUDING
THE
TIME
TO
LOAD
THE
PAGE
TABLE
WHAT
FRACTION
OF
THE
CPU
TIME
IS
DEVOTED
TO
LOADING
THE
PAGE
TABLES
SUPPOSE
THAT
A
MACHINE
HAS
BIT
VIRTUAL
ADDRESSES
AND
BIT
PHYSICAL
ADDRESSES
A
IF
PAGES
ARE
KB
HOW
MANY
ENTRIES
ARE
IN
THE
PAGE
TABLE
IF
IT
HAS
ONLY
A
SINGLE
LEVEL
EXPLAIN
B
SUPPOSE
THIS
SAME
SYSTEM
HAS
A
TLB
TRANSLATION
LOOKASIDE
BUFFER
WITH
EN
TRIES
FURTHERMORE
SUPPOSE
THAT
A
PROGRAM
CONTAINS
INSTRUCTIONS
THAT
FIT
INTO
ONE
PAGE
AND
IT
SEQUENTIALLY
READS
LONG
INTEGER
ELEMENTS
FROM
AN
ARRAY
THAT
SPANS
THOUSANDS
OF
PAGES
HOW
EFFECTIVE
WILL
THE
TLB
BE
FOR
THIS
CASE
A
COMPUTER
WITH
A
BIT
ADDRESS
USES
A
TWO
LEVEL
PAGE
TABLE
VIRTUAL
ADDRESSES
ARE
SPLIT
INTO
A
BIT
TOP
LEVEL
PAGE
TABLE
FIELD
AN
BIT
SECOND
LEVEL
PAGE
TABLE
FIELD
AND
AN
OFFSET
HOW
LARGE
ARE
THE
PAGES
AND
HOW
MANY
ARE
THERE
IN
THE
ADDRESS
SPACE
SUPPOSE
THAT
A
BIT
VIRTUAL
ADDRESS
IS
BROKEN
UP
INTO
FOUR
FIELDS
A
B
C
AND
D
THE
FIRST
THREE
ARE
USED
FOR
A
THREE
LEVEL
PAGE
TABLE
SYSTEM
THE
FOURTH
FIELD
D
IS
THE
OFFSET
DOES
THE
NUMBER
OF
PAGES
DEPEND
ON
THE
SIZES
OF
ALL
FOUR
FIELDS
IF
NOT
WHICH
ONES
MATTER
AND
WHICH
ONES
DO
NOT
N
A
COMRJUTER
HAS
BIT
VIRTUAL
ADDRESSES
AND
KB
PAGES
THE
PROGRAM
AND
DATA
TOGET
ES
T
PAGE
THE
STACK
FITS
IN
THE
HIGHEST
PAGE
HOW
EN
MEMORY
MANAGEMENT
CHAP
TRIES
ARE
NEEDED
IN
THE
PAGE
TABLE
IF
TRADITIONAL
ONE
LEVEL
PAGING
IS
USED
HOW
MANY
PAGE
TABLE
ENTRIES
ARE
NEEDED
FOR
TWO
LEVEL
PAGING
WITH
BITS
IN
EACH
PART
BELOW
IS
AN
EXECUTION
TRACE
OF
A
PROGRAM
FRAGMENT
FOR
A
COMPUTER
WITH
BYTE
PAGES
THE
PROGRAM
IS
LOCATED
AT
ADDRESS
AND
ITS
STACK
POINTER
IS
AT
THE
STACK
GROWS
TOWARD
GIVE
THE
PAGE
REFERENCE
STRING
GENERATED
BY
THIS
PROGRAM
EACH
INSTRUCTION
OCCUPIES
BYTES
WORD
INCLUDING
IMMEDIATE
CONSTANTS
BOTH
IN
STRUCTION
AND
DATA
REFERENCES
COUNT
IN
THE
REFERENCE
STRING
LOAD
WORD
INTO
REGISTER
PUSH
REGISTER
ONTO
THE
STACK
CALL
A
PROCEDURE
AT
STACKING
THE
RETURN
ADDRESS
SUBTRACT
THE
IMMEDIATE
CONSTANT
FROM
THE
STACK
POINTER
COMPARE
THE
ACTUAL
PARAMETER
TO
THE
IMMEDIATE
CONSTANT
JUMP
IF
EQUAL
TO
A
COMPUTER
WHOSE
PROCESSES
HAVE
PAGES
IN
THEIR
ADDRESS
SPACES
KEEPS
ITS
PAGE
TABLES
IN
MEMORY
THE
OVERHEAD
REQUIRED
FOR
READING
A
WORD
FROM
THE
PAGE
TABLE
IS
NSEC
TO
REDUCE
THIS
OVERHEAD
THE
COMPUTER
HAS
A
TLB
WHICH
HOLDS
VIRTUAL
PAGE
PHYSICAL
PAGE
FRAME
PAIRS
AND
CAN
DO
A
LOOK
UP
IN
NSEC
WHAT
HIT
RATE
IS
NEEDED
TO
REDUCE
THE
MEAN
OVERHEAD
TO
NSEC
THE
TLB
ON
THE
VAX
DOES
NOT
CONTAIN
AN
R
BIT
WHY
HOW
CAN
THE
ASSOCIATIVE
MEMORY
DEVICE
NEEDED
FOR
A
TLB
BE
IMPLEMENTED
IN
HARD
WARE
AND
WHAT
ARE
THE
IMPLICATIONS
OF
SUCH
A
DESIGN
FOR
EXPANDABILITY
A
COMPUTER
WITH
AN
KB
PAGE
A
KB
MAIN
MEMORY
AND
A
GB
VIRTUAL
ADDRESS
SPACE
USES
AN
INVERTED
PAGE
TABLE
TO
IMPLEMENT
ITS
VIRTUAL
MEMORY
HOW
BIG
SHOULD
THE
HASH
TABLE
BE
TO
ENSURE
A
MEAN
HASH
CHAIN
LENGTH
OF
LESS
THAN
ASSUME
THAT
THE
HASH
TABLE
SIZE
IS
A
POWER
OF
TWO
A
STUDENT
IN
A
COMPILER
DESIGN
COURSE
PROPOSES
TO
THE
PROFESSOR
A
PROJECT
OF
WRITING
A
COMPILER
THAT
WILL
PRODUCE
A
LIST
OF
PAGE
REFERENCES
THAT
CAN
BE
USED
TO
IMPLEMENT
THE
OPTIMAL
PAGE
REPLACEMENT
ALGORITHM
IS
THIS
POSSIBLE
WHY
OR
WHY
NOT
IS
THERE
ANY
THING
THAT
COULD
BE
DONE
TO
IMPROVE
PAGING
EFFICIENCY
AT
RUN
TIME
SUPPOSE
THAT
THE
VIRTUAL
PAGE
REFERENCE
STREAM
CONTAINS
REPETITIONS
OF
LONG
SEQUENCES
OF
PAGE
REFERENCES
FOLLOWED
OCCASIONALLY
BY
A
RANDOM
PAGE
REFERENCE
FOR
EXAMPLE
THE
SEQUENCE
CONSISTS
OF
REPETITIONS
OF
THE
SEQUENCE
FOLLOWED
BY
A
RANDOM
REFERENCE
TO
PAGES
AND
A
WHY
WON
T
THE
STANDARD
REPLACEMENT
ALGORITHMS
LRU
FIFO
CLOCK
BE
EFFECTIVE
IN
HANDLING
THIS
WORKLOAD
FOR
A
PAGE
ALLOCATION
THAT
IS
LESS
THAN
THE
SEQUENCE
LENGTH
B
IF
THIS
PROGRAM
WERE
ALLOCATED
PAGE
FRAMES
DESCRIBE
A
PAGE
REPLACEMENT
AP
PROACH
THAT
WOULD
PERFORM
MUCH
BETTER
THAN
THE
LRU
FIFO
OR
CLOCK
ALGORITHMS
IF
FIFO
PAGE
REPLACEMENT
IS
USED
WITH
FOUR
PAGE
FRAMES
AND
EIGHT
PAGES
HOW
MANY
PAGE
FAULTS
WILL
OCCUR
WITH
THE
REFERENCE
STRING
IF
THE
FOUR
FRAMES
ARE
INI
TIALLY
EMPTY
NOW
REPEAT
THIS
PROBLEM
FOR
LRU
CHAP
PROBLEMS
CONSIDER
THE
PAGE
SEQUENCE
OF
FIG
B
SUPPOSE
THAT
THE
R
BITS
FOR
THE
PAGES
B
THROUGH
A
ARE
RESPECTIVELY
WHICH
PAGE
WILL
SECOND
CHANCE
REMOVE
A
SMALL
COMPUTER
HAS
FOUR
PAGE
FRAMES
AT
THE
FIRST
CLOCK
TICK
THE
R
BITS
ARE
PAGE
IS
THE
REST
ARE
AT
SUBSEQUENT
CLOCK
TICKS
THE
VALUES
ARE
AND
IF
THE
AGING
ALGORITHM
IS
USED
WITH
AN
BIT
COUNT
ER
GIVE
THE
VALUES
OF
THE
FOUR
COUNTERS
AFTER
THE
LAST
TICK
SUPPOSE
THAT
IN
FIG
WHICH
PAGE
WILL
BE
REMOVED
GIVE
A
SIMPLE
EXAMPLE
OF
A
PAGE
REFERENCE
SEQUENCE
WHERE
THE
FIRST
PAGE
SELECTED
FOR
REPLACEMENT
WILL
BE
DIFFERENT
FOR
THE
CLOCK
AND
LRU
PAGE
REPLACEMENT
ALGORITHMS
AS
SUME
THAT
A
PROCESS
IS
ALLOCATED
FRAMES
AND
THE
REFERENCE
STRING
CONTAINS
PAGE
NUM
BERS
FROM
THE
SET
SUPPOSE
THAT
THE
WSCLOCK
PAGE
REPLACEMENT
ALGORITHM
USES
A
X
OF
TWO
TICKS
AND
THE
SYSTEM
STATE
IS
THE
FOLLOWING
PAG
E
TIM
E
STAM
P
V
R
M
WHERE
THE
THREE
FLAG
BITS
STAND
FOR
VALID
REFERENCED
AND
MODIFIED
RESPECTIVELY
A
IF
A
CLOCK
INTERRUPT
OCCURS
AT
TICK
SHOW
THE
CONTENTS
OF
THE
NEW
TABLE
ENTRIES
EXPLAIN
YOU
CAN
OMIT
ENTRIES
THAT
ARE
UNCHANGED
B
SUPPOSE
THAT
INSTEAD
OF
A
CLOCK
INTERRUPT
A
PAGE
FAULT
OCCURS
AT
TICK
DUE
TO
A
READ
REQUEST
TO
PAGE
SHOW
THE
CONTENTS
OF
THE
NEW
TABLE
ENTRIES
EXPLAIN
YOU
CAN
OMIT
ENTRIES
THAT
ARE
UNCHANGED
HOW
LONG
DOES
IT
TAKE
TO
LOAD
A
KB
PROGRAM
FROM
A
DISK
WHOSE
AVERAGE
SEEK
TIME
IS
MSEC
WHOSE
ROTATION
TIME
IS
MSEC
AND
WHOSE
TRACKS
HOLD
KB
A
FOR
A
KB
PAGE
SIZE
B
FOR
A
KB
PAGE
SIZE
THE
PAGES
ARE
SPREAD
RANDOMLY
AROUND
THE
DISK
AND
THE
NUMBER
OF
CYLINDERS
IS
SO
LARGE
THAT
THE
CHANCE
OF
TWO
PAGES
BEING
ON
THE
SAME
CYLINDER
IS
NEGLIGIBLE
A
COMPUTER
HAS
FOUR
PAGE
FRAMES
THE
TIME
OF
LOADING
TIME
OF
LAST
ACCESS
AND
THE
R
AND
M
BITS
FOR
EACH
PAGE
ARE
AS
SHOWN
BELOW
THE
TIMES
ARE
IN
CLOCK
TICKS
PAG
E
LOADE
D
LAS
T
REF
R
M
MEMORY
MANAGEMENT
CHAP
A
WHICH
PAGE
WILL
NRU
REPLACE
B
WHICH
PAGE
WILL
FIFO
REPLACE
C
WHICH
PAGE
WILL
LRU
REPLACE
D
WHICH
PAGE
WILL
SECOND
CHANCE
REPLACE
ONE
OF
THE
FIRST
TIMESHARING
MACHINES
THE
PDP
HAD
A
MEMORY
OF
BIT
WORDS
IT
HELD
ONE
PROCESS
AT
A
TIME
IN
MEMORY
WHEN
THE
SCHEDULER
DECIDED
TO
RUN
ANOTHER
PROCESS
THE
PROCESS
IN
MEMORY
WAS
WRITTEN
TO
A
PAGING
DRUM
WITH
BIT
WORDS
AROUND
THE
CIRCUMFERENCE
OF
THE
DRUM
THE
DRUM
COULD
START
WRITING
OR
READING
AT
ANY
WORD
RATHER
THAN
ONLY
AT
WORD
WHY
DO
YOU
SUPPOSE
THIS
DRUM
WAS
CHOSEN
A
COMPUTER
PROVIDES
EACH
PROCESS
WITH
BYTES
OF
ADDRESS
SPACE
DIVIDED
INTO
PAGES
OF
BYTES
A
PARTICULAR
PROGRAM
HAS
A
TEXT
SIZE
OF
BYTES
A
DATA
SIZE
OF
BYTES
AND
A
STACK
SIZE
OF
BYTES
WILL
THIS
PROGRAM
FIT
IN
THE
ADDRESS
SPACE
IF
THE
PAGE
SIZE
WERE
BYTES
WOULD
IT
FIT
REMEMBER
THAT
A
PAGE
MAY
NOT
CONTAIN
PARTS
OF
TWO
DIFFERENT
SEGMENTS
IF
A
PAGE
IS
SHARED
BETWEEN
TWO
PROCESSES
IS
IT
POSSIBLE
THAT
THE
PAGE
IS
READ
ONLY
FOR
ONE
PROCESS
AND
READ
WRITE
FOR
THE
OTHER
WHY
OR
WHY
NOT
IT
HAS
BEEN
OBSERVED
THAT
THE
NUMBER
OF
INSTRUCTIONS
EXECUTED
BETWEEN
PAGE
FAULTS
IS
DIRECTLY
PROPORTIONAL
TO
THE
NUMBER
OF
PAGE
FRAMES
ALLOCATED
TO
A
PROGRAM
IF
THE
AVAILABLE
MEMORY
IS
DOUBLED
THE
MEAN
INTERVAL
BETWEEN
PAGE
FAULTS
IS
ALSO
DOUBLED
SUPPOSE
THAT
A
NORMAL
INSTRUCTION
TAKES
MICROSEC
BUT
IF
A
PAGE
FAULT
OCCURS
IT
TAKES
USEE
I
E
MSEC
TO
HANDLE
THE
FAULT
IF
A
PROGRAM
TAKES
SEC
TO
RUN
DURING
WHICH
TIME
IT
GETS
PAGE
FAULTS
HOW
LONG
WOULD
IT
TAKE
TO
RUN
IF
TWICE
AS
MUCH
MEMORY
WERE
AVAILABLE
A
GROUP
OF
OPERATING
SYSTEM
DESIGNERS
FOR
THE
FRUGAL
COMPUTER
COMPANY
ARE
THINKING
ABOUT
WAYS
TO
REDUCE
THE
AMOUNT
OF
BACKING
STORE
NEEDED
IN
THEIR
NEW
OPERATING
SYS
TEM
THE
HEAD
GURU
HAS
JUST
SUGGESTED
NOT
BOTHERING
TO
SAVE
THE
PROGRAM
TEXT
IN
THE
SWAP
AREA
AT
ALL
BUT
JUST
PAGE
IT
IN
DIRECTLY
FROM
THE
BINARY
FILE
WHENEVER
IT
IS
NEEDED
UNDER
WHAT
CONDITIONS
IF
ANY
DOES
THIS
IDEA
WORK
FOR
THE
PROGRAM
TEXT
UNDER
WHAT
CONDITIONS
IF
ANY
DOES
IT
WORK
FOR
THE
DATA
A
MACHINE
LANGUAGE
INSTRUCTION
TO
LOAD
A
BIT
WORD
INTO
A
REGISTER
CONTAINS
THE
BIT
ADDRESS
OF
THE
WORD
TO
BE
LOADED
WHAT
IS
THE
MAXIMUM
NUMBER
OF
PAGE
FAULTS
THIS
INSTRUCTION
CAN
CAUSE
EXPLAIN
THE
DIFFERENCE
BETWEEN
INTERNAL
FRAGMENTATION
AND
EXTERNAL
FRAGMENTATION
WHICH
ONE
OCCURS
IN
PAGING
SYSTEMS
WHICH
ONE
OCCURS
IN
SYSTEMS
USING
PURE
SEG
MENTATION
WHEN
SEGMENTATION
AND
PAGING
ARE
BOTH
BEING
USED
AS
IN
MULTICS
FIRST
THE
SEG
MENT
DESCRIPTOR
MUST
BE
LOOKED
UP
THEN
THE
PAGE
DESCRIPTOR
DOES
THE
TLB
ALSO
WORK
THIS
WAY
WITH
TWO
LEVELS
OF
LOOKUP
WE
CONSIDER
A
PROGRAM
WHICH
HAS
THE
TWO
SEGMENTS
SHOWN
BELOW
CONSISTING
OF
IN
STRUCTIONS
IN
SEGMENT
AND
READ
WRITE
DATA
IN
SEGMENT
SEGMENT
HAS
READ
EXECUTE
PROTECTION
AND
SEGMENT
HAS
READ
WRITE
PROTECTION
THE
MEMORY
SYSTEM
IS
A
DE
MAND
PAGED
VIRTUAL
MEMORY
SYSTEM
WITH
VIRTUAL
ADDRESSES
THAT
HAVE
A
BIT
PAGE
NUM
CHAP
PROBLEMS
BER
AND
AN
BIT
OFFSET
THE
PAGE
TABLES
AND
PROTECTION
ARE
AS
FOLLOWS
ALL
NUMBERS
IN
THE
TABLE
ARE
IN
DECIMAL
SEGMEN
T
SEGMEN
T
READ
E
ECUT
E
READ
WRIT
E
VIRTUA
L
F
AG
E
PAG
E
FRAM
E
VIRTUA
L
PAG
E
PAG
E
FRAM
E
O
N
DISK
O
N
DIS
K
O
N
DIS
K
O
N
DISK
O
N
DIS
K
FOR
EACH
OF
THE
FOLLOWING
CASES
EITHER
GIVE
THE
REAL
ACTUAL
MEMORY
ADDRESS
WHICH
RE
SULTS
FROM
DYNAMIC
ADDRESS
TRANSLATION
OR
IDENTIFY
THE
TYPE
OF
FAULT
WHICH
OCCURS
EI
THER
PAGE
OR
PROTECTION
FAULT
A
FETCH
FROM
SEGMENT
PAGE
OFFSET
B
STORE
INTO
SEGMENT
PAGE
OFFSET
C
FETCH
FROM
SEGMENT
PAGE
OFFSET
D
JUMP
TO
LOCATION
IN
SEGMENT
PAGE
OFFSET
CAN
YOU
THINK
OF
ANY
SITUATIONS
WHERE
SUPPORTING
VIRTUAL
MEMORY
WOULD
BE
A
BAD
IDEA
AND
WHAT
WOULD
BE
GAINED
BY
NOT
HAVING
TO
SUPPORT
VIRTUAL
MEMORY
EXPLAIN
VIRTUAL
MEMORY
PROVIDES
A
MECHANISM
FOR
ISOLATING
ONE
PROCESS
FROM
ANOTHER
WHAT
MEMORY
MANAGEMENT
DIFFICULTIES
WOULD
BE
INVOLVED
IN
ALLOWING
TWO
OPERATING
SYS
TEMS
TO
RUN
CONCURRENTLY
HOW
MIGHT
THESE
DIFFICULTIES
BE
ADDRESSED
PLOT
A
HISTOGRAM
AND
CALCULATE
THE
MEAN
AND
MEDIAN
OF
THE
SIZES
OF
EXECUTABLE
BINARY
FILES
ON
A
COMPUTER
TO
WHICH
YOU
HAVE
ACCESS
ON
A
WINDOWS
SYSTEM
LOOK
AT
ALL
EXE
AND
DLL
FILES
ON
A
UNIX
SYSTEM
LOOK
AT
ALL
EXECUTABLE
FILES
IN
BIN
HISR
BIN
AND
AOCAL
BIN
THAT
ARE
NOT
SCRIPTS
OR
USE
THE
FILE
UTILITY
TO
FIND
ALL
EXECUTABLES
DETERMINE
THE
OPTIMAL
PAGE
SIZE
FOR
THIS
COMPUTER
JUST
CONSIDERING
THE
CODE
NOT
DATA
CONSIDER
INTERNAL
FRAGMENTATION
AND
PAGE
TABLE
SIZE
MAKING
SOME
REASONABLE
ASSUMPTION
ABOUT
THE
SIZE
OF
A
PAGE
TABLE
ENTRY
ASSUME
THAT
ALL
PROGRAMS
ARE
EQUALLY
LIKELY
TO
BE
RUN
AND
THUS
SHOULD
BE
WEIGHTED
EQUALLY
SMALL
PROGRAMS
FOR
MS
DOS
CAN
BE
COMPILED
AS
COM
FILES
THESE
FILES
ARE
ALWAYS
LOADED
AT
ADDRESS
IN
A
SINGLE
MEMORY
SEGMENT
THAT
IS
USED
FOR
CODE
DATA
AND
STACK
INSTRUCTIONS
THAT
TRANSFER
CONTROL
OF
EXECUTION
SUCH
AS
JMP
AND
CALL
OR
THAT
AC
CESS
STATIC
DATA
FROM
FIXED
ADDRESSES
HAVE
THE
ADDRESSES
COMPILED
INTO
THE
OBJECT
CODE
WRITE
A
PROGRAM
THAT
CAN
RELOCATE
SUCH
A
PROGRAM
FILE
TO
RUN
STARTING
AT
AN
ARBITRARY
AD
DRESS
YOUR
PROGRAM
MUST
SCAN
THROUGH
CODE
LOOKING
FOR
OBJECT
CODES
FOR
INSTRUCTIONS
THAT
REFER
TO
FIXED
MEMORY
ADDRESSES
THEN
MODIFY
THOSE
ADDRESSES
THAT
POINT
TO
MEMO
RY
LOCATIONS
WITHIN
THE
RANGE
TO
BE
RELOCATED
YOU
CAN
FIND
THE
OBJECT
CODES
IN
AN
AS
MEMORY
MANAGEMENT
CHAP
SEMBLY
LANGUAGE
PROGRAMMING
TEXT
NOTE
THAT
DOING
THIS
PERFECTLY
WITHOUT
ADDITIONAL
INFORMATION
IS
IN
GENERAL
AN
IMPOSSIBLE
TASK
BECAUSE
SOME
DATA
WORDS
MAY
HAVE
VALUES
THAT
MIMIC
INSTRUCTION
OBJECT
CODES
WRITE
A
PROGRAM
THAT
SIMULATES
A
PAGING
SYSTEM
USING
THE
AGING
ALGORITHM
THE
NUM
BER
OF
PAGE
FRAMES
IS
A
PARAMETER
THE
SEQUENCE
OF
PAGE
REFERENCES
SHOULD
BE
READ
FROM
A
FILE
FOR
A
GIVEN
INPUT
FILE
PLOT
THE
NUMBER
OF
PAGE
FAULTS
PER
MEMORY
REF
ERENCES
AS
A
FUNCTION
OF
THE
NUMBER
OF
PAGE
FRAMES
AVAILABLE
WRITE
A
PROGRAM
THAT
DEMONSTRATES
THE
EFFECT
OF
TLB
MISSES
ON
THE
EFFECTIVE
MEMORY
ACCESS
TIME
BY
MEASURING
THE
PER
ACCESS
TIME
IT
TAKES
TO
STRIDE
THROUGH
A
LARGE
ARRAY
A
EXPLAIN
THE
MAIN
CONCEPTS
BEHIND
THE
PROGRAM
AND
DESCRIBE
WHAT
YOU
EXPECT
THE
OUTPUT
TO
SHOW
FOR
SOME
PRACTICAL
VIRTUAL
MEMORY
ARCHITECTURE
B
RUN
THE
PROGRAM
ON
SOME
COMPUTER
AND
EXPLAIN
HOW
WELL
THE
DATA
FIT
YOUR
EXPEC
TATIONS
C
REPEAT
PART
B
BUT
FOR
AN
OLDER
COMPUTER
WITH
A
DIFFERENT
ARCHITECTURE
AND
EXPLAIN
ANY
MAJOR
DIFFERENCES
IN
THE
OUTPUT
WRITE
A
PROGRAM
THAT
WILL
DEMONSTRATE
THE
DIFFERENCE
BETWEEN
USING
A
LOCAL
PAGE
REPLACEMENT
POLICY
AND
A
GLOBAL
ONE
FOR
THE
SIMPLE
CASE
OF
TWO
PROCESSES
YOU
WILL
NEED
A
ROUTINE
THAT
CAN
GENERATE
A
PAGE
REFERENCE
STRING
BASED
ON
A
STATISTICAL
MODEL
THIS
MODEL
HAS
N
STATES
NUMBERED
FROM
TO
N
REPRESENTING
EACH
OF
THE
POSSIBLE
PAGE
REFERENCES
AND
A
PROBABILITY
P
ASSOCIATED
WITH
EACH
STATE
I
REPRESENTING
THE
CHANCE
THAT
THE
NEXT
REFERENCE
IS
TO
THE
SAME
PAGE
OTHERWISE
THE
NEXT
PAGE
REFERENCE
WILL
BE
ONE
OF
THE
OTHER
PAGES
WITH
EQUAL
PROBABILITY
A
DEMONSTRATE
THAT
THE
PAGE
REFERENCE
STRING
GENERATION
ROUTINE
BEHAVES
PROPERLY
FOR
SOME
SMALL
N
B
COMPUTE
THE
PAGE
FAULT
RATE
FOR
A
SMALL
EXAMPLE
IN
WHICH
THERE
IS
ONE
PROCESS
AND
A
FIXED
NUMBER
OF
PAGE
FRAMES
EXPLAIN
WHY
THE
BEHAVIOR
IS
CORRECT
C
REPEAT
PART
B
WITH
TWO
PROCESSES
WITH
INDEPENDENT
PAGE
REFERENCE
SEQUENCES
AND
TWICE
AS
MANY
PAGE
FRAMES
AS
IN
PART
B
D
REPEAT
PART
C
BUT
USING
A
GLOBAL
POLICY
INSTEAD
OF
A
LOCAL
ONE
ALSO
CONTRAST
THE
PER
PROCESS
PAGE
FAULT
RATE
WITH
THAT
OF
THE
LOCAL
POLICY
APPROACH
FILE
SYSTEMS
ALL
COMPUTER
APPLICATIONS
NEED
TO
STORE
AND
RETRIEVE
INFORMATION
TWHILE
A
PROCESS
IS
RUNNING
IT
CAN
STORE
A
LIMITED
AMOUNT
OF
INFORMATION
WITHIN
ITS
OWN
AD
DRESS
SPACE
HOWEVER
THE
STORAGE
CAPACITY
IS
RESTRICTED
TO
THE
SIZE
OF
THE
VIRTUAL
ADDRESS
SPACE
FOR
SOME
APPLICATIONS
THIS
SIZE
IS
ADEQUATE
BUT
FOR
OTHERS
SUCH
AS
AIRLINE
RESERVATIONS
BANKING
OR
CORPORATE
RECORD
KEEPING
IT
IS
FAR
TOO
SMALL
A
SECOND
PROBLEM
WITH
KEEPING
INFORMATION
WITHIN
A
PROCESS
ADDRESS
SPACE
IS
THAT
WHEN
THE
PROCESS
TERMINATES
THE
INFORMATION
IS
LOST
FOR
MANY
APPLICATIONS
E
G
FOR
DATABASES
THE
INFORMATION
MUST
BE
RETAINED
FOR
WEEKS
MONTHS
OR
EVEN
FOREVER
HAVING
IT
VANISH
WHEN
THE
PROCESS
USING
IT
TERMINATES
IS
UNACCEPTABLE
FURTHERMORE
IT
MUST
NOT
GO
AWAY
WHEN
A
COMPUTER
CRASH
KILLS
THE
PROCESS
A
THIRD
PROBLEM
IS
THAT
IT
IS
FREQUENTLY
NECESSARY
FOR
MULTIPLE
PROCESSES
TO
AC
CESS
PARTS
OF
THE
INFORMATION
AT
THE
SAME
TIME
IF
WE
HAVE
AN
ONLINE
TELEPHONE
DIRECTORY
STORED
INSIDE
THE
ADDRESS
SPACE
OF
A
SINGLE
PROCESS
ONLY
THAT
PROCESS
CAN
ACCESS
IT
THE
WAY
TO
SOLVE
THIS
PROBLEM
IS
TO
MAKE
THE
INFORMATION
ITSELF
INDEPEN
DENT
OF
ANY
ONE
PROCESS
THUS
WE
HAVE
THREE
ESSENTIAL
REQUIREMENTS
FOR
LONG
TERM
INFORMATION
STORAGE
IT
MUST
BE
POSSIBLE
TO
STORE
A
VERY
LARGE
AMOUNT
OF
INFORMATION
THE
INFORMATION
MUST
SURVIVE
THE
TERMINATION
OF
THE
PROCESS
USING
IT
MULTIPLE
PROCESSES
MUST
BE
ABLE
TO
ACCESS
THE
INFORMATION
CONCURRENTLY
MAGNETIC
DISKS
HAVE
BEEN
USED
FOR
YEARS
FOR
THIS
LONG
TERM
STORAGE
TAPES
AND
OPTICAL
DISKS
ARE
ALSO
USED
BUT
THEY
HAVE
MUCH
LOWER
PERFORMANCE
WE
WILL
STUDY
FILE
SYSTEMS
CHAP
DISKS
MORE
IN
CHAP
BUT
FOR
THE
MOMENT
IT
IS
SUFFICIENT
TO
THINK
OF
A
DISK
AS
A
LINEAR
SEQUENCE
OF
FIXED
SIZE
BLOCKS
AND
SUPPORTING
TWO
OPERATIONS
READ
BLOCK
K
WRITE
BLOCK
K
IN
REALITY
THERE
ARE
MORE
BUT
WITH
THESE
TWO
OPERATIONS
ONE
COULD
IN
PRINCIPLE
SOLVE
THE
LONG
TERM
STORAGE
PROBLEM
HOWEVER
THESE
ARE
VERY
INCONVENIENT
OPERATIONS
ESPECIALLY
ON
LARGE
SYSTEMS
USED
BY
MANY
APPLICATIONS
AND
POSSIBLY
MULTIPLE
USERS
E
G
ON
A
SERVER
JUST
A
FEW
OF
THE
QUESTIONS
THAT
QUICKLY
ARISE
ARE
HOW
DO
YOU
FIND
INFORMATION
HOW
DO
YOU
KEEP
ONE
USER
FROM
READING
ANOTHER
USER
DATA
HOW
DO
YOU
KNOW
WHICH
BLOCKS
ARE
FREE
AND
THERE
ARE
MANY
MORE
JUST
AS
WE
SAW
HOW
THE
OPERATING
SYSTEM
ABSTRACTED
AWAY
THE
CONCEPT
OF
THE
PROCESSOR
TO
CREATE
THE
ABSTRACTION
OF
A
PROCESS
AND
HOW
IT
ABSTRACTED
AWAY
THE
CONCEPT
OF
PHYSICAL
MEMORY
TO
OFFER
PROCESSES
VIRTUAL
ADDRESS
SPACES
WE
CAN
SOLVE
THIS
PROBLEM
WITH
A
NEW
ABSTRACTION
THE
FILE
TOGETHER
THE
ABSTRACTIONS
OF
PROCESSES
AND
THREADS
ADDRESS
SPACES
AND
FILES
ARE
THE
MOST
IMPORTANT
CONCEPTS
RELATING
TO
OPERATING
SYSTEMS
IF
YOU
REALLY
UNDERSTAND
THESE
THREE
CONCEPTS
FROM
BEGINNING
TO
END
YOU
ARE
WELL
ON
YOUR
WAY
TO
BECOMING
AN
OPERATING
SYSTEMS
EXPERT
FILES
ARE
LOGICAL
UNITS
OF
INFORMATION
CREATED
BY
PROCESSES
A
DISK
WILL
USUAL
LY
CONTAINS
THOUSANDS
OR
EVEN
MILLIONS
OF
THEM
EACH
ONE
INDEPENDENT
OF
THE
OTH
ERS
IN
FACT
IF
YOU
THINK
OF
EACH
FILE
AS
A
KIND
OF
ADDRESS
SPACE
YOU
ARE
NOT
THAT
FAR
OFF
EXCEPT
THAT
THEY
ARE
USED
TO
MODEL
THE
DISK
INSTEAD
OF
MODELING
THE
RAM
PROCESSES
CAN
READ
EXISTING
FILES
AND
CREATE
NEW
ONES
IF
NEED
BE
INFORMATION
STORED
IN
FILES
MUST
BE
PERSISTENT
THAT
IS
NOT
BE
AFFECTED
BY
PROCESS
CREATION
AND
TERMINATION
A
FILE
SHOULD
ONLY
DISAPPEAR
WHEN
ITS
OWNER
EXPLICITLY
REMOVES
IT
ALTHOUGH
OPERATIONS
FOR
READING
AND
WRITING
FILES
ARE
THE
MOST
COMMON
ONES
THERE
EXIST
MANY
OTHERS
SOME
OF
WHICH
WE
WILL
EXAMINE
BELOW
FILES
ARE
MANAGED
BY
THE
OPERATING
SYSTEM
HOW
THEY
ARE
STRUCTURED
NAMED
ACCESSED
USED
PROTECTED
IMPLEMENTED
AND
MANAGED
ARE
MAJOR
TOPICS
IN
OPERAT
ING
SYSTEM
DESIGN
AS
A
WHOLE
THAT
PART
OF
THE
OPERATING
SYSTEM
DEALING
WITH
FILES
IS
KNOWN
AS
THE
FILE
SYSTEM
AND
IS
THE
SUBJECT
OF
THIS
CHAPTER
FROM
THE
USER
STANDPOINT
THE
MOST
IMPORTANT
ASPECT
OF
A
FILE
SYSTEM
IS
HOW
IT
APPEARS
THAT
IS
WHAT
CONSTITUTES
A
FILE
HOW
FILES
ARE
NAMED
AND
PROTECTED
WHAT
OPERATIONS
ARE
ALLOWED
ON
FILES
AND
SO
ON
THE
DETAILS
OF
WHETHER
LINKED
LISTS
OR
BITMAPS
ARE
USED
TO
KEEP
TRACK
OF
FREE
STORAGE
AND
HOW
MANY
SECTORS
THERE
ARE
IN
A
LOGICAL
DISK
BLOCK
ARE
OF
NO
INTEREST
ALTHOUGH
THEY
ARE
OF
GREAT
IMPORTANCE
TO
THE
DESIGNERS
OF
THE
FILE
SYSTEM
FOR
THIS
REASON
WE
HAVE
STRUCTURED
THE
CHAPTER
AS
SEVERAL
SECTIONS
THE
FIRST
TWO
ARE
CONCERNED
WITH
THE
USER
INTERFACE
TO
FILES
AND
DIRECTORIES
RESPECTIVELY
THEN
COMES
A
DETAILED
DISCUSSION
OF
HOW
THE
FILE
SYSTEM
IS
IMPLEMENTED
AND
MANAGED
FINALLY
WE
GIVE
SOME
EXAMPLES
OF
REAL
FILE
SYS
TEMS
FILES
IN
THE
FOLLOWING
PAGES
WE
WILL
LOOK
AT
FILES
FROM
THE
USER
POINT
OF
VIEW
THAT
IS
HOW
THEY
ARE
USED
AND
WHAT
PROPERTIES
THEY
HAVE
FILE
NAMING
FILES
ARE
AN
ABSTRACTION
MECHANISM
THEY
PROVIDE
A
WAY
TO
STORE
INFORMATION
ON
THE
DISK
AND
READ
IT
BACK
LATER
THIS
MUST
BE
DONE
IN
SUCH
A
WAY
AS
TO
SHIELD
THE
USER
FROM
THE
DETAILS
OF
HOW
AND
WHERE
THE
INFORMATION
IS
STORED
AND
HOW
THE
DISKS
ACTUALLY
WORK
PROBABLY
THE
MOST
IMPORTANT
CHARACTERISTIC
OF
ANY
ABSTRACTION
MECHANISM
IS
THE
WAY
THE
OBJECTS
BEING
MANAGED
ARE
NAMED
SO
WE
WILL
START
OUR
EXAMINATION
OF
FILE
SYSTEMS
WITH
THE
SUBJECT
OF
FILE
NAMING
WHEN
A
PROCESS
CREATES
A
FILE
IT
GIVES
THE
FILE
A
NAME
WHEN
THE
PROCESS
TERMINATES
THE
FILE
CONTINUES
TO
EXIST
AND
CAN
BE
ACCESSED
BY
OTHER
PROCESSES
USING
ITS
NAME
THE
EXACT
RULES
FOR
FILE
NAMING
VARY
SOMEWHAT
FROM
SYSTEM
TO
SYSTEM
BUT
ALL
CURRENT
OPERATING
SYSTEMS
ALLOW
STRINGS
OF
ONE
TO
EIGHT
LETTERS
AS
LEGAL
FILE
NAMES
THUS
ANDREA
BRUCE
AND
CATHY
ARE
POSSIBLE
FILE
NAMES
FREQUENTLY
DIGITS
AND
SPE
CIAL
CHARACTERS
ARE
ALSO
PERMITTED
SO
NAMES
LIKE
URGENT
AND
FIG
ARE
OFTEN
VALID
AS
WELL
MANY
FILE
SYSTEMS
SUPPORT
NAMES
AS
LONG
AS
CHARACTERS
SOME
FILE
SYSTEMS
DISTINGUISH
BETWEEN
UPPER
AND
LOWER
CASE
LETTERS
WHEREAS
OTHERS
DO
NOT
UNIX
FALLS
IN
THE
FIRST
CATEGORY
MS
DOS
FALLS
IN
THE
SECOND
THUS
A
UNIX
SYSTEM
CAN
HAVE
ALL
OF
THE
FOLLOWING
AS
THREE
DISTINCT
FILES
MARIA
MARIA
AND
MARIA
IN
MS
DOS
ALL
THESE
NAMES
REFER
TO
THE
SAME
FILE
AN
ASIDE
ON
FILE
SYSTEMS
IS
PROBABLY
IN
ORDER
HERE
WINDOWS
AND
WINDOWS
BOTH
USE
THE
MS
DOS
FILE
SYSTEM
CALLED
FAT
AND
THUS
INHERIT
MANY
OF
ITS
PROPERTIES
SUCH
AS
HOW
FILE
NAMES
ARE
CONSTRUCTED
WINDOWS
INTRODUCED
SOME
EXTENSIONS
TO
FAT
LEADING
TO
FAT
BUT
THESE
TWO
ARE
QUITE
SIMILAR
IN
AD
DITION
WINDOWS
NT
WINDOWS
WINDOWS
XP
AND
WV
SUPPORT
BOTH
FAT
FILE
SYSTEMS
WHICH
ARE
REALLY
OBSOLETE
NOW
THESE
FOUR
NT
BASED
OPERATING
SYS
TEMS
HAVE
A
NATIVE
FILE
SYSTEM
NTFS
THAT
HAS
DIFFERENT
PROPERTIES
SUCH
AS
FILE
NAMES
IN
UNICODE
IN
THIS
CHAPTER
WHEN
WE
REFER
TO
THE
MS
DOS
OR
FAT
FILE
SYSTEMS
WE
MEAN
FAT
AND
FAT
AS
USED
ON
WINDOWS
UNLESS
SPECIFIED
OTHERWISE
WE
WILL
DISCUSS
THE
FAT
FILE
SYSTEMS
LATER
IN
THIS
CHAPTER
AND
NTFS
IN
CHAP
WHERE
WE
WILL
EXAMINE
WINDOWS
VISTA
IN
DETAIL
FILE
SYSTEMS
CHAP
MANY
OPERATING
SYSTEMS
SUPPORT
TWO
PART
FILE
NAMES
WITH
THE
TWO
PARTS
SEPA
RATED
BY
A
PERIOD
AS
IN
PROG
C
THE
PART
FOLLOWING
THE
PERIOD
IS
CALLED
THE
FILE
EXTENSION
AND
USUALLY
INDICATES
SOMETHING
ABOUT
THE
FILE
IN
MS
DOS
FOR
EX
AMPLE
FILE
NAMES
ARE
TO
CHARACTERS
PLUS
AN
OPTIONAL
EXTENSION
OF
TO
CHAR
ACTERS
IN
UNIX
THE
SIZE
OF
THE
EXTENSION
IF
ANY
IS
UP
TO
THE
USER
AND
A
FILE
MAY
EVEN
HAVE
TWO
OR
MORE
EXTENSIONS
AS
IN
HOMEPAGE
HTML
ZIP
WHERE
HTML
INDICATES
A
WEB
PAGE
IN
HTML
AND
ZIP
INDICATES
THAT
THE
FILE
HOMEPAGE
HTML
HAS
BEEN
COMPRESSED
USING
THE
ZIP
PROGRAM
SOME
OF
THE
MORE
COMMON
FILE
EXTENSIONS
AND
THEIR
MEANINGS
ARE
SHOWN
IN
FIG
EXTENSION
MEANING
FILE
BAK
BACKUP
FILE
FILE
C
C
M
IRR
FI
PROGRAM
FILE
GIF
COMPUSERVE
GRAPHICAL
INTERCHANGE
FORMAT
IMAGE
FILE
HIP
HELP
FILE
FILE
HTMI
WORLD
WIDE
WEB
HYPERTEXT
MARKUP
LANGUAGE
DOCUMENT
FILE
JPG
STILL
PICTURE
ENCODED
WITH
THE
JPEG
STANDARD
FILE
MUSIC
ENCODED
IN
MPEG
LAYER
AUDIO
FORMAT
FILE
MPG
MOVIE
ENCODED
WITH
THE
MPEG
STANDARD
FILE
O
OBJECT
FILE
COMPILER
OUTPUT
NOT
YET
LINKED
FILE
PDF
PORTABLE
DOCUMENT
FORMAT
FILE
FIIE
PS
POSTSCRIPT
FILE
FILE
TEX
INPUT
FOR
THE
TEX
FORMATTING
PROGRAM
FILE
TXT
GENERAL
TEXT
FILE
FIIE
ZIP
COMPRESSED
ARCHIVE
FIGURE
SOME
TYPICAL
FILE
EXTENSIONS
IN
SOME
SYSTEMS
E
G
UNIX
FILE
EXTENSIONS
ARE
JUST
CONVENTIONS
AND
ARE
NOT
ENFORCED
BY
THE
OPERATING
SYSTEM
A
FILE
NAMED
FILE
TXT
MIGHT
BE
SOME
KIND
OF
TEXT
FILE
BUT
THAT
NAME
IS
MORE
TO
REMIND
THE
OWNER
THAN
TO
CONVEY
ANY
ACTUAL
INFOR
MATION
TO
THE
COMPUTER
ON
THE
OTHER
HAND
A
C
COMPILER
MAY
ACTUALLY
INSIST
THAT
FILES
IT
IS
TO
COMPILE
END
IN
C
AND
IT
MAY
REFUSE
TO
COMPILE
THEM
IF
THEY
DO
NOT
CONVENTIONS
LIKE
THIS
ARE
ESPECIALLY
USEFUL
WHEN
THE
SAME
PROGRAM
CAN
HANDLE
SEVERAL
DIFFERENT
KINDS
OF
FILES
THE
C
COMPILER
FOR
EXAMPLE
CAN
BE
GIVEN
A
LIST
OF
SEVERAL
FILES
TO
COMPILE
AND
LINK
TOGETHER
SOME
OF
THEM
C
FILES
AND
SOME
OF
THEM
ASSEMBLY
LANGUAGE
FILES
THE
EXTENSION
THEN
BECOMES
ESSENTIAL
FOR
THE
COMPILER
TO
TELL
WHICH
ARE
C
FILES
WHICH
ARE
ASSEMBLY
FILES
AND
WHICH
ARE
OTHER
FILES
IN
CONTRAST
WINDOWS
IS
AWARE
OF
THE
EXTENSIONS
AND
ASSIGNS
MEANING
TO
THEM
USERS
OR
PROCESSES
CAN
REGISTER
EXTENSIONS
WITH
THE
OPERATING
SYSTEM
AND
SPECIFY
FOR
EACH
ONE
WHICH
PROGRAM
OWNS
THAT
EXTENSION
WHEN
A
USER
DOUBLE
CLICKS
ON
SEC
FILES
A
FILE
NAME
THE
PROGRAM
ASSIGNED
TO
ITS
FILE
EXTENSION
IS
LAUNCHED
WITH
THE
FILE
AS
PARAMETER
FOR
EXAMPLE
DOUBLE
CLICKING
ONFILE
DOC
STARTS
MICROSOFT
WORD
WITH
FILE
DOC
AS
THE
INITIAL
FILE
TO
EDIT
FILE
STRUCTURE
FILES
CAN
BE
STRUCTURED
IN
ANY
OF
SEVERAL
WAYS
THREE
COMMON
POSSIBILITIES
ARE
DEPICTED
IN
FIG
THE
FILE
IN
FIG
A
IS
AN
UNSTRUCTURED
SEQUENCE
OF
BYTES
IN
EFFECT
THE
OPERATING
SYSTEM
DOES
NOT
KNOW
OR
CARE
WHAT
IS
IN
THE
FILE
ALL
IT
SEES
ARE
BYTES
ANY
MEANING
MUST
BE
IMPOSED
BY
USER
LEVEL
PROGRAMS
BOTH
UNIX
AND
WINDOWS
USE
THIS
APPROACH
BYT
E
RECORD
FIGURE
THREE
KINDS
OF
FILES
A
BYTE
SEQUENCE
B
RECORD
SEQUENCE
C
TREE
HAVING
THE
OPERATING
SYSTEM
REGARD
FILES
AS
NOTHING
MORE
THAN
BYTE
SEQUENCES
PROVIDES
THE
MAXIMUM
FLEXIBILITY
USER
PROGRAMS
CAN
PUT
ANYTHING
THEY
WANT
IN
THEIR
FILES
AND
NAME
THEM
ANY
WAY
THAT
IS
CONVENIENT
THE
OPERATING
SYSTEM
DOES
NOT
HELP
BUT
IT
ALSO
DOES
NOT
GET
IN
THE
WAY
FOR
USERS
WHO
WANT
TO
DO
UNUSUAL
THINGS
THE
LATTER
CAN
BE
VERY
IMPORTANT
ALL
VERSIONS
OF
UNIX
MS
DOS
AND
WIN
DOWS
USE
THIS
FILE
MODEL
THE
FIRST
STEP
UP
IN
STRUCTURE
IS
SHOWN
IN
FIG
B
IN
THIS
MODEL
A
FILE
IS
A
SEQUENCE
OF
FIXED
LENGTH
RECORDS
EACH
WITH
SOME
INTERNAL
STRUCTURE
CENTRAL
TO
THE
IDEA
OF
A
FILE
BEING
A
SEQUENCE
OF
RECORDS
IS
THE
IDEA
THAT
THE
READ
OPERATION
RETURNS
ONE
RECORD
AND
THE
WRITE
OPERATION
OVERWRITES
OR
APPENDS
ONE
RECORD
AS
A
HISTORI
CAL
NOTE
IN
DECADES
GONE
BY
WHEN
THE
COLUMN
PUNCHED
CARD
WAS
KING
MANY
MAINFRAME
OPERATING
SYSTEMS
BASED
THEIR
FILE
SYSTEMS
ON
FILES
CONSISTING
OF
CHARACTER
RECORDS
IN
EFFECT
CARD
IMAGES
THESE
SYSTEMS
ALSO
SUPPORTED
FILES
OF
FILE
SYSTEMS
CHAP
SEC
FILES
CHARACTER
RECORDS
WHICH
WERE
INTENDED
FOR
THE
LINE
PRINTER
WHICH
IN
THOSE
DAYS
WERE
BIG
CHAIN
PRINTERS
HAVING
COLUMNS
PROGRAMS
READ
INPUT
IN
UNITS
OF
CHARACTERS
AND
WROTE
IT
IN
UNITS
OF
CHARACTERS
ALTHOUGH
THE
FINAL
COULD
BE
SPACES
OF
COURSE
NO
CURRENT
GENERAL
PURPOSE
SYSTEM
USES
THIS
MODEL
AS
ITS
PRIMARY
FILE
SYSTEM
ANY
MORE
BUT
BACK
IN
THE
DAYS
OF
COLUMN
PUNCHED
CARDS
AND
CHARACTER
LINE
PRINTER
PAPER
THIS
WAS
A
COMMON
MODEL
ON
MAINFRAME
COMPUTERS
THE
THIRD
KIND
OF
FILE
STRUCTURE
IS
SHOWN
IN
FIG
C
IN
THIS
ORGANIZATION
A
FILE
CONSISTS
OF
A
TREE
OF
RECORDS
NOT
NECESSARILY
ALL
THE
SAME
LENGTH
EACH
CON
TAINING
A
KEY
FIELD
IN
A
FIXED
POSITION
IN
THE
RECORD
THE
TREE
IS
SORTED
ON
THE
KEY
FIELD
TO
ALLOW
RAPID
SEARCHING
FOR
A
PARTICULAR
KEY
THE
BASIC
OPERATION
HERE
IS
NOT
TO
GET
THE
NEXT
RECORD
ALTHOUGH
THAT
IS
ALSO
POSSIBLE
BUT
TO
GET
THE
RECORD
WITH
A
SPECIFIC
KEY
FOR
THE
ZOO
FILE
OF
FIG
C
ONE
COULD
ASK
THE
SYSTEM
TO
GET
THE
RECORD
WHOSE
KEY
IS
PONY
FOR
EXAMPLE
WITH
FOR
EXAMPLE
IN
FIG
A
WE
SEE
A
SIMPLE
EXECUTABLE
BINARY
FILE
TAKEN
FROM
AN
EARLY
VERSION
OF
UNIX
ALTHOUGH
TECHNICALLY
THE
FILE
IS
JUST
A
SEQUENCE
OF
BYTES
THE
OPERATING
SYSTEM
WILL
ONLY
EXECUTE
A
FILE
IF
IT
HAS
THE
PROPER
FORMAT
IT
HAS
FIVE
SECTIONS
HEADER
TEXT
DATA
RELOCATION
BITS
AND
SYMBOL
TABLE
THE
HEADER
STARTS
WITH
A
SO
CALLED
MAGIC
NUMBER
IDENTIFYING
THE
FILE
AS
AN
EXECUTABLE
FILE
TO
PREVENT
THE
ACCIDENTAL
EXECUTION
OF
A
FILE
NOT
IN
THIS
FORMAT
THEN
COME
THE
SIZES
OF
THE
VARIOUS
PIECES
OF
THE
FILE
THE
ADDRESS
AT
WHICH
EXECUTION
STARTS
AND
SOME
FLAG
BITS
FOLLOWING
THE
HEADER
ARE
THE
TEXT
ARID
DATA
OF
THE
PROGRAM
ITSELF
THESE
ARE
LOADED
INTO
MEMORY
AND
RELOCATED
USING
THE
RELOCATION
BITS
THE
SYMBOL
TABLE
IS
USED
FOR
DEBUGGING
MODUL
E
OUT
WORRYING
ABOUT
ITS
EXACT
POSITION
IN
THE
FILE
FURTHERMORE
NEW
RECORDS
CAN
BE
ADDED
TO
THE
FILE
WITH
THE
OPERATING
SYSTEM
AND
NOT
THE
USER
DECIDING
WHERE
TO
PLACE
THEM
THIS
TYPE
OF
FILE
IS
CLEARLY
QUITE
DIFFERENT
FROM
THE
UNSTRUCTURED
BYTE
STREAMS
USED
IN
UNIX
AND
WINDOWS
BUT
IS
WIDELY
USED
ON
THE
LARGE
MAINFRAME
COMPUTERS
STILL
USED
IN
SOME
COMMERCIAL
DATA
PROCESSING
FILE
TYPES
MANY
OPERATING
SYSTEMS
SUPPORT
SEVERAL
TYPES
OF
FILES
UNIX
AND
WINDOWS
FOR
EXAMPLE
HAVE
REGULAR
FILES
AND
DIRECTORIES
UNIX
ALSO
HAS
CHARACTER
AND
BLOCK
SPECIAL
FILES
REGULAR
FILES
ARE
THE
ONES
THAT
CONTAIN
USER
INFORMATION
ALL
THE
FILES
OF
FIG
ARE
REGULAR
FILES
DIRECTORIES
ARE
SYSTEM
FILES
FOR
MAINTAINING
THE
STRUCTURE
OF
THE
FILE
SYSTEM
WE
WILL
STUDY
DIRECTORIES
BELOW
CHARACTER
SPECIAL
FILES
ARE
RELATED
TO
INPUT
OUTPUT
AND
USED
TO
MODEL
SERIAL
I
O
DEVICES
SUCH
AS
TER
MINALS
PRINTERS
AND
NETWORKS
BLOCK
SPECIAL
FILES
ARE
USED
TO
MODEL
DISKS
IN
THIS
CHAPTER
WE
WILL
BE
PRIMARILY
INTERESTED
IN
REGULAR
FILES
REGULAR
FILES
ARE
GENERALLY
EITHER
ASCII
FILES
OR
BINARY
FILES
ASCII
FILES
CON
SIST
OF
LINES
OF
TEXT
IN
SOME
SYSTEMS
EACH
LINE
IS
TERMINATED
BY
A
CARRIAGE
RETURN
CHARACTER
IN
OTHERS
THE
LINE
FEED
CHARACTER
IS
USED
SOME
SYSTEMS
E
G
MS
DOS
USE
BOTH
LINES
NEED
NOT
ALL
BE
OF
THE
SAME
LENGTH
THE
GREAT
ADVANTAGE
OF
ASCII
FILES
IS
THAT
THEY
CAN
BE
DISPLAYED
AND
PRINTED
AS
IS
AND
THEY
CAN
BE
EDITED
WITH
ANY
TEXT
EDITOR
FURTHERMORE
IF
LARGE
NUMBERS
OF
PROGRAMS
USE
ASCII
FILES
FOR
INPUT
AND
OUTPUT
IT
IS
EASY
TO
CONNECT
THE
OUTPUT
OF
ONE
PROGRAM
TO
THE
INPUT
OF
ANOTHER
AS
IN
SHELL
PIPELINES
THE
INTERPROCESS
PLUMBING
IS
NOT
ANY
EASIER
BUT
INTERPRETING
THE
INFORMATION
CERTAINLY
IS
IF
A
STAN
MAGIC
NUMBER
TEXT
SIZ
E
DAT
A
SIZ
E
BS
SIZ
E
SYMBO
L
TABLE
SIZ
E
ENTRY
POINT
FLAG
TEXT
DAT
A
RELOCATIO
N
BITS
SYMBO
L
TABLE
A
HEADE
R
OBJEC
T
MODUL
E
HEADE
R
OBJEC
T
MODUL
E
HEADE
R
OBJEC
T
MODUL
E
B
NAM
E
DAT
E
OWNER
PROTECTION
SIZE
DARD
CONVENTION
SUCH
AS
ASCII
IS
USED
FOR
EXPRESSING
IT
OTHER
FILES
ARE
BINARY
WHICH
JUST
MEANS
THAT
THEY
ARE
NOT
ASCII
FILES
LISTING
THEM
ON
THE
PRINTER
GIVES
AN
INCOMPREHENSIBLE
LISTING
FULL
OF
RANDOM
JUNK
USUAL
LY
THEY
HAVE
SOME
INTERNAL
STRUCTURE
KNOWN
TO
PROGRAMS
THAT
USE
THEM
FIGURE
A
AN
EXECUTABLE
FILE
B
AN
ARCHIVE
OUR
SECOND
EXAMPLE
OF
A
BINARY
FILE
IS
AN
ARCHIVE
ALSO
FROM
UNIX
IT
CONSISTS
OF
A
COLLECTION
OF
LIBRARY
PROCEDURES
MODULES
COMPILED
BUT
NOT
LINKED
EACH
ONE
IS
PREFACED
BY
A
HEADER
TELLING
ITS
NAME
CREATION
DATE
OWNER
PROTECTION
CODE
AND
FILE
SYSTEMS
CHAP
SIZE
JUST
AS
WITH
THE
EXECUTABLE
FILE
THE
MODULE
HEADERS
ARE
FULL
OF
BINARY
NUM
BERS
COPYING
THEM
TO
THE
PRINTER
WOULD
PRODUCE
COMPLETE
GIBBERISH
EVERY
OPERATING
SYSTEM
MUST
RECOGNIZE
AT
LEAST
ONE
FILE
TYPE
ITS
OWN
EX
ECUTABLE
FILE
BUT
SOME
RECOGNIZE
MORE
THE
OLD
TOPS
SYSTEM
FOR
THE
DECSYSTEM
WENT
SO
FAR
AS
TO
EXAMINE
THE
CREATION
TIME
OF
ANY
FILE
TO
BE
EXE
CUTED
THEN
IT
LOCATED
THE
SOURCE
FILE
AND
SAW
IF
THE
SOURCE
HAD
BEEN
MODIFIED
SINCE
THE
BINARY
WAS
MADE
IF
IT
HAD
BEEN
IT
AUTOMATICALLY
RECOMPILED
THE
SOURCE
IN
UNIX
TERMS
THE
MAKE
PROGRAM
HAD
BEEN
BUILT
INTO
THE
SHELL
THE
FILE
EXTENSIONS
WERE
MANDATORY
SO
THE
OPERATING
SYSTEM
COULD
TELL
WHICH
BINARY
PROGRAM
WAS
DERIVED
FROM
WHICH
SOURCE
HAVING
STRONGLY
TYPED
FILES
LIKE
THIS
CAUSES
PROBLEMS
WHENEVER
THE
USER
DOES
ANYTHING
THAT
THE
SYSTEM
DESIGNERS
DID
NOT
EXPECT
CONSIDER
AS
AN
EXAMPLE
A
SYS
TEM
IN
WHICH
PROGRAM
OUTPUT
FILES
HAVE
EXTENSION
DAT
DATA
FILES
IF
A
USER
WRITES
A
PROGRAM
FORMATTER
THAT
READS
A
C
FILE
C
PROGRAM
TRANSFORMS
IT
E
G
BY
CON
VERTING
IT
TO
A
STANDARD
INDENTATION
LAYOUT
AND
THEN
WRITES
THE
TRANSFORMED
FILE
AS
OUTPUT
THE
OUTPUT
FILE
WILL
BE
OF
TYPE
DAT
IF
THE
USER
TRIES
TO
OFFER
THIS
TO
THE
C
COMPILER
TO
COMPILE
IT
THE
SYSTEM
WILL
REFUSE
BECAUSE
IT
HAS
THE
WRONG
EXTENSION
ATTEMPTS
TO
COPY
FDE
DAT
LOFDE
C
WILL
BE
REJECTED
BY
THE
SYSTEM
AS
INVALID
TO
PRO
TECT
THE
USER
AGAINST
MISTAKES
WHILE
THIS
KIND
OF
USER
FRIENDLINESS
MAY
HELP
NOVICES
IT
DRIVES
EXPERIENCED
USERS
UP
THE
WALL
SINCE
THEY
HAVE
TO
DEVOTE
CONSIDERABLE
EFFORT
TO
CIRCUMVENTING
THE
OPERATING
SYSTEM
IDEA
OF
WHAT
IS
REASONABLE
AND
WHAT
IS
NOT
FILE
ACCESS
EARLY
OPERATING
SYSTEMS
PROVIDED
ONLY
ONE
KIND
OF
FILE
ACCESS
SEQUENTIAL
ACCESS
IN
THESE
SYSTEMS
A
PROCESS
COULD
READ
ALL
THE
BYTES
OR
RECORDS
IN
A
FILE
IN
ORDER
STARTING
AT
THE
BEGINNING
BUT
COULD
NOT
SKIP
AROUND
AND
READ
THEM
OUF
OF
ORDER
SEQUENTIAL
FILES
COULD
BE
REWOUND
HOWEVER
SO
THEY
COULD
BE
READ
AS
OFTEN
AS
NEEDED
SEQUENTIAL
FILES
WERE
CONVENIENT
WHEN
THE
STORAGE
MEDIUM
WAS
MAG
NETIC
TAPE
RATHER
THAN
DISK
WHEN
DISKS
CAME
INTO
USE
FOR
STORING
FILES
IT
BECAME
POSSIBLE
TO
READ
THE
BYTES
OR
RECORDS
OF
A
FILE
OUT
OF
ORDER
OR
TO
ACCESS
RECORDS
BY
KEY
RATHER
THAN
BY
POSITION
FILES
WHOSE
BYTES
OR
RECORDS
CAN
BE
READ
IN
ANY
ORDER
ARE
CALLED
RANDOM
ACCESS
FILES
THEY
ARE
REQUIRED
BY
MANY
APPLICATIONS
RANDOM
ACCESS
FILES
ARE
ESSENTIAL
FOR
MANY
APPLICATIONS
FOR
EXAMPLE
DATA
BASE
SYSTEMS
IF
AN
AIRLINE
CUSTOMER
CALLS
UP
AND
WANTS
TO
RESERVE
A
SEAT
ON
A
PAR
TICULAR
FLIGHT
THE
RESERVATION
PROGRAM
MUST
BE
ABLE
TO
ACCESS
THE
RECORD
FOR
THAT
FLIGHT
WITHOUT
HAVING
TO
READ
THE
RECORDS
FOR
THOUSANDS
OF
OTHER
FLIGHTS
FIRST
TWO
METHODS
CAN
BE
USED
FOR
SPECIFYING
WHERE
TO
START
READING
IN
THE
FIRST
ONE
EVERY
READ
OPERATION
GIVES
THE
POSITION
IN
THE
FILE
TO
START
READING
AT
IN
THE
SECOND
ONE
A
SPECIAL
OPERATION
SEEK
IS
PROVIDED
TO
SET
THE
CURRENT
POSITION
AFTER
A
SEEK
THE
FILE
CAN
BE
READ
SEQUENTIALLY
FROM
THE
NOW
CURRENT
POSITION
THE
LATTER
METHOD
IS
USED
IN
UNIX
AND
WINDOWS
SEC
PILES
FILE
ATTRIBUTES
EVERY
FILE
HAS
A
NAME
AND
ITS
DATA
IN
ADDITION
ALL
OPERATING
SYSTEMS
ASSOCIATE
OTHER
INFORMATION
WITH
EACH
FILE
FOR
EXAMPLE
THE
DATE
AND
TIME
THE
FILE
WAS
LAST
MODIFIED
AND
THE
FILE
SIZE
WE
WILL
CALL
THESE
EXTRA
ITEMS
THE
FILE
ATTRIBUTES
SOME
PEOPLE
CALL
THEM
METADATA
THE
LIST
OF
ATTRIBUTES
VARIES
CONSIDERABLY
FROM
SYSTEM
TO
SYSTEM
THE
TABLE
OF
FIG
SHOWS
SOME
OF
THE
POSSIBILITIES
BUT
OTHER
ONES
ALSO
EXIST
NO
EXISTING
SYSTEM
HAS
ALL
OF
THESE
BUT
EACH
ONE
IS
PRESENT
IN
SOME
SYSTEM
ATTRIBUTE
MEANING
PROTECTION
WHO
CAN
ACCESS
THE
FILE
AND
IN
WHAT
WAV
PASSWORD
PASSWORD
NEEDED
TO
ACCESS
THE
FILE
CREATOR
ID
OF
THE
PERSON
WHO
CREATED
THE
FILE
OWNER
CURRENT
OWNER
READ
ONLY
FLAG
FOR
READ
WRITE
FOR
READ
ONLY
HIDDEN
FLAG
FOR
NORMAL
FOR
DO
NOT
DISPLAY
IN
LISTINGS
SYSTEM
FLAG
FOR
NORMAL
FILES
FOR
SYSTEM
FILE
ARCHIVE
FLAG
FOR
HAS
BEEN
BACKED
UP
FOR
NEEDS
TO
BE
BACKED
UP
ASCII
BINARY
FLAG
FOR
ASCII
FILE
FOR
BINARY
FILE
RANDOM
ACCESS
FLAG
FOR
SEQUENTIAL
ACCESS
ONLY
FOR
RANDOM
ACCESS
TEMPORARY
FLAG
FOR
NORMAL
FOR
DELETE
FILE
ON
PROCESS
EXIT
LOCK
FLAGS
FOR
UNLOCKED
NONZERO
FOR
LOCKED
RECORD
LENGTH
NUMBER
OF
BYTES
IN
A
RECORD
KEY
POSITION
OFFSET
OF
THE
KEY
WITHIN
EACH
RECORD
KEY
LENGTH
NUMBER
OF
BYTES
IN
THE
KEY
FIELD
CREATION
TIME
DATE
AND
TIME
THE
FILE
WAS
CREATED
TIME
OF
LAST
ACCESS
DATE
AND
TIME
THE
FILE
WAS
LAST
ACCESSED
TIME
OF
LAST
CHANQE
DATE
AND
TIME
THE
FILE
WAS
LAST
CHANQED
CURRENT
SIZE
NUMBER
OF
BYTES
IN
THE
FILE
MAXIMUM
SIZE
NUMBER
OF
BYTES
THE
FILE
MAY
GROW
TO
FIGURE
SOME
POSSIBLE
FILE
ATTRIBUTES
THE
FIRST
FOUR
ATTRIBUTES
RELATE
TO
THE
FILE
PROTECTION
AND
TELL
WHO
MAY
ACCESS
IT
AND
WHO
MAY
NOT
ALL
KINDS
OF
SCHEMES
ARE
POSSIBLE
SOME
OF
WHICH
WE
WILL
STUDY
LATER
IN
SOME
SYSTEMS
THE
USER
MUST
PRESENT
A
PASSWORD
TO
ACCESS
A
FILE
IN
WMCN
CASE
THE
PASSWORD
MUST
BE
ONE
OF
THE
ATTRIBUTES
THE
FLAGS
ARE
BITS
OR
SHORT
FIELDS
THAT
CONTROL
OR
ENABLE
SOME
SPECIFIC
PROPERTY
HIDDEN
FILES
FOR
EXAMPLE
DO
NOT
APPEAR
IN
LISTINGS
OF
ALL
THE
FILES
THE
ARCHIVE
FILE
SYSTEMS
CHAP
FLAG
IS
A
BIT
THAT
KEEPS
TRACK
OF
WHETHER
THE
FILE
HAS
BEEN
BACKED
UP
RECENTLY
THE
BACKUP
PROGRAM
CLEARS
IT
AND
THE
OPERATING
SYSTEM
SETS
IT
WHENEVER
A
FILE
IS
CHANGED
IN
THIS
WAY
THE
BACKUP
PROGRAM
CAN
TELL
WHICH
FILES
NEED
BACKING
UP
THE
TEMPORARY
FLAG
ALLOWS
A
FILE
TO
BE
MARKED
FOR
AUTOMATIC
DELETION
WHEN
THE
PROCESS
THAT
CREATED
IT
TERMINATES
THE
RECORD
LENGTH
KEY
POSITION
AND
KEY
LENGTH
FIELDS
ARE
ONLY
PRESENT
IN
FILES
WHOSE
RECORDS
CAN
BE
LOOKED
UP
USING
A
KEY
THEY
PROVIDE
THE
INFORMATION
RE
QUIRED
TO
FIND
THE
KEYS
THE
VARIOUS
TIMES
KEEP
TRACK
OF
WHEN
THE
FILE
WAS
CREATED
MOST
RECENTLY
AC
CESSED
AND
MOST
RECENTLY
MODIFIED
THESE
ARE
USEFUL
FOR
A
VARIETY
OF
PURPOSES
FOR
EXAMPLE
A
SOURCE
FILE
THAT
HAS
BEEN
MODIFIED
AFTER
THE
CREATION
OF
THE
CORRES
PONDING
OBJECT
FILE
NEEDS
TO
BE
RECOMPILED
THESE
FIELDS
PROVIDE
THE
NECESSARY
INFORMATION
THE
CURRENT
SIZE
TELLS
HOW
BIG
THE
FILE
IS
AT
PRESENT
SOME
OLD
MAINFRAME
OPER
ATING
SYSTEMS
REQUIRE
THE
MAXIMUM
SIZE
TO
BE
SPECIFIED
WHEN
THE
FILE
IS
CREATED
IN
ORDER
TO
LET
THE
OPERATING
SYSTEM
RESERVE
THE
MAXIMUM
AMOUNT
OF
STORAGE
IN
AD
VANCE
WORKSTATION
AND
PERSONAL
COMPUTER
OPERATING
SYSTEMS
ARE
CLEVER
ENOUGH
TO
DO
WITHOUT
THIS
FEATURE
FILE
OPERATIONS
FILES
EXIST
TO
STORE
INFORMATION
AND
ALLOW
IT
TO
BE
RETRIEVED
LATER
DIFFERENT
SYS
TEMS
PROVIDE
DIFFERENT
OPERATIONS
TO
ALLOW
STORAGE
AND
RETRIEVAL
BELOW
IS
A
DIS
CUSSION
OF
THE
MOST
COMMON
SYSTEM
CALLS
RELATING
TO
FILES
CREATE
THE
FILE
IS
CREATED
WITH
NO
DATA
THE
PURPOSE
OF
THE
CALL
IS
TO
ANNOUNCE
THAT
THE
FILE
IS
COMING
AND
TO
SET
SOME
OF
THE
ATTRIBUTES
DELETE
WHEN
THE
FILE
IS
NO
LONGER
NEEDED
IT
HAS
TO
BE
DELETED
TO
FREE
UP
DISK
SPACE
THERE
IS
ALWAYS
A
SYSTEM
CALL
FOR
THIS
PURPOSE
OPEN
BEFORE
USING
A
FILE
A
PROCESS
MUST
OPEN
IT
THE
PURPOSE
OF
THE
OPEN
CALL
IS
TO
ALLOW
THE
SYSTEM
TO
FETCH
THE
ATTRIBUTES
AND
LIST
OF
DISK
ADDRESSES
INTO
MAIN
MEMORY
FOR
RAPID
ACCESS
ON
LATER
CALLS
CLOSE
WHEN
ALL
THE
ACCESSES
ARE
FINISHED
THE
ATTRIBUTES
AND
DISK
AD
DRESSES
ARE
NO
LONGER
NEEDED
SO
THE
FILE
SHOULD
BE
CLOSED
TO
FREE
UP
INTERNAL
TABLE
SPACE
MANY
SYSTEMS
ENCOURAGE
THIS
BY
IMPOSING
A
MAXIMUM
NUMBER
OF
OPEN
FILES
ON
PROCESSES
A
DISK
IS
WRITTEN
IN
BLOCKS
AND
CLOSING
A
FILE
FORCES
WRITING
OF
THE
FILE
LAST
BLOCK
EVEN
THOUGH
THAT
BLOCK
MAY
NOT
BE
ENTIRELY
FULL
YET
READ
DATA
ARE
READ
FROM
FILE
USUALLY
THE
BYTES
COME
FROM
THE
CUR
RENT
POSITION
THE
CALLER
MUST
SPECIFY
HOW
MANY
DATA
ARE
NEEDED
AND
MUST
ALSO
PROVIDE
A
BUFFER
TO
PUT
THEM
IN
SEC
FILES
WRITE
DATA
ARE
WRITTEN
TO
THE
FILE
AGAIN
USUALLY
AT
THE
CURRENT
POSI
TION
IF
THE
CURRENT
POSITION
IS
THE
END
OF
THE
FILE
THE
FILE
SIZE
INCREASES
IF
THE
CURRENT
POSITION
IS
IN
THE
MIDDLE
OF
THE
FILE
EXISTING
DATA
ARE
OVERWRITTEN
AND
LOST
FOREVER
APPEND
THIS
CALL
IS
A
RESTRICTED
FORM
OF
WRITE
IT
CAN
ONLY
ADD
DATA
TO
THE
END
OF
THE
FILE
SYSTEMS
THAT
PROVIDE
A
MINIMAL
SET
OF
SYSTEM
CALLS
DO
NOT
GENERALLY
HAVE
APPEND
BUT
MANY
SYSTEMS
PROVIDE
MULTI
PLE
WAYS
OF
DOING
THE
SAME
THING
AND
THESE
SYSTEMS
SOMETIMES
HAVE
APPEND
SEEK
FOR
RANDOM
ACCESS
FILES
A
METHOD
IS
NEEDED
TO
SPECIFY
FROM
WHERE
TO
TAKE
THE
DATA
ONE
COMMON
APPROACH
IS
A
SYSTEM
CALL
SEEK
THAT
REPOSITIONS
THE
FILE
POINTER
TO
A
SPECIFIC
PLACE
IN
THE
FILE
AFTER
THIS
CALL
HAS
COMPLETED
DATA
CAN
BE
READ
FROM
OR
WRITTEN
TO
THAT
POSITION
GET
ATTRIBUTES
PROCESSES
OFTEN
NEED
TO
READ
FILE
ATTRIBUTES
TO
DO
THEIR
WORK
FOR
EXAMPLE
THE
UNIX
MAKE
PROGRAM
IS
COMMONLY
USED
TO
MANAGE
SOFTWARE
DEVELOPMENT
PROJECTS
CONSISTING
OF
MANY
SOURCE
FILES
WHEN
MAKE
IS
CALLED
IT
EXAMINES
THE
MODIFICATION
TIMES
OF
ALL
THE
SOURCE
AND
OBJECT
FILES
AND
ARRANGES
FOR
THE
MINIMUM
NUMBER
OF
COMPILATIONS
REQUIRED
TO
BRING
EVERYTHING
UP
TO
DATE
TO
DO
ITS
JOB
IT
MUST
LOOK
AT
THE
ATTRIBUTES
NAMELY
THE
MODIFICATION
TIMES
SET
ATTRIBUTES
SOME
OF
THE
ATTRIBUTES
ARE
USER
SETTABLE
AND
CAN
BE
CHANGED
AFTER
THE
FILE
HAS
BEEN
CREATED
THIS
SYSTEM
CALL
MAKES
THAT
POSSIBLE
THE
PROTECTION
MODE
INFORMATION
IS
AN
OBVIOUS
EXAMPLE
MOST
OF
THE
FLAGS
ALSO
FALL
IN
THIS
CATEGORY
RENAME
IT
FREQUENTLY
HAPPENS
THAT
A
USER
NEEDS
TO
CHANGE
THE
NAME
OF
AN
EXISTING
FILE
THIS
SYSTEM
CALL
MAKES
THAT
POSSIBLE
IT
IS
NOT
AL
WAYS
STRICTLY
NECESSARY
BECAUSE
THE
FILE
CAN
USUALLY
BE
COPIED
TO
A
NEW
FILE
WITH
THE
NEW
NAME
AND
THE
OLD
FILE
THEN
DELETED
AN
EXAMPLE
PROGRAM
USING
FILE
SYSTEM
CALLS
IN
THIS
SECTION
WE
WILL
EXAMINE
A
SIMPLE
UNIX
PROGRAM
THAT
COPIES
ONE
FILE
FROM
ITS
SOURCE
FILE
TO
A
DESTINATION
FILE
IT
IS
LISTED
IN
FIG
THE
PROGRAM
HAS
MINIMAL
FUNCTIONALITY
AND
EVEN
WORSE
ERROR
REPORTING
BUT
IT
GIVES
A
REASONABLE
IDEA
OF
HOW
SOME
OF
THE
SYSTEM
CALLS
RELATED
TO
FILES
WORK
THE
PROGRAM
COPYFDE
CAN
BE
CALLED
FOR
EXAMPLE
BY
THE
COMMAND
LINE
COPYFILE
ABC
XYZ
TO
COPY
THE
FILE
ABC
TO
XYZ
IF
XYZ
ALREADY
EXISTS
IT
WILL
BE
OVERWRITTEN
OTHERWISE
FILE
SYSTEMS
CHAP
FILE
COPY
PROGRAM
ERROR
CHECKING
AND
REPORTING
IS
MINIMAL
INCLUDE
TY
ES
B
N
C
L
U
D
E
N
E
C
E
A
R
Y
H
E
A
D
E
R
INCLUDE
FCNTL
H
FFINCLUDE
STDLIB
H
INCLUDE
UNISTD
H
SEC
FILES
BUT
WILL
NOT
CONCERN
US
FURTHER
THE
NEXT
LINE
IS
A
FUNCTION
PROTOTYPE
FOR
MAIN
SOMETHING
REQUIRED
BY
ANSI
C
BUT
ALSO
NOT
IMPORTANT
FOR
OUR
PURPOSES
THE
FIRST
TFDEFINE
STATEMENT
IS
A
MACRO
DEFINITION
THAT
DEFINES
THE
CHARACTER
STRING
BUFSIZE
AS
A
MACRO
THAT
EXPANDS
INTO
THE
NUMBER
THE
PROGRAM
WILL
READ
AND
WRITE
IN
CHUNKS
OF
BYTES
IT
IS
CONSIDERED
GOOD
PROGRAMMING
PRACTICE
TO
GIVE
NAMES
TO
CONSTANTS
LIKE
THIS
AND
TO
USE
THE
NAMES
INSTEAD
OF
THE
CONSTANTS
NOT
ONLY
DOES
THIS
CONVENTION
MAKE
PROGRAMS
EASIER
TO
READ
BUT
IT
ALSO
INT
MAIN
INT
ARGC
CHAR
ARGVQ
DEFINE
DEFINE
INT
MAIN
INT
ARGC
CHAR
ARGV
INT
CHAR
BUFFER
IF
ARGC
EXIT
ANSI
PROTOTYPE
US
E
A
BUFFER
SIZE
OF
BYTES
PROTECTION
BITS
FOR
OUTPUT
FILE
SYNTAX
ERROR
IF
ARGC
IS
NOT
MAKES
THEM
EASIER
TO
MAINTAIN
THE
SECOND
DEFINE
STATEMENT
DETERMINES
WHO
CAN
ACCESS
THE
OUTPUT
FILE
THE
MAIN
PROGRAM
IS
CALLED
MAIN
AND
IT
HAS
TWO
ARGUMENTS
ARGC
AND
ARGV
THESE
ARE
SUPPLIED
BY
THE
OPERATING
SYSTEM
WHEN
THE
PROGRAM
IS
CALLED
THE
FIRST
ONE
TELLS
HOW
MANY
STRINGS
WERE
PRESENT
ON
THE
COMMAND
LINE
THAT
INVOKED
THE
PROGRAM
INCLUDING
THE
PROGRAM
NAME
IT
SHOULD
BE
THE
SECOND
ONE
IS
AN
ARRAY
OF
POINTERS
TO
THE
ARGUMENTS
IN
THE
EXAMPLE
CALL
GIVEN
ABOVE
THE
ELEMENTS
OF
THIS
ARRAY
WOULD
CONTAIN
POINTERS
TO
THE
FOLLOWING
VALUES
ARGV
COPYFILE
OPEN
THE
INPUT
FILE
AND
CREATE
THE
OUTPUT
FILE
OPEN
ARGV
OPEN
THE
SOURCE
FILE
IF
EXIT
IF
IT
CANNOT
BE
OPENED
EXIT
OUT
JD
CREAT
ARGV
CREATE
THE
DESTINATION
FILE
L
IF
EXIT
IF
IT
CANNOT
BE
CREATED
EXIT
COPY
LOOP
WHILE
TRUE
RD
COUNT
READ
BUFFER
READ
A
BLOCK
OF
DATA
IF
BREAK
IF
END
OF
FILE
OR
ERROR
EXIT
LOOP
WRITE
BUFFER
WRITE
DATA
IF
EXIT
IS
AN
ERROR
CLOSE
THE
FILES
CLOSE
CLOSE
OUT
FD
IF
NO
ERROR
ON
LAST
READ
EXIT
ELSE
EXIT
ERROR
ON
LAST
READ
FIGURE
A
SIMPLE
PROGRAM
TO
COPY
A
FILE
IT
WILL
BE
CREATED
THE
PROGRAM
MUST
BE
CALLED
WITH
EXACTLY
TWO
ARGUMENTS
BOTH
LEGAL
FILE
NAMES
THE
FIRST
IS
THE
SOURCE
THE
SECOND
IS
THE
OUTPUT
FILE
THE
FOUR
INCLUDE
STATEMENTS
NEAR
THE
TOP
OF
THE
PROGRAM
CAUSE
A
LARGE
NUM
BER
OF
DEFINITIONS
AND
FUNCTION
PROTOTYPES
TO
BE
INCLUDED
IN
THE
PROGRAM
THESE
ARE
NEEDED
TO
MAKE
THE
PROGRAM
CONFORMANT
TO
THE
RELEVANT
INTERNATIONAL
STANDARDS
ARGV
L
ABC
ARGV
XYZ
IT
IS
VIA
THIS
ARRAY
THAT
THE
PROGRAM
ACCESSES
ITS
ARGUMENTS
FIVE
VARIABLES
ARE
DECLARED
THE
FIRST
TWO
AND
OUT
FD
WILL
HOLD
THE
FILE
DESCRIPTORS
SMALL
INTEGERS
RETURNED
WHEN
A
FILE
IS
OPENED
THE
NEXT
TWO
RD
COUNT
AND
WT
COUNT
ARE
THE
BYTE
COUNTS
RETURNED
BY
THE
READ
AND
WRITE
SYSTEM
CALLS
RESPECTIVELY
THE
LAST
ONE
BUFFER
IS
THE
BUFFER
USED
TO
HOLD
THE
DATA
READ
AND
SUPPLY
THE
DATA
TO
BE
WRITTEN
THE
FIRST
ACTUAL
STATEMENT
CHECKS
ARGC
TO
SEE
IF
IT
IS
IF
NOT
IT
EXITS
WITH
STAT
US
CODE
ANY
STATUS
CODE
OTHER
THAN
MEANS
THAT
AN
ERROR
HAS
OCCURRED
THE
STATUS
CODE
IS
THE
ONLY
ERROR
REPORTING
PRESENT
IN
THIS
PROGRAM
A
PRODUCTION
VER
SION
WOULD
NORMALLY
PRINT
ERROR
MESSAGES
AS
WELL
THEN
WE
TRY
TO
OPEN
THE
SOURCE
FILE
AND
CREATE
THE
DESTINATION
FILE
IF
THE
SOURCE
FILE
IS
SUCCESSFULLY
OPENED
THE
SYSTEM
ASSIGNS
A
SMALL
INTEGER
TO
IN
FD
TO
IDENTIFY
THE
FILE
SUBSEQUENT
CALLS
MUST
INCLUDE
THIS
INTEGER
SO
THAT
THE
SYSTEM
KNOWS
WHICH
FILE
IT
WANTS
SIMILARLY
IF
THE
DESTINATION
IS
SUCCESSFULLY
CREATED
OUT
FD
IS
GIVEN
A
VALUE
TO
IDENTIFY
IT
THE
SECOND
ARGUMENT
TO
CREAT
SETS
THE
PRO
TECTION
MODE
IF
EITHER
THE
OPEN
OR
THE
CREATE
FAILS
THE
CORRESPONDING
FILE
DESCRIP
TOR
IS
SET
TO
AND
THE
PROGRAM
EXITS
WITH
AN
ERROR
CODE
NOW
COMES
THE
COPY
LOOP
IT
STARTS
BY
TRYING
TO
READ
IN
KB
OF
DATA
TO
BUFFER
IT
DOES
THIS
BY
CALLING
THE
LIBRARY
PROCEDURE
READ
WHICH
ACTUALLY
INVOKES
THE
READ
SYSTEM
CALL
THE
FIRST
PARAMETER
IDENTIFIES
THE
FILE
THE
SECOND
GIVES
THE
BUFFER
AND
THE
THIRD
TELLS
HOW
MANY
BYTES
TO
READ
THE
VALUE
ASSIGNED
TO
RD
COUNT
GIVES
THE
NUMBER
OF
BYTES
ACTUALLY
READ
NORMALLY
THIS
WILL
BE
EXCEPT
IF
FEWER
BYTES
ARE
REMAINING
IN
THE
FILE
WHEN
THE
END
OF
FILE
IHAS
BEEN
REACHED
IT
WILL
BE
IF
FILE
SYSTEMS
CHAP
RD
COUNT
IS
EVER
ZERO
OR
NEGATIVE
THE
COPYING
CANNOT
CONTINUE
SO
THE
BREAK
STATE
MENT
IS
EXECUTED
TO
TERMINATE
THE
OTHERWISE
ENDLESS
LOOP
THE
CALL
TO
WRITE
OUTPUTS
THE
BUFFER
TO
THE
DESTINATION
FILE
THE
FIRST
PARAMETER
IDENTIFIES
THE
FILE
THE
SECOND
GIVES
THE
BUFFER
AND
THE
THIRD
TELLS
HOW
MANY
BYTES
TO
WRITE
ANALOGOUS
TO
READ
NOTE
THAT
THE
BYTE
COUNT
IS
THE
NUMBER
OF
BYTES
AC
TUALLY
READ
NOT
BUF
SIZE
THIS
POINT
IS
IMPORTANT
BECAUSE
THE
LAST
READ
WILL
NOT
RETURN
UNLESS
THE
FILE
JUST
HAPPENS
TO
BE
A
MULTIPLE
OF
KB
WHEN
THE
ENTIRE
FILE
HAS
BEEN
PROCESSED
THE
FIRST
CALL
BEYOND
THE
END
OF
FILE
WILL
RETURN
TO
WHICH
WILL
MAKE
IT
EXIT
THE
LOOP
AT
THIS
POINT
THE
TWO
FILES
ARE
CLOSED
AND
THE
PROGRAM
EXITS
WITH
A
STATUS
INDICATING
NORMAL
TERMINATION
ALTHOUGH
THE
WINDOWS
SYSTEM
CALLS
ARE
DIFFERENT
FROM
THOSE
OF
UNIX
THE
GENERAL
STRUCTURE
OF
A
COMMAND
LINE
WINDOWS
PROGRAM
TO
COPY
A
FILE
IS
MODERATE
LY
SIMILAR
TO
THAT
OF
FIG
WE
WILL
EXAMINE
THE
WINDOWS
VISTA
CALLS
IN
CHAP
DIRECTORIE
TO
KEEP
TRACK
OF
FILES
FILE
SYSTEMS
NORMALLY
HAVE
DIRECTORIES
OR
FOLDERS
WHICH
IN
MANY
SYSTEMS
ARE
THEMSELVES
FILES
IN
THIS
SECTION
WE
WILL
DISCUSS
DIREC
TORIES
THEIR
ORGANIZATION
THEIR
PROPERTIES
AND
THE
OPERATIONS
THAT
CAN
BE
PERFORM
ED
ON
THEM
SINGLE
LEVEL
DIRECTORY
SYSTEMS
THE
SIMPLEST
FORM
OF
DIRECTORY
SYSTEM
IS
HAVING
ONE
DIRECTORY
CONTAINING
ALL
THE
FILES
SOMETIMES
IT
IS
CALLED
THE
ROOT
DIRECTORY
BUT
SINCE
IT
IS
THE
ONLY
ONE
THE
NAME
DOES
NOT
MATTER
MUCH
ON
EARLY
PERSONAL
COMPUTERS
THIS
SYSTEM
WAS
COM
MON
IN
PART
BECAUSE
THERE
WAS
ONLY
ONE
USER
INTERESTINGLY
ENOUGH
THE
WORLD
FIRST
SUPERCOMPUTER
THE
CDC
ALSO
HAD
ONLY
A
SINGLE
DIRECTORY
FOR
ALL
FILES
EVEN
THOUGH
IT
WAS
USED
BY
MANY
USERS
AT
ONCE
THIS
DECISION
WAS
NO
DOUBT
MADE
TO
KEEP
THE
SOFTWARE
DESIGN
SIMPLE
AN
EXAMPLE
OF
A
SYSTEM
WITH
ONE
DIRECTORY
IS
GIVEN
IN
FIG
HERE
THE
DI
RECTORY
CONTAINS
FOUR
FILES
THE
ADVANTAGES
OF
THIS
SCHEME
ARE
ITS
SIMPLICITY
AND
THE
ABILITY
TO
LOCATE
FILES
QUICKLY
THERE
IS
ONLY
ONE
PLACE
TO
LOOK
AFTER
ALL
IT
IS
OFTEN
USED
ON
SIMPLE
EMBEDDED
DEVICES
SUCH
AS
TELEPHONES
DIGITAL
CAMERAS
AND
SOME
PORTABLE
MUSIC
PLAYERS
HIERARCHICAL
DIRECTORY
SYSTEMS
THE
SINGLE
LEVEL
IS
ADEQUATE
FOR
SIMPLE
DEDICATED
APPLICATIONS
AND
WAS
EVEN
USED
ON
THE
FIRST
PERSONAL
COMPUTERS
BUT
FOR
MODERN
USERS
WITH
THOUSANDS
OF
FILES
IT
WOULD
BE
IMPOSSIBLE
TO
FIND
ANYTHING
IF
ALL
FILES
WERE
IN
A
SINGLE
DIRECTORY
SEC
DIRECTORIES
FIGURE
A
SINGLE
LEVEL
DIRECTORY
SYSTEM
CONTAINING
FOUR
FILES
CONSEQUENTLY
A
WAY
IS
NEEDED
TO
GROUP
RELATED
FILES
TOGETHER
A
PROFESSOR
FOR
EXAMPLE
MIGHT
HAVE
A
COLLECTION
OF
FILES
THAT
TOGETHER
FORM
A
BOOK
THAT
HE
IS
WRIT
ING
FOR
ONE
COURSE
A
SECOND
COLLECTION
OF
FILES
CONTAINING
STUDENT
PROGRAMS
SUB
MITTED
FOR
ANOTHER
COURSE
A
THIRD
GROUP
OF
FILES
CONTAINING
THE
CODE
OF
AN
AD
VANCED
COMPILER
WRITING
SYSTEM
HE
IS
BUILDING
A
FOURTH
GROUP
OF
FILES
CONTAINING
GRANT
PROPOSALS
AS
WELL
AS
OTHER
FILES
FOR
ELECTRONIC
MAIL
MINUTES
OF
MEETINGS
PAPERS
HE
IS
WRITING
GAMES
AND
SO
ON
WHAT
IS
NEEDED
IS
A
HIERARCHY
I
E
A
TREE
OF
DIRECTORIES
WITH
THIS
APPROACH
THERE
CAN
BE
AS
MANY
DIRECTORIES
AS
ARE
NEEDED
TO
GROUP
THE
FILES
IN
NATURAL
WAYS
FURTHERMORE
IF
MULTIPLE
USERS
SHARE
A
COMMON
FILE
SERVER
AS
IS
THE
CASE
ON
MANY
COMPANY
NETWORKS
EACH
USER
CAN
HAVE
A
PRIVATE
ROOT
DIRECTORY
FOR
HIS
OR
HER
OWN
HIERARCHY
THIS
APPROACH
IS
SHOWN
IN
FIG
HERE
THE
DIRECTORIES
A
B
AND
C
CONTAINED
IN
THE
ROOT
DIRECTORY
EACH
BELONG
TO
A
DIFFERENT
USER
TWO
OF
WHOM
HAVE
CREATED
SUBDIRECTORIES
FOR
PROJECTS
THEY
ARE
WORKING
ON
DI
FIGURE
A
HIERARCHICAL
DIRECTORY
SYSTEM
THE
ABILITY
FOR
USERS
TO
CREATE
AN
ARBITRARY
NUMBER
OF
SUBDIRECTORIES
PROVIDES
A
POWERFUL
STRUCTURING
TOOL
FOR
USERS
TO
ORGANIZE
THEIR
WORK
FOR
THIS
REASON
NEARLY
ALL
MODERN
FILE
SYSTEMS
ARE
ORGANIZED
IN
THIS
MANNER
PATH
NAMES
WHEN
THE
FILE
SYSTEM
IS
ORGANIZED
AS
A
DIRECTORY
TREE
SOME
WAY
IS
NEEDED
FOR
SPECIFYING
FILE
NAMES
TWO
DIFFERENT
METHODS
ARE
COMMONLY
USED
IN
THE
FIRST
METHOD
EACH
FILE
IS
GIVEN
AN
ABSOLUTE
PATH
NAME
CONSISTING
OF
THE
PATH
FROM
THE
FILE
SYSTEMS
CHAP
ROOT
DIRECTORY
TO
THE
FILE
AS
AN
EXAMPLE
THE
PATH
USR
AST
MAILBOX
MEANS
THAT
THE
ROOT
DIRECTORY
CONTAINS
A
SUBDIRECTORY
USR
WHICH
IN
TURN
CONTAINS
A
SUBDIRECTORY
AST
WHICH
CONTAINS
THE
FILE
MAILBOX
ABSOLUTE
PATH
NAMES
ALWAYS
START
AT
THE
ROOT
DIRECTORY
AND
ARE
UNIQUE
IN
UNIX
THE
COMPONENTS
OF
THE
PATH
ARE
SEPARATED
BY
IN
WINDOWS
THE
SEPARATOR
IS
IN
MULTICS
IT
WAS
THUS
THE
SAME
PATH
NAME
WOULD
BE
WRITTEN
AS
FOLLOWS
IN
THESE
THREE
SYSTEMS
WINDOWS
USR
AST
MAIIBOX
UNIX
USR
AST
MAIIBOX
MULTICS
USR
AST
MAILBOX
NO
MATTER
WHICH
CHARACTER
IS
USED
IF
THE
FIRST
CHARACTER
OF
THE
PATH
NAME
IS
THE
SEPARATOR
THEN
THE
PATH
IS
ABSOLUTE
THE
OTHER
KIND
OF
NAME
IS
THE
RELATIVE
PATH
NAME
THIS
IS
USED
IN
CONJUNC
TION
WITH
THE
CONCEPT
OF
THE
WORKING
DIRECTORY
ALSO
CALLED
THE
CURRENT
DIREC
TORY
A
USER
CAN
DESIGNATE
ONE
DIRECTORY
AS
THE
CURRENT
WORKING
DIRECTORY
IN
WHICH
CASE
ALL
PATH
NAMES
NOT
BEGINNING
AT
THE
ROOT
DIRECTORY
ARE
TAKEN
RELATIVE
TO
THE
WORKING
DIRECTORY
FOR
EXAMPLE
IF
THE
CURRENT
WORKING
DIRECTORY
IS
USR
AST
THEN
THE
FILE
WHOSE
ABSOLUTE
PATH
IS
USR
AST
MAILBOX
CAN
BE
REFERENCED
SIMPLY
AS
MAILBOX
IN
OTHER
WORDS
THE
UNIX
COMMAND
CP
USR
AST
MAILBOX
USR
AST
MAILBOX
BAK
AND
THE
COMMAND
CP
MAILBOX
MAILBOX
BAK
DO
EXACTLY
THE
SAME
THING
IF
THE
WORKING
DIRECTORY
IS
USR
AST
THE
RELATIVE
FORM
IS
OFTEN
MORE
CONVENIENT
BUT
IT
DOES
THE
SAME
THING
AS
THE
ABSOLUTE
FORM
SOME
PROGRAMS
NEED
TO
ACCESS
A
SPECIFIC
FILE
WITHOUT
REGARD
TO
WHAT
THE
WORK
ING
DIRECTORY
IS
IN
THAT
CASE
THEY
SHOULD
ALWAYS
USE
ABSOLUTE
PATH
NAMES
FOR
EXAMPLE
A
SPELLING
CHECKER
MIGHT
NEED
TO
READ
USR
LIB
DICTIONARY
TO
DO
ITS
WORK
IT
SHOULD
USE
THE
FULL
ABSOLUTE
PATH
NAME
IN
THIS
CASE
BECAUSE
IT
DOES
NOT
KNOW
WHAT
THE
WORKING
DIRECTORY
WILL
BE
WHEN
IT
IS
CALLED
THE
ABSOLUTE
PATH
NAME
WILL
ALWAYS
WORK
NO
MATTER
WHAT
THE
WORKING
DIRECTORY
IS
OF
COURSE
IF
THE
SPELLING
CHECKER
NEEDS
A
LARGE
NUMBER
OF
FILES
FROM
USR
LIB
AN
ALTERNATIVE
APPROACH
IS
FOR
IT
TO
ISSUE
A
SYSTEM
CALL
TO
CHANGE
ITS
WORKING
DIREC
TORY
TO
USR
LIB
AND
THEN
USE
JUST
DICTIONARY
AS
THE
FIRST
PARAMETER
TO
OPEN
BY
EXPLICITIY
CHANGING
THE
WORKING
DIRECTORY
IT
KNOWS
FOR
SURE
WHERE
IT
IS
IN
THE
DI
RECTORY
TREE
SO
IT
CAN
THEN
USE
RELATIVE
PATHS
EACH
PROCESS
HAS
ITS
OWN
WORKING
DIRECTORY
SO
WHEN
IT
CHANGES
ITS
WORKING
DIRECTORY
AND
LATER
EXITS
NO
OTHER
PROCESSES
ARE
AFFECTED
AND
NO
TRACES
OF
THE
CHANGE
ARE
LEFT
BEHIND
IN
THE
FILE
SYSTEM
IN
THIS
WAY
IT
IS
ALWAYS
PERFECTLY
SAFE
FOR
A
PROCESS
TO
CHANGE
ITS
WORKING
DIRECTORY
WHENEVER
THAT
IS
CONVENIENT
ON
THE
OTHER
HAND
IF
A
LIBRARY
PROCEDURE
CHANGES
THE
WORKING
DIRECTORY
AND
DOES
NOT
CHANGE
BACK
TO
WHERE
IT
WAS
WHEN
IT
IS
FINISHED
THE
REST
OF
THE
PROGRAM
MAY
NOT
SEC
DIRECTORIES
WORK
SINCE
ITS
ASSUMPTION
ABOUT
WHERE
IT
IS
MAY
NOW
SUDDENLY
BE
INVALID
FOR
THIS
REASON
LIBRARY
PROCEDURES
RARELY
CHANGE
THE
WORKING
DIRECTORY
AND
WHEN
THEY
MUST
THEY
ALWAYS
CHANGE
IT
BACK
AGAIN
BEFORE
RETURNING
MOST
OPERATING
SYSTEMS
THAT
SUPPORT
A
HIERARCHICAL
DIRECTORY
SYSTEM
HAVE
TWO
SPECIAL
ENTRIES
IN
EVERY
DIRECTORY
AND
GENERALLY
PRONOUNCED
DOT
AND
DOTDOT
DOT
REFERS
TO
THE
CURRENT
DIRECTORY
DOTDOT
REFERS
TO
ITS
PARENT
EXCEPT
IN
THE
ROOT
DIRECTORY
WHERE
IT
REFERS
TO
ITSELF
TO
SEE
HOW
THESE
ARE
USED
CONSIDER
THE
UNIX
FILE
TREE
OF
FIG
A
CERTAIN
PROCESS
HAS
USR
AST
AS
ITS
WORKING
DIREC
TORY
IT
CAN
USE
TO
GO
HIGHER
UP
THE
TREE
FOR
EXAMPLE
IT
CAN
COPY
THE
FILE
USR
LIB
DICTIONARY
TO
ITS
OWN
DIRECTORY
USING
THE
COMMAND
CP
LIB
DICTIONARY
THE
FIRST
PATH
INSTRUCTS
THE
SYSTEM
TO
GO
UPWARD
TO
THE
USR
DIRECTORY
THEN
TO
GO
DOWN
TO
THE
DIRECTORY
LIB
TO
FIND
THE
FILE
DICTIONARY
BIN
ETC
LIB
USR
TMP
FIGURE
A
UNIX
DIRECTORY
TREE
THE
SECOND
ARGUMENT
DOT
NAMES
THE
CURRENT
DIRECTORY
WHEN
THE
CP
COM
MAND
GETS
A
DIRECTORY
NAME
INCLUDING
DOT
AS
ITS
LAST
ARGUMENT
IT
COPIES
ALL
THE
FILE
SYSTEMS
CHAP
FILES
TO
THAT
DIRECTORY
OF
COURSE
A
MORE
NORMAL
WAY
TO
DO
THE
COPY
WOULD
BE
TO
USE
THE
FULL
ABSOLUTE
PATH
NAME
OF
THE
SOURCE
FILE
CP
USR
LIB
DICTIONARY
HERE
THE
USE
OF
DOT
SAVES
THE
USER
THE
TROUBLE
OF
TYPING
DICTIONARY
A
SECOND
TIME
NEVERTHELESS
TYPING
CP
USR
LIB
DICTIONARY
DICTIONARY
ALSO
WORKS
FINE
AS
DOES
CP
USR
LIB
DICTIONARY
USR
AST
DICTIONARY
ALL
OF
THESE
DO
EXACTLY
THE
SAME
THING
DIRECTORY
OPERATIONS
THE
ALLOWED
SYSTEM
CALLS
FOR
MANAGING
DIRECTORIES
EXHIBIT
MORE
VARIATION
FROM
SYSTEM
TO
SYSTEM
THAN
SYSTEM
CALLS
FOR
FILES
TO
GIVE
AN
IMPRESSION
OF
WHAT
THEY
ARE
AND
HOW
THEY
WORK
WE
WILL
GIVE
A
SAMPLE
TAKEN
FROM
UNIX
CREATE
A
DIRECTORY
IS
CREATED
IT
IS
EMPTY
EXCEPT
FOR
DOT
AND
DOTDOT
WHICH
ARE
PUT
THERE
AUTOMATICALLY
BY
THE
SYSTEM
OR
IN
A
FEW
CASES
BY
THE
MKDIR
PROGRAM
DELETE
A
DIRECTORY
IS
DELETED
ONLY
AN
EMPTY
DIRECTORY
CAN
BE
DE
LETED
A
DIRECTORY
CONTAINING
ONLY
DOT
AND
DOTDOT
IS
CONSIDERED
EM
PTY
AS
THESE
CANNOT
USUALLY
BE
DELETED
OPENDIR
DIRECTORIES
CAN
BE
READ
FOR
EXAMPLE
TO
LIST
ALL
THE
FILES
IN
A
DIRECTORY
A
LISTING
PROGRAM
OPENS
THE
DIRECTORY
TO
READ
OUT
THE
NAMES
OF
ALL
THE
FILES
IT
CONTAINS
BEFORE
A
DIRECTORY
CAN
BE
READ
IT
MUST
BE
OPENED
ANALOGOUS
TO
OPENING
AND
READING
A
FILE
CLOSEDIR
WHEN
A
DIRECTORY
HAS
BEEN
READ
IT
SHOULD
BE
CLOSED
TO
FREE
UP
INTERNAL
TABLE
SPACE
READDIR
THIS
CALL
RETURNS
THE
NEXT
ENTRY
IN
AN
OPEN
DIRECTORY
FORM
ERLY
IT
WAS
POSSIBLE
TO
READ
DIRECTORIES
USING
THE
USUAL
READ
SYSTEM
CALL
BUT
THAT
APPROACH
HAS
THE
DISADVANTAGE
OF
FORCING
THE
PRO
GRAMMER
TO
KNOW
AND
DEAL
WITH
THE
INTERNAL
STRUCTURE
OF
DIRECTORIES
IN
CONTRAST
READDIR
ALWAYS
RETURNS
ONE
ENTRY
IN
A
STANDARD
FORMAT
NO
MATTER
WHICH
OF
THE
POSSIBLE
DIRECTORY
STRUCTURES
IS
BEING
USED
RENAME
IN
MANY
RESPECTS
DIRECTORIES
ARE
JUST
LIKE
FILES
AND
CAN
BE
RENAMED
THE
SAME
WAY
FILES
CAN
BE
LINK
LINKING
IS
A
TECHNIQUE
THAT
ALLOWS
A
FILE
TO
APPEAR
IN
MORE
THAN
ONE
DIRECTORY
THIS
SYSTEM
CALL
SPECIFIES
AN
EXISTING
FILE
AND
A
PATH
SEC
DIRECTORIES
NAME
AND
CREATES
A
LINK
FROM
THE
EXISTING
FILE
TO
THE
NAME
SPECIFIED
BY
THE
PATH
IN
THIS
WAY
THE
SAME
FILE
MAY
APPEAR
IN
MULTIPLE
DIREC
TORIES
A
LINK
OF
THIS
KIND
WHICH
INCREMENTS
THE
COUNTER
IN
THE
FILE
I
NODE
TO
KEEP
TRACK
OF
THE
NUMBER
OF
DIRECTORY
ENTRIES
CONTAINING
THE
FILE
IS
SOMETIMES
CALLED
A
HARD
LINK
UNLINK
A
DIRECTORY
ENTRY
IS
REMOVED
IF
THE
FILE
BEING
UNLINKED
IS
ONLY
PRESENT
IN
ONE
DIRECTORY
THE
NORMAL
CASE
IT
IS
REMOVED
FROM
THE
FILE
SYSTEM
IF
IT
IS
PRESENT
IN
MULTIPLE
DIRECTORIES
ONLY
THE
PATH
NAME
SPECIFIED
IS
REMOVED
THE
OTHERS
REMAIN
IN
UNIX
THE
SYSTEM
CALL
FOR
DELETING
FILES
DISCUSSED
EARLIER
IS
IN
FACT
UNLINK
THE
ABOVE
LIST
GIVES
THE
MOST
IMPORTANT
CALLS
BUT
THERE
ARE
A
FEW
OTHERS
AS
WELL
FOR
EXAMPLE
FOR
MANAGING
THE
PROTECTION
INFORMATION
ASSOCIATED
WITH
A
DIRECTORY
A
VARIANT
ON
THE
IDEA
OF
LINKING
FILES
IS
THE
SYMBOLIC
LINK
INSTEAD
OF
HAVING
TWO
NAMES
POINT
TO
THE
SAME
INTERNAL
DATA
STRUCTURE
REPRESENTING
A
FILE
A
NAME
CAN
BE
CREATED
THAT
POINTS
TO
A
TINY
FILE
NAMING
ANOTHER
FILE
WHEN
THE
FIRST
FILE
IS
USED
FOR
EXAMPLE
OPENED
THE
FILE
SYSTEM
FOLLOWS
THE
PATH
AND
FINDS
THE
NAME
AT
THE
END
THEN
IT
STARTS
THE
LOOKUP
PROCESS
ALL
OVER
USING
THE
NEW
NAME
SYMBOLIC
LINKS
HAVE
THE
ADVANTAGE
THAT
THEY
CAN
CROSS
DISK
BOUNDARIES
AND
EVEN
NAME
FILES
ON
REMOTE
COMPUTERS
THEIR
IMPLEMENTATION
IS
SOMEWHAT
LESS
EFFICIENT
THAN
HARD
LINKS
THOUGH
FIL
E
SYSTE
M
IMPLEMENTATIO
N
NOW
IT
IS
TIME
TO
TURN
FROM
THE
USER
VIEW
OF
THE
FILE
SYSTEM
TO
THE
IMPLE
MENTOR
VIEW
USERS
ARE
CONCERNED
WITH
HOW
FILES
ARE
NAMED
WHAT
OPERATIONS
ARE
ALLOWED
ON
THEM
WHAT
THE
DIRECTORY
TREE
LOOKS
LIKE
AND
SIMILAR
INTERFACE
IS
SUES
IMPLEMENTORS
ARE
INTERESTED
IN
HOW
FILES
AND
DIRECTORIES
ARE
STORED
HOW
DISK
SPACE
IS
MANAGED
AND
HOW
TO
MAKE
EVERYTHING
WORK
EFFICIENTLY
AND
RELIABLY
IN
THE
FOLLOWING
SECTIONS
WE
WILL
EXAMINE
A
NUMBER
OF
THESE
AREAS
TO
SEE
WHAT
THE
IS
SUES
AND
TRADE
OFFS
ARE
FILE
SYSTEM
LAYOUT
FILE
SYSTEMS
ARE
STORED
ON
DISKS
MOST
DISKS
CAN
BE
DIVIDED
UP
INTO
ONE
OR
MORE
PARTITIONS
WITH
INDEPENDENT
FILE
SYSTEMS
ON
EACH
PARTITION
SECTOR
OF
THE
DISK
IS
CALLED
THE
MBR
MASTER
BOOT
RECORD
AND
IS
USED
TO
BOOT
THE
COMPUTER
THE
END
OF
THE
MBR
CONTAINS
THE
PARTITION
TABLE
THIS
TABLE
GIVES
THE
STARTING
AND
ENDING
ADDRESSES
OF
EACH
PARTITION
ONE
OF
THE
PARTITIONS
IN
THE
TABLE
IS
MARKED
AS
ACTIVE
WHEN
THE
COMPUTER
IS
BOOTED
THE
BIOS
READS
IN
AND
EXECUTES
THE
MBR
THE
FIRST
THING
THE
MBR
PROGRAM
DOES
IS
LOCATE
THE
ACTIVE
PARTITION
READ
IN
ITS
FIRST
BLOCK
CALLED
THE
BOOT
BLOCK
AND
EXECUTE
IT
THE
PROGRAM
IN
THE
BOOT
BLOCK
LOADS
FILE
SYSTEMS
CHAP
THE
OPERATING
SYSTEM
CONTAINED
IN
THAT
PARTITION
FOR
UNIFORMITY
EVERY
PARTITION
STARTS
WITH
A
BOOT
BLOCK
EVEN
IF
IT
DOES
NOT
CONTAIN
A
BOOTABLE
OPERATING
SYSTEM
BESIDES
IT
MIGHT
CONTAIN
ONE
IN
THE
FUTURE
OTHER
THAN
STARTING
WITH
A
BOOT
BLOCK
THE
LAYOUT
OF
A
DISK
PARTITION
VARIES
A
LOT
FROM
FILE
SYSTEM
TO
FILE
SYSTEM
OFTEN
THE
FILE
SYSTEM
WILL
CONTAIN
SOME
OF
THE
ITEMS
SHOWN
IN
FIG
THE
FIRST
ONE
IS
THE
SUPERBLOCK
IT
CONTAINS
ALL
THE
KEY
PARAMETERS
ABOUT
THE
FILE
SYSTEM
AND
IS
READ
INTO
MEMORY
WHEN
THE
COMPUTER
IS
BOOTED
OR
THE
FILE
SYSTEM
IS
FIRST
TOUCHED
TYPICAL
INFORMATION
IN
THE
SUPERBLOCK
INCLUDES
A
MAGIC
NUMBER
TO
IDENTIFY
THE
FILE
SYSTEM
TYPE
THE
NUMBER
OF
BLOCKS
IN
THE
FILE
SYSTEM
AND
OTHER
KEY
ADMINISTRATIVE
INFORMATION
PARTITION
TABLE
ENTIRE
DISK
DISK
PARTITION
MB
R
III
BOOT
BLOCK
SUPERBLOC
K
FRE
E
SPAC
E
MGM
T
L
N
ODE
ROO
T
DIR
FILES
AN
D
DIRECTORIES
FIGURE
A
POSSIBLE
FILE
SYSTEM
LAYOUT
NEXT
MIGHT
COME
INFORMATION
ABOUT
FREE
BLOCKS
IN
THE
FILE
SYSTEM
FOR
EX
AMPLE
IN
THE
FORM
OF
A
BITMAP
OR
A
LIST
OF
POINTERS
THIS
MIGHT
BE
FOLLOWED
BY
DIE
I
NODES
AN
ARRAY
OF
DATA
STRUCTURES
ONE
PER
FILE
TELLING
ALL
ABOUT
THE
FILE
AFTER
THAT
MIGHT
COME
THE
ROOT
DIRECTORY
WHICH
CONTAINS
THE
TOP
OF
THE
FILE
SYSTEM
TREE
FINALLY
THE
REMAINDER
OF
THE
DISK
CONTAINS
ALL
THE
OTHER
DIRECTORIES
AND
FILES
IMPLEMENTING
FILES
PROBABLY
THE
MOST
IMPORTANT
ISSUE
IN
IMPLEMENTING
FILE
STORAGE
IS
KEEPING
TRACK
OF
WHICH
DISK
BLOCKS
GO
WITH
WHICH
FILE
VARIOUS
METHODS
ARE
USED
IN
DIF
FERENT
OPERATING
SYSTEMS
IN
THIS
SECTION
WE
WILL
EXAMINE
A
FEW
OF
THEM
CONTIGUOUS
ALLOCATION
THE
SIMPLEST
ALLOCATION
SCHEME
IS
TO
STORE
EACH
FILE
AS
A
CONTIGUOUS
RUN
OF
DISK
BLOCKS
THUS
ON
A
DISK
WITH
KB
BLOCKS
A
KB
FILE
WOULD
BE
ALLOCATED
CON
SECUTIVE
BLOCKS
WITH
KB
BLOCKS
IT
WOULD
BE
ALLOCATED
CONSECUTIVE
BLOCKS
SEC
FILE
SYSTEM
IMPLEMENTATION
WE
SEE
AN
EXAMPLE
OF
CONTIGUOUS
STORAGE
ALLOCATION
IN
FIG
A
HERE
THE
FIRST
DISK
BLOCKS
ARE
SHOWN
STARTING
WITH
BLOCK
ON
THE
LEFT
INITIALLY
THE
DISK
WAS
EMPTY
THEN
A
FILE
A
OF
LENGTH
FOUR
BLOCKS
WAS
WRITTEN
TO
DISK
STARTING
AT
THE
BEGINNING
BLOCK
AFTER
THAT
A
SIX
BLOCK
FILE
B
WAS
WRITTEN
STARTING
RIGHT
AFTER
THE
END
OF
FILE
A
NOTE
THAT
EACH
FILE
BEGINS
AT
THE
START
OF
A
NEW
BLOCK
SO
THAT
IF
FILE
A
WAS
REAL
LY
BLOCKS
SOME
SPACE
IS
WASTED
AT
THE
END
OF
THE
LAST
BLOCK
IN
THE
FIGURE
A
TOTAL
OF
SEVEN
FILES
ARE
SHOWN
EACH
ONE
STARTING
AT
THE
BLOCK
FOLLOWING
THE
END
OF
THE
PREVIOUS
ONE
SHADING
IS
USED
JUST
TO
MAKE
IT
EASIER
TO
TELL
THE
FILES
APART
IT
HAS
NO
ACTUAL
SIGNIFICANCE
IN
TERMS
OF
STORAGE
FILE
A
FILE
C
FILE
E
FILE
G
BLOCKS
BLOCKS
BLOCKS
BLOCKS
II
IUXLLL
I
M
I
J
IFHT
RTTTTTT
I
I
I
MMM
KURR
R
FILE
B
FILE
D
FILE
F
BLOCKS
BLOCKS
BLOCKS
A
FILE
A
FILE
C
FILE
E
FIL
E
G
I
TVI
L
G
I
G
I
II
IT
I
MI
L
I
I
I
I
FRKHMS
L
I
I
I
I
IN
FILE
B
FRE
E
BLOCKS
FRE
E
BLOCKS
B
FIGURE
A
CONTIGUOUS
ALLOCATION
OF
DISK
SPACE
FOR
SEVEN
FILES
B
THE
STATE
OF
THE
DISK
AFTER
FILES
D
AND
F
HAVE
BEEN
REMOVED
CONTIGUOUS
DISK
SPACE
ALLOCATION
HAS
TWO
SIGNIFICANT
ADVANTAGES
FIRST
IT
IS
SIMPLE
TO
IMPLEMENT
BECAUSE
KEEPING
TRACK
OF
WHERE
A
FILE
BLOCKS
ARE
IS
REDUCED
TO
REMEMBERING
TWO
NUMBERS
THE
DISK
ADDRESS
OF
THE
FIRST
BLOCK
AND
THE
NUMBER
OF
BLOCKS
IN
THE
FILE
GIVEN
THE
NUMBER
OF
THE
FIRST
BLOCK
THE
NUMBER
OF
ANY
OTHER
BLOCK
CAN
BE
FOUND
BY
A
SIMPLE
ADDITION
SECOND
THE
READ
PERFORMANCE
IS
EXCELLENT
BECAUSE
THE
ENTIRE
FILE
CAN
BE
READ
FROM
THE
DISK
IN
A
SINGLE
OPERATION
ONLY
ONE
SEEK
IS
NEEDED
TO
THE
FIRST
BLOCK
AFTER
THAT
NO
MORE
SEEKS
OR
ROTATIONAL
DELAYS
ARE
NEEDED
SO
DATA
COME
IN
AT
THE
FULL
BANDWIDTH
OF
THE
DISK
THUS
CONTIGUOUS
ALLOCATION
IS
SIMPLE
TO
IMPLEMENT
AND
HAS
HIGH
PERFORMANCE
UNFORTUNATELY
CONTIGUOUS
ALLOCATION
ALSO
HAS
A
FAIRLY
SIGNIFICANT
DRAWBACK
OVER
THE
COURSE
OF
TIME
THE
DISK
BECOMES
FRAGMENTED
TO
SEE
HOW
THIS
COMES
FILE
SYSTEMS
CHAP
ABOUT
EXAMINE
FIG
B
HERE
TWO
FILES
D
AND
F
HAVE
BEEN
REMOVED
WHEN
A
FILE
IS
REMOVED
ITS
BLOCKS
ARE
NATURALLY
FREED
LEAVING
A
RUN
OF
FREE
BLOCKS
ON
THE
DISK
THE
DISK
IS
NOT
COMPACTED
ON
THE
SPOT
TO
SQUEEZE
OUT
THE
HOLE
SINCE
THAT
WOULD
INVOLVE
COPYING
ALL
THE
BLOCKS
FOLLOWING
THE
HOLE
POTENTIALLY
MILLIONS
OF
BLOCKS
AS
A
RESULT
THE
DISK
ULTIMATELY
CONSISTS
OF
FILES
AND
HOLES
AS
ILLUSTRATED
IN
THE
FIGURE
INITIALLY
THIS
FRAGMENTATION
IS
NOT
A
PROBLEM
SINCE
EACH
NEW
FILE
CAN
BE
WRIT
TEN
AT
THE
END
OF
DISK
FOLLOWING
THE
PREVIOUS
ONE
HOWEVER
EVENTUALLY
THE
DISK
WILL
FILL
UP
AND
IT
WILL
BECOME
NECESSARY
TO
EITHER
COMPACT
THE
DISK
WHICH
IS
PROHIBITIVELY
EXPENSIVE
OR
TO
REUSE
THE
FREE
SPACE
IN
THE
HOLES
REUSING
THE
SPACE
REQUIRES
MAINTAINING
A
LIST
OF
HOLES
WHICH
IS
DOABLE
HOWEVER
WHEN
A
NEW
FILE
IS
TO
BE
CREATED
IT
IS
NECESSARY
TO
KNOW
ITS
FINAL
SIZE
IN
ORDER
TO
CHOOSE
A
HOLE
OF
THE
CORRECT
SIZE
TO
PLACE
IT
IN
IMAGINE
THE
CONSEQUENCES
OF
SUCH
A
DESIGN
THE
USER
STARTS
A
TEXT
EDITOR
OR
WORD
PROCESSOR
IN
ORDER
TO
TYPE
A
DOCUMENT
THE
FIRST
THING
THE
PROGRAM
ASKS
IS
HOW
MANY
BYTES
THE
FINAL
DOCUMENT
WILL
BE
THE
QUESTION
MUST
BE
ANSWERED
OR
THE
PROGRAM
WILL
NOT
CONTINUE
IF
THE
NUMBER
GIVEN
ULTIMATELY
PROVES
TOO
SMALL
THE
PROGRAM
HAS
TO
TERMINATE
PREMATURELY
BECAUSE
THE
DISK
HOLE
IS
FULL
AND
THERE
IS
NO
PLACE
TO
PUT
THE
REST
OF
THE
FILE
IF
THE
USER
TRIES
TO
AVOID
THIS
PROBLEM
BY
GIVING
AN
UNREALISTICALLY
LARGE
NUMBER
AS
THE
FINAL
SIZE
SAY
MB
THE
EDITOR
MAY
BE
UN
ABLE
TO
FIND
SUCH
A
LARGE
HOLE
AND
ANNOUNCE
THAT
THE
FILE
CANNOT
BE
CREATED
OF
COURSE
THE
USER
WOULD
BE
FREE
TO
START
THE
PROGRAM
AGAIN
AND
SAY
MB
THIS
TIME
AND
SO
ON
UNTIL
A
SUITABLE
HOLE
WAS
LOCATED
STILL
THIS
SCHEME
IS
NOT
LIKELY
TO
LEAD
TO
HAPPY
USERS
HOWEVER
THERE
IS
ONE
SITUATION
IN
WHICH
CONTIGUOUS
ALLOCATION
IS
FEASIBLE
AND
IN
FACT
WIDELY
USED
ON
CD
ROMS
HERE
ALL
THE
FILE
SIZES
ARE
KNOWN
IN
ADVANCE
AND
WILL
NEVER
CHANGE
DURING
SUBSEQUENT
USE
OF
THE
CD
ROM
FILE
SYSTEM
WE
WILL
STUDY
THE
MOST
COMMON
CD
ROM
FILE
SYSTEM
LATER
IN
THIS
CHAPTER
THE
SITUATION
WITH
DVDS
IS
A
BIT
MORE
COMPLICATED
IN
PRINCIPLE
A
MIN
MOVIE
COULD
BE
ENCODED
AS
A
SINGLE
FILE
OF
LENGTH
ABOUT
GB
BUT
THE
FILE
SYSTEM
USED
UDF
UNIVERSAL
DISK
FORMAT
USES
A
BIT
NUMBER
TO
REPRESENT
FILE
LENGTH
WHICH
LIMITS
FILES
TO
GB
AS
A
CONSEQUENCE
DVD
MOVIES
ARE
GENERALLY
STORED
AS
THREE
OR
FOUR
GB
FILES
EACH
OF
WHICH
IS
CONTIGUOUS
THESE
PHYSICAL
PIECES
OF
THE
SINGLE
LOGICAL
FILE
THE
MOVIE
ARE
CALLED
EXTENTS
AS
WE
MENTIONED
IN
CHAP
HISTORY
OFTEN
REPEATS
ITSELF
IN
COMPUTER
SCIENCE
AS
NEW
GENERATIONS
OF
TECHNOLOGY
OCCUR
CONTIGUOUS
ALLOCATION
WAS
ACTUALLY
USED
ON
MAGNETIC
DISK
FILE
SYSTEMS
YEARS
AGO
DUE
TO
ITS
SIMPLICITY
AND
HIGH
PER
FORMANCE
USER
FRIENDLINESS
DID
NOT
COUNT
FOR
MUCH
THEN
THEN
THE
IDEA
WAS
DROPPED
DUE
TO
THE
NUISANCE
OF
HAVING
TO
SPECIFY
FINAL
FILE
SIZE
AT
FILE
CREATION
TIME
BUT
WITH
THE
ADVENT
OF
CD
ROMS
DVDS
AND
OTHER
WRITE
ONCE
OPTICAL
ME
DIA
SUDDENLY
CONTIGUOUS
FILES
ARE
A
GOOD
IDEA
AGAIN
IT
IS
THUS
IMPORTANT
TO
STUDY
OLD
SYSTEMS
AND
IDEAS
THAT
WERE
CONCEPTUALLY
CLEAN
AND
SIMPLE
BECAUSE
THEY
MAY
BE
APPLICABLE
TO
FUTURE
SYSTEMS
IN
SURPRISING
WAYS
SEC
FILE
SYSTEM
IMPLEMENTATION
LINKED
LIST
ALLOCATION
THE
SECOND
METHOD
FOR
STORING
FILES
IS
TO
KEEP
EACH
ONE
AS
A
LINKED
LIST
OF
DISK
BLOCKS
AS
SHOWN
IN
FIG
THE
FIRST
WORD
OF
EACH
BLOCK
IS
USED
AS
A
POINTER
TO
THE
NEXT
ONE
THE
REST
OF
THE
BLOCK
IS
FOR
DATA
FILE
A
PHYSICAL
BLOCK
FILE
B
PHYSICA
L
BLOCK
FIGURE
STORING
A
FILE
AS
A
LINKED
LIST
OF
DISK
BLOCKS
UNLIKE
CONTIGUOUS
ALLOCATION
EVERY
DISK
BLOCK
CAN
BE
USED
IN
THIS
METHOD
NO
SPACE
IS
LOST
TO
DISK
FRAGMENTATION
EXCEPT
FOR
INTERNAL
FRAGMENTATION
IN
THE
LAST
BLOCK
ALSO
IT
IS
SUFFICIENT
FOR
THE
DIRECTORY
ENTRY
TO
MERELY
STORE
THE
DISK
AD
DRESS
OF
THE
FIRST
BLOCK
THE
REST
CAN
BE
FOUND
STARTING
THERE
ON
THE
OTHER
HAND
ALTHOUGH
READING
A
FILE
SEQUENTIALLY
IS
STRAIGHTFORWARD
RAN
DOM
ACCESS
IS
EXTREMELY
SLOW
TO
GET
TO
BLOCK
N
THE
OPERATING
SYSTEM
HAS
TO
START
AT
THE
BEGINNING
AND
READ
THE
N
BLOCKS
PRIOR
TO
IT
ONE
AT
A
TIME
CLEARLY
DOING
SO
MANY
READS
WILL
BE
PAINFULLY
SLOW
ALSO
THE
AMOUNT
OF
DATA
STORAGE
IN
A
BLOCK
IS
NO
LONGER
A
POWER
OF
TWO
BE
CAUSE
THE
POINTER
TAKES
UP
A
FEW
BYTES
WHILE
NOT
FATAL
HAVING
A
PECULIAR
SIZE
IS
LESS
EFFICIENT
BECAUSE
MANY
PROGRAMS
READ
AND
WRITE
IN
BLOCKS
WHOSE
SIZE
IS
A
POWER
OF
TWO
WITH
THE
FIRST
FEW
BYTES
OF
EACH
BLOCK
OCCUPIED
TO
A
POINTER
TO
THE
NEXT
BLOCK
READS
OF
THE
FULL
BLOCK
SIZE
REQUIRE
ACQUIRING
AND
CONCATENATING
INFOR
MATION
FROM
TWO
DISK
BLOCKS
WHICH
GENERATES
EXTRA
OVERHEAD
DUE
TO
THE
COPYING
LINKED
LIST
ALLOCATION
USING
A
TABLE
IN
MEMORY
BOTH
DISADVANTAGES
OF
THE
LINKED
LIST
ALLOCATION
CAN
BE
ELIMINATED
BY
TAKING
THE
POINTER
WORD
FROM
EACH
DISK
BLOCK
AND
PUTTING
IT
IN
A
TABLE
IN
MEMORY
FIGURE
SHOWS
WHAT
THE
TABLE
LOOKS
LIKE
FOR
THE
EXAMPLE
OF
FIG
IN
BOTH
FIGURES
FILE
SYSTEMS
CHAP
WE
HAVE
TWO
FILES
FILE
A
USES
DISK
BLOCKS
AND
IN
THAT
ORDER
AND
FDE
B
USES
DISK
BLOCKS
AND
IN
THAT
ORDER
USING
THE
TABLE
OF
FIG
WE
CAN
START
WITH
BLOCK
AND
FOLLOW
THE
CHAIN
ALL
THE
WAY
TO
THE
END
THE
SAME
CAN
BE
DONE
STARTING
WITH
BLOCK
BOTH
CHAINS
ARE
TERMINATED
WITH
A
SPECIAL
MARKER
E
G
THAT
IS
NOT
A
VALID
BLOCK
NUMBER
SUCH
A
TABLE
IN
MAIN
MEMORY
IS
CALLED
A
FAT
FILE
ALLOCATION
TABLE
PHYSICAL
BLOCK
SEC
FILE
SYSTEM
IMPLEMENTATION
I
NODES
OUR
LAST
METHOD
FOR
KEEPING
TRACK
OF
WHICH
BLOCKS
BELONG
TO
WHICH
FILE
IS
TO
ASSOCIATE
WITH
EACH
FILE
A
DATA
STRUCTURE
CALLED
AN
I
NODE
INDEX
NODE
WHICH
LISTS
THE
ATTRIBUTES
AND
DISK
ADDRESSES
OF
THE
FILE
BLOCKS
A
SIMPLE
EXAMPLE
IS
DE
PICTED
IN
FIG
GIVEN
THE
I
NODE
IT
IS
THEN
POSSIBLE
TO
FIND
ALL
THE
BLOCKS
OF
THE
FILE
THE
BIG
ADVANTAGE
OF
THIS
SCHEME
OVER
LINKED
FILES
USING
AN
IN
MEMORY
TABLE
IS
THAT
THE
I
NODE
NEED
ONLY
BE
IN
MEMORY
WHEN
THE
CORRESPONDING
FILE
IS
OPEN
IF
EACH
I
NODE
OCCUPIES
N
BYTES
AND
A
MAXIMUM
OF
K
FILES
MAY
BE
OPEN
AT
ONCE
THE
TOTAL
MEMORY
OCCUPIED
BY
THE
ARRAY
HOLDING
THE
I
NODES
FOR
THE
OPEN
FILES
IS
ONLY
KN
BYTES
ONLY
THIS
MUCH
SPACE
NEED
BE
RESERVED
IN
ADVANCE
J
FILE
A
STARTS
HER
E
FILE
B
START
HER
E
UNUSE
D
BLOCK
FIGURE
LINKED
LIST
ALLOCATION
USING
A
FILE
ALLOCATION
TABLE
IN
MAIN
MEMORY
USING
THIS
ORGANIZATION
THE
ENTIRE
BLOCK
IS
AVAILABLE
FOR
DATA
FURTHERMORE
RANDOM
ACCESS
IS
MUCH
EASIER
ALTHOUGH
THE
CHAIN
MUST
STILL
BE
FOLLOWED
TO
FIND
A
GIVEN
OFFSET
WITHIN
THE
FILE
THE
CHAIN
IS
ENTIRELY
IN
MEMORY
SO
IT
CAN
BE
FOLLOWED
WITHOUT
MAKING
ANY
DISK
REFERENCES
LIKE
THE
PREVIOUS
METHOD
IT
IS
SUFFICIENT
FOR
FIGURE
AN
EXAMPLE
I
NODE
DISK
BLOCK
CONTAINING
ADDITIONAL
DISK
ADDRESSE
THE
DIRECTORY
ENTRY
TO
KEEP
A
SINGLE
INTEGER
THE
STARTING
BLOCK
NUMBER
AND
STILL
BE
ABLE
TO
LOCATE
ALL
THE
BLOCKS
NO
MATTER
HOW
LARGE
THE
FILE
IS
THE
PRIMARY
DISADVANTAGE
OF
THIS
METHOD
IS
THAT
THE
ENTIRE
TABLE
MUST
BE
IN
MEMORY
ALL
THE
TIME
TO
MAKE
IT
WORK
WITH
A
GB
DISK
AND
A
KB
BLOCK
SIZE
THE
TABLE
NEEDS
MILLION
ENTRIES
ONE
FOR
EACH
OF
THE
MILLION
DISK
BLOCKS
EACH
ENTRY
HAS
TO
BE
A
MINIMUM
OF
BYTES
FOR
SPEED
IN
LOOKUP
THEY
SHOULD
BE
BYTES
THUS
THE
TABLE
WILL
TAKE
UP
MB
OR
MB
OF
MAIN
MEMORY
ALL
THE
TIME
DEPENDING
ON
WHETHER
THE
SYSTEM
IS
OPTIMIZED
FOR
SPACE
OR
TIME
NOT
WILDLY
PRACTICAL
CLEARLY
THE
FAT
IDEA
DOES
NOT
SCALE
WELL
TO
LARGE
DISKS
THIS
ARRAY
IS
USUALLY
FAR
SMALLER
THAN
THE
SPACE
OCCUPIED
BY
THE
FILE
TABLE
DE
SCRIBED
IN
THE
PREVIOUS
SECTION
THE
REASON
IS
SIMPLE
THE
TABLE
FOR
HOLDING
THE
LINKED
LIST
OF
ALL
DISK
BLOCKS
IS
PROPORTIONAL
IN
SIZE
TO
THE
DISK
ITSELF
IF
THE
DISK
HAS
N
BLOCKS
THE
TABLE
NEEDS
N
ENTRIES
AS
DISKS
GROW
LARGER
THIS
TABLE
GROWS
LINE
ARLY
WITH
THEM
IN
CONTRAST
THE
I
NODE
SCHEME
REQUIRES
AN
ARRAY
IN
MEMORY
WHOSE
SIZE
IS
PROPORTIONAL
TO
THE
MAXIMUM
NUMBER
OF
FILES
THAT
MAY
BE
OPEN
AT
ONCE
IT
DOES
NOT
MATTER
IF
THE
DISK
IS
GB
OR
GB
OR
GB
ONE
PROBLEM
WITH
I
NODES
IS
THAT
IF
EACH
ONE
HAS
ROOM
FOR
A
FIXED
NUMBER
OF
DISK
ADDRESSES
WHAT
HAPPENS
WHEN
A
FILE
GROWS
BEYOND
THIS
LIMIT
ONE
SOLUTION
FILE
SYSTEMS
CHAP
IS
TO
RESERVE
THE
LAST
DISK
ADDRESS
NOT
FOR
A
DATA
BLOCK
BUT
INSTEAD
FOR
THE
ADDRESS
OF
A
BLOCK
CONTAINING
MORE
DISK
BLOCK
ADDRESSES
AS
SHOWN
IN
FIG
EVEN
MORE
ADVANCED
WOULD
BE
TWO
OR
MORE
SUCH
BLOCKS
CONTAINING
DISK
ADDRESSES
OR
EVEN
DISK
BLOCKS
POINTING
TO
OTHER
DISK
BLOCKS
FULL
OF
ADDRESSES
WE
WILL
COME
BACK
TO
I
NODES
WHEN
STUDYING
UNIX
LATER
IMPLEMENTING
DIRECTORIES
BEFORE
A
FILE
CAN
BE
READ
IT
MUST
BE
OPENED
WHEN
A
FILE
IS
OPENED
THE
OPER
ATING
SYSTEM
USES
THE
PATH
NAME
SUPPLIED
BY
THE
USER
TO
LOCATE
THE
DIRECTORY
ENTRY
THE
DIRECTORY
ENTRY
PROVIDES
THE
INFORMATION
NEEDED
TO
FIND
THE
DISK
BLOCKS
DE
PENDING
ON
THE
SYSTEM
THIS
INFORMATION
MAY
BE
THE
DISK
ADDRESS
OF
THE
ENTIRE
FILE
WITH
CONTIGUOUS
ALLOCATION
THE
NUMBER
OF
THE
FIRST
BLOCK
BOTH
LINKED
LIST
SCHEMES
OR
THE
NUMBER
OF
THE
I
NODE
IN
ALL
CASES
THE
MAIN
FUNCTION
OF
THE
DI
RECTORY
SYSTEM
IS
TO
MAP
THE
ASCII
NAME
OF
THE
FILE
ONTO
THE
INFORMATION
NEEDED
TO
LOCATE
THE
DATA
A
CLOSELY
RELATED
ISSUE
IS
WHERE
THE
ATTRIBUTES
SHOULD
BE
STORED
EVERY
FILE
SYS
TEM
MAINTAINS
FILE
ATTRIBUTES
SUCH
AS
EACH
FILE
OWNER
AND
CREATION
TIME
AND
THEY
MUST
BE
STORED
SOMEWHERE
ONE
OBVIOUS
POSSIBILITY
IS
TO
STORE
THEM
DIRECTLY
IN
THE
DIRECTORY
ENTRY
MANY
SYSTEMS
DO
PRECISELY
THAT
THIS
OPTION
IS
SHOWN
IN
FIG
A
IN
THIS
SIMPLE
DESIGN
A
DIRECTORY
CONSISTS
OF
A
LIST
OF
FIXED
SIZE
ENTRIES
ONE
PER
FILE
CONTAINING
A
FIXED
LENGTH
FILE
NAME
A
STRUCTURE
OF
THE
FILE
ATTRIBUTES
AND
ONE
OR
MORE
DISK
ADDRESSES
UP
TO
SOME
MAXIMUM
TELLING
WHERE
THE
DISK
BLOCKS
ARE
DAT
A
STRUCTURE
SEC
FILE
SYSTEM
IMPLEMENTATION
IN
FIG
B
AS
WE
SHALL
SEE
LATER
THIS
METHOD
HAS
SOME
ADVANTAGES
OVER
PUT
TING
THEM
IN
THE
DIRECTORY
ENTRY
THE
TWO
APPROACHES
SHOWN
IN
FIG
CORRES
POND
TO
WINDOWS
AND
UNIX
RESPECTIVELY
AS
WE
WILL
SEE
LATER
SO
FAR
WE
HAVE
MADE
THE
ASSUMPTION
THAT
FILES
HAVE
SHORT
FIXED
LENGTH
NAMES
IN
MS
DOS
FILES
HAVE
A
CHARACTER
BASE
NAME
AND
AN
OPTIONAL
EXTENSION
OF
CHARACTERS
IN
UNIX
VERSION
FILE
NAMES
WERE
CHARACTERS
INCLUDING
ANY
EXTENSIONS
HOWEVER
NEARLY
ALL
MODERN
OPERATING
SYSTEMS
SUPPORT
LONGER
VARI
ABLE
LENGTH
FILE
NAMES
HOW
CAN
THESE
BE
IMPLEMENTED
THE
SIMPLEST
APPROACH
IS
TO
SET
A
LIMIT
ON
FILE
NAME
LENGTH
TYPICALLY
CHAR
ACTERS
AND
THEN
USE
ONE
OF
THE
DESIGNS
OF
FIG
WITH
CHARACTERS
RESERVED
FOR
EACH
FILE
NAME
THIS
APPROACH
IS
SIMPLE
BUT
WASTES
A
GREAT
DEAL
OF
DIRECTORY
SPACE
SINCE
FEW
FILES
HAVE
SUCH
LONG
NAMES
FOR
EFFICIENCY
REASONS
A
DIFFERENT
STRUCTURE
IS
DESIRABLE
ONE
ALTERNATIVE
IS
TO
GIVE
UP
THE
IDEA
THAT
ALL
DIRECTORY
ENTRIES
ARE
THE
SAME
SIZE
WITH
THIS
METHOD
EACH
DIRECTORY
ENTRY
CONTAINS
A
FIXED
PORTION
TYPICALLY
STARTING
WITH
THE
LENGTH
OF
THE
ENTRY
AND
THEN
FOLLOWED
BY
DATA
WITH
A
FIXED
FOR
MAT
USUALLY
INCLUDING
THE
OWNER
CREATION
TIME
PROTECTION
INFORMATION
AND
OTHER
ATTRIBUTES
THIS
FIXED
LENGTH
HEADER
IS
FOLLOWED
BY
THE
ACTUAL
FILE
NAME
HOWEVER
LONG
IT
MAY
BE
AS
SHOWN
IN
FIG
A
IN
BIG
ENDIAN
FORMAT
E
G
SPARC
IN
THIS
EXAMPLE
WE
HAVE
THREE
FILES
PROJECT
BUDGET
PERSONNEL
AND
FOO
EACH
FILE
NAME
IS
TERMINATED
BY
A
SPECIAL
CHARACTER
USUALLY
WHICH
IS
REPRESENTED
IN
THE
FIGURE
BY
A
BOX
WITH
A
CROSS
IN
IT
TO
ALLOW
EACH
DIRECTORY
ENTRY
TO
BEGIN
ON
A
WORD
BOUNDARY
EACH
FILE
NAME
IS
FILLED
OUT
TO
AN
INTEGRAL
NUMBER
OF
WORDS
SHOWN
BY
SHADED
BOXES
IN
THE
FIGURE
A
DISADVANTAGE
OF
THIS
METHOD
IS
THAT
WHEN
A
FILE
IS
REMOVED
A
VARIABLE
SIZED
GAP
IS
INTRODUCED
INTO
THE
DIRECTORY
INTO
WHICH
THE
NEXT
FILE
TO
BE
ENTERED
MAY
NOT
FIT
THIS
PROBLEM
IS
THE
SAME
ONE
WE
SAW
WITH
CONTIGUOUS
DISK
FILES
ONLY
NOW
COMPACTING
THE
DIRECTORY
IS
FEASIBLE
BECAUSE
IT
IS
ENTIRELY
IN
MEMORY
ANOTHER
PROBLEM
IS
THAT
A
SINGLE
DIRECTORY
ENTRY
MAY
SPAN
MULTIPLE
PAGES
SO
A
PAGE
FAULT
MAY
OCCUR
WHILE
READING
A
FILE
NAME
ANOTHER
WAY
TO
HANDLE
VARIABLE
LENGTH
NAMES
IS
TO
MAKE
THE
DIRECTORY
ENTRIES
THEMSELVES
ALL
FIXED
LENGTH
AND
KEEP
THE
FILE
NAMES
TOGETHER
IN
A
HEAP
AT
THE
END
OF
THE
DIRECTORY
AS
SHOWN
IN
FIG
B
THIS
METHOD
HAS
THE
ADVANTAGE
THAT
WHEN
AN
ENTRY
IS
REMOVED
THE
NEXT
FILE
ENTERED
WILL
ALWAYS
FIT
THERE
OF
COURSE
A
CONTAINING
THE
ATTRIBUTES
THE
HEAP
MUST
BE
MANAGED
AND
PAGE
FAULTS
CAN
STILL
OCCUR
WHILE
PROCESSING
FILE
NAMES
ONE
MINOR
WIN
HERE
IS
THAT
THERE
IS
NO
LONGER
ANY
REAL
NEED
FOR
FILE
NAMES
TO
BEGIN
AT
WORD
BOUNDARIES
SO
NO
FILLER
CHARACTERS
ARE
NEEDED
AFTER
FILE
NAMES
IN
FIGUR
E
A
A
SIMPL
E
DIRECTORY
CONTAININ
G
FIXED
SIZ
E
ENTRIES
WIT
H
THE
DIS
K
ADDRESSES
AND
ATTRIBUTES
IN
THE
DIRECTORY
ENTRY
B
A
DIRECTORY
IN
WHIC
H
EAC
H
ENTRY
JUS
T
REFERS
TO
AN
I
NODE
FOR
SYSTEMS
THAT
USE
I
NODES
ANOTHER
POSSIBILITY
FOR
STORING
THE
ATTRIBUTES
IS
IN
THE
I
NODES
RATHER
THAN
IN
THE
DIRECTORY
ENTRIES
IN
THAT
CASE
THE
DIRECTORY
ENTRY
CAN
BE
SHORTER
JUST
A
FILE
NAME
AND
AN
I
NODE
NUMBER
THIS
APPROACH
IS
ILLUSTRATED
FIG
B
AS
THEY
ARE
IN
FIG
A
IN
ALL
OF
THE
DESIGNS
SO
FAR
DIRECTORIES
ARE
SEARCHED
LINEARLY
FROM
BEGINNING
TO
END
WHEN
A
FILE
NAME
HAS
TO
BE
LOOKED
UP
FOR
EXTREMELY
LONG
DIRECTORIES
LINEAR
SEARCHING
CAN
BE
SLOW
ONE
WAY
TO
SPEED
UP
THE
SEARCH
IS
TO
USE
A
HASH
TABLE
IN
EACH
DIRECTORY
CALL
THE
SIZE
OF
THE
TABLE
N
TO
ENTER
A
FILE
NAME
THE
NAME
IS
HASHED
ONTO
A
VALUE
BETWEEN
AND
N
L
FOR
EXAMPLE
BY
DIVIDING
IT
BY
N
AND
FILE
SYSTEMS
CHAP
SEC
FILE
SYSTEM
IMPLEMENTATION
SHARED
FILES
FILE
ENTRY
LENGTH
FILE
ATTRIBUTES
FILE
ENTRY
LENGTH
FILE
ATTRIBUTES
FILE
ENTRY
LENGTH
FILE
ATTRIBUTES
J
I
O
I
A
POINTER
TO
FILE
NAME
FILE
ATTRIBUTES
POINTER
TO
FILE
NAME
FILE
ATTRIBUTES
POINTER
TO
FILE
NAME
FILE
ATTRIBUTES
P
R
I
E
C
T
B
U
D
G
E
T
IS
P
E
R
P
N
E
I
F
IS
B
WHEN
SEVERAL
USERS
ARE
WORKING
TOGETHER
ON
A
PROJECT
THEY
OFTEN
NEED
TO
SHARE
FILES
AS
A
RESULT
IT
IS
OFTEN
CONVENIENT
FOR
A
SHARED
FILE
TO
APPEAR
SIMULTANEOUSLY
IN
DIFFERENT
DIRECTORIES
BELONGING
TO
DIFFERENT
USERS
FIGURE
SHOWS
THE
FILE
SYSTEM
OF
FIG
AGAIN
ONLY
WITH
ONE
OF
CS
FILES
NOW
PRESENT
IN
ONE
OF
B
DI
RECTORIES
AS
WELL
THE
CONNECTION
BETWEEN
TS
DIRECTORY
AND
THE
SHARED
FILE
IS
CALL
ED
A
LINK
THE
FILE
SYSTEM
ITSELF
IS
NOW
A
DIRECTED
ACYCLIC
GRAPH
OR
DAG
RATH
ER
THAN
A
TREE
FIGURE
TWO
WAYS
OF
HANDLING
LONG
FILE
NAMES
IN
A
DIRECTORY
A
IN
LINE
B
IN
A
HEAP
TAKING
THE
REMAINDER
ALTERNATIVELY
THE
WORDS
COMPRISING
THE
FILE
NAME
CANTJE
ADDED
UP
AND
THIS
QUANTITY
DIVIDED
BY
N
OR
SOMETHING
SIMILAR
EITHER
WAY
THE
TABLE
ENTRY
CORRESPONDING
TO
THE
HASH
CODE
IS
INSPECTED
IF
IT
IS
UNUSED
A
POINTER
IS
PLACED
THERE
TO
THE
FILE
ENTRY
FILE
ENTRIES
FOLLOW
THE
HASH
TABLE
IF
THAT
SLOT
IS
ALREADY
IN
USE
A
LINKED
LIST
IS
CONSTRUCTED
HEADED
AT
THE
TABLE
ENTRY
AND
THREADING
THROUGH
ALL
ENTRIES
WITH
THE
SAME
HASH
VALUE
LOOKING
UP
A
FILE
FOLLOWS
THE
SAME
PROCEDURE
THE
FILE
NAME
IS
HASHED
TO
SELECT
A
HASH
TABLE
ENTRY
ALL
THE
ENTRIES
ON
THE
CHAIN
HEADED
AT
THAT
SLOT
ARE
CHECKED
TO
SEE
IF
THE
FILE
NAME
IS
PRESENT
IF
THE
NAME
IS
NOT
ON
THE
CHAIN
THE
FILE
IS
NOT
PRESENT
IN
THE
DIRECTORY
USING
A
HASH
TABLE
HAS
THE
ADVANTAGE
OF
MUCH
FASTER
LOOKUP
BUT
THE
DISADVAN
TAGE
OF
MORE
COMPLEX
ADMINISTRATION
IT
IS
ONLY
REALLY
A
SERIOUS
CANDIDATE
IN
SYS
TEMS
WHERE
IT
IS
EXPECTED
THAT
DIRECTORIES
WILL
ROUTINELY
CONTAIN
HUNDREDS
OR
THOUSANDS
OF
FILES
A
DIFFERENT
WAY
TO
SPEED
UP
SEARCHING
LARGE
DIRECTORIES
IS
TO
CACHE
THE
RESULTS
OF
SEARCHES
BEFORE
STARTING
A
SEARCH
A
CHECK
IS
FIRST
MADE
TO
SEE
IF
THE
FILE
NAME
IS
IN
THE
CACHE
IF
SO
IT
CAN
BE
LOCATED
IMMEDIATELY
OF
COURSE
CACHING
ONLY
WORKS
IF
A
RELATIVELY
SMALL
NUMBER
OF
FILES
COMPRISE
THE
MAJORITY
OF
THE
LOOKUPS
SHARED
FILE
FIGURE
FILE
SYSTEM
CONTAINING
A
SHARED
FILE
SHARING
FILES
IS
CONVENIENT
BUT
IT
ALSO
INTRODUCES
SOME
PROBLEMS
TO
START
WITH
IF
DIRECTORIES
REALLY
DO
CONTAIN
DISK
ADDRESSES
THEN
A
COPY
OF
THE
DISK
AD
DRESSES
WILL
HAVE
TO
BE
MADE
IN
DIRECTORY
WHEN
THE
FILE
IS
LINKED
IF
EITHER
B
OR
C
SUBSEQUENTLY
APPENDS
TO
THE
FILE
THE
NEW
BLOCKS
WILL
BE
LISTED
ONLY
IN
THE
DIREC
TORY
OF
THE
USER
DOING
THE
APPEND
THE
CHANGES
WILL
NOT
BE
VISIBLE
TO
THE
OTHER
USER
THUS
DEFEATING
THE
PURPOSE
OF
SHARING
THIS
PROBLEM
CAN
BE
SOLVED
IN
TWO
WAYS
IN
THE
FIRST
SOLUTION
DISK
BLOCKS
ARE
NOT
LISTED
IN
DIRECTORIES
BUT
IN
A
LITTLE
DATA
STRUCTURE
ASSOCIATED
WITH
THE
FILE
ITSELF
THE
DIRECTORIES
WOULD
THEN
POINT
JUST
TO
THE
LITTLE
DATA
STRUCTURE
THIS
IS
THE
AP
PROACH
USED
IN
UNIX
WHERE
THE
LITTLE
DATA
STRUCTURE
IS
THE
I
NODE
IN
THE
SECOND
SOLUTION
B
LINKS
TO
ONE
OF
C
FILES
BY
HAVING
THE
SYSTEM
CREATE
A
NEW
FILE
OF
TYPE
LINK
AND
ENTERING
THAT
FILE
IN
B
DIRECTORY
THE
NEW
FILE
CON
TAINS
JUST
THE
PATH
NAME
OF
THE
FILE
TO
WHICH
IT
IS
LINKED
WHEN
B
READS
FROM
THE
LINKED
FILE
THE
OPERATING
SYSTEM
SEES
THAT
THE
FILE
BEING
READ
FROM
IS
OF
TYPE
LINK
LOOKS
UP
THE
NAME
OF
THE
FILE
AND
READS
THAT
FILE
THIS
APPROACH
IS
CALLED
SYMBOLIC
LINKING
TO
CONTRAST
IT
WITH
TRADITIONAL
HARD
LINKING
FILE
SYSTEMS
CHAP
EACH
OF
THESE
METHODS
HAS
ITS
DRAWBACKS
IN
THE
FIRST
METHOD
AT
THE
MOMENT
THAT
B
LINKS
TO
THE
SHARED
FILE
THE
I
NODE
RECORDS
THE
FILE
OWNER
AS
C
CREATING
A
LINK
DOES
NOT
CHANGE
THE
OWNERSHIP
SEE
FIG
BUT
IT
DOES
INCREASE
THE
LINK
COUNT
IN
THE
I
NODE
SO
THE
SYSTEM
KNOWS
HOW
MANY
DIRECTORY
ENTRIES
CURRENTLY
POINT
TO
THE
FILE
C
DIRECTORY
B
DIRECTORY
C
DIRECTORY
B
DIRECTORY
SEC
FILE
SYSTEM
IMPLEMENTATION
OPTIMIZATION
SYMBOLIC
LINKS
HAVE
THE
ADVANTAGE
THAT
THEY
CAN
BE
USED
TO
LINK
TO
FILES
ON
MACHINES
ANYWHERE
IN
THE
WORLD
BY
SIMPLY
PROVIDING
THE
NETWORK
ADDRESS
OF
THE
MACHINE
WHERE
THE
FILE
RESIDES
IN
ADDITION
TO
ITS
PATH
ON
THAT
MACHINE
THERE
IS
ALSO
ANOTHER
PROBLEM
INTRODUCED
BY
LINKS
SYMBOLIC
OR
OTHERWISE
WHEN
LINKS
ARE
ALLOWED
FILES
CAN
HAVE
TWO
OR
MORE
PATHS
PROGRAMS
THAT
START
AT
A
GIVEN
DIRECTORY
AND
FIND
ALL
THE
FILES
IN
THAT
DIRECTORY
AND
ITS
SUBDIRECTORIES
WILL
LOCATE
A
LINKED
FILE
MULTIPLE
TIMES
FOR
EXAMPLE
A
PROGRAM
THAT
DUMPS
ALL
THE
FILES
IN
A
DIRECTORY
AND
ITS
SUBDIRECTORIES
ONTO
A
TAPE
MAY
MAKE
MULTIPLE
COPIES
OF
A
LINKED
FILE
FURTHERMORE
IF
THE
TAPE
IS
THEN
READ
INTO
ANOTHER
MACHINE
UNLESS
THE
DUMP
PROGRAM
IS
CLEVER
THE
LINKED
FILE
WILL
BE
COPIED
TWICE
ONTO
THE
DISK
INSTEAD
OF
BEING
LINKED
J
COOUNT
LOG
STRUCTURED
FILE
SYSTEMS
CHANGES
IN
TECHNOLOGY
ARE
PUTTING
PRESSURE
ON
CURRENT
FILE
SYSTEMS
IN
PARTIC
A
C
FIGUR
E
A
SITUATION
PRIOR
T
O
LINKING
B
AFTE
R
THE
LIN
K
I
CREATED
C
AFTE
R
THE
ORIGINAL
OWNER
REMOVES
THE
FILE
IF
C
SUBSEQUENTLY
TRIES
TO
REMOVE
THE
FILE
THE
SYSTEM
IS
FACED
WITH
A
PROBLEM
IF
IT
REMOVES
THE
FILE
AND
CLEARS
THE
I
NODE
B
WILL
HAVE
A
DIRECTORY
ENTRY
POINTING
TO
AN
INVALID
I
NODE
IF
THE
I
NODE
IS
LATER
REASSIGNED
TO
ANOTHER
FILE
B
LINK
WILL
POINT
TO
THE
WRONG
FILE
THE
SYSTEM
CAN
SEE
FROM
THE
COUNT
IN
THE
I
NODE
THAT
THE
FILE
IS
STILL
IN
USE
BUT
THERE
IS
NO
EASY
WAY
FOR
IT
TO
FIND
ALL
THE
DIRECTORY
ENTRIES
FOR
THE
FILE
IN
ORDER
TO
ERASE
THEM
POINTERS
TO
THE
DIRECTORIES
CANNOT
BE
STORED
IN
THE
I
NODE
BECAUSE
THERE
CAN
BE
AN
UNLIMITED
NUMBER
OF
DIRECTORIES
THE
ONLY
THING
TO
DO
IS
REMOVE
C
DIRECTORY
ENTRY
BUT
LEAVE
THE
I
NODE
INTACT
WITH
COUNT
SET
TO
AS
SHOWN
IN
FIG
C
WE
NOW
HAVE
A
SITUATION
IN
WHICH
B
IS
THE
ONLY
USER
HAVING
A
DIRECTORY
ENTRY
FOR
A
FILE
OWNED
BY
C
IF
THE
SYSTEM
DOES
ACCOUNTING
OR
HAS
QUOTAS
C
WILL
CONTINUE
TO
BE
BILLED
FOR
THE
FILE
UNTIL
B
DECIDES
TO
REMOVE
IT
IF
EVER
AT
WHICH
TIME
THE
COUNT
GOES
TO
AND
THE
FILE
IS
DELETED
WITH
SYMBOLIC
LINKS
THIS
PROBLEM
DOES
NOT
ARISE
BECAUSE
ONLY
THE
TRUE
OWNER
HAS
A
POINTER
TO
THE
I
NODE
USERS
WHO
HAVE
LINKED
TO
THE
FILE
JUST
HAVE
PATH
NAMES
NOT
I
NODE
POINTERS
WHEN
THE
OWNER
REMOVES
THE
FILE
IT
IS
DESTROYED
SUBSEQUENT
ATTEMPTS
TO
USE
THE
FILE
VIA
A
SYMBOLIC
LINK
WILL
FAIL
WHEN
THE
SYSTEM
IS
UNABLE
TO
LOCATE
THE
FILE
REMOVING
A
SYMBOLIC
LINK
DOES
NOT
AFFECT
THE
FILE
AT
ALL
THE
PROBLEM
WITH
SYMBOLIC
LINKS
IS
THE
EXTRA
OVERHEAD
REQUIRED
THE
FILE
CON
TAINING
THE
PATH
MUST
BE
READ
THEN
THE
PATH
MUST
BE
PARSED
AND
FOLLOWED
COM
PONENT
BY
COMPONENT
UNTIL
THE
I
NODE
IS
REACHED
ALL
OF
THIS
ACTIVITY
MAY
REQUIRE
A
CONSIDERABLE
NUMBER
OF
EXTRA
DISK
ACCESSES
FURTHERMORE
AN
EXTRA
I
NODE
IS
NEEDED
FOR
EACH
SYMBOLIC
LINK
AS
IS
AN
EXTRA
DISK
BLOCK
TO
STORE
THE
PATH
ALTHOUGH
IF
THE
PATH
NAME
IS
SHORT
THE
SYSTEM
COULD
STORE
IT
IN
THE
I
NODE
ITSELF
AS
A
KIND
OF
ULAR
CPUS
KEEP
GETTING
FASTER
DISKS
ARE
BECOMING
MUCH
BIGGER
AND
CHEAPER
BUT
NOT
MUCH
FASTER
AND
MEMORIES
ARE
GROWING
EXPONENTIALLY
IN
SIZE
THE
ONE
PA
RAMETER
THAT
IS
NOT
IMPROVING
BY
LEAPS
AND
BOUNDS
IS
DISK
SEEK
TIME
THE
COMBINA
TION
OF
THESE
FACTORS
MEANS
THAT
A
PERFORMANCE
BOTTLENECK
IS
ARISING
IN
MANY
FILE
SYSTEMS
RESEARCH
DONE
AT
BERKELEY
ATTEMPTED
TO
ALLEVIATE
THIS
PROBLEM
BY
DE
SIGNING
A
COMPLETELY
NEW
KIND
OF
FILE
SYSTEM
LFS
THE
LOG
STRUCTURED
FILE
SYS
TEM
IN
THIS
SECTION
WE
WILL
BRIEFLY
DESCRIBE
HOW
LFS
WORKS
FOR
A
MORE
COM
PLETE
TREATMENT
SEE
ROSENBLUM
AND
OUSTERHOUT
THE
IDEA
THAT
DROVE
THE
LFS
DESIGN
IS
THAT
AS
CPUS
GET
FASTER
AND
RAM
MEMORIES
GET
LARGER
DISK
CACHES
ARE
ALSO
INCREASING
RAPIDLY
CONSEQUENTLY
IT
IS
NOW
POSSIBLE
TO
SATISFY
A
VERY
SUBSTANTIAL
FRACTION
OF
ALL
READ
REQUESTS
DIRECTLY
FROM
THE
FILE
SYSTEM
CACHE
WITH
NO
DISK
ACCESS
NEEDED
IT
FOLLOWS
FROM
THIS
OBSERVATION
THAT
IN
THE
FUTURE
MOST
DISK
ACCESSES
WILL
BE
WRITES
SO
THE
READ
AHEAD
MECHANISM
USED
IN
SOME
FILE
SYSTEMS
TO
FETCH
BLOCKS
BEFORE
THEY
ARE
NEEDED
NO
LONGER
GAINS
MUCH
PERFORMANCE
TO
MAKE
MATTERS
WORSE
IN
MOST
FILE
SYSTEMS
WRITES
ARE
DONE
IN
VERY
SMALL
CHUNKS
SMALL
WRITES
ARE
HIGHLY
INEFFICIENT
SINCE
A
PSEC
DISK
WRITE
IS
OFTEN
PRE
CEDED
BY
A
MSEC
SEEK
AND
A
MSEC
ROTATIONAL
DELAY
WITH
THESE
PARAMETERS
DISK
EFFICIENCY
DROPS
TO
A
FRACTION
OF
TO
SEE
WHERE
ALL
THE
SMALL
WRITES
COME
FROM
CONSIDER
CREATING
A
NEW
FILE
ON
A
UNIX
SYSTEM
TO
WRITE
THIS
FILE
THE
I
NODE
FOR
THE
DIRECTORY
THE
DIRECTORY
BLOCK
THE
I
NODE
FOR
THE
FILE
AND
THE
FILE
ITSELF
MUST
ALL
BE
WRITTEN
WHILE
THESE
WRITES
CAN
BE
DELAYED
DOING
SO
EXPOSES
THE
FILE
SYSTEM
TO
SERIOUS
CONSISTENCY
PROBLEMS
IF
A
CRASH
OCCURS
BEFORE
THE
WRITES
ARE
DONE
FOR
THIS
REASON
THE
I
NODE
WRITES
ARE
GENERALLY
DONE
IMMEDIATELY
FROM
THIS
REASONING
THE
LFS
DESIGNERS
DECIDED
TO
RE
IMPLEMENT
THE
UNIX
FILE
SYSTEM
IN
SUCH
A
WAY
AS
TO
ACHIEVE
THE
FULL
BANDWIDTH
OF
THE
DISK
EVEN
IN
THE
FACE
OF
A
WORKLOAD
CONSISTING
IN
LARGE
PART
OF
SMALL
RANDOM
WRITES
THE
BASIC
IDEA
IS
TO
FILE
SYSTEMS
CHAP
STRUCTURE
THE
ENTIRE
DISK
AS
A
LOG
PERIODICALLY
AND
WHEN
THERE
IS
A
SPECIAL
NEED
FOR
IT
ALL
THE
PENDING
WRITES
BEING
BUFFERED
IN
MEMORY
ARE
COLLECTED
INTO
A
SINGLE
SEGMENT
AND
WRITTEN
TO
THE
DISK
AS
A
SINGLE
CONTIGUOUS
SEGMENT
AT
THE
END
OF
THE
LOG
A
SINGLE
SEGMENT
MAY
THUS
CONTAIN
I
NODES
DIRECTORY
BLOCKS
AND
DATA
BLOCKS
ALL
MIXED
TOGETHER
AT
THE
START
OF
EACH
SEGMENT
IS
A
SEGMENT
SUMMARY
TELLING
WHAT
CAN
BE
FOUND
IN
THE
SEGMENT
IF
THE
AVERAGE
SEGMENT
CAN
BE
MADE
TO
BE
ABOUT
MB
ALMOST
THE
FULL
BANDWIDTH
OF
THE
DISK
CAN
BE
UTILIZED
IN
THIS
DESIGN
I
NODES
STILL
EXIST
AND
HAVE
THE
SAME
STRUCTURE
AS
IN
UNIX
BUT
THEY
ARE
NOW
SCATTERED
ALL
OVER
THE
LOG
INSTEAD
OF
BEING
AT
A
FIXED
POSITION
ON
THE
DISK
NEVERTHELESS
WHEN
AN
I
NODE
IS
LOCATED
LOCATING
THE
BLOCKS
IS
DONE
IN
THE
USUAL
WAY
OF
COURSE
FINDING
AN
I
NODE
IS
NOW
MUCH
HARDER
SINCE
ITS
ADDRESS
CANNOT
SIMPLY
BE
CALCULATED
FROM
ITS
I
NUMBER
AS
IN
UNIX
TO
MAKE
IT
POSSIBLE
TO
FIND
I
NODES
AN
I
NODE
MAP
INDEXED
BY
I
NUMBER
IS
MAINTAINED
ENTRY
I
IN
THIS
MAP
POINTS
TO
I
NODE
I
ON
THE
DISK
THE
MAP
IS
KEPT
ON
DISK
BUT
IT
IS
ALSO
CACHED
SO
THE
MOST
HEAVILY
USED
PARTS
WILL
BE
IN
MEMORY
MOST
OF
THE
TIME
TO
SUMMARIZE
WHAT
WE
HAVE
SAID
SO
FAR
ALL
WRITES
ARE
INITIALLY
BUFFERED
IN
MEMORY
AND
PERIODICALLY
ALL
THE
BUFFERED
WRITES
ARE
WRITTEN
TO
THE
DISK
IN
A
SINGLE
SEGMENT
AT
THE
END
OF
THE
LOG
OPENING
A
FILE
NOW
CONSISTS
OF
USING
THE
MAP
TO
LOCATE
THE
I
NODE
FOR
THE
FILE
ONCE
THE
I
NODE
HAS
BEEN
LOCATED
THE
ADDRESSES
OF
THE
BLOCKS
CAN
BE
FOUND
FROM
IT
ALL
OF
THE
BLOCKS
WILL
THEMSELVES
BE
IN
SEGMENTS
SOMEWHERE
IN
THE
LOG
IF
DISKS
WERE
INFINITELY
LARGE
THE
ABOVE
DESCRIPTION
WOULD
BE
THE
ENTIRE
STORY
HOWEVER
REAL
DISKS
ARE
FINITE
SO
EVENTUALLY
THE
LOG
WILL
OCCUPY
THE
ENTIRE
DISK
AT
WHICH
TIME
NO
NEW
SEGMENTS
CAN
BE
WRITTEN
TO
THE
LOG
FORTUNATELY
MANY
EXISTING
SEGMENTS
MAY
HAVE
BLOCKS
THAT
ARE
NO
LONGER
NEEDED
FOR
EXAMPLE
IF
A
FILE
ISNOVER
WRITTEN
ITS
I
NODE
WILL
NOW
POINT
TO
THE
NEW
BLOCKS
BUT
THE
OLD
ONES
WILL
STILL
BE
OCCUPYING
SPACE
IN
PREVIOUSLY
WRITTEN
SEGMENTS
TO
DEAL
WITH
THIS
PROBLEM
LFS
HAS
A
CLEANER
THREAD
THAT
SPENDS
ITS
TIME
SCAN
NING
THE
LOG
CIRCULARLY
TO
COMPACT
IT
IT
STARTS
OUT
BY
READING
THE
SUMMARY
OF
THE
FIRST
SEGMENT
IN
THE
LOG
TO
SEE
WHICH
I
NODES
AND
FILES
ARE
THERE
IT
THEN
CHECKS
THE
CURRENT
I
NODE
MAP
TO
SEE
IF
THE
I
NODES
ARE
STILL
CURRENT
AND
FILE
BLOCKS
ARE
STILL
IN
USE
IF
NOT
THAT
INFORMATION
IS
DISCARDED
THE
I
NODES
AND
BLOCKS
THAT
ARE
STILL
IN
USE
GO
INTO
MEMORY
TO
BE
WRITTEN
OUT
IN
THE
NEXT
SEGMENT
THE
ORIGINAL
SEGMENT
IS
THEN
MARKED
AS
FREE
SO
THAT
THE
LOG
CAN
USE
IT
FOR
NEW
DATA
IN
THIS
MANNER
THE
CLEANER
MOVES
ALONG
THE
LOG
REMOVING
OLD
SEGMENTS
FROM
THE
BACK
AND
PUTTING
ANY
LIVE
DATA
INTO
MEMORY
FOR
REWRITING
IN
THE
NEXT
SEGMENT
CONSEQUENTLY
THE
DISK
IS
A
BIG
CIRCULAR
BUFFER
WITH
THE
WRITER
THREAD
ADDING
NEW
SEGMENTS
TO
THE
FRONT
AND
THE
CLEANER
THREAD
REMOVING
OLD
ONES
FROM
THE
BACK
THE
BOOKKEEPING
HERE
IS
NONTRIVIAL
SINCE
WHEN
A
FILE
BLOCK
IS
WRITTEN
BACK
TO
A
NEW
SEGMENT
THE
I
NODE
OF
THE
FILE
SOMEWHERE
IN
THE
LOG
MUST
BE
LOCATED
UPDATED
AND
PUT
INTO
MEMORY
TO
BE
WRITTEN
OUT
IN
THE
NEXT
SEGMENT
THE
I
NODE
MAP
MUST
THEN
BE
UPDATED
TO
POINT
TO
THE
NEW
COPY
NEVERTHELESS
IT
IS
POSSIBLE
TO
DO
THE
ADMINISTRATION
AND
THE
PERFORMANCE
RESULTS
SHOW
THAT
ALL
THIS
COMPLEXITY
IS
SEC
FILE
SYSTEM
IMPLEMENTATION
WORTHWHILE
MEASUREMENTS
GIVEN
IN
THE
PAPERS
CITED
ABOVE
SHOW
THAT
LFS
OUTPER
FORMS
UNIX
BY
AN
ORDER
OF
MAGNITUDE
ON
SMALL
WRITES
WHILE
HAVING
A
PER
FORMANCE
THAT
IS
AS
GOOD
AS
OR
BETTER
THAN
UNIX
FOR
READS
AND
LARGE
WRITES
JOURNALING
FILE
SYSTEMS
WHILE
LOG
STRUCTURED
FILE
SYSTEMS
ARE
AN
INTERESTING
IDEA
THEY
ARE
NOT
WIDELY
USED
IN
PART
DUE
TO
THEIR
BEING
HIGHLY
INCOMPATIBLE
WITH
EXISTING
FILE
SYSTEMS
NEVERTHELESS
ONE
OF
THE
IDEAS
INHERENT
IN
THEM
ROBUSTNESS
IN
THE
FACE
OF
FAILURE
CAN
BE
EASILY
APPLIED
TO
MORE
CONVENTIONAL
FILE
SYSTEMS
THE
BASIC
IDEA
HERE
IS
TO
KEEP
A
LOG
OF
WHAT
THE
FILE
SYSTEM
IS
GOING
TO
DO
BEFORE
IT
DOES
IT
SO
THAT
IF
THE
SYS
TEM
CRASHES
BEFORE
IT
CAN
DO
ITS
PLANNED
WORK
UPON
REBOOTING
THE
SYSTEM
CAN
LOOK
IN
THE
LOG
TO
SEE
WHAT
WAS
GOING
ON
AT
THE
TIME
OF
THE
CRASH
AND
FINISH
THE
JOB
SUCH
FILE
SYSTEMS
CALLED
JOURNALING
FILE
SYSTEMS
ARE
ACTUALLY
IN
USE
MICROSOFT
NTFS
FILE
SYSTEM
AND
THE
LINUX
AND
REISERFS
FILE
SYSTEMS
USE
JOURNALING
BELOW
WE
WILL
GIVE
A
BRIEF
INTRODUCTION
TO
THIS
TOPIC
TO
SEE
THE
NATURE
OF
THE
PROBLEM
CONSIDER
A
SIMPLE
GARDEN
VARIETY
OPERATION
THAT
HAPPENS
ALL
THE
TIME
REMOVING
A
FILE
THIS
OPERATION
IN
UNIX
REQUIRES
THREE
STEPS
REMOVE
THE
FILE
FROM
ITS
DIRECTORY
RELEASE
THE
I
NODE
TO
THE
POOL
OF
FREE
I
NODES
RETURN
ALL
THE
DISK
BLOCKS
TO
THE
POOL
OF
FREE
DISK
BLOCKS
IN
WINDOWS
ANALOGOUS
STEPS
ARE
REQUIRED
IN
THE
ABSENCE
OF
SYSTEM
CRASHES
THE
ORDER
IN
WHICH
THESE
STEPS
ARE
TAKEN
DOES
NOT
MATTER
IN
THE
PRESENCE
OF
CRASHES
IT
DOES
SUPPOSE
THAT
THE
FIRST
STEP
IS
COMPLETED
AND
THEN
THE
SYSTEM
CRASHES
THE
I
NODE
AND
FILE
BLOCKS
WILL
NOT
BE
ACCESSIBLE
FROM
ANY
FILE
BUT
WILL
ALSO
NOT
BE
AVAILABLE
FOR
REASSIGNMENT
THEY
ARE
JUST
OFF
IN
LIMBO
SOMEWHERE
DECREASING
THE
AVAILABLE
RESOURCES
IF
THE
CRASH
OCCURS
AFTER
THE
SECOND
STEP
ONLY
THE
BLOCKS
ARE
LOST
IF
THE
ORDER
OF
OPERATIONS
IS
CHANGED
AND
THE
I
NODE
IS
RELEASED
FIRST
THEN
AFTER
REBOOTING
THE
I
NODE
MAY
BE
REASSIGNED
BUT
THE
OLD
DIRECTORY
ENTRY
WILL
CONTINUE
TO
POINT
TO
IT
HENCE
TO
THE
WRONG
FILE
IF
THE
BLOCKS
ARE
RELEASED
FIRST
THEN
A
CRASH
BEFORE
THE
I
NODE
IS
CLEARED
WILL
MEAN
THAT
A
VALID
DIRECTORY
ENTRY
POINTS
TO
AN
I
NODE
LISTING
BLOCKS
NOW
IN
THE
FREE
STORAGE
POOL
AND
WHICH
ARE
LIKELY
TO
BE
REUSED
SHORTLY
LEADING
TO
TWO
OR
MORE
FILES
RANDOMLY
SHARING
THE
SAME
BLOCKS
NONE
OF
THESE
OUTCOMES
ARE
GOOD
WHAT
THE
JOURNALING
FILE
SYSTEM
DOES
IS
FIRST
WRITE
A
LOG
ENTRY
LISTING
THE
THREE
ACTIONS
TO
BE
COMPLETED
THE
LOG
ENTRY
IS
THEN
WRITTEN
TO
DISK
AND
FOR
GOOD
MEAS
URE
POSSIBLY
READ
BACK
FROM
THE
DISK
TO
VERIFY
ITS
INTEGRITY
ONLY
AFTER
THE
LOG
ENTRY
HAS
BEEN
WRITTEN
DO
THE
VARIOUS
OPERATIONS
BEGIN
AFTER
THE
OPERATIONS
FILE
SYSTEMS
CHAP
COMPLETE
SUCCESSFULLY
THE
LOG
ENTRY
IS
ERASED
IF
THE
SYSTEM
NOW
CRASHES
UPON
RE
COVERY
THE
FDE
SYSTEM
CAN
CHECK
THE
LOG
TO
SEE
IF
ANY
OPERATIONS
WERE
PENDING
IF
SO
ALL
OF
THEM
CAN
BE
RERUN
MULTIPLE
TIMES
IN
THE
EVENT
OF
REPEATED
CRASHES
UNTIL
THE
FILE
IS
CORRECTLY
REMOVED
TO
MAKE
JOURNALING
WORK
THE
LOGGED
OPERATIONS
MUST
BE
IDEMPOTENT
WHICH
MEANS
THEY
CAN
BE
REPEATED
AS
OFTEN
AS
NECESSARY
WITHOUT
HARM
OPERATIONS
SUCH
AS
UPDATE
THE
BITMAP
TO
MARK
I
NODE
K
OR
BLOCK
N
AS
FREE
CAN
BE
REPEATED
UNTIL
THE
COWS
COME
HOME
WITH
NO
DANGER
SIMILARLY
SEARCHING
A
DIRECTORY
AND
REMOV
ING
ANY
ENTRY
CALLED
FOOBAR
IS
ALSO
IDEMPOTENT
ON
THE
OTHER
HAND
ADDING
THE
NEWLY
FREED
BLOCKS
FROM
I
NODE
K
TO
THE
END
OF
THE
FREE
LIST
IS
NOT
IDEMPOTENT
SINCE
THEY
MAY
ALREADY
BE
THERE
THE
MORE
EXPENSIVE
OPERATION
SEARCH
THE
LIST
OF
FREE
BLOCKS
AND
ADD
BLOCK
N
TO
IT
IF
IT
IS
NOT
ALREADY
PRESENT
IS
IDEMPOTENT
JOURNALING
FDE
SYSTEMS
HAVE
TO
ARRANGE
THEIR
DATA
STRUCTURES
AND
LOGGABLE
OPERATIONS
SO
THEY
ALL
OF
THEM
ARE
IDEMPOTENT
UNDER
THESE
CONDITIONS
CRASH
RECOVERY
CAN
BE
MADE
FAST
AND
SECURE
FOR
ADDED
RELIABILITY
A
FILE
SYSTEM
CAN
INTRODUCE
THE
DATABASE
CONCEPT
OF
AN
ATOMIC
TRANSACTION
WHEN
THIS
CONCEPT
IS
USED
A
GROUP
OF
ACTIONS
CAN
BE
BRACK
ETED
BY
THE
BEGIN
TRANSACTION
AND
END
TRANSACTION
OPERATIONS
THE
FDE
SYSTEM
THEN
KNOWS
IT
MUST
COMPLETE
EITHER
ALL
THE
BRACKETED
OPERATIONS
OR
NONE
OF
THEM
BUT
NOT
ANY
OTHER
COMBINATIONS
SEC
FILE
SYSTEM
IMPLEMENTATION
TEMPORARILY
MOUNTED
ON
MM
FROM
THE
USER
POINT
OF
VIEW
THERE
IS
A
SINGLE
FILE
SYSTEM
HIERARCHY
THAT
IT
HAPPENS
TO
ENCOMPASS
MULTIPLE
INCOMPATIBLE
FILE
SYS
TEMS
IS
NOT
VISIBLE
TO
USERS
OR
PROCESSES
HOWEVER
THE
PRESENCE
OF
MULTIPLE
FILE
SYSTEMS
IS
VERY
DEFINITELY
VISIBLE
TO
THE
IMPLEMENTATION
AND
SINCE
THE
PIONEERING
WORK
OF
SUN
MICROSYSTEMS
KLEIMAN
MOST
UNIX
SYSTEMS
HAVE
USED
THE
CONCEPT
OF
A
VFS
VIRTUAL
FILE
SYSTEM
TO
TRY
TO
INTEGRATE
MULTIPLE
FILE
SYSTEMS
INTO
AN
ORDERLY
STRUCTURE
THE
KEY
IDEA
IS
TO
ABSTRACT
OUT
THAT
PART
OF
THE
FILE
SYSTEM
THAT
IS
COMMON
TO
ALL
FILE
SYSTEMS
AND
PUT
THAT
CODE
IN
A
SEPARATE
LAYER
THAT
CALLS
THE
UNDERLYING
CONCRETE
FILE
SYSTEMS
TO
AC
TUAL
MANAGE
THE
DATA
THE
OVERALL
STRUCTURE
IS
ILLUSTRATED
IN
FIG
THE
DIS
CUSSION
BELOW
IS
NOT
SPECIFIC
TO
LINUX
OR
FREEBSD
OR
ANY
OTHER
VERSION
OF
UNIX
BUT
GIVES
THE
GENERAL
FLAVOR
OF
HOW
VIRTUAL
FILE
SYSTEMS
WORK
IN
UNIX
SYSTEMS
I
I
I
VF
INTERFACE
NTFS
HAS
AN
EXTENSIVE
JOURNALING
SYSTEM
AND
ITS
STRUCTURE
IS
RARELY
CORRUPTED
BY
SYSTEM
CRASHES
IT
HAS
BEEN
IN
DEVELOPMENT
SINCE
ITS
FIRST
RELEASE
WITH
WIN
DOWS
NT
IN
THE
FIRST
LINUX
FILE
SYSTEM
TO
DO
JOURNALING
WAS
REISERFS
BUT
ITS
POPULARITY
WAS
IMPEDED
BY
THE
FACT
THAT
IT
WAS
INCOMPATIBLE
WITH
THE
THEN
STAN
DARD
FILE
SYSTEM
IN
CONTRAST
WHICH
IS
A
LESS
AMBITIOUS
PROJECT
THAN
REISERFS
ALSO
DOES
JOURNALING
WHILE
MAINTAINING
COMPATIBILITY
WITH
THE
PREVIOUS
SYSTEM
FILE
SYSTE
M
T
T
BUFFER
CACH
E
FIGURE
POSITION
OF
THE
VIRTUAL
FILE
SYSTEM
VIRTUAL
FILE
SYSTEMS
MANY
DIFFERENT
FILE
SYSTEMS
ARE
IN
USE
OFTEN
ON
THE
SAME
COMPUTER
EVEN
FOR
THE
SAME
OPERATING
SYSTEM
A
WINDOWS
SYSTEM
MAY
HAVE
A
MAIN
NTFS
FILE
SYSTEM
BUT
ALSO
A
LEGACY
FAT
OR
FAT
DRIVE
OR
PARTITION
THAT
CONTAINS
OLD
BUT
STILL
NEEDED
DATA
AND
FROM
TIME
TO
TIME
A
CD
ROM
OR
DVD
EACH
WITH
ITS
OWN
UNIQUE
FILE
SYSTEM
MAY
BE
REQUIRED
AS
WELL
WINDOWS
HANDLES
THESE
DISPARATE
FILE
SYSTEMS
BY
IDENTIFYING
EACH
ONE
WITH
A
DIFFERENT
DRIVE
LETTER
AS
IN
C
D
ETC
WHEN
A
PROCESS
OPENS
A
FILE
THE
DRIVE
LETTER
IS
EXPLICITLY
OR
IMPLICITLY
PRESENT
SO
WINDOWS
KNOWS
WHICH
FILE
SYSTEM
TO
PASS
THE
REQUEST
TO
THERE
IS
NO
ATTEMPT
TO
INTEGRATE
HETEROGENEOUS
FILE
SYSTEMS
INTO
A
UNIFIED
WHOLE
IN
CONTRAST
ALL
MODERN
UNIX
SYSTEMS
MAKE
A
VERY
SERIOUS
ATTEMPT
TO
INTEGRATE
MULTIPLE
FILE
SYSTEMS
INTO
A
SINGLE
STRUCTURE
A
LINUX
SYSTEM
COULD
HAVE
AS
THE
ROOT
FILE
SYSTEM
WITH
AN
PARTITION
MOUNTED
ON
USR
AND
A
SECOND
HARD
DISK
WITH
A
REISERFS
FILE
SYSTEM
MOUNTED
ON
HOME
AS
WELL
AS
AN
ISO
CD
ROM
ALL
SYSTEM
CALLS
RELATING
TO
FILES
ARE
DIRECTED
TO
THE
VIRTUAL
FILE
SYSTEM
FOR
INI
TIAL
PROCESSING
THESE
CALLS
COMING
FROM
USER
PROCESSES
ARE
THE
STANDARD
POSIX
CALLS
SUCH
AS
OPEN
READ
WRITE
ISEEK
AND
SO
ON
THUS
THE
VFS
HAS
AN
UPPER
INTERFACE
TO
USER
PROCESSES
AND
IT
IS
THE
WELL
KNOWN
POSIX
INTERFACE
THE
VFS
ALSO
HAS
A
LOWER
INTERFACE
TO
THE
CONCRETE
FILE
SYSTEMS
WHICH
IS
LABELED
VFS
INTERFACE
IN
FIG
THIS
INTERFACE
CONSISTS
OF
SEVERAL
DOZEN
FUNC
TION
CALLS
THAT
THE
VFS
CAN
MAKE
TO
EACH
FILE
SYSTEM
TO
GET
WORK
DONE
THUS
TO
CREATE
A
NEW
FILE
SYSTEM
THAT
WORKS
WITH
THE
VFS
THE
DESIGNERS
OF
THE
NEW
FILE
SYSTEM
MUST
MAKE
SURE
THAT
IT
SUPPLIES
THE
FUNCTION
CALLS
THE
VFS
REQUIRES
AN
OBVIOUS
EXAMPLE
OF
SUCH
A
FUNCTION
IS
ONE
THAT
READS
A
SPECIFIC
BLOCK
FROM
DISK
PUTS
IT
IN
THE
FILE
SYSTEM
BUFFER
CACHE
AND
RETURNS
A
POINTER
TO
IT
THUS
THE
VFS
HAS
TWO
DISTINCT
INTERFACES
THE
UPPER
ONE
TO
THE
USER
PROCESSES
AND
THE
LOWER
ONE
TO
THE
CONCRETE
FILE
SYSTEMS
WHILE
MOST
OF
DIE
FILE
SYSTEMS
UNDER
THE
VFS
REPRESENT
PARTITIONS
ON
A
LOCAL
DISK
THIS
IS
NOT
ALWAYS
THE
CASE
IN
FACT
THE
ORIGINAL
MOTIVATION
FOR
SUN
TO
BUILD
FILE
SYSTEMS
CHAP
THE
VFS
WAS
TO
SUPPORT
REMOTE
FILE
SYSTEMS
USING
THE
NFS
NETWORK
FILE
SYS
TEM
PROTOCOL
THE
VFS
DESIGN
IS
SUCH
THAT
AS
LONG
AS
THE
CONCRETE
FILE
SYSTEM
SUPPLIES
THE
FUNCTIONS
THE
VFS
REQUIRES
THE
VFS
DOES
NOT
KNOW
OR
CARE
WHERE
THE
DATA
ARE
STORED
OR
WHAT
THE
UNDERLYING
FILE
SYSTEM
IS
LIKE
INTERNALLY
MOST
VFS
IMPLEMENTATIONS
ARE
ESSENTIALLY
OBJECT
ORIENTED
EVEN
IF
THEY
ARE
WRITTEN
IN
C
RATHER
THAN
C
THERE
ARE
SEVERAL
KEY
OBJECT
TYPES
THAT
ARE
NORMALLY
SUPPORTED
THESE
INCLUDE
THE
SUPERBLOCK
WHICH
DESCRIBES
A
FILE
SYSTEM
THE
V
NODE
WHICH
DESCRIBES
A
FILE
AND
THE
DIRECTORY
WHICH
DESCRIBES
A
FILE
SYS
TEM
DIRECTORY
EACH
OF
THESE
HAS
ASSOCIATED
OPERATIONS
METHODS
THAT
THE
CON
CRETE
FILE
SYSTEMS
MUST
SUPPORT
IN
ADDITION
THE
VFS
HAS
SOME
INTERNAL
DATA
STRUC
TURES
FOR
ITS
OWN
USE
INCLUDING
THE
MOUNT
TABLE
AND
AN
ARRAY
OF
FILE
DESCRIPTORS
TO
KEEP
TRACK
OF
ALL
THE
OPEN
FILES
IN
THE
USER
PROCESSES
TO
UNDERSTAND
HOW
THE
VFS
WORKS
LET
US
RUN
THROUGH
AN
EXAMPLE
CHRONOLOGI
CALLY
WHEN
THE
SYSTEM
IS
BOOTED
THE
ROOT
FILE
SYSTEM
IS
REGISTERED
WITH
THE
VFS
IN
ADDITION
WHEN
OTHER
FILE
SYSTEMS
ARE
MOUNTED
EITHER
AT
BOOT
TIME
OR
DURING
OP
ERATION
THEY
TOO
MUST
REGISTER
WITH
THE
VFS
WHEN
A
FILE
SYSTEM
REGISTERS
WHAT
IT
BASICALLY
DOES
IS
PROVIDE
A
LIST
OF
THE
ADDRESSES
OF
THE
FUNCTIONS
THE
VFS
RE
QUIRES
EITHER
AS
ONE
LONG
CALL
VECTOR
TABLE
OR
AS
SEVERAL
OF
THEM
ONE
PER
VFS
OBJECT
AS
THE
VFS
DEMANDS
THUS
ONCE
A
FILE
SYSTEM
HAS
REGISTERED
WITH
THE
VFS
THE
VFS
KNOWS
HOW
TO
SAY
READ
A
BLOCK
FROM
IT
IT
SIMPLY
DALLS
THE
FOURTH
OR
WHATEVER
FUNCTION
IN
THE
VECTOR
SUPPLIED
BY
THE
FILE
SYSTEM
SIMILARLY
THE
VFS
THEN
ALSO
KNOWS
HOW
TO
CARRY
OUT
EVERY
OTHER
FUNCTION
THE
CONCRETE
FILE
SYSTEM
MUST
SUPPLY
IT
JUST
CALLS
THE
FUNCTION
WHOSE
ADDRESS
WAS
SUPPLIED
WHEN
THE
FILE
SYSTEM
REGISTERED
AFTER
A
FILE
SYSTEM
HAS
BEEN
MOUNTED
IT
CAN
BE
USED
FOR
EXAMPLE
IF
A
FILE
SYSTEM
HAS
BEEN
MOUNTED
ON
USR
AND
A
PROCESS
MAKES
THE
CALL
OPEN
INCIUDE
UNISTD
H
CLRDONLY
WHILE
PARSING
THE
PATH
THE
VFS
SEES
THAT
A
NEW
FILE
SYSTEM
HAS
BEEN
MOUNTED
ON
USR
AND
LOCATES
ITS
SUPERBLOCK
BY
SEARCHING
THE
LIST
OF
SUPERBLOCKS
OF
MOUNTED
FILE
SYSTEMS
HAVING
DONE
THIS
IT
CAN
FIND
THE
ROOT
DIRECTORY
OF
THE
MOUNTED
FILE
SYS
TEM
AND
LOOK
UP
THE
PATH
INCLUDE
UNISTDH
THERE
THE
VFS
THEN
CREATES
A
V
NODE
AND
MAKES
A
CALL
TO
THE
CONCRETE
FILE
SYSTEM
TO
RETURN
ALL
THE
INFORMATION
IN
THE
FILE
I
NODE
THIS
INFORMATION
IS
COPIED
INTO
THE
V
NODE
IN
RAM
ALONG
WITH
OTHER
INFORMATION
MOST
IMPORTANTLY
THE
POINTER
TO
THE
TABLE
OF
FUNCTIONS
TO
CALL
FOR
OPERATIONS
ON
V
NODES
SUCH
AS
READ
WRITE
CLOSE
AND
SO
ON
AFTER
THE
V
NODE
HAS
BEEN
CREATED
THE
VFS
MAKES
AN
ENTRY
IN
THE
FILE
DESCRIP
TOR
TABLE
FOR
THE
CALLING
PROCESS
AND
SETS
IT
TO
POINT
TO
THE
NEW
V
NODE
FOR
THE
PURISTS
THE
FILE
DESCRIPTOR
ACTUALLY
POINTS
TO
ANOTHER
DATA
STRUCTURE
THAT
CONTAINS
THE
CURRENT
FILE
POSITION
AND
A
POINTER
TO
THE
V
NODE
BUT
THIS
DETAIL
IS
NOT
IMPORTANT
FOR
OUR
PURPOSES
HERE
FINALLY
THE
VFS
RETURNS
THE
FILE
DESCRIPTOR
TO
THE
CALLER
SO
IT
CAN
USE
IT
TO
READ
WRITE
AND
CLOSE
THE
FILE
SEC
FILE
SYSTEM
IMPLEMENTATION
LATER
WHEN
THE
PROCESS
DOES
A
READ
USING
THE
FILE
DESCRIPTOR
THE
VFS
LOCATES
THE
V
NODE
FROM
THE
PROCESS
AND
FILE
DESCRIPTOR
TABLES
AND
FOLLOWS
THE
POINTER
TO
THE
TABLE
OF
FUNCTIONS
ALL
OF
WHICH
ARE
ADDRESSES
WITHIN
THE
CONCRETE
FILE
SYSTEM
ON
WHICH
THE
REQUESTED
FILE
RESIDES
THE
FUNCTION
THAT
HANDLES
READ
IS
NOW
CALLED
AND
CODE
WITHIN
THE
CONCRETE
FILE
SYSTEM
GOES
AND
GETS
THE
REQUESTED
BLOCK
THE
VFS
HAS
NO
IDEA
WHETHER
THE
DATA
ARE
COMING
FROM
THE
LOCAL
DISK
A
REMOTE
FILE
SYSTEM
OVER
THE
NETWORK
A
CD
ROM
A
USB
STICK
OR
SOMETHING
DIFFERENT
THE
DATA
STRUCTURES
INVOLVED
ARE
SHOWN
IN
FIG
STARTING
WITH
THE
CALLER
PROCESS
NUMBER
AND
THE
FILE
DESCRIPTOR
SUCCESSIVELY
THE
V
NODE
READ
FUNCTION
POINTER
AND
ACCESS
FUNCTION
WITHIN
THE
CONCRETE
FILE
SYSTEM
ARE
LOCATED
J
F
C
I
M
P
L
I
F
I
E
D
V
I
E
W
O
F
T
H
E
STRUCTURES
AND
CODE
USED
B
Y
THE
VFS
AND
CONCRETE
FILE
SYSTEM
TO
DO
A
READ
TN
V
TT
J
B
E
C
O
M
E
R
E
L
C
I
V
E
I
Y
ST
GHTFBRWARD
TO
ADD
NEW
FILE
SYSTEMS
TO
MAKE
ONE
THE
DESIGNERS
FIRST
GET
A
LIST
OF
FUNCTION
CALLS
THE
VFS
EXPECTS
AS
THEN
WNTE
THEIR
FILE
SYSTEM
TO
PROVIDE
ALL
OF
THEM
ALTERNATIVELY
I
F
F
T
E
NEEDS
USUALLY
BY
MAKING
ONE
OR
MORE
NATIVE
CALLS
TO
THE
CONCRETE
FILE
SYSTEM
FILE
SYSTEMS
CHAP
FILE
SYSTEM
MANAGEMENT
AND
OPTIMIZATION
MAKING
THE
FILE
SYSTEM
WORK
IS
ONE
THING
MAKING
IT
WORK
EFFICIENTLY
AND
ROBUSTLY
IN
REAL
LIFE
IS
SOMETHING
QUITE
DIFFERENT
IN
THE
FOLLOWING
SECTIONS
WE
WILL
LOOK
AT
SOME
OF
THE
ISSUES
INVOLVED
IN
MANAGING
DISKS
DISK
SPACE
MANAGEMENT
FILES
ARE
NORMALLY
STORED
ON
DISK
SO
MANAGEMENT
OF
DISK
SPACE
IS
A
MAJOR
CONCERN
TO
FILE
SYSTEM
DESIGNERS
TWO
GENERAL
STRATEGIES
ARE
POSSIBLE
FOR
STORING
AN
N
BYTE
FILE
N
CONSECUTIVE
BYTES
OF
DISK
SPACE
ARE
ALLOCATED
OR
THE
FILE
IS
SPLIT
UP
INTO
A
NUMBER
OF
NOT
NECESSARILY
CONTIGUOUS
BLOCKS
THE
SAME
TRADE
OFF
IS
PRES
ENT
IN
MEMORY
MANAGEMENT
SYSTEMS
BETWEEN
PURE
SEGMENTATION
AND
PAGING
AS
WE
HAVE
SEEN
STORING
A
FILE
AS
A
CONTIGUOUS
SEQUENCE
OF
BYTES
HAS
THE
OB
VIOUS
PROBLEM
THAT
IF
A
FILE
GROWS
IT
WILL
PROBABLY
HAVE
TO
BE
MOVED
ON
THE
DISK
THE
SAME
PROBLEM
HOLDS
FOR
SEGMENTS
IN
MEMORY
EXCEPT
THAT
MOVING
A
SEGMENT
IN
MEMORY
IS
A
RELATIVELY
FAST
OPERATION
COMPARED
TO
MOVING
A
FILE
FROM
ONE
DISK
POSITION
TO
ANOTHER
FOR
THIS
REASON
NEARLY
ALL
FILE
SYSTEMS
CHOP
FILES
UP
INTO
FIXED
SIZE
BLOCKS
THAT
NEED
NOT
BE
ADJACENT
BLOCK
SIZE
ONCE
IT
HAS
BEEN
DECIDED
TO
STORE
FILES
IN
FIXED
SIZE
BLOCKS
THE
QUESTION
ARISES
OF
HOW
BIG
THE
BLOCK
SHOULD
BE
GIVEN
THE
WAY
DISKS
ARE
ORGANIZED
THE
SECTOR
THE
TRACK
AND
THE
CYLINDER
ARE
OBVIOUS
CANDIDATES
FOR
THE
UNIT
OF
ALLOCATION
ALTHOUGH
THESE
ARE
ALL
DEVICE
DEPENDENT
WHICH
IS
A
MINUS
IN
A
PAGING
SYSTEM
THE
PAGE
SIZE
IS
ALSO
A
MAJOR
CONTENDER
HAVING
A
LARGE
BLOCK
SIZE
MEANS
THAT
EVERY
FILE
EVEN
A
BYTE
FILE
TIES
UP
AN
ENTIRE
CYLINDER
IT
ALSO
MEANS
THAT
SMALL
FILES
WASTE
A
LARGE
AMOUNT
OF
DISK
SPACE
ON
THE
OTHER
HAND
A
SMALL
BLOCK
SIZE
MEANS
THAT
MOST
FILES
WILL
SPAN
MULTIPLE
BLOCKS
AND
THUS
NEED
MULTIPLE
SEEKS
AND
ROTATIONAL
DELAYS
TO
READ
THEM
REDUCING
PERFORMANCE
THUS
IF
THE
ALLOCATION
UNIT
IS
TOO
LARGE
WE
WASTE
SPACE
IF
IT
IS
TOO
SMALL
WE
WASTE
TIME
MAKING
A
GOOD
CHOICE
REQUIRES
HAVING
SOME
INFORMATION
ABOUT
THE
FILE
SIZE
DISTRIBUTION
TANENBAUM
ET
AL
STUDIED
THE
FILE
SIZE
DISTRIBUTION
IN
THE
COMPUTER
SCIENCE
DEPARTMENT
OF
A
LARGE
RESEARCH
UNIVERSITY
THE
VU
IN
AND
THEN
AGAIN
IN
AS
WELL
AS
ON
A
COMMERCIAL
WEB
SERVER
HOSTING
A
POLITICAL
WEBSITE
WWW
ELECTORAL
VOTE
COM
THE
RESULTS
ARE
SHOWN
IN
FIG
WHERE
FOR
EACH
POWER
OF
TWO
FILE
SIZE
THE
PERCENTAGE
OF
ALL
FILES
SMALLER
OR
EQUAL
TO
IT
IS
LIST
ED
FOR
EACH
OF
THE
THREE
DATA
SETS
FOR
EXAMPLE
IN
OF
ALL
FILES
AT
THE
VU
WERE
KB
OR
SMALLER
AND
OF
ALL
FILES
WERE
KB
OR
SMALLER
THE
MEDIAN
FILE
SIZE
WAS
BYTES
SOME
PEOPLE
MAY
FIND
THIS
SMALL
SIZE
SURPRISING
SEC
FILE
SYSTEM
MANAGEMEN
T
AN
D
OPTIMIZATION
FIGURE
PERCENTAGE
OF
FILES
SMALLER
THAN
A
GIVEN
SIZE
IN
BYTES
WHAT
CONCLUSIONS
CAN
WE
DRAW
FROM
THESE
DATA
FOR
ONE
THING
WITH
A
BLOCK
SIZE
OF
KB
ONLY
ABOUT
OF
ALL
FILES
FIT
IN
A
SINGLE
BLOCK
WHEREAS
WITH
A
KB
BLOCK
THE
PERCENTAGE
OF
FILES
THAT
FIT
IN
A
BLOCK
GOES
UP
TO
THE
RANGE
OTHER
DATA
IN
THE
PAPER
SHOW
THAT
WITH
A
KB
BLOCK
OF
THE
DISK
BLOCKS
ARE
USED
BY
THE
LARGEST
FILES
THIS
MEANS
THAT
WASTING
SOME
SPACE
AT
THE
END
OF
EACH
SMALL
FILE
HARDLY
MATTERS
BECAUSE
THE
DISK
IS
FILLED
UP
BY
A
SMALL
NUMBER
OF
LARGE
FILES
VIDEOS
AND
THE
TOTAL
AMOUNT
OF
SPACE
TAKEN
UP
BY
THE
SMALL
FILES
HARDLY
MATTERS
AT
ALL
EVEN
DOUBLING
THE
SPACE
THE
SMALLEST
OF
THE
FILES
TAKE
UP
WOULD
BE
BARELY
NOTICEABLE
ON
THE
OTHER
HAND
USING
A
SMALL
BLOCK
MEANS
THAT
EACH
FILE
WILL
CONSIST
OF
MANY
BLOCKS
READING
EACH
BLOCK
NORMALLY
REQUIRES
A
SEEK
AND
A
ROTATIONAL
DELAY
SO
READING
A
FILE
CONSISTING
OF
MANY
SMALL
BLOCKS
WILL
BE
SLOW
AS
AN
EXAMPLE
CONSIDER
A
DISK
WITH
MB
PER
TRACK
A
ROTATION
TIME
OF
MSEC
AND
AN
AVERAGE
SEEK
TIME
OF
MSEC
THE
TIME
IN
MILLISECONDS
TO
READ
A
BLOCK
OF
K
BYTES
IS
THEN
THE
SUM
OF
THE
SEEK
ROTATIONAL
DELAY
AND
TRANSFER
TIMES
X
THE
SOLID
CURVE
OF
FIG
SHOWS
THE
DATA
RATE
FOR
SUCH
A
DISK
AS
A
FUNCTION
OF
BLOCK
SIZE
TO
COMPUTE
THE
SPACE
EFFICIENCY
WE
NEED
TO
MAKE
AN
ASSUMPTION
ABOUT
THE
MEAN
FILE
SIZE
FOR
SIMPLICITY
LET
US
ASSUME
THAT
ALL
FILES
ARE
KB
AL
THOUGH
THIS
NUMBER
IS
SLIGHTLY
LARGER
THAN
THE
DATA
MEASURED
AT
THE
VU
STUDENTS
PROBABLY
HAVE
MORE
SMALL
FILES
THAN
WOULD
BE
PRESENT
IN
A
CORPORATE
DATA
CENTER
FILE
SYSTEMS
CHAP
SO
IT
MIGHT
BE
A
BETTER
GUESS
ON
THE
WHOLE
THE
DASHED
CURVE
OF
FIG
SHOWS
THE
SPACE
EFFICIENCY
AS
A
FUNCTION
OF
BLOCK
SIZE
FIGURE
THE
SOLID
CURVE
LEFT
HAND
SCALE
GIVES
IHE
DATA
RATE
OF
A
DISK
THE
DASHED
CURVE
RIGHT
HAND
SCALE
GIVES
THE
DISK
SPACE
EFFICIENCY
ALL
FILES
ARE
KB
SEC
FILE
SYSTEM
MANAGEMENT
AND
OPTIMIZATION
NEVERTHELESS
HE
OBSERVED
A
MEDIAN
SIZE
WEIGHTED
BY
USAGE
OF
FILES
JUST
READ
AT
KB
FILES
JUST
WRITTEN
AS
KB
AND
FILES
READ
AND
WRITTEN
AS
KB
GIVEN
THE
DIFFERENT
DATA
SETS
MEASUREMENT
TECHNIQUES
AND
THE
YEAR
THESE
RESULTS
ARE
CER
TAINLY
COMPATIBLE
WITH
THE
VU
RESULTS
KEEPING
TRACK
OF
FREE
BLOCKS
ONCE
A
BLOCK
SIZE
HAS
BEEN
CHOSEN
THE
NEXT
ISSUE
IS
HOW
TO
KEEP
TRACK
OF
FREE
BLOCKS
TWO
METHODS
ARE
WIDELY
USED
AS
SHOWN
IN
FIG
THE
FIRST
ONE
CON
SISTS
OF
USING
A
LINKED
LIST
OF
DISK
BLOCKS
WITH
EACH
BLOCK
HOLDING
AS
MANY
FREE
DISK
BLOCK
NUMBERS
AS
WILL
FIT
WITH
A
KB
BLOCK
AND
A
BIT
DISK
BLOCK
NUMBER
EACH
BLOCK
ON
THE
FREE
LIST
HOLDS
THE
NUMBERS
OF
FREE
BLOCKS
ONE
SLOT
IS
RE
QUIRED
FOR
THE
POINTER
TO
THE
NEXT
BLOCK
CONSIDER
A
GB
DISK
WHICH
HAS
ABOUT
MILLION
DISK
BLOCKS
TO
STORE
ALL
THESE
ADDRESS
AT
PER
BLOCK
REQUIRES
ABOUT
MILLION
BLOCKS
GENERALLY
FREE
BLOCKS
ARE
USED
TO
HOLD
THE
FREE
LIST
SO
THE
STORAGE
IS
ESSENTIALLY
FREE
FREE
DISK
BLOCKS
THE
TWO
CURVES
CAN
BE
UNDERSTOOD
AS
FOLLOWS
THE
ACCESS
TIME
FOR
A
BLOCK
IS
COMPLETELY
DOMINATED
BY
THE
SEEK
TIME
AND
ROTATIONAL
DELAY
SO
GIVEN
THAT
IT
IS
GOING
TO
COST
MSEC
TO
ACCESS
A
BLOCK
THE
MORE
DATA
THAT
ARE
FETCHED
THE
BETTER
HENCE
THE
DATA
RATE
GOES
UP
ALMOST
LINEARLY
WITH
BLOCK
SIZE
UNTIL
THE
TRANSFERS
TAKE
SO
LONG
THAT
THE
TRANSFER
TIME
BEGINS
TO
MATTER
NOW
CONSIDER
SPACE
EFFICIENCY
WITH
KB
FILES
AND
KB
KB
OR
KB
BLOCKS
FILES
USE
AND
BLOCK
RESPECTIVELY
WITH
NO
WASTAGE
WITH
AN
KB
BLOCK
AND
KB
FILES
THE
SPACE
EFFICIENCY
DROPS
TO
AND
WITH
A
KB
BLOCK
IT
IS
DOWN
TO
IN
REALITY
FEW
FILES
ARE
AN
EXACT
MULTIPLE
OF
THE
DISK
BLOCK
SIZE
SO
SOME
SPACE
IS
ALWAYS
WASTED
IN
THE
LAST
BLOCK
OF
A
FILE
WHAT
THE
CURVES
SHOW
HOWEVER
IS
THAT
PERFORMANCE
AND
SPACE
UTILIZATION
ARE
INHERENTLY
IN
CONFLICT
SMALL
BLOCKS
ARE
BAD
FOR
PERFORMANCE
BUT
GOOD
FOR
DISK
SPACE
UTILIZATION
FOR
THESE
DATA
NO
REASONABLE
COMPROMISE
IS
AVAILABLE
THE
SIZE
CLOSEST
TO
WHERE
THE
TWO
CURVES
CROSS
IS
KB
BUT
THE
DATA
RATE
IS
ONLY
MB
SEC
AND
THE
SPACE
EFFICIENCY
IS
ABOUT
NEITHER
OF
WHICH
IS
VERY
GOOD
HIS
TORICALLY
FILE
SYSTEMS
HAVE
CHOSEN
SIZES
IN
THE
KB
TO
KB
RANGE
BUT
WITH
DISKS
NOW
EXCEEDING
TB
IT
MIGHT
BE
BETTER
TO
INCREASE
THE
BLOCK
SIZE
TO
KB
AND
ACCEPT
THE
WASTED
DISK
SPACE
DISK
SPACE
IS
HARDLY
IN
SHORT
SUPPLY
ANY
MORE
IN
AN
EXPERIMENT
TO
SEE
IF
WINDOWS
NT
FILE
USAGE
WAS
APPRECIABLY
DIFFERENT
R
A
KB
DISK
BLOCK
CAN
HOLD
BIT
DISK
BLOCK
NUMBERS
A
A
BITMAP
B
FROM
UNIX
FILE
USAGE
VOGELS
MADE
MEASUREMENTS
ON
FILES
AT
CORNELL
UNIVERSITY
VOGELS
HE
OBSERVED
THAT
NT
FILE
USAGE
IS
MORE
COMPLICATED
THAN
ON
UNIX
HE
WROTE
WHEN
WE
TYPE
A
FEW
CHARACTERS
IN
THE
NOTEPAD
TEXT
EDITOR
SAVING
THIS
TO
A
FILE
WILL
TRIGGER
SYSTEM
CALLS
INCLUDING
FAILED
OPEN
ATTEMPTS
FILE
OVERWRITE
AND
ADDITIONAL
OPEN
AND
CLOSE
SEQUENCES
FIGURE
A
STORING
THE
FREE
LIST
ON
A
LINKED
LIST
B
A
BITMAP
THE
OTHER
FREE
SPACE
MANAGEMENT
TECHNIQUE
IS
THE
BITMAP
A
DISK
WITH
N
BLOCKS
REQUIRES
A
BITMAP
WITH
N
BITS
FREE
BLOCKS
ARE
REPRESENTED
BY
IS
IN
THE
MAP
ALLOCATED
BLOCKS
BY
OR
VICE
VERSA
FOR
OUR
EXAMPLE
GB
DISK
WE
NEED
MILLION
BITS
FOR
THE
MAP
WHICH
REQUIRES
JUST
UNDER
KB
BLOCKS
TO
FILE
SYSTEMS
CHAP
STORE
IT
IS
NOT
SURPRISING
THAT
THE
BITMAP
REQUIRES
LESS
SPACE
SINCE
IT
USES
BIT
PER
BLOCK
VERSUS
BITS
IN
THE
LINKED
LIST
MODEL
ONLY
IF
THE
DISK
IS
NEARLY
FULL
I
E
HAS
FEW
FREE
BLOCKS
WILL
THE
LINKED
LIST
SCHEME
REQUIRE
FEWER
BLOCKS
THAN
THE
BIT
MAP
IF
FREE
BLOCKS
TEND
TO
COME
IN
LONG
RUNS
OF
CONSECUTIVE
BLOCKS
THE
FREE
LIST
SYSTEM
CAN
BE
MODIFIED
TO
KEEP
TRACK
OF
RUNS
OF
BLOCKS
RATHER
THAN
SINGLE
BLOCKS
AN
OR
BIT
COUNT
COULD
BE
ASSOCIATED
WITH
EACH
BLOCK
GIVING
THE
NUMBER
OF
CONSECUTIVE
FREE
BLOCKS
IN
THE
BEST
CASE
A
BASICALLY
EMPTY
DISK
COULD
BE
REPRESENTED
BY
TWO
NUMBERS
THE
ADDRESS
OF
THE
FIRST
FREE
BLOCK
FOLLOWED
BY
THE
COUNT
OF
FREE
BLOCKS
ON
THE
OTHER
HAND
IF
THE
DISK
BECOMES
SEVERELY
FRAGMENTED
KEEPING
TRACK
OF
RUNS
IS
LESS
EFFICIENT
THAN
KEEPING
TRACK
OF
INDIVIDUAL
BLOCKS
BE
CAUSE
NOT
ONLY
MUST
THE
ADDRESS
BE
STORED
BUT
ALSO
THE
COUNT
THIS
ISSUE
ILLUSTRATES
A
PROBLEM
OPERATING
SYSTEM
DESIGNERS
OFTEN
HAVE
THERE
ARE
MULTIPLE
DATA
STRUCTURES
AND
ALGORITHMS
THAT
CAN
BE
USED
TO
SOLVE
A
PROBLEM
BUT
CHOOSING
THE
BEST
ONE
REQUIRES
DATA
THAT
THE
DESIGNERS
DO
NOT
HAVE
AND
WILL
NOT
HAVE
UNTIL
THE
SYSTEM
IS
DEPLOYED
AND
HEAVILY
USED
AND
EVEN
THEN
THE
DATA
MAY
NOT
BE
AVAILABLE
FOR
EXAMPLE
OUR
OWN
MEASUREMENTS
OF
FILE
SIZES
AT
THE
VU
IN
AND
THE
WEBSITE
DATA
AND
THE
CORNELL
DATA
ARE
ONLY
FOUR
SAMPLES
WHILE
A
LOT
BETTER
THAN
NOTHING
WE
HAVE
LITTLE
IDEA
IF
THEY
ARE
ALSO
REPRESENTATIVE
OF
HOME
COMPUTERS
CORPORATE
COMPUTERS
GOVERNMENT
COMPUTERS
AND
OTHERS
WITH
SOME
EFFORT
WE
MIGHT
HAVE
BEEN
ABLE
TO
GET
A
COUPLE
OF
SAMPLES
FROM
OTHER
KINDS
OF
COMPUTERS
BUT
EVEN
THEN
IT
WOULD
BE
FOOLISH
TO
EXTRAPOLATE
TO
AIL
COM
PUTERS
OF
THE
KIND
MEASURED
GETTING
BACK
TO
THE
FREE
LIST
METHOD
FOR
A
MOMENT
ONLY
ONE
BLOCK
OF
POINTERS
NEED
BE
KEPT
IN
MAIN
MEMORY
WHEN
A
FILE
IS
CREATED
THE
NEEDED
BLOCKS
ARE
TAKEN
FROM
THE
BLOCK
OF
POINTERS
WHEN
IT
RUNS
OUT
A
NEW
BLOCK
OF
POINTERS
IS
READ
IN
FROM
THE
DISK
SIMILARLY
WHEN
A
FILE
IS
DELETED
ITS
BLOCKS
ARE
FREED
AND
ADDED
TO
THE
BLOCK
OF
POINTERS
IN
MAIN
MEMORY
WHEN
THIS
BLOCK
FILLS
UP
IT
IS
WRITTEN
TO
DISK
UNDER
CERTAIN
CIRCUMSTANCES
THIS
METHOD
LEADS
TO
UNNECESSARY
DISK
I
O
CON
SIDER
THE
SITUATION
OF
FIG
A
IN
WHICH
THE
BLOCK
OF
POINTERS
IN
MEMORY
HAS
ROOM
FOR
ONLY
TWO
MORE
ENTRIES
IF
A
THREE
BLOCK
FILE
IS
FREED
THE
POINTER
BLOCK
OVERFLOWS
AND
HAS
TO
BE
WRITTEN
TO
DISK
LEADING
TO
THE
SITUATION
OF
FIG
B
IF
A
THREE
BLOCK
FILE
IS
NOW
WRITTEN
THE
FULL
BLOCK
OF
POINTERS
HAS
TO
BE
READ
IN
AGAIN
TAKING
US
BACK
TO
FIG
A
IF
THE
THREE
BLOCK
FILE
JUST
WRITTEN
WAS
A
TEMPORARY
FILE
WHEN
IT
IS
FREED
ANOTHER
DISK
WRITE
IS
NEEDED
TO
WRITE
THE
FULL
BLOCK
OF
POINT
ERS
BACK
TO
THE
DISK
IN
SHORT
WHEN
THE
BLOCK
OF
POINTERS
IS
ALMOST
EMPTY
A
SERIES
OF
SHORT
LIVED
TEMPORARY
FILES
CAN
CAUSE
A
LOT
OF
DISK
I
O
AN
ALTERNATIVE
APPROACH
THAT
AVOIDS
MOST
OF
THIS
DISK
I
O
IS
TO
SPLIT
THE
FULL
BLOCK
OF
POINTERS
THUS
INSTEAD
OF
GOING
FROM
FIG
A
TO
FIG
B
WE
GO
FROM
FIG
A
TO
FIG
C
WHEN
THREE
BLOCKS
ARE
FREED
NOW
THE
SYSTEM
CAN
HANDLE
A
SERIES
OF
TEMPORARY
FILES
WITHOUT
DOING
ANY
DISK
I
O
IF
THE
BLOCK
IN
MEMORY
FILLS
UP
IT
IS
WRITTEN
TO
THE
DISK
AND
THE
HALF
FULL
BLOCK
FROM
THE
DISK
IS
SEC
FILE
SYSTEM
MANAGEMENT
AND
OPTIMIZATION
A
B
FIGURE
A
AN
ALMOST
FULL
BLOCK
OF
POINTERS
TO
FREE
DISK
BLOCKS
IN
MEMORY
AND
THREE
BLOCKS
OF
POINTERS
ON
DISK
B
RESULT
OF
FREEING
A
THREE
BLOCK
FILE
C
AN
ALTERNATIVE
STRATEGY
FOR
HANDLING
THE
THREE
FREE
BLOCKS
THE
SHADED
ENTRIES
REPRESENT
POINTERS
TO
FREE
DISK
BLOCKS
READ
IN
THE
IDEA
HERE
IS
TO
KEEP
MOST
OF
THE
POINTER
BLOCKS
ON
DISK
FULL
TO
MINIM
IZE
DISK
USAGE
BUT
KEEP
THE
ONE
IN
MEMORY
ABOUT
HALF
FULL
SO
IT
CAN
HANDLE
BOTH
FILE
CREATION
AND
FILE
REMOVAL
WITHOUT
DISK
I
O
ON
THE
FREE
LIST
WITH
A
BITMAP
IT
IS
ALSO
POSSIBLE
TO
KEEP
JUST
ONE
BLOCK
IN
MEMORY
GOING
TO
DISK
FOR
ANOTHER
ONLY
WHEN
IT
BECOMES
FULL
OR
EMPTY
AN
ADDITIONAL
BENEFIT
OF
THIS
APPROACH
IS
THAT
BY
DOING
ALL
THE
ALLOCATION
FROM
A
SINGLE
BLOCK
OF
THE
BITMAP
THE
DISK
BLOCKS
WILL
BE
CLOSE
TOGETHER
THUS
MINIMIZING
DISK
ARM
MOTION
SINCE
THE
BIT
MAP
IS
A
FIXED
SIZE
DATA
STRUCTURE
IF
THE
KERNEL
IS
PARTIALLY
PAGED
THE
BITMAP
CAN
BE
PUT
IN
VIRTUAL
MEMORY
AND
HAVE
PAGES
OF
IT
PAGED
IN
AS
NEEDED
DISK
QUOTAS
TO
PREVENT
PEOPLE
FROM
HOGGING
TOO
MUCH
DISK
SPACE
MULTIUSER
OPERATING
SYSTEMS
OFTEN
PROVIDE
A
MECHANISM
FOR
ENFORCING
DISK
QUOTAS
THE
IDEA
IS
THAT
THE
SYSTEM
ADMINISTRATOR
ASSIGNS
EACH
USER
A
MAXIMUM
ALLOTMENT
OF
FILES
AND
BLOCKS
AND
THE
OPERATING
SYSTEM
MAKES
SURE
THAT
THE
USERS
DO
NOT
EXCEED
THEIR
QUOTAS
A
TYPICAL
MECHANISM
IS
DESCRIBED
BELOW
WHEN
A
USER
OPENS
A
FILE
THE
ATTRIBUTES
AND
DISK
ADDRESSES
ARE
LOCATED
AND
PUT
INTO
AN
OPEN
FILE
TABLE
IN
MAIN
MEMORY
AMONG
THE
ATTRIBUTES
IS
AN
ENTRY
TELLING
WHO
THE
OWNER
IS
ANY
INCREASES
IN
THE
FILE
SIZE
WILL
BE
CHARGED
TO
THE
OWNER
QUOTA
A
SECOND
TABLE
CONTAINS
THE
QUOTA
RECORD
FOR
EVERY
USER
WITH
A
CURRENTLY
OPEN
FILE
EVEN
IF
THE
FILE
WAS
OPENED
BY
SOMEONE
ELSE
THIS
TABLE
IS
SHOWN
IN
FIG
IT
IS
AN
EXTRACT
FROM
A
QUOTA
FILE
ON
DISK
FOR
THE
USERS
WHOSE
FILES
ARE
CURRENTLY
OPEN
WHEN
ALL
THE
FILES
ARE
CLOSED
THE
RECORD
IS
WRITTEN
BACK
TO
THE
QUOTA
FILE
FILE
SYSTEMS
CHAP
OPEN
FILE
TABLE
QUOTA
TABLE
SOFT
BLOCK
LIMIT
HARD
BLOCK
LIMIT
SEC
FILE
SYSTEM
MANAGEMENT
AND
OPTIMIZATION
EXCEPT
AT
UNIVERSITIES
WHERE
ISSUING
A
PURCHASE
ORDER
TAKES
THREE
COMMITTEES
FIVE
SIGNATURES
AND
DAYS
IF
A
COMPUTER
FILE
SYSTEM
IS
IRREVOCABLY
LOST
WHETHER
DUE
TO
HARDWARE
OR
SOFTWARE
RESTORING
ALL
THE
INFORMATION
WILL
BE
DIFFICULT
TIME
CONSUMING
AND
IN
MANY
CASES
IMPOSSIBLE
FOR
THE
PEOPLE
WHOSE
PROGRAMS
DOCUMENTS
TAX
RECORDS
CURRENT
OF
BLOCKS
BLOCK
WARNINGS
LEFT
SOFT
FILE
LIMIT
HARD
FILE
LIMIT
CURRENT
OF
FILES
FILE
WARNINGS
LEFT
QUOTA
V
RECORD
I
FOR
USER
L
CUSTOMER
FILES
DATABASES
MARKETING
PLANS
OR
OTHER
DATA
ARE
GONE
FOREVER
THE
CONSEQUENCES
CAN
BE
CATASTROPHIC
WHILE
THE
FILE
SYSTEM
CANNOT
OFFER
ANY
PROTEC
TION
AGAINST
PHYSICAL
DESTRUCTION
OF
THE
EQUIPMENT
AND
MEDIA
IT
CAN
HELP
PROTECT
THE
INFORMATION
IT
IS
PRETTY
STRAIGHTFORWARD
MAKE
BACKUPS
BUT
THAT
IS
NOT
QUITE
AS
SIMPLE
AS
IT
SOUNDS
LET
US
TAKE
A
LOOK
MOST
PEOPLE
DO
NOT
THINK
MAKING
BACKUPS
OF
THEIR
FILES
IS
WORTH
THE
TIME
AND
EFFORT
UNTIL
ONE
FINE
DAY
THEIR
DISK
ABRUPTLY
DIES
AT
WHICH
TIME
MOST
OF
THEM
UNDERGO
A
DEATHBED
CONVERSION
COMPANIES
HOWEVER
USUALLY
WELL
UNDERSTAND
THE
VALUE
OF
THEIR
DATA
AND
GENERALLY
DO
A
BACKUP
AT
LEAST
ONCE
A
DAY
USUALLY
TO
TAPE
MODERN
TAPES
HOLD
HUNDREDS
OF
GIGABYTES
AND
COST
PENNIES
PER
GIGABYTE
NEVERTHELESS
MAKING
BACKUPS
IS
NOT
QUITE
AS
TRIVIAL
AS
IT
SOUNDS
SO
WE
WILL
EXAM
FIGURE
QUOTAS
ARE
KEPT
TRACK
OF
ON
A
PER
USER
BASIS
IN
A
QUOTA
TABLE
WHEN
A
NEW
ENTRY
IS
MADE
IN
THE
OPEN
FILE
TABLE
A
POINTER
TO
THE
OWNER
QUOTA
RECORD
IS
ENTERED
INTO
IT
TO
MAKE
IT
EASY
TO
FIND
THE
VARIOUS
LIMITS
EVERY
TIME
A
BLOCK
IS
ADDED
TO
A
FILE
THE
TOTAL
NUMBER
OF
BLOCKS
CHARGED
TO
THE
OWNER
IS
INCREMENTED
AND
A
CHECK
IS
MADE
AGAINST
BOTH
THE
HARD
AND
SOFT
LIMITS
THE
SOFT
LIMIT
MAY
BE
EXCEEDED
BUT
THE
HARD
LIMIT
MAY
NOT
AN
ATTEMPT
TO
APPEND
TO
A
FILE
WHEN
THE
HARD
BLOCK
LIMIT
HAS
BEEN
REACHED
WILL
RESULT
IN
AN
ERROR
ANALOGOUS
CHECKS
ALSO
EXIST
FOR
THE
NUMBER
OF
FILES
WHEN
A
USER
ATTEMPTS
TO
LOG
IN
THE
SYSTEM
EXAMINES
THE
QUOTA
FILE
TO
SEE
IF
THE
USER
HAS
EXCEEDED
THE
SOFT
LIMIT
FOR
EITHER
NUMBER
OF
FILES
OR
NUMBER
OF
DISK
BLOCKS
IF
EITHER
LIMIT
HAS
BEEN
VIOLATED
A
WARNING
IS
DISPLAYED
AND
THE
COUNT
OF
WARNINGS
REMAINING
IS
REDUCED
BY
ONE
IF
THE
COUNT
EVER
GETS
TO
ZERO
THE
USER
HAS
IGNORED
THE
WARNING
ONE
TIME
TOO
MANY
AND
IS
NOT
PERMITTED
TO
LOG
IN
GETTING
PERMISSION
TO
LOG
IN
AGAIN
WILL
REQUIRE
SOME
DISCUSSION
WITH
THE
SYSTEM
ADMINIS
TRATOR
THIS
METHOD
HAS
THE
PROPERTY
THAT
USERS
MAY
GO
ABOVE
THEIR
SOFT
LIMITS
DURING
A
LOGIN
SESSION
PROVIDED
THEY
REMOVE
THE
EXCESS
BEFORE
LOGGING
OUT
THE
HARD
LIMITS
MAY
NEVER
BE
EXCEEDED
FILE
SYSTEM
BACKUPS
DESTRUCTION
OF
A
FILE
SYSTEM
IS
OFTEN
A
FAR
GREATER
DISASTER
THAN
DESTRUCTION
OF
A
COMPUTER
IF
A
COMPUTER
IS
DESTROYED
BY
FIRE
LIGHTNING
SURGES
OR
A
CUP
OF
COFFEE
POURED
ONTO
THE
KEYBOARD
IT
IS
ANNOYING
AND
WILL
COST
MONEY
BUT
GENERALLY
A
REPLACEMENT
CAN
BE
PURCHASED
WITH
A
MINIMUM
OF
FUSS
INEXPENSIVE
PERSONAL
COMPUTERS
CAN
EVEN
BE
REPLACED
WITHIN
AN
HOUR
BY
JUST
GOING
TO
A
COMPUTER
STORE
INE
SOME
OF
THE
RELATED
ISSUES
BELOW
BACKUPS
TO
TAPE
ARE
GENERALLY
MADE
TO
HANDLE
ONE
OF
TWO
POTENTIAL
PROBLEMS
RECOVER
FROM
DISASTER
RECOVER
FROM
STUPIDITY
THE
FIRST
ONE
COVERS
GETTING
THE
COMPUTER
RUNNING
AGAIN
AFTER
A
DISK
CRASH
FIRE
FLOOD
OR
OTHER
NATURAL
CATASTROPHE
IN
PRACTICE
THESE
THINGS
DO
NOT
HAPPEN
VERY
OFTEN
WHICH
IS
WHY
MANY
PEOPLE
DO
NOT
BOTHER
WITH
BACKUPS
THESE
PEOPLE
ALSO
TEND
NOT
TO
HAVE
FIRE
INSURANCE
ON
THEIR
HOUSES
FOR
THE
SAME
REASON
THE
SECOND
REASON
IS
THAT
USERS
OFTEN
ACCIDENTALLY
REMOVE
FILES
THAT
THEY
LATER
NEED
AGAIN
THIS
PROBLEM
OCCURS
SO
OFTEN
THAT
WHEN
A
FILE
IS
REMOVED
IN
WIN
DOWS
IT
IS
NOT
DELETED
AT
ALL
BUT
JUST
MOVED
TO
A
SPECIAL
DIRECTORY
THE
RECYCLE
BIN
SO
IT
CAN
BE
FISHED
OUT
AND
RESTORED
EASILY
LATER
BACKUPS
TAKE
THIS
PRINCIPLE
FURTHER
AND
ALLOW
FILES
THAT
WERE
REMOVED
DAYS
EVEN
WEEKS
AGO
TO
BE
RESTORED
FROM
OLD
BACKUP
TAPES
MAKING
A
BACKUP
TAKES
A
LONG
TIME
AND
OCCUPIES
A
LARGE
AMOUNT
OF
SPACE
SO
DOING
IT
EFFICIENTLY
AND
CONVENIENTLY
IS
IMPORTANT
THESE
CONSIDERATIONS
RAISE
THE
FOLLOWING
ISSUES
FIRST
SHOULD
THE
ENTIRE
FILE
SYSTEM
BE
BACKED
UP
OR
ONLY
PART
OF
IT
AT
MANY
INSTALLATIONS
THE
EXECUTABLE
BINARY
PROGRAMS
ARE
KEPT
IN
A
LIMITED
PART
OF
THE
FILE
SYSTEM
TREE
IT
IS
NOT
NECESSARY
TO
BACK
UP
THESE
FILES
IF
THEY
CAN
ALL
BE
REINSTALLED
FROM
THE
MANUFACTURER
CD
ROMS
ALSO
MOST
SYSTEMS
HAVE
A
DI
RECTORY
FOR
TEMPORARY
FILES
THERE
IS
USUALLY
NO
REASON
TO
BACK
IT
UP
EITHER
IN
UNDC
ALL
THE
SPECIAL
FILES
I
O
DEVICES
ARE
KEPT
IN
A
DIRECTORY
DEV
NOT
ONLY
IS
BACKING
UP
THIS
DIRECTORY
NOT
NECESSARY
IT
IS
DOWNRIGHT
DANGEROUS
BECAUSE
THE
BACKUP
PROGRAM
WOULD
HANG
FOREVER
IF
IT
TRIED
TO
READ
EACH
OF
THESE
TO
COMPLETION
IN
SHORT
IT
IS
USUALLY
DESIRABLE
TO
BACK
UP
ONLY
SPECIFIC
DIRECTORIES
AND
EVERYTHING
IN
THEM
RATHER
THAN
THE
ENTIRE
FILE
SYSTEM
FILE
SYSTEMS
CHAP
SECOND
IT
IS
WASTEFUL
TO
BACK
UP
FILES
THAT
HAVE
NOT
CHANGED
SINCE
THE
PREVIOUS
BACKUP
WHICH
LEADS
TO
THE
IDEA
OF
INCREMENTAL
DUMPS
THE
SIMPLEST
FORM
OF
IN
CREMENTAL
DUMPING
IS
TO
MAKE
A
COMPLETE
DUMP
BACKUP
PERIODICALLY
SAY
WEEKLY
OR
MONTHLY
AND
TO
MAKE
A
DAILY
DUMP
OF
ONLY
THOSE
FILES
THAT
HAVE
BEEN
MODIFIED
SINCE
THE
LAST
FULL
DUMP
EVEN
BETTER
IS
TO
DUMP
ONLY
THOSE
FILES
THAT
HAVE
CHANGED
SINCE
THEY
WERE
LAST
DUMPED
WHILE
THIS
SCHEME
MINIMIZES
DUMPING
TIME
IT
MAKES
RECOVERY
MORE
COMPLICATED
BECAUSE
FIRST
THE
MOST
RECENT
FULL
DUMP
HAS
TO
BE
RESTORED
FOLLOWED
BY
ALL
THE
INCREMENTAL
DUMPS
IN
REVERSE
ORDER
TO
EASE
RECOVERY
MORE
SOPHISTICATED
INCREMENTAL
DUMPING
SCHEMES
ARE
OFTEN
USED
THIRD
SINCE
IMMENSE
AMOUNTS
OF
DATA
ARE
TYPICALLY
DUMPED
IT
MAY
BE
DESIR
ABLE
TO
COMPRESS
THE
DATA
BEFORE
WRITING
THEM
TO
TAPE
HOWEVER
WITH
MANY
COM
PRESSION
ALGORITHMS
A
SINGLE
BAD
SPOT
ON
THE
BACKUP
TAPE
CAN
FOIL
THE
DECOMPRES
SION
ALGORITHM
AND
MAKE
AN
ENTIRE
FILE
OR
EVEN
AN
ENTIRE
TAPE
UNREADABLE
THUS
THE
DECISION
TO
COMPRESS
THE
BACKUP
STREAM
MUST
BE
CAREFULLY
CONSIDERED
FOURTH
IT
IS
DIFFICULT
TO
PERFORM
A
BACKUP
ON
AN
ACTIVE
FILE
SYSTEM
IF
FILES
AND
DIRECTORIES
ARE
BEING
ADDED
DELETED
AND
MODIFIED
DURING
THE
DUMPING
PROCESS
THE
RESULTING
DUMP
MAY
BE
INCONSISTENT
HOWEVER
SINCE
MAKING
A
DUMP
MAY
TAKE
HOURS
IT
MAY
BE
NECESSARY
TO
TAKE
THE
SYSTEM
OFFLINE
FOR
MUCH
OF
THE
NIGHT
TO
MAKE
THE
BACKUP
SOMETHING
THAT
IS
NOT
ALWAYS
ACCEPTABLE
FOR
THIS
REASON
ALGO
RITHMS
HAVE
BEEN
DEVISED
FOR
MAKING
RAPID
SNAPSHOTS
OF
THE
FILE
SYSTEM
STATE
BY
COPYING
CRITICAL
DATA
STRUCTURES
AND
THEN
REQUIRING
FUTURE
CHANGES
TO
FILES
AND
DI
RECTORIES
TO
COPY
THE
BLOCKS
INSTEAD
OF
UPDATING
THEM
IN
PLACE
HUTCHINSON
ET
AL
IN
THIS
WAY
THE
FILE
SYSTEM
IS
EFFECTIVELY
FROZEN
AT
THE
MOMENT
OF
THE
SNAPSHOT
SO
IT
CAN
BE
BACKED
UP
AT
LEISURE
AFTERWARD
FIFTH
AND
LAST
MAKING
BACKUPS
INTRODUCES
MANY
NONTECHNICAL
PROBLEMS
INTO
AN
ORGANIZATION
THE
BEST
ONLINE
SECURITY
SYSTEM
IN
THE
WORLD
MAY
BE
USELESS
IF
THE
SYSTEM
ADMINISTRATOR
KEEPS
ALL
THE
BACKUP
TAPES
IN
HIS
OFFICE
AND
LEAVES
IT
OPEN
AND
UNGUARDED
WHENEVER
HE
WALKS
DOWN
THE
HALL
TO
GET
OUTPUT
FROM
THE
PRINTER
ALL
A
SPY
HAS
TO
DO
IS
POP
IN
FOR
A
SECOND
PUT
ONE
TINY
TAPE
IN
HIS
POCKET
AND
SAUNTER
OFF
JAUNTILY
GOODBYE
SECURITY
ALSO
MAKING
A
DAILY
BACKUP
HAS
LITTLE
USE
IF
THE
FIRE
THAT
BUMS
DOWN
THE
COMPUTERS
ALSO
BURNS
UP
ALL
THE
BACKUP
TAPES
FOR
THIS
REASON
BACKUP
TAPES
SHOULD
BE
KEPT
OFF
SITE
BUT
THAT
INTRODUCES
MORE
SECURITY
RISKS
BECAUSE
NOW
TWO
SITES
MUST
BE
SECURED
FOR
A
THOROUGH
DISCUSSION
OF
THESE
AND
OTHER
PRACTICAL
ADMINISTRATION
ISSUES
SEE
NEMETH
ET
AL
BELOW
WE
WILL
DISCUSS
ONLY
THE
TECHNICAL
ISSUES
INVOLVED
IN
MAKING
FILE
SYSTEM
BACKUPS
TWO
STRATEGIES
CAN
BE
USED
FOR
DUMPING
A
DISK
TO
TAPE
A
PHYSICAL
DUMP
OR
A
LOGICAL
DUMP
A
PHYSICAL
DUMP
STARTS
AT
BLOCK
OF
THE
DISK
WRITES
ALL
THE
DISK
BLOCKS
ONTO
THE
OUTPUT
TAPE
IN
ORDER
AND
STOPS
WHEN
IT
HAS
COPIED
THE
LAST
ONE
SUCH
A
PROGRAM
IS
SO
SIMPLE
THAT
IT
CAN
PROBABLY
BE
MADE
BUG
FREE
SOME
THING
THAT
CAN
PROBABLY
NOT
BE
SAID
ABOUT
ANY
OTHER
USEFUL
PROGRAM
NEVERTHELESS
IT
IS
WORTH
MAKING
SEVERAL
COMMENTS
ABOUT
PHYSICAL
DUMPING
FOR
ONE
THING
THERE
IS
NO
VALUE
IN
BACKING
UP
UNUSED
DISK
BLOCKS
IF
THE
DUMPING
PROGRAM
CAN
OBTAIN
ACCESS
TO
THE
FREE
BLOCK
DATA
STRUCTURE
IT
CAN
AVOID
DUMPING
SEC
FILE
SYSTEM
MANAGEMENT
AND
OPTIMIZATION
UNUSED
BLOCKS
HOWEVER
SKIPPING
UNUSED
BLOCKS
REQUIRES
WRITING
THE
NUMBER
OF
EACH
BLOCK
IN
FRONT
OF
THE
BLOCK
OR
THE
EQUIVALENT
SINCE
IT
IS
NO
LONGER
TRUE
THAT
BLOCK
K
ON
THE
TAPE
WAS
BLOCK
K
ON
THE
DISK
A
SECOND
CONCERN
IS
DUMPING
BAD
BLOCKS
IT
IS
NEARLY
IMPOSSIBLE
TO
MANUFAC
TURE
LARGE
DISKS
WITHOUT
ANY
DEFECTS
SOME
BAD
BLOCKS
ARE
ALWAYS
PRESENT
SOME
TIMES
WHEN
A
LOW
LEVEL
FORMAT
IS
DONE
THE
BAD
BLOCKS
ARE
DETECTED
MARKED
AS
BAD
AND
REPLACED
BY
SPARE
BLOCKS
RESERVED
AT
THE
END
OF
EACH
TRACK
FOR
JUST
SUCH
EMERGENCIES
IN
MANY
CASES
THE
DISK
CONTROLLER
HANDLES
BAD
BLOCK
REPLACEMENT
TRANSPARENTLY
WITHOUT
THE
OPERATING
SYSTEM
EVEN
KNOWING
ABOUT
IT
HOWEVER
SOMETIMES
BLOCKS
GO
BAD
AFTER
FORMATTING
IN
WHICH
CASE
THE
OPERAT
ING
SYSTEM
WILL
EVENTUALLY
DETECT
THEM
USUALLY
IT
SOLVES
THE
PROBLEM
BY
CREATING
A
FILE
CONSISTING
OF
ALL
THE
BAD
BLOCKS
JUST
TO
MAKE
SURE
THEY
NEVER
APPEAR
IN
THE
FREE
BLOCK
POOL
AND
ARE
NEVER
ASSIGNED
NEEDLESS
TO
SAY
THIS
FILE
IS
COMPLETELY
UNREADABLE
IF
ALL
BAD
BLOCKS
ARE
REMAPPED
BY
THE
DISK
CONTROLLER
AND
HIDDEN
FROM
THE
OP
ERATING
SYSTEM
AS
JUST
DESCRIBED
PHYSICAL
DUMPING
WORKS
FINE
ON
THE
OTHER
HAND
IF
THEY
ARE
VISIBLE
TO
THE
OPERATING
SYSTEM
AND
MAINTAINED
IN
ONE
OR
MORE
BAD
BLOCK
FILES
OR
BITMAPS
IT
IS
ABSOLUTELY
ESSENTIAL
THAT
THE
PHYSICAL
DUMPING
PROGRAM
GET
ACCESS
TO
THIS
INFORMATION
AND
AVOID
DUMPING
THEM
TO
PREVENT
ENDLESS
DISK
READ
ERRORS
WHILE
TRYING
TO
BACK
UP
THE
BAD
BLOCK
FILE
THE
MAIN
ADVANTAGES
OF
PHYSICAL
DUMPING
ARE
SIMPLICITY
AND
GREAT
SPEED
BASICALLY
IT
CAN
RUN
AT
THE
SPEED
OF
THE
DISK
THE
MAIN
DISADVANTAGES
ARE
THE
INABILITY
TO
SKIP
SELECTED
DIRECTORIES
MAKE
INCREMENTAL
DUMPS
AND
RESTORE
INDIVI
DUAL
FILES
UPON
REQUEST
FOR
THESE
REASONS
MOST
INSTALLATIONS
MAKE
LOGICAL
DUMPS
A
LOGICAL
DUMP
STARTS
AT
ONE
OR
MORE
SPECIFIED
DIRECTORIES
AND
RECURSIVELY
DUMPS
ALL
FILES
AND
DIRECTORIES
FOUND
THERE
THAT
HAVE
CHANGED
SINCE
SOME
GIVEN
BASE
DATE
E
G
THE
LAST
BACKUP
FOR
AN
INCREMENTAL
DUMP
OR
SYSTEM
INSTALLATION
FOR
A
FULL
DUMP
THUS
IN
A
LOGICAL
DUMP
THE
DUMP
TAPE
GETS
A
SERIES
OF
CAREFULLY
IDENTIFIED
DIRECTORIES
AND
FILES
WHICH
MAKES
IT
EASY
TO
RESTORE
A
SPECIFIC
FILE
OR
DI
RECTORY
UPON
REQUEST
SINCE
LOGICAL
DUMPING
IS
THE
MOST
COMMON
FORM
LET
US
EXAMINE
A
COMMON
ALGORITHM
IN
DETAIL
USING
THE
EXAMPLE
OF
FIG
TO
GUIDE
US
MOST
UNIX
SYSTEMS
USE
THIS
ALGORITHM
IN
THE
FIGURE
WE
SEE
A
FILE
TREE
WITH
DIRECTORIES
SQUARES
AND
FILES
CIRCLES
THE
SHADED
ITEMS
HAVE
BEEN
MODIFIED
SINCE
THE
BASE
DATE
AND
THUS
NEED
TO
BE
DUMPED
THE
UNSHADED
ONES
DO
NOT
NEED
TO
BE
DUMPED
THIS
ALGORITHM
ALSO
DUMPS
ALL
DIRECTORIES
EVEN
UNMODIFIED
ONES
THAT
LIE
ON
THE
PATH
TO
A
MODIFIED
FILE
OR
DIRECTORY
FOR
TWO
REASONS
FIRST
TO
MAKE
IT
POSSIBLE
TO
RESTORE
THE
DUMPED
FILES
AND
DIRECTORIES
TO
A
FRESH
FILE
SYSTEM
ON
A
DIFFERENT
COMPUTER
IN
THIS
WAY
THE
DUMP
AND
RESTORE
PROGRAMS
CAN
BE
USED
TO
TRANSPORT
ENTIRE
FILE
SYSTEMS
BETWEEN
COMPUTERS
THE
SECOND
REASON
FOR
DUMPING
UNMODIFIED
DIRECTORIES
ABOVE
MODIFIED
FILES
IS
TO
MAKE
IT
POSSIBLE
TO
INCREMENTALLY
RESTORE
A
SINGLE
FILE
POSSIBLY
TO
HANDLE
RE
COVERY
FROM
STUPIDITY
SUPPOSE
THAT
A
FULL
FILE
SYSTEM
DUMP
IS
DONE
SUNDAY
FILE
SYSTEMS
CHAP
SEC
FILE
SYSTEM
MANAGEMENT
AND
OPTIMIZATION
THEMSELVES
HAVE
NOT
BEEN
MODIFIED
BECAUSE
THEY
WILL
BE
NEEDED
TO
RESTORE
TODAY
CHANGES
TO
A
FRESH
MACHINE
FOR
EFFICIENCY
PHASES
AND
CAN
BE
COMBINED
IN
ONE
TREE
WALK
B
I
A
Q
LPJL
LP
LFJLA
C
D
J
J
Y
J
M
FIGURE
BITMAPS
USED
BY
THE
LOGICAL
DUMPING
ALGORITHM
A
A
FILE
SYSTEM
TO
BE
DUMPED
THE
SQUARES
ARE
DIRECTORIES
AND
THE
X
L
THE
SE
A
ITEMS
HAVE
BEEN
MODIFIED
SINCE
THE
LAST
DUMP
EACH
DIRECTORY
AND
FILE
IS
LABELED
BY
ITS
I
NODE
NUMBER
EVENING
AND
AN
INCREMENTAL
DUMP
IS
DONE
ON
MONDAY
EVENING
ON
TUESDAY
THE
DIRECTORY
USR
JHS
PROJ
IS
REMOVED
ALONG
WITH
ALL
THE
DIRECTORIES
AND
FILES
UNDER
IT
ON
WEDNESDAY
MORNING
BRIGHT
AND
EARLY
THE
USER
WANTS
TO
RESTORE
THE
FILE
USR
JHS
PROJ
PLANS
SUMMARY
HOWEVER
IS
NOT
POSSIBLE
TO
JUST
RESTORE
THE
FILE
SUMMARY
BECAUSE
THERE
IS
NO
PLACE
TO
PUT
IT
THE
DIRECTORIES
AND
PLANS
MUST
BE
RESTORED
FIRST
TO
GET
THEIR
OWNERS
MODES
TIMES
AND
WHATEVER
CORRECT
THESE
DIRECTORIES
MUST
BE
PRESENT
ON
THE
DUMP
TAPE
EVEN
THOUGH
THEY
THEMSELVES
WERE
NOT
MODIFIED
SINCE
THE
PREVIOUS
FULL
DUMP
THE
DUMP
ALGORITHM
MAINTAINS
A
BITMAP
INDEXED
BY
I
NODE
NUMBER
WITH
SEV
ERAL
BITS
PER
I
NODE
BITS
WILL
BE
SET
AND
CLEARED
IN
THIS
MAP
AS
THE
ALGORITHM
PROCEEDS
THE
ALGORITHM
OPERATES
IN
FOUR
PHASES
PHASE
BEGINS
AT
THE
STARTING
DI
RECTORY
THE
ROOT
IN
THIS
EXAMPLE
AND
EXAMINES
ALL
THE
ENTRIES
IN
IT
FOR
EACH
MODI
FIED
FILE
ITS
I
NODE
IS
MARKED
IN
THE
BITMAP
EACH
DIRECTORY
IS
ALSO
MARKED
WHETHER
OR
NOT
IT
HAS
BEEN
MODIFIED
AND
THEN
RECURSIVELY
INSPECTED
AT
THE
END
OF
PHASE
ALL
MODIFIED
FILES
AND
ALL
DIRECTORIES
HAVE
BEEN
MARKED
IN
THE
BITMAP
AS
SHOWN
BY
SHADING
IN
FIG
A
PHASE
CONCEPTUALLY
RECUR
SIVELY
WALKS
THE
TREE
AGAIN
UNMARKING
ANY
DIRECTORIES
THAT
HAVE
NO
MODIFIED
FILES
OR
DIRECTORIES
IN
THEM
OR
UNDER
THEM
THIS
PHASE
LEAVES
THE
BITMAP
AS
SHOWN
IN
FIG
B
NOTE
THAT
DIRECTORIES
AND
ARE
NOW
UNMARKED
BECAUSE
THEY
CONTAIN
NOTHING
UNDER
THEM
THAT
HAS
BEEN
MODIFIED
THEY
WILL
NOT
BE
DUMPED
BY
WAY
OF
CONTRAST
DIRECTORIES
AND
WILL
BE
DUMPED
EVEN
THOUGH
THEY
AT
THIS
POINT
IT
IS
KNOWN
WHICH
DIRECTORIES
AND
FILES
MUST
BE
DUMPED
THESE
ARE
THE
ONES
MARKED
IN
FIG
B
PHASE
CONSISTS
OF
SCANNING
THE
I
NODES
IN
NUMERICAL
ORDER
AND
DUMPING
ALL
THE
DIRECTORIES
THAT
ARE
MARKED
FOR
DUMPING
THESE
ARE
SHOWN
IN
FIG
C
EACH
DIRECTORY
IS
PREFIXED
BY
THE
DIRECTORY
AT
TRIBUTES
OWNER
TIMES
ETC
SO
THAT
THEY
CAN
BE
RESTORED
FINALLY
IN
PHASE
THE
FILES
MARKED
IN
FIG
D
ARE
ALSO
DUMPED
AGAIN
PREFIXED
BY
THEIR
ATTRIBUTES
THIS
COMPLETES
THE
DUMP
RESTORING
A
FILE
SYSTEM
FROM
THE
DUMP
TAPES
IS
STRAIGHTFORWARD
TO
START
WITH
AN
EMPTY
FILE
SYSTEM
IS
CREATED
ON
THE
DISK
THEN
THE
MOST
RECENT
FULL
DUMP
IS
RE
STORED
SINCE
THE
DIRECTORIES
APPEAR
FIRST
ON
THE
TAPE
THEY
ARE
ALL
RESTORED
FIRST
GIV
ING
A
SKELETON
OF
THE
FILE
SYSTEM
THEN
THE
FILES
THEMSELVES
ARE
RESTORED
THIS
PROCESS
IS
THEN
REPEATED
WITH
THE
FIRST
INCREMENTAL
DUMP
MADE
AFTER
THE
FULL
DUMP
THEN
THE
NEXT
ONE
AND
SO
ON
ALTHOUGH
LOGICAL
DUMPING
IS
STRAIGHTFORWARD
THERE
ARE
A
FEW
TRICKY
ISSUES
FOR
ONE
SINCE
THE
FREE
BLOCK
LIST
IS
NOT
A
FILE
IT
IS
NOT
DUMPED
AND
HENCE
IT
MUST
BE
RECONSTRUCTED
FROM
SCRATCH
AFTER
ALL
THE
DUMPS
HAVE
BEEN
RESTORED
DOING
SO
IS
AL
WAYS
POSSIBLE
SINCE
THE
SET
OF
FREE
BLOCKS
IS
JUST
THE
COMPLEMENT
OF
THE
SET
OF
BLOCKS
CONTAINED
IN
ALL
THE
FILES
COMBINED
ANOTHER
ISSUE
IS
LINKS
IF
A
FILE
IS
LINKED
TO
TWO
OR
MORE
DIRECTORIES
IT
IS
IM
PORTANT
THAT
THE
FILE
IS
RESTORED
ONLY
ONE
TIME
AND
THAT
ALL
THE
DIRECTORIES
THAT
ARE
SUPPOSED
TO
POINT
TO
IT
DO
SO
STILL
ANOTHER
ISSUE
IS
THE
FACT
THAT
UNIX
FILES
MAY
CONTAIN
HOLES
IT
IS
LEGAL
TO
OPEN
A
FILE
WRITE
A
FEW
BYTES
THEN
SEEK
TO
A
DISTANT
FILE
OFFSET
AND
WRITE
A
FEW
MORE
BYTES
THE
BLOCKS
IN
BETWEEN
ARE
NOT
PART
OF
THE
FILE
AND
SHOULD
NOT
BE
DUMPED
AND
MUST
NOT
BE
RESTORED
CORE
FILES
OFTEN
HAVE
A
HOLE
OF
HUNDREDS
OF
FILE
SYSTEMS
CHAP
MEGABYTES
BETWEEN
THE
DATA
SEGMENT
AND
THE
STACK
IF
NOT
HANDLED
PROPERLY
EACH
RESTORED
CORE
FILE
WILL
FILL
THIS
AREA
WITH
ZEROS
AND
THUS
BE
THE
SAME
SIZE
AS
THE
VIR
TUAL
ADDRESS
SPACE
E
G
BYTES
OR
WORSE
YET
BYTES
FINALLY
SPECIAL
FILES
NAMED
PIPES
AND
THE
LIKE
SHOULD
NEVER
BE
DUMPED
NO
MATTER
IN
WHICH
DIRECTORY
THEY
MAY
OCCUR
THEY
NEED
NOT
BE
CONFINED
TO
DEV
FOR
MORE
INFORMATION
ABOUT
FILE
SYSTEM
BACKUPS
SEE
CHERVENAK
ET
AL
AND
ZWICKY
TAPE
DENSITIES
ARE
NOT
IMPROVING
AS
FAST
AS
DISK
DENSITIES
THIS
IS
GRADUALLY
LEADING
TO
A
SITUATION
IN
WHICH
BACKING
UP
A
VERY
LARGE
DISK
MAY
REQUIRE
MULTIPLE
SEC
FILE
SYSTEM
MANAGEMENT
AND
OPTIMIZATION
IF
THE
FILE
SYSTEM
IS
CONSISTENT
EACH
BLOCK
WILL
HAVE
A
EITHER
IN
THE
FIRST
TABLE
OR
IN
THE
SECOND
TABLE
AS
ILLUSTRATED
IN
FIG
A
HOWEVER
AS
A
RESULT
OF
A
CRASH
THE
TABLES
MIGHT
LOOK
LIKE
FIG
B
IN
WHICH
BLOCK
DOES
NOT
OCCUR
IN
EITHER
TABLE
IT
WILL
BE
REPORTED
AS
BEING
A
MISSING
BLOCK
WHILE
MISSING
BLOCKS
DO
NO
REAL
HARM
THEY
WASTE
SPACE
AND
THUS
REDUCE
THE
CAPACITY
OF
THE
DISK
THE
SOLUTION
TO
MISSING
BLOCKS
IS
STRAIGHTFORWARD
THE
FILE
SYSTEM
CHECKER
JUST
ADDS
THEM
TO
THE
FREE
LIST
BLOCK
NUMBER
BLOCK
NUMBER
TAPES
WHILE
TAPE
ROBOTS
ARE
AVAILABLE
TO
CHANGE
TAPES
AUTOMATICALLY
IF
THIS
TREND
CONTINUES
TAPES
WILL
EVENTUALLY
BECOME
TOO
SMALL
TO
USE
AS
A
BACKUP
MEDIUM
IN
THAT
CASE
THE
ONLY
WAY
TO
BACK
UP
A
DISK
WILL
BE
ON
ANOTHER
DISK
WHILE
SIMPLY
MIRRORING
EACH
DISK
WITH
A
SPARE
IS
ONE
POSSIBILITY
MORE
SOPHISTICATED
SCHEMES
CALLED
RAIDS
WILL
BE
DISCUSSED
IN
CHAP
FILE
SYSTEM
CONSISTENCY
A
BLOCKS
IN
USE
FREE
BLOCKS
B
BLOCKS
IN
USE
FREE
BLOCKS
ANOTHER
AREA
WHERE
RELIABILITY
IS
AN
ISSUE
IS
FILE
SYSTEM
CONSISTENCY
MANY
FILE
SYSTEMS
READ
BLOCKS
MODIFY
THEM
AND
WRITE
THEM
OUT
LATER
IF
THE
SYSTEM
CRASHES
BEFORE
ALL
THE
MODIFIED
BLOCKS
HAVE
BEEN
WRITTEN
OUT
THE
FILE
SYSTEM
CAN
BE
LEFT
IN
BLOCKS
IN
USE
FREE
BLOCKS
I
I
D
I
I
BLOCKS
IN
USE
FREE
BLOCKS
AN
INCONSISTENT
STATE
THIS
PROBLEM
IS
ESPECIALLY
CRITICAL
IF
SOME
OF
THE
BLOCKS
THAT
HAVE
NOT
BEEN
WRITTEN
OUT
ARE
I
NODE
BLOCKS
DIRECTORY
BLOCKS
OR
BLOCKS
CONTAINING
THE
FREE
LIST
TO
DEAL
WITH
THE
PROBLEM
OF
INCONSISTENT
FILE
SYSTEMS
MOST
COMPUTERS
HAVE
A
UTILITY
PROGRAM
THAT
CHECKS
FILE
SYSTEM
CONSISTENCY
FOR
EXAMPLE
UNIX
HAS
FSCK
AND
WINDOWS
HAS
SCANDISK
THIS
UTILITY
CAN
BE
RUN
WHENEVER
THE
SYSTEM
IS
BOOT
ED
ESPECIALLY
AFTER
A
CRASH
THE
DESCRIPTION
BELOW
TELLS
HOW
FSCK
WORKS
SCANDISK
IS
SOMEWHAT
DIFFERENT
BECAUSE
IT
WORKS
ON
A
DIFFERENT
FILE
SYSTEM
BUT
THE
GENERAL
PRINCIPLE
OF
USING
THE
FILE
SYSTEM
INHERENT
REDUNDANCY
TO
REPAIR
IT
IS
STILL
VALID
ALL
FILE
SYSTEM
CHECKERS
VERIFY
EACH
FILE
SYSTEM
DISK
PARTITION
INDEPENDENTLY
OF
THE
OTHER
ONES
TWO
KINDS
OF
CONSISTENCY
CHECKS
CAN
BE
MADE
BLOCKS
AND
FILES
TO
CHECK
FOR
BLOCK
CONSISTENCY
THE
PROGRAM
BUILDS
TWO
TABLES
EACH
ONE
CONTAINING
A
COUNTER
FOR
EACH
BLOCK
INITIALLY
SET
TO
THE
COUNTERS
IN
THE
FIRST
TABLE
KEEP
TRACK
OF
HOW
MANY
TIMES
EACH
BLOCK
IS
PRESENT
IN
A
FILE
THE
COUNTERS
IN
THE
SECOND
TABLE
RECORD
HOW
OFTEN
EACH
BLOCK
IS
PRESENT
IN
THE
FREE
LIST
OR
THE
BITMAP
OF
FREE
BLOCKS
THE
PROGRAM
THEN
READS
ALL
THE
I
NODES
USING
A
RAW
DEVICE
WHICH
IGNORES
THE
FILE
STRUCTURE
AND
JUST
RETURNS
ALL
THE
DISK
BLOCKS
STARTING
AT
STARTING
FROM
AN
I
NODE
IT
IS
POSSIBLE
TO
BUILD
A
LIST
OF
ALL
THE
BLOCK
NUMBERS
USED
IN
THE
CORRESPOND
ING
FILE
AS
EACH
BLOCK
NUMBER
IS
READ
ITS
COUNTER
IN
THE
FIRST
TABLE
IS
INCRE
MENTED
THE
PROGRAM
THEN
EXAMINES
THE
FREE
LIST
OR
BITMAP
TO
FIND
ALL
THE
BLOCKS
THAT
ARE
NOT
IN
USE
EACH
OCCURRENCE
OF
A
BLOCK
IN
THE
FREE
LIST
RESULTS
IN
ITS
COUNTER
IN
THE
SECOND
TABLE
BEING
INCREMENTED
FIGURE
FILE
SYSTEM
STATES
A
CONSISTENT
B
MISSING
BLOCK
C
DUPLI
CATE
BLOCK
IN
FREE
LIST
D
DUPLICATE
DATA
BLOCK
ANOTHER
SITUATION
THAT
MIGHT
OCCUR
IS
THAT
OF
FIG
C
HERE
WE
SEE
A
BLOCK
NUMBER
THAT
OCCURS
TWICE
IN
THE
FREE
LIST
DUPLICATES
CAN
OCCUR
ONLY
IF
THE
FREE
LIST
IS
REALLY
A
LIST
WITH
A
BITMAP
IT
IS
IMPOSSIBLE
THE
SOLUTION
HERE
IS
ALSO
SIMPLE
REBUILD
THE
FREE
LIST
THE
WORST
THING
THAT
CAN
HAPPEN
IS
THAT
THE
SAME
DATA
BLOCK
IS
PRESENT
IN
TWO
OR
MORE
FILES
AS
SHOWN
IN
FIG
D
WITH
BLOCK
IF
EITHER
OF
THESE
FILES
IS
RE
MOVED
BLOCK
WILL
BE
PUT
ON
THE
FREE
LIST
LEADING
TO
A
SITUATION
IN
WHICH
THE
SAME
BLOCK
IS
BOTH
IN
USE
AND
FREE
AT
THE
SAME
TIME
IF
BOTH
FILES
ARE
REMOVED
THE
BLOCK
WILL
BE
PUT
ONTO
THE
FREE
LIST
TWICE
THE
APPROPRIATE
ACTION
FOR
THE
FILE
SYSTEM
CHECKER
TO
TAKE
IS
TO
ALLOCATE
A
FREE
BLOCK
COPY
THE
CONTENTS
OF
BLOCK
INTO
IT
AND
INSERT
THE
COPY
INTO
ONE
OF
THE
FILES
IN
THIS
WAY
THE
INFORMATION
CONTENT
OF
THE
FILES
IS
UNCHANGED
ALTHOUGH
ALMOST
ASSUREDLY
ONE
IS
GARBLED
BUT
THE
FILE
SYSTEM
STRUCTURE
IS
AT
LEAST
MADE
CONSISTENT
THE
ERROR
SHOULD
BE
REPORTED
TO
ALLOW
THE
USER
TO
INSPECT
THE
DAMAGE
IN
ADDITION
TO
CHECKING
TO
SEE
THAT
EACH
BLOCK
IS
PROPERLY
ACCOUNTED
FOR
THE
FILE
SYSTEM
CHECKER
ALSO
CHECKS
THE
DIRECTORY
SYSTEM
IT
TOO
USES
A
TABLE
OF
COUNTERS
BUT
THESE
ARE
PER
FILE
RATHER
THAN
PER
BLOCK
IT
STARTS
AT
THE
ROOT
DIRECTORY
AND
RECURSIVELY
DESCENDS
THE
TREE
INSPECTING
EACH
DIRECTORY
IN
THE
FILE
SYSTEM
FOR
EVERY
I
NODE
IN
EVERY
DIRECTORY
IT
INCREMENTS
A
COUNTER
FOR
THAT
FILE
USAGE
COUNT
FILE
SYSTEMS
CHAP
REMEMBER
THAT
DUE
TO
HARD
LINKS
A
FILE
MAY
APPEAR
IN
TWO
OR
MORE
DIRECTORIES
SYMBOLIC
LINKS
DO
NOT
COUNT
AND
DO
NOT
CAUSE
THE
COUNTER
FOR
THE
TARGET
FILE
TO
BE
INCREMENTED
WHEN
THE
CHECKER
IS
ALL
DONE
IT
HAS
A
LIST
INDEXED
BY
I
NODE
NUMBER
TELLING
HOW
MANY
DIRECTORIES
CONTAIN
EACH
FILE
IT
THEN
COMPARES
THESE
NUMBERS
WITH
THE
LINK
COUNTS
STORED
IN
THE
I
NODES
THEMSELVES
THESE
COUNTS
START
AT
I
WHEN
A
FILE
IS
CREATED
AND
ARE
INCREMENTED
EACH
TIME
A
HARD
LINK
IS
MADE
TO
THE
FILE
IN
A
CON
SISTENT
FILE
SYSTEM
BOTH
COUNTS
WILL
AGREE
HOWEVER
TWO
KINDS
OF
ERRORS
CAN
OC
CUR
THE
LINK
COUNT
IN
THE
I
NODE
CAN
BE
TOO
HIGH
OR
IT
CAN
BE
TOO
LOW
IF
THE
LINK
COUNT
IS
HIGHER
THAN
THE
NUMBER
OF
DIRECTORY
ENTRIES
THEN
EVEN
IF
ALL
THE
FILES
ARE
REMOVED
FROM
THE
DIRECTORIES
THE
COUNT
WILL
STILL
BE
NONZERO
AND
THE
I
NODE
WILL
NOT
BE
REMOVED
THIS
ERROR
IS
NOT
SERIOUS
BUT
IT
WASTES
SPACE
ON
THE
DISK
WITH
FILES
THAT
ARE
NOT
IN
ANY
DIRECTORY
IT
SHOULD
BE
FIXED
BY
SETTING
THE
LINK
COUNT
IN
THE
I
NODE
TO
THE
CORRECT
VALUE
THE
OTHER
ERROR
IS
POTENTIALLY
CATASTROPHIC
IF
TWO
DIRECTORY
ENTRIES
ARE
LINKED
TO
A
FILE
BUT
THE
I
NODE
SAYS
THAT
THERE
IS
ONLY
ONE
WHEN
EITHER
DIRECTORY
ENTRY
IS
REMOVED
THE
I
NODE
COUNT
WILL
GO
TO
ZERO
WHEN
AN
I
NODE
COUNT
GOES
TO
ZERO
THE
FILE
SYSTEM
MARKS
IT
AS
UNUSED
AND
RELEASES
ALL
OF
ITS
BLOCKS
THIS
ACTION
WILL
RESULT
IN
ONE
OF
THE
DIRECTORIES
NOW
POINTING
TO
AN
UNUSED
I
NODE
WHOSE
BLOCKS
MAY
SOON
BE
ASSIGNED
TO
OTHER
FILES
AGAIN
THE
SOLUTION
IS
JUST
TO
FORCE
THE
LINK
COUNT
IN
THE
I
NODE
TO
THE
ACTUAL
NUMBER
OF
DIRECTORY
ENTRIES
THESE
TWO
OPERATIONS
CHECKING
BLOCKS
AND
CHECKING
DIRECTORIES
ARE
OFTEN
INTEGRATED
FOR
EFFICIENCY
REASONS
I
E
ONLY
ONE
PASS
OVER
THE
I
NODES
IS
REQUIRED
OTHER
CHECKS
ARE
ALSO
POSSIBLE
FOR
EXAMPLE
DIRECTORIES
HAVE
A
DEFINITE
FORMAT
WITH
I
NODE
NUMBERS
AND
ASCII
NAMES
IF
AN
I
NODE
NUMBER
IS
LARGER
THAN
THE
NUMBER
OF
I
NODES
ON
THE
DISK
THE
DIRECTORY
HAS
BEEN
DAMAGED
FURTHERMORE
EACH
I
NODE
HAS
A
MODE
SOME
OF
WHICH
ARE
LEGAL
BUT
STRANGE
SUCH
AS
WHICH
ALLOWS
THE
OWNER
AND
HIS
GROUP
NO
ACCESS
AT
ALL
BUT
ALLOWS
OUTSIDERS
TO
READ
WRITE
AND
EXECUTE
THE
FILE
IT
MIGHT
BE
USEFUL
TO
AT
LEAST
REPORT
FILES
THAT
GIVE
OUTSIDERS
MORE
RIGHTS
THAN
THE
OWNER
DIRECTORIES
WITH
MORE
THAN
SAY
ENTRIES
ARE
ALSO
SUSPICIOUS
FILES
LOCATED
IN
USER
DIRECTORIES
BUT
WHICH
ARE
OWNED
BY
THE
SUPERUSER
AND
HAVE
THE
SETUID
BIT
ON
ARE
POTENTIAL
SECURITY
PROBLEMS
BECAUSE
SUCH
FILES
ACQUIRE
THE
POWERS
OF
THE
SUPERUSER
WHEN
EXECUTED
BY
ANY
USER
WITH
A
LITTLE
EFFORT
ONE
CAN
PUT
TOGETHER
A
FAIRLY
LONG
LIST
OF
TECHNI
CALLY
LEGAL
BUT
STILL
PECULIAR
SITUATIONS
THAT
MIGHT
BE
WORTH
REPORTING
THE
PREVIOUS
PARAGRAPHS
HAVE
DISCUSSED
THE
PROBLEM
OF
PROTECTING
THE
USER
AGAINST
CRASHES
SOME
FILE
SYSTEMS
ALSO
WORRY
ABOUT
PROTECTING
THE
USER
AGAINST
HIMSELF
IF
THE
USER
INTENDS
TO
TYPE
RM
O
TO
REMOVE
ALL
THE
FILES
ENDING
WITH
O
COMPILER
GENERATED
OBJECT
FILES
BUT
AC
CIDENTALLY
TYPES
RM
O
SEC
FILE
SYSTEM
MANAGEMEN
T
AN
D
OPTIMIZATION
NOTE
THE
SPACE
AFTER
THE
ASTERISK
RM
WILL
REMOVE
ALL
THE
FILES
IN
THE
CURRENT
DIREC
TORY
AND
THEN
COMPLAIN
THAT
IT
CANNOT
FIND
O
IN
MS
DOS
AND
SOME
OTHER
SYSTEMS
WHEN
A
FILE
IS
REMOVED
ALL
THAT
HAPPENS
IS
THAT
A
BIT
IS
SET
IN
THE
DIRECTORY
OR
I
NODE
MARKING
THE
FILE
AS
REMOVED
NO
DISK
BLOCKS
ARE
RETURNED
TO
THE
FREE
LIST
UNTIL
THEY
ARE
ACTUALLY
NEEDED
THUS
IF
THE
USER
DISCOVERS
THE
ERROR
IMMEDIATELY
IT
IS
POSSIBLE
TO
RUN
A
SPECIAL
UTILITY
PROGRAM
THAT
UNREMOVES
I
E
RESTORES
THE
REMOVED
FILES
IN
WINDOWS
FILES
THAT
ARE
REMOVED
ARE
PLACED
IN
THE
RECYCLE
BIN
A
SPECIAL
DIRECTORY
FROM
WHICH
THEY
CAN
LATER
BE
RETRIEVED
IF
NEED
BE
OF
COURSE
NO
STORAGE
IS
RECLAIMED
UNTIL
THEY
ARE
ACTUALLY
DELETED
FROM
THIS
DIRECTORY
FILE
SYSTEM
PERFORMANCE
ACCESS
TO
DISK
IS
MUCH
SLOWER
THAN
ACCESS
TO
MEMORY
READING
A
BIT
MEM
ORY
WORD
MIGHT
TAKE
NSEC
READING
FROM
A
HARD
DISK
MIGHT
PROCEED
AT
MB
SEC
WHICH
IS
FOUR
TIMES
SLOWER
PER
BIT
WORD
BUT
TO
THIS
MUST
BE
ADDED
MSEC
TO
SEEK
TO
THE
TRACK
AND
THEN
WAIT
FOR
THE
DESIRED
SECTOR
TO
ARRIVE
UNDER
THE
READ
HEAD
IF
ONLY
A
SINGLE
WORD
IS
NEEDED
THE
MEMORY
ACCESS
IS
ON
THE
ORDER
OF
A
MILLION
TIMES
AS
FAST
AS
DISK
ACCESS
AS
A
RESULT
OF
THIS
DIFFERENCE
IN
ACCESS
TIME
MANY
FILE
SYSTEMS
HAVE
BEEN
DESIGNED
WITH
VARIOUS
OPTIMIZATIONS
TO
IMPROVE
PERFORMANCE
IN
THIS
SECTION
WE
WILL
COVER
THREE
OF
THEM
CACHING
THE
MOST
COMMON
TECHNIQUE
USED
TO
REDUCE
DISK
ACCESSES
IS
THE
BLOCK
CACHE
OR
BUFFER
CACHE
CACHE
IS
PRONOUNCED
CASH
AND
IS
DERIVED
FROM
THE
FRENCH
CACHER
MEANING
TO
HIDE
IN
THIS
CONTEXT
A
CACHE
IS
A
COLLECTION
OF
BLOCKS
THAT
LOGICALLY
BELONG
ON
THE
DISK
BUT
ARE
BEING
KEPT
IN
MEMORY
FOR
PERFORMANCE
REA
SONS
VARIOUS
ALGORITHMS
CAN
BE
USED
TO
MANAGE
THE
CACHE
BUT
A
COMMON
ONE
IS
TO
CHECK
ALL
READ
REQUESTS
TO
SEE
IF
THE
NEEDED
BLOCK
IS
IN
THE
CACHE
IF
IT
IS
THE
READ
REQUEST
CAN
BE
SATISFIED
WITHOUT
A
DISK
ACCESS
IF
THE
BLOCK
IS
NOT
IN
THE
CACHE
IT
IS
FIRST
READ
INTO
THE
CACHE
AND
THEN
COPIED
TO
WHEREVER
IT
IS
NEEDED
SUBSEQUENT
RE
QUESTS
FOR
THE
SAME
BLOCK
CAN
BE
SATISFIED
FROM
THE
CACHE
OPERATION
OF
THE
CACHE
IS
ILLUSTRATED
IN
FIG
SINCE
THERE
ARE
MANY
OFTEN
THOUSANDS
OF
BLOCKS
IN
THE
CACHE
SOME
WAY
IS
NEEDED
TO
DETERMINE
QUICKLY
IF
A
GIVEN
BLOCK
IS
PRESENT
THE
USUAL
WAY
IS
TO
HASH
THE
DEVICE
AND
DISK
ADDRESS
AND
LOOK
UP
THE
RESULT
IN
A
HASH
TABLE
ALL
THE
BLOCKS
WITH
THE
SAME
HASH
VALUE
ARE
CHAINED
TOGETHER
ON
A
LINKED
LIST
SO
THAT
THE
COLLISION
CHAIN
CAN
BE
FOLLOWED
WHEN
A
BLOCK
HAS
TO
BE
LOADED
INTO
A
FULL
CACHE
SOME
BLOCK
HAS
TO
BE
RE
MOVED
AND
REWRITTEN
TO
THE
DISK
IF
IT
HAS
BEEN
MODIFIED
SINCE
BEING
BROUGHT
IN
THIS
SITUATION
IS
VERY
MUCH
LIKE
PAGING
AND
ALL
THE
USUAL
PAGE
REPLACEMENT
ALGO
RITHMS
DESCRIBED
IN
CHAP
SUCH
AS
FIFO
SECOND
CHANCE
AND
LRU
ARE
APPLICA
BLE
ONE
PLEASANT
DIFFERENCE
BETWEEN
PAGING
AND
CACHING
IS
THAT
CACHE
REFERENCES
FILE
SYSTEMS
CHAP
FIGURE
THE
BUFFER
CACHE
DATA
STRUCTURES
SEC
FILE
SYSTEM
MANAGEMENT
AND
OPTIMIZATION
BEEN
MODIFIED
IT
SHOULD
BE
WRITTEN
TO
DISK
IMMEDIATELY
REGARDLESS
OF
WHICH
END
OF
THE
LRU
LIST
IT
IS
PUT
ON
BY
WRITING
CRITICAL
BLOCKS
QUICKLY
WE
GREATLY
REDUCE
THE
PROBABILITY
THAT
A
CRASH
WILL
WRECK
THE
FILE
SYSTEM
WHILE
A
USER
MAY
BE
UNHAPPY
IF
ONE
OF
HIS
FILES
IS
RUINED
IN
A
CRASH
HE
IS
LIKELY
TO
BE
FAR
MORE
UNHAPPY
IF
THE
WHOLE
FILE
SYSTEM
IS
LOST
EVEN
WITH
THIS
MEASURE
TO
KEEP
THE
FILE
SYSTEM
INTEGRITY
INTACT
IT
IS
UNDESIR
ABLE
TO
KEEP
DATA
BLOCKS
IN
THE
CACHE
TOO
LONG
BEFORE
WRITING
THEM
OUT
CONSIDER
THE
PLIGHT
OF
SOMEONE
WHO
IS
USING
A
PERSONAL
COMPUTER
TO
WRITE
A
BOOK
EVEN
IF
OUR
WRITER
PERIODICALLY
TELLS
THE
EDITOR
TO
WRITE
THE
FILE
BEING
EDITED
TO
THE
DISK
THERE
IS
A
GOOD
CHANCE
THAT
EVERYTHING
WILL
STILL
BE
IN
THE
CACHE
AND
NOTHING
ON
THE
DISK
IF
THE
SYSTEM
CRASHES
THE
FILE
SYSTEM
STRUCTURE
WILL
NOT
BE
CORRUPTED
BUT
A
WHOLE
DAY
WORK
WILL
BE
LOST
THIS
SITUATION
NEED
NOT
HAPPEN
VERY
OFTEN
BEFORE
WE
HAVE
A
FAIRLY
UNHAPPY
USER
SYSTEMS
TAKE
TWO
APPROACHES
TO
DEALING
WITH
IT
THE
UNIX
WAY
IS
TO
HAVE
A
ARE
RELATIVELY
INFREQUENT
SO
THAT
IT
IS
FEASIBLE
TO
KEEP
ALL
THE
BLOCKS
IN
EXACT
LRU
ORDER
WITH
LINKED
LISTS
IN
FIG
WE
SEE
THAT
IN
ADDITION
TO
THE
COLLISION
CHAINS
STARTING
AT
THE
HASH
TABLE
THERE
IS
ALSO
A
BIDIRECTIONAL
LIST
RUNNING
THROUGH
ALL
THE
BLOCKS
IN
THE
ORDER
OF
USAGE
WITH
THE
LEAST
RECENTLY
USED
BLOCK
ON
THE
FRONT
OF
THIS
LIST
AND
THE
MOST
RECENTLY
USED
BLOCK
AT
THE
END
OF
THIS
LIST
WHEN
A
BLOCK
IS
REFERENCED
IT
CAN
BE
RE
MOVED
FROM
ITS
POSITION
ON
THE
BIDIRECTIONAL
LIST
AND
PUT
AT
THE
END
IN
THIS
WAY
EXACT
LRU
ORDER
CAN
BE
MAINTAINED
UNFORTUNATELY
THERE
IS
A
CATCH
NOW
THAT
WE
HAVE
A
SITUATION
IN
WHICH
EXACT
LRU
IS
POSSIBLE
IT
TURNS
OUT
THAT
LRU
IS
UNDESIRABLE
THE
PROBLEM
HAS
TO
DO
WITH
THE
CRASHES
AND
FILE
SYSTEM
CONSISTENCY
DISCUSSED
IN
THE
PREVIOUS
SECTION
IF
A
CRIT
ICAL
BLOCK
SUCH
AS
AN
I
NODE
BLOCK
IS
READ
INTO
THE
CACHE
AND
MODIFIED
BUT
NOT
REWRITTEN
TO
THE
DISK
A
CRASH
WILL
LEAVE
THE
FILE
SYSTEM
IN
AN
INCONSISTENT
STATE
IF
THE
I
NODE
BLOCK
IS
PUT
AT
THE
END
OF
THE
LRU
CHAIN
IT
MAY
BE
QUITE
A
WHILE
BEFORE
IT
REACHES
THE
FRONT
AND
IS
REWRITTEN
TO
THE
DISK
FURTHERMORE
SOME
BLOCKS
SUCH
AS
I
NODE
BLOCKS
ARE
RARELY
REFERENCED
TWO
TIMES
WITHIN
A
SHORT
INTERVAL
THESE
CONSIDERATIONS
LEAD
TO
A
MODIFIED
LRU
SCHEME
TAKING
TWO
FACTORS
INTO
ACCOUNT
IS
THE
BLOCK
LIKELY
TO
BE
NEEDED
AGAIN
SOON
IS
THE
BLOCK
ESSENTIAL
TO
THE
CONSISTENCY
OF
THE
FILE
SYSTEM
FOR
BOTH
QUESTIONS
BLOCKS
CAN
BE
DIVIDED
INTO
CATEGORIES
SUCH
AS
I
NODE
BLOCKS
INDIRECT
BLOCKS
DIRECTORY
BLOCKS
FULL
DATA
BLOCKS
AND
PARTIALLY
FULL
DATA
BLOCKS
BLOCKS
THAT
WILL
PROBABLY
NOT
BE
NEEDED
AGAIN
SOON
GO
ON
THE
FRONT
RATHER
THAN
THE
REAR
OF
THE
LRU
LIST
SO
THEIR
BUFFERS
WILL
BE
REUSED
QUICKLY
BLOCKS
THAT
MIGHT
BE
NEEDED
AGAIN
SOON
SUCH
AS
A
PARTLY
FULL
BLOCK
THAT
IS
BEING
WRITTEN
GO
ON
THE
END
OF
THE
LIST
SO
THEY
WILL
STAY
AROUND
FOR
A
LONG
TIME
THE
SECOND
QUESTION
IS
INDEPENDENT
OF
THE
FIRST
ONE
IF
THE
BLOCK
IS
ESSENTIAL
TO
THE
FILE
SYSTEM
CONSISTENCY
BASICALLY
EVERYTHING
EXCEPT
DATA
BLOCKS
AND
IT
HAS
SYSTEM
CALL
SYNC
WHICH
FORCES
ALL
THE
MODIFIED
BLOCKS
OUT
ONTO
THE
DISK
IM
MEDIATELY
WHEN
THE
SYSTEM
IS
STARTED
UP
A
PROGRAM
USUALLY
CALLED
UPDATE
IS
STARTED
UP
IN
THE
BACKGROUND
TO
SIT
IN
AN
ENDLESS
LOOP
ISSUING
SYNC
CALLS
SLEEPING
FOR
SEC
BETWEEN
CALLS
AS
A
RESULT
NO
MORE
THAN
SECONDS
OF
WORK
IS
LOST
DUE
TO
A
CRASH
ALTHOUGH
WINDOWS
NOW
HAS
A
SYSTEM
CALL
EQUIVALENT
TO
SYNC
FLUSHFILEBUFF
ERS
IN
THE
PAST
IT
DID
NOT
INSTEAD
IT
HAD
A
DIFFERENT
STRATEGY
THAT
WAS
IN
SEME
WAYS
BETTER
THAN
THE
UNIX
APPROACH
AND
IN
SOME
WAYS
WORSE
WHAT
IT
DID
WAS
TO
WRITE
EVERY
MODIFIED
BLOCK
TO
DISK
AS
SOON
AS
IT
HAS
BEEN
WRITTEN
TO
THE
CACHE
CACHES
IN
WHICH
ALL
MODIFIED
BLOCKS
ARE
WRITTEN
BACK
TO
THE
DISK
IMMEDIATELY
ARE
CALLED
WRITE
THROUGH
CACHES
THEY
REQUIRE
MORE
DISK
I
O
THAN
NONWRITE
THROUGH
CACHES
THE
DIFFERENCE
BETWEEN
THESE
TWO
APPROACHES
CAN
BE
SEEN
WHEN
A
PROGRAM
WRITES
A
KB
BLOCK
FULL
ONE
CHARACTER
AT
A
TIME
UNIX
WILL
COLLECT
ALL
THE
CHARAC
TERS
IN
THE
CACHE
AND
WRITE
THE
BLOCK
OUT
ONCE
EVERY
SECONDS
OR
WHENEVER
THE
BLOCK
IS
REMOVED
FROM
THE
CACHE
WITH
A
WRITE
THROUGH
CACHE
THERE
IS
A
DISK
AC
CESS
FOR
EVERY
CHARACTER
WRITTEN
OF
COURSE
MOST
PROGRAMS
DO
INTERNAL
BUFFERING
SO
THEY
NORMALLY
WRITE
NOT
A
CHARACTER
BUT
A
LINE
OR
A
LARGER
UNIT
ON
EACH
WRITE
SYS
TEM
CALL
A
CONSEQUENCE
OF
THIS
DIFFERENCE
IN
CACHING
STRATEGY
IS
THAT
JUST
REMOVING
A
FLOPPY
DISK
FROM
A
UNIX
SYSTEM
WITHOUT
DOING
A
SYNC
WILL
ALMOST
ALWAYS
RESULT
IN
LOST
DATA
AND
FREQUENTLY
IN
A
CORRUPTED
FILE
SYSTEM
AS
WELL
WITH
WRITE
THROUGH
CACHING
NO
PROBLEM
ARISES
THESE
DIFFERING
STRATEGIES
WERE
CHOSEN
BECAUSE
UNIX
WAS
DEVELOPED
IN
AN
ENVIRONMENT
IN
WHICH
ALL
DISKS
WERE
HARD
DISKS
AND
NOT
REMOVABLE
WHEREAS
THE
FIRST
WINDOWS
FILE
SYSTEM
WAS
INHERITED
FROM
MS
DOS
WHICH
STARTED
OUT
IN
THE
FLOPPY
DISK
WORLD
AS
HARD
DISKS
BECAME
THE
NORM
THE
UNIX
APPROACH
WITH
ITS
BETTER
EFFICIENCY
BUT
WORSE
RELIABILITY
BECAME
THE
NORM
AND
IS
ALSO
USED
NOW
ON
WINDOWS
FOR
HARD
DISKS
HOWEVER
NTFS
TAKES
OTHER
MEASURES
JOURNALING
TO
IMPROVE
RELIABILITY
AS
DISCUSSED
EARLIER
FILE
SYSTEMS
CHAP
SOME
OPERATING
SYSTEMS
INTEGRATE
THE
BUFFER
CACHE
WITH
THE
PAGE
CACHE
THIS
IS
ESPECIALLY
ATTRACTIVE
WHEN
MEMORY
MAPPED
FILES
ARE
SUPPORTED
IF
A
FILE
IS
MAP
PED
ONTO
MEMORY
THEN
SOME
OF
ITS
PAGES
MAY
BE
IN
MEMORY
BECAUSE
THEY
WERE
DEMAND
PAGED
IN
SUCH
PAGES
ARE
HARDLY
DIFFERENT
FROM
FILE
BLOCKS
IN
THE
BUFFER
CACHE
IN
THIS
CASE
THEY
CAN
BE
TREATED
THE
SAME
WAY
WITH
A
SINGLE
CACHE
FOR
BOTH
FILE
BLOCKS
AND
PAGES
BLOCK
READ
AHEAD
A
SECOND
TECHNIQUE
FOR
IMPROVING
PERCEIVED
FILE
SYSTEM
PERFORMANCE
IS
TO
TRY
TO
GET
BLOCKS
INTO
THE
CACHE
BEFORE
THEY
ARE
NEEDED
TO
INCREASE
THE
HIT
RATE
IN
PAR
TICULAR
MANY
FILES
ARE
READ
SEQUENTIALLY
WHEN
THE
FILE
SYSTEM
IS
ASKED
TO
PRODUCE
BLOCK
K
IN
A
FILE
IT
DOES
THAT
BUT
WHEN
IT
IS
FINISHED
IT
MAKES
A
SNEAKY
CHECK
IN
THE
CACHE
TO
SEE
IF
BLOCK
K
IS
ALREADY
THERE
IF
IT
IS
NOT
IT
SCHEDULES
A
READ
FOR
BLOCK
K
L
M
THE
HOPE
THAT
WHEN
IT
IS
NEEDED
IT
WILL
HAVE
ALREADY
ARRIVED
IN
THE
CACHE
AT
THE
VERY
LEAST
IT
WILL
BE
ON
THE
WAY
OF
COURSE
THIS
READ
AHEAD
STRATEGY
ONLY
WORKS
FOR
FILES
THAT
ARE
BEING
READ
SE
QUENTIALLY
IF
A
FILE
IS
BEING
RANDOMLY
ACCESSED
READ
AHEAD
DOES
NOT
HELP
IN
FACT
IT
HURTS
BY
TYING
UP
DISK
BANDWIDTH
READING
IN
USELESS
BLOCKS
AND
REMOVING
POTEN
TIALLY
USEFUL
BLOCKS
FROM
THE
CACHE
AND
POSSIBLY
TYING
UP
MORE
DISK
BANDWIDTH
WRITING
THEM
BACK
TO
DISK
IF
THEY
ARE
DIRTY
TO
SEE
WHETHER
READ
AHEAD
IS
WORTH
DOING
THE
FILE
SYSTEM
CAN
KEEP
TRACK
OF
THE
ACCESS
PATTERNS
TO
EACH
OPEN
FILE
FOR
EXAMPLE
A
BIT
ASSOCIATED
WITH
EACH
FILE
CAN
KEEP
TRACK
OF
WHETHER
THE
FILE
IS
IN
SEQUENTIAL
ACCESS
MODE
OR
RANDOM
ACCESS
MODE
INITIALLY
THE
FILE
IS
GIVEN
THE
BENEFIT
OF
THE
DOUBT
AND
PUT
IN
SEQUENTIAL
ACCESS
MODE
HOWEVER
WHENEVER
A
SEEK
IS
DONE
THE
BIT
IS
CLEARED
IF
SEQUENTIAL
READS
START
HAPPENING
AGAIN
THE
BIT
IS
SET
ONCE
AGAIN
IN
THIS
WAY
THE
FILE
SYSTEM
CAN
MAKE
A
REASONABLE
GUESS
ABOUT
WHETHER
IT
SHOULD
READ
AHEAD
OR
NOT
IF
IT
GETS
IT
WRONG
ONCE
IN
A
WHILE
IT
IS
NOT
A
DISASTER
JUST
A
LITTLE
BIT
OF
WASTED
DISK
BANDWIDTH
REDUCING
DISK
ARM
MOTION
CACHING
AND
READ
AHEAD
ARE
NOT
THE
ONLY
WAYS
TO
INCREASE
FILE
SYSTEM
PER
FORMANCE
ANOTHER
IMPORTANT
TECHNIQUE
IS
TO
REDUCE
THE
AMOUNT
OF
DISK
ARM
MOTION
BY
PUTTING
BLOCKS
THAT
ARE
LIKELY
TO
BE
ACCESSED
IN
SEQUENCE
CLOSE
TO
EACH
OTHER
PREFERABLY
IN
THE
SAME
CYLINDER
WHEN
AN
OUTPUT
FILE
IS
WRITTEN
THE
FILE
SYS
TEM
HAS
TO
ALLOCATE
THE
BLOCKS
ONE
AT
A
TIME
ON
DEMAND
IF
THE
FREE
BLOCKS
ARE
RECORDED
IN
A
BITMAP
AND
THE
WHOLE
BITMAP
IS
IN
MAIN
MEMORY
IT
IS
EASY
ENOUGH
TO
CHOOSE
A
FREE
BLOCK
AS
CLOSE
AS
POSSIBLE
TO
THE
PREVIOUS
BLOCK
WITH
A
FREE
LIST
PART
OF
WHICH
IS
ON
DISK
IT
IS
MUCH
HARDER
TO
ALLOCATE
BLOCKS
CLOSE
TOGETHER
HOWEVER
EVEN
WITH
A
FREE
LIST
SOME
BLOCK
CLUSTERING
CAN
BE
DONE
THE
TRICK
IS
TO
KEEP
TRACK
OF
DISK
STORAGE
NOT
IN
BLOCKS
BUT
IN
GROUPS
OF
CONSECUTIVE
BLOCKS
IF
ALL
SECTORS
CONSIST
OF
BYTES
THE
SYSTEM
COULD
USE
KB
BLOCKS
SECTORS
SEC
FILE
SYSTEM
MANAGEMEN
T
AN
D
OPTIMIZATION
BUT
ALLOCATE
DISK
STORAGE
IN
UNITS
OF
BLOCKS
SECTORS
THIS
IS
NOT
THE
SAME
AS
HAVING
A
KB
DISK
BLOCKS
SINCE
THE
CACHE
WOULD
STILL
USE
KB
BLOCKS
AND
DISK
TRANSFERS
WOULD
STILL
BE
KB
BUT
READING
A
FILE
SEQUENTIALLY
ON
AN
OTHERWISE
IDLE
SYSTEM
WOULD
REDUCE
THE
NUMBER
OF
SEEKS
BY
A
FACTOR
OF
TWO
CONSIDERABLY
IM
PROVING
PERFORMANCE
A
VARIATION
ON
THE
SAME
THEME
IS
TO
TAKE
ACCOUNT
OF
ROTA
TIONAL
POSITIONING
WHEN
ALLOCATING
BLOCKS
THE
SYSTEM
ATTEMPTS
TO
PLACE
CONSECU
TIVE
BLOCKS
IN
A
FILE
IN
THE
SAME
CYLINDER
ANOTHER
PERFORMANCE
BOTTLENECK
IN
SYSTEMS
THAT
USE
I
NODES
OR
ANYTHING
LIKE
THEM
IS
THAT
READING
EVEN
A
SHORT
FILE
REQUIRES
TWO
DISK
ACCESSES
ONE
FOR
THE
I
NODE
AND
ONE
FOR
THE
BLOCK
THE
USUAL
I
NODE
PLACEMENT
IS
SHOWN
IN
FIG
A
HERE
ALL
THE
I
NODES
ARE
NEAR
THE
BEGINNING
OF
THE
DISK
SO
THE
AVERAGE
DISTANCE
BETWEEN
AN
I
NODE
AND
ITS
BLOCKS
WILL
BE
ABOUT
HALF
THE
NUMBER
OF
CYLINDERS
REQUIRING
LONG
SEEKS
FIGURE
A
I
NODES
PLACED
AT
THE
START
OF
THE
DISK
B
DISK
DIVIDED
INTO
CYLINDER
GROUPS
EACH
WITH
ITS
OWN
BLOCKS
AND
I
NODES
ONE
EASY
PERFORMANCE
IMPROVEMENT
IS
TO
PUT
THE
I
NODES
IN
THE
MIDDLE
OF
THE
DISK
RATHER
THAN
AT
THE
START
THUS
REDUCING
THE
AVERAGE
SEEK
BETWEEN
THE
I
NODE
AND
THE
FIRST
BLOCK
BY
A
FACTOR
OF
TWO
ANOTHER
IDEA
SHOWN
IN
FIG
B
IS
TO
DIVIDE
THE
DISK
INTO
CYLINDER
GROUPS
EACH
WITH
ITS
OWN
I
NODES
BLOCKS
AND
FREE
LIST
MCKUSICK
ET
AL
WHEN
CREATING
A
NEW
FILE
ANY
I
NODE
CAN
BE
CHOSEN
BUT
AN
ATTEMPT
IS
MADE
TO
FIND
A
BLOCK
IN
THE
SAME
CYLINDER
GROUP
AS
THE
I
NODE
IF
NONE
IS
AVAILABLE
THEN
A
BLOCK
IN
A
NEARBY
CYLINDER
GROUP
IS
USED
DEFRAGMENTING
DISKS
WHEN
THE
OPERATING
SYSTEM
IS
INITIALLY
INSTALLED
THE
PROGRAMS
AND
FILES
IT
NEEDS
ARE
INSTALLED
CONSECUTIVELY
STARTING
AT
THE
BEGINNING
OF
THE
DISK
EACH
ONE
DI
RECTLY
FOLLOWING
THE
PREVIOUS
ONE
ALL
FREE
DISK
SPACE
IS
IN
A
SINGLE
CONTIGUOUS
UNIT
FILE
SYSTEMS
CHAP
FOLLOWING
THE
INSTALLED
FILES
HOWEVER
AS
TIME
GOES
ON
FILES
ARE
CREATED
AND
RE
MOVED
AND
TYPICALLY
THE
DISK
BECOMES
BADLY
FRAGMENTED
WITH
FILES
AND
HOLES
ALL
OVER
THE
PLACE
AS
A
CONSEQUENCE
WHEN
A
NEW
FILE
IS
CREATED
THE
BLOCKS
USED
FOR
IT
MAY
BE
SPREAD
ALL
OVER
THE
DISK
GIVING
POOR
PERFORMANCE
THE
PERFORMANCE
CAN
BE
RESTORED
BY
MOVING
FILES
AROUND
TO
MAKE
THEM
CON
TIGUOUS
AND
TO
PUT
ALL
OR
AT
LEAST
MOST
OF
THE
FREE
SPACE
IN
ONE
OR
MORE
LARGE
CON
TIGUOUS
REGIONS
ON
THE
DISK
WINDOWS
HAS
A
PROGRAM
DEJRAG
THAT
DOES
PRECISELY
THIS
WINDOWS
USERS
SHOULD
RUN
IT
REGULARLY
DEFRAGMENTATION
WORKS
BETTER
ON
FILE
SYSTEMS
THAT
HAVE
A
FAIR
AMOUNT
OF
FREE
SPACE
IN
A
CONTIGUOUS
REGION
AT
THE
END
OF
THE
PARTITION
THIS
SPACE
ALLOWS
THE
DEFRAGMENTATION
PROGRAM
TO
SELECT
FRAGMENTED
FILES
NEAR
THE
START
OF
THE
PARTITION
AND
COPY
ALL
THEIR
BLOCKS
TO
THE
FREE
SPACE
THIS
ACTION
FREES
UP
A
CONTIGUOUS
BLOCK
OF
SPACE
NEAR
THE
START
OF
THE
PARTITION
INTO
WHICH
THE
ORIGINAL
OR
OTHER
FILES
CAN
BE
PLACED
CONTIGUOUSLY
THE
PROCESS
CAN
THEN
BE
REPEATED
WITH
THE
NEXT
CHUNK
OF
DISK
SPACE
AND
SO
ON
SOME
FILES
CANNOT
BE
MOVED
INCLUDING
THE
PAGING
FILE
THE
HIBERNATION
FILE
AND
THE
JOURNALING
LOG
BECAUSE
THE
ADMINISTRATION
THAT
WOULD
BE
REQUIRED
TO
DO
THIS
IS
MORE
TROUBLE
THAN
IT
IS
WORTH
IN
SOME
SYSTEMS
THESE
ARE
FIXED
SIZE
CONTIG
UOUS
AREAS
ANYWAY
SO
THEY
DO
NOT
HAVE
TO
BE
DEFRAGMENTED
THE
ONE
TIME
WHEN
THEIR
LACK
OF
MOBILITY
IS
A
PROBLEM
IS
WHEN
THEY
HAPPEN
TO
BE
NEAR
THE
END
OF
THE
PARTITION
AND
THE
USER
WANTS
TO
REDUCE
THE
PARTITION
SIZE
THE
ONLY
WAY
TO
SOLVE
THIS
PROBLEM
IS
TO
REMOVE
THEM
ALTOGETHER
RESIZE
THE
PARTITION
AND
THEN
RECREATE
THEM
AFTERWARD
LINUX
FILE
SYSTEMS
ESPECIALLY
AND
GENERALLY
SUFFER
LESS
FROM
DEFRAGMENTATION
THAN
WINDOWS
SYSTEMS
DUE
TO
THE
WAY
DISK
BLOCKS
ARE
SELECTED
SO
MANUAL
DEFRAGMENTATION
IS
RARELY
REQUIRED
EXAMPL
E
FIL
E
SYSTEM
IN
THE
FOLLOWING
SECTIONS
WE
WILL
DISCUSS
SEVERAL
EXAMPLE
FILE
SYSTEMS
RANG
ING
FROM
QUITE
SIMPLE
TO
MORE
SOPHISTICATED
SINCE
MODEM
UNIX
FILE
SYSTEMS
AND
WINDOWS
VISTA
NATIVE
FILE
SYSTEM
ARE
COVERED
IN
THE
CHAPTER
ON
UNIX
CHAP
AND
THE
CHAPTER
ON
WINDOWS
VISTA
CHAP
WE
WILL
NOT
COVER
THOSE
SYSTEMS
HERE
WE
WILL
HOWEVER
EXAMINE
THEIR
PREDECESSORS
BELOW
CD
ROM
FILE
SYSTEMS
AS
OUR
FIRST
EXAMPLE
OF
A
FILE
SYSTEM
LET
US
CONSIDER
THE
FILE
SYSTEMS
USED
ON
CD
ROMS
THESE
SYSTEMS
ARE
PARTICULARLY
SIMPLE
BECAUSE
THEY
WERE
DESIGNED
FOR
WRITE
ONCE
MEDIA
AMONG
OTHER
THINGS
FOR
EXAMPLE
THEY
HAVE
NO
PROVISION
FOR
KEEPING
TRACK
OF
FREE
BLOCKS
BECAUSE
ON
A
CD
ROM
FILES
CANNOT
BE
FREED
OR
ADDED
AFTER
THE
DISK
HAS
BEEN
MANUFACTURED
BELOW
WE
WILL
TAKE
A
LOOK
AT
THE
MAIN
CD
ROM
FILE
SYSTEM
TYPE
AND
TWO
EXTENSIONS
TO
IT
SEC
EXAMPLE
FILE
SYSTEMS
SOME
YEARS
AFTER
THE
CD
ROM
MADE
ITS
DEBUT
THE
CD
R
CD
RECORDABLE
WAS
INTRODUCED
UNLIKE
THE
CD
ROM
IT
IS
POSSIBLE
TO
ADD
FILES
AFTER
THE
INITIAL
BURNING
BUT
THESE
ARE
SIMPLY
APPENDED
TO
THE
END
OF
THE
CD
R
FILES
ARE
NEVER
REMOVED
ALTHOUGH
THE
DIRECTORY
CAN
BE
UPDATED
TO
HIDE
EXISTING
FILES
AS
A
CONSEQUENCE
OF
THIS
APPEND
ONLY
FILE
SYSTEM
THE
FUNDAMENTAL
PROPERTIES
ARE
NOT
ALTERED
IN
PARTICULAR
ALL
THE
FREE
SPACE
IS
IN
ONE
CONTIGUOUS
CHUNK
AT
THE
END
OF
THE
CD
THE
ISO
FILE
SYSTEM
THE
MOST
COMMON
STANDARD
FOR
CD
ROM
FILE
SYSTEMS
WAS
ADOPTED
AS
AN
INTERNATIONAL
STANDARD
IN
UNDER
THE
NAME
ISO
VIRTUALLY
EVERY
CD
ROM
CURRENTLY
ON
THE
MARKET
IS
COMPATIBLE
WITH
THIS
STANDARD
SOMETIMES
WITH
THE
EXTENSIONS
TO
BE
DISCUSSED
BELOW
ONE
OF
THE
GOALS
OF
THIS
STANDARD
WAS
TO
MAKE
EVERY
CD
ROM
READABLE
ON
EVERY
COMPUTER
INDEPENDENT
OF
THE
BYTE
ORDER
ING
USED
AND
INDEPENDENT
OF
THE
OPERATING
SYSTEM
USED
AS
A
CONSEQUENCE
SOME
LIMITATIONS
WERE
PLACED
ON
THE
FILE
SYSTEM
TO
MAKE
IT
POSSIBLE
FOR
THE
WEAKEST
OP
ERATING
SYSTEMS
THEN
IN
USE
SUCH
AS
MS
DOS
TO
READ
IT
CD
ROMS
DO
NOT
HAVE
CONCENTRIC
CYLINDERS
THE
WAY
MAGNETIC
DISKS
DO
IN
STEAD
THERE
IS
A
SINGLE
CONTINUOUS
SPIRAL
CONTAINING
THE
BITS
IN
A
LINEAR
SEQUENCE
ALTHOUGH
SEEKS
ACROSS
DIE
SPIRAL
ARE
POSSIBLE
THE
BITS
ALONG
THE
SPIRAL
ARE
DIVID
ED
INTO
LOGICAL
BLOCKS
ALSO
CALLED
LOGICAL
SECTORS
OF
BYTES
SOME
OF
THESE
ARE
FOR
PREAMBLES
ERROR
CORRECTION
AND
OTHER
OVERHEAD
THE
PAYLOAD
PORTION
OF
EACH
LOGICAL
BLOCK
IS
BYTES
WHEN
USED
FOR
MUSIC
CDS
HAVE
LEADINS
IEADOUTS
AND
INTERTRACK
GAPS
BUT
THESE
ARE
NOT
USED
FOR
DATA
CD
ROMS
OFTEN
THE
POSITION
OF
A
BLOCK
ALONG
THE
SPIRAL
IS
QUOTED
IN
MINUTES
AND
SECONDS
IT
CAN
BE
CONVERTED
TO
A
LINEAR
BLOCK
NUMBER
USING
THE
CONVERSION
FACTOR
OF
SEC
BLOCKS
ISO
SUPPORTS
CD
ROM
SETS
WITH
AS
MANY
AS
CDS
IN
THE
SET
THE
INDIVIDUAL
CD
ROMS
MAY
ALSO
BE
PARTITIONED
INTO
LOGICAL
VOLUMES
PARTITIONS
HOWEVER
BELOW
WE
WILL
CONCENTRATE
ON
ISO
FOR
A
SINGLE
UNPARTITIONED
CD
ROM
EVERY
CD
ROM
BEGINS
WITH
BLOCKS
WHOSE
FUNCTION
IS
NOT
DEFINED
BY
THE
ISO
STANDARD
A
CD
ROM
MANUFACTURER
COULD
USE
THIS
AREA
FOR
PROVIDING
A
BOOTSTRAP
PROGRAM
TO
ALLOW
THE
COMPUTER
TO
BE
BOOTED
FROM
THE
CD
ROM
OR
FOR
SOME
OTHER
PURPOSE
NEXT
COMES
ONE
BLOCK
CONTAINING
THE
PRIMARY
VOLUME
DESCRIPTOR
WHICH
CONTAINS
SOME
GENERAL
INFORMATION
ABOUT
THE
CD
ROM
THIS
INFORMATION
INCLUDES
THE
SYSTEM
IDENTIFIER
BYTES
VOLUME
IDENTIFIER
BYTES
PUBLISHER
IDENTIFIER
BYTES
AND
DATA
PREPARER
IDENTIFIER
BYTES
THE
MANUFACTURER
CAN
FILL
IN
THESE
FIELDS
IN
ANY
DESIRED
WAY
EXCEPT
THAT
ONLY
UPPER
CASE
LETTERS
DIGITS
AND
A
VERY
SMALL
NUMBER
OF
PUNCTUATION
MARKS
MAY
BE
USED
TO
ENSURE
CROSS
PLATFORM
COMPATIBILITY
FILE
SYSTEMS
CHAP
THE
PRIMARY
VOLUME
DESCRIPTOR
ALSO
CONTAINS
THE
NAMES
OF
THREE
FILES
WHICH
MAY
CONTAIN
THE
ABSTRACT
COPYRIGHT
NOTICE
AND
BIBLIOGRAPHIC
INFORMATION
RESPEC
TIVELY
IN
ADDITION
CERTAIN
KEY
NUMBERS
ARE
ALSO
PRESENT
INCLUDING
THE
LOGICAL
BLOCK
SIZE
NORMALLY
BUT
AND
LARGER
POWERS
OF
TWO
ARE
ALLOWED
IN
CERTAIN
CASES
THE
NUMBER
OF
BLOCKS
ON
THE
CD
ROM
AND
THE
CREATION
AND
EXPIRATION
DATES
OF
THE
CD
ROM
FINALLY
THE
PRIMARY
VOLUME
DESCRIPTOR
ALSO
CONTAINS
A
DIRECTORY
ENTRY
FOR
THE
ROOT
DIRECTORY
TELLING
WHERE
TO
FIND
IT
ON
THE
CD
ROM
I
E
WHICH
BLOCK
IT
STARTS
AT
FROM
THIS
DIRECTORY
THE
REST
OF
THE
FILE
SYSTEM
CAN
BE
LOCATED
IN
ADDITION
TO
THE
PRIMARY
VOLUME
DESCRIPTOR
A
CD
ROM
MAY
CONTAIN
A
SUP
PLEMENTARY
VOLUME
DESCRIPTOR
IT
CONTAINS
SIMILAR
INFORMATION
TO
THE
PRIMARY
BUT
THAT
WILL
NOT
CONCERN
US
HERE
THE
ROOT
DIRECTORY
AND
ALL
OTHER
DIRECTORIES
FOR
THAT
MATTER
CONSISTS
OF
A
VARI
ABLE
NUMBER
OF
ENTRIES
THE
LAST
OF
WHICH
CONTAINS
A
BIT
MARKING
IT
AS
THE
FINAL
ONE
THE
DIRECTORY
ENTRIES
THEMSELVES
ARE
ALSO
VARIABLE
LENGTH
EACH
DIRECTORY
ENTRY
CONSISTS
OF
TO
FIELDS
SOME
OF
WHICH
ARE
IN
ASCII
AND
OTHERS
OF
WHICH
ARE
NUMERICAL
FIELDS
IN
BINARY
THE
BINARY
FIELDS
ARE
ENCODED
TWICE
ONCE
IN
LITTLE
ENDIAN
FORMAT
USED
ON
PENTIUMS
FOR
EXAMPLE
AND
ONCE
IN
BIG
ENDIAN
FORMAT
USED
ON
SPARCS
FOR
EXAMPLE
THUS
A
BIT
NUMBER
USES
BYTES
AND
A
BIT
NUMBER
USES
BYTES
THE
USE
OF
THIS
REDUNDANT
CODING
WAS
NECESSARY
TO
AVOID
HURTING
ANYONE
FEELINGS
WHEN
THE
STANDARD
WAS
DEVELOPED
IF
THE
STANDARD
HAD
DICTATED
LITTLE
END
IAN
THEN
PEOPLE
FROM
COMPANIES
WHOSE
PRODUCTS
WERE
BIG
ENDIAN
WOULD
HAVE
FELT
LIKE
SECOND
CLASS
CITIZENS
AND
WOULD
NOT
HAVE
ACCEPTED
THE
STANDARD
THE
EMO
TIONAL
CONTENT
OF
A
CD
ROM
CAN
THUS
BE
QUANTIFIED
AND
MEASURED
EXACTLY
IN
KILOBYTES
HOUR
OF
WASTED
SPACE
THE
FORMAT
OF
AN
ISO
DIRECTORY
ENTRY
IS
ILLUSTRATED
IN
FIG
SINCE
DIRECTORY
ENTRIES
HAVE
VARIABLE
LENGTHS
THE
FIRST
FIELD
IS
A
BYTE
TELLING
HOW
LONG
THE
ENTRY
IS
THIS
BYTE
IS
DEFINED
TO
HAVE
THE
HIGH
ORDER
BIT
ON
THE
LEFT
TO
AVOID
ANY
AMBIGUITY
PADDING
SE
C
EXAMPLE
FILE
SYSTEMS
NEXT
COMES
THE
STARTING
BLOCK
OF
THE
FILE
ITSELF
FILES
ARE
STORED
AS
CONTIGUOUS
RUNS
OF
BLOCKS
SO
A
FILE
LOCATION
IS
COMPLETELY
SPECIFIED
BY
THE
STARTING
BLOCK
AND
THE
SIZE
WHICH
IS
CONTAINED
IN
THE
NEXT
FIELD
THE
DATE
AND
TIME
THAT
THE
CD
ROM
WAS
RECORDED
IS
STORED
IN
THE
NEXT
FIELD
WITH
SEPARATE
BYTES
FOR
THE
YEAR
MONTH
DAY
HOUR
MINUTE
SECOND
AND
TIME
ZONE
YEARS
BEGIN
TO
COUNT
AT
WHICH
MEANS
THAT
CD
ROMS
WILL
SUFFER
FROM
A
PROBLEM
BECAUSE
THE
YEAR
FOLLOWING
WILL
BE
THIS
PROBLEM
COULD
HAVE
BEEN
DELAYED
BY
DEFINING
THE
ORIGIN
OF
TIME
TO
BE
THE
YEAR
THE
STANDARD
WAS
ADOPTED
HAD
THAT
BEEN
DONE
THE
PROBLEM
WOULD
HAVE
BEEN
POST
PONED
UNTIL
EVERY
EXTRA
YEARS
HELPS
THE
FLAGS
FIELD
CONTAINS
A
FEW
MISCELLANEOUS
BITS
INCLUDING
ONE
TO
HIDE
THE
ENTRY
IN
LISTINGS
A
FEATURE
COPIED
FROM
MS
DOS
ONE
TO
DISTINGUISH
AN
ENTRY
THAT
IS
A
FILE
FROM
AN
ENTRY
THAT
IS
A
DIRECTORY
ONE
TO
ENABLE
THE
USE
OF
THE
EXTENDED
AT
TRIBUTES
AND
ONE
TO
MARK
THE
LAST
ENTRY
IN
A
DIRECTORY
A
FEW
OTHER
BITS
ARE
ALSO
PRESENT
IN
THIS
FIELD
BUT
THEY
WILL
NOT
CONCERN
US
HERE
THE
NEXT
FIELD
DEALS
WITH
INTERLEAVING
PIECES
OF
FILES
IN
A
WAY
THAT
IS
NOT
USED
IN
THE
SIMPLEST
VERSION
OF
ISO
SO
WE
WILL
NOT
CONSIDER
IT
FURTHER
THE
NEXT
FIELD
TELLS
WHICH
CD
ROM
THE
FILE
IS
LOCATED
ON
IT
IS
PERMITTED
THAT
A
DIRECTORY
ENTRY
ON
ONE
CD
ROM
REFERS
TO
A
FILE
LOCATED
ON
ANOTHER
CD
ROM
IN
THE
SET
IN
THIS
WAY
IT
IS
POSSIBLE
TO
BUILD
A
MASTER
DIRECTORY
ON
THE
FIRST
CD
ROM
THAT
LISTS
ALL
THE
FILES
ON
ALL
THE
CD
ROMS
IN
THE
COMPLETE
SET
THE
FIELD
MARKED
L
IN
FIG
GIVES
THE
SIZE
OF
THE
FILE
NAME
IN
BYTES
IT
IS
FOLLOWED
BY
THE
FILE
NAME
ITSELF
A
FILE
NAME
CONSISTS
OF
A
BASE
NAME
A
DOT
AN
EXTENSION
A
SEMICOLON
AND
A
BINARY
VERSION
NUMBER
OR
BYTES
THE
BASE
NAME
AND
EXTENSION
MAY
USE
UPPER
CASE
LETTERS
THE
DIGITS
AND
THE
UNDERSCORE
CHARACTER
ALL
OTHER
CHARACTERS
ARE
FORBIDDEN
TO
MAKE
SURE
THAT
EVERY
COMPUTER
CAN
HANDLE
EVERY
FILE
NAME
THE
BASE
NAME
CAN
BE
UP
TO
EIGHT
CHARACTERS
THE
EXTEN
SION
CAN
BE
UP
TO
THREE
CHARACTERS
THESE
CHOICES
WERE
DICTATED
BY
THE
NEED
TO
BE
MS
DOS
COMPATIBLE
A
FILE
NAME
MAY
BE
PRESENT
IN
A
DIRECTORY
MULTIPLE
TIMES
AS
LONG
AS
EACH
ONE
HAS
A
DIFFERENT
VERSION
NUMBER
THE
LAST
TWO
FIELDS
ARE
NOT
ALWAYS
PRESENT
THE
PADDING
FIELD
IS
USED
TO
FORCE
EVERY
DIRECTORY
ENTRY
TO
BE
AN
EVEN
NUMBER
OF
BYTES
TO
ALIGN
THE
NUMERIC
FIELDS
OF
SUBSEQUENT
ENTRIES
ON
BYTE
BOUNDARIES
IF
PADDING
IS
NEEDED
A
BYTE
IS
USED
BYTES
A
FINALLY
WE
HAVE
THE
SYSTEM
USE
FIELD
ITS
FUNCTION
AND
SIZE
ARE
UNDEFINED
EXCEPT
LOCATION
OF
FILE
FILE
SIZE
DATE
AND
TIME
GD
FILE
NAME
J
SYS
THAT
IT
MUST
BE
AN
EVEN
NUMBER
OF
BYTES
DIFFERENT
SYSTEMS
USE
IT
IN
DIFFERENT
WAYS
A
FFAGS
T
EXTENDED
ATTRIBUTE
RECORD
LENGTH
INTERLEAVE
I
BASE
NAME
DIRECTORY
ENTRY
LENGTH
FIGURE
THE
ISO
DIRECTORY
EIITY
EXT
VER
THE
MACINTOSH
KEEPS
FINDER
FLAGS
HERE
FOR
EXAMPLE
ENTRIES
WITHIN
A
DIRECTORY
ARE
LISTED
IN
ALPHABETICAL
ORDER
EXCEPT
FOR
THE
FIRST
TWO
ENTRIES
THE
FIRST
ENTRY
IS
FOR
THE
DIRECTORY
ITSELF
THE
SECOND
ONE
IS
FOR
ITS
PAR
ENT
IN
THIS
RESPECT
THESE
ENTRIES
ARE
SIMILAR
TO
THE
UNIX
AND
DIRECTORY
ENTRIES
THE
FILES
THEMSELVES
NEED
NOT
BE
IN
DIRECTORY
ORDER
THERE
IS
NO
EXPLICIT
LIMIT
TO
THE
NUMBER
OF
ENTRIES
IN
A
DIRECTORY
HOWEVER
DIRECTORY
ENTRIES
MAY
OPTIONALLY
HAVE
EXTENDED
ATTRIBUTES
IF
THIS
FEATURE
IS
USED
THE
SECOND
BYTE
TELLS
HOW
LONG
THE
EXTENDED
ATTRIBUTES
ARE
THERE
IS
A
LIMIT
TO
THE
DEPTH
OF
NESTING
THE
MAXIMUM
DEPTH
OF
DIRECTORY
NESTING
IS
EIGHT
THIS
LIMIT
WAS
ARBITRARILY
SET
TO
MAKE
SOME
IMPLEMENTATIONS
SIMPLER
FILE
SYSTEMS
CHAP
ISO
DEFINES
WHAT
ARE
CALLED
THREE
LEVELS
LEVEL
IS
THE
MOST
RESTRICTIVE
AND
SPECIFIES
THAT
FILE
NAMES
ARE
LIMITED
TO
CHARACTERS
AS
WE
HAVE
DESCRIBED
AND
ALSO
REQUIRES
ALL
FILES
TO
BE
CONTIGUOUS
AS
WE
HAVE
DESCRIBED
FURTHERMORE
IT
SPECIFIES
THAT
DIRECTORY
NAMES
BE
LIMITED
TO
EIGHT
CHARACTERS
WITH
NO
EXTENSIONS
USE
OF
THIS
LEVEL
MAXIMIZES
THE
CHANCES
THAT
A
CD
ROM
CAN
BE
READ
ON
EVERY
COMPUTER
LEVEL
RELAXES
THE
LENGTH
RESTRICTION
IT
ALLOWS
FILES
AND
DIRECTORIES
TO
HAVE
NAMES
OF
UP
TO
CHARACTERS
BUT
STILL
FROM
THE
SAME
SET
OF
CHARACTERS
LEVEL
USES
THE
SAME
NAME
LIMITS
AS
LEVEL
BUT
PARTIALLY
RELAXES
THE
ASSUMP
TION
THAT
FILES
HAVE
TO
BE
CONTIGUOUS
WITH
THIS
LEVEL
A
FILE
MAY
CONSIST
OF
SEVERAL
SECTIONS
EXTENTS
EACH
OF
WHICH
IS
A
CONTIGUOUS
RUN
OF
BLOCKS
THE
SAME
RUN
MAY
OCCUR
MULTIPLE
TIMES
IN
A
FILE
AND
MAY
ALSO
OCCUR
IN
TWO
OR
MORE
FILES
IF
LARGE
CHUNKS
OF
DATA
ARE
REPEATED
IN
SEVERAL
FILES
LEVEL
PROVIDES
SOME
SPACE
OPTIMIZA
TION
BY
NOT
REQUIRING
THE
DATA
TO
BE
PRESENT
MULTIPLE
TIMES
ROCK
RIDGE
EXTENSIONS
AS
WE
HAVE
SEEN
ISO
IS
HIGHLY
RESTRICTIVE
IN
SEVERAL
WAYS
SHORTLY
AFTER
IT
CAME
OUT
PEOPLE
IN
THE
UNIX
COMMUNITY
BEGAN
WORKING
ON
AN
EXTENSION
TO
MAKE
IT
POSSIBLE
TO
REPRESENT
UNIX
FILE
SYSTEMS
ON
A
CD
ROM
THESE
EXTENSIONS
WERE
NAMED
ROCK
RIDGE
AFTER
A
TOWN
IN
THE
GENE
WILDER
MOVIE
BLAZING
SADDLES
PROBABLY
BECAUSE
ONE
OF
THE
COMMITTEE
MEMBERS
LIKED
THE
FILM
THE
EXTENSIONS
USE
THE
SYSTEM
USE
FIELD
IN
ORDER
TO
MAKE
ROCK
RIDGE
CD
ROMS
READABLE
ON
ANY
COMPUTER
ALL
THE
OTHER
FIELDS
RETAIN
THEIR
NORMAL
ISO
MEANING
ANY
SYSTEM
NOT
AWARE
OF
THE
ROCK
RIDGE
EXTENSIONS
JUST
IGNORES
THEM
AND
SEES
A
NORMAL
CD
ROM
THE
EXTENSIONS
ARE
DIVIDED
UP
INTO
THE
FOLLOWING
FIELDS
PX
POSIX
ATTRIBUTES
PN
MAJOR
AND
MINOR
DEVICE
NUMBERS
SL
SYMBOLIC
LINK
NM
ALTERNATIVE
NAME
CL
CHILD
LOCATION
PL
PARENT
LOCATION
RE
RELOCATION
TF
TIME
STAMPS
THE
PX
FIELD
CONTAINS
THE
STANDARD
UNIX
RWXRWXRWX
PERMISSION
BITS
FOR
THE
OWNER
GROUP
AND
OTHERS
IT
ALSO
CONTAINS
THE
OTHER
BITS
CONTAINED
IN
THE
MODE
WORD
SUCH
AS
THE
SETUID
AND
SETGID
BITS
AND
SO
ON
SEC
EXAMPLE
FILE
SYSTEMS
TO
ALLOW
RAW
DEVICES
TO
BE
REPRESENTED
ON
A
CD
ROM
THE
PN
FIELD
IS
PRES
ENT
IT
CONTAINS
THE
MAJOR
AND
MINOR
DEVICE
NUMBERS
ASSOCIATED
WITH
THE
FILE
IN
THIS
WAY
THE
CONTENTS
OF
THE
DEV
DIRECTORY
CAN
BE
WRITTEN
TO
A
CD
ROM
AND
LATER
RECONSTRUCTED
CORRECTLY
ON
THE
TARGET
SYSTEM
THE
SL
FIELD
IS
FOR
SYMBOLIC
LINKS
IT
ALLOWS
A
FILE
ON
ONE
FILE
SYSTEM
TO
REFER
TO
A
FILE
ON
A
DIFFERENT
FILE
SYSTEM
PROBABLY
THE
MOST
IMPORTANT
FIELD
IS
MM
IT
ALLOWS
A
SECOND
NAME
TO
BE
ASSO
CIATED
WITH
THE
FILE
THIS
NAME
IS
NOT
SUBJECT
TO
THE
CHARACTER
SET
OR
LENGTH
RESTRIC
TIONS
OF
ISO
MAKING
IT
POSSIBLE
TO
EXPRESS
ARBITRARY
UNIX
FILE
NAMES
ON
A
CD
ROM
THE
NEXT
THREE
FIELDS
ARE
USED
TOGETHER
TO
GET
AROUND
THE
ISO
LIMIT
OF
DI
RECTORIES
THAT
MAY
ONLY
BE
NESTED
EIGHT
DEEP
USING
THEM
IT
IS
POSSIBLE
TO
SPECIFY
THAT
A
DIRECTORY
IS
TO
BE
RELOCATED
AND
TO
TELL
WHERE
IT
GOES
IN
THE
HIERARCHY
IT
IS
EFFECTIVELY
A
WAY
TO
WORK
AROUND
THE
ARTIFICIAL
DEPTH
LIMIT
FINALLY
THE
TF
FIELD
CONTAINS
THE
THREE
TIMESTAMPS
INCLUDED
IN
EACH
UNIX
I
NODE
NAMELY
THE
TIME
THE
FILE
WAS
CREATED
THE
TIME
IT
WAS
LAST
MODIFIED
AND
THE
TIME
IT
WAS
LAST
ACCESSED
TOGETHER
THESE
EXTENSIONS
MAKE
IT
POSSIBLE
TO
COPY
A
UNIX
FILE
SYSTEM
TO
A
CD
ROM
AND
THEN
RESTORE
IT
CORRECTLY
TO
A
DIFFERENT
SYSTEM
JOLIET
EXTENSIONS
THE
UNIX
COMMUNITY
WAS
NOT
THE
ONLY
GROUP
THAT
WANTED
A
WAY
TO
EXTEND
ISO
MICROSOFT
ALSO
FOUND
IT
TOO
RESTRICTIVE
ALTHOUGH
IT
WAS
MICROSOFT
OWN
MS
DOS
THAT
CAUSED
MOST
OF
THE
RESTRICTIONS
IN
THE
FIRST
PLACE
THEREFORE
MICROSOFT
INVENTED
SOME
EXTENSIONS
THAT
WERE
CALLED
JOLIET
THEY
WERE
DESIGNED
TO
ALLOW
WINDOWS
FILE
SYSTEMS
TO
BE
COPIED
TO
CD
ROM
AND
THEN
RESTORED
IN
PRE
CISELY
THE
SAME
WAY
THAT
ROCK
RIDGE
WAS
DESIGNED
FOR
UNIX
VIRTUALLY
ALL
PRO
GRAMS
THAT
RUN
UNDER
WINDOWS
AND
USE
CD
ROMS
SUPPORT
JOLIET
INCLUDING
PRO
GRAMS
THAT
BUM
CD
RECORDABLES
USUALLY
THESE
PROGRAMS
OFFER
A
CHOICE
BETWEEN
THE
VARIOUS
ISO
LEVELS
AND
JOLIET
THE
MAJOR
EXTENSIONS
PROVIDED
BY
JOLIET
ARE
LONG
FILE
NAMES
UNICODE
CHARACTER
SET
DIRECTORY
NESTING
DEEPER
THAN
EIGHT
LEVELS
DIRECTORY
NAMES
WITH
EXTENSIONS
THE
FIRST
EXTENSION
ALLOWS
FILE
NAMES
UP
TO
CHARACTERS
THE
SECOND
EXTENSION
ENABLES
THE
USE
OF
THE
UNICODE
CHARACTER
SET
FOR
FILE
NAMES
THIS
EXTENSION
IS
IM
PORTANT
FOR
SOFTWARE
INTENDED
FOR
USE
IN
COUNTRIES
THAT
DO
NOT
USE
THE
LATIN
ALPHA
BET
SUCH
AS
JAPAN
ISRAEL
AND
GREECE
SINCE
UNICODE
CHARACTERS
ARE
BYTES
THE
MAXIMUM
FILE
NAME
IN
JOLIET
OCCUPIES
BYTES
FILE
SYSTEMS
CHAP
LIKE
ROCK
RIDGE
THE
LIMITATION
ON
DIRECTORY
NESTING
IS
REMOVED
BY
JOLIET
DI
RECTORIES
CAN
BE
NESTED
AS
DEEPLY
AS
NEEDED
FINALLY
DIRECTORY
NAMES
CAN
HAVE
EXTENSIONS
IT
IS
NOT
CLEAR
WHY
THIS
EXTENSION
WAS
INCLUDED
SINCE
WINDOWS
DIREC
TORIES
VIRTUALLY
NEVER
USE
EXTENSIONS
BUT
MAYBE
SOME
DAY
THEY
WILL
THE
MS
DOS
FILE
SYSTEM
SEC
EXAMPLE
FILE
SYSTEMS
BYTES
FILE
NAME
EXTENSION
ATTRIBUTES
RESERVED
TIME
DATE
FIRST
BLOCK
NUMBER
THE
MS
DOS
FILE
SYSTEM
IS
THE
ONE
THE
FIRST
IBM
PCS
CAME
WITH
IT
WAS
THE
MAIN
FILE
SYSTEM
UP
THROUGH
WINDOWS
AND
WINDOWS
ME
IT
IS
STILL
SUPPORTED
ON
WINDOWS
WINDOWS
XP
AND
WINDOWS
VISTA
ALTHOUGH
IT
IS
NO
LONGER
STANDARD
ON
NEW
PCS
NOW
EXCEPT
FOR
FLOPPY
DISKS
HOWEVER
IT
AND
AN
EXTENSION
OF
IT
FAT
HAVE
BECOME
WIDELY
USED
FOR
MANY
EMBEDDED
SYSTEMS
MOST
DIGI
TAL
CAMERAS
USE
IT
MANY
PLAYERS
USE
IT
EXCLUSIVELY
THE
POPULAR
APPLE
IPOD
USES
IT
AS
THE
DEFAULT
FILE
SYSTEM
ALTHOUGH
KNOWLEDGEABLE
HACKERS
CAN
REFORMAT
THE
IPOD
AND
INSTATTA
DIFFERENT
FILE
SYSTEM
THUS
THE
NUMBER
OF
ELECTRONIC
DEVICES
USING
THE
MS
DOS
FILE
SYSTEM
IS
VASTLY
LARGER
NOW
THAN
AT
ANY
TIME
IN
THE
PAST
AND
CERTAINLY
MUCH
LARGER
THAN
THE
NUMBER
USING
THE
MORE
MODERN
NTFS
FILE
SYSTEM
FOR
THAT
REASON
ALONE
IT
IS
WORTH
LOOKING
AT
IN
SOME
DETAIL
TO
READ
A
FILE
AN
MS
DOS
PROGRAM
MUST
FIRST
MAKE
AN
OPEN
SYSTEM
CALL
TO
GET
A
HANDLE
FOR
IT
THE
OPEN
SYSTEM
CALL
SPECIFIES
A
PATH
WHICH
MAY
BE
EITHER
ABSO
LUTE
OR
RELATIVE
TO
THE
CURRENT
WORKING
DIRECTORY
THE
PATH
IS
LOOKED
UP
COMPONENT
BY
COMPONENT
UNTIL
THE
FINAL
DIRECTORY
IS
LOCATED
AND
READ
INTO
MEMORY
IT
IS
THEN
SEARCHED
FOR
THE
FILE
TO
BE
OPENED
ALTHOUGH
MS
DOS
DIRECTORIES
ARE
VARIABLE
SIZED
THEY
USE
A
FIXED
SIZE
BYTE
DIRECTORY
ENTRY
THE
FORMAT
OF
AN
MS
DOS
DIRECTORY
ENTRY
IS
SHOWN
IN
FIG
IT
CONTAINS
THE
FILE
NAME
ATTRIBUTES
CREATION
DATE
AND
TIME
STARTING
BLOCK
AND
EXACT
FILE
SIZE
FILE
NAMES
SHORTER
THAN
CHARACTERS
ARE
LEFT
JUSTIFIED
AND
PADDED
WITH
SPACES
ON
THE
RIGHT
IN
EACH
FIELD
SEPARATELY
THE
ATTRIBUTES
FIELD
IS
NEW
AND
CONTAINS
BITS
TO
INDICATE
THAT
A
FILE
IS
READ
ONLY
NEEDS
TO
BE
ARCHIVED
IS
HIDDEN
OR
IS
A
SYSTEM
FILE
READ
ONLY
FILES
CANNOT
BE
WRITTEN
THIS
IS
TO
PROTECT
THEM
FROM
ACCIDENTAL
DAMAGE
THE
ARCHIVED
BIT
HAS
NO
ACTUAL
OPERATING
SYSTEM
FUNCTION
I
E
MS
DOS
DOES
NOT
EXAMINE
OR
SET
IT
THE
INTENTION
IS
TO
ALLOW
USER
LEVEL
ARCHIVE
PROGRAMS
TO
CLEAR
IT
UPON
ARCHIVING
A
FILE
AND
TO
HAVE
OTHER
PROGRAMS
SET
IT
WHEN
MODIFYING
A
FILE
IN
THIS
WAY
A
BACKUP
PROGRAM
CAN
JUST
EXAMINE
THIS
ATTRIBUTE
BIT
ON
EVERY
FILE
TO
SEE
WHICH
FILES
TO
BACK
UP
THE
HIDDEN
BIT
CAN
BE
SET
TO
PREVENT
A
FILE
FROM
APPEARING
IN
DIRECTORY
LISTINGS
ITS
MAIN
USE
IS
TO
AVOID
CONFUSING
NOVICE
USERS
WITH
FILES
THEY
MIGHT
NOT
UNDERSTAND
FINALLY
THE
SYSTEM
BIT
ALSO
HIDES
FILES
IN
ADDITION
SYSTEM
FILES
CANNOT
ACCIDENTALLY
BE
DELETED
USING
THE
DEL
COMMAND
THE
MAIN
COMPONENTS
OF
MS
DOS
HAVE
THIS
BIT
SET
THE
DIRECTORY
ENTRY
ALSO
CONTAINS
THE
DATE
AND
TIME
THE
FILE
WAS
CREATED
OR
LAST
MODIFIED
THE
TIME
IS
ACCURATE
ONLY
TO
SEC
BECAUSE
IT
IS
STORED
IN
A
BYTE
FIELD
WHICH
CAN
STORE
ONLY
UNIQUE
VALUES
A
DAY
CONTAINS
SECONDS
THE
FIGURE
THE
MS
DOS
DIRECTORY
ENTRY
TIME
FIELD
IS
SUBDIVIDED
INTO
SECONDS
BITS
MINUTES
BITS
AND
HOURS
BITS
THE
DATE
COUNTS
IN
DAYS
USING
THREE
SUBFIELDS
DAY
BITS
MONTH
BITS
AND
YEAR
BITS
WITH
A
BIT
NUMBER
FOR
THE
YEAR
AND
TIME
BEGINNING
IN
THE
HIGHEST
EXPRESSIBLE
YEAR
IS
THUS
MS
DOS
HAS
A
BUILT
IN
PROBLEM
TO
AVOID
CATASTROPHE
MS
DOS
USERS
SHOULD
BEGIN
WITH
COMPLIANCE
AS
EARLY
AS
POSSIBLE
IF
MS
DOS
HAD
USED
THE
COMBINED
DATE
AND
TIME
FIELDS
AS
A
BIT
SECONDS
COUNTER
IT
COULD
HAVE
REPRESENTED
EVERY
SECOND
EXACTLY
AND
DELAYED
THE
CATASTROPHE
UNTIL
MS
DOS
STORES
THE
FILE
SIZE
AS
A
BIT
NUMBER
SO
IN
THEORY
FILES
CAN
BE
AS
LARGE
AS
GB
HOWEVER
OTHER
LIMITS
DESCRIBED
BELOW
RESTRICT
THE
MAXIMUM
FILE
SIZE
TO
GB
OR
LESS
A
SURPRISINGLY
LARGE
PART
OF
THE
ENTRY
BYTES
IS
UNUSED
MS
DOS
KEEPS
TRACK
OF
FILE
BLOCKS
VIA
A
FILE
ALLOCATION
TABLE
IN
MAIN
MEMORY
THE
DIRECTORY
ENTRY
CONTAINS
THE
NUMBER
OF
THE
FIRST
FILE
BLOCK
THIS
NUMBER
IS
USED
AS
AN
INDEX
INTO
A
ENTRY
FAT
IN
MAIN
MEMORY
BY
FOLLOWING
THE
CHAIN
ALL
THE
BLOCKS
CAN
BE
FOUND
THE
OPERATION
OF
THE
FAT
IS
ILLUSTRATED
IN
FIG
THE
FAT
FILE
SYSTEM
COMES
IN
THREE
VERSIONS
FAT
FAT
AND
FAT
DEPENDING
ON
HOW
MANY
BITS
A
DISK
ADDRESS
CONTAINS
ACTUALLY
FAT
IS
SOME
THING
OF
A
MISNOMER
SINCE
ONLY
THE
LOW
ORDER
BITS
OF
THE
DISK
ADDRESSES
ARE
USED
IT
SHOULD
HAVE
BEEN
CALLED
FAT
BUT
POWERS
OF
TWO
SOUND
SO
MUCH
NEATER
FOR
ALL
FATS
THE
DISK
BLOCK
CAN
BE
SET
TO
SOME
MULTIPLE
OF
BYTES
POSSIB
LY
DIFFERENT
FOR
EACH
PARTITION
WITH
THE
SET
OF
ALLOWED
BLOCK
SIZES
CALLED
CLUSTER
SIZES
BY
MICROSOFT
BEING
DIFFERENT
FOR
EACH
VARIANT
THE
FIRST
VERSION
OF
MS
DOS
USED
FAT
WITH
BYTE
BLOCKS
GIVING
A
MAXIMUM
PARTITION
SIZE
OF
X
BYTES
ACTUALLY
ONLY
X
BYTES
BECAUSE
OF
THE
DISK
ADDRESSES
WERE
USED
AS
SPECIAL
MARKERS
SUCH
AS
END
OF
FILE
BAD
BLOCK
ETC
WITH
THESE
PARAMETERS
THE
MAXIMUM
DISK
PARTITION
SIZE
WAS
ABOUT
MB
AND
THE
SIZE
OF
THE
FAT
TABLE
IN
MEMORY
WAS
ENTRIES
OF
BYTES
EACH
USING
A
BIT
TABLE
ENTRY
WOULD
HAVE
BEEN
TOO
SLOW
THIS
SYSTEM
WORKED
WELL
FOR
FLOPPY
DISKS
BUT
WHEN
HARD
DISKS
CAME
OUT
IT
BECAME
A
PROBLEM
MICROSOFT
SOLVED
THE
PROBLEM
BY
ALLOWING
ADDITIONAL
BLOCK
SIZES
OF
KB
KB
AND
KB
THIS
CHANGE
PRESERVED
THE
STRUCTURE
AND
SIZE
OF
THE
FAT
TABLE
BUT
ALLOWED
DISK
PARTITIONS
OF
UP
TO
MB
FILE
SYSTEMS
CHAP
SINCE
MS
DOS
SUPPORTED
FOUR
DISK
PARTITIONS
PER
DISK
DRIVE
THE
NEW
FAT
FDE
SYSTEM
WORKED
UP
TO
MB
DISKS
BEYOND
THAT
SOMETHING
HAD
TO
GIVE
WHAT
HAPPENED
WAS
THE
INTRODUCTION
OF
FAT
WITH
BIT
DISK
POINTERS
ADDITIONALLY
BLOCK
SIZES
OF
KB
KB
AND
KB
WERE
PERMITTED
IS
THE
LARGEST
POWER
OF
TWO
THAT
CAN
BE
REPRESENTED
IN
BITS
THE
FAT
TABLE
NOW
OCCUPIED
KB
OF
MAIN
MEMORY
ALL
THE
TIME
BUT
WITH
THE
LARGER
MEMORIES
BY
THEN
AVAIL
ABLE
IT
WAS
V
IDELY
USED
AND
RAPIDLY
REPLACED
THE
FAT
FILE
SYSTEM
THE
LARGEST
DISK
PARTITION
THAT
CAN
BE
SUPPORTED
BY
FAT
IS
GB
ENTRIES
OF
KB
EACH
AND
THE
LARGEST
DISK
GB
NAMELY
FOUR
PARTITIONS
OF
GB
EACH
FOR
BUSINESS
LETTERS
THIS
LIMIT
IS
NOT
A
PROBLEM
BUT
FOR
STORING
DIGITAL
VIDEO
USING
THE
DV
STANDARD
A
GB
FILE
HOLDS
JUST
OVER
MINUTES
OF
VIDEO
AS
A
CON
SEQUENCE
OF
THE
FACT
THAT
A
PC
DISK
CAN
SUPPORT
ONLY
FOUR
PARTITIONS
THE
LARGEST
VIDEO
THAT
CAN
BE
STORED
ON
A
DISK
IS
ABOUT
MINUTES
NO
MATTER
HOW
LARGE
THE
DISK
IS
THIS
LIMIT
ALSO
MEANS
THAT
THE
LARGEST
VIDEO
THAT
CAN
BE
EDITED
ON
LINE
IS
LESS
THAN
MINUTES
SINCE
BOTH
INPUT
AND
OUTPUT
FILES
ARE
NEEDED
STARTING
WITH
THE
SECOND
RELEASE
OF
WINDOWS
THE
FAT
FILE
SYSTEM
WITH
ITS
BIT
DISK
ADDRESSES
WAS
INTRODUCED
AND
THE
VERSION
OF
MS
DOS
UNDERLYING
WINDOWS
WAS
ADAPTED
TO
SUPPORT
FAT
IN
THIS
SYSTEM
PARTITIONS
COULD
THEORETICALLY
BE
X
BYTES
BUT
THEY
ARE
ACTUALLY
LIMITED
TO
TB
GB
BECAUSE
INTERNALLY
THE
SYSTEM
KEEPS
TRACK
OF
PARTITION
SIZES
IN
BYTE
SECTORS
USING
A
BIT
NUMBER
AND
X
IS
TB
THE
MAXIMUM
PARTITION
SIZE
FOR
VAR
IOUS
BLOCK
SIZES
AND
AIL
THREE
FAT
TYPES
IS
SHOWN
IN
FIG
BLOCK
SIZE
FAT
FAT
FAT
KB
KB
B
MB
MB
TB
TB
MB
TB
KB
MB
TB
FIGURE
MAXIMUM
PARTITION
SIZE
FOR
DIFFERENT
BLOCK
SIZES
THE
EMPTY
BOXES
REPRESENT
FORBIDDEN
COMBINATIONS
IN
ADDITION
TO
SUPPORTING
LARGER
DISKS
THE
FAT
FILE
SYSTEM
HAS
TWO
OTHER
ADVANTAGES
OVER
FAT
FIRST
AN
GB
DISK
USING
FAT
CAN
BE
A
SINGLE
PARTI
TION
USING
FAT
IT
HAS
TO
BE
FOUR
PARTITIONS
WHICH
APPEARS
TO
THE
WINDOWS
USER
AS
THE
C
D
E
AND
F
LOGICAL
DISK
DRIVES
IT
IS
UP
TO
THE
USER
TO
DECIDE
WHICH
FILE
TO
PLACE
ON
WHICH
DRIVE
AND
KEEP
TRACK
OF
WHAT
IS
WHERE
THE
OTHER
ADVANTAGE
OF
FAT
OVER
FAT
IS
THAT
FOR
A
GIVEN
SIZE
DISK
PAR
TITION
A
SMALLER
BLOCK
SIZE
CAN
BE
USED
FOR
EXAMPLE
FOR
A
GB
DISK
PARTITION
SEC
EXAMPLE
FILE
SYSTEMS
FAT
MUST
USE
KB
BLOCKS
OTHERWISE
WITH
ONLY
AVAILABLE
DISK
AD
DRESSES
IT
CANNOT
COVER
THE
WHOLE
PARTITION
IN
CONTRAST
FAT
CAN
USE
FOR
EX
AMPLE
KB
BLOCKS
FOR
A
GB
DISK
PARTITION
THE
ADVANTAGE
OF
THE
SMALLER
BLOCK
SIZE
IS
THAT
MOST
FILES
ARE
MUCH
SHORTER
THAN
KB
IF
THE
BLOCK
SIZE
IS
KB
A
FILE
OF
BYTES
TIES
UP
KB
OF
DISK
SPACE
IF
THE
AVERAGE
FILE
IS
SAY
KB
THEN
WITH
A
KB
BLOCK
OF
THE
DISK
WILL
BE
WASTED
NOT
A
TERRIBLY
EFFICIENT
WAY
TO
USE
THE
DISK
WITH
AN
KB
FILE
AND
A
KB
BLOCK
THERE
IS
NO
DISK
WASTAGE
BUT
THE
PRICE
PAID
IS
MORE
RAM
EATEN
UP
BY
THE
FAT
WITH
A
KB
BLOCK
AND
A
GB
DISK
PARTITION
THERE
ARE
BLOCKS
SO
THE
FAT
MUST
HAVE
ENTRIES
IN
MEM
ORY
OCCUPYING
MB
OF
RAM
MS
DOS
USES
THE
FAT
TO
KEEP
TRACK
OF
FREE
DISK
BLOCKS
ANY
BLOCK
THAT
IS
NOT
CURRENTLY
ALLOCATED
IS
MARKED
WITH
A
SPECIAL
CODE
WHEN
MS
DOS
NEEDS
A
NEW
DISK
BLOCK
IT
SEARCHES
THE
FAT
FOR
AN
ENTRY
CONTAINING
THIS
CODE
THUS
NO
BITMAP
OR
FREE
LIST
IS
REQUIRED
TH
E
UNIX
FILE
SYSTEM
EVEN
EARLY
VERSIONS
OF
UNIX
HAD
A
FAIRLY
SOPHISTICATED
MULTIUSER
FILE
SYSTEM
SINCE
IT
WAS
DERIVED
FROM
MULTICS
BELOW
WE
WILL
DISCUSS
THE
FILE
SYSTEM
THE
ONE
FOR
THE
PDP
THAT
MADE
UNIX
FAMOUS
WE
WILL
EXAMINE
A
MODEM
UNIX
FILE
SYSTEM
IN
THE
CONTEXT
OF
LINUX
IN
CHAP
THE
FILE
SYSTEM
IS
IN
THE
FORM
OF
A
TREE
STARTING
AT
THE
ROOT
DIRECTORY
WITH
THE
ADDITION
OF
LINKS
FORMING
A
DIRECTED
ACYCLIC
GRAPH
FILE
NAMES
ARE
UP
TO
CHAR
ACTERS
AND
CAN
CONTAIN
ANY
ASCII
CHARACTERS
EXCEPT
BECAUSE
THAT
IS
THE
SEPARATOR
BETWEEN
COMPONENTS
IN
A
PATH
AND
NUL
BECAUSE
THAT
IS
USED
TO
PAD
OUT
NAMES
SHORTER
THAN
CHARACTERS
NUL
HAS
THE
NUMERICAL
VALUE
OF
A
UNIX
DIRECTORY
ENTRY
CONTAINS
ONE
ENTRY
FOR
EACH
FILE
IN
THAT
DIRECTORY
EACH
ENTRY
IS
EXTREMELY
SIMPLE
BECAUSE
UNIX
USES
THE
I
NODE
SCHEME
ILLUSTRATED
IN
FIG
A
DIRECTORY
ENTRY
CONTAINS
ONLY
TWO
FIELDS
THE
FILE
NAME
BYTES
AND
THE
NUMBER
OF
THE
I
NODE
FOR
THAT
FILE
BYTES
AS
SHOWN
IN
FIG
THESE
PA
RAMETERS
LIMIT
THE
NUMBER
OF
FILES
PER
FILE
SYSTEM
TO
LIKE
THE
I
NODE
OF
FIG
THE
UNIX
I
NODES
CONTAINS
SOME
ATTRIBUTES
THE
ATTRIBUTES
CONTAIN
THE
FILE
SIZE
THREE
TIMES
CREATION
LAST
ACCESS
AND
LAST
MODIFICA
TION
OWNER
GROUP
PROTECTION
INFORMATION
AND
A
COUNT
OF
THE
NUMBER
OF
DIREC
TORY
ENTRIES
THAT
POINT
TO
THE
I
NODE
THE
LATTER
FIELD
IS
NEEDED
DUE
TO
LINKS
WHEN
EVER
A
NEW
LINK
IS
MADE
TO
AN
I
NODE
THE
COUNT
IN
THE
I
NODE
IS
INCREASED
WHEN
A
LINK
IS
REMOVED
THE
COUNT
IS
DECREMENTED
WHEN
IT
GETS
TO
THE
I
NODE
IS
RE
CLAIMED
AND
THE
DISK
BLOCKS
ARE
PUT
BACK
IN
THE
FREE
LIST
KEEPING
TRACK
OF
DISK
BLOCKS
IS
DONE
USING
A
GENERALIZATION
OF
FIG
IN
ORDER
TO
HANDLE
VERY
LARGE
FILES
THE
FIRST
DISK
ADDRESSES
ARE
STORED
IN
THE
I
NODE
FILE
SYSTEMS
CHAP
BYTES
FILE
NAME
I
NODE
NUMBER
FIGURE
A
UNIX
DIRECTORY
ENTRY
SEC
EXAMPLE
FILE
SYSTEMS
UP
WE
WILL
USE
UNIX
AS
AN
EXAMPLE
BUT
THE
ALGORITHM
IS
BASICALLY
THE
SAME
FOR
ALL
HIERARCHICAL
DIRECTORY
SYSTEMS
FIRST
THE
FDE
SYSTEM
LOCATES
THE
ROOT
DIRECTORY
IN
UNIX
ITS
I
NODE
IS
LOCATED
AT
A
FIXED
PLACE
ON
THE
DISK
FROM
THIS
I
NODE
IT
LOCATES
THE
ROOT
DIRECTORY
WHICH
CAN
BE
ANYWHERE
ON
THE
DISK
BUT
SAY
BLOCK
THEN
IT
READS
THE
ROOT
DIRECTORY
AND
LOOKS
UP
THE
FIRST
COMPONENT
OF
THE
PATH
USR
IN
THE
ROOT
DIRECTORY
TO
FIND
THE
I
NODE
NUMBER
OF
THE
FILE
USR
LOCATING
AN
I
NODE
FROM
ITS
NUMBER
IS
STRAIGHTFORWARD
SINCE
EACH
ONE
HAS
A
FIXED
LOCATION
ON
THE
DISK
FROM
THIS
I
NODE
THE
SYSTEM
LOCATES
THE
DIRECTORY
FOR
USR
AND
LOOKS
UP
THE
NEXT
COMPONENT
AST
IN
IT
WHEN
IT
HAS
FOUND
THE
ENTRY
FOR
AST
IT
HAS
THE
I
NODE
FOR
THE
DIRECTORY
USR
AST
FROM
THIS
I
NODE
IT
CAN
FIND
THE
DIRECTORY
ITSELF
AND
LOOK
UP
MBOX
THE
I
NODE
FOR
THIS
FILE
IS
THEN
READ
INTO
MEMORY
AND
KEPT
THERE
UNTIL
THE
ITSELF
SO
FOR
SMALL
FILES
ALL
THE
NECESSARY
INFORMATION
IS
RIGHT
IN
THE
I
NODE
WHICH
IS
FETCHED
FROM
DISK
TO
MAIN
MEMORY
WHEN
THE
FDE
IS
OPENED
FOR
SOMEWHAT
LARG
ER
FDES
ONE
OF
THE
ADDRESSES
IN
THE
I
NODE
IS
THE
ADDRESS
OF
A
DISK
BLOCK
CALLED
A
SINGLE
INDIRECT
BLOCK
THIS
BLOCK
CONTAINS
ADDITIONAL
DISK
ADDRESSES
IF
THIS
STILL
IS
NOT
ENOUGH
ANOTHER
ADDRESS
IN
THE
I
NODE
CALLED
A
DOUBLE
INDIRECT
BLOCK
CON
FILE
IS
CLOSED
THE
LOOKUP
PROCESS
IS
ILLUSTRATED
IN
FIG
BLOCK
IS
USR
BLOCK
IS
USR
AST
TAINS
THE
ADDRESS
OF
A
BLOCK
THAT
CONTAINS
A
LIST
OF
SINGLE
INDIRECT
BLOCKS
EACH
OF
THESE
SINGLE
INDIRECT
BLOCKS
POINTS
TO
A
FEW
HUNDRED
DATA
BLOCKS
IF
EVEN
THIS
IS
NOT
ENOUGH
A
TRIPLE
INDIRECT
BLOCK
CAN
ALSO
BE
USED
THE
COMPLETE
PICTURE
IS
GIVEN
IN
FIG
I
NODE
ROOT
DIRECTORY
DIRECTORY
DIRECTORY
LOOKING
UP
USR
YIELDS
I
NODE
I
NODE
SAYS
THAT
USR
IS
IN
BLOCK
USR
AST
IS
I
NODE
I
NODE
SAYS
THAT
USR
AST
IS
IN
BLOCK
USR
AST
MBOX
IS
I
NODE
FIGURE
THE
STEPS
IN
LOOKING
UP
USR
AST
MBOX
FIGURE
A
UNIX
I
NODE
SANARJACAAEASAEA
RELATIVE
PATH
NAMES
ARE
LOOKED
UP
THE
SAME
WAY
AS
ABSOLUTE
ONES
ONLY
START
ING
FROM
THE
WORKING
DIRECTORY
INSTEAD
OF
STARTING
FROM
THE
ROOT
DIRECTORY
EVERY
DIRECTORY
HAS
ENTRIES
FOR
AND
WHICH
ARE
PUT
THERE
WHEN
THE
DIRECTORY
IS
CREATED
THE
ENTRY
HAS
THE
I
NODE
NUMBER
FOR
THE
CURRENT
DIRECTORY
AND
THE
ENTRY
FOR
HAS
THE
I
NODE
NUMBER
FOR
THE
PARENT
DIRECTORY
THUS
A
PROCEDURE
LOOKING
UP
DICK
PROG
C
SIMPLY
LOOKS
UP
IN
THE
WORKING
DIRECTORY
FINDS
THE
I
NODE
NUMBER
FOR
THE
PARENT
DIRECTORY
AND
SEARCHES
THAT
DIRECTORY
FOR
DICK
NO
SPECIAL
MECHAN
ISM
IS
NEEDED
TO
HANDLE
THESE
NAMES
AS
FAR
AS
THE
DIRECTORY
SYSTEM
IS
CONCERNED
THEY
ARE
JUST
ORDINARY
ASCII
STRINGS
JUST
THE
SAME
AS
ANY
OTHER
NAMES
THE
ONLY
BIT
OF
TRICKERY
HERE
IS
THAT
IN
THE
ROOT
DIRECTORY
POINTS
TO
ITSELF
FILE
SYSTEMS
CHAP
RESEARCH
ON
FILE
SYSTEMS
FILE
SYSTEMS
HAVE
ALWAYS
ATTRACTED
MORE
RESEARCH
THAN
OTHER
PARTS
OF
THE
OPER
ATING
SYSTEM
AND
THAT
IS
STILL
THE
CASE
WHILE
STANDARD
FILE
SYSTEMS
ARE
FAIRLY
WELL
UNDERSTOOD
THERE
IS
STILL
QUITE
A
BIT
OF
RESEARCH
GOING
ON
ABOUT
OPTIMIZING
BUFFER
CACHE
MANAGEMENT
BURNETT
ET
AL
DING
ET
AL
GNAIDY
ET
AL
KROEGER
AND
LONG
PAI
ET
AL
AND
ZHOU
ET
AL
WORK
IS
GOING
ON
ABOUT
NEW
KINDS
OF
FDE
SYSTEMS
SUCH
AS
USER
LEVEL
FILE
SYSTEMS
MAZIETES
FLASH
FDE
SYSTEMS
GAL
ET
AL
JOURNALING
FILE
SYSTEMS
PRABHAKARAN
ET
AL
AND
STEIN
ET
AL
VERSIONING
FILE
SYSTEMS
CORNELL
ET
AL
PEER
TO
PEER
FILE
SYSTEMS
MUTHITACHAROEN
ET
AL
AND
OTHERS
THE
GOOGLE
FILE
SYSTEM
IS
ALSO
UNUSUAL
DUE
TO
ITS
GREAT
FAULT
TOLERANCE
GHEMAWAT
ET
AL
DIFFERENT
WAYS
OF
FINDING
THINGS
IN
FILE
SYSTEMS
ARE
ALSO
OF
INTEREST
PADIOLEAU
AND
RIDOUX
ANOTHER
AREA
THAT
HAS
BEEN
GETTING
ATTENTION
IS
PROVENANCE
KEEPING
TRACK
OF
THE
HISTORY
OF
THE
DATA
INCLUDING
WHERE
THEY
CAME
FROM
WHO
OWNS
THEM
AND
HOW
THEY
HAS
BEEN
TRANSFORMED
MUNISWARMY
REDDY
ET
AL
AND
SHAH
ET
AL
THIS
INFORMATION
CAN
BE
USED
IN
A
VARIETY
OF
WAYS
MAKING
BACKUPS
IS
STILL
GETTING
SOME
ATTENTION
TOO
COX
ET
AI
AND
RYCROFT
AS
IS
THE
RELATED
TOPIC
OF
RECOVERY
KEETON
ET
AL
RELATED
TO
BACKUPS
IS
KEEPING
DATA
AROUND
AND
USABLE
FOR
DECADES
BAKER
ET
AL
MANIATIS
ET
AL
RELIABILITY
AND
SECURITY
ARE
ALSO
FAR
FROM
SOLVED
PROBLEMS
GREENAN
AND
MILLER
WIRES
AND
FEELEY
WRIGHT
ET
AL
AND
YANG
ET
AL
AND
FINALLY
PER
FORMANCE
HAS
ALWAYS
BEEN
A
RESEARCH
TOPIC
AND
STILL
IS
CAUDILL
AND
GAVRIKOVSKA
CHIANG
AND
HUANG
SREIN
WANG
ET
AL
AND
ZHANG
AND
GHOSE
SUMMARY
WHEN
SEEN
FROM
THE
OUTSIDE
A
FILE
SYSTEM
IS
A
COLLECTION
OF
FILES
AND
DIREC
TORIES
PLUS
OPERATIONS
ON
THEM
FILES
CAN
BE
READ
AND
WRITTEN
DIRECTORIES
CAN
BE
CREATED
AND
DESTROYED
AND
FILES
CAN
BE
MOVED
FROM
DIRECTORY
TO
DIRECTORY
MOST
MODERN
FILE
SYSTEMS
SUPPORT
A
HIERARCHICAL
DIRECTORY
SYSTEM
IN
WHICH
DIRECTORIES
MAY
HAVE
SUBDIRECTORIES
AND
THESE
MAY
HAVE
SUBSUBDIRECTORIES
AD
INFINITUM
WHEN
SEEN
FROM
THE
INSIDE
A
FILE
SYSTEM
LOOKS
QUITE
DIFFERENT
THE
FILE
SYSTEM
DESIGNERS
HAVE
TO
BE
CONCERNED
WITH
HOW
STORAGE
IS
ALLOCATED
AND
HOW
THE
SYSTEM
KEEPS
TRACK
OF
WHICH
BLOCK
GOES
WITH
WHICH
FILE
POSSIBILITIES
INCLUDE
CONTIGUOUS
FILES
LINKED
LISTS
FILE
ALLOCATION
TABLES
AND
I
NODES
DIFFERENT
SYSTEMS
HAVE
DIF
FERENT
DIRECTORY
STRUCTURES
ATTRIBUTES
CAN
GO
IN
THE
DIRECTORIES
OR
SOMEWHERE
ELSE
E
G
AN
I
NODE
DISK
SPACE
CAN
BE
MANAGED
USING
FREE
LISTS
OF
BITMAPS
FILE
SYS
SZZBSES
I
E
N
B
A
N
L
E
I
B
Y
M
A
K
I
N
INCREMENTAL
DUMPS
AND
BY
HAVING
A
PRO
GRAM
THAT
CAN
REPAIR
L
C
K
FILE
SYSTEMS
FILE
SYSTEM
PERFORMANCE
IS
IMPORTANT
AND
SEC
SUMMARY
CAN
BE
ENHANCED
IN
SEVERAL
WAYS
INCLUDING
CACHING
READ
AHEAD
AND
CAREFULLY
PLACING
THE
BLOCKS
OF
A
FILE
CLOSE
TO
EACH
OTHER
LOG
STRUCTURED
FILE
SYSTEMS
ALSO
IMPROVE
PERFORMANCE
BY
DOING
WRITES
IN
LARGE
UNITS
EXAMPLES
OF
FILE
SYSTEMS
INCLUDE
ISO
MS
DOS
AND
UNIX
THESE
DIFFER
IN
MANY
WAYS
INCLUDING
HOW
THEY
KEEP
TRACK
OF
WHICH
BLOCKS
GO
WITH
WHICH
FILE
DIRECTORY
STRUCTURE
AND
MANAGEMENT
OF
FREE
DISK
SPACE
PROBLEMS
GIVE
FIVE
DIFFERENT
PATH
NAMES
FOR
THE
FILE
ETC
POSSWD
HINT
THINK
ABOUT
THE
DIREC
TORY
ENTRIES
AND
IN
WINDOWS
WHEN
A
USER
DOUBLE
CLICKS
ON
A
FILE
LISTED
BY
WINDOWS
EXPLORER
A
PRO
GRAM
IS
RUN
AND
GIVEN
THAT
FDE
AS
A
PARAMETER
LIST
TWO
DIFFERENT
WAYS
THE
OPERATING
SYSTEM
COULD
KNOW
WHICH
PROGRAM
TO
RUN
IN
EARLY
UNIX
SYSTEMS
EXECUTABLE
FILES
A
OUT
FILES
BEGAN
WITH
A
VERY
SPECIFIC
MAGIC
NUMBER
NOT
ONE
CHOSEN
AT
RANDOM
THESE
FILES
BEGAN
WITH
A
HEADER
FOLLOWED
BY
THE
TEXT
AND
DATA
SEGMENTS
WHY
DO
YOU
THINK
A
VERY
SPECIFIC
NUMBER
WAS
CHOSEN
FOR
EX
ECUTABLE
FILES
WHEREAS
OTHER
FILE
TYPES
HAD
A
MORE
OR
LESS
RANDOM
MAGIC
NUMBER
AS
THE
FIRST
WORD
IN
FIG
ONE
OF
THE
ATTRIBUTES
IS
THE
RECORD
LENGTH
WHY
DOES
THE
OPERATING
SYSTEM
EVER
CARE
ABOUT
THIS
SYSTEMS
THAT
SUPPORT
SEQUENTIAL
FILES
ALWAYS
HAVE
AN
OPERATION
TO
REWIND
FILES
DO
SYSTEMS
THAT
SUPPORT
RANDOM
ACCESS
FILES
NEED
THIS
TOO
IN
SOME
SYSTEMS
IT
IS
POSSIBLE
TO
MAP
PART
OF
A
FILE
INTO
MEMORY
WHAT
RESTRICTIONS
MUST
SUCH
SYSTEMS
IMPOSE
HOW
IS
THIS
PARTIAL
MAPPING
IMPLEMENTED
A
SIMPLE
OPERATING
SYSTEM
ONLY
SUPPORTS
A
SINGLE
DIRECTORY
BUT
ALLOWS
THAT
DIRECTORY
TO
HAVE
ARBITRARILY
MANY
FILES
WITH
ARBITRARILY
LONG
FILE
NAMES
CAN
SOMETHING
APPROXI
MATING
A
HIERARCHICAL
FILE
SYSTEM
BE
SIMULATED
HOW
IN
UNIX
AND
WINDOWS
RANDOM
ACCESS
IS
DONE
BY
HAVING
A
SPECIAL
SYSTEM
CALL
THAT
MOVES
THE
CURRENT
POSITION
POINTER
ASSOCIATED
WITH
A
FILE
TO
A
GIVEN
BYTE
IN
THE
FILE
PROPOSE
AN
ALTERNATIVE
WAY
TO
DO
RANDOM
ACCESS
WITHOUT
HAVING
THIS
SYSTEM
CALL
CONSIDER
THE
DIRECTORY
TREE
OF
FIG
IF
USR
JIM
IS
THE
WORKING
DIRECTORY
WHAT
IS
THE
ABSOLUTE
PATH
NAME
FOR
THE
FILE
WHOSE
RELATIVE
PATH
NAME
IS
JAST
XL
CONTIGUOUS
ALLOCATION
OF
FILES
LEADS
TO
DISK
FRAGMENTATION
AS
MENTIONED
IN
THE
TEXT
BECAUSE
SOME
SPACE
IN
THE
LAST
DISK
BLOCK
WILL
BE
WASTED
IN
FILES
WHOSE
LENGTH
IS
NOT
AN
INTEGRAL
NUMBER
OF
BLOCKS
IS
THIS
INTERNAL
FRAGMENTATION
OR
EXTERNAL
FRAGMENTATION
MATE
AN
ANALOGY
I
T
T
SOMETHING
TOED
III
FT
JEFFLLLJ
TTG
IN
LIGHT
OF
THE
ANSWER
TO
THE
PREVIOUS
QUESTION
DORS
ANY
SENSE
WRAI
FLFRIA
FILE
SYSTEMS
CHAP
SOME
DIGITAL
CONSUMER
DEVICES
NEED
TO
STORE
DATA
FOR
EXAMPLE
AS
FILES
NAME
A
MOD
ERN
DEVICE
THAT
REQUIRES
FILE
STORAGE
AND
FOR
WHICH
CONTIGUOUS
ALLOCATION
WOULD
BE
A
FINE
IDEA
HOW
DOES
MS
DOS
IMPLEMENT
RANDOM
ACCESS
TO
FILES
CONSIDER
THE
I
NODE
SHOWN
IN
FIG
IF
IT
CONTAINS
DIRECT
ADDRESSES
OF
BYTES
EACH
AND
ALL
DISK
BLOCKS
ARE
KB
WHAT
IS
THE
LARGEST
POSSIBLE
FILE
IT
HAS
BEEN
SUGGESTED
THAT
EFFICIENCY
COULD
BE
IMPROVED
AND
DISK
SPACE
SAVED
BY
STOR
ING
THE
DATA
OF
A
SHORT
FILE
WITHIN
THE
I
NODE
FOR
THE
I
NODE
OF
FIG
HOW
MANY
BYTES
OF
DATA
COULD
BE
STORED
INSIDE
THE
I
NODE
NAME
ONE
ADVANTAGE
OF
HARD
LINKS
OVER
SYMBOLIC
LINKS
AND
ONE
ADVANTAGE
OF
SYMBOLIC
LINKS
OVER
HARD
LINKS
FREE
DISK
SPACE
CAN
BE
KEPT
TRACK
OF
USING
A
FREE
LIST
OR
A
BITMAP
DISK
ADDRESSES
RE
QUIRE
D
BITS
FOR
A
DISK
WITH
B
BLOCKS
F
OF
WHICH
ARE
FREE
STATE
THE
CONDITION
UNDER
WHICH
THE
FREE
LIST
USES
LESS
SPACE
THAN
THE
BITMAP
FOR
D
HAVING
THE
VALUE
BITS
EXPRESS
YOUR
ANSWER
AS
A
PERCENTAGE
OF
THE
DISK
SPACE
THAT
MUST
BE
FREE
IS
WHAT
WOULD
HAPPEN
IF
THE
BITMAP
OR
FREE
LIST
CONTAINING
THE
INFORMATION
ABOUT
FREE
DISK
BLOCKS
WAS
COMPLETELY
LOST
DUE
TO
A
CRASH
IS
THERE
ANY
WAY
TO
RECOVER
FROM
THIS
DISASTER
OR
IS
IT
BYE
BYE
DISK
DISCUSS
YOUR
ANSWERS
FOR
UNIX
AND
THE
FAT
FILE
SYSTEM
SEPARATELY
CHAP
PROBLEMS
A
CERTAIN
FILE
SYSTEM
USES
KB
DISK
BLOCKS
THE
MEDIAN
FILE
SIZE
IS
KB
IF
ALL
FILES
WERE
EXACTLY
KB
WHAT
FRACTION
OF
THE
DISK
SPACE
WOULD
BE
WASTED
DO
YOU
THINK
THE
WASTAGE
FOR
A
REAL
FILE
SYSTEM
WILL
BE
HIGHER
THAN
THIS
NUMBER
OR
LOWER
THAN
IT
EXPLAIN
YOUR
ANSWER
THE
MS
DOS
FAT
TABLE
CONTAINS
ENTRIES
SUPPOSE
THAT
ONE
OF
THE
BITS
HAD
BEEN
NEEDED
FOR
SOME
OTHER
PURPOSE
AND
THAT
THE
TABLE
CONTAINED
EXACTLY
ENTRIES
IN
STEAD
WITH
NO
OTHER
CHANGES
WHAT
WOULD
THE
LARGEST
MS
DOS
FILE
HAVE
BEEN
UNDER
THIS
CONDITION
FILES
IN
MS
DOS
HAVE
TO
COMPETE
FOR
SPACE
IN
THE
FAT
TABLE
IN
MEMORY
IF
ONE
FILE
USES
K
ENTRIES
THAT
IS
K
ENTRIES
THAT
ARE
NOT
AVAILABLE
TO
ANY
OTHER
FILE
WHAT
CON
STRAINT
DOES
THIS
PLACE
ON
THE
TOTAL
LENGTH
OF
ALL
FILES
COMBINED
A
UNIX
FILE
SYSTEM
HAS
I
KB
BLOCKS
AND
BYTE
DISK
ADDRESSES
WHAT
IS
THE
MAXIMUM
FILE
SIZE
IF
I
NODES
CONTAIN
DIREET
ENTRIES
AND
ONE
SINGLE
DOUBLE
AND
TRIPLE
INDIRECT
ENTRY
EACH
HOW
MANY
DISK
OPERATIONS
ARE
NEEDED
TO
FETCH
THE
I
NODE
FOR
THE
FILE
USR
AST
COURSES
OS
HANDOUT
TL
ASSUME
THAT
THE
I
NODE
FOR
THE
ROOT
DIRECTORY
IS
IN
MEMORY
BUT
NOTHING
ELSE
ALONG
THE
PATH
IS
IN
MEMORY
ALSO
ASSUME
THAT
ALL
DIRECTORIES
FIT
IN
ONE
DISK
BLOCK
IN
MANY
UNIX
SYSTEMS
THE
I
NODES
ARE
KEPT
AT
THE
START
OF
THE
DISK
AN
ALTERNATIVE
DE
SIGN
IS
TO
ALLOCATE
AN
I
NODE
WHEN
A
FILE
IS
CREATED
AND
PUT
THE
I
NODE
AT
THE
STARTTOF
THE
OLIVER
OWL
NIGHT
JOB
AT
THE
UNIVERSITY
COMPUTING
CENTER
IS
TO
CHANGE
THE
TAPES
USED
FOR
OVERNIGHT
DATA
BACKUPS
WHILE
WAITING
FOR
EACH
TAPE
TO
COMPLETE
HE
WORKS
ON
WRITING
HIS
THESIS
THAT
PROVES
SHAKESPEARE
PLAYS
WERE
WRITTEN
BY
EXTRATERRESTRIAL
VISI
TORS
HIS
TEXT
PROCESSOR
RUNS
ON
THE
SYSTEM
BEING
BACKED
UP
SINCE
THAT
IS
THE
ONLY
ONE
THEY
HAVE
IS
THERE
A
PROBLEM
WITH
THIS
ARRANGEMENT
WE
DISCUSSED
MAKING
INCREMENTAL
DUMPS
IN
SOME
DETAIL
IN
THE
TEXT
IN
WINDOWS
IT
IS
EASY
TO
TELL
WHEN
TO
DUMP
A
FILE
BECAUSE
EVERY
FILE
HAS
AN
ARCHIVE
BIT
THIS
BIT
IS
MISS
ING
IN
UNIX
HOW
DO
UNIX
BACKUP
PROGRAMS
KNOW
WHICH
FILES
TO
DUMP
SUPPOSE
THAT
FILE
IN
FIG
WAS
NOT
MODIFIED
SINCE
THE
LAST
DUMP
IN
WHAT
WAY
WOULD
THE
FOUR
BITMAPS
OF
FIG
BE
DIFFERENT
IT
HAS
BEEN
SUGGESTED
THAT
THE
FIRST
PART
OF
EACH
UNIX
FILE
BE
KEPT
IN
THE
SAME
DISK
BLOCK
AS
ITS
I
NODE
WHAT
GOOD
WOULD
THIS
DO
CONSIDER
FIG
IS
IT
POSSIBLE
THAT
FOR
SOME
PARTICULAR
BLOCK
NUMBER
THE
COUNTERS
IN
BOTH
LISTS
HAVE
THE
VALUE
HOW
SHOULD
THIS
PROBLEM
BE
CORRECTED
THE
PERFORMANCE
OF
A
FILE
SYSTEM
DEPENDS
UPON
THE
CACHE
HIT
RATE
FRACTION
OF
BLOCKS
FOUND
IN
THE
CACHE
IF
IT
TAKES
MSEC
TO
SATISFY
A
REQUEST
FROM
THE
CACHE
BUT
MSEC
TO
SATISFY
A
REQUEST
IF
A
DISK
READ
IS
NEEDED
GIVE
A
FORMULA
FOR
THE
MEAN
TIME
REQUIRED
TO
SATISFY
A
REQUEST
IF
THE
HIT
RATE
IS
H
PLOT
THIS
FUNCTION
FOR
VALUES
OF
H
VARYING
FROM
TO
I
JO
CONSIDER
THE
IDEA
BEHIND
FIG
BUT
NOW
FOR
A
DISK
WITH
A
MEAN
SEEK
TIME
OF
MSEC
A
ROTATIONAL
RAFE
OF
RPM
AND
BYTES
PER
TRACK
WHAT
ARE
THE
DATA
RATES
FOR
BLOCK
SIZES
OF
KB
KB
AND
KB
RESPECTIVELY
FIRST
BLOCK
OF
THE
FILE
DISCUSS
THE
PROS
AND
CONS
OF
THIS
ALTERNATIVE
WRITE
A
PROGRAM
THAT
REVERSES
THE
BYTES
OF
A
FILE
SO
THAT
THE
LAST
BYTE
IS
NOW
FIRST
AND
THE
FIRST
BYTE
IS
NOW
LAST
IT
MUST
WORK
WITH
AN
ARBITRARILY
LONG
FILE
BUT
TRY
TO
MAKE
IT
REASONABLY
EFFICIENT
WRITE
A
PROGRAM
THAT
STARTS
AT
A
GIVEN
DIRECTORY
AND
DESCENDS
THE
FILE
TREE
FROM
THAT
POINT
RECORDING
THE
SIZES
OF
ALL
THE
FILES
IT
FINDS
WHEN
IT
IS
ALL
DONE
IT
SHOULD
PRINT
A
HISTOGRAM
OF
THE
FILE
SIZES
USING
A
BIN
WIDTH
SPECIFIED
AS
A
PARAMETER
E
G
WITH
FILE
SIZES
OF
TO
GO
IN
ONE
BIN
TO
GO
IN
THE
NEXT
BIN
ETC
WRITE
A
PROGRAM
THAT
SCANS
ALL
DIRECTORIES
IN
A
UNIX
FILE
SYSTEM
AND
FINDS
AND
LOCATES
ALL
I
NODES
WITH
A
HARD
LINK
COUNT
OF
TWO
OR
MORE
FOR
EACH
SUCH
FILE
IT
LISTS
TOGETHER
ALL
FILE
NAMES
THAT
POINT
TO
THE
FILE
WRITE
A
NEW
VERSION
OF
THE
UNIX
IS
PROGRAM
THIS
VERSION
TAKES
AS
AN
ARGUMENT
ONE
OR
MORE
DIRECTORY
NAMES
AND
FOR
EACH
DIRECTORY
LISTS
ALL
THE
FILES
IN
THAT
DIRECTORY
ONE
LINE
PER
FILE
EACH
FIELD
SHOULD
BE
FORMATTED
IN
A
REASONABLE
WAY
GIVEN
ITS
TYPE
LIST
ONLY
THE
FIRST
DISK
ADDRESS
IF
ANY
INPUT
OUTPUT
IN
ADDITION
TO
PROVIDING
ABSTRACTIONS
SUCH
AS
PROCESSES
AND
THREADS
ADDRESS
SPACES
AND
FILES
AN
OPERATING
SYSTEM
ALSO
CONTROLS
ALL
THE
COMPUTER
INPUT
OUTPUT
DEVICES
IT
MUST
ISSUE
COMMANDS
TO
THE
DEVICES
CATCH
INTERRUPTS
AND
HANDLE
ERRORS
IT
SHOULD
ALSO
PROVIDE
AN
INTERFACE
BETWEEN
THE
DEVICES
AND
THE
REST
OF
THE
SYSTEM
THAT
IS
SIMPLE
AND
EASY
TO
USE
TO
THE
EXTENT
POSSIBLE
THE
INTER
FACE
SHOULD
BE
THE
SAME
FOR
ALL
DEVICES
DEVICE
INDEPENDENCE
THE
I
O
CODE
REP
RESENTS
A
SIGNIFICANT
FRACTION
OF
THE
TOTAL
OPERATING
SYSTEM
HOW
THE
OPERATING
SYS
TEM
MANAGES
I
O
IS
THE
SUBJECT
OF
THIS
CHAPTER
THIS
CHAPTER
IS
ORGANIZED
AS
FOLLOWS
FIRST
WE
WILL
LOOK
AT
SOME
OF
THE
PRINCI
PLES
OF
I
O
HARDWARE
AND
THEN
WE
WILL
LOOK
AT
I
O
SOFTWARE
IN
GENERAL
I
O
SOFT
WARE
CAN
BE
STRUCTURED
IN
LAYERS
WITH
EACH
LAYER
HAVING
A
WELL
DEFINED
TASK
WE
WILL
LOOK
AT
THESE
LAYERS
TO
SEE
WHAT
THEY
DO
AND
HOW
THEY
FIT
TOGETHER
FOLLOWING
THAT
INTRODUCTION
WE
WILL
LOOK
AT
SEVERAL
J
DEVICES
IN
DETAIL
DISKS
CLOCKS
KEYBOARDS
AND
DISPLAYS
FOR
EACH
DEVICE
WE
WILL
LOOK
AT
ITS
HARD
WARE
AND
SOFTWARE
FINALLY
WE
WILL
CONSIDER
POWER
MANAGEMENT
PRINCIPLE
O
F
I
O
HARDWAR
E
DIFFERENT
PEOPLE
LOOK
AT
I
O
HARDWARE
IN
DIFFERENT
WAYS
ELECTRICAL
ENGINEERS
LOOK
AT
IT
IN
TERMS
OF
CHIPS
WIRES
POWER
SUPPLIES
MOTORS
AND
ALL
THE
OTHER
PHYSI
CAL
COMPONENTS
THAT
MAKE
UP
THE
HARDWARE
PROGRAMMERS
LOOK
AT
THE
INTERFACE
INPUT
OUTPUT
CHAP
PRESENTED
TO
THE
SOFTWARE
THE
COMMANDS
THE
HARDWARE
ACCEPTS
THE
FUNCTIONS
IT
SEC
PRINCIPLES
OF
I
O
HARDWARE
CARRIES
OUT
AND
THE
ERRORS
THAT
CAN
BE
REPORTED
BACK
IN
THIS
BOOK
WE
ARE
CON
CERNED
WITH
PROGRAMMING
I
O
DEVICES
NOT
DESIGNING
BUILDING
OR
MAINTAINING
THEM
SO
OUR
INTEREST
WILL
BE
RESTRICTED
TO
HOW
THE
HARDWARE
IS
PROGRAMMED
NOT
HOW
IT
WORKS
INSIDE
NEVERTHELESS
THE
PROGRAMMING
OF
MANY
I
O
DEVICES
IS
OFTEN
INTIMATELY
CONNECTED
WITH
THEIR
INTERNAL
OPERATION
IN
THE
NEXT
THREE
SECTIONS
WE
WILL
PROVIDE
A
LITTLE
GENERAL
BACKGROUND
ON
I
O
HARDWARE
AS
IT
RELATES
TO
PRO
GRAMMING
IT
MAY
BE
REGARDED
AS
A
REVIEW
AND
EXPANSION
OF
THE
INTRODUCTORY
MATERIAL
IN
SEC
I
O
DEVICES
I
O
DEVICES
CAN
BE
ROUGHLY
DIVIDED
INTO
TWO
CATEGORIES
BLOCK
DEVICES
AND
CHARACTER
DEVICES
A
BLOCK
DEVICE
IS
ONE
THAT
STORES
INFORMATION
IN
FIXED
SIZE
BLOCKS
EACH
ONE
WITH
ITS
OWN
ADDRESS
COMMON
BLOCK
SIZES
RANGE
FROM
BYTES
TO
BYTES
ALL
TRANSFERS
ARE
IN
UNITS
OF
ONE
OR
MORE
ENTIRE
CONSECUTIVE
BLOCKS
THE
ESSENTIAL
PROPERTY
OF
A
BLOCK
DEVICE
IS
THAT
IT
IS
POSSIBLE
TO
READ
OR
WRITE
EACH
BLOCK
INDEPENDENTLY
OF
AIL
THE
OTHER
ONES
HARD
DISKS
CD
ROMS
AND
USB
STICKS
ARE
COMMON
BLOCK
DEVICES
IF
YOU
LOOK
CLOSELY
THE
BOUNDARY
BETWEEN
DEVICES
THAT
ARE
BLOCK
ADDRESSABLE
AND
THOSE
THAT
ARE
NOT
IS
NOT
WELL
DEFINED
EVERYONE
AGREES
THAT
A
DISK
IS
A
BLOCK
ADDRESSABLE
DEVICE
BECAUSE
NO
MATTER
WHERE
THE
ARM
CURRENTLY
IS
IT
IS
ALWAYS
POS
SIBLE
TO
SEEK
TO
ANOTHER
CYLINDER
AND
THEN
WAIT
FOR
THE
REQUIRED
BLOCK
TO
ROTATE
UNDER
THE
HEAD
NOW
CONSIDER
A
TAPE
DRIVE
USED
FOR
MAKING
DISK
BACKUPS
TAPES
CONTAIN
A
SEQUENCE
OF
BLOCKS
IF
THE
TAPE
DRIVE
IS
GIVEN
A
COMMAND
TO
READ
BLOCK
N
IT
CAN
ALWAYS
REWIND
THE
TAPE
AND
GO
FORWARD
UNTIL
IT
COMES
TO
BLOCK
N
THIS
OPERATION
IS
ANALOGOUS
TO
A
DISK
DOING
A
SEEK
EXCEPT
THAT
IT
TAKES
MUCH
LONGER
ALSO
IT
MAY
OR
MAY
NOT
BE
POSSIBLE
TO
REWRITE
ONE
BLOCK
IN
THE
MIDDLE
OF
A
TAPE
EVEN
IF
IT
WERE
POSSIBLE
TO
USE
TAPES
AS
RANDOM
ACCESS
BLOCK
DEVICES
THAT
IS
STRETCHING
THE
POINT
SOMEWHAT
THEY
ARE
NORMALLY
NOT
USED
THAT
WAY
THE
OTHER
TYPE
OF
I
O
DEVICE
IS
THE
CHARACTER
DEVICE
A
CHARACTER
DEVICE
DE
LIVERS
OR
ACCEPTS
A
STREAM
OF
CHARACTERS
WITHOUT
REGARD
TO
ANY
BLOCK
STRUCTURE
IT
IS
NOT
ADDRESSABLE
AND
DOES
NOT
HAVE
ANY
SEEK
OPERATION
PRINTERS
NETWORK
INTER
FACES
MICE
FOR
POINTING
RATS
FOR
PSYCHOLOGY
LAB
EXPERIMENTS
AND
MOST
OTHER
DEVICES
THAT
ARE
NOT
DISK
LIKE
CAN
BE
SEEN
AS
CHARACTER
DEVICES
THIS
CLASSIFICATION
SCHEME
IS
NOT
PERFECT
SOME
DEVICES
JUST
DO
NOT
FIT
IN
CLOCKS
FOR
EXAMPLE
ARE
NOT
BLOCK
ADDRESSABLE
NOR
DO
THEY
GENERATE
OR
ACCEPT
CHARACTER
STREAMS
ALL
THEY
DO
IS
CAUSE
INTERRUPTS
AT
WELL
DEFINED
INTERVALS
MEM
ORY
MAPPED
SCREENS
DO
NOT
FIT
THE
MODEL
WELL
EITHER
STILL
THE
MODEL
OF
BLOCK
AND
CHARACTER
DEVICES
IS
GENERAL
ENOUGH
THAT
IT
CAN
BE
USED
AS
A
BASIS
FOR
MAKING
SOME
OF
THE
OPERATING
SYSTEM
SOFTWARE
DEALING
WITH
I
O
DEVICE
INDEPENDENT
THE
FILE
SYSTEM
FOR
EXAMPLE
DEALS
JUST
WITH
ABSTRACT
BLOCK
DEVICES
AND
LEAVES
THE
DEVICE
DEPENDENT
PART
TO
LOWER
LEVEL
SOFTWARE
H
U
G
E
R
A
N
G
E
I
N
P
E
E
D
W
H
I
C
H
P
U
T
CONSIDERABLE
PRESSURE
O
N
HE
SOFTWARE
TO
PERFORM
WELL
OVER
MANY
ORDERS
OF
MAGNITUDE
IN
DATA
RATES
FS
SHOWS
THE
DATA
RATES
OF
SOME
COMMON
DEVICES
MOST
OF
THESE
DEVICES
TEND
TORN
TASTER
AS
TIME
GOES
ON
TO
DEVICE
DATA
RATE
KEYBOARD
BYTES
SEC
MOUSE
BYTES
SEC
MODEM
KB
SEC
SCANNER
KB
SEC
DIGITAL
CAMCORDER
MB
SEC
WIRELESS
MB
SEC
CD
ROM
MB
SEC
FAST
ETHERNET
MB
SEC
COMPACT
FLASH
CARD
MB
SEC
FIREWIRE
IEEE
MB
SEC
USB
MB
SEC
SONET
OC
NETWORK
MB
SEC
SCSI
ULTRA
DISK
MB
SEC
GIGABIT
ETHERNET
MB
SEC
SATA
DISK
DRIVE
MB
SEC
ULTRIUM
TAPE
MB
SEC
PCI
BUS
MB
SEC
FIGURE
SOME
TYPICAL
DEVICE
NETWORK
AND
BUS
DATA
RATES
DEVICE
CONTROLLERS
I
O
UNITS
TYPICALLY
CONSIST
OF
A
MECHANICAL
COMPONENT
AND
AN
ELECTRONIC
COM
PONENT
IT
IS
OFTEN
POSSIBLE
TO
SEPARATE
THE
TWO
PORTIONS
TO
PROVIDE
A
MORE
MODULAR
AND
GENERAL
DESIGN
THE
ELECTRONIC
COMPONENT
IS
CALLED
THE
DEVICE
CONTROLLER
OR
ADAPTER
ON
PERSONAL
COMPUTERS
IT
OFTEN
TAKES
THE
FORM
OF
A
CHIP
ON
THE
PAR
ENTBOARD
OR
A
PRINTED
CIRCUIT
CARD
THAT
CAN
BE
INSERTED
INTO
A
PCI
EXPANSION
SLOT
THE
MECHANICAL
COMPONENT
IS
THE
DEVICE
ITSELF
THIS
ARRANGEMENT
IS
SHOWN
IN
FIG
THE
CONTROLLER
CARD
USUALLY
HAS
A
CONNECTOR
ON
IT
INTO
WHICH
A
CABLE
LEADING
TO
THE
DEVICE
ITSELF
CAN
BE
PLUGGED
MANY
CONTROLLERS
CAN
HANDLE
TWO
FOUR
OR
EVEN
EIGHT
IDENTICAL
DEVICES
IF
THE
INTERFACE
BETWEEN
THE
CONTROLLER
AND
DEVICE
IS
A
STANDARD
INTERFACE
EITHER
AN
OFFICIAL
ANSI
IEEE
OR
ISO
STANDARD
OR
A
DE
FACTO
INPUT
OUTPUT
CHAP
SEC
PRINCIPLES
OF
I
O
HARDWAR
E
ONE
THEN
COMPANIES
CAN
MAKE
CONTROLLERS
OR
DEVICES
THAT
FIT
THAT
INTERFACE
MANY
COMPANIES
FOR
EXAMPLE
MAKE
DISK
DRIVES
THAT
MATCH
THE
IDE
SATA
SCSI
USB
OR
FIREWIRE
IEEE
INTERFACE
THE
INTERFACE
BETWEEN
THE
CONTROLLER
AND
THE
DEVICE
IS
OFTEN
A
VERY
LOW
LEVEL
INTERFACE
A
DISK
FOR
EXAMPLE
MIGHT
BE
FORMATTED
WITH
SECTORS
OF
BYTES
PER
TRACK
WHAT
ACTUALLY
COMES
OFF
THE
DRIVE
HOWEVER
IS
A
SERIAL
BIT
STREAM
STARTING
WITH
A
PREAMBLE
THEN
THE
BITS
IN
A
SECTOR
AND
FINALLY
A
CHECKSUM
ALSO
CALLED
AN
ERROR
CORRECTING
CODE
ECC
THE
PREAMBLE
IS
WRITTEN
WHEN
THE
DISK
IS
FORMATTED
AND
CONTAINS
THE
CYLINDER
AND
SECTOR
NUMBER
THE
SECTOR
SIZE
AND
SIMILAR
DATA
AS
WELL
AS
SYNCHRONIZATION
INFORMATION
THE
CONTROLLER
JOB
IS
TO
CONVERT
THE
SERIAL
BIT
STREAM
INTO
A
BLOCK
OF
BYTES
AND
PERFORM
ANY
ERROR
CORRECTION
NECESSARY
THE
BLOCK
OF
BYTES
IS
TYPICALLY
FIRST
AS
SEMBLED
BIT
BY
BIT
IN
A
BUFFER
INSIDE
THE
CONTROLLER
AFTER
ITS
CHECKSUM
HAS
BEEN
VERIFIED
AND
THE
BLOCK
HAS
BEEN
DECLARED
TO
BE
ERROR
FREE
IT
CAN
THEN
BE
COPIED
TO
MAIN
MEMORY
THE
CONTROLLER
FOR
A
MONITOR
ALSO
WORKS
AS
A
BIT
SERIAL
DEVICE
AT
AN
EQUALLY
LOW
LEVEL
IT
READS
BYTES
CONTAINING
THE
CHARACTERS
TO
BE
DISPLAYED
FROM
MEMORY
AND
GENERATES
THE
SIGNALS
USED
TO
MODULATE
THE
CRT
BEAM
TO
CAUSE
IT
TO
WRITE
ON
THE
SCREEN
THE
CONTROLLER
ALSO
GENERATES
THE
SIGNALS
FOR
MAKING
THE
CRT
BEAM
DO
A
HORIZONTAL
RETRACE
AFTER
IT
HAS
FINISHED
A
SCAN
LINE
AS
WELL
AS
THE
SIGNALS
FOR
MAK
ING
IT
DO
A
VERTICAL
RETRACE
AFTER
THE
ENTIRE
SCREEN
HAS
BEEN
SCANNED
IF
IT
WERE
NOT
FOR
THE
CRT
CONTROLLER
THE
OPERATING
SYSTEM
PROGRAMMER
WOULD
HAVE
TO
EXPLICITLY
PROGRAM
THE
ANALOG
SCANNING
OF
THE
TUBE
WITH
THE
CONTROLLER
THE
OPERATING
SYSTEM
INITIALIZES
THE
CONTROLLER
WITH
A
FEW
PARAMETERS
SUCH
AS
THE
NUMBER
OF
CHARACTERS
OR
PIXELS
PER
LINE
AND
NUMBER
OF
LINES
PER
SCREEN
AND
LETS
THE
CONTROLLER
TAKE
CARE
OF
ACTUALLY
DRIVING
THE
BEAM
FLAT
SCREEN
TFT
DISPLAYS
ARE
DIFFERENT
BUT
JUST
AS
COMPLICATED
MEMORY
MAPPED
I
O
EACH
CONTROLLER
HAS
A
FEW
REGISTERS
THAT
ARE
USED
FOR
COMMUNICATING
WITH
THE
CPU
BY
WRITING
INTO
THESE
REGISTERS
THE
OPERATING
SYSTEM
CAN
COMMAND
THE
DE
VICE
TO
DELIVER
DATA
ACCEPT
DATA
SWITCH
ITSELF
ON
OR
OFF
OR
OTHERWISE
PERFORM
SOME
ACTION
BY
READING
FROM
THESE
REGISTERS
THE
OPERATING
SYSTEM
CAN
LEARN
WHAT
THE
DEVICE
STATE
IS
WHETHER
IT
IS
PREPARED
TO
ACCEPT
A
NEW
COMMAND
AND
SO
ON
IN
ADDITION
TO
THE
CONTROL
REGISTERS
MANY
DEVICES
HAVE
A
DATA
BUFFER
THAT
THE
OPERATING
SYSTEM
CAN
READ
AND
WRITE
FOR
EXAMPLE
A
COMMON
WAY
FOR
COMPUTERS
TO
DISPLAY
PIXELS
ON
THE
SCREEN
IS
TO
HAVE
A
VIDEO
RAM
WHICH
IS
BASICALLY
JUST
A
DATA
BUFFER
AVAILABLE
FOR
PROGRAMS
OR
THE
OPERATING
SYSTEM
TO
WRITE
INTO
THE
ISSUE
THUS
ARISES
OF
HOW
THE
CPU
COMMUNICATES
WITH
THE
CONTROL
REGISTERS
AND
THE
DEVICE
DATA
BUFFERS
TWO
ALTERNATIVES
EXIST
IN
THE
FIRST
APPROACH
EACH
CONTROL
REGISTER
IS
ASSIGNED
AN
I
O
PORT
NUMBER
AN
OR
BIT
INTEGER
THE
SET
OF
ALL
THE
I
O
PORTS
FORM
THE
I
O
PORT
SPACE
AND
IS
PROTECTED
SO
THAT
ORDINARY
USER
PROGRAMS
CANNOT
ACCESS
IT
ONLY
THE
OPERATING
SYSTEM
CAN
USING
A
SPECIAL
I
O
IN
STRUCTION
SUCH
AS
IN
REG
PORT
THE
CPU
CAN
READ
IN
CONTROL
REGISTER
PORT
AND
STORE
THE
RESULT
IN
CPU
REGISTER
REG
SIMILARLY
USING
OUT
PORT
REG
THE
CPU
CAN
WRITE
THE
CONTENTS
OF
REG
TO
A
CONTROL
REGISTER
MOST
EARLY
COMPUTERS
INCLUDING
NEARLY
ALL
MAINFRAMES
SUCH
AS
THE
IBM
AND
ALL
OF
ITS
SUCCESSORS
WORKED
THIS
WAY
IN
THIS
SCHEME
THE
ADDRESS
SPACES
FOR
MEMORY
AND
I
O
ARE
DIFFERENT
AS
SHOWN
IN
FIG
A
THE
INSTRUCTIONS
IN
AND
MOV
M
R
T
H
E
T
H
E
E
E
A
M
P
T
E
DIFFERENT
AND
UNRELATED
ADDRESS
CE
TWO
ADDRESS
ONE
ADDRESS
SPACE
TWO
ADDRESS
SPACES
OXFFFF
MEMORY
I
O
PORTS
FIGURE
A
SEPARATE
I
O
AND
MEMORY
SPACE
B
MEMORY
MAPPED
I
O
C
HYBRID
THE
SECOND
APPROACH
INTRODUCED
WITH
THE
PDP
IS
TO
MAP
ALL
THE
CONTROL
REGISTERS
INTO
THE
MEMORY
SPACE
AS
SHOWN
IN
FIG
B
EACH
CONTROL
REGISTER
IS
ASSIGNED
A
UNIQUE
MEMORY
ADDRESS
TO
WHICH
NO
MEMORY
IS
ASSIGNED
THIS
SYSTEM
IS
CALLED
MEMORY
MAPPED
I
O
USUALLY
THE
ASSIGNED
ADDRESSES
ARE
AT
THE
TOP
OF
THE
ADDRESS
SPACE
A
HYBRID
SCHEME
WITH
MEMORY
MAPPED
I
O
DATA
BUFFERS
AND
SEPARATE
I
O
PORTS
FOR
THE
CONTROL
REGISTERS
IS
SHOWN
IN
FIG
C
THE
PENTIUM
USES
THIS
ARCHITECTURE
WITH
ADDRESSES
TO
BEING
RESERVED
FOR
DEVICE
DATA
BUFFERS
IN
IBM
PC
COMPATIBLES
IN
ADDITION
TO
I
O
PORTS
THROUGH
INPUT
OUTPUT
CHAP
HOW
DO
THESE
SCHEMES
WORK
IN
ALL
CASES
WHEN
THE
CPU
WANTS
TO
READ
A
WORD
EITHER
FROM
MEMORY
OR
FROM
AN
I
O
PORT
IT
PUTS
THE
ADDRESS
IT
NEEDS
ON
THE
BUS
ADDRESS
LINES
AND
THEN
ASSERTS
A
REA
D
SIGNAL
ON
A
BUS
CONTROL
LINE
A
SECOND
SIGNAL
LINE
IS
USED
TO
TELL
WHETHER
I
O
SPACE
OR
MEMORY
SPACE
IS
NEEDED
IF
IT
IS
MEMORY
SPACE
THE
MEMORY
RESPONDS
TO
THE
REQUEST
IF
IT
IS
I
O
SPACE
THE
I
O
DE
VICE
RESPONDS
TO
THE
REQUEST
IF
THERE
IS
ONLY
MEMORY
SPACE
AS
IN
FIG
B
EV
ERY
MEMORY
MODULE
AND
EVERY
I
O
DEVICE
COMPARES
THE
ADDRESS
LINES
TO
THE
RANGE
OF
ADDRESSES
THAT
IT
SERVICES
IF
THE
ADDRESS
FALLS
IN
ITS
RANGE
IT
RESPONDS
TO
THE
RE
QUEST
SINCE
NO
ADDRESS
IS
EVER
ASSIGNED
TO
BOTH
MEMORY
AND
AN
I
O
DEVICE
THERE
IS
NO
AMBIGUITY
AND
NO
CONFLICT
THE
TWO
SCHEMES
FOR
ADDRESSING
THE
CONTROLLERS
HAVE
DIFFERENT
STRENGTHS
AND
WEAKNESSES
LET
US
START
WITH
THE
ADVANTAGES
OF
MEMORY
MAPPED
I
O
FIRST
IF
SPECIAL
I
O
INSTRUCTIONS
ARE
NEEDED
TO
READ
AND
WRITE
THE
DEVICE
CONTROL
REGISTERS
ACCESS
TO
THEM
REQUIRES
THE
USE
OF
ASSEMBLY
CODE
SINCE
THERE
IS
NO
WAY
TO
EXECUTE
AN
IN
OR
OU
T
INSTRUCTION
IN
C
OR
C
CALLING
SUCH
A
PROCEDURE
ADDS
OVERHEAD
TO
CONTROLLING
I
O
IN
CONTRAST
WITH
MEMORY
MAPPED
I
O
DEVICE
CONTROL
REGISTERS
ARE
JUST
VARIABLES
IN
MEMORY
AND
CAN
BE
ADDRESSED
IN
C
THE
SAME
WAY
AS
ANY
OTHER
VARIABLES
THUS
WITH
MEMORY
MAPPED
I
O
A
I
O
DEVICE
DRIVER
CAN
BE
WRITTEN
EN
TIRELY
IN
C
WITHOUT
MEMORY
MAPPED
I
O
SOME
ASSEMBLY
CODE
IS
NEEDED
SECOND
WITH
MEMORY
MAPPED
I
O
NO
SPECIAL
PROTECTION
MECHANISM
IS
NEED
ED
TO
KEEP
USER
PROCESSES
FROM
PERFORMING
I
O
ALL
THE
OPERATING
SYSTEM
HAS
TO
DO
SEC
PRINCIPLES
OF
I
O
HARDWARE
IN
COMPUTER
DESIGN
PRACTICALLY
EVERYTHING
INVOLVES
TRADE
OFFS
AND
THAT
IS
THE
CASE
HERE
TOO
MEMORY
MAPPED
I
O
ALSO
HAS
ITS
DISADVANTAGES
FIRST
MOST
COM
PUTERS
NOWADAYS
HAVE
SOME
FORM
OF
CACHING
OF
MEMORY
WORDS
CACHING
A
DEVICE
CONTROL
REGISTER
WOULD
BE
DISASTROUS
CONSIDER
THE
ASSEMBLY
CODE
LOOP
GIVEN
ABOVE
IN
THE
PRESENCE
OF
CACHING
THE
FIRST
REFERENCE
TO
WOULD
CAUSE
IT
TO
BE
CACHED
SUBSEQUENT
REFERENCES
WOULD
JUST
TAKE
THE
VALUE
FROM
THE
CACHE
AND
NOT
EVEN
ASK
THE
DEVICE
THEN
WHEN
THE
DEVICE
FINALLY
BECAME
READY
THE
SOFTWARE
WOULD
HAVE
NO
WAY
OF
FINDING
OUT
INSTEAD
THE
LOOP
WOULD
GO
ON
FOREVER
TO
PREVENT
THIS
SITUATION
WITH
MEMORY
MAPPED
I
O
THE
HARDWARE
HAS
TO
BE
EQUIPPED
WITH
THE
ABILITY
TO
SELECTIVELY
DISABLE
CACHING
FOR
EXAMPLE
ON
A
PER
PAGE
BASIS
THIS
FEATURE
ADDS
EXTRA
COMPLEXITY
TO
BOTH
THE
HARDWARE
AND
THE
OPER
ATING
SYSTEM
WHICH
HAS
TO
MANAGE
THE
SELECTIVE
CACHING
SECOND
IF
THERE
IS
ONLY
ONE
ADDRESS
SPACE
THEN
ALL
MEMORY
MODULES
AND
ALL
I
O
DEVICES
MUST
EXAMINE
ALL
MEMORY
REFERENCES
TO
SEE
WHICH
ONES
TO
RESPOND
TO
IF
THE
COMPUTER
HAS
A
SINGLE
BUS
AS
IN
FIG
A
HAVING
EVERYONE
LOOK
AT
EVERY
ADDRESS
IS
STRAIGHTFORWARD
CPU
READS
AND
WRITES
OF
MEMORY
GO
OVER
THIS
HIGH
BANDWIDTH
BUS
IS
REFRAIN
FROM
PUTTING
THAT
PORTION
OF
THE
ADDRESS
SPACE
CONTAINING
THE
CONTROL
REG
ISTERS
IN
ANY
USER
VIRTUAL
ADDRESS
SPACE
BETTER
YET
IF
EACH
DEVICE
HAS
ITS
CONTROL
REGISTERS
ON
A
DIFFERENT
PAGE
OF
THE
ADDRESS
SPACE
THE
OPERATING
SYSTEM
CAN
GIVE
A
USER
CONTROL
OVER
SPECIFIC
DEVICES
BUT
NOT
OTHERS
BY
SIMPLY
INCLUDING
THE
DESIRED
PAGES
IN
ITS
PAGE
TABLE
SUCH
A
SCHEME
CAN
ALLOW
DIFFERENT
DEVICE
DRIVERS
TO
BE
PLACED
IN
DIFFERENT
ADDRESS
SPACES
NOT
ONLY
REDUCING
KERNEL
SIZE
BUT
ALSO
KEEPING
ONE
DRIVER
FROM
INTERFERING
WITH
OTHERS
THIRD
WITH
MEMORY
MAPPED
I
O
EVERY
INSTRUCTION
THAT
CAN
REFERENCE
MEMORY
CPU
MEMORY
I
O
ALL
ADDRESSES
MEMORY
AND
I
O
GO
HERE
A
BUS
CPU
MEMORY
B
I
O
THIS
MEMORY
PORT
IS
TO
ALLOW
I
O
DEVICES
ACCESS
TO
MEMORY
CAN
ALSO
REFERENCE
CONTROL
REGISTERS
FOR
EXAMPLE
IF
THERE
IS
AN
INSTRUCTION
TEST
THAT
TESTS
A
MEMORY
WORD
FOR
IT
CAN
ALSO
BE
USED
TO
TEST
A
CONTROL
REGISTER
FOR
WHICH
MIGHT
BE
THE
SIGNAL
THAT
THE
DEVICE
IS
IDLE
AND
CAN
ACCEPT
A
NEW
COMMAND
THE
ASSEMBLY
LANGUAGE
CODE
MIGHT
LOOK
LIKE
THIS
LOOP
TES
T
CHECK
IF
PORT
IS
BE
Q
READ
Y
IF
IT
IS
GO
TO
READY
BRANC
H
LOO
P
OTHERWISE
CONTINUE
TESTING
READY
IF
MEMORY
MAPPED
I
O
IS
NOT
PRESENT
THE
CONTROL
REGISTER
MUST
FIRST
BE
READ
INTO
THE
CPU
THEN
TESTED
REQUIRING
TWO
INSTRUCTIONS
INSTEAD
OF
ONE
IN
THE
CASE
OF
THE
LOOP
GIVEN
ABOVE
A
FOURTH
INSTRUCTION
HAS
TO
BE
ADDED
SLIGHTLY
SLOWING
DOWN
THE
RESPONSIVENESS
OF
DETECTING
AN
IDLE
DEVICE
FIGURE
A
A
SINGLE
BUS
ARCHITECTURE
B
A
DUAL
BUS
MEMORY
ARCHITECTURE
HOWEVER
THE
TREND
IN
MODERN
PERSONAL
COMPUTERS
IS
TO
HAVE
A
DEDICATED
HIGH
SPEED
MEMORY
BUS
AS
SHOWN
IN
FIG
B
A
PROPERTY
ALSO
FOUND
IN
MAIN
FRAMES
INCIDENTALLY
THIS
BUS
IS
TAILORED
TO
OPTIMIZE
MEMORY
PERFORMANCE
WITH
NO
COMPROMISES
FOR
THE
SAKE
OF
SLOW
I
O
DEVICES
PENTIUM
SYSTEMS
CAN
HAVE
MUL
TIPLE
BUSES
MEMORY
PCI
SCSI
USB
ISA
AS
SHOWN
IN
FIG
THE
TROUBLE
WITH
HAVING
A
SEPARATE
MEMORY
BUS
ON
MEMORY
MAPPED
MACHINES
IS
THAT
THE
I
O
DEVICES
HAVE
NO
WAY
OF
SEEING
MEMORY
ADDRESSES
AS
THEY
GO
BY
ON
THE
MEMORY
BUS
SO
THEY
HAVE
NO
WAY
OF
RESPONDING
TO
THEM
AGAIN
SPECIAL
MEAS
URES
HAVE
TO
BE
TAKEN
TO
MAKE
MEMORY
MAPPED
I
O
WORK
ON
A
SYSTEM
WITH
MULTI
PLE
BUSES
ONE
POSSIBILITY
IS
TO
FIRST
SEND
ALL
MEMORY
REFERENCES
TO
THE
MEMORY
IF
THE
MEMORY
FAILS
TO
RESPOND
THEN
THE
CPU
TRIES
THE
OTHER
BUSES
THIS
DESIGN
CAN
BE
MADE
TO
WORK
BUT
REQUIRES
ADDITIONAL
HARDWARE
COMPLEXITY
INPUT
OUTPUT
CHAP
A
SECOND
POSSIBLE
DESIGN
IS
TO
PUT
A
SNOOPING
DEVICE
ON
THE
MEMORY
BUS
TO
PASS
ALL
ADDRESSES
PRESENTED
TO
POTENTIALLY
INTERESTED
I
O
DEVICES
THE
PROBLEM
HERE
IS
THAT
I
O
DEVICES
MAY
NOT
BE
ABLE
TO
PROCESS
REQUESTS
AT
THE
SPEED
THE
MEM
ORY
CAN
A
THIRD
POSSIBLE
DESIGN
WHICH
IS
THE
ONE
USED
ON
THE
PENTIUM
CONFIGURATION
OF
FIG
IS
TO
FILTER
ADDRESSES
IN
THE
PCI
BRIDGE
CHIP
THIS
CHIP
CONTAINS
RANGE
REGISTERS
THAT
ARE
PRELOADED
AT
BOOT
TIME
FOR
EXAMPLE
TO
COULD
BE
MARKED
AS
A
NONMEMORY
RANGE
ADDRESSES
THAT
FALL
WITHIN
ONE
OF
THE
RANGES
MARK
ED
AS
NONMEMORY
ARE
FORWARDED
ONTO
THE
PCI
BUS
INSTEAD
OF
TO
MEMORY
THE
DISADVANTAGE
OF
THIS
SCHEME
IS
THE
NEED
FOR
FIGURING
OUT
AT
BOOT
TIME
WHICH
MEM
ORY
ADDRESSES
ARE
NOT
REALLY
MEMORY
ADDRESSES
THUS
EACH
SCHEME
HAS
ARGUMENTS
FOR
AND
AGAINST
IT
SO
COMPROMISES
AND
TRADE
OFFS
ARE
INEVITABLE
DIRECT
MEMORY
ACCESS
DMA
NO
MATTER
WHETHER
A
CPU
DOES
OR
DOES
NOT
HAVE
MEMORY
MAPPED
I
O
IT
NEEDS
TO
ADDRESS
THE
DEVICE
CONTROLLERS
TO
EXCHANGE
DATA
WITH
THEM
THE
CPU
CAN
REQUEST
DATA
FROM
AN
I
O
CONTROLLER
ONE
BYTE
AT
A
TIME
BUT
DOING
SO
WASTES
THE
CPU
TIME
SO
A
DIFFERENT
SCHEME
CALLED
DMA
DIRECT
MEMORY
ACCESS
IS
OFTEN
USED
THE
OPERATING
SYSTEM
CAN
ONLY
USE
DMA
IF
THE
HARDWARE
HAS
A
DMA
CONTROLLER
WHICH
MOST
SYSTEMS
DO
SOMETIMES
THIS
CONTROLLER
IS
INTEGRATED
INTO
DISK
CONTROLLERS
AND
OTHER
CONTROLLERS
BUT
SUCH
A
DESIGN
REQUIRES
A
SEPARATE
DMA
CONTROLLER
FOR
EACH
DEVICE
MORE
COMMONLY
A
SINGLE
DMA
CONTROLLER
IS
AVAILABLE
E
G
ON
THE
PARENTBOARD
FOR
REGULATING
TRANSFERS
TO
MULTIPLE
DEVICES
OFTEN
CONCURRENTLY
NO
MATTER
WHERE
IT
IS
PHYSICALLY
LOCATED
THE
DMA
CONTROLLER
HAS
ACCESS
TO
THE
SYSTEM
BUS
INDEPENDENT
OF
THE
CPU
AS
SHOWN
IN
FIG
IT
CONTAINS
SEVERAL
REG
ISTERS
THAT
CAN
BE
WRITTEN
AND
READ
BY
THE
CPU
THESE
INCLUDE
A
MEMORY
ADDRESS
REGISTER
A
BYTE
COUNT
REGISTER
AND
ONE
OR
MORE
CONTROL
REGISTERS
THE
CONTROL
REG
ISTERS
SPECIFY
THE
I
O
PORT
TO
USE
THE
DIRECTION
OF
THE
TRANSFER
READING
FROM
THE
I
O
DEVICE
OR
WRITING
TO
THE
I
O
DEVICE
THE
TRANSFER
UNIT
BYTE
AT
A
TIME
OR
WORD
AT
A
TIME
AND
THE
NUMBER
OF
BYTES
TO
TRANSFER
IN
ONE
BURST
TO
EXPLAIN
HOW
DMA
WORKS
LET
US
FIRST
LOOK
AT
HOW
DISK
READS
OCCUR
WHEN
DMA
IS
NOT
USED
FIRST
THE
DISK
CONTROLLER
READS
THE
BLOCK
ONE
OR
MORE
SECTORS
FROM
THE
DRIVE
SERIALLY
BIT
BY
BIT
UNTIL
THE
ENTIRE
BLOCK
IS
IN
THE
CONTROLLER
INTER
NAL
BUFFER
NEXT
IT
COMPUTES
THE
CHECKSUM
TO
VERIFY
THAT
NO
READ
ERRORS
HAVE
OC
CURRED
THEN
THE
CONTROLLER
CAUSES
AN
INTERRUPT
WHEN
THE
OPERATING
SYSTEM
STARTS
RUNNING
IT
CAN
READ
THE
DISK
BLOCK
FROM
THE
CONTROLLER
BUFFER
A
BYTE
OR
A
WORD
AT
A
TIME
BY
EXECUTING
A
LOOP
WITH
EACH
ITERATION
READING
ONE
BYTE
OR
WORD
FROM
A
CONTROLLER
DEVICE
REGISTER
AND
STORING
IT
IN
MAIN
MEMORY
WHEN
DMA
IS
USED
THE
PROCEDURE
IS
DIFFERENT
FIRST
THE
CPU
PROGRAMS
THE
DMA
CONTROLLER
BY
SETTING
ITS
REGISTERS
SO
IT
KNOWS
WHAT
TO
TRANSFER
WHERE
STEP
SEC
PRINCIPLES
OF
I
O
HARDWARE
DRIVE
FIGURE
OPERATION
OF
A
DMA
TRANSFER
IN
FIG
IT
ALSO
ISSUES
A
COMMAND
TO
THE
DISK
CONTROLLER
TELLING
IT
TO
READ
DATA
FROM
THE
DISK
INTO
ITS
INTERNAL
BUFFER
AND
VERIFY
THE
CHECKSUM
WHEN
VALID
DATA
ARE
IN
THE
DISK
CONTROLLER
BUFFER
DMA
CAN
BEGIN
THE
DMA
CONTROLLER
INITIATES
THE
TRANSFER
BY
ISSUING
A
READ
REQUEST
OVER
THE
BUS
TO
THE
DISK
CONTROLLER
STEP
THIS
READ
REQUEST
LOOKS
LIKE
ANY
OTHER
READ
RE
QUEST
AND
THE
DISK
CONTROLLER
DOES
NOT
KNOW
OR
CARE
WHETHER
IT
CAME
FROM
THE
CPU
OR
FROM
A
DMA
CONTROLLER
TYPICALLY
THE
MEMORY
ADDRESS
TO
WRITE
TO
IS
ON
THE
BUS
ADDRESS
LINES
SO
WHEN
THE
DISK
CONTROLLER
FETCHES
THE
NEXT
WORD
FROM
ITS
INTERNAL
BUFFER
IT
KNOWS
WHERE
TO
WRITE
IT
THE
WRITE
TO
MEMORY
IS
ANOTHER
STAN
DARD
BUS
CYCLE
STEP
WHEN
THE
WRITE
IS
COMPLETE
THE
DISK
CONTROLLER
SENDS
AN
ACKNOWLEDGEMENT
SIGNAL
TO
THE
DMA
CONTROLLER
ALSO
OVER
THE
BUS
STEP
THE
DMA
CONTROLLER
THEN
INCREMENTS
THE
MEMORY
ADDRESS
TO
USE
AND
DECREMENTS
THE
BYTE
COUNT
IF
THE
BYTE
COUNT
IS
STILL
GREATER
THAN
STEPS
THROUGH
ARE
REPEATED
UNTIL
THE
COUNT
REACHES
AT
THAT
TIME
THE
DMA
CONTROLLER
INTERRUPTS
THE
CPU
TO
LET
IT
KNOW
THAT
THE
TRANSFER
IS
NOW
COMPLETE
WHEN
THE
OPERATING
SYSTEM
STARTS
UP
IT
DOES
NOT
HAVE
TO
COPY
THE
DISK
BLOCK
TO
MEMORY
IT
IS
ALREADY
THERE
DMA
CONTROLLERS
VARY
CONSIDERABLY
IN
THEIR
SOPHISTICATION
THE
SIMPLEST
ONES
HANDLE
ONE
TRANSFER
AT
A
TIME
AS
DESCRIBED
ABOVE
MORE
COMPLEX
ONES
CAN
BE
PRO
GRAMMED
TO
HANDLE
MULTIPLE
TRANSFERS
AT
ONCE
SUCH
CONTROLLERS
HAVE
MULTIPLE
SETS
OF
REGISTERS
INTERNALLY
ONE
FOR
EACH
CHANNEL
THE
CPU
STARTS
BY
LOADING
EACH
SET
OF
REGISTERS
WITH
THE
RELEVANT
PARAMETERS
FOR
ITS
TRANSFER
EACH
TRANSFER
MUST
USE
A
DIF
FERENT
DEVICE
CONTROLLER
AFTER
EACH
WORD
IS
TRANSFERRED
STEPS
THROUGH
IN
FIG
THE
DMA
CONTROLLER
DECIDES
WHICH
DEVICE
TO
SERVICE
NEXT
IT
MAY
BE
SET
UP
TO
USE
A
ROUND
ROBIN
ALGORITHM
OR
IT
MAY
HAVE
A
PRIORITY
SCHEME
DESIGN
TO
FAVOR
SOME
DEVICES
OVER
OTHERS
MULTIPLE
REQUESTS
TO
DIFFERENT
DEVICE
CONTROLLERS
MAY
BE
PENDING
AT
THE
SAME
TIME
PROVIDED
THAT
THERE
IS
AN
UNAMBIGUOUS
WAY
TO
INPUT
OUTPUT
CHAP
TELL
THE
ACKNOWLEDGEMENTS
APART
OFTEN
A
DIFFERENT
ACKNOWLEDGEMENT
LINE
ON
THE
BUS
IS
USED
FOR
EACH
DMA
CHANNEL
FOR
THIS
REASON
MANY
BUSES
CAN
OPERATE
IN
TWO
MODES
WORD
AT
A
TIME
MODE
AND
BLOCK
MODE
SOME
DMA
CONTROLLERS
CAN
ALSO
OPERATE
IN
EITHER
MODE
IN
THE
FORMER
MODE
THE
OPERATION
IS
AS
DESCRIBED
ABOVE
THE
DMA
CONTROLLER
REQUESTS
FOR
THE
TRANSFER
OF
ONE
WORD
AND
GETS
IT
IF
THE
CPU
ALSO
WANTS
THE
BUS
IT
HAS
TO
WAIT
THE
MECHAN
ISM
IS
CALLED
CYCLE
STEALING
BECAUSE
THE
DEVICE
CONTROLLER
SNEAKS
IN
AND
STEALS
AN
OCCASIONAL
BUS
CYCLE
FROM
THE
CPU
ONCE
IN
A
WHILE
DELAYING
IT
SLIGHTLY
IN
BLOCK
MODE
THE
DMA
CONTROLLER
TELLS
THE
DEVICE
TO
ACQUIRE
THE
BUS
ISSUE
A
SERIES
OF
TRANSFERS
THEN
RELEASE
THE
BUS
THIS
FORM
OF
OPERATION
IS
CALLED
BURST
MODE
IT
IS
MORE
EFFICIENT
THAN
CYCLE
STEALING
BECAUSE
ACQUIRING
THE
BUS
TAKES
TIME
AND
MULTI
PLE
WORDS
CAN
BE
TRANSFERRED
FOR
THE
PRICE
OF
ONE
BUS
ACQUISITION
THE
DOWN
SIDE
TO
BURST
MODE
IS
THAT
IT
CAN
BLOCK
THE
CPU
AND
OTHER
DEVICES
FOR
A
SUBSTANTIAL
PERIOD
OF
TIME
IF
A
LONG
BURST
IS
BEING
TRANSFERRED
IN
THE
MODEL
WE
HAVE
BEEN
DISCUSSING
SOMETIMES
CALLED
FLY
BY
MODE
THE
DMA
CONTROLLER
TELLS
THE
DEVICE
CONTROLLER
TO
TRANSFER
THE
DATA
DIRECTLY
TO
MAIN
MEMORY
AN
ALTERNATIVE
MODE
THAT
SOME
DMA
CONTROLLERS
USE
IS
TO
HAVE
THE
DE
VICE
CONTROLLER
SEND
THE
WORD
TO
THE
DMA
CONTROLLER
WHICH
THEN
ISSUES
A
SECOND
BUS
REQUEST
TO
WRITE
THE
WORD
TO
WHEREVER
IT
IS
SUPPOSED
TO
GO
THIS
SCHEME
RE
QUIRES
AN
EXTRA
BUS
CYCLE
PER
WORD
TRANSFERRED
BUT
IS
MORE
FLEXIBLE
IN
THAT
IT
CAN
ALSO
PERFORM
DEVICE
TO
DEVICE
COPIES
AND
EVEN
MEMORY
TO
MEMORY
COPIES
BY
FIRST
ISSUING
A
READ
TO
MEMORY
AND
THEN
ISSUING
A
WRITE
TO
MEMORY
AT
A
DIFFERENT
ADDRESS
MOST
DMA
CONTROLLERS
USE
PHYSICAL
MEMORY
ADDRESSES
FOR
THEIR
TRANSFERS
USING
PHYSICAL
ADDRESSES
REQUIRES
THE
OPERATING
SYSTEM
TO
CONVERT
THE
VIRTUAL
AD
DRESS
OF
THE
INTENDED
MEMORY
BUFFER
INTO
A
PHYSICAL
ADDRESS
AND
WRITE
THIS
PHYSI
SEC
PRINCIPLES
OF
I
O
HARDWAR
E
THE
NEXT
DISK
WORD
ARRIVED
BEFORE
THE
PREVIOUS
ONE
HAD
BEEN
STORED
THE
CONTROLLER
WOULD
HAVE
TO
STORE
IT
SOMEWHERE
IF
THE
BUS
WERE
VERY
BUSY
THE
CONTROLLER
MIGHT
END
UP
STORING
QUITE
A
FEW
WORDS
AND
HAVING
A
LOT
OF
ADMINISTRATION
TO
DO
AS
WELL
WHEN
THE
BLOCK
IS
BUFFERED
INTERNALLY
THE
BUS
IS
NOT
NEEDED
UNTIL
THE
DMA
BEGINS
SO
THE
DESIGN
OF
THE
CONTROLLER
IS
MUCH
SIMPLER
BECAUSE
THE
DMA
TRANSFER
TO
MEMORY
IS
NOT
TIME
CRITICAL
SOME
OLDER
CONTROLLERS
DID
IN
FACT
GO
DIRECTLY
TO
MEMORY
WITH
ONLY
A
SMALL
AMOUNT
OF
INTERNAL
BUFFERING
BUT
WHEN
THE
BUS
WAS
VERY
BUSY
A
TRANSFER
MIGHT
HAVE
HAD
TO
BE
TERMINATED
WITH
AN
OVERRUN
ERROR
NOT
ALL
COMPUTERS
USE
DMA
THE
ARGUMENT
AGAINST
IT
IS
THAT
THE
MAIN
CPU
IS
OFTEN
FAR
FASTER
THAN
THE
DMA
CONTROLLER
AND
CAN
DO
THE
JOB
MUCH
FASTER
WHEN
THE
LIMITING
FACTOR
IS
NOT
THE
SPEED
OF
THE
I
O
DEVICE
IF
THERE
IS
NO
OTHER
WORK
FOR
IT
TO
DO
HAVING
THE
FAST
CPU
WAIT
FOR
THE
SLOW
DMA
CONTROLLER
TO
FINISH
IS
POINT
LESS
ALSO
GETTING
RID
OF
THE
DMA
CONTROLLER
AND
HAVING
THE
CPU
DO
AIL
THE
WORK
IN
SOFTWARE
SAVES
MONEY
IMPORTANT
ON
LOW
END
EMBEDDED
COMPUTERS
INTERRUPTS
REVISITED
WE
BRIEFLY
INTRODUCED
INTERRUPTS
IN
SEC
BUT
THERE
IS
MORE
TO
BE
SAID
IN
A
TYPICAL
PERSONA
COMPUTER
SYSTEM
THE
INTERRUPT
STRUCTURE
IS
AS
SHOWN
IN
FIG
AT
THE
HARDWARE
LEVEL
INTERRUPTS
WORK
AS
FOLLOWS
WHEN
AN
I
O
DEVICE
HAS
FIN
ISHED
THE
WORK
GIVEN
TO
IT
IT
CAUSES
AN
INTERRUPT
ASSUMING
THAT
INTERRUPTS
HAVE
BEEN
ENABLED
BY
THE
OPERATING
SYSTEM
IT
DOES
THIS
BY
ASSERTING
A
SIGNAL
ON
A
BUS
LINE
THAT
IT
HAS
BEEN
ASSIGNED
THIS
SIGNAL
IS
DETECTED
BY
THE
INTERRUPT
CONTROLLER
CHIP
ON
THE
PARENTBOARD
WHICH
THEN
DECIDES
WHAT
TO
DO
CAL
ADDRESS
INTO
THE
DMA
CONTROLLER
ADDRESS
REGISTER
AN
ALTERNATIVE
SCHEME
USED
IN
A
FEW
DMA
CONTROLLERS
IS
TO
WRITE
VIRTUAL
ADDRESSES
INTO
THE
DMA
CON
TROLLER
INSTEAD
THEN
THE
DMA
CONTROLLER
MUST
USE
THE
MMU
TO
HAVE
THE
VIRTUAL
TO
PHYSICAL
TRANSLATION
DONE
ONLY
IN
THE
CASE
THAT
THE
MMU
IS
PART
OF
THE
MEMORY
POSSIBLE
BUT
RARE
RATHER
THAN
PART
OF
THE
CPU
CAN
VIRTUAL
ADDRESSES
BE
PUT
ON
THE
BUS
WE
MENTIONED
EARLIER
THAT
THE
DISK
FIRST
READS
DATA
INTO
ITS
INTERNAL
BUFFER
BE
FORE
DMA
CAN
START
YOU
MAY
BE
WONDERING
WHY
THE
CONTROLLER
DOES
NOT
JUST
STORE
THE
BYTES
IN
MAIN
MEMORY
AS
SOON
AS
IT
GETS
THEM
FROM
THE
DISK
IN
OTHER
WORDS
WHY
DOES
IT
NEED
AN
INTERNAL
BUFFER
THERE
ARE
TWO
REASONS
FIRST
BY
DOING
INTER
CPU
INTERRUPT
CONTROLLER
DEVIC
E
IS
FINISHED
BAGAESSS
KEYBOAR
D
PRINTER
BUS
NAL
BUFFERING
THE
DISK
CONTROLLER
CAN
VERIFY
THE
CHECKSUM
BEFORE
STARTING
A
TRANS
FER
IF
THE
CHECKSUM
IS
INCORRECT
AN
ERROR
IS
SIGNALED
AND
NO
TRANSFER
IS
DONE
THE
SECOND
REASON
IS
THAT
ONCE
A
DISK
TRANSFER
HAS
STARTED
THE
BITS
KEEP
ARRIV
ING
FROM
THE
DISK
AT
A
CONSTANT
RATE
WHETHER
THE
CONTROLLER
IS
READY
FOR
THEM
OR
NOT
IF
THE
CONTROLLER
TRIED
TO
WRITE
DATA
DIRECTLY
TO
MEMORY
IT
WOULD
HAVE
TO
GO
OVER
THE
SYSTEM
BUS
FOR
EACH
WORD
TRANSFERRED
IF
THE
BUS
WERE
BUSY
DUE
TO
SOME
OTHER
DEVICE
USING
IT
E
G
IN
BURST
MODE
THE
CONTROLLER
WOULD
HAVE
TO
WAIT
IF
FIGURE
HOW
AN
INTERRUPT
HAPPENS
THE
CONNECTIONS
BETWEEN
THE
DEVICES
AND
THE
INTERRUPT
CONTROLLER
ACTUALLY
USE
INTERRUPT
LINES
ON
THE
BUS
RATHER
THAN
DEDICATED
WIRES
IF
NO
OTHER
INTERRUPTS
ARE
PENDING
THE
INTERRUPT
CONTROLLER
PROCESSES
THE
INTER
RUPT
IMMEDIATELY
IF
ANOTHER
ONE
IS
IN
PROGRESS
OR
ANOTHER
DEVICE
HAS
MADE
A
SI
MULTANEOUS
REQUEST
ON
A
HIGHER
PRIORITY
INTERRUPT
REQUEST
LINE
ON
THE
BUS
THE
INPUT
OUTPUT
CHAP
DEVICE
IS
JUST
IGNORED
FOR
THE
MOMENT
IN
THIS
CASE
IT
CONTINUES
TO
ASSERT
AN
INTER
RUPT
SIGNAL
ON
THE
BUS
UNTIL
IT
IS
SERVICED
BY
THE
CPU
TO
HANDLE
THE
INTERRUPT
THE
CONTROLLER
PUTS
A
NUMBER
ON
THE
ADDRESS
LINES
SPECIFYING
WHICH
DEVICE
WANTS
ATTENTION
AND
ASSERTS
A
SIGNAL
TO
INTERRUPT
THE
CPU
THE
INTERRUPT
SIGNAL
CAUSES
THE
CPU
TO
STOP
WHAT
IT
IS
DOING
AND
START
DOING
SOMETHING
ELSE
THE
NUMBER
ON
THE
ADDRESS
LINES
IS
USED
AS
AN
INDEX
INTO
A
TABLE
CALLED
THE
INTERRUPT
VECTOR
TO
FETCH
A
NEW
PROGRAM
COUNTER
THIS
PROGRAM
COUNTER
POINTS
TO
THE
START
OF
THE
CORRESPONDING
INTERRUPT
SERVICE
PROCEDURE
TYPICALLY
TRAPS
AND
INTERRUPTS
USE
THE
SAME
MECHANISM
FROM
THIS
POINT
ON
AND
FREQUENTLY
SHARE
THE
SAME
INTERRUPT
VECTOR
THE
LOCATION
OF
THE
INTERRUPT
VECTOR
CAN
BE
HARDWIRED
INTO
THE
MACHINE
OR
IT
CAN
BE
ANYWHERE
IN
MEMORY
WITH
A
CPU
REGISTER
LOADED
BY
THE
OPERATING
SYSTEM
POINTING
TO
ITS
ORIGIN
SHORTLY
AFTER
IT
STARTS
RUNNING
THE
INTERRUPT
SERVICE
PROCEDURE
ACKNOWLEDGES
THE
INTERRUPT
BY
WRITING
A
CERTAIN
VALUE
TO
ONE
OF
THE
INTERRUPT
CONTROLLER
I
O
PORTS
THIS
ACKNOWLEDGEMENT
TELLS
THE
CONTROLLER
THAT
IT
IS
FREE
TO
ISSUE
ANOTHER
IN
TERRUPT
BY
HAVING
THE
CPU
DELAY
THIS
ACKNOWLEDGEMENT
UNTIL
IT
IS
READY
TO
HAND
LE
THE
NEXT
INTERRUPT
RACE
CONDITIONS
INVOLVING
MULTIPLE
ALMOST
SIMULTANEOUS
IN
TERRUPTS
CAN
BE
AVOIDED
AS
AN
ASIDE
SOME
OLDER
COMPUTERS
DO
NOT
HAVE
A
CEN
TRALIZED
INTERRUPT
CONTROLLER
SO
EACH
DEVICE
CONTROLLER
REQUESTS
ITS
OWN
INTERRUPTS
THE
HARDWARE
ALWAYS
SAVES
CERTAIN
INFORMATION
BEFORE
STARTING
THE
SERVICE
PROCEDURE
WHICH
INFORMATION
IS
SAVED
AND
WHERE
IT
IS
SAVED
VARIES
GREATLY
FROM
CPU
TO
CPU
AS
A
BARE
MINIMUM
THE
PROGRAM
COUNTER
MUST
BE
SAVED
SO
THE
IN
TERRUPTED
PROCESS
CAN
BE
RESTARTED
AT
THE
OTHER
EXTREME
ALL
THE
VISIBLE
REGISTERS
AND
A
LARGE
NUMBER
OF
INTERNAL
REGISTERS
MAY
BE
SAVED
AS
WELL
ONE
ISSUE
IS
WHERE
TO
SAVE
THIS
INFORMATION
ONE
OPTION
IS
TO
PUT
IT
IN
INTERNAL
REGISTERS
THAT
THE
OPERATING
SYSTEM
CAN
READ
OUT
AS
NEEDED
A
PROBLEM
WITH
THIS
APPROACH
IS
THAT
THEN
THE
INTERRUPT
CONTROLLER
CANNOT
BE
ACKNOWLEDGED
UNTIL
ALL
POTENTIALLY
RELEVANT
INFORMATION
HAS
BEEN
READ
OUT
LEST
A
SECOND
INTERRUPT
OVER
WRITE
THE
INTERNAL
REGISTERS
SAVING
THE
STATE
THIS
STRATEGY
LEADS
TO
LONG
DEAD
TIMES
WHEN
INTERRUPTS
ARE
DISABLED
AND
POSSIBLY
LOST
INTERRUPTS
AND
LOST
DATA
CONSEQUENTLY
MOST
CPUS
SAVE
THE
INFORMATION
ON
THE
STACK
HOWEVER
THIS
APPROACH
TOO
HAS
PROBLEMS
TO
START
WITH
WHOSE
STACK
IF
THE
CURRENT
STACK
IS
USED
IT
MAY
WELL
BE
A
USER
PROCESS
STACK
THE
STACK
POINTER
MAY
NOT
EVEN
BE
LEGAL
WHICH
WOULD
CAUSE
A
FATAL
ERROR
WHEN
THE
HARDWARE
TRIED
TO
WRITE
SOME
WORDS
AT
THE
ADDRESS
POINTED
TO
ALSO
IT
MIGHT
POINT
TO
THE
END
OF
A
PAGE
AFTER
SEVERAL
MEMORY
WRITES
THE
PAGE
BOUNDARY
MIGHT
BE
EXCEEDED
AND
A
PAGE
FAULT
GENERATED
HAVING
A
PAGE
FAULT
OCCUR
DURING
THE
HARDWARE
INTERRUPT
PROCESSING
CREATES
A
BIGGER
PROBLEM
WHERE
TO
SAVE
THE
STATE
TO
HANDLE
THE
PAGE
FAULT
IF
THE
KERNEL
STACK
IS
USED
THERE
IS
A
MUCH
BETTER
CHANCE
OF
THE
STACK
POINTER
BEING
LEGAL
AND
POINTING
TO
A
PINNED
PAGE
HOWEVER
SWITCHING
INTO
KERNEL
MODE
MAY
REQUIRE
CHANGING
MMU
CONTEXTS
AND
WILL
PROBABLY
INVALIDATE
MOST
OR
ALL
OF
THE
CACHE
AND
TLB
RELOADING
ALL
OF
THESE
STATICALLY
OR
DYNAMICALLY
WILL
INCREASE
THE
TIME
TO
PROCESS
AN
INTERRUPT
AND
THUS
WASTE
CPU
TIME
SEC
PRINCIPLES
OF
I
O
HARDWARE
PRECISE
AND
IMPRECISE
INTERRUPTS
ANOTHER
PROBLEM
IS
CAUSED
BY
THE
FACT
THAT
MOST
MODERN
CPUS
ARE
HEAVILY
PIPELINED
AND
OFTEN
SUPERSCALAR
INTERNALLY
PARALLEL
IN
OLDER
SYSTEMS
AFTER
EACH
INSTRUCTION
WAS
FINISHED
EXECUTING
THE
MICROPROGRAM
OR
HARDWARE
CHECKED
TO
SEE
IF
THERE
WAS
AN
INTERRUPT
PENDING
IF
SO
THE
PROGRAM
COUNTER
AND
PSW
WERE
PUSHED
ONTO
THE
STACK
AND
THE
INTERRUPT
SEQUENCE
BEGUN
AFTER
THE
INTERRUPT
HAND
LER
RAN
THE
REVERSE
PROCESS
TOOK
PLACE
WITH
THE
OLD
PSW
AND
PROGRAM
COUNTER
POPPED
FROM
THE
STACK
AND
THE
PREVIOUS
PROCESS
CONTINUED
THIS
MODEL
MAKES
THE
IMPLICIT
ASSUMPTION
THAT
IF
AN
INTERRUPT
OCCURS
JUST
AFTER
SOME
INSTRUCTION
ALL
THE
INSTRUCTIONS
UP
TO
AND
INCLUDING
THAT
INSTRUCTION
HAVE
BEEN
EXECUTED
COMPLETELY
AND
NO
INSTRUCTIONS
AFTER
IT
HAVE
EXECUTED
AT
ALL
ON
OLDER
MACHINES
THIS
ASSUMPTION
WAS
ALWAYS
VALID
ON
MODEM
ONES
IT
MAY
NOT
BE
FOR
STARTERS
CONSIDER
THE
PIPELINE
MODEL
OF
FIG
L
A
WHAT
HAPPENS
IF
AN
INTERRUPT
OCCURS
WHILE
THE
PIPELINE
IS
FULL
THE
USUAL
CASE
MANY
INSTRUCTIONS
ARE
IN
VARIOUS
STAGES
OF
EXECUTION
WHEN
THE
INTERRUPT
OCCURS
THE
VALUE
OF
THE
PRO
GRAM
COUNTER
MAY
NOT
REFLECT
THE
CORRECT
BOUNDARY
BETWEEN
EXECUTED
INSTRUCTIONS
AND
NONEXECUTED
INSTRUCTIONS
IN
FACT
MANY
INSTRUCTIONS
MAY
HAVE
BEEN
PARTIALLY
EXECUTED
WITH
DIFFERENT
INSTRUCTIONS
BEING
MORE
OR
LESS
COMPLETE
IN
THIS
SITUA
TION
THE
PROGRAM
COUNTER
MOST
LIKELY
REFLECTS
THE
ADDRESS
OF
THE
NEXT
INSTRUCTION
TO
BE
FETCHED
AND
PUSHED
INTO
THE
PIPELINE
RATHER
THAN
THE
ADDRESS
OF
THE
INSTRUCTION
THAT
JUST
WAS
PROCESSED
BY
THE
EXECUTION
UNIT
ON
A
SUPERSCALAR
MACHINE
SUCH
AS
THAT
OF
FIG
L
B
THINGS
ARE
EVEN
WORSE
INSTRUCTIONS
MAY
BE
DECOMPOSED
INTO
MICRO
OPERATIONS
AND
THE
MICRO
OPERATIONS
MAY
EXECUTE
OUT
OF
ORDER
DEPENDING
ON
THE
AVAILABILITY
OF
INTERNAL
RESOURCES
SUCH
AS
FUNCTIONAL
UNITS
AND
REGISTERS
AT
THE
TIME
OF
AN
INTERRUPT
SOME
INSTRUCTIONS
STARTED
LONG
AGO
MAY
NOT
HAVE
STARTED
AND
OTHERS
STARTED
MORE
RECENTLY
MAY
BE
AL
MOST
DONE
AT
THE
POINT
WHEN
AN
INTERRUPT
IS
SIGNALED
THERE
MAY
BE
MANY
INSTRUC
TIONS
IN
VARIOUS
STATES
OF
COMPLETENESS
WITH
LESS
RELATION
BETWEEN
THEM
AND
THE
PROGRAM
COUNTER
AN
INTERRUPT
THAT
LEAVES
THE
MACHINE
IN
A
WELL
DEFINED
STATE
IS
CALLED
A
PRECISE
INTERRUPT
WALKER
AND
CRAGON
SUCH
AN
INTERRUPT
HAS
FOUR
PROPERTIES
THE
PC
PROGRAM
COUNTER
IS
SAVED
IN
A
KNOWN
PLACE
ALL
INSTRUCTIONS
BEFORE
THE
ONE
POINTED
TO
BY
THE
PC
HAVE
FULLY
EXECUTED
NO
INSTRUCTION
BEYOND
THE
ONE
POINTED
TO
BY
THE
PC
HAS
BEEN
EXECUTED
THE
EXECUTION
STATE
OF
THE
INSTRUCTION
POINTED
TO
BY
THE
PC
IS
KNOWN
NOTE
THAT
THERE
IS
NO
PROHIBITION
ON
INSTRUCTIONS
BEYOND
THE
ONE
POINTED
TO
BY
THE
PC
FROM
STARTING
IT
IS
JUST
THAT
ANY
CHANGES
THEY
MAKE
TO
REGISTERS
OR
MEMORY
MUST
BE
UNDONE
BEFORE
THE
INTERRUPT
HAPPENS
IT
IS
PERMITTED
THAT
THE
INSTRUCTION
POINTED
TO
HAS
BEEN
EXECUTED
IT
IS
ALSO
PERMITTED
THAT
IT
HAS
NOT
BEEN
EXECUTED
INPUT
OUTPUT
CHAP
HOWEVER
IT
MUST
BE
CLEAR
WHICH
CASE
APPLIES
OFTEN
IF
THE
INTERRUPT
IS
AN
I
O
IN
TERRUPT
THE
INSTRUCTION
WILL
NOT
YET
HAVE
STARTED
HOWEVER
IF
THE
INTERRUPT
IS
REALLY
A
TRAP
OR
PAGE
FAULT
THEN
THE
PC
GENERALLY
POINTS
TO
THE
INSTRUCTION
THAT
CAUSED
THE
FAULT
SO
IT
CAN
BE
RESTARTED
LATER
THE
SITUATION
OF
FIG
A
ILLUSTRATES
A
PRECISE
INTERRUPT
ALL
INSTRUCTIONS
UP
TO
THE
PROGRAM
COUNTER
HAVE
COMPLETED
AND
NONE
OF
THOSE
BEYOND
IT
HAVE
STARTED
OR
HAVE
BEEN
ROLLED
BACK
TO
UNDO
THEIR
EF
FECTS
SEC
PRINCIPLES
OF
I
O
HARDWARE
SOME
POINT
ARE
ALLOWED
TO
FINISH
AND
NONE
BEYOND
THAT
POINT
ARE
ALLOWED
TO
HAVE
ANY
NOTICEABLE
EFFECT
ON
THE
MACHINE
STATE
HERE
THE
PRICE
IS
PAID
NOT
IN
TIME
BUT
IN
CHIP
AREA
AND
IN
COMPLEXITY
OF
THE
DESIGN
IF
PRECISE
INTERRUPTS
WERE
NOT
RE
QUIRED
FOR
BACKWARD
COMPATIBILITY
PURPOSES
THIS
CHIP
AREA
WOULD
BE
AVAILABLE
FOR
LARGER
ON
CHIP
CACHES
MAKING
THE
CPU
FASTER
ON
THE
OTHER
HAND
IMPRECISE
INTER
RUPTS
MAKE
THE
OPERATING
SYSTEM
FAR
MORE
COMPLICATED
AND
SLOWER
SO
IT
IS
HARD
TO
TELL
WHICH
APPROACH
IS
REALLY
BETTER
NOT
EXECUTED
EXECUTED
PRINCIPLE
O
F
I
O
SOFTWAR
E
CECUTETF
PC
A
PC
EXECUTED
EXECUTED
B
LET
US
NOW
TURN
AWAY
FROM
THE
I
O
HARDWARE
AND
LOOK
AT
THE
I
O
SOFTWARE
FIRST
WE
WILL
LOOK
AT
THE
GOALS
OF
THE
I
O
SOFTWARE
AND
THEN
AT
THE
DIFFERENT
WAYS
I
O
CAN
BE
DONE
FROM
THE
POINT
OF
VIEW
OF
THE
OPERATING
SYSTEM
GOALS
OF
THE
I
O
SOFTWARE
FIGURE
A
A
PRECISE
INTERRUPT
B
AN
IMPRECISE
INTERRUPT
AN
INTERRUPT
THAT
DOES
NOT
MEET
THESE
REQUIREMENTS
IS
CALLED
AN
IMPRECISE
INTERRUPT
AND
MAKES
LIFE
MOST
UNPLEASANT
FOR
THE
OPERATING
SYSTEM
WRITER
WHO
NOW
HAS
TO
FIGURE
OUT
WHAT
HAS
HAPPENED
AND
WHAT
STILL
HAS
TO
HAPPEN
FIG
B
SHOWS
AN
IMPRECISE
INTERRUPT
WHERE
DIFFERENT
INSTRUCTIONS
NEAR
THE
PROGRAM
COUNT
ER
ARE
IN
DIFFERENT
STAGES
OF
COMPLETION
WITH
OLDER
ONES
NOT
NECESSARILY
MORE
COM
PLETE
THAN
YOUNGER
ONES
MACHINES
WITH
IMPRECISE
INTERRUPTS
USUALLY
VOMIT
A
LARGE
AMOUNT
OF
INTERNAL
STATE
ONTO
THE
STACK
TO
GIVE
THE
OPERATING
SYSTEM
THE
POS
SIBILITY
OF
FIGURING
OUT
WHAT
WAS
GOING
ON
THE
CODE
NECESSARY
TO
RESTART
THE
MA
CHINE
IS
TYPICALLY
EXTREMELY
COMPLICATED
ALSO
SAVING
A
LARGE
AMOUNT
OF
INFOR
MATION
TO
MEMORY
ON
EVERY
INTERRUPT
MAKES
INTERRUPTS
SLOW
AND
RECOVERY
EVEN
WORSE
THIS
LEADS
TO
THE
IRONIC
SITUATION
OF
HAVING
VERY
FAST
SUPERSCALAR
CPUS
SOMETIMES
BEING
UNSUITABLE
FOR
REAL
TIME
WORK
DUE
TO
SLOW
INTERRUPTS
SOME
COMPUTERS
ARE
DESIGNED
SO
THAT
SOME
KINDS
OF
INTERRUPTS
AND
TRAPS
ARE
PRECISE
AND
OTHERS
ARE
NOT
FOR
EXAMPLE
HAVING
I
O
INTERRUPTS
BE
PRECISE
BUT
TRAPS
DUE
TO
FATAL
PROGRAMMING
ERRORS
BE
IMPRECISE
IS
NOT
SO
BAD
SINCE
NO
ATTEMPT
NEED
BE
MADE
TO
RESTART
A
RUNNING
PROCESS
AFTER
IT
HAS
DIVIDED
BY
ZERO
SOME
MACHINES
HAVE
A
BIT
THAT
CAN
BE
SET
TO
FORCE
ALL
INTERRUPTS
TO
BE
PRECISE
THE
DOWNSIDE
OF
SET
TING
THIS
BIT
IS
THAT
IT
FORCES
THE
CPU
TO
CAREFULLY
LOG
EVERYTHING
IT
IS
DOING
AND
MAINTAIN
SHADOW
COPIES
OF
REGISTERS
SO
IT
CAN
GENERATE
A
PRECISE
INTERRUPT
AT
ANY
INSTANT
ALL
THIS
OVERHEAD
HAS
A
MAJOR
IMPACT
ON
PERFORMANCE
SOME
SUPERSCALAR
MACHINES
SUCH
AS
THE
PENTIUM
SERIES
HAVE
PRECISE
INTERRUPTS
TO
ALLOW
OLD
SOFTWARE
TO
WORK
CORRECTLY
THE
PRICE
PAID
FOR
PRECISE
INTERRUPTS
IS
EXTREMELY
COMPLEX
INTERRUPT
LOGIC
WITHIN
THE
CPU
TO
MAKE
SURE
THAT
WHEN
THE
IN
TERRUPT
CONTROLLER
SIGNALS
THAT
IT
WANTS
TO
CAUSE
AN
INTERRUPT
ALL
INSTRUCTIONS
UP
TO
A
KEY
CONCEPT
IN
THE
DESIGN
OF
I
O
SOFTWARE
IS
KNOWN
AS
DEVICE
INDEPEN
DENCE
WHAT
IT
MEANS
IS
THAT
IT
SHOULD
BE
POSSIBLE
TO
WRITE
PROGRAMS
THAT
CAN
AC
CESS
ANY
I
O
DEVICE
WITHOUT
HAVING
TO
SPECIFY
THE
DEVICE
IN
ADVANCE
FOR
EXAMPLE
A
PROGRAM
THAT
READS
A
FILE
AS
INPUT
SHOULD
BE
ABLE
TO
READ
A
FILE
ON
A
HARD
DISK
A
CD
ROM
A
DVD
OR
A
USB
STICK
WITHOUT
HAVING
TO
MODIFY
THE
PROGRAM
FOR
EACH
DIFFERENT
DEVICE
SIMILARLY
ONE
SHOULD
BE
ABLE
TO
TYPE
A
COMMAND
SUCH
AS
SORT
INPUT
OUTPUT
AND
HAVE
IT
WORK
WITH
INPUT
COMING
FROM
ANY
KIND
OF
DISK
OR
THE
KEYBOARD
AND
THE
OUTPUT
GOING
TO
ANY
KIND
OF
DISK
OR
THE
SCREEN
IT
IS
UP
TO
THE
OPERATING
SYSTEM
TO
TAKE
CARE
OF
THE
PROBLEMS
CAUSED
BY
THE
FACT
THAT
THESE
DEVICES
REALLY
ARE
DIFFERENT
AND
REQUIRE
VERY
DIFFERENT
COMMAND
SEQUENCES
TO
READ
OR
WRITE
CLOSELY
RELATED
TO
DEVICE
INDEPENDENCE
IS
THE
GOAL
OF
UNIFORM
NAMING
THE
NAME
OF
A
FILE
OR
A
DEVICE
SHOULD
SIMPLY
BE
A
STRING
OR
AN
INTEGER
AND
NOT
DEPEND
ON
THE
DEVICE
IN
ANY
WAY
IN
UNIX
ALL
DISKS
CAN
BE
INTEGRATED
IN
THE
FILE
SYSTEM
HIERARCHY
IN
ARBITRARY
WAYS
SO
THE
USER
NEED
NOT
BE
AWARE
OF
WHICH
NAME
CORRESPONDS
TO
WHICH
DEVICE
FOR
EXAMPLE
A
USB
STICK
CAN
BE
MOUNTED
ON
TOP
OF
THE
DIRECTORY
USR
AST
BACKUP
SO
THAT
COPYING
A
FILE
TO
USR
ASTABACKUP
MONDAY
COP
IES
THE
FILE
TO
THE
USB
STICK
IN
THIS
WAY
ALL
FILES
AND
DEVICES
ARE
ADDRESSED
THE
SAME
WAY
BY
A
PATH
NAME
ANOTHER
IMPORTANT
ISSUE
FOR
I
O
SOFTWARE
IS
ERROR
HANDLING
IN
GENERAL
ER
RORS
SHOULD
BE
HANDLED
AS
CLOSE
TO
THE
HARDWARE
AS
POSSIBLE
IF
THE
CONTROLLER
DIS
COVERS
A
READ
ERROR
IT
SHOULD
TRY
TO
CORRECT
THE
ERROR
ITSELF
IF
IT
CAN
IF
IT
CANNOT
THEN
THE
DEVICE
DRIVER
SHOULD
HANDLE
IT
PERHAPS
BY
JUST
TRYING
TO
READ
THE
BLOCK
AGAIN
MANY
ERRORS
ARE
TRANSIENT
SUCH
AS
READ
ERRORS
CAUSED
BY
SPECKS
OF
DUST
ON
THE
READ
HEAD
AND
WILL
FREQUENTLY
GO
AWAY
IF
THE
OPERATION
IS
REPEATED
ONLY
IF
THE
INPUT
OUTPUT
CHAP
LOWER
LAYERS
ARE
NOT
ABLE
TO
DEAL
WITH
THE
PROBLEM
SHOULD
THE
UPPER
LAYERS
BE
TOLD
ABOUT
IT
IN
MANY
CASES
ERROR
RECOVERY
CAN
BE
DONE
TRANSPARENTLY
AT
A
LOW
LEVEL
WITHOUT
THE
UPPER
LEVELS
EVEN
KNOWING
ABOUT
THE
ERROR
STILL
ANOTHER
KEY
ISSUE
IS
THAT
OF
SYNCHRONOUS
BLOCKING
VERSUS
ASYNCHRO
NOUS
INTERRUPT
DRIVEN
TRANSFERS
MOST
PHYSICAL
I
O
IS
ASYNCHRONOUS
THE
CPU
SEC
PRINCIPLES
OF
I
O
SOFTWARE
USE
R
STARTS
THE
TRANSFER
AND
GOES
OFF
TO
DO
SOMETHING
ELSE
UNTIL
THE
INTERRUPT
ARRIVES
USER
PROGRAMS
ARE
MUCH
EASIER
TO
WRITE
IF
THE
I
O
OPERATIONS
ARE
BLOCKING
AFTER
A
READ
SYSTEM
CALL
THE
PROGRAM
IS
AUTOMATICALLY
SUSPENDED
UNTIL
THE
DATA
ARE
AVAIL
ABLE
IN
THE
BUFFER
IT
IS
UP
TO
THE
OPERATING
SYSTEM
TO
MAKE
OPERATIONS
THAT
ARE
AC
TUALLY
INTERRUPT
DRIVEN
LOOK
BLOCKING
TO
THE
USER
PROGRAMS
ANOTHER
ISSUE
FOR
THE
I
O
SOFTWARE
IS
BUFFERING
OFTEN
DATA
THAT
COME
OFF
A
DEVICE
CANNOT
BE
STORED
DIRECTLY
IN
ITS
FINAL
DESTINATION
FOR
EXAMPLE
WHEN
A
SPAC
E
KERNEL
SPAC
E
NEX
T
ABC
D
EFG
H
PRINTE
D
PAG
E
NEX
T
A
B
ABC
D
EFG
H
PACKET
COMES
IN
OFF
THE
NETWORK
THE
OPERATING
SYSTEM
DOES
NOT
KNOW
WHERE
TO
PUT
IT
UNTIL
IT
HAS
STORED
THE
PACKET
SOMEWHERE
AND
EXAMINED
IT
ALSO
SOME
DEVICES
HAVE
SEVERE
REAL
TIME
CONSTRAINTS
FOR
EXAMPLE
DIGITAL
AUDIO
DEVICES
SO
THE
DATA
MUST
BE
PUT
INTO
AN
OUTPUT
BUFFER
IN
ADVANCE
TO
DECOUPLE
THE
RATE
AT
WHICH
THE
BUFFER
IS
FILLED
FROM
THE
RATE
AT
WHICH
IT
IS
EMPTIED
IN
ORDER
TO
AVOID
BUFFER
UNDER
RUNS
BUFFERING
INVOLVES
CONSIDERABLE
COPYING
AND
OFTEN
HAS
A
MAJOR
IMPACT
ON
I
O
PERFORMANCE
THE
FINAL
CONCEPT
THAT
WE
WILL
MENTION
HERE
IS
SHARABLE
VERSUS
DEDICATED
DE
VICES
SOME
I
O
DEVICES
SUCH
AS
DISKS
CAN
BE
USED
BY
MANY
USERS
AT
THE
SAME
TIME
NO
PROBLEMS
ARE
CAUSED
BY
MULTIPLE
USERS
HAVING
OPEN
FILES
ON
THE
SAME
DISK
AT
THE
SAME
TIME
OTHER
DEVICES
SUCH
AS
TAPE
DRIVES
HAVE
TO
BE
DEDICATED
TO
A
SINGLE
USER
UNTIL
THAT
USER
IS
FINISHED
THEN
ANOTHER
USER
CAN
HAVE
THE
TAPE
DRIVE
HAVING
TWO
OR
MORE
USERS
WRITING
BLOCKS
INTERMIXED
AT
RANDOM
TO
THE
SAME
TAPE
WILL
DEFINITELY
NOT
WORK
INTRODUCING
DEDICATED
UNSHARED
DEVICES
ALSO
INTRODUCES
A
VARIETY
OF
PROBLEMS
SUCH
AS
DEADLOCKS
AGAIN
THE
OPERATING
SYSTEM
MUST
BE
ABLE
TO
HANDLE
BOTH
SHARED
AND
DEDICATED
DEVICES
IN
A
WAY
THAT
AVOIDS
PROBLEMS
PROGRAMME
D
I
O
THERE
ARE
THREE
FUNDAMENTALLY
DIFFERENT
WAYS
THAT
I
O
CAN
BE
PERFORMED
IN
THIS
SECTION
WE
WILL
LOOK
AT
THE
FIRST
ONE
PROGRAMMED
I
O
IN
THE
NEXT
TWO
SEC
TIONS
WE
WILL
EXAMINE
THE
OTHERS
INTERRUPT
DRIVEN
I
O
AND
I
O
USING
DMA
THE
SIMPLEST
FORM
OF
I
O
IS
TO
HAVE
THE
CPU
DO
ALL
THE
WORK
THIS
METHOD
IS
CALLED
PROGRAMMED
I
O
IT
IS
SIMPLEST
TO
ILLUSTRATE
PROGRAMMED
I
O
BY
MEANS
OF
AN
EXAMPLE
CONSIDER
A
USER
PROCESS
THAT
WANTS
TO
PRINT
THE
EIGHT
CHARACTER
STRING
ABCDEFGH
ON
THE
PRINTER
IT
FIRST
ASSEMBLES
THE
STRING
IN
A
BUFFER
IN
USER
SPACE
AS
SHOWN
IN
FIG
A
THE
USER
PROCESS
THEN
ACQUIRES
THE
PRINTER
FOR
WRITING
BY
MAKING
A
SYSTEM
CALL
TO
OPEN
IT
IF
THE
PRINTER
IS
CURRENTLY
IN
USE
BY
ANOTHER
PROCESS
THIS
CALL
WILL
FAIL
B
C
FIGURE
STEPS
I
N
PRINTIN
G
A
STRING
AND
RETURN
AN
ERROR
CODE
OR
WILL
BLOCK
UNTIL
THE
PRINTER
IS
AVAILABLE
DEPENDING
ON
THE
OPERATING
SYSTEM
AND
THE
PARAMETERS
OF
THE
CALL
ONCE
IT
HAS
THE
PRINTER
THE
USER
PROCESS
MAKES
A
SYSTEM
CALL
TELLING
THE
OPERATING
SYSTEM
TO
PRINT
THE
STRING
ON
THE
PRINTER
THE
OPERATING
SYSTEM
THEN
USUALLY
COPIES
THE
BUFFER
WITH
THE
STRMG
TO
AN
ARRAY
SAY
P
IN
KERNEL
SPACE
WHERE
IT
IS
MORE
EASILY
ACCESSED
BECAUSE
THE
KERNEL
MAY
HAVE
TO
CHANGE
THE
MEMORY
MAP
TO
GET
AT
USER
SPACE
IT
THEN
CHECKS
TO
SEE
IF
THE
PRINTER
IS
CURRENTLY
AVAILABLE
IF
NOT
IT
WAITS
UNTIL
IT
IS
AVAILABLE
AS
SOON
AS
THE
PRINTER
IS
AVAILABLE
THE
OPERATING
SYSTEM
COPIES
THE
FIRST
CHARACTER
TO
THE
PRINT
ER
DATA
REGISTER
IN
THIS
EXAMPLE
USING
MEMORY
MAPPED
I
O
THIS
ACTION
ACTIVATES
THE
PRINTER
THE
CHARACTER
MAY
NOT
APPEAR
YET
BECAUSE
SOME
PRINTERS
BUF
FER
A
LINE
OR
A
PAGE
BEFORE
PRINTING
ANYTHING
IN
FIG
B
HOWEVER
WE
SEE
THAT
THE
FIRST
CHARACTER
HAS
BEEN
PRINTED
AND
THAT
THE
SYSTEM
HAS
MARKED
THE
B
AS
THE
NEXT
CHARACTER
TO
BE
PRINTED
AS
SOON
AS
IT
HAS
COPIED
THE
FIRST
CHARACTER
TO
THE
PRINTER
THE
OPERATING
SYSTEM
CHECKS
TO
SEE
IF
THE
PRINTER
IS
READY
TO
ACCEPT
ANOTHER
ONE
GENERALLY
THE
PRINTER
HAS
A
SECOND
REGISTER
WHICH
GIVES
ITS
STATUS
THE
ACT
OF
WRITING
TO
THE
DATA
REGISTER
CAUSES
THE
STATUS
TO
BECOME
NOT
READY
WHEN
THE
PRINTER
CONTROLLER
HAS
PROCESSED
THE
CURRENT
CHARACTER
IT
INDICATES
ITS
AVAILABILITY
BY
SETTING
SOME
BIT
IN
ITS
STATUS
REGISTER
OR
PUTTING
SOME
VALUE
IN
IT
AT
THIS
POINT
THE
OPERATING
SYSTEM
WAITS
FOR
THE
PRINTER
TO
BECOME
READY
AGAIN
WHEN
THAT
HAPPENS
IT
PRINTS
THE
NEXT
CHARACTER
AS
SHOWN
IN
FIG
C
THIS
LOOP
CONTINUES
UNTIL
THE
ENTIRE
STRING
HAS
BEEN
PRINTED
THEN
CONTROL
RETURNS
TO
THE
USER
PROCESS
THE
ACTIONS
FOLLOWED
BY
THE
OPERATING
SYSTEM
ARE
SUMMARIZED
IN
FIG
FIRST
THE
DATA
ARE
COPIED
TO
THE
KERNEL
THEN
THE
OPERATING
SYSTEM
ENTERS
A
TIGHT
LOOP
OUTPUTTING
THE
CHARACTERS
ONE
AT
A
TIME
THE
ESSENTIAL
ASPECT
OF
PROGRAMMED
INPUT
OUTPUT
CHAP
I
O
CLEARLY
ILLUSTRATED
IN
THIS
FIGURE
IS
THAT
AFTER
OUTPUTTING
A
CHARACTER
THE
CPU
CONTINUOUSLY
POLLS
THE
DEVICE
TO
SEE
IF
IT
IS
READY
TO
ACCEPT
ANOTHER
ONE
THIS
BE
HAVIOR
IS
OFTEN
CALLED
POLLING
OR
BUSY
WAITING
BUFFER
P
COUNT
P
IS
THE
KERNEL
BUFFER
FOR
I
I
COUNT
I
LOOP
ON
EVERY
CHARACTER
WHILE
READY
LOOP
UNTIL
READY
P
I
OUTPUT
ONE
CHARACTER
FIGURE
WRITING
A
STRING
TO
THE
PRINTER
USING
PROGRAMMED
I
O
PROGRAMMED
I
O
IS
SIMPLE
BUT
HAS
THE
DISADVANTAGE
OF
TYING
UP
THE
CPU
FULL
TIME
UNTIL
ALL
THE
I
O
IS
DONE
IF
THE
TIME
TO
PRINT
A
CHARACTER
IS
VERY
SHORT
BECAUSE
ALL
THE
PRINTER
IS
DOING
IS
COPYING
THE
NEW
CHARACTER
TO
AN
INTERNAL
BUFFER
THEN
BUSY
WAITING
IS
FINE
ALSO
IN
AN
EMBEDDED
SYSTEM
WHERE
THE
CPU
HAS
NOTHING
ELSE
TO
DO
BUSY
WAITING
IS
REASONABLE
HOWEVER
IN
MORE
COMPLEX
SYSTEMS
WHERE
THE
CPU
HAS
OTHER
WORK
TO
DO
BUSY
WAITING
IS
INEFFICIENT
A
BETTER
I
O
METHOD
IS
NEEDED
INTERRUPT
DRIVEN
I
O
NOW
LET
US
CONSIDER
THE
CASE
OF
PRINTING
ON
A
PRINTER
THAT
DOES
NOT
BUFFER
CHAR
ACTERS
BUT
PRINTS
EACH
ONE
AS
IT
ARRIVES
IF
THE
PRINTER
CAN
PRINT
SAY
CHARAC
TERS
SEC
EACH
CHARACTER
TAKES
MSEC
TO
PRINT
THIS
MEANS
THAT
AFTER
EVERY
CHARAC
TER
IS
WRITTEN
TO
THE
PRINTER
DATA
REGISTER
THE
CPU
WILL
SIT
IN
AN
IDLE
LOOP
FOR
MSEC
WAITING
TO
BE
ALLOWED
TO
OUTPUT
THE
NEXT
CHARACTER
THIS
IS
MORE
THAN
ENOUGH
TIME
TO
DO
A
CONTEXT
SWITCH
AND
RUN
SOME
OTHER
PROCESS
FOR
THE
MSEC
THAT
WOULD
OTHERWISE
BE
WASTED
THE
WAY
TO
ALLOW
THE
CPU
TO
DO
SOMETHING
ELSE
WHILE
WAITING
FOR
THE
PRINTER
TO
BECOME
READY
IS
TO
USE
INTERRUPTS
WHEN
THE
SYSTEM
CALL
TO
PRINT
THE
STRING
IS
MADE
THE
BUFFER
IS
COPIED
TO
KERNEL
SPACE
AS
WE
SHOWED
EARLIER
AND
THE
FIRST
CHARACTER
IS
COPIED
TO
THE
PRINTER
AS
SOON
AS
IT
IS
WILLING
TO
ACCEPT
A
CHARACTER
AT
THAT
POINT
THE
CPU
CALLS
THE
SCHEDULER
AND
SOME
OTHER
PROCESS
IS
RUN
THE
PROCESS
THAT
ASKED
FOR
THE
STRING
TO
BE
PRINTED
IS
BLOCKED
UNTIL
THE
ENTIRE
STRING
HAS
PRINTED
THE
WORK
DONE
ON
THE
SYSTEM
CALL
IS
SHOWN
IN
FIG
A
WHEN
THE
PRINTER
HAS
PRINTED
THE
CHARACTER
AND
IS
PREPARED
TO
ACCEPT
THE
NEXT
ONE
IT
GENERATES
AN
INTERRUPT
THIS
INTERRUPT
STOPS
THE
CURRENT
PROCESS
AND
SAVES
ITS
STATE
THEN
THE
PRINTER
INTERRUPT
SERVICE
PROCEDURE
IS
RUN
A
CRUDE
VERSION
OF
THIS
CODE
IS
SHOWN
IN
FIG
B
IF
THERE
ARE
NO
MORE
CHARACTERS
TO
PRINT
THE
INTERRUPT
HANDLER
TAKES
SOME
ACTION
TO
UNBLOCK
THE
USER
OTHERWISE
IT
OUTPUTS
THE
NEXT
CHAR
ACTER
ACKNOWLEDGES
THE
INTERRUPT
AND
RETURNS
TO
THE
PROCESS
THAT
WAS
RUNNING
JUST
BEFORE
THE
INTERRUPT
WHICH
CONTINUES
FROM
WHERE
IT
LEFT
OFF
SEC
PRINCIPLES
OF
I
O
SOFTWARE
BUFFER
P
COUNT
IF
COUNT
WHILE
READY
ELSE
P
REGISTER
P
IJ
SCHEDULER
COUNT
COUNT
I
I
INTERRUPT
A
B
FIGURE
WRIUNG
A
STRING
TO
THE
PRINTER
USING
INTERRUPT
DRIVEN
I
O
A
CODE
EXECUTED
AT
THE
TIME
THE
PRINT
SYSTEM
CALL
IS
MADE
B
INTERRUPT
SERVICE
PROCE
DURE
FOR
THE
PRINTER
I
O
USING
DMA
AN
OBVIOUS
DISADVANTAGE
OF
INTERRUPT
DRIVEN
I
O
IS
THAT
AN
INTERRUPT
OCCURS
ON
EVERY
CHARACTER
INTERRUPTS
TAKE
TIME
SO
THIS
SCHEME
WASTES
A
CERTAIN
AMOUNT
OF
CPU
TIME
A
SOLUTION
IS
TO
USE
DMA
HERE
THE
IDEA
IS
TO
LET
THE
DMA
CONTROLLER
FEED
THE
CHARACTERS
TO
THE
PRINTER
ONE
AT
TIME
WITHOUT
THE
CPU
BEING
BOTHERED
IN
ESSENCE
DMA
IS
PROGRAMMED
I
O
ONLY
WITH
THE
DMA
CONTROLLER
DOING
ALL
THE
WORK
INSTEAD
OF
THE
MAIN
CPU
THIS
STRATEGY
REQUIRES
SPECIAL
HARDWARE
THE
DMA
CONTROLLER
BUT
FREES
UP
THE
CPU
DURING
THE
I
O
TO
DO
OTHER
WORK
AN
OUT
LINE
OF
THE
CODE
IS
GIVEN
IN
FIG
BUFFER
P
COUNT
SCHEDULER
A
B
FIGURE
PRINTING
A
STRING
USING
DMA
A
CODE
EXECUTED
WHEN
THE
PRINT
SYSTEM
CALL
IS
MADE
B
INTERRUPT
SERVICE
PROCEDURE
THE
BIG
WIN
WITH
DMA
IS
REDUCING
THE
NUMBER
OF
INTERRUPTS
FROM
ONE
PER
CHA
RACTER
TO
ONE
PER
BUFFER
PRINTED
IF
THERE
ARE
MANY
CHARACTERS
AND
INTERRUPTS
ARE
SLOW
THIS
CAN
BE
A
MAJOR
IMPROVEMENT
ON
THE
OTHER
HAND
THE
DMA
CONTROLLER
IS
USUALLY
MUCH
SLOWER
THAN
THE
MAIN
CPU
IF
THE
DMA
CONTROLLER
IS
NOT
CAPABLE
OF
DRIVING
THE
DEVICE
AT
FULL
SPEED
OR
THE
CPU
USUALLY
HAS
NOTHING
TO
DO
ANYWAY
WHILE
WAITING
FOR
THE
DMA
INTERRUPT
THEN
INTERRUPT
DRIVEN
I
O
OR
EVEN
PRO
GRAMMED
I
O
MAY
BE
BETTER
MOST
OF
THE
TIME
DMA
IS
WORTH
IT
THOUGH
INPUT
OUTPUT
CHAP
I
O
SOFTWARE
LAYERS
I
O
SOFTWARE
IS
TYPICALLY
ORGANIZED
IN
FOUR
LAYERS
AS
SHOWN
IN
FIG
EACH
LAYER
HAS
A
WELL
DEFINED
FUNCTION
TO
PERFORM
AND
A
WELL
DEFINED
INTERFACE
TO
THE
ADJACENT
LAYERS
THE
FUNCTIONALITY
AND
INTERFACES
DIFFER
FROM
SYSTEM
TO
SYSTEM
SO
THE
DISCUSSION
THAT
FOLLOWS
WHICH
EXAMINES
ALL
THE
LAYERS
STARTING
AT
THE
BOTTOM
IS
NOT
SPECIFIC
TO
ONE
MACHINE
USER
LEVEL
I
O
SOFTWARE
DEVICE
INDEPENDENT
OPERATING
SYSTEM
SOFTWARE
DEVICE
DRIVERS
INTERRUPT
HANDLERS
HARDWARE
FIGURE
LAYERS
OF
THE
I
O
SOFTWARE
SYSTEM
INTERRUPT
HANDLERS
WHILE
PROGRAMMED
I
O
IS
OCCASIONALLY
USEFUL
FOR
MOST
I
O
INTERRUPTS
ARE
AN
UNPLEASANT
FACT
OF
LIFE
AND
CANNOT
BE
AVOIDED
THEY
SHOULD
BE
HIDDEN
AWAY
DEEP
IN
THE
BOWELS
OF
THE
OPERATING
SYSTEM
SO
THAT
AS
LITTLE
OF
THE
OPERATING
SYSTEM
AS
POSSIBLE
KNOWS
ABOUT
THEM
THE
BEST
WAY
TO
HIDE
THEM
IS
TO
HAVE
THE
DRIVER
START
ING
AN
I
O
OPERATION
BLOCK
UNTIL
THE
I
O
HAS
COMPLETED
AND
THE
INTERRUPT
OCCURS
THE
DRIVER
CAN
BLOCK
ITSELF
BY
DOING
A
DOWN
ON
A
SEMAPHORE
A
WAIT
ON
A
CONDITION
VARIABLE
A
RECEIVE
ON
A
MESSAGE
OR
SOMETHING
SIMILAR
FOR
EXAMPLE
WHEN
THE
INTERRUPT
HAPPENS
THE
INTERRUPT
PROCEDURE
DOES
WHATEVER
IT
HAS
TO
IN
ORDER
TO
HANDLE
THE
INTERRUPT
THEN
IT
CAN
UNBLOCK
THE
DRIVER
THAT
STARTED
IT
IN
SOME
CASES
IT
WILL
JUST
COMPLETE
UP
ON
A
SEMAPHORE
IN
OTHERS
IT
WILL
DO
A
SIGNAL
ON
A
CONDITION
VARIABLE
IN
A
MONITOR
IN
STILL
OTHERS
IT
WILL
SEND
A
MESSAGE
TO
THE
BLOCKED
DRIVER
IN
ALL
CASES
THE
NET
EFFECT
OF
THE
INTERRUPT
WILL
BE
THAT
A
DRIVER
THAT
WAS
PREVIOUSLY
BLOCKED
WILL
NOW
BE
ABLE
TO
RUN
THIS
MODEL
WORKS
BEST
IF
DRIVERS
ARE
STRUCTURED
AS
KERNEL
PROCESSES
WITH
THEIR
OWN
STATES
STACKS
AND
PROGRAM
COUNTERS
OF
COURSE
REALITY
IS
NOT
QUITE
SO
SIMPLE
PROCESSING
AN
INTERRUPT
IS
NOT
JUST
A
MATTER
OF
TAKING
THE
INTERRUPT
DOING
AN
UP
ON
SOME
SEMAPHORE
AND
THEN
EXECUTING
AN
IRET
INSTRUCTION
TO
RETURN
FROM
THE
INTERRUPT
TO
THE
PREVIOUS
PROCESS
THERE
IS
A
GREAT
DEAL
MORE
WORK
INVOLVED
FOR
THE
OPERATING
SYSTEM
WE
WILL
NOW
GIVE
AN
OUTLINE
OF
THIS
WORK
AS
A
SERIES
OF
STEPS
THAT
MUST
BE
PERFORMED
IN
SOFTWARE
AFTER
THE
HARDWARE
INTERRUPT
HAS
COMPLETED
IT
SHOULD
BE
NOTED
THAT
THE
DETAILS
ARE
VERY
SEC
I
O
SOFTWARE
LAYERS
SYSTEM
DEPENDENT
SO
SOME
OF
THE
STEPS
LISTED
BELOW
MAY
NOT
BE
NEEDED
ON
A
PAR
TICULAR
MACHINE
AND
STEPS
NOT
LISTED
MAY
BE
REQUIRED
ALSO
THE
STEPS
THAT
DO
OCCUR
MAY
BE
IN
A
DIFFERENT
ORDER
ON
SOME
MACHINES
SAVE
ANY
REGISTERS
INCLUDING
THE
PSW
THAT
HAVE
NOT
ALREADY
BEEN
SAVED
BY
THE
INTERRUPT
HARDWARE
SET
UP
A
CONTEXT
FOR
THE
INTERRUPT
SERVICE
PROCEDURE
DOING
THIS
MAY
INVOLVE
SETTING
UP
THE
TLB
MMU
AND
A
PAGE
TABLE
SET
UP
A
STACK
FOR
THE
INTERRUPT
SERVICE
PROCEDURE
ACKNOWLEDGE
THE
INTERRUPT
CONTROLLER
IF
THERE
IS
NO
CENTRALIZED
INTER
RUPT
CONTROLLER
REENABLE
INTERRUPTS
COPY
THE
REGISTERS
FROM
WHERE
THEY
WERE
SAVED
POSSIBLY
SOME
STACK
TO
THE
PROCESS
TABLE
RUN
THE
INTERRUPT
SERVICE
PROCEDURE
IT
WILL
EXTRACT
INFORMATION
FROM
THE
INTERRUPTING
DEVICE
CONTROLLER
REGISTERS
CHOOSE
WHICH
PROCESS
TO
RUN
NEXT
IF
THE
INTERRUPT
HAS
CAUSED
SOME
HIGH
PRIORITY
PROCESS
THAT
WAS
BLOCKED
TO
BECOME
READY
IT
MAY
BE
CHOSEN
TO
RUN
NOW
SET
UP
THE
MMU
CONTEXT
FOR
THE
PROCESS
TO
RUN
NEXT
SOME
TLB
SET
UP
MAY
ALSO
BE
NEEDED
LOAD
THE
NEW
PROCESS
REGISTERS
INCLUDING
ITS
PSW
START
RUNNING
THE
NEW
PROCESS
AS
CAN
BE
SEEN
INTERRUPT
PROCESSING
IS
FAR
FROM
TRIVIAL
IT
ALSO
TAKES
A
CONSID
ERABLE
NUMBER
OF
CPU
INSTRUCTIONS
ESPECIALLY
ON
MACHINES
IN
WHICH
VIRTUAL
MEM
ORY
IS
PRESENT
AND
PAGE
TABLES
HAVE
TO
BE
SET
UP
OR
THE
STATE
OF
THE
MMU
STORED
E
G
THE
R
AND
M
BITS
ON
SOME
MACHINES
THE
TLB
AND
CPU
CACHE
MAY
ALSO
HAVE
TO
BE
MANAGED
WHEN
SWITCHING
BETWEEN
USER
AND
KERNEL
MODES
WHICH
TAKES
ADDITIONAL
MACHINE
CYCLES
DEVICE
DRIVERS
EARLIER
IN
THIS
CHAPTER
WE
LOOKED
AT
WHAT
DEVICE
CONTROLLERS
DO
WE
SAW
THAT
EACH
CONTROLLER
HAS
SOME
DEVICE
REGISTERS
USED
TO
GIVE
IT
COMMANDS
OR
SOME
DE
VICE
REGISTERS
USED
TO
READ
OUT
ITS
STATUS
OR
BOTH
THE
NUMBER
OF
DEVICE
REGISTERS
AND
THE
NATURE
OF
THE
COMMANDS
VARY
RADICALLY
FROM
DEVICE
TO
DEVICE
FOR
EX
AMPLE
A
MOUSE
DRIVER
HAS
TO
ACCEPT
INFORMATION
FROM
THE
MOUSE
TELLING
HOW
FAR
IT
HAS
MOVED
AND
WHICH
BUTTONS
ARE
CURRENTLY
DEPRESSED
IN
CONTRAST
A
DISK
DRIVER
MAY
HAVE
TO
KNOW
ALL
ABOUT
SECTORS
TRACKS
CYLINDERS
HEADS
ARM
MOTION
MOTOR
INPUT
OUTPUT
CHAP
DRIVES
HEAD
SETTLING
TIMES
AND
ALL
THE
OTHER
MECHANICS
OF
MAKING
THE
DISK
WORK
PROPERLY
OBVIOUSLY
THESE
DRIVERS
WILL
BE
VERY
DIFFERENT
AS
A
CONSEQUENCE
EACH
I
O
DEVICE
ATTACHED
TO
A
COMPUTER
NEEDS
SOME
DE
VICE
SPECIFIC
CODE
FOR
CONTROLLING
IT
THIS
CODE
CALLED
THE
DEVICE
DRIVER
IS
GEN
ERALLY
WRITTEN
BY
THE
DEVICE
MANUFACTURER
AND
DELIVERED
ALONG
WITH
THE
DEVICE
SINCE
EACH
OPERATING
SYSTEM
NEEDS
ITS
OWN
DRIVERS
DEVICE
MANUFACTURERS
COM
MONLY
SUPPLY
DRIVERS
FOR
SEVERAL
POPULAR
OPERATING
SYSTEMS
EACH
DEVICE
DRIVER
NORMALLY
HANDLES
ONE
DEVICE
TYPE
OR
AT
MOST
ONE
CLASS
OF
CLOSELY
RELATED
DEVICES
FOR
EXAMPLE
A
SCSI
DISK
DRIVER
CAN
USUALLY
HANDLE
MULTI
PLE
SCSI
DISKS
OF
DIFFERENT
SIZES
AND
DIFFERENT
SPEEDS
AND
PERHAPS
A
SCSI
CD
ROM
AS
WELL
ON
THE
OTHER
HAND
A
MOUSE
AND
JOYSTICK
ARE
SO
DIFFERENT
THAT
DIF
FERENT
DRIVERS
ARE
USUALLY
REQUIRED
HOWEVER
THERE
IS
NO
TECHNICAL
RESTRICTION
ON
HAVING
ONE
DEVICE
DRIVER
CONTROL
MULTIPLE
UNRELATED
DEVICES
IT
IS
JUST
NOT
A
GOOD
IDEA
IN
ORDER
TO
ACCESS
THE
DEVICE
HARDWARE
MEANING
THE
CONTROLLER
REGISTERS
THE
DEVICE
DRIVER
NORMALLY
HAS
TO
BE
PART
OF
THE
OPERATING
SYSTEM
KERNEL
AT
LEAST
WITH
CURRENT
ARCHITECTURES
ACTUALLY
IT
IS
POSSIBLE
TO
CONSTRUCT
DRIVERS
THAT
RUN
IN
USER
SPACE
WITH
SYSTEM
CALLS
FOR
READING
AND
WRITING
THE
DEVICE
REGISTERS
THIS
DE
SIGN
ISOLATES
THE
KERNEL
FROM
THE
DRIVERS
AND
THE
DRIVERS
FROM
EACH
OTHER
ELIMINAT
ING
A
MAJOR
SOURCE
OF
SYSTEM
CRASHES
BUGGY
DRIVERS
THAT
INTERFERE
WITH
THE
KERNEL
IN
ONE
WAY
OR
ANOTHER
FOR
BUILDING
HIGHLY
RELIABLE
SYSTEMS
THIS
IS
DEFINITELY
THE
WAY
TO
GO
AN
EXAMPLE
OF
A
SYSTEM
IN
WHICH
THE
DEVICE
DRIVERS
RUN
AS
USER
PROC
ESSES
IS
MINIX
HOWEVER
SINCE
MOST
OTHER
DESKTOP
OPERATING
SYSTEMS
EXPECT
DRIVERS
TO
RUN
IN
THE
KERNEL
THAT
IS
THE
MODEL
WE
WILL
CONSIDER
HERE
SINCE
THE
DESIGNERS
OF
EVERY
OPERATING
SYSTEM
KNOW
THAT
PIECES
OF
CODE
DRIV
ERS
WRITTEN
BY
OUTSIDERS
WILL
BE
INSTALLED
IN
IT
IT
NEEDS
TO
HAVE
AN
ARCHITECTURE
THAT
ALLOWS
SUCH
INSTALLATION
THIS
MEANS
HAVING
A
WELL
DEFINED
MODEL
OF
WHAT
A
DRIVER
DOES
AND
HOW
IT
INTERACTS
WITH
THE
REST
OF
THE
OPERATING
SYSTEM
DEVICE
DRIVERS
ARE
NORMALLY
POSITIONED
BELOW
THE
REST
OF
THE
OPERATING
SYSTEM
AS
IS
ILLUSTRATED
IN
FIG
OPERATING
SYSTEMS
USUALLY
CLASSIFY
DRIVERS
INTO
ONE
OF
A
SMALL
NUMBER
OF
CAT
EGORIES
THE
MOST
COMMON
CATEGORIES
ARE
THE
BLOCK
DEVICES
SUCH
AS
DISKS
WHICH
CONTAIN
MULTIPLE
DATA
BLOCKS
THAT
CAN
BE
ADDRESSED
INDEPENDENTLY
AND
THE
CHARAC
TER
DEVICES
SUCH
AS
KEYBOARDS
AND
PRINTERS
WHICH
GENERATE
OR
ACCEPT
A
STREAM
OF
CHARACTERS
MOST
OPERATING
SYSTEMS
DEFINE
A
STANDARD
INTERFACE
THAT
ALL
BLOCK
DRIVERS
MUST
SUPPORT
AND
A
SECOND
STANDARD
INTERFACE
THAT
ALL
CHARACTER
DRIVERS
MUST
SUPPORT
THESE
INTERFACES
CONSIST
OF
A
NUMBER
OF
PROCEDURES
THAT
THE
REST
OF
THE
OPERATING
SYSTEM
CAN
CALL
TO
GET
THE
DRIVER
TO
DO
WORK
FOR
IT
TYPICAL
PROCEDURES
ARE
THOSE
TO
READ
A
BLOCK
BLOCK
DEVICE
OR
WRITE
A
CHARACTER
STRING
CHARACTER
DEVICE
IN
SOME
SYSTEMS
THE
OPERATING
SYSTEM
IS
A
SINGLE
BINARY
PROGRAM
THAT
CONTAINS
ALL
OF
THE
DRIVERS
THAT
IT
WILL
NEED
COMPILED
INTO
IT
THIS
SCHEME
WAS
THE
NORM
FOR
YEARS
WITH
UNIX
SYSTEMS
BECAUSE
THEY
WERE
RUN
BY
COMPUTER
CENTERS
AND
I
O
DE
SEC
I
O
SOFTWARE
LAYERS
USER
PROCESS
USER
SPACE
KERNEL
SPACE
HARDWARE
DEVICES
FIGURE
LOGICAL
POSITIONING
OF
DEVICE
DRIVERS
IN
REALITY
ALL
COMMUNICA
TION
BETWEEN
DRIVERS
AND
DEVICE
CONTROLLERS
GOES
OVER
THE
BUS
VICES
RARELY
CHANGED
IF
A
NEW
DEVICE
WAS
ADDED
THE
SYSTEM
ADMINISTRATOR
SIMPLY
RECOMPILED
THE
KERNEL
WITH
THE
NEW
DRIVER
TO
BUILD
A
NEW
BINARY
WITH
THE
ADVENT
OF
PERSONAL
COMPUTERS
WITH
THEIR
MYRIAD
I
O
DEVICES
THIS
MODEL
NO
LONGER
WORKED
FEW
USERS
ARE
CAPABLE
OF
RECOMPILING
OR
RELINKING
THE
KERNEL
EVEN
IF
THEY
HAVE
THE
SOURCE
CODE
OR
OBJECT
MODULES
WHICH
IS
NOT
ALWAYS
THE
CASE
INSTEAD
OPERATING
SYSTEMS
STARTING
WITH
MS
DOS
WENT
OVER
TO
A
MODEL
IN
WHICH
DRIVERS
WERE
DYNAMICALLY
LOADED
INTO
THE
SYSTEM
DURING
EXECUTION
DIF
FERENT
SYSTEMS
HANDLE
LOADING
DRIVERS
IN
DIFFERENT
WAYS
A
DEVICE
DRIVER
HAS
SEVERAL
FUNCTIONS
THE
MOST
OBVIOUS
ONE
IS
TO
ACCEPT
ABSTRACT
READ
AND
WRITE
REQUESTS
FROM
THE
DEVICE
INDEPENDENT
SOFTWARE
ABOVE
IT
AND
SEE
THAT
THEY
ARE
CARRIED
OUT
BUT
THERE
ARE
ALSO
A
FEW
OTHER
FUNCTIONS
THEY
MUST
PERFORM
FOR
EXAMPLE
THE
DRIVER
MUST
INITIALIZE
THE
DEVICE
IF
NEEDED
IT
MAY
ALSO
NEED
TO
MANAGE
ITS
POWER
REQUIREMENTS
AND
LOG
EVENTS
MANY
DEVICE
DRIVERS
HAVE
A
SIMILAR
GENERAL
STRUCTURE
A
TYPICAL
DRIVER
STARTS
OUT
BY
CHECKING
THE
INPUT
PARAMETERS
TO
SEE
IF
THEY
ARE
VALID
IF
NOT
AN
ERROR
IS
INPUT
OUTPUT
CHAP
RETURNED
IF
THEY
ARE
VALID
A
TRANSLATION
FROM
ABSTRACT
TO
CONCRETE
TERMS
MAY
BE
NEEDED
FOR
A
DISK
DRIVER
THIS
MAY
MEAN
CONVERTING
A
LINEAR
BLOCK
NUMBER
INTO
THE
HEAD
TRACK
SECTOR
AND
CYLINDER
NUMBERS
FOR
THE
DISK
GEOMETRY
NEXT
THE
DRIVER
MAY
CHECK
IF
THE
DEVICE
IS
CURRENTLY
IN
USE
IF
IT
IS
THE
REQUEST
WILL
BE
QUEUED
FOR
LATER
PROCESSING
IF
THE
DEVICE
IS
IDLE
THE
HARDWARE
STATUS
WILL
BE
EXAMINED
TO
SEE
IF
THE
REQUEST
CAN
BE
HANDLED
NOW
IT
MAY
BE
NECESSARY
TO
SWITCH
THE
DEVICE
ON
OR
START
A
MOTOR
BEFORE
TRANSFERS
CAN
BE
BEGUN
ONCE
THE
DE
VICE
IS
ON
AND
READY
TO
GO
THE
ACTUAL
CONTROL
CAN
BEGIN
CONTROLLING
THE
DEVICE
MEANS
ISSUING
A
SEQUENCE
OF
COMMANDS
TO
IT
THE
DRIV
ER
IS
THE
PLACE
WHERE
THE
COMMAND
SEQUENCE
IS
DETERMINED
DEPENDING
ON
WHAT
HAS
TO
BE
DONE
AFTER
THE
DRIVER
KNOWS
WHICH
COMMANDS
IT
IS
GOING
TO
ISSUE
IT
STARTS
WRITING
THEM
INTO
THE
CONTROLLER
DEVICE
REGISTERS
AFTER
WRITING
EACH
COM
MAND
TO
THE
CONTROLLER
IT
MAY
BE
NECESSARY
TO
CHECK
TO
SEE
IF
THE
CONTROLLER
AC
CEPTED
THE
COMMAND
AND
IS
PREPARED
TO
ACCEPT
THE
NEXT
ONE
THIS
SEQUENCE
CONTIN
UES
UNTIL
ALL
THE
COMMANDS
HAVE
BEEN
ISSUED
SOME
CONTROLLERS
CAN
BE
GIVEN
A
LINKED
LIST
OF
COMMANDS
IN
MEMORY
AND
TOLD
TO
READ
AND
PROCESS
THEM
ALL
BY
IT
SELF
WITHOUT
FURTHER
HELP
FROM
THE
OPERATING
SYSTEM
AFTER
THE
COMMANDS
HAVE
BEEN
ISSUED
ONE
OF
TWO
SITUATIONS
WILL
APPLY
IN
MANY
CASES
THE
DEVICE
DRIVER
MUST
WAIT
UNTIL
THE
CONTROLLER
DOES
SOME
WORK
FOR
IT
SO
IT
BLOCKS
ITSELF
UNTIL
THE
INTERRUPT
COMES
IN
TO
UNBLOCK
IT
IN
OTHER
CASES
HOWEV
ER
THE
OPERATION
FINISHES
WITHOUT
DELAY
SO
THE
DRIVER
NEED
NOT
BLOCK
AS
AN
EX
AMPLE
OF
THE
LATTER
SITUATION
SCROLLING
THE
SCREEN
IN
CHARACTER
MODE
REQUIRES
JUST
WRITING
A
FEW
BYTES
INTO
THE
CONTROLLER
REGISTERS
NO
MECHANICAL
MOTION
IS
NEED
ED
SO
THE
ENTIRE
OPERATION
CAN
BE
COMPLETED
IN
NANOSECONDS
IN
THE
FORMER
CASE
THE
BLOCKED
DRIVER
WILL
BE
AWAKENED
BY
THE
INTERRUPT
IN
THE
LATTER
CASE
IT
WILL
NEVER
GO
TO
SLEEP
EITHER
WAY
AFTER
THE
OPERATION
HAS
BEEN
COMPLETED
THE
DRIVER
MUST
CHECK
FOR
ERRORS
IF
EVERYTHING
IS
ALL
RIGHT
THE
DRIVER
MAY
HAVE
DATA
TO
PASS
TO
THE
DEVICE
INDEPENDENT
SOFTWARE
E
G
A
BLOCK
JUST
READ
FINALLY
IT
RETURNS
SOME
STATUS
INFORMATION
FOR
ERROR
REPORTING
BACK
TO
ITS
CALLER
IF
ANY
OTHER
REQUESTS
ARE
QUEUED
ONE
OF
THEM
CAN
NOW
BE
SELECTED
AND
STARTED
IF
NOTHING
IS
QUEUED
THE
DRIVER
BLOCKS
WAITING
FOR
THE
NEXT
REQUEST
THIS
SIMPLE
MODEL
IS
ONLY
A
ROUGH
APPROXIMATION
TO
REALITY
MANY
FACTORS
MAKE
THE
CODE
MUCH
MORE
COMPLICATED
FOR
ONE
THING
AN
I
O
DEVICE
MAY
COM
PLETE
WHILE
A
DRIVER
IS
RUNNING
INTERRUPTING
THE
DRIVER
THE
INTERRUPT
MAY
CAUSE
A
DEVICE
DRIVER
TO
RUN
IN
FACT
IT
MAY
CAUSE
THE
CURRENT
DRIVER
TO
RUN
FOR
EXAMPLE
WHILE
THE
NETWORK
DRIVER
IS
PROCESSING
AN
INCOMING
PACKET
ANOTHER
PACKET
MAY
ARRIVE
CONSEQUENTLY
DRIVERS
HAVE
TO
BE
REENTRANT
MEANING
THAT
A
RUNNING
DRIVER
HAS
TO
EXPECT
THAT
IT
WILL
BE
CALLED
A
SECOND
TIME
BEFORE
THE
FIRST
CALL
HAS
COM
PLETED
IN
A
HOT
PLUGGABLE
SYSTEM
DEVICES
CAN
BE
ADDED
OR
REMOVED
WHILE
THE
COM
PUTER
IS
RUNNING
AS
A
RESULT
WHILE
A
DRIVER
IS
BUSY
READING
FROM
SOME
DEVICE
THE
SYSTEM
MAY
INFORM
IT
THAT
THE
USER
HAS
SUDDENLY
REMOVED
THAT
DEVICE
FROM
THE
SYS
TEM
NOT
ONLY
MUST
THE
CURRENT
I
O
TRANSFER
BE
ABORTED
WITHOUT
DAMAGING
ANY
SEC
I
O
SOFTWARE
LAYERS
KERNEL
DATA
STRUCTURES
BUT
ANY
PENDING
REQUESTS
FOR
THE
NOW
VANISHED
DEVICE
MUST
ALSO
BE
GRACEFULLY
REMOVED
FROM
THE
SYSTEM
AND
THEIR
CALLERS
GIVEN
THE
BAD
NEWS
FURTHERMORE
THE
UNEXPECTED
ADDITION
OF
NEW
DEVICES
MAY
CAUSE
THE
KERNEL
TO
JUG
GLE
RESOURCES
E
G
INTERRUPT
REQUEST
LINES
TAKING
OLD
ONES
AWAY
FROM
THE
DRIVER
AND
GIVING
IT
NEW
ONES
IN
THEIR
PLACE
DRIVERS
ARE
NOT
ALLOWED
TO
MAKE
SYSTEM
CALLS
BUT
THEY
OFTEN
NEED
TO
INTERACT
WITH
THE
REST
OF
THE
KERNEL
USUALLY
CALLS
TO
CERTAIN
KERNEL
PROCEDURES
ARE
PERMIT
TED
FOR
EXAMPLE
THERE
ARE
USUALLY
CALLS
TO
ALLOCATE
AND
DEALLOCATE
HARDWIRED
PAGES
OF
MEMORY
FOR
USE
AS
BUFFERS
OTHER
USEFUL
CALLS
ARE
NEEDED
TO
MANAGE
THE
MMU
TIMERS
THE
DMA
CONTROLLER
THE
INTERRUPT
CONTROLLER
AND
SO
ON
DEVICE
INDEPENDENT
I
O
SOFTWARE
ALTHOUGH
SOME
OF
THE
I
O
SOFTWARE
IS
DEVICE
SPECIFIC
OTHER
PARTS
OF
IT
ARE
DE
VICE
INDEPENDENT
THE
EXACT
BOUNDARY
BETWEEN
THE
DRIVERS
AND
THE
DEVICE
INDE
PENDENT
SOFTWARE
IS
SYSTEM
AND
DEVICE
DEPENDENT
BECAUSE
SOME
FUNCTIONS
THAT
COULD
BE
DONE
IN
A
DEVICE
INDEPENDENT
WAY
MAY
ACTUALLY
BE
DONE
IN
THE
DRIVERS
FOR
EFFICIENCY
OR
OTHER
REASONS
THE
FUNCTIONS
SHOWN
IN
FIG
ARE
TYPICALLY
DONE
IN
THE
DEVICE
INDEPENDENT
SOFTWARE
UNIFORM
INTERFACING
FOR
DEVICE
DRIVERS
BUFFERING
ERROR
REPORTING
ALLOCATING
AND
RELEASING
DEDICATED
DEVICES
PROVIDING
A
DEVICE
INDEPENDENT
BLOCK
SIZE
FIGURE
FUNCTIONS
OF
THE
DEVICE
INDEPENDENT
I
O
SOFTWARE
THE
BASIC
FUNCTION
OF
THE
DEVICE
INDEPENDENT
SOFTWARE
IS
TO
PERFORM
THE
I
O
FUNCTIONS
THAT
ARE
COMMON
TO
ALL
DEVICES
AND
TO
PROVIDE
A
UNIFORM
INTERFACE
TO
THE
USER
LEVEL
SOFTWARE
BELOW
WE
WILL
LOOK
AT
THE
ABOVE
ISSUES
IN
MORE
DETAIL
UNIFORM
INTERFACING
FOR
DEVICE
DRIVERS
A
MAJOR
ISSUE
IN
AN
OPERATING
SYSTEM
IS
HOW
TO
MAKE
ALL
I
O
DEVICES
AND
DRIV
ERS
LOOK
MORE
OR
LESS
THE
SAME
IF
DISKS
PRINTERS
KEYBOARDS
AND
SO
ON
ARE
ALL
IN
TERFACED
IN
DIFFERENT
WAYS
EVERY
TIME
A
NEW
DEVICE
COMES
ALONG
THE
OPERATING
SYSTEM
MUST
BE
MODIFIED
FOR
THE
NEW
DEVICE
HAVING
TO
HACK
ON
THE
OPERATING
SYS
TEM
FOR
EACH
NEW
DEVICE
IS
NOT
A
GOOD
IDEA
ONE
ASPECT
OF
THIS
ISSUE
IS
THE
INTERFACE
BETWEEN
THE
DEVICE
DRIVERS
AND
THE
REST
OF
THE
OPERATING
SYSTEM
IN
FIG
A
WE
ILLUSTRATE
A
SITUATION
IN
WHICH
EACH
INPUT
OUTPUT
CHAP
DEVICE
DRIVER
HAS
A
DIFFERENT
INTERFACE
TO
THE
OPERATING
SYSTEM
WHAT
THIS
MEANS
IS
THAT
THE
DRIVER
FUNCTIONS
AVAILABLE
FOR
THE
SYSTEM
TO
CALL
DIFFER
FROM
DRIVER
TO
DRIV
ER
IT
MIGHT
ALSO
MEAN
THAT
THE
KERNEL
FUNCTIONS
THAT
THE
DRIVER
NEEDS
ALSO
DIFFER
FROM
DRIVER
TO
DRIVER
TAKEN
TOGETHER
IT
MEANS
THAT
INTERFACING
EACH
NEW
DRIVER
RE
QUIRES
A
LOT
OF
NEW
PROGRAMMING
EFFORT
OPERATING
SYSTEM
OPERATING
SYSTEM
SATA
DISK
DRIVER
IDE
DISK
DRIVER
SCSI
DISK
DRIVER
SATA
DISK
DRIVER
IDE
DISK
DRIVER
SCSI
DISK
DRIVER
A
B
FIGUR
E
A
WITHOUT
A
STANDARD
DRIVER
INTERFACE
B
WITH
A
STANDARD
DRIVER
INTERFACE
IN
CONTRAST
IN
FIG
B
WE
SHOW
A
DIFFERENT
DESIGN
IN
WHICH
ALL
DRIVERS
HAVE
THE
SAME
INTERFACE
NOW
IT
BECOMES
MUCH
EASIER
TO
PLUG
IN
A
NEW
DRIVER
PRO
VIDING
IT
CONFORMS
TO
THE
DRIVER
INTERFACE
IT
ALSO
MEANS
THAT
DRIVER
WRITERS
KNOW
WHAT
IS
EXPECTED
OF
THEM
IN
PRACTICE
NOT
ALL
DEVICES
ARE
ABSOLUTELY
IDENTICAL
BUT
USUALLY
THERE
ARE
ONLY
A
SMALL
NUMBER
OF
DEVICE
TYPES
AND
EVEN
THESE
ARE
GENERALLY
ALMOST
THE
SAME
THE
WAY
THIS
WORKS
IS
AS
FOLLOWS
FOR
EACH
CLASS
OF
DEVICES
SUCH
AS
DISKS
OR
PRINTERS
THE
OPERATING
SYSTEM
DEFINES
A
SET
OF
FUNCTIONS
THAT
THE
DRIVER
MUST
SUP
PLY
FOR
A
DISK
THESE
WOULD
NATURALLY
INCLUDE
READ
AND
WRITE
BUT
ALSO
TURNING
THE
POWER
ON
AND
OFF
FORMATTING
AND
OTHER
DISKY
THINGS
OFTEN
THE
DRIVER
CONTAINS
A
TABLE
WITH
POINTERS
INTO
ITSELF
FOR
THESE
FUNCTIONS
WHEN
THE
DRIVER
IS
LOADED
THE
OPERATING
SYSTEM
RECORDS
THE
ADDRESS
OF
THIS
TABLE
OF
FUNCTION
POINTERS
SO
WHEN
IT
NEEDS
TO
ALL
ONE
OF
THE
FUNCTIONS
IT
CAN
MAKE
AN
INDIRECT
CALL
VIA
THIS
TABLE
THIS
TABLE
OF
FUNCTION
POINTERS
DEFINES
THE
INTERFACE
BETWEEN
THE
DRIVER
AND
THE
REST
OF
THE
OPERATING
SYSTEM
ALL
DEVICES
OF
A
GIVEN
CLASS
DISKS
PRINTERS
ETC
MUST
OBEY
IT
ANOTHER
ASPECT
OF
HAVING
A
UNIFORM
INTERFACE
IS
HOW
I
O
DEVICES
ARE
NAMED
THE
DE
J
INDEPENDENT
SOFTWARE
TAKES
CARE
OF
MAPPING
SYMBOLIC
DEVICE
NAMES
ONTO
TL
HFIER
DRIVER
FOR
EXAMPLE
IN
UNIX
A
DEVICE
NAME
SUCH
AS
DEV
DISKO
UNIQUELY
SPECIFIES
THE
I
NODE
FOR
A
SPECIAL
FILE
AND
THIS
I
NODE
TAINS
THE
MAJOR
DEVICE
NUMBER
WHICH
IS
USED
TO
LOCATE
THE
APPROPRIATE
DRH
P
HE
I
NODE
ALSO
CONTAINS
THE
MINOR
DEVICE
NUMBER
WHICH
IS
PASSED
AS
A
PARAMETER
TO
THE
DRIVER
SEC
I
O
SOFTWARE
LAYERS
IN
ORDER
TO
SPECIFY
THE
UNIT
TO
BE
READ
OR
WRITTEN
ALL
DEVICES
HAVE
MAJOR
AND
MINOR
NUMBERS
AND
ALL
DRIVERS
ARE
ACCESSED
BY
USING
THE
MAJOR
DEVICE
NUMBER
TO
SELECT
THE
DRIVER
CLOSELY
RELATED
TO
NAMING
IS
PROTECTION
HOW
DOES
THE
SYSTEM
PREVENT
USERS
FROM
ACCESSING
DEVICES
THAT
THEY
ARE
NOT
ENTITIED
TO
ACCESS
IN
BOTH
UNIX
AND
WINDOWS
DEVICES
APPEAR
IN
THE
FILE
SYSTEM
AS
NAMED
OBJECTS
WHICH
MEANS
THAT
THE
USUAL
PROTECTION
RULES
FOR
FILES
ALSO
APPLY
TO
I
O
DEVICES
THE
SYSTEM
ADMINIS
TRATOR
CAN
THEN
SET
THE
PROPER
PERMISSIONS
FOR
EACH
DEVICE
BUFFERING
BUFFERING
IS
ALSO
AN
ISSUE
BOTH
FOR
BLOCK
AND
CHARACTER
DEVICES
FOR
A
VARIETY
OF
REASONS
TO
SEE
ONE
OF
THEM
CONSIDER
A
PROCESS
THAT
WANTS
TO
READ
DATA
FROM
A
MODEM
ONE
POSSIBLE
STRATEGY
FOR
DEALING
WITH
THE
INCOMING
CHARACTERS
IS
TO
HAVE
THE
USER
PROCESS
DO
A
READ
SYSTEM
CALL
AND
BLOCK
WAITING
FOR
ONE
CHARACTER
EACH
ARRIVING
CHARACTER
CAUSES
AN
INTERRUPT
THE
INTERRUPT
SERVICE
PROCEDURE
HANDS
THE
CHARACTER
TO
THE
USER
PROCESS
AND
UNBLOCKS
IT
AFTER
PUTTING
THE
CHARACTER
SOME
WHERE
THE
PROCESS
READS
ANOTHER
CHARACTER
AND
BLOCKS
AGAIN
THIS
MODEL
IS
INDI
CATED
IN
FIG
A
USER
PROCESS
M
O
D
E
M
MODEM
MODEM
MODEM
A
B
C
D
FIGURE
A
UNBUFFERED
INPUT
B
BUFFERING
IN
USER
SPACE
C
BUFFERING
IN
THE
KERNEL
FOLLOWED
BY
COPYING
TO
USER
SPACE
D
DOUBLE
BUFFERING
IN
THE
KERNEL
THE
TROUBLE
WITH
THIS
WAY
OF
DOING
BUSINESS
IS
THAT
THE
USER
PROCESS
HAS
TO
BE
STARTED
UP
FOR
EVERY
INCOMING
CHARACTER
ALLOWING
A
PROCESS
TO
RUN
MANY
TIMES
FOR
SHORT
RUNS
IS
INEFFICIENT
SO
THIS
DESIGN
IS
NOT
A
GOOD
ONE
AN
IMPROVEMENT
IS
SHOWN
IN
FIG
B
HERE
THE
USER
PROCESS
PROVIDES
AN
N
CHARACTER
BUFFER
IN
USER
SPACE
AND
DOES
A
READ
OF
N
CHARACTERS
THE
INTERRUPT
SER
VICE
PROCEDURE
PUTS
INCOMING
CHARACTERS
IN
THIS
BUFFER
UNTIL
IT
FILLS
UP
THEN
IT
WAKES
UP
THE
USER
PROCESS
THIS
SCHEME
IS
FAR
MORE
EFFICIENT
THAN
THE
PREVIOUS
INPUT
OUTPUT
CHAP
ONE
BUT
IT
HAS
A
DRAWBACK
WHAT
HAPPENS
IF
THE
BUFFER
IS
PAGED
OUT
WHEN
A
CHARAC
TER
ARRIVES
THE
BUFFER
COULD
BE
LOCKED
IN
MEMORY
BUT
IF
MANY
PROCESSES
START
LOCKING
PAGES
IN
MEMORY
THE
POOL
OF
AVAILABLE
PAGES
WILL
SHRINK
AND
PERFORMANCE
WILL
DEGRADE
YET
ANOTHER
APPROACH
IS
TO
CREATE
A
BUFFER
INSIDE
THE
KERNEL
AND
HAVE
THE
INTER
RUPT
HANDLER
PUT
THE
CHARACTERS
THERE
AS
SHOWN
IN
FIG
C
WHEN
THIS
BUFFER
IS
FULL
THE
PAGE
WITH
THE
USER
BUFFER
IS
BROUGHT
IN
IF
NEEDED
AND
THE
BUFFER
COPIED
THERE
IN
ONE
OPERATION
THIS
SCHEME
IS
FAR
MORE
EFFICIENT
HOWEVER
EVEN
THIS
SCHEME
SUFFERS
FROM
A
PROBLEM
WHAT
HAPPENS
TO
CHARAC
SEC
I
O
SOFTWARE
LAYERS
USE
R
PROCES
USE
R
J
SPAC
E
KERNE
L
J
SPACE
NETWOR
K
TERS
THAT
ARRIVE
WHILE
THE
PAGE
WITH
THE
USER
BUFFER
IS
BEING
BROUGHT
IN
FROM
THE
DISK
SINCE
THE
BUFFER
IS
FULL
THERE
IS
NO
PLACE
TO
PUT
THEM
A
WAY
OUT
IS
TO
HAVE
A
SECOND
KERNEL
BUFFER
AFTER
THE
FIRST
BUFFER
FILLS
UP
BUT
BEFORE
IT
HAS
BEEN
EMP
TIED
THE
SECOND
ONE
IS
USED
AS
SHOWN
IN
FIG
D
WHEN
THE
SECOND
BUFFER
FILLS
UP
IT
IS
AVAILABLE
TO
BE
COPIED
TO
THE
USER
ASSUMING
THE
USER
HAS
ASKED
FOR
IT
CONTROLLER
NETWOR
K
R
WHILE
THE
SECOND
BUFFER
IS
BEING
COPIED
TO
USER
SPACE
THE
FIRST
ONE
CAN
BE
USED
FOR
NEW
CHARACTERS
IN
THIS
WAY
THE
TWO
BUFFERS
TAKE
TURNS
WHILE
ONE
IS
BEING
COPIED
TO
USER
SPACE
THE
OTHER
IS
ACCUMULATING
NEW
INPUT
A
BUFFERING
SCHEME
LIKE
THIS
IS
CALLED
DOUBLE
BUFFERING
ANOTHER
FORM
OF
BUFFERING
THAT
IS
WIDELY
USED
IS
THE
CIRCULAR
BUFFER
IT
CON
SISTS
OF
A
REGION
OF
MEMORY
AND
TWO
POINTERS
ONE
POINTER
POINTS
TO
THE
NEXT
FREE
WORD
WHERE
NEW
DATA
CAN
BE
PLACED
THE
OTHER
POINTER
POINTS
TO
THE
FIRST
WORD
OF
DATA
IN
THE
BUFFER
THAT
HAS
NOT
BEEN
REMOVED
YET
IN
MANY
SITUATIONS
THE
HARDWARE
ADVANCES
THE
FIRST
POINTER
AS
IT
ADDS
NEW
DATA
E
G
JUST
ARRIVING
FROM
THE
NETWORK
AND
THE
OPERATING
SYSTEM
ADVANCES
THE
SECOND
POINTER
AS
IT
REMOVES
AND
PROCESSES
DATA
BOTH
POINTERS
WRAP
AROUND
GOING
BACK
TO
THE
BOTTOM
WHEN
THEY
HIT
THE
TOP
BUFFERING
IS
ALSO
IMPORTANT
ON
OUTPUT
CONSIDER
FOR
EXAMPLE
HOW
OUTPUT
IS
DONE
TO
THE
MODEM
WITHOUT
BUFFERING
USING
THE
MODEL
OF
FIG
B
THE
USER
PROCESS
EXECUTES
A
WRITE
SYSTEM
CALL
TO
OUTPUT
N
CHARACTERS
THE
SYSTEM
HAS
TWO
CHOICES
AT
THIS
POINT
IT
CAN
BLOCK
THE
USER
UNTIL
ALL
THE
CHARACTERS
HAVE
BEEN
WRIT
TEN
BUT
THIS
COULD
TAKE
A
VERY
LONG
TIME
OVER
A
SLOW
TELEPHONE
LINE
IT
COULD
ALSO
RELEASE
THE
USER
IMMEDIATELY
AND
DO
THE
I
O
WHILE
THE
USER
COMPUTES
SOME
MORE
BUT
THIS
LEADS
TO
AN
EVEN
WORSE
PROBLEM
HOW
DOES
THE
USER
PROCESS
KNOW
THAT
THE
OUTPUT
HAS
BEEN
COMPLETED
AND
IT
CAN
REUSE
THE
BUFFER
THE
SYSTEM
COULD
GENERATE
A
SIGNAL
OR
SOFTWARE
INTERRUPT
BUT
THAT
STYLE
OF
PROGRAMMING
IS
DIFFICULT
AND
PRONE
TO
RACE
CONDITIONS
A
MUCH
BETTER
SOLUTION
IS
FOR
THE
KERNEL
TO
COPY
THE
DATA
TO
A
KERNEL
BUFFER
ANALOGOUS
IN
FIG
C
BUT
THE
OTHER
WAY
AND
UNBLOCK
THE
CALLER
IMMEDIATELY
NOW
IT
DOES
NOT
MATTER
WHEN
THE
ACTUAL
I
O
HAS
BEEN
COMPLETED
THE
USER
IS
FREE
TO
REUSE
THE
BUFFER
THE
INSTANT
IT
IS
UNBLOCKED
BUFFERING
IS
A
WIDELY
USED
TECHNIQUE
BUT
IT
HAS
A
DOWNSIDE
AS
WELL
IF
DATA
GET
BUFFERED
TOO
MANY
TIMES
PERFORMANCE
SUFFERS
CONSIDER
FOR
EXAMPLE
THE
NET
WORK
OF
FIG
HERE
A
USER
DOES
A
SYSTEM
CALL
TO
WRITE
TO
THE
NETWORK
THE
KERNEL
COPIES
THE
PACKET
TO
A
KERNEL
BUFFER
TO
ALLOW
THE
USER
TO
PROCEED
IMMEDIATE
LY
STEP
AT
THIS
POINT
THE
USER
PROGRAM
CAN
REUSE
THE
BUFFER
FIGURE
NETWORKING
MAY
INVOLVE
MANY
COPIES
OF
A
PACKET
WHEN
THE
DRIVER
IS
CALLED
IT
COPIES
THE
PACKET
TO
THE
CONTROLLER
FOR
OUTPUT
STEP
THE
REASON
IT
DOES
NOT
OUTPUT
TO
THE
WIRE
DIRECTLY
FROM
KERNEL
MEMORY
IS
THAT
ONCE
A
PACKET
TRANSMISSION
HAS
BEEN
STARTED
IT
MUST
CONTINUE
AT
A
UNIFORM
SPEED
THE
DRIVER
CANNOT
GUARANTEE
THAT
IT
CAN
GET
TO
MEMORY
AT
A
UNIFORM
SPEED
BECAUSE
DMA
CHANNELS
AND
OTHER
I
O
DEVICES
MAY
BE
STEALING
MANY
CYCLES
FAILI
NG
TO
GET
A
WORD
ON
TIME
WOULD
RUIN
THE
PACKET
BY
BUFFERING
THE
PACKET
INSIDE
THE
CON
TROLLER
THIS
PROBLEM
IS
AVOIDED
AFTER
THE
PACKET
HAS
BEEN
COPIED
TO
THE
CONTROLLER
INTERNAL
BUFFER
IT
IS
COPIED
OUT
ONTO
THE
NETWORK
STEP
BITS
ARRIVE
AT
THE
RECEIVER
SHORTLY
AFTER
BEING
SENT
SO
JUST
AFTER
THE
LAST
BIT
HAS
BEEN
SENT
THAT
BIT
ARRIVES
AT
THE
RECEIVER
WHERE
THE
PACKET
HAS
BEEN
BUFFERED
IN
THE
CONTROLLER
NEXT
THE
PACKET
IS
COPIED
TO
THE
RE
CEIVER
KERNEL
BUFFER
STEP
FINALLY
IT
IS
COPIED
TO
THE
RECEIVING
PROCESS
BUFF
ER
STEP
USUALLY
THE
RECEIVER
THEN
SENDS
BACK
AN
ACKNOWLEDGEMENT
WHEN
THE
SENDER
GETS
THE
ACKNOWLEDGEMENT
IT
IS
FREE
TO
SEND
THE
NEXT
PACKET
HOWEVER
IT
SHOULD
BE
CLEAR
THAT
ALL
THIS
COPYING
IS
GOING
TO
SLOW
DOWN
THE
TRANSMISSION
RATE
CONSIDERABLY
BECAUSE
ALL
THE
STEPS
MUST
HAPPEN
SEQUENTIALLY
ERROR
REPORTING
ERRORS
ARE
FAR
MORE
COMMON
IN
THE
CONTEXT
OF
I
O
THAN
IN
OTHER
CONTEXTS
WHEN
THEY
OCCUR
THE
OPERATING
SYSTEM
MUST
HANDLE
THEM
AS
BEST
IT
CAN
MANY
ER
RORS
ARE
DEVICE
SPECIFIC
AND
MUST
BE
HANDLED
BY
THE
APPROPRIATE
DRIVER
BUT
THE
FRAMEWORK
FOR
ERROR
HANDLING
IS
DEVICE
INDEPENDENT
ONE
CLASS
OF
I
O
ERRORS
IS
PROGRAMMING
ERRORS
THESE
OCCUR
WHEN
A
PROCESS
ASKS
FOR
SOMETHING
IMPOSSIBLE
SUCH
AS
WRITING
TO
AN
INPUT
DEVICE
KEYBOARD
SCAN
NER
MOUSE
ETC
OR
READING
FROM
AN
OUTPUT
DEVICE
PRINTER
PLOTTER
ETC
OTHER
ERRORS
ARE
PROVIDING
AN
INVALID
BUFFER
ADDRESS
OR
OTHER
PARAMETER
AND
SPECIFYING
INPUT
OUTPUT
CHAP
AN
INVALID
DEVICE
E
G
DISK
WHEN
THE
SYSTEM
HAS
ONLY
TWO
DISKS
AND
SO
ON
THE
ACTION
TO
TAKE
ON
THESE
ERRORS
IS
STRAIGHTFORWARD
JUST
REPORT
BACK
AN
ERROR
CODE
TO
THE
CALLER
ANOTHER
CLASS
OF
ERRORS
IS
THE
CLASS
OF
ACTUAL
I
O
ERRORS
FOR
EXAMPLE
TRYING
TO
WRITE
A
DISK
BLOCK
THAT
HAS
BEEN
DAMAGED
OR
TRYING
TO
READ
FROM
A
CAMCORDER
THAT
HAS
BEEN
SWITCHED
OFF
IN
THESE
CIRCUMSTANCES
IT
IS
UP
TO
THE
DRIVER
TO
DETERMINE
WHAT
TO
DO
IF
THE
DRIVER
DOES
NOT
KNOW
WHAT
TO
DO
IT
MAY
PASS
THE
PROBLEM
BACK
UP
TO
DEVICE
INDEPENDENT
SOFTWARE
WHAT
THIS
SOFTWARE
DOES
DEPENDS
ON
THE
ENVIRONMENT
AND
THE
NATURE
OF
THE
ER
ROR
IF
IT
IS
A
SIMPLE
READ
ERROR
AND
THERE
IS
AN
INTERACTIVE
USER
AVAILABLE
IT
MAY
DISPLAY
A
DIALOG
BOX
ASKING
THE
USER
WHAT
TO
DO
THE
OPTIONS
MAY
INCLUDE
RETRYING
A
CERTAIN
NUMBER
OF
TIMES
IGNORING
THE
ERROR
OR
KILLING
THE
CALLING
PROCESS
IF
THERE
IS
NO
USER
AVAILABLE
PROBABLY
THE
ONLY
REAL
OPTION
IS
TO
HAVE
THE
SYSTEM
CALL
FAIL
WITH
AN
ERROR
CODE
HOWEVER
SOME
ERRORS
CANNOT
BE
HANDLED
THIS
WAY
FOR
EXAMPLE
A
CRITICAL
DATA
STRUCTURE
SUCH
AS
THE
ROOT
DIRECTORY
OR
FREE
BLOCK
LIST
MAY
HAVE
BEEN
DES
TROYED
IN
THIS
CASE
THE
SYSTEM
MAY
HAVE
TO
DISPLAY
AN
ERROR
MESSAGE
AND
TERMI
NATE
ALLOCATING
AND
RELEASING
DEDICATED
DEVICES
SOME
DEVICES
SUCH
AS
CD
ROM
RECORDERS
CAN
BE
USED
ONLY
BY
A
SINGLE
PROC
ESS
AT
ANY
GIVEN
MOMENT
IT
IS
UP
TO
THE
OPERATING
SYSTEM
TO
EXAMINE
REQUESTS
FOR
DEVICE
USAGE
AND
ACCEPT
OR
REJECT
THEM
DEPENDING
ON
WHETHER
THE
REQUESTED
DE
VICE
IS
AVAILABLE
OR
NOT
A
SIMPLE
WAY
TO
HANDLE
THESE
REQUESTS
IS
TO
REQUIRE
PROC
ESSES
TO
PERFORM
OPENS
ON
THE
SPECIAL
FILES
FOR
DEVICES
DIRECTLY
IF
THE
DEVICE
IS
UNAVAILABLE
THE
OPEN
FAILS
CLOSING
SUCH
A
DEDICATED
DEVICE
THEN
RELEASES
IT
AN
ALTERNATIVE
APPROACH
IS
TO
HAVE
SPECIAL
MECHANISMS
FOR
REQUESTING
AND
RELEASING
DEDICATED
DEVICES
AN
ATTEMPT
TO
ACQUIRE
A
DEVICE
THAT
IS
NOT
AVAILABLE
BLOCKS
THE
CALLER
INSTEAD
OF
FAILING
BLOCKED
PROCESSES
ARE
PUT
ON
A
QUEUE
SOONER
OR
LATER
THE
REQUESTED
DEVICE
BECOMES
AVAILABLE
AND
THE
FIRST
PROCESS
ON
THE
QUEUE
IS
ALLOWED
TO
ACQUIRE
IT
AND
CONTINUE
EXECUTION
DEVICE
INDEPENDENT
BLOCK
SIZE
DIFFERENT
DISKS
MAY
HAVE
DIFFERENT
SECTOR
SIZES
IT
IS
UP
TO
THE
DEVICE
INDEPEN
DENT
SOFTWARE
TO
HIDE
THIS
FACT
AND
PROVIDE
A
UNIFORM
BLOCK
SIZE
TO
HIGHER
LAYERS
FOR
EXAMPLE
BY
TREATING
SEVERAL
SECTORS
AS
A
SINGLE
LOGICAL
BLOCK
IN
THIS
WAY
THE
HIGHER
LAYERS
ONLY
DEAL
WITH
ABSTRACT
DEVICES
THAT
ALL
USE
THE
SAME
LOGICAL
BLOCK
SIZE
INDEPENDENT
OF
THE
PHYSICAL
SECTOR
SIZE
SIMILARLY
SOME
CHARACTER
DEVICES
DELIVER
THEIR
DATA
ONE
BYTE
AT
A
TIME
E
G
MODEMS
WHILE
OTHERS
DELIVER
THEIRS
IN
LARGER
UNITS
E
G
NETWORK
INTERFACES
THESE
DIFFERENCES
MAY
ALSO
BE
HIDDEN
SEC
I
O
SOFFWARE
LAYERS
USER
SPACE
I
O
SOFTWARE
ALTHOUGH
MOST
OF
THE
I
O
SOFTWARE
IS
WITHIN
THE
OPERATING
SYSTEM
A
SMALL
POR
TION
OF
IT
CONSISTS
OF
LIBRARIES
LINKED
TOGETHER
WITH
USER
PROGRAMS
AND
EVEN
WHOLE
PROGRAMS
RUNNING
OUTSIDE
THE
KERNEL
SYSTEM
CALLS
INCLUDING
THE
I
O
SYSTEM
CALLS
ARE
NORMALLY
MADE
BY
LIBRARY
PROCEDURES
WHEN
A
C
PROGRAM
CONTAINS
THE
CALL
COUNT
WRITE
FD
BUFFER
NBYTES
THE
LIBRARY
PROCEDURE
WRITE
WILL
BE
LINKED
WITH
THE
PROGRAM
AND
CONTAINED
IN
THE
BINARY
PROGRAM
PRESENT
IN
MEMORY
AT
RUN
TIME
THE
COLLECTION
OF
ALL
THESE
LIBRARY
PROCEDURES
IS
CLEARLY
PART
OF
THE
I
O
SYSTEM
WHILE
THESE
PROCEDURES
DO
LITTLE
MORE
THAN
PUT
THEIR
PARAMETERS
IN
THE
APPROPRIATE
PLACE
FOR
THE
SYSTEM
CALL
THERE
ARE
OTHER
I
O
PROCEDURES
THAT
ACTUALLY
DO
REAL
WORK
IN
PARTICULAR
FORMATTING
OF
INPUT
AND
OUTPUT
IS
DONE
BY
LIBRARY
PRO
CEDURES
ONE
EXAMPLE
FROM
C
IS
PRINTF
WHICH
TAKES
A
FORMAT
STRING
AND
POSSIBLY
SOME
VARIABLES
AS
INPUT
BUILDS
AN
ASCII
STRING
AND
THEN
CALLS
WRITE
TO
OUTPUT
THE
STRING
AS
AN
EXAMPLE
OF
PRINTF
CONSIDER
THE
STATEMENT
PRINTF
THE
SQUARE
OF
IS
N
I
I
I
IT
FORMATS
A
STRING
CONSISTING
OF
THE
CHARACTER
STRING
THE
SQUARE
OF
FOLLOWED
BY
THE
VALUE
I
AS
A
CHARACTER
STRING
THEN
THE
CHARACTER
STRING
IS
THEN
I
AS
SIX
CHARACTERS
AND
FINALLY
A
LINE
FEED
AN
EXAMPLE
OF
A
SIMILAR
PROCEDURE
FOR
INPUT
IS
SCANF
WHICH
READS
INPUT
AND
STORES
IT
INTO
VARIABLES
DESCRIBED
IN
A
FORMAT
STRING
USING
THE
SAME
SYNTAX
AS
PRINTF
THE
STANDARD
I
O
LIBRARY
CONTAINS
A
NUMBER
OF
PROCEDURES
THAT
INVOLVE
I
O
AND
ALL
RUN
AS
PART
OF
USER
PROGRAMS
NOT
ALL
USER
LEVEL
I
O
SOFTWARE
CONSISTS
OF
LIBRARY
PROCEDURES
ANOTHER
IMPOR
TANT
CATEGORY
IS
THE
SPOOLING
SYSTEM
SPOOLING
IS
A
WAY
OF
DEALING
WITH
DEDICATED
I
O
DEVICES
IN
A
MULTIPROGRAMMING
SYSTEM
CONSIDER
A
TYPICAL
SPOOLED
DEVICE
A
PRINTER
ALTHOUGH
IT
WOULD
BE
TECHNICALLY
EASY
TO
LET
ANY
USER
PROCESS
OPEN
THE
CHARACTER
SPECIAL
FILE
FOR
THE
PRINTER
SUPPOSE
A
PROCESS
OPENED
IT
AND
THEN
DID
NOTHING
FOR
HOURS
NO
OTHER
PROCESS
COULD
PRINT
ANYTHING
INSTEAD
WHAT
IS
DONE
IS
TO
CREATE
A
SPECIAL
PROCESS
CALLED
A
DAEMON
AND
A
SPECIAL
DIRECTORY
CALLED
A
SPOOLING
DIRECTORY
TO
PRINT
A
FILE
A
PROCESS
FUST
GEN
ERATES
THE
ENTIRE
FILE
TO
BE
PRINTED
AND
PUTS
IT
IN
THE
SPOOLING
DIRECTORY
IT
IS
UP
TO
THE
DAEMON
WHICH
IS
THE
ONLY
PROCESS
HAVING
PERMISSION
TO
USE
THE
PRINTER
SPE
CIAL
FILE
TO
PRINT
THE
FILES
IN
THE
DIRECTORY
BY
PROTECTING
THE
SPECIAL
FILE
AGAINST
DIRECT
USE
BY
USERS
THE
PROBLEM
OF
HAVING
SOMEONE
KEEPING
IT
OPEN
UNNECESSARILY
LONG
IS
ELIMINATED
SPOOLING
IS
NOT
ONLY
USED
FOR
PRINTERS
IT
IS
ALSO
USED
IN
OTHER
I
O
SITUATIONS
FOR
EXAMPLE
FILE
TRANSFER
OVER
A
NETWORK
OFTEN
USES
A
NETWORK
DAEMON
TO
SEND
A
FILE
SOMEWHERE
A
USER
PUTS
IT
IN
A
NETWORK
SPOOLING
DIRECTORY
LATER
ON
THE
NET
WORK
DAEMON
TAKES
IT
OUT
AND
TRANSMITS
IT
ONE
PARTICULAR
USE
OF
SPOOLED
FILE
INPUT
OUTPUT
CHAP
TRANSMISSION
IS
THE
USENET
NEWS
SYSTEM
THIS
NETWORK
CONSISTS
OF
MILLIONS
OF
MACHINES
AROUND
THE
WORLD
COMMUNICATING
USING
THE
INTERNET
THOUSANDS
OF
NEWS
GROUPS
EXIST
ON
MANY
TOPICS
TO
POST
A
NEWS
MESSAGE
THE
USER
INVOKES
A
NEWS
PROGRAM
WHICH
ACCEPTS
THE
MESSAGE
TO
BE
POSTED
AND
THEN
DEPOSITS
IT
IN
A
SPOOL
ING
DIRECTORY
FOR
TRANSMISSION
TO
OTHER
MACHINES
LATER
THE
ENTIRE
NEWS
SYSTEM
RUNS
OUTSIDE
THE
OPERATING
SYSTEM
FIGURE
SUMMARIZES
THE
I
O
SYSTEM
SHOWING
ALL
THE
LAYERS
AND
THE
PRINCI
PAL
FUNCTIONS
OF
EACH
LAYER
STARTING
AT
THE
BOTTOM
THE
LAYERS
ARE
THE
HARDWARE
IN
TERRUPT
HANDLERS
DEVICE
DRIVERS
DEVICE
INDEPENDENT
SOFTWARE
AND
FINALLY
THE
USER
PROCESSES
SEC
DISKS
DISK
HARDWARE
DISKS
COME
IN
A
VARIETY
OF
TYPES
THE
MOST
COMMON
ONES
ARE
THE
MAGNETIC
DISKS
HARD
DISKS
AND
FLOPPY
DISKS
THEY
ARE
CHARACTERIZED
BY
THE
FACT
THAT
READS
AND
WRITES
ARE
EQUALLY
FAST
WHICH
MAKES
THEM
IDEAL
AS
SECONDARY
MEMORY
PAG
ING
FILE
SYSTEMS
ETC
ARRAYS
OF
THESE
DISKS
ARE
SOMETIMES
USED
TO
PROVIDE
HIGH
LY
RELIABLE
STORAGE
FOR
DISTRIBUTION
OF
PROGRAMS
DATA
AND
MOVIES
VARIOUS
KINDS
OF
OPTICAL
DISKS
CD
ROMS
CD
RECORDABLES
AND
DVDS
ARE
ALSO
IMPORTANT
IN
THE
FOLLOWING
SECTIONS
WE
WILL
FIRST
DESCRIBE
THE
HARDWARE
AND
THEN
THE
SOFTWARE
FOR
THESE
DEVICES
I
O
LAYE
R
Y
REPLY
I
O
FUNCTIONS
MAGNETIC
DISKS
I
O
REQUEST
USER
PROCESSES
A
DEVICE
INDEPENDEN
T
SOFTWARE
DEVICE
DRIVERS
INTERRUPT
HANDLERS
HARDWAR
E
MAK
E
I
O
CALL
FORMAT
I
O
SPOOLING
NAMING
PROTECTION
BLOCKING
BUFFERING
ALLOCATION
SE
T
UP
DEVICE
REGISTERS
CHECK
STATUS
WAK
E
U
P
DRIVER
WHE
N
I
O
COMPLETE
D
PERFORM
I
O
OPERATION
MAGNETIC
DISKS
ARE
ORGANIZED
INTO
CYLINDERS
EACH
ONE
CONTAINING
AS
MANY
TRACKS
AS
THERE
ARE
HEADS
STACKED
VERTICALLY
THE
TRACKS
ARE
DIVIDED
INTO
SECTORS
WITH
THE
NUMBER
OF
SECTORS
AROUND
THE
CIRCUMFERENCE
TYPICALLY
BEING
TO
ON
FLOPPY
DISKS
AND
UP
TO
SEVERAL
HUNDRED
ON
HARD
DISKS
THE
NUMBER
OF
HEADS
VARIES
FROM
TO
ABOUT
OLDER
DISKS
HAVE
LITTIE
ELECTRONICS
AND
JUST
DELIVER
A
SIMPLE
SERIAL
BIT
STREAM
ON
THESE
DISKS
THE
CONTROLLER
DOES
MOST
OF
THE
WORK
ON
OTHER
DISKS
IN
PARTICULAR
IDE
INTEGRATED
DRIVE
ELECTRONICS
AND
SAT
A
SERIAL
ATA
DISKS
THE
DISK
DRIVE
ITSELF
CONTAINS
A
MICROCONTROLLER
THAT
DOES
CONSIDERABLE
WORK
AND
ALLOWS
THE
REAL
CONTROLLER
TO
ISSUE
A
SET
OF
HIGHER
LEVEL
COMMANDS
THE
CONTROLLER
OFTEN
DOES
TRACK
CACHING
BAD
BLOCK
REMAPPING
AND
MUCH
MORE
FIGURE
LAYERS
OF
THE
I
O
SYSTEM
AND
THE
MAIN
FUNCTIONS
OF
EACH
LAYER
THE
ARROWS
IN
FIG
SHOW
THE
FLOW
OF
CONTROL
WHEN
A
USER
PROGRAM
TRIES
TO
READ
A
BLOCK
FROM
A
FILE
FOR
EXAMPLE
THE
OPERATING
SYSTEM
IS
INVOKED
TO
CARRY
OUT
THE
CALL
THE
DEVICE
INDEPENDENT
SOFTWARE
LOOKS
FOR
IT
IN
THE
BUFFER
CACHE
FOR
EXAMPLE
IF
THE
NEEDED
BLOCK
IS
NOT
THERE
IT
CALLS
THE
DEVICE
DRIVER
TO
ISSUE
THE
RE
QUEST
TO
THE
HARDWARE
TO
GO
GET
IT
FROM
THE
DISK
THE
PROCESS
IS
THEN
BLOCKED
UNTIL
THE
DISK
OPERATION
HAS
BEEN
COMPLETED
WHEN
THE
DISK
IS
FINISHED
THE
HARDWARE
GENERATES
AN
INTERRUPT
THE
INTERRUPT
HANDLER
IS
RUN
TO
DISCOVER
WHAT
HAS
HAPPENED
THAT
IS
WHICH
DEVICE
WANTS
ATTEN
TION
RIGHT
NOW
IT
THEN
EXTRACTS
THE
STATUS
FROM
THE
DEVICE
AND
WAKES
UP
THE
SLEEP
ING
PROCESS
TO
FINISH
OFF
THE
I
O
REQUEST
AND
LET
THE
USER
PROCESS
CONTINUE
DISKS
NOW
WE
WILL
BEGIN
STUDYING
SOME
REAL
I
O
DEVICES
WE
WILL
BEGIN
WITH
DISKS
WHICH
ARE
CONCEPTUALLY
SIMPLE
YET
VERY
IMPORTANT
AFTER
THAT
WE
WILL
EXAMINE
CLOCKS
KEYBOARDS
AND
DISPLAYS
A
DEVICE
FEATURE
THAT
HAS
IMPORTANT
IMPLICATIONS
FOR
THE
DISK
DRIVER
IS
THE
POS
SIBILITY
OF
A
CONTROLLER
DOING
SEEKS
ON
TWO
OR
MORE
DRIVES
AT
THE
SAME
TIME
THESE
ARE
KNOWN
AS
OVERLAPPED
SEEKS
WHILE
THE
CONTROLLER
AND
SOFTWARE
ARE
WAITING
FOR
A
SEEK
TO
COMPLETE
ON
ONE
DRIVE
THE
CONTROLLER
CAN
INITIATE
A
SEEK
ON
ANOTHER
DRIVE
MANY
CONTROLLERS
CAN
ALSO
READ
OR
WRITE
ON
ONE
DRIVE
WHILE
SEEKING
ON
ONE
OR
MORE
OTHER
DRIVES
BUT
A
FLOPPY
DISK
CONTROLLER
CANNOT
READ
OR
WRITE
ON
TWO
DRIVES
AT
THE
SAME
TIME
READING
OR
WRITING
REQUIRES
THE
CONTROLLER
TO
MOVE
BITS
ON
A
MICROSECOND
TIME
SCALE
SO
ONE
TRANSFER
USES
UP
MOST
OF
ITS
COMPUTING
POW
ER
THE
SITUATION
IS
DIFFERENT
FOR
HARD
DISKS
WITH
INTEGRATED
CONTROLLERS
AND
IN
A
SYSTEM
WITH
MORE
THAN
ONE
OF
THESE
HARD
DRIVES
THEY
CAN
OPERATE
SIMULTANEOUSLY
AT
LEAST
TO
THE
EXTENT
OF
TRANSFERRING
BETWEEN
THE
DISK
AND
THE
CONTROLLER
BUFFER
MEMORY
ONLY
ONE
TRANSFER
BETWEEN
THE
CONTROLLER
AND
THE
MAIN
MEMORY
IS
POS
SIBLE
AT
ONCE
HOWEVER
THE
ABILITY
TO
PERFORM
TWO
OR
MORE
OPERATIONS
AT
THE
SAME
TIME
CAN
REDUCE
THE
AVERAGE
ACCESS
TIME
CONSIDERABLY
FIGURE
COMPARES
PARAMETERS
OF
THE
STANDARD
STORAGE
MEDIUM
FOR
THE
ORIG
INAL
IBM
PC
WITH
PARAMETERS
OF
A
DISK
MADE
YEARS
LATER
TO
SHOW
HOW
MUCH
DISKS
CHANGED
IN
YEARS
IT
IS
INTERESTING
TO
NOTE
THAT
NOT
ALL
PARAMETERS
HAVE
IMPROVED
AS
MUCH
AVERAGE
SEEK
TIME
IS
SEVEN
TIMES
BETTER
THAN
IT
WAS
TRANSFER
RATE
IS
TIMES
BETTER
WHILE
CAPACITY
IS
UP
BY
A
FACTOR
OF
THIS
PATTERN
INPUT
OUTPUT
CHAP
HAS
TO
DO
WITH
RELATIVELY
GRADUAL
IMPROVEMENTS
IN
THE
MOVING
PARTS
BUT
MUCH
HIGHER
BIT
DENSITIES
ON
THE
RECORDING
SURFACES
PARAMETER
IBM
KB
FLOPPY
DISK
WD
HARD
DISK
NUMBER
OF
CYLINDERS
TRACKS
PER
CYLINDER
SECTORS
PER
TRACK
AVG
SECTORS
PER
DISK
BYTES
PER
SECTOR
DISK
CAPACITY
KB
GB
SEEK
TIME
ADJACENT
CYLINDERS
MSE
C
MSE
C
SEEK
TIME
AVERAGE
CASE
MSE
C
MSE
C
ROTATION
TIME
MSE
C
MSE
C
MOTOR
STOP
START
TIME
MSE
C
SE
C
TIME
TO
TRANSFER
SECTOR
MSE
C
USE
E
FIGURE
DISK
PARAMETERS
FOR
THE
ORIGINAL
IBM
PC
KB
FLOPPY
DISK
AND
A
WESTERN
DIGITAL
WD
HARD
DISK
ONE
THING
TO
BE
AWARE
OF
IN
LOOKING
AT
THE
SPECIFICATIONS
OF
MODERN
HARD
DISKS
IS
THAT
THE
GEOMETRY
SPECIFIED
AND
USED
BY
THE
DRIVER
SOFTWARE
IS
ALMOST
ALWAYS
DIFFERENT
FROM
THE
PHYSICAL
FORMAT
ON
OLD
DISKS
THE
NUMBER
OF
SECTORS
PER
TRACK
WAS
THE
SAME
FOR
ALL
CYLINDERS
MODEM
DISKS
ARE
DIVIDED
INTO
ZONES
WITH
MORE
SECTORS
ON
THE
OUTER
ZONES
THAN
THE
INNER
ONES
FIG
A
ILLUSTRATES
A
TINY
DISK
WITH
TWO
ZONES
THE
OUTER
ZONE
HAS
SECTORS
PER
TRACK
THE
INNER
ONE
HAS
SEC
TORS
PER
TRACK
A
REAL
DISK
SUCH
AS
THE
WD
TYPICALLY
HAS
OR
MORE
ZONES
WITH
THE
NUMBER
OF
SECTORS
INCREASING
BY
ABOUT
PER
ZONE
AS
ONE
GOES
OUT
FROM
THE
INNERMOST
ZONE
TO
THE
OUTERMOST
ZONE
TO
HIDE
THE
DETAILS
OF
HOW
MANY
SECTORS
EACH
TRACK
HAS
MOST
MODEM
DISKS
HAVE
A
VIRTUAL
GEOMETRY
THAT
IS
PRESENTED
TO
THE
OPERATING
SYSTEM
THE
SOFTWARE
IS
INSTRUCTED
TO
ACT
AS
THOUGH
THERE
ARE
X
CYLINDERS
Y
HEADS
AND
Z
SECTORS
PER
TRACK
THE
CONTROLLER
THEN
REMAPS
A
REQUEST
FOR
X
Y
Z
ONTO
THE
REAL
CYLINDER
HEAD
AND
SECTOR
A
POSSIBLE
VIRTUAL
GEOMETRY
FOR
THE
PHYSICAL
DISK
OF
FIG
A
IS
SHOWN
IN
FIG
B
IN
BOTH
CASES
THE
DISK
HAS
SECTORS
ONLY
THE
PUBLISHED
ARRANGE
MENT
IS
DIFFERENT
THAN
THE
REAL
ONE
FOR
PCS
THE
MAXIMUM
VALUES
FOR
THESE
THREE
PARAMETERS
ARE
OFTEN
AND
DUE
TO
THE
NEED
TO
BE
BACKWARD
COMPATIBLE
WITH
THE
LIMITATIONS
OF
THE
ORIGINAL
IBM
PC
ON
THIS
MACHINE
AND
BIT
FIELDS
WERE
USED
TO
SPECIFY
THESE
NUMBERS
WIDI
CYLINDERS
AND
SECTORS
NUMBERED
STARTING
AT
AND
HEADS
NUM
BERED
STARTING
AT
WITH
THESE
PARAMETERS
AND
BYTES
PER
SECTOR
THE
LARGEST
POSSIBLE
DISK
IS
GB
TO
GET
AROUND
THIS
LIMIT
ALL
MODERN
DISKS
NOW
SUPPORT
A
SEC
DISKS
FIGURE
A
PHYSICAL
GEOMETRY
OF
A
DISK
WITH
TWO
ZONES
B
A
POSSIBLE
VIRTUAL
GEOMETRY
FOR
THIS
DISK
SYSTEM
CALLED
LOGICAL
BLOCK
ADDRESSING
IN
WHICH
DISK
SECTORS
ARE
JUST
NUMBERED
CONSECUTIVELY
STARTING
AT
WITHOUT
REGARD
TO
THE
DISK
GEOMETRY
RAID
CPU
PERFORMANCE
HAS
BEEN
INCREASING
EXPONENTIALLY
OVER
THE
PAST
DECADE
ROUGHLY
DOUBLING
EVERY
MONTHS
NOT
SO
WITH
DISK
PERFORMANCE
IN
THE
AVERAGE
SEEK
TIMES
ON
MINICOMPUTER
DISKS
WERE
TO
MSEC
NOW
SEEK
TIMES
ARE
SLIGHTLY
UNDER
MSEC
IN
MOST
TECHNICAL
INDUSTRIES
SAY
AUTOMOBILES
OR
AVIA
TION
A
FACTOR
OF
TO
PERFORMANCE
IMPROVEMENT
IN
TWO
DECADES
WOULD
BE
MAJOR
NEWS
IMAGINE
MPG
CARS
BUT
IN
THE
COMPUTER
INDUSTRY
IT
IS
AN
EMBAR
RASSMENT
THUS
THE
GAP
BETWEEN
CPU
PERFORMANCE
AND
DISK
PERFORMANCE
HAS
BE
COME
MUCH
LARGER
OVER
TIME
AS
WE
HAVE
SEEN
PARALLEL
PROCESSING
IS
BEING
USED
MORE
AND
MORE
TO
SPEED
UP
CPU
PERFORMANCE
IT
HAS
OCCURRED
TO
VARIOUS
PEOPLE
OVER
THE
YEARS
THAT
PARALLEL
I
O
MIGHT
BE
A
GOOD
IDEA
TOO
IN
THEIR
PAPER
PATTERSON
ET
AL
SUGGESTED
SIX
SPECIFIC
DISK
ORGANIZATIONS
THAT
COULD
BE
USED
TO
IMPROVE
DISK
PERFORMANCE
RELIABILITY
OR
BOTH
PATTERSON
ET
AL
THESE
IDEAS
WERE
QUICKLY
ADOPTED
BY
INDUSTRY
AND
HAVE
LED
TO
A
NEW
CLASS
OF
I
O
DEVICE
CALLED
A
RAID
PATTERSON
ET
AL
DEFINED
RAID
AS
REDUNDANT
ARRAY
OF
INEXPENSIVE
DISKS
BUT
INDUSTRY
REDEFINED
THE
I
TO
BE
INDEPENDENT
RATHER
THAN
INEXPENSIVE
MAYBE
SO
THEY
COULD
CHARGE
MORE
SINCE
A
VILLAIN
WAS
ALSO
NEEDED
AS
IN
RISC
VERSUS
CISC
ALSO
DUE
TO
PATTERSON
THE
BAD
GUY
HERE
WAS
THE
SLED
SINGLE
LARGE
EXPENSIVE
DISK
INPUT
OUTPUT
CHAP
SEC
DISKS
THE
BASIC
IDEA
BEHIND
A
RAID
IS
TO
INSTALL
A
BOX
FULL
OF
DISKS
NEXT
TO
THE
COM
PUTER
TYPICALLY
A
LARGE
SERVER
REPLACE
THE
DISK
CONTROLLER
CARD
WITH
A
RAID
CON
TROLLER
COPY
THE
DATA
OVER
TO
THE
RAID
AND
THEN
CONTINUE
NORMAL
OPERATION
IN
OTHER
WORDS
A
RAID
SHOULD
LOOK
LIKE
A
SLED
TO
THE
OPERATING
SYSTEM
BUT
HAVE
BETTER
PERFORMANCE
AND
BETTER
RELIABILITY
SINCE
SCSI
DISKS
HAVE
GOOD
PER
FORMANCE
LOW
PRICE
AND
THE
ABILITY
TO
HAVE
UP
TO
SEVEN
DRIVES
ON
A
SINGLE
CON
TROLLER
FOR
WIDE
SCSI
IT
IS
NATURAL
THAT
MOST
RAIDS
CONSIST
OF
A
RAID
SCSI
CONTROLLER
PLUS
A
BOX
OF
SCSI
DISKS
THAT
APPEAR
TO
THE
OPERATING
SYSTEM
AS
A
SINGLE
LARGE
DISK
IN
THIS
WAY
NO
SOFTWARE
CHANGES
ARE
REQUIRED
TO
USE
THE
RAID
A
BIG
SELLING
POINT
FOR
MANY
SYSTEM
ADMINISTRATORS
IN
ADDITION
TO
APPEARING
LIKE
A
SINGLE
DISK
TO
THE
SOFTWARE
ALL
RAIDS
HAVE
THE
PROPERTY
THAT
THE
DATA
ARE
DISTRIBUTED
OVER
THE
DRIVES
TO
ALLOW
PARALLEL
OPERATION
SEVERAL
DIFFERENT
SCHEMES
FOR
DOING
THIS
WERE
DEFINED
BY
PATTERSON
ET
AL
AND
THEY
ARE
NOW
KNOWN
AS
RAID
LEVEL
THROUGH
RAID
LEVEL
IN
ADDITION
THERE
ARE
A
FEW
OTHER
MINOR
LEVELS
THAT
WE
WILL
NOT
DISCUSS
THE
TERM
LEVEL
IS
SOMETHING
OF
A
MISNOMER
SINCE
THERE
IS
NO
HIERARCHY
INVOLVED
THERE
ARE
SIMPLY
SIX
DIFFERENT
ORGANIZATIONS
POSSIBLE
RAID
LEVEL
IS
ILLUSTRATED
IN
FIG
A
IT
CONSISTS
OF
VIEWING
THE
VIRTUAL
SINGLE
DISK
SIMULATED
BY
THE
RAID
AS
BEING
DIVIDED
UP
INTO
STRIPS
OF
K
SECTORS
EACH
WITH
SECTORS
TO
K
BEING
STRIP
SECTORS
K
TO
AS
STRIP
AND
SO
ON
FOR
K
EACH
STRIP
IS
A
SECTOR
FOR
K
A
STRIP
IS
TWO
SECTORS
ETC
THE
RAID
LEVEL
ORGANIZATION
WRITES
CONSECUTIVE
STRIPS
OVER
THE
DRIVES
IN
ROUND
ROBIN
FASHION
AS
DEPICTED
IN
FIG
A
FOR
A
RAID
WITH
FOUR
DISK
DRIVES
DISTRIBUTING
DATA
OVER
MULTIPLE
DRIVES
LIKE
THIS
IS
CALLED
STRIPING
FOR
EX
AMPLE
IF
THE
SOFTWARE
ISSUES
A
COMMAND
TO
READ
A
DATA
BLOCK
CONSISTING
OF
FOUR
CONSECUTIVE
STRIPS
STARTING
AT
A
STRIP
BOUNDARY
THE
RAID
CONTROLLER
WILL
BREAK
THIS
COMMAND
UP
INTO
FOUR
SEPARATE
COMMANDS
ONE
FOR
EACH
OF
THE
FOUR
DISKS
AND
HAVE
THEM
OPERATE
IN
PARALLEL
THUS
WE
HAVE
PARALLEL
I
O
WITHOUT
THE
SOFTWARE
KNOWING
ABOUT
IT
RAID
LEVEL
WORKS
BEST
WITH
LARGE
REQUESTS
THE
BIGGER
THE
BETTER
IF
A
RE
QUEST
IS
LARGER
THAN
THE
NUMBER
OF
DRIVES
TIMES
THE
STRIP
SIZE
SOME
DRIVES
WILL
GET
MULTIPLE
REQUESTS
SO
THAT
WHEN
THEY
FINISH
THE
FIRST
REQUEST
THEY
START
THE
SECOND
ONE
IT
IS
UP
TO
THE
CONTROLLER
TO
SPLIT
THE
REQUEST
UP
AND
FEED
THE
PROPER
COMMANDS
TO
THE
PROPER
DISKS
IN
THE
RIGHT
SEQUENCE
AND
THEN
ASSEMBLE
THE
RESULTS
IN
MEMORY
CORRECTLY
PERFORMANCE
IS
EXCELLENT
AND
THE
IMPLEMENTATION
IS
STRAIGHTFORWARD
RAID
LEVEL
WORKS
WORST
WITH
OPERATING
SYSTEMS
THAT
HABITUALLY
ASK
FOR
DATA
ONE
SECTOR
AT
A
TIME
THE
RESULTS
WILL
BE
CORRECT
BUT
THERE
IS
NO
PARALLELISM
AND
HENCE
NO
PERFORMANCE
GAIN
ANOTHER
DISADVANTAGE
OF
THIS
ORGANIZATION
IS
THAT
THE
RELIABILITY
IS
POTENTIALLY
WORSE
THAN
HAVING
A
SLED
IF
A
RAID
CONSISTS
OF
FOUR
DISKS
EACH
WITH
A
MEAN
TIME
TO
FAILURE
OF
HOURS
ABOUT
ONCE
EVERY
HOURS
A
DRIVE
WILL
FAIL
AND
ALL
THE
DATA
WILL
BE
COMPLETELY
LOST
A
SLED
WITH
A
MEAN
TIME
TO
FAILURE
OF
HOURS
WOULD
BE
FOUR
TIMES
MORE
RELIABLE
BECAUSE
NO
REDUNDANCY
IS
PRESENT
IN
THIS
DESIGN
IT
IS
NOT
REALLY
A
TRUE
RAID
THE
NEXT
OPTION
RAID
LEVEL
SHOWN
IN
FIG
B
IS
A
TRUE
RAID
IT
DUPLICATES
ALL
THE
DISKS
SO
THERE
ARE
FOUR
PRIMARY
DISKS
AND
FOUR
BACKUP
DISKS
ON
A
WRITE
EVERY
STRIP
IS
WRITTEN
TWICE
ON
A
READ
EITHER
COPY
CAN
BE
USED
DISTRIBUT
ING
THE
LOAD
OVER
MORE
DRIVES
CONSEQUENTLY
WRITE
PERFORMANCE
IS
NO
BETTER
THAN
FOR
A
SINGLE
DRIVE
BUT
READ
PERFORMANCE
CAN
BE
UP
TO
TWICE
AS
GOOD
FAULT
TOLER
ANCE
IS
EXCELLENT
IF
A
DRIVE
CRASHES
THE
COPY
IS
SIMPLY
USED
INSTEAD
RECOVERY
CONSISTS
OF
SIMPLY
INSTALLING
A
NEW
DRIVE
AND
COPYING
THE
ENTIRE
BACKUP
DRIVE
TO
IT
UNLIKE
LEVELS
AND
WHICH
WORK
WITH
STRIPS
OF
SECTORS
RAID
LEVEL
WORKS
ON
A
WORD
BASIS
POSSIBLY
EVEN
A
BYTE
BASIS
IMAGINE
SPLITTING
EACH
BYTE
OF
THE
SIN
GLE
VIRTUAL
DISK
INTO
A
PAIR
OF
BIT
NIBBLES
THEN
ADDING
A
HAMMING
CODE
TO
EACH
ONE
TO
FORM
A
BIT
WORD
OF
WHICH
BITS
AND
WERE
PARITY
BITS
FURTHER
IMA
GINE
THAT
THE
SEVEN
DRIVES
OF
FIG
C
WERE
SYNCHRONIZED
IN
TERMS
OF
ARM
POSI
TION
AND
ROTATIONAL
POSITION
THEN
IT
WOULD
BE
POSSIBLE
TO
WRITE
THE
BIT
HAMMING
CODED
WORD
OVER
THE
SEVEN
DRIVES
ONE
BIT
PER
DRIVE
THE
THINKING
MACHINES
CM
COMPUTER
USED
THIS
SCHEME
TAKING
BIT
DATA
WORDS
AND
ADDING
PARITY
BITS
TO
FORM
A
BIT
HAINMING
WORD
PLUS
AN
EXTRA
BIT
FOR
WORD
PARITY
AND
SPREAD
EACH
WORD
OVER
DISK
DRIVES
THE
TOTAL
THROUGHPUT
WAS
IMMENSE
BECAUSE
IN
ONE
SECTOR
TIME
IT
COULD
WRITE
SECTORS
WORTH
OF
DATA
ALSO
LOSING
ONE
DRIVE
DID
NOT
CAUSE
PROBLEMS
BECAUSE
LOSS
OF
A
DRIVE
AMOUNTED
TO
LOSING
BIT
IN
EACH
BIT
WORD
READ
SOMETHING
THE
HAMMING
CODE
COULD
HAND
LE
ON
THE
FLY
ON
THE
DOWN
SIDE
THIS
SCHEME
REQUIRES
ALL
THE
DRIVES
TO
BE
ROTATIONALLY
SYN
CHRONIZED
AND
IT
ONLY
MAKES
SENSE
WITH
A
SUBSTANTIAL
NUMBER
OF
DRIVES
EVEN
WITH
DATA
DRIVES
AND
PARITY
DRIVES
THE
OVERHEAD
IS
IT
ALSO
ASKS
A
LOT
OF
THE
CONTROLLER
SINCE
IT
MUST
DO
A
HAMMING
CHECKSUM
EVERY
BIT
TIME
RAID
LEVEL
IS
A
SIMPLIFIED
VERSION
OF
RAID
LEVEL
IT
IS
ILLUSTRATED
IN
FIG
D
HERE
A
SINGLE
PARITY
BIT
IS
COMPUTED
FOR
EACH
DATA
WORD
AND
WRITTEN
TO
A
PARITY
DRIVE
AS
IN
RAID
LEVEL
THE
DRIVES
MUST
BE
EXACTLY
SYNCHRONIZED
SINCE
INDIVIDUAL
DATA
WORDS
ARE
SPREAD
OVER
MULTIPLE
DRIVES
AT
FIRST
THOUGHT
IT
MIGHT
APPEAR
THAT
A
SINGLE
PARITY
BIT
GIVES
ONLY
ERROR
DETEC
TION
NOT
ERROR
CORRECTION
FOR
THE
CASE
OF
RANDOM
UNDETECTED
ERRORS
THIS
OBSERVA
TION
IS
TRUE
HOWEVER
FOR
THE
CASE
OF
A
DRIVE
CRASHING
IT
PROVIDES
FULL
BIT
ERROR
CORRECTION
SINCE
THE
POSITION
OF
THE
BAD
BIT
IS
KNOWN
IF
A
DRIVE
CRASHES
THE
CON
TROLLER
JUST
PRETENDS
THAT
ALL
ITS
BITS
ARE
IF
A
WORD
HAS
A
PARITY
ERROR
THE
BIT
FROM
THE
DEAD
DRIVE
MUST
HAVE
BEEN
A
SO
IT
IS
CORRECTED
ALTHOUGH
BOTH
RAID
LEVELS
AND
OFFER
VERY
HIGH
DATA
RATES
THE
NUMBER
OF
SEPARATE
I
O
REQUESTS
PER
SECOND
THEY
CAN
HANDLE
IS
NO
BETTER
THAN
FOR
A
SINGLE
DRIVE
RAID
LEVELS
AND
WORK
WITH
STRIPS
AGAIN
NOT
INDIVIDUAL
WORDS
WITH
PARITY
AND
DO
NOT
REQUIRE
SYNCHRONIZED
DRIVES
RAID
LEVEL
SEE
FIG
E
IS
LIKE
RAID
LEVEL
WITH
A
STRIP
FOR
STRIP
PARITY
WRITTEN
ONTO
AN
EXTRA
DRIVE
FOR
EX
AMPLE
IF
EACH
STRIP
IS
K
BYTES
LONG
ALL
THE
STRIPS
ARE
EXCLUSIVE
ORED
TOGETHER
RESULTING
IN
A
PARITY
STRIP
K
BYTES
LONG
IF
A
DRIVE
CRASHES
THE
LOST
BYTES
CAN
BE
RECOMPUTED
FROM
THE
PARITY
DRIVE
BY
READING
THE
ENTIRE
SET
OF
DRIVES
INPUT
OUTPUT
STRIP
CHAP
SEC
DISKS
THIS
DESIGN
PROTECTS
AGAINST
THE
LOSS
OF
A
DRIVE
BUT
PERFORMS
POORIY
FOR
SMALL
UPDATES
IF
ONE
SECTOR
IS
CHANGED
IT
IS
NECESSARY
TO
READ
ALL
THE
DRIVES
IN
ORDER
TO
RECALCULATE
THE
PARITY
WHICH
MUST
THEN
BE
REWRITTEN
ALTERNATIVELY
IT
CAN
READ
THE
OLD
USER
DATA
AND
THE
OLD
PARITY
DATA
AND
RECOMPUTE
THE
NEW
PARITY
FROM
THEM
A
STRIP
STRIP
STRIP
STRIP
I
STRIP
I
STRIP
RAID
LEVEL
I
STRIP
STRIP
RAID
EVEN
WITH
THIS
OPTIMIZATION
A
SMALL
UPDATE
REQUIRES
TWO
READS
AND
TWO
WRITES
AS
A
CONSEQUENCE
OF
THE
HEAVY
LOAD
ON
THE
PARITY
DRIVE
IT
MAY
BECOME
A
BOTTLENECK
THIS
BOTTLENECK
IS
ELIMINATED
IN
RAID
LEVEL
BY
DISTRIBUTING
THE
PAR
ITY
BITS
UNIFORMLY
OVER
ALL
THE
DRIVES
ROUND
ROBIN
FASHION
AS
SHOWN
IN
FIG
F
HOWEVER
IN
THE
EVENT
OF
A
DRIVE
CRASH
RECONSTRUCTING
THE
CONTENTS
OF
THE
FAILED
DRIVE
IS
A
COMPLEX
PROCESS
B
STRIP
I
STRIP
STRIP
STRIP
STRIP
ISTRIP
STRIP
STRIP
STRIP
STRIP
STRIP
PI
STRIP
STRIP
STRIP
STRIP
STRIP
STRIP
STRIP
STRIP
STRIP
STRIP
STRIP
LEVEL
CD
ROMS
IN
RECENT
YEARS
OPTICAL
AS
OPPOSED
TO
MAGNETIC
DISKS
HAVE
BECOME
AVAILABLE
THEY
HAVE
MUCH
HIGHER
RECORDING
DENSITIES
THAN
CONVENTIONAL
MAGNETIC
DISKS
OPTICAL
DISKS
WERE
ORIGINALLY
DEVELOPED
FOR
RECORDING
TELEVISION
PROGRAMS
BUT
THEY
CAN
BE
PUT
TO
MORE
ESTHETIC
USE
AS
COMPUTER
STORAGE
DEVICES
DUE
TO
THEIR
POTENTIALLY
ENORMOUS
CAPACITY
OPTICAL
DISKS
HAVE
BEEN
THE
SUBJECT
OF
A
GREAT
DEAL
OF
RESEARCH
AND
HAVE
GONE
THROUGH
AN
INCREDIBLY
RAPID
EVOLUTION
FIRST
GENERATION
OPTICAL
DISKS
WERE
INVENTED
BY
THE
DUTCH
ELECTRONICS
CONGLOMERATE
PHILIPS
FOR
HOLDING
MOVIES
THEY
WERE
CM
ACROSS
AND
MARKETED
UNDER
THE
NAME
LASER
VISION
BUT
THEY
DID
NOT
CATCH
ON
EXCEPT
IN
JAPAN
IN
PHILIPS
TOGETHER
WITH
SONY
DEVELOPED
THE
CD
COMPACT
DISC
WHICH
RAPIDLY
REPLACED
THE
RPM
VINYL
RECORD
FOR
MUSIC
EXCEPT
AMONG
CONNOISSEURS
WHO
STILL
PREFER
VINYL
THE
PRECISE
TECHNICAL
DETAILS
FOR
THE
CD
WERE
PUBLISHED
IN
AN
OFFICIAL
INTERNATIONAL
STANDARD
IS
POPULARLY
CALLED
THE
RED
BOOK
DUE
TO
THE
COLOR
OF
ITS
COVER
INTERNATIONAL
STANDARDS
ARE
ISSUED
BY
THE
INTERNATIONAL
ORGANIZATION
FOR
STANDARDIZATION
WHICH
IS
THE
INTERNATIONAL
COUNTERPART
OF
NATIONAL
STANDARDS
GROUPS
LIKE
ANSI
DIN
ETC
EACH
ONE
HAS
AN
IS
NUMBER
THE
POINT
OF
PUBLISHING
THE
DISK
AND
DRIVE
SPECIFICATIONS
AS
AN
INTERNA
TIONAL
STANDARD
IS
TO
ALLOW
CDS
FROM
DIFFERENT
MUSIC
PUBLISHERS
AND
PLAYERS
FROM
DIFFERENT
ELECTRONICS
MANUFACTURERS
TO
WORK
TOGETHER
ALL
CDS
ARE
MM
ACROSS
AND
MM
THICK
WITH
A
MM
HOLE
IN
THE
MIDDLE
THE
AUDIO
CD
WAS
THE
FIRST
SUCCESSFUL
MASS
MARKET
DIGITAL
STORAGE
MEDIUM
THEY
ARE
SUPPOSED
TO
LAST
YEARS
PLEASE
CHECK
BACK
IN
FOR
AN
UPDATE
ON
HOW
WELL
THE
FIRST
BATCH
DID
A
CD
IS
PREPARED
IN
SEVERAL
STEPS
THE
STEP
CONSISTS
OF
USING
A
HIGH
POWER
INFRARED
LASER
TO
BURN
MICRON
DIAMETER
HOLES
IN
A
COATED
GLASS
MASTER
DISK
FROM
THIS
MASTER
A
MOLD
IS
MADE
WITH
BUMPS
WHERE
THE
LASER
HOLES
WERE
INTO
THIS
MOLD
MOLTEN
POLYCARBONATE
RESIN
IS
INJECTED
TO
FORM
A
CD
WITH
THE
SAME
PAT
TERN
OF
HOLES
AS
THE
GLASS
MASTER
THEN
A
VERY
THIN
LAYER
OF
REFLECTIVE
ALUMINUM
IS
DEPOSITED
ON
THE
POLYCARBONATE
TOPPED
BY
A
PROTECTIVE
LACQUER
AND
FINALLY
A
LABEL
THE
DEPRESSIONS
IN
THE
POLYCARBONATE
SUBSTRATE
ARE
CALLED
PITS
THE
UNBURNED
AREAS
BETWEEN
THE
PITS
ARE
CALLED
LANDS
FIGURE
RAID
LEVELS
THROUGH
BACKUP
AND
PARITY
DRIVES
ARE
SHOWN
SHADED
INPUT
OUTPUT
CHAP
WHEN
PLAYED
BACK
A
LOW
POWER
LASER
DIODE
SHINES
INFRARED
LIGHT
WITH
A
WAVE
LENGTH
OF
MICRON
ON
THE
PITS
AND
LANDS
AS
THEY
STREAM
BY
THE
LASER
IS
ON
THE
POLYCARBONATE
SIDE
SO
THE
PITS
STICK
OUT
TOWARD
THE
LASER
AS
BUMPS
IN
THE
OTHERWISE
FLAT
SURFACE
BECAUSE
THE
PITS
HAVE
A
HEIGHT
OF
ONE
QUARTER
THE
WAVELENGTH
OF
THE
LASER
LIGHT
LIGHT
REFLECTING
OFF
A
PIT
IS
HALF
A
WAVELENGTH
OUT
OF
PHASE
WITH
LIGHT
REFLECTING
OFF
THE
SURROUNDING
SURFACE
AS
A
RESULT
THE
TWO
PARTS
INTERFERE
DESTRUCT
IVELY
AND
RETURN
LESS
LIGHT
TO
THE
PLAYER
PHOTODETECTOR
THAN
LIGHT
BOUNCING
OFF
A
LAND
THIS
IS
HOW
THE
PLAYER
TELLS
A
PIT
FROM
A
LAND
ALTHOUGH
IT
MIGHT
SEEM
SIM
PLER
TO
USE
A
PIT
TO
RECORD
A
AND
A
LAND
TO
RECORD
A
IT
IS
MORE
RELIABLE
TO
USE
A
PIT
LAND
OR
LAND
PIT
TRANSITION
FOR
A
AND
ITS
ABSENCE
AS
A
SO
THIS
SCHEME
IS
USED
THE
PITS
AND
LANDS
ARE
WRITTEN
IN
A
SINGLE
CONTINUOUS
SPIRAL
STARTING
NEAR
THE
HOLE
AND
WORKING
OUT
A
DISTANCE
OF
MM
TOWARD
THE
EDGE
THE
SPIRAL
MAKES
REVOLUTIONS
AROUND
THE
DISK
ABOUT
PER
MM
IF
UNWOUND
IT
WOULD
BE
KM
LONG
THE
SPIRAL
IS
ILLUSTRATED
IN
FIG
FIGURE
RECORDING
STRUCTURE
OF
A
COMPACT
DISC
OR
CD
ROM
SEC
DISKS
WHAT
ARE
NOW
CALLED
CD
ROMS
COMPACT
DISC
READ
ONLY
MEMORY
TO
PIG
GYBACK
ON
THE
BY
THEN
ALREADY
SUBSTANTIAL
AUDIO
CD
MARKET
CD
ROMS
WERE
TO
BE
THE
SAME
PHYSICAL
SIZE
AS
AUDIO
CDS
MECHANICALLY
AND
OPTICALLY
COMPATIBLE
WITH
THEM
AND
PRODUCED
USING
THE
SAME
POLYCARBONATE
INJECTION
MOLDING
MA
CHINES
THE
CONSEQUENCES
OF
THIS
DECISION
WERE
NOT
ONLY
THAT
SLOW
VARIABLE
SPEED
MOTORS
WERE
REQUIRED
BUT
ALSO
THAT
THE
MANUFACTURING
COST
OF
A
CD
ROM
WOULD
BE
WELL
UNDER
ONE
DOLLAR
IN
MODERATE
VOLUME
WHAT
THE
YELLOW
BOOK
DEFINED
WAS
THE
FORMATTING
OF
THE
COMPUTER
DATA
IT
ALSO
IMPROVED
THE
ERROR
CORRECTING
ABILITIES
OF
THE
SYSTEM
AN
ESSENTIAL
STEP
BE
CAUSE
ALTHOUGH
MUSIC
LOVERS
DO
NOT
MIND
LOSING
A
BIT
HERE
AND
THERE
COMPUTER
LOVERS
TEND
TO
BE
VERY
PICKY
ABOUT
THAT
THE
BASIC
FORMAT
OF
A
CD
ROM
CONSISTS
OF
ENCODING
EVERY
BYTE
IN
A
BIT
SYMBOL
WHICH
IS
ENOUGH
TO
HAMMING
ENCODE
AN
BIT
BYTE
WITH
BITS
LEFT
OVER
IN
FACT
A
MORE
POWERFUL
ENCODING
SYSTEM
IS
USED
THE
TO
MAPPING
FOR
READING
IS
DONE
IN
HARDWARE
BY
TABLE
LOOKUP
AT
THE
NEXT
LEVEL
UP
A
GROUP
OF
CONSECUTIVE
SYMBOLS
FORMS
A
BIT
FRAME
EACH
FRAME
HOLDS
DATA
BITS
BYTES
THE
REMAINING
BITS
ARE
USED
FOR
ERROR
CORRECTION
AND
CONTROL
OF
THESE
ARE
THE
ERROR
CORRECTION
BITS
IN
THE
BIT
SYMBOLS
AND
ARE
CARRIED
IN
THE
BIT
SYMBOL
PAYLOADS
SO
FAR
THIS
SCHEME
IS
IDENTICAL
FOR
AUDIO
CDS
AND
CD
ROMS
WHAT
THE
YELLOW
BOOK
ADDS
IS
THE
GROUPING
OF
FRAMES
INTO
A
CD
ROM
SECTOR
AS
SHOWN
IN
FIG
EVERY
CD
ROM
SECTOR
BEGINS
WITH
A
BYTE
PREAMBLE
THE
FIRST
OF
WHICH
ARE
OOFFFFFFFFFFFFFFFFFFFFOO
HEXADE
CIMAL
TO
ALLOW
THE
PLAYER
TO
RECOGNIZE
THE
START
OF
A
CD
ROM
SECTOR
THE
NEXT
BYTES
CONTAIN
THE
SECTOR
NUMBER
NEEDED
BECAUSE
SEEKING
ON
A
CD
ROM
WITH
ITS
SINGLE
DATA
SPIRAL
IS
MUCH
MORE
DIFFICULT
THAN
ON
A
MAGNETIC
DISK
WITH
ITS
UNIFORM
CONCENTRIC
TRACKS
TO
SEEK
THE
SOFTWARE
IN
THE
DRIVE
CALCULATES
APPROXIMATELY
WHERE
TO
GO
MOVES
THE
HEAD
THERE
AND
THEN
STARTS
HUNTING
AROUND
FOR
A
PREAMBLE
TO
SEE
HOW
GOOD
ITS
GUESS
WAS
THE
LAST
BYTE
OF
THE
PREAMBLE
IS
THE
MODE
EACH
SYMBOL
HOLDS
DATA
BITS
AND
ERROR
CORRECTION
BITS
SYMBOLS
MAKE
FRAME
OF
X
BITS
TO
MAKE
THE
MUSIC
PLAY
AT
A
UNIFORM
RATE
IT
IS
NECESSARY
FOR
THE
PITS
AND
LANDS
TO
STREAM
BY
AT
A
CONSTANT
LINEAR
VELOCITY
CONSEQUENTLY
THE
ROTATION
RATE
OF
THE
C
N
A
R
Z
J
M
A
E
Z
J
C
Z
I
A
EACH
FRAME
CONTAINS
O
A
T
Z
J
O
A
O
DATA
BITS
BYTES
AND
CD
MUST
BE
CONTINUOUSLY
REDUCED
AS
THE
READING
HEAD
MOVES
FROM
THE
INSIDE
OF
THE
CD
TO
THE
OUTSIDE
AT
THE
INSIDE
THE
ROTATION
RATE
IS
RPM
TO
ACHIEVE
THE
PREAMBLE
FRAMES
MAKE
SECTOR
ERROR
CORRECTION
BITS
MODE
DESIRED
STREAMING
RATE
OF
CM
SEC
AT
THE
OUTSIDE
IT
HAS
TO
DROP
TO
RPM
TO
GIVE
THE
SAME
LINEAR
VELOCITY
AT
THE
HEAD
A
CONSTANT
LINEAR
VELOCITY
DRIVE
IS
QUITE
DIFFERENT
THAN
A
MAGNETIC
DISK
DRIVE
WHICH
OPERATES
AT
A
CONSTANT
ANGULAR
VELOCITY
BYTES
DATA
ECC
SECTOR
BYTES
INDEPENDENT
OF
WHERE
THE
HEAD
IS
CURRENTLY
POSITIONED
ALSO
RPM
IS
A
FAR
CRY
FROM
THE
TO
RPM
THAT
MOST
MAGNETIC
DISKS
WHIRL
AT
IN
PHILIPS
AND
SONY
REALIZED
THE
POTENTIAL
FOR
USING
CDS
TO
STORE
COM
PUTER
DATA
SO
THEY
PUBLISHED
THE
YELLOW
BOOK
DEFINING
A
PRECISE
STANDARD
FOR
FIGURE
LOGICAL
DATA
LAYOUT
ON
A
CD
ROM
THE
YELLOW
BOOK
DEFINES
TWO
MODES
MODE
USES
THE
LAYOUT
OF
FIG
WITH
A
BYTE
PREAMBLE
DATA
BYTES
AND
A
BYTE
ERROR
CORRECTING
CODE
A
INPUT
OUTPUT
CHAP
CROSSINTERLEAVED
REED
SOLOMON
CODE
MODE
COMBINES
THE
DATA
AND
ECC
FIELDS
INTO
A
BYTE
DATA
FIELD
FOR
THOSE
APPLICATIONS
THAT
DO
NOT
NEED
OR
CANNOT
AFFORD
THE
TIME
TO
PERFORM
ERROR
CORRECTION
SUCH
AS
AUDIO
AND
VIDEO
NOTE
THAT
TO
PROVIDE
EXCELLENT
RELIABILITY
THREE
SEPARATE
ERROR
CORRECTING
SCHEMES
ARE
USED
WITHIN
A
SYMBOL
WITHIN
A
FRAME
AND
WITHIN
A
CD
ROM
SECTOR
SINGLE
BIT
ERRORS
ARE
CORRECTED
AT
THE
LOWEST
LEVEL
SHORT
BURST
ERRORS
ARE
CORRECTED
AT
THE
FRAME
LEVEL
AND
ANY
RESIDUAL
ERRORS
ARE
CAUGHT
AT
THE
SECTOR
LEVEL
THE
PRICE
PAID
FOR
THIS
RELIABILITY
IS
THAT
IT
TAKES
FRAMES
OF
BITS
BYTES
TO
CARRY
A
SINGLE
BYTE
PAYLOAD
AN
EFFICIENCY
OF
ONLY
SINGLE
SPEED
CD
ROM
DRIVES
OPERATE
AT
SECTORS
SEC
WHICH
GIVES
A
DATA
RATE
OF
BYTES
SEC
IN
MODE
AND
BYTES
SEC
IN
MODE
DOUBLE
SPEED
DRIVES
ARE
TWICE
AS
FAST
AND
SO
ON
UP
TO
THE
HIGHEST
SPEED
THUS
A
DRIVE
CAN
DELIVER
DATA
AT
A
RATE
OF
X
BYTES
SEC
ASSUMING
THAT
THE
DRIVE
INTER
FACE
BUS
AND
OPERATING
SYSTEM
CAN
ALL
HANDLE
THIS
DATA
RATE
A
STANDARD
AUDIO
CD
HAS
ROOM
FOR
MINUTES
OF
MUSIC
WHICH
IF
USED
FOR
MODE
DATA
GIVES
A
CAPA
CITY
OF
BYTES
THIS
FIGURE
IS
USUALLY
REPORTED
AS
MB
BECAUSE
MB
IS
BYTES
BYTES
NOT
BYTES
NOTE
THAT
EVEN
A
CD
ROM
DRIVE
BYTES
SEC
IS
NO
MATCH
FOR
A
FAST
SCSI
MAGNETIC
DISK
DRIVE
AT
MB
SEC
EVEN
THOUGH
MANY
CD
ROM
DRIVES
USE
THE
SCSI
INTERFACE
IDE
CD
ROM
DRIVES
ALSO
EXIST
WHEN
YOU
REALIZE
THAT
THE
SEEK
TIME
IS
USUALLY
SEVERAL
HUNDRED
MILLISECONDS
IT
SHOULD
BE
CLEAR
THAT
CD
ROM
DRIVES
ARE
NOT
IN
THE
SAME
PERFORMANCE
CATEGORY
AS
MAGNETIC
DISK
DRIVES
DESPITE
THEIR
LARGE
CAPACITY
IN
PHILIPS
STRUCK
AGAIN
WITH
THE
GREEN
BOOK
ADDING
GRAPHICS
AND
THE
ABILITY
TO
INTERLEAVE
AUDIO
VIDEO
AND
DATA
IN
THE
SAME
SECTOR
A
FEATURE
ESSENTIAL
FOR
MULTIMEDIA
CD
ROMS
THE
LAST
PIECE
OF
THE
CD
ROM
PUZZLE
IS
THE
FDE
SYSTEM
TO
MAKE
IT
POSSIBLE
TO
USE
THE
SAME
CD
ROM
ON
DIFFERENT
COMPUTERS
AGREEMENT
WAS
NEEDED
ON
CD
ROM
FDE
SYSTEMS
TO
GET
THIS
AGREEMENT
REPRESENTATIVES
OF
MANY
COMPUTER
COMPANIES
MET
AT
LAKE
TAHOE
IN
THE
HIGH
SIERRAS
ON
THE
CALIFORNIA
NEVADA
BOUN
DARY
AND
DEVISED
A
FILE
SYSTEM
THAT
THEY
CALLED
HIGH
SIERRA
IT
LATER
EVOLVED
INTO
AN
INTERNATIONAL
STANDARD
IS
IT
HAS
THREE
LEVELS
LEVEL
USES
FDE
NAMES
OF
UP
TO
CHARACTERS
OPTIONALLY
FOLLOWED
BY
AN
EXTENSION
OF
UP
TO
CHARACTERS
THE
MS
DOS
FDE
NAMING
CONVENTION
FILE
NAMES
MAY
CONTAIN
ONLY
UPPER
CASE
LETTERS
DIGITS
AND
THE
UNDERSCORE
DIRECTORIES
MAY
BE
NESTED
UP
TO
EIGHT
DEEP
BUT
DIRECTORY
NAMES
MAY
NOT
CONTAIN
EXTENSIONS
LEVEL
REQUIRES
ALL
FILES
TO
BE
CON
TIGUOUS
WHICH
IS
NOT
A
PROBLEM
ON
A
MEDIUM
WRITTEN
ONLY
ONCE
ANY
CD
ROM
CONFORMANT
TO
IS
LEVEL
CAN
BE
READ
USING
MS
DOS
AN
APPLE
COMPUTER
A
UNIX
COMPUTER
OR
JUST
ABOUT
ANY
OTHER
COMPUTER
CD
ROM
PUBLISHERS
REGARD
THIS
PROPERTY
AS
BEING
A
BIG
PLUS
IS
LEVEL
ALLOWS
NAMES
UP
TO
CHARACTERS
AND
LEVEL
ALLOWS
NONCON
TIGUOUS
FILES
THE
ROCK
RIDGE
EXTENSIONS
WHIMSICALLY
NAMED
AFTER
THE
TOWN
IN
THE
GENE
WILDER
FILM
BLAZING
SADDLES
ALLOW
VERY
LONG
NAMES
FOR
UNIX
UIDS
SEC
DISKS
GIDS
AND
SYMBOLIC
IINKS
BUT
CD
ROMS
NOT
CONFORMING
TO
LEVEL
WILL
NOT
BE
READABLE
ON
ALL
COMPUTERS
CD
ROMS
HAVE
BECOME
EXTREMELY
POPULAR
FOR
PUBLISHING
GAMES
MOVIES
ENCYLOPEDIAS
ATLASES
AND
REFERENCE
WORKS
OF
ALL
KINDS
MOST
COMMERCIAL
SOFTWARE
NOW
COMES
ON
CD
ROMS
THEIR
COMBINATION
OF
LARGE
CAPACITY
AND
LOW
MANUFAC
TURING
COST
MAKES
THEM
WELL
SUITED
TO
INNUMERABLE
APPLICATIONS
CD
RECORDABLES
INITIALLY
THE
EQUIPMENT
NEEDED
TO
PRODUCE
A
MASTER
CD
ROM
OR
AUDIO
CD
FOR
THAT
MATTER
WAS
EXTREMELY
EXPENSIVE
BUT
AS
USUAL
IN
THE
COMPUTER
INDUSTRY
NOTHING
STAYS
EXPENSIVE
FOR
LONG
BY
THE
MID
CD
RECORDERS
NO
BIGGER
THAN
A
CD
PLAYER
WERE
A
COMMON
PERIPHERAL
AVAILABLE
IN
MOST
COMPUTER
STORES
THESE
DEVICES
WERE
STILL
DIFFERENT
FROM
MAGNETIC
DISKS
BECAUSE
ONCE
WRITTEN
CD
ROMS
COULD
NOT
BE
ERASED
NEVERTHELESS
THEY
QUICKLY
FOUND
A
NICHE
AS
A
BACKUP
MEDIUM
FOR
LARGE
HARD
DISKS
AND
ALSO
ALLOWED
INDIVIDUALS
OR
STARTUP
COMPANIES
TO
MANUFAC
TURE
THEIR
OWN
SMALL
RUN
CD
ROMS
OR
MAKE
MASTERS
FOR
DELIVERY
TO
HIGH
VOLUME
COMMERCIAL
CD
DUPLICATION
PLANTS
THESE
DRIVES
ARE
KNOWN
AS
CD
RS
CD
RECORDABLES
PHYSICALLY
CD
RS
START
WITH
MM
POLYCARBONATE
BLANKS
THAT
ARE
LIKE
CD
ROMS
EXCEPT
THAT
THEY
CONTAIN
A
MM
WIDE
GROOVE
TO
GUIDE
THE
LASEF
FOR
WRIT
ING
THE
GROOVE
HAS
A
SINUSOIDAL
EXCURSION
OF
MM
AT
A
FREQUENCY
OF
EXACTLY
KHZ
TO
PROVIDE
CONTINUOUS
FEEDBACK
SO
THE
ROTATION
SPEED
CAN
BE
ACCURATELY
MONITORED
AND
ADJUSTED
IF
NEED
BE
CD
RS
LOOK
LIKE
REGULAR
CD
ROMS
EXCEPT
THAT
THEY
ARE
GOLD
COLORED
ON
TOP
INSTEAD
OF
SILVER
COLORED
THE
GOLD
COLOR
COMES
FROM
THE
USE
OF
REAL
GOLD
INSTEAD
OF
ALUMINUM
FOR
THE
REFLECTIVE
LAYER
UNLIKE
SILVER
CDS
WHICH
HAVE
PHYSICAL
DEPRESSIONS
ON
THEM
ON
CD
RS
THE
DIFFERING
REFLECTIVITY
OF
PITS
AND
LANDS
HAS
TO
BE
SIMULATED
THIS
IS
DONE
BY
ADDING
A
LAYER
OF
DYE
BETWEEN
THE
POLYCARBONATE
AND
THE
REFLECTIVE
GOLD
LAYER
AS
SHOWN
IN
FIG
TWO
KINDS
OF
DYE
ARE
USED
CYANINE
WHICH
IS
GREEN
AND
PTHALOCYANINE
WHICH
IS
A
YELLOWISH
ORANGE
CHEMISTS
CAN
ARGUE
ENDLESSLY
ABOUT
WHICH
ONE
IS
BETTER
THESE
DYES
ARE
SIMILAR
TO
THOSE
USED
IN
PHOTOGRAPHY
WHICH
EXPLAINS
WHY
EASTMAN
KODAK
AND
FUJI
ARE
MAJOR
MANUFACTURERS
OF
BLANK
CD
RS
IN
ITS
INITIAL
STATE
THE
DYE
LAYER
IS
TRANSPARENT
AND
LETS
THE
LASER
LIGHT
PASS
THROUGH
AND
REFLECT
OFF
THE
GOLD
LAYER
TO
WRITE
THE
CD
R
LASER
IS
TURNED
UP
TO
HIGH
POWER
MW
WHEN
THE
BEAM
HITS
A
SPOT
OF
DYE
IT
HEATS
UP
BREAKING
A
CHEMICAL
BOND
THIS
CHANGE
TO
THE
MOLECULAR
STRUCTURE
CREATES
A
DARK
SPOT
WHEN
READ
BACK
AT
MW
THE
PHOTODETECTOR
SEES
A
DIFFERENCE
BETWEEN
THE
DARK
SPOTS
WHERE
THE
DYE
HAS
BEEN
HIT
AND
TRANSPARENT
AREAS
WHERE
IT
IS
INTACT
THIS
DIFFERENCE
IS
INTERPRETED
AS
THE
DIFFERENCE
BETWEEN
PITS
AND
LANDS
EVEN
WHEN
READ
BACK
ON
A
REGULAR
CD
ROM
READER
OR
EVEN
ON
AN
AUDIO
CD
PLAYER
NO
NEW
KIND
OF
CD
COULD
HOLD
UP
ITS
HEAD
WITH
PRIDE
WITHOUT
A
COLORED
BOOK
SO
CD
R
HAS
THE
ORANGE
BOOK
PUBLISHED
IN
THIS
DOCUMENT
DEFINES
CD
R
INPUT
OUTPUT
CHAP
PRINTED
LABEL
SEC
DISKS
CD
R
MAKES
IT
POSSIBLE
FOR
INDIVIDUALS
AND
COMPANIES
TO
EASILY
COPY
CD
ROMS
AND
AUDIO
CDS
GENERALLY
IN
VIOLATION
OF
THE
PUBLISHER
COPYRIGHT
SEVER
AL
SCHEMES
HAVE
BEEN
DEVISED
TO
MAKE
SUCH
PIRACY
HARDER
AND
TO
MAKE
IT
DIFFICULT
PROTECTIVE
LACQUER
REFLECTIVE
GOLD
LAYER
DARK
SPOT
IN
THE
DYE
LAYER
BURNED
TO
READ
A
CD
ROM
USING
ANYTHING
OTHER
THAN
THE
PUBLISHER
SOFTWARE
ONE
OF
THEM
INVOLVES
RECORDING
ALL
THE
FILE
LENGTHS
ON
THE
CD
ROM
AS
MULTIGIGABYTE
MM
POLYCARBONATE
DIRECTION
DYE
LAYER
SUBSTRATE
I
BY
LASER
WHEN
WRITING
THWARTING
ANY
ATTEMPTS
TO
COPY
THE
FILES
TO
HARD
DISK
USING
STANDARD
COPYING
SOFT
WARE
THE
TRUE
LENGTHS
ARE
EMBEDDED
IN
THE
PUBLISHER
SOFTWARE
OR
HIDDEN
POS
SIBLY
ENCRYPTED
ON
THE
CD
ROM
IN
AN
UNEXPECTED
PLACE
ANOTHER
SCHEME
USES
INTENTIONALLY
INCORRECT
ECCS
IN
SELECTED
SECTORS
IN
THE
EXPECTATION
THAT
CD
COPY
ING
SOFTWARE
WILL
FIX
THE
ERRORS
THE
APPLICATION
SOFTWARE
CHECKS
THE
ECCS
IT
OF
MOTION
PHOTODETECTOR
LENS
PRISM
F
J
INFRARED
L
LASER
J
DIODE
SELF
REFUSING
TO
WORK
IF
THEY
ARE
CORRECT
USING
NONSTANDARD
GAPS
BETWEEN
THE
TRACKS
AND
OTHER
PHYSICAL
DEFECTS
ARE
ALSO
POSSIBILITIES
CD
REWRITABLES
ALTHOUGH
PEOPLE
ARE
USED
TO
OTHER
WRITE
ONCE
MEDIA
SUCH
AS
PAPER
AND
PHOTO
GRAPHIC
FILM
THERE
IS
A
DEMAND
FOR
A
REWRITABLE
CD
ROM
ONE
TECHNOLOGY
NOW
AVAILABLE
IS
CD
RW
CD
REWRITABLE
WHICH
USES
THE
SAME
SIZE
MEDIA
AS
CD
FIGURE
CROSS
SECTION
OF
A
CD
R
DISK
AND
LASER
NOT
TO
SCALE
A
SILVER
CD
ROM
HAS
A
SIMILAR
STRUCTURE
EXCEPT
WITHOUT
THE
DYE
LAYER
AND
WITH
A
PITTED
ALUMINUM
LAYER
INSTEAD
OF
A
GOLD
LAYER
AND
ALSO
A
NEW
FORMAT
CD
ROM
XA
WHICH
ALLOWS
CD
RS
TO
BE
WRITTEN
INCRE
MENTALLY
A
FEW
SECTORS
TODAY
A
FEW
TOMORROW
AND
A
FEW
NEXT
MONTH
A
GROUP
OF
CONSECUTIVE
SECTORS
WRITTEN
AT
ONCE
IS
CALLED
A
CD
ROM
TRACK
ONE
OF
THE
FIRST
USES
OF
CD
R
WAS
FOR
THE
KODAK
PHOTOCD
IN
THIS
SYSTEM
THE
CUSTOMER
BRINGS
A
ROLL
OF
EXPOSED
FILM
AND
HIS
OLD
PHOTOCD
TO
THE
PHOTO
PROCESSOR
AND
GETS
BACK
THE
SAME
PHOTOCD
WITH
THE
NEW
PICTURES
ADDED
AFTER
THE
OLD
ONES
THE
NEW
BATCH
WHICH
IS
CREATED
BY
SCANNING
IN
THE
NEGATIVES
IS
WRITTEN
ONTO
THE
PHOTOCD
AS
A
SEPARATE
CD
ROM
TRACK
INCREMENTAL
WRITING
WAS
NEEDED
BECAUSE
WHEN
THIS
PRODUCT
WAS
INTRODUCED
THE
CD
R
BLANKS
WERE
TOO
EXPENSIVE
TO
PROVIDE
A
NEW
ONE
FOR
EVERY
FILM
ROLL
HOWEVER
INCREMENTAL
WRITING
CREATES
A
NEW
PROBLEM
PRIOR
TO
THE
ORANGE
BOOK
ALL
CD
ROMS
HAD
A
SINGLE
VTOC
VOLUME
TABLE
OF
CONTENTS
AT
THE
START
THAT
SCHEME
DOES
NOT
WORK
WITH
INCREMENTAL
I
E
MULTITRACK
WRITES
THE
ORANGE
BOOK
SOLUTION
IS
TO
GIVE
EACH
CD
ROM
TRACK
ITS
OWN
VTOC
THE
FILES
LISTED
IN
THE
VTOC
CAN
INCLUDE
SOME
OR
ALL
OF
THE
FILES
FROM
PREVIOUS
TRACKS
AFTER
THE
CD
R
IS
INSERTED
INTO
THE
DRIVE
THE
OPERATING
SYSTEM
SEARCHES
THROUGH
ALL
THE
CD
ROM
TRACKS
TO
LOCATE
THE
MOST
RECENT
VTOC
WHICH
GIVES
THE
CURRENT
STATUS
OF
THE
DISK
BY
INCLUDING
SOME
BUT
NOT
ALL
OF
THE
FILES
FROM
PREVIOUS
TRACKS
IN
THE
CURRENT
VTOC
IT
IS
POSSIBLE
TO
GIVE
THE
ILLUSION
THAT
FILES
HAVE
BEEN
DELETED
TRACKS
CAN
BE
GROUPED
INTO
SESSIONS
LEADING
TO
MULTISESSION
CD
ROMS
STAN
DARD
AUDIO
CD
PLAYERS
CANNOT
HANDLE
MULTISESSION
CDS
SINCE
THEY
EXPECT
A
SINGLE
VTOC
AT
THE
START
SOME
COMPUTER
APPLICATIONS
CAN
HANDLE
THEM
THOUGH
R
HOWEVER
INSTEAD
OF
CYANINE
OR
PTHALOCYANINE
DYE
CR
RW
USES
AN
ALLOY
OF
SILVER
INDIUM
ANTIMONY
AND
TELLURIUM
FOR
THE
RECORDING
LAYER
THIS
ALLOY
HAS
TWO
STABLE
STATES
CRYSTALLINE
AND
AMORPHOUS
WITH
DIFFERENT
REFLECTIVITIES
CD
RW
DRIVES
USE
LASERS
WITH
THREE
DIFFERENT
POWERS
AT
HIGH
POWER
THE
LASER
MELTS
THE
ALLOY
CONVERTING
IT
FROM
THE
HIGH
REFLECTIVITY
CRYSTALLINE
STATE
TO
THE
LOW
REFLECTIVITY
AMORPHOUS
STATE
TO
REPRESENT
A
PIT
AT
MEDIUM
POWER
THE
ALLOY
MELTS
AND
REFORMS
IN
ITS
NATURAL
CRYSTALLINE
STATE
TO
BECOME
A
LAND
AGAIN
AT
LOW
POWER
THE
STATE
OF
THE
MATERIAL
IS
SENSED
FOR
READING
BUT
NO
PHASE
TRANSITION
OCCURS
THE
REASON
CD
RW
HAS
NOT
REPLACED
CD
R
IS
THAT
THE
CD
RW
BLANKS
ARE
MORE
EXPENSIVE
THAN
THE
CR
R
BLANKS
ALSO
FOR
APPLICATIONS
CONSISTING
OF
BACK
ING
UP
HARD
DISKS
THE
FACT
THAT
ONCE
WRITTEN
A
CD
R
CANNOT
BE
ACCIDENTALLY
ERASED
IS
A
BIG
PLUS
DVD
THE
BASIC
CD
CD
ROM
FORMAT
HAS
BEEN
AROUND
SINCE
THE
TECHNOLOGY
HAS
IMPROVED
SINCE
THEN
SO
HIGHER
CAPACITY
OPTICAL
DISKS
ARE
NOW
ECONOMICALLY
FEASIBLE
AND
THERE
IS
GREAT
DEMAND
FOR
THEM
HOLLYWOOD
WOULD
DEARLY
LOVE
TO
ELIMINATE
ANALOG
VIDEO
TAPES
IN
FAVOR
OF
DIGITAL
DISKS
SINCE
DISKS
HAVE
A
HIGHER
QUALITY
ARE
CHEAPER
TO
MANUFACTURE
LAST
LONGER
TAKE
UP
LESS
SHELF
SPACE
IN
VIDEO
STORES
AND
DO
NOT
HAVE
TO
BE
REWOUND
THE
CONSUMER
ELECTRONICS
COMPANIES
ARE
ALWAYS
LOOKING
FOR
A
NEW
BLOCKBUSTER
PRODUCT
AND
MANY
COMPUTER
COMPANIES
WANT
TO
ADD
MULTIMEDIA
FEATURES
TO
THEIR
SOFTWARE
THIS
COMBINATION
OF
TECHNOLOGY
AND
DEMAND
BY
THREE
IMMENSELY
RICH
AND
POWERFUL
INDUSTRIES
LED
TO
DVD
ORIGINALLY
AN
ACRONYM
FOR
DIGITAL
VIDEO
DISK
INPUT
OUTPUT
CHAP
BUT
NOW
OFFICIALLY
DIGITAL
VERSATILE
DISK
DVDS
USE
THE
SAME
GENERAL
DESIGN
AS
CDS
WITH
MM
INJECTION
MOLDED
POLYCARBONATE
DISKS
CONTAINING
PITS
AND
LANDS
THAT
ARE
ILLUMINATED
BY
A
LASER
DIODE
AND
READ
BY
A
PHOTODETECTOR
WHAT
IS
NEW
IS
THE
USE
OF
SMALLER
PITS
MICRONS
VERSUS
MICRONS
FOR
CDS
A
TIGHTER
SPIRAL
MICRONS
BETWEEN
TRACKS
VERSUS
MICRONS
FOR
CDS
A
RED
LASER
AT
MICRONS
VERSUS
MICRONS
FOR
CDS
TOGETHER
THESE
IMPROVEMENTS
RAISE
THE
CAPACITY
SEVENFOLD
TO
GB
A
LX
DVD
DRIVE
OPERATES
AT
MB
SEC
VERSUS
KB
SEC
FOR
CDS
UNFORTUNATELY
THE
SWITCH
TO
THE
RED
LASERS
USED
IN
SUPERMARKETS
MEANS
THAT
DVD
PLAYERS
REQUIRE
A
SECOND
LASER
OR
FANCY
CONVERSION
OPTICS
TO
BE
ABLE
TO
READ
EXISTING
CDS
AND
CD
ROMS
BUT
WITH
THE
DROP
IN
PRICE
OF
LASERS
MOST
OF
THEM
NOW
HAVE
BOTH
OF
THEM
SO
THEY
CAN
READ
BOTH
KINDS
OF
MEDIA
IS
GB
ENOUGH
MAYBE
USING
MPEG
COMPRESSION
STANDARDIZED
IN
IS
A
GB
DVD
DISK
CAN
HOLD
MINUTES
OF
FULL
SCREEN
FULL
MOTION
VIDEO
AT
HIGH
RESOLUTION
X
AS
WELL
AS
SOUNDTRACKS
IN
UP
TO
EIGHT
LAN
GUAGES
AND
SUBTITLES
IN
MORE
ABOUT
OF
ALL
THE
MOVIES
HOLLYWOOD
HAS
EVER
MADE
ARE
UNDER
MINUTES
NEVERTHELESS
SOME
APPLICATIONS
SUCH
AS
MULTIMEDIA
GAMES
OR
REFERENCE
WORKS
MAY
NEED
MORE
AND
HOLLYWOOD
WOULD
LIKE
TO
PUT
MUL
TIPLE
MOVIES
ON
THE
SAME
DISK
SO
FOUR
FORMATS
HAVE
BEEN
DEFINED
SINGLE
SIDED
SINGLE
LAYER
GB
SINGLE
SIDED
DUAL
LAYER
GB
DOUBLE
SIDED
SINGLE
LAYER
GB
DOUBLE
SIDED
DUAL
LAYER
GB
WHY
SO
MANY
FORMATS
IN
A
WORD
POLITICS
PHILIPS
AND
SONY
WANTED
SINGLE
SIDED
DUAL
LAYER
DISKS
FOR
THE
HIGH
CAPACITY
VERSION
BUT
TOSHIBA
AND
TIME
WARNER
WANTED
DOUBLE
SIDED
SINGLE
LAYER
DISKS
PHILIPS
AND
SONY
DID
NOT
THINK
PEOPLE
WOULD
BE
WILLING
TO
TURN
THE
DISKS
OVER
AND
TIME
WARNER
DID
NOT
BELIEVE
PUTTING
TWO
LAYERS
ON
ONE
SIDE
COULD
BE
MADE
TO
WORK
THE
COMPROMISE
ALL
COMBINATIONS
BUT
THE
MARKET
WILL
DETERMINE
WHICH
ONES
SURVIVE
THE
DUAL
LAYERING
TECHNOLOGY
HAS
A
REFLECTIVE
LAYER
AT
THE
BOTTOM
TOPPED
WITH
A
SEMIREFLECTIVE
LAYER
DEPENDING
ON
WHERE
THE
LASER
IS
FOCUSED
IT
BOUNCES
OFF
ONE
LAYER
OR
THE
OTHER
THE
LOWER
LAYER
NEEDS
SLIGHTLY
LARGER
PITS
AND
LANDS
TO
BE
READ
RELIABLY
SO
ITS
CAPACITY
IS
SLIGHTLY
SMALLER
THAN
THE
UPPER
LAYER
DOUBLE
SIDED
DISKS
ARE
MADE
BY
TAKING
TWO
MM
SINGLE
SIDED
DISKS
AND
GLUING
THEM
TOGETHER
BACK
TO
BACK
TO
MAKE
THE
THICKNESSES
OF
ALL
VERSIONS
THE
SAME
A
SINGLE
SIDED
DISK
CONSISTS
OF
A
MM
DISK
BONDED
TO
A
BLANK
SUBSTRATE
OR
PERHAPS
IN
THE
FUTURE
ONE
CONSISTING
OF
MINUTES
OF
ADVERTISING
IN
THE
HOPE
SEC
DISKS
TIIAT
PEOPLE
WILL
BE
CURIOUS
AS
TO
WHAT
IS
DOWN
THERE
THE
STRUCTURE
OF
THE
DOUBLE
SIDED
DUAL
LAYER
DISK
IS
ILLUSTRATED
IN
FIG
MM
POLYCARBONATE
SUBSTRATE
SINGLE
SIDED
DISK
MM
SINGLE
SIDED
DISK
POLYCARBONATE
SUBSTRATE
FIGURE
A
DOUBLE
SIDED
DUAL
LAYER
DVD
DISK
DVD
WAS
DEVISED
BY
A
CONSORTIUM
OF
CONSUMER
ELECTRONICS
COMPANIES
SEVEN
OF
THEM
JAPANESE
IN
CLOSE
COOPERATION
WITH
THE
MAJOR
HOLLYWOOD
STUDIOS
SOME
OF
WHICH
ARE
OWNED
BY
THE
JAPANESE
ELECTRONICS
COMPANIES
IN
THE
CONSOR
TIUM
THE
COMPUTER
AND
TELECOMMUNICATIONS
INDUSTRIES
WERE
NOT
INVITED
TO
THE
PICNIC
AND
THE
RESULTING
FOCUS
WAS
ON
USING
DVD
FOR
MOVIE
RENTAL
AND
SALES
SHOWS
FOR
EXAMPLE
STANDARD
FEATURES
INCLUDE
REAL
TIME
SKIPPING
OF
DIRTY
SCENES
TO
ALLOW
PARENTS
TO
TURN
A
FILM
RATED
INTO
ONE
SAFE
FOR
TODDLERS
SIX
CHANNEL
SOUND
AND
SUPPORT
FOR
PAN
AND
SCAN
THE
LATTER
FEATURE
ALLOWS
THE
DVD
PLAYER
TO
DYNAMICALLY
DECIDE
HOW
TO
CROP
THE
LEFT
AND
RIGHT
EDGES
OFF
MOVIES
WHOSE
WIDTH
HEIGHT
RATIO
IS
TO
FIT
ON
CURRENT
TELEVISION
SETS
WHOSE
ASPECT
RATIO
IS
ANOTHER
ITEM
THE
COMPUTER
INDUSTRY
PROBABLY
WOULD
NOT
HAVE
THOUGHT
OF
IS
AN
INTENTIONAL
INCOMPATIBILITY
BETWEEN
DISKS
INTENDED
FOR
THE
UNITED
STATES
AND
DISKS
INTENDED
FOR
EUROPE
AND
YET
OTHER
STANDARDS
FOR
OTHER
CONTINENTS
HOLLYWOOD
DE
MANDED
THIS
FEATURE
BECAUSE
NEW
FILMS
ARE
ALWAYS
RELEASED
FIRST
IN
THE
UNITED
STATES
AND
THEN
SHIPPED
TO
EUROPE
WHEN
THE
VIDEOS
COME
OUT
IN
THE
UNITED
STATES
THE
IDEA
WAS
TO
MAKE
SURE
EUROPEAN
VIDEO
STORES
COULD
NOT
BUY
VIDEOS
IN
THE
U
TOO
EARLY
THEREBY
REDUCING
NEW
MOVIES
EUROPEAN
THEATER
SALES
IF
HOLLYWOOD
HAD
BEEN
RUNNING
THE
COMPUTER
INDUSTRY
WE
WOULD
HAVE
HAD
INCH
FLOPPY
DISKS
IN
THE
UNITED
STATES
AND
CM
FLOPPY
DISKS
IN
EUROPE
THE
FOLKS
WHO
BROUGHT
YOU
SINGLE
DOUBLE
SIDED
DVDS
AND
SINGLE
DOUBLE
LAYER
DVDS
ARE
AT
IT
AGAIN
THE
NEXT
GENERATION
ALSO
LACKS
A
SINGLE
STANDARD
DUE
TO
POLIT
ICAL
BICKERING
BY
THE
INDUSTRY
PLAYERS
ONE
OF
THE
NEW
DEVICES
IS
BLU
RAY
WHICH
USES
A
MICRON
BLUE
LASER
TO
PACK
GB
ONTO
A
SINGLE
LAYER
DISK
AND
GB
ONTO
A
DOUBLE
LAYER
DISK
THE
OTHER
ONE
IS
HD
DVD
WHICH
USES
THE
SAME
BLUE
LASER
BUT
HAS
A
CAPACITY
OF
ONLY
GB
SINGLE
LAYER
AND
GB
DOUBLE
LAYER
THIS
FORMAT
WAR
HAS
SPLIT
THE
MOVIE
STUDIOS
THE
COMPUTER
MANUFACTURERS
INPUT
OUTPUT
CHAP
AND
THE
SOFTWARE
COMPANIES
AS
A
RESULT
OF
THE
LACK
OF
STANDARDIZATION
THIS
GEN
ERATION
IS
TAKING
OFF
RATHER
SLOWLY
AS
CONSUMERS
WAIT
FOR
THE
DUST
TO
SETTLE
TO
SEE
WHICH
FORMAT
WILL
WIN
THIS
STUPIDITY
ON
THE
PART
OF
THE
INDUSTRY
BRINGS
TO
MIND
GEORGE
SANTAYANA
FAMOUS
REMARK
THOSE
WHO
CANNOT
LEARN
FROM
HISTORY
ARE
DOOMED
TO
REPEAT
IT
DISK
FORMATTING
A
HARD
DISK
CONSISTS
OF
A
STACK
OF
ALUMINUM
ALLOY
OR
GLASS
PLATTERS
INCH
OR
INCH
IN
DIAMETER
OR
EVEN
SMALLER
ON
NOTEBOOK
COMPUTERS
ON
EACH
PLATTER
IS
DEPOSITED
A
THIN
MAGNETIZABLE
METAL
OXIDE
AFTER
MANUFACTURING
THERE
IS
NO
INFORMATION
WHATSOEVER
ON
THE
DISK
BEFORE
THE
DISK
CAN
BE
USED
EACH
PLATTER
MUST
RECEIVE
A
LOW
LEVEL
FORMAT
DONE
BY
SOFTWARE
THE
FORMAT
CONSISTS
OF
A
SERIES
OF
CONCENTRIC
TRACKS
EACH
CON
TAINING
SOME
NUMBER
OF
SECTORS
WITH
SHORT
GAPS
BETWEEN
THE
SECTORS
THE
FORMAT
OF
A
SECTOR
IS
SHOWN
IN
FIG
PREAMBLE
DATA
EC
C
FIGURE
A
DISK
SECTOR
THE
PREAMBLE
STARTS
WITH
A
CERTAIN
BIT
PATTERN
THAT
ALLOWS
THE
HARDWARE
TO
RECOGNIZE
THE
START
OF
THE
SECTOR
IT
ALSO
CONTAINS
THE
CYLINDER
AND
SECTOR
NUMBERS
AND
SOME
OTHER
INFORMATION
THE
SIZE
OF
THE
DATA
PORTION
IS
DETERMINED
BY
THE
LOW
LEVEL
FORMATTING
PROGRAM
MOST
DISKS
USE
BYTE
SECTORS
THE
ECC
FIELD
CONTAINS
REDUNDANT
INFORMATION
THAT
CAN
BE
USED
TO
RECOVER
FROM
READ
ERRORS
THE
SIZE
AND
CONTENT
OF
THIS
FIELD
VARIES
FROM
MANUFACTURER
TO
MANUFACTURER
DEPENDING
ON
HOW
MUCH
DISK
SPACE
THE
DESIGNER
IS
WILLING
TO
GIVE
UP
FOR
HIGHER
RELIABILITY
AND
HOW
COMPLEX
AN
ECC
CODE
THE
CONTROLLER
CAN
HANDLE
A
BYTE
ECC
FIELD
IS
NOT
UNUSUAL
FURTHERMORE
ALL
HARD
DISKS
HAVE
SOME
NUMBER
OF
SPARE
SECTORS
ALLO
CATED
TO
BE
USED
TO
REPLACE
SECTORS
WITH
A
MANUFACTURING
DEFECT
THE
POSITION
OF
SECTOR
ON
EACH
TRACK
IS
OFFSET
FROM
THE
PREVIOUS
TRACK
WHEN
THE
LOW
LEVEL
FORMAT
IS
LAID
DOWN
THIS
OFFSET
CALLED
CYLINDER
SKEW
IS
DONE
TO
IMPROVE
PERFORMANCE
THE
IDEA
IS
TO
ALLOW
THE
DISK
TO
READ
MULTIPLE
TRACKS
IN
ONE
CONTINUOUS
OPERATION
WITHOUT
LOSING
DATA
THE
NATURE
OF
THE
PROBLEM
CAN
BE
SEEN
BY
LOOKING
AT
FIG
A
SUPPOSE
THAT
A
REQUEST
NEEDS
SECTORS
STARTING
AT
SEC
TOR
ON
THE
INNERMOST
TRACK
READING
THE
FIRST
SECTORS
TAKES
ONE
DISK
ROTATION
BUT
A
SEEK
IS
NEEDED
TO
MOVE
OUTWARD
ONE
TRACK
TO
GET
THE
SECTOR
BY
THE
TIME
THE
HEAD
HAS
MOVED
ONE
TRACK
SECTOR
HAS
ROTATED
PAST
THE
HEAD
SO
AN
ENTIRE
ROTA
TION
IS
NEEDED
UNTIL
IT
COMES
BY
AGAIN
THAT
PROBLEM
IS
ELIMINATED
BY
OFFSETTING
THE
SECTORS
AS
SHOWN
IN
FIG
SEC
DISKS
FIGURE
AN
ILLUSTRATION
OF
CYLINDER
SKEW
THE
AMOUNT
OF
CYLINDER
SKEW
DEPENDS
ON
THE
DRIVE
GEOMETRY
FOR
EXAMPLE
A
RPM
DRIVE
ROTATES
IN
MSEC
IF
A
TRACK
CONTAINS
SECTORS
A
NEW
SECTOR
PASSES
UNDER
THE
HEAD
EVERY
USEE
IF
THE
TRACK
TO
TRACK
SEEK
TIME
IS
PSEC
SECTORS
WILL
PASS
BY
DURING
THE
SEEK
SO
THE
CYLINDER
SKEW
SHOULD
BE
SECTORS
RATHER
THAN
THE
THREE
SECTORS
SHOWN
IN
FIG
IT
IS
WORTH
MENTIONING
THAT
SWITCHING
BETWEEN
HEADS
ALSO
TAKES
A
FINITE
TIME
SO
THERE
IS
HEAD
SKEW
AS
WELL
AS
CYLINDER
SKEW
BUT
HEAD
SKEW
IS
NOT
VERY
LARGE
AS
A
RESULT
OF
THE
LOW
LEVEL
FORMATTING
DISK
CAPACITY
IS
REDUCED
DEPENDING
ON
THE
SIZES
OF
THE
PREAMBLE
INTERSECTOR
GAP
AND
ECC
AS
WELL
AS
THE
NUMBER
OF
SPARE
SECTORS
RESERVED
OFTEN
THE
FORMATTED
CAPACITY
IS
LOWER
THAN
THE
UNFOR
MATTED
CAPACITY
THE
SPARE
SECTORS
DO
NOT
COUNT
TOWARD
THE
FORMATTED
CAPACITY
SO
ALL
DISKS
OF
A
GIVEN
TYPE
HAVE
EXACTLY
THE
SAME
CAPACITY
WHEN
SHIPPED
INDEPEN
DENT
OF
HOW
MANY
BAD
SECTORS
THEY
ACTUALLY
HAVE
IF
THE
NUMBER
OF
BAD
SECTORS
EXCEEDS
THE
NUMBER
OF
SPARES
THE
DRIVE
WILL
BE
REJECTED
AND
NOT
SHIPPED
THERE
IS
CONSIDERABLE
CONFUSION
ABOUT
DISK
CAPACITY
BECAUSE
SOME
MANUFACT
URERS
ADVERTISED
THE
UNFORMATTED
CAPACITY
TO
MAKE
THEIR
DRIVES
LOOK
LARGER
THAN
THEY
REALLY
ARE
FOR
EXAMPLE
CONSIDER
A
DRIVE
WHOSE
UNFORMATTED
CAPACITY
IS
X
BYTES
THIS
MIGHT
BE
SOLD
AS
A
GB
DISK
HOWEVER
AFTER
FORMATTING
PERHAPS
ONLY
X
BYTES
ARE
AVAILABLE
FOR
DATA
TO
ADD
TO
THE
CONFUSION
THE
INPUT
OUTPUT
CHAP
OPERATING
SYSTEM
WILL
PROBABLY
REPORT
THIS
CAPACITY
AS
GB
NOT
GB
BE
CAUSE
SOFTWARE
CONSIDERS
A
MEMORY
OF
GB
TO
BE
BYTES
NOT
BYTES
TO
MAKE
THINGS
WORSE
IN
THE
WORLD
OF
DATA
COMMUNICATIONS
GBPS
MEANS
BITS
SEC
BECAUSE
THE
PREFIX
GIGA
REALLY
DOES
MEAN
A
KILOMETER
IS
METERS
NOT
METERS
AFTER
ALL
ONLY
WITH
MEMORY
AND
DISK
SIZES
DO
KILO
MEGA
GIGA
AND
TERA
MEAN
AND
RESPECTIVELY
FORMATTING
ALSO
AFFECTS
PERFORMANCE
IF
A
RPM
DISK
HAS
SECTORS
PER
TRACK
OF
BYTES
EACH
IT
TAKES
MSEC
TO
READ
THE
BYTES
ON
A
TRACK
FOR
A
DATA
RATE
OF
BYTES
SEC
OR
MB
SEC
IT
IS
NOT
POSSIBLE
TO
GO
FASTER
THAN
THIS
NO
MATTER
WHAT
KIND
OF
INTERFACE
IS
PRESENT
EVEN
IF
IT
A
SCSI
INTER
FACE
AT
MB
SEC
OR
MB
SEC
ACTUALLY
READING
CONTINUOUSLY
AT
THIS
RATE
REQUIRES
A
LARGE
BUFFER
IN
THE
CON
TROLLER
CONSIDER
FOR
EXAMPLE
A
CONTROLLER
WITH
A
ONE
SECTOR
BUFFER
THAT
HAS
BEEN
GIVEN
A
COMMAND
TO
READ
TWO
CONSECUTIVE
SECTORS
AFTER
READING
THE
FIRST
SECTOR
FROM
THE
DISK
AND
DOING
THE
ECC
CALCULATION
THE
DATA
MUST
BE
TRANSFERRED
TO
MAIN
MEMORY
WHILE
THIS
TRANSFER
IS
TAKING
PLACE
THE
NEXT
SECTOR
WILL
FLY
BY
THE
HEAD
WHEN
THE
COPY
TO
MEMORY
IS
COMPLETE
THE
CONTROLLER
WILL
HAVE
TO
WAIT
ALMOST
AN
ENTIRE
ROTATION
TIME
FOR
THE
SECOND
SECTOR
TO
COME
AROUND
AGAIN
THIS
PROBLEM
CAN
BE
ELIMINATED
BY
NUMBERING
THE
SECTORS
IN
AN
INTERLEAVED
FASHION
WHEN
FORMATTING
THE
DISK
IN
FIG
A
WE
SEE
THE
USUAL
NUMBERING
PATTERN
IGNORING
CYLINDER
SKEW
HERE
IN
FIG
B
WE
SEE
SINGLE
INTERLEAV
ING
WHICH
GIVES
THE
CONTROLLER
SOME
BREATHING
SPACE
BETWEEN
CONSECUTIVE
SECTORS
IN
ORDER
TO
COPY
THE
BUFFER
TO
MAIN
MEMORY
O
M
FIGURE
A
NO
INTERLEAVING
B
SINGLE
INTERLEAVING
C
DOUBLE
INTERLEAVING
IF
THE
COPYING
PROCESS
IS
VERY
SLOW
THE
DOUBLE
INTERLEAVING
OF
FIG
C
MAY
BE
NEEDED
IF
THE
CONTROLLER
HAS
A
BUFFER
OF
ONLY
ONE
SECTOR
IT
DOES
NOT
MATTER
WHETHER
THE
COPYING
FROM
THE
BUFFER
TO
MAIN
MEMORY
IS
DONE
BY
THE
CONTROLLER
THE
MAIN
CPU
OR
A
DMA
CHIP
IT
STILL
TAKES
SOME
TIME
TO
AVOID
THE
NEED
FOR
INTER
LEAVING
THE
CONTROLLER
SHOULD
BE
ABLE
TO
BUFFER
AN
ENTIRE
TRACK
MANY
MODERN
CON
TROLLERS
CAN
DO
THIS
AFTER
LOW
LEVEL
FORMATTING
IS
COMPLETED
THE
DISK
IS
PARTITIONED
LOGICALLY
EACH
PARTITION
IS
LIKE
A
SEPARATE
DISK
PARTITIONS
ARE
NEEDED
TO
ALLOW
MULTIPLE
OPER
SEC
DISKS
ATING
SYSTEMS
TO
COEXIST
ALSO
IN
SOME
CASES
A
PARTITION
CAN
BE
USED
FOR
SWAP
PING
ON
THE
PENTIUM
AND
MOST
OTHER
COMPUTERS
SECTOR
CONTAINS
THE
MASTER
BOOT
RECORD
WHICH
CONTAINS
SOME
BOOT
CODE
PLUS
THE
PARTITION
TABLE
AT
THE
END
THE
PARTITION
TABLE
GIVES
THE
STARTING
SECTOR
AND
SIZE
OF
EACH
PARTITION
ON
THE
PEN
TIUM
THE
PARTITION
TABLE
HAS
ROOM
FOR
FOUR
PARTITIONS
IF
ALL
OF
THEM
ARE
FOR
WIN
DOWS
THEY
WILL
BE
CALLED
C
D
E
AND
F
AND
TREATED
AS
SEPARATE
DRIVES
IF
THREE
OF
THEM
ARE
FOR
WINDOWS
AND
ONE
IS
FOR
UNIX
THEN
WINDOWS
WILL
CALL
ITS
PARTI
TIONS
C
D
AND
E
THE
FIRST
CD
ROM
WILL
THEN
BE
F
TO
BE
ABLE
TO
BOOT
FROM
THE
HARD
DISK
ONE
PARTITION
MUST
BE
MARKED
AS
ACTIVE
IN
THE
PARTITION
TABLE
THE
FINAL
STEP
IN
PREPARING
A
DISK
FOR
USE
IS
TO
PERFORM
A
HIGH
LEVEL
FORMAT
OF
EACH
PARTITION
SEPARATELY
THIS
OPERATION
LAYS
DOWN
A
BOOT
BLOCK
THE
FREE
STOR
AGE
ADMINISTRATION
FREE
LIST
OR
BITMAP
ROOT
DIRECTORY
AND
AN
EMPTY
FILE
SYSTEM
IT
ALSO
PUTS
A
CODE
IN
THE
PARTITION
TABLE
ENTRY
TELLING
WHICH
FILE
SYSTEM
IS
USED
IN
THE
PARTITION
BECAUSE
MANY
OPERATING
SYSTEMS
SUPPORT
MULTIPLE
INCOMPATIBLE
FILE
SYSTEMS
FOR
HISTORICAL
REASONS
AT
THIS
POINT
THE
SYSTEM
CAN
BE
BOOTED
WHEN
THE
POWER
IS
TURNED
ON
THE
BIOS
RUNS
INITIALLY
AND
THEN
READS
IN
THE
MASTER
BOOT
RECORD
AND
JUMPS
TO
IT
THIS
BOOT
PROGRAM
THEN
CHECKS
TO
SEE
WHICH
PARTITION
IS
ACTIVE
THEN
IT
READS
IN
THE
BOOT
SECTOR
FROM
THAT
PARTITION
AND
RUNS
IT
THE
BOOT
SECTOR
CONTAINS
A
SMALL
PROGRAM
THAT
GENERAL
LOADS
A
LARGER
BOOTSTRAP
LOADER
THAT
SEARCHES
THE
FILE
SYSTEM
TO
FIND
THE
OPERATING
SYSTEM
KERNEL
THAT
PRO
GRAM
IS
LOADED
INTO
MEMORY
AND
EXECUTED
DISK
ARM
SCHEDULING
ALGORITHMS
IN
THIS
SECTION
WE
WILL
LOOK
AT
SOME
ISSUES
RELATED
TO
DISK
DRIVERS
IN
GENERAL
FIRST
CONSIDER
HOW
LONG
IT
TAKES
TO
READ
OR
WRITE
A
DISK
BLOCK
THE
TIME
REQUIRED
IS
DETERMINED
BY
THREE
FACTORS
SEEK
TIME
THE
TIME
TO
MOVE
THE
ARM
TO
THE
PROPER
CYLINDER
ROTATIONAL
DELAY
THE
TIME
FOR
THE
PROPER
SECTOR
TO
ROTATE
UNDER
THE
HEAD
ACTUAL
DATA
TRANSFER
TIME
FOR
MOST
DISKS
THE
SEEK
TIME
DOMINATES
THE
OTHER
TWO
TIMES
SO
REDUCING
THE
MEAN
SEEK
TIME
CAN
IMPROVE
SYSTEM
PERFORMANCE
SUBSTANTIALLY
IF
THE
DISK
DRIVER
ACCEPTS
REQUESTS
ONE
AT
A
TIME
AND
CARRIES
THEM
OUT
IN
THAT
ORDER
THAT
IS
FIRST
COME
FIRST
SERVED
FCFS
LITTLE
CAN
BE
DONE
TO
OPTIMIZE
SEEK
TIME
HOWEVER
ANOTHER
STRATEGY
IS
POSSIBLE
WHEN
THE
DISK
IS
HEAVILY
LOADED
IT
IS
LIKELY
THAT
WHILE
THE
ARM
IS
SEEKING
ON
BEHALF
OF
ONE
REQUEST
OTHER
DISK
RE
QUESTS
MAY
BE
GENERATED
BY
OTHER
PROCESSES
MANY
DISK
DRIVERS
MAINTAIN
A
TABLE
INDEXED
BY
CYLINDER
NUMBER
WITH
ALL
THE
PENDING
REQUESTS
FOR
EACH
CYLINDER
CHAINED
TOGETHER
IN
A
LINKED
LIST
HEADED
BY
THE
TABLE
ENTRIES
GIVEN
THIS
KIND
OF
DATA
STRUCTURE
WE
CAN
IMPROVE
UPON
THE
FIRST
COME
FIRST
SERVED
SCHEDULING
ALGORITHM
TO
SEE
HOW
CONSIDER
AN
IMAGINARY
DISK
WITH
INPUT
OUTPUT
CHAP
SEC
DISKS
CYLINDERS
A
REQUEST
COMES
IN
TO
READ
A
BLOCK
ON
CYLINDER
WHILE
THE
SEEK
TO
CYLINDER
IS
IN
PROGRESS
NEW
REQUESTS
COME
IN
FOR
CYLINDERS
AND
IN
THAT
ORDER
THEY
ARE
ENTERED
INTO
THE
TABLE
OF
PENDING
REQUESTS
WITH
A
SEPA
RATE
LINKED
LIST
FOR
EACH
CYLINDER
THE
REQUESTS
ARE
SHOWN
IN
FIG
SAME
DIRECTION
UNTIL
THERE
ARE
NO
MORE
OUTSTANDING
REQUESTS
IN
THAT
DIRECTION
THEN
THEY
SWITCH
DIRECTIONS
THIS
ALGORITHM
KNOWN
BOTH
IN
THE
DISK
WORLD
AND
THE
ELEVATOR
WORLD
AS
THE
ELEVATOR
ALGORITHM
REQUIRES
THE
SOFTWARE
TO
MAINTAIN
BIT
THE
CURRENT
DIRECTION
BIT
UP
OR
DOWN
WHEN
A
REQUEST
FINISHES
THE
DISK
OR
INITIAL
POSITION
PENDIN
G
REQUESTS
ELEVATOR
DRIVER
CHECKS
THE
BIT
IF
IT
IS
UP
THE
ARM
OR
CABIN
IS
MOVED
TO
THE
NEXT
HIGHEST
PENDING
REQUEST
IF
NO
REQUESTS
ARE
PENDING
AT
HIGHER
POSITIONS
THE
DIREC
TION
BIT
IS
REVERSED
WHEN
THE
BIT
IS
SET
TO
DOWN
THE
MOVE
IS
TO
THE
NEXT
LOWEST
REQUESTED
POSITION
IF
ANY
CYLINDE
R
SEQUENC
E
OF
SEEK
FIGURE
SHORTEST
SEEK
FIRST
SSF
DISK
SCHEDULING
ALGORITHM
WHEN
THE
CURRENT
REQUEST
FOR
CYLINDER
IS
FINISHED
THE
DISK
DRIVER
HAS
A
CHOICE
OF
WHICH
REQUEST
TO
HANDLE
NEXT
USING
FCFS
IT
WOULD
GO
NEXT
TO
CYLINDER
THEN
TO
AND
SO
ON
THIS
ALGORITHM
WOULD
REQUIRE
ARM
MOTIONS
OF
AND
RESPECTIVELY
FOR
A
TOTAL
OF
CYLINDERS
ALTERNATIVELY
IT
COULD
ALWAYS
HANDLE
THE
CLOSEST
REQUEST
NEXT
TO
MINIMIZE
SEEK
TIME
GIVEN
THE
REQUESTS
OF
FIG
THE
SEQUENCE
IS
AND
SHOWN
AS
THE
JAGGED
LINE
AT
THE
BOTTOM
OF
FIG
WITH
THIS
SEQUENCE
THE
ARM
MOTIONS
ARE
AND
FOR
A
TOTAL
OF
CYLINDERS
THIS
ALGORITHM
SHORTEST
SEEK
FIRST
SSF
CUTS
THE
TOTAL
ARM
MOTION
ALMOST
IN
HALF
COMPARED
TO
FCFS
UNFORTUNATELY
SSF
HAS
A
PROBLEM
SUPPOSE
MORE
REQUESTS
KEEP
COMING
IN
WHILE
THE
REQUESTS
OF
FIG
ARE
BEING
PROCESSED
FOR
EXAMPLE
IF
AFTER
GOING
TO
CYLINDER
A
NEW
REQUEST
FOR
CYLINDER
IS
PRESENT
THAT
REQUEST
WILL
HAVE
PRIORITY
OVER
CYLINDER
IF
A
REQUEST
FOR
CYLINDER
THEN
COMES
IN
THE
ARM
WILL
NEXT
GO
TO
INSTEAD
OF
WITH
A
HEAVILY
LOADED
DISK
THE
ARM
WILL
TEND
TO
STAY
IN
THE
MID
DLE
OF
THE
DISK
MOST
OF
THE
TIME
SO
REQUESTS
AT
EITHER
EXTREME
WILL
HAVE
TO
WAIT
UNTIL
A
STATISDCAL
FLUCTUATION
IN
THE
LOAD
CAUSES
THERE
TO
BE
NO
REQUESTS
NEAR
THE
MIDDLE
REQUESTS
FAR
FROM
THE
MIDDLE
MAY
GET
POOR
SERVICE
THE
GOALS
OF
MINIMAL
RESPONSE
TIME
AND
FAIRNESS
ARE
IN
CONFLICT
HERE
TALL
BUILDINGS
ALSO
HAVE
TO
DEAL
WITH
THIS
TRADE
OFF
THE
PROBLEM
OF
SCHEDULING
AN
ELEVATOR
IN
A
TALL
BUILDING
IS
SIMILAR
TO
THAT
OF
SCHEDULING
A
DISK
ARM
REQUESTS
COME
IN
CONTINUOUSLY
CALLING
THE
ELEVATOR
TO
FLOORS
CYLINDERS
AT
RANDOM
THE
COMPUTER
RUNNING
THE
ELEVATOR
COULD
EASILY
KEEP
TRACK
OF
THE
SEQUENCE
IN
WHICH
CUSTOMERS
PUSHED
THE
CALL
BUTTON
AND
SERVICE
THEM
USING
FCFS
OR
SSF
HOWEVER
MOST
ELEVATORS
USE
A
DIFFERENT
ALGORITHM
IN
ORDER
TO
RECONCILE
THE
MUTUALLY
CONFLICTING
GOALS
OF
EFFICIENCY
AND
FAIRNESS
THEY
KEEP
MOVING
IN
THE
FIGURE
SHOWS
THE
ELEVATOR
ALGORITHM
USING
THE
SAME
SEVEN
REQUESTS
AS
FIG
ASSUMING
THE
DIRECTION
BIT
WAS
INITIALLY
UP
THE
ORDER
IN
WHICH
THE
CYL
INDERS
ARE
SERVICED
IS
AND
WHICH
YIELDS
ARM
MOTIONS
OF
AND
FOR
A
TOTAL
OF
CYLINDERS
IN
THIS
CASE
THE
ELEVATOR
ALGORITHM
IS
SLIGHTLY
BETTER
THAN
SSF
ALTHOUGH
IT
IS
USUALLY
WORSE
ONE
NICE
PROPERTY
THAT
THE
ELEVATOR
ALGORITHM
HAS
IS
THAT
GIVEN
ANY
COLLECTION
OF
REQUESTS
THE
UPPER
BOUND
ON
THE
TOTAL
MOTION
IS
FIXED
IT
IS
JUST
TWICE
THE
NUMBER
OF
CYLINDERS
INITIAL
POSITION
FIGURE
THE
ELEVATOR
ALGORITHM
FOR
SCHEDULING
DISK
REQUESTS
A
SLIGHT
MODIFICATION
OF
THIS
ALGORITHM
THAT
HAS
A
SMALLER
VARIANCE
IN
RESPONSE
TIMES
TEORY
IS
TO
ALWAYS
SCAN
IN
THE
SAME
DIRECTION
WHEN
THE
HIGHEST
NUMBERED
CYLINDER
WITH
A
PENDING
REQUEST
HAS
BEEN
SERVICED
THE
ARM
GOES
TO
THE
LOWEST
NUMBERED
CYLINDER
WITH
A
PENDING
REQUEST
AND
THEN
CONTINUES
MOVING
IN
AN
UPWARD
DIRECTION
IN
EFFECT
THE
LOWEST
NUMBERED
CYLINDER
IS
THOUGHT
OF
AS
BEING
JUST
ABOVE
THE
HIGHEST
NUMBERED
CYLINDER
SOME
DISK
CONTROLLERS
PROVIDE
A
WAY
FOR
THE
SOFTWARE
TO
INSPECT
THE
CURRENT
SECTOR
NUMBER
UNDER
THE
HEAD
WITH
SUCH
A
CONTROLLER
ANOTHER
OPTIMIZATION
IS
POS
SIBLE
IF
TWO
OR
MORE
REQUESTS
FOR
THE
SAME
CYLINDER
ARE
PENDING
THE
DRIVER
CAN
ISSUE
A
REQUEST
FOR
THE
SECTOR
THAT
WILL
PASS
UNDER
THE
HEAD
NEXT
NOTE
THAT
WHEN
MULTIPLE
TRACKS
ARE
PRESENT
IN
A
CYLINDER
CONSECUTIVE
REQUESTS
CAN
BE
FOR
DIFFERENT
TRACKS
WITH
NO
PENALTY
THE
CONTROLLER
CAN
SELECT
ANY
OF
ITS
HEADS
ALMOST
INSTAN
TANEOUSLY
HEAD
SELECTION
INVOLVES
NEITHER
ARM
MOTION
NOR
ROTATIONAL
DELAY
INPUT
OUTPUT
CHAP
IF
THE
DISK
HAS
THE
PROPERTY
THAT
SEEK
TIME
IS
MUCH
FASTER
THAN
THE
ROTATIONAL
DELAY
THEN
A
DIFFERENT
OPTIMIZATION
SHOULD
BE
USED
PENDING
REQUESTS
SHOULD
BE
SORTED
BY
SECTOR
NUMBER
AND
AS
SOON
AS
THE
NEXT
SECTOR
IS
ABOUT
TO
PASS
UNDER
THE
HEAD
THE
ARM
SHOULD
BE
ZIPPED
OVER
TO
THE
RIGHT
TRACK
TO
READ
OR
WRITE
IT
WITH
A
MODERN
HARD
DISK
THE
SEEK
AND
ROTATIONAL
DELAYS
SO
DOMINATE
PER
FORMANCE
THAT
READING
ONE
OR
TWO
SECTORS
AT
A
TIME
IS
VERY
INEFFICIENT
FOR
THIS
REA
SON
MANY
DISK
CONTROLLERS
ALWAYS
READ
AND
CACHE
MULTIPLE
SECTORS
EVEN
WHEN
ONLY
ONE
IS
REQUESTED
TYPICALLY
ANY
REQUEST
TO
READ
A
SECTOR
WILL
CAUSE
THAT
SECTOR
AND
MUCH
OR
ALL
THE
REST
OF
THE
CURRENT
TRACK
TO
BE
READ
DEPENDING
UPON
HOW
MUCH
SPACE
IS
AVAILABLE
IN
THE
CONTROLLER
CACHE
MEMORY
THE
DISK
DESCRIBED
IN
FIG
HAS
A
MB
CACHE
FOR
EXAMPLE
THE
USE
OF
THE
CACHE
IS
DETERMINED
DYNAMI
CALLY
BY
THE
CONTROLLER
IN
ITS
SIMPLEST
MODE
THE
CACHE
IS
DIVIDED
INTO
TWO
SEC
TIONS
ONE
FOR
READS
AND
ONE
FOR
WRITES
IF
A
SUBSEQUENT
READ
CAN
BE
SATISFIED
OUT
OF
THE
CONTROLLER
CACHE
IT
CAN
RETURN
THE
REQUESTED
DATA
IMMEDIATELY
IT
IS
WORTH
NOTING
THAT
THE
DISK
CONTROLLER
CACHE
IS
COMPLETELY
INDEPENDENT
OF
THE
OPERATING
SYSTEM
CACHE
THE
CONTROLLER
CACHE
USUALLY
HOLDS
BLOCKS
THAT
HAVE
NOT
ACTUALLY
BEEN
REQUESTED
BUT
WHICH
WERE
CONVENIENT
THE
READ
BECAUSE
THEY
JUST
HAPPENED
TO
PASS
UNDER
THE
HEAD
AS
A
SIDE
EFFECT
OF
SOME
OTHER
READ
IN
CONTRAST
ANY
CACHE
MAINTAINED
BY
THE
OPERATING
SYSTEM
WILL
CONSIST
OF
BLOCKS
THAT
WERE
EXPLICITLY
READ
AND
WHICH
THE
OPERATING
SYSTEM
THINKS
MIGHT
BE
NEEDED
AGAIN
IN
THE
NEAR
FUTURE
E
G
A
DISK
BLOCK
HOLDING
A
DIRECTORY
BLOCK
WHEN
SEVERAL
DRIVES
ARE
PRESENT
ON
THE
SAME
CONTROLLER
THE
OPERATING
SYSTEM
SHOULD
MAINTAIN
A
PENDING
REQUEST
TABLE
FOR
EACH
DRIVE
SEPARATELY
WHENEVER
ANY
DRIVE
IS
IDLE
A
SEEK
SHOULD
BE
ISSUED
TO
MOVE
ITS
ARM
TO
THE
CYLINDER
WHERE
IT
WILL
BE
NEEDED
NEXT
ASSUMING
THE
CONTROLLER
ALLOWS
OVERLAPPED
SEEKS
WHEN
THE
CUR
RENT
TRANSFER
FINISHES
A
CHECK
CAN
BE
MADE
TO
SEE
IF
ANY
DRIVES
ARE
POSITIONED
ON
THE
CORRECT
CYLINDER
IF
ONE
OR
MORE
ARE
THE
NEXT
TRANSFER
CAN
BE
STARTED
ON
A
DRIVE
THAT
IS
ALREADY
ON
THE
RIGHT
CYLINDER
IF
NONE
OF
THE
ARMS
IS
IN
THE
RIGHT
PLACE
THE
DRIVER
SHOULD
ISSUE
A
NEW
SEEK
ON
THE
DRIVE
THAT
JUST
COMPLETED
A
TRANSFER
AND
WAIT
UNTIL
THE
NEXT
INTERRUPT
TO
SEE
WHICH
ARM
GETS
TO
ITS
DESTINATION
FIRST
IT
IS
IMPORTANT
TO
REALIZE
THAT
ALL
OF
THE
ABOVE
DISK
SCHEDULING
ALGORITHMS
TACITLY
ASSUME
THAT
THE
REAL
DISK
GEOMETRY
IS
THE
SAME
AS
THE
VIRTUAL
GEOMETRY
IF
IT
IS
NOT
THEN
SCHEDULING
DISK
REQUESTS
MAKES
NO
SENSE
BECAUSE
THE
OPERATING
SYSTEM
CANNOT
REALLY
TELL
WHETHER
CYLINDER
OR
CYLINDER
IS
CLOSER
TO
CYLINDER
ON
THE
OTHER
HAND
IF
THE
DISK
CONTROLLER
CAN
ACEEPT
MULTIPLE
OUTSTANDING
REQUESTS
IT
CAN
USE
THESE
SCHEDULING
ALGORITHMS
INTERNALLY
IN
THAT
CASE
THE
ALGORITHMS
ARE
STILL
VALID
BUT
ONE
LEVEL
DOWN
INSIDE
THE
CONTROLLER
ERROR
HANDLING
DISK
MANUFACTURERS
ARE
CONSTANTLY
PUSHING
THE
LIMITS
OF
THE
TECHNOLOGY
BY
INCREASING
LINEAR
BIT
DENSITIES
A
TRACK
MIDWAY
OUT
ON
A
INCH
DISK
HAS
A
CIR
CUMFERENCE
OF
ABOUT
MM
IF
THE
TRACK
HOLDS
SECTORS
OF
BYTES
THE
SEC
DISKS
LINEAR
RECORDING
DENSITY
MAY
BE
ABOUT
BITS
MM
TAKING
INTO
ACCOUNT
THE
FACT
THAT
SOME
SPACE
IS
LOST
TO
PREAMBLES
ECCS
AND
INTERSECTOR
GAPS
RECORDING
BITS
MM
REQUIRES
AN
EXTREMELY
UNIFORM
SUBSTRATE
AND
A
VERY
FINE
OXIDE
COATING
UNFORTUNATELY
IT
IS
NOT
POSSIBLE
TO
MANUFACTURE
A
DISK
TO
SUCH
SPECIFICATIONS
WITH
OUT
DEFECTS
AS
SOON
AS
MANUFACTURING
TECHNOLOGY
HAS
IMPROVED
TO
THE
POINT
WHERE
IT
IS
POSSIBLE
TO
OPERATE
FLAWLESSLY
AT
SUCH
DENSITIES
DISK
DESIGNERS
WILL
GO
TO
HIGHER
DENSITIES
TO
INCREASE
THE
CAPACITY
DOING
SO
WILL
PROBABLY
REINTRODUCE
DEFECTS
MANUFACTURING
DEFECTS
INTRODUCE
BAD
SECTORS
THAT
IS
SECTORS
THAT
DO
NOT
COR
RECTLY
READ
BACK
THE
VALUE
JUST
WRITTEN
TO
THEM
IF
THE
DEFECT
IS
VERY
SMALL
SAY
ONLY
A
FEW
BITS
IT
IS
POSSIBLE
TO
USE
THE
BAD
SECTOR
AND
JUST
LET
THE
ECC
CORRECT
THE
ERRORS
EVERY
TIME
IF
THE
DEFECT
IS
BIGGER
THE
ERROR
CANNOT
BE
MASKED
THERE
ARE
TWO
GENERAL
APPROACHES
TO
BAD
BLOCKS
DEAL
WITH
THEM
IN
THE
CON
TROLLER
OR
DEAL
WITH
THEM
IN
THE
OPERATING
SYSTEM
IN
THE
FORMER
APPROACH
BEFORE
THE
DISK
IS
SHIPPED
FROM
THE
FACTORY
IT
IS
TESTED
AND
A
LIST
OF
BAD
SECTORS
IS
WRITTEN
ONTO
THE
DISK
FOR
EACH
BAD
SECTOR
ONE
OF
THE
SPARES
IS
SUBSTITUTED
FOR
IT
THERE
ARE
TWO
WAYS
TO
DO
THIS
SUBSTITUTION
IN
FIG
A
WE
SEE
A
SINGLE
DISK
TRACK
WITH
DATA
SECTORS
AND
TWO
SPARES
SECTOR
IS
DEFECTIVE
WHAT
THE
CONTROLLER
CAN
DO
IS
REMAP
ONE
OF
THE
SPARES
AS
SECTOR
AS
SHOWN
IN
FIG
B
THE
OTHER
WAY
IS
TO
SHIFT
ALL
THE
SECTORS
UP
ONE
AS
SHOWN
IN
FIG
C
IN
BOTH
CASES
THE
CONTROLLER
HAS
TO
KNOW
WHICH
SECTOR
IS
WHICH
IT
CAN
KEEP
TRADE
OF
THIS
INFORMATION
THROUGH
INTERNAL
TABLES
ONE
PER
TRACK
OR
BY
REWRITING
THE
PREAMBLES
TO
GIVE
THE
REMAPPED
SECTOR
NUMBERS
IF
THE
PREAMBLES
ARE
REWRITTEN
THE
METHOD
OF
FIG
C
IS
MORE
WORK
BECAUSE
PREAMBLES
MUST
BE
REWRITTEN
BUT
ULTI
MATELY
GIVES
BETTER
PERFORMANCE
BECAUSE
AN
ENTIRE
TRACK
CAN
STILL
BE
READ
IN
ONE
ROTATION
A
B
C
FIGURE
A
A
DISK
TRACK
WITH
A
BAD
SECTOR
B
SUBSTITUTING
A
SPARE
FOR
THE
BAD
SECTOR
C
SHIFTING
ALL
THE
SECTORS
TO
BYPASS
THE
BAD
ONE
ERRORS
CAN
ALSO
DEVELOP
DURING
NORMAL
OPERATION
AFTER
THE
DRIVE
HAS
BEEN
INSTALLED
THE
FIRST
LINE
OF
DEFENSE
UPON
GETTING
AN
ERROR
THAT
THE
ECC
CANNOT
HAND
LE
IS
TO
JUST
TRY
THE
READ
AGAIN
SOME
READ
ERRORS
ARE
TRANSIENT
THAT
IS
ARE
CAUSED
BY
INPUT
OUTPUT
CHAP
SPECKS
OF
DUST
UNDER
THE
HEAD
AND
WILL
GO
AWAY
ON
A
SECOND
ATTEMPT
IF
THE
CON
TROLLER
NOTICES
THAT
IT
IS
GETTING
REPEATED
ERRORS
ON
A
CERTAIN
SECTOR
IT
CAN
SWITCH
TO
A
SPARE
BEFORE
THE
SECTOR
HAS
DIED
COMPLETELY
IN
THIS
WAY
NO
DATA
ARE
LOST
AND
THE
OPERATING
SYSTEM
AND
USER
DO
NOT
EVEN
NOTICE
THE
PROBLEM
USUALLY
THE
METHOD
OF
FIG
B
HAS
TO
BE
USED
SINCE
THE
OTHER
SECTORS
MIGHT
NOW
CONTAIN
DATA
USING
THE
METHOD
OF
FIG
C
WOULD
REQUIRE
NOT
ONLY
REWRITING
THE
PREAMBLES
BUT
COPYING
ALL
THE
DATA
AS
WELL
EARLIER
WE
SAID
THERE
WERE
TWO
GENERAL
APPROACHES
TO
HANDLING
ERRORS
HANDLE
THEM
IN
THE
CONTROLLER
OR
IN
THE
OPERATING
SYSTEM
IF
THE
CONTROLLER
DOES
NOT
HAVE
THE
CAPABILITY
TO
TRANSPARENTLY
REMAP
SECTORS
AS
WE
HAVE
DISCUSSED
THE
OPERATING
SYSTEM
MUST
DO
THE
SAME
THING
IN
SOFTWARE
THIS
MEANS
THAT
IT
MUST
FIRST
ACQUIRE
A
LIST
OF
BAD
SECTORS
EITHER
BY
READING
THEM
FROM
THE
DISK
OR
SIMPLY
TESTING
THE
EN
TIRE
DISK
ITSELF
ONCE
IT
KNOWS
WHICH
SECTORS
ARE
BAD
IT
CAN
BUILD
REMAPPING
TABLES
IF
THE
OPERATING
SYSTEM
WANTS
TO
USE
THE
APPROACH
OF
FIG
C
IT
MUST
SHIFT
THE
DATA
IN
SECTORS
THROUGH
UP
ONE
SECTOR
IF
THE
OPERATING
SYSTEM
IS
HANDLING
THE
REMAPPING
IT
MUST
MAKE
SURE
THAT
BAD
SECTORS
DO
NOT
OCCUR
IN
ANY
FILES
AND
ALSO
DO
NOT
OCCUR
IN
THE
FREE
LIST
OR
BITMAP
ONE
WAY
TO
DO
THIS
IS
TO
CREATE
A
SECRET
FDE
CONSISTING
OF
ALL
THE
BAD
SECTORS
IF
THIS
FDE
IS
NOT
ENTERED
INTO
THE
FDE
SYSTEM
USERS
WILL
NOT
ACCIDENTALLY
READ
IT
OR
WORSE
YET
FREE
IT
HOWEVER
THERE
IS
STILL
ANOTHER
PROBLEM
BACKUPS
IF
THE
DISK
IS
BACKED
UP
FDE
BY
FDE
IT
IS
IMPORTANT
THAT
THE
BACKUP
UTILITY
NOT
TRY
TO
COPY
THE
BAD
BLOCK
FDE
TO
PREVENT
THIS
THE
OPERATING
SYSTEM
HAS
TO
HIDE
THE
BAD
BLOCK
FDE
SO
WELL
THAT
EVEN
A
BACKUP
UTILITY
CANNOT
FIND
IT
IF
THE
DISK
IS
BACKED
UP
SECTOR
BY
SECTOR
RATHER
THAN
FILE
BY
FILE
IT
WILL
BE
DIFFICULT
IF
NOT
IMPOSSIBLE
TO
PREVENT
READ
ERRORS
DURING
BACKUP
THE
ONLY
HOPE
IS
THAT
THE
BACKUP
PROGRAM
HAS
ENOUGH
SMARTS
TO
GIVE
UP
AFTER
FAILED
READS
AND
CONTINUE
WITH
THE
NEXT
SECTOR
BAD
SECTORS
ARE
NOT
THE
ONLY
SOURCE
OF
ERRORS
SEEK
ERRORS
CAUSED
BY
MECHANI
CAL
PROBLEMS
IN
THE
ARM
ALSO
OCCUR
THE
CONTROLLER
KEEPS
TRACK
OF
THE
ARM
POSITION
INTERNALLY
TO
PERFORM
A
SEEK
IT
ISSUES
A
SERIES
OF
PULSES
TO
THE
ARM
MOTOR
ONE
PULSE
PER
CYLINDER
TO
MOVE
THE
ARM
TO
THE
NEW
CYLINDER
WHEN
THE
ARM
GETS
TO
ITS
DESTINATION
THE
CONTROLLER
READS
THE
ACTUAL
CYLINDER
NUMBER
FROM
THE
PREAMBLE
OF
THE
NEXT
SECTOR
IF
THE
ARM
IS
IN
THE
WRONG
PLACE
A
SEEK
ERROR
HAS
OCCURRED
MOST
HARD
DISK
CONTROLLERS
CORRECT
SEEK
ERRORS
AUTOMATICALLY
BUT
MOST
FLOPPY
CONTROLLERS
INCLUDING
THE
PENTIUM
JUST
SET
AN
ERROR
BIT
AND
LEAVE
THE
REST
TO
THE
DRIVER
THE
DRIVER
HANDLES
THIS
ERROR
BY
ISSUING
A
RECALIBRATE
COMMAND
TO
MOVE
THE
ARM
AS
FAR
OUT
AS
IT
WILL
GO
AND
RESET
THE
CONTROLLER
INTERNAL
IDEA
OF
THE
CURRENT
CYLINDER
TO
USUALLY
THIS
SOLVES
THE
PROBLEM
IF
IT
DOES
NOT
THE
DRIVE
MUST
BE
REPAIRED
AS
WE
HAVE
SEEN
THE
CONTROLLER
IS
REALLY
A
SPECIALIZED
LITTLE
COMPUTER
COM
PLETE
WITH
SOFTWARE
VARIABLES
BUFFERS
AND
OCCASIONALLY
BUGS
SOMETIMES
AN
UNUSUAL
SEQUENCE
OF
EVENTS
SUCH
AS
AN
INTERRUPT
ON
ONE
DRIVE
OCCURRING
SIMULTAN
EOUSLY
WITH
A
RECALIBRATE
COMMAND
FOR
ANOTHER
DRIVE
WILL
TRIGGER
A
BUG
AND
CAUSE
SEC
DISKS
THE
CONTROLLER
TO
GO
INTO
A
LOOP
OR
LOSE
TRACK
OF
WHAT
IT
WAS
DOING
CONTROLLER
DE
SIGNERS
USUALLY
PLAN
FOR
THE
WORST
AND
PROVIDE
A
PIN
ON
THE
CHIP
WHICH
WHEN
ASSERTED
FORCES
THE
CONTROLLER
TO
FORGET
WHATEVER
IT
WAS
DOING
AND
RESET
ITSELF
IF
ALL
ELSE
FAILS
THE
DISK
DRIVER
CAN
SET
A
BIT
TO
INVOKE
THIS
SIGNAL
AND
RESET
THE
CONTROLLER
IF
THAT
DOES
NOT
HELP
ALL
THE
DRIVER
CAN
DO
IS
PRINT
A
MESSAGE
AND
GIVE
UP
RECALIBRATING
A
DISK
MAKES
A
FUNNY
NOISE
BUT
OTHERWISE
NORMALLY
IS
NOT
DIS
TURBING
HOWEVER
THERE
IS
ONE
SITUATION
WHERE
RECALIBRATION
IS
A
SERIOUS
PROBLEM
SYSTEMS
WITH
REAL
TIME
CONSTRAINTS
WHEN
A
VIDEO
IS
BEING
PLAYED
OFF
A
HARD
DISK
OR
FILES
FROM
A
HARD
DISK
ARE
BEING
BURNED
ONTO
A
CD
ROM
IT
IS
ESSENTIAL
THAT
THE
BITS
ARRIVE
FROM
THE
HARD
DISK
AT
A
UNIFORM
RATE
UNDER
THESE
CIRCUMSTANCES
RECALI
BRATIONS
INSERT
GAPS
INTO
THE
BIT
STREAM
AND
ARE
THEREFORE
UNACCEPTABLE
SPECIAL
DRIVES
CALLED
AV
DISKS
AUDIO
VISUAL
DISKS
WHICH
NEVER
RECALIBRATE
ARE
AVAIL
ABLE
FOR
SUCH
APPLICATIONS
STABLE
STORAGE
AS
WE
HAVE
SEEN
DISKS
SOMETIMES
MAKE
ERRORS
GOOD
SECTORS
CAN
SUDDENLY
BECOME
BAD
SECTORS
WHOLE
DRIVES
CAN
DIE
UNEXPECTEDLY
RAJDS
PROTECT
AGAINST
A
FEW
SECTORS
GOING
BAD
OR
EVEN
A
DRIVE
FALLING
OUT
HOWEVER
THEY
DO
NOT
PROTECT
AGAINST
WRITE
ERRORS
LAYING
DOWN
BAD
DATA
IN
THE
FIRST
PLACE
THEY
ALSO
DO
NOT
PRO
TECT
AGAINST
CRASHES
DURING
WRITES
CORRUPTING
THE
ORIGINAL
DATA
WITHOUT
REPLACING
THEM
BY
NEWER
DATA
FOR
SOME
APPLICATIONS
IT
IS
ESSENTIAL
THAT
DATA
NEVER
BE
LOST
OR
CORRUPTED
EVEN
IN
THE
FACE
OF
DISK
AND
CPU
ERRORS
IDEALLY
A
DISK
SHOULD
SIMPLY
WORK
ALL
THE
TIME
WITH
NO
ERRORS
UNFORTUNATELY
THAT
IS
NOT
ACHIEVABLE
WHAT
IS
ACHIEVABLE
IS
A
DISK
SUBSYSTEM
THAT
HAS
THE
FOLLOWING
PROPERTY
WHEN
A
WRITE
IS
ISSUED
TO
IT
THE
DISK
EI
THER
CORRECTLY
WRITES
THE
DATA
OR
IT
DOES
NOTHING
LEAVING
THE
EXISTING
DATA
INTACT
SUCH
A
SYSTEM
IS
CALLED
STABLE
STORAGE
AND
IS
IMPLEMENTED
IN
SOFTWARE
LAMPSON
AND
STURGIS
THE
GOAL
IS
TO
KEEP
THE
DISK
CONSISTENT
AT
ALL
COSTS
BELOW
WE
WILL
DESCRIBE
A
SLIGHT
VARIANT
OF
THE
ORIGINAL
IDEA
BEFORE
DESCRIBING
THE
ALGORITHM
IT
IS
IMPORTANT
TO
HAVE
A
CLEAR
MODEL
OF
THE
POSSIBLE
ERRORS
THE
MODEL
ASSUMES
THAT
WHEN
A
DISK
WRITES
A
BLOCK
ONE
OR
MORE
SECTORS
EITHER
THE
WRITE
IS
CORRECT
OR
IT
IS
INCORRECT
AND
THIS
ERROR
CAN
BE
DETECTED
ON
A
SUBSEQUENT
READ
BY
EXAMINING
THE
VALUES
OF
THE
ECC
FIELDS
IN
PRINCIPLE
GUARANTEED
ERROR
DETECTION
IS
NEVER
POSSIBLE
BECAUSE
WITH
A
SAY
BYTE
ECC
FIELD
GUARDING
A
BYTE
SECTOR
THERE
ARE
DATA
VALUES
AND
ONLY
ECC
VALUES
THUS
IF
A
BLOCK
IS
GARBLED
DURING
WRITING
BUT
THE
ECC
IS
NOT
THERE
ARE
BIL
LIONS
UPON
BILLIONS
OF
INCORRECT
COMBINATIONS
THAT
YIELD
THE
SAME
ECC
IF
ANY
OF
THEM
OCCUR
THE
ERROR
WILL
NOT
BE
DETECTED
ON
THE
WHOLE
THE
PROBABILITY
OF
RAN
DOM
DATA
HAVING
THE
PROPER
BYTE
ECC
IS
ABOUT
WHICH
IS
SMALL
ENOUGH
THAT
WE
WILL
CALL
IT
ZERO
EVEN
THOUGH
IT
IS
REALLY
NOT
THE
MODEL
ALSO
ASSUMES
THAT
A
CORRECTLY
WRITTEN
SECTOR
CAN
SPONTANEOUSLY
GO
BAD
AND
BECOME
UNREADABLE
HOWEVER
THE
ASSUMPTION
IS
THAT
SUCH
EVENTS
ARE
SO
IWUT
OUTPUT
CHAP
RARE
THAT
HAVING
THE
SAME
SECTOR
GO
BAD
ON
A
SECOND
INDEPENDENT
DRIVE
DURING
A
REASONABLE
TIME
INTERVAL
E
G
DAY
IS
SMALL
ENOUGH
TO
IGNORE
THE
MODEL
ALSO
ASSUMES
THE
CPU
CAN
FAIL
IN
WHICH
CASE
IT
JUST
STOPS
ANY
SEC
DISKS
IN
THE
PRESENCE
OF
CPU
CRASHES
DURING
STABLE
WRITES
IT
DEPENDS
ON
PRECISELY
WHEN
THE
CRASH
OCCURS
THERE
ARE
FIVE
POSSIBILITIES
AS
DEPICTED
IN
FIG
ECC
DISK
WRITE
IN
PROGRESS
AT
THE
MOMENT
OF
FAILURE
ALSO
STOPS
LEADING
TO
INCORRECT
DATA
IN
ONE
SECTOR
AND
AN
INCORRECT
ECC
THAT
CAN
LATER
BE
DETECTED
UNDER
ALL
THESE
CON
DITIONS
STABLE
STORAGE
CAN
BE
MADE
RELIABLE
IN
THE
SENSE
OF
WRITES
EITHER
WORKING
CORRECTLY
OR
LEAVING
THE
OLD
DATA
IN
PLACE
OF
COURSE
IT
DOES
NOT
PROTECT
DISK
E
R
R
O
DISK
DISK
DISK
DISK
AGAINST
PHYSICAL
DISASTERS
SUCH
AS
AN
EARTHQUAKE
HAPPENING
AND
THE
COMPUTER
FAL
LING
METERS
INTO
A
FISSURE
AND
LANDING
IN
A
POOL
OF
BOILING
MAGMA
IT
IS
TOUGH
TO
RECOVER
FROM
THIS
CONDITION
IN
SOFTWARE
OLD
OLD
T
OLD
OLD
T
NEW
F
STABLE
STORAGE
USES
A
PAIR
OF
IDENTICAL
DISKS
WITH
THE
CORRESPONDING
BLOCKS
WORKING
TOGETHER
TO
FORM
ONE
ERROR
FREE
BLOCK
IN
THE
ABSENCE
OF
ERRORS
THE
COR
RESPONDING
BLOCKS
ON
BOTH
DRIVES
ARE
THE
SAME
EITHER
ONE
CAN
BE
READ
TO
GET
THE
CRASH
A
B
CRASH
CRASH
C
F
CRASH
CRASH
SAME
RESULT
TO
ACHIEVE
THIS
GOAL
THE
FOLLOWING
THREE
OPERATIONS
ARE
DEFINED
STABLE
WRITES
A
STABLE
WRITE
CONSISTS
OF
FIRST
WRITING
THE
BLOCK
ON
DRIVE
THEN
READING
IT
BACK
TO
VERIFY
THAT
IT
WAS
WRITTEN
CORRECTLY
IF
IT
WAS
NOT
WRITTEN
CORRECTLY
THE
WRITE
AND
REREAD
ARE
DONE
AGAIN
UP
TO
N
TIMES
UNTIL
THEY
WORK
AFTER
N
CONSECUTIVE
FAILURES
THE
BLOCK
IS
REMAPPED
ONTO
A
SPARE
AND
THE
OPERATION
REPEATED
UNTIL
IT
SUCCEEDS
NO
MATTER
HOW
MANY
SPARES
HAVE
TO
BE
TRIED
AFTER
THE
WRITE
TO
DRIVE
HAS
SUCCEEDED
THE
CORRESPONDING
BLOCK
ON
DRIVE
IS
WRITTEN
AND
REREAD
REPEATEDLY
IF
NEED
BE
UNTIL
IT
TOO
FINALLY
SUCCEEDS
IN
THE
ABSENCE
OF
CPU
CRASHES
WHEN
A
STABLE
WRITE
COMPLETES
THE
BLOCK
HAS
CORRECTLY
BEEN
WRITTEN
ONTO
BOTH
DRIVES
AND
VERIFIED
ON
BOTH
OF
THEM
STABLE
READS
A
STABLE
READ
FIRST
READS
THE
BLOCK
FROM
DRIVE
IF
THIS
YIELDS
AN
INCORRECT
ECC
THE
READ
IS
TRIED
AGAIN
UP
TO
N
TIMES
IF
ALL
OF
THESE
GIVE
BAD
ECCS
THE
CORRESPONDING
BLOCK
IS
READ
FROM
DRIVE
GIVEN
THE
FACT
THAT
A
SUCCESSFUL
STABLE
WRITE
LEAVES
TWO
GOOD
COP
IES
OF
THE
BLOCK
BEHIND
AND
OUR
ASSUMPTION
THAT
THE
PROBABILITY
OF
THE
SAME
BLOCK
SPONTANEOUSLY
GOING
BAD
ON
BOTH
DRIVES
IN
A
REASONABLE
TIME
INTERVAL
IS
NEGLIGIBLE
A
STABLE
READ
ALWAYS
SUCCEEDS
CRASH
RECOVERY
AFTER
A
CRASH
A
RECOVERY
PROGRAM
SCANS
BOTH
DISKS
COMPARING
CORRESPONDING
BLOCKS
IF
A
PAIR
OF
BLOCKS
ARE
BOTH
GOOD
AND
THE
SAME
NOTHING
IS
DONE
IF
ONE
OF
THEM
HAS
AN
ECC
ERROR
THE
BAD
BLOCK
IS
OVERWRITTEN
WITH
THE
CORRESPONDING
GOOD
BLOCK
IF
A
PAIR
OF
BLOCKS
ARE
BOTH
GOOD
BUT
DIFFERENT
THE
BLOCK
FROM
DRIVE
IS
WRIT
TEN
ONTO
DRIVE
IN
THE
ABSENCE
OF
CPLJG
ASHES
THIS
SCHEME
ALWAYS
WORKS
BECAUSE
STABLE
WRITES
ALWAYS
WRITE
TWO
VALID
COPIES
OF
EVERY
BLOCK
AND
SPONTANEOUS
ERRORS
ARE
AS
SUMED
NEVER
TO
OCCUR
ON
BOTH
CORRESPONDING
BLOCKS
AT
THE
SAME
TIME
WHAT
ABOUT
FIGURE
ANALYSIS
OF
THE
INFLUENCE
OF
CRASHES
ON
STABLE
WRITES
IN
FIG
A
THE
CPU
CRASH
HAPPENS
BEFORE
EITHER
COPY
OF
THE
BLOCK
IS
WRIT
TEN
DURING
RECOVERY
NEITHER
WILL
BE
CHANGED
AND
THE
OLD
VALUE
WILL
CONTINUE
TO
EXIST
WHICH
IS
ALLOWED
IN
FIG
B
THE
CPU
CRASHES
DURING
THE
WRITE
TO
DRIVE
DESTROYING
THE
CONTENTS
OF
THE
BLOCK
HOWEVER
THE
RECOVERY
PROGRAM
DETECTS
THIS
ERROR
AND
RE
STORES
THE
BLOCK
ON
DRIVE
FROM
DRIVE
THUS
THE
EFFECT
OF
THE
CRASH
IS
WIPED
OUT
AND
THE
OLD
STATE
IS
FULLY
RESTORED
IN
FIG
C
THE
CPU
CRASH
HAPPENS
AFTER
DRIVE
IS
WRITTEN
BUT
BEFORE
DRIVE
IS
WRITTEN
THE
POINT
OF
NO
RETURN
HAS
BEEN
PASSED
HERE
THE
RECOVERY
PRO
GRAM
COPIES
THE
BLOCK
FROM
DRIVE
TO
DRIVE
THE
WRITE
SUCCEEDS
FIG
D
IS
LIKE
FIG
B
DURING
RECOVERY
THE
GOOD
BLOCK
OVERWRITES
THE
BAD
BLOCK
AGAIN
THE
FINAL
VALUE
OF
BOTH
BLOCKS
IS
THE
NEW
ONE
FINALLY
IN
FIG
E
THE
RECOVERY
PROGRAM
SEES
THAT
BOTH
BLOCKS
ARE
THE
SAME
SO
NEITHER
IS
CHANGED
AND
THE
WRITE
SUCCEEDS
HERE
TOO
VARIOUS
OPTIMIZATIONS
AND
IMPROVEMENTS
ARE
POSSIBLE
TO
THIS
SCHEME
FOR
STAR
TERS
COMPARING
ALL
THE
BLOCKS
PAIRWISE
AFTER
A
CRASH
IS
DOABLE
BUT
EXPENSIVE
A
HUGE
IMPROVEMENT
IS
TO
KEEP
TRACK
OF
WHICH
BLOCK
WAS
BEING
WRITTEN
DURING
A
STABLE
WRITE
SO
THAT
ONLY
ONE
BLOCK
HAS
TO
BE
CHECKED
DURING
RECOVERY
SOME
COM
PUTERS
HAVE
A
SMALL
AMOUNT
OF
NONVOLATILE
RAM
WHICH
IS
A
SPECIAL
CMOS
MEM
ORY
POWERED
BY
A
LITHIUM
BATTERY
SUCH
BATTERIES
LAST
FOR
YEARS
POSSIBLY
EVEN
THE
WHOLE
LIFE
OF
THE
COMPUTER
UNLIKE
MAIN
MEMORY
WHICH
IS
LOST
AFTER
A
CRASH
NON
VOLATILE
RAM
IS
NOT
LOST
AFTER
A
CRASH
THE
TIME
OF
DAY
IS
NORMALLY
KEPT
HERE
AND
INCREMENTED
BY
A
SPECIAL
CIRCUIT
WHICH
IS
WHY
COMPUTERS
STILL
KNOW
WHAT
TIME
IT
IS
EVEN
AFTER
HAVING
BEEN
UNPLUGGED
SUPPOSE
THAT
A
FEW
BYTES
OF
NONVOLATILE
RAM
ARE
AVAILABLE
FOR
OPERATING
SYS
TEM
PURPOSES
THE
STABLE
WRITE
CAN
PUT
THE
NUMBER
OF
THE
BLOCK
IT
IS
ABOUT
TO
UP
DATE
IN
NONVOLATILE
RAM
BEFORE
STARTING
THE
WRITE
AFTER
SUCCESSFULLY
COMPLETING
IHPPUT
OUTPUT
CHAP
THE
STABLE
WRITE
THE
BLOCK
NUMBER
IN
NONVOLATILE
RAM
IS
OVERWRITTEN
WITH
AN
INVALID
BLOCK
NUMBER
FOR
EXAMPLE
UNDER
THESE
CONDITIONS
AFTER
A
CRASH
THE
RECOVERY
PROGRAM
CAN
CHECK
THE
NONVOLATILE
RAM
TO
SEE
IF
A
STABLE
WRITE
HAP
PENED
TO
BE
IN
PROGRESS
DURING
THE
CRASH
AND
IF
SO
WHICH
BLOCK
WAS
BEING
WRITTEN
WHEN
THE
CRASHED
HAPPENED
THE
TWO
COPIES
OF
THE
BLOCK
CAN
THEN
BE
CHECKED
FOR
CORRECTNESS
AND
CONSISTENCY
IF
NONVOLATILE
RAM
IS
NOT
AVAILABLE
IT
CAN
BE
SIMULATED
AS
FOLLOWS
AT
THE
START
OF
A
STABLE
WRITE
A
FIXED
DISK
BLOCK
ON
DRIVE
IS
OVERWRITTEN
WITH
THE
NUMBER
OF
THE
BLOCK
TO
BE
STABLY
WRITTEN
THIS
BLOCK
IS
THEN
READ
BACK
TO
VERIFY
IT
AFTER
GETTING
IT
CORRECT
THE
CORRESPONDING
BLOCK
ON
DRIVE
IS
WRITTEN
AND
VERIFIED
WHEN
THE
STABLE
WRITE
COMPLETES
CORRECTLY
BOTH
BLOCKS
ARE
OVERWRITTEN
WITH
AN
INVALID
BLOCK
NUMBER
AND
VERIFIED
AGAIN
HERE
AFTER
A
CRASH
IT
IS
EASY
TO
DETERMINE
WHETH
ER
OR
NOT
A
STABLE
WRITE
WAS
IN
PROGRESS
DURING
THE
CRASH
OF
COURSE
THIS
TECHNIQUE
REQUIRES
EIGHT
EXTRA
DISK
OPERATIONS
TO
WRITE
A
STABLE
BLOCK
SO
IT
SHOULD
BE
USED
EXCEEDINGLY
SPARINGLY
ONE
LAST
POINT
IS
WORTH
MAKING
WE
ASSUMED
THAT
ONLY
ONE
SPONTANEOUS
DECAY
OF
A
GOOD
BLOCK
TO
A
BAD
BLOCK
HAPPENS
PER
BLOCK
PAIR
PER
DAY
IF
ENOUGH
DAYS
GO
BY
THE
OTHER
ONE
MIGHT
GO
BAD
TOO
THEREFORE
ONCE
A
DAY
A
COMPLETE
SCAN
OF
BOTH
DISKS
MUST
BE
DONE
REPAIRING
ANY
DAMAGE
THAT
WAY
EVERY
MORNING
BOTH
DISKS
ARE
ALWAYS
IDENTICAL
EVEN
IF
BOTH
BLOCKS
IN
A
PAIR
GO
BAD
WITHIN
A
PERIOD
OF
A
FEW
DAYS
ALL
ERRORS
ARE
REPAIRED
CORRECTLY
CLOCKS
CLOCKS
ALSO
CALLED
TIMERS
ARE
ESSENTIAL
TO
THE
OPERATION
OF
ANY
MULTIPRO
GRAMMED
SYSTEM
FOR
A
VARIETY
OF
REASONS
THEY
MAINTAIN
THE
TIME
OF
DAY
AND
PRE
VENT
ONE
PROCESS
FROM
MONOPOLIZING
THE
CPU
AMONG
OTHER
THINGS
THE
CLOCK
SOFTWARE
CAN
TAKE
THE
FORM
OF
A
DEVICE
DRIVER
EVEN
THOUGH
A
CLOCK
IS
NEITHER
A
BLOCK
DEVICE
LIKE
A
DISK
NOR
A
CHARACTER
DEVICE
LIKE
A
MOUSE
OUR
EXAMINATION
OF
CLOCKS
WILL
FOLLOW
THE
SAME
PATTERN
AS
IN
THE
PREVIOUS
SECTION
FIRST
A
LOOK
AT
CLOCK
HARDWARE
AND
THEN
A
LOOK
AT
THE
CLOCK
SOFTWARE
CLOCK
HARDWAR
E
TWO
TYPES
OF
CLOCKS
ARE
COMMONLY
USED
IN
COMPUTERS
AND
BOTH
ARE
QUITE
DIF
FERENT
FROM
THE
CLOCKS
AND
WATCHES
USED
BY
PEOPLE
THE
SIMPLER
CLOCKS
ARE
TIED
TO
THE
OR
VOLT
POWER
LINE
AND
CAUSE
AN
INTERRUPT
ON
EVERY
VOLTAGE
CYCLE
AT
OR
HZ
THESE
CLOCKS
USED
TO
DOMINATE
BUT
ARE
RARE
NOWADAYS
THE
OTHER
KIND
OF
CLOCK
IS
BUILT
OUT
OF
THREE
COMPONENTS
A
CRYSTAL
OSCILLATOR
A
COUNTER
AND
A
HOLDING
REGISTER
AS
SHOWN
IN
FIG
WHEN
A
PIECE
OF
QUARTZ
CRYSTAL
IS
PROPERLY
CUT
AND
MOUNTED
UNDER
TENSION
IT
CAN
BE
MADE
TO
GENERATE
A
PERIODIC
SIGNAL
OF
VERY
GREAT
ACCURACY
TYPICALLY
IN
THE
RANGE
OF
SEVERAL
HUNDRED
SEC
CLOCKS
MEGAHERTZ
DEPENDING
ON
THE
CRYSTAL
CHOSEN
USING
ELECTRONICS
THIS
BASE
SIGNAL
CAN
BE
MULTIPLIED
BY
A
SMALL
INTEGER
TO
GET
FREQUENCIES
UP
TO
MHZ
OR
EVEN
MORE
AT
LEAST
ONE
SUCH
CIRCUIT
IS
USUALLY
FOUND
IN
ANY
COMPUTER
PROVIDING
A
SYN
CHROMZMG
SIGNAL
TO
THE
COMPUTER
VARIOUS
CIRCUITS
THIS
SIGNAL
IS
FED
INTO
THE
COUNTER
TO
MAKE
IT
COUNT
DOWN
TO
ZERO
WHEN
THE
COUNTER
GETS
TO
ZERO
IT
CAUSES
A
CPU
INTERRUPT
CRYSTAL
OSCILLATOR
IDI
J
COUNTER
IS
DECREMENTED
AT
EACH
PULSE
HOLDING
REGISTER
IS
USED
TO
LOAD
THE
COUNTER
FIGURE
A
PROGRAMMABLE
CLOCK
PROGRAMMABLE
CLOCKS
TYPICALLY
HAVE
SEVERAL
MODES
OF
OPERATION
IN
ONE
SHOT
MODE
WHEN
THE
CLOCK
IS
STARTED
IT
COPIES
THE
VALUE
OF
THE
HOLDING
REGISTER
INTO
THE
COUNTER
AND
THEN
DECREMENTS
THE
COUNTER
AT
EACH
PULSE
FROM
THE
CRYSTAL
WHEN
THE
COUNTER
GETS
TO
ZERO
IT
CAUSES
AN
INTERRUPT
AND
STOPS
UNTIL
IT
IS
EXPLICITLY
STARTED
AGAIN
BY
THE
SOFTWARE
IN
SQUARE
WAVE
MODE
AFTER
GETTING
TO
ZERO
AND
CAUSING
THE
INTERRUPT
THE
HOLDING
REGISTER
IS
AUTOMATICALLY
COPIED
INTO
THE
COUNTER
AND
THE
WHOLE
PROCESS
IS
REPEATED
AGAIN
INDEFINITELY
THESE
PERIODIC
INTERRUPTS
ARE
CALLED
CLOCK
TICKS
THE
ADVANTAGE
OF
THE
PROGRAMMABLE
CLOCK
IS
THAT
ITS
INTERRUPT
FREQUENCY
CAN
BE
CONTROLLED
BY
SOFTWARE
IF
A
MHZ
CRYSTAL
IS
USED
THEN
THE
COUNTER
IS
PULSED
EVERY
NSEC
WITH
UNSIGNED
BIT
REGISTERS
INTERRUPTS
CAN
BE
PROGRAMMED
TO
OCCUR
AT
INTERVALS
FROM
NSEC
TO
SEC
PROGRAMMABLE
CLOCK
CHIPS
USUALLY
CON
TAIN
TWO
OR
THREE
INDEPENDENTLY
PROGRAMMABLE
CLOCKS
AND
HAVE
MANY
OTHER
OPTIONS
AS
WELL
E
G
COUNTING
UP
INSTEAD
OF
DOWN
INTERRUPTS
DISABLED
AND
MORE
TO
PREVENT
THE
CURRENT
TIME
FROM
BEING
LOST
WHEN
THE
COMPUTER
POWER
IS
TURNED
OFF
MOST
COMPUTERS
HAVE
A
BATTERY
POWERED
BACKUP
CLOCK
IMPLEMENTED
WITH
THE
KIND
OF
LOW
POWER
CIRCUITRY
USED
IN
DIGITAL
WATCHES
THE
BATTERY
CLOCK
CAN
BE
READ
AT
STARTUP
IF
THE
BACKUP
CLOCK
IS
NOT
PRESENT
THE
SOFTWARE
MAY
ASK
THE
USER
FOR
THE
CURRENT
DATE
AND
TIME
THERE
IS
ALSO
A
STANDARD
WAY
FOR
A
NETWORKED
SYSTEM
TO
GET
THE
CURRENT
TIME
FROM
A
REMOTE
HOST
IN
ANY
CASE
THE
TIME
IS
THEN
TRANSLATED
INTO
THE
NUMBER
OF
CLOCK
TICKS
SINCE
A
M
UTC
UNIVERSAL
COORDI
NATED
TIME
FORMERLY
KNOWN
AS
GREENWICH
MEAN
TIME
ON
JAN
AS
UNIX
DOES
OR
SINCE
SOME
OTHER
BENCHMARK
MOMENT
THE
ORIGIN
OF
TIME
FOR
WIN
DOWS
IS
JAN
AT
EVERY
CLOCK
TICK
THE
REAL
TIME
IS
INCREMENTED
BY
ONE
INPUT
OUTPUT
CHAP
COUNT
USUALLY
UTILITY
PROGRAMS
ARE
PROVIDED
TO
MANUALLY
SET
THE
SYSTEM
CLOCK
AND
SEC
CLOCKS
THE
BACKUP
CLOCK
AND
TO
SYNCHRONIZE
THE
TWO
CLOCKS
CLOCK
SOFTWARE
BITS
TIM
E
OF
DA
Y
IN
TICKS
H
BITS
TIM
E
O
F
DA
Y
X
NUMBE
R
OF
TICKS
BITS
COUNTE
R
IN
TICKS
AIL
THE
CLOCK
HARDWARE
DOES
IS
GENERATE
INTERRUPTS
AT
KNOWN
INTERVALS
EVERY
THING
ELSE
INVOLVING
TIME
MUST
BE
DONE
BY
THE
SOFTWARE
THE
CLOCK
DRIVER
THE
EX
ACT
DUTIES
OF
THE
CLOCK
DRIVER
VARY
AMONG
OPERATING
SYSTEMS
BUT
USUALLY
INCLUDE
MOST
OF
THE
FOLLOWING
A
IN
SECOND
IN
CURRENT
SECON
D
SYSTE
M
BOOT
TIM
E
I
N
SECOND
C
MAINTAINING
THE
TIME
OF
DAY
PREVENTING
PROCESSES
FROM
RUNNING
LONGER
THAN
THEY
ARE
ALLOWED
TO
ACCOUNTING
FOR
CPU
USAGE
HANDLING
THE
ALARM
SYSTEM
CALL
MADE
BY
USER
PROCESSES
PROVIDING
WATCHDOG
TIMERS
FOR
PARTS
OF
THE
SYSTEM
ITSELF
DOING
PROFILING
MONITORING
AND
STATISTICS
GATHERING
THE
FIRST
CLOCK
FUNCTION
MAINTAINING
THE
TIME
OF
DAY
ALSO
CALLED
THE
REAL
TIME
IS
NOT
DIFFICULT
IT
JUST
REQUIRES
INCREMENTING
A
COUNTER
AT
EACH
CLOCK
TICK
AS
MENTIONED
BEFORE
THE
ONLY
THING
TO
WATCH
OUT
FOR
IS
THE
NUMBER
OF
BITS
IN
THE
TIME
OF
DAY
COUNTER
WITH
A
CLOCK
RATE
OF
HZ
A
BIT
COUNTER
WILL
OVERFLOW
IN
JUST
OVER
YEARS
CLEARLY
THE
SYSTEM
CANNOT
STORE
THE
REAL
TIME
AS
THE
NUMBER
OF
TICKS
SINCE
JAN
IN
BITS
THREE
APPROACHES
CAN
BE
TAKEN
TO
SOLVE
THIS
PROBLEM
THE
FIRST
WAY
IS
TO
USE
A
BIT
COUNTER
ALTHOUGH
DOING
SO
MAKES
MAINTAINING
THE
COUNTER
MORE
EXPENSIVE
SINCE
IT
HAS
TO
BE
DONE
MANY
TIMES
A
SECOND
THE
SECOND
WAY
IS
TO
MAINTAIN
THE
TIME
OF
DAY
IN
SECONDS
RATHER
THAN
IN
TICKS
USING
A
SUBSIDIARY
COUNTER
TO
COUNT
TICKS
UNTIL
A
WHOLE
SECOND
HAS
BEEN
ACCUMULATED
BECAUSE
SECONDS
IS
MORE
THAN
YEARS
THIS
METHOD
WILL
WORK
UNTIL
THE
TWENTY
SECOND
CENTURY
THE
THIRD
APPROACH
IS
TO
COUNT
IN
TICKS
BUT
TO
DO
THAT
RELATIVE
TO
THE
TIME
THE
SYSTEM
WAS
BOOTED
RATHER
THAN
RELATIVE
TO
A
FIXED
EXTERNAL
MOMENT
WHEN
THE
BACKUP
CLOCK
IS
READ
QJ
THE
USER
TYPES
IN
THE
REAL
TIME
THE
INTERN
BOOT
TIME
IS
CAL
CULATED
FROM
THE
CURRENT
TIME
OF
DAY
VALUE
AND
STORED
IN
MEMORY
IN
ANY
CON
VENIENT
FORM
LATER
WHEN
THE
TIME
OF
DAY
IS
REQUESTED
THE
STORED
TIME
OF
DAY
IS
ADDED
TO
THE
COUNTER
TO
GET
THE
CURRENT
TIME
OF
DAY
ALL
THREE
APPROACHES
ARE
SHOWN
IN
FIG
THE
SECOND
CLOCK
FUNCTION
IS
PREVENTING
PROCESSES
FROM
RUNNING
TOO
LONG
WHENEVER
A
PROCESS
IS
STARTED
THE
SCHEDULER
INITIALIZES
A
COUNTER
TO
THE
VALUE
OF
THAT
PROCESS
QUANTUM
IN
CLOCK
TICKS
AT
EVERY
CLOCK
INTERRUPT
THE
CLOCK
DRIVER
DECREMENTS
THE
QUANTUM
COUNTER
BY
WHEN
IT
GETS
TO
ZERO
THE
CLOCK
DRIVER
CALLS
THE
SCHEDULER
TO
SET
UP
ANOTHER
PROCESS
FIGURE
THREE
WAYS
TO
MAINTAIN
THE
TIME
OF
DAY
THE
THIRD
CLOCK
FUNCTION
IS
DOING
CPU
ACCOUNTING
THE
MOST
ACCURATE
WAY
TO
DO
IT
IS
TO
START
A
SECOND
TIMER
DISTINCT
FROM
THE
MAIN
SYSTEM
TIMER
WHENEVER
A
PROCESS
IS
STARTED
WHEN
THAT
PROCESS
IS
STOPPED
THE
TIMER
CAN
BE
READ
OUT
TO
TELL
HOW
LONG
THE
PROCESS
HAS
RUN
TO
DO
THINGS
RIGHT
THE
SECOND
TIMER
SHOULD
BE
SAVED
WHEN
AN
INTERRUPT
OCCURS
AND
RESTORED
AFTERWARD
A
LESS
ACCURATE
BUT
SIMPLER
WAY
TO
DO
ACCOUNTING
IS
TO
MAINTAIN
A
POINTER
TO
THE
PROCESS
TABLE
ENTRY
FOR
THE
CURRENTLY
RUNNING
PROCESS
IN
A
GLOBAL
VARIABLE
AT
EVERY
CLOCK
TICK
A
FIELD
IN
THE
CURRENT
PROCESS
ENTRY
IS
INCREMENTED
IN
THIS
WAY
EVERY
CLOCK
TICK
IS
CHARGED
TO
THE
PROCESS
RUNNING
AT
THE
TIME
OF
THE
TICK
A
MINOR
PROBLEM
WITH
THIS
STRATEGY
IS
THAT
IF
MANY
INTERRUPTS
OCCUR
DURING
A
PROCESS
RUN
IT
IS
STILL
CHARGED
FOR
A
FULL
TICK
EVEN
THOUGH
IT
DID
NOT
GET
MUCH
WORK
DONE
PROPERLY
ACCOUNTING
FOR
THE
CPU
DURING
INTERRUPTS
IS
TOO
EXPENSIVE
AND
IS
RARELY
DONE
IN
MANY
SYSTEMS
A
PROCESS
CAN
REQUEST
THAT
THE
OPERATING
SYSTEM
GIVE
IT
A
WARNING
AFTER
A
CERTAIN
INTERVAL
THE
WARNING
IS
USUALLY
A
SIGNAL
INTERRUPT
MES
SAGE
OR
SOMETHING
SIMILAR
ONE
APPLICATION
REQUIRING
SUCH
WARNINGS
IS
NETWORK
ING
IN
WHICH
A
PACKET
NOT
ACKNOWLEDGED
WITHIN
A
CERTAIN
TIME
INTERVAL
MUST
BE
RETRANSMITTED
ANOTHER
APPLICATION
IS
COMPUTER
AIDED
INSTRUCTION
WHERE
A
STUDENT
NOT
PROVIDING
A
RESPONSE
WITHIN
A
CERTAIN
TIME
IS
TOLD
THE
ANSWER
IF
THE
CLOCK
DRIVER
HAD
ENOUGH
CLOCKS
IT
COULD
SET
A
SEPARATE
CLOCK
FOR
EACH
RE
QUEST
THIS
NOT
BEING
THE
CASE
IT
MUST
SIMULATE
MULTIPLE
VIRTUAL
CLOCKS
WITH
A
SIN
GLE
PHYSICAL
CLOCK
ONE
WAY
IS
TO
MAINTAIN
A
TABLE
IN
WHICH
THE
SIGNAL
TIME
FOR
ALL
PENDING
TIMERS
IS
KEPT
AS
WELL
AS
A
VARIABLE
GIVING
THE
TIME
OF
THE
NEXT
ONE
WHENEVER
THE
TIME
OF
DAY
IS
UPDATED
THE
DRIVER
CHECKS
TO
SEE
IF
THE
CLOSEST
SIGNAL
HAS
OCCURRED
IF
IT
HAS
THE
TABLE
IS
SEARCHED
FOR
THE
NEXT
ONE
TO
OCCUR
IF
MANY
SIGNALS
ARE
EXPECTED
IT
IS
MORE
EFFICIENT
TO
SIMULATE
MULTIPLE
CLOCKS
BY
CHAINING
ALL
THE
PENDING
CLOCK
REQUESTS
TOGETHER
SORTED
ON
TIME
IN
A
LINKED
LIST
AS
SHOWN
IN
FIG
EACH
ENTRY
ON
THE
LIST
TELLS
HOW
MANY
CLOCK
TICKS
FOLLOWING
THE
PREVIOUS
ONE
TO
WAIT
BEFORE
CAUSING
A
SIGNAL
IN
THIS
EXAMPLE
SIGNALS
ARE
PEND
ING
FOR
AND
INPUT
OUTPUT
CHAP
CURRENT
TIME
NEXT
SIGNAL
FIGURE
SIMULATING
MULTIPLE
TIMERS
WITH
A
SINGLE
CLOCK
IN
FIG
THE
NEXT
INTERRUPT
OCCURS
IN
TICKS
ON
EACH
TICK
NEXT
SIGNAL
IS
DECREMENTED
WHEN
IT
GETS
TO
THE
SIGNAL
CORRESPONDING
TO
THE
FIRST
ITEM
ON
THE
LIST
IS
CAUSED
AND
THAT
ITEM
IS
REMOVED
FROM
THE
LIST
THEN
NEXT
SIGNAL
IS
SET
TO
THE
VALUE
IN
THE
ENTRY
NOW
AT
THE
HEAD
OF
THE
LIST
IN
THIS
EXAMPLE
NOTE
THAT
DURING
A
CLOCK
INTERRUPT
THE
CLOCK
DRIVER
HAS
SEVERAL
THINGS
TO
DO
INCREMENT
THE
REAL
TIME
DECREMENT
THE
QUANTUM
AND
CHECK
FOR
DO
CPU
ACCOUNT
ING
AND
DECREMENT
THE
ALARM
COUNTER
HOWEVER
EACH
OF
THESE
OPERATIONS
HAS
BEEN
CAREFULLY
ARRANGED
TO
BE
VERY
FAST
BECAUSE
THEY
HAVE
TO
BE
REPEATED
MANY
TIMES
A
SECOND
PARTS
OF
THE
OPERATING
SYSTEM
ALSO
NEED
TO
SET
TIMERS
THESE
ARE
CALLED
WATCH
DOG
TIMERS
FOR
EXAMPLE
FLOPPY
DISKS
DO
NOT
ROTATE
WHEN
NOT
IN
USE
TO
AVOID
WEAR
AND
TEAR
ON
THE
MEDIUM
AND
DISK
HEAD
WHEN
DATA
ARE
NEEDED
FROM
A
FLOPPY
DISK
THE
MOTOR
MUST
FIRST
BE
STARTED
ONLY
WHEN
THE
FLOPPY
DISK
IS
ROTATING
AT
FULL
SPEED
CAN
I
O
BEGIN
WHEN
A
PROCESS
ATTEMPTS
TO
READ
FROM
AN
IDLE
FLOPPY
DISK
THE
FLOPPY
DISK
DRIVER
STARTS
THE
MOTOR
AND
THEN
SETS
A
WATCHDOG
TIMER
TO
CAUSE
AN
INTERRUPT
AFTER
A
SUFFICIENTLY
LONG
TIME
INTERVAL
BECAUSE
THERE
IS
NO
UP
TO
SPEED
IN
TERRUPT
FROM
THE
FLOPPY
DISK
ITSELF
THE
MECHANISM
USED
BY
THE
CLOCK
DRIVER
TO
HANDLE
WATCHDOG
TIMERS
IS
THE
SAME
AS
FOR
USER
SIGNALS
THE
ONLY
DIFFERENCE
IS
THAT
WHEN
A
TIMER
GOES
OFF
INSTEAD
OF
CAUSING
A
SIGNAL
THE
CLOCK
DRIVER
CALLS
A
PROCEDURE
SUPPLIED
BY
THE
CALLER
THE
PROCEDURE
IS
PART
OF
THE
CALLER
CODE
THE
CALLED
PROCEDURE
CAN
DO
WHATEVER
IS
NECESSARY
EVEN
CAUSING
AN
INTERRUPT
ALTHOUGH
WITHIN
THE
KERNEL
INTERRUPTS
ARE
OFTEN
INCONVENIENT
AND
SIGNALS
DO
NOT
EXIST
THAT
IS
WHY
THE
WATCHDOG
MECHANISM
IS
PROVIDED
IT
IS
WORTH
NOTHING
THAT
THE
WATCHDOG
MECHANISM
WORKS
ONLY
WHEN
THE
CLOCK
DRIVER
AND
THE
PROCEDURE
TO
BE
CALLED
ARE
IN
THE
SAME
ADDRESS
SPACE
THE
LAST
THING
IN
OUR
LIST
IS
PROFILING
SOME
OPERATING
SYSTEMS
PROVIDE
A
ME
CHANISM
BY
WHICH
A
USER
PROGRAM
CAN
HAVE
THE
SYSTEM
BUILD
UP
A
HISTOGRAM
OF
ITS
PROGRAM
COUNTER
SO
IT
CAN
SEE
WHERE
IT
IS
SPENDING
ITS
TIME
WHEN
PROFILING
IS
A
POSSIBILITY
AT
EVERY
TICK
THE
DRIVER
CHECKS
TO
SEE
IF
THE
CURRENT
PROCESS
IS
BEING
PROFILED
AND
IF
SO
COMPUTES
THE
BIN
NUMBER
A
RANGE
OF
ADDRESSES
CORRESPONDING
TO
THE
CURRENT
PROGRAM
COUNTER
IT
THEN
JMFEMENTS
THAT
BIN
BY
ONE
THIS
MECHAN
ISM
CAN
ALSO
BE
USED
TO
PROFILE
THE
SYSTEM
ITSELF
SEC
CLOCKS
SOFT
TIMERS
MOST
COMPUTERS
HAVE
A
SECOND
PROGRAMMABLE
CLOCK
THAT
CAN
BE
SET
TO
CAUSE
TIMER
INTERRUPTS
AT
WHATEVER
RATE
A
PROGRAM
NEEDS
THIS
TIMER
IS
IN
ADDITION
TO
THE
MAIN
SYSTEM
TIMER
WHOSE
FUNCTIONS
WERE
DESCRIBED
ABOVE
AS
LONG
AS
THE
INTER
RUPT
FREQUENCY
IS
LOW
THERE
IS
NO
PROBLEM
USING
THIS
SECOND
TIMER
FOR
APPLICATION
SPECIFIC
PURPOSES
THE
TROUBLE
ARRIVES
WHEN
THE
FREQUENCY
OF
THE
APPLICATION
SPE
CIFIC
TIMER
IS
VERY
HIGH
BELOW
WE
WILL
BRIEFLY
DESCRIBE
A
SOFTWARE
BASED
TIMER
SCHEME
THAT
WORKS
WELL
UNDER
MANY
CIRCUMSTANCES
EVEN
AT
FAIRLY
HIGH
FREQUEN
CIES
THE
IDEA
IS
DUE
TO
ARON
AND
DRUSCHEL
FOR
MORE
DETAILS
PLEASE
SEE
THEIR
PAPER
GENERALLY
THERE
ARE
TWO
WAYS
TO
MANAGE
I
O
INTERRUPTS
AND
POLLING
INTER
RUPTS
HAVE
LOW
LATENCY
THAT
IS
THEY
HAPPEN
IMMEDIATELY
AFTER
THE
EVENT
ITSELF
WITH
LITTLE
OR
NO
DELAY
ON
THE
OTHER
HAND
WITH
MODERN
CPUS
INTERRUPTS
HAVE
A
SUB
STANTIAL
OVERHEAD
DUE
TO
THE
NEED
FOR
CONTEXT
SWITCHING
AND
THEIR
INFLUENCE
ON
THE
PIPELINE
TLB
AND
CACHE
THE
ALTERNATIVE
TO
INTERRUPTS
IS
TO
HAVE
THE
APPLICATION
POLL
FOR
THE
EVENT
EXPECTED
ITSELF
DOING
THIS
AVOIDS
INTERRUPTS
BUT
THERE
MAY
BE
SUBSTANTIAL
LATENCY
BECAUSE
AN
EVENT
MAY
HAPPEN
DIRECTLY
AFTER
A
POLL
IN
WHICH
CASE
IT
WAITS
ALMOST
A
WHOLE
POLLING
INTERVAL
ON
THE
AVERAGE
THE
LATENCY
IS
HALF
THE
POLLING
INTERVAL
FOR
CERTAINAPPLICATIONS
NEITHER
THE
OVERHEAD
OF
INTERRUPTS
NOR
THE
LATENCY
OF
POLLING
IS
ACCEPTABLE
CONSIDER
FOR
EXAMPLE
A
HIGH
PERFORMANCE
NETWORK
SUCH
AS
GIGABIT
ETHERNET
THIS
NETWORK
IS
CAPABLE
OF
ACCEPTING
OR
DELIVERING
A
FULL
SIZE
PACKET
EVERY
P
SEC
TO
RUN
AT
OPTIMAL
PERFORMANCE
ON
OUTPUT
ONE
PACKET
SHOULD
BE
SENT
EVERY
P
SEC
ONE
WAY
TO
ACHIEVE
THIS
RATE
IS
TO
HAVE
THE
COMPLETION
OF
A
PACKET
TRANSMIS
SION
CAUSE
AN
INTERRUPT
OR
TO
SET
THE
SECOND
TIMER
TO
INTERRUPT
EVERY
P
SEC
THE
PROBLEM
IS
THAT
THIS
INTERRUPT
HAS
BEEN
MEASURED
TO
TAKE
IXSEC
ON
A
MHZ
PENTIUM
II
ARON
AND
DRUSCHEL
THIS
OVERHEAD
IS
BARELY
BETTER
THAN
THAT
OF
COMPUTERS
IN
THE
ON
MOST
MINICOMPUTERS
FOR
EXAMPLE
AN
INTERRUPT
TOOK
FOUR
BUS
CYCLES
TO
STACK
THE
PROGRAM
COUNTER
AND
PSW
AND
TO
LOAD
A
NEW
PROGRAM
COUNTER
AND
PSW
NOWADAYS
DEALING
WITH
THE
PIPELINE
MMU
TLB
AND
CACHE
ADDS
A
GREAT
DEAL
TO
THE
OVERHEAD
THESE
EFFECTS
ARE
LIKELY
TO
GET
WORSE
RATHER
THAN
BETTER
IN
TIME
THUS
CANCELING
OUT
FASTER
CLOCK
RATES
SOFT
TIMERS
AVOID
INTERRUPTS
INSTEAD
WHENEVER
THE
KERNEL
IS
RUNNING
FOR
SOME
OTHER
REASON
JUST
BEFORE
IT
RETURNS
TO
USER
MODE
IT
CHECKS
THE
REAL
TIME
CLOCK
TO
SEE
IF
A
SOFT
TIMER
HAS
EXPIRED
IF
THE
TIMER
HAS
EXPIRED
THE
SCHEDULED
EVENT
E
G
PACKET
TRANSMISSION
OR
CHECKING
FOR
AN
INCOMING
PACKET
IS
PERFORMED
WITH
NO
NEED
TO
SWITCH
INTO
KERNEL
MODE
SINCE
THE
SYSTEM
IS
ALREADY
THERE
AFTER
THE
WORK
HAS
BEEN
PERFORMED
THE
SOFT
TIMER
IS
RESET
TO
GO
OFF
AGAIN
ALL
THAT
HAS
TO
BE
DONE
IS
COPY
THE
CURRENT
CLOCK
VALUE
TO
THE
TIMER
AND
ADD
THE
TIMEOUT
INTERVAL
TO
IT
SOFT
TIMERS
STAND
OR
FALL
WITH
THE
RATE
AT
WHICH
KERNEL
ENTRIES
ARE
MADE
FOR
OTHER
REASONS
THESE
REASONS
INCLUDE
INPUT
OUTPUT
CHAP
SYSTEM
CALLS
TLB
MISSES
PAGE
FAULTS
I
O
INTERRUPTS
THE
CPU
GOING
IDLE
TO
SEE
HOW
OFTEN
THESE
EVENTS
HAPPEN
ARON
AND
DRUSCHEL
MADE
MEASUREMENTS
WITH
SEVERAL
CPU
LOADS
INCLUDING
A
FULLY
LOADED
WEB
SERVER
A
WEB
SERVER
WITH
A
COMPUTE
BOUND
BACKGROUND
JOB
PLAYING
REAL
TIME
AUDIO
FROM
THE
INTERNET
AND
RECOMPILING
THE
UNIX
KERNEL
THE
AVERAGE
ENTRY
RATE
INTO
THE
KERNEL
VARIED
FROM
PSEC
TO
PSEC
WITH
ABOUT
HALF
OF
THESE
ENTRIES
BEING
SYSTEM
CALLS
THUS
TO
A
FIRST
ORDER
APPROXIMATION
HAVING
A
SOFT
TIMER
GO
OFF
EVERY
PSEC
IS
DOABLE
ALBEIT
WITH
AN
OCCASIONAL
MISSED
DEADLINE
FOR
APPLICATIONS
LIKE
SENDING
PACKETS
OR
POLLING
FOR
INCOMING
PACKETS
BEING
PSEC
LATE
FROM
TIME
TO
TIME
IS
BETTER
THAN
HAVING
INTERRUPTS
EAT
UP
OF
THE
CPU
OF
COURSE
THERE
WILL
BE
PERIODS
WHEN
THERE
ARE
NO
SYSTEM
CALLS
TLB
MISSES
OR
PAGE
FAULTS
IN
WHICH
CASE
NO
SOFT
TIMERS
WILL
GO
OFF
TO
PUT
AN
UPPER
BOUND
ON
THESE
INTERVALS
THE
SECOND
HARDWARE
TIMER
CAN
BE
SET
TO
GO
OFF
SAY
EVERY
MSEC
IF
THE
APPLICATION
CAN
LIVE
WITH
ONLY
PACKETS
SEC
FOR
OCCASIONAL
INTERVALS
THEN
THE
COMBINATION
OF
SOFT
TIMERS
AND
A
LOW
FREQUENCY
HARDWARE
TIMER
MAY
BE
BETTER
THAN
EITHER
PURE
INTERRUPT
DRIVEN
I
O
OR
PURE
POLLING
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
EVERY
GENERAL
PURPOSE
COMPUTER
HAS
A
KEYBOARD
AND
MONITOR
AND
USUALLY
A
MOUSE
TO
ALLOW
PEOPLE
TO
INTERACT
WITH
IT
ALTHOUGH
THE
KEYBOARD
AND
MONITOR
ARE
TECHNICALLY
SEPARATE
DEVICES
THEY
WORK
CLOSELY
TOGETHER
ON
MAINFRAMES
THERE
ARE
FREQUENTLY
MANY
REMOTE
USERS
EACH
WITH
A
DEVICE
CONTAINING
A
KEYBOARD
AND
AN
ATTACHED
DISPLAY
AS
A
UNIT
THESE
DEVICES
HAVE
HISTORICALLY
BEEN
CALLED
TERMI
NALS
PEOPLE
FREQUENTLY
STILL
USE
THAT
TERM
EVEN
WHEN
DISCUSSING
PERSONAL
COM
PUTER
KEYBOARDS
AND
MONITORS
MOSTLY
FOR
LACK
OF
A
BETTER
TERM
INPUT
SOFTWARE
USER
INPUT
COMES
PRIMARILY
FROM
THE
KEYBOARD
AND
MOUSE
SO
LET
US
LOOK
AT
THOSE
ON
A
PERSONAL
COMPUTER
THE
KEYBOARD
CONTAINS
AN
EMBEDDED
MICROPROC
ESSOR
WHICH
USUALLY
COMMUNICATES
THROUGH
A
SPECIALIZED
SERIAL
PORT
WITH
A
CON
TROLLER
CHIP
ON
THE
PARENTBOARD
ALTHOUGH
INCREASINGLY
KEYBOARDS
ARE
CONNECTED
TO
A
USB
PORT
AN
INTERRUPT
IS
GENERATED
WHENEVER
A
KEY
IS
STRUCK
AND
A
SECOND
ONE
IS
GENERATED
WHENEVER
A
KEY
IS
RELEASED
AT
EACH
OF
THESE
KEYBOARD
INTERRUPTS
THE
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
KEYBOARD
DRIVER
EXTRACTS
THE
INFORMATION
ABOUT
WHAT
HAPPENS
FROM
THE
I
O
PORT
AS
SOCIATED
WITH
THE
KEYBOARD
EVERYTHING
ELSE
HAPPENS
IN
SOFTWARE
AND
IS
PRETTY
MUCH
INDEPENDENT
OF
THE
HARDWARE
MOST
OF
THE
REST
OF
THIS
SECTION
CAN
BE
BEST
UNDERSTOOD
WHEN
THINKING
OF
TYPING
COMMANDS
TO
A
SHELL
WINDOW
COMMAND
LINE
INTERFACE
THIS
IS
HOW
PROGRAMMERS
COMMONLY
WORK
WE
WILL
DISCUSS
GRAPHICAL
INTERFACES
BELOW
KEYBOARD
SOFTWARE
THE
NUMBER
IN
THE
I
O
PORT
IS
THE
KEY
NUMBER
CALLED
THE
SCAN
CODE
NOT
THE
ASCII
CODE
KEYBOARDS
HAVE
FEWER
THAN
KEYS
SO
ONLY
BITS
ARE
NEEDED
TO
REPRESENT
THE
KEY
NUMBER
THE
EIGHTH
BIT
IS
SET
TO
ON
A
KEY
PRESS
AND
TO
ON
A
KEY
RELEASE
IT
IS
UP
TO
THE
DRIVER
TO
KEEP
TRACK
OF
THE
STATUS
OF
EACH
KEY
UP
OR
DOWN
WHEN
THE
A
KEY
IS
STRUCK
FOR
EXAMPLE
THE
SCAN
CODE
IS
PUT
IN
AN
I
O
REG
ISTER
IT
IS
UP
TO
THE
DRIVER
TO
DETERMINE
WHETHER
IT
IS
LOWER
CASE
UPPER
CASE
CTRL
A
ALT
A
CTRL
ALT
A
OR
SOME
OTHER
COMBINATION
SINCE
THE
DRIVER
CAN
TELL
WHICH
KEYS
HAVE
BEEN
STRUCK
BUT
NOT
YET
RELEASED
E
G
SHIFT
IT
HAS
ENOUGH
INFORMATION
TO
DO
THE
JOB
FOR
EXAMPLE
THE
KEY
SEQUENCE
RES
SHIFT
DEPRES
A
RELEAS
E
A
RELEAS
E
SHIF
T
INDICATES
AN
UPPER
CASE
A
HOWEVER
THE
KEY
SEQUENCE
RES
SHIFT
DEPRES
A
RELEAS
E
SHIFT
RELEAS
E
A
ALSO
INDICATES
AN
UPPER
CASE
A
ALTHOUGH
THIS
KEYBOARD
INTERFACE
PUTS
THE
FULL
BUR
DEN
ON
THE
SOFTWARE
IT
IS
EXTREMELY
FLEXIBLE
FOR
EXAMPLE
USER
PROGRAMS
MAY
BE
INTERESTED
IN
WHETHER
A
DIGIT
JUST
TYPED
CAME
FROM
THE
TOP
ROW
OF
KEYS
OR
THE
NUMERIC
KEY
PAD
ON
THE
SIDE
IN
PRINCIPLE
THE
DRIVER
CAN
PROVIDE
THIS
INFORMATION
TWO
POSSIBLE
PHILOSOPHIES
CAN
BE
ADOPTED
FOR
THE
DRIVER
IN
THE
FIRST
ONE
THE
DRIVER
JOB
IS
JUST
TO
ACCEPT
INPUT
AND
PASS
IT
UPWARD
UNMODIFIED
A
PROGRAM
READING
FROM
THE
KEYBOARD
GETS
A
RAW
SEQUENCE
OF
ASCII
CODES
GIVING
USER
PRO
GRAMS
THE
SCAN
CODES
IS
TOO
PRIMITIVE
AS
WELL
AS
BEING
HIGHLY
KEYBOARD
DEPEN
DENT
THIS
PHILOSOPHY
IS
WELL
SUITED
TO
THE
NEEDS
OF
SOPHISTICATED
SCREEN
EDITORS
SUCH
AS
EMACS
WHICH
ALLOW
THE
USER
TO
BIND
AN
ARBITRARY
ACTION
TO
ANY
CHARACTER
OR
SE
QUENCE
OF
CHARACTERS
IT
DOES
HOWEVER
MEAN
THAT
IF
THE
USER
TYPES
DSTE
INSTEAD
OF
DATE
AND
THEN
CORRECTS
THE
ERROR
BY
TYPING
THREE
BACKSPACES
AND
ATE
FOLLOWED
BY
A
CARRIAGE
RETURN
THE
USER
PROGRAM
WILL
BE
GIVEN
ALL
ASCII
CODES
TYPED
AS
FOL
LOWS
DSTE
ATEC
R
NOT
ALL
PROGRAMS
WA
FNUCH
DETAIL
OFTEN
THEY
JUST
WANT
THE
CORRECTED
INPUT
NOT
THE
EXACT
SEQUENCE
OTHOW
IT
WAS
PRODUCED
THIS
OBSERVATION
LEADS
TO
INPUT
OUTPUT
CHAF
THE
SECOND
PHILOSOPHY
THE
DRIVER
HANDLES
ALL
THE
INTRALINE
EDIDNG
AND
JUST
DELIVERS
CORRECTED
LINES
TO
THE
USER
PROGRAMS
THE
FIRST
PHILOSOPHY
IS
CHARACTER
ORIENTED
THE
SECOND
ONE
IS
LINE
ORIENTED
ORIGINALLY
THEY
WERE
REFERRED
TO
AS
RAW
MODE
AND
COOKED
MODE
RESPECTIVELY
THE
POSIX
STANDARD
USES
THE
LESS
PICTURESQUE
TERM
CANONICAL
MODE
TO
DESCRIBE
LINE
ORIENTED
MODE
NONCANONICAL
MODE
IS
EQUIVA
LENT
TO
RAW
MODE
ALTHOUGH
MANY
DETAILS
OF
THE
BEHAVIOR
CAN
BE
CHANGED
POSIX
COMPATIBLE
SYSTEMS
PROVIDE
SEVERAL
LIBRARY
FUNCTIONS
THAT
SUPPORT
SELECT
ING
EITHER
MODE
AND
CHANGING
MANY
PARAMETERS
IF
THE
KEYBOARD
IS
IN
CANONICAL
COOKED
MODE
CHARACTERS
MUST
BE
STORED
UNTIL
AN
ENURE
LINE
HAS
BEEN
ACCUMULATED
BECAUSE
THE
USER
MAY
SUBSEQUENTLY
DECIDE
TO
ERASE
PART
OF
IT
EVEN
IF
THE
KEYBOARD
IS
IN
RAW
MODE
THE
PROGRAM
MAY
NOT
YET
HAVE
REQUESTED
INPUT
SO
THE
CHARACTERS
MUST
BE
BUFFERED
TO
ALLOW
TYPE
AHEAD
EI
THER
A
DEDICATED
BUFFER
CAN
BE
USED
OR
BUFFERS
CAN
BE
ALLOCATED
FROM
A
POOL
THE
FORMER
PUTS
A
FIXED
LIMIT
ON
TYPE
AHEAD
THE
LATTER
DOES
NOT
THIS
ISSUE
ARISES
MOST
ACUTELY
WHEN
THE
USER
IS
TYPING
TO
A
SHELL
WINDOW
COMMAND
LINE
WINDOW
IN
WIN
DOWS
AND
HAS
JUST
ISSUED
A
COMMAND
SUCH
AS
A
COMPILATION
THAT
HAS
NOT
YET
COMPLETED
SUBSEQUENT
CHARACTERS
TYPED
HAVE
TO
BE
BUFFERED
BECAUSE
THE
SHELL
IS
NOT
READY
TO
READ
THEM
SYSTEM
DESIGNERS
WHO
DO
NOT
PERMIT
USERS
TO
TYPE
FAR
AHEAD
OUGHT
TO
BE
TARRED
AND
FEATHERED
OR
WORSE
YET
BE
FORCED
TO
USE
THEIR
OWN
SYSTEM
ALTHOUGH
THE
KEYBOARD
AND
MONITOR
ARE
LOGICALLY
SEPARATE
DEVICES
MANY
USERS
HAVE
GROWN
ACCUSTOMED
TO
SEEING
THE
CHARACTERS
THEY
HAVE
JUST
TYPED
APPEAR
ON
THE
SCREEN
THIS
PROCESS
IS
CALLED
ECHOING
ECHOING
IS
COMPLICATED
BY
THE
FACT
THAT
A
PROGRAM
MAY
BE
WRITING
TO
THE
SCREEN
WHILE
THE
USER
IS
TYPING
AGAIN
THINK
ABOUT
TYPING
TO
A
SHELL
WINDOW
AT
THE
VERY
LEAST
THE
KEYBOARD
DRIVER
HAS
TO
FIGURE
OUT
WHERE
TO
PUT
THE
NEW
INPUT
WITHOUT
IT
BEING
OVERWRITTEN
BY
PROGRAM
OUTPUT
ECHOING
ALSO
GETS
COMPLICATED
WHEN
MORE
THAN
CHARACTERS
HAVE
TO
BE
DIS
PLAYED
IN
A
WINDOW
WITH
CHARACTER
LINES
OR
SOME
OTHER
NUMBER
DEPENDING
ON
THE
APPLICATION
WRAPPING
AROUND
TO
THE
NEXT
LINE
MAY
BE
APPROPRIATE
SOME
DRIVERS
JUST
TRUNCATE
LINES
TO
CHARACTERS
BY
THROWING
AWAY
ALL
CHARACTERS
BEYOND
COLUMN
ANOTHER
PROBLEM
IS
TAB
HANDLING
IT
IS
USUALLY
UP
TO
THE
DRIVER
TO
COMPUTE
WHERE
THE
CURSOR
IS
CURRENTLY
LOCATED
TAKING
INTO
ACCOUNT
BOTH
OUTPUT
FROM
PRO
GRAMS
AND
OUTPUT
FROM
ECHOING
AND
COMPUTE
THE
PROPER
NUMBER
OF
SPACES
TO
BE
ECHOED
NOW
WE
COME
TO
THE
PROBLEM
OF
DEVICE
EQUIVALENCE
LOGICALLY
AT
THE
END
OF
A
LINE
OF
TEXT
ONE
WANTS
A
CARRIAGE
RETURN
TO
MOVE
THE
CURSOR
BACK
TO
COLUMN
AND
A
LINEFEED
TO
ADVANCE
TO
THE
NEXT
LINE
REQUIRING
USERS
TO
TYPE
BOTH
AT
THE
END
OF
EACH
LINE
WOULD
NOT
SELL
WELL
IT
IS
UP
TO
THE
DEVICE
DRIVER
TO
CONVERT
WHATEVER
COMES
IN
TO
THE
FORMAT
USED
BY
THE
OPERATING
SYSTEM
IN
UNIX
THE
ENTER
KEY
IS
CONVERTED
TO
A
LINE
FEED
FOR
INTERNAL
STORAGE
IN
WINDOWS
IT
IS
CONVERTED
TO
A
CAR
RIAGE
RETURN
FOLLOWED
BY
A
LINE
FEED
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
IF
THE
STANDARD
FORM
IS
JUST
TO
STORE
A
LINEFEED
THE
UNIX
CONVENTION
THEN
CAR
RIAGE
RETURNS
CREATED
BY
THE
ENTER
KEY
SHOULD
BE
TURNED
INTO
LINEFEEDS
IF
THE
IN
TERNAL
FORMAT
IS
TO
STORE
BOTH
THE
WINDOWS
CONVENTION
THEN
THE
DRIVER
SHOULD
GENERATE
A
LINEFEED
WHEN
IT
GETS
A
CARRIAGE
RETURN
AND
A
CARRIAGE
RETURN
WHEN
IT
GETS
A
LINEFEED
NO
MATTER
WHAT
THE
INTERNAL
CONVENTION
THE
MONITOR
MAY
REQUIRE
BOTH
A
LINEFEED
AND
A
CARRIAGE
RETURN
TO
BE
ECHOED
IN
ORDER
TO
GET
THE
SCREEN
UPDATED
PROPERLY
ON
MULTIUSER
SYSTEMS
SUCH
AS
MAINFRAMES
DIFFERENT
USERS
MAY
HAVE
DIFFERENT
TYPES
OF
TERMINALS
CONNECTED
TO
IT
AND
IT
IS
UP
TO
THE
KEYBOARD
DRIVER
TO
GET
ALL
THE
DIFFERENT
CARRIAGE
RETURN
LINEFEED
COMBINATIONS
CONVERTED
TO
THE
IN
TERNAL
SYSTEM
STANDARD
AND
ARRANGE
FOR
ALL
ECHOING
TO
BE
DONE
RIGHT
WHEN
OPERATING
IN
CANONICAL
MODE
SOME
INPUT
CHARACTERS
HAVE
SPECIAL
MEAN
INGS
FIGURE
SHOWS
ALL
OF
THE
SPECIAL
CHARACTERS
REQUIRED
BY
POSLX
THE
DE
FAULTS
ARE
ALL
CONTROL
CHARACTERS
THAT
SHOULD
NOT
CONFLICT
WITH
TEXT
INPUT
OR
CODES
USED
BY
PROGRAMS
ALL
EXCEPT
THE
LAST
TWO
CAN
BE
CHANGED
UNDER
PROGRAM
CONTROL
CHARACTER
POSIX
NAM
E
COMMENT
CTRL
H
ERASE
BACKSPACE
ON
E
CHARACTER
CTRL
U
KILL
ERASE
ENTIRE
LINE
BEING
TYPED
CTRL
V
LNEXT
INTERPRET
NEXT
CHARACTER
LITERALLY
CTRL
STOP
STOP
OUTPUT
CTRL
Q
START
START
OUTPUT
DEL
INTR
INTERRUPT
PROCESS
SIGINT
CTRL
X
QUIT
FORCE
CORE
DUMP
SIGQUIT
CTRL
D
EOF
END
OF
FILE
CTRL
M
CR
CARRIAGE
RETURN
UNCHANGEABLE
CTRL
J
NL
LINEFEED
UNCHANGEABLE
FIGURE
CHARACTERS
THAT
ARE
HANDLED
SPECIALLY
IN
CANONICAL
MODE
THE
ERASE
CHARACTER
ALLOWS
THE
USER
TO
RUB
OUT
THE
CHARACTER
JUST
TYPED
IT
IS
USUALLY
THE
BACKSPACE
CTRL
H
IT
IS
NOT
ADDED
TO
THE
CHARACTER
QUEUE
BUT
IN
STEAD
REMOVES
THE
PREVIOUS
CHARACTER
FROM
THE
QUEUE
IT
SHOULD
BE
ECHOED
AS
A
SE
QUENCE
OF
THREE
CHARACTERS
BACKSPACE
SPACE
AND
BACKSPACE
IN
ORDER
TO
REMOVE
THE
PREVIOUS
CHARACTER
FROM
THE
SCREEN
IF
THE
PREVIOUS
CHARACTER
WAS
A
TAB
ERAS
ING
IT
DEPENDS
ON
HOW
IT
WAS
PROCESSED
WHEN
IT
WAS
TYPED
IF
IT
IS
IMMEDIATELY
EX
PANDED
INTO
SPACES
SOME
EXTRA
INFORMATION
IS
NEEDED
TO
DETERMINE
HOW
FAR
TO
BACK
UP
IF
THE
TAB
ITSELF
IS
STORED
IN
THE
INPUT
QUEUE
IT
CAN
BE
REMOVED
AND
THE
ENTIRE
LINE
JUST
OUTPUT
AGAIN
IN
MOST
SYSTEMS
BACKSPACING
WILL
ONLY
ERASE
CHARAC
TERS
ON
THE
CURRENT
LINE
IT
WILL
NOT
ERASE
A
CARRIAGE
RETURN
AND
BACK
UP
INTO
THE
PREVIOUS
LINE
WHEN
THE
USER
NOTICES
AN
ERROR
AT
THE
START
OF
THE
LINE
BEING
TYPED
IN
IT
IS
OFTEN
CONVENIENT
TO
ERASE
THE
ENTIRE
LINE
AND
START
AGAIN
THE
KILL
CHARACTER
ERASES
THE
INPUT
OUTPUT
CHAP
ENTIRE
LINE
MOST
SYSTEMS
MAKE
THE
ERASED
LINE
VANISH
FROM
THE
SCREEN
BUT
A
FEW
OLDER
ONES
ECHO
IT
PLUS
A
CARRIAGE
RETURN
AND
LINEFEED
BECAUSE
SOME
USERS
LIKE
TO
SEE
THE
OLD
LINE
CONSEQUENTLY
HOW
TO
ECHO
KILL
IS
A
MATTER
OF
TASTE
AS
WITH
ERASE
IT
IS
USUALLY
NOT
POSSIBLE
TO
GO
FURTHER
BACK
THAN
THE
CURRENT
LINE
WHEN
A
BLOCK
OF
CHARACTERS
IS
KILLED
IT
MAY
OR
MAY
NOT
BE
WORTH
THE
TROUBLE
FOR
THE
DRIVER
TO
RETURN
BUFFERS
TO
THE
POOL
IF
ONE
IS
USED
SOMETIMES
THE
ERASE
OR
KILL
CHARACTERS
MUST
BE
ENTERED
AS
ORDINARY
DATA
THE
LNEXT
CHARACTER
SERVES
AS
AN
ESCAPE
CHARACTER
IN
UNIX
CTRL
V
IS
THE
DEFAULT
AS
AN
EXAMPLE
OLDER
UNIX
SYSTEMS
OFTEN
USED
THE
SIGN
FOR
KILL
BUT
THE
INTERNET
MAIL
SYSTEM
USES
ADDRESSES
OF
THE
FORM
LINDA
CS
WASHINGTON
EDU
SOMEONE
WHO
FEELS
MORE
COMFORTABLE
WITH
OLDER
CONVENTIONS
MIGHT
REDEFINE
KILL
AS
BUT
THEN
NEED
TO
ENTER
AN
SIGN
LITERALLY
TO
ADDRESS
E
MAIL
THIS
CAN
BE
DONE
BY
TYPING
CTRL
V
THE
CTRL
V
ITSELF
CAN
BE
ENTERED
LITERALLY
BY
TYP
ING
CTRL
V
CTRL
V
AFTER
SEEING
A
CTRL
V
THE
DRIVER
SETS
A
FLAG
SAYING
THAT
THE
NEXT
CHARACTER
IS
EXEMPT
FROM
SPECIAL
PROCESSING
THE
LNEXT
CHARACTER
ITSELF
IS
NOT
ENTERED
IN
THE
CHARACTER
QUEUE
TO
ALLOW
USERS
TO
STOP
A
SCREEN
IMAGE
FROM
SCROLLING
OUT
OF
VIEW
CONTROL
CODES
ARE
PROVIDED
TO
FREEZE
THE
SCREEN
AND
RESTART
IT
LATER
IN
UNIX
THESE
ARE
STOP
CTRL
AND
START
CTRL
Q
RESPECTIVELY
THEY
ARE
NOT
STORED
BUT
ARE
USED
TO
SET
AND
CLEAR
A
FLAG
IN
THE
KEYBOARD
DATA
STRUCTURE
WHENEVER
OUTPUT
IS
ATTEMPTED
THE
FLAG
IS
INSPECTED
IF
IT
IS
SET
NO
OUTPUT
OCCURS
USUALLY
ECHOING
IS
ALSO
SUPPRESSED
ALONG
WITH
PROGRAM
OUTPUT
IT
IS
OFTEN
NECESSARY
TO
KILL
A
RUNAWAY
PROGRAM
BEING
DEBUGGED
THE
INTR
DEL
AND
QUIT
CTRL
CHARACTERS
CAN
BE
USED
FOR
THIS
PURPOSE
IN
UNIX
DEL
SENDS
THE
SIGINT
SIGNAL
TO
ALL
THE
PROCESSES
STARTED
UP
FROM
THAT
KEYBOARD
IMPLEMENTING
DEL
CAN
BE
QUITE
TRICKY
BECAUSE
UNIX
WAS
DESIGNED
FROM
THE
BEGINNING
TO
HANDLE
MULTIPLE
USERS
AT
THE
SAME
TIME
THUS
IN
THE
GENERAL
CASE
THERE
MAY
BE
MANY
PROCESSES
RUNNING
ON
BEHALF
OF
MANY
USERS
AND
THE
DEL
KEY
MUST
ONLY
SIGNAL
THE
USER
OWN
PROCESSES
THE
HARD
PART
IS
GETTING
THE
INFORMATION
FROM
THE
DRIVER
TO
THE
PART
OF
THE
SYSTEM
THAT
HANDLES
SIGNALS
WHICH
AFTER
ALL
HAS
NOT
ASKED
FOR
THIS
INFORMATION
CTRLA
IS
SIMILAR
TO
DEL
EXCEPT
THAT
IT
SENDS
THE
SIGQUIT
SIGNAL
WHICH
FORCES
A
CORE
DUMP
IF
NOT
CAUGHT
OR
IGNORED
WHEN
EITHER
OF
THESE
KEYS
IS
STRUCK
THE
DRWER
SHOULD
ECHO
A
CARRIAGE
RETURN
AND
LINEFEED
AND
DISCARD
ALL
ACCUMULATED
INPUT
TO
ALLOW
FOR
A
FRESH
START
THE
DEFAULT
VALUE
FOR
INTR
IS
OFTEN
CTRL
C
IN
STEAD
OF
DEL
SINCE
MANY
PROGRAMS
USE
DEL
INTERCHANGEABLY
WITH
THE
BACKSPACE
FOR
EDITING
ANOTHER
SPECIAL
CHARACTER
IS
EOF
CTRL
D
WHICH
IN
UNIX
CAUSES
ANY
PENDING
READ
REQUESTS
FOR
THE
TERMINAL
TO
BE
SATISFIED
WITH
WHATEVER
IS
AVAILABLE
IN
THE
BUFFER
EVEN
IF
THE
BUFFER
IS
EMPTY
TYPING
CTRL
D
AT
THE
START
OF
A
LINE
CAUSES
THE
PROGRAM
TO
GET
A
READ
OF
BYTES
WHICH
IS
CONVENTIONALLY
INTERPRETED
AS
END
OF
FILE
AND
CAUSES
MOST
PROGRAMS
TO
ACT
THE
SAME
WAY
AS
THEY
WOULD
UPON
SEEING
END
OF
FILE
ON
AN
INPUT
FILE
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
MOUSE
SOFTWARE
MOST
PCS
HAVE
A
MOUSE
OR
SOMETIMES
A
TRACKBALL
WHICH
IS
JUST
A
MOUSE
LYING
ON
ITS
BACK
ONE
COMMON
TYPE
OF
MOUSE
HAS
A
RUBBER
BALL
INSIDE
THAT
PROTRUDES
THROUGH
A
HOLE
IN
THE
BOTTOM
AND
ROTATES
AS
THE
MOUSE
IS
MOVED
OVER
A
ROUGH
SUR
FACE
AS
THE
BALL
ROTATES
IT
RUBS
AGAINST
RUBBER
ROLLERS
PLACED
ON
ORTHOGONAL
SHAFTS
MOTION
IN
THE
EAST
WEST
DIRECTION
CAUSES
THE
SHAFT
PARALLEL
TO
THE
Y
AXIS
TO
ROTATE
MOTION
IN
THE
NORTH
SOUTH
DIRECTION
CAUSES
THE
SHAFT
PARALLEL
TO
THE
X
AXIS
TO
ROTATE
ANOTHER
POPULAR
MOUSE
TYPE
IS
THE
OPTICAL
MOUSE
WHICH
IS
EQUIPPED
WITH
ONE
OR
MORE
LIGHT
EMITTING
DIODES
AND
PHOTODETECTORS
ON
THE
BOTTOM
EARLY
ONES
HAD
TO
OPERATE
ON
A
SPECIAL
MOUSEPAD
WITH
A
RECTANGULAR
GRID
ETCHED
ONTO
IT
SO
THE
MOUSE
COULD
COUNT
LINES
CROSSED
MODERN
OPTICAL
MICE
HAVE
AN
IMAGE
PROCESSING
CHIP
IN
THEM
AND
MAKE
CONTINUOUS
LOW
RESOLUTION
PHOTOS
OF
THE
SURFACE
UNDER
THEM
LOOK
ING
FOR
CHANGES
FROM
IMAGE
TO
IMAGE
WHENEVER
A
MOUSE
HAS
MOVED
A
CERTAIN
MINIMUM
DISTANCE
IN
EITHER
DIRECTION
OR
A
BUTTON
IS
DEPRESSED
OR
RELEASED
A
MESSAGE
IS
SENT
TO
THE
COMPUTER
THE
MINIMUM
DISTANCE
IS
ABOUT
MM
ALTHOUGH
IT
CAN
BE
SET
IN
SOFTWARE
SOME
PEOPLE
CALL
THIS
UNIT
A
MICKEY
MICE
OR
OCCASIONALLY
MOUSES
CAN
HAVE
ONE
TWO
OR
THREE
BUTTONS
DEPENDING
ON
THE
DESIGNERS
ESTIMATE
OF
THE
USERS
INTELLECTUAL
ABILITY
TO
KEEP
TRACK
OF
MORE
THAN
ONE
BUTTON
SOME
MICE
HAVE
WHEELS
THAT
CAN
SEND
ADDITIONAL
DATA
BACK
TO
THE
COMPUTER
WIRELESS
MICE
ARE
THE
SAME
AS
WIRED
MICE
EXCEPT
INSTEAD
OF
SENDING
THEIR
DATA
BACK
TO
THE
COMPUTER
OVER
A
WIRE
THEY
USE
LOW
POWER
RADIOS
FOR
EXAMPLE
USING
THE
BLUETOOTH
STANDARD
THE
MESSAGE
TO
THE
COMPUTER
CONTAINS
THREE
ITEMS
AX
AY
BUTTONS
THE
FIRST
ITEM
IS
THE
CHANGE
IN
X
POSITION
SINCE
THE
LAST
MESSAGE
THEN
COMES
THE
CHANGE
IN
Y
POSITION
SINCE
THE
LAST
MESSAGE
FINALLY
THE
STATUS
OF
THE
BUTTONS
IS
INCLUDED
THE
FORMAT
OF
THE
MESSAGE
DEPENDS
ON
THE
SYSTEM
AND
THE
NUMBER
OF
BUTTONS
THE
MOUSE
HAS
USUALLY
IT
TAKES
BYTES
MOST
MICE
REPORT
BACK
A
MAXIMUM
OF
TIMES
SEC
SO
THE
MOUSE
MAY
HAVE
MOVED
MULTIPLE
MICKEYS
SINCE
THE
LAST
REPORT
NOTE
THAT
THE
MOUSE
ONLY
INDICATES
CHANGES
IN
POSITION
NOT
ABSOLUTE
POSITION
ITSELF
IF
THE
MOUSE
IS
PICKED
UP
AND
PUT
DOWN
GENTLY
WITHOUT
CAUSING
THE
BALL
TO
ROTATE
NO
MESSAGES
WILL
BE
SENT
SOME
GUIS
DISTINGUISH
BETWEEN
SINGLE
CLICKS
AND
DOUBLE
CLICKS
OF
A
MOUSE
BUTTON
IF
TWO
CLICKS
ARE
CLOSE
ENOUGH
IN
SPACE
MICKEYS
AND
ALSO
CLOSE
ENOUGH
IN
TIME
MILLISECONDS
A
DOUBLE
CLICK
IS
SIGNALED
THE
MAXIMUM
FOR
CLOSE
ENOUGH
IS
UP
TO
THE
SOFTWARE
WITH
BOTH
PARAMETERS
USUALLY
BEING
USER
SETTABLE
OUTPUT
SOFTWARE
NOW
LET
US
CONSIDER
OUTPUT
SOFTWARE
FIRST
WE
WILL
LOOK
AT
SIMPLE
OUTPUT
TO
A
TEXT
WINDOW
WHICH
IS
WHAT
PROGRAMMERS
NORMALLY
PREFER
TO
USE
THEN
WE
WILL
CONSIDER
GRAPHICAL
USER
INTERFACES
WHICH
OTHER
USERS
OFTEN
PREFER
INPUT
OUTPUT
CHAP
TEXT
WINDOWS
OUTPUT
IS
SIMPLER
THAN
INPUT
WHEN
THE
OUTPUT
IS
SEQUENTIALLY
IN
A
SINGLE
FONT
SIZE
AND
COLOR
FOR
THE
MOST
PART
THE
PROGRAM
SENDS
CHARACTERS
TO
THE
CURRENT
WIN
DOW
AND
THEY
ARE
DISPLAYED
THERE
USUALLY
A
BLOCK
OF
CHARACTERS
FOR
EXAMPLE
A
LINE
IS
WRITTEN
IN
ONE
SYSTEM
CALL
SCREEN
EDITORS
AND
MANY
OTHER
SOPHISTICATED
PROGRAMS
NEED
TO
BE
ABLE
TO
UPDATE
THE
SCREEN
IN
COMPLEX
WAYS
SUCH
AS
REPLACING
ONE
LINE
IN
THE
MIDDLE
OF
THE
SCREEN
TO
ACCOMMODATE
THIS
NEED
MOST
OUTPUT
DRIVERS
SUPPORT
A
SERIES
OF
COM
MANDS
TO
MOVE
THE
CURSOR
INSERT
AND
DELETE
CHARACTERS
OR
LINES
AT
THE
CURSOR
AND
SO
ON
THESE
COMMANDS
ARE
OFTEN
CALLED
ESCAPE
SEQUENCES
IN
THE
HEYDAY
OF
THE
DUMB
IMES
ASCII
TERMINAL
THERE
WERE
HUNDREDS
OF
TERMINAL
TYPES
EACH
WITH
ITS
OWN
ESCAPE
SEQUENCES
AS
A
CONSEQUENCE
IT
WAS
DIFFICULT
TO
WRITE
SOFT
WARE
THAT
WORKED
ON
MORE
THAN
ONE
TERMINAL
TYPE
ONE
SOLUTION
WHICH
WAS
INTRODUCED
IN
BERKELEY
UNIX
WAS
A
TERMINAL
DATA
BASE
CALLED
TERMCAP
THIS
SOFTWARE
PACKAGE
DEFINED
A
NUMBER
OF
BASIC
ACTIONS
SUCH
AS
MOVING
THE
CURSOR
TO
ROW
COLUMN
TO
MOVE
THE
CURSOR
TO
A
PARTICULAR
LOCATION
THE
SOFTWARE
SAY
AN
EDITOR
USED
A
GENERIC
ESCAPE
SEQUENCE
WHICH
WAS
THEN
CONVERTED
TO
THE
ACTUAL
ESCAPE
SEQUENCE
FOR
THE
TERMINAL
BEING
WRITTEN
TO
IN
THIS
WAY
THE
EDITOR
WORKED
ON
ANY
TERMINAL
THAT
HAD
AN
ENTRY
IN
THE
TERMCAP
DATA
BASE
MUCH
UNIX
SOFTWARE
STILL
WORKS
THIS
WAY
EVEN
ON
PERSONAL
COMPUTERS
EVENTUALLY
THE
INDUSTRY
SAW
THE
NEED
FOR
STANDARDIZATION
OF
THE
ESCAPE
SE
QUENCE
SO
AN
ANSI
STANDARD
WAS
DEVELOPED
A
FEW
OF
THE
VALUES
ARE
SHOWN
IN
FIG
CONSIDER
HOW
THESE
ESCAPE
SEQUENCES
MIGHT
BE
USED
BY
A
TEXT
EDITOR
SUPPOSE
THAT
THE
USER
TYPES
A
COMMAND
TELLING
THE
EDITOR
TO
DELETE
ALL
OF
LINE
AND
THEN
CLOSE
UP
THE
GAP
BETWEEN
LINES
AND
THE
EDITOR
MIGHT
SEND
THE
FOLLOWING
ESCAPE
SEQUENCE
OVER
THE
SERIAL
LINE
TO
THE
TERMINAL
ESC
H
ESC
K
ESC
M
WHERE
THE
SPACES
ARE
USED
ABOVE
ONLY
TO
SEPARATE
THE
SYMBOLS
THEY
ARE
NOT
TRANS
MITTED
THIS
SEQUENCE
MOVES
THE
CURSOR
TO
THE
START
OF
LINE
ERASES
THE
ENTIRE
LINE
AND
THEN
DELETES
THE
NOW
EMPTY
LINE
CAUSING
ALL
THE
LINES
STARTING
AT
TO
MOVE
UP
ONE
LINE
THEN
WHAT
WAS
LINE
BECOMES
LINE
WHAT
WAS
LINE
BECOMES
LINE
AND
SO
ON
ANALOGOUS
ESCAPE
SEQUENCES
CAN
BE
USED
TO
ADD
TEXT
TO
THE
MID
DLE
OF
THE
DISPLAY
WORDS
AND
BE
ADDED
OR
REMOVED
IN
A
SIMILAR
WAY
THE
X
WINDOW
SYSTEM
NEARLY
ALL
UNIX
SYSTEMS
BASE
THEIR
USER
INTERFACE
ON
THE
X
WINDOW
SYSTEM
OFTEN
JUST
CALLED
X
DEVELOPED
AT
M
I
T
AS
PART
OF
PROJECT
ATHENA
IN
THE
IT
IS
VERY
PORTABLE
AND
RUNS
ENTIRELY
IN
USER
SPACE
IT
WAS
ORIGINALLY
INTENDED
FOR
CONNECTING
A
LARGE
NUMBER
OF
REMOTE
USER
TERMINALS
WITH
A
CENTRAL
COMPUTE
SERVER
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
ESCAP
E
SEQUENC
E
MEANING
ESC
N
A
MOVE
UP
N
LINES
ESCFN
B
MOVE
DOWN
N
LINES
SC
N
C
MOVE
RIGHT
N
SPACE
ESC
N
D
MOVE
LEFT
N
SPACE
ESC
M
O
H
MOVE
CURSOR
TO
M
N
ESCFS
J
CLEAR
SCREEN
FROM
CURSOR
TO
END
START
ALL
ESC
K
CLEAR
LINE
FROM
CURSOR
TO
END
FROM
START
ALL
ESC
N
L
INSERT
N
LINES
AT
CURSOR
ESCFN
M
DELETE
N
LINES
AT
CURSOR
ESC
N
P
DELETE
N
CHARS
AT
CURSOR
ESC
N
INSERT
N
CHARS
AT
CURSOR
ESC
N
M
ENABLE
RENDITION
N
NORMAL
BOLD
BLINKING
REVERSE
ESC
M
SCROLL
THE
SCREEN
BACKWARD
IF
THE
CURSOR
IS
ON
THE
TOP
LINE
FIGURE
THE
ANSI
ESCAPE
SEQUENCES
ACCEPTED
BY
THE
TERMINAL
DRIVER
ON
OUTPUT
ESC
DENOTES
THE
ASCII
ESCAPE
CHARACTER
OXIB
AND
N
M
AND
ARE
OP
TIONAL
NUMERIC
PARAMETERS
SO
IT
IS
LOGICALLY
SPLIT
INTO
CLIENT
SOFTWARE
AND
HOST
SOFTWARE
WHICH
CAN
POTENTIALLY
RUN
ON
DIFFERENT
COMPUTERS
ON
MODERN
PERSONAL
COMPUTERS
BOTH
PARTS
CAN
RUN
ON
THE
SAME
MACHINE
ON
LINUX
SYSTEMS
THE
POPULAR
GNOME
AND
KDE
DESKTOP
ENVI
RONMENTS
RUN
ON
TOP
OF
X
WHEN
X
IS
RUNNING
ON
A
MACHINE
THE
SOFTWARE
THAT
COLLECTS
INPUT
FROM
THE
KEYBOARD
AND
MOUSE
AND
WRITES
OUTPUT
TO
THE
SCREEN
IS
CALLED
THE
X
SERVER
IT
HAS
TO
KEEP
TRACK
OF
WHICH
WINDOW
IS
CURRENTLY
SELECTED
WHERE
THE
MOUSE
POINTER
IS
SO
IT
KNOWS
WHICH
CLIENT
TO
SEND
ANY
NEW
KEYBOARD
INPUT
TO
IT
COMMUNICATES
WITH
RUNNING
PROGRAMS
POSSIBLE
OVER
A
NETWORK
CALLED
X
CLIENTS
IT
SENDS
THEM
KEYBOARD
AND
MOUSE
INPUT
AND
ACCEPTS
DISPLAY
COMMANDS
FROM
THEM
IT
MAY
SEEM
ODD
THAT
THE
X
SERVER
IS
ALWAYS
INSIDE
THE
USER
COMPUTER
WHILE
THE
X
CLIENT
MAY
BE
OFF
ON
A
REMOTE
COMPUTE
SERVER
BUT
JUST
THINK
OF
THE
X
SER
VER
MAIN
JOB
DISPLAYING
BITS
ON
THE
SCREEN
SO
IT
MAKES
SENSE
TO
BE
NEAR
THE
USER
FROM
THE
PROGRAM
POINT
OF
VIEW
IT
IS
A
CLIENT
TELLING
THE
SERVER
TO
DO
THINGS
LIKE
DISPLAY
TEXT
AND
GEOMETRIC
FIGURES
THE
SERVER
IN
THE
LOCAL
PC
JUST
DOES
WHAT
IT
IS
TOLD
AS
DO
ALL
SERVERS
THE
ARRANGEMENT
OF
CLIENT
AND
SERVER
IS
SHOWN
IN
FIG
FOR
THE
CASE
WHERE
THE
X
CLIENT
AND
X
SERVER
ARE
ON
DIFFERENT
MACHINES
BUT
WHEN
RUNNING
GNOME
OR
KDE
ON
A
SINGLE
MACHINE
THE
CLIENT
IS
JUST
SOME
APPLICATION
PROGRAM
USING
THE
X
LIBRARY
TALKING
TO
THE
X
SERVER
ON
THE
SAME
MACHINE
BUT
USING
A
TCP
CONNECTION
OVER
SOCKETS
THE
SAME
AS
IT
WOULD
DO
IN
THE
REMOTE
CASE
INPUT
OUTPUT
CHAP
REMOT
E
HOST
USE
R
SPAC
E
KERNE
L
SPAC
E
X
PROTOCOL
NETWOR
K
FIGURE
CLIENTS
AND
SERVERS
IN
THE
M
I
T
X
WINDOW
SYSTEM
THE
REASON
IT
IS
POSSIBLE
TO
RUN
THE
X
WINDOW
SYSTEM
ON
TOP
OF
UNIX
OR
AN
OTHER
OPERATING
SYSTEM
ON
A
SINGLE
MACHINE
OR
OVER
A
NETWORK
IS
THAT
WHAT
X
REAL
LY
DEFINES
IS
THE
X
PROTOCOL
BETWEEN
THE
X
CLIENT
AND
THE
X
SERVER
AS
SHOWN
IN
FIG
IT
DOES
NOT
MATTER
WHETHER
THE
CLIENT
AND
SERVER
ARE
ON
THE
SAME
MA
CHINE
SEPARATED
BY
METERS
OVER
A
LOCAL
AREA
NETWORK
OR
ARE
THOUSANDS
OF
KILOMETERS
APART
AND
CONNECTED
BY
THE
INTERNET
THE
PROTOCOL
AND
OPERATION
OF
THE
SYSTEM
IS
IDENTICAL
IN
ALL
CASES
X
IS
JUST
A
WINDOWING
SYSTEM
IT
IS
NOT
A
COMPLETE
GUI
TO
GET
A
COMPLETE
GUI
OTHERS
LAYER
OF
SOFTWARE
ARE
RUN
ON
TOP
OF
IT
ONE
LAYER
IS
XLIB
WHICH
IS
A
SET
OF
LIBRARY
PROCEDURES
FOR
ACCESSING
THE
X
FUNCTIONALITY
THESE
PROCEDURES
FORM
THE
BASIS
OF
THE
X
WINDOW
SYSTEM
AND
ARE
WHAT
WE
WILL
EXAMINE
BELOW
BUT
THEY
ARE
TOO
PRIMITIVE
FOR
MOST
USER
PROGRAMS
TO
ACCESS
DIRECTLY
FOR
EXAMPLE
EACH
MOUSE
CLICK
IS
REPORTED
SEPARATELY
SO
THAT
DETERMINING
THAT
TWO
CLICKS
REALLY
FORM
A
DOU
BLE
CLICK
HAS
TO
BE
HANDLED
ABOVE
XLIB
TO
MAKE
PROGRAMMING
WITH
X
EASIER
A
TOOLKIT
CONSISTING
OF
THE
INTRINSICS
IS
SUPPLIED
AS
PART
OF
X
THIS
LAYER
MANAGES
BUTTONS
SCROLL
BARS
AND
OTHER
GUI
ELE
MENTS
CALLED
WIDGETS
TO
MAKE
A
TRUE
GUI
INTERFACE
WITH
A
UNIFORM
LOOK
AND
FEEL
YET
ANOTHER
LAYER
IS
NEEDED
OR
SEVERAL
OF
THEM
ONE
EXAMPLE
IS
MOTIF
SHOWN
IN
FIG
WHICH
IS
THE
BASIS
OF
THE
COMMON
DESKTOP
ENVIRONMENT
USED
ON
SOLARIS
AND
OTHER
COMMERCIAL
UNIX
SYSTEMS
MOST
APPLICATIONS
MAKE
USE
OF
CALLS
TO
MOTIF
RATHER
THAN
XLIB
GNOME
AND
KDE
HAVE
A
SIMILAR
STRUCTURE
TO
FIG
ONLY
WITH
DIFFERENT
LIBRARIES
GNOME
USES
THE
GTK
LIBRARY
AND
KDE
USES
THE
QT
LIBRARY
WHETHER
HAVING
TWO
GUIS
IS
BETTER
THAN
ONE
IS
DEBATABLE
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
ALSO
WORTH
NOTING
IS
THAT
WINDOW
MANAGEMENT
IS
NOT
PART
OF
X
ITSELF
THE
DECISION
TO
LEAVE
IT
OUT
WAS
FULLY
INTENTIONAL
INSTEAD
A
SEPARATE
X
CLIENT
PROCESS
CALLED
A
WINDOW
MANAGER
CONTROLS
THE
CREATION
DELETION
AND
MOVEMENT
OF
WIN
DOWS
ON
THE
SCREEN
TO
MANAGE
WINDOWS
IT
SENDS
COMMANDS
TO
THE
X
SERVER
TEL
LING
WHAT
TO
DO
IT
OFTEN
RUNS
ON
THE
SAME
MACHINE
AS
THE
X
CLIENT
BUT
IN
THEORY
CAN
RUN
ANYWHERE
THIS
MODULAR
DESIGN
CONSISTING
OF
SEVERAL
LAYERS
AND
MULTIPLE
PROGRAMS
MAKES
X
HIGHLY
PORTABLE
AND
FLEXIBLE
IT
HAS
BEEN
PORTED
TO
MOST
VERSIONS
OF
UNIX
INCLUDING
SOLARIS
ALL
VARIANTS
OF
BSD
AIX
LINUX
AND
SO
ON
MAKING
IT
POSSIBLE
FOR
APPLICATION
DEVELOPERS
TO
HAVE
A
STANDARD
USER
INTERFACE
FOR
MULTIPLE
PLATFORMS
IT
HAS
ALSO
BEEN
PORTED
TO
OTHER
OPERATING
SYSTEMS
IN
CONTRAST
IN
WIN
DOWS
THE
WINDOWING
AND
GUI
SYSTEMS
ARE
MIXED
TOGETHER
IN
THE
GDI
AND
LOCATED
IN
THE
KERNEL
WHICH
MAKES
THEM
HARDER
TO
MAINTAIN
AND
OF
COURSE
NOT
PORTABLE
NOW
LET
US
TAKE
A
BRIEF
LOOK
AT
X
AS
VIEWED
FROM
THE
XLIB
LEVEL
WHEN
AN
X
PROGRAM
STARTS
IT
OPENS
A
CONNECTION
TO
ONE
OR
MORE
X
SERVERS
LET
US
CALL
THEM
WORKSTATIONS
EVEN
THOUGH
THEY
MIGHT
BE
COLLOCATED
ON
THE
SAME
MACHINE
AS
THE
X
PROGRAM
ITSELF
X
CONSIDERS
THIS
CONNECTION
TO
BE
RELIABLE
IN
THE
SENSE
THAT
LOST
AND
DUPLICATE
MESSAGES
ARE
HANDLED
BY
THE
NETWORKING
SOFTWARE
AND
IT
DOES
NOT
HAVE
TO
WORRY
ABOUT
COMMUNICATION
ERRORS
USUALLY
TCP
IP
IS
USED
BETWEEN
THE
CLIENT
AND
SERVER
FOUR
KINDS
OF
MESSAGES
GO
OVER
THE
CONNECTION
DRAWING
COMMANDS
FROM
THE
PROGRAM
TO
THE
WORKSTATION
REPLIES
BY
THE
WORKSTATION
TO
PROGRAM
QUERIES
KEYBOARD
MOUSE
AND
OTHER
EVENT
ANNOUNCEMENTS
ERROR
MESSAGES
MOST
DRAWING
COMMANDS
ARE
SENT
FROM
THE
PROGRAM
TO
THE
WORKSTADON
AS
ONE
WAY
MESSAGES
NO
REPLY
IS
EXPECTED
THE
REASON
FOR
THIS
DESIGN
IS
THAT
WHEN
THE
CLIENT
AND
SERVER
PROCESSES
ARE
ON
DIFFERENT
MACHINES
IT
MAY
TAKE
A
SUBSTANTIAL
PERIOD
OF
TIME
FOR
THE
COMMAND
TO
REACH
THE
SERVER
AND
BE
CARRIED
OUT
BLOCKING
THE
APPLICATION
PROGRAM
DURING
THIS
TIME
WOULD
SLOW
IT
DOWN
UNNECESSARILY
ON
THE
OTHER
HAND
WHEN
THE
PROGRAM
NEEDS
INFORMATION
FROM
THE
WORKSTATION
IT
SIM
PLY
HAS
TO
WAIT
UNTIL
THE
REPLY
COMES
BACK
LIKE
WINDOWS
X
IS
HIGHLY
EVENT
DRIVEN
EVENTS
FLOW
FROM
THE
WORKSTATION
TO
THE
PROGRAM
USUALLY
IN
RESPONSE
TO
SOME
HUMAN
ACTION
SUCH
AS
KEYBOARD
STROKES
MOUSE
MOVEMENTS
OR
A
WINDOW
BEING
UNCOVERED
EACH
EVENT
MESSAGE
IS
BYTES
WITH
THE
FIRST
BYTE
GIVING
THE
EVENT
TYPE
AND
THE
NEXT
BYTES
PROVIDING
AD
DITIONAL
INFORMATION
SEVERAL
DOZEN
KINDS
OF
EVENTS
EXIST
BUT
A
PROGRAM
IS
SENT
ONLY
THOSE
EVENTS
THAT
IT
HAS
SAID
IT
IS
WILLING
TO
HANDLE
FOR
EXAMPLE
IF
A
PROGRAM
DOES
NOT
WANT
TO
HEAR
ABOUT
KEY
RELEASES
IT
IS
NOT
SENT
ANY
KEY
RELEASE
EVENTS
AS
IN
WINDOWS
EVENTS
ARE
QUEUED
AND
PROGRAMS
READ
EVENTS
FROM
THE
INPUT
QUEUE
INPUT
OUTPUT
CHAP
HOWEVER
UNLIKE
WINDOWS
THE
OPERATING
SYSTEM
NEVER
CALLS
PROCEDURES
WITHIN
THE
APPLICATION
PROGRAM
ON
ITS
OWN
IT
DOES
NOT
EVEN
KNOW
WHICH
PROCEDURE
HANDLES
WHICH
EVENT
A
KEY
CONCEPT
IN
X
IS
THE
RESOURCE
A
RESOURCE
IS
A
DATA
STRUCTURE
THAT
HOLDS
CERTAIN
INFORMATION
APPLICATION
PROGRAMS
CREATE
RESOURCES
ON
WORKSTADONS
RE
SOURCES
CAN
BE
SHARED
AMONG
MULTIPLE
PROCESSES
ON
THE
WORKSTATION
RESOURCES
TEND
TO
BE
SHORT
LIVED
AND
DO
NOT
SURVIVE
WORKSTATION
REBOOTS
TYPICAL
RESOURCES
INCLUDE
WINDOWS
FONTS
COLORMAPS
COLOR
PALETTES
PIXMAPS
BITMAPS
CURSORS
AND
GRAPHIC
CONTEXTS
THE
LATTER
ARE
USED
TO
ASSOCIATE
PROPERTIES
WITH
WINDOWS
AND
ARE
SIMILAR
IN
CONCEPT
TO
DEVICE
CONTEXTS
IN
WINDOWS
A
ROUGH
INCOMPLETE
SKELETON
OF
AN
X
PROGRAM
IS
SHOWN
IN
FIG
IT
BEGINS
BY
INCLUDING
SOME
REQUIRED
HEADERS
AND
THEN
DECLARING
SOME
VARIABLES
IT
THEN
CONNECTS
TO
THE
X
SERVER
SPECIFIED
AS
THE
PARAMETER
TO
XOPENDISPLAY
THEN
IT
ALLOCATES
A
WINDOW
RESOURCE
AND
STORES
A
HANDLE
TO
IT
IN
WIN
IN
PRACTICE
SOME
INITIALIZATION
WOULD
HAPPEN
HERE
AFTER
THAT
IT
TELLS
THE
WINDOW
MANAGER
THAT
THE
NEW
WINDOW
EXISTS
SO
THE
V
INDOW
MANAGER
CAN
MANAGE
IT
THE
CALL
TO
XCREATEGC
CREATES
A
GRAPHIC
CONTEXT
IN
WHICH
PROPERTIES
OF
THE
WINDOW
ARE
STORED
IN
A
MORE
COMPLETE
PROGRAM
THEY
MIGHT
BE
INITIALIZED
HERE
THE
NEXT
STATEMENT
THE
CALL
TO
XSELECTLNPUT
TELLS
THE
X
SERVER
WHICH
EVENTS
THE
PROGRAM
IS
PREPARED
TO
HANDLE
IN
THIS
CASE
IT
IS
INTERESTED
IN
MOUSE
CLICKS
KEY
STROKES
AND
WINDOWS
BEING
UNCOVERED
IN
PRACTICE
A
REAL
PROGRAM
WOULD
BE
INTERESTED
IN
OTHER
EVENTS
AS
WELL
FINALLY
THE
CALL
TO
XMAPRAISED
MAPS
THE
NEW
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
INCLUDE
XLIB
H
INCLUDE
XUTIL
H
MAIN
INT
ARGC
CHAR
ARGVFJ
DISPLAY
DISP
SERVER
IDENTIFIER
WINDOW
WIN
WINDOW
IDENTIFIER
GC
GC
GRAPHIC
CONTEXT
IDENTIFIER
XEVENT
EVENT
STORAGE
FOR
ONE
EVENT
INT
RUNNING
DISP
XOPENDISPLAY
CONNECT
TO
THE
X
SERVER
WIN
XCREATESIMPLEWINDOW
DISP
ALLOCATE
MEMORY
FOR
NEW
WINDOW
XSETSTANDARDPROPERTIESFDISP
ANNOUNCES
WINDOW
TO
WINDOW
MGR
GC
XCREATEGC
DISP
WIN
CREATE
GRAPHIC
CONTEXT
XSELECTLNPUTFDISP
WIN
BUTTONPRESSMASK
KEYPRESSMASK
EXPOSUREMASK
XMAPRAISED
DISP
WIN
DISPLAY
WINDOW
SEND
EXPOSE
EVENT
WHIIE
RUNNING
XNEXTEVENT
DISP
EVENT
GET
NEXT
EVENT
SWITCH
EVENT
TYPE
CAS
E
EXPOSE
BREAK
REPAINT
WINDOW
CAS
E
BUTTONPRESS
BREAK
PROCESS
MOUSE
CLICK
CAS
E
KEYPRESS
BREAK
PROCESS
KEYBOARD
INPUT
WINDOW
ONTO
THE
SCREEN
AS
THE
UPPERMOST
WINDOW
AT
THIS
POINT
THE
WINDOW
BE
COMES
VISIBLE
ON
THE
SCREEN
THE
MAIN
LOOP
CONSISTS
OF
TWO
STATEMENTS
AND
IS
LOGICALLY
MUCH
SIMPLER
THAN
THE
CORRESPONDING
LOOP
IN
WINDOWS
THE
FIRST
STATEMENT
HERE
GETS
AN
EVENT
AND
THE
SECOND
ONE
DISPATCHES
ON
THE
EVENT
TYPE
FOR
PROCESSING
WHEN
SOME
EVENT
INDICATES
THAT
THE
PROGRAM
HAS
FINISHED
RUNNING
IS
SET
TO
AND
THE
LOOP
TERMINATES
XFREEGCFDISP
GC
XDESTROYWINDOW
DISP
WIN
XCIOSEDISPLAY
DISP
RELEASE
GRAPHIC
CONTEXT
DEALLOCATE
WINDOW
MEMORY
SPAC
E
FEAR
DOWN
NETWORK
CONNECTION
BEFORE
EXITING
THE
PROGRAM
RELEASES
THE
GRAPHIC
CONTEXT
WINDOW
AND
CONNECTION
IT
IS
WORTH
MENTIONING
THAT
NOT
EVERYONE
LIKES
A
GUI
MANY
PROGRAMMERS
PREFER
A
TRADITIONAL
COMMAND
LINE
ORIENTED
INTERFACE
OF
THE
TYPE
DISCUSSED
IN
SEC
ABOVE
X
HANDLES
THIS
VIA
A
CLIENT
PROGRAM
CALLED
XTERM
THIS
PROGRAM
EMU
LATES
A
VENERABLE
INTELLIGENT
TERMINAL
COMPLETE
WITH
ALL
THE
ESCAPE
SE
QUENCES
THUS
EDITORS
SUCH
AS
VI
AND
EMACS
AND
OTHER
SOFTWARE
THAT
USES
TERMCAP
WORK
IN
THESE
WINDOWS
WITHOUT
MODIFICATION
GRAPHICAL
USER
INTERFACES
MOST
PERSONAL
COMPUTERS
OFFER
A
GUI
GRAPHICAL
USER
INTERFACE
THE
ACRO
NYM
GUI
IS
PRONOUNCED
GOOEY
THE
GUI
WAS
INVENTED
BY
DOUGLAS
ENGELBART
AND
HIS
RESEARCH
GROUP
AT
THE
STANFORD
RESEARCH
INSTITUTE
IT
WAS
THEN
CORJ
JML
ESEARCHERS
AT
XEROX
PARC
ONE
FINE
DAY
STEVE
JOBS
COFOUNDER
OF
APPLE
WASTOURING
PARC
AND
SAW
A
GUI
FIGURE
A
SKELETON
OF
AN
X
WINDOW
APPLICATION
PROGRAM
ON
A
XEROX
COMPUTER
AND
SAID
SOMETHING
TO
THE
EFFECT
OF
HOLY
MACKEREL
THIS
IS
THE
FUTURE
OF
COMPUTING
THE
GUI
GAVE
HIM
THE
IDEA
FOR
A
NEW
COMPUTER
WHICH
BECAME
THE
APPLE
LISA
THE
LISA
WAS
TOO
EXPENSIVE
AND
WAS
A
COMMERCIAL
FAILURE
BUT
ITS
SUCCESSOR
THE
MACINTOSH
WAS
A
HUGE
SUCCESS
WHEN
MICROSOFT
GOT
A
MACINTOSH
PROTOTYPE
SO
IT
COULD
DEVELOP
MICROSOFT
OFFICE
ON
IT
IT
BEGGED
APPLE
TO
LICENSE
THE
INTERFACE
TO
ALL
COMERS
SO
IT
WOULD
BE
COME
THE
NEW
INDUSTRY
STANDARD
MICROSOFT
MADE
MUCH
MORE
MONEY
FROM
OFFICE
THAN
FROM
MS
DOS
SO
IT
WAS
WILLING
TO
ABANDON
MS
DOS
TO
HAVE
A
BETTER
PLATFORM
FOR
OFFICE
THE
APPLE
EXECUTIVE
IN
CHARGE
OF
THE
MACINTOSH
JEAN
LOUIS
GASSEE
REFUSED
AND
STEVE
JOBS
WAS
NO
LONGER
AROUND
TO
OVERRULE
HIM
EVENTUALLY
MICRO
SOFT
GOT
A
LICENSE
FOR
ELEMENTS
OF
THE
INTERFACE
THIS
FORMED
THE
BASIS
OF
WIN
DOWS
WHEN
WINDOWS
BEGAN
TO
CATCH
ON
APPLE
SUED
MICROSOFT
CLAIMING
MICRO
SOFT
HAD
EXCEEDED
THE
LICENSE
BUT
THE
JUDGE
DISAGREED
AND
WINDOWS
WENT
ON
TO
INPUT
OUTPUT
CHAP
OVERTAKE
THE
MACINTOSH
IF
GASSEE
HAD
AGREED
WITH
THE
MANY
PEOPLE
WITHIN
APPLE
WHO
ALSO
WANTED
TO
LICENSE
THE
MACINTOSH
SOFTWARE
TO
EVERYONE
AND
HIS
UNCLE
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
APPLE
WOULD
PROBABLY
HAVE
BECOME
IMMENSELY
RICH
ON
LICENSING
FEES
AND
WIN
DOWS
WOULD
NOT
EXIST
NOW
A
GUI
HAS
FOUR
ESSENTIAL
ELEMENTS
DENOTED
BY
THE
CHARACTERS
WIMP
THESE
LETTERS
STAND
FOR
WINDOWS
ICONS
MENUS
AND
POINTING
DEVICE
RESPECTIVELY
WIN
DOWS
ARE
RECTANGULAR
BLOCKS
OF
SCREEN
AREA
USED
TO
RUN
PROGRAMS
ICONS
ARE
LITTLE
SYMBOLS
THAT
CAN
BE
CLICKED
ON
TO
CAUSE
SOME
ACTION
TO
HAPPEN
MENUS
ARE
LISTS
OF
ACTIONS
FROM
WHICH
ONE
CAN
BE
CHOSEN
FINALLY
A
POINTING
DEVICE
IS
A
MOUSE
TRACKBALL
OR
OTHER
HARDWARE
DEVICE
USED
TO
MOVE
A
CURSOR
AROUND
THE
SCREEN
TO
SE
LECT
ITEMS
THE
GUI
SOFTWARE
CAN
BE
IMPLEMENTED
IN
EITHER
USER
LEVEL
CODE
AS
IS
DONE
IN
UNIX
SYSTEMS
OR
IN
THE
OPERATING
SYSTEM
ITSELF
AS
IN
THE
CASE
IN
WINDOWS
INPUT
FOR
GUI
SYSTEMS
STILL
USES
THE
KEYBOARD
AND
MOUSE
BUT
OUTPUT
ALMOST
ALWAYS
GOES
TO
A
SPECIAL
HARDWARE
BOARD
CALLED
A
GRAPHICS
ADAPTER
A
GRAPHICS
ADAPTER
CONTAINS
A
SPECIAL
MEMORY
CALLED
A
VIDEO
RAM
THAT
HOLDS
THE
IMAGES
THAT
APPEAR
ON
THE
SCREEN
HIGH
END
GRAPHICS
ADAPTERS
OFTEN
HAVE
POWERFUL
OR
BIT
CPUS
AND
UP
TO
GB
OF
THEIR
OWN
RAM
SEPARATE
FROM
THE
COMPUTER
MAIN
MEMORY
EACH
GRAPHICS
ADAPTER
SUPPORTS
SOME
NUMBER
OF
SCREEN
SIZES
COMMON
SIZES
ARE
X
X
X
AND
ALL
OF
THESE
EXCEPT
V
MENU
BAR
TOOL
BAR
WINDOW
FSIIQ
CLIENT
AREA
I
THUMB
SCROFL
BAR
J
MB
OR
MORE
THE
GRAPHICS
ADAPTER
CAN
HOLD
MANY
IMAGES
AT
ONCE
IF
THE
FULL
SCREEN
IS
REFRESHED
TIMES
SEC
THE
VIDEO
RAM
MUST
BE
CAPABLE
OF
DELIVERING
DATA
CONTINUOUSLY
AT
MB
SEC
OUTPUT
SOFTWARE
FOR
GUIS
IS
A
MASSIVE
TOPIC
MANY
PAGE
BOOKS
HAVE
BEEN
WRITTEN
ABOUT
THE
WINDOWS
GUI
ALONE
E
G
PETZOLD
SIMON
AND
RECTOR
AND
NEWCOMER
CLEARLY
IN
THIS
SECTION
WE
CAN
ONLY
SCRATCH
THE
SURFACE
AND
PRESENT
A
FEW
OF
THE
UNDERLYING
CONCEPTS
TO
MAKE
THE
DISCUSSION
CONCRETE
WE
WILL
DESCRIBE
THE
API
WHICH
IS
SUPPORTED
BY
ALL
BIT
VER
SIONS
OF
WINDOWS
THE
OUTPUT
SOFTWARE
FOR
OTHER
GUIS
IS
ROUGHLY
COMPARABLE
IN
A
GENERAL
SENSE
BUT
THE
DETAILS
ARE
VERY
DIFFERENT
THE
BASIC
ITEM
ON
THE
SCREEN
IS
A
RECTANGULAR
AREA
CALLED
A
WINDOW
A
WIN
DOW
POSITION
AND
SIZE
ARE
UNIQUELY
DETERMINED
BY
GIVING
THE
COORDINATES
IN
PIX
ELS
OF
TWO
DIAGONALLY
OPPOSITE
CORNERS
A
WINDOW
MAY
CONTAIN
A
TITLE
BAR
A
MENU
BAR
A
TOOL
BAR
A
VERTICAL
SCROLL
BAR
AND
A
HORIZONTAL
SCROLL
BAR
A
TYPICAL
WINDOW
IS
SHOWN
IN
FIG
NOTE
THAT
THE
WINDOWS
COORDINATE
SYSTEM
PUTS
THE
ORIGIN
IN
THE
UPPER
LEFTHAND
CORNER
AND
HAS
Y
INCREASE
DOWNWARD
WHICH
IS
DIF
FERENT
FRORNTHE
CARTESIAN
COORDINATES
USED
IN
MATHEMATICS
FIGURE
A
SAMPLE
WINDOW
LOCATED
AT
ON
AN
XG
A
DISPLAY
WHEN
A
WINDOW
IS
CREATED
THE
PARAMETERS
SPECIFY
WHETHER
THE
WINDOW
CAN
BE
MOVED
BY
THE
USER
RESIZED
BY
THE
USER
OR
SCROLLED
BY
DRAGGING
THE
THUMB
ON
THE
SCROLL
BAR
BY
THE
USER
THE
MAIN
WINDOW
PRODUCED
BY
MOST
PROGRAMS
CAN
BE
MOVED
RESIZED
AND
SCROLLED
WHICH
HAS
ENORMOUS
CONSEQUENCES
FOR
THE
WAY
WIN
DOWS
PROGRAMS
ARE
WRITTEN
IN
PARTICULAR
PROGRAMS
MUST
BE
INFORMED
ABOUT
CHANGES
TO
THE
SIZE
OF
THEIR
WINDOWS
AND
MUST
BE
PREPARED
TO
REDRAW
THE
CONTENTS
OF
THEIR
WINDOWS
AT
ANY
TIME
EVEN
WHEN
THEY
LEAST
EXPECT
IT
AS
A
CONSEQUENCE
WINDOWS
PROGRAMS
ARE
MESSAGE
ORIENTED
USER
ACTIONS
INVOLVING
THE
KEYBOARD
OR
MOUSE
ARE
CAPTURED
BY
WINDOWS
AND
CONVERTED
INTO
MESSAGES
TO
THE
PROGRAM
OWNING
THE
WINDOW
BEING
ADDRESSED
EACH
PROGRAM
HAS
A
MESSAGE
QUEUE
TO
WHICH
MESSAGES
RELATING
TO
ALL
ITS
WINDOWS
ARE
SENT
THE
MAIN
LOOP
OF
THE
PROGRAM
CONSISTS
OF
FISHING
OUT
THE
NEXT
MESSAGE
AND
PROCESSING
IT
BY
CALLING
AN
INTERNAL
PROCEDURE
FOR
THAT
MESSAGE
TYPE
IN
SOME
CASES
WINDOWS
IT
SELF
MAY
CALL
THESE
PROCEDURES
DIRECTLY
BYPASSING
THE
MESSAGE
QUEUE
THIS
MODEL
IS
QUITE
DIFFERENT
THAN
THE
UNIX
MODEL
OF
PROCEDURAL
CODE
THAT
MAKES
SYSTEM
CALLS
TO
INTERACT
WITH
THE
OPERATING
SYSTEM
X
HOWEVER
IS
EVENT
ORIENTED
INPUT
OUTPUT
CHAP
TO
MAKE
THIS
OROGRAMMING
MODEL
CLEARER
CONSIDER
THE
EXAMPLE
OF
FIG
H
E
J
W
E
T
Z
SEON
OF
A
MAIN
PROGRAM
FOR
WINDOW
IT
I
NOT
COMP
AND
DOES
NO
ERROR
CHECKING
BUT
IT
SHOWS
ENOUGH
DETARL
FOR
OUR
PULSES
IT
STARTS
BY
INCLUDING
A
HEADER
FILE
WINDOWS
H
WHICH
CONTAINS
MANY
MACROS
DATA
TYPES
CON
Z
V
R
O
AND
OTHER
INFORMATION
NEEDED
B
Y
WMDOWS
PROGRAM
INCLUDE
WINDOWS
H
INT
WINAPL
WIN
VLAIN
H
NSTANCE
H
HINSTANCE
HPREV
CHAR
SZCMD
H
ICMDSHOW
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
IS
AN
INSTANCE
HANDLE
AND
IS
USED
TO
IDENTIFY
THE
PROGRAM
TO
THE
REST
OF
THE
SYSTEM
TO
SOME
EXTENT
IS
OBJECT
ORIENTED
WHICH
MEANS
THAT
THE
SYSTEM
CONTAINS
OBJECTS
E
G
PROGRAMS
FILES
AND
WINDOWS
THAT
HAVE
SOME
STATE
AND
ASSOCIATED
CODE
CALLED
METHODS
THAT
OPERATE
ON
THAT
STATE
OBJECTS
ARE
REFERRED
TO
USING
HANDLES
AND
IN
THIS
CASE
H
IDENTIFIES
THE
PROGRAM
THE
SECOND
PARAMETER
IS
PRES
ENT
ONLY
FOR
REASONS
OF
BACKWARD
COMPATIBILITY
IT
IS
NO
LONGER
USED
THE
THIRD
PA
RAMETER
SZCMD
IS
A
ZERO
TERMINATED
STRING
CONTAINING
THE
COMMAND
LINE
THAT
STARTED
THE
PROGRAM
EVEN
IF
IT
WAS
NOT
STARTED
FROM
A
COMMAND
LINE
THE
FOURTH
PARAMETER
ICMDSHOW
TELLS
WHETHER
THE
PROGRAM
INITIAL
WINDOW
SHOULD
OCCUPY
WNDGLASS
WNDCLASS
MSG
MSG
HWND
HWND
CLASS
OBJECT
FOR
THIS
WINDOW
INCOMING
MESSAGES
ARE
STORED
HERE
HANDLE
POINTER
TO
THE
WINDOW
OBJECT
THE
ENTIRE
SCREEN
PART
OF
THE
SCREEN
OR
NONE
OF
THE
SCREEN
TASK
BAR
ONLY
THIS
DECLARATION
ILLUSTRATES
A
WIDELY
USED
MICROSOFT
CONVENTION
CALLED
HUN
GARIAN
NOTATION
THE
NAME
IS
A
PUN
ON
POLISH
NOTATION
THE
POSTFIX
SYSTEM
INVEN
TED
BY
THE
POLISH
LOGICIAN
J
LUKASIEWICZ
FOR
REPRESENTING
ALGEBRAIC
FORMULAS
WITHOUT
USING
PRECEDENCE
OR
PARENTHESES
HUNGARIAN
NOTATION
WAS
INVENTED
BY
A
HUNGARIAN
PROGRAMMER
AT
MICROSOFT
CHARLES
SIMONYI
AND
USES
THE
FIRST
FEW
V
NDDASS
LPSZC
E
T
W
C
W
PROGRA
M
ICON
CHARACTERS
OF
AN
IDENTIFIER
TO
SPECIFY
THE
TYPE
THE
ALLOWED
LETTERS
AND
TYPES
IN
CLUDE
C
CHARACTER
W
WORD
NOW
MEANING
AN
UNSIGNED
BIT
INTEGER
I
BIT
SIGNED
INTEGER
LONG
ALSO
A
BIT
SIGNED
INTEGER
STRING
SZ
STRING
TERMI
REGISTERCIASS
WNDCLASS
HWND
CREATEWINDOW
SHOWWINDOW
HWND
ICMDSHOW
UPDATEWINDOW
HWND
TELL
WINDOWS
ABOUT
WNDCTASS
ALLOCATE
STORAGE
FOR
THE
WINDOW
DISPLAY
THE
WINDOW
ON
THE
SCREEN
TELL
THE
WINDOW
TO
PAINT
ITSELF
NATED
BY
A
ZERO
BYTE
P
POINTER
FN
FUNCTION
AND
H
HANDLE
THUS
SZCMD
IS
A
ZERO
TERMINATED
STRING
AND
ICMDSHOW
IS
AN
INTEGER
FOR
EXAMPLE
MANY
PRO
GRAMMERS
BELIEVE
THAT
ENCODING
THE
TYPE
IN
VARIABLE
NAMES
THIS
WAY
HAS
LITTLE
VALUE
AND
MAKES
WINDOWS
CODE
EXCEPTIONALLY
HARD
TO
READ
NOTHING
ANALOGOUS
TO
WHITE
GETMESSAGE
MSG
NULL
T
Z
F
J
TRAN
IATPMESSAQE
MSQ
TRANSLATE
THE
MESSAGE
M
U
HEAPPROPRLAT
E
P
R
O
C
E
D
U
R
E
RETUM
MSG
WPARAM
LONG
CALLBACK
WNDPROC
HWND
HWND
MESSAGE
UINT
WPARAM
LONG
IPARAM
DECLARATIONS
GO
HERE
THIS
CONVENTION
IS
PRESENT
IN
UNIX
EVERY
WINDOW
MUST
HAVE
AN
ASSOCIATED
CLASS
OBJECT
THAT
DEFINES
ITS
PROPERTIES
IN
FIG
THAT
CLASS
OBJECT
IS
WNDCLASS
AN
OBJECT
OF
TYPE
WNDCLASS
HAS
FIELDS
FOUR
OF
WHICH
ARE
INITIALIZED
IN
FIG
IN
AN
ACTUAL
PROGRAM
THE
OTHER
SIX
WOULD
BE
INITIALIZED
AS
WELL
THE
MOST
IMPORTANT
FIELD
IS
IPFNWNDPROC
WHICH
IS
A
LONG
I
E
BIT
POINTER
TO
THE
FUNCTION
THAT
HANDLES
THE
MESSAGES
DIRECTED
TO
THIS
WINDOW
THE
OTHER
FIELDS
INITIALIZED
HERE
TELL
WHICH
NAME
AND
ICON
TO
USE
IN
THE
TITLE
BAR
AND
WHICH
SYMBOL
TO
USE
FOR
THE
MOUSE
CURSOR
AFTER
WNDCLASS
HAS
BEEN
INITIALIZED
REGISTERCLASS
IS
CALLED
TO
PASS
IT
TO
WIN
DOWS
IN
PARTICULAR
AFTER
THIS
CALL
WINDOWS
KNOWS
WHICH
PROCEDURE
TO
CALL
WHEN
SWITCH
MESSAGE
CASE
RETURN
CASE
RETURN
DESTROY
RETURN
CREATE
WINDOW
REPAINT
CONTENTS
OF
WINDOW
DESTROY
WINDOW
DEFAULT
VARIOUS
EVENTS
OCCUR
THAT
DO
NOT
GO
THROUGH
THE
MESSAGE
QUEUE
THE
NEXT
CALL
CREATEWINDOW
ALLOCATES
MEMORY
FOR
THE
WINDOW
DATA
STRUCTURE
AND
RETURNS
A
HANDLE
FOR
REFERENCING
IT
LATER
THE
PROGRAM
THEN
MAKES
TWO
MORE
CALLS
IN
A
ROW
TO
PUT
THE
WINDOW
OUTLINE
ON
THE
SCREEN
AND
FINALLY
FILL
IT
IN
COMPLETELY
RETUM
DEFWINDOWPROC
HWND
MESSAGE
WPARAM
IPARAM
FIGURE
A
SKELETON
OF
A
WINDOWS
MAIN
PROGRAM
THE
MAIN
PROGRAM
STARTS
WITH
A
DECLARATION
GIVING
ITS
NAME
AND
P
T
H
E
W
MACRO
I
A
N
INSTRUCTION
T
O
THE
COMPILER
T
O
PASSING
CONVENTION
AND
WILL
NGFOF
FURTHER
CONCERN
TO
US
THE
FIRST
PARAMETER
H
AT
THIS
POINT
WE
COME
TO
THE
PROGRAM
MAIN
LOOP
WHICH
CONSISTS
OF
GETTING
A
MESSAGE
HAVING
CERTAIN
TRANSLATIONS
DONE
TO
IT
AND
THEN
PASSING
IT
BACK
TO
WIN
DOWS
TO
HAVE
WINDOWS
INVOKE
WNDPROC
TO
PROCESS
IT
TO
ANSWER
THE
QUESTION
OF
WHETHER
THIS
WHOLE
MECHANISM
COULD
HAVE
BEEN
MADE
SIMPLER
THE
ANSWER
IS
YES
BUT
IT
WAS
DONE
THIS
WAY
FOR
HISTORICAL
REASONS
AND
WE
ARE
NOW
STUCK
WITH
IT
FOLLOWING
THE
MAIN
PROGRAM
IS
THE
PROCEDURE
WNDPROC
WHICH
HANDLES
THE
VARIOUS
MESSAGES
THAT
CAN
BE
SENT
TO
THE
WINDOW
THE
USE
OF
CALLBACK
HERE
INPUT
OUTPUT
CHAP
LIKE
WINAPL
ABOVE
SPECIFIES
THE
CALLING
SEQUENCE
TO
USE
FOR
PARAMETERS
THE
FIRST
PARAMETER
IS
THE
HANDLE
OF
THE
WINDOW
TO
USE
THE
SECOND
PARAMETER
IS
THE
MES
SAGE
TYPE
THE
THIRD
AND
FOURTH
PARAMETERS
CAN
BE
USED
TO
PROVIDE
ADDITIONAL
INFOR
MATION
WHEN
NEEDED
MESSAGE
TYPES
WM
CREATE
AND
WM
DESTROY
ARE
SENT
AT
THE
START
AND
END
OF
THE
PROGRAM
RESPECTIVELY
THEY
GIVE
THE
PROGRAM
THE
OPPORTUNITY
FOR
EX
AMPLE
TO
ALLOCATE
MEMORY
FOR
DATA
STRUCTURES
AND
THEN
RETURN
IT
THE
THIRD
MESSAGE
TYPE
WM
PAINT
IS
AN
INSTRUCTION
TO
THE
PROGRAM
TO
FILL
IN
THE
WINDOW
IT
IS
NOT
ONLY
CALLED
WHEN
THE
WINDOW
IS
FIRST
DRAWN
BUT
OFTEN
DURING
PROGRAM
EXECUTION
AS
WELL
IN
CONTRAST
TO
TEXT
BASED
SYSTEMS
IN
WINDOWS
A
PRO
GRAM
CANNOT
ASSUME
THAT
WHATEVER
IT
DRAWS
ON
THE
SCREEN
WILL
STAY
THERE
UNTIL
IT
RE
MOVES
IT
OTHER
WINDOWS
CAN
BE
DRAGGED
ON
TOP
OF
THIS
ONE
MENUS
CAN
BE
PULLED
DOWN
OVER
IT
DIALOG
BOXES
AND
TOOL
TIPS
CAN
COVER
PART
OF
IT
AND
SO
ON
WHEN
THESE
ITEMS
ARE
REMOVED
THE
WINDOW
HAS
TO
BE
REDRAWN
THE
WAY
WINDOWS
TELLS
A
PROGRAM
TO
REDRAW
A
WINDOW
IS
TO
SEND
IT
A
WM
PAINT
MESSAGE
AS
A
FRIENDLY
GESTURE
IT
ALSO
PROVIDES
INFORMATION
ABOUT
WHAT
PART
OF
THE
WINDOW
HAS
BEEN
OVER
WRITTEN
IN
CASE
IT
IS
EASIER
TO
REGENERATE
THAT
PART
OF
THE
WINDOW
INSTEAD
OF
REDRAW
ING
THE
WHOLE
THING
THERE
ARE
TWO
WAYS
WINDOWS
CAN
GET
A
PROGRAM
TO
DO
SOMETHING
ONE
WAY
IS
TO
POST
A
MESSAGE
TO
ITS
MESSAGE
QUEUE
THIS
METHOD
IS
USED
FOR
KEYBOARD
INPUT
MOUSE
INPUT
AND
TIMERS
THAT
HAVE
EXPIRED
THE
OTHER
WAY
SENDING
A
MESSAGE
TO
THE
WINDOW
INVOLVES
HAVING
WINDOWS
DIRECTLY
CALL
WNDPROC
ITSELF
THIS
METHOD
IS
USED
FOR
ALL
OTHER
EVENTS
SINCE
WINDOWS
IS
NOTIFIED
WHEN
A
MESSAGE
IS
FULLY
PROCESSED
IT
CAN
REFRAIN
FROM
MAKING
A
NEW
CALL
UNTIL
THE
PREVIOUS
ONE
IS
FINISHED
IN
THIS
WAY
RACE
CONDITIONS
ARE
AVOIDED
THERE
ARE
MANY
MORE
MESSAGE
TYPES
TO
AVOID
ERRATIC
BEHAVIOR
SHOULD
AN
UNEXPECTED
MESSAGE
ARRIVE
THE
PROGRAM
SHOULD
CALL
DEFWINDOWPROC
AT
THE
END
OF
WNDPROC
TO
LET
THE
DEFAULT
HANDLER
TAKE
CARE
OF
THE
OTHER
CASES
IN
SUMMARY
A
WINDOWS
PROGRAM
NORMALLY
CREATES
ONE
OR
MORE
WINDOWS
WITH
A
CLASS
OBJECT
FOR
EACH
ONE
ASSOCIATED
WITH
EACH
PROGRAM
IS
A
MESSAGE
QUEUE
AND
A
SET
OF
HANDLER
PROCEDURES
ULTIMATELY
THE
PROGRAM
S
BEHAVIOR
IS
DRIVEN
BY
THE
INCOMING
EVENTS
WHICH
ARE
PROCESSED
BY
THE
HANDLER
PROCEDURES
THIS
IS
A
VERY
DIFFERENT
MODEL
OF
THE
WORLD
THAN
THE
MORE
PROCEDURAL
VIEW
THAT
UNIX
TAKES
THE
ACTUAL
DRAWING
TO
THE
SCREEN
IS
HANDLED
BY
A
PACKAGE
CONSISTING
OF
HUN
DREDS
OF
PROCEDURES
THAT
ARE
BUNDLED
TOGETHER
TO
FORM
THE
GDI
GRAPHICS
DEVICE
INTERFACE
IT
CAN
HANDLE
TEXT
AND
ALL
KINDS
OF
GRAPHICS
AND
IS
DESIGNED
TO
BE
PLAT
FORM
AND
DEVICE
INDEPENDENT
BEFORE
A
PROGRAM
CAN
DRAW
I
E
PAINT
IN
A
WIN
DOW
IT
NEEDS
TO
ACQUIRE
A
DEVICE
CONTEXT
WHICH
IS
AN
INTERNAL
DATA
STRUCTURE
CON
TAINING
PROPERTIES
OF
THE
WINDOW
SUCH
AS
THE
CURRENT
FONT
TEXT
COLOR
BACKGROUND
COLOR
AND
SO
ON
MOST
GDI
CALLS
USE
THE
DEVICE
CONTEXT
EITHER
FOR
DRAWING
OR
FOR
GETTING
OR
SETTING
THE
PROPERTIES
VARIOUS
WAYS
EXIST
TO
ACQUIRE
THE
DEVICE
CONTEXT
A
SIMPLE
EXAMPLE
OF
ITS
ACQUISITION
AND
USE
IS
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
HDC
GETDC
HWND
TEXTOUT
HDC
X
Y
PSTEXT
ILENGTH
RELEASEDC
HWND
HDC
THE
FIRST
STATEMENT
GETS
A
HANDLE
TO
A
DEVICE
CONTENT
HDC
THE
SECOND
ONE
USES
THE
DEVICE
CONTEXT
TO
WRITE
A
LINE
OF
TEXT
ON
THE
SCREEN
SPECIFYING
THE
X
Y
COORDI
NATES
OF
WHERE
THE
STRING
STARTS
A
POINTER
TO
THE
STRING
ITSELF
AND
ITS
LENGTH
THE
THIRD
CALL
RELEASES
THE
DEVICE
CONTEXT
TO
INDICATE
THAT
THE
PROGRAM
IS
THROUGH
DRAW
ING
FOR
THE
MOMENT
NOTE
THAT
HDC
IS
USED
IN
A
WAY
ANALOGOUS
TO
A
UNIX
FILE
DE
SCRIPTOR
ALSO
NOTE
THAT
RELEASEDC
CONTAINS
REDUNDANT
INFORMATION
THE
USE
OF
HDC
UNIQUELY
SPECIFIES
A
WINDOW
THE
USE
OF
REDUNDANT
INFORMATION
THAT
HAS
NO
AC
TUAL
VALUE
IS
COMMON
IN
WINDOWS
ANOTHER
INTERESTING
NOTE
IS
THAT
WHEN
HDC
IS
ACQUIRED
IN
THIS
WAY
THE
PROGRAM
CAN
ONLY
WRITE
IN
THE
CLIENT
AREA
OF
THE
WINDOW
NOT
IN
THE
TITLE
BAR
AND
OTHER
PARTS
OF
IT
INTERNALLY
IN
THE
DEVICE
CONTEXT
S
DATA
STRUCTURE
A
CLIPPING
REGION
IS
MAIN
TAINED
ANY
DRAWING
OUTSIDE
THE
CLIPPING
REGION
IS
IGNORED
HOWEVER
THERE
IS
AN
OTHER
WAY
TO
ACQUIRE
A
DEVICE
CONTEXT
GETWINDOWDC
WHICH
SETS
THE
CLIPPING
RE
GION
TO
THE
ENTIRE
WINDOW
OTHER
CALLS
RESTRICT
THE
CLIPPING
REGION
IN
OTHER
WAYS
HAVING
MULTIPLE
CALLS
THAT
DO
ALMOST
THE
SAME
THING
IS
CHARACTERISTIC
OF
WINDOWS
A
COMPLETE
TREATMENT
OF
THE
GDI
IS
OUT
OF
THE
QUESTION
HERE
FOR
THE
INTERESTED
READER
THE
REFERENCES
CITED
ABOVE
PROVIDE
ADDITIONAL
INFORMATION
NEVERTHELESS
A
FEW
WORDS
ABOUT
THE
GDI
ARE
PROBABLY
WORTHWHILE
GIVEN
HOW
IMPORTANT
IT
IS
GDI
HAS
VARIOUS
PROCEDURE
CALLS
TO
GET
AND
RELEASE
DEVICE
CONTEXTS
OBTAIN
INFORMATION
ABOUT
DEVICE
CONTEXTS
GET
AND
SET
DEVICE
CONTEXT
ATTRIBUTES
E
G
THE
BACKGROUND
COLOR
MANIPULATE
GDI
OBJECTS
SUCH
AS
PENS
BRUSHES
AND
FONTS
EACH
OF
WHICH
HAS
ITS
OWN
ATTRIBUTES
FINALLY
OF
COURSE
THERE
ARE
A
LARGE
NUMBER
OF
GDI
CALLS
TO
ACTUALLY
DRAW
ON
THE
SCREEN
THE
DRAWING
PROCEDURES
FALL
INTO
FOUR
CATEGORIES
DRAWING
LINES
AND
CURVES
DRAWING
FILLED
AREAS
MANAGING
BITMAPS
AND
DISPLAYING
TEXT
WE
SAW
AN
EXAMPLE
OF
DRAWING
TEXT
ABOVE
SO
LET
US
TAKE
A
QUICK
LOOK
AT
ONE
OF
THE
OTHERS
THE
CALL
RECTANGLE
HDC
XLEFT
YTOP
XRIGHT
YBOTTOM
DRAWS
A
FILLED
RECTANGLE
WHOSE
CORNERS
ARE
XLEFT
YTOP
AND
XRIGHT
YBOTTOM
FOR
EXAMPLE
RECTANGLE
HDC
WILL
DRAW
THE
RECTANGLE
SHOWN
IN
FIG
THE
LINE
WIDTH
AND
COLOR
AND
FILL
COLOR
ARE
TAKEN
FROM
THE
DEVICE
CONTEXT
OTHER
GDI
CALLS
ARE
SIMILAR
IN
FLAVOR
BITMAPS
THE
GDI
PROCEDURES
ARE
EXAMPLES
OF
VECTOR
GRAPHICS
THEY
ARE
USED
TO
PLACE
GEOMETRIC
FIGURES
AND
TEXT
ON
THE
SCREEN
THEY
CAN
BE
SCALED
EASILY
TO
LARGER
OR
SMALLER
SCREENS
PROVIDED
THE
NUMBER
OF
PIXELS
ON
THE
SCREEN
IS
THE
SAME
THEY
INPUT
OUTPUT
CHAP
M
MM
A
K
I
FIGURE
AN
EXAMPLE
RECTANGLE
DRAWN
USING
RECTANGLE
EACH
BOX
REPRESENTS
ONE
PIXEL
ARE
ALSO
RELATIVELY
DEVICE
INDEPENDENT
A
COLLECTION
OF
CALLS
TO
GDI
PROCEDURES
CAN
BE
ASSEMBLED
IN
A
FILE
THAT
CAN
DESCRIBE
A
COMPLEX
DRAWING
SUCH
A
FILE
IS
CALL
ED
A
WINDOWS
METAFILE
AND
IS
WIDELY
USED
TO
TRANSMIT
DRAWINGS
FROM
ONE
WIN
DOWS
PROGRAM
TO
ANOTHER
SUCH
FILES
HAVE
EXTENSION
WMF
MANY
WINDOWS
PROGRAMS
ALLOW
THE
USER
TO
COPY
PART
OF
A
DRAWING
AND
PUT
IN
ON
THE
WINDOWS
CLIPBOARD
THE
USER
CAN
THEN
GO
TO
ANOTHER
PROGRAM
AND
PASTE
THE
CONTENTS
OF
THE
CLIPBOARD
INTO
ANOTHER
DOCUMENT
ONE
WAY
OF
DOING
THIS
IS
FOR
THE
FIRST
PROGRAM
TO
REPRESENT
THE
DRAWING
AS
A
WINDOWS
METAFILE
AND
PUT
IT
ON
THE
CLIPBOARD
IN
WMF
FORMAT
OTHER
WAYS
ALSO
EXIST
NOT
ALL
THE
IMAGES
THAT
COMPUTERS
MANIPULATE
CAN
BE
GENERATED
USING
VECTOR
GRAPHICS
PHOTOGRAPHS
AND
VIDEOS
FOR
EXAMPLE
DO
NOT
USE
VECTOR
GRAPHICS
IN
STEAD
THESE
ITEMS
ARE
SCANNED
IN
BY
OVERLAYING
A
GRID
ON
THE
IMAGE
THE
AVERAGE
RED
GREEN
AND
BLUE
VALUES
OF
EACH
GRID
SQUARE
ARE
THEN
SAMPLED
AND
SAVED
AS
THE
VALUE
OF
ONE
PIXEL
SUCH
A
FILE
IS
CALLED
A
BITMAP
THERE
ARE
EXTENSIVE
FACILITIES
IN
WINDOWS
FOR
MANIPULATING
BITMAPS
ANOTHER
USE
FOR
BITMAPS
IS
FOR
TEXT
ONE
WAY
TO
REPRESENT
A
PARTICULAR
CHARAC
TER
IN
SOME
FONT
IS
AS
A
SMALL
BITMAP
ADDING
TEXT
TO
THE
SCREEN
THEN
BECOMES
A
MATTER
OF
MOVING
BITMAPS
ONE
GENERAL
WAY
TO
USE
BITMAPS
IS
THROUGH
A
PROCEDURE
CALLED
BITBLT
IT
IS
CALL
ED
AS
FOLLOWS
BITBLT
DSTHDC
DX
DY
WID
HT
SRCHDC
SX
SY
RASTEROP
IN
ITS
SIMPLEST
FORM
IT
COPIES
A
BITMAP
FROM
A
RECTANGLE
IN
ONE
WINDOW
TO
A
REC
TANGLE
IN
ANOTHER
WINDOW
OR
THE
SAME
ONE
THE
FIRST
THREE
PARAMETERS
SPECIFY
THE
DESTINATION
WINDOW
AND
POSITION
THEN
COME
THE
WIDTH
AND
HEIGHT
NEXT
COME
THE
SOURCE
WINDOW
AND
POSITION
NOTE
THAT
EACH
WINDOW
HAS
ITS
OWN
COORDINATE
SEC
USER
INTERFACES
KEYBOARD
MOUSE
MONITOR
SYSTEM
WITH
IN
THE
UPPER
LEFT
HAND
COMER
OF
THE
WINDOW
THE
LAST
PARAME
TER
WILL
BE
DESCRIBED
BELOW
THE
EFFECT
OF
BITBIT
HDD
SRCCOPY
IS
SHOWN
IN
FIG
NOTICE
CAREFULLY
THAT
THE
ENTIRE
AREA
OF
THE
LETTER
A
HAS
BEEN
COPIED
INCLUDING
THE
BACKGROUND
COLOR
A
B
FIGURE
COPYING
BITMAPS
USING
BITBLT
A
BEFORE
B
AFTER
BITBLT
CAN
DO
MORE
THAN
JUST
COPY
BITMAPS
THE
LAST
PARAMETER
GIVES
THE
POSSI
BILITY
OF
PERFORMING
BOOLEAN
OPERATIONS
TO
COMBINE
THE
SOURCE
BITMAP
AND
THE
DESTINATION
BITMAP
FOR
EXAMPLE
THE
SOURCE
CAN
BE
ORED
INTO
THE
DESTINATION
TO
MERGE
WITH
IT
IT
CAN
ALSO
BE
EXCLUSIVE
ORED
INTO
IT
WHICH
MAINTAINS
THE
CHARACTERISTICS
OF
BOTH
SOURCE
AND
DESTINATION
A
PROBLEM
WITH
BITMAPS
IS
THAT
THEY
DO
NOT
SCALE
A
CHARACTER
THAT
IS
IN
A
BOX
OF
X
ON
A
DISPLAY
OF
X
WILL
LOOK
REASONABLE
HOWEVER
IF
THIS
BITMAP
IS
COPIED
TO
A
PRINTED
PAGE
AT
DOTS
INCH
WHICH
IS
BITS
X
BITS
THE
CHARACTER
WIDTH
PIXELS
WILL
BE
INCH
OR
MM
WIDE
IN
ADDITION
COPYING
BETWEEN
DEVICES
WITH
DIFFERENT
COLOR
PROPERTIES
OR
BETWEEN
MONOCHROME
AND
COLOR
DOES
NOT
WORK
WELL
FOR
THIS
REASON
WINDOWS
ALSO
SUPPORTS
A
DATA
STRUCTURE
CALLED
A
DIB
DEVICE
INDEPENDENT
BITMAP
FILES
USING
THIS
FORMAT
USE
THE
EXTENSION
BMP
THESE
FILES
HAVE
FILE
AND
INFORMATION
HEADERS
AND
A
COLOR
TABLE
BEFORE
THE
PIXELS
THIS
INFORMATION
MAKES
IT
EASIER
TO
MOVE
BITMAPS
BETWEEN
DISSIMILAR
DEVICES
FONTS
IN
VERSIONS
OF
WINDOWS
BEFORE
CHARACTERS
WERE
REPRESENTED
AS
BITMAPS
AND
COPIED
ONTO
THE
SCREEN
OR
PRINTER
USING
BITBLT
THE
PROBLEM
WITH
THAT
AS
WE
JUST
SAW
IS
THAT
A
BITMAP
THAT
MAKES
SENSE
ON
THE
SCREEN
IS
TOO
SMALL
FOR
THE
PRINT
ER
ALSO
A
DIFFERENT
BITMAP
IS
NEEDED
FOR
EACH
CHARACTER
IN
EACH
SIZE
IN
OTHER
INPUT
OUTPUT
CHAP
WORDS
GIVEN
THE
BITMAP
FOR
A
IN
POINT
TYPE
THERE
IS
NO
WAY
TO
COMPUTE
IT
FOR
POINT
TYPE
BECAUSE
EVERY
CHARACTER
OF
EVERY
FONT
MIGHT
BE
NEEDED
FOR
SIZES
RANGING
FROM
POINT
TO
POINT
A
VAST
NUMBER
OF
BITMAPS
WERE
NEEDED
THE
WHOLE
SYSTEM
WAS
JUST
TOO
CUMBERSOME
FOR
TEXT
THE
SOLUTION
WAS
THE
INTRODUCTION
OF
TRUETYPE
FONTS
WHICH
ARE
NOT
BITMAPS
BUT
OUTLINES
OF
THE
CHARACTERS
EACH
TRUETYPE
CHARACTER
IS
DEFINED
BY
A
SEQUENCE
OF
POINTS
AROUND
ITS
PERIMETER
ALL
THE
POINTS
ARE
RELATIVE
TO
THE
ORIGIN
USING
THIS
SYSTEM
IT
IS
EASY
TO
SCALE
THE
CHARACTERS
UP
OR
DOWN
ALL
THAT
HAS
TO
BE
DONE
IS
TO
MULTIPLY
EACH
COORDINATE
BY
THE
SAME
SCALE
FACTOR
IN
THIS
WAY
A
TRUE
TYPE
CHARACTER
CAN
BE
SCALED
UP
OR
DOWN
TO
ANY
POINT
SIZE
EVEN
FRACTIONAL
POINT
SIZES
ONCE
AT
THE
PROPER
SIZE
THE
POINTS
CA
N
BE
CONNECTED
USING
THE
WELL
KNOWN
FOLLOW
THE
DOTS
ALGORITHM
TAUGHT
IN
KINDERGARTEN
NOTE
THAT
MODERN
KINDERGARTENS
USE
SPLINES
FOR
SMOOTHER
RESULTS
AFTER
THE
OUTLINE
HAS
BEEN
COMPLETED
THE
CHAR
ACTER
CAN
BE
FILLED
IN
AN
EXAMPLE
OF
SOME
CHARACTERS
SCALED
TO
THREE
DIFFERENT
POINT
SIZES
IS
GIVEN
IN
FIG
P
ABCDEFGH
ONCE
THE
FILLED
CHARACTER
IS
AVAILABLE
IN
MATHEMATICAL
FORM
IT
CAN
BE
RASTER
IZED
THAT
IS
CONVERTED
TO
A
BITMAP
AT
WHATEVER
RESOLUTION
IS
DESIRED
BY
FIRST
SCAL
ING
AND
THEN
RASTERIZING
WE
CAN
BE
SURE
THAT
THE
CHARACTERS
DISPLAYED
ON
THE
SCREEN
AND
THOSE
THAT
APPEAR
ON
THE
PRINTER
WILL
BE
AS
CLOSE
AS
POSSIBLE
DIFFERING
ONLY
IN
QUANTIZATION
ERROR
TO
IMPROVE
THE
QUALITY
STILL
MORE
IT
IS
POSSIBLE
TO
EMBED
HINTS
IN
EACH
CHARACTER
TELLING
HOW
TO
DO
THE
RASTERIZATION
FOR
EXAMPLE
BOTH
SERIFS
ON
THE
TOP
OF
THE
LETTER
T
SHOULD
BE
IDENTICAL
SOMETHING
THAT
MIGHT
NOT
OTHERWISE
BE
THE
CASE
DUE
TO
ROUNDOFF
ERROR
HINTS
IMPROVE
THE
FINAL
APPEARANCE
SEC
THIN
CLIENTS
THI
N
CLIENT
S
OVER
THE
YEARS
THE
MAIN
COMPUTING
PARADIGM
HAS
OSCILLATED
BETWEEN
CENTRAL
IZED
AND
DECENTRALIZED
COMPUTING
THE
FIRST
COMPUTERS
SUCH
AS
THE
ENIAC
WERE
IN
FACT
PERSONAL
COMPUTERS
ALBEIT
LARGE
ONES
BECAUSE
ONLY
ONE
PERSON
COULD
USE
ONE
AT
ONCE
THEN
CAME
TIMESHARING
SYSTEMS
IN
WHICH
MANY
REMOTE
USERS
AT
SIM
PLE
TERMINALS
SHARED
A
BIG
CENTRAL
COMPUTER
NEXT
CAME
THE
PC
ERA
IN
WHICH
THE
USERS
HAD
THEIR
OWN
PERSONAL
COMPUTERS
AGAIN
WHILE
THE
DECENTRALIZED
PC
MODEL
HAS
ADVANTAGES
IT
ALSO
HAS
SOME
SEVERE
DISADVANTAGES
THAT
ARE
ONLY
BEGINNING
TO
BE
TAKEN
SERIOUSLY
PROBABLY
THE
BIGGEST
PROBLEM
IS
THAT
EACH
PC
HAS
A
LARGE
HARD
DISK
AND
COMPLEX
SOFTWARE
THAT
MUST
BE
MAINTAINED
FOR
EXAMPLE
WHEN
A
NEW
RELEASE
OF
THE
OPERATING
SYSTEM
COMES
OUT
A
GREAT
DEAL
OF
WORK
HAS
TO
BE
DONE
TO
PERFORM
THE
UPGRADE
ON
EACH
MACHINE
SEPA
RATELY
AT
MOST
CORPORATIONS
THE
LABOR
COSTS
OF
DOING
THIS
KIND
OF
SOFTWARE
MAIN
TENANCE
DWARF
THE
ACTUAL
HARDWARE
AND
SOFTWARE
COSTS
FOR
HOME
USERS
THE
LABOR
IS
TECHNICALLY
FREE
BUT
FEW
PEOPLE
ARE
CAPABLE
OF
DOING
IT
CORRECTLY
AND
FEWER
STILL
ENJOY
DOING
IT
WITH
A
CENTRALIZED
SYSTEM
ONLY
ONE
OR
A
FEW
MACHINES
HAVE
TO
BE
UPDATED
AND
THOSE
MACHINES
HAVE
A
STAFF
OF
EXPERTS
TO
DO
THE
WORK
A
RELATED
ISSUE
IS
THAT
USERS
SHOULD
MAKE
REGULAR
BACKUPS
OF
THEIR
GIGABYTE
FILE
SYSTEMS
BUT
FEW
OF
THEM
DO
WHEN
DISASTER
STRIKES
A
GREAT
DEAL
OF
MOANING
AND
WRINGING
OF
HANDS
TENDS
TO
FOLLOW
WITH
A
CENTRALIZED
SYSTEM
BACKUPS
CAN
BE
MADE
EVERY
NIGHT
BY
AUTOMATED
TAPE
ROBOTS
ANOTHER
ADVANTAGE
IS
THAT
RESOURCE
SHARING
IS
EASIER
WITH
CENTRALIZED
SYSTEMS
A
SYSTEM
WITH
REMOTE
USERS
EACH
WITH
MB
OF
RAM
WILL
HAVE
MOST
OF
THAT
RAM
IDLE
MOST
OF
THE
TIME
WITH
A
CENTRALIZED
SYSTEM
WITH
GB
OF
RAM
IT
NEVER
HAPPENS
THAT
SOME
USER
TEMPORARILY
NEEDS
A
LOT
OF
RAM
BUT
CANNOT
GET
IT
BECAUSE
IT
IS
ON
SOMEONE
ELSE
S
PC
THE
SAME
ARGUMENT
HOLDS
FOR
DISK
SPACE
AND
OTHER
RESOURCES
FINALLY
WE
ARE
STARTING
TO
SEE
A
SHIFT
FROM
PC
CENTRIC
COMPUTING
TO
WEB
CENTRIC
COMPUTING
ONE
AREA
WHERE
THIS
SHIFT
IS
VERY
FAR
ALONG
IS
E
MAIL
PEOPLE
USED
TO
GET
THEIR
E
MAIL
DELIVERED
TO
THEIR
HOME
MACHINE
AND
READ
IT
THERE
NOWA
DAYS
MANY
PEOPLE
LOG
INTO
GMAIL
HOTMAIL
OR
YAHOO
AND
READ
THEIR
MAIL
THERE
THE
NEXT
STEP
IS
FOR
PEOPLE
TO
LOG
INTO
OTHER
WEBSITES
TO
DO
WORD
PROCESSING
BUILD
SPREADSHEETS
AND
OTHER
THINGS
THAT
USED
TO
REQUIRE
PC
SOFTWARE
IT
IS
EVEN
POSSIBLE
THAT
EVENTUALLY
THE
ONLY
SOFTWARE
PEOPLE
RUN
ON
THEIR
PC
IS
A
WEB
BROWSER
AND
MAYBE
NOT
EVEN
THAT
IT
IS
PROBABLY
A
FAIR
CONCLUSION
TO
SAY
THAT
MOST
USERS
WANT
HIGH
PERFORMANCE
INTERACTIVE
COMPUTING
BUT
DO
NOT
REALLY
WANT
TO
ADMINISTER
A
COMPUTER
THIS
HAS
LED
RESEARCHERS
TO
REEXAMINE
TIMESHARING
USING
DUMB
TERMINALS
NOW
POLITELY
CALL
ED
THIN
CLIENTS
THAT
MEET
MODERN
TERMINAL
EXPECTATIONS
X
WAS
A
STEP
IN
THIS
DIRECTION
AND
DEDICATED
X
TERMINALS
WERE
POPULAR
FOR
A
LITTLE
WHILE
BUT
THEY
FELL
OUT
OF
FAVOR
BECAUSE
THEY
COST
AS
MUCH
AS
PCS
COULD
DO
LESS
AND
STILL
NEEDED
SOME
SOFTWARE
MAINTENANCE
THE
HOLY
GRAIL
WOULD
BE
A
HIGH
PERFORMANCE
INTERAC
INPUT
OUTPUT
CHAP
TIVE
COMPUTING
SYSTEM
IN
WHICH
THE
USER
MACHINES
HAD
NO
SOFTWARE
AT
ALL
INTEREST
INGLY
ENOUGH
THIS
GOAL
IS
ACHIEVABLE
BELOW
WE
WILL
DESCRIBE
ONE
SUCH
THIN
CLIENT
SYSTEM
CALLED
THINC
DEVELOPED
BY
RESEARCHERS
AT
COLUMBIA
UNIVERSITY
BARATTO
ET
AL
KIM
ET
AL
AND
LAI
AND
NIEH
THE
BASIC
IDEA
HERE
IS
TO
STRIP
THE
CLIENT
MACHINE
OF
ALL
IT
SMARTS
AND
SOFTWARE
AND
JUST
USE
IT
AS
A
DISPLAY
WITH
ALL
THE
COMPUTING
INCLUDING
BUILDING
THE
BITMAP
TO
BE
DISPLAYED
DONE
ON
THE
SERVER
SIDE
THE
PROTOCOL
BETWEEN
THE
CLIENT
AND
THE
SERVER
JUST
TELLS
THE
DISPLAY
HOW
TO
UPDATE
THE
VIDEO
RAM
NOTHING
MORE
FIVE
COMMANDS
ARE
USED
IN
THE
PROTOCOL
BETWEEN
THE
TWO
SIDES
THEY
ARE
LISTED
IN
FIG
COMMAND
DESCRIPTION
RAW
DISPLAY
RAW
PIXEL
DATA
AT
A
GIVEN
LOCATION
COPY
COPY
FRAME
BUFFER
AREA
TO
SPECIFIED
COORDINATES
SFILI
FILL
AN
AREA
WITH
A
GIVEN
PIXEL
COLOR
VALUE
PFILL
FILL
AN
AREA
WITH
A
GIVEN
PIXEL
PATTERN
BITMAP
FILL
A
REGION
USING
A
BITMAP
IMAGE
FIGURE
THE
THINC
PROTOCOL
DISPLAY
COMMANDS
LET
US
EXAMINE
THE
COMMANDS
NOW
RAW
IS
USED
TO
TRANSMIT
PIXEL
DATA
AND
HAVE
THEM
DISPLAY
VERBATIM
ON
THE
SCREEN
IN
PRINCIPLE
THIS
IS
THE
ONLY
COMMAND
NEEDED
THE
OTHERS
ARE
JUST
OPTIMIZATIONS
COPY
INSTRUCTS
THE
DISPLAY
TO
MOVE
DATA
FROM
ONE
PART
OF
ITS
VIDEO
RAM
TO
AN
OTHER
PART
IT
IS
USEFUL
FOR
SCROLLING
THE
SCREEN
WITHOUT
HAVING
TO
RETRANSMIT
ALL
THE
DATA
SFILL
FILLS
A
REGION
OF
THE
SCREEN
WITH
A
SINGLE
PIXEL
VALUE
MANY
SCREENS
HAVE
A
UNIFORM
BACKGROUND
IN
SOME
COLOR
AND
THIS
COMMAND
IS
USED
TO
FIRST
GENERATE
THE
BACKGROUND
AFTER
WHICH
TEXT
ICONS
AND
OTHER
ITEMS
CAN
BE
PAINTED
PFILL
REPLICATES
A
PATTERN
OVER
SOME
REGION
IT
IS
ALSO
USED
FOR
BACKGROUNDS
BUT
SOME
BACKGROUNDS
ARE
SLIGHTLY
MORE
COMPLEX
THAN
A
SINGLE
COLOR
IN
WHICH
CASE
THIS
COMMAND
DOES
THE
JOB
FINALLY
BITMAP
ALSO
PAINTS
A
REGION
BUT
WITH
A
FOREGROUND
COLOR
AND
A
BACK
GROUND
COLOR
ALL
IN
ALL
THESE
ARE
VERY
SIMPLE
COMMANDS
REQUIRING
VERY
LITTLE
SOFTWARE
ON
THE
CLIENT
SIDE
ALL
THE
COMPLEXITY
OF
BUILDING
THE
BITMAPS
THAT
FILL
THE
SCREEN
ARE
DONE
ON
THE
SERVER
TO
IMPROVE
EFFICIENCY
MULTIPLE
COMMANDS
CAN
BE
AGGREGATED
INTO
A
SINGLE
PACKET
FOR
TRANSMISSION
OVER
THE
NETWORK
FROM
SERVER
TO
CLIENT
ON
THE
SERVER
SIDE
GRAPHICAL
PROGRAMS
USE
HIGH
LEVEL
COMMANDS
TO
PAINT
THE
SCREEN
THESE
ARE
INTERCEPTED
BY
THE
THINC
SOFTWARE
AND
TRANSLATED
INTO
COM
MANDS
THAT
CAN
BE
SENT
TO
THE
CLIENT
THE
COMMANDS
MAY
BE
REORDERED
TO
IMPROVE
EFFICIENCY
SEC
THIN
CLIENTS
THE
PAPER
GIVES
EXTENSIVE
PERFORMANCE
MEASUREMENTS
RUNNING
NUMEROUS
COMMON
APPLICATIONS
ON
SERVERS
AT
DISTANCES
RANGING
FROM
KM
TO
KM
FROM
THE
CLIENT
IN
GENERAL
PERFORMANCE
EXCEEDED
OTHER
WIDE
AREA
NETWORK
SYS
TEMS
EVEN
FOR
REAL
TIME
VIDEO
FOR
MORE
INFORMATION
WE
REFER
YOU
TO
THE
PAPERS
POWER
MANAGEMENT
THE
FIRST
GENERAL
PURPOSE
ELECTRONIC
COMPUTER
THE
ENIAC
HAD
VACUUM
TUBES
AND
CONSUMED
WATTS
OF
POWER
AS
A
RESULT
IT
RAN
UP
A
NON
TRIVIAL
ELECTRICITY
BILL
AFTER
THE
INVENTION
OF
THE
TRANSISTOR
POWER
USAGE
DROPPED
DRAMATICALLY
AND
THE
COMPUTER
INDUSTRY
LOST
INTEREST
IN
POWER
REQUIREMENTS
HOW
EVER
NOWADAYS
POWER
MANAGEMENT
IS
BACK
IN
THE
SPOTLIGHT
FOR
SEVERAL
REASONS
AND
THE
OPERATING
SYSTEM
IS
PLAYING
A
ROLE
HERE
LET
US
START
WITH
DESKTOP
PCS
A
DESKTOP
PC
OFTEN
HAS
A
WATT
POWER
SUP
PLY
WHICH
IS
TYPICALLY
EFFICIENT
THAT
IS
LOSES
OF
THE
INCOMING
ENERGY
TO
HEAT
IF
MILLION
OF
THESE
MACHINES
ARE
TURNED
ON
AT
ONCE
WORLDWIDE
TOGETHER
TIIEY
USE
MEGAWATTS
OF
ELECTRICITY
THIS
IS
THE
TOTAL
OUTPUT
OF
AVERAGE
SIZED
NUCLEAR
POWER
PLANTS
IF
POWER
REQUIREMENTS
COULD
BE
CUT
IN
HALF
WE
COULD
GET
RID
OF
NUCLEAR
POWER
PLANTS
FROM
AN
ENVIRONMENTAL
POINT
OF
VIEW
GETTING
RID
OF
NUCLEAR
POWER
PLANTS
OR
AN
EQUIVALENT
NUMBER
OF
FOSSIL
FUEL
PLANTS
IS
A
BIG
WIN
AND
WELL
WORTH
PURSUING
THE
OTHER
PLACE
WHERE
POWER
IS
A
BIG
ISSUE
IS
ON
BATTERY
POWERED
COMPUTERS
INCLUDING
NOTEBOOKS
HANDHELDS
AND
WEBPADS
AMONG
OTHERS
THE
HEART
OF
THE
PROBLEM
IS
THAT
THE
BATTERIES
CANNOT
HOLD
ENOUGH
CHARGE
TO
LAST
VERY
LONG
A
FEW
HOURS
AT
MOST
FURTHERMORE
DESPITE
MASSIVE
RESEARCH
EFFORTS
BY
BATTERY
COM
PANIES
COMPUTER
COMPANIES
AND
CONSUMER
ELECTRONICS
COMPANIES
PROGRESS
IS
GLACIAL
TO
AN
INDUSTRY
USED
TO
A
DOUBLING
OF
PERFORMANCE
EVERY
MONTHS
MOORE
S
LAW
HAVING
NO
PROGRESS
AT
ALL
SEEMS
LIKE
A
VIOLATION
OF
THE
LAWS
OF
PHY
SICS
BUT
THAT
IS
THE
CURRENT
SITUATION
AS
A
CONSEQUENCE
MAKING
COMPUTERS
USE
LESS
ENERGY
SO
EXISTING
BATTERIES
LAST
LONGER
IS
HIGH
ON
EVERYONE
S
AGENDA
THE
OP
ERATING
SYSTEM
PLAYS
A
MAJOR
ROLE
HERE
AS
WE
WILL
SEE
BELOW
AT
THE
LOWEST
LEVEL
HARDWARE
VENDORS
ARE
TRYING
TO
MAKE
THEIR
ELECTRONICS
MORE
ENERGY
EFFICIENT
TECHNIQUES
USED
INCLUDE
REDUCING
TRANSISTOR
SIZE
EMPLOY
ING
DYNAMIC
VOLTAGE
SCALING
USING
LOW
SWING
AND
ADIABATIC
BUSES
AND
SIMILAR
TECHNIQUES
THESE
ARE
OUTSIDE
THE
SCOPE
OF
THIS
BOOK
BUT
INTERESTED
READERS
CAN
FIND
A
GOOD
SURVEY
IN
A
PAPER
BY
VENKATACHALAM
AND
FRANZ
THERE
ARE
TWO
GENERAL
APPROACHES
TO
REDUCING
ENERGY
CONSUMPTION
THE
FIRST
ONE
IS
FOR
THE
OPERATING
SYSTEM
TO
TURN
OFF
PARTS
OF
THE
COMPUTER
MOSTLY
I
O
DE
VICES
WHEN
THEY
ARE
NOT
IN
USE
BECAUSE
A
DEVICE
THAT
IS
OFF
USES
LITTLE
OR
NO
ENER
GY
THE
SECOND
ONE
IS
FOR
THE
APPLICATION
PROGRAM
TO
USE
LESS
ENERGY
POSSIBLY
DEGRADING
THE
QUALITY
OF
THE
USER
EXPERIENCE
IN
ORDER
TO
STRETCH
OUT
BATTERY
TIME
WE
WILL
LOOK
AT
EACH
OF
THESE
APPROACHES
IN
TURN
BUT
FIRST
WE
WILL
SAY
A
LITTLE
BIT
ABOUT
HARDWARE
DESIGN
WITH
RESPECT
TO
POWER
USAGE
INPUT
OUTPUT
CHAP
HARDWARE
ISSUES
BATTERIES
COME
IN
TWO
GENERAL
TYPES
DISPOSABLE
AND
RECHARGEABLE
DISPOSABLE
BATTERIES
MOST
COMMONLY
AAA
AA
AND
D
CELLS
CAN
BE
USED
TO
RUN
HANDHELD
DEVICES
BUT
DO
NOT
HAVE
ENOUGH
ENERGY
TO
POWER
NOTEBOOK
COMPUTERS
WITH
LARGE
BRIGHT
SCREENS
A
RECHARGEABLE
BATTERY
IN
CONTRAST
CAN
STORE
ENOUGH
ENERGY
TO
POWER
A
NOTEBOOK
FOR
A
FEW
HOURS
NICKEL
CADMIUM
BATTERIES
USED
TO
DOMINATE
HERE
BUT
THEY
GAVE
WAY
TO
NICKEL
METAL
HYDRIDE
BATTERIES
WHICH
LAST
LONGER
AND
DO
NOT
POLLUTE
THE
ENVIRONMENT
QUITE
AS
BADLY
WHEN
THEY
ARE
EVENTUALLY
DISCARDED
LITHIUM
ION
BATTERIES
ARE
EVEN
BETTER
AND
MAY
BE
RECHARGED
WITHOUT
FIRST
BEING
FULLY
DRAINED
BUT
THEIR
CAPACITIES
ARE
ALSO
SEVERELY
LIMITED
THE
GENERAL
APPROACH
MOST
COMPUTER
VENDORS
TAKE
TO
BATTERY
CONSERVADON
IS
TO
DESIGN
THE
CPU
MEMORY
AND
I
O
DEVICES
TO
HAVE
MULTIPLE
STATES
ON
SLEEPING
HIBERNATING
AND
OFF
TO
USE
THE
DEVICE
IT
MUST
BE
ON
WHEN
THE
DEVICE
WILL
NOT
BE
NEEDED
FOR
A
SHORT
TIME
IT
CAN
BE
PUT
TO
SLEEP
WHICH
REDUCES
ENERGY
CONSUMP
TION
WHEN
IT
IS
NOT
EXPECTED
TO
BE
NEEDED
FOR
A
LONGER
INTERVAL
IT
CAN
BE
MADE
TO
HIBERNATE
WHICH
REDUCES
ENERGY
CONSUMPTION
EVEN
MORE
THE
TRADE
OFF
HERE
IS
THAT
GETTING
A
DEVICE
OUT
OF
HIBERNATION
OFTEN
TAKES
MORE
TIME
AND
ENERGY
THAN
GET
TING
IT
OUT
OF
SLEEP
STATE
FINALLY
WHEN
A
DEVICE
IS
OFF
IT
DOES
NOTHING
AND
CON
SUMES
NO
POWER
NOT
ALL
DEVICES
HAVE
ALL
THESE
STATES
BUT
WHEN
THEY
DO
IT
IS
UP
TO
THE
OPERATING
SYSTEM
TO
MANAGE
THE
STATE
TRANSITIONS
AT
THE
RIGHT
MOMENTS
SOME
COMPUTERS
HAVE
TWO
OR
EVEN
THREE
POWER
BUTTONS
ONE
OF
THESE
MAY
PUT
THE
WHOLE
COMPUTER
IN
SLEEP
STATE
FROM
WHICH
IT
CAN
BE
AWAKENED
QUICKLY
BY
TYP
ING
A
CHARACTER
OR
MOVING
THE
MOUSE
ANOTHER
MAY
PUT
THE
COMPUTER
INTO
HIBERNA
TION
FROM
WHICH
WAKEUP
TAKES
MUCH
LONGER
IN
BOTH
CASES
THESE
BUTTONS
TYPI
CALLY
DO
NOTHING
EXCEPT
SEND
A
SIGNAL
TO
THE
OPERATING
SYSTEM
WHICH
DOES
THE
REST
IN
SOFTWARE
IN
SOME
COUNTRIES
ELECTRICAL
DEVICES
MUST
BY
LAW
HAVE
A
MECHANI
CAL
POWER
SWITCH
THAT
BREAKS
A
CIRCUIT
AND
REMOVES
POWER
FROM
THE
DEVICE
FOR
SAFETY
REASONS
TO
COMPLY
WITH
THIS
LAW
ANOTHER
SWITCH
MAY
BE
NEEDED
POWER
MANAGEMENT
BRINGS
UP
A
NUMBER
OF
QUESTIONS
THAT
THE
OPERATING
SYSTEM
MUST
DEAL
WITH
MANY
OF
THEM
DEAL
WITH
RESOURCE
HIBERNATION
SELECTIVELY
AND
TEMPORARILY
TURNING
OFF
DEVICES
OR
AT
LEAST
REDUCING
THEIR
POWER
CONSUMPTION
WHEN
THEY
ARE
IDLE
QUESTIONS
THAT
MUST
BE
ANSWERED
INCLUDE
THESE
WHICH
DEVICES
CAN
BE
CONTROLLED
ARE
THEY
ON
OFF
OR
DO
THEY
HAVE
INTERMEDIATE
STATES
HOW
MUCH
POWER
IS
SAVED
IN
THE
LOW
POWER
STATES
IS
ENERGY
EXPENDED
TO
RESTART
THE
DEVICE
MUST
SOME
CONTEXT
BE
SAVED
WHEN
GOING
TO
A
LOW
POWER
STATE
HOW
LONG
DOES
IT
TAKE
TO
GO
BACK
TO
FULL
POWER
OF
COURSE
THE
ANSWERS
TO
THESE
QUES
TIONS
VARY
FROM
DEVICE
TO
DEVICE
SO
THE
OPERATING
SYSTEM
MUST
BE
ABLE
TO
DEAL
WITH
A
RANGE
OF
POSSIBILITIES
VARIOUS
RESEARCHERS
HAVE
EXAMINED
NOTEBOOK
COMPUTERS
TO
SEE
WHERE
THE
POWER
GOES
LI
ET
AL
MEASURED
VARIOUS
WORKLOADS
AND
CAME
TO
THE
CONCLU
SIONS
SHOWN
IN
FIG
LORCH
AND
SMITH
MADE
MEASUREMENTS
ON
OTHER
MACHINES
AND
CAME
TO
THE
CONCLUSIONS
SHOWN
IN
FIG
WEISER
ET
AL
SEC
POW
CHAPTER
ARRAYS
QUALIFIERS
AND
READING
NUMBERS
THAT
MYSTERIOUS
INDEPENDENT
VARIABLE
OF
POLITICAL
CALCULATIONS
PUBLIC
OPINION
THOMAS
HENRY
HUXLEY
ARRAYS
IN
CONSTRUCTING
OUR
BUILDING
WE
HAVE
IDENTIFIED
EACH
BRICK
VARIABLE
BY
NAME
THAT
PROCESS
IS
FINE
FOR
A
SMALL
NUMBER
OF
BRICKS
BUT
WHAT
HAPPENS
WHEN
WE
WANT
TO
CONSTRUCT
SOMETHING
LARGER
WE
WOULD
LIKE
TO
POINT
TO
A
STACK
OF
BRICKS
AND
SAY
THAT
FOR
THE
LEFT
WALL
THAT
BRICK
BRICK
BRICK
ARRAYS
ALLOW
US
TO
DO
SOMETHING
SIMILAR
WITH
VARIABLES
AN
ARRAY
IS
A
SET
OF
CONSECUTIVE
MEMORY
LOCATIONS
USED
TO
STORE
DATA
EACH
ITEM
IN
THE
ARRAY
IS
CALLED
AN
ELEMENT
THE
NUMBER
OF
ELEMENTS
IN
AN
ARRAY
IS
CALLED
THE
DIMENSION
OF
THE
ARRAY
A
TYPICAL
ARRAY
DECLARATION
IS
LIST
OF
DATA
TO
BE
SORTED
AND
AVERAGED
INT
THE
ABOVE
EXAMPLE
DECLARES
TO
BE
AN
ARRAY
OF
THREE
ELEMENTS
AND
ARE
SEPARATE
VARIABLES
TO
REFERENCE
AN
ELEMENT
OF
AN
ARRAY
YOU
USE
A
NUMBER
CALLED
THE
INDEX
THE
NUMBER
INSIDE
THE
SQUARE
BRACKETS
C
IS
A
FUNNY
LANGUAGE
THAT
LIKES
TO
START
COUNTING
AT
SO
OUR
THREE
ELEMENTS
ARE
NUMBERED
TO
EXAMPLE
COMPUTES
THE
TOTAL
AND
AVERAGE
OF
FIVE
NUMBERS
EXAMPLE
ARRAY
ARRAY
C
FILE
ARRAY
ARRAY
C
INCLUDE
STDIO
H
FLOAT
DATA
DATA
TO
AVERAGE
AND
TOTAL
FLOAT
TOTAL
THE
TOTAL
OF
THE
DATA
ITEMS
FLOAT
AVERAGE
AVERAGE
OF
THE
ITEMS
INT
MAIN
DATA
DATA
DATA
DATA
DATA
TOTAL
DATA
DATA
DATA
DATA
DATA
AVERAGE
TOTAL
PRINTF
TOTAL
F
AVERAGE
F
N
TOTAL
AVERAGE
RETURN
THIS
PROGRAM
OUTPUTS
TOTAL
AVERAGE
STRINGS
STRINGS
ARE
SEQUENCES
OF
CHARACTERS
C
DOES
NOT
HAVE
A
BUILT
IN
STRING
TYPE
INSTEAD
STRINGS
ARE
CREATED
OUT
OF
CHARACTER
ARRAYS
IN
FACT
STRINGS
ARE
JUST
CHARACTER
ARRAYS
WITH
A
FEW
RESTRICTIONS
ONE
OF
THESE
RESTRICTIONS
IS
THAT
THE
SPECIAL
CHARACTER
NUL
IS
USED
TO
INDICATE
THE
END
OF
A
STRING
FOR
EXAMPLE
CHAR
N
AME
INT
MAIN
NAME
S
NAME
A
NAME
M
NAME
RETURN
THIS
CODE
CREATES
A
CHARACTER
ARRAY
OF
FOUR
ELEMENTS
NOTE
THAT
WE
HAD
TO
ALLOCATE
ONE
CHARACTER
FOR
THE
END
OF
STRING
MARKER
STRING
CONSTANTS
CONSIST
OF
TEXT
ENCLOSED
IN
DOUBLE
QUOTES
YOU
MAY
HAVE
NOTICED
THAT
THE
FIRST
PARAMETER
TOPRINTF
IS
A
STRING
CONSTANT
C
DOES
NOT
ALLOW
ONE
ARRAY
TO
BE
ASSIGNED
TO
ANOTHER
SO
WE
CAN
T
WRITE
AN
ASSIGNMENT
OF
THE
FORM
NAME
SAM
ILLEGAL
INSTEAD
WE
MUST
USE
THE
STANDARD
LIBRARY
FUNCTION
STRCPY
TO
COPY
THE
STRING
CONSTANT
INTO
THE
VARIABLE
STRCPY
COPIES
THE
WHOLE
STRING
INCLUDING
THE
END
O
F
STRING
CHARACTER
TO
INITIALIZE
THE
VARIABLE
NAME
TO
SAM
WE
WOULD
WRITE
INCLUDE
STRING
H
CHAR
NAME
INT
MAIN
STRCPY
NAME
SAM
LEGAL
RETURN
C
USES
VARIABLE
LENGTH
STRINGS
FOR
EXAMPLE
THE
DECLARATION
INCLUDE
STRING
H
CHAR
STRING
INT
MAIN
STRCPY
STRING
SAM
CREATE
AN
ARRAY
STRING
THAT
CAN
CONTAIN
UP
TO
CHARACTERS
THE
SIZE
OF
THE
ARRAY
IS
BUT
THE
LENGTH
OF
THE
STRING
IS
ANY
STRING
UP
TO
CHARACTERS
LONG
CAN
BE
STORED
IN
STRING
ONE
CHARACTER
IS
RESERVED
FOR
THE
NUL
THAT
INDICATES
END
O
F
STRING
THERE
ARE
SEVERAL
STANDARD
ROUTINES
THAT
WORK
ON
STRING
VARIABLES
AS
SHOWN
INTABLE
TABLE
PARTIAL
LIST
OF
STRING
FUNCTIONS
FUNCTION
DESCRIPTION
STRCPY
COPY
INTO
STRCAT
CONCATENATE
ONTO
THE
END
OF
LENGTH
STRLEN
STRING
GET
THE
LENGTH
OF
A
STRING
STRCMP
EQUALS
OTHERWISE
NONZERO
THE
PRINTF
FUNCTION
USES
THE
CONVERSION
FOR
PRINTING
STRING
VARIABLES
AS
SHOWN
IN
EXAMPLE
EXAMPLE
STR
STR
C
INCLUDE
STRING
H
INCLUDE
STDIO
H
CHAR
NAME
FIRST
NAME
OF
SOMEONE
INT
MAIN
STRCPY
NAME
SAM
INITIALIZE
THE
NAME
PRINTF
THE
NAME
IS
N
NAME
RETURN
EXAMPLE
TAKES
A
FIRST
NAME
AND
A
LAST
NAME
AND
COMBINES
THE
TWO
STRINGS
THE
PROGRAM
WORKS
BY
INITIALIZING
THE
VARIABLE
FIRST
TO
THE
FIRST
NAME
STEVE
THE
LAST
NAME
OUALLINE
IS
PUT
IN
THE
VARIABLELAST
TO
CONSTRUCT
THE
FULL
NAME
THE
FIRST
NAME
IS
COPIED
INTO
THEN
STRCAT
IS
USED
TO
ADD
A
SPACE
WE
CALLSTRCAT
AGAIN
TO
TACK
ON
THE
LAST
NAME
THE
DIMENSION
OF
THE
STRING
VARIABLE
IS
BECAUSE
WE
KNOW
THAT
NO
ONE
WE
ARE
GOING
TO
ENCOUNTER
HAS
A
NAME
MORE
THAN
CHARACTERS
LONG
IF
WE
GET
A
NAME
MORE
THAN
CHARACTERS
LONG
OUR
PROGRAM
WILL
MESS
UP
WHAT
ACTUALLY
HAPPENS
IS
THAT
YOU
WRITE
INTO
MEMORY
THAT
YOU
SHOULDN
T
ACCESS
THIS
ACCESS
CAN
CAUSE
YOUR
PROGRAM
TO
CRASH
RUN
NORMALLY
AND
GIVE
INCORRECT
RESULTS
OR
BEHAVE
IN
OTHER
UNEXPECTED
WAYS
EXAMPLE
FULL
FULL
C
INCLUDE
STRING
H
INCLUDE
STDIO
H
CHAR
FIRST
FIRST
NAME
CHAR
LAST
LAST
NAME
CHAR
FULL
VERSION
OF
FIRST
AND
LAST
NAME
INT
MAIN
STRCPY
FIRST
STEVE
INITIALIZE
FIRST
NAME
STRCPY
LAST
OUALLINE
INITIALIZE
LAST
NAME
STRCPY
FIRST
FULL
STEVE
NOTE
STRCAT
NOT
STRCPY
STRCAT
FULL
STEVE
STRCAT
LAST
FULL
STEVE
OUALLINE
PRINTF
THE
FULL
NAME
IS
N
RETURN
THE
OUTP
UT
OF
THIS
PROGRAM
IS
THE
FULL
NAME
IS
STEVE
OUALLINE
READING
STRINGS
THE
STANDARD
FUNCTION
FGETS
CAN
BE
USED
TO
READ
A
STRING
FROM
THE
KEYBOARD
THE
GENERAL
FORM
OF
AN
FGETS
CALL
IS
FGETS
NAME
SIZEOF
NAME
STDIN
WHERE
NAME
IDENTIFIES
A
STRINGVARIABLE
FGETS
WILL
BE
EXPLAINED
IN
DETAIL
IN
CHAPTER
THE
ARGUMENTS
ARE
NAME
IS
THE
NAME
OF
A
CHARACTER
ARRAY
THE
LINE
INCLUDING
THE
END
O
F
LINE
CHARACTER
IS
READ
INTO
THIS
ARRAY
SIZEOF
NAME
INDICATES
THE
MAXIMUM
NUMBER
OF
CHARACTERS
TO
READ
PLUS
ONE
FOR
THE
END
O
F
STRING
CHARACTER
THE
SIZEOF
FUNCTION
PROVIDES
A
CONVENIENT
WAY
OF
LIMITING
THE
NUMBER
OF
CHARACTERS
READ
TO
THE
MAXIMUM
NUMBERS
THAT
THE
VARIABLE
CAN
H
OLD
THIS
FUNCTION
WILL
BE
DISCUSSED
IN
MORE
DETAIL
IN
CHAPTER
STDIN
IS
THE
FILE
TO
READ
IN
THIS
CASE
THE
FILE
IS
THE
STANDARD
INPUT
OR
KEYBOARD
OTHER
FILES
ARE
DISCUSSED
I
N
CHAPTER
EXAMPLE
READS
A
LINE
FROM
THE
KEYBOARD
AND
REPORTS
ITS
LENGTH
EXAMPLE
LENGTH
LENGTH
C
INCLUDE
STRING
H
INCLUDE
STDIO
H
CHAR
LINE
LINE
WE
ARE
LOOKING
AT
INT
MAIN
PRINTF
ENTER
A
LINE
FGETS
LINE
SIZEOF
LINE
STDIN
PRINTF
THE
LENGTH
OF
THE
LINE
IS
D
N
STRLEN
LINE
RETURN
WHEN
WE
RUN
THIS
PROGRAM
WE
GET
ENTER
A
LINE
TEST
THE
LENGTH
OF
THE
LINE
IS
BUT
THE
STRING
TEST
IS
ONLY
FOUR
CHARACTERS
WHERE
THE
EXTRA
CHARACTER
COMING
FROM
FGETS
INCLUDES
THE
END
OF
LINE
IN
THE
STRING
SO
THE
FIFTH
CHARACTER
IS
NEWLINE
N
SUPPOSE
WE
WANTED
TO
CHANGE
OUR
NAME
PROGRAM
TO
ASK
THE
USER
FOR
HIS
FIRST
AND
LAST
NAME
EXAMPLE
SHOWS
HOW
WE
COULD
WRITE
THE
PROGRAM
EXAMPLE
C
INCLUDE
STDIO
H
INCLUDE
STRING
H
CHAR
FIRST
FIRST
NAME
OF
PERSON
WE
ARE
WORKING
WITH
CHAR
LAST
HIS
LAST
NAME
FIRST
AND
LAST
NAME
OF
THE
PERSON
COMPUTED
CHAR
FULL
INT
MAIN
PRINTF
ENTER
FIRST
NAME
FGETS
FIRST
SIZEOF
FIRST
STDIN
PRINTF
ENTER
LAST
NAME
FGETS
LAST
SIZEOF
LAST
STDIN
STRCPY
FULL
FIRST
STRCAT
FULL
STRCAT
FULL
LAST
PRINTF
THE
NAME
IS
N
FULL
RETURN
HOWEVER
WHEN
WE
RUN
THIS
PROGRAM
WE
GET
THE
RESULTS
ENTER
FIRST
NAME
JOHN
ENTER
LAST
NAME
DOE
THE
NAME
IS
JOHN
DOE
WHAT
WE
WANTED
WAS
JOHN
DOE
ON
THE
SAME
LINE
WHAT
HAPPENED
THE
FGETS
FUNCTION
GETS
THE
ENTIRE
LINE
INCLUDING
THE
END
OF
LINE
WE
MUST
GET
RID
OF
THIS
CHARACTER
BEFORE
PRINTING
FOR
EXAMPLE
THE
NAME
JOHN
WOULD
BE
STORED
AS
FIRST
J
FIRST
O
FIRST
H
FIRST
N
FIRST
N
FIRST
END
OF
STRING
BY
SETTING
FIRST
TO
NUL
WE
CAN
SHORTEN
THE
STRING
BY
ONE
CHARACTER
AND
GET
RID
OF
THE
UNWANTED
NEWLINE
THIS
CHANGE
CAN
BE
DONE
WITH
THE
STATEMENT
FIRST
THE
PROBLEM
IS
THAT
THIS
METHOD
WILL
WORK
ONLY
FOR
FOUR
CHARACTER
NAMES
WE
NEED
A
GENERAL
ALGORIT
HM
TO
SOLVE
THIS
PROBLEM
THE
LENGTH
OF
THIS
STRING
IS
THE
INDEX
OF
THE
END
O
F
STRING
NULL
CHARACTER
THE
CHARACTER
BEFORE
IT
IS
THE
ONE
WE
WANT
TO
GET
RID
OF
SO
TO
TRIM
THE
STRING
WE
USE
THE
STATEMENT
FIRST
STRLEN
FIRST
OUR
NEW
PROGRAM
IS
SHOWN
IN
EXAMPLE
EXAMPLE
C
INCLUDE
STDIO
H
INCLUDE
STRING
H
CHAR
FIRST
FIRST
NAME
OF
PERSON
WE
ARE
WORKING
WITH
CHAR
LAST
HIS
LAST
NAME
FIRST
AND
LAST
NAME
OF
THE
PERSON
COMPUTED
CHAR
FULL
INT
MAIN
PRINTF
ENTER
FIRST
NAME
FGETS
FIRST
SIZEOF
FIRST
STDIN
TRIM
OFF
LAST
CHARACTER
FIRST
STRLEN
FIRST
PRINTF
ENTER
LAST
NAME
FGETS
LAST
SIZEOF
LAST
STDIN
TRIM
OFF
LAST
CHARACTER
LAST
STRLEN
LAST
STRCPY
FULL
FIRST
STRCAT
FULL
STRCAT
FULL
LAST
PRINTF
THE
NAME
IS
N
FULL
RETURN
RUNNING
THIS
PROGRAM
GIVES
US
THE
FOLLOWING
RESULTS
ENTER
FIRST
NAME
JOHN
ENTER
LAST
NAME
SMITH
THE
NAME
IS
JOHN
SMITH
MULTIDIMENSIONAL
ARRAYS
ARRAYS
CAN
HAVE
MORE
THAN
ONE
DIMENSION
THE
DECLARATION
FOR
A
TWO
DIMENSIONA
L
ARRAY
IS
TYPE
VARIABLE
COMMENT
FOR
EXAMPLE
INT
MATRIX
A
TYPICAL
MATRIX
NOTICE
THAT
C
DOES
NOT
FOLLOW
THE
NOTATION
USED
IN
OTHER
LANGUAGES
OF
MATRIX
TO
ACCESS
AN
ELEMENT
OF
THE
MATRIX
WE
USE
THE
NOTATION
MATRIX
C
ALLOWS
THE
PROGRAMMER
TO
USE
AS
MANY
DIMENSIONS
AS
NEEDED
LIMITED
ONLY
BY
THE
AMOUNT
OF
MEMORY
AVAILABLE
ADDITIONAL
DIMENSIONS
CAN
BE
TACKED
ON
QUESTION
WHY
DOES
EXAMPLE
PRINT
THE
WRONG
ANSWER
CLICK
HERE
FOR
THE
ANSWER
SECTION
EXAMPLE
C
INCLUDE
STDIO
H
INT
ARR
AY
ARRAY
OF
NUMBERS
INT
MAIN
INT
X
Y
LOOP
INDICIES
ARRAY
ARRAY
ARRAY
ARRAY
ARRAY
ARRAY
PRINTF
ARRAY
D
PRINTF
D
ARRAY
PRINTF
D
ARRAY
PRINTF
N
PRINTF
ARRAY
D
PRINTF
D
ARRAY
PRINTF
D
ARRAY
PRINTF
N
PRINTF
ARRAY
D
PRINTF
D
ARRAY
PRINTF
D
ARRAY
PRINTF
N
RETURN
READING
NUMBERS
SO
FAR
WE
HAVE
ONLY
READ
SIMPLE
STRINGS
BUT
WE
WANT
MORE
WE
WANT
TO
READ
NUMBERS
AS
WELL
THE
FUNCTION
SCANF
WORKS
LIKE
PRINTF
EXCEPT
THAT
SCANF
READS
NUMBERS
INSTEAD
OF
WRITING
THEM
SCANF
PROVIDES
A
SIMPLE
AND
EASY
WAY
OF
READING
NUMBERS
THAT
ALMOST
NEVER
WORKS
THE
FUNCTION
SCANF
IS
NOTORIOUS
FOR
ITS
POOR
END
O
F
LINE
HANDLING
WHICH
MAKES
SCANF
USELESS
FOR
ALL
BUT
AN
EXPERT
HOWEVER
WE
VE
FOUND
A
SIMPLE
WAY
TO
GET
AROUND
THE
DEFICIENCIES
OF
SCANF
WE
DON
T
USE
IT
INSTEAD
WE
USE
FGETS
TO
READ
A
LINE
OF
INPUT
ANDSSCANF
TO
CONVERT
THE
TEXT
INTO
NUMBERS
THE
NAME
SSCANF
STANDS
FOR
STRING
SCANF
SSCANF
IS
LIKE
SCANF
BUT
WORKS
ON
STRINGS
INSTEAD
OF
THE
STANDARD
INPUT
NORMALLY
WE
USE
THE
VARIABLE
LINE
FOR
LINES
READ
FROM
THE
KEYBOARD
CHAR
LINE
LINE
OF
KEYBOARD
INPUT
WHEN
WE
WANT
TO
PROCESS
INPUT
WE
USE
THE
STATEMENTS
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
FORMAT
HERE
FGETS
READS
A
LINE
AND
SSCANF
PROCESSES
IT
FORMAT
IS
A
STRING
SIMILAR
TO
THE
PRINTF
FORMAT
STRING
NOTE
THE
AMPERSAND
IN
FRONT
OF
THE
VARIABLE
NAMES
THIS
SYMBOL
IS
USED
TO
INDICATE
THAT
SSCANF
WILL
CHANGE
THE
VALUE
OF
THE
ASSOCIATED
VARIABLES
FOR
INFORMATION
ON
WHY
WE
NEED
THE
AMPERSAND
SEE
CHAPTER
IN
EXAMPLE
WE
USE
SSCANF
TO
GET
AND
THEN
DOUBLE
A
NUMBER
FROM
THE
USER
EXAMPLE
DOUBLE
DOUBLE
C
FILE
DOUBLE
DOUBLE
C
INCLUDE
STDIO
H
CHAR
LINE
INPUT
LINE
FROM
CONSOLE
INT
VALUE
A
VALUE
TO
DOUBLE
INT
MAIN
PRINTF
ENTER
A
VALUE
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
VALUE
PRINTF
TWICE
D
IS
D
N
VALUE
VALUE
RETURN
THIS
PROGRAM
READS
IN
A
SINGLE
NUMBER
AND
THEN
DOUBLES
IT
NOTICE
THAT
THERE
IS
NO
N
AT
THE
END
OF
ENTER
A
VALUE
THIS
OMISSION
IS
INTENTIONAL
BECAUSE
WE
DO
NOT
WANT
THE
COMPUTER
TO
PRINT
A
NEWLINE
AFTER
THE
PROMPT
FOR
EXAMPLE
A
SAMPLE
RUN
OF
THE
PROGRAM
MIGHT
LOOK
LIKE
ENTER
A
VALUE
TWICE
IS
IF
WE
REPLACED
ENTER
A
VALUE
WITH
ENTER
A
VALUE
N
THE
RESULT
WOULD
BE
ENTER
A
VALUE
TWICE
IS
QUESTION
EXAMPLE
COMPUTES
THE
AREA
OF
A
TRIANGLE
GIVEN
THE
TRIANGLE
WIDTH
AND
HEIGHT
FOR
SOME
STRANGE
REASON
THE
COMPILER
REFUSES
TO
BELIEVE
THAT
WE
DECLARED
THE
VARIABLE
WIDTH
THE
DECLARATION
IS
RIGHT
THERE
ON
LINE
JUST
AFTER
THE
DEFINITION
OF
HEIGHT
WHY
ISN
T
THE
COMPILER
SEEING
IT
CLICK
HERE
FOR
THE
ANSWER
SECTION
EXAMPLE
TRI
TRI
C
INCLUDE
STDIO
H
CHAR
LINE
LINE
OF
INPUT
DATA
INT
HEIGHT
THE
HEIGHT
OF
THE
TRIANGLE
INT
WIDTH
THE
WIDTH
OF
THE
TRIANGLE
INT
AREA
AREA
OF
THE
TRIANGLE
COMPUTED
INT
MAIN
PRINTF
ENTER
WIDTH
HEIGHT
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
D
WIDTH
HEIGHT
AREA
WIDTH
HEIGHT
PRINT
F
THE
AREA
IS
D
N
AREA
RETURN
INITIALIZING
VARIABLES
C
ALLOWS
VARIABLES
TO
BE
INITIALIZED
IN
THE
DECLARATION
STATEMENT
FOR
EXAMPLE
THE
FOLLOWING
STATEMENT
DECLARES
THE
INTEGER
COUNTER
AND
INITIALIZES
IT
TO
INT
COUNTER
NUMBER
CASES
COUNTED
SO
FAR
ARRAYS
CAN
ALSO
BE
INITIALIZED
IN
THIS
MANNER
THE
ELEMENT
LIST
MUST
BE
ENCLOSED
IN
CURLY
BRACES
FOR
EXAMPLE
PRODUCT
NUMBERS
FOR
THE
PARTS
WE
ARE
MAKING
INT
THE
PREVIOUS
INITIA
LIZATION
IS
EQUIVALENT
TO
THE
NUMBER
OF
ELEMENTS
IN
DOES
NOT
HAVE
TO
MATCH
THE
ARRAY
SIZE
IF
TOO
MANY
NUMBERS
ARE
PRESENT
A
WARNING
WILL
BE
ISSUED
IF
AN
INSUFFICIENT
AMOUNT
OF
N
UMBERS
ARE
PRESENT
C
WILL
INITIALIZE
THE
EXTRA
ELEMENTS
TO
IF
NO
DIMENSION
IS
GIVEN
C
WILL
DETERMINE
THE
DIMENSION
FROM
THE
NUMBER
OF
ELEMENTS
IN
THE
INITIALIZATION
LIST
FOR
EXAMPLE
WE
COULD
HAVE
INITIALIZED
OUR
VARIABLE
WITH
THE
STATE
MENT
PRODUCT
NUMBERS
FOR
THE
PARTS
WE
ARE
MAKING
INT
INITIALIZING
MULTIDIMENSIONAL
ARRAYS
IS
SIMILAR
TO
INITIALIZING
SINGLE
DIMENSION
ARRAYS
A
SET
OF
BRACKETS
ENCLOSES
EACH
DIMENSION
THE
DECLARATION
INT
MAT
RIX
A
TYPICAL
MATRIX
CAN
BE
THOUGHT
OF
AS
A
DECLARATION
OF
AN
ARRAY
OF
DIMENSION
WITH
ELEMENTS
THAT
ARE
ARRAYS
OF
DIMENSION
THIS
ARRAY
IS
INITIALIZED
AS
FOLLOWS
A
TYPICAL
MATRIX
INT
MATRIX
STRINGS
CAN
BE
INITIALIZED
IN
A
SIMILAR
MANNER
FOR
EXAMPLE
TO
INITIALIZE
THE
VARIABLE
NAME
TO
THE
STRING
SAM
WE
USE
THE
STATEMENT
CHAR
NAME
S
A
M
C
HAS
A
SPECIAL
SHORTHAND
FOR
INITIALIZING
STRIN
GS
SURROUND
THE
STRING
WITH
DOUBLE
QUOTES
TO
SIMPLIFY
INITIALIZATION
THE
PREVIOUS
EXAMPLE
COULD
HAVE
BEEN
WRITTEN
CHAR
NAME
SAM
THE
DIMENSION
OF
NAME
IS
BECAUSE
C
ALLOCATES
A
PLACE
FOR
THE
CHARACTER
THAT
ENDS
THE
STRING
THE
FOLLOWING
DECLARATION
CHAR
STRING
SAM
IS
EQUIVALENT
TO
CHAR
STRING
STRCPY
STRING
SAM
AN
ARRAY
OF
CHARACTERS
IS
ALLOCATED
BUT
THE
LENGTH
OF
THE
STRING
IS
TYPES
OF
INTEGERS
C
IS
CONSIDERED
A
MEDIUM
LEVEL
LANGUAGE
BECAUSE
IT
ALLOWS
YOU
TO
GET
VERY
CLOSE
TO
THE
ACTUAL
HARDWARE
OF
THE
MACHINE
SOME
LANGUAGES
LIKE
BASIC
GO
TO
GREAT
LENGTHS
TO
COMPLETELY
ISOLATE
THE
USER
FRO
M
THE
DETAILS
OF
HOW
THE
PROCESSOR
WORKS
THIS
SIMPLIFICATION
COMES
AT
A
GREAT
LOSS
OF
EFFICIENCY
C
LETS
YOU
GIVE
DETAILED
INFORMATION
ABOUT
HOW
THE
HARDWARE
IS
TO
BE
USED
SOME
MORE
ADVANCED
VERSIONS
OF
BASIC
DO
HAVE
NUMBER
TYPES
HOWEVER
FOR
THIS
E
XAMPLE
WE
ARE
TALKING
ABOUT
BASIC
BASIC
FOR
EXAMPLE
MOST
MACHINES
LET
YOU
USE
DIFFERENT
LENGTH
NUMBERS
BASIC
PROVIDES
THE
PROGRAMMER
WITH
ONLY
ONE
NUMERIC
TYPE
THOUGH
THIS
RESTRICTION
SIMPLIFIES
THE
PROGRAMMING
BASIC
PROGRAMS
ARE
EXTREMELY
INEFFICIEN
T
C
ALLOWS
THE
PROGRAMMER
TO
SPECIFY
MANY
DIFFERENT
FLAVORS
OF
INTEGERS
SO
THAT
THE
PROGRAMMER
CAN
MAKE
BEST
USE
OF
HARDWARE
THE
TYPE
SPECIFIER
INT
TELLS
C
TO
USE
THE
MOST
EFFICIENT
SIZE
FOR
THE
MACHINE
YOU
ARE
USING
FOR
THE
INTEGER
THIS
CAN
BE
TWO
TO
FOUR
BYTES
DEPENDING
ON
THE
MACHINE
SOME
LESS
COMMON
MACHINES
USE
STRANGE
INTEGER
SIZES
SUCH
AS
OR
BITS
SOMETIMES
YOU
NEED
EXTRA
DIGITS
TO
STORE
NUMBERS
LARGER
THAN
THOSE
ALLOWED
IN
A
NORMAL
INT
THE
DECLARATION
LONG
INT
ANSWER
THE
RESULT
OF
OUR
CALCULATIONS
IS
USED
TO
ALLOCATE
A
LONG
INTEGER
THE
LONG
QUALIFIER
INFORMS
C
THAT
WE
WISH
TO
ALLOCATE
EXTRA
STORAGE
FOR
THE
INTEGER
IF
WE
ARE
GOING
TO
USE
SMALL
NUMBERS
AND
WISH
TO
REDUCE
STORAGE
WE
USE
THE
QUALIFIER
SHORT
FOR
EXAMPLE
SHORT
INT
YEAR
YEAR
INCLUDING
THE
PART
C
GUARANTEES
THAT
THE
SIZE
OF
STORAGE
FOR
SHORT
INT
LONG
IN
ACTUAL
PRACTICE
SHORT
ALMOST
ALWAYS
ALLOCATES
TWO
BYTES
LONG
FOUR
BYTES
AND
INT
TWO
OR
FOUR
BYTES
SEE
APPENDIX
B
FOR
NUMERIC
RANGES
THE
TYPE
SHORT
INT
USUALLY
USES
BYTES
OR
BITS
BITS
ARE
USED
NORMALLY
FOR
THE
NUMBER
AND
BIT
FOR
THE
SIGN
THIS
FORMAT
GIVES
THE
TYPE
A
RANGE
OF
TO
AN
UNSIGNED
SHORT
INT
USES
ALL
BITS
FOR
THE
NUMBER
GIVING
IT
THE
RANGE
OF
TO
ALL
INT
DECLARATIONS
DEFAULT
TO
SIGNED
SO
THAT
THE
DECLARATION
SIGNED
LONG
INT
ANSWER
FINAL
RESULT
IS
THE
SAME
AS
LONG
INT
ANSWER
FINAL
RESULT
FINALLY
WE
CONSIDER
THE
VERY
SHORT
INTEGER
OF
TYPE
CHAR
CHARACTER
VARIABLES
USE
BYTE
THEY
CAN
ALSO
BE
USED
FOR
NUMBERS
IN
THE
RANGE
OF
TO
SIGNED
CHAR
OR
TO
UNSIGNED
CHAR
UNLIKE
INTEGERS
THEY
DO
NOT
DEFAULT
TO
SIGNED
THE
DEFAULT
IS
COMPILER
DEPENDENT
VERY
SHORT
INTEGERS
MAY
BE
PRINTED
USING
THE
INTEGER
CONVERSION
D
TURBO
C
AND
GNU
GCC
EVEN
HAVE
A
COMMAND
LINE
SWITCH
TO
MAKE
THE
DEFAULT
FOR
TYPECHAR
EITHER
SIGNED
OR
UNSIGNED
YOU
CANNOT
READ
A
VERY
SHORT
INTEGER
DIRECTLY
YOU
MUST
READ
THE
NUMBER
INTO
AN
INTEGER
AND
THEN
USE
AN
ASSIGNMENT
STATEMENT
FOR
EXAMPLE
INCLUDE
STDIO
H
SIGNED
CHAR
A
VERY
SHORT
INTEGER
CHAR
LINE
INPUT
BUFFER
INT
TEMP
A
TEMPORARY
NUMBER
INT
MAIN
READ
A
VERY
SHORT
INTEGER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
TEMP
TEMP
TABLE
CONTAINS
THE
PRINTF
AND
SSCANF
CONVERSIONS
FOR
INTEGERS
TABLE
INTEGER
PRINTF
SSCANF
CONVERSIONS
CONVERSION
USES
HD
SIGNED
SHORT
INT
D
SIGNED
INT
LD
SIGNED
LONG
INT
HU
UNSIGNED
SHORT
INT
U
UNSIGNED
INT
LU
UNSIGNED
LONG
INT
THE
RANGE
OF
THE
VARIOUS
FLAVORS
OF
INTEGERS
IS
LISTED
IN
APPENDIX
B
LONG
INT
DECLARATIONS
ALLOW
THE
PROGRAM
TO
EXPLICITLY
SPECIFY
EXTRA
PRECISION
WHERE
IT
IS
NEEDED
AT
THE
EXPENSE
OF
MEMORY
SHORT
INT
NUMBERS
SAVE
SPACE
BUT
HAVE
A
MORE
LIMITED
RANGE
THE
MOST
COMPACT
INTEGERS
HAVE
TYPE
CHAR
THEY
ALSO
HAVE
THE
MOST
LI
MITED
RANGE
UNSIGNED
NUMBERS
PROVIDE
A
WAY
OF
DOUBLING
THE
POSITIVE
RANGE
AT
THE
EXPENSE
OF
ELIMINATING
NEGATIVE
NUMBERS
THEY
ARE
ALSO
USEFUL
FOR
THINGS
THAT
CAN
NEVER
BE
NEGATIVE
LIKE
COUNTERS
AND
INDICES
THE
FLAVOR
OF
NUMBER
YOU
USE
WILL
DEPEND
ON
YO
UR
PROGRAM
AND
STORAGE
REQUIREMENTS
TYPES
OF
FLOATS
THE
FLOAT
TYPE
ALSO
COMES
IN
VARIOUS
FLAVORS
FLOAT
DENOTES
NORMAL
PRECISION
USUALLY
BYTES
DOUBLE
INDICATES
DOUBLE
PRECISION
USUALLY
BYTES
DOUBLE
PRECISION
VARIABLES
GIVE
THE
PROGRAMMER
MANY
TIMES
THE
RANGE
AND
PRECISION
OF
SINGLE
PRECISION
FLOAT
VARIABLES
THE
QUALIFIER
LONG
DOUBLE
DENOTES
EXTENDED
PRECISION
ON
SOME
SYSTEMS
THIS
IS
THE
SAME
AS
DOUBLE
ON
OTHERS
IT
OFFERS
ADDITIONAL
PRECISION
ALL
TYPES
OF
FLOATING
POINT
NUMBERS
ARE
A
LWAYS
SIGNED
TABLE
CONTAINS
THE
PRINTF
AND
SSCANF
CONVERSIONS
FOR
FLOATING
POINT
NUMBERS
TABLE
FLOAT
PRINTF
SSCANF
CONVERSIONS
CONVERSION
USES
NOTES
F
FLOAT
PRINTF
ONLY
LF
DOUBLE
SCANF
ONLY
LF
LONG
DOUBLE
NOT
AVAILABLE
ON
ALL
COMPILERS
THE
F
FORMAT
WORKS
FOR
PRINTING
DOUBLE
AND
FLOAT
BECAUSE
OF
AN
AUTOMATIC
CONVERSION
BUILT
INTO
C
PARAMETER
PASSING
ON
SOME
MACHINES
SINGLE
PRECISION
FLOATING
POINT
INSTRUCTIONS
EXECUTE
FASTER
BUT
LESS
ACCURATELY
THAN
DOUBLE
PRECISION
INSTRUCTIONS
DOUBLE
PRECISION
INSTRUCTIONS
GAIN
ACCURACY
AT
THE
EXPENSE
OF
TIME
AND
STORAGE
IN
MOST
CASES
FLOAT
IS
ADEQUATE
HOWEVER
IF
ACCURACY
IS
A
PROBLEM
SWITCH
TO
DOUBLE
SEE
CHAPTER
CONSTANT
DECLARATIONS
SOMETIMES
YOU
WANT
TO
USE
A
VALUE
THAT
DOES
NOT
CHANGE
SUCH
AS
THE
KEYWORD
CONST
INDICATES
A
VARIABLE
THAT
NEVER
CHANGES
FOR
EXAMPLE
TO
DECLARE
A
VALUE
FOR
PI
WE
USE
THE
STATEMENT
CONST
FLOAT
PI
THE
CLASSIC
CIRCLE
CONSTANT
CONSTANTS
MUST
BE
INITIALIZED
AT
DECLARATION
TIME
AND
CAN
NEVER
BE
CHANGED
FOR
EXAMPLE
IF
WE
TRIED
TO
RESET
THE
VALUE
OF
PI
TO
WE
WOULD
GENERATE
AN
ERROR
MESSAGE
PI
ILLEGAL
INTEGER
CONSTANTS
CAN
BE
USED
AS
A
SIZE
P
ARAMETER
WHEN
DECLARING
AN
ARRAY
MAX
NUMBER
OF
ELEMENTS
IN
THE
TOTAL
LIST
CONST
INT
FLOAT
TOTAL
VALUES
FOR
EACH
CATEGORY
HEXADECIMAL
AND
OCTAL
CONSTANTS
INTEGER
NUMBERS
ARE
SPECIFIED
AS
A
STRING
OF
DIGITS
SU
CH
AS
ETC
THESE
STRINGS
ARE
DECIMAL
BASE
NUMBERS
OR
COMPUTERS
DEAL
WITH
BINARY
BASE
NUMBERS
THE
OCTAL
BASE
SYSTEM
EASILY
CONVERTS
TO
AND
FROM
BINARY
EACH
GROUP
OF
THREE
DIGITS
CAN
BE
TRANSFORME
D
INTO
A
SINGLE
OCTAL
DIGIT
THUS
CAN
BE
WRITTEN
AS
AND
CHANGED
TO
THE
OCTAL
HEXADECIMAL
BASE
NUMBERS
HAVE
A
SIMILAR
CONVERSION
ONLY
BITS
ARE
USED
AT
A
TIME
THE
C
LANGUAGE
HAS
CONVENTIONS
FOR
REPRESENTING
OCTAL
AND
HEXADE
CIMAL
VALUES
LEADING
ZEROS
ARE
USED
TO
SIGNAL
AN
OCTAL
CONSTANT
FOR
EXAMPLE
IS
OCTAL
OR
DECIMAL
STARTING
A
NUMBER
WITH
INDICATES
A
HEXADECIMAL
BASE
CONSTANT
SO
IS
DECIMAL
TABLE
SHOWS
SEVERAL
NUMBERS
IN
ALL
THREE
BASES
TABLE
INTEGER
EXAMPLES
BASE
BASE
BASE
OPERATORS
FOR
PERFORMING
SHORTCUTS
C
NOT
ONLY
PROVIDES
YOU
WITH
A
RICH
SET
OF
DECLARATIONS
BUT
ALSO
GIVES
YOU
A
LARGE
NUMBER
OF
SPECIAL
PURPOSE
OPERATORS
FREQUENTLY
THE
PROGRAMMER
WANTS
TO
INCREMENT
INCREASE
BY
A
VARIABLE
USING
A
NORMAL
ASSIGNMENT
STATEMENT
THIS
OPERATION
WOULD
LOOK
LIKE
C
PROVIDES
US
WITH
A
SHORTHAND
FOR
PERFORMING
THIS
COMMON
TASK
THE
OPERATOR
IS
USED
FOR
INCREMENTING
A
SIMILAR
OPERATOR
CAN
BE
USED
FOR
DECREMENTING
DECREASING
BY
A
VARIABLE
IS
THE
SAME
AS
BUT
SUPPOSE
THAT
WE
WANT
TO
ADD
INSTEAD
OF
THEN
WE
CAN
USE
THE
FOLLOWING
NOTATION
THIS
NOTATION
IS
EQUIVALENT
TO
EACH
OF
THE
SIMPLE
OPERATORS
AS
SHOWN
IN
TABLE
CAN
BE
USED
IN
THIS
MANNER
TABLE
SHORTHAND
OPERATORS
OPERATOR
SHORTHAND
EQUIVALENT
STATEMENT
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
SIDE
EFFECTS
UNFORTUNATELY
C
ALLOWS
THE
PROGRAMMER
TO
USE
SIDE
EFFECTS
A
SIDE
EFFECT
IS
AN
OPERATION
THAT
IS
PERFORMED
IN
ADDITION
TO
THE
MAIN
OPERATION
EXECUTED
BY
THE
STATEMENT
FOR
EXAMPLE
THE
FOLLO
WING
IS
LEGAL
C
CODE
SIZE
RESULT
SIZE
THE
FIRST
STATEMENT
ASSIGNS
TOSIZE
THE
VALUE
OF
THE
SECOND
STATEMENT
ASSIGNS
TO
RESULT
THE
VALUE
OF
SIZE
MAIN
OPERATION
AND
INCREMENTS
SIZE
SIDE
EFFECT
BUT
IN
WHAT
ORDER
ARE
THESE
PROCESSES
PERFORMED
THERE
ARE
FOUR
POSSIBLE
ANSWERS
RESULT
IS
ASSIGNED
THE
VALUE
OF
SIZE
AND
THEN
SIZE
IS
INCREMENTED
RESULT
IS
AND
SIZE
IS
SIZE
IS
INCREMENTED
AND
THEN
RESULT
IS
ASSIGNED
THE
VALUE
OF
SIZE
RESULT
IS
AND
SIZE
IS
THE
ANSWER
IS
COMPILE
R
DEPENDENT
AND
VARIES
FROM
COMPUTER
TO
COMPUTER
IF
WE
DON
T
WRITE
CODE
LIKE
THIS
THEN
WE
DON
T
HAVE
TO
WORRY
ABOUT
SUCH
QUESTIONS
THE
CORRECT
ANSWER
IS
NUMBER
THE
INCREMENT
OCCURS
BEFORE
THE
ASSIGNMENT
HOWEVER
NUMBER
IS
A
MUCH
BETTER
ANSWER
TH
E
MAIN
EFFECTS
OF
C
ARE
CONFUSING
ENOUGH
WITHOUT
HAVING
TO
WORRY
ABOUT
SIDE
EFFECTS
C
ACTUALLY
PROVIDES
TWO
FLAVORS
OF
THE
OPERATOR
ONE
IS
VARIABLE
AND
THE
OTHER
IS
VARIABLE
THE
FIRST
NUMBER
RESULT
NUMBER
EVALUATES
THE
EXPRESSIONS
AND
THEN
INCREMENTS
THE
NUMBER
RESULT
IS
THE
SECOND
NUMBER
RESULT
NUMBER
INCREMENTS
THE
NUMBER
FIRST
AND
THEN
EVALUATES
THE
EXPRESSION
RESULT
IS
HOWEVER
USING
OR
IN
THIS
WAY
CAN
LEAD
TO
SOME
SURPRISING
CODE
O
O
O
THE
PROBLEM
WITH
THIS
LINE
IS
THAT
IT
LOOKS
AS
IF
SOMEONE
WROTE
MORSE
CODE
THE
PROGRAMMER
DOESN
T
READ
THIS
STATEMENT
BUT
RATHER
DECODES
IT
IF
WE
NEVER
USE
OR
AS
PART
OF
ANY
OTHER
STATEMENT
AND
INSTEAD
ALWAYS
PUT
THEM
ON
LINES
BY
THEMSELVES
THE
DIFFERENCE
BETWEEN
THE
TWO
FLAVORS
OF
THESE
OPERATORS
WILL
NOT
BE
NOTICEABLE
X
OR
X
THE
TWO
FORMS
OF
THE
INCREMENT
OPERATOR
ARE
CALLED
THE
PREFIX
FORM
X
AND
THE
POSTFIX
FORM
X
WHICH
FORM
SHOULD
YOU
USE
ACTUALLY
IN
C
YOUR
CHOICE
DOESN
T
MATTER
HOWEVER
IF
YOU
USE
C
WITH
ITS
OVERLOADABLE
OPERATORS
THE
PREFIX
VERSION
X
IS
MORE
EFFICIENT
SO
TO
DEVELOP
GOOD
HABITS
FOR
LEARNING
C
USE
THE
PREFIX
FORM
FOR
DETAILS
SEE
THE
BOOK
PRACTICAL
C
PROGRAMMING
O
REILLY
ASSOCIATES
CONSIDER
THE
IRONY
OF
A
LANGUAGE
WITH
ITS
NAME
IN
POSTFIX
FORM
C
WORKING
MORE
EFFICIENTLY
WITH
PREFIX
FORMS
OF
THE
INCREMENT
AND
DECREMENT
OPERATORS
MAYBE
THE
NAME
SHOULD
BE
C
MORE
SIDE
EFFECT
PROBLEMS
MORE
COMPLEX
SIDE
EFFECTS
CAN
CONFUSE
EVEN
THE
C
COMPILER
CONSIDER
THE
FOLLOWING
CODE
FRAGMENT
VALUE
RESULT
VALUE
VALUE
THIS
EXPRESSION
TELLS
C
TO
PERFORM
THE
FOLLOWING
STEPS
MULTIPLY
VALUE
BY
AND
ADD
TO
VALUE
MULTIPLY
VALUE
BY
AND
ADD
TO
VALUE
ADD
THE
RESULTS
OF
THE
TWO
MULTIPLICATIONS
TOGETHER
STEPS
AND
ARE
OF
EQUAL
PRIORITY
UNLIKE
IN
THE
PREVIOUS
E
XAMPLE
SO
THE
COMPILER
CAN
CHOOSE
THE
ORDER
OF
EXECUTION
SUPPOSE
THE
COMPILER
EXECUTES
STEP
FIRST
AS
SHOWN
IN
FIGURE
FIGURE
EXPRESSION
EVALUATION
METHOD
OR
SUPPOSE
THE
COMPILER
EXECUTES
STEP
FIRST
AS
SHOWN
IN
FIGURE
FIGURE
EXPRESSION
EVALUATION
METHOD
BY
USING
THE
FIRST
METHOD
WE
GET
A
RESULT
OF
BY
USING
THE
SECOND
METHOD
WE
GET
A
RESULT
OF
THE
RESULT
OF
THIS
EXPRESSION
IS
AMBIGUOUS
DEPENDING
ON
HOW
THE
COMPILER
WAS
IMPLEMENTED
THE
RESULT
MAY
BE
OR
EVEN
WORSE
SOME
COMPILERS
CHANGE
THE
BEHAVIOR
IF
OPTIMIZATION
IS
TURNED
ON
SO
WHAT
WAS
WORKING
CODE
MAY
BREAK
WHEN
OPTIMIZED
BY
USING
THE
OPERATOR
IN
THE
MIDDLE
OF
A
LARGER
EXPRESSION
WE
CREATED
A
PROBLEM
THIS
PROBLEM
IS
NOT
THE
ONLY
PROBLEM
THAT
AND
CAN
CAUSE
WE
WILL
GET
INTO
MORE
TROUBLE
IN
CHAPTER
IN
ORDER
TO
AVOID
TROUBLE
AND
KEEP
THE
PROGRAM
SIMPLE
ALWAYS
PUT
AND
ON
A
LINE
BY
THEMSELVES
ANSWERS
ANSWER
THE
PROBLEM
IS
THE
USE
OF
THE
EXPRESSION
ARRAY
X
Y
IN
THE
PRINTF
STATEMENT
PRINTF
D
ARRAY
X
Y
EACH
INDEX
TO
A
MULTIDIMENSION
ARRAY
MUST
BE
PLACED
INSIDE
ITS
OWN
SET
OF
SQUARE
BRACKETS
THE
STATEMENT
SHOULD
READ
PRINTF
D
ARRAY
X
Y
FOR
THOSE
OF
YOU
WHO
WANT
TO
READ
AHEAD
A
LITTLE
THE
COMMA
OPERATOR
CAN
BE
USED
TO
STRING
MULTIPLE
EXPRESSIONS
TOGETHER
THE
VALUE
OF
THIS
OPERATOR
IS
THE
VALUE
OF
THE
LAST
EXPRESSIONS
AS
A
RESULT
X
Y
IS
EQUIVALENT
TO
Y
AND
ARRAY
Y
IS
ACTUALLY
A
POINTER
TO
ROW
Y
OF
THE
ARRAY
BECAUSE
POINTERS
HAVE
STRANGE
VALUES
THE
PRINTF
OUTPUTS
STRANGE
RESULTS
SEE
CHAPTER
AND
CHAPTER
ANSWER
THE
PROGRAMMER
ACCIDENTALLY
OMITTED
THE
END
COMMENT
AFTER
THE
COMMENT
FOR
HEIGHT
THE
COMMENT
CONTINUES
ONTO
THE
NEXT
LINE
AND
ENGULFS
THE
DECLARATION
AS
SHOWN
IN
EXAMPLE
EXAMPLE
COMMENT
ANSWER
CONSIDER
ANOTHER
MINOR
PROBLEM
WITH
THIS
PROGRAM
IF
WIDTH
AND
HEIGHT
ARE
BOTH
ODD
WE
GET
AN
ANSWER
THAT
SLIGHTLY
WRONG
HOW
WOULD
YOU
CORRECT
THIS
ERROR
PROGRAMMING
EXERCISES
EXERCISE
WRITE
A
PROGRAM
T
HAT
CONVERTS
CENTIGRADE
TO
FAHRENHEIT
EXERCISE
WRITE
A
PROGRAM
TO
CALCULATE
THE
VOLUME
OF
A
SPHERE
EXERCISE
WRITE
A
PROGRAM
THAT
PRINTS
THE
PERIMETER
OF
A
RECTANGLE
GIVEN
ITS
HEIGHT
AND
WIDTH
PERIMETER
WIDTH
HEIGHT
EXERCISE
WRITE
A
PROGRAM
THAT
CONVERTS
KILOMETERS
PER
HOUR
TO
MILES
PER
H
OUR
MILES
KILOMETER
EXERCISE
WRITE
A
PROGRAM
THAT
TAKES
HOURS
AND
MINUTES
AS
INPUT
AND
THEN
OUTPUTS
THE
TOTAL
NUMBER
OF
MINUTES
HOUR
MINUTES
MINUTES
EXERCISE
WRITE
A
PROGRAM
THAT
TAKES
AN
INTEGER
AS
THE
NUMBER
OF
MINUTES
AND
OUTPUTS
THE
TOTAL
HOURS
AND
MINUTES
MINUTES
HOUR
MINUTES
CHAPTER
DECISION
AND
CONTROL
STATEMENTS
ONCE
A
DECISION
WAS
MADE
I
DID
NOT
WORRY
ABOUT
IT
AFTERWARD
HARRY
TRUMAN
CALCULATIONS
AND
EXPRESSIONS
ARE
ONLY
A
SMALL
PART
OF
COMPUTER
PROGRAMMING
DECISION
AND
CONTROL
STATEMENTS
ARE
NEEDED
THEY
SPECIFY
THE
ORDER
IN
WHICH
STATEMENTS
ARE
TO
BE
EXECUTED
SO
FAR
WE
HAVE
CONSTRUCTED
LINEAR
PROGRAMS
THAT
IS
PROGRAMS
THAT
EXECUTE
IN
A
STRAIGHT
LINE
ONE
STATEMENT
AFTER
ANOTHER
IN
THIS
CHAPTER
WE
WILL
SEE
HOW
TO
CHANGE
THE
CONTROL
FLOW
OF
A
PROGRAM
WITH
BRANCHING
STATEMENTS
AND
LOOPING
STATEMENTS
BRANCHING
STATEMENTS
CAUSE
ONE
SECTION
OF
CODE
TO
BE
EXECUTED
OR
NOT
EXECUTED
DEPENDING
ON
A
CONDITIONAL
CLAUSE
LOOPING
STATEMENTS
ARE
USED
TO
REPEAT
A
SECTION
OF
CODE
A
NUMBER
OF
TIMES
OR
UNTIL
SOME
CONDITION
OCCURS
IF
STATEMENT
THE
IF
STATEMENT
ALLOWS
US
TO
PUT
SOME
DECISION
MAKING
INTO
OUR
PROGRAMS
THE
GENERAL
FORM
OF
THE
IF
STATEMENT
IS
IF
CONDITION
STATEMENT
IF
THE
CONDITION
IS
TRUE
NONZERO
THE
STATEMENT
WILL
BE
EXECUTED
IF
THE
CONDITION
IS
FALSE
THE
STATEMENT
WILL
NOT
BE
EXECUTED
FOR
EXAMPLE
SUPPOSE
WE
ARE
WRITING
A
BILLING
PROGRAM
AT
THE
END
IF
THE
CUSTOMER
OWES
US
NOTHING
OR
HAS
A
CREDIT
OWES
US
A
NE
GATIVE
AMOUNT
WE
WANT
TO
PRINT
A
MESSAGE
IN
C
THIS
PROGRAM
IS
WRITTEN
IF
PRINTF
YOU
OWE
NOTHING
N
THE
OPERATOR
IS
A
RELATIONAL
OPERATOR
THAT
REPRESENTS
LESS
THAN
OR
EQUAL
TO
THIS
STATEMENT
READS
IF
THE
IS
LESS
THAN
OR
EQUAL
TO
ZERO
PRINT
THE
MESSAGE
THE
COMPLETE
LIST
OF
RELATIONAL
OPERATORS
IS
FOUND
IN
TABLE
TABLE
RELATIONAL
OPERATORS
OPERATOR
MEANING
LESS
THAN
OR
EQUAL
TO
LESS
THAN
GREATER
THAN
GREATER
THAN
OR
EQUAL
TO
EQUAL
NOT
EQUAL
THE
EQUAL
TEST
IS
DIFFERENT
FROM
THE
ASSIGNMENT
OPERATOR
ONE
OF
THE
MOST
COMMON
PROBLEMS
THE
C
PROGRAMMER
FACES
IS
MIXING
THEM
UP
MULTIPLE
STATEMENTS
MAY
BE
GROUPED
BY
PUTTING
THEM
INSIDE
CURLY
BRACES
FOR
EXAMPLE
IF
PRINTF
YOU
OWE
NOTHING
N
FOR
READABILITY
THE
STATEMENTS
ENCLOSED
IN
ARE
USUALLY
INDENTED
THIS
ALLOWS
THE
PROGRAMMER
TO
QUICKLY
TELL
WHICH
STATEMENTS
ARE
TO
BE
CONDITIONALLY
EXECUTED
AS
WE
WILL
SEE
LATER
MISTAKES
IN
INDENTATION
CAN
RESULT
IN
PROGRAMS
THAT
ARE
MISLEADING
AND
HARD
TO
READ
ELSE
STATEMENT
AN
ALTERNATE
FORM
OF
THE
IF
STATEMENT
IS
IF
CONDITION
STATEMENT
ELSE
STATEMENT
IF
THE
CONDITION
IS
TRUE
NONZERO
THE
FIRST
STATEMENT
IS
EXECUTED
IF
IT
IS
FALSE
THE
SECOND
STATEMENT
IS
EXECUTED
IN
OUR
ACCOUNTING
EXAMPLE
WE
WROTE
OUT
A
MESSAGE
ONLY
IF
NOTHING
WAS
OWED
IN
REAL
LIFE
WE
PROBABLY
WOULD
WANT
TO
TELL
THE
CUSTOMER
HOW
MUCH
IS
OWED
IF
THERE
IS
A
BALANCE
DUE
IF
PRINTF
YOU
OWE
NOTHING
N
ELSE
PRINTF
YOU
OWE
D
DOLLARS
N
NOW
CONSIDER
THIS
PROGRAM
FRAGMENT
WITH
INCORRECT
INDENTATION
IF
COUNT
IF
IF
COUNT
IF
PRINTF
CONDITION
WHITE
N
ELSE
PRINTF
CONDITION
TAN
N
THERE
ARE
TWO
IF
STATEMENTS
AND
ONE
ELSE
WHICH
IF
DOES
THE
ELSE
BELONG
TO
IT
BELONGS
TO
IF
IT
BELONGS
TO
IF
IF
YOU
NEVER
WRITE
CODE
LIKE
THIS
DON
T
WORRY
ABOUT
THIS
SITUATION
THE
CORRECT
ANSWER
IS
C
ACCORDING
TO
THE
C
SYNTAX
RULES
THE
ELSE
GOES
WITH
THE
NEAREST
IF
SO
B
IS
SYNTA
CTICALLY
CORRECT
BUT
WRITING
CODE
LIKE
THIS
VIOLATES
THE
KISS
PRINCIPLE
KEEP
IT
SIMPLE
STUPID
WE
SHOULD
WRITE
CODE
AS
CLEARLY
AND
SIMPLY
AS
POSSIBLE
THIS
CODE
FRAGMENT
SHOULD
BE
WRITTEN
AS
IF
COUNT
IF
IF
COUNT
IF
PRINTF
CONDITION
WHITE
N
ELSE
PRINTF
CONDITION
TAN
N
IN
OUR
ORIGINAL
EXAMPLE
WE
COULD
NOT
CLEARLY
DETERMINE
WHICH
IF
STATEMENT
HAD
THE
ELSE
CLAUSE
HOWEVER
BY
ADDING
AN
EXTRA
SET
OF
BRACES
WE
IMPROVE
RE
ADABILITY
UNDERSTANDING
AND
CLARITY
HOW
NOT
TO
USE
STRCMP
THE
FUNCTION
STRCMP
COMPARES
TWO
STRINGS
AND
THEN
RETURNS
ZERO
IF
THEY
ARE
EQUAL
OR
NONZERO
IF
THEY
ARE
DIFFERENT
TO
CHECK
IF
TWO
STRINGS
ARE
EQUAL
WE
USE
THE
CODE
CHECK
TO
SEE
IF
IF
STRCMP
PRINTF
STRINGS
EQUAL
N
ELSE
PRINTF
STRINGS
NOT
EQUAL
N
SOME
PROGRAMMERS
OMIT
THE
COMMENT
AND
THE
CLAUSE
THESE
OMISSIONS
LEAD
TO
THE
FOLLOWING
CONFUSING
CODE
IF
STRCMP
PRINTF
AT
FIRST
GLANCE
THIS
PROGRAM
OBVIOUSLY
COMPARES
TWO
STRINGS
AND
EXECUTES
THE
PRINTF
STATEMENT
IF
THEY
ARE
EQUAL
UNFORTUNATELY
THE
OBVIOUS
IS
WRONG
IF
THE
STRINGS
ARE
EQUAL
STRCMP
RETURNS
AND
THEPRINTF
IS
NOT
EXECUTED
BECAUSE
OF
THIS
BACKWARD
BEHAVIOR
OF
STRCMP
YOU
SHOULD
BE
VERY
CAREFUL
IN
YOUR
USE
OF
STRCMP
AND
ALWAYS
COMMENT
ITS
USE
IT
ALSO
HELPS
TO
PUT
IN
A
COMMENT
EXPLAINING
WHAT
YOU
RE
DOING
LOOPING
STATEMENTS
LOOPING
STATEMENTS
ALLOW
THE
PROGRAM
TO
REPEAT
A
SECTION
OF
CODE
ANY
NUMBER
OF
TIMES
OR
UNTIL
SOME
CONDITION
OCCURS
FOR
EXAMPLE
LOOPS
ARE
USED
TO
COUNT
THE
NUMBER
OF
WORDS
IN
A
DOCUMENT
OR
TO
COUNT
THE
NUMBER
OF
ACCOUNTS
THAT
HAVE
PAST
DUE
BALANCES
WHILE
STATEMENT
THE
WHILE
STATEMENT
IS
USED
WHEN
THE
PROGRAM
NEEDS
TO
PERFORM
REPETITIVE
TASKS
THE
GENERAL
FORM
OF
A
WHILE
STATEMENT
IS
WHILE
CONDITION
STATEMENT
THE
PROGRAM
WILL
REPEATEDLY
EXECUTE
THE
STATEMENT
INSIDE
THE
WHILE
UNTIL
THE
CONDITION
BECOMES
FALSE
IF
THE
CONDITION
IS
INITIALLY
FALSE
THE
STATEMENT
WILL
NOT
BE
EXECUTED
FOR
EXAMPLE
EXAMPLE
LATER
IN
THIS
CHAPTER
WILL
COMPUTE
ALL
THE
FIBONACCI
NUMBERS
THAT
ARE
LESS
THAN
THE
FIBONACCI
SEQUENCE
IS
THE
TERMS
ARE
COMPUTED
FROM
THE
EQUATIONS
ETC
IN
GENERAL
TERMS
THIS
IS
F
N
FN
FN
THIS
IS
A
MATHEMATICAL
EQUATION
USING
MATHEMATICAL
VARIABLE
NAMES
FN
MATHEMATICIANS
USE
THIS
VERY
TERSE
STYLE
OF
NAMING
VARIABLES
IN
PROGRAMMING
TERSE
IS
DANGEROUS
SO
WE
TRANSLATE
THESE
NAMES
INTO
SOMETHING
VERBOSE
FOR
C
TABLE
SHOWS
THIS
TRANSLATION
TABLE
MATH
TO
C
NAME
TRANSLATION
MATH
STYLE
NAME
C
STYLE
NAME
FN
FN
FN
IN
C
CODE
THE
EQUATION
IS
EXPRESSED
AS
WE
WANT
TO
LOOP
UNTIL
OUR
CURRENT
TERM
IS
OR
LARGER
THE
WHILE
LOOP
WHILE
WILL
REPEAT
OUR
COMPUTATION
AND
PRINTING
UNTIL
WE
REACH
THIS
LIMIT
FIGURE
SHOWS
WHAT
HAPPENS
TO
THE
VARIABLE
DURINGTHE
EXECUTION
OF
THE
PROGRAM
AT
THE
BEGINNING
AND
ARE
WE
PRINT
THE
VALUE
OF
THE
CURRENT
TERM
THEN
THE
VARIABLE
IS
COMPUTED
VALUE
NEXT
WE
ADVANCE
ONE
TERM
BY
PUTTING
INTO
AND
INTO
THIS
PROCESS
IS
REPEATED
UNTIL
WE
COMPUTE
THE
LAST
TERM
AND
THE
WHILE
LOOP
EXITS
FIGURE
FIBONACCI
EXECUTION
THIS
COMPLETES
T
HE
BODY
OF
THE
LOOP
THE
FIRST
TWO
TERMS
OF
THE
FIBONACCI
SEQUENCE
ARE
AND
WE
INITIALIZE
OUR
FIRST
TWO
TERMS
TO
THESE
VALUES
PUTTING
IT
ALL
TOGETHER
WE
GET
THE
CODE
IN
EXAMPLE
EXAMPLE
FIB
FIB
C
INCLUDE
STDIO
H
INT
PREVIOUS
FIBONACCI
NUMBER
INT
CURRENT
FIBONACCI
NUMBER
INT
NEXT
NUMBER
IN
THE
SERIES
INT
MAIN
START
THINGS
OUT
PRINTF
N
PRINT
FIRST
NUMBER
WHILE
PRINTF
D
N
RETURN
BREAK
STATEMENT
WE
HAVE
USED
A
WHILE
STATEMENT
TO
COMPUTE
THE
FIBONACCI
NUMBERS
LESS
THAN
THE
LOOP
EXITS
WHEN
THE
CONDITION
AFTER
THE
WHILE
BECOMES
FALSE
LOOPS
CAN
BE
EXITED
AT
ANY
POINT
THROUGH
THE
USE
OF
A
BREAK
STATEMENT
SUPPOSE
WE
WANT
TO
ADD
A
SERIES
OF
NUMBERS
BUT
WE
DON
T
KNOW
HOW
MANY
NUMBERS
ARE
TO
BE
ADDED
TOGETHER
WE
NEED
SOME
WAY
OF
LETTING
THE
PROGRAM
KNOW
THAT
WE
HAVE
REACHED
THE
END
OF
OUR
LIST
IN
EXAMPLE
WE
USE
THE
NUMBER
ZERO
TO
SIGNAL
THE
END
OF
LIST
NOTE
THAT
THE
WHILE
STATEMENT
BEGINS
WITH
WHILE
LEFT
TO
ITS
OWN
DEVICES
THE
PROGRAM
WILL
LOOP
FOREVER
BECAUSE
THE
WHILE
WILL
EXIT
ONLY
WHEN
THE
EXPRESSION
IS
THE
ONLY
WAY
TO
EXIT
THIS
LOOP
IS
THROUGH
A
BREAK
STATEMENT
WHEN
WE
SEE
THE
END
OF
THE
LIST
INDICATOR
WE
USE
THE
STATEMENT
IF
ITEM
BREAK
TO
EXIT
THE
LOOP
EXAMPLE
TOTAL
TOTAL
C
INCLUDE
STDIO
H
CHAR
LINE
LINE
OF
DATA
FOR
INPUT
INT
TOTAL
RUNNING
TOTAL
OF
ALL
NUMBERS
SO
FAR
INT
ITEM
NEXT
ITEM
TO
ADD
TO
THE
LIST
INT
MAIN
TOTAL
WHILE
PRINTF
ENTER
TO
ADD
N
PRINTF
OR
TO
STOP
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
ITEM
IF
ITEM
BREAK
TOTAL
ITEM
PRINTF
TOTAL
D
N
TOTAL
PRINTF
FINAL
TOTAL
D
N
TOTAL
RETURN
CONTINUE
STATEMENT
THE
CONTINUE
STATEMENT
IS
VERY
SIMILAR
TO
THE
BREAK
STATEMENT
EXCEPT
THAT
INSTEAD
OF
TERMINATING
THE
LOOP
CONTINUE
STARTS
REEXECUTING
THE
BODY
OF
THE
LOOP
FROM
THE
TOP
FOR
EXAMPLE
IF
WE
WANT
TO
MODIFY
THE
PREVIOUS
PROGRAM
TO
TOTAL
ONLY
NUMBERS
LARGER
THAN
WE
COULD
WRITE
A
PROGRAM
SUCH
AS
EXAMPLE
EXAMPLE
TOTALB
TOTALB
C
FILE
TOTALB
TOTALB
C
INCLUDE
STDIO
H
CHAR
LINE
LINE
FROM
INPUT
INT
TOTAL
RUNNING
TOTAL
OF
ALL
NUMBERS
SO
FAR
INT
ITEM
NEXT
ITEM
TO
ADD
TO
THE
LIST
INT
NUMBER
OF
NEGATIVE
ITEMS
INT
MAIN
TOTAL
WHILE
PRINTF
ENTER
TO
ADD
N
PRINTF
OR
TO
STOP
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
ITEM
IF
ITEM
BREAK
IF
ITEM
CONTINUE
TOTAL
ITEM
PRINTF
TOTAL
D
N
TOTAL
PRINTF
FINAL
TOTAL
D
N
TOTAL
PRINTF
WITH
D
NEGATIVE
ITEMS
OMITTED
N
RETURN
ASSIGNMENT
ANYWHERE
SIDE
EFFECT
C
ALLOWS
THE
USE
OF
ASSIGNMENT
STATEMENTS
ALMOST
ANYWHERE
FOR
EXAMPLE
YOU
CAN
PUT
ASSIGNMENT
STATEMENTS
INSIDE
ASSIGNMENT
STATEMENTS
DON
T
PROGRAM
LIKE
THIS
AVERAGE
LAST
FIRST
THIS
IS
THE
EQUIVALENT
OF
SAYING
PROGRAM
LIKE
THIS
LAST
FIRST
AVERAGE
THE
FIRST
VERSION
BURIES
THE
ASSIGNMENT
OF
INSIDE
THE
EXPRESSION
PROGRAMS
SHOULD
BE
CLEAR
AND
SIMPLE
AND
SHOULD
NOT
HIDE
ANYTHING
THE
MOST
IMPORTANT
RULE
OF
PROGRAMMING
IS
KEEP
IT
SIMPLE
C
ALSO
ALLOWS
THE
PROGRAMMER
TO
PUT
ASSIGNMENT
STATEMENTS
IN
THE
WHILE
CONDITIONAL
FOR
EXAMPLE
DO
NOT
PROGRAM
LIKE
THIS
WHILE
PRINTF
TERM
D
N
AVOID
THIS
TYPE
OF
PROGRAMMING
NOTICE
HOW
MUCH
CLEARER
THE
LOGIC
IS
IN
THE
VERSION
BELOW
PROGRAM
LIKE
THIS
WHILE
IF
BREAK
PRINTF
TERM
D
N
QUESTION
FOR
SOME
STRANGE
REASON
EXAMPLE
THINKS
THAT
EVERYONE
OWES
A
BALANCE
OF
DOLLARS
WHY
CLICK
HERE
FOR
THE
ANSWER
SECTION
EXAMPLE
C
INCLUDE
STDIO
H
CHAR
LINE
INPUT
LINE
INT
AMOUNT
OWED
INT
MAIN
PRINTF
ENTER
NUMBER
OF
DOLLARS
OWED
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
IF
PRINTF
YOU
OWE
NOTHING
N
ELSE
PRINTF
YOU
OWE
D
DOLLARS
N
RETURN
SAMPLE
OUTPUT
ENTER
NUMBER
OF
DOLLARS
OWED
YOU
OWE
DOLLARS
ANSWER
ANSWER
THIS
PROGRAM
ILLUSTRATES
ONE
OF
THE
MOST
COMMON
AND
FRUSTRATING
OF
C
ERRORS
THE
PROBLEM
IS
THAT
C
ALLOWS
ASSIGNMENT
STATEMENTS
INSIDE
IF
CONDITIONALS
THE
STATEMENT
IF
USES
A
SINGLE
EQUAL
SIGN
INSTEAD
OF
THE
DOUBLE
EQUAL
SIGN
C
WILL
ASSIGN
THE
VALUE
AND
TEST
THE
RESULT
WHICH
IS
IF
THE
RESULT
WAS
NONZERO
TRUE
THEIF
CLAUSE
WOULD
BE
EXECUTED
BECA
USE
THE
RESULT
IS
FALSE
THE
ELSE
CLAUSE
IS
EXECUTED
AND
THE
PROGRAM
PRINTS
THE
WRONG
ANSWER
THE
STATEMENT
IF
IS
EQUIVALENT
TO
IF
THE
STATEMENT
SHOULD
BE
WRITTEN
IF
THIS
E
RROR
IS
THE
MOST
COMMON
ERROR
THAT
BEGINNING
C
PROGRAMMERS
MAKE
PROGRAMMING
EXERCISES
EXERCISE
WRITE
A
PROGRAM
TO
FIND
THE
SQUARE
OF
THE
DISTANCE
BETWEEN
TWO
POINTS
FOR
A
MORE
ADVANCED
PROBLEM
FIND
THE
ACTUAL
DISTANCE
THIS
PROBLEM
INVOLVES
U
SING
THE
STANDARD
FUNCTION
SQRT
USE
YOUR
HELP
SYSTEM
TO
FIND
OUT
MORE
ABOUT
HOW
TO
USE
THIS
FUNCTION
EXERCISE
A
PROFESSOR
GENERATES
LETTER
GRADES
USING
TABLE
TABLE
GRADE
VALUES
RIGHT
GRADE
F
D
C
B
A
GIVEN
A
NUMERIC
GRADE
PRINT
THE
LETTER
EXERCISE
MODIFY
THE
PRE
VIOUS
PROGRAM
TO
PRINT
A
OR
AFTER
THE
LETTER
GRADE
BASED
ON
THE
LAST
DIGIT
OF
THE
SCORE
THE
MODIFIERS
ARE
LISTED
IN
TABLE
TABLE
GRADE
MODIFICATION
VALUES
LAST
DIGIT
MODIFIER
BLANK
FOR
EXAMPLE
B
A
AND
D
NOTE
AN
F
IS
ONLY
AN
F
THERE
IS
NO
F
OR
F
EXERCISE
GIVEN
AN
AMOUNT
OF
MONEY
LESS
THAN
COMPUTE
THE
NUMBER
OF
QUARTERS
DIMES
NICKELS
AND
PENNIES
NEEDED
EXERCISE
A
LEAP
YEAR
IS
ANY
YEAR
DIVISIBLE
BY
UNLESS
THE
YEAR
IS
DIVISIBLE
BY
BUT
NOT
WRITE
A
PROGRAM
TO
TELL
IF
A
YEAR
IS
A
LEAP
YEAR
EXERCISE
WRITE
A
PROGRAM
THAT
GIVEN
THE
NUMBER
OF
HOURS
AN
EMPLOYEE
WORKED
AND
THE
HOURLY
WAGE
COMPUTES
THE
EMPLOYEE
WEEKLY
PAY
COUNT
ANY
HOURS
OVER
AS
OVERTIME
AT
TIME
AND
A
HALF
CHAPTER
PROGRAMMING
PROCESS
IT
JUST
A
SIMPLE
MATTER
OF
PROGRAMMING
ANY
BOSS
WHO
HAS
NEVER
WRITTEN
A
PROGRAM
PROGRAMMING
IS
MORE
THAN
JUST
WRITING
CODE
SOFTWARE
HAS
A
LIFE
CYCLE
IT
IS
BORN
GROWS
UP
BECOMES
MATURE
AND
FINALLY
DIES
ONLY
TO
BE
REPLACED
BY
A
NEWER
YOUNGER
PRODUCT
FIGURE
ILLUSTRATES
THE
LIFE
CYCLE
OF
A
PROGRAM
UNDERSTANDING
THIS
CYCLE
IS
IMPORTANT
BECAUSE
AS
A
PROGRAMMER
YOU
WILL
SPEND
ONLY
A
SMALL
AMOUNT
OF
TIME
WRITING
NEW
CODE
MOST
PROGRAMMING
TIME
IS
SPENT
MODIFYING
AND
DEBUGGING
EXISTING
CODE
SOFTWARE
DOES
NOT
EXIST
IN
A
VACUUM
IT
MUST
BE
DOCUME
NTED
MAINTAINED
ENHANCED
AND
SOLD
IN
THIS
CHAPTER
WE
WILL
TAKE
A
LOOK
AT
A
SMALL
PROGRAMMING
PROJECT
USING
ONE
PROGRAMMER
LARGER
PROJECTS
THAT
INVOLVE
MANY
PEOPLE
WILL
BE
DISCUSSED
IN
CHAPTER
ALTHOUGH
OUR
FINAL
CODE
IS
LESS
THAN
LINES
THE
PRINCIPLES
USED
IN
ITS
CONSTRUCTION
CAN
BE
APPLIED
TO
PROGRAMS
WITH
THOUSANDS
OF
LINES
OF
CODE
FIGURE
SOFTWARE
LIFE
CYCLE
THE
MAJOR
STEPS
IN
MAKING
A
PROGRAM
ARE
REQUIREMENTS
PROGRAMS
START
WHEN
SOMEONE
GETS
AN
IDEA
AND
STARTS
TO
IMPLEMENT
IT
THE
REQUIREMENT
DOCUMENT
DESCRIBES
IN
VERY
GENERAL
TERMS
WHAT
IS
WANTED
PROGRAM
SP
ECIFICATION
THE
SPECIFICATION
IS
A
DESCRIPTION
OF
WHAT
THE
PROGRAM
DOES
IN
THE
BEGINNING
A
PRELIMINARY
SPECIFICATION
IS
USED
TO
DESCRIBE
WHAT
THE
PROGRAM
IS
GOING
TO
DO
LATER
AS
THE
PROGRAM
BECOMES
MORE
REFINED
SO
DOES
THE
SPECIFICATION
FINALLY
WHE
N
THE
PROGRAM
IS
FINISHED
THE
SPECIFICATION
SERVES
AS
A
COMPLETE
DESCRIPTION
OF
WHAT
THE
PROGRAM
DOES
CODE
DESIGN
THE
PROGRAMMER
DOES
AN
OVERALL
DESIGN
OF
THE
PROGRAM
THE
DESIGN
SHOULD
INCLUDE
MAJOR
ALGORITHMS
MODULE
DEFINITIONS
FILE
FORMATS
AND
DATA
STRUCTURES
CODING
THE
NEXT
STEP
IS
WRITING
THE
PROGRAM
THIS
STEP
INVOLVES
FIRST
WRITING
A
PROTOTYPE
AND
THEN
FILLING
IT
IN
TO
CREATE
THE
FULL
PROGRAM
TESTING
THE
PROGRAMMER
SHOULD
DESIGN
A
TEST
PLAN
AND
THEN
USE
IT
TO
TEST
HIS
PROGRAM
WHEN
POSSIB
LE
THE
PROGRAMMER
SHOULD
HAVE
SOMEONE
ELSE
TEST
THE
PROGRAM
DEBUGGING
UNFORTUNATELY
VERY
FEW
PROGRAMS
WORK
THE
FIRST
TIME
THEY
MUST
BE
CORRECTED
AND
TESTED
AGAIN
RELEASE
THE
PROGRAM
IS
PACKAGED
DOCUMENTED
AND
SENT
OUT
INTO
THE
WORLD
TO
BE
USED
MAINTENANCE
PROGRAMS
ARE
NEVER
PERFECT
BUGS
WILL
BE
FOUND
AND
WILL
NEED
CORRECTION
THIS
STEP
IS
THE
MAINTENANCE
PHASE
OF
PROGRAMMING
REVISION
AND
UPDATING
AFTER
A
PROGRAM
HAS
BEEN
WORKING
FOR
A
WHILE
THE
USERS
WILL
WANT
CHANGES
SUCH
AS
MORE
FEATURE
OR
MORE
INTELLIGENT
ALGORITHMS
AT
THIS
POINT
A
NEW
SPECIFICATION
IS
CREATED
AND
THE
PROCESS
STARTS
AGAIN
SETTING
UP
THE
OPERATING
SYSTEM
ALLOWS
YOU
TO
GROUP
FILES
IN
DIRECTORIES
JUST
AS
FILE
FOLDERS
SERVE
AS
A
WAY
OF
KEEPING
PAPERS
TOGETHER
IN
A
FILING
CABINET
DIRECTORIES
SERVE
AS
A
WAY
OF
KEEPING
FILES
TOGETHER
WINDOWS
GOES
SO
FAR
AS
TO
CALL
ITS
DIRECTORIES
FOLDERS
IN
THIS
CHAPTER
WE
CREATE
A
SIMPLE
CALCULATOR
PROGRAM
ALL
THE
FILES
FOR
THIS
PROGRAM
ARE
STORED
IN
A
DIRECTORY
NAMED
CALC
IN
UNIX
WE
CREATE
A
NEW
DIRECTORY
UNDER
OUR
HOME
DIRECTORY
AND
THEN
MOVE
TO
IT
AS
SHOWN
IN
THE
FOLLOWING
EXAMPLE
CD
MKDIR
CALC
CD
CALC
ON
MS
DOS
TYPE
C
CD
C
MKDIR
CALC
C
CD
CALC
C
CALC
THIS
DIRECTORY
SETUP
IS
EXTREMELY
IMPLE
AS
YOU
GENERATE
MORE
AND
MORE
PROGRAMS
YOU
WILL
PROBABLY
WANT
A
MORE
ELABORATE
DIRECTORY
STRUCTURE
MORE
INFORMATION
ON
HOW
TO
ORGANIZE
DIRECTORIES
OR
FOLDERS
CAN
BE
FOUND
IN
YOUR
OPERATING
SYSTEM
MANUAL
SPECIFICATION
FOR
THIS
CHAPTER
WE
ASSUME
THAT
WE
HAVE
THE
REQUIREMENT
TO
WRITE
A
PROGRAM
THAT
ACTS
LIKE
A
FOUR
FUNCTION
CALCULATOR
TYPICALLY
THE
REQUIREMENTS
THAT
YOU
ARE
GIVEN
IS
VAGUE
AND
INCOMPLETE
THE
PROGRAMMER
REFINES
IT
INTO
SOMETHING
THAT
EXACTLY
DEFINES
THE
PROGRAM
THAT
HE
IS
GO
ING
TO
PRODUCE
SO
THE
FIRST
STEP
IS
TO
WRITE
A
PRELIMINARY
USERS
SPECIFICATION
DOCUMENT
THAT
DESCRIBES
WHAT
YOUR
PROGRAM
IS
GOING
TO
DO
AND
HOW
TO
USE
IT
THE
DOCUMENT
DOES
NOT
DESCRIBE
THE
INTERNAL
STRUCTURE
OF
THE
PROGRAM
OR
THE
ALGORITHM
YOU
PLAN
ON
U
SING
A
SAMPLE
SPECIFICATION
FOR
OUR
FOUR
FUNCTION
CALCULATOR
APPEARS
BELOW
IN
CALC
A
FOUR
FUNCTION
CALCULATOR
THE
PRELIMINARY
SPECIFICATION
SERVES
TWO
PURPOSES
FIRST
YOU
SHOULD
GIVE
IT
TO
YOUR
BOSS
OR
CUSTOMER
TO
MAKE
SURE
THAT
YOU
AGREE
ON
WHAT
EACH
OF
YOU
SAID
SECOND
YOU
CAN
CIRCULATE
IT
AMONG
YOUR
COLLEAGUES
AND
SEE
IF
THEY
HAVE
ANY
SUGGESTIONS
OR
CORRECTIONS
THIS
PRELIMINARY
SPECIFICATION
WAS
CIRCULATED
AND
RECEIVED
THE
COMMENTS
HOW
ARE
YOU
GOING
TO
GET
OUT
OF
THE
PROGRAM
WHAT
HAPPENS
WHEN
YOU
TRY
TO
DIVIDE
BY
CALC
A
FOUR
FUNCTION
CALCULATOR
PRELIMINARY
SPECIFICATION
DEC
STEVE
OUALLINE
WARNING
THIS
DOCUMENT
IS
A
PRELIMINARY
SPECIFICATION
ANY
RESEMBLANCE
TO
ANY
SOFTWARE
LIVING
OR
DEAD
IS
PURELY
COINCIDENTAL
CALC
IS
A
PROGRAM
THAT
ALLOWS
THE
USER
TO
TURN
A
COMPUTER
INTO
A
FOUR
FUNCTION
CALCULATOR
THE
PROGRAM
WILL
ADD
SUBTRACT
MULTIPLY
AND
DIVIDE
SIMPLE
INTEGERS
WHEN
THE
PROGRA
M
IS
RUN
IT
WILL
ZERO
THE
RESULT
REGISTER
AND
DISPLAY
THE
REGISTER
CONTENTS
THE
USER
CAN
THEN
TYPE
IN
AN
OPERATOR
AND
NUMBER
THE
RESULT
WILL
BE
UPDATED
AND
DISPLAYED
THE
FOLLOWING
OPERATORS
ARE
VALID
OPERATOR
MEANING
ADDITION
SUBTRACTION
MU
LTIPLICATION
DIVISION
FOR
EXAMPLE
USER
INPUT
IS
IN
BOLDFACE
CALC
RESULT
ENTER
OPERATOR
AND
NUMBER
RESULT
ENTER
OPERATOR
AND
NUMBER
RESULT
ENTER
OPERATOR
AND
NUMBER
RESULT
ENTER
OPERATOR
AND
NUMBER
RESULT
SO
WE
ADD
A
NEW
OPERATOR
Q
FOR
QUIT
AND
WE
ADD
THE
STATEMENT
DIVIDING
BY
RESULTS
IN
AN
ERROR
MESSAGE
AND
THE
RESULT
REGISTER
IS
LEFT
UNCHANGED
CODE
DESIGN
AFTER
THE
PRELIMINARY
SPECIFICATION
HAS
BEEN
APPROVED
WE
CAN
START
DESIGNING
CODE
IN
THE
CODE
DESIGN
PHASE
THE
PROGRAMMER
PLANS
HIS
WORK
IN
LARGE
PROGRAMMING
PROJECTS
INVOLVING
MANY
PEOPLE
THE
CODE
WOULD
BE
BROKEN
UP
INTO
MODULES
TO
BE
ASSIGNED
TO
THE
PROGRAMMERS
AT
THIS
STAGE
FILE
FORMATS
ARE
PLANNED
DATA
STRUCTURES
ARE
DESIGNED
AND
MAJOR
ALGORITHMS
ARE
DECIDED
UPON
OUR
SIMPLE
CA
LCULATOR
USES
NO
FILES
AND
REQUIRES
NO
FANCY
DATA
STRUCTURES
WHAT
LEFT
FOR
THIS
PHASE
IS
TO
DESIGN
THE
MAJOR
ALGORITHM
OUTLINED
IN
PSEUDO
CODE
A
SHORTHAND
HALFWAY
BETWEEN
ENGLISH
AND
REAL
CODE
THE
MAJOR
ALGORITHM
IS
LOOP
READ
AN
OPERATOR
AND
NUMBER
DO
THE
CALCULATION
DISPLAY
THE
RESULT
END
LOOP
PROTOTYPE
AFTER
THE
CODE
DESIGN
IS
COMPLETED
WE
CAN
BEGIN
WRITING
THE
PROGRAM
BUT
RATHER
THAN
TRY
TO
WRITE
THE
ENTIRE
PROGRAM
AT
ONCE
AND
THEN
DEBUG
IT
WE
WILL
USE
A
METHOD
CALLED
FAST
PROTOTYPING
WE
IMPLEMENT
THE
SMALLEST
PORTION
OF
THE
SPECIFICATION
THAT
WILL
STILL
DO
SOMETHING
IN
OUR
CASE
WE
WILL
CUT
OUR
FOUR
FUNCTIONS
DOWN
TO
A
ONE
FUNCTION
CALCULATOR
AFTER
WE
GET
THIS
SMALL
PART
WORKING
WE
CAN
BUILD
THE
REST
OF
THE
FUNCTIONS
ONTO
THIS
STABLE
FOUNDATION
ALSO
THE
PROTOTYPE
GIVES
THE
BOSS
SOMETHING
TO
LOOK
AT
AND
PLAY
WITH
GIVING
HIM
A
GOOD
IDEA
OF
THE
PROJECT
DIRECTION
GOOD
COMMUNICATION
IS
THE
KEY
TO
GOOD
PROGRAMMING
AND
THE
MORE
YOU
CAN
SHOW
SOMEONE
THE
BETTER
THE
CODE
FOR
TH
E
FIRST
VERSION
OF
OUR
FOUR
FUNCTION
CALCULATOR
IS
FOUND
IN
EXAMPLE
EXAMPLE
C
INCLUDE
STDIO
H
CHAR
LINE
LINE
OF
DATA
FROM
THE
INPUT
INT
RESULT
THE
RESULT
OF
THE
CALCULATIONS
CHAR
OPERATOR
OPERATOR
THE
USER
SPECIFIED
INT
VALUE
VALUE
SPECIFIED
AFTER
THE
OPERATOR
INT
MAIN
RESULT
INITIALIZE
THE
RESULT
LOOP
FOREVER
OR
TILL
WE
HIT
THE
BREAK
STATEMENT
WHILE
PRINTF
RESULT
D
N
RESULT
PRINTF
ENTER
OPERATOR
AND
NUMBER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
C
D
OPERATOR
VALUE
IF
OPERATOR
RESULT
VALUE
ELSE
PRINTF
UNKNOWN
OPERATOR
C
N
OPERATOR
THE
PROGRAM
BEGINS
BY
INITIALIZING
THE
VARIABLE
RESULT
TO
THE
MAIN
BODY
OF
THE
PROGRAM
IS
A
LOOP
STARTING
WITH
WHILE
THIS
LOOP
WILL
REPEAT
UNTIL
A
BREAK
STATEMENT
IS
REACHED
THE
CODE
PRINTF
ENTER
OPERATOR
AND
NUMBER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
C
D
OPERATOR
VALUE
ASKS
THE
USER
FOR
AN
OPERATOR
AND
NUMBER
THESE
ARE
SCANNED
AND
STORED
IN
THE
VARIABLES
OPERATOR
AND
VALUE
NEXT
WE
START
CHECKING
THE
OPERATORS
IF
THE
OPERATOR
IS
A
PLUS
SIGN
WE
PERFORM
AN
ADDITION
USING
THE
LINE
IF
OPERATOR
RESULT
VALUE
SO
FAR
WE
ONLY
RECOGNIZE
THE
PLUS
OPERATOR
AS
SOON
AS
THIS
OPERATOR
WORKS
CORRECTLY
WE
WILL
ADD
MORE
OPERATORS
BY
ADDING
MORE
IF
STATEMENTS
FINALLY
IF
AN
ILLEGAL
OPERATOR
IS
ENTERED
THE
LINE
ELSE
PRINTF
UNKNOWN
OPERATOR
C
N
OPERATOR
WRITES
AN
ERROR
MESSAGE
TELLING
THE
USER
THAT
HE
MADE
A
MISTAKE
MAKEFILE
AFTER
THE
SOURCE
HAS
BEEN
ENTERED
IT
NEEDS
TO
BE
COMPILED
AND
LINKED
UP
UNTIL
NOW
WE
HAVE
BEEN
RUNNING
THE
COMPILER
MANUALLY
THIS
PROCESS
IS
SOMEWHAT
TEDIOUS
AND
PRONE
TO
ERROR
ALSO
LARGER
PROGRAMS
CONSIST
OF
MANY
MODULES
AND
ARE
EXTREMELY
DIFFICULT
TO
COMPILE
BY
HAND
FORTUNATELY
BOTH
UNIX
AND
MS
DOS
WINDOWS
HAVE
A
UTILITY
CALLED
MAKE
THAT
WILL
HANDLE
THE
DETAILS
OF
COMPILATION
FOR
N
OW
USE
THIS
EXAMPLE
AS
A
TEMPLATE
AND
SUBSTITUTE
THE
NAME
OF
YOUR
PROGRAM
IN
PLACE
OF
CALC
MAKE
WILL
BE
DISCUSSED
IN
DETAIL
IN
CHAPTER
THE
PROGRAM
LOOKS
AT
THE
FILE
CALLED
MAKEFILE
FOR
A
DESCRIPTION
OF
HOW
TO
COMPILE
YOUR
PROGRAM
AND
RUNS
THE
COMPILER
FOR
YOU
MICROSOFT
VISUAL
C
CALLS
THIS
UTILITY
NMAKE
BECAUSE
THE
MAKEFILE
CONTAINS
THE
RULES
FOR
COMPILATION
IT
IS
CUSTOMIZED
FOR
THE
COMPILER
THE
FOLLOWING
IS
A
SET
OF
MAKEFILES
FOR
ALL
OF
THE
COMPILERS
DESCRIBED
IN
THIS
BOOK
GENERIC
UNIX
FILE
MAKEFILE
UNX
MAKEFILE
FOR
UNIX
SYSTEMS
USING
A
GNU
C
COMPILER
CC
GCC
CFLAGS
G
COMPILER
FLAGS
G
ENABLE
DEBUGGING
C
CC
CFLAGS
O
C
CLEAN
RM
F
UNIX
WITH
THE
FREE
SOFTWARE
FOUNDATION
GCC
COMPILER
FILE
MAKEFILE
GCC
MAKEFILE
FOR
UNIX
SYSTEMS
USING
A
GNU
C
COMPILER
CC
GCC
CFLAGS
G
ANSI
COMPILER
FLAGS
G
ENABLE
DEBUGGING
WALL
TURN
ON
ALL
WARNINGS
NOT
USED
SINCE
IT
GIVES
AWAY
THE
BUG
IN
THIS
PROGRAM
FORCE
THE
COMPILER
TO
USE
THE
CORRECT
HEADERS
ANSI
DON
T
USE
GNU
EXTENSIONS
STICK
TO
ANSI
C
C
CC
CFLAGS
O
C
CLEAN
RM
F
BORLAND
C
FILE
MAKEFILE
BCC
MAKEFILE
FOR
BORLAND
BORLAND
C
COMPILER
CC
BCC
FLAGS
N
CHECK
FOR
STACK
OVERFLOW
V
ENABLE
DEBUGGING
W
TURN
ON
ALL
WARNINGS
ML
LARGE
MODEL
CFLAGS
N
V
W
ML
EXE
C
CC
CFLAGS
C
CLEAN
ERASE
EXE
TURBO
C
FILE
MAKEFILE
TCC
MAKEFILE
FOR
DOS
SYSTEMS
USING
A
TURBO
C
COMPILER
CC
TCC
CFLAGS
V
W
ML
EXE
C
CC
CFLAGS
EXE
C
CLEAN
DEL
EXE
VISUAL
C
FILE
MAKEFILE
MSC
MAKEFILE
FOR
DOS
SYSTEMS
USING
A
MICROSOFT
VISUAL
C
COMPILER
CC
CL
FLAGS
AL
COMPILE
FOR
LARGE
MODEL
ZI
ENABLE
DEBUGGING
TURN
ON
WARNINGS
CFLAGS
AL
ZI
EXE
C
CC
CFLAGS
C
CLEAN
ERASE
EXE
TO
COMPILE
THE
PROGRAM
JUST
EXECUTE
THE
MAKE
COMMAND
MAKE
WILL
DETERMINE
WHICH
COMPILATION
COMMANDS
ARE
NEEDED
AND
THEN
EXECUTE
THEM
MAKE
USES
THE
MODIFICATION
DATES
OF
THE
FILES
TO
DETERMINE
WHETHER
OR
NOT
A
COMPILE
IS
NECESSARY
COMPILATION
CREATES
AN
OBJECT
FILE
THE
MODIFICATION
DATE
OF
THE
OBJECT
FILE
IS
LATER
THAN
THE
MODIFICATION
DATE
OF
ITS
SOURCE
IF
THE
SOURCE
IS
EDITED
THE
SOURCE
MODIFICATION
DATE
IS
UPDATED
AND
THE
OBJECT
FILE
IS
THEN
OUT
OF
DATE
MAKE
CHECKS
THESE
DATES
AND
IF
THE
SOURCE
WAS
MODIFIED
AFTER
THE
OBJECT
MAKE
RECOMPILES
THE
OBJECT
TESTING
AFTER
THE
PROGRAM
IS
COMPILED
WITHOUT
ERRORS
WE
CAN
MOVE
ON
TO
THE
TESTING
PHASE
NOW
IS
THE
TIME
TO
START
WRITING
A
TEST
PLAN
THIS
DOCUMENT
IS
SIMPLY
A
LIST
OF
THE
STEPS
WE
PERFORM
TO
MAKE
SURE
THE
PROGRAM
WORKS
IT
IS
WRITTEN
FOR
TWO
REASONS
IF
A
BUG
IS
FOUND
WE
WANT
TO
BE
ABLE
TO
REPRODUCE
IT
IF
WE
CHANGE
THE
PROGRAM
WE
WILL
WANT
TO
RETEST
IT
TO
MAKE
SURE
NEW
CODE
DID
NOT
BREAK
ANY
OF
THE
SECTIONS
OF
THE
PROGRAM
THAT
WERE
PREVIOUSLY
WORKING
OUR
TEST
PLAN
STARTS
OUT
AS
TRY
THE
FOLLOWING
OPERATIONS
RESULT
SHOULD
BE
RESULT
SHOULD
BE
X
ERROR
MESSAGE
SHOULD
BE
OUTPUT
AFTER
WE
RUN
THE
PROGRAM
WE
GET
RESULT
ENTER
OPERATOR
AND
NUMBER
RESULT
ENTER
OPERATOR
AND
NUMBER
RESULT
ENTER
OPERATOR
AND
NUMBER
X
RESULT
SOMETHING
IS
CLEARLY
WRONG
THE
ENTRY
X
SHOULD
HAVE
GENERATED
AN
ERROR
MESSAGE
BUT
IT
DIDN
T
A
BUG
IS
IN
THE
PROGRA
M
SO
WE
BEGIN
THE
DEBUGGING
PHASE
ONE
OF
THE
ADVANTAGES
OF
MAKING
A
SMALL
WORKING
PROTOTYPE
IS
THAT
WE
CAN
ISOLATE
ERRORS
EARLY
DEBUGGING
FIRST
WE
INSPECT
THE
PROGRAM
TO
SEE
IF
WE
CAN
DETECT
THE
ERROR
IN
SUCH
A
SMALL
PROGRAM
WE
CAN
EASILY
SPOT
THE
MISTAKE
HOWEVER
LET
ASSUME
THAT
INSTEAD
OF
A
LINE
PROGRAM
WE
HAVE
A
MUCH
LARGER
PROGRAM
CONTAINING
LINES
SUCH
A
PROGRAM
WOULD
MAKE
INSPECTION
MORE
DIFFICULT
SO
WE
NEED
TO
PROCEED
TO
THE
NEXT
STEP
MOST
SYSTEMS
HAVE
C
DEBUGGING
PROGRAMS
HOWEVER
EACH
SYSTEM
IS
DIFFERENT
SOME
SYSTEMS
HAVE
NO
DEBUGGER
IN
SUCH
A
CASE
WE
MUST
RESORT
TO
A
DIAGNOSTIC
PRINT
STATEMENT
THE
TECHNIQUE
IS
SIMPLE
PUT
A
PRINTF
AT
THE
POINTS
AT
WHICH
YOU
KNOW
THE
DATA
IS
GOOD
JUST
TO
MAKE
SURE
THE
DATA
IS
REALLY
GO
OD
THEN
PUT
APRINTF
AT
POINTS
AT
WHICH
THE
DATA
IS
BAD
RUN
THE
PROGRAM
AND
KEEP
PUTTING
IN
PRINTF
STATEMENTS
UNTIL
YOU
ISOLATE
THE
AREA
IN
THE
PROGRAM
THAT
CONTAINS
THE
MISTAKE
OUR
PROGRAM
WITH
DIAGNOSTIC
PRINTF
STATEMENTS
ADDED
LOOKS
LIKE
PRINTF
ENTER
OPERATOR
AND
NUMBER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
D
C
VALUE
OPERATOR
PRINTF
AFTER
SCANF
C
N
OPERATOR
IF
OPERATOR
PRINTF
AFTER
IF
C
N
OPERATOR
RESULT
VALUE
RUNNING
OUR
PROGRAM
AGAIN
RESULTS
IN
RESULT
ENTER
OPERATOR
AND
NUMBER
RESULT
ENTER
OPERATOR
AND
NUMBER
AFTER
SCANF
AFTER
IF
RESULT
ENTER
OPERATOR
AND
NUMBER
X
AFTER
SCANF
X
AFTER
IF
RESULT
FROM
THIS
EXAMPLE
WE
SEE
THAT
SOMETHING
IS
GOING
WRONG
WITH
THE
IF
STATEMENT
SOMEHOW
THE
VARIABLE
OPERATOR
IS
AN
X
GOING
IN
AND
A
COMING
OUT
CLOSER
INSPECTION
REVEALS
THAT
WE
HAVE
MADE
THE
OLD
MISTAKE
OF
USING
INSTEAD
OF
AFTER
WE
FIX
THIS
BUG
THE
PROGRAM
RUNS
CORRECTLY
BUILDING
ON
THIS
WORKING
FOUNDATION
WE
ADD
CODE
FOR
THE
OTHER
OPERATORS
DASH
ASTERISK
AND
SLASH
THE
RESULT
IS
SHOWN
IN
EXAMPLE
EXAMPLE
C
INCLUDE
STDIO
H
CHAR
LINE
LINE
OF
TEXT
FROM
INPUT
INT
RESULT
THE
RESULT
OF
THE
CALCULATIONS
CHAR
OPERATOR
OPERATOR
THE
USER
SPECIFIED
INT
VALUE
VALUE
SPECIFIED
AFTER
THE
OPERA
TOR
INT
MAIN
RESULT
INITIALIZE
THE
RESULT
LOOP
FOREVER
OR
UNTIL
BREAK
REACHED
WHILE
PRINTF
RESULT
D
N
RESULT
PRINTF
ENTER
OPERATOR
AND
NUMBER
FGETS
LINE
SIZEOF
LINE
STDI
N
SSCANF
LINE
C
D
OPERATOR
VALUE
IF
OPERATOR
Q
OPERATOR
Q
BREAK
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
IF
VALUE
PRINTF
ERROR
DIVIDE
BY
ZERO
N
PRINTF
OPERATION
IGNORED
N
ELSE
RESULT
VALUE
ELSE
PRINTF
UNKNOWN
OPERATOR
C
N
OPERATOR
RETURN
WE
EXPAND
OUR
TEST
PLAN
TO
INCLUDE
THE
NEW
OPERATORS
AND
TRY
IT
AGAIN
RESULT
SHOULD
BE
RESULT
SHO
ULD
BE
X
ERROR
MESSAGE
SHOULD
BE
OUTPUT
RESULT
SHOULD
BE
ZERO
RESULT
SHOULD
BE
RESULT
SHOULD
BE
DIVIDE
BY
ZERO
ERROR
RESULT
SHOULD
BE
Q
PROGRAM
SHOULD
EXIT
WHILE
TESTING
THE
PROG
RAM
WE
FIND
THAT
MUCH
TO
OUR
SURPRISE
THE
PROGRAM
WORKS
THE
WORD
PRELIMINARY
IS
REMOVED
FROM
THE
SPECIFICATION
AND
THE
PROGRAM
TEST
PLAN
AND
SPECIFICATION
ARE
RELEASED
DEBUGGING
FIRST
WE
INSPECT
THE
PROGRAM
TO
SEE
IF
WE
CAN
DETECT
THE
ERROR
IN
SUCH
A
SMALL
PROGRAM
WE
CAN
EASILY
SPOT
THE
MISTAKE
HOWEVER
LET
ASSUME
THAT
INSTEAD
OF
A
LINE
PROGRAM
WE
HAVE
A
MUCH
LARGER
PROGRAM
CONTAINING
LINES
SUCH
A
PROGRAM
WOULD
MAKE
INSPECTION
MORE
DIFFICULT
SO
WE
NEED
TO
PROCEED
TO
THE
NEXT
STEP
MOST
SYSTEMS
HAVE
C
DEBUGGING
PROGRAMS
HOWEVER
EACH
SYSTEM
IS
DIFFERENT
SOME
SYSTEMS
HAVE
NO
DEBUGGER
IN
SUCH
A
CASE
WE
MUST
RESORT
TO
A
DIAGNOSTIC
PRINT
STATEMENT
THE
TECHNIQUE
IS
SIMPLE
PUT
A
PRINTF
AT
THE
POINTS
AT
WHICH
YOU
KNOW
THE
DATA
IS
GOOD
JUST
TO
MAKE
SURE
THE
DATA
IS
REALLY
GOOD
THEN
PUT
APRINTF
AT
POINTS
AT
WHICH
THE
DATA
IS
BAD
RUN
THE
PROGRAM
AND
KEEP
PUTTING
IN
PRINTF
STATEMENTS
UNTIL
YOU
ISOLATE
THE
AREA
IN
THE
PROGRAM
THAT
CONTAINS
THE
MISTAKE
OUR
PROGRAM
WITH
DIAGNOSTIC
PRINTF
STATEMENTS
ADDED
LOOKS
LIKE
PRINTF
ENTER
OPERATOR
AND
NUMBER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
D
C
VALUE
OPERATOR
PRINTF
AFTER
SCANF
C
N
OPERATOR
IF
OPERATOR
PRINTF
AFTER
IF
C
N
OPERATOR
RESULT
VALUE
RUNNING
OUR
PROGRAM
AGAIN
RESULTS
IN
RESULT
ENTER
OPERATOR
AND
NUMBER
RESULT
ENTER
OPERATOR
AND
NUMBER
AFTER
SCANF
AFTER
IF
RESULT
ENTER
OPERATOR
AND
NUMBER
X
AFTER
SCANF
X
AFTER
IF
RESULT
FROM
THIS
EXAMPLE
WE
SEE
THAT
SOMETHING
IS
GOING
WRONG
WITH
THE
IF
STATEMENT
SOMEHOW
THE
VARIABLE
OPERATOR
IS
AN
X
GOING
IN
AND
A
COMING
OUT
CLOSER
INSPECTION
REVEALS
THAT
WE
HAVE
MADE
THE
OLD
MISTAKE
OF
USING
INSTEAD
OF
AFTER
WE
FIX
THIS
BUG
THE
PROGRAM
RUNS
CORRECTLY
BUILDING
ON
THIS
WORKING
FOUNDATION
WE
ADD
CODE
FOR
THE
OTHER
OPERATORS
DASH
ASTERISK
AND
SLASH
THE
RESULT
IS
SHOWN
IN
EXAMPLE
EXAMPLE
C
INCLUDE
STDIO
H
CHAR
LINE
LINE
OF
TEXT
FROM
INPUT
INT
RESULT
THE
RESULT
OF
THE
CALCULATIONS
CHAR
OPERATOR
OPERATOR
THE
USER
SPECIFIED
INT
VALUE
VALUE
SPECIFIED
AFTER
THE
OPERATOR
INT
MAIN
RESULT
INITIALIZE
THE
RESULT
LOOP
FOREVER
OR
UNTIL
BREAK
REACHED
WHILE
PRINTF
RESULT
D
N
RESULT
PRINTF
ENTER
OPERATOR
AND
NUMBER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
C
D
OPERATOR
VALUE
IF
OPERATOR
Q
OPERATOR
Q
BREAK
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
IF
VALUE
PRINTF
ERROR
DIVIDE
BY
ZERO
N
PRINTF
OPERATION
IGNORED
N
ELSE
RESULT
VALUE
ELSE
PRINTF
UNKNOWN
OPERATOR
C
N
OPERATOR
RETURN
WE
EXPAND
OUR
TEST
PLAN
TO
INCLUDE
THE
NEW
OPERATORS
AND
TRY
IT
AGAIN
RESULT
SHOULD
BE
RESULT
SHOULD
BE
X
ERROR
MESSAGE
SHOULD
BE
OUTPUT
RESULT
SHOULD
BE
ZERO
RESULT
SHOULD
BE
RESULT
SHOULD
BE
DIVIDE
BY
ZERO
ERROR
Q
RESULT
SHOULD
BE
PROGRAM
SHOULD
EXIT
WHILE
TESTING
THE
PROGRAM
WE
FIND
THAT
MUCH
TO
OUR
SURPRISE
THE
PROGRAM
WORKS
THE
WORD
PRELIMINARY
IS
REMOVED
FROM
THE
SPECIFICATION
AND
THE
PROGRAM
TEST
PLAN
AND
SPECIFICATION
ARE
RELEASED
MAINTENANCE
GOOD
PROGRAMME
RS
PUT
EACH
PROGRAM
THROUGH
A
LONG
AND
RIGOROUS
TESTING
PROCESS
BEFORE
RELEASING
IT
TO
THE
OUTSIDE
WORLD
THEN
THE
FIRST
USER
TRIES
THE
PROGRAM
AND
ALMOST
IMMEDIATELY
FINDS
A
BUG
THIS
STEP
IS
THE
MAINTENANCE
PHASE
BUGS
ARE
FIXED
THE
PROGRAM
IS
TESTED
T
O
MAKE
SURE
THAT
THE
FIXES
DIDN
T
BREAK
ANYTHING
AND
THE
PROGRAM
IS
RELEASED
AGAIN
REVISIONS
ALTHOUGH
THE
PROGRAM
IS
OFFICIALLY
FINISHED
WE
ARE
NOT
DONE
WITH
IT
AFTER
THE
PROGRAM
IS
IN
USE
FOR
A
FEW
MONTHS
SOMEONE
WILL
COME
TO
US
AND
ASK
CAN
YOU
ADD
A
MODULUS
OPERATOR
SO
WE
REVISE
THE
SPECIFICATIONS
ADD
THE
CHANGE
TO
THE
PROGRAM
UPDATE
THE
TEST
PLAN
TEST
THE
PROGRAM
AND
THEN
RELEASE
THE
PROGRAM
AGAIN
AS
TIME
PASSES
MORE
PEOPLE
WILL
COME
TO
US
WITH
ADDITIONAL
REQUESTS
FOR
CHANGES
SOON
O
UR
PROGRAM
HAS
TRIG
FUNCTIONS
LINEAR
REGRESSIONS
STATISTICS
BINARY
ARITHMETIC
AND
FINANCIAL
CALCULATIONS
OUR
DESIGN
IS
BASED
ON
THE
CONCEPT
OF
ONE
CHARACTER
OPERATORS
SOON
WE
FIND
OURSELVES
RUNNING
OUT
OF
CHARACTERS
TO
USE
AT
THIS
POINT
OUR
PROGRAM
IS
DOING
WORK
FAR
IN
EXCESS
OF
WHAT
IT
WAS
INITIALLY
DESIGNED
TO
DO
SOONER
OR
LATER
WE
REACH
THE
POINT
WHERE
THE
PROGRAM
NEEDS
TO
BE
SCRAPPED
AND
A
NEW
ONE
WRITTEN
FROM
SCRATCH
AT
THAT
POINT
WE
WRITE
A
PRELIMINARY
SPECIFICATION
AND
START
THE
PROCESS
AG
AIN
ELECTRONIC
ARCHAEOLOGY
ELECTRONIC
ARCHEOLOGY
IS
THE
ART
OF
DIGGING
THROUGH
OLD
CODE
TO
DISCOVER
AMAZING
THINGS
LIKE
HOW
AND
WHY
THE
CODE
WORKS
UNFORTUNATELY
MOST
PROGRAMMERS
DON
T
START
A
PROJECT
AT
THE
DESIGN
STEP
INSTEAD
THEY
ARE
IMMEDIA
TELY
THRUST
INTO
THE
MAINTENANCE
OR
REVISION
STAGE
AND
MUST
FACE
THE
WORST
POSSIBLE
JOB
UNDERSTANDING
AND
MODIFYING
SOMEONE
ELSE
CODE
YOUR
COMPUTER
CAN
AID
GREATLY
IN
YOUR
SEARCH
TO
DISCOVER
THE
TRUE
MEANING
OF
SOMEONE
ELSE
CODE
MANY
TOOLS
ARE
AVAILABLE
FOR
EXAMINING
AND
FORMATTING
CODE
SOME
OF
THESE
TOOLS
INCLUDE
CROSS
REFERENCES
THESE
PROGRAMS
HAVE
NAMES
LIKE
XREF
CXREF
AND
CROSS
SYSTEM
V
UNIX
HAS
THE
UTILITY
CSCOPE
A
CROSS
REFERENCE
PRINTS
OUT
A
LIST
OF
VARIABLES
AND
INDICATES
WHERE
EACH
VA
RIABLE
IS
USED
PROGRAM
INDENTERS
PROGRAMS
LIKE
CB
AND
INDENT
WILL
TAKE
A
PROGRAM
AND
INDENT
IT
CORRECTLY
CORRECT
INDENTATION
IS
SOMETHING
DEFINED
BY
THE
TOOL
MAKER
PRETTY
PRINTERS
A
PRETTY
PRINTER
SUCH
AS
VGRIND
OR
CPRINT
WILL
TAKE
THE
SOURCE
AND
TYPESET
IT
FOR
PRINTING
ON
A
LASER
PRINTER
CALL
GRAPHS
ON
SYSTEM
V
UNIX
THE
PROGRAM
CFLOW
CAN
BE
USED
TO
ANALYZE
THE
PROGRAM
ON
OTHER
SYSTEMS
THERE
IS
A
PUBLIC
DOMAIN
UTILITY
CALLS
WHICH
PRODUCES
CALL
GRAPHS
THE
CALL
GRAPHS
SHOW
WHO
CALLS
WHOM
AND
WHO
IS
CALLED
BY
WHOM
WHICH
TOOLS
SHOULD
YOU
USE
WHICHEVER
WORK
FOR
YOU
DIFFERENT
PROGRAMMERS
WORK
IN
DIFFERENT
WAYS
SOME
OF
THE
TECHNIQUES
FOR
EXAMINING
CODE
ARE
LISTED
IN
THE
SECTIONS
BELOW
CHOOSE
THE
ONES
THAT
WORK
FOR
YOU
AND
USE
THEM
MARKING
UP
THE
PROGRAM
TAKE
A
PRINTOUT
OF
THE
PROGRAM
AND
MAKE
NOTES
ALL
OVER
IT
USE
RED
OR
BLUE
INK
SO
THAT
YOU
CAN
TELL
THE
DIFFERENCE
BETWEEN
THE
PRINTOUT
AND
THE
NOTES
USE
A
HIGHLIGHTER
TO
EMPHASIZE
IMPORTANT
SECTIONS
THESE
NOTES
ARE
USEFUL
PUT
THEM
IN
THE
PROGRAM
AS
COMMENTS
THEN
MAKE
A
NEW
PRINTOUT
AND
START
THE
PROCESS
AGAIN
USING
THE
DEBUGGER
THE
DEBUGGER
IS
A
GREAT
TOOL
FOR
UNDERSTANDING
HOW
SOMETHING
WORKS
MOST
DEBUGGERS
ALLOW
THE
USER
TO
STEP
THROUGH
THE
PROGRAM
ONE
LINE
AT
A
TIME
EXAMININ
G
VARIABLES
AND
DISCOVERING
HOW
THINGS
REALLY
WORK
AFTER
YOU
FIND
OUT
WHAT
THE
CODE
DOES
MAKE
NOTES
AND
PUT
THEM
IN
THE
PROGRAM
AS
COMMENTS
TEXT
EDITOR
AS
A
BROWSER
ONE
OF
THE
BEST
TOOLS
FOR
GOING
THROUGH
SOMEONE
ELSE
CODE
IS
YOUR
TEXT
EDITOR
SUPPOSE
YOU
WANT
TO
FIND
OUT
WHAT
THE
VARIABLE
SC
IS
USED
FOR
USE
THE
SEARCH
COMMAND
TO
FIND
THE
FIRST
PLACE
SC
IS
USED
SEARCH
AGAIN
AND
FIND
THE
SECOND
TIME
IT
IS
USED
CONTINUE
SEARCHING
UNTIL
YOU
KNOW
WHAT
THE
VARIABLE
DOES
SUPPOSE
YOU
FIND
OUT
THAT
SC
IS
USED
AS
A
SEQUENCE
COUNTER
BECAUSE
YOU
RE
ALREADY
IN
THE
EDITOR
YOU
CAN
EASILY
DO
A
GLOBAL
SEARCH
AND
REPLACE
TO
CHANGE
SC
TO
DISASTER
WARNING
BEFORE
YOU
MAKE
THE
CHANGE
MAKE
SURE
THAT
IS
NOT
ALREADY
DEFINED
AS
A
VARIABLE
ALSO
WATCH
OUT
FOR
UNWANTED
REPLACEMENTS
SUCH
AS
CHANGING
THE
SC
IN
ESCAPE
COMMENT
THE
DECLARATION
AND
YOU
RE
ON
YOUR
WAY
TO
CREATING
AN
UNDERSTANDABLE
PROGRAM
ADD
COMMENTS
DON
T
BE
AFRAID
OF
PUTTING
ANY
INFORMATION
YOU
HAVE
NO
MA
TTER
HOW
LITTLE
INTO
THE
COMMENTS
SOME
OF
THE
COMMENTS
I
VE
USED
INCLUDE
INT
STATE
CONTROLS
SOME
SORT
OF
STATE
MACHINE
INT
RMXY
SOMETHING
TO
DO
WITH
COLOR
CORRECTION
FINALLY
THERE
IS
A
CATCH
ALL
COMMENT
INT
IDN
WHICH
MEANS
I
HAVE
NO
IDEA
WHAT
THIS
VARIABLE
DOES
EVEN
THOUGH
THE
VARIABLE
PURPOSE
IS
UNKNOWN
IT
IS
NOW
MARKED
AS
SOMETHING
THAT
NEEDS
MORE
WORK
AS
YOU
GO
THROUGH
SOMEONE
ELSE
CODE
ADDING
COMMENTS
AND
IMPROVING
STYLE
THE
STRUCTURE
WILL
BECOME
CLEARER
TO
YOU
BY
INSERTING
NOTES
COMMENTS
YOU
MAKE
THE
CODE
BETTER
AND
EASIER
TO
UNDERSTAND
FOR
FUTURE
PROGRAMMERS
FOR
EXAMPLE
SUPPOSE
WE
ARE
CONFRONTED
WITH
THE
FOLLOWING
PROGRAM
WRITTEN
BY
SOMEONE
FROM
THE
TERSER
THE
BETTER
SCHOOL
OF
PROGRAMMING
OUR
ASSIGNMENT
IS
TO
FIGURE
OUT
WHAT
THIS
CODE
DOES
FIRST
WE
PENCIL
IN
SOME
COMMENTS
AS
SHOWN
IN
FIGURE
FIGURE
A
TERSE
PROGRAM
OUR
MYSTERY
PROGRAM
REQUIRES
SOME
WORK
AFTER
GOING
THROUGH
IT
AND
APPLYING
THE
PRINCIPLES
DESCRIBED
IN
THIS
SECTION
WE
GET
A
WELL
COMMENTED
EASY
TO
UNDERSTAND
PROGRAM
SUCH
AS
EXAMPLE
EXAMPLE
GOOD
GOOD
C
GUESS
A
SIMPLE
GUESSING
GAME
USAGE
GUESS
A
RANDOM
NUMBER
IS
CHOSEN
BETWEEN
AND
THE
PLAYER
IS
GIVEN
A
SET
OF
BOUNDS
AND
MUST
CHOOSE
A
NUMBER
BETWEEN
THEM
IF
THE
PLAYER
CHOOSES
THE
CORRECT
NUMBER
HE
WINS
OTHERWISE
THE
BOUNDS
ARE
ADJUSTED
TO
REFLECT
THE
PLAYER
GUESS
AND
THE
GAME
CONTINUES
RESTRICTIONS
THE
RANDOM
NUMBER
IS
GENERATED
BY
THE
STATEMENT
RAND
BECAUSE
RAND
RETURNS
A
NUMBER
RAND
MAXINT
THIS
SLIGHTLY
FAVORS
THE
LOWER
NUMBERS
INCLUDE
STDIO
H
INCLUDE
STDLIB
H
INT
RANDOM
NUMBER
TO
BE
GUESSED
INT
CURRENT
LOWER
LIMIT
OF
PLAYER
RANGE
INT
CURRENT
UPPER
L
IMIT
OF
PLAYER
RANGE
INT
NUMBER
OF
TIMES
PLAYER
GUESSED
INT
NUMBER
GOTTEN
FROM
THE
PLAYER
CHAR
LINE
INPUT
BUFFER
FOR
A
SINGLE
LINE
INT
MAIN
WHILE
NOT
A
PURE
RANDOM
NUMBER
SEE
RESTRICTIONS
RAND
INITIALIZE
VARIABLES
FOR
LOOP
WHILE
TELL
USER
WHAT
THE
BOUNDS
ARE
AND
GET
HIS
GUESS
PRINTF
BOUNDS
D
D
N
PRINTF
VALUE
D
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
DID
HE
GUESS
RIGHT
IF
BREAK
ADJUST
BOUNDS
FOR
NEXT
GUESS
IF
ELSE
PRINTF
BINGO
N
PROGRAMMING
EXERCISES
FOR
EACH
OF
THESE
ASSIGNMENTS
FOLLOW
THE
SOFTWARE
LIFE
CYCLE
FROM
SPECIFICATION
THROUGH
RELEASE
EXERCISE
WRITE
A
PROGRAM
TO
CONVERT
ENGLISH
UNITS
TO
METRIC
I
E
MILES
TO
KILOMETERS
GALLONS
TO
LITERS
ETC
INCLUDE
A
SPECIFICATION
AND
A
CODE
DESIGN
EXERCISE
WRITE
A
PROGRAM
TO
PERFORM
DATE
ARITHMETIC
SUCH
AS
HOW
MANY
DAYS
THERE
ARE
BETWEEN
AND
INCLUDE
A
SPECIFICATION
AND
A
CODE
DESIGN
EXERCISE
A
SERIAL
TRANSMISSION
LINE
CAN
TRANSMIT
CHARACTERS
EACH
SECOND
WRITE
A
PROGRAM
THAT
WILL
CALCULATE
THE
TIME
REQUIRED
TO
SEND
A
FILE
GIVEN
THE
FILE
SIZE
TRY
THE
PROG
RAM
ON
A
BYTE
FILE
USE
APPROPRIATE
UNITS
A
FILE
TAKES
DAYS
EXERCISE
WRITE
A
PROGRAM
TO
ADD
AN
SALES
TAX
TO
A
GIVEN
AMOUNT
AND
ROUND
THE
RESULT
TO
THE
NEAREST
PENNY
EXERCISE
WRITE
A
PROGRAM
TO
TELL
IF
A
NUMBER
IS
PRIME
EXERCISE
WRITE
A
PROGRAM
THAT
TAKES
A
SERIES
OF
NUMBERS
AND
COUNTS
THE
NUMBER
OF
POSITIVE
AND
NEGATIVE
VALUES
PART
II
SIMPLE
PROGRAMMING
THIS
PART
BUILDS
ON
THE
BASICS
TO
ROUND
OUT
OUR
DESCRIPTION
OF
SIMPLE
C
PROGRAMMING
IN
THIS
PART
WE
L
EARN
THE
REST
OF
THE
CONTROL
STATEMENTS
AS
WELL
AS
SOME
MORE
ADVANCED
OPERATIONS
SUCH
AS
BIT
OPERATIONS
FINALLY
WE
GET
AN
INTRODUCTION
TO
SOME
MORE
SOPHISTICATED
PROGRAMMING
TASKS
SUCH
AS
FILE
I
O
AND
DEBUGGING
CHAPTER
DESCRIBES
ADDITIONAL
CONTROL
STATEMENTS
INCLUDED
ARE
FOR
BREAK
AND
CONTINUE
THE
SWITCH
STATEMENT
IS
DISCUSSED
IN
DETAIL
CHAPTER
INTRODUCES
LOCAL
VARIABLES
FUNCTIONS
AND
PARAMETERS
CHAPTER
DESCRIBES
THE
C
PREPROCESSOR
WHICH
GIVES
THE
PROGRAMMER
TREMENDOUS
FLEXIBILITY
IN
WRITING
CODE
THE
CHAPTER
ALSO
P
ROVIDES
THE
PROGRAMMER
WITH
A
TREMENDOUS
NUMBER
OF
WAYS
TO
MESS
UP
SIMPLE
RULES
THAT
HELP
KEEP
THE
PREPROCESSOR
FROM
BECOMING
A
PROBLEM
ARE
DESCRIBED
CHAPTER
DISCUSSES
THE
LOG
ICAL
C
OPERATORS
THAT
WORK
ON
BITS
CHAPTER
EXPLAINS
STRUCTURES
AND
OTHER
ADVANCED
TYPES
THE
SIZEOF
OPERATOR
AND
THE
ENUM
TYPE
ARE
INCLUDED
CHAPTER
INTRODUCES
C
POINTER
VARIABLES
AND
SHOWS
SOME
OF
THEIR
USES
CHAPTER
DESCRIBES
BOTH
BUFFERED
AND
UNBUFFERED
INPUT
OUTPUT
ASC
II
VERSUS
BINARY
FILES
ARE
DISCUSSED
AND
YOU
ARE
SHOWN
HOW
TO
CONSTRUCT
A
SIMPLE
FILE
CHAPTER
DESCRIBES
HOW
TO
DEBUG
A
PROGRAM
AS
WELL
AS
HOW
TO
USE
AN
INTERACTIVE
DEBUGGER
YOU
ARE
SHOWN
NOT
ONLY
HOW
TO
DEBUG
A
PROGRAM
BUT
ALSO
HOW
TO
WRITE
A
PROGRAM
SO
THAT
IT
IS
EASY
TO
DEBUG
THIS
CHAPTER
ALSO
DESCRIBES
MANY
OPTIMIZATION
TECHNIQUES
FOR
MAKING
YOUR
PROGRAM
RUN
FASTER
AND
MORE
EFFICIENTLY
CHAPTER
USES
A
SIMPLE
DECIMAL
FLOATING
POINT
FORMAT
TO
INTRODUCE
YOU
TO
THE
PROBLEMS
INHERENT
IN
FLOATING
POINT
SUCH
AS
ROUNDOFF
ERROR
PRECISION
LOSS
OVERFLOW
AND
UNDERFLOW
CHAPTER
MORE
CONTROL
STATEMENTS
GRAMMAR
WHICH
KNOWS
HOW
TO
CONTROL
EVEN
KINGS
MOLIÈRE
FOR
STATEMENT
THE
FOR
STATEMENT
ALLOWS
THE
PROGRAMMER
TO
EXECUTE
A
BLOCK
OF
CODE
FOR
A
SPECIFIED
NUMBER
OF
TIMES
THE
GENERAL
FORM
OF
THE
FOR
STATEMENT
IS
FOR
INITIAL
STATEMENT
CONDITION
ITERATION
STATEMENT
BODY
STATEMENT
THIS
STATEMENT
IS
EQUIVALENT
TO
INITIAL
STATEMENT
WHILE
CONDITION
BODY
STATEMENT
ITERATION
STATEMENT
FOR
EXAMPLE
EXAMPLE
USES
A
WHILE
LOOP
TO
ADD
FIVE
NUMBERS
EXAMPLE
TOTALW
C
INCLUDE
STDIO
H
INT
TOTAL
TOTAL
OF
ALL
THE
NUMBERS
INT
CURRENT
CURRENT
VALUE
FROM
THE
USER
INT
COUNTER
WHILE
LOOP
COUNTER
CHAR
LINE
LINE
FROM
KEYBOARD
INT
MAIN
TOTAL
COUNTER
WHILE
COUNTER
PRINTF
NUMBER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
CURRENT
TOTAL
CURRENT
COUNTER
PRINTF
THE
GRAND
TOTAL
IS
D
N
TOTAL
RETURN
THE
SAME
PROGRAM
CAN
BE
REWRITTEN
USING
A
FOR
STATEMENT
AS
SHOWN
IN
EXAMPLE
EXAMPLE
C
INCLUDE
STDIO
H
INT
TOTAL
TOTAL
OF
ALL
THE
NUMBERS
INT
CURRENT
CURRENT
VALUE
FROM
THE
USER
INT
COUNTER
FOR
LOOP
COUNTER
CHAR
LINE
INPUT
FROM
KEYBOARD
INT
MAIN
TOTAL
FOR
COUNTER
COUNTER
COUNTER
PRINTF
NUMBER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
CURRENT
TOTAL
CURRENT
PRINTF
THE
GRAND
TOTAL
IS
D
N
TOTAL
RETURN
NOTE
THAT
COUNTER
GOES
FROM
TO
ORDINARILY
YOU
COUNT
FIVE
ITEMS
AS
BUT
YOU
WILL
PERFORM
MUCH
BETTER
IN
C
IF
YOU
CHANGE
YOUR
THINKING
TO
ZERO
BASED
COUNTING
AND
THEN
COUNT
FIVE
ITEMS
AS
ONE
BASED
COUNTING
IS
ONE
OF
THE
MAIN
CA
USES
OF
ARRAY
OVERFLOW
ERRORS
SEE
CHAPTER
CAREFUL
EXAMINATION
OF
THE
TWO
FLAVORS
OF
OUR
PROGRAM
REVEALS
THE
SIMILARITIES
BETWEEN
THE
TWO
VERSIONS
AS
SEEN
IN
FIGURE
FIGURE
SIMILARITIES
BETWEEN
WHILE
AND
FOR
MANY
OTHER
PROGRAMMING
LANGUAG
ES
DO
NOT
ALLOW
YOU
TO
CHANGE
THE
CONTROL
VARIABLE
IN
THIS
CASE
COUNTER
INSIDE
THE
LOOP
C
IS
NOT
SO
PICKY
YOU
CAN
CHANGE
THE
CONTROL
VARIABLE
AT
ANY
TIME
YOU
CAN
JUMP
INTO
AND
OUT
OF
THE
LOOP
AND
GENERALLY
DO
THINGS
THAT
WOULD
MAKE
A
PASCAL
OR
FORTRAN
PROGRAMMER
CRINGE
ALTHOUGH
C
GIVES
YOU
THE
FREEDOM
TO
DO
SUCH
INSANE
THINGS
THAT
DOESN
T
MEAN
YOU
SHOULD
DO
THEM
QUESTION
WHEN
EXAMPLE
RUNS
IT
PRINTS
CELSIUS
FAHRENHEIT
AND
NOTHING
MORE
WHY
CLICK
HERE
FOR
THE
ANSWER
SECTION
EXAMPLE
CENT
CENT
C
INCLUDE
STDIO
H
THIS
PROGRAM
PRODUCES
A
CELSIUS
TO
FAHRENHEIT
CONVERSION
CHART
FOR
THE
NUMBERS
TO
THE
CURRENT
CELSIUS
TEMPERATURE
WE
ARE
WORKING
WITH
INT
CELSIUS
INT
MAIN
FOR
CELSIUS
CELSIUS
CELSIUS
PRINTF
CELSIUS
D
FAHRENHEIT
D
N
CELSIUS
CELSIUS
RETURN
QUESTION
EXAMPLE
READS
A
LIST
OF
FIVE
NUMBERS
AND
COUNTS
THE
NUMBER
OF
AND
IN
THE
DATA
WHY
DOES
IT
GIVE
US
THE
WRON
G
ANSWERS
CLICK
HERE
FOR
THE
ANSWER
SECTION
EXAMPLE
SEVEN
SEVEN
C
INCLUDE
STDIO
H
CHAR
LINE
LINE
OF
INPUT
INT
NUMBER
OF
IN
THE
DATA
INT
DATA
THE
DATA
TO
COUNT
AND
IN
INT
THE
NUMBER
OF
IN
THE
DATA
INT
INDEX
INDEX
INTO
THE
DATA
INT
MAIN
PRINTF
ENTER
NUMBERS
N
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
D
D
D
D
D
DATA
DATA
DATA
DATA
DATA
FOR
INDEX
INDEX
INDEX
IF
DATA
INDEX
IF
DATA
INDEX
PRINTF
THREES
D
SEVENS
D
N
RETURN
WHEN
WE
RUN
THIS
PROGRAM
WITH
THE
DATA
THE
RESULTS
ARE
THREES
SEVENS
YOUR
RESULTS
MAY
VARY
SWITCH
STATEMENT
THE
SWITCH
STATEMENT
IS
SIMILAR
TO
A
CHAIN
OF
IF
ELSE
STATEMENTS
THE
GENERAL
FORM
OF
A
SWITCH
STATEMENT
IS
SWITCH
EXPRESSION
CASE
STATEMENT
BREAK
CASE
STATEMENT
FALL
THROUGH
DEFAULT
STATEMENT
BREAK
CASE
STATEMENT
BREAK
THE
SWITCH
STATEMENT
EVALUATES
THE
VALUE
OF
AN
EXPRESSION
AND
BRANCHES
TO
ONE
OF
THE
CASE
LABELS
DUPLICATE
LABELS
ARE
NOT
ALLOWED
SO
ONLY
ONE
CASE
WILL
BE
SELECTED
THE
EXPRESSION
MUST
EVALUATE
AN
INTEGER
CHARACTER
OR
ENUMERATION
THE
CASE
LABELS
CAN
BE
IN
ANY
ORDER
AND
MUST
BE
CONSTANTS
TH
E
DEFAULT
LABEL
CAN
BE
PUT
ANYWHERE
IN
THE
SWITCH
NO
TWO
CASE
LABELS
CAN
HAVE
THE
SAME
VALUE
WHEN
C
SEES
A
SWITCH
STATEMENT
IT
EVALUATES
THE
EXPRESSION
AND
THEN
LOOKS
FOR
A
MATCHING
CASE
LABEL
IF
NONE
IS
FOUND
THE
DEFAULT
LABEL
IS
USED
IF
NO
DEFAULT
IS
FOUND
THE
STATEMENT
DOES
NOTHING
EXAMPLE
CONTAINS
A
SERIES
OF
IF
AND
ELSE
STATEMENTS
EXAMPLE
SYNTAX
FOR
IF
AND
ELSE
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
RESULT
VALUE
ELSE
IF
OPERATOR
IF
VALUE
PRINTF
ERROR
DIVIDE
BY
ZERO
N
PRINTF
OPERATION
IGNORED
N
ELSE
RESULT
VALUE
ELSE
PRINTF
UNKNOWN
OPERATOR
C
N
OPERATOR
THIS
SECTION
OF
CODE
CAN
EASILY
BE
REWRITTEN
AS
A
SWITCH
STATEMENT
IN
THIS
SWITCH
WE
USE
A
DIFFERENT
CASE
FOR
EACH
OPERATION
THE
DEFAULT
CLAUSE
TAKES
CARE
OF
ALL
THE
ILLEGAL
OPERATORS
REWRITING
OUR
PROGRAM
USING
A
SWITCH
STATEMENT
MAKES
IT
NOT
ONLY
SIMPLER
BUT
EASIER
TO
READ
OUR
REVISED
CALC
PROGRAM
IS
SHOWN
AS
EXAMPLE
EXAMPLE
C
INCLUDE
STDIO
H
CHAR
LINE
LINE
OF
TEXT
FROM
INPUT
INT
RESULT
THE
RESULT
OF
THE
CALCULATIONS
CHAR
OPERATOR
OPERATOR
THE
USER
SPECIFIED
INT
VALUE
VALUE
SPECIFIED
AFTER
THE
OPERATOR
INT
MAIN
RESULT
INITIALIZE
THE
RESULT
LOOP
FOREVER
OR
UNTIL
BREAK
REACHED
WHILE
PRINTF
RESULT
D
N
RESULT
PRINTF
ENTER
OPERATOR
AND
NUMBER
FGETS
LINE
SIZEOF
LINE
STDIN
SSCANF
LINE
C
D
OPERATOR
VALUE
IF
OPERATOR
Q
OPERATOR
Q
BREAK
SWITCH
OPERATOR
CASE
RESULT
VALUE
BREAK
CASE
RESULT
VALUE
BREAK
CASE
RESULT
VALUE
BREAK
CASE
IF
VALUE
PRINTF
ERROR
DIVIDE
BY
ZERO
N
PRINTF
OPERATION
IGNORED
N
ELSE
RESULT
VALUE
BREAK
DEFAULT
PRINTF
UNKNOWN
OPERATOR
C
N
OPERATOR
BREAK
RETURN
A
BREAK
STATEMENT
INSIDE
A
SWITCH
TELLS
THE
COMPUTER
TO
CONTINUE
EXECUTION
AFTER
THE
SWITCH
IF
A
BREAK
STATEMENT
IS
NOT
THERE
EXECUTION
WILL
CONTINUE
WITH
THE
NEXT
STATEMENT
FOR
EXAMPLE
CONTROL
A
NOT
SO
GOOD
EXAMPLE
OF
PROGRAMMING
SWITCH
CONTROL
CASE
PRINTF
RESET
N
CASE
PRINTF
INITIALIZING
N
BREAK
CASE
PRINTF
WORKING
N
IN
THIS
CASE
W
HEN
CONTROL
THE
PROGRAM
WILL
PRINT
RESET
INITIALIZING
CASE
DOES
NOT
END
WITH
A
BREAK
STATEMENT
AFTER
PRINTINGRESET
THE
PROGRAM
FALLS
THROUGH
TO
THE
NEXT
STATEMENT
CASE
AND
PRINTS
INITIALIZING
A
PROBLEM
EXISTS
WITH
THIS
SYNTAX
YOU
CANNOT
DETERMINE
IF
THE
PROGRAM
IS
SUPPOSED
TO
FALL
THROUGH
FROM
CASE
TO
CASE
OR
IF
THE
PROGRAMMER
FORGOT
TO
PUT
IN
A
BREAK
STATEMENT
IN
ORDER
TO
CLEAR
UP
THIS
CONFUSION
A
CASE
SECTION
SHOULD
ALWAYS
END
WITH
A
BREAK
STATEMENT
OR
THE
COMMENT
FALL
THROUGH
AS
SHOWN
IN
THE
FOLLOWING
EXAMPLE
A
BETTER
EXAMPLE
OF
PROGRAMMING
SWITCH
CONTROL
CASE
PRINTF
RESET
N
FALL
THROUGH
CASE
PRINTF
INITIALIZING
N
BREAK
CASE
PRINTF
WORKING
N
BECAUSE
CASE
IS
LAST
IT
DOESN
T
NEED
ABREAK
STATEMENT
A
BREAK
WOULD
CAUSE
THE
PROGRAM
TO
SKIP
TO
THE
END
OF
THE
SWITCH
AND
WE
RE
ALREADY
THERE
SUPPOSE
WE
MODIFY
THE
PROGRAM
SLIGHTLY
AND
ADD
ANOTHER
CASE
TO
THE
SWITCH
WE
HAVE
A
LITTLE
PROBLEM
SWITCH
CONTROL
CASE
PRINTF
RESET
N
FALL
THROUGH
CASE
PRINTF
INITIALIZING
N
BREAK
CASE
PRINTF
WORKING
N
CASE
PRINTF
CLOSING
DOWN
N
NOW
WHEN
CONTROL
THE
PROGRAM
PRINTS
WORKING
CLOSING
DOWN
THIS
RESULT
IS
AN
UNPLEASANT
SURPRISE
THE
PROBLEM
IS
CAUSED
BY
THE
FACT
THAT
CASE
IS
NO
LONGER
THE
LAST
CASE
WE
FALL
THROUGH
UNINTENTIONALLY
OTHERWISE
WE
WOULD
HAVE
INCLUDED
A
FALL
THROUGH
COMMENT
A
BREAK
IS
NOW
NECESSARY
IF
WE
ALWAYS
PUT
IN
A
BREAK
STATEMENT
WE
DON
T
HAVE
TO
WORRY
ABOUT
WHETHER
OR
NOT
IT
IS
REALLY
NEEDED
ALMOST
THERE
SWITCH
CONTROL
CASE
PRINTF
RESET
N
FALL
THROUGH
CASE
PRINTF
INITIALIZING
N
BREAK
CASE
PRINTF
WORKING
N
BREAK
FINALLY
WE
ASK
THE
QUESTION
WHAT
HAPPENS
WHEN
CONTROL
IN
THIS
CASE
BECAUSE
NO
MATCHING
CASE
OR
DEFAULT
CLAUSE
EXISTS
THE
ENTIRE
SWITCH
STATEMENT
IS
SKIPPED
IN
THIS
EXAMPLE
THE
PROGRAMMER
DID
NOT
INCLUDE
A
DEFAULT
STATEMENT
BECAUSE
CONTROL
WILL
NEVER
BE
ANYTHING
BUT
OR
HOWEVER
VARIABLES
CAN
GET
ASSIGNED
STRANGE
VALUES
SO
WE
NEED
A
LITTLE
MORE
DEFENSIVE
PROGRAMMING
AS
SHOWN
IN
THE
FOLLOWING
EXAMPLE
THE
FINAL
VERSION
SWITCH
CONTROL
CASE
PRINTF
RESET
N
FALL
THROUGH
CASE
PRINTF
INITIALIZING
N
BREAK
CASE
PRINTF
WORKING
N
BREAK
DEFAULT
PRINTF
INTERNAL
ERROR
CONTROL
VALUE
D
IMPOSSIBLE
N
CONTROL
BREAK
ALTHOUGH
A
DEFAULT
IS
NOT
REQUIRED
IT
SHOULD
BE
PUT
IN
EVERY
SWITCH
EVEN
THOUGH
THE
DEFAULT
MAY
BE
DEFAULT
DO
NOTHING
BREAK
IT
SHOULD
BE
INCLUDED
THIS
METHOD
INDICATES
AT
THE
VERY
LEAST
THAT
YOU
WANT
TO
IGNORE
OUT
OF
RANGE
DATA
SWITCH
BREAK
AND
CONTINUE
THE
BREAK
STATEMENT
HAS
TWO
USES
USED
INSIDE
A
SWITCH
BREAK
CAUSES
THE
PROGRAM
TO
GO
TO
THE
END
OF
THE
SWITCH
INSIDE
A
FOR
OR
WHILE
LOOP
BREAK
CAUSES
A
LOOP
EXIT
THECONTINUE
STATEMENT
IS
VALID
ONLY
INSIDE
A
LOOP
CONTINUE
WILL
CAUSE
THE
PROGRAM
TO
GO
TO
THE
TOP
OF
THE
LOOP
FIGURE
ILLU
STRATES
BOTH
CONTINUE
AND
BREAK
INSIDE
A
SWITCH
STATEMENT
THE
PROGRAM
IN
FIGURE
IS
DESIGNED
TO
CONVERT
AN
INTEGER
WITH
A
NUMBER
OF
DIFFERENT
FORMATS
INTO
DIFFERENT
BASE
IF
YOU
WANT
TO
KNOW
THE
VALUE
OF
AN
OCTAL
NUMBER
YOU
WOULD
ENTER
O
FOR
OCTAL
AND
THE
NUMBER
THE
COMMAND
Q
IS
USED
TO
QUIT
THE
PROGRAM
FOR
EXAMPLE
ENTER
CONVERSION
AND
NUMBER
O
RESULT
IS
ENTER
CONVERSION
AND
NUMBER
Q
THE
HELP
COMMAND
IS
SPECIAL
BECAUSE
WE
DON
T
WANT
TO
PRINT
A
NUMBER
AFTER
THE
COMMAND
AFTER
ALL
THE
RESULT
OF
HELP
IS
A
FEW
LINES
OF
TEXT
NOT
A
NUMBER
SO
A
CONTINUE
IS
USED
INSIDE
THE
SWITCH
TO
START
THE
LOOP
AT
THE
BEGINNING
INSIDE
THE
SWITCH
THE
CONTINUE
STATEMENT
WORK
ON
THE
LOOP
WHILE
THE
BREAK
STATEMENT
WORKS
ON
THE
SWITCH
THERE
IS
ONE
BREAK
OUTSIDE
THE
SWITCH
THAT
ISDESIGNED
TO
LET
THE
USER
EXIT
THE
PROGRAM
THE
CONTROL
FLOW
FOR
THIS
PROGRAM
CAN
BE
SEEN
IN
FIGURE
FIGURE
SWITCH
CONTINUE
ANSWERS
ANSWER
THE
PROBLEM
LIES
WITH
THE
SEMICOLON
AT
THE
END
OF
THE
FOR
STATEMENT
THE
BODY
OF
THE
FOR
STATEMENT
IS
BETWEEN
THE
CLOSING
PARENTHESES
AND
THE
SEMICOLON
IN
THIS
CASE
THE
BODY
DOES
NOT
EXIST
EVEN
THOUGH
THE
PRINTF
STATEMENT
IS
INDENTED
IT
IS
NOT
PART
OF
THE
FOR
STATEMENT
THE
INDENTATION
IS
MISLEADING
THE
C
COMPILER
DOES
NOT
LOOK
AT
INDENTATION
THE
PROGRAM
DOES
NOTHING
UNTIL
THE
EXPRESSION
CELSIUS
BECOMES
FALSE
CELSIUS
THEN
THE
PRINTF
IS
EXECUTED
ANSWER
THE
PROBLEM
IS
THAT
WE
READ
THE
NUMBER
INTO
DATA
THROUGH
DATA
IN
C
THE
RANGE
O
F
LEGAL
ARRAY
INDICES
IS
TO
ARRAY
SIZE
OR
IN
THIS
CASE
TO
DATA
IS
ILLEGAL
WHEN
WE
USE
IT
STRANGE
THINGS
HAPPEN
IN
THIS
CASE
THE
VARIABLE
IS
CHANGED
THE
SOLUTION
IS
TO
ONLY
USE
DATA
TO
DATA
SO
WE
NEED
TO
CHANGE
THE
SSCANF
LINE
TO
READ
SSCANF
LINE
D
D
D
D
D
DATA
DATA
DATA
DATA
DATA
ALSO
THE
FOR
LOOP
MUST
BE
CHANGED
FROM
FOR
INDEX
INDEX
INDEX
TO
FOR
INDEX
INDEX
INDEX
PROGRAMMING
EXERCISES
EXERCISE
PRINT
A
CHECKER
BOARD
B
Y
GRID
EACH
SQUARE
SHOULD
BE
BY
CHARACTERS
WIDE
A
B
Y
EXAMPLE
FOLLOWS
EXERCISE
THE
TOTAL
RESISTANCE
OF
N
RESISTORS
IN
PARALLEL
IS
SUPPOSE
WE
HAVE
A
NETWORK
OF
TWO
RESISTORS
WITH
THE
VALUES
AND
THEN
OUR
EQUATION
WOULD
BE
SUBSTITUTING
IN
THE
VALUE
OF
THE
RESISTORS
WE
GET
SO
THE
TOTAL
RESISTANCE
OF
OUR
TWO
RESISTOR
NETWORK
IS
WRITE
A
PROGRAM
TO
COMPUTE
THE
TOTAL
RESISTANCE
FOR
ANY
NUMBER
OF
PARALLEL
RESISTORS
EXERCISE
WRITE
A
PROGRAM
TO
AVERAGE
N
NUMBERS
EXERCISE
WRITE
A
PROGRAM
TO
PRINT
OUT
THE
MULTIPLICATION
TABLE
EXERCISE
WRITE
A
PROGRAM
THAT
READS
A
CHARACTER
AND
PRINTS
OUT
WHETHER
OR
NOT
IT
IS
A
VOWEL
OR
A
CONSONANT
EXERCISE
WRITE
A
PROGRAM
THAT
CONVERTS
NUMBERS
TO
WORDS
FOR
EXAMPLE
RESULTS
IN
EIGHT
NINE
FIVE
EXERCISE
THE
NUMBER
IS
PRONOUNCED
EIGHTY
FIVE
NOT
EIGHT
FIVE
MODIFY
THE
PREVIOUS
PROGRAM
TO
HANDLE
THE
NUMBERS
THROUGH
SO
THAT
ALL
NUMBERS
COME
OUT
AS
WE
REALLY
SAY
THEM
FOR
EXAMPLE
WOULD
BE
THIRTEEN
AND
WOULD
BE
ONE
HUNDRED
CHAPTER
VARIABLE
SCOPE
AND
FUNCTIONS
BUT
IN
THE
GROSS
AND
SCOPE
OF
MY
OPINION
THIS
BODES
SOME
STRANGE
ERUPTION
TO
OUR
STATE
SHAKESP
EARE
HAMLET
ACT
SCENE
SO
FAR
WE
HAVE
BEEN
USING
ONLY
GLOBAL
VARIABLES
IN
THIS
CHAPTER
WE
WILL
LEARN
ABOUT
OTHER
KINDS
OF
VARIABLES
AND
HOW
TO
USE
THEM
THIS
CHAPTER
ALSO
TELLS
YOU
HOW
TO
DIVIDE
YOUR
CODE
INTO
FUNCTIONS
SCOPE
AND
CLASS
ALL
VARIABLES
HAVE
TWO
ATTRIBUTES
SCOPE
AND
CLASS
THE
SCOPE
OF
A
VARIABLE
IS
THE
AREA
OF
THE
PROGRAM
IN
WHICH
THE
VARIABLE
IS
VALID
A
GLOBAL
VARIABLE
IS
VALID
EVERYWHERE
HENCE
THE
NAME
GLOBAL
SO
ITS
SCOPE
IS
THE
WHOLE
PROGRAM
A
LOCAL
VARIABLE
HAS
A
SCOPE
THAT
IS
LIMITED
TO
THE
BLOCK
IN
WHICH
IT
IS
DECLARED
AND
CANNOT
BE
ACCESSED
OUTSIDE
THAT
BLOCK
A
BLOCK
IS
A
SECTION
OF
CODE
ENCLOSED
IN
CURLY
BRACES
FIGURE
SHOWS
THE
DIFFERENCE
BETWEEN
LOCAL
AND
GLOBAL
VARIABLES
FIGURE
LOCAL
AND
GLOBAL
VARIABLES
YOU
CAN
DECLARE
A
LOCAL
VARIABLE
WITH
THE
SAME
NAME
AS
A
GLOBAL
V
ARIABLE
NORMALLY
THE
SCOPE
OF
THE
VARIABLECOUNT
FIRST
DECLARATION
WOULD
BE
THE
WHOLE
PROGRAM
THE
DECLARATION
OF
A
SECOND
LOCAL
COUNT
TAKES
PRECEDENCE
OVER
THE
GLOBAL
DECLARATION
INSIDE
THE
SMALL
BLOCK
IN
WHICH
THE
LOCAL
COUNT
IS
DECLARED
IN
THIS
BLO
CK
THE
GLOBAL
COUNT
ISHIDDEN
YOU
CAN
ALSO
NEST
LOCAL
DECLARATIONS
AND
HIDE
LOCAL
VARIABLES
FIGURE
ILLUSTRATES
A
HIDDEN
VARIABLE
FIGURE
HIDDEN
VARIABLES
THE
VARIABLE
COUNT
IS
DECLARED
AS
BOTH
A
LOCAL
VARIABLE
AND
A
GLOBAL
VARIABLE
NORMALLY
THE
SCOPE
OF
COUNT
GLOBAL
IS
THE
ENTIRE
PROGRAM
HOWEVER
WHEN
A
VARIABLE
IS
DECLARED
INSIDE
A
BLOCK
THAT
INSTANCE
OF
THE
VARIABLE
BECOMES
THE
ACTIVE
ONE
FOR
THE
LENGTH
OF
THE
BLOCK
THE
GLOBAL
COUNT
HAS
BEEN
HIDDEN
BY
THE
LOCAL
COUNT
FOR
THE
SCOPE
OF
THIS
BLOCK
THE
SHADED
BLOCK
IN
THE
FIGURE
SHOWS
WHERE
THE
SCOPE
OF
COUNT
GLOBAL
IS
HIDDEN
A
PROBLEM
EXISTS
IN
THAT
WHEN
YOU
HAVE
THE
STATEMENT
COUNT
YOU
CANNOT
TELL
EASILY
TO
WHICHCOUNT
YOU
ARE
REFERRING
IS
IT
THE
GLOBAL
COUNT
THE
ONE
DECLARED
AT
THE
TOP
OF
MAIN
OR
THE
ONE
IN
THE
MIDDLE
OF
THE
WHILE
LOOP
YOU
SHOULD
GIVE
THESE
VARIABLES
DIFFERENT
NAMES
LIKE
AND
THE
CLASS
OF
A
VARIABLE
MAY
BE
EITHER
PERMANENT
OR
TEMPORARY
GLOBAL
VARIABLES
ARE
ALWAYS
PERMANENT
THEY
ARE
CREATED
AND
INITIALIZED
BEFORE
THE
PROGRAM
STARTS
AND
REMA
IN
UNTIL
IT
TERMINATES
TEMPORARY
VARIABLES
ARE
ALLOCATED
FROM
A
SECTION
OF
MEMORY
CALLED
THE
STACK
AT
THE
BEGINNING
OF
THE
BLOCK
IF
YOU
TRY
TO
ALLOCATE
TOO
MANY
TEMPORARY
VARIABLES
YOU
WILL
GET
A
STACK
OVERFLOW
ERROR
THE
SPACE
USED
BY
THE
TEMPORARY
VARIABLES
IS
RETURNED
TO
THE
STACK
AT
THE
END
OF
THE
BLOCK
EACH
TIME
THE
BLOCK
IS
ENTERED
THE
TEMPORARY
VARIABLES
ARE
INITIALIZED
THE
SIZE
OF
THE
STACK
DEPENDS
ON
THE
SYSTEM
AND
COMPILER
YOU
ARE
USING
ON
MANY
UNIX
SYSTEMS
THE
PROGRAM
IS
AUTOMATICALLY
ALLOCATED
THE
LARGEST
POSSIBLE
STACK
ON
OTHER
SYSTEMS
A
DEFAULT
STACK
SIZE
IS
ALLOCATED
THAT
CAN
BE
CHANGED
BY
A
COMPILER
SWITCH
ON
MS
DOS
WINDOWS
SYSTEMS
THE
STACK
SPACE
MUST
BE
LESS
THAN
BYTES
THIS
MAY
SEEM
LIKE
A
LOT
OF
SPACE
HOWEVER
SEVERAL
LARGE
ARRAYS
CAN
EAT
IT
UP
QUICKLY
YOU
SHOULD
CONSIDER
MAKING
ALL
LARGE
ARRAYS
PERMANENT
LOCAL
VARIABLES
ARE
TEMPORARY
UNLESS
THEY
ARE
DECLARED
STATIC
EXAMPLE
ILLUSTRATES
THE
DIFFERENCE
BETWEEN
PERMANENT
AND
TEMPORARY
VARIABLES
WE
HAVE
CHOSEN
OBVIOUS
NAMES
TEMPORARY
IS
A
TEMPORARY
VARIABLE
WHILE
PERMANENT
IS
PERMANENT
THE
VARIABLE
TEMPORARY
IS
INITIALIZED
EACH
TIME
IT
IS
CREATED
AT
THE
BEGINNING
OF
THE
FOR
STATEMENT
BLOCK
THE
VARIABLE
PERMANENT
IS
INITIALIZED
ONLY
ONCE
AT
STARTUP
TIME
IN
THE
LOOP
BOTH
VARIABLES
ARE
INCREMENTED
HOWEVER
AT
THE
TOP
OF
THE
LOOP
TEMPORARY
IS
INITIALIZED
TO
ONE
AS
SHOWN
IN
EXAMPLE
EXAMPLE
VARS
VARS
C
INCLUDE
STDIO
H
INT
MAIN
INT
COUNTER
LOOP
COUNTER
FOR
COUNTER
COUNTER
COUNTER
INT
TEMPORARY
A
TEMPORARY
VARIABLE
STATIC
INT
PERMANENT
A
PERMANENT
VARIABLE
PRINTF
TEMPORARY
D
PERMANENT
D
N
TEMPORARY
PERMANENT
TEMPORARY
PERMANENT
RETURN
THE
OUTPUT
OF
THIS
PROGRAM
IS
TEMPORARY
PERMANENT
TEMPORARY
PERMANENT
TEMPORARY
PERMANENT
TABLE
DESCRIBES
THE
DIFFERENT
WAYS
IN
WHICH
A
VARIABLE
CAN
BE
DECLARED
TABLE
DECLARATION
MODIFIERS
DECLARED
SCOPE
CLASS
INITIALIZED
OUTSIDE
ALL
BLOCKS
GLOBAL
PERMANENT
ONCE
STATIC
OUTSIDE
ALL
BLOCKS
GLOBAL
PERMANENT
ONCE
INSIDE
A
BLOCK
LOCAL
TEMPORARY
EACH
TIME
BLOCK
IS
ENTERED
STATIC
INSIDE
A
BLOCK
LOCAL
PERMANENT
ONCE
A
STATIC
DECLARATION
MADE
OUTSIDE
BLOCKS
INDICATES
THE
VARIABLE
IS
LOCAL
TO
THE
FILE
IN
WHICH
IT
IS
DECLARED
SEE
CHAPTER
FOR
MORE
INFORMATION
ON
PROGRAMMING
WITH
MULTIPLE
FILES
FUNCTIONS
FUNCTIONS
ALLOW
US
TO
GROUP
COMMONLY
USED
CODE
INTO
A
COMPACT
UNIT
THAT
CAN
BE
USED
REPEATEDLY
WE
HAVE
ALREADY
ENCOUNTERED
ONE
FUNCTION
MAIN
IT
IS
A
SPECIAL
FUNCTION
CALLED
AT
THE
BEGINNING
OF
THE
PROGRAM
ALL
OTHER
FUNCTIONS
ARE
DIRECTLY
OR
INDIRECTLY
CALLED
FROM
MAIN
SUPPOSE
WE
WANT
TO
WRITE
A
PROGRAM
TO
COMPUTE
THE
AREA
OF
THREE
TRIANGLES
WE
COULD
WRITE
OUT
THE
FORMULA
THREE
TIMES
OR
WE
COULD
CREATE
A
FUNCTION
TO
DO
THE
WORK
EACH
FUNCTION
SHOULD
BEGIN
WITH
A
COMMENT
BLOCK
CONTAINING
THE
FOLLOWING
NAME
NAME
OF
THE
FUNCTION
DESCRIPTION
DESCRIPTION
OF
WHAT
THE
FUNCTION
DOES
PARAMETERS
DESCRIPTION
OF
EACH
OF
THE
PARAMETERS
TO
THE
FUNCTION
RETURNS
DESCRIPTION
OF
THE
RETURN
VALUE
OF
THE
FUNCTION
ADDITIONAL
SECTIONS
MAY
BE
ADDED
SUCH
AS
FILE
FORMATS
REFERENCES
OR
NOTES
R
EFER
TO
CHAPTER
FOR
OTHER
SUGGESTIONS
OUR
FUNCTION
TO
COMPUTE
THE
AREA
OF
A
TRIANGLE
BEGINS
WITH
TRIANGLE
COMPUTES
AREA
OF
A
TRIANGLE
PARAMETERS
WIDTH
WIDTH
OF
THE
TRIANGLE
HEIGHT
HEIGHT
OF
THE
TRIANGLE
RETURNS
AREA
OF
THE
TRIANGLE
THE
FUNCTION
PROPER
BEGINS
WITH
THE
LINE
FLOAT
TRIANGLE
FLOAT
WIDTH
FLOAT
HEIGHT
FLOAT
IS
THE
FUNCTION
TYPE
THE
TWO
PARAMETERS
ARE
WIDTH
AND
HEIGHT
THEY
ARE
OF
TYPE
FLOAT
ALSO
C
USES
A
FORM
OF
PARAMETER
PASSING
CALLED
CALL
BY
VALUE
WHEN
OUR
PROCEDURE
TRIANGLE
IS
CALLED
WITH
CODE
SUCH
AS
TRIANGLE
C
COPIES
THE
VALUE
OF
THE
PARAMETERS
IN
THIS
CA
SE
AND
INTO
THE
FUNCTION
PARAMETERS
WIDTH
AND
HEIGHT
AND
THEN
STARTS
EXECUTING
THE
FUNCTION
CODE
WITH
THIS
FORM
OF
PARAMETER
PASSING
A
FUNCTION
CANNOT
PASS
DATA
BACK
TO
THE
CALLER
USING
PARAMETERS
THIS
STATEMENT
IS
NOT
STRICTLY
TRUE
WE
CAN
TRICK
C
INTO
PASSING
INFORMATION
BACK
THROUGH
THE
USE
OF
POINTERS
AS
WE
LL
SEE
IN
CHAPTER
THE
FUNCTION
COMPUTES
THE
AREA
WITH
THE
STATEMENT
AREA
WIDTH
HEIGHT
WHAT
LEFT
IS
TO
GIVE
THE
RESULT
TO
THE
CALLER
THIS
STEP
IS
DONE
WITH
THE
RETURN
STATEMENT
RETURN
AREA
EXAMPLE
SHOWS
OUR
FULL
TRIANGLE
FUNCTION
EXAMPLE
TRI
SUB
TRI
SUB
C
INCLUDE
STDIO
H
TRIANGLE
COMPUTES
AREA
OF
A
TRIANGLE
PARAMETERS
WIDTH
WIDTH
OF
THE
TRIANGLE
HEIGHT
HEIGHT
OF
THE
TRIANGLE
RETURNS
AREA
OF
THE
TRIANGLE
FLOAT
TRIANGLE
FLOAT
WIDTH
FLOAT
HEIGHT
FLOAT
AREA
AREA
OF
THE
TRIANGLE
AREA
WIDTH
HEIGHT
RETURN
AREA
THE
LINE
SIZE
TRIANGLE
IS
A
CALL
TO
THE
FUNCTIONTRIANGLE
C
ASSIGNS
TO
THE
PARAMETER
WIDTH
AND
TO
HEIGHT
IF
FUNCTIONS
ARE
THE
ROOMS
OF
OUR
BUILDING
THEN
PARAMETERS
ARE
THE
DOORS
BETWEEN
THE
ROOMS
IN
THIS
CASE
THE
VALUE
IS
GOING
THROUGH
THE
DOOR
MARKED
WIDTH
PARAMETERS
DOORS
ARE
ONE
WAY
THINGS
CAN
GO
IN
BUT
THEY
CAN
T
GO
OUT
THE
RETURN
STATEMENT
IS
HOW
WE
GET
DATA
OUT
OF
THE
FUNCTION
IN
OUR
TRIANGLE
EXAMPLE
THE
FUNCTION
ASSIGNS
THE
LOCAL
VARIABLE
AREA
THE
THEN
EXECUTES
THE
STATEMENT
RETURN
AREA
THE
RETURN
VALUE
OF
THIS
FUNCTION
IS
SO
OUR
STATEMENT
SIZE
TRIANGLE
ASSIGNS
SIZE
THE
VALUE
EXAMPLE
COMPUTES
THE
AREA
OF
THREE
TRIANGLES
EXAMPLE
TRI
PROG
TRI
PROG
C
FILE
TRI
SUB
TRI
PROG
C
INCLUDE
STDIO
H
TRIANGLE
COMPUTES
AREA
OF
A
TRIANGLE
PARAMETERS
WIDTH
WIDTH
OF
THE
TRIANGLE
HEIGHT
HEIGHT
OF
THE
TRIANGLE
RETURNS
AREA
OF
THE
TRIANGLE
FLOAT
TRIANGLE
FLOAT
WIDTH
FLOAT
HEIGH
T
FLOAT
AREA
AREA
OF
THE
TRIANGLE
AREA
WIDTH
HEIGHT
RETURN
AREA
INT
MAIN
PRINTF
TRIANGLE
F
N
TRIANGLE
PRINTF
TRIANGLE
F
N
TRIANGLE
PRINTF
TRIANGLE
F
N
TRIANGLE
RETURN
IF
WE
WANT
TO
USE
A
FUNCTION
BEFORE
WE
DEFINE
IT
WE
MUST
DECLARE
IT
JUST
LIKE
A
VARIABLE
TO
INFORM
THE
COMPILER
ABOUT
THE
FUNCTION
WE
USE
THE
DECLARATION
COMPUTE
A
TRIANGLE
FLOAT
TRIANGLE
FLOAT
WIDTH
FLOAT
HEIGHT
FOR
THE
TRIANGLE
FUNCTION
THIS
DECLARATION
IS
CALLED
THE
FUNCTION
PROTOTYPE
THE
VARIABLE
NAMES
ARE
NOT
REQUIRED
WHEN
DECLARING
A
FUNCTION
PROTOTYPE
OUR
PROTOTYPE
COULD
HAVE
JUST
AS
EASILY
BEEN
WRITTEN
AS
FLOAT
TRIANGLE
FLOAT
FLOAT
HOWEVER
WE
USE
THE
LONGER
VERSION
BECAUSE
IT
GIVES
THE
PROGRAMMER
ADDITIONAL
INFORMATION
AND
IT
EASY
TO
CREATE
PROTOTYPES
USING
THE
EDITOR
CUT
AND
PASTE
FUNCTIONS
STRICTLY
SPEAKING
THE
PROTOTYPES
ARE
OPTIONAL
FOR
SOME
FUNCTIONS
IF
NO
PROTOTYPE
IS
SPECIFIE
D
THE
C
COMPILER
ASSUMES
THE
FUNCTION
RETURNS
AN
INT
AND
TAKES
ANY
NUMBER
OF
PARAMETERS
OMITTING
A
PROTOTYPE
ROBS
THE
C
COMPILER
OF
VALUABLE
INFORMATION
THAT
IT
CAN
USE
TO
CHECK
FUNCTION
CALLS
MOST
COMPILERS
HAVE
A
COMPILE
TIME
SWITCH
THAT
WARNS
THE
PROGRAMMER
ABOUT
FUNCTION
CALLS
WITHOUT
PROTOTYPES
FUNCTIONS
WITH
NO
PARAMETERS
A
FUNCTION
CAN
HAVE
ANY
NUMBER
OF
PARAMETERS
INCLUDING
NONE
BUT
EVEN
WHEN
USING
A
FUNCTION
WITH
NO
PARAMETERS
YOU
STILL
NEED
THE
PARENTHESES
VALUE
DECLARING
A
PROTOTYPE
FOR
A
FUNCTION
WITHOUT
PARAMETERS
IS
A
LITTLE
TRICKY
YOU
CAN
T
USE
THE
STATEMENT
INT
BECAUSE
THE
C
COMPILER
WILL
SEE
THE
EMPTY
PARENTHESES
AND
ASSUME
THAT
THIS
IS
A
K
R
STYLE
FUNCTION
DECLARATION
SEE
CHAPTER
FOR
DETAILS
ON
THIS
OLDER
STYLE
THE
KEYWORD
VOID
IS
USED
TO
INDICATE
AN
EMPTY
PARAMETER
LIST
SO
THE
PROTOTYPE
FOR
OUR
FUNCTION
IS
INT
VOID
VOID
IS
ALSO
USED
TO
INDICATE
THAT
A
FUNCTION
DOES
NOT
RETURN
A
VALUE
VOID
IS
SIMILAR
TO
THE
FORTRAN
SUBROUTINE
OR
PASCAL
PROCEDURE
FOR
EXAMPLE
THIS
FUNCTION
JUST
PRINTS
A
RESULT
IT
DOES
NOT
RETURN
A
VALUE
VOID
INT
ANSWER
IF
ANSWER
PRINTF
ANSWER
CORRUPT
N
RETURN
PRINTF
THE
ANSWER
IS
D
N
ANSWER
QUESTION
EXAMPLE
SHOULD
COMPUTE
THE
LENGTH
OF
A
STRING
INSTEAD
IT
INSISTS
THAT
ALL
STRINGS
ARE
OF
LENGTH
WHY
CLICK
HERE
FOR
THE
ANSWER
SECTION
THIS
FUNCTION
PERFORMS
THE
SAME
FUNCTION
AS
THE
LIBRARY
FUNCTION
STRLEN
EXAMPLE
LEN
LEN
C
QUESTION
WHY
DOES
THIS
PROGRAM
ALWAYS
REPORT
THE
LENGTH
OF
ANY
STRING
AS
A
SAMPLE
MAIN
HAS
BEEN
PROVIDED
IT
WILL
ASK
FOR
A
STRING
AND
THEN
PRINT
THE
LENGTH
INCLUDE
STDIO
H
LENGTH
COMPUTES
THE
LENGTH
OF
A
STRING
PARAMETERS
STRING
THE
STRING
WHOSE
LENGTH
WE
WANT
RETURNS
THE
LENGTH
OF
THE
STRING
INT
LENGTH
CHAR
STRING
INT
INDEX
INDEX
INTO
THE
STRING
LOOP
UNTIL
WE
REACH
THE
END
OF
STRING
CHARACTER
FOR
INDEX
STRING
INDEX
INDEX
DO
NOTHING
RETURN
INDEX
INT
MAIN
CHAR
LINE
INPUT
LINE
FROM
USER
WHILE
PRINTF
ENTER
LINE
FGETS
LINE
SIZEOF
LINE
STDIN
PRINTF
LENGTH
INCLUDING
NEWLINE
IS
D
N
LENGTH
LINE
STRUCTURED
PROGRAMMING
COMPUTER
SCIENTISTS
SPEND
A
GREAT
DEAL
OF
TIME
AND
EFFORT
STUDYING
HOW
TO
PROGRAM
THE
RESULT
IS
THAT
THEY
COME
UP
WITH
ABSOLUTELY
POSITIVELY
THE
BEST
PROGRAMMING
METHODOLOGY
A
NEW
ONE
EACH
MONTH
SOME
OF
THESE
SYSTEMS
INCLUDE
FLOW
CHARTS
TOP
DOWN
PROGRAMMING
BOTTOM
UP
PROGRAMMING
STRUCTURED
PROGRAMMING
AND
OBJECT
ORIENTED
DESIGN
OOD
NOW
THAT
WE
HAVE
LEARNED
ABOUT
FUNCTIONS
WE
CAN
TALK
ABOUT
U
SING
STRUCTURED
PROGRAMMING
TECHNIQUES
TO
DESIGN
PROGRAMS
THESE
TECHNIQUES
ARE
WAYS
OF
DIVIDING
UP
OR
STRUCTURING
A
PROGRAM
INTO
SMALL
WELL
DEFINED
FUNCTIONS
THEY
MAKE
THE
PROGRAM
EASY
TO
WRITE
AND
EASY
TO
UNDERSTAND
I
DON
T
CLAIM
THAT
THIS
METHOD
IS
THE
ABSOLUTE
BEST
WAY
TO
PROGRAM
IT
HAPPENS
TO
BE
THE
METHOD
THAT
WORKS
BEST
FOR
ME
IF
ANOTHER
SYSTEM
WORKS
BETTER
FOR
YOU
USE
IT
THE
FIRST
STEP
IN
PROGRAMMING
IS
TO
DECIDE
WHAT
YOU
ARE
GOING
TO
DO
THIS
HAS
ALREADY
BEEN
DESCRIBED
IN
CHAPTER
NEXT
DECIDE
HOW
YOU
ARE
GOING
TO
STRUCTURE
YOUR
DATA
FINALLY
THE
CODING
PHASE
BEGINS
WHEN
WRITING
A
PAPER
YOU
START
WITH
AN
OUTLINE
OF
EACH
SECTION
IN
THE
PAPER
DESCRIBED
BY
A
SINGLE
ENTENCE
THE
DETAILS
WILL
BE
FILLED
IN
LATER
WRITING
A
PROGRAM
IS
A
SIMILAR
PROCESS
YOU
START
WITH
AN
OUTLINE
AND
THIS
BECOMES
YOUR
MAIN
FUNCTION
THE
DETAILS
CAN
BE
HIDDEN
WITHIN
OTHER
FUNCTIONS
FOR
EXAMPLE
EXAMPLE
SOLVES
ALL
THE
WORLD
PROBLEMS
EXAMPLE
SOLVE
THE
WORLD
PROBLEMS
INT
MAIN
INIT
RETURN
OF
COURSE
SOME
OF
THE
DETAILS
WILL
HAVE
TO
BE
FILLED
IN
LATER
START
BY
WRITING
THE
MAIN
FUNCTION
IT
SHOULD
BE
LESS
THAN
THREE
PAGES
LONG
IF
IT
GROWS
LONGER
CONSIDER
SPLITTING
IT
UP
INTO
TWO
SMALLER
SIMPLER
FUNCTIONS
AFTER
THE
MAIN
FUNCTION
IS
COMPLETE
YOU
CAN
START
ON
THE
OTHERS
THIS
TYPE
O
F
STRUCTURED
PROGRAMMING
IS
CALLED
TOP
DOWN
PROGRAMMING
YOU
START
AT
THE
TOP
MAIN
AND
WORK
YOUR
WAY
DOWN
ANOTHER
TYPE
OF
CODING
IS
CALLED
BOTTOM
UP
PROGRAMMING
THIS
METHOD
INVOLVES
WRITING
THE
LOWEST
LEVEL
FUNCTION
FIRST
TESTING
IT
AND
THEN
BUILDING
ON
THAT
WORKING
SET
I
TEND
TO
USE
SOME
BOTTOM
UP
TECHNIQUES
WHEN
I
M
WORKING
WITH
A
NEW
STANDARD
FUNCTION
THAT
I
HAVEN
T
USED
BEFORE
I
WRITE
A
SMALL
FUNCTION
TO
MAKE
SURE
THAT
I
REALLY
KNOW
HOW
THE
FUNCTION
WORKS
AND
THEN
CONTINUE
FROM
THERE
THIS
APPROACH
IS
USED
IN
CHAPTER
TO
CONSTRUCT
THE
CALCULATOR
PROGRAM
SO
IN
ACTUAL
PRACTICE
BOTH
TECHNIQUES
ARE
USEFUL
A
MOSTLY
TOP
DOWN
PARTIALLY
BOTTOM
UP
TECHNIQUE
RESULTS
COMPUTER
SCIENTISTS
HAVE
A
TERM
FOR
THIS
METHODOLOGY
CHAOS
THE
ONE
RULE
YOU
SHOULD
FOLLOW
IN
PROGRAMMING
IS
USE
WHAT
WORKS
BEST
RECURSION
RECURSION
OCCURS
WHEN
A
FUNCTION
CALLS
ITSELF
DIRECTLY
OR
INDIRECTLY
SOME
PROGRAMMING
FUNCTIONS
SUCH
AS
THE
FACTORIAL
LEND
THEMSELVES
NATURALLY
TO
RECURSIVE
ALGORITHMS
A
RECURSIVE
FUNCTION
MUST
FOLLOW
TWO
BASIC
RULES
IT
MUST
HAVE
AN
ENDING
POINT
IT
MUST
MAKE
THE
PROBLEM
SIMPLER
A
DEFINITION
OF
FACTORIAL
IS
FACT
FACT
N
N
FACT
N
IN
C
THIS
DEFINITION
IS
INT
FACT
INT
NUMBER
IF
NUMBER
RETURN
ELSE
RETURN
NUMBER
FACT
NUMBER
THIS
DEFINITION
SATISFIES
OUR
TWO
RULES
FIRST
IT
HAS
A
DEFINITE
ENDING
POINT
WHEN
NUMBER
SECOND
IT
SIMPLIFIES
THE
PROBLEM
BECAUSE
THE
CALCULATION
OF
FACT
NUMBER
IS
SIMPLER
THAN
FACT
NUMBER
FACTORIAL
IS
LEGAL
ONLY
FOR
NUMBER
BUT
WHAT
HAPPENS
IF
WE
TRY
TO
COMPUTE
FACT
THE
PROGRAM
WILL
ABORT
WITH
A
STACK
OVERFLOW
OR
SIMILAR
MESSAGE
FACT
CALLS
FACT
WHICH
CALLS
FACT
ETC
NO
ENDING
POINT
EXISTS
THIS
ERROR
IS
REFERRED
TO
AS
AN
INFINITE
RECURSION
ERROR
MANY
THINGS
THAT
WE
DO
ITERATIVELY
CAN
BE
DONE
RECURSIVELY
FOR
EXAMPLE
SUMMING
UP
THE
ELEMENTS
OF
AN
ARRAY
WE
DEFINE
A
FUNCTION
TO
ADD
ELEME
NTS
M
N
OF
AN
ARRAY
AS
FOLLOWS
IF
WE
HAVE
ONLY
ONE
ELEMENT
THEN
THE
SUM
IS
SIMPLE
OTHERWISE
WE
USE
THE
SUM
OF
THE
FIRST
ELEMENT
AND
THE
SUM
OF
THE
REST
IN
C
THIS
FUNCTION
IS
INT
SUM
INT
FIRST
INT
LAST
INT
ARRAY
IF
FIRST
LAST
RETURN
ARRAY
FIRST
ELSE
RETURN
ARRAY
FIRST
SUM
FIRST
LAST
ARRAY
FOR
EXAMPLE
SUM
SUM
SUM
SUM
ANSWER
ANSWERS
ANSWER
THE
PROGRAMMER
WENT
TO
A
LOT
OF
TROUBLE
TO
EXPLAIN
THAT
THE
FOR
LOOP
DID
NOTHING
EXCEPT
INCREMENT
THE
INDEX
HOWEVER
THERE
IS
NO
SEMICOLON
AT
THE
END
OF
THE
FO
R
C
KEEPS
ON
READING
UNTIL
IT
SEES
A
STATEMENT
IN
THIS
CASE
RETURN
INDEX
AND
THEN
PUTS
THAT
STATEMENT
IN
THE
FOR
LOOP
PROPERLY
DONE
THIS
PROGRAM
SHOULD
LOOK
LIKE
EXAMPLE
EXAMPLE
C
INCLUDE
STDIO
H
INT
LENGTH
CHAR
STRING
INT
INDEX
INDEX
INTO
THE
STRING
LOOP
UNTIL
WE
REACH
THE
END
OF
STRING
CHARACTER
FOR
INDEX
STRING
INDEX
INDEX
CONTINUE
DO
NOTHING
RETURN
INDEX
INT
MAIN
CHAR
LINE
INPUT
LINE
FROM
USER
WHILE
PRINTF
ENTER
LINE
FGETS
LINE
SIZEOF
LINE
STDIN
PRINTF
LENGTH
INCLUDING
NEWLINE
IS
D
N
LENGTH
LINE
PROGRAMMING
EXERCISES
EXERCISE
WRITE
A
PROCEDURE
THAT
COUNTS
THE
NUMBER
OF
WORDS
IN
A
STRING
YOUR
DOCUMENTATION
SHOULD
DESCRIBE
EXACTLY
HOW
YOU
DEFINE
A
WORD
WRITE
A
PROGRAM
TO
TEST
YOUR
NEW
PROCEDURE
EXERCISE
WRITE
A
FUNCTION
BEGINS
THAT
RETURNS
TRUE
IF
BEGINS
WRITE
A
PROGRAM
TO
TEST
THE
FUNCTION
EXERCISE
WRITE
A
FUNCTION
COUNT
NUMBER
ARRAY
LENGTH
THAT
COUNTS
THE
NUMBER
OF
TIMES
NUMBER
APPEARS
IN
ARRAY
THE
ARRAY
HAS
LENGTH
E
LEMENTS
THE
FUNCTION
SHOULD
BE
RECURSIVE
WRITE
A
TEST
PROGRAM
TO
GO
WITH
THE
FUNCTION
EXERCISE
WRITE
A
FUNCTION
THAT
TAKES
A
CHARACTER
ARRAY
AND
RETURNS
A
PRIMITIVE
HASH
CODE
BY
ADDING
UP
THE
VALUE
OF
EACH
CHARACTER
IN
THE
ARRAY
EXERCISE
WRITE
A
FUNCTION
THAT
RETURNS
THE
MAXIMUM
VALUE
OF
AN
ARRAY
OF
NUMBERS
EXERCISE
WRITE
A
FUNCTION
THAT
SCANS
A
CHARACTER
ARRAY
FOR
THE
CHARACTER
AND
REPLACES
IT
WITH
CHAPTER
C
PREPROCESSOR
THE
SPEECH
OF
MAN
IS
LIKE
EMBROIDERED
TAPESTRIES
SINCE
LIKE
THEM
THIS
HAS
TO
BE
EXTENDED
IN
ORDER
TO
DISPLAY
ITS
PATTERNS
BUT
WHEN
IT
IS
ROLLED
UP
IT
CONCEALS
AND
DISTORTS
THEM
THEMISTOCLES
IN
THE
EARLY
DAYS
WHEN
C
WAS
STILL
BEING
DEVELOPED
IT
SOON
BECAME
APPARENT
THAT
C
NEEDED
A
FACILITY
FOR
HANDLING
NAMED
CONSTANTS
MACROS
AND
INCLUDE
FILES
THE
SOLUTION
WAS
TO
CREATE
A
PREPROCESSOR
THAT
RECOGNIZED
THESE
CONSTRUCTS
IN
THE
PROGRAMS
BEFORE
THEY
WERE
PASSED
TO
THE
C
COMPILER
THE
PREPROCESSOR
IS
NOTHING
MORE
THAN
A
SPECIALIZED
TEXT
EDITOR
ITS
SYNTAX
IS
CO
MPLETELY
DIFFERENT
FROM
THAT
OF
C
AND
IT
HAS
NO
UNDERSTANDING
OF
C
CONSTRUCTS
THE
PREPROCESSOR
WAS
VERY
USEFUL
AND
SOON
IT
WAS
MERGED
INTO
THE
MAIN
C
COMPILER
ON
SOME
SYSTEMS
LIKE
UNIX
THE
PREPROCESSOR
IS
STILL
A
SEPARATE
PROGRAM
AUTOMATICALLY
EXECU
TED
BY
THE
COMPILER
WRAPPER
CC
SOME
OF
THE
NEW
COMPILERS
LIKE
TURBO
C
AND
MICROSOFT
VISUAL
C
HAVE
THE
PREPROCESSOR
BUILT
IN
DEFINE
STATEMENT
EXAMPLE
INIT
IALIZES
TWO
ARRAYS
DATA
AND
TWICE
EACH
ARRAY
CONTAINS
ELEMENTS
SUPPOSE
WE
WANTED
TO
CHANGE
THE
PROGRAM
TO
USE
ELEMENTS
THEN
WE
WOULD
HAVE
TO
CHANGE
THE
ARRAY
SIZE
TWO
PLACES
AND
THE
INDEX
LIMIT
ONE
PLACE
ASIDE
FROM
BEING
A
LOT
OF
WORK
MULTIPLE
CHANGES
CAN
LEAD
TO
ERRORS
EXAMPLE
C
INT
DATA
SOME
DATA
INT
TWICE
TWICE
SOME
DATA
INT
MAIN
INT
INDEX
INDEX
INTO
THE
DATA
FOR
INDEX
INDEX
INDEX
DATA
INDEX
INDEX
TWICE
INDEX
INDEX
RETURN
WE
WOULD
LIKE
TO
BE
ABLE
TO
WRITE
A
GENERIC
PROGRAM
IN
WHICH
WE
CAN
DEFINE
A
CONSTANT
FOR
THE
SIZE
OF
THE
ARRAY
AND
THEN
LET
C
ADJUST
THE
DIMENSIONS
OF
OUR
TWO
ARRAYS
BY
USING
THE
DEFI
NE
STATEMENT
WE
CAN
DO
JUST
THAT
EXAMPLE
IS
A
NEW
VERSION
OF
EXAMPLE
EXAMPLE
C
DEFINE
SIZE
WORK
ON
ELEMENTS
INT
DATA
SIZE
SOME
DATA
INT
TWICE
SIZE
TWICE
SOME
DATA
INT
MAIN
INT
INDEX
INDEX
INTO
THE
DATA
FOR
INDEX
INDEX
SIZE
INDEX
DATA
INDEX
INDEX
TWICE
INDEX
INDEX
RETURN
THE
LINE
DEFINE
SIZE
ACTS
AS
A
COMMAND
TO
A
SPECIAL
TEXT
EDITOR
TO
GLOBALLY
CHANGE
SIZE
TO
THIS
LINE
TAKES
THE
DRUDGERY
AND
GUESSWORK
OUT
OF
MAKING
CHANGES
ALL
PREPROCESSOR
COMMANDS
BEGIN
WITH
A
HASH
MARK
IN
COLUMN
ONE
ALTHOUGH
C
IS
FREE
FORMAT
THE
PREPROCESSOR
IS
NOT
AND
IT
DEPENDS
ON
THE
HASH
MARK
BEING
IN
THE
FIRST
COLUMN
AS
WE
WILL
SEE
THE
PREPROCESSOR
KNOWS
NOTHING
ABOUT
C
IT
CAN
BE
AND
IS
USED
TO
EDIT
THINGS
OTHER
THAN
C
PROGRAMS
A
PREPROCESSOR
DIRECTIVE
TERMINATES
AT
THE
END
O
F
LINE
THIS
FORMAT
IS
DIFFERENT
FROM
THAT
OF
C
WHERE
A
SEMICOLON
IS
USED
TO
END
A
STATEMENT
PUTTING
A
SEMICOLON
AT
THE
END
OF
A
PREPROCESSOR
DIRECTIVE
CAN
LEAD
TO
UNEXPECTED
RESULTS
A
LINE
MAY
BE
CONTINUED
BY
PUTTING
A
BACKSLASH
AT
THE
END
THE
SIMPLEST
USE
OF
THE
PREPROCESSOR
IS
TO
DEFINE
A
REPLACEMENT
MACRO
FOR
EXAMPLE
THE
COMMAND
DEFINE
FOO
BAR
CAUSES
THE
PREPROCESSOR
TO
RE
PLACE
THE
WORD
FOO
WITH
THE
WORD
BAR
EVERYWHERE
FOO
OCCURS
IT
IS
COMMON
PROGRAMMING
PRACTICE
TO
USE
ALL
UPPERCASE
LETTERS
FOR
MACRO
NAMES
THIS
PRACTICE
MAKES
TELLING
THE
DIFFERENCE
BETWEEN
A
VARIABLE
ALL
LOWERCASE
AND
A
MACRO
ALL
UPPERCASE
VERY
EASY
THE
GENERAL
FORM
OF
A
SIMPLE
DEFINE
STATEMENT
IS
DEFINE
NAME
SUBSTITUTE
TEXT
WHERE
NAME
CAN
BE
ANY
VALID
C
IDENTIFIER
AND
SUBSTITUTE
TEXT
CAN
BE
ANYTHING
YOU
COULD
USE
THE
FOLLOWING
DEFINITION
DEFINE
FOR
I
I
I
AND
USE
IT
LIKE
THIS
CLEAR
THE
ARRAY
DATA
I
HOWEVER
DEFINING
MACROS
IN
THIS
MANNER
IS
CONSIDERED
BAD
PROGRAMMING
PRACTICE
SUCH
DEFINITIONS
TEND
TO
OBSCURE
THE
BASIC
CONTROL
FLOW
OF
THE
PROGRAM
IN
THIS
EXAMPLE
A
PROGRAMMER
WHO
WANTS
TO
KNOW
WHAT
THE
LOOP
DOES
WOULD
HAVE
TO
SEARCH
THE
BEGINNING
OF
THE
PROGRAM
FOR
THE
DEFINITION
OF
AN
EVEN
WORSE
PRACTICE
IS
TO
DEFINE
MACROS
THAT
DO
LARGE
SCALE
REPLACEMENT
OF
BASIC
C
PROGRAMMING
CONSTRUCTS
FOR
EXAMPLE
YOU
CAN
DEFINE
THE
FOLLOWING
DEFINE
BEGIN
DEFINE
END
IF
INDEX
BEGIN
PRINTF
STARTING
N
END
THE
PROBLEM
IS
THAT
YOU
ARE
NO
LONGER
PROGRAMMING
IN
C
BUT
IN
A
HALF
C
HALF
PASCAL
MONGREL
YOU
CAN
FIND
THE
EXTREMES
TO
WHICH
SUCH
MIMICRY
CAN
BE
TAKEN
IN
THE
BOURNE
SHELL
WHICH
USES
PREPROCESSOR
DIRECTIVES
TO
DEFINE
A
LANGUAGE
THAT
LOOKS
A
LOT
LIKE
ALGOL
HERE
A
SAMPLE
SECTION
OF
CODE
IF
X
OR
Y
THEN
CASE
VALUE
OF
SELECT
START
SELECT
BACKSPACE
OTHERWISE
ERROR
ESAC
FI
MOST
PROGRAMMERS
ENCOUNTERING
THIS
PROGRAM
CURSE
AT
FIRST
AND
THEN
USE
THE
EDITOR
TO
TURN
THE
SOURCE
BACK
INTO
A
REASONABLE
VERSION
OF
C
THE
PREPROCESSOR
CAN
CAUSE
UNEXPECTED
PROBLEMS
BECAUSE
IT
DOES
NOT
CHECK
FOR
CORRECT
C
SYNTAX
FOR
EXAMPLE
EXAMPLE
GENERATES
AN
ERROR
ON
LINE
EXAMPLE
BIG
BIG
C
DEFINE
MAIN
INDEX
FOR
OUR
CALCULATIONS
INT
INDEX
INDEX
SYNTAX
ERROR
ON
NEXT
LINE
WHILE
INDEX
INDEX
INDEX
RETURN
THE
PROBLEM
IS
IN
THE
DEFINE
STATEMENT
ON
LINE
BUT
THE
ERROR
MESSAGE
POINTS
TO
LINE
THE
DEFINITION
IN
LINE
CAUSES
THE
PREPROCESSOR
TO
EXPAND
LINE
TO
LOOK
LIKE
WHILE
INDEX
BECAUSE
IS
AN
ILLEGAL
OPERATOR
THIS
EXPANSION
GENERATES
A
SYNTAX
ERROR
QUESTION
EXAMPLE
GENERATES
THE
ANSWER
INSTEAD
OF
THE
EXPECTED
ANSWER
WHY
SEE
THE
HINT
BELOW
EXAMPLE
FIRST
FIRST
C
INCLUDE
STDIO
H
DEFINE
DEFINE
DEFINE
INT
MAIN
PRINTF
THE
SQUARE
OF
ALL
THE
PARTS
IS
D
N
RETURN
HINT
THE
ANSWER
MAY
NOT
BE
READILY
APPARENT
LUCKILY
C
ALLOWS
YOU
TO
RUN
YOUR
PROGRAM
THROUGH
THE
PREPROCESSOR
AND
VIEW
THE
OUTPUT
IN
UNIX
THE
COMMAND
CC
E
PROG
C
WILL
SEND
THE
OUTPUT
OF
THE
PREPROCESSOR
TO
THE
STANDARD
OUTPUT
IN
MS
DOS
WINDOWS
THE
COMMAND
C
CPP
PROG
C
WILL
DO
THE
SAME
THING
RUNNING
THIS
PROGRAM
THROUGH
THE
PREPROCESSOR
GIVES
US
FIRST
C
USR
INCLUDE
STDIO
H
LISTING
OF
DATA
IN
INCLUDE
FILE
STDIO
H
FIRST
C
MAIN
PRINTF
THE
SQUARE
OF
ALL
THE
PARTS
IS
D
N
RETURN
CLICK
HERE
FOR
THE
ANSWER
SECTION
QUESTION
EXAMPLE
GENERATES
A
WARNING
THAT
COUNTER
IS
USED
BEFORE
IT
IS
SET
THIS
WARNING
IS
A
SURPRISE
TO
US
BECAUSE
THE
FOR
LOOP
SHOULD
SET
IT
WE
ALSO
GET
A
VERY
STRANGE
WARNING
NULL
EFFECT
FOR
LIN
E
EXAMPLE
MAX
MAX
C
WARNING
SPACING
IS
VERY
IMPORTANT
INCLUDE
STDIO
H
DEFINE
MAX
INT
MAIN
INT
COUNTER
FOR
COUNTER
MAX
COUNTER
COUNTER
PRINTF
HI
THERE
N
RETURN
HINT
TAKE
A
LOOK
AT
THE
PREPROCESSOR
OUTPUT
CLICK
HERE
FOR
THE
ANSWER
SECTION
QUESTION
EXAMPLE
COMPUTES
THE
WRONG
VALUE
FOR
SIZE
WHY
CLICK
HERE
FOR
THE
ANSWER
SECTION
EXAMPLE
SIZE
SIZE
C
INCLUDE
STDIO
H
DEFINE
SIZE
DEFINE
FUDGE
SIZE
INT
MAIN
INT
SIZE
SIZE
TO
REALLY
USE
SIZE
FUDGE
PRINTF
SIZE
IS
D
N
SIZE
RETURN
QUESTION
EXAMPLE
IS
SUPPOSED
TO
PRINT
THE
MESSAGE
FATAL
ERROR
ABORT
AND
EXIT
WHEN
IT
RECEIVES
BAD
DATA
BUT
WHEN
IT
GETS
GOOD
DATA
IT
EXITS
WHY
CLICK
HERE
FOR
THE
ANSWER
SE
CTION
EXAMPLE
DIE
DIE
C
INCLUDE
STDIO
H
INCLUDE
STDLIB
H
DEFINE
DIE
FPRINTF
STDERR
FATAL
ERROR
ABORT
N
EXIT
INT
MAIN
A
RANDOM
VALUE
FOR
TESTING
INT
VALUE
VALUE
IF
VALUE
DIE
PRINTF
WE
DID
NOT
DIE
N
RETURN
DEFINE
VS
CONST
THE
CONST
KEYWORD
IS
RELATIVELY
NEW
BEFORECONST
DEFINE
WAS
THE
ONLY
KEYWORD
AVAILABLE
TO
DEFINE
CONSTANTS
SO
MOST
OLDER
CODE
USES
DEFINE
DIRECTIVES
HOWEVER
THE
USEOF
CONST
IS
PREFERRED
OVER
DEFINE
FOR
SEVERAL
REASONS
FIRST
OF
ALL
C
CHECKS
THE
SYNTAX
OF
CONST
STATEMENTS
IMMEDIATELY
THE
DEFINE
DIRECTIVE
IS
NOT
CHECKED
UNTIL
THE
MACRO
IS
USED
ALSO
CONST
USE
C
SYNTAX
WHILE
THE
DEFINE
HAS
A
SYNTAX
ALL
ITS
OWN
FINALLY
CONST
FOLLOWS
NORMAL
C
SCOPE
RULES
WHILE
CONSTANTSDEFINED
BY
A
DEFINE
DIRECTIVE
CONTINUE
ON
FOREVER
SO
IN
MOST
CASES
ACONST
STATEMENT
IS
PREFERRED
OVER
DEFINE
HERE
ARE
TWO
WAYS
OF
DEFINING
THE
SAME
CONSTANT
DEFINE
MAX
DEFINE
A
VALUE
USING
THE
PREPROCESSOR
THIS
DEFINITION
CAN
EASILY
CAUSE
PROBLEMS
CONST
INT
MAX
DEFINE
A
C
CONSTANT
INTEGER
SAFER
THE
DEFINE
DIRECTIVE
CAN
ONLY
DE
FINE
SIMPLE
CONSTANTS
THE
CONST
STATEMENT
CAN
DEFINE
ALMOST
ANY
TYPE
OF
C
CONSTANT
INCLUDING
THINGS
LIKE
STRUCTURE
CLASSES
FOR
EXAMPLE
STRUCT
BOX
INT
WIDTH
HEIGHT
DIMENSIONS
OF
THE
BOX
IN
PIXELS
CONST
BOX
SIZE
OF
A
PINK
BOX
TO
BE
USED
FOR
INPUT
THE
DEFINE
DIRECTIVE
IS
HOWEVER
ESSENTIAL
FOR
THINGS
LIKE
CONDITIONAL
COMPILATION
AND
OTHER
SPECIALIZED
USES
CONDITIONAL
COMPILATION
ONE
OF
THE
PROBLEMS
PROGRAMMERS
HAVE
IS
WRITING
CODE
THAT
CAN
WORK
ON
MANY
DIFFERENT
MACHINES
IN
THEORY
C
CODE
IS
PORTABLE
IN
ACTUAL
PRACTICE
DIFFERENT
OPERATING
SYSTEMS
HAVE
LITTLE
QUIRKS
THAT
MUST
BE
ACCOUNTED
FOR
FOR
EXAMPLE
THIS
BOOK
COVERS
BOTH
THE
MS
DOS
WINDOWS
COMPILER
AND
UNIX
C
ALTHOUGH
THEY
ARE
ALMOST
THE
SAME
THERE
ARE
DIFFERENCES
ESPECIALLY
WHEN
YOU
MUST
ACCESS
THE
MORE
ADVANCED
FEATURES
OF
THE
OPERATING
SYSTEM
ANOTHER
PORTABILITY
PROBLEM
IS
CAUSED
BY
THE
FACT
THAT
THE
STANDARD
LEAVES
SOME
OF
THE
FEATURES
OF
THE
LANGUAGE
UP
TO
THE
IMPLEMENTERS
FOR
EXAMPLE
THE
SIZE
OF
AN
INTEGER
IS
IMPLEMENTATION
DEPENDENT
THE
PREPROCESSOR
THROUGH
THE
USE
OF
CONDITIONAL
COMPILATION
ALLOWS
THE
PROGRAMMER
GREAT
FLEXIBILITY
IN
CHANGING
THE
CODE
GENERATED
SUPPOSE
WE
WANT
TO
PUT
DEBUGGING
CODE
IN
THE
PROGRAM
WHILE
WE
ARE
WORKING
ON
IT
AND
THEN
REMOVE
IT
IN
THE
PRODUCTION
VERSION
WE
COULD
DO
SO
BY
INCLUDING
THE
CODE
IN
A
IFDEF
ENDIF
SECTION
IFDEF
DEBUG
PRINTF
IN
VALUE
D
HASH
D
N
VALUE
HASH
ENDIF
DEBUG
IF
THE
BEGINNING
OF
THE
PROGRAM
CONTAINS
THE
DIRECTIVE
DEFINE
DEBUG
TURN
DEBUGG
ING
ON
THE
PRINTF
WILL
BE
INCLUDED
IF
THE
PROGRAM
CONTAINS
THE
DIRECTIVE
UNDEF
DEBUG
TURN
DEBUGGING
OFF
THE
PRINTF
WILL
BE
OMITTED
STRICTLY
SPEAKING
THE
UNDEF
DEBUG
IS
UNNECESSARY
IF
THERE
IS
NO
DEFINE
DEBUG
STATEMENT
THEN
DEBUG
IS
UNDEFINED
THE
UNDEF
DEBUG
STATEMENT
IS
USED
TO
INDICATE
EXPLICITLY
THAT
DEBUG
IS
USED
FOR
CONDITIONAL
COMPILATION
AND
IS
NOW
TURNED
OFF
THE
DIRECTIVE
IFNDEF
WILL
CAUSE
THE
CODE
TO
BE
COMPILED
IF
THE
SYMBOL
IS
NOT
DEFINED
IFNDEF
DEBUG
PRINTF
PRO
DUCTION
CODE
NO
DEBUGGING
ENABLED
N
ENDIF
DEBUG
THE
ELSE
DIRECTIVE
REVERSES
THE
SENSE
OF
THE
CONDITIONAL
FOR
EXAMPLE
IFDEF
DEBUG
PRINTF
TEST
VERSION
DEBUGGING
IS
ON
N
ELSE
DEBUG
PRINTF
PRODUCTION
VERSION
N
ENDIF
DEBUG
A
PROGRAMMER
MAY
WISH
TO
REMOVE
A
SECTION
OF
CODE
TEMPORARILY
ONE
COMMON
METHOD
IS
TO
COMMENT
OUT
THE
CODE
BY
ENCLOSING
IT
IN
THIS
METHOD
CAN
CAUSE
PROBLEMS
AS
SHOWN
BY
THE
FOLLOWING
EXAMPLE
COMMENT
OUT
THIS
SECTION
SEC
HANDLE
THE
END
OF
SECTION
STUFF
END
OF
COMMENTED
OUT
SECTION
THIS
CODE
GENERATES
A
SYNTAX
ERROR
FOR
THE
FIFTH
LINE
WHY
A
BETTER
METHOD
IS
TO
USE
THE
IFDEF
CONSTRUCT
TO
REMOVE
THE
CODE
IFDEF
UNDEF
HANDLE
THE
END
OF
SECTION
STUFF
ENDIF
UNDEF
OF
COURSE
THE
CODE
WILL
BE
INCLUDED
IF
ANYONE
DEFINES
THE
SYMBOL
UNDEF
HOWEVER
ANYONE
WHO
DOES
SO
SHOULD
BE
SHOT
THE
COMPILER
SWITCH
DSYMBOL
ALLOWS
SYMBOLS
TO
BE
DEFINED
ON
THE
COMMAND
LINE
FOR
EXAMPLE
THE
COMMAND
CC
DDEBUG
G
O
PROG
PROG
C
COMPILES
THE
PROGRAM
PROG
C
AND
INCLUDES
ALL
THE
CODE
IN
BETWEEN
IFDEF
DEBUG
AND
ENDIF
DEBUG
EVEN
THOUGH
THERE
IS
NO
DEFINE
DEBUG
IN
THE
PROGRAM
THE
GENERAL
FORM
OF
THE
OPTION
IS
DSYMBOL
OR
DSYMBOL
VALUE
FOR
EXAMPLE
THE
FOLLOWING
SETS
MAX
TO
CC
DMAX
O
PROG
PROG
C
NOTICE
THAT
THE
PROGRAMMER
CAN
OVERRIDE
THE
COMMAND
LINE
OPTIONS
WITH
DIRECTIVES
IN
THE
PROGRAM
FOR
EXAMPLE
T
HE
DIRECTIVE
UNDEF
DEBUG
WILL
RESULT
IN
DEBUG
BEING
UNDEFINED
WHETHER
OR
NOT
YOU
USE
DDEBUG
MOST
C
COMPILERS
AUTOMATICALLY
DEFINE
SOME
SYSTEM
DEPENDENT
SYMBOLS
FOR
EXAMPLE
TURBO
C
DEFINES
THE
SYMBOLS
AND
THE
ANSI
STANDARD
COMPILER
DEFINES
THE
SYMBOL
MOST
UNIX
COMPILERS
DEFINE
A
NAME
FOR
THE
SYSTEM
I
E
SUN
VAX
CELERITY
ETC
HOWEVER
THEY
ARE
RARELY
DOCUMENTED
THE
SYMBOL
IS
ALWAYS
DEFINED
FOR
ALL
UNIX
MACHINES
INCLUDE
FILES
THE
INCLUDE
DIRECTIVE
ALLOWS
THE
PROGRAM
TO
USE
SOURCE
CODE
FROM
ANOTHER
FILE
FOR
EXAMPLE
WE
HAVE
BEEN
USING
THE
DIRECTIVE
INCLUDE
STDIO
H
IN
OUR
PROGRAMS
THIS
DIRECTIVE
TELLS
THE
PREPROCESSOR
TO
TAKE
THE
FILE
STDIO
H
STANDARD
I
O
AND
INSERT
IT
IN
THE
PROGRA
M
FILES
THAT
ARE
INCLUDED
IN
OTHER
PROGRAMS
ARE
CALLED
HEADER
FILES
MOST
INCLUDE
DIRECTIVES
COME
AT
THE
HEAD
OF
THE
PROGRAM
THE
ANGLE
BRACKETS
INDICATE
THAT
THE
FILE
IS
A
STANDARD
HEADER
FILE
ON
UNIX
THESE
FILES
ARE
LOCATED
IN
USR
INCLUDE
ON
MS
DOS
WINDOWS
THEY
ARE
LOCATED
IN
WHATEVER
DIRECTORY
WAS
SPECIFIED
AT
THE
TIME
THE
COMPILER
WAS
INSTALLED
STANDARD
INCLUDE
FILES
DEFINE
DATA
STRUCTURES
AND
MACROS
USED
BY
LIBRARY
ROUTINES
FOR
EXAMPLE
PRINTF
IS
A
LIBRARY
ROUTINE
THAT
PRINTS
DATA
ON
TH
E
STANDARD
OUTPUT
THE
FILE
STRUCTURE
USED
BY
PRINTF
AND
ITS
RELATED
ROUTINES
IS
DEFINED
IN
STDIO
H
SOMETIMES
THE
PROGRAMMER
MAY
WANT
TO
WRITE
HER
OWN
SET
OF
INCLUDE
FILES
LOCAL
INCLUDE
FILES
ARE
PARTICULARLY
USEFUL
FOR
STORING
CONSTANTS
AND
DATA
STRUCTURES
WHEN
A
PROGRAM
SPANS
SEVERAL
FILES
THEY
ARE
ESPECIALLY
USEFUL
FOR
INFORMATION
PASSING
WHEN
A
TEAM
OF
PROGRAMMERS
IS
WORKING
ON
A
SINGLE
PROJECT
SEE
CHAPTER
LOCAL
INCLU
DE
FILES
MAY
BE
SPECIFIED
BY
USING
DOUBLE
QUOTES
AROUND
THE
FILE
NAME
FOR
EXAMPLE
INCLUDE
DEFS
H
THE
FILENAME
DEFS
H
CAN
BE
ANY
VALID
FILENAME
THIS
SPECIFICATION
CAN
BE
A
SIMPLE
FILE
DEFS
H
A
RELATIVE
PATH
DATA
H
OR
AN
ABSOLUTE
PATH
ROOT
INCLUDE
CONST
H
ON
MS
DOS
WINDOWS
YOU
SHOULD
USE
BACKSLASH
INSTEAD
OF
SLASH
AS
A
DIRECTORY
SEPARATOR
INCLUDE
FILES
MAY
BE
NESTED
AND
THIS
FEATURE
CAN
CAUSE
PROBLEMS
SUPPOSE
YOU
DEFINE
SEVERAL
USEFUL
CONSTANTS
IN
THE
FILE
CONST
H
IF
THE
FILES
DATA
H
AND
IO
H
BOTH
INCLUDE
CONST
H
AND
YOU
PUT
THE
FOLLOWING
IN
YOUR
PROGRAM
INCLUDE
DATA
H
INCLUDE
IO
H
YOU
WILL
GENERATE
ERRORS
BECAUSE
THE
PREPROCESSOR
WILL
SET
THE
DEFINITIONS
IN
CONST
H
TWICE
DEFINING
A
CONSTANT
TWICE
IS
NOT
A
FATAL
ERROR
HOWEVER
DEFINING
A
DATA
STRUCTURE
OR
UNION
TWICE
IS
A
FATAL
ERROR
AND
MUST
BE
AVOIDED
ONE
WAY
AROUND
THIS
PROBLEM
IS
TO
HAVE
CONST
H
CHECK
TO
SEE
IF
IT
HAS
ALREADY
BEEN
INCLUDED
AND
DOES
NOT
DEFINE
ANY
SYMBOL
THAT
HAS
ALREADY
BEEN
DEFINED
THE
DIRECTIVE
IFNDEF
SYMBOL
IS
TRUE
IF
THE
SYMBOL
IS
NOT
DEFINED
THE
DIRECTIVE
IS
THE
REVERSE
OF
IFDEF
LOOK
AT
THE
FOLLOWING
CODE
IFNDEF
DEFINE
CONSTANTS
DEFINE
ENDIF
WHEN
CONST
H
IS
INCLUDED
IT
DEFINES
THE
SYMBOL
IF
THAT
SYMBOL
IS
ALREADY
DEFINED
BECAUSE
THE
FILE
WAS
INCLUDED
EARLIER
THE
IFNDEF
CONDITIONAL
HIDES
ALL
DEFINES
SO
THEY
DON
T
CAUSE
TROUBLE
PARAMETERIZED
MACROS
SO
FAR
WE
HAVE
DISCUSSED
ONLY
SIMPLE
DEFINES
OR
MACROS
BUT
MACROS
CAN
TAKE
PARAMETERS
THE
FOLLOWING
MACRO
WILL
COMPUTE
THE
SQUARE
OF
A
NUMBER
DEFINE
SQR
X
X
X
SQUARE
A
NUMBER
WHEN
USED
THE
MACRO
WILL
REPLACE
X
BY
THE
TEXT
OF
THE
FOLLOWING
ARGUMENT
SQR
EXPANDS
TO
ALWAYS
PUT
PARENTHESE
AROUND
THE
PARAMETERS
OF
A
MACRO
EXAMPLE
ILLUSTRATES
THE
PROBLEMS
THAT
CAN
OCCUR
IF
THIS
RULE
IS
NOT
FOLLOWED
EXAMPLE
SQR
SQR
C
INCLUDE
STDIO
H
DEFINE
SQR
X
X
X
INT
MAIN
INT
COUNTER
COUNTER
FOR
LOOP
FOR
COUNTER
COUNTER
COUNTER
PRINTF
X
D
X
SQUARED
D
N
COUNTER
SQR
COUNTER
RETURN
QUESTION
WHAT
DOES
EXAMPLE
OUTPUT
TRY
RUNNING
IT
ON
YOUR
MACHINE
WHY
DID
IT
OUTPUT
WHAT
IT
DID
TRY
CHECKING
THE
OUTPUT
OF
THE
PREPROCESSOR
CLICK
HERE
FOR
THE
ANSWER
SECTION
THE
KEEP
IT
SIMPLE
SYSTEM
OF
PROGRAMMING
TELLS
US
TO
USE
THE
INCREMENT
AND
DECREMENT
OPERATORS
ONLY
ON
LINE
BY
THEMSELVES
WHEN
USED
IN
A
MACRO
PARAMETER
THEY
CAN
LEAD
TO
UN
EXPECTED
RESULTS
AS
ILLUSTRATED
BY
EXAMPLE
EXAMPLE
SQR
I
SQR
I
C
INCLUDE
STDIO
H
DEFINE
SQR
X
X
X
INT
MAIN
INT
COUNTER
COUNTER
FOR
LOOP
COUNTER
WHILE
COUNTER
PRINTF
X
D
SQUARE
D
N
COUNTER
SQR
COUNTER
RETURN
QUESTION
WHY
WILL
EXAMPLE
NOT
PRODUCE
THE
EXPECTED
OUTPUT
BY
HOW
MUCH
WILL
THE
COUNTER
GO
UP
EACH
TIME
CLICK
HERE
FOR
THE
ANSWER
SECTION
QUESTION
EXAMPLE
TELLS
US
THAT
WE
HAVE
AN
UNDEFINED
VARIABLENUMBER
BUT
OUR
ONLY
VARIABLE
NAME
IS
COUNTER
CLICK
HERE
FOR
THE
ANSWER
SECTION
EXAMPLE
REC
REC
C
INCLUDE
STDIO
H
DEFINE
RECIPROCAL
NUMBER
NUMBER
INT
MAIN
FLOAT
COUNTER
COUNTER
FOR
OUR
TABLE
FOR
COUNTER
COUNTER
COUNTER
PRINTF
F
F
N
COUNTER
RECIPROCAL
COUNTER
RETURN
ADVANCED
FEATURES
THIS
BOOK
DOES
NOT
COVER
THE
COMPLETE
LIST
OF
C
PREPROCESSOR
DIRECTIVES
AMONG
THE
MORE
ADVANCED
FEATURES
ARE
AN
ADVANCED
FORM
OF
THE
IF
DIRECTIVE
FOR
CONDITIO
NAL
COMPILATIONS
AND
THE
PRAGMA
DIRECTIVE
FOR
INSERTING
COMPILER
DEPENDENT
COMMANDS
INTO
A
FILE
SEE
YOUR
C
REFERENCE
MANUAL
FOR
MORE
INFORMATION
ABOUT
THESE
FEATURES
SUMMARY
THE
C
PREPROCESSOR
IS
A
VERY
USEFUL
PART
OF
THE
C
LANGUAGE
IT
HAS
A
COMPLETELY
DIFFERENT
LOOK
AND
FEEL
THOUGH
AND
IT
MUST
BE
TREATED
APART
FROM
THE
MAIN
C
COMPILER
PROBLEMS
IN
MACRO
DEFINITIONS
OFTEN
DO
NOT
SHOW
UP
WHERE
THE
MACRO
IS
DEFINED
BUT
RESULT
IN
ERRORS
MUCH
FURTHER
DOWN
IN
THE
PROGRAM
BY
FOLLOWING
A
FEW
SIMPLE
RULES
YOU
CAN
DECREASE
THE
CHANCES
OF
HAVING
PROBLEMS
PUT
PARENTHESES
AROUND
EVERYTHING
IN
PARTICULAR
THEY
SHOULD
ENCLOSE
DEFINE
CONSTANTS
AND
MACRO
PARAMETERS
WHEN
DEFINING
A
MACRO
WITH
MORE
THAN
ONE
STATEMENT
ENCLOSE
THE
CODE
IN
CURLY
BRACES
THE
PREPROCESSOR
IS
NOT
C
DON
T
USE
AND
FINALLY
IF
YOU
GOT
THIS
FAR
BE
GLAD
THAT
THE
WORST
IS
OVER
ANSWERS
ANSWER
AFTER
THE
PROGRAM
HAS
BEEN
RUN
THROUGH
THE
PREPROCESSOR
THE
PRINTF
STATEMENT
IS
EXPANDED
TO
LOOK
LIKE
PRINTF
THE
SQUARE
OF
ALL
THE
PARTS
IS
D
N
THE
EQUATION
EVALUATES
TO
PUT
PARENTHESES
AROUND
ALL
EXPRESSIONS
IN
MACROS
IF
YOU
CHANGE
THE
DEFINITION
OF
TO
DEFINE
THE
PROGRAM
WILL
EXECUTE
CORRECTLY
ANSWER
THE
PREPROCESSOR
IS
A
VERY
SIMPLE
MINDED
PROGRAM
WHEN
IT
DEFINES
A
MACRO
EVERYTHING
PAST
THE
IDENTIFIER
IS
PART
OF
THE
MACRO
IN
THIS
CASE
THE
DEFINITION
OF
MAX
IS
LITERALLY
WHEN
THE
FOR
STATEMENT
IS
EXPANDED
THE
RESULT
IS
FOR
COUNTER
COUNTER
COUNTER
C
ALLOWS
YOU
TO
COMPUTE
A
RESULT
AND
THROW
IT
AWAY
THIS
WILL
GENERATE
A
NULL
EFFECT
WARNING
IN
SOME
COMPILERS
FOR
THIS
STATEMENT
THE
PROGRAM
CHECKS
TO
SEE
IF
COUNTER
IS
AND
THEN
DISCARDS
THE
A
NSWER
REMOVING
THE
FROM
THE
DEFINITION
WILL
CORRECT
THE
PROBLEM
ANSWER
AS
WITH
THE
PREVIOUS
PROBLEM
THE
PREPROCESSOR
DOES
NOT
RESPECT
C
SYNTAX
CONVENTIONS
IN
THIS
CASE
THE
PROGRAMMER
USED
A
SEMICOLON
TO
END
THE
STATEMENT
BUT
THE
PREPROCESSOR
INCLUDED
IT
AS
PART
OF
THE
DEFINITION
FOR
SIZE
THE
ASSIGNMENT
STATEMENT
FOR
SIZE
WHEN
EXPANDED
IS
SIZE
THE
TWO
SEMICOLONS
AT
THE
END
DO
NOT
HURT
US
BUT
THE
ONE
IN
THE
MIDDLE
IS
THE
KILLER
THIS
LINE
TELLS
C
TO
DO
TWO
THINGS
ASSIGN
TO
SIZE
COMPUTE
THE
VALUE
AND
THROW
IT
AWAY
THIS
CODE
RESULTS
IN
THE
NULL
EFFECT
WARNING
REMOVING
THE
SEMICOLONS
WILL
FIX
THE
PROBLEM
ANSWER
THE
OUTPUT
OF
THE
PREPROCESSOR
LOOKS
LIKE
VOID
EXIT
MAIN
INT
VALUE
VALUE
IF
VALUE
PRINTF
FATAL
ERROR
ABORT
N
EXIT
PRINTF
WE
DID
NOT
DIE
N
RETURN
THE
PROBLEM
IS
THAT
TWO
STATEMENTS
FOLLOW
THE
IF
LINE
NORMALLY
THEY
WOULD
BE
PUT
ON
TWO
LINES
LET
LOOK
AT
THIS
PROGRA
M
PROPERLY
INDENTED
INCLUDE
STDIO
H
INCLUDE
STDLIB
H
MAIN
INT
VALUE
A
RANDOM
VALUE
FOR
TESTING
VALUE
IF
VALUE
PRINTF
FATAL
ERROR
ABORT
N
EXIT
PRINTF
WE
DID
NOT
DIE
N
RETURN
WITH
THIS
NEW
FORMAT
WE
CAN
EASILY
DETERMINE
WHY
WE
ALWAYS
EXIT
THE
FACT
THAT
THERE
WERE
TWO
STATEMENTS
AFTER
THE
IF
WAS
HIDDEN
FROM
US
BY
USING
A
SINGLE
PREPROCESSOR
MACRO
THE
CURE
FOR
THIS
PROBLEM
IS
TO
PUT
CURLY
BRACES
A
ROUND
ALL
MULTISTATEMENT
MACROS
FOR
EXAMPLE
DEFINE
DIE
PRINTF
FATAL
ERROR
ABORT
N
EXIT
ANSWER
THE
PROGRAM
PRINTS
X
X
SQUARED
X
X
SQUARED
X
X
SQUARED
X
X
SQUARED
X
X
SQUARED
THE
PROBLEM
IS
WITH
THE
SQR
COUNTER
EXPRESSION
EXPANDING
THIS
EXPRESSION
WE
GET
SQR
COUNTER
COUNTER
COUNTER
SO
OUR
SQR
MACRO
DOES
NOT
WORK
PUTTING
PARENTHESES
AROUND
THE
PARAMETERS
SOLVES
THIS
PROBLEM
DEFINE
SQR
X
X
X
ANSWER
THE
ANSWER
IS
THAT
THE
COUNTER
IS
INCREMENTED
BY
TWO
EACH
TIME
THROUGH
THE
LOOP
THIS
INCREMENTATION
OCCURS
BECAUSE
THE
MACRO
CALL
SQR
COUNTER
IS
EXPANDED
TO
COUNTER
COUNTER
ANSWER
THE
ONLY
DIFFERENCE
BETWEEN
A
PARAMETERIZED
MACRO
AND
ONE
WITHOUT
PARAMETERS
IS
THE
PARENTHESIS
IMMEDIATELY
FOLLOWING
THE
MACRO
NAME
IN
THIS
CASE
A
SPACE
FOLLOWS
THE
DEFINITION
OF
RECIPROCAL
SO
THE
MACRO
IS
NOT
PARAMETERIZED
INSTEAD
IT
IS
A
SIMPLE
TEXT
REPLACEMENT
MACRO
THAT
WILL
REPLACE
RECIPROCAL
WITH
NUMBER
NUMBER
REMO
VING
THE
SPACE
BETWEEN
RECIPROCAL
AND
NUMBER
WILL
CORRECT
THE
PROBLEM
BRIAN
W
KERNIGHAN
ROB
PIKE
ADDISON
WESLEY
BOSTON
SAN
FRANCISCO
NEW
YORK
TORONTO
MONTREAL
LONDON
MUNICH
PARIS
MADRID
CAPETOWN
SYDNEY
TOKYO
SINGAPORE
MEXICO
CITY
MANY
OF
THE
DESIGNATIONS
USED
BY
MANUFACTURERS
AND
SELLERS
TO
DISTINGUISH
THEIR
PRODUCTS
AS
TRADEMARKS
WHERE
THOSE
DESIGNATIONS
APPEAR
IN
THIS
BOOK
AND
ADDISON
WESLEY
WERE
TRADEMARK
CLAIM
THE
DESIGNATIONS
HAVE
BEEN
PRINTED
IN
INITIAL
CAPITAL
LETTERS
OR
IN
ALL
CAP
THE
AUTHOR
AND
PUBLISHER
HAVE
TAKEN
CARE
IN
THE
PREPARATION
OF
THIS
BOOK
BUT
MAKE
NO
IMPLIED
WARRANTY
OF
ANY
KIND
AND
ASSUME
NO
RESPONSIBILITY
FOR
ERRORS
OR
OMISSIONS
NO
ASSUMED
FOR
INCIDENTAL
OR
CONSEQUENTIAL
DAMAGES
IN
CONNECTION
WITH
OR
ARISING
OUT
OF
TH
INFORMATION
OR
PROGRAMS
CONTAINED
HEREIN
THE
PUBLISHER
OFFERS
DISCOUNTS
ON
THIS
BOOK
WHEN
ORDERED
IN
QUANTITY
FOR
SPECIAL
SALES
F
TION
PLEASE
CONTACT
PEARSON
EDUCATION
CORPORATE
SALES
DIVISION
W
STREET
INDIANAPOLIS
IN
CORPSALES
PEARSONED
COM
VISIT
AW
ON
THE
WEB
WWW
AWPROFESSIONAL
COM
THIS
BOOK
WAS
TYPESET
GRAP
I
PI
C
I
TBL
IEQN
ITROFF
MPM
IN
TIMES
AND
LUCIDA
SANS
TYP
AUTHORS
LIBRARY
OF
CONGRESS
CATALOGING
IN
PUBLICATION
DATA
KERNIGHAN
BRIAN
W
THE
PRACTICE
OF
PROGRAMMING
BRIAN
W
KERNIGHAN
ROB
PIKE
P
CM
ADDISON
WESLEY
PROFESSIONAL
COMPUTING
SERIES
INCLUDES
BIBLIOGRAPHICAL
REFERENCES
ISBN
X
I
COMPUTER
PROGRAMMING
I
PIKE
ROB
II
TITLE
III
SERIES
L
CIP
COPYRIGHT
BY
LUCENT
TECHNOLOGIES
ALL
RIGHTS
RESERVED
NO
PART
OF
THIS
PUBLICATION
MAY
BE
REPRODUCED
STORED
IN
A
RETRIEVAL
TRANSMITTED
IN
ANY
FORM
OR
BY
ANY
MEANS
ELECTRONIC
MECHANICAL
PHOTOCOPYING
RECORD
WISE
WITHOUT
THE
PRIOR
CONSENT
OF
THE
PUBLISHER
PRINTED
IN
THE
UNITED
STATES
OF
AMERICA
SIMULTANEOUSLY
IN
CANADA
ISBN
X
TEXT
PRINTED
IN
THE
UNITED
STATES
ON
RECYCLED
PAPER
AT
RR
DONNELLEY
IN
HARRISONBURG
VIRGINIA
TWENTY
SECOND
PRINTING
FEBRUARY
CONTEN
PREFACE
CHAPTER
STYLE
NAMES
EXPRESSIONS
AND
STATEMENTS
CONSISTENCY
AND
IDIOMS
FUNCTION
MACROS
MAGIC
NUMBERS
COMMENTS
WHY
BOTHER
CHAPTER
ALGORITHMS
AND
DATA
STRUCTURES
SEARCHING
SORTING
LIBRARIES
A
JAVA
QUICKSORT
O
NOTATION
GROWING
ARRAYS
LISTS
TREES
HASH
TABLES
SUMMARY
CHAPTER
DESIGN
AND
IMPLEMENTATION
THE
MARKOV
CHAIN
ALGORITHM
DATA
STRUCTURE
ALTERNATIVES
BUILDING
THE
DATA
STRUCTURE
IN
C
GENERATING
OUTPUT
V
VI
THE
PRACTICE
OF
PROGRAMMING
JAVA
C
AWKANDPERL
PERFORMANCE
LESSONS
CHAPTER
INTERFACES
COMMA
SEPARATED
VALUES
A
PROTOTYPE
LIBRARY
A
LIBRARY
FOR
OTHERS
A
C
IMPLEMENTATION
INTERFACE
PRINCIPLES
RESOURCE
MANAGEMENT
ABORT
RETRY
FAIL
USER
INTERFACES
CHAPTER
DEBUGGING
DEBUGGERS
GOOD
CLUES
EASY
BUGS
NO
CLUES
HARD
BUGS
LAST
RESORTS
NON
REPRODUCIBLE
BUGS
DEBUGGING
TOOLS
OTHER
PEOPLE
BUGS
SUMMARY
CHAPTER
TESTING
TEST
AS
YOU
WRITE
THE
CODE
SYSTEMATIC
TESTING
TEST
AUTOMATION
TEST
SCAFFOLDS
STRESS
TESTS
TIPS
FOR
TESTING
WHO
DOES
THE
TESTING
TESTING
THE
MARKOV
PROGRAM
SUMMARY
CHAPTER
PERFORMANCE
A
BOTTLENECK
TIMING
AND
PROFILING
STRATEGIES
FOR
SPEED
TUNING
THE
CODE
SPACE
EFFICIENCY
THE
PRACTICE
OF
PROGRAMMING
ESTIMATION
SUMMARY
CHAPTER
PORTABILITY
LANGUAGE
HEADERS
AND
LIBRARIES
PROGRAM
ORGANIZATION
ISOLATION
DATA
EXCHANGE
BYTE
ORDER
PORTABILITY
AND
UPGRADE
INTERNATIONALIZATION
SUMMARY
CHAPTER
NOTATION
FORMATTING
DATA
REGULAR
EXPRESSIONS
PROGRAMMABLE
TOOLS
INTERPRETERS
COMPILERS
AND
VIRTUAL
MACHINES
PROGRAMS
THAT
WRITE
PROGRAMS
USING
MACROS
TO
GENERATE
CODE
COMPILING
ON
THE
FLY
EPILOGUE
APPENDIX
COLLECTED
RULES
INDEX
PREFA
HAVE
YOU
EVER
WASTED
A
LOT
OF
TIME
CODING
THE
WRONG
ALGORITHM
USED
A
DATA
STRUCTURE
THAT
WAS
MUCH
TOO
COMPLICATED
TESTED
A
PROGRAM
BUT
MISSED
AN
OBVIOUS
PROBLEM
SPENT
A
DAY
LOOKING
FOR
A
BUG
YOU
SHOULD
HAVE
FOUND
IN
FIVE
MINUTES
NEEDED
TO
MAKE
A
PROGRAM
RUN
THREE
TIMES
FASTER
AND
USE
LESS
MEMORY
STRUGGLED
TO
MOVE
A
PROGRAM
FROM
A
WORKSTATION
TO
A
PC
OR
VICE
VERSA
TRIED
TO
MAKE
A
MODEST
CHANGE
IN
SOMEONE
ELSE
PROGRAM
REWRITTEN
A
PROGRAM
BECAUSE
YOU
COULDN
T
UNDERSTAND
IT
WAS
IT
FUN
THESE
THINGS
HAPPEN
TO
PROGRAMMERS
ALL
THE
TIME
BUT
DEALING
WITH
SUCH
PR
IS
OFTEN
HARDER
THAN
IT
SHOULD
BE
BECAUSE
TOPICS
LIKE
TESTING
DEBUGGING
PORT
PERFORMANCE
DESIGN
ALTERNATIVES
AND
STYLE
THE
PRACTICE
OF
PROGRAMMING
USUALLY
THE
FOCUS
OF
COMPUTER
SCIENCE
OR
PROGRAMMING
COURSES
MOST
PROGRA
LEARN
THEM
HAPHAZARDLY
AS
THEIR
EXPERIENCE
GROWS
AND
A
FEW
NEVER
LEARN
THEM
A
IN
A
WORLD
OF
ENORMOUS
AND
INTRICATE
INTERFACES
CONSTANTLY
CHANGING
TOOLS
A
GUAGES
AND
SYSTEMS
AND
RELENTLESS
PRESSURE
FOR
MORE
OF
EVERYTHING
ONE
CAN
LOS
OF
THE
BASIC
PRINCIPLES
SIMPLICITY
CLARITY
GENERALITY
THAT
FORM
THE
BEDROCK
O
SOFTWARE
ONE
CAN
ALSO
OVERLOOK
THE
VALUE
OF
TOOLS
AND
NOTATIONS
THAT
MECHANIZ
OF
SOFTWARE
CREATION
AND
THUS
ENLIST
THE
COMPUTER
IN
ITS
OWN
PROGRAMMING
OUR
APPROACH
IN
THIS
BOOK
IS
BASED
ON
THESE
UNDERLYING
INTERRELATED
PRIN
WHICH
APPLY
AT
ALL
LEVELS
OF
COMPUTING
THESE
INCLUDE
SIMPLICITY
WHICH
KEE
GRAMS
SHORT
AND
MANAGEABLE
CLARITY
WHICH
MAKES
SURE
THEY
ARE
EASY
TO
UNDE
FOR
PEOPLE
AS
WELL
AS
MACHINES
GENERALITY
WHICH
MEANS
THEY
WORK
WELL
IN
A
RANGE
OF
SITUATIONS
AND
ADAPT
WELL
AS
NEW
SITUATIONS
ARISE
AND
AUTOMATION
WHI
THE
MACHINE
DO
THE
WORK
FOR
US
FREEING
US
FROM
MUNDANE
TASKS
BY
LOOKING
A
PUTER
PROGRAMMING
IN
A
VARIETY
OF
LANGUAGES
FROM
ALGORITHMS
AND
DATA
STR
THROUGH
DESIGN
DEBUGGING
TESTING
AND
PERFORMANCE
IMPROVEMENT
WE
CAN
ILL
IX
X
PREFACE
UNIVERSAL
ENGINEERING
CONCEPTS
THAT
ARE
INDEPENDENT
OF
LANGUAGE
OPERATING
PROGRAMMING
PARADIGM
THIS
BOOK
COMES
FROM
MANY
YEARS
OF
EXPERIENCE
WRITING
AND
MAINTAINING
SOFTWARE
TEACHING
PROGRAMMING
COURSES
AND
WORKING
WITH
A
WIDE
VARIETY
GRAMMERS
WE
WANT
TO
SHARE
LESSONS
ABOUT
PRACTICAL
ISSUES
TO
PASS
ON
INSIG
OUR
EXPERIENCE
AND
TO
SUGGEST
WAYS
FOR
PROGRAMMERS
OF
ALL
LEVELS
TO
BE
MO
CIENT
AND
PRODUCTIVE
WE
ARE
WRITING
FOR
SEVERAL
KINDS
OF
READERS
IF
YOU
ARE
A
STUDENT
WHO
HA
PROGRAMMING
COURSE
OR
TWO
AND
WOULD
LIKE
TO
BE
A
BETTER
PROGRAMMER
THIS
EXPAND
ON
SOME
OF
THE
TOPICS
FOR
WHICH
THERE
WASN
T
ENOUGH
TIME
IN
SCHOO
WRITE
PROGRAMS
AS
PART
OF
YOUR
WORK
BUT
IN
SUPPORT
OF
OTHER
ACTIVITIES
RATHE
THE
GOAL
IN
ITSELF
THE
INFORMATION
WILL
HELP
YOU
TO
PROGRAM
MORE
EFFECTIVELY
ARE
A
PROFESSIONAL
PROGRAMMER
WHO
DIDN
T
GET
ENOUGH
EXPOSURE
TO
SUCH
SCHOOL
OR
WHO
WOULD
LIKE
A
REFRESHER
OR
IF
YOU
ARE
A
SOFTWARE
MANAGER
WHO
GUIDE
YOUR
STAFF
IN
THE
RIGHT
DIRECTION
THE
MATERIAL
HERE
SHOULD
BE
OF
VALUE
WE
HOPE
THAT
THE
ADVICE
WILL
HELP
YOU
TO
WRITE
BETTER
PROGRAMS
THE
ONL
UISITE
IS
THAT
YOU
HAVE
DONE
SOME
PROGRAMMING
PREFERABLY
IN
C
C
OR
COURSE
THE
MORE
EXPERIENCE
YOU
HAVE
THE
EASIER
IT
WILL
BE
NOTHING
CAN
TAKE
NEOPHYTE
TO
EXPERT
IN
DAYS
UNIX
AND
LINUX
PROGRAMMERS
WILL
FIND
SOM
EXAMPLES
MORE
FAMILIAR
THAN
WILL
THOSE
WHO
HAVE
USED
ONLY
WINDOWS
AND
SYSTEMS
BUT
PROGRAMMERS
FROM
ANY
ENVIRONMENT
SHOULD
DISCOVER
THINGS
TO
M
LIVES
EASIER
THE
PRESENTATION
IS
ORGANIZED
INTO
NINE
CHAPTERS
EACH
FOCUSING
ON
O
ASPECT
OF
PROGRAMMING
PRACTICE
CHAPTER
DISCUSSES
PROGRAMMING
STYLE
GOOD
STYLE
IS
SO
IMPORTANT
TO
G
GRAMMING
THAT
WE
HAVE
CHOSEN
TO
COVER
IT
FIRST
WELL
WRITTEN
PROGRAMS
ARE
BE
BADLY
WRITTEN
ONES
THEY
HAVE
FEWER
ERRORS
AND
ARE
EASIER
TO
DEBUG
AND
TO
SO
IT
IS
IMPORTANT
TO
THINK
ABOUT
STYLE
FROM
THE
BEGINNING
THIS
CHAPTER
AL
DUCES
AN
IMPORTANT
THEME
IN
GOOD
PROGRAMMING
THE
USE
OF
IDIOMS
APPROPRIA
LANGUAGE
BEING
USED
ALGORITHMS
AND
DATA
STRUCTURES
THE
TOPICS
OF
CHAPTER
ARE
THE
CORE
OF
PUTER
SCIENCE
CURRICULUM
AND
A
MAJOR
PART
OF
PROGRAMMING
COURSES
SINCE
M
ERS
WILL
ALREADY
BE
FAMILIAR
WITH
THIS
MATERIAL
OUR
TREATMENT
IS
INTENDED
A
REVIEW
OF
THE
HANDFUL
OF
ALGORITHMS
AND
DATA
STRUCTURES
THAT
SHOW
UP
IN
ALM
PROGRAM
MORE
COMPLEX
ALGORITHMS
AND
DATA
STRUCTURES
USUALLY
EVOLVE
FRO
BUILDING
BLOCKS
SO
ONE
SHOULD
MASTER
THE
BASICS
CHAPTER
DESCRIBES
THE
DESIGN
AND
IMPLEMENTATION
OF
A
SMALL
PROGRAM
T
TRATES
ALGORITHM
AND
DATA
STRUCTURE
ISSUES
IN
A
REALISTIC
SETTING
THE
PROGRAM
MENTED
IN
FIVE
LANGUAGES
COMPARING
THE
VERSIONS
SHOWS
HOW
THE
SAME
DATA
ARE
HANDLED
IN
EACH
AND
HOW
EXPRESSIVENESS
AND
PERFORMANCE
VARY
ACROSS
A
OF
LANGUAGES
PREFAC
INTERFACES
BETWEEN
USERS
PROGRAMS
AND
PARTS
OF
PROGRAMS
ARE
FUNDAMENTAL
GRAMMING
AND
MUCH
OF
THE
SUCCESS
OF
SOFTWARE
IS
DETERMINED
BY
HOW
WELL
INT
ARE
DESIGNED
AND
IMPLEMENTED
CHAPTER
SHOWS
THE
EVOLUTION
OF
A
SMALL
LIBR
PARSING
A
WIDELY
USED
DATA
FORMAT
EVEN
THOUGH
THE
EXAMPLE
IS
SMALL
IT
ILLU
MANY
OF
THE
CONCERNS
OF
INTERFACE
DESIGN
ABSTRACTION
INFORMATION
HIDING
MANAGEMENT
AND
ERROR
HANDLING
MUCH
AS
WE
TRY
TO
WRITE
PROGRAMS
CORRECTLY
THE
FIRST
TIME
BUGS
AND
TH
DEBUGGING
ARE
INEVITABLE
CHAPTER
GIVES
STRATEGIES
AND
TACTICS
FOR
SYSTEMA
EFFECTIVE
DEBUGGING
AMONG
THE
TOPICS
ARE
THE
SIGNATURES
OF
COMMON
BUGS
IMPORTANCE
OF
NUMEROLOGY
WHERE
PATTERNS
IN
DEBUGGING
OUTPUT
OFTEN
WHERE
A
PROBLEM
LIES
TESTING
IS
AN
ATTEMPT
TO
DEVELOP
A
REASONABLE
ASSURANCE
THAT
A
PROGRAM
IS
CORRECTLY
AND
THAT
IT
STAYS
CORRECT
AS
IT
EVOLVES
THE
EMPHASIS
IN
CHAPTER
IS
TEMATIC
TESTING
BY
HAND
AND
MACHINE
BOUNDARY
CONDITION
TESTS
PROBE
AT
WEAK
SPOTS
MECHANIZATION
AND
TEST
SCAFFOLDS
MAKE
IT
EASY
TO
DO
EXTENSIVE
WITH
MODEST
EFFORT
STRESS
TESTS
PROVIDE
A
DIFFERENT
KIND
OF
TESTING
THAN
TYPICA
DO
AND
FERRET
OUT
A
DIFFERENT
CLASS
OF
BUGS
COMPUTERS
ARE
SO
FAST
AND
COMPILERS
ARE
SO
GOOD
THAT
MANY
PROGRAMS
A
ENOUGH
THE
DAY
THEY
ARE
WRITTEN
BUT
OTHERS
ARE
TOO
SLOW
OR
THEY
USE
TOO
MUCH
ORY
OR
BOTH
CHAPTER
PRESENTS
AN
ORDERLY
WAY
TO
APPROACH
THE
TASK
OF
MAKING
GRAM
USE
RESOURCES
EFFICIENTLY
SO
THAT
THE
PROGRAM
REMAINS
CORRECT
AND
SOUND
MADE
MORE
EFFICIENT
CHAPTER
COVERS
PORTABILITY
SUCCESSFUL
PROGRAMS
LIVE
LONG
ENOUGH
TH
ENVIRONMENT
CHANGES
OR
THEY
MUST
BE
MOVED
TO
NEW
SYSTEMS
OR
NEW
HARDWARE
COUNTRIES
THE
GOAL
OF
PORTABILITY
IS
TO
REDUCE
THE
MAINTENANCE
OF
A
PROGRAM
B
MIZING
THE
AMOUNT
OF
CHANGE
NECESSARY
TO
ADAPT
IT
TO
A
NEW
ENVIRONMENT
COMPUTING
IS
RICH
IN
LANGUAGES
NOT
JUST
THE
GENERAL
PURPOSE
ONES
THAT
WE
THE
BULK
OF
PROGRAMMING
BUT
ALSO
MANY
SPECIALIZED
LANGUAGES
THAT
FOCUS
ON
DOMAINS
CHAPTER
PRESENTS
SEVERAL
EXAMPLES
OF
THE
IMPORTANCE
OF
NOTATION
I
PUTING
AND
SHOWS
HOW
WE
CAN
USE
IT
TO
SIMPLIFY
PROGRAMS
TO
GUIDE
IMPLEMEN
AND
EVEN
TO
HELP
US
WRITE
PROGRAMS
THAT
WRITE
PROGRAMS
TO
TALK
ABOUT
PROGRAMMING
WE
HAVE
TO
SHOW
A
LOT
OF
CODE
MOST
OF
THE
EX
WERE
WRITTEN
EXPRESSLY
FOR
THE
BOOK
ALTHOUGH
SOME
SMALL
ONES
WERE
ADAPTE
OTHER
SOURCES
WE
VE
TRIED
HARD
TO
WRITE
OUR
OWN
CODE
WELL
AND
HAVE
TESTED
IT
A
DOZEN
SYSTEMS
DIRECTLY
FROM
THE
MACHINE
READABLE
TEXT
MORE
INFORMATION
I
ABLE
AT
THE
WEB
SITE
FOR
THE
PRACTICE
OF
PROGRAMMING
HTTP
TPOP
AWL
COM
THE
MAJORITY
OF
THE
PROGRAMS
ARE
IN
C
WITH
A
NUMBER
OF
EXAMPLES
IN
C
JAVA
AND
SOME
BRIEF
EXCURSIONS
INTO
SCRIPTING
LANGUAGES
AT
THE
LOWEST
LEVEL
C
ARE
ALMOST
IDENTICAL
AND
OUR
C
PROGRAMS
ARE
VALID
C
PROGRAMS
AS
WEL
AND
JAVA
ARE
LINEAL
DESCENDANTS
OF
C
SHARING
MORE
THAN
A
LITTLE
OF
ITS
SYNTAX
AN
OF
ITS
EFFICIENCY
AND
EXPRESSIVENESS
WHILE
ADDING
RICHER
TYPE
SYSTEMS
AND
LI
XII
PREFACE
IN
OUR
OWN
WORK
WE
ROUTINELY
USE
ALL
THREE
OF
THESE
LANGUAGES
AND
MANY
OTH
CHOICE
OF
LANGUAGE
DEPENDS
ON
THE
PROBLEM
OPERATING
SYSTEMS
ARE
BEST
WRIT
EFFICIENT
AND
UNRESTRICTIVE
LANGUAGE
LIKE
C
OR
C
QUICK
PROTOTYPES
ARE
OFTE
IN
A
COMMAND
INTERPRETER
OR
A
SCRIPTING
LANGUAGE
LIKE
AWK
OR
PERL
FOR
USER
VISUAL
BASIC
AND
TCL
TK
ARE
STRONG
CONTENDERS
ALONG
WITH
JAVA
THERE
IS
AN
IMPORTANT
PEDAGOGICAL
ISSUE
IN
CHOOSING
A
LANGUAGE
FOR
OUR
JUST
AS
NO
LANGUAGE
SOLVES
ALL
PROBLEMS
EQUALLY
WELL
NO
SINGLE
LANGUAGE
IS
PRESENTING
ALL
TOPICS
HIGHER
LEVEL
LANGUAGES
PREEMPT
SOME
DESIGN
DECISION
USE
A
LOWER
LEVEL
LANGUAGE
WE
GET
TO
CONSIDER
ALTERNATIVE
ANSWERS
TO
THE
QUES
EXPOSING
MORE
OF
THE
DETAILS
WE
CAN
TALK
ABOUT
THEM
BETTER
EXPERIENCE
SH
EVEN
WHEN
WE
USE
THE
FACILITIES
OF
HIGH
LEVEL
LANGUAGES
IT
INVALUABLE
TO
K
THEY
RELATE
TO
LOWER
LEVEL
ISSUES
WITHOUT
THAT
INSIGHT
IT
EASY
TO
RUN
INTO
PERF
PROBLEMS
AND
MYSTERIOUS
BEHAVIOR
SO
WE
WILL
OFTEN
USE
C
FOR
OUR
EXAMP
THOUGH
IN
PRACTICE
WE
MIGHT
CHOOSE
SOMETHING
ELSE
FOR
THE
MOST
PART
HOWEVER
THE
LESSONS
ARE
INDEPENDENT
OF
ANY
PARTICULAR
MING
LANGUAGE
THE
CHOICE
OF
DATA
STRUCTURE
IS
AFFECTED
BY
THE
LANGUAGE
AT
HA
MAY
BE
FEW
OPTIONS
IN
SOME
LANGUAGES
WHILE
OTHERS
MIGHT
SUPPORT
A
VARIETY
O
TIVES
BUT
THE
WAY
TO
APPROACH
MAKING
THE
CHOICE
WILL
BE
THE
SAME
THE
HOW
TO
TEST
AND
DEBUG
ARE
DIFFERENT
IN
DIFFERENT
LANGUAGES
BUT
STRATEGIES
AN
ARE
SIMILAR
IN
ALL
MOST
OF
THE
TECHNIQUES
FOR
MAKING
A
PROGRAM
EFFICIEN
APPLIED
IN
ANY
LANGUAGE
WHATEVER
LANGUAGE
YOU
WRITE
IN
YOUR
TASK
AS
A
PROGRAMMER
IS
TO
DO
THE
CAN
WITH
THE
TOOLS
AT
HAND
A
GOOD
PROGRAMMER
CAN
OVERCOME
A
POOR
LANGU
CLUMSY
OPERATING
SYSTEM
BUT
EVEN
A
GREAT
PROGRAMMING
ENVIRONMENT
WILL
N
A
BAD
PROGRAMMER
WE
HOPE
THAT
NO
MATTER
WHAT
YOUR
CURRENT
EXPERIENCE
THIS
BOOK
WILL
HELP
YOU
TO
PROGRAM
BETTER
AND
ENJOY
IT
MORE
WE
ARE
DEEPLY
GRATEFUL
TO
FRIENDS
AND
COLLEAGUES
WHO
READ
DRAFTS
OF
THE
AND
GAVE
US
MANY
HELPFUL
COMMENTS
JON
BENTLEY
RUSS
COX
JOHN
LAKOS
DERMAN
PETER
MEMISHIAN
IAN
LANCE
TAYLOR
HOWARD
TRICKEY
AND
CHRIS
V
READ
THE
MANUSCRIPT
SOME
MORE
THAN
ONCE
WITH
EXCEPTIONAL
CARE
AND
THORO
WE
ARE
INDEBTED
TO
TOM
CARGILL
CHRIS
CLEELAND
STEVE
DEWHURST
ERIC
ANDREW
HERRON
GERARD
HOLZMANN
DOUG
MCILROY
PAUL
MCNAMEE
PETER
DENNIS
RITCHIE
RICH
STEVENS
TOM
SZYMANSKI
KENTARO
TOYAMA
JOHN
WAI
C
WANG
PETER
WEINBERGER
MARGARET
WRIGHT
AND
CLIFF
YOUNG
FOR
INVALUA
MENTS
ON
DRAFTS
AT
VARIOUS
STAGES
WE
ALSO
APPRECIATE
GOOD
ADVICE
AND
THOUGH
GESTIONS
FROM
AL
AHO
KEN
ARNOLD
CHUCK
BIGELOW
JOSHUA
BLOCH
BILL
BOB
FLANDRENA
RENEE
FRENCH
MARK
KERNIGHAN
ANDY
KOENIG
SAPE
MULLE
NEMETH
MARTY
RABINOWITZ
MARK
V
SHANEY
BJAME
STROUSTRUP
KEN
THOMP
PHIL
WADLER
THANK
YOU
ALL
BRIAN
W
KERN
ROB
PIKE
DESIGN
AND
IMPLEMENTATI
SHOW
ME
YOUR
FLOWCHARTS
AND
CONCEAL
YOUR
TABLES
AND
I
SHA
TINUE
TO
BE
MYSTIFIED
SHOW
ME
YOUR
TABLES
AND
I
WON
T
NEED
YOUR
FLOWCHARTS
THEY
LL
BE
OBVIOUS
FREDERICK
P
BROOKS
JR
THE
MYTHICAL
MAN
AS
THE
QUOTATION
FROM
BROOKS
CLASSIC
BOOK
SUGGESTS
THE
DESIGN
OF
THE
DAT
TURES
IS
THE
CENTRAL
DECISION
IN
THE
CREATION
OF
A
PROGRAM
ONCE
THE
DATA
STRUCTU
LAID
OUT
THE
ALGORITHMS
TEND
TO
FALL
INTO
PLACE
AND
THE
CODING
IS
COMPARATIVELY
THIS
POINT
OF
VIEW
IS
OVERSIMPLIFIED
BUT
NOT
MISLEADING
IN
THE
PREVIOUS
WE
EXAMINED
THE
BASIC
DATA
STRUCTURES
THAT
ARE
THE
BUILDING
BLOCKS
OF
MOST
PRO
IN
THIS
CHAPTER
WE
WILL
COMBINE
SUCH
STRUCTURES
AS
WE
WORK
THROUGH
THE
DESI
IMPLEMENTATION
OF
A
MODEST
SIZED
PROGRAM
WE
WILL
SHOW
HOW
THE
PROBLEM
ENCES
THE
DATA
STRUCTURES
AND
HOW
THE
CODE
THAT
FOLLOWS
IS
STRAIGHTFORWARD
O
HAVE
THE
DATA
STRUCTURES
MAPPED
OUT
ONE
ASPECT
OF
THIS
POINT
OF
VIEW
IS
THAT
THE
CHOICE
OF
PROGRAMMING
LANGUAGE
ATIVELY
UNIMPORTANT
TO
THE
OVERALL
DESIGN
WE
WILL
DESIGN
THE
PROGRAM
IN
THE
AND
THEN
WRITE
IT
IN
C
JAVA
C
AWK
AND
PERL
COMPARING
THE
IMPLEMEN
DEMONSTRATES
HOW
LANGUAGES
CAN
HELP
OR
HINDER
AND
WAYS
IN
WHICH
THEY
ARE
UN
TANT
PROGRAM
DESIGN
CAN
CERTAINLY
BE
COLORED
BY
A
LANGUAGE
BUT
IS
NOT
USUALLY
NATED
BY
IT
THE
PROBLEM
WE
HAVE
CHOSEN
IS
UNUSUAL
BUT
IN
BASIC
FORM
IT
IS
TYPICAL
OF
PROGRAMS
SOME
DATA
COMES
IN
SOME
DATA
GOES
OUT
AND
THE
PROCESSING
DEPEN
LITTLE
INGENUITY
SPECIFICALLY
WE
RE
GOING
TO
GENERATE
RANDOM
ENGLISH
TEXT
THAT
READS
WELL
EMIT
RANDOM
LETTERS
OR
RANDOM
WORDS
THE
RESULT
WILL
BE
NONSENSE
FOR
EXAMPLE
GRAM
THAT
RANDOMLY
SELECTS
LETTERS
AND
BLANKS
TO
SEPARATE
WORDS
MIGHT
PRODUC
XPTMXGN
XUSAJA
AFQNZGXL
LHIDLWCD
RJDJUVPYDRLWNJY
WHICH
IS
NOT
VERY
CONVINCING
IF
WE
WEIGHT
THE
LETTERS
BY
THEIR
FREQUENCY
O
ANCE
IN
ENGLISH
TEXT
WE
MIGHT
GET
THIS
IDTEFOAE
TCS
TRDER
JCII
OFDSLNQETACP
T
OLA
WHICH
ISN
T
A
GREAT
DEAL
BETTER
WORDS
CHOSEN
FROM
THE
DICTIONARY
AT
RANDO
MAKE
MUCH
MORE
SENSE
POLYDACTYL
EQUATORIAL
SPLASHILY
JOWL
VERANDAH
CIRCUMSCRIB
FOR
BETTER
RESULTS
WE
NEED
A
STATISTICAL
MODEL
WITH
MORE
STRUCTURE
SUCH
AS
QUENCY
OF
APPEARANCE
OF
WHOLE
PHRASES
BUT
WHERE
CAN
WE
FIND
SUCH
STATISTIC
WE
COULD
GRAB
A
LARGE
BODY
OF
ENGLISH
AND
STUDY
IT
IN
DETAIL
BUT
THERE
IS
AND
MORE
ENTERTAINING
APPROACH
THE
KEY
OBSERVATION
IS
THAT
WE
CAN
USE
ANY
TEXT
TO
CONSTRUCT
A
STATISTICAL
MODEL
OF
THE
LANGUAGE
AS
USED
IN
THAT
TEXT
AND
GENERATE
RANDOM
TEXT
THAT
HAS
SIMILAR
STATISTICS
TO
THE
ORIGINAL
THE
MARKOV
CHAIN
ALGORITHM
AN
ELEGANT
WAY
TO
DO
THIS
SORT
OF
PROCESSING
IS
A
TECHNIQUE
CALLED
A
MARK
ALGORITHM
IF
WE
IMAGINE
THE
INPUT
AS
A
SEQUENCE
OF
OVERLAPPING
PHRASES
RITHM
DIVIDES
EACH
PHRASE
INTO
TWO
PARTS
A
MULTI
WORD
PREFIX
AND
A
SINGLE
SU
THAT
FOLLOWS
THE
PREFIX
A
MARKOV
CHAIN
ALGORITHM
EMITS
OUTPUT
PHRASES
BY
CHOOSING
THE
SUFFIX
THAT
FOLLOWS
THE
PREFIX
ACCORDING
TO
THE
STATISTICS
OF
IN
THE
ORIGINAL
TEXT
THREE
WORD
PHRASES
WORK
WELL
A
TWO
WORD
PREFIX
IS
USED
THE
SUFFIX
WORD
SET
W
AND
W
TO
THE
FIRST
TWO
WORDS
IN
THE
TEXT
PRINT
W
AND
LOOP
RANDOMLY
CHOOSE
W
ONE
OF
THE
SUCCESSORS
OF
PREFIX
W
W
IN
TH
PRINT
REPLACE
W
AND
W
BY
W
AND
W
REPEAT
LOOP
TO
ILLUSTRATE
SUPPOSE
WE
WANT
TO
GENERATE
RANDOM
TEXT
BASED
ON
A
FEW
SENTEN
PHRASED
FROM
THE
EPIGRAPH
ABOVE
USING
TWO
WORD
PREFIXES
SHOW
YOUR
FLOWCHARTS
AND
CONCEAL
YOUR
TABLES
AND
I
WILL
MYSTIFIED
SHOW
YOUR
TABLES
AND
YOUR
FLOWCHARTS
WILL
BE
OBVIOUS
END
THESE
ARE
SOME
OF
THE
PAIRS
OF
INPUT
WORDS
AND
THE
WORDS
THAT
FOLLOW
THEM
INPUT
PREFIX
SHOW
YOUR
YOUR
FLOWCHARTS
FLOWCHARTS
AND
FLOWCHARTS
WI
YOUR
TABLES
WI
BE
BE
MYSTIFIED
BE
OBVIOUS
SUFFIX
WORDS
THAT
FOLLOW
FLOWCHARTS
TABLES
AND
WILL
CONCEAL
BE
AND
AND
MYSTIFIED
OBVIOUS
SHOW
END
A
MARKOV
ALGORITHM
PROCESSING
THIS
TEXT
WILL
BEGIN
BY
PRINTING
SHOW
YOUR
A
THEN
RANDOMLY
PICK
EITHER
FLOWCHARTS
OR
TABLES
IF
IT
CHOOSES
THE
FORMER
T
RENT
PREFIX
BECOMES
YOUR
FLOWCHARTS
AND
THE
NEXT
WORD
WILL
BE
AND
OR
WIL
CHOOSES
TAB
ES
THE
NEXT
WORD
WILL
BE
AND
THIS
CONTINUES
UNTIL
ENOUGH
OUT
BEEN
GENERATED
OR
UNTIL
THE
END
MARKER
IS
ENCOUNTERED
AS
A
SUFFIX
OUR
PROGRAM
WILL
READ
A
PIECE
OF
ENGLISH
TEXT
AND
USE
A
MARKOV
CHAIN
ALGORI
GENERATE
NEW
TEXT
BASED
ON
THE
FREQUENCY
OF
APPEARANCE
OF
PHRASES
OF
A
FIXED
THE
NUMBER
OF
WORDS
IN
THE
PREFIX
WHICH
IS
TWO
IN
OUR
EXAMPLE
IS
A
PARA
MAKING
THE
PREFIX
SHORTER
TENDS
TO
PRODUCE
LESS
COHERENT
PROSE
MAKING
IT
LONGE
TO
REPRODUCE
THE
INPUT
TEXT
VERBATIM
FOR
ENGLISH
TEXT
USING
TWO
WORDS
TO
THIRD
IS
A
GOOD
COMPROMISE
IT
SEEMS
TO
RECREATE
THE
FLAVOR
OF
THE
INPUT
WHILE
ITS
OWN
WHIMSICAL
TOUCH
WHAT
IS
A
WORD
THE
OBVIOUS
ANSWER
IS
A
SEQUENCE
OF
ALPHABETIC
CHARACTERS
IS
DESIRABLE
TO
LEAVE
PUNCTUATION
ATTACHED
TO
THE
WORDS
SO
WORDS
AND
WORD
DIFFERENT
THIS
HELPS
TO
IMPROVE
THE
QUALITY
OF
THE
GENERATED
PROSE
BY
LETTING
PU
TION
AND
THEREFORE
INDIRECTLY
GRAMMAR
INFLUENCE
THE
WORD
CHOICE
ALTHOUGH
PERMITS
UNBALANCED
QUOTES
AND
PARENTHESES
TO
SNEAK
IN
WE
WILL
THEREFORE
D
WORD
AS
ANYTHING
BETWEEN
WHITE
SPACE
A
DECISION
THAT
PLACES
NO
RESTRICT
INPUT
LANGUAGE
AND
LEAVES
PUNCTUATION
ATTACHED
TO
THE
WORDS
SINCE
MOST
PR
MING
LANGUAGES
HAVE
FACILITIES
TO
SPLIT
TEXT
INTO
WHITE
SPACE
SEPARATED
WORDS
ALSO
EASY
TO
IMPLEMENT
BECAUSE
OF
THE
METHOD
ALL
WORDS
ALL
TWO
WORD
PHRASES
AND
ALL
THRE
PHRASES
IN
THE
OUTPUT
MUST
HAVE
APPEARED
IN
THE
INPUT
BUT
THERE
SHOULD
BE
MAN
WORD
AND
LONGER
PHRASES
THAT
ARE
SYNTHESIZED
HERE
ARE
A
FEW
SENTENCES
PRODU
THE
PROGRAM
WE
WILL
DEVELOP
IN
THIS
CHAPTER
WHEN
GIVEN
THE
TEXT
OF
CHAPTER
THE
SUN
ALSO
RISES
BY
ERNEST
HEMINGWAY
AS
I
STARTED
UP
THE
UNDERSHIRT
ONTO
HIS
CHEST
BLACK
AND
BIG
STOMACH
MUS
CLES
BULGING
UNDER
THE
LIGHT
YOU
SEE
THEM
BELOW
THE
LINE
WHERE
HI
RIBS
STOPPED
WERE
TWO
RAISED
WHITE
WELTS
SEE
ON
THE
FOREHEAD
OH
BRETT
I
LOVE
YOU
LET
NOT
TALK
TALKING
ALL
BILGE
I
M
GOING
AWAY
TOMORROW
TOMORROW
YES
DIDN
T
I
SAY
SO
I
AM
LET
HAVE
DRINK
THEN
WE
WERE
LUCKY
HERE
THAT
PUNCTUATION
CAME
OUT
CORRECTLY
THAT
NEED
NOT
HAPPEN
DATA
STRUCTURE
ALTERNATIVES
HOW
MUCH
INPUT
DO
WE
INTEND
TO
DEAL
WITH
HOW
FAST
MUST
THE
PROGRAM
SEEMS
REASONABLE
TO
ASK
OUR
PROGRAM
TO
READ
IN
A
WHOLE
BOOK
SO
WE
SHOUL
PARED
FOR
INPUT
SIZES
OF
N
WORDS
OR
MORE
THE
OUTPUT
WILL
BE
HUN
PERHAPS
THOUSANDS
OF
WORDS
AND
THE
PROGRAM
SHOULD
RUN
IN
A
FEW
SECONDS
MINUTES
WITH
WORDS
OF
INPUT
TEXT
N
IS
FAIRLY
LARGE
SO
THE
ALGORITHMS
TOO
SIMPLISTIC
IF
WE
WANT
THE
PROGRAM
TO
BE
FAST
THE
MARKOV
ALGORITHM
MUST
SEE
ALL
THE
INPUT
BEFORE
IT
CAN
BEGIN
TO
GENE
PUT
SO
IT
MUST
STORE
THE
ENTIRE
INPUT
IN
SOME
FORM
ONE
POSSIBILITY
IS
TO
WHOLE
INPUT
AND
STORE
IT
IN
A
LONG
STRING
BUT
WE
CLEARLY
WANT
THE
INPUT
BROK
INTO
WORDS
IF
WE
STORE
IT
AS
AN
ARRAY
OF
POINTERS
TO
WORDS
OUTPUT
GENERATION
I
TO
PRODUCE
EACH
WORD
SCAN
THE
INPUT
TEXT
TO
SEE
WHAT
POSSIBLE
SUFFIX
WORDS
F
PREFIX
THAT
WAS
JUST
EMITTED
AND
THEN
CHOOSE
ONE
AT
RANDOM
HOWEVER
TH
SCANNING
ALL
INPUT
WORDS
FOR
EACH
WORD
WE
GENERATE
WORDS
MEANS
HUNDREDS
OF
MILLIONS
OF
STRING
COMPARISONS
WHICH
WILL
NOT
BE
FAST
ANOTHER
POSSIBILITY
IS
TO
STORE
ONLY
UNIQUE
INPUT
WORDS
TOGETHER
WITH
WHERE
THEY
APPEAR
IN
THE
INPUT
SO
THAT
WE
CAN
LOCATE
SUCCESSOR
WORDS
MORE
WE
COULD
USE
A
HASH
TABLE
LIKE
THE
ONE
IN
CHAPTER
BUT
THAT
VERSION
DOESN
ADDRESS
THE
NEEDS
OF
THE
MARKOV
ALGORITHM
WHICH
MUST
QUICKLY
LOCATE
ALL
THE
OF
A
GIVEN
PREFIX
WE
NEED
A
DATA
STRUCTURE
THAT
BETTER
REPRESENTS
A
PREFIX
AND
ITS
ASSOCIATED
THE
PROGRAM
WILL
HAVE
TWO
PASSES
AN
INPUT
PASS
THAT
BUILDS
THE
DATA
STRUCTU
SENTING
THE
PHRASES
AND
AN
OUTPUT
PASS
THAT
USES
THE
DATA
STRUCTURE
TO
GENERATE
DOM
OUTPUT
IN
BOTH
PASSES
WE
NEED
TO
LOOK
UP
A
PREFIX
QUICKLY
IN
THE
INP
UPDATE
ITS
SUFFIXES
AND
IN
THE
OUTPUT
PASS
TO
SELECT
AT
RANDOM
FROM
THE
POSS
FIXES
THIS
SUGGESTS
A
HASH
TABLE
WHOSE
KEYS
ARE
PREFIXES
AND
WHOSE
VALUE
SETS
OF
SUFFIXES
FOR
THE
CORRESPONDING
PREFIXES
FOR
PURPOSES
OF
DESCRIPTION
WE
LL
ASSUME
A
TWO
WORD
PREFIX
SO
EACH
OUT
IS
BASED
ON
THE
PAIR
OF
WORDS
THAT
PRECEDE
IT
THE
NUMBER
OF
WORDS
IN
T
DOESN
T
AFFECT
THE
DESIGN
AND
THE
PROGRAMS
SHOULD
HANDLE
ANY
PREFIX
LENGTH
B
ING
A
NUMBER
MAKES
THE
DISCUSSION
CONCRETE
THE
PREFIX
AND
THE
SET
OF
ALL
ITS
SUFFIXES
WE
LL
CALL
A
STATE
WHICH
IS
STANDARD
TERMINOLOGY
FOR
MARKOV
ALGORITH
GIVEN
A
PREFIX
WE
NEED
TO
STORE
ALL
THE
SUFFIXES
THAT
FOLLOW
IT
SO
WE
CA
THEM
LATER
THE
SUFFIXES
ARE
UNORDERED
AND
ADDED
ONE
AT
A
TIME
WE
DON
T
K
MANY
THERE
WILL
BE
SO
WE
NEED
A
DATA
STRUCTURE
THAT
GROWS
EASILY
AND
EFFICIEN
AS
A
LIST
OR
A
DYNAMIC
ARRAY
WHEN
WE
ARE
GENERATING
OUTPUT
WE
NEED
TO
B
CHOOSE
ONE
SUFFIX
AT
RANDOM
FROM
THE
SET
OF
SUFFIXES
ASSOCIATED
WITH
A
PARTIC
FIX
ITEMS
ARE
NEVER
DELETED
WHAT
HAPPENS
IF
A
PHRASE
APPEARS
MORE
THAN
ONCE
FOR
EXAMPLE
MIGH
TWICE
MIGHT
APPEAR
TWICE
BUT
MIGHT
APPEAR
ONCE
ONLY
ONCE
THIS
COULD
SENTED
BY
PUTTING
TWICE
TWICE
IN
THE
SUFFIX
LIST
FOR
MIGHT
APPEAR
OR
BY
PUT
ONCE
WITH
AN
ASSOCIATED
COUNTER
SET
TO
WE
VE
TRIED
IT
WITH
AND
WITHOUT
WITHOUT
IS
EASIER
SINCE
ADDING
A
SUFFIX
DOESN
T
REQUIRE
CHECKING
WHETHER
IT
ALREADY
AND
EXPERIMENTS
SHOWED
THAT
THE
DIFFERENCE
IN
RUN
TIME
WAS
NEGLIGIBLE
IN
SUMMARY
EACH
STATE
COMPRISES
A
PREFIX
AND
A
LIST
OF
SUFFIXES
THIS
INFOR
IS
STORED
IN
A
HASH
TABLE
WITH
PREFIX
AS
KEY
EACH
PREFIX
IS
A
FIXED
SIZE
SET
OF
IF
A
SUFFIX
OCCURS
MORE
THAN
ONCE
FOR
A
GIVEN
PREFIX
EACH
OCCURRENCE
WILL
BE
IN
SEPARATELY
IN
THE
LIST
THE
NEXT
DECISION
IS
HOW
TO
REPRESENT
THE
WORDS
THEMSELVES
THE
EASY
WA
STORE
THEM
AS
INDIVIDUAL
STRINGS
SINCE
MOST
TEXT
HAS
MANY
WORDS
APPEARING
TIMES
IT
WOULD
PROBABLY
SAVE
STORAGE
IF
WE
KEPT
A
SECOND
HASH
TABLE
OF
SINGLE
SO
THE
TEXT
OF
EACH
WORD
WAS
STORED
ONLY
ONCE
THIS
WOULD
ALSO
SPEED
UP
HAS
PREFIXES
SINCE
WE
COULD
COMPARE
POINTERS
RATHER
THAN
INDIVIDUAL
CHARACTERS
STRINGS
HAVE
UNIQUE
ADDRESSES
WE
LL
LEAVE
THAT
DESIGN
AS
AN
EXERCISE
FOR
NOW
WILL
BE
STORED
INDIVIDUALLY
BUILDING
THE
DATA
STRUCTURE
IN
C
LET
BEGIN
WITH
A
C
IMPLEMENTATION
THE
FIRST
STEP
IS
TO
DEFINE
SOME
CONSTA
ENUM
NPREF
NHASH
MAXGEN
I
NUMBER
OF
PREFIX
WORDS
I
SIZE
OF
STATE
HASH
TABLE
ARRAY
I
MAXIMUM
WORDS
GENERATED
THIS
DECLARATION
DEFINES
THE
NUMBER
OF
WORDS
NPREF
FOR
THE
PREFIX
THE
SIZE
HASH
TABLE
ARRAY
NHASH
AND
AN
UPPER
LIMIT
ON
THE
NUMBER
OF
WORDS
TO
MAXGEN
IF
NPREF
IS
A
COMPILE
TIME
CONSTANT
RATHER
THAN
A
RUN
TIME
VARIABLE
MANAGEMENT
IS
SIMPLER
THE
ARRAY
SIZE
IS
SET
FAIRLY
LARGE
BECAUSE
WE
EXPECT
THE
PROGRAM
LARGE
INPUT
DOCUMENTS
PERHAPS
A
WHOLE
BOOK
WE
CHOSE
NHASH
SO
THAT
IF
THE
INPUT
HAS
DISTINCT
PREFIXES
WORD
PAIRS
THE
AVERAGE
CHAIN
VERY
SHORT
TWO
OR
THREE
PREFIXES
THE
LARGER
THE
SIZE
THE
SHORTER
THE
EXPECTED
OF
THE
CHAINS
AND
THUS
THE
FASTER
THE
LOOKUP
THIS
PROGRAM
IS
REALLY
A
TOY
SO
T
FORMANCE
ISN
T
CRITICAL
BUT
IF
WE
MAKE
THE
ARRAY
TOO
SMALL
THE
PROGRAM
WILL
NOT
OUR
EXPECTED
INPUT
IN
REASONABLE
TIME
ON
THE
OTHER
HAND
IF
WE
MAKE
IT
TOO
MIGHT
NOT
FIT
IN
THE
AVAILABLE
MEMORY
THE
PREFIX
CAN
BE
STORED
AS
AN
ARRAY
OF
WORDS
THE
ELEMENTS
OF
THE
HASH
TAB
BE
REPRESENTED
AS
A
STATE
DATA
TYPE
ASSOCIATING
THE
SUFFIX
LIST
WITH
THE
PREFIX
TYPEDEF
STRUCT
STATE
STATE
TYPEDEF
STRUCT
SUFFIX
SUFFIX
STRUCT
STATE
I
PREFIX
SUFFIX
LIST
I
CHAR
PREF
NPREF
PREFIX
WORDS
SUFFIX
SUF
I
LIST
OF
SUFFIXES
I
STATE
NEXT
I
NEXT
IN
HASH
TABLE
I
STRUCT
SUFFIX
F
LIST
OF
SUFFIXES
CHAR
WORD
F
SUFFIX
SUFFIX
NEXT
F
NEXT
IN
LIST
OF
SUFFIXES
STATE
STATETAB
NHASH
HASH
TABLE
OF
STATES
I
PICTORIALLY
THE
DATA
STRUCTURES
LOOK
LIKE
THIS
STATETAB
A
STATE
T
PREF
O
SHOW
PREF
L
YOUR
SUF
NEXT
ANOTHER
STATE
PREF
O
PREF
SUF
WORD
NEXT
FLOWCHARTS
ANOTHER
SUFFIX
WORD
NEXT
TA
WE
NEED
A
HASH
FUNCTION
FOR
PREFIXES
WHICH
ARE
ARRAYS
OF
STRINGS
IT
IS
MODIFY
THE
STRING
HASH
FUNCTION
FROM
CHAPTER
TO
LOOP
OVER
THE
STRINGS
IN
T
THUS
IN
EFFECT
HASHING
THE
CONCATENATION
OF
THE
STRINGS
HASH
COMPUTE
HASH
VALUE
FOR
ARRAY
OF
NPREF
STRINGS
UNSIGNED
INT
HASH
CHAR
NPREF
UNSIGNED
INT
H
UNSIGNED
CHAR
P
INT
I
H
O
FOR
I
O
I
NPREF
I
FOR
P
UNSIGNED
CHAR
I
P
O
P
H
MULTIPLIER
H
P
RETURN
H
NHASH
A
SIMILAR
MODIFICATION
TO
THE
LOOKUP
ROUTINE
COMPLETES
THE
IMPLEMENTATI
HASH
TABLE
LOOKUP
SEARCH
FOR
PREFIX
CREATE
IF
REQUESTED
I
I
RETURNS
POINTER
IF
PRESENT
OR
CREATED
NULL
IF
NOT
I
F
CREATION
DOESN
T
STRDUP
SO
STRINGS
MUSTN
T
CHANGE
LATE
STATE
LOOKUP
CHAR
PREFIX
NPREF
INT
CREATE
INT
I
H
STATE
SP
H
HASH
PREFIX
FOR
SP
STATETAB
H
SP
NULL
SP
SP
NEXT
FOR
I
O
I
NPREF
I
IF
STRCMP
PREFIX
I
SP
PREF
I
BREAK
IF
I
NPREF
FOUND
IT
I
RETURN
SP
IF
CREATE
SP
STATE
EMALLOC
SIZEOF
STATE
FOR
I
O
I
NPREF
I
SP
PREF
I
PREFIX
I
SP
SUF
NULL
SP
NEXT
STATETAB
H
STATETAB
H
SP
RETURN
SP
NOTICE
THAT
LOOKUP
DOESN
T
MAKE
A
COPY
OF
THE
INCOMING
STRINGS
WHEN
IT
CREATES
STATE
IT
JUST
STORES
POINTERS
IN
SP
PREF
CALLERS
OF
LOOKUP
MUST
GUARANTEE
T
DATA
WON
T
BE
OVERWRITTEN
LATER
FOR
EXAMPLE
IF
THE
STRINGS
ARE
IN
AN
BU
COPY
MUST
BE
MADE
BEFORE
LOOKUP
IS
CALLED
OTHERWISE
SUBSEQUENT
INPUT
COULD
WRITE
THE
DATA
THAT
THE
HASH
TABLE
POINTS
TO
DECISIONS
ABOUT
WHO
OWNS
A
RE
SHARED
ACROSS
AN
INTERFACE
ARISE
OFTEN
WE
WILL
EXPLORE
THIS
TOPIC
AT
LENGTH
IN
TH
CHAPTER
NEXT
WE
NEED
TO
BUILD
THE
HASH
TABLE
AS
THE
FILE
IS
READ
BUILD
READ
INPUT
BUILD
PREFIX
TABLE
I
VOID
BUILD
CHAR
PREFIX
NPREF
FILE
F
CHAR
BUF
LOO
FMT
LO
CREATE
A
FORMAT
STRING
COULD
OVERFLOW
BUF
I
SPRINTF
FMT
DS
SIZEOF
BUF
WHILE
FSCANF
F
FMT
BUF
EOF
ADD
PREFIX
ESTRDUP
BUF
THE
PECULIAR
CALL
TO
SPRI
NTF
GETS
AROUND
AN
IRRITATING
PROBLEM
WITH
FSCANF
IS
OTHERWISE
PERFECT
FOR
THE
JOB
A
CALL
TO
FSCANF
WITH
FORMAT
WILL
READ
TH
WHITE
SPACE
DELIMITED
WORD
FROM
THE
FILE
INTO
THE
BUFFER
BUT
THERE
IS
NO
LIMIT
O
A
LONG
WORD
MIGHT
OVERFLOW
THE
INPUT
BUFFER
WREAKING
HAVOC
IF
THE
BUFFER
BYTES
LONG
WHICH
IS
FAR
BEYOND
WHAT
WE
EXPECT
EVER
TO
APPEAR
IN
NORMAL
TEXT
USE
THE
FORMAT
LEAVING
ONE
BYTE
FOR
THE
TERMINAL
WHICH
TELLS
STOP
AFTER
BYTES
A
LONG
WORD
WILL
BE
BROKEN
INTO
PIECES
WHICH
IS
UNFORT
SAFE
WE
COULD
DECLARE
ENUM
BUFSIZE
CHAR
FMT
BUFSIZE
BUT
THAT
REQUIRES
TWO
CONSTANTS
FOR
ONE
ARBITRARY
DECISION
THE
SIZE
OF
THE
BU
INTRODUCES
THE
NEED
TO
MAINTAIN
THEIR
RELATIONSHIP
THE
PROBLEM
CAN
BE
SOL
AND
FOR
ALL
BY
CREATING
THE
FORMAT
STRING
DYNAMICALLY
WITH
SPRI
NTF
SO
APPROACH
WE
TAKE
THE
TWO
ARGUMENTS
TO
BUILD
ARE
THE
PREFIX
ARRAY
HOLDING
THE
PREVIO
WORDS
OF
INPUT
AND
A
FILE
POINTER
IT
PASSES
THE
PREFIX
AND
A
COPY
OF
THE
IN
TO
ADD
WHICH
ADDS
THE
NEW
ENTRY
TO
THE
HASH
TABLE
AND
ADVANCES
THE
PREFIX
I
ADD
ADD
WORD
TO
SUFFIX
LIST
UPDATE
PREFIX
I
VOID
ADD
CHAR
PREFIX
NPREF
CHAR
SUFFIX
STATE
SP
SP
LOOKUP
PREFIX
I
CREATE
IF
NOT
FOUND
I
ADDSUFFIX
SP
SUFFIX
MOVE
THE
WORDS
DOWN
THE
PREFIX
I
MEMMOVE
PREFIX
PREFIX
L
NPREF
L
SIZEOF
PREFIX
PREFIX
NPREF
SUFFIX
THE
CALL
TO
MEMMOVE
IS
THE
IDIOM
FOR
DELETING
FROM
AN
ARRAY
IT
SHIFTS
EL
THROUGH
NPREF
IN
THE
PREFIX
DOWN
TO
POSITIONS
O
THROUGH
NPREF
DELETING
PREFIX
WORD
AND
OPENING
A
SPACE
FOR
A
NEW
ONE
AT
THE
END
THE
ADDSUFFI
X
ROUTINE
ADDS
THE
NEW
SUFFIX
I
ADDSUFFIX
ADD
TO
STATE
SUFFIX
MUST
NOT
CHANGE
LATER
VOID
ADDSUFFIX
STATE
SP
CHAR
SUFFIX
SUFFIX
SUF
SUF
SUFFIX
EMALLOC
SIZEOF
SUFFIX
SUF
WORD
SUFFIX
SUF
NEXT
SP
SUF
SP
SUF
SUF
WE
SPLIT
THE
ACTION
OF
UPDATING
THE
STATE
INTO
TWO
FUNCTIONS
ADD
PERFORMS
TH
SERVICE
OF
ADDING
A
SUFFIX
TO
A
PREFIX
WHILE
ADDSUFFI
X
PERFORMS
THE
IMPLEM
SPECIFIC
ACTION
OF
ADDING
A
WORD
TO
A
SUFFIX
LIST
THE
ADD
ROUTINE
IS
USED
BY
B
ADDSUFFI
X
IS
USED
INTERNALLY
ONLY
BY
ADD
IT
IS
AN
IMPLEMENTATION
DETAIL
TH
CHANGE
AND
IT
SEEMS
BETTER
TO
HAVE
IT
IN
A
SEPARATE
FUNCTION
EVEN
THOUGH
IT
IS
ONLY
ONE
PLACE
GENERATING
OUTPUT
WITH
THE
DATA
STRUCTURE
BUILT
THE
NEXT
STEP
IS
TO
GENERATE
THE
OUTPUT
THE
BA
IS
AS
BEFORE
GIVEN
A
PREFIX
SELECT
ONE
OF
ITS
SUFFIXES
AT
RANDOM
PRINT
IT
THEN
THE
PREFIX
THIS
IS
THE
STEADY
STATE
OF
PROCESSING
WE
MUST
STILL
FIGURE
OUT
HOW
AND
STOP
THE
ALGORITHM
STARTING
IS
EASY
IF
WE
REMEMBER
THE
WORDS
OF
THE
FIRST
AND
BEGIN
WITH
THEM
STOPPING
IS
EASY
TOO
WE
NEED
A
MARKER
WORD
TO
TERMIN
ALGORITHM
AFTER
ALL
THE
REGULAR
INPUT
WE
CAN
ADD
A
TERMINATOR
A
WORD
THAT
I
ANTEED
NOT
TO
APPEAR
IN
ANY
INPUT
BUILD
PREFIX
STDIN
ADD
PREFIX
SHOULD
BE
SOME
VALUE
THAT
WILL
NEVER
BE
ENCOUNTERED
IN
REGULAR
INPUT
THE
INPUT
WORDS
ARE
DELIMITED
BY
WHITE
SPACE
A
WORD
OF
WHITE
SPACE
WILL
SUCH
AS
A
NEWLINE
CHARACTER
CHAR
N
CANNOT
APPEAR
AS
REAL
WORD
I
ONE
MORE
WORRY
WHAT
HAPPENS
IF
THERE
IS
INSUFFICIENT
INPUT
TO
START
THE
ALGO
THERE
ARE
TWO
APPROACHES
TO
THIS
SORT
OF
PROBLEM
EITHER
EXIT
PREMATURELY
IF
INSUFFICIENT
INPUT
OR
ARRANGE
THAT
THERE
IS
ALWAYS
ENOUGH
AND
DON
T
BOTHER
TO
IN
THIS
PROGRAM
THE
LATTER
APPROACH
WORKS
WELL
WE
CAN
INITIALIZE
BUILDING
AND
GENERATING
WITH
A
FABRICATED
PREFIX
WHICH
TEES
THERE
IS
ALWAYS
ENOUGH
INPUT
FOR
THE
PROGRAM
TO
PRIME
THE
LOOPS
INITIAL
PREFIX
ARRAY
TO
BE
ALL
WORDS
THIS
HAS
THE
NICE
BENEFIT
THAT
THE
FIRST
THE
INPUT
FILE
WILL
BE
THE
FIRST
SUFFIX
OF
THE
FAKE
PREFIX
SO
THE
GENERATION
LOOP
N
PRINT
ONLY
THE
SUFFIXES
IT
PRODUCES
IN
CASE
THE
OUTPUT
IS
UNMANAGEABLY
LONG
WE
CAN
TERMINATE
THE
ALGORITH
SOME
NUMBER
OF
WORDS
ARE
PRODUCED
OR
WHEN
WE
HIT
AS
A
SUFFIX
WHI
COMES
FIRST
ADDING
A
FEW
TO
THE
ENDS
OF
THE
DATA
SIMPLIFIES
THE
MAIN
PRO
LOOPS
OF
THE
PROGRAM
SIGNIFICANTLY
IT
IS
AN
EXAMPLE
OF
THE
TECHNIQUE
OF
ADDING
VALUES
TO
MARK
BOUNDARIES
AS
A
RULE
TRY
TO
HANDLE
IRREGULARITIES
AND
EXCEPTIONS
AND
SPECIAL
CASES
I
CODE
IS
HARDER
TO
GET
RIGHT
SO
THE
CONTROL
FLOW
SHOULD
BE
AS
SIMPLE
AND
REGULAR
SIBLE
THE
GENERATE
FUNCTION
USES
THE
ALGORITHM
WE
SKETCHED
ORIGINALLY
IT
PR
ONE
WORD
PER
LINE
OF
OUTPUT
WHICH
CAN
BE
GROUPED
INTO
LONGER
LINES
WITH
A
WO
CESSOR
CHAPTER
SHOWS
A
SIMPLE
FORMATTER
CALLED
FRNT
FOR
THIS
TASK
WITH
THE
USE
OF
THE
INITIAL
AND
FINAL
STRINGS
GENERATE
STARTS
AN
PROPERLY
GENERATE
PRODUCE
OUTPUT
ONE
WORD
PER
LINE
VOID
GENERATE
INT
NWORDS
STATE
SP
SUFFIX
SUF
CHAR
PREFIX
NPREF
W
INTI
NMATCH
FOR
I
O
I
NPREF
I
RESET
INITIAL
PREFIX
PREFIX
I
NONWORD
FOR
I
O
I
NWORDS
I
SP
LOOKUP
PREFIX
O
NMATCH
O
FOR
SUF
SP
SUF
SUF
NULL
SUF
SUF
NEXT
IF
RAND
NMATCH
PROB
NMAT
W
SUF
WORD
IF
STRCMP
W
NONWORD
O
BREAK
PRI
NTF
N
W
MEMMOVE
PREFIX
PREFIX
L
NPREF
L
SIZEOF
PREFI
PREFIX
NPREF
W
NOTICE
THE
ALGORITHM
FOR
SELECTING
ONE
ITEM
AT
RANDOM
WHEN
WE
DON
T
KNOW
H
ITEMS
THERE
ARE
THE
VARIABLE
NMATCH
COUNTS
THE
NUMBER
OF
MATCHES
AS
T
SCANNED
THE
EXPRESSION
RAND
NMATCH
INCREMENTS
NMATCH
AND
IS
THEN
TRUE
WITH
PROBABILITY
NMATCH
THUS
THE
FIRS
ING
ITEM
IS
SELECTED
WITH
PROBABILITY
THE
SECOND
WILL
REPLACE
IT
WITH
PROBAB
THE
THIRD
WILL
REPLACE
THE
SURVIVOR
WITH
PROBABILITY
AND
SO
ON
AT
ANY
TI
ONE
OF
THE
K
MATCHING
ITEMS
SEEN
SO
FAR
HAS
BEEN
SELECTED
WITH
PROBABILITY
K
AT
THE
BEGINNING
WE
SET
THE
PREFIX
TO
THE
STARTING
VALUE
WHICH
IS
GUAR
BE
INSTALLED
IN
THE
HASH
TABLE
THE
FIRST
SUFFIX
VALUES
WE
FIND
WILL
BE
THE
FIR
OF
THE
DOCUMENT
SINCE
THEY
ARE
THE
UNIQUE
FOLLOW
ON
TO
THE
STARTING
PREFIX
A
RANDOM
SUFFIXES
WILL
BE
CHOSEN
THE
LOOP
CALLS
LOOKUP
TO
FIND
THE
HASH
TABLE
THE
CURRENT
PREFIX
THEN
CHOOSES
A
RANDOM
SUFFIX
PRINTS
IT
AND
ADVANCES
THE
IF
THE
SUFFIX
WE
CHOOSE
IS
NONWORD
WE
RE
DONE
BECAUSE
WE
HAVE
CHOSEN
THAT
CORRESPONDS
TO
THE
END
OF
THE
INPUT
IF
THE
SUFFIX
IS
NOT
NONWORD
WE
PRIN
DROP
THE
FIRST
WORD
OF
THE
PREFIX
WITH
A
CALL
TO
MEMMOVE
PROMOTE
THE
SUFFIX
LAST
WORD
OF
THE
PREFIX
AND
LOOP
NOW
WE
CAN
PUT
ALL
THIS
TOGETHER
INTO
AMAIN
ROUTINE
THAT
READS
THE
STAND
AND
GENERATES
AT
MOST
A
SPECIFIED
NUMBER
OF
WORDS
MARKOV
MAIN
MARKOV
CHAIN
RANDOM
TEXT
GENERATION
INT
MAIN
VOID
INTI
NWORDS
MAXGEN
CHAR
PREFIX
NPREF
CURRENT
INPUT
PREFIX
FOR
I
I
NPREF
I
SETUP
INITIAL
PREFIX
PREFIX
I
BUILD
PREFIX
STDIN
ADD
PREFIX
GENERATE
NWORDS
RETURN
THIS
COMPLETES
OUR
C
IMPLEMENTATION
WE
WILL
RETURN
AT
THE
END
OF
THE
CHA
A
COMPARISON
OF
PROGRAMS
IN
DIFFERENT
LANGUAGES
THE
GREAT
STRENGTHS
OF
C
ARE
GIVES
THE
PROGRAMMER
COMPLETE
CONTROL
OVER
IMPLEMENTATION
AND
PROGRAMS
WR
IT
TEND
TO
BE
FAST
THE
COST
HOWEVER
IS
THAT
THE
C
PROGRAMMER
MUST
DO
MORE
WORK
ALLOCATING
AND
RECLAIMING
MEMORY
CREATING
HASH
TABLES
AND
LINKED
LISTS
LIKE
C
IS
A
RAZOR
SHARP
TOOL
WITH
WHICH
ONE
CAN
CREATE
AN
ELEGANT
AND
EFFICIE
GRAM
OR
A
BLOODY
MESS
EXERCISE
THE
ALGORITHM
FOR
SELECTING
A
RANDOM
ITEM
FROM
A
LIST
OF
UN
LENGTH
DEPENDS
ON
HAVING
A
GOOD
RANDOM
NUMBER
GENERATOR
DESIGN
AND
CA
EXPERIMENTS
TO
DETERMINE
HOW
WELL
THE
METHOD
WORKS
IN
PRACTICE
D
EXERCISE
IF
EACH
INPUT
WORD
IS
STORED
IN
A
SECOND
HASH
TABLE
THE
TEXT
I
STORED
ONCE
WHICH
SHOULD
SAVE
SPACE
MEASURE
SOME
DOCUMENTS
TO
ESTIMAT
MUCH
THIS
ORGANIZATION
WOULD
ALLOW
US
TO
COMPARE
POINTERS
RATHER
THAN
STRINGS
HASH
CHAINS
FOR
PREFIXES
WHICH
SHOULD
RUN
FASTER
IMPLEMENT
THIS
VERSION
AN
SURE
THE
CHANGE
IN
SPEED
AND
MEMORY
CONSUMPTION
D
EXERCISE
REMOVE
THE
STATEMENTS
THAT
PLACE
SENTINEL
AT
THE
BEG
AND
END
OF
THE
DATA
AND
MODIFY
GENERATE
SO
IT
STARTS
AND
STOPS
PROPERLY
THEM
MAKE
SURE
IT
PRODUCES
CORRECT
OUTPUT
FOR
INPUT
WITH
AND
COMPARE
THIS
IMPLEMENTATION
TO
THE
VERSION
USING
SENTINELS
D
JAVA
OUR
SECOND
IMPLEMENTATION
OF
THE
MARKOV
CHAIN
ALGORITHM
IS
IN
JAVA
ORIENTED
LANGUAGES
LIKE
JAVA
ENCOURAGE
ONE
TO
PAY
PARTICULAR
ATTENTION
TO
THE
INT
BETWEEN
THE
COMPONENTS
OF
THE
PROGRAM
WHICH
ARE
THEN
ENCAPSULATED
AS
INDEP
DATA
ITEMS
CALLED
OBJECTS
OR
CLASSES
WITH
ASSOCIATED
FUNCTIONS
CALLED
METHODS
JAVA
HAS
A
RICHER
LIBRARY
THAN
C
INCLUDING
A
SET
OF
CONTAINER
CLASSES
TO
GROUP
ING
OBJECTS
IN
VARIOUS
WAYS
ONE
EXAMPLE
IS
A
VECTOR
THAT
PROVIDES
A
DYNAM
GROWABLE
ARRAY
THAT
CAN
STORE
ANY
OBJECT
TYPE
ANOTHER
EXAMPLE
IS
THE
HAS
CLASS
WITH
WHICH
ONE
CAN
STORE
AND
RETRIEVE
VALUES
OF
ONE
TYPE
USING
ANOTHER
TYPE
AS
KEYS
IN
OUR
APPLICATION
VECTORS
OF
STRINGS
ARE
THE
NATURAL
CHOICE
TO
HOLD
PRE
SUFFIXES
WE
CAN
USE
A
HASHTABLE
WHOSE
KEYS
ARE
PREFIX
VECTORS
AND
WHO
ARE
SUFFIX
VECTORS
THE
TERMINOLOGY
FOR
THIS
TYPE
OF
CONSTRUCTION
IS
A
MAP
FIXES
TO
SUFFIXES
IN
JAVA
WE
NEED
NO
EXPLICIT
STATE
TYPE
BECAUSE
HASHTABL
ITLY
CONNECTS
MAPS
PREFIXES
TO
SUFFIXES
THIS
DESIGN
IS
DIFFERENT
FROM
THE
C
IN
WHICH
WE
INSTALLED
STATE
STRUCTURES
THAT
HELD
BOTH
PREFIX
AND
SUFFIX
LIST
AN
ON
THE
PREFIX
TO
RECOVER
THE
FULL
STATE
A
HASHTABLE
PROVIDES
A
PUT
METHOD
TO
STORE
A
KEY
VALUE
PAIR
AND
A
GET
TO
RETRIEVE
THE
VALUE
FOR
A
KEY
HASHTABLE
H
NEW
HASHTABLE
H
PUT
KEY
VALUE
SOMETYPE
V
SOMETYPE
H
GET
KEY
OUR
IMPLEMENTATION
HAS
THREE
CLASSES
THE
FIRST
CLASS
PREFIX
HOLDS
THE
THE
PREFIX
CLASS
PREFIX
PUBLIC
VECTOR
PREF
NPREF
ADJACENT
WORDS
FROM
INP
THE
SECOND
CLASS
CHAIN
READS
THE
INPUT
BUILDS
THE
HASH
TABLE
AND
GENE
OUTPUT
HERE
ARE
ITS
CLASS
VARIABLES
CLASS
CHAIN
STATIC
FINAL
INT
NPREF
SIZE
OF
PREFIX
STATIC
FINAL
STRING
N
WORD
THAT
CAN
T
APPEAR
HASHTABLE
STATETAB
NEW
HASHTABLE
KEY
PREFIX
VALUE
SUFFIX
VECT
PREFIX
PREFIX
NEW
PREFIX
NPREF
INITIAL
PREFIX
RANDOM
RAND
NEW
RANDOM
THE
THIRD
CLASS
IS
THE
PUBLIC
INTERFACE
IT
HOLDS
MAIN
AND
INSTANTIATES
A
CHA
CLASS
MARKOV
STATIC
FINAL
INT
MAXGEN
MAXIMUM
WORDS
GE
PUBLIC
STATIC
VOID
MAIN
STRING
ARGS
THROWS
IOEXCE
CHAIN
CHAIN
NEW
CHAIN
INT
NWORDS
MAXGEN
CHAIN
BUILD
SYSTEM
IN
CHAIN
GENERATE
NWORDS
WHEN
AN
INSTANCE
OF
CLASS
CHAIN
IS
CREATED
IT
IN
TUM
CREATES
A
HASH
TABLE
A
UP
THE
INITIAL
PREFIX
OF
NPREF
NONWORDS
THE
BUILD
FUNCTION
USES
THE
LIBRARY
ST
REAMTOKENI
ZE
R
TO
PARSE
THE
INPUT
INTO
WORDS
SEPARATED
BY
WHITE
SPACE
CHAR
THE
THREE
CALLS
BEFORE
THE
LOOP
SET
THE
TOKENIZER
INTO
THE
PROPER
STATE
FOR
OUR
DEF
OF
WORD
II
CHAIN
BUILD
BUILD
STATE
TABLE
FROM
INPUT
STREAM
VOID
BUILD
INPUTSTREAM
IN
THROWS
IOEXCEPTION
STREAMTOKENIZER
ST
NEW
STREAMTOKENIZER
IN
ST
RESETSYNTAX
II
REMOVE
DEFAUL
ST
WORDCHARS
O
CHARACTER
II
TURN
ON
ALL
ST
WHITESPACECHARS
O
WHILE
ST
NEXTTOKEN
ST
ADD
ST
SVAL
ADD
NONWORD
II
EXCEPT
UP
TO
THE
ADD
FUNCTION
RETRIEVES
THE
VECTOR
OF
SUFFIXES
FOR
THE
CURRENT
PREFIX
FR
HASH
TABLE
IF
THERE
ARE
NONE
THE
VECTOR
IS
NULL
ADD
CREATES
A
NEW
VECTOR
AND
PREFIX
TO
STORE
IN
THE
HASH
TABLE
IN
EITHER
CASE
IT
ADDS
THE
NEW
WORD
TO
THE
SUFF
TOR
AND
ADVANCES
THE
PREFIX
BY
DROPPING
THE
FIRST
WORD
AND
ADDING
THE
NEW
WOR
END
II
CHAIN
ADD
ADD
WORD
TO
SUFFIX
LIST
UPDATE
PREFIX
VOID
ADD
STRING
WORD
VECTOR
SUF
VECTOR
STATETAB
GET
PREFIX
IF
SUF
NULL
SUF
NEW
VECTOR
STATETAB
PUT
NEW
PREFIX
PREFIX
SUF
SUF
ADDELEMENT
WORD
PREFIX
PREF
REMOVEELEMENTAT
O
PREFIX
PREF
ADDELEMENT
WORD
NOTICE
THAT
IF
SUF
IS
NULL
ADD
INSTALLS
A
NEW
PREFIX
IN
THE
HASH
TABLE
RATH
PREFIX
ITSELF
THIS
IS
BECAUSE
THE
HASHTABLE
CLASS
STORES
ITEMS
BY
REFERENCE
WE
DON
T
MAKE
A
COPY
WE
COULD
OVERWRITE
DATA
IN
THE
TABLE
THIS
IS
THE
SAM
THAT
WE
HAD
TO
DEAL
WITH
IN
THE
C
PROGRAM
THE
GENERATION
FUNCTION
IS
SIMILAR
TO
THE
C
VERSION
BUT
SLIGHTLY
MORE
CO
BECAUSE
IT
CAN
INDEX
A
RANDOM
VECTOR
ELEMENT
DIRECTLY
INSTEAD
OF
LOOPING
THR
LIST
II
CHAIN
GENERATE
GENERATE
OUTPUT
WORDS
VOID
GENERATE
INT
NWORDS
PREFIX
NEW
PREFIX
NPREF
NONWORD
FOR
INTI
O
I
NWORDS
I
VECTORS
VECTOR
STATETAB
GET
PREFIX
INT
R
MATH
ABS
RAND
NEXTINT
SIZE
STRING
SUF
STRING
ELEMENTAT
R
IF
SUF
EQUALS
NONWORD
BREAK
SYSTEM
OUT
PRINTLN
SUF
PREFIX
PREF
REMOVEELEMENTAT
O
PREFIX
PREF
ADDELEMENT
SUF
THE
TWO
CONSTRUCTORS
OF
PREFIX
CREATE
NEW
INSTANCES
FROM
SUPPLIED
DATA
COPIES
AN
EXISTING
PREFIX
AND
THE
SECOND
CREATES
A
PREFIX
FROM
N
COPIES
OF
WE
USE
IT
TO
MAKE
NPREF
COPIES
OF
NONWORD
WHEN
INITIALIZING
II
PREFIX
CONSTRUCTOR
DUPLICATE
EXISTING
PREFIX
PREFIX
PREFIX
P
PREF
VECTOR
P
PREF
CLONE
II
PREFIX
CONSTRUCTOR
N
COPIES
OF
STR
PREFIX
INT
N
STRING
STR
PREF
NEW
VECTOR
FOR
INTI
O
I
N
I
PREF
ADDELEMENT
STR
PREFIX
ALSO
HAS
TWO
METHODS
HASHCODE
AND
EQUALS
THAT
ARE
CALLED
IMP
THE
IMPLEMENTATION
OF
HASHTABL
E
TO
INDEX
AND
SEARCH
THE
TABLE
IT
IS
THE
NEE
AN
EXPLICIT
CLASS
FOR
THESE
TWO
METHODS
FOR
HASHTABL
E
THAT
FORCED
US
TO
MAK
A
FULL
FLEDGED
CLASS
RATHER
THAN
JUST
A
VECTOR
LIKE
THE
SUFFIX
THE
HASHCODE
METHOD
BUILDS
A
SINGLE
HASH
VALUE
BY
COMBINING
TH
HASHCODES
FOR
THE
ELEMENTS
OF
THE
VECTOR
STATIC
FINAL
INT
MULTIPLIER
II
FOR
HASHCODE
II
PREFIX
HASHCODE
GENERATE
HASH
FROM
ALL
PREFIX
WORDS
PUBLIC
INT
HASHCODE
INT
H
O
FOR
INTI
O
I
PREF
SIZE
I
H
MULTIPLIER
H
PREF
ELEMENTAT
I
HASHCODE
RETURN
H
AND
EQUALS
DOES
AN
ELEMENTWISE
COMPARISON
OF
THE
WORDS
IN
TWO
PREFIXES
PREFIX
EQUALS
COMPARE
TWO
PREFIXES
FOR
EQUAL
WORDS
PUBLIC
BOOLEAN
EQUALS
OBJECT
O
PREFIX
P
PREFIX
O
FOR
INTI
O
I
PREF
SIZE
I
IF
PREF
ELEMENTAT
I
EQUALS
P
PREF
ELEMENTAT
I
RETURN
FALSE
RETURN
TRUE
THE
JAVA
PROGRAM
IS
SIGNIFICANTLY
SMALLER
THAN
THE
C
PROGRAM
AND
TAKES
MORE
DETAILS
VECTORS
AND
THE
HASHTABL
E
ARE
THE
OBVIOUS
EXAMPLES
IN
GENERA
AGE
MANAGEMENT
IS
EASY
SINCE
VECTORS
GROW
AS
NEEDED
AND
GARBAGE
COLLECTION
CARE
OF
RECLAIMING
MEMORY
THAT
IS
NO
LONGER
REFERENCED
BUT
TO
USE
THE
HAS
CLASS
WE
STILL
NEED
TO
WRITE
FUNCTIONS
HASHCODE
AND
EQUALS
SO
JAVA
ISN
T
TAKI
OF
ALL
THE
DETAILS
COMPARING
THE
WAY
THE
C
AND
JAVA
PROGRAMS
REPRESENT
AND
OPERATE
ON
TH
BASIC
DATA
STRUCTURE
WE
SEE
THAT
THE
JAVA
VERSION
HAS
BETTER
SEPARATION
OF
FUNCTIO
FOR
EXAMPLE
TO
SWITCH
FROM
VECTORS
TO
ARRAYS
WOULD
BE
EASY
IN
THE
C
EVERYTHING
KNOWS
WHAT
EVERYTHING
ELSE
IS
DOING
THE
HASH
TABLE
OPERATES
ON
ARRA
ARE
MAINTAINED
IN
VARIOUS
PLACES
LOOKUP
KNOWS
THE
LAYOUT
OF
THE
STATE
AND
STRUCTURES
AND
EVERYONE
KNOWS
THE
SIZE
OF
THE
PREFIX
ARRAY
JAVA
MARKOV
TXT
I
FMT
WASH
THE
BLACKBOARD
WATCH
IT
DRY
THE
WATER
GOES
INTO
THE
AIR
WHEN
WATER
GOES
INTO
THE
AIR
IT
EVAPORATES
TIE
A
DAMP
CLOTH
TO
ONE
END
OF
A
SOLID
OR
LIQUID
LOOK
AROUND
WHAT
ARE
THE
SOLID
THINGS
CHEMICAL
CHANGES
TAKE
PLACE
WHEN
SOMETHING
BURNS
IF
THE
BURNING
MATERIAL
HAS
LIQUIDS
THEY
ARE
STABLE
AND
THE
SPONGE
RISE
IT
LOOKED
LIKE
DOUGH
BUT
IT
IS
BURNING
BREAK
UP
THE
LUMP
OF
SUGAR
INTO
SMALL
PIECES
AND
PUT
THEM
TOGETHER
AGAIN
IN
THE
BOTTOM
OF
A
LIQUID
EXERCISE
REVISE
THE
JAVA
VERSION
OF
MARKOV
TO
USE
AN
ARRAY
INSTEAD
OF
A
FOR
THE
PREFIX
IN
THE
STATE
CLASS
D
C
OUR
THIRD
IMPLEMENTATION
IS
IN
C
SINCE
C
IS
ALMOST
A
SUPERSET
OF
BE
USED
AS
IF
IT
WERE
C
WITH
A
FEW
NOTATIONAL
CONVENIENCES
AND
OUR
ORIGINAL
C
OF
MARKOV
IS
ALSO
A
LEGAL
C
PROGRAM
A
MORE
APPROPRIATE
USE
OF
C
WOULD
BE
TO
DEFINE
CLASSES
FOR
THE
OBJECTS
IN
THE
PROGRAM
MORE
OR
LESS
AS
W
JAVA
THIS
WOULD
LET
US
HIDE
IMPLEMENTATION
DETAILS
WE
DECIDED
TO
GO
EVEN
F
USING
THE
STANDARD
TEMPLATE
LIBRARY
OR
STL
SINCE
THE
STL
HAS
BUILT
IN
MEC
THAT
WILL
DO
MUCH
OF
WHAT
WE
NEED
THE
ISO
STANDARD
FOR
C
INCLUDES
TH
PART
OF
THE
LANGUAGE
DEFINITION
THE
STL
PROVIDES
CONTAINERS
SUCH
AS
VECTORS
LISTS
AND
SETS
AND
A
FAMILY
O
MENTAL
ALGORITHMS
FOR
SEARCHING
SORTING
INSERTING
AND
DELETING
USING
THE
FEATURES
OF
C
EVERY
STL
ALGORITHM
WORKS
ON
A
VARIETY
OF
CONTAINERS
INCLUD
USER
DEFINED
TYPES
AND
BUILT
IN
TYPES
LIKE
INTEGERS
CONTAINERS
ARE
EXPRESSED
TEMPLATES
THAT
ARE
INSTANTIATED
FOR
SPECIFIC
DATA
TYPES
FOR
EXAMPLE
THERE
IS
A
CONTAINER
THAT
CAN
BE
USED
TO
MAKE
PARTICULAR
TYPES
LIKE
VECTOR
VECTOR
STRI
NG
ALL
VECTOR
OPERATIONS
INCLUDING
STANDARD
ALGORITHMS
FOR
CAN
BE
USED
ON
SUCH
DATA
TYPES
IN
ADDITION
TO
A
VECTOR
CONTAINER
THAT
IS
SIMILAR
TO
JAVA
VECTOR
THE
VIDES
A
DEQUE
CONTAINER
A
DEQUE
PRONOUNCED
DECK
IS
A
DOUBLE
ENDED
Q
MATCHES
WHAT
WE
DO
WITH
PREFIXES
IT
HOLDS
NPREF
ELEMENTS
AND
LETS
US
POP
ELEMENT
AND
ADD
A
NEW
ONE
TO
THE
END
IN
TIME
FOR
BOTH
THE
STL
DEQUE
GENERAL
THAN
WE
NEED
SINCE
IT
PERMITS
PUSH
AND
POP
AT
EITHER
END
BUT
THE
PERF
GUARANTEES
MAKE
IT
AN
OBVIOUS
CHOICE
THE
STL
ALSO
PROVIDES
AN
EXPLICIT
MAP
CONTAINER
BASED
ON
BALANCED
TR
STORES
KEY
VALUE
PAIRS
AND
PROVIDES
LOGN
RETRIEVAL
OF
THE
VALUE
ASSOCIATED
KEY
MAPS
MIGHT
NOT
BE
AS
EFFICIENT
AS
HASH
TABLES
BUT
IT
NICE
NOT
TO
WRITE
ANY
CODE
WHATSOEVER
TO
USE
THEM
SOME
NON
STANDARD
C
LIBRARIES
HASH
OR
CONTAINER
WHOSE
PERFORMANCE
MAY
BE
BETTER
WE
ALSO
USE
THE
BUILT
IN
COMPARISON
FUNCTIONS
WHICH
IN
THIS
CASE
WILL
COMPARISONS
USING
THE
INDIVIDUAL
STRINGS
IN
THE
PREFIX
WITH
THESE
COMPONENTS
IN
HAND
THE
CODE
GOES
TOGETHER
SMOOTHLY
HER
DECLARATIONS
TYPEDEF
DEQUE
STRING
PREFIX
MAP
PREFIX
VECTOR
STRING
STATETAB
PREFIX
SUFFI
THE
STL
PROVIDES
A
TEMPLATE
FOR
DEQUES
THE
NOTATION
DEQUE
STRI
NG
SPECIAL
A
DEQUE
WHOSE
ELEMENTS
ARE
STRINGS
SINCE
THIS
TYPE
APPEARS
SEVERAL
TIMES
IN
GRAM
WE
USED
A
TYPEDEF
TO
GIVE
IT
THE
NAME
PREFIX
THE
MAP
TYPE
THAT
ST
FIXES
AND
SUFFIXES
OCCURS
ONLY
ONCE
HOWEVER
SO
WE
DID
NOT
GIVE
IT
A
SEPARA
THE
MAP
DECLARATION
DECLARES
A
VARIABLE
STATETAB
THAT
IS
A
MAP
FROM
PREFIXE
TORS
OF
STRINGS
THIS
IS
MORE
CONVENIENT
THAN
EITHER
C
OR
JAVA
BECAUSE
WE
DO
TO
PROVIDE
A
HASH
FUNCTION
OR
EQUALS
METHOD
THE
MAIN
ROUTINE
INITIALIZES
THE
PREFIX
READS
THE
INPUT
FROM
STANDARD
INPUT
CIN
IN
THE
C
IOSTREAM
LIBRARY
ADDS
A
TAIL
AND
GENERATES
THE
OUTPUT
EXACTL
THE
EARLIER
VERSIONS
MARKOV
MAIN
MARKOV
CHAIN
RANDOM
TEXT
GENERATION
INT
MAIN
VOID
INT
NWORDS
MAXGEN
PREFIX
PREFIX
CURRENT
INPUT
PREFI
FOR
INTI
O
I
NPREF
I
SETUP
INITIAL
PREF
ADD
PREFIX
NONWORD
BUILD
PREFIX
CIN
ADD
PREFIX
NONWORD
GENERATE
NWORDS
RETURN
O
THE
FUNCTION
BUILD
USES
THE
I
OSTREAM
LIBRARY
TO
READ
THE
INPUT
ONE
WO
TIME
BUILD
READ
INPUT
WORDS
BUILD
STATE
TABLE
VOID
BUILD
PREFIX
PREFIX
ISTREAM
IN
STRING
BUF
WHILE
IN
BUF
ADD
PREFIX
BUF
THE
STRING
BUF
WILL
GROW
AS
NECESSARY
TO
HANDLE
INPUT
WORDS
OF
ARBITRARY
LENGTH
THE
ADD
FUNCTION
SHOWS
MORE
OF
THE
ADVANTAGES
OF
USING
THE
STL
ADD
ADD
WORD
TO
SUFFIX
LIST
UPDATE
PREFIX
VOID
ADD
PREFIX
PREFIX
CANST
STRING
IF
PREFIX
SIZE
NPREF
STATETAB
PREFIX
PREFIX
PREFIX
QUITE
A
BIT
IS
GOING
ON
UNDER
THESE
APPARENTLY
SIMPLE
STATEMENTS
THE
MAP
CO
OVERLOADS
SUBSCRIPTING
THE
OPERATOR
TO
BEHAVE
AS
A
LOOKUP
OPERATION
THE
SION
STATETAB
PREFIX
DOES
A
LOOKUP
IN
STATETAB
WITH
PREFIX
AS
KEY
AND
RE
REFERENCE
TO
THE
DESIRED
ENTRY
THE
VECTOR
IS
CREATED
IF
IT
DOES
NOT
EXIST
ALREAD
MEMBER
FUNCTIONS
OF
VECTOR
AND
DEQUE
PUSH
A
NEW
STRING
ONTO
TH
END
OF
THE
VECTOR
OR
DEQUE
POPS
THE
FIRST
ELEMENT
OFF
THE
DEQUE
GENERATION
IS
SIMILAR
TO
THE
PREVIOUS
VERSIONS
II
GENERATE
PRODUCE
OUTPUT
ONE
WORD
PER
LINE
VOID
GENERATECINT
NWORDS
PREFIX
PREFIX
INT
I
FOR
CI
O
I
NPREF
I
II
RESET
INITIAL
PREFIX
ADDCPREFIX
NONWORD
FOR
CI
O
I
NWORDS
I
VECTOR
STRING
SUF
STATETAB
PREFIX
CANST
STRING
W
SUF
RANDC
SUF
SIZEC
J
IF
CW
NONWORD
BREAK
COUT
W
N
PREFIX
PREFIX
II
ADVANCE
OVERALL
THIS
VERSION
SEEMS
ESPECIALLY
CLEAR
AND
ELEGANT
THE
CODE
IS
COM
DATA
STRUCTURE
IS
VISIBLE
AND
THE
ALGORITHM
IS
COMPLETELY
TRANSPARENT
SADLY
PRICE
TO
PAY
THIS
VERSION
RUNS
MUCH
SLOWER
THAN
THE
ORIGINAL
C
VERSION
THO
NOT
THE
SLOWEST
WE
LL
COME
BACK
TO
PERFORMANCE
MEASUREMENTS
SHORTLY
EXERCISE
THE
GREAT
STRENGTH
OF
THE
STL
IS
THE
EASE
WITH
WHICH
ONE
CA
MENT
WITH
DIFFERENT
DATA
STRUCTURES
MODIFY
THE
C
VERSION
OF
MARKOV
TO
US
STRUCTURES
TO
REPRESENT
THE
PREFIX
SUFFIX
LIST
AND
STATE
TABLE
HOW
DOES
PERF
CHANGE
FOR
THE
DIFFERENT
STRUCTURES
D
EXERCISE
WRITE
A
C
VERSION
THAT
USES
ONLY
CLASSES
AND
THE
STRING
BUT
NO
OTHER
ADVANCED
LIBRARY
FACILITIES
COMPARE
IT
IN
STYLE
AND
SPEED
TO
THE
SIONS
O
AWK
AND
PERL
TO
ROUND
OUT
THE
EXERCISE
WE
ALSO
WROTE
THE
PROGRAM
IN
TWO
POPULAR
SCRIP
GUAGES
AWK
AND
PERL
THESE
PROVIDE
THE
NECESSARY
FEATURES
FOR
THIS
APPLICATI
CIATIVE
ARRAYS
AND
STRING
HANDLING
AN
ASSOCIATIVE
ARRAY
IS
A
CONVENIENT
PACKAGING
OF
A
HASH
TABLE
IT
LOOK
ARRAY
BUT
ITS
SUBSCRIPTS
ARE
ARBITRARY
STRINGS
OR
NUMBERS
OR
COMMA
SEPARATE
THEM
IT
IS
A
FORM
OF
MAP
FROM
ONE
DATA
TYPE
TO
ANOTHER
IN
AWK
ALL
ARRAYS
CIATIVE
PERL
HAS
BOTH
CONVENTIONAL
INDEXED
ARRAYS
WITH
INTEGER
SUBSCRIPTS
AND
TIVE
ARRAYS
WHICH
ARE
CALLED
HASHES
A
NAME
THAT
SUGGESTS
HOW
THEY
AR
MENTED
THE
AWK
AND
PERL
IMPLEMENTATIONS
ARE
SPECIALIZED
TO
PREFIXES
OF
LENGTH
MARKOV
AWK
MARKOV
CHAIN
ALGORITHM
FOR
WORD
PREFIXES
BEGIN
MAXGEN
N
WL
FOR
I
I
NF
I
READ
ALL
WORDS
STATETAB
WL
NSUFFIX
WL
I
WL
I
END
STATETAB
WL
NSUFFIX
WL
ADD
TAIL
WL
FOR
I
I
MAXGEN
I
GENERATE
R
INT
RAND
NSUFFIX
WL
NSUFFIX
P
STATETAB
WL
R
IF
P
EXIT
PRINT
P
WL
P
ADVANCE
CHAIN
AWK
IS
A
PATTERN
ACTION
LANGUAGE
THE
INPUT
IS
READ
A
LINE
AT
A
TIME
EACH
MATCHED
AGAINST
THE
PATTERNS
AND
FOR
EACH
MATCH
THE
CORRESPONDING
ACTION
IS
EXE
THERE
ARE
TWO
SPECIAL
PATTERNS
BEGIN
AND
END
THAT
MATCH
BEFORE
THE
FIRST
LINE
O
AND
AFTER
THE
LAST
AN
ACTION
IS
A
BLOCK
OF
STATEMENTS
ENCLOSED
IN
BRACES
IN
THE
AWK
VERSION
O
KOV
THE
BEGIN
BLOCK
INITIALIZES
THE
PREFIX
AND
A
COUPLE
OF
OTHER
VARIABLES
THE
NEXT
BLOCK
HAS
NO
PATTERN
SO
BY
DEFAULT
IT
IS
EXECUTED
ONCE
FOR
EACH
INP
AWK
AUTOMATICALLY
SPLITS
EACH
INPUT
LINE
INTO
FIELDS
WHITE
SPACE
DELIMITED
CALLED
THROUGH
NF
THE
VARIABLE
NF
IS
THE
NUMBER
OF
FIELDS
THE
STATEMENT
STATETAB
WL
NSUFFIX
WL
I
BUILDS
THE
MAP
FROM
PREFIX
TO
SUFFIXES
THE
ARRAY
NSUFFI
X
COUNTS
SUFFIXES
A
ELEMENT
NSUFFI
X
WL
COUNTS
THE
NUMBER
OF
SUFFIXES
ASSOCIATED
WITH
THAT
THE
SUFFIXES
THEMSELVES
ARE
STORED
IN
ARRAY
ELEMENTS
STATETAB
WL
STATETAB
WL
AND
SO
ON
WHEN
THE
END
BLOCK
IS
EXECUTED
ALL
THE
INPUT
HAS
BEEN
READ
AT
THAT
POI
EACH
PREFIX
THERE
IS
AN
ELEMENT
OF
NSUFFI
X
CONTAINING
THE
SUFFIX
COUNT
AND
TH
THAT
MANY
ELEMENTS
OF
STATETAB
CONTAINING
THE
SUFFIXES
THE
PERL
VERSION
IS
SIMILAR
BUT
USES
AN
ANONYMOUS
ARRAY
INSTEAD
OF
A
THIR
SCRIPT
TO
KEEP
TRACK
OF
SUFFIXES
IT
ALSO
USES
MULTIPLE
ASSIGNMENT
TO
UPDATE
THE
PERL
USES
SPECIAL
CHARACTERS
TO
INDICATE
THE
TYPES
OF
VARIABLES
MARKS
A
SCALAR
AN
INDEXED
ARRAY
WHILE
BRACKETS
ARE
USED
TO
INDEX
ARRAYS
AND
BRACES
TO
HASHES
MARKOV
PL
MARKOV
CHAIN
ALGORITHM
FOR
WORD
PREFIXES
MAXGEN
N
WL
WHILE
FOREACH
SPLIT
INITIAL
STATE
READ
EACH
LINE
OF
INPU
PUSH
STATETAB
WL
MULTIPLE
PUSH
STATETAB
NONWORD
WL
FOR
I
I
MAXGEN
I
ASSIGNMENT
ADD
TAIL
SUF
STATETAB
ARRAY
REFERENCE
R
INT
RAND
SUF
SUF
IS
NUMBER
OF
ELE
EXIT
IF
T
SUF
R
EQ
PRINT
T
N
WL
T
ADVANCE
CHAIN
AS
IN
THE
PREVIOUS
PROGRAMS
THE
MAP
IS
STORED
USING
THE
VARIABLE
STATET
HEART
OF
THE
PROGRAM
IS
THE
LINE
PUSH
STATETAB
WHICH
PUSHES
A
NEW
SUFFIX
ONTO
THE
END
OF
THE
ANONYMOUS
ARRAY
STATETAB
IN
THE
GENERATION
PHASE
STATETAB
IS
ENCE
TO
AN
ARRAY
OF
SUFFIXES
AND
SUF
R
POINTS
TO
THE
R
TH
SUFFIX
BOTH
THE
PERL
AND
AWK
PROGRAMS
ARE
SHORT
COMPARED
TO
THE
THREE
EARLIER
BUT
THEY
ARE
HARDER
TO
ADAPT
TO
HANDLE
PREFIXES
THAT
ARE
NOT
EXACTLY
TWO
WO
CORE
OF
THE
C
STL
IMPLEMENTATION
THE
ADD
AND
GENERATE
FUNCTIONS
IS
OF
BLE
LENGTH
AND
SEEMS
CLEARER
NEVERTHELESS
SCRIPTING
LANGUAGES
ARE
OFTEN
CHOICE
FOR
EXPERIMENTAL
PROGRAMMING
FOR
MAKING
PROTOTYPES
AND
EVEN
FOR
TION
USE
IF
RUN
TIME
IS
NOT
A
MAJOR
ISSUE
EXERCISE
MODIFY
THE
AWK
AND
PERL
VERSIONS
TO
HANDLE
PREFIXES
OF
AN
EXPERIMENT
TO
DETERMINE
WHAT
EFFECT
THIS
CHANGE
HAS
ON
PERFORMANCE
D
PERFORMANCE
WE
HAVE
SEVERAL
IMPLEMENTATIONS
TO
COMPARE
WE
TIMED
THE
PROGRAM
BOOK
OF
PSALMS
FROM
THE
KING
JAMES
BIBLE
WHICH
HAS
WORDS
238
WORDS
PREFIXES
THIS
TEXT
HAS
ENOUGH
REPEATED
PHRASES
BLESSED
IS
THAT
ONE
SUFFIX
LIST
HAS
MORE
THAN
ELEMENTS
AND
THERE
ARE
A
FEW
HUNDRED
WITH
DOZENS
OF
SUFFIXES
SO
IT
IS
A
GOOD
TEST
DATA
SET
BLESSED
IS
THE
MAN
OF
THE
NET
TUM
THEE
UNTO
ME
AND
RAISE
ME
UP
THAT
MAY
TELL
ALL
MY
FEARS
THEY
LOOKED
UNTO
HIM
HE
HEARD
MY
PRAISE
SHAL
BE
BLESSED
WEALTH
AND
RICHES
SHALL
BE
SAVED
THOU
HAST
DEALT
WELL
WITH
THY
HID
TREASURE
THEY
ARE
CAST
INTO
A
STANDING
WATER
THE
FLINT
INTO
A
STAND
ING
WATER
AND
DRY
GROUND
INTO
WATERSPRINGS
THE
TIMES
IN
THE
FOLLOWING
TABLE
ARE
THE
NUMBER
OF
SECONDS
FOR
GENERATING
WORDS
OF
OUTPUT
ONE
MACHINE
IS
A
MIPS
RUNNING
LRIX
A
OTHER
IS
A
PENTIUM
II
WITH
MEGABYTES
OF
MEMORY
RUNNING
WINDO
RUN
TIME
IS
ALMOST
ENTIRELY
DETERMINED
BY
THE
INPUT
SIZE
GENERATION
IS
VERY
COMPARISON
THE
TABLE
ALSO
INCLUDES
THE
APPROXIMATE
PROGRAM
SIZE
IN
LINES
OF
CODE
PENTIUM
II
LINES
OF
SOURCE
CODE
C
SEC
SEC
JAVA
C
STL
DEQUE
C
STL
LIST
AWK
PERL
THE
C
AND
C
VERSIONS
WERE
COMPILED
WITH
OPTIMIZING
COMPILERS
WHILE
TH
RUNS
HAD
JUST
IN
TIME
COMPILERS
ENABLED
THE
IRIX
C
AND
C
TIMES
ARE
THE
OBTAINED
FROM
THREE
DIFFERENT
COMPILERS
SIMILAR
RESULTS
WERE
OBSERVED
ON
SUN
AND
DEC
ALPHA
MACHINES
THE
C
VERSION
OF
THE
PROGRAM
IS
FASTEST
BY
A
LARGE
PERL
COMES
SECOND
THE
TIMES
IN
THE
TABLE
ARE
A
SNAPSHOT
OF
OUR
EXPERIENCE
WITH
TICULAR
SET
OF
COMPILERS
AND
LIBRARIES
HOWEVER
SO
YOU
MAY
SEE
VERY
DIFFERENT
RE
YOUR
ENVIRONMENT
SOMETHING
IS
CLEARLY
WRONG
WITH
THE
STL
DEQUE
VERSION
ON
WINDOWS
MENTS
SHOWED
THAT
THE
DEQUE
THAT
REPRESENTS
THE
PREFIX
ACCOUNTS
FOR
MOST
OF
T
TIME
ALTHOUGH
IT
NEVER
HOLDS
MORE
THAN
TWO
ELEMENTS
WE
WOULD
EXPECT
THE
DATA
STRUCTURE
THE
MAP
TO
DOMINATE
SWITCHING
FROM
A
DEQUE
TO
A
LIST
WHI
DOUBLY
LINKED
LIST
IN
THE
STL
IMPROVES
THE
TIME
DRAMATICALLY
ON
THE
OTHER
SWITCHING
FROM
A
MAP
TO
A
NON
STANDARD
HASH
CONTAINER
MADE
NO
DIFFERENCE
O
HASHES
WERE
NOT
AVAILABLE
ON
OUR
WINDOWS
MACHINE
IT
IS
A
TESTAMENT
TO
THE
MENTAL
SOUNDNESS
OF
THE
STL
DESIGN
THAT
THESE
CHANGES
REQUIRED
ONLY
SUBSTITUTI
WORD
LIST
FOR
THE
WORD
DEQUE
OR
HASH
FOR
MAP
IN
TWO
PLACES
AND
RECOMPILIN
CONCLUDE
THAT
THE
STL
WHICH
IS
A
NEW
COMPONENT
OF
C
STILL
SUFFERS
FROM
IM
IMPLEMENTATIONS
THE
PERFORMANCE
IS
UNPREDICTABLE
BETWEEN
IMPLEMENTATIONS
STL
AND
BETWEEN
INDIVIDUAL
DATA
STRUCTURES
THE
SAME
IS
TRUE
OF
JAVA
WHERE
MENTATIONS
ARE
ALSO
CHANGING
RAPIDLY
THERE
ARE
SOME
INTERESTING
CHALLENGES
IN
TESTING
A
PROGRAM
THAT
IS
MEAN
DUCE
VOLUMINOUS
RANDOM
OUTPUT
HOW
DO
WE
KNOW
IT
WORKS
AT
ALL
HOW
DO
IT
WORKS
ALL
THE
TIME
CHAPTER
WHICH
DISCUSSES
TESTING
CONTAINS
SOME
SU
AND
DESCRIBES
HOW
WE
TESTED
THE
MARKOV
PROGRAMS
LESSONS
THE
MARKOV
PROGRAM
HAS
A
LONG
HISTORY
THE
FIRST
VERSION
WAS
WRITTEN
B
MITCHELL
ADAPTED
BY
BRUCE
ELLIS
AND
APPLIED
TO
HUMOROUS
DECONSTRUCTIONIST
THROUGHOUT
THE
IT
LAY
DORMANT
UNTIL
WE
THOUGHT
TO
USE
IT
IN
A
UNIVERSIT
AS
AN
ILLUSTRATION
OF
PROGRAM
DESIGN
RATHER
THAN
DUSTING
OFF
THE
ORIGINAL
W
IT
FROM
SCRATCH
IN
C
TO
REFRESH
OUR
MEMORIES
OF
THE
VARIOUS
ISSUES
THAT
ARISE
WROTE
IT
AGAIN
IN
SEVERAL
OTHER
LANGUAGES
USING
EACH
LANGUAGE
UNIQUE
EXPRESS
THE
SAME
BASIC
IDEA
AFTER
THE
COURSE
WE
REWORKED
THE
PROGRAMS
MA
TO
IMPROVE
CLARITY
AND
PRESENTATION
OVER
ALL
THAT
TIME
HOWEVER
THE
BASIC
DESIGN
HAS
REMAINED
THE
SAME
TH
VERSION
USED
THE
SAME
APPROACH
AS
THE
ONES
WE
HAVE
PRESENTED
HERE
ALTHOU
EMPLOY
A
SECOND
HASH
TABLE
TO
REPRESENT
INDIVIDUAL
WORDS
IF
WE
WERE
TO
AGAIN
WE
WOULD
PROBABLY
NOT
CHANGE
MUCH
THE
DESIGN
OF
A
PROGRAM
IS
ROOT
LAYOUT
OF
ITS
DATA
THE
DATA
STRUCTURES
DON
T
DEFINE
EVERY
DETAIL
BUT
THEY
DO
OVERALL
SOLUTION
SOME
DATA
STRUCTURE
CHOICES
MAKE
LITTLE
DIFFERENCE
SUCH
AS
LISTS
VERSUS
ARRAYS
SOME
IMPLEMENTATIONS
GENERALIZE
BETTER
THAN
OTHERS
THE
PERL
AND
A
COULD
BE
READILY
MODIFIED
TO
ONE
OR
THREE
WORD
PREFIXES
BUT
PARAMETERI
CHOICE
WOULD
BE
AWKWARD
AS
BEFITS
OBJECT
ORIENTED
LANGUAGES
TINY
CHANG
C
AND
JAVA
IMPLEMENTATIONS
WOULD
MAKE
THE
DATA
STRUCTURES
SUITABLE
FO
OTHER
THAN
ENGLISH
TEXT
FOR
INSTANCE
PROGRAMS
WHERE
WHITE
SPACE
WOULD
B
CANT
OR
NOTES
OF
MUSIC
OR
EVEN
MOUSE
CLICKS
AND
MENU
SELECTIONS
FOR
GENER
SEQUENCES
OF
COURSE
WHILE
THE
DATA
STRUCTURES
ARE
MUCH
THE
SAME
THERE
IS
A
WIDE
VA
THE
GENERAL
APPEARANCE
OF
THE
PROGRAMS
IN
THE
SIZE
OF
THE
SOURCE
CODE
AND
I
MANCE
VERY
ROUGHLY
HIGHER
LEVEL
LANGUAGES
GIVE
SLOWER
PROGRAMS
THAN
LO
ONES
ALTHOUGH
IT
UNWISE
TO
GENERALIZE
OTHER
THAN
QUALITATIVELY
BIG
BUILDIN
LIKE
THE
C
STL
OR
THE
ASSOCIATIVE
ARRAYS
AND
STRING
HANDLING
OF
SCRIPTING
CAN
LEAD
TO
MORE
COMPACT
CODE
AND
SHORTER
DEVELOPMENT
TIME
THESE
ARE
NO
PRICE
ALTHOUGH
THE
PERFORMANCE
PENALTY
MAY
NOT
MATTER
MUCH
FOR
PROGRAMS
KOV
THAT
RUN
FOR
ONLY
A
FEW
SECONDS
LESS
CLEAR
HOWEVER
IS
HOW
TO
ASSESS
THE
LOSS
OF
CONTROL
AND
INSIGHT
WHEN
OF
SYSTEM
SUPPLIED
CODE
GETS
SO
BIG
THAT
ONE
NO
LONGER
KNOWS
WHAT
GOING
O
NEATH
THIS
IS
THE
CASE
WITH
THE
STL
VERSION
ITS
PERFORMANCE
IS
UNPREDICT
THERE
IS
NO
EASY
WAY
TO
ADDRESS
THAT
ONE
IMMATURE
IMPLEMENTATION
WE
USE
TO
BE
REPAIRED
BEFORE
IT
WOULD
RUN
OUR
PROGRAM
FEW
OF
US
HAVE
THE
RESOURCES
ENERGY
TO
TRACK
DOWN
SUCH
PROBLEMS
AND
FIX
THEM
THIS
IS
A
PERVASIVE
AND
GROWING
CONCERN
IN
SOFTWARE
AS
LIBRARIES
INTERFAC
TOOLS
BECOME
MORE
COMPLICATED
THEY
BECOME
LESS
UNDERSTOOD
AND
LESS
CONTRO
WHEN
EVERYTHING
WORKS
RICH
PROGRAMMING
ENVIRONMENTS
CAN
BE
VERY
PRODUCTI
WHEN
THEY
FAIL
THERE
IS
LITTLE
RECOURSE
INDEED
WE
MAY
NOT
EVEN
REALIZE
THAT
THING
IS
WRONG
IF
THE
PROBLEMS
INVOLVE
PERFORMANCE
OR
SUBTLE
LOGIC
ERRORS
THE
DESIGN
AND
IMPLEMENTATION
OF
THIS
PROGRAM
ILLUSTRATE
A
NUMBER
OF
LESS
LARGER
PROGRAMS
FIRST
IS
THE
IMPORTANCE
OF
CHOOSING
SIMPLE
ALGORITHMS
AN
STRUCTURES
THE
SIMPLEST
THAT
WILL
DO
THE
JOB
IN
REASONABLE
TIME
FOR
THE
EXPECTED
LEM
SIZE
IF
SOMEONE
ELSE
HAS
ALREADY
WRITTEN
THEM
AND
PUT
THEM
IN
A
LIBRARY
F
THAT
EVEN
BETTER
OUR
C
IMPLEMENTATION
PROFITED
FROM
THAT
FOLLOWING
BROOKS
S
ADVICE
WE
FIND
IT
BEST
TO
START
DETAILED
DESIGN
WITH
DATA
TURES
GUIDED
BY
KNOWLEDGE
OF
WHAT
ALGORITHMS
MIGHT
BE
USED
WITH
THE
DATA
STR
SETTLED
THE
CODE
GOES
TOGETHER
EASILY
IT
S
HARD
TO
DESIGN
A
PROGRAM
COMPLETELY
AND
THEN
BUILD
IT
CONSTRUCTING
RE
GRAMS
INVOLVES
ITERATION
AND
EXPERIMENTATION
THE
ACT
OF
BUILDING
FORCES
ONE
T
IFY
DECISIONS
THAT
HAD
PREVIOUSLY
BEEN
GLOSSED
OVER
THAT
WAS
CERTAINLY
THE
CAS
OUR
PROGRAMS
HERE
WHICH
HAVE
GONE
THROUGH
MANY
CHANGES
OF
DETAIL
AS
M
POSSIBLE
START
WITH
SOMETHING
SIMPLE
AND
EVOLVE
IT
AS
EXPERIENCE
DICTATES
IF
O
HAD
BEEN
JUST
TO
WRITE
A
PERSONAL
VERSION
OF
THE
MARKOV
CHAIN
ALGORITHM
FOR
F
WOULD
ALMOST
SURELY
HAVE
WRITTEN
IT
IN
AWK
OR
PERL
THOUGH
NOT
WITH
AS
MUCH
ING
AS
THE
ONES
WE
SHOWED
HERE
AND
LET
IT
GO
AT
THAT
PRODUCTION
CODE
TAKES
MUCH
MORE
EFFORT
THAN
PROTOTYPES
DO
HOWEVER
IF
W
OF
THE
PROGRAMS
PRESENTED
HERE
AS
PRODUCTION
CODE
SINCE
THEY
HAVE
BEEN
POLISH
THOROUGHLY
TESTED
PRODUCTION
QUALITY
REQUIRES
ONE
OR
TWO
ORDERS
OF
MAGNITUD
EFFORT
THAN
A
PROGRAM
INTENDED
FOR
PERSONAL
USE
EXERCISE
WE
HAVE
SEEN
VERSIONS
OF
THE
MARKOV
PROGRAM
IN
A
WIDE
VARIETY
GUAGES
INCLUDING
SCHEME
TEL
PROLOG
PYTHON
GENERIC
JAVA
ML
AND
HASKEL
PRESENTS
ITS
OWN
CHALLENGES
AND
ADVANTAGES
IMPLEMENT
THE
PROGRAM
IN
YOUR
LANGUAGE
AND
COMPARE
ITS
GENERAL
FLAVOR
AND
PERFORMANCE
D
SUPPLEMENTARY
READING
THE
STANDARD
TEMPLATE
LIBRARY
IS
DESCRIBED
IN
A
VARIETY
OF
BOOKS
INCLUDIN
ERIC
PROGRAMMING
AND
THE
STL
BY
MATTHEW
AUSTEM
ADDISON
WESLEY
DEFINITIVE
REFERENCE
ON
C
ITSELF
IS
THE
C
PROGRAMMING
LA
NGUAGE
BY
STROUSTRUP
EDITION
ADDISON
WESLEY
FOR
JAVA
WE
REFER
TO
THE
JAV
GRAMMING
LA
NGUAGE
EDITION
BY
KEN
ARNOLD
AND
JAMES
GOSLING
AD
WESLEY
THE
BEST
DESCRIPTION
OF
PERL
IS
PROGRAMMING
PERL
EDITI
LARRY
WALL
TOM
CHRISTIANSEN
AND
RANDAL
SCHWARTZ
O
REILLY
THE
IDEA
BEHIND
DESIGN
PATTERNS
IS
THAT
THERE
ARE
ONLY
A
FEW
DISTINCT
DES
STRUCTS
IN
MOST
PROGRAMS
IN
THE
SAME
WAY
THAT
THERE
ARE
ONLY
A
FEW
BASIC
D
TURES
VERY
LOOSELY
IT
IS
THE
DESIGN
ANALOG
OF
THE
CODE
IDIOMS
THAT
WE
DISC
CHAPTER
THE
STANDARD
REFERENCE
IS
DESIGN
PATTERNS
ELEMENTS
OF
REUSABLE
ORIENTED
SOFTWARE
BY
ERICH
GAMMA
RICHARD
HELM
RALPH
JOHNSON
AND
JO
SIDES
ADDISON
WESLEY
THE
PICARESQUE
ADVENTURES
OF
THE
MARKOV
PROGRAM
ORIGINALLY
CALLED
SHAN
DESCRIBED
IN
THE
COMPUTING
RECREATIONS
COLUMN
OF
THE
JUNE
SCIENTIF
ICAN
THE
ARTICLE
WAS
REPUBLISHED
IN
THE
MAGIC
MACHINE
BY
A
K
DEWDNE
FREEMAN
THREADS
PROCESSES
AND
THREADS
MULTITHREADING
THREAD
FUNCTIONALITY
TYPES
OF
THREADS
USER
LEVEL
AND
KERNEL
LEVEL
THREADS
OTHER
ARRANGEMENTS
MULTICORE
AND
MULTITHREADING
PERFORMANCE
OF
SOFTWARE
ON
MULTICORE
APPLICATION
EXAMPLE
VALVE
GAME
SOFTWARE
WINDOWS
THREAD
AND
SMP
MANAGEMENT
PROCESS
AND
THREAD
OBJECTS
MULTITHREADING
THREAD
STATES
SUPPORT
FOR
OS
SUBSYSTEMS
SYMMETRIC
MULTIPROCESSING
SUPPORT
SOLARIS
THREAD
AND
SMP
MANAGEMENT
MULTITHREADED
ARCHITECTURE
MOTIVATION
PROCESS
STRUCTURE
THREAD
EXECUTION
INTERRUPTS
AS
THREADS
LINUX
PROCESS
AND
THREAD
MANAGEMENT
LINUX
TASKS
LINUX
THREADS
MAC
OS
X
GRAND
CENTRAL
DISPATCH
SUMMARY
RECOMMENDED
READING
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
CHAPTER
THREADS
THE
BASIC
IDEA
IS
THAT
THE
SEVERAL
COMPONENTS
IN
ANY
COMPLEX
SYSTEM
WILL
PERFORM
PARTICULAR
SUBFUNCTIONS
THAT
CONTRIBUTE
TO
THE
OVERALL
FUNCTION
THE
SCIENCES
OF
THE
ARTIFICIAL
HERBERT
SIMON
THIS
CHAPTER
EXAMINES
SOME
MORE
ADVANCED
CONCEPTS
RELATED
TO
PROCESS
MANAGE
MENT
WHICH
ARE
FOUND
IN
A
NUMBER
OF
CONTEMPORARY
OPERATING
SYSTEMS
WE
SHOW
THAT
THE
CONCEPT
OF
PROCESS
IS
MORE
COMPLEX
AND
SUBTLE
THAN
PRESENTED
SO
FAR
AND
IN
FACT
EMBODIES
TWO
SEPARATE
AND
POTENTIALLY
INDEPENDENT
CONCEPTS
ONE
RELATING
TO
RESOURCE
OWNERSHIP
AND
ANOTHER
RELATING
TO
EXECUTION
THIS
DISTINCTION
HAS
LED
TO
THE
DEVELOPMENT
IN
MANY
OPERATING
SYSTEMS
OF
A
CONSTRUCT
KNOWN
AS
THE
THREAD
PROCESSES
AND
THREADS
THE
DISCUSSION
SO
FAR
HAS
PRESENTED
THE
CONCEPT
OF
A
PROCESS
AS
EMBODYING
TWO
CHARACTERISTICS
RESOURCE
OWNERSHIP
A
PROCESS
INCLUDES
A
VIRTUAL
ADDRESS
SPACE
TO
HOLD
THE
PROCESS
IMAGE
RECALL
FROM
CHAPTER
THAT
THE
PROCESS
IMAGE
IS
THE
COLLECTION
OF
PROGRAM
DATA
STACK
AND
ATTRIBUTES
DEFINED
IN
THE
PROCESS
CONTROL
BLOCK
FROM
TIME
TO
TIME
A
PROCESS
MAY
BE
ALLOCATED
CONTROL
OR
OWNERSHIP
OF
RESOURCES
SUCH
AS
MAIN
MEMORY
I
O
CHANNELS
I
O
DEVICES
AND
FILES
THE
OS
PERFORMS
A
PROTECTION
FUNCTION
TO
PREVENT
UNWANTED
INTERFERENCE
BETWEEN
PROCESSES
WITH
RESPECT
TO
RESOURCES
SCHEDULING
EXECUTION
THE
EXECUTION
OF
A
PROCESS
FOLLOWS
AN
EXECUTION
PATH
TRACE
THROUGH
ONE
OR
MORE
PROGRAMS
E
G
FIGURE
THIS
EXECUTION
MAY
BE
INTERLEAVED
WITH
THAT
OF
OTHER
PROCESSES
THUS
A
PROCESS
HAS
AN
EXECUTION
STATE
RUNNING
READY
ETC
AND
A
DISPATCHING
PRIORITY
AND
IS
THE
ENTITY
THAT
IS
SCHEDULED
AND
DISPATCHED
BY
THE
OS
SOME
THOUGHT
SHOULD
CONVINCE
THE
READER
THAT
THESE
TWO
CHARACTERISTICS
ARE
INDEPENDENT
AND
COULD
BE
TREATED
INDEPENDENTLY
BY
THE
OS
THIS
IS
DONE
IN
A
NUMBER
OF
OPERATING
SYSTEMS
PARTICULARLY
RECENTLY
DEVELOPED
SYSTEMS
TO
PROCESSES
AND
THREADS
DISTINGUISH
THE
TWO
CHARACTERISTICS
THE
UNIT
OF
DISPATCHING
IS
USUALLY
REFERRED
TO
AS
A
THREAD
OR
LIGHTWEIGHT
PROCESS
WHILE
THE
UNIT
OF
RESOURCE
OWNERSHIP
IS
USUALLY
REFERRED
TO
AS
A
PROCESS
OR
TASK
MULTITHREADING
MULTITHREADING
REFERS
TO
THE
ABILITY
OF
AN
OS
TO
SUPPORT
MULTIPLE
CONCURRENT
PATHS
OF
EXECUTION
WITHIN
A
SINGLE
PROCESS
THE
TRADITIONAL
APPROACH
OF
A
SINGLE
THREAD
OF
EXECUTION
PER
PROCESS
IN
WHICH
THE
CONCEPT
OF
A
THREAD
IS
NOT
RECOGNIZED
IS
REFERRED
TO
AS
A
SINGLE
THREADED
APPROACH
THE
TWO
ARRANGEMENTS
SHOWN
IN
THE
LEFT
HALF
OF
FIGURE
ARE
SINGLE
THREADED
APPROACHES
MS
DOS
IS
AN
EXAMPLE
OF
AN
OS
THAT
SUPPORTS
A
SINGLE
USER
PROCESS
AND
A
SINGLE
THREAD
OTHER
OPERATING
SYSTEMS
SUCH
AS
SOME
VARIANTS
OF
UNIX
SUPPORT
MULTIPLE
USER
PROCESSES
BUT
ONLY
SUPPORT
ONE
THREAD
PER
PROCESS
THE
RIGHT
HALF
OF
FIGURE
DEPICTS
MULTITHREADED
APPROACHES
A
JAVA
RUN
TIME
ENVIRONMENT
IS
AN
EXAMPLE
OF
A
SYSTEM
OF
ONE
PROCESS
WITH
MULTI
PLE
THREADS
OF
INTEREST
IN
THIS
SECTION
IS
THE
USE
OF
MULTIPLE
PROCESSES
EACH
OF
WHICH
SUPPORTS
MULTIPLE
THREADS
THIS
APPROACH
IS
TAKEN
IN
WINDOWS
SOLARIS
AND
MANY
MODERN
VERSIONS
OF
UNIX
AMONG
OTHERS
IN
THIS
SECTION
WE
GIVE
A
GENERAL
DESCRIPTION
INSTRUCTION
TRACE
FIGURE
THREADS
AND
PROCESSES
EVEN
THIS
DEGREE
OF
CONSISTENCY
IS
NOT
MAINTAINED
IN
IBM
MAINFRAME
OPERATING
SYSTEMS
THE
CON
CEPTS
OF
ADDRESS
SPACE
AND
TASK
RESPECTIVELY
CORRESPOND
ROUGHLY
TO
THE
CONCEPTS
OF
PROCESS
AND
THREAD
THAT
WE
DESCRIBE
IN
THIS
SECTION
ALSO
IN
THE
LITERATURE
THE
TERM
LIGHTWEIGHT
PROCESS
IS
USED
AS
EITHER
EQUIVALENT
TO
THE
TERM
THREAD
A
PARTICULAR
TYPE
OF
THREAD
KNOWN
AS
A
KERNEL
LEVEL
THREAD
OR
IN
THE
CASE
OF
SOLARIS
AN
ENTITY
THAT
MAPS
USER
LEVEL
THREADS
TO
KERNEL
LEVEL
THREADS
CHAPTER
THREADS
OF
MULTITHREADING
THE
DETAILS
OF
THE
WINDOWS
SOLARIS
AND
LINUX
APPROACHES
ARE
DISCUSSED
LATER
IN
THIS
CHAPTER
IN
A
MULTITHREADED
ENVIRONMENT
A
PROCESS
IS
DEFINED
AS
THE
UNIT
OF
RESOURCE
ALLOCATION
AND
A
UNIT
OF
PROTECTION
THE
FOLLOWING
ARE
ASSOCIATED
WITH
PROCESSES
A
VIRTUAL
ADDRESS
SPACE
THAT
HOLDS
THE
PROCESS
IMAGE
PROTECTED
ACCESS
TO
PROCESSORS
OTHER
PROCESSES
FOR
INTERPROCESS
COMMUNICA
TION
FILES
AND
I
O
RESOURCES
DEVICES
AND
CHANNELS
WITHIN
A
PROCESS
THERE
MAY
BE
ONE
OR
MORE
THREADS
EACH
WITH
THE
FOLLOWING
A
THREAD
EXECUTION
STATE
RUNNING
READY
ETC
A
SAVED
THREAD
CONTEXT
WHEN
NOT
RUNNING
ONE
WAY
TO
VIEW
A
THREAD
IS
AS
AN
INDEPENDENT
PROGRAM
COUNTER
OPERATING
WITHIN
A
PROCESS
AN
EXECUTION
STACK
SOME
PER
THREAD
STATIC
STORAGE
FOR
LOCAL
VARIABLES
ACCESS
TO
THE
MEMORY
AND
RESOURCES
OF
ITS
PROCESS
SHARED
WITH
ALL
OTHER
THREADS
IN
THAT
PROCESS
FIGURE
ILLUSTRATES
THE
DISTINCTION
BETWEEN
THREADS
AND
PROCESSES
FROM
THE
POINT
OF
VIEW
OF
PROCESS
MANAGEMENT
IN
A
SINGLE
THREADED
PROCESS
MODEL
I
E
THERE
IS
NO
DISTINCT
CONCEPT
OF
THREAD
THE
REPRESENTATION
OF
A
PROCESS
INCLUDES
ITS
PROCESS
CONTROL
BLOCK
AND
USER
ADDRESS
SPACE
AS
WELL
AS
USER
AND
KERNEL
STACKS
TO
MANAGE
THE
CALL
RETURN
BEHAVIOR
OF
THE
EXECUTION
OF
THE
PROCESS
WHILE
THE
PROCESS
IS
RUNNING
IT
CONTROLS
THE
PROCESSOR
REGISTERS
THE
CONTENTS
OF
THESE
REGISTERS
ARE
SAVED
WHEN
THE
PROCESS
IS
NOT
RUNNING
IN
A
MULTITHREADED
ENVIRONMENT
THERE
IS
STILL
A
SINGLE
PROCESS
CONTROL
BLOCK
AND
USER
ADDRESS
SPACE
ASSOCIATED
WITH
THE
PROC
ESS
BUT
NOW
THERE
ARE
SEPARATE
STACKS
FOR
EACH
THREAD
AS
WELL
AS
A
SEPARATE
CONTROL
FIGURE
SINGLE
THREADED
AND
MULTITHREADED
PROCESS
MODELS
PROCESSES
AND
THREADS
BLOCK
FOR
EACH
THREAD
CONTAINING
REGISTER
VALUES
PRIORITY
AND
OTHER
THREAD
RELATED
STATE
INFORMATION
THUS
ALL
OF
THE
THREADS
OF
A
PROCESS
SHARE
THE
STATE
AND
RESOURCES
OF
THAT
PROCESS
THEY
RESIDE
IN
THE
SAME
ADDRESS
SPACE
AND
HAVE
ACCESS
TO
THE
SAME
DATA
WHEN
ONE
THREAD
ALTERS
AN
ITEM
OF
DATA
IN
MEMORY
OTHER
THREADS
SEE
THE
RESULTS
IF
AND
WHEN
THEY
ACCESS
THAT
ITEM
IF
ONE
THREAD
OPENS
A
FILE
WITH
READ
PRIVILEGES
OTHER
THREADS
IN
THE
SAME
PROCESS
CAN
ALSO
READ
FROM
THAT
FILE
THE
KEY
BENEFITS
OF
THREADS
DERIVE
FROM
THE
PERFORMANCE
IMPLICATIONS
IT
TAKES
FAR
LESS
TIME
TO
CREATE
A
NEW
THREAD
IN
AN
EXISTING
PROCESS
THAN
TO
CREATE
A
BRAND
NEW
PROCESS
STUDIES
DONE
BY
THE
MACH
DEVELOPERS
SHOW
THAT
THREAD
CREATION
IS
TEN
TIMES
FASTER
THAN
PROCESS
CREATION
IN
UNIX
IT
TAKES
LESS
TIME
TO
TERMINATE
A
THREAD
THAN
A
PROCESS
IT
TAKES
LESS
TIME
TO
SWITCH
BETWEEN
TWO
THREADS
WITHIN
THE
SAME
PROCESS
THAN
TO
SWITCH
BETWEEN
PROCESSES
THREADS
ENHANCE
EFFICIENCY
IN
COMMUNICATION
BETWEEN
DIFFERENT
EXECUTING
PROGRAMS
IN
MOST
OPERATING
SYSTEMS
COMMUNICATION
BETWEEN
INDEPENDENT
PROCESSES
REQUIRES
THE
INTERVENTION
OF
THE
KERNEL
TO
PROVIDE
PROTECTION
AND
THE
MECHANISMS
NEEDED
FOR
COMMUNICATION
HOWEVER
BECAUSE
THREADS
WITHIN
THE
SAME
PROCESS
SHARE
MEMORY
AND
FILES
THEY
CAN
COMMUNICATE
WITH
EACH
OTHER
WITHOUT
INVOKING
THE
KERNEL
THUS
IF
THERE
IS
AN
APPLICATION
OR
FUNCTION
THAT
SHOULD
BE
IMPLEMENTED
AS
A
SET
OF
RELATED
UNITS
OF
EXECUTION
IT
IS
FAR
MORE
EFFICIENT
TO
DO
SO
AS
A
COLLECTION
OF
THREADS
RATHER
THAN
A
COLLECTION
OF
SEPARATE
PROCESSES
AN
EXAMPLE
OF
AN
APPLICATION
THAT
COULD
MAKE
USE
OF
THREADS
IS
A
FILE
SERVER
AS
EACH
NEW
FILE
REQUEST
COMES
IN
A
NEW
THREAD
CAN
BE
SPAWNED
FOR
THE
FILE
MANAGE
MENT
PROGRAM
BECAUSE
A
SERVER
WILL
HANDLE
MANY
REQUESTS
MANY
THREADS
WILL
BE
CREATED
AND
DESTROYED
IN
A
SHORT
PERIOD
IF
THE
SERVER
RUNS
ON
A
MULTIPROCESSOR
COM
PUTER
THEN
MULTIPLE
THREADS
WITHIN
THE
SAME
PROCESS
CAN
BE
EXECUTING
SIMULTANEOUSLY
ON
DIFFERENT
PROCESSORS
FURTHER
BECAUSE
PROCESSES
OR
THREADS
IN
A
FILE
SERVER
MUST
SHARE
FILE
DATA
AND
THEREFORE
COORDINATE
THEIR
ACTIONS
IT
IS
FASTER
TO
USE
THREADS
AND
SHARED
MEMORY
THAN
PROCESSES
AND
MESSAGE
PASSING
FOR
THIS
COORDINATION
THE
THREAD
CONSTRUCT
IS
ALSO
USEFUL
ON
A
SINGLE
PROCESSOR
TO
SIMPLIFY
THE
STRUCTURE
OF
A
PROGRAM
THAT
IS
LOGICALLY
DOING
SEVERAL
DIFFERENT
FUNCTIONS
GIVES
FOUR
EXAMPLES
OF
THE
USES
OF
THREADS
IN
A
SINGLE
USER
MULTI
PROCESSING
SYSTEM
FOREGROUND
AND
BACKGROUND
WORK
FOR
EXAMPLE
IN
A
SPREADSHEET
PROGRAM
ONE
THREAD
COULD
DISPLAY
MENUS
AND
READ
USER
INPUT
WHILE
ANOTHER
THREAD
EXECUTES
USER
COMMANDS
AND
UPDATES
THE
SPREADSHEET
THIS
ARRANGEMENT
OFTEN
INCREASES
THE
PERCEIVED
SPEED
OF
THE
APPLICATION
BY
ALLOWING
THE
PROGRAM
TO
PROMPT
FOR
THE
NEXT
COMMAND
BEFORE
THE
PREVIOUS
COMMAND
IS
COMPLETE
ASYNCHRONOUS
PROCESSING
ASYNCHRONOUS
ELEMENTS
IN
THE
PROGRAM
CAN
BE
IMPLEMENTED
AS
THREADS
FOR
EXAMPLE
AS
A
PROTECTION
AGAINST
POWER
FAILURE
ONE
CAN
DESIGN
A
WORD
PROCESSOR
TO
WRITE
ITS
RANDOM
ACCESS
MEMORY
RAM
BUFFER
TO
DISK
ONCE
EVERY
MINUTE
A
THREAD
CAN
BE
CREATED
WHOSE
SOLE
JOB
IS
CHAPTER
THREADS
PERIODIC
BACKUP
AND
THAT
SCHEDULES
ITSELF
DIRECTLY
WITH
THE
OS
THERE
IS
NO
NEED
FOR
FANCY
CODE
IN
THE
MAIN
PROGRAM
TO
PROVIDE
FOR
TIME
CHECKS
OR
TO
COORDINATE
INPUT
AND
OUTPUT
SPEED
OF
EXECUTION
A
MULTITHREADED
PROCESS
CAN
COMPUTE
ONE
BATCH
OF
DATA
WHILE
READING
THE
NEXT
BATCH
FROM
A
DEVICE
ON
A
MULTIPROCESSOR
SYSTEM
MUL
TIPLE
THREADS
FROM
THE
SAME
PROCESS
MAY
BE
ABLE
TO
EXECUTE
SIMULTANEOUSLY
THUS
EVEN
THOUGH
ONE
THREAD
MAY
BE
BLOCKED
FOR
AN
I
O
OPERATION
TO
READ
IN
A
BATCH
OF
DATA
ANOTHER
THREAD
MAY
BE
EXECUTING
MODULAR
PROGRAM
STRUCTURE
PROGRAMS
THAT
INVOLVE
A
VARIETY
OF
ACTIVITIES
OR
A
VARIETY
OF
SOURCES
AND
DESTINATIONS
OF
INPUT
AND
OUTPUT
MAY
BE
EASIER
TO
DESIGN
AND
IMPLEMENT
USING
THREADS
IN
AN
OS
THAT
SUPPORTS
THREADS
SCHEDULING
AND
DISPATCHING
IS
DONE
ON
A
THREAD
BASIS
HENCE
MOST
OF
THE
STATE
INFORMATION
DEALING
WITH
EXECUTION
IS
MAINTAINED
IN
THREAD
LEVEL
DATA
STRUCTURES
THERE
ARE
HOWEVER
SEVERAL
ACTIONS
THAT
AFFECT
ALL
OF
THE
THREADS
IN
A
PROCESS
AND
THAT
THE
OS
MUST
MANAGE
AT
THE
PROCESS
LEVEL
FOR
EXAMPLE
SUSPENSION
INVOLVES
SWAPPING
THE
ADDRESS
SPACE
OF
ONE
PROCESS
OUT
OF
MAIN
MEMORY
TO
MAKE
ROOM
FOR
THE
ADDRESS
SPACE
OF
ANOTHER
PROCESS
BECAUSE
ALL
THREADS
IN
A
PROCESS
SHARE
THE
SAME
ADDRESS
SPACE
ALL
THREADS
ARE
SUSPENDED
AT
THE
SAME
TIME
SIMILARLY
TERMINATION
OF
A
PROCESS
TERMINATES
ALL
THREADS
WITHIN
THAT
PROCESS
THREAD
FUNCTIONALITY
LIKE
PROCESSES
THREADS
HAVE
EXECUTION
STATES
AND
MAY
SYNCHRONIZE
WITH
ONE
ANOTHER
WE
LOOK
AT
THESE
TWO
ASPECTS
OF
THREAD
FUNCTIONALITY
IN
TURN
THREAD
STATES
AS
WITH
PROCESSES
THE
KEY
STATES
FOR
A
THREAD
ARE
RUNNING
READY
AND
BLOCKED
GENERALLY
IT
DOES
NOT
MAKE
SENSE
TO
ASSOCIATE
SUSPEND
STATES
WITH
THREADS
BECAUSE
SUCH
STATES
ARE
PROCESS
LEVEL
CONCEPTS
IN
PARTICULAR
IF
A
PROCESS
IS
SWAPPED
OUT
ALL
OF
ITS
THREADS
ARE
NECESSARILY
SWAPPED
OUT
BECAUSE
THEY
ALL
SHARE
THE
ADDRESS
SPACE
OF
THE
PROCESS
THERE
ARE
FOUR
BASIC
THREAD
OPERATIONS
ASSOCIATED
WITH
A
CHANGE
IN
THREAD
STATE
SPAWN
TYPICALLY
WHEN
A
NEW
PROCESS
IS
SPAWNED
A
THREAD
FOR
THAT
PROCESS
IS
ALSO
SPAWNED
SUBSEQUENTLY
A
THREAD
WITHIN
A
PROCESS
MAY
SPAWN
ANOTHER
THREAD
WITHIN
THE
SAME
PROCESS
PROVIDING
AN
INSTRUCTION
POINTER
AND
ARGU
MENTS
FOR
THE
NEW
THREAD
THE
NEW
THREAD
IS
PROVIDED
WITH
ITS
OWN
REGISTER
CONTEXT
AND
STACK
SPACE
AND
PLACED
ON
THE
READY
QUEUE
BLOCK
WHEN
A
THREAD
NEEDS
TO
WAIT
FOR
AN
EVENT
IT
WILL
BLOCK
SAVING
ITS
USER
REGISTERS
PROGRAM
COUNTER
AND
STACK
POINTERS
THE
PROCESSOR
MAY
NOW
TURN
TO
THE
EXECUTION
OF
ANOTHER
READY
THREAD
IN
THE
SAME
OR
A
DIFFERENT
PROCESS
UNBLOCK
WHEN
THE
EVENT
FOR
WHICH
A
THREAD
IS
BLOCKED
OCCURS
THE
THREAD
IS
MOVED
TO
THE
READY
QUEUE
FINISH
WHEN
A
THREAD
COMPLETES
ITS
REGISTER
CONTEXT
AND
STACKS
ARE
DEALLOCATED
PROCESSES
AND
THREADS
A
SIGNIFICANT
ISSUE
IS
WHETHER
THE
BLOCKING
OF
A
THREAD
RESULTS
IN
THE
BLOCKING
OF
THE
ENTIRE
PROCESS
IN
OTHER
WORDS
IF
ONE
THREAD
IN
A
PROCESS
IS
BLOCKED
DOES
THIS
PREVENT
THE
RUNNING
OF
ANY
OTHER
THREAD
IN
THE
SAME
PROCESS
EVEN
IF
THAT
OTHER
THREAD
IS
IN
A
READY
STATE
CLEARLY
SOME
OF
THE
FLEXIBILITY
AND
POWER
OF
THREADS
IS
LOST
IF
THE
ONE
BLOCKED
THREAD
BLOCKS
AN
ENTIRE
PROCESS
WE
RETURN
TO
THIS
ISSUE
SUBSEQUENTLY
IN
OUR
DISCUSSION
OF
USER
LEVEL
VERSUS
KERNEL
LEVEL
THREADS
BUT
FOR
NOW
LET
US
CONSIDER
THE
PERFORMANCE
BENEFITS
OF
THREADS
THAT
DO
NOT
BLOCK
AN
ENTIRE
PROCESS
FIGURE
BASED
ON
ONE
IN
SHOWS
A
PROGRAM
THAT
PERFORMS
TWO
REMOTE
PROCEDURE
CALLS
RPCS
TO
TWO
DIFFERENT
HOSTS
TO
OBTAIN
A
COMBINED
RESULT
IN
A
SINGLE
THREADED
PROGRAM
THE
RESULTS
ARE
OBTAINED
IN
SEQUENCE
SO
THE
PROGRAM
HAS
TO
WAIT
FOR
A
RESPONSE
FROM
EACH
SERVER
IN
TURN
REWRITING
THE
PROGRAM
TO
USE
A
SEPARATE
THREAD
FOR
EACH
RPC
RESULTS
IN
A
SUBSTANTIAL
SPEEDUP
NOTE
THAT
IF
THIS
PROGRAM
OPERATES
ON
A
UNIPROCESSOR
THE
REQUESTS
MUST
BE
GENERATED
SEQUENTIALLY
AND
THE
RESULTS
PROCESSED
IN
SEQUENCE
HOWEVER
THE
PROGRAM
WAITS
CONCURRENTLY
FOR
THE
TWO
REPLIES
RPC
TIME
RPC
REQUEST
REQUEST
PROCESS
A
RPC
USING
SINGLE
THREAD
THREAD
A
PROCESS
THREAD
B
PROCESS
B
RPC
USING
ONE
THREAD
PER
SERVER
ON
A
UNIPROCESSOR
BLOCKED
WAITING
FOR
RESPONSE
TO
RPC
BLOCKED
WAITING
FOR
PROCESSOR
WHICH
IS
IN
USE
BY
THREAD
B
RUNNING
FIGURE
REMOTE
PROCEDURE
CALL
RPC
USING
THREADS
RPC
IS
A
TECHNIQUE
BY
WHICH
TWO
PROGRAMS
WHICH
MAY
EXECUTE
ON
DIFFERENT
MACHINES
INTERACT
USING
PROCEDURE
CALL
RETURN
SYNTAX
AND
SEMANTICS
BOTH
THE
CALLED
AND
CALLING
PROGRAM
BEHAVE
AS
IF
THE
PARTNER
PROGRAM
WERE
RUNNING
ON
THE
SAME
MACHINE
RPCS
ARE
OFTEN
USED
FOR
CLIENT
SERVER
APPLICATIONS
AND
ARE
DISCUSSED
IN
CHAPTER
CHAPTER
THREADS
TIME
I
O
REQUEST
REQUEST
COMPLETE
TIME
QUANTUM
EXPIRES
THREAD
A
PROCESS
THREAD
B
PROCESS
THREAD
C
PROCESS
TIME
QUANTUM
EXPIRES
PROCESS
CREATED
BLOCKED
READY
RUNNING
FIGURE
MULTITHREADING
EXAMPLE
ON
A
UNIPROCESSOR
ON
A
UNIPROCESSOR
MULTIPROGRAMMING
ENABLES
THE
INTERLEAVING
OF
MULTIPLE
THREADS
WITHIN
MULTIPLE
PROCESSES
IN
THE
EXAMPLE
OF
FIGURE
THREE
THREADS
IN
TWO
PROCESSES
ARE
INTERLEAVED
ON
THE
PROCESSOR
EXECUTION
PASSES
FROM
ONE
THREAD
TO
ANOTHER
EITHER
WHEN
THE
CURRENTLY
RUNNING
THREAD
IS
BLOCKED
OR
WHEN
ITS
TIME
SLICE
IS
EXHAUSTED
THREAD
SYNCHRONIZATION
ALL
OF
THE
THREADS
OF
A
PROCESS
SHARE
THE
SAME
ADDRESS
SPACE
AND
OTHER
RESOURCES
SUCH
AS
OPEN
FILES
ANY
ALTERATION
OF
A
RESOURCE
BY
ONE
THREAD
AFFECTS
THE
ENVIRONMENT
OF
THE
OTHER
THREADS
IN
THE
SAME
PROCESS
IT
IS
THEREFORE
NECESSARY
TO
SYNCHRONIZE
THE
ACTIVITIES
OF
THE
VARIOUS
THREADS
SO
THAT
THEY
DO
NOT
INTERFERE
WITH
EACH
OTHER
OR
CORRUPT
DATA
STRUCTURES
FOR
EXAMPLE
IF
TWO
THREADS
EACH
TRY
TO
ADD
AN
ELEMENT
TO
A
DOUBLY
LINKED
LIST
AT
THE
SAME
TIME
ONE
ELEMENT
MAY
BE
LOST
OR
THE
LIST
MAY
END
UP
MALFORMED
THE
ISSUES
RAISED
AND
THE
TECHNIQUES
USED
IN
THE
SYNCHRONIZATION
OF
THREADS
ARE
IN
GENERAL
THE
SAME
AS
FOR
THE
SYNCHRONIZATION
OF
PROCESSES
THESE
ISSUES
AND
TECHNIQUES
ARE
THE
SUBJECT
OF
CHAPTERS
AND
TYPES
OF
THREADS
USER
LEVEL
AND
KERNEL
LEVEL
THREADS
THERE
ARE
TWO
BROAD
CATEGORIES
OF
THREAD
IMPLEMENTATION
USER
LEVEL
THREADS
ULTS
AND
KERNEL
LEVEL
THREADS
KLTS
THE
LATTER
ARE
ALSO
REFERRED
TO
IN
THE
LIT
ERATURE
AS
KERNEL
SUPPORTED
THREADS
OR
LIGHTWEIGHT
PROCESSES
USER
LEVEL
THREADS
IN
A
PURE
ULT
FACILITY
ALL
OF
THE
WORK
OF
THREAD
MANAGEMENT
IS
DONE
BY
THE
APPLICATION
AND
THE
KERNEL
IS
NOT
AWARE
OF
THE
EXISTENCE
OF
THREADS
FIGURE
ILLUSTRATES
THE
PURE
ULT
APPROACH
ANY
APPLICATION
CAN
BE
THIS
EXAMPLE
THREAD
C
BEGINS
TO
RUN
AFTER
THREAD
A
EXHAUSTS
ITS
TIME
QUANTUM
EVEN
THOUGH
THREAD
B
IS
ALSO
READY
TO
RUN
THE
CHOICE
BETWEEN
B
AND
C
IS
A
SCHEDULING
DECISION
A
TOPIC
COVERED
IN
PART
FOUR
ACRONYMS
ULT
AND
KLT
ARE
NOT
WIDELY
USED
BUT
ARE
INTRODUCED
FOR
CONCISENESS
TYPES
OF
THREADS
A
PURE
USER
LEVEL
B
PURE
KERNEL
LEVEL
C
COMBINED
USER
LEVEL
THREAD
KERNEL
LEVEL
THREAD
PROCESS
FIGURE
USER
LEVEL
AND
KERNEL
LEVEL
THREADS
PROGRAMMED
TO
BE
MULTITHREADED
BY
USING
A
THREADS
LIBRARY
WHICH
IS
A
PACKAGE
OF
ROUTINES
FOR
ULT
MANAGEMENT
THE
THREADS
LIBRARY
CONTAINS
CODE
FOR
CREATING
AND
DESTROYING
THREADS
FOR
PASSING
MESSAGES
AND
DATA
BETWEEN
THREADS
FOR
SCHEDULING
THREAD
EXECUTION
AND
FOR
SAVING
AND
RESTORING
THREAD
CONTEXTS
BY
DEFAULT
AN
APPLICATION
BEGINS
WITH
A
SINGLE
THREAD
AND
BEGINS
RUNNING
IN
THAT
THREAD
THIS
APPLICATION
AND
ITS
THREAD
ARE
ALLOCATED
TO
A
SINGLE
PROCESS
MAN
AGED
BY
THE
KERNEL
AT
ANY
TIME
THAT
THE
APPLICATION
IS
RUNNING
THE
PROCESS
IS
IN
THE
RUNNING
STATE
THE
APPLICATION
MAY
SPAWN
A
NEW
THREAD
TO
RUN
WITHIN
THE
SAME
PROCESS
SPAWNING
IS
DONE
BY
INVOKING
THE
SPAWN
UTILITY
IN
THE
THREADS
LIBRARY
CONTROL
IS
PASSED
TO
THAT
UTILITY
BY
A
PROCEDURE
CALL
THE
THREADS
LIBRARY
CREATES
A
DATA
STRUCTURE
FOR
THE
NEW
THREAD
AND
THEN
PASSES
CONTROL
TO
ONE
OF
THE
THREADS
WITHIN
THIS
PROCESS
THAT
IS
IN
THE
READY
STATE
USING
SOME
SCHEDULING
ALGORITHM
WHEN
CONTROL
IS
PASSED
TO
THE
LIBRARY
THE
CONTEXT
OF
THE
CURRENT
THREAD
IS
SAVED
AND
WHEN
CONTROL
IS
PASSED
FROM
THE
LIBRARY
TO
A
THREAD
THE
CONTEXT
OF
THAT
THREAD
IS
RESTORED
THE
CONTEXT
ESSENTIALLY
CONSISTS
OF
THE
CONTENTS
OF
USER
REGISTERS
THE
PROGRAM
COUNTER
AND
STACK
POINTERS
ALL
OF
THE
ACTIVITY
DESCRIBED
IN
THE
PRECEDING
PARAGRAPH
TAKES
PLACE
IN
USER
SPACE
AND
WITHIN
A
SINGLE
PROCESS
THE
KERNEL
IS
UNAWARE
OF
THIS
ACTIVITY
THE
KERNEL
CONTINUES
TO
SCHEDULE
THE
PROCESS
AS
A
UNIT
AND
ASSIGNS
A
SINGLE
EXECUTION
STATE
READY
RUNNING
BLOCKED
ETC
TO
THAT
PROCESS
THE
FOLLOWING
EXAMPLES
SHOULD
CLARIFY
THE
RELATIONSHIP
BETWEEN
THREAD
SCHEDULING
AND
PROCESS
SCHEDULING
SUPPOSE
THAT
PROCESS
B
IS
EXECUTING
IN
ITS
THREAD
THE
STATES
OF
THE
PROCESS
AND
TWO
ULTS
THAT
ARE
PART
OF
THE
PROCESS
ARE
SHOWN
IN
FIGURE
EACH
OF
THE
FOLLOWING
IS
A
POSSIBLE
OCCURRENCE
THE
APPLICATION
EXECUTING
IN
THREAD
MAKES
A
SYSTEM
CALL
THAT
BLOCKS
B
FOR
EXAMPLE
AN
I
O
CALL
IS
MADE
THIS
CAUSES
CONTROL
TO
TRANSFER
TO
THE
KERNEL
THE
KERNEL
INVOKES
THE
I
O
ACTION
PLACES
PROCESS
B
IN
THE
BLOCKED
STATE
AND
SWITCHES
TO
ANOTHER
PROCESS
MEANWHILE
ACCORDING
TO
THE
DATA
STRUCTURE
MAINTAINED
BY
A
READY
THREAD
BLOCKED
RUNNING
PROCESS
B
READY
BLOCKED
THREAD
READY
BLOCKED
RUNNING
RUNNING
B
READY
THREAD
BLOCKED
RUNNING
PROCESS
B
READY
BLOCKED
THREAD
READY
BLOCKED
RUNNING
RUNNING
C
READY
THREAD
BLOCKED
RUNNING
PROCESS
B
READY
BLOCKED
THREAD
READY
BLOCKED
RUNNING
RUNNING
D
READY
THREAD
BLOCKED
RUNNING
PROCESS
B
READY
BLOCKED
THREAD
READY
BLOCKED
RUNNING
RUNNING
FIGURE
EXAMPLES
OF
THE
RELATIONSHIPS
BETWEEN
USER
LEVEL
THREAD
STATES
AND
PROCESS
STATES
TYPES
OF
THREADS
THE
THREADS
LIBRARY
THREAD
OF
PROCESS
B
IS
STILL
IN
THE
RUNNING
STATE
IT
IS
IMPOR
TANT
TO
NOTE
THAT
THREAD
IS
NOT
ACTUALLY
RUNNING
IN
THE
SENSE
OF
BEING
EXECUTED
ON
A
PROCESSOR
BUT
IT
IS
PERCEIVED
AS
BEING
IN
THE
RUNNING
STATE
BY
THE
THREADS
LIBRARY
THE
CORRESPONDING
STATE
DIAGRAMS
ARE
SHOWN
IN
FIGURE
A
CLOCK
INTERRUPT
PASSES
CONTROL
TO
THE
KERNEL
AND
THE
KERNEL
DETERMINES
THAT
THE
CURRENTLY
RUNNING
PROCESS
B
HAS
EXHAUSTED
ITS
TIME
SLICE
THE
KERNEL
PLACES
PROCESS
B
IN
THE
READY
STATE
AND
SWITCHES
TO
ANOTHER
PROCESS
MEANWHILE
ACCORDING
TO
THE
DATA
STRUCTURE
MAINTAINED
BY
THE
THREADS
LIBRARY
THREAD
OF
PROCESS
B
IS
STILL
IN
THE
RUNNING
STATE
THE
CORRESPONDING
STATE
DIAGRAMS
ARE
SHOWN
IN
FIGURE
THREAD
HAS
REACHED
A
POINT
WHERE
IT
NEEDS
SOME
ACTION
PERFORMED
BY
THREAD
OF
PROCESS
B
THREAD
ENTERS
A
BLOCKED
STATE
AND
THREAD
TRANSITIONS
FROM
READY
TO
RUNNING
THE
PROCESS
ITSELF
REMAINS
IN
THE
RUNNING
STATE
THE
CORRESPONDING
STATE
DIAGRAMS
ARE
SHOWN
IN
FIGURE
IN
CASES
AND
FIGURES
AND
WHEN
THE
KERNEL
SWITCHES
CONTROL
BACK
TO
PROCESS
B
EXECUTION
RESUMES
IN
THREAD
ALSO
NOTE
THAT
A
PROCESS
CAN
BE
INTERRUPTED
EITHER
BY
EXHAUSTING
ITS
TIME
SLICE
OR
BY
BEING
PREEMPTED
BY
A
HIGHER
PRIORITY
PROCESS
WHILE
IT
IS
EXECUTING
CODE
IN
THE
THREADS
LIBRARY
THUS
A
PROCESS
MAY
BE
IN
THE
MIDST
OF
A
THREAD
SWITCH
FROM
ONE
THREAD
TO
ANOTHER
WHEN
INTER
RUPTED
WHEN
THAT
PROCESS
IS
RESUMED
EXECUTION
CONTINUES
WITHIN
THE
THREADS
LIBRARY
WHICH
COMPLETES
THE
THREAD
SWITCH
AND
TRANSFERS
CONTROL
TO
ANOTHER
THREAD
WITHIN
THAT
PROCESS
THERE
ARE
A
NUMBER
OF
ADVANTAGES
TO
THE
USE
OF
ULTS
INSTEAD
OF
KLTS
INCLUDING
THE
FOLLOWING
THREAD
SWITCHING
DOES
NOT
REQUIRE
KERNEL
MODE
PRIVILEGES
BECAUSE
ALL
OF
THE
THREAD
MANAGEMENT
DATA
STRUCTURES
ARE
WITHIN
THE
USER
ADDRESS
SPACE
OF
A
SINGLE
PROCESS
THEREFORE
THE
PROCESS
DOES
NOT
SWITCH
TO
THE
KERNEL
MODE
TO
DO
THREAD
MANAGEMENT
THIS
SAVES
THE
OVERHEAD
OF
TWO
MODE
SWITCHES
USER
TO
KERNEL
KERNEL
BACK
TO
USER
SCHEDULING
CAN
BE
APPLICATION
SPECIFIC
ONE
APPLICATION
MAY
BENEFIT
MOST
FROM
A
SIMPLE
ROUND
ROBIN
SCHEDULING
ALGORITHM
WHILE
ANOTHER
MIGHT
BENEFIT
FROM
A
PRIORITY
BASED
SCHEDULING
ALGORITHM
THE
SCHEDULING
ALGORITHM
CAN
BE
TAILORED
TO
THE
APPLICATION
WITHOUT
DISTURBING
THE
UNDERLYING
OS
SCHEDULER
ULTS
CAN
RUN
ON
ANY
OS
NO
CHANGES
ARE
REQUIRED
TO
THE
UNDERLYING
KERNEL
TO
SUPPORT
ULTS
THE
THREADS
LIBRARY
IS
A
SET
OF
APPLICATION
LEVEL
FUNCTIONS
SHARED
BY
ALL
APPLICATIONS
THERE
ARE
TWO
DISTINCT
DISADVANTAGES
OF
ULTS
COMPARED
TO
KLTS
IN
A
TYPICAL
OS
MANY
SYSTEM
CALLS
ARE
BLOCKING
AS
A
RESULT
WHEN
A
ULT
EXECUTES
A
SYSTEM
CALL
NOT
ONLY
IS
THAT
THREAD
BLOCKED
BUT
ALSO
ALL
OF
THE
THREADS
WITHIN
THE
PROCESS
ARE
BLOCKED
IN
A
PURE
ULT
STRATEGY
A
MULTITHREADED
APPLICATION
CANNOT
TAKE
ADVANTAGE
OF
MULTIPROCESSING
A
KERNEL
ASSIGNS
ONE
PROCESS
TO
ONLY
ONE
PROCESSOR
AT
A
TIME
THEREFORE
ONLY
A
SINGLE
THREAD
WITHIN
A
PROCESS
CAN
EXECUTE
AT
A
TIME
IN
EFFECT
WE
HAVE
APPLICATION
LEVEL
MULTIPROGRAMMING
WITHIN
A
SINGLE
PROCESS
CHAPTER
THREADS
WHILE
THIS
MULTIPROGRAMMING
CAN
RESULT
IN
A
SIGNIFICANT
SPEEDUP
OF
THE
APPLI
CATION
THERE
ARE
APPLICATIONS
THAT
WOULD
BENEFIT
FROM
THE
ABILITY
TO
EXECUTE
PORTIONS
OF
CODE
SIMULTANEOUSLY
THERE
ARE
WAYS
TO
WORK
AROUND
THESE
TWO
PROBLEMS
FOR
EXAMPLE
BOTH
PROB
LEMS
CAN
BE
OVERCOME
BY
WRITING
AN
APPLICATION
AS
MULTIPLE
PROCESSES
RATHER
THAN
MULTIPLE
THREADS
BUT
THIS
APPROACH
ELIMINATES
THE
MAIN
ADVANTAGE
OF
THREADS
EACH
SWITCH
BECOMES
A
PROCESS
SWITCH
RATHER
THAN
A
THREAD
SWITCH
RESULTING
IN
MUCH
GREATER
OVERHEAD
ANOTHER
WAY
TO
OVERCOME
THE
PROBLEM
OF
BLOCKING
THREADS
IS
TO
USE
A
TECH
NIQUE
REFERRED
TO
AS
JACKETING
THE
PURPOSE
OF
JACKETING
IS
TO
CONVERT
A
BLOCKING
SYSTEM
CALL
INTO
A
NONBLOCKING
SYSTEM
CALL
FOR
EXAMPLE
INSTEAD
OF
DIRECTLY
CALLING
A
SYSTEM
I
O
ROUTINE
A
THREAD
CALLS
AN
APPLICATION
LEVEL
I
O
JACKET
ROUTINE
WITHIN
THIS
JACKET
ROUTINE
IS
CODE
THAT
CHECKS
TO
DETERMINE
IF
THE
I
O
DEVICE
IS
BUSY
IF
IT
IS
THE
THREAD
ENTERS
THE
BLOCKED
STATE
AND
PASSES
CONTROL
THROUGH
THE
THREADS
LIBRARY
TO
ANOTHER
THREAD
WHEN
THIS
THREAD
LATER
IS
GIVEN
CONTROL
AGAIN
THE
JACKET
ROUTINE
CHECKS
THE
I
O
DEVICE
AGAIN
KERNEL
LEVEL
THREADS
IN
A
PURE
KLT
FACILITY
ALL
OF
THE
WORK
OF
THREAD
MANAGEMENT
IS
DONE
BY
THE
KERNEL
THERE
IS
NO
THREAD
MANAGEMENT
CODE
IN
THE
APPLICATION
LEVEL
SIMPLY
AN
APPLICATION
PROGRAMMING
INTERFACE
API
TO
THE
KERNEL
THREAD
FACILITY
WINDOWS
IS
AN
EXAMPLE
OF
THIS
APPROACH
FIGURE
DEPICTS
THE
PURE
KLT
APPROACH
THE
KERNEL
MAINTAINS
CONTEXT
INFORMATION
FOR
THE
PROCESS
AS
A
WHOLE
AND
FOR
INDIVIDUAL
THREADS
WITHIN
THE
PROCESS
SCHEDULING
BY
THE
KERNEL
IS
DONE
ON
A
THREAD
BASIS
THIS
APPROACH
OVERCOMES
THE
TWO
PRINCIPAL
DRAWBACKS
OF
THE
ULT
APPROACH
FIRST
THE
KERNEL
CAN
SIMULTANEOUSLY
SCHEDULE
MULTIPLE
THREADS
FROM
THE
SAME
PROCESS
ON
MULTIPLE
PROCESSORS
SECOND
IF
ONE
THREAD
IN
A
PROCESS
IS
BLOCKED
THE
KERNEL
CAN
SCHEDULE
ANOTHER
THREAD
OF
THE
SAME
PROCESS
ANOTHER
ADVANTAGE
OF
THE
KLT
APPROACH
IS
THAT
KERNEL
ROUTINES
THEMSELVES
CAN
BE
MULTITHREADED
THE
PRINCIPAL
DISADVANTAGE
OF
THE
KLT
APPROACH
COMPARED
TO
THE
ULT
APPROACH
IS
THAT
THE
TRANSFER
OF
CONTROL
FROM
ONE
THREAD
TO
ANOTHER
WITHIN
THE
SAME
PROCESS
REQUIRES
A
MODE
SWITCH
TO
THE
KERNEL
TO
ILLUSTRATE
THE
DIFFERENCES
TABLE
SHOWS
THE
RESULTS
OF
MEASUREMENTS
TAKEN
ON
A
UNIPROCESSOR
VAX
COMPUTER
RUNNING
A
UNIX
LIKE
OS
THE
TWO
BENCHMARKS
ARE
AS
FOLLOWS
NULL
FORK
THE
TIME
TO
CREATE
SCHEDULE
EXECUTE
AND
COMPLETE
A
PROCESS
THREAD
THAT
INVOKES
THE
NULL
PROCEDURE
I
E
THE
OVERHEAD
OF
FORKING
A
PROCESS
THREAD
AND
SIGNAL
WAIT
THE
TIME
FOR
A
PROCESS
THREAD
TO
SIGNAL
A
WAITING
PROCESS
THREAD
AND
THEN
WAIT
ON
A
CONDITION
I
E
THE
OVERHEAD
OF
SYNCHRONIZING
TWO
PROCESSES
THREADS
TOGETHER
WE
SEE
THAT
THERE
IS
AN
ORDER
OF
MAGNITUDE
OR
MORE
OF
DIFFERENCE
BETWEEN
ULTS
AND
KLTS
AND
SIMILARLY
BETWEEN
KLTS
AND
PROCESSES
TABLE
THREAD
AND
PROCESS
OPERATION
LATENCIES
ΜS
OPERATION
USER
LEVEL
THREADS
KERNEL
LEVEL
THREADS
PROCESSES
NULL
FORK
SIGNAL
WAIT
TYPES
OF
THREADS
THUS
ON
THE
FACE
OF
IT
WHILE
THERE
IS
A
SIGNIFICANT
SPEEDUP
BY
USING
KLT
MUL
TITHREADING
COMPARED
TO
SINGLE
THREADED
PROCESSES
THERE
IS
AN
ADDITIONAL
SIGNIFI
CANT
SPEEDUP
BY
USING
ULTS
HOWEVER
WHETHER
OR
NOT
THE
ADDITIONAL
SPEEDUP
IS
REALIZED
DEPENDS
ON
THE
NATURE
OF
THE
APPLICATIONS
INVOLVED
IF
MOST
OF
THE
THREAD
SWITCHES
IN
AN
APPLICATION
REQUIRE
KERNEL
MODE
ACCESS
THEN
A
ULT
BASED
SCHEME
MAY
NOT
PERFORM
MUCH
BETTER
THAN
A
KLT
BASED
SCHEME
COMBINED
APPROACHES
SOME
OPERATING
SYSTEMS
PROVIDE
A
COMBINED
ULT
KLT
FACILITY
FIGURE
IN
A
COMBINED
SYSTEM
THREAD
CREATION
IS
DONE
COMPLETELY
IN
USER
SPACE
AS
IS
THE
BULK
OF
THE
SCHEDULING
AND
SYNCHRONIZATION
OF
THREADS
WITHIN
AN
APPLICATION
THE
MULTIPLE
ULTS
FROM
A
SINGLE
APPLICATION
ARE
MAPPED
ONTO
SOME
SMALLER
OR
EQUAL
NUMBER
OF
KLTS
THE
PROGRAMMER
MAY
ADJUST
THE
NUMBER
OF
KLTS
FOR
A
PARTICULAR
APPLICATION
AND
PROCESSOR
TO
ACHIEVE
THE
BEST
OVERALL
RESULTS
IN
A
COMBINED
APPROACH
MULTIPLE
THREADS
WITHIN
THE
SAME
APPLICATION
CAN
RUN
IN
PARALLEL
ON
MULTIPLE
PROCESSORS
AND
A
BLOCKING
SYSTEM
CALL
NEED
NOT
BLOCK
THE
ENTIRE
PROCESS
IF
PROPERLY
DESIGNED
THIS
APPROACH
SHOULD
COMBINE
THE
ADVAN
TAGES
OF
THE
PURE
ULT
AND
KLT
APPROACHES
WHILE
MINIMIZING
THE
DISADVANTAGES
SOLARIS
IS
A
GOOD
EXAMPLE
OF
AN
OS
USING
THIS
COMBINED
APPROACH
THE
CURRENT
SOLARIS
VERSION
LIMITS
THE
ULT
KLT
RELATIONSHIP
TO
BE
ONE
TO
ONE
OTHER
ARRANGEMENTS
AS
WE
HAVE
SAID
THE
CONCEPTS
OF
RESOURCE
ALLOCATION
AND
DISPATCHING
UNIT
HAVE
TRADITIONALLY
BEEN
EMBODIED
IN
THE
SINGLE
CONCEPT
OF
THE
PROCESS
THAT
IS
AS
A
RELATIONSHIP
BETWEEN
THREADS
AND
PROCESSES
RECENTLY
THERE
HAS
BEEN
MUCH
INTER
EST
IN
PROVIDING
FOR
MULTIPLE
THREADS
WITHIN
A
SINGLE
PROCESS
WHICH
IS
A
MANY
TO
ONE
RELATIONSHIP
HOWEVER
AS
TABLE
SHOWS
THE
OTHER
TWO
COMBINATIONS
HAVE
ALSO
BEEN
INVESTIGATED
NAMELY
A
MANY
TO
MANY
RELATIONSHIP
AND
A
ONE
TO
MANY
RELATIONSHIP
MANY
TO
MANY
RELATIONSHIP
THE
IDEA
OF
HAVING
A
MANY
TO
MANY
RELATIONSHIP
BETWEEN
THREADS
AND
PROCESSES
HAS
BEEN
EXPLORED
IN
THE
EXPERIMENTAL
OPERATING
SYSTEM
TRIX
IN
TRIX
THERE
ARE
THE
CONCEPTS
OF
DOMAIN
TABLE
RELATIONSHIP
BETWEEN
THREADS
AND
PROCESSES
THREADS
PROCESSES
DESCRIPTION
EXAMPLE
SYSTEMS
EACH
THREAD
OF
EXECUTION
IS
A
UNIQUE
PROCESS
WITH
ITS
OWN
ADDRESS
SPACE
AND
RESOURCES
TRADITIONAL
UNIX
IMPLEMENTATIONS
M
A
PROCESS
DEFINES
AN
ADDRESS
SPACE
AND
DYNAMIC
RESOURCE
OWNERSHIP
MULTIPLE
THREADS
MAY
BE
CREATED
AND
EXECUTED
WITHIN
THAT
PROCESS
WINDOWS
NT
SOLARIS
LINUX
OS
OS
MACH
M
A
THREAD
MAY
MIGRATE
FROM
ONE
PROCESS
ENVIRONMENT
TO
ANOTHER
THIS
ALLOWS
A
THREAD
TO
BE
EASILY
MOVED
AMONG
DISTINCT
SYSTEMS
RA
CLOUDS
EMERALD
M
N
COMBINES
ATTRIBUTES
OF
M
AND
M
CASES
TRIX
CHAPTER
THREADS
AND
THREAD
A
DOMAIN
IS
A
STATIC
ENTITY
CONSISTING
OF
AN
ADDRESS
SPACE
AND
PORTS
THROUGH
WHICH
MESSAGES
MAY
BE
SENT
AND
RECEIVED
A
THREAD
IS
A
SINGLE
EXECUTION
PATH
WITH
AN
EXECUTION
STACK
PROCESSOR
STATE
AND
SCHEDULING
INFORMATION
AS
WITH
THE
MULTITHREADING
APPROACHES
DISCUSSED
SO
FAR
MULTIPLE
THREADS
MAY
EXECUTE
IN
A
SINGLE
DOMAIN
PROVIDING
THE
EFFICIENCY
GAINS
DISCUSSED
EARLIER
HOWEVER
IT
IS
ALSO
POSSIBLE
FOR
A
SINGLE
USER
ACTIVITY
OR
APPLICATION
TO
BE
PER
FORMED
IN
MULTIPLE
DOMAINS
IN
THIS
CASE
A
THREAD
EXISTS
THAT
CAN
MOVE
FROM
ONE
DOMAIN
TO
ANOTHER
THE
USE
OF
A
SINGLE
THREAD
IN
MULTIPLE
DOMAINS
SEEMS
PRIMARILY
MOTIVATED
BY
A
DESIRE
TO
PROVIDE
STRUCTURING
TOOLS
FOR
THE
PROGRAMMER
FOR
EXAMPLE
CONSIDER
A
PROGRAM
THAT
MAKES
USE
OF
AN
I
O
SUBPROGRAM
IN
A
MULTIPROGRAMMING
ENVIRON
MENT
THAT
ALLOWS
USER
SPAWNED
PROCESSES
THE
MAIN
PROGRAM
COULD
GENERATE
A
NEW
PROCESS
TO
HANDLE
I
O
AND
THEN
CONTINUE
TO
EXECUTE
HOWEVER
IF
THE
FUTURE
PROGRESS
OF
THE
MAIN
PROGRAM
DEPENDS
ON
THE
OUTCOME
OF
THE
I
O
OPERATION
THEN
THE
MAIN
PROGRAM
WILL
HAVE
TO
WAIT
FOR
THE
OTHER
I
O
PROGRAM
TO
FINISH
THERE
ARE
SEVERAL
WAYS
TO
IMPLEMENT
THIS
APPLICATION
THE
ENTIRE
PROGRAM
CAN
BE
IMPLEMENTED
AS
A
SINGLE
PROCESS
THIS
IS
A
REA
SONABLE
AND
STRAIGHTFORWARD
SOLUTION
THERE
ARE
DRAWBACKS
RELATED
TO
MEMORY
MANAGEMENT
THE
PROCESS
AS
A
WHOLE
MAY
REQUIRE
CONSIDERABLE
MAIN
MEMORY
TO
EXECUTE
EFFICIENTLY
WHEREAS
THE
I
O
SUBPROGRAM
REQUIRES
A
RELATIVELY
SMALL
ADDRESS
SPACE
TO
BUFFER
I
O
AND
TO
HANDLE
THE
RELATIVELY
SMALL
AMOUNT
OF
PROGRAM
CODE
BECAUSE
THE
I
O
PROGRAM
EXECUTES
IN
THE
ADDRESS
SPACE
OF
THE
LARGER
PROGRAM
EITHER
THE
ENTIRE
PROCESS
MUST
REMAIN
IN
MAIN
MEMORY
DURING
THE
I
O
OPERATION
OR
THE
I
O
OPERATION
IS
SUBJECT
TO
SWAPPING
THIS
MEMORY
MANAGEMENT
EFFECT
WOULD
ALSO
EXIST
IF
THE
MAIN
PROGRAM
AND
THE
I
O
SUBPROGRAM
WERE
IMPLEMENTED
AS
TWO
THREADS
IN
THE
SAME
ADDRESS
SPACE
THE
MAIN
PROGRAM
AND
I
O
SUBPROGRAM
CAN
BE
IMPLEMENTED
AS
TWO
SEPARATE
PROCESSES
THIS
INCURS
THE
OVERHEAD
OF
CREATING
THE
SUBORDINATE
PROCESS
IF
THE
I
O
ACTIVITY
IS
FREQUENT
ONE
MUST
EITHER
LEAVE
THE
SUBORDINATE
PROCESS
ALIVE
WHICH
CONSUMES
MANAGEMENT
RESOURCES
OR
FREQUENTLY
CREATE
AND
DESTROY
THE
SUBPROGRAM
WHICH
IS
INEFFICIENT
TREAT
THE
MAIN
PROGRAM
AND
THE
I
O
SUBPROGRAM
AS
A
SINGLE
ACTIVITY
THAT
IS
TO
BE
IMPLEMENTED
AS
A
SINGLE
THREAD
HOWEVER
ONE
ADDRESS
SPACE
DOMAIN
COULD
BE
CREATED
FOR
THE
MAIN
PROGRAM
AND
ONE
FOR
THE
I
O
SUBPROGRAM
THUS
THE
THREAD
CAN
BE
MOVED
BETWEEN
THE
TWO
ADDRESS
SPACES
AS
EXECU
TION
PROCEEDS
THE
OS
CAN
MANAGE
THE
TWO
ADDRESS
SPACES
INDEPENDENTLY
AND
NO
PROCESS
CREATION
OVERHEAD
IS
INCURRED
FURTHERMORE
THE
ADDRESS
SPACE
USED
BY
THE
I
O
SUBPROGRAM
COULD
ALSO
BE
SHARED
BY
OTHER
SIMPLE
I
O
PROGRAMS
THE
EXPERIENCES
OF
THE
TRIX
DEVELOPERS
INDICATE
THAT
THE
THIRD
OPTION
HAS
MERIT
AND
MAY
BE
THE
MOST
EFFECTIVE
SOLUTION
FOR
SOME
APPLICATIONS
ONE
TO
MANY
RELATIONSHIP
IN
THE
FIELD
OF
DISTRIBUTED
OPERATING
SYSTEMS
DESIGNED
TO
CONTROL
DISTRIBUTED
COMPUTER
SYSTEMS
THERE
HAS
BEEN
INTEREST
IN
THE
MULTICORE
AND
MULTITHREADING
CONCEPT
OF
A
THREAD
AS
PRIMARILY
AN
ENTITY
THAT
CAN
MOVE
AMONG
ADDRESS
SPACES
A
NOTABLE
EXAMPLE
OF
THIS
RESEARCH
IS
THE
CLOUDS
OPERATING
SYSTEM
AND
ESPECIALLY
ITS
KERNEL
KNOWN
AS
RA
ANOTHER
EXAMPLE
IS
THE
EMERALD
SYSTEM
A
THREAD
IN
CLOUDS
IS
A
UNIT
OF
ACTIVITY
FROM
THE
USER
PERSPECTIVE
A
PROCESS
IS
A
VIRTUAL
ADDRESS
SPACE
WITH
AN
ASSOCIATED
PROCESS
CONTROL
BLOCK
UPON
CREATION
A
THREAD
STARTS
EXECUTING
IN
A
PROCESS
BY
INVOKING
AN
ENTRY
POINT
TO
A
PROGRAM
IN
THAT
PROCESS
THREADS
MAY
MOVE
FROM
ONE
ADDRESS
SPACE
TO
ANOTHER
AND
ACTUALLY
SPAN
COMPUTER
BOUNDARIES
I
E
MOVE
FROM
ONE
COMPUTER
TO
ANOTHER
AS
A
THREAD
MOVES
IT
MUST
CARRY
WITH
IT
CERTAIN
INFORMATION
SUCH
AS
THE
CONTROLLING
TERMINAL
GLOBAL
PARAMETERS
AND
SCHEDULING
GUIDANCE
E
G
PRIORITY
THE
CLOUDS
APPROACH
PROVIDES
AN
EFFECTIVE
WAY
OF
INSULATING
BOTH
USERS
AND
PROGRAMMERS
FROM
THE
DETAILS
OF
THE
DISTRIBUTED
ENVIRONMENT
A
USER
ACTIVITY
MAY
BE
REPRESENTED
AS
A
SINGLE
THREAD
AND
THE
MOVEMENT
OF
THAT
THREAD
AMONG
COMPUT
ERS
MAY
BE
DICTATED
BY
THE
OS
FOR
A
VARIETY
OF
SYSTEM
RELATED
REASONS
SUCH
AS
THE
NEED
TO
ACCESS
A
REMOTE
RESOURCE
AND
LOAD
BALANCING
MULTICORE
AND
MULTITHREADING
THE
USE
OF
A
MULTICORE
SYSTEM
TO
SUPPORT
A
SINGLE
APPLICATION
WITH
MULTIPLE
THREADS
SUCH
AS
MIGHT
OCCUR
ON
A
WORKSTATION
A
VIDEO
GAME
CONSOLE
OR
A
PERSONAL
COMPUTER
RUNNING
A
PROCESSOR
INTENSE
APPLICATION
RAISES
ISSUES
OF
PERFORMANCE
AND
APPLICA
TION
DESIGN
IN
THIS
SECTION
WE
FIRST
LOOK
AT
SOME
OF
THE
PERFORMANCE
IMPLICATIONS
OF
A
MULTITHREADED
APPLICATION
ON
A
MULTICORE
SYSTEM
AND
THEN
DESCRIBE
A
SPECIFIC
EXAMPLE
OF
AN
APPLICATION
DESIGNED
TO
EXPLOIT
MULTICORE
CAPABILITIES
PERFORMANCE
OF
SOFTWARE
ON
MULTICORE
THE
POTENTIAL
PERFORMANCE
BENEFITS
OF
A
MULTICORE
ORGANIZATION
DEPEND
ON
THE
ABILITY
TO
EFFECTIVELY
EXPLOIT
THE
PARALLEL
RESOURCES
AVAILABLE
TO
THE
APPLICATION
LET
US
FOCUS
FIRST
ON
A
SINGLE
APPLICATION
RUNNING
ON
A
MULTICORE
SYSTEM
AMDAHL
LAW
SEE
APPENDIX
E
STATES
THAT
SPEEDUP
TIME
TO
EXECUTE
PROGRAM
ON
A
SINGLE
PROCESSOR
TIME
TO
EXECUTE
PROGRAM
ON
N
PARALLEL
PROCESSORS
F
F
N
THE
LAW
ASSUMES
A
PROGRAM
IN
WHICH
A
FRACTION
F
OF
THE
EXECUTION
TIME
INVOLVES
CODE
THAT
IS
INHERENTLY
SERIAL
AND
A
FRACTION
F
THAT
INVOLVES
CODE
THAT
IS
INFI
NITELY
PARALLELIZABLE
WITH
NO
SCHEDULING
OVERHEAD
THIS
LAW
APPEARS
TO
MAKE
THE
PROSPECT
OF
A
MULTICORE
ORGANIZATION
ATTRACTIVE
BUT
AS
FIGURE
SHOWS
EVEN
A
SMALL
AMOUNT
OF
SERIAL
CODE
HAS
A
NOTICEABLE
IMPACT
IF
ONLY
OF
THE
CODE
IS
INHERENTLY
SERIAL
F
RUNNING
THE
PROGRAM
ON
A
MULTICORE
SYSTEM
WITH
EIGHT
PROCESSORS
YIELDS
A
PERFORMANCE
GAIN
OF
ONLY
A
FACTOR
OF
IN
ADDITION
SOFTWARE
TYPICALLY
INCURS
OVERHEAD
AS
A
RESULT
OF
COMMUNICATION
MOVEMENT
OF
PROCESSES
OR
THREADS
AMONG
ADDRESS
SPACES
OR
THREAD
MIGRATION
ON
DIFFERENT
MACHINES
HAS
BECOME
A
HOT
TOPIC
IN
RECENT
YEARS
CHAPTER
EXPLORES
THIS
TOPIC
CHAPTER
THREADS
NUMBER
OF
PROCESSORS
A
SPEEDUP
WITH
AND
SEQUENTIAL
PORTIONS
NUMBER
OF
PROCESSORS
B
SPEEDUP
WITH
OVERHEADS
FIGURE
PERFORMANCE
EFFECT
OF
MULTIPLE
CORES
AND
DISTRIBUTION
OF
WORK
TO
MULTIPLE
PROCESSORS
AND
CACHE
COHERENCE
OVERHEAD
THIS
RESULTS
IN
A
CURVE
WHERE
PERFORMANCE
PEAKS
AND
THEN
BEGINS
TO
DEGRADE
BECAUSE
OF
THE
INCREASED
BURDEN
OF
THE
OVERHEAD
OF
USING
MULTIPLE
PROCESSORS
FIGURE
FROM
IS
A
REPRESENTATIVE
EXAMPLE
HOWEVER
SOFTWARE
ENGINEERS
HAVE
BEEN
ADDRESSING
THIS
PROBLEM
AND
THERE
ARE
NUMEROUS
APPLICATIONS
IN
WHICH
IT
IS
POSSIBLE
TO
EFFECTIVELY
EXPLOIT
A
MULTICORE
SYS
TEM
REPORTS
ON
A
SET
OF
DATABASE
APPLICATIONS
IN
WHICH
GREAT
ATTENTION
MULTICORE
AND
MULTITHREADING
ORACLE
DSS
WAY
JOIN
TMC
DATA
MINING
DSS
SCAN
AGGS
ORACLE
AD
HOC
INSURANCE
OLTP
NUMBER
OF
CPUS
FIGURE
SCALING
OF
DATABASE
WORKLOADS
ON
MULTIPLE
PROCESSOR
HARDWARE
WAS
PAID
TO
REDUCING
THE
SERIAL
FRACTION
WITHIN
HARDWARE
ARCHITECTURES
OPERATING
SYSTEMS
MIDDLEWARE
AND
THE
DATABASE
APPLICATION
SOFTWARE
FIGURE
SHOWS
THE
RESULT
AS
THIS
EXAMPLE
SHOWS
DATABASE
MANAGEMENT
SYSTEMS
AND
DATABASE
APPLICA
TIONS
ARE
ONE
AREA
IN
WHICH
MULTICORE
SYSTEMS
CAN
BE
USED
EFFECTIVELY
MANY
KINDS
OF
SERVERS
CAN
ALSO
EFFECTIVELY
USE
THE
PARALLEL
MULTICORE
ORGANIZATION
BECAUSE
SERVERS
TYPICALLY
HANDLE
NUMEROUS
RELATIVELY
INDEPENDENT
TRANSACTIONS
IN
PARALLEL
IN
ADDITION
TO
GENERAL
PURPOSE
SERVER
SOFTWARE
A
NUMBER
OF
CLASSES
OF
APPLICA
TIONS
BENEFIT
DIRECTLY
FROM
THE
ABILITY
TO
SCALE
THROUGHPUT
WITH
THE
NUMBER
OF
CORES
LISTS
THE
FOLLOWING
EXAMPLES
MULTITHREADED
NATIVE
APPLICATIONS
MULTITHREADED
APPLICATIONS
ARE
CHARAC
TERIZED
BY
HAVING
A
SMALL
NUMBER
OF
HIGHLY
THREADED
PROCESSES
EXAMPLES
OF
THREADED
APPLICATIONS
INCLUDE
LOTUS
DOMINO
OR
SIEBEL
CRM
CUSTOMER
RELATIONSHIP
MANAGER
MULTIPROCESS
APPLICATIONS
MULTIPROCESS
APPLICATIONS
ARE
CHARACTERIZED
BY
THE
PRESENCE
OF
MANY
SINGLE
THREADED
PROCESSES
EXAMPLES
OF
MULTIPROCESS
APPLICATIONS
INCLUDE
THE
ORACLE
DATABASE
SAP
AND
PEOPLESOFT
JAVA
APPLICATIONS
JAVA
APPLICATIONS
EMBRACE
THREADING
IN
A
FUNDAMENTAL
WAY
NOT
ONLY
DOES
THE
JAVA
LANGUAGE
GREATLY
FACILITATE
MULTITHREADED
APPLICATIONS
BUT
THE
JAVA
VIRTUAL
MACHINE
IS
A
MULTITHREADED
PROCESS
THAT
PROVIDES
SCHED
ULING
AND
MEMORY
MANAGEMENT
FOR
JAVA
APPLICATIONS
JAVA
APPLICATIONS
THAT
CAN
BENEFIT
DIRECTLY
FROM
MULTICORE
RESOURCES
INCLUDE
APPLICATION
SERVERS
SUCH
AS
SUN
JAVA
APPLICATION
SERVER
BEA
WEBLOGIC
IBM
WEBSPHERE
AND
THE
OPEN
SOURCE
TOMCAT
APPLICATION
SERVER
ALL
APPLICATIONS
THAT
USE
A
JAVA
PLATFORM
ENTERPRISE
EDITION
PLATFORM
APPLICATION
SERVER
CAN
IMMEDI
ATELY
BENEFIT
FROM
MULTICORE
TECHNOLOGY
CHAPTER
THREADS
MULTIINSTANCE
APPLICATIONS
EVEN
IF
AN
INDIVIDUAL
APPLICATION
DOES
NOT
SCALE
TO
TAKE
ADVANTAGE
OF
A
LARGE
NUMBER
OF
THREADS
IT
IS
STILL
POSSIBLE
TO
GAIN
FROM
MULTICORE
ARCHITECTURE
BY
RUNNING
MULTIPLE
INSTANCES
OF
THE
APPLICATION
IN
PARALLEL
IF
MULTIPLE
APPLICATION
INSTANCES
REQUIRE
SOME
DEGREE
OF
ISOLATION
VIRTUALIZATION
TECHNOLOGY
FOR
THE
HARDWARE
OF
THE
OPERATING
SYSTEM
CAN
BE
USED
TO
PROVIDE
EACH
OF
THEM
WITH
ITS
OWN
SEPARATE
AND
SECURE
ENVIRONMENT
APPLICATION
EXAMPLE
VALVE
GAME
SOFTWARE
VALVE
IS
AN
ENTERTAINMENT
AND
TECHNOLOGY
COMPANY
THAT
HAS
DEVELOPED
A
NUMBER
OF
POPULAR
GAMES
AS
WELL
AS
THE
SOURCE
ENGINE
ONE
OF
THE
MOST
WIDELY
PLAYED
GAME
ENGINES
AVAILABLE
SOURCE
IS
AN
ANIMATION
ENGINE
USED
BY
VALVE
FOR
ITS
GAMES
AND
LICENSED
FOR
OTHER
GAME
DEVELOPERS
IN
RECENT
YEARS
VALVE
HAS
REPROGRAMMED
THE
SOURCE
ENGINE
SOFTWARE
TO
USE
MULTITHREADING
TO
EXPLOIT
THE
POWER
OF
MULTICORE
PROCESSOR
CHIPS
FROM
INTEL
AND
AMD
THE
REVISED
SOURCE
ENGINE
CODE
PROVIDES
MORE
POWERFUL
SUPPORT
FOR
VALVE
GAMES
SUCH
AS
HALF
LIFE
FROM
VALVE
PERSPECTIVE
THREADING
GRANULARITY
OPTIONS
ARE
DEFINED
AS
FOLLOWS
COARSE
THREADING
INDIVIDUAL
MODULES
CALLED
SYSTEMS
ARE
ASSIGNED
TO
INDIVID
UAL
PROCESSORS
IN
THE
SOURCE
ENGINE
CASE
THIS
WOULD
MEAN
PUTTING
RENDERING
ON
ONE
PROCESSOR
AI
ARTIFICIAL
INTELLIGENCE
ON
ANOTHER
PHYSICS
ON
ANOTHER
AND
SO
ON
THIS
IS
STRAIGHTFORWARD
IN
ESSENCE
EACH
MAJOR
MODULE
IS
SINGLE
THREADED
AND
THE
PRINCIPAL
COORDINATION
INVOLVES
SYNCHRONIZING
ALL
THE
THREADS
WITH
A
TIMELINE
THREAD
FINE
GRAINED
THREADING
MANY
SIMILAR
OR
IDENTICAL
TASKS
ARE
SPREAD
ACROSS
MUL
TIPLE
PROCESSORS
FOR
EXAMPLE
A
LOOP
THAT
ITERATES
OVER
AN
ARRAY
OF
DATA
CAN
BE
SPLIT
UP
INTO
A
NUMBER
OF
SMALLER
PARALLEL
LOOPS
IN
INDIVIDUAL
THREADS
THAT
CAN
BE
SCHEDULED
IN
PARALLEL
HYBRID
THREADING
THIS
INVOLVES
THE
SELECTIVE
USE
OF
FINE
GRAINED
THREADING
FOR
SOME
SYSTEMS
AND
SINGLE
THREADING
FOR
OTHER
SYSTEMS
VALVE
FOUND
THAT
THROUGH
COARSE
THREADING
IT
COULD
ACHIEVE
UP
TO
TWICE
THE
PERFORMANCE
ACROSS
TWO
PROCESSORS
COMPARED
TO
EXECUTING
ON
A
SINGLE
PROCESSOR
BUT
THIS
PERFORMANCE
GAIN
COULD
ONLY
BE
ACHIEVED
WITH
CONTRIVED
CASES
FOR
REAL
WORLD
GAMEPLAY
THE
IMPROVEMENT
WAS
ON
THE
ORDER
OF
A
FACTOR
OF
VALVE
ALSO
FOUND
THAT
EFFECTIVE
USE
OF
FINE
GRAINED
THREADING
WAS
DIFFICULT
THE
TIME
PER
WORK
UNIT
CAN
BE
VARIABLE
AND
MANAGING
THE
TIMELINE
OF
OUTCOMES
AND
CONSEQUENCES
INVOLVED
COMPLEX
PROGRAMMING
VALVE
FOUND
THAT
A
HYBRID
THREADING
APPROACH
WAS
THE
MOST
PROMISING
AND
WOULD
SCALE
THE
BEST
AS
MULTICORE
SYSTEMS
WITH
OR
PROCESSORS
BECAME
AVAILABLE
VALVE
IDENTIFIED
SYSTEMS
THAT
OPERATE
VERY
EFFECTIVELY
BEING
PERMANENTLY
ASSIGNED
TO
A
SINGLE
PROCESSOR
AN
EXAMPLE
IS
SOUND
MIXING
WHICH
HAS
LITTLE
USER
INTERACTION
IS
NOT
CONSTRAINED
BY
THE
FRAME
CONFIGURATION
OF
WINDOWS
AND
WORKS
ON
ITS
OWN
SET
OF
DATA
OTHER
MODULES
SUCH
AS
SCENE
RENDERING
CAN
BE
ORGANIZED
INTO
A
NUMBER
OF
THREADS
SO
THAT
THE
MODULE
CAN
EXECUTE
ON
A
SINGLE
PROCESSOR
BUT
ACHIEVE
GREATER
PERFORMANCE
AS
IT
IS
SPREAD
OUT
OVER
MORE
AND
MORE
PROCESSORS
MULTICORE
AND
MULTITHREADING
FIGURE
HYBRID
THREADING
FOR
RENDERING
MODULE
FIGURE
ILLUSTRATES
THE
THREAD
STRUCTURE
FOR
THE
RENDERING
MODULE
IN
THIS
HIER
ARCHICAL
STRUCTURE
HIGHER
LEVEL
THREADS
SPAWN
LOWER
LEVEL
THREADS
AS
NEEDED
THE
RENDERING
MODULE
RELIES
ON
A
CRITICAL
PART
OF
THE
SOURCE
ENGINE
THE
WORLD
LIST
WHICH
IS
A
DATABASE
REPRESENTATION
OF
THE
VISUAL
ELEMENTS
IN
THE
GAME
WORLD
THE
FIRST
TASK
IS
TO
DETERMINE
WHAT
ARE
THE
AREAS
OF
THE
WORLD
THAT
NEED
TO
BE
RENDERED
THE
NEXT
TASK
IS
TO
DETERMINE
WHAT
OBJECTS
ARE
IN
THE
SCENE
AS
VIEWED
FROM
MULTIPLE
ANGLES
THEN
COMES
THE
PROCESSOR
INTENSIVE
WORK
THE
RENDERING
MODULE
HAS
TO
WORK
OUT
THE
RENDERING
OF
EACH
OBJECT
FROM
MULTIPLE
POINTS
OF
VIEW
SUCH
AS
THE
PLAYER
VIEW
THE
VIEW
OF
TV
MONITORS
AND
THE
POINT
OF
VIEW
OF
REFLECTIONS
IN
WATER
SOME
OF
THE
KEY
ELEMENTS
OF
THE
THREADING
STRATEGY
FOR
THE
RENDERING
MODULE
ARE
LISTED
IN
AND
INCLUDE
THE
FOLLOWING
CONSTRUCT
SCENE
RENDERING
LISTS
FOR
MULTIPLE
SCENES
IN
PARALLEL
E
G
THE
WORLD
AND
ITS
REFLECTION
IN
WATER
OVERLAP
GRAPHICS
SIMULATION
COMPUTE
CHARACTER
BONE
TRANSFORMATIONS
FOR
ALL
CHARACTERS
IN
ALL
SCENES
IN
PARALLEL
ALLOW
MULTIPLE
THREADS
TO
DRAW
IN
PARALLEL
THE
DESIGNERS
FOUND
THAT
SIMPLY
LOCKING
KEY
DATABASES
SUCH
AS
THE
WORLD
LIST
FOR
A
THREAD
WAS
TOO
INEFFICIENT
OVER
OF
THE
TIME
A
THREAD
IS
TRYING
TO
READ
FROM
A
DATA
SET
AND
ONLY
OF
THE
TIME
AT
MOST
IS
SPENT
IN
WRITING
TO
A
DATA
SET
THUS
A
CONCURRENCY
MECHANISM
KNOWN
AS
THE
SINGLE
WRITER
MULTIPLE
READERS
MODEL
WORKS
EFFECTIVELY
CHAPTER
THREADS
WINDOWS
THREAD
AND
SMP
MANAGEMENT
WINDOWS
PROCESS
DESIGN
IS
DRIVEN
BY
THE
NEED
TO
PROVIDE
SUPPORT
FOR
A
VARIETY
OF
OS
ENVIRONMENTS
PROCESSES
SUPPORTED
BY
DIFFERENT
OS
ENVIRONMENTS
DIFFER
IN
A
NUMBER
OF
WAYS
INCLUDING
THE
FOLLOWING
HOW
PROCESSES
ARE
NAMED
WHETHER
THREADS
ARE
PROVIDED
WITHIN
PROCESSES
HOW
PROCESSES
ARE
REPRESENTED
HOW
PROCESS
RESOURCES
ARE
PROTECTED
WHAT
MECHANISMS
ARE
USED
FOR
INTERPROCESS
COMMUNICATION
AND
SYNCHRONIZATION
HOW
PROCESSES
ARE
RELATED
TO
EACH
OTHER
ACCORDINGLY
THE
NATIVE
PROCESS
STRUCTURES
AND
SERVICES
PROVIDED
BY
THE
WINDOWS
KERNEL
ARE
RELATIVELY
SIMPLE
AND
GENERAL
PURPOSE
ALLOWING
EACH
OS
SUBSYSTEM
TO
EMULATE
A
PARTICULAR
PROCESS
STRUCTURE
AND
FUNCTIONALITY
IMPORTANT
CHARACTERISTICS
OF
WINDOWS
PROCESSES
ARE
THE
FOLLOWING
WINDOWS
PROCESSES
ARE
IMPLEMENTED
AS
OBJECTS
A
PROCESS
CAN
BE
CREATED
AS
NEW
PROCESS
OR
AS
A
COPY
OF
AN
EXISTING
PROCESS
AN
EXECUTABLE
PROCESS
MAY
CONTAIN
ONE
OR
MORE
THREADS
BOTH
PROCESS
AND
THREAD
OBJECTS
HAVE
BUILT
IN
SYNCHRONIZATION
CAPABILITIES
FIGURE
BASED
ON
ONE
IN
ILLUSTRATES
THE
WAY
IN
WHICH
A
PROCESS
RELATES
TO
THE
RESOURCES
IT
CONTROLS
OR
USES
EACH
PROCESS
IS
ASSIGNED
A
SECURITY
ACCESS
FIGURE
A
WINDOWS
PROCESS
AND
ITS
RESOURCES
WINDOWS
THREAD
AND
SMP
MANAGEMENT
TOKEN
CALLED
THE
PRIMARY
TOKEN
OF
THE
PROCESS
WHEN
A
USER
FIRST
LOGS
ON
WINDOWS
CREATES
AN
ACCESS
TOKEN
THAT
INCLUDES
THE
SECURITY
ID
FOR
THE
USER
EVERY
PROCESS
THAT
IS
CREATED
BY
OR
RUNS
ON
BEHALF
OF
THIS
USER
HAS
A
COPY
OF
THIS
ACCESS
TOKEN
WINDOWS
USES
THE
TOKEN
TO
VALIDATE
THE
USER
ABILITY
TO
ACCESS
SECURED
OBJECTS
OR
TO
PERFORM
RESTRICTED
FUNCTIONS
ON
THE
SYSTEM
AND
ON
SECURED
OBJECTS
THE
ACCESS
TOKEN
CONTROLS
WHETHER
THE
PROCESS
CAN
CHANGE
ITS
OWN
ATTRIBUTES
IN
THIS
CASE
THE
PROCESS
DOES
NOT
HAVE
A
HANDLE
OPENED
TO
ITS
ACCESS
TOKEN
IF
THE
PROCESS
ATTEMPTS
TO
OPEN
SUCH
A
HAN
DLE
THE
SECURITY
SYSTEM
DETERMINES
WHETHER
THIS
IS
PERMITTED
AND
THEREFORE
WHETHER
THE
PROCESS
MAY
CHANGE
ITS
OWN
ATTRIBUTES
ALSO
RELATED
TO
THE
PROCESS
IS
A
SERIES
OF
BLOCKS
THAT
DEFINE
THE
VIRTUAL
ADDRESS
SPACE
CURRENTLY
ASSIGNED
TO
THIS
PROCESS
THE
PROCESS
CANNOT
DIRECTLY
MODIFY
THESE
STRUCTURES
BUT
MUST
RELY
ON
THE
VIRTUAL
MEMORY
MANAGER
WHICH
PROVIDES
A
MEMORY
ALLOCATION
SERVICE
FOR
THE
PROCESS
FINALLY
THE
PROCESS
INCLUDES
AN
OBJECT
TABLE
WITH
HANDLES
TO
OTHER
OBJECTS
KNOWN
TO
THIS
PROCESS
FIGURE
SHOWS
A
SINGLE
THREAD
IN
ADDITION
THE
PROCESS
HAS
ACCESS
TO
A
FILE
OBJECT
AND
TO
A
SECTION
OBJECT
THAT
DEFINES
A
SECTION
OF
SHARED
MEMORY
PROCESS
AND
THREAD
OBJECTS
THE
OBJECT
ORIENTED
STRUCTURE
OF
WINDOWS
FACILITATES
THE
DEVELOPMENT
OF
A
GEN
ERAL
PURPOSE
PROCESS
FACILITY
WINDOWS
MAKES
USE
OF
TWO
TYPES
OF
PROCESS
RELATED
OBJECTS
PROCESSES
AND
THREADS
A
PROCESS
IS
AN
ENTITY
CORRESPONDING
TO
A
USER
JOB
OR
APPLICATION
THAT
OWNS
RESOURCES
SUCH
AS
MEMORY
AND
OPEN
FILES
A
THREAD
IS
A
DISPATCHABLE
UNIT
OF
WORK
THAT
EXECUTES
SEQUENTIALLY
AND
IS
INTERRUPTIBLE
SO
THAT
THE
PROCESSOR
CAN
TURN
TO
ANOTHER
THREAD
EACH
WINDOWS
PROCESS
IS
REPRESENTED
BY
AN
OBJECT
WHOSE
GENERAL
STRUCTURE
IS
SHOWN
IN
FIGURE
EACH
PROCESS
IS
DEFINED
BY
A
NUMBER
OF
ATTRIBUTES
AND
ENCAPSULATES
A
NUMBER
OF
ACTIONS
OR
SERVICES
THAT
IT
MAY
PERFORM
A
PROCESS
WILL
PERFORM
A
SERVICE
WHEN
CALLED
UPON
THROUGH
A
SET
OF
PUBLISHED
INTERFACE
METHODS
WHEN
WINDOWS
CREATES
A
NEW
PROCESS
IT
USES
THE
OBJECT
CLASS
OR
TYPE
DEFINED
FOR
THE
WINDOWS
PROCESS
AS
A
TEMPLATE
TO
GENERATE
A
NEW
OBJECT
INSTANCE
AT
THE
TIME
OF
CREATION
ATTRIBUTE
VALUES
ARE
ASSIGNED
TABLE
GIVES
A
BRIEF
DEFINITION
OF
EACH
OF
THE
OBJECT
ATTRIBUTES
FOR
A
PROCESS
OBJECT
A
WINDOWS
PROCESS
MUST
CONTAIN
AT
LEAST
ONE
THREAD
TO
EXECUTE
THAT
THREAD
MAY
THEN
CREATE
OTHER
THREADS
IN
A
MULTIPROCESSOR
SYSTEM
MULTIPLE
THREADS
FROM
THE
SAME
PROCESS
MAY
EXECUTE
IN
PARALLEL
FIGURE
DEPICTS
THE
OBJECT
STRUCTURE
FOR
A
THREAD
OBJECT
AND
TABLE
DEFINES
THE
THREAD
OBJECT
ATTRIBUTES
NOTE
THAT
SOME
OF
THE
ATTRIBUTES
OF
A
THREAD
RESEMBLE
THOSE
OF
A
PROCESS
IN
THOSE
CASES
THE
THREAD
ATTRIBUTE
VALUE
IS
DERIVED
FROM
THE
PROCESS
ATTRIBUTE
VALUE
FOR
EXAMPLE
THE
THREAD
PROCESSOR
AFFINITY
IS
THE
SET
OF
PROCESSORS
IN
A
MULTIPROCESSOR
SYSTEM
THAT
MAY
EXECUTE
THIS
THREAD
THIS
SET
IS
EQUAL
TO
OR
A
SUBSET
OF
THE
PROCESS
PROCESSOR
AFFINITY
NOTE
THAT
ONE
OF
THE
ATTRIBUTES
OF
A
THREAD
OBJECT
IS
CONTEXT
WHICH
CONTAINS
THE
VALUES
OF
THE
PROCESSOR
REGISTERS
WHEN
THE
THREAD
LAST
RAN
THIS
INFORMATION
ENABLES
THREADS
TO
BE
SUSPENDED
AND
RESUMED
FURTHERMORE
IT
IS
POSSIBLE
TO
ALTER
THE
BEHAV
IOR
OF
A
THREAD
BY
ALTERING
ITS
CONTEXT
WHILE
IT
IS
SUSPENDED
CHAPTER
THREADS
OBJECT
TYPE
OBJECT
BODY
ATTRIBUTES
SERVICES
OBJECT
TYPE
OBJECT
BODY
ATTRIBUTES
SERVICES
A
PROCESS
OBJECT
B
THREAD
OBJECT
FIGURE
WINDOWS
PROCESS
AND
THREAD
OBJECTS
TABLE
WINDOWS
PROCESS
OBJECT
ATTRIBUTES
WINDOWS
THREAD
AND
SMP
MANAGEMENT
TABLE
WINDOWS
THREAD
OBJECT
ATTRIBUTES
THREAD
ID
A
UNIQUE
VALUE
THAT
IDENTIFIES
A
THREAD
WHEN
IT
CALLS
A
SERVER
THREAD
CONTEXT
THE
SET
OF
REGISTER
VALUES
AND
OTHER
VOLATILE
DATA
THAT
DEFINES
THE
EXECUTION
STATE
OF
A
THREAD
DYNAMIC
PRIORITY
THE
THREAD
EXECUTION
PRIORITY
AT
ANY
GIVEN
MOMENT
BASE
PRIORITY
THE
LOWER
LIMIT
OF
THE
THREAD
DYNAMIC
PRIORITY
THREAD
PROCESSOR
AFFINITY
THE
SET
OF
PROCESSORS
ON
WHICH
THE
THREAD
CAN
RUN
WHICH
IS
A
SUBSET
OR
ALL
OF
THE
PROCESSOR
AFFINITY
OF
THE
THREAD
PROCESS
THREAD
EXECUTION
TIME
THE
CUMULATIVE
AMOUNT
OF
TIME
A
THREAD
HAS
EXECUTED
IN
USER
MODE
AND
IN
KERNEL
MODE
ALERT
STATUS
A
FLAG
THAT
INDICATES
WHETHER
A
WAITING
THREAD
MAY
EXECUTE
AN
ASYNCHRONOUS
PROCEDURE
CALL
SUSPENSION
COUNT
THE
NUMBER
OF
TIMES
THE
THREAD
EXECUTION
HAS
BEEN
SUSPENDED
WITHOUT
BEING
RESUMED
IMPERSONATION
TOKEN
A
TEMPORARY
ACCESS
TOKEN
ALLOWING
A
THREAD
TO
PERFORM
OPERATIONS
ON
BEHALF
OF
ANOTHER
PROCESS
USED
BY
SUBSYSTEMS
TERMINATION
PORT
AN
INTERPROCESS
COMMUNICATION
CHANNEL
TO
WHICH
THE
PROCESS
MANAGER
SENDS
A
MESSAGE
WHEN
THE
THREAD
TERMINATES
USED
BY
SUBSYSTEMS
THREAD
EXIT
STATUS
THE
REASON
FOR
A
THREAD
TERMINATION
MULTITHREADING
WINDOWS
SUPPORTS
CONCURRENCY
AMONG
PROCESSES
BECAUSE
THREADS
IN
DIFFERENT
PROCESSES
MAY
EXECUTE
CONCURRENTLY
APPEAR
TO
RUN
AT
THE
SAME
TIME
MOREOVER
MUL
TIPLE
THREADS
WITHIN
THE
SAME
PROCESS
MAY
BE
ALLOCATED
TO
SEPARATE
PROCESSORS
AND
EXECUTE
SIMULTANEOUSLY
ACTUALLY
RUN
AT
THE
SAME
TIME
A
MULTITHREADED
PROCESS
ACHIEVES
CONCURRENCY
WITHOUT
THE
OVERHEAD
OF
USING
MULTIPLE
PROCESSES
THREADS
WITHIN
THE
SAME
PROCESS
CAN
EXCHANGE
INFORMATION
THROUGH
THEIR
COMMON
ADDRESS
SPACE
AND
HAVE
ACCESS
TO
THE
SHARED
RESOURCES
OF
THE
PROCESS
THREADS
IN
DIFFERENT
PROCESSES
CAN
EXCHANGE
INFORMATION
THROUGH
SHARED
MEMORY
THAT
HAS
BEEN
SET
UP
BETWEEN
THE
TWO
PROCESSES
AN
OBJECT
ORIENTED
MULTITHREADED
PROCESS
IS
AN
EFFICIENT
MEANS
OF
IMPLEMENTING
A
SERVER
APPLICATION
FOR
EXAMPLE
ONE
SERVER
PROCESS
CAN
SERVICE
A
NUMBER
OF
CLIENTS
CONCURRENTLY
THREAD
STATES
AN
EXISTING
WINDOWS
THREAD
IS
IN
ONE
OF
SIX
STATES
FIGURE
READY
A
READY
THREAD
MAY
BE
SCHEDULED
FOR
EXECUTION
THE
KERNEL
DISPATCHER
KEEPS
TRACK
OF
ALL
READY
THREADS
AND
SCHEDULES
THEM
IN
PRIORITY
ORDER
STANDBY
A
STANDBY
THREAD
HAS
BEEN
SELECTED
TO
RUN
NEXT
ON
A
PARTICULAR
PROC
ESSOR
THE
THREAD
WAITS
IN
THIS
STATE
UNTIL
THAT
PROCESSOR
IS
MADE
AVAILABLE
IF
THE
STANDBY
THREAD
PRIORITY
IS
HIGH
ENOUGH
THE
RUNNING
THREAD
ON
THAT
CHAPTER
THREADS
FIGURE
WINDOWS
THREAD
STATES
PROCESSOR
MAY
BE
PREEMPTED
IN
FAVOR
OF
THE
STANDBY
THREAD
OTHERWISE
THE
STANDBY
THREAD
WAITS
UNTIL
THE
RUNNING
THREAD
BLOCKS
OR
EXHAUSTS
ITS
TIME
SLICE
RUNNING
ONCE
THE
KERNEL
DISPATCHER
PERFORMS
A
THREAD
SWITCH
THE
STANDBY
THREAD
ENTERS
THE
RUNNING
STATE
AND
BEGINS
EXECUTION
AND
CONTINUES
EXECUTION
UNTIL
IT
IS
PREEMPTED
BY
A
HIGHER
PRIORITY
THREAD
EXHAUSTS
ITS
TIME
SLICE
BLOCKS
OR
TERMINATES
IN
THE
FIRST
TWO
CASES
IT
GOES
BACK
TO
THE
READY
STATE
WAITING
A
THREAD
ENTERS
THE
WAITING
STATE
WHEN
IT
IS
BLOCKED
ON
AN
EVENT
E
G
I
O
IT
VOLUNTARILY
WAITS
FOR
SYNCHRONIZATION
PURPOSES
OR
AN
ENVIRONMENT
SUBSYSTEM
DIRECTS
THE
THREAD
TO
SUSPEND
ITSELF
WHEN
THE
WAITING
CONDITION
IS
SATISFIED
THE
THREAD
MOVES
TO
THE
READY
STATE
IF
ALL
OF
ITS
RESOURCES
ARE
AVAILABLE
TRANSITION
A
THREAD
ENTERS
THIS
STATE
AFTER
WAITING
IF
IT
IS
READY
TO
RUN
BUT
THE
RESOURCES
ARE
NOT
AVAILABLE
FOR
EXAMPLE
THE
THREAD
STACK
MAY
BE
PAGED
OUT
OF
MEMORY
WHEN
THE
RESOURCES
ARE
AVAILABLE
THE
THREAD
GOES
TO
THE
READY
STATE
TERMINATED
A
THREAD
CAN
BE
TERMINATED
BY
ITSELF
BY
ANOTHER
THREAD
OR
WHEN
ITS
PARENT
PROCESS
TERMINATES
ONCE
HOUSEKEEPING
CHORES
ARE
COMPLETED
THE
THREAD
IS
REMOVED
FROM
THE
SYSTEM
OR
IT
MAY
BE
RETAINED
BY
THE
FOR
FUTURE
REINITIALIZATION
WINDOWS
EXECUTIVE
IS
DESCRIBED
IN
CHAPTER
IT
CONTAINS
THE
BASE
OPERATING
SYSTEM
SERVICES
SUCH
AS
MEMORY
MANAGEMENT
PROCESS
AND
THREAD
MANAGEMENT
SECURITY
I
O
AND
INTERPROCESS
COMMUNICATION
WINDOWS
THREAD
AND
SMP
MANAGEMENT
SUPPORT
FOR
OS
SUBSYSTEMS
THE
GENERAL
PURPOSE
PROCESS
AND
THREAD
FACILITY
MUST
SUPPORT
THE
PARTICULAR
PROCESS
AND
THREAD
STRUCTURES
OF
THE
VARIOUS
OS
ENVIRONMENTS
IT
IS
THE
RESPONSIBILITY
OF
EACH
OS
SUBSYSTEM
TO
EXPLOIT
THE
WINDOWS
PROCESS
AND
THREAD
FEATURES
TO
EMULATE
THE
PROCESS
AND
THREAD
FACILITIES
OF
ITS
CORRESPONDING
OS
THIS
AREA
OF
PROCESS
THREAD
MANAGEMENT
IS
COMPLICATED
AND
WE
GIVE
ONLY
A
BRIEF
OVERVIEW
HERE
PROCESS
CREATION
BEGINS
WITH
A
REQUEST
FOR
A
NEW
PROCESS
FROM
AN
APPLICATION
THE
APPLICATION
ISSUES
A
CREATE
PROCESS
REQUEST
TO
THE
CORRESPONDING
PROTECTED
SUBSYSTEM
WHICH
PASSES
THE
REQUEST
TO
THE
EXECUTIVE
THE
EXECUTIVE
CREATES
A
PROC
ESS
OBJECT
AND
RETURNS
A
HANDLE
FOR
THAT
OBJECT
TO
THE
SUBSYSTEM
WHEN
WINDOWS
CREATES
A
PROCESS
IT
DOES
NOT
AUTOMATICALLY
CREATE
A
THREAD
IN
THE
CASE
OF
A
NEW
PROCESS
MUST
ALWAYS
BE
CREATED
WITH
AN
INITIAL
THREAD
THEREFORE
FOR
THE
SUBSYSTEM
CALLS
THE
WINDOWS
PROCESS
MANAGER
AGAIN
TO
CREATE
A
THREAD
FOR
THE
NEW
PROCESS
RECEIVING
A
THREAD
HANDLE
BACK
FROM
WINDOWS
THE
APPROPRIATE
THREAD
AND
PROCESS
INFORMATION
ARE
THEN
RETURNED
TO
THE
APPLICATION
IN
THE
CASE
OF
POSIX
THREADS
ARE
NOT
SUPPORTED
THEREFORE
THE
POSIX
SUBSYSTEM
OBTAINS
A
THREAD
FOR
THE
NEW
PROCESS
FROM
WINDOWS
SO
THAT
THE
PROCESS
MAY
BE
ACTIVATED
BUT
RETURNS
ONLY
PROCESS
INFORMATION
TO
THE
APPLICATION
THE
FACT
THAT
THE
POSIX
PROCESS
IS
IMPLE
MENTED
USING
BOTH
A
PROCESS
AND
A
THREAD
FROM
THE
WINDOWS
EXECUTIVE
IS
NOT
VISIBLE
TO
THE
APPLICATION
WHEN
A
NEW
PROCESS
IS
CREATED
BY
THE
EXECUTIVE
THE
NEW
PROCESS
INHERITS
MANY
OF
ITS
ATTRIBUTES
FROM
THE
CREATING
PROCESS
HOWEVER
IN
THE
ENVIRON
MENT
THIS
PROCESS
CREATION
IS
DONE
INDIRECTLY
AN
APPLICATION
CLIENT
PROCESS
ISSUES
ITS
PROCESS
CREATION
REQUEST
TO
THE
SUBSYSTEM
THEN
THE
SUBSYSTEM
IN
TURN
ISSUES
A
PROCESS
REQUEST
TO
THE
WINDOWS
EXECUTIVE
BECAUSE
THE
DESIRED
EFFECT
IS
THAT
THE
NEW
PROCESS
INHERITS
CHARACTERISTICS
OF
THE
CLIENT
PROCESS
AND
NOT
OF
THE
SERVER
PROCESS
WINDOWS
ENABLES
THE
SUBSYSTEM
TO
SPECIFY
THE
PARENT
OF
THE
NEW
PROCESS
THE
NEW
PROCESS
THEN
INHERITS
THE
PARENT
ACCESS
TOKEN
QUOTA
LIMITS
BASE
PRIORITY
AND
DEFAULT
PROCESSOR
AFFINITY
SYMMETRIC
MULTIPROCESSING
SUPPORT
WINDOWS
SUPPORTS
SMP
HARDWARE
CONFIGURATIONS
THE
THREADS
OF
ANY
PROCESS
INCLUDING
THOSE
OF
THE
EXECUTIVE
CAN
RUN
ON
ANY
PROCESSOR
IN
THE
ABSENCE
OF
AFFIN
ITY
RESTRICTIONS
EXPLAINED
IN
THE
NEXT
PARAGRAPH
THE
KERNEL
DISPATCHER
ASSIGNS
A
READY
THREAD
TO
THE
NEXT
AVAILABLE
PROCESSOR
THIS
ASSURES
THAT
NO
PROCESSOR
IS
IDLE
OR
IS
EXECUTING
A
LOWER
PRIORITY
THREAD
WHEN
A
HIGHER
PRIORITY
THREAD
IS
READY
MULTIPLE
THREADS
FROM
THE
SAME
PROCESS
CAN
BE
EXECUTING
SIMULTANEOUSLY
ON
MULTIPLE
PROCESSORS
AS
A
DEFAULT
THE
KERNEL
DISPATCHER
USES
THE
POLICY
OF
SOFT
AFFINITY
IN
ASSIGN
ING
THREADS
TO
PROCESSORS
THE
DISPATCHER
TRIES
TO
ASSIGN
A
READY
THREAD
TO
THE
SAME
PROCESSOR
IT
LAST
RAN
ON
THIS
HELPS
REUSE
DATA
STILL
IN
THAT
PROCESSOR
MEMORY
CACHES
FROM
THE
PREVIOUS
EXECUTION
OF
THE
THREAD
IT
IS
POSSIBLE
FOR
AN
APPLICATION
TO
RESTRICT
ITS
THREAD
EXECUTION
ONLY
TO
CERTAIN
PROCESSORS
HARD
AFFINITY
CHAPTER
THREADS
SOLARIS
THREAD
AND
SMP
MANAGEMENT
SOLARIS
IMPLEMENTS
MULTILEVEL
THREAD
SUPPORT
DESIGNED
TO
PROVIDE
CONSIDERABLE
FLEXIBILITY
IN
EXPLOITING
PROCESSOR
RESOURCES
MULTITHREADED
ARCHITECTURE
SOLARIS
MAKES
USE
OF
FOUR
SEPARATE
THREAD
RELATED
CONCEPTS
PROCESS
THIS
IS
THE
NORMAL
UNIX
PROCESS
AND
INCLUDES
THE
USER
ADDRESS
SPACE
STACK
AND
PROCESS
CONTROL
BLOCK
USER
LEVEL
THREADS
IMPLEMENTED
THROUGH
A
THREADS
LIBRARY
IN
THE
ADDRESS
SPACE
OF
A
PROCESS
THESE
ARE
INVISIBLE
TO
THE
OS
A
USER
LEVEL
THREAD
ULT
IS
A
USER
CREATED
UNIT
OF
EXECUTION
WITHIN
A
PROCESS
LIGHTWEIGHT
PROCESSES
A
LIGHTWEIGHT
PROCESS
LWP
CAN
BE
VIEWED
AS
A
MAP
PING
BETWEEN
ULTS
AND
KERNEL
THREADS
EACH
LWP
SUPPORTS
ULT
AND
MAPS
TO
ONE
KERNEL
THREAD
LWPS
ARE
SCHEDULED
BY
THE
KERNEL
INDEPENDENTLY
AND
MAY
EXECUTE
IN
PARALLEL
ON
MULTIPROCESSORS
KERNEL
THREADS
THESE
ARE
THE
FUNDAMENTAL
ENTITIES
THAT
CAN
BE
SCHEDULED
AND
DISPATCHED
TO
RUN
ON
ONE
OF
THE
SYSTEM
PROCESSORS
FIGURE
ILLUSTRATES
THE
RELATIONSHIP
AMONG
THESE
FOUR
ENTITIES
NOTE
THAT
THERE
IS
ALWAYS
EXACTLY
ONE
KERNEL
THREAD
FOR
EACH
LWP
AN
LWP
IS
VISIBLE
WITHIN
A
PROCESS
TO
THE
APPLICATION
THUS
LWP
DATA
STRUCTURES
EXIST
WITHIN
THEIR
RESPECTIVE
PROCESS
ADDRESS
SPACE
AT
THE
SAME
TIME
EACH
LWP
IS
BOUND
TO
A
SINGLE
DISPATCHABLE
KERNEL
THREAD
AND
THE
DATA
STRUCTURE
FOR
THAT
KERNEL
THREAD
IS
MAINTAINED
WITHIN
THE
KERNEL
ADDRESS
SPACE
FIGURE
PROCESSES
AND
THREADS
IN
SOLARIS
THE
ACRONYM
ULT
IS
UNIQUE
TO
THIS
BOOK
AND
IS
NOT
FOUND
IN
THE
SOLARIS
LITERATURE
SOLARIS
THREAD
AND
SMP
MANAGEMENT
A
PROCESS
MAY
CONSIST
OF
A
SINGLE
ULT
BOUND
TO
A
SINGLE
LWP
IN
THIS
CASE
THERE
IS
A
SINGLE
THREAD
OF
EXECUTION
CORRESPONDING
TO
A
TRADITIONAL
UNIX
PROCESS
WHEN
CONCURRENCY
IS
NOT
REQUIRED
WITHIN
A
SINGLE
PROCESS
AN
APPLICATION
USES
THIS
PROCESS
STRUCTURE
IF
AN
APPLICATION
REQUIRES
CONCURRENCY
ITS
PROCESS
CONTAINS
MULTIPLE
THREADS
EACH
BOUND
TO
A
SINGLE
LWP
WHICH
IN
TURN
ARE
EACH
BOUND
TO
A
SINGLE
KERNEL
THREAD
IN
ADDITION
THERE
ARE
KERNEL
THREADS
THAT
ARE
NOT
ASSOCIATED
WITH
LWPS
THE
KERNEL
CREATES
RUNS
AND
DESTROYS
THESE
KERNEL
THREADS
TO
EXECUTE
SPECIFIC
SYSTEM
FUNCTIONS
THE
USE
OF
KERNEL
THREADS
RATHER
THAN
KERNEL
PROCESSES
TO
IMPLEMENT
SYSTEM
FUNCTIONS
REDUCES
THE
OVERHEAD
OF
SWITCHING
WITHIN
THE
KERNEL
FROM
A
PROCESS
SWITCH
TO
A
THREAD
SWITCH
MOTIVATION
THE
THREE
LEVEL
THREAD
STRUCTURE
ULT
LWP
KERNEL
THREAD
IN
SOLARIS
IS
INTENDED
TO
FACILITATE
THREAD
MANAGEMENT
BY
THE
OS
AND
TO
PROVIDE
A
CLEAN
INTERFACE
TO
APPLI
CATIONS
THE
ULT
INTERFACE
CAN
BE
A
STANDARD
THREAD
LIBRARY
A
DEFINED
ULT
MAPS
ONTO
A
LWP
WHICH
IS
MANAGED
BY
THE
OS
AND
WHICH
HAS
DEFINED
STATES
OF
EXECUTION
DEFINED
SUBSEQUENTLY
AN
LWP
IS
BOUND
TO
A
KERNEL
THREAD
WITH
A
ONE
TO
ONE
CORRE
SPONDENCE
IN
EXECUTION
STATES
THUS
CONCURRENCY
AND
EXECUTION
ARE
MANAGED
AT
THE
LEVEL
OF
THE
KERNEL
THREAD
IN
ADDITION
AN
APPLICATION
HAS
ACCESS
TO
HARDWARE
THROUGH
AN
APPLICATION
PRO
GRAMMING
INTERFACE
CONSISTING
OF
SYSTEM
CALLS
THE
API
ALLOWS
THE
USER
TO
INVOKE
KERNEL
SERVICES
TO
PERFORM
PRIVILEGED
TASKS
ON
BEHALF
OF
THE
CALLING
PROCESS
SUCH
AS
READ
OR
WRITE
A
FILE
ISSUE
A
CONTROL
COMMAND
TO
A
DEVICE
CREATE
A
NEW
PROCESS
OR
THREAD
ALLOCATE
MEMORY
FOR
THE
PROCESS
TO
USE
AND
SO
ON
PROCESS
STRUCTURE
FIGURE
COMPARES
IN
GENERAL
TERMS
THE
PROCESS
STRUCTURE
OF
A
TRADITIONAL
UNIX
SYSTEM
WITH
THAT
OF
SOLARIS
ON
A
TYPICAL
UNIX
IMPLEMENTATION
THE
PROCESS
STRUC
TURE
INCLUDES
THE
PROCESS
ID
THE
USER
IDS
A
SIGNAL
DISPATCH
TABLE
WHICH
THE
KERNEL
USES
TO
DECIDE
WHAT
TO
DO
WHEN
SENDING
A
SIGNAL
TO
A
PROCESS
FILE
DESCRIPTORS
WHICH
DESCRIBE
THE
STATE
OF
FILES
IN
USE
BY
THIS
PROCESS
A
MEMORY
MAP
WHICH
DEFINES
THE
ADDRESS
SPACE
FOR
THIS
PROCESS
AND
A
PROCESSOR
STATE
STRUCTURE
WHICH
INCLUDES
THE
KERNEL
STACK
FOR
THIS
PROCESS
SOLARIS
RETAINS
THIS
BASIC
STRUCTURE
BUT
REPLACES
THE
PRO
CESSOR
STATE
BLOCK
WITH
A
LIST
OF
STRUCTURES
CONTAINING
ONE
DATA
BLOCK
FOR
EACH
LWP
THE
LWP
DATA
STRUCTURE
INCLUDES
THE
FOLLOWING
ELEMENTS
AN
LWP
IDENTIFIER
THE
PRIORITY
OF
THIS
LWP
AND
HENCE
THE
KERNEL
THREAD
THAT
SUPPORTS
IT
A
SIGNAL
MASK
THAT
TELLS
THE
KERNEL
WHICH
SIGNALS
WILL
BE
ACCEPTED
SAVED
VALUES
OF
USER
LEVEL
REGISTERS
WHEN
THE
LWP
IS
NOT
RUNNING
THE
KERNEL
STACK
FOR
THIS
LWP
WHICH
INCLUDES
SYSTEM
CALL
ARGUMENTS
RESULTS
AND
ERROR
CODES
FOR
EACH
CALL
LEVEL
RESOURCE
USAGE
AND
PROFILING
DATA
POINTER
TO
THE
CORRESPONDING
KERNEL
THREAD
POINTER
TO
THE
PROCESS
STRUCTURE
CHAPTER
THREADS
UNIX
PROCESS
STRUCTURE
SOLARIS
PROCESS
STRUCTURE
PRIORITY
SIGNAL
MASK
REGISTERS
STACK
LWP
ID
PRIORITY
SIGNAL
MASK
REGISTERS
STACK
FIGURE
PROCESS
STRUCTURE
IN
TRADITIONAL
UNIX
AND
SOLARIS
THREAD
EXECUTION
FIGURE
SHOWS
A
SIMPLIFIED
VIEW
OF
BOTH
THREAD
EXECUTION
STATES
THESE
STATES
REFLECT
THE
EXECUTION
STATUS
OF
BOTH
A
KERNEL
THREAD
AND
THE
LWP
BOUND
TO
IT
AS
MENTIONED
SOME
KERNEL
THREADS
ARE
NOT
ASSOCIATED
WITH
AN
LWP
THE
SAME
EXECU
TION
DIAGRAM
APPLIES
THE
STATES
ARE
AS
FOLLOWS
RUN
THE
THREAD
IS
RUNNABLE
THAT
IS
THE
THREAD
IS
READY
TO
EXECUTE
ONPROC
THE
THREAD
IS
EXECUTING
ON
A
PROCESSOR
SLEEP
THE
THREAD
IS
BLOCKED
STOP
THE
THREAD
IS
STOPPED
ZOMBIE
THE
THREAD
HAS
TERMINATED
FREE
THREAD
RESOURCES
HAVE
BEEN
RELEASED
AND
THE
THREAD
IS
AWAITING
REMOVAL
FROM
THE
OS
THREAD
DATA
STRUCTURE
A
THREAD
MOVES
FROM
ONPROC
TO
RUN
IF
IT
IS
PREEMPTED
BY
A
HIGHER
PRIORITY
THREAD
OR
BECAUSE
OF
TIME
SLICING
A
THREAD
MOVES
FROM
ONPROC
TO
SLEEP
IF
IT
SOLARIS
THREAD
AND
SMP
MANAGEMENT
FIGURE
SOLARIS
THREAD
STATES
IS
BLOCKED
AND
MUST
AWAIT
AN
EVENT
TO
RETURN
THE
RUN
STATE
BLOCKING
OCCURS
IF
THE
THREAD
INVOKES
A
SYSTEM
CALL
AND
MUST
WAIT
FOR
THE
SYSTEM
SERVICE
TO
BE
PERFORMED
A
THREAD
ENTERS
THE
STOP
STATE
IF
ITS
PROCESS
IS
STOPPED
THIS
MIGHT
BE
DONE
FOR
DEBUGGING
PURPOSES
INTERRUPTS
AS
THREADS
MOST
OPERATING
SYSTEMS
CONTAIN
TWO
FUNDAMENTAL
FORMS
OF
CONCURRENT
ACTIVITY
PROCESSES
AND
INTERRUPTS
PROCESSES
OR
THREADS
COOPERATE
WITH
EACH
OTHER
AND
MANAGE
THE
USE
OF
SHARED
DATA
STRUCTURES
BY
MEANS
OF
A
VARIETY
OF
PRIMITIVES
THAT
ENFORCE
MUTUAL
EXCLUSION
ONLY
ONE
PROCESS
AT
A
TIME
CAN
EXECUTE
CERTAIN
CODE
OR
ACCESS
CERTAIN
DATA
AND
THAT
SYNCHRONIZE
THEIR
EXECUTION
INTERRUPTS
ARE
SYNCHRONIZED
BY
PREVENTING
THEIR
HANDLING
FOR
A
PERIOD
OF
TIME
SOLARIS
UNIFIES
THESE
TWO
CONCEPTS
INTO
A
SINGLE
MODEL
NAMELY
KERNEL
THREADS
AND
THE
MECHANISMS
FOR
SCHEDULING
AND
EXECUTING
KERNEL
THREADS
TO
DO
THIS
INTERRUPTS
ARE
CONVERTED
TO
KERNEL
THREADS
THE
MOTIVATION
FOR
CONVERTING
INTERRUPTS
TO
THREADS
IS
TO
REDUCE
OVERHEAD
INTERRUPT
HANDLERS
OFTEN
MANIPULATE
DATA
SHARED
BY
THE
REST
OF
THE
KERNEL
THEREFORE
WHILE
A
KERNEL
ROUTINE
THAT
ACCESSES
SUCH
DATA
IS
EXECUTING
INTERRUPTS
MUST
BE
BLOCKED
EVEN
THOUGH
MOST
INTERRUPTS
WILL
NOT
AFFECT
THAT
DATA
TYPICALLY
THE
WAY
THIS
IS
DONE
IS
FOR
THE
ROUTINE
TO
SET
THE
INTERRUPT
PRIORITY
LEVEL
HIGHER
TO
BLOCK
INTER
RUPTS
AND
THEN
LOWER
THE
PRIORITY
LEVEL
AFTER
ACCESS
IS
COMPLETED
THESE
OPERATIONS
TAKE
TIME
THE
PROBLEM
IS
MAGNIFIED
ON
A
MULTIPROCESSOR
SYSTEM
THE
KERNEL
MUST
PROTECT
MORE
OBJECTS
AND
MAY
NEED
TO
BLOCK
INTERRUPTS
ON
ALL
PROCESSORS
CHAPTER
THREADS
THE
SOLUTION
IN
SOLARIS
CAN
BE
SUMMARIZED
AS
FOLLOWS
SOLARIS
EMPLOYS
A
SET
OF
KERNEL
THREADS
TO
HANDLE
INTERRUPTS
AS
WITH
ANY
KERNEL
THREAD
AN
INTERRUPT
THREAD
HAS
ITS
OWN
IDENTIFIER
PRIORITY
CONTEXT
AND
STACK
THE
KERNEL
CONTROLS
ACCESS
TO
DATA
STRUCTURES
AND
SYNCHRONIZES
AMONG
INTER
RUPT
THREADS
USING
MUTUAL
EXCLUSION
PRIMITIVES
OF
THE
TYPE
DISCUSSED
IN
CHAPTER
THAT
IS
THE
NORMAL
SYNCHRONIZATION
TECHNIQUES
FOR
THREADS
ARE
USED
IN
HANDLING
INTERRUPTS
INTERRUPT
THREADS
ARE
ASSIGNED
HIGHER
PRIORITIES
THAN
ALL
OTHER
TYPES
OF
KERNEL
THREADS
WHEN
AN
INTERRUPT
OCCURS
IT
IS
DELIVERED
TO
A
PARTICULAR
PROCESSOR
AND
THE
THREAD
THAT
WAS
EXECUTING
ON
THAT
PROCESSOR
IS
PINNED
A
PINNED
THREAD
CANNOT
MOVE
TO
ANOTHER
PROCESSOR
AND
ITS
CONTEXT
IS
PRESERVED
IT
IS
SIMPLY
SUSPENDED
UNTIL
THE
INTERRUPT
IS
PROCESSED
THE
PROCESSOR
THEN
BEGINS
EXECUTING
AN
INTERRUPT
THREAD
THERE
IS
A
POOL
OF
DEACTIVATED
INTERRUPT
THREADS
AVAILABLE
SO
THAT
A
NEW
THREAD
CREA
TION
IS
NOT
REQUIRED
THE
INTERRUPT
THREAD
THEN
EXECUTES
TO
HANDLE
THE
INTERRUPT
IF
THE
HANDLER
ROUTINE
NEEDS
ACCESS
TO
A
DATA
STRUCTURE
THAT
IS
CURRENTLY
LOCKED
IN
SOME
FASHION
FOR
USE
BY
ANOTHER
EXECUTING
THREAD
THE
INTERRUPT
THREAD
MUST
WAIT
FOR
ACCESS
TO
THAT
DATA
STRUCTURE
AN
INTERRUPT
THREAD
CAN
ONLY
BE
PREEMPTED
BY
ANOTHER
INTERRUPT
THREAD
OF
HIGHER
PRIORITY
EXPERIENCE
WITH
SOLARIS
INTERRUPT
THREADS
INDICATES
THAT
THIS
APPROACH
PROVIDES
SUPERIOR
PERFORMANCE
TO
THE
TRADITIONAL
INTERRUPT
HANDLING
STRATEGY
LINUX
PROCESS
AND
THREAD
MANAGEMENT
LINUX
TASKS
A
PROCESS
OR
TASK
IN
LINUX
IS
REPRESENTED
BY
A
DATA
STRUCTURE
THE
DATA
STRUCTURE
CONTAINS
INFORMATION
IN
A
NUMBER
OF
CATEGORIES
STATE
THE
EXECUTION
STATE
OF
THE
PROCESS
EXECUTING
READY
SUSPENDED
STOPPED
ZOMBIE
THIS
IS
DESCRIBED
SUBSEQUENTLY
SCHEDULING
INFORMATION
INFORMATION
NEEDED
BY
LINUX
TO
SCHEDULE
PROCESSES
A
PROCESS
CAN
BE
NORMAL
OR
REAL
TIME
AND
HAS
A
PRIORITY
REAL
TIME
PROCESSES
ARE
SCHEDULED
BEFORE
NORMAL
PROCESSES
AND
WITHIN
EACH
CATEGORY
RELATIVE
PRI
ORITIES
CAN
BE
USED
A
COUNTER
KEEPS
TRACK
OF
THE
AMOUNT
OF
TIME
A
PROCESS
IS
ALLOWED
TO
EXECUTE
IDENTIFIERS
EACH
PROCESS
HAS
A
UNIQUE
PROCESS
IDENTIFIER
AND
ALSO
HAS
USER
AND
GROUP
IDENTIFIERS
A
GROUP
IDENTIFIER
IS
USED
TO
ASSIGN
RESOURCE
ACCESS
PRIVI
LEGES
TO
A
GROUP
OF
PROCESSES
INTERPROCESS
COMMUNICATION
LINUX
SUPPORTS
THE
IPC
MECHANISMS
FOUND
IN
UNIX
DESCRIBED
IN
CHAPTER
LINKS
EACH
PROCESS
INCLUDES
A
LINK
TO
ITS
PARENT
PROCESS
LINKS
TO
ITS
SIBLINGS
PROCESSES
WITH
THE
SAME
PARENT
AND
LINKS
TO
ALL
OF
ITS
CHILDREN
LINUX
PROCESS
AND
THREAD
MANAGEMENT
TIMES
AND
TIMERS
INCLUDES
PROCESS
CREATION
TIME
AND
THE
AMOUNT
OF
PROCES
SOR
TIME
SO
FAR
CONSUMED
BY
THE
PROCESS
A
PROCESS
MAY
ALSO
HAVE
ASSOCIATED
ONE
OR
MORE
INTERVAL
TIMERS
A
PROCESS
DEFINES
AN
INTERVAL
TIMER
BY
MEANS
OF
A
SYSTEM
CALL
AS
A
RESULT
A
SIGNAL
IS
SENT
TO
THE
PROCESS
WHEN
THE
TIMER
EXPIRES
A
TIMER
MAY
BE
SINGLE
USE
OR
PERIODIC
FILE
SYSTEM
INCLUDES
POINTERS
TO
ANY
FILES
OPENED
BY
THIS
PROCESS
AS
WELL
AS
POINTERS
TO
THE
CURRENT
AND
THE
ROOT
DIRECTORIES
FOR
THIS
PROCESS
ADDRESS
SPACE
DEFINES
THE
VIRTUAL
ADDRESS
SPACE
ASSIGNED
TO
THIS
PROCESS
PROCESSOR
SPECIFIC
CONTEXT
THE
REGISTERS
AND
STACK
INFORMATION
THAT
CONSTITUTE
THE
CONTEXT
OF
THIS
PROCESS
FIGURE
SHOWS
THE
EXECUTION
STATES
OF
A
PROCESS
THESE
ARE
AS
FOLLOWS
RUNNING
THIS
STATE
VALUE
CORRESPONDS
TO
TWO
STATES
A
RUNNING
PROCESS
IS
EITHER
EXECUTING
OR
IT
IS
READY
TO
EXECUTE
INTERRUPTIBLE
THIS
IS
A
BLOCKED
STATE
IN
WHICH
THE
PROCESS
IS
WAITING
FOR
AN
EVENT
SUCH
AS
THE
END
OF
AN
I
O
OPERATION
THE
AVAILABILITY
OF
A
RESOURCE
OR
A
SIGNAL
FROM
ANOTHER
PROCESS
UNINTERRUPTIBLE
THIS
IS
ANOTHER
BLOCKED
STATE
THE
DIFFERENCE
BETWEEN
THIS
AND
THE
INTERRUPTIBLE
STATE
IS
THAT
IN
AN
UNINTERRUPTIBLE
STATE
A
PROCESS
IS
WAIT
ING
DIRECTLY
ON
HARDWARE
CONDITIONS
AND
THEREFORE
WILL
NOT
HANDLE
ANY
SIGNALS
FIGURE
LINUX
PROCESS
THREAD
MODEL
CHAPTER
THREADS
STOPPED
THE
PROCESS
HAS
BEEN
HALTED
AND
CAN
ONLY
RESUME
BY
POSITIVE
ACTION
FROM
ANOTHER
PROCESS
FOR
EXAMPLE
A
PROCESS
THAT
IS
BEING
DEBUGGED
CAN
BE
PUT
INTO
THE
STOPPED
STATE
ZOMBIE
THE
PROCESS
HAS
BEEN
TERMINATED
BUT
FOR
SOME
REASON
STILL
MUST
HAVE
ITS
TASK
STRUCTURE
IN
THE
PROCESS
TABLE
LINUX
THREADS
TRADITIONAL
UNIX
SYSTEMS
SUPPORT
A
SINGLE
THREAD
OF
EXECUTION
PER
PROCESS
WHILE
MODERN
UNIX
SYSTEMS
TYPICALLY
PROVIDE
SUPPORT
FOR
MULTIPLE
KERNEL
LEVEL
THREADS
PER
PROCESS
AS
WITH
TRADITIONAL
UNIX
SYSTEMS
OLDER
VERSIONS
OF
THE
LINUX
KER
NEL
OFFERED
NO
SUPPORT
FOR
MULTITHREADING
INSTEAD
APPLICATIONS
WOULD
NEED
TO
BE
WRITTEN
WITH
A
SET
OF
USER
LEVEL
LIBRARY
FUNCTIONS
THE
MOST
POPULAR
OF
WHICH
IS
KNOWN
AS
PTHREAD
POSIX
THREAD
LIBRARIES
WITH
ALL
OF
THE
THREADS
MAPPING
INTO
A
SINGLE
KERNEL
LEVEL
PROCESS
WE
HAVE
SEEN
THAT
MODERN
VERSIONS
OF
UNIX
OFFER
KERNEL
LEVEL
THREADS
LINUX
PROVIDES
A
UNIQUE
SOLUTION
IN
THAT
IT
DOES
NOT
RECOG
NIZE
A
DISTINCTION
BETWEEN
THREADS
AND
PROCESSES
USING
A
MECHANISM
SIMILAR
TO
THE
LIGHTWEIGHT
PROCESSES
OF
SOLARIS
USER
LEVEL
THREADS
ARE
MAPPED
INTO
KERNEL
LEVEL
PROCESSES
MULTIPLE
USER
LEVEL
THREADS
THAT
CONSTITUTE
A
SINGLE
USER
LEVEL
PROCESS
ARE
MAPPED
INTO
LINUX
KERNEL
LEVEL
PROCESSES
THAT
SHARE
THE
SAME
GROUP
ID
THIS
ENABLES
THESE
PROCESSES
TO
SHARE
RESOURCES
SUCH
AS
FILES
AND
MEMORY
AND
TO
AVOID
THE
NEED
FOR
A
CONTEXT
SWITCH
WHEN
THE
SCHEDULER
SWITCHES
AMONG
PROCESSES
IN
THE
SAME
GROUP
A
NEW
PROCESS
IS
CREATED
IN
LINUX
BY
COPYING
THE
ATTRIBUTES
OF
THE
CURRENT
PROCESS
A
NEW
PROCESS
CAN
BE
CLONED
SO
THAT
IT
SHARES
RESOURCES
SUCH
AS
FILES
SIG
NAL
HANDLERS
AND
VIRTUAL
MEMORY
WHEN
THE
TWO
PROCESSES
SHARE
THE
SAME
VIRTUAL
MEMORY
THEY
FUNCTION
AS
THREADS
WITHIN
A
SINGLE
PROCESS
HOWEVER
NO
SEPARATE
TYPE
OF
DATA
STRUCTURE
IS
DEFINED
FOR
A
THREAD
IN
PLACE
OF
THE
USUAL
FORK
COM
MAND
PROCESSES
ARE
CREATED
IN
LINUX
USING
THE
CLONE
COMMAND
THIS
COMMAND
INCLUDES
A
SET
OF
FLAGS
AS
ARGUMENTS
DEFINED
IN
TABLE
THE
TRADITIONAL
FORK
SYSTEM
CALL
IS
IMPLEMENTED
BY
LINUX
AS
A
CLONE
SYSTEM
CALL
WITH
ALL
OF
THE
CLONE
FLAGS
CLEARED
WHEN
THE
LINUX
KERNEL
PERFORMS
A
SWITCH
FROM
ONE
PROCESS
TO
ANOTHER
IT
CHECKS
WHETHER
THE
ADDRESS
OF
THE
PAGE
DIRECTORY
OF
THE
CURRENT
PROCESS
IS
THE
SAME
AS
THAT
OF
THE
TO
BE
SCHEDULED
PROCESS
IF
THEY
ARE
THEN
THEY
ARE
SHARING
THE
SAME
ADDRESS
SPACE
SO
THAT
A
CONTEXT
SWITCH
IS
BASICALLY
JUST
A
JUMP
FROM
ONE
LOCATION
OF
CODE
TO
ANOTHER
LOCATION
OF
CODE
ALTHOUGH
CLONED
PROCESSES
THAT
ARE
PART
OF
THE
SAME
PROCESS
GROUP
CAN
SHARE
THE
SAME
MEMORY
SPACE
THEY
CANNOT
SHARE
THE
SAME
USER
STACKS
THUS
THE
CLONE
CALL
CREATES
SEPARATE
STACK
SPACES
FOR
EACH
PROCESS
PORTABLE
OPERATING
SYSTEMS
BASED
ON
UNIX
IS
AN
IEEE
API
STANDARD
THAT
INCLUDES
A
STAN
DARD
FOR
A
THREAD
API
LIBRARIES
IMPLEMENTING
THE
POSIX
THREADS
STANDARD
ARE
OFTEN
NAMED
PTHREADS
PTHREADS
ARE
MOST
COMMONLY
USED
ON
UNIX
LIKE
POSIX
SYSTEMS
SUCH
AS
LINUX
AND
SOLARIS
BUT
MICROSOFT
WINDOWS
IMPLEMENTATIONS
ALSO
EXIST
MAC
OS
X
GRAND
CENTRAL
DISPATCH
TABLE
LINUX
CLONE
FLAGS
CLEAR
THE
TASK
ID
THE
PARENT
DOES
NOT
WANT
A
SIGCHLD
SIGNAL
SENT
ON
EXIT
SHARE
THE
TABLE
THAT
IDENTIFIES
THE
OPEN
FILES
SHARE
THE
TABLE
THAT
IDENTIFIES
THE
ROOT
DIRECTORY
AND
THE
CURRENT
WORKING
DIRECTORY
AS
WELL
AS
THE
VALUE
OF
THE
BIT
MASK
USED
TO
MASK
THE
INITIAL
FILE
PERMISSIONS
OF
A
NEW
FILE
SET
PID
TO
ZERO
WHICH
REFERS
TO
AN
IDLE
TASK
THE
IDLE
TASK
IS
EMPLOYED
WHEN
ALL
AVAILABLE
TASKS
ARE
BLOCKED
WAITING
FOR
RESOURCES
CREATE
A
NEW
NAMESPACE
FOR
THE
CHILD
CALLER
AND
NEW
TASK
SHARE
THE
SAME
PARENT
PROCESS
IF
THE
PARENT
PROCESS
IS
BEING
TRACED
THE
CHILD
PROCESS
WILL
ALSO
BE
TRACED
WRITE
THE
TID
BACK
TO
USER
SPACE
CREATE
A
NEW
TLS
FOR
THE
CHILD
SHARE
THE
TABLE
THAT
IDENTIFIES
THE
SIGNAL
HANDLERS
SHARE
SYSTEM
V
SEMANTICS
INSERT
THIS
PROCESS
INTO
THE
SAME
THREAD
GROUP
OF
THE
PARENT
IF
THIS
FLAG
IS
TRUE
IT
IMPLICITLY
ENFORCES
IF
SET
THE
PARENT
DOES
NOT
GET
SCHEDULED
FOR
EXECUTION
UNTIL
THE
CHILD
INVOKES
THE
EXECVE
SYSTEM
CALL
SHARE
THE
ADDRESS
SPACE
MEMORY
DESCRIPTOR
AND
ALL
PAGE
TABLES
MAC
OS
X
GRAND
CENTRAL
DISPATCH
AS
WAS
MENTIONED
IN
CHAPTER
MAC
OS
X
GRAND
CENTRAL
DISPATCH
GCD
PRO
VIDES
A
POOL
OF
AVAILABLE
THREADS
DESIGNERS
CAN
DESIGNATE
PORTIONS
OF
APPLICATIONS
CALLED
BLOCKS
THAT
CAN
BE
DISPATCHED
INDEPENDENTLY
AND
RUN
CONCURRENTLY
THE
OS
WILL
PROVIDE
AS
MUCH
CONCURRENCY
AS
POSSIBLE
BASED
ON
THE
NUMBER
OF
CORES
AVAIL
ABLE
AND
THE
THREAD
CAPACITY
OF
THE
SYSTEM
ALTHOUGH
OTHER
OPERATING
SYSTEMS
HAVE
IMPLEMENTED
THREAD
POOLS
GCD
PROVIDES
A
QUALITATIVE
IMPROVEMENT
IN
EASE
OF
USE
AND
EFFICIENCY
A
BLOCK
IS
A
SIMPLE
EXTENSION
TO
C
OR
OTHER
LANGUAGES
SUCH
AS
C
THE
PUR
POSE
OF
DEFINING
A
BLOCK
IS
TO
DEFINE
A
SELF
CONTAINED
UNIT
OF
WORK
INCLUDING
CODE
PLUS
DATA
HERE
IS
A
SIMPLE
EXAMPLE
OF
A
BLOCK
DEFINITION
X
PRINTF
HELLO
WORLD
N
A
BLOCK
IS
DENOTED
BY
A
CARET
AT
THE
START
OF
THE
FUNCTION
WHICH
IS
ENCLOSED
IN
CURLY
BRACKETS
THE
ABOVE
BLOCK
DEFINITION
DEFINES
X
AS
A
WAY
OF
CALLING
THE
FUNC
TION
SO
THAT
INVOKING
THE
FUNCTION
X
WOULD
PRINT
THE
WORDS
HELLO
WORLD
CHAPTER
THREADS
BLOCKS
ENABLE
THE
PROGRAMMER
TO
ENCAPSULATE
COMPLEX
FUNCTIONS
TOGETHER
WITH
THEIR
ARGUMENTS
AND
DATA
SO
THAT
THEY
CAN
EASILY
BE
REFERENCED
AND
PASSED
AROUND
IN
A
PROGRAM
MUCH
LIKE
A
VARIABLE
SYMBOLICALLY
F
DATA
BLOCKS
ARE
SCHEDULED
AND
DISPATCHED
BY
MEANS
OF
QUEUES
THE
APPLICATION
MAKES
USE
OF
SYSTEM
QUEUES
PROVIDED
BY
GCD
AND
MAY
ALSO
SET
UP
PRIVATE
QUEUES
BLOCKS
ARE
PUT
ONTO
A
QUEUE
AS
THEY
ARE
ENCOUNTERED
DURING
PROGRAM
EXECUTION
GCD
THEN
USES
THOSE
QUEUES
TO
DESCRIBE
CONCURRENCY
SERIALIZATION
AND
CALLBACKS
QUEUES
ARE
LIGHTWEIGHT
USER
SPACE
DATA
STRUCTURES
WHICH
GENERALLY
MAKES
THEM
FAR
MORE
EFFICIENT
THAN
MANUALLY
MANAGING
THREADS
AND
LOCKS
FOR
EXAMPLE
THIS
QUEUE
HAS
THREE
BLOCKS
QUEUE
DEPENDING
ON
THE
QUEUE
AND
HOW
IT
IS
DEFINED
GCD
EITHER
TREATS
THESE
BLOCKS
AS
POTENTIALLY
CONCURRENT
ACTIVITIES
OR
TREATS
THEM
AS
SERIAL
ACTIVITIES
IN
EITHER
CASE
BLOCKS
ARE
DISPATCHED
ON
A
FIRST
IN
FIRST
OUT
BASIS
IF
THIS
IS
A
CONCURRENT
QUEUE
THEN
THE
DISPATCHER
ASSIGNS
F
TO
A
THREAD
AS
SOON
AS
ONE
IS
AVAILABLE
THEN
G
THEN
H
IF
THIS
IS
A
SERIAL
QUEUE
THE
DISPATCHER
ASSIGNS
F
TO
A
THREAD
AND
THEN
ONLY
ASSIGNS
G
TO
A
THREAD
AFTER
F
HAS
COMPLETED
THE
USE
OF
PREDEFINED
THREADS
SAVES
THE
COST
OF
CREATING
A
NEW
THREAD
FOR
EACH
REQUEST
REDUCING
THE
LATENCY
ASSOCIATED
WITH
PROCESS
ING
A
BLOCK
THREAD
POOLS
ARE
AUTOMATICALLY
SIZED
BY
THE
SYSTEM
TO
MAXIMIZE
THE
PERFORMANCE
OF
THE
APPLICATIONS
USING
GCD
WHILE
MINIMIZING
THE
NUMBER
OF
IDLE
OR
COMPETING
THREADS
IN
ADDITION
TO
SCHEDULING
BLOCKS
DIRECTLY
THE
APPLICATION
CAN
ASSOCIATE
A
SIN
GLE
BLOCK
AND
QUEUE
WITH
AN
EVENT
SOURCE
SUCH
AS
A
TIMER
NETWORK
SOCKET
OR
FILE
DESCRIPTOR
EVERY
TIME
THE
SOURCE
ISSUES
AN
EVENT
THE
BLOCK
IS
SCHEDULED
IF
IT
IS
NOT
OF
THE
MATERIAL
IN
THE
REMAINDER
OF
THIS
SECTION
IS
BASED
ON
MAC
OS
X
GRAND
CENTRAL
DISPATCH
ALREADY
RUNNING
THIS
ALLOWS
RAPID
RESPONSE
WITHOUT
THE
EXPENSE
OF
POLLING
OR
PARK
ING
A
THREAD
ON
THE
EVENT
SOURCE
AN
EXAMPLE
FROM
INDICATES
THE
EASE
OF
USING
GCD
CONSIDER
A
DOCUMENT
BASED
APPLICATION
WITH
A
BUTTON
THAT
WHEN
CLICKED
WILL
ANALYZE
THE
CURRENT
DOCUMENT
AND
DISPLAY
SOME
INTERESTING
STATISTICS
ABOUT
IT
IN
THE
COMMON
CASE
THIS
ANALYSIS
SHOULD
EXECUTE
IN
UNDER
A
SECOND
SO
THE
FOLLOWING
CODE
IS
USED
TO
CONNECT
THE
BUTTON
WITH
AN
ACTION
INACTION
ANALYZEDOCUMENT
NSBUTTON
SENDER
NSDICTIONARY
STATS
MYDOC
ANALYZE
MYMODEL
SETDICT
STATS
MYSTATSVIEW
SETNEEDSDISPLAY
YES
STATS
RELEASE
THE
FIRST
LINE
OF
THE
FUNCTION
BODY
ANALYZES
THE
DOCUMENT
THE
SECOND
LINE
UPDATES
THE
APPLICATION
INTERNAL
STATE
AND
THE
THIRD
LINE
TELLS
THE
APPLICATION
THAT
THE
STATISTICS
VIEW
NEEDS
TO
BE
UPDATED
TO
REFLECT
THIS
NEW
STATE
THIS
CODE
WHICH
FOL
LOWS
A
COMMON
PATTERN
IS
EXECUTED
IN
THE
MAIN
THREAD
THE
DESIGN
IS
ACCEPTABLE
SO
LONG
AS
THE
ANALYSIS
DOES
NOT
TAKE
TOO
LONG
BECAUSE
AFTER
THE
USER
CLICKS
THE
BUTTON
THE
MAIN
THREAD
OF
THE
APPLICATION
NEEDS
TO
HANDLE
THAT
USER
INPUT
AS
FAST
AS
POS
SIBLE
SO
IT
CAN
GET
BACK
TO
THE
MAIN
EVENT
LOOP
TO
PROCESS
THE
NEXT
USER
ACTION
BUT
IF
THE
USER
OPENS
A
VERY
LARGE
OR
COMPLEX
DOCUMENT
THE
ANALYZE
STEP
MAY
TAKE
AN
UNACCEPTABLY
LONG
AMOUNT
OF
TIME
A
DEVELOPER
MAY
BE
RELUCTANT
TO
ALTER
THE
CODE
TO
MEET
THIS
UNLIKELY
EVENT
WHICH
MAY
INVOLVE
APPLICATION
GLOBAL
OBJECTS
THREAD
MANAGEMENT
CALLBACKS
ARGUMENT
MARSHALLING
CONTEXT
OBJECTS
NEW
VARIABLES
AND
SO
ON
BUT
WITH
GCD
A
MODEST
ADDITION
TO
THE
CODE
PRODUCES
THE
DESIRED
RESULT
IBACTION
ANALYZEDOCUMENT
NSBUTTON
SENDER
NSDICTIONARY
STATS
MYDOC
ANALYZE
MYMODEL
SETDICT
STATS
MYSTATSVIEW
SETNEEDSDISPLAY
YES
STATS
RELEASE
CHAPTER
THREADS
ALL
FUNCTIONS
IN
GCD
BEGIN
WITH
THE
OUTER
ASYNC
CALL
PUTS
A
TASK
ON
A
GLOBAL
CONCURRENT
QUEUE
THIS
TELLS
THE
OS
THAT
THE
BLOCK
CAN
BE
ASSIGNED
TO
A
SEPARATE
CONCURRENT
QUEUE
OFF
THE
MAIN
QUEUE
AND
EXE
CUTED
IN
PARALLEL
THEREFORE
THE
MAIN
THREAD
OF
EXECUTION
IS
NOT
DELAYED
WHEN
THE
ANALYZE
FUNCTION
IS
COMPLETE
THE
INNER
CALL
IS
ENCOUNTERED
THIS
DIRECTS
THE
OS
TO
PUT
THE
FOLLOWING
BLOCK
OF
CODE
AT
THE
END
OF
THE
MAIN
QUEUE
TO
BE
EXECUTED
WHEN
IT
REACHES
THE
HEAD
OF
THE
QUEUE
SO
WITH
VERY
LITTLE
WORK
ON
THE
PART
OF
THE
PROGRAMMER
THE
DESIRED
REQUIREMENT
IS
MET
SUMMARY
SOME
OPERATING
SYSTEMS
DISTINGUISH
THE
CONCEPTS
OF
PROCESS
AND
THREAD
THE
FOR
MER
RELATED
TO
RESOURCE
OWNERSHIP
AND
THE
LATTER
RELATED
TO
PROGRAM
EXECUTION
THIS
APPROACH
MAY
LEAD
TO
IMPROVED
EFFICIENCY
AND
CODING
CONVENIENCE
IN
A
MUL
TITHREADED
SYSTEM
MULTIPLE
CONCURRENT
THREADS
MAY
BE
DEFINED
WITHIN
A
SINGLE
PROCESS
THIS
MAY
BE
DONE
USING
EITHER
USER
LEVEL
THREADS
OR
KERNEL
LEVEL
THREADS
USER
LEVEL
THREADS
ARE
UNKNOWN
TO
THE
OS
AND
ARE
CREATED
AND
MANAGED
BY
A
THREADS
LIBRARY
THAT
RUNS
IN
THE
USER
SPACE
OF
A
PROCESS
USER
LEVEL
THREADS
ARE
VERY
EFFICIENT
BECAUSE
A
MODE
SWITCH
IS
NOT
REQUIRED
TO
SWITCH
FROM
ONE
THREAD
TO
ANOTHER
HOWEVER
ONLY
A
SINGLE
USER
LEVEL
THREAD
WITHIN
A
PROCESS
CAN
EXECUTE
AT
A
TIME
AND
IF
ONE
THREAD
BLOCKS
THE
ENTIRE
PROCESS
IS
BLOCKED
KERNEL
LEVEL
THREADS
ARE
THREADS
WITHIN
A
PROCESS
THAT
ARE
MAINTAINED
BY
THE
KERNEL
BECAUSE
THEY
ARE
RECOGNIZED
BY
THE
KERNEL
MULTIPLE
THREADS
WITHIN
THE
SAME
PROCESS
CAN
EXECUTE
IN
PARALLEL
ON
A
MULTIPROCESSOR
AND
THE
BLOCKING
OF
A
THREAD
DOES
NOT
BLOCK
THE
ENTIRE
PROCESS
HOWEVER
A
MODE
SWITCH
IS
REQUIRED
TO
SWITCH
FROM
ONE
THREAD
TO
ANOTHER
RECOMMENDED
READING
AND
PROVIDE
GOOD
OVERVIEWS
OF
THREAD
CONCEPTS
AND
A
DISCUS
SION
OF
PROGRAMMING
STRATEGIES
THE
FORMER
FOCUSES
MORE
ON
CONCEPTS
AND
THE
LATTER
MORE
ON
PROGRAMMING
BUT
BOTH
PROVIDE
USEFUL
COVERAGE
OF
BOTH
TOPICS
DISCUSSES
THE
WINDOWS
NT
THREAD
FACILITY
IN
DEPTH
GOOD
COVERAGE
OF
UNIX
THREADS
CONCEPTS
IS
FOUND
IN
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
KEY
TERMS
KERNEL
LEVEL
THREAD
LIGHTWEIGHT
PROCESS
MESSAGE
MULTITHREADING
PORT
PROCESS
TASK
THREAD
USER
LEVEL
THREAD
REVIEW
QUESTIONS
TABLE
LISTS
TYPICAL
ELEMENTS
FOUND
IN
A
PROCESS
CONTROL
BLOCK
FOR
AN
UNTHREADED
OS
OF
THESE
WHICH
SHOULD
BELONG
TO
A
THREAD
CONTROL
BLOCK
AND
WHICH
SHOULD
BELONG
TO
A
PROCESS
CONTROL
BLOCK
FOR
A
MULTITHREADED
SYSTEM
LIST
REASONS
WHY
A
MODE
SWITCH
BETWEEN
THREADS
MAY
BE
CHEAPER
THAN
A
MODE
SWITCH
BETWEEN
PROCESSES
WHAT
ARE
THE
TWO
SEPARATE
AND
POTENTIALLY
INDEPENDENT
CHARACTERISTICS
EMBODIED
IN
THE
CONCEPT
OF
PROCESS
GIVE
FOUR
GENERAL
EXAMPLES
OF
THE
USE
OF
THREADS
IN
A
SINGLE
USER
MULTIPROCESSING
SYS
TEM
WHAT
RESOURCES
ARE
TYPICALLY
SHARED
BY
ALL
OF
THE
THREADS
OF
A
PROCESS
LIST
THREE
ADVANTAGES
OF
ULTS
OVER
KLTS
LIST
TWO
DISADVANTAGES
OF
ULTS
COMPARED
TO
KLTS
DEFINE
JACKETING
PROBLEMS
IT
WAS
POINTED
OUT
THAT
TWO
ADVANTAGES
OF
USING
MULTIPLE
THREADS
WITHIN
A
PROCESS
ARE
THAT
LESS
WORK
IS
INVOLVED
IN
CREATING
A
NEW
THREAD
WITHIN
AN
EXISTING
PROCESS
THAN
IN
CREATING
A
NEW
PROCESS
AND
COMMUNICATION
AMONG
THREADS
WITHIN
THE
SAME
PROCESS
IS
SIMPLIFIED
IS
IT
ALSO
THE
CASE
THAT
A
MODE
SWITCH
BETWEEN
TWO
THREADS
WITHIN
THE
SAME
PROCESS
INVOLVES
LESS
WORK
THAN
A
MODE
SWITCH
BETWEEN
TWO
THREADS
IN
DIFFERENT
PROCESSES
IN
THE
DISCUSSION
OF
ULTS
VERSUS
KLTS
IT
WAS
POINTED
OUT
THAT
A
DISADVANTAGE
OF
ULTS
IS
THAT
WHEN
A
ULT
EXECUTES
A
SYSTEM
CALL
NOT
ONLY
IS
THAT
THREAD
BLOCKED
BUT
ALSO
ALL
OF
THE
THREADS
WITHIN
THE
PROCESS
ARE
BLOCKED
WHY
IS
THAT
SO
OS
IS
AN
OBSOLETE
OS
FOR
PCS
FROM
IBM
IN
OS
WHAT
IS
COMMONLY
EMBODIED
IN
THE
CONCEPT
OF
PROCESS
IN
OTHER
OPERATING
SYSTEMS
IS
SPLIT
INTO
THREE
SEPARATE
TYPES
OF
ENTITIES
SESSION
PROCESSES
AND
THREADS
A
SESSION
IS
A
COLLECTION
OF
ONE
OR
MORE
PROCESSES
ASSOCIATED
WITH
A
USER
INTERFACE
KEYBOARD
DISPLAY
AND
MOUSE
THE
SES
SION
REPRESENTS
AN
INTERACTIVE
USER
APPLICATION
SUCH
AS
A
WORD
PROCESSING
PROGRAM
OR
A
SPREADSHEET
THIS
CONCEPT
ALLOWS
THE
PERSONAL
COMPUTER
USER
TO
OPEN
MORE
THAN
ONE
APPLICATION
GIVING
EACH
ONE
OR
MORE
WINDOWS
ON
THE
SCREEN
THE
OS
MUST
KEEP
TRACK
OF
WHICH
WINDOW
AND
THEREFORE
WHICH
SESSION
IS
ACTIVE
SO
THAT
KEYBOARD
AND
MOUSE
INPUT
ARE
ROUTED
TO
THE
APPROPRIATE
SESSION
AT
ANY
TIME
ONE
SESSION
IS
IN
FOREGROUND
MODE
WITH
OTHER
SESSIONS
IN
BACKGROUND
MODE
ALL
KEYBOARD
AND
MOUSE
INPUT
IS
DIRECTED
TO
ONE
OF
THE
PROCESSES
OF
THE
FOREGROUND
SESSION
AS
DICTATED
BY
CHAPTER
THREADS
THE
APPLICATIONS
WHEN
A
SESSION
IS
IN
FOREGROUND
MODE
A
PROCESS
PERFORMING
VIDEO
OUTPUT
SENDS
IT
DIRECTLY
TO
THE
HARDWARE
VIDEO
BUFFER
AND
THENCE
TO
THE
USER
SCREEN
WHEN
THE
SESSION
IS
MOVED
TO
THE
BACKGROUND
THE
HARDWARE
VIDEO
BUFFER
IS
SAVED
TO
A
LOGICAL
VIDEO
BUFFER
FOR
THAT
SESSION
WHILE
A
SESSION
IS
IN
BACKGROUND
IF
ANY
OF
THE
THREADS
OF
ANY
OF
THE
PROCESSES
OF
THAT
SESSION
EXECUTES
AND
PRODUCES
SCREEN
OUTPUT
THAT
OUTPUT
IS
DIRECTED
TO
THE
LOGICAL
VIDEO
BUFFER
WHEN
THE
SESSION
RETURNS
TO
FORE
GROUND
THE
SCREEN
IS
UPDATED
TO
REFLECT
THE
CURRENT
CONTENTS
OF
THE
LOGICAL
VIDEO
BUFFER
FOR
THE
NEW
FOREGROUND
SESSION
THERE
IS
A
WAY
TO
REDUCE
THE
NUMBER
OF
PROCESS
RELATED
CONCEPTS
IN
OS
FROM
THREE
TO
TWO
ELIMINATE
SESSIONS
AND
ASSOCIATE
THE
USER
INTERFACE
KEYBOARD
MOUSE
AND
SCREEN
WITH
PROCESSES
THUS
ONE
PROCESS
AT
A
TIME
IS
IN
FOREGROUND
MODE
FOR
FURTHER
STRUCTURING
PROCESSES
CAN
BE
BROKEN
UP
INTO
THREADS
A
WHAT
BENEFITS
ARE
LOST
WITH
THIS
APPROACH
B
IF
YOU
GO
AHEAD
WITH
THIS
MODIFICATION
WHERE
DO
YOU
ASSIGN
RESOURCES
MEMORY
FILES
ETC
AT
THE
PROCESS
OR
THREAD
LEVEL
CONSIDER
AN
ENVIRONMENT
IN
WHICH
THERE
IS
A
ONE
TO
ONE
MAPPING
BETWEEN
USER
LEVEL
THREADS
AND
KERNEL
LEVEL
THREADS
THAT
ALLOWS
ONE
OR
MORE
THREADS
WITHIN
A
PROCESS
TO
ISSUE
BLOCKING
SYSTEM
CALLS
WHILE
OTHER
THREADS
CONTINUE
TO
RUN
EXPLAIN
WHY
THIS
MODEL
CAN
MAKE
MULTITHREADED
PROGRAMS
RUN
FASTER
THAN
THEIR
SINGLE
THREADED
COUN
TERPARTS
ON
A
UNIPROCESSOR
COMPUTER
IF
A
PROCESS
EXITS
AND
THERE
ARE
STILL
THREADS
OF
THAT
PROCESS
RUNNING
WILL
THEY
CONTINUE
TO
RUN
THE
OS
MAINFRAME
OPERATING
SYSTEM
IS
STRUCTURED
AROUND
THE
CONCEPTS
OF
ADDRESS
SPACE
AND
TASK
ROUGHLY
SPEAKING
A
SINGLE
ADDRESS
SPACE
CORRESPONDS
TO
A
SINGLE
APPLICATION
AND
CORRESPONDS
MORE
OR
LESS
TO
A
PROCESS
IN
OTHER
OPERAT
ING
SYSTEMS
WITHIN
AN
ADDRESS
SPACE
A
NUMBER
OF
TASKS
MAY
BE
GENERATED
AND
EXECUTE
CONCURRENTLY
THIS
CORRESPONDS
ROUGHLY
TO
THE
CONCEPT
OF
MULTITHREADING
TWO
DATA
STRUCTURES
ARE
KEY
TO
MANAGING
THIS
TASK
STRUCTURE
AN
ADDRESS
SPACE
CONTROL
BLOCK
ASCB
CONTAINS
INFORMATION
ABOUT
AN
ADDRESS
SPACE
NEEDED
BY
OS
WHETHER
OR
NOT
THAT
ADDRESS
SPACE
IS
EXECUTING
INFORMATION
IN
THE
ASCB
INCLUDES
DISPATCHING
PRIORITY
REAL
AND
VIRTUAL
MEMORY
ALLOCATED
TO
THIS
ADDRESS
SPACE
THE
NUMBER
OF
READY
TASKS
IN
THIS
ADDRESS
SPACE
AND
WHETHER
EACH
IS
SWAPPED
OUT
A
TASK
CONTROL
BLOCK
TCB
REPRESENTS
A
USER
PROGRAM
IN
EXECUTION
IT
CONTAINS
INFORMATION
NEEDED
FOR
MANAGING
A
TASK
WITHIN
AN
ADDRESS
SPACE
INCLUDING
PROCESSOR
STATUS
INFORMATION
POINTERS
TO
PROGRAMS
THAT
ARE
PART
OF
THIS
TASK
AND
TASK
EXECUTION
STATE
ASCBS
ARE
GLOBAL
STRUCTURES
MAINTAINED
IN
SYSTEM
MEMORY
WHILE
TCBS
ARE
LOCAL
STRUCTURES
MAINTAINED
WITHIN
THEIR
ADDRESS
SPACE
WHAT
IS
THE
ADVANTAGE
OF
SPLITTING
THE
CONTROL
INFORMATION
INTO
GLOBAL
AND
LOCAL
PORTIONS
MANY
CURRENT
LANGUAGE
SPECIFICATIONS
SUCH
AS
FOR
C
AND
C
ARE
INADEQUATE
FOR
MULTITHREADED
PROGRAMS
THIS
CAN
HAVE
AN
IMPACT
ON
COMPILERS
AND
THE
CORRECTNESS
OF
CODE
AS
THIS
PROBLEM
ILLUSTRATES
CONSIDER
THE
FOLLOWING
DECLARATIONS
AND
FUNCTION
DEFINITION
INT
TYPEDEF
STRUCT
LIST
STRUCT
LIST
NEXT
DOUBLE
VAL
LIST
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
VOID
LIST
L
LIST
P
FOR
P
L
P
P
P
NEXT
IF
P
VAL
NOW
CONSIDER
THE
CASE
IN
WHICH
THREAD
A
PERFORMS
LIST
CONTAINING
ONLY
NEGATIVE
VALUES
WHILE
THREAD
B
PERFORMS
A
WHAT
DOES
THE
FUNCTION
DO
B
THE
C
LANGUAGE
ONLY
ADDRESSES
SINGLE
THREADED
EXECUTION
DOES
THE
USE
OF
TWO
PARALLEL
THREADS
CREATE
ANY
PROBLEMS
OR
POTENTIAL
PROBLEMS
BUT
SOME
EXISTING
OPTIMIZING
COMPILERS
INCLUDING
GCC
WHICH
TENDS
TO
BE
RELATIVELY
CONSERVATIVE
WILL
OPTIMIZE
TO
SOMETHING
SIMILAR
TO
VOID
LIST
L
LIST
P
REGISTER
INT
R
R
FOR
P
L
P
P
P
NEXT
IF
P
VAL
R
R
WHAT
PROBLEM
OR
POTENTIAL
PROBLEM
OCCURS
WITH
THIS
COMPILED
VERSION
OF
THE
PROGRAM
IF
THREADS
A
AND
B
ARE
EXECUTED
CONCURRENTLY
CONSIDER
THE
FOLLOWING
CODE
USING
THE
POSIX
PTHREADS
API
C
INCLUDE
PTHREAD
H
INCLUDE
STDLIB
H
INCLUDE
UNISTD
H
INCLUDE
STDIO
H
INT
MYGLOBAL
VOID
VOID
ARG
INT
I
J
FOR
I
I
I
J
MYGLOBAL
J
J
PRINTF
FFLUSH
STDOUT
SLEEP
MYGLOBAL
J
CHAPTER
THREADS
RETURN
NULL
INT
MAIN
VOID
MYTHREAD
INT
I
IF
MYTHREAD
NULL
NULL
PRINTF
LDQUO
ERROR
CREATING
THREAD
ABORT
FOR
I
I
I
MYGLOBAL
MYGLOBAL
PRINTF
O
FFLUSH
STDOUT
SLEEP
IF
MYTHREAD
NULL
PRINTF
ERROR
JOINING
THREAD
ABORT
PRINTF
NMYGLOBAL
EQUALS
D
N
MYGLOBAL
EXIT
IN
MAIN
WE
FIRST
DECLARE
A
VARIABLE
CALLED
MYTHREAD
WHICH
HAS
A
TYPE
OF
THIS
IS
ESSENTIALLY
AN
ID
FOR
A
THREAD
NEXT
THE
IF
STATEMENT
CRE
ATES
A
THREAD
ASSOCIATED
WITH
MYTHREAD
THE
CALL
RETURNS
ZERO
ON
SUCCESS
AND
A
NONZERO
VALUE
ON
FAILURE
THE
THIRD
ARGUMENT
OF
CREATE
IS
THE
NAME
OF
A
FUNCTION
THAT
THE
NEW
THREAD
WILL
EXECUTE
WHEN
IT
STARTS
WHEN
THIS
RETURNS
THE
THREAD
TERMINATES
MEANWHILE
THE
MAIN
PROGRAM
ITSELF
DEFINES
A
THREAD
SO
THAT
THERE
ARE
TWO
THREADS
EXECUTING
THE
FUNCTION
ENABLES
THE
MAIN
THREAD
TO
WAIT
UNTIL
THE
NEW
THREAD
COMPLETES
A
WHAT
DOES
THIS
PROGRAM
ACCOMPLISH
B
HERE
IS
THE
OUTPUT
FROM
THE
EXECUTED
PROGRAM
O
O
O
O
OO
O
O
O
O
O
O
O
O
O
O
O
O
O
O
MYGLOBAL
EQUALS
IS
THIS
THE
OUTPUT
YOU
WOULD
EXPECT
IF
NOT
WHAT
HAS
GONE
WRONG
THE
SOLARIS
DOCUMENTATION
STATES
THAT
A
ULT
MAY
YIELD
TO
ANOTHER
THREAD
OF
THE
SAME
PRIORITY
ISN
T
IT
POSSIBLE
THAT
THERE
WILL
BE
A
RUNNABLE
THREAD
OF
HIGHER
PRIORITY
AND
THAT
THEREFORE
THE
YIELD
FUNCTION
SHOULD
RESULT
IN
YIELDING
TO
A
THREAD
OF
THE
SAME
OR
HIGHER
PRIORITY
IN
SOLARIS
AND
SOLARIS
THERE
IS
A
ONE
TO
ONE
MAPPING
BETWEEN
ULTS
AND
LWPS
IN
SOLARIS
A
SINGLE
LWP
SUPPORTS
ONE
OR
MORE
ULTS
A
WHAT
IS
THE
POSSIBLE
BENEFIT
OF
ALLOWING
A
MANY
TO
ONE
MAPPING
OF
ULTS
TO
LWPS
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
FIGURE
SOLARIS
USER
LEVEL
THREAD
AND
LWP
STATES
B
IN
SOLARIS
THE
THREAD
EXECUTION
STATE
OF
A
ULT
IS
DISTINCT
FROM
THAT
OF
ITS
LWP
EXPLAIN
WHY
C
FIGURE
SHOWS
THE
STATE
TRANSITION
DIAGRAMS
FOR
A
ULT
AND
ITS
ASSOCIATED
LWP
IN
SOLARIS
AND
EXPLAIN
THE
OPERATION
OF
THE
TWO
DIAGRAMS
AND
THEIR
RELATIONSHIPS
EXPLAIN
THE
RATIONALE
FOR
THE
UNINTERRUPTIBLE
STATE
IN
LINUX
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
PRINCIPLES
OF
CONCURRENCY
A
SIMPLE
EXAMPLE
RACE
CONDITION
OPERATING
SYSTEM
CONCERNS
PROCESS
INTERACTION
REQUIREMENTS
FOR
MUTUAL
EXCLUSION
MUTUAL
EXCLUSION
HARDWARE
SUPPORT
INTERRUPT
DISABLING
SPECIAL
MACHINE
INSTRUCTIONS
SEMAPHORES
MUTUAL
EXCLUSION
THE
PRODUCER
CONSUMER
PROBLEM
IMPLEMENTATION
OF
SEMAPHORES
MONITORS
MONITOR
WITH
SIGNAL
ALTERNATE
MODEL
OF
MONITORS
WITH
NOTIFY
AND
BROADCAST
MESSAGE
PASSING
SYNCHRONIZATION
ADDRESSING
MESSAGE
FORMAT
QUEUEING
DISCIPLINE
MUTUAL
EXCLUSION
READERS
WRITERS
PROBLEM
READERS
HAVE
PRIORITY
WRITERS
HAVE
PRIORITY
SUMMARY
RECOMMENDED
READING
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
DESIGNING
CORRECT
ROUTINES
FOR
CONTROLLING
CONCURRENT
ACTIVITIES
PROVED
TO
BE
ONE
OF
THE
MOST
DIFFICULT
ASPECTS
OF
SYSTEMS
PROGRAMMING
THE
AD
HOC
TECHNIQUES
USED
BY
PROGRAMMERS
OF
EARLY
MULTIPROGRAMMING
AND
REAL
TIME
SYSTEMS
WERE
ALWAYS
VULNERABLE
TO
SUBTLE
PROGRAMMING
ERRORS
WHOSE
EFFECTS
COULD
BE
OBSERVED
ONLY
WHEN
CERTAIN
RELATIVELY
RARE
SEQUENCES
OF
ACTIONS
OCCURRED
THE
ERRORS
ARE
PARTICULARLY
DIFFICULT
TO
LOCATE
SINCE
THE
PRECISE
CONDITIONS
UNDER
WHICH
THEY
APPEAR
ARE
VERY
HARD
TO
REPRODUCE
THE
COMPUTER
SCIENCE
AND
ENGINEERING
RESEARCH
STUDY
MIT
PRESS
THE
CENTRAL
THEMES
OF
OPERATING
SYSTEM
DESIGN
ARE
ALL
CONCERNED
WITH
THE
MANAGE
MENT
OF
PROCESSES
AND
THREADS
MULTIPROGRAMMING
THE
MANAGEMENT
OF
MULTIPLE
PROCESSES
WITHIN
A
UNIPRO
CESSOR
SYSTEM
MULTIPROCESSING
THE
MANAGEMENT
OF
MULTIPLE
PROCESSES
WITHIN
A
MULTIPROCESSOR
DISTRIBUTED
PROCESSING
THE
MANAGEMENT
OF
MULTIPLE
PROCESSES
EXECUTING
ON
MULTIPLE
DISTRIBUTED
COMPUTER
SYSTEMS
THE
RECENT
PROLIFERATION
OF
CLUSTERS
IS
A
PRIME
EXAMPLE
OF
THIS
TYPE
OF
SYSTEM
FUNDAMENTAL
TO
ALL
OF
THESE
AREAS
AND
FUNDAMENTAL
TO
OS
DESIGN
IS
CONCURRENCY
CONCURRENCY
ENCOMPASSES
A
HOST
OF
DESIGN
ISSUES
INCLUDING
COMMUNICATION
AMONG
PRO
CESSES
SHARING
OF
AND
COMPETING
FOR
RESOURCES
SUCH
AS
MEMORY
FILES
AND
I
O
ACCESS
SYNCHRONIZATION
OF
THE
ACTIVITIES
OF
MULTIPLE
PROCESSES
AND
ALLOCATION
OF
PROCESSOR
TIME
TO
PROCESSES
WE
SHALL
SEE
THAT
THESE
ISSUES
ARISE
NOT
JUST
IN
MULTIPROCESSING
AND
DISTRIB
UTED
PROCESSING
ENVIRONMENTS
BUT
EVEN
IN
SINGLE
PROCESSOR
MULTIPROGRAMMING
SYSTEMS
CONCURRENCY
ARISES
IN
THREE
DIFFERENT
CONTEXTS
MULTIPLE
APPLICATIONS
MULTIPROGRAMMING
WAS
INVENTED
TO
ALLOW
PROCESSING
TIME
TO
BE
DYNAMICALLY
SHARED
AMONG
A
NUMBER
OF
ACTIVE
APPLICATIONS
STRUCTURED
APPLICATIONS
AS
AN
EXTENSION
OF
THE
PRINCIPLES
OF
MODULAR
DESIGN
AND
STRUCTURED
PROGRAMMING
SOME
APPLICATIONS
CAN
BE
EFFECTIVELY
PROGRAMMED
AS
A
SET
OF
CONCURRENT
PROCESSES
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
OPERATING
SYSTEM
STRUCTURE
THE
SAME
STRUCTURING
ADVANTAGES
APPLY
TO
SYSTEMS
PROGRAMS
AND
WE
HAVE
SEEN
THAT
OPERATING
SYSTEMS
ARE
THEMSELVES
OFTEN
IM
PLEMENTED
AS
A
SET
OF
PROCESSES
OR
THREADS
BECAUSE
OF
THE
IMPORTANCE
OF
THIS
TOPIC
FOUR
CHAPTERS
AND
AN
APPENDIX
FOCUS
ON
CONCURRENCY
RELATED
ISSUES
CHAPTERS
AND
DEAL
WITH
CONCURRENCY
IN
MULTIPRO
GRAMMING
AND
MULTIPROCESSING
SYSTEMS
CHAPTERS
AND
EXAMINE
CONCURRENCY
ISSUES
RELATED
TO
DISTRIBUTED
PROCESSING
THIS
CHAPTER
BEGINS
WITH
AN
INTRODUCTION
TO
THE
CONCEPT
OF
CONCURRENCY
AND
THE
IMPLICATIONS
OF
THE
EXECUTION
OF
MULTIPLE
CONCURRENT
PROCESSES
WE
FIND
THAT
THE
BASIC
REQUIREMENT
FOR
SUPPORT
OF
CONCURRENT
PROCESSES
IS
THE
ABILITY
TO
ENFORCE
MUTUAL
EXCLU
SION
THAT
IS
THE
ABILITY
TO
EXCLUDE
ALL
OTHER
PROCESSES
FROM
A
COURSE
OF
ACTION
WHILE
ONE
PROCESS
IS
GRANTED
THAT
ABILITY
NEXT
WE
EXAMINE
SOME
HARDWARE
MECHANISMS
THAT
CAN
SUPPORT
MUTUAL
EXCLUSION
THEN
WE
LOOK
AT
SOLUTIONS
THAT
DO
NOT
INVOLVE
BUSY
WAITING
AND
THAT
CAN
BE
SUPPORTED
EITHER
BY
THE
OS
OR
ENFORCED
BY
LANGUAGE
COMPILERS
WE
EXAMINE
THREE
APPROACHES
SEMAPHORES
MONITORS
AND
MESSAGE
PASSING
TWO
CLASSIC
PROBLEMS
IN
CONCURRENCY
ARE
USED
TO
ILLUSTRATE
THE
CONCEPTS
AND
COMPARE
THE
APPROACHES
PRESENTED
IN
THIS
CHAPTER
THE
PRODUCER
CONSUMER
PROB
LEM
IS
INTRODUCED
IN
SECTION
AND
USED
AS
A
RUNNING
EXAMPLE
THE
CHAPTER
CLOSES
WITH
THE
READERS
WRITERS
PROBLEM
OUR
DISCUSSION
OF
CONCURRENCY
CONTINUES
IN
CHAPTER
AND
WE
DEFER
A
DISCUS
SION
OF
THE
CONCURRENCY
MECHANISMS
OF
OUR
EXAMPLE
SYSTEMS
UNTIL
THE
END
OF
THAT
CHAPTER
APPENDIX
A
COVERS
ADDITIONAL
TOPICS
ON
CONCURRENCY
TABLE
LISTS
SOME
KEY
TERMS
RELATED
TO
CONCURRENCY
A
SET
OF
ANIMATIONS
THAT
ILLUSTRATE
CONCEPTS
IN
THIS
CHAPTER
IS
AVAILABLE
ONLINE
CLICK
ON
THE
ROTATING
GLOBE
AT
THIS
BOOK
WEB
SITE
AT
WILLIAMSTALLINGS
COM
OS
HTML
FOR
ACCESS
TABLE
SOME
KEY
TERMS
RELATED
TO
CONCURRENCY
ATOMIC
OPERATION
A
FUNCTION
OR
ACTION
IMPLEMENTED
AS
A
SEQUENCE
OF
ONE
OR
MORE
INSTRUCTIONS
THAT
APPEARS
TO
BE
INDIVISIBLE
THAT
IS
NO
OTHER
PROCESS
CAN
SEE
AN
INTERMEDIATE
STATE
OR
INTERRUPT
THE
OPERATION
THE
SEQUENCE
OF
INSTRUCTION
IS
GUARANTEED
TO
EXECUTE
AS
A
GROUP
OR
NOT
EXECUTE
AT
ALL
HAVING
NO
VISIBLE
EFFECT
ON
SYSTEM
STATE
ATOMICITY
GUARANTEES
ISOLATION
FROM
CONCURRENT
PROCESSES
CRITICAL
SECTION
A
SECTION
OF
CODE
WITHIN
A
PROCESS
THAT
REQUIRES
ACCESS
TO
SHARED
RESOURCES
AND
THAT
MUST
NOT
BE
EXECUTED
WHILE
ANOTHER
PROCESS
IS
IN
A
CORRESPONDING
SECTION
OF
CODE
DEADLOCK
A
SITUATION
IN
WHICH
TWO
OR
MORE
PROCESSES
ARE
UNABLE
TO
PROCEED
BECAUSE
EACH
IS
WAITING
FOR
ONE
OF
THE
OTHERS
TO
DO
SOMETHING
LIVELOCK
A
SITUATION
IN
WHICH
TWO
OR
MORE
PROCESSES
CONTINUOUSLY
CHANGE
THEIR
STATES
IN
RESPONSE
TO
CHANGES
IN
THE
OTHER
PROCESS
ES
WITHOUT
DOING
ANY
USEFUL
WORK
MUTUAL
EXCLUSION
THE
REQUIREMENT
THAT
WHEN
ONE
PROCESS
IS
IN
A
CRITICAL
SECTION
THAT
ACCESSES
SHARED
RESOURCES
NO
OTHER
PROCESS
MAY
BE
IN
A
CRITICAL
SECTION
THAT
ACCESSES
ANY
OF
THOSE
SHARED
RESOURCES
RACE
CONDITION
A
SITUATION
IN
WHICH
MULTIPLE
THREADS
OR
PROCESSES
READ
AND
WRITE
A
SHARED
DATA
ITEM
AND
THE
FINAL
RESULT
DEPENDS
ON
THE
RELATIVE
TIMING
OF
THEIR
EXECUTION
STARVATION
A
SITUATION
IN
WHICH
A
RUNNABLE
PROCESS
IS
OVERLOOKED
INDEFINITELY
BY
THE
SCHEDULER
ALTHOUGH
IT
IS
ABLE
TO
PROCEED
IT
IS
NEVER
CHOSEN
SIMPLICITY
WE
GENERALLY
REFER
TO
THE
CONCURRENT
EXECUTION
OF
PROCESSES
IN
FACT
AS
WE
HAVE
SEEN
IN
THE
PRECEDING
CHAPTER
IN
SOME
SYSTEMS
THE
FUNDAMENTAL
UNIT
OF
CONCURRENCY
IS
A
THREAD
RATHER
THAN
A
PROCESS
PRINCIPLES
OF
CONCURRENCY
PRINCIPLES
OF
CONCURRENCY
IN
A
SINGLE
PROCESSOR
MULTIPROGRAMMING
SYSTEM
PROCESSES
ARE
INTERLEAVED
IN
TIME
TO
YIELD
THE
APPEARANCE
OF
SIMULTANEOUS
EXECUTION
FIGURE
EVEN
THOUGH
ACTUAL
PARALLEL
PROCESSING
IS
NOT
ACHIEVED
AND
EVEN
THOUGH
THERE
IS
A
CERTAIN
AMOUNT
OF
OVERHEAD
INVOLVED
IN
SWITCHING
BACK
AND
FORTH
BETWEEN
PROCESSES
INTERLEAVED
EXECUTION
PROVIDES
MAJOR
BENEFITS
IN
PROCESSING
EFFICIENCY
AND
IN
PROGRAM
STRUCTURING
IN
A
MULTIPLE
PROCESSOR
SYSTEM
IT
IS
POSSIBLE
NOT
ONLY
TO
INTERLEAVE
THE
EXECUTION
OF
MULTIPLE
PROCESSES
BUT
ALSO
TO
OVERLAP
THEM
FIGURE
AT
FIRST
GLANCE
IT
MAY
SEEM
THAT
INTERLEAVING
AND
OVERLAPPING
REPRESENT
FUNDA
MENTALLY
DIFFERENT
MODES
OF
EXECUTION
AND
PRESENT
DIFFERENT
PROBLEMS
IN
FACT
BOTH
TECHNIQUES
CAN
BE
VIEWED
AS
EXAMPLES
OF
CONCURRENT
PROCESSING
AND
BOTH
PRESENT
THE
SAME
PROBLEMS
IN
THE
CASE
OF
A
UNIPROCESSOR
THE
PROBLEMS
STEM
FROM
A
BASIC
CHARACTERISTIC
OF
MULTIPROGRAMMING
SYSTEMS
THE
RELATIVE
SPEED
OF
EXECUTION
OF
PROCESSES
CANNOT
BE
PREDICTED
IT
DEPENDS
ON
THE
ACTIVITIES
OF
OTHER
PROCESSES
THE
WAY
IN
WHICH
THE
OS
HANDLES
INTERRUPTS
AND
THE
SCHEDULING
POLICIES
OF
THE
OS
THE
FOLLOWING
DIFFICULTIES
ARISE
THE
SHARING
OF
GLOBAL
RESOURCES
IS
FRAUGHT
WITH
PERIL
FOR
EXAMPLE
IF
TWO
PROCESSES
BOTH
MAKE
USE
OF
THE
SAME
GLOBAL
VARIABLE
AND
BOTH
PERFORM
READS
AND
WRITES
ON
THAT
VARIABLE
THEN
THE
ORDER
IN
WHICH
THE
VARIOUS
READS
AND
WRITES
ARE
EXECUTED
IS
CRITICAL
AN
EXAMPLE
OF
THIS
PROBLEM
IS
SHOWN
IN
THE
FOLLOWING
SUBSECTION
IT
IS
DIFFICULT
FOR
THE
OS
TO
MANAGE
THE
ALLOCATION
OF
RESOURCES
OPTIMALLY
FOR
EXAMPLE
PROCESS
A
MAY
REQUEST
USE
OF
AND
BE
GRANTED
CONTROL
OF
A
PARTICULAR
I
O
CHANNEL
AND
THEN
BE
SUSPENDED
BEFORE
USING
THAT
CHANNEL
IT
MAY
BE
UNDE
SIRABLE
FOR
THE
OS
SIMPLY
TO
LOCK
THE
CHANNEL
AND
PREVENT
ITS
USE
BY
OTHER
PRO
CESSES
INDEED
THIS
MAY
LEAD
TO
A
DEADLOCK
CONDITION
AS
DESCRIBED
IN
CHAPTER
IT
BECOMES
VERY
DIFFICULT
TO
LOCATE
A
PROGRAMMING
ERROR
BECAUSE
RESULTS
ARE
TYPICALLY
NOT
DETERMINISTIC
AND
REPRODUCIBLE
E
G
SEE
FOR
A
DISCUSSION
OF
THIS
POINT
ALL
OF
THE
FOREGOING
DIFFICULTIES
PRESENT
THEMSELVES
IN
A
MULTIPROCESSOR
SYSTEM
AS
WELL
BECAUSE
HERE
TOO
THE
RELATIVE
SPEED
OF
EXECUTION
OF
PROCESSES
IS
UNPREDICTABLE
A
MULTIPROCESSOR
SYSTEM
MUST
ALSO
DEAL
WITH
PROBLEMS
ARISING
FROM
THE
SIMULTANEOUS
EXECUTION
OF
MULTIPLE
PROCESSES
FUNDAMENTALLY
HOWEVER
THE
PROBLEMS
ARE
THE
SAME
AS
THOSE
FOR
UNIPROCESSOR
SYSTEMS
THIS
SHOULD
BECOME
CLEAR
AS
THE
DISCUSSION
PROCEEDS
A
SIMPLE
EXAMPLE
CONSIDER
THE
FOLLOWING
PROCEDURE
VOID
ECHO
CHIN
GETCHAR
CHOUT
CHIN
PUTCHAR
CHOUT
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
THIS
PROCEDURE
SHOWS
THE
ESSENTIAL
ELEMENTS
OF
A
PROGRAM
THAT
WILL
PROVIDE
A
CHAR
ACTER
ECHO
PROCEDURE
INPUT
IS
OBTAINED
FROM
A
KEYBOARD
ONE
KEYSTROKE
AT
A
TIME
EACH
INPUT
CHARACTER
IS
STORED
IN
VARIABLE
CHIN
IT
IS
THEN
TRANSFERRED
TO
VARIABLE
CHOUT
AND
SENT
TO
THE
DISPLAY
ANY
PROGRAM
CAN
CALL
THIS
PROCEDURE
REPEATEDLY
TO
ACCEPT
USER
INPUT
AND
DISPLAY
IT
ON
THE
USER
SCREEN
NOW
CONSIDER
THAT
WE
HAVE
A
SINGLE
PROCESSOR
MULTIPROGRAMMING
SYSTEM
SUPPORTING
A
SINGLE
USER
THE
USER
CAN
JUMP
FROM
ONE
APPLICATION
TO
ANOTHER
AND
EACH
APPLICATION
USES
THE
SAME
KEYBOARD
FOR
INPUT
AND
THE
SAME
SCREEN
FOR
OUTPUT
BECAUSE
EACH
APPLICATION
NEEDS
TO
USE
THE
PROCEDURE
ECHO
IT
MAKES
SENSE
FOR
IT
TO
BE
A
SHARED
PROCEDURE
THAT
IS
LOADED
INTO
A
PORTION
OF
MEMORY
GLOBAL
TO
ALL
APPLICATIONS
THUS
ONLY
A
SINGLE
COPY
OF
THE
ECHO
PROCEDURE
IS
USED
SAVING
SPACE
THE
SHARING
OF
MAIN
MEMORY
AMONG
PROCESSES
IS
USEFUL
TO
PERMIT
EFFICIENT
AND
CLOSE
INTERACTION
AMONG
PROCESSES
HOWEVER
SUCH
SHARING
CAN
LEAD
TO
PROBLEMS
CONSIDER
THE
FOLLOWING
SEQUENCE
PROCESS
INVOKES
THE
ECHO
PROCEDURE
AND
IS
INTERRUPTED
IMMEDIATELY
AFTER
GETCHAR
RETURNS
ITS
VALUE
AND
STORES
IT
IN
CHIN
AT
THIS
POINT
THE
MOST
RECENTLY
ENTERED
CHARACTER
X
IS
STORED
IN
VARIABLE
CHIN
PROCESS
IS
ACTIVATED
AND
INVOKES
THE
ECHO
PROCEDURE
WHICH
RUNS
TO
CONCLU
SION
INPUTTING
AND
THEN
DISPLAYING
A
SINGLE
CHARACTER
Y
ON
THE
SCREEN
PROCESS
IS
RESUMED
BY
THIS
TIME
THE
VALUE
X
HAS
BEEN
OVERWRITTEN
IN
CHIN
AND
THEREFORE
LOST
INSTEAD
CHIN
CONTAINS
Y
WHICH
IS
TRANSFERRED
TO
CHOUT
AND
DISPLAYED
THUS
THE
FIRST
CHARACTER
IS
LOST
AND
THE
SECOND
CHARACTER
IS
DISPLAYED
TWICE
THE
ESSENCE
OF
THIS
PROBLEM
IS
THE
SHARED
GLOBAL
VARIABLE
CHIN
MULTIPLE
PROCESSES
HAVE
ACCESS
TO
THIS
VARIABLE
IF
ONE
PROCESS
UPDATES
THE
GLOBAL
VARIABLE
AND
THEN
IS
INTERRUPTED
ANOTHER
PROCESS
MAY
ALTER
THE
VARIABLE
BEFORE
THE
FIRST
PROCESS
CAN
USE
ITS
VALUE
SUPPOSE
HOWEVER
THAT
WE
PERMIT
ONLY
ONE
PROCESS
AT
A
TIME
TO
BE
IN
THAT
PROCEDURE
THEN
THE
FOREGOING
SEQUENCE
WOULD
RESULT
IN
THE
FOLLOWING
PROCESS
INVOKES
THE
ECHO
PROCEDURE
AND
IS
INTERRUPTED
IMMEDIATELY
AFTER
THE
CONCLUSION
OF
THE
INPUT
FUNCTION
AT
THIS
POINT
THE
MOST
RECENTLY
ENTERED
CHARACTER
X
IS
STORED
IN
VARIABLE
CHIN
PROCESS
IS
ACTIVATED
AND
INVOKES
THE
ECHO
PROCEDURE
HOWEVER
BECAUSE
IS
STILL
INSIDE
THE
ECHO
PROCEDURE
ALTHOUGH
CURRENTLY
SUSPENDED
IS
BLOCKED
FROM
ENTERING
THE
PROCEDURE
THEREFORE
IS
SUSPENDED
AWAITING
THE
AVAIL
ABILITY
OF
THE
ECHO
PROCEDURE
AT
SOME
LATER
TIME
PROCESS
IS
RESUMED
AND
COMPLETES
EXECUTION
OF
ECHO
THE
PROPER
CHARACTER
X
IS
DISPLAYED
WHEN
EXITS
ECHO
THIS
REMOVES
THE
BLOCK
ON
WHEN
IS
LATER
RESUMED
THE
ECHO
PROCEDURE
IS
SUCCESSFULLY
INVOKED
THIS
EXAMPLE
SHOWS
THAT
IT
IS
NECESSARY
TO
PROTECT
SHARED
GLOBAL
VARIABLES
AND
OTHER
SHARED
GLOBAL
RESOURCES
AND
THAT
THE
ONLY
WAY
TO
DO
THAT
IS
TO
CONTROL
THE
CODE
THAT
ACCESSES
THE
VARIABLE
IF
WE
IMPOSE
THE
DISCIPLINE
THAT
ONLY
ONE
PRINCIPLES
OF
CONCURRENCY
PROCESS
AT
A
TIME
MAY
ENTER
ECHO
AND
THAT
ONCE
IN
ECHO
THE
PROCEDURE
MUST
RUN
TO
COMPLETION
BEFORE
IT
IS
AVAILABLE
FOR
ANOTHER
PROCESS
THEN
THE
TYPE
OF
ERROR
JUST
DISCUSSED
WILL
NOT
OCCUR
HOW
THAT
DISCIPLINE
MAY
BE
IMPOSED
IS
A
MAJOR
TOPIC
OF
THIS
CHAPTER
THIS
PROBLEM
WAS
STATED
WITH
THE
ASSUMPTION
THAT
THERE
WAS
A
SINGLE
PROCESSOR
MULTIPROGRAMMING
OS
THE
EXAMPLE
DEMONSTRATES
THAT
THE
PROBLEMS
OF
CONCUR
RENCY
OCCUR
EVEN
WHEN
THERE
IS
A
SINGLE
PROCESSOR
IN
A
MULTIPROCESSOR
SYSTEM
THE
SAME
PROBLEMS
OF
PROTECTED
SHARED
RESOURCES
ARISE
AND
THE
SAME
SOLUTION
WORKS
FIRST
SUPPOSE
THAT
THERE
IS
NO
MECHANISM
FOR
CONTROLLING
ACCESS
TO
THE
SHARED
GLOBAL
VARIABLE
PROCESSES
AND
ARE
BOTH
EXECUTING
EACH
ON
A
SEPARATE
PROCESSOR
BOTH
PROCESSES
INVOKE
THE
ECHO
PROCEDURE
THE
FOLLOWING
EVENTS
OCCUR
EVENTS
ON
THE
SAME
LINE
TAKE
PLACE
IN
PARALLEL
PROCESS
PROCESS
CHIN
GETCHAR
CHIN
GETCHAR
CHOUT
CHIN
CHOUT
CHIN
PUTCHAR
CHOUT
PUTCHAR
CHOUT
THE
RESULT
IS
THAT
THE
CHARACTER
INPUT
TO
IS
LOST
BEFORE
BEING
DISPLAYED
AND
THE
CHARACTER
INPUT
TO
IS
DISPLAYED
BY
BOTH
AND
AGAIN
LET
US
ADD
THE
CAPA
BILITY
OF
ENFORCING
THE
DISCIPLINE
THAT
ONLY
ONE
PROCESS
AT
A
TIME
MAY
BE
IN
ECHO
THEN
THE
FOLLOWING
SEQUENCE
OCCURS
PROCESSES
AND
ARE
BOTH
EXECUTING
EACH
ON
A
SEPARATE
PROCESSOR
INVOKES
THE
ECHO
PROCEDURE
WHILE
IS
INSIDE
THE
ECHO
PROCEDURE
INVOKES
ECHO
BECAUSE
IS
STILL
INSIDE
THE
ECHO
PROCEDURE
WHETHER
IS
SUSPENDED
OR
EXECUTING
IS
BLOCKED
FROM
ENTERING
THE
PROCEDURE
THEREFORE
IS
SUSPENDED
AWAITING
THE
AVAILABILITY
OF
THE
ECHO
PROCEDURE
AT
A
LATER
TIME
PROCESS
COMPLETES
EXECUTION
OF
ECHO
EXITS
THAT
PROCEDURE
AND
CONTINUES
EXECUTING
IMMEDIATELY
UPON
THE
EXIT
OF
FROM
ECHO
IS
RESUMED
AND
BEGINS
EXECUTING
ECHO
IN
THE
CASE
OF
A
UNIPROCESSOR
SYSTEM
THE
REASON
WE
HAVE
A
PROBLEM
IS
THAT
AN
INTERRUPT
CAN
STOP
INSTRUCTION
EXECUTION
ANYWHERE
IN
A
PROCESS
IN
THE
CASE
OF
A
MUL
TIPROCESSOR
SYSTEM
WE
HAVE
THAT
SAME
CONDITION
AND
IN
ADDITION
A
PROBLEM
CAN
BE
CAUSED
BECAUSE
TWO
PROCESSES
MAY
BE
EXECUTING
SIMULTANEOUSLY
AND
BOTH
TRYING
TO
ACCESS
THE
SAME
GLOBAL
VARIABLE
HOWEVER
THE
SOLUTION
TO
BOTH
TYPES
OF
PROBLEM
IS
THE
SAME
CONTROL
ACCESS
TO
THE
SHARED
RESOURCE
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
RACE
CONDITION
A
RACE
CONDITION
OCCURS
WHEN
MULTIPLE
PROCESSES
OR
THREADS
READ
AND
WRITE
DATA
ITEMS
SO
THAT
THE
FINAL
RESULT
DEPENDS
ON
THE
ORDER
OF
EXECUTION
OF
INSTRUCTIONS
IN
THE
MULTIPLE
PROCESSES
LET
US
CONSIDER
TWO
SIMPLE
EXAMPLES
AS
A
FIRST
EXAMPLE
SUPPOSE
THAT
TWO
PROCESSES
AND
SHARE
THE
GLOBAL
VARIABLE
A
AT
SOME
POINT
IN
ITS
EXECUTION
UPDATES
A
TO
THE
VALUE
AND
AT
SOME
POINT
IN
ITS
EXECUTION
UPDATES
A
TO
THE
VALUE
THUS
THE
TWO
TASKS
ARE
IN
A
RACE
TO
WRITE
VARIABLE
A
IN
THIS
EXAMPLE
THE
LOSER
OF
THE
RACE
THE
PROCESS
THAT
UPDATES
LAST
DETERMINES
THE
FINAL
VALUE
OF
A
FOR
OUR
SECOND
EXAMPLE
CONSIDER
TWO
PROCESS
AND
THAT
SHARE
GLOBAL
VARIABLES
B
AND
C
WITH
INITIAL
VALUES
B
AND
C
AT
SOME
POINT
IN
ITS
EXECU
TION
EXECUTES
THE
ASSIGNMENT
B
B
C
AND
AT
SOME
POINT
IN
ITS
EXECUTION
EXECUTES
THE
ASSIGNMENT
C
B
C
NOTE
THAT
THE
TWO
PROCESSES
UPDATE
DIFFER
ENT
VARIABLES
HOWEVER
THE
FINAL
VALUES
OF
THE
TWO
VARIABLES
DEPEND
ON
THE
ORDER
IN
WHICH
THE
TWO
PROCESSES
EXECUTE
THESE
TWO
ASSIGNMENTS
IF
EXECUTES
ITS
ASSIGNMENT
STATEMENT
FIRST
THEN
THE
FINAL
VALUES
ARE
B
AND
C
IF
EXECUTES
ITS
ASSIGN
MENT
STATEMENT
FIRST
THEN
THE
FINAL
VALUES
ARE
B
AND
C
APPENDIX
A
INCLUDES
A
DISCUSSION
OF
RACE
CONDITIONS
USING
SEMAPHORES
AS
AN
EXAMPLE
OPERATING
SYSTEM
CONCERNS
WHAT
DESIGN
AND
MANAGEMENT
ISSUES
ARE
RAISED
BY
THE
EXISTENCE
OF
CONCURRENCY
WE
CAN
LIST
THE
FOLLOWING
CONCERNS
THE
OS
MUST
BE
ABLE
TO
KEEP
TRACK
OF
THE
VARIOUS
PROCESSES
THIS
IS
DONE
WITH
THE
USE
OF
PROCESS
CONTROL
BLOCKS
AND
WAS
DESCRIBED
IN
CHAPTER
THE
OS
MUST
ALLOCATE
AND
DEALLOCATE
VARIOUS
RESOURCES
FOR
EACH
ACTIVE
PROCESS
AT
TIMES
MULTIPLE
PROCESSES
WANT
ACCESS
TO
THE
SAME
RESOURCE
THESE
RESOURCES
INCLUDE
PROCESSOR
TIME
THIS
IS
THE
SCHEDULING
FUNCTION
DISCUSSED
IN
PART
FOUR
MEMORY
MOST
OPERATING
SYSTEMS
USE
A
VIRTUAL
MEMORY
SCHEME
THE
TOPIC
IS
ADDRESSED
IN
PART
THREE
FILES
DISCUSSED
IN
CHAPTER
I
O
DEVICES
DISCUSSED
IN
CHAPTER
THE
OS
MUST
PROTECT
THE
DATA
AND
PHYSICAL
RESOURCES
OF
EACH
PROCESS
AGAINST
UNINTENDED
INTERFERENCE
BY
OTHER
PROCESSES
THIS
INVOLVES
TECHNIQUES
THAT
RELATE
TO
MEMORY
FILES
AND
I
O
DEVICES
A
GENERAL
TREATMENT
OF
PROTECTION
IS
FOUND
IN
PART
SEVEN
THE
FUNCTIONING
OF
A
PROCESS
AND
THE
OUTPUT
IT
PRODUCES
MUST
BE
INDEPENDENT
OF
THE
SPEED
AT
WHICH
ITS
EXECUTION
IS
CARRIED
OUT
RELATIVE
TO
THE
SPEED
OF
OTHER
CONCURRENT
PROCESSES
THIS
IS
THE
SUBJECT
OF
THIS
CHAPTER
TO
UNDERSTAND
HOW
THE
ISSUE
OF
SPEED
INDEPENDENCE
CAN
BE
ADDRESSED
WE
NEED
TO
LOOK
AT
THE
WAYS
IN
WHICH
PROCESSES
CAN
INTERACT
PRINCIPLES
OF
CONCURRENCY
PROCESS
INTERACTION
WE
CAN
CLASSIFY
THE
WAYS
IN
WHICH
PROCESSES
INTERACT
ON
THE
BASIS
OF
THE
DEGREE
TO
WHICH
THEY
ARE
AWARE
OF
EACH
OTHER
EXISTENCE
TABLE
LISTS
THREE
POSSIBLE
DEGREES
OF
AWARENESS
PLUS
THE
CONSEQUENCES
OF
EACH
PROCESSES
UNAWARE
OF
EACH
OTHER
THESE
ARE
INDEPENDENT
PROCESSES
THAT
ARE
NOT
INTENDED
TO
WORK
TOGETHER
THE
BEST
EXAMPLE
OF
THIS
SITUATION
IS
THE
MULTIPRO
GRAMMING
OF
MULTIPLE
INDEPENDENT
PROCESSES
THESE
CAN
EITHER
BE
BATCH
JOBS
OR
INTERACTIVE
SESSIONS
OR
A
MIXTURE
ALTHOUGH
THE
PROCESSES
ARE
NOT
WORKING
TOGETHER
THE
OS
NEEDS
TO
BE
CONCERNED
ABOUT
COMPETITION
FOR
RESOURCES
FOR
EXAMPLE
TWO
INDEPENDENT
APPLICATIONS
MAY
BOTH
WANT
TO
ACCESS
THE
SAME
DISK
OR
FILE
OR
PRINTER
THE
OS
MUST
REGULATE
THESE
ACCESSES
PROCESSES
INDIRECTLY
AWARE
OF
EACH
OTHER
THESE
ARE
PROCESSES
THAT
ARE
NOT
NEC
ESSARILY
AWARE
OF
EACH
OTHER
BY
THEIR
RESPECTIVE
PROCESS
IDS
BUT
THAT
SHARE
ACCESS
TO
SOME
OBJECT
SUCH
AS
AN
I
O
BUFFER
SUCH
PROCESSES
EXHIBIT
COOPERATION
IN
SHARING
THE
COMMON
OBJECT
PROCESSES
DIRECTLY
AWARE
OF
EACH
OTHER
THESE
ARE
PROCESSES
THAT
ARE
ABLE
TO
COMMUNICATE
WITH
EACH
OTHER
BY
PROCESS
ID
AND
THAT
ARE
DESIGNED
TO
WORK
JOINTLY
ON
SOME
ACTIVITY
AGAIN
SUCH
PROCESSES
EXHIBIT
COOPERATION
CONDITIONS
WILL
NOT
ALWAYS
BE
AS
CLEAR
CUT
AS
SUGGESTED
IN
TABLE
RATHER
SEVERAL
PROCESSES
MAY
EXHIBIT
ASPECTS
OF
BOTH
COMPETITION
AND
COOPERATION
NEVERTHELESS
IT
IS
PRODUCTIVE
TO
EXAMINE
EACH
OF
THE
THREE
ITEMS
IN
THE
PRECEDING
LIST
SEPARATELY
AND
DETERMINE
THEIR
IMPLICATIONS
FOR
THE
OS
TABLE
PROCESS
INTERACTION
DEGREE
OF
AWARENESS
RELATIONSHIP
INFLUENCE
THAT
ONE
PROCESS
HAS
ON
THE
OTHER
POTENTIAL
CONTROL
PROBLEMS
PROCESSES
UNAWARE
OF
EACH
OTHER
COMPETITION
RESULTS
OF
ONE
PROCESS
INDEPENDENT
OF
THE
ACTION
OF
OTHERS
TIMING
OF
PROCESS
MAY
BE
AFFECTED
MUTUAL
EXCLUSION
DEADLOCK
RENEWABLE
RESOURCE
STARVATION
PROCESSES
INDIRECTLY
AWARE
OF
EACH
OTHER
E
G
SHARED
OBJECT
COOPERATION
BY
SHARING
RESULTS
OF
ONE
PROCESS
MAY
DEPEND
ON
INFOR
MATION
OBTAINED
FROM
OTHERS
TIMING
OF
PROCESS
MAY
BE
AFFECTED
MUTUAL
EXCLUSION
DEADLOCK
RENEWABLE
RESOURCE
STARVATION
DATA
COHERENCE
PROCESSES
DIRECTLY
AWARE
OF
EACH
OTHER
HAVE
COMMUNICATION
PRIMITIVES
AVAILABLE
TO
THEM
COOPERATION
BY
COMMU
NICATION
RESULTS
OF
ONE
PROCESS
MAY
DEPEND
ON
INFOR
MATION
OBTAINED
FROM
OTHERS
TIMING
OF
PROCESS
MAY
BE
AFFECTED
DEADLOCK
CONSUM
ABLE
RESOURCE
STARVATION
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
COMPETITION
AMONG
PROCESSES
FOR
RESOURCES
CONCURRENT
PROCESSES
COME
INTO
CONFLICT
WITH
EACH
OTHER
WHEN
THEY
ARE
COMPETING
FOR
THE
USE
OF
THE
SAME
RESOURCE
IN
ITS
PURE
FORM
WE
CAN
DESCRIBE
THE
SITUATION
AS
FOLLOWS
TWO
OR
MORE
PROCESSES
NEED
TO
ACCESS
A
RESOURCE
DURING
THE
COURSE
OF
THEIR
EXECUTION
EACH
PROCESS
IS
UNAWARE
OF
THE
EXISTENCE
OF
OTHER
PROCESSES
AND
EACH
IS
TO
BE
UNAFFECTED
BY
THE
EXECUTION
OF
THE
OTHER
PROCESSES
IT
FOLLOWS
FROM
THIS
THAT
EACH
PROCESS
SHOULD
LEAVE
THE
STATE
OF
ANY
RESOURCE
THAT
IT
USES
UNAFFECTED
EXAMPLES
OF
RESOURCES
INCLUDE
I
O
DEVICES
MEMORY
PROCESSOR
TIME
AND
THE
CLOCK
THERE
IS
NO
EXCHANGE
OF
INFORMATION
BETWEEN
THE
COMPETING
PROCESSES
HOWEVER
THE
EXECUTION
OF
ONE
PROCESS
MAY
AFFECT
THE
BEHAVIOR
OF
COMPETING
PROCESSES
IN
PARTICULAR
IF
TWO
PROCESSES
BOTH
WISH
ACCESS
TO
A
SINGLE
RESOURCE
THEN
ONE
PROCESS
WILL
BE
ALLOCATED
THAT
RESOURCE
BY
THE
OS
AND
THE
OTHER
WILL
HAVE
TO
WAIT
THEREFORE
THE
PROCESS
THAT
IS
DENIED
ACCESS
WILL
BE
SLOWED
DOWN
IN
AN
EXTREME
CASE
THE
BLOCKED
PROCESS
MAY
NEVER
GET
ACCESS
TO
THE
RESOURCE
AND
HENCE
WILL
NEVER
TERMI
NATE
SUCCESSFULLY
IN
THE
CASE
OF
COMPETING
PROCESSES
THREE
CONTROL
PROBLEMS
MUST
BE
FACED
FIRST
IS
THE
NEED
FOR
MUTUAL
EXCLUSION
SUPPOSE
TWO
OR
MORE
PROCESSES
REQUIRE
ACCESS
TO
A
SINGLE
NONSHARABLE
RESOURCE
SUCH
AS
A
PRINTER
DURING
THE
COURSE
OF
EXECUTION
EACH
PROCESS
WILL
BE
SENDING
COMMANDS
TO
THE
I
O
DEVICE
RECEIVING
STATUS
INFORMATION
SENDING
DATA
AND
OR
RECEIVING
DATA
WE
WILL
REFER
TO
SUCH
A
RESOURCE
AS
A
CRITICAL
RESOURCE
AND
THE
PORTION
OF
THE
PROGRAM
THAT
USES
IT
AS
A
CRITICAL
SECTION
OF
THE
PROGRAM
IT
IS
IMPORTANT
THAT
ONLY
ONE
PROGRAM
AT
A
TIME
BE
ALLOWED
IN
ITS
CRITICAL
SECTION
WE
CANNOT
SIMPLY
RELY
ON
THE
OS
TO
UNDERSTAND
AND
ENFORCE
THIS
RESTRICTION
BECAUSE
THE
DETAILED
REQUIREMENTS
MAY
NOT
BE
OBVIOUS
IN
THE
CASE
OF
THE
PRINTER
FOR
EXAMPLE
WE
WANT
ANY
INDIVIDUAL
PROCESS
TO
HAVE
CON
TROL
OF
THE
PRINTER
WHILE
IT
PRINTS
AN
ENTIRE
FILE
OTHERWISE
LINES
FROM
COMPETING
PROCESSES
WILL
BE
INTERLEAVED
THE
ENFORCEMENT
OF
MUTUAL
EXCLUSION
CREATES
TWO
ADDITIONAL
CONTROL
PROBLEMS
ONE
IS
THAT
OF
DEADLOCK
FOR
EXAMPLE
CONSIDER
TWO
PROCESSES
AND
AND
TWO
RESOURCES
AND
SUPPOSE
THAT
EACH
PROCESS
NEEDS
ACCESS
TO
BOTH
RESOURCES
TO
PERFORM
PART
OF
ITS
FUNCTION
THEN
IT
IS
POSSIBLE
TO
HAVE
THE
FOLLOWING
SITUATION
THE
OS
ASSIGNS
TO
AND
TO
EACH
PROCESS
IS
WAITING
FOR
ONE
OF
THE
TWO
RESOURCES
NEITHER
WILL
RELEASE
THE
RESOURCE
THAT
IT
ALREADY
OWNS
UNTIL
IT
HAS
ACQUIRED
THE
OTHER
RESOURCE
AND
PERFORMED
THE
FUNCTION
REQUIRING
BOTH
RESOURCES
THE
TWO
PROCESSES
ARE
DEADLOCKED
A
FINAL
CONTROL
PROBLEM
IS
STARVATION
SUPPOSE
THAT
THREE
PROCESSES
EACH
REQUIRE
PERIODIC
ACCESS
TO
RESOURCE
R
CONSIDER
THE
SITUATION
IN
WHICH
IS
IN
POSSESSION
OF
THE
RESOURCE
AND
BOTH
AND
ARE
DELAYED
WAITING
FOR
THAT
RESOURCE
WHEN
EXITS
ITS
CRITICAL
SECTION
EITHER
OR
SHOULD
BE
ALLOWED
ACCESS
TO
R
ASSUME
THAT
THE
OS
GRANTS
ACCESS
TO
AND
THAT
AGAIN
REQUIRES
ACCESS
BEFORE
COMPLETES
ITS
CRITICAL
SECTION
IF
THE
OS
GRANTS
ACCESS
TO
AFTER
HAS
FINISHED
AND
SUBSEQUENTLY
ALTERNATELY
GRANTS
ACCESS
TO
AND
THEN
MAY
INDEFINITELY
BE
DENIED
ACCESS
TO
THE
RESOURCE
EVEN
THOUGH
THERE
IS
NO
DEADLOCK
SITUATION
CONTROL
OF
COMPETITION
INEVITABLY
INVOLVES
THE
OS
BECAUSE
IT
IS
THE
OS
THAT
ALLOCATES
RESOURCES
IN
ADDITION
THE
PROCESSES
THEMSELVES
WILL
NEED
TO
BE
ABLE
TO
PRINCIPLES
OF
CONCURRENCY
PROCESS
VOID
PROCESS
VOID
PROCESS
N
VOID
PN
WHILE
TRUE
WHILE
TRUE
WHILE
TRUE
PRECEDING
CODE
ENTERCRITICAL
RA
PRECEDING
CODE
ENTERCRITICAL
RA
PRECEDING
CODE
ENTERCRITICAL
RA
CRITICAL
SECTION
CRITICAL
SECTION
CRITICAL
SECTION
EXITCRITICAL
RA
EXITCRITICAL
RA
EXITCRITICAL
RA
FOLLOWING
CODE
FOLLOWING
CODE
FOLLOWING
CODE
FIGURE
ILLUSTRATION
OF
MUTUAL
EXCLUSION
EXPRESS
THE
REQUIREMENT
FOR
MUTUAL
EXCLUSION
IN
SOME
FASHION
SUCH
AS
LOCKING
A
RESOURCE
PRIOR
TO
ITS
USE
ANY
SOLUTION
WILL
INVOLVE
SOME
SUPPORT
FROM
THE
OS
SUCH
AS
THE
PROVISION
OF
THE
LOCKING
FACILITY
FIGURE
ILLUSTRATES
THE
MUTUAL
EXCLUSION
MECHANISM
IN
ABSTRACT
TERMS
THERE
ARE
N
PROCESSES
TO
BE
EXECUTED
CONCURRENTLY
EACH
PROCESS
INCLUDES
A
CRITICAL
SECTION
THAT
OPERATES
ON
SOME
RESOURCE
RA
AND
ADDITIONAL
CODE
PRECEDING
AND
FOLLOWING
THE
CRITICAL
SECTION
THAT
DOES
NOT
INVOLVE
ACCESS
TO
RA
BECAUSE
ALL
PROCESSES
ACCESS
THE
SAME
RESOURCE
RA
IT
IS
DESIRED
THAT
ONLY
ONE
PROCESS
AT
A
TIME
BE
IN
ITS
CRITICAL
SECTION
TO
ENFORCE
MUTUAL
EXCLUSION
TWO
FUNCTIONS
ARE
PROVIDED
ENTERCRITICAL
AND
EXITCRITICAL
EACH
FUNCTION
TAKES
AS
AN
ARGUMENT
THE
NAME
OF
THE
RESOURCE
THAT
IS
THE
SUBJECT
OF
COMPETITION
ANY
PROCESS
THAT
ATTEMPTS
TO
ENTER
ITS
CRITICAL
SECTION
WHILE
ANOTHER
PROCESS
IS
IN
ITS
CRITICAL
SECTION
FOR
THE
SAME
RESOURCE
IS
MADE
TO
WAIT
IT
REMAINS
TO
EXAMINE
SPECIFIC
MECHANISMS
FOR
PROVIDING
THE
FUNCTIONS
ENTERCRITICAL
AND
EXITCRITICAL
FOR
THE
MOMENT
WE
DEFER
THIS
ISSUE
WHILE
WE
CONSIDER
THE
OTHER
CASES
OF
PROCESS
INTERACTION
COOPERATION
AMONG
PROCESSES
BY
SHARING
THE
CASE
OF
COOPERATION
BY
SHARING
COVERS
PROCESSES
THAT
INTERACT
WITH
OTHER
PROCESSES
WITHOUT
BEING
EXPLICITLY
AWARE
OF
THEM
FOR
EXAMPLE
MULTIPLE
PROCESSES
MAY
HAVE
ACCESS
TO
SHARED
VARIABLES
OR
TO
SHARED
FILES
OR
DATABASES
PROCESSES
MAY
USE
AND
UPDATE
THE
SHARED
DATA
WITHOUT
REFERENCE
TO
OTHER
PROCESSES
BUT
KNOW
THAT
OTHER
PROCESSES
MAY
HAVE
ACCESS
TO
THE
SAME
DATA
THUS
THE
PROCESSES
MUST
COOPERATE
TO
ENSURE
THAT
THE
DATA
THEY
SHARE
ARE
PROPERLY
MANAGED
THE
CONTROL
MECHANISMS
MUST
ENSURE
THE
INTEGRITY
OF
THE
SHARED
DATA
BECAUSE
DATA
ARE
HELD
ON
RESOURCES
DEVICES
MEMORY
THE
CONTROL
PROBLEMS
OF
MUTUAL
EXCLUSION
DEADLOCK
AND
STARVATION
ARE
AGAIN
PRESENT
THE
ONLY
DIFFERENCE
IS
THAT
DATA
ITEMS
MAY
BE
ACCESSED
IN
TWO
DIFFERENT
MODES
READING
AND
WRITING
AND
ONLY
WRITING
OPERATIONS
MUST
BE
MUTUALLY
EXCLUSIVE
HOWEVER
OVER
AND
ABOVE
THESE
PROBLEMS
A
NEW
REQUIREMENT
IS
INTRODUCED
THAT
OF
DATA
COHERENCE
AS
A
SIMPLE
EXAMPLE
CONSIDER
A
BOOKKEEPING
APPLICATION
IN
WHICH
VARIOUS
DATA
ITEMS
MAY
BE
UPDATED
SUPPOSE
TWO
ITEMS
OF
DATA
A
AND
B
ARE
TO
BE
MAINTAINED
IN
THE
RELATIONSHIP
A
B
THAT
IS
ANY
PROGRAM
THAT
UPDATES
ONE
VALUE
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
MUST
ALSO
UPDATE
THE
OTHER
TO
MAINTAIN
THE
RELATIONSHIP
NOW
CONSIDER
THE
FOLLOWING
TWO
PROCESSES
A
A
B
B
B
B
A
A
IF
THE
STATE
IS
INITIALLY
CONSISTENT
EACH
PROCESS
TAKEN
SEPARATELY
WILL
LEAVE
THE
SHARED
DATA
IN
A
CONSISTENT
STATE
NOW
CONSIDER
THE
FOLLOWING
CONCURRENT
EXECUTION
SEQUENCE
IN
WHICH
THE
TWO
PROCESSES
RESPECT
MUTUAL
EXCLUSION
ON
EACH
INDIVIDUAL
DATA
ITEM
A
AND
B
A
A
B
B
B
B
A
A
AT
THE
END
OF
THIS
EXECUTION
SEQUENCE
THE
CONDITION
A
B
NO
LONGER
HOLDS
FOR
EXAMPLE
IF
WE
START
WITH
A
B
AT
THE
END
OF
THIS
EXECUTION
SEQUENCE
WE
HAVE
A
AND
B
THE
PROBLEM
CAN
BE
AVOIDED
BY
DECLARING
THE
ENTIRE
SEQUENCE
IN
EACH
PROCESS
TO
BE
A
CRITICAL
SECTION
THUS
WE
SEE
THAT
THE
CONCEPT
OF
CRITICAL
SECTION
IS
IMPORTANT
IN
THE
CASE
OF
COOPERATION
BY
SHARING
THE
SAME
ABSTRACT
FUNCTIONS
OF
ENTERCRITICAL
AND
EXITCRITICAL
DISCUSSED
EARLIER
FIGURE
CAN
BE
USED
HERE
IN
THIS
CASE
THE
ARGUMENT
FOR
THE
FUNCTIONS
COULD
BE
A
VARIABLE
A
FILE
OR
ANY
OTHER
SHARED
OBJECT
FURTHERMORE
IF
CRITICAL
SECTIONS
ARE
USED
TO
PROVIDE
DATA
INTEGRITY
THEN
THERE
MAY
BE
NO
SPECIFIC
RESOURCE
OR
VARIABLE
THAT
CAN
BE
IDENTIFIED
AS
AN
ARGUMENT
IN
THAT
CASE
WE
CAN
THINK
OF
THE
ARGUMENT
AS
BEING
AN
IDENTIFIER
THAT
IS
SHARED
AMONG
CON
CURRENT
PROCESSES
TO
IDENTIFY
CRITICAL
SECTIONS
THAT
MUST
BE
MUTUALLY
EXCLUSIVE
COOPERATION
AMONG
PROCESSES
BY
COMMUNICATION
IN
THE
FIRST
TWO
CASES
THAT
WE
HAVE
DISCUSSED
EACH
PROCESS
HAS
ITS
OWN
ISOLATED
ENVIRONMENT
THAT
DOES
NOT
INCLUDE
THE
OTHER
PROCESSES
THE
INTERACTIONS
AMONG
PROCESSES
ARE
INDIRECT
IN
BOTH
CASES
THERE
IS
A
SHARING
IN
THE
CASE
OF
COMPETITION
THEY
ARE
SHARING
RESOURCES
WITHOUT
BEING
AWARE
OF
THE
OTHER
PROCESSES
IN
THE
SECOND
CASE
THEY
ARE
SHARING
VALUES
AND
ALTHOUGH
EACH
PROCESS
IS
NOT
EXPLICITLY
AWARE
OF
THE
OTHER
PROCESSES
IT
IS
AWARE
OF
THE
NEED
TO
MAINTAIN
DATA
INTEGRITY
WHEN
PROCESSES
COOPERATE
BY
COMMUNICATION
HOWEVER
THE
VARIOUS
PROCESSES
PARTICIPATE
IN
A
COMMON
EFFORT
THAT
LINKS
ALL
OF
THE
PROCESSES
THE
COMMUNICATION
PROVIDES
A
WAY
TO
SYNCHRONIZE
OR
COORDINATE
THE
VARIOUS
ACTIVITIES
TYPICALLY
COMMUNICATION
CAN
BE
CHARACTERIZED
AS
CONSISTING
OF
MESSAGES
OF
SOME
SORT
PRIMITIVES
FOR
SENDING
AND
RECEIVING
MESSAGES
MAY
BE
PROVIDED
AS
PART
OF
THE
PROGRAMMING
LANGUAGE
OR
PROVIDED
BY
THE
OS
KERNEL
BECAUSE
NOTHING
IS
SHARED
BETWEEN
PROCESSES
IN
THE
ACT
OF
PASSING
MESSAGES
MUTUAL
EXCLUSION
IS
NOT
A
CONTROL
REQUIREMENT
FOR
THIS
SORT
OF
COOPERATION
HOWEVER
MUTUAL
EXCLUSION
HARDWARE
SUPPORT
THE
PROBLEMS
OF
DEADLOCK
AND
STARVATION
ARE
STILL
PRESENT
AS
AN
EXAMPLE
OF
DEAD
LOCK
TWO
PROCESSES
MAY
BE
BLOCKED
EACH
WAITING
FOR
A
COMMUNICATION
FROM
THE
OTHER
AS
AN
EXAMPLE
OF
STARVATION
CONSIDER
THREE
PROCESSES
AND
THAT
EXHIBIT
THE
FOLLOWING
BEHAVIOR
IS
REPEATEDLY
ATTEMPTING
TO
COMMUNICATE
WITH
EITHER
OR
AND
AND
ARE
BOTH
ATTEMPTING
TO
COMMUNICATE
WITH
A
SEQUENCE
COULD
ARISE
IN
WHICH
AND
EXCHANGE
INFORMATION
REPEATEDLY
WHILE
IS
BLOCKED
WAITING
FOR
A
COMMUNICATION
FROM
THERE
IS
NO
DEADLOCK
BECAUSE
REMAINS
ACTIVE
BUT
IS
STARVED
REQUIREMENTS
FOR
MUTUAL
EXCLUSION
ANY
FACILITY
OR
CAPABILITY
THAT
IS
TO
PROVIDE
SUPPORT
FOR
MUTUAL
EXCLUSION
SHOULD
MEET
THE
FOLLOWING
REQUIREMENTS
MUTUAL
EXCLUSION
MUST
BE
ENFORCED
ONLY
ONE
PROCESS
AT
A
TIME
IS
ALLOWED
INTO
ITS
CRITICAL
SECTION
AMONG
ALL
PROCESSES
THAT
HAVE
CRITICAL
SECTIONS
FOR
THE
SAME
RESOURCE
OR
SHARED
OBJECT
A
PROCESS
THAT
HALTS
IN
ITS
NONCRITICAL
SECTION
MUST
DO
SO
WITHOUT
INTERFERING
WITH
OTHER
PROCESSES
IT
MUST
NOT
BE
POSSIBLE
FOR
A
PROCESS
REQUIRING
ACCESS
TO
A
CRITICAL
SECTION
TO
BE
DELAYED
INDEFINITELY
NO
DEADLOCK
OR
STARVATION
WHEN
NO
PROCESS
IS
IN
A
CRITICAL
SECTION
ANY
PROCESS
THAT
REQUESTS
ENTRY
TO
ITS
CRITICAL
SECTION
MUST
BE
PERMITTED
TO
ENTER
WITHOUT
DELAY
NO
ASSUMPTIONS
ARE
MADE
ABOUT
RELATIVE
PROCESS
SPEEDS
OR
NUMBER
OF
PROCESSORS
A
PROCESS
REMAINS
INSIDE
ITS
CRITICAL
SECTION
FOR
A
FINITE
TIME
ONLY
THERE
ARE
A
NUMBER
OF
WAYS
IN
WHICH
THE
REQUIREMENTS
FOR
MUTUAL
EXCLUSION
CAN
BE
SATISFIED
ONE
APPROACH
IS
TO
LEAVE
THE
RESPONSIBILITY
WITH
THE
PROCESSES
THAT
WISH
TO
EXECUTE
CONCURRENTLY
PROCESSES
WHETHER
THEY
ARE
SYSTEM
PROGRAMS
OR
APPLICATION
PROGRAMS
WOULD
BE
REQUIRED
TO
COORDINATE
WITH
ONE
ANOTHER
TO
ENFORCE
MUTUAL
EXCLUSION
WITH
NO
SUPPORT
FROM
THE
PROGRAMMING
LANGUAGE
OR
THE
OS
WE
CAN
REFER
TO
THESE
AS
SOFTWARE
APPROACHES
ALTHOUGH
THIS
APPROACH
IS
PRONE
TO
HIGH
PROCESSING
OVERHEAD
AND
BUGS
IT
IS
NEVERTHELESS
USEFUL
TO
EXAMINE
SUCH
APPROACHES
TO
GAIN
A
BETTER
UNDERSTANDING
OF
THE
COMPLEXITY
OF
CONCURRENT
PROCESSING
THIS
TOPIC
IS
COVERED
IN
APPENDIX
A
A
SECOND
APPROACH
INVOLVES
THE
USE
OF
SPECIAL
PURPOSE
MACHINE
INSTRUCTIONS
THESE
HAVE
THE
ADVANTAGE
OF
REDUCING
OVERHEAD
BUT
NEVERTHELESS
WILL
BE
SHOWN
TO
BE
UNATTRACTIVE
AS
A
GENERAL
PURPOSE
SOLUTION
THEY
ARE
COVERED
IN
SECTION
A
THIRD
APPROACH
IS
TO
PROVIDE
SOME
LEVEL
OF
SUPPORT
WITHIN
THE
OS
OR
A
PROGRAMMING
LANGUAGE
THREE
OF
THE
MOST
IMPORTANT
SUCH
APPROACHES
ARE
EXAMINED
IN
SECTIONS
THROUGH
MUTUAL
EXCLUSION
HARDWARE
SUPPORT
IN
THIS
SECTION
WE
LOOK
AT
SEVERAL
INTERESTING
HARDWARE
APPROACHES
TO
MUTUAL
EXCLUSION
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
INTERRUPT
DISABLING
IN
A
UNIPROCESSOR
SYSTEM
CONCURRENT
PROCESSES
CANNOT
HAVE
OVERLAPPED
EXECUTION
THEY
CAN
ONLY
BE
INTERLEAVED
FURTHERMORE
A
PROCESS
WILL
CONTINUE
TO
RUN
UNTIL
IT
INVOKES
AN
OS
SERVICE
OR
UNTIL
IT
IS
INTERRUPTED
THEREFORE
TO
GUARANTEE
MUTUAL
EXCLUSION
IT
IS
SUFFICIENT
TO
PREVENT
A
PROCESS
FROM
BEING
INTERRUPTED
THIS
CAPABILITY
CAN
BE
PROVIDED
IN
THE
FORM
OF
PRIMITIVES
DEFINED
BY
THE
OS
KERNEL
FOR
DISABLING
AND
ENABLING
INTERRUPTS
A
PROCESS
CAN
THEN
ENFORCE
MUTUAL
EXCLUSION
IN
THE
FOLLOWING
WAY
COMPARE
FIGURE
WHILE
TRUE
DISABLE
INTERRUPTS
CRITICAL
SECTION
ENABLE
INTERRUPTS
REMAINDER
BECAUSE
THE
CRITICAL
SECTION
CANNOT
BE
INTERRUPTED
MUTUAL
EXCLUSION
IS
GUAR
ANTEED
THE
PRICE
OF
THIS
APPROACH
HOWEVER
IS
HIGH
THE
EFFICIENCY
OF
EXECUTION
COULD
BE
NOTICEABLY
DEGRADED
BECAUSE
THE
PROCESSOR
IS
LIMITED
IN
ITS
ABILITY
TO
INTERLEAVE
PROCESSES
ANOTHER
PROBLEM
IS
THAT
THIS
APPROACH
WILL
NOT
WORK
IN
A
MULTIPROCESSOR
ARCHITECTURE
WHEN
THE
COMPUTER
INCLUDES
MORE
THAN
ONE
PROCES
SOR
IT
IS
POSSIBLE
AND
TYPICAL
FOR
MORE
THAN
ONE
PROCESS
TO
BE
EXECUTING
AT
A
TIME
IN
THIS
CASE
DISABLED
INTERRUPTS
DO
NOT
GUARANTEE
MUTUAL
EXCLUSION
SPECIAL
MACHINE
INSTRUCTIONS
IN
A
MULTIPROCESSOR
CONFIGURATION
SEVERAL
PROCESSORS
SHARE
ACCESS
TO
A
COMMON
MAIN
MEMORY
IN
THIS
CASE
THERE
IS
NOT
A
MASTER
SLAVE
RELATIONSHIP
RATHER
THE
PRO
CESSORS
BEHAVE
INDEPENDENTLY
IN
A
PEER
RELATIONSHIP
THERE
IS
NO
INTERRUPT
MECHA
NISM
BETWEEN
PROCESSORS
ON
WHICH
MUTUAL
EXCLUSION
CAN
BE
BASED
AT
THE
HARDWARE
LEVEL
AS
WAS
MENTIONED
ACCESS
TO
A
MEMORY
LOCATION
EXCLUDES
ANY
OTHER
ACCESS
TO
THAT
SAME
LOCATION
WITH
THIS
AS
A
FOUNDATION
PROC
ESSOR
DESIGNERS
HAVE
PROPOSED
SEVERAL
MACHINE
INSTRUCTIONS
THAT
CARRY
OUT
TWO
ACTIONS
ATOMICALLY
SUCH
AS
READING
AND
WRITING
OR
READING
AND
TESTING
OF
A
SINGLE
MEMORY
LOCATION
WITH
ONE
INSTRUCTION
FETCH
CYCLE
DURING
EXECUTION
OF
THE
INSTRUC
TION
ACCESS
TO
THE
MEMORY
LOCATION
IS
BLOCKED
FOR
ANY
OTHER
INSTRUCTION
REFERENCING
THAT
LOCATION
IN
THIS
SECTION
WE
LOOK
AT
TWO
OF
THE
MOST
COMMONLY
IMPLEMENTED
INSTRUC
TIONS
OTHERS
ARE
DESCRIBED
IN
AND
COMPARE
SWAP
INSTRUCTION
THE
COMPARE
SWAP
INSTRUCTION
ALSO
CALLED
A
COMPARE
AND
EXCHANGE
INSTRUCTION
CAN
BE
DEFINED
AS
FOLLOWS
TERM
ATOMIC
MEANS
THAT
THE
INSTRUCTION
IS
TREATED
AS
A
SINGLE
STEP
THAT
CANNOT
BE
INTERRUPTED
MUTUAL
EXCLUSION
HARDWARE
SUPPORT
INT
INT
WORD
INT
TESTVAL
INT
NEWVAL
INT
OLDVAL
OLDVAL
WORD
IF
OLDVAL
TESTVAL
WORD
NEWVAL
RETURN
OLDVAL
THIS
VERSION
OF
THE
INSTRUCTION
CHECKS
A
MEMORY
LOCATION
WORD
AGAINST
A
TEST
VALUE
TESTVAL
IF
THE
MEMORY
LOCATION
CURRENT
VALUE
IS
TESTVAL
IT
IS
REPLACED
WITH
NEWVAL
OTHERWISE
IT
IS
LEFT
UNCHANGED
THE
OLD
MEMORY
VALUE
IS
ALWAYS
RETURNED
THUS
THE
MEMORY
LOCATION
HAS
BEEN
UPDATED
IF
THE
RETURNED
VALUE
IS
THE
SAME
AS
THE
TEST
VALUE
THIS
ATOMIC
INSTRUCTION
THEREFORE
HAS
TWO
PARTS
A
COMPARE
IS
MADE
BETWEEN
A
MEMORY
VALUE
AND
A
TEST
VALUE
IF
THE
VALUES
ARE
THE
SAME
A
SWAP
OCCURS
THE
ENTIRE
COMPARE
SWAP
FUNCTION
IS
CARRIED
OUT
ATOMICALLY
THAT
IS
IT
IS
NOT
SUB
JECT
TO
INTERRUPTION
ANOTHER
VERSION
OF
THIS
INSTRUCTION
RETURNS
A
BOOLEAN
VALUE
TRUE
IF
THE
SWAP
OCCURRED
FALSE
OTHERWISE
SOME
VERSION
OF
THIS
INSTRUCTION
IS
AVAILABLE
ON
NEARLY
ALL
PROCESSOR
FAMILIES
SPARC
IBM
Z
SERIES
ETC
AND
MOST
OPERATING
SYSTEMS
USE
THIS
INSTRUCTION
FOR
SUPPORT
OF
CONCURRENCY
FIGURE
SHOWS
A
MUTUAL
EXCLUSION
PROTOCOL
BASED
ON
THE
USE
OF
THIS
INSTRUC
TION
A
SHARED
VARIABLE
BOLT
IS
INITIALIZED
TO
THE
ONLY
PROCESS
THAT
MAY
ENTER
ITS
CRITICAL
SECTION
IS
ONE
THAT
FINDS
BOLT
EQUAL
TO
ALL
OTHER
PROCESSES
ATTEMPTING
A
COMPARE
AND
SWAP
INSTRUCTION
B
EXCHANGE
INSTRUCTION
FIGURE
HARDWARE
SUPPORT
FOR
MUTUAL
EXCLUSION
CONSTRUCT
PARBEGIN
PN
MEANS
THE
FOLLOWING
SUSPEND
THE
EXECUTION
OF
THE
MAIN
PROGRAM
INITIATE
CONCURRENT
EXECUTION
OF
PROCEDURES
PN
WHEN
ALL
OF
PN
HAVE
TER
MINATED
RESUME
THE
MAIN
PROGRAM
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
TO
ENTER
THEIR
CRITICAL
SECTION
GO
INTO
A
BUSY
WAITING
MODE
THE
TERM
BUSY
WAITING
OR
SPIN
WAITING
REFERS
TO
A
TECHNIQUE
IN
WHICH
A
PROCESS
CAN
DO
NOTHING
UNTIL
IT
GETS
PERMISSION
TO
ENTER
ITS
CRITICAL
SECTION
BUT
CONTINUES
TO
EXECUTE
AN
INSTRUCTION
OR
SET
OF
INSTRUCTIONS
THAT
TESTS
THE
APPROPRIATE
VARIABLE
TO
GAIN
ENTRANCE
WHEN
A
PROCESS
LEAVES
ITS
CRITICAL
SECTION
IT
RESETS
BOLT
TO
AT
THIS
POINT
ONE
AND
ONLY
ONE
OF
THE
WAIT
ING
PROCESSES
IS
GRANTED
ACCESS
TO
ITS
CRITICAL
SECTION
THE
CHOICE
OF
PROCESS
DEPENDS
ON
WHICH
PROCESS
HAPPENS
TO
EXECUTE
THE
COMPARE
SWAP
INSTRUCTION
NEXT
EXCHANGE
INSTRUCTION
THE
EXCHANGE
INSTRUCTION
CAN
BE
DEFINED
AS
FOLLOWS
VOID
EXCHANGE
INT
REGISTER
INT
MEMORY
INT
TEMP
TEMP
MEMORY
MEMORY
REGISTER
REGISTER
TEMP
THE
INSTRUCTION
EXCHANGES
THE
CONTENTS
OF
A
REGISTER
WITH
THAT
OF
A
MEMORY
LOCATION
BOTH
THE
INTEL
IA
ARCHITECTURE
PENTIUM
AND
THE
IA
ARCHITECTURE
ITANIUM
CONTAIN
AN
XCHG
INSTRUCTION
FIGURE
SHOWS
A
MUTUAL
EXCLUSION
PROTOCOL
BASED
ON
THE
USE
OF
AN
EXCHANGE
INSTRUCTION
A
SHARED
VARIABLE
BOLT
IS
INITIALIZED
TO
EACH
PROCESS
USES
A
LOCAL
VARI
ABLE
KEY
THAT
IS
INITIALIZED
TO
THE
ONLY
PROCESS
THAT
MAY
ENTER
ITS
CRITICAL
SECTION
IS
ONE
THAT
FINDS
BOLT
EQUAL
TO
IT
EXCLUDES
ALL
OTHER
PROCESSES
FROM
THE
CRITICAL
SEC
TION
BY
SETTING
BOLT
TO
WHEN
A
PROCESS
LEAVES
ITS
CRITICAL
SECTION
IT
RESETS
BOLT
TO
ALLOWING
ANOTHER
PROCESS
TO
GAIN
ACCESS
TO
ITS
CRITICAL
SECTION
NOTE
THAT
THE
FOLLOWING
EXPRESSION
ALWAYS
HOLDS
BECAUSE
OF
THE
WAY
IN
WHICH
THE
VARIABLES
ARE
INITIALIZED
AND
BECAUSE
OF
THE
NATURE
OF
THE
EXCHANGE
ALGORITHM
BOLT
A
KEYI
N
I
IF
BOLT
THEN
NO
PROCESS
IS
IN
ITS
CRITICAL
SECTION
IF
BOLT
THEN
EXACTLY
ONE
PRO
CESS
IS
IN
ITS
CRITICAL
SECTION
NAMELY
THE
PROCESS
WHOSE
KEY
VALUE
EQUALS
PROPERTIES
OF
THE
MACHINE
INSTRUCTION
APPROACH
THE
USE
OF
A
SPECIAL
MACHINE
INSTRUCTION
TO
ENFORCE
MUTUAL
EXCLUSION
HAS
A
NUMBER
OF
ADVANTAGES
IT
IS
APPLICABLE
TO
ANY
NUMBER
OF
PROCESSES
ON
EITHER
A
SINGLE
PROCESSOR
OR
MUL
TIPLE
PROCESSORS
SHARING
MAIN
MEMORY
IT
IS
SIMPLE
AND
THEREFORE
EASY
TO
VERIFY
IT
CAN
BE
USED
TO
SUPPORT
MULTIPLE
CRITICAL
SECTIONS
EACH
CRITICAL
SECTION
CAN
BE
DEFINED
BY
ITS
OWN
VARIABLE
THERE
ARE
SOME
SERIOUS
DISADVANTAGES
BUSY
WAITING
IS
EMPLOYED
THUS
WHILE
A
PROCESS
IS
WAITING
FOR
ACCESS
TO
A
CRITI
CAL
SECTION
IT
CONTINUES
TO
CONSUME
PROCESSOR
TIME
SEMAPHORES
STARVATION
IS
POSSIBLE
WHEN
A
PROCESS
LEAVES
A
CRITICAL
SECTION
AND
MORE
THAN
ONE
PROCESS
IS
WAITING
THE
SELECTION
OF
A
WAITING
PROCESS
IS
ARBITRARY
THUS
SOME
PROCESS
COULD
INDEFINITELY
BE
DENIED
ACCESS
DEADLOCK
IS
POSSIBLE
CONSIDER
THE
FOLLOWING
SCENARIO
ON
A
SINGLE
PROCESSOR
SYSTEM
PROCESS
EXECUTES
THE
SPECIAL
INSTRUCTION
E
G
COMPARE
SWAP
EXCHANGE
AND
ENTERS
ITS
CRITICAL
SECTION
IS
THEN
INTERRUPTED
TO
GIVE
THE
PROCESSOR
TO
WHICH
HAS
HIGHER
PRIORITY
IF
NOW
ATTEMPTS
TO
USE
THE
SAME
RESOURCE
AS
IT
WILL
BE
DENIED
ACCESS
BECAUSE
OF
THE
MUTUAL
EXCLUSION
MECHA
NISM
THUS
IT
WILL
GO
INTO
A
BUSY
WAITING
LOOP
HOWEVER
WILL
NEVER
BE
DIS
PATCHED
BECAUSE
IT
IS
OF
LOWER
PRIORITY
THAN
ANOTHER
READY
PROCESS
BECAUSE
OF
THE
DRAWBACKS
OF
BOTH
THE
SOFTWARE
AND
HARDWARE
SOLUTIONS
WE
NEED
TO
LOOK
FOR
OTHER
MECHANISMS
SEMAPHORES
WE
NOW
TURN
TO
OS
AND
PROGRAMMING
LANGUAGE
MECHANISMS
THAT
ARE
USED
TO
PRO
VIDE
CONCURRENCY
TABLE
SUMMARIZES
MECHANISMS
IN
COMMON
USE
WE
BEGIN
IN
THIS
SECTION
WITH
SEMAPHORES
THE
NEXT
TWO
SECTIONS
DISCUSS
MONITORS
AND
MESSAGE
PASSING
THE
OTHER
MECHANISMS
IN
TABLE
ARE
DISCUSSED
WHEN
TREATING
SPECIFIC
OS
EXAMPLES
IN
CHAPTERS
AND
TABLE
COMMON
CONCURRENCY
MECHANISMS
SEMAPHORE
AN
INTEGER
VALUE
USED
FOR
SIGNALING
AMONG
PROCESSES
ONLY
THREE
OPERATIONS
MAY
BE
PERFORMED
ON
A
SEMAPHORE
ALL
OF
WHICH
ARE
ATOMIC
INITIALIZE
DECREMENT
AND
INCRE
MENT
THE
DECREMENT
OPERATION
MAY
RESULT
IN
THE
BLOCKING
OF
A
PROCESS
AND
THE
INCRE
MENT
OPERATION
MAY
RESULT
IN
THE
UNBLOCKING
OF
A
PROCESS
ALSO
KNOWN
AS
A
COUNTING
SEMAPHORE
OR
A
GENERAL
SEMAPHORE
BINARY
SEMAPHORE
A
SEMAPHORE
THAT
TAKES
ON
ONLY
THE
VALUES
AND
MUTEX
SIMILAR
TO
A
BINARY
SEMAPHORE
A
KEY
DIFFERENCE
BETWEEN
THE
TWO
IS
THAT
THE
PROCESS
THAT
LOCKS
THE
MUTEX
SETS
THE
VALUE
TO
ZERO
MUST
BE
THE
ONE
TO
UNLOCK
IT
SETS
THE
VALUE
TO
CONDITION
VARIABLE
A
DATA
TYPE
THAT
IS
USED
TO
BLOCK
A
PROCESS
OR
THREAD
UNTIL
A
PARTICULAR
CONDITION
IS
TRUE
MONITOR
A
PROGRAMMING
LANGUAGE
CONSTRUCT
THAT
ENCAPSULATES
VARIABLES
ACCESS
PROCEDURES
AND
INITIALIZATION
CODE
WITHIN
AN
ABSTRACT
DATA
TYPE
THE
MONITOR
VARIABLE
MAY
ONLY
BE
ACCESSED
VIA
ITS
ACCESS
PROCEDURES
AND
ONLY
ONE
PROCESS
MAY
BE
ACTIVELY
ACCESSING
THE
MONITOR
AT
ANY
ONE
TIME
THE
ACCESS
PROCEDURES
ARE
CRITICAL
SECTIONS
A
MONITOR
MAY
HAVE
A
QUEUE
OF
PROCESSES
THAT
ARE
WAITING
TO
ACCESS
IT
EVENT
FLAGS
A
MEMORY
WORD
USED
AS
A
SYNCHRONIZATION
MECHANISM
APPLICATION
CODE
MAY
ASSOCI
ATE
A
DIFFERENT
EVENT
WITH
EACH
BIT
IN
A
FLAG
A
THREAD
CAN
WAIT
FOR
EITHER
A
SINGLE
EVENT
OR
A
COMBINATION
OF
EVENTS
BY
CHECKING
ONE
OR
MULTIPLE
BITS
IN
THE
CORRESPONDING
FLAG
THE
THREAD
IS
BLOCKED
UNTIL
ALL
OF
THE
REQUIRED
BITS
ARE
SET
AND
OR
UNTIL
AT
LEAST
ONE
OF
THE
BITS
IS
SET
OR
MAILBOXES
MESSAGES
A
MEANS
FOR
TWO
PROCESSES
TO
EXCHANGE
INFORMATION
AND
THAT
MAY
BE
USED
FOR
SYNCHRONIZATION
SPINLOCKS
MUTUAL
EXCLUSION
MECHANISM
IN
WHICH
A
PROCESS
EXECUTES
IN
AN
INFINITE
LOOP
WAITING
FOR
THE
VALUE
OF
A
LOCK
VARIABLE
TO
INDICATE
AVAILABILITY
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
THE
FIRST
MAJOR
ADVANCE
IN
DEALING
WITH
THE
PROBLEMS
OF
CONCURRENT
PROC
ESSES
CAME
IN
WITH
DIJKSTRA
TREATISE
DIJKSTRA
WAS
CONCERNED
WITH
THE
DESIGN
OF
AN
OS
AS
A
COLLECTION
OF
COOPERATING
SEQUENTIAL
PROCESSES
AND
WITH
THE
DEVELOPMENT
OF
EFFICIENT
AND
RELIABLE
MECHANISMS
FOR
SUPPORTING
COOPERATION
THESE
MECHANISMS
CAN
JUST
AS
READILY
BE
USED
BY
USER
PROCESSES
IF
THE
PROCESSOR
AND
OS
MAKE
THE
MECHANISMS
AVAILABLE
THE
FUNDAMENTAL
PRINCIPLE
IS
THIS
TWO
OR
MORE
PROCESSES
CAN
COOPERATE
BY
MEANS
OF
SIMPLE
SIGNALS
SUCH
THAT
A
PROCESS
CAN
BE
FORCED
TO
STOP
AT
A
SPECIFIED
PLACE
UNTIL
IT
HAS
RECEIVED
A
SPECIFIC
SIGNAL
ANY
COMPLEX
COORDINATION
REQUIREMENT
CAN
BE
SATISFIED
BY
THE
APPROPRIATE
STRUCTURE
OF
SIGNALS
FOR
SIGNALING
SPECIAL
VARIABLES
CALLED
SEMAPHORES
ARE
USED
TO
TRANSMIT
A
SIGNAL
VIA
SEMAPHORE
A
PROCESS
EXE
CUTES
THE
PRIMITIVE
SEMSIGNAL
TO
RECEIVE
A
SIGNAL
VIA
SEMAPHORE
A
PROCESS
EXECUTES
THE
PRIMITIVE
SEMWAIT
IF
THE
CORRESPONDING
SIGNAL
HAS
NOT
YET
BEEN
TRANSMITTED
THE
PROCESS
IS
SUSPENDED
UNTIL
THE
TRANSMISSION
TAKES
PLACE
TO
ACHIEVE
THE
DESIRED
EFFECT
WE
CAN
VIEW
THE
SEMAPHORE
AS
A
VARIABLE
THAT
HAS
AN
INTEGER
VALUE
UPON
WHICH
ONLY
THREE
OPERATIONS
ARE
DEFINED
A
SEMAPHORE
MAY
BE
INITIALIZED
TO
A
NONNEGATIVE
INTEGER
VALUE
THE
SEMWAIT
OPERATION
DECREMENTS
THE
SEMAPHORE
VALUE
IF
THE
VALUE
BECOMES
NEGATIVE
THEN
THE
PROCESS
EXECUTING
THE
SEMWAIT
IS
BLOCKED
OTHERWISE
THE
PROCESS
CONTINUES
EXECUTION
THE
SEMSIGNAL
OPERATION
INCREMENTS
THE
SEMAPHORE
VALUE
IF
THE
RESULTING
VALUE
IS
LESS
THAN
OR
EQUAL
TO
ZERO
THEN
A
PROCESS
BLOCKED
BY
A
SEMWAIT
OPER
ATION
IF
ANY
IS
UNBLOCKED
OTHER
THAN
THESE
THREE
OPERATIONS
THERE
IS
NO
WAY
TO
INSPECT
OR
MANIPULATE
SEMAPHORES
WE
EXPLAIN
THESE
OPERATIONS
AS
FOLLOWS
TO
BEGIN
THE
SEMAPHORE
HAS
A
ZERO
OR
POSITIVE
VALUE
IF
THE
VALUE
IS
POSITIVE
THAT
VALUE
EQUALS
THE
NUMBER
OF
PROCESSES
THAT
CAN
ISSUE
A
WAIT
AND
IMMEDIATELY
CONTINUE
TO
EXECUTE
IF
THE
VALUE
IS
ZERO
EITHER
BY
INITIALIZATION
OR
BECAUSE
A
NUMBER
OF
PROCESSES
EQUAL
TO
THE
INITIAL
SEMAPHORE
VALUE
HAVE
ISSUED
A
WAIT
THE
NEXT
PROCESS
TO
ISSUE
A
WAIT
IS
BLOCKED
AND
THE
SEMAPHORE
VALUE
GOES
NEGATIVE
EACH
SUBSEQUENT
WAIT
DRIVES
THE
SEMAPHORE
VALUE
FURTHER
INTO
MINUS
TERRITORY
THE
NEGATIVE
VALUE
EQUALS
THE
NUMBER
OF
PROCESSES
WAITING
TO
BE
UNBLOCKED
EACH
SIGNAL
UNBLOCKS
ONE
OF
THE
WAITING
PROCESSES
WHEN
THE
SEMAPHORE
VALUE
IS
NEGATIVE
POINTS
OUT
THREE
INTERESTING
CONSEQUENCES
OF
THE
SEMAPHORE
DEFINITION
IN
GENERAL
THERE
IS
NO
WAY
TO
KNOW
BEFORE
A
PROCESS
DECREMENTS
A
SEMAPHORE
WHETHER
IT
WILL
BLOCK
OR
NOT
DIJKSTRA
ORIGINAL
PAPER
AND
IN
MUCH
OF
THE
LITERATURE
THE
LETTER
P
IS
USED
FOR
SEMWAIT
AND
THE
LETTER
V
FOR
SEMSIGNAL
THESE
ARE
THE
INITIALS
OF
THE
DUTCH
WORDS
FOR
TEST
PROBEREN
AND
INCREMENT
VERHOGEN
IN
SOME
OF
THE
LITERATURE
THE
TERMS
WAIT
AND
SIGNAL
ARE
USED
THIS
BOOK
USES
SEMWAIT
AND
SEMSIG
NAL
FOR
CLARITY
AND
TO
AVOID
CONFUSION
WITH
SIMILAR
WAIT
AND
SIGNAL
OPERATIONS
IN
MONITORS
DISCUSSED
SUBSEQUENTLY
SEMAPHORES
FIGURE
A
DEFINITION
OF
SEMAPHORE
PRIMITIVES
AFTER
A
PROCESS
INCREMENTS
A
SEMAPHORE
AND
ANOTHER
PROCESS
GETS
WOKEN
UP
BOTH
PROCESSES
CONTINUE
RUNNING
CONCURRENTLY
THERE
IS
NO
WAY
TO
KNOW
WHICH
PROCESS
IF
EITHER
WILL
CONTINUE
IMMEDIATELY
ON
A
UNIPROCESSOR
SYSTEM
WHEN
YOU
SIGNAL
A
SEMAPHORE
YOU
DON
T
NECESSARILY
KNOW
WHETHER
ANOTHER
PROCESS
IS
WAITING
SO
THE
NUMBER
OF
UNBLOCKED
PROCESSES
MAY
BE
ZERO
OR
ONE
FIGURE
SUGGESTS
A
MORE
FORMAL
DEFINITION
OF
THE
PRIMITIVES
FOR
SEMA
PHORES
THE
SEMWAIT
AND
SEMSIGNAL
PRIMITIVES
ARE
ASSUMED
TO
BE
ATOMIC
A
MORE
RESTRICTED
VERSION
KNOWN
AS
THE
BINARY
SEMAPHORE
IS
DEFINED
IN
FIGURE
A
BINARY
SEMAPHORE
MAY
ONLY
TAKE
ON
THE
VALUES
AND
AND
CAN
BE
DEFINED
BY
THE
FOLLOWING
THREE
OPERATIONS
A
BINARY
SEMAPHORE
MAY
BE
INITIALIZED
TO
OR
THE
SEMWAITB
OPERATION
CHECKS
THE
SEMAPHORE
VALUE
IF
THE
VALUE
IS
ZERO
THEN
THE
PROCESS
EXECUTING
THE
SEMWAITB
IS
BLOCKED
IF
THE
VALUE
IS
ONE
THEN
THE
VALUE
IS
CHANGED
TO
ZERO
AND
THE
PROCESS
CONTINUES
EXECUTION
THE
SEMSIGNALB
OPERATION
CHECKS
TO
SEE
IF
ANY
PROCESSES
ARE
BLOCKED
ON
THIS
SEMAPHORE
SEMAPHORE
VALUE
EQUALS
IF
SO
THEN
A
PROCESS
BLOCKED
BY
A
SEMWAITB
OPERATION
IS
UNBLOCKED
IF
NO
PROCESSES
ARE
BLOCKED
THEN
THE
VALUE
OF
THE
SEMAPHORE
IS
SET
TO
ONE
IN
PRINCIPLE
IT
SHOULD
BE
EASIER
TO
IMPLEMENT
THE
BINARY
SEMAPHORE
AND
IT
CAN
BE
SHOWN
THAT
IT
HAS
THE
SAME
EXPRESSIVE
POWER
AS
THE
GENERAL
SEMAPHORE
SEE
PROBLEM
TO
CONTRAST
THE
TWO
TYPES
OF
SEMAPHORES
THE
NONBINARY
SEMAPHORE
IS
OFTEN
REFERRED
TO
AS
EITHER
A
COUNTING
SEMAPHORE
OR
A
GENERAL
SEMAPHORE
A
CONCEPT
RELATED
TO
THE
BINARY
SEMAPHORE
IS
THE
MUTEX
A
KEY
DIFFERENCE
BETWEEN
THE
TWO
IS
THAT
THE
PROCESS
THAT
LOCKS
THE
MUTEX
SETS
THE
VALUE
TO
ZERO
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
FIGURE
A
DEFINITION
OF
BINARY
SEMAPHORE
PRIMITIVES
MUST
BE
THE
ONE
TO
UNLOCK
IT
SETS
THE
VALUE
TO
IN
CONTRAST
IT
IS
POSSIBLE
FOR
ONE
PROCESS
TO
LOCK
A
BINARY
SEMAPHORE
AND
FOR
ANOTHER
TO
UNLOCK
IT
FOR
BOTH
COUNTING
SEMAPHORES
AND
BINARY
SEMAPHORES
A
QUEUE
IS
USED
TO
HOLD
PROCESSES
WAITING
ON
THE
SEMAPHORE
THE
QUESTION
ARISES
OF
THE
ORDER
IN
WHICH
PROCESSES
ARE
REMOVED
FROM
SUCH
A
QUEUE
THE
FAIREST
REMOVAL
POLICY
IS
FIRST
IN
FIRST
OUT
FIFO
THE
PROCESS
THAT
HAS
BEEN
BLOCKED
THE
LONGEST
IS
RELEASED
FROM
THE
QUEUE
FIRST
A
SEMAPHORE
WHOSE
DEFINITION
INCLUDES
THIS
POLICY
IS
CALLED
A
STRONG
SEMAPHORE
A
SEMAPHORE
THAT
DOES
NOT
SPECIFY
THE
ORDER
IN
WHICH
PROC
ESSES
ARE
REMOVED
FROM
THE
QUEUE
IS
A
WEAK
SEMAPHORE
FIGURE
BASED
ON
ONE
IN
IS
AN
EXAMPLE
OF
THE
OPERATION
OF
A
STRONG
SEMAPHORE
HERE
PROCESSES
A
B
AND
C
DEPEND
ON
A
RESULT
FROM
PROCESS
D
INITIALLY
A
IS
RUN
NING
B
C
AND
D
ARE
READY
AND
THE
SEMAPHORE
COUNT
IS
INDICATING
THAT
ONE
OF
D
RESULTS
IS
AVAILABLE
WHEN
A
ISSUES
A
SEMWAIT
INSTRUCTION
ON
SEMAPHORE
THE
SEMAPHORE
DECREMENTS
TO
AND
A
CAN
CONTINUE
TO
EXECUTE
SUBSEQUENTLY
IT
REJOINS
THE
READY
QUEUE
THEN
B
RUNS
EVENTUALLY
ISSUES
A
SEMWAIT
INSTRUC
TION
AND
IS
BLOCKED
ALLOWING
D
TO
RUN
WHEN
D
COMPLETES
A
NEW
RESULT
IT
ISSUES
A
SEMSIGNAL
INSTRUCTION
WHICH
ALLOWS
B
TO
MOVE
TO
THE
READY
QUEUE
D
REJOINS
THE
READY
QUEUE
AND
C
BEGINS
TO
RUN
BUT
IS
BLOCKED
WHEN
IT
ISSUES
A
SEMWAIT
INSTRUCTION
SIMILARLY
A
AND
B
RUN
AND
ARE
BLOCKED
ON
THE
SEMAPHORE
ALLOWING
D
TO
RESUME
EXECUTION
WHEN
D
HAS
A
RESULT
IT
ISSUES
A
SEMSIGNAL
WHICH
TRANSFERS
C
TO
THE
READY
QUEUE
LATER
CYCLES
OF
D
WILL
RELEASE
A
AND
B
FROM
THE
BLOCKED
STATE
SOME
OF
THE
LITERATURE
AND
IN
SOME
TEXTBOOKS
NO
DISTINCTION
IS
MADE
BETWEEN
A
MUTEX
AND
A
BINARY
SEMAPHORE
HOWEVER
IN
PRACTICE
A
NUMBER
OF
OPERATING
SYSTEMS
SUCH
AS
LINUX
WINDOWS
AND
SOLARIS
OFFER
A
MUTEX
FACILITY
WHICH
CONFORMS
TO
THE
DEFINITION
IN
THIS
BOOK
PROCESSOR
SEMAPHORES
BLOCKED
QUEUE
BLOCKED
QUEUE
BLOCKED
QUEUE
BLOCKED
QUEUE
BLOCKED
QUEUE
BLOCKED
QUEUE
BLOCKED
QUEUE
SEMAPHORE
PROCESSOR
SEMAPHORE
PROCESSOR
SEMAPHORE
PROCESSOR
SEMAPHORE
PROCESSOR
SEMAPHORE
PROCESSOR
SEMAPHORE
PROCESSOR
SEMAPHORE
READY
QUEUE
READY
QUEUE
READY
QUEUE
READY
QUEUE
READY
QUEUE
READY
QUEUE
READY
QUEUE
FIGURE
EXAMPLE
OF
SEMAPHORE
MECHANISM
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
FIGURE
MUTUAL
EXCLUSION
USING
SEMAPHORES
FOR
THE
MUTUAL
EXCLUSION
ALGORITHM
DISCUSSED
IN
THE
NEXT
SUBSECTION
AND
ILLUS
TRATED
IN
FIGURE
STRONG
SEMAPHORES
GUARANTEE
FREEDOM
FROM
STARVATION
WHILE
WEAK
SEMAPHORES
DO
NOT
WE
WILL
ASSUME
STRONG
SEMAPHORES
BECAUSE
THEY
ARE
MORE
CONVENIENT
AND
BECAUSE
THIS
IS
THE
FORM
OF
SEMAPHORE
TYPICALLY
PROVIDED
BY
OPERAT
ING
SYSTEMS
MUTUAL
EXCLUSION
FIGURE
SHOWS
A
STRAIGHTFORWARD
SOLUTION
TO
THE
MUTUAL
EXCLUSION
PROBLEM
USING
A
SEMAPHORE
COMPARE
FIGURE
CONSIDER
N
PROCESSES
IDENTIFIED
IN
THE
ARRAY
P
I
ALL
OF
WHICH
NEED
ACCESS
TO
THE
SAME
RESOURCE
EACH
PROCESS
HAS
A
CRITICAL
SEC
TION
USED
TO
ACCESS
THE
RESOURCE
IN
EACH
PROCESS
A
SEMWAIT
IS
EXECUTED
JUST
BEFORE
ITS
CRITICAL
SECTION
IF
THE
VALUE
OF
BECOMES
NEGATIVE
THE
PROCESS
IS
BLOCKED
IF
THE
VALUE
IS
THEN
IT
IS
DECREMENTED
TO
AND
THE
PROCESS
IMMEDIATELY
ENTERS
ITS
CRITICAL
SECTION
BECAUSE
IS
NO
LONGER
POSITIVE
NO
OTHER
PROCESS
WILL
BE
ABLE
TO
ENTER
ITS
CRITICAL
SECTION
THE
SEMAPHORE
IS
INITIALIZED
TO
THUS
THE
FIRST
PROCESS
THAT
EXECUTES
A
SEMWAIT
WILL
BE
ABLE
TO
ENTER
THE
CRITICAL
SECTION
IMMEDIATELY
SETTING
THE
VALUE
OF
TO
ANY
OTHER
PROCESS
ATTEMPTING
TO
ENTER
THE
CRITICAL
SECTION
WILL
FIND
IT
BUSY
AND
WILL
BE
BLOCKED
SETTING
THE
VALUE
OF
TO
ANY
NUMBER
OF
PROCESSES
MAY
ATTEMPT
ENTRY
EACH
SUCH
UNSUCCESSFUL
ATTEMPT
RESULTS
IN
A
FURTHER
DECREMENT
OF
THE
VALUE
OF
WHEN
THE
PROCESS
THAT
INITIALLY
ENTERED
ITS
CRITICAL
SECTION
DEPARTS
IS
INCREMENTED
AND
ONE
OF
THE
BLOCKED
PROCESSES
IF
ANY
IS
REMOVED
FROM
THE
QUEUE
OF
BLOCKED
PROCESSES
ASSOCIATED
WITH
THE
SEMAPHORE
AND
PUT
IN
A
READY
STATE
WHEN
IT
IS
NEXT
SCHEDULED
BY
THE
OS
IT
MAY
ENTER
THE
CRITICAL
SECTION
FIGURE
BASED
ON
ONE
IN
SHOWS
A
POSSIBLE
SEQUENCE
FOR
THREE
PROCESSES
USING
THE
MUTUAL
EXCLUSION
DISCIPLINE
OF
FIGURE
IN
THIS
EXAMPLE
THREE
PROCESSES
A
B
C
ACCESS
A
SHARED
RESOURCE
PROTECTED
BY
THE
SEMAPHORE
LOCK
PROCESS
A
EXECUTES
SEMWAIT
LOCK
BECAUSE
THE
SEMAPHORE
HAS
A
VALUE
OF
AT
THE
TIME
OF
THE
SEMWAIT
OPERATION
A
CAN
IMMEDIATELY
ENTER
ITS
CRITICAL
SECTION
AND
THE
SEMAPHORE
TAKES
ON
THE
VALUE
WHILE
A
IS
IN
ITS
CRITICAL
SECTION
BOTH
B
AND
C
SEMAPHORES
QUEUE
FOR
SEMAPHORE
LOCK
VALUE
OF
SEMAPHORE
LOCK
A
B
C
CRITICAL
REGION
NORMAL
EXECUTION
BLOCKED
ON
SEMAPHORE
LOCK
NOTE
THAT
NORMAL
EXECUTION
CAN
PROCEED
IN
PARALLEL
BUT
THAT
CRITICAL
REGIONS
ARE
SERIALIZED
FIGURE
PROCESSES
ACCESSING
SHARED
DATA
PROTECTED
BY
A
SEMAPHORE
PERFORM
A
SEMWAIT
OPERATION
AND
ARE
BLOCKED
PENDING
THE
AVAILABILITY
OF
THE
SEMA
PHORE
WHEN
A
EXITS
ITS
CRITICAL
SECTION
AND
PERFORMS
SEMSIGNAL
LOCK
B
WHICH
WAS
THE
FIRST
PROCESS
IN
THE
QUEUE
CAN
NOW
ENTER
ITS
CRITICAL
SECTION
THE
PROGRAM
OF
FIGURE
CAN
EQUALLY
WELL
HANDLE
A
REQUIREMENT
THAT
MORE
THAN
ONE
PROCESS
BE
ALLOWED
IN
ITS
CRITICAL
SECTION
AT
A
TIME
THIS
REQUIREMENT
IS
MET
SIMPLY
BY
INITIALIZING
THE
SEMAPHORE
TO
THE
SPECIFIED
VALUE
THUS
AT
ANY
TIME
THE
VALUE
OF
COUNT
CAN
BE
INTERPRETED
AS
FOLLOWS
COUNT
COUNT
IS
THE
NUMBER
OF
PROCESSES
THAT
CAN
EXECUTE
SEMWAIT
WITHOUT
SUSPENSION
IF
NO
SEMSIGNAL
IS
EXECUTED
IN
THE
MEANTIME
SUCH
SITUATIONS
WILL
ALLOW
SEMAPHORES
TO
SUPPORT
SYNCHRONIZATION
AS
WELL
AS
MUTUAL
EXCLUSION
COUNT
THE
MAGNITUDE
OF
COUNT
IS
THE
NUMBER
OF
PROCESSES
SUSPENDED
IN
QUEUE
THE
PRODUCER
CONSUMER
PROBLEM
WE
NOW
EXAMINE
ONE
OF
THE
MOST
COMMON
PROBLEMS
FACED
IN
CONCURRENT
PROCESS
ING
THE
PRODUCER
CONSUMER
PROBLEM
THE
GENERAL
STATEMENT
IS
THIS
THERE
ARE
ONE
OR
MORE
PRODUCERS
GENERATING
SOME
TYPE
OF
DATA
RECORDS
CHARACTERS
AND
PLACING
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
THESE
IN
A
BUFFER
THERE
IS
A
SINGLE
CONSUMER
THAT
IS
TAKING
ITEMS
OUT
OF
THE
BUFFER
ONE
AT
A
TIME
THE
SYSTEM
IS
TO
BE
CONSTRAINED
TO
PREVENT
THE
OVERLAP
OF
BUFFER
OPER
ATIONS
THAT
IS
ONLY
ONE
AGENT
PRODUCER
OR
CONSUMER
MAY
ACCESS
THE
BUFFER
AT
ANY
ONE
TIME
THE
PROBLEM
IS
TO
MAKE
SURE
THAT
THE
PRODUCER
WON
T
TRY
TO
ADD
DATA
INTO
THE
BUFFER
IF
IT
FULL
AND
THAT
THE
CONSUMER
WON
T
TRY
TO
REMOVE
DATA
FROM
AN
EMPTY
BUFFER
WE
WILL
LOOK
AT
A
NUMBER
OF
SOLUTIONS
TO
THIS
PROBLEM
TO
ILLUSTRATE
BOTH
THE
POWER
AND
THE
PITFALLS
OF
SEMAPHORES
TO
BEGIN
LET
US
ASSUME
THAT
THE
BUFFER
IS
INFINITE
AND
CONSISTS
OF
A
LINEAR
ARRAY
OF
ELEMENTS
IN
ABSTRACT
TERMS
WE
CAN
DEFINE
THE
PRODUCER
AND
CONSUMER
FUNCTIONS
AS
FOLLOWS
PRODUCER
CONSUMER
WHILE
TRUE
WHILE
TRUE
PRODUCE
ITEM
V
WHILE
IN
OUT
B
IN
V
DO
NOTHING
IN
W
B
OUT
OUT
CONSUME
ITEM
W
FIGURE
ILLUSTRATES
THE
STRUCTURE
OF
BUFFER
B
THE
PRODUCER
CAN
GENERATE
ITEMS
AND
STORE
THEM
IN
THE
BUFFER
AT
ITS
OWN
PACE
EACH
TIME
AN
INDEX
IN
INTO
THE
BUFFER
IS
INCREMENTED
THE
CONSUMER
PROCEEDS
IN
A
SIMILAR
FASHION
BUT
MUST
MAKE
SURE
THAT
IT
DOES
NOT
ATTEMPT
TO
READ
FROM
AN
EMPTY
BUFFER
HENCE
THE
CONSUMER
MAKES
SURE
THAT
THE
PRODUCER
HAS
ADVANCED
BEYOND
IT
IN
OUT
BEFORE
PROCEEDING
LET
US
TRY
TO
IMPLEMENT
THIS
SYSTEM
USING
BINARY
SEMAPHORES
FIGURE
IS
A
FIRST
ATTEMPT
RATHER
THAN
DEAL
WITH
THE
INDICES
IN
AND
OUT
WE
CAN
SIMPLY
KEEP
TRACK
OF
THE
NUMBER
OF
ITEMS
IN
THE
BUFFER
USING
THE
INTEGER
VARIABLE
N
IN
OUT
THE
SEMAPHORE
IS
USED
TO
ENFORCE
MUTUAL
EXCLUSION
THE
SEMAPHORE
DELAY
IS
USED
TO
FORCE
THE
CONSUMER
TO
SEMWAIT
IF
THE
BUFFER
IS
EMPTY
THIS
SOLUTION
SEEMS
RATHER
STRAIGHTFORWARD
THE
PRODUCER
IS
FREE
TO
ADD
TO
THE
BUFFER
AT
ANY
TIME
IT
PERFORMS
SEMWAITB
BEFORE
APPENDING
AND
SEMSIGNALB
AFTERWARD
TO
PREVENT
THE
CONSUMER
OR
ANY
OTHER
PRODUCER
FROM
B
B
B
B
B
OUT
IN
NOTE
SHADED
AREA
INDICATES
PORTION
OF
BUFFER
THAT
IS
OCCUPIED
FIGURE
INFINITE
BUFFER
FOR
THE
PRODUCER
CONSUMER
PROBLEM
SEMAPHORES
FIGURE
AN
INCORRECT
SOLUTION
TO
THE
INFINITE
BUFFER
PRODUCER
CONSUMER
PROBLEM
USING
BINARY
SEMAPHORES
ACCESSING
THE
BUFFER
DURING
THE
APPEND
OPERATION
ALSO
WHILE
IN
THE
CRITICAL
SECTION
THE
PRODUCER
INCREMENTS
THE
VALUE
OF
N
IF
N
THEN
THE
BUFFER
WAS
EMPTY
JUST
PRIOR
TO
THIS
APPEND
SO
THE
PRODUCER
PERFORMS
SEMSIGNALB
DELAY
TO
ALERT
THE
CON
SUMER
OF
THIS
FACT
THE
CONSUMER
BEGINS
BY
WAITING
FOR
THE
FIRST
ITEM
TO
BE
PRODUCED
USING
SEMWAITB
DELAY
IT
THEN
TAKES
AN
ITEM
AND
DECREMENTS
N
IN
ITS
CRITICAL
SECTION
IF
THE
PRODUCER
IS
ABLE
TO
STAY
AHEAD
OF
THE
CONSUMER
A
COMMON
SITUATION
THEN
THE
CONSUMER
WILL
RARELY
BLOCK
ON
THE
SEMAPHORE
DELAY
BECAUSE
N
WILL
USUALLY
BE
POSITIVE
HENCE
BOTH
PRODUCER
AND
CONSUMER
RUN
SMOOTHLY
THERE
IS
HOWEVER
A
FLAW
IN
THIS
PROGRAM
WHEN
THE
CONSUMER
HAS
EXHAUSTED
THE
BUFFER
IT
NEEDS
TO
RESET
THE
DELAY
SEMAPHORE
SO
THAT
IT
WILL
BE
FORCED
TO
WAIT
UNTIL
THE
PRODUCER
HAS
PLACED
MORE
ITEMS
IN
THE
BUFFER
THIS
IS
THE
PURPOSE
OF
THE
STATE
MENT
IF
N
SEMWAITB
DELAY
CONSIDER
THE
SCENARIO
OUTLINED
IN
TABLE
IN
LINE
THE
CONSUMER
FAILS
TO
EXECUTE
THE
SEMWAITB
OPERATION
THE
CONSUMER
DID
INDEED
EXHAUST
THE
BUFFER
AND
SET
N
TO
LINE
BUT
THE
PRODUCER
HAS
INCREMENTED
N
BEFORE
THE
CONSUMER
CAN
TEST
IT
IN
LINE
THE
RESULT
IS
A
SEMSIGNALB
NOT
MATCHED
BY
A
PRIOR
SEMWAITB
THE
VALUE
OF
FOR
N
IN
LINE
MEANS
THAT
THE
CONSUMER
HAS
CONSUMED
AN
ITEM
FROM
THE
BUFFER
THAT
DOES
NOT
EXIST
IT
WOULD
NOT
DO
SIMPLY
TO
MOVE
THE
CONDITIONAL
STATEMENT
INSIDE
THE
CRITICAL
SECTION
OF
THE
CONSUMER
BECAUSE
THIS
COULD
LEAD
TO
DEADLOCK
E
G
AFTER
LINE
OF
TABLE
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
TABLE
POSSIBLE
SCENARIO
FOR
THE
PROGRAM
OF
FIGURE
PRODUCER
CONSUMER
N
DELAY
SEMWAITB
N
IF
N
SEMSIGNALB
DELAY
SEMSIGNALB
SEMWAITB
DELAY
SEMWAITB
N
SEMSIGNALB
SEMWAITB
N
IF
N
SEMSIGNALB
DELAY
SEMSIGNALB
IF
N
SEMWAITB
DELAY
SEMWAITB
N
SEMSIGNALB
IF
N
SEMWAITB
DELAY
SEMWAITB
N
SEMSIGNALB
NOTE
WHITE
AREAS
REPRESENT
THE
CRITICAL
SECTION
CONTROLLED
BY
SEMAPHORE
A
FIX
FOR
THE
PROBLEM
IS
TO
INTRODUCE
AN
AUXILIARY
VARIABLE
THAT
CAN
BE
SET
IN
THE
CONSUMER
CRITICAL
SECTION
FOR
USE
LATER
ON
THIS
IS
SHOWN
IN
FIGURE
A
CAREFUL
TRACE
OF
THE
LOGIC
SHOULD
CONVINCE
YOU
THAT
DEADLOCK
CAN
NO
LONGER
OCCUR
A
SOMEWHAT
CLEANER
SOLUTION
CAN
BE
OBTAINED
IF
GENERAL
SEMAPHORES
ALSO
CALLED
COUNTING
SEMAPHORES
ARE
USED
AS
SHOWN
IN
FIGURE
THE
VARIABLE
N
IS
NOW
A
SEMAPHORE
ITS
VALUE
STILL
IS
EQUAL
TO
THE
NUMBER
OF
ITEMS
IN
THE
BUFFER
SUPPOSE
NOW
THAT
IN
TRANSCRIBING
THIS
PROGRAM
A
MISTAKE
IS
MADE
AND
THE
OPERA
TIONS
SEMSIGNAL
AND
SEMSIGNAL
N
ARE
INTERCHANGED
THIS
WOULD
REQUIRE
THAT
THE
SEMSIGNAL
N
OPERATION
BE
PERFORMED
IN
THE
PRODUCER
CRITICAL
SEC
TION
WITHOUT
INTERRUPTION
BY
THE
CONSUMER
OR
ANOTHER
PRODUCER
WOULD
THIS
AFFECT
SEMAPHORES
FIGURE
A
CORRECT
SOLUTION
TO
THE
INFINITE
BUFFER
PRODUCER
CONSUMER
PROBLEM
USING
BINARY
SEMAPHORES
THE
PROGRAM
NO
BECAUSE
THE
CONSUMER
MUST
WAIT
ON
BOTH
SEMAPHORES
BEFORE
PROCEEDING
IN
ANY
CASE
NOW
SUPPOSE
THAT
THE
SEMWAIT
N
AND
SEMWAIT
OPERATIONS
ARE
ACCI
DENTALLY
REVERSED
THIS
PRODUCES
A
SERIOUS
INDEED
A
FATAL
FLAW
IF
THE
CONSUMER
EVER
ENTERS
ITS
CRITICAL
SECTION
WHEN
THE
BUFFER
IS
EMPTY
N
COUNT
THEN
NO
PRODUCER
CAN
EVER
APPEND
TO
THE
BUFFER
AND
THE
SYSTEM
IS
DEADLOCKED
THIS
IS
A
GOOD
EXAMPLE
OF
THE
SUBTLETY
OF
SEMAPHORES
AND
THE
DIFFICULTY
OF
PRODUCING
CORRECT
DESIGNS
FINALLY
LET
US
ADD
A
NEW
AND
REALISTIC
RESTRICTION
TO
THE
PRODUCER
CONSUMER
PROBLEM
NAMELY
THAT
THE
BUFFER
IS
FINITE
THE
BUFFER
IS
TREATED
AS
A
CIRCULAR
STORAGE
FIGURE
AND
POINTER
VALUES
MUST
BE
EXPRESSED
MODULO
THE
SIZE
OF
THE
BUFFER
THE
FOLLOWING
RELATIONSHIPS
HOLD
BLOCK
ON
UNBLOCK
ON
PRODUCER
INSERT
IN
FULL
BUFFER
CONSUMER
ITEM
INSERTED
CONSUMER
REMOVE
FROM
EMPTY
BUFFER
PRODUCER
ITEM
REMOVED
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
FIGURE
A
SOLUTION
TO
THE
INFINITE
BUFFER
PRODUCER
CONSUMER
PROBLEM
USING
SEMAPHORES
THE
PRODUCER
AND
CONSUMER
FUNCTIONS
CAN
BE
EXPRESSED
AS
FOLLOWS
VARIABLE
IN
AND
OUT
ARE
INITIALIZED
TO
AND
N
IS
THE
SIZE
OF
THE
BUFFER
PRODUCER
CONSUMER
WHILE
TRUE
WHILE
TRUE
PRODUCE
ITEM
V
WHILE
IN
OUT
WHILE
IN
N
OUT
DO
NOTHING
DO
NOTHING
W
B
OUT
B
IN
V
OUT
OUT
N
IN
IN
N
CONSUME
ITEM
W
FIGURE
SHOWS
A
SOLUTION
USING
GENERAL
SEMAPHORES
THE
SEMAPHORE
E
HAS
BEEN
ADDED
TO
KEEP
TRACK
OF
THE
NUMBER
OF
EMPTY
SPACES
ANOTHER
INSTRUCTIVE
EXAMPLE
IN
THE
USE
OF
SEMAPHORES
IS
THE
BARBERSHOP
PROB
LEM
DESCRIBED
IN
APPENDIX
A
APPENDIX
A
ALSO
INCLUDES
ADDITIONAL
EXAMPLES
OF
THE
PROBLEM
OF
RACE
CONDITIONS
WHEN
USING
SEMAPHORES
IMPLEMENTATION
OF
SEMAPHORES
AS
WAS
MENTIONED
EARLIER
IT
IS
IMPERATIVE
THAT
THE
SEMWAIT
AND
SEMSIGNAL
OPER
ATIONS
BE
IMPLEMENTED
AS
ATOMIC
PRIMITIVES
ONE
OBVIOUS
WAY
IS
TO
IMPLEMENT
THEM
SEMAPHORES
OUT
IN
A
B
B
B
B
B
B
N
IN
OUT
B
FIGURE
FINITE
CIRCULAR
BUFFER
FOR
THE
PRODUCER
CONSUMER
PROBLEM
IN
HARDWARE
OR
FIRMWARE
FAILING
THIS
A
VARIETY
OF
SCHEMES
HAVE
BEEN
SUGGESTED
THE
ESSENCE
OF
THE
PROBLEM
IS
ONE
OF
MUTUAL
EXCLUSION
ONLY
ONE
PROCESS
AT
A
TIME
MAY
MANIPULATE
A
SEMAPHORE
WITH
EITHER
A
SEMWAIT
OR
SEMSIGNAL
OPERATION
THUS
ANY
OF
THE
SOFTWARE
SCHEMES
SUCH
AS
DEKKER
ALGORITHM
OR
PETERSON
ALGORITHM
APPENDIX
A
COULD
BE
USED
THIS
WOULD
ENTAIL
A
SUBSTANTIAL
PROCESSING
OVERHEAD
FIGURE
A
SOLUTION
TO
THE
BOUNDED
BUFFER
PRODUCER
CONSUMER
PROBLEM
USING
SEMAPHORES
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
SEMWAIT
WHILE
FLAG
DO
NOTHING
COUNT
IF
COUNT
PLACE
THIS
PROCESS
IN
QUEUE
BLOCK
THIS
PROCESS
MUST
ALSO
SET
FLAG
TO
FLAG
SEMSIGNAL
WHILE
FLAG
DO
NOTHING
COUNT
IF
COUNT
REMOVE
A
PROCESS
P
FROM
QUEUE
PLACE
PROCESS
P
ON
READY
LIST
FLAG
SEMWAIT
INHIBIT
INTERRUPTS
COUNT
IF
COUNT
PLACE
THIS
PROCESS
IN
QUEUE
BLOCK
THIS
PROCESS
AND
ALLOW
INTER
RUPTS
ELSE
ALLOW
INTERRUPTS
SEMSIGNAL
INHIBIT
INTERRUPTS
COUNT
IF
COUNT
REMOVE
A
PROCESS
P
FROM
QUEUE
PLACE
PROCESS
P
ON
READY
LIST
ALLOW
INTERRUPTS
A
COMPARE
AND
SWAP
INSTRUCTION
B
INTERRUPTS
FIGURE
TWO
POSSIBLE
IMPLEMENTATIONS
OF
SEMAPHORES
ANOTHER
ALTERNATIVE
IS
TO
USE
ONE
OF
THE
HARDWARE
SUPPORTED
SCHEMES
FOR
MUTUAL
EXCLUSION
FOR
EXAMPLE
FIGURE
SHOWS
THE
USE
OF
A
COMPARE
SWAP
INSTRUC
TION
IN
THIS
IMPLEMENTATION
THE
SEMAPHORE
IS
AGAIN
A
STRUCTURE
AS
IN
FIGURE
BUT
NOW
INCLUDES
A
NEW
INTEGER
COMPONENT
FLAG
ADMITTEDLY
THIS
INVOLVES
A
FORM
OF
BUSY
WAITING
HOWEVER
THE
SEMWAIT
AND
SEMSIGNAL
OPERATIONS
ARE
RELATIVELY
SHORT
SO
THE
AMOUNT
OF
BUSY
WAITING
INVOLVED
SHOULD
BE
MINOR
FOR
A
SINGLE
PROCESSOR
SYSTEM
IT
IS
POSSIBLE
TO
INHIBIT
INTERRUPTS
FOR
THE
DURATION
OF
A
SEMWAIT
OR
SEMSIGNAL
OPERATION
AS
SUGGESTED
IN
FIGURE
ONCE
AGAIN
THE
RELATIVELY
SHORT
DURATION
OF
THESE
OPERATIONS
MEANS
THAT
THIS
APPROACH
IS
REASONABLE
MONITORS
SEMAPHORES
PROVIDE
A
PRIMITIVE
YET
POWERFUL
AND
FLEXIBLE
TOOL
FOR
ENFORCING
MUTUAL
EXCLUSION
AND
FOR
COORDINATING
PROCESSES
HOWEVER
AS
FIGURE
SUGGESTS
IT
MAY
BE
DIFFICULT
TO
PRODUCE
A
CORRECT
PROGRAM
USING
SEMAPHORES
THE
DIFFICULTY
IS
THAT
SEM
WAIT
AND
SEMSIGNAL
OPERATIONS
MAY
BE
SCATTERED
THROUGHOUT
A
PROGRAM
AND
IT
IS
NOT
EASY
TO
SEE
THE
OVERALL
EFFECT
OF
THESE
OPERATIONS
ON
THE
SEMAPHORES
THEY
AFFECT
THE
MONITOR
IS
A
PROGRAMMING
LANGUAGE
CONSTRUCT
THAT
PROVIDES
EQUIVALENT
FUNCTIONALITY
TO
THAT
OF
SEMAPHORES
AND
THAT
IS
EASIER
TO
CONTROL
THE
CONCEPT
WAS
FIRST
FORMALLY
DEFINED
IN
THE
MONITOR
CONSTRUCT
HAS
BEEN
IMPLEMENTED
IN
A
NUMBER
OF
PROGRAMMING
LANGUAGES
INCLUDING
CONCURRENT
PASCAL
PASCAL
PLUS
MODULA
MODULA
AND
JAVA
IT
HAS
ALSO
BEEN
IMPLEMENTED
AS
A
PROGRAM
LIBRARY
THIS
ALLOWS
PROGRAMMERS
TO
PUT
A
MONITOR
LOCK
ON
ANY
OBJECT
IN
PARTICULAR
FOR
MONITORS
SOMETHING
LIKE
A
LINKED
LIST
YOU
MAY
WANT
TO
LOCK
ALL
LINKED
LISTS
WITH
ONE
LOCK
OR
HAVE
ONE
LOCK
FOR
EACH
LIST
OR
HAVE
ONE
LOCK
FOR
EACH
ELEMENT
OF
EACH
LIST
WE
BEGIN
WITH
A
LOOK
AT
HOARE
VERSION
AND
THEN
EXAMINE
A
REFINEMENT
MONITOR
WITH
SIGNAL
A
MONITOR
IS
A
SOFTWARE
MODULE
CONSISTING
OF
ONE
OR
MORE
PROCEDURES
AN
INITIAL
IZATION
SEQUENCE
AND
LOCAL
DATA
THE
CHIEF
CHARACTERISTICS
OF
A
MONITOR
ARE
THE
FOLLOWING
THE
LOCAL
DATA
VARIABLES
ARE
ACCESSIBLE
ONLY
BY
THE
MONITOR
PROCEDURES
AND
NOT
BY
ANY
EXTERNAL
PROCEDURE
A
PROCESS
ENTERS
THE
MONITOR
BY
INVOKING
ONE
OF
ITS
PROCEDURES
ONLY
ONE
PROCESS
MAY
BE
EXECUTING
IN
THE
MONITOR
AT
A
TIME
ANY
OTHER
PRO
CESSES
THAT
HAVE
INVOKED
THE
MONITOR
ARE
BLOCKED
WAITING
FOR
THE
MONITOR
TO
BECOME
AVAILABLE
THE
FIRST
TWO
CHARACTERISTICS
ARE
REMINISCENT
OF
THOSE
FOR
OBJECTS
IN
OBJECT
ORIENTED
SOFTWARE
INDEED
AN
OBJECT
ORIENTED
OS
OR
PROGRAMMING
LANGUAGE
CAN
READILY
IMPLEMENT
A
MONITOR
AS
AN
OBJECT
WITH
SPECIAL
CHARACTERISTICS
BY
ENFORCING
THE
DISCIPLINE
OF
ONE
PROCESS
AT
A
TIME
THE
MONITOR
IS
ABLE
TO
PRO
VIDE
A
MUTUAL
EXCLUSION
FACILITY
THE
DATA
VARIABLES
IN
THE
MONITOR
CAN
BE
ACCESSED
BY
ONLY
ONE
PROCESS
AT
A
TIME
THUS
A
SHARED
DATA
STRUCTURE
CAN
BE
PROTECTED
BY
PLACING
IT
IN
A
MONITOR
IF
THE
DATA
IN
A
MONITOR
REPRESENT
SOME
RESOURCE
THEN
THE
MONITOR
PROVIDES
A
MUTUAL
EXCLUSION
FACILITY
FOR
ACCESSING
THE
RESOURCE
TO
BE
USEFUL
FOR
CONCURRENT
PROCESSING
THE
MONITOR
MUST
INCLUDE
SYNCHRONI
ZATION
TOOLS
FOR
EXAMPLE
SUPPOSE
A
PROCESS
INVOKES
THE
MONITOR
AND
WHILE
IN
THE
MONITOR
MUST
BE
BLOCKED
UNTIL
SOME
CONDITION
IS
SATISFIED
A
FACILITY
IS
NEEDED
BY
WHICH
THE
PROCESS
IS
NOT
ONLY
BLOCKED
BUT
RELEASES
THE
MONITOR
SO
THAT
SOME
OTHER
PROCESS
MAY
ENTER
IT
LATER
WHEN
THE
CONDITION
IS
SATISFIED
AND
THE
MONITOR
IS
AGAIN
AVAILABLE
THE
PROCESS
NEEDS
TO
BE
RESUMED
AND
ALLOWED
TO
REENTER
THE
MONITOR
AT
THE
POINT
OF
ITS
SUSPENSION
A
MONITOR
SUPPORTS
SYNCHRONIZATION
BY
THE
USE
OF
CONDITION
VARIABLES
THAT
ARE
CONTAINED
WITHIN
THE
MONITOR
AND
ACCESSIBLE
ONLY
WITHIN
THE
MONITOR
CONDITION
VAR
IABLES
ARE
A
SPECIAL
DATA
TYPE
IN
MONITORS
WHICH
ARE
OPERATED
ON
BY
TWO
FUNCTIONS
CWAIT
C
SUSPEND
EXECUTION
OF
THE
CALLING
PROCESS
ON
CONDITION
C
THE
MON
ITOR
IS
NOW
AVAILABLE
FOR
USE
BY
ANOTHER
PROCESS
CSIGNAL
C
RESUME
EXECUTION
OF
SOME
PROCESS
BLOCKED
AFTER
A
CWAIT
ON
THE
SAME
CONDITION
IF
THERE
ARE
SEVERAL
SUCH
PROCESSES
CHOOSE
ONE
OF
THEM
IF
THERE
IS
NO
SUCH
PROCESS
DO
NOTHING
NOTE
THAT
MONITOR
WAIT
AND
SIGNAL
OPERATIONS
ARE
DIFFERENT
FROM
THOSE
FOR
THE
SEMAPHORE
IF
A
PROCESS
IN
A
MONITOR
SIGNALS
AND
NO
TASK
IS
WAITING
ON
THE
CONDITION
VARIABLE
THE
SIGNAL
IS
LOST
FIGURE
ILLUSTRATES
THE
STRUCTURE
OF
A
MONITOR
ALTHOUGH
A
PROCESS
CAN
ENTER
THE
MONITOR
BY
INVOKING
ANY
OF
ITS
PROCEDURES
WE
CAN
THINK
OF
THE
MONITOR
AS
HAV
ING
A
SINGLE
ENTRY
POINT
THAT
IS
GUARDED
SO
THAT
ONLY
ONE
PROCESS
MAY
BE
IN
THE
MONI
TOR
AT
A
TIME
OTHER
PROCESSES
THAT
ATTEMPT
TO
ENTER
THE
MONITOR
JOIN
A
QUEUE
OF
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
FIGURE
STRUCTURE
OF
A
MONITOR
PROCESSES
BLOCKED
WAITING
FOR
MONITOR
AVAILABILITY
ONCE
A
PROCESS
IS
IN
THE
MONITOR
IT
MAY
TEMPORARILY
BLOCK
ITSELF
ON
CONDITION
X
BY
ISSUING
CWAIT
X
IT
IS
THEN
PLACED
IN
A
QUEUE
OF
PROCESSES
WAITING
TO
REENTER
THE
MONITOR
WHEN
THE
CONDITION
CHANGES
AND
RESUME
EXECUTION
AT
THE
POINT
IN
ITS
PROGRAM
FOLLOWING
THE
CWAIT
X
CALL
IF
A
PROCESS
THAT
IS
EXECUTING
IN
THE
MONITOR
DETECTS
A
CHANGE
IN
THE
CONDITION
VARIABLE
X
IT
ISSUES
CSIGNAL
X
WHICH
ALERTS
THE
CORRESPONDING
CONDITION
QUEUE
THAT
THE
CONDITION
HAS
CHANGED
AS
AN
EXAMPLE
OF
THE
USE
OF
A
MONITOR
LET
US
RETURN
TO
THE
BOUNDED
BUFFER
PRODUCER
CONSUMER
PROBLEM
FIGURE
SHOWS
A
SOLUTION
USING
A
MONITOR
THE
MONITOR
MODULE
BOUNDEDBUFFER
CONTROLS
THE
BUFFER
USED
TO
STORE
AND
RETRIEVE
CHARACTERS
THE
MONITOR
INCLUDES
TWO
CONDITION
VARIABLES
DECLARED
WITH
THE
CON
STRUCT
COND
NOTFULL
IS
TRUE
WHEN
THERE
IS
ROOM
TO
ADD
AT
LEAST
ONE
CHARACTER
TO
THE
BUFFER
AND
NOTEMPTY
IS
TRUE
WHEN
THERE
IS
AT
LEAST
ONE
CHARACTER
IN
THE
BUFFER
MONITORS
FIGURE
A
SOLUTION
TO
THE
BOUNDED
BUFFER
PRODUCER
CONSUMER
PROBLEM
USING
A
MONITOR
A
PRODUCER
CAN
ADD
CHARACTERS
TO
THE
BUFFER
ONLY
BY
MEANS
OF
THE
PROCEDURE
APPEND
INSIDE
THE
MONITOR
THE
PRODUCER
DOES
NOT
HAVE
DIRECT
ACCESS
TO
BUFFER
THE
PROCEDURE
FIRST
CHECKS
THE
CONDITION
NOTFULL
TO
DETERMINE
IF
THERE
IS
SPACE
AVAILABLE
IN
THE
BUFFER
IF
NOT
THE
PROCESS
EXECUTING
THE
MONITOR
IS
BLOCKED
ON
THAT
CONDITION
SOME
OTHER
PROCESS
PRODUCER
OR
CONSUMER
MAY
NOW
ENTER
THE
MONITOR
LATER
WHEN
THE
BUFFER
IS
NO
LONGER
FULL
THE
BLOCKED
PROCESS
MAY
BE
REMOVED
FROM
THE
QUEUE
REACTIVATED
AND
RESUME
PROCESSING
AFTER
PLACING
A
CHARACTER
IN
THE
BUFFER
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
THE
PROCESS
SIGNALS
THE
NOTEMPTY
CONDITION
A
SIMILAR
DESCRIPTION
CAN
BE
MADE
OF
THE
CONSUMER
FUNCTION
THIS
EXAMPLE
POINTS
OUT
THE
DIVISION
OF
RESPONSIBILITY
WITH
MONITORS
COMPARED
TO
SEMAPHORES
IN
THE
CASE
OF
MONITORS
THE
MONITOR
CONSTRUCT
ITSELF
ENFORCES
MUTUAL
EXCLUSION
IT
IS
NOT
POSSIBLE
FOR
BOTH
A
PRODUCER
AND
A
CONSUMER
SIMULTANEOUSLY
TO
ACCESS
THE
BUFFER
HOWEVER
THE
PROGRAMMER
MUST
PLACE
THE
APPROPRIATE
CWAIT
AND
CSIGNAL
PRIMITIVES
INSIDE
THE
MONITOR
TO
PREVENT
PROCESSES
FROM
DEPOSITING
ITEMS
IN
A
FULL
BUFFER
OR
REMOVING
THEM
FROM
AN
EMPTY
ONE
IN
THE
CASE
OF
SEMAPHORES
BOTH
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
ARE
THE
RESPONSIBILITY
OF
THE
PROGRAMMER
NOTE
THAT
IN
FIGURE
A
PROCESS
EXITS
THE
MONITOR
IMMEDIATELY
AFTER
EXECUTING
THE
CSIGNAL
FUNCTION
IF
THE
CSIGNAL
DOES
NOT
OCCUR
AT
THE
END
OF
THE
PROCEDURE
THEN
IN
HOARE
PROPOSAL
THE
PROCESS
ISSUING
THE
SIGNAL
IS
BLOCKED
TO
MAKE
THE
MONI
TOR
AVAILABLE
AND
PLACED
IN
A
QUEUE
UNTIL
THE
MONITOR
IS
FREE
ONE
POSSIBILITY
AT
THIS
POINT
WOULD
BE
TO
PLACE
THE
BLOCKED
PROCESS
IN
THE
ENTRANCE
QUEUE
SO
THAT
IT
WOULD
HAVE
TO
COMPETE
FOR
ACCESS
WITH
OTHER
PROCESSES
THAT
HAD
NOT
YET
ENTERED
THE
MONITOR
HOWEVER
BECAUSE
A
PROCESS
BLOCKED
ON
A
CSIGNAL
FUNCTION
HAS
ALREADY
PARTIALLY
PERFORMED
ITS
TASK
IN
THE
MONITOR
IT
MAKES
SENSE
TO
GIVE
THIS
PROCESS
PRECEDENCE
OVER
NEWLY
ENTERING
PROCESSES
BY
SETTING
UP
A
SEPARATE
URGENT
QUEUE
FIGURE
ONE
LANGUAGE
THAT
USES
MONITORS
CONCURRENT
PASCAL
REQUIRES
THAT
CSIGNAL
ONLY
APPEAR
AS
THE
LAST
OPERATION
EXECUTED
BY
A
MONITOR
PROCEDURE
IF
THERE
ARE
NO
PROCESSES
WAITING
ON
CONDITION
X
THEN
THE
EXECUTION
OF
CSIGNAL
X
HAS
NO
EFFECT
AS
WITH
SEMAPHORES
IT
IS
POSSIBLE
TO
MAKE
MISTAKES
IN
THE
SYNCHRONIZA
TION
FUNCTION
OF
MONITORS
FOR
EXAMPLE
IF
EITHER
OF
THE
CSIGNAL
FUNCTIONS
IN
THE
BOUNDEDBUFFER
MONITOR
ARE
OMITTED
THEN
PROCESSES
ENTERING
THE
CORRESPONDING
CONDITION
QUEUE
ARE
PERMANENTLY
HUNG
UP
THE
ADVANTAGE
THAT
MONITORS
HAVE
OVER
SEMAPHORES
IS
THAT
ALL
OF
THE
SYNCHRONIZATION
FUNCTIONS
ARE
CONFINED
TO
THE
MONITOR
THEREFORE
IT
IS
EASIER
TO
VERIFY
THAT
THE
SYNCHRONIZATION
HAS
BEEN
DONE
CORRECTLY
AND
TO
DETECT
BUGS
FURTHERMORE
ONCE
A
MONITOR
IS
CORRECTLY
PROGRAMMED
ACCESS
TO
THE
PROTECTED
RESOURCE
IS
CORRECT
FOR
ACCESS
FROM
ALL
PROCESSES
IN
CONTRAST
WITH
SEMA
PHORES
RESOURCE
ACCESS
IS
CORRECT
ONLY
IF
ALL
OF
THE
PROCESSES
THAT
ACCESS
THE
RESOURCE
ARE
PROGRAMMED
CORRECTLY
ALTERNATE
MODEL
OF
MONITORS
WITH
NOTIFY
AND
BROADCAST
HOARE
DEFINITION
OF
MONITORS
REQUIRES
THAT
IF
THERE
IS
AT
LEAST
ONE
PRO
CESS
IN
A
CONDITION
QUEUE
A
PROCESS
FROM
THAT
QUEUE
RUNS
IMMEDIATELY
WHEN
ANOTHER
PROCESS
ISSUES
A
CSIGNAL
FOR
THAT
CONDITION
THUS
THE
PROCESS
ISSUING
THE
CSIGNAL
MUST
EITHER
IMMEDIATELY
EXIT
THE
MONITOR
OR
BE
BLOCKED
ON
THE
MONITOR
THERE
ARE
TWO
DRAWBACKS
TO
THIS
APPROACH
IF
THE
PROCESS
ISSUING
THE
CSIGNAL
HAS
NOT
FINISHED
WITH
THE
MONITOR
THEN
TWO
ADDITIONAL
PROCESS
SWITCHES
ARE
REQUIRED
ONE
TO
BLOCK
THIS
PROCESS
AND
ANOTHER
TO
RESUME
IT
WHEN
THE
MONITOR
BECOMES
AVAILABLE
PROCESS
SCHEDULING
ASSOCIATED
WITH
A
SIGNAL
MUST
BE
PERFECTLY
RELIABLE
WHEN
A
CSIGNAL
IS
ISSUED
A
PROCESS
FROM
THE
CORRESPONDING
CONDITION
QUEUE
MUST
BE
ACTIVATED
IMMEDIATELY
AND
THE
SCHEDULER
MUST
ENSURE
THAT
NO
OTHER
PROCESS
MONITORS
ENTERS
THE
MONITOR
BEFORE
ACTIVATION
OTHERWISE
THE
CONDITION
UNDER
WHICH
THE
PROCESS
WAS
ACTIVATED
COULD
CHANGE
FOR
EXAMPLE
IN
FIGURE
WHEN
A
CSIGNAL
NOTEMPTY
IS
ISSUED
A
PROCESS
FROM
THE
NOTEMPTY
QUEUE
MUST
BE
ACTIVATED
BEFORE
A
NEW
CONSUMER
ENTERS
THE
MONITOR
ANOTHER
EXAMPLE
A
PRODUCER
PROCESS
MAY
APPEND
A
CHARACTER
TO
AN
EMPTY
BUFFER
AND
THEN
FAIL
BEFORE
SIGNALING
ANY
PROCESSES
IN
THE
NOTEMPTY
QUEUE
WOULD
BE
PERMANENTLY
HUNG
UP
LAMPSON
AND
REDELL
DEVELOPED
A
DIFFERENT
DEFINITION
OF
MONITORS
FOR
THE
LAN
GUAGE
MESA
THEIR
APPROACH
OVERCOMES
THE
PROBLEMS
JUST
LISTED
AND
SUPPORTS
SEVERAL
USEFUL
EXTENSIONS
THE
MESA
MONITOR
STRUCTURE
IS
ALSO
USED
IN
THE
MODULA
SYSTEMS
PROGRAMMING
LANGUAGE
IN
MESA
THE
CSIGNAL
PRIM
ITIVE
IS
REPLACED
BY
CNOTIFY
WITH
THE
FOLLOWING
INTERPRETATION
WHEN
A
PROCESS
EXECUTING
IN
A
MONITOR
EXECUTES
CNOTIFY
X
IT
CAUSES
THE
X
CONDITION
QUEUE
TO
BE
NOTIFIED
BUT
THE
SIGNALING
PROCESS
CONTINUES
TO
EXECUTE
THE
RESULT
OF
THE
NOTIFICA
TION
IS
THAT
THE
PROCESS
AT
THE
HEAD
OF
THE
CONDITION
QUEUE
WILL
BE
RESUMED
AT
SOME
CONVENIENT
FUTURE
TIME
WHEN
THE
MONITOR
IS
AVAILABLE
HOWEVER
BECAUSE
THERE
IS
NO
GUARANTEE
THAT
SOME
OTHER
PROCESS
WILL
NOT
ENTER
THE
MONITOR
BEFORE
THE
WAITING
PROCESS
THE
WAITING
PROCESS
MUST
RECHECK
THE
CONDITION
FOR
EXAMPLE
THE
PROCE
DURES
IN
THE
BOUNDEDBUFFER
MONITOR
WOULD
NOW
HAVE
THE
CODE
OF
FIGURE
THE
IF
STATEMENTS
ARE
REPLACED
BY
WHILE
LOOPS
THUS
THIS
ARRANGEMENT
RESULTS
IN
AT
LEAST
ONE
EXTRA
EVALUATION
OF
THE
CONDITION
VARIABLE
IN
RETURN
HOWEVER
THERE
ARE
NO
EXTRA
PROCESS
SWITCHES
AND
NO
CONSTRAINTS
ON
WHEN
THE
WAITING
PROCESS
MUST
RUN
AFTER
A
CNOTIFY
ONE
USEFUL
REFINEMENT
THAT
CAN
BE
ASSOCIATED
WITH
THE
CNOTIFY
PRIMITIVE
IS
A
WATCHDOG
TIMER
ASSOCIATED
WITH
EACH
CONDITION
PRIMITIVE
A
PROCESS
THAT
HAS
BEEN
WAITING
FOR
THE
MAXIMUM
TIMEOUT
INTERVAL
WILL
BE
PLACED
IN
A
READY
STATE
REGARD
LESS
OF
WHETHER
THE
CONDITION
HAS
BEEN
NOTIFIED
WHEN
ACTIVATED
THE
PROCESS
CHECKS
THE
CONDITION
AND
CONTINUES
IF
THE
CONDITION
IS
SATISFIED
THE
TIMEOUT
PREVENTS
THE
INDEFINITE
STARVATION
OF
A
PROCESS
IN
THE
EVENT
THAT
SOME
OTHER
PROCESS
FAILS
BEFORE
SIGNALING
A
CONDITION
FIGURE
BOUNDED
BUFFER
MONITOR
CODE
FOR
MESA
MONITOR
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
WITH
THE
RULE
THAT
A
PROCESS
IS
NOTIFIED
RATHER
THAN
FORCIBLY
REACTIVATED
IT
IS
POSSIBLE
TO
ADD
A
CBROADCAST
PRIMITIVE
TO
THE
REPERTOIRE
THE
BROADCAST
CAUSES
ALL
PROCESSES
WAITING
ON
A
CONDITION
TO
BE
PLACED
IN
A
READY
STATE
THIS
IS
CONVENIENT
IN
SITUATIONS
WHERE
A
PROCESS
DOES
NOT
KNOW
HOW
MANY
OTHER
PROCESSES
SHOULD
BE
REACTIVATED
FOR
EXAMPLE
IN
THE
PRODUCER
CONSUMER
PROGRAM
SUPPOSE
THAT
BOTH
THE
APPEND
AND
THE
TAKE
FUNCTIONS
CAN
APPLY
TO
VARIABLE
LENGTH
BLOCKS
OF
CHARAC
TERS
IN
THAT
CASE
IF
A
PRODUCER
ADDS
A
BLOCK
OF
CHARACTERS
TO
THE
BUFFER
IT
NEED
NOT
KNOW
HOW
MANY
CHARACTERS
EACH
WAITING
CONSUMER
IS
PREPARED
TO
CONSUME
IT
SIM
PLY
ISSUES
A
CBROADCAST
AND
ALL
WAITING
PROCESSES
ARE
ALERTED
TO
TRY
AGAIN
IN
ADDITION
A
BROADCAST
CAN
BE
USED
WHEN
A
PROCESS
WOULD
HAVE
DIFFICULTY
FIG
URING
OUT
PRECISELY
WHICH
OTHER
PROCESS
TO
REACTIVATE
A
GOOD
EXAMPLE
IS
A
MEMORY
MANAGER
THE
MANAGER
HAS
J
BYTES
FREE
A
PROCESS
FREES
UP
AN
ADDITIONAL
K
BYTES
BUT
IT
DOES
NOT
KNOW
WHICH
WAITING
PROCESS
CAN
PROCEED
WITH
A
TOTAL
OF
K
J
BYTES
HENCE
IT
USES
BROADCAST
AND
ALL
PROCESSES
CHECK
FOR
THEMSELVES
IF
THERE
IS
ENOUGH
MEMORY
FREE
AN
ADVANTAGE
OF
LAMPSON
REDELL
MONITORS
OVER
HOARE
MONITORS
IS
THAT
THE
LAMPSON
REDELL
APPROACH
IS
LESS
PRONE
TO
ERROR
IN
THE
LAMPSON
REDELL
APPROACH
BECAUSE
EACH
PROCEDURE
CHECKS
THE
MONITOR
VARIABLE
AFTER
BEING
SIGNALED
WITH
THE
USE
OF
THE
WHILE
CONSTRUCT
A
PROCESS
CAN
SIGNAL
OR
BROADCAST
INCORRECTLY
WITHOUT
CAUSING
AN
ERROR
IN
THE
SIGNALED
PROGRAM
THE
SIGNALED
PROGRAM
WILL
CHECK
THE
REL
EVANT
VARIABLE
AND
IF
THE
DESIRED
CONDITION
IS
NOT
MET
CONTINUE
TO
WAIT
ANOTHER
ADVANTAGE
OF
THE
LAMPSON
REDELL
MONITOR
IS
THAT
IT
LENDS
ITSELF
TO
A
MORE
MODULAR
APPROACH
TO
PROGRAM
CONSTRUCTION
FOR
EXAMPLE
CONSIDER
THE
IMPLE
MENTATION
OF
A
BUFFER
ALLOCATOR
THERE
ARE
TWO
LEVELS
OF
CONDITIONS
TO
BE
SATISFIED
FOR
COOPERATING
SEQUENTIAL
PROCESSES
CONSISTENT
DATA
STRUCTURES
THUS
THE
MONITOR
ENFORCES
MUTUAL
EXCLUSION
AND
COMPLETES
AN
INPUT
OR
OUTPUT
OPERATION
BEFORE
ALLOWING
ANOTHER
OPERATION
ON
THE
BUFFER
LEVEL
PLUS
ENOUGH
MEMORY
FOR
THIS
PROCESS
TO
COMPLETE
ITS
ALLOCATION
REQUEST
IN
THE
HOARE
MONITOR
EACH
SIGNAL
CONVEYS
THE
LEVEL
CONDITION
BUT
ALSO
CAR
RIES
THE
IMPLICIT
MESSAGE
I
HAVE
FREED
ENOUGH
BYTES
FOR
YOUR
PARTICULAR
ALLOCATE
CALL
TO
WORK
NOW
THUS
THE
SIGNAL
IMPLICITLY
CARRIES
THE
LEVEL
CONDITION
IF
THE
PRO
GRAMMER
LATER
CHANGES
THE
DEFINITION
OF
THE
LEVEL
CONDITION
IT
WILL
BE
NECESSARY
TO
REPROGRAM
ALL
SIGNALING
PROCESSES
IF
THE
PROGRAMMER
CHANGES
THE
ASSUMPTIONS
MADE
BY
ANY
PARTICULAR
WAITING
PROCESS
I
E
WAITING
FOR
A
SLIGHTLY
DIFFERENT
LEVEL
INVARI
ANT
IT
MAY
BE
NECESSARY
TO
REPROGRAM
ALL
SIGNALING
PROCESSES
THIS
IS
UNMODULAR
AND
LIKELY
TO
CAUSE
SYNCHRONIZATION
ERRORS
E
G
WAKE
UP
BY
MISTAKE
WHEN
THE
CODE
IS
MODIFIED
THE
PROGRAMMER
HAS
TO
REMEMBER
TO
MODIFY
ALL
PROCEDURES
IN
THE
MONITOR
EVERY
TIME
A
SMALL
CHANGE
IS
MADE
TO
THE
LEVEL
CONDITION
WITH
A
LAMPSON
REDELL
MONITOR
A
BROADCAST
ENSURES
THE
LEVEL
CONDITION
AND
CARRIES
A
HINT
THAT
LEVEL
MIGHT
HOLD
EACH
PROCESS
SHOULD
CHECK
THE
LEVEL
CONDITION
ITSELF
IF
A
CHANGE
IS
MADE
IN
THE
LEVEL
CONDITION
IN
EITHER
A
WAITER
OR
A
SIGNALER
THERE
IS
NO
POSSIBILITY
OF
ERRONE
OUS
WAKEUP
BECAUSE
EACH
PROCEDURE
CHECKS
ITS
OWN
LEVEL
CONDITION
THEREFORE
THE
LEVEL
CONDITION
CAN
BE
HIDDEN
WITHIN
EACH
PROCEDURE
WITH
THE
HOARE
MONITOR
THE
LEVEL
CONDITION
MUST
BE
CARRIED
FROM
THE
WAITER
INTO
THE
CODE
OF
EVERY
SIGNALING
PROCESS
WHICH
VIOLATES
DATA
ABSTRACTION
AND
INTERPROCEDURAL
MODULARITY
PRINCIPLES
MESSAGE
PASSING
MESSAGE
PASSING
WHEN
PROCESSES
INTERACT
WITH
ONE
ANOTHER
TWO
FUNDAMENTAL
REQUIREMENTS
MUST
BE
SATISFIED
SYNCHRONIZATION
AND
COMMUNICATION
PROCESSES
NEED
TO
BE
SYNCHRO
NIZED
TO
ENFORCE
MUTUAL
EXCLUSION
COOPERATING
PROCESSES
MAY
NEED
TO
EXCHANGE
INFORMATION
ONE
APPROACH
TO
PROVIDING
BOTH
OF
THESE
FUNCTIONS
IS
MESSAGE
PASSING
MESSAGE
PASSING
HAS
THE
FURTHER
ADVANTAGE
THAT
IT
LENDS
ITSELF
TO
IMPLEMENTATION
IN
DISTRIBUTED
SYSTEMS
AS
WELL
AS
IN
SHARED
MEMORY
MULTIPROCESSOR
AND
UNIPROCESSOR
SYSTEMS
MESSAGE
PASSING
SYSTEMS
COME
IN
MANY
FORMS
IN
THIS
SECTION
WE
PROVIDE
A
GENERAL
INTRODUCTION
THAT
DISCUSSES
FEATURES
TYPICALLY
FOUND
IN
SUCH
SYSTEMS
THE
ACTUAL
FUNCTION
OF
MESSAGE
PASSING
IS
NORMALLY
PROVIDED
IN
THE
FORM
OF
A
PAIR
OF
PRIMITIVES
SEND
DESTINATION
MESSAGE
RECEIVE
SOURCE
MESSAGE
THIS
IS
THE
MINIMUM
SET
OF
OPERATIONS
NEEDED
FOR
PROCESSES
TO
ENGAGE
IN
MES
SAGE
PASSING
A
PROCESS
SENDS
INFORMATION
IN
THE
FORM
OF
A
MESSAGE
TO
ANOTHER
PROC
ESS
DESIGNATED
BY
A
DESTINATION
A
PROCESS
RECEIVES
INFORMATION
BY
EXECUTING
THE
RECEIVE
PRIMITIVE
INDICATING
THE
SOURCE
AND
THE
MESSAGE
A
NUMBER
OF
DESIGN
ISSUES
RELATING
TO
MESSAGE
PASSING
SYSTEMS
ARE
LISTED
IN
TABLE
AND
EXAMINED
IN
THE
REMAINDER
OF
THIS
SECTION
TABLE
DESIGN
CHARACTERISTICS
OF
MESSAGE
SYSTEMS
FOR
INTERPROCESS
COMMUNICATION
AND
SYNCHRONIZATION
SYNCHRONIZATION
FORMAT
SEND
CONTENT
BLOCKING
LENGTH
NONBLOCKING
FIXED
RECEIVE
VARIABLE
BLOCKING
NONBLOCKING
QUEUEING
DISCIPLINE
TEST
FOR
ARRIVAL
FIFO
PRIORITY
ADDRESSING
DIRECT
SEND
RECEIVE
EXPLICIT
IMPLICIT
INDIRECT
STATIC
DYNAMIC
OWNERSHIP
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
SYNCHRONIZATION
THE
COMMUNICATION
OF
A
MESSAGE
BETWEEN
TWO
PROCESSES
IMPLIES
SOME
LEVEL
OF
SYN
CHRONIZATION
BETWEEN
THE
TWO
THE
RECEIVER
CANNOT
RECEIVE
A
MESSAGE
UNTIL
IT
HAS
BEEN
SENT
BY
ANOTHER
PROCESS
IN
ADDITION
WE
NEED
TO
SPECIFY
WHAT
HAPPENS
TO
A
PROCESS
AFTER
IT
ISSUES
A
SEND
OR
RECEIVE
PRIMITIVE
CONSIDER
THE
SEND
PRIMITIVE
FIRST
WHEN
A
SEND
PRIMITIVE
IS
EXECUTED
IN
A
PROCESS
THERE
ARE
TWO
POSSIBILITIES
EITHER
THE
SENDING
PROCESS
IS
BLOCKED
UNTIL
THE
MESSAGE
IS
RECEIVED
OR
IT
IS
NOT
SIMILARLY
WHEN
A
PROCESS
ISSUES
A
RECEIVE
PRIMI
TIVE
THERE
ARE
TWO
POSSIBILITIES
IF
A
MESSAGE
HAS
PREVIOUSLY
BEEN
SENT
THE
MESSAGE
IS
RECEIVED
AND
EXECUTION
CONTINUES
IF
THERE
IS
NO
WAITING
MESSAGE
THEN
EITHER
A
THE
PROCESS
IS
BLOCKED
UNTIL
A
MESSAGE
ARRIVES
OR
B
THE
PROCESS
CONTINUES
TO
EXECUTE
ABANDONING
THE
ATTEMPT
TO
RECEIVE
THUS
BOTH
THE
SENDER
AND
RECEIVER
CAN
BE
BLOCKING
OR
NONBLOCKING
THREE
COMBINATIONS
ARE
COMMON
ALTHOUGH
ANY
PARTICULAR
SYSTEM
WILL
USUALLY
HAVE
ONLY
ONE
OR
TWO
COMBINATIONS
IMPLEMENTED
BLOCKING
SEND
BLOCKING
RECEIVE
BOTH
THE
SENDER
AND
RECEIVER
ARE
BLOCKED
UN
TIL
THE
MESSAGE
IS
DELIVERED
THIS
IS
SOMETIMES
REFERRED
TO
AS
A
RENDEZVOUS
THIS
COMBINATION
ALLOWS
FOR
TIGHT
SYNCHRONIZATION
BETWEEN
PROCESSES
NONBLOCKING
SEND
BLOCKING
RECEIVE
ALTHOUGH
THE
SENDER
MAY
CONTINUE
ON
THE
RECEIVER
IS
BLOCKED
UNTIL
THE
REQUESTED
MESSAGE
ARRIVES
THIS
IS
PROBABLY
THE
MOST
USEFUL
COMBINATION
IT
ALLOWS
A
PROCESS
TO
SEND
ONE
OR
MORE
MESSAGES
TO
A
VARIETY
OF
DESTINATIONS
AS
QUICKLY
AS
POSSIBLE
A
PROCESS
THAT
MUST
RECEIVE
A
MESSAGE
BEFORE
IT
CAN
DO
USEFUL
WORK
NEEDS
TO
BE
BLOCKED
UNTIL
SUCH
A
MES
SAGE
ARRIVES
AN
EXAMPLE
IS
A
SERVER
PROCESS
THAT
EXISTS
TO
PROVIDE
A
SERVICE
OR
RESOURCE
TO
OTHER
PROCESSES
NONBLOCKING
SEND
NONBLOCKING
RECEIVE
NEITHER
PARTY
IS
REQUIRED
TO
WAIT
THE
NONBLOCKING
SEND
IS
MORE
NATURAL
FOR
MANY
CONCURRENT
PROGRAMMING
TASKS
FOR
EXAMPLE
IF
IT
IS
USED
TO
REQUEST
AN
OUTPUT
OPERATION
SUCH
AS
PRINTING
IT
ALLOWS
THE
REQUESTING
PROCESS
TO
ISSUE
THE
REQUEST
IN
THE
FORM
OF
A
MESSAGE
AND
THEN
CARRY
ON
ONE
POTENTIAL
DANGER
OF
THE
NONBLOCKING
SEND
IS
THAT
AN
ERROR
COULD
LEAD
TO
A
SITUATION
IN
WHICH
A
PROCESS
REPEATEDLY
GENERATES
MESSAGES
BECAUSE
THERE
IS
NO
BLOCKING
TO
DISCIPLINE
THE
PROCESS
THESE
MESSAGES
COULD
CONSUME
SYSTEM
RESOURCES
INCLUDING
PROCESSOR
TIME
AND
BUFFER
SPACE
TO
THE
DETRIMENT
OF
OTHER
PROCESSES
AND
THE
OS
ALSO
THE
NONBLOCKING
SEND
PLACES
THE
BURDEN
ON
THE
PROGRAMMER
TO
DETER
MINE
THAT
A
MESSAGE
HAS
BEEN
RECEIVED
PROCESSES
MUST
EMPLOY
REPLY
MESSAGES
TO
ACKNOWLEDGE
RECEIPT
OF
A
MESSAGE
FOR
THE
RECEIVE
PRIMITIVE
THE
BLOCKING
VERSION
APPEARS
TO
BE
MORE
NATURAL
FOR
MANY
CONCURRENT
PROGRAMMING
TASKS
GENERALLY
A
PROCESS
THAT
REQUESTS
A
MES
SAGE
WILL
NEED
THE
EXPECTED
INFORMATION
BEFORE
PROCEEDING
HOWEVER
IF
A
MESSAGE
IS
LOST
WHICH
CAN
HAPPEN
IN
A
DISTRIBUTED
SYSTEM
OR
IF
A
PROCESS
FAILS
BEFORE
IT
SENDS
AN
ANTICIPATED
MESSAGE
A
RECEIVING
PROCESS
COULD
BE
BLOCKED
INDEFINITELY
THIS
MESSAGE
PASSING
PROBLEM
CAN
BE
SOLVED
BY
THE
USE
OF
THE
NONBLOCKING
RECEIVE
HOWEVER
THE
DAN
GER
OF
THIS
APPROACH
IS
THAT
IF
A
MESSAGE
IS
SENT
AFTER
A
PROCESS
HAS
ALREADY
EXECUTED
A
MATCHING
RECEIVE
THE
MESSAGE
WILL
BE
LOST
OTHER
POSSIBLE
APPROACHES
ARE
TO
ALLOW
A
PROCESS
TO
TEST
WHETHER
A
MESSAGE
IS
WAITING
BEFORE
ISSUING
A
RECEIVE
AND
ALLOW
A
PROCESS
TO
SPECIFY
MORE
THAN
ONE
SOURCE
IN
A
RECEIVE
PRIMITIVE
THE
LATTER
APPROACH
IS
USEFUL
IF
A
PROCESS
IS
WAITING
FOR
MESSAGES
FROM
MORE
THAN
ONE
SOURCE
AND
CAN
PROCEED
IF
ANY
OF
THESE
MESSAGES
ARRIVE
ADDRESSING
CLEARLY
IT
IS
NECESSARY
TO
HAVE
A
WAY
OF
SPECIFYING
IN
THE
SEND
PRIMITIVE
WHICH
PRO
CESS
IS
TO
RECEIVE
THE
MESSAGE
SIMILARLY
MOST
IMPLEMENTATIONS
ALLOW
A
RECEIVING
PROCESS
TO
INDICATE
THE
SOURCE
OF
A
MESSAGE
TO
BE
RECEIVED
THE
VARIOUS
SCHEMES
FOR
SPECIFYING
PROCESSES
IN
SEND
AND
RECEIVE
PRIMI
TIVES
FALL
INTO
TWO
CATEGORIES
DIRECT
ADDRESSING
AND
INDIRECT
ADDRESSING
WITH
DIRECT
ADDRESSING
THE
SEND
PRIMITIVE
INCLUDES
A
SPECIFIC
IDENTIFIER
OF
THE
DESTINATION
PROC
ESS
THE
RECEIVE
PRIMITIVE
CAN
BE
HANDLED
IN
ONE
OF
TWO
WAYS
ONE
POSSIBILITY
IS
TO
REQUIRE
THAT
THE
PROCESS
EXPLICITLY
DESIGNATE
A
SENDING
PROCESS
THUS
THE
PROC
ESS
MUST
KNOW
AHEAD
OF
TIME
FROM
WHICH
PROCESS
A
MESSAGE
IS
EXPECTED
THIS
WILL
OFTEN
BE
EFFECTIVE
FOR
COOPERATING
CONCURRENT
PROCESSES
IN
OTHER
CASES
HOWEVER
IT
IS
IMPOSSIBLE
TO
SPECIFY
THE
ANTICIPATED
SOURCE
PROCESS
AN
EXAMPLE
IS
A
PRINTER
SERVER
PROCESS
WHICH
WILL
ACCEPT
A
PRINT
REQUEST
MESSAGE
FROM
ANY
OTHER
PROCESS
FOR
SUCH
APPLICATIONS
A
MORE
EFFECTIVE
APPROACH
IS
THE
USE
OF
IMPLICIT
ADDRESSING
IN
THIS
CASE
THE
SOURCE
PARAMETER
OF
THE
RECEIVE
PRIMITIVE
POSSESSES
A
VALUE
RETURNED
WHEN
THE
RECEIVE
OPERATION
HAS
BEEN
PERFORMED
THE
OTHER
GENERAL
APPROACH
IS
INDIRECT
ADDRESSING
IN
THIS
CASE
MESSAGES
ARE
NOT
SENT
DIRECTLY
FROM
SENDER
TO
RECEIVER
BUT
RATHER
ARE
SENT
TO
A
SHARED
DATA
STRUC
TURE
CONSISTING
OF
QUEUES
THAT
CAN
TEMPORARILY
HOLD
MESSAGES
SUCH
QUEUES
ARE
GEN
ERALLY
REFERRED
TO
AS
MAILBOXES
THUS
FOR
TWO
PROCESSES
TO
COMMUNICATE
ONE
PROC
ESS
SENDS
A
MESSAGE
TO
THE
APPROPRIATE
MAILBOX
AND
THE
OTHER
PROCESS
PICKS
UP
THE
MESSAGE
FROM
THE
MAILBOX
A
STRENGTH
OF
THE
USE
OF
INDIRECT
ADDRESSING
IS
THAT
BY
DECOUPLING
THE
SENDER
AND
RECEIVER
IT
ALLOWS
FOR
GREATER
FLEXIBILITY
IN
THE
USE
OF
MESSAGES
THE
RELATIONSHIP
BETWEEN
SENDERS
AND
RECEIVERS
CAN
BE
ONE
TO
ONE
MANY
TO
ONE
ONE
TO
MANY
OR
MANY
TO
MANY
FIGURE
A
ONE
TO
ONE
RELATIONSHIP
ALLOWS
A
PRIVATE
COMMUNI
CATIONS
LINK
TO
BE
SET
UP
BETWEEN
TWO
PROCESSES
THIS
INSULATES
THEIR
INTERACTION
FROM
ERRONEOUS
INTERFERENCE
FROM
OTHER
PROCESSES
A
MANY
TO
ONE
RELATIONSHIP
IS
USE
FUL
FOR
CLIENT
SERVER
INTERACTION
ONE
PROCESS
PROVIDES
SERVICE
TO
A
NUMBER
OF
OTHER
PROCESSES
IN
THIS
CASE
THE
MAILBOX
IS
OFTEN
REFERRED
TO
AS
A
PORT
A
ONE
TO
MANY
RELATIONSHIP
ALLOWS
FOR
ONE
SENDER
AND
MULTIPLE
RECEIVERS
IT
IS
USEFUL
FOR
APPLICATIONS
WHERE
A
MESSAGE
OR
SOME
INFORMATION
IS
TO
BE
BROADCAST
TO
A
SET
OF
PROCESSES
A
MANY
TO
MANY
RELATIONSHIP
ALLOWS
MULTIPLE
SERVER
PROCESSES
TO
PROVIDE
CONCURRENT
SERVICE
TO
MULTIPLE
CLIENTS
THE
ASSOCIATION
OF
PROCESSES
TO
MAILBOXES
CAN
BE
EITHER
STATIC
OR
DYNAMIC
PORTS
ARE
OFTEN
STATICALLY
ASSOCIATED
WITH
A
PARTICULAR
PROCESS
THAT
IS
THE
PORT
IS
CREATED
AND
ASSIGNED
TO
THE
PROCESS
PERMANENTLY
SIMILARLY
A
ONE
TO
ONE
RELATION
SHIP
IS
TYPICALLY
DEFINED
STATICALLY
AND
PERMANENTLY
WHEN
THERE
ARE
MANY
SENDERS
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
A
ONE
TO
ONE
B
MANY
TO
ONE
C
ONE
TO
MANY
FIGURE
INDIRECT
PROCESS
COMMUNICATION
D
MANY
TO
MANY
THE
ASSOCIATION
OF
A
SENDER
TO
A
MAILBOX
MAY
OCCUR
DYNAMICALLY
PRIMITIVES
SUCH
AS
CONNECT
AND
DISCONNECT
MAY
BE
USED
FOR
THIS
PURPOSE
A
RELATED
ISSUE
HAS
TO
DO
WITH
THE
OWNERSHIP
OF
A
MAILBOX
IN
THE
CASE
OF
A
PORT
IT
IS
TYPICALLY
OWNED
BY
AND
CREATED
BY
THE
RECEIVING
PROCESS
THUS
WHEN
THE
PROCESS
IS
DESTROYED
THE
PORT
IS
ALSO
DESTROYED
FOR
THE
GENERAL
MAILBOX
CASE
THE
OS
MAY
OFFER
A
CREATE
MAILBOX
SERVICE
SUCH
MAILBOXES
CAN
BE
VIEWED
EITHER
AS
BEING
OWNED
BY
THE
CREATING
PROCESS
IN
WHICH
CASE
THEY
TERMINATE
WITH
THE
PROCESS
OR
AS
BEING
OWNED
BY
THE
OS
IN
WHICH
CASE
AN
EXPLICIT
COMMAND
WILL
BE
REQUIRED
TO
DESTROY
THE
MAILBOX
MESSAGE
FORMAT
THE
FORMAT
OF
THE
MESSAGE
DEPENDS
ON
THE
OBJECTIVES
OF
THE
MESSAGING
FACILITY
AND
WHETHER
THE
FACILITY
RUNS
ON
A
SINGLE
COMPUTER
OR
ON
A
DISTRIBUTED
SYSTEM
FOR
SOME
OPERATING
SYSTEMS
DESIGNERS
HAVE
PREFERRED
SHORT
FIXED
LENGTH
MESSAGES
TO
MINI
MIZE
PROCESSING
AND
STORAGE
OVERHEAD
IF
A
LARGE
AMOUNT
OF
DATA
IS
TO
BE
PASSED
THE
DATA
CAN
BE
PLACED
IN
A
FILE
AND
THE
MESSAGE
THEN
SIMPLY
REFERENCES
THAT
FILE
A
MORE
FLEXIBLE
APPROACH
IS
TO
ALLOW
VARIABLE
LENGTH
MESSAGES
FIGURE
SHOWS
A
TYPICAL
MESSAGE
FORMAT
FOR
OPERATING
SYSTEMS
THAT
SUPPORT
VARIABLE
LENGTH
MESSAGES
THE
MESSAGE
IS
DIVIDED
INTO
TWO
PARTS
A
HEADER
WHICH
CONTAINS
INFORMATION
ABOUT
THE
MESSAGE
AND
A
BODY
WHICH
CONTAINS
THE
ACTUAL
CON
TENTS
OF
THE
MESSAGE
THE
HEADER
MAY
CONTAIN
AN
IDENTIFICATION
OF
THE
SOURCE
AND
INTENDED
DESTINATION
OF
THE
MESSAGE
A
LENGTH
FIELD
AND
A
TYPE
FIELD
TO
DISCRIMINATE
AMONG
VARIOUS
TYPES
OF
MESSAGES
THERE
MAY
ALSO
BE
ADDITIONAL
CONTROL
INFORMATION
MESSAGE
PASSING
HEADER
BODY
FIGURE
GENERAL
MESSAGE
FORMAT
SUCH
AS
A
POINTER
FIELD
SO
THAT
A
LINKED
LIST
OF
MESSAGES
CAN
BE
CREATED
A
SEQUENCE
NUMBER
TO
KEEP
TRACK
OF
THE
NUMBER
AND
ORDER
OF
MESSAGES
PASSED
BETWEEN
SOURCE
AND
DESTINATION
AND
A
PRIORITY
FIELD
QUEUEING
DISCIPLINE
THE
SIMPLEST
QUEUEING
DISCIPLINE
IS
FIRST
IN
FIRST
OUT
BUT
THIS
MAY
NOT
BE
SUFFICIENT
IF
SOME
MESSAGES
ARE
MORE
URGENT
THAN
OTHERS
AN
ALTERNATIVE
IS
TO
ALLOW
THE
SPECI
FYING
OF
MESSAGE
PRIORITY
ON
THE
BASIS
OF
MESSAGE
TYPE
OR
BY
DESIGNATION
BY
THE
SENDER
ANOTHER
ALTERNATIVE
IS
TO
ALLOW
THE
RECEIVER
TO
INSPECT
THE
MESSAGE
QUEUE
AND
SELECT
WHICH
MESSAGE
TO
RECEIVE
NEXT
MUTUAL
EXCLUSION
FIGURE
SHOWS
ONE
WAY
IN
WHICH
MESSAGE
PASSING
CAN
BE
USED
TO
ENFORCE
MUTUAL
EXCLUSION
COMPARE
FIGURES
AND
WE
ASSUME
THE
USE
OF
THE
BLOCKING
RECEIVE
PRIMITIVE
AND
THE
NONBLOCKING
SEND
PRIMITIVE
A
SET
OF
CONCURRENT
PRO
CESSES
SHARE
A
MAILBOX
BOX
WHICH
CAN
BE
USED
BY
ALL
PROCESSES
TO
SEND
AND
RECEIVE
FIGURE
MUTUAL
EXCLUSION
USING
MESSAGES
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
THE
MAILBOX
IS
INITIALIZED
TO
CONTAIN
A
SINGLE
MESSAGE
WITH
NULL
CONTENT
A
PROCESS
WISHING
TO
ENTER
ITS
CRITICAL
SECTION
FIRST
ATTEMPTS
TO
RECEIVE
A
MESSAGE
IF
THE
MAILBOX
IS
EMPTY
THEN
THE
PROCESS
IS
BLOCKED
ONCE
A
PROCESS
HAS
ACQUIRED
THE
MESSAGE
IT
PERFORMS
ITS
CRITICAL
SECTION
AND
THEN
PLACES
THE
MESSAGE
BACK
INTO
THE
MAILBOX
THUS
THE
MESSAGE
FUNCTIONS
AS
A
TOKEN
THAT
IS
PASSED
FROM
PROCESS
TO
PROCESS
THE
PRECEDING
SOLUTION
ASSUMES
THAT
IF
MORE
THAN
ONE
PROCESS
PERFORMS
THE
RECEIVE
OPERATION
CONCURRENTLY
THEN
IF
THERE
IS
A
MESSAGE
IT
IS
DELIVERED
TO
ONLY
ONE
PROCESS
AND
THE
OTHERS
ARE
BLOCKED
OR
IF
THE
MESSAGE
QUEUE
IS
EMPTY
ALL
PROCESSES
ARE
BLOCKED
WHEN
A
MESSAGE
IS
AVAILABLE
ONLY
ONE
BLOCKED
PROCESS
IS
ACTIVATED
AND
GIVEN
THE
MESSAGE
THESE
ASSUMPTIONS
ARE
TRUE
OF
VIRTUALLY
ALL
MESSAGE
PASSING
FACILITIES
AS
AN
EXAMPLE
OF
THE
USE
OF
MESSAGE
PASSING
FIGURE
IS
A
SOLUTION
TO
THE
BOUNDED
BUFFER
PRODUCER
CONSUMER
PROBLEM
USING
THE
BASIC
MUTUAL
EXCLUSION
POWER
OF
MESSAGE
PASSING
THE
PROBLEM
COULD
HAVE
BEEN
SOLVED
WITH
AN
ALGORITHMIC
STRUCTURE
SIMILAR
TO
THAT
OF
FIGURE
INSTEAD
THE
PROGRAM
OF
FIGURE
TAKES
ADVANTAGE
OF
THE
ABILITY
OF
MESSAGE
PASSING
TO
BE
USED
TO
PASS
DATA
IN
ADDITION
TO
SIGNALS
TWO
MAILBOXES
ARE
USED
AS
THE
PRODUCER
GENERATES
DATA
IT
IS
SENT
AS
MES
SAGES
TO
THE
MAILBOX
MAYCONSUME
AS
LONG
AS
THERE
IS
AT
LEAST
ONE
MESSAGE
IN
THAT
MAILBOX
THE
CONSUMER
CAN
CONSUME
HENCE
MAYCONSUME
SERVES
AS
THE
BUFFER
THE
DATA
IN
THE
BUFFER
ARE
ORGANIZED
AS
A
QUEUE
OF
MESSAGES
THE
SIZE
OF
THE
BUFFER
IS
FIGURE
A
SOLUTION
TO
THE
BOUNDED
BUFFER
PRODUCER
CONSUMER
PROBLEM
USING
MESSAGES
READERS
WRITERS
PROBLEM
DETERMINED
BY
THE
GLOBAL
VARIABLE
CAPACITY
INITIALLY
THE
MAILBOX
MAYPRODUCE
IS
FILLED
WITH
A
NUMBER
OF
NULL
MESSAGES
EQUAL
TO
THE
CAPACITY
OF
THE
BUFFER
THE
NUMBER
OF
MESSAGES
IN
MAYPRODUCE
SHRINKS
WITH
EACH
PRODUCTION
AND
GROWS
WITH
EACH
CONSUMPTION
THIS
APPROACH
IS
QUITE
FLEXIBLE
THERE
MAY
BE
MULTIPLE
PRODUCERS
AND
CONSUM
ERS
AS
LONG
AS
ALL
HAVE
ACCESS
TO
BOTH
MAILBOXES
THE
SYSTEM
MAY
EVEN
BE
DISTRIB
UTED
WITH
ALL
PRODUCER
PROCESSES
AND
THE
MAYPRODUCE
MAILBOX
AT
ONE
SITE
AND
ALL
THE
CONSUMER
PROCESSES
AND
THE
MAYCONSUME
MAILBOX
AT
ANOTHER
READERS
WRITERS
PROBLEM
IN
DEALING
WITH
THE
DESIGN
OF
SYNCHRONIZATION
AND
CONCURRENCY
MECHANISMS
IT
IS
USEFUL
TO
BE
ABLE
TO
RELATE
THE
PROBLEM
AT
HAND
TO
KNOWN
PROBLEMS
AND
TO
BE
ABLE
TO
TEST
ANY
SOLUTION
IN
TERMS
OF
ITS
ABILITY
TO
SOLVE
THESE
KNOWN
PROBLEMS
IN
THE
LITERATURE
SEVERAL
PROBLEMS
HAVE
ASSUMED
IMPORTANCE
AND
APPEAR
FREQUENTLY
BOTH
BECAUSE
THEY
ARE
EXAMPLES
OF
COMMON
DESIGN
PROBLEMS
AND
BECAUSE
OF
THEIR
EDU
CATIONAL
VALUE
ONE
SUCH
PROBLEM
IS
THE
PRODUCER
CONSUMER
PROBLEM
WHICH
HAS
ALREADY
BEEN
EXPLORED
IN
THIS
SECTION
WE
LOOK
AT
ANOTHER
CLASSIC
PROBLEM
THE
READ
ERS
WRITERS
PROBLEM
THE
READERS
WRITERS
PROBLEM
IS
DEFINED
AS
FOLLOWS
THERE
IS
A
DATA
AREA
SHARED
AMONG
A
NUMBER
OF
PROCESSES
THE
DATA
AREA
COULD
BE
A
FILE
A
BLOCK
OF
MAIN
MEM
ORY
OR
EVEN
A
BANK
OF
PROCESSOR
REGISTERS
THERE
ARE
A
NUMBER
OF
PROCESSES
THAT
ONLY
READ
THE
DATA
AREA
READERS
AND
A
NUMBER
THAT
ONLY
WRITE
TO
THE
DATA
AREA
WRITERS
THE
CONDITIONS
THAT
MUST
BE
SATISFIED
ARE
AS
FOLLOWS
ANY
NUMBER
OF
READERS
MAY
SIMULTANEOUSLY
READ
THE
FILE
ONLY
ONE
WRITER
AT
A
TIME
MAY
WRITE
TO
THE
FILE
IF
A
WRITER
IS
WRITING
TO
THE
FILE
NO
READER
MAY
READ
IT
THUS
READERS
ARE
PROCESSES
THAT
ARE
NOT
REQUIRED
TO
EXCLUDE
ONE
ANOTHER
AND
WRITERS
ARE
PROCESSES
THAT
ARE
REQUIRED
TO
EXCLUDE
ALL
OTHER
PROCESSES
READERS
AND
WRITERS
ALIKE
BEFORE
PROCEEDING
LET
US
DISTINGUISH
THIS
PROBLEM
FROM
TWO
OTHERS
THE
GENERAL
MUTUAL
EXCLUSION
PROBLEM
AND
THE
PRODUCER
CONSUMER
PROBLEM
IN
THE
READERS
WRIT
ERS
PROBLEM
READERS
DO
NOT
ALSO
WRITE
TO
THE
DATA
AREA
NOR
DO
WRITERS
READ
THE
DATA
AREA
WHILE
WRITING
A
MORE
GENERAL
CASE
WHICH
INCLUDES
THIS
CASE
IS
TO
ALLOW
ANY
OF
THE
PROCESSES
TO
READ
OR
WRITE
THE
DATA
AREA
IN
THAT
CASE
WE
CAN
DECLARE
ANY
POR
TION
OF
A
PROCESS
THAT
ACCESSES
THE
DATA
AREA
TO
BE
A
CRITICAL
SECTION
AND
IMPOSE
THE
GENERAL
MUTUAL
EXCLUSION
SOLUTION
THE
REASON
FOR
BEING
CONCERNED
WITH
THE
MORE
RESTRICTED
CASE
IS
THAT
MORE
EFFICIENT
SOLUTIONS
ARE
POSSIBLE
FOR
THIS
CASE
AND
THAT
THE
LESS
EFFICIENT
SOLUTIONS
TO
THE
GENERAL
PROBLEM
ARE
UNACCEPTABLY
SLOW
FOR
EXAMPLE
SUPPOSE
THAT
THE
SHARED
AREA
IS
A
LIBRARY
CATALOG
ORDINARY
USERS
OF
THE
LIBRARY
READ
THE
CATALOG
TO
LOCATE
A
BOOK
ONE
OR
MORE
LIBRARIANS
ARE
ABLE
TO
UPDATE
THE
CATALOG
IN
THE
GENERAL
SOLUTION
EVERY
ACCESS
TO
THE
CATALOG
WOULD
BE
TREATED
AS
A
CRITICAL
SEC
TION
AND
USERS
WOULD
BE
FORCED
TO
READ
THE
CATALOG
ONE
AT
A
TIME
THIS
WOULD
CLEARLY
IMPOSE
INTOLERABLE
DELAYS
AT
THE
SAME
TIME
IT
IS
IMPORTANT
TO
PREVENT
WRITERS
FROM
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
INTERFERING
WITH
EACH
OTHER
AND
IT
IS
ALSO
REQUIRED
TO
PREVENT
READING
WHILE
WRITING
IS
IN
PROGRESS
TO
PREVENT
THE
ACCESS
OF
INCONSISTENT
INFORMATION
CAN
THE
PRODUCER
CONSUMER
PROBLEM
BE
CONSIDERED
SIMPLY
A
SPECIAL
CASE
OF
THE
READERS
WRITERS
PROBLEM
WITH
A
SINGLE
WRITER
THE
PRODUCER
AND
A
SINGLE
READER
THE
CONSUMER
THE
ANSWER
IS
NO
THE
PRODUCER
IS
NOT
JUST
A
WRITER
IT
MUST
READ
QUEUE
POINTERS
TO
DETERMINE
WHERE
TO
WRITE
THE
NEXT
ITEM
AND
IT
MUST
DETERMINE
IF
THE
BUFFER
IS
FULL
SIMILARLY
THE
CONSUMER
IS
NOT
JUST
A
READER
BECAUSE
IT
MUST
ADJUST
THE
QUEUE
POINTERS
TO
SHOW
THAT
IT
HAS
REMOVED
A
UNIT
FROM
THE
BUFFER
WE
NOW
EXAMINE
TWO
SOLUTIONS
TO
THE
PROBLEM
READERS
HAVE
PRIORITY
FIGURE
IS
A
SOLUTION
USING
SEMAPHORES
SHOWING
ONE
INSTANCE
EACH
OF
A
READER
AND
A
WRITER
THE
SOLUTION
DOES
NOT
CHANGE
FOR
MULTIPLE
READERS
AND
WRITERS
THE
WRITER
PROCESS
IS
SIMPLE
THE
SEMAPHORE
WSEM
IS
USED
TO
ENFORCE
MUTUAL
EXCLUSION
AS
LONG
AS
ONE
WRITER
IS
ACCESSING
THE
SHARED
DATA
AREA
NO
OTHER
WRITERS
AND
NO
READERS
MAY
ACCESS
IT
THE
READER
PROCESS
ALSO
MAKES
USE
OF
WSEM
TO
ENFORCE
MUTUAL
EXCLUSION
HOWEVER
TO
ALLOW
MULTIPLE
READERS
WE
REQUIRE
THAT
WHEN
THERE
ARE
NO
READERS
READING
THE
FIRST
READER
THAT
ATTEMPTS
TO
READ
SHOULD
WAIT
ON
WSEM
WHEN
FIGURE
A
SOLUTION
TO
THE
READERS
WRITERS
PROBLEM
USING
SEMAPHORE
READERS
HAVE
PRIORITY
READERS
WRITERS
PROBLEM
THERE
IS
ALREADY
AT
LEAST
ONE
READER
READING
SUBSEQUENT
READERS
NEED
NOT
WAIT
BEFORE
ENTERING
THE
GLOBAL
VARIABLE
READCOUNT
IS
USED
TO
KEEP
TRACK
OF
THE
NUMBER
OF
READERS
AND
THE
SEMAPHORE
X
IS
USED
TO
ASSURE
THAT
READCOUNT
IS
UPDATED
PROPERLY
WRITERS
HAVE
PRIORITY
IN
THE
PREVIOUS
SOLUTION
READERS
HAVE
PRIORITY
ONCE
A
SINGLE
READER
HAS
BEGUN
TO
ACCESS
THE
DATA
AREA
IT
IS
POSSIBLE
FOR
READERS
TO
RETAIN
CONTROL
OF
THE
DATA
AREA
AS
LONG
AS
THERE
IS
AT
LEAST
ONE
READER
IN
THE
ACT
OF
READING
THEREFORE
WRITERS
ARE
SUB
JECT
TO
STARVATION
FIGURE
SHOWS
A
SOLUTION
THAT
GUARANTEES
THAT
NO
NEW
READERS
ARE
ALLOWED
ACCESS
TO
THE
DATA
AREA
ONCE
AT
LEAST
ONE
WRITER
HAS
DECLARED
A
DESIRE
TO
WRITE
FOR
FIGURE
A
SOLUTION
TO
THE
READERS
WRITERS
PROBLEM
USING
SEMAPHORE
WRITERS
HAVE
PRIORITY
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
TABLE
STATE
OF
THE
PROCESS
QUEUES
FOR
PROGRAM
OF
FIGURE
READERS
ONLY
IN
THE
SYSTEM
WSEM
SET
NO
QUEUES
WRITERS
ONLY
IN
THE
SYSTEM
WSEM
AND
RSEM
SET
WRITERS
QUEUE
ON
WSEM
BOTH
READERS
AND
WRITERS
WITH
READ
FIRST
WSEM
SET
BY
READER
RSEM
SET
BY
WRITER
ALL
WRITERS
QUEUE
ON
WSEM
ONE
READER
QUEUES
ON
RSEM
OTHER
READERS
QUEUE
ON
Z
BOTH
READERS
AND
WRITERS
WITH
WRITE
FIRST
WSEM
SET
BY
WRITER
RSEM
SET
BY
WRITER
WRITERS
QUEUE
ON
WSEM
ONE
READER
QUEUES
ON
RSEM
OTHER
READERS
QUEUE
ON
Z
WRITERS
THE
FOLLOWING
SEMAPHORES
AND
VARIABLES
ARE
ADDED
TO
THE
ONES
ALREADY
DEFINED
A
SEMAPHORE
RSEM
THAT
INHIBITS
ALL
READERS
WHILE
THERE
IS
AT
LEAST
ONE
WRITER
DESIRING
ACCESS
TO
THE
DATA
AREA
A
VARIABLE
WRITECOUNT
THAT
CONTROLS
THE
SETTING
OF
RSEM
A
SEMAPHORE
Y
THAT
CONTROLS
THE
UPDATING
OF
WRITECOUNT
FOR
READERS
ONE
ADDITIONAL
SEMAPHORE
IS
NEEDED
A
LONG
QUEUE
MUST
NOT
BE
ALLOWED
TO
BUILD
UP
ON
RSEM
OTHERWISE
WRITERS
WILL
NOT
BE
ABLE
TO
JUMP
THE
QUEUE
THEREFORE
ONLY
ONE
READER
IS
ALLOWED
TO
QUEUE
ON
RSEM
WITH
ANY
ADDITIONAL
READERS
QUEUEING
ON
SEMAPHORE
Z
IMMEDIATELY
BEFORE
WAITING
ON
RSEM
TABLE
SUMMARIZES
THE
POSSIBILITIES
AN
ALTERNATIVE
SOLUTION
WHICH
GIVES
WRITERS
PRIORITY
AND
WHICH
IS
IMPLEMENTED
USING
MESSAGE
PASSING
IS
SHOWN
IN
FIGURE
IN
THIS
CASE
THERE
IS
A
CONTROLLER
PROCESS
THAT
HAS
ACCESS
TO
THE
SHARED
DATA
AREA
OTHER
PROCESSES
WISHING
TO
ACCESS
THE
DATA
AREA
SEND
A
REQUEST
MESSAGE
TO
THE
CONTROLLER
ARE
GRANTED
ACCESS
WITH
AN
OK
REPLY
MESSAGE
AND
INDICATE
COMPLETION
OF
ACCESS
WITH
A
FINISHED
MESSAGE
THE
CONTROLLER
IS
EQUIPPED
WITH
THREE
MAILBOXES
ONE
FOR
EACH
TYPE
OF
MESSAGE
THAT
IT
MAY
RECEIVE
THE
CONTROLLER
PROCESS
SERVICES
WRITE
REQUEST
MESSAGES
BEFORE
READ
REQUEST
MESSAGES
TO
GIVE
WRITERS
PRIORITY
IN
ADDITION
MUTUAL
EXCLUSION
MUST
BE
ENFORCED
TO
DO
THIS
THE
VARIABLE
COUNT
IS
USED
WHICH
IS
INITIALIZED
TO
SOME
NUMBER
GREATER
THAN
THE
MAXIMUM
POSSIBLE
NUMBER
OF
READERS
IN
THIS
EXAMPLE
WE
USE
A
VALUE
OF
THE
ACTION
OF
THE
CONTROLLER
CAN
BE
SUMMARIZED
AS
FOLLOWS
IF
COUNT
THEN
NO
WRITER
IS
WAITING
AND
THERE
MAY
OR
MAY
NOT
BE
READ
ERS
ACTIVE
SERVICE
ALL
FINISHED
MESSAGES
FIRST
TO
CLEAR
ACTIVE
READERS
THEN
SERVICE
WRITE
REQUESTS
AND
THEN
READ
REQUESTS
IF
COUNT
THEN
THE
ONLY
REQUEST
OUTSTANDING
IS
A
WRITE
REQUEST
ALLOW
THE
WRITER
TO
PROCEED
AND
WAIT
FOR
A
FINISHED
MESSAGE
SUMMARY
VOID
READER
INT
I
MESSAGE
RMSG
WHILE
TRUE
RMSG
I
SEND
READREQUEST
RMSG
RECEIVE
MBOX
I
RMSG
READUNIT
RMSG
I
SEND
FINISHED
RMSG
VOID
WRITER
INT
J
MESSAGE
RMSG
WHILE
TRUE
RMSG
J
SEND
WRITEREQUEST
RMSG
RECEIVE
MBOX
J
RMSG
WRITEUNIT
RMSG
J
SEND
FINISHED
RMSG
VOID
CONTROLLER
WHILE
TRUE
IF
COUNT
IF
EMPTY
FINISHED
RECEIVE
FINISHED
MSG
COUNT
ELSE
IF
EMPTY
WRITEREQUEST
RECEIVE
WRITEREQUEST
MSG
MSG
ID
COUNT
COUNT
ELSE
IF
EMPTY
READREQUEST
RECEIVE
READREQUEST
MSG
COUNT
SEND
MSG
ID
OK
IF
COUNT
SEND
OK
RECEIVE
FINISHED
MSG
COUNT
WHILE
COUNT
RECEIVE
FINISHED
MSG
COUNT
FIGURE
A
SOLUTION
TO
THE
READERS
WRITERS
PROBLEM
USING
MESSAGE
PASSING
IF
COUNT
THEN
A
WRITER
HAS
MADE
A
REQUEST
AND
IS
BEING
MADE
TO
WAIT
TO
CLEAR
ALL
ACTIVE
READERS
THEREFORE
ONLY
FINISHED
MESSAGES
SHOULD
BE
SERVICED
SUMMARY
THE
CENTRAL
THEMES
OF
MODERN
OPERATING
SYSTEMS
ARE
MULTIPROGRAMMING
MULTIPRO
CESSING
AND
DISTRIBUTED
PROCESSING
FUNDAMENTAL
TO
THESE
THEMES
AND
FUNDAMEN
TAL
TO
THE
TECHNOLOGY
OF
OS
DESIGN
IS
CONCURRENCY
WHEN
MULTIPLE
PROCESSES
ARE
EXECUTING
CONCURRENTLY
EITHER
ACTUALLY
IN
THE
CASE
OF
A
MULTIPROCESSOR
SYSTEM
OR
VIR
TUALLY
IN
THE
CASE
OF
A
SINGLE
PROCESSOR
MULTIPROGRAMMING
SYSTEM
ISSUES
OF
CONFLICT
RESOLUTION
AND
COOPERATION
ARISE
CONCURRENT
PROCESSES
MAY
INTERACT
IN
A
NUMBER
OF
WAYS
PROCESSES
THAT
ARE
UNAWARE
OF
EACH
OTHER
MAY
NEVERTHELESS
COMPETE
FOR
RESOURCES
SUCH
AS
PROCESSOR
TIME
OR
ACCESS
TO
I
O
DEVICES
PROCESSES
MAY
BE
INDIRECTLY
AWARE
OF
ONE
ANOTHER
BECAUSE
THEY
SHARE
ACCESS
TO
A
COMMON
OBJECT
SUCH
AS
A
BLOCK
OF
MAIN
MEMORY
OR
A
FILE
FINALLY
PROCESSES
MAY
BE
DIRECTLY
AWARE
OF
EACH
OTHER
AND
COOPERATE
BY
THE
EXCHANGE
OF
INFORMATION
THE
KEY
ISSUES
THAT
ARISE
IN
THESE
INTERACTIONS
ARE
MUTUAL
EXCLUSION
AND
DEADLOCK
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
MUTUAL
EXCLUSION
IS
A
CONDITION
IN
WHICH
THERE
IS
A
SET
OF
CONCURRENT
PROCESSES
ONLY
ONE
OF
WHICH
IS
ABLE
TO
ACCESS
A
GIVEN
RESOURCE
OR
PERFORM
A
GIVEN
FUNCTION
AT
ANY
TIME
MUTUAL
EXCLUSION
TECHNIQUES
CAN
BE
USED
TO
RESOLVE
CONFLICTS
SUCH
AS
COMPETITION
FOR
RESOURCES
AND
TO
SYNCHRONIZE
PROCESSES
SO
THAT
THEY
CAN
COOP
ERATE
AN
EXAMPLE
OF
THE
LATTER
IS
THE
PRODUCER
CONSUMER
MODEL
IN
WHICH
ONE
PROCESS
IS
PUTTING
DATA
INTO
A
BUFFER
AND
ONE
OR
MORE
PROCESSES
ARE
EXTRACTING
DATA
FROM
THAT
BUFFER
ONE
APPROACH
TO
SUPPORTING
MUTUAL
EXCLUSION
INVOLVES
THE
USE
OF
SPECIAL
PUR
POSE
MACHINE
INSTRUCTIONS
THIS
APPROACH
REDUCES
OVERHEAD
BUT
IS
STILL
INEFFICIENT
BECAUSE
IT
USES
BUSY
WAITING
ANOTHER
APPROACH
TO
SUPPORTING
MUTUAL
EXCLUSION
IS
TO
PROVIDE
FEATURES
WITHIN
THE
OS
TWO
OF
THE
MOST
COMMON
TECHNIQUES
ARE
SEMAPHORES
AND
MESSAGE
FACILI
TIES
SEMAPHORES
ARE
USED
FOR
SIGNALING
AMONG
PROCESSES
AND
CAN
BE
READILY
USED
TO
ENFORCE
A
MUTUAL
EXCLUSION
DISCIPLINE
MESSAGES
ARE
USEFUL
FOR
THE
ENFORCEMENT
OF
MUTUAL
EXCLUSION
AND
ALSO
PROVIDE
AN
EFFECTIVE
MEANS
OF
INTERPROCESS
COMMUNICATION
RECOMMENDED
READING
THE
MISNAMED
LITTLE
BOOK
OF
SEMAPHORES
PAGES
PROVIDES
NUMER
OUS
EXAMPLES
OF
THE
USES
OF
SEMAPHORES
AVAILABLE
FREE
ONLINE
SURVEYS
MANY
OF
THE
MECHANISMS
DESCRIBED
IN
THIS
CHAPTER
PROVIDES
A
VERY
CLEAR
AND
EVEN
ENTERTAINING
DISCUSSION
OF
CONCURRENCY
MUTUAL
EXCLUSION
SEMAPHORES
AND
OTHER
RELATED
TOPICS
A
MORE
FORMAL
TREATMENT
EXPANDED
TO
INCLUDE
DISTRIBUTED
SYSTEMS
IS
CONTAINED
IN
IS
ANOTHER
READABLE
AND
USEFUL
TREATMENT
IT
ALSO
CONTAINS
A
NUMBER
OF
PROBLEMS
WITH
WORKED
OUT
SOLUTIONS
IS
A
COMPREHENSIVE
AND
LUCID
COLLECTION
OF
ALGORITHMS
FOR
MUTUAL
EXCLUSION
COVERING
SOFTWARE
E
G
DEKKER
AND
HARDWARE
APPROACHES
AS
WELL
AS
SEMAPHORES
AND
MESSAGES
IS
A
VERY
READABLE
CLASSIC
THAT
PRESENTS
A
FORMAL
APPROACH
TO
DEFINING
SEQUENTIAL
PROCESSES
AND
CONCUR
RENCY
IS
A
LENGTHY
FORMAL
TREATMENT
OF
MUTUAL
EXCLUSION
IS
A
USEFUL
AID
IN
UNDERSTANDING
CONCURRENCY
IS
A
WELL
ORGANIZED
TREATMENT
OF
CONCURRENCY
PROVIDES
A
GOOD
PRACTICAL
INTRODUCTION
TO
PROGRAMMING
USING
CONCURRENCY
IS
AN
EXHAUSTIVE
SURVEY
OF
MONITORS
IS
AN
INSTRUCTIVE
ANALYSIS
OF
DIFFERENT
SCHEDULING
POLICIES
FOR
THE
READERS
WRITERS
PROBLEM
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
KEY
TERMS
ATOMIC
CRITICAL
RESOURCE
NONBLOCKING
BINARY
SEMAPHORE
CRITICAL
SECTION
RACE
CONDITION
BLOCKING
DEADLOCK
SEMAPHORE
BUSY
WAITING
GENERAL
SEMAPHORE
SPIN
WAITING
CONCURRENCY
MESSAGE
PASSING
STARVATION
CONCURRENT
PROCESSES
MONITOR
STRONG
SEMAPHORE
COROUTINE
MUTUAL
EXCLUSION
WEAK
SEMAPHORE
COUNTING
SEMAPHORE
MUTEX
REVIEW
QUESTIONS
LIST
FOUR
DESIGN
ISSUES
FOR
WHICH
THE
CONCEPT
OF
CONCURRENCY
IS
RELEVANT
WHAT
ARE
THREE
CONTEXTS
IN
WHICH
CONCURRENCY
ARISES
WHAT
IS
THE
BASIC
REQUIREMENT
FOR
THE
EXECUTION
OF
CONCURRENT
PROCESSES
LIST
THREE
DEGREES
OF
AWARENESS
BETWEEN
PROCESSES
AND
BRIEFLY
DEFINE
EACH
WHAT
IS
THE
DISTINCTION
BETWEEN
COMPETING
PROCESSES
AND
COOPERATING
PROCESSES
LIST
THE
THREE
CONTROL
PROBLEMS
ASSOCIATED
WITH
COMPETING
PROCESSES
AND
BRIEFLY
DE
FINE
EACH
LIST
THE
REQUIREMENTS
FOR
MUTUAL
EXCLUSION
WHAT
OPERATIONS
CAN
BE
PERFORMED
ON
A
SEMAPHORE
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
WHAT
IS
THE
DIFFERENCE
BETWEEN
BINARY
AND
GENERAL
SEMAPHORES
WHAT
IS
THE
DIFFERENCE
BETWEEN
STRONG
AND
WEAK
SEMAPHORES
WHAT
IS
A
MONITOR
WHAT
IS
THE
DISTINCTION
BETWEEN
BLOCKING
AND
NONBLOCKING
WITH
RESPECT
TO
MESSAGES
WHAT
CONDITIONS
ARE
GENERALLY
ASSOCIATED
WITH
THE
READERS
WRITERS
PROBLEM
PROBLEMS
AT
THE
BEGINNING
OF
SECTION
IT
IS
STATED
THAT
MULTIPROGRAMMING
AND
MULTIPROCESS
ING
PRESENT
THE
SAME
PROBLEMS
WITH
RESPECT
TO
CONCURRENCY
THIS
IS
TRUE
AS
FAR
AS
IT
GOES
HOWEVER
CITE
TWO
DIFFERENCES
IN
TERMS
OF
CONCURRENCY
BETWEEN
MULTIPROGRAM
MING
AND
MULTIPROCESSING
PROCESSES
AND
THREADS
PROVIDE
A
POWERFUL
STRUCTURING
TOOL
FOR
IMPLEMENTING
PROGRAMS
THAT
WOULD
BE
MUCH
MORE
COMPLEX
AS
SIMPLE
SEQUENTIAL
PROGRAMS
AN
EARLIER
CON
STRUCT
THAT
IS
INSTRUCTIVE
TO
EXAMINE
IS
THE
COROUTINE
THE
PURPOSE
OF
THIS
PROBLEM
IS
TO
INTRODUCE
COROUTINES
AND
COMPARE
THEM
TO
PROCESSES
CONSIDER
THIS
SIMPLE
PROBLEM
FROM
READ
COLUMN
CARDS
AND
PRINT
THEM
ON
CHARACTER
LINES
WITH
THE
FOLLOWING
CHANGES
AFTER
EVERY
CARD
IMAGE
AN
EXTRA
BLANK
IS
INSERTED
AND
EVERY
ADJACENT
PAIR
OF
ASTERISKS
ON
A
CARD
IS
REPLACED
BY
THE
CHARACTER
A
DEVELOP
A
SOLUTION
TO
THIS
PROBLEM
AS
AN
ORDINARY
SEQUENTIAL
PROGRAM
YOU
WILL
FIND
THAT
THE
PROGRAM
IS
TRICKY
TO
WRITE
THE
INTERACTIONS
AMONG
THE
VARIOUS
ELEMENTS
OF
THE
PROGRAM
ARE
UNEVEN
BECAUSE
OF
THE
CONVERSION
FROM
A
LENGTH
OF
TO
FURTHERMORE
THE
LENGTH
OF
THE
CARD
IMAGE
AFTER
CONVERSION
WILL
VARY
DEPENDING
ON
THE
NUMBER
OF
DOUBLE
ASTERISK
OCCURRENCES
ONE
WAY
TO
IMPROVE
CLARITY
AND
TO
MINIMIZE
THE
POTENTIAL
FOR
BUGS
IS
TO
WRITE
THE
APPLICATION
AS
THREE
SEPARATE
PROCEDURES
THE
FIRST
PROCEDURE
READS
IN
CARD
IMAGES
PADS
EACH
IMAGE
WITH
A
BLANK
AND
WRITES
A
STREAM
OF
CHARACTERS
TO
A
TEMPORARY
FILE
AFTER
ALL
OF
THE
CARDS
HAVE
BEEN
READ
THE
SECOND
PROCEDURE
READS
THE
TEMPORARY
FILE
DOES
THE
CHARACTER
SUBSTITUTION
AND
WRITES
OUT
A
SECOND
TEMPORARY
FILE
THE
THIRD
PROCEDURE
READS
THE
STREAM
OF
CHARACTERS
FROM
THE
SECOND
TEMPORARY
FILE
AND
PRINTS
LINES
OF
CHARACTERS
EACH
B
THE
SEQUENTIAL
SOLUTION
IS
UNATTRACTIVE
BECAUSE
OF
THE
OVERHEAD
OF
I
O
AND
TEMPO
RARY
FILES
CONWAY
PROPOSED
A
NEW
FORM
OF
PROGRAM
STRUCTURE
THE
COROUTINE
THAT
ALLOWS
THE
APPLICATION
TO
BE
WRITTEN
AS
THREE
PROGRAMS
CONNECTED
BY
ONE
CHARACTER
BUFFERS
FIGURE
IN
A
TRADITIONAL
PROCEDURE
THERE
IS
A
MASTER
SLAVE
RELATION
SHIP
BETWEEN
THE
CALLED
AND
CALLING
PROCEDURE
THE
CALLING
PROCEDURE
MAY
EXECUTE
A
CALL
FROM
ANY
POINT
IN
THE
PROCEDURE
THE
CALLED
PROCEDURE
IS
BEGUN
AT
ITS
ENTRY
POINT
AND
RETURNS
TO
THE
CALLING
PROCEDURE
AT
THE
POINT
OF
CALL
THE
COROUTINE
EXHIB
ITS
A
MORE
SYMMETRIC
RELATIONSHIP
AS
EACH
CALL
IS
MADE
EXECUTION
TAKES
UP
FROM
THE
LAST
ACTIVE
POINT
IN
THE
CALLED
PROCEDURE
BECAUSE
THERE
IS
NO
SENSE
IN
WHICH
A
CALLING
PROCEDURE
IS
HIGHER
THAN
THE
CALLED
THERE
IS
NO
RETURN
RATHER
ANY
CO
ROUTINE
CAN
PASS
CONTROL
TO
ANY
OTHER
COROUTINE
WITH
A
RESUME
COMMAND
THE
FIRST
TIME
A
COROUTINE
IS
INVOKED
IT
IS
RESUMED
AT
ITS
ENTRY
POINT
SUBSEQUENTLY
THE
CO
ROUTINE
IS
REACTIVATED
AT
THE
POINT
OF
ITS
OWN
LAST
RESUME
COMMAND
NOTE
THAT
ONLY
ONE
COROUTINE
IN
A
PROGRAM
CAN
BE
IN
EXECUTION
AT
ONE
TIME
AND
THAT
THE
TRANSITION
POINTS
ARE
EXPLICITLY
DEFINED
IN
THE
CODE
SO
THIS
IS
NOT
AN
EXAMPLE
OF
CONCURRENT
PROCESSING
EXPLAIN
THE
OPERATION
OF
THE
PROGRAM
IN
FIGURE
C
THE
PROGRAM
DOES
NOT
ADDRESS
THE
TERMINATION
CONDITION
ASSUME
THAT
THE
I
O
ROUTINE
READCARD
RETURNS
THE
VALUE
TRUE
IF
IT
HAS
PLACED
AN
CHARACTER
IMAGE
IN
INBUF
OTHERWISE
IT
RETURNS
FALSE
MODIFY
THE
PROGRAM
TO
INCLUDE
THIS
CONTINGENCY
NOTE
THAT
THE
LAST
PRINTED
LINE
MAY
THEREFORE
CONTAIN
LESS
THAN
CHARACTERS
D
REWRITE
THE
SOLUTION
AS
A
SET
OF
THREE
PROCESSES
USING
SEMAPHORES
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
CHAR
RS
SP
CHAR
INBUF
OUTBUF
VOID
READ
WHILE
TRUE
READCARD
INBUF
FOR
INT
I
I
I
RS
INBUF
I
RESUME
SQUASH
RS
RESUME
SQUASH
VOID
PRINT
WHILE
TRUE
FOR
INT
J
J
J
OUTBUF
J
SP
RESUME
SQUASH
OUTPUT
OUTBUF
VOID
SQUASH
WHILE
TRUE
IF
RS
SP
RS
RESUME
PRINT
ELSE
RESUME
READ
IF
RS
SP
RESUME
PRINT
ELSE
SP
RESUME
PRINT
SP
RS
RESUME
PRINT
RESUME
READ
FIGURE
AN
APPLICATION
OF
COROUTINES
CONSIDER
THE
FOLLOWING
PROGRAM
SHARED
INT
X
SHARED
INT
X
X
X
WHILE
WHILE
X
X
X
X
X
X
X
X
IF
X
IF
X
PRINTF
X
IS
D
X
PRINTF
X
IS
D
X
NOTE
THAT
THE
SCHEDULER
IN
A
UNIPROCESSOR
SYSTEM
WOULD
IMPLEMENT
PSEUDO
PARALLEL
EXECUTION
OF
THESE
TWO
CONCURRENT
PROCESSES
BY
INTERLEAVING
THEIR
INSTRUCTIONS
WITHOUT
RESTRICTION
ON
THE
ORDER
OF
THE
INTERLEAVING
A
SHOW
A
SEQUENCE
I
E
TRACE
THE
SEQUENCE
OF
INTERLEAVINGS
OF
STATEMENTS
SUCH
THAT
THE
STATEMENT
X
IS
IS
PRINTED
B
SHOW
A
SEQUENCE
SUCH
THAT
THE
STATEMENT
X
IS
IS
PRINTED
YOU
SHOULD
REMEMBER
THAT
THE
INCREMENT
DECREMENTS
AT
THE
SOURCE
LANGUAGE
LEVEL
ARE
NOT
DONE
ATOMI
CALLY
THAT
IS
THE
ASSEMBLY
LANGUAGE
CODE
LD
X
LOAD
FROM
MEMORY
LOCATION
X
INCR
INCREMENT
STO
X
STORE
THE
INCREMENTED
VALUE
BACK
IN
X
IMPLEMENTS
THE
SINGLE
C
INCREMENT
INSTRUCTION
X
X
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
CONSIDER
THE
FOLLOWING
PROGRAM
CONST
INT
N
INT
TALLY
VOID
TOTAL
INT
COUNT
FOR
COUNT
COUNT
N
COUNT
TALLY
VOID
MAIN
TALLY
PARBEGIN
TOTAL
TOTAL
WRITE
TALLY
A
DETERMINE
THE
PROPER
LOWER
BOUND
AND
UPPER
BOUND
ON
THE
FINAL
VALUE
OF
THE
SHARED
VARIABLE
TALLY
OUTPUT
BY
THIS
CONCURRENT
PROGRAM
ASSUME
PROCESSES
CAN
EXECUTE
AT
ANY
RELATIVE
SPEED
AND
THAT
A
VALUE
CAN
ONLY
BE
INCREMENTED
AFTER
IT
HAS
BEEN
LOADED
INTO
A
REGISTER
BY
A
SEPARATE
MACHINE
INSTRUCTION
B
SUPPOSE
THAT
AN
ARBITRARY
NUMBER
OF
THESE
PROCESSES
ARE
PERMITTED
TO
EXECUTE
IN
PARALLEL
UNDER
THE
ASSUMPTIONS
OF
PART
A
WHAT
EFFECT
WILL
THIS
MODIFICATION
HAVE
ON
THE
RANGE
OF
FINAL
VALUES
OF
TALLY
IS
BUSY
WAITING
ALWAYS
LESS
EFFICIENT
IN
TERMS
OF
USING
PROCESSOR
TIME
THAN
A
BLOCKING
WAIT
EXPLAIN
CONSIDER
THE
FOLLOWING
PROGRAM
BOOLEAN
BLOCKED
INT
TURN
VOID
P
INT
ID
WHILE
TRUE
BLOCKED
ID
TRUE
WHILE
TURN
ID
WHILE
BLOCKED
ID
DO
NOTHING
TURN
ID
CRITICAL
SECTION
BLOCKED
ID
FALSE
REMAINDER
VOID
MAIN
BLOCKED
FALSE
BLOCKED
FALSE
TURN
PARBEGIN
P
P
THIS
SOFTWARE
SOLUTION
TO
THE
MUTUAL
EXCLUSION
PROBLEM
FOR
TWO
PROCESSES
IS
PROPOSED
IN
FIND
A
COUNTEREXAMPLE
THAT
DEMONSTRATES
THAT
THIS
SOLUTION
IS
INCOR
RECT
IT
IS
INTERESTING
TO
NOTE
THAT
EVEN
THE
COMMUNICATIONS
OF
THE
ACM
WAS
FOOLED
ON
THIS
ONE
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
A
SOFTWARE
APPROACH
TO
MUTUAL
EXCLUSION
IS
LAMPORT
BAKERY
ALGORITHM
SO
CALLED
BECAUSE
IT
IS
BASED
ON
THE
PRACTICE
IN
BAKERIES
AND
OTHER
SHOPS
IN
WHICH
EVERY
CUSTOMER
RECEIVES
A
NUMBERED
TICKET
ON
ARRIVAL
ALLOWING
EACH
TO
BE
SERVED
IN
TURN
THE
ALGORITHM
IS
AS
FOLLOWS
BOOLEAN
CHOOSING
N
INT
NUMBER
N
WHILE
TRUE
CHOOSING
I
TRUE
NUMBER
I
GETMAX
NUMBER
N
CHOOSING
I
FALSE
FOR
INT
J
J
N
J
WHILE
CHOOSING
J
WHILE
NUMBER
J
NUMBER
J
J
NUMBER
I
I
CRITICAL
SECTION
NUMBER
I
REMAINDER
THE
ARRAYS
CHOOSING
AND
NUMBER
ARE
INITIALIZED
TO
FALSE
AND
RESPECTIVELY
THE
ITH
ELEMENT
OF
EACH
ARRAY
MAY
BE
READ
AND
WRITTEN
BY
PROCESS
I
BUT
ONLY
READ
BY
OTHER
PROCESSES
THE
NOTATION
A
B
C
D
IS
DEFINED
AS
A
C
OR
A
C
AND
B
D
A
DESCRIBE
THE
ALGORITHM
IN
WORDS
B
SHOW
THAT
THIS
ALGORITHM
AVOIDS
DEADLOCK
C
SHOW
THAT
IT
ENFORCES
MUTUAL
EXCLUSION
NOW
CONSIDER
A
VERSION
OF
THE
BAKERY
ALGORITHM
WITHOUT
THE
VARIABLE
CHOOSING
THEN
WE
HAVE
INT
NUMBER
N
WHILE
TRUE
NUMBER
I
GETMAX
NUMBER
N
FOR
INT
J
J
N
J
WHILE
NUMBER
J
NUMBER
J
J
NUMBER
I
I
CRITICAL
SECTION
NUMBER
I
REMAINDER
DOES
THIS
VERSION
VIOLATE
MUTUAL
EXCLUSION
EXPLAIN
WHY
OR
WHY
NOT
CONSIDER
THE
FOLLOWING
PROGRAM
WHICH
PROVIDES
A
SOFTWARE
APPROACH
TO
MUTUAL
EXCLUSION
INTEGER
ARRAY
CONTROL
N
INTEGER
K
WHERE
K
N
AND
EACH
ELEMENT
OF
CONTROL
IS
EITHER
OR
ALL
ELEMENTS
OF
CONTROL
ARE
INITIALLY
ZERO
THE
INITIAL
VALUE
OF
K
IS
IMMATERIAL
THE
PROGRAM
OF
THE
ITH
PROCESS
I
N
IS
BEGIN
INTEGER
J
CONTROL
I
L
LI
FOR
J
K
STEP
L
UNTIL
N
L
STEP
L
UNTIL
K
DO
BEGIN
IF
J
I
THEN
GOTO
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
IF
CONTROL
J
THEN
GOTO
END
CONTROL
I
FOR
J
STEP
UNTIL
N
DO
IF
J
I
AND
CONTROL
J
THEN
GOTO
IF
CONTROL
K
AND
K
I
THEN
GOTO
K
I
CRITICAL
SECTION
FOR
J
K
STEP
UNTIL
N
STEP
UNTIL
K
DO
IF
J
K
AND
CONTROL
J
THEN
BEGIN
K
J
GOTO
END
CONTROL
I
REMAINDER
OF
CYCLE
GOTO
END
THIS
IS
REFERRED
TO
AS
THE
EISENBERG
MCGUIRE
ALGORITHM
EXPLAIN
ITS
OPERATION
AND
ITS
KEY
FEATURES
CONSIDER
THE
FIRST
INSTANCE
OF
THE
STATEMENT
BOLT
IN
FIGURE
A
ACHIEVE
THE
SAME
RESULT
USING
THE
EXCHANGE
INSTRUCTION
B
WHICH
METHOD
IS
PREFERABLE
WHEN
A
SPECIAL
MACHINE
INSTRUCTION
IS
USED
TO
PROVIDE
MUTUAL
EXCLUSION
IN
THE
FASH
ION
OF
FIGURE
THERE
IS
NO
CONTROL
OVER
HOW
LONG
A
PROCESS
MUST
WAIT
BEFORE
BEING
GRANTED
ACCESS
TO
ITS
CRITICAL
SECTION
DEVISE
AN
ALGORITHM
THAT
USES
THE
COMPARE
SWAP
INSTRUCTION
BUT
THAT
GUARANTEES
THAT
ANY
PROCESS
WAITING
TO
ENTER
ITS
CRITICAL
SECTION
WILL
DO
SO
WITHIN
N
TURNS
WHERE
N
IS
THE
NUMBER
OF
PROCESSES
THAT
MAY
REQUIRE
ACCESS
TO
THE
CRITICAL
SECTION
AND
A
TURN
IS
AN
EVENT
CONSISTING
OF
ONE
PROCESS
LEAVING
THE
CRITICAL
SECTION
AND
ANOTHER
PROCESS
BEING
GRANTED
ACCESS
CONSIDER
THE
FOLLOWING
DEFINITION
OF
SEMAPHORES
VOID
SEMWAIT
IF
COUNT
COUNT
ELSE
PLACE
THIS
PROCESS
IN
QUEUE
BLOCK
VOID
SEMSIGNAL
IF
THERE
IS
AT
LEAST
ONE
PROCESS
BLOCKED
ON
SEMAPHORE
REMOVE
A
PROCESS
P
FROM
QUEUE
PLACE
PROCESS
P
ON
READY
LIST
ELSE
COUNT
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
COMPARE
THIS
SET
OF
DEFINITIONS
WITH
THAT
OF
FIGURE
NOTE
ONE
DIFFERENCE
WITH
THE
PRECEDING
DEFINITION
A
SEMAPHORE
CAN
NEVER
TAKE
ON
A
NEGATIVE
VALUE
IS
THERE
ANY
DIFFERENCE
IN
THE
EFFECT
OF
THE
TWO
SETS
OF
DEFINITIONS
WHEN
USED
IN
PROGRAMS
THAT
IS
COULD
YOU
SUBSTITUTE
ONE
SET
FOR
THE
OTHER
WITHOUT
ALTERING
THE
MEANING
OF
THE
PROGRAM
CONSIDER
A
SHARABLE
RESOURCE
WITH
THE
FOLLOWING
CHARACTERISTICS
AS
LONG
AS
THERE
ARE
FEWER
THAN
THREE
PROCESSES
USING
THE
RESOURCE
NEW
PROCESSES
CAN
START
USING
IT
RIGHT
AWAY
ONCE
THERE
ARE
THREE
PROCESS
USING
THE
RESOURCE
ALL
THREE
MUST
LEAVE
BEFORE
ANY
NEW
PROCESSES
CAN
BEGIN
USING
IT
WE
REALIZE
THAT
COUNTERS
ARE
NEEDED
TO
KEEP
TRACK
OF
HOW
MANY
PROCESSES
ARE
WAITING
AND
ACTIVE
AND
THAT
THESE
COUNTERS
ARE
THEMSELVES
SHARED
RESOURCES
THAT
MUST
BE
PROTECTED
WITH
MUTUAL
EXCLUSION
SO
WE
MIGHT
CREATE
THE
FOLLOWING
SOLUTION
SEMAPHORE
MUTEX
BLOCK
SHARE
VARIABLES
SEMAPHORES
INT
ACTIVE
WAITING
COUNTERS
AND
BOOLEAN
FALSE
STATE
INFORMATION
SEMWAIT
MUTEX
ENTER
THE
MUTUAL
EXCLUSION
IF
IF
THERE
ARE
OR
WERE
THEN
WAITING
WE
MUST
WAIT
BUT
WE
MUST
LEAVE
SEMSIGNAL
MUTEX
THE
MUTUAL
EXCLUSION
FIRST
SEMWAIT
BLOCK
WAIT
FOR
ALL
CURRENT
USERS
TO
DEPART
SEMWAIT
MUTEX
REENTER
THE
MUTUAL
EXCLUSION
WAITING
AND
UPDATE
THE
WAITING
COUNT
ACTIVE
UPDATE
ACTIVE
COUNT
AND
REMEMBER
ACTIVE
IF
THE
COUNT
REACHED
SEMSIGNAL
MUTEX
LEAVE
THE
MUTUAL
EXCLUSION
CRITICAL
SECTION
SEMWAIT
MUTEX
ENTER
MUTUAL
EXCLUSION
ACTIVE
AND
UPDATE
THE
ACTIVE
COUNT
IF
ACTIVE
LAST
ONE
TO
LEAVE
INT
N
IF
WAITING
N
WAITING
ELSE
N
IF
SO
UNBLOCK
UP
TO
WHILE
N
WAITING
PROCESSES
SEMSIGNAL
BLOCK
N
FALSE
ALL
ACTIVE
PROCESSES
HAVE
LEFT
SEMSIGNAL
MUTEX
LEAVE
THE
MUTUAL
EXCLUSION
THE
SOLUTION
APPEARS
TO
DO
EVERYTHING
RIGHT
ALL
ACCESSES
TO
THE
SHARED
VARIABLES
ARE
PROTECTED
BY
MUTUAL
EXCLUSION
PROCESSES
DO
NOT
BLOCK
THEMSELVES
WHILE
IN
THE
MUTUAL
EXCLUSION
NEW
PROCESSES
ARE
PREVENTED
FROM
USING
THE
RESOURCE
IF
THERE
ARE
OR
WERE
THREE
ACTIVE
USERS
AND
THE
LAST
PROCESS
TO
DEPART
UNBLOCKS
UP
TO
THREE
WAITING
PROCESSES
A
THE
PROGRAM
IS
NEVERTHELESS
INCORRECT
EXPLAIN
WHY
B
SUPPOSE
WE
CHANGE
THE
IF
IN
LINE
TO
A
WHILE
DOES
THIS
SOLVE
ANY
PROBLEM
IN
THE
PROGRAM
DO
ANY
DIFFICULTIES
REMAIN
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
NOW
CONSIDER
THIS
CORRECT
SOLUTION
TO
THE
PRECEDING
PROBLEM
SEMAPHORE
MUTEX
BLOCK
SHARE
VARIABLES
SEMAPHORES
INT
ACTIVE
WAITING
COUNTERS
AND
BOOLEAN
FALSE
STATE
INFORMATION
SEMWAIT
MUTEX
ENTER
THE
MUTUAL
EXCLUSION
IF
IF
THERE
ARE
OR
WERE
THEN
WAITING
WE
MUST
WAIT
BUT
WE
MUST
LEAVE
SEMSIGNAL
MUTEX
THE
MUTUAL
EXCLUSION
FIRST
SEMWAIT
BLOCK
WAIT
FOR
ALL
CURRENT
USERS
TO
DEPART
ELSE
ACTIVE
UPDATE
ACTIVE
COUNT
AND
ACTIVE
REMEMBER
IF
THE
COUNT
REACHED
SEMSIGNAL
MUTEX
LEAVE
MUTUAL
EXCLUSION
CRITICAL
SECTION
SEMWAIT
MUTEX
ENTER
MUTUAL
EXCLUSION
ACTIVE
AND
UPDATE
THE
ACTIVE
COUNT
IF
ACTIVE
LAST
ONE
TO
LEAVE
INT
N
IF
WAITING
N
WAITING
ELSE
N
IF
SO
SEE
HOW
MANY
PROCESSES
TO
UNBLOCK
WAITING
N
DEDUCT
THIS
NUMBER
FROM
WAITING
COUNT
ACTIVE
N
AND
SET
ACTIVE
TO
THIS
NUMBER
WHILE
N
NOW
UNBLOCK
THE
PROCESSES
SEMSIGNAL
BLOCK
ONE
BY
ONE
N
ACTIVE
REMEMBER
IF
THE
COUNT
IS
SEMSIGNAL
MUTEX
LEAVE
THE
MUTUAL
EXCLUSION
A
EXPLAIN
HOW
THIS
PROGRAM
WORKS
AND
WHY
IT
IS
CORRECT
B
THIS
SOLUTION
DOES
NOT
COMPLETELY
PREVENT
NEWLY
ARRIVING
PROCESSES
FROM
CUTTING
IN
LINE
BUT
IT
DOES
MAKE
IT
LESS
LIKELY
GIVE
AN
EXAMPLE
OF
CUTTING
IN
LINE
C
THIS
PROGRAM
IS
AN
EXAMPLE
OF
A
GENERAL
DESIGN
PATTERN
THAT
IS
A
UNIFORM
WAY
TO
IMPLEMENT
SOLUTIONS
TO
MANY
CONCURRENCY
PROBLEMS
USING
SEMAPHORES
IT
HAS
BEEN
REFERRED
TO
AS
THE
I
LL
DO
IT
FOR
YOU
PATTERN
DESCRIBE
THE
PATTERN
NOW
CONSIDER
ANOTHER
CORRECT
SOLUTION
TO
THE
PRECEDING
PROBLEM
SEMAPHORE
MUTEX
BLOCK
SHARE
VARIABLES
SEMAPHORES
INT
ACTIVE
WAITING
COUNTERS
AND
BOOLEAN
FALSE
STATE
INFORMATION
SEMWAIT
MUTEX
ENTER
THE
MUTUAL
EXCLUSION
IF
IF
THERE
ARE
OR
WERE
THEN
WAITING
WE
MUST
WAIT
BUT
WE
MUST
LEAVE
SEMSIGNAL
MUTEX
THE
MUTUAL
EXCLUSION
FIRST
SEMWAIT
BLOCK
WAIT
FOR
ALL
CURRENT
USERS
TO
DEPART
WAITING
WE
VE
GOT
THE
MUTUAL
EXCLUSION
UPDATE
COUNT
ACTIVE
UPDATE
ACTIVE
COUNT
AND
REMEMBER
ACTIVE
IF
THE
COUNT
REACHED
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
IF
WAITING
IF
THERE
ARE
OTHERS
WAITING
SEMSIGNAL
BLOCK
AND
WE
DON
T
YET
HAVE
ACTIVE
UNBLOCK
A
WAITING
PROCESS
ELSE
SEMSIGNAL
MUTEX
OTHERWISE
OPEN
THE
MUTUAL
EXCLUSION
CRITICAL
SECTION
SEMWAIT
MUTEX
ENTER
MUTUAL
EXCLUSION
ACTIVE
AND
UPDATE
THE
ACTIVE
COUNT
IF
ACTIVE
IF
LAST
ONE
TO
LEAVE
FALSE
SET
UP
TO
LET
NEW
PROCESSES
ENTER
IF
WAITING
IF
THERE
ARE
OTHERS
WAITING
SEMSIGNAL
BLOCK
AND
WE
DON
T
HAVE
ACTIVE
UNBLOCK
A
WAITING
PROCESS
ELSE
SEMSIGNAL
MUTEX
OTHERWISE
OPEN
THE
MUTUAL
EXCLUSION
A
EXPLAIN
HOW
THIS
PROGRAM
WORKS
AND
WHY
IT
IS
CORRECT
B
DOES
THIS
SOLUTION
DIFFER
FROM
THE
PRECEDING
ONE
IN
TERMS
OF
THE
NUMBER
OF
PRO
CESSES
THAT
CAN
BE
UNBLOCKED
AT
A
TIME
EXPLAIN
C
THIS
PROGRAM
IS
AN
EXAMPLE
OF
A
GENERAL
DESIGN
PATTERN
THAT
IS
A
UNIFORM
WAY
TO
IMPLEMENT
SOLUTIONS
TO
MANY
CONCURRENCY
PROBLEMS
USING
SEMAPHORES
IT
HAS
BEEN
REFERRED
TO
AS
THE
PASS
THE
BATON
PATTERN
DESCRIBE
THE
PATTERN
IT
SHOULD
BE
POSSIBLE
TO
IMPLEMENT
GENERAL
SEMAPHORES
USING
BINARY
SEMAPHORES
WE
CAN
USE
THE
OPERATIONS
SEMWAITB
AND
SEMSIGNALB
AND
TWO
BINARY
SEMAPHORES
DELAY
AND
MUTEX
CONSIDER
THE
FOLLOWING
VOID
SEMWAIT
SEMAPHORE
SEMWAITB
MUTEX
IF
SEMSIGNALB
MUTEX
SEMWAITB
DELAY
ELSE
SEMSIGNALB
MUTEX
VOID
SEMSIGNAL
SEMAPHORE
SEMWAITB
MUTEX
IF
SEMSIGNALB
DELAY
SEMSIGNALB
MUTEX
INITIALLY
IS
SET
TO
THE
DESIRED
SEMAPHORE
VALUE
EACH
SEMWAIT
OPERATION
DECREMENTS
AND
EACH
SEMSIGNAL
OPERATION
INCREMENTS
THE
BINARY
SEMAPHORE
MUTEX
WHICH
IS
INITIALIZED
TO
ASSURES
THAT
THERE
IS
MUTUAL
EXCLUSION
FOR
THE
UPDATING
OF
THE
BI
NARY
SEMAPHORE
DELAY
WHICH
IS
INITIALIZED
TO
IS
USED
TO
BLOCK
PROCESSES
THERE
IS
A
FLAW
IN
THE
PRECEDING
PROGRAM
DEMONSTRATE
THE
FLAW
AND
PROPOSE
A
CHANGE
THAT
WILL
FIX
IT
HINT
SUPPOSE
TWO
PROCESSES
EACH
CALL
SEMWAIT
WHEN
IS
INITIALLY
AND
AFTER
THE
FIRST
HAS
JUST
PERFORMED
SEMSIGNALB
MUTEX
BUT
NOT
PER
FORMED
SEMWAITB
DELAY
THE
SECOND
CALL
TO
SEMWAIT
PROCEEDS
TO
THE
SAME
POINT
ALL
THAT
YOU
NEED
TO
DO
IS
MOVE
A
SINGLE
LINE
OF
THE
PROGRAM
IN
DIJKSTRA
PUT
FORWARD
THE
CONJECTURE
THAT
THERE
WAS
NO
SOLUTION
TO
THE
MUTUAL
EXCLUSION
PROBLEM
AVOIDING
STARVATION
APPLICABLE
TO
AN
UNKNOWN
BUT
FINITE
NUMBER
OF
PROCESSES
USING
A
FINITE
NUMBER
OF
WEAK
SEMAPHORES
IN
J
M
MORRIS
REFUTED
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
THIS
CONJECTURE
BY
PUBLISHING
AN
ALGORITHM
USING
THREE
WEAK
SEMAPHORES
THE
BEHAVIOR
OF
THE
ALGORITHM
CAN
BE
DESCRIBED
AS
FOLLOWS
IF
ONE
OR
SEVERAL
PROCESS
ARE
WAITING
IN
A
SEMWAIT
S
OPERATION
AND
ANOTHER
PROCESS
IS
EXECUTING
SEMSIGNAL
S
THE
VALUE
OF
THE
SEMAPHORE
S
IS
NOT
MODIFIED
AND
ONE
OF
THE
WAITING
PROCESSES
IS
UNBLOCKED
INDE
PENDENTLY
OF
SEMWAIT
S
APART
FROM
THE
THREE
SEMAPHORES
THE
ALGORITHM
USES
TWO
NONNEGATIVE
INTEGER
VARIABLES
AS
COUNTERS
OF
THE
NUMBER
OF
PROCESSES
IN
CERTAIN
SECTIONS
OF
THE
ALGORITHM
THUS
SEMAPHORES
A
AND
B
ARE
INITIALIZED
TO
WHILE
SEMAPHORE
M
AND
COUNTERS
NA
AND
NM
ARE
INITIALIZED
TO
THE
MUTUAL
EXCLUSION
SEMAPHORE
B
PRO
TECTS
ACCESS
TO
THE
SHARED
VARIABLE
NA
A
PROCESS
ATTEMPTING
TO
ENTER
ITS
CRITICAL
SECTION
MUST
CROSS
TWO
BARRIERS
REPRESENTED
BY
SEMAPHORES
A
AND
M
COUNTERS
NA
AND
NM
RESPECTIVELY
CONTAIN
THE
NUMBER
OF
PROCESSES
READY
TO
CROSS
BARRIER
A
AND
THOSE
HAVING
ALREADY
CROSSED
BARRIER
A
BUT
NOT
YET
BARRIER
M
IN
THE
SECOND
PART
OF
THE
PROTOCOL
THE
NM
PROCESSES
BLOCKED
AT
M
WILL
ENTER
THEIR
CRITICAL
SECTIONS
ONE
BY
ONE
USING
A
CASCADE
TECHNIQUE
SIMILAR
TO
THAT
USED
IN
THE
FIRST
PART
DEFINE
AN
ALGORITHM
THAT
CONFORMS
TO
THIS
DESCRIPTION
THE
FOLLOWING
PROBLEM
WAS
ONCE
USED
ON
AN
EXAM
JURASSIC
PARK
CONSISTS
OF
A
DINOSAUR
MUSEUM
AND
A
PARK
FOR
SAFARI
RIDING
THERE
ARE
M
PASSENGERS
AND
N
SINGLE
PASSENGER
CARS
PASSENGERS
WANDER
AROUND
THE
MUSEUM
FOR
A
WHILE
THEN
LINE
UP
TO
TAKE
A
RIDE
IN
A
SAFARI
CAR
WHEN
A
CAR
IS
AVAILABLE
IT
LOADS
THE
ONE
PASSENGER
IT
CAN
HOLD
AND
RIDES
AROUND
THE
PARK
FOR
A
RANDOM
AMOUNT
OF
TIME
IF
THE
N
CARS
ARE
ALL
OUT
RIDING
PASSENGERS
AROUND
THEN
A
PASSENGER
WHO
WANTS
TO
RIDE
WAITS
IF
A
CAR
IS
READY
TO
LOAD
BUT
THERE
ARE
NO
WAITING
PASSENGERS
THEN
THE
CAR
WAITS
USE
SEMAPHORES
TO
SYNCHRONIZE
THE
M
PASSENGER
PROCESSES
AND
THE
N
CAR
PROCESSES
THE
FOLLOWING
SKELETON
CODE
WAS
FOUND
ON
A
SCRAP
OF
PAPER
ON
THE
FLOOR
OF
THE
EXAM
ROOM
GRADE
IT
FOR
CORRECTNESS
IGNORE
SYNTAX
AND
MISSING
VARIABLE
DECLARATIONS
REMEMBER
THAT
P
AND
V
CORRESPOND
TO
SEMWAIT
AND
SEMSIGNAL
RESOURCE
SEM
PROCESS
PASSENGER
I
TO
DO
TRUE
NAP
INT
RANDOM
P
V
P
P
OD
END
PASSENGER
PROCESS
CAR
J
TO
DO
TRUE
V
P
V
NAP
INT
RANDOM
V
OD
END
CAR
END
IN
THE
COMMENTARY
ON
FIGURE
AND
TABLE
IT
WAS
STATED
THAT
IT
WOULD
NOT
DO
SIMPLY
TO
MOVE
THE
CONDITIONAL
STATEMENT
INSIDE
THE
CRITICAL
SECTION
CONTROLLED
BY
OF
THE
CONSUMER
BECAUSE
THIS
COULD
LEAD
TO
DEADLOCK
DEMONSTRATE
THIS
WITH
A
TABLE
SIMILAR
TO
TABLE
CONSIDER
THE
SOLUTION
TO
THE
INFINITE
BUFFER
PRODUCER
CONSUMER
PROBLEM
DEFINED
IN
FIGURE
SUPPOSE
WE
HAVE
THE
COMMON
CASE
IN
WHICH
THE
PRODUCER
AND
CONSUMER
ARE
RUNNING
AT
ROUGHLY
THE
SAME
SPEED
THE
SCENARIO
COULD
BE
PRODUCER
APPEND
SEMSIGNAL
PRODUCE
APPEND
SEMSIGNAL
PRODUCE
CONSUMER
CONSUME
TAKE
SEMWAIT
CONSUME
TAKE
SEMWAIT
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
THE
PRODUCER
ALWAYS
MANAGES
TO
APPEND
A
NEW
ELEMENT
TO
THE
BUFFER
AND
SIGNAL
DURING
THE
CONSUMPTION
OF
THE
PREVIOUS
ELEMENT
BY
THE
CONSUMER
THE
PRODUCER
IS
ALWAYS
APPENDING
TO
AN
EMPTY
BUFFER
AND
THE
CONSUMER
IS
ALWAYS
TAKING
THE
SOLE
ITEM
IN
THE
BUFFER
ALTHOUGH
THE
CONSUMER
NEVER
BLOCKS
ON
THE
SEMAPHORE
A
LARGE
NUMBER
OF
CALLS
TO
THE
SEMAPHORE
MECHANISM
IS
MADE
CREATING
CONSIDERABLE
OVERHEAD
CONSTRUCT
A
NEW
PROGRAM
THAT
WILL
BE
MORE
EFFICIENT
UNDER
THESE
CIRCUMSTANCES
HINTS
ALLOW
N
TO
HAVE
THE
VALUE
WHICH
IS
TO
MEAN
THAT
NOT
ONLY
IS
THE
BUFFER
EMPTY
BUT
THAT
THE
CONSUMER
HAS
DETECTED
THIS
FACT
AND
IS
GOING
TO
BLOCK
UNTIL
THE
PRODUCER
SUP
PLIES
FRESH
DATA
THE
SOLUTION
DOES
NOT
REQUIRE
THE
USE
OF
THE
LOCAL
VARIABLE
M
FOUND
IN
FIGURE
CONSIDER
FIGURE
WOULD
THE
MEANING
OF
THE
PROGRAM
CHANGE
IF
THE
FOLLOWING
WERE
INTERCHANGED
A
SEMWAIT
E
SEMWAIT
B
SEMSIGNAL
SEMSIGNAL
N
C
SEMWAIT
N
SEMWAIT
D
SEMSIGNAL
SEMSIGNAL
E
THE
FOLLOWING
PSEUDOCODE
IS
A
CORRECT
IMPLEMENTATION
OF
THE
PRODUCER
CONSUMER
PROBLEM
WITH
A
BOUNDED
BUFFER
ITEM
BUFFER
INITIALLY
EMPTY
SEMAPHORE
EMPTY
INITIALIZED
TO
SEMAPHORE
FULL
INITIALIZED
TO
MUTEX
INITIALIZED
TO
VOID
PRODUCER
WHILE
TRUE
ITEM
PRODUCE
WAIT
EMPTY
WAIT
MUTEX
APPEND
ITEM
SIGNAL
MUTEX
SIGNAL
FULL
VOID
CONSUMER
WHILE
TRUE
WAIT
FULL
WAIT
MUTEX
ITEM
TAKE
SIGNAL
MUTEX
SIGNAL
EMPTY
CONSUME
ITEM
LABELS
AND
REFER
TO
THE
LINES
OF
CODE
SHOWN
ABOVE
AND
EACH
COVER
THREE
LINES
OF
CODE
SEMAPHORES
EMPTY
AND
FULL
ARE
LINEAR
SEMAPHORES
THAT
CAN
TAKE
UNBOUNDED
NEGATIVE
AND
POSITIVE
VALUES
THERE
ARE
MULTIPLE
PRODUCER
PROCESSES
REFERRED
TO
AS
PA
PB
PC
ETC
AND
MULTIPLE
CONSUMER
PROCESSES
REFERRED
TO
AS
CA
CB
CC
ETC
EACH
SEMAPHORE
MAINTAINS
A
FIFO
FIRST
IN
FIRST
OUT
QUEUE
OF
BLOCKED
PRO
CESSES
IN
THE
SCHEDULING
CHART
BELOW
EACH
LINE
REPRESENTS
THE
STATE
OF
THE
BUFFER
AND
SEMAPHORES
AFTER
THE
SCHEDULED
EXECUTION
HAS
OCCURRED
TO
SIMPLIFY
WE
ASSUME
THAT
SCHEDULING
IS
SUCH
THAT
PROCESSES
ARE
NEVER
INTERRUPTED
WHILE
EXECUTING
A
GIVEN
POR
TION
OF
CODE
OR
OR
YOUR
TASK
IS
TO
COMPLETE
THE
FOLLOWING
CHART
SCHEDULED
STEP
OF
EXECUTION
FULL
STATE
AND
QUEUE
BUFFER
EMPTY
STATE
AND
QUEUE
INITIALIZATION
FULL
OOO
EMPTY
CA
EXECUTES
FULL
CA
OOO
EMPTY
CB
EXECUTES
FULL
CA
CB
OOO
EMPTY
CHAPTER
CONCURRENCY
MUTUAL
EXCLUSION
AND
SYNCHRONIZATION
SCHEDULED
STEP
OF
EXECUTION
FULL
STATE
AND
QUEUE
BUFFER
EMPTY
STATE
AND
QUEUE
PA
EXECUTES
FULL
CA
CB
OOO
EMPTY
PA
EXECUTES
FULL
CA
CB
X
OO
EMPTY
PA
EXECUTES
FULL
CB
CA
X
OO
EMPTY
CA
EXECUTES
FULL
CB
OOO
EMPTY
CA
EXECUTES
FULL
CB
OOO
EMPTY
PB
EXECUTES
FULL
EMPTY
PA
EXECUTES
FULL
EMPTY
PA
EXECUTES
FULL
EMPTY
PB
EXECUTES
FULL
EMPTY
PB
EXECUTES
FULL
EMPTY
PC
EXECUTES
FULL
EMPTY
CB
EXECUTES
FULL
EMPTY
PC
EXECUTES
FULL
EMPTY
CB
EXECUTES
FULL
EMPTY
PA
EXECUTES
FULL
EMPTY
PB
EXECUTES
FULL
EMPTY
PC
EXECUTES
FULL
EMPTY
PA
EXECUTES
FULL
EMPTY
PD
EXECUTES
FULL
EMPTY
CA
EXECUTES
FULL
EMPTY
PA
EXECUTES
FULL
EMPTY
CC
EXECUTES
FULL
EMPTY
PA
EXECUTES
FULL
EMPTY
CC
EXECUTES
FULL
EMPTY
PD
EXECUTES
FULL
EMPTY
THIS
PROBLEM
DEMONSTRATES
THE
USE
OF
SEMAPHORES
TO
COORDINATE
THREE
TYPES
OF
PRO
CESSES
SANTA
CLAUS
SLEEPS
IN
HIS
SHOP
AT
THE
NORTH
POLE
AND
CAN
ONLY
BE
WAKENED
BY
EITHER
ALL
NINE
REINDEER
BEING
BACK
FROM
THEIR
VACATION
IN
THE
SOUTH
PACIFIC
OR
SOME
OF
THE
ELVES
HAVING
DIFFICULTIES
MAKING
TOYS
TO
ALLOW
SANTA
TO
GET
SOME
SLEEP
THE
ELVES
CAN
ONLY
WAKE
HIM
WHEN
THREE
OF
THEM
HAVE
PROBLEMS
WHEN
THREE
ELVES
ARE
HAVING
THEIR
PROBLEMS
SOLVED
ANY
OTHER
ELVES
WISHING
TO
VISIT
SANTA
MUST
WAIT
FOR
THOSE
ELVES
TO
RETURN
IF
SANTA
WAKES
UP
TO
FIND
THREE
ELVES
WAITING
AT
HIS
SHOP
DOOR
ALONG
WITH
THE
LAST
REINDEER
HAVING
COME
BACK
FROM
THE
TROPICS
SANTA
HAS
DECIDED
THAT
THE
ELVES
CAN
WAIT
UNTIL
AFTER
CHRISTMAS
BECAUSE
IT
IS
MORE
IMPORTANT
TO
GET
HIS
SLEIGH
READY
IT
IS
ASSUMED
THAT
THE
REINDEER
DO
NOT
WANT
TO
LEAVE
THE
TROPICS
AND
THEREFORE
THEY
STAY
THERE
UNTIL
THE
LAST
POSSIBLE
MOMENT
THE
LAST
REINDEER
TO
ARRIVE
MUST
GET
SANTA
WHILE
THE
OTHERS
WAIT
IN
A
WARMING
HUT
BEFORE
BEING
HARNESSED
TO
THE
SLEIGH
SOLVE
THIS
PROBLEM
USING
SEMAPHORES
SHOW
THAT
MESSAGE
PASSING
AND
SEMAPHORES
HAVE
EQUIVALENT
FUNCTIONALITY
BY
A
IMPLEMENTING
MESSAGE
PASSING
USING
SEMAPHORES
HINT
MAKE
USE
OF
A
SHARED
BUFFER
AREA
TO
HOLD
MAILBOXES
EACH
ONE
CONSISTING
OF
AN
ARRAY
OF
MESSAGE
SLOTS
B
IMPLEMENTING
A
SEMAPHORE
USING
MESSAGE
PASSING
HINT
INTRODUCE
A
SEPARATE
SYNCHRONIZATION
PROCESS
AM
GRATEFUL
TO
JOHN
TRONO
OF
ST
MICHAEL
COLLEGE
IN
VERMONT
FOR
SUPPLYING
THIS
PROBLEM
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
EXPLAIN
WHAT
IS
THE
PROBLEM
WITH
THIS
IMPLEMENTATION
OF
THE
ONE
WRITER
MANY
READERS
PROBLEM
INT
READCOUNT
SHARED
AND
INITIALIZED
TO
SEMAPHORE
MUTEX
WRT
SHARED
AND
INITIALIZED
TO
WRITER
READERS
SEMWAIT
MUTEX
READCOUNT
READCOUNT
SEMWAIT
WRT
IF
READCOUNT
THEN
SEMWAIT
WRT
WRITING
PERFORMED
SEMSIGNAL
MUTEX
SEMSIGNAL
WRT
READING
PERFORMED
SEMWAIT
MUTEX
READCOUNT
READCOUNT
IF
READCOUNT
THEN
UP
WRT
SEMSIGNAL
MUTEX
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
PRINCIPLES
OF
DEADLOCK
REUSABLE
RESOURCES
CONSUMABLE
RESOURCES
RESOURCE
ALLOCATION
GRAPHS
THE
CONDITIONS
FOR
DEADLOCK
DEADLOCK
PREVENTION
MUTUAL
EXCLUSION
HOLD
AND
WAIT
NO
PREEMPTION
CIRCULAR
WAIT
DEADLOCK
AVOIDANCE
PROCESS
INITIATION
DENIAL
RESOURCE
ALLOCATION
DENIAL
DEADLOCK
DETECTION
DEADLOCK
DETECTION
ALGORITHM
RECOVERY
AN
INTEGRATED
DEADLOCK
STRATEGY
DINING
PHILOSOPHERS
PROBLEM
SOLUTION
USING
SEMAPHORES
SOLUTION
USING
A
MONITOR
UNIX
CONCURRENCY
MECHANISMS
LINUX
KERNEL
CONCURRENCY
MECHANISMS
SOLARIS
THREAD
SYNCHRONIZATION
PRIMITIVES
WINDOWS
CONCURRENCY
MECHANISMS
SUMMARY
RECOMMENDED
READING
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
PRINCIPLES
OF
DEADLOCK
WHEN
TWO
TRAINS
APPROACH
EACH
OTHER
AT
A
CROSSING
BOTH
SHALL
COME
TO
A
FULL
STOP
AND
NEITHER
SHALL
START
UP
AGAIN
UNTIL
THE
OTHER
HAS
GONE
STATUTE
PASSED
BY
THE
KANSAS
STATE
LEGISLATURE
EARLY
IN
THE
CENTURY
A
TREASURY
OF
RAILROAD
FOLKLORE
B
A
BOTKIN
AND
ALVIN
F
HARLOW
THIS
CHAPTER
EXAMINES
TWO
PROBLEMS
THAT
PLAGUE
ALL
EFFORTS
TO
SUPPORT
CONCURRENT
PROCESSING
DEADLOCK
AND
STARVATION
WE
BEGIN
WITH
A
DISCUSSION
OF
THE
UNDERLYING
PRINCIPLES
OF
DEADLOCK
AND
THE
RELATED
PROBLEM
OF
STARVATION
THEN
WE
EXAMINE
THE
THREE
COMMON
APPROACHES
TO
DEALING
WITH
DEADLOCK
PREVENTION
DETECTION
AND
AVOIDANCE
WE
THEN
LOOK
AT
ONE
OF
THE
CLASSIC
PROBLEMS
USED
TO
ILLUSTRATE
BOTH
SYNCHRONIZATION
AND
DEADLOCK
ISSUES
THE
DINING
PHILOSOPHERS
PROBLEM
AS
WITH
CHAPTER
THE
DISCUSSION
IN
THIS
CHAPTER
IS
LIMITED
TO
A
CONSIDERATION
OF
CONCURRENCY
AND
DEADLOCK
ON
A
SINGLE
SYSTEM
MEASURES
TO
DEAL
WITH
DISTRIBUTED
DEADLOCK
PROBLEMS
ARE
ASSESSED
IN
CHAPTER
AN
ANIMATION
ILLUSTRATING
DEADLOCK
IS
AVAILABLE
ONLINE
CLICK
ON
THE
ROTATING
GLOBE
AT
WILLIAMSTALLINGS
COM
OS
HTML
FOR
ACCESS
PRINCIPLES
OF
DEADLOCK
DEADLOCK
CAN
BE
DEFINED
AS
THE
PERMANENT
BLOCKING
OF
A
SET
OF
PROCESSES
THAT
EITHER
COMPETE
FOR
SYSTEM
RESOURCES
OR
COMMUNICATE
WITH
EACH
OTHER
A
SET
OF
PROCESSES
IS
DEADLOCKED
WHEN
EACH
PROCESS
IN
THE
SET
IS
BLOCKED
AWAITING
AN
EVENT
TYPICALLY
THE
FREEING
UP
OF
SOME
REQUESTED
RESOURCE
THAT
CAN
ONLY
BE
TRIGGERED
BY
ANOTHER
BLOCKED
PROCESS
IN
THE
SET
DEADLOCK
IS
PERMANENT
BECAUSE
NONE
OF
THE
EVENTS
IS
EVER
TRIGGERED
UNLIKE
OTHER
PROBLEMS
IN
CONCURRENT
PROCESS
MANAGEMENT
THERE
IS
NO
EFFICIENT
SOLUTION
IN
THE
GENERAL
CASE
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
C
B
D
A
A
DEADLOCK
POSSIBLE
B
DEADLOCK
FIGURE
ILLUSTRATION
OF
DEADLOCK
ALL
DEADLOCKS
INVOLVE
CONFLICTING
NEEDS
FOR
RESOURCES
BY
TWO
OR
MORE
PROC
ESSES
A
COMMON
EXAMPLE
IS
THE
TRAFFIC
DEADLOCK
FIGURE
SHOWS
A
SITUATION
IN
WHICH
FOUR
CARS
HAVE
ARRIVED
AT
A
FOUR
WAY
STOP
INTERSECTION
AT
APPROXIMATELY
THE
SAME
TIME
THE
FOUR
QUADRANTS
OF
THE
INTERSECTION
ARE
THE
RESOURCES
OVER
WHICH
CON
TROL
IS
NEEDED
IN
PARTICULAR
IF
ALL
FOUR
CARS
WISH
TO
GO
STRAIGHT
THROUGH
THE
INTERSEC
TION
THE
RESOURCE
REQUIREMENTS
ARE
AS
FOLLOWS
CAR
TRAVELING
NORTH
NEEDS
QUADRANTS
A
AND
B
CAR
NEEDS
QUADRANTS
B
AND
C
CAR
NEEDS
QUADRANTS
C
AND
D
CAR
NEEDS
QUADRANTS
D
AND
A
THE
RULE
OF
THE
ROAD
IN
THE
UNITED
STATES
IS
THAT
A
CAR
AT
A
FOUR
WAY
STOP
SHOULD
DEFER
TO
A
CAR
IMMEDIATELY
TO
ITS
RIGHT
THIS
RULE
WORKS
IF
THERE
ARE
ONLY
TWO
OR
THREE
CARS
AT
THE
INTERSECTION
FOR
EXAMPLE
IF
ONLY
THE
NORTHBOUND
AND
WESTBOUND
CARS
ARRIVE
AT
THE
INTERSECTION
THE
NORTHBOUND
CAR
WILL
WAIT
AND
THE
WESTBOUND
CAR
PRO
CEEDS
HOWEVER
IF
ALL
FOUR
CARS
ARRIVE
AT
ABOUT
THE
SAME
TIME
AND
ALL
FOUR
FOLLOW
THE
RULE
EACH
WILL
REFRAIN
FROM
ENTERING
THE
INTERSECTION
THIS
CAUSES
A
POTENTIAL
DEADLOCK
IT
IS
ONLY
A
POTENTIAL
DEADLOCK
BECAUSE
THE
NECESSARY
RESOURCES
ARE
AVAILABLE
FOR
ANY
OF
THE
CARS
TO
PROCEED
IF
ONE
CAR
EVENTUALLY
CHOOSES
TO
PROCEED
IT
CAN
DO
SO
HOWEVER
IF
ALL
FOUR
CARS
IGNORE
THE
RULES
AND
PROCEED
CAUTIOUSLY
INTO
THE
INTERSECTION
AT
THE
SAME
TIME
THEN
EACH
CAR
SEIZES
ONE
RESOURCE
ONE
QUADRANT
BUT
CANNOT
PROCEED
BECAUSE
THE
REQUIRED
SECOND
RESOURCE
HAS
ALREADY
BEEN
SEIZED
BY
ANOTHER
CAR
THIS
IS
AN
ACTUAL
DEADLOCK
LET
US
NOW
LOOK
AT
A
DEPICTION
OF
DEADLOCK
INVOLVING
PROCESSES
AND
COM
PUTER
RESOURCES
FIGURE
BASED
ON
ONE
IN
WHICH
WE
REFER
TO
AS
A
JOINT
PROGRESS
DIAGRAM
ILLUSTRATES
THE
PROGRESS
OF
TWO
PROCESSES
COMPETING
FOR
TWO
PRINCIPLES
OF
DEADLOCK
PROGRESS
OF
Q
A
REQUIRED
RELEASE
A
RELEASE
B
B
REQUIRED
GET
A
GET
B
BOTH
P
AND
Q
WANT
RESOURCE
A
BOTH
P
AND
Q
WANT
RESOURCE
B
DEADLOCK
INEVITABLE
REGION
GET
A
GET
B
A
REQUIRED
RELEASE
A
RELEASE
B
B
REQUIRED
PROGRESS
OF
P
POSSIBLE
PROGRESS
PATH
OF
P
AND
Q
HORIZONTAL
PORTION
OF
PATH
INDICATES
P
IS
EXECUTING
AND
Q
IS
WAITING
VERTICAL
PORTION
OF
PATH
INDICATES
Q
IS
EXECUTING
AND
P
IS
WAITING
FIGURE
EXAMPLE
OF
DEADLOCK
RESOURCES
EACH
PROCESS
NEEDS
EXCLUSIVE
USE
OF
BOTH
RESOURCES
FOR
A
CERTAIN
PERIOD
OF
TIME
TWO
PROCESSES
P
AND
Q
HAVE
THE
FOLLOWING
GENERAL
FORM
PROCESS
P
PROCESS
Q
GET
A
GET
B
GET
B
GET
A
RELEASE
A
RELEASE
B
RELEASE
B
RELEASE
A
IN
FIGURE
THE
X
AXIS
REPRESENTS
PROGRESS
IN
THE
EXECUTION
OF
P
AND
THE
Y
AXIS
REPRESENTS
PROGRESS
IN
THE
EXECUTION
OF
Q
THE
JOINT
PROGRESS
OF
THE
TWO
PROCESSES
IS
THEREFORE
REPRESENTED
BY
A
PATH
THAT
PROGRESSES
FROM
THE
ORIGIN
IN
A
NORTHEASTERLY
DIRECTION
FOR
A
UNIPROCESSOR
SYSTEM
ONLY
ONE
PROCESS
AT
A
TIME
MAY
EXECUTE
AND
THE
PATH
CONSISTS
OF
ALTERNATING
HORIZONTAL
AND
VERTICAL
SEGMENTS
WITH
A
HORIZONTAL
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
SEGMENT
REPRESENTING
A
PERIOD
WHEN
P
EXECUTES
AND
Q
WAITS
AND
A
VERTICAL
SEGMENT
REPRESENTING
A
PERIOD
WHEN
Q
EXECUTES
AND
P
WAITS
THE
FIGURE
INDICATES
AREAS
IN
WHICH
BOTH
P
AND
Q
REQUIRE
RESOURCE
A
UPWARD
SLANTED
LINES
BOTH
P
AND
Q
REQUIRE
RESOURCE
B
DOWNWARD
SLANTED
LINES
AND
BOTH
P
AND
Q
REQUIRE
BOTH
RESOURCES
BECAUSE
WE
ASSUME
THAT
EACH
PROCESS
REQUIRES
EXCLUSIVE
CONTROL
OF
ANY
RESOURCE
THESE
ARE
ALL
FORBIDDEN
REGIONS
THAT
IS
IT
IS
IMPOSSIBLE
FOR
ANY
PATH
REPRESENTING
THE
JOINT
EXECUTION
PROGRESS
OF
P
AND
Q
TO
ENTER
THESE
REGIONS
THE
FIGURE
SHOWS
SIX
DIFFERENT
EXECUTION
PATHS
THESE
CAN
BE
SUMMARIZED
AS
FOLLOWS
Q
ACQUIRES
B
AND
THEN
A
AND
THEN
RELEASES
B
AND
A
WHEN
P
RESUMES
EXECUTION
IT
WILL
BE
ABLE
TO
ACQUIRE
BOTH
RESOURCES
Q
ACQUIRES
B
AND
THEN
A
P
EXECUTES
AND
BLOCKS
ON
A
REQUEST
FOR
A
Q
RELEASES
B
AND
A
WHEN
P
RESUMES
EXECUTION
IT
WILL
BE
ABLE
TO
ACQUIRE
BOTH
RESOURCES
Q
ACQUIRES
B
AND
THEN
P
ACQUIRES
A
DEADLOCK
IS
INEVITABLE
BECAUSE
AS
EXECUTION
PROCEEDS
Q
WILL
BLOCK
ON
A
AND
P
WILL
BLOCK
ON
B
P
ACQUIRES
A
AND
THEN
Q
ACQUIRES
B
DEADLOCK
IS
INEVITABLE
BECAUSE
AS
EXECU
TION
PROCEEDS
Q
WILL
BLOCK
ON
A
AND
P
WILL
BLOCK
ON
B
P
ACQUIRES
A
AND
THEN
B
Q
EXECUTES
AND
BLOCKS
ON
A
REQUEST
FOR
B
P
RELEASES
A
AND
B
WHEN
Q
RESUMES
EXECUTION
IT
WILL
BE
ABLE
TO
ACQUIRE
BOTH
RESOURCES
P
ACQUIRES
A
AND
THEN
B
AND
THEN
RELEASES
A
AND
B
WHEN
Q
RESUMES
EXECUTION
IT
WILL
BE
ABLE
TO
ACQUIRE
BOTH
RESOURCES
THE
GRAY
SHADED
AREA
OF
FIGURE
WHICH
CAN
BE
REFERRED
TO
AS
A
FATAL
REGION
APPLIES
TO
THE
COMMENTARY
ON
PATHS
AND
IF
AN
EXECUTION
PATH
ENTERS
THIS
FATAL
REGION
THEN
DEADLOCK
IS
INEVITABLE
NOTE
THAT
THE
EXISTENCE
OF
A
FATAL
REGION
DEPENDS
ON
THE
LOGIC
OF
THE
TWO
PROCESSES
HOWEVER
DEADLOCK
IS
ONLY
INEVITABLE
IF
THE
JOINT
PROGRESS
OF
THE
TWO
PROCESSES
CREATES
A
PATH
THAT
ENTERS
THE
FATAL
REGION
WHETHER
OR
NOT
DEADLOCK
OCCURS
DEPENDS
ON
BOTH
THE
DYNAMICS
OF
THE
EXECU
TION
AND
ON
THE
DETAILS
OF
THE
APPLICATION
FOR
EXAMPLE
SUPPOSE
THAT
P
DOES
NOT
NEED
BOTH
RESOURCES
AT
THE
SAME
TIME
SO
THAT
THE
TWO
PROCESSES
HAVE
THE
FOLLOWING
FORM
PROCESS
P
PROCESS
Q
GET
A
GET
B
RELEASE
A
GET
A
GET
B
RELEASE
B
RELEASE
B
RELEASE
A
THIS
SITUATION
IS
REFLECTED
IN
FIGURE
SOME
THOUGHT
SHOULD
CONVINCE
YOU
THAT
REGARDLESS
OF
THE
RELATIVE
TIMING
OF
THE
TWO
PROCESSES
DEADLOCK
CANNOT
OCCUR
AS
SHOWN
THE
JOINT
PROGRESS
DIAGRAM
CAN
BE
USED
TO
RECORD
THE
EXECUTION
HIS
TORY
OF
TWO
PROCESSES
THAT
SHARE
RESOURCES
IN
CASES
WHERE
MORE
THAN
TWO
PROCESSES
PRINCIPLES
OF
DEADLOCK
PROGRESS
OF
Q
A
REQUIRED
B
REQUIRED
RELEASE
A
RELEASE
B
GET
A
GET
B
GET
A
RELEASE
A
GET
B
RELEASE
B
PROGRESS
OF
P
BOTH
P
AND
Q
WANT
RESOURCE
A
BOTH
P
AND
Q
WANT
RESOURCE
B
A
REQUIRED
B
REQUIRED
POSSIBLE
PROGRESS
PATH
OF
P
AND
Q
HORIZONTAL
PORTION
OF
PATH
INDICATES
P
IS
EXECUTING
AND
Q
IS
WAITING
VERTICAL
PORTION
OF
PATH
INDICATES
Q
IS
EXECUTING
AND
P
IS
WAITING
FIGURE
EXAMPLE
OF
NO
DEADLOCK
MAY
COMPETE
FOR
THE
SAME
RESOURCE
A
HIGHER
DIMENSIONAL
DIAGRAM
WOULD
BE
REQUIRED
THE
PRINCIPLES
CONCERNING
FATAL
REGIONS
AND
DEADLOCK
WOULD
REMAIN
THE
SAME
REUSABLE
RESOURCES
TWO
GENERAL
CATEGORIES
OF
RESOURCES
CAN
BE
DISTINGUISHED
REUSABLE
AND
CONSUMABLE
A
REUSABLE
RESOURCE
IS
ONE
THAT
CAN
BE
SAFELY
USED
BY
ONLY
ONE
PROCESS
AT
A
TIME
AND
IS
NOT
DEPLETED
BY
THAT
USE
PROCESSES
OBTAIN
RESOURCE
UNITS
THAT
THEY
LATER
RELEASE
FOR
REUSE
BY
OTHER
PROCESSES
EXAMPLES
OF
REUSABLE
RESOURCES
INCLUDE
PROCESSORS
I
O
CHANNELS
MAIN
AND
SECONDARY
MEMORY
DEVICES
AND
DATA
STRUCTURES
SUCH
AS
FILES
DATABASES
AND
SEMAPHORES
AS
AN
EXAMPLE
OF
DEADLOCK
INVOLVING
REUSABLE
RESOURCES
CONSIDER
TWO
PROCESSES
THAT
COMPETE
FOR
EXCLUSIVE
ACCESS
TO
A
DISK
FILE
D
AND
A
TAPE
DRIVE
T
THE
PROGRAMS
ENGAGE
IN
THE
OPERATIONS
DEPICTED
IN
FIGURE
DEADLOCK
OCCURS
IF
EACH
PROCESS
HOLDS
ONE
RESOURCE
AND
REQUESTS
THE
OTHER
FOR
EXAMPLE
DEADLOCK
OCCURS
IF
THE
MULTIPROGRAMMING
SYSTEM
INTERLEAVES
THE
EXECUTION
OF
THE
TWO
PROCESSES
AS
FOLLOWS
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
STEP
PROCESS
P
ACTION
STEP
PROCESS
Q
ACTION
FIGURE
EXAMPLE
OF
TWO
PROCESSES
COMPETING
FOR
REUSABLE
RESOURCES
IT
MAY
APPEAR
THAT
THIS
IS
A
PROGRAMMING
ERROR
RATHER
THAN
A
PROBLEM
FOR
THE
OS
DESIGNER
HOWEVER
WE
HAVE
SEEN
THAT
CONCURRENT
PROGRAM
DESIGN
IS
CHALLENGING
SUCH
DEADLOCKS
DO
OCCUR
AND
THE
CAUSE
IS
OFTEN
EMBEDDED
IN
COMPLEX
PROGRAM
LOGIC
MAKING
DETECTION
DIFFICULT
ONE
STRATEGY
FOR
DEALING
WITH
SUCH
A
DEADLOCK
IS
TO
IMPOSE
SYSTEM
DESIGN
CONSTRAINTS
CONCERNING
THE
ORDER
IN
WHICH
RESOURCES
CAN
BE
REQUESTED
ANOTHER
EXAMPLE
OF
DEADLOCK
WITH
A
REUSABLE
RESOURCE
HAS
TO
DO
WITH
REQUESTS
FOR
MAIN
MEMORY
SUPPOSE
THE
SPACE
AVAILABLE
FOR
ALLOCATION
IS
KBYTES
AND
THE
FOLLOWING
SEQUENCE
OF
REQUESTS
OCCURS
DEADLOCK
OCCURS
IF
BOTH
PROCESSES
PROGRESS
TO
THEIR
SECOND
REQUEST
IF
THE
AMOUNT
OF
MEMORY
TO
BE
REQUESTED
IS
NOT
KNOWN
AHEAD
OF
TIME
IT
IS
DIFFICULT
TO
DEAL
WITH
THIS
TYPE
OF
DEADLOCK
BY
MEANS
OF
SYSTEM
DESIGN
CONSTRAINTS
THE
BEST
WAY
TO
DEAL
WITH
THIS
PARTICULAR
PROBLEM
IS
IN
EFFECT
TO
ELIMINATE
THE
POSSIBILITY
BY
USING
VIRTUAL
MEMORY
WHICH
IS
DISCUSSED
IN
CHAPTER
CONSUMABLE
RESOURCES
A
CONSUMABLE
RESOURCE
IS
ONE
THAT
CAN
BE
CREATED
PRODUCED
AND
DESTROYED
CON
SUMED
TYPICALLY
THERE
IS
NO
LIMIT
ON
THE
NUMBER
OF
CONSUMABLE
RESOURCES
OF
A
PARTICULAR
TYPE
AN
UNBLOCKED
PRODUCING
PROCESS
MAY
CREATE
ANY
NUMBER
OF
SUCH
RESOURCES
WHEN
A
RESOURCE
IS
ACQUIRED
BY
A
CONSUMING
PROCESS
THE
RESOURCE
CEASES
TO
EXIST
EXAMPLES
OF
CONSUMABLE
RESOURCES
ARE
INTERRUPTS
SIGNALS
MESSAGES
AND
INFORMATION
IN
I
O
BUFFERS
PRINCIPLES
OF
DEADLOCK
AS
AN
EXAMPLE
OF
DEADLOCK
INVOLVING
CONSUMABLE
RESOURCES
CONSIDER
THE
FOLLOWING
PAIR
OF
PROCESSES
IN
WHICH
EACH
PROCESS
ATTEMPTS
TO
RECEIVE
A
MESSAGE
FROM
THE
OTHER
PROCESS
AND
THEN
SEND
A
MESSAGE
TO
THE
OTHER
PROCESS
DEADLOCK
OCCURS
IF
THE
RECEIVE
IS
BLOCKING
I
E
THE
RECEIVING
PROCESS
IS
BLOCKED
UNTIL
THE
MESSAGE
IS
RECEIVED
ONCE
AGAIN
A
DESIGN
ERROR
IS
THE
CAUSE
OF
THE
DEADLOCK
SUCH
ERRORS
MAY
BE
QUITE
SUBTLE
AND
DIFFICULT
TO
DETECT
FURTHERMORE
IT
MAY
TAKE
A
RARE
COMBINATION
OF
EVENTS
TO
CAUSE
THE
DEADLOCK
THUS
A
PROGRAM
TABLE
SUMMARY
OF
DEADLOCK
DETECTION
PREVENTION
AND
AVOIDANCE
APPROACHES
FOR
OPERATING
SYSTEMS
APPROACH
RESOURCE
ALLOCATION
POLICY
DIFFERENT
SCHEMES
MAJOR
ADVANTAGES
MAJOR
DISADVANTAGES
PREVENTION
CONSERVATIVE
UNDERCOMMITS
RESOURCES
REQUESTING
ALL
RESOURCES
AT
ONCE
WORKS
WELL
FOR
PROCESSES
THAT
PERFORM
A
SINGLE
BURST
OF
ACTIVITY
NO
PREEMPTION
NECESSARY
INEFFICIENT
DELAYS
PROCESS
INITIATION
FUTURE
RESOURCE
REQUIRE
MENTS
MUST
BE
KNOWN
BY
PROCESSES
PREEMPTION
CONVENIENT
WHEN
APPLIED
TO
RESOURCES
WHOSE
STATE
CAN
BE
SAVED
AND
RESTORED
EASILY
PREEMPTS
MORE
OFTEN
THAN
NECESSARY
RESOURCE
ORDERING
FEASIBLE
TO
ENFORCE
VIA
COMPILE
TIME
CHECKS
NEEDS
NO
RUN
TIME
COM
PUTATION
SINCE
PROBLEM
IS
SOLVED
IN
SYSTEM
DESIGN
DISALLOWS
INCREMENTAL
RESOURCE
REQUESTS
AVOIDANCE
MIDWAY
BETWEEN
THAT
OF
DETECTION
AND
PREVENTION
MANIPULATE
TO
FIND
AT
LEAST
ONE
SAFE
PATH
NO
PREEMPTION
NECESSARY
FUTURE
RESOURCE
REQUIRE
MENTS
MUST
BE
KNOWN
BY
OS
PROCESSES
CAN
BE
BLOCKED
FOR
LONG
PERIODS
DETECTION
VERY
LIBERAL
REQUESTED
RESOURCES
ARE
GRANTED
WHERE
POSSIBLE
INVOKE
PERI
ODICALLY
TO
TEST
FOR
DEADLOCK
NEVER
DELAYS
PROCESS
INITIATION
FACILITATES
ONLINE
HANDLING
INHERENT
PREEMPTION
LOSSES
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
COULD
BE
IN
USE
FOR
A
CONSIDERABLE
PERIOD
OF
TIME
EVEN
YEARS
BEFORE
THE
DEADLOCK
ACTUALLY
OCCURS
THERE
IS
NO
SINGLE
EFFECTIVE
STRATEGY
THAT
CAN
DEAL
WITH
ALL
TYPES
OF
DEADLOCK
TABLE
SUMMARIZES
THE
KEY
ELEMENTS
OF
THE
MOST
IMPORTANT
APPROACHES
THAT
HAVE
BEEN
DEVELOPED
PREVENTION
AVOIDANCE
AND
DETECTION
WE
EXAMINE
EACH
OF
THESE
IN
TURN
AFTER
FIRST
INTRODUCING
RESOURCE
ALLOCATION
GRAPHS
AND
THEN
DISCUSSING
THE
CONDITIONS
FOR
DEADLOCK
RESOURCE
ALLOCATION
GRAPHS
A
USEFUL
TOOL
IN
CHARACTERIZING
THE
ALLOCATION
OF
RESOURCES
TO
PROCESSES
IS
THE
RESOURCE
ALLOCATION
GRAPH
INTRODUCED
BY
HOLT
THE
RESOURCE
ALLOCATION
GRAPH
IS
A
DIRECTED
GRAPH
THAT
DEPICTS
A
STATE
OF
THE
SYSTEM
OF
RESOURCES
AND
PRO
CESSES
WITH
EACH
PROCESS
AND
EACH
RESOURCE
REPRESENTED
BY
A
NODE
A
GRAPH
EDGE
DIRECTED
FROM
A
PROCESS
TO
A
RESOURCE
INDICATES
A
RESOURCE
THAT
HAS
BEEN
REQUESTED
BY
THE
PROCESS
BUT
NOT
YET
GRANTED
FIGURE
WITHIN
A
RESOURCE
NODE
A
DOT
IS
SHOWN
FOR
EACH
INSTANCE
OF
THAT
RESOURCE
EXAMPLES
OF
RESOURCE
TYPES
THAT
MAY
HAVE
MULTIPLE
INSTANCES
ARE
I
O
DEVICES
THAT
ARE
ALLOCATED
BY
A
RESOURCE
MANAGEMENT
MODULE
IN
THE
OS
A
GRAPH
EDGE
DIRECTED
FROM
A
REUSABLE
RESOURCE
NODE
DOT
TO
A
PROCESS
INDICATES
A
REQUEST
THAT
HAS
BEEN
GRANTED
FIGURE
THAT
IS
THE
PROCESS
A
RESOURCE
IS
REQUESTED
B
RESOURCE
IS
HELD
C
CIRCULAR
WAIT
FIGURE
EXAMPLES
OF
RESOURCE
ALLOCATION
GRAPHS
D
NO
DEADLOCK
PRINCIPLES
OF
DEADLOCK
FIGURE
RESOURCE
ALLOCATION
GRAPH
FOR
FIGURE
HAS
BEEN
ASSIGNED
ONE
UNIT
OF
THAT
RESOURCE
A
GRAPH
EDGE
DIRECTED
FROM
A
CONSUM
ABLE
RESOURCE
NODE
DOT
TO
A
PROCESS
INDICATES
THAT
THE
PROCESS
IS
THE
PRODUCER
OF
THAT
RESOURCE
FIGURE
SHOWS
AN
EXAMPLE
DEADLOCK
THERE
IS
ONLY
ONE
UNIT
EACH
OF
RESOURCES
RA
AND
RB
PROCESS
HOLDS
RB
AND
REQUESTS
RA
WHILE
HOLDS
RA
BUT
REQUESTS
RB
FIGURE
HAS
THE
SAME
TOPOLOGY
AS
FIGURE
BUT
THERE
IS
NO
DEAD
LOCK
BECAUSE
MULTIPLE
UNITS
OF
EACH
RESOURCE
ARE
AVAILABLE
THE
RESOURCE
ALLOCATION
GRAPH
OF
FIGURE
CORRESPONDS
TO
THE
DEADLOCK
SITUA
TION
IN
FIGURE
NOTE
THAT
IN
THIS
CASE
WE
DO
NOT
HAVE
A
SIMPLE
SITUATION
IN
WHICH
TWO
PROCESSES
EACH
HAVE
ONE
RESOURCE
THE
OTHER
NEEDS
RATHER
IN
THIS
CASE
THERE
IS
A
CIRCULAR
CHAIN
OF
PROCESSES
AND
RESOURCES
THAT
RESULTS
IN
DEADLOCK
THE
CONDITIONS
FOR
DEADLOCK
THREE
CONDITIONS
OF
POLICY
MUST
BE
PRESENT
FOR
A
DEADLOCK
TO
BE
POSSIBLE
MUTUAL
EXCLUSION
ONLY
ONE
PROCESS
MAY
USE
A
RESOURCE
AT
A
TIME
NO
PROCESS
MAY
ACCESS
A
RESOURCE
UNIT
THAT
HAS
BEEN
ALLOCATED
TO
ANOTHER
PROCESS
HOLD
AND
WAIT
A
PROCESS
MAY
HOLD
ALLOCATED
RESOURCES
WHILE
AWAITING
ASSIGN
MENT
OF
OTHER
RESOURCES
NO
PREEMPTION
NO
RESOURCE
CAN
BE
FORCIBLY
REMOVED
FROM
A
PROCESS
HOLDING
IT
IN
MANY
WAYS
THESE
CONDITIONS
ARE
QUITE
DESIRABLE
FOR
EXAMPLE
MUTUAL
EXCLUSION
IS
NEEDED
TO
ENSURE
CONSISTENCY
OF
RESULTS
AND
THE
INTEGRITY
OF
A
DATA
BASE
SIMILARLY
PREEMPTION
SHOULD
NOT
BE
DONE
ARBITRARILY
FOR
EXAMPLE
WHEN
DATA
RESOURCES
ARE
INVOLVED
PREEMPTION
MUST
BE
SUPPORTED
BY
A
ROLLBACK
RECOVERY
MECH
ANISM
WHICH
RESTORES
A
PROCESS
AND
ITS
RESOURCES
TO
A
SUITABLE
PREVIOUS
STATE
FROM
WHICH
THE
PROCESS
CAN
EVENTUALLY
REPEAT
ITS
ACTIONS
THE
FIRST
THREE
CONDITIONS
ARE
NECESSARY
BUT
NOT
SUFFICIENT
FOR
A
DEADLOCK
TO
EXIST
FOR
DEADLOCK
TO
ACTUALLY
TAKE
PLACE
A
FOURTH
CONDITION
IS
REQUIRED
CIRCULAR
WAIT
A
CLOSED
CHAIN
OF
PROCESSES
EXISTS
SUCH
THAT
EACH
PROCESS
HOLDS
AT
LEAST
ONE
RESOURCE
NEEDED
BY
THE
NEXT
PROCESS
IN
THE
CHAIN
E
G
FIGURE
AND
FIGURE
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
THE
FOURTH
CONDITION
IS
ACTUALLY
A
POTENTIAL
CONSEQUENCE
OF
THE
FIRST
THREE
THAT
IS
GIVEN
THAT
THE
FIRST
THREE
CONDITIONS
EXIST
A
SEQUENCE
OF
EVENTS
MAY
OCCUR
THAT
LEAD
TO
AN
UNRESOLVABLE
CIRCULAR
WAIT
THE
UNRESOLVABLE
CIRCULAR
WAIT
IS
IN
FACT
THE
DEFINITION
OF
DEADLOCK
THE
CIRCULAR
WAIT
LISTED
AS
CONDITION
IS
UNRESOLVABLE
BECAUSE
THE
FIRST
THREE
CONDITIONS
HOLD
THUS
THE
FOUR
CONDITIONS
TAKEN
TOGETHER
CONSTITUTE
NECESSARY
AND
SUFFICIENT
CONDITIONS
FOR
DEADLOCK
TO
CLARIFY
THIS
DISCUSSION
IT
IS
USEFUL
TO
RETURN
TO
THE
CONCEPT
OF
THE
JOINT
PROGRESS
DIAGRAM
SUCH
AS
THE
ONE
SHOWN
IN
FIGURE
RECALL
THAT
WE
DEFINED
A
FATAL
REGION
AS
ONE
SUCH
THAT
ONCE
THE
PROCESSES
HAVE
PROGRESSED
INTO
THAT
REGION
THOSE
PROCESSES
WILL
DEADLOCK
A
FATAL
REGION
EXISTS
ONLY
IF
ALL
OF
THE
FIRST
THREE
CON
DITIONS
LISTED
ABOVE
ARE
MET
IF
ONE
OR
MORE
OF
THESE
CONDITIONS
ARE
NOT
MET
THERE
IS
NO
FATAL
REGION
AND
DEADLOCK
CANNOT
OCCUR
THUS
THESE
ARE
NECESSARY
CONDITIONS
FOR
DEADLOCK
FOR
DEADLOCK
TO
OCCUR
THERE
MUST
NOT
ONLY
BE
A
FATAL
REGION
BUT
ALSO
A
SEQUENCE
OF
RESOURCE
REQUESTS
THAT
HAS
LED
INTO
THE
FATAL
REGION
IF
A
CIRCULAR
WAIT
CONDITION
OCCURS
THEN
IN
FACT
THE
FATAL
REGION
HAS
BEEN
ENTERED
THUS
ALL
FOUR
CONDITIONS
LISTED
ABOVE
ARE
SUFFICIENT
FOR
DEADLOCK
TO
SUMMARIZE
THREE
GENERAL
APPROACHES
EXIST
FOR
DEALING
WITH
DEADLOCK
FIRST
ONE
CAN
PREVENT
DEADLOCK
BY
ADOPTING
A
POLICY
THAT
ELIMINATES
ONE
OF
THE
CONDITIONS
CONDITIONS
THROUGH
SECOND
ONE
CAN
AVOID
DEADLOCK
BY
MAKING
THE
APPROPRI
ATE
DYNAMIC
CHOICES
BASED
ON
THE
CURRENT
STATE
OF
RESOURCE
ALLOCATION
THIRD
ONE
CAN
ATTEMPT
TO
DETECT
THE
PRESENCE
OF
DEADLOCK
CONDITIONS
THROUGH
HOLD
AND
TAKE
ACTION
TO
RECOVER
WE
DISCUSS
EACH
OF
THESE
APPROACHES
IN
TURN
DEADLOCK
PREVENTION
THE
STRATEGY
OF
DEADLOCK
PREVENTION
IS
SIMPLY
PUT
TO
DESIGN
A
SYSTEM
IN
SUCH
A
WAY
THAT
THE
POSSIBILITY
OF
DEADLOCK
IS
EXCLUDED
WE
CAN
VIEW
DEADLOCK
PREVENTION
METHODS
AS
FALLING
INTO
TWO
CLASSES
AN
INDIRECT
METHOD
OF
DEADLOCK
PREVENTION
IS
TO
PREVENT
THE
OCCURRENCE
OF
ONE
OF
THE
THREE
NECESSARY
CONDITIONS
LISTED
PREVI
OUSLY
ITEMS
THROUGH
A
DIRECT
METHOD
OF
DEADLOCK
PREVENTION
IS
TO
PREVENT
THE
OCCURRENCE
OF
A
CIRCULAR
WAIT
ITEM
WE
NOW
EXAMINE
TECHNIQUES
RELATED
TO
EACH
OF
THE
FOUR
CONDITIONS
ALL
TEXTBOOKS
SIMPLY
LIST
THESE
FOUR
CONDITIONS
AS
THE
CONDITIONS
NEEDED
FOR
DEADLOCK
BUT
SUCH
A
PRESENTATION
OBSCURES
SOME
OF
THE
SUBTLER
ISSUES
ITEM
THE
CIRCULAR
WAIT
CONDITION
IS
FUNDAMENTALLY
DIFFERENT
FROM
THE
OTHER
THREE
CONDITIONS
ITEMS
THROUGH
ARE
POLICY
DECISIONS
WHILE
ITEM
IS
A
CIRCUM
STANCE
THAT
MIGHT
OCCUR
DEPENDING
ON
THE
SEQUENCING
OF
REQUESTS
AND
RELEASES
BY
THE
INVOLVED
PROCESSES
LINKING
CIRCULAR
WAIT
WITH
THE
THREE
NECESSARY
CONDITIONS
LEADS
TO
INADEQUATE
DISTINCTION
BETWEEN
PREVEN
TION
AND
AVOIDANCE
SEE
AND
FOR
A
DISCUSSION
DEADLOCK
PREVENTION
MUTUAL
EXCLUSION
IN
GENERAL
THE
FIRST
OF
THE
FOUR
LISTED
CONDITIONS
CANNOT
BE
DISALLOWED
IF
ACCESS
TO
A
RESOURCE
REQUIRES
MUTUAL
EXCLUSION
THEN
MUTUAL
EXCLUSION
MUST
BE
SUPPORTED
BY
THE
OS
SOME
RESOURCES
SUCH
AS
FILES
MAY
ALLOW
MULTIPLE
ACCESSES
FOR
READS
BUT
ONLY
EXCLUSIVE
ACCESS
FOR
WRITES
EVEN
IN
THIS
CASE
DEADLOCK
CAN
OCCUR
IF
MORE
THAN
ONE
PROCESS
REQUIRES
WRITE
PERMISSION
HOLD
AND
WAIT
THE
HOLD
AND
WAIT
CONDITION
CAN
BE
PREVENTED
BY
REQUIRING
THAT
A
PROCESS
REQUEST
ALL
OF
ITS
REQUIRED
RESOURCES
AT
ONE
TIME
AND
BLOCKING
THE
PROCESS
UNTIL
ALL
REQUESTS
CAN
BE
GRANTED
SIMULTANEOUSLY
THIS
APPROACH
IS
INEFFICIENT
IN
TWO
WAYS
FIRST
A
PROCESS
MAY
BE
HELD
UP
FOR
A
LONG
TIME
WAITING
FOR
ALL
OF
ITS
RESOURCE
REQUESTS
TO
BE
FILLED
WHEN
IN
FACT
IT
COULD
HAVE
PROCEEDED
WITH
ONLY
SOME
OF
THE
RESOURCES
SECOND
RESOURCES
ALLOCATED
TO
A
PROCESS
MAY
REMAIN
UNUSED
FOR
A
CONSIDERABLE
PERIOD
DURING
WHICH
TIME
THEY
ARE
DENIED
TO
OTHER
PROCESSES
ANOTHER
PROBLEM
IS
THAT
A
PROCESS
MAY
NOT
KNOW
IN
ADVANCE
ALL
OF
THE
RESOURCES
THAT
IT
WILL
REQUIRE
THERE
IS
ALSO
THE
PRACTICAL
PROBLEM
CREATED
BY
THE
USE
OF
MODULAR
PROGRAM
MING
OR
A
MULTITHREADED
STRUCTURE
FOR
AN
APPLICATION
AN
APPLICATION
WOULD
NEED
TO
BE
AWARE
OF
ALL
RESOURCES
THAT
WILL
BE
REQUESTED
AT
ALL
LEVELS
OR
IN
ALL
MODULES
TO
MAKE
THE
SIMULTANEOUS
REQUEST
NO
PREEMPTION
THIS
CONDITION
CAN
BE
PREVENTED
IN
SEVERAL
WAYS
FIRST
IF
A
PROCESS
HOLDING
CERTAIN
RESOURCES
IS
DENIED
A
FURTHER
REQUEST
THAT
PROCESS
MUST
RELEASE
ITS
ORIGINAL
RESOURCES
AND
IF
NECESSARY
REQUEST
THEM
AGAIN
TOGETHER
WITH
THE
ADDITIONAL
RESOURCE
ALTERNATIVELY
IF
A
PROCESS
REQUESTS
A
RESOURCE
THAT
IS
CURRENTLY
HELD
BY
ANOTHER
PRO
CESS
THE
OS
MAY
PREEMPT
THE
SECOND
PROCESS
AND
REQUIRE
IT
TO
RELEASE
ITS
RESOURCES
THIS
LATTER
SCHEME
WOULD
PREVENT
DEADLOCK
ONLY
IF
NO
TWO
PROCESSES
POSSESSED
THE
SAME
PRIORITY
THIS
APPROACH
IS
PRACTICAL
ONLY
WHEN
APPLIED
TO
RESOURCES
WHOSE
STATE
CAN
BE
EASILY
SAVED
AND
RESTORED
LATER
AS
IS
THE
CASE
WITH
A
PROCESSOR
CIRCULAR
WAIT
THE
CIRCULAR
WAIT
CONDITION
CAN
BE
PREVENTED
BY
DEFINING
A
LINEAR
ORDERING
OF
RESOURCE
TYPES
IF
A
PROCESS
HAS
BEEN
ALLOCATED
RESOURCES
OF
TYPE
R
THEN
IT
MAY
SUBSEQUENTLY
REQUEST
ONLY
THOSE
RESOURCES
OF
TYPES
FOLLOWING
R
IN
THE
ORDERING
TO
SEE
THAT
THIS
STRATEGY
WORKS
LET
US
ASSOCIATE
AN
INDEX
WITH
EACH
RESOURCE
TYPE
THEN
RESOURCE
RI
PRECEDES
RJ
IN
THE
ORDERING
IF
I
J
NOW
SUPPOSE
THAT
TWO
PROCESSES
A
AND
B
ARE
DEADLOCKED
BECAUSE
A
HAS
ACQUIRED
RI
AND
REQUESTED
RJ
AND
B
HAS
ACQUIRED
RJ
AND
REQUESTED
RI
THIS
CONDITION
IS
IMPOSSIBLE
BECAUSE
IT
IMPLIES
I
J
AND
J
I
AS
WITH
HOLD
AND
WAIT
PREVENTION
CIRCULAR
WAIT
PREVENTION
MAY
BE
INEFFICIENT
SLOWING
DOWN
PROCESSES
AND
DENYING
RESOURCE
ACCESS
UNNECESSARILY
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
DEADLOCK
AVOIDANCE
AN
APPROACH
TO
SOLVING
THE
DEADLOCK
PROBLEM
THAT
DIFFERS
SUBTLY
FROM
DEADLOCK
PREVENTION
IS
DEADLOCK
AVOIDANCE
IN
DEADLOCK
PREVENTION
WE
CONSTRAIN
RESOURCE
REQUESTS
TO
PREVENT
AT
LEAST
ONE
OF
THE
FOUR
CONDITIONS
OF
DEADLOCK
THIS
IS
EITHER
DONE
INDIRECTLY
BY
PREVENTING
ONE
OF
THE
THREE
NECESSARY
POLICY
CONDITIONS
MUTUAL
EXCLUSION
HOLD
AND
WAIT
NO
PREEMPTION
OR
DIRECTLY
BY
PREVENTING
CIRCULAR
WAIT
THIS
LEADS
TO
INEFFICIENT
USE
OF
RESOURCES
AND
INEFFICIENT
EXECUTION
OF
PROCESSES
DEADLOCK
AVOIDANCE
ON
THE
OTHER
HAND
ALLOWS
THE
THREE
NECESSARY
CONDITIONS
BUT
MAKES
JUDICIOUS
CHOICES
TO
ASSURE
THAT
THE
DEADLOCK
POINT
IS
NEVER
REACHED
AS
SUCH
AVOIDANCE
ALLOWS
MORE
CONCURRENCY
THAN
PREVENTION
WITH
DEADLOCK
AVOIDANCE
A
DECISION
IS
MADE
DYNAMICALLY
WHETHER
THE
CURRENT
RESOURCE
ALLOCATION
REQUEST
WILL
IF
GRANTED
POTENTIALLY
LEAD
TO
A
DEADLOCK
DEADLOCK
AVOIDANCE
THUS
REQUIRES
KNOWL
EDGE
OF
FUTURE
PROCESS
RESOURCE
REQUESTS
IN
THIS
SECTION
WE
DESCRIBE
TWO
APPROACHES
TO
DEADLOCK
AVOIDANCE
DO
NOT
START
A
PROCESS
IF
ITS
DEMANDS
MIGHT
LEAD
TO
DEADLOCK
DO
NOT
GRANT
AN
INCREMENTAL
RESOURCE
REQUEST
TO
A
PROCESS
IF
THIS
ALLOCATION
MIGHT
LEAD
TO
DEADLOCK
PROCESS
INITIATION
DENIAL
CONSIDER
A
SYSTEM
OF
N
PROCESSES
AND
M
DIFFERENT
TYPES
OF
RESOURCES
LET
US
DEFINE
THE
FOLLOWING
VECTORS
AND
MATRICES
RESOURCE
R
C
RM
TOTAL
AMOUNT
OF
EACH
RESOURCE
IN
THE
SYSTEM
AVAILABLE
V
C
VM
TOTAL
AMOUNT
OF
EACH
RESOURCE
NOT
ALLOCATED
TO
ANY
PROCESS
CLAIM
C
C
Μ
C
F
F
F
F
C
CNM
CIJ
REQUIREMENT
OF
PROCESS
I
FOR
RESOURCE
J
ALLOCATION
A
C
Μ
C
F
F
F
F
C
ANM
AIJ
CURRENT
ALLOCATION
TO
PROCESS
I
OF
RESOURCE
J
THE
MATRIX
CLAIM
GIVES
THE
MAXIMUM
REQUIREMENT
OF
EACH
PROCESS
FOR
EACH
RESOURCE
WITH
ONE
ROW
DEDICATED
TO
EACH
PROCESS
THIS
INFORMATION
MUST
BE
TERM
AVOIDANCE
IS
A
BIT
CONFUSING
IN
FACT
ONE
COULD
CONSIDER
THE
STRATEGIES
DISCUSSED
IN
THIS
SECTION
TO
BE
EXAMPLES
OF
DEADLOCK
PREVENTION
BECAUSE
THEY
INDEED
PREVENT
THE
OCCURRENCE
OF
A
DEADLOCK
DEADLOCK
AVOIDANCE
DECLARED
IN
ADVANCE
BY
A
PROCESS
FOR
DEADLOCK
AVOIDANCE
TO
WORK
SIMILARLY
THE
MATRIX
ALLOCATION
GIVES
THE
CURRENT
ALLOCATION
TO
EACH
PROCESS
THE
FOLLOWING
RELA
TIONSHIPS
HOLD
RJ
VJ
AN
AIJ
FOR
ALL
J
ALL
RESOURCES
ARE
EITHER
AVAILABLE
OR
ALLOCATED
I
CIJ
RJ
FOR
ALL
I
J
NO
PROCESS
CAN
CLAIM
MORE
THAN
THE
TOTAL
AMOUNT
OF
RESOURCES
IN
THE
SYSTEM
AIJ
CIJ
FOR
ALL
I
J
NO
PROCESS
IS
ALLOCATED
MORE
RESOURCES
OF
ANY
TYPE
THAN
THE
PROCESS
ORIGINALLY
CLAIMED
TO
NEED
WITH
THESE
QUANTITIES
DEFINED
WE
CAN
DEFINE
A
DEADLOCK
AVOIDANCE
POLICY
THAT
REFUSES
TO
START
A
NEW
PROCESS
IF
ITS
RESOURCE
REQUIREMENTS
MIGHT
LEAD
TO
DEADLOCK
START
A
NEW
PROCESS
PN
ONLY
IF
N
RJ
Ú
C
N
J
CIJ
FOR
ALL
J
I
THAT
IS
A
PROCESS
IS
ONLY
STARTED
IF
THE
MAXIMUM
CLAIM
OF
ALL
CURRENT
PROCESSES
PLUS
THOSE
OF
THE
NEW
PROCESS
CAN
BE
MET
THIS
STRATEGY
IS
HARDLY
OPTIMAL
BECAUSE
IT
ASSUMES
THE
WORST
THAT
ALL
PROCESSES
WILL
MAKE
THEIR
MAXIMUM
CLAIMS
TOGETHER
RESOURCE
ALLOCATION
DENIAL
THE
STRATEGY
OF
RESOURCE
ALLOCATION
DENIAL
REFERRED
TO
AS
THE
BANKER
ALGORITHM
WAS
FIRST
PROPOSED
IN
LET
US
BEGIN
BY
DEFINING
THE
CONCEPTS
OF
STATE
AND
SAFE
STATE
CONSIDER
A
SYSTEM
WITH
A
FIXED
NUMBER
OF
PROCESSES
AND
A
FIXED
NUMBER
OF
RESOURCES
AT
ANY
TIME
A
PROCESS
MAY
HAVE
ZERO
OR
MORE
RESOURCES
ALLOCATED
TO
IT
THE
STATE
OF
THE
SYSTEM
REFLECTS
THE
CURRENT
ALLOCATION
OF
RESOURCES
TO
PROCESSES
THUS
THE
STATE
CONSISTS
OF
THE
TWO
VECTORS
RESOURCE
AND
AVAILABLE
AND
THE
TWO
MATRICES
CLAIM
AND
ALLOCATION
DEFINED
EARLIER
A
SAFE
STATE
IS
ONE
IN
WHICH
THERE
IS
AT
LEAST
ONE
SEQUENCE
OF
RESOURCE
ALLOCATIONS
TO
PROCESSES
THAT
DOES
NOT
RESULT
IN
A
DEADLOCK
I
E
ALL
OF
THE
PROCESSES
CAN
BE
RUN
TO
COMPLETION
AN
UNSAFE
STATE
IS
OF
COURSE
A
STATE
THAT
IS
NOT
SAFE
THE
FOLLOWING
EXAMPLE
ILLUSTRATES
THESE
CONCEPTS
FIGURE
SHOWS
THE
STATE
OF
A
SYSTEM
CONSISTING
OF
FOUR
PROCESSES
AND
THREE
RESOURCES
THE
TOTAL
AMOUNT
OF
RESOURCES
AND
ARE
AND
UNITS
RESPECTIVELY
IN
THE
CUR
RENT
STATE
ALLOCATIONS
HAVE
BEEN
MADE
TO
THE
FOUR
PROCESSES
LEAVING
UNIT
OF
USED
THIS
NAME
BECAUSE
OF
THE
ANALOGY
OF
THIS
PROBLEM
TO
ONE
IN
BANKING
WITH
CUSTOMERS
WHO
WISH
TO
BORROW
MONEY
CORRESPONDING
TO
PROCESSES
AND
THE
MONEY
TO
BE
BORROWED
CORRESPONDING
TO
RESOURCES
STATED
AS
A
BANKING
PROBLEM
THE
BANK
HAS
A
LIMITED
RESERVE
OF
MONEY
TO
LEND
AND
A
LIST
OF
CUSTOMERS
EACH
WITH
A
LINE
OF
CREDIT
A
CUSTOMER
MAY
CHOOSE
TO
BORROW
AGAINST
THE
LINE
OF
CREDIT
A
POR
TION
AT
A
TIME
AND
THERE
IS
NO
GUARANTEE
THAT
THE
CUSTOMER
WILL
MAKE
ANY
REPAYMENT
UNTIL
AFTER
HAVING
TAKEN
OUT
THE
MAXIMUM
AMOUNT
OF
LOAN
THE
BANKER
CAN
REFUSE
A
LOAN
TO
A
CUSTOMER
IF
THERE
IS
A
RISK
THAT
THE
BANK
WILL
HAVE
INSUFFICIENT
FUNDS
TO
MAKE
FURTHER
LOANS
THAT
WILL
PERMIT
THE
CUSTOMERS
TO
REPAY
EVENTUALLY
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
CLAIM
MATRIX
C
ALLOCATION
MATRIX
A
C
A
RESOURCE
VECTOR
R
A
INITIAL
STATE
AVAILABLE
VECTOR
V
CLAIM
MATRIX
C
ALLOCATION
MATRIX
A
C
A
RESOURCE
VECTOR
R
AVAILABLE
VECTOR
V
B
RUNS
TO
COMPLETION
CLAIM
MATRIX
C
ALLOCATION
MATRIX
A
C
A
RESOURCE
VECTOR
R
AVAILABLE
VECTOR
V
C
RUNS
TO
COMPLETION
CLAIM
MATRIX
C
ALLOCATION
MATRIX
A
C
A
RESOURCE
VECTOR
R
AVAILABLE
VECTOR
V
D
RUNS
TO
COMPLETION
FIGURE
DETERMINATION
OF
A
SAFE
STATE
DEADLOCK
AVOIDANCE
AND
UNIT
OF
AVAILABLE
IS
THIS
A
SAFE
STATE
TO
ANSWER
THIS
QUESTION
WE
ASK
AN
INTERMEDIATE
QUESTION
CAN
ANY
OF
THE
FOUR
PROCESSES
BE
RUN
TO
COMPLETION
WITH
THE
RESOURCES
AVAILABLE
THAT
IS
CAN
THE
DIFFERENCE
BETWEEN
THE
MAXIMUM
REQUIRE
MENT
AND
CURRENT
ALLOCATION
FOR
ANY
PROCESS
BE
MET
WITH
THE
AVAILABLE
RESOURCES
IN
TERMS
OF
THE
MATRICES
AND
VECTORS
INTRODUCED
EARLIER
THE
CONDITION
TO
BE
MET
FOR
PROCESS
I
IS
CIJ
AIJ
VJ
FOR
ALL
J
CLEARLY
THIS
IS
NOT
POSSIBLE
FOR
WHICH
HAS
ONLY
UNIT
OF
AND
REQUIRES
MORE
UNITS
OF
UNITS
OF
AND
UNITS
OF
HOWEVER
BY
ASSIGNING
ONE
UNIT
OF
TO
PROCESS
HAS
ITS
MAXIMUM
REQUIRED
RESOURCES
ALLOCATED
AND
CAN
RUN
TO
COMPLETION
LET
US
ASSUME
THAT
THIS
IS
ACCOMPLISHED
WHEN
COMPLETES
ITS
RESOURCES
CAN
BE
RETURNED
TO
THE
POOL
OF
AVAILABLE
RESOURCES
THE
RESULTING
STATE
IS
SHOWN
IN
FIGURE
NOW
WE
CAN
ASK
AGAIN
IF
ANY
OF
THE
REMAINING
PROCESSES
CAN
BE
COMPLETED
IN
THIS
CASE
EACH
OF
THE
REMAINING
PROCESSES
COULD
BE
COMPLETED
SUPPOSE
WE
CHOOSE
ALLOCATE
THE
REQUIRED
RESOURCES
COMPLETE
AND
RETURN
ALL
OF
RESOURCES
TO
THE
AVAILABLE
POOL
WE
ARE
LEFT
IN
THE
STATE
SHOWN
IN
FIGURE
NEXT
WE
CAN
COMPLETE
RESULTING
IN
THE
STATE
OF
FIGURE
FINALLY
WE
CAN
COMPLETE
AT
THIS
POINT
ALL
OF
THE
PROCESSES
HAVE
BEEN
RUN
TO
COMPLETION
THUS
THE
STATE
DEFINED
BY
FIGURE
IS
A
SAFE
STATE
THESE
CONCEPTS
SUGGEST
THE
FOLLOWING
DEADLOCK
AVOIDANCE
STRATEGY
WHICH
ENSURES
THAT
THE
SYSTEM
OF
PROCESSES
AND
RESOURCES
IS
ALWAYS
IN
A
SAFE
STATE
WHEN
A
PROCESS
MAKES
A
REQUEST
FOR
A
SET
OF
RESOURCES
ASSUME
THAT
THE
REQUEST
IS
GRANTED
UPDATE
THE
SYSTEM
STATE
ACCORDINGLY
AND
THEN
DETERMINE
IF
THE
RESULT
IS
A
SAFE
STATE
IF
SO
GRANT
THE
REQUEST
AND
IF
NOT
BLOCK
THE
PROCESS
UNTIL
IT
IS
SAFE
TO
GRANT
THE
REQUEST
CONSIDER
THE
STATE
DEFINED
IN
FIGURE
SUPPOSE
MAKES
A
REQUEST
FOR
ONE
ADDITIONAL
UNIT
OF
AND
ONE
ADDITIONAL
UNIT
OF
IF
WE
ASSUME
THE
REQUEST
IS
GRANTED
THEN
THE
RESULTING
STATE
IS
THAT
OF
FIGURE
WE
HAVE
ALREADY
SEEN
THAT
THIS
IS
A
SAFE
STATE
THEREFORE
IT
IS
SAFE
TO
GRANT
THE
REQUEST
NOW
LET
US
RETURN
TO
THE
STATE
OF
FIGURE
AND
SUPPOSE
THAT
MAKES
THE
REQUEST
FOR
ONE
ADDITIONAL
UNIT
EACH
OF
AND
IF
WE
ASSUME
THAT
THE
REQUEST
IS
GRANTED
WE
ARE
LEFT
IN
THE
STATE
OF
FIGURE
IS
THIS
A
SAFE
STATE
THE
ANSWER
IS
NO
BECAUSE
EACH
PROCESS
WILL
NEED
AT
LEAST
ONE
ADDITIONAL
UNIT
OF
AND
THERE
ARE
NONE
AVAILABLE
THUS
ON
THE
BASIS
OF
DEADLOCK
AVOIDANCE
THE
REQUEST
BY
SHOULD
BE
DENIED
AND
SHOULD
BE
BLOCKED
IT
IS
IMPORTANT
TO
POINT
OUT
THAT
FIGURE
IS
NOT
A
DEADLOCKED
STATE
IT
MERELY
HAS
THE
POTENTIAL
FOR
DEADLOCK
IT
IS
POSSIBLE
FOR
EXAMPLE
THAT
IF
WERE
RUN
FROM
THIS
STATE
IT
WOULD
SUBSEQUENTLY
RELEASE
ONE
UNIT
OF
AND
ONE
UNIT
OF
PRIOR
TO
NEEDING
THESE
RESOURCES
AGAIN
IF
THAT
HAPPENED
THE
SYSTEM
WOULD
RETURN
TO
A
SAFE
STATE
THUS
THE
DEADLOCK
AVOIDANCE
STRATEGY
DOES
NOT
PREDICT
DEADLOCK
WITH
CERTAINTY
IT
MERELY
ANTICIPATES
THE
POSSIBILITY
OF
DEADLOCK
AND
ASSURES
THAT
THERE
IS
NEVER
SUCH
A
POSSIBILITY
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
CLAIM
MATRIX
C
ALLOCATION
MATRIX
A
C
A
RESOURCE
VECTOR
R
A
INITIAL
STATE
AVAILABLE
VECTOR
V
CLAIM
MATRIX
C
ALLOCATION
MATRIX
A
C
A
RESOURCE
VECTOR
R
AVAILABLE
VECTOR
V
B
REQUESTS
ONE
UNIT
EACH
OF
AND
FIGURE
DETERMINATION
OF
AN
UNSAFE
STATE
FIGURE
GIVES
AN
ABSTRACT
VERSION
OF
THE
DEADLOCK
AVOIDANCE
LOGIC
THE
MAIN
ALGORITHM
IS
SHOWN
IN
PART
B
WITH
THE
STATE
OF
THE
SYSTEM
DEFINED
BY
THE
DATA
STRUCTURE
STATE
REQUEST
IS
A
VECTOR
DEFINING
THE
RESOURCES
REQUESTED
BY
PROCESS
I
FIRST
A
CHECK
IS
MADE
TO
ASSURE
THAT
THE
REQUEST
DOES
NOT
EXCEED
THE
ORIGINAL
CLAIM
OF
THE
PROCESS
IF
THE
REQUEST
IS
VALID
THE
NEXT
STEP
IS
TO
DETERMINE
IF
IT
IS
POSSIBLE
TO
FULFILL
THE
REQUEST
I
E
THERE
ARE
SUFFICIENT
RESOURCES
AVAILABLE
IF
IT
IS
NOT
POSSIBLE
THEN
THE
PROCESS
IS
SUSPENDED
IF
IT
IS
POSSIBLE
THE
FINAL
STEP
IS
TO
DETERMINE
IF
IT
IS
SAFE
TO
FULFILL
THE
REQUEST
TO
DO
THIS
THE
RESOURCES
ARE
TENTATIVELY
ASSIGNED
TO
PROCESS
I
TO
FORM
NEWSTATE
THEN
A
TEST
FOR
SAFETY
IS
MADE
USING
THE
ALGORITHM
IN
FIGURE
DEADLOCK
AVOIDANCE
HAS
THE
ADVANTAGE
THAT
IT
IS
NOT
NECESSARY
TO
PREEMPT
AND
ROLLBACK
PROCESSES
AS
IN
DEADLOCK
DETECTION
AND
IS
LESS
RESTRICTIVE
THAN
DEADLOCK
PREVENTION
HOWEVER
IT
DOES
HAVE
A
NUMBER
OF
RESTRICTIONS
ON
ITS
USE
THE
MAXIMUM
RESOURCE
REQUIREMENT
FOR
EACH
PROCESS
MUST
BE
STATED
IN
ADVANCE
THE
PROCESSES
UNDER
CONSIDERATION
MUST
BE
INDEPENDENT
THAT
IS
THE
ORDER
IN
WHICH
THEY
EXECUTE
MUST
BE
UNCONSTRAINED
BY
ANY
SYNCHRONIZATION
REQUIREMENTS
THERE
MUST
BE
A
FIXED
NUMBER
OF
RESOURCES
TO
ALLOCATE
NO
PROCESS
MAY
EXIT
WHILE
HOLDING
RESOURCES
DEADLOCK
AVOIDANCE
STRUCT
STATE
INT
RESOURCE
M
INT
AVAILABLE
M
INT
CLAIM
N
M
INT
ALLOC
N
M
A
GLOBAL
DATA
STRUCTURES
IF
ALLOC
I
REQUEST
CLAIM
I
ERROR
TOTAL
REQUEST
CLAIM
ELSE
IF
REQUEST
AVAILABLE
SUSPEND
PROCESS
ELSE
SIMULATE
ALLOC
DEFINE
NEWSTATE
BY
ALLOC
I
ALLOC
I
REQUEST
AVAILABLE
AVAILABLE
REQUEST
IF
SAFE
NEWSTATE
CARRY
OUT
ALLOCATION
ELSE
RESTORE
ORIGINAL
STATE
SUSPEND
PROCESS
B
RESOURCE
ALLOC
ALGORITHM
BOOLEAN
SAFE
STATE
S
INT
CURRENTAVAIL
M
PROCESS
REST
NUMBER
OF
PROCESSES
CURRENTAVAIL
AVAILABLE
REST
ALL
PROCESSES
POSSIBLE
TRUE
WHILE
POSSIBLE
FIND
A
PROCESS
PK
IN
REST
SUCH
THAT
CLAIM
K
ALLOC
K
CURRENTAVAIL
IF
FOUND
SIMULATE
EXECUTION
OF
PK
CURRENTAVAIL
CURRENTAVAIL
ALLOC
K
REST
REST
PK
ELSE
POSSIBLE
FALSE
RETURN
REST
NULL
C
TEST
FOR
SAFETY
ALGORITHM
BANKER
ALGORITHM
FIGURE
DEADLOCK
AVOIDANCE
LOGIC
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
DEADLOCK
DETECTION
DEADLOCK
PREVENTION
STRATEGIES
ARE
VERY
CONSERVATIVE
THEY
SOLVE
THE
PROBLEM
OF
DEADLOCK
BY
LIMITING
ACCESS
TO
RESOURCES
AND
BY
IMPOSING
RESTRICTIONS
ON
PRO
CESSES
AT
THE
OPPOSITE
EXTREME
DEADLOCK
DETECTION
STRATEGIES
DO
NOT
LIMIT
RESOURCE
ACCESS
OR
RESTRICT
PROCESS
ACTIONS
WITH
DEADLOCK
DETECTION
REQUESTED
RESOURCES
ARE
GRANTED
TO
PROCESSES
WHENEVER
POSSIBLE
PERIODICALLY
THE
OS
PERFORMS
AN
ALGORITHM
THAT
ALLOWS
IT
TO
DETECT
THE
CIRCULAR
WAIT
CONDITION
DESCRIBED
EARLIER
IN
CONDITION
AND
ILLUSTRATED
IN
FIGURE
DEADLOCK
DETECTION
ALGORITHM
A
CHECK
FOR
DEADLOCK
CAN
BE
MADE
AS
FREQUENTLY
AS
EACH
RESOURCE
REQUEST
OR
LESS
FREQUENTLY
DEPENDING
ON
HOW
LIKELY
IT
IS
FOR
A
DEADLOCK
TO
OCCUR
CHECKING
AT
EACH
RESOURCE
REQUEST
HAS
TWO
ADVANTAGES
IT
LEADS
TO
EARLY
DETECTION
AND
THE
ALGORITHM
IS
RELATIVELY
SIMPLE
BECAUSE
IT
IS
BASED
ON
INCREMENTAL
CHANGES
TO
THE
STATE
OF
THE
SYSTEM
ON
THE
OTHER
HAND
SUCH
FREQUENT
CHECKS
CONSUME
CONSIDER
ABLE
PROCESSOR
TIME
A
COMMON
ALGORITHM
FOR
DEADLOCK
DETECTION
IS
ONE
DESCRIBED
IN
THE
ALLOCATION
MATRIX
AND
AVAILABLE
VECTOR
DESCRIBED
IN
THE
PREVIOUS
SECTION
ARE
USED
IN
ADDITION
A
REQUEST
MATRIX
Q
IS
DEFINED
SUCH
THAT
QIJ
REPRESENTS
THE
AMOUNT
OF
RESOURCES
OF
TYPE
J
REQUESTED
BY
PROCESS
I
THE
ALGORITHM
PROCEEDS
BY
MARKING
PROCESSES
THAT
ARE
NOT
DEADLOCKED
INITIALLY
ALL
PROCESSES
ARE
UNMARKED
THEN
THE
FOLLOWING
STEPS
ARE
PERFORMED
MARK
EACH
PROCESS
THAT
HAS
A
ROW
IN
THE
ALLOCATION
MATRIX
OF
ALL
ZEROS
INITIALIZE
A
TEMPORARY
VECTOR
W
TO
EQUAL
THE
AVAILABLE
VECTOR
FIND
AN
INDEX
I
SUCH
THAT
PROCESS
I
IS
CURRENTLY
UNMARKED
AND
THE
ITH
ROW
OF
Q
IS
LESS
THAN
OR
EQUAL
TO
W
THAT
IS
QIK
WK
FOR
K
M
IF
NO
SUCH
ROW
IS
FOUND
TERMINATE
THE
ALGORITHM
IF
SUCH
A
ROW
IS
FOUND
MARK
PROCESS
I
AND
ADD
THE
CORRESPONDING
ROW
OF
THE
ALLOCATION
MATRIX
TO
W
THAT
IS
SET
WK
WK
AIK
FOR
K
M
RETURN
TO
STEP
A
DEADLOCK
EXISTS
IF
AND
ONLY
IF
THERE
ARE
UNMARKED
PROCESSES
AT
THE
END
OF
THE
ALGORITHM
EACH
UNMARKED
PROCESS
IS
DEADLOCKED
THE
STRATEGY
IN
THIS
ALGO
RITHM
IS
TO
FIND
A
PROCESS
WHOSE
RESOURCE
REQUESTS
CAN
BE
SATISFIED
WITH
THE
AVAILABLE
RESOURCES
AND
THEN
ASSUME
THAT
THOSE
RESOURCES
ARE
GRANTED
AND
THAT
THE
PROCESS
RUNS
TO
COMPLETION
AND
RELEASES
ALL
OF
ITS
RESOURCES
THE
ALGORITHM
THEN
LOOKS
FOR
ANOTHER
PROCESS
TO
SATISFY
NOTE
THAT
THIS
ALGORITHM
DOES
NOT
GUARANTEE
TO
PREVENT
DEADLOCK
THAT
WILL
DEPEND
ON
THE
ORDER
IN
WHICH
FUTURE
REQUESTS
ARE
GRANTED
ALL
THAT
IT
DOES
IS
DETERMINE
IF
DEADLOCK
CURRENTLY
EXISTS
WE
CAN
USE
FIGURE
TO
ILLUSTRATE
THE
DEADLOCK
DETECTION
ALGORITHM
THE
ALGORITHM
PROCEEDS
AS
FOLLOWS
MARK
BECAUSE
HAS
NO
ALLOCATED
RESOURCES
SET
W
DEADLOCK
DETECTION
REQUEST
MATRIX
Q
ALLOCATION
MATRIX
A
RESOURCE
VECTOR
FIGURE
EXAMPLE
FOR
DEADLOCK
DETECTION
AVAILABLE
VECTOR
THE
REQUEST
OF
PROCESS
IS
LESS
THAN
OR
EQUAL
TO
W
SO
MARK
AND
SET
W
W
NO
OTHER
UNMARKED
PROCESS
HAS
A
ROW
IN
Q
THAT
IS
LESS
THAN
OR
EQUAL
TO
W
THEREFORE
TERMINATE
THE
ALGORITHM
THE
ALGORITHM
CONCLUDES
WITH
AND
UNMARKED
INDICATING
THAT
THESE
PROCESSES
ARE
DEADLOCKED
RECOVERY
ONCE
DEADLOCK
HAS
BEEN
DETECTED
SOME
STRATEGY
IS
NEEDED
FOR
RECOVERY
THE
FOLLOW
ING
ARE
POSSIBLE
APPROACHES
LISTED
IN
ORDER
OF
INCREASING
SOPHISTICATION
ABORT
ALL
DEADLOCKED
PROCESSES
THIS
IS
BELIEVE
IT
OR
NOT
ONE
OF
THE
MOST
COMMON
IF
NOT
THE
MOST
COMMON
SOLUTION
ADOPTED
IN
OPERATING
SYSTEMS
BACK
UP
EACH
DEADLOCKED
PROCESS
TO
SOME
PREVIOUSLY
DEFINED
CHECKPOINT
AND
RESTART
ALL
PROCESSES
THIS
REQUIRES
THAT
ROLLBACK
AND
RESTART
MECHANISMS
BE
BUILT
IN
TO
THE
SYSTEM
THE
RISK
IN
THIS
APPROACH
IS
THAT
THE
ORIGINAL
DEADLOCK
MAY
RECUR
HOWEVER
THE
NONDETERMINANCY
OF
CONCURRENT
PROCESSING
MAY
ENSURE
THAT
THIS
DOES
NOT
HAPPEN
SUCCESSIVELY
ABORT
DEADLOCKED
PROCESSES
UNTIL
DEADLOCK
NO
LONGER
EXISTS
THE
ORDER
IN
WHICH
PROCESSES
ARE
SELECTED
FOR
ABORTION
SHOULD
BE
ON
THE
BASIS
OF
SOME
CRITERION
OF
MINIMUM
COST
AFTER
EACH
ABORTION
THE
DETECTION
ALGORITHM
MUST
BE
REINVOKED
TO
SEE
WHETHER
DEADLOCK
STILL
EXISTS
SUCCESSIVELY
PREEMPT
RESOURCES
UNTIL
DEADLOCK
NO
LONGER
EXISTS
AS
IN
A
COST
BASED
SELECTION
SHOULD
BE
USED
AND
REINVOCATION
OF
THE
DETECTION
ALGORITHM
IS
REQUIRED
AFTER
EACH
PREEMPTION
A
PROCESS
THAT
HAS
A
RESOURCE
PREEMPTED
FROM
IT
MUST
BE
ROLLED
BACK
TO
A
POINT
PRIOR
TO
ITS
ACQUISITION
OF
THAT
RESOURCE
FOR
AND
THE
SELECTION
CRITERIA
COULD
BE
ONE
OF
THE
FOLLOWING
CHOOSE
THE
PROCESS
WITH
THE
LEAST
AMOUNT
OF
PROCESSOR
TIME
CONSUMED
SO
FAR
LEAST
AMOUNT
OF
OUTPUT
PRODUCED
SO
FAR
MOST
ESTIMATED
TIME
REMAINING
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
LEAST
TOTAL
RESOURCES
ALLOCATED
SO
FAR
LOWEST
PRIORITY
SOME
OF
THESE
QUANTITIES
ARE
EASIER
TO
MEASURE
THAN
OTHERS
ESTIMATED
TIME
REMAINING
IS
PARTICULARLY
SUSPECT
ALSO
OTHER
THAN
BY
MEANS
OF
THE
PRIORITY
MEASURE
THERE
IS
NO
INDICATION
OF
THE
COST
TO
THE
USER
AS
OPPOSED
TO
THE
COST
TO
THE
SYSTEM
AS
A
WHOLE
AN
INTEGRATED
DEADLOCK
STRATEGY
AS
TABLE
SUGGESTS
THERE
ARE
STRENGTHS
AND
WEAKNESSES
TO
ALL
OF
THE
STRATEGIES
FOR
DEALING
WITH
DEADLOCK
RATHER
THAN
ATTEMPTING
TO
DESIGN
AN
OS
FACILITY
THAT
EMPLOYS
ONLY
ONE
OF
THESE
STRATEGIES
IT
MIGHT
BE
MORE
EFFICIENT
TO
USE
DIFFERENT
STRATEGIES
IN
DIFFERENT
SITUATIONS
SUGGESTS
ONE
APPROACH
GROUP
RESOURCES
INTO
A
NUMBER
OF
DIFFERENT
RESOURCE
CLASSES
USE
THE
LINEAR
ORDERING
STRATEGY
DEFINED
PREVIOUSLY
FOR
THE
PREVENTION
OF
CIRCULAR
WAIT
TO
PREVENT
DEADLOCKS
BETWEEN
RESOURCE
CLASSES
WITHIN
A
RESOURCE
CLASS
USE
THE
ALGORITHM
THAT
IS
MOST
APPROPRIATE
FOR
THAT
CLASS
AS
AN
EXAMPLE
OF
THIS
TECHNIQUE
CONSIDER
THE
FOLLOWING
CLASSES
OF
RESOURCES
SWAPPABLE
SPACE
BLOCKS
OF
MEMORY
ON
SECONDARY
STORAGE
FOR
USE
IN
SWAPPING
PROCESSES
PROCESS
RESOURCES
ASSIGNABLE
DEVICES
SUCH
AS
TAPE
DRIVES
AND
FILES
MAIN
MEMORY
ASSIGNABLE
TO
PROCESSES
IN
PAGES
OR
SEGMENTS
INTERNAL
RESOURCES
SUCH
AS
I
O
CHANNELS
THE
ORDER
OF
THE
PRECEDING
LIST
REPRESENTS
THE
ORDER
IN
WHICH
RESOURCES
ARE
ASSIGNED
THE
ORDER
IS
A
REASONABLE
ONE
CONSIDERING
THE
SEQUENCE
OF
STEPS
THAT
A
PROCESS
MAY
FOLLOW
DURING
ITS
LIFETIME
WITHIN
EACH
CLASS
THE
FOLLOWING
STRATEGIES
COULD
BE
USED
SWAPPABLE
SPACE
PREVENTION
OF
DEADLOCKS
BY
REQUIRING
THAT
ALL
OF
THE
REQUIRED
RESOURCES
THAT
MAY
BE
USED
BE
ALLOCATED
AT
ONE
TIME
AS
IN
THE
HOLD
AND
WAIT
PREVENTION
STRATEGY
THIS
STRATEGY
IS
REASONABLE
IF
THE
MAXIMUM
STORAGE
REQUIREMENTS
ARE
KNOWN
WHICH
IS
OFTEN
THE
CASE
DEADLOCK
AVOIDANCE
IS
ALSO
A
POSSIBILITY
PROCESS
RESOURCES
AVOIDANCE
WILL
OFTEN
BE
EFFECTIVE
IN
THIS
CATEGORY
BECAUSE
IT
IS
REASONABLE
TO
EXPECT
PROCESSES
TO
DECLARE
AHEAD
OF
TIME
THE
RESOURCES
THAT
THEY
WILL
REQUIRE
IN
THIS
CLASS
PREVENTION
BY
MEANS
OF
RESOURCE
ORDERING
WITHIN
THIS
CLASS
IS
ALSO
POSSIBLE
MAIN
MEMORY
PREVENTION
BY
PREEMPTION
APPEARS
TO
BE
THE
MOST
APPROPRIATE
STRATEGY
FOR
MAIN
MEMORY
WHEN
A
PROCESS
IS
PREEMPTED
IT
IS
SIMPLY
SWAPPED
TO
SECONDARY
MEMORY
FREEING
SPACE
TO
RESOLVE
THE
DEADLOCK
INTERNAL
RESOURCES
PREVENTION
BY
MEANS
OF
RESOURCE
ORDERING
CAN
BE
USED
DINING
PHILOSOPHERS
PROBLEM
DINING
PHILOSOPHERS
PROBLEM
WE
NOW
TURN
TO
THE
DINING
PHILOSOPHERS
PROBLEM
INTRODUCED
BY
DIJKSTRA
FIVE
PHILOSOPHERS
LIVE
IN
A
HOUSE
WHERE
A
TABLE
IS
LAID
FOR
THEM
THE
LIFE
OF
EACH
PHI
LOSOPHER
CONSISTS
PRINCIPALLY
OF
THINKING
AND
EATING
AND
THROUGH
YEARS
OF
THOUGHT
ALL
OF
THE
PHILOSOPHERS
HAD
AGREED
THAT
THE
ONLY
FOOD
THAT
CONTRIBUTED
TO
THEIR
THINK
ING
EFFORTS
WAS
SPAGHETTI
DUE
TO
A
LACK
OF
MANUAL
SKILL
EACH
PHILOSOPHER
REQUIRES
TWO
FORKS
TO
EAT
SPAGHETTI
THE
EATING
ARRANGEMENTS
ARE
SIMPLE
FIGURE
A
ROUND
TABLE
ON
WHICH
IS
SET
A
LARGE
SERVING
BOWL
OF
SPAGHETTI
FIVE
PLATES
ONE
FOR
EACH
PHILOSOPHER
AND
FIVE
FORKS
A
PHILOSOPHER
WISHING
TO
EAT
GOES
TO
HIS
OR
HER
ASSIGNED
PLACE
AT
THE
TABLE
AND
USING
THE
TWO
FORKS
ON
EITHER
SIDE
OF
THE
PLATE
TAKES
AND
EATS
SOME
SPAGHETTI
THE
PROBLEM
DEVISE
A
RITUAL
ALGORITHM
THAT
WILL
ALLOW
THE
PHILOSOPHERS
TO
EAT
THE
ALGORITHM
MUST
SATISFY
MUTUAL
EXCLUSION
NO
TWO
PHILOSOPHERS
CAN
USE
THE
SAME
FORK
AT
THE
SAME
TIME
WHILE
AVOIDING
DEADLOCK
AND
STARVATION
IN
THIS
CASE
THE
TERM
HAS
LITERAL
AS
WELL
AS
ALGORITHMIC
MEANING
THIS
PROBLEM
MAY
NOT
SEEM
IMPORTANT
OR
RELEVANT
IN
ITSELF
HOWEVER
IT
DOES
ILLUSTRATE
BASIC
PROBLEMS
IN
DEADLOCK
AND
STARVATION
FURTHERMORE
ATTEMPTS
TO
DEVELOP
SOLUTIONS
REVEAL
MANY
OF
THE
DIFFICULTIES
IN
CONCURRENT
PROGRAMMING
E
G
SEE
IN
ADDITION
THE
DINING
PHILOSOPHERS
PROBLEM
CAN
BE
SEEN
AS
REPRE
SENTATIVE
OF
PROBLEMS
DEALING
WITH
THE
COORDINATION
OF
SHARED
RESOURCES
WHICH
MAY
FIGURE
DINING
ARRANGEMENT
FOR
PHILOSOPHERS
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
OCCUR
WHEN
AN
APPLICATION
INCLUDES
CONCURRENT
THREADS
OF
EXECUTION
ACCORDINGLY
THIS
PROBLEM
IS
A
STANDARD
TEST
CASE
FOR
EVALUATING
APPROACHES
TO
SYNCHRONIZATION
SOLUTION
USING
SEMAPHORES
FIGURE
SUGGESTS
A
SOLUTION
USING
SEMAPHORES
EACH
PHILOSOPHER
PICKS
UP
FIRST
THE
FORK
ON
THE
LEFT
AND
THEN
THE
FORK
ON
THE
RIGHT
AFTER
THE
PHILOSOPHER
IS
FINISHED
EATING
THE
TWO
FORKS
ARE
REPLACED
ON
THE
TABLE
THIS
SOLUTION
ALAS
LEADS
TO
DEADLOCK
IF
ALL
OF
THE
PHILOSOPHERS
ARE
HUNGRY
AT
THE
SAME
TIME
THEY
ALL
SIT
DOWN
THEY
ALL
PICK
UP
THE
FORK
ON
THEIR
LEFT
AND
THEY
ALL
REACH
OUT
FOR
THE
OTHER
FORK
WHICH
IS
NOT
THERE
IN
THIS
UNDIGNIFIED
POSITION
ALL
PHILOSOPHERS
STARVE
TO
OVERCOME
THE
RISK
OF
DEADLOCK
WE
COULD
BUY
FIVE
ADDITIONAL
FORKS
A
MORE
SANITARY
SOLUTION
OR
TEACH
THE
PHILOSOPHERS
TO
EAT
SPAGHETTI
WITH
JUST
ONE
FORK
AS
ANOTHER
APPROACH
WE
COULD
CONSIDER
ADDING
AN
ATTENDANT
WHO
ONLY
ALLOWS
FOUR
PHILOSOPHERS
AT
A
TIME
INTO
THE
DINING
ROOM
WITH
AT
MOST
FOUR
SEATED
PHILOSOPHERS
AT
LEAST
ONE
PHILOSOPHER
WILL
HAVE
ACCESS
TO
TWO
FORKS
FIGURE
SHOWS
SUCH
A
SOLU
TION
AGAIN
USING
SEMAPHORES
THIS
SOLUTION
IS
FREE
OF
DEADLOCK
AND
STARVATION
SOLUTION
USING
A
MONITOR
FIGURE
SHOWS
A
SOLUTION
TO
THE
DINING
PHILOSOPHERS
PROBLEM
USING
A
MONITOR
A
VECTOR
OF
FIVE
CONDITION
VARIABLES
IS
DEFINED
ONE
CONDITION
VARIABLE
PER
FORK
THESE
CONDITION
VARIABLES
ARE
USED
TO
ENABLE
A
PHILOSOPHER
TO
WAIT
FOR
THE
AVAILABILITY
OF
A
FORK
IN
ADDITION
THERE
IS
A
BOOLEAN
VECTOR
THAT
RECORDS
THE
AVAILABILITY
STATUS
OF
EACH
FORK
TRUE
MEANS
THE
FORK
IS
AVAILABLE
THE
MONITOR
CONSISTS
OF
TWO
PROCEDURES
THE
PROCEDURE
IS
USED
BY
A
PHILOSOPHER
TO
SEIZE
HIS
OR
HER
LEFT
AND
PROGRAM
DININGPHILOSOPHERS
SEMAPHORE
FORK
INT
I
VOID
PHILOSOPHER
INT
I
WHILE
TRUE
THINK
WAIT
FORK
I
WAIT
FORK
I
MOD
EAT
SIGNAL
FORK
I
MOD
SIGNAL
FORK
I
VOID
MAIN
PARBEGIN
PHILOSOPHER
PHILOSOPHER
PHILOSOPHER
PHILOSOPHER
PHILOSOPHER
FIGURE
A
FIRST
SOLUTION
TO
THE
DINING
PHILOSOPHERS
PROBLEM
UNIX
CONCURRENCY
MECHANISMS
PROGRAM
DININGPHILOSOPHERS
SEMAPHORE
FORK
SEMAPHORE
ROOM
INT
I
VOID
PHILOSOPHER
INT
I
WHILE
TRUE
THINK
WAIT
ROOM
WAIT
FORK
I
WAIT
FORK
I
MOD
EAT
SIGNAL
FORK
I
MOD
SIGNAL
FORK
I
SIGNAL
ROOM
VOID
MAIN
PARBEGIN
PHILOSOPHER
PHILOSOPHER
PHILOSOPHER
PHILOSOPHER
PHILOSOPHER
FIGURE
A
SECOND
SOLUTION
TO
THE
DINING
PHILOSOPHERS
PROBLEM
RIGHT
FORKS
IF
EITHER
FORK
IS
UNAVAILABLE
THE
PHILOSOPHER
PROCESS
IS
QUEUED
ON
THE
APPROPRIATE
CONDITION
VARIABLE
THIS
ENABLES
ANOTHER
PHILOSOPHER
PROCESS
TO
ENTER
THE
MONITOR
THE
RELEASE
FORKS
PROCEDURE
IS
USED
TO
MAKE
TWO
FORKS
AVAILABLE
NOTE
THAT
THE
STRUCTURE
OF
THIS
SOLUTION
IS
SIMILAR
TO
THAT
OF
THE
SEMAPHORE
SOLUTION
PROPOSED
IN
FIGURE
IN
BOTH
CASES
A
PHILOSOPHER
SEIZES
FIRST
THE
LEFT
FORK
AND
THEN
THE
RIGHT
FORK
UNLIKE
THE
SEMAPHORE
SOLUTION
THIS
MONITOR
SOLUTION
DOES
NOT
SUFFER
FROM
DEADLOCK
BECAUSE
ONLY
ONE
PROCESS
AT
A
TIME
MAY
BE
IN
THE
MONITOR
FOR
EXAMPLE
THE
FIRST
PHILOSOPHER
PROCESS
TO
ENTER
THE
MONITOR
IS
GUARANTEED
THAT
IT
CAN
PICK
UP
THE
RIGHT
FORK
AFTER
IT
PICKS
UP
THE
LEFT
FORK
BEFORE
THE
NEXT
PHILOSOPHER
TO
THE
RIGHT
HAS
A
CHANCE
TO
SEIZE
ITS
LEFT
FORK
WHICH
IS
THIS
PHILOSOPHER
RIGHT
FORK
UNIX
CONCURRENCY
MECHANISMS
UNIX
PROVIDES
A
VARIETY
OF
MECHANISMS
FOR
INTERPROCESSOR
COMMUNICATION
AND
SYN
CHRONIZATION
HERE
WE
LOOK
AT
THE
MOST
IMPORTANT
OF
THESE
PIPES
MESSAGES
SHARED
MEMORY
SEMAPHORES
SIGNALS
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
MONITOR
COND
FORKREADY
CONDITION
VARIABLE
FOR
SYNCHRONIZATION
BOOLEAN
FORK
TRUE
AVAILABILITY
STATUS
OF
EACH
FORK
VOID
INT
PID
PID
IS
THE
PHILOSOPHER
ID
NUMBER
INT
LEFT
PID
INT
RIGHT
PID
GRANT
THE
LEFT
FORK
IF
FORK
LEFT
CWAIT
FORKREADY
LEFT
QUEUE
ON
CONDITION
VARIABLE
FORK
LEFT
FALSE
GRANT
THE
RIGHT
FORK
IF
FORK
RIGHT
CWAIT
FORKREADY
RIGHT
QUEUE
ON
CONDITION
VARIABLE
FORK
RIGHT
FALSE
VOID
INT
PID
INT
LEFT
PID
INT
RIGHT
PID
RELEASE
THE
LEFT
FORK
IF
EMPTY
FORKREADY
LEFT
NO
ONE
IS
WAITING
FOR
THIS
FORK
FORK
LEFT
TRUE
ELSE
AWAKEN
A
PROCESS
WAITING
ON
THIS
FORK
CSIGNAL
FORKREADY
LEFT
RELEASE
THE
RIGHT
FORK
IF
EMPTY
FORKREADY
RIGHT
NO
ONE
IS
WAITING
FOR
THIS
FORK
FORK
RIGHT
TRUE
ELSE
AWAKEN
A
PROCESS
WAITING
ON
THIS
FORK
CSIGNAL
FORKREADY
RIGHT
VOID
PHILOSOPHER
K
TO
THE
FIVE
PHILOSOPHER
CLIENTS
WHILE
TRUE
THINK
K
CLIENT
REQUESTS
TWO
FORKS
VIA
MONITOR
EAT
SPAGHETTI
K
CLIENT
RELEASES
FORKS
VIA
THE
MONITOR
FIGURE
A
SOLUTION
TO
THE
DINING
PHILOSOPHERS
PROBLEM
USING
A
MONITOR
UNIX
CONCURRENCY
MECHANISMS
PIPES
MESSAGES
AND
SHARED
MEMORY
CAN
BE
USED
TO
COMMUNICATE
DATA
BETWEEN
PROCESSES
WHEREAS
SEMAPHORES
AND
SIGNALS
ARE
USED
TO
TRIGGER
ACTIONS
BY
OTHER
PROCESSES
PIPES
ONE
OF
THE
MOST
SIGNIFICANT
CONTRIBUTIONS
OF
UNIX
TO
THE
DEVELOPMENT
OF
OPERATING
SYSTEMS
IS
THE
PIPE
INSPIRED
BY
THE
CONCEPT
OF
COROUTINES
A
PIPE
IS
A
CIRCU
LAR
BUFFER
ALLOWING
TWO
PROCESSES
TO
COMMUNICATE
ON
THE
PRODUCER
CONSUMER
MODEL
THUS
IT
IS
A
FIRST
IN
FIRST
OUT
QUEUE
WRITTEN
BY
ONE
PROCESS
AND
READ
BY
ANOTHER
WHEN
A
PIPE
IS
CREATED
IT
IS
GIVEN
A
FIXED
SIZE
IN
BYTES
WHEN
A
PROCESS
ATTEMPTS
TO
WRITE
INTO
THE
PIPE
THE
WRITE
REQUEST
IS
IMMEDIATELY
EXECUTED
IF
THERE
IS
SUFFICIENT
ROOM
OTHERWISE
THE
PROCESS
IS
BLOCKED
SIMILARLY
A
READING
PROCESS
IS
BLOCKED
IF
IT
ATTEMPTS
TO
READ
MORE
BYTES
THAN
ARE
CURRENTLY
IN
THE
PIPE
OTHERWISE
THE
READ
REQUEST
IS
IMMEDIATELY
EXECUTED
THE
OS
ENFORCES
MUTUAL
EXCLUSION
THAT
IS
ONLY
ONE
PROCESS
CAN
ACCESS
A
PIPE
AT
A
TIME
THERE
ARE
TWO
TYPES
OF
PIPES
NAMED
AND
UNNAMED
ONLY
RELATED
PROCESSES
CAN
SHARE
UNNAMED
PIPES
WHILE
EITHER
RELATED
OR
UNRELATED
PROCESSES
CAN
SHARE
NAMED
PIPES
MESSAGES
A
MESSAGE
IS
A
BLOCK
OF
BYTES
WITH
AN
ACCOMPANYING
TYPE
UNIX
PROVIDES
MSGSND
AND
MSGRCV
SYSTEM
CALLS
FOR
PROCESSES
TO
ENGAGE
IN
MESSAGE
PASSING
ASSOCIATED
WITH
EACH
PROCESS
IS
A
MESSAGE
QUEUE
WHICH
FUNCTIONS
LIKE
A
MAILBOX
THE
MESSAGE
SENDER
SPECIFIES
THE
TYPE
OF
MESSAGE
WITH
EACH
MESSAGE
SENT
AND
THIS
CAN
BE
USED
AS
A
SELECTION
CRITERION
BY
THE
RECEIVER
THE
RECEIVER
CAN
EITHER
RETRIEVE
MESSAGES
IN
FIRST
IN
FIRST
OUT
ORDER
OR
BY
TYPE
A
PROCESS
WILL
BLOCK
WHEN
TRYING
TO
SEND
A
MESSAGE
TO
A
FULL
QUEUE
A
PROCESS
WILL
ALSO
BLOCK
WHEN
TRYING
TO
READ
FROM
AN
EMPTY
QUEUE
IF
A
PROCESS
ATTEMPTS
TO
READ
A
MESSAGE
OF
A
CERTAIN
TYPE
AND
FAILS
BECAUSE
NO
MESSAGE
OF
THAT
TYPE
IS
PRESENT
THE
PROCESS
IS
NOT
BLOCKED
SHARED
MEMORY
THE
FASTEST
FORM
OF
INTERPROCESS
COMMUNICATION
PROVIDED
IN
UNIX
IS
SHARED
MEMORY
THIS
IS
A
COMMON
BLOCK
OF
VIRTUAL
MEMORY
SHARED
BY
MULTIPLE
PROCESSES
PROCESSES
READ
AND
WRITE
SHARED
MEMORY
USING
THE
SAME
MACHINE
INSTRUCTIONS
THEY
USE
TO
READ
AND
WRITE
OTHER
PORTIONS
OF
THEIR
VIRTUAL
MEMORY
SPACE
PERMISSION
IS
READ
ONLY
OR
READ
WRITE
FOR
A
PROCESS
DETERMINED
ON
A
PER
PROCESS
BASIS
MUTUAL
EXCLUSION
CONSTRAINTS
ARE
NOT
PART
OF
THE
SHARED
MEMORY
FACILITY
BUT
MUST
BE
PROVIDED
BY
THE
PROCESSES
USING
THE
SHARED
MEMORY
SEMAPHORES
THE
SEMAPHORE
SYSTEM
CALLS
IN
UNIX
SYSTEM
V
ARE
A
GENERALIZATION
OF
THE
SEMWAIT
AND
SEMSIGNAL
PRIMITIVES
DEFINED
IN
CHAPTER
SEVERAL
OPERATIONS
CAN
BE
PER
FORMED
SIMULTANEOUSLY
AND
THE
INCREMENT
AND
DECREMENT
OPERATIONS
CAN
BE
VALUES
GREATER
THAN
THE
KERNEL
DOES
ALL
OF
THE
REQUESTED
OPERATIONS
ATOMICALLY
NO
OTHER
PROCESS
MAY
ACCESS
THE
SEMAPHORE
UNTIL
ALL
OPERATIONS
HAVE
COMPLETED
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
A
SEMAPHORE
CONSISTS
OF
THE
FOLLOWING
ELEMENTS
CURRENT
VALUE
OF
THE
SEMAPHORE
PROCESS
ID
OF
THE
LAST
PROCESS
TO
OPERATE
ON
THE
SEMAPHORE
NUMBER
OF
PROCESSES
WAITING
FOR
THE
SEMAPHORE
VALUE
TO
BE
GREATER
THAN
ITS
CURRENT
VALUE
NUMBER
OF
PROCESSES
WAITING
FOR
THE
SEMAPHORE
VALUE
TO
BE
ZERO
ASSOCIATED
WITH
THE
SEMAPHORE
ARE
QUEUES
OF
PROCESSES
BLOCKED
ON
THAT
SEMAPHORE
SEMAPHORES
ARE
ACTUALLY
CREATED
IN
SETS
WITH
A
SEMAPHORE
SET
CONSISTING
OF
ONE
OR
MORE
SEMAPHORES
THERE
IS
A
SEMCTL
SYSTEM
CALL
THAT
ALLOWS
ALL
OF
THE
SEMA
PHORE
VALUES
IN
THE
SET
TO
BE
SET
AT
THE
SAME
TIME
IN
ADDITION
THERE
IS
A
SYSTEM
CALL
THAT
TAKES
AS
AN
ARGUMENT
A
LIST
OF
SEMAPHORE
OPERATIONS
EACH
DEFINED
ON
ONE
OF
THE
SEMAPHORES
IN
A
SET
WHEN
THIS
CALL
IS
MADE
THE
KERNEL
PERFORMS
THE
INDICATED
OPERATIONS
ONE
AT
A
TIME
FOR
EACH
OPERATION
THE
ACTUAL
FUNCTION
IS
SPECI
FIED
BY
THE
VALUE
THE
FOLLOWING
ARE
THE
POSSIBILITIES
IF
IS
POSITIVE
THE
KERNEL
INCREMENTS
THE
VALUE
OF
THE
SEMAPHORE
AND
AWAKENS
ALL
PROCESSES
WAITING
FOR
THE
VALUE
OF
THE
SEMAPHORE
TO
INCREASE
IF
IS
THE
KERNEL
CHECKS
THE
SEMAPHORE
VALUE
IF
THE
SEMAPHORE
VALUE
EQUALS
THE
KERNEL
CONTINUES
WITH
THE
OTHER
OPERATIONS
ON
THE
LIST
OTHERWISE
THE
KERNEL
INCREMENTS
THE
NUMBER
OF
PROCESSES
WAITING
FOR
THIS
SEMAPHORE
TO
BE
AND
SUSPENDS
THE
PROCESS
TO
WAIT
FOR
THE
EVENT
THAT
THE
VALUE
OF
THE
SEMAPHORE
EQUALS
IF
IS
NEGATIVE
AND
ITS
ABSOLUTE
VALUE
IS
LESS
THAN
OR
EQUAL
TO
THE
SEMA
PHORE
VALUE
THE
KERNEL
ADDS
A
NEGATIVE
NUMBER
TO
THE
SEMAPHORE
VALUE
IF
THE
RESULT
IS
THE
KERNEL
AWAKENS
ALL
PROCESSES
WAITING
FOR
THE
VALUE
OF
THE
SEMAPHORE
TO
EQUAL
IF
IS
NEGATIVE
AND
ITS
ABSOLUTE
VALUE
IS
GREATER
THAN
THE
SEMAPHORE
VALUE
THE
KERNEL
SUSPENDS
THE
PROCESS
ON
THE
EVENT
THAT
THE
VALUE
OF
THE
SEMA
PHORE
INCREASES
THIS
GENERALIZATION
OF
THE
SEMAPHORE
PROVIDES
CONSIDERABLE
FLEXIBILITY
IN
PER
FORMING
PROCESS
SYNCHRONIZATION
AND
COORDINATION
SIGNALS
A
SIGNAL
IS
A
SOFTWARE
MECHANISM
THAT
INFORMS
A
PROCESS
OF
THE
OCCURRENCE
OF
ASYN
CHRONOUS
EVENTS
A
SIGNAL
IS
SIMILAR
TO
A
HARDWARE
INTERRUPT
BUT
DOES
NOT
EMPLOY
PRIORITIES
THAT
IS
ALL
SIGNALS
ARE
TREATED
EQUALLY
SIGNALS
THAT
OCCUR
AT
THE
SAME
TIME
ARE
PRESENTED
TO
A
PROCESS
ONE
AT
A
TIME
WITH
NO
PARTICULAR
ORDERING
PROCESSES
MAY
SEND
EACH
OTHER
SIGNALS
OR
THE
KERNEL
MAY
SEND
SIGNALS
INTER
NALLY
A
SIGNAL
IS
DELIVERED
BY
UPDATING
A
FIELD
IN
THE
PROCESS
TABLE
FOR
THE
PROCESS
TO
WHICH
THE
SIGNAL
IS
BEING
SENT
BECAUSE
EACH
SIGNAL
IS
MAINTAINED
AS
A
SINGLE
BIT
SIGNALS
OF
A
GIVEN
TYPE
CANNOT
BE
QUEUED
A
SIGNAL
IS
PROCESSED
JUST
AFTER
A
PROCESS
WAKES
UP
TO
RUN
OR
WHENEVER
THE
PROCESS
IS
PREPARING
TO
RETURN
FROM
A
SYSTEM
CALL
A
PROCESS
MAY
RESPOND
TO
A
SIGNAL
BY
PERFORMING
SOME
DEFAULT
ACTION
E
G
TERMINA
TION
EXECUTING
A
SIGNAL
HANDLER
FUNCTION
OR
IGNORING
THE
SIGNAL
TABLE
LISTS
SIGNALS
DEFINED
FOR
UNIX
LINUX
KERNEL
CONCURRENCY
MECHANISMS
TABLE
UNIX
SIGNALS
VALUE
NAME
DESCRIPTION
SIGHUP
HANG
UP
SENT
TO
PROCESS
WHEN
KERNEL
ASSUMES
THAT
THE
USER
OF
THAT
PROCESS
IS
DOING
NO
USEFUL
WORK
SIGINT
INTERRUPT
SIGQUIT
QUIT
SENT
BY
USER
TO
INDUCE
HALTING
OF
PROCESS
AND
PRODUCTION
OF
CORE
DUMP
SIGILL
ILLEGAL
INSTRUCTION
SIGTRAP
TRACE
TRAP
TRIGGERS
THE
EXECUTION
OF
CODE
FOR
PROCESS
TRACING
SIGIOT
IOT
INSTRUCTION
SIGEMT
EMT
INSTRUCTION
SIGFPE
FLOATING
POINT
EXCEPTION
SIGKILL
KILL
TERMINATE
PROCESS
SIGBUS
BUS
ERROR
SIGSEGV
SEGMENTATION
VIOLATION
PROCESS
ATTEMPTS
TO
ACCESS
LOCATION
OUTSIDE
ITS
VIRTUAL
ADDRESS
SPACE
SIGSYS
BAD
ARGUMENT
TO
SYSTEM
CALL
SIGPIPE
WRITE
ON
A
PIPE
THAT
HAS
NO
READERS
ATTACHED
TO
IT
SIGALRM
ALARM
CLOCK
ISSUED
WHEN
A
PROCESS
WISHES
TO
RECEIVE
A
SIGNAL
AFTER
A
PERIOD
OF
TIME
SIGTERM
SOFTWARE
TERMINATION
USER
DEFINED
SIGNAL
USER
DEFINED
SIGNAL
SIGCHLD
DEATH
OF
A
CHILD
SIGPWR
POWER
FAILURE
LINUX
KERNEL
CONCURRENCY
MECHANISMS
LINUX
INCLUDES
ALL
OF
THE
CONCURRENCY
MECHANISMS
FOUND
IN
OTHER
UNIX
SYSTEMS
SUCH
AS
INCLUDING
PIPES
MESSAGES
SHARED
MEMORY
AND
SIGNALS
IN
ADDI
TION
LINUX
INCLUDES
A
RICH
SET
OF
CONCURRENCY
MECHANISMS
SPECIFICALLY
INTENDED
FOR
USE
WHEN
A
THREAD
IS
EXECUTING
IN
KERNEL
MODE
THAT
IS
THESE
ARE
MECHANISMS
USED
WITHIN
THE
KERNEL
TO
PROVIDE
CONCURRENCY
IN
THE
EXECUTION
OF
KERNEL
CODE
THIS
SECTION
EXAMINES
THE
LINUX
KERNEL
CONCURRENCY
MECHANISMS
ATOMIC
OPERATIONS
LINUX
PROVIDES
A
SET
OF
OPERATIONS
THAT
GUARANTEE
ATOMIC
OPERATIONS
ON
A
VARIABLE
THESE
OPERATIONS
CAN
BE
USED
TO
AVOID
SIMPLE
RACE
CONDITIONS
AN
ATOMIC
OPERATION
EXECUTES
WITHOUT
INTERRUPTION
AND
WITHOUT
INTERFERENCE
ON
A
UNIPROCESSOR
SYSTEM
A
THREAD
PERFORMING
AN
ATOMIC
OPERATION
CANNOT
BE
INTERRUPTED
ONCE
THE
OPERATION
HAS
STARTED
UNTIL
THE
OPERATION
IS
FINISHED
IN
ADDITION
ON
A
MULTIPROCESSOR
SYSTEM
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
THE
VARIABLE
BEING
OPERATED
ON
IS
LOCKED
FROM
ACCESS
BY
OTHER
THREADS
UNTIL
THIS
OPER
ATION
IS
COMPLETED
TWO
TYPES
OF
ATOMIC
OPERATIONS
ARE
DEFINED
IN
LINUX
INTEGER
OPERATIONS
WHICH
OPERATE
ON
AN
INTEGER
VARIABLE
AND
BITMAP
OPERATIONS
WHICH
OPERATE
ON
ONE
BIT
IN
A
BITMAP
TABLE
THESE
OPERATIONS
MUST
BE
IMPLEMENTED
ON
ANY
ARCHITECTURE
THAT
IMPLEMENTS
LINUX
FOR
SOME
ARCHITECTURES
THERE
ARE
COR
RESPONDING
ASSEMBLY
LANGUAGE
INSTRUCTIONS
FOR
THE
ATOMIC
OPERATIONS
ON
OTHER
ARCHITECTURES
AN
OPERATION
THAT
LOCKS
THE
MEMORY
BUS
IS
USED
TO
GUARANTEE
THAT
THE
OPERATION
IS
ATOMIC
FOR
ATOMIC
INTEGER
OPERATIONS
A
SPECIAL
DATA
TYPE
IS
USED
THE
ATOMIC
INTEGER
OPERATIONS
CAN
BE
USED
ONLY
ON
THIS
DATA
TYPE
AND
NO
OTHER
OPERATIONS
TABLE
LINUX
ATOMIC
OPERATIONS
ATOMIC
INTEGER
OPERATIONS
INT
I
AT
DECLARATION
INITIALIZE
AN
TO
I
INT
V
READ
INTEGER
VALUE
OF
V
VOID
V
INT
I
SET
THE
VALUE
OF
V
TO
INTEGER
I
VOID
INT
I
V
ADD
I
TO
V
VOID
INT
I
V
SUBTRACT
I
FROM
V
VOID
V
ADD
TO
V
VOID
V
SUBTRACT
FROM
V
INT
INT
I
V
SUBTRACT
I
FROM
V
RETURN
IF
THE
RESULT
IS
ZERO
RETURN
OTHERWISE
INT
INT
I
V
ADD
I
TO
V
RETURN
IF
THE
RESULT
IS
NEGATIVE
RETURN
OTHERWISE
USED
FOR
IMPLEMENTING
SEMAPHORES
INT
V
SUBTRACT
FROM
V
RETURN
IF
THE
RESULT
IS
ZERO
RETURN
OTHERWISE
INT
V
ADD
TO
V
RETURN
IF
THE
RESULT
IS
ZERO
RETURN
OTHERWISE
ATOMIC
BITMAP
OPERATIONS
VOID
INT
NR
VOID
ADDR
SET
BIT
NR
IN
THE
BITMAP
POINTED
TO
BY
ADDR
VOID
INT
NR
VOID
ADDR
CLEAR
BIT
NR
IN
THE
BITMAP
POINTED
TO
BY
ADDR
VOID
INT
NR
VOID
ADDR
INVERT
BIT
NR
IN
THE
BITMAP
POINTED
TO
BY
ADDR
INT
INT
NR
VOID
ADDR
SET
BIT
NR
IN
THE
BITMAP
POINTED
TO
BY
ADDR
RETURN
THE
OLD
BIT
VALUE
INT
INT
NR
VOID
ADDR
CLEAR
BIT
NR
IN
THE
BITMAP
POINTED
TO
BY
ADDR
RETURN
THE
OLD
BIT
VALUE
INT
INT
NR
VOID
ADDR
INVERT
BIT
NR
IN
THE
BITMAP
POINTED
TO
BY
ADDR
RETURN
THE
OLD
BIT
VALUE
INT
INT
NR
VOID
ADDR
RETURN
THE
VALUE
OF
BIT
NR
IN
THE
BITMAP
POINTED
TO
BY
ADDR
LINUX
KERNEL
CONCURRENCY
MECHANISMS
ARE
ALLOWED
ON
THIS
DATA
TYPE
LISTS
THE
FOLLOWING
ADVANTAGES
FOR
THESE
RESTRICTIONS
THE
ATOMIC
OPERATIONS
ARE
NEVER
USED
ON
VARIABLES
THAT
MIGHT
IN
SOME
CIRCUM
STANCES
BE
UNPROTECTED
FROM
RACE
CONDITIONS
VARIABLES
OF
THIS
DATA
TYPE
ARE
PROTECTED
FROM
IMPROPER
USE
BY
NONATOMIC
OPERATIONS
THE
COMPILER
CANNOT
ERRONEOUSLY
OPTIMIZE
ACCESS
TO
THE
VALUE
E
G
BY
USING
AN
ALIAS
RATHER
THAN
THE
CORRECT
MEMORY
ADDRESS
THIS
DATA
TYPE
SERVES
TO
HIDE
ARCHITECTURE
SPECIFIC
DIFFERENCES
IN
ITS
IMPLE
MENTATION
A
TYPICAL
USE
OF
THE
ATOMIC
INTEGER
DATA
TYPE
IS
TO
IMPLEMENT
COUNTERS
THE
ATOMIC
BITMAP
OPERATIONS
OPERATE
ON
ONE
OF
A
SEQUENCE
OF
BITS
AT
AN
ARBI
TRARY
MEMORY
LOCATION
INDICATED
BY
A
POINTER
VARIABLE
THUS
THERE
IS
NO
EQUIVALENT
TO
THE
DATA
TYPE
NEEDED
FOR
ATOMIC
INTEGER
OPERATIONS
ATOMIC
OPERATIONS
ARE
THE
SIMPLEST
OF
THE
APPROACHES
TO
KERNEL
SYNCHRONIZA
TION
MORE
COMPLEX
LOCKING
MECHANISMS
CAN
BE
BUILT
ON
TOP
OF
THEM
SPINLOCKS
THE
MOST
COMMON
TECHNIQUE
USED
FOR
PROTECTING
A
CRITICAL
SECTION
IN
LINUX
IS
THE
SPIN
LOCK
ONLY
ONE
THREAD
AT
A
TIME
CAN
ACQUIRE
A
SPINLOCK
ANY
OTHER
THREAD
ATTEMPTING
TO
ACQUIRE
THE
SAME
LOCK
WILL
KEEP
TRYING
SPINNING
UNTIL
IT
CAN
ACQUIRE
THE
LOCK
IN
ESSENCE
A
SPINLOCK
IS
BUILT
ON
AN
INTEGER
LOCATION
IN
MEMORY
THAT
IS
CHECKED
BY
EACH
THREAD
BEFORE
IT
ENTERS
ITS
CRITICAL
SECTION
IF
THE
VALUE
IS
THE
THREAD
SETS
THE
VALUE
TO
AND
ENTERS
ITS
CRITICAL
SECTION
IF
THE
VALUE
IS
NONZERO
THE
THREAD
CONTINUALLY
CHECKS
THE
VALUE
UNTIL
IT
IS
ZERO
THE
SPINLOCK
IS
EASY
TO
IMPLEMENT
BUT
HAS
THE
DISADVANTAGE
THAT
LOCKED
OUT
THREADS
CONTINUE
TO
EXECUTE
IN
A
BUSY
WAITING
MODE
THUS
SPINLOCKS
ARE
MOST
EFFECTIVE
IN
SITUATIONS
WHERE
THE
WAIT
TIME
FOR
ACQUIRING
A
LOCK
IS
EXPECTED
TO
BE
VERY
SHORT
SAY
ON
THE
ORDER
OF
LESS
THAN
TWO
CONTEXT
CHANGES
THE
BASIC
FORM
OF
USE
OF
A
SPINLOCK
IS
THE
FOLLOWING
LOCK
CRITICAL
SECTION
LOCK
BASIC
SPINLOCKS
THE
BASIC
SPINLOCK
AS
OPPOSED
TO
THE
READER
WRITER
SPINLOCK
EXPLAINED
SUBSEQUENTLY
COMES
IN
FOUR
FLAVORS
TABLE
PLAIN
IF
THE
CRITICAL
SECTION
OF
CODE
IS
NOT
EXECUTED
BY
INTERRUPT
HANDLERS
OR
IF
THE
INTERRUPTS
ARE
DISABLED
DURING
THE
EXECUTION
OF
THE
CRITICAL
SECTION
THEN
THE
PLAIN
SPINLOCK
CAN
BE
USED
IT
DOES
NOT
AFFECT
THE
INTERRUPT
STATE
ON
THE
PROCESSOR
ON
WHICH
IT
IS
RUN
IF
INTERRUPTS
ARE
ALWAYS
ENABLED
THEN
THIS
SPINLOCK
SHOULD
BE
USED
IF
IT
IS
NOT
KNOWN
IF
INTERRUPTS
WILL
BE
ENABLED
OR
DISABLED
AT
THE
TIME
OF
EXECUTION
THEN
THIS
VERSION
SHOULD
BE
USED
WHEN
A
LOCK
IS
ACQUIRED
THE
CUR
RENT
STATE
OF
INTERRUPTS
ON
THE
LOCAL
PROCESSOR
IS
SAVED
TO
BE
RESTORED
WHEN
THE
LOCK
IS
RELEASED
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
TABLE
LINUX
SPINLOCKS
VOID
LOCK
ACQUIRES
THE
SPECIFIED
LOCK
SPINNING
IF
NEEDED
UNTIL
IT
IS
AVAILABLE
VOID
LOCK
LIKE
BUT
ALSO
DISABLES
INTERRUPTS
ON
THE
LOCAL
PROCESSOR
VOID
LOCK
UNSIGNED
LONG
FLAGS
LIKE
BUT
ALSO
SAVES
THE
CURRENT
INTERRUPT
STATE
IN
FLAGS
VOID
LOCK
LIKE
BUT
ALSO
DISABLES
THE
EXECUTION
OF
ALL
BOTTOM
HALVES
VOID
LOCK
RELEASES
GIVEN
LOCK
VOID
LOCK
RELEASES
GIVEN
LOCK
AND
ENABLES
LOCAL
INTERRUPTS
VOID
LOCK
UNSIGNED
LONG
FLAGS
RELEASES
GIVEN
LOCK
AND
RESTORES
LOCAL
INTERRUPTS
TO
GIVEN
PREVIOUS
STATE
VOID
LOCK
RELEASES
GIVEN
LOCK
AND
ENABLES
BOTTOM
HALVES
VOID
LOCK
INITIALIZES
GIVEN
SPINLOCK
INT
LOCK
TRIES
TO
ACQUIRE
SPECIFIED
LOCK
RETURNS
NONZERO
IF
LOCK
IS
CURRENTLY
HELD
AND
ZERO
OTHERWISE
INT
LOCK
RETURNS
NONZERO
IF
LOCK
IS
CURRENTLY
HELD
AND
ZERO
OTHERWISE
WHEN
AN
INTERRUPT
OCCURS
THE
MINIMUM
AMOUNT
OF
WORK
NECESSARY
IS
PERFORMED
BY
THE
CORRESPONDING
INTERRUPT
HANDLER
A
PIECE
OF
CODE
CALLED
THE
BOTTOM
HALF
PERFORMS
THE
REMAINDER
OF
THE
INTERRUPT
RELATED
WORK
ALLOW
ING
THE
CURRENT
INTERRUPT
TO
BE
ENABLED
AS
SOON
AS
POSSIBLE
THE
SPINLOCK
IS
USED
TO
DISABLE
AND
THEN
ENABLE
BOTTOM
HALVES
TO
AVOID
CONFLICT
WITH
THE
PROTECTED
CRITICAL
SECTION
THE
PLAIN
SPINLOCK
IS
USED
IF
THE
PROGRAMMER
KNOWS
THAT
THE
PROTECTED
DATA
IS
NOT
ACCESSED
BY
AN
INTERRUPT
HANDLER
OR
BOTTOM
HALF
OTHERWISE
THE
APPROPRIATE
NONPLAIN
SPINLOCK
IS
USED
SPINLOCKS
ARE
IMPLEMENTED
DIFFERENTLY
ON
A
UNIPROCESSOR
SYSTEM
VERSUS
A
MUL
TIPROCESSOR
SYSTEM
FOR
A
UNIPROCESSOR
SYSTEM
THE
FOLLOWING
CONSIDERATIONS
APPLY
IF
KERNEL
PREEMPTION
IS
TURNED
OFF
SO
THAT
A
THREAD
EXECUTING
IN
KERNEL
MODE
CANNOT
BE
INTERRUPTED
THEN
THE
LOCKS
ARE
DELETED
AT
COMPILE
TIME
THEY
ARE
NOT
NEEDED
IF
KERNEL
PREEMPTION
IS
ENABLED
WHICH
DOES
PERMIT
INTERRUPTS
THEN
THE
SPINLOCKS
AGAIN
COMPILE
AWAY
I
E
NO
TEST
OF
A
SPINLOCK
MEMORY
LOCATION
OCCURS
BUT
ARE
SIM
PLY
IMPLEMENTED
AS
CODE
THAT
ENABLES
DISABLES
INTERRUPTS
ON
A
MULTIPLE
PROCESSOR
SYSTEM
THE
SPINLOCK
IS
COMPILED
INTO
CODE
THAT
DOES
IN
FACT
TEST
THE
SPINLOCK
LOCA
TION
THE
USE
OF
THE
SPINLOCK
MECHANISM
IN
A
PROGRAM
ALLOWS
IT
TO
BE
INDEPENDENT
OF
WHETHER
IT
IS
EXECUTED
ON
A
UNIPROCESSOR
OR
MULTIPROCESSOR
SYSTEM
READER
WRITER
SPINLOCK
THE
READER
WRITER
SPINLOCK
IS
A
MECHANISM
THAT
ALLOWS
A
GREATER
DEGREE
OF
CONCURRENCY
WITHIN
THE
KERNEL
THAN
THE
BASIC
SPINLOCK
THE
READER
WRITER
SPINLOCK
ALLOWS
MULTIPLE
THREADS
TO
HAVE
SIMULTANEOUS
ACCESS
TO
THE
SAME
DATA
STRUCTURE
FOR
READING
ONLY
BUT
GIVES
EXCLUSIVE
ACCESS
TO
THE
LINUX
KERNEL
CONCURRENCY
MECHANISMS
SPINLOCK
FOR
A
THREAD
THAT
INTENDS
TO
UPDATE
THE
DATA
STRUCTURE
EACH
READER
WRITER
SPINLOCK
CONSISTS
OF
A
BIT
READER
COUNTER
AND
AN
UNLOCK
FLAG
WITH
THE
FOLLOWING
INTERPRETATION
COUNTER
FLAG
INTERPRETATION
THE
SPINLOCK
IS
RELEASED
AND
AVAILABLE
FOR
USE
SPINLOCK
HAS
BEEN
ACQUIRED
FOR
WRITING
BY
ONE
THREAD
N
N
SPINLOCK
HAS
BEEN
ACQUIRED
FOR
READING
BY
N
THREADS
N
N
NOT
VALID
AS
WITH
THE
BASIC
SPINLOCK
THERE
ARE
PLAIN
AND
VERSIONS
OF
THE
READER
WRITER
SPINLOCK
NOTE
THAT
THE
READER
WRITER
SPINLOCK
FAVORS
READERS
OVER
WRITERS
IF
THE
SPIN
LOCK
IS
HELD
FOR
READERS
THEN
SO
LONG
AS
THERE
IS
AT
LEAST
ONE
READER
THE
SPINLOCK
CANNOT
BE
PREEMPTED
BY
A
WRITER
FURTHERMORE
NEW
READERS
MAY
BE
ADDED
TO
THE
SPINLOCK
EVEN
WHILE
A
WRITER
IS
WAITING
SEMAPHORES
AT
THE
USER
LEVEL
LINUX
PROVIDES
A
SEMAPHORE
INTERFACE
CORRESPONDING
TO
THAT
IN
UNIX
INTERNALLY
LINUX
PROVIDES
AN
IMPLEMENTATION
OF
SEMAPHORES
FOR
ITS
OWN
USE
THAT
IS
CODE
THAT
IS
PART
OF
THE
KERNEL
CAN
INVOKE
KERNEL
SEMAPHORES
THESE
KERNEL
SEMAPHORES
CANNOT
BE
ACCESSED
DIRECTLY
BY
THE
USER
PROGRAM
VIA
SYS
TEM
CALLS
THEY
ARE
IMPLEMENTED
AS
FUNCTIONS
WITHIN
THE
KERNEL
AND
ARE
THUS
MORE
EFFICIENT
THAN
USER
VISIBLE
SEMAPHORES
LINUX
PROVIDES
THREE
TYPES
OF
SEMAPHORE
FACILITIES
IN
THE
KERNEL
BINARY
SEMA
PHORES
COUNTING
SEMAPHORES
AND
READER
WRITER
SEMAPHORES
BINARY
AND
COUNTING
SEMAPHORES
THE
BINARY
AND
COUNTING
SEMAPHORES
DEFINED
IN
LINUX
TABLE
HAVE
THE
SAME
FUNCTIONALITY
AS
DESCRIBED
FOR
SUCH
SEMAPHORES
IN
CHAPTER
THE
FUNCTION
NAMES
DOWN
AND
UP
ARE
USED
FOR
THE
FUNCTIONS
REFERRED
TO
IN
CHAPTER
AS
SEMWAIT
AND
SEMSIGNAL
RESPECTIVELY
A
COUNTING
SEMAPHORE
IS
INITIALIZED
USING
THE
FUNCTION
WHICH
GIVES
THE
SEMAPHORE
A
NAME
AND
ASSIGNS
AN
INITIAL
VALUE
TO
THE
SEMAPHORE
BINARY
SEMA
PHORES
CALLED
MUTEXES
IN
LINUX
ARE
INITIALIZED
USING
THE
AND
FUNCTIONS
WHICH
INITIALIZE
THE
SEMAPHORE
TO
OR
RESPECTIVELY
LINUX
PROVIDES
THREE
VERSIONS
OF
THE
DOWN
SEMWAIT
OPERATION
THE
DOWN
FUNCTION
CORRESPONDS
TO
THE
TRADITIONAL
SEMWAIT
OPERATION
THAT
IS
THE
THREAD
TESTS
THE
SEMAPHORE
AND
BLOCKS
IF
THE
SEMAPHORE
IS
NOT
AVAILABLE
THE
THREAD
WILL
AWAKEN
WHEN
A
CORRESPONDING
UP
OPERATION
ON
THIS
SEMA
PHORE
OCCURS
NOTE
THAT
THIS
FUNCTION
NAME
IS
USED
FOR
AN
OPERATION
ON
EITHER
A
COUNTING
SEMAPHORE
OR
A
BINARY
SEMAPHORE
THE
FUNCTION
ALLOWS
THE
THREAD
TO
RECEIVE
AND
RESPOND
TO
A
KERNEL
SIGNAL
WHILE
BEING
BLOCKED
ON
THE
DOWN
OPERATION
IF
THE
THREAD
IS
WOKEN
UP
BY
A
SIGNAL
THE
FUNCTION
INCRE
MENTS
THE
COUNT
VALUE
OF
THE
SEMAPHORE
AND
RETURNS
AN
ERROR
CODE
KNOWN
IN
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
LINUX
AS
EINTR
THIS
ALERTS
THE
THREAD
THAT
THE
INVOKED
SEMAPHORE
FUNCTION
HAS
ABORTED
IN
EFFECT
THE
THREAD
HAS
BEEN
FORCED
TO
GIVE
UP
THE
SEMAPHORE
THIS
FEATURE
IS
USEFUL
FOR
DEVICE
DRIVERS
AND
OTHER
SERVICES
IN
WHICH
IT
IS
CONVE
NIENT
TO
OVERRIDE
A
SEMAPHORE
OPERATION
THE
FUNCTION
MAKES
IT
POSSIBLE
TO
TRY
TO
ACQUIRE
A
SEMAPHORE
WITHOUT
BEING
BLOCKED
IF
THE
SEMAPHORE
IS
AVAILABLE
IT
IS
ACQUIRED
OTHERWISE
THIS
FUNCTION
RETURNS
A
NONZERO
VALUE
WITHOUT
BLOCKING
THE
THREAD
READER
WRITER
SEMAPHORES
THE
READER
WRITER
SEMAPHORE
DIVIDES
USERS
INTO
READERS
AND
WRITERS
IT
ALLOWS
MULTIPLE
CONCURRENT
READERS
WITH
NO
WRITERS
BUT
ONLY
A
SINGLE
WRITER
WITH
NO
CONCURRENT
READERS
IN
EFFECT
THE
SEMAPHORE
FUNCTIONS
AS
A
COUNTING
SEMAPHORE
FOR
READERS
BUT
A
BINARY
SEMAPHORE
MUTEX
FOR
WRITERS
TABLE
SHOWS
THE
BASIC
READER
WRITER
SEMAPHORE
OPERATIONS
THE
READER
WRITER
SEMAPHORE
USES
UNINTERRUPTIBLE
SLEEP
SO
THERE
IS
ONLY
ONE
VERSION
OF
EACH
OF
THE
DOWN
OPERATIONS
TABLE
LINUX
SEMAPHORES
TRADITIONAL
SEMAPHORES
VOID
STRUCT
SEMAPHORE
SEM
INT
COUNT
INITIALIZES
THE
DYNAMICALLY
CREATED
SEMAPHORE
TO
THE
GIVEN
COUNT
VOID
STRUCT
SEMAPHORE
SEM
INITIALIZES
THE
DYNAMICALLY
CREATED
SEMAPHORE
WITH
A
COUNT
OF
INITIALLY
UNLOCKED
VOID
STRUCT
SEMA
PHORE
SEM
INITIALIZES
THE
DYNAMICALLY
CREATED
SEMAPHORE
WITH
A
COUNT
OF
INITIALLY
LOCKED
VOID
DOWN
STRUCT
SEMAPHORE
SEM
ATTEMPTS
TO
ACQUIRE
THE
GIVEN
SEMAPHORE
ENTERING
UNINTERRUPTIBLE
SLEEP
IF
SEMAPHORE
IS
UNAVAILABLE
INT
STRUCT
SEMAPHORE
SEM
ATTEMPTS
TO
ACQUIRE
THE
GIVEN
SEMAPHORE
ENTER
ING
INTERRUPTIBLE
SLEEP
IF
SEMAPHORE
IS
UNAVAILABLE
RETURNS
EINTR
VALUE
IF
A
SIGNAL
OTHER
THAN
THE
RESULT
OF
AN
UP
OPERATION
IS
RECEIVED
INT
STRUCT
SEMAPHORE
SEM
ATTEMPTS
TO
ACQUIRE
THE
GIVEN
SEMAPHORE
AND
RETURNS
A
NONZERO
VALUE
IF
SEMAPHORE
IS
UNAVAILABLE
VOID
UP
STRUCT
SEMAPHORE
SEM
RELEASES
THE
GIVEN
SEMAPHORE
READER
WRITER
SEMAPHORES
VOID
STRUCT
RWSEM
INITIALIZES
THE
DYNAMICALLY
CREATED
SEMAPHORE
WITH
A
COUNT
OF
VOID
STRUCT
RWSEM
DOWN
OPERATION
FOR
READERS
VOID
STRUCT
RWSEM
UP
OPERATION
FOR
READERS
VOID
STRUCT
RWSEM
DOWN
OPERATION
FOR
WRITERS
VOID
STRUCT
RWSEM
UP
OPERATION
FOR
WRITERS
LINUX
KERNEL
CONCURRENCY
MECHANISMS
BARRIERS
IN
SOME
ARCHITECTURES
COMPILERS
AND
OR
THE
PROCESSOR
HARDWARE
MAY
REORDER
MEM
ORY
ACCESSES
IN
SOURCE
CODE
TO
OPTIMIZE
PERFORMANCE
THESE
REORDERINGS
ARE
DONE
TO
OPTIMIZE
THE
USE
OF
THE
INSTRUCTION
PIPELINE
IN
THE
PROCESSOR
THE
REORDERING
ALGORITHMS
CONTAIN
CHECKS
TO
ENSURE
THAT
DATA
DEPENDENCIES
ARE
NOT
VIOLATED
FOR
EXAMPLE
THE
CODE
A
B
MAY
BE
REORDERED
SO
THAT
MEMORY
LOCATION
B
IS
UPDATED
BEFORE
MEMORY
LOCATION
A
IS
UPDATED
HOWEVER
THE
CODE
A
B
A
WILL
NOT
BE
REORDERED
EVEN
SO
THERE
ARE
OCCASIONS
WHEN
IT
IS
IMPORTANT
THAT
READS
OR
WRITES
ARE
EXECUTED
IN
THE
ORDER
SPECIFIED
BECAUSE
OF
USE
OF
THE
INFORMATION
THAT
IS
MADE
BY
ANOTHER
THREAD
OR
A
HARDWARE
DEVICE
TO
ENFORCE
THE
ORDER
IN
WHICH
INSTRUCTIONS
ARE
EXECUTED
LINUX
PROVIDES
THE
MEMORY
BARRIER
FACILITY
TABLE
LISTS
THE
MOST
IMPORTANT
FUNCTIONS
THAT
ARE
DEFINED
FOR
THIS
FACILITY
THE
RMB
OPERATION
INSURES
THAT
NO
READS
OCCUR
ACROSS
THE
BAR
RIER
DEFINED
BY
THE
PLACE
OF
THE
RMB
IN
THE
CODE
SIMILARLY
THE
WMB
OPERATION
INSURES
THAT
NO
WRITES
OCCUR
ACROSS
THE
BARRIER
DEFINED
BY
THE
PLACE
OF
THE
WMB
IN
THE
CODE
THE
MB
OPERATION
PROVIDES
BOTH
A
LOAD
AND
STORE
BARRIER
TWO
IMPORTANT
POINTS
TO
NOTE
ABOUT
THE
BARRIER
OPERATIONS
THE
BARRIERS
RELATE
TO
MACHINE
INSTRUCTIONS
NAMELY
LOADS
AND
STORES
THUS
THE
HIGHER
LEVEL
LANGUAGE
INSTRUCTION
A
B
INVOLVES
BOTH
A
LOAD
READ
FROM
LOCA
TION
B
AND
A
STORE
WRITE
TO
LOCATION
A
THE
RMB
WMB
AND
MB
OPERATIONS
DICTATE
THE
BEHAVIOR
OF
BOTH
THE
COMPILER
AND
THE
PROCESSOR
IN
THE
CASE
OF
THE
COMPILER
THE
BARRIER
OPERATION
DICTATES
THAT
THE
COMPILER
NOT
REORDER
INSTRUCTIONS
DURING
THE
COMPILE
PROCESS
IN
THE
CASE
OF
THE
PROCESSOR
THE
BARRIER
OPERATION
DICTATES
THAT
ANY
INSTRUCTIONS
PEND
ING
IN
THE
PIPELINE
BEFORE
THE
BARRIER
MUST
BE
COMMITTED
FOR
EXECUTION
BEFORE
ANY
INSTRUCTIONS
ENCOUNTERED
AFTER
THE
BARRIER
TABLE
LINUX
MEMORY
BARRIER
OPERATIONS
RMB
PREVENTS
LOADS
FROM
BEING
REORDERED
ACROSS
THE
BARRIER
WMB
PREVENTS
STORES
FROM
BEING
REORDERED
ACROSS
THE
BARRIER
MB
PREVENTS
LOADS
AND
STORES
FROM
BEING
REORDERED
ACROSS
THE
BARRIER
BARRIER
PREVENTS
THE
COMPILER
FROM
REORDERING
LOADS
OR
STORES
ACROSS
THE
BARRIER
ON
SMP
PROVIDES
A
RMB
AND
ON
UP
PROVIDES
A
BARRIER
ON
SMP
PROVIDES
A
WMB
AND
ON
UP
PROVIDES
A
BARRIER
ON
SMP
PROVIDES
A
MB
AND
ON
UP
PROVIDES
A
BARRIER
NOTE
SMP
SYMMETRIC
MULTIPROCESSOR
UP
UNIPROCESSOR
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
THE
BARRIER
OPERATION
IS
A
LIGHTER
WEIGHT
VERSION
OF
THE
MB
OPERATION
IN
THAT
IT
ONLY
CONTROLS
THE
COMPILER
BEHAVIOR
THIS
WOULD
BE
USEFUL
IF
IT
IS
KNOWN
THAT
THE
PROCESSOR
WILL
NOT
PERFORM
UNDESIRABLE
REORDERINGS
FOR
EXAMPLE
THE
INTEL
PROCESSORS
DO
NOT
REORDER
WRITES
THE
AND
OPERATIONS
PROVIDE
AN
OPTIMIZATION
FOR
CODE
THAT
MAY
BE
COMPILED
ON
EITHER
A
UNIPROCESSOR
UP
OR
A
SYMMETRIC
MULTIPROC
ESSOR
SMP
THESE
INSTRUCTIONS
ARE
DEFINED
AS
THE
USUAL
MEMORY
BARRIERS
FOR
AN
SMP
BUT
FOR
A
UP
THEY
ARE
ALL
TREATED
ONLY
AS
COMPILER
BARRIERS
THE
OPERA
TIONS
ARE
USEFUL
IN
SITUATIONS
IN
WHICH
THE
DATA
DEPENDENCIES
OF
CONCERN
WILL
ONLY
ARISE
IN
AN
SMP
CONTEXT
SOLARIS
THREAD
SYNCHRONIZATION
PRIMITIVES
IN
ADDITION
TO
THE
CONCURRENCY
MECHANISMS
OF
UNIX
SOLARIS
SUPPORTS
FOUR
THREAD
SYNCHRONIZATION
PRIMITIVES
MUTUAL
EXCLUSION
MUTEX
LOCKS
SEMAPHORES
MULTIPLE
READERS
SINGLE
WRITER
READERS
WRITER
LOCKS
CONDITION
VARIABLES
SOLARIS
IMPLEMENTS
THESE
PRIMITIVES
WITHIN
THE
KERNEL
FOR
KERNEL
THREADS
THEY
ARE
ALSO
PROVIDED
IN
THE
THREADS
LIBRARY
FOR
USER
LEVEL
THREADS
FIGURE
SHOWS
THE
DATA
STRUCTURES
FOR
THESE
PRIMITIVES
THE
INITIALIZATION
FUNCTIONS
FOR
THE
PRIMI
TIVES
FILL
IN
SOME
OF
THE
DATA
MEMBERS
ONCE
A
SYNCHRONIZATION
OBJECT
IS
CREATED
THERE
ARE
ESSENTIALLY
ONLY
TWO
OPERATIONS
THAT
CAN
BE
PERFORMED
ENTER
ACQUIRE
LOCK
AND
RELEASE
UNLOCK
THERE
ARE
NO
MECHANISMS
IN
THE
KERNEL
OR
THE
THREADS
LIBRARY
TO
ENFORCE
MUTUAL
EXCLUSION
OR
TO
PREVENT
DEADLOCK
IF
A
THREAD
ATTEMPTS
TO
ACCESS
A
PIECE
OF
DATA
OR
CODE
THAT
IS
SUPPOSED
TO
BE
PROTECTED
BUT
DOES
NOT
USE
THE
APPROPRIATE
SYNCHRONIZATION
PRIMITIVE
THEN
SUCH
ACCESS
OCCURS
IF
A
THREAD
LOCKS
AN
OBJECT
AND
THEN
FAILS
TO
UNLOCK
IT
NO
KERNEL
ACTION
IS
TAKEN
ALL
OF
THE
SYNCHRONIZATION
PRIMITIVES
REQUIRE
THE
EXISTENCE
OF
A
HARDWARE
INSTRUCTION
THAT
ALLOWS
AN
OBJECT
TO
BE
TESTED
AND
SET
IN
ONE
ATOMIC
OPERATION
MUTUAL
EXCLUSION
LOCK
A
MUTEX
IS
USED
TO
ENSURE
THAT
ONLY
ONE
THREAD
AT
A
TIME
CAN
ACCESS
THE
RESOURCE
PROTECTED
BY
THE
MUTEX
THE
THREAD
THAT
LOCKS
THE
MUTEX
MUST
BE
THE
ONE
THAT
UNLOCKS
IT
A
THREAD
ATTEMPTS
TO
ACQUIRE
A
MUTEX
LOCK
BY
EXECUTING
THE
ENTER
PRIMITIVE
IF
CANNOT
SET
THE
LOCK
BECAUSE
IT
IS
ALREADY
SET
BY
ANOTHER
THREAD
THE
BLOCKING
ACTION
DEPENDS
ON
TYPE
SPECIFIC
INFORMATION
STORED
IN
THE
MUTEX
OBJECT
THE
DEFAULT
BLOCKING
POLICY
IS
A
SPINLOCK
A
BLOCKED
THREAD
POLLS
THE
STATUS
OF
THE
LOCK
WHILE
EXECUTING
IN
A
BUSY
WAITING
LOOP
AN
INTERRUPT
BASED
BLOCKING
MECHANISM
IS
OPTIONAL
IN
THIS
LATTER
CASE
THE
MUTEX
INCLUDES
A
TURNSTILE
ID
THAT
IDENTIFIES
A
QUEUE
OF
THREADS
SLEEPING
ON
THIS
LOCK
SOLARIS
THREAD
SYNCHRONIZATION
PRIMITIVES
OWNER
OCTETS
LOCK
OCTET
WAITERS
OCTETS
TYPE
SPECIFIC
INFO
OCTETS
POSSIBLY
A
TURNSTILE
ID
LOCK
TYPE
FILLER
OR
STATISTICS
POINTER
A
MUTEX
LOCK
C
READER
WRITER
LOCK
WAITERS
OCTETS
D
CONDITION
VARIABLE
B
SEMAPHORE
FIGURE
SOLARIS
SYNCHRONIZATION
DATA
STRUCTURES
THE
OPERATIONS
ON
A
MUTEX
LOCK
ARE
ACQUIRES
THE
LOCK
POTENTIALLY
BLOCKING
IF
IT
IS
ALREADY
HELD
RELEASES
THE
LOCK
POTENTIALLY
UNBLOCKING
A
WAITER
ACQUIRES
THE
LOCK
IF
IT
IS
NOT
ALREADY
HELD
THE
PRIMITIVE
PROVIDES
A
NONBLOCKING
WAY
OF
PERFORMING
THE
MUTUAL
EXCLUSION
FUNCTION
THIS
ENABLES
THE
PROGRAMMER
TO
USE
A
BUSY
WAIT
APPROACH
FOR
USER
LEVEL
THREADS
WHICH
AVOIDS
BLOCKING
THE
ENTIRE
PROCESS
BECAUSE
ONE
THREAD
IS
BLOCKED
SEMAPHORES
SOLARIS
PROVIDES
CLASSIC
COUNTING
SEMAPHORES
WITH
THE
FOLLOWING
PRIMITIVES
DECREMENTS
THE
SEMAPHORE
POTENTIALLY
BLOCKING
THE
THREAD
INCREMENTS
THE
SEMAPHORE
POTENTIALLY
UNBLOCKING
A
WAITING
THREAD
DECREMENTS
THE
SEMAPHORE
IF
BLOCKING
IS
NOT
REQUIRED
AGAIN
THE
PRIMITIVE
PERMITS
BUSY
WAITING
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
READERS
WRITER
LOCK
THE
READERS
WRITER
LOCK
ALLOWS
MULTIPLE
THREADS
TO
HAVE
SIMULTANEOUS
READ
ONLY
ACCESS
TO
AN
OBJECT
PROTECTED
BY
THE
LOCK
IT
ALSO
ALLOWS
A
SINGLE
THREAD
TO
ACCESS
THE
OBJECT
FOR
WRITING
AT
ONE
TIME
WHILE
EXCLUDING
ALL
READERS
WHEN
THE
LOCK
IS
ACQUIRED
FOR
WRITING
IT
TAKES
ON
THE
STATUS
OF
WRITE
LOCK
ALL
THREADS
ATTEMPTING
ACCESS
FOR
READING
OR
WRITING
MUST
WAIT
IF
ONE
OR
MORE
READERS
HAVE
ACQUIRED
THE
LOCK
ITS
STATUS
IS
READ
LOCK
THE
PRIMITIVES
ARE
AS
FOLLOWS
ATTEMPTS
TO
ACQUIRE
A
LOCK
AS
READER
OR
WRITER
RELEASES
A
LOCK
AS
READER
OR
WRITER
ACQUIRES
THE
LOCK
IF
BLOCKING
IS
NOT
REQUIRED
A
THREAD
THAT
HAS
ACQUIRED
A
WRITE
LOCK
CONVERTS
IT
TO
A
READ
LOCK
ANY
WAITING
WRITER
REMAINS
WAITING
UNTIL
THIS
THREAD
RELEASES
THE
LOCK
IF
THERE
ARE
NO
WAITING
WRITERS
THE
PRIMITIVE
WAKES
UP
ANY
PENDING
READERS
ATTEMPTS
TO
CONVERT
A
READER
LOCK
INTO
A
WRITER
LOCK
CONDITION
VARIABLES
A
CONDITION
VARIABLE
IS
USED
TO
WAIT
UNTIL
A
PARTICULAR
CONDITION
IS
TRUE
CONDITION
VARIABLES
MUST
BE
USED
IN
CONJUNCTION
WITH
A
MUTEX
LOCK
THIS
IMPLEMENTS
A
MONITOR
OF
THE
TYPE
ILLUSTRATED
IN
FIGURE
THE
PRIMITIVES
ARE
AS
FOLLOWS
BLOCKS
UNTIL
THE
CONDITION
IS
SIGNALED
WAKES
UP
ONE
OF
THE
THREADS
BLOCKED
IN
WAKES
UP
ALL
OF
THE
THREADS
BLOCKED
IN
RELEASES
THE
ASSOCIATED
MUTEX
BEFORE
BLOCKING
AND
REACQUIRES
IT
BEFORE
RETURNING
BECAUSE
REACQUISITION
OF
THE
MUTEX
MAY
BE
BLOCKED
BY
OTHER
THREADS
WAITING
FOR
THE
MUTEX
THE
CONDITION
THAT
CAUSED
THE
WAIT
MUST
BE
RETESTED
THUS
TYPICAL
USAGE
IS
AS
FOLLOWS
M
WHILE
CV
M
M
THIS
ALLOWS
THE
CONDITION
TO
BE
A
COMPLEX
EXPRESSION
BECAUSE
IT
IS
PROTECTED
BY
THE
MUTEX
WINDOWS
CONCURRENCY
MECHANISMS
WINDOWS
PROVIDES
SYNCHRONIZATION
AMONG
THREADS
AS
PART
OF
THE
OBJECT
ARCHITECTURE
THE
MOST
IMPORTANT
METHODS
OF
SYNCHRONIZATION
ARE
EXECUTIVE
DISPATCHER
OBJECTS
USER
MODE
CRITICAL
SECTIONS
SLIM
READER
WRITER
LOCKS
CONDITION
VARIABLES
AND
LOCK
FREE
WINDOWS
CONCURRENCY
MECHANISMS
OPERATIONS
DISPATCHER
OBJECTS
MAKE
USE
OF
WAIT
FUNCTIONS
WE
FIRST
DESCRIBE
WAIT
FUNC
TIONS
AND
THEN
LOOK
AT
THE
SYNCHRONIZATION
METHODS
WAIT
FUNCTIONS
THE
WAIT
FUNCTIONS
ALLOW
A
THREAD
TO
BLOCK
ITS
OWN
EXECUTION
THE
WAIT
FUNCTIONS
DO
NOT
RETURN
UNTIL
THE
SPECIFIED
CRITERIA
HAVE
BEEN
MET
THE
TYPE
OF
WAIT
FUNC
TION
DETERMINES
THE
SET
OF
CRITERIA
USED
WHEN
A
WAIT
FUNCTION
IS
CALLED
IT
CHECKS
WHETHER
THE
WAIT
CRITERIA
HAVE
BEEN
MET
IF
THE
CRITERIA
HAVE
NOT
BEEN
MET
THE
CALLING
THREAD
ENTERS
THE
WAIT
STATE
IT
USES
NO
PROCESSOR
TIME
WHILE
WAITING
FOR
THE
CRITERIA
TO
BE
MET
THE
MOST
STRAIGHTFORWARD
TYPE
OF
WAIT
FUNCTION
IS
ONE
THAT
WAITS
ON
A
SINGLE
OBJECT
THE
WAITFORSINGLEOBJECT
FUNCTION
REQUIRES
A
HANDLE
TO
ONE
SYNCHRONI
ZATION
OBJECT
THE
FUNCTION
RETURNS
WHEN
ONE
OF
THE
FOLLOWING
OCCURS
THE
SPECIFIED
OBJECT
IS
IN
THE
SIGNALED
STATE
THE
TIME
OUT
INTERVAL
ELAPSES
THE
TIME
OUT
INTERVAL
CAN
BE
SET
TO
INFINITE
TO
SPECIFY
THAT
THE
WAIT
WILL
NOT
TIME
OUT
DISPATCHER
OBJECTS
THE
MECHANISM
USED
BY
THE
WINDOWS
EXECUTIVE
TO
IMPLEMENT
SYNCHRONIZATION
FACILITIES
IS
THE
FAMILY
OF
DISPATCHER
OBJECTS
WHICH
ARE
LISTED
WITH
BRIEF
DESCRIPTIONS
IN
TABLE
TABLE
WINDOWS
SYNCHRONIZATION
OBJECTS
OBJECT
TYPE
DEFINITION
SET
TO
SIGNALED
STATE
WHEN
EFFECT
ON
WAITING
THREADS
NOTIFICATION
EVENT
AN
ANNOUNCEMENT
THAT
A
SYSTEM
EVENT
HAS
OCCURRED
THREAD
SETS
THE
EVENT
ALL
RELEASED
SYNCHRONIZATION
EVENT
AN
ANNOUNCEMENT
THAT
A
SYSTEM
EVENT
HAS
OCCURRED
THREAD
SETS
THE
EVENT
ONE
THREAD
RELEASED
MUTEX
A
MECHANISM
THAT
PROVIDES
MUTUAL
EXCLUSION
CAPABILITIES
EQUIVALENT
TO
A
BINARY
SEMAPHORE
OWNING
THREAD
OR
OTHER
THREAD
RELEASES
THE
MUTEX
ONE
THREAD
RELEASED
SEMAPHORE
A
COUNTER
THAT
REGULATES
THE
NUMBER
OF
THREADS
THAT
CAN
USE
A
RESOURCE
SEMAPHORE
COUNT
DROPS
TO
ZERO
ALL
RELEASED
WAITABLE
TIMER
A
COUNTER
THAT
RECORDS
THE
PASSAGE
OF
TIME
SET
TIME
ARRIVES
OR
TIME
INTERVAL
EXPIRES
ALL
RELEASED
FILE
AN
INSTANCE
OF
AN
OPENED
FILE
OR
I
O
DEVICE
I
O
OPERATION
COMPLETES
ALL
RELEASED
PROCESS
A
PROGRAM
INVOCATION
INCLUDING
THE
ADDRESS
SPACE
AND
RESOURCES
REQUIRED
TO
RUN
THE
PROGRAM
LAST
THREAD
TERMINATES
ALL
RELEASED
THREAD
AN
EXECUTABLE
ENTITY
WITHIN
A
PROCESS
THREAD
TERMINATES
ALL
RELEASED
NOTE
SHADED
ROWS
CORRESPOND
TO
OBJECTS
THAT
EXIST
FOR
THE
SOLE
PURPOSE
OF
SYNCHRONIZATION
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
THE
FIRST
FIVE
OBJECT
TYPES
IN
THE
TABLE
ARE
SPECIFICALLY
DESIGNED
TO
SUPPORT
SYNCHRONIZATION
THE
REMAINING
OBJECT
TYPES
HAVE
OTHER
USES
BUT
ALSO
MAY
BE
USED
FOR
SYNCHRONIZATION
EACH
DISPATCHER
OBJECT
INSTANCE
CAN
BE
IN
EITHER
A
SIGNALED
OR
UNSIGNALED
STATE
A
THREAD
CAN
BE
BLOCKED
ON
AN
OBJECT
IN
AN
UNSIGNALED
STATE
THE
THREAD
IS
RELEASED
WHEN
THE
OBJECT
ENTERS
THE
SIGNALED
STATE
THE
MECHANISM
IS
STRAIGHT
FORWARD
A
THREAD
ISSUES
A
WAIT
REQUEST
TO
THE
WINDOWS
EXECUTIVE
USING
THE
HANDLE
OF
THE
SYNCHRONIZATION
OBJECT
WHEN
AN
OBJECT
ENTERS
THE
SIGNALED
STATE
THE
WINDOWS
EXECUTIVE
RELEASES
ONE
OR
ALL
OF
THE
THREAD
OBJECTS
THAT
ARE
WAITING
ON
THAT
DISPATCHER
OBJECT
THE
EVENT
OBJECT
IS
USEFUL
IN
SENDING
A
SIGNAL
TO
A
THREAD
INDICATING
THAT
A
PAR
TICULAR
EVENT
HAS
OCCURRED
FOR
EXAMPLE
IN
OVERLAPPED
INPUT
AND
OUTPUT
THE
SYSTEM
SETS
A
SPECIFIED
EVENT
OBJECT
TO
THE
SIGNALED
STATE
WHEN
THE
OVERLAPPED
OPERATION
HAS
BEEN
COMPLETED
THE
MUTEX
OBJECT
IS
USED
TO
ENFORCE
MUTUALLY
EXCLUSIVE
ACCESS
TO
A
RESOURCE
ALLOWING
ONLY
ONE
THREAD
OBJECT
AT
A
TIME
TO
GAIN
ACCESS
IT
THERE
FORE
FUNCTIONS
AS
A
BINARY
SEMAPHORE
WHEN
THE
MUTEX
OBJECT
ENTERS
THE
SIGNALED
STATE
ONLY
ONE
OF
THE
THREADS
WAITING
ON
THE
MUTEX
IS
RELEASED
MUTEXES
CAN
BE
USED
TO
SYNCHRONIZE
THREADS
RUNNING
IN
DIFFERENT
PROCESSES
LIKE
MUTEXES
SEMAPHORE
OBJECTS
MAY
BE
SHARED
BY
THREADS
IN
MULTIPLE
PROCESSES
THE
WINDOWS
SEMAPHORE
IS
A
COUNTING
SEMAPHORE
IN
ESSENCE
THE
WAITABLE
TIMER
OBJECT
SIGNALS
AT
A
CERTAIN
TIME
AND
OR
AT
REGULAR
INTERVALS
CRITICAL
SECTIONS
CRITICAL
SECTIONS
PROVIDE
A
SYNCHRONIZATION
MECHANISM
SIMILAR
TO
THAT
PROVIDED
BY
MUTEX
OBJECTS
EXCEPT
THAT
CRITICAL
SECTIONS
CAN
BE
USED
ONLY
BY
THE
THREADS
OF
A
SINGLE
PROCESS
EVENT
MUTEX
AND
SEMAPHORE
OBJECTS
CAN
ALSO
BE
USED
IN
A
SINGLE
PROCESS
APPLICATION
BUT
CRITICAL
SECTIONS
PROVIDE
A
MUCH
FASTER
MORE
EFFICIENT
MECH
ANISM
FOR
MUTUAL
EXCLUSION
SYNCHRONIZATION
THE
PROCESS
IS
RESPONSIBLE
FOR
ALLOCATING
THE
MEMORY
USED
BY
A
CRITICAL
SECTION
TYPICALLY
THIS
IS
DONE
BY
SIMPLY
DECLARING
A
VARIABLE
OF
TYPE
BEFORE
THE
THREADS
OF
THE
PROCESS
CAN
USE
IT
INITIALIZE
THE
CRITICAL
SECTION
BY
USING
THE
INITIALIZECRITICALSECTION
FUNCTION
A
THREAD
USES
THE
ENTERCRITICALSECTION
OR
TRYENTERCRITICALSECTION
FUNCTION
TO
REQUEST
OWNERSHIP
OF
A
CRITICAL
SECTION
IT
USES
THE
LEAVECRITICALSECTION
FUNCTION
TO
RELEASE
OWNERSHIP
OF
A
CRITICAL
SECTION
IF
THE
CRITICAL
SECTION
IS
CURRENTLY
OWNED
BY
ANOTHER
THREAD
ENTERCRITICALSECTION
WAITS
INDEFINITELY
FOR
OWNER
SHIP
IN
CONTRAST
WHEN
A
MUTEX
OBJECT
IS
USED
FOR
MUTUAL
EXCLUSION
THE
WAIT
FUNCTIONS
ACCEPT
A
SPECIFIED
TIME
OUT
INTERVAL
THE
TRYENTERCRITICALSECTION
FUNCTION
ATTEMPTS
TO
ENTER
A
CRITICAL
SECTION
WITHOUT
BLOCKING
THE
CALLING
THREAD
CRITICAL
SECTIONS
USE
A
SOPHISTICATED
ALGORITHM
WHEN
TRYING
TO
ACQUIRE
THE
MUTEX
IF
THE
SYSTEM
IS
A
MULTIPROCESSOR
THE
CODE
WILL
ATTEMPT
TO
ACQUIRE
A
SPINLOCK
THIS
WORKS
WELL
IN
SITUATIONS
WHERE
THE
CRITICAL
SECTION
IS
ACQUIRED
FOR
ONLY
A
SHORT
TIME
EFFECTIVELY
THE
SPINLOCK
OPTIMIZES
FOR
THE
CASE
WHERE
THE
THREAD
THAT
CURRENTLY
OWNS
THE
CRITICAL
SECTION
IS
EXECUTING
ON
ANOTHER
PROCESSOR
IF
THE
SPINLOCK
CANNOT
BE
ACQUIRED
WITHIN
A
REASONABLE
NUMBER
OF
ITERATIONS
A
DISPATCHER
OBJECT
IS
USED
TO
BLOCK
THE
THREAD
SO
THAT
THE
KERNEL
CAN
DISPATCH
ANOTHER
THREAD
ONTO
THE
PROCESSOR
WINDOWS
CONCURRENCY
MECHANISMS
THE
DISPATCHER
OBJECT
IS
ONLY
ALLOCATED
AS
A
LAST
RESORT
MOST
CRITICAL
SECTIONS
ARE
NEEDED
FOR
CORRECTNESS
BUT
IN
PRACTICE
ARE
RARELY
CONTENDED
BY
LAZILY
ALLOCATING
THE
DISPATCHER
OBJECT
THE
SYSTEM
SAVES
SIGNIFICANT
AMOUNTS
OF
KERNEL
VIRTUAL
MEMORY
SLIM
READ
WRITER
LOCKS
AND
CONDITION
VARIABLES
WINDOWS
VISTA
ADDED
A
USER
MODE
READER
WRITER
LIKE
CRITICAL
SECTIONS
THE
READER
WRITER
LOCK
ENTERS
THE
KERNEL
TO
BLOCK
ONLY
AFTER
ATTEMPTING
TO
USE
A
SPINLOCK
IT
IS
SLIM
IN
THE
SENSE
THAT
IT
NORMALLY
ONLY
REQUIRES
ALLOCATION
OF
A
SINGLE
POINTER
SIZED
PIECE
OF
MEMORY
TO
USE
AN
SRW
LOCK
A
PROCESS
DECLARES
A
VARIABLE
OF
TYPE
SRWLOCK
AND
A
CALLS
INITIALIZESRWLOCK
TO
INITIALIZE
IT
THREADS
CALL
ACQUIRESRWLOCKEXCLUSIVE
OR
ACQUIRESRWLOCKSHARED
TO
ACQUIRE
THE
LOCK
AND
RELEASESRWLOCKEXCLUSIVE
OR
RELEASESRWLOCKSHARED
TO
RELEASE
IT
WINDOWS
ALSO
HAS
CONDITION
VARIABLES
THE
PROCESS
MUST
DECLARE
A
AND
INITIALIZE
IT
IN
SOME
THREAD
BY
CALLING
INITIALIZECONDITIONVARIABLE
CONDITION
VARIABLES
CAN
BE
USED
WITH
EITHER
CRIT
ICAL
SECTIONS
OR
SRW
LOCKS
SO
THERE
ARE
TWO
METHODS
SLEEPCONDITIONVARIABLECS
AND
SLEEPCONDITIONVARIABLESRW
WHICH
SLEEP
ON
THE
SPECIFIED
CONDITION
AND
RELEASES
THE
SPECIFIED
LOCK
AS
AN
ATOMIC
OPERATION
THERE
ARE
TWO
WAKE
METHODS
WAKECONDITIONVARIABLE
AND
WAKE
ALLCONDITIONVARIABLE
WHICH
WAKE
ONE
OR
ALL
OF
THE
SLEEPING
THREADS
RESPEC
TIVELY
CONDITION
VARIABLES
ARE
USED
AS
FOLLOWS
ACQUIRE
EXCLUSIVE
LOCK
WHILE
PREDICATE
FALSE
SLEEPCONDITIONVARIABLE
PERFORM
THE
PROTECTED
OPERATION
RELEASE
THE
LOCK
LOCK
FREE
SYNCHRONIZATION
WINDOWS
ALSO
RELIES
HEAVILY
ON
INTERLOCKED
OPERATIONS
FOR
SYNCHRONIZATION
INTERLOCKED
OPERATIONS
USE
HARDWARE
FACILITIES
TO
GUARANTEE
THAT
MEMORY
LOCATIONS
CAN
BE
READ
MODIFIED
AND
WRITTEN
IN
A
SINGLE
ATOMIC
OPERATION
EXAMPLES
INCLUDE
INTERLOCKEDINCREMENT
AND
INTERLOCKEDCOMPAREEXCHANGE
THE
LATTER
ALLOWS
A
MEMORY
LOCATION
TO
BE
UPDATED
ONLY
IF
IT
HASN
T
CHANGED
VALUES
SINCE
BEING
READ
MANY
OF
THE
SYNCHRONIZATION
PRIMITIVES
USE
INTERLOCKED
OPERATIONS
WITHIN
THEIR
IMPLEMENTATION
BUT
THESE
OPERATIONS
ARE
ALSO
AVAILABLE
TO
PROGRAMMERS
FOR
SITUATIONS
WHERE
THEY
WANT
TO
SYNCHRONIZE
WITHOUT
TAKING
A
SOFTWARE
LOCK
THESE
SO
CALLED
LOCK
FREE
SYNCHRONIZATION
PRIMITIVES
HAVE
THE
ADVANTAGE
THAT
A
THREAD
CAN
NEVER
BE
SWITCHED
AWAY
FROM
A
PROCESSOR
SAY
AT
THE
END
OF
ITS
TIMESLICE
WHILE
STILL
HOLDING
A
LOCK
THUS
THEY
CANNOT
BLOCK
ANOTHER
THREAD
FROM
RUNNING
MORE
COMPLEX
LOCK
FREE
PRIMITIVES
CAN
BE
BUILT
OUT
OF
THE
INTERLOCKED
OPER
ATIONS
MOST
NOTABLY
WINDOWS
SLISTS
WHICH
PROVIDE
A
LOCK
FREE
LIFO
QUEUE
SLISTS
ARE
MANAGED
USING
FUNCTIONS
LIKE
INTERLOCKEDPUSHENTRYSLIST
AND
INTERLOCKEDPOPENTRYSLIST
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
SUMMARY
DEADLOCK
IS
THE
BLOCKING
OF
A
SET
OF
PROCESSES
THAT
EITHER
COMPETE
FOR
SYSTEM
RESOURCES
OR
COMMUNICATE
WITH
EACH
OTHER
THE
BLOCKAGE
IS
PERMANENT
UNLESS
THE
OS
TAKES
SOME
EXTRAORDINARY
ACTION
SUCH
AS
KILLING
ONE
OR
MORE
PROCESSES
OR
FORCING
ONE
OR
MORE
PROCESSES
TO
BACKTRACK
DEADLOCK
MAY
INVOLVE
REUSABLE
RESOURCES
OR
CONSUMABLE
RESOURCES
A
REUSABLE
RESOURCE
IS
ONE
THAT
IS
NOT
DEPLETED
OR
DESTROYED
BY
USE
SUCH
AS
AN
I
O
CHANNEL
OR
A
REGION
OF
MEMORY
A
CONSUMABLE
RESOURCE
IS
ONE
THAT
IS
DESTROYED
WHEN
IT
IS
ACQUIRED
BY
A
PROCESS
EXAMPLES
INCLUDE
MESSAGES
AND
INFORMATION
IN
I
O
BUFFERS
THERE
ARE
THREE
GENERAL
APPROACHES
TO
DEALING
WITH
DEADLOCK
PREVENTION
DETECTION
AND
AVOIDANCE
DEADLOCK
PREVENTION
GUARANTEES
THAT
DEADLOCK
WILL
NOT
OCCUR
BY
ASSURING
THAT
ONE
OF
THE
NECESSARY
CONDITIONS
FOR
DEADLOCK
IS
NOT
MET
DEADLOCK
DETECTION
IS
NEEDED
IF
THE
OS
IS
ALWAYS
WILLING
TO
GRANT
RESOURCE
REQUESTS
PERIODICALLY
THE
OS
MUST
CHECK
FOR
DEADLOCK
AND
TAKE
ACTION
TO
BREAK
THE
DEADLOCK
DEADLOCK
AVOIDANCE
INVOLVES
THE
ANALYSIS
OF
EACH
NEW
RESOURCE
REQUEST
TO
DETER
MINE
IF
IT
COULD
LEAD
TO
DEADLOCK
AND
GRANTING
IT
ONLY
IF
DEADLOCK
IS
NOT
POSSIBLE
RECOMMENDED
READING
THE
CLASSIC
PAPER
ON
DEADLOCKS
IS
STILL
WELL
WORTH
A
READ
AS
IS
ANOTHER
GOOD
SURVEY
IS
IS
A
THOROUGH
TREATMENT
OF
DEADLOCK
DETECTION
IS
A
NICE
OVERVIEW
OF
DEADLOCKS
TWO
PAPERS
BY
LEVINE
CLARIFY
SOME
OF
THE
CONCEPTS
USED
IN
DISCUSSIONS
OF
DEADLOCK
IS
A
USEFUL
OVERVIEW
OF
DEADLOCK
DESCRIBES
A
DEADLOCK
DETECTION
PACKAGE
THE
CONCURRENCY
MECHANISMS
IN
UNIX
LINUX
AND
SOLARIS
ARE
WELL
COVERED
IN
AND
RESPECTIVELY
IS
A
THOROUGH
TREAT
MENT
OF
UNIX
CONCURRENCY
AND
INTERPROCESS
COMMUNICATION
MECHANISMS
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
KEY
TERMS
BANKER
ALGORITHM
DEADLOCK
PREVENTION
PIPE
CIRCULAR
WAIT
HOLD
AND
WAIT
PREEMPTION
CONSUMABLE
RESOURCE
JOINT
PROGRESS
DIAGRAM
RESOURCE
ALLOCATION
GRAPH
DEADLOCK
MEMORY
BARRIER
REUSABLE
RESOURCE
DEADLOCK
AVOIDANCE
MESSAGE
SPINLOCK
DEADLOCK
DETECTION
MUTUAL
EXCLUSION
STARVATION
REVIEW
QUESTIONS
GIVE
EXAMPLES
OF
REUSABLE
AND
CONSUMABLE
RESOURCES
WHAT
ARE
THE
THREE
CONDITIONS
THAT
MUST
BE
PRESENT
FOR
DEADLOCK
TO
BE
POSSIBLE
WHAT
ARE
THE
FOUR
CONDITIONS
THAT
CREATE
DEADLOCK
HOW
CAN
THE
HOLD
AND
WAIT
CONDITION
BE
PREVENTED
LIST
TWO
WAYS
IN
WHICH
THE
NO
PREEMPTION
CONDITION
CAN
BE
PREVENTED
HOW
CAN
THE
CIRCULAR
WAIT
CONDITION
BE
PREVENTED
WHAT
IS
THE
DIFFERENCE
AMONG
DEADLOCK
AVOIDANCE
DETECTION
AND
PREVENTION
PROBLEMS
SHOW
THAT
THE
FOUR
CONDITIONS
OF
DEADLOCK
APPLY
TO
FIGURE
SHOW
HOW
EACH
OF
THE
TECHNIQUES
OF
PREVENTION
AVOIDANCE
AND
DETECTION
CAN
BE
APPLIED
TO
FIGURE
FOR
FIGURE
PROVIDE
A
NARRATIVE
DESCRIPTION
OF
EACH
OF
THE
SIX
DEPICTED
PATHS
SIMI
LAR
TO
THE
DESCRIPTION
OF
THE
PATHS
OF
FIGURE
PROVIDED
IN
SECTION
IT
WAS
STATED
THAT
DEADLOCK
CANNOT
OCCUR
FOR
THE
SITUATION
REFLECTED
IN
FIGURE
JUSTIFY
THAT
STATEMENT
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
GIVEN
THE
FOLLOWING
STATE
FOR
THE
BANKER
ALGORITHM
PROCESSES
THROUGH
RESOURCE
TYPES
A
INSTANCES
B
INSTANCES
C
INSTANCES
D
INSTANCES
SNAPSHOT
AT
TIME
AVAILABLE
A
B
C
D
CURRENT
ALLOCATION
MAXIMUM
DEMAND
PROCESS
A
B
C
D
A
B
C
D
P4
A
VERIFY
THAT
THE
AVAILABLE
ARRAY
HAS
BEEN
CALCULATED
CORRECTLY
B
CALCULATE
THE
NEED
MATRIX
C
SHOW
THAT
THE
CURRENT
STATE
IS
SAFE
THAT
IS
SHOW
A
SAFE
SEQUENCE
OF
PROCESSES
IN
ADDITION
TO
THE
SEQUENCE
SHOW
HOW
THE
AVAILABLE
WORKING
ARRAY
CHANGES
AS
EACH
PROCESS
TERMINATES
D
GIVEN
THE
REQUEST
FROM
PROCESS
SHOULD
THIS
REQUEST
BE
GRANTED
WHY
OR
WHY
NOT
IN
THE
CODE
BELOW
THREE
PROCESSES
ARE
COMPETING
FOR
SIX
RESOURCES
LABELED
A
TO
F
A
USING
A
RESOURCE
ALLOCATION
GRAPH
FIGURES
AND
SHOW
THE
POSSIBILITY
OF
A
DEADLOCK
IN
THIS
IMPLEMENTATION
B
MODIFY
THE
ORDER
OF
SOME
OF
THE
GET
REQUESTS
TO
PREVENT
THE
POSSIBILITY
OF
ANY
DEADLOCK
YOU
CANNOT
MOVE
REQUESTS
ACROSS
PROCEDURES
ONLY
CHANGE
THE
ORDER
INSIDE
EACH
PROCEDURE
USE
A
RESOURCE
ALLOCATION
GRAPH
TO
JUSTIFY
YOUR
ANSWER
VOID
WHILE
TRUE
GET
A
GET
B
GET
C
CRITICAL
REGION
USE
A
B
C
RELEASE
A
RELEASE
B
RELEASE
C
VOID
WHILE
TRUE
GET
D
GET
E
GET
B
CRITICAL
REGION
USE
D
E
B
RELEASE
D
RELEASE
E
RELEASE
B
VOID
WHILE
TRUE
GET
C
GET
F
GET
D
CRITICAL
REGION
USE
C
F
D
RELEASE
C
RELEASE
F
RELEASE
D
A
SPOOLING
SYSTEM
FIGURE
CONSISTS
OF
AN
INPUT
PROCESS
I
A
USER
PROCESS
P
AND
AN
OUTPUT
PROCESS
O
CONNECTED
BY
TWO
BUFFERS
THE
PROCESSES
EXCHANGE
DATA
IN
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
FIGURE
A
SPOOLING
SYSTEM
BLOCKS
OF
EQUAL
SIZE
THESE
BLOCKS
ARE
BUFFERED
ON
A
DISK
USING
A
FLOATING
BOUNDARY
BETWEEN
THE
INPUT
AND
THE
OUTPUT
BUFFERS
DEPENDING
ON
THE
SPEED
OF
THE
PROCESSES
THE
COMMUNICATION
PRIMITIVES
USED
ENSURE
THAT
THE
FOLLOWING
RESOURCE
CONSTRAINT
IS
SATISFIED
WHERE
I
O
MAX
MAX
MAXIMUM
NUMBER
OF
BLOCKS
ON
DISK
I
NUMBER
OF
INPUT
BLOCKS
ON
DISK
O
NUMBER
OF
OUTPUT
BLOCKS
ON
DISK
THE
FOLLOWING
IS
KNOWN
ABOUT
THE
PROCESSES
AS
LONG
AS
THE
ENVIRONMENT
SUPPLIES
DATA
PROCESS
I
WILL
EVENTUALLY
INPUT
IT
TO
THE
DISK
PROVIDED
DISK
SPACE
BECOMES
AVAILABLE
AS
LONG
AS
INPUT
IS
AVAILABLE
ON
THE
DISK
PROCESS
P
WILL
EVENTUALLY
CONSUME
IT
AND
OUTPUT
A
FINITE
AMOUNT
OF
DATA
ON
THE
DISK
FOR
EACH
BLOCK
INPUT
PROVIDED
DISK
SPACE
BECOMES
AVAILABLE
AS
LONG
AS
OUTPUT
IS
AVAILABLE
ON
THE
DISK
PROCESS
O
WILL
EVENTUALLY
CONSUME
IT
SHOW
THAT
THIS
SYSTEM
CAN
BECOME
DEADLOCKED
SUGGEST
AN
ADDITIONAL
RESOURCE
CONSTRAINT
THAT
WILL
PREVENT
THE
DEADLOCK
IN
PROBLEM
BUT
STILL
PERMIT
THE
BOUNDARY
BETWEEN
INPUT
AND
OUTPUT
BUFFERS
TO
VARY
IN
ACCOR
DANCE
WITH
THE
PRESENT
NEEDS
OF
THE
PROCESSES
IN
THE
THE
MULTIPROGRAMMING
SYSTEM
A
DRUM
PRECURSOR
TO
THE
DISK
FOR
SECONDARY
STORAGE
IS
DIVIDED
INTO
INPUT
BUFFERS
PROCESSING
AREAS
AND
OUTPUT
BUFFERS
WITH
FLOATING
BOUNDARIES
DEPENDING
ON
THE
SPEED
OF
THE
PROCESSES
INVOLVED
THE
CURRENT
STATE
OF
THE
DRUM
CAN
BE
CHARACTERIZED
BY
THE
FOLLOWING
PARAMETERS
MAX
MAXIMUM
NUMBER
OF
PAGES
ON
DRUM
I
NUMBER
OF
INPUT
PAGES
ON
DRUM
P
NUMBER
OF
PROCESSING
PAGES
ON
DRUM
O
NUMBER
OF
OUTPUT
PAGES
ON
DRUM
RESO
MINIMUM
NUMBER
OF
PAGES
RESERVED
FOR
OUTPUT
RESP
MINIMUM
NUMBER
OF
PAGES
RESERVED
FOR
PROCESSING
FORMULATE
THE
NECESSARY
RESOURCE
CONSTRAINTS
THAT
GUARANTEE
THAT
THE
DRUM
CAPACITY
IS
NOT
EXCEEDED
AND
THAT
A
MINIMUM
NUMBER
OF
PAGES
IS
RESERVED
PERMANENTLY
FOR
OUTPUT
AND
PROCESSING
IN
THE
THE
MULTIPROGRAMMING
SYSTEM
A
PAGE
CAN
MAKE
THE
FOLLOWING
STATE
TRANSITIONS
EMPTY
INPUT
BUFFER
INPUT
PRODUCTION
INPUT
BUFFER
PROCESSING
AREA
INPUT
CONSUMPTION
PROCESSING
AREA
OUTPUT
BUFFER
OUTPUT
PRODUCTION
OUTPUT
BUFFER
EMPTY
OUTPUT
CONSUMPTION
EMPTY
PROCESSING
AREA
PROCEDURE
CALL
PROCESSING
AREA
EMPTY
PROCEDURE
RETURN
A
DEFINE
THE
EFFECT
OF
THESE
TRANSITIONS
IN
TERMS
OF
THE
QUANTITIES
I
O
AND
P
B
CAN
ANY
OF
THEM
LEAD
TO
A
DEADLOCK
IF
THE
ASSUMPTIONS
MADE
IN
PROBLEM
ABOUT
INPUT
PROCESSES
USER
PROCESSES
AND
OUTPUT
PROCESSES
HOLD
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
CONSIDER
A
SYSTEM
WITH
A
TOTAL
OF
UNITS
OF
MEMORY
ALLOCATED
TO
THREE
PROCESSES
AS
SHOWN
PROCESS
MAX
HOLD
APPLY
THE
BANKER
ALGORITHM
TO
DETERMINE
WHETHER
IT
WOULD
BE
SAFE
TO
GRANT
EACH
OF
THE
FOLLOWING
REQUESTS
IF
YES
INDICATE
A
SEQUENCE
OF
TERMINATIONS
THAT
COULD
BE
GUAR
ANTEED
POSSIBLE
IF
NO
SHOW
THE
REDUCTION
OF
THE
RESULTING
ALLOCATION
TABLE
A
A
FOURTH
PROCESS
ARRIVES
WITH
A
MAXIMUM
MEMORY
NEED
OF
AND
AN
INITIAL
NEED
OF
25
UNITS
B
A
FOURTH
PROCESS
ARRIVES
WITH
A
MAXIMUM
MEMORY
NEED
OF
AND
AN
INITIAL
NEED
OF
UNITS
EVALUATE
THE
BANKER
ALGORITHM
FOR
ITS
USEFULNESS
IN
AN
OS
A
PIPELINE
ALGORITHM
IS
IMPLEMENTED
SO
THAT
A
STREAM
OF
DATA
ELEMENTS
OF
TYPE
T
PRO
DUCED
BY
A
PROCESS
PASSES
THROUGH
A
SEQUENCE
OF
PROCESSES
P2
PN
WHICH
OPERATES
ON
THE
ELEMENTS
IN
THAT
ORDER
A
DEFINE
A
GENERALIZED
MESSAGE
BUFFER
THAT
CONTAINS
ALL
THE
PARTIALLY
CONSUMED
DATA
ELEMENTS
AND
WRITE
AN
ALGORITHM
FOR
PROCESS
PI
I
N
OF
THE
FORM
REPEAT
RECEIVE
FROM
PREDECESSOR
CONSUME
ELEMENT
SEND
TO
SUCCESSOR
FOREVER
ASSUME
RECEIVES
INPUT
ELEMENTS
SENT
BY
PN
THE
ALGORITHM
SHOULD
ENABLE
THE
PROCESSES
TO
OPERATE
DIRECTLY
ON
MESSAGES
STORED
IN
THE
BUFFER
SO
THAT
COPYING
IS
UNNECESSARY
B
SHOW
THAT
THE
PROCESSES
CANNOT
BE
DEADLOCKED
WITH
RESPECT
TO
THE
COMMON
BUFFER
SUPPOSE
THE
FOLLOWING
TWO
PROCESSES
FOO
AND
BAR
ARE
EXECUTED
CONCURRENTLY
AND
SHARE
THE
SEMAPHORE
VARIABLES
S
AND
R
EACH
INITIALIZED
TO
AND
THE
INTEGER
VARIABLE
X
INITIALIZED
TO
A
CAN
THE
CONCURRENT
EXECUTION
OF
THESE
TWO
PROCESSES
RESULT
IN
ONE
OR
BOTH
BEING
BLOCKED
FOREVER
IF
YES
GIVE
AN
EXECUTION
SEQUENCE
IN
WHICH
ONE
OR
BOTH
ARE
BLOCKED
FOREVER
B
CAN
THE
CONCURRENT
EXECUTION
OF
THESE
TWO
PROCESSES
RESULT
IN
THE
INDEFINITE
POSTPONEMENT
OF
ONE
OF
THEM
IF
YES
GIVE
AN
EXECUTION
SEQUENCE
IN
WHICH
ONE
IS
INDEFINITELY
POSTPONED
CONSIDER
A
SYSTEM
CONSISTING
OF
FOUR
PROCESSES
AND
A
SINGLE
RESOURCE
THE
CURRENT
STATE
OF
THE
CLAIM
AND
ALLOCATION
MATRICES
ARE
C
WHAT
IS
THE
MINIMUM
NUMBER
OF
UNITS
OF
THE
RESOURCE
NEEDED
TO
BE
AVAILABLE
FOR
THIS
STATE
TO
BE
SAFE
CONSIDER
THE
FOLLOWING
WAYS
OF
HANDLING
DEADLOCK
BANKER
ALGORITHM
DETECT
DEADLOCK
AND
KILL
THREAD
RELEASING
ALL
RESOURCES
RESERVE
ALL
RESOURCES
IN
ADVANCE
RESTART
THREAD
AND
RELEASE
ALL
RESOURCES
IF
THREAD
NEEDS
TO
WAIT
RESOURCE
ORDER
ING
AND
DETECT
DEADLOCK
AND
ROLL
BACK
THREAD
ACTIONS
A
ONE
CRITERION
TO
USE
IN
EVALUATING
DIFFERENT
APPROACHES
TO
DEADLOCK
IS
WHICH
APPROACH
PERMITS
THE
GREATEST
CONCURRENCY
IN
OTHER
WORDS
WHICH
APPROACH
ALLOWS
THE
MOST
THREADS
TO
MAKE
PROGRESS
WITHOUT
WAITING
WHEN
THERE
IS
NO
DEADLOCK
GIVE
A
RANK
ORDER
FROM
TO
FOR
EACH
OF
THE
WAYS
OF
HANDLING
DEADLOCK
JUST
LISTED
WHERE
ALLOWS
THE
GREATEST
DEGREE
OF
CONCURRENCY
COMMENT
ON
YOUR
ORDERING
B
ANOTHER
CRITERION
IS
EFFICIENCY
IN
OTHER
WORDS
WHICH
REQUIRES
THE
LEAST
PROCESSOR
OVERHEAD
RANK
ORDER
THE
APPROACHES
FROM
TO
WITH
BEING
THE
MOST
EFFICIENT
ASSUMING
THAT
DEADLOCK
IS
A
VERY
RARE
EVENT
COMMENT
ON
YOUR
ORDERING
DOES
YOUR
ORDERING
CHANGE
IF
DEADLOCKS
OCCUR
FREQUENTLY
COMMENT
ON
THE
FOLLOWING
SOLUTION
TO
THE
DINING
PHILOSOPHERS
PROBLEM
A
HUNGRY
PHI
LOSOPHER
FIRST
PICKS
UP
HIS
LEFT
FORK
IF
HIS
RIGHT
FORK
IS
ALSO
AVAILABLE
HE
PICKS
UP
HIS
RIGHT
FORK
AND
STARTS
EATING
OTHERWISE
HE
PUTS
DOWN
HIS
LEFT
FORK
AGAIN
AND
REPEATS
THE
CYCLE
SUPPOSE
THAT
THERE
ARE
TWO
TYPES
OF
PHILOSOPHERS
ONE
TYPE
ALWAYS
PICKS
UP
HIS
LEFT
FORK
FIRST
A
LEFTY
AND
THE
OTHER
TYPE
ALWAYS
PICKS
UP
HIS
RIGHT
FORK
FIRST
A
RIGHTY
THE
BEHAVIOR
OF
A
LEFTY
IS
DEFINED
IN
FIGURE
THE
BEHAVIOR
OF
A
RIGHTY
IS
AS
FOLLOWS
BEGIN
REPEAT
THINK
WAIT
FORK
I
MOD
WAIT
FORK
I
EAT
SIGNAL
FORK
I
SIGNAL
FORK
I
MOD
FOREVER
END
PROVE
THE
FOLLOWING
A
ANY
SEATING
ARRANGEMENT
OF
LEFTIES
AND
RIGHTIES
WITH
AT
LEAST
ONE
OF
EACH
AVOIDS
DEADLOCK
B
ANY
SEATING
ARRANGEMENT
OF
LEFTIES
AND
RIGHTIES
WITH
AT
LEAST
ONE
OF
EACH
PREVENTS
STARVATION
FIGURE
SHOWS
ANOTHER
SOLUTION
TO
THE
DINING
PHILOSOPHERS
PROBLEM
USING
MONI
TORS
COMPARE
TO
FIGURE
AND
REPORT
YOUR
CONCLUSIONS
IN
TABLE
SOME
OF
THE
LINUX
ATOMIC
OPERATIONS
DO
NOT
INVOLVE
TWO
ACCESSES
TO
A
VARIABLE
SUCH
AS
V
A
SIMPLE
READ
OPERATION
IS
OBVI
OUSLY
ATOMIC
IN
ANY
ARCHITECTURE
THEREFORE
WHY
IS
THIS
OPERATION
ADDED
TO
THE
REPER
TOIRE
OF
ATOMIC
OPERATIONS
CONSIDER
THE
FOLLOWING
FRAGMENT
OF
CODE
ON
A
LINUX
SYSTEM
WHERE
IS
A
READER
WRITER
LOCK
WHAT
IS
THE
EFFECT
OF
THIS
CODE
CHAPTER
CONCURRENCY
DEADLOCK
AND
STARVATION
MONITOR
ENUM
STATES
THINKING
HUNGRY
EATING
STATE
COND
NEEDFORK
CONDITION
VARIABLE
VOID
INT
PID
PID
IS
THE
PHILOSOPHER
ID
NUMBER
STATE
PID
HUNGRY
ANNOUNCE
THAT
I
M
HUNGRY
IF
STATE
PID
EATING
STATE
PID
EATING
CWAIT
NEEDFORK
PID
WAIT
IF
EITHER
NEIGHBOR
IS
EATING
STATE
PID
EATING
PROCEED
IF
NEITHER
NEIGHBOR
IS
EATING
VOID
INT
PID
STATE
PID
THINKING
GIVE
RIGHT
HIGHER
NEIGHBOR
A
CHANCE
TO
EAT
IF
STATE
PID
HUNGRY
STATE
PID
EATING
CSIGNAL
NEEDFORK
PID
GIVE
LEFT
LOWER
NEIGHBOR
A
CHANCE
TO
EAT
ELSE
IF
STATE
PID
HUNGRY
STATE
PID
EATING
CSIGNAL
NEEDFORK
PID
VOID
PHILOSOPHER
K
TO
THE
FIVE
PHILOSOPHER
CLIENTS
WHILE
TRUE
THINK
K
CLIENT
REQUESTS
TWO
FORKS
VIA
MONITOR
EAT
SPAGHETTI
K
CLIENT
RELEASES
FORKS
VIA
THE
MONITOR
FIGURE
ANOTHER
SOLUTION
TO
THE
DINING
PHILOSOPHERS
PROBLEM
USING
A
MONITOR
THE
TWO
VARIABLES
A
AND
B
HAVE
INITIAL
VALUES
OF
AND
RESPECTIVELY
THE
FOLLOWING
CODE
IS
FOR
A
LINUX
SYSTEM
THREAD
THREAD
A
MB
B
C
B
RMB
D
A
WHAT
POSSIBLE
ERRORS
ARE
AVOIDED
BY
THE
USE
OF
THE
MEMORY
BARRIERS
PART
MEMORY
CHAPTER
MEMORY
MANAGEMENT
MEMORY
MANAGEMENT
REQUIREMENTS
RELOCATION
PROTECTION
SHARING
LOGICAL
ORGANIZATION
PHYSICAL
ORGANIZATION
MEMORY
PARTITIONING
FIXED
PARTITIONING
DYNAMIC
PARTITIONING
BUDDY
SYSTEM
RELOCATION
PAGING
SEGMENTATION
SECURITY
ISSUES
BUFFER
OVERFLOW
ATTACKS
DEFENDING
AGAINST
BUFFER
OVERFLOWS
SUMMARY
RECOMMENDED
READING
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
APPENDIX
LOADING
AND
LINKING
CHAPTER
MEMORY
MANAGEMENT
I
CANNOT
GUARANTEE
THAT
I
CARRY
ALL
THE
FACTS
IN
MY
MIND
INTENSE
MENTAL
CONCENTRATION
HAS
A
CURIOUS
WAY
OF
BLOTTING
OUT
WHAT
HAS
PASSED
EACH
OF
MY
CASES
DISPLACES
THE
LAST
AND
MLLE
CARÈRE
HAS
BLURRED
MY
RECOLLECTION
OF
BASKERVILLE
HALL
TOMORROW
SOME
OTHER
LITTLE
PROBLEM
MAY
BE
SUBMITTED
TO
MY
NOTICE
WHICH
WILL
IN
TURN
DISPOSSESS
THE
FAIR
FRENCH
LADY
AND
THE
INFAMOUS
UPWOOD
THE
HOUND
OF
THE
BASKERVILLES
ARTHUR
CONAN
DOYLE
IN
A
UNIPROGRAMMING
SYSTEM
MAIN
MEMORY
IS
DIVIDED
INTO
TWO
PARTS
ONE
PART
FOR
THE
OPERATING
SYSTEM
RESIDENT
MONITOR
KERNEL
AND
ONE
PART
FOR
THE
PROGRAM
CUR
RENTLY
BEING
EXECUTED
IN
A
MULTIPROGRAMMING
SYSTEM
THE
USER
PART
OF
MEMORY
MUST
BE
FURTHER
SUBDIVIDED
TO
ACCOMMODATE
MULTIPLE
PROCESSES
THE
TASK
OF
SUBDI
VISION
IS
CARRIED
OUT
DYNAMICALLY
BY
THE
OPERATING
SYSTEM
AND
IS
KNOWN
AS
MEMORY
MANAGEMENT
EFFECTIVE
MEMORY
MANAGEMENT
IS
VITAL
IN
A
MULTIPROGRAMMING
SYSTEM
IF
ONLY
A
FEW
PROCESSES
ARE
IN
MEMORY
THEN
FOR
MUCH
OF
THE
TIME
ALL
OF
THE
PROCESSES
WILL
BE
WAITING
FOR
I
O
AND
THE
PROCESSOR
WILL
BE
IDLE
THUS
MEMORY
NEEDS
TO
BE
ALLOCATED
TO
ENSURE
A
REASONABLE
SUPPLY
OF
READY
PROCESSES
TO
CONSUME
AVAILABLE
PROCESSOR
TIME
WE
BEGIN
WITH
THE
REQUIREMENTS
THAT
MEMORY
MANAGEMENT
IS
INTENDED
TO
SATISFY
NEXT
WE
DISCUSS
A
VARIETY
OF
SIMPLE
SCHEMES
THAT
HAVE
BEEN
USED
FOR
MEMORY
MANAGEMENT
TABLE
INTRODUCES
SOME
KEY
TERMS
FOR
OUR
DISCUSSION
A
SET
OF
ANIMATIONS
THAT
ILLUSTRATE
CONCEPTS
IN
THIS
CHAPTER
IS
AVAILABLE
ONLINE
CLICK
ON
THE
ROTATING
GLOBE
AT
WILLIAMSTALLINGS
COM
OS
HTML
FOR
ACCESS
MEMORY
MANAGEMENT
REQUIREMENTS
TABLE
MEMORY
MANAGEMENT
TERMS
FRAME
A
FIXED
LENGTH
BLOCK
OF
MAIN
MEMORY
PAGE
A
FIXED
LENGTH
BLOCK
OF
DATA
THAT
RESIDES
IN
SECONDARY
MEMORY
SUCH
AS
DISK
A
PAGE
OF
DATA
MAY
TEMPORARILY
BE
COPIED
INTO
A
FRAME
OF
MAIN
MEMORY
SEGMENT
A
VARIABLE
LENGTH
BLOCK
OF
DATA
THAT
RESIDES
IN
SECONDARY
MEMORY
AN
ENTIRE
SEGMENT
MAY
TEMPO
RARILY
BE
COPIED
INTO
AN
AVAILABLE
REGION
OF
MAIN
MEMORY
SEGMENTATION
OR
THE
SEGMENT
MAY
BE
DIVIDED
INTO
PAGES
WHICH
CAN
BE
INDIVIDUALLY
COPIED
INTO
MAIN
MEMORY
COMBINED
SEGMENTATION
AND
PAGING
MEMORY
MANAGEMENT
REQUIREMENTS
WHILE
SURVEYING
THE
VARIOUS
MECHANISMS
AND
POLICIES
ASSOCIATED
WITH
MEMORY
MAN
AGEMENT
IT
IS
HELPFUL
TO
KEEP
IN
MIND
THE
REQUIREMENTS
THAT
MEMORY
MANAGEMENT
IS
INTENDED
TO
SATISFY
THESE
REQUIREMENTS
INCLUDE
THE
FOLLOWING
RELOCATION
PROTECTION
SHARING
LOGICAL
ORGANIZATION
PHYSICAL
ORGANIZATION
RELOCATION
IN
A
MULTIPROGRAMMING
SYSTEM
THE
AVAILABLE
MAIN
MEMORY
IS
GENERALLY
SHARED
AMONG
A
NUMBER
OF
PROCESSES
TYPICALLY
IT
IS
NOT
POSSIBLE
FOR
THE
PROGRAMMER
TO
KNOW
IN
ADVANCE
WHICH
OTHER
PROGRAMS
WILL
BE
RESIDENT
IN
MAIN
MEMORY
AT
THE
TIME
OF
EXECUTION
OF
HIS
OR
HER
PROGRAM
IN
ADDITION
WE
WOULD
LIKE
TO
BE
ABLE
TO
SWAP
ACTIVE
PROCESSES
IN
AND
OUT
OF
MAIN
MEMORY
TO
MAXIMIZE
PROCESSOR
UTILIZATION
BY
PROVIDING
A
LARGE
POOL
OF
READY
PROCESSES
TO
EXECUTE
ONCE
A
PROGRAM
IS
SWAPPED
OUT
TO
DISK
IT
WOULD
BE
QUITE
LIMITING
TO
SPECIFY
THAT
WHEN
IT
IS
NEXT
SWAPPED
BACK
IN
IT
MUST
BE
PLACED
IN
THE
SAME
MAIN
MEMORY
REGION
AS
BEFORE
INSTEAD
WE
MAY
NEED
TO
RELOCATE
THE
PROCESS
TO
A
DIFFERENT
AREA
OF
MEMORY
THUS
WE
CANNOT
KNOW
AHEAD
OF
TIME
WHERE
A
PROGRAM
WILL
BE
PLACED
AND
WE
MUST
ALLOW
FOR
THE
POSSIBILITY
THAT
THE
PROGRAM
MAY
BE
MOVED
ABOUT
IN
MAIN
MEMORY
DUE
TO
SWAPPING
THESE
FACTS
RAISE
SOME
TECHNICAL
CONCERNS
RELATED
TO
ADDRESSING
AS
ILLUSTRATED
IN
FIGURE
THE
FIGURE
DEPICTS
A
PROCESS
IMAGE
FOR
SIMPLICITY
LET
US
ASSUME
THAT
THE
PROCESS
IMAGE
OCCUPIES
A
CONTIGUOUS
REGION
OF
MAIN
MEMORY
CLEARLY
THE
OPERATING
SYSTEM
WILL
NEED
TO
KNOW
THE
LOCATION
OF
PROCESS
CONTROL
INFORMATION
AND
OF
THE
EXECUTION
STACK
AS
WELL
AS
THE
ENTRY
POINT
TO
BEGIN
EXECUTION
OF
THE
PROGRAM
FOR
THIS
PROCESS
BECAUSE
THE
OPERATING
SYSTEM
IS
MANAGING
MEM
ORY
AND
IS
RESPONSIBLE
FOR
BRINGING
THIS
PROCESS
INTO
MAIN
MEMORY
THESE
ADDRESSES
ARE
EASY
TO
COME
BY
IN
ADDITION
HOWEVER
THE
PROCESSOR
MUST
DEAL
WITH
MEMORY
CHAPTER
MEMORY
MANAGEMENT
PROCESS
CONTROL
INFORMATION
BRANCH
INSTRUCTION
INCREASING
ADDRESS
VALUES
REFERENCE
TO
DATA
FIGURE
ADDRESSING
REQUIREMENTS
FOR
A
PROCESS
REFERENCES
WITHIN
THE
PROGRAM
BRANCH
INSTRUCTIONS
CONTAIN
AN
ADDRESS
TO
REFERENCE
THE
INSTRUCTION
TO
BE
EXECUTED
NEXT
DATA
REFERENCE
INSTRUCTIONS
CONTAIN
THE
ADDRESS
OF
THE
BYTE
OR
WORD
OF
DATA
REFERENCED
SOMEHOW
THE
PROCESSOR
HARDWARE
AND
OPER
ATING
SYSTEM
SOFTWARE
MUST
BE
ABLE
TO
TRANSLATE
THE
MEMORY
REFERENCES
FOUND
IN
THE
CODE
OF
THE
PROGRAM
INTO
ACTUAL
PHYSICAL
MEMORY
ADDRESSES
REFLECTING
THE
CURRENT
LOCATION
OF
THE
PROGRAM
IN
MAIN
MEMORY
PROTECTION
EACH
PROCESS
SHOULD
BE
PROTECTED
AGAINST
UNWANTED
INTERFERENCE
BY
OTHER
PROCESSES
WHETHER
ACCIDENTAL
OR
INTENTIONAL
THUS
PROGRAMS
IN
OTHER
PROCESSES
SHOULD
NOT
BE
ABLE
TO
REFERENCE
MEMORY
LOCATIONS
IN
A
PROCESS
FOR
READING
OR
WRITING
PURPOSES
WITHOUT
PERMISSION
IN
ONE
SENSE
SATISFACTION
OF
THE
RELOCATION
REQUIRE
MENT
INCREASES
THE
DIFFICULTY
OF
SATISFYING
THE
PROTECTION
REQUIREMENT
BECAUSE
THE
LOCATION
OF
A
PROGRAM
IN
MAIN
MEMORY
IS
UNPREDICTABLE
IT
IS
IMPOSSIBLE
TO
CHECK
ABSOLUTE
ADDRESSES
AT
COMPILE
TIME
TO
ASSURE
PROTECTION
FURTHERMORE
MOST
PROGRAMMING
LANGUAGES
ALLOW
THE
DYNAMIC
CALCULATION
OF
ADDRESSES
AT
RUN
TIME
E
G
BY
COMPUTING
AN
ARRAY
SUBSCRIPT
OR
A
POINTER
INTO
A
DATA
STRUCTURE
HENCE
ALL
MEMORY
REFERENCES
GENERATED
BY
A
PROCESS
MUST
BE
CHECKED
AT
RUN
TIME
TO
ENSURE
THAT
THEY
REFER
ONLY
TO
THE
MEMORY
SPACE
ALLOCATED
TO
THAT
PROCESS
FORTUNATELY
WE
SHALL
SEE
THAT
MECHANISMS
THAT
SUPPORT
RELOCATION
ALSO
SUPPORT
THE
PROTECTION
REQUIREMENT
NORMALLY
A
USER
PROCESS
CANNOT
ACCESS
ANY
PORTION
OF
THE
OPERATING
SYSTEM
NEITHER
PROGRAM
NOR
DATA
AGAIN
USUALLY
A
PROGRAM
IN
ONE
PROCESS
CANNOT
BRANCH
TO
AN
INSTRUCTION
IN
ANOTHER
PROCESS
WITHOUT
SPECIAL
ARRANGEMENT
A
PROGRAM
IN
ONE
PROCESS
CANNOT
ACCESS
THE
DATA
AREA
OF
ANOTHER
PROCESS
THE
PROCESSOR
MUST
BE
ABLE
TO
ABORT
SUCH
INSTRUCTIONS
AT
THE
POINT
OF
EXECUTION
MEMORY
MANAGEMENT
REQUIREMENTS
NOTE
THAT
THE
MEMORY
PROTECTION
REQUIREMENT
MUST
BE
SATISFIED
BY
THE
PROCES
SOR
HARDWARE
RATHER
THAN
THE
OPERATING
SYSTEM
SOFTWARE
THIS
IS
BECAUSE
THE
OS
CANNOT
ANTICIPATE
ALL
OF
THE
MEMORY
REFERENCES
THAT
A
PROGRAM
WILL
MAKE
EVEN
IF
SUCH
ANTICIPATION
WERE
POSSIBLE
IT
WOULD
BE
PROHIBITIVELY
TIME
CONSUMING
TO
SCREEN
EACH
PROGRAM
IN
ADVANCE
FOR
POSSIBLE
MEMORY
REFERENCE
VIOLATIONS
THUS
IT
IS
ONLY
POSSIBLE
TO
ASSESS
THE
PERMISSIBILITY
OF
A
MEMORY
REFERENCE
DATA
ACCESS
OR
BRANCH
AT
THE
TIME
OF
EXECUTION
OF
THE
INSTRUCTION
MAKING
THE
REFERENCE
TO
ACCOMPLISH
THIS
THE
PROCESSOR
HARDWARE
MUST
HAVE
THAT
CAPABILITY
SHARING
ANY
PROTECTION
MECHANISM
MUST
HAVE
THE
FLEXIBILITY
TO
ALLOW
SEVERAL
PROCESSES
TO
ACCESS
THE
SAME
PORTION
OF
MAIN
MEMORY
FOR
EXAMPLE
IF
A
NUMBER
OF
PROCESSES
ARE
EXECUTING
THE
SAME
PROGRAM
IT
IS
ADVANTAGEOUS
TO
ALLOW
EACH
PROCESS
TO
ACCESS
THE
SAME
COPY
OF
THE
PROGRAM
RATHER
THAN
HAVE
ITS
OWN
SEPARATE
COPY
PROCESSES
THAT
ARE
COOPERATING
ON
SOME
TASK
MAY
NEED
TO
SHARE
ACCESS
TO
THE
SAME
DATA
STRUCTURE
THE
MEMORY
MANAGEMENT
SYSTEM
MUST
THEREFORE
ALLOW
CONTROLLED
ACCESS
TO
SHARED
AREAS
OF
MEMORY
WITHOUT
COMPROMISING
ESSENTIAL
PROTECTION
AGAIN
WE
WILL
SEE
THAT
THE
MECHANISMS
USED
TO
SUPPORT
RELOCATION
SUPPORT
SHARING
CAPABILITIES
LOGICAL
ORGANIZATION
ALMOST
INVARIABLY
MAIN
MEMORY
IN
A
COMPUTER
SYSTEM
IS
ORGANIZED
AS
A
LINEAR
OR
ONE
DIMENSIONAL
ADDRESS
SPACE
CONSISTING
OF
A
SEQUENCE
OF
BYTES
OR
WORDS
SECONDARY
MEMORY
AT
ITS
PHYSICAL
LEVEL
IS
SIMILARLY
ORGANIZED
WHILE
THIS
ORGANI
ZATION
CLOSELY
MIRRORS
THE
ACTUAL
MACHINE
HARDWARE
IT
DOES
NOT
CORRESPOND
TO
THE
WAY
IN
WHICH
PROGRAMS
ARE
TYPICALLY
CONSTRUCTED
MOST
PROGRAMS
ARE
ORGANIZED
INTO
MODULES
SOME
OF
WHICH
ARE
UNMODIFIABLE
READ
ONLY
EXECUTE
ONLY
AND
SOME
OF
WHICH
CONTAIN
DATA
THAT
MAY
BE
MODIFIED
IF
THE
OPERATING
SYSTEM
AND
COMPUTER
HARDWARE
CAN
EFFECTIVELY
DEAL
WITH
USER
PROGRAMS
AND
DATA
IN
THE
FORM
OF
MODULES
OF
SOME
SORT
THEN
A
NUMBER
OF
ADVANTAGES
CAN
BE
REALIZED
MODULES
CAN
BE
WRITTEN
AND
COMPILED
INDEPENDENTLY
WITH
ALL
REFERENCES
FROM
ONE
MODULE
TO
ANOTHER
RESOLVED
BY
THE
SYSTEM
AT
RUN
TIME
WITH
MODEST
ADDITIONAL
OVERHEAD
DIFFERENT
DEGREES
OF
PROTECTION
READ
ONLY
EXECUTE
ONLY
CAN
BE
GIVEN
TO
DIFFERENT
MODULES
IT
IS
POSSIBLE
TO
INTRODUCE
MECHANISMS
BY
WHICH
MODULES
CAN
BE
SHARED
AMONG
PROCESSES
THE
ADVANTAGE
OF
PROVIDING
SHARING
ON
A
MODULE
LEVEL
IS
THAT
THIS
CORRESPONDS
TO
THE
USER
WAY
OF
VIEWING
THE
PROBLEM
AND
HENCE
IT
IS
EASY
FOR
THE
USER
TO
SPECIFY
THE
SHARING
THAT
IS
DESIRED
THE
TOOL
THAT
MOST
READILY
SATISFIES
THESE
REQUIREMENTS
IS
SEGMENTATION
WHICH
IS
ONE
OF
THE
MEMORY
MANAGEMENT
TECHNIQUES
EXPLORED
IN
THIS
CHAPTER
PHYSICAL
ORGANIZATION
AS
WE
DISCUSSED
IN
SECTION
COMPUTER
MEMORY
IS
ORGANIZED
INTO
AT
LEAST
TWO
LEVELS
REFERRED
TO
AS
MAIN
MEMORY
AND
SECONDARY
MEMORY
MAIN
MEMORY
PROVIDES
FAST
ACCESS
AT
RELATIVELY
HIGH
COST
IN
ADDITION
MAIN
MEMORY
IS
VOLATILE
THAT
IS
IT
CHAPTER
MEMORY
MANAGEMENT
DOES
NOT
PROVIDE
PERMANENT
STORAGE
SECONDARY
MEMORY
IS
SLOWER
AND
CHEAPER
THAN
MAIN
MEMORY
AND
IS
USUALLY
NOT
VOLATILE
THUS
SECONDARY
MEMORY
OF
LARGE
CAPACITY
CAN
BE
PROVIDED
FOR
LONG
TERM
STORAGE
OF
PROGRAMS
AND
DATA
WHILE
A
SMALLER
MAIN
MEMORY
HOLDS
PROGRAMS
AND
DATA
CURRENTLY
IN
USE
IN
THIS
TWO
LEVEL
SCHEME
THE
ORGANIZATION
OF
THE
FLOW
OF
INFORMATION
BETWEEN
MAIN
AND
SECONDARY
MEMORY
IS
A
MAJOR
SYSTEM
CONCERN
THE
RESPONSIBILITY
FOR
THIS
FLOW
COULD
BE
ASSIGNED
TO
THE
INDIVIDUAL
PROGRAMMER
BUT
THIS
IS
IMPRACTICAL
AND
UNDESIRABLE
FOR
TWO
REASONS
THE
MAIN
MEMORY
AVAILABLE
FOR
A
PROGRAM
PLUS
ITS
DATA
MAY
BE
INSUFFICIENT
IN
THAT
CASE
THE
PROGRAMMER
MUST
ENGAGE
IN
A
PRACTICE
KNOWN
AS
OVERLAYING
IN
WHICH
THE
PROGRAM
AND
DATA
ARE
ORGANIZED
IN
SUCH
A
WAY
THAT
VARIOUS
MODULES
CAN
BE
ASSIGNED
THE
SAME
REGION
OF
MEMORY
WITH
A
MAIN
PROGRAM
RESPONSIBLE
FOR
SWITCHING
THE
MODULES
IN
AND
OUT
AS
NEEDED
EVEN
WITH
THE
AID
OF
COMPILER
TOOLS
OVERLAY
PROGRAMMING
WASTES
PROGRAMMER
TIME
IN
A
MULTIPROGRAMMING
ENVIRONMENT
THE
PROGRAMMER
DOES
NOT
KNOW
AT
THE
TIME
OF
CODING
HOW
MUCH
SPACE
WILL
BE
AVAILABLE
OR
WHERE
THAT
SPACE
WILL
BE
IT
IS
CLEAR
THEN
THAT
THE
TASK
OF
MOVING
INFORMATION
BETWEEN
THE
TWO
LEVELS
OF
MEMORY
SHOULD
BE
A
SYSTEM
RESPONSIBILITY
THIS
TASK
IS
THE
ESSENCE
OF
MEMORY
MANAGEMENT
MEMORY
PARTITIONING
THE
PRINCIPAL
OPERATION
OF
MEMORY
MANAGEMENT
IS
TO
BRING
PROCESSES
INTO
MAIN
MEMORY
FOR
EXECUTION
BY
THE
PROCESSOR
IN
ALMOST
ALL
MODERN
MULTIPROGRAMMING
SYSTEMS
THIS
INVOLVES
A
SOPHISTICATED
SCHEME
KNOWN
AS
VIRTUAL
MEMORY
VIRTUAL
MEMORY
IS
IN
TURN
BASED
ON
THE
USE
OF
ONE
OR
BOTH
OF
TWO
BASIC
TECHNIQUES
SEGMEN
TATION
AND
PAGING
BEFORE
WE
CAN
LOOK
AT
THESE
VIRTUAL
MEMORY
TECHNIQUES
WE
MUST
PREPARE
THE
GROUND
BY
LOOKING
AT
SIMPLER
TECHNIQUES
THAT
DO
NOT
INVOLVE
VIRTUAL
MEMORY
TABLE
SUMMARIZES
ALL
THE
TECHNIQUES
EXAMINED
IN
THIS
CHAPTER
AND
THE
NEXT
ONE
OF
THESE
TECHNIQUES
PARTITIONING
HAS
BEEN
USED
IN
SEVERAL
VARIATIONS
IN
SOME
NOW
OBSOLETE
OPERATING
SYSTEMS
THE
OTHER
TWO
TECHNIQUES
SIMPLE
PAGING
AND
SIMPLE
SEGMENTATION
ARE
NOT
USED
BY
THEMSELVES
HOWEVER
IT
WILL
CLARIFY
THE
DIS
CUSSION
OF
VIRTUAL
MEMORY
IF
WE
LOOK
FIRST
AT
THESE
TWO
TECHNIQUES
IN
THE
ABSENCE
OF
VIRTUAL
MEMORY
CONSIDERATIONS
FIXED
PARTITIONING
IN
MOST
SCHEMES
FOR
MEMORY
MANAGEMENT
WE
CAN
ASSUME
THAT
THE
OS
OCCUPIES
SOME
FIXED
PORTION
OF
MAIN
MEMORY
AND
THAT
THE
REST
OF
MAIN
MEMORY
IS
AVAILABLE
FOR
USE
BY
MULTIPLE
PROCESSES
THE
SIMPLEST
SCHEME
FOR
MANAGING
THIS
AVAILABLE
MEMORY
IS
TO
PARTITION
IT
INTO
REGIONS
WITH
FIXED
BOUNDARIES
PARTITION
SIZES
FIGURE
SHOWS
EXAMPLES
OF
TWO
ALTERNATIVES
FOR
FIXED
PARTITIONING
ONE
POSSIBILITY
IS
TO
MAKE
USE
OF
EQUAL
SIZE
PARTITIONS
IN
THIS
CASE
ANY
PROCESS
WHOSE
SIZE
IS
LESS
THAN
OR
EQUAL
TO
THE
PARTITION
SIZE
CAN
BE
LOADED
INTO
MEMORY
PARTITIONING
TABLE
MEMORY
MANAGEMENT
TECHNIQUES
TECHNIQUE
DESCRIPTION
STRENGTHS
WEAKNESSES
FIXED
PARTITIONING
MAIN
MEMORY
IS
DIVIDED
INTO
A
NUMBER
OF
STATIC
PARTITIONS
AT
SYSTEM
GENERATION
TIME
A
PROCESS
MAY
BE
LOADED
INTO
A
PARTITION
OF
EQUAL
OR
GREATER
SIZE
SIMPLE
TO
IMPLEMENT
LITTLE
OPERATING
SYSTEM
OVERHEAD
INEFFICIENT
USE
OF
MEMORY
DUE
TO
INTERNAL
FRAGMENTA
TION
MAXIMUM
NUMBER
OF
ACTIVE
PROCESSES
IS
FIXED
DYNAMIC
PARTITIONING
PARTITIONS
ARE
CREATED
DYNAMICALLY
SO
THAT
EACH
PROCESS
IS
LOADED
INTO
A
PARTITION
OF
EXACTLY
THE
SAME
SIZE
AS
THAT
PROCESS
NO
INTERNAL
FRAGMENTATION
MORE
EFFICIENT
USE
OF
MAIN
MEMORY
INEFFICIENT
USE
OF
PROCESSOR
DUE
TO
THE
NEED
FOR
COM
PACTION
TO
COUNTER
EXTERNAL
FRAGMENTATION
SIMPLE
PAGING
MAIN
MEMORY
IS
DIVIDED
INTO
A
NUMBER
OF
EQUAL
SIZE
FRAMES
EACH
PROCESS
IS
DIVIDED
INTO
A
NUMBER
OF
EQUAL
SIZE
PAGES
OF
THE
SAME
LENGTH
AS
FRAMES
A
PROCESS
IS
LOADED
BY
LOADING
ALL
OF
ITS
PAGES
INTO
AVAILABLE
NOT
NEC
ESSARILY
CONTIGUOUS
FRAMES
NO
EXTERNAL
FRAGMENTATION
A
SMALL
AMOUNT
OF
INTERNAL
FRAGMENTATION
SIMPLE
SEGMENTATION
EACH
PROCESS
IS
DIVIDED
INTO
A
NUMBER
OF
SEGMENTS
A
PROCESS
IS
LOADED
BY
LOAD
ING
ALL
OF
ITS
SEGMENTS
INTO
DYNAMIC
PARTITIONS
THAT
NEED
NOT
BE
CONTIGUOUS
NO
INTERNAL
FRAGMENTATION
IMPROVED
MEMORY
UTILIZA
TION
AND
REDUCED
OVERHEAD
COMPARED
TO
DYNAMIC
PARTITIONING
EXTERNAL
FRAGMENTATION
VIRTUAL
MEMORY
PAGING
AS
WITH
SIMPLE
PAGING
EXCEPT
THAT
IT
IS
NOT
NECESSARY
TO
LOAD
ALL
OF
THE
PAGES
OF
A
PROCESS
NONRESIDENT
PAGES
THAT
ARE
NEEDED
ARE
BROUGHT
IN
LATER
AUTOMATICALLY
NO
EXTERNAL
FRAGMENTATION
HIGHER
DEGREE
OF
MULTIPRO
GRAMMING
LARGE
VIRTUAL
ADDRESS
SPACE
OVERHEAD
OF
COMPLEX
MEMORY
MANAGEMENT
VIRTUAL
MEMORY
SEGMENTATION
AS
WITH
SIMPLE
SEGMENTA
TION
EXCEPT
THAT
IT
IS
NOT
NECESSARY
TO
LOAD
ALL
OF
THE
SEGMENTS
OF
A
PROCESS
NONRESIDENT
SEGMENTS
THAT
ARE
NEEDED
ARE
BROUGHT
IN
LATER
AUTOMATICALLY
NO
INTERNAL
FRAGMENTATION
HIGHER
DEGREE
OF
MULTIPRO
GRAMMING
LARGE
VIRTUAL
ADDRESS
SPACE
PROTECTION
AND
SHARING
SUPPORT
OVERHEAD
OF
COMPLEX
MEMORY
MANAGEMENT
ANY
AVAILABLE
PARTITION
IF
ALL
PARTITIONS
ARE
FULL
AND
NO
PROCESS
IS
IN
THE
READY
OR
RUNNING
STATE
THE
OPERATING
SYSTEM
CAN
SWAP
A
PROCESS
OUT
OF
ANY
OF
THE
PARTITIONS
AND
LOAD
IN
ANOTHER
PROCESS
SO
THAT
THERE
IS
SOME
WORK
FOR
THE
PROCESSOR
THERE
ARE
TWO
DIFFICULTIES
WITH
THE
USE
OF
EQUAL
SIZE
FIXED
PARTITIONS
A
PROGRAM
MAY
BE
TOO
BIG
TO
FIT
INTO
A
PARTITION
IN
THIS
CASE
THE
PROGRAMMER
MUST
DESIGN
THE
PROGRAM
WITH
THE
USE
OF
OVERLAYS
SO
THAT
ONLY
A
PORTION
OF
THE
PROGRAM
NEED
BE
IN
MAIN
MEMORY
AT
ANY
ONE
TIME
WHEN
A
MODULE
IS
NEEDED
CHAPTER
MEMORY
MANAGEMENT
A
EQUAL
SIZE
PARTITIONS
B
UNEQUAL
SIZE
PARTITIONS
FIGURE
EXAMPLE
OF
FIXED
PARTITIONING
OF
A
MBYTE
MEMORY
THAT
IS
NOT
PRESENT
THE
USER
PROGRAM
MUST
LOAD
THAT
MODULE
INTO
THE
PRO
GRAM
PARTITION
OVERLAYING
WHATEVER
PROGRAMS
OR
DATA
ARE
THERE
MAIN
MEMORY
UTILIZATION
IS
EXTREMELY
INEFFICIENT
ANY
PROGRAM
NO
MATTER
HOW
SMALL
OCCUPIES
AN
ENTIRE
PARTITION
IN
OUR
EXAMPLE
THERE
MAY
BE
A
PRO
GRAM
WHOSE
LENGTH
IS
LESS
THAN
MBYTES
YET
IT
OCCUPIES
AN
MBYTE
PARTITION
WHENEVER
IT
IS
SWAPPED
IN
THIS
PHENOMENON
IN
WHICH
THERE
IS
WASTED
SPACE
INTERNAL
TO
A
PARTITION
DUE
TO
THE
FACT
THAT
THE
BLOCK
OF
DATA
LOADED
IS
SMALLER
THAN
THE
PARTITION
IS
REFERRED
TO
AS
INTERNAL
FRAGMENTATION
BOTH
OF
THESE
PROBLEMS
CAN
BE
LESSENED
THOUGH
NOT
SOLVED
BY
USING
UNEQUAL
SIZE
PARTITIONS
FIGURE
IN
THIS
EXAMPLE
PROGRAMS
AS
LARGE
AS
MBYTES
CAN
BE
ACCOMMODATED
WITHOUT
OVERLAYS
PARTITIONS
SMALLER
THAN
MBYTES
ALLOW
SMALLER
PROGRAMS
TO
BE
ACCOMMODATED
WITH
LESS
INTERNAL
FRAGMENTATION
PLACEMENT
ALGORITHM
WITH
EQUAL
SIZE
PARTITIONS
THE
PLACEMENT
OF
PROCESSES
IN
MEMORY
IS
TRIVIAL
AS
LONG
AS
THERE
IS
ANY
AVAILABLE
PARTITION
A
PROCESS
CAN
BE
MEMORY
PARTITIONING
LOADED
INTO
THAT
PARTITION
BECAUSE
ALL
PARTITIONS
ARE
OF
EQUAL
SIZE
IT
DOES
NOT
MATTER
WHICH
PARTITION
IS
USED
IF
ALL
PARTITIONS
ARE
OCCUPIED
WITH
PROCESSES
THAT
ARE
NOT
READY
TO
RUN
THEN
ONE
OF
THESE
PROCESSES
MUST
BE
SWAPPED
OUT
TO
MAKE
ROOM
FOR
A
NEW
PROCESS
WHICH
ONE
TO
SWAP
OUT
IS
A
SCHEDULING
DECISION
THIS
TOPIC
IS
EXPLORED
IN
PART
FOUR
WITH
UNEQUAL
SIZE
PARTITIONS
THERE
ARE
TWO
POSSIBLE
WAYS
TO
ASSIGN
PROCESSES
TO
PARTITIONS
THE
SIMPLEST
WAY
IS
TO
ASSIGN
EACH
PROCESS
TO
THE
SMALLEST
PARTITION
WITHIN
WHICH
IT
WILL
FIT
IN
THIS
CASE
A
SCHEDULING
QUEUE
IS
NEEDED
FOR
EACH
PARTI
TION
TO
HOLD
SWAPPED
OUT
PROCESSES
DESTINED
FOR
THAT
PARTITION
FIGURE
THE
ADVANTAGE
OF
THIS
APPROACH
IS
THAT
PROCESSES
ARE
ALWAYS
ASSIGNED
IN
SUCH
A
WAY
AS
TO
MINIMIZE
WASTED
MEMORY
WITHIN
A
PARTITION
INTERNAL
FRAGMENTATION
ALTHOUGH
THIS
TECHNIQUE
SEEMS
OPTIMUM
FROM
THE
POINT
OF
VIEW
OF
AN
INDI
VIDUAL
PARTITION
IT
IS
NOT
OPTIMUM
FROM
THE
POINT
OF
VIEW
OF
THE
SYSTEM
AS
A
WHOLE
IN
FIGURE
FOR
EXAMPLE
CONSIDER
A
CASE
IN
WHICH
THERE
ARE
NO
PROCESSES
WITH
A
SIZE
BETWEEN
AND
AT
A
CERTAIN
POINT
IN
TIME
IN
THAT
CASE
THE
PARTITION
WILL
REMAIN
UNUSED
EVEN
THOUGH
SOME
SMALLER
PROCESS
COULD
HAVE
BEEN
ASSIGNED
TO
IT
THUS
A
PREFERABLE
APPROACH
WOULD
BE
TO
EMPLOY
A
SINGLE
QUEUE
FOR
ALL
PROCESSES
FIGURE
WHEN
IT
IS
TIME
TO
LOAD
A
PROCESS
INTO
MAIN
MEMORY
THE
SMALLEST
AVAILABLE
PARTITION
THAT
WILL
HOLD
THE
PROCESS
IS
SELECTED
IF
ALL
PARTITIONS
ARE
OCCUPIED
THEN
A
SWAPPING
DECISION
MUST
BE
MADE
PREFERENCE
MIGHT
BE
GIVEN
TO
SWAPPING
OUT
OF
THE
SMALLEST
PARTITION
THAT
WILL
HOLD
THE
INCOMING
PROCESS
IT
IS
ALSO
POSSIBLE
TO
NEW
PROCESSES
NEW
PROCESSES
A
ONE
PROCESS
QUEUE
PER
PARTITION
B
SINGLE
QUEUE
FIGURE
MEMORY
ASSIGNMENT
FOR
FIXED
PARTITIONING
ASSUMES
THAT
ONE
KNOWS
THE
MAXIMUM
AMOUNT
OF
MEMORY
THAT
A
PROCESS
WILL
REQUIRE
THIS
IS
NOT
ALWAYS
THE
CASE
IF
IT
IS
NOT
KNOWN
HOW
LARGE
A
PROCESS
MAY
BECOME
THE
ONLY
ALTERNATIVES
ARE
AN
OVERLAY
SCHEME
OR
THE
USE
OF
VIRTUAL
MEMORY
CHAPTER
MEMORY
MANAGEMENT
CONSIDER
OTHER
FACTORS
SUCH
AS
PRIORITY
AND
A
PREFERENCE
FOR
SWAPPING
OUT
BLOCKED
PROCESSES
VERSUS
READY
PROCESSES
THE
USE
OF
UNEQUAL
SIZE
PARTITIONS
PROVIDES
A
DEGREE
OF
FLEXIBILITY
TO
FIXED
PARTITIONING
IN
ADDITION
IT
CAN
BE
SAID
THAT
FIXED
PARTITIONING
SCHEMES
ARE
RELATIVELY
SIMPLE
AND
REQUIRE
MINIMAL
OS
SOFTWARE
AND
PROCESSING
OVERHEAD
HOWEVER
THERE
ARE
DISADVANTAGES
THE
NUMBER
OF
PARTITIONS
SPECIFIED
AT
SYSTEM
GENERATION
TIME
LIMITS
THE
NUMBER
OF
ACTIVE
NOT
SUSPENDED
PROCESSES
IN
THE
SYSTEM
BECAUSE
PARTITION
SIZES
ARE
PRESET
AT
SYSTEM
GENERATION
TIME
SMALL
JOBS
WILL
NOT
UTILIZE
PARTITION
SPACE
EFFICIENTLY
IN
AN
ENVIRONMENT
WHERE
THE
MAIN
STORAGE
REQUIREMENT
OF
ALL
JOBS
IS
KNOWN
BEFOREHAND
THIS
MAY
BE
REASONABLE
BUT
IN
MOST
CASES
IT
IS
AN
INEFFICIENT
TECHNIQUE
THE
USE
OF
FIXED
PARTITIONING
IS
ALMOST
UNKNOWN
TODAY
ONE
EXAMPLE
OF
A
SUC
CESSFUL
OPERATING
SYSTEM
THAT
DID
USE
THIS
TECHNIQUE
WAS
AN
EARLY
IBM
MAINFRAME
OPERATING
SYSTEM
OS
MFT
MULTIPROGRAMMING
WITH
A
FIXED
NUMBER
OF
TASKS
DYNAMIC
PARTITIONING
TO
OVERCOME
SOME
OF
THE
DIFFICULTIES
WITH
FIXED
PARTITIONING
AN
APPROACH
KNOWN
AS
DYNAMIC
PARTITIONING
WAS
DEVELOPED
AGAIN
THIS
APPROACH
HAS
BEEN
SUPPLANTED
BY
MORE
SOPHISTICATED
MEMORY
MANAGEMENT
TECHNIQUES
AN
IMPORTANT
OPERATING
SYSTEM
THAT
USED
THIS
TECHNIQUE
WAS
IBM
MAINFRAME
OPERATING
SYSTEM
OS
MVT
MULTIPROGRAMMING
WITH
A
VARIABLE
NUMBER
OF
TASKS
WITH
DYNAMIC
PARTITIONING
THE
PARTITIONS
ARE
OF
VARIABLE
LENGTH
AND
NUMBER
WHEN
A
PROCESS
IS
BROUGHT
INTO
MAIN
MEMORY
IT
IS
ALLOCATED
EXACTLY
AS
MUCH
MEM
ORY
AS
IT
REQUIRES
AND
NO
MORE
AN
EXAMPLE
USING
MBYTES
OF
MAIN
MEMORY
IS
SHOWN
IN
FIGURE
INITIALLY
MAIN
MEMORY
IS
EMPTY
EXCEPT
FOR
THE
OS
A
THE
FIRST
THREE
PROCESSES
ARE
LOADED
IN
STARTING
WHERE
THE
OPERATING
SYSTEM
ENDS
AND
OCCUPYING
JUST
ENOUGH
SPACE
FOR
EACH
PROCESS
B
C
D
THIS
LEAVES
A
HOLE
AT
THE
END
OF
MEMORY
THAT
IS
TOO
SMALL
FOR
A
FOURTH
PROCESS
AT
SOME
POINT
NONE
OF
THE
PROCESSES
IN
MEMORY
IS
READY
THE
OPERATING
SYSTEM
SWAPS
OUT
PROCESS
E
WHICH
LEAVES
SUFFICIENT
ROOM
TO
LOAD
A
NEW
PROCESS
PROCESS
F
BECAUSE
PROCESS
IS
SMALLER
THAN
PROCESS
ANOTHER
SMALL
HOLE
IS
CREATED
LATER
A
POINT
IS
REACHED
AT
WHICH
NONE
OF
THE
PROCESSES
IN
MAIN
MEMORY
IS
READY
BUT
PROCESS
IN
THE
READY
SUSPEND
STATE
IS
AVAILABLE
BECAUSE
THERE
IS
INSUFFICIENT
ROOM
IN
MEMORY
FOR
PROCESS
THE
OPERATING
SYSTEM
SWAPS
PROCESS
OUT
G
AND
SWAPS
PROCESS
BACK
IN
H
AS
THIS
EXAMPLE
SHOWS
THIS
METHOD
STARTS
OUT
WELL
BUT
EVENTUALLY
IT
LEADS
TO
A
SITUATION
IN
WHICH
THERE
ARE
A
LOT
OF
SMALL
HOLES
IN
MEMORY
AS
TIME
GOES
ON
MEM
ORY
BECOMES
MORE
AND
MORE
FRAGMENTED
AND
MEMORY
UTILIZATION
DECLINES
THIS
PHENOMENON
IS
REFERRED
TO
AS
EXTERNAL
FRAGMENTATION
INDICATING
THAT
THE
MEMORY
THAT
IS
EXTERNAL
TO
ALL
PARTITIONS
BECOMES
INCREASINGLY
FRAGMENTED
THIS
IS
IN
CONTRAST
TO
INTERNAL
FRAGMENTATION
REFERRED
TO
EARLIER
ONE
TECHNIQUE
FOR
OVERCOMING
EXTERNAL
FRAGMENTATION
IS
COMPACTION
FROM
TIME
TO
TIME
THE
OS
SHIFTS
THE
PROCESSES
SO
THAT
THEY
ARE
CONTIGUOUS
AND
SO
THAT
ALL
OF
THE
FREE
MEMORY
IS
TOGETHER
IN
ONE
BLOCK
FOR
EXAMPLE
IN
FIGURE
COMPACTION
MEMORY
PARTITIONING
A
OPERATING
SYSTEM
PROCESS
PROCESS
E
B
OPERATING
SYSTEM
PROCESS
PROCESS
PROCESS
F
C
OPERATING
SYSTEM
PROCESS
PROCESS
G
D
OPERATING
SYSTEM
PROCESS
PROCESS
PROCESS
H
FIGURE
THE
EFFECT
OF
DYNAMIC
PARTITIONING
WILL
RESULT
IN
A
BLOCK
OF
FREE
MEMORY
OF
LENGTH
THIS
MAY
WELL
BE
SUFFICIENT
TO
LOAD
IN
AN
ADDITIONAL
PROCESS
THE
DIFFICULTY
WITH
COMPACTION
IS
THAT
IT
IS
A
TIME
CONSUMING
PROCEDURE
AND
WASTEFUL
OF
PROCESSOR
TIME
NOTE
THAT
COMPACTION
IMPLIES
THE
NEED
FOR
A
DYNAMIC
RELOCATION
CAPABILITY
THAT
IS
IT
MUST
BE
POSSIBLE
TO
MOVE
A
PROGRAM
FROM
ONE
REGION
TO
ANOTHER
IN
MAIN
MEMORY
WITHOUT
INVALIDATING
THE
MEMORY
REFERENCES
IN
THE
PROGRAM
SEE
APPENDIX
PLACEMENT
ALGORITHM
BECAUSE
MEMORY
COMPACTION
IS
TIME
CONSUMING
THE
OS
DESIGNER
MUST
BE
CLEVER
IN
DECIDING
HOW
TO
ASSIGN
PROCESSES
TO
MEMORY
HOW
TO
PLUG
THE
HOLES
WHEN
IT
IS
TIME
TO
LOAD
OR
SWAP
A
PROCESS
INTO
MAIN
MEMORY
AND
IF
THERE
IS
MORE
THAN
ONE
FREE
BLOCK
OF
MEMORY
OF
SUFFICIENT
SIZE
THEN
THE
OPERATING
SYSTEM
MUST
DECIDE
WHICH
FREE
BLOCK
TO
ALLOCATE
THREE
PLACEMENT
ALGORITHMS
THAT
MIGHT
BE
CONSIDERED
ARE
BEST
FIT
FIRST
FIT
AND
NEXT
FIT
ALL
OF
COURSE
ARE
LIMITED
TO
CHOOSING
AMONG
FREE
BLOCKS
OF
MAIN
MEMORY
THAT
ARE
EQUAL
TO
OR
LARGER
THAN
THE
PROCESS
TO
BE
BROUGHT
IN
BEST
FIT
CHOOSES
THE
BLOCK
THAT
IS
CLOSEST
IN
SIZE
TO
THE
REQUEST
FIRST
FIT
BEGINS
TO
SCAN
MEMORY
FROM
THE
CHAPTER
MEMORY
MANAGEMENT
FIRST
FIT
LAST
ALLOCATED
BLOCK
A
BEFORE
BEST
FIT
ALLOCATED
BLOCK
FREE
BLOCK
POSSIBLE
NEW
ALLOCATION
NEXT
FIT
20M
B
AFTER
FIGURE
EXAMPLE
MEMORY
CONFIGURATION
BEFORE
AND
AFTER
ALLOCATION
OF
MBYTE
BLOCK
BEGINNING
AND
CHOOSES
THE
FIRST
AVAILABLE
BLOCK
THAT
IS
LARGE
ENOUGH
NEXT
FIT
BEGINS
TO
SCAN
MEMORY
FROM
THE
LOCATION
OF
THE
LAST
PLACEMENT
AND
CHOOSES
THE
NEXT
AVAIL
ABLE
BLOCK
THAT
IS
LARGE
ENOUGH
FIGURE
SHOWS
AN
EXAMPLE
MEMORY
CONFIGURATION
AFTER
A
NUMBER
OF
PLACE
MENT
AND
SWAPPING
OUT
OPERATIONS
THE
LAST
BLOCK
THAT
WAS
USED
WAS
A
MBYTE
BLOCK
FROM
WHICH
A
MBYTE
PARTITION
WAS
CREATED
FIGURE
SHOWS
THE
DIFFERENCE
BETWEEN
THE
BEST
FIRST
AND
NEXT
FIT
PLACEMENT
ALGORITHMS
IN
SATISFYING
A
MBYTE
ALLOCATION
REQUEST
BEST
FIT
WILL
SEARCH
THE
ENTIRE
LIST
OF
AVAILABLE
BLOCKS
AND
MAKE
USE
OF
THE
MBYTE
BLOCK
LEAVING
A
MBYTE
FRAGMENT
FIRST
FIT
RESULTS
IN
A
MBYTE
FRAGMENT
AND
NEXT
FIT
RESULTS
IN
A
20
MBYTE
FRAGMENT
WHICH
OF
THESE
APPROACHES
IS
BEST
WILL
DEPEND
ON
THE
EXACT
SEQUENCE
OF
PROC
ESS
SWAPPINGS
THAT
OCCURS
AND
THE
SIZE
OF
THOSE
PROCESSES
HOWEVER
SOME
GENERAL
COMMENTS
CAN
BE
MADE
SEE
ALSO
AND
THE
FIRST
FIT
ALGORITHM
IS
NOT
ONLY
THE
SIMPLEST
BUT
USUALLY
THE
BEST
AND
FASTEST
AS
WELL
THE
NEXT
FIT
ALGORITHM
TENDS
TO
PRODUCE
SLIGHTLY
WORSE
RESULTS
THAN
THE
FIRST
FIT
THE
NEXT
FIT
ALGORITHM
WILL
MORE
FREQUENTLY
LEAD
TO
AN
ALLOCATION
FROM
A
FREE
BLOCK
AT
THE
END
OF
MEMORY
THE
RESULT
IS
THAT
THE
LARGEST
BLOCK
OF
FREE
MEMORY
WHICH
USUALLY
MEMORY
PARTITIONING
APPEARS
AT
THE
END
OF
THE
MEMORY
SPACE
IS
QUICKLY
BROKEN
UP
INTO
SMALL
FRAGMENTS
THUS
COMPACTION
MAY
BE
REQUIRED
MORE
FREQUENTLY
WITH
NEXT
FIT
ON
THE
OTHER
HAND
THE
FIRST
FIT
ALGORITHM
MAY
LITTER
THE
FRONT
END
WITH
SMALL
FREE
PARTITIONS
THAT
NEED
TO
BE
SEARCHED
OVER
ON
EACH
SUBSEQUENT
FIRST
FIT
PASS
THE
BEST
FIT
ALGORITHM
DESPITE
ITS
NAME
IS
USUALLY
THE
WORST
PERFORMER
BECAUSE
THIS
ALGORITHM
LOOKS
FOR
THE
SMALLEST
BLOCK
THAT
WILL
SATISFY
THE
REQUIREMENT
IT
GUARANTEES
THAT
THE
FRAGMENT
LEFT
BEHIND
IS
AS
SMALL
AS
POSSIBLE
ALTHOUGH
EACH
MEMORY
REQUEST
ALWAYS
WASTES
THE
SMALLEST
AMOUNT
OF
MEMORY
THE
RESULT
IS
THAT
MAIN
MEMORY
IS
QUICKLY
LITTERED
BY
BLOCKS
TOO
SMALL
TO
SATISFY
MEMORY
ALLOCATION
REQUESTS
THUS
MEMORY
COMPAC
TION
MUST
BE
DONE
MORE
FREQUENTLY
THAN
WITH
THE
OTHER
ALGORITHMS
REPLACEMENT
ALGORITHM
IN
A
MULTIPROGRAMMING
SYSTEM
USING
DYNAMIC
PARTITIONING
THERE
WILL
COME
A
TIME
WHEN
ALL
OF
THE
PROCESSES
IN
MAIN
MEMORY
ARE
IN
A
BLOCKED
STATE
AND
THERE
IS
INSUFFICIENT
MEMORY
EVEN
AFTER
COMPACTION
FOR
AN
ADDITIONAL
PROCESS
TO
AVOID
WASTING
PROCESSOR
TIME
WAITING
FOR
AN
ACTIVE
PROCESS
TO
BECOME
UNBLOCKED
THE
OS
WILL
SWAP
ONE
OF
THE
PROCESSES
OUT
OF
MAIN
MEMORY
TO
MAKE
ROOM
FOR
A
NEW
PROCESS
OR
FOR
A
PROCESS
IN
A
READY
SUSPEND
STATE
THEREFORE
THE
OPERATING
SYSTEM
MUST
CHOOSE
WHICH
PROCESS
TO
REPLACE
BECAUSE
THE
TOPIC
OF
REPLACEMENT
ALGORITHMS
WILL
BE
COVERED
IN
SOME
DETAIL
WITH
RESPECT
TO
VARIOUS
VIRTUAL
MEMORY
SCHEMES
WE
DEFER
A
DISCUSSION
OF
REPLACEMENT
ALGORITHMS
UNTIL
THEN
BUDDY
SYSTEM
BOTH
FIXED
AND
DYNAMIC
PARTITIONING
SCHEMES
HAVE
DRAWBACKS
A
FIXED
PARTITION
ING
SCHEME
LIMITS
THE
NUMBER
OF
ACTIVE
PROCESSES
AND
MAY
USE
SPACE
INEFFICIENTLY
IF
THERE
IS
A
POOR
MATCH
BETWEEN
AVAILABLE
PARTITION
SIZES
AND
PROCESS
SIZES
A
DYNAMIC
PARTITIONING
SCHEME
IS
MORE
COMPLEX
TO
MAINTAIN
AND
INCLUDES
THE
OVER
HEAD
OF
COMPACTION
AN
INTERESTING
COMPROMISE
IS
THE
BUDDY
SYSTEM
IN
A
BUDDY
SYSTEM
MEMORY
BLOCKS
ARE
AVAILABLE
OF
SIZE
WORDS
L
K
U
WHERE
SMALLEST
SIZE
BLOCK
THAT
IS
ALLOCATED
LARGEST
SIZE
BLOCK
THAT
IS
ALLOCATED
GENERALLY
IS
THE
SIZE
OF
THE
ENTIRE
MEMORY
AVAILABLE
FOR
ALLOCATION
TO
BEGIN
THE
ENTIRE
SPACE
AVAILABLE
FOR
ALLOCATION
IS
TREATED
AS
A
SINGLE
BLOCK
OF
SIZE
IF
A
REQUEST
OF
SIZE
SUCH
THAT
IS
MADE
THEN
THE
ENTIRE
BLOCK
IS
ALLOCATED
OTHERWISE
THE
BLOCK
IS
SPLIT
INTO
TWO
EQUAL
BUDDIES
OF
SIZE
IF
THEN
THE
REQUEST
IS
ALLOCATED
TO
ONE
OF
THE
TWO
BUDDIES
OTHERWISE
ONE
OF
THE
BUDDIES
IS
SPLIT
IN
HALF
AGAIN
THIS
PROCESS
CONTINUES
UNTIL
THE
SMALLEST
BLOCK
GREATER
THAN
OR
EQUAL
TO
IS
GENERATED
AND
ALLOCATED
TO
THE
REQUEST
AT
ANY
TIME
THE
BUDDY
SYSTEM
MAINTAINS
A
LIST
OF
HOLES
UNALLOCATED
BLOCKS
OF
EACH
SIZE
A
HOLE
MAY
BE
REMOVED
FROM
THE
I
LIST
BY
SPLITTING
IT
IN
HALF
TO
CREATE
TWO
BUDDIES
OF
SIZE
IN
THE
I
LIST
WHENEVER
A
PAIR
OF
BUDDIES
ON
THE
I
LIST
BOTH
BECOME
UNALLOCATED
THEY
ARE
REMOVED
FROM
THAT
LIST
AND
COALESCED
INTO
A
SINGLE
BLOCK
ON
THE
I
CHAPTER
MEMORY
MANAGEMENT
LIST
PRESENTED
WITH
A
REQUEST
FOR
AN
ALLOCATION
OF
SIZE
K
SUCH
THAT
K
CS
THE
FOLLOWING
RECURSIVE
ALGORITHM
IS
USED
TO
FIND
A
HOLE
OF
SIZE
VOID
INT
I
IF
I
U
FAILURE
IF
EMPTY
I
SPLIT
HOLE
INTO
BUDDIES
PUT
BUDDIES
ON
TAKE
FIRST
HOLE
ON
FIGURE
GIVES
AN
EXAMPLE
USING
A
MBYTE
INITIAL
BLOCK
THE
FIRST
REQUEST
A
IS
FOR
KBYTES
FOR
WHICH
A
BLOCK
IS
NEEDED
THE
INITIAL
BLOCK
IS
DIVIDED
INTO
TWO
BUDDIES
THE
FIRST
OF
THESE
IS
DIVIDED
INTO
TWO
BUDDIES
AND
THE
FIRST
OF
THESE
IS
DIVIDED
INTO
TWO
BUDDIES
ONE
OF
WHICH
IS
ALLOCATED
TO
A
THE
NEXT
REQUEST
B
REQUIRES
A
BLOCK
SUCH
A
BLOCK
IS
ALREADY
AVAILABLE
AND
IS
ALLOCATED
THE
PROCESS
CONTINUES
WITH
SPLITTING
AND
COALESCING
OCCURRING
AS
NEEDED
NOTE
THAT
WHEN
E
IS
RELEASED
TWO
BUDDIES
ARE
COALESCED
INTO
A
BLOCK
WHICH
IS
IMMEDIATELY
COALESCED
WITH
ITS
BUDDY
FIGURE
SHOWS
A
BINARY
TREE
REPRESENTATION
OF
THE
BUDDY
ALLOCATION
IMMEDI
ATELY
AFTER
THE
RELEASE
B
REQUEST
THE
LEAF
NODES
REPRESENT
THE
CURRENT
PARTITIONING
OF
THE
MEMORY
IF
TWO
BUDDIES
ARE
LEAF
NODES
THEN
AT
LEAST
ONE
MUST
BE
ALLOCATED
OTHERWISE
THEY
WOULD
BE
COALESCED
INTO
A
LARGER
BLOCK
MBYTE
BLOCK
REQUEST
REQUEST
REQUEST
REQUEST
RELEASE
B
RELEASE
A
REQUEST
RELEASE
C
RELEASE
E
RELEASE
D
FIGURE
EXAMPLE
OF
BUDDY
SYSTEM
MEMORY
PARTITIONING
A
C
D
LEAF
NODE
FOR
ALLOCATED
BLOCK
LEAF
NODE
FOR
UNALLOCATED
BLOCK
NON
LEAF
NODE
FIGURE
TREE
REPRESENTATION
OF
BUDDY
SYSTEM
THE
BUDDY
SYSTEM
IS
A
REASONABLE
COMPROMISE
TO
OVERCOME
THE
DISADVANTAGES
OF
BOTH
THE
FIXED
AND
VARIABLE
PARTITIONING
SCHEMES
BUT
IN
CONTEMPORARY
OPERATING
SYSTEMS
VIRTUAL
MEMORY
BASED
ON
PAGING
AND
SEGMENTATION
IS
SUPERIOR
HOWEVER
THE
BUDDY
SYSTEM
HAS
FOUND
APPLICATION
IN
PARALLEL
SYSTEMS
AS
AN
EFFICIENT
MEANS
OF
ALLOCATION
AND
RELEASE
FOR
PARALLEL
PROGRAMS
E
G
SEE
A
MODIFIED
FORM
OF
THE
BUDDY
SYSTEM
IS
USED
FOR
UNIX
KERNEL
MEMORY
ALLOCATION
DESCRIBED
IN
CHAPTER
RELOCATION
BEFORE
WE
CONSIDER
WAYS
OF
DEALING
WITH
THE
SHORTCOMINGS
OF
PARTITIONING
WE
MUST
CLEAR
UP
ONE
LOOSE
END
WHICH
RELATES
TO
THE
PLACEMENT
OF
PROCESSES
IN
MEMORY
WHEN
THE
FIXED
PARTITION
SCHEME
OF
FIGURE
IS
USED
WE
CAN
EXPECT
THAT
A
PRO
CESS
WILL
ALWAYS
BE
ASSIGNED
TO
THE
SAME
PARTITION
THAT
IS
WHICHEVER
PARTITION
IS
SELECTED
WHEN
A
NEW
PROCESS
IS
LOADED
WILL
ALWAYS
BE
USED
TO
SWAP
THAT
PROCESS
BACK
INTO
MEMORY
AFTER
IT
HAS
BEEN
SWAPPED
OUT
IN
THAT
CASE
A
SIMPLE
RELOCATING
LOADER
SUCH
AS
IS
DESCRIBED
IN
APPENDIX
CAN
BE
USED
WHEN
THE
PROCESS
IS
FIRST
LOADED
ALL
RELATIVE
MEMORY
REFERENCES
IN
THE
CODE
ARE
REPLACED
BY
ABSOLUTE
MAIN
MEMORY
ADDRESSES
DETERMINED
BY
THE
BASE
ADDRESS
OF
THE
LOADED
PROCESS
IN
THE
CASE
OF
EQUAL
SIZE
PARTITIONS
FIGURE
AND
IN
THE
CASE
OF
A
SINGLE
PROC
ESS
QUEUE
FOR
UNEQUAL
SIZE
PARTITIONS
FIGURE
A
PROCESS
MAY
OCCUPY
DIFFERENT
PARTITIONS
DURING
THE
COURSE
OF
ITS
LIFE
WHEN
A
PROCESS
IMAGE
IS
FIRST
CREATED
IT
IS
CHAPTER
MEMORY
MANAGEMENT
LOADED
INTO
SOME
PARTITION
IN
MAIN
MEMORY
LATER
THE
PROCESS
MAY
BE
SWAPPED
OUT
WHEN
IT
IS
SUBSEQUENTLY
SWAPPED
BACK
IN
IT
MAY
BE
ASSIGNED
TO
A
DIFFERENT
PARTITION
THAN
THE
LAST
TIME
THE
SAME
IS
TRUE
FOR
DYNAMIC
PARTITIONING
OBSERVE
IN
FIGURE
AND
FIGURE
THAT
PROCESS
OCCUPIES
TWO
DIFFERENT
REGIONS
OF
MEMORY
ON
THE
TWO
OCCASIONS
WHEN
IT
IS
BROUGHT
IN
FURTHERMORE
WHEN
COMPACTION
IS
USED
PROCESSES
ARE
SHIFTED
WHILE
THEY
ARE
IN
MAIN
MEMORY
THUS
THE
LOCATIONS
OF
INSTRUCTIONS
AND
DATA
REFERENCED
BY
A
PROCESS
ARE
NOT
FIXED
THEY
WILL
CHANGE
EACH
TIME
A
PROCESS
IS
SWAPPED
IN
OR
SHIFTED
TO
SOLVE
THIS
PROBLEM
A
DISTINCTION
IS
MADE
AMONG
SEVERAL
TYPES
OF
ADDRESSES
A
LOGICAL
ADDRESS
IS
A
REFERENCE
TO
A
MEMORY
LOCATION
INDEPEND
ENT
OF
THE
CURRENT
ASSIGNMENT
OF
DATA
TO
MEMORY
A
TRANSLATION
MUST
BE
MADE
TO
A
PHYSICAL
ADDRESS
BEFORE
THE
MEMORY
ACCESS
CAN
BE
ACHIEVED
A
RELATIVE
ADDRESS
IS
A
PARTICULAR
EXAMPLE
OF
LOGICAL
ADDRESS
IN
WHICH
THE
ADDRESS
IS
EXPRESSED
AS
A
LOCATION
RELATIVE
TO
SOME
KNOWN
POINT
USUALLY
A
VALUE
IN
A
PROCESSOR
REGISTER
A
PHYSICAL
ADDRESS
OR
ABSOLUTE
ADDRESS
IS
AN
ACTUAL
LOCATION
IN
MAIN
MEMORY
PROGRAMS
THAT
EMPLOY
RELATIVE
ADDRESSES
IN
MEMORY
ARE
LOADED
USING
DYNAMIC
RUN
TIME
LOADING
SEE
APPENDIX
FOR
A
DISCUSSION
TYPICALLY
ALL
OF
THE
MEMORY
REFERENCES
IN
THE
LOADED
PROCESS
ARE
RELATIVE
TO
THE
ORIGIN
OF
THE
PROGRAM
THUS
A
HARD
WARE
MECHANISM
IS
NEEDED
FOR
TRANSLATING
RELATIVE
ADDRESSES
TO
PHYSICAL
MAIN
MEMORY
ADDRESSES
AT
THE
TIME
OF
EXECUTION
OF
THE
INSTRUCTION
THAT
CONTAINS
THE
REFERENCE
FIGURE
SHOWS
THE
WAY
IN
WHICH
THIS
ADDRESS
TRANSLATION
IS
TYPICALLY
ACCOM
PLISHED
WHEN
A
PROCESS
IS
ASSIGNED
TO
THE
RUNNING
STATE
A
SPECIAL
PROCESSOR
REGISTER
SOMETIMES
CALLED
THE
BASE
REGISTER
IS
LOADED
WITH
THE
STARTING
ADDRESS
IN
MAIN
MEMORY
OF
THE
PROGRAM
THERE
IS
ALSO
A
BOUNDS
REGISTER
THAT
INDICATES
THE
ENDING
LOCATION
RELATIVE
ADDRESS
FIGURE
HARDWARE
SUPPORT
FOR
RELOCATION
PROCESS
IMAGE
IN
MAIN
MEMORY
PAGING
OF
THE
PROGRAM
THESE
VALUES
MUST
BE
SET
WHEN
THE
PROGRAM
IS
LOADED
INTO
MEMORY
OR
WHEN
THE
PROCESS
IMAGE
IS
SWAPPED
IN
DURING
THE
COURSE
OF
EXECUTION
OF
THE
PROC
ESS
RELATIVE
ADDRESSES
ARE
ENCOUNTERED
THESE
INCLUDE
THE
CONTENTS
OF
THE
INSTRUC
TION
REGISTER
INSTRUCTION
ADDRESSES
THAT
OCCUR
IN
BRANCH
AND
CALL
INSTRUCTIONS
AND
DATA
ADDRESSES
THAT
OCCUR
IN
LOAD
AND
STORE
INSTRUCTIONS
EACH
SUCH
RELATIVE
ADDRESS
GOES
THROUGH
TWO
STEPS
OF
MANIPULATION
BY
THE
PROCESSOR
FIRST
THE
VALUE
IN
THE
BASE
REGISTER
IS
ADDED
TO
THE
RELATIVE
ADDRESS
TO
PRODUCE
AN
ABSOLUTE
ADDRESS
SECOND
THE
RESULTING
ADDRESS
IS
COMPARED
TO
THE
VALUE
IN
THE
BOUNDS
REGISTER
IF
THE
ADDRESS
IS
WITHIN
BOUNDS
THEN
THE
INSTRUCTION
EXECUTION
MAY
PROCEED
OTHERWISE
AN
INTERRUPT
IS
GENERATED
TO
THE
OPERATING
SYSTEM
WHICH
MUST
RESPOND
TO
THE
ERROR
IN
SOME
FASHION
THE
SCHEME
OF
FIGURE
ALLOWS
PROGRAMS
TO
BE
SWAPPED
IN
AND
OUT
OF
MEM
ORY
DURING
THE
COURSE
OF
EXECUTION
IT
ALSO
PROVIDES
A
MEASURE
OF
PROTECTION
EACH
PROCESS
IMAGE
IS
ISOLATED
BY
THE
CONTENTS
OF
THE
BASE
AND
BOUNDS
REGISTERS
AND
SAFE
FROM
UNWANTED
ACCESSES
BY
OTHER
PROCESSES
PAGING
BOTH
UNEQUAL
FIXED
SIZE
AND
VARIABLE
SIZE
PARTITIONS
ARE
INEFFICIENT
IN
THE
USE
OF
MEMORY
THE
FORMER
RESULTS
IN
INTERNAL
FRAGMENTATION
THE
LATTER
IN
EXTERNAL
FRAG
MENTATION
SUPPOSE
HOWEVER
THAT
MAIN
MEMORY
IS
PARTITIONED
INTO
EQUAL
FIXED
SIZE
CHUNKS
THAT
ARE
RELATIVELY
SMALL
AND
THAT
EACH
PROCESS
IS
ALSO
DIVIDED
INTO
SMALL
FIXED
SIZE
CHUNKS
OF
THE
SAME
SIZE
THEN
THE
CHUNKS
OF
A
PROCESS
KNOWN
AS
PAGES
COULD
BE
ASSIGNED
TO
AVAILABLE
CHUNKS
OF
MEMORY
KNOWN
AS
FRAMES
OR
PAGE
FRAMES
WE
SHOW
IN
THIS
SECTION
THAT
THE
WASTED
SPACE
IN
MEMORY
FOR
EACH
PROCESS
IS
DUE
TO
INTERNAL
FRAGMENTATION
CONSISTING
OF
ONLY
A
FRACTION
OF
THE
LAST
PAGE
OF
A
PROCESS
THERE
IS
NO
EXTERNAL
FRAGMENTATION
FIGURE
ILLUSTRATES
THE
USE
OF
PAGES
AND
FRAMES
AT
A
GIVEN
POINT
IN
TIME
SOME
OF
THE
FRAMES
IN
MEMORY
ARE
IN
USE
AND
SOME
ARE
FREE
A
LIST
OF
FREE
FRAMES
IS
MAIN
TAINED
BY
THE
OS
PROCESS
A
STORED
ON
DISK
CONSISTS
OF
FOUR
PAGES
WHEN
IT
IS
TIME
TO
LOAD
THIS
PROCESS
THE
OS
FINDS
FOUR
FREE
FRAMES
AND
LOADS
THE
FOUR
PAGES
OF
PROCESS
A
INTO
THE
FOUR
FRAMES
FIGURE
PROCESS
B
CONSISTING
OF
THREE
PAGES
AND
PROCESS
C
CONSISTING
OF
FOUR
PAGES
ARE
SUBSEQUENTLY
LOADED
THEN
PROCESS
B
IS
SUSPENDED
AND
IS
SWAPPED
OUT
OF
MAIN
MEMORY
LATER
ALL
OF
THE
PROCESSES
IN
MAIN
MEMORY
ARE
BLOCKED
AND
THE
OS
NEEDS
TO
BRING
IN
A
NEW
PROCESS
PROCESS
D
WHICH
CONSISTS
OF
FIVE
PAGES
NOW
SUPPOSE
AS
IN
THIS
EXAMPLE
THAT
THERE
ARE
NOT
SUFFICIENT
UNUSED
CONTIGUOUS
FRAMES
TO
HOLD
THE
PROCESS
DOES
THIS
PREVENT
THE
OPERATING
SYSTEM
FROM
LOADING
D
THE
ANSWER
IS
NO
BECAUSE
WE
CAN
ONCE
AGAIN
USE
THE
CONCEPT
OF
LOGICAL
ADDRESS
A
SIMPLE
BASE
ADDRESS
REGISTER
WILL
NO
LONGER
SUFFICE
RATHER
THE
OPERATING
SYSTEM
MAINTAINS
A
PAGE
TABLE
FOR
EACH
PROCESS
THE
PAGE
TABLE
SHOWS
THE
FRAME
LOCATION
FOR
EACH
PAGE
OF
THE
PROCESS
WITHIN
THE
PROGRAM
EACH
LOGICAL
ADDRESS
CONSISTS
OF
A
PAGE
NUMBER
AND
AN
OFFSET
WITHIN
THE
PAGE
RECALL
THAT
IN
THE
CASE
OF
SIMPLE
PARTITION
A
LOGICAL
ADDRESS
IS
THE
LOCATION
OF
A
WORD
RELATIVE
TO
THE
BEGINNING
OF
THE
PROGRAM
THE
PROCESSOR
TRANSLATES
THAT
INTO
A
PHYSICAL
ADDRESS
WITH
PAGING
THE
LOGICAL
TO
PHYSICAL
ADDRESS
TRANSLATION
IS
STILL
DONE
BY
PROCESSOR
HARDWARE
NOW
THE
PROCESSOR
MUST
KNOW
HOW
TO
ACCESS
THE
PAGE
TABLE
OF
THE
CURRENT
PROCESS
PRESENTED
WITH
A
LOGICAL
CHAPTER
MEMORY
MANAGEMENT
FRAME
NUMBER
MAIN
MEMORY
MAIN
MEMORY
MAIN
MEMORY
A
FIFTEEN
AVAILABLE
FRAMES
B
LOAD
PROCESS
A
C
LOAD
PROCESS
B
MAIN
MEMORY
MAIN
MEMORY
MAIN
MEMORY
A
A
A
A
A
A
A
A
A
A
A
A
B
D
B
D
B
D
C
C
C
C
C
C
C
C
C
C
C
C
D
D
D
LOAD
PROCESS
C
E
SWAP
OUT
B
F
LOAD
PROCESS
D
FIGURE
ASSIGNMENT
OF
PROCESS
TO
FREE
FRAMES
ADDRESS
PAGE
NUMBER
OFFSET
THE
PROCESSOR
USES
THE
PAGE
TABLE
TO
PRODUCE
A
PHYSI
CAL
ADDRESS
FRAME
NUMBER
OFFSET
CONTINUING
OUR
EXAMPLE
THE
FIVE
PAGES
OF
PROCESS
D
ARE
LOADED
INTO
FRAMES
AND
FIGURE
SHOWS
THE
VARIOUS
PAGE
TABLES
AT
THIS
TIME
A
PAGE
TABLE
CONTAINS
ONE
ENTRY
FOR
EACH
PAGE
OF
THE
PROCESS
SO
THAT
THE
TABLE
IS
EASILY
INDEXED
BY
THE
PAGE
NUMBER
STARTING
AT
PAGE
EACH
PAGE
TABLE
ENTRY
CONTAINS
THE
NUMBER
OF
THE
FRAME
IN
MAIN
MEMORY
IF
ANY
THAT
HOLDS
THE
CORRESPONDING
PAGE
IN
ADDITION
THE
OS
MAINTAINS
A
SINGLE
FREE
FRAME
LIST
OF
ALL
THE
FRAMES
IN
MAIN
MEMORY
THAT
ARE
CURRENTLY
UNOCCUPIED
AND
AVAILABLE
FOR
PAGES
THUS
WE
SEE
THAT
SIMPLE
PAGING
AS
DESCRIBED
HERE
IS
SIMILAR
TO
FIXED
PARTI
TIONING
THE
DIFFERENCES
ARE
THAT
WITH
PAGING
THE
PARTITIONS
ARE
RATHER
SMALL
A
PAGING
PROCESS
A
PROCESS
B
PAGE
TABLE
PROCESS
C
FREE
FRAME
LIST
PAGE
TABLE
PAGE
TABLE
PROCESS
D
PAGE
TABLE
FIGURE
DATA
STRUCTURES
FOR
THE
EXAMPLE
OF
FIGURE
AT
TIME
EPOCH
F
PROGRAM
MAY
OCCUPY
MORE
THAN
ONE
PARTITION
AND
THESE
PARTITIONS
NEED
NOT
BE
CONTIGUOUS
TO
MAKE
THIS
PAGING
SCHEME
CONVENIENT
LET
US
DICTATE
THAT
THE
PAGE
SIZE
HENCE
THE
FRAME
SIZE
MUST
BE
A
POWER
OF
WITH
THE
USE
OF
A
PAGE
SIZE
THAT
IS
A
POWER
OF
IT
IS
EASY
TO
DEMONSTRATE
THAT
THE
RELATIVE
ADDRESS
WHICH
IS
DEFINED
WITH
REFERENCE
TO
THE
ORIGIN
OF
THE
PROGRAM
AND
THE
LOGICAL
ADDRESS
EXPRESSED
AS
A
PAGE
NUMBER
AND
OFFSET
ARE
THE
SAME
AN
EXAMPLE
IS
SHOWN
IN
FIGURE
IN
THIS
EXAM
PLE
BIT
ADDRESSES
ARE
USED
AND
THE
PAGE
SIZE
IS
BYTES
THE
RELATIVE
ADDRESS
IN
BINARY
FORM
IS
WITH
A
PAGE
SIZE
OF
AN
OFFSET
FIELD
OF
BITS
IS
NEEDED
LEAVING
BITS
FOR
THE
PAGE
NUMBER
THUS
A
PROGRAM
CAN
CONSIST
OF
A
MAXIMUM
OF
PAGES
OF
BYTES
EACH
AS
FIGURE
SHOWS
REL
ATIVE
ADDRESS
CORRESPONDS
TO
AN
OFFSET
OF
ON
PAGE
WHICH
YIELDS
THE
SAME
BIT
NUMBER
THE
CONSEQUENCES
OF
USING
A
PAGE
SIZE
THAT
IS
A
POWER
OF
ARE
TWOFOLD
FIRST
THE
LOGICAL
ADDRESSING
SCHEME
IS
TRANSPARENT
TO
THE
PROGRAMMER
THE
ASSEMBLER
AND
RELATIVE
ADDRESS
A
PARTITIONING
FIGURE
LOGICAL
ADDRESSES
LOGICAL
ADDRESS
PAGE
OFFSET
B
PAGING
PAGE
SIZE
LOGICAL
ADDRESS
SEGMENT
OFFSET
C
SEGMENTATION
CHAPTER
MEMORY
MANAGEMENT
THE
LINKER
EACH
LOGICAL
ADDRESS
PAGE
NUMBER
OFFSET
OF
A
PROGRAM
IS
IDENTICAL
TO
ITS
RELATIVE
ADDRESS
SECOND
IT
IS
A
RELATIVELY
EASY
MATTER
TO
IMPLEMENT
A
FUNCTION
IN
HARDWARE
TO
PERFORM
DYNAMIC
ADDRESS
TRANSLATION
AT
RUN
TIME
CONSIDER
AN
ADDRESS
OF
N
M
BITS
WHERE
THE
LEFTMOST
N
BITS
ARE
THE
PAGE
NUMBER
AND
THE
RIGHTMOST
M
BITS
ARE
THE
OFFSET
IN
OUR
EXAMPLE
FIGURE
N
AND
M
THE
FOLLOWING
STEPS
ARE
NEEDED
FOR
ADDRESS
TRANSLATION
EXTRACT
THE
PAGE
NUMBER
AS
THE
LEFTMOST
N
BITS
OF
THE
LOGICAL
ADDRESS
USE
THE
PAGE
NUMBER
AS
AN
INDEX
INTO
THE
PROCESS
PAGE
TABLE
TO
FIND
THE
FRAME
NUMBER
K
THE
STARTING
PHYSICAL
ADDRESS
OF
THE
FRAME
IS
K
AND
THE
PHYSICAL
ADDRESS
OF
THE
REFERENCED
BYTE
IS
THAT
NUMBER
PLUS
THE
OFFSET
THIS
PHYSICAL
ADDRESS
NEED
NOT
BE
CALCULATED
IT
IS
EASILY
CONSTRUCTED
BY
APPENDING
THE
FRAME
NUMBER
TO
THE
OFFSET
IN
OUR
EXAMPLE
WE
HAVE
THE
LOGICAL
ADDRESS
WHICH
IS
PAGE
NUMBER
OFFSET
SUPPOSE
THAT
THIS
PAGE
IS
RESIDING
IN
MAIN
MEMORY
FRAME
BINARY
THEN
THE
PHYSICAL
ADDRESS
IS
FRAME
NUMBER
OFFSET
FIGURE
BIT
LOGICAL
ADDRESS
BIT
PAGE
BIT
OFFSET
BIT
LOGICAL
ADDRESS
A
PAGING
BIT
PHYSICAL
ADDRESS
BIT
SEGMENT
BIT
OFFSET
BIT
PHYSICAL
ADDRESS
B
SEGMENTATION
FIGURE
EXAMPLES
OF
LOGICAL
TO
PHYSICAL
ADDRESS
TRANSLATION
SEGMENTATION
TO
SUMMARIZE
WITH
SIMPLE
PAGING
MAIN
MEMORY
IS
DIVIDED
INTO
MANY
SMALL
EQUAL
SIZE
FRAMES
EACH
PROCESS
IS
DIVIDED
INTO
FRAME
SIZE
PAGES
SMALLER
PROCESSES
REQUIRE
FEWER
PAGES
LARGER
PROCESSES
REQUIRE
MORE
WHEN
A
PROCESS
IS
BROUGHT
IN
ALL
OF
ITS
PAGES
ARE
LOADED
INTO
AVAILABLE
FRAMES
AND
A
PAGE
TABLE
IS
SET
UP
THIS
APPROACH
SOLVES
MANY
OF
THE
PROBLEMS
INHERENT
IN
PARTITIONING
SEGMENTATION
A
USER
PROGRAM
CAN
BE
SUBDIVIDED
USING
SEGMENTATION
IN
WHICH
THE
PROGRAM
AND
ITS
ASSOCIATED
DATA
ARE
DIVIDED
INTO
A
NUMBER
OF
SEGMENTS
IT
IS
NOT
REQUIRED
THAT
ALL
SEG
MENTS
OF
ALL
PROGRAMS
BE
OF
THE
SAME
LENGTH
ALTHOUGH
THERE
IS
A
MAXIMUM
SEGMENT
LENGTH
AS
WITH
PAGING
A
LOGICAL
ADDRESS
USING
SEGMENTATION
CONSISTS
OF
TWO
PARTS
IN
THIS
CASE
A
SEGMENT
NUMBER
AND
AN
OFFSET
BECAUSE
OF
THE
USE
OF
UNEQUAL
SIZE
SEGMENTS
SEGMENTATION
IS
SIMILAR
TO
DYNAMIC
PARTITIONING
IN
THE
ABSENCE
OF
AN
OVERLAY
SCHEME
OR
THE
USE
OF
VIRTUAL
MEMORY
IT
WOULD
BE
REQUIRED
THAT
ALL
OF
A
PROGRAM
S
SEGMENTS
BE
LOADED
INTO
MEM
ORY
FOR
EXECUTION
THE
DIFFERENCE
COMPARED
TO
DYNAMIC
PARTITIONING
IS
THAT
WITH
SEGMENTATION
A
PROGRAM
MAY
OCCUPY
MORE
THAN
ONE
PARTITION
AND
THESE
PARTITIONS
NEED
NOT
BE
CONTIGUOUS
SEGMENTATION
ELIMINATES
INTERNAL
FRAGMENTATION
BUT
LIKE
DYNAMIC
PARTITIONING
IT
SUFFERS
FROM
EXTERNAL
FRAGMENTATION
HOWEVER
BECAUSE
A
PROCESS
IS
BROKEN
UP
INTO
A
NUMBER
OF
SMALLER
PIECES
THE
EXTERNAL
FRAGMENTATION
SHOULD
BE
LESS
WHEREAS
PAGING
IS
INVISIBLE
TO
THE
PROGRAMMER
SEGMENTATION
IS
USUALLY
VISIBLE
AND
IS
PROVIDED
AS
A
CONVENIENCE
FOR
ORGANIZING
PROGRAMS
AND
DATA
TYPICALLY
THE
PROGRAMMER
OR
COMPILER
WILL
ASSIGN
PROGRAMS
AND
DATA
TO
DIFFERENT
SEGMENTS
FOR
PURPOSES
OF
MODULAR
PROGRAMMING
THE
PROGRAM
OR
DATA
MAY
BE
FURTHER
BROKEN
DOWN
INTO
MULTIPLE
SEGMENTS
THE
PRINCIPAL
INCONVENIENCE
OF
THIS
SERVICE
IS
THAT
THE
PROGRAMMER
MUST
BE
AWARE
OF
THE
MAXIMUM
SEGMENT
SIZE
LIMITATION
ANOTHER
CONSEQUENCE
OF
UNEQUAL
SIZE
SEGMENTS
IS
THAT
THERE
IS
NO
SIMPLE
RELA
TIONSHIP
BETWEEN
LOGICAL
ADDRESSES
AND
PHYSICAL
ADDRESSES
ANALOGOUS
TO
PAGING
A
SIMPLE
SEGMENTATION
SCHEME
WOULD
MAKE
USE
OF
A
SEGMENT
TABLE
FOR
EACH
PROCESS
AND
A
LIST
OF
FREE
BLOCKS
OF
MAIN
MEMORY
EACH
SEGMENT
TABLE
ENTRY
WOULD
HAVE
TO
GIVE
THE
STARTING
ADDRESS
IN
MAIN
MEMORY
OF
THE
CORRESPONDING
SEGMENT
THE
ENTRY
SHOULD
ALSO
PROVIDE
THE
LENGTH
OF
THE
SEGMENT
TO
ASSURE
THAT
INVALID
ADDRESSES
ARE
NOT
USED
WHEN
A
PROCESS
ENTERS
THE
RUNNING
STATE
THE
ADDRESS
OF
ITS
SEGMENT
TABLE
IS
LOADED
INTO
A
SPECIAL
REGISTER
USED
BY
THE
MEMORY
MANAGEMENT
HARDWARE
CONSIDER
AN
ADDRESS
OF
N
M
BITS
WHERE
THE
LEFTMOST
N
BITS
ARE
THE
SEGMENT
NUMBER
AND
THE
RIGHTMOST
M
BITS
ARE
THE
OFFSET
IN
OUR
EXAMPLE
FIGURE
N
AND
M
THUS
THE
MAXIMUM
SEGMENT
SIZE
IS
THE
FOLLOWING
STEPS
ARE
NEEDED
FOR
ADDRESS
TRANSLATION
EXTRACT
THE
SEGMENT
NUMBER
AS
THE
LEFTMOST
N
BITS
OF
THE
LOGICAL
ADDRESS
USE
THE
SEGMENT
NUMBER
AS
AN
INDEX
INTO
THE
PROCESS
SEGMENT
TABLE
TO
FIND
THE
STARTING
PHYSICAL
ADDRESS
OF
THE
SEGMENT
COMPARE
THE
OFFSET
EXPRESSED
IN
THE
RIGHTMOST
M
BITS
TO
THE
LENGTH
OF
THE
SEG
MENT
IF
THE
OFFSET
IS
GREATER
THAN
OR
EQUAL
TO
THE
LENGTH
THE
ADDRESS
IS
INVALID
CHAPTER
MEMORY
MANAGEMENT
THE
DESIRED
PHYSICAL
ADDRESS
IS
THE
SUM
OF
THE
STARTING
PHYSICAL
ADDRESS
OF
THE
SEGMENT
PLUS
THE
OFFSET
IN
OUR
EXAMPLE
WE
HAVE
THE
LOGICAL
ADDRESS
WHICH
IS
SEGMENT
NUMBER
OFFSET
SUPPOSE
THAT
THIS
SEGMENT
IS
RESIDING
IN
MAIN
MEM
ORY
STARTING
AT
PHYSICAL
ADDRESS
THEN
THE
PHYSICAL
ADDRESS
IS
FIGURE
TO
SUMMARIZE
WITH
SIMPLE
SEGMENTATION
A
PROCESS
IS
DIVIDED
INTO
A
NUMBER
OF
SEGMENTS
THAT
NEED
NOT
BE
OF
EQUAL
SIZE
WHEN
A
PROCESS
IS
BROUGHT
IN
ALL
OF
ITS
SEGMENTS
ARE
LOADED
INTO
AVAILABLE
REGIONS
OF
MEMORY
AND
A
SEGMENT
TABLE
IS
SET
UP
SECURITY
ISSUES
MAIN
MEMORY
AND
VIRTUAL
MEMORY
ARE
SYSTEM
RESOURCES
SUBJECT
TO
SECURITY
THREATS
AND
FOR
WHICH
SECURITY
COUNTERMEASURES
NEED
TO
BE
TAKEN
THE
MOST
OBVIOUS
SECU
RITY
REQUIREMENT
IS
THE
PREVENTION
OF
UNAUTHORIZED
ACCESS
TO
THE
MEMORY
CONTENTS
OF
PROCESSES
IF
A
PROCESS
HAS
NOT
DECLARED
A
PORTION
OF
ITS
MEMORY
TO
BE
SHARABLE
THEN
NO
OTHER
PROCESS
SHOULD
HAVE
ACCESS
TO
THE
CONTENTS
OF
THAT
PORTION
OF
MEMORY
IF
A
PROCESS
DECLARES
THAT
A
PORTION
OF
MEMORY
MAY
BE
SHARED
BY
OTHER
DESIGNATED
PROCESSES
THEN
THE
SECURITY
SERVICE
OF
THE
OS
MUST
ENSURE
THAT
ONLY
THE
DESIGNATED
PROCESSES
HAVE
ACCESS
THE
SECURITY
THREATS
AND
COUNTERMEASURES
DISCUSSED
IN
CHAPTER
ARE
RELEVANT
TO
THIS
TYPE
OF
MEMORY
PROTECTION
IN
THIS
SECTION
WE
SUMMARIZE
ANOTHER
THREAT
THAT
INVOLVES
MEMORY
PROTECTION
PART
SEVEN
PROVIDES
MORE
DETAIL
BUFFER
OVERFLOW
ATTACKS
ONE
SERIOUS
SECURITY
THREAT
RELATED
TO
MEMORY
MANAGEMENT
REMAINS
TO
BE
INTRO
DUCED
BUFFER
OVERFLOW
ALSO
KNOWN
AS
A
BUFFER
OVERRUN
WHICH
IS
DEFINED
IN
THE
NIST
NATIONAL
INSTITUTE
OF
STANDARDS
AND
TECHNOLOGY
GLOSSARY
OF
KEY
INFORMATION
SECURITY
TERMS
AS
FOLLOWS
A
BUFFER
OVERFLOW
CAN
OCCUR
AS
A
RESULT
OF
A
PROGRAMMING
ERROR
WHEN
A
PROCESS
ATTEMPTS
TO
STORE
DATA
BEYOND
THE
LIMITS
OF
A
FIXED
SIZED
BUFFER
AND
CONSEQUENTLY
OVERWRITES
ADJACENT
MEMORY
LOCATIONS
THESE
LOCATIONS
COULD
HOLD
OTHER
PROGRAM
VARIABLES
OR
PARAMETERS
OR
PROGRAM
CONTROL
FLOW
DATA
SUCH
AS
RETURN
ADDRESSES
AND
POINTERS
TO
PREVIOUS
STACK
FRAMES
THE
BUFFER
COULD
BE
LOCATED
ON
THE
STACK
IN
THE
HEAP
OR
IN
THE
DATA
SECTION
OF
THE
PROCESS
THE
CONSEQUENCES
OF
THIS
ERROR
INCLUDE
CORRUPTION
OF
DATA
USED
BY
THE
PROGRAM
UNEXPECTED
TRANSFER
OF
CONTROL
IN
THE
PROGRAM
POSSIBLY
MEMORY
ACCESS
VIOLATIONS
AND
VERY
LIKELY
EVENTUAL
PROGRAM
SECURITY
ISSUES
TERMINATION
WHEN
DONE
DELIBERATELY
AS
PART
OF
AN
ATTACK
ON
A
SYSTEM
THE
TRANSFER
OF
CONTROL
COULD
BE
TO
CODE
OF
THE
ATTACKER
S
CHOOSING
RESULTING
IN
THE
ABILITY
TO
EXECUTE
ARBITRARY
CODE
WITH
THE
PRIVILEGES
OF
THE
ATTACKED
PROCESS
BUFFER
OVERFLOW
ATTACKS
ARE
ONE
OF
THE
MOST
PREVALENT
AND
DANGEROUS
TYPES
OF
SECURITY
ATTACKS
TO
ILLUSTRATE
THE
BASIC
OPERATION
OF
A
COMMON
TYPE
OF
BUFFER
OVERFLOW
KNOWN
AS
STACK
OVERFLOW
CONSIDER
THE
C
MAIN
FUNCTION
GIVEN
IN
FIGURE
THIS
CONTAINS
THREE
VARIABLES
VALID
AND
WHOSE
VALUES
WILL
TYPICALLY
BE
SAVED
IN
ADJACENT
MEMORY
LOCATIONS
THEIR
ORDER
AND
LOCATION
DEPENDS
ON
THE
TYPE
OF
VARIABLE
LOCAL
OR
GLOBAL
THE
LANGUAGE
AND
COMPILER
USED
AND
THE
TARGET
MACHINE
ARCHITECTURE
FOR
THIS
EXAMPLE
WE
ASSUME
THAT
THEY
ARE
SAVED
IN
CONSECU
TIVE
MEMORY
LOCATIONS
FROM
HIGHEST
TO
LOWEST
AS
SHOWN
IN
FIGURE
THIS
IS
TYPICALLY
THE
CASE
FOR
LOCAL
VARIABLES
IN
A
C
FUNCTION
ON
COMMON
PROCESSOR
ARCHI
TECTURES
SUCH
AS
THE
INTEL
PENTIUM
FAMILY
THE
PURPOSE
OF
THE
CODE
FRAGMENT
IS
TO
CALL
THE
FUNCTION
TO
COPY
INTO
SOME
EXPECTED
TAG
VALUE
INT
MAIN
INT
ARGC
CHAR
ARGV
INT
VALID
FALSE
CHAR
CHAR
GETS
IF
STRNCMP
0
VALID
TRUE
PRINTF
S
S
VALID
D
N
VALID
A
BASIC
BUFFER
OVERFLOW
C
CODE
CC
G
O
C
START
START
START
VALID
EVILINPUTVALUE
TVALUE
EVILINPUTVALUE
VALID
0
BADINPUTBADINPUT
BADINPUT
BADINPUTBADINPUT
VALID
B
BASIC
BUFFER
OVERFLOW
EXAMPLE
RUNS
FIGURE
BASIC
BUFFER
OVERFLOW
EXAMPLE
THIS
EXAMPLE
THE
FLAG
VARIABLE
IS
SAVED
AS
AN
INTEGER
RATHER
THAN
A
BOOLEAN
THIS
IS
DONE
BOTH
BECAUSE
IT
IS
THE
CLASSIC
C
STYLE
AND
TO
AVOID
ISSUES
OF
WORD
ALIGNMENT
IN
ITS
STORAGE
THE
BUFFERS
ARE
DELIBERATELY
SMALL
TO
ACCENTUATE
THE
BUFFER
OVERFLOW
ISSUE
BEING
ILLUSTRATED
AND
DATA
VALUES
ARE
SPECIFIED
IN
HEXADECIMAL
IN
THIS
AND
RELATED
FIGURES
DATA
VALUES
ARE
ALSO
SHOWN
IN
ASCII
WHERE
APPROPRIATE
CHAPTER
MEMORY
MANAGEMENT
MEMORY
ADDRESS
BEFORE
GETS
AFTER
GETS
CONTAINS
VALUE
OF
BFFFFBEC
BFFFFBDC
FIGURE
BASIC
BUFFER
OVERFLOW
STACK
VALUES
ARGV
ARGC
RETURN
ADDR
OLD
BASE
PTR
VALID
0
0
LET
S
ASSUME
THIS
WILL
BE
THE
STRING
START
IT
THEN
READS
THE
NEXT
LINE
FROM
THE
STANDARD
INPUT
FOR
THE
PROGRAM
USING
THE
C
LIBRARY
GETS
FUNCTION
AND
THEN
COMPARES
THE
STRING
READ
WITH
THE
EXPECTED
TAG
IF
THE
NEXT
LINE
DID
INDEED
CONTAIN
JUST
THE
STRING
START
THIS
COMPARISON
WOULD
SUCCEED
AND
THE
VARIABLE
VALID
WOULD
BE
SET
TO
TRUE
THIS
CASE
IS
SHOWN
IN
THE
FIRST
OF
THE
THREE
EXAMPLE
PROGRAM
RUNS
IN
FIGURE
ANY
OTHER
INPUT
TAG
WOULD
LEAVE
IT
WITH
THE
VALUE
FALSE
SUCH
A
CODE
FRAGMENT
MIGHT
BE
USED
TO
PARSE
SOME
STRUCTURED
NETWORK
PROTOCOL
INTERAC
TION
OR
FORMATTED
TEXT
FILE
THE
PROBLEM
WITH
THIS
CODE
EXISTS
BECAUSE
THE
TRADITIONAL
C
LIBRARY
GETS
FUNCTION
DOES
NOT
INCLUDE
ANY
CHECKING
ON
THE
AMOUNT
OF
DATA
COPIED
IT
READS
THE
NEXT
LINE
OF
TEXT
FROM
THE
PROGRAM
S
STANDARD
INPUT
UP
UNTIL
THE
FIRST
CHAR
ACTER
OCCURS
AND
COPIES
IT
INTO
THE
SUPPLIED
BUFFER
FOLLOWED
BY
THE
NULL
TERMINATOR
C
THE
LOGICAL
VALUES
FALSE
AND
TRUE
ARE
SIMPLY
INTEGERS
WITH
THE
VALUES
0
AND
OR
INDEED
ANY
NONZERO
VALUE
RESPECTIVELY
SYMBOLIC
DEFINES
ARE
OFTEN
USED
TO
MAP
THESE
SYMBOLIC
NAMES
TO
THEIR
UNDERLYING
VALUE
AS
WAS
DONE
IN
THIS
PROGRAM
NEWLINE
NL
OR
LINEFEED
LF
CHARACTER
IS
THE
STANDARD
END
OF
LINE
TERMINATOR
FOR
UNIX
SYSTEMS
AND
HENCE
FOR
C
AND
IS
THE
CHARACTER
WITH
THE
ASCII
VALUE
SECURITY
ISSUES
USED
WITH
C
STRINGS
IF
MORE
THAN
SEVEN
CHARACTERS
ARE
PRESENT
ON
THE
INPUT
LINE
WHEN
READ
IN
THEY
WILL
ALONG
WITH
THE
TERMINATING
NULL
CHARACTER
REQUIRE
MORE
ROOM
THAN
IS
AVAILABLE
IN
THE
BUFFER
CONSEQUENTLY
THE
EXTRA
CHARACTERS
WILL
OVERWRITE
THE
VALUES
OF
THE
ADJACENT
VARIABLE
IN
THIS
CASE
FOR
EXAMPLE
IF
THE
INPUT
LINE
CONTAINED
EVILINPUTVALUE
THE
RESULT
WILL
BE
THAT
WILL
BE
OVER
WRITTEN
WITH
THE
CHARACTERS
TVALUE
AND
WILL
USE
NOT
ONLY
THE
EIGHT
CHARACTERS
ALLOCATED
TO
IT
BUT
SEVEN
MORE
FROM
AS
WELL
THIS
CAN
BE
SEEN
IN
THE
SECOND
EXAMPLE
RUN
IN
FIGURE
THE
OVERFLOW
HAS
RESULTED
IN
CORRUPTION
OF
A
VARIABLE
NOT
DIRECTLY
USED
TO
SAVE
THE
INPUT
BECAUSE
THESE
STRINGS
ARE
NOT
EQUAL
VALID
ALSO
RETAINS
THE
VALUE
FALSE
FURTHER
IF
OR
MORE
CHARACTERS
WERE
INPUT
ADDITIONAL
MEMORY
LOCATIONS
WOULD
BE
OVERWRITTEN
THE
PRECEDING
EXAMPLE
ILLUSTRATES
THE
BASIC
BEHAVIOR
OF
A
BUFFER
OVERFLOW
AT
ITS
SIMPLEST
ANY
UNCHECKED
COPYING
OF
DATA
INTO
A
BUFFER
COULD
RESULT
IN
CORRUPTION
OF
ADJACENT
MEMORY
LOCATIONS
WHICH
MAY
BE
OTHER
VARIABLES
OR
POSSIBLY
PROGRAM
CONTROL
ADDRESSES
AND
DATA
EVEN
THIS
SIMPLE
EXAMPLE
COULD
BE
TAKEN
FURTHER
KNOWING
THE
STRUCTURE
OF
THE
CODE
PROCESSING
IT
AN
ATTACKER
COULD
ARRANGE
FOR
THE
OVERWRITTEN
VALUE
TO
SET
THE
VALUE
IN
EQUAL
TO
THE
VALUE
PLACED
IN
RESULT
ING
IN
THE
SUBSEQUENT
COMPARISON
SUCCEEDING
FOR
EXAMPLE
THE
INPUT
LINE
COULD
BE
THE
STRING
BADINPUTBADINPUT
THIS
RESULTS
IN
THE
COMPARISON
SUCCEEDING
AS
SHOWN
IN
THE
THIRD
OF
THE
THREE
EXAMPLE
PROGRAM
RUNS
IN
FIGURE
AND
ILLUS
TRATED
IN
FIGURE
WITH
THE
VALUES
OF
THE
LOCAL
VARIABLES
BEFORE
AND
AFTER
THE
CALL
TO
GETS
NOTE
ALSO
THAT
THE
TERMINATING
NULL
FOR
THE
INPUT
STRING
WAS
WRITTEN
TO
THE
MEMORY
LOCATION
FOLLOWING
THIS
MEANS
THE
FLOW
OF
CONTROL
IN
THE
PROGRAM
WILL
CONTINUE
AS
IF
THE
EXPECTED
TAG
WAS
FOUND
WHEN
IN
FACT
THE
TAG
READ
WAS
SOME
THING
COMPLETELY
DIFFERENT
THIS
WILL
ALMOST
CERTAINLY
RESULT
IN
PROGRAM
BEHAVIOR
THAT
WAS
NOT
INTENDED
HOW
SERIOUS
THIS
IS
DEPENDS
VERY
MUCH
ON
THE
LOGIC
IN
THE
ATTACKED
PROGRAM
ONE
DANGEROUS
POSSIBILITY
OCCURS
IF
INSTEAD
OF
BEING
A
TAG
THE
VALUES
IN
THESE
BUFFERS
WERE
AN
EXPECTED
AND
SUPPLIED
PASSWORD
NEEDED
TO
ACCESS
PRIVILEGED
FEATURES
IF
SO
THE
BUFFER
OVERFLOW
PROVIDES
THE
ATTACKER
WITH
A
MEANS
OF
ACCESSING
THESE
FEATURES
WITHOUT
ACTUALLY
KNOWING
THE
CORRECT
PASSWORD
TO
EXPLOIT
ANY
TYPE
OF
BUFFER
OVERFLOW
SUCH
AS
THOSE
WE
HAVE
ILLUSTRATED
HERE
THE
ATTACKER
NEEDS
TO
IDENTIFY
A
BUFFER
OVERFLOW
VULNERABILITY
IN
SOME
PROGRAM
THAT
CAN
BE
TRIG
GERED
USING
EXTERNALLY
SOURCED
DATA
UNDER
THE
ATTACKERS
CONTROL
AND
TO
UNDERSTAND
HOW
THAT
BUFFER
WILL
BE
STORED
IN
THE
PROCESSES
MEMORY
AND
HENCE
THE
POTENTIAL
FOR
CORRUPTING
ADJACENT
MEMORY
LOCATIONS
AND
POTENTIALLY
ALTERING
THE
FLOW
OF
EXECUTION
OF
THE
PROGRAM
IDENTIFYING
VULNERABLE
PROGRAMS
MAY
BE
DONE
BY
INSPECTION
OF
PROGRAM
SOURCE
TRACING
THE
EXECUTION
OF
PROGRAMS
AS
THEY
PROCESS
OVERSIZED
INPUT
OR
USING
TOOLS
SUCH
AS
FUZZING
WHICH
WE
DISCUSS
IN
PART
SEVEN
TO
AUTOMATICALLY
IDENTIFY
POTENTIALLY
IN
C
ARE
STORED
IN
AN
ARRAY
OF
CHARACTERS
AND
TERMINATED
WITH
THE
NULL
CHARACTER
WHICH
HAS
THE
ASCII
VALUE
ANY
REMAINING
LOCATIONS
IN
THE
ARRAY
ARE
UNDEFINED
AND
TYPICALLY
CONTAIN
WHATEVER
VALUE
WAS
PREVIOUSLY
SAVED
IN
THAT
AREA
OF
MEMORY
THIS
CAN
BE
CLEARLY
SEEN
IN
THE
VALUE
IN
THE
VARIABLE
IN
THE
BEFORE
COLUMN
OF
FIGURE
CHAPTER
MEMORY
MANAGEMENT
VULNERABLE
PROGRAMS
WHAT
THE
ATTACKER
DOES
WITH
THE
RESULTING
CORRUPTION
OF
MEMORY
VARIES
CONSIDERABLY
DEPENDING
ON
WHAT
VALUES
ARE
BEING
OVERWRITTEN
DEFENDING
AGAINST
BUFFER
OVERFLOWS
FINDING
AND
EXPLOITING
A
STACK
BUFFER
OVERFLOW
IS
NOT
THAT
DIFFICULT
THE
LARGE
NUM
BER
OF
EXPLOITS
OVER
THE
PREVIOUS
COUPLE
OF
DECADES
CLEARLY
ILLUSTRATES
THIS
THERE
IS
CONSEQUENTLY
A
NEED
TO
DEFEND
SYSTEMS
AGAINST
SUCH
ATTACKS
BY
EITHER
PREVENT
ING
THEM
OR
AT
LEAST
DETECTING
AND
ABORTING
SUCH
ATTACKS
COUNTERMEASURES
CAN
BE
BROADLY
CLASSIFIED
INTO
TWO
CATEGORIES
COMPILE
TIME
DEFENSES
WHICH
AIM
TO
HARDEN
PROGRAMS
TO
RESIST
ATTACKS
IN
NEW
PROGRAMS
RUN
TIME
DEFENSES
WHICH
AIM
TO
DETECT
AND
ABORT
ATTACKS
IN
EXISTING
PROGRAMS
WHILE
SUITABLE
DEFENSES
HAVE
BEEN
KNOWN
FOR
A
COUPLE
OF
DECADES
THE
VERY
LARGE
EXISTING
BASE
OF
VULNERABLE
SOFTWARE
AND
SYSTEMS
HINDERS
THEIR
DEPLOYMENT
HENCE
THE
INTEREST
IN
RUN
TIME
DEFENSES
WHICH
CAN
BE
DEPLOYED
IN
OPERATING
SYSTEMS
AND
UPDATES
AND
CAN
PROVIDE
SOME
PROTECTION
FOR
EXISTING
VULNERABLE
PROGRAMS
SUMMARY
ONE
OF
THE
MOST
IMPORTANT
AND
COMPLEX
TASKS
OF
AN
OPERATING
SYSTEM
IS
MEMORY
MANAGEMENT
MEMORY
MANAGEMENT
INVOLVES
TREATING
MAIN
MEMORY
AS
A
RESOURCE
TO
BE
ALLOCATED
TO
AND
SHARED
AMONG
A
NUMBER
OF
ACTIVE
PROCESSES
TO
USE
THE
PRO
CESSOR
AND
THE
I
O
FACILITIES
EFFICIENTLY
IT
IS
DESIRABLE
TO
MAINTAIN
AS
MANY
PROCESSES
IN
MAIN
MEMORY
AS
POSSIBLE
IN
ADDITION
IT
IS
DESIRABLE
TO
FREE
PROGRAMMERS
FROM
SIZE
RESTRICTIONS
IN
PROGRAM
DEVELOPMENT
THE
BASIC
TOOLS
OF
MEMORY
MANAGEMENT
ARE
PAGING
AND
SEGMENTATION
WITH
PAGING
EACH
PROCESS
IS
DIVIDED
INTO
RELATIVELY
SMALL
FIXED
SIZE
PAGES
SEGMENTATION
PROVIDES
FOR
THE
USE
OF
PIECES
OF
VARYING
SIZE
IT
IS
ALSO
POSSIBLE
TO
COMBINE
SEGMEN
TATION
AND
PAGING
IN
A
SINGLE
MEMORY
MANAGEMENT
SCHEME
RECOMMENDED
READING
BECAUSE
PARTITIONING
HAS
BEEN
SUPPLANTED
BY
VIRTUAL
MEMORY
TECHNIQUES
MOST
OS
BOOKS
OFFER
ONLY
CURSORY
COVERAGE
ONE
OF
THE
MORE
COMPLETE
AND
INTERESTING
TREAT
MENTS
IS
IN
A
THOROUGH
DISCUSSION
OF
PARTITIONING
STRATEGIES
IS
FOUND
IN
THE
TOPICS
OF
LINKING
AND
LOADING
ARE
COVERED
IN
MANY
BOOKS
ON
PROGRAM
DEVELOPMENT
COMPUTER
ARCHITECTURE
AND
OPERATING
SYSTEMS
A
PARTICULARLY
DETAILED
TREATMENT
IS
ALSO
CONTAINS
A
GOOD
DISCUSSION
A
THOROUGH
PRACTICAL
DISCUSSION
OF
THIS
TOPIC
WITH
NUMEROUS
OS
EXAMPLES
IS
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
KEY
TERMS
ABSOLUTE
LOADING
LINKAGE
EDITOR
PHYSICAL
ADDRESS
BUDDY
SYSTEM
LINKING
PHYSICAL
ORGANIZATION
COMPACTION
LOADING
PROTECTION
DYNAMIC
LINKING
LOGICAL
ADDRESS
RELATIVE
ADDRESS
DYNAMIC
PARTITIONING
LOGICAL
ORGANIZATION
RELOCATABLE
LOADING
DYNAMIC
RUN
TIME
LOADING
MEMORY
MANAGEMENT
RELOCATION
EXTERNAL
FRAGMENTATION
PAGE
SEGMENT
FIXED
PARTITIONING
PAGE
TABLE
SEGMENTATION
FRAME
PAGING
SHARING
INTERNAL
FRAGMENTATION
PARTITIONING
REVIEW
QUESTIONS
WHAT
REQUIREMENTS
IS
MEMORY
MANAGEMENT
INTENDED
TO
SATISFY
WHY
IS
THE
CAPABILITY
TO
RELOCATE
PROCESSES
DESIRABLE
WHY
IS
IT
NOT
POSSIBLE
TO
ENFORCE
MEMORY
PROTECTION
AT
COMPILE
TIME
WHAT
ARE
SOME
REASONS
TO
ALLOW
TWO
OR
MORE
PROCESSES
TO
ALL
HAVE
ACCESS
TO
A
PARTICU
LAR
REGION
OF
MEMORY
5
IN
A
FIXED
PARTITIONING
SCHEME
WHAT
ARE
THE
ADVANTAGES
OF
USING
UNEQUAL
SIZE
PARTITIONS
WHAT
IS
THE
DIFFERENCE
BETWEEN
INTERNAL
AND
EXTERNAL
FRAGMENTATION
WHAT
ARE
THE
DISTINCTIONS
AMONG
LOGICAL
RELATIVE
AND
PHYSICAL
ADDRESSES
WHAT
IS
THE
DIFFERENCE
BETWEEN
A
PAGE
AND
A
FRAME
9
WHAT
IS
THE
DIFFERENCE
BETWEEN
A
PAGE
AND
A
SEGMENT
PROBLEMS
IN
SECTION
WE
LISTED
FIVE
OBJECTIVES
OF
MEMORY
MANAGEMENT
AND
IN
SECTION
WE
LISTED
FIVE
REQUIREMENTS
ARGUE
THAT
EACH
LIST
ENCOMPASSES
ALL
OF
THE
CONCERNS
AD
DRESSED
IN
THE
OTHER
CHAPTER
MEMORY
MANAGEMENT
CONSIDER
A
FIXED
PARTITIONING
SCHEME
WITH
EQUAL
SIZE
PARTITIONS
OF
BYTES
AND
A
TOTAL
MAIN
MEMORY
SIZE
OF
BYTES
A
PROCESS
TABLE
IS
MAINTAINED
THAT
INCLUDES
A
POINTER
TO
A
PARTITION
FOR
EACH
RESIDENT
PROCESS
HOW
MANY
BITS
ARE
REQUIRED
FOR
THE
POINTER
3
CONSIDER
A
DYNAMIC
PARTITIONING
SCHEME
SHOW
THAT
ON
AVERAGE
THE
MEMORY
CONTAINS
HALF
AS
MANY
HOLES
AS
SEGMENTS
4
TO
IMPLEMENT
THE
VARIOUS
PLACEMENT
ALGORITHMS
DISCUSSED
FOR
DYNAMIC
PARTITIONING
SECTION
A
LIST
OF
THE
FREE
BLOCKS
OF
MEMORY
MUST
BE
KEPT
FOR
EACH
OF
THE
THREE
METHODS
DISCUSSED
BEST
FIT
FIRST
FIT
NEXT
FIT
WHAT
IS
THE
AVERAGE
LENGTH
OF
THE
SEARCH
5
ANOTHER
PLACEMENT
ALGORITHM
FOR
DYNAMIC
PARTITIONING
IS
REFERRED
TO
AS
WORST
FIT
IN
THIS
CASE
THE
LARGEST
FREE
BLOCK
OF
MEMORY
IS
USED
FOR
BRINGING
IN
A
PROCESS
A
DISCUSS
THE
PROS
AND
CONS
OF
THIS
METHOD
COMPARED
TO
FIRST
NEXT
AND
BEST
FIT
B
WHAT
IS
THE
AVERAGE
LENGTH
OF
THE
SEARCH
FOR
WORST
FIT
THIS
DIAGRAM
SHOWS
AN
EXAMPLE
OF
MEMORY
CONFIGURATION
UNDER
DYNAMIC
PARTITION
ING
AFTER
A
NUMBER
OF
PLACEMENT
AND
SWAPPING
OUT
OPERATIONS
HAVE
BEEN
CARRIED
OUT
ADDRESSES
GO
FROM
LEFT
TO
RIGHT
GRAY
AREAS
INDICATE
BLOCKS
OCCUPIED
BY
PROCESSES
WHITE
AREAS
INDICATE
FREE
MEMORY
BLOCKS
THE
LAST
PROCESS
PLACED
IS
MBYTE
AND
IS
MARKED
WITH
AN
X
ONLY
ONE
PROCESS
WAS
SWAPPED
OUT
AFTER
THAT
M
X
2M
A
WHAT
WAS
THE
MAXIMUM
SIZE
OF
THE
SWAPPED
OUT
PROCESS
B
WHAT
WAS
THE
SIZE
OF
THE
FREE
BLOCK
JUST
BEFORE
IT
WAS
PARTITIONED
BY
X
C
A
NEW
3
MBYTE
ALLOCATION
REQUEST
MUST
BE
SATISFIED
NEXT
INDICATE
THE
INTERVALS
OF
MEMORY
WHERE
A
PARTITION
WILL
BE
CREATED
FOR
THE
NEW
PROCESS
UNDER
THE
FOLLOWING
FOUR
PLACEMENT
ALGORITHMS
BEST
FIT
FIRST
FIT
NEXT
FIT
WORST
FIT
FOR
EACH
ALGORITHM
DRAW
A
HORIZONTAL
SEGMENT
UNDER
THE
MEMORY
STRIP
AND
LABEL
IT
CLEARLY
A
MBYTE
BLOCK
OF
MEMORY
IS
ALLOCATED
USING
THE
BUDDY
SYSTEM
A
SHOW
THE
RESULTS
OF
THE
FOLLOWING
SEQUENCE
IN
A
FIGURE
SIMILAR
TO
FIGURE
REQUEST
REQUEST
REQUEST
RETURN
A
REQUEST
RETURN
B
RETURN
D
RETURN
C
B
SHOW
THE
BINARY
TREE
REPRESENTATION
FOLLOWING
RETURN
B
CONSIDER
A
BUDDY
SYSTEM
IN
WHICH
A
PARTICULAR
BLOCK
UNDER
THE
CURRENT
ALLOCATION
HAS
AN
ADDRESS
OF
A
IF
THE
BLOCK
IS
OF
SIZE
4
WHAT
IS
THE
BINARY
ADDRESS
OF
ITS
BUDDY
B
IF
THE
BLOCK
IS
OF
SIZE
WHAT
IS
THE
BINARY
ADDRESS
OF
ITS
BUDDY
9
LET
BUDDYK
X
ADDRESS
OF
THE
BUDDY
OF
THE
BLOCK
OF
SIZE
WHOSE
ADDRESS
IS
X
WRITE
A
GENERAL
EXPRESSION
FOR
BUDDYK
X
THE
FIBONACCI
SEQUENCE
IS
DEFINED
AS
FOLLOWS
0
FN
FN
FN
N
0
A
COULD
THIS
SEQUENCE
BE
USED
TO
ESTABLISH
A
BUDDY
SYSTEM
B
WHAT
WOULD
BE
THE
ADVANTAGE
OF
THIS
SYSTEM
OVER
THE
BINARY
BUDDY
SYSTEM
DESCRIBED
IN
THIS
CHAPTER
DURING
THE
COURSE
OF
EXECUTION
OF
A
PROGRAM
THE
PROCESSOR
WILL
INCREMENT
THE
CONTENTS
OF
THE
INSTRUCTION
REGISTER
PROGRAM
COUNTER
BY
ONE
WORD
AFTER
EACH
INSTRUCTION
FETCH
BUT
WILL
ALTER
THE
CONTENTS
OF
THAT
REGISTER
IF
IT
ENCOUNTERS
A
BRANCH
OR
CALL
INSTRUCTION
THAT
CAUSES
EXECUTION
TO
CONTINUE
ELSEWHERE
IN
THE
PROGRAM
NOW
CONSIDER
FIGURE
THERE
ARE
TWO
ALTERNATIVES
WITH
RESPECT
TO
INSTRUCTION
ADDRESSES
MAINTAIN
A
RELATIVE
ADDRESS
IN
THE
INSTRUCTION
REGISTER
AND
DO
THE
DYNAMIC
ADDRESS
TRANSLATION
USING
THE
INSTRUCTION
REGISTER
AS
INPUT
WHEN
A
SUCCESSFUL
BRANCH
OR
CALL
IS
ENCOUNTERED
THE
RELATIVE
ADDRESS
GENERATED
BY
THAT
BRANCH
OR
CALL
IS
LOADED
INTO
THE
INSTRUCTION
REGISTER
MAINTAIN
AN
ABSOLUTE
ADDRESS
IN
THE
INSTRUCTION
REGISTER
WHEN
A
SUCCESSFUL
BRANCH
OR
CALL
IS
ENCOUNTERED
DYNAMIC
ADDRESS
TRANSLATION
IS
EMPLOYED
WITH
THE
RESULTS
STORED
IN
THE
INSTRUCTION
REGISTER
WHICH
APPROACH
IS
PREFERABLE
8
KEY
TERMS
REVIEW
QUESTIONS
AND
PROBLEMS
CONSIDER
A
SIMPLE
PAGING
SYSTEM
WITH
THE
FOLLOWING
PARAMETERS
BYTES
OF
PHYSICAL
MEMORY
PAGE
SIZE
OF
BYTES
PAGES
OF
LOGICAL
ADDRESS
SPACE
A
HOW
MANY
BITS
ARE
IN
A
LOGICAL
ADDRESS
B
HOW
MANY
BYTES
IN
A
FRAME
C
HOW
MANY
BITS
IN
THE
PHYSICAL
ADDRESS
SPECIFY
THE
FRAME
D
HOW
MANY
ENTRIES
IN
THE
PAGE
TABLE
E
HOW
MANY
BITS
IN
EACH
PAGE
TABLE
ENTRY
ASSUME
EACH
PAGE
TABLE
ENTRY
CONTAINS
A
VALID
INVALID
BIT
13
WRITE
THE
BINARY
TRANSLATION
OF
THE
LOGICAL
ADDRESS
UNDER
THE
FOLLOWING
HYPOTHETICAL
MEMORY
MANAGEMENT
SCHEMES
AND
EXPLAIN
YOUR
ANSWER
A
A
PAGING
SYSTEM
WITH
A
ADDRESS
PAGE
SIZE
USING
A
PAGE
TABLE
IN
WHICH
THE
FRAME
NUMBER
HAPPENS
TO
BE
FOUR
TIMES
SMALLER
THAN
THE
PAGE
NUMBER
B
A
SEGMENTATION
SYSTEM
WITH
A
ADDRESS
MAXIMUM
SEGMENT
SIZE
USING
A
SEGMENT
TABLE
IN
WHICH
BASES
HAPPEN
TO
BE
REGULARLY
PLACED
AT
REAL
ADDRESSES
4
X
SEGMENT
CONSIDER
A
SIMPLE
SEGMENTATION
SYSTEM
THAT
HAS
THE
FOLLOWING
SEGMENT
TABLE
STARTING
ADDRESS
LENGTH
BYTES
222
FOR
EACH
OF
THE
FOLLOWING
LOGICAL
ADDRESSES
DETERMINE
THE
PHYSICAL
ADDRESS
OR
INDICATE
IF
A
SEGMENT
FAULT
OCCURS
A
0
198
B
2
156
C
530
D
3
444
E
0
222
CONSIDER
A
MEMORY
IN
WHICH
CONTIGUOUS
SEGMENTS
SN
ARE
PLACED
IN
THEIR
ORDER
OF
CREATION
FROM
ONE
END
OF
THE
STORE
TO
THE
OTHER
AS
SUGGESTED
BY
THE
FOLLOWING
FIGURE
SN
HOLE
WHEN
SEGMENT
SN
IS
BEING
CREATED
IT
IS
PLACED
IMMEDIATELY
AFTER
SEGMENT
SN
EVEN
THOUGH
SOME
OF
THE
SEGMENTS
SN
MAY
ALREADY
HAVE
BEEN
DELETED
WHEN
THE
BOUNDARY
BETWEEN
SEGMENTS
IN
USE
OR
DELETED
AND
THE
HOLE
REACHES
THE
OTHER
END
OF
THE
MEMORY
THE
SEGMENTS
IN
USE
ARE
COMPACTED
A
SHOW
THAT
THE
FRACTION
OF
TIME
F
SPENT
ON
COMPACTING
OBEYS
THE
FOLLOWING
INEQUALITY
WHERE
F
Ú
F
KF
WHERE
K
T
1
2S
S
AVERAGE
LENGTH
OF
A
SEGMENT
IN
WORDS
T
AVERAGE
LIFETIME
OF
A
SEGMENT
IN
MEMORY
REFERENCES
F
FRACTION
OF
THE
MEMORY
THAT
IS
UNUSED
UNDER
EQUILIBRIUM
CONDITIONS
HINT
FIND
THE
AVERAGE
SPEED
AT
WHICH
THE
BOUNDARY
CROSSES
THE
MEMORY
AND
ASSUME
THAT
THE
COPYING
OF
A
SINGLE
WORD
REQUIRES
AT
LEAST
TWO
MEMORY
REFERENCES
B
FIND
F
FOR
F
0
2
T
1
000
AND
S
CHAPTER
MEMORY
MANAGEMENT
APPENDIX
LOADING
AND
LINKING
THE
FIRST
STEP
IN
THE
CREATION
OF
AN
ACTIVE
PROCESS
IS
TO
LOAD
A
PROGRAM
INTO
MAIN
MEMORY
AND
CREATE
A
PROCESS
IMAGE
FIGURE
FIGURE
DEPICTS
A
SCENARIO
TYP
ICAL
FOR
MOST
SYSTEMS
THE
APPLICATION
CONSISTS
OF
A
NUMBER
OF
COMPILED
OR
ASSEMBLED
MODULES
IN
OBJECT
CODE
FORM
THESE
ARE
LINKED
TO
RESOLVE
ANY
REFERENCES
BETWEEN
MODULES
AT
THE
SAME
TIME
REFERENCES
TO
LIBRARY
ROUTINES
ARE
RESOLVED
THE
LIBRARY
ROUTINES
THEMSELVES
MAY
BE
INCORPORATED
INTO
THE
PROGRAM
OR
REFERENCED
AS
SHARED
CODE
THAT
MUST
BE
SUPPLIED
BY
THE
OPERATING
SYSTEM
AT
RUN
TIME
IN
THIS
APPENDIX
WE
SUMMARIZE
THE
KEY
FEATURES
OF
LINKERS
AND
LOADERS
FOR
CLARITY
IN
THE
PRESENTATION
WE
BEGIN
WITH
A
DESCRIPTION
OF
THE
LOADING
TASK
WHEN
A
SINGLE
PROGRAM
MODULE
IS
INVOLVED
NO
LINKING
IS
REQUIRED
LOADING
IN
FIGURE
THE
LOADER
PLACES
THE
LOAD
MODULE
IN
MAIN
MEMORY
STARTING
AT
LOCA
TION
X
IN
LOADING
THE
PROGRAM
THE
ADDRESSING
REQUIREMENT
ILLUSTRATED
IN
FIGURE
1
MUST
BE
SATISFIED
IN
GENERAL
THREE
APPROACHES
CAN
BE
TAKEN
ABSOLUTE
LOADING
RELOCATABLE
LOADING
DYNAMIC
RUN
TIME
LOADING
ABSOLUTE
LOADING
AN
ABSOLUTE
LOADER
REQUIRES
THAT
A
GIVEN
LOAD
MODULE
ALWAYS
BE
LOADED
INTO
THE
SAME
LOCATION
IN
MAIN
MEMORY
THUS
IN
THE
LOAD
MODULE
PRESENTED
TO
THE
LOADER
ALL
ADDRESS
REFERENCES
MUST
BE
TO
SPECIFIC
OR
ABSOLUTE
MAIN
PROGRAM
DATA
OBJECT
CODE
FIGURE
THE
LOADING
FUNCTION
PROCESS
IMAGE
IN
MAIN
MEMORY
APPENDIX
LOADING
AND
LINKING
STATIC
LIBRARY
X
MODULE
1
MODULE
2
LOAD
MODULE
DYNAMIC
LIBRARY
MODULE
N
FIGURE
A
LINKING
AND
LOADING
SCENARIO
MAIN
MEMORY
MEMORY
ADDRESSES
FOR
EXAMPLE
IF
X
IN
FIGURE
IS
LOCATION
THEN
THE
FIRST
WORD
IN
A
LOAD
MODULE
DESTINED
FOR
THAT
REGION
OF
MEMORY
HAS
ADDRESS
THE
ASSIGNMENT
OF
SPECIFIC
ADDRESS
VALUES
TO
MEMORY
REFERENCES
WITHIN
A
PROGRAM
CAN
BE
DONE
EITHER
BY
THE
PROGRAMMER
OR
AT
COMPILE
OR
ASSEMBLY
TIME
TABLE
THERE
ARE
SEVERAL
DISADVANTAGES
TO
THE
FORMER
APPROACH
FIRST
EVERY
PROGRAMMER
WOULD
HAVE
TO
KNOW
THE
INTENDED
ASSIGNMENT
STRATEGY
FOR
PLACING
MOD
ULES
INTO
MAIN
MEMORY
SECOND
IF
ANY
MODIFICATIONS
ARE
MADE
TO
THE
PROGRAM
THAT
INVOLVE
INSERTIONS
OR
DELETIONS
IN
THE
BODY
OF
THE
MODULE
THEN
ALL
OF
THE
ADDRESSES
WILL
HAVE
TO
BE
ALTERED
ACCORDINGLY
IT
IS
PREFERABLE
TO
ALLOW
MEMORY
REFERENCES
WITHIN
PROGRAMS
TO
BE
EXPRESSED
SYMBOLICALLY
AND
THEN
RESOLVE
THOSE
SYMBOLIC
REFER
ENCES
AT
THE
TIME
OF
COMPILATION
OR
ASSEMBLY
THIS
IS
ILLUSTRATED
IN
FIGURE
EVERY
REFERENCE
TO
AN
INSTRUCTION
OR
ITEM
OF
DATA
IS
INITIALLY
REPRESENTED
BY
A
SYMBOL
IN
PREPARING
THE
MODULE
FOR
INPUT
TO
AN
ABSOLUTE
LOADER
THE
ASSEMBLER
OR
COMPILER
WILL
CONVERT
ALL
OF
THESE
REFERENCES
TO
SPECIFIC
ADDRESSES
IN
THIS
EXAMPLE
FOR
A
MODULE
TO
BE
LOADED
STARTING
AT
LOCATION
AS
SHOWN
IN
FIGURE
RELOCATABLE
LOADING
THE
DISADVANTAGE
OF
BINDING
MEMORY
REFERENCES
TO
SPECIFIC
ADDRESSES
PRIOR
TO
LOADING
IS
THAT
THE
RESULTING
LOAD
MODULE
CAN
ONLY
BE
PLACED
IN
ONE
REGION
OF
MAIN
MEMORY
HOWEVER
WHEN
MANY
PROGRAMS
SHARE
MAIN
MEMORY
IT
MAY
NOT
BE
DESIRABLE
TO
DECIDE
AHEAD
OF
TIME
INTO
WHICH
REGION
OF
MEMORY
A
PARTICULAR
MODULE
SHOULD
BE
LOADED
IT
IS
BETTER
TO
MAKE
THAT
DECISION
AT
LOAD
TIME
THUS
WE
NEED
A
LOAD
MODULE
THAT
CAN
BE
LOCATED
ANYWHERE
IN
MAIN
MEMORY
TO
SATISFY
THIS
NEW
REQUIREMENT
THE
ASSEMBLER
OR
COMPILER
PRODUCES
NOT
ACTUAL
MAIN
MEMORY
ADDRESSES
ABSOLUTE
ADDRESSES
BUT
ADDRESSES
THAT
ARE
RELATIVE
TO
SOME
KNOWN
POINT
SUCH
AS
THE
START
OF
THE
PROGRAM
THIS
TECHNIQUE
IS
ILLUSTRATED
IN
FIGURE
THE
START
OF
THE
LOAD
MODULE
IS
ASSIGNED
THE
RELATIVE
ADDRESS
0
AND
CHAPTER
MEMORY
MANAGEMENT
TABLE
3
ADDRESS
BINDING
A
LOADER
BINDING
TIME
FUNCTION
PROGRAMMING
TIME
ALL
ACTUAL
PHYSICAL
ADDRESSES
ARE
DIRECTLY
SPECIFIED
BY
THE
PROGRAMMER
IN
THE
PROGRAM
ITSELF
COMPILE
OR
ASSEMBLY
TIME
THE
PROGRAM
CONTAINS
SYMBOLIC
ADDRESS
REFERENCES
AND
THESE
ARE
CONVERTED
TO
ACTUAL
PHYSICAL
ADDRESSES
BY
THE
COMPILER
OR
ASSEMBLER
LOAD
TIME
THE
COMPILER
OR
ASSEMBLER
PRODUCES
RELATIVE
ADDRESSES
THE
LOADER
TRANSLATES
THESE
TO
ABSOLUTE
ADDRESSES
AT
THE
TIME
OF
PROGRAM
LOADING
RUN
TIME
THE
LOADED
PROGRAM
RETAINS
RELATIVE
ADDRESSES
THESE
ARE
CONVERTED
DYNAMICALLY
TO
ABSOLUTE
ADDRESSES
BY
PROCESSOR
HARDWARE
B
LINKER
LINKAGE
TIME
FUNCTION
PROGRAMMING
TIME
NO
EXTERNAL
PROGRAM
OR
DATA
REFERENCES
ARE
ALLOWED
THE
PROGRAMMER
MUST
PLACE
INTO
THE
PROGRAM
THE
SOURCE
CODE
FOR
ALL
SUBPROGRAMS
THAT
ARE
REFERENCED
COMPILE
OR
ASSEMBLY
TIME
THE
ASSEMBLER
MUST
FETCH
THE
SOURCE
CODE
OF
EVERY
SUBROUTINE
THAT
IS
REFERENCED
AND
ASSEMBLE
THEM
AS
A
UNIT
LOAD
MODULE
CREATION
ALL
OBJECT
MODULES
HAVE
BEEN
ASSEMBLED
USING
RELATIVE
ADDRESSES
THESE
MODULES
ARE
LINKED
TOGETHER
AND
ALL
REFERENCES
ARE
RESTATED
RELATIVE
TO
THE
ORIGIN
OF
THE
FINAL
LOAD
MODULE
LOAD
TIME
EXTERNAL
REFERENCES
ARE
NOT
RESOLVED
UNTIL
THE
LOAD
MODULE
IS
TO
BE
LOADED
INTO
MAIN
MEMORY
AT
THAT
TIME
REFERENCED
DYNAMIC
LINK
MODULES
ARE
APPENDED
TO
THE
LOAD
MODULE
AND
THE
ENTIRE
PACKAGE
IS
LOADED
INTO
MAIN
OR
VIRTUAL
MEMORY
RUN
TIME
EXTERNAL
REFERENCES
ARE
NOT
RESOLVED
UNTIL
THE
EXTERNAL
CALL
IS
EXECUTED
BY
THE
PROCESSOR
AT
THAT
TIME
THE
PROCESS
IS
INTERRUPTED
AND
THE
DESIRED
MODULE
IS
LINKED
TO
THE
CALLING
PROGRAM
SYMBOLIC
ADDRESSES
ABSOLUTE
ADDRESSES
RELATIVE
ADDRESSES
MAIN
MEMORY
ADDRESSES
0
X
X
Y
X
X
A
OBJECT
MODULE
B
ABSOLUTE
LOAD
MODULE
C
RELATIVE
LOAD
MODULE
D
RELATIVE
LOAD
MODULE
LOADED
INTO
MAIN
MEMORY
STARTING
AT
LOCATION
X
FIGURE
17
ABSOLUTE
AND
RELOCATABLE
LOAD
MODULES
APPENDIX
LOADING
AND
LINKING
ALL
OTHER
MEMORY
REFERENCES
WITHIN
THE
MODULE
ARE
EXPRESSED
RELATIVE
TO
THE
BEGIN
NING
OF
THE
MODULE
