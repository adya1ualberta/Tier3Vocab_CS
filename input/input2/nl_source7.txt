THE
PROBLEM
OF
SEARCHING
FOR
PATTERNS
IN
DATA
IS
A
FUNDAMENTAL
ONE
AND
HAS
A
LONG
AND
SUCCESSFUL
HISTORY
FOR
INSTANCE
THE
EXTENSIVE
ASTRONOMICAL
OBSERVATIONS
OF
TYCHO
BRAHE
IN
THE
CENTURY
ALLOWED
JOHANNES
KEPLER
TO
DISCOVER
THE
EMPIRICAL
LAWS
OF
PLANETARY
MOTION
WHICH
IN
TURN
PROVIDED
A
SPRINGBOARD
FOR
THE
DEVELOPMENT
OF
CLAS
SICAL
MECHANICS
SIMILARLY
THE
DISCOVERY
OF
REGULARITIES
IN
ATOMIC
SPECTRA
PLAYED
A
KEY
ROLE
IN
THE
DEVELOPMENT
AND
VERIFICATION
OF
QUANTUM
PHYSICS
IN
THE
EARLY
TWENTI
ETH
CENTURY
THE
FIELD
OF
PATTERN
RECOGNITION
IS
CONCERNED
WITH
THE
AUTOMATIC
DISCOV
ERY
OF
REGULARITIES
IN
DATA
THROUGH
THE
USE
OF
COMPUTER
ALGORITHMS
AND
WITH
THE
USE
OF
THESE
REGULARITIES
TO
TAKE
ACTIONS
SUCH
AS
CLASSIFYING
THE
DATA
INTO
DIFFERENT
CATEGORIES
CONSIDER
THE
EXAMPLE
OF
RECOGNIZING
HANDWRITTEN
DIGITS
ILLUSTRATED
IN
FIGURE
EACH
DIGIT
CORRESPONDS
TO
A
PIXEL
IMAGE
AND
SO
CAN
BE
REPRESENTED
BY
A
VECTOR
X
COMPRISING
REAL
NUMBERS
THE
GOAL
IS
TO
BUILD
A
MACHINE
THAT
WILL
TAKE
SUCH
A
VECTOR
X
AS
INPUT
AND
THAT
WILL
PRODUCE
THE
IDENTITY
OF
THE
DIGIT
AS
THE
OUTPUT
THIS
IS
A
NONTRIVIAL
PROBLEM
DUE
TO
THE
WIDE
VARIABILITY
OF
HANDWRITING
IT
COULD
BE
FIGURE
EXAMPLES
OF
HAND
WRITTEN
DIG
ITS
TAKEN
FROM
US
ZIP
CODES
TACKLED
USING
HANDCRAFTED
RULES
OR
HEURISTICS
FOR
DISTINGUISHING
THE
DIGITS
BASED
ON
THE
SHAPES
OF
THE
STROKES
BUT
IN
PRACTICE
SUCH
AN
APPROACH
LEADS
TO
A
PROLIFERATION
OF
RULES
AND
OF
EXCEPTIONS
TO
THE
RULES
AND
SO
ON
AND
INVARIABLY
GIVES
POOR
RESULTS
FAR
BETTER
RESULTS
CAN
BE
OBTAINED
BY
ADOPTING
A
MACHINE
LEARNING
APPROACH
IN
WHICH
A
LARGE
SET
OF
N
DIGITS
XN
CALLED
A
TRAINING
SET
IS
USED
TO
TUNE
THE
PARAMETERS
OF
AN
ADAPTIVE
MODEL
THE
CATEGORIES
OF
THE
DIGITS
IN
THE
TRAINING
SET
ARE
KNOWN
IN
ADVANCE
TYPICALLY
BY
INSPECTING
THEM
INDIVIDUALLY
AND
HAND
LABELLING
THEM
WE
CAN
EXPRESS
THE
CATEGORY
OF
A
DIGIT
USING
TARGET
VECTOR
T
WHICH
REPRESENTS
THE
IDENTITY
OF
THE
CORRESPONDING
DIGIT
SUITABLE
TECHNIQUES
FOR
REPRESENTING
CATE
GORIES
IN
TERMS
OF
VECTORS
WILL
BE
DISCUSSED
LATER
NOTE
THAT
THERE
IS
ONE
SUCH
TARGET
VECTOR
T
FOR
EACH
DIGIT
IMAGE
X
THE
RESULT
OF
RUNNING
THE
MACHINE
LEARNING
ALGORITHM
CAN
BE
EXPRESSED
AS
A
FUNCTION
Y
X
WHICH
TAKES
A
NEW
DIGIT
IMAGE
X
AS
INPUT
AND
THAT
GENERATES
AN
OUTPUT
VECTOR
Y
ENCODED
IN
THE
SAME
WAY
AS
THE
TARGET
VECTORS
THE
PRECISE
FORM
OF
THE
FUNCTION
Y
X
IS
DETERMINED
DURING
THE
TRAINING
PHASE
ALSO
KNOWN
AS
THE
LEARNING
PHASE
ON
THE
BASIS
OF
THE
TRAINING
DATA
ONCE
THE
MODEL
IS
TRAINED
IT
CAN
THEN
DE
TERMINE
THE
IDENTITY
OF
NEW
DIGIT
IMAGES
WHICH
ARE
SAID
TO
COMPRISE
A
TEST
SET
THE
ABILITY
TO
CATEGORIZE
CORRECTLY
NEW
EXAMPLES
THAT
DIFFER
FROM
THOSE
USED
FOR
TRAIN
ING
IS
KNOWN
AS
GENERALIZATION
IN
PRACTICAL
APPLICATIONS
THE
VARIABILITY
OF
THE
INPUT
VECTORS
WILL
BE
SUCH
THAT
THE
TRAINING
DATA
CAN
COMPRISE
ONLY
A
TINY
FRACTION
OF
ALL
POSSIBLE
INPUT
VECTORS
AND
SO
GENERALIZATION
IS
A
CENTRAL
GOAL
IN
PATTERN
RECOGNITION
FOR
MOST
PRACTICAL
APPLICATIONS
THE
ORIGINAL
INPUT
VARIABLES
ARE
TYPICALLY
PREPRO
CESSED
TO
TRANSFORM
THEM
INTO
SOME
NEW
SPACE
OF
VARIABLES
WHERE
IT
IS
HOPED
THE
PATTERN
RECOGNITION
PROBLEM
WILL
BE
EASIER
TO
SOLVE
FOR
INSTANCE
IN
THE
DIGIT
RECOGNI
TION
PROBLEM
THE
IMAGES
OF
THE
DIGITS
ARE
TYPICALLY
TRANSLATED
AND
SCALED
SO
THAT
EACH
DIGIT
IS
CONTAINED
WITHIN
A
BOX
OF
A
FIXED
SIZE
THIS
GREATLY
REDUCES
THE
VARIABILITY
WITHIN
EACH
DIGIT
CLASS
BECAUSE
THE
LOCATION
AND
SCALE
OF
ALL
THE
DIGITS
ARE
NOW
THE
SAME
WHICH
MAKES
IT
MUCH
EASIER
FOR
A
SUBSEQUENT
PATTERN
RECOGNITION
ALGORITHM
TO
DISTINGUISH
BETWEEN
THE
DIFFERENT
CLASSES
THIS
PRE
PROCESSING
STAGE
IS
SOMETIMES
ALSO
CALLED
FEATURE
EXTRACTION
NOTE
THAT
NEW
TEST
DATA
MUST
BE
PRE
PROCESSED
USING
THE
SAME
STEPS
AS
THE
TRAINING
DATA
PRE
PROCESSING
MIGHT
ALSO
BE
PERFORMED
IN
ORDER
TO
SPEED
UP
COMPUTATION
FOR
EXAMPLE
IF
THE
GOAL
IS
REAL
TIME
FACE
DETECTION
IN
A
HIGH
RESOLUTION
VIDEO
STREAM
THE
COMPUTER
MUST
HANDLE
HUGE
NUMBERS
OF
PIXELS
PER
SECOND
AND
PRESENTING
THESE
DIRECTLY
TO
A
COMPLEX
PATTERN
RECOGNITION
ALGORITHM
MAY
BE
COMPUTATIONALLY
INFEASI
BLE
INSTEAD
THE
AIM
IS
TO
FIND
USEFUL
FEATURES
THAT
ARE
FAST
TO
COMPUTE
AND
YET
THAT
INTRODUCTION
ALSO
PRESERVE
USEFUL
DISCRIMINATORY
INFORMATION
ENABLING
FACES
TO
BE
DISTINGUISHED
FROM
NON
FACES
THESE
FEATURES
ARE
THEN
USED
AS
THE
INPUTS
TO
THE
PATTERN
RECOGNITION
ALGORITHM
FOR
INSTANCE
THE
AVERAGE
VALUE
OF
THE
IMAGE
INTENSITY
OVER
A
RECTANGULAR
SUBREGION
CAN
BE
EVALUATED
EXTREMELY
EFFICIENTLY
VIOLA
AND
JONES
AND
A
SET
OF
SUCH
FEATURES
CAN
PROVE
VERY
EFFECTIVE
IN
FAST
FACE
DETECTION
BECAUSE
THE
NUMBER
OF
SUCH
FEATURES
IS
SMALLER
THAN
THE
NUMBER
OF
PIXELS
THIS
KIND
OF
PRE
PROCESSING
REPRE
SENTS
A
FORM
OF
DIMENSIONALITY
REDUCTION
CARE
MUST
BE
TAKEN
DURING
PRE
PROCESSING
BECAUSE
OFTEN
INFORMATION
IS
DISCARDED
AND
IF
THIS
INFORMATION
IS
IMPORTANT
TO
THE
SOLUTION
OF
THE
PROBLEM
THEN
THE
OVERALL
ACCURACY
OF
THE
SYSTEM
CAN
SUFFER
APPLICATIONS
IN
WHICH
THE
TRAINING
DATA
COMPRISES
EXAMPLES
OF
THE
INPUT
VECTORS
ALONG
WITH
THEIR
CORRESPONDING
TARGET
VECTORS
ARE
KNOWN
AS
SUPERVISED
LEARNING
PROB
LEMS
CASES
SUCH
AS
THE
DIGIT
RECOGNITION
EXAMPLE
IN
WHICH
THE
AIM
IS
TO
ASSIGN
EACH
INPUT
VECTOR
TO
ONE
OF
A
FINITE
NUMBER
OF
DISCRETE
CATEGORIES
ARE
CALLED
CLASSIFICATION
PROBLEMS
IF
THE
DESIRED
OUTPUT
CONSISTS
OF
ONE
OR
MORE
CONTINUOUS
VARIABLES
THEN
THE
TASK
IS
CALLED
REGRESSION
AN
EXAMPLE
OF
A
REGRESSION
PROBLEM
WOULD
BE
THE
PRE
DICTION
OF
THE
YIELD
IN
A
CHEMICAL
MANUFACTURING
PROCESS
IN
WHICH
THE
INPUTS
CONSIST
OF
THE
CONCENTRATIONS
OF
REACTANTS
THE
TEMPERATURE
AND
THE
PRESSURE
IN
OTHER
PATTERN
RECOGNITION
PROBLEMS
THE
TRAINING
DATA
CONSISTS
OF
A
SET
OF
INPUT
VECTORS
X
WITHOUT
ANY
CORRESPONDING
TARGET
VALUES
THE
GOAL
IN
SUCH
UNSUPERVISED
LEARNING
PROBLEMS
MAY
BE
TO
DISCOVER
GROUPS
OF
SIMILAR
EXAMPLES
WITHIN
THE
DATA
WHERE
IT
IS
CALLED
CLUSTERING
OR
TO
DETERMINE
THE
DISTRIBUTION
OF
DATA
WITHIN
THE
INPUT
SPACE
KNOWN
AS
DENSITY
ESTIMATION
OR
TO
PROJECT
THE
DATA
FROM
A
HIGH
DIMENSIONAL
SPACE
DOWN
TO
TWO
OR
THREE
DIMENSIONS
FOR
THE
PURPOSE
OF
VISUALIZATION
FINALLY
THE
TECHNIQUE
OF
REINFORCEMENT
LEARNING
SUTTON
AND
BARTO
IS
CON
CERNED
WITH
THE
PROBLEM
OF
FINDING
SUITABLE
ACTIONS
TO
TAKE
IN
A
GIVEN
SITUATION
IN
ORDER
TO
MAXIMIZE
A
REWARD
HERE
THE
LEARNING
ALGORITHM
IS
NOT
GIVEN
EXAMPLES
OF
OPTIMAL
OUTPUTS
IN
CONTRAST
TO
SUPERVISED
LEARNING
BUT
MUST
INSTEAD
DISCOVER
THEM
BY
A
PROCESS
OF
TRIAL
AND
ERROR
TYPICALLY
THERE
IS
A
SEQUENCE
OF
STATES
AND
ACTIONS
IN
WHICH
THE
LEARNING
ALGORITHM
IS
INTERACTING
WITH
ITS
ENVIRONMENT
IN
MANY
CASES
THE
CURRENT
ACTION
NOT
ONLY
AFFECTS
THE
IMMEDIATE
REWARD
BUT
ALSO
HAS
AN
IMPACT
ON
THE
RE
WARD
AT
ALL
SUBSEQUENT
TIME
STEPS
FOR
EXAMPLE
BY
USING
APPROPRIATE
REINFORCEMENT
LEARNING
TECHNIQUES
A
NEURAL
NETWORK
CAN
LEARN
TO
PLAY
THE
GAME
OF
BACKGAMMON
TO
A
HIGH
STANDARD
TESAURO
HERE
THE
NETWORK
MUST
LEARN
TO
TAKE
A
BOARD
POSITION
AS
INPUT
ALONG
WITH
THE
RESULT
OF
A
DICE
THROW
AND
PRODUCE
A
STRONG
MOVE
AS
THE
OUTPUT
THIS
IS
DONE
BY
HAVING
THE
NETWORK
PLAY
AGAINST
A
COPY
OF
ITSELF
FOR
PERHAPS
A
MILLION
GAMES
A
MAJOR
CHALLENGE
IS
THAT
A
GAME
OF
BACKGAMMON
CAN
INVOLVE
DOZENS
OF
MOVES
AND
YET
IT
IS
ONLY
AT
THE
END
OF
THE
GAME
THAT
THE
REWARD
IN
THE
FORM
OF
VICTORY
IS
ACHIEVED
THE
REWARD
MUST
THEN
BE
ATTRIBUTED
APPROPRIATELY
TO
ALL
OF
THE
MOVES
THAT
LED
TO
IT
EVEN
THOUGH
SOME
MOVES
WILL
HAVE
BEEN
GOOD
ONES
AND
OTHERS
LESS
SO
THIS
IS
AN
EXAMPLE
OF
A
CREDIT
ASSIGNMENT
PROBLEM
A
GENERAL
FEATURE
OF
RE
INFORCEMENT
LEARNING
IS
THE
TRADE
OFF
BETWEEN
EXPLORATION
IN
WHICH
THE
SYSTEM
TRIES
OUT
NEW
KINDS
OF
ACTIONS
TO
SEE
HOW
EFFECTIVE
THEY
ARE
AND
EXPLOITATION
IN
WHICH
THE
SYSTEM
MAKES
USE
OF
ACTIONS
THAT
ARE
KNOWN
TO
YIELD
A
HIGH
REWARD
TOO
STRONG
A
FOCUS
ON
EITHER
EXPLORATION
OR
EXPLOITATION
WILL
YIELD
POOR
RESULTS
REINFORCEMENT
LEARNING
CONTINUES
TO
BE
AN
ACTIVE
AREA
OF
MACHINE
LEARNING
RESEARCH
HOWEVER
A
FIGURE
PLOT
OF
A
TRAINING
DATA
SET
OF
N
POINTS
SHOWN
AS
BLUE
CIRCLES
EACH
COMPRISING
AN
OBSERVATION
OF
THE
INPUT
VARIABLE
X
ALONG
WITH
THE
CORRESPONDING
TARGET
VARIABLE
T
T
THE
GREEN
CURVE
SHOWS
THE
FUNCTION
SIN
USED
TO
GENER
ATE
THE
DATA
OUR
GOAL
IS
TO
PRE
DICT
THE
VALUE
OF
T
FOR
SOME
NEW
VALUE
OF
X
WITHOUT
KNOWLEDGE
OF
THE
GREEN
CURVE
X
DETAILED
TREATMENT
LIES
BEYOND
THE
SCOPE
OF
THIS
BOOK
ALTHOUGH
EACH
OF
THESE
TASKS
NEEDS
ITS
OWN
TOOLS
AND
TECHNIQUES
MANY
OF
THE
KEY
IDEAS
THAT
UNDERPIN
THEM
ARE
COMMON
TO
ALL
SUCH
PROBLEMS
ONE
OF
THE
MAIN
GOALS
OF
THIS
CHAPTER
IS
TO
INTRODUCE
IN
A
RELATIVELY
INFORMAL
WAY
SEVERAL
OF
THE
MOST
IMPORTANT
OF
THESE
CONCEPTS
AND
TO
ILLUSTRATE
THEM
USING
SIMPLE
EXAMPLES
LATER
IN
THE
BOOK
WE
SHALL
SEE
THESE
SAME
IDEAS
RE
EMERGE
IN
THE
CONTEXT
OF
MORE
SOPHISTI
CATED
MODELS
THAT
ARE
APPLICABLE
TO
REAL
WORLD
PATTERN
RECOGNITION
APPLICATIONS
THIS
CHAPTER
ALSO
PROVIDES
A
SELF
CONTAINED
INTRODUCTION
TO
THREE
IMPORTANT
TOOLS
THAT
WILL
BE
USED
THROUGHOUT
THE
BOOK
NAMELY
PROBABILITY
THEORY
DECISION
THEORY
AND
INFOR
MATION
THEORY
ALTHOUGH
THESE
MIGHT
SOUND
LIKE
DAUNTING
TOPICS
THEY
ARE
IN
FACT
STRAIGHTFORWARD
AND
A
CLEAR
UNDERSTANDING
OF
THEM
IS
ESSENTIAL
IF
MACHINE
LEARNING
TECHNIQUES
ARE
TO
BE
USED
TO
BEST
EFFECT
IN
PRACTICAL
APPLICATIONS
EXAMPLE
POLYNOMIAL
CURVE
FITTING
WE
BEGIN
BY
INTRODUCING
A
SIMPLE
REGRESSION
PROBLEM
WHICH
WE
SHALL
USE
AS
A
RUN
NING
EXAMPLE
THROUGHOUT
THIS
CHAPTER
TO
MOTIVATE
A
NUMBER
OF
KEY
CONCEPTS
SUP
POSE
WE
OBSERVE
A
REAL
VALUED
INPUT
VARIABLE
X
AND
WE
WISH
TO
USE
THIS
OBSERVATION
TO
PREDICT
THE
VALUE
OF
A
REAL
VALUED
TARGET
VARIABLE
T
FOR
THE
PRESENT
PURPOSES
IT
IS
IN
STRUCTIVE
TO
CONSIDER
AN
ARTIFICIAL
EXAMPLE
USING
SYNTHETICALLY
GENERATED
DATA
BECAUSE
WE
THEN
KNOW
THE
PRECISE
PROCESS
THAT
GENERATED
THE
DATA
FOR
COMPARISON
AGAINST
ANY
LEARNED
MODEL
THE
DATA
FOR
THIS
EXAMPLE
IS
GENERATED
FROM
THE
FUNCTION
SIN
WITH
RANDOM
NOISE
INCLUDED
IN
THE
TARGET
VALUES
AS
DESCRIBED
IN
DETAIL
IN
APPENDIX
A
NOW
SUPPOSE
THAT
WE
ARE
GIVEN
A
TRAINING
SET
COMPRISING
N
OBSERVATIONS
OF
X
WRITTEN
X
XN
T
TOGETHER
WITH
CORRESPONDING
OBSERVATIONS
OF
THE
VALUES
OF
T
DENOTED
T
TN
T
FIGURE
SHOWS
A
PLOT
OF
A
TRAINING
SET
COMPRISING
N
DATA
POINTS
THE
INPUT
DATA
SET
X
IN
FIGURE
WAS
GENERATED
BY
CHOOS
ING
VALUES
OF
XN
FOR
N
N
SPACED
UNIFORMLY
IN
RANGE
AND
THE
TARGET
DATA
SET
T
WAS
OBTAINED
BY
FIRST
COMPUTING
THE
CORRESPONDING
VALUES
OF
THE
FUNCTION
SIN
AND
THEN
ADDING
A
SMALL
LEVEL
OF
RANDOM
NOISE
HAVING
A
GAUSSIAN
DISTRI
BUTION
THE
GAUSSIAN
DISTRIBUTION
IS
DISCUSSED
IN
SECTION
TO
EACH
SUCH
POINT
IN
ORDER
TO
OBTAIN
THE
CORRESPONDING
VALUE
TN
BY
GENERATING
DATA
IN
THIS
WAY
WE
ARE
CAPTURING
A
PROPERTY
OF
MANY
REAL
DATA
SETS
NAMELY
THAT
THEY
POSSESS
AN
UNDERLYING
REGULARITY
WHICH
WE
WISH
TO
LEARN
BUT
THAT
INDIVIDUAL
OBSERVATIONS
ARE
CORRUPTED
BY
RANDOM
NOISE
THIS
NOISE
MIGHT
ARISE
FROM
INTRINSICALLY
STOCHASTIC
I
E
RANDOM
PRO
CESSES
SUCH
AS
RADIOACTIVE
DECAY
BUT
MORE
TYPICALLY
IS
DUE
TO
THERE
BEING
SOURCES
OF
VARIABILITY
THAT
ARE
THEMSELVES
UNOBSERVED
OUR
GOAL
IS
TO
EXPLOIT
THIS
TRAINING
SET
IN
ORDER
TO
MAKE
PREDICTIONS
OF
THE
VALUE
T
OF
THE
TARGET
VARIABLE
FOR
SOME
NEW
VALUE
X
OF
THE
INPUT
VARIABLE
AS
WE
SHALL
SEE
LATER
THIS
INVOLVES
IMPLICITLY
TRYING
TO
DISCOVER
THE
UNDERLYING
FUNCTION
SIN
THIS
IS
INTRINSICALLY
A
DIFFICULT
PROBLEM
AS
WE
HAVE
TO
GENERALIZE
FROM
A
FINITE
DATA
SET
FURTHERMORE
THE
OBSERVED
DATA
ARE
CORRUPTED
WITH
NOISE
AND
SO
FOR
A
GIVEN
X
THERE
IS
UNCERTAINTY
AS
TO
THE
APPROPRIATE
VALUE
FOR
T
PROBABILITY
THEORY
DISCUSSED
IN
SECTION
PROVIDES
A
FRAMEWORK
FOR
EXPRESSING
SUCH
UNCERTAINTY
IN
A
PRECISE
AND
QUANTITATIVE
MANNER
AND
DECISION
THEORY
DISCUSSED
IN
SECTION
ALLOWS
US
TO
EXPLOIT
THIS
PROBABILISTIC
REPRESENTATION
IN
ORDER
TO
MAKE
PREDICTIONS
THAT
ARE
OPTIMAL
ACCORDING
TO
APPROPRIATE
CRITERIA
FOR
THE
MOMENT
HOWEVER
WE
SHALL
PROCEED
RATHER
INFORMALLY
AND
CONSIDER
A
SIMPLE
APPROACH
BASED
ON
CURVE
FITTING
IN
PARTICULAR
WE
SHALL
FIT
THE
DATA
USING
A
POLYNOMIAL
FUNCTION
OF
THE
FORM
M
Y
X
W
WMXM
WJXJ
J
WHERE
M
IS
THE
ORDER
OF
THE
POLYNOMIAL
AND
XJ
DENOTES
X
RAISED
TO
THE
POWER
OF
J
THE
POLYNOMIAL
COEFFICIENTS
WM
ARE
COLLECTIVELY
DENOTED
BY
THE
VECTOR
W
NOTE
THAT
ALTHOUGH
THE
POLYNOMIAL
FUNCTION
Y
X
W
IS
A
NONLINEAR
FUNCTION
OF
X
IT
IS
A
LINEAR
FUNCTION
OF
THE
COEFFICIENTS
W
FUNCTIONS
SUCH
AS
THE
POLYNOMIAL
WHICH
ARE
LINEAR
IN
THE
UNKNOWN
PARAMETERS
HAVE
IMPORTANT
PROPERTIES
AND
ARE
CALLED
LINEAR
MODELS
AND
WILL
BE
DISCUSSED
EXTENSIVELY
IN
CHAPTERS
AND
THE
VALUES
OF
THE
COEFFICIENTS
WILL
BE
DETERMINED
BY
FITTING
THE
POLYNOMIAL
TO
THE
TRAINING
DATA
THIS
CAN
BE
DONE
BY
MINIMIZING
AN
ERROR
FUNCTION
THAT
MEASURES
THE
MISFIT
BETWEEN
THE
FUNCTION
Y
X
W
FOR
ANY
GIVEN
VALUE
OF
W
AND
THE
TRAINING
SET
DATA
POINTS
ONE
SIMPLE
CHOICE
OF
ERROR
FUNCTION
WHICH
IS
WIDELY
USED
IS
GIVEN
BY
THE
SUM
OF
THE
SQUARES
OF
THE
ERRORS
BETWEEN
THE
PREDICTIONS
Y
XN
W
FOR
EACH
DATA
POINT
XN
AND
THE
CORRESPONDING
TARGET
VALUES
TN
SO
THAT
WE
MINIMIZE
E
W
Y
X
W
T
WHERE
THE
FACTOR
OF
IS
INCLUDED
FOR
LATER
CONVENIENCE
WE
SHALL
DISCUSS
THE
MO
TIVATION
FOR
THIS
CHOICE
OF
ERROR
FUNCTION
LATER
IN
THIS
CHAPTER
FOR
THE
MOMENT
WE
SIMPLY
NOTE
THAT
IT
IS
A
NONNEGATIVE
QUANTITY
THAT
WOULD
BE
ZERO
IF
AND
ONLY
IF
THE
FIGURE
THE
ERROR
FUNCTION
CORRE
SPONDS
TO
ONE
HALF
OF
THE
SUM
OF
T
THE
SQUARES
OF
THE
DISPLACEMENTS
SHOWN
BY
THE
VERTICAL
GREEN
BARS
OF
EACH
DATA
POINT
FROM
THE
FUNCTION
Y
X
W
XN
X
EXERCISE
FUNCTION
Y
X
W
WERE
TO
PASS
EXACTLY
THROUGH
EACH
TRAINING
DATA
POINT
THE
GEOMET
RICAL
INTERPRETATION
OF
THE
SUM
OF
SQUARES
ERROR
FUNCTION
IS
ILLUSTRATED
IN
FIGURE
WE
CAN
SOLVE
THE
CURVE
FITTING
PROBLEM
BY
CHOOSING
THE
VALUE
OF
W
FOR
WHICH
E
W
IS
AS
SMALL
AS
POSSIBLE
BECAUSE
THE
ERROR
FUNCTION
IS
A
QUADRATIC
FUNCTION
OF
THE
COEFFICIENTS
W
ITS
DERIVATIVES
WITH
RESPECT
TO
THE
COEFFICIENTS
WILL
BE
LINEAR
IN
THE
ELEMENTS
OF
W
AND
SO
THE
MINIMIZATION
OF
THE
ERROR
FUNCTION
HAS
A
UNIQUE
SOLUTION
DENOTED
BY
W
WHICH
CAN
BE
FOUND
IN
CLOSED
FORM
THE
RESULTING
POLYNOMIAL
IS
GIVEN
BY
THE
FUNCTION
Y
X
W
THERE
REMAINS
THE
PROBLEM
OF
CHOOSING
THE
ORDER
M
OF
THE
POLYNOMIAL
AND
AS
WE
SHALL
SEE
THIS
WILL
TURN
OUT
TO
BE
AN
EXAMPLE
OF
AN
IMPORTANT
CONCEPT
CALLED
MODEL
COMPARISON
OR
MODEL
SELECTION
IN
FIGURE
WE
SHOW
FOUR
EXAMPLES
OF
THE
RESULTS
OF
FITTING
POLYNOMIALS
HAVING
ORDERS
M
AND
TO
THE
DATA
SET
SHOWN
IN
FIGURE
WE
NOTICE
THAT
THE
CONSTANT
M
AND
FIRST
ORDER
M
POLYNOMIALS
GIVE
RATHER
POOR
FITS
TO
THE
DATA
AND
CONSEQUENTLY
RATHER
POOR
REPRESENTATIONS
OF
THE
FUNCTION
SIN
THE
THIRD
ORDER
M
POLYNOMIAL
SEEMS
TO
GIVE
THE
BEST
FIT
TO
THE
FUNCTION
SIN
OF
THE
EXAMPLES
SHOWN
IN
FIGURE
WHEN
WE
GO
TO
A
MUCH
HIGHER
ORDER
POLYNOMIAL
M
WE
OBTAIN
AN
EXCELLENT
FIT
TO
THE
TRAINING
DATA
IN
FACT
THE
POLYNOMIAL
PASSES
EXACTLY
THROUGH
EACH
DATA
POINT
AND
E
W
HOWEVER
THE
FITTED
CURVE
OSCILLATES
WILDLY
AND
GIVES
A
VERY
POOR
REPRESENTATION
OF
THE
FUNCTION
SIN
THIS
LATTER
BEHAVIOUR
IS
KNOWN
AS
OVER
FITTING
AS
WE
HAVE
NOTED
EARLIER
THE
GOAL
IS
TO
ACHIEVE
GOOD
GENERALIZATION
BY
MAKING
ACCURATE
PREDICTIONS
FOR
NEW
DATA
WE
CAN
OBTAIN
SOME
QUANTITATIVE
INSIGHT
INTO
THE
DEPENDENCE
OF
THE
GENERALIZATION
PERFORMANCE
ON
M
BY
CONSIDERING
A
SEPARATE
TEST
SET
COMPRISING
DATA
POINTS
GENERATED
USING
EXACTLY
THE
SAME
PROCEDURE
USED
TO
GENERATE
THE
TRAINING
SET
POINTS
BUT
WITH
NEW
CHOICES
FOR
THE
RANDOM
NOISE
VALUES
INCLUDED
IN
THE
TARGET
VALUES
FOR
EACH
CHOICE
OF
M
WE
CAN
THEN
EVALUATE
THE
RESIDUAL
VALUE
OF
E
W
GIVEN
BY
FOR
THE
TRAINING
DATA
AND
WE
CAN
ALSO
EVALUATE
E
W
FOR
THE
TEST
DATA
SET
IT
IS
SOMETIMES
MORE
CONVENIENT
TO
USE
THE
ROOT
MEAN
SQUARE
T
T
X
X
T
T
X
X
FIGURE
PLOTS
OF
POLYNOMIALS
HAVING
VARIOUS
ORDERS
M
SHOWN
AS
RED
CURVES
FITTED
TO
THE
DATA
SET
SHOWN
IN
FIGURE
RMS
ERROR
DEFINED
BY
ERMS
W
N
IN
WHICH
THE
DIVISION
BY
N
ALLOWS
US
TO
COMPARE
DIFFERENT
SIZES
OF
DATA
SETS
ON
AN
EQUAL
FOOTING
AND
THE
SQUARE
ROOT
ENSURES
THAT
ERMS
IS
MEASURED
ON
THE
SAME
SCALE
AND
IN
THE
SAME
UNITS
AS
THE
TARGET
VARIABLE
T
GRAPHS
OF
THE
TRAINING
AND
TEST
SET
RMS
ERRORS
ARE
SHOWN
FOR
VARIOUS
VALUES
OF
M
IN
FIGURE
THE
TEST
SET
ERROR
IS
A
MEASURE
OF
HOW
WELL
WE
ARE
DOING
IN
PREDICTING
THE
VALUES
OF
T
FOR
NEW
DATA
OBSERVATIONS
OF
X
WE
NOTE
FROM
FIGURE
THAT
SMALL
VALUES
OF
M
GIVE
RELATIVELY
LARGE
VALUES
OF
THE
TEST
SET
ERROR
AND
THIS
CAN
BE
ATTRIBUTED
TO
THE
FACT
THAT
THE
CORRESPONDING
POLYNOMIALS
ARE
RATHER
INFLEXIBLE
AND
ARE
INCAPABLE
OF
CAPTURING
THE
OSCILLATIONS
IN
THE
FUNCTION
SIN
VALUES
OF
M
IN
THE
RANGE
M
GIVE
SMALL
VALUES
FOR
THE
TEST
SET
ERROR
AND
THESE
ALSO
GIVE
REASONABLE
REPRESENTATIONS
OF
THE
GENERATING
FUNCTION
SIN
AS
CAN
BE
SEEN
FOR
THE
CASE
OF
M
FROM
FIGURE
FIGURE
GRAPHS
OF
THE
ROOT
MEAN
SQUARE
ERROR
DEFINED
BY
EVALUATED
ON
THE
TRAINING
SET
AND
ON
AN
INDE
PENDENT
TEST
SET
FOR
VARIOUS
VALUES
OF
M
M
FOR
M
THE
TRAINING
SET
ERROR
GOES
TO
ZERO
AS
WE
MIGHT
EXPECT
BECAUSE
THIS
POLYNOMIAL
CONTAINS
DEGREES
OF
FREEDOM
CORRESPONDING
TO
THE
COEFFICIENTS
AND
SO
CAN
BE
TUNED
EXACTLY
TO
THE
DATA
POINTS
IN
THE
TRAINING
SET
HOWEVER
THE
TEST
SET
ERROR
HAS
BECOME
VERY
LARGE
AND
AS
WE
SAW
IN
FIGURE
THE
CORRESPONDING
FUNCTION
Y
X
W
EXHIBITS
WILD
OSCILLATIONS
THIS
MAY
SEEM
PARADOXICAL
BECAUSE
A
POLYNOMIAL
OF
GIVEN
ORDER
CONTAINS
ALL
LOWER
ORDER
POLYNOMIALS
AS
SPECIAL
CASES
THE
M
POLYNOMIAL
IS
THEREFORE
CAPA
BLE
OF
GENERATING
RESULTS
AT
LEAST
AS
GOOD
AS
THE
M
POLYNOMIAL
FURTHERMORE
WE
MIGHT
SUPPOSE
THAT
THE
BEST
PREDICTOR
OF
NEW
DATA
WOULD
BE
THE
FUNCTION
SIN
FROM
WHICH
THE
DATA
WAS
GENERATED
AND
WE
SHALL
SEE
LATER
THAT
THIS
IS
INDEED
THE
CASE
WE
KNOW
THAT
A
POWER
SERIES
EXPANSION
OF
THE
FUNCTION
SIN
CONTAINS
TERMS
OF
ALL
ORDERS
SO
WE
MIGHT
EXPECT
THAT
RESULTS
SHOULD
IMPROVE
MONOTONICALLY
AS
WE
INCREASE
M
WE
CAN
GAIN
SOME
INSIGHT
INTO
THE
PROBLEM
BY
EXAMINING
THE
VALUES
OF
THE
CO
EFFICIENTS
W
OBTAINED
FROM
POLYNOMIALS
OF
VARIOUS
ORDER
AS
SHOWN
IN
TABLE
WE
SEE
THAT
AS
M
INCREASES
THE
MAGNITUDE
OF
THE
COEFFICIENTS
TYPICALLY
GETS
LARGER
IN
PARTICULAR
FOR
THE
M
POLYNOMIAL
THE
COEFFICIENTS
HAVE
BECOME
FINELY
TUNED
TO
THE
DATA
BY
DEVELOPING
LARGE
POSITIVE
AND
NEGATIVE
VALUES
SO
THAT
THE
CORRESPOND
TABLE
TABLE
OF
THE
COEFFICIENTS
W
FOR
POLYNOMIALS
OF
VARIOUS
ORDER
OBSERVE
HOW
THE
TYPICAL
MAG
NITUDE
OF
THE
COEFFICIENTS
IN
CREASES
DRAMATICALLY
AS
THE
OR
DER
OF
THE
POLYNOMIAL
INCREASES
T
T
X
X
FIGURE
PLOTS
OF
THE
SOLUTIONS
OBTAINED
BY
MINIMIZING
THE
SUM
OF
SQUARES
ERROR
FUNCTION
USING
THE
M
POLYNOMIAL
FOR
N
DATA
POINTS
LEFT
PLOT
AND
N
DATA
POINTS
RIGHT
PLOT
WE
SEE
THAT
INCREASING
THE
SIZE
OF
THE
DATA
SET
REDUCES
THE
OVER
FITTING
PROBLEM
SECTION
ING
POLYNOMIAL
FUNCTION
MATCHES
EACH
OF
THE
DATA
POINTS
EXACTLY
BUT
BETWEEN
DATA
POINTS
PARTICULARLY
NEAR
THE
ENDS
OF
THE
RANGE
THE
FUNCTION
EXHIBITS
THE
LARGE
OSCILLA
TIONS
OBSERVED
IN
FIGURE
INTUITIVELY
WHAT
IS
HAPPENING
IS
THAT
THE
MORE
FLEXIBLE
POLYNOMIALS
WITH
LARGER
VALUES
OF
M
ARE
BECOMING
INCREASINGLY
TUNED
TO
THE
RANDOM
NOISE
ON
THE
TARGET
VALUES
IT
IS
ALSO
INTERESTING
TO
EXAMINE
THE
BEHAVIOUR
OF
A
GIVEN
MODEL
AS
THE
SIZE
OF
THE
DATA
SET
IS
VARIED
AS
SHOWN
IN
FIGURE
WE
SEE
THAT
FOR
A
GIVEN
MODEL
COMPLEXITY
THE
OVER
FITTING
PROBLEM
BECOME
LESS
SEVERE
AS
THE
SIZE
OF
THE
DATA
SET
INCREASES
ANOTHER
WAY
TO
SAY
THIS
IS
THAT
THE
LARGER
THE
DATA
SET
THE
MORE
COMPLEX
IN
OTHER
WORDS
MORE
FLEXIBLE
THE
MODEL
THAT
WE
CAN
AFFORD
TO
FIT
TO
THE
DATA
ONE
ROUGH
HEURISTIC
THAT
IS
SOMETIMES
ADVOCATED
IS
THAT
THE
NUMBER
OF
DATA
POINTS
SHOULD
BE
NO
LESS
THAN
SOME
MULTIPLE
SAY
OR
OF
THE
NUMBER
OF
ADAPTIVE
PARAMETERS
IN
THE
MODEL
HOWEVER
AS
WE
SHALL
SEE
IN
CHAPTER
THE
NUMBER
OF
PARAMETERS
IS
NOT
NECESSARILY
THE
MOST
APPROPRIATE
MEASURE
OF
MODEL
COMPLEXITY
ALSO
THERE
IS
SOMETHING
RATHER
UNSATISFYING
ABOUT
HAVING
TO
LIMIT
THE
NUMBER
OF
PARAMETERS
IN
A
MODEL
ACCORDING
TO
THE
SIZE
OF
THE
AVAILABLE
TRAINING
SET
IT
WOULD
SEEM
MORE
REASONABLE
TO
CHOOSE
THE
COMPLEXITY
OF
THE
MODEL
ACCORDING
TO
THE
COM
PLEXITY
OF
THE
PROBLEM
BEING
SOLVED
WE
SHALL
SEE
THAT
THE
LEAST
SQUARES
APPROACH
TO
FINDING
THE
MODEL
PARAMETERS
REPRESENTS
A
SPECIFIC
CASE
OF
MAXIMUM
LIKELIHOOD
DISCUSSED
IN
SECTION
AND
THAT
THE
OVER
FITTING
PROBLEM
CAN
BE
UNDERSTOOD
AS
A
GENERAL
PROPERTY
OF
MAXIMUM
LIKELIHOOD
BY
ADOPTING
A
BAYESIAN
APPROACH
THE
OVER
FITTING
PROBLEM
CAN
BE
AVOIDED
WE
SHALL
SEE
THAT
THERE
IS
NO
DIFFICULTY
FROM
A
BAYESIAN
PERSPECTIVE
IN
EMPLOYING
MODELS
FOR
WHICH
THE
NUMBER
OF
PARAMETERS
GREATLY
EXCEEDS
THE
NUMBER
OF
DATA
POINTS
INDEED
IN
A
BAYESIAN
MODEL
THE
EFFECTIVE
NUMBER
OF
PARAMETERS
ADAPTS
AUTOMATICALLY
TO
THE
SIZE
OF
THE
DATA
SET
FOR
THE
MOMENT
HOWEVER
IT
IS
INSTRUCTIVE
TO
CONTINUE
WITH
THE
CURRENT
APPROACH
AND
TO
CONSIDER
HOW
IN
PRACTICE
WE
CAN
APPLY
IT
TO
DATA
SETS
OF
LIMITED
SIZE
WHERE
WE
T
T
X
X
FIGURE
PLOTS
OF
M
POLYNOMIALS
FITTED
TO
THE
DATA
SET
SHOWN
IN
FIGURE
USING
THE
REGULARIZED
ERROR
FUNCTION
FOR
TWO
VALUES
OF
THE
REGULARIZATION
PARAMETER
Λ
CORRESPONDING
TO
LN
Λ
AND
LN
Λ
THE
CASE
OF
NO
REGULARIZER
I
E
Λ
CORRESPONDING
TO
LN
Λ
IS
SHOWN
AT
THE
BOTTOM
RIGHT
OF
FIGURE
MAY
WISH
TO
USE
RELATIVELY
COMPLEX
AND
FLEXIBLE
MODELS
ONE
TECHNIQUE
THAT
IS
OFTEN
USED
TO
CONTROL
THE
OVER
FITTING
PHENOMENON
IN
SUCH
CASES
IS
THAT
OF
REGULARIZATION
WHICH
INVOLVES
ADDING
A
PENALTY
TERM
TO
THE
ERROR
FUNCTION
IN
ORDER
TO
DISCOURAGE
THE
COEFFICIENTS
FROM
REACHING
LARGE
VALUES
THE
SIMPLEST
SUCH
PENALTY
TERM
TAKES
THE
FORM
OF
A
SUM
OF
SQUARES
OF
ALL
OF
THE
COEFFICIENTS
LEADING
TO
A
MODIFIED
ERROR
FUNCTION
OF
THE
FORM
E
W
Y
X
W
T
LWL
WHERE
WTW
AND
THE
COEFFICIENT
Λ
GOVERNS
THE
REL
EXERCISE
ATIVE
IMPORTANCE
OF
THE
REGULARIZATION
TERM
COMPARED
WITH
THE
SUM
OF
SQUARES
ERROR
TERM
NOTE
THAT
OFTEN
THE
COEFFICIENT
IS
OMITTED
FROM
THE
REGULARIZER
BECAUSE
ITS
INCLUSION
CAUSES
THE
RESULTS
TO
DEPEND
ON
THE
CHOICE
OF
ORIGIN
FOR
THE
TARGET
VARIABLE
HASTIE
ET
AL
OR
IT
MAY
BE
INCLUDED
BUT
WITH
ITS
OWN
REGULARIZATION
COEFFICIENT
WE
SHALL
DISCUSS
THIS
TOPIC
IN
MORE
DETAIL
IN
SECTION
AGAIN
THE
ERROR
FUNCTION
IN
CAN
BE
MINIMIZED
EXACTLY
IN
CLOSED
FORM
TECHNIQUES
SUCH
AS
THIS
ARE
KNOWN
IN
THE
STATISTICS
LITERATURE
AS
SHRINKAGE
METHODS
BECAUSE
THEY
REDUCE
THE
VALUE
OF
THE
COEFFICIENTS
THE
PARTICULAR
CASE
OF
A
QUADRATIC
REGULARIZER
IS
CALLED
RIDGE
REGRES
SION
HOERL
AND
KENNARD
IN
THE
CONTEXT
OF
NEURAL
NETWORKS
THIS
APPROACH
IS
KNOWN
AS
WEIGHT
DECAY
FIGURE
SHOWS
THE
RESULTS
OF
FITTING
THE
POLYNOMIAL
OF
ORDER
M
TO
THE
SAME
DATA
SET
AS
BEFORE
BUT
NOW
USING
THE
REGULARIZED
ERROR
FUNCTION
GIVEN
BY
WE
SEE
THAT
FOR
A
VALUE
OF
LN
Λ
THE
OVER
FITTING
HAS
BEEN
SUPPRESSED
AND
WE
NOW
OBTAIN
A
MUCH
CLOSER
REPRESENTATION
OF
THE
UNDERLYING
FUNCTION
SIN
IF
HOWEVER
WE
USE
TOO
LARGE
A
VALUE
FOR
Λ
THEN
WE
AGAIN
OBTAIN
A
POOR
FIT
AS
SHOWN
IN
FIGURE
FOR
LN
Λ
THE
CORRESPONDING
COEFFICIENTS
FROM
THE
FITTED
POLYNOMIALS
ARE
GIVEN
IN
TABLE
SHOWING
THAT
REGULARIZATION
HAS
THE
DESIRED
EFFECT
OF
REDUCING
TABLE
TABLE
OF
THE
COEFFICIENTS
W
FOR
M
POLYNOMIALS
WITH
VARIOUS
VALUES
FOR
THE
REGULARIZATION
PARAMETER
Λ
NOTE
THAT
LN
Λ
CORRESPONDS
TO
A
MODEL
WITH
NO
REGULARIZATION
I
E
TO
THE
GRAPH
AT
THE
BOTTOM
RIGHT
IN
FIG
URE
WE
SEE
THAT
AS
THE
VALUE
OF
Λ
INCREASES
THE
TYPICAL
MAGNITUDE
OF
THE
COEFFICIENTS
GETS
SMALLER
SECTION
THE
MAGNITUDE
OF
THE
COEFFICIENTS
THE
IMPACT
OF
THE
REGULARIZATION
TERM
ON
THE
GENERALIZATION
ERROR
CAN
BE
SEEN
BY
PLOTTING
THE
VALUE
OF
THE
RMS
ERROR
FOR
BOTH
TRAINING
AND
TEST
SETS
AGAINST
LN
Λ
AS
SHOWN
IN
FIGURE
WE
SEE
THAT
IN
EFFECT
Λ
NOW
CONTROLS
THE
EFFECTIVE
COMPLEXITY
OF
THE
MODEL
AND
HENCE
DETERMINES
THE
DEGREE
OF
OVER
FITTING
THE
ISSUE
OF
MODEL
COMPLEXITY
IS
AN
IMPORTANT
ONE
AND
WILL
BE
DISCUSSED
AT
LENGTH
IN
SECTION
HERE
WE
SIMPLY
NOTE
THAT
IF
WE
WERE
TRYING
TO
SOLVE
A
PRACTICAL
APPLICATION
USING
THIS
APPROACH
OF
MINIMIZING
AN
ERROR
FUNCTION
WE
WOULD
HAVE
TO
FIND
A
WAY
TO
DETERMINE
A
SUITABLE
VALUE
FOR
THE
MODEL
COMPLEXITY
THE
RESULTS
ABOVE
SUGGEST
A
SIMPLE
WAY
OF
ACHIEVING
THIS
NAMELY
BY
TAKING
THE
AVAILABLE
DATA
AND
PARTITIONING
IT
INTO
A
TRAINING
SET
USED
TO
DETERMINE
THE
COEFFICIENTS
W
AND
A
SEPARATE
VALIDATION
SET
ALSO
CALLED
A
HOLD
OUT
SET
USED
TO
OPTIMIZE
THE
MODEL
COMPLEXITY
EITHER
M
OR
Λ
IN
MANY
CASES
HOWEVER
THIS
WILL
PROVE
TO
BE
TOO
WASTEFUL
OF
VALUABLE
TRAINING
DATA
AND
WE
HAVE
TO
SEEK
MORE
SOPHISTICATED
APPROACHES
SO
FAR
OUR
DISCUSSION
OF
POLYNOMIAL
CURVE
FITTING
HAS
APPEALED
LARGELY
TO
IN
TUITION
WE
NOW
SEEK
A
MORE
PRINCIPLED
APPROACH
TO
SOLVING
PROBLEMS
IN
PATTERN
RECOGNITION
BY
TURNING
TO
A
DISCUSSION
OF
PROBABILITY
THEORY
AS
WELL
AS
PROVIDING
THE
FOUNDATION
FOR
NEARLY
ALL
OF
THE
SUBSEQUENT
DEVELOPMENTS
IN
THIS
BOOK
IT
WILL
ALSO
FIGURE
GRAPH
OF
THE
ROOT
MEAN
SQUARE
ER
ROR
VERSUS
LN
Λ
FOR
THE
M
POLYNOMIAL
LN
Λ
GIVE
US
SOME
IMPORTANT
INSIGHTS
INTO
THE
CONCEPTS
WE
HAVE
INTRODUCED
IN
THE
CON
TEXT
OF
POLYNOMIAL
CURVE
FITTING
AND
WILL
ALLOW
US
TO
EXTEND
THESE
TO
MORE
COMPLEX
SITUATIONS
PROBABILITY
THEORY
A
KEY
CONCEPT
IN
THE
FIELD
OF
PATTERN
RECOGNITION
IS
THAT
OF
UNCERTAINTY
IT
ARISES
BOTH
THROUGH
NOISE
ON
MEASUREMENTS
AS
WELL
AS
THROUGH
THE
FINITE
SIZE
OF
DATA
SETS
PROB
ABILITY
THEORY
PROVIDES
A
CONSISTENT
FRAMEWORK
FOR
THE
QUANTIFICATION
AND
MANIPULA
TION
OF
UNCERTAINTY
AND
FORMS
ONE
OF
THE
CENTRAL
FOUNDATIONS
FOR
PATTERN
RECOGNITION
WHEN
COMBINED
WITH
DECISION
THEORY
DISCUSSED
IN
SECTION
IT
ALLOWS
US
TO
MAKE
OPTIMAL
PREDICTIONS
GIVEN
ALL
THE
INFORMATION
AVAILABLE
TO
US
EVEN
THOUGH
THAT
INFOR
MATION
MAY
BE
INCOMPLETE
OR
AMBIGUOUS
WE
WILL
INTRODUCE
THE
BASIC
CONCEPTS
OF
PROBABILITY
THEORY
BY
CONSIDERING
A
SIM
PLE
EXAMPLE
IMAGINE
WE
HAVE
TWO
BOXES
ONE
RED
AND
ONE
BLUE
AND
IN
THE
RED
BOX
WE
HAVE
APPLES
AND
ORANGES
AND
IN
THE
BLUE
BOX
WE
HAVE
APPLES
AND
ORANGE
THIS
IS
ILLUSTRATED
IN
FIGURE
NOW
SUPPOSE
WE
RANDOMLY
PICK
ONE
OF
THE
BOXES
AND
FROM
THAT
BOX
WE
RANDOMLY
SELECT
AN
ITEM
OF
FRUIT
AND
HAVING
OBSERVED
WHICH
SORT
OF
FRUIT
IT
IS
WE
REPLACE
IT
IN
THE
BOX
FROM
WHICH
IT
CAME
WE
COULD
IMAGINE
REPEATING
THIS
PROCESS
MANY
TIMES
LET
US
SUPPOSE
THAT
IN
SO
DOING
WE
PICK
THE
RED
BOX
OF
THE
TIME
AND
WE
PICK
THE
BLUE
BOX
OF
THE
TIME
AND
THAT
WHEN
WE
REMOVE
AN
ITEM
OF
FRUIT
FROM
A
BOX
WE
ARE
EQUALLY
LIKELY
TO
SELECT
ANY
OF
THE
PIECES
OF
FRUIT
IN
THE
BOX
IN
THIS
EXAMPLE
THE
IDENTITY
OF
THE
BOX
THAT
WILL
BE
CHOSEN
IS
A
RANDOM
VARIABLE
WHICH
WE
SHALL
DENOTE
BY
B
THIS
RANDOM
VARIABLE
CAN
TAKE
ONE
OF
TWO
POSSIBLE
VALUES
NAMELY
R
CORRESPONDING
TO
THE
RED
BOX
OR
B
CORRESPONDING
TO
THE
BLUE
BOX
SIMILARLY
THE
IDENTITY
OF
THE
FRUIT
IS
ALSO
A
RANDOM
VARIABLE
AND
WILL
BE
DENOTED
BY
F
IT
CAN
TAKE
EITHER
OF
THE
VALUES
A
FOR
APPLE
OR
O
FOR
ORANGE
TO
BEGIN
WITH
WE
SHALL
DEFINE
THE
PROBABILITY
OF
AN
EVENT
TO
BE
THE
FRACTION
OF
TIMES
THAT
EVENT
OCCURS
OUT
OF
THE
TOTAL
NUMBER
OF
TRIALS
IN
THE
LIMIT
THAT
THE
TOTAL
NUMBER
OF
TRIALS
GOES
TO
INFINITY
THUS
THE
PROBABILITY
OF
SELECTING
THE
RED
BOX
IS
FIGURE
WE
USE
A
SIMPLE
EXAMPLE
OF
TWO
COLOURED
BOXES
EACH
CONTAINING
FRUIT
APPLES
SHOWN
IN
GREEN
AND
OR
ANGES
SHOWN
IN
ORANGE
TO
INTRO
DUCE
THE
BASIC
IDEAS
OF
PROBABILITY
FIGURE
WE
CAN
DERIVE
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
BY
CONSIDERING
TWO
RANDOM
VARIABLES
X
WHICH
TAKES
THE
VALUES
XI
WHERE
I
M
AND
Y
WHICH
TAKES
THE
VALUES
YJ
WHERE
J
L
IN
THIS
ILLUSTRATION
WE
HAVE
M
AND
L
IF
WE
CONSIDER
A
TOTAL
NUMBER
N
OF
INSTANCES
OF
THESE
VARIABLES
THEN
WE
DENOTE
THE
NUMBER
OF
INSTANCES
WHERE
X
XI
AND
Y
YJ
BY
NIJ
WHICH
IS
THE
NUMBER
OF
YJ
POINTS
IN
THE
CORRESPONDING
CELL
OF
THE
ARRAY
THE
NUMBER
OF
POINTS
IN
COLUMN
I
CORRESPONDING
TO
X
XI
IS
DENOTED
BY
CI
AND
THE
NUMBER
OF
POINTS
IN
ROW
J
CORRESPONDING
TO
Y
YJ
IS
DENOTED
BY
RJ
CI
RJ
XI
AND
THE
PROBABILITY
OF
SELECTING
THE
BLUE
BOX
IS
WE
WRITE
THESE
PROBABILITIES
AS
P
B
R
AND
P
B
B
NOTE
THAT
BY
DEFINITION
PROBABILITIES
MUST
LIE
IN
THE
INTERVAL
ALSO
IF
THE
EVENTS
ARE
MUTUALLY
EXCLUSIVE
AND
IF
THEY
INCLUDE
ALL
POSSIBLE
OUTCOMES
FOR
INSTANCE
IN
THIS
EXAMPLE
THE
BOX
MUST
BE
EITHER
RED
OR
BLUE
THEN
WE
SEE
THAT
THE
PROBABILITIES
FOR
THOSE
EVENTS
MUST
SUM
TO
ONE
WE
CAN
NOW
ASK
QUESTIONS
SUCH
AS
WHAT
IS
THE
OVERALL
PROBABILITY
THAT
THE
SE
LECTION
PROCEDURE
WILL
PICK
AN
APPLE
OR
GIVEN
THAT
WE
HAVE
CHOSEN
AN
ORANGE
WHAT
IS
THE
PROBABILITY
THAT
THE
BOX
WE
CHOSE
WAS
THE
BLUE
ONE
WE
CAN
ANSWER
QUESTIONS
SUCH
AS
THESE
AND
INDEED
MUCH
MORE
COMPLEX
QUESTIONS
ASSOCIATED
WITH
PROBLEMS
IN
PATTERN
RECOGNITION
ONCE
WE
HAVE
EQUIPPED
OURSELVES
WITH
THE
TWO
EL
EMENTARY
RULES
OF
PROBABILITY
KNOWN
AS
THE
SUM
RULE
AND
THE
PRODUCT
RULE
HAVING
OBTAINED
THESE
RULES
WE
SHALL
THEN
RETURN
TO
OUR
BOXES
OF
FRUIT
EXAMPLE
IN
ORDER
TO
DERIVE
THE
RULES
OF
PROBABILITY
CONSIDER
THE
SLIGHTLY
MORE
GENERAL
EX
AMPLE
SHOWN
IN
FIGURE
INVOLVING
TWO
RANDOM
VARIABLES
X
AND
Y
WHICH
COULD
FOR
INSTANCE
BE
THE
BOX
AND
FRUIT
VARIABLES
CONSIDERED
ABOVE
WE
SHALL
SUPPOSE
THAT
X
CAN
TAKE
ANY
OF
THE
VALUES
XI
WHERE
I
M
AND
Y
CAN
TAKE
THE
VALUES
YJ
WHERE
J
L
CONSIDER
A
TOTAL
OF
N
TRIALS
IN
WHICH
WE
SAMPLE
BOTH
OF
THE
VARIABLES
X
AND
Y
AND
LET
THE
NUMBER
OF
SUCH
TRIALS
IN
WHICH
X
XI
AND
Y
YJ
BE
NIJ
ALSO
LET
THE
NUMBER
OF
TRIALS
IN
WHICH
X
TAKES
THE
VALUE
XI
IRRESPECTIVE
OF
THE
VALUE
THAT
Y
TAKES
BE
DENOTED
BY
CI
AND
SIMILARLY
LET
THE
NUMBER
OF
TRIALS
IN
WHICH
Y
TAKES
THE
VALUE
YJ
BE
DENOTED
BY
RJ
THE
PROBABILITY
THAT
X
WILL
TAKE
THE
VALUE
XI
AND
Y
WILL
TAKE
THE
VALUE
YJ
IS
WRITTEN
P
X
XI
Y
YJ
AND
IS
CALLED
THE
JOINT
PROBABILITY
OF
X
XI
AND
Y
YJ
IT
IS
GIVEN
BY
THE
NUMBER
OF
POINTS
FALLING
IN
THE
CELL
I
J
AS
A
FRACTION
OF
THE
TOTAL
NUMBER
OF
POINTS
AND
HENCE
P
X
X
Y
Y
NIJ
I
J
N
HERE
WE
ARE
IMPLICITLY
CONSIDERING
THE
LIMIT
N
SIMILARLY
THE
PROBABILITY
THAT
X
TAKES
THE
VALUE
XI
IRRESPECTIVE
OF
THE
VALUE
OF
Y
IS
WRITTEN
AS
P
X
XI
AND
IS
GIVEN
BY
THE
FRACTION
OF
THE
TOTAL
NUMBER
OF
POINTS
THAT
FALL
IN
COLUMN
I
SO
THAT
P
X
X
CI
I
N
BECAUSE
THE
NUMBER
OF
INSTANCES
IN
COLUMN
I
IN
FIGURE
IS
JUST
THE
SUM
OF
THE
NUMBER
OF
INSTANCES
IN
EACH
CELL
OF
THAT
COLUMN
WE
HAVE
CI
J
NIJ
AND
THEREFORE
FROM
AND
WE
HAVE
L
P
X
XI
P
X
XI
Y
YJ
J
WHICH
IS
THE
SUM
RULE
OF
PROBABILITY
NOTE
THAT
P
X
XI
IS
SOMETIMES
CALLED
THE
MARGINAL
PROBABILITY
BECAUSE
IT
IS
OBTAINED
BY
MARGINALIZING
OR
SUMMING
OUT
THE
OTHER
VARIABLES
IN
THIS
CASE
Y
IF
WE
CONSIDER
ONLY
THOSE
INSTANCES
FOR
WHICH
X
XI
THEN
THE
FRACTION
OF
SUCH
INSTANCES
FOR
WHICH
Y
YJ
IS
WRITTEN
P
Y
YJ
X
XI
AND
IS
CALLED
THE
CONDITIONAL
PROBABILITY
OF
Y
YJ
GIVEN
X
XI
IT
IS
OBTAINED
BY
FINDING
THE
FRACTION
OF
THOSE
POINTS
IN
COLUMN
I
THAT
FALL
IN
CELL
I
J
AND
HENCE
IS
GIVEN
BY
P
Y
Y
X
X
NIJ
J
I
CI
FROM
AND
WE
CAN
THEN
DERIVE
THE
FOLLOWING
RELATIONSHIP
P
X
X
Y
Y
NIJ
NIJ
CI
I
J
N
CI
N
P
Y
YJ
X
XI
P
X
XI
WHICH
IS
THE
PRODUCT
RULE
OF
PROBABILITY
SO
FAR
WE
HAVE
BEEN
QUITE
CAREFUL
TO
MAKE
A
DISTINCTION
BETWEEN
A
RANDOM
VARI
ABLE
SUCH
AS
THE
BOX
B
IN
THE
FRUIT
EXAMPLE
AND
THE
VALUES
THAT
THE
RANDOM
VARIABLE
CAN
TAKE
FOR
EXAMPLE
R
IF
THE
BOX
WERE
THE
RED
ONE
THUS
THE
PROBABILITY
THAT
B
TAKES
THE
VALUE
R
IS
DENOTED
P
B
R
ALTHOUGH
THIS
HELPS
TO
AVOID
AMBIGUITY
IT
LEADS
TO
A
RATHER
CUMBERSOME
NOTATION
AND
IN
MANY
CASES
THERE
WILL
BE
NO
NEED
FOR
SUCH
PEDANTRY
INSTEAD
WE
MAY
SIMPLY
WRITE
P
B
TO
DENOTE
A
DISTRIBUTION
OVER
THE
RAN
DOM
VARIABLE
B
OR
P
R
TO
DENOTE
THE
DISTRIBUTION
EVALUATED
FOR
THE
PARTICULAR
VALUE
R
PROVIDED
THAT
THE
INTERPRETATION
IS
CLEAR
FROM
THE
CONTEXT
WITH
THIS
MORE
COMPACT
NOTATION
WE
CAN
WRITE
THE
TWO
FUNDAMENTAL
RULES
OF
PROBABILITY
THEORY
IN
THE
FOLLOWING
FORM
THE
RULES
OF
PROBABILITY
SUM
RULE
P
X
P
X
Y
Y
PRODUCT
RULE
P
X
Y
P
Y
X
P
X
HERE
P
X
Y
IS
A
JOINT
PROBABILITY
AND
IS
VERBALIZED
AS
THE
PROBABILITY
OF
X
AND
Y
SIMILARLY
THE
QUANTITY
P
Y
X
IS
A
CONDITIONAL
PROBABILITY
AND
IS
VERBALIZED
AS
THE
PROBABILITY
OF
Y
GIVEN
X
WHEREAS
THE
QUANTITY
P
X
IS
A
MARGINAL
PROBABILITY
AND
IS
SIMPLY
THE
PROBABILITY
OF
X
THESE
TWO
SIMPLE
RULES
FORM
THE
BASIS
FOR
ALL
OF
THE
PROBABILISTIC
MACHINERY
THAT
WE
USE
THROUGHOUT
THIS
BOOK
FROM
THE
PRODUCT
RULE
TOGETHER
WITH
THE
SYMMETRY
PROPERTY
P
X
Y
P
Y
X
WE
IMMEDIATELY
OBTAIN
THE
FOLLOWING
RELATIONSHIP
BETWEEN
CONDITIONAL
PROBABILITIES
P
X
Y
P
Y
P
X
WHICH
IS
CALLED
BAYES
THEOREM
AND
WHICH
PLAYS
A
CENTRAL
ROLE
IN
PATTERN
RECOGNITION
AND
MACHINE
LEARNING
USING
THE
SUM
RULE
THE
DENOMINATOR
IN
BAYES
THEOREM
CAN
BE
EXPRESSED
IN
TERMS
OF
THE
QUANTITIES
APPEARING
IN
THE
NUMERATOR
P
X
P
X
Y
P
Y
Y
WE
CAN
VIEW
THE
DENOMINATOR
IN
BAYES
THEOREM
AS
BEING
THE
NORMALIZATION
CONSTANT
REQUIRED
TO
ENSURE
THAT
THE
SUM
OF
THE
CONDITIONAL
PROBABILITY
ON
THE
LEFT
HAND
SIDE
OF
OVER
ALL
VALUES
OF
Y
EQUALS
ONE
IN
FIGURE
WE
SHOW
A
SIMPLE
EXAMPLE
INVOLVING
A
JOINT
DISTRIBUTION
OVER
TWO
VARIABLES
TO
ILLUSTRATE
THE
CONCEPT
OF
MARGINAL
AND
CONDITIONAL
DISTRIBUTIONS
HERE
A
FINITE
SAMPLE
OF
N
DATA
POINTS
HAS
BEEN
DRAWN
FROM
THE
JOINT
DISTRIBUTION
AND
IS
SHOWN
IN
THE
TOP
LEFT
IN
THE
TOP
RIGHT
IS
A
HISTOGRAM
OF
THE
FRACTIONS
OF
DATA
POINTS
HAVING
EACH
OF
THE
TWO
VALUES
OF
Y
FROM
THE
DEFINITION
OF
PROBABILITY
THESE
FRACTIONS
WOULD
EQUAL
THE
CORRESPONDING
PROBABILITIES
P
Y
IN
THE
LIMIT
N
WE
CAN
VIEW
THE
HISTOGRAM
AS
A
SIMPLE
WAY
TO
MODEL
A
PROBABILITY
DISTRIBUTION
GIVEN
ONLY
A
FINITE
NUMBER
OF
POINTS
DRAWN
FROM
THAT
DISTRIBUTION
MODELLING
DISTRIBUTIONS
FROM
DATA
LIES
AT
THE
HEART
OF
STATISTICAL
PATTERN
RECOGNITION
AND
WILL
BE
EXPLORED
IN
GREAT
DETAIL
IN
THIS
BOOK
THE
REMAINING
TWO
PLOTS
IN
FIGURE
SHOW
THE
CORRESPONDING
HISTOGRAM
ESTIMATES
OF
P
X
AND
P
X
Y
LET
US
NOW
RETURN
TO
OUR
EXAMPLE
INVOLVING
BOXES
OF
FRUIT
FOR
THE
MOMENT
WE
SHALL
ONCE
AGAIN
BE
EXPLICIT
ABOUT
DISTINGUISHING
BETWEEN
THE
RANDOM
VARIABLES
AND
THEIR
INSTANTIATIONS
WE
HAVE
SEEN
THAT
THE
PROBABILITIES
OF
SELECTING
EITHER
THE
RED
OR
THE
BLUE
BOXES
ARE
GIVEN
BY
P
B
R
P
B
B
RESPECTIVELY
NOTE
THAT
THESE
SATISFY
P
B
R
P
B
B
NOW
SUPPOSE
THAT
WE
PICK
A
BOX
AT
RANDOM
AND
IT
TURNS
OUT
TO
BE
THE
BLUE
BOX
THEN
THE
PROBABILITY
OF
SELECTING
AN
APPLE
IS
JUST
THE
FRACTION
OF
APPLES
IN
THE
BLUE
BOX
WHICH
IS
AND
SO
P
F
A
B
B
IN
FACT
WE
CAN
WRITE
OUT
ALL
FOUR
CONDITIONAL
PROBABILITIES
FOR
THE
TYPE
OF
FRUIT
GIVEN
THE
SELECTED
BOX
P
F
A
B
R
P
F
O
B
R
P
F
A
B
B
P
F
O
B
B
Y
Y
P
X
Y
X
P
X
P
Y
P
X
Y
X
X
FIGURE
AN
ILLUSTRATION
OF
A
DISTRIBUTION
OVER
TWO
VARIABLES
X
WHICH
TAKES
POSSIBLE
VALUES
AND
Y
WHICH
TAKES
TWO
POSSIBLE
VALUES
THE
TOP
LEFT
FIGURE
SHOWS
A
SAMPLE
OF
POINTS
DRAWN
FROM
A
JOINT
PROBABILITY
DISTRI
BUTION
OVER
THESE
VARIABLES
THE
REMAINING
FIGURES
SHOW
HISTOGRAM
ESTIMATES
OF
THE
MARGINAL
DISTRIBUTIONS
P
X
AND
P
Y
AS
WELL
AS
THE
CONDITIONAL
DISTRIBUTION
P
X
Y
CORRESPONDING
TO
THE
BOTTOM
ROW
IN
THE
TOP
LEFT
FIGURE
AGAIN
NOTE
THAT
THESE
PROBABILITIES
ARE
NORMALIZED
SO
THAT
P
F
A
B
R
P
F
O
B
R
AND
SIMILARLY
P
F
A
B
B
P
F
O
B
B
WE
CAN
NOW
USE
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
TO
EVALUATE
THE
OVERALL
PROBABILITY
OF
CHOOSING
AN
APPLE
P
F
A
P
F
A
B
R
P
B
R
P
F
A
B
B
P
B
B
FROM
WHICH
IT
FOLLOWS
USING
THE
SUM
RULE
THAT
P
F
O
SUPPOSE
INSTEAD
WE
ARE
TOLD
THAT
A
PIECE
OF
FRUIT
HAS
BEEN
SELECTED
AND
IT
IS
AN
ORANGE
AND
WE
WOULD
LIKE
TO
KNOW
WHICH
BOX
IT
CAME
FROM
THIS
REQUIRES
THAT
WE
EVALUATE
THE
PROBABILITY
DISTRIBUTION
OVER
BOXES
CONDITIONED
ON
THE
IDENTITY
OF
THE
FRUIT
WHEREAS
THE
PROBABILITIES
IN
GIVE
THE
PROBABILITY
DISTRIBUTION
OVER
THE
FRUIT
CONDITIONED
ON
THE
IDENTITY
OF
THE
BOX
WE
CAN
SOLVE
THE
PROBLEM
OF
REVERSING
THE
CONDITIONAL
PROBABILITY
BY
USING
BAYES
THEOREM
TO
GIVE
P
B
R
F
O
P
F
O
B
R
P
B
R
P
F
O
FROM
THE
SUM
RULE
IT
THEN
FOLLOWS
THAT
P
B
B
F
O
WE
CAN
PROVIDE
AN
IMPORTANT
INTERPRETATION
OF
BAYES
THEOREM
AS
FOLLOWS
IF
WE
HAD
BEEN
ASKED
WHICH
BOX
HAD
BEEN
CHOSEN
BEFORE
BEING
TOLD
THE
IDENTITY
OF
THE
SELECTED
ITEM
OF
FRUIT
THEN
THE
MOST
COMPLETE
INFORMATION
WE
HAVE
AVAILABLE
IS
PROVIDED
BY
THE
PROBABILITY
P
B
WE
CALL
THIS
THE
PRIOR
PROBABILITY
BECAUSE
IT
IS
THE
PROBABILITY
AVAILABLE
BEFORE
WE
OBSERVE
THE
IDENTITY
OF
THE
FRUIT
ONCE
WE
ARE
TOLD
THAT
THE
FRUIT
IS
AN
ORANGE
WE
CAN
THEN
USE
BAYES
THEOREM
TO
COMPUTE
THE
PROBABILITY
P
B
F
WHICH
WE
SHALL
CALL
THE
POSTERIOR
PROBABILITY
BECAUSE
IT
IS
THE
PROBABILITY
OBTAINED
AFTER
WE
HAVE
OBSERVED
F
NOTE
THAT
IN
THIS
EXAMPLE
THE
PRIOR
PROBABILITY
OF
SELECTING
THE
RED
BOX
WAS
SO
THAT
WE
WERE
MORE
LIKELY
TO
SELECT
THE
BLUE
BOX
THAN
THE
RED
ONE
HOWEVER
ONCE
WE
HAVE
OBSERVED
THAT
THE
PIECE
OF
SELECTED
FRUIT
IS
AN
ORANGE
WE
FIND
THAT
THE
POSTERIOR
PROBABILITY
OF
THE
RED
BOX
IS
NOW
SO
THAT
IT
IS
NOW
MORE
LIKELY
THAT
THE
BOX
WE
SELECTED
WAS
IN
FACT
THE
RED
ONE
THIS
RESULT
ACCORDS
WITH
OUR
INTUITION
AS
THE
PROPORTION
OF
ORANGES
IS
MUCH
HIGHER
IN
THE
RED
BOX
THAN
IT
IS
IN
THE
BLUE
BOX
AND
SO
THE
OBSERVATION
THAT
THE
FRUIT
WAS
AN
ORANGE
PROVIDES
SIGNIFICANT
EVIDENCE
FAVOURING
THE
RED
BOX
IN
FACT
THE
EVIDENCE
IS
SUFFICIENTLY
STRONG
THAT
IT
OUTWEIGHS
THE
PRIOR
AND
MAKES
IT
MORE
LIKELY
THAT
THE
RED
BOX
WAS
CHOSEN
RATHER
THAN
THE
BLUE
ONE
FINALLY
WE
NOTE
THAT
IF
THE
JOINT
DISTRIBUTION
OF
TWO
VARIABLES
FACTORIZES
INTO
THE
PRODUCT
OF
THE
MARGINALS
SO
THAT
P
X
Y
P
X
P
Y
THEN
X
AND
Y
ARE
SAID
TO
BE
INDEPENDENT
FROM
THE
PRODUCT
RULE
WE
SEE
THAT
P
Y
X
P
Y
AND
SO
THE
CONDITIONAL
DISTRIBUTION
OF
Y
GIVEN
X
IS
INDEED
INDEPENDENT
OF
THE
VALUE
OF
X
FOR
INSTANCE
IN
OUR
BOXES
OF
FRUIT
EXAMPLE
IF
EACH
BOX
CONTAINED
THE
SAME
FRACTION
OF
APPLES
AND
ORANGES
THEN
P
F
B
P
F
SO
THAT
THE
PROBABILITY
OF
SELECTING
SAY
AN
APPLE
IS
INDEPENDENT
OF
WHICH
BOX
IS
CHOSEN
PROBABILITY
DENSITIES
AS
WELL
AS
CONSIDERING
PROBABILITIES
DEFINED
OVER
DISCRETE
SETS
OF
EVENTS
WE
ALSO
WISH
TO
CONSIDER
PROBABILITIES
WITH
RESPECT
TO
CONTINUOUS
VARIABLES
WE
SHALL
LIMIT
OURSELVES
TO
A
RELATIVELY
INFORMAL
DISCUSSION
IF
THE
PROBABILITY
OF
A
REAL
VALUED
VARIABLE
X
FALLING
IN
THE
INTERVAL
X
X
ΔX
IS
GIVEN
BY
P
X
ΔX
FOR
ΔX
THEN
P
X
IS
CALLED
THE
PROBABILITY
DENSITY
OVER
X
THIS
IS
ILLUSTRATED
IN
FIGURE
THE
PROBABILITY
THAT
X
WILL
LIE
IN
AN
INTERVAL
A
B
IS
THEN
GIVEN
BY
B
FIGURE
THE
CONCEPT
OF
PROBABILITY
FOR
DISCRETE
VARIABLES
CAN
BE
EX
TENDED
TO
THAT
OF
A
PROBABILITY
DENSITY
P
X
OVER
A
CONTINUOUS
VARIABLE
X
AND
IS
SUCH
THAT
THE
PROBABILITY
OF
X
LYING
IN
THE
INTER
VAL
X
X
ΔX
IS
GIVEN
BY
P
X
ΔX
FOR
ΔX
THE
PROBABILITY
DENSITY
CAN
BE
EXPRESSED
AS
THE
DERIVATIVE
OF
A
CUMULATIVE
DISTRI
BUTION
FUNCTION
P
X
ΔX
X
BECAUSE
PROBABILITIES
ARE
NONNEGATIVE
AND
BECAUSE
THE
VALUE
OF
X
MUST
LIE
SOME
WHERE
ON
THE
REAL
AXIS
THE
PROBABILITY
DENSITY
P
X
MUST
SATISFY
THE
TWO
CONDITIONS
P
X
P
X
DX
UNDER
A
NONLINEAR
CHANGE
OF
VARIABLE
A
PROBABILITY
DENSITY
TRANSFORMS
DIFFERENTLY
FROM
A
SIMPLE
FUNCTION
DUE
TO
THE
JACOBIAN
FACTOR
FOR
INSTANCE
IF
WE
CONSIDER
A
CHANGE
OF
VARIABLES
X
G
Y
THEN
A
FUNCTION
F
X
BECOMES
F
Y
F
G
Y
NOW
CONSIDER
A
PROBABILITY
DENSITY
PX
X
THAT
CORRESPONDS
TO
A
DENSITY
PY
Y
WITH
RESPECT
TO
THE
NEW
VARIABLE
Y
WHERE
THE
SUFFICES
DENOTE
THE
FACT
THAT
PX
X
AND
PY
Y
ARE
DIFFERENT
DENSITIES
OBSERVATIONS
FALLING
IN
THE
RANGE
X
X
ΔX
WILL
FOR
SMALL
VALUES
OF
ΔX
BE
TRANSFORMED
INTO
THE
RANGE
Y
Y
ΔY
WHERE
PX
X
ΔX
PY
Y
ΔY
AND
HENCE
PY
Y
PX
DX
X
L
DY
L
EXERCISE
PX
G
Y
GT
Y
ONE
CONSEQUENCE
OF
THIS
PROPERTY
IS
THAT
THE
CONCEPT
OF
THE
MAXIMUM
OF
A
PROBABILITY
DENSITY
IS
DEPENDENT
ON
THE
CHOICE
OF
VARIABLE
THE
PROBABILITY
THAT
X
LIES
IN
THE
INTERVAL
Z
IS
GIVEN
BY
THE
CUMULATIVE
DISTRIBUTION
FUNCTION
DEFINED
BY
Z
P
Z
P
X
DX
WHICH
SATISFIES
P
T
X
P
X
AS
SHOWN
IN
FIGURE
IF
WE
HAVE
SEVERAL
CONTINUOUS
VARIABLES
XD
DENOTED
COLLECTIVELY
BY
THE
VECTOR
X
THEN
WE
CAN
DEFINE
A
JOINT
PROBABILITY
DENSITY
P
X
P
XD
SUCH
THAT
THE
PROBABILITY
OF
X
FALLING
IN
AN
INFINITESIMAL
VOLUME
ΔX
CONTAINING
THE
POINT
X
IS
GIVEN
BY
P
X
ΔX
THIS
MULTIVARIATE
PROBABILITY
DENSITY
MUST
SATISFY
P
X
P
X
DX
IN
WHICH
THE
INTEGRAL
IS
TAKEN
OVER
THE
WHOLE
OF
X
SPACE
WE
CAN
ALSO
CONSIDER
JOINT
PROBABILITY
DISTRIBUTIONS
OVER
A
COMBINATION
OF
DISCRETE
AND
CONTINUOUS
VARIABLES
NOTE
THAT
IF
X
IS
A
DISCRETE
VARIABLE
THEN
P
X
IS
SOMETIMES
CALLED
A
PROBABILITY
MASS
FUNCTION
BECAUSE
IT
CAN
BE
REGARDED
AS
A
SET
OF
PROBABILITY
MASSES
CONCENTRATED
AT
THE
ALLOWED
VALUES
OF
X
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
AS
WELL
AS
BAYES
THEOREM
APPLY
EQUALLY
TO
THE
CASE
OF
PROBABILITY
DENSITIES
OR
TO
COMBINATIONS
OF
DISCRETE
AND
CON
TINUOUS
VARIABLES
FOR
INSTANCE
IF
X
AND
Y
ARE
TWO
REAL
VARIABLES
THEN
THE
SUM
AND
PRODUCT
RULES
TAKE
THE
FORM
P
X
P
X
Y
DY
P
X
Y
P
Y
X
P
X
A
FORMAL
JUSTIFICATION
OF
THE
SUM
AND
PRODUCT
RULES
FOR
CONTINUOUS
VARIABLES
FELLER
REQUIRES
A
BRANCH
OF
MATHEMATICS
CALLED
MEASURE
THEORY
AND
LIES
OUTSIDE
THE
SCOPE
OF
THIS
BOOK
ITS
VALIDITY
CAN
BE
SEEN
INFORMALLY
HOWEVER
BY
DIVIDING
EACH
REAL
VARIABLE
INTO
INTERVALS
OF
WIDTH
AND
CONSIDERING
THE
DISCRETE
PROBABILITY
DIS
TRIBUTION
OVER
THESE
INTERVALS
TAKING
THE
LIMIT
THEN
TURNS
SUMS
INTO
INTEGRALS
AND
GIVES
THE
DESIRED
RESULT
EXPECTATIONS
AND
COVARIANCES
ONE
OF
THE
MOST
IMPORTANT
OPERATIONS
INVOLVING
PROBABILITIES
IS
THAT
OF
FINDING
WEIGHTED
AVERAGES
OF
FUNCTIONS
THE
AVERAGE
VALUE
OF
SOME
FUNCTION
F
X
UNDER
A
PROBABILITY
DISTRIBUTION
P
X
IS
CALLED
THE
EXPECTATION
OF
F
X
AND
WILL
BE
DENOTED
BY
E
F
FOR
A
DISCRETE
DISTRIBUTION
IT
IS
GIVEN
BY
E
F
P
X
F
X
X
SO
THAT
THE
AVERAGE
IS
WEIGHTED
BY
THE
RELATIVE
PROBABILITIES
OF
THE
DIFFERENT
VALUES
OF
X
IN
THE
CASE
OF
CONTINUOUS
VARIABLES
EXPECTATIONS
ARE
EXPRESSED
IN
TERMS
OF
AN
INTEGRATION
WITH
RESPECT
TO
THE
CORRESPONDING
PROBABILITY
DENSITY
E
F
P
X
F
X
DX
IN
EITHER
CASE
IF
WE
ARE
GIVEN
A
FINITE
NUMBER
N
OF
POINTS
DRAWN
FROM
THE
PROBABILITY
DISTRIBUTION
OR
PROBABILITY
DENSITY
THEN
THE
EXPECTATION
CAN
BE
APPROXIMATED
AS
A
FINITE
SUM
OVER
THESE
POINTS
E
F
N
F
XN
N
WE
SHALL
MAKE
EXTENSIVE
USE
OF
THIS
RESULT
WHEN
WE
DISCUSS
SAMPLING
METHODS
IN
CHAPTER
THE
APPROXIMATION
IN
BECOMES
EXACT
IN
THE
LIMIT
N
SOMETIMES
WE
WILL
BE
CONSIDERING
EXPECTATIONS
OF
FUNCTIONS
OF
SEVERAL
VARIABLES
IN
WHICH
CASE
WE
CAN
USE
A
SUBSCRIPT
TO
INDICATE
WHICH
VARIABLE
IS
BEING
AVERAGED
OVER
SO
THAT
FOR
INSTANCE
EX
F
X
Y
DENOTES
THE
AVERAGE
OF
THE
FUNCTION
F
X
Y
WITH
RESPECT
TO
THE
DISTRIBUTION
OF
X
NOTE
THAT
EX
F
X
Y
WILL
BE
A
FUNCTION
OF
Y
WE
CAN
ALSO
CONSIDER
A
CONDITIONAL
EXPECTATION
WITH
RESPECT
TO
A
CONDITIONAL
DISTRIBUTION
SO
THAT
EXERCISE
EXERCISE
EX
F
Y
P
X
Y
F
X
X
WITH
AN
ANALOGOUS
DEFINITION
FOR
CONTINUOUS
VARIABLES
THE
VARIANCE
OF
F
X
IS
DEFINED
BY
VAR
F
E
F
X
E
F
X
AND
PROVIDES
A
MEASURE
OF
HOW
MUCH
VARIABILITY
THERE
IS
IN
F
X
AROUND
ITS
MEAN
VALUE
E
F
X
EXPANDING
OUT
THE
SQUARE
WE
SEE
THAT
THE
VARIANCE
CAN
ALSO
BE
WRITTEN
IN
TERMS
OF
THE
EXPECTATIONS
OF
F
X
AND
F
X
VAR
F
E
F
X
E
F
X
IN
PARTICULAR
WE
CAN
CONSIDER
THE
VARIANCE
OF
THE
VARIABLE
X
ITSELF
WHICH
IS
GIVEN
BY
VAR
X
E
E
X
FOR
TWO
RANDOM
VARIABLES
X
AND
Y
THE
COVARIANCE
IS
DEFINED
BY
COV
X
Y
EX
Y
X
E
X
Y
E
Y
EX
Y
XY
E
X
E
Y
WHICH
EXPRESSES
THE
EXTENT
TO
WHICH
X
AND
Y
VARY
TOGETHER
IF
X
AND
Y
ARE
INDEPEN
DENT
THEN
THEIR
COVARIANCE
VANISHES
IN
THE
CASE
OF
TWO
VECTORS
OF
RANDOM
VARIABLES
X
AND
Y
THE
COVARIANCE
IS
A
MATRIX
COV
X
Y
EX
Y
X
E
X
YT
E
YT
EX
Y
XYT
E
X
E
YT
IF
WE
CONSIDER
THE
COVARIANCE
OF
THE
COMPONENTS
OF
A
VECTOR
X
WITH
EACH
OTHER
THEN
WE
USE
A
SLIGHTLY
SIMPLER
NOTATION
COV
X
COV
X
X
BAYESIAN
PROBABILITIES
SO
FAR
IN
THIS
CHAPTER
WE
HAVE
VIEWED
PROBABILITIES
IN
TERMS
OF
THE
FREQUENCIES
OF
RANDOM
REPEATABLE
EVENTS
WE
SHALL
REFER
TO
THIS
AS
THE
CLASSICAL
OR
FREQUENTIST
INTERPRETATION
OF
PROBABILITY
NOW
WE
TURN
TO
THE
MORE
GENERAL
BAYESIAN
VIEW
IN
WHICH
PROBABILITIES
PROVIDE
A
QUANTIFICATION
OF
UNCERTAINTY
CONSIDER
AN
UNCERTAIN
EVENT
FOR
EXAMPLE
WHETHER
THE
MOON
WAS
ONCE
IN
ITS
OWN
ORBIT
AROUND
THE
SUN
OR
WHETHER
THE
ARCTIC
ICE
CAP
WILL
HAVE
DISAPPEARED
BY
THE
END
OF
THE
CENTURY
THESE
ARE
NOT
EVENTS
THAT
CAN
BE
REPEATED
NUMEROUS
TIMES
IN
ORDER
TO
DEFINE
A
NOTION
OF
PROBABILITY
AS
WE
DID
EARLIER
IN
THE
CONTEXT
OF
BOXES
OF
FRUIT
NEVERTHELESS
WE
WILL
GENERALLY
HAVE
SOME
IDEA
FOR
EXAMPLE
OF
HOW
QUICKLY
WE
THINK
THE
POLAR
ICE
IS
MELTING
IF
WE
NOW
OBTAIN
FRESH
EVIDENCE
FOR
INSTANCE
FROM
A
NEW
EARTH
OBSERVATION
SATELLITE
GATHERING
NOVEL
FORMS
OF
DIAGNOSTIC
INFORMATION
WE
MAY
REVISE
OUR
OPINION
ON
THE
RATE
OF
ICE
LOSS
OUR
ASSESSMENT
OF
SUCH
MATTERS
WILL
AFFECT
THE
ACTIONS
WE
TAKE
FOR
INSTANCE
THE
EXTENT
TO
WHICH
WE
ENDEAVOUR
TO
REDUCE
THE
EMISSION
OF
GREENHOUSE
GASSES
IN
SUCH
CIRCUMSTANCES
WE
WOULD
LIKE
TO
BE
ABLE
TO
QUANTIFY
OUR
EXPRESSION
OF
UNCERTAINTY
AND
MAKE
PRECISE
REVISIONS
OF
UNCERTAINTY
IN
THE
LIGHT
OF
NEW
EVIDENCE
AS
WELL
AS
SUBSEQUENTLY
TO
BE
ABLE
TO
TAKE
OPTIMAL
ACTIONS
OR
DECISIONS
AS
A
CONSEQUENCE
THIS
CAN
ALL
BE
ACHIEVED
THROUGH
THE
ELEGANT
AND
VERY
GENERAL
BAYESIAN
INTERPRETATION
OF
PROBABILITY
THE
USE
OF
PROBABILITY
TO
REPRESENT
UNCERTAINTY
HOWEVER
IS
NOT
AN
AD
HOC
CHOICE
BUT
IS
INEVITABLE
IF
WE
ARE
TO
RESPECT
COMMON
SENSE
WHILE
MAKING
RATIONAL
COHERENT
INFERENCES
FOR
INSTANCE
COX
SHOWED
THAT
IF
NUMERICAL
VALUES
ARE
USED
TO
REPRESENT
DEGREES
OF
BELIEF
THEN
A
SIMPLE
SET
OF
AXIOMS
ENCODING
COMMON
SENSE
PROPERTIES
OF
SUCH
BELIEFS
LEADS
UNIQUELY
TO
A
SET
OF
RULES
FOR
MANIPULATING
DEGREES
OF
BELIEF
THAT
ARE
EQUIVALENT
TO
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
THIS
PROVIDED
THE
FIRST
RIGOROUS
PROOF
THAT
PROBABILITY
THEORY
COULD
BE
REGARDED
AS
AN
EXTENSION
OF
BOOLEAN
LOGIC
TO
SITUATIONS
INVOLVING
UNCERTAINTY
JAYNES
NUMEROUS
OTHER
AUTHORS
HAVE
PROPOSED
DIFFERENT
SETS
OF
PROPERTIES
OR
AXIOMS
THAT
SUCH
MEASURES
OF
UNCERTAINTY
SHOULD
SATISFY
RAMSEY
GOOD
SAVAGE
DEFINETTI
LINDLEY
IN
EACH
CASE
THE
RESULTING
NUMERICAL
QUANTITIES
BEHAVE
PRE
CISELY
ACCORDING
TO
THE
RULES
OF
PROBABILITY
IT
IS
THEREFORE
NATURAL
TO
REFER
TO
THESE
QUANTITIES
AS
BAYESIAN
PROBABILITIES
IN
THE
FIELD
OF
PATTERN
RECOGNITION
TOO
IT
IS
HELPFUL
TO
HAVE
A
MORE
GENERAL
NO
THOMAS
BAYES
THOMAS
BAYES
WAS
BORN
IN
TUN
BRIDGE
WELLS
AND
WAS
A
CLERGYMAN
AS
WELL
AS
AN
AMATEUR
SCIENTIST
AND
A
MATHEMATICIAN
HE
STUDIED
LOGIC
AND
THEOLOGY
AT
EDINBURGH
UNIVER
SITY
AND
WAS
ELECTED
FELLOW
OF
THE
ROYAL
SOCIETY
IN
DURING
THE
CENTURY
IS
SUES
REGARDING
PROBABILITY
AROSE
IN
CONNECTION
WITH
GAMBLING
AND
WITH
THE
NEW
CONCEPT
OF
INSURANCE
ONE
PARTICULARLY
IMPORTANT
PROBLEM
CONCERNED
SO
CALLED
IN
VERSE
PROBABILITY
A
SOLUTION
WAS
PROPOSED
BY
THOMAS
BAYES
IN
HIS
PAPER
ESSAY
TOWARDS
SOLVING
A
PROBLEM
IN
THE
DOCTRINE
OF
CHANCES
WHICH
WAS
PUBLISHED
IN
SOME
THREE
YEARS
AFTER
HIS
DEATH
IN
THE
PHILO
SOPHICAL
TRANSACTIONS
OF
THE
ROYAL
SOCIETY
IN
FACT
BAYES
ONLY
FORMULATED
HIS
THEORY
FOR
THE
CASE
OF
A
UNI
FORM
PRIOR
AND
IT
WAS
PIERRE
SIMON
LAPLACE
WHO
INDE
PENDENTLY
REDISCOVERED
THE
THEORY
IN
GENERAL
FORM
AND
WHO
DEMONSTRATED
ITS
BROAD
APPLICABILITY
TION
OF
PROBABILITY
CONSIDER
THE
EXAMPLE
OF
POLYNOMIAL
CURVE
FITTING
DISCUSSED
IN
SECTION
IT
SEEMS
REASONABLE
TO
APPLY
THE
FREQUENTIST
NOTION
OF
PROBABILITY
TO
THE
RANDOM
VALUES
OF
THE
OBSERVED
VARIABLES
TN
HOWEVER
WE
WOULD
LIKE
TO
ADDRESS
AND
QUANTIFY
THE
UNCERTAINTY
THAT
SURROUNDS
THE
APPROPRIATE
CHOICE
FOR
THE
MODEL
PARAM
ETERS
W
WE
SHALL
SEE
THAT
FROM
A
BAYESIAN
PERSPECTIVE
WE
CAN
USE
THE
MACHINERY
OF
PROBABILITY
THEORY
TO
DESCRIBE
THE
UNCERTAINTY
IN
MODEL
PARAMETERS
SUCH
AS
W
OR
INDEED
IN
THE
CHOICE
OF
MODEL
ITSELF
BAYES
THEOREM
NOW
ACQUIRES
A
NEW
SIGNIFICANCE
RECALL
THAT
IN
THE
BOXES
OF
FRUIT
EXAMPLE
THE
OBSERVATION
OF
THE
IDENTITY
OF
THE
FRUIT
PROVIDED
RELEVANT
INFORMATION
THAT
ALTERED
THE
PROBABILITY
THAT
THE
CHOSEN
BOX
WAS
THE
RED
ONE
IN
THAT
EXAMPLE
BAYES
THEOREM
WAS
USED
TO
CONVERT
A
PRIOR
PROBABILITY
INTO
A
POSTERIOR
PROBABILITY
BY
INCORPORATING
THE
EVIDENCE
PROVIDED
BY
THE
OBSERVED
DATA
AS
WE
SHALL
SEE
IN
DETAIL
LATER
WE
CAN
ADOPT
A
SIMILAR
APPROACH
WHEN
MAKING
INFERENCES
ABOUT
QUANTITIES
SUCH
AS
THE
PARAMETERS
W
IN
THE
POLYNOMIAL
CURVE
FITTING
EXAMPLE
WE
CAPTURE
OUR
ASSUMPTIONS
ABOUT
W
BEFORE
OBSERVING
THE
DATA
IN
THE
FORM
OF
A
PRIOR
PROBABILITY
DISTRIBUTION
P
W
THE
EFFECT
OF
THE
OBSERVED
DATA
TN
IS
EXPRESSED
THROUGH
THE
CONDITIONAL
PROBABILITY
P
W
AND
WE
SHALL
SEE
LATER
IN
SECTION
HOW
THIS
CAN
BE
REPRESENTED
EXPLICITLY
BAYES
THEOREM
WHICH
TAKES
THE
FORM
W
P
D
W
P
W
P
D
THEN
ALLOWS
US
TO
EVALUATE
THE
UNCERTAINTY
IN
W
AFTER
WE
HAVE
OBSERVED
IN
THE
FORM
OF
THE
POSTERIOR
PROBABILITY
P
W
THE
QUANTITY
P
W
ON
THE
RIGHT
HAND
SIDE
OF
BAYES
THEOREM
IS
EVALUATED
FOR
THE
OBSERVED
DATA
SET
AND
CAN
BE
VIEWED
AS
A
FUNCTION
OF
THE
PARAMETER
VECTOR
W
IN
WHICH
CASE
IT
IS
CALLED
THE
LIKELIHOOD
FUNCTION
IT
EXPRESSES
HOW
PROBABLE
THE
OBSERVED
DATA
SET
IS
FOR
DIFFERENT
SETTINGS
OF
THE
PARAMETER
VECTOR
W
NOTE
THAT
THE
LIKELIHOOD
IS
NOT
A
PROBABILITY
DISTRIBUTION
OVER
W
AND
ITS
INTEGRAL
WITH
RESPECT
TO
W
DOES
NOT
NECESSARILY
EQUAL
ONE
GIVEN
THIS
DEFINITION
OF
LIKELIHOOD
WE
CAN
STATE
BAYES
THEOREM
IN
WORDS
POSTERIOR
LIKELIHOOD
PRIOR
WHERE
ALL
OF
THESE
QUANTITIES
ARE
VIEWED
AS
FUNCTIONS
OF
W
THE
DENOMINATOR
IN
IS
THE
NORMALIZATION
CONSTANT
WHICH
ENSURES
THAT
THE
POSTERIOR
DISTRIBUTION
ON
THE
LEFT
HAND
SIDE
IS
A
VALID
PROBABILITY
DENSITY
AND
INTEGRATES
TO
ONE
INDEED
INTEGRATING
BOTH
SIDES
OF
WITH
RESPECT
TO
W
WE
CAN
EXPRESS
THE
DENOMINATOR
IN
BAYES
THEOREM
IN
TERMS
OF
THE
PRIOR
DISTRIBUTION
AND
THE
LIKELIHOOD
FUNCTION
P
D
P
D
W
P
W
DW
IN
BOTH
THE
BAYESIAN
AND
FREQUENTIST
PARADIGMS
THE
LIKELIHOOD
FUNCTION
P
W
PLAYS
A
CENTRAL
ROLE
HOWEVER
THE
MANNER
IN
WHICH
IT
IS
USED
IS
FUNDAMENTALLY
DIF
FERENT
IN
THE
TWO
APPROACHES
IN
A
FREQUENTIST
SETTING
W
IS
CONSIDERED
TO
BE
A
FIXED
PARAMETER
WHOSE
VALUE
IS
DETERMINED
BY
SOME
FORM
OF
ESTIMATOR
AND
ERROR
BARS
SECTION
SECTION
SECTION
ON
THIS
ESTIMATE
ARE
OBTAINED
BY
CONSIDERING
THE
DISTRIBUTION
OF
POSSIBLE
DATA
SETS
BY
CONTRAST
FROM
THE
BAYESIAN
VIEWPOINT
THERE
IS
ONLY
A
SINGLE
DATA
SET
NAMELY
THE
ONE
THAT
IS
ACTUALLY
OBSERVED
AND
THE
UNCERTAINTY
IN
THE
PARAMETERS
IS
EXPRESSED
THROUGH
A
PROBABILITY
DISTRIBUTION
OVER
W
A
WIDELY
USED
FREQUENTIST
ESTIMATOR
IS
MAXIMUM
LIKELIHOOD
IN
WHICH
W
IS
SET
TO
THE
VALUE
THAT
MAXIMIZES
THE
LIKELIHOOD
FUNCTION
P
W
THIS
CORRESPONDS
TO
CHOOSING
THE
VALUE
OF
W
FOR
WHICH
THE
PROBABILITY
OF
THE
OBSERVED
DATA
SET
IS
MAXI
MIZED
IN
THE
MACHINE
LEARNING
LITERATURE
THE
NEGATIVE
LOG
OF
THE
LIKELIHOOD
FUNCTION
IS
CALLED
AN
ERROR
FUNCTION
BECAUSE
THE
NEGATIVE
LOGARITHM
IS
A
MONOTONICALLY
DE
CREASING
FUNCTION
MAXIMIZING
THE
LIKELIHOOD
IS
EQUIVALENT
TO
MINIMIZING
THE
ERROR
ONE
APPROACH
TO
DETERMINING
FREQUENTIST
ERROR
BARS
IS
THE
BOOTSTRAP
EFRON
HASTIE
ET
AL
IN
WHICH
MULTIPLE
DATA
SETS
ARE
CREATED
AS
FOLLOWS
SUPPOSE
OUR
ORIGINAL
DATA
SET
CONSISTS
OF
N
DATA
POINTS
X
XN
WE
CAN
CREATE
A
NEW
DATA
SET
XB
BY
DRAWING
N
POINTS
AT
RANDOM
FROM
X
WITH
REPLACEMENT
SO
THAT
SOME
POINTS
IN
X
MAY
BE
REPLICATED
IN
XB
WHEREAS
OTHER
POINTS
IN
X
MAY
BE
ABSENT
FROM
XB
THIS
PROCESS
CAN
BE
REPEATED
L
TIMES
TO
GENERATE
L
DATA
SETS
EACH
OF
SIZE
N
AND
EACH
OBTAINED
BY
SAMPLING
FROM
THE
ORIGINAL
DATA
SET
X
THE
STATISTICAL
ACCURACY
OF
PARAMETER
ESTIMATES
CAN
THEN
BE
EVALUATED
BY
LOOKING
AT
THE
VARIABILITY
OF
PREDICTIONS
BETWEEN
THE
DIFFERENT
BOOTSTRAP
DATA
SETS
ONE
ADVANTAGE
OF
THE
BAYESIAN
VIEWPOINT
IS
THAT
THE
INCLUSION
OF
PRIOR
KNOWL
EDGE
ARISES
NATURALLY
SUPPOSE
FOR
INSTANCE
THAT
A
FAIR
LOOKING
COIN
IS
TOSSED
THREE
TIMES
AND
LANDS
HEADS
EACH
TIME
A
CLASSICAL
MAXIMUM
LIKELIHOOD
ESTIMATE
OF
THE
PROBABILITY
OF
LANDING
HEADS
WOULD
GIVE
IMPLYING
THAT
ALL
FUTURE
TOSSES
WILL
LAND
HEADS
BY
CONTRAST
A
BAYESIAN
APPROACH
WITH
ANY
REASONABLE
PRIOR
WILL
LEAD
TO
A
MUCH
LESS
EXTREME
CONCLUSION
THERE
HAS
BEEN
MUCH
CONTROVERSY
AND
DEBATE
ASSOCIATED
WITH
THE
RELATIVE
MER
ITS
OF
THE
FREQUENTIST
AND
BAYESIAN
PARADIGMS
WHICH
HAVE
NOT
BEEN
HELPED
BY
THE
FACT
THAT
THERE
IS
NO
UNIQUE
FREQUENTIST
OR
EVEN
BAYESIAN
VIEWPOINT
FOR
INSTANCE
ONE
COMMON
CRITICISM
OF
THE
BAYESIAN
APPROACH
IS
THAT
THE
PRIOR
DISTRIBUTION
IS
OF
TEN
SELECTED
ON
THE
BASIS
OF
MATHEMATICAL
CONVENIENCE
RATHER
THAN
AS
A
REFLECTION
OF
ANY
PRIOR
BELIEFS
EVEN
THE
SUBJECTIVE
NATURE
OF
THE
CONCLUSIONS
THROUGH
THEIR
DE
PENDENCE
ON
THE
CHOICE
OF
PRIOR
IS
SEEN
BY
SOME
AS
A
SOURCE
OF
DIFFICULTY
REDUCING
THE
DEPENDENCE
ON
THE
PRIOR
IS
ONE
MOTIVATION
FOR
SO
CALLED
NONINFORMATIVE
PRIORS
HOWEVER
THESE
LEAD
TO
DIFFICULTIES
WHEN
COMPARING
DIFFERENT
MODELS
AND
INDEED
BAYESIAN
METHODS
BASED
ON
POOR
CHOICES
OF
PRIOR
CAN
GIVE
POOR
RESULTS
WITH
HIGH
CONFIDENCE
FREQUENTIST
EVALUATION
METHODS
OFFER
SOME
PROTECTION
FROM
SUCH
PROB
LEMS
AND
TECHNIQUES
SUCH
AS
CROSS
VALIDATION
REMAIN
USEFUL
IN
AREAS
SUCH
AS
MODEL
COMPARISON
THIS
BOOK
PLACES
A
STRONG
EMPHASIS
ON
THE
BAYESIAN
VIEWPOINT
REFLECTING
THE
HUGE
GROWTH
IN
THE
PRACTICAL
IMPORTANCE
OF
BAYESIAN
METHODS
IN
THE
PAST
FEW
YEARS
WHILE
ALSO
DISCUSSING
USEFUL
FREQUENTIST
CONCEPTS
AS
REQUIRED
ALTHOUGH
THE
BAYESIAN
FRAMEWORK
HAS
ITS
ORIGINS
IN
THE
CENTURY
THE
PRAC
TICAL
APPLICATION
OF
BAYESIAN
METHODS
WAS
FOR
A
LONG
TIME
SEVERELY
LIMITED
BY
THE
DIFFICULTIES
IN
CARRYING
THROUGH
THE
FULL
BAYESIAN
PROCEDURE
PARTICULARLY
THE
NEED
TO
MARGINALIZE
SUM
OR
INTEGRATE
OVER
THE
WHOLE
OF
PARAMETER
SPACE
WHICH
AS
WE
SHALL
SEE
IS
REQUIRED
IN
ORDER
TO
MAKE
PREDICTIONS
OR
TO
COMPARE
DIFFERENT
MODELS
THE
DEVELOPMENT
OF
SAMPLING
METHODS
SUCH
AS
MARKOV
CHAIN
MONTE
CARLO
DISCUSSED
IN
CHAPTER
ALONG
WITH
DRAMATIC
IMPROVEMENTS
IN
THE
SPEED
AND
MEMORY
CAPACITY
OF
COMPUTERS
OPENED
THE
DOOR
TO
THE
PRACTICAL
USE
OF
BAYESIAN
TECHNIQUES
IN
AN
IM
PRESSIVE
RANGE
OF
PROBLEM
DOMAINS
MONTE
CARLO
METHODS
ARE
VERY
FLEXIBLE
AND
CAN
BE
APPLIED
TO
A
WIDE
RANGE
OF
MODELS
HOWEVER
THEY
ARE
COMPUTATIONALLY
INTENSIVE
AND
HAVE
MAINLY
BEEN
USED
FOR
SMALL
SCALE
PROBLEMS
MORE
RECENTLY
HIGHLY
EFFICIENT
DETERMINISTIC
APPROXIMATION
SCHEMES
SUCH
AS
VARIATIONAL
BAYES
AND
EXPECTATION
PROPAGATION
DISCUSSED
IN
CHAPTER
HAVE
BEEN
DEVELOPED
THESE
OFFER
A
COMPLEMENTARY
ALTERNATIVE
TO
SAMPLING
METHODS
AND
HAVE
ALLOWED
BAYESIAN
TECHNIQUES
TO
BE
USED
IN
LARGE
SCALE
APPLICATIONS
BLEI
ET
AL
THE
GAUSSIAN
DISTRIBUTION
WE
SHALL
DEVOTE
THE
WHOLE
OF
CHAPTER
TO
A
STUDY
OF
VARIOUS
PROBABILITY
DIS
TRIBUTIONS
AND
THEIR
KEY
PROPERTIES
IT
IS
CONVENIENT
HOWEVER
TO
INTRODUCE
HERE
ONE
OF
THE
MOST
IMPORTANT
PROBABILITY
DISTRIBUTIONS
FOR
CONTINUOUS
VARIABLES
CALLED
THE
NORMAL
OR
GAUSSIAN
DISTRIBUTION
WE
SHALL
MAKE
EXTENSIVE
USE
OF
THIS
DISTRIBUTION
IN
THE
REMAINDER
OF
THIS
CHAPTER
AND
INDEED
THROUGHOUT
MUCH
OF
THE
BOOK
FOR
THE
CASE
OF
A
SINGLE
REAL
VALUED
VARIABLE
X
THE
GAUSSIAN
DISTRIBUTION
IS
DE
FINED
BY
N
X
Μ
EXP
X
Μ
EXERCISE
WHICH
IS
GOVERNED
BY
TWO
PARAMETERS
Μ
CALLED
THE
MEAN
AND
CALLED
THE
VARI
ANCE
THE
SQUARE
ROOT
OF
THE
VARIANCE
GIVEN
BY
Σ
IS
CALLED
THE
STANDARD
DEVIATION
AND
THE
RECIPROCAL
OF
THE
VARIANCE
WRITTEN
AS
Β
IS
CALLED
THE
PRECISION
WE
SHALL
SEE
THE
MOTIVATION
FOR
THESE
TERMS
SHORTLY
FIGURE
SHOWS
A
PLOT
OF
THE
GAUSSIAN
DISTRIBUTION
FROM
THE
FORM
OF
WE
SEE
THAT
THE
GAUSSIAN
DISTRIBUTION
SATISFIES
N
X
Μ
ALSO
IT
IS
STRAIGHTFORWARD
TO
SHOW
THAT
THE
GAUSSIAN
IS
NORMALIZED
SO
THAT
PIERRE
SIMON
LAPLACE
IT
IS
SAID
THAT
LAPLACE
WAS
SERI
OUSLY
LACKING
IN
MODESTY
AND
AT
ONE
POINT
DECLARED
HIMSELF
TO
BE
THE
BEST
MATHEMATICIAN
IN
FRANCE
AT
THE
TIME
A
CLAIM
THAT
WAS
ARGUABLY
TRUE
AS
WELL
AS
BEING
PROLIFIC
IN
MATHE
MATICS
HE
ALSO
MADE
NUMEROUS
CONTRIBUTIONS
TO
AS
TRONOMY
INCLUDING
THE
NEBULAR
HYPOTHESIS
BY
WHICH
THE
EARTH
IS
THOUGHT
TO
HAVE
FORMED
FROM
THE
CONDENSA
TION
AND
COOLING
OF
A
LARGE
ROTATING
DISK
OF
GAS
AND
DUST
IN
HE
PUBLISHED
THE
FIRST
EDITION
OF
THE
ORIE
ANALYTIQUE
DES
PROBABILITE
IN
WHICH
LAPLACE
STATES
THAT
PROBABILITY
THEORY
IS
NOTHING
BUT
COMMON
SENSE
REDUCED
TO
CALCULATION
THIS
WORK
INCLUDED
A
DISCUS
SION
OF
THE
INVERSE
PROBABILITY
CALCULATION
LATER
TERMED
BAYES
THEOREM
BY
POINCARE
WHICH
HE
USED
TO
SOLVE
PROBLEMS
IN
LIFE
EXPECTANCY
JURISPRUDENCE
PLANETARY
MASSES
TRIANGULATION
AND
ERROR
ESTIMATION
FIGURE
PLOT
OF
THE
UNIVARIATE
GAUSSIAN
SHOWING
THE
MEAN
Μ
AND
THE
STANDARD
DEVIATION
Σ
Μ
X
EXERCISE
EXERCISE
N
X
Μ
DX
THUS
SATISFIES
THE
TWO
REQUIREMENTS
FOR
A
VALID
PROBABILITY
DENSITY
WE
CAN
READILY
FIND
EXPECTATIONS
OF
FUNCTIONS
OF
X
UNDER
THE
GAUSSIAN
DISTRIBU
TION
IN
PARTICULAR
THE
AVERAGE
VALUE
OF
X
IS
GIVEN
BY
E
X
N
X
Μ
X
DX
Μ
BECAUSE
THE
PARAMETER
Μ
REPRESENTS
THE
AVERAGE
VALUE
OF
X
UNDER
THE
DISTRIBUTION
IT
IS
REFERRED
TO
AS
THE
MEAN
SIMILARLY
FOR
THE
SECOND
ORDER
MOMENT
E
N
X
Μ
DX
FROM
AND
IT
FOLLOWS
THAT
THE
VARIANCE
OF
X
IS
GIVEN
BY
VAR
X
E
E
X
AND
HENCE
IS
REFERRED
TO
AS
THE
VARIANCE
PARAMETER
THE
MAXIMUM
OF
A
DISTRIBUTION
IS
KNOWN
AS
ITS
MODE
FOR
A
GAUSSIAN
THE
MODE
COINCIDES
WITH
THE
MEAN
WE
ARE
ALSO
INTERESTED
IN
THE
GAUSSIAN
DISTRIBUTION
DEFINED
OVER
A
D
DIMENSIONAL
VECTOR
X
OF
CONTINUOUS
VARIABLES
WHICH
IS
GIVEN
BY
N
X
Μ
Σ
EXP
X
Μ
TΣ
X
Μ
D
Σ
WHERE
THE
D
DIMENSIONAL
VECTOR
Μ
IS
CALLED
THE
MEAN
THE
D
D
MATRIX
Σ
IS
CALLED
THE
COVARIANCE
AND
Σ
DENOTES
THE
DETERMINANT
OF
Σ
WE
SHALL
MAKE
USE
OF
THE
MULTIVARIATE
GAUSSIAN
DISTRIBUTION
BRIEFLY
IN
THIS
CHAPTER
ALTHOUGH
ITS
PROPERTIES
WILL
BE
STUDIED
IN
DETAIL
IN
SECTION
FIGURE
ILLUSTRATION
OF
THE
LIKELIHOOD
FUNCTION
FOR
A
GAUSSIAN
DISTRIBUTION
SHOWN
BY
THE
RED
CURVE
HERE
THE
BLACK
POINTS
DE
NOTE
A
DATA
SET
OF
VALUES
XN
AND
THE
LIKELIHOOD
FUNCTION
GIVEN
BY
CORRESPONDS
TO
THE
PRODUCT
OF
THE
BLUE
VALUES
MAXIMIZING
THE
LIKELIHOOD
IN
VOLVES
ADJUSTING
THE
MEAN
AND
VARI
ANCE
OF
THE
GAUSSIAN
SO
AS
TO
MAXI
MIZE
THIS
PRODUCT
P
X
XN
X
NOW
SUPPOSE
THAT
WE
HAVE
A
DATA
SET
OF
OBSERVATIONS
X
XN
T
REP
RESENTING
N
OBSERVATIONS
OF
THE
SCALAR
VARIABLE
X
NOTE
THAT
WE
ARE
USING
THE
TYPE
FACE
X
TO
DISTINGUISH
THIS
FROM
A
SINGLE
OBSERVATION
OF
THE
VECTOR
VALUED
VARIABLE
XD
T
WHICH
WE
DENOTE
BY
X
WE
SHALL
SUPPOSE
THAT
THE
OBSERVATIONS
ARE
DRAWN
INDEPENDENTLY
FROM
A
GAUSSIAN
DISTRIBUTION
WHOSE
MEAN
Μ
AND
VARIANCE
ARE
UNKNOWN
AND
WE
WOULD
LIKE
TO
DETERMINE
THESE
PARAMETERS
FROM
THE
DATA
SET
DATA
POINTS
THAT
ARE
DRAWN
INDEPENDENTLY
FROM
THE
SAME
DISTRIBUTION
ARE
SAID
TO
BE
INDEPENDENT
AND
IDENTICALLY
DISTRIBUTED
WHICH
IS
OFTEN
ABBREVIATED
TO
I
I
D
WE
HAVE
SEEN
THAT
THE
JOINT
PROBABILITY
OF
TWO
INDEPENDENT
EVENTS
IS
GIVEN
BY
THE
PRODUCT
OF
THE
MARGINAL
PROBABILITIES
FOR
EACH
EVENT
SEPARATELY
BECAUSE
OUR
DATA
SET
X
IS
I
I
D
WE
CAN
THEREFORE
WRITE
THE
PROBABILITY
OF
THE
DATA
SET
GIVEN
Μ
AND
IN
THE
FORM
SECTION
N
P
X
Μ
N
XN
Μ
N
WHEN
VIEWED
AS
A
FUNCTION
OF
Μ
AND
THIS
IS
THE
LIKELIHOOD
FUNCTION
FOR
THE
GAUS
SIAN
AND
IS
INTERPRETED
DIAGRAMMATICALLY
IN
FIGURE
ONE
COMMON
CRITERION
FOR
DETERMINING
THE
PARAMETERS
IN
A
PROBABILITY
DISTRIBU
TION
USING
AN
OBSERVED
DATA
SET
IS
TO
FIND
THE
PARAMETER
VALUES
THAT
MAXIMIZE
THE
LIKELIHOOD
FUNCTION
THIS
MIGHT
SEEM
LIKE
A
STRANGE
CRITERION
BECAUSE
FROM
OUR
FORE
GOING
DISCUSSION
OF
PROBABILITY
THEORY
IT
WOULD
SEEM
MORE
NATURAL
TO
MAXIMIZE
THE
PROBABILITY
OF
THE
PARAMETERS
GIVEN
THE
DATA
NOT
THE
PROBABILITY
OF
THE
DATA
GIVEN
THE
PARAMETERS
IN
FACT
THESE
TWO
CRITERIA
ARE
RELATED
AS
WE
SHALL
DISCUSS
IN
THE
CONTEXT
OF
CURVE
FITTING
FOR
THE
MOMENT
HOWEVER
WE
SHALL
DETERMINE
VALUES
FOR
THE
UNKNOWN
PARAME
TERS
Μ
AND
IN
THE
GAUSSIAN
BY
MAXIMIZING
THE
LIKELIHOOD
FUNCTION
IN
PRAC
TICE
IT
IS
MORE
CONVENIENT
TO
MAXIMIZE
THE
LOG
OF
THE
LIKELIHOOD
FUNCTION
BECAUSE
THE
LOGARITHM
IS
A
MONOTONICALLY
INCREASING
FUNCTION
OF
ITS
ARGUMENT
MAXIMIZATION
OF
THE
LOG
OF
A
FUNCTION
IS
EQUIVALENT
TO
MAXIMIZATION
OF
THE
FUNCTION
ITSELF
TAKING
THE
LOG
NOT
ONLY
SIMPLIFIES
THE
SUBSEQUENT
MATHEMATICAL
ANALYSIS
BUT
IT
ALSO
HELPS
NUMERICALLY
BECAUSE
THE
PRODUCT
OF
A
LARGE
NUMBER
OF
SMALL
PROBABILITIES
CAN
EASILY
UNDERFLOW
THE
NUMERICAL
PRECISION
OF
THE
COMPUTER
AND
THIS
IS
RESOLVED
BY
COMPUTING
INSTEAD
THE
SUM
OF
THE
LOG
PROBABILITIES
FROM
AND
THE
LOG
LIKELIHOOD
FUNCTION
CAN
BE
WRITTEN
IN
THE
FORM
LN
P
X
Μ
X
Μ
LN
Σ
LN
EXERCISE
MAXIMIZING
WITH
RESPECT
TO
Μ
WE
OBTAIN
THE
MAXIMUM
LIKELIHOOD
SOLUTION
GIVEN
BY
ΜML
N
N
XN
N
WHICH
IS
THE
SAMPLE
MEAN
I
E
THE
MEAN
OF
THE
OBSERVED
VALUES
XN
SIMILARLY
MAXIMIZING
WITH
RESPECT
TO
WE
OBTAIN
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
VARIANCE
IN
THE
FORM
ML
N
N
XN
N
ΜML
SECTION
WHICH
IS
THE
SAMPLE
VARIANCE
MEASURED
WITH
RESPECT
TO
THE
SAMPLE
MEAN
ΜML
NOTE
THAT
WE
ARE
PERFORMING
A
JOINT
MAXIMIZATION
OF
WITH
RESPECT
TO
Μ
AND
BUT
IN
THE
CASE
OF
THE
GAUSSIAN
DISTRIBUTION
THE
SOLUTION
FOR
Μ
DECOUPLES
FROM
THAT
FOR
SO
THAT
WE
CAN
FIRST
EVALUATE
AND
THEN
SUBSEQUENTLY
USE
THIS
RESULT
TO
EVALUATE
LATER
IN
THIS
CHAPTER
AND
ALSO
IN
SUBSEQUENT
CHAPTERS
WE
SHALL
HIGHLIGHT
THE
SIG
NIFICANT
LIMITATIONS
OF
THE
MAXIMUM
LIKELIHOOD
APPROACH
HERE
WE
GIVE
AN
INDICATION
OF
THE
PROBLEM
IN
THE
CONTEXT
OF
OUR
SOLUTIONS
FOR
THE
MAXIMUM
LIKELIHOOD
PARAM
ETER
SETTINGS
FOR
THE
UNIVARIATE
GAUSSIAN
DISTRIBUTION
IN
PARTICULAR
WE
SHALL
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
APPROACH
SYSTEMATICALLY
UNDERESTIMATES
THE
VARIANCE
OF
THE
DISTRIBUTION
THIS
IS
AN
EXAMPLE
OF
A
PHENOMENON
CALLED
BIAS
AND
IS
RELATED
TO
THE
PROBLEM
OF
OVER
FITTING
ENCOUNTERED
IN
THE
CONTEXT
OF
POLYNOMIAL
CURVE
FITTING
WE
FIRST
NOTE
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTIONS
ΜML
AND
ARE
FUNCTIONS
OF
EXERCISE
THE
DATA
SET
VALUES
XN
CONSIDER
THE
EXPECTATIONS
OF
THESE
QUANTITIES
WITH
RESPECT
TO
THE
DATA
SET
VALUES
WHICH
THEMSELVES
COME
FROM
A
GAUSSIAN
DISTRIBUTION
WITH
PARAMETERS
Μ
AND
IT
IS
STRAIGHTFORWARD
TO
SHOW
THAT
E
ΜML
Μ
E
N
SO
THAT
ON
AVERAGE
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
WILL
OBTAIN
THE
CORRECT
MEAN
BUT
WILL
UNDERESTIMATE
THE
TRUE
VARIANCE
BY
A
FACTOR
N
N
THE
INTUITION
BEHIND
THIS
RESULT
IS
GIVEN
BY
FIGURE
FROM
IT
FOLLOWS
THAT
THE
FOLLOWING
ESTIMATE
FOR
THE
VARIANCE
PARAMETER
IS
UNBIASED
N
X
Μ
FIGURE
ILLUSTRATION
OF
HOW
BIAS
ARISES
IN
USING
MAX
IMUM
LIKELIHOOD
TO
DETERMINE
THE
VARIANCE
OF
A
GAUSSIAN
THE
GREEN
CURVE
SHOWS
THE
TRUE
GAUSSIAN
DISTRIBUTION
FROM
WHICH
DATA
IS
GENERATED
AND
THE
THREE
RED
CURVES
SHOW
THE
GAUSSIAN
DISTRIBUTIONS
OBTAINED
BY
FITTING
TO
THREE
DATA
SETS
EACH
CONSIST
ING
OF
TWO
DATA
POINTS
SHOWN
IN
BLUE
US
ING
THE
MAXIMUM
LIKELIHOOD
RESULTS
AND
AVERAGED
ACROSS
THE
THREE
DATA
SETS
THE
MEAN
IS
CORRECT
BUT
THE
VARIANCE
IS
SYSTEMATICALLY
UNDER
ESTIMATED
BECAUSE
IT
IS
MEASURED
RELATIVE
TO
THE
SAMPLE
MEAN
AND
NOT
RELATIVE
TO
THE
TRUE
MEAN
A
C
SECTION
IN
SECTION
WE
SHALL
SEE
HOW
THIS
RESULT
ARISES
AUTOMATICALLY
WHEN
WE
ADOPT
A
BAYESIAN
APPROACH
NOTE
THAT
THE
BIAS
OF
THE
MAXIMUM
LIKELIHOOD
SOLUTION
BECOMES
LESS
SIGNIFICANT
AS
THE
NUMBER
N
OF
DATA
POINTS
INCREASES
AND
IN
THE
LIMIT
N
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
VARIANCE
EQUALS
THE
TRUE
VARIANCE
OF
THE
DISTRIBUTION
THAT
GENERATED
THE
DATA
IN
PRACTICE
FOR
ANYTHING
OTHER
THAN
SMALL
N
THIS
BIAS
WILL
NOT
PROVE
TO
BE
A
SERIOUS
PROBLEM
HOWEVER
THROUGHOUT
THIS
BOOK
WE
SHALL
BE
INTERESTED
IN
MORE
COMPLEX
MODELS
WITH
MANY
PARAMETERS
FOR
WHICH
THE
BIAS
PROBLEMS
ASSO
CIATED
WITH
MAXIMUM
LIKELIHOOD
WILL
BE
MUCH
MORE
SEVERE
IN
FACT
AS
WE
SHALL
SEE
THE
ISSUE
OF
BIAS
IN
MAXIMUM
LIKELIHOOD
LIES
AT
THE
ROOT
OF
THE
OVER
FITTING
PROBLEM
THAT
WE
ENCOUNTERED
EARLIER
IN
THE
CONTEXT
OF
POLYNOMIAL
CURVE
FITTING
CURVE
FITTING
RE
VISITED
WE
HAVE
SEEN
HOW
THE
PROBLEM
OF
POLYNOMIAL
CURVE
FITTING
CAN
BE
EXPRESSED
IN
TERMS
OF
ERROR
MINIMIZATION
HERE
WE
RETURN
TO
THE
CURVE
FITTING
EXAMPLE
AND
VIEW
IT
FROM
A
PROBABILISTIC
PERSPECTIVE
THEREBY
GAINING
SOME
INSIGHTS
INTO
ERROR
FUNCTIONS
AND
REGULARIZATION
AS
WELL
AS
TAKING
US
TOWARDS
A
FULL
BAYESIAN
TREATMENT
THE
GOAL
IN
THE
CURVE
FITTING
PROBLEM
IS
TO
BE
ABLE
TO
MAKE
PREDICTIONS
FOR
THE
TARGET
VARIABLE
T
GIVEN
SOME
NEW
VALUE
OF
THE
INPUT
VARIABLE
X
ON
THE
BASIS
OF
A
SET
OF
TRAINING
DATA
COMPRISING
N
INPUT
VALUES
X
XN
T
AND
THEIR
CORRESPONDING
TARGET
VALUES
T
TN
T
WE
CAN
EXPRESS
OUR
UNCERTAINTY
OVER
THE
VALUE
OF
THE
TARGET
VARIABLE
USING
A
PROBABILITY
DISTRIBUTION
FOR
THIS
PURPOSE
WE
SHALL
ASSUME
THAT
GIVEN
THE
VALUE
OF
X
THE
CORRESPONDING
VALUE
OF
T
HAS
A
GAUSSIAN
DISTRIBUTION
WITH
A
MEAN
EQUAL
TO
THE
VALUE
Y
X
W
OF
THE
POLYNOMIAL
CURVE
GIVEN
BY
THUS
WE
HAVE
P
T
X
W
Β
N
T
Y
X
W
Β
WHERE
FOR
CONSISTENCY
WITH
THE
NOTATION
IN
LATER
CHAPTERS
WE
HAVE
DEFINED
A
PRECI
SION
PARAMETER
Β
CORRESPONDING
TO
THE
INVERSE
VARIANCE
OF
THE
DISTRIBUTION
THIS
IS
ILLUSTRATED
SCHEMATICALLY
IN
FIGURE
FIGURE
SCHEMATIC
ILLUSTRATION
OF
A
GAUS
SIAN
CONDITIONAL
DISTRIBUTION
FOR
T
GIVEN
X
GIVEN
BY
IN
WHICH
THE
MEAN
IS
GIVEN
BY
THE
POLYNO
MIAL
FUNCTION
Y
X
W
AND
THE
PRECISION
IS
GIVEN
BY
THE
PARAMETER
Β
WHICH
IS
RELATED
TO
THE
VARI
ANCE
BY
Β
T
Y
W
X
WE
NOW
USE
THE
TRAINING
DATA
X
T
TO
DETERMINE
THE
VALUES
OF
THE
UNKNOWN
PARAMETERS
W
AND
Β
BY
MAXIMUM
LIKELIHOOD
IF
THE
DATA
ARE
ASSUMED
TO
BE
DRAWN
INDEPENDENTLY
FROM
THE
DISTRIBUTION
THEN
THE
LIKELIHOOD
FUNCTION
IS
GIVEN
BY
N
P
T
X
W
Β
N
TN
Y
XN
W
Β
N
AS
WE
DID
IN
THE
CASE
OF
THE
SIMPLE
GAUSSIAN
DISTRIBUTION
EARLIER
IT
IS
CONVENIENT
TO
MAXIMIZE
THE
LOGARITHM
OF
THE
LIKELIHOOD
FUNCTION
SUBSTITUTING
FOR
THE
FORM
OF
THE
GAUSSIAN
DISTRIBUTION
GIVEN
BY
WE
OBTAIN
THE
LOG
LIKELIHOOD
FUNCTION
IN
THE
FORM
LN
P
T
X
W
Β
Β
Y
X
W
T
LN
Β
LN
CONSIDER
FIRST
THE
DETERMINATION
OF
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
POLYNO
MIAL
COEFFICIENTS
WHICH
WILL
BE
DENOTED
BY
WML
THESE
ARE
DETERMINED
BY
MAXI
MIZING
WITH
RESPECT
TO
W
FOR
THIS
PURPOSE
WE
CAN
OMIT
THE
LAST
TWO
TERMS
ON
THE
RIGHT
HAND
SIDE
OF
BECAUSE
THEY
DO
NOT
DEPEND
ON
W
ALSO
WE
NOTE
THAT
SCALING
THE
LOG
LIKELIHOOD
BY
A
POSITIVE
CONSTANT
COEFFICIENT
DOES
NOT
ALTER
THE
LOCATION
OF
THE
MAXIMUM
WITH
RESPECT
TO
W
AND
SO
WE
CAN
REPLACE
THE
COEFFICIENT
Β
WITH
FINALLY
INSTEAD
OF
MAXIMIZING
THE
LOG
LIKELIHOOD
WE
CAN
EQUIVALENTLY
MINIMIZE
THE
NEGATIVE
LOG
LIKELIHOOD
WE
THEREFORE
SEE
THAT
MAXIMIZING
LIKELIHOOD
IS
EQUIVALENT
SO
FAR
AS
DETERMINING
W
IS
CONCERNED
TO
MINIMIZING
THE
SUM
OF
SQUARES
ERROR
FUNCTION
DEFINED
BY
THUS
THE
SUM
OF
SQUARES
ERROR
FUNCTION
HAS
ARISEN
AS
A
CONSEQUENCE
OF
MAXIMIZING
LIKELIHOOD
UNDER
THE
ASSUMPTION
OF
A
GAUSSIAN
NOISE
DISTRIBUTION
WE
CAN
ALSO
USE
MAXIMUM
LIKELIHOOD
TO
DETERMINE
THE
PRECISION
PARAMETER
Β
OF
THE
GAUSSIAN
CONDITIONAL
DISTRIBUTION
MAXIMIZING
WITH
RESPECT
TO
Β
GIVES
Y
X
W
T
SECTION
AGAIN
WE
CAN
FIRST
DETERMINE
THE
PARAMETER
VECTOR
WML
GOVERNING
THE
MEAN
AND
SUB
SEQUENTLY
USE
THIS
TO
FIND
THE
PRECISION
ΒML
AS
WAS
THE
CASE
FOR
THE
SIMPLE
GAUSSIAN
DISTRIBUTION
HAVING
DETERMINED
THE
PARAMETERS
W
AND
Β
WE
CAN
NOW
MAKE
PREDICTIONS
FOR
NEW
VALUES
OF
X
BECAUSE
WE
NOW
HAVE
A
PROBABILISTIC
MODEL
THESE
ARE
EXPRESSED
IN
TERMS
OF
THE
PREDICTIVE
DISTRIBUTION
THAT
GIVES
THE
PROBABILITY
DISTRIBUTION
OVER
T
RATHER
THAN
SIMPLY
A
POINT
ESTIMATE
AND
IS
OBTAINED
BY
SUBSTITUTING
THE
MAXIMUM
LIKELIHOOD
PARAMETERS
INTO
TO
GIVE
P
T
X
WML
ΒML
N
T
Y
X
WML
Β
NOW
LET
US
TAKE
A
STEP
TOWARDS
A
MORE
BAYESIAN
APPROACH
AND
INTRODUCE
A
PRIOR
DISTRIBUTION
OVER
THE
POLYNOMIAL
COEFFICIENTS
W
FOR
SIMPLICITY
LET
US
CONSIDER
A
GAUSSIAN
DISTRIBUTION
OF
THE
FORM
Α
M
F
Α
T
L
WHERE
Α
IS
THE
PRECISION
OF
THE
DISTRIBUTION
AND
M
IS
THE
TOTAL
NUMBER
OF
ELEMENTS
IN
THE
VECTOR
W
FOR
AN
M
TH
ORDER
POLYNOMIAL
VARIABLES
SUCH
AS
Α
WHICH
CONTROL
THE
DISTRIBUTION
OF
MODEL
PARAMETERS
ARE
CALLED
HYPERPARAMETERS
USING
BAYES
THEOREM
THE
POSTERIOR
DISTRIBUTION
FOR
W
IS
PROPORTIONAL
TO
THE
PRODUCT
OF
THE
PRIOR
DISTRIBUTION
AND
THE
LIKELIHOOD
FUNCTION
P
W
X
T
Α
Β
P
T
X
W
Β
P
W
Α
WE
CAN
NOW
DETERMINE
W
BY
FINDING
THE
MOST
PROBABLE
VALUE
OF
W
GIVEN
THE
DATA
IN
OTHER
WORDS
BY
MAXIMIZING
THE
POSTERIOR
DISTRIBUTION
THIS
TECHNIQUE
IS
CALLED
MAXIMUM
POSTERIOR
OR
SIMPLY
MAP
TAKING
THE
NEGATIVE
LOGARITHM
OF
AND
COMBINING
WITH
AND
WE
FIND
THAT
THE
MAXIMUM
OF
THE
POSTERIOR
IS
GIVEN
BY
THE
MINIMUM
OF
Β
Y
X
W
T
W
W
THUS
WE
SEE
THAT
MAXIMIZING
THE
POSTERIOR
DISTRIBUTION
IS
EQUIVALENT
TO
MINIMIZING
THE
REGULARIZED
SUM
OF
SQUARES
ERROR
FUNCTION
ENCOUNTERED
EARLIER
IN
THE
FORM
WITH
A
REGULARIZATION
PARAMETER
GIVEN
BY
Λ
Α
Β
BAYESIAN
CURVE
FITTING
ALTHOUGH
WE
HAVE
INCLUDED
A
PRIOR
DISTRIBUTION
P
W
Α
WE
ARE
SO
FAR
STILL
MAK
ING
A
POINT
ESTIMATE
OF
W
AND
SO
THIS
DOES
NOT
YET
AMOUNT
TO
A
BAYESIAN
TREATMENT
IN
A
FULLY
BAYESIAN
APPROACH
WE
SHOULD
CONSISTENTLY
APPLY
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
WHICH
REQUIRES
AS
WE
SHALL
SEE
SHORTLY
THAT
WE
INTEGRATE
OVER
ALL
VAL
UES
OF
W
SUCH
MARGINALIZATIONS
LIE
AT
THE
HEART
OF
BAYESIAN
METHODS
FOR
PATTERN
RECOGNITION
IN
THE
CURVE
FITTING
PROBLEM
WE
ARE
GIVEN
THE
TRAINING
DATA
X
AND
T
ALONG
WITH
A
NEW
TEST
POINT
X
AND
OUR
GOAL
IS
TO
PREDICT
THE
VALUE
OF
T
WE
THEREFORE
WISH
TO
EVALUATE
THE
PREDICTIVE
DISTRIBUTION
P
T
X
X
T
HERE
WE
SHALL
ASSUME
THAT
THE
PARAMETERS
Α
AND
Β
ARE
FIXED
AND
KNOWN
IN
ADVANCE
IN
LATER
CHAPTERS
WE
SHALL
DISCUSS
HOW
SUCH
PARAMETERS
CAN
BE
INFERRED
FROM
DATA
IN
A
BAYESIAN
SETTING
A
BAYESIAN
TREATMENT
SIMPLY
CORRESPONDS
TO
A
CONSISTENT
APPLICATION
OF
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
WHICH
ALLOW
THE
PREDICTIVE
DISTRIBUTION
TO
BE
WRITTEN
IN
THE
FORM
P
T
X
X
T
P
T
X
W
P
W
X
T
DW
HERE
P
T
X
W
IS
GIVEN
BY
AND
WE
HAVE
OMITTED
THE
DEPENDENCE
ON
Α
AND
Β
TO
SIMPLIFY
THE
NOTATION
HERE
P
W
X
T
IS
THE
POSTERIOR
DISTRIBUTION
OVER
PARAM
ETERS
AND
CAN
BE
FOUND
BY
NORMALIZING
THE
RIGHT
HAND
SIDE
OF
WE
SHALL
SEE
IN
SECTION
THAT
FOR
PROBLEMS
SUCH
AS
THE
CURVE
FITTING
EXAMPLE
THIS
POSTERIOR
DISTRIBUTION
IS
A
GAUSSIAN
AND
CAN
BE
EVALUATED
ANALYTICALLY
SIMILARLY
THE
INTEGRA
TION
IN
CAN
ALSO
BE
PERFORMED
ANALYTICALLY
WITH
THE
RESULT
THAT
THE
PREDICTIVE
DISTRIBUTION
IS
GIVEN
BY
A
GAUSSIAN
OF
THE
FORM
P
T
X
X
T
N
T
M
X
X
WHERE
THE
MEAN
AND
VARIANCE
ARE
GIVEN
BY
N
M
X
ΒΦ
X
TS
Φ
XN
TN
N
X
Β
Φ
X
TSΦ
X
HERE
THE
MATRIX
IS
GIVEN
BY
N
ΑI
Β
Φ
XN
Φ
X
T
N
WHERE
I
IS
THE
UNIT
MATRIX
AND
WE
HAVE
DEFINED
THE
VECTOR
Φ
X
WITH
ELEMENTS
ΦI
X
XI
FOR
I
M
WE
SEE
THAT
THE
VARIANCE
AS
WELL
AS
THE
MEAN
OF
THE
PREDICTIVE
DISTRIBUTION
IN
IS
DEPENDENT
ON
X
THE
FIRST
TERM
IN
REPRESENTS
THE
UNCERTAINTY
IN
THE
PREDICTED
VALUE
OF
T
DUE
TO
THE
NOISE
ON
THE
TARGET
VARIABLES
AND
WAS
EXPRESSED
ALREADY
IN
THE
MAXIMUM
LIKELIHOOD
PREDICTIVE
DISTRIBUTION
THROUGH
Β
HOWEVER
THE
SECOND
TERM
ARISES
FROM
THE
UNCERTAINTY
IN
THE
PARAMETERS
W
AND
IS
A
CONSEQUENCE
OF
THE
BAYESIAN
TREATMENT
THE
PREDICTIVE
DISTRIBUTION
FOR
THE
SYNTHETIC
SINUSOIDAL
REGRESSION
PROBLEM
IS
ILLUSTRATED
IN
FIGURE
FIGURE
THE
PREDICTIVE
DISTRIBUTION
RESULT
ING
FROM
A
BAYESIAN
TREATMENT
OF
POLYNOMIAL
CURVE
FITTING
USING
AN
M
POLYNOMIAL
WITH
THE
FIXED
PARAMETERS
Α
AND
Β
T
CORRESPONDING
TO
THE
KNOWN
NOISE
VARIANCE
IN
WHICH
THE
RED
CURVE
DENOTES
THE
MEAN
OF
THE
PREDICTIVE
DISTRIBUTION
AND
THE
RED
REGION
CORRESPONDS
TO
STAN
DARD
DEVIATION
AROUND
THE
MEAN
X
MODEL
SELECTION
IN
OUR
EXAMPLE
OF
POLYNOMIAL
CURVE
FITTING
USING
LEAST
SQUARES
WE
SAW
THAT
THERE
WAS
AN
OPTIMAL
ORDER
OF
POLYNOMIAL
THAT
GAVE
THE
BEST
GENERALIZATION
THE
ORDER
OF
THE
POLYNOMIAL
CONTROLS
THE
NUMBER
OF
FREE
PARAMETERS
IN
THE
MODEL
AND
THEREBY
GOVERNS
THE
MODEL
COMPLEXITY
WITH
REGULARIZED
LEAST
SQUARES
THE
REGULARIZATION
COEFFICIENT
Λ
ALSO
CONTROLS
THE
EFFECTIVE
COMPLEXITY
OF
THE
MODEL
WHEREAS
FOR
MORE
COMPLEX
MODELS
SUCH
AS
MIXTURE
DISTRIBUTIONS
OR
NEURAL
NETWORKS
THERE
MAY
BE
MULTIPLE
PA
RAMETERS
GOVERNING
COMPLEXITY
IN
A
PRACTICAL
APPLICATION
WE
NEED
TO
DETERMINE
THE
VALUES
OF
SUCH
PARAMETERS
AND
THE
PRINCIPAL
OBJECTIVE
IN
DOING
SO
IS
USUALLY
TO
ACHIEVE
THE
BEST
PREDICTIVE
PERFORMANCE
ON
NEW
DATA
FURTHERMORE
AS
WELL
AS
FIND
ING
THE
APPROPRIATE
VALUES
FOR
COMPLEXITY
PARAMETERS
WITHIN
A
GIVEN
MODEL
WE
MAY
WISH
TO
CONSIDER
A
RANGE
OF
DIFFERENT
TYPES
OF
MODEL
IN
ORDER
TO
FIND
THE
BEST
ONE
FOR
OUR
PARTICULAR
APPLICATION
WE
HAVE
ALREADY
SEEN
THAT
IN
THE
MAXIMUM
LIKELIHOOD
APPROACH
THE
PERFOR
MANCE
ON
THE
TRAINING
SET
IS
NOT
A
GOOD
INDICATOR
OF
PREDICTIVE
PERFORMANCE
ON
UN
SEEN
DATA
DUE
TO
THE
PROBLEM
OF
OVER
FITTING
IF
DATA
IS
PLENTIFUL
THEN
ONE
APPROACH
IS
SIMPLY
TO
USE
SOME
OF
THE
AVAILABLE
DATA
TO
TRAIN
A
RANGE
OF
MODELS
OR
A
GIVEN
MODEL
WITH
A
RANGE
OF
VALUES
FOR
ITS
COMPLEXITY
PARAMETERS
AND
THEN
TO
COMPARE
THEM
ON
INDEPENDENT
DATA
SOMETIMES
CALLED
A
VALIDATION
SET
AND
SELECT
THE
ONE
HAVING
THE
BEST
PREDICTIVE
PERFORMANCE
IF
THE
MODEL
DESIGN
IS
ITERATED
MANY
TIMES
USING
A
LIM
ITED
SIZE
DATA
SET
THEN
SOME
OVER
FITTING
TO
THE
VALIDATION
DATA
CAN
OCCUR
AND
SO
IT
MAY
BE
NECESSARY
TO
KEEP
ASIDE
A
THIRD
TEST
SET
ON
WHICH
THE
PERFORMANCE
OF
THE
SELECTED
MODEL
IS
FINALLY
EVALUATED
IN
MANY
APPLICATIONS
HOWEVER
THE
SUPPLY
OF
DATA
FOR
TRAINING
AND
TESTING
WILL
BE
LIMITED
AND
IN
ORDER
TO
BUILD
GOOD
MODELS
WE
WISH
TO
USE
AS
MUCH
OF
THE
AVAILABLE
DATA
AS
POSSIBLE
FOR
TRAINING
HOWEVER
IF
THE
VALIDATION
SET
IS
SMALL
IT
WILL
GIVE
A
RELATIVELY
NOISY
ESTIMATE
OF
PREDICTIVE
PERFORMANCE
ONE
SOLUTION
TO
THIS
DILEMMA
IS
TO
USE
CROSS
VALIDATION
WHICH
IS
ILLUSTRATED
IN
FIGURE
THIS
ALLOWS
A
PROPORTION
OF
THE
AVAILABLE
DATA
TO
BE
USED
FOR
TRAINING
WHILE
MAKING
USE
OF
ALL
OF
THE
FIGURE
THE
TECHNIQUE
OF
FOLD
CROSS
VALIDATION
ILLUS
TRATED
HERE
FOR
THE
CASE
OF
INVOLVES
TAK
ING
THE
AVAILABLE
DATA
AND
PARTITIONING
IT
INTO
GROUPS
IN
THE
SIMPLEST
CASE
THESE
ARE
OF
EQUAL
SIZE
THEN
OF
THE
GROUPS
ARE
USED
TO
TRAIN
A
SET
OF
MODELS
THAT
ARE
THEN
EVALUATED
ON
THE
RE
MAINING
GROUP
THIS
PROCEDURE
IS
THEN
REPEATED
FOR
ALL
POSSIBLE
CHOICES
FOR
THE
HELD
OUT
GROUP
INDICATED
HERE
BY
THE
RED
BLOCKS
AND
THE
PERFOR
MANCE
SCORES
FROM
THE
RUNS
ARE
THEN
AVERAGED
RUN
RUN
RUN
RUN
DATA
TO
ASSESS
PERFORMANCE
WHEN
DATA
IS
PARTICULARLY
SCARCE
IT
MAY
BE
APPROPRIATE
TO
CONSIDER
THE
CASE
N
WHERE
N
IS
THE
TOTAL
NUMBER
OF
DATA
POINTS
WHICH
GIVES
THE
LEAVE
ONE
OUT
TECHNIQUE
ONE
MAJOR
DRAWBACK
OF
CROSS
VALIDATION
IS
THAT
THE
NUMBER
OF
TRAINING
RUNS
THAT
MUST
BE
PERFORMED
IS
INCREASED
BY
A
FACTOR
OF
AND
THIS
CAN
PROVE
PROBLEMATIC
FOR
MODELS
IN
WHICH
THE
TRAINING
IS
ITSELF
COMPUTATIONALLY
EXPENSIVE
A
FURTHER
PROBLEM
WITH
TECHNIQUES
SUCH
AS
CROSS
VALIDATION
THAT
USE
SEPARATE
DATA
TO
ASSESS
PERFORMANCE
IS
THAT
WE
MIGHT
HAVE
MULTIPLE
COMPLEXITY
PARAMETERS
FOR
A
SINGLE
MODEL
FOR
IN
STANCE
THERE
MIGHT
BE
SEVERAL
REGULARIZATION
PARAMETERS
EXPLORING
COMBINATIONS
OF
SETTINGS
FOR
SUCH
PARAMETERS
COULD
IN
THE
WORST
CASE
REQUIRE
A
NUMBER
OF
TRAINING
RUNS
THAT
IS
EXPONENTIAL
IN
THE
NUMBER
OF
PARAMETERS
CLEARLY
WE
NEED
A
BETTER
AP
PROACH
IDEALLY
THIS
SHOULD
RELY
ONLY
ON
THE
TRAINING
DATA
AND
SHOULD
ALLOW
MULTIPLE
HYPERPARAMETERS
AND
MODEL
TYPES
TO
BE
COMPARED
IN
A
SINGLE
TRAINING
RUN
WE
THERE
FORE
NEED
TO
FIND
A
MEASURE
OF
PERFORMANCE
WHICH
DEPENDS
ONLY
ON
THE
TRAINING
DATA
AND
WHICH
DOES
NOT
SUFFER
FROM
BIAS
DUE
TO
OVER
FITTING
HISTORICALLY
VARIOUS
INFORMATION
CRITERIA
HAVE
BEEN
PROPOSED
THAT
ATTEMPT
TO
CORRECT
FOR
THE
BIAS
OF
MAXIMUM
LIKELIHOOD
BY
THE
ADDITION
OF
A
PENALTY
TERM
TO
COMPENSATE
FOR
THE
OVER
FITTING
OF
MORE
COMPLEX
MODELS
FOR
EXAMPLE
THE
AKAIKE
INFORMATION
CRITERION
OR
AIC
AKAIKE
CHOOSES
THE
MODEL
FOR
WHICH
THE
QUAN
TITY
LN
P
D
WML
M
IS
LARGEST
HERE
P
WML
IS
THE
BEST
FIT
LOG
LIKELIHOOD
AND
M
IS
THE
NUMBER
OF
ADJUSTABLE
PARAMETERS
IN
THE
MODEL
A
VARIANT
OF
THIS
QUANTITY
CALLED
THE
BAYESIAN
INFORMATION
CRITERION
OR
BIC
WILL
BE
DISCUSSED
IN
SECTION
SUCH
CRITERIA
DO
NOT
TAKE
ACCOUNT
OF
THE
UNCERTAINTY
IN
THE
MODEL
PARAMETERS
HOWEVER
AND
IN
PRACTICE
THEY
TEND
TO
FAVOUR
OVERLY
SIMPLE
MODELS
WE
THEREFORE
TURN
IN
SECTION
TO
A
FULLY
BAYESIAN
APPROACH
WHERE
WE
SHALL
SEE
HOW
COMPLEXITY
PENALTIES
ARISE
IN
A
NATURAL
AND
PRINCIPLED
WAY
THE
CURSE
OF
DIMENSIONALITY
IN
THE
POLYNOMIAL
CURVE
FITTING
EXAMPLE
WE
HAD
JUST
ONE
INPUT
VARIABLE
X
FOR
PRAC
TICAL
APPLICATIONS
OF
PATTERN
RECOGNITION
HOWEVER
WE
WILL
HAVE
TO
DEAL
WITH
SPACES
FIGURE
SCATTER
PLOT
OF
THE
OIL
FLOW
DATA
FOR
INPUT
VARIABLES
AND
IN
WHICH
RED
DENOTES
THE
HOMOGE
NOUS
CLASS
GREEN
DENOTES
THE
ANNULAR
CLASS
AND
BLUE
DENOTES
THE
LAMINAR
CLASS
OUR
GOAL
IS
TO
CLASSIFY
THE
NEW
TEST
POINT
DE
NOTED
BY
OF
HIGH
DIMENSIONALITY
COMPRISING
MANY
INPUT
VARIABLES
AS
WE
NOW
DISCUSS
THIS
POSES
SOME
SERIOUS
CHALLENGES
AND
IS
AN
IMPORTANT
FACTOR
INFLUENCING
THE
DESIGN
OF
PATTERN
RECOGNITION
TECHNIQUES
IN
ORDER
TO
ILLUSTRATE
THE
PROBLEM
WE
CONSIDER
A
SYNTHETICALLY
GENERATED
DATA
SET
REPRESENTING
MEASUREMENTS
TAKEN
FROM
A
PIPELINE
CONTAINING
A
MIXTURE
OF
OIL
WA
TER
AND
GAS
BISHOP
AND
JAMES
THESE
THREE
MATERIALS
CAN
BE
PRESENT
IN
ONE
OF
THREE
DIFFERENT
GEOMETRICAL
CONFIGURATIONS
KNOWN
AS
HOMOGENOUS
ANNULAR
AND
LAMINAR
AND
THE
FRACTIONS
OF
THE
THREE
MATERIALS
CAN
ALSO
VARY
EACH
DATA
POINT
COM
PRISES
A
DIMENSIONAL
INPUT
VECTOR
CONSISTING
OF
MEASUREMENTS
TAKEN
WITH
GAMMA
RAY
DENSITOMETERS
THAT
MEASURE
THE
ATTENUATION
OF
GAMMA
RAYS
PASSING
ALONG
NAR
ROW
BEAMS
THROUGH
THE
PIPE
THIS
DATA
SET
IS
DESCRIBED
IN
DETAIL
IN
APPENDIX
A
FIGURE
SHOWS
POINTS
FROM
THIS
DATA
SET
ON
A
PLOT
SHOWING
TWO
OF
THE
MEA
SUREMENTS
AND
THE
REMAINING
TEN
INPUT
VALUES
ARE
IGNORED
FOR
THE
PURPOSES
OF
THIS
ILLUSTRATION
EACH
DATA
POINT
IS
LABELLED
ACCORDING
TO
WHICH
OF
THE
THREE
GEOMET
RICAL
CLASSES
IT
BELONGS
TO
AND
OUR
GOAL
IS
TO
USE
THIS
DATA
AS
A
TRAINING
SET
IN
ORDER
TO
BE
ABLE
TO
CLASSIFY
A
NEW
OBSERVATION
SUCH
AS
THE
ONE
DENOTED
BY
THE
CROSS
IN
FIGURE
WE
OBSERVE
THAT
THE
CROSS
IS
SURROUNDED
BY
NUMEROUS
RED
POINTS
AND
SO
WE
MIGHT
SUPPOSE
THAT
IT
BELONGS
TO
THE
RED
CLASS
HOWEVER
THERE
ARE
ALSO
PLENTY
OF
GREEN
POINTS
NEARBY
SO
WE
MIGHT
THINK
THAT
IT
COULD
INSTEAD
BELONG
TO
THE
GREEN
CLASS
IT
SEEMS
UNLIKELY
THAT
IT
BELONGS
TO
THE
BLUE
CLASS
THE
INTUITION
HERE
IS
THAT
THE
IDENTITY
OF
THE
CROSS
SHOULD
BE
DETERMINED
MORE
STRONGLY
BY
NEARBY
POINTS
FROM
THE
TRAINING
SET
AND
LESS
STRONGLY
BY
MORE
DISTANT
POINTS
IN
FACT
THIS
INTUITION
TURNS
OUT
TO
BE
REASONABLE
AND
WILL
BE
DISCUSSED
MORE
FULLY
IN
LATER
CHAPTERS
HOW
CAN
WE
TURN
THIS
INTUITION
INTO
A
LEARNING
ALGORITHM
ONE
VERY
SIMPLE
AP
PROACH
WOULD
BE
TO
DIVIDE
THE
INPUT
SPACE
INTO
REGULAR
CELLS
AS
INDICATED
IN
FIG
URE
WHEN
WE
ARE
GIVEN
A
TEST
POINT
AND
WE
WISH
TO
PREDICT
ITS
CLASS
WE
FIRST
DECIDE
WHICH
CELL
IT
BELONGS
TO
AND
WE
THEN
FIND
ALL
OF
THE
TRAINING
DATA
POINTS
THAT
FIGURE
ILLUSTRATION
OF
A
SIMPLE
APPROACH
TO
THE
SOLUTION
OF
A
CLASSIFICATION
PROBLEM
IN
WHICH
THE
INPUT
SPACE
IS
DIVIDED
INTO
CELLS
AND
ANY
NEW
TEST
POINT
IS
ASSIGNED
TO
THE
CLASS
THAT
HAS
A
MAJORITY
NUMBER
OF
REP
RESENTATIVES
IN
THE
SAME
CELL
AS
THE
TEST
POINT
AS
WE
SHALL
SEE
SHORTLY
THIS
SIMPLISTIC
APPROACH
HAS
SOME
SEVERE
SHORTCOMINGS
SECTION
FALL
IN
THE
SAME
CELL
THE
IDENTITY
OF
THE
TEST
POINT
IS
PREDICTED
AS
BEING
THE
SAME
AS
THE
CLASS
HAVING
THE
LARGEST
NUMBER
OF
TRAINING
POINTS
IN
THE
SAME
CELL
AS
THE
TEST
POINT
WITH
TIES
BEING
BROKEN
AT
RANDOM
THERE
ARE
NUMEROUS
PROBLEMS
WITH
THIS
NAIVE
APPROACH
BUT
ONE
OF
THE
MOST
SE
VERE
BECOMES
APPARENT
WHEN
WE
CONSIDER
ITS
EXTENSION
TO
PROBLEMS
HAVING
LARGER
NUMBERS
OF
INPUT
VARIABLES
CORRESPONDING
TO
INPUT
SPACES
OF
HIGHER
DIMENSIONALITY
THE
ORIGIN
OF
THE
PROBLEM
IS
ILLUSTRATED
IN
FIGURE
WHICH
SHOWS
THAT
IF
WE
DIVIDE
A
REGION
OF
A
SPACE
INTO
REGULAR
CELLS
THEN
THE
NUMBER
OF
SUCH
CELLS
GROWS
EXPONEN
TIALLY
WITH
THE
DIMENSIONALITY
OF
THE
SPACE
THE
PROBLEM
WITH
AN
EXPONENTIALLY
LARGE
NUMBER
OF
CELLS
IS
THAT
WE
WOULD
NEED
AN
EXPONENTIALLY
LARGE
QUANTITY
OF
TRAINING
DATA
IN
ORDER
TO
ENSURE
THAT
THE
CELLS
ARE
NOT
EMPTY
CLEARLY
WE
HAVE
NO
HOPE
OF
APPLYING
SUCH
A
TECHNIQUE
IN
A
SPACE
OF
MORE
THAN
A
FEW
VARIABLES
AND
SO
WE
NEED
TO
FIND
A
MORE
SOPHISTICATED
APPROACH
WE
CAN
GAIN
FURTHER
INSIGHT
INTO
THE
PROBLEMS
OF
HIGH
DIMENSIONAL
SPACES
BY
RETURNING
TO
THE
EXAMPLE
OF
POLYNOMIAL
CURVE
FITTING
AND
CONSIDERING
HOW
WE
WOULD
FIGURE
ILLUSTRATION
OF
THE
CURSE
OF
DIMENSIONALITY
SHOWING
HOW
THE
NUMBER
OF
REGIONS
OF
A
REGULAR
GRID
GROWS
EXPONENTIALLY
WITH
THE
DIMENSIONALITY
D
OF
THE
SPACE
FOR
CLARITY
ONLY
A
SUBSET
OF
THE
CUBICAL
REGIONS
ARE
SHOWN
FOR
D
D
D
D
EXTEND
THIS
APPROACH
TO
DEAL
WITH
INPUT
SPACES
HAVING
SEVERAL
VARIABLES
IF
WE
HAVE
D
INPUT
VARIABLES
THEN
A
GENERAL
POLYNOMIAL
WITH
COEFFICIENTS
UP
TO
ORDER
WOULD
TAKE
THE
FORM
D
D
D
D
D
D
Y
X
W
WIXI
WIJXIXJ
WIJKXIXJXK
EXERCISE
EXERCISE
EXERCISE
AS
D
INCREASES
SO
THE
NUMBER
OF
INDEPENDENT
COEFFICIENTS
NOT
ALL
OF
THE
COEFFICIENTS
ARE
INDEPENDENT
DUE
TO
INTERCHANGE
SYMMETRIES
AMONGST
THE
X
VARIABLES
GROWS
PRO
PORTIONALLY
TO
IN
PRACTICE
TO
CAPTURE
COMPLEX
DEPENDENCIES
IN
THE
DATA
WE
MAY
NEED
TO
USE
A
HIGHER
ORDER
POLYNOMIAL
FOR
A
POLYNOMIAL
OF
ORDER
M
THE
GROWTH
IN
THE
NUMBER
OF
COEFFICIENTS
IS
LIKE
DM
ALTHOUGH
THIS
IS
NOW
A
POWER
LAW
GROWTH
RATHER
THAN
AN
EXPONENTIAL
GROWTH
IT
STILL
POINTS
TO
THE
METHOD
BECOMING
RAPIDLY
UNWIELDY
AND
OF
LIMITED
PRACTICAL
UTILITY
OUR
GEOMETRICAL
INTUITIONS
FORMED
THROUGH
A
LIFE
SPENT
IN
A
SPACE
OF
THREE
DI
MENSIONS
CAN
FAIL
BADLY
WHEN
WE
CONSIDER
SPACES
OF
HIGHER
DIMENSIONALITY
AS
A
SIMPLE
EXAMPLE
CONSIDER
A
SPHERE
OF
RADIUS
R
IN
A
SPACE
OF
D
DIMENSIONS
AND
ASK
WHAT
IS
THE
FRACTION
OF
THE
VOLUME
OF
THE
SPHERE
THAT
LIES
BETWEEN
RADIUS
R
E
AND
R
WE
CAN
EVALUATE
THIS
FRACTION
BY
NOTING
THAT
THE
VOLUME
OF
A
SPHERE
OF
RADIUS
R
IN
D
DIMENSIONS
MUST
SCALE
AS
RD
AND
SO
WE
WRITE
VD
R
KDRD
WHERE
THE
CONSTANT
KD
DEPENDS
ONLY
ON
D
THUS
THE
REQUIRED
FRACTION
IS
GIVEN
BY
VD
VD
E
D
VD
WHICH
IS
PLOTTED
AS
A
FUNCTION
OF
E
FOR
VARIOUS
VALUES
OF
D
IN
FIGURE
WE
SEE
THAT
FOR
LARGE
D
THIS
FRACTION
TENDS
TO
EVEN
FOR
SMALL
VALUES
OF
E
THUS
IN
SPACES
OF
HIGH
DIMENSIONALITY
MOST
OF
THE
VOLUME
OF
A
SPHERE
IS
CONCENTRATED
IN
A
THIN
SHELL
NEAR
THE
SURFACE
AS
A
FURTHER
EXAMPLE
OF
DIRECT
RELEVANCE
TO
PATTERN
RECOGNITION
CONSIDER
THE
BEHAVIOUR
OF
A
GAUSSIAN
DISTRIBUTION
IN
A
HIGH
DIMENSIONAL
SPACE
IF
WE
TRANSFORM
FROM
CARTESIAN
TO
POLAR
COORDINATES
AND
THEN
INTEGRATE
OUT
THE
DIRECTIONAL
VARIABLES
WE
OBTAIN
AN
EXPRESSION
FOR
THE
DENSITY
P
R
AS
A
FUNCTION
OF
RADIUS
R
FROM
THE
ORIGIN
THUS
P
R
ΔR
IS
THE
PROBABILITY
MASS
INSIDE
A
THIN
SHELL
OF
THICKNESS
ΔR
LOCATED
AT
RADIUS
R
THIS
DISTRIBUTION
IS
PLOTTED
FOR
VARIOUS
VALUES
OF
D
IN
FIGURE
AND
WE
SEE
THAT
FOR
LARGE
D
THE
PROBABILITY
MASS
OF
THE
GAUSSIAN
IS
CONCENTRATED
IN
A
THIN
SHELL
THE
SEVERE
DIFFICULTY
THAT
CAN
ARISE
IN
SPACES
OF
MANY
DIMENSIONS
IS
SOMETIMES
CALLED
THE
CURSE
OF
DIMENSIONALITY
BELLMAN
IN
THIS
BOOK
WE
SHALL
MAKE
EX
TENSIVE
USE
OF
ILLUSTRATIVE
EXAMPLES
INVOLVING
INPUT
SPACES
OF
ONE
OR
TWO
DIMENSIONS
BECAUSE
THIS
MAKES
IT
PARTICULARLY
EASY
TO
ILLUSTRATE
THE
TECHNIQUES
GRAPHICALLY
THE
READER
SHOULD
BE
WARNED
HOWEVER
THAT
NOT
ALL
INTUITIONS
DEVELOPED
IN
SPACES
OF
LOW
DIMENSIONALITY
WILL
GENERALIZE
TO
SPACES
OF
MANY
DIMENSIONS
FIGURE
PLOT
OF
THE
FRACTION
OF
THE
VOLUME
OF
A
SPHERE
LYING
IN
THE
RANGE
R
E
TO
R
FOR
VARIOUS
VALUES
OF
THE
DIMENSIONALITY
D
E
ALTHOUGH
THE
CURSE
OF
DIMENSIONALITY
CERTAINLY
RAISES
IMPORTANT
ISSUES
FOR
PAT
TERN
RECOGNITION
APPLICATIONS
IT
DOES
NOT
PREVENT
US
FROM
FINDING
EFFECTIVE
TECHNIQUES
APPLICABLE
TO
HIGH
DIMENSIONAL
SPACES
THE
REASONS
FOR
THIS
ARE
TWOFOLD
FIRST
REAL
DATA
WILL
OFTEN
BE
CONFINED
TO
A
REGION
OF
THE
SPACE
HAVING
LOWER
EFFECTIVE
DIMENSION
ALITY
AND
IN
PARTICULAR
THE
DIRECTIONS
OVER
WHICH
IMPORTANT
VARIATIONS
IN
THE
TARGET
VARIABLES
OCCUR
MAY
BE
SO
CONFINED
SECOND
REAL
DATA
WILL
TYPICALLY
EXHIBIT
SOME
SMOOTHNESS
PROPERTIES
AT
LEAST
LOCALLY
SO
THAT
FOR
THE
MOST
PART
SMALL
CHANGES
IN
THE
INPUT
VARIABLES
WILL
PRODUCE
SMALL
CHANGES
IN
THE
TARGET
VARIABLES
AND
SO
WE
CAN
EX
PLOIT
LOCAL
INTERPOLATION
LIKE
TECHNIQUES
TO
ALLOW
US
TO
MAKE
PREDICTIONS
OF
THE
TARGET
VARIABLES
FOR
NEW
VALUES
OF
THE
INPUT
VARIABLES
SUCCESSFUL
PATTERN
RECOGNITION
TECH
NIQUES
EXPLOIT
ONE
OR
BOTH
OF
THESE
PROPERTIES
CONSIDER
FOR
EXAMPLE
AN
APPLICATION
IN
MANUFACTURING
IN
WHICH
IMAGES
ARE
CAPTURED
OF
IDENTICAL
PLANAR
OBJECTS
ON
A
CON
VEYOR
BELT
IN
WHICH
THE
GOAL
IS
TO
DETERMINE
THEIR
ORIENTATION
EACH
IMAGE
IS
A
POINT
FIGURE
PLOT
OF
THE
PROBABILITY
DENSITY
WITH
RESPECT
TO
RADIUS
R
OF
A
GAUS
SIAN
DISTRIBUTION
FOR
VARIOUS
VALUES
OF
THE
DIMENSIONALITY
D
IN
A
HIGH
DIMENSIONAL
SPACE
MOST
OF
THE
PROBABILITY
MASS
OF
A
GAUSSIAN
IS
LO
CATED
WITHIN
A
THIN
SHELL
AT
A
SPECIFIC
RADIUS
R
IN
A
HIGH
DIMENSIONAL
SPACE
WHOSE
DIMENSIONALITY
IS
DETERMINED
BY
THE
NUMBER
OF
PIXELS
BECAUSE
THE
OBJECTS
CAN
OCCUR
AT
DIFFERENT
POSITIONS
WITHIN
THE
IMAGE
AND
IN
DIFFERENT
ORIENTATIONS
THERE
ARE
THREE
DEGREES
OF
FREEDOM
OF
VARIABILITY
BETWEEN
IMAGES
AND
A
SET
OF
IMAGES
WILL
LIVE
ON
A
THREE
DIMENSIONAL
MANIFOLD
EMBEDDED
WITHIN
THE
HIGH
DIMENSIONAL
SPACE
DUE
TO
THE
COMPLEX
RELATIONSHIPS
BETWEEN
THE
OBJECT
POSITION
OR
ORIENTATION
AND
THE
PIXEL
INTENSITIES
THIS
MANIFOLD
WILL
BE
HIGHLY
NONLINEAR
IF
THE
GOAL
IS
TO
LEARN
A
MODEL
THAT
CAN
TAKE
AN
INPUT
IMAGE
AND
OUTPUT
THE
ORIENTATION
OF
THE
OBJECT
IRRESPECTIVE
OF
ITS
POSITION
THEN
THERE
IS
ONLY
ONE
DEGREE
OF
FREEDOM
OF
VARIABILITY
WITHIN
THE
MANIFOLD
THAT
IS
SIGNIFICANT
DECISION
THEORY
WE
HAVE
SEEN
IN
SECTION
HOW
PROBABILITY
THEORY
PROVIDES
US
WITH
A
CONSISTENT
MATHEMATICAL
FRAMEWORK
FOR
QUANTIFYING
AND
MANIPULATING
UNCERTAINTY
HERE
WE
TURN
TO
A
DISCUSSION
OF
DECISION
THEORY
THAT
WHEN
COMBINED
WITH
PROBABILITY
THEORY
ALLOWS
US
TO
MAKE
OPTIMAL
DECISIONS
IN
SITUATIONS
INVOLVING
UNCERTAINTY
SUCH
AS
THOSE
ENCOUNTERED
IN
PATTERN
RECOGNITION
SUPPOSE
WE
HAVE
AN
INPUT
VECTOR
X
TOGETHER
WITH
A
CORRESPONDING
VECTOR
T
OF
TARGET
VARIABLES
AND
OUR
GOAL
IS
TO
PREDICT
T
GIVEN
A
NEW
VALUE
FOR
X
FOR
REGRESSION
PROBLEMS
T
WILL
COMPRISE
CONTINUOUS
VARIABLES
WHEREAS
FOR
CLASSIFICATION
PROBLEMS
T
WILL
REPRESENT
CLASS
LABELS
THE
JOINT
PROBABILITY
DISTRIBUTION
P
X
T
PROVIDES
A
COMPLETE
SUMMARY
OF
THE
UNCERTAINTY
ASSOCIATED
WITH
THESE
VARIABLES
DETERMINATION
OF
P
X
T
FROM
A
SET
OF
TRAINING
DATA
IS
AN
EXAMPLE
OF
INFERENCE
AND
IS
TYPICALLY
A
VERY
DIFFICULT
PROBLEM
WHOSE
SOLUTION
FORMS
THE
SUBJECT
OF
MUCH
OF
THIS
BOOK
IN
A
PRACTICAL
APPLICATION
HOWEVER
WE
MUST
OFTEN
MAKE
A
SPECIFIC
PREDICTION
FOR
THE
VALUE
OF
T
OR
MORE
GENERALLY
TAKE
A
SPECIFIC
ACTION
BASED
ON
OUR
UNDERSTANDING
OF
THE
VALUES
T
IS
LIKELY
TO
TAKE
AND
THIS
ASPECT
IS
THE
SUBJECT
OF
DECISION
THEORY
CONSIDER
FOR
EXAMPLE
A
MEDICAL
DIAGNOSIS
PROBLEM
IN
WHICH
WE
HAVE
TAKEN
AN
X
RAY
IMAGE
OF
A
PATIENT
AND
WE
WISH
TO
DETERMINE
WHETHER
THE
PATIENT
HAS
CANCER
OR
NOT
IN
THIS
CASE
THE
INPUT
VECTOR
X
IS
THE
SET
OF
PIXEL
INTENSITIES
IN
THE
IMAGE
AND
OUTPUT
VARIABLE
T
WILL
REPRESENT
THE
PRESENCE
OF
CANCER
WHICH
WE
DENOTE
BY
THE
CLASS
OR
THE
ABSENCE
OF
CANCER
WHICH
WE
DENOTE
BY
THE
CLASS
WE
MIGHT
FOR
INSTANCE
CHOOSE
T
TO
BE
A
BINARY
VARIABLE
SUCH
THAT
T
CORRESPONDS
TO
CLASS
AND
T
CORRESPONDS
TO
CLASS
WE
SHALL
SEE
LATER
THAT
THIS
CHOICE
OF
LABEL
VALUES
IS
PARTICULARLY
CONVENIENT
FOR
PROBABILISTIC
MODELS
THE
GENERAL
INFERENCE
PROBLEM
THEN
INVOLVES
DETERMINING
THE
JOINT
DISTRIBUTION
P
X
K
OR
EQUIVALENTLY
P
X
T
WHICH
GIVES
US
THE
MOST
COMPLETE
PROBABILISTIC
DESCRIPTION
OF
THE
SITUATION
ALTHOUGH
THIS
CAN
BE
A
VERY
USEFUL
AND
INFORMATIVE
QUANTITY
IN
THE
END
WE
MUST
DECIDE
EITHER
TO
GIVE
TREATMENT
TO
THE
PATIENT
OR
NOT
AND
WE
WOULD
LIKE
THIS
CHOICE
TO
BE
OPTIMAL
IN
SOME
APPROPRIATE
SENSE
DUDA
AND
HART
THIS
IS
THE
DECISION
STEP
AND
IT
IS
THE
SUBJECT
OF
DECISION
THEORY
TO
TELL
US
HOW
TO
MAKE
OPTIMAL
DECISIONS
GIVEN
THE
APPROPRIATE
PROBABILITIES
WE
SHALL
SEE
THAT
THE
DECISION
STAGE
IS
GENERALLY
VERY
SIMPLE
EVEN
TRIVIAL
ONCE
WE
HAVE
SOLVED
THE
INFERENCE
PROBLEM
HERE
WE
GIVE
AN
INTRODUCTION
TO
THE
KEY
IDEAS
OF
DECISION
THEORY
AS
REQUIRED
FOR
THE
REST
OF
THE
BOOK
FURTHER
BACKGROUND
AS
WELL
AS
MORE
DETAILED
ACCOUNTS
CAN
BE
FOUND
IN
BERGER
AND
BATHER
BEFORE
GIVING
A
MORE
DETAILED
ANALYSIS
LET
US
FIRST
CONSIDER
INFORMALLY
HOW
WE
MIGHT
EXPECT
PROBABILITIES
TO
PLAY
A
ROLE
IN
MAKING
DECISIONS
WHEN
WE
OBTAIN
THE
X
RAY
IMAGE
X
FOR
A
NEW
PATIENT
OUR
GOAL
IS
TO
DECIDE
WHICH
OF
THE
TWO
CLASSES
TO
ASSIGN
TO
THE
IMAGE
WE
ARE
INTERESTED
IN
THE
PROBABILITIES
OF
THE
TWO
CLASSES
GIVEN
THE
IMAGE
WHICH
ARE
GIVEN
BY
P
K
X
USING
BAYES
THEOREM
THESE
PROBABILITIES
CAN
BE
EXPRESSED
IN
THE
FORM
P
CK
X
P
X
CK
P
CK
P
X
NOTE
THAT
ANY
OF
THE
QUANTITIES
APPEARING
IN
BAYES
THEOREM
CAN
BE
OBTAINED
FROM
THE
JOINT
DISTRIBUTION
P
X
K
BY
EITHER
MARGINALIZING
OR
CONDITIONING
WITH
RESPECT
TO
THE
APPROPRIATE
VARIABLES
WE
CAN
NOW
INTERPRET
P
K
AS
THE
PRIOR
PROBABILITY
FOR
THE
CLASS
K
AND
P
K
X
AS
THE
CORRESPONDING
POSTERIOR
PROBABILITY
THUS
P
REPRE
SENTS
THE
PROBABILITY
THAT
A
PERSON
HAS
CANCER
BEFORE
WE
TAKE
THE
X
RAY
MEASUREMENT
SIMILARLY
P
X
IS
THE
CORRESPONDING
PROBABILITY
REVISED
USING
BAYES
THEOREM
IN
LIGHT
OF
THE
INFORMATION
CONTAINED
IN
THE
X
RAY
IF
OUR
AIM
IS
TO
MINIMIZE
THE
CHANCE
OF
ASSIGNING
X
TO
THE
WRONG
CLASS
THEN
INTUITIVELY
WE
WOULD
CHOOSE
THE
CLASS
HAVING
THE
HIGHER
POSTERIOR
PROBABILITY
WE
NOW
SHOW
THAT
THIS
INTUITION
IS
CORRECT
AND
WE
ALSO
DISCUSS
MORE
GENERAL
CRITERIA
FOR
MAKING
DECISIONS
MINIMIZING
THE
MISCLASSIFICATION
RATE
SUPPOSE
THAT
OUR
GOAL
IS
SIMPLY
TO
MAKE
AS
FEW
MISCLASSIFICATIONS
AS
POSSIBLE
WE
NEED
A
RULE
THAT
ASSIGNS
EACH
VALUE
OF
X
TO
ONE
OF
THE
AVAILABLE
CLASSES
SUCH
A
RULE
WILL
DIVIDE
THE
INPUT
SPACE
INTO
REGIONS
K
CALLED
DECISION
REGIONS
ONE
FOR
EACH
CLASS
SUCH
THAT
ALL
POINTS
IN
K
ARE
ASSIGNED
TO
CLASS
K
THE
BOUNDARIES
BETWEEN
DECISION
REGIONS
ARE
CALLED
DECISION
BOUNDARIES
OR
DECISION
SURFACES
NOTE
THAT
EACH
DECISION
REGION
NEED
NOT
BE
CONTIGUOUS
BUT
COULD
COMPRISE
SOME
NUMBER
OF
DISJOINT
REGIONS
WE
SHALL
ENCOUNTER
EXAMPLES
OF
DECISION
BOUNDARIES
AND
DECISION
REGIONS
IN
LATER
CHAPTERS
IN
ORDER
TO
FIND
THE
OPTIMAL
DECISION
RULE
CONSIDER
FIRST
OF
ALL
THE
CASE
OF
TWO
CLASSES
AS
IN
THE
CANCER
PROBLEM
FOR
INSTANCE
A
MISTAKE
OCCURS
WHEN
AN
INPUT
VECTOR
BELONGING
TO
CLASS
IS
ASSIGNED
TO
CLASS
OR
VICE
VERSA
THE
PROBABILITY
OF
THIS
OCCURRING
IS
GIVEN
BY
P
MISTAKE
P
X
P
X
P
X
DX
P
X
DX
WE
ARE
FREE
TO
CHOOSE
THE
DECISION
RULE
THAT
ASSIGNS
EACH
POINT
X
TO
ONE
OF
THE
TWO
CLASSES
CLEARLY
TO
MINIMIZE
P
MISTAKE
WE
SHOULD
ARRANGE
THAT
EACH
X
IS
ASSIGNED
TO
WHICHEVER
CLASS
HAS
THE
SMALLER
VALUE
OF
THE
INTEGRAND
IN
THUS
IF
P
X
P
X
FOR
A
GIVEN
VALUE
OF
X
THEN
WE
SHOULD
ASSIGN
THAT
X
TO
CLASS
FROM
THE
PRODUCT
RULE
OF
PROBABILITY
WE
HAVE
P
X
K
P
K
X
P
X
BECAUSE
THE
FACTOR
P
X
IS
COMMON
TO
BOTH
TERMS
WE
CAN
RESTATE
THIS
RESULT
AS
SAYING
THAT
THE
MINIMUM
FIGURE
SCHEMATIC
ILLUSTRATION
OF
THE
JOINT
PROBABILITIES
P
X
K
FOR
EACH
OF
TWO
CLASSES
PLOTTED
AGAINST
X
TOGETHER
WITH
THE
DECISION
BOUNDARY
X
X
VALUES
OF
X
X
ARE
CLASSIFIED
AS
CLASS
AND
HENCE
BELONG
TO
DECISION
REGION
WHEREAS
POINTS
X
X
ARE
CLASSIFIED
AS
AND
BELONG
TO
ERRORS
ARISE
FROM
THE
BLUE
GREEN
AND
RED
REGIONS
SO
THAT
FOR
THE
ERRORS
ARE
DUE
TO
POINTS
FROM
CLASS
BEING
MISCLASSIFIED
AS
REPRESENTED
BY
X
X
THE
SUM
OF
THE
RED
AND
GREEN
REGIONS
AND
CONVERSELY
FOR
POINTS
IN
THE
REGION
X
X
THE
ERRORS
ARE
DUE
TO
POINTS
FROM
CLASS
BEING
MISCLASSIFIED
AS
REPRESENTED
BY
THE
BLUE
REGION
AS
WE
VARY
THE
LOCATION
X
OF
THE
DECISION
BOUNDARY
THE
COMBINED
AREAS
OF
THE
BLUE
AND
GREEN
REGIONS
REMAINS
CONSTANT
WHEREAS
THE
SIZE
OF
THE
RED
REGION
VARIES
THE
OPTIMAL
CHOICE
FOR
X
IS
WHERE
THE
CURVES
FOR
P
X
AND
P
X
CROSS
CORRESPONDING
TO
MISCLASSIFICATION
RATE
DECISION
RULE
WHICH
ASSIGNS
EACH
VALUE
OF
X
TO
THE
CLASS
HAVING
THE
HIGHER
POSTERIOR
PROBABILITY
P
CK
X
PROBABILITY
OF
MAKING
A
MISTAKE
IS
OBTAINED
IF
EACH
VALUE
OF
X
IS
ASSIGNED
TO
THE
CLASS
FOR
WHICH
THE
POSTERIOR
PROBABILITY
P
K
X
IS
LARGEST
THIS
RESULT
IS
ILLUSTRATED
FOR
TWO
CLASSES
AND
A
SINGLE
INPUT
VARIABLE
X
IN
FIGURE
FOR
THE
MORE
GENERAL
CASE
OF
K
CLASSES
IT
IS
SLIGHTLY
EASIER
TO
MAXIMIZE
THE
PROBABILITY
OF
BEING
CORRECT
WHICH
IS
GIVEN
BY
P
CORRECT
K
P
X
RK
CK
K
K
RK
P
X
CK
DX
WHICH
IS
MAXIMIZED
WHEN
THE
REGIONS
K
ARE
CHOSEN
SUCH
THAT
EACH
X
IS
ASSIGNED
TO
THE
CLASS
FOR
WHICH
P
X
K
IS
LARGEST
AGAIN
USING
THE
PRODUCT
RULE
P
X
K
P
K
X
P
X
AND
NOTING
THAT
THE
FACTOR
OF
P
X
IS
COMMON
TO
ALL
TERMS
WE
SEE
THAT
EACH
X
SHOULD
BE
ASSIGNED
TO
THE
CLASS
HAVING
THE
LARGEST
POSTERIOR
PROBABILITY
P
CK
X
FIGURE
AN
EXAMPLE
OF
A
LOSS
MATRIX
WITH
ELE
MENTS
LKJ
FOR
THE
CANCER
TREATMENT
PROBLEM
THE
ROWS
CORRESPOND
TO
THE
TRUE
CLASS
WHEREAS
THE
COLUMNS
COR
CANCER
CANCER
NORMAL
RESPOND
TO
THE
ASSIGNMENT
OF
CLASS
MADE
BY
OUR
DECI
SION
CRITERION
NORMAL
MINIMIZING
THE
EXPECTED
LOSS
FOR
MANY
APPLICATIONS
OUR
OBJECTIVE
WILL
BE
MORE
COMPLEX
THAN
SIMPLY
MINI
MIZING
THE
NUMBER
OF
MISCLASSIFICATIONS
LET
US
CONSIDER
AGAIN
THE
MEDICAL
DIAGNOSIS
PROBLEM
WE
NOTE
THAT
IF
A
PATIENT
WHO
DOES
NOT
HAVE
CANCER
IS
INCORRECTLY
DIAGNOSED
AS
HAVING
CANCER
THE
CONSEQUENCES
MAY
BE
SOME
PATIENT
DISTRESS
PLUS
THE
NEED
FOR
FURTHER
INVESTIGATIONS
CONVERSELY
IF
A
PATIENT
WITH
CANCER
IS
DIAGNOSED
AS
HEALTHY
THE
RESULT
MAY
BE
PREMATURE
DEATH
DUE
TO
LACK
OF
TREATMENT
THUS
THE
CONSEQUENCES
OF
THESE
TWO
TYPES
OF
MISTAKE
CAN
BE
DRAMATICALLY
DIFFERENT
IT
WOULD
CLEARLY
BE
BETTER
TO
MAKE
FEWER
MISTAKES
OF
THE
SECOND
KIND
EVEN
IF
THIS
WAS
AT
THE
EXPENSE
OF
MAKING
MORE
MISTAKES
OF
THE
FIRST
KIND
WE
CAN
FORMALIZE
SUCH
ISSUES
THROUGH
THE
INTRODUCTION
OF
A
LOSS
FUNCTION
ALSO
CALLED
A
COST
FUNCTION
WHICH
IS
A
SINGLE
OVERALL
MEASURE
OF
LOSS
INCURRED
IN
TAKING
ANY
OF
THE
AVAILABLE
DECISIONS
OR
ACTIONS
OUR
GOAL
IS
THEN
TO
MINIMIZE
THE
TOTAL
LOSS
INCURRED
NOTE
THAT
SOME
AUTHORS
CONSIDER
INSTEAD
A
UTILITY
FUNCTION
WHOSE
VALUE
THEY
AIM
TO
MAXIMIZE
THESE
ARE
EQUIVALENT
CONCEPTS
IF
WE
TAKE
THE
UTILITY
TO
BE
SIMPLY
THE
NEGATIVE
OF
THE
LOSS
AND
THROUGHOUT
THIS
TEXT
WE
SHALL
USE
THE
LOSS
FUNCTION
CONVENTION
SUPPOSE
THAT
FOR
A
NEW
VALUE
OF
X
THE
TRUE
CLASS
IS
K
AND
THAT
WE
ASSIGN
X
TO
CLASS
J
WHERE
J
MAY
OR
MAY
NOT
BE
EQUAL
TO
K
IN
SO
DOING
WE
INCUR
SOME
LEVEL
OF
LOSS
THAT
WE
DENOTE
BY
LKJ
WHICH
WE
CAN
VIEW
AS
THE
K
J
ELEMENT
OF
A
LOSS
MATRIX
FOR
INSTANCE
IN
OUR
CANCER
EXAMPLE
WE
MIGHT
HAVE
A
LOSS
MATRIX
OF
THE
FORM
SHOWN
IN
FIGURE
THIS
PARTICULAR
LOSS
MATRIX
SAYS
THAT
THERE
IS
NO
LOSS
INCURRED
IF
THE
CORRECT
DECISION
IS
MADE
THERE
IS
A
LOSS
OF
IF
A
HEALTHY
PATIENT
IS
DIAGNOSED
AS
HAVING
CANCER
WHEREAS
THERE
IS
A
LOSS
OF
IF
A
PATIENT
HAVING
CANCER
IS
DIAGNOSED
AS
HEALTHY
THE
OPTIMAL
SOLUTION
IS
THE
ONE
WHICH
MINIMIZES
THE
LOSS
FUNCTION
HOWEVER
THE
LOSS
FUNCTION
DEPENDS
ON
THE
TRUE
CLASS
WHICH
IS
UNKNOWN
FOR
A
GIVEN
INPUT
VECTOR
X
OUR
UNCERTAINTY
IN
THE
TRUE
CLASS
IS
EXPRESSED
THROUGH
THE
JOINT
PROBABILITY
DISTRIBUTION
P
X
K
AND
SO
WE
SEEK
INSTEAD
TO
MINIMIZE
THE
AVERAGE
LOSS
WHERE
THE
AVERAGE
IS
COMPUTED
WITH
RESPECT
TO
THIS
DISTRIBUTION
WHICH
IS
GIVEN
BY
E
L
LKJP
X
CK
DX
EACH
X
CAN
BE
ASSIGNED
INDEPENDENTLY
TO
ONE
OF
THE
DECISION
REGIONS
RJ
OUR
GOAL
THE
PRODUCT
RULE
P
X
K
P
K
X
P
X
TO
ELIMINATE
THE
COMMON
FACTOR
OF
P
X
THUS
THE
DECISION
RULE
THAT
MINIMIZES
THE
EXPECTED
LOSS
IS
THE
ONE
THAT
ASSIGNS
EACH
FIGURE
ILLUSTRATION
OF
THE
REJECT
OPTION
INPUTS
X
SUCH
THAT
THE
LARGER
OF
THE
TWO
POSTE
RIOR
PROBABILITIES
IS
LESS
THAN
OR
EQUAL
TO
SOME
THRESHOLD
Θ
WILL
BE
REJECTED
Θ
P
X
P
X
X
REJECT
REGION
EXERCISE
NEW
X
TO
THE
CLASS
J
FOR
WHICH
THE
QUANTITY
LKJP
CK
X
K
IS
A
MINIMUM
THIS
IS
CLEARLY
TRIVIAL
TO
DO
ONCE
WE
KNOW
THE
POSTERIOR
CLASS
PROBA
BILITIES
P
CK
X
THE
REJECT
OPTION
WE
HAVE
SEEN
THAT
CLASSIFICATION
ERRORS
ARISE
FROM
THE
REGIONS
OF
INPUT
SPACE
WHERE
THE
LARGEST
OF
THE
POSTERIOR
PROBABILITIES
P
K
X
IS
SIGNIFICANTLY
LESS
THAN
UNITY
OR
EQUIVALENTLY
WHERE
THE
JOINT
DISTRIBUTIONS
P
X
K
HAVE
COMPARABLE
VALUES
THESE
ARE
THE
REGIONS
WHERE
WE
ARE
RELATIVELY
UNCERTAIN
ABOUT
CLASS
MEMBERSHIP
IN
SOME
APPLICATIONS
IT
WILL
BE
APPROPRIATE
TO
AVOID
MAKING
DECISIONS
ON
THE
DIFFICULT
CASES
IN
ANTICIPATION
OF
A
LOWER
ERROR
RATE
ON
THOSE
EXAMPLES
FOR
WHICH
A
CLASSIFICATION
DE
CISION
IS
MADE
THIS
IS
KNOWN
AS
THE
REJECT
OPTION
FOR
EXAMPLE
IN
OUR
HYPOTHETICAL
MEDICAL
ILLUSTRATION
IT
MAY
BE
APPROPRIATE
TO
USE
AN
AUTOMATIC
SYSTEM
TO
CLASSIFY
THOSE
X
RAY
IMAGES
FOR
WHICH
THERE
IS
LITTLE
DOUBT
AS
TO
THE
CORRECT
CLASS
WHILE
LEAV
ING
A
HUMAN
EXPERT
TO
CLASSIFY
THE
MORE
AMBIGUOUS
CASES
WE
CAN
ACHIEVE
THIS
BY
INTRODUCING
A
THRESHOLD
Θ
AND
REJECTING
THOSE
INPUTS
X
FOR
WHICH
THE
LARGEST
OF
THE
POSTERIOR
PROBABILITIES
P
K
X
IS
LESS
THAN
OR
EQUAL
TO
Θ
THIS
IS
ILLUSTRATED
FOR
THE
CASE
OF
TWO
CLASSES
AND
A
SINGLE
CONTINUOUS
INPUT
VARIABLE
X
IN
FIGURE
NOTE
THAT
SETTING
Θ
WILL
ENSURE
THAT
ALL
EXAMPLES
ARE
REJECTED
WHEREAS
IF
THERE
ARE
K
CLASSES
THEN
SETTING
Θ
K
WILL
ENSURE
THAT
NO
EXAMPLES
ARE
REJECTED
THUS
THE
FRACTION
OF
EXAMPLES
THAT
GET
REJECTED
IS
CONTROLLED
BY
THE
VALUE
OF
Θ
WE
CAN
EASILY
EXTEND
THE
REJECT
CRITERION
TO
MINIMIZE
THE
EXPECTED
LOSS
WHEN
A
LOSS
MATRIX
IS
GIVEN
TAKING
ACCOUNT
OF
THE
LOSS
INCURRED
WHEN
A
REJECT
DECISION
IS
MADE
INFERENCE
AND
DECISION
WE
HAVE
BROKEN
THE
CLASSIFICATION
PROBLEM
DOWN
INTO
TWO
SEPARATE
STAGES
THE
INFERENCE
STAGE
IN
WHICH
WE
USE
TRAINING
DATA
TO
LEARN
A
MODEL
FOR
P
CK
X
AND
THE
SUBSEQUENT
DECISION
STAGE
IN
WHICH
WE
USE
THESE
POSTERIOR
PROBABILITIES
TO
MAKE
OP
TIMAL
CLASS
ASSIGNMENTS
AN
ALTERNATIVE
POSSIBILITY
WOULD
BE
TO
SOLVE
BOTH
PROBLEMS
TOGETHER
AND
SIMPLY
LEARN
A
FUNCTION
THAT
MAPS
INPUTS
X
DIRECTLY
INTO
DECISIONS
SUCH
A
FUNCTION
IS
CALLED
A
DISCRIMINANT
FUNCTION
IN
FACT
WE
CAN
IDENTIFY
THREE
DISTINCT
APPROACHES
TO
SOLVING
DECISION
PROBLEMS
ALL
OF
WHICH
HAVE
BEEN
USED
IN
PRACTICAL
APPLICATIONS
THESE
ARE
GIVEN
IN
DECREASING
ORDER
OF
COMPLEXITY
BY
A
FIRST
SOLVE
THE
INFERENCE
PROBLEM
OF
DETERMINING
THE
CLASS
CONDITIONAL
DENSITIES
P
X
CK
FOR
EACH
CLASS
CK
INDIVIDUALLY
ALSO
SEPARATELY
INFER
THE
PRIOR
CLASS
PROBABILITIES
P
CK
THEN
USE
BAYES
THEOREM
IN
THE
FORM
P
CK
X
P
X
CK
P
CK
P
X
TO
FIND
THE
POSTERIOR
CLASS
PROBABILITIES
P
K
X
AS
USUAL
THE
DENOMINATOR
IN
BAYES
THEOREM
CAN
BE
FOUND
IN
TERMS
OF
THE
QUANTITIES
APPEARING
IN
THE
NUMERATOR
BECAUSE
P
X
P
X
CK
P
CK
K
EQUIVALENTLY
WE
CAN
MODEL
THE
JOINT
DISTRIBUTION
P
X
K
DIRECTLY
AND
THEN
NORMALIZE
TO
OBTAIN
THE
POSTERIOR
PROBABILITIES
HAVING
FOUND
THE
POSTERIOR
PROBABILITIES
WE
USE
DECISION
THEORY
TO
DETERMINE
CLASS
MEMBERSHIP
FOR
EACH
NEW
INPUT
X
APPROACHES
THAT
EXPLICITLY
OR
IMPLICITLY
MODEL
THE
DISTRIBUTION
OF
INPUTS
AS
WELL
AS
OUTPUTS
ARE
KNOWN
AS
GENERATIVE
MODELS
BECAUSE
BY
SAMPLING
FROM
THEM
IT
IS
POSSIBLE
TO
GENERATE
SYNTHETIC
DATA
POINTS
IN
THE
INPUT
SPACE
B
FIRST
SOLVE
THE
INFERENCE
PROBLEM
OF
DETERMINING
THE
POSTERIOR
CLASS
PROBABILITIES
P
K
X
AND
THEN
SUBSEQUENTLY
USE
DECISION
THEORY
TO
ASSIGN
EACH
NEW
X
TO
ONE
OF
THE
CLASSES
APPROACHES
THAT
MODEL
THE
POSTERIOR
PROBABILITIES
DIRECTLY
ARE
CALLED
DISCRIMINATIVE
MODELS
C
FIND
A
FUNCTION
F
X
CALLED
A
DISCRIMINANT
FUNCTION
WHICH
MAPS
EACH
INPUT
X
DIRECTLY
ONTO
A
CLASS
LABEL
FOR
INSTANCE
IN
THE
CASE
OF
TWO
CLASS
PROBLEMS
F
MIGHT
BE
BINARY
VALUED
AND
SUCH
THAT
F
REPRESENTS
CLASS
AND
F
REPRESENTS
CLASS
IN
THIS
CASE
PROBABILITIES
PLAY
NO
ROLE
LET
US
CONSIDER
THE
RELATIVE
MERITS
OF
THESE
THREE
ALTERNATIVES
APPROACH
A
IS
THE
MOST
DEMANDING
BECAUSE
IT
INVOLVES
FINDING
THE
JOINT
DISTRIBUTION
OVER
BOTH
X
AND
K
FOR
MANY
APPLICATIONS
X
WILL
HAVE
HIGH
DIMENSIONALITY
AND
CONSEQUENTLY
WE
MAY
NEED
A
LARGE
TRAINING
SET
IN
ORDER
TO
BE
ABLE
TO
DETERMINE
THE
CLASS
CONDITIONAL
DENSITIES
TO
REASONABLE
ACCURACY
NOTE
THAT
THE
CLASS
PRIORS
P
K
CAN
OFTEN
BE
ESTI
MATED
SIMPLY
FROM
THE
FRACTIONS
OF
THE
TRAINING
SET
DATA
POINTS
IN
EACH
OF
THE
CLASSES
ONE
ADVANTAGE
OF
APPROACH
A
HOWEVER
IS
THAT
IT
ALSO
ALLOWS
THE
MARGINAL
DENSITY
OF
DATA
P
X
TO
BE
DETERMINED
FROM
THIS
CAN
BE
USEFUL
FOR
DETECTING
NEW
DATA
POINTS
THAT
HAVE
LOW
PROBABILITY
UNDER
THE
MODEL
AND
FOR
WHICH
THE
PREDICTIONS
MAY
X
X
FIGURE
EXAMPLE
OF
THE
CLASS
CONDITIONAL
DENSITIES
FOR
TWO
CLASSES
HAVING
A
SINGLE
INPUT
VARIABLE
X
LEFT
PLOT
TOGETHER
WITH
THE
CORRESPONDING
POSTERIOR
PROBABILITIES
RIGHT
PLOT
NOTE
THAT
THE
LEFT
HAND
MODE
OF
THE
CLASS
CONDITIONAL
DENSITY
P
X
SHOWN
IN
BLUE
ON
THE
LEFT
PLOT
HAS
NO
EFFECT
ON
THE
POSTERIOR
PROBABILITIES
THE
VERTICAL
GREEN
LINE
IN
THE
RIGHT
PLOT
SHOWS
THE
DECISION
BOUNDARY
IN
X
THAT
GIVES
THE
MINIMUM
MISCLASSIFICATION
RATE
BE
OF
LOW
ACCURACY
WHICH
IS
KNOWN
AS
OUTLIER
DETECTION
OR
NOVELTY
DETECTION
BISHOP
TARASSENKO
HOWEVER
IF
WE
ONLY
WISH
TO
MAKE
CLASSIFICATION
DECISIONS
THEN
IT
CAN
BE
WASTE
FUL
OF
COMPUTATIONAL
RESOURCES
AND
EXCESSIVELY
DEMANDING
OF
DATA
TO
FIND
THE
JOINT
DISTRIBUTION
P
X
K
WHEN
IN
FACT
WE
ONLY
REALLY
NEED
THE
POSTERIOR
PROBABILITIES
P
K
X
WHICH
CAN
BE
OBTAINED
DIRECTLY
THROUGH
APPROACH
B
INDEED
THE
CLASS
CONDITIONAL
DENSITIES
MAY
CONTAIN
A
LOT
OF
STRUCTURE
THAT
HAS
LITTLE
EFFECT
ON
THE
POS
TERIOR
PROBABILITIES
AS
ILLUSTRATED
IN
FIGURE
THERE
HAS
BEEN
MUCH
INTEREST
IN
EXPLORING
THE
RELATIVE
MERITS
OF
GENERATIVE
AND
DISCRIMINATIVE
APPROACHES
TO
MACHINE
LEARNING
AND
IN
FINDING
WAYS
TO
COMBINE
THEM
JEBARA
LASSERRE
ET
AL
AN
EVEN
SIMPLER
APPROACH
IS
C
IN
WHICH
WE
USE
THE
TRAINING
DATA
TO
FIND
A
DISCRIMINANT
FUNCTION
F
X
THAT
MAPS
EACH
X
DIRECTLY
ONTO
A
CLASS
LABEL
THEREBY
COMBINING
THE
INFERENCE
AND
DECISION
STAGES
INTO
A
SINGLE
LEARNING
PROBLEM
IN
THE
EXAMPLE
OF
FIGURE
THIS
WOULD
CORRESPOND
TO
FINDING
THE
VALUE
OF
X
SHOWN
BY
THE
VERTICAL
GREEN
LINE
BECAUSE
THIS
IS
THE
DECISION
BOUNDARY
GIVING
THE
MINIMUM
PROBABILITY
OF
MISCLASSIFICATION
WITH
OPTION
C
HOWEVER
WE
NO
LONGER
HAVE
ACCESS
TO
THE
POSTERIOR
PROBABILITIES
P
K
X
THERE
ARE
MANY
POWERFUL
REASONS
FOR
WANTING
TO
COMPUTE
THE
POSTERIOR
PROBABILITIES
EVEN
IF
WE
SUBSEQUENTLY
USE
THEM
TO
MAKE
DECISIONS
THESE
INCLUDE
MINIMIZING
RISK
CONSIDER
A
PROBLEM
IN
WHICH
THE
ELEMENTS
OF
THE
LOSS
MATRIX
ARE
SUBJECTED
TO
REVISION
FROM
TIME
TO
TIME
SUCH
AS
MIGHT
OCCUR
IN
A
FINANCIAL
APPLICATION
IF
WE
KNOW
THE
POSTERIOR
PROBABILITIES
WE
CAN
TRIVIALLY
REVISE
THE
MINIMUM
RISK
DECISION
CRITERION
BY
MODIFYING
APPROPRIATELY
IF
WE
HAVE
ONLY
A
DISCRIMINANT
FUNCTION
THEN
ANY
CHANGE
TO
THE
LOSS
MATRIX
WOULD
REQUIRE
THAT
WE
RETURN
TO
THE
TRAINING
DATA
AND
SOLVE
THE
CLASSIFICATION
PROBLEM
AFRESH
REJECT
OPTION
POSTERIOR
PROBABILITIES
ALLOW
US
TO
DETERMINE
A
REJECTION
CRITERION
THAT
WILL
MINIMIZE
THE
MISCLASSIFICATION
RATE
OR
MORE
GENERALLY
THE
EXPECTED
LOSS
FOR
A
GIVEN
FRACTION
OF
REJECTED
DATA
POINTS
COMPENSATING
FOR
CLASS
PRIORS
CONSIDER
OUR
MEDICAL
X
RAY
PROBLEM
AGAIN
AND
SUPPOSE
THAT
WE
HAVE
COLLECTED
A
LARGE
NUMBER
OF
X
RAY
IMAGES
FROM
THE
GEN
ERAL
POPULATION
FOR
USE
AS
TRAINING
DATA
IN
ORDER
TO
BUILD
AN
AUTOMATED
SCREENING
SYSTEM
BECAUSE
CANCER
IS
RARE
AMONGST
THE
GENERAL
POPULATION
WE
MIGHT
FIND
THAT
SAY
ONLY
IN
EVERY
EXAMPLES
CORRESPONDS
TO
THE
PRESENCE
OF
CAN
CER
IF
WE
USED
SUCH
A
DATA
SET
TO
TRAIN
AN
ADAPTIVE
MODEL
WE
COULD
RUN
INTO
SEVERE
DIFFICULTIES
DUE
TO
THE
SMALL
PROPORTION
OF
THE
CANCER
CLASS
FOR
INSTANCE
A
CLASSIFIER
THAT
ASSIGNED
EVERY
POINT
TO
THE
NORMAL
CLASS
WOULD
ALREADY
ACHIEVE
ACCURACY
AND
IT
WOULD
BE
DIFFICULT
TO
AVOID
THIS
TRIVIAL
SOLUTION
ALSO
EVEN
A
LARGE
DATA
SET
WILL
CONTAIN
VERY
FEW
EXAMPLES
OF
X
RAY
IMAGES
CORRE
SPONDING
TO
CANCER
AND
SO
THE
LEARNING
ALGORITHM
WILL
NOT
BE
EXPOSED
TO
A
BROAD
RANGE
OF
EXAMPLES
OF
SUCH
IMAGES
AND
HENCE
IS
NOT
LIKELY
TO
GENERALIZE
WELL
A
BALANCED
DATA
SET
IN
WHICH
WE
HAVE
SELECTED
EQUAL
NUMBERS
OF
EXAM
PLES
FROM
EACH
OF
THE
CLASSES
WOULD
ALLOW
US
TO
FIND
A
MORE
ACCURATE
MODEL
HOWEVER
WE
THEN
HAVE
TO
COMPENSATE
FOR
THE
EFFECTS
OF
OUR
MODIFICATIONS
TO
THE
TRAINING
DATA
SUPPOSE
WE
HAVE
USED
SUCH
A
MODIFIED
DATA
SET
AND
FOUND
MODELS
FOR
THE
POSTERIOR
PROBABILITIES
FROM
BAYES
THEOREM
WE
SEE
THAT
THE
POSTERIOR
PROBABILITIES
ARE
PROPORTIONAL
TO
THE
PRIOR
PROBABILITIES
WHICH
WE
CAN
INTERPRET
AS
THE
FRACTIONS
OF
POINTS
IN
EACH
CLASS
WE
CAN
THEREFORE
SIMPLY
TAKE
THE
POSTERIOR
PROBABILITIES
OBTAINED
FROM
OUR
ARTIFICIALLY
BALANCED
DATA
SET
AND
FIRST
DIVIDE
BY
THE
CLASS
FRACTIONS
IN
THAT
DATA
SET
AND
THEN
MULTIPLY
BY
THE
CLASS
FRACTIONS
IN
THE
POPULATION
TO
WHICH
WE
WISH
TO
APPLY
THE
MODEL
FINALLY
WE
NEED
TO
NORMALIZE
TO
ENSURE
THAT
THE
NEW
POSTERIOR
PROBABILITIES
SUM
TO
ONE
NOTE
THAT
THIS
PROCEDURE
CANNOT
BE
APPLIED
IF
WE
HAVE
LEARNED
A
DISCRIMINANT
FUNCTION
DIRECTLY
INSTEAD
OF
DETERMINING
POSTERIOR
PROBABILITIES
COMBINING
MODELS
FOR
COMPLEX
APPLICATIONS
WE
MAY
WISH
TO
BREAK
THE
PROBLEM
INTO
A
NUMBER
OF
SMALLER
SUBPROBLEMS
EACH
OF
WHICH
CAN
BE
TACKLED
BY
A
SEP
ARATE
MODULE
FOR
EXAMPLE
IN
OUR
HYPOTHETICAL
MEDICAL
DIAGNOSIS
PROBLEM
WE
MAY
HAVE
INFORMATION
AVAILABLE
FROM
SAY
BLOOD
TESTS
AS
WELL
AS
X
RAY
IM
AGES
RATHER
THAN
COMBINE
ALL
OF
THIS
HETEROGENEOUS
INFORMATION
INTO
ONE
HUGE
INPUT
SPACE
IT
MAY
BE
MORE
EFFECTIVE
TO
BUILD
ONE
SYSTEM
TO
INTERPRET
THE
X
RAY
IMAGES
AND
A
DIFFERENT
ONE
TO
INTERPRET
THE
BLOOD
DATA
AS
LONG
AS
EACH
OF
THE
TWO
MODELS
GIVES
POSTERIOR
PROBABILITIES
FOR
THE
CLASSES
WE
CAN
COMBINE
THE
OUTPUTS
SYSTEMATICALLY
USING
THE
RULES
OF
PROBABILITY
ONE
SIMPLE
WAY
TO
DO
THIS
IS
TO
ASSUME
THAT
FOR
EACH
CLASS
SEPARATELY
THE
DISTRIBUTIONS
OF
INPUTS
FOR
THE
X
RAY
IMAGES
DENOTED
BY
XI
AND
THE
BLOOD
DATA
DENOTED
BY
XB
ARE
INDEPENDENT
SO
THAT
P
XI
XB
CK
P
XI
CK
P
XB
CK
SECTION
THIS
IS
AN
EXAMPLE
OF
CONDITIONAL
INDEPENDENCE
PROPERTY
BECAUSE
THE
INDEPEN
DENCE
HOLDS
WHEN
THE
DISTRIBUTION
IS
CONDITIONED
ON
THE
CLASS
K
THE
POSTERIOR
PROBABILITY
GIVEN
BOTH
THE
X
RAY
AND
BLOOD
DATA
IS
THEN
GIVEN
BY
P
CK
XI
XB
P
XI
XB
CK
P
CK
P
XI
CK
P
XB
CK
P
CK
P
CK
XI
P
CK
XB
P
CK
SECTION
SECTION
APPENDIX
D
THUS
WE
NEED
THE
CLASS
PRIOR
PROBABILITIES
P
K
WHICH
WE
CAN
EASILY
ESTIMATE
FROM
THE
FRACTIONS
OF
DATA
POINTS
IN
EACH
CLASS
AND
THEN
WE
NEED
TO
NORMALIZE
THE
RESULTING
POSTERIOR
PROBABILITIES
SO
THEY
SUM
TO
ONE
THE
PARTICULAR
CONDI
TIONAL
INDEPENDENCE
ASSUMPTION
IS
AN
EXAMPLE
OF
THE
NAIVE
BAYES
MODEL
NOTE
THAT
THE
JOINT
MARGINAL
DISTRIBUTION
P
XI
XB
WILL
TYPICALLY
NOT
FACTORIZE
UNDER
THIS
MODEL
WE
SHALL
SEE
IN
LATER
CHAPTERS
HOW
TO
CONSTRUCT
MODELS
FOR
COMBINING
DATA
THAT
DO
NOT
REQUIRE
THE
CONDITIONAL
INDEPENDENCE
ASSUMPTION
LOSS
FUNCTIONS
FOR
REGRESSION
SO
FAR
WE
HAVE
DISCUSSED
DECISION
THEORY
IN
THE
CONTEXT
OF
CLASSIFICATION
PROB
LEMS
WE
NOW
TURN
TO
THE
CASE
OF
REGRESSION
PROBLEMS
SUCH
AS
THE
CURVE
FITTING
EXAMPLE
DISCUSSED
EARLIER
THE
DECISION
STAGE
CONSISTS
OF
CHOOSING
A
SPECIFIC
ESTI
MATE
Y
X
OF
THE
VALUE
OF
T
FOR
EACH
INPUT
X
SUPPOSE
THAT
IN
DOING
SO
WE
INCUR
A
LOSS
L
T
Y
X
THE
AVERAGE
OR
EXPECTED
LOSS
IS
THEN
GIVEN
BY
E
L
L
T
Y
X
P
X
T
DX
DT
A
COMMON
CHOICE
OF
LOSS
FUNCTION
IN
REGRESSION
PROBLEMS
IS
THE
SQUARED
LOSS
GIVEN
BY
L
T
Y
X
Y
X
T
IN
THIS
CASE
THE
EXPECTED
LOSS
CAN
BE
WRITTEN
E
L
Y
X
T
X
T
DX
DT
OUR
GOAL
IS
TO
CHOOSE
Y
X
SO
AS
TO
MINIMIZE
E
L
IF
WE
ASSUME
A
COMPLETELY
FLEXIBLE
FUNCTION
Y
X
WE
CAN
DO
THIS
FORMALLY
USING
THE
CALCULUS
OF
VARIATIONS
TO
GIVE
ΔE
L
Y
X
T
P
X
T
DT
SOLVING
FOR
Y
X
AND
USING
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
WE
OBTAIN
Y
X
TP
X
T
DT
P
X
TP
T
X
DT
ET
T
X
FIGURE
THE
REGRESSION
FUNCTION
Y
X
WHICH
MINIMIZES
THE
EXPECTED
SQUARED
LOSS
IS
GIVEN
BY
THE
MEAN
OF
THE
CONDITIONAL
DISTRI
BUTION
P
T
X
T
Y
X
EXERCISE
WHICH
IS
THE
CONDITIONAL
AVERAGE
OF
T
CONDITIONED
ON
X
AND
IS
KNOWN
AS
THE
REGRESSION
FUNCTION
THIS
RESULT
IS
ILLUSTRATED
IN
FIGURE
IT
CAN
READILY
BE
EXTENDED
TO
MUL
TIPLE
TARGET
VARIABLES
REPRESENTED
BY
THE
VECTOR
T
IN
WHICH
CASE
THE
OPTIMAL
SOLUTION
IS
THE
CONDITIONAL
AVERAGE
Y
X
ET
T
X
WE
CAN
ALSO
DERIVE
THIS
RESULT
IN
A
SLIGHTLY
DIFFERENT
WAY
WHICH
WILL
ALSO
SHED
LIGHT
ON
THE
NATURE
OF
THE
REGRESSION
PROBLEM
ARMED
WITH
THE
KNOWLEDGE
THAT
THE
OPTIMAL
SOLUTION
IS
THE
CONDITIONAL
EXPECTATION
WE
CAN
EXPAND
THE
SQUARE
TERM
AS
FOLLOWS
Y
X
T
Y
X
E
T
X
E
T
X
T
Y
X
E
T
X
Y
X
E
T
X
E
T
X
T
E
T
X
T
WHERE
TO
KEEP
THE
NOTATION
UNCLUTTERED
WE
USE
E
T
X
TO
DENOTE
ET
T
X
SUBSTITUTING
INTO
THE
LOSS
FUNCTION
AND
PERFORMING
THE
INTEGRAL
OVER
T
WE
SEE
THAT
THE
CROSS
TERM
VANISHES
AND
WE
OBTAIN
AN
EXPRESSION
FOR
THE
LOSS
FUNCTION
IN
THE
FORM
E
L
Y
X
E
T
X
P
X
DX
E
T
X
T
X
DX
THE
FUNCTION
Y
X
WE
SEEK
TO
DETERMINE
ENTERS
ONLY
IN
THE
FIRST
TERM
WHICH
WILL
BE
MINIMIZED
WHEN
Y
X
IS
EQUAL
TO
E
T
X
IN
WHICH
CASE
THIS
TERM
WILL
VANISH
THIS
IS
SIMPLY
THE
RESULT
THAT
WE
DERIVED
PREVIOUSLY
AND
THAT
SHOWS
THAT
THE
OPTIMAL
LEAST
SQUARES
PREDICTOR
IS
GIVEN
BY
THE
CONDITIONAL
MEAN
THE
SECOND
TERM
IS
THE
VARIANCE
OF
THE
DISTRIBUTION
OF
T
AVERAGED
OVER
X
IT
REPRESENTS
THE
INTRINSIC
VARIABILITY
OF
THE
TARGET
DATA
AND
CAN
BE
REGARDED
AS
NOISE
BECAUSE
IT
IS
INDEPENDENT
OF
Y
X
IT
REPRESENTS
THE
IRREDUCIBLE
MINIMUM
VALUE
OF
THE
LOSS
FUNCTION
AS
WITH
THE
CLASSIFICATION
PROBLEM
WE
CAN
EITHER
DETERMINE
THE
APPROPRIATE
PROB
ABILITIES
AND
THEN
USE
THESE
TO
MAKE
OPTIMAL
DECISIONS
OR
WE
CAN
BUILD
MODELS
THAT
MAKE
DECISIONS
DIRECTLY
INDEED
WE
CAN
IDENTIFY
THREE
DISTINCT
APPROACHES
TO
SOLVING
REGRESSION
PROBLEMS
GIVEN
IN
ORDER
OF
DECREASING
COMPLEXITY
BY
A
FIRST
SOLVE
THE
INFERENCE
PROBLEM
OF
DETERMINING
THE
JOINT
DENSITY
P
X
T
THEN
NORMALIZE
TO
FIND
THE
CONDITIONAL
DENSITY
P
T
X
AND
FINALLY
MARGINALIZE
TO
FIND
THE
CONDITIONAL
MEAN
GIVEN
BY
SECTION
B
FIRST
SOLVE
THE
INFERENCE
PROBLEM
OF
DETERMINING
THE
CONDITIONAL
DENSITY
P
T
X
AND
THEN
SUBSEQUENTLY
MARGINALIZE
TO
FIND
THE
CONDITIONAL
MEAN
GIVEN
BY
C
FIND
A
REGRESSION
FUNCTION
Y
X
DIRECTLY
FROM
THE
TRAINING
DATA
THE
RELATIVE
MERITS
OF
THESE
THREE
APPROACHES
FOLLOW
THE
SAME
LINES
AS
FOR
CLASSIFICA
TION
PROBLEMS
ABOVE
THE
SQUARED
LOSS
IS
NOT
THE
ONLY
POSSIBLE
CHOICE
OF
LOSS
FUNCTION
FOR
REGRESSION
INDEED
THERE
ARE
SITUATIONS
IN
WHICH
SQUARED
LOSS
CAN
LEAD
TO
VERY
POOR
RESULTS
AND
WHERE
WE
NEED
TO
DEVELOP
MORE
SOPHISTICATED
APPROACHES
AN
IMPORTANT
EXAMPLE
CONCERNS
SITUATIONS
IN
WHICH
THE
CONDITIONAL
DISTRIBUTION
P
T
X
IS
MULTIMODAL
AS
OFTEN
ARISES
IN
THE
SOLUTION
OF
INVERSE
PROBLEMS
HERE
WE
CONSIDER
BRIEFLY
ONE
SIMPLE
GENERALIZATION
OF
THE
SQUARED
LOSS
CALLED
THE
MINKOWSKI
LOSS
WHOSE
EXPECTATION
IS
GIVEN
BY
EXERCISE
E
LQ
Y
X
T
QP
X
T
DX
DT
WHICH
REDUCES
TO
THE
EXPECTED
SQUARED
LOSS
FOR
Q
THE
FUNCTION
Y
T
Q
IS
PLOTTED
AGAINST
Y
T
FOR
VARIOUS
VALUES
OF
Q
IN
FIGURE
THE
MINIMUM
OF
E
LQ
IS
GIVEN
BY
THE
CONDITIONAL
MEAN
FOR
Q
THE
CONDITIONAL
MEDIAN
FOR
Q
AND
THE
CONDITIONAL
MODE
FOR
Q
INFORMATION
THEORY
EXERCISE
IN
THIS
CHAPTER
WE
HAVE
DISCUSSED
A
VARIETY
OF
CONCEPTS
FROM
PROBABILITY
THEORY
AND
DECISION
THEORY
THAT
WILL
FORM
THE
FOUNDATIONS
FOR
MUCH
OF
THE
SUBSEQUENT
DISCUSSION
IN
THIS
BOOK
WE
CLOSE
THIS
CHAPTER
BY
INTRODUCING
SOME
ADDITIONAL
CONCEPTS
FROM
THE
FIELD
OF
INFORMATION
THEORY
WHICH
WILL
ALSO
PROVE
USEFUL
IN
OUR
DEVELOPMENT
OF
PATTERN
RECOGNITION
AND
MACHINE
LEARNING
TECHNIQUES
AGAIN
WE
SHALL
FOCUS
ONLY
ON
THE
KEY
CONCEPTS
AND
WE
REFER
THE
READER
ELSEWHERE
FOR
MORE
DETAILED
DISCUSSIONS
VITERBI
AND
OMURA
COVER
AND
THOMAS
MACKAY
WE
BEGIN
BY
CONSIDERING
A
DISCRETE
RANDOM
VARIABLE
X
AND
WE
ASK
HOW
MUCH
INFORMATION
IS
RECEIVED
WHEN
WE
OBSERVE
A
SPECIFIC
VALUE
FOR
THIS
VARIABLE
THE
AMOUNT
OF
INFORMATION
CAN
BE
VIEWED
AS
THE
DEGREE
OF
SURPRISE
ON
LEARNING
THE
VALUE
OF
X
IF
WE
ARE
TOLD
THAT
A
HIGHLY
IMPROBABLE
EVENT
HAS
JUST
OCCURRED
WE
WILL
HAVE
RECEIVED
MORE
INFORMATION
THAN
IF
WE
WERE
TOLD
THAT
SOME
VERY
LIKELY
EVENT
HAS
JUST
OCCURRED
AND
IF
WE
KNEW
THAT
THE
EVENT
WAS
CERTAIN
TO
HAPPEN
WE
WOULD
RECEIVE
NO
INFORMATION
OUR
MEASURE
OF
INFORMATION
CONTENT
WILL
THEREFORE
DEPEND
ON
THE
PROBABILITY
DISTRIBUTION
P
X
AND
WE
THEREFORE
LOOK
FOR
A
QUANTITY
H
X
THAT
IS
A
MONOTONIC
FUNCTION
OF
THE
PROBABILITY
P
X
AND
THAT
EXPRESSES
THE
INFORMATION
CONTENT
THE
FORM
OF
H
CAN
BE
FOUND
BY
NOTING
THAT
IF
WE
HAVE
TWO
EVENTS
X
AND
Y
THAT
ARE
UNRELATED
THEN
THE
INFORMATION
GAIN
FROM
OBSERVING
BOTH
OF
THEM
SHOULD
BE
THE
SUM
OF
THE
INFORMATION
GAINED
FROM
EACH
OF
THEM
SEPARATELY
SO
THAT
H
X
Y
H
X
H
Y
TWO
UNRELATED
EVENTS
WILL
BE
STATISTICALLY
INDEPENDENT
AND
SO
P
X
Y
P
X
P
Y
FROM
THESE
TWO
RELATIONSHIPS
IT
IS
EASILY
SHOWN
THAT
H
X
MUST
BE
GIVEN
BY
THE
LOGARITHM
OF
P
X
AND
SO
WE
HAVE
Y
T
Y
T
FIGURE
PLOTS
OF
THE
QUANTITY
LQ
Y
T
Q
FOR
VARIOUS
VALUES
OF
Q
H
X
P
X
WHERE
THE
NEGATIVE
SIGN
ENSURES
THAT
INFORMATION
IS
POSITIVE
OR
ZERO
NOTE
THAT
LOW
PROBABILITY
EVENTS
X
CORRESPOND
TO
HIGH
INFORMATION
CONTENT
THE
CHOICE
OF
BASIS
FOR
THE
LOGARITHM
IS
ARBITRARY
AND
FOR
THE
MOMENT
WE
SHALL
ADOPT
THE
CONVENTION
PREVALENT
IN
INFORMATION
THEORY
OF
USING
LOGARITHMS
TO
THE
BASE
OF
IN
THIS
CASE
AS
WE
SHALL
SEE
SHORTLY
THE
UNITS
OF
H
X
ARE
BITS
BINARY
DIGITS
NOW
SUPPOSE
THAT
A
SENDER
WISHES
TO
TRANSMIT
THE
VALUE
OF
A
RANDOM
VARIABLE
TO
A
RECEIVER
THE
AVERAGE
AMOUNT
OF
INFORMATION
THAT
THEY
TRANSMIT
IN
THE
PROCESS
IS
OBTAINED
BY
TAKING
THE
EXPECTATION
OF
WITH
RESPECT
TO
THE
DISTRIBUTION
P
X
AND
IS
GIVEN
BY
H
X
P
X
P
X
X
THIS
IMPORTANT
QUANTITY
IS
CALLED
THE
ENTROPY
OF
THE
RANDOM
VARIABLE
X
NOTE
THAT
LIMP
P
LN
P
AND
SO
WE
SHALL
TAKE
P
X
LN
P
X
WHENEVER
WE
ENCOUNTER
A
VALUE
FOR
X
SUCH
THAT
P
X
SO
FAR
WE
HAVE
GIVEN
A
RATHER
HEURISTIC
MOTIVATION
FOR
THE
DEFINITION
OF
INFORMA
TION
AND
THE
CORRESPONDING
ENTROPY
WE
NOW
SHOW
THAT
THESE
DEFINITIONS
INDEED
POSSESS
USEFUL
PROPERTIES
CONSIDER
A
RANDOM
VARIABLE
X
HAVING
POSSIBLE
STATES
EACH
OF
WHICH
IS
EQUALLY
LIKELY
IN
ORDER
TO
COMMUNICATE
THE
VALUE
OF
X
TO
A
RECEIVER
WE
WOULD
NEED
TO
TRANSMIT
A
MESSAGE
OF
LENGTH
BITS
NOTICE
THAT
THE
ENTROPY
OF
THIS
VARIABLE
IS
GIVEN
BY
H
X
LOG
BITS
NOW
CONSIDER
AN
EXAMPLE
COVER
AND
THOMAS
OF
A
VARIABLE
HAVING
POS
SIBLE
STATES
A
B
C
D
E
F
G
H
FOR
WHICH
THE
RESPECTIVE
PROBABILITIES
ARE
GIVEN
BY
THE
ENTROPY
IN
THIS
CASE
IS
GIVEN
BY
H
X
LOG
LOG
LOG
LOG
LOG
BITS
WE
SEE
THAT
THE
NONUNIFORM
DISTRIBUTION
HAS
A
SMALLER
ENTROPY
THAN
THE
UNIFORM
ONE
AND
WE
SHALL
GAIN
SOME
INSIGHT
INTO
THIS
SHORTLY
WHEN
WE
DISCUSS
THE
INTERPRETATION
OF
ENTROPY
IN
TERMS
OF
DISORDER
FOR
THE
MOMENT
LET
US
CONSIDER
HOW
WE
WOULD
TRANSMIT
THE
IDENTITY
OF
THE
VARIABLE
STATE
TO
A
RECEIVER
WE
COULD
DO
THIS
AS
BEFORE
USING
A
BIT
NUMBER
HOWEVER
WE
CAN
TAKE
ADVANTAGE
OF
THE
NONUNIFORM
DISTRIBUTION
BY
USING
SHORTER
CODES
FOR
THE
MORE
PROBABLE
EVENTS
AT
THE
EXPENSE
OF
LONGER
CODES
FOR
THE
LESS
PROBABLE
EVENTS
IN
THE
HOPE
OF
GETTING
A
SHORTER
AVERAGE
CODE
LENGTH
THIS
CAN
BE
DONE
BY
REPRESENTING
THE
STATES
A
B
C
D
E
F
G
H
USING
FOR
INSTANCE
THE
FOLLOWING
SET
OF
CODE
STRINGS
THE
AVERAGE
LENGTH
OF
THE
CODE
THAT
HAS
TO
BE
TRANSMITTED
IS
THEN
AVERAGE
CODE
LENGTH
BITS
WHICH
AGAIN
IS
THE
SAME
AS
THE
ENTROPY
OF
THE
RANDOM
VARIABLE
NOTE
THAT
SHORTER
CODE
STRINGS
CANNOT
BE
USED
BECAUSE
IT
MUST
BE
POSSIBLE
TO
DISAMBIGUATE
A
CONCATENATION
OF
SUCH
STRINGS
INTO
ITS
COMPONENT
PARTS
FOR
INSTANCE
DECODES
UNIQUELY
INTO
THE
STATE
SEQUENCE
C
A
D
THIS
RELATION
BETWEEN
ENTROPY
AND
SHORTEST
CODING
LENGTH
IS
A
GENERAL
ONE
THE
NOISELESS
CODING
THEOREM
SHANNON
STATES
THAT
THE
ENTROPY
IS
A
LOWER
BOUND
ON
THE
NUMBER
OF
BITS
NEEDED
TO
TRANSMIT
THE
STATE
OF
A
RANDOM
VARIABLE
FROM
NOW
ON
WE
SHALL
SWITCH
TO
THE
USE
OF
NATURAL
LOGARITHMS
IN
DEFINING
EN
TROPY
AS
THIS
WILL
PROVIDE
A
MORE
CONVENIENT
LINK
WITH
IDEAS
ELSEWHERE
IN
THIS
BOOK
IN
THIS
CASE
THE
ENTROPY
IS
MEASURED
IN
UNITS
OF
NATS
INSTEAD
OF
BITS
WHICH
DIFFER
SIMPLY
BY
A
FACTOR
OF
LN
WE
HAVE
INTRODUCED
THE
CONCEPT
OF
ENTROPY
IN
TERMS
OF
THE
AVERAGE
AMOUNT
OF
INFORMATION
NEEDED
TO
SPECIFY
THE
STATE
OF
A
RANDOM
VARIABLE
IN
FACT
THE
CONCEPT
OF
ENTROPY
HAS
MUCH
EARLIER
ORIGINS
IN
PHYSICS
WHERE
IT
WAS
INTRODUCED
IN
THE
CONTEXT
OF
EQUILIBRIUM
THERMODYNAMICS
AND
LATER
GIVEN
A
DEEPER
INTERPRETATION
AS
A
MEASURE
OF
DISORDER
THROUGH
DEVELOPMENTS
IN
STATISTICAL
MECHANICS
WE
CAN
UNDERSTAND
THIS
ALTERNATIVE
VIEW
OF
ENTROPY
BY
CONSIDERING
A
SET
OF
N
IDENTICAL
OBJECTS
THAT
ARE
TO
BE
DIVIDED
AMONGST
A
SET
OF
BINS
SUCH
THAT
THERE
ARE
NI
OBJECTS
IN
THE
ITH
BIN
CONSIDER
THE
NUMBER
OF
DIFFERENT
WAYS
OF
ALLOCATING
THE
OBJECTS
TO
THE
BINS
THERE
ARE
N
WAYS
TO
CHOOSE
THE
FIRST
OBJECT
N
WAYS
TO
CHOOSE
THE
SECOND
OBJECT
AND
SO
ON
LEADING
TO
A
TOTAL
OF
N
WAYS
TO
ALLOCATE
ALL
N
OBJECTS
TO
THE
BINS
WHERE
N
PRONOUNCED
FACTORIAL
N
DENOTES
THE
PRODUCT
N
N
HOWEVER
WE
DON
T
WISH
TO
DISTINGUISH
BETWEEN
REARRANGEMENTS
OF
OBJECTS
WITHIN
EACH
BIN
IN
THE
ITH
BIN
THERE
ARE
NI
WAYS
OF
REORDERING
THE
OBJECTS
AND
SO
THE
TOTAL
NUMBER
OF
WAYS
OF
ALLOCATING
THE
N
OBJECTS
TO
THE
BINS
IS
GIVEN
BY
N
I
NI
WHICH
IS
CALLED
THE
MULTIPLICITY
THE
ENTROPY
IS
THEN
DEFINED
AS
THE
LOGARITHM
OF
THE
MULTIPLICITY
SCALED
BY
AN
APPROPRIATE
CONSTANT
H
N
LN
W
N
LN
N
N
LN
NI
I
WE
NOW
CONSIDER
THE
LIMIT
N
IN
WHICH
THE
FRACTIONS
NI
N
ARE
HELD
FIXED
AND
APPLY
STIRLING
APPROXIMATION
WHICH
GIVES
LN
N
N
LN
N
N
H
LIM
NI
LN
NI
P
LN
PI
N
I
N
N
I
WHERE
WE
HAVE
USED
I
NI
N
HERE
PI
LIMN
NI
N
IS
THE
PROBABILITY
OF
AN
OBJECT
BEING
ASSIGNED
TO
THE
ITH
BIN
IN
PHYSICS
TERMINOLOGY
THE
SPECIFIC
AR
RANGEMENTS
OF
OBJECTS
IN
THE
BINS
IS
CALLED
A
MICROSTATE
AND
THE
OVERALL
DISTRIBUTION
OF
OCCUPATION
NUMBERS
EXPRESSED
THROUGH
THE
RATIOS
NI
N
IS
CALLED
A
MACROSTATE
THE
MULTIPLICITY
W
IS
ALSO
KNOWN
AS
THE
WEIGHT
OF
THE
MACROSTATE
WE
CAN
INTERPRET
THE
BINS
AS
THE
STATES
XI
OF
A
DISCRETE
RANDOM
VARIABLE
X
WHERE
P
X
XI
PI
THE
ENTROPY
OF
THE
RANDOM
VARIABLE
X
IS
THEN
H
P
P
XI
LN
P
XI
I
DISTRIBUTIONS
P
XI
THAT
ARE
SHARPLY
PEAKED
AROUND
A
FEW
VALUES
WILL
HAVE
A
RELATIVELY
LOW
ENTROPY
WHEREAS
THOSE
THAT
ARE
SPREAD
MORE
EVENLY
ACROSS
MANY
VALUES
WILL
HAVE
HIGHER
ENTROPY
AS
ILLUSTRATED
IN
FIGURE
BECAUSE
PI
THE
ENTROPY
IS
NONNEGATIVE
AND
IT
WILL
EQUAL
ITS
MINIMUM
VALUE
OF
WHEN
ONE
OF
THE
PI
AND
ALL
OTHER
PJ
I
THE
MAXIMUM
ENTROPY
CONFIGURATION
CAN
BE
FOUND
BY
APPENDIX
E
MAXIMIZING
H
USING
A
LAGRANGE
MULTIPLIER
TO
ENFORCE
THE
NORMALIZATION
CONSTRAINT
ON
THE
PROBABILITIES
THUS
WE
MAXIMIZE
H
P
XI
LN
P
XI
Λ
P
XI
FIGURE
HISTOGRAMS
OF
TWO
PROBABILITY
DISTRIBUTIONS
OVER
BINS
ILLUSTRATING
THE
HIGHER
VALUE
OF
THE
ENTROPY
H
FOR
THE
BROADER
DISTRIBUTION
THE
LARGEST
ENTROPY
WOULD
ARISE
FROM
A
UNIFORM
DISTRIBUTION
THAT
WOULD
GIVE
H
LN
EXERCISE
FROM
WHICH
WE
FIND
THAT
ALL
OF
THE
P
XI
ARE
EQUAL
AND
ARE
GIVEN
BY
P
XI
M
WHERE
M
IS
THE
TOTAL
NUMBER
OF
STATES
XI
THE
CORRESPONDING
VALUE
OF
THE
ENTROPY
IS
THEN
H
LN
M
THIS
RESULT
CAN
ALSO
BE
DERIVED
FROM
JENSEN
INEQUALITY
TO
BE
DISCUSSED
SHORTLY
TO
VERIFY
THAT
THE
STATIONARY
POINT
IS
INDEED
A
MAXIMUM
WE
CAN
EVALUATE
THE
SECOND
DERIVATIVE
OF
THE
ENTROPY
WHICH
GIVES
H
I
P
XI
P
XJ
WHERE
IIJ
ARE
THE
ELEMENTS
OF
THE
IDENTITY
MATRIX
IJ
PI
WE
CAN
EXTEND
THE
DEFINITION
OF
ENTROPY
TO
INCLUDE
DISTRIBUTIONS
P
X
OVER
CON
TINUOUS
VARIABLES
X
AS
FOLLOWS
FIRST
DIVIDE
X
INTO
BINS
OF
WIDTH
THEN
ASSUMING
P
X
IS
CONTINUOUS
THE
MEAN
VALUE
THEOREM
WEISSTEIN
TELLS
US
THAT
FOR
EACH
SUCH
BIN
THERE
MUST
EXIST
A
VALUE
XI
SUCH
THAT
I
WE
CAN
NOW
QUANTIZE
THE
CONTINUOUS
VARIABLE
X
BY
ASSIGNING
ANY
VALUE
X
TO
THE
VALUE
XI
WHENEVER
X
FALLS
IN
THE
ITH
BIN
THE
PROBABILITY
OF
OBSERVING
THE
VALUE
XI
IS
THEN
P
XI
THIS
GIVES
A
DISCRETE
DISTRIBUTION
FOR
WHICH
THE
ENTROPY
TAKES
THE
FORM
H
P
XI
LN
P
XI
P
XI
LN
P
XI
LN
WHERE
WE
HAVE
USED
I
P
XI
WHICH
FOLLOWS
FROM
WE
NOW
OMIT
THE
SECOND
TERM
LN
ON
THE
RIGHT
HAND
SIDE
OF
AND
THEN
CONSIDER
THE
LIMIT
THE
FIRST
TERM
ON
THE
RIGHT
HAND
SIDE
OF
WILL
APPROACH
THE
INTEGRAL
OF
P
X
LN
P
X
IN
THIS
LIMIT
SO
THAT
LIM
P
XI
LN
P
XI
P
X
LN
P
X
DX
WHERE
THE
QUANTITY
ON
THE
RIGHT
HAND
SIDE
IS
CALLED
THE
DIFFERENTIAL
ENTROPY
WE
SEE
THAT
THE
DISCRETE
AND
CONTINUOUS
FORMS
OF
THE
ENTROPY
DIFFER
BY
A
QUANTITY
LN
WHICH
DIVERGES
IN
THE
LIMIT
THIS
REFLECTS
THE
FACT
THAT
TO
SPECIFY
A
CONTINUOUS
VARIABLE
VERY
PRECISELY
REQUIRES
A
LARGE
NUMBER
OF
BITS
FOR
A
DENSITY
DEFINED
OVER
MULTIPLE
CONTINUOUS
VARIABLES
DENOTED
COLLECTIVELY
BY
THE
VECTOR
X
THE
DIFFERENTIAL
ENTROPY
IS
GIVEN
BY
H
X
P
X
LN
P
X
DX
IN
THE
CASE
OF
DISCRETE
DISTRIBUTIONS
WE
SAW
THAT
THE
MAXIMUM
ENTROPY
CON
FIGURATION
CORRESPONDED
TO
AN
EQUAL
DISTRIBUTION
OF
PROBABILITIES
ACROSS
THE
POSSIBLE
STATES
OF
THE
VARIABLE
LET
US
NOW
CONSIDER
THE
MAXIMUM
ENTROPY
CONFIGURATION
FOR
A
CONTINUOUS
VARIABLE
IN
ORDER
FOR
THIS
MAXIMUM
TO
BE
WELL
DEFINED
IT
WILL
BE
NEC
ESSARY
TO
CONSTRAIN
THE
FIRST
AND
SECOND
MOMENTS
OF
P
X
AS
WELL
AS
PRESERVING
THE
NORMALIZATION
CONSTRAINT
WE
THEREFORE
MAXIMIZE
THE
DIFFERENTIAL
ENTROPY
WITH
THE
LUDWIG
BOLTZMANN
LUDWIG
EDUARD
BOLTZMANN
WAS
AN
AUSTRIAN
PHYSICIST
WHO
CREATED
THE
FIELD
OF
STATISTICAL
MECHANICS
PRIOR
TO
BOLTZMANN
THE
CONCEPT
OF
EN
TROPY
WAS
ALREADY
KNOWN
FROM
CLASSICAL
THERMODYNAMICS
WHERE
IT
QUANTIFIES
THE
FACT
THAT
WHEN
WE
TAKE
ENERGY
FROM
A
SYSTEM
NOT
ALL
OF
THAT
ENERGY
IS
TYPICALLY
AVAILABLE
TO
DO
USEFUL
WORK
BOLTZMANN
SHOWED
THAT
THE
THER
MODYNAMIC
ENTROPY
A
MACROSCOPIC
QUANTITY
COULD
BE
RELATED
TO
THE
STATISTICAL
PROPERTIES
AT
THE
MICRO
SCOPIC
LEVEL
THIS
IS
EXPRESSED
THROUGH
THE
FAMOUS
EQUATION
K
LN
W
IN
WHICH
W
REPRESENTS
THE
NUMBER
OF
POSSIBLE
MICROSTATES
IN
A
MACROSTATE
AND
K
IN
UNITS
OF
JOULES
PER
KELVIN
IS
KNOWN
AS
BOLTZMANN
CONSTANT
BOLTZMANN
IDEAS
WERE
DISPUTED
BY
MANY
SCIENTISTS
OF
THEY
DAY
ONE
DIF
FICULTY
THEY
SAW
AROSE
FROM
THE
SECOND
LAW
OF
THERMO
DYNAMICS
WHICH
STATES
THAT
THE
ENTROPY
OF
A
CLOSED
SYSTEM
TENDS
TO
INCREASE
WITH
TIME
BY
CONTRAST
AT
THE
MICROSCOPIC
LEVEL
THE
CLASSICAL
NEWTONIAN
EQUA
TIONS
OF
PHYSICS
ARE
REVERSIBLE
AND
SO
THEY
FOUND
IT
DIFFICULT
TO
SEE
HOW
THE
LATTER
COULD
EXPLAIN
THE
FOR
MER
THEY
DIDN
T
FULLY
APPRECIATE
BOLTZMANN
ARGU
MENTS
WHICH
WERE
STATISTICAL
IN
NATURE
AND
WHICH
CON
CLUDED
NOT
THAT
ENTROPY
COULD
NEVER
DECREASE
OVER
TIME
BUT
SIMPLY
THAT
WITH
OVERWHELMING
PROBABILITY
IT
WOULD
GENERALLY
INCREASE
BOLTZMANN
EVEN
HAD
A
LONG
RUNNING
DISPUTE
WITH
THE
EDITOR
OF
THE
LEADING
GERMAN
PHYSICS
JOURNAL
WHO
REFUSED
TO
LET
HIM
REFER
TO
ATOMS
AND
MOLECULES
AS
ANYTHING
OTHER
THAN
CONVENIENT
THE
ORETICAL
CONSTRUCTS
THE
CONTINUED
ATTACKS
ON
HIS
WORK
LEAD
TO
BOUTS
OF
DEPRESSION
AND
EVENTUALLY
HE
COM
MITTED
SUICIDE
SHORTLY
AFTER
BOLTZMANN
DEATH
NEW
EXPERIMENTS
BY
PERRIN
ON
COLLOIDAL
SUSPENSIONS
VERI
FIED
HIS
THEORIES
AND
CONFIRMED
THE
VALUE
OF
THE
BOLTZ
MANN
CONSTANT
THE
EQUATION
K
LN
W
IS
CARVED
ON
BOLTZMANN
TOMBSTONE
THREE
CONSTRAINTS
P
X
DX
XP
X
DX
Μ
X
Μ
X
DX
APPENDIX
E
THE
CONSTRAINED
MAXIMIZATION
CAN
BE
PERFORMED
USING
LAGRANGE
MULTIPLIERS
SO
THAT
WE
MAXIMIZE
THE
FOLLOWING
FUNCTIONAL
WITH
RESPECT
TO
P
X
P
X
LN
P
X
DX
Λ
P
X
DX
XP
X
DX
Μ
Λ
X
Μ
X
DX
APPENDIX
D
EXERCISE
USING
THE
CALCULUS
OF
VARIATIONS
WE
SET
THE
DERIVATIVE
OF
THIS
FUNCTIONAL
TO
ZERO
GIVING
P
X
EXP
X
Μ
THE
LAGRANGE
MULTIPLIERS
CAN
BE
FOUND
BY
BACK
SUBSTITUTION
OF
THIS
RESULT
INTO
THE
THREE
CONSTRAINT
EQUATIONS
LEADING
FINALLY
TO
THE
RESULT
P
X
EXP
X
Μ
EXERCISE
AND
SO
THE
DISTRIBUTION
THAT
MAXIMIZES
THE
DIFFERENTIAL
ENTROPY
IS
THE
GAUSSIAN
NOTE
THAT
WE
DID
NOT
CONSTRAIN
THE
DISTRIBUTION
TO
BE
NONNEGATIVE
WHEN
WE
MAXIMIZED
THE
ENTROPY
HOWEVER
BECAUSE
THE
RESULTING
DISTRIBUTION
IS
INDEED
NONNEGATIVE
WE
SEE
WITH
HINDSIGHT
THAT
SUCH
A
CONSTRAINT
IS
NOT
NECESSARY
IF
WE
EVALUATE
THE
DIFFERENTIAL
ENTROPY
OF
THE
GAUSSIAN
WE
OBTAIN
H
X
LN
THUS
WE
SEE
AGAIN
THAT
THE
ENTROPY
INCREASES
AS
THE
DISTRIBUTION
BECOMES
BROADER
I
E
AS
INCREASES
THIS
RESULT
ALSO
SHOWS
THAT
THE
DIFFERENTIAL
ENTROPY
UNLIKE
THE
DISCRETE
ENTROPY
CAN
BE
NEGATIVE
BECAUSE
H
X
IN
FOR
SUPPOSE
WE
HAVE
A
JOINT
DISTRIBUTION
P
X
Y
FROM
WHICH
WE
DRAW
PAIRS
OF
VALUES
OF
X
AND
Y
IF
A
VALUE
OF
X
IS
ALREADY
KNOWN
THEN
THE
ADDITIONAL
INFORMATION
NEEDED
TO
SPECIFY
THE
CORRESPONDING
VALUE
OF
Y
IS
GIVEN
BY
LN
P
Y
X
THUS
THE
AVERAGE
ADDITIONAL
INFORMATION
NEEDED
TO
SPECIFY
Y
CAN
BE
WRITTEN
AS
H
Y
X
P
Y
X
LN
P
Y
X
DY
DX
EXERCISE
WHICH
IS
CALLED
THE
CONDITIONAL
ENTROPY
OF
Y
GIVEN
X
IT
IS
EASILY
SEEN
USING
THE
PRODUCT
RULE
THAT
THE
CONDITIONAL
ENTROPY
SATISFIES
THE
RELATION
H
X
Y
H
Y
X
H
X
WHERE
H
X
Y
IS
THE
DIFFERENTIAL
ENTROPY
OF
P
X
Y
AND
H
X
IS
THE
DIFFERENTIAL
EN
TROPY
OF
THE
MARGINAL
DISTRIBUTION
P
X
THUS
THE
INFORMATION
NEEDED
TO
DESCRIBE
X
AND
Y
IS
GIVEN
BY
THE
SUM
OF
THE
INFORMATION
NEEDED
TO
DESCRIBE
X
ALONE
PLUS
THE
ADDITIONAL
INFORMATION
REQUIRED
TO
SPECIFY
Y
GIVEN
X
RELATIVE
ENTROPY
AND
MUTUAL
INFORMATION
SO
FAR
IN
THIS
SECTION
WE
HAVE
INTRODUCED
A
NUMBER
OF
CONCEPTS
FROM
INFORMATION
THEORY
INCLUDING
THE
KEY
NOTION
OF
ENTROPY
WE
NOW
START
TO
RELATE
THESE
IDEAS
TO
PATTERN
RECOGNITION
CONSIDER
SOME
UNKNOWN
DISTRIBUTION
P
X
AND
SUPPOSE
THAT
WE
HAVE
MODELLED
THIS
USING
AN
APPROXIMATING
DISTRIBUTION
Q
X
IF
WE
USE
Q
X
TO
CONSTRUCT
A
CODING
SCHEME
FOR
THE
PURPOSE
OF
TRANSMITTING
VALUES
OF
X
TO
A
RECEIVER
THEN
THE
AVERAGE
ADDITIONAL
AMOUNT
OF
INFORMATION
IN
NATS
REQUIRED
TO
SPECIFY
THE
VALUE
OF
X
ASSUMING
WE
CHOOSE
AN
EFFICIENT
CODING
SCHEME
AS
A
RESULT
OF
USING
Q
X
INSTEAD
OF
THE
TRUE
DISTRIBUTION
P
X
IS
GIVEN
BY
KL
PLQ
P
X
LN
Q
X
DX
P
X
LN
P
X
DX
P
X
LN
Q
X
DX
THIS
IS
KNOWN
AS
THE
RELATIVE
ENTROPY
OR
KULLBACK
LEIBLER
DIVERGENCE
OR
KL
DIVER
GENCE
KULLBACK
AND
LEIBLER
BETWEEN
THE
DISTRIBUTIONS
P
X
AND
Q
X
NOTE
THAT
IT
IS
NOT
A
SYMMETRICAL
QUANTITY
THAT
IS
TO
SAY
KL
P
Q
KL
Q
P
WE
NOW
SHOW
THAT
THE
KULLBACK
LEIBLER
DIVERGENCE
SATISFIES
KL
P
Q
WITH
EQUALITY
IF
AND
ONLY
IF
P
X
Q
X
TO
DO
THIS
WE
FIRST
INTRODUCE
THE
CONCEPT
OF
CONVEX
FUNCTIONS
A
FUNCTION
F
X
IS
SAID
TO
BE
CONVEX
IF
IT
HAS
THE
PROPERTY
THAT
EVERY
CHORD
LIES
ON
OR
ABOVE
THE
FUNCTION
AS
SHOWN
IN
FIGURE
ANY
VALUE
OF
X
IN
THE
INTERVAL
FROM
X
A
TO
X
B
CAN
BE
WRITTEN
IN
THE
FORM
ΛA
Λ
B
WHERE
Λ
THE
CORRESPONDING
POINT
ON
THE
CHORD
IS
GIVEN
BY
ΛF
A
Λ
F
B
CLAUDE
SHANNON
AFTER
GRADUATING
FROM
MICHIGAN
AND
MIT
SHANNON
JOINED
THE
AT
T
BELL
TELEPHONE
LABORATORIES
IN
HIS
PAPER
A
MATHEMATICAL
THEORY
OF
COMMUNICATION
PUBLISHED
IN
THE
BELL
SYSTEM
TECHNICAL
JOURNAL
IN
LAID
THE
FOUNDATIONS
FOR
MODERN
INFORMATION
THE
ORY
THIS
PAPER
INTRODUCED
THE
WORD
BIT
AND
HIS
CON
CEPT
THAT
INFORMATION
COULD
BE
SENT
AS
A
STREAM
OF
AND
PAVED
THE
WAY
FOR
THE
COMMUNICATIONS
REVO
LUTION
IT
IS
SAID
THAT
VON
NEUMANN
RECOMMENDED
TO
SHANNON
THAT
HE
USE
THE
TERM
ENTROPY
NOT
ONLY
BE
CAUSE
OF
ITS
SIMILARITY
TO
THE
QUANTITY
USED
IN
PHYSICS
BUT
ALSO
BECAUSE
NOBODY
KNOWS
WHAT
ENTROPY
REALLY
IS
SO
IN
ANY
DISCUSSION
YOU
WILL
ALWAYS
HAVE
AN
ADVAN
TAGE
FIGURE
A
CONVEX
FUNCTION
F
X
IS
ONE
FOR
WHICH
EV
ERY
CHORD
SHOWN
IN
BLUE
LIES
ON
OR
ABOVE
THE
FUNCTION
SHOWN
IN
RED
A
XΛ
B
X
AND
THE
CORRESPONDING
VALUE
OF
THE
FUNCTION
IS
F
ΛA
Λ
B
CONVEXITY
THEN
IMPLIES
EXERCISE
EXERCISE
F
ΛA
Λ
B
ΛF
A
Λ
F
B
THIS
IS
EQUIVALENT
TO
THE
REQUIREMENT
THAT
THE
SECOND
DERIVATIVE
OF
THE
FUNCTION
BE
EVERYWHERE
POSITIVE
EXAMPLES
OF
CONVEX
FUNCTIONS
ARE
X
LN
X
FOR
X
AND
A
FUNCTION
IS
CALLED
STRICTLY
CONVEX
IF
THE
EQUALITY
IS
SATISFIED
ONLY
FOR
Λ
AND
Λ
IF
A
FUNCTION
HAS
THE
OPPOSITE
PROPERTY
NAMELY
THAT
EVERY
CHORD
LIES
ON
OR
BELOW
THE
FUNCTION
IT
IS
CALLED
CONCAVE
WITH
A
CORRESPONDING
DEFINITION
FOR
STRICTLY
CONCAVE
IF
A
FUNCTION
F
X
IS
CONVEX
THEN
F
X
WILL
BE
CONCAVE
USING
THE
TECHNIQUE
OF
PROOF
BY
INDUCTION
WE
CAN
SHOW
FROM
THAT
A
CONVEX
FUNCTION
F
X
SATISFIES
M
F
I
ΛIXI
I
ΛIF
XI
WHERE
ΛI
AND
I
ΛI
FOR
ANY
SET
OF
POINTS
XI
THE
RESULT
IS
KNOWN
AS
JENSEN
INEQUALITY
IF
WE
INTERPRET
THE
ΛI
AS
THE
PROBABILITY
DISTRIBUTION
OVER
A
DISCRETE
VARIABLE
X
TAKING
THE
VALUES
XI
THEN
CAN
BE
WRITTEN
F
E
X
E
F
X
WHERE
E
DENOTES
THE
EXPECTATION
FOR
CONTINUOUS
VARIABLES
JENSEN
INEQUALITY
TAKES
THE
FORM
F
XP
X
DX
F
X
P
X
DX
WE
CAN
APPLY
JENSEN
INEQUALITY
IN
THE
FORM
TO
THE
KULLBACK
LEIBLER
DIVERGENCE
TO
GIVE
KL
PLQ
P
X
LN
Q
X
DX
LN
Q
X
DX
SO
THE
EQUALITY
WILL
HOLD
IF
AND
ONLY
IF
Q
X
P
X
FOR
ALL
X
THUS
WE
CAN
IN
TERPRET
THE
KULLBACK
LEIBLER
DIVERGENCE
AS
A
MEASURE
OF
THE
DISSIMILARITY
OF
THE
TWO
DISTRIBUTIONS
P
X
AND
Q
X
WE
SEE
THAT
THERE
IS
AN
INTIMATE
RELATIONSHIP
BETWEEN
DATA
COMPRESSION
AND
DEN
SITY
ESTIMATION
I
E
THE
PROBLEM
OF
MODELLING
AN
UNKNOWN
PROBABILITY
DISTRIBUTION
BECAUSE
THE
MOST
EFFICIENT
COMPRESSION
IS
ACHIEVED
WHEN
WE
KNOW
THE
TRUE
DISTRI
BUTION
IF
WE
USE
A
DISTRIBUTION
THAT
IS
DIFFERENT
FROM
THE
TRUE
ONE
THEN
WE
MUST
NECESSARILY
HAVE
A
LESS
EFFICIENT
CODING
AND
ON
AVERAGE
THE
ADDITIONAL
INFORMATION
THAT
MUST
BE
TRANSMITTED
IS
AT
LEAST
EQUAL
TO
THE
KULLBACK
LEIBLER
DIVERGENCE
BE
TWEEN
THE
TWO
DISTRIBUTIONS
SUPPOSE
THAT
DATA
IS
BEING
GENERATED
FROM
AN
UNKNOWN
DISTRIBUTION
P
X
THAT
WE
WISH
TO
MODEL
WE
CAN
TRY
TO
APPROXIMATE
THIS
DISTRIBUTION
USING
SOME
PARAMETRIC
DISTRIBUTION
Q
X
Θ
GOVERNED
BY
A
SET
OF
ADJUSTABLE
PARAMETERS
Θ
FOR
EXAMPLE
A
MULTIVARIATE
GAUSSIAN
ONE
WAY
TO
DETERMINE
Θ
IS
TO
MINIMIZE
THE
KULLBACK
LEIBLER
DIVERGENCE
BETWEEN
P
X
AND
Q
X
Θ
WITH
RESPECT
TO
Θ
WE
CANNOT
DO
THIS
DIRECTLY
BECAUSE
WE
DON
T
KNOW
P
X
SUPPOSE
HOWEVER
THAT
WE
HAVE
OBSERVED
A
FINITE
SET
OF
TRAINING
POINTS
XN
FOR
N
N
DRAWN
FROM
P
X
THEN
THE
EXPECTATION
WITH
RESPECT
TO
P
X
CAN
BE
APPROXIMATED
BY
A
FINITE
SUM
OVER
THESE
POINTS
USING
SO
THAT
EXERCISE
N
KL
PLQ
LN
Q
XN
Θ
LN
P
XN
N
THE
SECOND
TERM
ON
THE
RIGHT
HAND
SIDE
OF
IS
INDEPENDENT
OF
Θ
AND
THE
FIRST
TERM
IS
THE
NEGATIVE
LOG
LIKELIHOOD
FUNCTION
FOR
Θ
UNDER
THE
DISTRIBUTION
Q
X
Θ
EVAL
UATED
USING
THE
TRAINING
SET
THUS
WE
SEE
THAT
MINIMIZING
THIS
KULLBACK
LEIBLER
DIVERGENCE
IS
EQUIVALENT
TO
MAXIMIZING
THE
LIKELIHOOD
FUNCTION
NOW
CONSIDER
THE
JOINT
DISTRIBUTION
BETWEEN
TWO
SETS
OF
VARIABLES
X
AND
Y
GIVEN
BY
P
X
Y
IF
THE
SETS
OF
VARIABLES
ARE
INDEPENDENT
THEN
THEIR
JOINT
DISTRIBUTION
WILL
FACTORIZE
INTO
THE
PRODUCT
OF
THEIR
MARGINALS
P
X
Y
P
X
P
Y
IF
THE
VARIABLES
ARE
NOT
INDEPENDENT
WE
CAN
GAIN
SOME
IDEA
OF
WHETHER
THEY
ARE
CLOSE
TO
BEING
INDEPEN
DENT
BY
CONSIDERING
THE
KULLBACK
LEIBLER
DIVERGENCE
BETWEEN
THE
JOINT
DISTRIBUTION
AND
THE
PRODUCT
OF
THE
MARGINALS
GIVEN
BY
I
X
Y
KL
P
X
Y
LP
X
P
Y
X
Y
LN
P
X
P
Y
DX
DY
P
X
Y
WHICH
IS
CALLED
THE
MUTUAL
INFORMATION
BETWEEN
THE
VARIABLES
X
AND
Y
FROM
THE
PROPERTIES
OF
THE
KULLBACK
LEIBLER
DIVERGENCE
WE
SEE
THAT
I
X
Y
WITH
EQUAL
ITY
IF
AND
ONLY
IF
X
AND
Y
ARE
INDEPENDENT
USING
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
WE
SEE
THAT
THE
MUTUAL
INFORMATION
IS
RELATED
TO
THE
CONDITIONAL
ENTROPY
THROUGH
I
X
Y
H
X
H
X
Y
H
Y
H
Y
X
THUS
WE
CAN
VIEW
THE
MUTUAL
INFORMATION
AS
THE
REDUCTION
IN
THE
UNCERTAINTY
ABOUT
X
BY
VIRTUE
OF
BEING
TOLD
THE
VALUE
OF
Y
OR
VICE
VERSA
FROM
A
BAYESIAN
PERSPECTIVE
WE
CAN
VIEW
P
X
AS
THE
PRIOR
DISTRIBUTION
FOR
X
AND
P
X
Y
AS
THE
POSTERIOR
DISTRIBU
TION
AFTER
WE
HAVE
OBSERVED
NEW
DATA
Y
THE
MUTUAL
INFORMATION
THEREFORE
REPRESENTS
THE
REDUCTION
IN
UNCERTAINTY
ABOUT
X
AS
A
CONSEQUENCE
OF
THE
NEW
OBSERVATION
Y
EXERCISES
WWW
CONSIDER
THE
SUM
OF
SQUARES
ERROR
FUNCTION
GIVEN
BY
IN
WHICH
THE
FUNCTION
Y
X
W
IS
GIVEN
BY
THE
POLYNOMIAL
SHOW
THAT
THE
COEFFICIENTS
W
WI
THAT
MINIMIZE
THIS
ERROR
FUNCTION
ARE
GIVEN
BY
THE
SOLUTION
TO
THE
FOLLOWING
SET
OF
LINEAR
EQUATIONS
WHERE
M
AIJWJ
TI
J
N
N
AIJ
XN
I
J
TI
XN
ITN
HERE
A
SUFFIX
I
OR
J
DENOTES
THE
INDEX
OF
A
COMPONENT
WHEREAS
X
I
DENOTES
X
RAISED
TO
THE
POWER
OF
I
WRITE
DOWN
THE
SET
OF
COUPLED
LINEAR
EQUATIONS
ANALOGOUS
TO
SATISFIED
BY
THE
COEFFICIENTS
WI
WHICH
MINIMIZE
THE
REGULARIZED
SUM
OF
SQUARES
ERROR
FUNCTION
GIVEN
BY
SUPPOSE
THAT
WE
HAVE
THREE
COLOURED
BOXES
R
RED
B
BLUE
AND
G
GREEN
BOX
R
CONTAINS
APPLES
ORANGES
AND
LIMES
BOX
B
CONTAINS
APPLE
ORANGE
AND
LIMES
AND
BOX
G
CONTAINS
APPLES
ORANGES
AND
LIMES
IF
A
BOX
IS
CHOSEN
AT
RANDOM
WITH
PROBABILITIES
P
R
P
B
P
G
AND
A
PIECE
OF
FRUIT
IS
REMOVED
FROM
THE
BOX
WITH
EQUAL
PROBABILITY
OF
SELECTING
ANY
OF
THE
ITEMS
IN
THE
BOX
THEN
WHAT
IS
THE
PROBABILITY
OF
SELECTING
AN
APPLE
IF
WE
OBSERVE
THAT
THE
SELECTED
FRUIT
IS
IN
FACT
AN
ORANGE
WHAT
IS
THE
PROBABILITY
THAT
IT
CAME
FROM
THE
GREEN
BOX
WWW
CONSIDER
A
PROBABILITY
DENSITY
PX
X
DEFINED
OVER
A
CONTINUOUS
VARI
ABLE
X
AND
SUPPOSE
THAT
WE
MAKE
A
NONLINEAR
CHANGE
OF
VARIABLE
USING
X
G
Y
SO
THAT
THE
DENSITY
TRANSFORMS
ACCORDING
TO
BY
DIFFERENTIATING
SHOW
THAT
THE
LOCATION
Y
OF
THE
MAXIMUM
OF
THE
DENSITY
IN
Y
IS
NOT
IN
GENERAL
RELATED
TO
THE
LOCATION
X
OF
THE
MAXIMUM
OF
THE
DENSITY
OVER
X
BY
THE
SIMPLE
FUNCTIONAL
RELATION
G
Y
AS
A
CONSEQUENCE
OF
THE
JACOBIAN
FACTOR
THIS
SHOWS
THAT
THE
MAXIMUM
X
OF
A
PROBABILITY
DENSITY
IN
CONTRAST
TO
A
SIMPLE
FUNCTION
IS
DEPENDENT
ON
THE
CHOICE
OF
VARIABLE
VERIFY
THAT
IN
THE
CASE
OF
A
LINEAR
TRANSFORMATION
THE
LOCATION
OF
THE
MAXIMUM
TRANSFORMS
IN
THE
SAME
WAY
AS
THE
VARIABLE
ITSELF
USING
THE
DEFINITION
SHOW
THAT
VAR
F
X
SATISFIES
SHOW
THAT
IF
TWO
VARIABLES
X
AND
Y
ARE
INDEPENDENT
THEN
THEIR
COVARIANCE
IS
ZERO
WWW
IN
THIS
EXERCISE
WE
PROVE
THE
NORMALIZATION
CONDITION
FOR
THE
UNIVARIATE
GAUSSIAN
TO
DO
THIS
CONSIDER
THE
INTEGRAL
I
EXP
DX
WHICH
WE
CAN
EVALUATE
BY
FIRST
WRITING
ITS
SQUARE
IN
THE
FORM
EXP
DX
DY
NOW
MAKE
THE
TRANSFORMATION
FROM
CARTESIAN
COORDINATES
X
Y
TO
POLAR
COORDINATES
R
Θ
AND
THEN
SUBSTITUTE
U
SHOW
THAT
BY
PERFORMING
THE
INTEGRALS
OVER
Θ
AND
U
AND
THEN
TAKING
THE
SQUARE
ROOT
OF
BOTH
SIDES
WE
OBTAIN
I
FINALLY
USE
THIS
RESULT
TO
SHOW
THAT
THE
GAUSSIAN
DISTRIBUTION
X
Μ
IS
NORMAL
IZED
WWW
BY
USING
A
CHANGE
OF
VARIABLES
VERIFY
THAT
THE
UNIVARIATE
GAUSSIAN
DISTRIBUTION
GIVEN
BY
SATISFIES
NEXT
BY
DIFFERENTIATING
BOTH
SIDES
OF
THE
NORMALIZATION
CONDITION
N
X
Μ
DX
WITH
RESPECT
TO
VERIFY
THAT
THE
GAUSSIAN
SATISFIES
FINALLY
SHOW
THAT
HOLDS
WWW
SHOW
THAT
THE
MODE
I
E
THE
MAXIMUM
OF
THE
GAUSSIAN
DISTRIBUTION
IS
GIVEN
BY
Μ
SIMILARLY
SHOW
THAT
THE
MODE
OF
THE
MULTIVARIATE
GAUSSIAN
IS
GIVEN
BY
Μ
WWW
SUPPOSE
THAT
THE
TWO
VARIABLES
X
AND
Z
ARE
STATISTICALLY
INDEPENDENT
SHOW
THAT
THE
MEAN
AND
VARIANCE
OF
THEIR
SUM
SATISFIES
E
X
Z
E
X
E
Z
VAR
X
Z
VAR
X
VAR
Z
BY
SETTING
THE
DERIVATIVES
OF
THE
LOG
LIKELIHOOD
FUNCTION
WITH
RESPECT
TO
Μ
AND
EQUAL
TO
ZERO
VERIFY
THE
RESULTS
AND
WWW
USING
THE
RESULTS
AND
SHOW
THAT
E
XNXM
WHERE
XN
AND
XM
DENOTE
DATA
POINTS
SAMPLED
FROM
A
GAUSSIAN
DISTRIBUTION
WITH
MEAN
Μ
AND
VARIANCE
AND
INM
SATISFIES
INM
IF
N
M
AND
INM
OTHERWISE
HENCE
PROVE
THE
RESULTS
AND
SUPPOSE
THAT
THE
VARIANCE
OF
A
GAUSSIAN
IS
ESTIMATED
USING
THE
RESULT
BUT
WITH
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
ΜML
REPLACED
WITH
THE
TRUE
VALUE
Μ
OF
THE
MEAN
SHOW
THAT
THIS
ESTIMATOR
HAS
THE
PROPERTY
THAT
ITS
EXPECTATION
IS
GIVEN
BY
THE
TRUE
VARIANCE
SHOW
THAT
AN
ARBITRARY
SQUARE
MATRIX
WITH
ELEMENTS
WIJ
CAN
BE
WRITTEN
IN
THE
FORM
WIJ
WS
WA
WHERE
WS
AND
WA
ARE
SYMMETRIC
AND
ANTI
SYMMETRIC
IJ
IJ
IJ
IJ
MATRICES
RESPECTIVELY
SATISFYING
WS
AND
WA
WA
FOR
ALL
I
AND
J
NOW
CONSIDER
THE
SECOND
ORDER
TERM
IN
A
HIGHER
ORDER
POLYNOMIAL
IN
D
DIMENSIONS
GIVEN
BY
D
D
SHOW
THAT
WIJXIXJ
I
J
D
D
D
D
WIJXIXJ
WS
XIXJ
I
J
I
J
SO
THAT
THE
CONTRIBUTION
FROM
THE
ANTI
SYMMETRIC
MATRIX
VANISHES
WE
THEREFORE
SEE
THAT
WITHOUT
LOSS
OF
GENERALITY
THE
MATRIX
OF
COEFFICIENTS
WIJ
CAN
BE
CHOSEN
TO
BE
SYMMETRIC
AND
SO
NOT
ALL
OF
THE
ELEMENTS
OF
THIS
MATRIX
CAN
BE
CHOSEN
INDEPEN
DENTLY
SHOW
THAT
THE
NUMBER
OF
INDEPENDENT
PARAMETERS
IN
THE
MATRIX
WS
IS
GIVEN
BY
D
D
WWW
IN
THIS
EXERCISE
AND
THE
NEXT
WE
EXPLORE
HOW
THE
NUMBER
OF
INDEPEN
DENT
PARAMETERS
IN
A
POLYNOMIAL
GROWS
WITH
THE
ORDER
M
OF
THE
POLYNOMIAL
AND
WITH
THE
DIMENSIONALITY
D
OF
THE
INPUT
SPACE
WE
START
BY
WRITING
DOWN
THE
M
TH
ORDER
TERM
FOR
A
POLYNOMIAL
IN
D
DIMENSIONS
IN
THE
FORM
D
D
D
IM
XIM
THE
COEFFICIENTS
IM
COMPRISE
D
ELEMENTS
BUT
THE
NUMBER
OF
INDEPENDENT
M
PARAMETERS
IS
SIGNIFICANTLY
FEWER
DUE
TO
THE
MANY
INTERCHANGE
SYMMETRIES
OF
THE
FACTOR
XIM
BEGIN
BY
SHOWING
THAT
THE
REDUNDANCY
IN
THE
COEFFICIENTS
CAN
BE
REMOVED
BY
REWRITING
THIS
M
TH
ORDER
TERM
IN
THE
FORM
D
IM
W
IM
XIM
NOTE
THAT
THE
PRECISE
RELATIONSHIP
BETWEEN
THE
W
COEFFICIENTS
AND
W
COEFFICIENTS
NEED
NOT
BE
MADE
EXPLICIT
USE
THIS
RESULT
TO
SHOW
THAT
THE
NUMBER
OF
INDEPENDENT
PARAM
ETERS
N
D
M
WHICH
APPEAR
AT
ORDER
M
SATISFIES
THE
FOLLOWING
RECURSION
RELATION
D
N
D
M
N
I
M
I
NEXT
USE
PROOF
BY
INDUCTION
TO
SHOW
THAT
THE
FOLLOWING
RESULT
HOLDS
I
M
D
M
WHICH
CAN
BE
DONE
BY
FIRST
PROVING
THE
RESULT
FOR
D
AND
ARBITRARY
M
BY
MAKING
USE
OF
THE
RESULT
THEN
ASSUMING
IT
IS
CORRECT
FOR
DIMENSION
D
AND
VERIFYING
THAT
IT
IS
CORRECT
FOR
DIMENSION
D
FINALLY
USE
THE
TWO
PREVIOUS
RESULTS
TOGETHER
WITH
PROOF
BY
INDUCTION
TO
SHOW
N
D
M
D
M
D
M
TO
DO
THIS
FIRST
SHOW
THAT
THE
RESULT
IS
TRUE
FOR
M
AND
ANY
VALUE
OF
D
BY
COMPARISON
WITH
THE
RESULT
OF
EXERCISE
THEN
MAKE
USE
OF
TOGETHER
WITH
TO
SHOW
THAT
IF
THE
RESULT
HOLDS
AT
ORDER
M
THEN
IT
WILL
ALSO
HOLD
AT
ORDER
M
IN
EXERCISE
WE
PROVED
THE
RESULT
FOR
THE
NUMBER
OF
INDEPENDENT
PARAMETERS
IN
THE
M
TH
ORDER
TERM
OF
A
D
DIMENSIONAL
POLYNOMIAL
WE
NOW
FIND
AN
EXPRESSION
FOR
THE
TOTAL
NUMBER
N
D
M
OF
INDEPENDENT
PARAMETERS
IN
ALL
OF
THE
TERMS
UP
TO
AND
INCLUDING
THE
M
ORDER
FIRST
SHOW
THAT
N
D
M
SATISFIES
M
N
D
M
N
D
M
M
WHERE
N
D
M
IS
THE
NUMBER
OF
INDEPENDENT
PARAMETERS
IN
THE
TERM
OF
ORDER
M
NOW
MAKE
USE
OF
THE
RESULT
TOGETHER
WITH
PROOF
BY
INDUCTION
TO
SHOW
THAT
D
M
N
D
M
D
M
THIS
CAN
BE
DONE
BY
FIRST
PROVING
THAT
THE
RESULT
HOLDS
FOR
M
AND
ARBITRARY
D
THEN
ASSUMING
THAT
IT
HOLDS
AT
ORDER
M
AND
HENCE
SHOWING
THAT
IT
HOLDS
AT
ORDER
M
FINALLY
MAKE
USE
OF
STIRLING
APPROXIMATION
IN
THE
FORM
N
NNE
N
FOR
LARGE
N
TO
SHOW
THAT
FOR
D
M
THE
QUANTITY
N
D
M
GROWS
LIKE
DM
AND
FOR
M
D
IT
GROWS
LIKE
MD
CONSIDER
A
CUBIC
M
POLYNOMIAL
IN
D
DIMENSIONS
AND
EVALUATE
NUMERICALLY
THE
TOTAL
NUMBER
OF
INDEPENDENT
PARAMETERS
FOR
I
D
AND
II
D
WHICH
CORRESPOND
TO
TYPICAL
SMALL
SCALE
AND
MEDIUM
SCALE
MACHINE
LEARNING
APPLICATIONS
WWW
THE
GAMMA
FUNCTION
IS
DEFINED
BY
Γ
X
UX
U
DU
USING
INTEGRATION
BY
PARTS
PROVE
THE
RELATION
Γ
X
XΓ
X
SHOW
ALSO
THAT
Γ
AND
HENCE
THAT
Γ
X
X
WHEN
X
IS
AN
INTEGER
WWW
WE
CAN
USE
THE
RESULT
TO
DERIVE
AN
EXPRESSION
FOR
THE
SURFACE
AREA
SD
AND
THE
VOLUME
VD
OF
A
SPHERE
OF
UNIT
RADIUS
IN
D
DIMENSIONS
TO
DO
THIS
CONSIDER
THE
FOLLOWING
RESULT
WHICH
IS
OBTAINED
BY
TRANSFORMING
FROM
CARTESIAN
TO
POLAR
COORDINATES
TLI
E
XI
DXI
SD
E
R
RD
DR
USING
THE
DEFINITION
OF
THE
GAMMA
FUNCTION
TOGETHER
WITH
EVALUATE
BOTH
SIDES
OF
THIS
EQUATION
AND
HENCE
SHOW
THAT
SD
Γ
D
NEXT
BY
INTEGRATING
WITH
RESPECT
TO
RADIUS
FROM
TO
SHOW
THAT
THE
VOLUME
OF
THE
UNIT
SPHERE
IN
D
DIMENSIONS
IS
GIVEN
BY
V
SD
D
D
FINALLY
USE
THE
RESULTS
Γ
AND
Γ
Π
TO
SHOW
THAT
AND
REDUCE
TO
THE
USUAL
EXPRESSIONS
FOR
D
AND
D
CONSIDER
A
SPHERE
OF
RADIUS
A
IN
D
DIMENSIONS
TOGETHER
WITH
THE
CONCENTRIC
HYPERCUBE
OF
SIDE
SO
THAT
THE
SPHERE
TOUCHES
THE
HYPERCUBE
AT
THE
CENTRES
OF
EACH
OF
ITS
SIDES
BY
USING
THE
RESULTS
OF
EXERCISE
SHOW
THAT
THE
RATIO
OF
THE
VOLUME
OF
THE
SPHERE
TO
THE
VOLUME
OF
THE
CUBE
IS
GIVEN
BY
VOLUME
OF
SPHERE
ΠD
VOLUME
OF
CUBE
D
NOW
MAKE
USE
OF
STIRLING
FORMULA
IN
THE
FORM
Γ
X
XXX
WHICH
IS
VALID
FOR
X
TO
SHOW
THAT
AS
D
THE
RATIO
GOES
TO
ZERO
SHOW
ALSO
THAT
THE
RATIO
OF
THE
DISTANCE
FROM
THE
CENTRE
OF
THE
HYPERCUB
E
TO
ONE
OF
THE
CORNERS
DIVIDED
BY
THE
PERPENDICULAR
DISTANCE
TO
ONE
OF
THE
SIDES
IS
D
WHICH
THEREFORE
GOES
TO
AS
D
FROM
THESE
RESULTS
WE
SEE
THAT
IN
A
SPACE
OF
HIGH
DIMENSIONALITY
MOST
OF
THE
VOLUME
OF
A
CUBE
IS
CONCENTRATED
IN
THE
LARGE
NUMBER
OF
CORNERS
WHICH
THEMSELVES
BECOME
VERY
LONG
SPIKES
WWW
IN
THIS
EXERCISE
WE
EXPLORE
THE
BEHAVIOUR
OF
THE
GAUSSIAN
DISTRIBUTION
IN
HIGH
DIMENSIONAL
SPACES
CONSIDER
A
GAUSSIAN
DISTRIBUTION
IN
D
DIMENSIONS
GIVEN
BY
WE
WISH
TO
FIND
THE
DENSITY
WITH
RESPECT
TO
RADIUS
IN
POLAR
COORDINATES
IN
WHICH
THE
DIRECTION
VARIABLES
HAVE
BEEN
INTEGRATED
OUT
TO
DO
THIS
SHOW
THAT
THE
INTEGRAL
OF
THE
PROBABILITY
DENSITY
OVER
A
THIN
SHELL
OF
RADIUS
R
AND
THICKNESS
E
WHERE
E
IS
GIVEN
BY
P
R
E
WHERE
SDRD
WHERE
SD
IS
THE
SURFACE
AREA
OF
A
UNIT
SPHERE
IN
D
DIMENSIONS
SHOW
THAT
THE
FUNCTION
P
R
HAS
A
SINGLE
STATIONARY
POINT
LOCATED
FOR
LARGE
D
AT
R
DΣ
BY
CONSIDERING
P
R
E
WHERE
E
R
SHOW
THAT
FOR
LARGE
D
WHICH
SHOWS
THAT
R
IS
A
MAXI
MUM
OF
THE
RADIAL
PROBABILITY
DENSITY
AND
ALSO
THAT
P
R
DECAYS
EXPONENTIALLY
AWAY
FROM
ITS
MAXIMUM
AT
R
WITH
LENGTH
SCALE
Σ
WE
HAVE
ALREADY
SEEN
THAT
Σ
R
FOR
LARGE
D
AND
SO
WE
SEE
THAT
MOST
OF
THE
PROBABILITY
MASS
IS
CONCENTRATED
IN
A
THIN
SHELL
AT
LARGE
RADIUS
FINALLY
SHOW
THAT
THE
PROBABILITY
DENSITY
P
X
IS
LARGER
AT
THE
ORIGIN
THAN
AT
THE
RADIUS
R
BY
A
FACTOR
OF
EXP
D
WE
THEREFORE
SEE
THAT
MOST
OF
THE
PROBABILITY
MASS
IN
A
HIGH
DIMENSIONAL
GAUSSIAN
DISTRIBUTION
IS
LOCATED
AT
A
DIFFERENT
RADIUS
FROM
THE
REGION
OF
HIGH
PROBABILITY
DENSITY
THIS
PROPERTY
OF
DISTRIBUTIONS
IN
SPACES
OF
HIGH
DIMENSIONALITY
WILL
HAVE
IMPORTANT
CONSEQUENCES
WHEN
WE
CONSIDER
BAYESIAN
INFERENCE
OF
MODEL
PARAMETERS
IN
LATER
CHAPTERS
CONSIDER
TWO
NONNEGATIVE
NUMBERS
A
AND
B
AND
SHOW
THAT
IF
A
B
THEN
A
AB
USE
THIS
RESULT
TO
SHOW
THAT
IF
THE
DECISION
REGIONS
OF
A
TWO
CLASS
CLASSIFICATION
PROBLEM
ARE
CHOSEN
TO
MINIMIZE
THE
PROBABILITY
OF
MISCLASSIFICATION
THIS
PROBABILITY
WILL
SATISFY
P
MISTAKE
P
X
P
X
DX
WWW
GIVEN
A
LOSS
MATRIX
WITH
ELEMENTS
LKJ
THE
EXPECTED
RISK
IS
MINIMIZED
IF
FOR
EACH
X
WE
CHOOSE
THE
CLASS
THAT
MINIMIZES
VERIFY
THAT
WHEN
THE
LOSS
MATRIX
IS
GIVEN
BY
LKJ
IKJ
WHERE
IKJ
ARE
THE
ELEMENTS
OF
THE
IDENTITY
MATRIX
THIS
REDUCES
TO
THE
CRITERION
OF
CHOOSING
THE
CLASS
HAVING
THE
LARGEST
POSTERIOR
PROBABILITY
WHAT
IS
THE
INTERPRETATION
OF
THIS
FORM
OF
LOSS
MATRIX
DERIVE
THE
CRITERION
FOR
MINIMIZING
THE
EXPECTED
LOSS
WHEN
THERE
IS
A
GENERAL
LOSS
MATRIX
AND
GENERAL
PRIOR
PROBABILITIES
FOR
THE
CLASSES
WWW
CONSIDER
A
CLASSIFICATION
PROBLEM
IN
WHICH
THE
LOSS
INCURRED
WHEN
AN
INPUT
VECTOR
FROM
CLASS
K
IS
CLASSIFIED
AS
BELONGING
TO
CLASS
J
IS
GIVEN
BY
THE
LOSS
MATRIX
LKJ
AND
FOR
WHICH
THE
LOSS
INCURRED
IN
SELECTING
THE
REJECT
OPTION
IS
Λ
FIND
THE
DECISION
CRITERION
THAT
WILL
GIVE
THE
MINIMUM
EXPECTED
LOSS
VERIFY
THAT
THIS
REDUCES
TO
THE
REJECT
CRITERION
DISCUSSED
IN
SECTION
WHEN
THE
LOSS
MATRIX
IS
GIVEN
BY
LKJ
IKJ
WHAT
IS
THE
RELATIONSHIP
BETWEEN
Λ
AND
THE
REJECTION
THRESHOLD
Θ
WWW
CONSIDER
THE
GENERALIZATION
OF
THE
SQUARED
LOSS
FUNCTION
FOR
A
SINGLE
TARGET
VARIABLE
T
TO
THE
CASE
OF
MULTIPLE
TARGET
VARIABLES
DESCRIBED
BY
THE
VECTOR
T
GIVEN
BY
E
L
T
Y
X
LY
X
X
T
DX
DT
USING
THE
CALCULUS
OF
VARIATIONS
SHOW
THAT
THE
FUNCTION
Y
X
FOR
WHICH
THIS
EXPECTED
LOSS
IS
MINIMIZED
IS
GIVEN
BY
Y
X
ET
T
X
SHOW
THAT
THIS
RESULT
REDUCES
TO
FOR
THE
CASE
OF
A
SINGLE
TARGET
VARIABLE
T
BY
EXPANSION
OF
THE
SQUARE
IN
DERIVE
A
RESULT
ANALOGOUS
TO
AND
HENCE
SHOW
THAT
THE
FUNCTION
Y
X
THAT
MINIMIZES
THE
EXPECTED
SQUARED
LOSS
FOR
THE
CASE
OF
A
VECTOR
T
OF
TARGET
VARIABLES
IS
AGAIN
GIVEN
BY
THE
CONDITIONAL
EXPECTATION
OF
T
WWW
CONSIDER
THE
EXPECTED
LOSS
FOR
REGRESSION
PROBLEMS
UNDER
THE
LQ
LOSS
FUNCTION
GIVEN
BY
WRITE
DOWN
THE
CONDITION
THAT
Y
X
MUST
SATISFY
IN
ORDER
TO
MINIMIZE
E
LQ
SHOW
THAT
FOR
Q
THIS
SOLUTION
REPRESENTS
THE
CONDITIONAL
MEDIAN
I
E
THE
FUNCTION
Y
X
SUCH
THAT
THE
PROBABILITY
MASS
FOR
T
Y
X
IS
THE
SAME
AS
FOR
T
Y
X
ALSO
SHOW
THAT
THE
MINIMUM
EXPECTED
LQ
LOSS
FOR
Q
IS
GIVEN
BY
THE
CONDITIONAL
MODE
I
E
BY
THE
FUNCTION
Y
X
EQUAL
TO
THE
VALUE
OF
T
THAT
MAXIMIZES
P
T
X
FOR
EACH
X
IN
SECTION
WE
INTRODUCED
THE
IDEA
OF
ENTROPY
H
X
AS
THE
INFORMATION
GAINED
ON
OBSERVING
THE
VALUE
OF
A
RANDOM
VARIABLE
X
HAVING
DISTRIBUTION
P
X
WE
SAW
THAT
FOR
INDEPENDENT
VARIABLES
X
AND
Y
FOR
WHICH
P
X
Y
P
X
P
Y
THE
ENTROPY
FUNCTIONS
ARE
ADDITIVE
SO
THAT
H
X
Y
H
X
H
Y
IN
THIS
EXERCISE
WE
DERIVE
THE
RELATION
BETWEEN
H
AND
P
IN
THE
FORM
OF
A
FUNCTION
H
P
FIRST
SHOW
THAT
H
P
AND
HENCE
BY
INDUCTION
THAT
H
PN
NH
P
WHERE
N
IS
A
POSITIVE
INTEGER
HENCE
SHOW
THAT
H
PN
M
N
M
H
P
WHERE
M
IS
ALSO
A
POSITIVE
INTEGER
THIS
IMPLIES
THAT
H
PX
XH
P
WHERE
X
IS
A
POSITIVE
RATIONAL
NUMBER
AND
HENCE
BY
CONTINUITY
WHEN
IT
IS
A
POSITIVE
REAL
NUMBER
FINALLY
SHOW
THAT
THIS
IMPLIES
H
P
MUST
TAKE
THE
FORM
H
P
LN
P
WWW
CONSIDER
AN
M
STATE
DISCRETE
RANDOM
VARIABLE
X
AND
USE
JENSEN
IN
EQUALITY
IN
THE
FORM
TO
SHOW
THAT
THE
ENTROPY
OF
ITS
DISTRIBUTION
P
X
SATISFIES
H
X
LN
M
EVALUATE
THE
KULLBACK
LEIBLER
DIVERGENCE
BETWEEN
TWO
GAUSSIANS
P
X
N
X
Μ
AND
Q
X
N
X
M
TABLE
THE
JOINT
DISTRIBUTION
P
X
Y
FOR
TWO
BINARY
VARIABLES
Y
X
AND
Y
USED
IN
EXERCISE
X
WWW
CONSIDER
TWO
VARIABLES
X
AND
Y
HAVING
JOINT
DISTRIBUTION
P
X
Y
SHOW
THAT
THE
DIFFERENTIAL
ENTROPY
OF
THIS
PAIR
OF
VARIABLES
SATISFIES
H
X
Y
H
X
H
Y
WITH
EQUALITY
IF
AND
ONLY
IF
X
AND
Y
ARE
STATISTICALLY
INDEPENDENT
CONSIDER
A
VECTOR
X
OF
CONTINUOUS
VARIABLES
WITH
DISTRIBUTION
P
X
AND
CORRE
SPONDING
ENTROPY
H
X
SUPPOSE
THAT
WE
MAKE
A
NONSINGULAR
LINEAR
TRANSFORMATION
OF
X
TO
OBTAIN
A
NEW
VARIABLE
Y
AX
SHOW
THAT
THE
CORRESPONDING
ENTROPY
IS
GIVEN
BY
H
Y
H
X
LN
A
WHERE
A
DENOTES
THE
DETERMINANT
OF
A
SUPPOSE
THAT
THE
CONDITIONAL
ENTROPY
H
Y
X
BETWEEN
TWO
DISCRETE
RANDOM
VARIABLES
X
AND
Y
IS
ZERO
SHOW
THAT
FOR
ALL
VALUES
OF
X
SUCH
THAT
P
X
THE
VARIABLE
Y
MUST
BE
A
FUNCTION
OF
X
IN
OTHER
WORDS
FOR
EACH
X
THERE
IS
ONLY
ONE
VALUE
OF
Y
SUCH
THAT
P
Y
X
WWW
USE
THE
CALCULUS
OF
VARIATIONS
TO
SHOW
THAT
THE
STATIONARY
POINT
OF
THE
FUNCTIONAL
IS
GIVEN
BY
THEN
USE
THE
CONSTRAINTS
AND
TO
ELIMINATE
THE
LAGRANGE
MULTIPLIERS
AND
HENCE
SHOW
THAT
THE
MAXIMUM
ENTROPY
SOLUTION
IS
GIVEN
BY
THE
GAUSSIAN
WWW
USE
THE
RESULTS
AND
TO
SHOW
THAT
THE
ENTROPY
OF
THE
UNIVARIATE
GAUSSIAN
IS
GIVEN
BY
A
STRICTLY
CONVEX
FUNCTION
IS
DEFINED
AS
ONE
FOR
WHICH
EVERY
CHORD
LIES
ABOVE
THE
FUNCTION
SHOW
THAT
THIS
IS
EQUIVALENT
TO
THE
CONDITION
THAT
THE
SECOND
DERIVATIVE
OF
THE
FUNCTION
BE
POSITIVE
USING
THE
DEFINITION
TOGETHER
WITH
THE
PRODUCT
RULE
OF
PROBABILITY
PROVE
THE
RESULT
WWW
USING
PROOF
BY
INDUCTION
SHOW
THAT
THE
INEQUALITY
FOR
CONVEX
FUNCTIONS
IMPLIES
THE
RESULT
CONSIDER
TWO
BINARY
VARIABLES
X
AND
Y
HAVING
THE
JOINT
DISTRIBUTION
GIVEN
IN
TABLE
EVALUATE
THE
FOLLOWING
QUANTITIES
A
H
X
C
H
Y
X
E
H
X
Y
B
H
Y
D
H
X
Y
F
I
X
Y
DRAW
A
DIAGRAM
TO
SHOW
THE
RELATIONSHIP
BETWEEN
THESE
VARIOUS
QUANTITIES
BY
APPLYING
JENSEN
INEQUALITY
WITH
F
X
LN
X
SHOW
THAT
THE
ARITH
METIC
MEAN
OF
A
SET
OF
REAL
NUMBERS
IS
NEVER
LESS
THAN
THEIR
GEOMETRICAL
MEAN
WWW
USING
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
SHOW
THAT
THE
MUTUAL
INFORMATION
I
X
Y
SATISFIES
THE
RELATION
IN
CHAPTER
WE
EMPHASIZED
THE
CENTRAL
ROLE
PLAYED
BY
PROBABILITY
THEORY
IN
THE
SOLUTION
OF
PATTERN
RECOGNITION
PROBLEMS
WE
TURN
NOW
TO
AN
EXPLORATION
OF
SOME
PARTICULAR
EXAMPLES
OF
PROBABILITY
DISTRIBUTIONS
AND
THEIR
PROPERTIES
AS
WELL
AS
BE
ING
OF
GREAT
INTEREST
IN
THEIR
OWN
RIGHT
THESE
DISTRIBUTIONS
CAN
FORM
BUILDING
BLOCKS
FOR
MORE
COMPLEX
MODELS
AND
WILL
BE
USED
EXTENSIVELY
THROUGHOUT
THE
BOOK
THE
DISTRIBUTIONS
INTRODUCED
IN
THIS
CHAPTER
WILL
ALSO
SERVE
ANOTHER
IMPORTANT
PURPOSE
NAMELY
TO
PROVIDE
US
WITH
THE
OPPORTUNITY
TO
DISCUSS
SOME
KEY
STATISTICAL
CONCEPTS
SUCH
AS
BAYESIAN
INFERENCE
IN
THE
CONTEXT
OF
SIMPLE
MODELS
BEFORE
WE
ENCOUNTER
THEM
IN
MORE
COMPLEX
SITUATIONS
IN
LATER
CHAPTERS
ONE
ROLE
FOR
THE
DISTRIBUTIONS
DISCUSSED
IN
THIS
CHAPTER
IS
TO
MODEL
THE
PROB
ABILITY
DISTRIBUTION
P
X
OF
A
RANDOM
VARIABLE
X
GIVEN
A
FINITE
SET
XN
OF
OBSERVATIONS
THIS
PROBLEM
IS
KNOWN
AS
DENSITY
ESTIMATION
FOR
THE
PURPOSES
OF
THIS
CHAPTER
WE
SHALL
ASSUME
THAT
THE
DATA
POINTS
ARE
INDEPENDENT
AND
IDENTICALLY
DISTRIBUTED
IT
SHOULD
BE
EMPHASIZED
THAT
THE
PROBLEM
OF
DENSITY
ESTIMATION
IS
FUN
DAMENTALLY
ILL
POSED
BECAUSE
THERE
ARE
INFINITELY
MANY
PROBABILITY
DISTRIBUTIONS
THAT
COULD
HAVE
GIVEN
RISE
TO
THE
OBSERVED
FINITE
DATA
SET
INDEED
ANY
DISTRIBUTION
P
X
THAT
IS
NONZERO
AT
EACH
OF
THE
DATA
POINTS
XN
IS
A
POTENTIAL
CANDIDATE
THE
ISSUE
OF
CHOOSING
AN
APPROPRIATE
DISTRIBUTION
RELATES
TO
THE
PROBLEM
OF
MODEL
SELEC
TION
THAT
HAS
ALREADY
BEEN
ENCOUNTERED
IN
THE
CONTEXT
OF
POLYNOMIAL
CURVE
FITTING
IN
CHAPTER
AND
THAT
IS
A
CENTRAL
ISSUE
IN
PATTERN
RECOGNITION
WE
BEGIN
BY
CONSIDERING
THE
BINOMIAL
AND
MULTINOMIAL
DISTRIBUTIONS
FOR
DISCRETE
RANDOM
VARIABLES
AND
THE
GAUSSIAN
DISTRIBUTION
FOR
CONTINUOUS
RANDOM
VARIABLES
THESE
ARE
SPECIFIC
EXAMPLES
OF
PARAMETRIC
DISTRIBUTIONS
SO
CALLED
BECAUSE
THEY
ARE
GOVERNED
BY
A
SMALL
NUMBER
OF
ADAPTIVE
PARAMETERS
SUCH
AS
THE
MEAN
AND
VARIANCE
IN
THE
CASE
OF
A
GAUSSIAN
FOR
EXAMPLE
TO
APPLY
SUCH
MODELS
TO
THE
PROBLEM
OF
DENSITY
ESTIMATION
WE
NEED
A
PROCEDURE
FOR
DETERMINING
SUITABLE
VALUES
FOR
THE
PARAMETERS
GIVEN
AN
OBSERVED
DATA
SET
IN
A
FREQUENTIST
TREATMENT
WE
CHOOSE
SPECIFIC
VALUES
FOR
THE
PARAMETERS
BY
OPTIMIZING
SOME
CRITERION
SUCH
AS
THE
LIKELIHOOD
FUNCTION
BY
CONTRAST
IN
A
BAYESIAN
TREATMENT
WE
INTRODUCE
PRIOR
DISTRIBUTIONS
OVER
THE
PARAMETERS
AND
THEN
USE
BAYES
THEOREM
TO
COMPUTE
THE
CORRESPONDING
POSTERIOR
DISTRIBUTION
GIVEN
THE
OBSERVED
DATA
WE
SHALL
SEE
THAT
AN
IMPORTANT
ROLE
IS
PLAYED
BY
CONJUGATE
PRIORS
THAT
LEAD
TO
POSTERIOR
DISTRIBUTIONS
HAVING
THE
SAME
FUNCTIONAL
FORM
AS
THE
PRIOR
AND
THAT
THERE
FORE
LEAD
TO
A
GREATLY
SIMPLIFIED
BAYESIAN
ANALYSIS
FOR
EXAMPLE
THE
CONJUGATE
PRIOR
FOR
THE
PARAMETERS
OF
THE
MULTINOMIAL
DISTRIBUTION
IS
CALLED
THE
DIRICHLET
DISTRIBUTION
WHILE
THE
CONJUGATE
PRIOR
FOR
THE
MEAN
OF
A
GAUSSIAN
IS
ANOTHER
GAUSSIAN
ALL
OF
THESE
DISTRIBUTIONS
ARE
EXAMPLES
OF
THE
EXPONENTIAL
FAMILY
OF
DISTRIBUTIONS
WHICH
POSSESS
A
NUMBER
OF
IMPORTANT
PROPERTIES
AND
WHICH
WILL
BE
DISCUSSED
IN
SOME
DETAIL
ONE
LIMITATION
OF
THE
PARAMETRIC
APPROACH
IS
THAT
IT
ASSUMES
A
SPECIFIC
FUNCTIONAL
FORM
FOR
THE
DISTRIBUTION
WHICH
MAY
TURN
OUT
TO
BE
INAPPROPRIATE
FOR
A
PARTICULAR
APPLICATION
AN
ALTERNATIVE
APPROACH
IS
GIVEN
BY
NONPARAMETRIC
DENSITY
ESTIMATION
METHODS
IN
WHICH
THE
FORM
OF
THE
DISTRIBUTION
TYPICALLY
DEPENDS
ON
THE
SIZE
OF
THE
DATA
SET
SUCH
MODELS
STILL
CONTAIN
PARAMETERS
BUT
THESE
CONTROL
THE
MODEL
COMPLEXITY
RATHER
THAN
THE
FORM
OF
THE
DISTRIBUTION
WE
END
THIS
CHAPTER
BY
CONSIDERING
THREE
NONPARAMETRIC
METHODS
BASED
RESPECTIVELY
ON
HISTOGRAMS
NEAREST
NEIGHBOURS
AND
KERNELS
BINARY
VARIABLES
WE
BEGIN
BY
CONSIDERING
A
SINGLE
BINARY
RANDOM
VARIABLE
X
FOR
EXAMPLE
X
MIGHT
DESCRIBE
THE
OUTCOME
OF
FLIPPING
A
COIN
WITH
X
REPRESENTING
HEADS
AND
X
REPRESENTING
TAILS
WE
CAN
IMAGINE
THAT
THIS
IS
A
DAMAGED
COIN
SO
THAT
THE
PROBABILITY
OF
LANDING
HEADS
IS
NOT
NECESSARILY
THE
SAME
AS
THAT
OF
LANDING
TAILS
THE
PROBABILITY
OF
X
WILL
BE
DENOTED
BY
THE
PARAMETER
Μ
SO
THAT
P
X
Μ
Μ
EXERCISE
WHERE
Μ
FROM
WHICH
IT
FOLLOWS
THAT
P
X
Μ
Μ
THE
PROBABILITY
DISTRIBUTION
OVER
X
CAN
THEREFORE
BE
WRITTEN
IN
THE
FORM
BERN
X
Μ
ΜX
Μ
X
WHICH
IS
KNOWN
AS
THE
BERNOULLI
DISTRIBUTION
IT
IS
EASILY
VERIFIED
THAT
THIS
DISTRIBUTION
IS
NORMALIZED
AND
THAT
IT
HAS
MEAN
AND
VARIANCE
GIVEN
BY
E
X
Μ
VAR
X
Μ
Μ
NOW
SUPPOSE
WE
HAVE
A
DATA
SET
XN
OF
OBSERVED
VALUES
OF
X
WE
CAN
CONSTRUCT
THE
LIKELIHOOD
FUNCTION
WHICH
IS
A
FUNCTION
OF
Μ
ON
THE
ASSUMPTION
THAT
THE
OBSERVATIONS
ARE
DRAWN
INDEPENDENTLY
FROM
P
X
Μ
SO
THAT
N
N
P
D
Μ
TT
P
XN
Μ
TT
ΜXN
Μ
XN
IN
A
FREQUENTIST
SETTING
WE
CAN
ESTIMATE
A
VALUE
FOR
Μ
BY
MAXIMIZING
THE
LIKELIHOOD
FUNCTION
OR
EQUIVALENTLY
BY
MAXIMIZING
THE
LOGARITHM
OF
THE
LIKELIHOOD
IN
THE
CASE
OF
THE
BERNOULLI
DISTRIBUTION
THE
LOG
LIKELIHOOD
FUNCTION
IS
GIVEN
BY
N
N
LN
P
D
Μ
LN
P
XN
Μ
XN
LN
Μ
XN
LN
Μ
SECTION
AT
THIS
POINT
IT
IS
WORTH
NOTING
THAT
THE
LOG
LIKELIHOOD
FUNCTION
DEPENDS
ON
THE
N
OBSERVATIONS
XN
ONLY
THROUGH
THEIR
SUM
N
XN
THIS
SUM
PROVIDES
AN
EXAMPLE
OF
A
SUFFICIENT
STATISTIC
FOR
THE
DATA
UNDER
THIS
DISTRIBUTION
AND
WE
SHALL
STUDY
THE
IMPOR
TANT
ROLE
OF
SUFFICIENT
STATISTICS
IN
SOME
DETAIL
IF
WE
SET
THE
DERIVATIVE
OF
LN
P
Μ
WITH
RESPECT
TO
Μ
EQUAL
TO
ZERO
WE
OBTAIN
THE
MAXIMUM
LIKELIHOOD
ESTIMATOR
ΜML
N
N
XN
N
JACOB
BERNOULLI
JACOB
BERNOULLI
ALSO
KNOWN
AS
JACQUES
OR
JAMES
BERNOULLI
WAS
A
SWISS
MATHEMATICIAN
AND
WAS
THE
FIRST
OF
MANY
IN
THE
BERNOULLI
FAMILY
TO
PURSUE
A
CAREER
IN
SCIENCE
AND
MATHEMATICS
ALTHOUGH
COMPELLED
TO
STUDY
PHILOSOPHY
AND
THEOLOGY
AGAINST
HIS
WILL
BY
HIS
PARENTS
HE
TRAVELLED
EXTENSIVELY
AFTER
GRADUATING
IN
ORDER
TO
MEET
WITH
MANY
OF
THE
LEADING
SCIENTISTS
OF
HIS
TIME
INCLUDING
BOYLE
AND
HOOKE
IN
ENGLAND
WHEN
HE
RETURNED
TO
SWITZERLAND
HE
TAUGHT
MECHANICS
AND
BECAME
PROFESSOR
OF
MATHEMATICS
AT
BASEL
IN
UNFORTUNATELY
RIVALRY
BETWEEN
JACOB
AND
HIS
YOUNGER
BROTHER
JOHANN
TURNED
AN
INITIALLY
PRODUCTIVE
COLLABORA
TION
INTO
A
BITTER
AND
PUBLIC
DISPUTE
JACOB
MOST
SIG
NIFICANT
CONTRIBUTIONS
TO
MATHEMATICS
APPEARED
IN
THE
ART
OF
CONJECTURE
PUBLISHED
IN
EIGHT
YEARS
AFTER
HIS
DEATH
WHICH
DEALS
WITH
TOPICS
IN
PROBABILITY
THE
ORY
INCLUDING
WHAT
HAS
BECOME
KNOWN
AS
THE
BERNOULLI
DISTRIBUTION
FIGURE
HISTOGRAM
PLOT
OF
THE
BINOMIAL
DIS
TRIBUTION
AS
A
FUNCTION
OF
M
FOR
N
AND
Μ
M
WHICH
IS
ALSO
KNOWN
AS
THE
SAMPLE
MEAN
IF
WE
DENOTE
THE
NUMBER
OF
OBSERVATIONS
OF
X
HEADS
WITHIN
THIS
DATA
SET
BY
M
THEN
WE
CAN
WRITE
IN
THE
FORM
M
ΜML
N
SO
THAT
THE
PROBABILITY
OF
LANDING
HEADS
IS
GIVEN
IN
THIS
MAXIMUM
LIKELIHOOD
FRAME
WORK
BY
THE
FRACTION
OF
OBSERVATIONS
OF
HEADS
IN
THE
DATA
SET
NOW
SUPPOSE
WE
FLIP
A
COIN
SAY
TIMES
AND
HAPPEN
TO
OBSERVE
HEADS
THEN
N
M
AND
ΜML
IN
THIS
CASE
THE
MAXIMUM
LIKELIHOOD
RESULT
WOULD
PREDICT
THAT
ALL
FUTURE
OBSERVATIONS
SHOULD
GIVE
HEADS
COMMON
SENSE
TELLS
US
THAT
THIS
IS
UNREASONABLE
AND
IN
FACT
THIS
IS
AN
EXTREME
EXAMPLE
OF
THE
OVER
FITTING
ASSOCI
ATED
WITH
MAXIMUM
LIKELIHOOD
WE
SHALL
SEE
SHORTLY
HOW
TO
ARRIVE
AT
MORE
SENSIBLE
CONCLUSIONS
THROUGH
THE
INTRODUCTION
OF
A
PRIOR
DISTRIBUTION
OVER
Μ
WE
CAN
ALSO
WORK
OUT
THE
DISTRIBUTION
OF
THE
NUMBER
M
OF
OBSERVATIONS
OF
X
GIVEN
THAT
THE
DATA
SET
HAS
SIZE
N
THIS
IS
CALLED
THE
BINOMIAL
DISTRIBUTION
AND
FROM
WE
SEE
THAT
IT
IS
PROPORTIONAL
TO
ΜM
Μ
N
M
IN
ORDER
TO
OBTAIN
THE
NORMALIZATION
COEFFICIENT
WE
NOTE
THAT
OUT
OF
N
COIN
FLIPS
WE
HAVE
TO
ADD
UP
ALL
OF
THE
POSSIBLE
WAYS
OF
OBTAINING
M
HEADS
SO
THAT
THE
BINOMIAL
DISTRIBUTION
CAN
BE
WRITTEN
WHERE
BIN
M
N
Μ
N
ΜM
Μ
N
M
N
N
M
N
M
M
EXERCISE
IS
THE
NUMBER
OF
WAYS
OF
CHOOSING
M
OBJECTS
OUT
OF
A
TOTAL
OF
N
IDENTICAL
OBJECTS
FIGURE
SHOWS
A
PLOT
OF
THE
BINOMIAL
DISTRIBUTION
FOR
N
AND
Μ
THE
MEAN
AND
VARIANCE
OF
THE
BINOMIAL
DISTRIBUTION
CAN
BE
FOUND
BY
USING
THE
RESULT
OF
EXERCISE
WHICH
SHOWS
THAT
FOR
INDEPENDENT
EVENTS
THE
MEAN
OF
THE
SUM
IS
THE
SUM
OF
THE
MEANS
AND
THE
VARIANCE
OF
THE
SUM
IS
THE
SUM
OF
THE
VARIANCES
BECAUSE
M
XN
AND
FOR
EACH
OBSERVATION
THE
MEAN
AND
VARIANCE
ARE
GIVEN
BY
AND
RESPECTIVELY
WE
HAVE
N
E
M
MBIN
M
N
Μ
NΜ
M
N
EXERCISE
EXERCISE
VAR
M
M
E
M
BIN
M
N
Μ
N
Μ
Μ
M
THESE
RESULTS
CAN
ALSO
BE
PROVED
DIRECTLY
USING
CALCULUS
THE
BETA
DISTRIBUTION
WE
HAVE
SEEN
IN
THAT
THE
MAXIMUM
LIKELIHOOD
SETTING
FOR
THE
PARAMETER
Μ
IN
THE
BERNOULLI
DISTRIBUTION
AND
HENCE
IN
THE
BINOMIAL
DISTRIBUTION
IS
GIVEN
BY
THE
FRACTION
OF
THE
OBSERVATIONS
IN
THE
DATA
SET
HAVING
X
AS
WE
HAVE
ALREADY
NOTED
THIS
CAN
GIVE
SEVERELY
OVER
FITTED
RESULTS
FOR
SMALL
DATA
SETS
IN
ORDER
TO
DEVELOP
A
BAYESIAN
TREATMENT
FOR
THIS
PROBLEM
WE
NEED
TO
INTRODUCE
A
PRIOR
DISTRIBUTION
P
Μ
OVER
THE
PARAMETER
Μ
HERE
WE
CONSIDER
A
FORM
OF
PRIOR
DISTRIBUTION
THAT
HAS
A
SIMPLE
INTERPRETATION
AS
WELL
AS
SOME
USEFUL
ANALYTICAL
PROPERTIES
TO
MOTIVATE
THIS
PRIOR
WE
NOTE
THAT
THE
LIKELIHOOD
FUNCTION
TAKES
THE
FORM
OF
THE
PRODUCT
OF
FACTORS
OF
THE
FORM
ΜX
Μ
X
IF
WE
CHOOSE
A
PRIOR
TO
BE
PROPORTIONAL
TO
POWERS
OF
Μ
AND
Μ
THEN
THE
POSTERIOR
DISTRIBUTION
WHICH
IS
PROPORTIONAL
TO
THE
PRODUCT
OF
THE
PRIOR
AND
THE
LIKELIHOOD
FUNCTION
WILL
HAVE
THE
SAME
FUNCTIONAL
FORM
AS
THE
PRIOR
THIS
PROPERTY
IS
CALLED
CONJUGACY
AND
WE
WILL
SEE
SEVERAL
EXAMPLES
OF
IT
LATER
IN
THIS
CHAPTER
WE
THEREFORE
CHOOSE
A
PRIOR
CALLED
THE
BETA
DISTRIBUTION
GIVEN
BY
BETA
Γ
A
B
A
B
Γ
A
Γ
B
WHERE
Γ
X
IS
THE
GAMMA
FUNCTION
DEFINED
BY
AND
THE
COEFFICIENT
IN
ENSURES
THAT
THE
BETA
DISTRIBUTION
IS
NORMALIZED
SO
THAT
R
EXERCISE
THE
MEAN
AND
VARIANCE
OF
THE
BETA
DISTRIBUTION
ARE
GIVEN
BY
E
Μ
A
A
B
VAR
Μ
AB
A
B
A
B
THE
PARAMETERS
A
AND
B
ARE
OFTEN
CALLED
HYPERPARAMETERS
BECAUSE
THEY
CONTROL
THE
DISTRIBUTION
OF
THE
PARAMETER
Μ
FIGURE
SHOWS
PLOTS
OF
THE
BETA
DISTRIBUTION
FOR
VARIOUS
VALUES
OF
THE
HYPERPARAMETERS
THE
POSTERIOR
DISTRIBUTION
OF
Μ
IS
NOW
OBTAINED
BY
MULTIPLYING
THE
BETA
PRIOR
BY
THE
BINOMIAL
LIKELIHOOD
FUNCTION
AND
NORMALIZING
KEEPING
ONLY
THE
FACTORS
THAT
DEPEND
ON
Μ
WE
SEE
THAT
THIS
POSTERIOR
DISTRIBUTION
HAS
THE
FORM
P
Μ
M
L
A
B
ΜM
A
Μ
L
B
Μ
Μ
Μ
Μ
FIGURE
PLOTS
OF
THE
BETA
DISTRIBUTION
BETA
Μ
A
B
GIVEN
BY
AS
A
FUNCTION
OF
Μ
FOR
VARIOUS
VALUES
OF
THE
HYPERPARAMETERS
A
AND
B
WHERE
L
N
M
AND
THEREFORE
CORRESPONDS
TO
THE
NUMBER
OF
TAILS
IN
THE
COIN
EXAMPLE
WE
SEE
THAT
HAS
THE
SAME
FUNCTIONAL
DEPENDENCE
ON
Μ
AS
THE
PRIOR
DISTRIBUTION
REFLECTING
THE
CONJUGACY
PROPERTIES
OF
THE
PRIOR
WITH
RESPECT
TO
THE
LIKE
LIHOOD
FUNCTION
INDEED
IT
IS
SIMPLY
ANOTHER
BETA
DISTRIBUTION
AND
ITS
NORMALIZATION
COEFFICIENT
CAN
THEREFORE
BE
OBTAINED
BY
COMPARISON
WITH
TO
GIVE
P
Μ
M
L
A
B
Γ
M
A
L
B
ΜM
A
Μ
L
B
WE
SEE
THAT
THE
EFFECT
OF
OBSERVING
A
DATA
SET
OF
M
OBSERVATIONS
OF
X
AND
L
OBSERVATIONS
OF
X
HAS
BEEN
TO
INCREASE
THE
VALUE
OF
A
BY
M
AND
THE
VALUE
OF
B
BY
L
IN
GOING
FROM
THE
PRIOR
DISTRIBUTION
TO
THE
POSTERIOR
DISTRIBUTION
THIS
ALLOWS
US
TO
PROVIDE
A
SIMPLE
INTERPRETATION
OF
THE
HYPERPARAMETERS
A
AND
B
IN
THE
PRIOR
AS
AN
EFFECTIVE
NUMBER
OF
OBSERVATIONS
OF
X
AND
X
RESPECTIVELY
NOTE
THAT
A
AND
B
NEED
NOT
BE
INTEGERS
FURTHERMORE
THE
POSTERIOR
DISTRIBUTION
CAN
ACT
AS
THE
PRIOR
IF
WE
SUBSEQUENTLY
OBSERVE
ADDITIONAL
DATA
TO
SEE
THIS
WE
CAN
IMAGINE
TAKING
OBSERVATIONS
ONE
AT
A
TIME
AND
AFTER
EACH
OBSERVATION
UPDATING
THE
CURRENT
POSTERIOR
FIGURE
ILLUSTRATION
OF
ONE
STEP
OF
SEQUENTIAL
BAYESIAN
INFERENCE
THE
PRIOR
IS
GIVEN
BY
A
BETA
DISTRIBUTION
WITH
PARAMETERS
A
B
AND
THE
LIKELIHOOD
FUNCTION
GIVEN
BY
WITH
N
M
CORRESPONDS
TO
A
SINGLE
OBSERVATION
OF
X
SO
THAT
THE
POSTERIOR
IS
GIVEN
BY
A
BETA
DISTRIBUTION
WITH
PARAMETERS
A
B
SECTION
DISTRIBUTION
BY
MULTIPLYING
BY
THE
LIKELIHOOD
FUNCTION
FOR
THE
NEW
OBSERVATION
AND
THEN
NORMALIZING
TO
OBTAIN
THE
NEW
REVISED
POSTERIOR
DISTRIBUTION
AT
EACH
STAGE
THE
POSTERIOR
IS
A
BETA
DISTRIBUTION
WITH
SOME
TOTAL
NUMBER
OF
PRIOR
AND
ACTUAL
OBSERVED
VALUES
FOR
X
AND
X
GIVEN
BY
THE
PARAMETERS
A
AND
B
INCORPORATION
OF
AN
ADDITIONAL
OBSERVATION
OF
X
SIMPLY
CORRESPONDS
TO
INCREMENTING
THE
VALUE
OF
A
BY
WHEREAS
FOR
AN
OBSERVATION
OF
X
WE
INCREMENT
B
BY
FIGURE
ILLUSTRATES
ONE
STEP
IN
THIS
PROCESS
WE
SEE
THAT
THIS
SEQUENTIAL
APPROACH
TO
LEARNING
ARISES
NATURALLY
WHEN
WE
ADOPT
A
BAYESIAN
VIEWPOINT
IT
IS
INDEPENDENT
OF
THE
CHOICE
OF
PRIOR
AND
OF
THE
LIKELIHOOD
FUNCTION
AND
DEPENDS
ONLY
ON
THE
ASSUMPTION
OF
I
I
D
DATA
SEQUENTIAL
METHODS
MAKE
USE
OF
OBSERVATIONS
ONE
AT
A
TIME
OR
IN
SMALL
BATCHES
AND
THEN
DISCARD
THEM
BEFORE
THE
NEXT
OBSERVATIONS
ARE
USED
THEY
CAN
BE
USED
FOR
EXAMPLE
IN
REAL
TIME
LEARNING
SCENARIOS
WHERE
A
STEADY
STREAM
OF
DATA
IS
ARRIVING
AND
PREDICTIONS
MUST
BE
MADE
BEFORE
ALL
OF
THE
DATA
IS
SEEN
BECAUSE
THEY
DO
NOT
REQUIRE
THE
WHOLE
DATA
SET
TO
BE
STORED
OR
LOADED
INTO
MEMORY
SEQUENTIAL
METHODS
ARE
ALSO
USEFUL
FOR
LARGE
DATA
SETS
MAXIMUM
LIKELIHOOD
METHODS
CAN
ALSO
BE
CAST
INTO
A
SEQUENTIAL
FRAMEWORK
IF
OUR
GOAL
IS
TO
PREDICT
AS
BEST
WE
CAN
THE
OUTCOME
OF
THE
NEXT
TRIAL
THEN
WE
MUST
EVALUATE
THE
PREDICTIVE
DISTRIBUTION
OF
X
GIVEN
THE
OBSERVED
DATA
SET
FROM
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
THIS
TAKES
THE
FORM
P
X
D
P
X
Μ
P
Μ
D
DΜ
ΜP
Μ
D
DΜ
E
Μ
D
USING
THE
RESULT
FOR
THE
POSTERIOR
DISTRIBUTION
P
Μ
TOGETHER
WITH
THE
RESULT
FOR
THE
MEAN
OF
THE
BETA
DISTRIBUTION
WE
OBTAIN
M
A
P
X
D
M
A
L
B
WHICH
HAS
A
SIMPLE
INTERPRETATION
AS
THE
TOTAL
FRACTION
OF
OBSERVATIONS
BOTH
REAL
OB
SERVATIONS
AND
FICTITIOUS
PRIOR
OBSERVATIONS
THAT
CORRESPOND
TO
X
NOTE
THAT
IN
THE
LIMIT
OF
AN
INFINITELY
LARGE
DATA
SET
M
L
THE
RESULT
REDUCES
TO
THE
MAXIMUM
LIKELIHOOD
RESULT
AS
WE
SHALL
SEE
IT
IS
A
VERY
GENERAL
PROPERTY
THAT
THE
BAYESIAN
AND
MAXIMUM
LIKELIHOOD
RESULTS
WILL
AGREE
IN
THE
LIMIT
OF
AN
INFINITELY
EXERCISE
EXERCISE
LARGE
DATA
SET
FOR
A
FINITE
DATA
SET
THE
POSTERIOR
MEAN
FOR
Μ
ALWAYS
LIES
BETWEEN
THE
PRIOR
MEAN
AND
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
FOR
Μ
CORRESPONDING
TO
THE
RELATIVE
FREQUENCIES
OF
EVENTS
GIVEN
BY
FROM
FIGURE
WE
SEE
THAT
AS
THE
NUMBER
OF
OBSERVATIONS
INCREASES
SO
THE
POSTERIOR
DISTRIBUTION
BECOMES
MORE
SHARPLY
PEAKED
THIS
CAN
ALSO
BE
SEEN
FROM
THE
RESULT
FOR
THE
VARIANCE
OF
THE
BETA
DISTRIBUTION
IN
WHICH
WE
SEE
THAT
THE
VARIANCE
GOES
TO
ZERO
FOR
A
OR
B
IN
FACT
WE
MIGHT
WONDER
WHETHER
IT
IS
A
GENERAL
PROPERTY
OF
BAYESIAN
LEARNING
THAT
AS
WE
OBSERVE
MORE
AND
MORE
DATA
THE
UNCERTAINTY
REPRESENTED
BY
THE
POSTERIOR
DISTRIBUTION
WILL
STEADILY
DECREASE
TO
ADDRESS
THIS
WE
CAN
TAKE
A
FREQUENTIST
VIEW
OF
BAYESIAN
LEARNING
AND
SHOW
THAT
ON
AVERAGE
SUCH
A
PROPERTY
DOES
INDEED
HOLD
CONSIDER
A
GENERAL
BAYESIAN
INFERENCE
PROBLEM
FOR
A
PARAMETER
Θ
FOR
WHICH
WE
HAVE
OBSERVED
A
DATA
SET
D
DE
SCRIBED
BY
THE
JOINT
DISTRIBUTION
P
Θ
D
THE
FOLLOWING
RESULT
EΘ
Θ
ED
EΘ
Θ
D
WHERE
EΘ
Θ
R
P
Θ
Θ
DΘ
ED
EΘ
Θ
D
R
RR
ΘP
Θ
D
P
D
DD
SAYS
THAT
THE
POSTERIOR
MEAN
OF
Θ
AVERAGED
OVER
THE
DISTRIBUTION
GENERATING
THE
DATA
IS
EQUAL
TO
THE
PRIOR
MEAN
OF
Θ
SIMILARLY
WE
CAN
SHOW
THAT
VARΘ
Θ
ED
VARΘ
Θ
D
VARD
EΘ
Θ
D
THE
TERM
ON
THE
LEFT
HAND
SIDE
OF
IS
THE
PRIOR
VARIANCE
OF
Θ
ON
THE
RIGHT
HAND
SIDE
THE
FIRST
TERM
IS
THE
AVERAGE
POSTERIOR
VARIANCE
OF
Θ
AND
THE
SECOND
TERM
MEASURES
THE
VARIANCE
IN
THE
POSTERIOR
MEAN
OF
Θ
BECAUSE
THIS
VARIANCE
IS
A
POSITIVE
QUANTITY
THIS
RESULT
SHOWS
THAT
ON
AVERAGE
THE
POSTERIOR
VARIANCE
OF
Θ
IS
SMALLER
THAN
THE
PRIOR
VARIANCE
THE
REDUCTION
IN
VARIANCE
IS
GREATER
IF
THE
VARIANCE
IN
THE
POSTERIOR
MEAN
IS
GREATER
NOTE
HOWEVER
THAT
THIS
RESULT
ONLY
HOLDS
ON
AVERAGE
AND
THAT
FOR
A
PARTICULAR
OBSERVED
DATA
SET
IT
IS
POSSIBLE
FOR
THE
POSTERIOR
VARIANCE
TO
BE
LARGER
THAN
THE
PRIOR
VARIANCE
MULTINOMIAL
VARIABLES
BINARY
VARIABLES
CAN
BE
USED
TO
DESCRIBE
QUANTITIES
THAT
CAN
TAKE
ONE
OF
TWO
POSSIBLE
VALUES
OFTEN
HOWEVER
WE
ENCOUNTER
DISCRETE
VARIABLES
THAT
CAN
TAKE
ON
ONE
OF
K
POSSIBLE
MUTUALLY
EXCLUSIVE
STATES
ALTHOUGH
THERE
ARE
VARIOUS
ALTERNATIVE
WAYS
TO
EXPRESS
SUCH
VARIABLES
WE
SHALL
SEE
SHORTLY
THAT
A
PARTICULARLY
CONVENIENT
REPRESEN
TATION
IS
THE
OF
K
SCHEME
IN
WHICH
THE
VARIABLE
IS
REPRESENTED
BY
A
K
DIMENSIONAL
VECTOR
X
IN
WHICH
ONE
OF
THE
ELEMENTS
XK
EQUALS
AND
ALL
REMAINING
ELEMENTS
EQUAL
SO
FOR
INSTANCE
IF
WE
HAVE
A
VARIABLE
THAT
CAN
TAKE
K
STATES
AND
A
PARTICULAR
OBSERVATION
OF
THE
VARIABLE
HAPPENS
TO
CORRESPOND
TO
THE
STATE
WHERE
THEN
X
WILL
BE
REPRESENTED
BY
X
T
NOTE
THAT
SUCH
VECTORS
SATISFY
K
XK
IF
WE
DENOTE
THE
PROBABILITY
OF
XK
BY
THE
PARAMETER
ΜK
THEN
THE
DISTRIBUTION
OF
X
IS
GIVEN
K
XK
K
WHERE
Μ
ΜK
T
AND
THE
PARAMETERS
ΜK
ARE
CONSTRAINED
TO
SATISFY
ΜK
AND
K
ΜK
BECAUSE
THEY
REPRESENT
PROBABILITIES
THE
DISTRIBUTION
CAN
BE
REGARDED
AS
A
GENERALIZATION
OF
THE
BERNOULLI
DISTRIBUTION
TO
MORE
THAN
TWO
OUTCOMES
IT
IS
EASILY
SEEN
THAT
THE
DISTRIBUTION
IS
NORMALIZED
P
X
Μ
ΜK
AND
THAT
X
K
E
X
Μ
P
X
Μ
X
ΜM
T
Μ
X
NOW
CONSIDER
A
DATA
SET
OF
N
INDEPENDENT
OBSERVATIONS
XN
THE
CORRESPONDING
LIKELIHOOD
FUNCTION
TAKES
THE
FORM
N
K
K
K
P
D
Μ
TT
TT
ΜXNK
TT
PN
XNK
TT
ΜMK
WE
SEE
THAT
THE
LIKELIHOOD
FUNCTION
DEPENDS
ON
THE
N
DATA
POINTS
ONLY
THROUGH
THE
K
QUANTITIES
SECTION
APPENDIX
E
MK
XNK
N
WHICH
REPRESENT
THE
NUMBER
OF
OBSERVATIONS
OF
XK
THESE
ARE
CALLED
THE
SUFFICIENT
STATISTICS
FOR
THIS
DISTRIBUTION
IN
ORDER
TO
FIND
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
Μ
WE
NEED
TO
MAXIMIZE
LN
P
Μ
WITH
RESPECT
TO
ΜK
TAKING
ACCOUNT
OF
THE
CONSTRAINT
THAT
THE
ΜK
MUST
SUM
TO
ONE
THIS
CAN
BE
ACHIEVED
USING
A
LAGRANGE
MULTIPLIER
Λ
AND
MAXIMIZING
K
MK
LN
ΜK
Λ
K
K
ΜK
SETTING
THE
DERIVATIVE
OF
WITH
RESPECT
TO
ΜK
TO
ZERO
WE
OBTAIN
ΜK
MK
Λ
WE
CAN
SOLVE
FOR
THE
LAGRANGE
MULTIPLIER
Λ
BY
SUBSTITUTING
INTO
THE
CONSTRAINT
THE
FORM
ΜML
MK
N
WHICH
IS
THE
FRACTION
OF
THE
N
OBSERVATIONS
FOR
WHICH
XK
WE
CAN
CONSIDER
THE
JOINT
DISTRIBUTION
OF
THE
QUANTITIES
MK
CONDITIONED
ON
THE
PARAMETERS
Μ
AND
ON
THE
TOTAL
NUMBER
N
OF
OBSERVATIONS
FROM
THIS
TAKES
THE
FORM
MULT
MK
Μ
N
N
MK
K
MK
K
K
WHICH
IS
KNOWN
AS
THE
MULTINOMIAL
DISTRIBUTION
THE
NORMALIZATION
COEFFICIENT
IS
THE
NUMBER
OF
WAYS
OF
PARTITIONING
N
OBJECTS
INTO
K
GROUPS
OF
SIZE
MK
AND
IS
GIVEN
BY
N
MK
N
MK
NOTE
THAT
THE
VARIABLES
MK
ARE
SUBJECT
TO
THE
CONSTRAINT
K
MK
N
K
THE
DIRICHLET
DISTRIBUTION
WE
NOW
INTRODUCE
A
FAMILY
OF
PRIOR
DISTRIBUTIONS
FOR
THE
PARAMETERS
ΜK
OF
THE
MULTINOMIAL
DISTRIBUTION
BY
INSPECTION
OF
THE
FORM
OF
THE
MULTINOMIAL
DISTRIBUTION
WE
SEE
THAT
THE
CONJUGATE
PRIOR
IS
GIVEN
BY
K
ΑK
K
EXERCISE
K
WHERE
ΜK
AND
K
ΜK
HERE
ΑK
ARE
THE
PARAMETERS
OF
THE
DISTRIBUTION
AND
Α
DENOTES
ΑK
T
NOTE
THAT
BECAUSE
OF
THE
SUMMATION
CONSTRAINT
THE
DISTRIBUTION
OVER
THE
SPACE
OF
THE
ΜK
IS
CONFINED
TO
A
SIMPLEX
OF
DIMENSIONALITY
K
AS
ILLUSTRATED
FOR
K
IN
FIGURE
THE
NORMALIZED
FORM
FOR
THIS
DISTRIBUTION
IS
BY
DIR
Μ
Α
Γ
TT
ΜΑK
WHICH
IS
CALLED
THE
DIRICHLET
DISTRIBUTION
HERE
Γ
X
IS
THE
GAMMA
FUNCTION
DEFINED
BY
WHILE
K
ΑK
K
FIGURE
THE
DIRICHLET
DISTRIBUTION
OVER
THREE
VARIABLES
IS
CONFINED
TO
A
SIMPLEX
A
BOUNDED
LINEAR
MANIFOLD
OF
THE
FORM
SHOWN
AS
A
CONSEQUENCE
OF
THE
CONSTRAINTS
ΜK
AND
K
ΜK
PLOTS
OF
THE
DIRICHLET
DISTRIBUTION
OVER
THE
SIMPLEX
FOR
VARIOUS
SETTINGS
OF
THE
PARAM
ETERS
ΑK
ARE
SHOWN
IN
FIGURE
MULTIPLYING
THE
PRIOR
BY
THE
LIKELIHOOD
FUNCTION
WE
OBTAIN
THE
POSTERIOR
DISTRIBUTION
FOR
THE
PARAMETERS
ΜK
IN
THE
FORM
P
Μ
D
Α
P
D
Μ
P
Μ
Α
TT
ΜΑK
MK
K
WE
SEE
THAT
THE
POSTERIOR
DISTRIBUTION
AGAIN
TAKES
THE
FORM
OF
A
DIRICHLET
DISTRIBUTION
CONFIRMING
THAT
THE
DIRICHLET
IS
INDEED
A
CONJUGATE
PRIOR
FOR
THE
MULTINOMIAL
THIS
ALLOWS
US
TO
DETERMINE
THE
NORMALIZATION
COEFFICIENT
BY
COMPARISON
WITH
SO
THAT
P
Μ
D
Α
DIR
Μ
Α
M
K
Γ
N
TT
ΜΑK
MK
WHERE
WE
HAVE
DENOTED
M
MK
T
AS
FOR
THE
CASE
OF
THE
BINOMIAL
DISTRIBUTION
WITH
ITS
BETA
PRIOR
WE
CAN
INTERPRET
THE
PARAMETERS
ΑK
OF
THE
DIRICHLET
PRIOR
AS
AN
EFFECTIVE
NUMBER
OF
OBSERVATIONS
OF
XK
NOTE
THAT
TWO
STATE
QUANTITIES
CAN
EITHER
BE
REPRESENTED
AS
BINARY
VARIABLES
AND
LEJEUNE
DIRICHLET
JOHANN
PETER
GUSTAV
LEJEUNE
DIRICHLET
WAS
A
MODEST
AND
RE
SERVED
MATHEMATICIAN
WHO
MADE
CONTRIBUTIONS
IN
NUMBER
THEORY
ME
CHANICS
AND
ASTRONOMY
AND
WHO
GAVE
THE
FIRST
RIGOROUS
ANALYSIS
OF
FOURIER
SERIES
HIS
FAMILY
ORIGINATED
FROM
RICHELET
IN
BELGIUM
AND
THE
NAME
LEJEUNE
DIRICHLET
COMES
FROM
LE
JEUNE
DE
RICHELET
THE
YOUNG
PERSON
FROM
RICHELET
DIRICHLET
FIRST
PAPER
WHICH
WAS
PUBLISHED
IN
BROUGHT
HIM
INSTANT
FAME
IT
CONCERNED
FER
MAT
LAST
THEOREM
WHICH
CLAIMS
THAT
THERE
ARE
NO
POSITIVE
INTEGER
SOLUTIONS
TO
XN
YN
ZN
FOR
N
DIRICHLET
GAVE
A
PARTIAL
PROOF
FOR
THE
CASE
N
WHICH
WAS
SENT
TO
LEGENDRE
FOR
REVIEW
AND
WHO
IN
TURN
COM
PLETED
THE
PROOF
LATER
DIRICHLET
GAVE
A
COMPLETE
PROOF
FOR
N
ALTHOUGH
A
FULL
PROOF
OF
FERMAT
LAST
THEO
REM
FOR
ARBITRARY
N
HAD
TO
WAIT
UNTIL
THE
WORK
OF
ANDREW
WILES
IN
THE
CLOSING
YEARS
OF
THE
CENTURY
FIGURE
PLOTS
OF
THE
DIRICHLET
DISTRIBUTION
OVER
THREE
VARIABLES
WHERE
THE
TWO
HORIZONTAL
AXES
ARE
COORDINATES
IN
THE
PLANE
OF
THE
SIMPLEX
AND
THE
VERTICAL
AXIS
CORRESPONDS
TO
THE
VALUE
OF
THE
DENSITY
HERE
ΑK
ON
THE
LEFT
PLOT
ΑK
IN
THE
CENTRE
PLOT
AND
ΑK
IN
THE
RIGHT
PLOT
MODELLED
USING
THE
BINOMIAL
DISTRIBUTION
OR
AS
OF
VARIABLES
AND
MODELLED
USING
THE
MULTINOMIAL
DISTRIBUTION
WITH
K
THE
GAUSSIAN
DISTRIBUTION
THE
GAUSSIAN
ALSO
KNOWN
AS
THE
NORMAL
DISTRIBUTION
IS
A
WIDELY
USED
MODEL
FOR
THE
DISTRIBUTION
OF
CONTINUOUS
VARIABLES
IN
THE
CASE
OF
A
SINGLE
VARIABLE
X
THE
GAUSSIAN
DISTRIBUTION
CAN
BE
WRITTEN
IN
THE
FORM
N
X
Μ
Σ
EXP
X
Μ
WHERE
Μ
IS
THE
MEAN
AND
IS
THE
VARIANCE
FOR
A
D
DIMENSIONAL
VECTOR
X
THE
MULTIVARIATE
GAUSSIAN
DISTRIBUTION
TAKES
THE
FORM
N
X
Μ
Σ
EXP
X
Μ
TΣ
X
Μ
D
Σ
SECTION
EXERCISE
WHERE
Μ
IS
A
D
DIMENSIONAL
MEAN
VECTOR
Σ
IS
A
D
D
COVARIANCE
MATRIX
AND
Σ
DENOTES
THE
DETERMINANT
OF
Σ
THE
GAUSSIAN
DISTRIBUTION
ARISES
IN
MANY
DIFFERENT
CONTEXTS
AND
CAN
BE
MOTIVATED
FROM
A
VARIETY
OF
DIFFERENT
PERSPECTIVES
FOR
EXAMPLE
WE
HAVE
ALREADY
SEEN
THAT
FOR
A
SINGLE
REAL
VARIABLE
THE
DISTRIBUTION
THAT
MAXIMIZES
THE
ENTROPY
IS
THE
GAUSSIAN
THIS
PROPERTY
APPLIES
ALSO
TO
THE
MULTIVARIATE
GAUSSIAN
ANOTHER
SITUATION
IN
WHICH
THE
GAUSSIAN
DISTRIBUTION
ARISES
IS
WHEN
WE
CONSIDER
THE
SUM
OF
MULTIPLE
RANDOM
VARIABLES
THE
CENTRAL
LIMIT
THEOREM
DUE
TO
LAPLACE
TELLS
US
THAT
SUBJECT
TO
CERTAIN
MILD
CONDITIONS
THE
SUM
OF
A
SET
OF
RANDOM
VARIABLES
WHICH
IS
OF
COURSE
ITSELF
A
RANDOM
VARIABLE
HAS
A
DISTRIBUTION
THAT
BECOMES
INCREAS
INGLY
GAUSSIAN
AS
THE
NUMBER
OF
TERMS
IN
THE
SUM
INCREASES
WALKER
WE
CAN
FIGURE
HISTOGRAM
PLOTS
OF
THE
MEAN
OF
N
UNIFORMLY
DISTRIBUTED
NUMBERS
FOR
VARIOUS
VALUES
OF
N
WE
OBSERVE
THAT
AS
N
INCREASES
THE
DISTRIBUTION
TENDS
TOWARDS
A
GAUSSIAN
APPENDIX
C
ILLUSTRATE
THIS
BY
CONSIDERING
N
VARIABLES
XN
EACH
OF
WHICH
HAS
A
UNIFORM
DISTRIBUTION
OVER
THE
INTERVAL
AND
THEN
CONSIDERING
THE
DISTRIBUTION
OF
THE
MEAN
XN
N
FOR
LARGE
N
THIS
DISTRIBUTION
TENDS
TO
A
GAUSSIAN
AS
ILLUSTRATED
IN
FIGURE
IN
PRACTICE
THE
CONVERGENCE
TO
A
GAUSSIAN
AS
N
INCREASES
CAN
BE
VERY
RAPID
ONE
CONSEQUENCE
OF
THIS
RESULT
IS
THAT
THE
BINOMIAL
DISTRIBUTION
WHICH
IS
A
DISTRIBUTION
OVER
M
DEFINED
BY
THE
SUM
OF
N
OBSERVATIONS
OF
THE
RANDOM
BINARY
VARIABLE
X
WILL
TEND
TO
A
GAUSSIAN
AS
N
SEE
FIGURE
FOR
THE
CASE
OF
N
THE
GAUSSIAN
DISTRIBUTION
HAS
MANY
IMPORTANT
ANALYTICAL
PROPERTIES
AND
WE
SHALL
CONSIDER
SEVERAL
OF
THESE
IN
DETAIL
AS
A
RESULT
THIS
SECTION
WILL
BE
RATHER
MORE
TECH
NICALLY
INVOLVED
THAN
SOME
OF
THE
EARLIER
SECTIONS
AND
WILL
REQUIRE
FAMILIARITY
WITH
VARIOUS
MATRIX
IDENTITIES
HOWEVER
WE
STRONGLY
ENCOURAGE
THE
READER
TO
BECOME
PRO
FICIENT
IN
MANIPULATING
GAUSSIAN
DISTRIBUTIONS
USING
THE
TECHNIQUES
PRESENTED
HERE
AS
THIS
WILL
PROVE
INVALUABLE
IN
UNDERSTANDING
THE
MORE
COMPLEX
MODELS
PRESENTED
IN
LATER
CHAPTERS
WE
BEGIN
BY
CONSIDERING
THE
GEOMETRICAL
FORM
OF
THE
GAUSSIAN
DISTRIBUTION
THE
CARL
FRIEDRICH
GAUSS
IT
IS
SAID
THAT
WHEN
GAUSS
WENT
TO
ELEMENTARY
SCHOOL
AT
AGE
HIS
TEACHER
BU
TTNER
TRYING
TO
KEEP
THE
CLASS
OCCUPIED
ASKED
THE
PUPILS
TO
SUM
THE
INTEGERS
FROM
TO
TO
THE
TEACHER
AMAZEMENT
GAUSS
ARRIVED
AT
THE
ANSWER
IN
A
MATTER
OF
MOMENTS
BY
NOTING
THAT
THE
SUM
CAN
BE
REPRESENTED
AS
PAIRS
ETC
EACH
OF
WHICH
ADDED
TO
GIVING
THE
AN
SWER
IT
IS
NOW
BELIEVED
THAT
THE
PROBLEM
WHICH
WAS
ACTUALLY
SET
WAS
OF
THE
SAME
FORM
BUT
SOMEWHAT
HARDER
IN
THAT
THE
SEQUENCE
HAD
A
LARGER
STARTING
VALUE
AND
A
LARGER
INCREMENT
GAUSS
WAS
A
GERMAN
MATH
EMATICIAN
AND
SCIENTIST
WITH
A
REPUTATION
FOR
BEING
A
HARD
WORKING
PERFECTIONIST
ONE
OF
HIS
MANY
CONTRIBU
TIONS
WAS
TO
SHOW
THAT
LEAST
SQUARES
CAN
BE
DERIVED
UNDER
THE
ASSUMPTION
OF
NORMALLY
DISTRIBUTED
ERRORS
HE
ALSO
CREATED
AN
EARLY
FORMULATION
OF
NON
EUCLIDEAN
GEOMETRY
A
SELF
CONSISTENT
GEOMETRICAL
THEORY
THAT
VI
OLATES
THE
AXIOMS
OF
EUCLID
BUT
WAS
RELUCTANT
TO
DIS
CUSS
IT
OPENLY
FOR
FEAR
THAT
HIS
REPUTATION
MIGHT
SUFFER
IF
IT
WERE
SEEN
THAT
HE
BELIEVED
IN
SUCH
A
GEOMETRY
AT
ONE
POINT
GAUSS
WAS
ASKED
TO
CONDUCT
A
GEODETIC
SURVEY
OF
THE
STATE
OF
HANOVER
WHICH
LED
TO
HIS
FOR
MULATION
OF
THE
NORMAL
DISTRIBUTION
NOW
ALSO
KNOWN
AS
THE
GAUSSIAN
AFTER
HIS
DEATH
A
STUDY
OF
HIS
DI
ARIES
REVEALED
THAT
HE
HAD
DISCOVERED
SEVERAL
IMPOR
TANT
MATHEMATICAL
RESULTS
YEARS
OR
EVEN
DECADES
BE
FORE
THEY
WERE
PUBLISHED
BY
OTHERS
EXERCISE
EXERCISE
FUNCTIONAL
DEPENDENCE
OF
THE
GAUSSIAN
ON
X
IS
THROUGH
THE
QUADRATIC
FORM
X
Μ
TΣ
X
Μ
WHICH
APPEARS
IN
THE
EXPONENT
THE
QUANTITY
IS
CALLED
THE
MAHALANOBIS
DISTANCE
FROM
Μ
TO
X
AND
REDUCES
TO
THE
EUCLIDEAN
DISTANCE
WHEN
Σ
IS
THE
IDENTITY
MATRIX
THE
GAUSSIAN
DISTRIBUTION
WILL
BE
CONSTANT
ON
SURFACES
IN
X
SPACE
FOR
WHICH
THIS
QUADRATIC
FORM
IS
CONSTANT
FIRST
OF
ALL
WE
NOTE
THAT
THE
MATRIX
Σ
CAN
BE
TAKEN
TO
BE
SYMMETRIC
WITHOUT
LOSS
OF
GENERALITY
BECAUSE
ANY
ANTISYMMETRIC
COMPONENT
WOULD
DISAPPEAR
FROM
THE
EXPONENT
NOW
CONSIDER
THE
EIGENVECTOR
EQUATION
FOR
THE
COVARIANCE
MATRIX
ΣUI
ΛIUI
WHERE
I
D
BECAUSE
Σ
IS
A
REAL
SYMMETRIC
MATRIX
ITS
EIGENVALUES
WILL
BE
REAL
AND
ITS
EIGENVECTORS
CAN
BE
CHOSEN
TO
FORM
AN
ORTHONORMAL
SET
SO
THAT
UTUJ
IIJ
WHERE
IIJ
IS
THE
I
J
ELEMENT
OF
THE
IDENTITY
MATRIX
AND
SATISFIES
IF
I
J
OTHERWISE
EXERCISE
THE
COVARIANCE
MATRIX
Σ
CAN
BE
EXPRESSED
AS
AN
EXPANSION
IN
TERMS
OF
ITS
EIGENVEC
TORS
IN
THE
FORM
D
T
I
AND
SIMILARLY
THE
INVERSE
COVARIANCE
MATRIX
Σ
CAN
BE
EXPRESSED
AS
Σ
U
UT
SUBSTITUTING
INTO
THE
QUADRATIC
FORM
BECOMES
D
YI
ΛI
WHERE
WE
HAVE
DEFINED
I
YI
UT
X
Μ
WE
CAN
INTERPRET
YI
AS
A
NEW
COORDINATE
SYSTEM
DEFINED
BY
THE
ORTHONORMAL
VECTORS
UI
THAT
ARE
SHIFTED
AND
ROTATED
WITH
RESPECT
TO
THE
ORIGINAL
XI
COORDINATES
FORMING
THE
VECTOR
Y
YD
T
WE
HAVE
Y
U
X
Μ
FIGURE
THE
RED
CURVE
SHOWS
THE
ELLIP
TICAL
SURFACE
OF
CONSTANT
PROBA
BILITY
DENSITY
FOR
A
GAUSSIAN
IN
A
TWO
DIMENSIONAL
SPACE
X
ON
WHICH
THE
DENSITY
IS
EXP
OF
ITS
VALUE
AT
X
Μ
THE
MAJOR
AXES
OF
THE
ELLIPSE
ARE
DEFINED
BY
THE
EIGENVECTORS
UI
OF
THE
COVARI
ANCE
MATRIX
WITH
CORRESPOND
ING
EIGENVALUES
ΛI
APPENDIX
C
WHERE
U
IS
A
MATRIX
WHOSE
ROWS
ARE
GIVEN
BY
UT
FROM
IT
FOLLOWS
THAT
U
IS
AN
ORTHOGONAL
MATRIX
I
E
IT
SATISFIES
UUT
I
AND
HENCE
ALSO
UTU
I
WHERE
I
IS
THE
IDENTITY
MATRIX
THE
QUADRATIC
FORM
AND
HENCE
THE
GAUSSIAN
DENSITY
WILL
BE
CONSTANT
ON
SURFACES
FOR
WHICH
IS
CONSTANT
IF
ALL
OF
THE
EIGENVALUES
ΛI
ARE
POSITIVE
THEN
THESE
SURFACES
REPRESENT
ELLIPSOIDS
WITH
THEIR
CENTRES
AT
Μ
AND
THEIR
AXES
ORIENTED
ALONG
UI
AND
WITH
SCALING
FACTORS
IN
THE
DIRECTIONS
OF
THE
AXES
GIVEN
BY
AS
ILLUSTRATED
IN
FIGURE
FOR
THE
GAUSSIAN
DISTRIBUTION
TO
BE
WELL
DEFINED
IT
IS
NECESSARY
FOR
ALL
OF
THE
EIGENVALUES
ΛI
OF
THE
COVARIANCE
MATRIX
TO
BE
STRICTLY
POSITIVE
OTHERWISE
THE
DIS
TRIBUTION
CANNOT
BE
PROPERLY
NORMALIZED
A
MATRIX
WHOSE
EIGENVALUES
ARE
STRICTLY
POSITIVE
IS
SAID
TO
BE
POSITIVE
DEFINITE
IN
CHAPTER
WE
WILL
ENCOUNTER
GAUSSIAN
DISTRIBUTIONS
FOR
WHICH
ONE
OR
MORE
OF
THE
EIGENVALUES
ARE
ZERO
IN
WHICH
CASE
THE
DISTRIBUTION
IS
SINGULAR
AND
IS
CONFINED
TO
A
SUBSPACE
OF
LOWER
DIMENSIONALITY
IF
ALL
OF
THE
EIGENVALUES
ARE
NONNEGATIVE
THEN
THE
COVARIANCE
MATRIX
IS
SAID
TO
BE
POSITIVE
SEMIDEFINITE
NOW
CONSIDER
THE
FORM
OF
THE
GAUSSIAN
DISTRIBUTION
IN
THE
NEW
COORDINATE
SYSTEM
DEFINED
BY
THE
YI
IN
GOING
FROM
THE
X
TO
THE
Y
COORDINATE
SYSTEM
WE
HAVE
A
JACOBIAN
MATRIX
J
WITH
ELEMENTS
GIVEN
BY
JIJ
XI
U
YJ
JI
WHERE
UJI
ARE
THE
ELEMENTS
OF
THE
MATRIX
UT
USING
THE
ORTHONORMALITY
PROPERTY
OF
THE
MATRIX
U
WE
SEE
THAT
THE
SQUARE
OF
THE
DETERMINANT
OF
THE
JACOBIAN
MATRIX
IS
J
UT
UT
U
UTU
I
AND
HENCE
J
ALSO
THE
DETERMINANT
Σ
OF
THE
COVARIANCE
MATRIX
CAN
BE
WRITTEN
AS
THE
PRODUCT
OF
ITS
EIGENVALUES
AND
HENCE
D
J
J
THUS
IN
THE
YJ
COORDINATE
SYSTEM
THE
GAUSSIAN
DISTRIBUTION
TAKES
THE
FORM
P
Y
P
X
J
JTT
EXP
WHICH
IS
THE
PRODUCT
OF
D
INDEPENDENT
UNIVARIATE
GAUSSIAN
DISTRIBUTIONS
THE
EIGEN
VECTORS
THEREFORE
DEFINE
A
NEW
SET
OF
SHIFTED
AND
ROTATED
COORDINATES
WITH
RESPECT
TO
WHICH
THE
JOINT
PROBABILITY
DISTRIBUTION
FACTORIZES
INTO
A
PRODUCT
OF
INDEPENDENT
DISTRIBUTIONS
THE
INTEGRAL
OF
THE
DISTRIBUTION
IN
THE
Y
COORDINATE
SYSTEM
IS
THEN
R
P
Y
DY
TT
R
EXP
R
DYJ
J
WHERE
WE
HAVE
USED
THE
RESULT
FOR
THE
NORMALIZATION
OF
THE
UNIVARIATE
GAUSSIAN
THIS
CONFIRMS
THAT
THE
MULTIVARIATE
GAUSSIAN
IS
INDEED
NORMALIZED
WE
NOW
LOOK
AT
THE
MOMENTS
OF
THE
GAUSSIAN
DISTRIBUTION
AND
THEREBY
PROVIDE
AN
INTERPRETATION
OF
THE
PARAMETERS
Μ
AND
Σ
THE
EXPECTATION
OF
X
UNDER
THE
GAUSSIAN
DISTRIBUTION
IS
GIVEN
BY
E
X
R
EXP
R
X
Μ
TΣ
X
Μ
X
DX
R
EXP
R
ZTΣ
Z
Μ
DZ
WHERE
WE
HAVE
CHANGED
VARIABLES
USING
Z
X
Μ
WE
NOW
NOTE
THAT
THE
EXPONENT
IS
AN
EVEN
FUNCTION
OF
THE
COMPONENTS
OF
Z
AND
BECAUSE
THE
INTEGRALS
OVER
THESE
ARE
TAKEN
OVER
THE
RANGE
THE
TERM
IN
Z
IN
THE
FACTOR
Z
Μ
WILL
VANISH
BY
SYMMETRY
THUS
E
X
Μ
AND
SO
WE
REFER
TO
Μ
AS
THE
MEAN
OF
THE
GAUSSIAN
DISTRIBUTION
WE
NOW
CONSIDER
SECOND
ORDER
MOMENTS
OF
THE
GAUSSIAN
IN
THE
UNIVARIATE
CASE
WE
CONSIDERED
THE
SECOND
ORDER
MOMENT
GIVEN
BY
E
FOR
THE
MULTIVARIATE
GAUS
SIAN
THERE
ARE
SECOND
ORDER
MOMENTS
GIVEN
BY
E
XIXJ
WHICH
WE
CAN
GROUP
TOGETHER
TO
FORM
THE
MATRIX
E
XXT
THIS
MATRIX
CAN
BE
WRITTEN
AS
E
XXT
R
EXP
R
X
Μ
TΣ
X
Μ
XXT
DX
R
EXP
R
ZTΣ
Z
Μ
Z
Μ
T
DZ
WHERE
AGAIN
WE
HAVE
CHANGED
VARIABLES
USING
Z
X
Μ
NOTE
THAT
THE
CROSS
TERMS
INVOLVING
ΜZT
AND
ΜTZ
WILL
AGAIN
VANISH
BY
SYMMETRY
THE
TERM
ΜΜT
IS
CONSTANT
AND
CAN
BE
TAKEN
OUTSIDE
THE
INTEGRAL
WHICH
ITSELF
IS
UNITY
BECAUSE
THE
GAUSSIAN
DISTRIBUTION
IS
NORMALIZED
CONSIDER
THE
TERM
INVOLVING
ZZT
AGAIN
WE
CAN
MAKE
USE
OF
THE
EIGENVECTOR
EXPANSION
OF
THE
COVARIANCE
MATRIX
GIVEN
BY
TOGETHER
WITH
THE
COMPLETENESS
OF
THE
SET
OF
EIGENVECTORS
TO
WRITE
WHERE
YJ
UTZ
WHICH
GIVES
D
Z
YJUJ
J
R
EXP
R
ZTΣ
ZZT
DZ
D
Σ
R
Π
D
Σ
D
J
I
J
ΛK
K
UIUTΛI
Σ
I
WHERE
WE
HAVE
MADE
USE
OF
THE
EIGENVECTOR
EQUATION
TOGETHER
WITH
THE
FACT
THAT
THE
INTEGRAL
ON
THE
RIGHT
HAND
SIDE
OF
THE
MIDDLE
LINE
VANISHES
BY
SYMMETRY
UNLESS
I
J
AND
IN
THE
FINAL
LINE
WE
HAVE
MADE
USE
OF
THE
RESULTS
AND
TOGETHER
WITH
THUS
WE
HAVE
E
XXT
ΜΜT
Σ
FOR
SINGLE
RANDOM
VARIABLES
WE
SUBTRACTED
THE
MEAN
BEFORE
TAKING
SECOND
MO
MENTS
IN
ORDER
TO
DEFINE
A
VARIANCE
SIMILARLY
IN
THE
MULTIVARIATE
CASE
IT
IS
AGAIN
CONVENIENT
TO
SUBTRACT
OFF
THE
MEAN
GIVING
RISE
TO
THE
COVARIANCE
OF
A
RANDOM
VECTOR
X
DEFINED
BY
EXERCISE
COV
X
E
X
E
X
X
E
X
T
FOR
THE
SPECIFIC
CASE
OF
A
GAUSSIAN
DISTRIBUTION
WE
CAN
MAKE
USE
OF
E
X
Μ
TOGETHER
WITH
THE
RESULT
TO
GIVE
COV
X
Σ
BECAUSE
THE
PARAMETER
MATRIX
Σ
GOVERNS
THE
COVARIANCE
OF
X
UNDER
THE
GAUSSIAN
DISTRIBUTION
IT
IS
CALLED
THE
COVARIANCE
MATRIX
ALTHOUGH
THE
GAUSSIAN
DISTRIBUTION
IS
WIDELY
USED
AS
A
DENSITY
MODEL
IT
SUFFERS
FROM
SOME
SIGNIFICANT
LIMITATIONS
CONSIDER
THE
NUMBER
OF
FREE
PARAMETERS
IN
THE
DISTRIBUTION
A
GENERAL
SYMMETRIC
COVARIANCE
MATRIX
Σ
WILL
HAVE
D
D
INDEPENDENT
PARAMETERS
AND
THERE
ARE
ANOTHER
D
INDEPENDENT
PARAMETERS
IN
Μ
GIV
ING
D
D
PARAMETERS
IN
TOTAL
FOR
LARGE
D
THE
TOTAL
NUMBER
OF
PARAMETERS
FIGURE
CONTOURS
OF
CONSTANT
PROBABILITY
DENSITY
FOR
A
GAUSSIAN
DISTRIBUTION
IN
TWO
DIMENSIONS
IN
WHICH
THE
COVARIANCE
MATRIX
IS
A
OF
GENERAL
FORM
B
DIAGONAL
IN
WHICH
THE
ELLIPTICAL
CONTOURS
ARE
ALIGNED
WITH
THE
COORDINATE
AXES
AND
C
PROPORTIONAL
TO
THE
IDENTITY
MATRIX
IN
WHICH
THE
CONTOURS
ARE
CONCENTRIC
CIRCLES
A
B
C
SECTION
SECTION
THEREFORE
GROWS
QUADRATICALLY
WITH
D
AND
THE
COMPUTATIONAL
TASK
OF
MANIPULATING
AND
INVERTING
LARGE
MATRICES
CAN
BECOME
PROHIBITIVE
ONE
WAY
TO
ADDRESS
THIS
PROB
LEM
IS
TO
USE
RESTRICTED
FORMS
OF
THE
COVARIANCE
MATRIX
IF
WE
CONSIDER
COVARIANCE
MATRICES
THAT
ARE
DIAGONAL
SO
THAT
Σ
DIAG
WE
THEN
HAVE
A
TOTAL
OF
INDE
PENDENT
PARAMETERS
IN
THE
DENSITY
MODEL
THE
CORRESPONDING
CONTOURS
OF
CONSTANT
DENSITY
ARE
GIVEN
BY
AXIS
ALIGNED
ELLIPSOIDS
WE
COULD
FURTHER
RESTRICT
THE
COVARIANCE
MATRIX
TO
BE
PROPORTIONAL
TO
THE
IDENTITY
MATRIX
Σ
KNOWN
AS
AN
ISOTROPIC
CO
VARIANCE
GIVING
D
INDEPENDENT
PARAMETERS
IN
THE
MODEL
AND
SPHERICAL
SURFACES
OF
CONSTANT
DENSITY
THE
THREE
POSSIBILITIES
OF
GENERAL
DIAGONAL
AND
ISOTROPIC
COVARI
ANCE
MATRICES
ARE
ILLUSTRATED
IN
FIGURE
UNFORTUNATELY
WHEREAS
SUCH
APPROACHES
LIMIT
THE
NUMBER
OF
DEGREES
OF
FREEDOM
IN
THE
DISTRIBUTION
AND
MAKE
INVERSION
OF
THE
COVARIANCE
MATRIX
A
MUCH
FASTER
OPERATION
THEY
ALSO
GREATLY
RESTRICT
THE
FORM
OF
THE
PROBABILITY
DENSITY
AND
LIMIT
ITS
ABILITY
TO
CAPTURE
INTERESTING
CORRELATIONS
IN
THE
DATA
A
FURTHER
LIMITATION
OF
THE
GAUSSIAN
DISTRIBUTION
IS
THAT
IT
IS
INTRINSICALLY
UNI
MODAL
I
E
HAS
A
SINGLE
MAXIMUM
AND
SO
IS
UNABLE
TO
PROVIDE
A
GOOD
APPROXIMATION
TO
MULTIMODAL
DISTRIBUTIONS
THUS
THE
GAUSSIAN
DISTRIBUTION
CAN
BE
BOTH
TOO
FLEXIBLE
IN
THE
SENSE
OF
HAVING
TOO
MANY
PARAMETERS
WHILE
ALSO
BEING
TOO
LIMITED
IN
THE
RANGE
OF
DISTRIBUTIONS
THAT
IT
CAN
ADEQUATELY
REPRESENT
WE
WILL
SEE
LATER
THAT
THE
INTRODUC
TION
OF
LATENT
VARIABLES
ALSO
CALLED
HIDDEN
VARIABLES
OR
UNOBSERVED
VARIABLES
ALLOWS
BOTH
OF
THESE
PROBLEMS
TO
BE
ADDRESSED
IN
PARTICULAR
A
RICH
FAMILY
OF
MULTIMODAL
DISTRIBUTIONS
IS
OBTAINED
BY
INTRODUCING
DISCRETE
LATENT
VARIABLES
LEADING
TO
MIXTURES
OF
GAUSSIANS
AS
DISCUSSED
IN
SECTION
SIMILARLY
THE
INTRODUCTION
OF
CONTINUOUS
LATENT
VARIABLES
AS
DESCRIBED
IN
CHAPTER
LEADS
TO
MODELS
IN
WHICH
THE
NUMBER
OF
FREE
PARAMETERS
CAN
BE
CONTROLLED
INDEPENDENTLY
OF
THE
DIMENSIONALITY
D
OF
THE
DATA
SPACE
WHILE
STILL
ALLOWING
THE
MODEL
TO
CAPTURE
THE
DOMINANT
CORRELATIONS
IN
THE
DATA
SET
INDEED
THESE
TWO
APPROACHES
CAN
BE
COMBINED
AND
FURTHER
EXTENDED
TO
DERIVE
A
VERY
RICH
SET
OF
HIERARCHICAL
MODELS
THAT
CAN
BE
ADAPTED
TO
A
BROAD
RANGE
OF
PRAC
TICAL
APPLICATIONS
FOR
INSTANCE
THE
GAUSSIAN
VERSION
OF
THE
MARKOV
RANDOM
FIELD
WHICH
IS
WIDELY
USED
AS
A
PROBABILISTIC
MODEL
OF
IMAGES
IS
A
GAUSSIAN
DISTRIBUTION
OVER
THE
JOINT
SPACE
OF
PIXEL
INTENSITIES
BUT
RENDERED
TRACTABLE
THROUGH
THE
IMPOSITION
OF
CONSIDERABLE
STRUCTURE
REFLECTING
THE
SPATIAL
ORGANIZATION
OF
THE
PIXELS
SIMILARLY
THE
LINEAR
DYNAMICAL
SYSTEM
USED
TO
MODEL
TIME
SERIES
DATA
FOR
APPLICATIONS
SUCH
AS
TRACKING
IS
ALSO
A
JOINT
GAUSSIAN
DISTRIBUTION
OVER
A
POTENTIALLY
LARGE
NUMBER
OF
OBSERVED
AND
LATENT
VARIABLES
AND
AGAIN
IS
TRACTABLE
DUE
TO
THE
STRUCTURE
IMPOSED
ON
THE
DISTRIBUTION
A
POWERFUL
FRAMEWORK
FOR
EXPRESSING
THE
FORM
AND
PROPERTIES
OF
SUCH
COMPLEX
DISTRIBUTIONS
IS
THAT
OF
PROBABILISTIC
GRAPHICAL
MODELS
WHICH
WILL
FORM
THE
SUBJECT
OF
CHAPTER
CONDITIONAL
GAUSSIAN
DISTRIBUTIONS
AN
IMPORTANT
PROPERTY
OF
THE
MULTIVARIATE
GAUSSIAN
DISTRIBUTION
IS
THAT
IF
TWO
SETS
OF
VARIABLES
ARE
JOINTLY
GAUSSIAN
THEN
THE
CONDITIONAL
DISTRIBUTION
OF
ONE
SET
CONDITIONED
ON
THE
OTHER
IS
AGAIN
GAUSSIAN
SIMILARLY
THE
MARGINAL
DISTRIBUTION
OF
EITHER
SET
IS
ALSO
GAUSSIAN
CONSIDER
FIRST
THE
CASE
OF
CONDITIONAL
DISTRIBUTIONS
SUPPOSE
X
IS
A
D
DIMENSIONAL
VECTOR
WITH
GAUSSIAN
DISTRIBUTION
X
Μ
Σ
AND
THAT
WE
PARTITION
X
INTO
TWO
DIS
JOINT
SUBSETS
XA
AND
XB
WITHOUT
LOSS
OF
GENERALITY
WE
CAN
TAKE
XA
TO
FORM
THE
FIRST
M
COMPONENTS
OF
X
WITH
XB
COMPRISING
THE
REMAINING
D
M
COMPONENTS
SO
THAT
X
XA
WE
ALSO
DEFINE
CORRESPONDING
PARTITIONS
OF
THE
MEAN
VECTOR
Μ
GIVEN
BY
Μ
ΜA
ΜB
AND
OF
THE
COVARIANCE
MATRIX
Σ
GIVEN
BY
Σ
ΣAA
ΣAB
NOTE
THAT
THE
SYMMETRY
ΣT
Σ
OF
THE
COVARIANCE
MATRIX
IMPLIES
THAT
ΣAA
AND
ΣBB
ARE
SYMMETRIC
WHILE
ΣBA
ΣT
IN
MANY
SITUATIONS
IT
WILL
BE
CONVENIENT
TO
WORK
WITH
THE
INVERSE
OF
THE
COVARI
ANCE
MATRIX
EXERCISE
Λ
Σ
WHICH
IS
KNOWN
AS
THE
PRECISION
MATRIX
IN
FACT
WE
SHALL
SEE
THAT
SOME
PROPERTIES
OF
GAUSSIAN
DISTRIBUTIONS
ARE
MOST
NATURALLY
EXPRESSED
IN
TERMS
OF
THE
COVARIANCE
WHEREAS
OTHERS
TAKE
A
SIMPLER
FORM
WHEN
VIEWED
IN
TERMS
OF
THE
PRECISION
WE
THEREFORE
ALSO
INTRODUCE
THE
PARTITIONED
FORM
OF
THE
PRECISION
MATRIX
Λ
ΛAA
ΛAB
CORRESPONDING
TO
THE
PARTITIONING
OF
THE
VECTOR
X
BECAUSE
THE
INVERSE
OF
A
SYMMETRIC
MATRIX
IS
ALSO
SYMMETRIC
WE
SEE
THAT
ΛAA
AND
ΛBB
ARE
SYMMETRIC
WHILE
T
ΛBA
IT
SHOULD
BE
STRESSED
AT
THIS
POINT
THAT
FOR
INSTANCE
ΛAA
IS
NOT
SIMPLY
GIVEN
BY
THE
INVERSE
OF
ΣAA
IN
FACT
WE
SHALL
SHORTLY
EXAMINE
THE
RELATION
BETWEEN
THE
INVERSE
OF
A
PARTITIONED
MATRIX
AND
THE
INVERSES
OF
ITS
PARTITIONS
LET
US
BEGIN
BY
FINDING
AN
EXPRESSION
FOR
THE
CONDITIONAL
DISTRIBUTION
P
XA
XB
FROM
THE
PRODUCT
RULE
OF
PROBABILITY
WE
SEE
THAT
THIS
CONDITIONAL
DISTRIBUTION
CAN
BE
EVALUATED
FROM
THE
JOINT
DISTRIBUTION
P
X
P
XA
XB
SIMPLY
BY
FIXING
XB
TO
THE
OBSERVED
VALUE
AND
NORMALIZING
THE
RESULTING
EXPRESSION
TO
OBTAIN
A
VALID
PROBABILITY
DISTRIBUTION
OVER
XA
INSTEAD
OF
PERFORMING
THIS
NORMALIZATION
EXPLICITLY
WE
CAN
OBTAIN
THE
SOLUTION
MORE
EFFICIENTLY
BY
CONSIDERING
THE
QUADRATIC
FORM
IN
THE
EXPONENT
OF
THE
GAUSSIAN
DISTRIBUTION
GIVEN
BY
AND
THEN
REINSTATING
THE
NORMALIZATION
COEFFICIENT
AT
THE
END
OF
THE
CALCULATION
IF
WE
MAKE
USE
OF
THE
PARTITIONING
AND
WE
OBTAIN
X
Μ
TΣ
X
Μ
XA
ΜA
TΛ
X
Μ
X
Μ
TΛAB
XB
ΜB
XB
ΜB
TΛ
X
Μ
X
Μ
TΛBB
XB
ΜB
WE
SEE
THAT
AS
A
FUNCTION
OF
XA
THIS
IS
AGAIN
A
QUADRATIC
FORM
AND
HENCE
THE
COR
RESPONDING
CONDITIONAL
DISTRIBUTION
P
XA
XB
WILL
BE
GAUSSIAN
BECAUSE
THIS
DISTRI
BUTION
IS
COMPLETELY
CHARACTERIZED
BY
ITS
MEAN
AND
ITS
COVARIANCE
OUR
GOAL
WILL
BE
TO
IDENTIFY
EXPRESSIONS
FOR
THE
MEAN
AND
COVARIANCE
OF
P
XA
XB
BY
INSPECTION
OF
THIS
IS
AN
EXAMPLE
OF
A
RATHER
COMMON
OPERATION
ASSOCIATED
WITH
GAUSSIAN
DISTRIBUTIONS
SOMETIMES
CALLED
COMPLETING
THE
SQUARE
IN
WHICH
WE
ARE
GIVEN
A
QUADRATIC
FORM
DEFINING
THE
EXPONENT
TERMS
IN
A
GAUSSIAN
DISTRIBUTION
AND
WE
NEED
TO
DETERMINE
THE
CORRESPONDING
MEAN
AND
COVARIANCE
SUCH
PROBLEMS
CAN
BE
SOLVED
STRAIGHTFORWARDLY
BY
NOTING
THAT
THE
EXPONENT
IN
A
GENERAL
GAUSSIAN
DISTRIBUTION
N
X
Μ
Σ
CAN
BE
WRITTEN
X
Μ
TΣ
X
Μ
XTΣ
XTΣ
CONST
WHERE
CONST
DENOTES
TERMS
WHICH
ARE
INDEPENDENT
OF
X
AND
WE
HAVE
MADE
USE
OF
THE
SYMMETRY
OF
Σ
THUS
IF
WE
TAKE
OUR
GENERAL
QUADRATIC
FORM
AND
EXPRESS
IT
IN
THE
FORM
GIVEN
BY
THE
RIGHT
HAND
SIDE
OF
THEN
WE
CAN
IMMEDIATELY
EQUATE
THE
MATRIX
OF
COEFFICIENTS
ENTERING
THE
SECOND
ORDER
TERM
IN
X
TO
THE
INVERSE
COVARIANCE
MATRIX
Σ
AND
THE
COEFFICIENT
OF
THE
LINEAR
TERM
IN
X
TO
Σ
FROM
WHICH
WE
CAN
OBTAIN
Μ
NOW
LET
US
APPLY
THIS
PROCEDURE
TO
THE
CONDITIONAL
GAUSSIAN
DISTRIBUTION
P
XA
XB
FOR
WHICH
THE
QUADRATIC
FORM
IN
THE
EXPONENT
IS
GIVEN
BY
WE
WILL
DENOTE
THE
MEAN
AND
COVARIANCE
OF
THIS
DISTRIBUTION
BY
ΜA
B
AND
ΣA
B
RESPECTIVELY
CONSIDER
THE
FUNCTIONAL
DEPENDENCE
OF
ON
XA
IN
WHICH
XB
IS
REGARDED
AS
A
CONSTANT
IF
WE
PICK
OUT
ALL
TERMS
THAT
ARE
SECOND
ORDER
IN
XA
WE
HAVE
T
XA
ΛAAXA
FROM
WHICH
WE
CAN
IMMEDIATELY
CONCLUDE
THAT
THE
COVARIANCE
INVERSE
PRECISION
OF
P
XA
XB
IS
GIVEN
BY
Σ
A
B
Λ
NOW
CONSIDER
ALL
OF
THE
TERMS
IN
THAT
ARE
LINEAR
IN
XA
XT
ΛAAΜA
ΛAB
XB
ΜB
WHERE
WE
HAVE
USED
ΛT
ΛAB
FROM
OUR
DISCUSSION
OF
THE
GENERAL
FORM
THE
COEFFICIENT
OF
XA
IN
THIS
EXPRESSION
MUST
EQUAL
Σ
B
AND
HENCE
ΜA
B
ΣA
B
ΛAAΜA
ΛAB
XB
ΜB
ΜA
Λ
XB
ΜB
EXERCISE
WHERE
WE
HAVE
MADE
USE
OF
THE
RESULTS
AND
ARE
EXPRESSED
IN
TERMS
OF
THE
PARTITIONED
PRECISION
MATRIX
OF
THE
ORIGINAL
JOINT
DISTRIBUTION
P
XA
XB
WE
CAN
ALSO
EXPRESS
THESE
RESULTS
IN
TERMS
OF
THE
CORRESPONDING
PARTITIONED
COVARIANCE
MATRIX
TO
DO
THIS
WE
MAKE
USE
OF
THE
FOLLOWING
IDENTITY
FOR
THE
INVERSE
OF
A
PARTITIONED
MATRIX
A
B
C
D
WHERE
WE
HAVE
DEFINED
M
MBD
D
D
D
M
A
BD
THE
QUANTITY
M
IS
KNOWN
AS
THE
SCHUR
COMPLEMENT
OF
THE
MATRIX
ON
THE
LEFT
HAND
SIDE
OF
WITH
RESPECT
TO
THE
SUBMATRIX
D
USING
THE
DEFINITION
ΣAA
ΣAB
ΛAA
ΛAB
ΣBA
ΣBB
AND
MAKING
USE
OF
WE
HAVE
ΛBA
ΛBB
ΛAA
ΣAA
ΣABΣ
ΛAB
ΣAA
ΣABΣ
SECTION
FROM
THESE
WE
OBTAIN
THE
FOLLOWING
EXPRESSIONS
FOR
THE
MEAN
AND
COVARIANCE
OF
THE
CONDITIONAL
DISTRIBUTION
P
XA
XB
ΜA
B
ΜA
ΣABΣ
XB
ΜB
ΣA
B
ΣAA
ΣABΣ
COMPARING
AND
WE
SEE
THAT
THE
CONDITIONAL
DISTRIBUTION
P
XA
XB
TAKES
A
SIMPLER
FORM
WHEN
EXPRESSED
IN
TERMS
OF
THE
PARTITIONED
PRECISION
MATRIX
THAN
WHEN
IT
IS
EXPRESSED
IN
TERMS
OF
THE
PARTITIONED
COVARIANCE
MATRIX
NOTE
THAT
THE
MEAN
OF
THE
CONDITIONAL
DISTRIBUTION
P
XA
XB
GIVEN
BY
IS
A
LINEAR
FUNCTION
OF
XB
AND
THAT
THE
COVARIANCE
GIVEN
BY
IS
INDEPENDENT
OF
XA
THIS
REPRESENTS
AN
EXAMPLE
OF
A
LINEAR
GAUSSIAN
MODEL
MARGINAL
GAUSSIAN
DISTRIBUTIONS
WE
HAVE
SEEN
THAT
IF
A
JOINT
DISTRIBUTION
P
XA
XB
IS
GAUSSIAN
THEN
THE
CONDI
TIONAL
DISTRIBUTION
P
XA
XB
WILL
AGAIN
BE
GAUSSIAN
NOW
WE
TURN
TO
A
DISCUSSION
OF
THE
MARGINAL
DISTRIBUTION
GIVEN
BY
P
XA
R
P
XA
XB
DXB
WHICH
AS
WE
SHALL
SEE
IS
ALSO
GAUSSIAN
ONCE
AGAIN
OUR
STRATEGY
FOR
EVALUATING
THIS
DISTRIBUTION
EFFICIENTLY
WILL
BE
TO
FOCUS
ON
THE
QUADRATIC
FORM
IN
THE
EXPONENT
OF
THE
JOINT
DISTRIBUTION
AND
THEREBY
TO
IDENTIFY
THE
MEAN
AND
COVARIANCE
OF
THE
MARGINAL
DISTRIBUTION
P
XA
THE
QUADRATIC
FORM
FOR
THE
JOINT
DISTRIBUTION
CAN
BE
EXPRESSED
USING
THE
PAR
TITIONED
PRECISION
MATRIX
IN
THE
FORM
BECAUSE
OUR
GOAL
IS
TO
INTEGRATE
OUT
XB
THIS
IS
MOST
EASILY
ACHIEVED
BY
FIRST
CONSIDERING
THE
TERMS
INVOLVING
XB
AND
THEN
COMPLETING
THE
SQUARE
IN
ORDER
TO
FACILITATE
INTEGRATION
PICKING
OUT
JUST
THOSE
TERMS
THAT
INVOLVE
XB
WE
HAVE
X
Λ
X
XT
M
X
Λ
TΛ
X
Λ
MTΛ
WHERE
WE
HAVE
DEFINED
M
ΛBBΜB
ΛBA
XA
ΜA
WE
SEE
THAT
THE
DEPENDENCE
ON
XB
HAS
BEEN
CAST
INTO
THE
STANDARD
QUADRATIC
FORM
OF
A
GAUSSIAN
DISTRIBUTION
CORRESPONDING
TO
THE
FIRST
TERM
ON
THE
RIGHT
HAND
SIDE
OF
PLUS
A
TERM
THAT
DOES
NOT
DEPEND
ON
XB
BUT
THAT
DOES
DEPEND
ON
XA
THUS
WHEN
WE
TAKE
THE
EXPONENTIAL
OF
THIS
QUADRATIC
FORM
WE
SEE
THAT
THE
INTEGRATION
OVER
XB
REQUIRED
BY
WILL
TAKE
THE
FORM
R
EXP
R
X
Λ
TΛ
X
Λ
DX
THIS
INTEGRATION
IS
EASILY
PERFORMED
BY
NOTING
THAT
IT
IS
THE
INTEGRAL
OVER
AN
UNNOR
MALIZED
GAUSSIAN
AND
SO
THE
RESULT
WILL
BE
THE
RECIPROCAL
OF
THE
NORMALIZATION
CO
EFFICIENT
WE
KNOW
FROM
THE
FORM
OF
THE
NORMALIZED
GAUSSIAN
GIVEN
BY
THAT
THIS
COEFFICIENT
IS
INDEPENDENT
OF
THE
MEAN
AND
DEPENDS
ONLY
ON
THE
DETERMINANT
OF
THE
COVARIANCE
MATRIX
THUS
BY
COMPLETING
THE
SQUARE
WITH
RESPECT
TO
XB
WE
CAN
INTEGRATE
OUT
XB
AND
THE
ONLY
TERM
REMAINING
FROM
THE
CONTRIBUTIONS
ON
THE
LEFT
HAND
SIDE
OF
THAT
DEPENDS
ON
XA
IS
THE
LAST
TERM
ON
THE
RIGHT
HAND
SIDE
OF
IN
WHICH
M
IS
GIVEN
BY
COMBINING
THIS
TERM
WITH
THE
REMAINING
TERMS
FROM
THAT
DEPEND
ON
XA
WE
OBTAIN
Λ
Μ
BB
B
ΛBA
XA
ΜA
T
Λ
ΛBBΜ
ΛBA
XA
ΜA
T
T
XA
ΛAAXA
XA
ΛAAΜA
ΛABΜB
CONST
XT
Λ
Λ
Λ
X
XT
ΛAA
ΛABΛ
CONST
WHERE
CONST
DENOTES
QUANTITIES
INDEPENDENT
OF
XA
AGAIN
BY
COMPARISON
WITH
WE
SEE
THAT
THE
COVARIANCE
OF
THE
MARGINAL
DISTRIBUTION
OF
P
XA
IS
GIVEN
BY
ΣA
ΛAA
ΛABΛ
SIMILARLY
THE
MEAN
IS
GIVEN
BY
ΣA
ΛAA
ΛABΛ
ΜA
ΜA
WHERE
WE
HAVE
USED
THE
COVARIANCE
IN
IS
EXPRESSED
IN
TERMS
OF
THE
PARTITIONED
PRECISION
MATRIX
GIVEN
BY
WE
CAN
REWRITE
THIS
IN
TERMS
OF
THE
CORRESPONDING
PARTITIONING
OF
THE
COVARIANCE
MATRIX
GIVEN
BY
AS
WE
DID
FOR
THE
CONDITIONAL
DISTRIBUTION
THESE
PARTITIONED
MATRICES
ARE
RELATED
BY
ΛAA
ΛAB
ΣAA
ΣAB
ΛBA
ΛBB
MAKING
USE
OF
WE
THEN
HAVE
ΣBA
ΣBB
ΛAA
ΛAB
Λ
Σ
THUS
WE
OBTAIN
THE
INTUITIVELY
SATISFYING
RESULT
THAT
THE
MARGINAL
DISTRIBUTION
P
XA
HAS
MEAN
AND
COVARIANCE
GIVEN
BY
E
XA
ΜA
COV
XA
ΣAA
WE
SEE
THAT
FOR
A
MARGINAL
DISTRIBUTION
THE
MEAN
AND
COVARIANCE
ARE
MOST
SIMPLY
EX
PRESSED
IN
TERMS
OF
THE
PARTITIONED
COVARIANCE
MATRIX
IN
CONTRAST
TO
THE
CONDITIONAL
DISTRIBUTION
FOR
WHICH
THE
PARTITIONED
PRECISION
MATRIX
GIVES
RISE
TO
SIMPLER
EXPRES
SIONS
OUR
RESULTS
FOR
THE
MARGINAL
AND
CONDITIONAL
DISTRIBUTIONS
OF
A
PARTITIONED
GAUS
SIAN
ARE
SUMMARIZED
BELOW
PARTITIONED
GAUSSIANS
GIVEN
A
JOINT
GAUSSIAN
DISTRIBUTION
N
X
Μ
Σ
WITH
Λ
Σ
AND
X
XA
Μ
ΜA
XB
XA
XA
FIGURE
THE
PLOT
ON
THE
LEFT
SHOWS
THE
CONTOURS
OF
A
GAUSSIAN
DISTRIBUTION
P
XA
XB
OVER
TWO
VARIABLES
AND
THE
PLOT
ON
THE
RIGHT
SHOWS
THE
MARGINAL
DISTRIBUTION
P
XA
BLUE
CURVE
AND
THE
CONDITIONAL
DISTRIBUTION
P
XA
XB
FOR
XB
RED
CURVE
ΣBA
ΣBB
CONDITIONAL
DISTRIBUTION
ΛBA
ΛBB
P
XA
XB
N
X
ΜA
B
Λ
ΜA
B
ΜA
Λ
XB
ΜB
MARGINAL
DISTRIBUTION
P
XA
N
XA
ΜA
ΣAA
WE
ILLUSTRATE
THE
IDEA
OF
CONDITIONAL
AND
MARGINAL
DISTRIBUTIONS
ASSOCIATED
WITH
A
MULTIVARIATE
GAUSSIAN
USING
AN
EXAMPLE
INVOLVING
TWO
VARIABLES
IN
FIGURE
BAYES
THEOREM
FOR
GAUSSIAN
VARIABLES
IN
SECTIONS
AND
WE
CONSIDERED
A
GAUSSIAN
P
X
IN
WHICH
WE
PARTI
TIONED
THE
VECTOR
X
INTO
TWO
SUBVECTORS
X
XA
XB
AND
THEN
FOUND
EXPRESSIONS
FOR
THE
CONDITIONAL
DISTRIBUTION
P
XA
XB
AND
THE
MARGINAL
DISTRIBUTION
P
XA
WE
NOTED
THAT
THE
MEAN
OF
THE
CONDITIONAL
DISTRIBUTION
P
XA
XB
WAS
A
LINEAR
FUNCTION
OF
XB
HERE
WE
SHALL
SUPPOSE
THAT
WE
ARE
GIVEN
A
GAUSSIAN
MARGINAL
DISTRIBUTION
P
X
AND
A
GAUSSIAN
CONDITIONAL
DISTRIBUTION
P
Y
X
IN
WHICH
P
Y
X
HAS
A
MEAN
THAT
IS
A
LINEAR
FUNCTION
OF
X
AND
A
COVARIANCE
WHICH
IS
INDEPENDENT
OF
X
THIS
IS
AN
EXAMPLE
OF
A
LINEAR
GAUSSIAN
MODEL
ROWEIS
AND
GHAHRAMANI
WHICH
WE
SHALL
STUDY
IN
GREATER
GENERALITY
IN
SECTION
WE
WISH
TO
FIND
THE
MARGINAL
DISTRIBUTION
P
Y
AND
THE
CONDITIONAL
DISTRIBUTION
P
X
Y
THIS
IS
A
PROBLEM
THAT
WILL
ARISE
FREQUENTLY
IN
SUBSEQUENT
CHAPTERS
AND
IT
WILL
PROVE
CONVENIENT
TO
DERIVE
THE
GENERAL
RESULTS
HERE
WE
SHALL
TAKE
THE
MARGINAL
AND
CONDITIONAL
DISTRIBUTIONS
TO
BE
P
X
N
X
Μ
Λ
WHERE
Μ
A
AND
B
ARE
PARAMETERS
GOVERNING
THE
MEANS
AND
Λ
AND
L
ARE
PRECISION
MATRICES
IF
X
HAS
DIMENSIONALITY
M
AND
Y
HAS
DIMENSIONALITY
D
THEN
THE
MATRIX
A
HAS
SIZE
D
M
FIRST
WE
FIND
AN
EXPRESSION
FOR
THE
JOINT
DISTRIBUTION
OVER
X
AND
Y
TO
DO
THIS
WE
DEFINE
Z
X
AND
THEN
CONSIDER
THE
LOG
OF
THE
JOINT
DISTRIBUTION
LN
P
Z
LN
P
X
LN
P
Y
X
X
Μ
TΛ
X
Μ
Y
AX
B
T
L
Y
AX
B
CONST
WHERE
CONST
DENOTES
TERMS
INDEPENDENT
OF
X
AND
Y
AS
BEFORE
WE
SEE
THAT
THIS
IS
A
QUADRATIC
FUNCTION
OF
THE
COMPONENTS
OF
Z
AND
HENCE
P
Z
IS
GAUSSIAN
DISTRIBUTION
TO
FIND
THE
PRECISION
OF
THIS
GAUSSIAN
WE
CONSIDER
THE
SECOND
ORDER
TERMS
IN
WHICH
CAN
BE
WRITTEN
AS
XT
Λ
AT
LA
X
YT
LY
YT
LAX
XT
ATLY
X
T
Λ
ATLA
ATL
X
AND
SO
THE
GAUSSIAN
DISTRIBUTION
OVER
Z
HAS
PRECISION
INVERSE
COVARIANCE
MATRIX
GIVEN
BY
Λ
ATLA
ATL
LA
L
EXERCISE
THE
COVARIANCE
MATRIX
IS
FOUND
BY
TAKING
THE
INVERSE
OF
THE
PRECISION
WHICH
CAN
BE
DONE
USING
THE
MATRIX
INVERSION
FORMULA
TO
GIVE
Λ
Λ
AΛ
L
AΛ
SIMILARLY
WE
CAN
FIND
THE
MEAN
OF
THE
GAUSSIAN
DISTRIBUTION
OVER
Z
BY
IDENTIFY
ING
THE
LINEAR
TERMS
IN
WHICH
ARE
GIVEN
BY
X
T
ΛΜ
ATLB
USING
OUR
EARLIER
RESULT
OBTAINED
BY
COMPLETING
THE
SQUARE
OVER
THE
QUADRATIC
FORM
OF
A
MULTIVARIATE
GAUSSIAN
WE
FIND
THAT
THE
MEAN
OF
Z
IS
GIVEN
BY
E
Z
R
ΛΜ
ATLB
EXERCISE
MAKING
USE
OF
WE
THEN
OBTAIN
E
Z
A
Μ
B
SECTION
NEXT
WE
FIND
AN
EXPRESSION
FOR
THE
MARGINAL
DISTRIBUTION
P
Y
IN
WHICH
WE
HAVE
MARGINALIZED
OVER
X
RECALL
THAT
THE
MARGINAL
DISTRIBUTION
OVER
A
SUBSET
OF
THE
COM
PONENTS
OF
A
GAUSSIAN
RANDOM
VECTOR
TAKES
A
PARTICULARLY
SIMPLE
FORM
WHEN
EX
PRESSED
IN
TERMS
OF
THE
PARTITIONED
COVARIANCE
MATRIX
SPECIFICALLY
ITS
MEAN
AND
COVARIANCE
ARE
GIVEN
BY
AND
RESPECTIVELY
MAKING
USE
OF
AND
WE
SEE
THAT
THE
MEAN
AND
COVARIANCE
OF
THE
MARGINAL
DISTRIBUTION
P
Y
ARE
GIVEN
BY
SECTION
E
Y
AΜ
B
COV
Y
L
AΛ
A
SPECIAL
CASE
OF
THIS
RESULT
IS
WHEN
A
I
IN
WHICH
CASE
IT
REDUCES
TO
THE
CONVOLU
TION
OF
TWO
GAUSSIANS
FOR
WHICH
WE
SEE
THAT
THE
MEAN
OF
THE
CONVOLUTION
IS
THE
SUM
OF
THE
MEAN
OF
THE
TWO
GAUSSIANS
AND
THE
COVARIANCE
OF
THE
CONVOLUTION
IS
THE
SUM
OF
THEIR
COVARIANCES
FINALLY
WE
SEEK
AN
EXPRESSION
FOR
THE
CONDITIONAL
P
X
Y
RECALL
THAT
THE
RESULTS
FOR
THE
CONDITIONAL
DISTRIBUTION
ARE
MOST
EASILY
EXPRESSED
IN
TERMS
OF
THE
PARTITIONED
PRECISION
MATRIX
USING
AND
APPLYING
THESE
RESULTS
TO
AND
WE
SEE
THAT
THE
CONDITIONAL
DISTRIBUTION
P
X
Y
HAS
MEAN
AND
COVARIANCE
GIVEN
BY
E
X
Y
Λ
ATLA
ATL
Y
B
ΛΜ
COV
X
Y
Λ
ATLA
THE
EVALUATION
OF
THIS
CONDITIONAL
CAN
BE
SEEN
AS
AN
EXAMPLE
OF
BAYES
THEOREM
WE
CAN
INTERPRET
THE
DISTRIBUTION
P
X
AS
A
PRIOR
DISTRIBUTION
OVER
X
IF
THE
VARIABLE
Y
IS
OBSERVED
THEN
THE
CONDITIONAL
DISTRIBUTION
P
X
Y
REPRESENTS
THE
CORRESPONDING
POSTERIOR
DISTRIBUTION
OVER
X
HAVING
FOUND
THE
MARGINAL
AND
CONDITIONAL
DISTRIBU
TIONS
WE
EFFECTIVELY
EXPRESSED
THE
JOINT
DISTRIBUTION
P
Z
P
X
P
Y
X
IN
THE
FORM
P
X
Y
P
Y
THESE
RESULTS
ARE
SUMMARIZED
BELOW
MARGINAL
AND
CONDITIONAL
GAUSSIANS
GIVEN
A
MARGINAL
GAUSSIAN
DISTRIBUTION
FOR
X
AND
A
CONDITIONAL
GAUSSIAN
DISTRI
BUTION
FOR
Y
GIVEN
X
IN
THE
FORM
P
X
N
X
Μ
Λ
P
Y
X
N
Y
AX
B
L
THE
MARGINAL
DISTRIBUTION
OF
Y
AND
THE
CONDITIONAL
DISTRIBUTION
OF
X
GIVEN
Y
ARE
GIVEN
BY
WHERE
P
Y
N
Y
AΜ
B
L
AΛ
P
X
Y
N
X
Σ
ATL
Y
B
ΛΜ
Σ
Σ
Λ
ATLA
MAXIMUM
LIKELIHOOD
FOR
THE
GAUSSIAN
GIVEN
A
DATA
SET
X
XN
T
IN
WHICH
THE
OBSERVATIONS
XN
ARE
AS
SUMED
TO
BE
DRAWN
INDEPENDENTLY
FROM
A
MULTIVARIATE
GAUSSIAN
DISTRIBUTION
WE
CAN
ESTIMATE
THE
PARAMETERS
OF
THE
DISTRIBUTION
BY
MAXIMUM
LIKELIHOOD
THE
LOG
LIKELI
HOOD
FUNCTION
IS
GIVEN
BY
LN
P
X
Μ
Σ
ND
LN
N
LN
Σ
X
Μ
TΣ
X
Μ
BY
SIMPLE
REARRANGEMENT
WE
SEE
THAT
THE
LIKELIHOOD
FUNCTION
DEPENDS
ON
THE
DATA
SET
ONLY
THROUGH
THE
TWO
QUANTITIES
XN
T
N
N
APPENDIX
C
THESE
ARE
KNOWN
AS
THE
SUFFICIENT
STATISTICS
FOR
THE
GAUSSIAN
DISTRIBUTION
USING
C
THE
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
WITH
RESPECT
TO
Μ
IS
GIVEN
BY
N
LN
P
X
Μ
Σ
Σ
X
Μ
N
N
Μ
AND
SETTING
THIS
DERIVATIVE
TO
ZERO
WE
OBTAIN
THE
SOLUTION
FOR
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
OF
THE
MEAN
GIVEN
BY
ΜML
N
N
XN
N
EXERCISE
WHICH
IS
THE
MEAN
OF
THE
OBSERVED
SET
OF
DATA
POINTS
THE
MAXIMIZATION
OF
WITH
RESPECT
TO
Σ
IS
RATHER
MORE
INVOLVED
THE
SIMPLEST
APPROACH
IS
TO
IGNORE
THE
SYMMETRY
CONSTRAINT
AND
SHOW
THAT
THE
RESULTING
SOLUTION
IS
SYMMETRIC
AS
REQUIRED
ALTERNATIVE
DERIVATIONS
OF
THIS
RESULT
WHICH
IMPOSE
THE
SYMMETRY
AND
POSITIVE
DEFI
NITENESS
CONSTRAINTS
EXPLICITLY
CAN
BE
FOUND
IN
MAGNUS
AND
NEUDECKER
THE
RESULT
IS
AS
EXPECTED
AND
TAKES
THE
FORM
ΣML
N
N
XN
N
ΜML
XN
ΜML
T
EXERCISE
WHICH
INVOLVES
ΜML
BECAUSE
THIS
IS
THE
RESULT
OF
A
JOINT
MAXIMIZATION
WITH
RESPECT
TO
Μ
AND
Σ
NOTE
THAT
THE
SOLUTION
FOR
ΜML
DOES
NOT
DEPEND
ON
ΣML
AND
SO
WE
CAN
FIRST
EVALUATE
ΜML
AND
THEN
USE
THIS
TO
EVALUATE
ΣML
IF
WE
EVALUATE
THE
EXPECTATIONS
OF
THE
MAXIMUM
LIKELIHOOD
SOLUTIONS
UNDER
THE
TRUE
DISTRIBUTION
WE
OBTAIN
THE
FOLLOWING
RESULTS
E
ΜML
Μ
E
ΣML
N
Σ
N
WE
SEE
THAT
THE
EXPECTATION
OF
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
FOR
THE
MEAN
IS
EQUAL
TO
THE
TRUE
MEAN
HOWEVER
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
FOR
THE
COVARIANCE
HAS
AN
EXPECTATION
THAT
IS
LESS
THAN
THE
TRUE
VALUE
AND
HENCE
IT
IS
BIASED
WE
CAN
CORRECT
THIS
BIAS
BY
DEFINING
A
DIFFERENT
ESTIMATOR
Σ
GIVEN
BY
N
Σ
X
N
N
ΜML
XN
ΜML
T
CLEARLY
FROM
AND
THE
EXPECTATION
OF
Σ
IS
EQUAL
TO
Σ
SEQUENTIAL
ESTIMATION
OUR
DISCUSSION
OF
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
PARAMETERS
OF
A
GAUS
SIAN
DISTRIBUTION
PROVIDES
A
CONVENIENT
OPPORTUNITY
TO
GIVE
A
MORE
GENERAL
DISCUSSION
OF
THE
TOPIC
OF
SEQUENTIAL
ESTIMATION
FOR
MAXIMUM
LIKELIHOOD
SEQUENTIAL
METHODS
ALLOW
DATA
POINTS
TO
BE
PROCESSED
ONE
AT
A
TIME
AND
THEN
DISCARDED
AND
ARE
IMPORTANT
FOR
ON
LINE
APPLICATIONS
AND
ALSO
WHERE
LARGE
DATA
SETS
ARE
INVOLVED
SO
THAT
BATCH
PROCESSING
OF
ALL
DATA
POINTS
AT
ONCE
IS
INFEASIBLE
CONSIDER
THE
RESULT
FOR
THE
MAXIMUM
LIKELIHOOD
ESTIMATOR
OF
THE
MEAN
ΜML
WHICH
WE
WILL
DENOTE
BY
Μ
N
WHEN
IT
IS
BASED
ON
N
OBSERVATIONS
IF
WE
FIGURE
A
SCHEMATIC
ILLUSTRATION
OF
TWO
CORRELATED
RAN
DOM
VARIABLES
Z
AND
Θ
TOGETHER
WITH
THE
REGRESSION
FUNCTION
F
Θ
GIVEN
BY
THE
CON
DITIONAL
EXPECTATION
E
Z
Θ
THE
ROBBINS
MONRO
ALGORITHM
PROVIDES
A
GENERAL
SEQUEN
TIAL
PROCEDURE
FOR
FINDING
THE
ROOT
Θ
OF
SUCH
FUNCTIONS
DISSECT
OUT
THE
CONTRIBUTION
FROM
THE
FINAL
DATA
POINT
XN
WE
OBTAIN
N
ML
N
N
XN
N
N
X
N
N
N
XN
N
X
N
Μ
N
N
N
N
ML
Μ
N
X
Μ
N
THIS
RESULT
HAS
A
NICE
INTERPRETATION
AS
FOLLOWS
AFTER
OBSERVING
N
DATA
POINTS
WE
HAVE
ESTIMATED
Μ
BY
Μ
N
WE
NOW
OBSERVE
DATA
POINT
XN
AND
WE
OBTAIN
OUR
REVISED
ESTIMATE
Μ
N
BY
MOVING
THE
OLD
ESTIMATE
A
SMALL
AMOUNT
PROPORTIONAL
TO
N
IN
THE
DIRECTION
OF
THE
ERROR
SIGNAL
XN
Μ
N
NOTE
THAT
AS
N
INCREASES
SO
THE
CONTRIBUTION
FROM
SUCCESSIVE
DATA
POINTS
GETS
SMALLER
THE
RESULT
WILL
CLEARLY
GIVE
THE
SAME
ANSWER
AS
THE
BATCH
RESULT
BECAUSE
THE
TWO
FORMULAE
ARE
EQUIVALENT
HOWEVER
WE
WILL
NOT
ALWAYS
BE
ABLE
TO
DE
RIVE
A
SEQUENTIAL
ALGORITHM
BY
THIS
ROUTE
AND
SO
WE
SEEK
A
MORE
GENERAL
FORMULATION
OF
SEQUENTIAL
LEARNING
WHICH
LEADS
US
TO
THE
ROBBINS
MONRO
ALGORITHM
CONSIDER
A
PAIR
OF
RANDOM
VARIABLES
Θ
AND
Z
GOVERNED
BY
A
JOINT
DISTRIBUTION
P
Z
Θ
THE
CON
DITIONAL
EXPECTATION
OF
Z
GIVEN
Θ
DEFINES
A
DETERMINISTIC
FUNCTION
F
Θ
THAT
IS
GIVEN
BY
F
Θ
E
Z
Θ
R
ZP
Z
Θ
DZ
AND
IS
ILLUSTRATED
SCHEMATICALLY
IN
FIGURE
FUNCTIONS
DEFINED
IN
THIS
WAY
ARE
CALLED
REGRESSION
FUNCTIONS
OUR
GOAL
IS
TO
FIND
THE
ROOT
Θ
AT
WHICH
F
Θ
IF
WE
HAD
A
LARGE
DATA
SET
OF
OBSERVATIONS
OF
Z
AND
Θ
THEN
WE
COULD
MODEL
THE
REGRESSION
FUNCTION
DIRECTLY
AND
THEN
OBTAIN
AN
ESTIMATE
OF
ITS
ROOT
SUPPOSE
HOWEVER
THAT
WE
OBSERVE
VALUES
OF
Z
ONE
AT
A
TIME
AND
WE
WISH
TO
FIND
A
CORRESPONDING
SEQUENTIAL
ESTIMATION
SCHEME
FOR
Θ
THE
FOLLOWING
GENERAL
PROCEDURE
FOR
SOLVING
SUCH
PROBLEMS
WAS
GIVEN
BY
ROBBINS
AND
MONRO
WE
SHALL
ASSUME
THAT
THE
CONDITIONAL
VARIANCE
OF
Z
IS
FINITE
SO
THAT
E
Z
F
Θ
AND
WE
SHALL
ALSO
WITHOUT
LOSS
OF
GENERALITY
CONSIDER
THE
CASE
WHERE
F
Θ
FOR
Θ
Θ
AND
F
Θ
FOR
Θ
Θ
AS
IS
THE
CASE
IN
FIGURE
THE
ROBBINS
MONRO
PROCEDURE
THEN
DEFINES
A
SEQUENCE
OF
SUCCESSIVE
ESTIMATES
OF
THE
ROOT
Θ
GIVEN
BY
Θ
N
Θ
N
AN
Θ
N
WHERE
Z
Θ
N
IS
AN
OBSERVED
VALUE
OF
Z
WHEN
Θ
TAKES
THE
VALUE
Θ
N
THE
COEFFICIENTS
AN
REPRESENT
A
SEQUENCE
OF
POSITIVE
NUMBERS
THAT
SATISFY
THE
CONDITIONS
LIM
N
AN
AN
N
N
IT
CAN
THEN
BE
SHOWN
ROBBINS
AND
MONRO
FUKUNAGA
THAT
THE
SEQUENCE
OF
ESTIMATES
GIVEN
BY
DOES
INDEED
CONVERGE
TO
THE
ROOT
WITH
PROBABILITY
ONE
NOTE
THAT
THE
FIRST
CONDITION
ENSURES
THAT
THE
SUCCESSIVE
CORRECTIONS
DECREASE
IN
MAGNITUDE
SO
THAT
THE
PROCESS
CAN
CONVERGE
TO
A
LIMITING
VALUE
THE
SECOND
CON
DITION
IS
REQUIRED
TO
ENSURE
THAT
THE
ALGORITHM
DOES
NOT
CONVERGE
SHORT
OF
THE
ROOT
AND
THE
THIRD
CONDITION
IS
NEEDED
TO
ENSURE
THAT
THE
ACCUMULATED
NOISE
HAS
FINITE
VARIANCE
AND
HENCE
DOES
NOT
SPOIL
CONVERGENCE
NOW
LET
US
CONSIDER
HOW
A
GENERAL
MAXIMUM
LIKELIHOOD
PROBLEM
CAN
BE
SOLVED
SEQUENTIALLY
USING
THE
ROBBINS
MONRO
ALGORITHM
BY
DEFINITION
THE
MAXIMUM
LIKE
LIHOOD
SOLUTION
ΘML
IS
A
STATIONARY
POINT
OF
THE
LOG
LIKELIHOOD
FUNCTION
AND
HENCE
SATISFIES
Θ
N
N
LN
P
XN
Θ
ΘML
EXCHANGING
THE
DERIVATIVE
AND
THE
SUMMATION
AND
TAKING
THE
LIMIT
N
WE
HAVE
LIM
LN
P
XN
Θ
EX
LN
P
X
Θ
N
N
N
Θ
Θ
AND
SO
WE
SEE
THAT
FINDING
THE
MAXIMUM
LIKELIHOOD
SOLUTION
CORRESPONDS
TO
FIND
ING
THE
ROOT
OF
A
REGRESSION
FUNCTION
WE
CAN
THEREFORE
APPLY
THE
ROBBINS
MONRO
PROCEDURE
WHICH
NOW
TAKES
THE
FORM
Θ
N
Θ
N
A
LN
P
X
Θ
N
N
Θ
N
N
FIGURE
IN
THE
CASE
OF
A
GAUSSIAN
DISTRIBUTION
WITH
Θ
Z
CORRESPONDING
TO
THE
MEAN
Μ
THE
REGRESSION
FUNCTION
ILLUSTRATED
IN
FIGURE
TAKES
THE
FORM
OF
A
STRAIGHT
LINE
AS
SHOWN
IN
RED
IN
THIS
CASE
THE
RANDOM
VARIABLE
Z
CORRESPONDS
TO
THE
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
FUNCTION
AND
IS
GIVEN
BY
X
ΜML
AND
ITS
EXPECTATION
THAT
DEFINES
THE
REGRESSION
FUNCTION
IS
A
STRAIGHT
LINE
GIVEN
BY
Μ
ΜML
THE
ROOT
OF
THE
REGRES
SION
FUNCTION
CORRESPONDS
TO
THE
MAXIMUM
LIKE
LIHOOD
ESTIMATOR
ΜML
ΜML
P
Z
Μ
Μ
AS
A
SPECIFIC
EXAMPLE
WE
CONSIDER
ONCE
AGAIN
THE
SEQUENTIAL
ESTIMATION
OF
THE
MEAN
OF
A
GAUSSIAN
DISTRIBUTION
IN
WHICH
CASE
THE
PARAMETER
Θ
N
IS
THE
ESTIMATE
Μ
N
OF
THE
MEAN
OF
THE
GAUSSIAN
AND
THE
RANDOM
VARIABLE
Z
IS
GIVEN
BY
Z
LN
P
X
Μ
X
Μ
ΜML
ML
ML
THUS
THE
DISTRIBUTION
OF
Z
IS
GAUSSIAN
WITH
MEAN
Μ
ΜML
AS
ILLUSTRATED
IN
FIG
URE
SUBSTITUTING
INTO
WE
OBTAIN
THE
UNIVARIATE
FORM
OF
PROVIDED
WE
CHOOSE
THE
COEFFICIENTS
AN
TO
HAVE
THE
FORM
AN
N
NOTE
THAT
ALTHOUGH
WE
HAVE
FOCUSSED
ON
THE
CASE
OF
A
SINGLE
VARIABLE
THE
SAME
TECHNIQUE
TOGETHER
WITH
THE
SAME
RESTRICTIONS
ON
THE
COEFFICIENTS
AN
APPLY
EQUALLY
TO
THE
MULTIVARIATE
CASE
BLUM
BAYESIAN
INFERENCE
FOR
THE
GAUSSIAN
THE
MAXIMUM
LIKELIHOOD
FRAMEWORK
GAVE
POINT
ESTIMATES
FOR
THE
PARAMETERS
Μ
AND
Σ
NOW
WE
DEVELOP
A
BAYESIAN
TREATMENT
BY
INTRODUCING
PRIOR
DISTRIBUTIONS
OVER
THESE
PARAMETERS
LET
US
BEGIN
WITH
A
SIMPLE
EXAMPLE
IN
WHICH
WE
CONSIDER
A
SINGLE
GAUSSIAN
RANDOM
VARIABLE
X
WE
SHALL
SUPPOSE
THAT
THE
VARIANCE
IS
KNOWN
AND
WE
CONSIDER
THE
TASK
OF
INFERRING
THE
MEAN
Μ
GIVEN
A
SET
OF
N
OBSERVATIONS
X
XN
THE
LIKELIHOOD
FUNCTION
THAT
IS
THE
PROBABILITY
OF
THE
OBSERVED
DATA
GIVEN
Μ
VIEWED
AS
A
FUNCTION
OF
Μ
IS
GIVEN
BY
TT
AGAIN
WE
EMPHASIZE
THAT
THE
LIKELIHOOD
FUNCTION
P
X
Μ
IS
NOT
A
PROBABILITY
DISTRI
BUTION
OVER
Μ
AND
IS
NOT
NORMALIZED
WE
SEE
THAT
THE
LIKELIHOOD
FUNCTION
TAKES
THE
FORM
OF
THE
EXPONENTIAL
OF
A
QUAD
RATIC
FORM
IN
Μ
THUS
IF
WE
CHOOSE
A
PRIOR
P
Μ
GIVEN
BY
A
GAUSSIAN
IT
WILL
BE
A
CONJUGATE
DISTRIBUTION
FOR
THIS
LIKELIHOOD
FUNCTION
BECAUSE
THE
CORRESPONDING
POSTE
RIOR
WILL
BE
A
PRODUCT
OF
TWO
EXPONENTIALS
OF
QUADRATIC
FUNCTIONS
OF
Μ
AND
HENCE
WILL
ALSO
BE
GAUSSIAN
WE
THEREFORE
TAKE
OUR
PRIOR
DISTRIBUTION
TO
BE
P
Μ
N
Μ
EXERCISE
AND
THE
POSTERIOR
DISTRIBUTION
IS
GIVEN
BY
P
Μ
X
P
X
Μ
P
Μ
SIMPLE
MANIPULATION
INVOLVING
COMPLETING
THE
SQUARE
IN
THE
EXPONENT
SHOWS
THAT
THE
POSTERIOR
DISTRIBUTION
IS
GIVEN
BY
P
Μ
X
N
Μ
ΜN
WHERE
ΜN
ΜML
N
N
IN
WHICH
ΜML
IS
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
Μ
GIVEN
BY
THE
SAMPLE
MEAN
ΜML
N
N
XN
N
IT
IS
WORTH
SPENDING
A
MOMENT
STUDYING
THE
FORM
OF
THE
POSTERIOR
MEAN
AND
VARIANCE
FIRST
OF
ALL
WE
NOTE
THAT
THE
MEAN
OF
THE
POSTERIOR
DISTRIBUTION
GIVEN
BY
IS
A
COMPROMISE
BETWEEN
THE
PRIOR
MEAN
AND
THE
MAXIMUM
LIKELIHOOD
SOLUTION
ΜML
IF
THE
NUMBER
OF
OBSERVED
DATA
POINTS
N
THEN
REDUCES
TO
THE
PRIOR
MEAN
AS
EXPECTED
FOR
N
THE
POSTERIOR
MEAN
IS
GIVEN
BY
THE
MAXIMUM
LIKELIHOOD
SOLUTION
SIMILARLY
CONSIDER
THE
RESULT
FOR
THE
VARIANCE
OF
THE
POSTERIOR
DISTRIBUTION
WE
SEE
THAT
THIS
IS
MOST
NATURALLY
EXPRESSED
IN
TERMS
OF
THE
INVERSE
VARIANCE
WHICH
IS
CALLED
THE
PRECISION
FURTHERMORE
THE
PRECISIONS
ARE
ADDITIVE
SO
THAT
THE
PRECISION
OF
THE
POSTERIOR
IS
GIVEN
BY
THE
PRECISION
OF
THE
PRIOR
PLUS
ONE
CONTRIBUTION
OF
THE
DATA
PRECISION
FROM
EACH
OF
THE
OBSERVED
DATA
POINTS
AS
WE
INCREASE
THE
NUMBER
OF
OBSERVED
DATA
POINTS
THE
PRECISION
STEADILY
INCREASES
CORRESPONDING
TO
A
POSTERIOR
DISTRIBUTION
WITH
STEADILY
DECREASING
VARIANCE
WITH
NO
OBSERVED
DATA
POINTS
WE
HAVE
THE
PRIOR
VARIANCE
WHEREAS
IF
THE
NUMBER
OF
DATA
POINTS
N
THE
VARIANCE
GOES
TO
ZERO
AND
THE
POSTERIOR
DISTRIBUTION
BECOMES
INFINITELY
PEAKED
AROUND
THE
MAXIMUM
LIKELIHOOD
SOLUTION
WE
THEREFORE
SEE
THAT
THE
MAXIMUM
LIKELIHOOD
RESULT
OF
A
POINT
ESTIMATE
FOR
Μ
GIVEN
BY
IS
RECOVERED
PRECISELY
FROM
THE
BAYESIAN
FORMALISM
IN
THE
LIMIT
OF
AN
INFINITE
NUMBER
OF
OBSERVATIONS
NOTE
ALSO
THAT
FOR
FINITE
N
IF
WE
TAKE
THE
LIMIT
IN
WHICH
THE
PRIOR
HAS
INFINITE
VARIANCE
THEN
THE
POSTERIOR
MEAN
REDUCES
TO
THE
MAXIMUM
LIKELIHOOD
RESULT
WHILE
FROM
THE
POSTERIOR
VARIANCE
IS
GIVEN
BY
N
FIGURE
ILLUSTRATION
OF
BAYESIAN
INFERENCE
FOR
THE
MEAN
Μ
OF
A
GAUSSIAN
DISTRI
BUTION
IN
WHICH
THE
VARIANCE
IS
AS
SUMED
TO
BE
KNOWN
THE
CURVES
SHOW
THE
PRIOR
DISTRIBUTION
OVER
Μ
THE
CURVE
LABELLED
N
WHICH
IN
THIS
CASE
IS
ITSELF
GAUSSIAN
ALONG
WITH
THE
POSTERIOR
DISTRIBUTION
GIVEN
BY
FOR
INCREASING
NUMBERS
N
OF
DATA
POINTS
THE
DATA
POINTS
ARE
GENERATED
FROM
A
GAUSSIAN
OF
MEAN
AND
VARIANCE
AND
THE
PRIOR
IS
CHOSEN
TO
HAVE
MEAN
IN
BOTH
THE
PRIOR
AND
THE
LIKELIHOOD
FUNCTION
THE
VARIANCE
IS
SET
TO
THE
TRUE
VALUE
EXERCISE
SECTION
WE
ILLUSTRATE
OUR
ANALYSIS
OF
BAYESIAN
INFERENCE
FOR
THE
MEAN
OF
A
GAUSSIAN
DISTRIBUTION
IN
FIGURE
THE
GENERALIZATION
OF
THIS
RESULT
TO
THE
CASE
OF
A
D
DIMENSIONAL
GAUSSIAN
RANDOM
VARIABLE
X
WITH
KNOWN
COVARIANCE
AND
UNKNOWN
MEAN
IS
STRAIGHTFORWARD
WE
HAVE
ALREADY
SEEN
HOW
THE
MAXIMUM
LIKELIHOOD
EXPRESSION
FOR
THE
MEAN
OF
A
GAUSSIAN
CAN
BE
RE
CAST
AS
A
SEQUENTIAL
UPDATE
FORMULA
IN
WHICH
THE
MEAN
AFTER
OBSERVING
N
DATA
POINTS
WAS
EXPRESSED
IN
TERMS
OF
THE
MEAN
AFTER
OBSERVING
N
DATA
POINTS
TOGETHER
WITH
THE
CONTRIBUTION
FROM
DATA
POINT
XN
IN
FACT
THE
BAYESIAN
PARADIGM
LEADS
VERY
NATURALLY
TO
A
SEQUENTIAL
VIEW
OF
THE
INFERENCE
PROBLEM
TO
SEE
THIS
IN
THE
CONTEXT
OF
THE
INFERENCE
OF
THE
MEAN
OF
A
GAUSSIAN
WE
WRITE
THE
POSTERIOR
DISTRIBUTION
WITH
THE
CONTRIBUTION
FROM
THE
FINAL
DATA
POINT
XN
SEPARATED
OUT
SO
THAT
P
Μ
D
P
Μ
N
N
P
XN
Μ
L
P
XN
Μ
THE
TERM
IN
SQUARE
BRACKETS
IS
UP
TO
A
NORMALIZATION
COEFFICIENT
JUST
THE
POSTERIOR
DISTRIBUTION
AFTER
OBSERVING
N
DATA
POINTS
WE
SEE
THAT
THIS
CAN
BE
VIEWED
AS
A
PRIOR
DISTRIBUTION
WHICH
IS
COMBINED
USING
BAYES
THEOREM
WITH
THE
LIKELIHOOD
FUNCTION
ASSOCIATED
WITH
DATA
POINT
XN
TO
ARRIVE
AT
THE
POSTERIOR
DISTRIBUTION
AFTER
OBSERVING
N
DATA
POINTS
THIS
SEQUENTIAL
VIEW
OF
BAYESIAN
INFERENCE
IS
VERY
GENERAL
AND
APPLIES
TO
ANY
PROBLEM
IN
WHICH
THE
OBSERVED
DATA
ARE
ASSUMED
TO
BE
INDEPENDENT
AND
IDENTICALLY
DISTRIBUTED
SO
FAR
WE
HAVE
ASSUMED
THAT
THE
VARIANCE
OF
THE
GAUSSIAN
DISTRIBUTION
OVER
THE
DATA
IS
KNOWN
AND
OUR
GOAL
IS
TO
INFER
THE
MEAN
NOW
LET
US
SUPPOSE
THAT
THE
MEAN
IS
KNOWN
AND
WE
WISH
TO
INFER
THE
VARIANCE
AGAIN
OUR
CALCULATIONS
WILL
BE
GREATLY
SIMPLIFIED
IF
WE
CHOOSE
A
CONJUGATE
FORM
FOR
THE
PRIOR
DISTRIBUTION
IT
TURNS
OUT
TO
BE
MOST
CONVENIENT
TO
WORK
WITH
THE
PRECISION
Λ
THE
LIKELIHOOD
FUNCTION
FOR
Λ
TAKES
THE
FORM
P
X
Λ
NTT
N
XN
Μ
Λ
ΛN
EXP
N
Λ
N
XN
Μ
Λ
Λ
Λ
FIGURE
PLOT
OF
THE
GAMMA
DISTRIBUTION
GAM
Λ
A
B
DEFINED
BY
FOR
VARIOUS
VALUES
OF
THE
PARAMETERS
A
AND
B
EXERCISE
EXERCISE
THE
CORRESPONDING
CONJUGATE
PRIOR
SHOULD
THEREFORE
BE
PROPORTIONAL
TO
THE
PRODUCT
OF
A
POWER
OF
Λ
AND
THE
EXPONENTIAL
OF
A
LINEAR
FUNCTION
OF
Λ
THIS
CORRESPONDS
TO
THE
GAMMA
DISTRIBUTION
WHICH
IS
DEFINED
BY
GAM
Λ
A
B
BAΛA
EXP
BΛ
HERE
Γ
A
IS
THE
GAMMA
FUNCTION
THAT
IS
DEFINED
BY
AND
THAT
ENSURES
THAT
IS
CORRECTLY
NORMALIZED
THE
GAMMA
DISTRIBUTION
HAS
A
FINITE
INTEGRAL
IF
A
AND
THE
DISTRIBUTION
ITSELF
IS
FINITE
IF
A
IT
IS
PLOTTED
FOR
VARIOUS
VALUES
OF
A
AND
B
IN
FIGURE
THE
MEAN
AND
VARIANCE
OF
THE
GAMMA
DISTRIBUTION
ARE
GIVEN
BY
E
Λ
A
B
VAR
Λ
A
CONSIDER
A
PRIOR
DISTRIBUTION
GAM
Λ
IF
WE
MULTIPLY
BY
THE
LIKELIHOOD
FUNCTION
THEN
WE
OBTAIN
A
POSTERIOR
DISTRIBUTION
Λ
WHICH
WE
RECOGNIZE
AS
A
GAMMA
DISTRIBUTION
OF
THE
FORM
GAM
Λ
AN
BN
WHERE
N
AN
N
B
B
X
Μ
B
N
WHERE
IS
THE
MAXIMUM
LIKELIHOOD
ESTIMATOR
OF
THE
VARIANCE
NOTE
THAT
IN
THERE
IS
NO
NEED
TO
KEEP
TRACK
OF
THE
NORMALIZATION
CONSTANTS
IN
THE
PRIOR
AND
THE
LIKELIHOOD
FUNCTION
BECAUSE
IF
REQUIRED
THE
CORRECT
COEFFICIENT
CAN
BE
FOUND
AT
THE
END
USING
THE
NORMALIZED
FORM
FOR
THE
GAMMA
DISTRIBUTION
FROM
WE
SEE
THAT
THE
EFFECT
OF
OBSERVING
N
DATA
POINTS
IS
TO
INCREASE
THE
VALUE
OF
THE
COEFFICIENT
A
BY
N
THUS
WE
CAN
INTERPRET
THE
PARAMETER
IN
THE
PRIOR
IN
TERMS
OF
EFFECTIVE
PRIOR
OBSERVATIONS
SIMILARLY
FROM
WE
SEE
THAT
THE
N
DATA
POINTS
CONTRIBUTE
TO
THE
PARAMETER
B
WHERE
IS
SECTION
THE
VARIANCE
AND
SO
WE
CAN
INTERPRET
THE
PARAMETER
IN
THE
PRIOR
AS
ARISING
FROM
THE
EFFECTIVE
PRIOR
OBSERVATIONS
HAVING
VARIANCE
RECALL
THAT
WE
MADE
AN
ANALOGOUS
INTERPRETATION
FOR
THE
DIRICHLET
PRIOR
THESE
DISTRIBUTIONS
ARE
EXAMPLES
OF
THE
EXPONENTIAL
FAMILY
AND
WE
SHALL
SEE
THAT
THE
INTERPRETATION
OF
A
CONJUGATE
PRIOR
IN
TERMS
OF
EFFECTIVE
FICTITIOUS
DATA
POINTS
IS
A
GENERAL
ONE
FOR
THE
EXPONENTIAL
FAMILY
OF
DISTRIBUTIONS
INSTEAD
OF
WORKING
WITH
THE
PRECISION
WE
CAN
CONSIDER
THE
VARIANCE
ITSELF
THE
CONJUGATE
PRIOR
IN
THIS
CASE
IS
CALLED
THE
INVERSE
GAMMA
DISTRIBUTION
ALTHOUGH
WE
SHALL
NOT
DISCUSS
THIS
FURTHER
BECAUSE
WE
WILL
FIND
IT
MORE
CONVENIENT
TO
WORK
WITH
THE
PRECISION
NOW
SUPPOSE
THAT
BOTH
THE
MEAN
AND
THE
PRECISION
ARE
UNKNOWN
TO
FIND
A
CONJUGATE
PRIOR
WE
CONSIDER
THE
DEPENDENCE
OF
THE
LIKELIHOOD
FUNCTION
ON
Μ
AND
Λ
TT
Λ
R
Λ
EXP
N
EXP
ΛΜ
N
N
Λ
N
N
N
WE
NOW
WISH
TO
IDENTIFY
A
PRIOR
DISTRIBUTION
P
Μ
Λ
THAT
HAS
THE
SAME
FUNCTIONAL
DEPENDENCE
ON
Μ
AND
Λ
AS
THE
LIKELIHOOD
FUNCTION
AND
THAT
SHOULD
THEREFORE
TAKE
THE
FORM
P
Μ
Λ
EXP
Β
EXP
CΛΜ
DΛ
EXP
R
ΒΛ
Β
EXP
R
WHERE
C
D
AND
Β
ARE
CONSTANTS
SINCE
WE
CAN
ALWAYS
WRITE
P
Μ
Λ
P
Μ
Λ
P
Λ
WE
CAN
FIND
P
Μ
Λ
AND
P
Λ
BY
INSPECTION
IN
PARTICULAR
WE
SEE
THAT
P
Μ
Λ
IS
A
GAUSSIAN
WHOSE
PRECISION
IS
A
LINEAR
FUNCTION
OF
Λ
AND
THAT
P
Λ
IS
A
GAMMA
DISTRI
BUTION
SO
THAT
THE
NORMALIZED
PRIOR
TAKES
THE
FORM
P
Μ
Λ
N
Μ
ΒΛ
GAM
Λ
A
B
WHERE
WE
HAVE
DEFINED
NEW
CONSTANTS
GIVEN
BY
C
Β
A
Β
B
D
THE
DISTRIBUTION
IS
CALLED
THE
NORMAL
GAMMA
OR
GAUSSIAN
GAMMA
DISTRIBUTION
AND
IS
PLOTTED
IN
FIGURE
NOTE
THAT
THIS
IS
NOT
SIMPLY
THE
PRODUCT
OF
AN
INDEPENDENT
GAUSSIAN
PRIOR
OVER
Μ
AND
A
GAMMA
PRIOR
OVER
Λ
BECAUSE
THE
PRECISION
OF
Μ
IS
A
LINEAR
FUNCTION
OF
Λ
EVEN
IF
WE
CHOSE
A
PRIOR
IN
WHICH
Μ
AND
Λ
WERE
INDEPENDENT
THE
POSTERIOR
DISTRIBUTION
WOULD
EXHIBIT
A
COUPLING
BETWEEN
THE
PRECISION
OF
Μ
AND
THE
VALUE
OF
Λ
FIGURE
CONTOUR
PLOT
OF
THE
NORMAL
GAMMA
DISTRIBUTION
FOR
PARAMETER
VALUES
Β
A
AND
B
Λ
EXERCISE
Μ
IN
THE
CASE
OF
THE
MULTIVARIATE
GAUSSIAN
DISTRIBUTION
X
Μ
Λ
FOR
A
D
DIMENSIONAL
VARIABLE
X
THE
CONJUGATE
PRIOR
DISTRIBUTION
FOR
THE
MEAN
Μ
ASSUMING
THE
PRECISION
IS
KNOWN
IS
AGAIN
A
GAUSSIAN
FOR
KNOWN
MEAN
AND
UNKNOWN
PRECISION
MATRIX
Λ
THE
CONJUGATE
PRIOR
IS
THE
WISHART
DISTRIBUTION
GIVEN
BY
W
Λ
W
Ν
B
Λ
Ν
D
EXP
TR
W
WHERE
Ν
IS
CALLED
THE
NUMBER
OF
DEGREES
OF
FREEDOM
OF
THE
DISTRIBUTION
W
IS
A
D
D
SCALE
MATRIX
AND
TR
DENOTES
THE
TRACE
THE
NORMALIZATION
CONSTANT
B
IS
GIVEN
BY
B
W
Ν
W
Ν
ΠD
D
TTI
Ν
I
SECTION
EXERCISE
AGAIN
IT
IS
ALSO
POSSIBLE
TO
DEFINE
A
CONJUGATE
PRIOR
OVER
THE
COVARIANCE
MATRIX
ITSELF
RATHER
THAN
OVER
THE
PRECISION
MATRIX
WHICH
LEADS
TO
THE
INVERSE
WISHART
DISTRIBU
TION
ALTHOUGH
WE
SHALL
NOT
DISCUSS
THIS
FURTHER
IF
BOTH
THE
MEAN
AND
THE
PRECISION
ARE
UNKNOWN
THEN
FOLLOWING
A
SIMILAR
LINE
OF
REASONING
TO
THE
UNIVARIATE
CASE
THE
CONJUGATE
PRIOR
IS
GIVEN
BY
P
Μ
Λ
Β
W
Ν
N
Μ
ΒΛ
W
Λ
W
Ν
WHICH
IS
KNOWN
AS
THE
NORMAL
WISHART
OR
GAUSSIAN
WISHART
DISTRIBUTION
STUDENT
T
DISTRIBUTION
WE
HAVE
SEEN
THAT
THE
CONJUGATE
PRIOR
FOR
THE
PRECISION
OF
A
GAUSSIAN
IS
GIVEN
BY
A
GAMMA
DISTRIBUTION
IF
WE
HAVE
A
UNIVARIATE
GAUSSIAN
X
Μ
Τ
TOGETHER
WITH
A
GAMMA
PRIOR
GAM
Τ
A
B
AND
WE
INTEGRATE
OUT
THE
PRECISION
WE
OBTAIN
THE
MARGINAL
DISTRIBUTION
OF
X
IN
THE
FORM
FIGURE
PLOT
OF
STUDENT
T
DISTRIBUTION
FOR
Μ
AND
Λ
FOR
VARIOUS
VALUES
OF
Ν
THE
LIMIT
Ν
CORRESPONDS
TO
A
GAUSSIAN
DISTRIBUTION
WITH
MEAN
Μ
AND
PRECISION
Λ
P
X
Μ
A
B
R
N
X
Μ
Τ
GAM
Τ
A
B
DΤ
R
BAE
BΤ
ΤA
Τ
EXP
J
Τ
D
BA
X
Μ
A
WHERE
WE
HAVE
MADE
THE
CHANGE
OF
VARIABLE
Z
Τ
B
X
Μ
BY
CONVENTION
WE
DEFINE
NEW
PARAMETERS
GIVEN
BY
Ν
AND
Λ
A
B
IN
TERMS
OF
WHICH
THE
DISTRIBUTION
P
X
Μ
A
B
TAKES
THE
FORM
Γ
Ν
Λ
Λ
X
Μ
Ν
EXERCISE
EXERCISE
WHICH
IS
KNOWN
AS
STUDENT
T
DISTRIBUTION
THE
PARAMETER
Λ
IS
SOMETIMES
CALLED
THE
PRECISION
OF
THE
T
DISTRIBUTION
EVEN
THOUGH
IT
IS
NOT
IN
GENERAL
EQUAL
TO
THE
INVERSE
OF
THE
VARIANCE
THE
PARAMETER
Ν
IS
CALLED
THE
DEGREES
OF
FREEDOM
AND
ITS
EFFECT
IS
ILLUSTRATED
IN
FIGURE
FOR
THE
PARTICULAR
CASE
OF
Ν
THE
T
DISTRIBUTION
REDUCES
TO
THE
CAUCHY
DISTRIBUTION
WHILE
IN
THE
LIMIT
Ν
THE
T
DISTRIBUTION
ST
X
Μ
Λ
Ν
BECOMES
A
GAUSSIAN
X
Μ
Λ
WITH
MEAN
Μ
AND
PRECISION
Λ
FROM
WE
SEE
THAT
STUDENT
T
DISTRIBUTION
IS
OBTAINED
BY
ADDING
UP
AN
INFINITE
NUMBER
OF
GAUSSIAN
DISTRIBUTIONS
HAVING
THE
SAME
MEAN
BUT
DIFFERENT
PRECI
SIONS
THIS
CAN
BE
INTERPRETED
AS
AN
INFINITE
MIXTURE
OF
GAUSSIANS
GAUSSIAN
MIXTURES
WILL
BE
DISCUSSED
IN
DETAIL
IN
SECTION
THE
RESULT
IS
A
DISTRIBUTION
THAT
IN
GEN
ERAL
HAS
LONGER
TAILS
THAN
A
GAUSSIAN
AS
WAS
SEEN
IN
FIGURE
THIS
GIVES
THE
T
DISTRIBUTION
AN
IMPORTANT
PROPERTY
CALLED
ROBUSTNESS
WHICH
MEANS
THAT
IT
IS
MUCH
LESS
SENSITIVE
THAN
THE
GAUSSIAN
TO
THE
PRESENCE
OF
A
FEW
DATA
POINTS
WHICH
ARE
OUTLIERS
THE
ROBUSTNESS
OF
THE
T
DISTRIBUTION
IS
ILLUSTRATED
IN
FIGURE
WHICH
COMPARES
THE
MAXIMUM
LIKELIHOOD
SOLUTIONS
FOR
A
GAUSSIAN
AND
A
T
DISTRIBUTION
NOTE
THAT
THE
MAX
IMUM
LIKELIHOOD
SOLUTION
FOR
THE
T
DISTRIBUTION
CAN
BE
FOUND
USING
THE
EXPECTATION
MAXIMIZATION
EM
ALGORITHM
HERE
WE
SEE
THAT
THE
EFFECT
OF
A
SMALL
NUMBER
OF
A
B
FIGURE
ILLUSTRATION
OF
THE
ROBUSTNESS
OF
STUDENT
T
DISTRIBUTION
COMPARED
TO
A
GAUSSIAN
A
HISTOGRAM
DISTRIBUTION
OF
DATA
POINTS
DRAWN
FROM
A
GAUSSIAN
DISTRIBUTION
TOGETHER
WITH
THE
MAXIMUM
LIKELIHOOD
FIT
OB
TAINED
FROM
A
T
DISTRIBUTION
RED
CURVE
AND
A
GAUSSIAN
GREEN
CURVE
LARGELY
HIDDEN
BY
THE
RED
CURVE
BECAUSE
THE
T
DISTRIBUTION
CONTAINS
THE
GAUSSIAN
AS
A
SPECIAL
CASE
IT
GIVES
ALMOST
THE
SAME
SOLUTION
AS
THE
GAUSSIAN
B
THE
SAME
DATA
SET
BUT
WITH
THREE
ADDITIONAL
OUTLYING
DATA
POINTS
SHOWING
HOW
THE
GAUSSIAN
GREEN
CURVE
IS
STRONGLY
DISTORTED
BY
THE
OUTLIERS
WHEREAS
THE
T
DISTRIBUTION
RED
CURVE
IS
RELATIVELY
UNAFFECTED
EXERCISE
OUTLIERS
IS
MUCH
LESS
SIGNIFICANT
FOR
THE
T
DISTRIBUTION
THAN
FOR
THE
GAUSSIAN
OUTLIERS
CAN
ARISE
IN
PRACTICAL
APPLICATIONS
EITHER
BECAUSE
THE
PROCESS
THAT
GENERATES
THE
DATA
CORRESPONDS
TO
A
DISTRIBUTION
HAVING
A
HEAVY
TAIL
OR
SIMPLY
THROUGH
MISLABELLED
DATA
ROBUSTNESS
IS
ALSO
AN
IMPORTANT
PROPERTY
FOR
REGRESSION
PROBLEMS
UNSURPRISINGLY
THE
LEAST
SQUARES
APPROACH
TO
REGRESSION
DOES
NOT
EXHIBIT
ROBUSTNESS
BECAUSE
IT
COR
RESPONDS
TO
MAXIMUM
LIKELIHOOD
UNDER
A
CONDITIONAL
GAUSSIAN
DISTRIBUTION
BY
BASING
A
REGRESSION
MODEL
ON
A
HEAVY
TAILED
DISTRIBUTION
SUCH
AS
A
T
DISTRIBUTION
WE
OBTAIN
A
MORE
ROBUST
MODEL
IF
WE
GO
BACK
TO
AND
SUBSTITUTE
THE
ALTERNATIVE
PARAMETERS
Ν
Λ
A
B
AND
Η
ΤB
A
WE
SEE
THAT
THE
T
DISTRIBUTION
CAN
BE
WRITTEN
IN
THE
FORM
ST
X
Μ
Λ
Ν
R
N
X
Μ
ΗΛ
GAM
Η
Ν
Ν
DΗ
WE
CAN
THEN
GENERALIZE
THIS
TO
A
MULTIVARIATE
GAUSSIAN
X
Μ
Λ
TO
OBTAIN
THE
COR
RESPONDING
MULTIVARIATE
STUDENT
T
DISTRIBUTION
IN
THE
FORM
ST
X
Μ
Λ
Ν
R
N
X
Μ
ΗΛ
GAM
Η
Ν
Ν
DΗ
USING
THE
SAME
TECHNIQUE
AS
FOR
THE
UNIVARIATE
CASE
WE
CAN
EVALUATE
THIS
INTEGRAL
TO
GIVE
ST
X
Μ
Λ
Ν
Γ
D
Ν
Γ
Ν
Λ
ΠΝ
D
D
Ν
Ν
WHERE
D
IS
THE
DIMENSIONALITY
OF
X
AND
IS
THE
SQUARED
MAHALANOBIS
DISTANCE
DEFINED
BY
EXERCISE
X
Μ
TΛ
X
Μ
THIS
IS
THE
MULTIVARIATE
FORM
OF
STUDENT
T
DISTRIBUTION
AND
SATISFIES
THE
FOLLOWING
PROPERTIES
E
X
Μ
IF
Ν
COV
X
Ν
Ν
Λ
IF
Ν
MODE
X
Μ
WITH
CORRESPONDING
RESULTS
FOR
THE
UNIVARIATE
CASE
PERIODIC
VARIABLES
ALTHOUGH
GAUSSIAN
DISTRIBUTIONS
ARE
OF
GREAT
PRACTICAL
SIGNIFICANCE
BOTH
IN
THEIR
OWN
RIGHT
AND
AS
BUILDING
BLOCKS
FOR
MORE
COMPLEX
PROBABILISTIC
MODELS
THERE
ARE
SITUATIONS
IN
WHICH
THEY
ARE
INAPPROPRIATE
AS
DENSITY
MODELS
FOR
CONTINUOUS
VARI
ABLES
ONE
IMPORTANT
CASE
WHICH
ARISES
IN
PRACTICAL
APPLICATIONS
IS
THAT
OF
PERIODIC
VARIABLES
AN
EXAMPLE
OF
A
PERIODIC
VARIABLE
WOULD
BE
THE
WIND
DIRECTION
AT
A
PARTICULAR
GEOGRAPHICAL
LOCATION
WE
MIGHT
FOR
INSTANCE
MEASURE
VALUES
OF
WIND
DIRECTION
ON
A
NUMBER
OF
DAYS
AND
WISH
TO
SUMMARIZE
THIS
USING
A
PARAMETRIC
DISTRIBUTION
ANOTHER
EXAMPLE
IS
CALENDAR
TIME
WHERE
WE
MAY
BE
INTERESTED
IN
MODELLING
QUANTITIES
THAT
ARE
BELIEVED
TO
BE
PERIODIC
OVER
HOURS
OR
OVER
AN
ANNUAL
CYCLE
SUCH
QUANTITIES
CAN
CONVENIENTLY
BE
REPRESENTED
USING
AN
ANGULAR
POLAR
COORDINATE
Θ
WE
MIGHT
BE
TEMPTED
TO
TREAT
PERIODIC
VARIABLES
BY
CHOOSING
SOME
DIRECTION
AS
THE
ORIGIN
AND
THEN
APPLYING
A
CONVENTIONAL
DISTRIBUTION
SUCH
AS
THE
GAUSSIAN
SUCH
AN
APPROACH
HOWEVER
WOULD
GIVE
RESULTS
THAT
WERE
STRONGLY
DEPENDENT
ON
THE
ARBITRARY
CHOICE
OF
ORIGIN
SUPPOSE
FOR
INSTANCE
THAT
WE
HAVE
TWO
OBSERVATIONS
AT
AND
AND
WE
MODEL
THEM
USING
A
STANDARD
UNIVARIATE
GAUSSIAN
DISTRIBUTION
IF
WE
CHOOSE
THE
ORIGIN
AT
THEN
THE
SAMPLE
MEAN
OF
THIS
DATA
SET
WILL
BE
WITH
STANDARD
DEVIATION
WHEREAS
IF
WE
CHOOSE
THE
ORIGIN
AT
THEN
THE
MEAN
WILL
BE
AND
THE
STANDARD
DEVIATION
WILL
BE
WE
CLEARLY
NEED
TO
DEVELOP
A
SPECIAL
APPROACH
FOR
THE
TREATMENT
OF
PERIODIC
VARIABLES
LET
US
CONSIDER
THE
PROBLEM
OF
EVALUATING
THE
MEAN
OF
A
SET
OF
OBSERVATIONS
ΘN
OF
A
PERIODIC
VARIABLE
FROM
NOW
ON
WE
SHALL
ASSUME
THAT
Θ
IS
MEASURED
IN
RADIANS
WE
HAVE
ALREADY
SEEN
THAT
THE
SIMPLE
AVERAGE
ΘN
N
WILL
BE
STRONGLY
COORDINATE
DEPENDENT
TO
FIND
AN
INVARIANT
MEASURE
OF
THE
MEAN
WE
NOTE
THAT
THE
OBSERVATIONS
CAN
BE
VIEWED
AS
POINTS
ON
THE
UNIT
CIRCLE
AND
CAN
THEREFORE
BE
DESCRIBED
INSTEAD
BY
TWO
DIMENSIONAL
UNIT
VECTORS
XN
WHERE
LXNL
FOR
N
N
AS
ILLUSTRATED
IN
FIGURE
WE
CAN
AVERAGE
THE
VECTORS
XN
FIGURE
ILLUSTRATION
OF
THE
REPRESENTATION
OF
VAL
UES
ΘN
OF
A
PERIODIC
VARIABLE
AS
TWO
DIMENSIONAL
VECTORS
XN
LIVING
ON
THE
UNIT
CIRCLE
ALSO
SHOWN
IS
THE
AVERAGE
X
OF
THOSE
VECTORS
INSTEAD
TO
GIVE
X
N
N
XN
N
AND
THEN
FIND
THE
CORRESPONDING
ANGLE
Θ
OF
THIS
AVERAGE
CLEARLY
THIS
DEFINITION
WILL
ENSURE
THAT
THE
LOCATION
OF
THE
MEAN
IS
INDEPENDENT
OF
THE
ORIGIN
OF
THE
ANGULAR
COOR
DINATE
NOTE
THAT
X
WILL
TYPICALLY
LIE
INSIDE
THE
UNIT
CIRCLE
THE
CARTESIAN
COORDINATES
OF
THE
OBSERVATIONS
ARE
GIVEN
BY
XN
COS
ΘN
SIN
ΘN
AND
WE
CAN
WRITE
THE
CARTE
SIAN
COORDINATES
OF
THE
SAMPLE
MEAN
IN
THE
FORM
X
R
COS
Θ
R
SIN
Θ
SUBSTITUTING
INTO
AND
EQUATING
THE
AND
COMPONENTS
THEN
GIVES
R
COS
Θ
N
N
COS
ΘN
N
R
SIN
Θ
N
N
SIN
ΘN
N
TAKING
THE
RATIO
AND
USING
THE
IDENTITY
TAN
Θ
SIN
Θ
COS
Θ
WE
CAN
SOLVE
FOR
Θ
TO
GIVE
TAN
N
SIN
ΘN
N
COS
ΘN
SHORTLY
WE
SHALL
SEE
HOW
THIS
RESULT
ARISES
NATURALLY
AS
THE
MAXIMUM
LIKELIHOOD
ESTIMATOR
FOR
AN
APPROPRIATELY
DEFINED
DISTRIBUTION
OVER
A
PERIODIC
VARIABLE
WE
NOW
CONSIDER
A
PERIODIC
GENERALIZATION
OF
THE
GAUSSIAN
CALLED
THE
VON
MISES
DISTRIBUTION
HERE
WE
SHALL
LIMIT
OUR
ATTENTION
TO
UNIVARIATE
DISTRIBUTIONS
ALTHOUGH
PERIODIC
DISTRIBUTIONS
CAN
ALSO
BE
FOUND
OVER
HYPERSPHERES
OF
ARBITRARY
DIMENSION
FOR
AN
EXTENSIVE
DISCUSSION
OF
PERIODIC
DISTRIBUTIONS
SEE
MARDIA
AND
JUPP
BY
CONVENTION
WE
WILL
CONSIDER
DISTRIBUTIONS
P
Θ
THAT
HAVE
PERIOD
ANY
PROBABILITY
DENSITY
P
Θ
DEFINED
OVER
Θ
MUST
NOT
ONLY
BE
NONNEGATIVE
AND
INTEGRATE
FIGURE
THE
VON
MISES
DISTRIBUTION
CAN
BE
DERIVED
BY
CONSIDERING
A
TWO
DIMENSIONAL
GAUSSIAN
OF
THE
FORM
WHOSE
DENSITY
CONTOURS
ARE
SHOWN
IN
BLUE
AND
CONDITIONING
ON
THE
UNIT
CIRCLE
SHOWN
IN
RED
TO
ONE
BUT
IT
MUST
ALSO
BE
PERIODIC
THUS
P
Θ
MUST
SATISFY
THE
THREE
CONDITIONS
P
Θ
P
Θ
DΘ
P
Θ
P
Θ
FROM
IT
FOLLOWS
THAT
P
Θ
M
P
Θ
FOR
ANY
INTEGER
M
WE
CAN
EASILY
OBTAIN
A
GAUSSIAN
LIKE
DISTRIBUTION
THAT
SATISFIES
THESE
THREE
PROP
ERTIES
AS
FOLLOWS
CONSIDER
A
GAUSSIAN
DISTRIBUTION
OVER
TWO
VARIABLES
X
HAVING
MEAN
Μ
AND
A
COVARIANCE
MATRIX
Σ
WHERE
I
IS
THE
IDENTITY
MATRIX
SO
THAT
P
EXP
R
THE
CONTOURS
OF
CONSTANT
P
X
ARE
CIRCLES
AS
ILLUSTRATED
IN
FIGURE
NOW
SUPPOSE
WE
CONSIDER
THE
VALUE
OF
THIS
DISTRIBUTION
ALONG
A
CIRCLE
OF
FIXED
RADIUS
THEN
BY
CON
STRUCTION
THIS
DISTRIBUTION
WILL
BE
PERIODIC
ALTHOUGH
IT
WILL
NOT
BE
NORMALIZED
WE
CAN
DETERMINE
THE
FORM
OF
THIS
DISTRIBUTION
BY
TRANSFORMING
FROM
CARTESIAN
COORDINATES
TO
POLAR
COORDINATES
R
Θ
SO
THAT
R
COS
Θ
R
SIN
Θ
WE
ALSO
MAP
THE
MEAN
Μ
INTO
POLAR
COORDINATES
BY
WRITING
COS
SIN
NEXT
WE
SUBSTITUTE
THESE
TRANSFORMATIONS
INTO
THE
TWO
DIMENSIONAL
GAUSSIAN
DISTRIBU
TION
AND
THEN
CONDITION
ON
THE
UNIT
CIRCLE
R
NOTING
THAT
WE
ARE
INTERESTED
ONLY
IN
THE
DEPENDENCE
ON
Θ
FOCUSSING
ON
THE
EXPONENT
IN
THE
GAUSSIAN
DISTRIBUTION
WE
HAVE
R
COS
Θ
COS
R
SIN
Θ
SIN
COS
Θ
COS
SIN
Θ
SIN
COS
Θ
Θ
CONST
FIGURE
THE
VON
MISES
DISTRIBUTION
PLOTTED
FOR
TWO
DIFFERENT
PARAMETER
VALUES
SHOWN
AS
A
CARTESIAN
PLOT
ON
THE
LEFT
AND
AS
THE
CORRESPONDING
POLAR
PLOT
ON
THE
RIGHT
EXERCISE
WHERE
CONST
DENOTES
TERMS
INDEPENDENT
OF
Θ
AND
WE
HAVE
MADE
USE
OF
THE
FOLLOWING
TRIGONOMETRICAL
IDENTITIES
A
A
COS
A
COS
B
SIN
A
SIN
B
COS
A
B
IF
WE
NOW
DEFINE
M
WE
OBTAIN
OUR
FINAL
EXPRESSION
FOR
THE
DISTRIBUTION
OF
P
Θ
ALONG
THE
UNIT
CIRCLE
R
IN
THE
FORM
P
Θ
M
M
EXP
M
COS
Θ
WHICH
IS
CALLED
THE
VON
MISES
DISTRIBUTION
OR
THE
CIRCULAR
NORMAL
HERE
THE
PARAM
ETER
CORRESPONDS
TO
THE
MEAN
OF
THE
DISTRIBUTION
WHILE
M
WHICH
IS
KNOWN
AS
THE
CONCENTRATION
PARAMETER
IS
ANALOGOUS
TO
THE
INVERSE
VARIANCE
PRECISION
FOR
THE
GAUSSIAN
THE
NORMALIZATION
COEFFICIENT
IN
IS
EXPRESSED
IN
TERMS
OF
M
WHICH
IS
THE
ZEROTH
ORDER
BESSEL
FUNCTION
OF
THE
FIRST
KIND
ABRAMOWITZ
AND
STEGUN
AND
IS
DEFINED
BY
M
EXP
M
COS
Θ
DΘ
EXERCISE
FOR
LARGE
M
THE
DISTRIBUTION
BECOMES
APPROXIMATELY
GAUSSIAN
THE
VON
MISES
DIS
TRIBUTION
IS
PLOTTED
IN
FIGURE
AND
THE
FUNCTION
M
IS
PLOTTED
IN
FIGURE
NOW
CONSIDER
THE
MAXIMUM
LIKELIHOOD
ESTIMATORS
FOR
THE
PARAMETERS
AND
M
FOR
THE
VON
MISES
DISTRIBUTION
THE
LOG
LIKELIHOOD
FUNCTION
IS
GIVEN
BY
N
LN
P
D
M
N
LN
N
LN
M
M
COS
ΘN
N
M
A
M
M
M
FIGURE
PLOT
OF
THE
BESSEL
FUNCTION
M
DEFINED
BY
TOGETHER
WITH
THE
FUNCTION
A
M
DEFINED
BY
EXERCISE
SETTING
THE
DERIVATIVE
WITH
RESPECT
TO
EQUAL
TO
ZERO
GIVES
N
SIN
ΘN
N
TO
SOLVE
FOR
WE
MAKE
USE
OF
THE
TRIGONOMETRIC
IDENTITY
SIN
A
B
COS
B
SIN
A
COS
A
SIN
B
FROM
WHICH
WE
OBTAIN
ΘML
TAN
R
N
SIN
ΘN
WHICH
WE
RECOGNIZE
AS
THE
RESULT
OBTAINED
EARLIER
FOR
THE
MEAN
OF
THE
OBSER
VATIONS
VIEWED
IN
A
TWO
DIMENSIONAL
CARTESIAN
SPACE
SIMILARLY
MAXIMIZING
WITH
RESPECT
TO
M
AND
MAKING
USE
OF
M
M
ABRAMOWITZ
AND
STEGUN
WE
HAVE
N
A
M
N
COS
ΘN
N
ΘML
WHERE
WE
HAVE
SUBSTITUTED
FOR
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
ΘML
RECALLING
THAT
WE
ARE
PERFORMING
A
JOINT
OPTIMIZATION
OVER
Θ
AND
M
AND
WE
HAVE
DEFINED
M
A
M
M
THE
FUNCTION
A
M
IS
PLOTTED
IN
FIGURE
MAKING
USE
OF
THE
TRIGONOMETRIC
IDEN
TITY
WE
CAN
WRITE
IN
THE
FORM
A
MML
N
N
COS
ΘN
COS
ΘML
N
N
SIN
ΘN
SIN
ΘML
FIGURE
PLOTS
OF
THE
OLD
FAITH
FUL
DATA
IN
WHICH
THE
BLUE
CURVES
SHOW
CONTOURS
OF
CONSTANT
PROBA
BILITY
DENSITY
ON
THE
LEFT
IS
A
SINGLE
GAUSSIAN
DISTRIBUTION
WHICH
HAS
BEEN
FITTED
TO
THE
DATA
US
ING
MAXIMUM
LIKELIHOOD
NOTE
THAT
THIS
DISTRIBUTION
FAILS
TO
CAPTURE
THE
TWO
CLUMPS
IN
THE
DATA
AND
INDEED
PLACES
MUCH
OF
ITS
PROBABILITY
MASS
IN
THE
CENTRAL
REGION
BETWEEN
THE
CLUMPS
WHERE
THE
DATA
ARE
RELATIVELY
SPARSE
ON
THE
RIGHT
THE
DISTRIBUTION
IS
GIVEN
BY
A
LINEAR
COMBINATION
OF
TWO
GAUSSIANS
WHICH
HAS
BEEN
FITTED
TO
THE
DATA
BY
MAXIMUM
LIKELIHOOD
USING
TECHNIQUES
DISCUSSED
CHAP
TER
AND
WHICH
GIVES
A
BETTER
REP
RESENTATION
OF
THE
DATA
APPENDIX
A
THE
RIGHT
HAND
SIDE
OF
IS
EASILY
EVALUATED
AND
THE
FUNCTION
A
M
CAN
BE
INVERTED
NUMERICALLY
FOR
COMPLETENESS
WE
MENTION
BRIEFLY
SOME
ALTERNATIVE
TECHNIQUES
FOR
THE
CON
STRUCTION
OF
PERIODIC
DISTRIBUTIONS
THE
SIMPLEST
APPROACH
IS
TO
USE
A
HISTOGRAM
OF
OBSERVATIONS
IN
WHICH
THE
ANGULAR
COORDINATE
IS
DIVIDED
INTO
FIXED
BINS
THIS
HAS
THE
VIRTUE
OF
SIMPLICITY
AND
FLEXIBILITY
BUT
ALSO
SUFFERS
FROM
SIGNIFICANT
LIMITATIONS
AS
WE
SHALL
SEE
WHEN
WE
DISCUSS
HISTOGRAM
METHODS
IN
MORE
DETAIL
IN
SECTION
ANOTHER
APPROACH
STARTS
LIKE
THE
VON
MISES
DISTRIBUTION
FROM
A
GAUSSIAN
DISTRIBUTION
OVER
A
EUCLIDEAN
SPACE
BUT
NOW
MARGINALIZES
ONTO
THE
UNIT
CIRCLE
RATHER
THAN
CONDITIONING
MARDIA
AND
JUPP
HOWEVER
THIS
LEADS
TO
MORE
COMPLEX
FORMS
OF
DISTRIBUTION
AND
WILL
NOT
BE
DISCUSSED
FURTHER
FINALLY
ANY
VALID
DISTRIBUTION
OVER
THE
REAL
AXIS
SUCH
AS
A
GAUSSIAN
CAN
BE
TURNED
INTO
A
PERIODIC
DISTRIBUTION
BY
MAPPING
SUCCES
SIVE
INTERVALS
OF
WIDTH
ONTO
THE
PERIODIC
VARIABLE
WHICH
CORRESPONDS
TO
WRAPPING
THE
REAL
AXIS
AROUND
UNIT
CIRCLE
AGAIN
THE
RESULTING
DISTRIBUTION
IS
MORE
COMPLEX
TO
HANDLE
THAN
THE
VON
MISES
DISTRIBUTION
ONE
LIMITATION
OF
THE
VON
MISES
DISTRIBUTION
IS
THAT
IT
IS
UNIMODAL
BY
FORMING
MIXTURES
OF
VON
MISES
DISTRIBUTIONS
WE
OBTAIN
A
FLEXIBLE
FRAMEWORK
FOR
MODELLING
PERIODIC
VARIABLES
THAT
CAN
HANDLE
MULTIMODALITY
FOR
AN
EXAMPLE
OF
A
MACHINE
LEARN
ING
APPLICATION
THAT
MAKES
USE
OF
VON
MISES
DISTRIBUTIONS
SEE
LAWRENCE
ET
AL
AND
FOR
EXTENSIONS
TO
MODELLING
CONDITIONAL
DENSITIES
FOR
REGRESSION
PROBLEMS
SEE
BISHOP
AND
NABNEY
MIXTURES
OF
GAUSSIANS
WHILE
THE
GAUSSIAN
DISTRIBUTION
HAS
SOME
IMPORTANT
ANALYTICAL
PROPERTIES
IT
SUF
FERS
FROM
SIGNIFICANT
LIMITATIONS
WHEN
IT
COMES
TO
MODELLING
REAL
DATA
SETS
CONSIDER
THE
EXAMPLE
SHOWN
IN
FIGURE
THIS
IS
KNOWN
AS
THE
OLD
FAITHFUL
DATA
SET
AND
COMPRISES
MEASUREMENTS
OF
THE
ERUPTION
OF
THE
OLD
FAITHFUL
GEYSER
AT
YEL
LOWSTONE
NATIONAL
PARK
IN
THE
USA
EACH
MEASUREMENT
COMPRISES
THE
DURATION
OF
FIGURE
EXAMPLE
OF
A
GAUSSIAN
MIXTURE
DISTRIBUTION
IN
ONE
DIMENSION
SHOWING
THREE
GAUSSIANS
EACH
SCALED
BY
A
COEFFICIENT
IN
BLUE
AND
THEIR
SUM
IN
RED
P
X
X
THE
ERUPTION
IN
MINUTES
HORIZONTAL
AXIS
AND
THE
TIME
IN
MINUTES
TO
THE
NEXT
ERUP
TION
VERTICAL
AXIS
WE
SEE
THAT
THE
DATA
SET
FORMS
TWO
DOMINANT
CLUMPS
AND
THAT
A
SIMPLE
GAUSSIAN
DISTRIBUTION
IS
UNABLE
TO
CAPTURE
THIS
STRUCTURE
WHEREAS
A
LINEAR
SUPERPOSITION
OF
TWO
GAUSSIANS
GIVES
A
BETTER
CHARACTERIZATION
OF
THE
DATA
SET
SUCH
SUPERPOSITIONS
FORMED
BY
TAKING
LINEAR
COMBINATIONS
OF
MORE
BASIC
DIS
TRIBUTIONS
SUCH
AS
GAUSSIANS
CAN
BE
FORMULATED
AS
PROBABILISTIC
MODELS
KNOWN
AS
MIXTURE
DISTRIBUTIONS
MCLACHLAN
AND
BASFORD
MCLACHLAN
AND
PEEL
IN
FIGURE
WE
SEE
THAT
A
LINEAR
COMBINATION
OF
GAUSSIANS
CAN
GIVE
RISE
TO
VERY
COMPLEX
DENSITIES
BY
USING
A
SUFFICIENT
NUMBER
OF
GAUSSIANS
AND
BY
ADJUSTING
THEIR
MEANS
AND
COVARIANCES
AS
WELL
AS
THE
COEFFICIENTS
IN
THE
LINEAR
COMBINATION
ALMOST
ANY
CONTINUOUS
DENSITY
CAN
BE
APPROXIMATED
TO
ARBITRARY
ACCURACY
WE
THEREFORE
CONSIDER
A
SUPERPOSITION
OF
K
GAUSSIAN
DENSITIES
OF
THE
FORM
SECTION
K
P
X
ΠKN
X
ΜK
ΣK
K
WHICH
IS
CALLED
A
MIXTURE
OF
GAUSSIANS
EACH
GAUSSIAN
DENSITY
X
ΜK
ΣK
IS
CALLED
A
COMPONENT
OF
THE
MIXTURE
AND
HAS
ITS
OWN
MEAN
ΜK
AND
COVARIANCE
ΣK
CONTOUR
AND
SURFACE
PLOTS
FOR
A
GAUSSIAN
MIXTURE
HAVING
COMPONENTS
ARE
SHOWN
IN
FIGURE
IN
THIS
SECTION
WE
SHALL
CONSIDER
GAUSSIAN
COMPONENTS
TO
ILLUSTRATE
THE
FRAME
WORK
OF
MIXTURE
MODELS
MORE
GENERALLY
MIXTURE
MODELS
CAN
COMPRISE
LINEAR
COM
BINATIONS
OF
OTHER
DISTRIBUTIONS
FOR
INSTANCE
IN
SECTION
WE
SHALL
CONSIDER
MIXTURES
OF
BERNOULLI
DISTRIBUTIONS
AS
AN
EXAMPLE
OF
A
MIXTURE
MODEL
FOR
DISCRETE
VARIABLES
THE
PARAMETERS
ΠK
IN
ARE
CALLED
MIXING
COEFFICIENTS
IF
WE
INTEGRATE
BOTH
SIDES
OF
WITH
RESPECT
TO
X
AND
NOTE
THAT
BOTH
P
X
AND
THE
INDIVIDUAL
GAUSSIAN
COMPONENTS
ARE
NORMALIZED
WE
OBTAIN
K
ΠK
K
ALSO
THE
REQUIREMENT
THAT
P
X
TOGETHER
WITH
X
ΜK
ΣK
IMPLIES
ΠK
FOR
ALL
K
COMBINING
THIS
WITH
THE
CONDITION
WE
OBTAIN
ΠK
FIGURE
ILLUSTRATION
OF
A
MIXTURE
OF
GAUSSIANS
IN
A
TWO
DIMENSIONAL
SPACE
A
CONTOURS
OF
CONSTANT
DENSITY
FOR
EACH
OF
THE
MIXTURE
COMPONENTS
IN
WHICH
THE
COMPONENTS
ARE
DENOTED
RED
BLUE
AND
GREEN
AND
THE
VALUES
OF
THE
MIXING
COEFFICIENTS
ARE
SHOWN
BELOW
EACH
COMPONENT
B
CONTOURS
OF
THE
MARGINAL
PROBABILITY
DENSITY
P
X
OF
THE
MIXTURE
DISTRIBUTION
C
A
SURFACE
PLOT
OF
THE
DISTRIBUTION
P
X
WE
THEREFORE
SEE
THAT
THE
MIXING
COEFFICIENTS
SATISFY
THE
REQUIREMENTS
TO
BE
PROBABIL
ITIES
FROM
THE
SUM
AND
PRODUCT
RULES
THE
MARGINAL
DENSITY
IS
GIVEN
BY
K
P
X
P
K
P
X
K
K
WHICH
IS
EQUIVALENT
TO
IN
WHICH
WE
CAN
VIEW
ΠK
P
K
AS
THE
PRIOR
PROB
ABILITY
OF
PICKING
THE
KTH
COMPONENT
AND
THE
DENSITY
X
ΜK
ΣK
P
X
K
AS
THE
PROBABILITY
OF
X
CONDITIONED
ON
K
AS
WE
SHALL
SEE
IN
LATER
CHAPTERS
AN
IMPOR
TANT
ROLE
IS
PLAYED
BY
THE
POSTERIOR
PROBABILITIES
P
K
X
WHICH
ARE
ALSO
KNOWN
AS
RESPONSIBILITIES
FROM
BAYES
THEOREM
THESE
ARE
GIVEN
BY
ΓK
X
P
K
X
P
K
P
X
K
L
P
L
P
X
L
ΠKN
X
ΜK
ΣK
L
ΠLN
X
ΜL
ΣL
WE
SHALL
DISCUSS
THE
PROBABILISTIC
INTERPRETATION
OF
THE
MIXTURE
DISTRIBUTION
IN
GREATER
DETAIL
IN
CHAPTER
THE
FORM
OF
THE
GAUSSIAN
MIXTURE
DISTRIBUTION
IS
GOVERNED
BY
THE
PARAMETERS
Π
Μ
AND
Σ
WHERE
WE
HAVE
USED
THE
NOTATION
Π
ΠK
Μ
ΜK
AND
Σ
ΣK
ONE
WAY
TO
SET
THE
VALUES
OF
THESE
PARAMETERS
IS
TO
USE
MAXIMUM
LIKELIHOOD
FROM
THE
LOG
OF
THE
LIKELIHOOD
FUNCTION
IS
GIVEN
BY
N
LN
P
X
Π
Μ
Σ
LN
N
K
K
ΠKN
XN
ΜK
ΣK
WHERE
X
XN
WE
IMMEDIATELY
SEE
THAT
THE
SITUATION
IS
NOW
MUCH
MORE
COMPLEX
THAN
WITH
A
SINGLE
GAUSSIAN
DUE
TO
THE
PRESENCE
OF
THE
SUMMATION
OVER
K
INSIDE
THE
LOGARITHM
AS
A
RESULT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
PARAMETERS
NO
LONGER
HAS
A
CLOSED
FORM
ANALYTICAL
SOLUTION
ONE
APPROACH
TO
MAXI
MIZING
THE
LIKELIHOOD
FUNCTION
IS
TO
USE
ITERATIVE
NUMERICAL
OPTIMIZATION
TECHNIQUES
FLETCHER
NOCEDAL
AND
WRIGHT
BISHOP
AND
NABNEY
ALTERNA
TIVELY
WE
CAN
EMPLOY
A
POWERFUL
FRAMEWORK
CALLED
EXPECTATION
MAXIMIZATION
WHICH
WILL
BE
DISCUSSED
AT
LENGTH
IN
CHAPTER
THE
EXPONENTIAL
FAMILY
THE
PROBABILITY
DISTRIBUTIONS
THAT
WE
HAVE
STUDIED
SO
FAR
IN
THIS
CHAPTER
WITH
THE
EXCEPTION
OF
THE
GAUSSIAN
MIXTURE
ARE
SPECIFIC
EXAMPLES
OF
A
BROAD
CLASS
OF
DISTRI
BUTIONS
CALLED
THE
EXPONENTIAL
FAMILY
DUDA
AND
HART
BERNARDO
AND
SMITH
MEMBERS
OF
THE
EXPONENTIAL
FAMILY
HAVE
MANY
IMPORTANT
PROPERTIES
IN
COM
MON
AND
IT
IS
ILLUMINATING
TO
DISCUSS
THESE
PROPERTIES
IN
SOME
GENERALITY
THE
EXPONENTIAL
FAMILY
OF
DISTRIBUTIONS
OVER
X
GIVEN
PARAMETERS
Η
IS
DEFINED
TO
BE
THE
SET
OF
DISTRIBUTIONS
OF
THE
FORM
P
X
Η
H
X
G
Η
EXP
ΗTU
X
WHERE
X
MAY
BE
SCALAR
OR
VECTOR
AND
MAY
BE
DISCRETE
OR
CONTINUOUS
HERE
Η
ARE
CALLED
THE
NATURAL
PARAMETERS
OF
THE
DISTRIBUTION
AND
U
X
IS
SOME
FUNCTION
OF
X
THE
FUNCTION
G
Η
CAN
BE
INTERPRETED
AS
THE
COEFFICIENT
THAT
ENSURES
THAT
THE
DISTRIBU
TION
IS
NORMALIZED
AND
THEREFORE
SATISFIES
G
Η
R
H
X
EXP
ΗTU
X
DX
WHERE
THE
INTEGRATION
IS
REPLACED
BY
SUMMATION
IF
X
IS
A
DISCRETE
VARIABLE
WE
BEGIN
BY
TAKING
SOME
EXAMPLES
OF
THE
DISTRIBUTIONS
INTRODUCED
EARLIER
IN
THE
CHAPTER
AND
SHOWING
THAT
THEY
ARE
INDEED
MEMBERS
OF
THE
EXPONENTIAL
FAMILY
CONSIDER
FIRST
THE
BERNOULLI
DISTRIBUTION
P
X
Μ
BERN
X
Μ
ΜX
Μ
X
EXPRESSING
THE
RIGHT
HAND
SIDE
AS
THE
EXPONENTIAL
OF
THE
LOGARITHM
WE
HAVE
P
X
Μ
EXP
X
LN
Μ
X
LN
Μ
EXP
LN
Μ
Μ
COMPARISON
WITH
ALLOWS
US
TO
IDENTIFY
LN
Μ
Μ
WHICH
WE
CAN
SOLVE
FOR
Μ
TO
GIVE
Μ
Σ
Η
WHERE
Σ
Η
EXP
Η
IS
CALLED
THE
LOGISTIC
SIGMOID
FUNCTION
THUS
WE
CAN
WRITE
THE
BERNOULLI
DISTRIBUTION
USING
THE
STANDARD
REPRESENTATION
IN
THE
FORM
P
X
Η
Σ
Η
EXP
ΗX
WHERE
WE
HAVE
USED
Σ
Η
Σ
Η
WHICH
IS
EASILY
PROVED
FROM
COM
PARISON
WITH
SHOWS
THAT
U
X
X
H
X
G
Η
Σ
Η
NEXT
CONSIDER
THE
MULTINOMIAL
DISTRIBUTION
THAT
FOR
A
SINGLE
OBSERVATION
X
TAKES
THE
FORM
P
X
Μ
TT
ΜXK
EXP
M
XK
LN
ΜK
K
K
WHERE
X
XN
T
AGAIN
WE
CAN
WRITE
THIS
IN
THE
STANDARD
REPRESENTATION
SO
THAT
P
X
Η
EXP
ΗTX
WHERE
ΗK
LN
ΜK
AND
WE
HAVE
DEFINED
Η
ΗM
T
AGAIN
COMPARING
WITH
WE
HAVE
U
X
X
H
X
G
Η
NOTE
THAT
THE
PARAMETERS
ΗK
ARE
NOT
INDEPENDENT
BECAUSE
THE
PARAMETERS
ΜK
ARE
SUB
JECT
TO
THE
CONSTRAINT
M
ΜK
K
SO
THAT
GIVEN
ANY
M
OF
THE
PARAMETERS
ΜK
THE
VALUE
OF
THE
REMAINING
PARAMETER
IS
FIXED
IN
SOME
CIRCUMSTANCES
IT
WILL
BE
CONVENIENT
TO
REMOVE
THIS
CONSTRAINT
BY
EXPRESSING
THE
DISTRIBUTION
IN
TERMS
OF
ONLY
M
PARAMETERS
THIS
CAN
BE
ACHIEVED
BY
USING
THE
RELATIONSHIP
TO
ELIMINATE
ΜM
BY
EXPRESSING
IT
IN
TERMS
OF
THE
REMAINING
ΜK
WHERE
K
M
THEREBY
LEAVING
M
PARAMETERS
NOTE
THAT
THESE
REMAINING
PARAMETERS
ARE
STILL
SUBJECT
TO
THE
CONSTRAINTS
M
ΜK
ΜK
K
MAKING
USE
OF
THE
CONSTRAINT
THE
MULTINOMIAL
DISTRIBUTION
IN
THIS
REPRESENTA
TION
THEN
BECOMES
EXP
M
K
XK
LN
ΜK
M
M
M
EXP
M
XK
LN
ΜK
LN
M
ΜK
WE
NOW
IDENTIFY
K
M
J
K
LN
ΜK
Η
WHICH
WE
CAN
SOLVE
FOR
ΜK
BY
FIRST
SUMMING
BOTH
SIDES
OVER
K
AND
THEN
REARRANGING
AND
BACK
SUBSTITUTING
TO
GIVE
EXP
ΗK
J
EXP
Η
THIS
IS
CALLED
THE
SOFTMAX
FUNCTION
OR
THE
NORMALIZED
EXPONENTIAL
IN
THIS
REPRESEN
TATION
THE
MULTINOMIAL
DISTRIBUTION
THEREFORE
TAKES
THE
FORM
P
X
Η
M
K
EXP
ΗK
EXP
ΗTX
THIS
IS
THE
STANDARD
FORM
OF
THE
EXPONENTIAL
FAMILY
WITH
PARAMETER
VECTOR
Η
ΗM
T
IN
WHICH
U
X
X
H
X
G
Η
M
K
EXP
ΗK
FINALLY
LET
US
CONSIDER
THE
GAUSSIAN
DISTRIBUTION
FOR
THE
UNIVARIATE
GAUSSIAN
WE
HAVE
P
X
Μ
EXP
R
X
Μ
EXP
R
Μ
X
EXERCISE
WHICH
AFTER
SOME
SIMPLE
REARRANGEMENT
CAN
BE
CAST
IN
THE
STANDARD
EXPONENTIAL
FAMILY
FORM
WITH
Μ
Η
U
X
X
H
X
EXP
EXERCISE
MAXIMUM
LIKELIHOOD
AND
SUFFICIENT
STATISTICS
LET
US
NOW
CONSIDER
THE
PROBLEM
OF
ESTIMATING
THE
PARAMETER
VECTOR
Η
IN
THE
GEN
ERAL
EXPONENTIAL
FAMILY
DISTRIBUTION
USING
THE
TECHNIQUE
OF
MAXIMUM
LIKELI
HOOD
TAKING
THE
GRADIENT
OF
BOTH
SIDES
OF
WITH
RESPECT
TO
Η
WE
HAVE
G
Η
R
H
X
EXP
ΗTU
X
DX
G
Η
R
H
X
EXP
ΗTU
X
U
X
DX
REARRANGING
AND
MAKING
USE
AGAIN
OF
THEN
GIVES
X
EXP
TU
X
U
X
DX
U
X
G
Η
WHERE
WE
HAVE
USED
WE
THEREFORE
OBTAIN
THE
RESULT
LN
G
Η
E
U
X
NOTE
THAT
THE
COVARIANCE
OF
U
X
CAN
BE
EXPRESSED
IN
TERMS
OF
THE
SECOND
DERIVATIVES
OF
G
Η
AND
SIMILARLY
FOR
HIGHER
ORDER
MOMENTS
THUS
PROVIDED
WE
CAN
NORMALIZE
A
DISTRIBUTION
FROM
THE
EXPONENTIAL
FAMILY
WE
CAN
ALWAYS
FIND
ITS
MOMENTS
BY
SIMPLE
DIFFERENTIATION
NOW
CONSIDER
A
SET
OF
INDEPENDENT
IDENTICALLY
DISTRIBUTED
DATA
DENOTED
BY
X
XN
FOR
WHICH
THE
LIKELIHOOD
FUNCTION
IS
GIVEN
BY
N
P
X
Η
N
H
XN
G
Η
N
EXP
ΗT
N
U
XN
SETTING
THE
GRADIENT
OF
LN
P
X
Η
WITH
RESPECT
TO
Η
TO
ZERO
WE
GET
THE
FOLLOWING
CONDITION
TO
BE
SATISFIED
BY
THE
MAXIMUM
LIKELIHOOD
ESTIMATOR
ΗML
LN
G
ΗML
N
N
U
XN
N
WHICH
CAN
IN
PRINCIPLE
BE
SOLVED
TO
OBTAIN
ΗML
WE
SEE
THAT
THE
SOLUTION
FOR
THE
MAXIMUM
LIKELIHOOD
ESTIMATOR
DEPENDS
ON
THE
DATA
ONLY
THROUGH
N
U
XN
WHICH
IS
THEREFORE
CALLED
THE
SUFFICIENT
STATISTIC
OF
THE
DISTRIBUTION
WE
DO
NOT
NEED
TO
STORE
THE
ENTIRE
DATA
SET
ITSELF
BUT
ONLY
THE
VALUE
OF
THE
SUFFICIENT
STATISTIC
FOR
THE
BERNOULLI
DISTRIBUTION
FOR
EXAMPLE
THE
FUNCTION
U
X
IS
GIVEN
JUST
BY
X
AND
SO
WE
NEED
ONLY
KEEP
THE
SUM
OF
THE
DATA
POINTS
XN
WHEREAS
FOR
THE
GAUSSIAN
U
X
X
T
AND
SO
WE
SHOULD
KEEP
BOTH
THE
SUM
OF
XN
AND
THE
SUM
OF
IF
WE
CONSIDER
THE
LIMIT
N
THEN
THE
RIGHT
HAND
SIDE
OF
BECOMES
E
U
X
AND
SO
BY
COMPARING
WITH
WE
SEE
THAT
IN
THIS
LIMIT
ΗML
WILL
EQUAL
THE
TRUE
VALUE
Η
IN
FACT
THIS
SUFFICIENCY
PROPERTY
HOLDS
ALSO
FOR
BAYESIAN
INFERENCE
ALTHOUGH
WE
SHALL
DEFER
DISCUSSION
OF
THIS
UNTIL
CHAPTER
WHEN
WE
HAVE
EQUIPPED
OURSELVES
WITH
THE
TOOLS
OF
GRAPHICAL
MODELS
AND
CAN
THEREBY
GAIN
A
DEEPER
INSIGHT
INTO
THESE
IMPORTANT
CONCEPTS
CONJUGATE
PRIORS
WE
HAVE
ALREADY
ENCOUNTERED
THE
CONCEPT
OF
A
CONJUGATE
PRIOR
SEVERAL
TIMES
FOR
EXAMPLE
IN
THE
CONTEXT
OF
THE
BERNOULLI
DISTRIBUTION
FOR
WHICH
THE
CONJUGATE
PRIOR
IS
THE
BETA
DISTRIBUTION
OR
THE
GAUSSIAN
WHERE
THE
CONJUGATE
PRIOR
FOR
THE
MEAN
IS
A
GAUSSIAN
AND
THE
CONJUGATE
PRIOR
FOR
THE
PRECISION
IS
THE
WISHART
DISTRIBUTION
IN
GENERAL
FOR
A
GIVEN
PROBABILITY
DISTRIBUTION
P
X
Η
WE
CAN
SEEK
A
PRIOR
P
Η
THAT
IS
CONJUGATE
TO
THE
LIKELIHOOD
FUNCTION
SO
THAT
THE
POSTERIOR
DISTRIBUTION
HAS
THE
SAME
FUNCTIONAL
FORM
AS
THE
PRIOR
FOR
ANY
MEMBER
OF
THE
EXPONENTIAL
FAMILY
THERE
EXISTS
A
CONJUGATE
PRIOR
THAT
CAN
BE
WRITTEN
IN
THE
FORM
P
Η
Χ
Ν
F
Χ
Ν
G
Η
Ν
EXP
ΝΗTΧ
WHERE
F
Χ
Ν
IS
A
NORMALIZATION
COEFFICIENT
AND
G
Η
IS
THE
SAME
FUNCTION
AS
AP
PEARS
IN
TO
SEE
THAT
THIS
IS
INDEED
CONJUGATE
LET
US
MULTIPLY
THE
PRIOR
BY
THE
LIKELIHOOD
FUNCTION
TO
OBTAIN
THE
POSTERIOR
DISTRIBUTION
UP
TO
A
NOR
MALIZATION
COEFFICIENT
IN
THE
FORM
P
Η
X
Χ
Ν
G
Η
Ν
N
EXP
ΗT
N
N
U
XN
ΝΧ
THIS
AGAIN
TAKES
THE
SAME
FUNCTIONAL
FORM
AS
THE
PRIOR
CONFIRMING
CONJUGACY
FURTHERMORE
WE
SEE
THAT
THE
PARAMETER
Ν
CAN
BE
INTERPRETED
AS
A
EFFECTIVE
NUMBER
OF
PSEUDO
OBSERVATIONS
IN
THE
PRIOR
EACH
OF
WHICH
HAS
A
VALUE
FOR
THE
SUFFICIENT
STATISTIC
U
X
GIVEN
BY
Χ
NONINFORMATIVE
PRIORS
IN
SOME
APPLICATIONS
OF
PROBABILISTIC
INFERENCE
WE
MAY
HAVE
PRIOR
KNOWLEDGE
THAT
CAN
BE
CONVENIENTLY
EXPRESSED
THROUGH
THE
PRIOR
DISTRIBUTION
FOR
EXAMPLE
IF
THE
PRIOR
ASSIGNS
ZERO
PROBABILITY
TO
SOME
VALUE
OF
VARIABLE
THEN
THE
POSTERIOR
DIS
TRIBUTION
WILL
NECESSARILY
ALSO
ASSIGN
ZERO
PROBABILITY
TO
THAT
VALUE
IRRESPECTIVE
OF
ANY
SUBSEQUENT
OBSERVATIONS
OF
DATA
IN
MANY
CASES
HOWEVER
WE
MAY
HAVE
LITTLE
IDEA
OF
WHAT
FORM
THE
DISTRIBUTION
SHOULD
TAKE
WE
MAY
THEN
SEEK
A
FORM
OF
PRIOR
DISTRIBUTION
CALLED
A
NONINFORMATIVE
PRIOR
WHICH
IS
INTENDED
TO
HAVE
AS
LITTLE
INFLU
ENCE
ON
THE
POSTERIOR
DISTRIBUTION
AS
POSSIBLE
JEFFRIES
BOX
AND
TAO
BERNARDO
AND
SMITH
THIS
IS
SOMETIMES
REFERRED
TO
AS
LETTING
THE
DATA
SPEAK
FOR
THEMSELVES
IF
WE
HAVE
A
DISTRIBUTION
P
X
Λ
GOVERNED
BY
A
PARAMETER
Λ
WE
MIGHT
BE
TEMPTED
TO
PROPOSE
A
PRIOR
DISTRIBUTION
P
Λ
CONST
AS
A
SUITABLE
PRIOR
IF
Λ
IS
A
DISCRETE
VARIABLE
WITH
K
STATES
THIS
SIMPLY
AMOUNTS
TO
SETTING
THE
PRIOR
PROBABILITY
OF
EACH
STATE
TO
K
IN
THE
CASE
OF
CONTINUOUS
PARAMETERS
HOWEVER
THERE
ARE
TWO
POTENTIAL
DIFFICULTIES
WITH
THIS
APPROACH
THE
FIRST
IS
THAT
IF
THE
DOMAIN
OF
Λ
IS
UNBOUNDED
THIS
PRIOR
DISTRIBUTION
CANNOT
BE
CORRECTLY
NORMALIZED
BECAUSE
THE
INTEGRAL
OVER
Λ
DIVERGES
SUCH
PRIORS
ARE
CALLED
IMPROPER
IN
PRACTICE
IMPROPER
PRIORS
CAN
OFTEN
BE
USED
PROVIDED
THE
CORRESPONDING
POSTERIOR
DISTRIBUTION
IS
PROPER
I
E
THAT
IT
CAN
BE
CORRECTLY
NORMALIZED
FOR
INSTANCE
IF
WE
PUT
A
UNIFORM
PRIOR
DISTRIBUTION
OVER
THE
MEAN
OF
A
GAUSSIAN
THEN
THE
POSTERIOR
DISTRIBUTION
FOR
THE
MEAN
ONCE
WE
HAVE
OBSERVED
AT
LEAST
ONE
DATA
POINT
WILL
BE
PROPER
A
SECOND
DIFFICULTY
ARISES
FROM
THE
TRANSFORMATION
BEHAVIOUR
OF
A
PROBABILITY
DENSITY
UNDER
A
NONLINEAR
CHANGE
OF
VARIABLES
GIVEN
BY
IF
A
FUNCTION
H
Λ
IS
CONSTANT
AND
WE
CHANGE
VARIABLES
TO
Λ
THEN
H
Η
H
WILL
ALSO
BE
CONSTANT
HOWEVER
IF
WE
CHOOSE
THE
DENSITY
PΛ
Λ
TO
BE
CONSTANT
THEN
THE
DENSITY
OF
Η
WILL
BE
GIVEN
FROM
BY
PΗ
Η
PΛ
DΛ
Λ
DΗ
PΛ
Η
AND
SO
THE
DENSITY
OVER
Η
WILL
NOT
BE
CONSTANT
THIS
ISSUE
DOES
NOT
ARISE
WHEN
WE
USE
MAXIMUM
LIKELIHOOD
BECAUSE
THE
LIKELIHOOD
FUNCTION
P
X
Λ
IS
A
SIMPLE
FUNCTION
OF
Λ
AND
SO
WE
ARE
FREE
TO
USE
ANY
CONVENIENT
PARAMETERIZATION
IF
HOWEVER
WE
ARE
TO
CHOOSE
A
PRIOR
DISTRIBUTION
THAT
IS
CONSTANT
WE
MUST
TAKE
CARE
TO
USE
AN
APPROPRIATE
REPRESENTATION
FOR
THE
PARAMETERS
HERE
WE
CONSIDER
TWO
SIMPLE
EXAMPLES
OF
NONINFORMATIVE
PRIORS
BERGER
FIRST
OF
ALL
IF
A
DENSITY
TAKES
THE
FORM
P
X
Μ
F
X
Μ
THEN
THE
PARAMETER
Μ
IS
KNOWN
AS
A
LOCATION
PARAMETER
THIS
FAMILY
OF
DENSITIES
EXHIBITS
TRANSLATION
INVARIANCE
BECAUSE
IF
WE
SHIFT
X
BY
A
CONSTANT
TO
GIVE
X
X
C
THEN
P
X
Μ
F
X
Μ
WHERE
WE
HAVE
DEFINED
Μ
Μ
C
THUS
THE
DENSITY
TAKES
THE
SAME
FORM
IN
THE
NEW
VARIABLE
AS
IN
THE
ORIGINAL
ONE
AND
SO
THE
DENSITY
IS
INDEPENDENT
OF
THE
CHOICE
OF
ORIGIN
WE
WOULD
LIKE
TO
CHOOSE
A
PRIOR
DISTRIBUTION
THAT
REFLECTS
THIS
TRANSLATION
INVARIANCE
PROPERTY
AND
SO
WE
CHOOSE
A
PRIOR
THAT
ASSIGNS
EQUAL
PROBABILITY
MASS
TO
AN
INTERVAL
A
Μ
B
AS
TO
THE
SHIFTED
INTERVAL
A
C
Μ
B
C
THIS
IMPLIES
B
P
Μ
DΜ
A
B
C
A
C
P
Μ
DΜ
B
P
Μ
C
DΜ
A
AND
BECAUSE
THIS
MUST
HOLD
FOR
ALL
CHOICES
OF
A
AND
B
WE
HAVE
P
Μ
C
P
Μ
WHICH
IMPLIES
THAT
P
Μ
IS
CONSTANT
AN
EXAMPLE
OF
A
LOCATION
PARAMETER
WOULD
BE
THE
MEAN
Μ
OF
A
GAUSSIAN
DISTRIBUTION
AS
WE
HAVE
SEEN
THE
CONJUGATE
PRIOR
DISTRI
BUTION
FOR
Μ
IN
THIS
CASE
IS
A
GAUSSIAN
P
Μ
N
Μ
AND
WE
OBTAIN
A
NONINFORMATIVE
PRIOR
BY
TAKING
THE
LIMIT
INDEED
FROM
AND
WE
SEE
THAT
THIS
GIVES
A
POSTERIOR
DISTRIBUTION
OVER
Μ
IN
WHICH
THE
CONTRIBUTIONS
FROM
THE
PRIOR
VANISH
AS
A
SECOND
EXAMPLE
CONSIDER
A
DENSITY
OF
THE
FORM
P
X
Σ
F
X
EXERCISE
WHERE
Σ
NOTE
THAT
THIS
WILL
BE
A
NORMALIZED
DENSITY
PROVIDED
F
X
IS
CORRECTLY
NORMALIZED
THE
PARAMETER
Σ
IS
KNOWN
AS
A
SCALE
PARAMETER
AND
THE
DENSITY
EXHIBITS
SCALE
INVARIANCE
BECAUSE
IF
WE
SCALE
X
BY
A
CONSTANT
TO
GIVE
X
CX
THEN
P
X
Σ
F
X
Σ
Σ
WHERE
WE
HAVE
DEFINED
Σ
CΣ
THIS
TRANSFORMATION
CORRESPONDS
TO
A
CHANGE
OF
SCALE
FOR
EXAMPLE
FROM
METERS
TO
KILOMETERS
IF
X
IS
A
LENGTH
AND
WE
WOULD
LIKE
TO
CHOOSE
A
PRIOR
DISTRIBUTION
THAT
REFLECTS
THIS
SCALE
INVARIANCE
IF
WE
CONSIDER
AN
INTERVAL
A
Σ
B
AND
A
SCALED
INTERVAL
A
C
Σ
B
C
THEN
THE
PRIOR
SHOULD
ASSIGN
EQUAL
PROBABILITY
MASS
TO
THESE
TWO
INTERVALS
THUS
WE
HAVE
B
P
Σ
DΣ
A
B
C
A
C
B
P
Σ
DΣ
P
Σ
A
C
DΣ
C
AND
BECAUSE
THIS
MUST
HOLD
FOR
CHOICES
OF
A
AND
B
WE
HAVE
P
Σ
P
Σ
AND
HENCE
P
Σ
Σ
NOTE
THAT
AGAIN
THIS
IS
AN
IMPROPER
PRIOR
BECAUSE
THE
INTEGRAL
OF
THE
DISTRIBUTION
OVER
Σ
IS
DIVERGENT
IT
IS
SOMETIMES
ALSO
CONVENIENT
TO
THINK
OF
THE
PRIOR
DISTRIBUTION
FOR
A
SCALE
PARAMETER
IN
TERMS
OF
THE
DENSITY
OF
THE
LOG
OF
THE
PARAMETER
USING
THE
TRANSFORMATION
RULE
FOR
DENSITIES
WE
SEE
THAT
P
LN
Σ
CONST
THUS
FOR
THIS
PRIOR
THERE
IS
THE
SAME
PROBABILITY
MASS
IN
THE
RANGE
Σ
AS
IN
THE
RANGE
Σ
AND
IN
Σ
SECTION
AN
EXAMPLE
OF
A
SCALE
PARAMETER
WOULD
BE
THE
STANDARD
DEVIATION
Σ
OF
A
GAUSSIAN
DISTRIBUTION
AFTER
WE
HAVE
TAKEN
ACCOUNT
OF
THE
LOCATION
PARAMETER
Μ
BECAUSE
N
X
Μ
Σ
EXP
X
Σ
WHERE
X
X
Μ
AS
DISCUSSED
EARLIER
IT
IS
OFTEN
MORE
CONVENIENT
TO
WORK
IN
TERMS
DENSITIES
WE
SEE
THAT
A
DISTRIBUTION
P
Σ
Σ
CORRESPONDS
TO
A
DISTRIBUTION
OVER
Λ
OF
THE
FORM
P
Λ
Λ
WE
HAVE
SEEN
THAT
THE
CONJUGATE
PRIOR
FOR
Λ
WAS
THE
GAMMA
DISTRIBUTION
GAM
Λ
GIVEN
BY
THE
NONINFORMATIVE
PRIOR
IS
OBTAINED
AS
THE
SPECIAL
CASE
AGAIN
IF
WE
EXAMINE
THE
RESULTS
AND
FOR
THE
POSTERIOR
DISTRIBUTION
OF
Λ
WE
SEE
THAT
FOR
THE
POSTERIOR
DEPENDS
ONLY
ON
TERMS
ARISING
FROM
THE
DATA
AND
NOT
FROM
THE
PRIOR
NONPARAMETRIC
METHODS
THROUGHOUT
THIS
CHAPTER
WE
HAVE
FOCUSSED
ON
THE
USE
OF
PROBABILITY
DISTRIBUTIONS
HAVING
SPECIFIC
FUNCTIONAL
FORMS
GOVERNED
BY
A
SMALL
NUMBER
OF
PARAMETERS
WHOSE
VALUES
ARE
TO
BE
DETERMINED
FROM
A
DATA
SET
THIS
IS
CALLED
THE
PARAMETRIC
APPROACH
TO
DENSITY
MODELLING
AN
IMPORTANT
LIMITATION
OF
THIS
APPROACH
IS
THAT
THE
CHOSEN
DENSITY
MIGHT
BE
A
POOR
MODEL
OF
THE
DISTRIBUTION
THAT
GENERATES
THE
DATA
WHICH
CAN
RESULT
IN
POOR
PREDICTIVE
PERFORMANCE
FOR
INSTANCE
IF
THE
PROCESS
THAT
GENERATES
THE
DATA
IS
MULTIMODAL
THEN
THIS
ASPECT
OF
THE
DISTRIBUTION
CAN
NEVER
BE
CAPTURED
BY
A
GAUSSIAN
WHICH
IS
NECESSARILY
UNIMODAL
IN
THIS
FINAL
SECTION
WE
CONSIDER
SOME
NONPARAMETRIC
APPROACHES
TO
DENSITY
ES
TIMATION
THAT
MAKE
FEW
ASSUMPTIONS
ABOUT
THE
FORM
OF
THE
DISTRIBUTION
HERE
WE
SHALL
FOCUS
MAINLY
ON
SIMPLE
FREQUENTIST
METHODS
THE
READER
SHOULD
BE
AWARE
HOWEVER
THAT
NONPARAMETRIC
BAYESIAN
METHODS
ARE
ATTRACTING
INCREASING
INTEREST
WALKER
ET
AL
NEAL
MU
LLER
AND
QUINTANA
TEH
ET
AL
LET
US
START
WITH
A
DISCUSSION
OF
HISTOGRAM
METHODS
FOR
DENSITY
ESTIMATION
WHICH
WE
HAVE
ALREADY
ENCOUNTERED
IN
THE
CONTEXT
OF
MARGINAL
AND
CONDITIONAL
DISTRIBUTIONS
IN
FIGURE
AND
IN
THE
CONTEXT
OF
THE
CENTRAL
LIMIT
THEOREM
IN
FIGURE
HERE
WE
EXPLORE
THE
PROPERTIES
OF
HISTOGRAM
DENSITY
MODELS
IN
MORE
DETAIL
FOCUSSING
ON
THE
CASE
OF
A
SINGLE
CONTINUOUS
VARIABLE
X
STANDARD
HISTOGRAMS
SIMPLY
PARTITION
X
INTO
DISTINCT
BINS
OF
WIDTH
I
AND
THEN
COUNT
THE
NUMBER
NI
OF
OBSERVATIONS
OF
X
FALLING
IN
BIN
I
IN
ORDER
TO
TURN
THIS
COUNT
INTO
A
NORMALIZED
PROBABILITY
DENSITY
WE
SIMPLY
DIVIDE
BY
THE
TOTAL
NUMBER
N
OF
OBSERVATIONS
AND
BY
THE
WIDTH
I
OF
THE
BINS
TO
OBTAIN
PROBABILITY
VALUES
FOR
EACH
BIN
GIVEN
BY
P
NI
I
N
I
FOR
WHICH
IT
IS
EASILY
SEEN
THAT
P
X
DX
THIS
GIVES
A
MODEL
FOR
THE
DENSITY
P
X
THAT
IS
CONSTANT
OVER
THE
WIDTH
OF
EACH
BIN
AND
OFTEN
THE
BINS
ARE
CHOSEN
TO
HAVE
THE
SAME
WIDTH
I
FIGURE
AN
ILLUSTRATION
OF
THE
HISTOGRAM
APPROACH
TO
DENSITY
ESTIMATION
IN
WHICH
A
DATA
SET
OF
DATA
POINTS
IS
GENERATED
FROM
THE
DISTRIBUTION
SHOWN
BY
THE
GREEN
CURVE
HISTOGRAM
DENSITY
ESTIMATES
BASED
ON
WITH
A
COMMON
BIN
WIDTH
ARE
SHOWN
FOR
VARIOUS
VALUES
OF
SECTION
IN
FIGURE
WE
SHOW
AN
EXAMPLE
OF
HISTOGRAM
DENSITY
ESTIMATION
HERE
THE
DATA
IS
DRAWN
FROM
THE
DISTRIBUTION
CORRESPONDING
TO
THE
GREEN
CURVE
WHICH
IS
FORMED
FROM
A
MIXTURE
OF
TWO
GAUSSIANS
ALSO
SHOWN
ARE
THREE
EXAMPLES
OF
HIS
TOGRAM
DENSITY
ESTIMATES
CORRESPONDING
TO
THREE
DIFFERENT
CHOICES
FOR
THE
BIN
WIDTH
WE
SEE
THAT
WHEN
IS
VERY
SMALL
TOP
FIGURE
THE
RESULTING
DENSITY
MODEL
IS
VERY
SPIKY
WITH
A
LOT
OF
STRUCTURE
THAT
IS
NOT
PRESENT
IN
THE
UNDERLYING
DISTRIBUTION
THAT
GENERATED
THE
DATA
SET
CONVERSELY
IF
IS
TOO
LARGE
BOTTOM
FIGURE
THEN
THE
RESULT
IS
A
MODEL
THAT
IS
TOO
SMOOTH
AND
THAT
CONSEQUENTLY
FAILS
TO
CAPTURE
THE
BIMODAL
PROP
ERTY
OF
THE
GREEN
CURVE
THE
BEST
RESULTS
ARE
OBTAINED
FOR
SOME
INTERMEDIATE
VALUE
OF
MIDDLE
FIGURE
IN
PRINCIPLE
A
HISTOGRAM
DENSITY
MODEL
IS
ALSO
DEPENDENT
ON
THE
CHOICE
OF
EDGE
LOCATION
FOR
THE
BINS
THOUGH
THIS
IS
TYPICALLY
MUCH
LESS
SIGNIFICANT
THAN
THE
VALUE
OF
NOTE
THAT
THE
HISTOGRAM
METHOD
HAS
THE
PROPERTY
UNLIKE
THE
METHODS
TO
BE
DIS
CUSSED
SHORTLY
THAT
ONCE
THE
HISTOGRAM
HAS
BEEN
COMPUTED
THE
DATA
SET
ITSELF
CAN
BE
DISCARDED
WHICH
CAN
BE
ADVANTAGEOUS
IF
THE
DATA
SET
IS
LARGE
ALSO
THE
HISTOGRAM
APPROACH
IS
EASILY
APPLIED
IF
THE
DATA
POINTS
ARE
ARRIVING
SEQUENTIALLY
IN
PRACTICE
THE
HISTOGRAM
TECHNIQUE
CAN
BE
USEFUL
FOR
OBTAINING
A
QUICK
VISUAL
IZATION
OF
DATA
IN
ONE
OR
TWO
DIMENSIONS
BUT
IS
UNSUITED
TO
MOST
DENSITY
ESTIMATION
APPLICATIONS
ONE
OBVIOUS
PROBLEM
IS
THAT
THE
ESTIMATED
DENSITY
HAS
DISCONTINUITIES
THAT
ARE
DUE
TO
THE
BIN
EDGES
RATHER
THAN
ANY
PROPERTY
OF
THE
UNDERLYING
DISTRIBUTION
THAT
GENERATED
THE
DATA
ANOTHER
MAJOR
LIMITATION
OF
THE
HISTOGRAM
APPROACH
IS
ITS
SCALING
WITH
DIMENSIONALITY
IF
WE
DIVIDE
EACH
VARIABLE
IN
A
D
DIMENSIONAL
SPACE
INTO
M
BINS
THEN
THE
TOTAL
NUMBER
OF
BINS
WILL
BE
MD
THIS
EXPONENTIAL
SCALING
WITH
D
IS
AN
EXAMPLE
OF
THE
CURSE
OF
DIMENSIONALITY
IN
A
SPACE
OF
HIGH
DIMENSIONAL
ITY
THE
QUANTITY
OF
DATA
NEEDED
TO
PROVIDE
MEANINGFUL
ESTIMATES
OF
LOCAL
PROBABILITY
DENSITY
WOULD
BE
PROHIBITIVE
THE
HISTOGRAM
APPROACH
TO
DENSITY
ESTIMATION
DOES
HOWEVER
TEACH
US
TWO
IM
PORTANT
LESSONS
FIRST
TO
ESTIMATE
THE
PROBABILITY
DENSITY
AT
A
PARTICULAR
LOCATION
WE
SHOULD
CONSIDER
THE
DATA
POINTS
THAT
LIE
WITHIN
SOME
LOCAL
NEIGHBOURHOOD
OF
THAT
POINT
NOTE
THAT
THE
CONCEPT
OF
LOCALITY
REQUIRES
THAT
WE
ASSUME
SOME
FORM
OF
DIS
TANCE
MEASURE
AND
HERE
WE
HAVE
BEEN
ASSUMING
EUCLIDEAN
DISTANCE
FOR
HISTOGRAMS
THIS
NEIGHBOURHOOD
PROPERTY
WAS
DEFINED
BY
THE
BINS
AND
THERE
IS
A
NATURAL
SMOOTH
ING
PARAMETER
DESCRIBING
THE
SPATIAL
EXTENT
OF
THE
LOCAL
REGION
IN
THIS
CASE
THE
BIN
WIDTH
SECOND
THE
VALUE
OF
THE
SMOOTHING
PARAMETER
SHOULD
BE
NEITHER
TOO
LARGE
NOR
TOO
SMALL
IN
ORDER
TO
OBTAIN
GOOD
RESULTS
THIS
IS
REMINISCENT
OF
THE
CHOICE
OF
MODEL
COMPLEXITY
IN
POLYNOMIAL
CURVE
FITTING
DISCUSSED
IN
CHAPTER
WHERE
THE
DEGREE
M
OF
THE
POLYNOMIAL
OR
ALTERNATIVELY
THE
VALUE
Α
OF
THE
REGULARIZATION
PARAMETER
WAS
OPTIMAL
FOR
SOME
INTERMEDIATE
VALUE
NEITHER
TOO
LARGE
NOR
TOO
SMALL
ARMED
WITH
THESE
INSIGHTS
WE
TURN
NOW
TO
A
DISCUSSION
OF
TWO
WIDELY
USED
NONPARAMETRIC
TECH
NIQUES
FOR
DENSITY
ESTIMATION
KERNEL
ESTIMATORS
AND
NEAREST
NEIGHBOURS
WHICH
HAVE
BETTER
SCALING
WITH
DIMENSIONALITY
THAN
THE
SIMPLE
HISTOGRAM
MODEL
KERNEL
DENSITY
ESTIMATORS
LET
US
SUPPOSE
THAT
OBSERVATIONS
ARE
BEING
DRAWN
FROM
SOME
UNKNOWN
PROBABIL
ITY
DENSITY
P
X
IN
SOME
D
DIMENSIONAL
SPACE
WHICH
WE
SHALL
TAKE
TO
BE
EUCLIDEAN
AND
WE
WISH
TO
ESTIMATE
THE
VALUE
OF
P
X
FROM
OUR
EARLIER
DISCUSSION
OF
LOCALITY
LET
US
CONSIDER
SOME
SMALL
REGION
CONTAINING
X
THE
PROBABILITY
MASS
ASSOCIATED
WITH
THIS
REGION
IS
GIVEN
BY
P
P
X
DX
R
SECTION
NOW
SUPPOSE
THAT
WE
HAVE
COLLECTED
A
DATA
SET
COMPRISING
N
OBSERVATIONS
DRAWN
FROM
P
X
BECAUSE
EACH
DATA
POINT
HAS
A
PROBABILITY
P
OF
FALLING
WITHIN
THE
TOTAL
NUMBER
K
OF
POINTS
THAT
LIE
INSIDE
WILL
BE
DISTRIBUTED
ACCORDING
TO
THE
BINOMIAL
DISTRIBUTION
BIN
K
N
P
N
PK
P
K
USING
WE
SEE
THAT
THE
MEAN
FRACTION
OF
POINTS
FALLING
INSIDE
THE
REGION
IS
E
K
N
P
AND
SIMILARLY
USING
WE
SEE
THAT
THE
VARIANCE
AROUND
THIS
MEAN
IS
VAR
K
N
P
P
N
FOR
LARGE
N
THIS
DISTRIBUTION
WILL
BE
SHARPLY
PEAKED
AROUND
THE
MEAN
AND
SO
K
NP
IF
HOWEVER
WE
ALSO
ASSUME
THAT
THE
REGION
IS
SUFFICIENTLY
SMALL
THAT
THE
PROBABILITY
DENSITY
P
X
IS
ROUGHLY
CONSTANT
OVER
THE
REGION
THEN
WE
HAVE
P
P
X
V
WHERE
V
IS
THE
VOLUME
OF
COMBINING
AND
WE
OBTAIN
OUR
DENSITY
ESTIMATE
IN
THE
FORM
P
X
K
NV
NOTE
THAT
THE
VALIDITY
OF
DEPENDS
ON
TWO
CONTRADICTORY
ASSUMPTIONS
NAMELY
THAT
THE
REGION
BE
SUFFICIENTLY
SMALL
THAT
THE
DENSITY
IS
APPROXIMATELY
CONSTANT
OVER
THE
REGION
AND
YET
SUFFICIENTLY
LARGE
IN
RELATION
TO
THE
VALUE
OF
THAT
DENSITY
THAT
THE
NUMBER
K
OF
POINTS
FALLING
INSIDE
THE
REGION
IS
SUFFICIENT
FOR
THE
BINOMIAL
DISTRIBUTION
TO
BE
SHARPLY
PEAKED
WE
CAN
EXPLOIT
THE
RESULT
IN
TWO
DIFFERENT
WAYS
EITHER
WE
CAN
FIX
K
AND
DETERMINE
THE
VALUE
OF
V
FROM
THE
DATA
WHICH
GIVES
RISE
TO
THE
K
NEAREST
NEIGHBOUR
TECHNIQUE
DISCUSSED
SHORTLY
OR
WE
CAN
FIX
V
AND
DETERMINE
K
FROM
THE
DATA
GIV
ING
RISE
TO
THE
KERNEL
APPROACH
IT
CAN
BE
SHOWN
THAT
BOTH
THE
K
NEAREST
NEIGHBOUR
DENSITY
ESTIMATOR
AND
THE
KERNEL
DENSITY
ESTIMATOR
CONVERGE
TO
THE
TRUE
PROBABILITY
DENSITY
IN
THE
LIMIT
N
PROVIDED
V
SHRINKS
SUITABLY
WITH
N
AND
K
GROWS
WITH
N
DUDA
AND
HART
WE
BEGIN
BY
DISCUSSING
THE
KERNEL
METHOD
IN
DETAIL
AND
TO
START
WITH
WE
TAKE
THE
REGION
TO
BE
A
SMALL
HYPERCUBE
CENTRED
ON
THE
POINT
X
AT
WHICH
WE
WISH
TO
DETERMINE
THE
PROBABILITY
DENSITY
IN
ORDER
TO
COUNT
THE
NUMBER
K
OF
POINTS
FALLING
WITHIN
THIS
REGION
IT
IS
CONVENIENT
TO
DEFINE
THE
FOLLOWING
FUNCTION
UI
I
D
OTHERWISE
WHICH
REPRESENTS
A
UNIT
CUBE
CENTRED
ON
THE
ORIGIN
THE
FUNCTION
K
U
IS
AN
EXAMPLE
OF
A
KERNEL
FUNCTION
AND
IN
THIS
CONTEXT
IS
ALSO
CALLED
A
PARZEN
WINDOW
FROM
THE
QUANTITY
K
X
XN
H
WILL
BE
ONE
IF
THE
DATA
POINT
XN
LIES
INSIDE
A
CUBE
OF
SIDE
H
CENTRED
ON
X
AND
ZERO
OTHERWISE
THE
TOTAL
NUMBER
OF
DATA
POINTS
LYING
INSIDE
THIS
CUBE
WILL
THEREFORE
BE
K
K
N
X
XN
SUBSTITUTING
THIS
EXPRESSION
INTO
THEN
GIVES
THE
FOLLOWING
RESULT
FOR
THE
ESTI
MATED
DENSITY
AT
X
P
X
N
N
K
X
XN
HD
WHERE
WE
HAVE
USED
V
HD
FOR
THE
VOLUME
OF
A
HYPERCUBE
OF
SIDE
H
IN
D
DI
MENSIONS
USING
THE
SYMMETRY
OF
THE
FUNCTION
K
U
WE
CAN
NOW
RE
INTERPRET
THIS
EQUATION
NOT
AS
A
SINGLE
CUBE
CENTRED
ON
X
BUT
AS
THE
SUM
OVER
N
CUBES
CENTRED
ON
THE
N
DATA
POINTS
XN
AS
IT
STANDS
THE
KERNEL
DENSITY
ESTIMATOR
WILL
SUFFER
FROM
ONE
OF
THE
SAME
PROBLEMS
THAT
THE
HISTOGRAM
METHOD
SUFFERED
FROM
NAMELY
THE
PRESENCE
OF
ARTIFICIAL
DISCONTINUITIES
IN
THIS
CASE
AT
THE
BOUNDARIES
OF
THE
CUBES
WE
CAN
OBTAIN
A
SMOOTHER
DENSITY
MODEL
IF
WE
CHOOSE
A
SMOOTHER
KERNEL
FUNCTION
AND
A
COMMON
CHOICE
IS
THE
GAUSSIAN
WHICH
GIVES
RISE
TO
THE
FOLLOWING
KERNEL
DENSITY
MODEL
P
X
N
N
EXP
LX
XN
WHERE
H
REPRESENTS
THE
STANDARD
DEVIATION
OF
THE
GAUSSIAN
COMPONENTS
THUS
OUR
DENSITY
MODEL
IS
OBTAINED
BY
PLACING
A
GAUSSIAN
OVER
EACH
DATA
POINT
AND
THEN
ADDING
UP
THE
CONTRIBUTIONS
OVER
THE
WHOLE
DATA
SET
AND
THEN
DIVIDING
BY
N
SO
THAT
THE
DEN
SITY
IS
CORRECTLY
NORMALIZED
IN
FIGURE
WE
APPLY
THE
MODEL
TO
THE
DATA
FIGURE
ILLUSTRATION
OF
THE
KERNEL
DENSITY
MODEL
APPLIED
TO
THE
SAME
DATA
SET
USED
TO
DEMONSTRATE
THE
HISTOGRAM
APPROACH
IN
FIGURE
WE
SEE
THAT
H
ACTS
AS
A
SMOOTHING
PARAMETER
AND
THAT
IF
IT
IS
SET
TOO
SMALL
TOP
PANEL
THE
RESULT
IS
A
VERY
NOISY
DENSITY
MODEL
WHEREAS
IF
IT
IS
SET
TOO
LARGE
BOTTOM
PANEL
THEN
THE
BIMODAL
NATURE
OF
THE
UNDERLYING
DISTRIBUTION
FROM
WHICH
THE
DATA
IS
GENERATED
SHOWN
BY
THE
GREEN
CURVE
IS
WASHED
OUT
THE
BEST
DEN
SITY
MODEL
IS
OBTAINED
FOR
SOME
INTERMEDI
ATE
VALUE
OF
H
MIDDLE
PANEL
SET
USED
EARLIER
TO
DEMONSTRATE
THE
HISTOGRAM
TECHNIQUE
WE
SEE
THAT
AS
EXPECTED
THE
PARAMETER
H
PLAYS
THE
ROLE
OF
A
SMOOTHING
PARAMETER
AND
THERE
IS
A
TRADE
OFF
BETWEEN
SENSITIVITY
TO
NOISE
AT
SMALL
H
AND
OVER
SMOOTHING
AT
LARGE
H
AGAIN
THE
OPTIMIZATION
OF
H
IS
A
PROBLEM
IN
MODEL
COMPLEXITY
ANALOGOUS
TO
THE
CHOICE
OF
BIN
WIDTH
IN
HISTOGRAM
DENSITY
ESTIMATION
OR
THE
DEGREE
OF
THE
POLYNOMIAL
USED
IN
CURVE
FITTING
WE
CAN
CHOOSE
ANY
OTHER
KERNEL
FUNCTION
K
U
IN
SUBJECT
TO
THE
CONDI
TIONS
K
U
R
K
U
DU
WHICH
ENSURE
THAT
THE
RESULTING
PROBABILITY
DISTRIBUTION
IS
NONNEGATIVE
EVERYWHERE
AND
INTEGRATES
TO
ONE
THE
CLASS
OF
DENSITY
MODEL
GIVEN
BY
IS
CALLED
A
KERNEL
DENSITY
ESTIMATOR
OR
PARZEN
ESTIMATOR
IT
HAS
A
GREAT
MERIT
THAT
THERE
IS
NO
COMPU
TATION
INVOLVED
IN
THE
TRAINING
PHASE
BECAUSE
THIS
SIMPLY
REQUIRES
STORAGE
OF
THE
TRAINING
SET
HOWEVER
THIS
IS
ALSO
ONE
OF
ITS
GREAT
WEAKNESSES
BECAUSE
THE
COMPUTA
TIONAL
COST
OF
EVALUATING
THE
DENSITY
GROWS
LINEARLY
WITH
THE
SIZE
OF
THE
DATA
SET
NEAREST
NEIGHBOUR
METHODS
ONE
OF
THE
DIFFICULTIES
WITH
THE
KERNEL
APPROACH
TO
DENSITY
ESTIMATION
IS
THAT
THE
PARAMETER
H
GOVERNING
THE
KERNEL
WIDTH
IS
FIXED
FOR
ALL
KERNELS
IN
REGIONS
OF
HIGH
DATA
DENSITY
A
LARGE
VALUE
OF
H
MAY
LEAD
TO
OVER
SMOOTHING
AND
A
WASHING
OUT
OF
STRUCTURE
THAT
MIGHT
OTHERWISE
BE
EXTRACTED
FROM
THE
DATA
HOWEVER
REDUCING
H
MAY
LEAD
TO
NOISY
ESTIMATES
ELSEWHERE
IN
DATA
SPACE
WHERE
THE
DENSITY
IS
SMALLER
THUS
THE
OPTIMAL
CHOICE
FOR
H
MAY
BE
DEPENDENT
ON
LOCATION
WITHIN
THE
DATA
SPACE
THIS
ISSUE
IS
ADDRESSED
BY
NEAREST
NEIGHBOUR
METHODS
FOR
DENSITY
ESTIMATION
WE
THEREFORE
RETURN
TO
OUR
GENERAL
RESULT
FOR
LOCAL
DENSITY
ESTIMATION
AND
INSTEAD
OF
FIXING
V
AND
DETERMINING
THE
VALUE
OF
K
FROM
THE
DATA
WE
CONSIDER
A
FIXED
VALUE
OF
K
AND
USE
THE
DATA
TO
FIND
AN
APPROPRIATE
VALUE
FOR
V
TO
DO
THIS
WE
CONSIDER
A
SMALL
SPHERE
CENTRED
ON
THE
POINT
X
AT
WHICH
WE
WISH
TO
ESTIMATE
THE
FIGURE
ILLUSTRATION
OF
K
NEAREST
NEIGHBOUR
DEN
SITY
ESTIMATION
USING
THE
SAME
DATA
SET
AS
IN
FIGURES
AND
WE
SEE
THAT
THE
PARAMETER
K
GOVERNS
THE
DEGREE
OF
SMOOTHING
SO
THAT
A
SMALL
VALUE
OF
K
LEADS
TO
A
VERY
NOISY
DENSITY
MODEL
TOP
PANEL
WHEREAS
A
LARGE
VALUE
BOT
TOM
PANEL
SMOOTHES
OUT
THE
BIMODAL
NA
TURE
OF
THE
TRUE
DISTRIBUTION
SHOWN
BY
THE
GREEN
CURVE
FROM
WHICH
THE
DATA
SET
WAS
GENERATED
EXERCISE
DENSITY
P
X
AND
WE
ALLOW
THE
RADIUS
OF
THE
SPHERE
TO
GROW
UNTIL
IT
CONTAINS
PRECISELY
K
DATA
POINTS
THE
ESTIMATE
OF
THE
DENSITY
P
X
IS
THEN
GIVEN
BY
WITH
V
SET
TO
THE
VOLUME
OF
THE
RESULTING
SPHERE
THIS
TECHNIQUE
IS
KNOWN
AS
K
NEAREST
NEIGHBOURS
AND
IS
ILLUSTRATED
IN
FIGURE
FOR
VARIOUS
CHOICES
OF
THE
PARAMETER
K
USING
THE
SAME
DATA
SET
AS
USED
IN
FIGURE
AND
FIGURE
WE
SEE
THAT
THE
VALUE
OF
K
NOW
GOVERNS
THE
DEGREE
OF
SMOOTHING
AND
THAT
AGAIN
THERE
IS
AN
OPTIMUM
CHOICE
FOR
K
THAT
IS
NEITHER
TOO
LARGE
NOR
TOO
SMALL
NOTE
THAT
THE
MODEL
PRODUCED
BY
K
NEAREST
NEIGHBOURS
IS
NOT
A
TRUE
DENSITY
MODEL
BECAUSE
THE
INTEGRAL
OVER
ALL
SPACE
DIVERGES
WE
CLOSE
THIS
CHAPTER
BY
SHOWING
HOW
THE
K
NEAREST
NEIGHBOUR
TECHNIQUE
FOR
DENSITY
ESTIMATION
CAN
BE
EXTENDED
TO
THE
PROBLEM
OF
CLASSIFICATION
TO
DO
THIS
WE
APPLY
THE
K
NEAREST
NEIGHBOUR
DENSITY
ESTIMATION
TECHNIQUE
TO
EACH
CLASS
SEPARATELY
AND
THEN
MAKE
USE
OF
BAYES
THEOREM
LET
US
SUPPOSE
THAT
WE
HAVE
A
DATA
SET
COM
PRISING
NK
POINTS
IN
CLASS
K
WITH
N
POINTS
IN
TOTAL
SO
THAT
K
NK
N
IF
WE
WISH
TO
CLASSIFY
A
NEW
POINT
X
WE
DRAW
A
SPHERE
CENTRED
ON
X
CONTAINING
PRECISELY
K
POINTS
IRRESPECTIVE
OF
THEIR
CLASS
SUPPOSE
THIS
SPHERE
HAS
VOLUME
V
AND
CONTAINS
KK
POINTS
FROM
CLASS
K
THEN
PROVIDES
AN
ESTIMATE
OF
THE
DENSITY
ASSOCIATED
WITH
EACH
CLASS
P
X
CK
KK
NKV
SIMILARLY
THE
UNCONDITIONAL
DENSITY
IS
GIVEN
BY
P
X
NV
WHILE
THE
CLASS
PRIORS
ARE
GIVEN
BY
P
CK
NK
N
WE
CAN
NOW
COMBINE
AND
USING
BAYES
THEOREM
TO
OBTAIN
THE
POSTERIOR
PROBABILITY
OF
CLASS
MEMBERSHIP
P
C
X
P
X
CK
P
CK
KK
FIGURE
A
IN
THE
K
NEAREST
NEIGHBOUR
CLASSIFIER
A
NEW
POINT
SHOWN
BY
THE
BLACK
DIAMOND
IS
CLAS
SIFIED
ACCORDING
TO
THE
MAJORITY
CLASS
MEMBERSHIP
OF
THE
K
CLOSEST
TRAIN
ING
DATA
POINTS
IN
THIS
CASE
K
B
IN
THE
NEAREST
NEIGHBOUR
K
APPROACH
TO
CLASSIFICATION
THE
RESULTING
DECISION
BOUNDARY
IS
COMPOSED
OF
HYPERPLANES
THAT
FORM
PERPENDICULAR
BISECTORS
OF
PAIRS
OF
POINTS
FROM
DIFFERENT
CLASSES
A
B
IF
WE
WISH
TO
MINIMIZE
THE
PROBABILITY
OF
MISCLASSIFICATION
THIS
IS
DONE
BY
ASSIGNING
THE
TEST
POINT
X
TO
THE
CLASS
HAVING
THE
LARGEST
POSTERIOR
PROBABILITY
CORRESPONDING
TO
THE
LARGEST
VALUE
OF
KK
K
THUS
TO
CLASSIFY
A
NEW
POINT
WE
IDENTIFY
THE
K
NEAREST
POINTS
FROM
THE
TRAINING
DATA
SET
AND
THEN
ASSIGN
THE
NEW
POINT
TO
THE
CLASS
HAVING
THE
LARGEST
NUMBER
OF
REPRESENTATIVES
AMONGST
THIS
SET
TIES
CAN
BE
BROKEN
AT
RANDOM
THE
PARTICULAR
CASE
OF
K
IS
CALLED
THE
NEAREST
NEIGHBOUR
RULE
BECAUSE
A
TEST
POINT
IS
SIMPLY
ASSIGNED
TO
THE
SAME
CLASS
AS
THE
NEAREST
POINT
FROM
THE
TRAINING
SET
THESE
CONCEPTS
ARE
ILLUSTRATED
IN
FIGURE
IN
FIGURE
WE
SHOW
THE
RESULTS
OF
APPLYING
THE
K
NEAREST
NEIGHBOUR
ALGO
RITHM
TO
THE
OIL
FLOW
DATA
INTRODUCED
IN
CHAPTER
FOR
VARIOUS
VALUES
OF
K
AS
EXPECTED
WE
SEE
THAT
K
CONTROLS
THE
DEGREE
OF
SMOOTHING
SO
THAT
SMALL
K
PRODUCES
MANY
SMALL
REGIONS
OF
EACH
CLASS
WHEREAS
LARGE
K
LEADS
TO
FEWER
LARGER
REGIONS
K
K
K
FIGURE
PLOT
OF
DATA
POINTS
FROM
THE
OIL
DATA
SET
SHOWING
VALUES
OF
PLOTTED
AGAINST
WHERE
THE
RED
GREEN
AND
BLUE
POINTS
CORRESPOND
TO
THE
LAMINAR
ANNULAR
AND
HOMOGENEOUS
CLASSES
RESPECTIVELY
ALSO
SHOWN
ARE
THE
CLASSIFICATIONS
OF
THE
INPUT
SPACE
GIVEN
BY
THE
K
NEAREST
NEIGHBOUR
ALGORITHM
FOR
VARIOUS
VALUES
OF
K
AN
INTERESTING
PROPERTY
OF
THE
NEAREST
NEIGHBOUR
K
CLASSIFIER
IS
THAT
IN
THE
LIMIT
N
THE
ERROR
RATE
IS
NEVER
MORE
THAN
TWICE
THE
MINIMUM
ACHIEVABLE
ERROR
RATE
OF
AN
OPTIMAL
CLASSIFIER
I
E
ONE
THAT
USES
THE
TRUE
CLASS
DISTRIBUTIONS
COVER
AND
HART
AS
DISCUSSED
SO
FAR
BOTH
THE
K
NEAREST
NEIGHBOUR
METHOD
AND
THE
KERNEL
DEN
SITY
ESTIMATOR
REQUIRE
THE
ENTIRE
TRAINING
DATA
SET
TO
BE
STORED
LEADING
TO
EXPENSIVE
COMPUTATION
IF
THE
DATA
SET
IS
LARGE
THIS
EFFECT
CAN
BE
OFFSET
AT
THE
EXPENSE
OF
SOME
ADDITIONAL
ONE
OFF
COMPUTATION
BY
CONSTRUCTING
TREE
BASED
SEARCH
STRUCTURES
TO
ALLOW
APPROXIMATE
NEAR
NEIGHBOURS
TO
BE
FOUND
EFFICIENTLY
WITHOUT
DOING
AN
EXHAUSTIVE
SEARCH
OF
THE
DATA
SET
NEVERTHELESS
THESE
NONPARAMETRIC
METHODS
ARE
STILL
SEVERELY
LIMITED
ON
THE
OTHER
HAND
WE
HAVE
SEEN
THAT
SIMPLE
PARAMETRIC
MODELS
ARE
VERY
RESTRICTED
IN
TERMS
OF
THE
FORMS
OF
DISTRIBUTION
THAT
THEY
CAN
REPRESENT
WE
THEREFORE
NEED
TO
FIND
DENSITY
MODELS
THAT
ARE
VERY
FLEXIBLE
AND
YET
FOR
WHICH
THE
COMPLEXITY
OF
THE
MODELS
CAN
BE
CONTROLLED
INDEPENDENTLY
OF
THE
SIZE
OF
THE
TRAINING
SET
AND
WE
SHALL
SEE
IN
SUBSEQUENT
CHAPTERS
HOW
TO
ACHIEVE
THIS
EXERCISES
WWW
VERIFY
THAT
THE
BERNOULLI
DISTRIBUTION
SATISFIES
THE
FOLLOWING
PROP
ERTIES
P
X
Μ
X
E
X
Μ
VAR
X
Μ
Μ
SHOW
THAT
THE
ENTROPY
H
X
OF
A
BERNOULLI
DISTRIBUTED
RANDOM
BINARY
VARIABLE
X
IS
GIVEN
BY
H
X
Μ
LN
Μ
Μ
LN
Μ
THE
FORM
OF
THE
BERNOULLI
DISTRIBUTION
GIVEN
BY
IS
NOT
SYMMETRIC
BE
TWEEN
THE
TWO
VALUES
OF
X
IN
SOME
SITUATIONS
IT
WILL
BE
MORE
CONVENIENT
TO
USE
AN
EQUIVALENT
FORMULATION
FOR
WHICH
X
IN
WHICH
CASE
THE
DISTRIBUTION
CAN
BE
WRITTEN
Μ
X
Μ
X
WHERE
Μ
SHOW
THAT
THE
DISTRIBUTION
IS
NORMALIZED
AND
EVALUATE
ITS
MEAN
VARIANCE
AND
ENTROPY
WWW
IN
THIS
EXERCISE
WE
PROVE
THAT
THE
BINOMIAL
DISTRIBUTION
IS
NOR
MALIZED
FIRST
USE
THE
DEFINITION
OF
THE
NUMBER
OF
COMBINATIONS
OF
M
IDENTICAL
OBJECTS
CHOSEN
FROM
A
TOTAL
OF
N
TO
SHOW
THAT
N
N
N
M
M
M
USE
THIS
RESULT
TO
PROVE
BY
INDUCTION
THE
FOLLOWING
RESULT
X
N
M
N
XM
WHICH
IS
KNOWN
AS
THE
BINOMIAL
THEOREM
AND
WHICH
IS
VALID
FOR
ALL
REAL
VALUES
OF
X
FINALLY
SHOW
THAT
THE
BINOMIAL
DISTRIBUTION
IS
NORMALIZED
SO
THAT
N
ΜM
Μ
N
M
M
M
WHICH
CAN
BE
DONE
BY
FIRST
PULLING
OUT
A
FACTOR
Μ
N
OUT
OF
THE
SUMMATION
AND
THEN
MAKING
USE
OF
THE
BINOMIAL
THEOREM
SHOW
THAT
THE
MEAN
OF
THE
BINOMIAL
DISTRIBUTION
IS
GIVEN
BY
TO
DO
THIS
DIFFERENTIATE
BOTH
SIDES
OF
THE
NORMALIZATION
CONDITION
WITH
RESPECT
TO
Μ
AND
THEN
REARRANGE
TO
OBTAIN
AN
EXPRESSION
FOR
THE
MEAN
OF
N
SIMILARLY
BY
DIFFERENTIATING
TWICE
WITH
RESPECT
TO
Μ
AND
MAKING
USE
OF
THE
RESULT
FOR
THE
MEAN
OF
THE
BINOMIAL
DISTRIBUTION
PROVE
THE
RESULT
FOR
THE
VARIANCE
OF
THE
BINOMIAL
WWW
IN
THIS
EXERCISE
WE
PROVE
THAT
THE
BETA
DISTRIBUTION
GIVEN
BY
IS
CORRECTLY
NORMALIZED
SO
THAT
HOLDS
THIS
IS
EQUIVALENT
TO
SHOWING
THAT
R
Γ
A
Γ
B
FROM
THE
DEFINITION
OF
THE
GAMMA
FUNCTION
WE
HAVE
Γ
A
Γ
B
R
EXP
X
XA
DX
R
EXP
Y
YB
DY
USE
THIS
EXPRESSION
TO
PROVE
AS
FOLLOWS
FIRST
BRING
THE
INTEGRAL
OVER
Y
INSIDE
THE
INTEGRAND
OF
THE
INTEGRAL
OVER
X
NEXT
MAKE
THE
CHANGE
OF
VARIABLE
T
Y
X
WHERE
X
IS
FIXED
THEN
INTERCHANGE
THE
ORDER
OF
THE
X
AND
T
INTEGRATIONS
AND
FINALLY
MAKE
THE
CHANGE
OF
VARIABLE
X
TΜ
WHERE
T
IS
FIXED
MAKE
USE
OF
THE
RESULT
TO
SHOW
THAT
THE
MEAN
VARIANCE
AND
MODE
OF
THE
BETA
DISTRIBUTION
ARE
GIVEN
RESPECTIVELY
BY
E
Μ
A
A
B
VAR
Μ
AB
A
B
A
B
MODE
Μ
A
A
B
CONSIDER
A
BINOMIAL
RANDOM
VARIABLE
X
GIVEN
BY
WITH
PRIOR
DISTRIBUTION
FOR
Μ
GIVEN
BY
THE
BETA
DISTRIBUTION
AND
SUPPOSE
WE
HAVE
OBSERVED
M
OCCUR
RENCES
OF
X
AND
L
OCCURRENCES
OF
X
SHOW
THAT
THE
POSTERIOR
MEAN
VALUE
OF
X
LIES
BETWEEN
THE
PRIOR
MEAN
AND
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
FOR
Μ
TO
DO
THIS
SHOW
THAT
THE
POSTERIOR
MEAN
CAN
BE
WRITTEN
AS
Λ
TIMES
THE
PRIOR
MEAN
PLUS
Λ
TIMES
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
WHERE
Λ
THIS
ILLUSTRATES
THE
CON
CEPT
OF
THE
POSTERIOR
DISTRIBUTION
BEING
A
COMPROMISE
BETWEEN
THE
PRIOR
DISTRIBUTION
AND
THE
MAXIMUM
LIKELIHOOD
SOLUTION
CONSIDER
TWO
VARIABLES
X
AND
Y
WITH
JOINT
DISTRIBUTION
P
X
Y
PROVE
THE
FOLLOW
ING
TWO
RESULTS
E
X
EY
EX
X
Y
VAR
X
EY
VARX
X
Y
VARY
EX
X
Y
HERE
EX
X
Y
DENOTES
THE
EXPECTATION
OF
X
UNDER
THE
CONDITIONAL
DISTRIBUTION
P
X
Y
WITH
A
SIMILAR
NOTATION
FOR
THE
CONDITIONAL
VARIANCE
WWW
IN
THIS
EXERCISE
WE
PROVE
THE
NORMALIZATION
OF
THE
DIRICHLET
DIS
TRIBUTION
USING
INDUCTION
WE
HAVE
ALREADY
SHOWN
IN
EXERCISE
THAT
THE
BETA
DISTRIBUTION
WHICH
IS
A
SPECIAL
CASE
OF
THE
DIRICHLET
FOR
M
IS
NORMALIZED
WE
NOW
ASSUME
THAT
THE
DIRICHLET
DISTRIBUTION
IS
NORMALIZED
FOR
M
VARIABLES
AND
PROVE
THAT
IT
IS
NORMALIZED
FOR
M
VARIABLES
TO
DO
THIS
CONSIDER
THE
DIRICHLET
DISTRIBUTION
OVER
M
VARIABLES
AND
TAKE
ACCOUNT
OF
THE
CONSTRAINT
M
ΜK
BY
ELIMINATING
ΜM
SO
THAT
THE
DIRICHLET
IS
WRITTEN
M
M
ΑM
AND
OUR
GOAL
IS
TO
FIND
AN
EXPRESSION
FOR
CM
TO
DO
THIS
INTEGRATE
OVER
ΜM
TAKING
CARE
OVER
THE
LIMITS
OF
INTEGRATION
AND
THEN
MAKE
A
CHANGE
OF
VARIABLE
SO
THAT
THIS
INTEGRAL
HAS
LIMITS
AND
BY
ASSUMING
THE
CORRECT
RESULT
FOR
CM
AND
MAKING
USE
OF
DERIVE
THE
EXPRESSION
FOR
CM
USING
THE
PROPERTY
Γ
X
XΓ
X
OF
THE
GAMMA
FUNCTION
DERIVE
THE
FOLLOWING
RESULTS
FOR
THE
MEAN
VARIANCE
AND
COVARIANCE
OF
THE
DIRICHLET
DISTRIBUTION
GIVEN
BY
E
Μ
ΑJ
VAR
Μ
ΑJ
ΑJ
J
Α
COV
Μ
Μ
ΑJΑL
J
L
WHERE
IS
DEFINED
BY
WWW
BY
EXPRESSING
THE
EXPECTATION
OF
LN
ΜJ
UNDER
THE
DIRICHLET
DISTRIBUTION
AS
A
DERIVATIVE
WITH
RESPECT
TO
ΑJ
SHOW
THAT
E
LN
ΜJ
Ψ
ΑJ
Ψ
WHERE
IS
GIVEN
BY
AND
IS
THE
DIGAMMA
FUNCTION
Ψ
A
DA
LN
Γ
A
THE
UNIFORM
DISTRIBUTION
FOR
A
CONTINUOUS
VARIABLE
X
IS
DEFINED
BY
U
X
A
B
A
X
B
B
A
VERIFY
THAT
THIS
DISTRIBUTION
IS
NORMALIZED
AND
FIND
EXPRESSIONS
FOR
ITS
MEAN
AND
VARIANCE
EVALUATE
THE
KULLBACK
LEIBLER
DIVERGENCE
BETWEEN
TWO
GAUSSIANS
P
X
N
X
Μ
Σ
AND
Q
X
N
X
M
L
WWW
THIS
EXERCISE
DEMONSTRATES
THAT
THE
MULTIVARIATE
DISTRIBUTION
WITH
MAX
IMUM
ENTROPY
FOR
A
GIVEN
COVARIANCE
IS
A
GAUSSIAN
THE
ENTROPY
OF
A
DISTRIBUTION
P
X
IS
GIVEN
BY
H
X
R
P
X
LN
P
X
DX
WE
WISH
TO
MAXIMIZE
H
X
OVER
ALL
DISTRIBUTIONS
P
X
SUBJECT
TO
THE
CONSTRAINTS
THAT
P
X
BE
NORMALIZED
AND
THAT
IT
HAVE
A
SPECIFIC
MEAN
AND
COVARIANCE
SO
THAT
R
P
X
DX
R
P
X
X
DX
Μ
R
P
X
X
Μ
X
Μ
T
DX
Σ
BY
PERFORMING
A
VARIATIONAL
MAXIMIZATION
OF
AND
USING
LAGRANGE
MULTIPLIERS
TO
ENFORCE
THE
CONSTRAINTS
AND
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
DISTRIBUTION
IS
GIVEN
BY
THE
GAUSSIAN
SHOW
THAT
THE
ENTROPY
OF
THE
MULTIVARIATE
GAUSSIAN
N
X
Μ
Σ
IS
GIVEN
BY
H
X
LN
Σ
D
LN
WHERE
D
IS
THE
DIMENSIONALITY
OF
X
WWW
CONSIDER
TWO
RANDOM
VARIABLES
AND
HAVING
GAUSSIAN
DISTRI
BUTIONS
WITH
MEANS
AND
PRECISIONS
RESPECTIVELY
DERIVE
AN
EXPRESSION
FOR
THE
DIFFERENTIAL
ENTROPY
OF
THE
VARIABLE
X
TO
DO
THIS
FIRST
FIND
THE
DISTRIBUTION
OF
X
BY
USING
THE
RELATION
P
X
R
P
X
X
P
X
DX
AND
COMPLETING
THE
SQUARE
IN
THE
EXPONENT
THEN
OBSERVE
THAT
THIS
REPRESENTS
THE
CONVOLUTION
OF
TWO
GAUSSIAN
DISTRIBUTIONS
WHICH
ITSELF
WILL
BE
GAUSSIAN
AND
FINALLY
MAKE
USE
OF
THE
RESULT
FOR
THE
ENTROPY
OF
THE
UNIVARIATE
GAUSSIAN
WWW
CONSIDER
THE
MULTIVARIATE
GAUSSIAN
DISTRIBUTION
GIVEN
BY
BY
WRITING
THE
PRECISION
MATRIX
INVERSE
COVARIANCE
MATRIX
Σ
AS
THE
SUM
OF
A
SYM
METRIC
AND
AN
ANTI
SYMMETRIC
MATRIX
SHOW
THAT
THE
ANTI
SYMMETRIC
TERM
DOES
NOT
APPEAR
IN
THE
EXPONENT
OF
THE
GAUSSIAN
AND
HENCE
THAT
THE
PRECISION
MATRIX
MAY
BE
TAKEN
TO
BE
SYMMETRIC
WITHOUT
LOSS
OF
GENERALITY
BECAUSE
THE
INVERSE
OF
A
SYMMETRIC
MATRIX
IS
ALSO
SYMMETRIC
SEE
EXERCISE
IT
FOLLOWS
THAT
THE
COVARIANCE
MATRIX
MAY
ALSO
BE
CHOSEN
TO
BE
SYMMETRIC
WITHOUT
LOSS
OF
GENERALITY
CONSIDER
A
REAL
SYMMETRIC
MATRIX
Σ
WHOSE
EIGENVALUE
EQUATION
IS
GIVEN
BY
BY
TAKING
THE
COMPLEX
CONJUGATE
OF
THIS
EQUATION
AND
SUBTRACTING
THE
ORIGINAL
EQUATION
AND
THEN
FORMING
THE
INNER
PRODUCT
WITH
EIGENVECTOR
UI
SHOW
THAT
THE
EIGENVALUES
ΛI
ARE
REAL
SIMILARLY
USE
THE
SYMMETRY
PROPERTY
OF
Σ
TO
SHOW
THAT
TWO
EIGENVECTORS
UI
AND
UJ
WILL
BE
ORTHOGONAL
PROVIDED
ΛJ
ΛI
FINALLY
SHOW
THAT
WITHOUT
LOSS
OF
GENERALITY
THE
SET
OF
EIGENVECTORS
CAN
BE
CHOSEN
TO
BE
ORTHONORMAL
SO
THAT
THEY
SATISFY
EVEN
IF
SOME
OF
THE
EIGENVALUES
ARE
ZERO
SHOW
THAT
A
REAL
SYMMETRIC
MATRIX
Σ
HAVING
THE
EIGENVECTOR
EQUATION
CAN
BE
EXPRESSED
AS
AN
EXPANSION
IN
THE
EIGENVECTORS
WITH
COEFFICIENTS
GIVEN
BY
THE
EIGENVALUES
OF
THE
FORM
SIMILARLY
SHOW
THAT
THE
INVERSE
MATRIX
Σ
HAS
A
REPRESENTATION
OF
THE
FORM
WWW
A
POSITIVE
DEFINITE
MATRIX
Σ
CAN
BE
DEFINED
AS
ONE
FOR
WHICH
THE
QUADRATIC
FORM
ATΣA
IS
POSITIVE
FOR
ANY
REAL
VALUE
OF
THE
VECTOR
A
SHOW
THAT
A
NECESSARY
AND
SUFFICIENT
CONDITION
FOR
Σ
TO
BE
POSITIVE
DEFINITE
IS
THAT
ALL
OF
THE
EIGENVALUES
ΛI
OF
Σ
DEFINED
BY
ARE
POSITIVE
SHOW
THAT
A
REAL
SYMMETRIC
MATRIX
OF
SIZE
D
D
HAS
D
D
INDEPENDENT
PARAMETERS
WWW
SHOW
THAT
THE
INVERSE
OF
A
SYMMETRIC
MATRIX
IS
ITSELF
SYMMETRIC
BY
DIAGONALIZING
THE
COORDINATE
SYSTEM
USING
THE
EIGENVECTOR
EXPANSION
SHOW
THAT
THE
VOLUME
CONTAINED
WITHIN
THE
HYPERELLIPSOID
CORRESPONDING
TO
A
CONSTANT
MAHALANOBIS
DISTANCE
IS
GIVEN
BY
VD
Σ
D
WHERE
VD
IS
THE
VOLUME
OF
THE
UNIT
SPHERE
IN
D
DIMENSIONS
AND
THE
MAHALANOBIS
DISTANCE
IS
DEFINED
BY
WWW
PROVE
THE
IDENTITY
BY
MULTIPLYING
BOTH
SIDES
BY
THE
MATRIX
A
B
AND
MAKING
USE
OF
THE
DEFINITION
IN
SECTIONS
AND
WE
CONSIDERED
THE
CONDITIONAL
AND
MARGINAL
DISTRI
BUTIONS
FOR
A
MULTIVARIATE
GAUSSIAN
MORE
GENERALLY
WE
CAN
CONSIDER
A
PARTITIONING
OF
THE
COMPONENTS
OF
X
INTO
THREE
GROUPS
XA
XB
AND
XC
WITH
A
CORRESPONDING
PAR
TITIONING
OF
THE
MEAN
VECTOR
Μ
AND
OF
THE
COVARIANCE
MATRIX
Σ
IN
THE
FORM
ΜA
Μ
ΜB
ΜC
Σ
ΣAA
ΣAB
ΣAC
ΣBA
ΣBB
ΣBC
ΣCA
ΣCB
ΣCC
BY
MAKING
USE
OF
THE
RESULTS
OF
SECTION
FIND
AN
EXPRESSION
FOR
THE
CONDITIONAL
DISTRIBUTION
P
XA
XB
IN
WHICH
XC
HAS
BEEN
MARGINALIZED
OUT
A
VERY
USEFUL
RESULT
FROM
LINEAR
ALGEBRA
IS
THE
WOODBURY
MATRIX
INVERSION
FORMULA
GIVEN
BY
A
BCD
A
A
C
DA
BY
MULTIPLYING
BOTH
SIDES
BY
A
BCD
PROVE
THE
CORRECTNESS
OF
THIS
RESULT
LET
X
AND
Z
BE
TWO
INDEPENDENT
RANDOM
VECTORS
SO
THAT
P
X
Z
P
X
P
Z
SHOW
THAT
THE
MEAN
OF
THEIR
SUM
Y
X
Z
IS
GIVEN
BY
THE
SUM
OF
THE
MEANS
OF
EACH
OF
THE
VARIABLE
SEPARATELY
SIMILARLY
SHOW
THAT
THE
COVARIANCE
MATRIX
OF
Y
IS
GIVEN
BY
THE
SUM
OF
THE
COVARIANCE
MATRICES
OF
X
AND
Z
CONFIRM
THAT
THIS
RESULT
AGREES
WITH
THAT
OF
EXERCISE
WWW
CONSIDER
A
JOINT
DISTRIBUTION
OVER
THE
VARIABLE
Z
X
WHOSE
MEAN
AND
COVARIANCE
ARE
GIVEN
BY
AND
RESPECTIVELY
BY
MAK
ING
USE
OF
THE
RESULTS
AND
SHOW
THAT
THE
MARGINAL
DISTRIBUTION
P
X
IS
GIVEN
SIMILARLY
BY
MAKING
USE
OF
THE
RESULTS
AND
SHOW
THAT
THE
CONDITIONAL
DISTRIBUTION
P
Y
X
IS
GIVEN
BY
USING
THE
PARTITIONED
MATRIX
INVERSION
FORMULA
SHOW
THAT
THE
INVERSE
OF
THE
PRECISION
MATRIX
IS
GIVEN
BY
THE
COVARIANCE
MATRIX
BY
STARTING
FROM
AND
MAKING
USE
OF
THE
RESULT
VERIFY
THE
RESULT
CONSIDER
TWO
MULTIDIMENSIONAL
RANDOM
VECTORS
X
AND
Z
HAVING
GAUSSIAN
DISTRIBUTIONS
P
X
X
ΜX
ΣX
AND
P
Z
Z
ΜZ
ΣZ
RESPECTIVELY
TOGETHER
WITH
THEIR
SUM
Y
X
Z
USE
THE
RESULTS
AND
TO
FIND
AN
EXPRESSION
FOR
THE
MARGINAL
DISTRIBUTION
P
Y
BY
CONSIDERING
THE
LINEAR
GAUSSIAN
MODEL
COMPRISING
THE
PRODUCT
OF
THE
MARGINAL
DISTRIBUTION
P
X
AND
THE
CONDITIONAL
DISTRIBUTION
P
Y
X
WWW
THIS
EXERCISE
AND
THE
NEXT
PROVIDE
PRACTICE
AT
MANIPULATING
THE
QUADRATIC
FORMS
THAT
ARISE
IN
LINEAR
GAUSSIAN
MODELS
AS
WELL
AS
GIVING
AN
INDEPEN
DENT
CHECK
OF
RESULTS
DERIVED
IN
THE
MAIN
TEXT
CONSIDER
A
JOINT
DISTRIBUTION
P
X
Y
DEFINED
BY
THE
MARGINAL
AND
CONDITIONAL
DISTRIBUTIONS
GIVEN
BY
AND
BY
EXAMINING
THE
QUADRATIC
FORM
IN
THE
EXPONENT
OF
THE
JOINT
DISTRIBUTION
AND
USING
THE
TECHNIQUE
OF
COMPLETING
THE
SQUARE
DISCUSSED
IN
SECTION
FIND
EXPRESSIONS
FOR
THE
MEAN
AND
COVARIANCE
OF
THE
MARGINAL
DISTRIBUTION
P
Y
IN
WHICH
THE
VARIABLE
X
HAS
BEEN
INTEGRATED
OUT
TO
DO
THIS
MAKE
USE
OF
THE
WOODBURY
MATRIX
INVERSION
FORMULA
VERIFY
THAT
THESE
RESULTS
AGREE
WITH
AND
OBTAINED
USING
THE
RESULTS
OF
CHAPTER
CONSIDER
THE
SAME
JOINT
DISTRIBUTION
AS
IN
EXERCISE
BUT
NOW
USE
THE
TECHNIQUE
OF
COMPLETING
THE
SQUARE
TO
FIND
EXPRESSIONS
FOR
THE
MEAN
AND
COVARIANCE
OF
THE
CONDITIONAL
DISTRIBUTION
P
X
Y
AGAIN
VERIFY
THAT
THESE
AGREE
WITH
THE
CORRE
SPONDING
EXPRESSIONS
AND
WWW
TO
FIND
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
COVARIANCE
MATRIX
OF
A
MULTIVARIATE
GAUSSIAN
WE
NEED
TO
MAXIMIZE
THE
LOG
LIKELIHOOD
FUNCTION
WITH
RESPECT
TO
Σ
NOTING
THAT
THE
COVARIANCE
MATRIX
MUST
BE
SYMMETRIC
AND
POSITIVE
DEFINITE
HERE
WE
PROCEED
BY
IGNORING
THESE
CONSTRAINTS
AND
DOING
A
STRAIGHTFORWARD
MAXIMIZATION
USING
THE
RESULTS
C
C
AND
C
FROM
APPENDIX
C
SHOW
THAT
THE
COVARIANCE
MATRIX
Σ
THAT
MAXIMIZES
THE
LOG
LIKELIHOOD
FUNCTION
IS
GIVEN
BY
THE
SAMPLE
COVARIANCE
WE
NOTE
THAT
THE
FINAL
RESULT
IS
NECESSARILY
SYMMETRIC
AND
POSITIVE
DEFINITE
PROVIDED
THE
SAMPLE
COVARIANCE
IS
NONSINGULAR
USE
THE
RESULT
TO
PROVE
NOW
USING
THE
RESULTS
AND
SHOW
THAT
E
XNXM
ΜΜT
INMΣ
WHERE
XN
DENOTES
A
DATA
POINT
SAMPLED
FROM
A
GAUSSIAN
DISTRIBUTION
WITH
MEAN
Μ
AND
COVARIANCE
Σ
AND
INM
DENOTES
THE
N
M
ELEMENT
OF
THE
IDENTITY
MATRIX
HENCE
PROVE
THE
RESULT
WWW
USING
AN
ANALOGOUS
PROCEDURE
TO
THAT
USED
TO
OBTAIN
DERIVE
AN
EXPRESSION
FOR
THE
SEQUENTIAL
ESTIMATION
OF
THE
VARIANCE
OF
A
UNIVARIATE
GAUSSIAN
DISTRIBUTION
BY
STARTING
WITH
THE
MAXIMUM
LIKELIHOOD
EXPRESSION
ML
N
N
XN
N
Μ
VERIFY
THAT
SUBSTITUTING
THE
EXPRESSION
FOR
A
GAUSSIAN
DISTRIBUTION
INTO
THE
ROBBINS
MONRO
SEQUENTIAL
ESTIMATION
FORMULA
GIVES
A
RESULT
OF
THE
SAME
FORM
AND
HENCE
OBTAIN
AN
EXPRESSION
FOR
THE
CORRESPONDING
COEFFICIENTS
AN
USING
AN
ANALOGOUS
PROCEDURE
TO
THAT
USED
TO
OBTAIN
DERIVE
AN
EX
PRESSION
FOR
THE
SEQUENTIAL
ESTIMATION
OF
THE
COVARIANCE
OF
A
MULTIVARIATE
GAUSSIAN
DISTRIBUTION
BY
STARTING
WITH
THE
MAXIMUM
LIKELIHOOD
EXPRESSION
VERIFY
THAT
SUBSTITUTING
THE
EXPRESSION
FOR
A
GAUSSIAN
DISTRIBUTION
INTO
THE
ROBBINS
MONRO
SE
QUENTIAL
ESTIMATION
FORMULA
GIVES
A
RESULT
OF
THE
SAME
FORM
AND
HENCE
OBTAIN
AN
EXPRESSION
FOR
THE
CORRESPONDING
COEFFICIENTS
AN
USE
THE
TECHNIQUE
OF
COMPLETING
THE
SQUARE
FOR
THE
QUADRATIC
FORM
IN
THE
EXPO
NENT
TO
DERIVE
THE
RESULTS
AND
STARTING
FROM
THE
RESULTS
AND
FOR
THE
POSTERIOR
DISTRIBUTION
OF
THE
MEAN
OF
A
GAUSSIAN
RANDOM
VARIABLE
DISSECT
OUT
THE
CONTRIBUTIONS
FROM
THE
FIRST
N
DATA
POINTS
AND
HENCE
OBTAIN
EXPRESSIONS
FOR
THE
SEQUENTIAL
UPDATE
OF
ΜN
AND
NOW
DERIVE
THE
SAME
RESULTS
STARTING
FROM
THE
POSTERIOR
DISTRIBUTION
P
Μ
XN
N
Μ
ΜN
AND
MULTIPLYING
BY
THE
LIKELIHOOD
FUNC
TION
P
N
XN
Μ
N
XN
Μ
Σ
AND
THEN
COMPLETING
THE
SQUARE
AND
NORMALIZING
TO
OBTAIN
THE
POSTERIOR
DISTRIBUTION
AFTER
N
OBSERVATIONS
WWW
CONSIDER
A
D
DIMENSIONAL
GAUSSIAN
RANDOM
VARIABLE
X
WITH
DISTRIBU
TION
N
X
Μ
Σ
IN
WHICH
THE
COVARIANCE
Σ
IS
KNOWN
AND
FOR
WHICH
WE
WISH
TO
INFER
THE
MEAN
Μ
FROM
A
SET
OF
OBSERVATIONS
X
XN
GIVEN
A
PRIOR
DISTRIBUTION
P
Μ
N
Μ
FIND
THE
CORRESPONDING
POSTERIOR
DISTRIBUTION
P
Μ
X
USE
THE
DEFINITION
OF
THE
GAMMA
FUNCTION
TO
SHOW
THAT
THE
GAMMA
DIS
TRIBUTION
IS
NORMALIZED
EVALUATE
THE
MEAN
VARIANCE
AND
MODE
OF
THE
GAMMA
DISTRIBUTION
THE
FOLLOWING
DISTRIBUTION
Q
X
Q
IS
A
GENERALIZATION
OF
THE
UNIVARIATE
GAUSSIAN
DISTRIBUTION
SHOW
THAT
THIS
DISTRIBUTION
IS
NORMALIZED
SO
THAT
P
X
Q
DX
AND
THAT
IT
REDUCES
TO
THE
GAUSSIAN
WHEN
Q
CONSIDER
A
REGRESSION
MODEL
IN
WHICH
THE
TARGET
VARIABLE
IS
GIVEN
BY
T
Y
X
W
E
AND
E
IS
A
RANDOM
NOISE
VARIABLE
DRAWN
FROM
THE
DISTRIBUTION
SHOW
THAT
THE
LOG
LIKELIHOOD
FUNCTION
OVER
W
AND
FOR
AN
OBSERVED
DATA
SET
OF
INPUT
VECTORS
X
XN
AND
CORRESPONDING
TARGET
VARIABLES
T
TN
T
IS
GIVEN
BY
N
LN
T
X
W
XN
N
W
TN
Q
LN
CONST
WHERE
CONST
DENOTES
TERMS
INDEPENDENT
OF
BOTH
W
AND
NOTE
THAT
AS
A
FUNCTION
OF
W
THIS
IS
THE
LQ
ERROR
FUNCTION
CONSIDERED
IN
SECTION
CONSIDER
A
UNIVARIATE
GAUSSIAN
DISTRIBUTION
X
Μ
Τ
HAVING
CONJUGATE
GAUSSIAN
GAMMA
PRIOR
GIVEN
BY
AND
A
DATA
SET
X
XN
OF
I
I
D
OBSERVATIONS
SHOW
THAT
THE
POSTERIOR
DISTRIBUTION
IS
ALSO
A
GAUSSIAN
GAMMA
DISTRI
BUTION
OF
THE
SAME
FUNCTIONAL
FORM
AS
THE
PRIOR
AND
WRITE
DOWN
EXPRESSIONS
FOR
THE
PARAMETERS
OF
THIS
POSTERIOR
DISTRIBUTION
VERIFY
THAT
THE
WISHART
DISTRIBUTION
DEFINED
BY
IS
INDEED
A
CONJUGATE
PRIOR
FOR
THE
PRECISION
MATRIX
OF
A
MULTIVARIATE
GAUSSIAN
WWW
VERIFY
THAT
EVALUATING
THE
INTEGRAL
IN
LEADS
TO
THE
RESULT
WWW
SHOW
THAT
IN
THE
LIMIT
Ν
THE
T
DISTRIBUTION
BECOMES
A
GAUSSIAN
HINT
IGNORE
THE
NORMALIZATION
COEFFICIENT
AND
SIMPLY
LOOK
AT
THE
DEPEN
DENCE
ON
X
BY
FOLLOWING
ANALOGOUS
STEPS
TO
THOSE
USED
TO
DERIVE
THE
UNIVARIATE
STUDENT
T
DISTRIBUTION
VERIFY
THE
RESULT
FOR
THE
MULTIVARIATE
FORM
OF
THE
STU
DENT
T
DISTRIBUTION
BY
MARGINALIZING
OVER
THE
VARIABLE
Η
IN
USING
THE
DEFINITION
SHOW
BY
EXCHANGING
INTEGRATION
VARIABLES
THAT
THE
MULTIVARIATE
T
DISTRIBUTION
IS
CORRECTLY
NORMALIZED
BY
USING
THE
DEFINITION
OF
THE
MULTIVARIATE
STUDENT
T
DISTRIBUTION
AS
A
CONVOLUTION
OF
A
GAUSSIAN
WITH
A
GAMMA
DISTRIBUTION
VERIFY
THE
PROPERTIES
AND
FOR
THE
MULTIVARIATE
T
DISTRIBUTION
DEFINED
BY
SHOW
THAT
IN
THE
LIMIT
Ν
THE
MULTIVARIATE
STUDENT
T
DISTRIBUTION
REDUCES
TO
A
GAUSSIAN
WITH
MEAN
Μ
AND
PRECISION
Λ
WWW
THE
VARIOUS
TRIGONOMETRIC
IDENTITIES
USED
IN
THE
DISCUSSION
OF
PERIODIC
VARIABLES
IN
THIS
CHAPTER
CAN
BE
PROVEN
EASILY
FROM
THE
RELATION
EXP
IA
COS
A
I
SIN
A
IN
WHICH
I
IS
THE
SQUARE
ROOT
OF
MINUS
ONE
BY
CONSIDERING
THE
IDENTITY
EXP
IA
EXP
IA
PROVE
THE
RESULT
SIMILARLY
USING
THE
IDENTITY
COS
A
B
R
EXP
I
A
B
WHERE
R
DENOTES
THE
REAL
PART
PROVE
FINALLY
BY
USING
SIN
A
B
EXP
I
A
B
WHERE
DENOTES
THE
IMAGINARY
PART
PROVE
THE
RESULT
FOR
LARGE
M
THE
VON
MISES
DISTRIBUTION
BECOMES
SHARPLY
PEAKED
AROUND
THE
MODE
BY
DEFINING
Ξ
Θ
AND
MAKING
THE
TAYLOR
EX
PANSION
OF
THE
COSINE
FUNCTION
GIVEN
BY
COS
Α
O
Α
SHOW
THAT
AS
M
THE
VON
MISES
DISTRIBUTION
TENDS
TO
A
GAUSSIAN
USING
THE
TRIGONOMETRIC
IDENTITY
SHOW
THAT
SOLUTION
OF
FOR
IS
GIVEN
BY
BY
COMPUTING
FIRST
AND
SECOND
DERIVATIVES
OF
THE
VON
MISES
DISTRIBUTION
AND
USING
M
FOR
M
SHOW
THAT
THE
MAXIMUM
OF
THE
DISTRIBUTION
OCCURS
WHEN
Θ
AND
THAT
THE
MINIMUM
OCCURS
WHEN
Θ
Π
MOD
BY
MAKING
USE
OF
THE
RESULT
TOGETHER
WITH
AND
THE
TRIGONOMETRIC
IDENTITY
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
MML
FOR
THE
CONCENTRA
TION
OF
THE
VON
MISES
DISTRIBUTION
SATISFIES
A
MML
R
WHERE
R
IS
THE
RADIUS
OF
THE
MEAN
OF
THE
OBSERVATIONS
VIEWED
AS
UNIT
VECTORS
IN
THE
TWO
DIMENSIONAL
EUCLIDEAN
PLANE
AS
ILLUSTRATED
IN
FIGURE
WWW
EXPRESS
THE
BETA
DISTRIBUTION
THE
GAMMA
DISTRIBUTION
AND
THE
VON
MISES
DISTRIBUTION
AS
MEMBERS
OF
THE
EXPONENTIAL
FAMILY
AND
THEREBY
IDENTIFY
THEIR
NATURAL
PARAMETERS
VERIFY
THAT
THE
MULTIVARIATE
GAUSSIAN
DISTRIBUTION
CAN
BE
CAST
IN
EXPONENTIAL
FAMILY
FORM
AND
DERIVE
EXPRESSIONS
FOR
Η
U
X
H
X
AND
G
Η
ANALOGOUS
TO
THE
RESULT
SHOWED
THAT
THE
NEGATIVE
GRADIENT
OF
LN
G
Η
FOR
THE
EXPONEN
TIAL
FAMILY
IS
GIVEN
BY
THE
EXPECTATION
OF
U
X
BY
TAKING
THE
SECOND
DERIVATIVES
OF
SHOW
THAT
LN
G
Η
E
U
X
U
X
T
E
U
X
E
U
X
T
COV
U
X
BY
CHANGING
VARIABLES
USING
Y
X
Σ
SHOW
THAT
THE
DENSITY
WILL
BE
CORRECTLY
NORMALIZED
PROVIDED
F
X
IS
CORRECTLY
NORMALIZED
WWW
CONSIDER
A
HISTOGRAM
LIKE
DENSITY
MODEL
IN
WHICH
THE
SPACE
X
IS
DI
VIDED
INTO
FIXED
REGIONS
FOR
WHICH
THE
DENSITY
P
X
TAKES
THE
CONSTANT
VALUE
HI
OVER
THE
ITH
REGION
AND
THAT
THE
VOLUME
OF
REGION
I
IS
DENOTED
I
SUPPOSE
WE
HAVE
A
SET
OF
N
OBSERVATIONS
OF
X
SUCH
THAT
NI
OF
THESE
OBSERVATIONS
FALL
IN
REGION
I
USING
A
LAGRANGE
MULTIPLIER
TO
ENFORCE
THE
NORMALIZATION
CONSTRAINT
ON
THE
DENSITY
DERIVE
AN
EXPRESSION
FOR
THE
MAXIMUM
LIKELIHOOD
ESTIMATOR
FOR
THE
HI
SHOW
THAT
THE
K
NEAREST
NEIGHBOUR
DENSITY
MODEL
DEFINES
AN
IMPROPER
DISTRIBU
TION
WHOSE
INTEGRAL
OVER
ALL
SPACE
IS
DIVERGENT
THE
FOCUS
SO
FAR
IN
THIS
BOOK
HAS
BEEN
ON
UNSUPERVISED
LEARNING
INCLUDING
TOPICS
SUCH
AS
DENSITY
ESTIMATION
AND
DATA
CLUSTERING
WE
TURN
NOW
TO
A
DISCUSSION
OF
SUPER
VISED
LEARNING
STARTING
WITH
REGRESSION
THE
GOAL
OF
REGRESSION
IS
TO
PREDICT
THE
VALUE
OF
ONE
OR
MORE
CONTINUOUS
TARGET
VARIABLES
T
GIVEN
THE
VALUE
OF
A
D
DIMENSIONAL
VEC
TOR
X
OF
INPUT
VARIABLES
WE
HAVE
ALREADY
ENCOUNTERED
AN
EXAMPLE
OF
A
REGRESSION
PROBLEM
WHEN
WE
CONSIDERED
POLYNOMIAL
CURVE
FITTING
IN
CHAPTER
THE
POLYNOMIAL
IS
A
SPECIFIC
EXAMPLE
OF
A
BROAD
CLASS
OF
FUNCTIONS
CALLED
LINEAR
REGRESSION
MODELS
WHICH
SHARE
THE
PROPERTY
OF
BEING
LINEAR
FUNCTIONS
OF
THE
ADJUSTABLE
PARAMETERS
AND
WHICH
WILL
FORM
THE
FOCUS
OF
THIS
CHAPTER
THE
SIMPLEST
FORM
OF
LINEAR
REGRESSION
MODELS
ARE
ALSO
LINEAR
FUNCTIONS
OF
THE
INPUT
VARIABLES
HOWEVER
WE
CAN
OBTAIN
A
MUCH
MORE
USEFUL
CLASS
OF
FUNCTIONS
BY
TAKING
LINEAR
COMBINATIONS
OF
A
FIXED
SET
OF
NONLINEAR
FUNCTIONS
OF
THE
INPUT
VARIABLES
KNOWN
AS
BASIS
FUNCTIONS
SUCH
MODELS
ARE
LINEAR
FUNCTIONS
OF
THE
PARAMETERS
WHICH
GIVES
THEM
SIMPLE
ANALYTICAL
PROPERTIES
AND
YET
CAN
BE
NONLINEAR
WITH
RESPECT
TO
THE
INPUT
VARIABLES
GIVEN
A
TRAINING
DATA
SET
COMPRISING
N
OBSERVATIONS
XN
WHERE
N
N
TOGETHER
WITH
CORRESPONDING
TARGET
VALUES
TN
THE
GOAL
IS
TO
PREDICT
THE
VALUE
OF
T
FOR
A
NEW
VALUE
OF
X
IN
THE
SIMPLEST
APPROACH
THIS
CAN
BE
DONE
BY
DIRECTLY
CON
STRUCTING
AN
APPROPRIATE
FUNCTION
Y
X
WHOSE
VALUES
FOR
NEW
INPUTS
X
CONSTITUTE
THE
PREDICTIONS
FOR
THE
CORRESPONDING
VALUES
OF
T
MORE
GENERALLY
FROM
A
PROBABILISTIC
PERSPECTIVE
WE
AIM
TO
MODEL
THE
PREDICTIVE
DISTRIBUTION
P
T
X
BECAUSE
THIS
EXPRESSES
OUR
UNCERTAINTY
ABOUT
THE
VALUE
OF
T
FOR
EACH
VALUE
OF
X
FROM
THIS
CONDITIONAL
DIS
TRIBUTION
WE
CAN
MAKE
PREDICTIONS
OF
T
FOR
ANY
NEW
VALUE
OF
X
IN
SUCH
A
WAY
AS
TO
MINIMIZE
THE
EXPECTED
VALUE
OF
A
SUITABLY
CHOSEN
LOSS
FUNCTION
AS
DISCUSSED
IN
SEC
TION
A
COMMON
CHOICE
OF
LOSS
FUNCTION
FOR
REAL
VALUED
VARIABLES
IS
THE
SQUARED
LOSS
FOR
WHICH
THE
OPTIMAL
SOLUTION
IS
GIVEN
BY
THE
CONDITIONAL
EXPECTATION
OF
T
ALTHOUGH
LINEAR
MODELS
HAVE
SIGNIFICANT
LIMITATIONS
AS
PRACTICAL
TECHNIQUES
FOR
PATTERN
RECOGNITION
PARTICULARLY
FOR
PROBLEMS
INVOLVING
INPUT
SPACES
OF
HIGH
DIMEN
SIONALITY
THEY
HAVE
NICE
ANALYTICAL
PROPERTIES
AND
FORM
THE
FOUNDATION
FOR
MORE
SO
PHISTICATED
MODELS
TO
BE
DISCUSSED
IN
LATER
CHAPTERS
LINEAR
BASIS
FUNCTION
MODELS
THE
SIMPLEST
LINEAR
MODEL
FOR
REGRESSION
IS
ONE
THAT
INVOLVES
A
LINEAR
COMBINATION
OF
THE
INPUT
VARIABLES
Y
X
W
WDXD
WHERE
X
XD
T
THIS
IS
OFTEN
SIMPLY
KNOWN
AS
LINEAR
REGRESSION
THE
KEY
PROPERTY
OF
THIS
MODEL
IS
THAT
IT
IS
A
LINEAR
FUNCTION
OF
THE
PARAMETERS
WD
IT
IS
ALSO
HOWEVER
A
LINEAR
FUNCTION
OF
THE
INPUT
VARIABLES
XI
AND
THIS
IMPOSES
SIGNIFICANT
LIMITATIONS
ON
THE
MODEL
WE
THEREFORE
EXTEND
THE
CLASS
OF
MODELS
BY
CONSIDERING
LINEAR
COMBINATIONS
OF
FIXED
NONLINEAR
FUNCTIONS
OF
THE
INPUT
VARIABLES
OF
THE
FORM
M
Y
X
W
WJΦJ
X
J
WHERE
ΦJ
X
ARE
KNOWN
AS
BASIS
FUNCTIONS
BY
DENOTING
THE
MAXIMUM
VALUE
OF
THE
INDEX
J
BY
M
THE
TOTAL
NUMBER
OF
PARAMETERS
IN
THIS
MODEL
WILL
BE
M
THE
PARAMETER
ALLOWS
FOR
ANY
FIXED
OFFSET
IN
THE
DATA
AND
IS
SOMETIMES
CALLED
A
BIAS
PARAMETER
NOT
TO
BE
CONFUSED
WITH
BIAS
IN
A
STATISTICAL
SENSE
IT
IS
OFTEN
CONVENIENT
TO
DEFINE
AN
ADDITIONAL
DUMMY
BASIS
FUNCTION
X
SO
THAT
M
Y
X
W
WJΦJ
X
WTΦ
X
J
WHERE
W
WM
T
AND
Φ
ΦM
T
IN
MANY
PRACTICAL
AP
PLICATIONS
OF
PATTERN
RECOGNITION
WE
WILL
APPLY
SOME
FORM
OF
FIXED
PRE
PROCESSING
OR
FEATURE
EXTRACTION
TO
THE
ORIGINAL
DATA
VARIABLES
IF
THE
ORIGINAL
VARIABLES
COM
PRISE
THE
VECTOR
X
THEN
THE
FEATURES
CAN
BE
EXPRESSED
IN
TERMS
OF
THE
BASIS
FUNCTIONS
ΦJ
X
BY
USING
NONLINEAR
BASIS
FUNCTIONS
WE
ALLOW
THE
FUNCTION
Y
X
W
TO
BE
A
NON
LINEAR
FUNCTION
OF
THE
INPUT
VECTOR
X
FUNCTIONS
OF
THE
FORM
ARE
CALLED
LINEAR
MODELS
HOWEVER
BECAUSE
THIS
FUNCTION
IS
LINEAR
IN
W
IT
IS
THIS
LINEARITY
IN
THE
PA
RAMETERS
THAT
WILL
GREATLY
SIMPLIFY
THE
ANALYSIS
OF
THIS
CLASS
OF
MODELS
HOWEVER
IT
ALSO
LEADS
TO
SOME
SIGNIFICANT
LIMITATIONS
AS
WE
DISCUSS
IN
SECTION
THE
EXAMPLE
OF
POLYNOMIAL
REGRESSION
CONSIDERED
IN
CHAPTER
IS
A
PARTICULAR
EXAMPLE
OF
THIS
MODEL
IN
WHICH
THERE
IS
A
SINGLE
INPUT
VARIABLE
X
AND
THE
BASIS
FUNC
TIONS
TAKE
THE
FORM
OF
POWERS
OF
X
SO
THAT
ΦJ
X
XJ
ONE
LIMITATION
OF
POLYNOMIAL
BASIS
FUNCTIONS
IS
THAT
THEY
ARE
GLOBAL
FUNCTIONS
OF
THE
INPUT
VARIABLE
SO
THAT
CHANGES
IN
ONE
REGION
OF
INPUT
SPACE
AFFECT
ALL
OTHER
REGIONS
THIS
CAN
BE
RESOLVED
BY
DIVIDING
THE
INPUT
SPACE
UP
INTO
REGIONS
AND
FIT
A
DIFFERENT
POLYNOMIAL
IN
EACH
REGION
LEADING
TO
SPLINE
FUNCTIONS
HASTIE
ET
AL
THERE
ARE
MANY
OTHER
POSSIBLE
CHOICES
FOR
THE
BASIS
FUNCTIONS
FOR
EXAMPLE
ΦJ
X
EXP
X
Μ
WHERE
THE
ΜJ
GOVERN
THE
LOCATIONS
OF
THE
BASIS
FUNCTIONS
IN
INPUT
SPACE
AND
THE
PA
RAMETER
GOVERNS
THEIR
SPATIAL
SCALE
THESE
ARE
USUALLY
REFERRED
TO
AS
GAUSSIAN
BASIS
FUNCTIONS
ALTHOUGH
IT
SHOULD
BE
NOTED
THAT
THEY
ARE
NOT
REQUIRED
TO
HAVE
A
PROB
ABILISTIC
INTERPRETATION
AND
IN
PARTICULAR
THE
NORMALIZATION
COEFFICIENT
IS
UNIMPORTANT
BECAUSE
THESE
BASIS
FUNCTIONS
WILL
BE
MULTIPLIED
BY
ADAPTIVE
PARAMETERS
WJ
ANOTHER
POSSIBILITY
IS
THE
SIGMOIDAL
BASIS
FUNCTION
OF
THE
FORM
Φ
X
Σ
X
ΜJ
WHERE
Σ
A
IS
THE
LOGISTIC
SIGMOID
FUNCTION
DEFINED
BY
Σ
A
EXP
A
EQUIVALENTLY
WE
CAN
USE
THE
TANH
FUNCTION
BECAUSE
THIS
IS
RELATED
TO
THE
LOGISTIC
SIGMOID
BY
TANH
A
A
AND
SO
A
GENERAL
LINEAR
COMBINATION
OF
LOGISTIC
SIGMOID
FUNCTIONS
IS
EQUIVALENT
TO
A
GENERAL
LINEAR
COMBINATION
OF
TANH
FUNCTIONS
THESE
VARIOUS
CHOICES
OF
BASIS
FUNCTION
ARE
ILLUSTRATED
IN
FIGURE
YET
ANOTHER
POSSIBLE
CHOICE
OF
BASIS
FUNCTION
IS
THE
FOURIER
BASIS
WHICH
LEADS
TO
AN
EXPANSION
IN
SINUSOIDAL
FUNCTIONS
EACH
BASIS
FUNCTION
REPRESENTS
A
SPECIFIC
FRE
QUENCY
AND
HAS
INFINITE
SPATIAL
EXTENT
BY
CONTRAST
BASIS
FUNCTIONS
THAT
ARE
LOCALIZED
TO
FINITE
REGIONS
OF
INPUT
SPACE
NECESSARILY
COMPRISE
A
SPECTRUM
OF
DIFFERENT
SPATIAL
FREQUENCIES
IN
MANY
SIGNAL
PROCESSING
APPLICATIONS
IT
IS
OF
INTEREST
TO
CONSIDER
BA
SIS
FUNCTIONS
THAT
ARE
LOCALIZED
IN
BOTH
SPACE
AND
FREQUENCY
LEADING
TO
A
CLASS
OF
FUNCTIONS
KNOWN
AS
WAVELETS
THESE
ARE
ALSO
DEFINED
TO
BE
MUTUALLY
ORTHOGONAL
TO
SIMPLIFY
THEIR
APPLICATION
WAVELETS
ARE
MOST
APPLICABLE
WHEN
THE
INPUT
VALUES
LIVE
FIGURE
EXAMPLES
OF
BASIS
FUNCTIONS
SHOWING
POLYNOMIALS
ON
THE
LEFT
GAUSSIANS
OF
THE
FORM
IN
THE
CENTRE
AND
SIGMOIDAL
OF
THE
FORM
ON
THE
RIGHT
ON
A
REGULAR
LATTICE
SUCH
AS
THE
SUCCESSIVE
TIME
POINTS
IN
A
TEMPORAL
SEQUENCE
OR
THE
PIXELS
IN
AN
IMAGE
USEFUL
TEXTS
ON
WAVELETS
INCLUDE
OGDEN
MALLAT
AND
VIDAKOVIC
MOST
OF
THE
DISCUSSION
IN
THIS
CHAPTER
HOWEVER
IS
INDEPENDENT
OF
THE
PARTICULAR
CHOICE
OF
BASIS
FUNCTION
SET
AND
SO
FOR
MOST
OF
OUR
DISCUSSION
WE
SHALL
NOT
SPECIFY
THE
PARTICULAR
FORM
OF
THE
BASIS
FUNCTIONS
EXCEPT
FOR
THE
PURPOSES
OF
NUMERICAL
IL
LUSTRATION
INDEED
MUCH
OF
OUR
DISCUSSION
WILL
BE
EQUALLY
APPLICABLE
TO
THE
SITUATION
IN
WHICH
THE
VECTOR
Φ
X
OF
BASIS
FUNCTIONS
IS
SIMPLY
THE
IDENTITY
Φ
X
X
FUR
THERMORE
IN
ORDER
TO
KEEP
THE
NOTATION
SIMPLE
WE
SHALL
FOCUS
ON
THE
CASE
OF
A
SINGLE
TARGET
VARIABLE
T
HOWEVER
IN
SECTION
WE
CONSIDER
BRIEFLY
THE
MODIFICATIONS
NEEDED
TO
DEAL
WITH
MULTIPLE
TARGET
VARIABLES
MAXIMUM
LIKELIHOOD
AND
LEAST
SQUARES
IN
CHAPTER
WE
FITTED
POLYNOMIAL
FUNCTIONS
TO
DATA
SETS
BY
MINIMIZING
A
SUM
OF
SQUARES
ERROR
FUNCTION
WE
ALSO
SHOWED
THAT
THIS
ERROR
FUNCTION
COULD
BE
MOTIVATED
AS
THE
MAXIMUM
LIKELIHOOD
SOLUTION
UNDER
AN
ASSUMED
GAUSSIAN
NOISE
MODEL
LET
US
RETURN
TO
THIS
DISCUSSION
AND
CONSIDER
THE
LEAST
SQUARES
APPROACH
AND
ITS
RELATION
TO
MAXIMUM
LIKELIHOOD
IN
MORE
DETAIL
AS
BEFORE
WE
ASSUME
THAT
THE
TARGET
VARIABLE
T
IS
GIVEN
BY
A
DETERMINISTIC
FUNC
TION
Y
X
W
WITH
ADDITIVE
GAUSSIAN
NOISE
SO
THAT
T
Y
X
W
E
WHERE
E
IS
A
ZERO
MEAN
GAUSSIAN
RANDOM
VARIABLE
WITH
PRECISION
INVERSE
VARIANCE
Β
THUS
WE
CAN
WRITE
SECTION
P
T
X
W
Β
N
T
Y
X
W
Β
RECALL
THAT
IF
WE
ASSUME
A
SQUARED
LOSS
FUNCTION
THEN
THE
OPTIMAL
PREDICTION
FOR
A
NEW
VALUE
OF
X
WILL
BE
GIVEN
BY
THE
CONDITIONAL
MEAN
OF
THE
TARGET
VARIABLE
IN
THE
CASE
OF
A
GAUSSIAN
CONDITIONAL
DISTRIBUTION
OF
THE
FORM
THE
CONDITIONAL
MEAN
WILL
BE
SIMPLY
E
T
X
R
TP
T
X
DT
Y
X
W
NOTE
THAT
THE
GAUSSIAN
NOISE
ASSUMPTION
IMPLIES
THAT
THE
CONDITIONAL
DISTRIBUTION
OF
T
GIVEN
X
IS
UNIMODAL
WHICH
MAY
BE
INAPPROPRIATE
FOR
SOME
APPLICATIONS
AN
EX
TENSION
TO
MIXTURES
OF
CONDITIONAL
GAUSSIAN
DISTRIBUTIONS
WHICH
PERMIT
MULTIMODAL
CONDITIONAL
DISTRIBUTIONS
WILL
BE
DISCUSSED
IN
SECTION
NOW
CONSIDER
A
DATA
SET
OF
INPUTS
X
XN
WITH
CORRESPONDING
TARGET
VALUES
TN
WE
GROUP
THE
TARGET
VARIABLES
TN
INTO
A
COLUMN
VECTOR
THAT
WE
DENOTE
BY
T
WHERE
THE
TYPEFACE
IS
CHOSEN
TO
DISTINGUISH
IT
FROM
A
SINGLE
OBSERVATION
OF
A
MULTIVARIATE
TARGET
WHICH
WOULD
BE
DENOTED
T
MAKING
THE
ASSUMPTION
THAT
THESE
DATA
POINTS
ARE
DRAWN
INDEPENDENTLY
FROM
THE
DISTRIBUTION
WE
OBTAIN
THE
FOLLOWING
EXPRESSION
FOR
THE
LIKELIHOOD
FUNCTION
WHICH
IS
A
FUNCTION
OF
THE
ADJUSTABLE
PARAMETERS
W
AND
Β
IN
THE
FORM
N
P
T
X
W
Β
N
TN
WTΦ
XN
Β
N
WHERE
WE
HAVE
USED
NOTE
THAT
IN
SUPERVISED
LEARNING
PROBLEMS
SUCH
AS
REGRES
SION
AND
CLASSIFICATION
WE
ARE
NOT
SEEKING
TO
MODEL
THE
DISTRIBUTION
OF
THE
INPUT
VARIABLES
THUS
X
WILL
ALWAYS
APPEAR
IN
THE
SET
OF
CONDITIONING
VARIABLES
AND
SO
FROM
NOW
ON
WE
WILL
DROP
THE
EXPLICIT
X
FROM
EXPRESSIONS
SUCH
AS
P
T
X
W
Β
IN
OR
DER
TO
KEEP
THE
NOTATION
UNCLUTTERED
TAKING
THE
LOGARITHM
OF
THE
LIKELIHOOD
FUNCTION
AND
MAKING
USE
OF
THE
STANDARD
FORM
FOR
THE
UNIVARIATE
GAUSSIAN
WE
HAVE
LN
P
T
W
Β
N
LN
N
TN
WTΦ
XN
Β
N
N
LN
Β
N
LN
ΒE
W
WHERE
THE
SUM
OF
SQUARES
ERROR
FUNCTION
IS
DEFINED
BY
N
ED
W
N
N
WTΦ
XN
HAVING
WRITTEN
DOWN
THE
LIKELIHOOD
FUNCTION
WE
CAN
USE
MAXIMUM
LIKELIHOOD
TO
DETERMINE
W
AND
Β
CONSIDER
FIRST
THE
MAXIMIZATION
WITH
RESPECT
TO
W
AS
OBSERVED
ALREADY
IN
SECTION
WE
SEE
THAT
MAXIMIZATION
OF
THE
LIKELIHOOD
FUNCTION
UNDER
A
CONDITIONAL
GAUSSIAN
NOISE
DISTRIBUTION
FOR
A
LINEAR
MODEL
IS
EQUIVALENT
TO
MINIMIZING
A
SUM
OF
SQUARES
ERROR
FUNCTION
GIVEN
BY
ED
W
THE
GRADIENT
OF
THE
LOG
LIKELIHOOD
FUNCTION
TAKES
THE
FORM
N
LN
P
T
W
Β
TN
WTΦ
XN
Φ
XN
T
N
SETTING
THIS
GRADIENT
TO
ZERO
GIVES
N
SOLVING
FOR
W
WE
OBTAIN
WML
ΦTΦ
ΦTT
WHICH
ARE
KNOWN
AS
THE
NORMAL
EQUATIONS
FOR
THE
LEAST
SQUARES
PROBLEM
HERE
Φ
IS
AN
N
M
MATRIX
CALLED
THE
DESIGN
MATRIX
WHOSE
ELEMENTS
ARE
GIVEN
BY
ΦNJ
ΦJ
XN
SO
THAT
Φ
ΦM
ΦM
THE
QUANTITY
XN
XN
ΦM
XN
Φ
ΦTΦ
ΦT
IS
KNOWN
AS
THE
MOORE
PENROSE
PSEUDO
INVERSE
OF
THE
MATRIX
Φ
RAO
AND
MITRA
GOLUB
AND
VAN
LOAN
IT
CAN
BE
REGARDED
AS
A
GENERALIZATION
OF
THE
NOTION
OF
MATRIX
INVERSE
TO
NONSQUARE
MATRICES
INDEED
IF
Φ
IS
SQUARE
AND
INVERTIBLE
THEN
USING
THE
PROPERTY
AB
B
WE
SEE
THAT
Φ
Φ
AT
THIS
POINT
WE
CAN
GAIN
SOME
INSIGHT
INTO
THE
ROLE
OF
THE
BIAS
PARAMETER
IF
WE
MAKE
THE
BIAS
PARAMETER
EXPLICIT
THEN
THE
ERROR
FUNCTION
BECOMES
M
E
W
T
W
W
Φ
X
SETTING
THE
DERIVATIVE
WITH
RESPECT
TO
EQUAL
TO
ZERO
AND
SOLVING
FOR
WE
OBTAIN
M
T
WJΦJ
J
WHERE
WE
HAVE
DEFINED
T
N
N
TN
N
ΦJ
N
N
ΦJ
XN
N
THUS
THE
BIAS
COMPENSATES
FOR
THE
DIFFERENCE
BETWEEN
THE
AVERAGES
OVER
THE
TRAINING
SET
OF
THE
TARGET
VALUES
AND
THE
WEIGHTED
SUM
OF
THE
AVERAGES
OF
THE
BASIS
FUNCTION
VALUES
WE
CAN
ALSO
MAXIMIZE
THE
LOG
LIKELIHOOD
FUNCTION
WITH
RESPECT
TO
THE
NOISE
PRECISION
PARAMETER
Β
GIVING
T
WT
Φ
X
FIGURE
GEOMETRICAL
INTERPRETATION
OF
THE
LEAST
SQUARES
SOLUTION
IN
AN
N
DIMENSIONAL
SPACE
WHOSE
AXES
ARE
THE
VALUES
OF
TN
THE
LEAST
SQUARES
REGRESSION
FUNCTION
IS
OBTAINED
BY
FINDING
THE
OR
THOGONAL
PROJECTION
OF
THE
DATA
VECTOR
T
ONTO
THE
SUBSPACE
SPANNED
BY
THE
BASIS
FUNCTIONS
ΦJ
X
IN
WHICH
EACH
BASIS
FUNCTION
IS
VIEWED
AS
A
VEC
TOR
ΦJ
OF
LENGTH
N
WITH
ELEMENTS
ΦJ
XN
EXERCISE
AND
SO
WE
SEE
THAT
THE
INVERSE
OF
THE
NOISE
PRECISION
IS
GIVEN
BY
THE
RESIDUAL
VARIANCE
OF
THE
TARGET
VALUES
AROUND
THE
REGRESSION
FUNCTION
GEOMETRY
OF
LEAST
SQUARES
AT
THIS
POINT
IT
IS
INSTRUCTIVE
TO
CONSIDER
THE
GEOMETRICAL
INTERPRETATION
OF
THE
LEAST
SQUARES
SOLUTION
TO
DO
THIS
WE
CONSIDER
AN
N
DIMENSIONAL
SPACE
WHOSE
AXES
ARE
GIVEN
BY
THE
TN
SO
THAT
T
TN
T
IS
A
VECTOR
IN
THIS
SPACE
EACH
BASIS
FUNCTION
ΦJ
XN
EVALUATED
AT
THE
N
DATA
POINTS
CAN
ALSO
BE
REPRESENTED
AS
A
VECTOR
IN
THE
SAME
SPACE
DENOTED
BY
ΦJ
AS
ILLUSTRATED
IN
FIGURE
NOTE
THAT
ΦJ
CORRESPONDS
TO
THE
JTH
COLUMN
OF
Φ
WHEREAS
Φ
XN
CORRESPONDS
TO
THE
NTH
ROW
OF
Φ
IF
THE
NUMBER
M
OF
BASIS
FUNCTIONS
IS
SMALLER
THAN
THE
NUMBER
N
OF
DATA
POINTS
THEN
THE
M
VECTORS
ΦJ
XN
WILL
SPAN
A
LINEAR
SUBSPACE
OF
DIMENSIONALITY
M
WE
DEFINE
Y
TO
BE
AN
N
DIMENSIONAL
VECTOR
WHOSE
NTH
ELEMENT
IS
GIVEN
BY
Y
XN
W
WHERE
N
N
BECAUSE
Y
IS
AN
ARBITRARY
LINEAR
COMBINATION
OF
THE
VECTORS
ΦJ
IT
CAN
LIVE
ANYWHERE
IN
THE
M
DIMENSIONAL
SUBSPACE
THE
SUM
OF
SQUARES
ERROR
IS
THEN
EQUAL
UP
TO
A
FACTOR
OF
TO
THE
SQUARED
EUCLIDEAN
DISTANCE
BETWEEN
Y
AND
T
THUS
THE
LEAST
SQUARES
SOLUTION
FOR
W
CORRESPONDS
TO
THAT
CHOICE
OF
Y
THAT
LIES
IN
SUBSPACE
AND
THAT
IS
CLOSEST
TO
T
INTUITIVELY
FROM
FIGURE
WE
ANTICIPATE
THAT
THIS
SOLUTION
CORRESPONDS
TO
THE
ORTHOGONAL
PROJECTION
OF
T
ONTO
THE
SUBSPACE
THIS
IS
INDEED
THE
CASE
AS
CAN
EASILY
BE
VERIFIED
BY
NOTING
THAT
THE
SOLUTION
FOR
Y
IS
GIVEN
BY
ΦWML
AND
THEN
CONFIRMING
THAT
THIS
TAKES
THE
FORM
OF
AN
ORTHOGONAL
PROJECTION
IN
PRACTICE
A
DIRECT
SOLUTION
OF
THE
NORMAL
EQUATIONS
CAN
LEAD
TO
NUMERICAL
DIFFI
CULTIES
WHEN
ΦTΦ
IS
CLOSE
TO
SINGULAR
IN
PARTICULAR
WHEN
TWO
OR
MORE
OF
THE
BASIS
VECTORS
ΦJ
ARE
CO
LINEAR
OR
NEARLY
SO
THE
RESULTING
PARAMETER
VALUES
CAN
HAVE
LARGE
MAGNITUDES
SUCH
NEAR
DEGENERACIES
WILL
NOT
BE
UNCOMMON
WHEN
DEALING
WITH
REAL
DATA
SETS
THE
RESULTING
NUMERICAL
DIFFICULTIES
CAN
BE
ADDRESSED
USING
THE
TECHNIQUE
OF
SINGULAR
VALUE
DECOMPOSITION
OR
SVD
PRESS
ET
AL
BISHOP
AND
NABNEY
NOTE
THAT
THE
ADDITION
OF
A
REGULARIZATION
TERM
ENSURES
THAT
THE
MATRIX
IS
NON
SINGULAR
EVEN
IN
THE
PRESENCE
OF
DEGENERACIES
SEQUENTIAL
LEARNING
BATCH
TECHNIQUES
SUCH
AS
THE
MAXIMUM
LIKELIHOOD
SOLUTION
WHICH
IN
VOLVE
PROCESSING
THE
ENTIRE
TRAINING
SET
IN
ONE
GO
CAN
BE
COMPUTATIONALLY
COSTLY
FOR
LARGE
DATA
SETS
AS
WE
HAVE
DISCUSSED
IN
CHAPTER
IF
THE
DATA
SET
IS
SUFFICIENTLY
LARGE
IT
MAY
BE
WORTHWHILE
TO
USE
SEQUENTIAL
ALGORITHMS
ALSO
KNOWN
AS
ON
LINE
ALGORITHMS
IN
WHICH
THE
DATA
POINTS
ARE
CONSIDERED
ONE
AT
A
TIME
AND
THE
MODEL
PARAMETERS
UP
DATED
AFTER
EACH
SUCH
PRESENTATION
SEQUENTIAL
LEARNING
IS
ALSO
APPROPRIATE
FOR
REAL
TIME
APPLICATIONS
IN
WHICH
THE
DATA
OBSERVATIONS
ARE
ARRIVING
IN
A
CONTINUOUS
STREAM
AND
PREDICTIONS
MUST
BE
MADE
BEFORE
ALL
OF
THE
DATA
POINTS
ARE
SEEN
WE
CAN
OBTAIN
A
SEQUENTIAL
LEARNING
ALGORITHM
BY
APPLYING
THE
TECHNIQUE
OF
STOCHASTIC
GRADIENT
DESCENT
ALSO
KNOWN
AS
SEQUENTIAL
GRADIENT
DESCENT
AS
FOLLOWS
IF
THE
ERROR
FUNCTION
COMPRISES
A
SUM
OVER
DATA
POINTS
E
N
EN
THEN
AFTER
PRESEN
TATION
OF
PATTERN
N
THE
STOCHASTIC
GRADIENT
DESCENT
ALGORITHM
UPDATES
THE
PARAMETER
VECTOR
W
USING
W
Τ
W
Τ
Η
EN
WHERE
Τ
DENOTES
THE
ITERATION
NUMBER
AND
Η
IS
A
LEARNING
RATE
PARAMETER
WE
SHALL
DISCUSS
THE
CHOICE
OF
VALUE
FOR
Η
SHORTLY
THE
VALUE
OF
W
IS
INITIALIZED
TO
SOME
STARTING
VECTOR
W
FOR
THE
CASE
OF
THE
SUM
OF
SQUARES
ERROR
FUNCTION
THIS
GIVES
W
Τ
W
Τ
Η
TN
W
Τ
TΦN
ΦN
WHERE
ΦN
Φ
XN
THIS
IS
KNOWN
AS
LEAST
MEAN
SQUARES
OR
THE
LMS
ALGORITHM
THE
VALUE
OF
Η
NEEDS
TO
BE
CHOSEN
WITH
CARE
TO
ENSURE
THAT
THE
ALGORITHM
CONVERGES
BISHOP
AND
NABNEY
REGULARIZED
LEAST
SQUARES
IN
SECTION
WE
INTRODUCED
THE
IDEA
OF
ADDING
A
REGULARIZATION
TERM
TO
AN
ERROR
FUNCTION
IN
ORDER
TO
CONTROL
OVER
FITTING
SO
THAT
THE
TOTAL
ERROR
FUNCTION
TO
BE
MINIMIZED
TAKES
THE
FORM
ED
W
ΛEW
W
WHERE
Λ
IS
THE
REGULARIZATION
COEFFICIENT
THAT
CONTROLS
THE
RELATIVE
IMPORTANCE
OF
THE
DATA
DEPENDENT
ERROR
ED
W
AND
THE
REGULARIZATION
TERM
EW
W
ONE
OF
THE
SIM
PLEST
FORMS
OF
REGULARIZER
IS
GIVEN
BY
THE
SUM
OF
SQUARES
OF
THE
WEIGHT
VECTOR
ELE
MENTS
E
W
WTW
W
IF
WE
ALSO
CONSIDER
THE
SUM
OF
SQUARES
ERROR
FUNCTION
GIVEN
BY
N
W
N
THEN
THE
TOTAL
ERROR
FUNCTION
BECOMES
N
WTΦ
XN
N
N
N
WTΦ
XN
Λ
WTW
THIS
PARTICULAR
CHOICE
OF
REGULARIZER
IS
KNOWN
IN
THE
MACHINE
LEARNING
LITERATURE
AS
WEIGHT
DECAY
BECAUSE
IN
SEQUENTIAL
LEARNING
ALGORITHMS
IT
ENCOURAGES
WEIGHT
VALUES
TO
DECAY
TOWARDS
ZERO
UNLESS
SUPPORTED
BY
THE
DATA
IN
STATISTICS
IT
PROVIDES
AN
EX
AMPLE
OF
A
PARAMETER
SHRINKAGE
METHOD
BECAUSE
IT
SHRINKS
PARAMETER
VALUES
TOWARDS
Q
Q
Q
Q
FIGURE
CONTOURS
OF
THE
REGULARIZATION
TERM
IN
FOR
VARIOUS
VALUES
OF
THE
PARAMETER
Q
ZERO
IT
HAS
THE
ADVANTAGE
THAT
THE
ERROR
FUNCTION
REMAINS
A
QUADRATIC
FUNCTION
OF
W
AND
SO
ITS
EXACT
MINIMIZER
CAN
BE
FOUND
IN
CLOSED
FORM
SPECIFICALLY
SETTING
THE
GRADIENT
OF
WITH
RESPECT
TO
W
TO
ZERO
AND
SOLVING
FOR
W
AS
BEFORE
WE
OBTAIN
W
ΛI
ΦTΦ
ΦTT
THIS
REPRESENTS
A
SIMPLE
EXTENSION
OF
THE
LEAST
SQUARES
SOLUTION
A
MORE
GENERAL
REGULARIZER
IS
SOMETIMES
USED
FOR
WHICH
THE
REGULARIZED
ERROR
TAKES
THE
FORM
T
WTΦ
X
Λ
W
Q
EXERCISE
WHERE
Q
CORRESPONDS
TO
THE
QUADRATIC
REGULARIZER
FIGURE
SHOWS
CON
TOURS
OF
THE
REGULARIZATION
FUNCTION
FOR
DIFFERENT
VALUES
OF
Q
THE
CASE
OF
Q
IS
KNOW
AS
THE
LASSO
IN
THE
STATISTICS
LITERATURE
TIBSHIRANI
IT
HAS
THE
PROPERTY
THAT
IF
Λ
IS
SUFFICIENTLY
LARGE
SOME
OF
THE
COEFFICIENTS
WJ
ARE
DRIVEN
TO
ZERO
LEADING
TO
A
SPARSE
MODEL
IN
WHICH
THE
CORRESPONDING
BASIS
FUNCTIONS
PLAY
NO
ROLE
TO
SEE
THIS
WE
FIRST
NOTE
THAT
MINIMIZING
IS
EQUIVALENT
TO
MINIMIZING
THE
UNREGULARIZED
SUM
OF
SQUARES
ERROR
SUBJECT
TO
THE
CONSTRAINT
APPENDIX
E
M
WJ
Q
Η
J
FOR
AN
APPROPRIATE
VALUE
OF
THE
PARAMETER
Η
WHERE
THE
TWO
APPROACHES
CAN
BE
RELATED
USING
LAGRANGE
MULTIPLIERS
THE
ORIGIN
OF
THE
SPARSITY
CAN
BE
SEEN
FROM
FIGURE
WHICH
SHOWS
THAT
THE
MINIMUM
OF
THE
ERROR
FUNCTION
SUBJECT
TO
THE
CONSTRAINT
AS
Λ
IS
INCREASED
SO
AN
INCREASING
NUMBER
OF
PARAMETERS
ARE
DRIVEN
TO
ZERO
REGULARIZATION
ALLOWS
COMPLEX
MODELS
TO
BE
TRAINED
ON
DATA
SETS
OF
LIMITED
SIZE
WITHOUT
SEVERE
OVER
FITTING
ESSENTIALLY
BY
LIMITING
THE
EFFECTIVE
MODEL
COMPLEXITY
HOWEVER
THE
PROBLEM
OF
DETERMINING
THE
OPTIMAL
MODEL
COMPLEXITY
IS
THEN
SHIFTED
FROM
ONE
OF
FINDING
THE
APPROPRIATE
NUMBER
OF
BASIS
FUNCTIONS
TO
ONE
OF
DETERMINING
A
SUITABLE
VALUE
OF
THE
REGULARIZATION
COEFFICIENT
Λ
WE
SHALL
RETURN
TO
THE
ISSUE
OF
MODEL
COMPLEXITY
LATER
IN
THIS
CHAPTER
FIGURE
PLOT
OF
THE
CONTOURS
OF
THE
UNREGULARIZED
ERROR
FUNCTION
BLUE
ALONG
WITH
THE
CONSTRAINT
RE
GION
FOR
THE
QUADRATIC
REGULAR
IZER
Q
ON
THE
LEFT
AND
THE
LASSO
REGULARIZER
Q
ON
THE
RIGHT
IN
WHICH
THE
OPTIMUM
VALUE
FOR
THE
PA
RAMETER
VECTOR
W
IS
DENOTED
BY
W
THE
LASSO
GIVES
A
SPARSE
SOLUTION
IN
WHICH
W
W
W
FOR
THE
REMAINDER
OF
THIS
CHAPTER
WE
SHALL
FOCUS
ON
THE
QUADRATIC
REGULARIZER
BOTH
FOR
ITS
PRACTICAL
IMPORTANCE
AND
ITS
ANALYTICAL
TRACTABILITY
MULTIPLE
OUTPUTS
SO
FAR
WE
HAVE
CONSIDERED
THE
CASE
OF
A
SINGLE
TARGET
VARIABLE
T
IN
SOME
APPLICA
TIONS
WE
MAY
WISH
TO
PREDICT
K
TARGET
VARIABLES
WHICH
WE
DENOTE
COLLECTIVELY
BY
THE
TARGET
VECTOR
T
THIS
COULD
BE
DONE
BY
INTRODUCING
A
DIFFERENT
SET
OF
BASIS
FUNC
TIONS
FOR
EACH
COMPONENT
OF
T
LEADING
TO
MULTIPLE
INDEPENDENT
REGRESSION
PROBLEMS
HOWEVER
A
MORE
INTERESTING
AND
MORE
COMMON
APPROACH
IS
TO
USE
THE
SAME
SET
OF
BASIS
FUNCTIONS
TO
MODEL
ALL
OF
THE
COMPONENTS
OF
THE
TARGET
VECTOR
SO
THAT
Y
X
W
WTΦ
X
WHERE
Y
IS
A
K
DIMENSIONAL
COLUMN
VECTOR
W
IS
AN
M
K
MATRIX
OF
PARAMETERS
AND
Φ
X
IS
AN
M
DIMENSIONAL
COLUMN
VECTOR
WITH
ELEMENTS
ΦJ
X
WITH
X
AS
BEFORE
SUPPOSE
WE
TAKE
THE
CONDITIONAL
DISTRIBUTION
OF
THE
TARGET
VECTOR
TO
BE
AN
ISOTROPIC
GAUSSIAN
OF
THE
FORM
P
T
X
W
Β
N
T
WTΦ
X
Β
IF
WE
HAVE
A
SET
OF
OBSERVATIONS
TN
WE
CAN
COMBINE
THESE
INTO
A
MATRIX
T
OF
SIZE
N
K
SUCH
THAT
THE
NTH
ROW
IS
GIVEN
BY
TT
SIMILARLY
WE
CAN
COMBINE
THE
INPUT
VECTORS
XN
INTO
A
MATRIX
X
THE
LOG
LIKELIHOOD
FUNCTION
IS
THEN
GIVEN
BY
LN
P
T
X
W
Β
N
LN
N
TN
WTΦ
XN
Β
N
NK
Β
Β
I
AS
BEFORE
WE
CAN
MAXIMIZE
THIS
FUNCTION
WITH
RESPECT
TO
W
GIVING
WML
ΦTΦ
ΦTT
IF
WE
EXAMINE
THIS
RESULT
FOR
EACH
TARGET
VARIABLE
TK
WE
HAVE
W
ΦTΦ
ΦTT
Φ
TK
EXERCISE
WHERE
TK
IS
AN
N
DIMENSIONAL
COLUMN
VECTOR
WITH
COMPONENTS
TNK
FOR
N
N
THUS
THE
SOLUTION
TO
THE
REGRESSION
PROBLEM
DECOUPLES
BETWEEN
THE
DIFFERENT
TARGET
VARIABLES
AND
WE
NEED
ONLY
COMPUTE
A
SINGLE
PSEUDO
INVERSE
MATRIX
Φ
WHICH
IS
SHARED
BY
ALL
OF
THE
VECTORS
WK
THE
EXTENSION
TO
GENERAL
GAUSSIAN
NOISE
DISTRIBUTIONS
HAVING
ARBITRARY
COVARI
ANCE
MATRICES
IS
STRAIGHTFORWARD
AGAIN
THIS
LEADS
TO
A
DECOUPLING
INTO
K
INDE
PENDENT
REGRESSION
PROBLEMS
THIS
RESULT
IS
UNSURPRISING
BECAUSE
THE
PARAMETERS
W
DEFINE
ONLY
THE
MEAN
OF
THE
GAUSSIAN
NOISE
DISTRIBUTION
AND
WE
KNOW
FROM
SEC
TION
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
MEAN
OF
A
MULTIVARIATE
GAUS
SIAN
IS
INDEPENDENT
OF
THE
COVARIANCE
FROM
NOW
ON
WE
SHALL
THEREFORE
CONSIDER
A
SINGLE
TARGET
VARIABLE
T
FOR
SIMPLICITY
THE
BIAS
VARIANCE
DECOMPOSITION
SO
FAR
IN
OUR
DISCUSSION
OF
LINEAR
MODELS
FOR
REGRESSION
WE
HAVE
ASSUMED
THAT
THE
FORM
AND
NUMBER
OF
BASIS
FUNCTIONS
ARE
BOTH
FIXED
AS
WE
HAVE
SEEN
IN
CHAPTER
THE
USE
OF
MAXIMUM
LIKELIHOOD
OR
EQUIVALENTLY
LEAST
SQUARES
CAN
LEAD
TO
SEVERE
OVER
FITTING
IF
COMPLEX
MODELS
ARE
TRAINED
USING
DATA
SETS
OF
LIMITED
SIZE
HOWEVER
LIMITING
THE
NUMBER
OF
BASIS
FUNCTIONS
IN
ORDER
TO
AVOID
OVER
FITTING
HAS
THE
SIDE
EFFECT
OF
LIMITING
THE
FLEXIBILITY
OF
THE
MODEL
TO
CAPTURE
INTERESTING
AND
IMPORTANT
TRENDS
IN
THE
DATA
ALTHOUGH
THE
INTRODUCTION
OF
REGULARIZATION
TERMS
CAN
CONTROL
OVER
FITTING
FOR
MODELS
WITH
MANY
PARAMETERS
THIS
RAISES
THE
QUESTION
OF
HOW
TO
DETERMINE
A
SUITABLE
VALUE
FOR
THE
REGULARIZATION
COEFFICIENT
Λ
SEEKING
THE
SOLUTION
THAT
MINIMIZES
THE
REGULARIZED
ERROR
FUNCTION
WITH
RESPECT
TO
BOTH
THE
WEIGHT
VECTOR
W
AND
THE
REGULARIZATION
COEFFICIENT
Λ
IS
CLEARLY
NOT
THE
RIGHT
APPROACH
SINCE
THIS
LEADS
TO
THE
UNREGULARIZED
SOLUTION
WITH
Λ
AS
WE
HAVE
SEEN
IN
EARLIER
CHAPTERS
THE
PHENOMENON
OF
OVER
FITTING
IS
REALLY
AN
UNFORTUNATE
PROPERTY
OF
MAXIMUM
LIKELIHOOD
AND
DOES
NOT
ARISE
WHEN
WE
MARGINALIZE
OVER
PARAMETERS
IN
A
BAYESIAN
SETTING
IN
THIS
CHAPTER
WE
SHALL
CONSIDER
THE
BAYESIAN
VIEW
OF
MODEL
COMPLEXITY
IN
SOME
DEPTH
BEFORE
DOING
SO
HOWEVER
IT
IS
INSTRUCTIVE
TO
CONSIDER
A
FREQUENTIST
VIEWPOINT
OF
THE
MODEL
COMPLEXITY
ISSUE
KNOWN
AS
THE
BIAS
VARIANCE
TRADE
OFF
ALTHOUGH
WE
SHALL
INTRODUCE
THIS
CONCEPT
IN
THE
CONTEXT
OF
LINEAR
BASIS
FUNCTION
MODELS
WHERE
IT
IS
EASY
TO
ILLUSTRATE
THE
IDEAS
USING
SIMPLE
EXAMPLES
THE
DISCUSSION
HAS
MORE
GENERAL
APPLICABILITY
IN
SECTION
WHEN
WE
DISCUSSED
DECISION
THEORY
FOR
REGRESSION
PROBLEMS
WE
CONSIDERED
VARIOUS
LOSS
FUNCTIONS
EACH
OF
WHICH
LEADS
TO
A
CORRESPONDING
OPTIMAL
PREDICTION
ONCE
WE
ARE
GIVEN
THE
CONDITIONAL
DISTRIBUTION
P
T
X
A
POPULAR
CHOICE
IS
THE
SQUARED
LOSS
FUNCTION
FOR
WHICH
THE
OPTIMAL
PREDICTION
IS
GIVEN
BY
THE
CONDITIONAL
EXPECTATION
WHICH
WE
DENOTE
BY
H
X
AND
WHICH
IS
GIVEN
BY
H
X
E
T
X
R
TP
T
X
DT
AT
THIS
POINT
IT
IS
WORTH
DISTINGUISHING
BETWEEN
THE
SQUARED
LOSS
FUNCTION
ARISING
FROM
DECISION
THEORY
AND
THE
SUM
OF
SQUARES
ERROR
FUNCTION
THAT
AROSE
IN
THE
MAXI
MUM
LIKELIHOOD
ESTIMATION
OF
MODEL
PARAMETERS
WE
MIGHT
USE
MORE
SOPHISTICATED
TECHNIQUES
THAN
LEAST
SQUARES
FOR
EXAMPLE
REGULARIZATION
OR
A
FULLY
BAYESIAN
AP
PROACH
TO
DETERMINE
THE
CONDITIONAL
DISTRIBUTION
P
T
X
THESE
CAN
ALL
BE
COMBINED
WITH
THE
SQUARED
LOSS
FUNCTION
FOR
THE
PURPOSE
OF
MAKING
PREDICTIONS
WE
SHOWED
IN
SECTION
THAT
THE
EXPECTED
SQUARED
LOSS
CAN
BE
WRITTEN
IN
THE
FORM
E
L
R
Y
X
H
X
P
X
DX
R
H
X
T
X
T
DX
DT
RECALL
THAT
THE
SECOND
TERM
WHICH
IS
INDEPENDENT
OF
Y
X
ARISES
FROM
THE
INTRINSIC
NOISE
ON
THE
DATA
AND
REPRESENTS
THE
MINIMUM
ACHIEVABLE
VALUE
OF
THE
EXPECTED
LOSS
THE
FIRST
TERM
DEPENDS
ON
OUR
CHOICE
FOR
THE
FUNCTION
Y
X
AND
WE
WILL
SEEK
A
SO
LUTION
FOR
Y
X
WHICH
MAKES
THIS
TERM
A
MINIMUM
BECAUSE
IT
IS
NONNEGATIVE
THE
SMALLEST
THAT
WE
CAN
HOPE
TO
MAKE
THIS
TERM
IS
ZERO
IF
WE
HAD
AN
UNLIMITED
SUPPLY
OF
DATA
AND
UNLIMITED
COMPUTATIONAL
RESOURCES
WE
COULD
IN
PRINCIPLE
FIND
THE
REGRES
SION
FUNCTION
H
X
TO
ANY
DESIRED
DEGREE
OF
ACCURACY
AND
THIS
WOULD
REPRESENT
THE
OPTIMAL
CHOICE
FOR
Y
X
HOWEVER
IN
PRACTICE
WE
HAVE
A
DATA
SET
CONTAINING
ONLY
A
FINITE
NUMBER
N
OF
DATA
POINTS
AND
CONSEQUENTLY
WE
DO
NOT
KNOW
THE
REGRESSION
FUNCTION
H
X
EXACTLY
IF
WE
MODEL
THE
H
X
USING
A
PARAMETRIC
FUNCTION
Y
X
W
GOVERNED
BY
A
PA
RAMETER
VECTOR
W
THEN
FROM
A
BAYESIAN
PERSPECTIVE
THE
UNCERTAINTY
IN
OUR
MODEL
IS
EXPRESSED
THROUGH
A
POSTERIOR
DISTRIBUTION
OVER
W
A
FREQUENTIST
TREATMENT
HOWEVER
INVOLVES
MAKING
A
POINT
ESTIMATE
OF
W
BASED
ON
THE
DATA
SET
AND
TRIES
INSTEAD
TO
INTERPRET
THE
UNCERTAINTY
OF
THIS
ESTIMATE
THROUGH
THE
FOLLOWING
THOUGHT
EXPERI
MENT
SUPPOSE
WE
HAD
A
LARGE
NUMBER
OF
DATA
SETS
EACH
OF
SIZE
N
AND
EACH
DRAWN
INDEPENDENTLY
FROM
THE
DISTRIBUTION
P
T
X
FOR
ANY
GIVEN
DATA
SET
WE
CAN
RUN
OUR
LEARNING
ALGORITHM
AND
OBTAIN
A
PREDICTION
FUNCTION
Y
X
DIFFERENT
DATA
SETS
FROM
THE
ENSEMBLE
WILL
GIVE
DIFFERENT
FUNCTIONS
AND
CONSEQUENTLY
DIFFERENT
VALUES
OF
THE
SQUARED
LOSS
THE
PERFORMANCE
OF
A
PARTICULAR
LEARNING
ALGORITHM
IS
THEN
ASSESSED
BY
TAKING
THE
AVERAGE
OVER
THIS
ENSEMBLE
OF
DATA
SETS
CONSIDER
THE
INTEGRAND
OF
THE
FIRST
TERM
IN
WHICH
FOR
A
PARTICULAR
DATA
SET
D
TAKES
THE
FORM
Y
X
D
H
X
BECAUSE
THIS
QUANTITY
WILL
BE
DEPENDENT
ON
THE
PARTICULAR
DATA
SET
D
WE
TAKE
ITS
AVER
AGE
OVER
THE
ENSEMBLE
OF
DATA
SETS
IF
WE
ADD
AND
SUBTRACT
THE
QUANTITY
ED
Y
X
D
INSIDE
THE
BRACES
AND
THEN
EXPAND
WE
OBTAIN
Y
X
D
ED
Y
X
D
ED
Y
X
D
H
X
Y
X
D
ED
Y
X
D
ED
Y
X
D
H
X
Y
X
D
ED
Y
X
D
ED
Y
X
D
H
X
WE
NOW
TAKE
THE
EXPECTATION
OF
THIS
EXPRESSION
WITH
RESPECT
TO
AND
NOTE
THAT
THE
FINAL
TERM
WILL
VANISH
GIVING
E
Y
X
H
X
BIVA
VARVIA
NCE
WE
SEE
THAT
THE
EXPECTED
SQUARED
DIFFERENCE
BETWEEN
Y
X
AND
THE
REGRESSION
FUNCTION
H
X
CAN
BE
EXPRESSED
AS
THE
SUM
OF
TWO
TERMS
THE
FIRST
TERM
CALLED
THE
SQUARED
BIAS
REPRESENTS
THE
EXTENT
TO
WHICH
THE
AVERAGE
PREDICTION
OVER
ALL
DATA
SETS
DIFFERS
FROM
THE
DESIRED
REGRESSION
FUNCTION
THE
SECOND
TERM
CALLED
THE
VARIANCE
MEASURES
THE
EXTENT
TO
WHICH
THE
SOLUTIONS
FOR
INDIVIDUAL
DATA
SETS
VARY
AROUND
THEIR
AVERAGE
AND
HENCE
THIS
MEASURES
THE
EXTENT
TO
WHICH
THE
FUNCTION
Y
X
IS
SENSITIVE
TO
THE
PARTICULAR
CHOICE
OF
DATA
SET
WE
SHALL
PROVIDE
SOME
INTUITION
TO
SUPPORT
THESE
DEFINITIONS
SHORTLY
WHEN
WE
CONSIDER
A
SIMPLE
EXAMPLE
SO
FAR
WE
HAVE
CONSIDERED
A
SINGLE
INPUT
VALUE
X
IF
WE
SUBSTITUTE
THIS
EXPANSION
BACK
INTO
WE
OBTAIN
THE
FOLLOWING
DECOMPOSITION
OF
THE
EXPECTED
SQUARED
LOSS
EXPECTED
LOSS
BIAS
VARIANCE
NOISE
WHERE
BIAS
R
ED
Y
X
D
H
X
X
DX
VARIANCE
R
ED
Y
X
D
ED
Y
X
D
P
X
DX
NOISE
R
H
X
T
X
T
DX
DT
APPENDIX
A
AND
THE
BIAS
AND
VARIANCE
TERMS
NOW
REFER
TO
INTEGRATED
QUANTITIES
OUR
GOAL
IS
TO
MINIMIZE
THE
EXPECTED
LOSS
WHICH
WE
HAVE
DECOMPOSED
INTO
THE
SUM
OF
A
SQUARED
BIAS
A
VARIANCE
AND
A
CONSTANT
NOISE
TERM
AS
WE
SHALL
SEE
THERE
IS
A
TRADE
OFF
BETWEEN
BIAS
AND
VARIANCE
WITH
VERY
FLEXIBLE
MODELS
HAVING
LOW
BIAS
AND
HIGH
VARIANCE
AND
RELATIVELY
RIGID
MODELS
HAVING
HIGH
BIAS
AND
LOW
VARIANCE
THE
MODEL
WITH
THE
OPTIMAL
PREDICTIVE
CAPABILITY
IS
THE
ONE
THAT
LEADS
TO
THE
BEST
BALANCE
BETWEEN
BIAS
AND
VARIANCE
THIS
IS
ILLUSTRATED
BY
CONSIDERING
THE
SINUSOIDAL
DATA
SET
FROM
CHAPTER
HERE
WE
GENERATE
DATA
SETS
EACH
CONTAINING
N
DATA
POINTS
INDEPENDENTLY
FROM
THE
SINUSOIDAL
CURVE
H
X
SIN
THE
DATA
SETS
ARE
INDEXED
BY
L
L
WHERE
L
AND
FOR
EACH
DATA
SET
D
L
WE
T
T
X
X
T
T
X
X
T
T
X
X
FIGURE
ILLUSTRATION
OF
THE
DEPENDENCE
OF
BIAS
AND
VARIANCE
ON
MODEL
COMPLEXITY
GOVERNED
BY
A
REGULARIZA
TION
PARAMETER
Λ
USING
THE
SINUSOIDAL
DATA
SET
FROM
CHAPTER
THERE
ARE
L
DATA
SETS
EACH
HAVING
N
DATA
POINTS
AND
THERE
ARE
GAUSSIAN
BASIS
FUNCTIONS
IN
THE
MODEL
SO
THAT
THE
TOTAL
NUMBER
OF
PARAMETERS
IS
M
INCLUDING
THE
BIAS
PARAMETER
THE
LEFT
COLUMN
SHOWS
THE
RESULT
OF
FITTING
THE
MODEL
TO
THE
DATA
SETS
FOR
VARIOUS
VALUES
OF
LN
Λ
FOR
CLARITY
ONLY
OF
THE
FITS
ARE
SHOWN
THE
RIGHT
COLUMN
SHOWS
THE
CORRESPONDING
AVERAGE
OF
THE
FITS
RED
ALONG
WITH
THE
SINUSOIDAL
FUNCTION
FROM
WHICH
THE
DATA
SETS
WERE
GENERATED
GREEN
FIGURE
PLOT
OF
SQUARED
BIAS
AND
VARIANCE
TOGETHER
WITH
THEIR
SUM
CORRESPOND
ING
TO
THE
RESULTS
SHOWN
IN
FIG
URE
ALSO
SHOWN
IS
THE
AVERAGE
TEST
SET
ERROR
FOR
A
TEST
DATA
SET
SIZE
OF
POINTS
THE
MINIMUM
VALUE
OF
BIAS
VARIANCE
OCCURS
AROUND
LN
Λ
WHICH
IS
CLOSE
TO
THE
VALUE
THAT
GIVES
THE
MINIMUM
ERROR
ON
THE
TEST
DATA
LN
Λ
FIT
A
MODEL
WITH
GAUSSIAN
BASIS
FUNCTIONS
BY
MINIMIZING
THE
REGULARIZED
ERROR
FUNCTION
TO
GIVE
A
PREDICTION
FUNCTION
Y
L
X
AS
SHOWN
IN
FIGURE
THE
TOP
ROW
CORRESPONDS
TO
A
LARGE
VALUE
OF
THE
REGULARIZATION
COEFFICIENT
Λ
THAT
GIVES
LOW
VARIANCE
BECAUSE
THE
RED
CURVES
IN
THE
LEFT
PLOT
LOOK
SIMILAR
BUT
HIGH
BIAS
BECAUSE
THE
TWO
CURVES
IN
THE
RIGHT
PLOT
ARE
VERY
DIFFERENT
CONVERSELY
ON
THE
BOTTOM
ROW
FOR
WHICH
Λ
IS
SMALL
THERE
IS
LARGE
VARIANCE
SHOWN
BY
THE
HIGH
VARIABILITY
BETWEEN
THE
RED
CURVES
IN
THE
LEFT
PLOT
BUT
LOW
BIAS
SHOWN
BY
THE
GOOD
FIT
BETWEEN
THE
AVERAGE
MODEL
FIT
AND
THE
ORIGINAL
SINUSOIDAL
FUNCTION
NOTE
THAT
THE
RESULT
OF
AVERAGING
MANY
SOLUTIONS
FOR
THE
COMPLEX
MODEL
WITH
M
IS
A
VERY
GOOD
FIT
TO
THE
REGRESSION
FUNCTION
WHICH
SUGGESTS
THAT
AVERAGING
MAY
BE
A
BENEFICIAL
PROCEDURE
INDEED
A
WEIGHTED
AVERAGING
OF
MULTIPLE
SOLUTIONS
LIES
AT
THE
HEART
OF
A
BAYESIAN
APPROACH
ALTHOUGH
THE
AVERAGING
IS
WITH
RESPECT
TO
THE
POSTERIOR
DISTRIBUTION
OF
PARAMETERS
NOT
WITH
RESPECT
TO
MULTIPLE
DATA
SETS
WE
CAN
ALSO
EXAMINE
THE
BIAS
VARIANCE
TRADE
OFF
QUANTITATIVELY
FOR
THIS
EXAMPLE
THE
AVERAGE
PREDICTION
IS
ESTIMATED
FROM
L
Y
X
Y
L
X
L
L
AND
THE
INTEGRATED
SQUARED
BIAS
AND
INTEGRATED
VARIANCE
ARE
THEN
GIVEN
BY
BIAS
N
N
Y
XN
N
H
XN
N
L
VARIANCE
Y
L
X
Y
X
WHERE
THE
INTEGRAL
OVER
X
WEIGHTED
BY
THE
DISTRIBUTION
P
X
IS
APPROXIMATED
BY
A
FINITE
SUM
OVER
DATA
POINTS
DRAWN
FROM
THAT
DISTRIBUTION
THESE
QUANTITIES
ALONG
WITH
THEIR
SUM
ARE
PLOTTED
AS
A
FUNCTION
OF
LN
Λ
IN
FIGURE
WE
SEE
THAT
SMALL
VALUES
OF
Λ
ALLOW
THE
MODEL
TO
BECOME
FINELY
TUNED
TO
THE
NOISE
ON
EACH
INDIVIDUAL
DATA
SET
LEADING
TO
LARGE
VARIANCE
CONVERSELY
A
LARGE
VALUE
OF
Λ
PULLS
THE
WEIGHT
PARAMETERS
TOWARDS
ZERO
LEADING
TO
LARGE
BIAS
ALTHOUGH
THE
BIAS
VARIANCE
DECOMPOSITION
MAY
PROVIDE
SOME
INTERESTING
IN
SIGHTS
INTO
THE
MODEL
COMPLEXITY
ISSUE
FROM
A
FREQUENTIST
PERSPECTIVE
IT
IS
OF
LIM
ITED
PRACTICAL
VALUE
BECAUSE
THE
BIAS
VARIANCE
DECOMPOSITION
IS
BASED
ON
AVERAGES
WITH
RESPECT
TO
ENSEMBLES
OF
DATA
SETS
WHEREAS
IN
PRACTICE
WE
HAVE
ONLY
THE
SINGLE
OBSERVED
DATA
SET
IF
WE
HAD
A
LARGE
NUMBER
OF
INDEPENDENT
TRAINING
SETS
OF
A
GIVEN
SIZE
WE
WOULD
BE
BETTER
OFF
COMBINING
THEM
INTO
A
SINGLE
LARGE
TRAINING
SET
WHICH
OF
COURSE
WOULD
REDUCE
THE
LEVEL
OF
OVER
FITTING
FOR
A
GIVEN
MODEL
COMPLEXITY
GIVEN
THESE
LIMITATIONS
WE
TURN
IN
THE
NEXT
SECTION
TO
A
BAYESIAN
TREATMENT
OF
LINEAR
BASIS
FUNCTION
MODELS
WHICH
NOT
ONLY
PROVIDES
POWERFUL
INSIGHTS
INTO
THE
ISSUES
OF
OVER
FITTING
BUT
WHICH
ALSO
LEADS
TO
PRACTICAL
TECHNIQUES
FOR
ADDRESSING
THE
QUESTION
MODEL
COMPLEXITY
BAYESIAN
LINEAR
REGRESSION
IN
OUR
DISCUSSION
OF
MAXIMUM
LIKELIHOOD
FOR
SETTING
THE
PARAMETERS
OF
A
LINEAR
RE
GRESSION
MODEL
WE
HAVE
SEEN
THAT
THE
EFFECTIVE
MODEL
COMPLEXITY
GOVERNED
BY
THE
NUMBER
OF
BASIS
FUNCTIONS
NEEDS
TO
BE
CONTROLLED
ACCORDING
TO
THE
SIZE
OF
THE
DATA
SET
ADDING
A
REGULARIZATION
TERM
TO
THE
LOG
LIKELIHOOD
FUNCTION
MEANS
THE
EFFECTIVE
MODEL
COMPLEXITY
CAN
THEN
BE
CONTROLLED
BY
THE
VALUE
OF
THE
REGULARIZATION
COEFFI
CIENT
ALTHOUGH
THE
CHOICE
OF
THE
NUMBER
AND
FORM
OF
THE
BASIS
FUNCTIONS
IS
OF
COURSE
STILL
IMPORTANT
IN
DETERMINING
THE
OVERALL
BEHAVIOUR
OF
THE
MODEL
THIS
LEAVES
THE
ISSUE
OF
DECIDING
THE
APPROPRIATE
MODEL
COMPLEXITY
FOR
THE
PAR
TICULAR
PROBLEM
WHICH
CANNOT
BE
DECIDED
SIMPLY
BY
MAXIMIZING
THE
LIKELIHOOD
FUNC
TION
BECAUSE
THIS
ALWAYS
LEADS
TO
EXCESSIVELY
COMPLEX
MODELS
AND
OVER
FITTING
IN
DEPENDENT
HOLD
OUT
DATA
CAN
BE
USED
TO
DETERMINE
MODEL
COMPLEXITY
AS
DISCUSSED
IN
SECTION
BUT
THIS
CAN
BE
BOTH
COMPUTATIONALLY
EXPENSIVE
AND
WASTEFUL
OF
VALU
ABLE
DATA
WE
THEREFORE
TURN
TO
A
BAYESIAN
TREATMENT
OF
LINEAR
REGRESSION
WHICH
WILL
AVOID
THE
OVER
FITTING
PROBLEM
OF
MAXIMUM
LIKELIHOOD
AND
WHICH
WILL
ALSO
LEAD
TO
AUTOMATIC
METHODS
OF
DETERMINING
MODEL
COMPLEXITY
USING
THE
TRAINING
DATA
ALONE
AGAIN
FOR
SIMPLICITY
WE
WILL
FOCUS
ON
THE
CASE
OF
A
SINGLE
TARGET
VARIABLE
T
EX
TENSION
TO
MULTIPLE
TARGET
VARIABLES
IS
STRAIGHTFORWARD
AND
FOLLOWS
THE
DISCUSSION
OF
SECTION
PARAMETER
DISTRIBUTION
WE
BEGIN
OUR
DISCUSSION
OF
THE
BAYESIAN
TREATMENT
OF
LINEAR
REGRESSION
BY
IN
TRODUCING
A
PRIOR
PROBABILITY
DISTRIBUTION
OVER
THE
MODEL
PARAMETERS
W
FOR
THE
MO
MENT
WE
SHALL
TREAT
THE
NOISE
PRECISION
PARAMETER
Β
AS
A
KNOWN
CONSTANT
FIRST
NOTE
THAT
THE
LIKELIHOOD
FUNCTION
P
T
W
DEFINED
BY
IS
THE
EXPONENTIAL
OF
A
QUADRATIC
FUNCTION
OF
W
THE
CORRESPONDING
CONJUGATE
PRIOR
IS
THEREFORE
GIVEN
BY
A
GAUSSIAN
DISTRIBUTION
OF
THE
FORM
P
W
N
W
HAVING
MEAN
AND
COVARIANCE
EXERCISE
NEXT
WE
COMPUTE
THE
POSTERIOR
DISTRIBUTION
WHICH
IS
PROPORTIONAL
TO
THE
PRODUCT
OF
THE
LIKELIHOOD
FUNCTION
AND
THE
PRIOR
DUE
TO
THE
CHOICE
OF
A
CONJUGATE
GAUS
SIAN
PRIOR
DISTRIBUTION
THE
POSTERIOR
WILL
ALSO
BE
GAUSSIAN
WE
CAN
EVALUATE
THIS
DISTRIBUTION
BY
THE
USUAL
PROCEDURE
OF
COMPLETING
THE
SQUARE
IN
THE
EXPONENTIAL
AND
THEN
FINDING
THE
NORMALIZATION
COEFFICIENT
USING
THE
STANDARD
RESULT
FOR
A
NORMALIZED
GAUSSIAN
HOWEVER
WE
HAVE
ALREADY
DONE
THE
NECESSARY
WORK
IN
DERIVING
THE
GEN
ERAL
RESULT
WHICH
ALLOWS
US
TO
WRITE
DOWN
THE
POSTERIOR
DISTRIBUTION
DIRECTLY
IN
THE
FORM
WHERE
P
W
T
N
W
MN
SN
MN
SN
ΒΦTT
ΒΦTΦ
EXERCISE
NOTE
THAT
BECAUSE
THE
POSTERIOR
DISTRIBUTION
IS
GAUSSIAN
ITS
MODE
COINCIDES
WITH
ITS
MEAN
THUS
THE
MAXIMUM
POSTERIOR
WEIGHT
VECTOR
IS
SIMPLY
GIVEN
BY
WMAP
MN
IF
WE
CONSIDER
AN
INFINITELY
BROAD
PRIOR
Α
WITH
Α
THE
MEAN
MN
OF
THE
POSTERIOR
DISTRIBUTION
REDUCES
TO
THE
MAXIMUM
LIKELIHOOD
VALUE
WML
GIVEN
BY
SIMILARLY
IF
N
THEN
THE
POSTERIOR
DISTRIBUTION
REVERTS
TO
THE
PRIOR
FURTHERMORE
IF
DATA
POINTS
ARRIVE
SEQUENTIALLY
THEN
THE
POSTERIOR
DISTRIBUTION
AT
ANY
STAGE
ACTS
AS
THE
PRIOR
DISTRIBUTION
FOR
THE
SUBSEQUENT
DATA
POINT
SUCH
THAT
THE
NEW
POSTERIOR
DISTRIBUTION
IS
AGAIN
GIVEN
BY
FOR
THE
REMAINDER
OF
THIS
CHAPTER
WE
SHALL
CONSIDER
A
PARTICULAR
FORM
OF
GAUS
SIAN
PRIOR
IN
ORDER
TO
SIMPLIFY
THE
TREATMENT
SPECIFICALLY
WE
CONSIDER
A
ZERO
MEAN
ISOTROPIC
GAUSSIAN
GOVERNED
BY
A
SINGLE
PRECISION
PARAMETER
Α
SO
THAT
P
W
Α
N
W
Α
AND
THE
CORRESPONDING
POSTERIOR
DISTRIBUTION
OVER
W
IS
THEN
GIVEN
BY
WITH
MN
ΒSNΦTT
ΑI
ΒΦTΦ
THE
LOG
OF
THE
POSTERIOR
DISTRIBUTION
IS
GIVEN
BY
THE
SUM
OF
THE
LOG
LIKELIHOOD
AND
THE
LOG
OF
THE
PRIOR
AND
AS
A
FUNCTION
OF
W
TAKES
THE
FORM
N
LN
W
T
Β
N
N
WTΦ
XN
Α
WTW
CONST
MAXIMIZATION
OF
THIS
POSTERIOR
DISTRIBUTION
WITH
RESPECT
TO
W
IS
THEREFORE
EQUIVA
LENT
TO
THE
MINIMIZATION
OF
THE
SUM
OF
SQUARES
ERROR
FUNCTION
WITH
THE
ADDITION
OF
A
QUADRATIC
REGULARIZATION
TERM
CORRESPONDING
TO
WITH
Λ
Α
Β
WE
CAN
ILLUSTRATE
BAYESIAN
LEARNING
IN
A
LINEAR
BASIS
FUNCTION
MODEL
AS
WELL
AS
THE
SEQUENTIAL
UPDATE
OF
A
POSTERIOR
DISTRIBUTION
USING
A
SIMPLE
EXAMPLE
INVOLVING
STRAIGHT
LINE
FITTING
CONSIDER
A
SINGLE
INPUT
VARIABLE
X
A
SINGLE
TARGET
VARIABLE
T
AND
A
LINEAR
MODEL
OF
THE
FORM
Y
X
W
BECAUSE
THIS
HAS
JUST
TWO
ADAP
TIVE
PARAMETERS
WE
CAN
PLOT
THE
PRIOR
AND
POSTERIOR
DISTRIBUTIONS
DIRECTLY
IN
PARAMETER
SPACE
WE
GENERATE
SYNTHETIC
DATA
FROM
THE
FUNCTION
F
X
A
WITH
PARAM
ETER
VALUES
AND
BY
FIRST
CHOOSING
VALUES
OF
XN
FROM
THE
UNIFORM
DISTRIBUTION
U
X
THEN
EVALUATING
F
XN
A
AND
FINALLY
ADDING
GAUSSIAN
NOISE
WITH
STANDARD
DEVIATION
OF
TO
OBTAIN
THE
TARGET
VALUES
TN
OUR
GOAL
IS
TO
RECOVER
THE
VALUES
OF
AND
FROM
SUCH
DATA
AND
WE
WILL
EXPLORE
THE
DEPENDENCE
ON
THE
SIZE
OF
THE
DATA
SET
WE
ASSUME
HERE
THAT
THE
NOISE
VARIANCE
IS
KNOWN
AND
HENCE
WE
SET
THE
PRECISION
PARAMETER
TO
ITS
TRUE
VALUE
Β
SIMILARLY
WE
FIX
THE
PARAMETER
Α
TO
WE
SHALL
SHORTLY
DISCUSS
STRATEGIES
FOR
DETERMINING
Α
AND
Β
FROM
THE
TRAINING
DATA
FIGURE
SHOWS
THE
RESULTS
OF
BAYESIAN
LEARNING
IN
THIS
MODEL
AS
THE
SIZE
OF
THE
DATA
SET
IS
INCREASED
AND
DEMONSTRATES
THE
SEQUENTIAL
NATURE
OF
BAYESIAN
LEARNING
IN
WHICH
THE
CURRENT
POSTERIOR
DISTRIBUTION
FORMS
THE
PRIOR
WHEN
A
NEW
DATA
POINT
IS
OBSERVED
IT
IS
WORTH
TAKING
TIME
TO
STUDY
THIS
FIGURE
IN
DETAIL
AS
IT
ILLUSTRATES
SEVERAL
IMPORTANT
ASPECTS
OF
BAYESIAN
INFERENCE
THE
FIRST
ROW
OF
THIS
FIGURE
CORRESPONDS
TO
THE
SITUATION
BEFORE
ANY
DATA
POINTS
ARE
OBSERVED
AND
SHOWS
A
PLOT
OF
THE
PRIOR
DISTRIBUTION
IN
W
SPACE
TOGETHER
WITH
SIX
SAMPLES
OF
THE
FUNCTION
Y
X
W
IN
WHICH
THE
VALUES
OF
W
ARE
DRAWN
FROM
THE
PRIOR
IN
THE
SECOND
ROW
WE
SEE
THE
SITUATION
AFTER
OBSERVING
A
SINGLE
DATA
POINT
THE
LOCATION
X
T
OF
THE
DATA
POINT
IS
SHOWN
BY
A
BLUE
CIRCLE
IN
THE
RIGHT
HAND
COLUMN
IN
THE
LEFT
HAND
COLUMN
IS
A
PLOT
OF
THE
LIKELIHOOD
FUNCTION
P
T
X
W
FOR
THIS
DATA
POINT
AS
A
FUNCTION
OF
W
NOTE
THAT
THE
LIKELIHOOD
FUNCTION
PROVIDES
A
SOFT
CONSTRAINT
THAT
THE
LINE
MUST
PASS
CLOSE
TO
THE
DATA
POINT
WHERE
CLOSE
IS
DETERMINED
BY
THE
NOISE
PRECISION
Β
FOR
COMPARISON
THE
TRUE
PARAMETER
VALUES
AND
USED
TO
GENERATE
THE
DATA
SET
ARE
SHOWN
BY
A
WHITE
CROSS
IN
THE
PLOTS
IN
THE
LEFT
COLUMN
OF
FIGURE
WHEN
WE
MULTIPLY
THIS
LIKELIHOOD
FUNCTION
BY
THE
PRIOR
FROM
THE
TOP
ROW
AND
NORMALIZE
WE
OBTAIN
THE
POSTERIOR
DISTRIBUTION
SHOWN
IN
THE
MIDDLE
PLOT
ON
THE
SECOND
ROW
SAM
PLES
OF
THE
REGRESSION
FUNCTION
Y
X
W
OBTAINED
BY
DRAWING
SAMPLES
OF
W
FROM
THIS
POSTERIOR
DISTRIBUTION
ARE
SHOWN
IN
THE
RIGHT
HAND
PLOT
NOTE
THAT
THESE
SAMPLE
LINES
ALL
PASS
CLOSE
TO
THE
DATA
POINT
THE
THIRD
ROW
OF
THIS
FIGURE
SHOWS
THE
EFFECT
OF
OB
SERVING
A
SECOND
DATA
POINT
AGAIN
SHOWN
BY
A
BLUE
CIRCLE
IN
THE
PLOT
IN
THE
RIGHT
HAND
COLUMN
THE
CORRESPONDING
LIKELIHOOD
FUNCTION
FOR
THIS
SECOND
DATA
POINT
ALONE
IS
SHOWN
IN
THE
LEFT
PLOT
WHEN
WE
MULTIPLY
THIS
LIKELIHOOD
FUNCTION
BY
THE
POSTERIOR
DISTRIBUTION
FROM
THE
SECOND
ROW
WE
OBTAIN
THE
POSTERIOR
DISTRIBUTION
SHOWN
IN
THE
MIDDLE
PLOT
OF
THE
THIRD
ROW
NOTE
THAT
THIS
IS
EXACTLY
THE
SAME
POSTERIOR
DISTRIBUTION
AS
WOULD
BE
OBTAINED
BY
COMBINING
THE
ORIGINAL
PRIOR
WITH
THE
LIKELIHOOD
FUNCTION
FOR
THE
TWO
DATA
POINTS
THIS
POSTERIOR
HAS
NOW
BEEN
INFLUENCED
BY
TWO
DATA
POINTS
AND
BECAUSE
TWO
POINTS
ARE
SUFFICIENT
TO
DEFINE
A
LINE
THIS
ALREADY
GIVES
A
RELATIVELY
COMPACT
POSTERIOR
DISTRIBUTION
SAMPLES
FROM
THIS
POSTERIOR
DISTRIBUTION
GIVE
RISE
TO
THE
FUNCTIONS
SHOWN
IN
RED
IN
THE
THIRD
COLUMN
AND
WE
SEE
THAT
THESE
FUNCTIONS
PASS
CLOSE
TO
BOTH
OF
THE
DATA
POINTS
THE
FOURTH
ROW
SHOWS
THE
EFFECT
OF
OBSERVING
A
TOTAL
OF
DATA
POINTS
THE
LEFT
HAND
PLOT
SHOWS
THE
LIKELIHOOD
FUNCTION
FOR
THE
DATA
POINT
ALONE
AND
THE
MIDDLE
PLOT
SHOWS
THE
RESULTING
POSTERIOR
DISTRIBUTION
THAT
HAS
NOW
ABSORBED
INFORMATION
FROM
ALL
OBSERVATIONS
NOTE
HOW
THE
POSTERIOR
IS
MUCH
SHARPER
THAN
IN
THE
THIRD
ROW
IN
THE
LIMIT
OF
AN
INFINITE
NUMBER
OF
DATA
POINTS
THE
FIGURE
ILLUSTRATION
OF
SEQUENTIAL
BAYESIAN
LEARNING
FOR
A
SIMPLE
LINEAR
MODEL
OF
THE
FORM
Y
X
W
A
DETAILED
DESCRIPTION
OF
THIS
FIGURE
IS
GIVEN
IN
THE
TEXT
POSTERIOR
DISTRIBUTION
WOULD
BECOME
A
DELTA
FUNCTION
CENTRED
ON
THE
TRUE
PARAMETER
VALUES
SHOWN
BY
THE
WHITE
CROSS
OTHER
FORMS
OF
PRIOR
OVER
THE
PARAMETERS
CAN
BE
CONSIDERED
FOR
INSTANCE
WE
CAN
GENERALIZE
THE
GAUSSIAN
PRIOR
TO
GIVE
Q
Α
Q
M
Α
EXERCISE
IN
WHICH
Q
CORRESPONDS
TO
THE
GAUSSIAN
DISTRIBUTION
AND
ONLY
IN
THIS
CASE
IS
THE
PRIOR
CONJUGATE
TO
THE
LIKELIHOOD
FUNCTION
FINDING
THE
MAXIMUM
OF
THE
POSTE
RIOR
DISTRIBUTION
OVER
W
CORRESPONDS
TO
MINIMIZATION
OF
THE
REGULARIZED
ERROR
FUNCTION
IN
THE
CASE
OF
THE
GAUSSIAN
PRIOR
THE
MODE
OF
THE
POSTERIOR
DISTRIBUTION
WAS
EQUAL
TO
THE
MEAN
ALTHOUGH
THIS
WILL
NO
LONGER
HOLD
IF
Q
PREDICTIVE
DISTRIBUTION
IN
PRACTICE
WE
ARE
NOT
USUALLY
INTERESTED
IN
THE
VALUE
OF
W
ITSELF
BUT
RATHER
IN
MAKING
PREDICTIONS
OF
T
FOR
NEW
VALUES
OF
X
THIS
REQUIRES
THAT
WE
EVALUATE
THE
PREDICTIVE
DISTRIBUTION
DEFINED
BY
P
T
T
Α
Β
R
P
T
W
Β
P
W
T
Α
Β
DW
IN
WHICH
T
IS
THE
VECTOR
OF
TARGET
VALUES
FROM
THE
TRAINING
SET
AND
WE
HAVE
OMITTED
THE
CORRESPONDING
INPUT
VECTORS
FROM
THE
RIGHT
HAND
SIDE
OF
THE
CONDITIONING
STATEMENTS
TO
SIMPLIFY
THE
NOTATION
THE
CONDITIONAL
DISTRIBUTION
P
T
X
W
Β
OF
THE
TARGET
VARI
ABLE
IS
GIVEN
BY
AND
THE
POSTERIOR
WEIGHT
DISTRIBUTION
IS
GIVEN
BY
WE
SEE
THAT
INVOLVES
THE
CONVOLUTION
OF
TWO
GAUSSIAN
DISTRIBUTIONS
AND
SO
MAKING
USE
OF
THE
RESULT
FROM
SECTION
WE
SEE
THAT
THE
PREDICTIVE
DISTRIBUTION
TAKES
THE
FORM
P
T
X
T
Α
Β
N
T
MT
Φ
X
X
WHERE
THE
VARIANCE
X
OF
THE
PREDICTIVE
DISTRIBUTION
IS
GIVEN
BY
X
Β
Φ
X
TSN
Φ
X
THE
FIRST
TERM
IN
REPRESENTS
THE
NOISE
ON
THE
DATA
WHEREAS
THE
SECOND
TERM
REFLECTS
THE
UNCERTAINTY
ASSOCIATED
WITH
THE
PARAMETERS
W
BECAUSE
THE
NOISE
PROCESS
AND
THE
DISTRIBUTION
OF
W
ARE
INDEPENDENT
GAUSSIANS
THEIR
VARIANCES
ARE
ADDITIVE
NOTE
THAT
AS
ADDITIONAL
DATA
POINTS
ARE
OBSERVED
THE
POSTERIOR
DISTRIBUTION
BECOMES
NARROWER
AS
A
CONSEQUENCE
IT
CAN
BE
SHOWN
QAZAZ
ET
AL
THAT
X
EXERCISE
X
IN
THE
LIMIT
N
THE
SECOND
TERM
IN
GOES
TO
ZERO
AND
THE
VARIANCE
OF
THE
PREDICTIVE
DISTRIBUTION
ARISES
SOLELY
FROM
THE
ADDITIVE
NOISE
GOVERNED
BY
THE
PARAMETER
Β
AS
AN
ILLUSTRATION
OF
THE
PREDICTIVE
DISTRIBUTION
FOR
BAYESIAN
LINEAR
REGRESSION
MODELS
LET
US
RETURN
TO
THE
SYNTHETIC
SINUSOIDAL
DATA
SET
OF
SECTION
IN
FIGURE
T
T
X
X
T
T
X
X
FIGURE
EXAMPLES
OF
THE
PREDICTIVE
DISTRIBUTION
FOR
A
MODEL
CONSISTING
OF
GAUSSIAN
BASIS
FUNCTIONS
OF
THE
FORM
USING
THE
SYNTHETIC
SINUSOIDAL
DATA
SET
OF
SECTION
SEE
THE
TEXT
FOR
A
DETAILED
DISCUSSION
WE
FIT
A
MODEL
COMPRISING
A
LINEAR
COMBINATION
OF
GAUSSIAN
BASIS
FUNCTIONS
TO
DATA
SETS
OF
VARIOUS
SIZES
AND
THEN
LOOK
AT
THE
CORRESPONDING
POSTERIOR
DISTRIBUTIONS
HERE
THE
GREEN
CURVES
CORRESPOND
TO
THE
FUNCTION
SIN
FROM
WHICH
THE
DATA
POINTS
WERE
GENERATED
WITH
THE
ADDITION
OF
GAUSSIAN
NOISE
DATA
SETS
OF
SIZE
N
N
N
AND
N
ARE
SHOWN
IN
THE
FOUR
PLOTS
BY
THE
BLUE
CIRCLES
FOR
EACH
PLOT
THE
RED
CURVE
SHOWS
THE
MEAN
OF
THE
CORRESPONDING
GAUSSIAN
PREDICTIVE
DISTRIBUTION
AND
THE
RED
SHADED
REGION
SPANS
ONE
STANDARD
DEVIATION
EITHER
SIDE
OF
THE
MEAN
NOTE
THAT
THE
PREDICTIVE
UNCERTAINTY
DEPENDS
ON
X
AND
IS
SMALLEST
IN
THE
NEIGHBOURHOOD
OF
THE
DATA
POINTS
ALSO
NOTE
THAT
THE
LEVEL
OF
UNCERTAINTY
DECREASES
AS
MORE
DATA
POINTS
ARE
OBSERVED
THE
PLOTS
IN
FIGURE
ONLY
SHOW
THE
POINT
WISE
PREDICTIVE
VARIANCE
AS
A
FUNC
TION
OF
X
IN
ORDER
TO
GAIN
INSIGHT
INTO
THE
COVARIANCE
BETWEEN
THE
PREDICTIONS
AT
DIFFERENT
VALUES
OF
X
WE
CAN
DRAW
SAMPLES
FROM
THE
POSTERIOR
DISTRIBUTION
OVER
W
AND
THEN
PLOT
THE
CORRESPONDING
FUNCTIONS
Y
X
W
AS
SHOWN
IN
FIGURE
T
T
X
X
T
T
X
X
FIGURE
PLOTS
OF
THE
FUNCTION
Y
X
W
USING
SAMPLES
FROM
THE
POSTERIOR
DISTRIBUTIONS
OVER
W
CORRESPONDING
TO
THE
PLOTS
IN
FIGURE
SECTION
EXERCISE
EXERCISE
IF
WE
USED
LOCALIZED
BASIS
FUNCTIONS
SUCH
AS
GAUSSIANS
THEN
IN
REGIONS
AWAY
FROM
THE
BASIS
FUNCTION
CENTRES
THE
CONTRIBUTION
FROM
THE
SECOND
TERM
IN
THE
PREDIC
TIVE
VARIANCE
WILL
GO
TO
ZERO
LEAVING
ONLY
THE
NOISE
CONTRIBUTION
Β
THUS
THE
MODEL
BECOMES
VERY
CONFIDENT
IN
ITS
PREDICTIONS
WHEN
EXTRAPOLATING
OUTSIDE
THE
REGION
OCCUPIED
BY
THE
BASIS
FUNCTIONS
WHICH
IS
GENERALLY
AN
UNDESIRABLE
BEHAVIOUR
THIS
PROBLEM
CAN
BE
AVOIDED
BY
ADOPTING
AN
ALTERNATIVE
BAYESIAN
APPROACH
TO
RE
GRESSION
KNOWN
AS
A
GAUSSIAN
PROCESS
NOTE
THAT
IF
BOTH
W
AND
Β
ARE
TREATED
AS
UNKNOWN
THEN
WE
CAN
INTRODUCE
A
CONJUGATE
PRIOR
DISTRIBUTION
P
W
Β
THAT
FROM
THE
DISCUSSION
IN
SECTION
WILL
BE
GIVEN
BY
A
GAUSSIAN
GAMMA
DISTRIBUTION
DENISON
ET
AL
IN
THIS
CASE
THE
PREDICTIVE
DISTRIBUTION
IS
A
STUDENT
T
DISTRIBUTION
FIGURE
THE
EQUIVALENT
KER
NEL
K
X
XT
FOR
THE
GAUSSIAN
BASIS
FUNCTIONS
IN
FIGURE
SHOWN
AS
A
PLOT
OF
X
VERSUS
XT
TOGETHER
WITH
THREE
SLICES
THROUGH
THIS
MATRIX
COR
RESPONDING
TO
THREE
DIFFERENT
VALUES
OF
X
THE
DATA
SET
USED
TO
GENERATE
THIS
KERNEL
COMPRISED
VALUES
OF
X
EQUALLY
SPACED
OVER
THE
INTERVAL
CHAPTER
EQUIVALENT
KERNEL
THE
POSTERIOR
MEAN
SOLUTION
FOR
THE
LINEAR
BASIS
FUNCTION
MODEL
HAS
AN
IN
TERESTING
INTERPRETATION
THAT
WILL
SET
THE
STAGE
FOR
KERNEL
METHODS
INCLUDING
GAUSSIAN
PROCESSES
IF
WE
SUBSTITUTE
INTO
THE
EXPRESSION
WE
SEE
THAT
THE
PREDICTIVE
MEAN
CAN
BE
WRITTEN
IN
THE
FORM
Y
X
MN
MT
Φ
X
ΒΦ
X
TSNΦTT
ΒΦ
X
TSNΦ
XN
TN
N
WHERE
SN
IS
DEFINED
BY
THUS
THE
MEAN
OF
THE
PREDICTIVE
DISTRIBUTION
AT
A
POINT
X
IS
GIVEN
BY
A
LINEAR
COMBINATION
OF
THE
TRAINING
SET
TARGET
VARIABLES
TN
SO
THAT
WE
CAN
WRITE
WHERE
THE
FUNCTION
N
Y
X
MN
K
X
XN
TN
N
K
X
XI
ΒΦ
X
TSNΦ
XI
IS
KNOWN
AS
THE
SMOOTHER
MATRIX
OR
THE
EQUIVALENT
KERNEL
REGRESSION
FUNCTIONS
SUCH
AS
THIS
WHICH
MAKE
PREDICTIONS
BY
TAKING
LINEAR
COMBINATIONS
OF
THE
TRAINING
SET
TARGET
VALUES
ARE
KNOWN
AS
LINEAR
SMOOTHERS
NOTE
THAT
THE
EQUIVALENT
KERNEL
DEPENDS
ON
THE
INPUT
VALUES
XN
FROM
THE
DATA
SET
BECAUSE
THESE
APPEAR
IN
THE
DEFINITION
OF
SN
THE
EQUIVALENT
KERNEL
IS
ILLUSTRATED
FOR
THE
CASE
OF
GAUSSIAN
BASIS
FUNCTIONS
IN
FIGURE
IN
WHICH
THE
KERNEL
FUNCTIONS
K
X
XI
HAVE
BEEN
PLOTTED
AS
A
FUNCTION
OF
XI
FOR
THREE
DIFFERENT
VALUES
OF
X
WE
SEE
THAT
THEY
ARE
LOCALIZED
AROUND
X
AND
SO
THE
MEAN
OF
THE
PREDICTIVE
DISTRIBUTION
AT
X
GIVEN
BY
Y
X
MN
IS
OBTAINED
BY
FORMING
A
WEIGHTED
COMBINATION
OF
THE
TARGET
VALUES
IN
WHICH
DATA
POINTS
CLOSE
TO
X
ARE
GIVEN
HIGHER
WEIGHT
THAN
POINTS
FURTHER
REMOVED
FROM
X
INTUITIVELY
IT
SEEMS
REASONABLE
THAT
WE
SHOULD
WEIGHT
LOCAL
EVIDENCE
MORE
STRONGLY
THAN
DISTANT
EVIDENCE
NOTE
THAT
THIS
LOCALIZATION
PROPERTY
HOLDS
NOT
ONLY
FOR
THE
LOCALIZED
GAUSSIAN
BASIS
FUNCTIONS
BUT
ALSO
FOR
THE
NONLOCAL
POLYNOMIAL
AND
SIGMOIDAL
BASIS
FUNCTIONS
AS
ILLUSTRATED
IN
FIGURE
FIGURE
EXAMPLES
OF
EQUIVA
LENT
KERNELS
K
X
XT
FOR
X
PLOTTED
AS
A
FUNCTION
OF
XT
CORRE
SPONDING
LEFT
TO
THE
POLYNOMIAL
BA
SIS
FUNCTIONS
AND
RIGHT
TO
THE
SIG
MOIDAL
BASIS
FUNCTIONS
SHOWN
IN
FIG
URE
NOTE
THAT
THESE
ARE
LOCAL
IZED
FUNCTIONS
OF
XT
EVEN
THOUGH
THE
CORRESPONDING
BASIS
FUNCTIONS
ARE
NONLOCAL
FURTHER
INSIGHT
INTO
THE
ROLE
OF
THE
EQUIVALENT
KERNEL
CAN
BE
OBTAINED
BY
CONSID
ERING
THE
COVARIANCE
BETWEEN
Y
X
AND
Y
XI
WHICH
IS
GIVEN
BY
COV
Y
X
Y
XI
COV
Φ
X
TW
WTΦ
XI
Φ
X
TSNΦ
XI
Β
X
XI
WHERE
WE
HAVE
MADE
USE
OF
AND
FROM
THE
FORM
OF
THE
EQUIVALENT
KERNEL
WE
SEE
THAT
THE
PREDICTIVE
MEAN
AT
NEARBY
POINTS
WILL
BE
HIGHLY
CORRELATED
WHEREAS
FOR
MORE
DISTANT
PAIRS
OF
POINTS
THE
CORRELATION
WILL
BE
SMALLER
THE
PREDICTIVE
DISTRIBUTION
SHOWN
IN
FIGURE
ALLOWS
US
TO
VISUALIZE
THE
POINT
WISE
UNCERTAINTY
IN
THE
PREDICTIONS
GOVERNED
BY
HOWEVER
BY
DRAWING
SAM
PLES
FROM
THE
POSTERIOR
DISTRIBUTION
OVER
W
AND
PLOTTING
THE
CORRESPONDING
MODEL
FUNCTIONS
Y
X
W
AS
IN
FIGURE
WE
ARE
VISUALIZING
THE
JOINT
UNCERTAINTY
IN
THE
POSTERIOR
DISTRIBUTION
BETWEEN
THE
Y
VALUES
AT
TWO
OR
MORE
X
VALUES
AS
GOVERNED
BY
THE
EQUIVALENT
KERNEL
THE
FORMULATION
OF
LINEAR
REGRESSION
IN
TERMS
OF
A
KERNEL
FUNCTION
SUGGESTS
AN
ALTERNATIVE
APPROACH
TO
REGRESSION
AS
FOLLOWS
INSTEAD
OF
INTRODUCING
A
SET
OF
BASIS
FUNCTIONS
WHICH
IMPLICITLY
DETERMINES
AN
EQUIVALENT
KERNEL
WE
CAN
INSTEAD
DEFINE
A
LOCALIZED
KERNEL
DIRECTLY
AND
USE
THIS
TO
MAKE
PREDICTIONS
FOR
NEW
INPUT
VECTORS
X
GIVEN
THE
OBSERVED
TRAINING
SET
THIS
LEADS
TO
A
PRACTICAL
FRAMEWORK
FOR
REGRESSION
AND
CLASSIFICATION
CALLED
GAUSSIAN
PROCESSES
WHICH
WILL
BE
DISCUSSED
IN
DETAIL
IN
SECTION
WE
HAVE
SEEN
THAT
THE
EFFECTIVE
KERNEL
DEFINES
THE
WEIGHTS
BY
WHICH
THE
TRAINING
SET
TARGET
VALUES
ARE
COMBINED
IN
ORDER
TO
MAKE
A
PREDICTION
AT
A
NEW
VALUE
OF
X
AND
IT
CAN
BE
SHOWN
THAT
THESE
WEIGHTS
SUM
TO
ONE
IN
OTHER
WORDS
EXERCISE
N
K
X
XN
N
FOR
ALL
VALUES
OF
X
THIS
INTUITIVELY
PLEASING
RESULT
CAN
EASILY
BE
PROVEN
INFORMALLY
BY
NOTING
THAT
THE
SUMMATION
IS
EQUIVALENT
TO
CONSIDERING
THE
PREDICTIVE
MEAN
Y
X
FOR
A
SET
OF
TARGET
DATA
IN
WHICH
TN
FOR
ALL
N
PROVIDED
THE
BASIS
FUNCTIONS
ARE
LINEARLY
INDEPENDENT
THAT
THERE
ARE
MORE
DATA
POINTS
THAN
BASIS
FUNCTIONS
AND
THAT
ONE
OF
THE
BASIS
FUNCTIONS
IS
CONSTANT
CORRESPONDING
TO
THE
BIAS
PARAMETER
THEN
IT
IS
CLEAR
THAT
WE
CAN
FIT
THE
TRAINING
DATA
EXACTLY
AND
HENCE
THAT
THE
PREDICTIVE
MEAN
WILL
CHAPTER
BE
SIMPLY
Y
X
FROM
WHICH
WE
OBTAIN
NOTE
THAT
THE
KERNEL
FUNCTION
CAN
BE
NEGATIVE
AS
WELL
AS
POSITIVE
SO
ALTHOUGH
IT
SATISFIES
A
SUMMATION
CONSTRAINT
THE
CORRESPONDING
PREDICTIONS
ARE
NOT
NECESSARILY
CONVEX
COMBINATIONS
OF
THE
TRAINING
SET
TARGET
VARIABLES
FINALLY
WE
NOTE
THAT
THE
EQUIVALENT
KERNEL
SATISFIES
AN
IMPORTANT
PROPERTY
SHARED
BY
KERNEL
FUNCTIONS
IN
GENERAL
NAMELY
THAT
IT
CAN
BE
EXPRESSED
IN
THE
FORM
AN
INNER
PRODUCT
WITH
RESPECT
TO
A
VECTOR
Ψ
X
OF
NONLINEAR
FUNCTIONS
SO
THAT
K
X
Z
Ψ
X
TΨ
Z
WHERE
Ψ
X
X
BAYESIAN
MODEL
COMPARISON
SECTION
IN
CHAPTER
WE
HIGHLIGHTED
THE
PROBLEM
OF
OVER
FITTING
AS
WELL
AS
THE
USE
OF
CROSS
VALIDATION
AS
A
TECHNIQUE
FOR
SETTING
THE
VALUES
OF
REGULARIZATION
PARAMETERS
OR
FOR
CHOOSING
BETWEEN
ALTERNATIVE
MODELS
HERE
WE
CONSIDER
THE
PROBLEM
OF
MODEL
SE
LECTION
FROM
A
BAYESIAN
PERSPECTIVE
IN
THIS
SECTION
OUR
DISCUSSION
WILL
BE
VERY
GENERAL
AND
THEN
IN
SECTION
WE
SHALL
SEE
HOW
THESE
IDEAS
CAN
BE
APPLIED
TO
THE
DETERMINATION
OF
REGULARIZATION
PARAMETERS
IN
LINEAR
REGRESSION
AS
WE
SHALL
SEE
THE
OVER
FITTING
ASSOCIATED
WITH
MAXIMUM
LIKELIHOOD
CAN
BE
AVOIDED
BY
MARGINALIZING
SUMMING
OR
INTEGRATING
OVER
THE
MODEL
PARAMETERS
IN
STEAD
OF
MAKING
POINT
ESTIMATES
OF
THEIR
VALUES
MODELS
CAN
THEN
BE
COMPARED
DI
RECTLY
ON
THE
TRAINING
DATA
WITHOUT
THE
NEED
FOR
A
VALIDATION
SET
THIS
ALLOWS
ALL
AVAILABLE
DATA
TO
BE
USED
FOR
TRAINING
AND
AVOIDS
THE
MULTIPLE
TRAINING
RUNS
FOR
EACH
MODEL
ASSOCIATED
WITH
CROSS
VALIDATION
IT
ALSO
ALLOWS
MULTIPLE
COMPLEXITY
PARAME
TERS
TO
BE
DETERMINED
SIMULTANEOUSLY
AS
PART
OF
THE
TRAINING
PROCESS
FOR
EXAMPLE
IN
CHAPTER
WE
SHALL
INTRODUCE
THE
RELEVANCE
VECTOR
MACHINE
WHICH
IS
A
BAYESIAN
MODEL
HAVING
ONE
COMPLEXITY
PARAMETER
FOR
EVERY
TRAINING
DATA
POINT
THE
BAYESIAN
VIEW
OF
MODEL
COMPARISON
SIMPLY
INVOLVES
THE
USE
OF
PROBABILITIES
TO
REPRESENT
UNCERTAINTY
IN
THE
CHOICE
OF
MODEL
ALONG
WITH
A
CONSISTENT
APPLICATION
OF
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
SUPPOSE
WE
WISH
TO
COMPARE
A
SET
OF
L
MODELS
I
WHERE
I
L
HERE
A
MODEL
REFERS
TO
A
PROBABILITY
DISTRIBUTION
OVER
THE
OBSERVED
DATA
IN
THE
CASE
OF
THE
POLYNOMIAL
CURVE
FITTING
PROBLEM
THE
DISTRIBUTION
IS
DEFINED
OVER
THE
SET
OF
TARGET
VALUES
T
WHILE
THE
SET
OF
INPUT
VALUES
X
IS
ASSUMED
TO
BE
KNOWN
OTHER
TYPES
OF
MODEL
DEFINE
A
JOINT
DISTRIBUTIONS
OVER
X
AND
T
WE
SHALL
SUPPOSE
THAT
THE
DATA
IS
GENERATED
FROM
ONE
OF
THESE
MODELS
BUT
WE
ARE
UNCERTAIN
WHICH
ONE
OUR
UNCERTAINTY
IS
EXPRESSED
THROUGH
A
PRIOR
PROBABILITY
DISTRIBUTION
P
I
GIVEN
A
TRAINING
SET
WE
THEN
WISH
TO
EVALUATE
THE
POSTERIOR
DISTRIBUTION
P
MI
D
P
MI
P
D
MI
THE
PRIOR
ALLOWS
US
TO
EXPRESS
A
PREFERENCE
FOR
DIFFERENT
MODELS
LET
US
SIMPLY
ASSUME
THAT
ALL
MODELS
ARE
GIVEN
EQUAL
PRIOR
PROBABILITY
THE
INTERESTING
TERM
IS
THE
MODEL
EVIDENCE
P
D
MI
WHICH
EXPRESSES
THE
PREFERENCE
SHOWN
BY
THE
DATA
FOR
DIFFERENT
MODELS
AND
WE
SHALL
EXAMINE
THIS
TERM
IN
MORE
DETAIL
SHORTLY
THE
MODEL
EVIDENCE
IS
SOMETIMES
ALSO
CALLED
THE
MARGINAL
LIKELIHOOD
BECAUSE
IT
CAN
BE
VIEWED
AS
A
LIKELIHOOD
FUNCTION
OVER
THE
SPACE
OF
MODELS
IN
WHICH
THE
PARAMETERS
HAVE
BEEN
MARGINALIZED
OUT
THE
RATIO
OF
MODEL
EVIDENCES
P
I
P
J
FOR
TWO
MODELS
IS
KNOWN
AS
A
BAYES
FACTOR
KASS
AND
RAFTERY
ONCE
WE
KNOW
THE
POSTERIOR
DISTRIBUTION
OVER
MODELS
THE
PREDICTIVE
DISTRIBUTION
IS
GIVEN
FROM
THE
SUM
AND
PRODUCT
RULES
BY
CHAPTER
L
P
T
X
D
P
T
X
MI
D
P
MI
D
I
THIS
IS
AN
EXAMPLE
OF
A
MIXTURE
DISTRIBUTION
IN
WHICH
THE
OVERALL
PREDICTIVE
DISTRIBU
TION
IS
OBTAINED
BY
AVERAGING
THE
PREDICTIVE
DISTRIBUTIONS
P
T
X
I
OF
INDIVIDUAL
MODELS
WEIGHTED
BY
THE
POSTERIOR
PROBABILITIES
P
I
OF
THOSE
MODELS
FOR
IN
STANCE
IF
WE
HAVE
TWO
MODELS
THAT
ARE
A
POSTERIORI
EQUALLY
LIKELY
AND
ONE
PREDICTS
A
NARROW
DISTRIBUTION
AROUND
T
A
WHILE
THE
OTHER
PREDICTS
A
NARROW
DISTRIBUTION
AROUND
T
B
THE
OVERALL
PREDICTIVE
DISTRIBUTION
WILL
BE
A
BIMODAL
DISTRIBUTION
WITH
MODES
AT
T
A
AND
T
B
NOT
A
SINGLE
MODEL
AT
T
A
B
A
SIMPLE
APPROXIMATION
TO
MODEL
AVERAGING
IS
TO
USE
THE
SINGLE
MOST
PROBABLE
MODEL
ALONE
TO
MAKE
PREDICTIONS
THIS
IS
KNOWN
AS
MODEL
SELECTION
FOR
A
MODEL
GOVERNED
BY
A
SET
OF
PARAMETERS
W
THE
MODEL
EVIDENCE
IS
GIVEN
FROM
THE
SUM
AND
PRODUCT
RULES
OF
PROBABILITY
BY
P
D
MI
R
P
D
W
MI
P
W
MI
DW
FROM
A
SAMPLING
PERSPECTIVE
THE
MARGINAL
LIKELIHOOD
CAN
BE
VIEWED
AS
THE
PROBA
BILITY
OF
GENERATING
THE
DATA
SET
FROM
A
MODEL
WHOSE
PARAMETERS
ARE
SAMPLED
AT
RANDOM
FROM
THE
PRIOR
IT
IS
ALSO
INTERESTING
TO
NOTE
THAT
THE
EVIDENCE
IS
PRECISELY
THE
NORMALIZING
TERM
THAT
APPEARS
IN
THE
DENOMINATOR
IN
BAYES
THEOREM
WHEN
EVALUATING
THE
POSTERIOR
DISTRIBUTION
OVER
PARAMETERS
BECAUSE
P
W
D
M
P
D
W
MI
P
W
MI
P
D
MI
WE
CAN
OBTAIN
SOME
INSIGHT
INTO
THE
MODEL
EVIDENCE
BY
MAKING
A
SIMPLE
APPROX
IMATION
TO
THE
INTEGRAL
OVER
PARAMETERS
CONSIDER
FIRST
THE
CASE
OF
A
MODEL
HAVING
A
SINGLE
PARAMETER
W
THE
POSTERIOR
DISTRIBUTION
OVER
PARAMETERS
IS
PROPORTIONAL
TO
P
W
P
W
WHERE
WE
OMIT
THE
DEPENDENCE
ON
THE
MODEL
I
TO
KEEP
THE
NOTATION
UNCLUTTERED
IF
WE
ASSUME
THAT
THE
POSTERIOR
DISTRIBUTION
IS
SHARPLY
PEAKED
AROUND
THE
MOST
PROBABLE
VALUE
WMAP
WITH
WIDTH
WPOSTERIOR
THEN
WE
CAN
APPROXIMATE
THE
IN
TEGRAL
BY
THE
VALUE
OF
THE
INTEGRAND
AT
ITS
MAXIMUM
TIMES
THE
WIDTH
OF
THE
PEAK
IF
WE
FURTHER
ASSUME
THAT
THE
PRIOR
IS
FLAT
WITH
WIDTH
WPRIOR
SO
THAT
P
W
WPRIOR
THEN
WE
HAVE
P
D
R
P
D
W
P
W
DW
P
D
WMAP
WPOSTERIOR
WPRIOR
FIGURE
WE
CAN
OBTAIN
A
ROUGH
APPROXIMATION
TO
THE
MODEL
EVIDENCE
IF
WE
ASSUME
THAT
THE
POSTERIOR
DISTRIBUTION
OVER
PARAME
TERS
IS
SHARPLY
PEAKED
AROUND
ITS
MODE
WMAP
WPOSTERIOR
WMAP
W
WPRIOR
AND
SO
TAKING
LOGS
WE
OBTAIN
LN
P
D
LN
P
D
WMAP
LN
WPOSTERIOR
WPRIOR
THIS
APPROXIMATION
IS
ILLUSTRATED
IN
FIGURE
THE
FIRST
TERM
REPRESENTS
THE
FIT
TO
THE
DATA
GIVEN
BY
THE
MOST
PROBABLE
PARAMETER
VALUES
AND
FOR
A
FLAT
PRIOR
THIS
WOULD
CORRESPOND
TO
THE
LOG
LIKELIHOOD
THE
SECOND
TERM
PENALIZES
THE
MODEL
ACCORDING
TO
ITS
COMPLEXITY
BECAUSE
WPOSTERIOR
WPRIOR
THIS
TERM
IS
NEGATIVE
AND
IT
INCREASES
IN
MAGNITUDE
AS
THE
RATIO
WPOSTERIOR
WPRIOR
GETS
SMALLER
THUS
IF
PARAMETERS
ARE
FINELY
TUNED
TO
THE
DATA
IN
THE
POSTERIOR
DISTRIBUTION
THEN
THE
PENALTY
TERM
IS
LARGE
FOR
A
MODEL
HAVING
A
SET
OF
M
PARAMETERS
WE
CAN
MAKE
A
SIMILAR
APPROXIMATION
FOR
EACH
PARAMETER
IN
TURN
ASSUMING
THAT
ALL
PARAMETERS
HAVE
THE
SAME
RATIO
OF
WPOSTERIOR
WPRIOR
WE
OBTAIN
LN
P
D
LN
P
D
W
MAP
LN
WPOSTERIOR
WPRIOR
SECTION
THUS
IN
THIS
VERY
SIMPLE
APPROXIMATION
THE
SIZE
OF
THE
COMPLEXITY
PENALTY
INCREASES
LINEARLY
WITH
THE
NUMBER
M
OF
ADAPTIVE
PARAMETERS
IN
THE
MODEL
AS
WE
INCREASE
THE
COMPLEXITY
OF
THE
MODEL
THE
FIRST
TERM
WILL
TYPICALLY
DECREASE
BECAUSE
A
MORE
COMPLEX
MODEL
IS
BETTER
ABLE
TO
FIT
THE
DATA
WHEREAS
THE
SECOND
TERM
WILL
INCREASE
DUE
TO
THE
DEPENDENCE
ON
M
THE
OPTIMAL
MODEL
COMPLEXITY
AS
DETERMINED
BY
THE
MAXIMUM
EVIDENCE
WILL
BE
GIVEN
BY
A
TRADE
OFF
BETWEEN
THESE
TWO
COMPETING
TERMS
WE
SHALL
LATER
DEVELOP
A
MORE
REFINED
VERSION
OF
THIS
APPROXIMATION
BASED
ON
A
GAUSSIAN
APPROXIMATION
TO
THE
POSTERIOR
DISTRIBUTION
WE
CAN
GAIN
FURTHER
INSIGHT
INTO
BAYESIAN
MODEL
COMPARISON
AND
UNDERSTAND
HOW
THE
MARGINAL
LIKELIHOOD
CAN
FAVOUR
MODELS
OF
INTERMEDIATE
COMPLEXITY
BY
CON
SIDERING
FIGURE
HERE
THE
HORIZONTAL
AXIS
IS
A
ONE
DIMENSIONAL
REPRESENTATION
OF
THE
SPACE
OF
POSSIBLE
DATA
SETS
SO
THAT
EACH
POINT
ON
THIS
AXIS
CORRESPONDS
TO
A
SPECIFIC
DATA
SET
WE
NOW
CONSIDER
THREE
MODELS
AND
OF
SUCCESSIVELY
INCREASING
COMPLEXITY
IMAGINE
RUNNING
THESE
MODELS
GENERATIVELY
TO
PRODUCE
EXAM
PLE
DATA
SETS
AND
THEN
LOOKING
AT
THE
DISTRIBUTION
OF
DATA
SETS
THAT
RESULT
ANY
GIVEN
FIGURE
SCHEMATIC
ILLUSTRATION
OF
THE
DISTRIBUTION
OF
DATA
SETS
FOR
THREE
MODELS
OF
DIFFERENT
COM
PLEXITY
IN
WHICH
IS
THE
SIMPLEST
AND
IS
THE
MOST
COMPLEX
NOTE
THAT
THE
DIS
TRIBUTIONS
ARE
NORMALIZED
IN
THIS
EXAMPLE
FOR
THE
PARTIC
ULAR
OBSERVED
DATA
SET
THE
MODEL
WITH
INTERMEDI
ATE
COMPLEXITY
HAS
THE
LARGEST
EVIDENCE
P
D
D
MODEL
CAN
GENERATE
A
VARIETY
OF
DIFFERENT
DATA
SETS
SINCE
THE
PARAMETERS
ARE
GOVERNED
BY
A
PRIOR
PROBABILITY
DISTRIBUTION
AND
FOR
ANY
CHOICE
OF
THE
PARAMETERS
THERE
MAY
BE
RANDOM
NOISE
ON
THE
TARGET
VARIABLES
TO
GENERATE
A
PARTICULAR
DATA
SET
FROM
A
SPE
CIFIC
MODEL
WE
FIRST
CHOOSE
THE
VALUES
OF
THE
PARAMETERS
FROM
THEIR
PRIOR
DISTRIBUTION
P
W
AND
THEN
FOR
THESE
PARAMETER
VALUES
WE
SAMPLE
THE
DATA
FROM
P
W
A
SIM
PLE
MODEL
FOR
EXAMPLE
BASED
ON
A
FIRST
ORDER
POLYNOMIAL
HAS
LITTLE
VARIABILITY
AND
SO
WILL
GENERATE
DATA
SETS
THAT
ARE
FAIRLY
SIMILAR
TO
EACH
OTHER
ITS
DISTRIBUTION
P
IS
THEREFORE
CONFINED
TO
A
RELATIVELY
SMALL
REGION
OF
THE
HORIZONTAL
AXIS
BY
CONTRAST
A
COMPLEX
MODEL
SUCH
AS
A
NINTH
ORDER
POLYNOMIAL
CAN
GENERATE
A
GREAT
VARIETY
OF
DIFFERENT
DATA
SETS
AND
SO
ITS
DISTRIBUTION
P
IS
SPREAD
OVER
A
LARGE
REGION
OF
THE
SPACE
OF
DATA
SETS
BECAUSE
THE
DISTRIBUTIONS
P
I
ARE
NORMALIZED
WE
SEE
THAT
THE
PARTICULAR
DATA
SET
CAN
HAVE
THE
HIGHEST
VALUE
OF
THE
EVIDENCE
FOR
THE
MODEL
OF
INTERMEDIATE
COMPLEXITY
ESSENTIALLY
THE
SIMPLER
MODEL
CANNOT
FIT
THE
DATA
WELL
WHEREAS
THE
MORE
COMPLEX
MODEL
SPREADS
ITS
PREDICTIVE
PROBABILITY
OVER
TOO
BROAD
A
RANGE
OF
DATA
SETS
AND
SO
ASSIGNS
RELATIVELY
SMALL
PROBABILITY
TO
ANY
ONE
OF
THEM
IMPLICIT
IN
THE
BAYESIAN
MODEL
COMPARISON
FRAMEWORK
IS
THE
ASSUMPTION
THAT
THE
TRUE
DISTRIBUTION
FROM
WHICH
THE
DATA
ARE
GENERATED
IS
CONTAINED
WITHIN
THE
SET
OF
MODELS
UNDER
CONSIDERATION
PROVIDED
THIS
IS
SO
WE
CAN
SHOW
THAT
BAYESIAN
MODEL
COMPARISON
WILL
ON
AVERAGE
FAVOUR
THE
CORRECT
MODEL
TO
SEE
THIS
CONSIDER
TWO
MODELS
AND
IN
WHICH
THE
TRUTH
CORRESPONDS
TO
FOR
A
GIVEN
FINITE
DATA
SET
IT
IS
POSSIBLE
FOR
THE
BAYES
FACTOR
TO
BE
LARGER
FOR
THE
INCORRECT
MODEL
HOWEVER
IF
WE
AVERAGE
THE
BAYES
FACTOR
OVER
THE
DISTRIBUTION
OF
DATA
SETS
WE
OBTAIN
THE
EXPECTED
BAYES
FACTOR
IN
THE
FORM
R
P
D
M
LN
P
D
DD
SECTION
WHERE
THE
AVERAGE
HAS
BEEN
TAKEN
WITH
RESPECT
TO
THE
TRUE
DISTRIBUTION
OF
THE
DATA
THIS
QUANTITY
IS
AN
EXAMPLE
OF
THE
KULLBACK
LEIBLER
DIVERGENCE
AND
SATISFIES
THE
PROP
ERTY
OF
ALWAYS
BEING
POSITIVE
UNLESS
THE
TWO
DISTRIBUTIONS
ARE
EQUAL
IN
WHICH
CASE
IT
IS
ZERO
THUS
ON
AVERAGE
THE
BAYES
FACTOR
WILL
ALWAYS
FAVOUR
THE
CORRECT
MODEL
WE
HAVE
SEEN
THAT
THE
BAYESIAN
FRAMEWORK
AVOIDS
THE
PROBLEM
OF
OVER
FITTING
AND
ALLOWS
MODELS
TO
BE
COMPARED
ON
THE
BASIS
OF
THE
TRAINING
DATA
ALONE
HOWEVER
A
BAYESIAN
APPROACH
LIKE
ANY
APPROACH
TO
PATTERN
RECOGNITION
NEEDS
TO
MAKE
AS
SUMPTIONS
ABOUT
THE
FORM
OF
THE
MODEL
AND
IF
THESE
ARE
INVALID
THEN
THE
RESULTS
CAN
BE
MISLEADING
IN
PARTICULAR
WE
SEE
FROM
FIGURE
THAT
THE
MODEL
EVIDENCE
CAN
BE
SENSITIVE
TO
MANY
ASPECTS
OF
THE
PRIOR
SUCH
AS
THE
BEHAVIOUR
IN
THE
TAILS
INDEED
THE
EVIDENCE
IS
NOT
DEFINED
IF
THE
PRIOR
IS
IMPROPER
AS
CAN
BE
SEEN
BY
NOTING
THAT
AN
IMPROPER
PRIOR
HAS
AN
ARBITRARY
SCALING
FACTOR
IN
OTHER
WORDS
THE
NORMALIZATION
COEFFICIENT
IS
NOT
DEFINED
BECAUSE
THE
DISTRIBUTION
CANNOT
BE
NORMALIZED
IF
WE
CON
SIDER
A
PROPER
PRIOR
AND
THEN
TAKE
A
SUITABLE
LIMIT
IN
ORDER
TO
OBTAIN
AN
IMPROPER
PRIOR
FOR
EXAMPLE
A
GAUSSIAN
PRIOR
IN
WHICH
WE
TAKE
THE
LIMIT
OF
INFINITE
VARIANCE
THEN
THE
EVIDENCE
WILL
GO
TO
ZERO
AS
CAN
BE
SEEN
FROM
AND
FIGURE
IT
MAY
HOWEVER
BE
POSSIBLE
TO
CONSIDER
THE
EVIDENCE
RATIO
BETWEEN
TWO
MODELS
FIRST
AND
THEN
TAKE
A
LIMIT
TO
OBTAIN
A
MEANINGFUL
ANSWER
IN
A
PRACTICAL
APPLICATION
THEREFORE
IT
WILL
BE
WISE
TO
KEEP
ASIDE
AN
INDEPENDENT
TEST
SET
OF
DATA
ON
WHICH
TO
EVALUATE
THE
OVERALL
PERFORMANCE
OF
THE
FINAL
SYSTEM
THE
EVIDENCE
APPROXIMATION
IN
A
FULLY
BAYESIAN
TREATMENT
OF
THE
LINEAR
BASIS
FUNCTION
MODEL
WE
WOULD
INTRO
DUCE
PRIOR
DISTRIBUTIONS
OVER
THE
HYPERPARAMETERS
Α
AND
Β
AND
MAKE
PREDICTIONS
BY
MARGINALIZING
WITH
RESPECT
TO
THESE
HYPERPARAMETERS
AS
WELL
AS
WITH
RESPECT
TO
THE
PARAMETERS
W
HOWEVER
ALTHOUGH
WE
CAN
INTEGRATE
ANALYTICALLY
OVER
EITHER
W
OR
OVER
THE
HYPERPARAMETERS
THE
COMPLETE
MARGINALIZATION
OVER
ALL
OF
THESE
VARIABLES
IS
ANALYTICALLY
INTRACTABLE
HERE
WE
DISCUSS
AN
APPROXIMATION
IN
WHICH
WE
SET
THE
HYPERPARAMETERS
TO
SPECIFIC
VALUES
DETERMINED
BY
MAXIMIZING
THE
MARGINAL
LIKELI
HOOD
FUNCTION
OBTAINED
BY
FIRST
INTEGRATING
OVER
THE
PARAMETERS
W
THIS
FRAMEWORK
IS
KNOWN
IN
THE
STATISTICS
LITERATURE
AS
EMPIRICAL
BAYES
BERNARDO
AND
SMITH
GELMAN
ET
AL
OR
TYPE
MAXIMUM
LIKELIHOOD
BERGER
OR
GENERALIZED
MAXIMUM
LIKELIHOOD
WAHBA
AND
IN
THE
MACHINE
LEARNING
LITERATURE
IS
ALSO
CALLED
THE
EVIDENCE
APPROXIMATION
GULL
MACKAY
IF
WE
INTRODUCE
HYPERPRIORS
OVER
Α
AND
Β
THE
PREDICTIVE
DISTRIBUTION
IS
OBTAINED
BY
MARGINALIZING
OVER
W
Α
AND
Β
SO
THAT
P
T
T
RRR
P
T
W
Β
P
W
T
Α
Β
P
Α
Β
T
DW
DΑ
DΒ
WHERE
P
T
W
Β
IS
GIVEN
BY
AND
P
W
T
Α
Β
IS
GIVEN
BY
WITH
MN
AND
SN
DEFINED
BY
AND
RESPECTIVELY
HERE
WE
HAVE
OMITTED
THE
DEPENDENCE
ON
THE
INPUT
VARIABLE
X
TO
KEEP
THE
NOTATION
UNCLUTTERED
IF
THE
POSTERIOR
DISTRIBUTION
P
Α
Β
T
IS
SHARPLY
PEAKED
AROUND
VALUES
Α
AND
Β
THEN
THE
PREDICTIVE
DISTRIBUTION
IS
OBTAINED
SIMPLY
BY
MARGINALIZING
OVER
W
IN
WHICH
Α
AND
Β
ARE
FIXED
TO
THE
VALUES
Α
AND
Β
SO
THAT
P
T
T
P
T
T
Α
Β
P
T
W
Β
P
W
T
Α
Β
DW
EXERCISE
EXERCISE
FROM
BAYES
THEOREM
THE
POSTERIOR
DISTRIBUTION
FOR
Α
AND
Β
IS
GIVEN
BY
P
Α
Β
T
P
T
Α
Β
P
Α
Β
IF
THE
PRIOR
IS
RELATIVELY
FLAT
THEN
IN
THE
EVIDENCE
FRAMEWORK
THE
VALUES
OF
Α
AND
Β
ARE
OBTAINED
BY
MAXIMIZING
THE
MARGINAL
LIKELIHOOD
FUNCTION
P
T
Α
Β
WE
SHALL
PROCEED
BY
EVALUATING
THE
MARGINAL
LIKELIHOOD
FOR
THE
LINEAR
BASIS
FUNCTION
MODEL
AND
THEN
FINDING
ITS
MAXIMA
THIS
WILL
ALLOW
US
TO
DETERMINE
VALUES
FOR
THESE
HYPERPA
RAMETERS
FROM
THE
TRAINING
DATA
ALONE
WITHOUT
RECOURSE
TO
CROSS
VALIDATION
RECALL
THAT
THE
RATIO
Α
Β
IS
ANALOGOUS
TO
A
REGULARIZATION
PARAMETER
AS
AN
ASIDE
IT
IS
WORTH
NOTING
THAT
IF
WE
DEFINE
CONJUGATE
GAMMA
PRIOR
DISTRI
BUTIONS
OVER
Α
AND
Β
THEN
THE
MARGINALIZATION
OVER
THESE
HYPERPARAMETERS
IN
CAN
BE
PERFORMED
ANALYTICALLY
TO
GIVE
A
STUDENT
T
DISTRIBUTION
OVER
W
SEE
SEC
TION
ALTHOUGH
THE
RESULTING
INTEGRAL
OVER
W
IS
NO
LONGER
ANALYTICALLY
TRACTABLE
IT
MIGHT
BE
THOUGHT
THAT
APPROXIMATING
THIS
INTEGRAL
FOR
EXAMPLE
USING
THE
LAPLACE
APPROXIMATION
DISCUSSED
SECTION
WHICH
IS
BASED
ON
A
LOCAL
GAUSSIAN
APPROXI
MATION
CENTRED
ON
THE
MODE
OF
THE
POSTERIOR
DISTRIBUTION
MIGHT
PROVIDE
A
PRACTICAL
ALTERNATIVE
TO
THE
EVIDENCE
FRAMEWORK
BUNTINE
AND
WEIGEND
HOWEVER
THE
INTEGRAND
AS
A
FUNCTION
OF
W
TYPICALLY
HAS
A
STRONGLY
SKEWED
MODE
SO
THAT
THE
LAPLACE
APPROXIMATION
FAILS
TO
CAPTURE
THE
BULK
OF
THE
PROBABILITY
MASS
LEADING
TO
POORER
RE
SULTS
THAN
THOSE
OBTAINED
BY
MAXIMIZING
THE
EVIDENCE
MACKAY
RETURNING
TO
THE
EVIDENCE
FRAMEWORK
WE
NOTE
THAT
THERE
ARE
TWO
APPROACHES
THAT
WE
CAN
TAKE
TO
THE
MAXIMIZATION
OF
THE
LOG
EVIDENCE
WE
CAN
EVALUATE
THE
EVIDENCE
FUNCTION
ANALYTICALLY
AND
THEN
SET
ITS
DERIVATIVE
EQUAL
TO
ZERO
TO
OBTAIN
RE
ESTIMATION
EQUATIONS
FOR
Α
AND
Β
WHICH
WE
SHALL
DO
IN
SECTION
ALTERNATIVELY
WE
USE
A
TECHNIQUE
CALLED
THE
EXPECTATION
MAXIMIZATION
EM
ALGORITHM
WHICH
WILL
BE
DIS
CUSSED
IN
SECTION
WHERE
WE
SHALL
ALSO
SHOW
THAT
THESE
TWO
APPROACHES
CONVERGE
TO
THE
SAME
SOLUTION
EVALUATION
OF
THE
EVIDENCE
FUNCTION
THE
MARGINAL
LIKELIHOOD
FUNCTION
P
T
Α
Β
IS
OBTAINED
BY
INTEGRATING
OVER
THE
WEIGHT
PARAMETERS
W
SO
THAT
P
T
Α
Β
R
P
T
W
Β
P
W
Α
DW
ONE
WAY
TO
EVALUATE
THIS
INTEGRAL
IS
TO
MAKE
USE
ONCE
AGAIN
OF
THE
RESULT
FOR
THE
CONDITIONAL
DISTRIBUTION
IN
A
LINEAR
GAUSSIAN
MODEL
HERE
WE
SHALL
EVALUATE
THE
INTEGRAL
INSTEAD
BY
COMPLETING
THE
SQUARE
IN
THE
EXPONENT
AND
MAKING
USE
OF
THE
STANDARD
FORM
FOR
THE
NORMALIZATION
COEFFICIENT
OF
A
GAUSSIAN
FROM
AND
WE
CAN
WRITE
THE
EVIDENCE
FUNCTION
IN
THE
FORM
Β
N
Α
M
R
WHERE
M
IS
THE
DIMENSIONALITY
OF
W
AND
WE
HAVE
DEFINED
E
W
ΒED
W
ΑEW
W
Β
Α
WTW
EXERCISE
WE
RECOGNIZE
AS
BEING
EQUAL
UP
TO
A
CONSTANT
OF
PROPORTIONALITY
TO
THE
REG
ULARIZED
SUM
OF
SQUARES
ERROR
FUNCTION
WE
NOW
COMPLETE
THE
SQUARE
OVER
W
GIVING
E
W
E
MN
WHERE
WE
HAVE
INTRODUCED
W
MN
TA
W
MN
A
ΑI
ΒΦTΦ
TOGETHER
WITH
E
M
Β
ΦM
M
M
EXERCISE
NOTE
THAT
A
CORRESPONDS
TO
THE
MATRIX
OF
SECOND
DERIVATIVES
OF
THE
ERROR
FUNCTION
A
E
W
AND
IS
KNOWN
AS
THE
HESSIAN
MATRIX
HERE
WE
HAVE
ALSO
DEFINED
MN
GIVEN
BY
MN
ΒA
USING
WE
SEE
THAT
A
AND
HENCE
IS
EQUIVALENT
TO
THE
PREVIOUS
DEFINITION
AND
THEREFORE
REPRESENTS
THE
MEAN
OF
THE
POSTERIOR
DISTRIBUTION
THE
INTEGRAL
OVER
W
CAN
NOW
BE
EVALUATED
SIMPLY
BY
APPEALING
TO
THE
STANDARD
RESULT
FOR
THE
NORMALIZATION
COEFFICIENT
OF
A
MULTIVARIATE
GAUSSIAN
GIVING
R
EXP
E
W
DW
EXP
E
MN
R
EXP
W
M
TA
W
M
DW
EXP
E
MN
M
A
USING
WE
CAN
THEN
WRITE
THE
LOG
OF
THE
MARGINAL
LIKELIHOOD
IN
THE
FORM
LN
P
T
Α
Β
M
LN
Α
N
LN
Β
E
M
LN
A
N
LN
WHICH
IS
THE
REQUIRED
EXPRESSION
FOR
THE
EVIDENCE
FUNCTION
RETURNING
TO
THE
POLYNOMIAL
REGRESSION
PROBLEM
WE
CAN
PLOT
THE
MODEL
EVIDENCE
AGAINST
THE
ORDER
OF
THE
POLYNOMIAL
AS
SHOWN
IN
FIGURE
HERE
WE
HAVE
ASSUMED
A
PRIOR
OF
THE
FORM
WITH
THE
PARAMETER
Α
FIXED
AT
Α
THE
FORM
OF
THIS
PLOT
IS
VERY
INSTRUCTIVE
REFERRING
BACK
TO
FIGURE
WE
SEE
THAT
THE
M
POLYNOMIAL
HAS
VERY
POOR
FIT
TO
THE
DATA
AND
CONSEQUENTLY
GIVES
A
RELATIVELY
LOW
VALUE
FIGURE
PLOT
OF
THE
MODEL
EVIDENCE
VERSUS
THE
ORDER
M
FOR
THE
POLYNOMIAL
RE
GRESSION
MODEL
SHOWING
THAT
THE
EVIDENCE
FAVOURS
THE
MODEL
WITH
M
M
FOR
THE
EVIDENCE
GOING
TO
THE
M
POLYNOMIAL
GREATLY
IMPROVES
THE
DATA
FIT
AND
HENCE
THE
EVIDENCE
IS
SIGNIFICANTLY
HIGHER
HOWEVER
IN
GOING
TO
M
THE
DATA
FIT
IS
IMPROVED
ONLY
VERY
MARGINALLY
DUE
TO
THE
FACT
THAT
THE
UNDERLYING
SINUSOIDAL
FUNCTION
FROM
WHICH
THE
DATA
IS
GENERATED
IS
AN
ODD
FUNCTION
AND
SO
HAS
NO
EVEN
TERMS
IN
A
POLYNOMIAL
EXPANSION
INDEED
FIGURE
SHOWS
THAT
THE
RESIDUAL
DATA
ERROR
IS
REDUCED
ONLY
SLIGHTLY
IN
GOING
FROM
M
TO
M
BECAUSE
THIS
RICHER
MODEL
SUFFERS
A
GREATER
COMPLEXITY
PENALTY
THE
EVIDENCE
ACTUALLY
FALLS
IN
GOING
FROM
M
TO
M
WHEN
WE
GO
TO
M
WE
OBTAIN
A
SIGNIFICANT
FURTHER
IMPROVEMENT
IN
DATA
FIT
AS
SEEN
IN
FIGURE
AND
SO
THE
EVIDENCE
IS
INCREASED
AGAIN
GIVING
THE
HIGHEST
OVERALL
EVIDENCE
FOR
ANY
OF
THE
POLYNOMIALS
FURTHER
INCREASES
IN
THE
VALUE
OF
M
PRODUCE
ONLY
SMALL
IMPROVEMENTS
IN
THE
FIT
TO
THE
DATA
BUT
SUFFER
INCREASING
COMPLEXITY
PENALTY
LEADING
OVERALL
TO
A
DECREASE
IN
THE
EVIDENCE
VALUES
LOOKING
AGAIN
AT
FIGURE
WE
SEE
THAT
THE
GENERALIZATION
ERROR
IS
ROUGHLY
CONSTANT
BETWEEN
M
AND
M
AND
IT
WOULD
BE
DIFFICULT
TO
CHOOSE
BETWEEN
THESE
MODELS
ON
THE
BASIS
OF
THIS
PLOT
ALONE
THE
EVIDENCE
VALUES
HOWEVER
SHOW
A
CLEAR
PREFERENCE
FOR
M
SINCE
THIS
IS
THE
SIMPLEST
MODEL
WHICH
GIVES
A
GOOD
EXPLANATION
FOR
THE
OBSERVED
DATA
MAXIMIZING
THE
EVIDENCE
FUNCTION
LET
US
FIRST
CONSIDER
THE
MAXIMIZATION
OF
P
T
Α
Β
WITH
RESPECT
TO
Α
THIS
CAN
BE
DONE
BY
FIRST
DEFINING
THE
FOLLOWING
EIGENVECTOR
EQUATION
ΒΦTΦ
UI
ΛIUI
FROM
IT
THEN
FOLLOWS
THAT
A
HAS
EIGENVALUES
Α
ΛI
NOW
CONSIDER
THE
DERIVA
TIVE
OF
THE
TERM
INVOLVING
LN
A
IN
WITH
RESPECT
TO
Α
WE
HAVE
D
LN
A
D
DΑ
DΑ
LN
TT
ΛI
Α
D
DΑ
LN
ΛI
I
ΛI
Α
I
THUS
THE
STATIONARY
POINTS
OF
WITH
RESPECT
TO
Α
SATISFY
M
M
M
MULTIPLYING
THROUGH
BY
AND
REARRANGING
WE
OBTAIN
ΑMT
MN
ΛI
Α
I
SINCE
THERE
ARE
M
TERMS
IN
THE
SUM
OVER
I
THE
QUANTITY
Γ
CAN
BE
WRITTEN
ΛI
Α
ΛI
EXERCISE
I
THE
INTERPRETATION
OF
THE
QUANTITY
Γ
WILL
BE
DISCUSSED
SHORTLY
FROM
WE
SEE
THAT
THE
VALUE
OF
Α
THAT
MAXIMIZES
THE
MARGINAL
LIKELIHOOD
SATISFIES
Γ
Α
MT
MN
NOTE
THAT
THIS
IS
AN
IMPLICIT
SOLUTION
FOR
Α
NOT
ONLY
BECAUSE
Γ
DEPENDS
ON
Α
BUT
ALSO
BECAUSE
THE
MODE
MN
OF
THE
POSTERIOR
DISTRIBUTION
ITSELF
DEPENDS
ON
THE
CHOICE
OF
Α
WE
THEREFORE
ADOPT
AN
ITERATIVE
PROCEDURE
IN
WHICH
WE
MAKE
AN
INITIAL
CHOICE
FOR
Α
AND
USE
THIS
TO
FIND
MN
WHICH
IS
GIVEN
BY
AND
ALSO
TO
EVALUATE
Γ
WHICH
IS
GIVEN
BY
THESE
VALUES
ARE
THEN
USED
TO
RE
ESTIMATE
Α
USING
AND
THE
PROCESS
REPEATED
UNTIL
CONVERGENCE
NOTE
THAT
BECAUSE
THE
MATRIX
ΦTΦ
IS
FIXED
WE
CAN
COMPUTE
ITS
EIGENVALUES
ONCE
AT
THE
START
AND
THEN
SIMPLY
MULTIPLY
THESE
BY
Β
TO
OBTAIN
THE
ΛI
IT
SHOULD
BE
EMPHASIZED
THAT
THE
VALUE
OF
Α
HAS
BEEN
DETERMINED
PURELY
BY
LOOK
ING
AT
THE
TRAINING
DATA
IN
CONTRAST
TO
MAXIMUM
LIKELIHOOD
METHODS
NO
INDEPENDENT
DATA
SET
IS
REQUIRED
IN
ORDER
TO
OPTIMIZE
THE
MODEL
COMPLEXITY
WE
CAN
SIMILARLY
MAXIMIZE
THE
LOG
MARGINAL
LIKELIHOOD
WITH
RESPECT
TO
Β
TO
DO
THIS
WE
NOTE
THAT
THE
EIGENVALUES
ΛI
DEFINED
BY
ARE
PROPORTIONAL
TO
Β
AND
HENCE
DΛI
DΒ
ΛI
Β
GIVING
D
LN
A
D
LN
Λ
Α
ΛI
Γ
THE
STATIONARY
POINT
OF
THE
MARGINAL
LIKELIHOOD
THEREFORE
SATISFIES
EXERCISE
N
AND
REARRANGING
WE
OBTAIN
N
N
N
MT
Φ
XN
Γ
T
MT
Φ
X
AGAIN
THIS
IS
AN
IMPLICIT
SOLUTION
FOR
Β
AND
CAN
BE
SOLVED
BY
CHOOSING
AN
INITIAL
VALUE
FOR
Β
AND
THEN
USING
THIS
TO
CALCULATE
MN
AND
Γ
AND
THEN
RE
ESTIMATE
Β
USING
REPEATING
UNTIL
CONVERGENCE
IF
BOTH
Α
AND
Β
ARE
TO
BE
DETERMINED
FROM
THE
DATA
THEN
THEIR
VALUES
CAN
BE
RE
ESTIMATED
TOGETHER
AFTER
EACH
UPDATE
OF
Γ
FIGURE
CONTOURS
OF
THE
LIKELIHOOD
FUNCTION
RED
AND
THE
PRIOR
GREEN
IN
WHICH
THE
AXES
IN
PARAMETER
SPACE
HAVE
BEEN
ROTATED
TO
ALIGN
WITH
THE
EIGENVECTORS
UI
OF
THE
HESSIAN
FOR
Α
THE
MODE
OF
THE
POSTE
RIOR
IS
GIVEN
BY
THE
MAXIMUM
LIKELIHOOD
SOLUTION
WML
WHEREAS
FOR
NONZERO
Α
THE
MODE
IS
AT
WMAP
MN
IN
THE
DIRECTION
THE
EIGENVALUE
DEFINED
BY
IS
SMALL
COMPARED
WITH
Α
AND
SO
THE
QUANTITY
Α
IS
CLOSE
TO
ZERO
AND
THE
CORRESPONDING
MAP
VALUE
OF
IS
ALSO
CLOSE
TO
ZERO
BY
CONTRAST
IN
THE
DIRECTION
THE
EIGENVALUE
IS
LARGE
COMPARED
WITH
Α
AND
SO
THE
QUANTITY
Α
IS
CLOSE
TO
UNITY
AND
THE
MAP
VALUE
OF
IS
CLOSE
TO
ITS
MAXIMUM
LIKELIHOOD
VALUE
WMAP
WML
EFFECTIVE
NUMBER
OF
PARAMETERS
THE
RESULT
HAS
AN
ELEGANT
INTERPRETATION
MACKAY
WHICH
PROVIDES
INSIGHT
INTO
THE
BAYESIAN
SOLUTION
FOR
Α
TO
SEE
THIS
CONSIDER
THE
CONTOURS
OF
THE
LIKE
LIHOOD
FUNCTION
AND
THE
PRIOR
AS
ILLUSTRATED
IN
FIGURE
HERE
WE
HAVE
IMPLICITLY
TRANSFORMED
TO
A
ROTATED
SET
OF
AXES
IN
PARAMETER
SPACE
ALIGNED
WITH
THE
EIGENVEC
TORS
UI
DEFINED
IN
CONTOURS
OF
THE
LIKELIHOOD
FUNCTION
ARE
THEN
AXIS
ALIGNED
ELLIPSES
THE
EIGENVALUES
ΛI
MEASURE
THE
CURVATURE
OF
THE
LIKELIHOOD
FUNCTION
AND
SO
IN
FIGURE
THE
EIGENVALUE
IS
SMALL
COMPARED
WITH
BECAUSE
A
SMALLER
CURVATURE
CORRESPONDS
TO
A
GREATER
ELONGATION
OF
THE
CONTOURS
OF
THE
LIKELIHOOD
FUNC
TION
BECAUSE
ΒΦTΦ
IS
A
POSITIVE
DEFINITE
MATRIX
IT
WILL
HAVE
POSITIVE
EIGENVALUES
AND
SO
THE
RATIO
ΛI
ΛI
Α
WILL
LIE
BETWEEN
AND
CONSEQUENTLY
THE
QUANTITY
Γ
DEFINED
BY
WILL
LIE
IN
THE
RANGE
Γ
M
FOR
DIRECTIONS
IN
WHICH
ΛI
Α
THE
CORRESPONDING
PARAMETER
WI
WILL
BE
CLOSE
TO
ITS
MAXIMUM
LIKELIHOOD
VALUE
AND
THE
RATIO
ΛI
ΛI
Α
WILL
BE
CLOSE
TO
SUCH
PARAMETERS
ARE
CALLED
WELL
DETERMINED
BECAUSE
THEIR
VALUES
ARE
TIGHTLY
CONSTRAINED
BY
THE
DATA
CONVERSELY
FOR
DIRECTIONS
IN
WHICH
ΛI
Α
THE
CORRESPONDING
PARAMETERS
WI
WILL
BE
CLOSE
TO
ZERO
AS
WILL
THE
RATIOS
ΛI
ΛI
Α
THESE
ARE
DIRECTIONS
IN
WHICH
THE
LIKELIHOOD
FUNCTION
IS
RELATIVELY
INSENSITIVE
TO
THE
PARAMETER
VALUE
AND
SO
THE
PARAMETER
HAS
BEEN
SET
TO
A
SMALL
VALUE
BY
THE
PRIOR
THE
QUANTITY
Γ
DEFINED
BY
THEREFORE
MEASURES
THE
EFFECTIVE
TOTAL
NUMBER
OF
WELL
DETERMINED
PARAMETERS
WE
CAN
OBTAIN
SOME
INSIGHT
INTO
THE
RESULT
FOR
RE
ESTIMATING
Β
BY
COM
PARING
IT
WITH
THE
CORRESPONDING
MAXIMUM
LIKELIHOOD
RESULT
GIVEN
BY
BOTH
OF
THESE
FORMULAE
EXPRESS
THE
VARIANCE
THE
INVERSE
PRECISION
AS
AN
AVERAGE
OF
THE
SQUARED
DIFFERENCES
BETWEEN
THE
TARGETS
AND
THE
MODEL
PREDICTIONS
HOWEVER
THEY
DIFFER
IN
THAT
THE
NUMBER
OF
DATA
POINTS
N
IN
THE
DENOMINATOR
OF
THE
MAXIMUM
LIKE
LIHOOD
RESULT
IS
REPLACED
BY
N
Γ
IN
THE
BAYESIAN
RESULT
WE
RECALL
FROM
THAT
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
OF
THE
VARIANCE
FOR
A
GAUSSIAN
DISTRIBUTION
OVER
A
SINGLE
VARIABLE
X
IS
GIVEN
BY
ML
N
N
XN
N
ΜML
AND
THAT
THIS
ESTIMATE
IS
BIASED
BECAUSE
THE
MAXIMUM
LIKELIHOOD
SOLUTION
ΜML
FOR
THE
MEAN
HAS
FITTED
SOME
OF
THE
NOISE
ON
THE
DATA
IN
EFFECT
THIS
HAS
USED
UP
ONE
DEGREE
OF
FREEDOM
IN
THE
MODEL
THE
CORRESPONDING
UNBIASED
ESTIMATE
IS
GIVEN
BY
AND
TAKES
THE
FORM
X
Μ
WE
SHALL
SEE
IN
SECTION
THAT
THIS
RESULT
CAN
BE
OBTAINED
FROM
A
BAYESIAN
TREAT
MENT
IN
WHICH
WE
MARGINALIZE
OVER
THE
UNKNOWN
MEAN
THE
FACTOR
OF
N
IN
THE
DENOMINATOR
OF
THE
BAYESIAN
RESULT
TAKES
ACCOUNT
OF
THE
FACT
THAT
ONE
DEGREE
OF
FREE
DOM
HAS
BEEN
USED
IN
FITTING
THE
MEAN
AND
REMOVES
THE
BIAS
OF
MAXIMUM
LIKELIHOOD
NOW
CONSIDER
THE
CORRESPONDING
RESULTS
FOR
THE
LINEAR
REGRESSION
MODEL
THE
MEAN
OF
THE
TARGET
DISTRIBUTION
IS
NOW
GIVEN
BY
THE
FUNCTION
WTΦ
X
WHICH
CONTAINS
M
PARAMETERS
HOWEVER
NOT
ALL
OF
THESE
PARAMETERS
ARE
TUNED
TO
THE
DATA
THE
EFFECTIVE
NUMBER
OF
PARAMETERS
THAT
ARE
DETERMINED
BY
THE
DATA
IS
Γ
WITH
THE
REMAINING
M
Γ
PARAMETERS
SET
TO
SMALL
VALUES
BY
THE
PRIOR
THIS
IS
REFLECTED
IN
THE
BAYESIAN
RESULT
FOR
THE
VARIANCE
THAT
HAS
A
FACTOR
N
Γ
IN
THE
DENOMINATOR
THEREBY
CORRECTING
FOR
THE
BIAS
OF
THE
MAXIMUM
LIKELIHOOD
RESULT
WE
CAN
ILLUSTRATE
THE
EVIDENCE
FRAMEWORK
FOR
SETTING
HYPERPARAMETERS
USING
THE
SINUSOIDAL
SYNTHETIC
DATA
SET
FROM
SECTION
TOGETHER
WITH
THE
GAUSSIAN
BASIS
FUNC
TION
MODEL
COMPRISING
BASIS
FUNCTIONS
SO
THAT
THE
TOTAL
NUMBER
OF
PARAMETERS
IN
THE
MODEL
IS
GIVEN
BY
M
INCLUDING
THE
BIAS
HERE
FOR
SIMPLICITY
OF
ILLUSTRA
TION
WE
HAVE
SET
Β
TO
ITS
TRUE
VALUE
OF
AND
THEN
USED
THE
EVIDENCE
FRAMEWORK
TO
DETERMINE
Α
AS
SHOWN
IN
FIGURE
WE
CAN
ALSO
SEE
HOW
THE
PARAMETER
Α
CONTROLS
THE
MAGNITUDE
OF
THE
PARAMETERS
WI
BY
PLOTTING
THE
INDIVIDUAL
PARAMETERS
VERSUS
THE
EFFECTIVE
NUMBER
Γ
OF
PARAM
ETERS
AS
SHOWN
IN
FIGURE
IF
WE
CONSIDER
THE
LIMIT
N
M
IN
WHICH
THE
NUMBER
OF
DATA
POINTS
IS
LARGE
IN
RELATION
TO
THE
NUMBER
OF
PARAMETERS
THEN
FROM
ALL
OF
THE
PARAMETERS
WILL
BE
WELL
DETERMINED
BY
THE
DATA
BECAUSE
ΦTΦ
INVOLVES
AN
IMPLICIT
SUM
OVER
DATA
POINTS
AND
SO
THE
EIGENVALUES
ΛI
INCREASE
WITH
THE
SIZE
OF
THE
DATA
SET
IN
THIS
CASE
Γ
M
AND
THE
RE
ESTIMATION
EQUATIONS
FOR
Α
AND
Β
BECOME
M
Α
MN
Β
N
D
MN
WHERE
EW
AND
ED
ARE
DEFINED
BY
AND
RESPECTIVELY
THESE
RESULTS
CAN
BE
USED
AS
AN
EASY
TO
COMPUTE
APPROXIMATION
TO
THE
FULL
EVIDENCE
RE
ESTIMATION
LN
Α
LN
Α
FIGURE
THE
LEFT
PLOT
SHOWS
Γ
RED
CURVE
AND
MN
BLUE
CURVE
VERSUS
LN
Α
FOR
THE
SINUSOIDAL
SYNTHETIC
DATA
SET
IT
IS
THE
INTERSECTION
OF
THESE
TWO
CURVES
THAT
DEFINES
THE
OPTIMUM
VALUE
FOR
Α
GIVEN
BY
THE
EVIDENCE
PROCEDURE
THE
RIGHT
PLOT
SHOWS
THE
CORRESPONDING
GRAPH
OF
LOG
EVIDENCE
LN
P
T
Α
Β
VERSUS
LN
Α
RED
CURVE
SHOWING
THAT
THE
PEAK
COINCIDES
WITH
THE
CROSSING
POINT
OF
THE
CURVES
IN
THE
LEFT
PLOT
ALSO
SHOWN
IS
THE
TEST
SET
ERROR
BLUE
CURVE
SHOWING
THAT
THE
EVIDENCE
MAXIMUM
OCCURS
CLOSE
TO
THE
POINT
OF
BEST
GENERALIZATION
FORMULAE
BECAUSE
THEY
DO
NOT
REQUIRE
EVALUATION
OF
THE
EIGENVALUE
SPECTRUM
OF
THE
HESSIAN
FIGURE
PLOT
OF
THE
PARAMETERS
WI
FROM
THE
GAUSSIAN
BASIS
FUNCTION
MODEL
VERSUS
THE
EFFECTIVE
NUM
BER
OF
PARAMETERS
Γ
IN
WHICH
THE
WI
HYPERPARAMETER
Α
IS
VARIED
IN
THE
RANGE
Α
CAUSING
Γ
TO
VARY
IN
THE
RANGE
Γ
M
Γ
LIMITATIONS
OF
FIXED
BASIS
FUNCTIONS
THROUGHOUT
THIS
CHAPTER
WE
HAVE
FOCUSSED
ON
MODELS
COMPRISING
A
LINEAR
COMBINA
TION
OF
FIXED
NONLINEAR
BASIS
FUNCTIONS
WE
HAVE
SEEN
THAT
THE
ASSUMPTION
OF
LINEARITY
IN
THE
PARAMETERS
LED
TO
A
RANGE
OF
USEFUL
PROPERTIES
INCLUDING
CLOSED
FORM
SOLUTIONS
TO
THE
LEAST
SQUARES
PROBLEM
AS
WELL
AS
A
TRACTABLE
BAYESIAN
TREATMENT
FURTHERMORE
FOR
A
SUITABLE
CHOICE
OF
BASIS
FUNCTIONS
WE
CAN
MODEL
ARBITRARY
NONLINEARITIES
IN
THE
MAPPING
FROM
INPUT
VARIABLES
TO
TARGETS
IN
THE
NEXT
CHAPTER
WE
SHALL
STUDY
AN
ANAL
OGOUS
CLASS
OF
MODELS
FOR
CLASSIFICATION
IT
MIGHT
APPEAR
THEREFORE
THAT
SUCH
LINEAR
MODELS
CONSTITUTE
A
GENERAL
PURPOSE
FRAMEWORK
FOR
SOLVING
PROBLEMS
IN
PATTERN
RECOGNITION
UNFORTUNATELY
THERE
ARE
SOME
SIGNIFICANT
SHORTCOMINGS
WITH
LINEAR
MODELS
WHICH
WILL
CAUSE
US
TO
TURN
IN
LATER
CHAPTERS
TO
MORE
COMPLEX
MODELS
SUCH
AS
SUPPORT
VECTOR
MACHINES
AND
NEURAL
NETWORKS
THE
DIFFICULTY
STEMS
FROM
THE
ASSUMPTION
THAT
THE
BASIS
FUNCTIONS
ΦJ
X
ARE
FIXED
BEFORE
THE
TRAINING
DATA
SET
IS
OBSERVED
AND
IS
A
MANIFESTATION
OF
THE
CURSE
OF
DIMEN
SIONALITY
DISCUSSED
IN
SECTION
AS
A
CONSEQUENCE
THE
NUMBER
OF
BASIS
FUNCTIONS
NEEDS
TO
GROW
RAPIDLY
OFTEN
EXPONENTIALLY
WITH
THE
DIMENSIONALITY
D
OF
THE
INPUT
SPACE
FORTUNATELY
THERE
ARE
TWO
PROPERTIES
OF
REAL
DATA
SETS
THAT
WE
CAN
EXPLOIT
TO
HELP
ALLEVIATE
THIS
PROBLEM
FIRST
OF
ALL
THE
DATA
VECTORS
XN
TYPICALLY
LIE
CLOSE
TO
A
NON
LINEAR
MANIFOLD
WHOSE
INTRINSIC
DIMENSIONALITY
IS
SMALLER
THAN
THAT
OF
THE
INPUT
SPACE
AS
A
RESULT
OF
STRONG
CORRELATIONS
BETWEEN
THE
INPUT
VARIABLES
WE
WILL
SEE
AN
EXAMPLE
OF
THIS
WHEN
WE
CONSIDER
IMAGES
OF
HANDWRITTEN
DIGITS
IN
CHAPTER
IF
WE
ARE
USING
LOCALIZED
BASIS
FUNCTIONS
WE
CAN
ARRANGE
THAT
THEY
ARE
SCATTERED
IN
INPUT
SPACE
ONLY
IN
REGIONS
CONTAINING
DATA
THIS
APPROACH
IS
USED
IN
RADIAL
BASIS
FUNCTION
NETWORKS
AND
ALSO
IN
SUPPORT
VECTOR
AND
RELEVANCE
VECTOR
MACHINES
NEURAL
NETWORK
MODELS
WHICH
USE
ADAPTIVE
BASIS
FUNCTIONS
HAVING
SIGMOIDAL
NONLINEARITIES
CAN
ADAPT
THE
PARAMETERS
SO
THAT
THE
REGIONS
OF
INPUT
SPACE
OVER
WHICH
THE
BASIS
FUNCTIONS
VARY
CORRESPONDS
TO
THE
DATA
MANIFOLD
THE
SECOND
PROPERTY
IS
THAT
TARGET
VARIABLES
MAY
HAVE
SIGNIFICANT
DEPENDENCE
ON
ONLY
A
SMALL
NUMBER
OF
POSSIBLE
DIRECTIONS
WITHIN
THE
DATA
MANIFOLD
NEURAL
NETWORKS
CAN
EXPLOIT
THIS
PROPERTY
BY
CHOOSING
THE
DIRECTIONS
IN
INPUT
SPACE
TO
WHICH
THE
BASIS
FUNCTIONS
RESPOND
EXERCISES
WWW
SHOW
THAT
THE
TANH
FUNCTION
AND
THE
LOGISTIC
SIGMOID
FUNCTION
ARE
RELATED
BY
TANH
A
HENCE
SHOW
THAT
A
GENERAL
LINEAR
COMBINATION
OF
LOGISTIC
SIGMOID
FUNCTIONS
OF
THE
FORM
Y
X
W
J
WJΣ
X
ΜJ
IS
EQUIVALENT
TO
A
LINEAR
COMBINATION
OF
TANH
FUNCTIONS
OF
THE
FORM
Y
X
U
UJ
J
TANH
X
ΜJ
AND
FIND
EXPRESSIONS
TO
RELATE
THE
NEW
PARAMETERS
UM
TO
THE
ORIGINAL
PA
RAMETERS
WM
SHOW
THAT
THE
MATRIX
Φ
ΦTΦ
TAKES
ANY
VECTOR
V
AND
PROJECTS
IT
ONTO
THE
SPACE
SPANNED
BY
THE
COLUMNS
OF
Φ
USE
THIS
RESULT
TO
SHOW
THAT
THE
LEAST
SQUARES
SOLUTION
CORRESPONDS
TO
AN
ORTHOGONAL
PROJECTION
OF
THE
VECTOR
T
ONTO
THE
MANIFOLD
AS
SHOWN
IN
FIGURE
CONSIDER
A
DATA
SET
IN
WHICH
EACH
DATA
POINT
TN
IS
ASSOCIATED
WITH
A
WEIGHTING
FACTOR
RN
SO
THAT
THE
SUM
OF
SQUARES
ERROR
FUNCTION
BECOMES
E
W
R
T
WTΦ
X
FIND
AN
EXPRESSION
FOR
THE
SOLUTION
W
THAT
MINIMIZES
THIS
ERROR
FUNCTION
GIVE
TWO
ALTERNATIVE
INTERPRETATIONS
OF
THE
WEIGHTED
SUM
OF
SQUARES
ERROR
FUNCTION
IN
TERMS
OF
I
DATA
DEPENDENT
NOISE
VARIANCE
AND
II
REPLICATED
DATA
POINTS
WWW
CONSIDER
A
LINEAR
MODEL
OF
THE
FORM
D
Y
X
W
WIXI
I
TOGETHER
WITH
A
SUM
OF
SQUARES
ERROR
FUNCTION
OF
THE
FORM
E
W
Y
X
W
T
NOW
SUPPOSE
THAT
GAUSSIAN
NOISE
EI
WITH
ZERO
MEAN
AND
VARIANCE
IS
ADDED
IN
DEPENDENTLY
TO
EACH
OF
THE
INPUT
VARIABLES
XI
BY
MAKING
USE
OF
E
EI
AND
E
EIEJ
SHOW
THAT
MINIMIZING
ED
AVERAGED
OVER
THE
NOISE
DISTRIBUTION
IS
EQUIVALENT
TO
MINIMIZING
THE
SUM
OF
SQUARES
ERROR
FOR
NOISE
FREE
INPUT
VARIABLES
WITH
THE
ADDITION
OF
A
WEIGHT
DECAY
REGULARIZATION
TERM
IN
WHICH
THE
BIAS
PARAMETER
IS
OMITTED
FROM
THE
REGULARIZER
WWW
USING
THE
TECHNIQUE
OF
LAGRANGE
MULTIPLIERS
DISCUSSED
IN
APPENDIX
E
SHOW
THAT
MINIMIZATION
OF
THE
REGULARIZED
ERROR
FUNCTION
IS
EQUIVALENT
TO
MINI
MIZING
THE
UNREGULARIZED
SUM
OF
SQUARES
ERROR
SUBJECT
TO
THE
CONSTRAINT
DISCUSS
THE
RELATIONSHIP
BETWEEN
THE
PARAMETERS
Η
AND
Λ
WWW
CONSIDER
A
LINEAR
BASIS
FUNCTION
REGRESSION
MODEL
FOR
A
MULTIVARIATE
TARGET
VARIABLE
T
HAVING
A
GAUSSIAN
DISTRIBUTION
OF
THE
FORM
P
T
W
Σ
N
T
Y
X
W
Σ
WHERE
Y
X
W
WTΦ
X
TOGETHER
WITH
A
TRAINING
DATA
SET
COMPRISING
INPUT
BASIS
VECTORS
Φ
XN
AND
CORRE
SPONDING
TARGET
VECTORS
TN
WITH
N
N
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
WML
FOR
THE
PARAMETER
MATRIX
W
HAS
THE
PROPERTY
THAT
EACH
COLUMN
IS
GIVEN
BY
AN
EXPRESSION
OF
THE
FORM
WHICH
WAS
THE
SOLUTION
FOR
AN
ISOTROPIC
NOISE
DISTRIBUTION
NOTE
THAT
THIS
IS
INDEPENDENT
OF
THE
COVARIANCE
MATRIX
Σ
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
Σ
IS
GIVEN
BY
Σ
N
N
TN
N
T
Φ
XN
TN
T
Φ
XN
T
BY
USING
THE
TECHNIQUE
OF
COMPLETING
THE
SQUARE
VERIFY
THE
RESULT
FOR
THE
POSTERIOR
DISTRIBUTION
OF
THE
PARAMETERS
W
IN
THE
LINEAR
BASIS
FUNCTION
MODEL
IN
WHICH
MN
AND
SN
ARE
DEFINED
BY
AND
RESPECTIVELY
WWW
CONSIDER
THE
LINEAR
BASIS
FUNCTION
MODEL
IN
SECTION
AND
SUPPOSE
THAT
WE
HAVE
ALREADY
OBSERVED
N
DATA
POINTS
SO
THAT
THE
POSTERIOR
DISTRIBUTION
OVER
W
IS
GIVEN
BY
THIS
POSTERIOR
CAN
BE
REGARDED
AS
THE
PRIOR
FOR
THE
NEXT
OBSER
VATION
BY
CONSIDERING
AN
ADDITIONAL
DATA
POINT
XN
TN
AND
BY
COMPLETING
THE
SQUARE
IN
THE
EXPONENTIAL
SHOW
THAT
THE
RESULTING
POSTERIOR
DISTRIBUTION
IS
AGAIN
GIVEN
BY
BUT
WITH
SN
REPLACED
BY
SN
AND
MN
REPLACED
BY
MN
REPEAT
THE
PREVIOUS
EXERCISE
BUT
INSTEAD
OF
COMPLETING
THE
SQUARE
BY
HAND
MAKE
USE
OF
THE
GENERAL
RESULT
FOR
LINEAR
GAUSSIAN
MODELS
GIVEN
BY
WWW
BY
MAKING
USE
OF
THE
RESULT
TO
EVALUATE
THE
INTEGRAL
IN
VERIFY
THAT
THE
PREDICTIVE
DISTRIBUTION
FOR
THE
BAYESIAN
LINEAR
REGRESSION
MODEL
IS
GIVEN
BY
IN
WHICH
THE
INPUT
DEPENDENT
VARIANCE
IS
GIVEN
BY
WE
HAVE
SEEN
THAT
AS
THE
SIZE
OF
A
DATA
SET
INCREASES
THE
UNCERTAINTY
ASSOCIATED
WITH
THE
POSTERIOR
DISTRIBUTION
OVER
MODEL
PARAMETERS
DECREASES
MAKE
USE
OF
THE
MATRIX
IDENTITY
APPENDIX
C
M
VTM
TO
SHOW
THAT
THE
UNCERTAINTY
X
ASSOCIATED
WITH
THE
LINEAR
REGRESSION
FUNCTION
GIVEN
BY
SATISFIES
N
X
X
WE
SAW
IN
SECTION
THAT
THE
CONJUGATE
PRIOR
FOR
A
GAUSSIAN
DISTRIBUTION
WITH
UNKNOWN
MEAN
AND
UNKNOWN
PRECISION
INVERSE
VARIANCE
IS
A
NORMAL
GAMMA
DISTRIBUTION
THIS
PROPERTY
ALSO
HOLDS
FOR
THE
CASE
OF
THE
CONDITIONAL
GAUSSIAN
DIS
TRIBUTION
P
T
X
W
Β
OF
THE
LINEAR
REGRESSION
MODEL
IF
WE
CONSIDER
THE
LIKELIHOOD
FUNCTION
THEN
THE
CONJUGATE
PRIOR
FOR
W
AND
Β
IS
GIVEN
BY
P
W
Β
N
W
Β
GAM
Β
SHOW
THAT
THE
CORRESPONDING
POSTERIOR
DISTRIBUTION
TAKES
THE
SAME
FUNCTIONAL
FORM
SO
THAT
P
W
Β
T
N
W
MN
Β
GAM
Β
AN
BN
AND
FIND
EXPRESSIONS
FOR
THE
POSTERIOR
PARAMETERS
MN
SN
AN
AND
BN
SHOW
THAT
THE
PREDICTIVE
DISTRIBUTION
P
T
X
T
FOR
THE
MODEL
DISCUSSED
IN
EX
ERCISE
IS
GIVEN
BY
A
STUDENT
T
DISTRIBUTION
OF
THE
FORM
P
T
X
T
ST
T
Μ
Λ
Ν
AND
OBTAIN
EXPRESSIONS
FOR
Μ
Λ
AND
Ν
IN
THIS
EXERCISE
WE
EXPLORE
IN
MORE
DETAIL
THE
PROPERTIES
OF
THE
EQUIVALENT
KERNEL
DEFINED
BY
WHERE
SN
IS
DEFINED
BY
SUPPOSE
THAT
THE
BASIS
FUNCTIONS
ΦJ
X
ARE
LINEARLY
INDEPENDENT
AND
THAT
THE
NUMBER
N
OF
DATA
POINTS
IS
GREATER
THAN
THE
NUMBER
M
OF
BASIS
FUNCTIONS
FURTHERMORE
LET
ONE
OF
THE
BASIS
FUNCTIONS
BE
CONSTANT
SAY
X
BY
TAKING
SUITABLE
LINEAR
COMBINATIONS
OF
THESE
BASIS
FUNCTIONS
WE
CAN
CONSTRUCT
A
NEW
BASIS
SET
ΨJ
X
SPANNING
THE
SAME
SPACE
BUT
THAT
ARE
ORTHONORMAL
SO
THAT
N
ΨJ
XN
ΨK
XN
IJK
N
WHERE
IJK
IS
DEFINED
TO
BE
IF
J
K
AND
OTHERWISE
AND
WE
TAKE
X
SHOW
THAT
FOR
Α
THE
EQUIVALENT
KERNEL
CAN
BE
WRITTEN
AS
K
X
XI
Ψ
X
TΨ
XI
WHERE
Ψ
ΨM
T
USE
THIS
RESULT
TO
SHOW
THAT
THE
KERNEL
SATISFIES
THE
SUMMATION
CONSTRAINT
N
K
X
XN
N
WWW
CONSIDER
A
LINEAR
BASIS
FUNCTION
MODEL
FOR
REGRESSION
IN
WHICH
THE
PA
RAMETERS
Α
AND
Β
ARE
SET
USING
THE
EVIDENCE
FRAMEWORK
SHOW
THAT
THE
FUNCTION
E
MN
DEFINED
BY
SATISFIES
THE
RELATION
MN
N
DERIVE
THE
RESULT
FOR
THE
LOG
EVIDENCE
FUNCTION
P
T
Α
Β
OF
THE
LINEAR
REGRESSION
MODEL
BY
MAKING
USE
OF
TO
EVALUATE
THE
INTEGRAL
DIRECTLY
SHOW
THAT
THE
EVIDENCE
FUNCTION
FOR
THE
BAYESIAN
LINEAR
REGRESSION
MODEL
CAN
BE
WRITTEN
IN
THE
FORM
IN
WHICH
E
W
IS
DEFINED
BY
WWW
BY
COMPLETING
THE
SQUARE
OVER
W
SHOW
THAT
THE
ERROR
FUNCTION
IN
BAYESIAN
LINEAR
REGRESSION
CAN
BE
WRITTEN
IN
THE
FORM
SHOW
THAT
THE
INTEGRATION
OVER
W
IN
THE
BAYESIAN
LINEAR
REGRESSION
MODEL
GIVES
THE
RESULT
HENCE
SHOW
THAT
THE
LOG
MARGINAL
LIKELIHOOD
IS
GIVEN
BY
WWW
STARTING
FROM
VERIFY
ALL
OF
THE
STEPS
NEEDED
TO
SHOW
THAT
MAXI
MIZATION
OF
THE
LOG
MARGINAL
LIKELIHOOD
FUNCTION
WITH
RESPECT
TO
Α
LEADS
TO
THE
RE
ESTIMATION
EQUATION
AN
ALTERNATIVE
WAY
TO
DERIVE
THE
RESULT
FOR
THE
OPTIMAL
VALUE
OF
Α
IN
THE
EVIDENCE
FRAMEWORK
IS
TO
MAKE
USE
OF
THE
IDENTITY
D
LN
A
TR
A
D
A
PROVE
THIS
IDENTITY
BY
CONSIDERING
THE
EIGENVALUE
EXPANSION
OF
A
REAL
SYMMETRIC
MATRIX
A
AND
MAKING
USE
OF
THE
STANDARD
RESULTS
FOR
THE
DETERMINANT
AND
TRACE
OF
A
EXPRESSED
IN
TERMS
OF
ITS
EIGENVALUES
APPENDIX
C
THEN
MAKE
USE
OF
TO
DERIVE
STARTING
FROM
STARTING
FROM
VERIFY
ALL
OF
THE
STEPS
NEEDED
TO
SHOW
THAT
MAXIMIZA
TION
OF
THE
LOG
MARGINAL
LIKELIHOOD
FUNCTION
WITH
RESPECT
TO
Β
LEADS
TO
THE
RE
ESTIMATION
EQUATION
WWW
SHOW
THAT
THE
MARGINAL
PROBABILITY
OF
THE
DATA
IN
OTHER
WORDS
THE
MODEL
EVIDENCE
FOR
THE
MODEL
DESCRIBED
IN
EXERCISE
IS
GIVEN
BY
P
T
Γ
AN
SN
N
BAN
Γ
BY
FIRST
MARGINALIZING
WITH
RESPECT
TO
W
AND
THEN
WITH
RESPECT
TO
Β
REPEAT
THE
PREVIOUS
EXERCISE
BUT
NOW
USE
BAYES
THEOREM
IN
THE
FORM
P
T
P
T
W
Β
P
W
Β
P
W
Β
T
AND
THEN
SUBSTITUTE
FOR
THE
PRIOR
AND
POSTERIOR
DISTRIBUTIONS
AND
THE
LIKELIHOOD
FUNC
TION
IN
ORDER
TO
DERIVE
THE
RESULT
IN
THE
PREVIOUS
CHAPTER
WE
EXPLORED
A
CLASS
OF
REGRESSION
MODELS
HAVING
PARTICULARLY
SIMPLE
ANALYTICAL
AND
COMPUTATIONAL
PROPERTIES
WE
NOW
DISCUSS
AN
ANALOGOUS
CLASS
OF
MODELS
FOR
SOLVING
CLASSIFICATION
PROBLEMS
THE
GOAL
IN
CLASSIFICATION
IS
TO
TAKE
AN
INPUT
VECTOR
X
AND
TO
ASSIGN
IT
TO
ONE
OF
K
DISCRETE
CLASSES
K
WHERE
K
K
IN
THE
MOST
COMMON
SCENARIO
THE
CLASSES
ARE
TAKEN
TO
BE
DISJOINT
SO
THAT
EACH
INPUT
IS
ASSIGNED
TO
ONE
AND
ONLY
ONE
CLASS
THE
INPUT
SPACE
IS
THEREBY
DIVIDED
INTO
DECISION
REGIONS
WHOSE
BOUNDARIES
ARE
CALLED
DECISION
BOUNDARIES
OR
DECISION
SURFACES
IN
THIS
CHAPTER
WE
CONSIDER
LINEAR
MODELS
FOR
CLASSIFICATION
BY
WHICH
WE
MEAN
THAT
THE
DECISION
SURFACES
ARE
LINEAR
FUNCTIONS
OF
THE
INPUT
VECTOR
X
AND
HENCE
ARE
DEFINED
BY
D
DIMENSIONAL
HYPERPLANES
WITHIN
THE
D
DIMENSIONAL
INPUT
SPACE
DATA
SETS
WHOSE
CLASSES
CAN
BE
SEPARATED
EXACTLY
BY
LINEAR
DECISION
SURFACES
ARE
SAID
TO
BE
LINEARLY
SEPARABLE
FOR
REGRESSION
PROBLEMS
THE
TARGET
VARIABLE
T
WAS
SIMPLY
THE
VECTOR
OF
REAL
NUM
BERS
WHOSE
VALUES
WE
WISH
TO
PREDICT
IN
THE
CASE
OF
CLASSIFICATION
THERE
ARE
VARIOUS
WAYS
OF
USING
TARGET
VALUES
TO
REPRESENT
CLASS
LABELS
FOR
PROBABILISTIC
MODELS
THE
MOST
CONVENIENT
IN
THE
CASE
OF
TWO
CLASS
PROBLEMS
IS
THE
BINARY
REPRESENTATION
IN
WHICH
THERE
IS
A
SINGLE
TARGET
VARIABLE
T
SUCH
THAT
T
REPRESENTS
CLASS
AND
T
REPRESENTS
CLASS
WE
CAN
INTERPRET
THE
VALUE
OF
T
AS
THE
PROBABILITY
THAT
THE
CLASS
IS
WITH
THE
VALUES
OF
PROBABILITY
TAKING
ONLY
THE
EXTREME
VALUES
OF
AND
FOR
K
CLASSES
IT
IS
CONVENIENT
TO
USE
A
OF
K
CODING
SCHEME
IN
WHICH
T
IS
A
VECTOR
OF
LENGTH
K
SUCH
THAT
IF
THE
CLASS
IS
J
THEN
ALL
ELEMENTS
TK
OF
T
ARE
ZERO
EXCEPT
ELEMENT
TJ
WHICH
TAKES
THE
VALUE
FOR
INSTANCE
IF
WE
HAVE
K
CLASSES
THEN
A
PATTERN
FROM
CLASS
WOULD
BE
GIVEN
THE
TARGET
VECTOR
T
T
AGAIN
WE
CAN
INTERPRET
THE
VALUE
OF
TK
AS
THE
PROBABILITY
THAT
THE
CLASS
IS
K
FOR
NONPROBABILISTIC
MODELS
ALTERNATIVE
CHOICES
OF
TARGET
VARIABLE
REPRESENTATION
WILL
SOMETIMES
PROVE
CONVENIENT
IN
CHAPTER
WE
IDENTIFIED
THREE
DISTINCT
APPROACHES
TO
THE
CLASSIFICATION
PROB
LEM
THE
SIMPLEST
INVOLVES
CONSTRUCTING
A
DISCRIMINANT
FUNCTION
THAT
DIRECTLY
ASSIGNS
EACH
VECTOR
X
TO
A
SPECIFIC
CLASS
A
MORE
POWERFUL
APPROACH
HOWEVER
MODELS
THE
CONDITIONAL
PROBABILITY
DISTRIBUTION
P
K
X
IN
AN
INFERENCE
STAGE
AND
THEN
SUBSE
QUENTLY
USES
THIS
DISTRIBUTION
TO
MAKE
OPTIMAL
DECISIONS
BY
SEPARATING
INFERENCE
AND
DECISION
WE
GAIN
NUMEROUS
BENEFITS
AS
DISCUSSED
IN
SECTION
THERE
ARE
TWO
DIFFERENT
APPROACHES
TO
DETERMINING
THE
CONDITIONAL
PROBABILITIES
P
K
X
ONE
TECHNIQUE
IS
TO
MODEL
THEM
DIRECTLY
FOR
EXAMPLE
BY
REPRESENTING
THEM
AS
PARAMETRIC
MODELS
AND
THEN
OPTIMIZING
THE
PARAMETERS
USING
A
TRAINING
SET
ALTERNATIVELY
WE
CAN
ADOPT
A
GENERATIVE
APPROACH
IN
WHICH
WE
MODEL
THE
CLASS
CONDITIONAL
DENSITIES
GIVEN
BY
P
X
K
TOGETHER
WITH
THE
PRIOR
PROBABILITIES
P
K
FOR
THE
CLASSES
AND
THEN
WE
COMPUTE
THE
REQUIRED
POSTERIOR
PROBABILITIES
USING
BAYES
THEOREM
P
CK
X
P
X
CK
P
CK
P
X
WE
SHALL
DISCUSS
EXAMPLES
OF
ALL
THREE
APPROACHES
IN
THIS
CHAPTER
IN
THE
LINEAR
REGRESSION
MODELS
CONSIDERED
IN
CHAPTER
THE
MODEL
PREDICTION
Y
X
W
WAS
GIVEN
BY
A
LINEAR
FUNCTION
OF
THE
PARAMETERS
W
IN
THE
SIMPLEST
CASE
THE
MODEL
IS
ALSO
LINEAR
IN
THE
INPUT
VARIABLES
AND
THEREFORE
TAKES
THE
FORM
Y
X
WTX
SO
THAT
Y
IS
A
REAL
NUMBER
FOR
CLASSIFICATION
PROBLEMS
HOWEVER
WE
WISH
TO
PREDICT
DISCRETE
CLASS
LABELS
OR
MORE
GENERALLY
POSTERIOR
PROBABILITIES
THAT
LIE
IN
THE
RANGE
TO
ACHIEVE
THIS
WE
CONSIDER
A
GENERALIZATION
OF
THIS
MODEL
IN
WHICH
WE
TRANSFORM
THE
LINEAR
FUNCTION
OF
W
USING
A
NONLINEAR
FUNCTION
F
SO
THAT
Y
X
F
WTX
IN
THE
MACHINE
LEARNING
LITERATURE
F
IS
KNOWN
AS
AN
ACTIVATION
FUNCTION
WHEREAS
ITS
INVERSE
IS
CALLED
A
LINK
FUNCTION
IN
THE
STATISTICS
LITERATURE
THE
DECISION
SURFACES
CORRESPOND
TO
Y
X
CONSTANT
SO
THAT
WTX
CONSTANT
AND
HENCE
THE
DECI
SION
SURFACES
ARE
LINEAR
FUNCTIONS
OF
X
EVEN
IF
THE
FUNCTION
F
IS
NONLINEAR
FOR
THIS
REASON
THE
CLASS
OF
MODELS
DESCRIBED
BY
ARE
CALLED
GENERALIZED
LINEAR
MODELS
MCCULLAGH
AND
NELDER
NOTE
HOWEVER
THAT
IN
CONTRAST
TO
THE
MODELS
USED
FOR
REGRESSION
THEY
ARE
NO
LONGER
LINEAR
IN
THE
PARAMETERS
DUE
TO
THE
PRESENCE
OF
THE
NONLINEAR
FUNCTION
F
THIS
WILL
LEAD
TO
MORE
COMPLEX
ANALYTICAL
AND
COMPUTA
TIONAL
PROPERTIES
THAN
FOR
LINEAR
REGRESSION
MODELS
NEVERTHELESS
THESE
MODELS
ARE
STILL
RELATIVELY
SIMPLE
COMPARED
TO
THE
MORE
GENERAL
NONLINEAR
MODELS
THAT
WILL
BE
STUDIED
IN
SUBSEQUENT
CHAPTERS
THE
ALGORITHMS
DISCUSSED
IN
THIS
CHAPTER
WILL
BE
EQUALLY
APPLICABLE
IF
WE
FIRST
MAKE
A
FIXED
NONLINEAR
TRANSFORMATION
OF
THE
INPUT
VARIABLES
USING
A
VECTOR
OF
BASIS
FUNCTIONS
Φ
X
AS
WE
DID
FOR
REGRESSION
MODELS
IN
CHAPTER
WE
BEGIN
BY
CONSIDER
ING
CLASSIFICATION
DIRECTLY
IN
THE
ORIGINAL
INPUT
SPACE
X
WHILE
IN
SECTION
WE
SHALL
FIND
IT
CONVENIENT
TO
SWITCH
TO
A
NOTATION
INVOLVING
BASIS
FUNCTIONS
FOR
CONSISTENCY
WITH
LATER
CHAPTERS
DISCRIMINANT
FUNCTIONS
A
DISCRIMINANT
IS
A
FUNCTION
THAT
TAKES
AN
INPUT
VECTOR
X
AND
ASSIGNS
IT
TO
ONE
OF
K
CLASSES
DENOTED
K
IN
THIS
CHAPTER
WE
SHALL
RESTRICT
ATTENTION
TO
LINEAR
DISCRIMINANTS
NAMELY
THOSE
FOR
WHICH
THE
DECISION
SURFACES
ARE
HYPERPLANES
TO
SIMPLIFY
THE
DIS
CUSSION
WE
CONSIDER
FIRST
THE
CASE
OF
TWO
CLASSES
AND
THEN
INVESTIGATE
THE
EXTENSION
TO
K
CLASSES
TWO
CLASSES
THE
SIMPLEST
REPRESENTATION
OF
A
LINEAR
DISCRIMINANT
FUNCTION
IS
OBTAINED
BY
TAK
ING
A
LINEAR
FUNCTION
OF
THE
INPUT
VECTOR
SO
THAT
Y
X
WTX
WHERE
W
IS
CALLED
A
WEIGHT
VECTOR
AND
IS
A
BIAS
NOT
TO
BE
CONFUSED
WITH
BIAS
IN
THE
STATISTICAL
SENSE
THE
NEGATIVE
OF
THE
BIAS
IS
SOMETIMES
CALLED
A
THRESHOLD
AN
INPUT
VECTOR
X
IS
ASSIGNED
TO
CLASS
IF
Y
X
AND
TO
CLASS
OTHERWISE
THE
COR
RESPONDING
DECISION
BOUNDARY
IS
THEREFORE
DEFINED
BY
THE
RELATION
Y
X
WHICH
CORRESPONDS
TO
A
D
DIMENSIONAL
HYPERPLANE
WITHIN
THE
D
DIMENSIONAL
INPUT
SPACE
CONSIDER
TWO
POINTS
XA
AND
XB
BOTH
OF
WHICH
LIE
ON
THE
DECISION
SURFACE
BECAUSE
Y
XA
Y
XB
WE
HAVE
WT
XA
XB
AND
HENCE
THE
VECTOR
W
IS
ORTHOGONAL
TO
EVERY
VECTOR
LYING
WITHIN
THE
DECISION
SURFACE
AND
SO
W
DETERMINES
THE
ORIENTATION
OF
THE
DECISION
SURFACE
SIMILARLY
IF
X
IS
A
POINT
ON
THE
DECISION
SURFACE
THEN
Y
X
AND
SO
THE
NORMAL
DISTANCE
FROM
THE
ORIGIN
TO
THE
DECISION
SURFACE
IS
GIVEN
BY
WTX
LWL
W
LWL
WE
THEREFORE
SEE
THAT
THE
BIAS
PARAMETER
DETERMINES
THE
LOCATION
OF
THE
DECISION
SURFACE
THESE
PROPERTIES
ARE
ILLUSTRATED
FOR
THE
CASE
OF
D
IN
FIGURE
FURTHERMORE
WE
NOTE
THAT
THE
VALUE
OF
Y
X
GIVES
A
SIGNED
MEASURE
OF
THE
PER
PENDICULAR
DISTANCE
R
OF
THE
POINT
X
FROM
THE
DECISION
SURFACE
TO
SEE
THIS
CONSIDER
FIGURE
ILLUSTRATION
OF
THE
GEOMETRY
OF
A
LINEAR
DISCRIMINANT
FUNCTION
IN
TWO
DIMENSIONS
THE
DECISION
SURFACE
SHOWN
IN
RED
IS
PERPEN
Y
DICULAR
TO
W
AND
ITS
DISPLACEMENT
FROM
THE
Y
ORIGIN
IS
CONTROLLED
BY
THE
BIAS
PARAMETER
ALSO
THE
SIGNED
ORTHOGONAL
DISTANCE
OF
A
GEN
ERAL
POINT
X
FROM
THE
DECISION
SURFACE
IS
GIVEN
BY
Y
X
W
AN
ARBITRARY
POINT
X
AND
LET
X
BE
ITS
ORTHOGONAL
PROJECTION
ONTO
THE
DECISION
SURFACE
SO
THAT
X
X
R
W
LWL
MULTIPLYING
BOTH
SIDES
OF
THIS
RESULT
BY
WT
AND
ADDING
AND
MAKING
USE
OF
Y
X
WTX
AND
Y
X
WTX
WE
HAVE
Y
X
R
THIS
RESULT
IS
ILLUSTRATED
IN
FIGURE
LWL
AS
WITH
THE
LINEAR
REGRESSION
MODELS
IN
CHAPTER
IT
IS
SOMETIMES
CONVENIENT
TO
USE
A
MORE
COMPACT
NOTATION
IN
WHICH
WE
INTRODUCE
AN
ADDITIONAL
DUMMY
INPUT
VALUE
X
AND
THEN
DEFINE
W
W
W
AND
X
X
X
SO
THAT
Y
X
WTX
IN
THIS
CASE
THE
DECISION
SURFACES
ARE
D
DIMENSIONAL
HYPERPLANES
PASSING
THROUGH
THE
ORIGIN
OF
THE
D
DIMENSIONAL
EXPANDED
INPUT
SPACE
MULTIPLE
CLASSES
NOW
CONSIDER
THE
EXTENSION
OF
LINEAR
DISCRIMINANTS
TO
K
CLASSES
WE
MIGHT
BE
TEMPTED
BE
TO
BUILD
A
K
CLASS
DISCRIMINANT
BY
COMBINING
A
NUMBER
OF
TWO
CLASS
DISCRIMINANT
FUNCTIONS
HOWEVER
THIS
LEADS
TO
SOME
SERIOUS
DIFFICULTIES
DUDA
AND
HART
AS
WE
NOW
SHOW
CONSIDER
THE
USE
OF
K
CLASSIFIERS
EACH
OF
WHICH
SOLVES
A
TWO
CLASS
PROBLEM
OF
SEPARATING
POINTS
IN
A
PARTICULAR
CLASS
K
FROM
POINTS
NOT
IN
THAT
CLASS
THIS
IS
KNOWN
AS
A
ONE
VERSUS
THE
REST
CLASSIFIER
THE
LEFT
HAND
EXAMPLE
IN
FIGURE
SHOWS
AN
NOT
C
NOT
FIGURE
ATTEMPTING
TO
CONSTRUCT
A
K
CLASS
DISCRIMINANT
FROM
A
SET
OF
TWO
CLASS
DISCRIMINANTS
LEADS
TO
AM
BIGUOUS
REGIONS
SHOWN
IN
GREEN
ON
THE
LEFT
IS
AN
EXAMPLE
INVOLVING
THE
USE
OF
TWO
DISCRIMINANTS
DESIGNED
TO
DISTINGUISH
POINTS
IN
CLASS
CK
FROM
POINTS
NOT
IN
CLASS
CK
ON
THE
RIGHT
IS
AN
EXAMPLE
INVOLVING
THREE
DISCRIMINANT
FUNCTIONS
EACH
OF
WHICH
IS
USED
TO
SEPARATE
A
PAIR
OF
CLASSES
CK
AND
CJ
EXAMPLE
INVOLVING
THREE
CLASSES
WHERE
THIS
APPROACH
LEADS
TO
REGIONS
OF
INPUT
SPACE
THAT
ARE
AMBIGUOUSLY
CLASSIFIED
AN
ALTERNATIVE
IS
TO
INTRODUCE
K
K
BINARY
DISCRIMINANT
FUNCTIONS
ONE
FOR
EVERY
POSSIBLE
PAIR
OF
CLASSES
THIS
IS
KNOWN
AS
A
ONE
VERSUS
ONE
CLASSIFIER
EACH
POINT
IS
THEN
CLASSIFIED
ACCORDING
TO
A
MAJORITY
VOTE
AMONGST
THE
DISCRIMINANT
FUNC
TIONS
HOWEVER
THIS
TOO
RUNS
INTO
THE
PROBLEM
OF
AMBIGUOUS
REGIONS
AS
ILLUSTRATED
IN
THE
RIGHT
HAND
DIAGRAM
OF
FIGURE
WE
CAN
AVOID
THESE
DIFFICULTIES
BY
CONSIDERING
A
SINGLE
K
CLASS
DISCRIMINANT
COMPRISING
K
LINEAR
FUNCTIONS
OF
THE
FORM
YK
X
WTX
AND
THEN
ASSIGNING
A
POINT
X
TO
CLASS
CK
IF
YK
X
YJ
X
FOR
ALL
J
K
THE
DECISION
BOUNDARY
BETWEEN
CLASS
CK
AND
CLASS
CJ
IS
THEREFORE
GIVEN
BY
YK
X
YJ
X
AND
HENCE
CORRESPONDS
TO
A
D
DIMENSIONAL
HYPERPLANE
DEFINED
BY
WK
WJ
TX
THIS
HAS
THE
SAME
FORM
AS
THE
DECISION
BOUNDARY
FOR
THE
TWO
CLASS
CASE
DISCUSSED
IN
SECTION
AND
SO
ANALOGOUS
GEOMETRICAL
PROPERTIES
APPLY
THE
DECISION
REGIONS
OF
SUCH
A
DISCRIMINANT
ARE
ALWAYS
SINGLY
CONNECTED
AND
CONVEX
TO
SEE
THIS
CONSIDER
TWO
POINTS
XA
AND
XB
BOTH
OF
WHICH
LIE
INSIDE
DECISION
REGION
K
AS
ILLUSTRATED
IN
FIGURE
ANY
POINT
X
THAT
LIES
ON
THE
LINE
CONNECTING
XA
AND
XB
CAN
BE
EXPRESSED
IN
THE
FORM
X
ΛXA
Λ
XB
FIGURE
ILLUSTRATION
OF
THE
DECISION
REGIONS
FOR
A
MUL
TICLASS
LINEAR
DISCRIMINANT
WITH
THE
DECISION
BOUNDARIES
SHOWN
IN
RED
IF
TWO
POINTS
XA
AND
XB
BOTH
LIE
INSIDE
THE
SAME
DECISION
RE
J
CONNECTING
THESE
TWO
POBINTS
MUST
ALSO
LIE
IN
RI
SINGLY
CONNECTED
AND
CONVEX
K
XB
XA
Xˆ
WHERE
Λ
FROM
THE
LINEARITY
OF
THE
DISCRIMINANT
FUNCTIONS
IT
FOLLOWS
THAT
YK
X
ΛYK
XA
Λ
YK
XB
BECAUSE
BOTH
XA
AND
XB
LIE
INSIDE
K
IT
FOLLOWS
THAT
YK
XA
YJ
XA
AND
YK
XB
YJ
XB
FOR
ALL
J
K
AND
HENCE
YK
X
YJ
X
AND
SO
X
ALSO
LIES
INSIDE
K
THUS
K
IS
SINGLY
CONNECTED
AND
CONVEX
NOTE
THAT
FOR
TWO
CLASSES
WE
CAN
EITHER
EMPLOY
THE
FORMALISM
DISCUSSED
HERE
BASED
ON
TWO
DISCRIMINANT
FUNCTIONS
X
AND
X
OR
ELSE
USE
THE
SIMPLER
BUT
EQUIVALENT
FORMULATION
DESCRIBED
IN
SECTION
BASED
ON
A
SINGLE
DISCRIMINANT
FUNCTION
Y
X
WE
NOW
EXPLORE
THREE
APPROACHES
TO
LEARNING
THE
PARAMETERS
OF
LINEAR
DISCRIMI
NANT
FUNCTIONS
BASED
ON
LEAST
SQUARES
FISHER
LINEAR
DISCRIMINANT
AND
THE
PERCEP
TRON
ALGORITHM
LEAST
SQUARES
FOR
CLASSIFICATION
IN
CHAPTER
WE
CONSIDERED
MODELS
THAT
WERE
LINEAR
FUNCTIONS
OF
THE
PARAME
TERS
AND
WE
SAW
THAT
THE
MINIMIZATION
OF
A
SUM
OF
SQUARES
ERROR
FUNCTION
LED
TO
A
SIMPLE
CLOSED
FORM
SOLUTION
FOR
THE
PARAMETER
VALUES
IT
IS
THEREFORE
TEMPTING
TO
SEE
IF
WE
CAN
APPLY
THE
SAME
FORMALISM
TO
CLASSIFICATION
PROBLEMS
CONSIDER
A
GENERAL
CLASSIFICATION
PROBLEM
WITH
K
CLASSES
WITH
A
OF
K
BINARY
CODING
SCHEME
FOR
THE
TARGET
VECTOR
T
ONE
JUSTIFICATION
FOR
USING
LEAST
SQUARES
IN
SUCH
A
CONTEXT
IS
THAT
IT
APPROXIMATES
THE
CONDITIONAL
EXPECTATION
E
T
X
OF
THE
TARGET
VALUES
GIVEN
THE
INPUT
VECTOR
FOR
THE
BINARY
CODING
SCHEME
THIS
CONDITIONAL
EXPECTATION
IS
GIVEN
BY
THE
VECTOR
OF
POSTERIOR
CLASS
PROBABILITIES
UNFORTUNATELY
HOWEVER
THESE
PROBABILITIES
ARE
TYPICALLY
APPROXIMATED
RATHER
POORLY
INDEED
THE
APPROXIMATIONS
CAN
HAVE
VALUES
OUTSIDE
THE
RANGE
DUE
TO
THE
LIMITED
FLEXIBILITY
OF
A
LINEAR
MODEL
AS
WE
SHALL
SEE
SHORTLY
EACH
CLASS
CK
IS
DESCRIBED
BY
ITS
OWN
LINEAR
MODEL
SO
THAT
YK
X
WTX
WHERE
K
K
WE
CAN
CONVENIENTLY
GROUP
THESE
TOGETHER
USING
VECTOR
NOTA
TION
SO
THAT
Y
X
W
TX
K
K
NEW
INPUT
X
IS
THEN
ASSIGNED
TO
THE
CLASS
FOR
WHICH
THE
OUTPUT
YK
WTX
IS
LARGEST
WE
NOW
DETERMINE
THE
PARAMETER
MATRIX
W
BY
MINIMIZING
A
SUM
OF
SQUARES
ERROR
FUNCTION
AS
WE
DID
FOR
REGRESSION
IN
CHAPTER
CONSIDER
A
TRAINING
DATA
SET
XN
TN
WHERE
N
N
AND
DEFINE
A
MATRIX
T
WHOSE
NTH
ROW
IS
THE
VECTOR
TT
TOGETHER
WITH
A
MATRIX
X
WHOSE
NTH
ROW
IS
X
T
THE
SUM
OF
SQUARES
ERROR
FUNCTION
CAN
THEN
BE
WRITTEN
AS
ED
W
TR
J
X
W
T
T
X
W
T
L
SETTING
THE
DERIVATIVE
WITH
RESPECT
TO
W
TO
ZERO
AND
REARRANGING
WE
THEN
OBTAIN
THE
SOLUTION
FOR
W
IN
THE
FORM
W
XTX
X
T
WHERE
X
IS
THE
PSEUDO
INVERSE
OF
THE
MATRIX
X
AS
DISCUSSED
IN
SECTION
WE
THEN
OBTAIN
THE
DISCRIMINANT
FUNCTION
IN
THE
FORM
Y
X
W
TX
TT
X
T
X
EXERCISE
AN
INTERESTING
PROPERTY
OF
LEAST
SQUARES
SOLUTIONS
WITH
MULTIPLE
TARGET
VARIABLES
IS
THAT
IF
EVERY
TARGET
VECTOR
IN
THE
TRAINING
SET
SATISFIES
SOME
LINEAR
CONSTRAINT
ATTN
B
FOR
SOME
CONSTANTS
A
AND
B
THEN
THE
MODEL
PREDICTION
FOR
ANY
VALUE
OF
X
WILL
SATISFY
THE
SAME
CONSTRAINT
SO
THAT
SECTION
ATY
X
B
THUS
IF
WE
USE
A
OF
K
CODING
SCHEME
FOR
K
CLASSES
THEN
THE
PREDICTIONS
MADE
BY
THE
MODEL
WILL
HAVE
THE
PROPERTY
THAT
THE
ELEMENTS
OF
Y
X
WILL
SUM
TO
FOR
ANY
VALUE
OF
X
HOWEVER
THIS
SUMMATION
CONSTRAINT
ALONE
IS
NOT
SUFFICIENT
TO
ALLOW
THE
MODEL
OUTPUTS
TO
BE
INTERPRETED
AS
PROBABILITIES
BECAUSE
THEY
ARE
NOT
CONSTRAINED
TO
LIE
WITHIN
THE
INTERVAL
THE
LEAST
SQUARES
APPROACH
GIVES
AN
EXACT
CLOSED
FORM
SOLUTION
FOR
THE
DISCRIMI
NANT
FUNCTION
PARAMETERS
HOWEVER
EVEN
AS
A
DISCRIMINANT
FUNCTION
WHERE
WE
USE
IT
TO
MAKE
DECISIONS
DIRECTLY
AND
DISPENSE
WITH
ANY
PROBABILISTIC
INTERPRETATION
IT
SUF
FERS
FROM
SOME
SEVERE
PROBLEMS
WE
HAVE
ALREADY
SEEN
THAT
LEAST
SQUARES
SOLUTIONS
LACK
ROBUSTNESS
TO
OUTLIERS
AND
THIS
APPLIES
EQUALLY
TO
THE
CLASSIFICATION
APPLICATION
AS
ILLUSTRATED
IN
FIGURE
HERE
WE
SEE
THAT
THE
ADDITIONAL
DATA
POINTS
IN
THE
RIGHT
HAND
FIGURE
PRODUCE
A
SIGNIFICANT
CHANGE
IN
THE
LOCATION
OF
THE
DECISION
BOUNDARY
EVEN
THOUGH
THESE
POINT
WOULD
BE
CORRECTLY
CLASSIFIED
BY
THE
ORIGINAL
DECISION
BOUND
ARY
IN
THE
LEFT
HAND
FIGURE
THE
SUM
OF
SQUARES
ERROR
FUNCTION
PENALIZES
PREDICTIONS
THAT
ARE
TOO
CORRECT
IN
THAT
THEY
LIE
A
LONG
WAY
ON
THE
CORRECT
SIDE
OF
THE
DECISION
FIGURE
THE
LEFT
PLOT
SHOWS
DATA
FROM
TWO
CLASSES
DENOTED
BY
RED
CROSSES
AND
BLUE
CIRCLES
TOGETHER
WITH
THE
DECISION
BOUNDARY
FOUND
BY
LEAST
SQUARES
MAGENTA
CURVE
AND
ALSO
BY
THE
LOGISTIC
REGRESSION
MODEL
GREEN
CURVE
WHICH
IS
DISCUSSED
LATER
IN
SECTION
THE
RIGHT
HAND
PLOT
SHOWS
THE
CORRESPONDING
RESULTS
OBTAINED
WHEN
EXTRA
DATA
POINTS
ARE
ADDED
AT
THE
BOTTOM
LEFT
OF
THE
DIAGRAM
SHOWING
THAT
LEAST
SQUARES
IS
HIGHLY
SENSITIVE
TO
OUTLIERS
UNLIKE
LOGISTIC
REGRESSION
BOUNDARY
IN
SECTION
WE
SHALL
CONSIDER
SEVERAL
ALTERNATIVE
ERROR
FUNCTIONS
FOR
CLASSIFICATION
AND
WE
SHALL
SEE
THAT
THEY
DO
NOT
SUFFER
FROM
THIS
DIFFICULTY
HOWEVER
PROBLEMS
WITH
LEAST
SQUARES
CAN
BE
MORE
SEVERE
THAN
SIMPLY
LACK
OF
ROBUSTNESS
AS
ILLUSTRATED
IN
FIGURE
THIS
SHOWS
A
SYNTHETIC
DATA
SET
DRAWN
FROM
THREE
CLASSES
IN
A
TWO
DIMENSIONAL
INPUT
SPACE
HAVING
THE
PROPERTY
THAT
LIN
EAR
DECISION
BOUNDARIES
CAN
GIVE
EXCELLENT
SEPARATION
BETWEEN
THE
CLASSES
INDEED
THE
TECHNIQUE
OF
LOGISTIC
REGRESSION
DESCRIBED
LATER
IN
THIS
CHAPTER
GIVES
A
SATISFAC
TORY
SOLUTION
AS
SEEN
IN
THE
RIGHT
HAND
PLOT
HOWEVER
THE
LEAST
SQUARES
SOLUTION
GIVES
POOR
RESULTS
WITH
ONLY
A
SMALL
REGION
OF
THE
INPUT
SPACE
ASSIGNED
TO
THE
GREEN
CLASS
THE
FAILURE
OF
LEAST
SQUARES
SHOULD
NOT
SURPRISE
US
WHEN
WE
RECALL
THAT
IT
COR
RESPONDS
TO
MAXIMUM
LIKELIHOOD
UNDER
THE
ASSUMPTION
OF
A
GAUSSIAN
CONDITIONAL
DISTRIBUTION
WHEREAS
BINARY
TARGET
VECTORS
CLEARLY
HAVE
A
DISTRIBUTION
THAT
IS
FAR
FROM
GAUSSIAN
BY
ADOPTING
MORE
APPROPRIATE
PROBABILISTIC
MODELS
WE
SHALL
OBTAIN
CLAS
SIFICATION
TECHNIQUES
WITH
MUCH
BETTER
PROPERTIES
THAN
LEAST
SQUARES
FOR
THE
MOMENT
HOWEVER
WE
CONTINUE
TO
EXPLORE
ALTERNATIVE
NONPROBABILISTIC
METHODS
FOR
SETTING
THE
PARAMETERS
IN
THE
LINEAR
CLASSIFICATION
MODELS
FISHER
LINEAR
DISCRIMINANT
ONE
WAY
TO
VIEW
A
LINEAR
CLASSIFICATION
MODEL
IS
IN
TERMS
OF
DIMENSIONALITY
REDUCTION
CONSIDER
FIRST
THE
CASE
OF
TWO
CLASSES
AND
SUPPOSE
WE
TAKE
THE
D
FIGURE
EXAMPLE
OF
A
SYNTHETIC
DATA
SET
COMPRISING
THREE
CLASSES
WITH
TRAINING
DATA
POINTS
DENOTED
IN
RED
GREEN
AND
BLUE
LINES
DENOTE
THE
DECISION
BOUNDARIES
AND
THE
BACKGROUND
COLOURS
DENOTE
THE
RESPECTIVE
CLASSES
OF
THE
DECISION
REGIONS
ON
THE
LEFT
IS
THE
RESULT
OF
USING
A
LEAST
SQUARES
DISCRIMINANT
WE
SEE
THAT
THE
REGION
OF
INPUT
SPACE
ASSIGNED
TO
THE
GREEN
CLASS
IS
TOO
SMALL
AND
SO
MOST
OF
THE
POINTS
FROM
THIS
CLASS
ARE
MISCLASSIFIED
ON
THE
RIGHT
IS
THE
RESULT
OF
USING
LOGISTIC
REGRESSIONS
AS
DESCRIBED
IN
SECTION
SHOWING
CORRECT
CLASSIFICATION
OF
THE
TRAINING
DATA
DIMENSIONAL
INPUT
VECTOR
X
AND
PROJECT
IT
DOWN
TO
ONE
DIMENSION
USING
Y
WTX
IF
WE
PLACE
A
THRESHOLD
ON
Y
AND
CLASSIFY
Y
AS
CLASS
AND
OTHERWISE
CLASS
THEN
WE
OBTAIN
OUR
STANDARD
LINEAR
CLASSIFIER
DISCUSSED
IN
THE
PREVIOUS
SECTION
IN
GENERAL
THE
PROJECTION
ONTO
ONE
DIMENSION
LEADS
TO
A
CONSIDERABLE
LOSS
OF
INFOR
MATION
AND
CLASSES
THAT
ARE
WELL
SEPARATED
IN
THE
ORIGINAL
D
DIMENSIONAL
SPACE
MAY
BECOME
STRONGLY
OVERLAPPING
IN
ONE
DIMENSION
HOWEVER
BY
ADJUSTING
THE
COM
PONENTS
OF
THE
WEIGHT
VECTOR
W
WE
CAN
SELECT
A
PROJECTION
THAT
MAXIMIZES
THE
CLASS
SEPARATION
TO
BEGIN
WITH
CONSIDER
A
TWO
CLASS
PROBLEM
IN
WHICH
THERE
ARE
POINTS
OF
CLASS
AND
POINTS
OF
CLASS
SO
THAT
THE
MEAN
VECTORS
OF
THE
TWO
CLASSES
ARE
GIVEN
BY
M
N
XN
N
XN
N
N
THE
SIMPLEST
MEASURE
OF
THE
SEPARATION
OF
THE
CLASSES
WHEN
PROJECTED
ONTO
W
IS
THE
SEPARATION
OF
THE
PROJECTED
CLASS
MEANS
THIS
SUGGESTS
THAT
WE
MIGHT
CHOOSE
W
SO
AS
TO
MAXIMIZE
WHERE
WT
MK
WTMK
FIGURE
THE
LEFT
PLOT
SHOWS
SAMPLES
FROM
TWO
CLASSES
DEPICTED
IN
RED
AND
BLUE
ALONG
WITH
THE
HISTOGRAMS
RESULTING
FROM
PROJECTION
ONTO
THE
LINE
JOINING
THE
CLASS
MEANS
NOTE
THAT
THERE
IS
CONSIDERABLE
CLASS
OVERLAP
IN
THE
PROJECTED
SPACE
THE
RIGHT
PLOT
SHOWS
THE
CORRESPONDING
PROJECTION
BASED
ON
THE
FISHER
LINEAR
DISCRIMINANT
SHOWING
THE
GREATLY
IMPROVED
CLASS
SEPARATION
IS
THE
MEAN
OF
THE
PROJECTED
DATA
FROM
CLASS
K
HOWEVER
THIS
EXPRESSION
CAN
BE
MADE
ARBITRARILY
LARGE
SIMPLY
BY
INCREASING
THE
MAGNITUDE
OF
W
TO
SOLVE
THIS
PROBLEM
WE
COULD
CONSTRAIN
W
TO
HAVE
UNIT
LENGTH
SO
THAT
I
USING
APPENDIX
E
EXERCISE
A
LAGRANGE
MULTIPLIER
TO
PERFORM
THE
CONSTRAINED
MAXIMIZATION
WE
THEN
FIND
THAT
W
THERE
IS
STILL
A
PROBLEM
WITH
THIS
APPROACH
HOWEVER
AS
ILLUSTRATED
IN
FIGURE
THIS
SHOWS
TWO
CLASSES
THAT
ARE
WELL
SEPARATED
IN
THE
ORIGINAL
TWO
DIMENSIONAL
SPACE
BUT
THAT
HAVE
CONSIDERABLE
OVERLAP
WHEN
PROJECTED
ONTO
THE
LINE
JOINING
THEIR
MEANS
THIS
DIFFICULTY
ARISES
FROM
THE
STRONGLY
NONDIAGONAL
COVARIANCES
OF
THE
CLASS
DISTRIBUTIONS
THE
IDEA
PROPOSED
BY
FISHER
IS
TO
MAXIMIZE
A
FUNCTION
THAT
WILL
GIVE
A
LARGE
SEPARATION
BETWEEN
THE
PROJECTED
CLASS
MEANS
WHILE
ALSO
GIVING
A
SMALL
VARIANCE
WITHIN
EACH
CLASS
THEREBY
MINIMIZING
THE
CLASS
OVERLAP
THE
PROJECTION
FORMULA
TRANSFORMS
THE
SET
OF
LABELLED
DATA
POINTS
IN
X
INTO
A
LABELLED
SET
IN
THE
ONE
DIMENSIONAL
SPACE
Y
THE
WITHIN
CLASS
VARIANCE
OF
THE
TRANSFORMED
DATA
FROM
CLASS
CK
IS
THEREFORE
GIVEN
BY
YN
MK
N
CK
WHERE
YN
WTXN
WE
CAN
DEFINE
THE
TOTAL
WITHIN
CLASS
VARIANCE
FOR
THE
WHOLE
DATA
SET
TO
BE
SIMPLY
THE
FISHER
CRITERION
IS
DEFINED
TO
BE
THE
RATIO
OF
THE
BETWEEN
CLASS
VARIANCE
TO
THE
WITHIN
CLASS
VARIANCE
AND
IS
GIVEN
BY
J
W
EXERCISE
WE
CAN
MAKE
THE
DEPENDENCE
ON
W
EXPLICIT
BY
USING
AND
TO
REWRITE
THE
FISHER
CRITERION
IN
THE
FORM
J
W
WTSBW
WTSWW
WHERE
SB
IS
THE
BETWEEN
CLASS
COVARIANCE
MATRIX
AND
IS
GIVEN
BY
SB
T
AND
SW
IS
THE
TOTAL
WITHIN
CLASS
COVARIANCE
MATRIX
GIVEN
BY
SW
XN
XN
T
XN
XN
T
DIFFERENTIATING
WITH
RESPECT
TO
W
WE
FIND
THAT
J
W
IS
MAXIMIZED
WHEN
WTSBW
SWW
WTSWW
SBW
FROM
WE
SEE
THAT
SBW
IS
ALWAYS
IN
THE
DIRECTION
OF
FURTHERMORE
WE
DO
NOT
CARE
ABOUT
THE
MAGNITUDE
OF
W
ONLY
ITS
DIRECTION
AND
SO
WE
CAN
DROP
THE
SCALAR
FACTORS
WTSBW
AND
WTSWW
MULTIPLYING
BOTH
SIDES
OF
BY
WE
THEN
OBTAIN
W
NOTE
THAT
IF
THE
WITHIN
CLASS
COVARIANCE
IS
ISOTROPIC
SO
THAT
SW
IS
PROPORTIONAL
TO
THE
UNIT
MATRIX
WE
FIND
THAT
W
IS
PROPORTIONAL
TO
THE
DIFFERENCE
OF
THE
CLASS
MEANS
AS
DISCUSSED
ABOVE
THE
RESULT
IS
KNOWN
AS
FISHER
LINEAR
DISCRIMINANT
ALTHOUGH
STRICTLY
IT
IS
NOT
A
DISCRIMINANT
BUT
RATHER
A
SPECIFIC
CHOICE
OF
DIRECTION
FOR
PROJECTION
OF
THE
DATA
DOWN
TO
ONE
DIMENSION
HOWEVER
THE
PROJECTED
DATA
CAN
SUBSEQUENTLY
BE
USED
TO
CONSTRUCT
A
DISCRIMINANT
BY
CHOOSING
A
THRESHOLD
SO
THAT
WE
CLASSIFY
A
NEW
POINT
AS
BELONGING
TO
IF
Y
X
AND
CLASSIFY
IT
AS
BELONGING
TO
OTHERWISE
FOR
EXAMPLE
WE
CAN
MODEL
THE
CLASS
CONDITIONAL
DENSITIES
P
Y
K
USING
GAUSSIAN
DISTRIBUTIONS
AND
THEN
USE
THE
TECHNIQUES
OF
SECTION
TO
FIND
THE
PARAMETERS
OF
THE
GAUSSIAN
DISTRIBUTIONS
BY
MAXIMUM
LIKELIHOOD
HAVING
FOUND
GAUSSIAN
AP
PROXIMATIONS
TO
THE
PROJECTED
CLASSES
THE
FORMALISM
OF
SECTION
THEN
GIVES
AN
EXPRESSION
FOR
THE
OPTIMAL
THRESHOLD
SOME
JUSTIFICATION
FOR
THE
GAUSSIAN
ASSUMPTION
COMES
FROM
THE
CENTRAL
LIMIT
THEOREM
BY
NOTING
THAT
Y
WTX
IS
THE
SUM
OF
A
SET
OF
RANDOM
VARIABLES
RELATION
TO
LEAST
SQUARES
THE
LEAST
SQUARES
APPROACH
TO
THE
DETERMINATION
OF
A
LINEAR
DISCRIMINANT
WAS
BASED
ON
THE
GOAL
OF
MAKING
THE
MODEL
PREDICTIONS
AS
CLOSE
AS
POSSIBLE
TO
A
SET
OF
TARGET
VALUES
BY
CONTRAST
THE
FISHER
CRITERION
WAS
DERIVED
BY
REQUIRING
MAXIMUM
CLASS
SEPARATION
IN
THE
OUTPUT
SPACE
IT
IS
INTERESTING
TO
SEE
THE
RELATIONSHIP
BETWEEN
THESE
TWO
APPROACHES
IN
PARTICULAR
WE
SHALL
SHOW
THAT
FOR
THE
TWO
CLASS
PROBLEM
THE
FISHER
CRITERION
CAN
BE
OBTAINED
AS
A
SPECIAL
CASE
OF
LEAST
SQUARES
SO
FAR
WE
HAVE
CONSIDERED
OF
K
CODING
FOR
THE
TARGET
VALUES
IF
HOWEVER
WE
ADOPT
A
SLIGHTLY
DIFFERENT
TARGET
CODING
SCHEME
THEN
THE
LEAST
SQUARES
SOLUTION
FOR
THE
WEIGHTS
BECOMES
EQUIVALENT
TO
THE
FISHER
SOLUTION
DUDA
AND
HART
IN
PARTICULAR
WE
SHALL
TAKE
THE
TARGETS
FOR
CLASS
TO
BE
N
WHERE
IS
THE
NUMBER
OF
PATTERNS
IN
CLASS
AND
N
IS
THE
TOTAL
NUMBER
OF
PATTERNS
THIS
TARGET
VALUE
APPROXIMATES
THE
RECIPROCAL
OF
THE
PRIOR
PROBABILITY
FOR
CLASS
FOR
CLASS
WE
SHALL
TAKE
THE
TARGETS
TO
BE
N
WHERE
IS
THE
NUMBER
OF
PATTERNS
IN
CLASS
THE
SUM
OF
SQUARES
ERROR
FUNCTION
CAN
BE
WRITTEN
N
WTXN
N
TN
SETTING
THE
DERIVATIVES
OF
E
WITH
RESPECT
TO
AND
W
TO
ZERO
WE
OBTAIN
RESPECTIVELY
N
WTXN
TN
N
N
WTXN
TN
XN
N
FROM
AND
MAKING
USE
OF
OUR
CHOICE
OF
TARGET
CODING
SCHEME
FOR
THE
TN
WE
OBTAIN
AN
EXPRESSION
FOR
THE
BIAS
IN
THE
FORM
WTM
WHERE
WE
HAVE
USED
N
TN
N
N
N
AND
WHERE
M
IS
THE
MEAN
OF
THE
TOTAL
DATA
SET
AND
IS
GIVEN
BY
M
X
N
M
N
M
EXERCISE
AFTER
SOME
STRAIGHTFORWARD
ALGEBRA
AND
AGAIN
MAKING
USE
OF
THE
CHOICE
OF
TN
THE
SECOND
EQUATION
BECOMES
SW
N
B
W
N
WHERE
SW
IS
DEFINED
BY
SB
IS
DEFINED
BY
AND
WE
HAVE
SUBSTITUTED
FOR
THE
BIAS
USING
USING
WE
NOTE
THAT
SBW
IS
ALWAYS
IN
THE
DIRECTION
OF
THUS
WE
CAN
WRITE
W
WHERE
WE
HAVE
IGNORED
IRRELEVANT
SCALE
FACTORS
THUS
THE
WEIGHT
VECTOR
COINCIDES
WITH
THAT
FOUND
FROM
THE
FISHER
CRITERION
IN
ADDITION
WE
HAVE
ALSO
FOUND
AN
EXPRES
SION
FOR
THE
BIAS
VALUE
GIVEN
BY
THIS
TELLS
US
THAT
A
NEW
VECTOR
X
SHOULD
BE
CLASSIFIED
AS
BELONGING
TO
CLASS
IF
Y
X
WT
X
M
AND
CLASS
OTHERWISE
FISHER
DISCRIMINANT
FOR
MULTIPLE
CLASSES
WE
NOW
CONSIDER
THE
GENERALIZATION
OF
THE
FISHER
DISCRIMINANT
TO
K
CLASSES
AND
WE
SHALL
ASSUME
THAT
THE
DIMENSIONALITY
D
OF
THE
INPUT
SPACE
IS
GREATER
THAN
THE
NUMBER
K
OF
CLASSES
NEXT
WE
INTRODUCE
DI
LINEAR
FEATURES
YK
WTX
WHERE
K
DI
THESE
FEATURE
VALUES
CAN
CONVENIENTLY
BE
GROUPED
TOGETHER
TO
FORM
A
VECTOR
Y
SIMILARLY
THE
WEIGHT
VECTORS
WK
CAN
BE
CONSIDERED
TO
BE
THE
COLUMNS
OF
A
MATRIX
W
SO
THAT
Y
WTX
NOTE
THAT
AGAIN
WE
ARE
NOT
INCLUDING
ANY
BIAS
PARAMETERS
IN
THE
DEFINITION
OF
Y
THE
GENERALIZATION
OF
THE
WITHIN
CLASS
COVARIANCE
MATRIX
TO
THE
CASE
OF
K
CLASSES
FOLLOWS
FROM
TO
GIVE
WHERE
K
SW
SK
K
SK
XN
MK
XN
MK
T
N
CK
M
K
NK
XN
N
CK
AND
NK
IS
THE
NUMBER
OF
PATTERNS
IN
CLASS
K
IN
ORDER
TO
FIND
A
GENERALIZATION
OF
THE
BETWEEN
CLASS
COVARIANCE
MATRIX
WE
FOLLOW
DUDA
AND
HART
AND
CONSIDER
FIRST
THE
TOTAL
COVARIANCE
MATRIX
N
ST
XN
M
XN
M
T
N
WHERE
M
IS
THE
MEAN
OF
THE
TOTAL
DATA
SET
M
N
N
XN
N
N
K
NKMK
K
AND
N
K
NK
IS
THE
TOTAL
NUMBER
OF
DATA
POINTS
THE
TOTAL
COVARIANCE
MATRIX
CAN
BE
DECOMPOSED
INTO
THE
SUM
OF
THE
WITHIN
CLASS
COVARIANCE
MATRIX
GIVEN
BY
AND
PLUS
AN
ADDITIONAL
MATRIX
SB
WHICH
WE
IDENTIFY
AS
A
MEASURE
OF
THE
BETWEEN
CLASS
COVARIANCE
WHERE
ST
SW
SB
K
SB
NK
MK
M
MK
M
T
K
THESE
COVARIANCE
MATRICES
HAVE
BEEN
DEFINED
IN
THE
ORIGINAL
X
SPACE
WE
CAN
NOW
DEFINE
SIMILAR
MATRICES
IN
THE
PROJECTED
DI
DIMENSIONAL
Y
SPACE
AND
WHERE
K
SW
YN
ΜK
YN
ΜK
T
K
N
CK
K
SB
NK
ΜK
Μ
ΜK
Μ
T
K
Μ
K
NK
YN
N
CK
Μ
N
K
NK
K
ΜK
AGAIN
WE
WISH
TO
CONSTRUCT
A
SCALAR
THAT
IS
LARGE
WHEN
THE
BETWEEN
CLASS
COVARIANCE
IS
LARGE
AND
WHEN
THE
WITHIN
CLASS
COVARIANCE
IS
SMALL
THERE
ARE
NOW
MANY
POSSIBLE
CHOICES
OF
CRITERION
FUKUNAGA
ONE
EXAMPLE
IS
GIVEN
BY
W
THIS
CRITERION
CAN
THEN
BE
REWRITTEN
AS
AN
EXPLICIT
FUNCTION
OF
THE
PROJECTION
MATRIX
W
IN
THE
FORM
J
W
TR
WSWWT
WSBWT
MAXIMIZATION
OF
SUCH
CRITERIA
IS
STRAIGHTFORWARD
THOUGH
SOMEWHAT
INVOLVED
AND
IS
DISCUSSED
AT
LENGTH
IN
FUKUNAGA
THE
WEIGHT
VALUES
ARE
DETERMINED
BY
THOSE
EIGENVECTORS
OF
THAT
CORRESPOND
TO
THE
DI
LARGEST
EIGENVALUES
THERE
IS
ONE
IMPORTANT
RESULT
THAT
IS
COMMON
TO
ALL
SUCH
CRITERIA
WHICH
IS
WORTH
EMPHASIZING
WE
FIRST
NOTE
FROM
THAT
SB
IS
COMPOSED
OF
THE
SUM
OF
K
MA
TRICES
EACH
OF
WHICH
IS
AN
OUTER
PRODUCT
OF
TWO
VECTORS
AND
THEREFORE
OF
RANK
IN
ADDITION
ONLY
K
OF
THESE
MATRICES
ARE
INDEPENDENT
AS
A
RESULT
OF
THE
CONSTRAINT
THUS
SB
HAS
RANK
AT
MOST
EQUAL
TO
K
AND
SO
THERE
ARE
AT
MOST
K
NONZERO
EIGENVALUES
THIS
SHOWS
THAT
THE
PROJECTION
ONTO
THE
K
DIMENSIONAL
SUBSPACE
SPANNED
BY
THE
EIGENVECTORS
OF
SB
DOES
NOT
ALTER
THE
VALUE
OF
J
W
AND
SO
WE
ARE
THEREFORE
UNABLE
TO
FIND
MORE
THAN
K
LINEAR
FEATURES
BY
THIS
MEANS
FUKUNAGA
THE
PERCEPTRON
ALGORITHM
ANOTHER
EXAMPLE
OF
A
LINEAR
DISCRIMINANT
MODEL
IS
THE
PERCEPTRON
OF
ROSENBLATT
WHICH
OCCUPIES
AN
IMPORTANT
PLACE
IN
THE
HISTORY
OF
PATTERN
RECOGNITION
AL
GORITHMS
IT
CORRESPONDS
TO
A
TWO
CLASS
MODEL
IN
WHICH
THE
INPUT
VECTOR
X
IS
FIRST
TRANSFORMED
USING
A
FIXED
NONLINEAR
TRANSFORMATION
TO
GIVE
A
FEATURE
VECTOR
Φ
X
AND
THIS
IS
THEN
USED
TO
CONSTRUCT
A
GENERALIZED
LINEAR
MODEL
OF
THE
FORM
Y
X
F
WTΦ
X
WHERE
THE
NONLINEAR
ACTIVATION
FUNCTION
F
IS
GIVEN
BY
A
STEP
FUNCTION
OF
THE
FORM
A
A
THE
VECTOR
Φ
X
WILL
TYPICALLY
INCLUDE
A
BIAS
COMPONENT
X
IN
EARLIER
DISCUSSIONS
OF
TWO
CLASS
CLASSIFICATION
PROBLEMS
WE
HAVE
FOCUSSED
ON
A
TARGET
CODING
SCHEME
IN
WHICH
T
WHICH
IS
APPROPRIATE
IN
THE
CONTEXT
OF
PROBABILISTIC
MODELS
FOR
THE
PERCEPTRON
HOWEVER
IT
IS
MORE
CONVENIENT
TO
USE
TARGET
VALUES
T
FOR
CLASS
AND
T
FOR
CLASS
WHICH
MATCHES
THE
CHOICE
OF
ACTIVATION
FUNCTION
THE
ALGORITHM
USED
TO
DETERMINE
THE
PARAMETERS
W
OF
THE
PERCEPTRON
CAN
MOST
EASILY
BE
MOTIVATED
BY
ERROR
FUNCTION
MINIMIZATION
A
NATURAL
CHOICE
OF
ERROR
FUNC
TION
WOULD
BE
THE
TOTAL
NUMBER
OF
MISCLASSIFIED
PATTERNS
HOWEVER
THIS
DOES
NOT
LEAD
TO
A
SIMPLE
LEARNING
ALGORITHM
BECAUSE
THE
ERROR
IS
A
PIECEWISE
CONSTANT
FUNCTION
OF
W
WITH
DISCONTINUITIES
WHEREVER
A
CHANGE
IN
W
CAUSES
THE
DECISION
BOUNDARY
TO
MOVE
ACROSS
ONE
OF
THE
DATA
POINTS
METHODS
BASED
ON
CHANGING
W
USING
THE
GRADI
ENT
OF
THE
ERROR
FUNCTION
CANNOT
THEN
BE
APPLIED
BECAUSE
THE
GRADIENT
IS
ZERO
ALMOST
EVERYWHERE
WE
THEREFORE
CONSIDER
AN
ALTERNATIVE
ERROR
FUNCTION
KNOWN
AS
THE
PERCEPTRON
CRI
TERION
TO
DERIVE
THIS
WE
NOTE
THAT
WE
ARE
SEEKING
A
WEIGHT
VECTOR
W
SUCH
THAT
PATTERNS
XN
IN
CLASS
WILL
HAVE
WTΦ
XN
WHEREAS
PATTERNS
XN
IN
CLASS
HAVE
WTΦ
XN
USING
THE
T
TARGET
CODING
SCHEME
IT
FOLLOWS
THAT
WE
WOULD
LIKE
ALL
PATTERNS
TO
SATISFY
WTΦ
XN
TN
THE
PERCEPTRON
CRITERION
ASSOCIATES
ZERO
ERROR
WITH
ANY
PATTERN
THAT
IS
CORRECTLY
CLASSIFIED
WHEREAS
FOR
A
MIS
CLASSIFIED
PATTERN
XN
IT
TRIES
TO
MINIMIZE
THE
QUANTITY
WTΦ
XN
TN
THE
PERCEPTRON
CRITERION
IS
THEREFORE
GIVEN
BY
EP
W
WTΦNTN
N
M
FRANK
ROSENBLATT
ROSENBLATT
PERCEPTRON
PLAYED
AN
IMPORTANT
ROLE
IN
THE
HISTORY
OF
MA
CHINE
LEARNING
INITIALLY
ROSENBLATT
SIMULATED
THE
PERCEPTRON
ON
AN
IBM
COMPUTER
AT
CORNELL
IN
BUT
BY
THE
EARLY
HE
HAD
BUILT
SPECIAL
PURPOSE
HARDWARE
THAT
PROVIDED
A
DIRECT
PAR
ALLEL
IMPLEMENTATION
OF
PERCEPTRON
LEARNING
MANY
OF
HIS
IDEAS
WERE
ENCAPSULATED
IN
PRINCIPLES
OF
NEURO
DYNAMICS
PERCEPTRONS
AND
THE
THEORY
OF
BRAIN
MECH
ANISMS
PUBLISHED
IN
ROSENBLATT
WORK
WAS
CRITICIZED
BY
MARVIN
MINKSY
WHOSE
OBJECTIONS
WERE
PUBLISHED
IN
THE
BOOK
PERCEPTRONS
CO
AUTHORED
WITH
SEYMOUR
PAPERT
THIS
BOOK
WAS
WIDELY
MISINTER
PRETED
AT
THE
TIME
AS
SHOWING
THAT
NEURAL
NETWORKS
WERE
FATALLY
FLAWED
AND
COULD
ONLY
LEARN
SOLUTIONS
FOR
LINEARLY
SEPARABLE
PROBLEMS
IN
FACT
IT
ONLY
PROVED
SUCH
LIMITATIONS
IN
THE
CASE
OF
SINGLE
LAYER
NETWORKS
SUCH
AS
THE
PERCEPTRON
AND
MERELY
CONJECTURED
IN
CORRECTLY
THAT
THEY
APPLIED
TO
MORE
GENERAL
NETWORK
MODELS
UNFORTUNATELY
HOWEVER
THIS
BOOK
CONTRIBUTED
TO
THE
SUBSTANTIAL
DECLINE
IN
RESEARCH
FUNDING
FOR
NEU
RAL
COMPUTING
A
SITUATION
THAT
WAS
NOT
REVERSED
UN
TIL
THE
MID
TODAY
THERE
ARE
MANY
HUNDREDS
IF
NOT
THOUSANDS
OF
APPLICATIONS
OF
NEURAL
NETWORKS
IN
WIDESPREAD
USE
WITH
EXAMPLES
IN
AREAS
SUCH
AS
HANDWRITING
RECOGNITION
AND
INFORMATION
RETRIEVAL
BE
ING
USED
ROUTINELY
BY
MILLIONS
OF
PEOPLE
SECTION
WHERE
DENOTES
THE
SET
OF
ALL
MISCLASSIFIED
PATTERNS
THE
CONTRIBUTION
TO
THE
ERROR
ASSOCIATED
WITH
A
PARTICULAR
MISCLASSIFIED
PATTERN
IS
A
LINEAR
FUNCTION
OF
W
IN
REGIONS
OF
W
SPACE
WHERE
THE
PATTERN
IS
MISCLASSIFIED
AND
ZERO
IN
REGIONS
WHERE
IT
IS
CORRECTLY
CLASSIFIED
THE
TOTAL
ERROR
FUNCTION
IS
THEREFORE
PIECEWISE
LINEAR
WE
NOW
APPLY
THE
STOCHASTIC
GRADIENT
DESCENT
ALGORITHM
TO
THIS
ERROR
FUNCTION
THE
CHANGE
IN
THE
WEIGHT
VECTOR
W
IS
THEN
GIVEN
BY
W
Τ
W
Τ
Η
EP
W
W
Τ
ΗΦNTN
WHERE
Η
IS
THE
LEARNING
RATE
PARAMETER
AND
Τ
IS
AN
INTEGER
THAT
INDEXES
THE
STEPS
OF
THE
ALGORITHM
BECAUSE
THE
PERCEPTRON
FUNCTION
Y
X
W
IS
UNCHANGED
IF
WE
MULTIPLY
W
BY
A
CONSTANT
WE
CAN
SET
THE
LEARNING
RATE
PARAMETER
Η
EQUAL
TO
WITHOUT
OF
GENERALITY
NOTE
THAT
AS
THE
WEIGHT
VECTOR
EVOLVES
DURING
TRAINING
THE
SET
OF
PATTERNS
THAT
ARE
MISCLASSIFIED
WILL
CHANGE
THE
PERCEPTRON
LEARNING
ALGORITHM
HAS
A
SIMPLE
INTERPRETATION
AS
FOLLOWS
WE
CYCLE
THROUGH
THE
TRAINING
PATTERNS
IN
TURN
AND
FOR
EACH
PATTERN
XN
WE
EVALUATE
THE
PERCEPTRON
FUNCTION
IF
THE
PATTERN
IS
CORRECTLY
CLASSIFIED
THEN
THE
WEIGHT
VECTOR
REMAINS
UNCHANGED
WHEREAS
IF
IT
IS
INCORRECTLY
CLASSIFIED
THEN
FOR
CLASS
WE
ADD
THE
VECTOR
Φ
XN
ONTO
THE
CURRENT
ESTIMATE
OF
WEIGHT
VECTOR
W
WHILE
FOR
CLASS
WE
SUBTRACT
THE
VECTOR
Φ
XN
FROM
W
THE
PERCEPTRON
LEARNING
ALGORITHM
IS
ILLUSTRATED
IN
FIGURE
IF
WE
CONSIDER
THE
EFFECT
OF
A
SINGLE
UPDATE
IN
THE
PERCEPTRON
LEARNING
ALGORITHM
WE
SEE
THAT
THE
CONTRIBUTION
TO
THE
ERROR
FROM
A
MISCLASSIFIED
PATTERN
WILL
BE
REDUCED
BECAUSE
FROM
WE
HAVE
W
Τ
TΦNTN
W
Τ
TΦNTN
ΦNTN
TΦNTN
W
Τ
TΦNTN
WHERE
WE
HAVE
SET
Η
AND
MADE
USE
OF
ΦNTN
OF
COURSE
THIS
DOES
NOT
IMPLY
THAT
THE
CONTRIBUTION
TO
THE
ERROR
FUNCTION
FROM
THE
OTHER
MISCLASSIFIED
PATTERNS
WILL
HAVE
BEEN
REDUCED
FURTHERMORE
THE
CHANGE
IN
WEIGHT
VECTOR
MAY
HAVE
CAUSED
SOME
PREVIOUSLY
CORRECTLY
CLASSIFIED
PATTERNS
TO
BECOME
MISCLASSIFIED
THUS
THE
PERCEPTRON
LEARNING
RULE
IS
NOT
GUARANTEED
TO
REDUCE
THE
TOTAL
ERROR
FUNCTION
AT
EACH
STAGE
HOWEVER
THE
PERCEPTRON
CONVERGENCE
THEOREM
STATES
THAT
IF
THERE
EXISTS
AN
EX
ACT
SOLUTION
IN
OTHER
WORDS
IF
THE
TRAINING
DATA
SET
IS
LINEARLY
SEPARABLE
THEN
THE
PERCEPTRON
LEARNING
ALGORITHM
IS
GUARANTEED
TO
FIND
AN
EXACT
SOLUTION
IN
A
FINITE
NUM
BER
OF
STEPS
PROOFS
OF
THIS
THEOREM
CAN
BE
FOUND
FOR
EXAMPLE
IN
ROSENBLATT
BLOCK
NILSSON
MINSKY
AND
PAPERT
HERTZ
ET
AL
AND
BISHOP
NOTE
HOWEVER
THAT
THE
NUMBER
OF
STEPS
REQUIRED
TO
ACHIEVE
CON
VERGENCE
COULD
STILL
BE
SUBSTANTIAL
AND
IN
PRACTICE
UNTIL
CONVERGENCE
IS
ACHIEVED
WE
WILL
NOT
BE
ABLE
TO
DISTINGUISH
BETWEEN
A
NONSEPARABLE
PROBLEM
AND
ONE
THAT
IS
SIMPLY
SLOW
TO
CONVERGE
EVEN
WHEN
THE
DATA
SET
IS
LINEARLY
SEPARABLE
THERE
MAY
BE
MANY
SOLUTIONS
AND
WHICH
ONE
IS
FOUND
WILL
DEPEND
ON
THE
INITIALIZATION
OF
THE
PARAMETERS
AND
ON
THE
OR
DER
OF
PRESENTATION
OF
THE
DATA
POINTS
FURTHERMORE
FOR
DATA
SETS
THAT
ARE
NOT
LINEARLY
SEPARABLE
THE
PERCEPTRON
LEARNING
ALGORITHM
WILL
NEVER
CONVERGE
FIGURE
ILLUSTRATION
OF
THE
CONVERGENCE
OF
THE
PERCEPTRON
LEARNING
ALGORITHM
SHOWING
DATA
POINTS
FROM
TWO
CLASSES
RED
AND
BLUE
IN
A
TWO
DIMENSIONAL
FEATURE
SPACE
THE
TOP
LEFT
PLOT
SHOWS
THE
INITIAL
PARAMETER
VECTOR
W
SHOWN
AS
A
BLACK
ARROW
TOGETHER
WITH
THE
CORRESPONDING
DECISION
BOUNDARY
BLACK
LINE
IN
WHICH
THE
ARROW
POINTS
TOWARDS
THE
DECISION
REGION
WHICH
CLASSIFIED
AS
BELONGING
TO
THE
RED
CLASS
THE
DATA
POINT
CIRCLED
IN
GREEN
IS
MISCLASSIFIED
AND
SO
ITS
FEATURE
VECTOR
IS
ADDED
TO
THE
CURRENT
WEIGHT
VECTOR
GIVING
THE
NEW
DECISION
BOUNDARY
SHOWN
IN
THE
TOP
RIGHT
PLOT
THE
BOTTOM
LEFT
PLOT
SHOWS
THE
NEXT
MISCLASSIFIED
POINT
TO
BE
CONSIDERED
INDICATED
BY
THE
GREEN
CIRCLE
AND
ITS
FEATURE
VECTOR
IS
AGAIN
ADDED
TO
THE
WEIGHT
VECTOR
GIVING
THE
DECISION
BOUNDARY
SHOWN
IN
THE
BOTTOM
RIGHT
PLOT
FOR
WHICH
ALL
DATA
POINTS
ARE
CORRECTLY
CLASSIFIED
FIGURE
ILLUSTRATION
OF
THE
MARK
PERCEPTRON
HARDWARE
THE
PHOTOGRAPH
ON
THE
LEFT
SHOWS
HOW
THE
INPUTS
WERE
OBTAINED
USING
A
SIMPLE
CAMERA
SYSTEM
IN
WHICH
AN
INPUT
SCENE
IN
THIS
CASE
A
PRINTED
CHARACTER
WAS
ILLUMINATED
BY
POWERFUL
LIGHTS
AND
AN
IMAGE
FOCUSSED
ONTO
A
ARRAY
OF
CADMIUM
SULPHIDE
PHOTOCELLS
GIVING
A
PRIMITIVE
PIXEL
IMAGE
THE
PERCEPTRON
ALSO
HAD
A
PATCH
BOARD
SHOWN
IN
THE
MIDDLE
PHOTOGRAPH
WHICH
ALLOWED
DIFFERENT
CONFIGURATIONS
OF
INPUT
FEATURES
TO
BE
TRIED
OFTEN
THESE
WERE
WIRED
UP
AT
RANDOM
TO
DEMONSTRATE
THE
ABILITY
OF
THE
PERCEPTRON
TO
LEARN
WITHOUT
THE
NEED
FOR
PRECISE
WIRING
IN
CONTRAST
TO
A
MODERN
DIGITAL
COMPUTER
THE
PHOTOGRAPH
ON
THE
RIGHT
SHOWS
ONE
OF
THE
RACKS
OF
ADAPTIVE
WEIGHTS
EACH
WEIGHT
WAS
IMPLEMENTED
USING
A
ROTARY
VARIABLE
RESISTOR
ALSO
CALLED
A
POTENTIOMETER
DRIVEN
BY
AN
ELECTRIC
MOTOR
THEREBY
ALLOWING
THE
VALUE
OF
THE
WEIGHT
TO
BE
ADJUSTED
AUTOMATICALLY
BY
THE
LEARNING
ALGORITHM
ASIDE
FROM
DIFFICULTIES
WITH
THE
LEARNING
ALGORITHM
THE
PERCEPTRON
DOES
NOT
PRO
VIDE
PROBABILISTIC
OUTPUTS
NOR
DOES
IT
GENERALIZE
READILY
TO
K
CLASSES
THE
MOST
IMPORTANT
LIMITATION
HOWEVER
ARISES
FROM
THE
FACT
THAT
IN
COMMON
WITH
ALL
OF
THE
MODELS
DISCUSSED
IN
THIS
CHAPTER
AND
THE
PREVIOUS
ONE
IT
IS
BASED
ON
LINEAR
COM
BINATIONS
OF
FIXED
BASIS
FUNCTIONS
MORE
DETAILED
DISCUSSIONS
OF
THE
LIMITATIONS
OF
PERCEPTRONS
CAN
BE
FOUND
IN
MINSKY
AND
PAPERT
AND
BISHOP
ANALOGUE
HARDWARE
IMPLEMENTATIONS
OF
THE
PERCEPTRON
WERE
BUILT
BY
ROSENBLATT
BASED
ON
MOTOR
DRIVEN
VARIABLE
RESISTORS
TO
IMPLEMENT
THE
ADAPTIVE
PARAMETERS
WJ
THESE
ARE
ILLUSTRATED
IN
FIGURE
THE
INPUTS
WERE
OBTAINED
FROM
A
SIMPLE
CAMERA
SYSTEM
BASED
ON
AN
ARRAY
OF
PHOTO
SENSORS
WHILE
THE
BASIS
FUNCTIONS
Φ
COULD
BE
CHOSEN
IN
A
VARIETY
OF
WAYS
FOR
EXAMPLE
BASED
ON
SIMPLE
FIXED
FUNCTIONS
OF
RANDOMLY
CHOSEN
SUBSETS
OF
PIXELS
FROM
THE
INPUT
IMAGE
TYPICAL
APPLICATIONS
INVOLVED
LEARNING
TO
DISCRIMINATE
SIMPLE
SHAPES
OR
CHARACTERS
AT
THE
SAME
TIME
THAT
THE
PERCEPTRON
WAS
BEING
DEVELOPED
A
CLOSELY
RELATED
SYSTEM
CALLED
THE
ADALINE
WHICH
IS
SHORT
FOR
ADAPTIVE
LINEAR
ELEMENT
WAS
BEING
EXPLORED
BY
WIDROW
AND
CO
WORKERS
THE
FUNCTIONAL
FORM
OF
THE
MODEL
WAS
THE
SAME
AS
FOR
THE
PERCEPTRON
BUT
A
DIFFERENT
APPROACH
TO
TRAINING
WAS
ADOPTED
WIDROW
AND
HOFF
WIDROW
AND
LEHR
PROBABILISTIC
GENERATIVE
MODELS
WE
TURN
NEXT
TO
A
PROBABILISTIC
VIEW
OF
CLASSIFICATION
AND
SHOW
HOW
MODELS
WITH
LINEAR
DECISION
BOUNDARIES
ARISE
FROM
SIMPLE
ASSUMPTIONS
ABOUT
THE
DISTRIBUTION
OF
THE
DATA
IN
SECTION
WE
DISCUSSED
THE
DISTINCTION
BETWEEN
THE
DISCRIMINATIVE
AND
THE
GENERATIVE
APPROACHES
TO
CLASSIFICATION
HERE
WE
SHALL
ADOPT
A
GENERATIVE
FIGURE
PLOT
OF
THE
LOGISTIC
SIGMOID
FUNCTION
Σ
A
DEFINED
BY
SHOWN
IN
RED
TOGETHER
WITH
THE
SCALED
PRO
BIT
FUNCTION
Φ
ΛA
FOR
Π
SHOWN
IN
DASHED
BLUE
WHERE
Φ
A
IS
DEFINED
BY
THE
SCAL
ING
FACTOR
Π
IS
CHOSEN
SO
THAT
THE
DERIVATIVES
OF
THE
TWO
CURVES
ARE
EQUAL
FOR
A
APPROACH
IN
WHICH
WE
MODEL
THE
CLASS
CONDITIONAL
DENSITIES
P
X
K
AS
WELL
AS
THE
CLASS
PRIORS
P
K
AND
THEN
USE
THESE
TO
COMPUTE
POSTERIOR
PROBABILITIES
P
K
X
THROUGH
BAYES
THEOREM
CONSIDER
FIRST
OF
ALL
THE
CASE
OF
TWO
CLASSES
THE
POSTERIOR
PROBABILITY
FOR
CLASS
CAN
BE
WRITTEN
AS
P
C
X
P
X
P
P
X
C
P
C
P
X
C
P
C
WHERE
WE
HAVE
DEFINED
EXP
A
Σ
A
A
LN
P
X
P
P
X
P
AND
Σ
A
IS
THE
LOGISTIC
SIGMOID
FUNCTION
DEFINED
BY
Σ
A
EXP
A
WHICH
IS
PLOTTED
IN
FIGURE
THE
TERM
SIGMOID
MEANS
SHAPED
THIS
TYPE
OF
FUNCTION
IS
SOMETIMES
ALSO
CALLED
A
SQUASHING
FUNCTION
BECAUSE
IT
MAPS
THE
WHOLE
REAL
AXIS
INTO
A
FINITE
INTERVAL
THE
LOGISTIC
SIGMOID
HAS
BEEN
ENCOUNTERED
ALREADY
IN
EARLIER
CHAPTERS
AND
PLAYS
AN
IMPORTANT
ROLE
IN
MANY
CLASSIFICATION
ALGORITHMS
IT
SATISFIES
THE
FOLLOWING
SYMMETRY
PROPERTY
Σ
A
Σ
A
AS
IS
EASILY
VERIFIED
THE
INVERSE
OF
THE
LOGISTIC
SIGMOID
IS
GIVEN
BY
LN
Σ
Σ
AND
IS
KNOWN
AS
THE
LOGIT
FUNCTION
IT
REPRESENTS
THE
LOG
OF
THE
RATIO
OF
PROBABILITIES
LN
P
X
P
X
FOR
THE
TWO
CLASSES
ALSO
KNOWN
AS
THE
LOG
ODDS
NOTE
THAT
IN
WE
HAVE
SIMPLY
REWRITTEN
THE
POSTERIOR
PROBABILITIES
IN
AN
EQUIVALENT
FORM
AND
SO
THE
APPEARANCE
OF
THE
LOGISTIC
SIGMOID
MAY
SEEM
RATHER
VAC
UOUS
HOWEVER
IT
WILL
HAVE
SIGNIFICANCE
PROVIDED
A
X
TAKES
A
SIMPLE
FUNCTIONAL
FORM
WE
SHALL
SHORTLY
CONSIDER
SITUATIONS
IN
WHICH
A
X
IS
A
LINEAR
FUNCTION
OF
X
IN
WHICH
CASE
THE
POSTERIOR
PROBABILITY
IS
GOVERNED
BY
A
GENERALIZED
LINEAR
MODEL
FOR
THE
CASE
OF
K
CLASSES
WE
HAVE
C
X
P
X
CK
P
CK
P
X
C
P
C
EXP
AK
J
EXP
AJ
WHICH
IS
KNOWN
AS
THE
NORMALIZED
EXPONENTIAL
AND
CAN
BE
REGARDED
AS
A
MULTICLASS
GENERALIZATION
OF
THE
LOGISTIC
SIGMOID
HERE
THE
QUANTITIES
AK
ARE
DEFINED
BY
AK
LN
P
X
CK
P
CK
THE
NORMALIZED
EXPONENTIAL
IS
ALSO
KNOWN
AS
THE
SOFTMAX
FUNCTION
AS
IT
REPRESENTS
A
SMOOTHED
VERSION
OF
THE
MAX
FUNCTION
BECAUSE
IF
AK
AJ
FOR
ALL
J
K
THEN
P
K
X
AND
P
J
X
WE
NOW
INVESTIGATE
THE
CONSEQUENCES
OF
CHOOSING
SPECIFIC
FORMS
FOR
THE
CLASS
CONDITIONAL
DENSITIES
LOOKING
FIRST
AT
CONTINUOUS
INPUT
VARIABLES
X
AND
THEN
DIS
CUSSING
BRIEFLY
THE
CASE
OF
DISCRETE
INPUTS
CONTINUOUS
INPUTS
LET
US
ASSUME
THAT
THE
CLASS
CONDITIONAL
DENSITIES
ARE
GAUSSIAN
AND
THEN
EXPLORE
THE
RESULTING
FORM
FOR
THE
POSTERIOR
PROBABILITIES
TO
START
WITH
WE
SHALL
ASSUME
THAT
ALL
CLASSES
SHARE
THE
SAME
COVARIANCE
MATRIX
THUS
THE
DENSITY
FOR
CLASS
K
IS
GIVEN
BY
P
X
C
EXP
X
Μ
TΣ
X
Μ
D
Σ
K
K
CONSIDER
FIRST
THE
CASE
OF
TWO
CLASSES
FROM
AND
WE
HAVE
P
X
Σ
WTX
WHERE
WE
HAVE
DEFINED
W
Σ
Μ
Μ
W
ΜTΣ
ΜTΣ
LN
P
P
WE
SEE
THAT
THE
QUADRATIC
TERMS
IN
X
FROM
THE
EXPONENTS
OF
THE
GAUSSIAN
DENSITIES
HAVE
CANCELLED
DUE
TO
THE
ASSUMPTION
OF
COMMON
COVARIANCE
MATRICES
LEADING
TO
A
LINEAR
FUNCTION
OF
X
IN
THE
ARGUMENT
OF
THE
LOGISTIC
SIGMOID
THIS
RESULT
IS
ILLUS
TRATED
FOR
THE
CASE
OF
A
TWO
DIMENSIONAL
INPUT
SPACE
X
IN
FIGURE
THE
RESULTING
FIGURE
THE
LEFT
HAND
PLOT
SHOWS
THE
CLASS
CONDITIONAL
DENSITIES
FOR
TWO
CLASSES
DENOTED
RED
AND
BLUE
ON
THE
RIGHT
IS
THE
CORRESPONDING
POSTERIOR
PROBABILITY
P
X
WHICH
IS
GIVEN
BY
A
LOGISTIC
SIGMOID
OF
A
LINEAR
FUNCTION
OF
X
THE
SURFACE
IN
THE
RIGHT
HAND
PLOT
IS
COLOURED
USING
A
PROPORTION
OF
RED
INK
GIVEN
BY
P
X
AND
A
PROPORTION
OF
BLUE
INK
GIVEN
BY
P
X
P
X
DECISION
BOUNDARIES
CORRESPOND
TO
SURFACES
ALONG
WHICH
THE
POSTERIOR
PROBABILITIES
P
K
X
ARE
CONSTANT
AND
SO
WILL
BE
GIVEN
BY
LINEAR
FUNCTIONS
OF
X
AND
THEREFORE
THE
DECISION
BOUNDARIES
ARE
LINEAR
IN
INPUT
SPACE
THE
PRIOR
PROBABILITIES
P
K
ENTER
ONLY
THROUGH
THE
BIAS
PARAMETER
SO
THAT
CHANGES
IN
THE
PRIORS
HAVE
THE
EFFECT
OF
MAKING
PARALLEL
SHIFTS
OF
THE
DECISION
BOUNDARY
AND
MORE
GENERALLY
OF
THE
PARALLEL
CONTOURS
OF
CONSTANT
POSTERIOR
PROBABILITY
FOR
THE
GENERAL
CASE
OF
K
CLASSES
WE
HAVE
FROM
AND
AK
X
WTX
WHERE
WE
HAVE
DEFINED
WK
Σ
W
ΜTΣ
LN
P
C
WE
SEE
THAT
THE
AK
X
ARE
AGAIN
LINEAR
FUNCTIONS
OF
X
AS
A
CONSEQUENCE
OF
THE
CANCEL
LATION
OF
THE
QUADRATIC
TERMS
DUE
TO
THE
SHARED
COVARIANCES
THE
RESULTING
DECISION
BOUNDARIES
CORRESPONDING
TO
THE
MINIMUM
MISCLASSIFICATION
RATE
WILL
OCCUR
WHEN
TWO
OF
THE
POSTERIOR
PROBABILITIES
THE
TWO
LARGEST
ARE
EQUAL
AND
SO
WILL
BE
DEFINED
BY
LINEAR
FUNCTIONS
OF
X
AND
SO
AGAIN
WE
HAVE
A
GENERALIZED
LINEAR
MODEL
IF
WE
RELAX
THE
ASSUMPTION
OF
A
SHARED
COVARIANCE
MATRIX
AND
ALLOW
EACH
CLASS
CONDITIONAL
DENSITY
P
X
K
TO
HAVE
ITS
OWN
COVARIANCE
MATRIX
ΣK
THEN
THE
EARLIER
CANCELLATIONS
WILL
NO
LONGER
OCCUR
AND
WE
WILL
OBTAIN
QUADRATIC
FUNCTIONS
OF
X
GIV
ING
RISE
TO
A
QUADRATIC
DISCRIMINANT
THE
LINEAR
AND
QUADRATIC
DECISION
BOUNDARIES
ARE
ILLUSTRATED
IN
FIGURE
FIGURE
THE
LEFT
HAND
PLOT
SHOWS
THE
CLASS
CONDITIONAL
DENSITIES
FOR
THREE
CLASSES
EACH
HAVING
A
GAUSSIAN
DISTRIBUTION
COLOURED
RED
GREEN
AND
BLUE
IN
WHICH
THE
RED
AND
GREEN
CLASSES
HAVE
THE
SAME
COVARIANCE
MATRIX
THE
RIGHT
HAND
PLOT
SHOWS
THE
CORRESPONDING
POSTERIOR
PROBABILITIES
IN
WHICH
THE
RGB
COLOUR
VECTOR
REPRESENTS
THE
POSTERIOR
PROBABILITIES
FOR
THE
RESPECTIVE
THREE
CLASSES
THE
DECISION
BOUNDARIES
ARE
ALSO
SHOWN
NOTICE
THAT
THE
BOUNDARY
BETWEEN
THE
RED
AND
GREEN
CLASSES
WHICH
HAVE
THE
SAME
COVARIANCE
MATRIX
IS
LINEAR
WHEREAS
THOSE
BETWEEN
THE
OTHER
PAIRS
OF
CLASSES
ARE
QUADRATIC
MAXIMUM
LIKELIHOOD
SOLUTION
ONCE
WE
HAVE
SPECIFIED
A
PARAMETRIC
FUNCTIONAL
FORM
FOR
THE
CLASS
CONDITIONAL
DENSITIES
P
X
K
WE
CAN
THEN
DETERMINE
THE
VALUES
OF
THE
PARAMETERS
TOGETHER
WITH
THE
PRIOR
CLASS
PROBABILITIES
P
K
USING
MAXIMUM
LIKELIHOOD
THIS
REQUIRES
A
DATA
SET
COMPRISING
OBSERVATIONS
OF
X
ALONG
WITH
THEIR
CORRESPONDING
CLASS
LABELS
CONSIDER
FIRST
THE
CASE
OF
TWO
CLASSES
EACH
HAVING
A
GAUSSIAN
CLASS
CONDITIONAL
DENSITY
WITH
A
SHARED
COVARIANCE
MATRIX
AND
SUPPOSE
WE
HAVE
A
DATA
SET
XN
TN
WHERE
N
N
HERE
TN
DENOTES
CLASS
AND
TN
DENOTES
CLASS
WE
DENOTE
THE
PRIOR
CLASS
PROBABILITY
P
Π
SO
THAT
P
Π
FOR
A
DATA
POINT
XN
FROM
CLASS
WE
HAVE
TN
AND
HENCE
P
XN
P
P
XN
ΠN
XN
Σ
SIMILARLY
FOR
CLASS
WE
HAVE
TN
AND
HENCE
P
XN
P
P
XN
Π
N
XN
Σ
THUS
THE
LIKELIHOOD
FUNCTION
IS
GIVEN
BY
P
T
Π
Μ
Μ
Σ
TT
ΠN
XN
Μ
Σ
TN
Π
N
XN
Μ
Σ
TN
WHERE
T
TN
T
AS
USUAL
IT
IS
CONVENIENT
TO
MAXIMIZE
THE
LOG
OF
THE
LIKELIHOOD
FUNCTION
CONSIDER
FIRST
THE
MAXIMIZATION
WITH
RESPECT
TO
Π
THE
TERMS
IN
THE
LOG
LIKELIHOOD
FUNCTION
THAT
DEPEND
ON
Π
ARE
N
TN
LN
Π
TN
LN
Π
N
SETTING
THE
DERIVATIVE
WITH
RESPECT
TO
Π
EQUAL
TO
ZERO
AND
REARRANGING
WE
OBTAIN
Π
T
EXERCISE
WHERE
DENOTES
THE
TOTAL
NUMBER
OF
DATA
POINTS
IN
CLASS
AND
DENOTES
THE
TOTAL
NUMBER
OF
DATA
POINTS
IN
CLASS
THUS
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
FOR
Π
IS
SIMPLY
THE
FRACTION
OF
POINTS
IN
CLASS
AS
EXPECTED
THIS
RESULT
IS
EASILY
GENERALIZED
TO
THE
MULTICLASS
CASE
WHERE
AGAIN
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
OF
THE
PRIOR
PROBABILITY
ASSOCIATED
WITH
CLASS
K
IS
GIVEN
BY
THE
FRACTION
OF
THE
TRAINING
SET
POINTS
ASSIGNED
TO
THAT
CLASS
NOW
CONSIDER
THE
MAXIMIZATION
WITH
RESPECT
TO
AGAIN
WE
CAN
PICK
OUT
OF
THE
LOG
LIKELIHOOD
FUNCTION
THOSE
TERMS
THAT
DEPEND
ON
GIVING
T
LN
N
X
Μ
Σ
T
X
Μ
TΣ
X
Μ
CONST
SETTING
THE
DERIVATIVE
WITH
RESPECT
TO
TO
ZERO
AND
REARRANGING
WE
OBTAIN
N
TNXN
N
WHICH
IS
SIMPLY
THE
MEAN
OF
ALL
THE
INPUT
VECTORS
XN
ASSIGNED
TO
CLASS
BY
A
SIMILAR
ARGUMENT
THE
CORRESPONDING
RESULT
FOR
IS
GIVEN
BY
N
TN
N
XN
WHICH
AGAIN
IS
THE
MEAN
OF
ALL
THE
INPUT
VECTORS
XN
ASSIGNED
TO
CLASS
FINALLY
CONSIDER
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
SHARED
COVARIANCE
MATRIX
Σ
PICKING
OUT
THE
TERMS
IN
THE
LOG
LIKELIHOOD
FUNCTION
THAT
DEPEND
ON
Σ
WE
HAVE
N
N
N
N
N
LN
Σ
N
N
XN
N
Μ
TΣ
XN
N
LN
Σ
N
N
N
XN
Μ
TΣ
XN
N
LN
Σ
N
TR
Σ
WHERE
WE
HAVE
DEFINED
N
N
X
N
X
N
XN
XN
T
T
EXERCISE
SECTION
SECTION
USING
THE
STANDARD
RESULT
FOR
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
A
GAUSSIAN
DISTRI
BUTION
WE
SEE
THAT
Σ
WHICH
REPRESENTS
A
WEIGHTED
AVERAGE
OF
THE
COVARIANCE
MATRICES
ASSOCIATED
WITH
EACH
OF
THE
TWO
CLASSES
SEPARATELY
THIS
RESULT
IS
EASILY
EXTENDED
TO
THE
K
CLASS
PROBLEM
TO
OBTAIN
THE
CORRESPONDING
MAXIMUM
LIKELIHOOD
SOLUTIONS
FOR
THE
PARAMETERS
IN
WHICH
EACH
CLASS
CONDITIONAL
DENSITY
IS
GAUSSIAN
WITH
A
SHARED
COVARIANCE
MATRIX
NOTE
THAT
THE
APPROACH
OF
FITTING
GAUSSIAN
DISTRIBUTIONS
TO
THE
CLASSES
IS
NOT
ROBUST
TO
OUTLIERS
BECAUSE
THE
MAXIMUM
LIKELIHOOD
ESTIMATION
OF
A
GAUSSIAN
IS
NOT
ROBUST
DISCRETE
FEATURES
LET
US
NOW
CONSIDER
THE
CASE
OF
DISCRETE
FEATURE
VALUES
XI
FOR
SIMPLICITY
WE
BEGIN
BY
LOOKING
AT
BINARY
FEATURE
VALUES
XI
AND
DISCUSS
THE
EXTENSION
TO
MORE
GENERAL
DISCRETE
FEATURES
SHORTLY
IF
THERE
ARE
D
INPUTS
THEN
A
GENERAL
DISTRIBU
TION
WOULD
CORRESPOND
TO
A
TABLE
OF
NUMBERS
FOR
EACH
CLASS
CONTAINING
INDEPENDENT
VARIABLES
DUE
TO
THE
SUMMATION
CONSTRAINT
BECAUSE
THIS
GROWS
EXPO
NENTIALLY
WITH
THE
NUMBER
OF
FEATURES
WE
MIGHT
SEEK
A
MORE
RESTRICTED
REPRESENTA
TION
HERE
WE
WILL
MAKE
THE
NAIVE
BAYES
ASSUMPTION
IN
WHICH
THE
FEATURE
VALUES
ARE
TREATED
AS
INDEPENDENT
CONDITIONED
ON
THE
CLASS
K
THUS
WE
HAVE
CLASS
CONDITIONAL
DISTRIBUTIONS
OF
THE
FORM
D
XI
XI
KI
I
WHICH
CONTAIN
D
INDEPENDENT
PARAMETERS
FOR
EACH
CLASS
SUBSTITUTING
INTO
THEN
GIVES
EXERCISE
D
AK
X
XI
LN
ΜKI
XI
LN
ΜKI
LN
P
CK
I
WHICH
AGAIN
ARE
LINEAR
FUNCTIONS
OF
THE
INPUT
VALUES
XI
FOR
THE
CASE
OF
K
CLASSES
WE
CAN
ALTERNATIVELY
CONSIDER
THE
LOGISTIC
SIGMOID
FORMULATION
GIVEN
BY
ANAL
OGOUS
RESULTS
ARE
OBTAINED
FOR
DISCRETE
VARIABLES
EACH
OF
WHICH
CAN
TAKE
M
STATES
EXPONENTIAL
FAMILY
AS
WE
HAVE
SEEN
FOR
BOTH
GAUSSIAN
DISTRIBUTED
AND
DISCRETE
INPUTS
THE
POSTERIOR
CLASS
PROBABILITIES
ARE
GIVEN
BY
GENERALIZED
LINEAR
MODELS
WITH
LOGISTIC
SIGMOID
K
CLASSES
OR
SOFTMAX
K
CLASSES
ACTIVATION
FUNCTIONS
THESE
ARE
PARTICULAR
CASES
OF
A
MORE
GENERAL
RESULT
OBTAINED
BY
ASSUMING
THAT
THE
CLASS
CONDITIONAL
DENSITIES
P
X
K
ARE
MEMBERS
OF
THE
EXPONENTIAL
FAMILY
OF
DISTRIBUTIONS
USING
THE
FORM
FOR
MEMBERS
OF
THE
EXPONENTIAL
FAMILY
WE
SEE
THAT
THE
DISTRIBUTION
OF
X
CAN
BE
WRITTEN
IN
THE
FORM
P
X
ΛK
H
X
G
ΛK
EXP
ΛTU
X
WE
NOW
RESTRICT
ATTENTION
TO
THE
SUBCLASS
OF
SUCH
DISTRIBUTIONS
FOR
WHICH
U
X
X
THEN
WE
MAKE
USE
OF
TO
INTRODUCE
A
SCALING
PARAMETER
SO
THAT
WE
OBTAIN
THE
RESTRICTED
SET
OF
EXPONENTIAL
FAMILY
CLASS
CONDITIONAL
DENSITIES
OF
THE
FORM
P
X
Λ
H
X
G
Λ
EXP
ΛTX
NOTE
THAT
WE
ARE
ALLOWING
EACH
CLASS
TO
HAVE
ITS
OWN
PARAMETER
VECTOR
ΛK
BUT
WE
ARE
ASSUMING
THAT
THE
CLASSES
SHARE
THE
SAME
SCALE
PARAMETER
FOR
THE
TWO
CLASS
PROBLEM
WE
SUBSTITUTE
THIS
EXPRESSION
FOR
THE
CLASS
CONDITIONAL
DENSITIES
INTO
AND
WE
SEE
THAT
THE
POSTERIOR
CLASS
PROBABILITY
IS
AGAIN
GIVEN
BY
A
LOGISTIC
SIGMOID
ACTING
ON
A
LINEAR
FUNCTION
A
X
WHICH
IS
GIVEN
BY
A
X
TX
LN
G
LN
G
LN
P
LN
P
SIMILARLY
FOR
THE
K
CLASS
PROBLEM
WE
SUBSTITUTE
THE
CLASS
CONDITIONAL
DENSITY
EX
PRESSION
INTO
TO
GIVE
AK
X
ΛTX
LN
G
ΛK
LN
P
CK
AND
SO
AGAIN
IS
A
LINEAR
FUNCTION
OF
X
PROBABILISTIC
DISCRIMINATIVE
MODELS
FOR
THE
TWO
CLASS
CLASSIFICATION
PROBLEM
WE
HAVE
SEEN
THAT
THE
POSTERIOR
PROBABILITY
OF
CLASS
CAN
BE
WRITTEN
AS
A
LOGISTIC
SIGMOID
ACTING
ON
A
LINEAR
FUNCTION
OF
X
FOR
A
WIDE
CHOICE
OF
CLASS
CONDITIONAL
DISTRIBUTIONS
P
X
K
SIMILARLY
FOR
THE
MULTICLASS
CASE
THE
POSTERIOR
PROBABILITY
OF
CLASS
K
IS
GIVEN
BY
A
SOFTMAX
TRANSFORMATION
OF
A
LINEAR
FUNCTION
OF
X
FOR
SPECIFIC
CHOICES
OF
THE
CLASS
CONDITIONAL
DENSITIES
P
X
K
WE
HAVE
USED
MAXIMUM
LIKELIHOOD
TO
DETERMINE
THE
PARAMETERS
OF
THE
DENSITIES
AS
WELL
AS
THE
CLASS
PRIORS
P
K
AND
THEN
USED
BAYES
THEOREM
TO
FIND
THE
POSTERIOR
CLASS
PROBABILITIES
HOWEVER
AN
ALTERNATIVE
APPROACH
IS
TO
USE
THE
FUNCTIONAL
FORM
OF
THE
GENERALIZED
LINEAR
MODEL
EXPLICITLY
AND
TO
DETERMINE
ITS
PARAMETERS
DIRECTLY
BY
USING
MAXIMUM
LIKELIHOOD
WE
SHALL
SEE
THAT
THERE
IS
AN
EFFICIENT
ALGORITHM
FINDING
SUCH
SOLUTIONS
KNOWN
AS
ITERATIVE
REWEIGHTED
LEAST
SQUARES
OR
IRLS
THE
INDIRECT
APPROACH
TO
FINDING
THE
PARAMETERS
OF
A
GENERALIZED
LINEAR
MODEL
BY
FITTING
CLASS
CONDITIONAL
DENSITIES
AND
CLASS
PRIORS
SEPARATELY
AND
THEN
APPLYING
FIGURE
ILLUSTRATION
OF
THE
ROLE
OF
NONLINEAR
BASIS
FUNCTIONS
IN
LINEAR
CLASSIFICATION
MODELS
THE
LEFT
PLOT
SHOWS
THE
ORIGINAL
INPUT
SPACE
TOGETHER
WITH
DATA
POINTS
FROM
TWO
CLASSES
LABELLED
RED
AND
BLUE
TWO
GAUSSIAN
BASIS
FUNCTIONS
X
AND
X
ARE
DEFINED
IN
THIS
SPACE
WITH
CENTRES
SHOWN
BY
THE
GREEN
CROSSES
AND
WITH
CONTOURS
SHOWN
BY
THE
GREEN
CIRCLES
THE
RIGHT
HAND
PLOT
SHOWS
THE
CORRESPONDING
FEATURE
SPACE
TOGETHER
WITH
THE
LINEAR
DECISION
BOUNDARY
OBTAINED
GIVEN
BY
A
LOGISTIC
REGRESSION
MODEL
OF
THE
FORM
DISCUSSED
IN
SECTION
THIS
CORRESPONDS
TO
A
NONLINEAR
DECISION
BOUNDARY
IN
THE
ORIGINAL
INPUT
SPACE
SHOWN
BY
THE
BLACK
CURVE
IN
THE
LEFT
HAND
PLOT
BAYES
THEOREM
REPRESENTS
AN
EXAMPLE
OF
GENERATIVE
MODELLING
BECAUSE
WE
COULD
TAKE
SUCH
A
MODEL
AND
GENERATE
SYNTHETIC
DATA
BY
DRAWING
VALUES
OF
X
FROM
THE
MARGINAL
DISTRIBUTION
P
X
IN
THE
DIRECT
APPROACH
WE
ARE
MAXIMIZING
A
LIKELIHOOD
FUNCTION
DEFINED
THROUGH
THE
CONDITIONAL
DISTRIBUTION
P
K
X
WHICH
REPRESENTS
A
FORM
OF
DISCRIMINATIVE
TRAINING
ONE
ADVANTAGE
OF
THE
DISCRIMINATIVE
APPROACH
IS
THAT
THERE
WILL
TYPICALLY
BE
FEWER
ADAPTIVE
PARAMETERS
TO
BE
DETERMINED
AS
WE
SHALL
SEE
SHORTLY
IT
MAY
ALSO
LEAD
TO
IMPROVED
PREDICTIVE
PERFORMANCE
PARTICULARLY
WHEN
THE
CLASS
CONDITIONAL
DENSITY
ASSUMPTIONS
GIVE
A
POOR
APPROXIMATION
TO
THE
TRUE
DIS
TRIBUTIONS
FIXED
BASIS
FUNCTIONS
SO
FAR
IN
THIS
CHAPTER
WE
HAVE
CONSIDERED
CLASSIFICATION
MODELS
THAT
WORK
DI
RECTLY
WITH
THE
ORIGINAL
INPUT
VECTOR
X
HOWEVER
ALL
OF
THE
ALGORITHMS
ARE
EQUALLY
APPLICABLE
IF
WE
FIRST
MAKE
A
FIXED
NONLINEAR
TRANSFORMATION
OF
THE
INPUTS
USING
A
VECTOR
OF
BASIS
FUNCTIONS
Φ
X
THE
RESULTING
DECISION
BOUNDARIES
WILL
BE
LINEAR
IN
THE
FEATURE
SPACE
Φ
AND
THESE
CORRESPOND
TO
NONLINEAR
DECISION
BOUNDARIES
IN
THE
ORIGINAL
X
SPACE
AS
ILLUSTRATED
IN
FIGURE
CLASSES
THAT
ARE
LINEARLY
SEPARABLE
IN
THE
FEATURE
SPACE
Φ
X
NEED
NOT
BE
LINEARLY
SEPARABLE
IN
THE
ORIGINAL
OBSERVATION
SPACE
X
NOTE
THAT
AS
IN
OUR
DISCUSSION
OF
LINEAR
MODELS
FOR
REGRESSION
ONE
OF
THE
SECTION
BASIS
FUNCTIONS
IS
TYPICALLY
SET
TO
A
CONSTANT
SAY
X
SO
THAT
THE
CORRESPOND
ING
PARAMETER
PLAYS
THE
ROLE
OF
A
BIAS
FOR
THE
REMAINDER
OF
THIS
CHAPTER
WE
SHALL
INCLUDE
A
FIXED
BASIS
FUNCTION
TRANSFORMATION
Φ
X
AS
THIS
WILL
HIGHLIGHT
SOME
USEFUL
SIMILARITIES
TO
THE
REGRESSION
MODELS
DISCUSSED
IN
CHAPTER
FOR
MANY
PROBLEMS
OF
PRACTICAL
INTEREST
THERE
IS
SIGNIFICANT
OVERLAP
BETWEEN
THE
CLASS
CONDITIONAL
DENSITIES
P
X
K
THIS
CORRESPONDS
TO
POSTERIOR
PROBABILITIES
P
K
X
WHICH
FOR
AT
LEAST
SOME
VALUES
OF
X
ARE
NOT
OR
IN
SUCH
CASES
THE
OPTI
MAL
SOLUTION
IS
OBTAINED
BY
MODELLING
THE
POSTERIOR
PROBABILITIES
ACCURATELY
AND
THEN
APPLYING
STANDARD
DECISION
THEORY
AS
DISCUSSED
IN
CHAPTER
NOTE
THAT
NONLINEAR
TRANSFORMATIONS
Φ
X
CANNOT
REMOVE
SUCH
CLASS
OVERLAP
INDEED
THEY
CAN
INCREASE
THE
LEVEL
OF
OVERLAP
OR
CREATE
OVERLAP
WHERE
NONE
EXISTED
IN
THE
ORIGINAL
OBSERVATION
SPACE
HOWEVER
SUITABLE
CHOICES
OF
NONLINEARITY
CAN
MAKE
THE
PROCESS
OF
MODELLING
THE
POSTERIOR
PROBABILITIES
EASIER
SUCH
FIXED
BASIS
FUNCTION
MODELS
HAVE
IMPORTANT
LIMITATIONS
AND
THESE
WILL
BE
RESOLVED
IN
LATER
CHAPTERS
BY
ALLOWING
THE
BASIS
FUNCTIONS
THEMSELVES
TO
ADAPT
TO
THE
DATA
NOTWITHSTANDING
THESE
LIMITATIONS
MODELS
WITH
FIXED
NONLINEAR
BASIS
FUNCTIONS
PLAY
AN
IMPORTANT
ROLE
IN
APPLICATIONS
AND
A
DISCUSSION
OF
SUCH
MODELS
WILL
INTRO
DUCE
MANY
OF
THE
KEY
CONCEPTS
NEEDED
FOR
AN
UNDERSTANDING
OF
THEIR
MORE
COMPLEX
COUNTERPARTS
LOGISTIC
REGRESSION
WE
BEGIN
OUR
TREATMENT
OF
GENERALIZED
LINEAR
MODELS
BY
CONSIDERING
THE
PROBLEM
OF
TWO
CLASS
CLASSIFICATION
IN
OUR
DISCUSSION
OF
GENERATIVE
APPROACHES
IN
SECTION
WE
SAW
THAT
UNDER
RATHER
GENERAL
ASSUMPTIONS
THE
POSTERIOR
PROBABILITY
OF
CLASS
CAN
BE
WRITTEN
AS
A
LOGISTIC
SIGMOID
ACTING
ON
A
LINEAR
FUNCTION
OF
THE
FEATURE
VECTOR
Φ
SO
THAT
EXERCISE
P
Φ
Y
Φ
Σ
WTΦ
WITH
P
Φ
P
Φ
HERE
Σ
IS
THE
LOGISTIC
SIGMOID
FUNCTION
DEFINED
BY
IN
THE
TERMINOLOGY
OF
STATISTICS
THIS
MODEL
IS
KNOWN
AS
LOGISTIC
REGRESSION
ALTHOUGH
IT
SHOULD
BE
EMPHASIZED
THAT
THIS
IS
A
MODEL
FOR
CLASSIFICATION
RATHER
THAN
REGRESSION
FOR
AN
M
DIMENSIONAL
FEATURE
SPACE
Φ
THIS
MODEL
HAS
M
ADJUSTABLE
PARAMETERS
BY
CONTRAST
IF
WE
HAD
FITTED
GAUSSIAN
CLASS
CONDITIONAL
DENSITIES
USING
MAXIMUM
LIKELIHOOD
WE
WOULD
HAVE
USED
PARAMETERS
FOR
THE
MEANS
AND
M
M
PARAMETERS
FOR
THE
SHARED
COVARIANCE
MATRIX
TOGETHER
WITH
THE
CLASS
PRIOR
P
THIS
GIVES
A
TOTAL
OF
M
M
PARAMETERS
WHICH
GROWS
QUADRATICALLY
WITH
M
IN
CONTRAST
TO
THE
LINEAR
DEPENDENCE
ON
M
OF
THE
NUMBER
OF
PARAMETERS
IN
LOGISTIC
REGRESSION
FOR
LARGE
VALUES
OF
M
THERE
IS
A
CLEAR
ADVANTAGE
IN
WORKING
WITH
THE
LOGISTIC
REGRESSION
MODEL
DIRECTLY
WE
NOW
USE
MAXIMUM
LIKELIHOOD
TO
DETERMINE
THE
PARAMETERS
OF
THE
LOGISTIC
REGRESSION
MODEL
TO
DO
THIS
WE
SHALL
MAKE
USE
OF
THE
DERIVATIVE
OF
THE
LOGISTIC
SIG
MOID
FUNCTION
WHICH
CAN
CONVENIENTLY
BE
EXPRESSED
IN
TERMS
OF
THE
SIGMOID
FUNCTION
ITSELF
DΣ
Σ
Σ
DA
FOR
A
DATA
SET
ΦN
TN
WHERE
TN
AND
ΦN
Φ
XN
WITH
N
N
TN
TN
N
N
WHERE
T
TN
T
AND
YN
P
ΦN
AS
USUAL
WE
CAN
DEFINE
AN
ERROR
FUNCTION
BY
TAKING
THE
NEGATIVE
LOGARITHM
OF
THE
LIKELIHOOD
WHICH
GIVES
THE
CROSS
ENTROPY
ERROR
FUNCTION
IN
THE
FORM
EXERCISE
N
E
W
LN
P
T
W
TN
LN
YN
TN
LN
YN
N
WHERE
YN
Σ
AN
AND
AN
WTΦN
TAKING
THE
GRADIENT
OF
THE
ERROR
FUNCTION
WITH
RESPECT
TO
W
WE
OBTAIN
SECTION
EXERCISE
N
E
W
YN
TN
ΦN
N
WHERE
WE
HAVE
MADE
USE
OF
WE
SEE
THAT
THE
FACTOR
INVOLVING
THE
DERIVATIVE
OF
THE
LOGISTIC
SIGMOID
HAS
CANCELLED
LEADING
TO
A
SIMPLIFIED
FORM
FOR
THE
GRADIENT
OF
THE
LOG
LIKELIHOOD
IN
PARTICULAR
THE
CONTRIBUTION
TO
THE
GRADIENT
FROM
DATA
POINT
N
IS
GIVEN
BY
THE
ERROR
YN
TN
BETWEEN
THE
TARGET
VALUE
AND
THE
PREDICTION
OF
THE
MODEL
TIMES
THE
BASIS
FUNCTION
VECTOR
ΦN
FURTHERMORE
COMPARISON
WITH
SHOWS
THAT
THIS
TAKES
PRECISELY
THE
SAME
FORM
AS
THE
GRADIENT
OF
THE
SUM
OF
SQUARES
ERROR
FUNCTION
FOR
THE
LINEAR
REGRESSION
MODEL
IF
DESIRED
WE
COULD
MAKE
USE
OF
THE
RESULT
TO
GIVE
A
SEQUENTIAL
ALGORITHM
IN
WHICH
PATTERNS
ARE
PRESENTED
ONE
AT
A
TIME
IN
WHICH
EACH
OF
THE
WEIGHT
VECTORS
IS
UPDATED
USING
IN
WHICH
EN
IS
THE
NTH
TERM
IN
IT
IS
WORTH
NOTING
THAT
MAXIMUM
LIKELIHOOD
CAN
EXHIBIT
SEVERE
OVER
FITTING
FOR
DATA
SETS
THAT
ARE
LINEARLY
SEPARABLE
THIS
ARISES
BECAUSE
THE
MAXIMUM
LIKELIHOOD
SO
LUTION
OCCURS
WHEN
THE
HYPERPLANE
CORRESPONDING
TO
Σ
EQUIVALENT
TO
WTΦ
SEPARATES
THE
TWO
CLASSES
AND
THE
MAGNITUDE
OF
W
GOES
TO
INFINITY
IN
THIS
CASE
THE
LOGISTIC
SIGMOID
FUNCTION
BECOMES
INFINITELY
STEEP
IN
FEATURE
SPACE
CORRESPONDING
TO
A
HEAVISIDE
STEP
FUNCTION
SO
THAT
EVERY
TRAINING
POINT
FROM
EACH
CLASS
K
IS
ASSIGNED
A
POSTERIOR
PROBABILITY
P
K
X
FURTHERMORE
THERE
IS
TYPICALLY
A
CONTINUUM
OF
SUCH
SOLUTIONS
BECAUSE
ANY
SEPARATING
HYPERPLANE
WILL
GIVE
RISE
TO
THE
SAME
POS
TERIOR
PROBABILITIES
AT
THE
TRAINING
DATA
POINTS
AS
WILL
BE
SEEN
LATER
IN
FIGURE
MAXIMUM
LIKELIHOOD
PROVIDES
NO
WAY
TO
FAVOUR
ONE
SUCH
SOLUTION
OVER
ANOTHER
AND
WHICH
SOLUTION
IS
FOUND
IN
PRACTICE
WILL
DEPEND
ON
THE
CHOICE
OF
OPTIMIZATION
ALGO
RITHM
AND
ON
THE
PARAMETER
INITIALIZATION
NOTE
THAT
THE
PROBLEM
WILL
ARISE
EVEN
IF
THE
NUMBER
OF
DATA
POINTS
IS
LARGE
COMPARED
WITH
THE
NUMBER
OF
PARAMETERS
IN
THE
MODEL
SO
LONG
AS
THE
TRAINING
DATA
SET
IS
LINEARLY
SEPARABLE
THE
SINGULARITY
CAN
BE
AVOIDED
BY
INCLUSION
OF
A
PRIOR
AND
FINDING
A
MAP
SOLUTION
FOR
W
OR
EQUIVALENTLY
BY
ADDING
A
REGULARIZATION
TERM
TO
THE
ERROR
FUNCTION
ITERATIVE
REWEIGHTED
LEAST
SQUARES
IN
THE
CASE
OF
THE
LINEAR
REGRESSION
MODELS
DISCUSSED
IN
CHAPTER
THE
MAXI
MUM
LIKELIHOOD
SOLUTION
ON
THE
ASSUMPTION
OF
A
GAUSSIAN
NOISE
MODEL
LEADS
TO
A
CLOSED
FORM
SOLUTION
THIS
WAS
A
CONSEQUENCE
OF
THE
QUADRATIC
DEPENDENCE
OF
THE
LOG
LIKELIHOOD
FUNCTION
ON
THE
PARAMETER
VECTOR
W
FOR
LOGISTIC
REGRESSION
THERE
IS
NO
LONGER
A
CLOSED
FORM
SOLUTION
DUE
TO
THE
NONLINEARITY
OF
THE
LOGISTIC
SIGMOID
FUNCTION
HOWEVER
THE
DEPARTURE
FROM
A
QUADRATIC
FORM
IS
NOT
SUBSTANTIAL
TO
BE
PRECISE
THE
ERROR
FUNCTION
IS
CONCAVE
AS
WE
SHALL
SEE
SHORTLY
AND
HENCE
HAS
A
UNIQUE
MINIMUM
FURTHERMORE
THE
ERROR
FUNCTION
CAN
BE
MINIMIZED
BY
AN
EFFICIENT
ITERATIVE
TECHNIQUE
BASED
ON
THE
NEWTON
RAPHSON
ITERATIVE
OPTIMIZATION
SCHEME
WHICH
USES
A
LOCAL
QUADRATIC
APPROXIMATION
TO
THE
LOG
LIKELIHOOD
FUNCTION
THE
NEWTON
RAPHSON
UPDATE
FOR
MINIMIZING
A
FUNCTION
E
W
TAKES
THE
FORM
FLETCHER
BISHOP
AND
NABNEY
W
NEW
W
OLD
H
E
W
WHERE
H
IS
THE
HESSIAN
MATRIX
WHOSE
ELEMENTS
COMPRISE
THE
SECOND
DERIVATIVES
OF
E
W
WITH
RESPECT
TO
THE
COMPONENTS
OF
W
LET
US
FIRST
OF
ALL
APPLY
THE
NEWTON
RAPHSON
METHOD
TO
THE
LINEAR
REGRESSION
MODEL
WITH
THE
SUM
OF
SQUARES
ERROR
FUNCTION
THE
GRADIENT
AND
HESSIAN
OF
THIS
ERROR
FUNCTION
ARE
GIVEN
BY
E
W
H
E
W
N
WTΦN
TN
ΦN
ΦTΦW
ΦTT
N
N
T
T
N
SECTION
N
WHERE
Φ
IS
THE
N
M
DESIGN
MATRIX
WHOSE
NTH
ROW
IS
GIVEN
BY
ΦT
THE
NEWTON
RAPHSON
UPDATE
THEN
TAKES
THE
FORM
W
NEW
W
OLD
ΦTΦ
ΦTΦW
OLD
ΦTT
ΦTΦ
WHICH
WE
RECOGNIZE
AS
THE
STANDARD
LEAST
SQUARES
SOLUTION
NOTE
THAT
THE
ERROR
FUNC
TION
IN
THIS
CASE
IS
QUADRATIC
AND
HENCE
THE
NEWTON
RAPHSON
FORMULA
GIVES
THE
EXACT
SOLUTION
IN
ONE
STEP
NOW
LET
US
APPLY
THE
NEWTON
RAPHSON
UPDATE
TO
THE
CROSS
ENTROPY
ERROR
FUNCTION
FOR
THE
LOGISTIC
REGRESSION
MODEL
FROM
WE
SEE
THAT
THE
GRADIENT
AND
HESSIAN
OF
THIS
ERROR
FUNCTION
ARE
GIVEN
BY
E
W
N
YN
TN
ΦN
ΦT
Y
T
N
N
H
E
W
YN
YN
ΦN
N
ΦT
ΦTRΦ
WHERE
WE
HAVE
MADE
USE
OF
ALSO
WE
HAVE
INTRODUCED
THE
N
N
DIAGONAL
MATRIX
R
WITH
ELEMENTS
EXERCISE
RNN
YN
YN
WE
SEE
THAT
THE
HESSIAN
IS
NO
LONGER
CONSTANT
BUT
DEPENDS
ON
W
THROUGH
THE
WEIGHT
ING
MATRIX
R
CORRESPONDING
TO
THE
FACT
THAT
THE
ERROR
FUNCTION
IS
NO
LONGER
QUADRATIC
USING
THE
PROPERTY
YN
WHICH
FOLLOWS
FROM
THE
FORM
OF
THE
LOGISTIC
SIGMOID
FUNCTION
WE
SEE
THAT
UTHU
FOR
AN
ARBITRARY
VECTOR
U
AND
SO
THE
HESSIAN
MATRIX
H
IS
POSITIVE
DEFINITE
IT
FOLLOWS
THAT
THE
ERROR
FUNCTION
IS
A
CONCAVE
FUNCTION
OF
W
AND
HENCE
HAS
A
UNIQUE
MINIMUM
THE
NEWTON
RAPHSON
UPDATE
FORMULA
FOR
THE
LOGISTIC
REGRESSION
MODEL
THEN
BE
COMES
W
NEW
W
OLD
ΦTRΦ
Y
T
Φ
RΦ
Φ
RΦW
OLD
Φ
Y
T
ΦTRΦ
WHERE
Z
IS
AN
N
DIMENSIONAL
VECTOR
WITH
ELEMENTS
Z
ΦW
OLD
R
Y
T
WE
SEE
THAT
THE
UPDATE
FORMULA
TAKES
THE
FORM
OF
A
SET
OF
NORMAL
EQUATIONS
FOR
A
WEIGHTED
LEAST
SQUARES
PROBLEM
BECAUSE
THE
WEIGHING
MATRIX
R
IS
NOT
CONSTANT
BUT
DEPENDS
ON
THE
PARAMETER
VECTOR
W
WE
MUST
APPLY
THE
NORMAL
EQUATIONS
ITERATIVELY
EACH
TIME
USING
THE
NEW
WEIGHT
VECTOR
W
TO
COMPUTE
A
REVISED
WEIGHING
MATRIX
R
FOR
THIS
REASON
THE
ALGORITHM
IS
KNOWN
AS
ITERATIVE
REWEIGHTED
LEAST
SQUARES
OR
IRLS
RUBIN
AS
IN
THE
WEIGHTED
LEAST
SQUARES
PROBLEM
THE
ELEMENTS
OF
THE
DIAGONAL
WEIGHTING
MATRIX
R
CAN
BE
INTERPRETED
AS
VARIANCES
BECAUSE
THE
MEAN
AND
VARIANCE
OF
T
IN
THE
LOGISTIC
REGRESSION
MODEL
ARE
GIVEN
BY
E
T
Σ
X
Y
VAR
T
E
E
T
Σ
X
Σ
X
Y
Y
WHERE
WE
HAVE
USED
THE
PROPERTY
T
FOR
T
IN
FACT
WE
CAN
INTERPRET
IRLS
AS
THE
SOLUTION
TO
A
LINEARIZED
PROBLEM
IN
THE
SPACE
OF
THE
VARIABLE
A
WTΦ
THE
QUANTITY
ZN
WHICH
CORRESPONDS
TO
THE
NTH
ELEMENT
OF
Z
CAN
THEN
BE
GIVEN
A
SIMPLE
INTERPRETATION
AS
AN
EFFECTIVE
TARGET
VALUE
IN
THIS
SPACE
OBTAINED
BY
MAKING
A
LOCAL
LINEAR
APPROXIMATION
TO
THE
LOGISTIC
SIGMOID
FUNCTION
AROUND
THE
CURRENT
OPERATING
POINT
W
OLD
A
W
A
W
OLD
DAN
W
OLD
TN
YN
ΦTW
OLD
YN
TN
Z
N
YN
YN
SECTION
MULTICLASS
LOGISTIC
REGRESSION
IN
OUR
DISCUSSION
OF
GENERATIVE
MODELS
FOR
MULTICLASS
CLASSIFICATION
WE
HAVE
SEEN
THAT
FOR
A
LARGE
CLASS
OF
DISTRIBUTIONS
THE
POSTERIOR
PROBABILITIES
ARE
GIVEN
BY
A
SOFTMAX
TRANSFORMATION
OF
LINEAR
FUNCTIONS
OF
THE
FEATURE
VARIABLES
SO
THAT
Φ
EXP
AK
P
CK
Φ
YK
J
EXP
AJ
EXERCISE
WHERE
THE
ACTIVATIONS
AK
ARE
GIVEN
BY
AK
WTΦ
THERE
WE
USED
MAXIMUM
LIKELIHOOD
TO
DETERMINE
SEPARATELY
THE
CLASS
CONDITIONAL
DENSITIES
AND
THE
CLASS
PRIORS
AND
THEN
FOUND
THE
CORRESPONDING
POSTERIOR
PROBABILITIES
USING
BAYES
THEOREM
THEREBY
IMPLICITLY
DETERMINING
THE
PARAMETERS
WK
HERE
WE
CONSIDER
THE
USE
OF
MAXIMUM
LIKELIHOOD
TO
DETERMINE
THE
PARAMETERS
WK
OF
THIS
MODEL
DIRECTLY
TO
DO
THIS
WE
WILL
REQUIRE
THE
DERIVATIVES
OF
YK
WITH
RESPECT
TO
ALL
OF
THE
ACTIVATIONS
AJ
THESE
ARE
GIVEN
BY
YK
Y
AJ
K
IKJ
YJ
WHERE
IKJ
ARE
THE
ELEMENTS
OF
THE
IDENTITY
MATRIX
NEXT
WE
WRITE
DOWN
THE
LIKELIHOOD
FUNCTION
THIS
IS
MOST
EASILY
DONE
USING
THE
OF
K
CODING
SCHEME
IN
WHICH
THE
TARGET
VECTOR
TN
FOR
A
FEATURE
VECTOR
ΦN
BELONGING
TO
CLASS
K
IS
A
BINARY
VECTOR
WITH
ALL
ELEMENTS
ZERO
EXCEPT
FOR
ELEMENT
K
WHICH
EQUALS
ONE
THE
LIKELIHOOD
FUNCTION
IS
THEN
GIVEN
BY
N
K
N
K
P
T
WK
TT
TT
P
CK
ΦN
TNK
TT
TT
YTNK
N
K
N
K
WHERE
YNK
YK
ΦN
AND
T
IS
AN
N
K
MATRIX
OF
TARGET
VARIABLES
WITH
ELEMENTS
TNK
TAKING
THE
NEGATIVE
LOGARITHM
THEN
GIVES
N
K
EXERCISE
E
WK
LN
P
T
WK
TNK
LN
YNK
N
K
WHICH
IS
KNOWN
AS
THE
CROSS
ENTROPY
ERROR
FUNCTION
FOR
THE
MULTICLASS
CLASSIFICATION
PROBLEM
WE
NOW
TAKE
THE
GRADIENT
OF
THE
ERROR
FUNCTION
WITH
RESPECT
TO
ONE
OF
THE
PARAM
ETER
VECTORS
WJ
MAKING
USE
OF
THE
RESULT
FOR
THE
DERIVATIVES
OF
THE
SOFTMAX
FUNCTION
WE
OBTAIN
N
WJ
E
WK
YNJ
TNJ
ΦN
N
WHERE
WE
HAVE
MADE
USE
OF
K
TNK
ONCE
AGAIN
WE
SEE
THE
SAME
FORM
ARISING
FOR
THE
GRADIENT
AS
WAS
FOUND
FOR
THE
SUM
OF
SQUARES
ERROR
FUNCTION
WITH
THE
LINEAR
MODEL
AND
THE
CROSS
ENTROPY
ERROR
FOR
THE
LOGISTIC
REGRESSION
MODEL
NAMELY
THE
PROD
UCT
OF
THE
ERROR
YNJ
TNJ
TIMES
THE
BASIS
FUNCTION
ΦN
AGAIN
WE
COULD
USE
THIS
TO
FORMULATE
A
SEQUENTIAL
ALGORITHM
IN
WHICH
PATTERNS
ARE
PRESENTED
ONE
AT
A
TIME
IN
WHICH
EACH
OF
THE
WEIGHT
VECTORS
IS
UPDATED
USING
WE
HAVE
SEEN
THAT
THE
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
FUNCTION
FOR
A
LINEAR
REGRES
SION
MODEL
WITH
RESPECT
TO
THE
PARAMETER
VECTOR
W
FOR
A
DATA
POINT
N
TOOK
THE
FORM
OF
THE
ERROR
YN
TN
TIMES
THE
FEATURE
VECTOR
ΦN
SIMILARLY
FOR
THE
COMBINATION
OF
LOGISTIC
SIGMOID
ACTIVATION
FUNCTION
AND
CROSS
ENTROPY
ERROR
FUNCTION
AND
FOR
THE
SOFTMAX
ACTIVATION
FUNCTION
WITH
THE
MULTICLASS
CROSS
ENTROPY
ERROR
FUNCTION
WE
AGAIN
OBTAIN
THIS
SAME
SIMPLE
FORM
THIS
IS
AN
EXAMPLE
OF
A
MORE
GENERAL
RESULT
AS
WE
SHALL
SEE
IN
SECTION
TO
FIND
A
BATCH
ALGORITHM
WE
AGAIN
APPEAL
TO
THE
NEWTON
RAPHSON
UPDATE
TO
OBTAIN
THE
CORRESPONDING
IRLS
ALGORITHM
FOR
THE
MULTICLASS
PROBLEM
THIS
REQUIRES
EVALUATION
OF
THE
HESSIAN
MATRIX
THAT
COMPRISES
BLOCKS
OF
SIZE
M
M
IN
WHICH
BLOCK
J
K
IS
GIVEN
BY
W
W
E
WK
YNK
IKJ
YNJ
ΦNΦT
EXERCISE
AS
WITH
THE
TWO
CLASS
PROBLEM
THE
HESSIAN
MATRIX
FOR
THE
MULTICLASS
LOGISTIC
REGRES
SION
MODEL
IS
POSITIVE
DEFINITE
AND
SO
THE
ERROR
FUNCTION
AGAIN
HAS
A
UNIQUE
MINIMUM
PRACTICAL
DETAILS
OF
IRLS
FOR
THE
MULTICLASS
CASE
CAN
BE
FOUND
IN
BISHOP
AND
NABNEY
PROBIT
REGRESSION
WE
HAVE
SEEN
THAT
FOR
A
BROAD
RANGE
OF
CLASS
CONDITIONAL
DISTRIBUTIONS
DESCRIBED
BY
THE
EXPONENTIAL
FAMILY
THE
RESULTING
POSTERIOR
CLASS
PROBABILITIES
ARE
GIVEN
BY
A
LOGISTIC
OR
SOFTMAX
TRANSFORMATION
ACTING
ON
A
LINEAR
FUNCTION
OF
THE
FEATURE
VARI
ABLES
HOWEVER
NOT
ALL
CHOICES
OF
CLASS
CONDITIONAL
DENSITY
GIVE
RISE
TO
SUCH
A
SIMPLE
FORM
FOR
THE
POSTERIOR
PROBABILITIES
FOR
INSTANCE
IF
THE
CLASS
CONDITIONAL
DENSITIES
ARE
MODELLED
USING
GAUSSIAN
MIXTURES
THIS
SUGGESTS
THAT
IT
MIGHT
BE
WORTH
EXPLORING
OTHER
TYPES
OF
DISCRIMINATIVE
PROBABILISTIC
MODEL
FOR
THE
PURPOSES
OF
THIS
CHAPTER
HOWEVER
WE
SHALL
RETURN
TO
THE
TWO
CLASS
CASE
AND
AGAIN
REMAIN
WITHIN
THE
FRAME
WORK
OF
GENERALIZED
LINEAR
MODELS
SO
THAT
P
T
A
F
A
WHERE
A
WTΦ
AND
F
IS
THE
ACTIVATION
FUNCTION
ONE
WAY
TO
MOTIVATE
AN
ALTERNATIVE
CHOICE
FOR
THE
LINK
FUNCTION
IS
TO
CONSIDER
A
NOISY
THRESHOLD
MODEL
AS
FOLLOWS
FOR
EACH
INPUT
ΦN
WE
EVALUATE
AN
WTΦN
AND
THEN
WE
SET
THE
TARGET
VALUE
ACCORDING
TO
TN
IF
AN
Θ
TN
OTHERWISE
FIGURE
SCHEMATIC
EXAMPLE
OF
A
PROBABILITY
DENSITY
P
Θ
SHOWN
BY
THE
BLUE
CURVE
GIVEN
IN
THIS
EXAMPLE
BY
A
MIXTURE
OF
TWO
GAUSSIANS
ALONG
WITH
ITS
CUMULATIVE
DISTRIBUTION
FUNCTION
F
A
SHOWN
BY
THE
RED
CURVE
NOTE
THAT
THE
VALUE
OF
THE
BLUE
CURVE
AT
ANY
POINT
SUCH
AS
THAT
INDICATED
BY
THE
VERTICAL
GREEN
LINE
CORRESPONDS
TO
THE
SLOPE
OF
THE
RED
CURVE
AT
THE
SAME
POINT
CONVERSELY
THE
VALUE
OF
THE
RED
CURVE
AT
THIS
POINT
CORRESPONDS
TO
THE
AREA
UNDER
THE
BLUE
CURVE
INDICATED
BY
THE
SHADED
GREEN
REGION
IN
THE
STOCHASTIC
THRESHOLD
MODEL
THE
CLASS
LABEL
TAKES
THE
VALUE
T
IF
THE
VALUE
OF
A
WTΦ
EXCEEDS
A
THRESHOLD
OTH
ERWISE
IT
TAKES
THE
VALUE
T
THIS
IS
EQUIVALENT
TO
AN
ACTIVATION
FUNCTION
GIVEN
BY
THE
CUMULATIVE
DISTRIBUTION
FUNCTION
F
A
IF
THE
VALUE
OF
Θ
IS
DRAWN
FROM
A
PROBABILITY
DENSITY
P
Θ
THEN
THE
CORRESPONDING
ACTIVATION
FUNCTION
WILL
BE
GIVEN
BY
THE
CUMULATIVE
DISTRIBUTION
FUNCTION
AS
ILLUSTRATED
IN
FIGURE
F
A
P
Θ
DΘ
AS
A
SPECIFIC
EXAMPLE
SUPPOSE
THAT
THE
DENSITY
P
Θ
IS
GIVEN
BY
A
ZERO
MEAN
UNIT
VARIANCE
GAUSSIAN
THE
CORRESPONDING
CUMULATIVE
DISTRIBUTION
FUNCTION
IS
GIVEN
BY
A
Φ
A
N
Θ
DΘ
WHICH
IS
KNOWN
AS
THE
PROBIT
FUNCTION
IT
HAS
A
SIGMOIDAL
SHAPE
AND
IS
COMPARED
WITH
THE
LOGISTIC
SIGMOID
FUNCTION
IN
FIGURE
NOTE
THAT
THE
USE
OF
A
MORE
GEN
ERAL
GAUSSIAN
DISTRIBUTION
DOES
NOT
CHANGE
THE
MODEL
BECAUSE
THIS
IS
EQUIVALENT
TO
A
RE
SCALING
OF
THE
LINEAR
COEFFICIENTS
W
MANY
NUMERICAL
PACKAGES
PROVIDE
FOR
THE
EVALUATION
OF
A
CLOSELY
RELATED
FUNCTION
DEFINED
BY
F
A
EXERCISE
AND
KNOWN
AS
THE
ERF
FUNCTION
OR
ERROR
FUNCTION
NOT
TO
BE
CONFUSED
WITH
THE
ERROR
FUNCTION
OF
A
MACHINE
LEARNING
MODEL
IT
IS
RELATED
TO
THE
PROBIT
FUNCTION
BY
THE
GENERALIZED
LINEAR
MODEL
BASED
ON
A
PROBIT
ACTIVATION
FUNCTION
IS
KNOWN
AS
PROBIT
REGRESSION
WE
CAN
DETERMINE
THE
PARAMETERS
OF
THIS
MODEL
USING
MAXIMUM
LIKELIHOOD
BY
A
STRAIGHTFORWARD
EXTENSION
OF
THE
IDEAS
DISCUSSED
EARLIER
IN
PRACTICE
THE
RESULTS
FOUND
USING
PROBIT
REGRESSION
TEND
TO
BE
SIMILAR
TO
THOSE
OF
LOGISTIC
REGRESSION
WE
SHALL
HOWEVER
FIND
ANOTHER
USE
FOR
THE
PROBIT
MODEL
WHEN
WE
DISCUSS
BAYESIAN
TREATMENTS
OF
LOGISTIC
REGRESSION
IN
SECTION
ONE
ISSUE
THAT
CAN
OCCUR
IN
PRACTICAL
APPLICATIONS
IS
THAT
OF
OUTLIERS
WHICH
CAN
ARISE
FOR
INSTANCE
THROUGH
ERRORS
IN
MEASURING
THE
INPUT
VECTOR
X
OR
THROUGH
MISLA
BELLING
OF
THE
TARGET
VALUE
T
BECAUSE
SUCH
POINTS
CAN
LIE
A
LONG
WAY
TO
THE
WRONG
SIDE
OF
THE
IDEAL
DECISION
BOUNDARY
THEY
CAN
SERIOUSLY
DISTORT
THE
CLASSIFIER
NOTE
THAT
THE
LOGISTIC
AND
PROBIT
REGRESSION
MODELS
BEHAVE
DIFFERENTLY
IN
THIS
RESPECT
BECAUSE
THE
TAILS
OF
THE
LOGISTIC
SIGMOID
DECAY
ASYMPTOTICALLY
LIKE
EXP
X
FOR
X
WHEREAS
FOR
THE
PROBIT
ACTIVATION
FUNCTION
THEY
DECAY
LIKE
EXP
AND
SO
THE
PROBIT
MODEL
CAN
BE
SIGNIFICANTLY
MORE
SENSITIVE
TO
OUTLIERS
HOWEVER
BOTH
THE
LOGISTIC
AND
THE
PROBIT
MODELS
ASSUME
THE
DATA
IS
CORRECTLY
LABELLED
THE
EFFECT
OF
MISLABELLING
IS
EASILY
INCORPORATED
INTO
A
PROBABILISTIC
MODEL
BY
INTRODUCING
A
PROBABILITY
E
THAT
THE
TARGET
VALUE
T
HAS
BEEN
FLIPPED
TO
THE
WRONG
VALUE
OPPER
AND
WINTHER
LEADING
TO
A
TARGET
VALUE
DISTRIBUTION
FOR
DATA
POINT
X
OF
THE
FORM
P
T
X
E
Σ
X
E
Σ
X
E
Σ
X
WHERE
Σ
X
IS
THE
ACTIVATION
FUNCTION
WITH
INPUT
VECTOR
X
HERE
E
MAY
BE
SET
IN
ADVANCE
OR
IT
MAY
BE
TREATED
AS
A
HYPERPARAMETER
WHOSE
VALUE
IS
INFERRED
FROM
THE
DATA
CANONICAL
LINK
FUNCTIONS
FOR
THE
LINEAR
REGRESSION
MODEL
WITH
A
GAUSSIAN
NOISE
DISTRIBUTION
THE
ERROR
FUNCTION
CORRESPONDING
TO
THE
NEGATIVE
LOG
LIKELIHOOD
IS
GIVEN
BY
IF
WE
TAKE
THE
DERIVATIVE
WITH
RESPECT
TO
THE
PARAMETER
VECTOR
W
OF
THE
CONTRIBUTION
TO
THE
ERROR
FUNCTION
FROM
A
DATA
POINT
N
THIS
TAKES
THE
FORM
OF
THE
ERROR
YN
TN
TIMES
THE
FEATURE
VECTOR
ΦN
WHERE
YN
WTΦN
SIMILARLY
FOR
THE
COMBINATION
OF
THE
LOGISTIC
SIGMOID
ACTIVATION
FUNCTION
AND
THE
CROSS
ENTROPY
ERROR
FUNCTION
AND
FOR
THE
SOFTMAX
ACTIVATION
FUNCTION
WITH
THE
MULTICLASS
CROSS
ENTROPY
ERROR
FUNCTION
WE
AGAIN
OBTAIN
THIS
SAME
SIMPLE
FORM
WE
NOW
SHOW
THAT
THIS
IS
A
GENERAL
RESULT
OF
ASSUMING
A
CONDITIONAL
DISTRIBUTION
FOR
THE
TARGET
VARIABLE
FROM
THE
EXPONENTIAL
FAMILY
ALONG
WITH
A
CORRESPONDING
CHOICE
FOR
THE
ACTIVATION
FUNCTION
KNOWN
AS
THE
CANONICAL
LINK
FUNCTION
WE
AGAIN
MAKE
USE
OF
THE
RESTRICTED
FORM
OF
EXPONENTIAL
FAMILY
DISTRIBU
TIONS
NOTE
THAT
HERE
WE
ARE
APPLYING
THE
ASSUMPTION
OF
EXPONENTIAL
FAMILY
DISTRIBU
TION
TO
THE
TARGET
VARIABLE
T
IN
CONTRAST
TO
SECTION
WHERE
WE
APPLIED
IT
TO
THE
INPUT
VECTOR
X
WE
THEREFORE
CONSIDER
CONDITIONAL
DISTRIBUTIONS
OF
THE
TARGET
VARIABLE
OF
THE
FORM
P
T
Η
H
T
G
Η
EXP
J
ΗTL
USING
THE
SAME
LINE
OF
ARGUMENT
AS
LED
TO
THE
DERIVATION
OF
THE
RESULT
WE
SEE
THAT
THE
CONDITIONAL
MEAN
OF
T
WHICH
WE
DENOTE
BY
Y
IS
GIVEN
BY
Y
E
T
Η
DΗ
LN
G
Η
THUS
Y
AND
Η
MUST
RELATED
AND
WE
DENOTE
THIS
RELATION
THROUGH
Η
Ψ
Y
FOLLOWING
NELDER
AND
WEDDERBURN
WE
DEFINE
A
GENERALIZED
LINEAR
MODEL
TO
BE
ONE
FOR
WHICH
Y
IS
A
NONLINEAR
FUNCTION
OF
A
LINEAR
COMBINATION
OF
THE
INPUT
OR
FEATURE
VARIABLES
SO
THAT
Y
F
WTΦ
WHERE
F
IS
KNOWN
AS
THE
ACTIVATION
FUNCTION
IN
THE
MACHINE
LEARNING
LITERATURE
AND
F
IS
KNOWN
AS
THE
LINK
FUNCTION
IN
STATISTICS
NOW
CONSIDER
THE
LOG
LIKELIHOOD
FUNCTION
FOR
THIS
MODEL
WHICH
AS
A
FUNCTION
OF
Η
IS
GIVEN
BY
LN
P
T
Η
N
LN
P
TN
Η
N
JLN
G
ΗN
ΗNTN
CONST
WHERE
WE
ARE
ASSUMING
THAT
ALL
OBSERVATIONS
SHARE
A
COMMON
SCALE
PARAMETER
WHICH
CORRESPONDS
TO
THE
NOISE
VARIANCE
FOR
A
GAUSSIAN
DISTRIBUTION
FOR
INSTANCE
AND
SO
IS
INDEPENDENT
OF
N
THE
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
WITH
RESPECT
TO
THE
MODEL
PARAMETERS
W
IS
THEN
GIVEN
BY
D
T
DΗ
DY
W
LN
P
T
Η
N
N
DΗN
LN
G
ΗN
N
N
DYN
N
DAN
AN
T
YN
ΨI
YN
F
I
AN
ΦN
N
WHERE
AN
WTΦN
AND
WE
HAVE
USED
YN
F
AN
TOGETHER
WITH
THE
RESULT
FOR
E
T
Η
WE
NOW
SEE
THAT
THERE
IS
A
CONSIDERABLE
SIMPLIFICATION
IF
WE
CHOOSE
A
PARTICULAR
FORM
FOR
THE
LINK
FUNCTION
F
Y
GIVEN
BY
F
Y
Ψ
Y
WHICH
GIVES
F
Ψ
Y
Y
AND
HENCE
F
I
Ψ
ΨI
Y
ALSO
BECAUSE
A
F
Y
WE
HAVE
A
Ψ
AND
HENCE
F
I
A
ΨI
Y
IN
THIS
CASE
THE
GRADIENT
OF
THE
ERROR
FUNCTION
REDUCES
TO
N
LN
E
W
YN
N
TN
ΦN
FOR
THE
GAUSSIAN
Β
WHEREAS
FOR
THE
LOGISTIC
MODEL
THE
LAPLACE
APPROXIMATION
IN
SECTION
WE
SHALL
DISCUSS
THE
BAYESIAN
TREATMENT
OF
LOGISTIC
REGRESSION
AS
WE
SHALL
SEE
THIS
IS
MORE
COMPLEX
THAN
THE
BAYESIAN
TREATMENT
OF
LINEAR
REGRESSION
MODELS
DISCUSSED
IN
SECTIONS
AND
IN
PARTICULAR
WE
CANNOT
INTEGRATE
EXACTLY
CHAPTER
CHAPTER
OVER
THE
PARAMETER
VECTOR
W
SINCE
THE
POSTERIOR
DISTRIBUTION
IS
NO
LONGER
GAUSSIAN
IT
IS
THEREFORE
NECESSARY
TO
INTRODUCE
SOME
FORM
OF
APPROXIMATION
LATER
IN
THE
BOOK
WE
SHALL
CONSIDER
A
RANGE
OF
TECHNIQUES
BASED
ON
ANALYTICAL
APPROXIMATIONS
AND
NUMERICAL
SAMPLING
HERE
WE
INTRODUCE
A
SIMPLE
BUT
WIDELY
USED
FRAMEWORK
CALLED
THE
LAPLACE
AP
PROXIMATION
THAT
AIMS
TO
FIND
A
GAUSSIAN
APPROXIMATION
TO
A
PROBABILITY
DENSITY
DEFINED
OVER
A
SET
OF
CONTINUOUS
VARIABLES
CONSIDER
FIRST
THE
CASE
OF
A
SINGLE
CONTIN
UOUS
VARIABLE
Z
AND
SUPPOSE
THE
DISTRIBUTION
P
Z
IS
DEFINED
BY
P
Z
Z
F
Z
WHERE
Z
F
Z
DZ
IS
THE
NORMALIZATION
COEFFICIENT
WE
SHALL
SUPPOSE
THAT
THE
VALUE
OF
Z
IS
UNKNOWN
IN
THE
LAPLACE
METHOD
THE
GOAL
IS
TO
FIND
A
GAUSSIAN
APPROX
IMATION
Q
Z
WHICH
IS
CENTRED
ON
A
MODE
OF
THE
DISTRIBUTION
P
Z
THE
FIRST
STEP
IS
TO
FIND
A
MODE
OF
P
Z
IN
OTHER
WORDS
A
POINT
SUCH
THAT
PI
OR
EQUIVALENTLY
DF
Z
DZ
Z
A
GAUSSIAN
DISTRIBUTION
HAS
THE
PROPERTY
THAT
ITS
LOGARITHM
IS
A
QUADRATIC
FUNCTION
OF
THE
VARIABLES
WE
THEREFORE
CONSIDER
A
TAYLOR
EXPANSION
OF
LN
F
Z
CENTRED
ON
THE
MODE
SO
THAT
LN
F
Z
LN
F
Z
A
Z
Z
WHERE
NOTE
THAT
THE
FIRST
ORDER
TERM
IN
THE
TAYLOR
EXPANSION
DOES
NOT
APPEAR
SINCE
IS
A
LOCAL
MAXIMUM
OF
THE
DISTRIBUTION
TAKING
THE
EXPONENTIAL
WE
OBTAIN
F
Z
F
Z
EXP
A
Z
Z
WE
CAN
THEN
OBTAIN
A
NORMALIZED
DISTRIBUTION
Q
Z
BY
MAKING
USE
OF
THE
STANDARD
RESULT
FOR
THE
NORMALIZATION
OF
A
GAUSSIAN
SO
THAT
A
A
THE
LAPLACE
APPROXIMATION
IS
ILLUSTRATED
IN
FIGURE
NOTE
THAT
THE
GAUSSIAN
APPROXIMATION
WILL
ONLY
BE
WELL
DEFINED
IF
ITS
PRECISION
A
IN
OTHER
WORDS
THE
STATIONARY
POINT
MUST
BE
A
LOCAL
MAXIMUM
SO
THAT
THE
SECOND
DERIVATIVE
OF
F
Z
AT
THE
POINT
IS
NEGATIVE
FIGURE
ILLUSTRATION
OF
THE
LAPLACE
APPROXIMATION
APPLIED
TO
THE
DISTRIBUTION
P
Z
EXP
Σ
WHERE
Σ
Z
IS
THE
LOGISTIC
SIGMOID
FUNCTION
DEFINED
BY
Σ
Z
E
Z
THE
LEFT
PLOT
SHOWS
THE
NORMALIZED
DISTRIBUTION
P
Z
IN
YELLOW
TOGETHER
WITH
THE
LAPLACE
APPROXIMATION
CENTRED
ON
THE
MODE
OF
P
Z
IN
RED
THE
RIGHT
PLOT
SHOWS
THE
NEGATIVE
LOGARITHMS
OF
THE
CORRESPONDING
CURVES
WE
CAN
EXTEND
THE
LAPLACE
METHOD
TO
APPROXIMATE
A
DISTRIBUTION
P
Z
F
Z
Z
DEFINED
OVER
AN
M
DIMENSIONAL
SPACE
Z
AT
A
STATIONARY
POINT
THE
GRADIENT
F
Z
WILL
VANISH
EXPANDING
AROUND
THIS
STATIONARY
POINT
WE
HAVE
LN
F
Z
LN
F
Z
Z
Z
TA
Z
Z
WHERE
THE
M
M
HESSIAN
MATRIX
A
IS
DEFINED
BY
A
LN
F
Z
Z
AND
IS
THE
GRADIENT
OPERATOR
TAKING
THE
EXPONENTIAL
OF
BOTH
SIDES
WE
OBTAIN
F
Z
F
Z
EXP
Z
Z
TA
Z
Z
THE
DISTRIBUTION
Q
Z
IS
PROPORTIONAL
TO
F
Z
AND
THE
APPROPRIATE
NORMALIZATION
COEF
FICIENT
CAN
BE
FOUND
BY
INSPECTION
USING
THE
STANDARD
RESULT
FOR
A
NORMALIZED
MULTIVARIATE
GAUSSIAN
GIVING
A
T
WHERE
A
DENOTES
THE
DETERMINANT
OF
A
THIS
GAUSSIAN
DISTRIBUTION
WILL
BE
WELL
DEFINED
PROVIDED
ITS
PRECISION
MATRIX
GIVEN
BY
A
IS
POSITIVE
DEFINITE
WHICH
IMPLIES
THAT
THE
STATIONARY
POINT
MUST
BE
A
LOCAL
MAXIMUM
NOT
A
MINIMUM
OR
A
SADDLE
POINT
IN
ORDER
TO
APPLY
THE
LAPLACE
APPROXIMATION
WE
FIRST
NEED
TO
FIND
THE
MODE
AND
THEN
EVALUATE
THE
HESSIAN
MATRIX
AT
THAT
MODE
IN
PRACTICE
A
MODE
WILL
TYPI
CALLY
BE
FOUND
BY
RUNNING
SOME
FORM
OF
NUMERICAL
OPTIMIZATION
ALGORITHM
BISHOP
AND
NABNEY
MANY
OF
THE
DISTRIBUTIONS
ENCOUNTERED
IN
PRACTICE
WILL
BE
MUL
TIMODAL
AND
SO
THERE
WILL
BE
DIFFERENT
LAPLACE
APPROXIMATIONS
ACCORDING
TO
WHICH
MODE
IS
BEING
CONSIDERED
NOTE
THAT
THE
NORMALIZATION
CONSTANT
Z
OF
THE
TRUE
DISTRI
BUTION
DOES
NOT
NEED
TO
BE
KNOWN
IN
ORDER
TO
APPLY
THE
LAPLACE
METHOD
AS
A
RESULT
OF
THE
CENTRAL
LIMIT
THEOREM
THE
POSTERIOR
DISTRIBUTION
FOR
A
MODEL
IS
EXPECTED
TO
BECOME
INCREASINGLY
BETTER
APPROXIMATED
BY
A
GAUSSIAN
AS
THE
NUMBER
OF
OBSERVED
DATA
POINTS
IS
INCREASED
AND
SO
WE
WOULD
EXPECT
THE
LAPLACE
APPROXIMATION
TO
BE
MOST
USEFUL
IN
SITUATIONS
WHERE
THE
NUMBER
OF
DATA
POINTS
IS
RELATIVELY
LARGE
ONE
MAJOR
WEAKNESS
OF
THE
LAPLACE
APPROXIMATION
IS
THAT
SINCE
IT
IS
BASED
ON
A
GAUSSIAN
DISTRIBUTION
IT
IS
ONLY
DIRECTLY
APPLICABLE
TO
REAL
VARIABLES
IN
OTHER
CASES
IT
MAY
BE
POSSIBLE
TO
APPLY
THE
LAPLACE
APPROXIMATION
TO
A
TRANSFORMATION
OF
THE
VARIABLE
FOR
INSTANCE
IF
Τ
THEN
WE
CAN
CONSIDER
A
LAPLACE
APPROXIMATION
OF
LN
Τ
THE
MOST
SERIOUS
LIMITATION
OF
THE
LAPLACE
FRAMEWORK
HOWEVER
IS
THAT
IT
IS
BASED
PURELY
ON
THE
ASPECTS
OF
THE
TRUE
DISTRIBUTION
AT
A
SPECIFIC
VALUE
OF
THE
VARIABLE
AND
SO
CAN
FAIL
TO
CAPTURE
IMPORTANT
GLOBAL
PROPERTIES
IN
CHAPTER
WE
SHALL
CONSIDER
ALTERNATIVE
APPROACHES
WHICH
ADOPT
A
MORE
GLOBAL
PERSPECTIVE
MODEL
COMPARISON
AND
BIC
AS
WELL
AS
APPROXIMATING
THE
DISTRIBUTION
P
Z
WE
CAN
ALSO
OBTAIN
AN
APPROXI
MATION
TO
THE
NORMALIZATION
CONSTANT
Z
USING
THE
APPROXIMATION
WE
HAVE
Z
F
F
Z
DZ
F
M
A
WHERE
WE
HAVE
NOTED
THAT
THE
INTEGRAND
IS
GAUSSIAN
AND
MADE
USE
OF
THE
STANDARD
RESULT
FOR
A
NORMALIZED
GAUSSIAN
DISTRIBUTION
WE
CAN
USE
THE
RESULT
TO
OBTAIN
AN
APPROXIMATION
TO
THE
MODEL
EVIDENCE
WHICH
AS
DISCUSSED
IN
SECTION
PLAYS
A
CENTRAL
ROLE
IN
BAYESIAN
MODEL
COMPARISON
CONSIDER
A
DATA
SET
AND
A
SET
OF
MODELS
I
HAVING
PARAMETERS
ΘI
FOR
EACH
MODEL
WE
DEFINE
A
LIKELIHOOD
FUNCTION
P
ΘI
I
IF
WE
INTRODUCE
A
PRIOR
P
ΘI
I
OVER
THE
PARAMETERS
THEN
WE
ARE
INTERESTED
IN
COMPUTING
THE
MODEL
EVI
DENCE
P
I
FOR
THE
VARIOUS
MODELS
FROM
NOW
ON
WE
OMIT
THE
CONDITIONING
ON
I
TO
KEEP
THE
NOTATION
UNCLUTTERED
FROM
BAYES
THEOREM
THE
MODEL
EVIDENCE
IS
GIVEN
BY
EXERCISE
P
D
F
P
D
Θ
P
Θ
DΘ
IDENTIFYING
F
Θ
P
Θ
P
Θ
AND
Z
P
AND
APPLYING
THE
RESULT
WE
OBTAIN
LN
P
D
LN
P
D
Θ
LN
P
Θ
M
LN
LN
A
OCCAM
FACTOR
EXERCISE
WHERE
ΘMAP
IS
THE
VALUE
OF
Θ
AT
THE
MODE
OF
THE
POSTERIOR
DISTRIBUTION
AND
A
IS
THE
HESSIAN
MATRIX
OF
SECOND
DERIVATIVES
OF
THE
NEGATIVE
LOG
POSTERIOR
A
LN
P
D
ΘMAP
P
ΘMAP
LN
P
ΘMAP
D
THE
FIRST
TERM
ON
THE
RIGHT
HAND
SIDE
OF
REPRESENTS
THE
LOG
LIKELIHOOD
EVALU
ATED
USING
THE
OPTIMIZED
PARAMETERS
WHILE
THE
REMAINING
THREE
TERMS
COMPRISE
THE
OCCAM
FACTOR
WHICH
PENALIZES
MODEL
COMPLEXITY
IF
WE
ASSUME
THAT
THE
GAUSSIAN
PRIOR
DISTRIBUTION
OVER
PARAMETERS
IS
BROAD
AND
THAT
THE
HESSIAN
HAS
FULL
RANK
THEN
WE
CAN
APPROXIMATE
VERY
ROUGHLY
USING
LN
P
D
LN
P
D
Θ
MAP
M
LN
N
SECTION
WHERE
N
IS
THE
NUMBER
OF
DATA
POINTS
M
IS
THE
NUMBER
OF
PARAMETERS
IN
Θ
AND
WE
HAVE
OMITTED
ADDITIVE
CONSTANTS
THIS
IS
KNOWN
AS
THE
BAYESIAN
INFORMATION
CRITERION
BIC
OR
THE
SCHWARZ
CRITERION
SCHWARZ
NOTE
THAT
COMPARED
TO
AIC
GIVEN
BY
THIS
PENALIZES
MODEL
COMPLEXITY
MORE
HEAVILY
COMPLEXITY
MEASURES
SUCH
AS
AIC
AND
BIC
HAVE
THE
VIRTUE
OF
BEING
EASY
TO
EVALUATE
BUT
CAN
ALSO
GIVE
MISLEADING
RESULTS
IN
PARTICULAR
THE
ASSUMPTION
THAT
THE
HESSIAN
MATRIX
HAS
FULL
RANK
IS
OFTEN
NOT
VALID
SINCE
MANY
OF
THE
PARAMETERS
ARE
NOT
WELL
DETERMINED
WE
CAN
USE
THE
RESULT
TO
OBTAIN
A
MORE
ACCURATE
ESTIMATE
OF
THE
MODEL
EVIDENCE
STARTING
FROM
THE
LAPLACE
APPROXIMATION
AS
WE
ILLUSTRATE
IN
THE
CONTEXT
OF
NEURAL
NETWORKS
IN
SECTION
BAYESIAN
LOGISTIC
REGRESSION
WE
NOW
TURN
TO
A
BAYESIAN
TREATMENT
OF
LOGISTIC
REGRESSION
EXACT
BAYESIAN
INFER
ENCE
FOR
LOGISTIC
REGRESSION
IS
INTRACTABLE
IN
PARTICULAR
EVALUATION
OF
THE
POSTERIOR
DISTRIBUTION
WOULD
REQUIRE
NORMALIZATION
OF
THE
PRODUCT
OF
A
PRIOR
DISTRIBUTION
AND
A
LIKELIHOOD
FUNCTION
THAT
ITSELF
COMPRISES
A
PRODUCT
OF
LOGISTIC
SIGMOID
FUNCTIONS
ONE
FOR
EVERY
DATA
POINT
EVALUATION
OF
THE
PREDICTIVE
DISTRIBUTION
IS
SIMILARLY
INTRACTABLE
HERE
WE
CONSIDER
THE
APPLICATION
OF
THE
LAPLACE
APPROXIMATION
TO
THE
PROBLEM
OF
BAYESIAN
LOGISTIC
REGRESSION
SPIEGELHALTER
AND
LAURITZEN
MACKAY
LAPLACE
APPROXIMATION
RECALL
FROM
SECTION
THAT
THE
LAPLACE
APPROXIMATION
IS
OBTAINED
BY
FINDING
THE
MODE
OF
THE
POSTERIOR
DISTRIBUTION
AND
THEN
FITTING
A
GAUSSIAN
CENTRED
AT
THAT
MODE
THIS
REQUIRES
EVALUATION
OF
THE
SECOND
DERIVATIVES
OF
THE
LOG
POSTERIOR
WHICH
IS
EQUIVALENT
TO
FINDING
THE
HESSIAN
MATRIX
BECAUSE
WE
SEEK
A
GAUSSIAN
REPRESENTATION
FOR
THE
POSTERIOR
DISTRIBUTION
IT
IS
NATURAL
TO
BEGIN
WITH
A
GAUSSIAN
PRIOR
WHICH
WE
WRITE
IN
THE
GENERAL
FORM
P
W
N
W
WHERE
AND
ARE
FIXED
HYPERPARAMETERS
THE
POSTERIOR
DISTRIBUTION
OVER
W
IS
GIVEN
BY
P
W
T
P
W
P
T
W
WHERE
T
TN
T
TAKING
THE
LOG
OF
BOTH
SIDES
AND
SUBSTITUTING
FOR
THE
PRIOR
DISTRIBUTION
USING
AND
FOR
THE
LIKELIHOOD
FUNCTION
USING
WE
OBTAIN
LN
P
W
T
W
M
TS
W
M
N
TN
LN
YN
TN
LN
YN
CONST
N
WHERE
YN
Σ
WTΦN
TO
OBTAIN
A
GAUSSIAN
APPROXIMATION
TO
THE
POSTERIOR
DIS
TRIBUTION
WE
FIRST
MAXIMIZE
THE
POSTERIOR
DISTRIBUTION
TO
GIVE
THE
MAP
MAXIMUM
POSTERIOR
SOLUTION
WMAP
WHICH
DEFINES
THE
MEAN
OF
THE
GAUSSIAN
THE
COVARIANCE
IS
THEN
GIVEN
BY
THE
INVERSE
OF
THE
MATRIX
OF
SECOND
DERIVATIVES
OF
THE
NEGATIVE
LOG
LIKELIHOOD
WHICH
TAKES
THE
FORM
N
N
ΦT
THE
GAUSSIAN
APPROXIMATION
TO
THE
POSTERIOR
DISTRIBUTION
THEREFORE
TAKES
THE
FORM
Q
W
N
W
WMAP
SN
HAVING
OBTAINED
A
GAUSSIAN
APPROXIMATION
TO
THE
POSTERIOR
DISTRIBUTION
THERE
REMAINS
THE
TASK
OF
MARGINALIZING
WITH
RESPECT
TO
THIS
DISTRIBUTION
IN
ORDER
TO
MAKE
PREDICTIONS
PREDICTIVE
DISTRIBUTION
THE
PREDICTIVE
DISTRIBUTION
FOR
CLASS
GIVEN
A
NEW
FEATURE
VECTOR
Φ
X
IS
OBTAINED
BY
MARGINALIZING
WITH
RESPECT
TO
THE
POSTERIOR
DISTRIBUTION
P
W
T
WHICH
IS
ITSELF
APPROXIMATED
BY
A
GAUSSIAN
DISTRIBUTION
Q
W
SO
THAT
P
Φ
T
F
P
Φ
W
P
W
T
DW
F
Σ
WTΦ
Q
W
DW
WITH
THE
CORRESPONDING
PROBABILITY
FOR
CLASS
GIVEN
BY
P
Φ
T
P
Φ
T
TO
EVALUATE
THE
PREDICTIVE
DISTRIBUTION
WE
FIRST
NOTE
THAT
THE
FUNCTION
Σ
WTΦ
DE
PENDS
ON
W
ONLY
THROUGH
ITS
PROJECTION
ONTO
Φ
DENOTING
A
WTΦ
WE
HAVE
Σ
WTΦ
F
Δ
A
WTΦ
Σ
A
DA
WHERE
Δ
IS
THE
DIRAC
DELTA
FUNCTION
FROM
THIS
WE
OBTAIN
F
Σ
WTΦ
Q
W
DW
F
Σ
A
P
A
DA
WHERE
P
A
F
Δ
A
WTΦ
Q
W
DW
WE
CAN
EVALUATE
P
A
BY
NOTING
THAT
THE
DELTA
FUNCTION
IMPOSES
A
LINEAR
CONSTRAINT
ON
W
AND
SO
FORMS
A
MARGINAL
DISTRIBUTION
FROM
THE
JOINT
DISTRIBUTION
Q
W
BY
INTE
GRATING
OUT
ALL
DIRECTIONS
ORTHOGONAL
TO
Φ
BECAUSE
Q
W
IS
GAUSSIAN
WE
KNOW
FROM
SECTION
THAT
THE
MARGINAL
DISTRIBUTION
WILL
ALSO
BE
GAUSSIAN
WE
CAN
EVALUATE
THE
MEAN
AND
COVARIANCE
OF
THIS
DISTRIBUTION
BY
TAKING
MOMENTS
AND
INTERCHANGING
THE
ORDER
OF
INTEGRATION
OVER
A
AND
W
SO
THAT
ΜA
E
A
F
P
A
A
DA
F
Q
W
WTΦ
DW
WT
Φ
WHERE
WE
HAVE
USED
THE
RESULT
FOR
THE
VARIATIONAL
POSTERIOR
DISTRIBUTION
Q
W
SIMILARLY
VAR
A
F
P
A
E
A
DA
F
Q
W
WTΦ
MT
Φ
DW
ΦTSNΦ
NOTE
THAT
THE
DISTRIBUTION
OF
A
TAKES
THE
SAME
FORM
AS
THE
PREDICTIVE
DISTRIBUTION
FOR
THE
LINEAR
REGRESSION
MODEL
WITH
THE
NOISE
VARIANCE
SET
TO
ZERO
THUS
OUR
VARIATIONAL
APPROXIMATION
TO
THE
PREDICTIVE
DISTRIBUTION
BECOMES
P
T
F
Σ
A
P
A
DA
F
Σ
A
N
A
ΜA
DA
EXERCISE
EXERCISE
EXERCISE
THIS
RESULT
CAN
ALSO
BE
DERIVED
DIRECTLY
BY
MAKING
USE
OF
THE
RESULTS
FOR
THE
MARGINAL
OF
A
GAUSSIAN
DISTRIBUTION
GIVEN
IN
SECTION
THE
INTEGRAL
OVER
A
REPRESENTS
THE
CONVOLUTION
OF
A
GAUSSIAN
WITH
A
LOGISTIC
SIG
MOID
AND
CANNOT
BE
EVALUATED
ANALYTICALLY
WE
CAN
HOWEVER
OBTAIN
A
GOOD
APPROX
IMATION
SPIEGELHALTER
AND
LAURITZEN
MACKAY
BARBER
AND
BISHOP
BY
MAKING
USE
OF
THE
CLOSE
SIMILARITY
BETWEEN
THE
LOGISTIC
SIGMOID
FUNCTION
Σ
A
DEFINED
BY
AND
THE
PROBIT
FUNCTION
Φ
A
DEFINED
BY
IN
ORDER
TO
OBTAIN
THE
BEST
APPROXIMATION
TO
THE
LOGISTIC
FUNCTION
WE
NEED
TO
RE
SCALE
THE
HORI
ZONTAL
AXIS
SO
THAT
WE
APPROXIMATE
Σ
A
BY
Φ
ΛA
WE
CAN
FIND
A
SUITABLE
VALUE
OF
Λ
BY
REQUIRING
THAT
THE
TWO
FUNCTIONS
HAVE
THE
SAME
SLOPE
AT
THE
ORIGIN
WHICH
GIVES
Π
THE
SIMILARITY
OF
THE
LOGISTIC
SIGMOID
AND
THE
PROBIT
FUNCTION
FOR
THIS
CHOICE
OF
Λ
IS
ILLUSTRATED
IN
FIGURE
THE
ADVANTAGE
OF
USING
A
PROBIT
FUNCTION
IS
THAT
ITS
CONVOLUTION
WITH
A
GAUSSIAN
CAN
BE
EXPRESSED
ANALYTICALLY
IN
TERMS
OF
ANOTHER
PROBIT
FUNCTION
SPECIFICALLY
WE
CAN
SHOW
THAT
F
Φ
ΛA
N
A
Μ
DA
Φ
Μ
WE
NOW
APPLY
THE
APPROXIMATION
Σ
A
Φ
ΛA
TO
THE
PROBIT
FUNCTIONS
APPEARING
ON
BOTH
SIDES
OF
THIS
EQUATION
LEADING
TO
THE
FOLLOWING
APPROXIMATION
FOR
THE
CONVO
LUTION
OF
A
LOGISTIC
SIGMOID
WITH
A
GAUSSIAN
F
Σ
A
N
A
Μ
DA
Σ
Κ
Μ
WHERE
WE
HAVE
DEFINED
Κ
APPLYING
THIS
RESULT
TO
WE
OBTAIN
THE
APPROXIMATE
PREDICTIVE
DISTRIBUTION
IN
THE
FORM
A
WHERE
ΜA
AND
ARE
DEFINED
BY
AND
RESPECTIVELY
AND
Κ
IS
DE
A
A
FINED
BY
NOTE
THAT
THE
DECISION
BOUNDARY
CORRESPONDING
TO
P
Φ
T
IS
GIVEN
BY
ΜA
WHICH
IS
THE
SAME
AS
THE
DECISION
BOUNDARY
OBTAINED
BY
USING
THE
MAP
VALUE
FOR
W
THUS
IF
THE
DECISION
CRITERION
IS
BASED
ON
MINIMIZING
MISCLASSIFICA
TION
RATE
WITH
EQUAL
PRIOR
PROBABILITIES
THEN
THE
MARGINALIZATION
OVER
W
HAS
NO
EF
FECT
HOWEVER
FOR
MORE
COMPLEX
DECISION
CRITERIA
IT
WILL
PLAY
AN
IMPORTANT
ROLE
MARGINALIZATION
OF
THE
LOGISTIC
SIGMOID
MODEL
UNDER
A
GAUSSIAN
APPROXIMATION
TO
THE
POSTERIOR
DISTRIBUTION
WILL
BE
ILLUSTRATED
IN
THE
CONTEXT
OF
VARIATIONAL
INFERENCE
IN
FIGURE
EXERCISES
GIVEN
A
SET
OF
DATA
POINTS
XN
WE
CAN
DEFINE
THE
CONVEX
HULL
TO
BE
THE
SET
OF
ALL
POINTS
X
GIVEN
BY
X
ΑNXN
N
WHERE
ΑN
AND
N
ΑN
CONSIDER
A
SECOND
SET
OF
POINTS
YN
TOGETHER
WITH
THEIR
CORRESPONDING
CONVEX
HULL
BY
DEFINITION
THE
TWO
SETS
OF
POINTS
WILL
BE
LINEARLY
SEPARABLE
IF
THERE
EXISTS
A
VECTOR
W
AND
A
SCALAR
W
SUCH
THAT
WTX
W
FOR
ALL
X
AND
WTY
FOR
ALL
Y
N
N
N
N
SHOW
THAT
IF
THEIR
CONVEX
HULLS
INTERSECT
THE
TWO
SETS
OF
POINTS
CANNOT
BE
LINEARLY
SEPARABLE
AND
CONVERSELY
THAT
IF
THEY
ARE
LINEARLY
SEPARABLE
THEIR
CONVEX
HULLS
DO
NOT
INTERSECT
WWW
CONSIDER
THE
MINIMIZATION
OF
A
SUM
OF
SQUARES
ERROR
FUNCTION
AND
SUPPOSE
THAT
ALL
OF
THE
TARGET
VECTORS
IN
THE
TRAINING
SET
SATISFY
A
LINEAR
CONSTRAINT
ATTN
B
WHERE
TN
CORRESPONDS
TO
THE
NTH
ROW
OF
THE
MATRIX
T
IN
SHOW
THAT
AS
A
CONSEQUENCE
OF
THIS
CONSTRAINT
THE
ELEMENTS
OF
THE
MODEL
PREDICTION
Y
X
GIVEN
BY
THE
LEAST
SQUARES
SOLUTION
ALSO
SATISFY
THIS
CONSTRAINT
SO
THAT
ATY
X
B
TO
DO
SO
ASSUME
THAT
ONE
OF
THE
BASIS
FUNCTIONS
X
SO
THAT
THE
CORRESPONDING
PARAMETER
PLAYS
THE
ROLE
OF
A
BIAS
EXTEND
THE
RESULT
OF
EXERCISE
TO
SHOW
THAT
IF
MULTIPLE
LINEAR
CONSTRAINTS
ARE
SATISFIED
SIMULTANEOUSLY
BY
THE
TARGET
VECTORS
THEN
THE
SAME
CONSTRAINTS
WILL
ALSO
BE
SATISFIED
BY
THE
LEAST
SQUARES
PREDICTION
OF
A
LINEAR
MODEL
WWW
SHOW
THAT
MAXIMIZATION
OF
THE
CLASS
SEPARATION
CRITERION
GIVEN
BY
WITH
RESPECT
TO
W
USING
A
LAGRANGE
MULTIPLIER
TO
ENFORCE
THE
CONSTRAINT
WTW
LEADS
TO
THE
RESULT
THAT
W
BY
MAKING
USE
OF
AND
SHOW
THAT
THE
FISHER
CRITERION
CAN
BE
WRITTEN
IN
THE
FORM
USING
THE
DEFINITIONS
OF
THE
BETWEEN
CLASS
AND
WITHIN
CLASS
COVARIANCE
MATRICES
GIVEN
BY
AND
RESPECTIVELY
TOGETHER
WITH
AND
AND
THE
CHOICE
OF
TARGET
VALUES
DESCRIBED
IN
SECTION
SHOW
THAT
THE
EXPRESSION
THAT
MINIMIZES
THE
SUM
OF
SQUARES
ERROR
FUNCTION
CAN
BE
WRITTEN
IN
THE
FORM
WWW
SHOW
THAT
THE
LOGISTIC
SIGMOID
FUNCTION
SATISFIES
THE
PROPERTY
Σ
A
Σ
A
AND
THAT
ITS
INVERSE
IS
GIVEN
BY
Σ
Y
LN
Y
Y
USING
AND
DERIVE
THE
RESULT
FOR
THE
POSTERIOR
CLASS
PROBABILITY
IN
THE
TWO
CLASS
GENERATIVE
MODEL
WITH
GAUSSIAN
DENSITIES
AND
VERIFY
THE
RESULTS
AND
FOR
THE
PARAMETERS
W
AND
WWW
CONSIDER
A
GENERATIVE
CLASSIFICATION
MODEL
FOR
K
CLASSES
DEFINED
BY
PRIOR
CLASS
PROBABILITIES
P
K
ΠK
AND
GENERAL
CLASS
CONDITIONAL
DENSITIES
P
Φ
K
WHERE
Φ
IS
THE
INPUT
FEATURE
VECTOR
SUPPOSE
WE
ARE
GIVEN
A
TRAINING
DATA
SET
ΦN
TN
WHERE
N
N
AND
TN
IS
A
BINARY
TARGET
VECTOR
OF
LENGTH
K
THAT
USES
THE
OF
K
CODING
SCHEME
SO
THAT
IT
HAS
COMPONENTS
TNJ
IJK
IF
PATTERN
N
IS
FROM
CLASS
K
ASSUMING
THAT
THE
DATA
POINTS
ARE
DRAWN
INDEPENDENTLY
FROM
THIS
MODEL
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
PRIOR
PROBABILITIES
IS
GIVEN
BY
Π
NK
K
N
WHERE
NK
IS
THE
NUMBER
OF
DATA
POINTS
ASSIGNED
TO
CLASS
CK
CONSIDER
THE
CLASSIFICATION
MODEL
OF
EXERCISE
AND
NOW
SUPPOSE
THAT
THE
CLASS
CONDITIONAL
DENSITIES
ARE
GIVEN
BY
GAUSSIAN
DISTRIBUTIONS
WITH
A
SHARED
COVARI
ANCE
MATRIX
SO
THAT
P
Φ
CK
N
Φ
ΜK
Σ
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
MEAN
OF
THE
GAUSSIAN
DISTRIBUTION
FOR
CLASS
CK
IS
GIVEN
BY
N
ΜK
TNKΦN
NK
N
WHICH
REPRESENTS
THE
MEAN
OF
THOSE
FEATURE
VECTORS
ASSIGNED
TO
CLASS
K
SIMILARLY
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
SHARED
COVARIANCE
MATRIX
IS
GIVEN
BY
K
Σ
NK
N
K
WHERE
N
SK
TNK
NK
N
K
ΦN
ΜK
ΦN
ΜK
T
THUS
Σ
IS
GIVEN
BY
A
WEIGHTED
AVERAGE
OF
THE
COVARIANCES
OF
THE
DATA
ASSOCIATED
WITH
EACH
CLASS
IN
WHICH
THE
WEIGHTING
COEFFICIENTS
ARE
GIVEN
BY
THE
PRIOR
PROBABILITIES
OF
THE
CLASSES
CONSIDER
A
CLASSIFICATION
PROBLEM
WITH
K
CLASSES
FOR
WHICH
THE
FEATURE
VECTOR
Φ
HAS
M
COMPONENTS
EACH
OF
WHICH
CAN
TAKE
L
DISCRETE
STATES
LET
THE
VALUES
OF
THE
COMPONENTS
BE
REPRESENTED
BY
A
OF
L
BINARY
CODING
SCHEME
FURTHER
SUPPOSE
THAT
CONDITIONED
ON
THE
CLASS
K
THE
M
COMPONENTS
OF
Φ
ARE
INDEPENDENT
SO
THAT
THE
CLASS
CONDITIONAL
DENSITY
FACTORIZES
WITH
RESPECT
TO
THE
FEATURE
VECTOR
COMPONENTS
SHOW
THAT
THE
QUANTITIES
AK
GIVEN
BY
WHICH
APPEAR
IN
THE
ARGUMENT
TO
THE
SOFTMAX
FUNCTION
DESCRIBING
THE
POSTERIOR
CLASS
PROBABILITIES
ARE
LINEAR
FUNCTIONS
OF
THE
COMPONENTS
OF
Φ
NOTE
THAT
THIS
REPRESENTS
AN
EXAMPLE
OF
THE
NAIVE
BAYES
MODEL
WHICH
IS
DISCUSSED
IN
SECTION
INTRODUCTION
WHAT
IS
COMPUTER
VISION
A
BRIEF
HISTORY
BOOK
OVERVIEW
SAMPLE
SYLLABUS
A
NOTE
ON
NOTATION
ADDITIONAL
READING
FIGURE
THE
HUMAN
VISUAL
SYSTEM
HAS
NO
PROBLEM
INTERPRETING
THE
SUBTLE
VARIATIONS
IN
TRANSLUCENCY
AND
SHADING
IN
THIS
PHOTOGRAPH
AND
CORRECTLY
SEGMENTING
THE
OBJECT
FROM
ITS
BACKGROUND
A
B
C
D
FIGURE
SOME
EXAMPLES
OF
COMPUTER
VISION
ALGORITHMS
AND
APPLICATIONS
A
STRUCTURE
FROM
MOTION
ALGORITHMS
CAN
RECONSTRUCT
A
SPARSE
POINT
MODEL
OF
A
LARGE
COMPLEX
SCENE
FROM
HUNDREDS
OF
PARTIALLY
OVERLAPPING
PHOTOGRAPHS
SNAVELY
SEITZ
AND
SZELISKI
QC
ACM
B
STEREO
MATCHING
ALGORITHMS
CAN
BUILD
A
DETAILED
MODEL
OF
A
BUILDING
FAC
ADE
FROM
HUNDREDS
OF
DIFFERENTLY
EXPOSED
PHOTOGRAPHS
TAKEN
FROM
THE
INTERNET
GOESELE
SNAVELY
CURLESS
ET
AL
QC
IEEE
C
PERSON
TRACKING
ALGORITHMS
CAN
TRACK
A
PERSON
WALKING
IN
FRONT
OF
A
CLUTTERED
BACKGROUND
SIDENBLADH
BLACK
AND
FLEET
QC
SPRINGER
D
FACE
DETECTION
ALGORITHMS
COUPLED
WITH
COLOR
BASED
CLOTHING
AND
HAIR
DETECTION
ALGORITHMS
CAN
LOCATE
AND
RECOGNIZE
THE
INDIVIDUALS
IN
THIS
IMAGE
SIVIC
ZITNICK
AND
SZELISKI
QC
SPRINGER
WHAT
IS
COMPUTER
VISION
AS
HUMANS
WE
PERCEIVE
THE
THREE
DIMENSIONAL
STRUCTURE
OF
THE
WORLD
AROUND
US
WITH
APPARENT
EASE
THINK
OF
HOW
VIVID
THE
THREE
DIMENSIONAL
PERCEPT
IS
WHEN
YOU
LOOK
AT
A
VASE
OF
FLOWERS
SITTING
ON
THE
TABLE
NEXT
TO
YOU
YOU
CAN
TELL
THE
SHAPE
AND
TRANSLUCENCY
OF
EACH
PETAL
THROUGH
THE
SUBTLE
PATTERNS
OF
LIGHT
AND
SHADING
THAT
PLAY
ACROSS
ITS
SURFACE
AND
EFFORTLESSLY
SEGMENT
EACH
FLOWER
FROM
THE
BACKGROUND
OF
THE
SCENE
FIGURE
LOOKING
AT
A
FRAMED
GROUP
POR
TRAIT
YOU
CAN
EASILY
COUNT
AND
NAME
ALL
OF
THE
PEOPLE
IN
THE
PICTURE
AND
EVEN
GUESS
AT
THEIR
EMOTIONS
FROM
THEIR
FACIAL
APPEARANCE
PERCEPTUAL
PSYCHOLOGISTS
HAVE
SPENT
DECADES
TRYING
TO
UNDERSTAND
HOW
THE
VISUAL
SYSTEM
WORKS
AND
EVEN
THOUGH
THEY
CAN
DEVISE
OPTICAL
TO
TEASE
APART
SOME
OF
ITS
PRINCIPLES
FIGURE
A
COMPLETE
SOLUTION
TO
THIS
PUZZLE
REMAINS
ELUSIVE
MARR
PALMER
LIVINGSTONE
RESEARCHERS
IN
COMPUTER
VISION
HAVE
BEEN
DEVELOPING
IN
PARALLEL
MATHEMATICAL
TECH
NIQUES
FOR
RECOVERING
THE
THREE
DIMENSIONAL
SHAPE
AND
APPEARANCE
OF
OBJECTS
IN
IMAGERY
WE
NOW
HAVE
RELIABLE
TECHNIQUES
FOR
ACCURATELY
COMPUTING
A
PARTIAL
MODEL
OF
AN
ENVIRONMENT
FROM
THOUSANDS
OF
PARTIALLY
OVERLAPPING
PHOTOGRAPHS
FIGURE
GIVEN
A
LARGE
ENOUGH
SET
OF
VIEWS
OF
A
PARTICULAR
OBJECT
OR
FAC
ADE
WE
CAN
CREATE
ACCURATE
DENSE
SURFACE
MOD
ELS
USING
STEREO
MATCHING
FIGURE
WE
CAN
TRACK
A
PERSON
MOVING
AGAINST
A
COMPLEX
BACKGROUND
FIGURE
WE
CAN
EVEN
WITH
MODERATE
SUCCESS
ATTEMPT
TO
FIND
AND
NAME
ALL
OF
THE
PEOPLE
IN
A
PHOTOGRAPH
USING
A
COMBINATION
OF
FACE
CLOTHING
AND
HAIR
DETECTION
AND
RECOGNITION
FIGURE
HOWEVER
DESPITE
ALL
OF
THESE
ADVANCES
THE
DREAM
OF
HAVING
A
COMPUTER
INTERPRET
AN
IMAGE
AT
THE
SAME
LEVEL
AS
A
TWO
YEAR
OLD
FOR
EXAMPLE
COUNTING
ALL
OF
THE
ANIMALS
IN
A
PICTURE
REMAINS
ELUSIVE
WHY
IS
VISION
SO
DIFFICULT
IN
PART
IT
IS
BECAUSE
VISION
IS
AN
INVERSE
PROBLEM
IN
WHICH
WE
SEEK
TO
RECOVER
SOME
UNKNOWNS
GIVEN
INSUFFICIENT
INFORMATION
TO
FULLY
SPECIFY
THE
SOLUTION
WE
MUST
THEREFORE
RESORT
TO
PHYSICS
BASED
AND
PROB
ABILISTIC
MODELS
TO
DISAMBIGUATE
BETWEEN
POTENTIAL
SOLUTIONS
HOWEVER
MODELING
THE
VISUAL
WORLD
IN
ALL
OF
ITS
RICH
COMPLEXITY
IS
FAR
MORE
DIFFICULT
THAN
SAY
MODELING
THE
VOCAL
TRACT
THAT
PRODUCES
SPOKEN
SOUNDS
THE
FORWARD
MODELS
THAT
WE
USE
IN
COMPUTER
VISION
ARE
USUALLY
DEVELOPED
IN
PHYSICS
RA
DIOMETRY
OPTICS
AND
SENSOR
DESIGN
AND
IN
COMPUTER
GRAPHICS
BOTH
OF
THESE
FIELDS
MODEL
HOW
OBJECTS
MOVE
AND
ANIMATE
HOW
LIGHT
REFLECTS
OFF
THEIR
SURFACES
IS
SCATTERED
BY
THE
AT
MOSPHERE
REFRACTED
THROUGH
CAMERA
LENSES
OR
HUMAN
EYES
AND
FINALLY
PROJECTED
ONTO
A
FLAT
OR
CURVED
IMAGE
PLANE
WHILE
COMPUTER
GRAPHICS
ARE
NOT
YET
PERFECT
NO
FULLY
COMPUTER
ANIMATED
MOVIE
WITH
HUMAN
CHARACTERS
HAS
YET
SUCCEEDED
AT
CROSSING
THE
UNCANNY
THAT
SEPARATES
REAL
HUMANS
FROM
ANDROID
ROBOTS
AND
COMPUTER
ANIMATED
HUMANS
IN
LIMITED
HTTP
WWW
MICHAELBACH
DE
OT
SZE
MUELUE
THE
TERM
UNCANNY
VALLEY
WAS
ORIGINALLY
COINED
BY
ROBOTICIST
MASAHIRO
MORI
AS
APPLIED
TO
ROBOTICS
MORI
IT
IS
ALSO
COMMONLY
APPLIED
TO
COMPUTER
ANIMATED
FILMS
SUCH
AS
FINAL
FANTASY
AND
POLAR
EXPRESS
GELLER
A
B
X
X
X
X
X
X
X
O
X
O
X
O
X
X
X
X
X
X
X
X
X
X
O
X
X
X
O
X
X
X
X
X
X
X
X
O
X
X
O
X
X
O
X
X
X
X
X
X
X
X
X
O
X
O
O
X
X
X
X
X
X
X
X
O
X
X
O
X
X
X
X
X
X
X
X
X
X
X
O
X
X
X
O
X
X
X
X
X
X
X
X
O
X
X
O
X
X
O
X
X
X
X
X
X
X
X
O
X
X
X
O
X
X
X
X
X
X
X
X
X
X
X
O
O
X
X
X
X
X
X
X
X
X
X
O
X
X
X
O
X
C
D
FIGURE
SOME
COMMON
OPTICAL
ILLUSIONS
AND
WHAT
THEY
MIGHT
TELL
US
ABOUT
THE
VISUAL
SYS
TEM
A
THE
CLASSIC
MU
LLER
LYER
ILLUSION
WHERE
THE
LENGTH
OF
THE
TWO
HORIZONTAL
LINES
APPEAR
DIFFERENT
PROBABLY
DUE
TO
THE
IMAGINED
PERSPECTIVE
EFFECTS
B
THE
WHITE
SQUARE
B
IN
THE
SHADOW
AND
THE
BLACK
SQUARE
A
IN
THE
LIGHT
ACTUALLY
HAVE
THE
SAME
ABSOLUTE
INTENSITY
VALUE
THE
PERCEPT
IS
DUE
TO
BRIGHTNESS
CONSTANCY
THE
VISUAL
SYSTEM
ATTEMPT
TO
DISCOUNT
ILLUMI
NATION
WHEN
INTERPRETING
COLORS
IMAGE
COURTESY
OF
TED
ADELSON
HTTP
WEB
MIT
EDU
PERSCI
PEOPLE
ADELSON
CHECKERSHADOW
ILLUSION
HTML
C
A
VARIATION
OF
THE
HERMANN
GRID
ILLUSION
COURTESY
OF
HANY
FARID
HTTP
WWW
CS
DARTMOUTH
EDU
FARID
ILLUSIONS
HERMANN
HTML
AS
YOU
MOVE
YOUR
EYES
OVER
THE
FIGURE
GRAY
SPOTS
APPEAR
AT
THE
INTERSECTIONS
D
COUNT
THE
RED
XS
IN
THE
LEFT
HALF
OF
THE
FIGURE
NOW
COUNT
THEM
IN
THE
RIGHT
HALF
IS
IT
SIGNIFICANTLY
HARDER
THE
EXPLANATION
HAS
TO
DO
WITH
A
POP
OUT
EFFECT
TREISMAN
WHICH
TELLS
US
ABOUT
THE
OPERATIONS
OF
PARALLEL
PERCEPTION
AND
INTEGRATION
PATHWAYS
IN
THE
BRAIN
DOMAINS
SUCH
AS
RENDERING
A
STILL
SCENE
COMPOSED
OF
EVERYDAY
OBJECTS
OR
ANIMATING
EXTINCT
CREATURES
SUCH
AS
DINOSAURS
THE
ILLUSION
OF
REALITY
IS
PERFECT
IN
COMPUTER
VISION
WE
ARE
TRYING
TO
DO
THE
INVERSE
I
E
TO
DESCRIBE
THE
WORLD
THAT
WE
SEE
IN
ONE
OR
MORE
IMAGES
AND
TO
RECONSTRUCT
ITS
PROPERTIES
SUCH
AS
SHAPE
ILLUMINATION
AND
COLOR
DISTRIBUTIONS
IT
IS
AMAZING
THAT
HUMANS
AND
ANIMALS
DO
THIS
SO
EFFORTLESSLY
WHILE
COMPUTER
VISION
ALGORITHMS
ARE
SO
ERROR
PRONE
PEOPLE
WHO
HAVE
NOT
WORKED
IN
THE
FIELD
OFTEN
UNDER
ESTIMATE
THE
DIFFICULTY
OF
THE
PROBLEM
COLLEAGUES
AT
WORK
OFTEN
ASK
ME
FOR
SOFTWARE
TO
FIND
AND
NAME
ALL
THE
PEOPLE
IN
PHOTOS
SO
THEY
CAN
GET
ON
WITH
THE
MORE
INTERESTING
WORK
THIS
MISPERCEPTION
THAT
VISION
SHOULD
BE
EASY
DATES
BACK
TO
THE
EARLY
DAYS
OF
ARTIFICIAL
INTELLIGENCE
SEE
SECTION
WHEN
IT
WAS
INITIALLY
BELIEVED
THAT
THE
COGNITIVE
LOGIC
PROVING
AND
PLAN
NING
PARTS
OF
INTELLIGENCE
WERE
INTRINSICALLY
MORE
DIFFICULT
THAN
THE
PERCEPTUAL
COMPONENTS
BODEN
THE
GOOD
NEWS
IS
THAT
COMPUTER
VISION
IS
BEING
USED
TODAY
IN
A
WIDE
VARIETY
OF
REAL
WORLD
APPLICATIONS
WHICH
INCLUDE
OPTICAL
CHARACTER
RECOGNITION
OCR
READING
HANDWRITTEN
POSTAL
CODES
ON
LETTERS
FIGURE
AND
AUTOMATIC
NUMBER
PLATE
RECOGNITION
ANPR
MACHINE
INSPECTION
RAPID
PARTS
INSPECTION
FOR
QUALITY
ASSURANCE
USING
STEREO
VISION
WITH
SPECIALIZED
ILLUMINATION
TO
MEASURE
TOLERANCES
ON
AIRCRAFT
WINGS
OR
AUTO
BODY
PARTS
FIGURE
OR
LOOKING
FOR
DEFECTS
IN
STEEL
CASTINGS
USING
X
RAY
VISION
RETAIL
OBJECT
RECOGNITION
FOR
AUTOMATED
CHECKOUT
LANES
FIGURE
MODEL
BUILDING
PHOTOGRAMMETRY
FULLY
AUTOMATED
CONSTRUCTION
OF
MODELS
FROM
AERIAL
PHOTOGRAPHS
USED
IN
SYSTEMS
SUCH
AS
BING
MAPS
MEDICAL
IMAGING
REGISTERING
PRE
OPERATIVE
AND
INTRA
OPERATIVE
IMAGERY
FIGURE
OR
PERFORMING
LONG
TERM
STUDIES
OF
PEOPLE
BRAIN
MORPHOLOGY
AS
THEY
AGE
AUTOMOTIVE
SAFETY
DETECTING
UNEXPECTED
OBSTACLES
SUCH
AS
PEDESTRIANS
ON
THE
STREET
UNDER
CONDITIONS
WHERE
ACTIVE
VISION
TECHNIQUES
SUCH
AS
RADAR
OR
LIDAR
DO
NOT
WORK
WELL
FIGURE
SEE
ALSO
MILLER
CAMPBELL
HUTTENLOCHER
ET
AL
MONTEMERLO
BECKER
BHAT
ET
AL
URMSON
ANHALT
BAGNELL
ET
AL
FOR
EXAMPLES
OF
FULLY
AUTOMATED
DRIVING
MATCH
MOVE
MERGING
COMPUTER
GENERATED
IMAGERY
CGI
WITH
LIVE
ACTION
FOOTAGE
BY
TRACKING
FEATURE
POINTS
IN
THE
SOURCE
VIDEO
TO
ESTIMATE
THE
CAMERA
MOTION
AND
SHAPE
OF
THE
ENVIRONMENT
SUCH
TECHNIQUES
ARE
WIDELY
USED
IN
HOLLYWOOD
E
G
IN
MOVIES
SUCH
AS
JURASSIC
PARK
ROBLE
ROBLE
AND
ZAFAR
THEY
ALSO
REQUIRE
THE
USE
OF
PRECISE
MATTING
TO
INSERT
NEW
ELEMENTS
BETWEEN
FOREGROUND
AND
BACKGROUND
ELEMENTS
CHUANG
AGARWALA
CURLESS
ET
AL
A
B
C
D
E
F
FIGURE
SOME
INDUSTRIAL
APPLICATIONS
OF
COMPUTER
VISION
A
OPTICAL
CHARACTER
RECOGNITION
OCR
HTTP
YANN
LECUN
COM
EXDB
LENET
B
MECHANICAL
INSPECTION
HTTP
WWW
COGNITENS
COM
C
RETAIL
HTTP
WWW
EVORETAIL
COM
D
MEDICAL
IMAGING
HTTP
WWW
CLARONTECH
COM
E
AUTOMOTIVE
SAFETY
HTTP
WWW
MOBILEYE
COM
F
SURVEILLANCE
AND
TRAFFIC
MONITORING
HTTP
WWW
HONEYWELLVIDEO
COM
COURTESY
OF
HONEYWELL
INTERNATIONAL
INC
MOTION
CAPTURE
MOCAP
USING
RETRO
REFLECTIVE
MARKERS
VIEWED
FROM
MULTIPLE
CAM
ERAS
OR
OTHER
VISION
BASED
TECHNIQUES
TO
CAPTURE
ACTORS
FOR
COMPUTER
ANIMATION
SURVEILLANCE
MONITORING
FOR
INTRUDERS
ANALYZING
HIGHWAY
TRAFFIC
FIGURE
AND
MONITORING
POOLS
FOR
DROWNING
VICTIMS
FINGERPRINT
RECOGNITION
AND
BIOMETRICS
FOR
AUTOMATIC
ACCESS
AUTHENTICATION
AS
WELL
AS
FORENSIC
APPLICATIONS
DAVID
LOWE
WEB
SITE
OF
INDUSTRIAL
VISION
APPLICATIONS
HTTP
WWW
CS
UBC
CA
SPIDER
LOWE
VISION
HTML
LISTS
MANY
OTHER
INTERESTING
INDUSTRIAL
APPLICATIONS
OF
COMPUTER
VISION
WHILE
THE
ABOVE
APPLICATIONS
ARE
ALL
EXTREMELY
IMPORTANT
THEY
MOSTLY
PERTAIN
TO
FAIRLY
SPECIALIZED
KINDS
OF
IMAGERY
AND
NARROW
DOMAINS
IN
THIS
BOOK
WE
FOCUS
MORE
ON
BROADER
CONSUMER
LEVEL
APPLICATIONS
SUCH
AS
FUN
THINGS
YOU
CAN
DO
WITH
YOUR
OWN
PERSONAL
PHOTOGRAPHS
AND
VIDEO
THESE
INCLUDE
STITCHING
TURNING
OVERLAPPING
PHOTOS
INTO
A
SINGLE
SEAMLESSLY
STITCHED
PANORAMA
FIG
URE
AS
DESCRIBED
IN
CHAPTER
EXPOSURE
BRACKETING
MERGING
MULTIPLE
EXPOSURES
TAKEN
UNDER
CHALLENGING
LIGHTING
CONDITIONS
STRONG
SUNLIGHT
AND
SHADOWS
INTO
A
SINGLE
PERFECTLY
EXPOSED
IMAGE
FIG
URE
AS
DESCRIBED
IN
SECTION
MORPHING
TURNING
A
PICTURE
OF
ONE
OF
YOUR
FRIENDS
INTO
ANOTHER
USING
A
SEAMLESS
MORPH
TRANSITION
FIGURE
MODELING
CONVERTING
ONE
OR
MORE
SNAPSHOTS
INTO
A
MODEL
OF
THE
OBJECT
OR
PERSON
YOU
ARE
PHOTOGRAPHING
FIGURE
AS
DESCRIBED
IN
SECTION
VIDEO
MATCH
MOVE
AND
STABILIZATION
INSERTING
PICTURES
OR
MODELS
INTO
YOUR
VIDEOS
BY
AUTOMATICALLY
TRACKING
NEARBY
REFERENCE
POINTS
SEE
SECTION
OR
USING
MOTION
ESTIMATES
TO
REMOVE
SHAKE
FROM
YOUR
VIDEOS
SEE
SECTION
PHOTO
BASED
WALKTHROUGHS
NAVIGATING
A
LARGE
COLLECTION
OF
PHOTOGRAPHS
SUCH
AS
THE
INTERIOR
OF
YOUR
HOUSE
BY
FLYING
BETWEEN
DIFFERENT
PHOTOS
IN
SEE
SECTIONS
AND
FACE
DETECTION
FOR
IMPROVED
CAMERA
FOCUSING
AS
WELL
AS
MORE
RELEVANT
IMAGE
SEARCH
ING
SEE
SECTION
VISUAL
AUTHENTICATION
AUTOMATICALLY
LOGGING
FAMILY
MEMBERS
ONTO
YOUR
HOME
COM
PUTER
AS
THEY
SIT
DOWN
IN
FRONT
OF
THE
WEBCAM
SEE
SECTION
A
B
C
D
FIGURE
SOME
CONSUMER
APPLICATIONS
OF
COMPUTER
VISION
A
IMAGE
STITCHING
MERGING
DIFFERENT
VIEWS
SZELISKI
AND
SHUM
QC
ACM
B
EXPOSURE
BRACKETING
MERGING
DIFFERENT
EXPOSURES
C
MORPHING
BLENDING
BETWEEN
TWO
PHOTOGRAPHS
GOMES
DARSA
COSTA
ET
AL
QC
MORGAN
KAUFMANN
D
TURNING
A
COLLECTION
OF
PHOTOGRAPHS
INTO
A
MODEL
SINHA
STEEDLY
SZELISKI
ET
AL
QC
ACM
THE
GREAT
THING
ABOUT
THESE
APPLICATIONS
IS
THAT
THEY
ARE
ALREADY
FAMILIAR
TO
MOST
STUDENTS
THEY
ARE
AT
LEAST
TECHNOLOGIES
THAT
STUDENTS
CAN
IMMEDIATELY
APPRECIATE
AND
USE
WITH
THEIR
OWN
PERSONAL
MEDIA
SINCE
COMPUTER
VISION
IS
A
CHALLENGING
TOPIC
GIVEN
THE
WIDE
RANGE
OF
MATHEMATICS
BEING
AND
THE
INTRINSICALLY
DIFFICULT
NATURE
OF
THE
PROBLEMS
BEING
SOLVED
HAVING
FUN
AND
RELEVANT
PROBLEMS
TO
WORK
ON
CAN
BE
HIGHLY
MOTIVATING
AND
INSPIRING
THE
OTHER
MAJOR
REASON
WHY
THIS
BOOK
HAS
A
STRONG
FOCUS
ON
APPLICATIONS
IS
THAT
THEY
CAN
BE
USED
TO
FORMULATE
AND
CONSTRAIN
THE
POTENTIALLY
OPEN
ENDED
PROBLEMS
ENDEMIC
IN
VISION
FOR
EXAMPLE
IF
SOMEONE
COMES
TO
ME
AND
ASKS
FOR
A
GOOD
EDGE
DETECTOR
MY
FIRST
QUESTION
IS
USUALLY
TO
ASK
WHY
WHAT
KIND
OF
PROBLEM
ARE
THEY
TRYING
TO
SOLVE
AND
WHY
DO
THEY
BELIEVE
THAT
EDGE
DETECTION
IS
AN
IMPORTANT
COMPONENT
IF
THEY
ARE
TRYING
TO
LOCATE
FACES
I
USUALLY
POINT
OUT
THAT
MOST
SUCCESSFUL
FACE
DETECTORS
USE
A
COMBINATION
OF
SKIN
COLOR
DETECTION
EXER
CISE
AND
SIMPLE
BLOB
FEATURES
SECTION
THEY
DO
NOT
RELY
ON
EDGE
DETECTION
IF
THEY
ARE
TRYING
TO
MATCH
DOOR
AND
WINDOW
EDGES
IN
A
BUILDING
FOR
THE
PURPOSE
OF
RECONSTRUCTION
I
TELL
THEM
THAT
EDGES
ARE
A
FINE
IDEA
BUT
IT
IS
BETTER
TO
TUNE
THE
EDGE
DETECTOR
FOR
LONG
EDGES
SEE
SECTIONS
AND
AND
LINK
THEM
TOGETHER
INTO
STRAIGHT
LINES
WITH
COMMON
VANISHING
POINTS
BEFORE
MATCHING
SEE
SECTION
THUS
IT
IS
BETTER
TO
THINK
BACK
FROM
THE
PROBLEM
AT
HAND
TO
SUITABLE
TECHNIQUES
RATHER
THAN
TO
GRAB
THE
FIRST
TECHNIQUE
THAT
YOU
MAY
HAVE
HEARD
OF
THIS
KIND
OF
WORKING
BACK
FROM
PROBLEMS
TO
SOLUTIONS
IS
TYPICAL
OF
AN
ENGINEERING
APPROACH
TO
THE
STUDY
OF
VISION
AND
REFLECTS
MY
OWN
BACKGROUND
IN
THE
FIELD
FIRST
I
COME
UP
WITH
A
DETAILED
PROBLEM
DEFINITION
AND
DECIDE
ON
THE
CONSTRAINTS
AND
SPECIFICATIONS
FOR
THE
PROBLEM
THEN
I
TRY
TO
FIND
OUT
WHICH
TECHNIQUES
ARE
KNOWN
TO
WORK
IMPLEMENT
A
FEW
OF
THESE
EVALUATE
THEIR
PERFORMANCE
AND
FINALLY
MAKE
A
SELECTION
IN
ORDER
FOR
THIS
PROCESS
TO
WORK
IT
IS
IMPORTANT
TO
HAVE
REALISTIC
TEST
DATA
BOTH
SYNTHETIC
WHICH
CAN
BE
USED
TO
VERIFY
CORRECTNESS
AND
ANALYZE
NOISE
SENSITIVITY
AND
REAL
WORLD
DATA
TYPICAL
OF
THE
WAY
THE
SYSTEM
WILL
FINALLY
BE
USED
HOWEVER
THIS
BOOK
IS
NOT
JUST
AN
ENGINEERING
TEXT
A
SOURCE
OF
RECIPES
IT
ALSO
TAKES
A
SCIENTIFIC
APPROACH
TO
BASIC
VISION
PROBLEMS
HERE
I
TRY
TO
COME
UP
WITH
THE
BEST
POSSIBLE
MODELS
OF
THE
PHYSICS
OF
THE
SYSTEM
AT
HAND
HOW
THE
SCENE
IS
CREATED
HOW
LIGHT
INTERACTS
WITH
THE
SCENE
AND
ATMOSPHERIC
EFFECTS
AND
HOW
THE
SENSORS
WORK
INCLUDING
SOURCES
OF
NOISE
AND
UNCERTAINTY
THE
TASK
IS
THEN
TO
TRY
TO
INVERT
THE
ACQUISITION
PROCESS
TO
COME
UP
WITH
THE
BEST
POSSIBLE
DESCRIPTION
OF
THE
SCENE
THE
BOOK
OFTEN
USES
A
STATISTICAL
APPROACH
TO
FORMULATING
AND
SOLVING
COMPUTER
VISION
PROBLEMS
WHERE
APPROPRIATE
PROBABILITY
DISTRIBUTIONS
ARE
USED
TO
MODEL
THE
SCENE
AND
THE
NOISY
IMAGE
ACQUISITION
PROCESS
THE
ASSOCIATION
OF
PRIOR
DISTRIBUTIONS
WITH
UNKNOWNS
IS
OFTEN
FOR
A
FUN
STUDENT
PROJECT
ON
THIS
TOPIC
SEE
THE
PHOTOBOOK
PROJECT
AT
HTTP
WWW
CC
GATECH
EDU
DVFX
VIDEOS
HTML
THESE
TECHNIQUES
INCLUDE
PHYSICS
EUCLIDEAN
AND
PROJECTIVE
GEOMETRY
STATISTICS
AND
OPTIMIZATION
THEY
MAKE
COMPUTER
VISION
A
FASCINATING
FIELD
TO
STUDY
AND
A
GREAT
WAY
TO
LEARN
TECHNIQUES
WIDELY
APPLICABLE
IN
OTHER
FIELDS
CALLED
BAYESIAN
MODELING
APPENDIX
B
IT
IS
POSSIBLE
TO
ASSOCIATE
A
RISK
OR
LOSS
FUNCTION
WITH
MIS
ESTIMATING
THE
ANSWER
SECTION
B
AND
TO
SET
UP
YOUR
INFERENCE
ALGORITHM
TO
MINIMIZE
THE
EXPECTED
RISK
CONSIDER
A
ROBOT
TRYING
TO
ESTIMATE
THE
DISTANCE
TO
AN
OBSTACLE
IT
IS
USUALLY
SAFER
TO
UNDERESTIMATE
THAN
TO
OVERESTIMATE
WITH
STATISTICAL
TECHNIQUES
IT
OFTEN
HELPS
TO
GATHER
LOTS
OF
TRAINING
DATA
FROM
WHICH
TO
LEARN
PROBABILISTIC
MODELS
FINALLY
STATISTICAL
APPROACHES
ENABLE
YOU
TO
USE
PROVEN
INFERENCE
TECHNIQUES
TO
ESTIMATE
THE
BEST
ANSWER
OR
DISTRIBUTION
OF
ANSWERS
AND
TO
QUANTIFY
THE
UNCERTAINTY
IN
THE
RESULTING
ESTIMATES
BECAUSE
SO
MUCH
OF
COMPUTER
VISION
INVOLVES
THE
SOLUTION
OF
INVERSE
PROBLEMS
OR
THE
ESTI
MATION
OF
UNKNOWN
QUANTITIES
MY
BOOK
ALSO
HAS
A
HEAVY
EMPHASIS
ON
ALGORITHMS
ESPECIALLY
THOSE
THAT
ARE
KNOWN
TO
WORK
WELL
IN
PRACTICE
FOR
MANY
VISION
PROBLEMS
IT
IS
ALL
TOO
EASY
TO
COME
UP
WITH
A
MATHEMATICAL
DESCRIPTION
OF
THE
PROBLEM
THAT
EITHER
DOES
NOT
MATCH
REALISTIC
REAL
WORLD
CONDITIONS
OR
DOES
NOT
LEND
ITSELF
TO
THE
STABLE
ESTIMATION
OF
THE
UNKNOWNS
WHAT
WE
NEED
ARE
ALGORITHMS
THAT
ARE
BOTH
ROBUST
TO
NOISE
AND
DEVIATION
FROM
OUR
MODELS
AND
REA
SONABLY
EFFICIENT
IN
TERMS
OF
RUN
TIME
RESOURCES
AND
SPACE
IN
THIS
BOOK
I
GO
INTO
THESE
ISSUES
IN
DETAIL
USING
BAYESIAN
TECHNIQUES
WHERE
APPLICABLE
TO
ENSURE
ROBUSTNESS
AND
EFFICIENT
SEARCH
MINIMIZATION
AND
LINEAR
SYSTEM
SOLVING
ALGORITHMS
TO
ENSURE
EFFICIENCY
MOST
OF
THE
ALGORITHMS
DESCRIBED
IN
THIS
BOOK
ARE
AT
A
HIGH
LEVEL
BEING
MOSTLY
A
LIST
OF
STEPS
THAT
HAVE
TO
BE
FILLED
IN
BY
STUDENTS
OR
BY
READING
MORE
DETAILED
DESCRIPTIONS
ELSEWHERE
IN
FACT
MANY
OF
THE
ALGORITHMS
ARE
SKETCHED
OUT
IN
THE
EXERCISES
NOW
THAT
I
VE
DESCRIBED
THE
GOALS
OF
THIS
BOOK
AND
THE
FRAMEWORKS
THAT
I
USE
I
DEVOTE
THE
REST
OF
THIS
CHAPTER
TO
TWO
ADDITIONAL
TOPICS
SECTION
IS
A
BRIEF
SYNOPSIS
OF
THE
HISTORY
OF
COMPUTER
VISION
IT
CAN
EASILY
BE
SKIPPED
BY
THOSE
WHO
WANT
TO
GET
TO
THE
MEAT
OF
THE
NEW
MATERIAL
IN
THIS
BOOK
AND
DO
NOT
CARE
AS
MUCH
ABOUT
WHO
INVENTED
WHAT
WHEN
THE
SECOND
IS
AN
OVERVIEW
OF
THE
BOOK
CONTENTS
SECTION
WHICH
IS
USEFUL
READING
FOR
EVERYONE
WHO
INTENDS
TO
MAKE
A
STUDY
OF
THIS
TOPIC
OR
TO
JUMP
IN
PARTWAY
SINCE
IT
DESCRIBES
CHAPTER
INTER
DEPENDENCIES
THIS
OUTLINE
IS
ALSO
USEFUL
FOR
INSTRUCTORS
LOOKING
TO
STRUCTURE
ONE
OR
MORE
COURSES
AROUND
THIS
TOPIC
AS
IT
PROVIDES
SAMPLE
CURRICULA
BASED
ON
THE
BOOK
CONTENTS
A
BRIEF
HISTORY
IN
THIS
SECTION
I
PROVIDE
A
BRIEF
PERSONAL
SYNOPSIS
OF
THE
MAIN
DEVELOPMENTS
IN
COMPUTER
VISION
OVER
THE
LAST
YEARS
FIGURE
AT
LEAST
THOSE
THAT
I
FIND
PERSONALLY
INTERESTING
AND
WHICH
APPEAR
TO
HAVE
STOOD
THE
TEST
OF
TIME
READERS
NOT
INTERESTED
IN
THE
PROVENANCE
OF
VARIOUS
IDEAS
AND
THE
EVOLUTION
OF
THIS
FIELD
SHOULD
SKIP
AHEAD
TO
THE
BOOK
OVERVIEW
IN
SECTION
FIGURE
A
ROUGH
TIMELINE
OF
SOME
OF
THE
MOST
ACTIVE
TOPICS
OF
RESEARCH
IN
COMPUTER
VISION
WHEN
COMPUTER
VISION
FIRST
STARTED
OUT
IN
THE
EARLY
IT
WAS
VIEWED
AS
THE
VISUAL
PERCEPTION
COMPONENT
OF
AN
AMBITIOUS
AGENDA
TO
MIMIC
HUMAN
INTELLIGENCE
AND
TO
ENDOW
ROBOTS
WITH
INTELLIGENT
BEHAVIOR
AT
THE
TIME
IT
WAS
BELIEVED
BY
SOME
OF
THE
EARLY
PIONEERS
OF
ARTIFICIAL
INTELLIGENCE
AND
ROBOTICS
AT
PLACES
SUCH
AS
MIT
STANFORD
AND
CMU
THAT
SOLVING
THE
VISUAL
INPUT
PROBLEM
WOULD
BE
AN
EASY
STEP
ALONG
THE
PATH
TO
SOLVING
MORE
DIFFICULT
PROBLEMS
SUCH
AS
HIGHER
LEVEL
REASONING
AND
PLANNING
ACCORDING
TO
ONE
WELL
KNOWN
STORY
IN
MARVIN
MINSKY
AT
MIT
ASKED
HIS
UNDERGRADUATE
STUDENT
GERALD
JAY
SUSSMAN
TO
SPEND
THE
SUMMER
LINKING
A
CAMERA
TO
A
COMPUTER
AND
GETTING
THE
COMPUTER
TO
DESCRIBE
WHAT
IT
SAW
BODEN
P
WE
NOW
KNOW
THAT
THE
PROBLEM
IS
SLIGHTLY
MORE
DIFFICULT
THAN
THAT
WHAT
DISTINGUISHED
COMPUTER
VISION
FROM
THE
ALREADY
EXISTING
FIELD
OF
DIGITAL
IMAGE
PRO
CESSING
ROSENFELD
AND
PFALTZ
ROSENFELD
AND
KAK
WAS
A
DESIRE
TO
RECOVER
THE
THREE
DIMENSIONAL
STRUCTURE
OF
THE
WORLD
FROM
IMAGES
AND
TO
USE
THIS
AS
A
STEPPING
STONE
TO
WARDS
FULL
SCENE
UNDERSTANDING
WINSTON
AND
HANSON
AND
RISEMAN
PROVIDE
TWO
NICE
COLLECTIONS
OF
CLASSIC
PAPERS
FROM
THIS
EARLY
PERIOD
EARLY
ATTEMPTS
AT
SCENE
UNDERSTANDING
INVOLVED
EXTRACTING
EDGES
AND
THEN
INFERRING
THE
STRUCTURE
OF
AN
OBJECT
OR
A
BLOCKS
WORLD
FROM
THE
TOPOLOGICAL
STRUCTURE
OF
THE
LINES
ROBERTS
SEVERAL
LINE
LABELING
ALGORITHMS
FIGURE
WERE
DEVELOPED
AT
THAT
TIME
HUFFMAN
CLOWES
WALTZ
ROSENFELD
HUMMEL
AND
ZUCKER
KANADE
NALWA
GIVES
A
NICE
REVIEW
OF
THIS
AREA
THE
TOPIC
OF
EDGE
DETECTION
WAS
ALSO
BODEN
CITES
CREVIER
AS
THE
ORIGINAL
SOURCE
THE
ACTUAL
VISION
MEMO
WAS
AUTHORED
BY
SEYMOUR
PAPERT
AND
INVOLVED
A
WHOLE
COHORT
OF
STUDENTS
TO
SEE
HOW
FAR
ROBOTIC
VISION
HAS
COME
IN
THE
LAST
FOUR
DECADES
HAVE
A
LOOK
AT
THE
TOWEL
FOLDING
ROBOT
AT
HTTP
RLL
EECS
BERKELEY
EDU
PR
MAITIN
SHEPARD
CUSUMANO
TOWNER
LEI
ET
AL
A
B
C
D
E
F
FIGURE
SOME
EARLY
EXAMPLES
OF
COMPUTER
VISION
ALGORITHMS
A
LINE
LABEL
ING
NALWA
QC
ADDISON
WESLEY
B
PICTORIAL
STRUCTURES
FISCHLER
AND
ELSCHLAGER
QC
IEEE
C
ARTICULATED
BODY
MODEL
MARR
QC
DAVID
MARR
D
INTRIN
SIC
IMAGES
BARROW
AND
TENENBAUM
QC
IEEE
E
STEREO
CORRESPONDENCE
MARR
QC
DAVID
MARR
F
OPTICAL
FLOW
NAGEL
AND
ENKELMANN
QC
IEEE
AN
ACTIVE
AREA
OF
RESEARCH
A
NICE
SURVEY
OF
CONTEMPORANEOUS
WORK
CAN
BE
FOUND
IN
DAVIS
THREE
DIMENSIONAL
MODELING
OF
NON
POLYHEDRAL
OBJECTS
WAS
ALSO
BEING
STUDIED
BAUM
GART
BAKER
ONE
POPULAR
APPROACH
USED
GENERALIZED
CYLINDERS
I
E
SOLIDS
OF
REVOLUTION
AND
SWEPT
CLOSED
CURVES
AGIN
AND
BINFORD
NEVATIA
AND
BINFORD
OF
TEN
ARRANGED
INTO
PARTS
HINTON
MARR
FIGURE
FISCHLER
AND
ELSCHLAGER
CALLED
SUCH
ELASTIC
ARRANGEMENTS
OF
PARTS
PICTORIAL
STRUCTURES
FIGURE
THIS
IS
CURRENTLY
ONE
OF
THE
FAVORED
APPROACHES
BEING
USED
IN
OBJECT
RECOGNITION
SEE
SEC
TION
AND
FELZENSZWALB
AND
HUTTENLOCHER
A
QUALITATIVE
APPROACH
TO
UNDERSTANDING
INTENSITIES
AND
SHADING
VARIATIONS
AND
EXPLAINING
THEM
BY
THE
EFFECTS
OF
IMAGE
FORMATION
PHENOMENA
SUCH
AS
SURFACE
ORIENTATION
AND
SHADOWS
WAS
CHAMPIONED
BY
BARROW
AND
TENENBAUM
IN
THEIR
PAPER
ON
INTRINSIC
IMAGES
FIG
URE
ALONG
WITH
THE
RELATED
D
SKETCH
IDEAS
OF
MARR
THIS
APPROACH
IS
AGAIN
SEEING
A
BIT
OF
A
REVIVAL
IN
THE
WORK
OF
TAPPEN
FREEMAN
AND
ADELSON
MORE
QUANTITATIVE
APPROACHES
TO
COMPUTER
VISION
WERE
ALSO
DEVELOPED
AT
THE
TIME
IN
CLUDING
THE
FIRST
OF
MANY
FEATURE
BASED
STEREO
CORRESPONDENCE
ALGORITHMS
FIGURE
DEV
IN
ROBOTICS
AND
COMPUTER
ANIMATION
THESE
LINKED
PART
GRAPHS
ARE
OFTEN
CALLED
KINEMATIC
CHAINS
MARR
AND
POGGIO
MORAVEC
MARR
AND
POGGIO
MAYHEW
AND
FRISBY
BAKER
BARNARD
AND
FISCHLER
OHTA
AND
KANADE
GRIMSON
POL
LARD
MAYHEW
AND
FRISBY
PRAZDNY
AND
INTENSITY
BASED
OPTICAL
FLOW
ALGORITHMS
FIGURE
HORN
AND
SCHUNCK
HUANG
LUCAS
AND
KANADE
NAGEL
THE
EARLY
WORK
IN
SIMULTANEOUSLY
RECOVERING
STRUCTURE
AND
CAMERA
MOTION
SEE
CHAPTER
ALSO
BEGAN
AROUND
THIS
TIME
ULLMAN
LONGUET
HIGGINS
A
LOT
OF
THE
PHILOSOPHY
OF
HOW
VISION
WAS
BELIEVED
TO
WORK
AT
THE
TIME
IS
SUMMARIZED
IN
DAVID
MARR
BOOK
IN
PARTICULAR
MARR
INTRODUCED
HIS
NOTION
OF
THE
THREE
LEVELS
OF
DESCRIPTION
OF
A
VISUAL
INFORMATION
PROCESSING
SYSTEM
THESE
THREE
LEVELS
VERY
LOOSELY
PARAPHRASED
ACCORDING
TO
MY
OWN
INTERPRETATION
ARE
COMPUTATIONAL
THEORY
WHAT
IS
THE
GOAL
OF
THE
COMPUTATION
TASK
AND
WHAT
ARE
THE
CONSTRAINTS
THAT
ARE
KNOWN
OR
CAN
BE
BROUGHT
TO
BEAR
ON
THE
PROBLEM
REPRESENTATIONS
AND
ALGORITHMS
HOW
ARE
THE
INPUT
OUTPUT
AND
INTERMEDIATE
INFOR
MATION
REPRESENTED
AND
WHICH
ALGORITHMS
ARE
USED
TO
CALCULATE
THE
DESIRED
RESULT
HARDWARE
IMPLEMENTATION
HOW
ARE
THE
REPRESENTATIONS
AND
ALGORITHMS
MAPPED
ONTO
ACTUAL
HARDWARE
E
G
A
BIOLOGICAL
VISION
SYSTEM
OR
A
SPECIALIZED
PIECE
OF
SILICON
CON
VERSELY
HOW
CAN
HARDWARE
CONSTRAINTS
BE
USED
TO
GUIDE
THE
CHOICE
OF
REPRESENTATION
AND
ALGORITHM
WITH
THE
INCREASING
USE
OF
GRAPHICS
CHIPS
GPUS
AND
MANY
CORE
AR
CHITECTURES
FOR
COMPUTER
VISION
SEE
SECTION
C
THIS
QUESTION
IS
AGAIN
BECOMING
QUITE
RELEVANT
AS
I
MENTIONED
EARLIER
IN
THIS
INTRODUCTION
IT
IS
MY
CONVICTION
THAT
A
CAREFUL
ANALYSIS
OF
THE
PROBLEM
SPECIFICATION
AND
KNOWN
CONSTRAINTS
FROM
IMAGE
FORMATION
AND
PRIORS
THE
SCIENTIFIC
AND
STATISTICAL
APPROACHES
MUST
BE
MARRIED
WITH
EFFICIENT
AND
ROBUST
ALGORITHMS
THE
ENGINEER
ING
APPROACH
TO
DESIGN
SUCCESSFUL
VISION
ALGORITHMS
THUS
IT
SEEMS
THAT
MARR
PHILOSOPHY
IS
AS
GOOD
A
GUIDE
TO
FRAMING
AND
SOLVING
PROBLEMS
IN
OUR
FIELD
TODAY
AS
IT
WAS
YEARS
AGO
IN
THE
A
LOT
OF
ATTENTION
WAS
FOCUSED
ON
MORE
SOPHISTICATED
MATHEMATICAL
TECHNIQUES
FOR
PERFORMING
QUANTITATIVE
IMAGE
AND
SCENE
ANALYSIS
IMAGE
PYRAMIDS
SEE
SECTION
STARTED
BEING
WIDELY
USED
TO
PERFORM
TASKS
SUCH
AS
IM
AGE
BLENDING
FIGURE
AND
COARSE
TO
FINE
CORRESPONDENCE
SEARCH
ROSENFELD
BURT
AND
ADELSON
B
ROSENFELD
QUAM
ANANDAN
CONTINUOUS
VERSIONS
OF
PYRAMIDS
USING
THE
CONCEPT
OF
SCALE
SPACE
PROCESSING
WERE
ALSO
DEVELOPED
WITKIN
WITKIN
TERZOPOULOS
AND
KASS
LINDEBERG
IN
THE
LATE
WAVELETS
SEE
SEC
TION
STARTED
DISPLACING
OR
AUGMENTING
REGULAR
IMAGE
PYRAMIDS
IN
SOME
APPLICATIONS
MORE
RECENT
DEVELOPMENTS
IN
VISUAL
PERCEPTION
THEORY
ARE
COVERED
IN
PALMER
LIVINGSTONE
A
B
C
D
E
F
FIGURE
EXAMPLES
OF
COMPUTER
VISION
ALGORITHMS
FROM
THE
A
PYRAMID
BLENDING
BURT
AND
ADELSON
QC
ACM
B
SHAPE
FROM
SHADING
FREEMAN
AND
ADELSON
QC
IEEE
C
EDGE
DETECTION
FREEMAN
AND
ADELSON
QC
IEEE
D
PHYSICALLY
BASED
MODELS
TERZOPOULOS
AND
WITKIN
QC
IEEE
E
REGULARIZATION
BASED
SURFACE
RECONSTRUCTION
TERZOPOULOS
QC
IEEE
F
RANGE
DATA
ACQUISITION
AND
MERGING
BANNO
MASUDA
OISHI
ET
AL
QC
SPRINGER
ADELSON
SIMONCELLI
AND
HINGORANI
MALLAT
SIMONCELLI
AND
ADELSON
B
SIMONCELLI
FREEMAN
ADELSON
ET
AL
THE
USE
OF
STEREO
AS
A
QUANTITATIVE
SHAPE
CUE
WAS
EXTENDED
BY
A
WIDE
VARIETY
OF
SHAPE
FROM
X
TECHNIQUES
INCLUDING
SHAPE
FROM
SHADING
FIGURE
SEE
SECTION
AND
HORN
PENTLAND
BLAKE
ZIMMERMAN
AND
KNOWLES
HORN
AND
BROOKS
PHOTOMETRIC
STEREO
SEE
SECTION
AND
WOODHAM
SHAPE
FROM
TEXTURE
SEE
SEC
TION
AND
WITKIN
PENTLAND
MALIK
AND
ROSENHOLTZ
AND
SHAPE
FROM
FOCUS
SEE
SECTION
AND
NAYAR
WATANABE
AND
NOGUCHI
HORN
HAS
A
NICE
DISCUSSION
OF
MOST
OF
THESE
TECHNIQUES
RESEARCH
INTO
BETTER
EDGE
AND
CONTOUR
DETECTION
FIGURE
SEE
SECTION
WAS
ALSO
ACTIVE
DURING
THIS
PERIOD
CANNY
NALWA
AND
BINFORD
INCLUDING
THE
INTRODUC
TION
OF
DYNAMICALLY
EVOLVING
CONTOUR
TRACKERS
SECTION
SUCH
AS
SNAKES
KASS
WITKIN
AND
TERZOPOULOS
AS
WELL
AS
THREE
DIMENSIONAL
PHYSICALLY
BASED
MODELS
FIGURE
TERZOPOULOS
WITKIN
AND
KASS
KASS
WITKIN
AND
TERZOPOULOS
TERZOPOULOS
AND
FLEISCHER
TERZOPOULOS
WITKIN
AND
KASS
RESEARCHERS
NOTICED
THAT
A
LOT
OF
THE
STEREO
FLOW
SHAPE
FROM
X
AND
EDGE
DETECTION
AL
GORITHMS
COULD
BE
UNIFIED
OR
AT
LEAST
DESCRIBED
USING
THE
SAME
MATHEMATICAL
FRAMEWORK
IF
THEY
WERE
POSED
AS
VARIATIONAL
OPTIMIZATION
PROBLEMS
SEE
SECTION
AND
MADE
MORE
RO
BUST
WELL
POSED
USING
REGULARIZATION
FIGURE
SEE
SECTION
AND
TERZOPOULOS
POGGIO
TORRE
AND
KOCH
TERZOPOULOS
BLAKE
AND
ZISSERMAN
BERTERO
POG
GIO
AND
TORRE
TERZOPOULOS
AROUND
THE
SAME
TIME
GEMAN
AND
GEMAN
POINTED
OUT
THAT
SUCH
PROBLEMS
COULD
EQUALLY
WELL
BE
FORMULATED
USING
DISCRETE
MARKOV
RAN
DOM
FIELD
MRF
MODELS
SEE
SECTION
WHICH
ENABLED
THE
USE
OF
BETTER
GLOBAL
SEARCH
AND
OPTIMIZATION
ALGORITHMS
SUCH
AS
SIMULATED
ANNEALING
ONLINE
VARIANTS
OF
MRF
ALGORITHMS
THAT
MODELED
AND
UPDATED
UNCERTAINTIES
USING
THE
KALMAN
FILTER
WERE
INTRODUCED
A
LITTLE
LATER
DICKMANNS
AND
GRAEFE
MATTHIES
KANADE
AND
SZELISKI
SZELISKI
ATTEMPTS
WERE
ALSO
MADE
TO
MAP
BOTH
REGULARIZED
AND
MRF
ALGORITHMS
ONTO
PARALLEL
HARDWARE
POGGIO
AND
KOCH
POGGIO
LITTLE
GAMBLE
ET
AL
FISCHLER
FIRSCHEIN
BARNARD
ET
AL
THE
BOOK
BY
FISCHLER
AND
FIRSCHEIN
CONTAINS
A
NICE
COLLECTION
OF
ARTICLES
FOCUSING
ON
ALL
OF
THESE
TOPICS
STEREO
FLOW
REGULARIZATION
MRFS
AND
EVEN
HIGHER
LEVEL
VISION
THREE
DIMENSIONAL
RANGE
DATA
PROCESSING
ACQUISITION
MERGING
MODELING
AND
RECOGNI
TION
SEE
FIGURE
CONTINUED
BEING
ACTIVELY
EXPLORED
DURING
THIS
DECADE
AGIN
AND
BINFORD
BESL
AND
JAIN
FAUGERAS
AND
HEBERT
CURLESS
AND
LEVOY
THE
COMPI
LATION
BY
KANADE
CONTAINS
A
LOT
OF
THE
INTERESTING
PAPERS
IN
THIS
AREA
WHILE
A
LOT
OF
THE
PREVIOUSLY
MENTIONED
TOPICS
CONTINUED
TO
BE
EXPLORED
A
FEW
OF
THEM
BECAME
SIGNIFICANTLY
MORE
ACTIVE
A
BURST
OF
ACTIVITY
IN
USING
PROJECTIVE
INVARIANTS
FOR
RECOGNITION
MUNDY
AND
ZISSERMAN
EVOLVED
INTO
A
CONCERTED
EFFORT
TO
SOLVE
THE
STRUCTURE
FROM
MOTION
PROBLEM
SEE
CHAP
TER
A
LOT
OF
THE
INITIAL
ACTIVITY
WAS
DIRECTED
AT
PROJECTIVE
RECONSTRUCTIONS
WHICH
DID
NOT
REQUIRE
KNOWLEDGE
OF
CAMERA
CALIBRATION
FAUGERAS
HARTLEY
GUPTA
AND
CHANG
HARTLEY
FAUGERAS
AND
LUONG
HARTLEY
AND
ZISSERMAN
SIMULTANEOUSLY
FAC
TORIZATION
TECHNIQUES
SECTION
WERE
DEVELOPED
TO
SOLVE
EFFICIENTLY
PROBLEMS
FOR
WHICH
OR
THOGRAPHIC
CAMERA
APPROXIMATIONS
WERE
APPLICABLE
FIGURE
TOMASI
AND
KANADE
POELMAN
AND
KANADE
ANANDAN
AND
IRANI
AND
THEN
LATER
EXTENDED
TO
THE
PERSPEC
TIVE
CASE
CHRISTY
AND
HORAUD
TRIGGS
EVENTUALLY
THE
FIELD
STARTED
USING
FULL
GLOBAL
OPTIMIZATION
SEE
SECTION
AND
TAYLOR
KRIEGMAN
AND
ANANDAN
SZELISKI
AND
KANG
AZARBAYEJANI
AND
PENTLAND
WHICH
WAS
LATER
RECOGNIZED
AS
BEING
THE
SAME
AS
THE
BUNDLE
ADJUSTMENT
TECHNIQUES
TRADITIONALLY
USED
IN
PHOTOGRAMMETRY
TRIGGS
MCLAUCH
LAN
HARTLEY
ET
AL
FULLY
AUTOMATED
SPARSE
MODELING
SYSTEMS
WERE
BUILT
USING
SUCH
TECHNIQUES
BEARDSLEY
TORR
AND
ZISSERMAN
SCHAFFALITZKY
AND
ZISSERMAN
BROWN
AND
LOWE
SNAVELY
SEITZ
AND
SZELISKI
WORK
BEGUN
IN
THE
ON
USING
DETAILED
MEASUREMENTS
OF
COLOR
AND
INTENSITY
COMBINED
A
B
C
D
E
F
FIGURE
EXAMPLES
OF
COMPUTER
VISION
ALGORITHMS
FROM
THE
A
FACTORIZATION
BASED
STRUCTURE
FROM
MOTION
TOMASI
AND
KANADE
QC
SPRINGER
B
DENSE
STEREO
MATCH
ING
BOYKOV
VEKSLER
AND
ZABIH
C
MULTI
VIEW
RECONSTRUCTION
SEITZ
AND
DYER
QC
SPRINGER
D
FACE
TRACKING
MATTHEWS
XIAO
AND
BAKER
E
IMAGE
SEGMENTA
TION
BELONGIE
FOWLKES
CHUNG
ET
AL
QC
PENTLAND
SPRINGER
F
FACE
RECOGNITION
TURK
AND
WITH
ACCURATE
PHYSICAL
MODELS
OF
RADIANCE
TRANSPORT
AND
COLOR
IMAGE
FORMATION
CREATED
ITS
OWN
SUBFIELD
KNOWN
AS
PHYSICS
BASED
VISION
A
GOOD
SURVEY
OF
THE
FIELD
CAN
BE
FOUND
IN
THE
THREE
VOLUME
COLLECTION
ON
THIS
TOPIC
WOLFF
SHAFER
AND
HEALEY
HEALEY
AND
SHAFER
SHAFER
HEALEY
AND
WOLFF
OPTICAL
FLOW
METHODS
SEE
CHAPTER
CONTINUED
TO
BE
IMPROVED
NAGEL
AND
ENKELMANN
BOLLES
BAKER
AND
MARIMONT
HORN
AND
WELDON
JR
ANANDAN
BERGEN
ANANDAN
HANNA
ET
AL
BLACK
AND
ANANDAN
BRUHN
WEICKERT
AND
SCHNO
RR
PAPENBERG
BRUHN
BROX
ET
AL
WITH
NAGEL
BARRON
FLEET
AND
BEAUCHEMIN
BAKER
BLACK
LEWIS
ET
AL
BEING
GOOD
SURVEYS
SIMILARLY
A
LOT
OF
PROGRESS
WAS
MADE
ON
DENSE
STEREO
CORRESPONDENCE
ALGORITHMS
SEE
CHAPTER
OKUTOMI
AND
KANADE
BOYKOV
VEKSLER
AND
ZABIH
BIRCHFIELD
AND
TOMASI
BOYKOV
VEKSLER
AND
ZABIH
AND
THE
SURVEY
AND
COMPARISON
IN
SCHARSTEIN
AND
SZELISKI
WITH
THE
BIGGEST
BREAKTHROUGH
BEING
PERHAPS
GLOBAL
OPTIMIZATION
USING
GRAPH
CUT
TECHNIQUES
FIG
URE
BOYKOV
VEKSLER
AND
ZABIH
MULTI
VIEW
STEREO
ALGORITHMS
FIGURE
THAT
PRODUCE
COMPLETE
SURFACES
SEE
SEC
TION
WERE
ALSO
AN
ACTIVE
TOPIC
OF
RESEARCH
SEITZ
AND
DYER
KUTULAKOS
AND
SEITZ
THAT
CONTINUES
TO
BE
ACTIVE
TODAY
SEITZ
CURLESS
DIEBEL
ET
AL
TECHNIQUES
FOR
PRODUCING
VOLUMETRIC
DESCRIPTIONS
FROM
BINARY
SILHOUETTES
SEE
SECTION
CONTINUED
TO
BE
DEVELOPED
POTMESIL
SRIVASAN
LIANG
AND
HACKWOOD
SZELISKI
LAU
RENTINI
ALONG
WITH
TECHNIQUES
BASED
ON
TRACKING
AND
RECONSTRUCTING
SMOOTH
OCCLUDING
CONTOURS
SEE
SECTION
AND
CIPOLLA
AND
BLAKE
VAILLANT
AND
FAUGERAS
ZHENG
BOYER
AND
BERGER
SZELISKI
AND
WEISS
CIPOLLA
AND
GIBLIN
TRACKING
ALGORITHMS
ALSO
IMPROVED
A
LOT
INCLUDING
CONTOUR
TRACKING
USING
ACTIVE
CONTOURS
SEE
SECTION
SUCH
AS
SNAKES
KASS
WITKIN
AND
TERZOPOULOS
PARTICLE
FILTERS
BLAKE
AND
ISARD
AND
LEVEL
SETS
MALLADI
SETHIAN
AND
VEMURI
AS
WELL
AS
INTENSITY
BASED
DIRECT
TECHNIQUES
LUCAS
AND
KANADE
SHI
AND
TOMASI
REHG
AND
KANADE
OFTEN
APPLIED
TO
TRACKING
FACES
FIGURE
LANITIS
TAYLOR
AND
COOTES
MATTHEWS
AND
BAKER
MATTHEWS
XIAO
AND
BAKER
AND
WHOLE
BODIES
SIDENBLADH
BLACK
AND
FLEET
HILTON
FUA
AND
RONFARD
MOESLUND
HILTON
AND
KRU
GER
IMAGE
SEGMENTATION
SEE
CHAPTER
FIGURE
A
TOPIC
WHICH
HAS
BEEN
ACTIVE
SINCE
THE
EARLIEST
DAYS
OF
COMPUTER
VISION
BRICE
AND
FENNEMA
HOROWITZ
AND
PAVLIDIS
RISEMAN
AND
ARBIB
ROSENFELD
AND
DAVIS
HARALICK
AND
SHAPIRO
PAVLIDIS
AND
LIOW
WAS
ALSO
AN
ACTIVE
TOPIC
OF
RESEARCH
PRODUCING
TECHNIQUES
BASED
ON
MIN
IMUM
ENERGY
MUMFORD
AND
SHAH
AND
MINIMUM
DESCRIPTION
LENGTH
LECLERC
NORMALIZED
CUTS
SHI
AND
MALIK
AND
MEAN
SHIFT
COMANICIU
AND
MEER
STATISTICAL
LEARNING
TECHNIQUES
STARTED
APPEARING
FIRST
IN
THE
APPLICATION
OF
PRINCIPAL
COM
PONENT
EIGENFACE
ANALYSIS
TO
FACE
RECOGNITION
FIGURE
SEE
SECTION
AND
TURK
AND
PENTLAND
AND
LINEAR
DYNAMICAL
SYSTEMS
FOR
CURVE
TRACKING
SEE
SECTION
AND
BLAKE
AND
ISARD
PERHAPS
THE
MOST
NOTABLE
DEVELOPMENT
IN
COMPUTER
VISION
DURING
THIS
DECADE
WAS
THE
INCREASED
INTERACTION
WITH
COMPUTER
GRAPHICS
SEITZ
AND
SZELISKI
ESPECIALLY
IN
THE
CROSS
DISCIPLINARY
AREA
OF
IMAGE
BASED
MODELING
AND
RENDERING
SEE
CHAPTER
THE
IDEA
OF
MANIPULATING
REAL
WORLD
IMAGERY
DIRECTLY
TO
CREATE
NEW
ANIMATIONS
FIRST
CAME
TO
PROMINENCE
WITH
IMAGE
MORPHING
TECHNIQUES
SEE
SECTION
AND
BEIER
AND
NEELY
AND
WAS
LATER
APPLIED
TO
VIEW
INTERPOLATION
CHEN
AND
WILLIAMS
SEITZ
AND
DYER
PANORAMIC
IMAGE
STITCHING
SEE
CHAPTER
AND
MANN
AND
PICARD
CHEN
SZELISKI
SZELISKI
AND
SHUM
SZELISKI
AND
FULL
LIGHT
FIELD
RENDERING
FIGURE
SEE
SECTION
AND
GORTLER
GRZESZCZUK
SZELISKI
ET
AL
LEVOY
AND
HANRAHAN
SHADE
GORTLER
HE
ET
AL
AT
THE
SAME
TIME
IMAGE
BASED
MODELING
TECHNIQUES
FIGURE
FOR
AUTOMATICALLY
CREATING
REALISTIC
MODELS
FROM
COLLECTIONS
OF
IMAGES
WERE
ALSO
BEING
INTRODUCED
BEARDSLEY
TORR
AND
ZISSERMAN
DEBEVEC
TAYLOR
AND
MALIK
TAYLOR
DEBEVEC
AND
MALIK
A
B
C
D
E
F
FIGURE
RECENT
EXAMPLES
OF
COMPUTER
VISION
ALGORITHMS
A
IMAGE
BASED
RENDERING
GORTLER
GRZESZCZUK
SZELISKI
ET
AL
B
IMAGE
BASED
MODELING
DEBEVEC
TAYLOR
AND
MALIK
QC
ACM
C
INTERACTIVE
TONE
MAPPING
LISCHINSKI
FARBMAN
UYTTENDAELE
ET
AL
D
TEXTURE
SYNTHESIS
EFROS
AND
FREEMAN
E
FEATURE
BASED
RECOGNITION
FERGUS
PERONA
AND
ZISSERMAN
F
REGION
BASED
RECOGNITION
MORI
REN
EFROS
ET
AL
QC
IEEE
THIS
PAST
DECADE
HAS
CONTINUED
TO
SEE
A
DEEPENING
INTERPLAY
BETWEEN
THE
VISION
AND
GRAPHICS
FIELDS
IN
PARTICULAR
MANY
OF
THE
TOPICS
INTRODUCED
UNDER
THE
RUBRIC
OF
IMAGE
BASED
RENDERING
SUCH
AS
IMAGE
STITCHING
SEE
CHAPTER
LIGHT
FIELD
CAPTURE
AND
RENDERING
SEE
SECTION
AND
HIGH
DYNAMIC
RANGE
HDR
IMAGE
CAPTURE
THROUGH
EXPOSURE
BRACKETING
SEE
SECTION
AND
MANN
AND
PICARD
DEBEVEC
AND
MALIK
WERE
RE
CHRISTENED
AS
COMPUTATIONAL
PHOTOGRAPHY
SEE
CHAPTER
TO
ACKNOWLEDGE
THE
INCREASED
USE
OF
SUCH
TECHNIQUES
IN
EVERYDAY
DIGITAL
PHOTOGRAPHY
FOR
EXAMPLE
THE
RAPID
ADOPTION
OF
EXPOSURE
BRACKETING
TO
CREATE
HIGH
DYNAMIC
RANGE
IMAGES
NECESSITATED
THE
DEVELOPMENT
OF
TONE
MAPPING
ALGORITHMS
FIGURE
SEE
SECTION
TO
CONVERT
SUCH
IMAGES
BACK
TO
DISPLAYABLE
RESULTS
FATTAL
LISCHINSKI
AND
WERMAN
DURAND
AND
DORSEY
REIN
HARD
STARK
SHIRLEY
ET
AL
LISCHINSKI
FARBMAN
UYTTENDAELE
ET
AL
IN
ADDITION
TO
MERGING
MULTIPLE
EXPOSURES
TECHNIQUES
WERE
DEVELOPED
TO
MERGE
FLASH
IMAGES
WITH
NON
FLASH
COUNTERPARTS
EISEMANN
AND
DURAND
PETSCHNIGG
AGRAWALA
HOPPE
ET
AL
AND
TO
INTERACTIVELY
OR
AUTOMATICALLY
SELECT
DIFFERENT
REGIONS
FROM
OVERLAPPING
IMAGES
AGARWALA
DONTCHEVA
AGRAWALA
ET
AL
TEXTURE
SYNTHESIS
FIGURE
SEE
SECTION
QUILTING
EFROS
AND
LEUNG
EFROS
AND
FREEMAN
KWATRA
SCHO
DL
ESSA
ET
AL
AND
INPAINTING
BERTALMIO
SAPIRO
CASELLES
ET
AL
BERTALMIO
VESE
SAPIRO
ET
AL
CRIMINISI
PE
REZ
AND
TOYAMA
ARE
ADDITIONAL
TOPICS
THAT
CAN
BE
CLASSIFIED
AS
COMPUTATIONAL
PHOTOGRAPHY
TECHNIQUES
SINCE
THEY
RE
COMBINE
INPUT
IMAGE
SAMPLES
TO
PRODUCE
NEW
PHOTOGRAPHS
A
SECOND
NOTABLE
TREND
DURING
THIS
PAST
DECADE
HAS
BEEN
THE
EMERGENCE
OF
FEATURE
BASED
TECHNIQUES
COMBINED
WITH
LEARNING
FOR
OBJECT
RECOGNITION
SEE
SECTION
AND
PONCE
HEBERT
SCHMID
ET
AL
SOME
OF
THE
NOTABLE
PAPERS
IN
THIS
AREA
INCLUDE
THE
CONSTELLATION
MODEL
OF
FERGUS
PERONA
AND
ZISSERMAN
FIGURE
AND
THE
PICTORIAL
STRUCTURES
OF
FELZENSZWALB
AND
HUTTENLOCHER
FEATURE
BASED
TECHNIQUES
ALSO
DOMINATE
OTHER
RECOGNITION
TASKS
SUCH
AS
SCENE
RECOGNITION
ZHANG
MARSZALEK
LAZEBNIK
ET
AL
AND
PANORAMA
AND
LOCATION
RECOGNITION
BROWN
AND
LOWE
SCHINDLER
BROWN
AND
SZELISKI
AND
WHILE
INTEREST
POINT
PATCH
BASED
FEATURES
TEND
TO
DOMINATE
CURRENT
RESEARCH
SOME
GROUPS
ARE
PURSUING
RECOGNITION
BASED
ON
CONTOURS
BELONGIE
MALIK
AND
PUZICHA
AND
REGION
SEGMENTATION
FIGURE
MORI
REN
EFROS
ET
AL
ANOTHER
SIGNIFICANT
TREND
FROM
THIS
PAST
DECADE
HAS
BEEN
THE
DEVELOPMENT
OF
MORE
EFFICIENT
ALGORITHMS
FOR
COMPLEX
GLOBAL
OPTIMIZATION
PROBLEMS
SEE
SECTIONS
AND
B
AND
SZELISKI
ZABIH
SCHARSTEIN
ET
AL
BLAKE
KOHLI
AND
ROTHER
WHILE
THIS
TREND
BEGAN
WITH
WORK
ON
GRAPH
CUTS
BOYKOV
VEKSLER
AND
ZABIH
KOHLI
AND
TORR
A
LOT
OF
PROGRESS
HAS
ALSO
BEEN
MADE
IN
MESSAGE
PASSING
ALGORITHMS
SUCH
AS
LOOPY
BELIEF
PROPAGATION
LBP
YEDIDIA
FREEMAN
AND
WEISS
KUMAR
AND
TORR
THE
FINAL
TREND
WHICH
NOW
DOMINATES
A
LOT
OF
THE
VISUAL
RECOGNITION
RESEARCH
IN
OUR
COM
MUNITY
IS
THE
APPLICATION
OF
SOPHISTICATED
MACHINE
LEARNING
TECHNIQUES
TO
COMPUTER
VISION
PROBLEMS
SEE
SECTION
AND
FREEMAN
PERONA
AND
SCHO
LKOPF
THIS
TREND
COIN
CIDES
WITH
THE
INCREASED
AVAILABILITY
OF
IMMENSE
QUANTITIES
OF
PARTIALLY
LABELLED
DATA
ON
THE
INTERNET
WHICH
MAKES
IT
MORE
FEASIBLE
TO
LEARN
OBJECT
CATEGORIES
WITHOUT
THE
USE
OF
CAREFUL
HUMAN
SUPERVISION
BOOK
OVERVIEW
IN
THE
FINAL
PART
OF
THIS
INTRODUCTION
I
GIVE
A
BRIEF
TOUR
OF
THE
MATERIAL
IN
THIS
BOOK
AS
WELL
AS
A
FEW
NOTES
ON
NOTATION
AND
SOME
ADDITIONAL
GENERAL
REFERENCES
SINCE
COMPUTER
VISION
IS
SUCH
A
BROAD
FIELD
IT
IS
POSSIBLE
TO
STUDY
CERTAIN
ASPECTS
OF
IT
E
G
GEOMETRIC
IMAGE
FORMATION
AND
STRUCTURE
RECOVERY
WITHOUT
ENGAGING
OTHER
PARTS
E
G
THE
MODELING
OF
REFLECTANCE
AND
SHADING
SOME
OF
THE
CHAPTERS
IN
THIS
BOOK
ARE
ONLY
LOOSELY
COUPLED
WITH
OTHERS
AND
IT
IS
NOT
STRICTLY
NECESSARY
TO
READ
ALL
OF
THE
MATERIAL
IN
SEQUENCE
IMAGE
PROCESSING
VISION
GRAPHICS
FIGURE
RELATIONSHIP
BETWEEN
IMAGES
GEOMETRY
AND
PHOTOMETRY
AS
WELL
AS
A
TAXONOMY
OF
THE
TOPICS
COVERED
IN
THIS
BOOK
TOPICS
ARE
ROUGHLY
POSITIONED
ALONG
THE
LEFT
RIGHT
AXIS
DEPENDING
ON
WHETHER
THEY
ARE
MORE
CLOSELY
RELATED
TO
IMAGE
BASED
LEFT
GEOMETRY
BASED
MIDDLE
OR
APPEARANCE
BASED
RIGHT
REPRESENTATIONS
AND
ON
THE
VERTICAL
AXIS
BY
INCREASING
LEVEL
OF
ABSTRACTION
THE
WHOLE
FIGURE
SHOULD
BE
TAKEN
WITH
A
LARGE
GRAIN
OF
SALT
AS
THERE
ARE
MANY
ADDITIONAL
SUBTLE
CONNECTIONS
BETWEEN
TOPICS
NOT
ILLUSTRATED
HERE
FIGURE
SHOWS
A
ROUGH
LAYOUT
OF
THE
CONTENTS
OF
THIS
BOOK
SINCE
COMPUTER
VISION
INVOLVES
GOING
FROM
IMAGES
TO
A
STRUCTURAL
DESCRIPTION
OF
THE
SCENE
AND
COMPUTER
GRAPHICS
THE
CONVERSE
I
HAVE
POSITIONED
THE
CHAPTERS
HORIZONTALLY
IN
TERMS
OF
WHICH
MAJOR
COMPONENT
THEY
ADDRESS
IN
ADDITION
TO
VERTICALLY
ACCORDING
TO
THEIR
DEPENDENCE
GOING
FROM
LEFT
TO
RIGHT
WE
SEE
THE
MAJOR
COLUMN
HEADINGS
AS
IMAGES
WHICH
ARE
IN
NATURE
GEOMETRY
WHICH
ENCOMPASSES
DESCRIPTIONS
AND
PHOTOMETRY
WHICH
ENCOM
PASSES
OBJECT
APPEARANCE
AN
ALTERNATIVE
LABELING
FOR
THESE
LATTER
TWO
COULD
ALSO
BE
SHAPE
AND
APPEARANCE
SEE
E
G
CHAPTER
AND
KANG
SZELISKI
AND
ANANDAN
GOING
FROM
TOP
TO
BOTTOM
WE
SEE
INCREASING
LEVELS
OF
MODELING
AND
ABSTRACTION
AS
WELL
AS
TECH
NIQUES
THAT
BUILD
ON
PREVIOUSLY
DEVELOPED
ALGORITHMS
OF
COURSE
THIS
TAXONOMY
SHOULD
BE
TAKEN
WITH
A
LARGE
GRAIN
OF
SALT
AS
THE
PROCESSING
AND
DEPENDENCIES
IN
THIS
DIAGRAM
ARE
NOT
STRICTLY
SEQUENTIAL
AND
SUBTLE
ADDITIONAL
DEPENDENCIES
AND
RELATIONSHIPS
ALSO
EXIST
E
G
SOME
RECOGNITION
TECHNIQUES
MAKE
USE
OF
INFORMATION
THE
PLACEMENT
OF
TOPICS
ALONG
THE
HOR
IZONTAL
AXIS
SHOULD
ALSO
BE
TAKEN
LIGHTLY
AS
MOST
VISION
ALGORITHMS
INVOLVE
MAPPING
BETWEEN
AT
LEAST
TWO
DIFFERENT
REPRESENTATIONS
INTERSPERSED
THROUGHOUT
THE
BOOK
ARE
SAMPLE
APPLICATIONS
WHICH
RELATE
THE
ALGORITHMS
AND
MATHEMATICAL
MATERIAL
BEING
PRESENTED
IN
VARIOUS
CHAPTERS
TO
USEFUL
REAL
WORLD
APPLICA
TIONS
MANY
OF
THESE
APPLICATIONS
ARE
ALSO
PRESENTED
IN
THE
EXERCISES
SECTIONS
SO
THAT
STUDENTS
CAN
WRITE
THEIR
OWN
AT
THE
END
OF
EACH
SECTION
I
PROVIDE
A
SET
OF
EXERCISES
THAT
THE
STUDENTS
CAN
USE
TO
IMPLE
MENT
TEST
AND
REFINE
THE
ALGORITHMS
AND
TECHNIQUES
PRESENTED
IN
EACH
SECTION
SOME
OF
THE
EXERCISES
ARE
SUITABLE
AS
WRITTEN
HOMEWORK
ASSIGNMENTS
OTHERS
AS
SHORTER
ONE
WEEK
PROJECTS
AND
STILL
OTHERS
AS
OPEN
ENDED
RESEARCH
PROBLEMS
THAT
MAKE
FOR
CHALLENGING
FINAL
PROJECTS
MOTIVATED
STUDENTS
WHO
IMPLEMENT
A
REASONABLE
SUBSET
OF
THESE
EXERCISES
WILL
BY
THE
END
OF
THE
BOOK
HAVE
A
COMPUTER
VISION
SOFTWARE
LIBRARY
THAT
CAN
BE
USED
FOR
A
VARIETY
OF
INTERESTING
TASKS
AND
PROJECTS
AS
A
REFERENCE
BOOK
I
TRY
WHEREVER
POSSIBLE
TO
DISCUSS
WHICH
TECHNIQUES
AND
ALGORITHMS
WORK
WELL
IN
PRACTICE
AS
WELL
AS
PROVIDING
UP
TO
DATE
POINTERS
TO
THE
LATEST
RESEARCH
RESULTS
IN
THE
AREAS
THAT
I
COVER
THE
EXERCISES
CAN
BE
USED
TO
BUILD
UP
YOUR
OWN
PERSONAL
LIBRARY
OF
SELF
TESTED
AND
VALIDATED
VISION
ALGORITHMS
WHICH
IS
MORE
WORTHWHILE
IN
THE
LONG
TERM
ASSUMING
YOU
HAVE
THE
TIME
THAN
SIMPLY
PULLING
ALGORITHMS
OUT
OF
A
LIBRARY
WHOSE
PERFORMANCE
YOU
DO
NOT
REALLY
UNDERSTAND
THE
BOOK
BEGINS
IN
CHAPTER
WITH
A
REVIEW
OF
THE
IMAGE
FORMATION
PROCESSES
THAT
CREATE
THE
IMAGES
THAT
WE
SEE
AND
CAPTURE
UNDERSTANDING
THIS
PROCESS
IS
FUNDAMENTAL
IF
YOU
WANT
TO
TAKE
A
SCIENTIFIC
MODEL
BASED
APPROACH
TO
COMPUTER
VISION
STUDENTS
WHO
ARE
EAGER
TO
JUST
START
IMPLEMENTING
ALGORITHMS
OR
COURSES
THAT
HAVE
LIMITED
TIME
CAN
SKIP
AHEAD
TO
THE
FOR
AN
INTERESTING
COMPARISON
WITH
WHAT
IS
KNOWN
ABOUT
THE
HUMAN
VISUAL
SYSTEM
E
G
THE
LARGELY
PARALLEL
WHAT
AND
WHERE
PATHWAYS
SEE
SOME
TEXTBOOKS
ON
HUMAN
PERCEPTION
PALMER
LIVINGSTONE
N
IMAGE
FORMATION
IMAGE
PROCESSING
FEATURES
SEGMENTATION
STRUCTURE
FROM
MOTION
MOTION
STITCHING
COMPUTATIONAL
PHOTOGRAPHY
STEREO
SHAPE
IMAGE
BASED
RENDERING
RECOGNITION
FIGURE
A
PICTORIAL
SUMMARY
OF
THE
CHAPTER
CONTENTS
SOURCES
BROWN
SZELISKI
AND
WINDER
COMANICIU
AND
MEER
SNAVELY
SEITZ
AND
SZELISKI
NAGEL
AND
ENKELMANN
SZELISKI
AND
SHUM
DEBEVEC
AND
MALIK
GORTLER
GRZESZCZUK
SZELISKI
ET
AL
VIOLA
AND
JONES
SEE
THE
FIGURES
IN
THE
RESPEC
TIVE
CHAPTERS
FOR
COPYRIGHT
INFORMATION
NEXT
CHAPTER
AND
DIP
INTO
THIS
MATERIAL
LATER
IN
CHAPTER
WE
BREAK
DOWN
IMAGE
FORMATION
INTO
THREE
MAJOR
COMPONENTS
GEOMETRIC
IMAGE
FORMATION
SECTION
DEALS
WITH
POINTS
LINES
AND
PLANES
AND
HOW
THESE
ARE
MAPPED
ONTO
IMAGES
USING
PROJECTIVE
GEOMETRY
AND
OTHER
MODELS
INCLUDING
RADIAL
LENS
DISTORTION
PHOTOMETRIC
IMAGE
FORMATION
SECTION
COVERS
RADIOMETRY
WHICH
DESCRIBES
HOW
LIGHT
INTERACTS
WITH
SURFACES
IN
THE
WORLD
AND
OPTICS
WHICH
PROJECTS
LIGHT
ONTO
THE
SENSOR
PLANE
FINALLY
SECTION
COVERS
HOW
SENSORS
WORK
INCLUDING
TOPICS
SUCH
AS
SAMPLING
AND
ALIASING
COLOR
SENSING
AND
IN
CAMERA
COMPRESSION
CHAPTER
COVERS
IMAGE
PROCESSING
WHICH
IS
NEEDED
IN
ALMOST
ALL
COMPUTER
VISION
APPLI
CATIONS
THIS
INCLUDES
TOPICS
SUCH
AS
LINEAR
AND
NON
LINEAR
FILTERING
SECTION
THE
FOURIER
TRANSFORM
SECTION
IMAGE
PYRAMIDS
AND
WAVELETS
SECTION
GEOMETRIC
TRANSFORMA
TIONS
SUCH
AS
IMAGE
WARPING
SECTION
AND
GLOBAL
OPTIMIZATION
TECHNIQUES
SUCH
AS
REGU
LARIZATION
AND
MARKOV
RANDOM
FIELDS
MRFS
SECTION
WHILE
MOST
OF
THIS
MATERIAL
IS
COVERED
IN
COURSES
AND
TEXTBOOKS
ON
IMAGE
PROCESSING
THE
USE
OF
OPTIMIZATION
TECHNIQUES
IS
MORE
TYPICALLY
ASSOCIATED
WITH
COMPUTER
VISION
ALTHOUGH
MRFS
ARE
NOW
BEING
WIDELY
USED
IN
IMAGE
PROCESSING
AS
WELL
THE
SECTION
ON
MRFS
IS
ALSO
THE
FIRST
INTRODUCTION
TO
THE
USE
OF
BAYESIAN
INFERENCE
TECHNIQUES
WHICH
ARE
COVERED
AT
A
MORE
ABSTRACT
LEVEL
IN
APPENDIX
B
CHAPTER
ALSO
PRESENTS
APPLICATIONS
SUCH
AS
SEAMLESS
IMAGE
BLENDING
AND
IMAGE
RESTORATION
IN
CHAPTER
WE
COVER
FEATURE
DETECTION
AND
MATCHING
A
LOT
OF
CURRENT
RECONSTRUCTION
AND
RECOGNITION
TECHNIQUES
ARE
BUILT
ON
EXTRACTING
AND
MATCHING
FEATURE
POINTS
SECTION
SO
THIS
IS
A
FUNDAMENTAL
TECHNIQUE
REQUIRED
BY
MANY
SUBSEQUENT
CHAPTERS
CHAPTERS
AND
WE
ALSO
COVER
EDGE
AND
STRAIGHT
LINE
DETECTION
IN
SECTIONS
AND
CHAPTER
COVERS
REGION
SEGMENTATION
TECHNIQUES
INCLUDING
ACTIVE
CONTOUR
DETECTION
AND
TRACKING
SECTION
SEGMENTATION
TECHNIQUES
INCLUDE
TOP
DOWN
SPLIT
AND
BOTTOM
UP
MERGE
TECHNIQUES
MEAN
SHIFT
TECHNIQUES
THAT
FIND
MODES
OF
CLUSTERS
AND
VARIOUS
GRAPH
BASED
SEGMENTATION
APPROACHES
ALL
OF
THESE
TECHNIQUES
ARE
ESSENTIAL
BUILDING
BLOCKS
THAT
ARE
WIDELY
USED
IN
A
VARIETY
OF
APPLICATIONS
INCLUDING
PERFORMANCE
DRIVEN
ANIMATION
INTERACTIVE
IMAGE
EDITING
AND
RECOGNITION
IN
CHAPTER
WE
COVER
GEOMETRIC
ALIGNMENT
AND
CAMERA
CALIBRATION
WE
INTRODUCE
THE
BASIC
TECHNIQUES
OF
FEATURE
BASED
ALIGNMENT
IN
SECTION
AND
SHOW
HOW
THIS
PROBLEM
CAN
BE
SOLVED
USING
EITHER
LINEAR
OR
NON
LINEAR
LEAST
SQUARES
DEPENDING
ON
THE
MOTION
INVOLVED
WE
ALSO
INTRODUCE
ADDITIONAL
CONCEPTS
SUCH
AS
UNCERTAINTY
WEIGHTING
AND
ROBUST
REGRESSION
WHICH
ARE
ESSENTIAL
TO
MAKING
REAL
WORLD
SYSTEMS
WORK
FEATURE
BASED
ALIGNMENT
IS
THEN
USED
AS
A
BUILDING
BLOCK
FOR
POSE
ESTIMATION
EXTRINSIC
CALIBRATION
IN
SECTION
AND
CAMERA
INTRINSIC
CALIBRATION
IN
SECTION
CHAPTER
ALSO
DESCRIBES
APPLICATIONS
OF
THESE
TECHNIQUES
TO
PHOTO
ALIGNMENT
FOR
FLIP
BOOK
ANIMATIONS
POSE
ESTIMATION
FROM
A
HAND
HELD
CAMERA
AND
SINGLE
VIEW
RECONSTRUCTION
OF
BUILDING
MODELS
CHAPTER
COVERS
THE
TOPIC
OF
STRUCTURE
FROM
MOTION
WHICH
INVOLVES
THE
SIMULTANEOUS
RECOVERY
OF
CAMERA
MOTION
AND
SCENE
STRUCTURE
FROM
A
COLLECTION
OF
TRACKED
FEA
TURES
THIS
CHAPTER
BEGINS
WITH
THE
EASIER
PROBLEM
OF
POINT
TRIANGULATION
SECTION
WHICH
IS
THE
RECONSTRUCTION
OF
POINTS
FROM
MATCHED
FEATURES
WHEN
THE
CAMERA
POSITIONS
ARE
KNOWN
IT
THEN
DESCRIBES
TWO
FRAME
STRUCTURE
FROM
MOTION
SECTION
FOR
WHICH
AL
GEBRAIC
TECHNIQUES
EXIST
AS
WELL
AS
ROBUST
SAMPLING
TECHNIQUES
SUCH
AS
RANSAC
THAT
CAN
DISCOUNT
ERRONEOUS
FEATURE
MATCHES
THE
SECOND
HALF
OF
CHAPTER
DESCRIBES
TECHNIQUES
FOR
MULTI
FRAME
STRUCTURE
FROM
MOTION
INCLUDING
FACTORIZATION
SECTION
BUNDLE
ADJUSTMENT
SECTION
AND
CONSTRAINED
MOTION
AND
STRUCTURE
MODELS
SECTION
IT
ALSO
PRESENTS
APPLICATIONS
IN
VIEW
MORPHING
SPARSE
MODEL
CONSTRUCTION
AND
MATCH
MOVE
IN
CHAPTER
WE
GO
BACK
TO
A
TOPIC
THAT
DEALS
DIRECTLY
WITH
IMAGE
INTENSITIES
AS
OP
POSED
TO
FEATURE
TRACKS
NAMELY
DENSE
INTENSITY
BASED
MOTION
ESTIMATION
OPTICAL
FLOW
WE
START
WITH
THE
SIMPLEST
POSSIBLE
MOTION
MODELS
TRANSLATIONAL
MOTION
SECTION
AND
COVER
TOPICS
SUCH
AS
HIERARCHICAL
COARSE
TO
FINE
MOTION
ESTIMATION
FOURIER
BASED
TECHNIQUES
AND
ITERATIVE
REFINEMENT
WE
THEN
PRESENT
PARAMETRIC
MOTION
MODELS
WHICH
CAN
BE
USED
TO
COM
PENSATE
FOR
CAMERA
ROTATION
AND
ZOOMING
AS
WELL
AS
AFFINE
OR
PLANAR
PERSPECTIVE
MOTION
SEC
TION
THIS
IS
THEN
GENERALIZED
TO
SPLINE
BASED
MOTION
MODELS
SECTION
AND
FINALLY
TO
GENERAL
PER
PIXEL
OPTICAL
FLOW
SECTION
INCLUDING
LAYERED
AND
LEARNED
MOTION
MODELS
SECTION
APPLICATIONS
OF
THESE
TECHNIQUES
INCLUDE
AUTOMATED
MORPHING
FRAME
INTERPO
LATION
SLOW
MOTION
AND
MOTION
BASED
USER
INTERFACES
CHAPTER
IS
DEVOTED
TO
IMAGE
STITCHING
I
E
THE
CONSTRUCTION
OF
LARGE
PANORAMAS
AND
COM
POSITES
WHILE
STITCHING
IS
JUST
ONE
EXAMPLE
OF
COMPUTATION
PHOTOGRAPHY
SEE
CHAPTER
THERE
IS
ENOUGH
DEPTH
HERE
TO
WARRANT
A
SEPARATE
CHAPTER
WE
START
BY
DISCUSSING
VARIOUS
POS
SIBLE
MOTION
MODELS
SECTION
INCLUDING
PLANAR
MOTION
AND
PURE
CAMERA
ROTATION
WE
THEN
DISCUSS
GLOBAL
ALIGNMENT
SECTION
WHICH
IS
A
SPECIAL
SIMPLIFIED
CASE
OF
GENERAL
BUNDLE
ADJUSTMENT
AND
THEN
PRESENT
PANORAMA
RECOGNITION
I
E
TECHNIQUES
FOR
AUTOMATICALLY
DISCOVERING
WHICH
IMAGES
ACTUALLY
FORM
OVERLAPPING
PANORAMAS
FINALLY
WE
COVER
THE
TOPICS
OF
IMAGE
COMPOSITING
AND
BLENDING
SECTION
WHICH
INVOLVE
BOTH
SELECTING
WHICH
PIXELS
FROM
WHICH
IMAGES
TO
USE
AND
BLENDING
THEM
TOGETHER
SO
AS
TO
DISGUISE
EXPOSURE
DIFFERENCES
IMAGE
STITCHING
IS
A
WONDERFUL
APPLICATION
THAT
TIES
TOGETHER
MOST
OF
THE
MATERIAL
COVERED
IN
EARLIER
PARTS
OF
THIS
BOOK
IT
ALSO
MAKES
FOR
A
GOOD
MID
TERM
COURSE
PROJECT
THAT
CAN
BUILD
ON
PREVIOUSLY
DEVELOPED
TECHNIQUES
SUCH
AS
IMAGE
WARPING
AND
FEATURE
DETECTION
AND
MATCH
ING
CHAPTER
ALSO
PRESENTS
MORE
SPECIALIZED
VARIANTS
OF
STITCHING
SUCH
AS
WHITEBOARD
AND
DOCUMENT
SCANNING
VIDEO
SUMMARIZATION
PANOGRAPHY
FULL
SPHERICAL
PANORAMAS
AND
INTERACTIVE
PHOTOMONTAGE
FOR
BLENDING
REPEATED
ACTION
SHOTS
TOGETHER
CHAPTER
PRESENTS
ADDITIONAL
EXAMPLES
OF
COMPUTATIONAL
PHOTOGRAPHY
WHICH
IS
THE
PRO
CESS
OF
CREATING
NEW
IMAGES
FROM
ONE
OR
MORE
INPUT
PHOTOGRAPHS
OFTEN
BASED
ON
THE
CAREFUL
MODELING
AND
CALIBRATION
OF
THE
IMAGE
FORMATION
PROCESS
SECTION
COMPUTATIONAL
PHO
TOGRAPHY
TECHNIQUES
INCLUDE
MERGING
MULTIPLE
EXPOSURES
TO
CREATE
HIGH
DYNAMIC
RANGE
IMAGES
SECTION
INCREASING
IMAGE
RESOLUTION
THROUGH
BLUR
REMOVAL
AND
SUPER
RESOLUTION
SEC
TION
AND
IMAGE
EDITING
AND
COMPOSITING
OPERATIONS
SECTION
WE
ALSO
COVER
THE
TOPICS
OF
TEXTURE
ANALYSIS
SYNTHESIS
AND
INPAINTING
HOLE
FILLING
IN
SECTION
AS
WELL
AS
NON
PHOTOREALISTIC
RENDERING
SECTION
IN
CHAPTER
WE
TURN
TO
THE
ISSUE
OF
STEREO
CORRESPONDENCE
WHICH
CAN
BE
THOUGHT
OF
AS
A
SPECIAL
CASE
OF
MOTION
ESTIMATION
WHERE
THE
CAMERA
POSITIONS
ARE
ALREADY
KNOWN
SEC
TION
THIS
ADDITIONAL
KNOWLEDGE
ENABLES
STEREO
ALGORITHMS
TO
SEARCH
OVER
A
MUCH
SMALLER
SPACE
OF
CORRESPONDENCES
AND
IN
MANY
CASES
TO
PRODUCE
DENSE
DEPTH
ESTIMATES
THAT
CAN
BE
CONVERTED
INTO
VISIBLE
SURFACE
MODELS
SECTION
WE
ALSO
COVER
MULTI
VIEW
STEREO
ALGORITHMS
THAT
BUILD
A
TRUE
SURFACE
REPRESENTATION
INSTEAD
OF
JUST
A
SINGLE
DEPTH
MAP
SECTION
APPLICATIONS
OF
STEREO
MATCHING
INCLUDE
HEAD
AND
GAZE
TRACKING
AS
WELL
AS
DEPTH
BASED
BACKGROUND
REPLACEMENT
Z
KEYING
CHAPTER
COVERS
ADDITIONAL
SHAPE
AND
APPEARANCE
MODELING
TECHNIQUES
THESE
IN
CLUDE
CLASSIC
SHAPE
FROM
X
TECHNIQUES
SUCH
AS
SHAPE
FROM
SHADING
SHAPE
FROM
TEXTURE
AND
SHAPE
FROM
FOCUS
SECTION
AS
WELL
AS
SHAPE
FROM
SMOOTH
OCCLUDING
CONTOURS
SEC
TION
AND
SILHOUETTES
SECTION
AN
ALTERNATIVE
TO
ALL
OF
THESE
PASSIVE
COMPUTER
VISION
TECHNIQUES
IS
TO
USE
ACTIVE
RANGEFINDING
SECTION
I
E
TO
PROJECT
PATTERNED
LIGHT
ONTO
SCENES
AND
RECOVER
THE
GEOMETRY
THROUGH
TRIANGULATION
PROCESSING
ALL
OF
THESE
REPRESENTATIONS
OFTEN
INVOLVES
INTERPOLATING
OR
SIMPLIFYING
THE
GEOMETRY
SECTION
OR
USING
ALTERNATIVE
REPRESENTATIONS
SUCH
AS
SURFACE
POINT
SETS
SECTION
THE
COLLECTION
OF
TECHNIQUES
FOR
GOING
FROM
ONE
OR
MORE
IMAGES
TO
PARTIAL
OR
FULL
MODELS
IS
OFTEN
CALLED
IMAGE
BASED
MODELING
OR
PHOTOGRAPHY
SECTION
EXAMINES
THREE
MORE
SPECIALIZED
APPLICATION
AREAS
ARCHITECTURE
FACES
AND
HUMAN
BODIES
WHICH
CAN
USE
MODEL
BASED
RECONSTRUCTION
TO
FIT
PARAMETERIZED
MODELS
TO
THE
SENSED
DATA
SECTION
EXAMINES
THE
TOPIC
OF
APPEARANCE
MODELING
I
E
TECHNIQUES
FOR
ESTIMATING
THE
TEXTURE
MAPS
ALBEDOS
OR
EVEN
SOMETIMES
COMPLETE
BI
DIRECTIONAL
REFLECTANCE
DISTRIBUTION
FUNCTIONS
BRDFS
THAT
DESCRIBE
THE
APPEARANCE
OF
SURFACES
IN
CHAPTER
WE
DISCUSS
THE
LARGE
NUMBER
OF
IMAGE
BASED
RENDERING
TECHNIQUES
THAT
HAVE
BEEN
DEVELOPED
IN
THE
LAST
TWO
DECADES
INCLUDING
SIMPLER
TECHNIQUES
SUCH
AS
VIEW
IN
TERPOLATION
SECTION
LAYERED
DEPTH
IMAGES
SECTION
AND
SPRITES
AND
LAYERS
SEC
TION
AS
WELL
AS
THE
MORE
GENERAL
FRAMEWORK
OF
LIGHT
FIELDS
AND
LUMIGRAPHS
SEC
TION
AND
HIGHER
ORDER
FIELDS
SUCH
AS
ENVIRONMENT
MATTES
SECTION
APPLICATIONS
OF
THESE
TECHNIQUES
INCLUDE
NAVIGATING
COLLECTIONS
OF
PHOTOGRAPHS
USING
PHOTO
TOURISM
AND
VIEWING
MODELS
AS
OBJECT
MOVIES
IN
CHAPTER
WE
ALSO
DISCUSS
VIDEO
BASED
RENDERING
WHICH
IS
THE
TEMPORAL
EXTENSION
OF
IMAGE
BASED
RENDERING
THE
TOPICS
WE
COVER
INCLUDE
VIDEO
BASED
ANIMATION
SECTION
PERIODIC
VIDEO
TURNED
INTO
VIDEO
TEXTURES
SECTION
AND
VIDEO
CONSTRUCTED
FROM
MULTIPLE
VIDEO
STREAMS
SECTION
APPLICATIONS
OF
THESE
TECHNIQUES
INCLUDE
VIDEO
DE
NOISING
MORPHING
AND
TOURS
BASED
ON
VIDEO
WEEK
MATERIAL
PROJECT
CHAPTER
IMAGE
FORMATION
CHAPTER
IMAGE
PROCESSING
CHAPTER
FEATURE
DETECTION
AND
MATCHING
CHAPTER
FEATURE
BASED
ALIGNMENT
CHAPTER
IMAGE
STITCHING
CHAPTER
DENSE
MOTION
ESTIMATION
CHAPTER
STRUCTURE
FROM
MOTION
PP
CHAPTER
RECOGNITION
CHAPTER
COMPUTATIONAL
PHOTOGRAPHY
CHAPTER
STEREO
CORRESPONDENCE
CHAPTER
RECONSTRUCTION
CHAPTER
IMAGE
BASED
RENDERING
FINAL
PROJECT
PRESENTATIONS
FP
TABLE
SAMPLE
SYLLABI
FOR
WEEK
AND
WEEK
COURSES
THE
WEEKS
IN
PARENTHESES
ARE
NOT
USED
IN
THE
SHORTER
VERSION
AND
ARE
TWO
EARLY
TERM
MINI
PROJECTS
PP
IS
WHEN
THE
STUDENT
SELECTED
FINAL
PROJECT
PROPOSALS
ARE
DUE
AND
FP
IS
THE
FINAL
PROJECT
PRESENTATIONS
CHAPTER
DESCRIBES
DIFFERENT
APPROACHES
TO
RECOGNITION
IT
BEGINS
WITH
TECHNIQUES
FOR
DETECTING
AND
RECOGNIZING
FACES
SECTIONS
AND
THEN
LOOKS
AT
TECHNIQUES
FOR
FINDING
AND
RECOGNIZING
PARTICULAR
OBJECTS
INSTANCE
RECOGNITION
IN
SECTION
NEXT
WE
COVER
THE
MOST
DIFFICULT
VARIANT
OF
RECOGNITION
NAMELY
THE
RECOGNITION
OF
BROAD
CATEGORIES
SUCH
AS
CARS
MOTORCYCLES
HORSES
AND
OTHER
ANIMALS
SECTION
AND
THE
ROLE
THAT
SCENE
CONTEXT
PLAYS
IN
RECOGNITION
SECTION
TO
SUPPORT
THE
BOOK
USE
AS
A
TEXTBOOK
THE
APPENDICES
AND
ASSOCIATED
WEB
SITE
CONTAIN
MORE
DETAILED
MATHEMATICAL
TOPICS
AND
ADDITIONAL
MATERIAL
APPENDIX
A
COVERS
LINEAR
ALGEBRA
AND
NUMERICAL
TECHNIQUES
INCLUDING
MATRIX
ALGEBRA
LEAST
SQUARES
AND
ITERATIVE
TECHNIQUES
APPENDIX
B
COVERS
BAYESIAN
ESTIMATION
THEORY
INCLUDING
MAXIMUM
LIKELIHOOD
ESTIMATION
ROBUST
STATISTICS
MARKOV
RANDOM
FIELDS
AND
UNCERTAINTY
MODELING
APPENDIX
C
DESCRIBES
THE
SUPPLEMENTARY
MATERIAL
AVAILABLE
TO
COMPLEMENT
THIS
BOOK
INCLUDING
IMAGES
AND
DATA
SETS
POINTERS
TO
SOFTWARE
COURSE
SLIDES
AND
AN
ON
LINE
BIBLIOGRAPHY
SAMPLE
SYLLABUS
TEACHING
ALL
OF
THE
MATERIAL
COVERED
IN
THIS
BOOK
IN
A
SINGLE
QUARTER
OR
SEMESTER
COURSE
IS
A
HERCULEAN
TASK
AND
LIKELY
ONE
NOT
WORTH
ATTEMPTING
IT
IS
BETTER
TO
SIMPLY
PICK
AND
CHOOSE
A
NOTE
ON
NOTATION
TOPICS
RELATED
TO
THE
LECTURER
PREFERRED
EMPHASIS
AND
TAILORED
TO
THE
SET
OF
MINI
PROJECTS
ENVISIONED
FOR
THE
STUDENTS
STEVE
SEITZ
AND
I
HAVE
SUCCESSFULLY
USED
A
WEEK
SYLLABUS
SIMILAR
TO
THE
ONE
SHOWN
IN
TABLE
OMITTING
THE
PARENTHESIZED
WEEKS
AS
BOTH
AN
UNDERGRADUATE
AND
A
GRADUATE
LEVEL
COURSE
IN
COMPUTER
VISION
THE
UNDERGRADUATE
TENDS
TO
GO
LIGHTER
ON
THE
MATHEMATICS
AND
TAKES
MORE
TIME
REVIEWING
BASICS
WHILE
THE
GRADUATE
LEVEL
DIVES
MORE
DEEPLY
INTO
TECHNIQUES
AND
ASSUMES
THE
STUDENTS
ALREADY
HAVE
A
DECENT
GROUNDING
IN
EITHER
VISION
OR
RELATED
MATHEMATICAL
TECHNIQUES
SEE
ALSO
THE
INTRODUCTION
TO
COMPUTER
VISION
COURSE
AT
STANFORD
WHICH
USES
A
SIMILAR
CURRICULUM
RELATED
COURSES
HAVE
ALSO
BEEN
TAUGHT
ON
THE
TOPICS
OF
AND
COMPUTATIONAL
PHOTOGRAPHY
WHEN
STEVE
AND
I
TEACH
THE
COURSE
WE
PREFER
TO
GIVE
THE
STUDENTS
SEVERAL
SMALL
PROGRAM
MING
PROJECTS
EARLY
IN
THE
COURSE
RATHER
THAN
FOCUSING
ON
WRITTEN
HOMEWORK
OR
QUIZZES
WITH
A
SUITABLE
CHOICE
OF
TOPICS
IT
IS
POSSIBLE
FOR
THESE
PROJECTS
TO
BUILD
ON
EACH
OTHER
FOR
EXAM
PLE
INTRODUCING
FEATURE
MATCHING
EARLY
ON
CAN
BE
USED
IN
A
SECOND
ASSIGNMENT
TO
DO
IMAGE
ALIGNMENT
AND
STITCHING
ALTERNATIVELY
DIRECT
OPTICAL
FLOW
TECHNIQUES
CAN
BE
USED
TO
DO
THE
ALIGNMENT
AND
MORE
FOCUS
CAN
BE
PUT
ON
EITHER
GRAPH
CUT
SEAM
SELECTION
OR
MULTI
RESOLUTION
BLENDING
TECHNIQUES
WE
ALSO
ASK
THE
STUDENTS
TO
PROPOSE
A
FINAL
PROJECT
WE
PROVIDE
A
SET
OF
SUGGESTED
TOPICS
FOR
THOSE
WHO
NEED
IDEAS
BY
THE
MIDDLE
OF
THE
COURSE
AND
RESERVE
THE
LAST
WEEK
OF
THE
CLASS
FOR
STUDENT
PRESENTATIONS
WITH
ANY
LUCK
SOME
OF
THESE
FINAL
PROJECTS
CAN
ACTUALLY
TURN
INTO
CONFERENCE
SUBMISSIONS
NO
MATTER
HOW
YOU
DECIDE
TO
STRUCTURE
THE
COURSE
OR
HOW
YOU
CHOOSE
TO
USE
THIS
BOOK
I
ENCOURAGE
YOU
TO
TRY
AT
LEAST
A
FEW
SMALL
PROGRAMMING
TASKS
TO
GET
A
GOOD
FEEL
FOR
HOW
VISION
TECHNIQUES
WORK
AND
WHEN
THEY
DO
NOT
BETTER
YET
PICK
TOPICS
THAT
ARE
FUN
AND
CAN
BE
USED
ON
YOUR
OWN
PHOTOGRAPHS
AND
TRY
TO
PUSH
YOUR
CREATIVE
BOUNDARIES
TO
COME
UP
WITH
SURPRISING
RESULTS
A
NOTE
ON
NOTATION
FOR
BETTER
OR
WORSE
THE
NOTATION
FOUND
IN
COMPUTER
VISION
AND
MULTI
VIEW
GEOMETRY
TEXTBOOKS
TENDS
TO
VARY
ALL
OVER
THE
MAP
FAUGERAS
HARTLEY
AND
ZISSERMAN
GIROD
GREINER
AND
NIEMANN
FAUGERAS
AND
LUONG
FORSYTH
AND
PONCE
IN
THIS
BOOK
I
USE
THE
CONVENTION
I
FIRST
LEARNED
IN
MY
HIGH
SCHOOL
PHYSICS
CLASS
AND
LATER
MULTI
VARIATE
HTTP
WWW
CS
WASHINGTON
EDU
EDUCATION
COURSES
HTTP
WWW
CS
WASHINGTON
EDU
EDUCATION
COURSES
VISION
STANFORD
EDU
TEACHING
HTTP
WWW
CS
WASHINGTON
EDU
EDUCATION
COURSES
HTTP
GRAPHICS
CS
CMU
EDU
COURSES
CALCULUS
AND
COMPUTER
GRAPHICS
COURSES
WHICH
IS
THAT
VECTORS
V
ARE
LOWER
CASE
BOLD
MATRICES
M
ARE
UPPER
CASE
BOLD
AND
SCALARS
T
ARE
MIXED
CASE
ITALIC
UNLESS
OTHERWISE
NOTED
VECTORS
OPERATE
AS
COLUMN
VECTORS
I
E
THEY
POST
MULTIPLY
MATRICES
M
V
ALTHOUGH
THEY
ARE
SOMETIMES
WRITTEN
AS
COMMA
SEPARATED
PARENTHESIZED
LISTS
X
X
Y
INSTEAD
OF
BRACKETED
COLUMN
VECTORS
X
X
Y
T
SOME
COMMONLY
USED
MATRICES
ARE
R
FOR
ROTATIONS
K
FOR
CALIBRATION
MATRICES
AND
I
FOR
THE
IDENTITY
MATRIX
HOMOGENEOUS
COORDINATES
SECTION
ARE
DENOTED
WITH
A
TILDE
OVER
THE
VECTOR
E
G
X
X
Y
W
W
X
Y
W
X
IN
THE
CROSS
PRODUCT
OPERATOR
IN
MATRIX
FORM
IS
DENOTED
BY
ADDITIONAL
READING
THIS
BOOK
ATTEMPTS
TO
BE
SELF
CONTAINED
SO
THAT
STUDENTS
CAN
IMPLEMENT
THE
BASIC
ASSIGNMENTS
AND
ALGORITHMS
DESCRIBED
HERE
WITHOUT
THE
NEED
FOR
OUTSIDE
REFERENCES
HOWEVER
IT
DOES
PRE
SUPPOSE
A
GENERAL
FAMILIARITY
WITH
BASIC
CONCEPTS
IN
LINEAR
ALGEBRA
AND
NUMERICAL
TECHNIQUES
WHICH
ARE
REVIEWED
IN
APPENDIX
A
AND
IMAGE
PROCESSING
WHICH
IS
REVIEWED
IN
CHAPTER
STUDENTS
WHO
WANT
TO
DELVE
MORE
DEEPLY
INTO
THESE
TOPICS
CAN
LOOK
IN
GOLUB
AND
VAN
LOAN
FOR
MATRIX
ALGEBRA
AND
STRANG
FOR
LINEAR
ALGEBRA
IN
IMAGE
PROCESSING
THERE
ARE
A
NUMBER
OF
POPULAR
TEXTBOOKS
INCLUDING
CRANE
GOMES
AND
VELHO
JA
HNE
PRATT
RUSS
BURGER
AND
BURGE
GONZALES
AND
WOODS
FOR
COMPUTER
GRAPHICS
POPULAR
TEXTS
INCLUDE
FOLEY
VAN
DAM
FEINER
ET
AL
WATT
WITH
GLASSNER
PROVIDING
A
MORE
IN
DEPTH
LOOK
AT
IMAGE
FORMATION
AND
RENDERING
FOR
STATISTICS
AND
MACHINE
LEARNING
CHRIS
BISHOP
BOOK
IS
A
WONDERFUL
AND
COMPREHEN
SIVE
INTRODUCTION
WITH
A
WEALTH
OF
EXERCISES
STUDENTS
MAY
ALSO
WANT
TO
LOOK
IN
OTHER
TEXTBOOKS
ON
COMPUTER
VISION
FOR
MATERIAL
THAT
WE
DO
NOT
COVER
HERE
AS
WELL
AS
FOR
ADDITIONAL
PROJECT
IDEAS
BALLARD
AND
BROWN
FAUGERAS
NALWA
TRUCCO
AND
VERRI
FORSYTH
AND
PONCE
THERE
IS
HOWEVER
NO
SUBSTITUTE
FOR
READING
THE
LATEST
RESEARCH
LITERATURE
BOTH
FOR
THE
LAT
EST
IDEAS
AND
TECHNIQUES
AND
FOR
THE
MOST
UP
TO
DATE
REFERENCES
TO
RELATED
LITERATURE
IN
THIS
BOOK
I
HAVE
ATTEMPTED
TO
CITE
THE
MOST
RECENT
WORK
IN
EACH
FIELD
SO
THAT
STUDENTS
CAN
READ
THEM
DIRECTLY
AND
USE
THEM
AS
INSPIRATION
FOR
THEIR
OWN
WORK
BROWSING
THE
LAST
FEW
YEARS
CON
FERENCE
PROCEEDINGS
FROM
THE
MAJOR
VISION
AND
GRAPHICS
CONFERENCES
SUCH
AS
CVPR
ECCV
ICCV
AND
SIGGRAPH
WILL
PROVIDE
A
WEALTH
OF
NEW
IDEAS
THE
TUTORIALS
OFFERED
AT
THESE
CONFERENCES
FOR
WHICH
SLIDES
OR
NOTES
ARE
OFTEN
AVAILABLE
ON
LINE
ARE
ALSO
AN
INVALUABLE
RE
SOURCE
FOR
A
COMPREHENSIVE
BIBLIOGRAPHY
AND
TAXONOMY
OF
COMPUTER
VISION
RESEARCH
KEITH
PRICE
ANNOTATED
COM
PUTER
VISION
BIBLIOGRAPHY
HTTP
WWW
VISIONBIB
COM
BIBLIOGRAPHY
CONTENTS
HTML
IS
AN
INVALUABLE
RESOURCE
CHAPTER
IMAGE
FORMATION
GEOMETRIC
PRIMITIVES
AND
TRANSFORMATIONS
GEOMETRIC
PRIMITIVES
TRANSFORMATIONS
TRANSFORMATIONS
ROTATIONS
TO
PROJECTIONS
LENS
DISTORTIONS
PHOTOMETRIC
IMAGE
FORMATION
LIGHTING
REFLECTANCE
AND
SHADING
OPTICS
THE
DIGITAL
CAMERA
SAMPLING
AND
ALIASING
COLOR
COMPRESSION
ADDITIONAL
READING
EXERCISES
A
B
ZI
ZO
C
D
FIGURE
A
FEW
COMPONENTS
OF
THE
IMAGE
FORMATION
PROCESS
A
PERSPECTIVE
PROJECTION
B
LIGHT
SCATTERING
WHEN
HITTING
A
SURFACE
C
LENS
OPTICS
D
BAYER
COLOR
FILTER
ARRAY
BEFORE
WE
CAN
INTELLIGENTLY
ANALYZE
AND
MANIPULATE
IMAGES
WE
NEED
TO
ESTABLISH
A
VOCABULARY
FOR
DESCRIBING
THE
GEOMETRY
OF
A
SCENE
WE
ALSO
NEED
TO
UNDERSTAND
THE
IMAGE
FORMATION
PROCESS
THAT
PRODUCED
A
PARTICULAR
IMAGE
GIVEN
A
SET
OF
LIGHTING
CONDITIONS
SCENE
GEOMETRY
SURFACE
PROPERTIES
AND
CAMERA
OPTICS
IN
THIS
CHAPTER
WE
PRESENT
A
SIMPLIFIED
MODEL
OF
SUCH
AN
IMAGE
FORMATION
PROCESS
SECTION
INTRODUCES
THE
BASIC
GEOMETRIC
PRIMITIVES
USED
THROUGHOUT
THE
BOOK
POINTS
LINES
AND
PLANES
AND
THE
GEOMETRIC
TRANSFORMATIONS
THAT
PROJECT
THESE
QUANTITIES
INTO
IMAGE
FEATURES
FIGURE
SECTION
DESCRIBES
HOW
LIGHTING
SURFACE
PROPERTIES
FIG
URE
AND
CAMERA
OPTICS
FIGURE
INTERACT
IN
ORDER
TO
PRODUCE
THE
COLOR
VALUES
THAT
FALL
ONTO
THE
IMAGE
SENSOR
SECTION
DESCRIBES
HOW
CONTINUOUS
COLOR
IMAGES
ARE
TURNED
INTO
DISCRETE
DIGITAL
SAMPLES
INSIDE
THE
IMAGE
SENSOR
FIGURE
AND
HOW
TO
AVOID
OR
AT
LEAST
CHARACTERIZE
SAMPLING
DEFICIENCIES
SUCH
AS
ALIASING
THE
MATERIAL
COVERED
IN
THIS
CHAPTER
IS
BUT
A
BRIEF
SUMMARY
OF
A
VERY
RICH
AND
DEEP
SET
OF
TOPICS
TRADITIONALLY
COVERED
IN
A
NUMBER
OF
SEPARATE
FIELDS
A
MORE
THOROUGH
INTRODUCTION
TO
THE
GEOMETRY
OF
POINTS
LINES
PLANES
AND
PROJECTIONS
CAN
BE
FOUND
IN
TEXTBOOKS
ON
MULTI
VIEW
GEOMETRY
HARTLEY
AND
ZISSERMAN
FAUGERAS
AND
LUONG
AND
COMPUTER
GRAPHICS
FOLEY
VAN
DAM
FEINER
ET
AL
THE
IMAGE
FORMATION
SYNTHESIS
PROCESS
IS
TRADITIONALLY
TAUGHT
AS
PART
OF
A
COMPUTER
GRAPHICS
CURRICULUM
FOLEY
VAN
DAM
FEINER
ET
AL
GLASS
NER
WATT
SHIRLEY
BUT
IT
IS
ALSO
STUDIED
IN
PHYSICS
BASED
COMPUTER
VISION
WOLFF
SHAFER
AND
HEALEY
THE
BEHAVIOR
OF
CAMERA
LENS
SYSTEMS
IS
STUDIED
IN
OPTICS
MO
LLER
HECHT
RAY
TWO
GOOD
BOOKS
ON
COLOR
THEORY
ARE
WYSZECKI
AND
STILES
HEALEY
AND
SHAFER
WITH
LIVINGSTONE
PROVIDING
A
MORE
FUN
AND
IN
FORMAL
INTRODUCTION
TO
THE
TOPIC
OF
COLOR
PERCEPTION
TOPICS
RELATING
TO
SAMPLING
AND
ALIASING
ARE
COVERED
IN
TEXTBOOKS
ON
SIGNAL
AND
IMAGE
PROCESSING
CRANE
JA
HNE
OPPEN
HEIM
AND
SCHAFER
OPPENHEIM
SCHAFER
AND
BUCK
PRATT
RUSS
BURGER
AND
BURGE
GONZALES
AND
WOODS
A
NOTE
TO
STUDENTS
IF
YOU
HAVE
ALREADY
STUDIED
COMPUTER
GRAPHICS
YOU
MAY
WANT
TO
SKIM
THE
MATERIAL
IN
SECTION
ALTHOUGH
THE
SECTIONS
ON
PROJECTIVE
DEPTH
AND
OBJECT
CENTERED
PROJECTION
NEAR
THE
END
OF
SECTION
MAY
BE
NEW
TO
YOU
SIMILARLY
PHYSICS
STUDENTS
AS
WELL
AS
COMPUTER
GRAPHICS
STUDENTS
WILL
MOSTLY
BE
FAMILIAR
WITH
SECTION
FINALLY
STUDENTS
WITH
A
GOOD
BACKGROUND
IN
IMAGE
PROCESSING
WILL
ALREADY
BE
FAMILIAR
WITH
SAMPLING
ISSUES
SECTION
AS
WELL
AS
SOME
OF
THE
MATERIAL
IN
CHAPTER
GEOMETRIC
PRIMITIVES
AND
TRANSFORMATIONS
IN
THIS
SECTION
WE
INTRODUCE
THE
BASIC
AND
PRIMITIVES
USED
IN
THIS
TEXTBOOK
NAMELY
POINTS
LINES
AND
PLANES
WE
ALSO
DESCRIBE
HOW
FEATURES
ARE
PROJECTED
INTO
FEATURES
MORE
DETAILED
DESCRIPTIONS
OF
THESE
TOPICS
ALONG
WITH
A
GENTLER
AND
MORE
INTUITIVE
INTRODUC
TION
CAN
BE
FOUND
IN
TEXTBOOKS
ON
MULTIPLE
VIEW
GEOMETRY
HARTLEY
AND
ZISSERMAN
FAUGERAS
AND
LUONG
GEOMETRIC
PRIMITIVES
GEOMETRIC
PRIMITIVES
FORM
THE
BASIC
BUILDING
BLOCKS
USED
TO
DESCRIBE
THREE
DIMENSIONAL
SHAPES
IN
THIS
SECTION
WE
INTRODUCE
POINTS
LINES
AND
PLANES
LATER
SECTIONS
OF
THE
BOOK
DISCUSS
CURVES
SECTIONS
AND
SURFACES
SECTION
AND
VOLUMES
SECTION
POINTS
POINTS
PIXEL
COORDINATES
IN
AN
IMAGE
CAN
BE
DENOTED
USING
A
PAIR
OF
VALUES
X
X
Y
OR
ALTERNATIVELY
X
R
X
L
AS
STATED
IN
THE
INTRODUCTION
WE
USE
THE
NOTATION
TO
DENOTE
COLUMN
VECTORS
POINTS
CAN
ALSO
BE
REPRESENTED
USING
HOMOGENEOUS
COORDINATES
X
X
Y
W
WHERE
VECTORS
THAT
DIFFER
ONLY
BY
SCALE
ARE
CONSIDERED
TO
BE
EQUIVALENT
IS
CALLED
THE
PROJECTIVE
SPACE
A
HOMOGENEOUS
VECTOR
X
CAN
BE
CONVERTED
BACK
INTO
AN
INHOMOGENEOUS
VECTOR
X
BY
DIVIDING
THROUGH
BY
THE
LAST
ELEMENT
W
I
E
X
X
Y
W
W
X
Y
W
X
WHERE
X
X
Y
IS
THE
AUGMENTED
VECTOR
HOMOGENEOUS
POINTS
WHOSE
LAST
ELEMENT
IS
W
ARE
CALLED
IDEAL
POINTS
OR
POINTS
AT
INFINITY
AND
DO
NOT
HAVE
AN
EQUIVALENT
INHOMOGENEOUS
REPRESENTATION
LINES
LINES
CAN
ALSO
BE
REPRESENTED
USING
HOMOGENEOUS
COORDINATES
L
A
B
C
THE
CORRESPONDING
LINE
EQUATION
IS
X
L
AX
BY
C
WE
CAN
NORMALIZE
THE
LINE
EQUATION
VECTOR
SO
THAT
L
NˆX
NˆY
D
Nˆ
D
WITH
IN
THIS
CASE
Nˆ
IS
THE
NORMAL
VECTOR
PERPENDICULAR
TO
THE
LINE
AND
D
IS
ITS
DISTANCE
TO
THE
ORIGIN
FIGURE
THE
ONE
EXCEPTION
TO
THIS
NORMALIZATION
IS
THE
LINE
AT
INFINITY
L
WHICH
INCLUDES
ALL
IDEAL
POINTS
AT
INFINITY
WE
CAN
ALSO
EXPRESS
Nˆ
AS
A
FUNCTION
OF
ROTATION
ANGLE
Θ
Nˆ
NˆX
NˆY
COS
Θ
SIN
Θ
FIGURE
THIS
REPRESENTATION
IS
COMMONLY
USED
IN
THE
HOUGH
TRANSFORM
LINE
FINDING
A
B
FIGURE
A
LINE
EQUATION
AND
B
PLANE
EQUATION
EXPRESSED
IN
TERMS
OF
THE
NORMAL
Nˆ
AND
DISTANCE
TO
THE
ORIGIN
D
ALGORITHM
WHICH
IS
DISCUSSED
IN
SECTION
THE
COMBINATION
Θ
D
IS
ALSO
KNOWN
AS
POLAR
COORDINATES
WHEN
USING
HOMOGENEOUS
COORDINATES
WE
CAN
COMPUTE
THE
INTERSECTION
OF
TWO
LINES
AS
X
WHERE
IS
THE
CROSS
PRODUCT
OPERATOR
SIMILARLY
THE
LINE
JOINING
TWO
POINTS
CAN
BE
WRITTEN
AS
L
X
X
WHEN
TRYING
TO
FIT
AN
INTERSECTION
POINT
TO
MULTIPLE
LINES
OR
CONVERSELY
A
LINE
TO
MULTIPLE
POINTS
LEAST
SQUARES
TECHNIQUES
SECTION
AND
APPENDIX
A
CAN
BE
USED
AS
DISCUSSED
IN
EXERCISE
CONICS
THERE
ARE
OTHER
ALGEBRAIC
CURVES
THAT
CAN
BE
EXPRESSED
WITH
SIMPLE
POLYNOMIAL
HOMOGENEOUS
EQUATIONS
FOR
EXAMPLE
THE
CONIC
SECTIONS
SO
CALLED
BECAUSE
THEY
ARISE
AS
THE
INTERSECTION
OF
A
PLANE
AND
A
CONE
CAN
BE
WRITTEN
USING
A
QUADRIC
EQUATION
X
T
QX
QUADRIC
EQUATIONS
PLAY
USEFUL
ROLES
IN
THE
STUDY
OF
MULTI
VIEW
GEOMETRY
AND
CAMERA
CALIBRA
TION
HARTLEY
AND
ZISSERMAN
FAUGERAS
AND
LUONG
BUT
ARE
NOT
USED
EXTENSIVELY
IN
THIS
BOOK
POINTS
POINT
COORDINATES
IN
THREE
DIMENSIONS
CAN
BE
WRITTEN
USING
INHOMOGENEOUS
CO
ORDINATES
X
X
Y
Z
OR
HOMOGENEOUS
COORDINATES
X
X
Y
Z
W
AS
BEFORE
IT
IS
SOMETIMES
USEFUL
TO
DENOTE
A
POINT
USING
THE
AUGMENTED
VECTOR
X
X
Y
Z
WITH
X
W
X
Z
P
Λ
R
Λ
P
ΛQ
Q
X
Y
FIGURE
LINE
EQUATION
R
Λ
P
ΛQ
PLANES
PLANES
CAN
ALSO
BE
REPRESENTED
AS
HOMOGENEOUS
COORDINATES
M
WITH
A
CORRESPONDING
PLANE
EQUATION
A
B
C
D
X
M
AX
BY
CZ
D
WE
CAN
ALSO
NORMALIZE
THE
PLANE
EQUATION
AS
M
NˆX
NˆY
NˆZ
D
Nˆ
D
WITH
IN
THIS
CASE
Nˆ
IS
THE
NORMAL
VECTOR
PERPENDICULAR
TO
THE
PLANE
AND
D
IS
ITS
DISTANCE
TO
THE
ORIGIN
FIGURE
AS
WITH
THE
CASE
OF
LINES
THE
PLANE
AT
INFINITY
M
WHICH
CONTAINS
ALL
THE
POINTS
AT
INFINITY
CANNOT
BE
NORMALIZED
I
E
IT
DOES
NOT
HAVE
A
UNIQUE
NORMAL
OR
A
FINITE
DISTANCE
WE
CAN
EXPRESS
Nˆ
AS
A
FUNCTION
OF
TWO
ANGLES
Θ
Φ
Nˆ
COS
Θ
COS
Φ
SIN
Θ
COS
Φ
SIN
Φ
I
E
USING
SPHERICAL
COORDINATES
BUT
THESE
ARE
LESS
COMMONLY
USED
THAN
POLAR
COORDINATES
SINCE
THEY
DO
NOT
UNIFORMLY
SAMPLE
THE
SPACE
OF
POSSIBLE
NORMAL
VECTORS
LINES
LINES
IN
ARE
LESS
ELEGANT
THAN
EITHER
LINES
IN
OR
PLANES
IN
ONE
POSSIBLE
REPRESENTATION
IS
TO
USE
TWO
POINTS
ON
THE
LINE
P
Q
ANY
OTHER
POINT
ON
THE
LINE
CAN
BE
EXPRESSED
AS
A
LINEAR
COMBINATION
OF
THESE
TWO
POINTS
R
Λ
P
ΛQ
AS
SHOWN
IN
FIGURE
IF
WE
RESTRICT
Λ
WE
GET
THE
LINE
SEGMENT
JOINING
P
AND
Q
IF
WE
USE
HOMOGENEOUS
COORDINATES
WE
CAN
WRITE
THE
LINE
AS
R
ΜP
ΛQ
A
SPECIAL
CASE
OF
THIS
IS
WHEN
THE
SECOND
POINT
IS
AT
INFINITY
I
E
Q
DˆX
DˆY
DˆZ
Dˆ
HERE
WE
SEE
THAT
DˆIS
THE
DIRECTION
OF
THE
LINE
WE
CAN
THEN
RE
WRITE
THE
INHOMOGENEOUS
LINE
EQUATION
AS
R
P
ΛDˆ
A
DISADVANTAGE
OF
THE
ENDPOINT
REPRESENTATION
FOR
LINES
IS
THAT
IT
HAS
TOO
MANY
DEGREES
OF
FREEDOM
I
E
SIX
THREE
FOR
EACH
ENDPOINT
INSTEAD
OF
THE
FOUR
DEGREES
THAT
A
LINE
TRULY
HAS
HOWEVER
IF
WE
FIX
THE
TWO
POINTS
ON
THE
LINE
TO
LIE
IN
SPECIFIC
PLANES
WE
OBTAIN
A
REP
RESENTATION
WITH
FOUR
DEGREES
OF
FREEDOM
FOR
EXAMPLE
IF
WE
ARE
REPRESENTING
NEARLY
VERTICAL
LINES
THEN
Z
AND
Z
FORM
TWO
SUITABLE
PLANES
I
E
THE
X
Y
COORDINATES
IN
BOTH
PLANES
PROVIDE
THE
FOUR
COORDINATES
DESCRIBING
THE
LINE
THIS
KIND
OF
TWO
PLANE
PARAMETERI
ZATION
IS
USED
IN
THE
LIGHT
FIELD
AND
LUMIGRAPH
IMAGE
BASED
RENDERING
SYSTEMS
DESCRIBED
IN
CHAPTER
TO
REPRESENT
THE
COLLECTION
OF
RAYS
SEEN
BY
A
CAMERA
AS
IT
MOVES
IN
FRONT
OF
AN
OBJECT
THE
TWO
ENDPOINT
REPRESENTATION
IS
ALSO
USEFUL
FOR
REPRESENTING
LINE
SEGMENTS
EVEN
WHEN
THEIR
EXACT
ENDPOINTS
CANNOT
BE
SEEN
ONLY
GUESSED
AT
IF
WE
WISH
TO
REPRESENT
ALL
POSSIBLE
LINES
WITHOUT
BIAS
TOWARDS
ANY
PARTICULAR
ORIENTATION
WE
CAN
USE
PLU
CKER
COORDINATES
HARTLEY
AND
ZISSERMAN
CHAPTER
FAUGERAS
AND
LUONG
CHAPTER
THESE
COORDINATES
ARE
THE
SIX
INDEPENDENT
NON
ZERO
ENTRIES
IN
THE
SKEW
SYMMETRIC
MATRIX
L
P
Q
T
Q
P
T
WHERE
P
AND
Q
ARE
ANY
TWO
NON
IDENTICAL
POINTS
ON
THE
LINE
THIS
REPRESENTATION
HAS
ONLY
FOUR
DEGREES
OF
FREEDOM
SINCE
L
IS
HOMOGENEOUS
AND
ALSO
SATISFIES
DET
L
WHICH
RESULTS
IN
A
QUADRATIC
CONSTRAINT
ON
THE
PLU
CKER
COORDINATES
IN
PRACTICE
THE
MINIMAL
REPRESENTATION
IS
NOT
ESSENTIAL
FOR
MOST
APPLICATIONS
AN
ADE
QUATE
MODEL
OF
LINES
CAN
BE
OBTAINED
BY
ESTIMATING
THEIR
DIRECTION
WHICH
MAY
BE
KNOWN
AHEAD
OF
TIME
E
G
FOR
ARCHITECTURE
AND
SOME
POINT
WITHIN
THE
VISIBLE
PORTION
OF
THE
LINE
SEE
SECTION
OR
BY
USING
THE
TWO
ENDPOINTS
SINCE
LINES
ARE
MOST
OFTEN
VISIBLE
AS
FINITE
LINE
SEGMENTS
HOWEVER
IF
YOU
ARE
INTERESTED
IN
MORE
DETAILS
ABOUT
THE
TOPIC
OF
MINIMAL
LINE
PARAMETERIZATIONS
FO
RSTNER
DISCUSSES
VARIOUS
WAYS
TO
INFER
AND
MODEL
LINES
IN
PROJECTIVE
GEOMETRY
AS
WELL
AS
HOW
TO
ESTIMATE
THE
UNCERTAINTY
IN
SUCH
FITTED
MODELS
QUADRICS
THE
ANALOG
OF
A
CONIC
SECTION
IS
A
QUADRIC
SURFACE
X
T
QX
HARTLEY
AND
ZISSERMAN
CHAPTER
AGAIN
WHILE
QUADRIC
SURFACES
ARE
USEFUL
IN
THE
STUDY
OF
MULTI
VIEW
GEOMETRY
AND
CAN
ALSO
SERVE
AS
USEFUL
MODELING
PRIMITIVES
SPHERES
ELLIPSOIDS
CYLINDERS
WE
DO
NOT
STUDY
THEM
IN
GREAT
DETAIL
IN
THIS
BOOK
TRANSFORMATIONS
HAVING
DEFINED
OUR
BASIC
PRIMITIVES
WE
CAN
NOW
TURN
OUR
ATTENTION
TO
HOW
THEY
CAN
BE
TRANS
FORMED
THE
SIMPLEST
TRANSFORMATIONS
OCCUR
IN
THE
PLANE
AND
ARE
ILLUSTRATED
IN
FIGURE
FIGURE
BASIC
SET
OF
PLANAR
TRANSFORMATIONS
TRANSLATION
TRANSLATIONS
CAN
BE
WRITTEN
AS
XI
X
T
OR
XI
I
T
L
X
WHERE
I
IS
THE
IDENTITY
MATRIX
OR
I
I
T
L
X
WHERE
IS
THE
ZERO
VECTOR
USING
A
MATRIX
RESULTS
IN
A
MORE
COMPACT
NOTATION
WHEREAS
USING
A
FULL
RANK
MATRIX
WHICH
CAN
BE
OBTAINED
FROM
THE
MATRIX
BY
APPENDING
A
ROW
MAKES
IT
POSSIBLE
TO
CHAIN
TRANSFORMATIONS
USING
MATRIX
MULTIPLICATION
NOTE
THAT
IN
ANY
EQUATION
WHERE
AN
AUGMENTED
VECTOR
SUCH
AS
X
APPEARS
ON
BOTH
SIDES
IT
CAN
ALWAYS
BE
REPLACED
WITH
A
FULL
HOMOGENEOUS
VECTOR
X
ROTATION
TRANSLATION
THIS
TRANSFORMATION
IS
ALSO
KNOWN
AS
RIGID
BODY
MOTION
OR
THE
EUCLIDEAN
TRANSFORMATION
SINCE
EUCLIDEAN
DISTANCES
ARE
PRESERVED
IT
CAN
BE
WRITTEN
AS
XI
RX
T
OR
WHERE
XI
R
T
L
X
COS
Θ
SIN
Θ
SIN
Θ
COS
Θ
L
IS
AN
ORTHONORMAL
ROTATION
MATRIX
WITH
RRT
I
AND
R
SCALED
ROTATION
ALSO
KNOWN
AS
THE
SIMILARITY
TRANSFORM
THIS
TRANSFORMATION
CAN
BE
EX
PRESSED
AS
XI
SRX
T
WHERE
IS
AN
ARBITRARY
SCALE
FACTOR
IT
CAN
ALSO
BE
WRITTEN
AS
XI
SR
T
X
A
B
TX
B
A
TY
L
X
WHERE
WE
NO
LONGER
REQUIRE
THAT
THE
SIMILARITY
TRANSFORM
PRESERVES
ANGLES
BETWEEN
LINES
AFFINE
THE
AFFINE
TRANSFORMATION
IS
WRITTEN
AS
XI
AX
WHERE
A
IS
AN
ARBITRARY
MATRIX
I
E
XI
L
X
PARALLEL
LINES
REMAIN
PARALLEL
UNDER
AFFINE
TRANSFORMATIONS
PROJECTIVE
THIS
TRANSFORMATION
ALSO
KNOWN
AS
A
PERSPECTIVE
TRANSFORM
OR
HOMOGRAPHY
OPERATES
ON
HOMOGENEOUS
COORDINATES
X
I
H
X
WHERE
H
IS
AN
ARBITRARY
MATRIX
NOTE
THAT
H
IS
HOMOGENEOUS
I
E
IT
IS
ONLY
DEFINED
UP
TO
A
SCALE
AND
THAT
TWO
H
MATRICES
THAT
DIFFER
ONLY
BY
SCALE
ARE
EQUIVALENT
THE
RESULTING
HOMOGENEOUS
COORDINATE
X
I
MUST
BE
NORMALIZED
IN
ORDER
TO
OBTAIN
AN
INHOMOGENEOUS
RESULT
X
I
E
XI
AND
YI
PERSPECTIVE
TRANSFORMATIONS
PRESERVE
STRAIGHT
LINES
I
E
THEY
REMAIN
STRAIGHT
AFTER
THE
TRANS
FORMATION
HIERARCHY
OF
TRANSFORMATIONS
THE
PRECEDING
SET
OF
TRANSFORMATIONS
ARE
ILLUSTRATED
IN
FIGURE
AND
SUMMARIZED
IN
TABLE
THE
EASIEST
WAY
TO
THINK
OF
THEM
IS
AS
A
SET
OF
POTENTIALLY
RESTRICTED
MATRICES
OPERATING
ON
HOMOGENEOUS
COORDINATE
VECTORS
HARTLEY
AND
ZISSERMAN
CONTAINS
A
MORE
DETAILED
DESCRIPTION
OF
THE
HIERARCHY
OF
PLANAR
TRANSFORMATIONS
THE
ABOVE
TRANSFORMATIONS
FORM
A
NESTED
SET
OF
GROUPS
I
E
THEY
ARE
CLOSED
UNDER
COM
POSITION
AND
HAVE
AN
INVERSE
THAT
IS
A
MEMBER
OF
THE
SAME
GROUP
THIS
WILL
BE
IMPORTANT
LATER
WHEN
APPLYING
THESE
TRANSFORMATIONS
TO
IMAGES
IN
SECTION
EACH
SIMPLER
GROUP
IS
A
SUBSET
OF
THE
MORE
COMPLEX
GROUP
BELOW
IT
CO
VECTORS
WHILE
THE
ABOVE
TRANSFORMATIONS
CAN
BE
USED
TO
TRANSFORM
POINTS
IN
A
PLANE
CAN
THEY
ALSO
BE
USED
DIRECTLY
TO
TRANSFORM
A
LINE
EQUATION
CONSIDER
THE
HOMOGENEOUS
EQUA
TION
L
X
IF
WE
TRANSFORM
XI
H
X
WE
OBTAIN
LI
X
I
IT
T
I
T
X
L
X
I
E
LI
H
T
L
THUS
THE
ACTION
OF
A
PROJECTIVE
TRANSFORMATION
ON
A
CO
VECTOR
SUCH
AS
A
LINE
OR
NORMAL
CAN
BE
REPRESENTED
BY
THE
TRANSPOSED
INVERSE
OF
THE
MATRIX
WHICH
IS
EQUIV
ALENT
TO
THE
ADJOINT
OF
H
SINCE
PROJECTIVE
TRANSFORMATION
MATRICES
ARE
HOMOGENEOUS
JIM
TRANSFORMATION
MATRIX
DOF
PRESERVES
ICON
TRANSLATION
I
T
ORIENTATION
RIGID
EUCLIDEAN
R
T
L
LENGTHS
SIMILARITY
SR
T
L
ANGLES
AFFINE
A
L
PARALLELISM
PROJECTIVE
H
STRAIGHT
LINES
TABLE
HIERARCHY
OF
COORDINATE
TRANSFORMATIONS
EACH
TRANSFORMATION
ALSO
PRESERVES
THE
PROPERTIES
LISTED
IN
THE
ROWS
BELOW
IT
I
E
SIMILARITY
PRESERVES
NOT
ONLY
ANGLES
BUT
ALSO
PARALLELISM
AND
STRAIGHT
LINES
THE
MATRICES
ARE
EXTENDED
WITH
A
THIRD
ROW
TO
FORM
A
FULL
MATRIX
FOR
HOMOGENEOUS
COORDINATE
TRANSFORMATIONS
BLINN
DESCRIBES
IN
CHAPTERS
AND
THE
INS
AND
OUTS
OF
NOTATING
AND
MANIPULATING
CO
VECTORS
WHILE
THE
ABOVE
TRANSFORMATIONS
ARE
THE
ONES
WE
USE
MOST
EXTENSIVELY
A
NUMBER
OF
ADDI
TIONAL
TRANSFORMATIONS
ARE
SOMETIMES
USED
STRETCH
SQUASH
THIS
TRANSFORMATION
CHANGES
THE
ASPECT
RATIO
OF
AN
IMAGE
XI
SXX
TX
YI
SYY
TY
AND
IS
A
RESTRICTED
FORM
OF
AN
AFFINE
TRANSFORMATION
UNFORTUNATELY
IT
DOES
NOT
NEST
CLEANLY
WITH
THE
GROUPS
LISTED
IN
TABLE
PLANAR
SURFACE
FLOW
THIS
EIGHT
PARAMETER
TRANSFORMATION
HORN
BERGEN
ANANDAN
HANNA
ET
AL
GIROD
GREINER
AND
NIEMANN
XI
YI
ARISES
WHEN
A
PLANAR
SURFACE
UNDERGOES
A
SMALL
MOTION
IT
CAN
THUS
BE
THOUGHT
OF
AS
A
SMALL
MOTION
APPROXIMATION
TO
A
FULL
HOMOGRAPHY
ITS
MAIN
ATTRACTION
IS
THAT
IT
IS
LINEAR
IN
THE
MOTION
PARAMETERS
AK
WHICH
ARE
OFTEN
THE
QUANTITIES
BEING
ESTIMATED
TRANSFORMATION
MATRIX
DOF
PRESERVES
ICON
TRANSLATION
I
T
ORIENTATION
RIGID
EUCLIDEAN
R
T
L
LENGTHS
SIMILARITY
SR
T
L
ANGLES
AFFINE
A
L
PARALLELISM
PROJECTIVE
H
STRAIGHT
LINES
TABLE
HIERARCHY
OF
COORDINATE
TRANSFORMATIONS
EACH
TRANSFORMATION
ALSO
PRESERVES
THE
PROPERTIES
LISTED
IN
THE
ROWS
BELOW
IT
I
E
SIMILARITY
PRESERVES
NOT
ONLY
ANGLES
BUT
ALSO
PARALLELISM
AND
STRAIGHT
LINES
THE
MATRICES
ARE
EXTENDED
WITH
A
FOURTH
ROW
TO
FORM
A
FULL
MATRIX
FOR
HOMOGENEOUS
COORDINATE
TRANSFORMATIONS
THE
MNEMONIC
ICONS
ARE
DRAWN
IN
BUT
ARE
MEANT
TO
SUGGEST
TRANSFORMATIONS
OCCURRING
IN
A
FULL
CUBE
BILINEAR
INTERPOLANT
THIS
EIGHT
PARAMETER
TRANSFORM
WOLBERG
XI
YI
CAN
BE
USED
TO
INTERPOLATE
THE
DEFORMATION
DUE
TO
THE
MOTION
OF
THE
FOUR
CORNER
POINTS
OF
A
SQUARE
IN
FACT
IT
CAN
INTERPOLATE
THE
MOTION
OF
ANY
FOUR
NON
COLLINEAR
POINTS
WHILE
THE
DEFORMATION
IS
LINEAR
IN
THE
MOTION
PARAMETERS
IT
DOES
NOT
GENERALLY
PRESERVE
STRAIGHT
LINES
ONLY
LINES
PARALLEL
TO
THE
SQUARE
AXES
HOWEVER
IT
IS
OFTEN
QUITE
USEFUL
E
G
IN
THE
INTERPOLATION
OF
SPARSE
GRIDS
USING
SPLINES
SECTION
TRANSFORMATIONS
THE
SET
OF
THREE
DIMENSIONAL
COORDINATE
TRANSFORMATIONS
IS
VERY
SIMILAR
TO
THAT
AVAILABLE
FOR
TRANSFORMATIONS
AND
IS
SUMMARIZED
IN
TABLE
AS
IN
THESE
TRANSFORMATIONS
FORM
A
NESTED
SET
OF
GROUPS
HARTLEY
AND
ZISSERMAN
SECTION
GIVE
A
MORE
DETAILED
DESCRIP
TION
OF
THIS
HIERARCHY
TRANSLATION
TRANSLATIONS
CAN
BE
WRITTEN
AS
XI
X
T
OR
XI
I
T
L
X
WHERE
I
IS
THE
IDENTITY
MATRIX
AND
IS
THE
ZERO
VECTOR
ROTATION
TRANSLATION
ALSO
KNOWN
AS
RIGID
BODY
MOTION
OR
THE
EUCLIDEAN
TRANS
FORMATION
IT
CAN
BE
WRITTEN
AS
XI
RX
T
OR
XI
R
T
L
X
WHERE
R
IS
A
ORTHONORMAL
ROTATION
MATRIX
WITH
RRT
I
AND
R
NOTE
THAT
SOMETIMES
IT
IS
MORE
CONVENIENT
TO
DESCRIBE
A
RIGID
MOTION
USING
XI
R
X
C
RX
RC
WHERE
C
IS
THE
CENTER
OF
ROTATION
OFTEN
THE
CAMERA
CENTER
COMPACTLY
PARAMETERIZING
A
ROTATION
IS
A
NON
TRIVIAL
TASK
WHICH
WE
DESCRIBE
IN
MORE
DETAIL
BELOW
SCALED
ROTATION
THE
SIMILARITY
TRANSFORM
CAN
BE
EXPRESSED
AS
XI
SRX
T
WHERE
IS
AN
ARBITRARY
SCALE
FACTOR
IT
CAN
ALSO
BE
WRITTEN
AS
XI
SR
T
L
X
THIS
TRANSFORMATION
PRESERVES
ANGLES
BETWEEN
LINES
AND
PLANES
AFFINE
THE
AFFINE
TRANSFORM
IS
WRITTEN
AS
XI
AX
WHERE
A
IS
AN
ARBITRARY
MATRIX
I
E
XI
X
PARALLEL
LINES
AND
PLANES
REMAIN
PARALLEL
UNDER
AFFINE
TRANSFORMATIONS
PROJECTIVE
THIS
TRANSFORMATION
VARIOUSLY
KNOWN
AS
A
PERSPECTIVE
TRANSFORM
HOMOGRA
PHY
OR
COLLINEATION
OPERATES
ON
HOMOGENEOUS
COORDINATES
X
I
H
X
WHERE
H
IS
AN
ARBITRARY
HOMOGENEOUS
MATRIX
AS
IN
THE
RESULTING
HOMOGENEOUS
COORDINATE
X
I
MUST
BE
NORMALIZED
IN
ORDER
TO
OBTAIN
AN
INHOMOGENEOUS
RESULT
X
PERSPECTIVE
TRANSFORMATIONS
PRESERVE
STRAIGHT
LINES
I
E
THEY
REMAIN
STRAIGHT
AFTER
THE
TRANSFORMATION
FIGURE
ROTATION
AROUND
AN
AXIS
Nˆ
BY
AN
ANGLE
Θ
ROTATIONS
THE
BIGGEST
DIFFERENCE
BETWEEN
AND
COORDINATE
TRANSFORMATIONS
IS
THAT
THE
PARAMETER
IZATION
OF
THE
ROTATION
MATRIX
R
IS
NOT
AS
STRAIGHTFORWARD
BUT
SEVERAL
POSSIBILITIES
EXIST
EULER
ANGLES
A
ROTATION
MATRIX
CAN
BE
FORMED
AS
THE
PRODUCT
OF
THREE
ROTATIONS
AROUND
THREE
CARDINAL
AXES
E
G
X
Y
AND
Z
OR
X
Y
AND
X
THIS
IS
GENERALLY
A
BAD
IDEA
AS
THE
RESULT
DEPENDS
ON
THE
ORDER
IN
WHICH
THE
TRANSFORMS
ARE
APPLIED
WHAT
IS
WORSE
IT
IS
NOT
ALWAYS
POSSIBLE
TO
MOVE
SMOOTHLY
IN
THE
PARAMETER
SPACE
I
E
SOMETIMES
ONE
OR
MORE
OF
THE
EULER
ANGLES
CHANGE
DRAMATICALLY
IN
RESPONSE
TO
A
SMALL
CHANGE
IN
ROTATION
FOR
THESE
REASONS
WE
DO
NOT
EVEN
GIVE
THE
FORMULA
FOR
EULER
ANGLES
IN
THIS
BOOK
INTERESTED
READERS
CAN
LOOK
IN
OTHER
TEXTBOOKS
OR
TECHNICAL
REPORTS
FAUGERAS
DIEBEL
NOTE
THAT
IN
SOME
APPLICATIONS
IF
THE
ROTATIONS
ARE
KNOWN
TO
BE
A
SET
OF
UNI
AXIAL
TRANSFORMS
THEY
CAN
ALWAYS
BE
REPRESENTED
USING
AN
EXPLICIT
SET
OF
RIGID
TRANSFORMATIONS
AXIS
ANGLE
EXPONENTIAL
TWIST
A
ROTATION
CAN
BE
REPRESENTED
BY
A
ROTATION
AXIS
Nˆ
AND
AN
ANGLE
Θ
OR
EQUIVALENTLY
BY
A
VECTOR
Ω
ΘNˆ
FIGURE
SHOWS
HOW
WE
CAN
COMPUTE
THE
EQUIVALENT
ROTATION
FIRST
WE
PROJECT
THE
VECTOR
V
ONTO
THE
AXIS
Nˆ
TO
OBTAIN
Nˆ
Nˆ
V
NˆNˆT
V
WHICH
IS
THE
COMPONENT
OF
V
THAT
IS
NOT
AFFECTED
BY
THE
ROTATION
NEXT
WE
COMPUTE
THE
PERPENDICULAR
RESIDUAL
OF
V
FROM
Nˆ
V
V
I
NˆNˆT
V
IN
ROBOTICS
THIS
IS
SOMETIMES
REFERRED
TO
AS
GIMBAL
LOCK
WE
CAN
ROTATE
THIS
VECTOR
BY
USING
THE
CROSS
PRODUCT
V
Nˆ
V
Nˆ
V
WHERE
Nˆ
IS
THE
MATRIX
FORM
OF
THE
CROSS
PRODUCT
OPERATOR
WITH
THE
VECTOR
Nˆ
NˆX
NˆY
NˆZ
NˆZ
NˆY
Nˆ
NˆZ
NˆX
NOTE
THAT
ROTATING
THIS
VECTOR
BY
ANOTHER
IS
EQUIVALENT
TO
TAKING
THE
CROSS
PRODUCT
AGAIN
V
Nˆ
V
Nˆ
V
V
AND
HENCE
V
V
V
V
I
Nˆ
V
WE
CAN
NOW
COMPUTE
THE
IN
PLANE
COMPONENT
OF
THE
ROTATED
VECTOR
U
AS
U
COS
ΘV
SIN
ΘV
SIN
Θ
Nˆ
COS
Θ
Nˆ
V
PUTTING
ALL
THESE
TERMS
TOGETHER
WE
OBTAIN
THE
FINAL
ROTATED
VECTOR
AS
U
U
I
SIN
Θ
Nˆ
COS
Θ
Nˆ
V
WE
CAN
THEREFORE
WRITE
THE
ROTATION
MATRIX
CORRESPONDING
TO
A
ROTATION
BY
Θ
AROUND
AN
AXIS
Nˆ
AS
R
Nˆ
Θ
I
SIN
Θ
Nˆ
COS
Θ
Nˆ
WHICH
IS
KNOWN
AS
RODRIGUEZ
FORMULA
AYACHE
THE
PRODUCT
OF
THE
AXIS
Nˆ
AND
ANGLE
Θ
Ω
ΘNˆ
ΩX
ΩY
ΩZ
IS
A
MINIMAL
REPRESEN
TATION
FOR
A
ROTATION
ROTATIONS
THROUGH
COMMON
ANGLES
SUCH
AS
MULTIPLES
OF
CAN
BE
REPRESENTED
EXACTLY
AND
CONVERTED
TO
EXACT
MATRICES
IF
Θ
IS
STORED
IN
DEGREES
UNFORTUNATELY
THIS
REPRESENTATION
IS
NOT
UNIQUE
SINCE
WE
CAN
ALWAYS
ADD
A
MULTIPLE
OF
RADIANS
TO
Θ
AND
GET
THE
SAME
ROTATION
MATRIX
AS
WELL
Nˆ
Θ
AND
Nˆ
Θ
REPRESENT
THE
SAME
ROTATION
HOWEVER
FOR
SMALL
ROTATIONS
E
G
CORRECTIONS
TO
ROTATIONS
THIS
IS
AN
EXCELLENT
CHOICE
IN
PARTICULAR
FOR
SMALL
INFINITESIMAL
OR
INSTANTANEOUS
ROTATIONS
AND
Θ
EXPRESSED
IN
RADIANS
RODRIGUEZ
FORMULA
SIMPLIFIES
TO
R
Ω
I
SIN
Θ
Nˆ
I
ΘNˆ
ΩZ
ΩY
ΩZ
ΩX
ΩY
ΩX
WHICH
GIVES
A
NICE
LINEARIZED
RELATIONSHIP
BETWEEN
THE
ROTATION
PARAMETERS
Ω
AND
R
WE
CAN
ALSO
WRITE
R
Ω
V
V
Ω
V
WHICH
IS
HANDY
WHEN
WE
WANT
TO
COMPUTE
THE
DERIVATIVE
OF
RV
WITH
RESPECT
TO
Ω
RV
Z
Y
ANOTHER
WAY
TO
DERIVE
A
ROTATION
THROUGH
A
FINITE
ANGLE
IS
CALLED
THE
EXPONENTIAL
TWIST
MURRAY
LI
AND
SASTRY
A
ROTATION
BY
AN
ANGLE
Θ
IS
EQUIVALENT
TO
K
ROTATIONS
THROUGH
Θ
K
IN
THE
LIMIT
AS
K
WE
OBTAIN
R
Nˆ
Θ
LIM
I
ΘNˆ
K
EXP
Ω
K
K
IF
WE
EXPAND
THE
MATRIX
EXPONENTIAL
AS
A
TAYLOR
SERIES
USING
THE
IDENTITY
Nˆ
K
Nˆ
K
K
AND
AGAIN
ASSUMING
Θ
IS
IN
RADIANS
EXP
ˆ
ˆ
ˆ
Ω
I
Θ
N
N
N
I
Θ
Nˆ
Nˆ
I
SIN
Θ
Nˆ
COS
Θ
Nˆ
WHICH
YIELDS
THE
FAMILIAR
RODRIGUEZ
FORMULA
UNIT
QUATERNIONS
THE
UNIT
QUATERNION
REPRESENTATION
IS
CLOSELY
RELATED
TO
THE
ANGLE
AXIS
REPRESENTATION
A
UNIT
QUATERNION
IS
A
UNIT
LENGTH
VECTOR
WHOSE
COMPONENTS
CAN
BE
WRITTEN
AS
Q
QX
QY
QZ
QW
OR
Q
X
Y
Z
W
FOR
SHORT
UNIT
QUATERNIONS
LIVE
ON
THE
UNIT
SPHERE
AND
ANTIPODAL
OPPOSITE
SIGN
QUATERNIONS
Q
AND
Q
REPRESENT
THE
SAME
ROTATION
FIGURE
OTHER
THAN
THIS
AMBIGUITY
DUAL
COVERING
THE
UNIT
QUATERNION
REPRESENTATION
OF
A
ROTATION
IS
UNIQUE
FURTHERMORE
THE
REPRESENTATION
IS
CONTINUOUS
I
E
AS
ROTATION
MATRICES
VARY
CONTINUOUSLY
ONE
CAN
FIND
A
CONTINUOUS
QUATERNION
REPRESENTATION
ALTHOUGH
THE
PATH
ON
THE
QUATERNION
SPHERE
MAY
WRAP
ALL
THE
WAY
AROUND
BEFORE
RETURNING
TO
THE
ORIGIN
QO
FOR
THESE
AND
OTHER
REASONS
GIVEN
BELOW
QUATERNIONS
ARE
A
VERY
POPULAR
REPRESENTATION
FOR
POSE
AND
FOR
POSE
INTERPOLATION
IN
COMPUTER
GRAPHICS
SHOEMAKE
QUATERNIONS
CAN
BE
DERIVED
FROM
THE
AXIS
ANGLE
REPRESENTATION
THROUGH
THE
FORMULA
Θ
Θ
Q
V
W
SIN
Nˆ
COS
FIGURE
UNIT
QUATERNIONS
LIVE
ON
THE
UNIT
SPHERE
THIS
FIGURE
SHOWS
A
SMOOTH
TRAJECTORY
THROUGH
THE
THREE
QUATERNIONS
AND
THE
ANTIPODAL
POINT
TO
NAMELY
REPRESENTS
THE
SAME
ROTATION
AS
WHERE
Nˆ
AND
Θ
ARE
THE
ROTATION
AXIS
AND
ANGLE
USING
THE
TRIGONOMETRIC
IDENTITIES
SIN
Θ
SIN
Θ
COS
Θ
AND
COS
Θ
Θ
RODRIGUEZ
FORMULA
CAN
BE
CONVERTED
TO
R
Nˆ
Θ
I
SIN
Θ
Nˆ
COS
Θ
Nˆ
I
V
V
THIS
SUGGESTS
A
QUICK
WAY
TO
ROTATE
A
VECTOR
V
BY
A
QUATERNION
USING
A
SERIES
OF
CROSS
PRODUCTS
SCALINGS
AND
ADDITIONS
TO
OBTAIN
A
FORMULA
FOR
R
Q
AS
A
FUNCTION
OF
X
Y
Z
W
RECALL
THAT
Z
Y
XY
XZ
V
AND
Y
X
XY
YZ
XZ
YZ
WE
THUS
OBTAIN
XY
ZW
XZ
YW
R
Q
XY
ZW
YZ
XW
XZ
YW
YZ
XW
THE
DIAGONAL
TERMS
CAN
BE
MADE
MORE
SYMMETRICAL
BY
REPLACING
WITH
ETC
THE
NICEST
ASPECT
OF
UNIT
QUATERNIONS
IS
THAT
THERE
IS
A
SIMPLE
ALGEBRA
FOR
COMPOSING
ROTA
TIONS
EXPRESSED
AS
UNIT
QUATERNIONS
GIVEN
TWO
QUATERNIONS
AND
THE
QUATERNION
MULTIPLY
OPERATOR
IS
DEFINED
AS
WITH
THE
PROPERTY
THAT
R
R
R
NOTE
THAT
QUATERNION
MULTIPLICATION
IS
NOT
COMMUTATIVE
JUST
AS
ROTATIONS
AND
MATRIX
MULTIPLICATIONS
ARE
NOT
TAKING
THE
INVERSE
OF
A
QUATERNION
IS
EASY
JUST
FLIP
THE
SIGN
OF
V
OR
W
BUT
NOT
BOTH
YOU
CAN
VERIFY
THIS
HAS
THE
DESIRED
EFFECT
OF
TRANSPOSING
THE
R
MATRIX
IN
THUS
WE
CAN
ALSO
DEFINE
QUATERNION
DIVISION
AS
THIS
IS
USEFUL
WHEN
THE
INCREMENTAL
ROTATION
BETWEEN
TWO
ROTATIONS
IS
DESIRED
IN
PARTICULAR
IF
WE
WANT
TO
DETERMINE
A
ROTATION
THAT
IS
PARTWAY
BETWEEN
TWO
GIVEN
ROTA
TIONS
WE
CAN
COMPUTE
THE
INCREMENTAL
ROTATION
TAKE
A
FRACTION
OF
THE
ANGLE
AND
COMPUTE
THE
NEW
ROTATION
THIS
PROCEDURE
IS
CALLED
SPHERICAL
LINEAR
INTERPOLATION
OR
SLERP
FOR
SHORT
SHOE
MAKE
AND
IS
GIVEN
IN
ALGORITHM
NOTE
THAT
SHOEMAKE
PRESENTS
TWO
FORMULAS
OTHER
THAN
THE
ONE
GIVEN
HERE
THE
FIRST
EXPONENTIATES
QR
BY
ALPHA
BEFORE
MULTIPLYING
THE
ORIGINAL
QUATERNION
WHILE
THE
SECOND
TREATS
THE
QUATERNIONS
AS
VECTORS
ON
A
SPHERE
AND
USES
Q
SIN
Α
Θ
Q
SIN
ΑΘ
Q
SIN
Θ
SIN
Θ
WHERE
Θ
COS
AND
THE
DOT
PRODUCT
IS
DIRECTLY
BETWEEN
THE
QUATERNION
VECTORS
ALL
OF
THESE
FORMULAS
GIVE
COMPARABLE
RESULTS
ALTHOUGH
CARE
SHOULD
BE
TAKEN
WHEN
AND
ARE
CLOSE
TOGETHER
WHICH
IS
WHY
I
PREFER
TO
USE
AN
ARCTANGENT
TO
ESTABLISH
THE
ROTATION
ANGLE
WHICH
ROTATION
REPRESENTATION
IS
BETTER
THE
CHOICE
OF
REPRESENTATION
FOR
ROTATIONS
DEPENDS
PARTLY
ON
THE
APPLICATION
THE
AXIS
ANGLE
REPRESENTATION
IS
MINIMAL
AND
HENCE
DOES
NOT
REQUIRE
ANY
ADDITIONAL
CON
STRAINTS
ON
THE
PARAMETERS
NO
NEED
TO
RE
NORMALIZE
AFTER
EACH
UPDATE
IF
THE
ANGLE
IS
EX
PRESSED
IN
DEGREES
IT
IS
EASIER
TO
UNDERSTAND
THE
POSE
SAY
TWIST
AROUND
X
AXIS
AND
ALSO
EASIER
TO
EXPRESS
EXACT
ROTATIONS
WHEN
THE
ANGLE
IS
IN
RADIANS
THE
DERIVATIVES
OF
R
WITH
RESPECT
TO
Ω
CAN
EASILY
BE
COMPUTED
QUATERNIONS
ON
THE
OTHER
HAND
ARE
BETTER
IF
YOU
WANT
TO
KEEP
TRACK
OF
A
SMOOTHLY
MOVING
CAMERA
SINCE
THERE
ARE
NO
DISCONTINUITIES
IN
THE
REPRESENTATION
IT
IS
ALSO
EASIER
TO
INTERPOLATE
BETWEEN
ROTATIONS
AND
TO
CHAIN
RIGID
TRANSFORMATIONS
MURRAY
LI
AND
SASTRY
BREGLER
AND
MALIK
MY
USUAL
PREFERENCE
IS
TO
USE
QUATERNIONS
BUT
TO
UPDATE
THEIR
ESTIMATES
USING
AN
INCRE
MENTAL
ROTATION
AS
DESCRIBED
IN
SECTION
ALGORITHM
SPHERICAL
LINEAR
INTERPOLATION
SLERP
THE
AXIS
AND
TOTAL
ANGLE
ARE
FIRST
COM
PUTED
FROM
THE
QUATERNION
RATIO
THIS
COMPUTATION
CAN
BE
LIFTED
OUTSIDE
AN
INNER
LOOP
THAT
GENERATES
A
SET
OF
INTERPOLATED
POSITION
FOR
ANIMATION
AN
INCREMENTAL
QUATERNION
IS
THEN
COMPUTED
AND
MULTIPLIED
BY
THE
STARTING
ROTATION
QUATERNION
TO
PROJECTIONS
NOW
THAT
WE
KNOW
HOW
TO
REPRESENT
AND
GEOMETRIC
PRIMITIVES
AND
HOW
TO
TRANSFORM
THEM
SPATIALLY
WE
NEED
TO
SPECIFY
HOW
PRIMITIVES
ARE
PROJECTED
ONTO
THE
IMAGE
PLANE
WE
CAN
DO
THIS
USING
A
LINEAR
TO
PROJECTION
MATRIX
THE
SIMPLEST
MODEL
IS
ORTHOGRAPHY
WHICH
REQUIRES
NO
DIVISION
TO
GET
THE
FINAL
INHOMOGENEOUS
RESULT
THE
MORE
COMMONLY
USED
MODEL
IS
PERSPECTIVE
SINCE
THIS
MORE
ACCURATELY
MODELS
THE
BEHAVIOR
OF
REAL
CAMERAS
ORTHOGRAPHY
AND
PARA
PERSPECTIVE
AN
ORTHOGRAPHIC
PROJECTION
SIMPLY
DROPS
THE
Z
COMPONENT
OF
THE
THREE
DIMENSIONAL
COORDI
NATE
P
TO
OBTAIN
THE
POINT
X
IN
THIS
SECTION
WE
USE
P
TO
DENOTE
POINTS
AND
X
TO
DENOTE
POINTS
THIS
CAN
BE
WRITTEN
AS
X
P
IF
WE
ARE
USING
HOMOGENEOUS
PROJECTIVE
COORDINATES
WE
CAN
WRITE
X
P
A
VIEW
B
ORTHOGRAPHY
C
SCALED
ORTHOGRAPHY
D
PARA
PERSPECTIVE
E
PERSPECTIVE
F
OBJECT
CENTERED
FIGURE
COMMONLY
USED
PROJECTION
MODELS
A
VIEW
OF
WORLD
B
ORTHOGRAPHY
C
SCALED
ORTHOGRAPHY
D
PARA
PERSPECTIVE
E
PERSPECTIVE
F
OBJECT
CENTERED
EACH
DIAGRAM
SHOWS
A
TOP
DOWN
VIEW
OF
THE
PROJECTION
NOTE
HOW
PARALLEL
LINES
ON
THE
GROUND
PLANE
AND
BOX
SIDES
REMAIN
PARALLEL
IN
THE
NON
PERSPECTIVE
PROJECTIONS
I
E
WE
DROP
THE
Z
COMPONENT
BUT
KEEP
THE
W
COMPONENT
ORTHOGRAPHY
IS
AN
APPROXIMATE
MODEL
FOR
LONG
FOCAL
LENGTH
TELEPHOTO
LENSES
AND
OBJECTS
WHOSE
DEPTH
IS
SHALLOW
RELATIVE
TO
THEIR
DISTANCE
TO
THE
CAMERA
SAWHNEY
AND
HANSON
IT
IS
EXACT
ONLY
FOR
TELECENTRIC
LENSES
BAKER
AND
NAYAR
IN
PRACTICE
WORLD
COORDINATES
WHICH
MAY
MEASURE
DIMENSIONS
IN
METERS
NEED
TO
BE
SCALED
TO
FIT
ONTO
AN
IMAGE
SENSOR
PHYSICALLY
MEASURED
IN
MILLIMETERS
BUT
ULTIMATELY
MEA
SURED
IN
PIXELS
FOR
THIS
REASON
SCALED
ORTHOGRAPHY
IS
ACTUALLY
MORE
COMMONLY
USED
X
P
THIS
MODEL
IS
EQUIVALENT
TO
FIRST
PROJECTING
THE
WORLD
POINTS
ONTO
A
LOCAL
FRONTO
PARALLEL
IMAGE
PLANE
AND
THEN
SCALING
THIS
IMAGE
USING
REGULAR
PERSPECTIVE
PROJECTION
THE
SCALING
CAN
BE
THE
SAME
FOR
ALL
PARTS
OF
THE
SCENE
FIGURE
OR
IT
CAN
BE
DIFFERENT
FOR
OBJECTS
THAT
ARE
BEING
MODELED
INDEPENDENTLY
FIGURE
MORE
IMPORTANTLY
THE
SCALING
CAN
VARY
FROM
FRAME
TO
FRAME
WHEN
ESTIMATING
STRUCTURE
FROM
MOTION
WHICH
CAN
BETTER
MODEL
THE
SCALE
CHANGE
THAT
OCCURS
AS
AN
OBJECT
APPROACHES
THE
CAMERA
SCALED
ORTHOGRAPHY
IS
A
POPULAR
MODEL
FOR
RECONSTRUCTING
THE
SHAPE
OF
OBJECTS
FAR
AWAY
FROM
THE
CAMERA
SINCE
IT
GREATLY
SIMPLIFIES
CERTAIN
COMPUTATIONS
FOR
EXAMPLE
POSE
CAMERA
ORIENTATION
CAN
BE
ESTIMATED
USING
SIMPLE
LEAST
SQUARES
SECTION
UNDER
ORTHOGRAPHY
STRUCTURE
AND
MOTION
CAN
SIMULTANEOUSLY
BE
ESTIMATED
USING
FACTORIZATION
SINGULAR
VALUE
DE
COMPOSITION
AS
DISCUSSED
IN
SECTION
TOMASI
AND
KANADE
A
CLOSELY
RELATED
PROJECTION
MODEL
IS
PARA
PERSPECTIVE
ALOIMONOS
POELMAN
AND
KANADE
IN
THIS
MODEL
OBJECT
POINTS
ARE
AGAIN
FIRST
PROJECTED
ONTO
A
LOCAL
REFERENCE
PARALLEL
TO
THE
IMAGE
PLANE
HOWEVER
RATHER
THAN
BEING
PROJECTED
ORTHOGONALLY
TO
THIS
PLANE
THEY
ARE
PROJECTED
PARALLEL
TO
THE
LINE
OF
SIGHT
TO
THE
OBJECT
CENTER
FIGURE
THIS
IS
FOLLOWED
BY
THE
USUAL
PROJECTION
ONTO
THE
FINAL
IMAGE
PLANE
WHICH
AGAIN
AMOUNTS
TO
A
SCALING
THE
COMBINATION
OF
THESE
TWO
PROJECTIONS
IS
THEREFORE
AFFINE
AND
CAN
BE
WRITTEN
AS
X
P
NOTE
HOW
PARALLEL
LINES
IN
REMAIN
PARALLEL
AFTER
PROJECTION
IN
FIGURE
D
PARA
PERSPECTIVE
PROVIDES
A
MORE
ACCURATE
PROJECTION
MODEL
THAN
SCALED
ORTHOGRAPHY
WITHOUT
INCURRING
THE
ADDED
COMPLEXITY
OF
PER
PIXEL
PERSPECTIVE
DIVISION
WHICH
INVALIDATES
TRADITIONAL
FACTORIZA
TION
METHODS
POELMAN
AND
KANADE
PERSPECTIVE
THE
MOST
COMMONLY
USED
PROJECTION
IN
COMPUTER
GRAPHICS
AND
COMPUTER
VISION
IS
TRUE
PERSPECTIVE
FIGURE
HERE
POINTS
ARE
PROJECTED
ONTO
THE
IMAGE
PLANE
BY
DIVIDING
THEM
BY
THEIR
Z
COMPONENT
USING
INHOMOGENEOUS
COORDINATES
THIS
CAN
BE
WRITTEN
AS
X
Z
X
PZ
P
Y
Z
IN
HOMOGENEOUS
COORDINATES
THE
PROJECTION
HAS
A
SIMPLE
LINEAR
FORM
X
P
I
E
WE
DROP
THE
W
COMPONENT
OF
P
THUS
AFTER
PROJECTION
IT
IS
NOT
POSSIBLE
TO
RECOVER
THE
DISTANCE
OF
THE
POINT
FROM
THE
IMAGE
WHICH
MAKES
SENSE
FOR
A
IMAGING
SENSOR
A
FORM
OFTEN
SEEN
IN
COMPUTER
GRAPHICS
SYSTEMS
IS
A
TWO
STEP
PROJECTION
THAT
FIRST
PROJECTS
COORDINATES
INTO
NORMALIZED
DEVICE
COORDINATES
IN
THE
RANGE
X
Y
Z
AND
THEN
RESCALES
THESE
COORDINATES
TO
INTEGER
PIXEL
COORDINATES
USING
A
VIEW
PORT
TRANSFORMATION
WATT
OPENGL
ARB
THE
INITIAL
PERSPECTIVE
PROJECTION
IS
THEN
REPRESENTED
USING
A
MATRIX
X
FAR
RANGE
NEAR
FAR
RANGE
P
WHERE
ZNEAR
AND
ZFAR
ARE
THE
NEAR
AND
FAR
Z
CLIPPING
PLANES
AND
ZRANGE
ZFAR
ZNEAR
NOTE
THAT
THE
FIRST
TWO
ROWS
ARE
ACTUALLY
SCALED
BY
THE
FOCAL
LENGTH
AND
THE
ASPECT
RATIO
SO
THAT
VISIBLE
RAYS
ARE
MAPPED
TO
X
Y
Z
THE
REASON
FOR
KEEPING
THE
THIRD
ROW
RATHER
THAN
DROPPING
IT
IS
THAT
VISIBILITY
OPERATIONS
SUCH
AS
Z
BUFFERING
REQUIRE
A
DEPTH
FOR
EVERY
GRAPHICAL
ELEMENT
THAT
IS
BEING
RENDERED
IF
WE
SET
ZNEAR
ZFAR
AND
SWITCH
THE
SIGN
OF
THE
THIRD
ROW
THE
THIRD
ELEMENT
OF
THE
NORMALIZED
SCREEN
VECTOR
BECOMES
THE
INVERSE
DEPTH
I
E
THE
DISPARITY
OKUTOMI
AND
KANADE
THIS
CAN
BE
QUITE
CONVENIENT
IN
MANY
CASES
SINCE
FOR
CAMERAS
MOVING
AROUND
OUTDOORS
THE
INVERSE
DEPTH
TO
THE
CAMERA
IS
OFTEN
A
MORE
WELL
CONDITIONED
PARAMETERIZATION
THAN
DIRECT
DISTANCE
WHILE
A
REGULAR
IMAGE
SENSOR
HAS
NO
WAY
OF
MEASURING
DISTANCE
TO
A
SURFACE
POINT
RANGE
SENSORS
SECTION
AND
STEREO
MATCHING
ALGORITHMS
CHAPTER
CAN
COMPUTE
SUCH
VALUES
IT
IS
THEN
CONVENIENT
TO
BE
ABLE
TO
MAP
FROM
A
SENSOR
BASED
DEPTH
OR
DISPARITY
VALUE
D
DIRECTLY
BACK
TO
A
LOCATION
USING
THE
INVERSE
OF
A
MATRIX
SECTION
WE
CAN
DO
THIS
IF
WE
REPRESENT
PERSPECTIVE
PROJECTION
USING
A
FULL
RANK
MATRIX
AS
IN
FIGURE
PROJECTION
OF
A
CAMERA
CENTERED
POINT
PC
ONTO
THE
SENSOR
PLANES
AT
LOCATION
P
OC
IS
THE
CAMERA
CENTER
NODAL
POINT
CS
IS
THE
ORIGIN
OF
THE
SENSOR
PLANE
COORDINATE
SYSTEM
AND
SX
AND
SY
ARE
THE
PIXEL
SPACINGS
CAMERA
INTRINSICS
ONCE
WE
HAVE
PROJECTED
A
POINT
THROUGH
AN
IDEAL
PINHOLE
USING
A
PROJECTION
MATRIX
WE
MUST
STILL
TRANSFORM
THE
RESULTING
COORDINATES
ACCORDING
TO
THE
PIXEL
SENSOR
SPACING
AND
THE
RELATIVE
POSITION
OF
THE
SENSOR
PLANE
TO
THE
ORIGIN
FIGURE
SHOWS
AN
ILLUSTRATION
OF
THE
GEOMETRY
INVOLVED
IN
THIS
SECTION
WE
FIRST
PRESENT
A
MAPPING
FROM
PIXEL
COORDINATES
TO
RAYS
USING
A
SENSOR
HOMOGRAPHY
M
SINCE
THIS
IS
EASIER
TO
EXPLAIN
IN
TERMS
OF
PHYSICALLY
MEASURABLE
QUANTITIES
WE
THEN
RELATE
THESE
QUANTITIES
TO
THE
MORE
COMMONLY
USED
CAMERA
IN
TRINSIC
MATRIX
K
WHICH
IS
USED
TO
MAP
CAMERA
CENTERED
POINTS
PC
TO
PIXEL
COORDINATES
X
IMAGE
SENSORS
RETURN
PIXEL
VALUES
INDEXED
BY
INTEGER
PIXEL
COORDINATES
XS
YS
OFTEN
WITH
THE
COORDINATES
STARTING
AT
THE
UPPER
LEFT
CORNER
OF
THE
IMAGE
AND
MOVING
DOWN
AND
TO
THE
RIGHT
THIS
CONVENTION
IS
NOT
OBEYED
BY
ALL
IMAGING
LIBRARIES
BUT
THE
ADJUSTMENT
FOR
OTHER
COORDINATE
SYSTEMS
IS
STRAIGHTFORWARD
TO
MAP
PIXEL
CENTERS
TO
COORDINATES
WE
FIRST
SCALE
THE
XS
YS
VALUES
BY
THE
PIXEL
SPACINGS
SX
SY
SOMETIMES
EXPRESSED
IN
MICRONS
FOR
SOLID
STATE
SENSORS
AND
THEN
DESCRIBE
THE
ORIENTATION
OF
THE
SENSOR
ARRAY
RELATIVE
TO
THE
CAMERA
PROJECTION
CENTER
OC
WITH
AN
ORIGIN
CS
AND
A
ROTATION
RS
FIGURE
THE
COMBINED
TO
PROJECTION
CAN
THEN
BE
WRITTEN
AS
X
P
RS
CS
L
YS
M
SX
THE
FIRST
TWO
COLUMNS
OF
THE
MATRIX
M
ARE
THE
VECTORS
CORRESPONDING
TO
UNIT
STEPS
IN
THE
IMAGE
PIXEL
ARRAY
ALONG
THE
XS
AND
YS
DIRECTIONS
WHILE
THE
THIRD
COLUMN
IS
THE
IMAGE
ARRAY
ORIGIN
CS
THE
MATRIX
M
IS
PARAMETERIZED
BY
EIGHT
UNKNOWNS
THE
THREE
PARAMETERS
DESCRIBING
THE
ROTATION
RS
THE
THREE
PARAMETERS
DESCRIBING
THE
TRANSLATION
CS
AND
THE
TWO
SCALE
FACTORS
SX
SY
NOTE
THAT
WE
IGNORE
HERE
THE
POSSIBILITY
OF
SKEW
BETWEEN
THE
TWO
AXES
ON
THE
IMAGE
PLANE
SINCE
SOLID
STATE
MANUFACTURING
TECHNIQUES
RENDER
THIS
NEGLIGIBLE
IN
PRACTICE
UNLESS
WE
HAVE
ACCURATE
EXTERNAL
KNOWLEDGE
OF
THE
SENSOR
SPACING
OR
SENSOR
ORIENTATION
THERE
ARE
ONLY
SEVEN
DEGREES
OF
FREEDOM
SINCE
THE
DISTANCE
OF
THE
SENSOR
FROM
THE
ORIGIN
CANNOT
BE
TEASED
APART
FROM
THE
SENSOR
SPACING
BASED
ON
EXTERNAL
IMAGE
MEASUREMENT
ALONE
HOWEVER
ESTIMATING
A
CAMERA
MODEL
M
WITH
THE
REQUIRED
SEVEN
DEGREES
OF
FREEDOM
I
E
WHERE
THE
FIRST
TWO
COLUMNS
ARE
ORTHOGONAL
AFTER
AN
APPROPRIATE
RE
SCALING
IS
IMPRACTICAL
SO
MOST
PRACTITIONERS
ASSUME
A
GENERAL
HOMOGENEOUS
MATRIX
FORM
THE
RELATIONSHIP
BETWEEN
THE
PIXEL
CENTER
P
AND
THE
CAMERA
CENTERED
POINT
PC
IS
GIVEN
BY
AN
UNKNOWN
SCALING
P
SPC
WE
CAN
THEREFORE
WRITE
THE
COMPLETE
PROJECTION
BETWEEN
PC
AND
A
HOMOGENEOUS
VERSION
OF
THE
PIXEL
ADDRESS
X
AS
X
ΑM
KPC
THE
MATRIX
K
IS
CALLED
THE
CALIBRATION
MATRIX
AND
DESCRIBES
THE
CAMERA
INTRINSICS
AS
OPPOSED
TO
THE
CAMERA
ORIENTATION
IN
SPACE
WHICH
ARE
CALLED
THE
EXTRINSICS
FROM
THE
ABOVE
DISCUSSION
WE
SEE
THAT
K
HAS
SEVEN
DEGREES
OF
FREEDOM
IN
THEORY
AND
EIGHT
DEGREES
OF
FREEDOM
THE
FULL
DIMENSIONALITY
OF
A
HOMOGENEOUS
MATRIX
IN
PRACTICE
WHY
THEN
DO
MOST
TEXTBOOKS
ON
COMPUTER
VISION
AND
MULTI
VIEW
GEOMETRY
FAUGERAS
HARTLEY
AND
ZISSERMAN
FAUGERAS
AND
LUONG
TREAT
K
AS
AN
UPPER
TRIANGULAR
MATRIX
WITH
FIVE
DEGREES
OF
FREEDOM
WHILE
THIS
IS
USUALLY
NOT
MADE
EXPLICIT
IN
THESE
BOOKS
IT
IS
BECAUSE
WE
CANNOT
RECOVER
THE
FULL
K
MATRIX
BASED
ON
EXTERNAL
MEASUREMENT
ALONE
WHEN
CALIBRATING
A
CAMERA
CHAP
TER
BASED
ON
EXTERNAL
POINTS
OR
OTHER
MEASUREMENTS
TSAI
WE
END
UP
ESTIMATING
THE
INTRINSIC
K
AND
EXTRINSIC
R
T
CAMERA
PARAMETERS
SIMULTANEOUSLY
USING
A
SERIES
OF
MEASUREMENTS
X
K
R
T
PW
P
PW
WHERE
PW
ARE
KNOWN
WORLD
COORDINATES
AND
P
K
R
T
IS
KNOWN
AS
THE
CAMERA
MATRIX
INSPECTING
THIS
EQUATION
WE
SEE
THAT
WE
CAN
POST
MULTIPLY
K
BY
AND
PRE
MULTIPLY
R
T
BY
RT
AND
STILL
END
UP
WITH
A
VALID
CALIBRATION
THUS
IT
IS
IMPOSSIBLE
BASED
ON
IMAGE
MEASUREMENTS
ALONE
TO
KNOW
THE
TRUE
ORIENTATION
OF
THE
SENSOR
AND
THE
TRUE
CAMERA
INTRINSICS
THE
CHOICE
OF
AN
UPPER
TRIANGULAR
FORM
FOR
K
SEEMS
TO
BE
CONVENTIONAL
GIVEN
A
FULL
CAMERA
MATRIX
P
K
R
T
WE
CAN
COMPUTE
AN
UPPER
TRIANGULAR
K
MATRIX
USING
QR
W
FIGURE
SIMPLIFIED
CAMERA
INTRINSICS
SHOWING
THE
FOCAL
LENGTH
F
AND
THE
OPTICAL
CENTER
CX
CY
THE
IMAGE
WIDTH
AND
HEIGHT
ARE
W
AND
H
FACTORIZATION
GOLUB
AND
VAN
LOAN
NOTE
THE
UNFORTUNATE
CLASH
OF
TERMINOLOGIES
IN
MATRIX
ALGEBRA
TEXTBOOKS
R
REPRESENTS
AN
UPPER
TRIANGULAR
RIGHT
OF
THE
DIAGONAL
MATRIX
IN
COMPUTER
VISION
R
IS
AN
ORTHOGONAL
ROTATION
THERE
ARE
SEVERAL
WAYS
TO
WRITE
THE
UPPER
TRIANGULAR
FORM
OF
K
ONE
POSSIBILITY
IS
FX
CX
K
FY
CY
WHICH
USES
INDEPENDENT
FOCAL
LENGTHS
FX
AND
FY
FOR
THE
SENSOR
X
AND
Y
DIMENSIONS
THE
ENTRY
ENCODES
ANY
POSSIBLE
SKEW
BETWEEN
THE
SENSOR
AXES
DUE
TO
THE
SENSOR
NOT
BEING
MOUNTED
PERPENDICULAR
TO
THE
OPTICAL
AXIS
AND
CX
CY
DENOTES
THE
OPTICAL
CENTER
EXPRESSED
IN
PIXEL
COORDINATES
ANOTHER
POSSIBILITY
IS
F
CX
K
AF
CY
WHERE
THE
ASPECT
RATIO
A
HAS
BEEN
MADE
EXPLICIT
AND
A
COMMON
FOCAL
LENGTH
F
IS
USED
IN
PRACTICE
FOR
MANY
APPLICATIONS
AN
EVEN
SIMPLER
FORM
CAN
BE
OBTAINED
BY
SETTING
A
AND
F
CX
K
F
CY
OFTEN
SETTING
THE
ORIGIN
AT
ROUGHLY
THE
CENTER
OF
THE
IMAGE
E
G
CX
CY
W
H
WHERE
W
AND
H
ARE
THE
IMAGE
HEIGHT
AND
WIDTH
CAN
RESULT
IN
A
PERFECTLY
USABLE
CAMERA
MODEL
WITH
A
SINGLE
UNKNOWN
I
E
THE
FOCAL
LENGTH
F
FIGURE
CENTRAL
PROJECTION
SHOWING
THE
RELATIONSHIP
BETWEEN
THE
AND
COORDI
NATES
P
AND
X
AS
WELL
AS
THE
RELATIONSHIP
BETWEEN
THE
FOCAL
LENGTH
F
IMAGE
WIDTH
W
AND
THE
FIELD
OF
VIEW
Θ
FIGURE
SHOWS
HOW
THESE
QUANTITIES
CAN
BE
VISUALIZED
AS
PART
OF
A
SIMPLIFIED
IMAGING
MODEL
NOTE
THAT
NOW
WE
HAVE
PLACED
THE
IMAGE
PLANE
IN
FRONT
OF
THE
NODAL
POINT
PROJECTION
CENTER
OF
THE
LENS
THE
SENSE
OF
THE
Y
AXIS
HAS
ALSO
BEEN
FLIPPED
TO
GET
A
COORDINATE
SYSTEM
COMPATIBLE
WITH
THE
WAY
THAT
MOST
IMAGING
LIBRARIES
TREAT
THE
VERTICAL
ROW
COORDINATE
CER
TAIN
GRAPHICS
LIBRARIES
SUCH
AS
USE
A
LEFT
HANDED
COORDINATE
SYSTEM
WHICH
CAN
LEAD
TO
SOME
CONFUSION
A
NOTE
ON
FOCAL
LENGTHS
THE
ISSUE
OF
HOW
TO
EXPRESS
FOCAL
LENGTHS
IS
ONE
THAT
OFTEN
CAUSES
CONFUSION
IN
IMPLEMENTING
COMPUTER
VISION
ALGORITHMS
AND
DISCUSSING
THEIR
RESULTS
THIS
IS
BECAUSE
THE
FOCAL
LENGTH
DEPENDS
ON
THE
UNITS
USED
TO
MEASURE
PIXELS
IF
WE
NUMBER
PIXEL
COORDINATES
USING
INTEGER
VALUES
SAY
W
H
THE
FOCAL
LENGTH
F
AND
CAMERA
CENTER
CX
CY
IN
CAN
BE
EXPRESSED
AS
PIXEL
VALUES
HOW
DO
THESE
QUAN
TITIES
RELATE
TO
THE
MORE
FAMILIAR
FOCAL
LENGTHS
USED
BY
PHOTOGRAPHERS
FIGURE
ILLUSTRATES
THE
RELATIONSHIP
BETWEEN
THE
FOCAL
LENGTH
F
THE
SENSOR
WIDTH
W
AND
THE
FIELD
OF
VIEW
Θ
WHICH
OBEY
THE
FORMULA
Θ
W
W
Θ
L
FOR
CONVENTIONAL
FILM
CAMERAS
W
AND
HENCE
F
IS
ALSO
EXPRESSED
IN
MILLIMETERS
SINCE
WE
WORK
WITH
DIGITAL
IMAGES
IT
IS
MORE
CONVENIENT
TO
EXPRESS
W
IN
PIXELS
SO
THAT
THE
FOCAL
LENGTH
F
CAN
BE
USED
DIRECTLY
IN
THE
CALIBRATION
MATRIX
K
AS
IN
ANOTHER
POSSIBILITY
IS
TO
SCALE
THE
PIXEL
COORDINATES
SO
THAT
THEY
GO
FROM
ALONG
THE
LONGER
IMAGE
DIMENSION
AND
A
A
ALONG
THE
SHORTER
AXIS
WHERE
A
IS
THE
IMAGE
ASPECT
RATIO
AS
OPPOSED
TO
THE
SENSOR
CELL
ASPECT
RATIO
INTRODUCED
EARLIER
THIS
CAN
BE
ACCOMPLISHED
USING
MODIFIED
NORMALIZED
DEVICE
COORDINATES
XIS
W
AND
YSI
H
WHERE
MAX
W
H
THIS
HAS
THE
ADVANTAGE
THAT
THE
FOCAL
LENGTH
F
AND
OPTICAL
CENTER
CX
CY
BECOME
INDEPENDENT
OF
THE
IMAGE
RESOLUTION
WHICH
CAN
BE
USEFUL
WHEN
USING
MULTI
RESOLUTION
IMAGE
PROCESSING
ALGORITHMS
SUCH
AS
IMAGE
PYRAMIDS
SECTION
THE
USE
OF
INSTEAD
OF
W
ALSO
MAKES
THE
FOCAL
LENGTH
THE
SAME
FOR
LANDSCAPE
HORIZONTAL
AND
PORTRAIT
VERTICAL
PICTURES
AS
IS
THE
CASE
IN
PHOTOGRAPHY
IN
SOME
COMPUTER
GRAPHICS
TEXTBOOKS
AND
SYSTEMS
NORMALIZED
DEVICE
COORDINATES
GO
FROM
WHICH
REQUIRES
THE
USE
OF
TWO
DIFFERENT
FOCAL
LENGTHS
TO
DESCRIBE
THE
CAMERA
INTRINSICS
WATT
OPENGL
ARB
SETTING
W
IN
WE
OBTAIN
THE
SIMPLER
UNITLESS
RELATIONSHIP
F
TAN
Θ
THE
CONVERSION
BETWEEN
THE
VARIOUS
FOCAL
LENGTH
REPRESENTATIONS
IS
STRAIGHTFORWARD
E
G
TO
GO
FROM
A
UNITLESS
F
TO
ONE
EXPRESSED
IN
PIXELS
MULTIPLY
BY
W
WHILE
TO
CONVERT
FROM
AN
F
EXPRESSED
IN
PIXELS
TO
THE
EQUIVALENT
FOCAL
LENGTH
MULTIPLY
BY
W
CAMERA
MATRIX
NOW
THAT
WE
HAVE
SHOWN
HOW
TO
PARAMETERIZE
THE
CALIBRATION
MATRIX
K
WE
CAN
PUT
THE
CAMERA
INTRINSICS
AND
EXTRINSICS
TOGETHER
TO
OBTAIN
A
SINGLE
CAMERA
MATRIX
P
K
R
T
L
IT
IS
SOMETIMES
PREFERABLE
TO
USE
AN
INVERTIBLE
MATRIX
WHICH
CAN
BE
OBTAINED
BY
NOT
DROPPING
THE
LAST
ROW
IN
THE
P
MATRIX
P
R
K
L
R
R
T
L
K
E
WHERE
E
IS
A
RIGID
BODY
EUCLIDEAN
TRANSFORMATION
AND
K
IS
THE
FULL
RANK
CALIBRATION
MATRIX
THE
CAMERA
MATRIX
P
CAN
BE
USED
TO
MAP
DIRECTLY
FROM
WORLD
COORDINATES
P
W
XW
YW
ZW
TO
SCREEN
COORDINATES
PLUS
DISPARITY
XS
XS
YS
D
XS
P
P
W
WHERE
INDICATES
EQUALITY
UP
TO
SCALE
NOTE
THAT
AFTER
MULTIPLICATION
BY
P
THE
VECTOR
IS
DIVIDED
BY
THE
THIRD
ELEMENT
OF
THE
VECTOR
TO
OBTAIN
THE
NORMALIZED
FORM
XS
XS
YS
D
TO
MAKE
THE
CONVERSION
TRULY
ACCURATE
AFTER
A
DOWNSAMPLING
STEP
IN
A
PYRAMID
FLOATING
POINT
VALUES
OF
W
AND
H
WOULD
HAVE
TO
BE
MAINTAINED
SINCE
THEY
CAN
BECOME
NON
INTEGRAL
IF
THEY
ARE
EVER
ODD
AT
A
LARGER
RESOLUTION
IN
THE
PYRAMID
D
D
D
D
D
D
D
PARALLAX
XW
YW
ZW
C
XS
YS
D
Z
Z
IMAGE
PLANE
PLANE
D
INVERSE
DEPTH
D
PROJECTIVE
DEPTH
FIGURE
REGULAR
DISPARITY
INVERSE
DEPTH
AND
PROJECTIVE
DEPTH
PARALLAX
FROM
A
REFERENCE
PLANE
PLANE
PLUS
PARALLAX
PROJECTIVE
DEPTH
IN
GENERAL
WHEN
USING
THE
MATRIX
P
WE
HAVE
THE
FREEDOM
TO
REMAP
THE
LAST
ROW
TO
WHATEVER
SUITS
OUR
PURPOSE
RATHER
THAN
JUST
BEING
THE
STANDARD
INTERPRETATION
OF
DISPARITY
AS
INVERSE
DEPTH
LET
US
RE
WRITE
THE
LAST
ROW
OF
P
AS
WHERE
WE
THEN
HAVE
THE
EQUATION
D
Nˆ
P
C
Z
W
WHERE
Z
P
W
RZ
PW
C
IS
THE
DISTANCE
OF
PW
FROM
THE
CAMERA
CENTER
C
ALONG
THE
OPTICAL
AXIS
Z
FIGURE
THUS
WE
CAN
INTERPRET
D
AS
THE
PROJECTIVE
DISPARITY
OR
PROJECTIVE
DEPTH
OF
A
SCENE
POINT
PW
FROM
THE
REFERENCE
PLANE
PW
SZELISKI
AND
COUGHLAN
SZELISKI
AND
GOLLAND
SHADE
GORTLER
HE
ET
AL
BAKER
SZELISKI
AND
ANANDAN
THE
PROJECTIVE
DEPTH
IS
ALSO
SOMETIMES
CALLED
PARALLAX
IN
RECONSTRUCTION
ALGORITHMS
THAT
USE
THE
TERM
PLANE
PLUS
PARALLAX
KUMAR
ANANDAN
AND
HANNA
SAWHNEY
SETTING
AND
I
E
PUTTING
THE
REFERENCE
PLANE
AT
INFINITY
RESULTS
IN
THE
MORE
STANDARD
D
Z
VERSION
OF
DISPARITY
OKUTOMI
AND
KANADE
ANOTHER
WAY
TO
SEE
THIS
IS
TO
INVERT
THE
P
MATRIX
SO
THAT
WE
CAN
MAP
PIXELS
PLUS
DISPARITY
DIRECTLY
BACK
TO
POINTS
P
W
P
IN
GENERAL
WE
CAN
CHOOSE
P
TO
HAVE
WHATEVER
FORM
IS
CONVENIENT
I
E
TO
SAMPLE
SPACE
US
ING
AN
ARBITRARY
PROJECTION
THIS
CAN
COME
IN
PARTICULARLY
HANDY
WHEN
SETTING
UP
MULTI
VIEW
STEREO
RECONSTRUCTION
ALGORITHMS
SINCE
IT
ALLOWS
US
TO
SWEEP
A
SERIES
OF
PLANES
SECTION
THROUGH
SPACE
WITH
A
VARIABLE
PROJECTIVE
SAMPLING
THAT
BEST
MATCHES
THE
SENSED
IMAGE
MO
TIONS
COLLINS
SZELISKI
AND
GOLLAND
SAITO
AND
KANADE
Y
D
N
P
A
B
FIGURE
A
POINT
IS
PROJECTED
INTO
TWO
IMAGES
A
RELATIONSHIP
BETWEEN
THE
POINT
CO
ORDINATE
X
Y
Z
AND
THE
PROJECTED
POINT
X
Y
D
B
PLANAR
HOMOGRAPHY
INDUCED
BY
POINTS
ALL
LYING
ON
A
COMMON
PLANE
P
MAPPING
FROM
ONE
CAMERA
TO
ANOTHER
WHAT
HAPPENS
WHEN
WE
TAKE
TWO
IMAGES
OF
A
SCENE
FROM
DIFFERENT
CAMERA
POSITIONS
OR
ORIENTATIONS
FIGURE
USING
THE
FULL
RANK
CAMERA
MATRIX
P
K
E
FROM
WE
CAN
WRITE
THE
PROJECTION
FROM
WORLD
TO
SCREEN
COORDINATES
AS
X
K
P
ASSUMING
THAT
WE
KNOW
THE
Z
BUFFER
OR
DISPARITY
VALUE
FOR
A
PIXEL
IN
ONE
IMAGE
WE
CAN
COMPUTE
THE
POINT
LOCATION
P
USING
P
E
AND
THEN
PROJECT
IT
INTO
ANOTHER
IMAGE
YIELDING
X
K
E
P
K
E
E
P
P
M
X
UNFORTUNATELY
WE
DO
NOT
USUALLY
HAVE
ACCESS
TO
THE
DEPTH
COORDINATES
OF
PIXELS
IN
A
REGULAR
PHOTOGRAPHIC
IMAGE
HOWEVER
FOR
A
PLANAR
SCENE
AS
DISCUSSED
ABOVE
IN
WE
CAN
REPLACE
THE
LAST
ROW
OF
P
IN
WITH
A
GENERAL
PLANE
EQUATION
P
THAT
MAPS
POINTS
ON
THE
PLANE
TO
VALUES
FIGURE
THUS
IF
WE
SET
WE
CAN
IGNORE
THE
LAST
COLUMN
OF
M
IN
AND
ALSO
ITS
LAST
ROW
SINCE
WE
DO
NOT
CARE
ABOUT
THE
FINAL
Z
BUFFER
DEPTH
THE
MAPPING
EQUATION
THUS
REDUCES
TO
X
H
WHERE
H
IS
A
GENERAL
HOMOGRAPHY
MATRIX
AND
X
AND
X
ARE
NOW
HOMOGENEOUS
COORDINATES
I
E
VECTORS
SZELISKI
THIS
JUSTIFIES
THE
USE
OF
THE
PARAMETER
HOMOG
RAPHY
AS
A
GENERAL
ALIGNMENT
MODEL
FOR
MOSAICS
OF
PLANAR
SCENES
MANN
AND
PICARD
SZELISKI
THE
OTHER
SPECIAL
CASE
WHERE
WE
DO
NOT
NEED
TO
KNOW
DEPTH
TO
PERFORM
INTER
CAMERA
MAPPING
IS
WHEN
THE
CAMERA
IS
UNDERGOING
PURE
ROTATION
SECTION
I
E
WHEN
IN
THIS
CASE
WE
CAN
WRITE
X
WHICH
AGAIN
CAN
BE
REPRESENTED
WITH
A
HOMOGRAPHY
IF
WE
ASSUME
THAT
THE
CALIBRATION
MATRICES
HAVE
KNOWN
ASPECT
RATIOS
AND
CENTERS
OF
PROJECTION
THIS
HOMOGRAPHY
CAN
BE
PARAMETERIZED
BY
THE
ROTATION
AMOUNT
AND
THE
TWO
UNKNOWN
FOCAL
LENGTHS
THIS
PARTICULAR
FORMULATION
IS
COMMONLY
USED
IN
IMAGE
STITCHING
APPLICATIONS
SECTION
OBJECT
CENTERED
PROJECTION
WHEN
WORKING
WITH
LONG
FOCAL
LENGTH
LENSES
IT
OFTEN
BECOMES
DIFFICULT
TO
RELIABLY
ESTIMATE
THE
FOCAL
LENGTH
FROM
IMAGE
MEASUREMENTS
ALONE
THIS
IS
BECAUSE
THE
FOCAL
LENGTH
AND
THE
DISTANCE
TO
THE
OBJECT
ARE
HIGHLY
CORRELATED
AND
IT
BECOMES
DIFFICULT
TO
TEASE
THESE
TWO
EFFECTS
APART
FOR
EXAMPLE
THE
CHANGE
IN
SCALE
OF
AN
OBJECT
VIEWED
THROUGH
A
ZOOM
TELEPHOTO
LENS
CAN
EITHER
BE
DUE
TO
A
ZOOM
CHANGE
OR
A
MOTION
TOWARDS
THE
USER
THIS
EFFECT
WAS
PUT
TO
DRAMATIC
USE
IN
SOME
OF
ALFRED
HITCHCOCK
FILM
VERTIGO
WHERE
THE
SIMULTANEOUS
CHANGE
OF
ZOOM
AND
CAMERA
MOTION
PRODUCES
A
DISQUIETING
EFFECT
THIS
AMBIGUITY
BECOMES
CLEARER
IF
WE
WRITE
OUT
THE
PROJECTION
EQUATION
CORRESPONDING
TO
THE
SIMPLE
CALIBRATION
MATRIX
K
X
F
RX
P
TX
C
RZ
P
TZ
Y
F
RY
P
TY
C
RZ
P
TZ
WHERE
RX
RY
AND
RZ
ARE
THE
THREE
ROWS
OF
R
IF
THE
DISTANCE
TO
THE
OBJECT
CENTER
TZ
THE
SIZE
OF
THE
OBJECT
THE
DENOMINATOR
IS
APPROXIMATELY
TZ
AND
THE
OVERALL
SCALE
OF
THE
PROJECTED
OBJECT
DEPENDS
ON
THE
RATIO
OF
F
TO
TZ
IT
THEREFORE
BECOMES
DIFFICULT
TO
DISENTANGLE
THESE
TWO
QUANTITIES
TO
SEE
THIS
MORE
CLEARLY
LET
ΗZ
T
Z
AND
ΗZF
WE
CAN
THEN
RE
WRITE
THE
ABOVE
EQUATIONS
AS
X
RX
P
TX
ΗZRZ
P
Y
RY
P
TY
ΗZRZ
P
CX
CY
SZELISKI
AND
KANG
PIGHIN
HECKER
LISCHINSKI
ET
AL
THE
SCALE
OF
THE
PROJECTION
CAN
BE
RELIABLY
ESTIMATED
IF
WE
ARE
LOOKING
AT
A
KNOWN
OBJECT
I
E
THE
COORDINATES
P
ARE
KNOWN
THE
INVERSE
DISTANCE
ΗZ
IS
NOW
MOSTLY
DECOUPLED
FROM
THE
ESTIMATES
OF
AND
CAN
BE
ESTIMATED
FROM
THE
AMOUNT
OF
FORESHORTENING
AS
THE
OBJECT
ROTATES
FURTHERMORE
AS
THE
LENS
BECOMES
LONGER
I
E
THE
PROJECTION
MODEL
BECOMES
ORTHOGRAPHIC
THERE
IS
NO
NEED
TO
REPLACE
A
PERSPECTIVE
IMAGING
MODEL
WITH
AN
ORTHOGRAPHIC
ONE
SINCE
THE
SAME
EQUATION
CAN
BE
USED
WITH
ΗZ
AS
OPPOSED
TO
F
AND
TZ
BOTH
GOING
TO
INFINITY
THIS
ALLOWS
US
TO
FORM
A
NATURAL
LINK
BETWEEN
ORTHOGRAPHIC
RECONSTRUCTION
TECHNIQUES
SUCH
AS
FACTORIZATION
AND
THEIR
PROJECTIVE
PERSPECTIVE
COUNTERPARTS
SECTION
LENS
DISTORTIONS
THE
ABOVE
IMAGING
MODELS
ALL
ASSUME
THAT
CAMERAS
OBEY
A
LINEAR
PROJECTION
MODEL
WHERE
STRAIGHT
LINES
IN
THE
WORLD
RESULT
IN
STRAIGHT
LINES
IN
THE
IMAGE
THIS
FOLLOWS
AS
A
NATURAL
CONSEQUENCE
OF
LINEAR
MATRIX
OPERATIONS
BEING
APPLIED
TO
HOMOGENEOUS
COORDINATES
UNFOR
TUNATELY
MANY
WIDE
ANGLE
LENSES
HAVE
NOTICEABLE
RADIAL
DISTORTION
WHICH
MANIFESTS
ITSELF
AS
A
VISIBLE
CURVATURE
IN
THE
PROJECTION
OF
STRAIGHT
LINES
SEE
SECTION
FOR
A
MORE
DETAILED
DISCUSSION
OF
LENS
OPTICS
INCLUDING
CHROMATIC
ABERRATION
UNLESS
THIS
DISTORTION
IS
TAKEN
INTO
ACCOUNT
IT
BECOMES
IMPOSSIBLE
TO
CREATE
HIGHLY
ACCURATE
PHOTOREALISTIC
RECONSTRUCTIONS
FOR
EXAMPLE
IMAGE
MOSAICS
CONSTRUCTED
WITHOUT
TAKING
RADIAL
DISTORTION
INTO
ACCOUNT
WILL
OFTEN
EXHIBIT
BLURRING
DUE
TO
THE
MIS
REGISTRATION
OF
CORRESPONDING
FEATURES
BEFORE
PIXEL
BLENDING
CHAPTER
FORTUNATELY
COMPENSATING
FOR
RADIAL
DISTORTION
IS
NOT
THAT
DIFFICULT
IN
PRACTICE
FOR
MOST
LENSES
A
SIMPLE
QUARTIC
MODEL
OF
DISTORTION
CAN
PRODUCE
GOOD
RESULTS
LET
XC
YC
BE
THE
PIXEL
COORDINATES
OBTAINED
AFTER
PERSPECTIVE
DIVISION
BUT
BEFORE
SCALING
BY
FOCAL
LENGTH
F
AND
SHIFTING
BY
THE
OPTICAL
CENTER
CX
CY
I
E
X
RX
P
TX
RZ
P
TZ
Y
RY
P
TY
RZ
P
TZ
THE
RADIAL
DISTORTION
MODEL
SAYS
THAT
COORDINATES
IN
THE
OBSERVED
IMAGES
ARE
DISPLACED
AWAY
BARREL
DISTORTION
OR
TOWARDS
PINCUSHION
DISTORTION
THE
IMAGE
CENTER
BY
AN
AMOUNT
PROPOR
TIONAL
TO
THEIR
RADIAL
DISTANCE
FIGURE
B
THE
SIMPLEST
RADIAL
DISTORTION
MODELS
USE
LOW
ORDER
POLYNOMIALS
E
G
XˆC
XC
C
C
YˆC
YC
C
C
ANAMORPHIC
LENSES
WHICH
ARE
WIDELY
USED
IN
FEATURE
FILM
PRODUCTION
DO
NOT
FOLLOW
THIS
RADIAL
DISTORTION
MODEL
INSTEAD
THEY
CAN
BE
THOUGHT
OF
TO
A
FIRST
APPROXIMATION
AS
INDUCING
DIFFERENT
VERTICAL
AND
HORIZONTAL
SCALINGS
I
E
NON
SQUARE
PIXELS
A
B
C
FIGURE
RADIAL
LENS
DISTORTIONS
A
BARREL
B
PINCUSHION
AND
C
FISHEYE
THE
FISHEYE
IMAGE
SPANS
ALMOST
FROM
SIDE
TO
SIDE
WHERE
AND
AND
ARE
CALLED
THE
RADIAL
DISTORTION
PARAMETERS
AFTER
THE
C
C
C
RADIAL
DISTORTION
STEP
THE
FINAL
PIXEL
COORDINATES
CAN
BE
COMPUTED
USING
XS
F
XIC
CX
YS
F
YCI
CY
A
VARIETY
OF
TECHNIQUES
CAN
BE
USED
TO
ESTIMATE
THE
RADIAL
DISTORTION
PARAMETERS
FOR
A
GIVEN
LENS
AS
DISCUSSED
IN
SECTION
SOMETIMES
THE
ABOVE
SIMPLIFIED
MODEL
DOES
NOT
MODEL
THE
TRUE
DISTORTIONS
PRODUCED
BY
COMPLEX
LENSES
ACCURATELY
ENOUGH
ESPECIALLY
AT
VERY
WIDE
ANGLES
A
MORE
COMPLETE
ANA
LYTIC
MODEL
ALSO
INCLUDES
TANGENTIAL
DISTORTIONS
AND
DECENTERING
DISTORTIONS
SLAMA
BUT
THESE
DISTORTIONS
ARE
NOT
COVERED
IN
THIS
BOOK
FISHEYE
LENSES
FIGURE
REQUIRE
A
MODEL
THAT
DIFFERS
FROM
TRADITIONAL
POLYNOMIAL
MODELS
OF
RADIAL
DISTORTION
FISHEYE
LENSES
BEHAVE
TO
A
FIRST
APPROXIMATION
AS
EQUI
DISTANCE
PROJECTORS
OF
ANGLES
AWAY
FROM
THE
OPTICAL
AXIS
XIONG
AND
TURKOWSKI
WHICH
IS
THE
SAME
AS
THE
POLAR
PROJECTION
DESCRIBED
BY
EQUATIONS
XIONG
AND
TURKOWSKI
DESCRIBE
HOW
THIS
MODEL
CAN
BE
EXTENDED
WITH
THE
ADDITION
OF
AN
EXTRA
QUADRATIC
COR
RECTION
IN
Φ
AND
HOW
THE
UNKNOWN
PARAMETERS
CENTER
OF
PROJECTION
SCALING
FACTOR
ETC
CAN
BE
ESTIMATED
FROM
A
SET
OF
OVERLAPPING
FISHEYE
IMAGES
USING
A
DIRECT
INTENSITY
BASED
NON
LINEAR
MINIMIZATION
ALGORITHM
FOR
EVEN
LARGER
LESS
REGULAR
DISTORTIONS
A
PARAMETRIC
DISTORTION
MODEL
USING
SPLINES
MAY
BE
NECESSARY
GOSHTASBY
IF
THE
LENS
DOES
NOT
HAVE
A
SINGLE
CENTER
OF
PROJECTION
IT
SOMETIMES
THE
RELATIONSHIP
BETWEEN
XC
AND
XˆC
IS
EXPRESSED
THE
OTHER
WAY
AROUND
I
E
XC
XˆC
THIS
IS
CONVENIENT
IF
WE
MAP
IMAGE
PIXELS
INTO
WARPED
RAYS
BY
DIVIDING
THROUGH
BY
F
WE
CAN
THEN
UNDISTORT
THE
RAYS
AND
HAVE
TRUE
RAYS
IN
SPACE
MAY
BECOME
NECESSARY
TO
MODEL
THE
LINE
AS
OPPOSED
TO
DIRECTION
CORRESPONDING
TO
EACH
PIXEL
SEPARATELY
GREMBAN
THORPE
AND
KANADE
CHAMPLEBOUX
LAVALLE
E
SAUTOT
ET
AL
GROSSBERG
AND
NAYAR
STURM
AND
RAMALINGAM
TARDIF
STURM
TRUDEAU
ET
AL
SOME
OF
THESE
TECHNIQUES
ARE
DESCRIBED
IN
MORE
DETAIL
IN
SECTION
WHICH
DISCUSSES
HOW
TO
CALIBRATE
LENS
DISTORTIONS
THERE
IS
ONE
SUBTLE
ISSUE
ASSOCIATED
WITH
THE
SIMPLE
RADIAL
DISTORTION
MODEL
THAT
IS
OFTEN
GLOSSED
OVER
WE
HAVE
INTRODUCED
A
NON
LINEARITY
BETWEEN
THE
PERSPECTIVE
PROJECTION
AND
FINAL
SENSOR
ARRAY
PROJECTION
STEPS
THEREFORE
WE
CANNOT
IN
GENERAL
POST
MULTIPLY
AN
ARBITRARY
MATRIX
K
WITH
A
ROTATION
TO
PUT
IT
INTO
UPPER
TRIANGULAR
FORM
AND
ABSORB
THIS
INTO
THE
GLOBAL
ROTATION
HOWEVER
THIS
SITUATION
IS
NOT
AS
BAD
AS
IT
MAY
AT
FIRST
APPEAR
FOR
MANY
APPLICATIONS
KEEPING
THE
SIMPLIFIED
DIAGONAL
FORM
OF
IS
STILL
AN
ADEQUATE
MODEL
FURTHERMORE
IF
WE
CORRECT
RADIAL
AND
OTHER
DISTORTIONS
TO
AN
ACCURACY
WHERE
STRAIGHT
LINES
ARE
PRESERVED
WE
HAVE
ESSENTIALLY
CONVERTED
THE
SENSOR
BACK
INTO
A
LINEAR
IMAGER
AND
THE
PREVIOUS
DECOMPOSITION
STILL
APPLIES
PHOTOMETRIC
IMAGE
FORMATION
IN
MODELING
THE
IMAGE
FORMATION
PROCESS
WE
HAVE
DESCRIBED
HOW
GEOMETRIC
FEATURES
IN
THE
WORLD
ARE
PROJECTED
INTO
FEATURES
IN
AN
IMAGE
HOWEVER
IMAGES
ARE
NOT
COMPOSED
OF
FEATURES
INSTEAD
THEY
ARE
MADE
UP
OF
DISCRETE
COLOR
OR
INTENSITY
VALUES
WHERE
DO
THESE
VALUES
COME
FROM
HOW
DO
THEY
RELATE
TO
THE
LIGHTING
IN
THE
ENVIRONMENT
SURFACE
PROPERTIES
AND
GEOMETRY
CAMERA
OPTICS
AND
SENSOR
PROPERTIES
FIGURE
IN
THIS
SECTION
WE
DEVELOP
A
SET
OF
MODELS
TO
DESCRIBE
THESE
INTERACTIONS
AND
FORMULATE
A
GENERATIVE
PROCESS
OF
IMAGE
FORMATION
A
MORE
DETAILED
TREATMENT
OF
THESE
TOPICS
CAN
BE
FOUND
IN
OTHER
TEXTBOOKS
ON
COMPUTER
GRAPHICS
AND
IMAGE
SYNTHESIS
GLASSNER
WEYRICH
LAWRENCE
LENSCH
ET
AL
FOLEY
VAN
DAM
FEINER
ET
AL
WATT
COHEN
AND
WALLACE
SILLION
AND
PUECH
LIGHTING
IMAGES
CANNOT
EXIST
WITHOUT
LIGHT
TO
PRODUCE
AN
IMAGE
THE
SCENE
MUST
BE
ILLUMINATED
WITH
ONE
OR
MORE
LIGHT
SOURCES
CERTAIN
MODALITIES
SUCH
AS
FLUORESCENT
MICROSCOPY
AND
X
RAY
TOMOGRAPHY
DO
NOT
FIT
THIS
MODEL
BUT
WE
DO
NOT
DEAL
WITH
THEM
IN
THIS
BOOK
LIGHT
SOURCES
CAN
GENERALLY
BE
DIVIDED
INTO
POINT
AND
AREA
LIGHT
SOURCES
A
POINT
LIGHT
SOURCE
ORIGINATES
AT
A
SINGLE
LOCATION
IN
SPACE
E
G
A
SMALL
LIGHT
BULB
POTENTIALLY
AT
INFINITY
E
G
THE
SUN
NOTE
THAT
FOR
SOME
APPLICATIONS
SUCH
AS
MODELING
SOFT
SHADOWS
PENUMBRAS
THE
SUN
MAY
HAVE
TO
BE
TREATED
AS
AN
AREA
LIGHT
SOURCE
IN
ADDITION
TO
ITS
LOCATION
A
POINT
LIGHT
SOURCE
HAS
AN
INTENSITY
AND
A
COLOR
SPECTRUM
I
E
A
DISTRIBUTION
OVER
FIGURE
A
SIMPLIFIED
MODEL
OF
PHOTOMETRIC
IMAGE
FORMATION
LIGHT
IS
EMITTED
BY
ONE
OR
MORE
LIGHT
SOURCES
AND
IS
THEN
REFLECTED
FROM
AN
OBJECT
SURFACE
A
PORTION
OF
THIS
LIGHT
IS
DIRECTED
TOWARDS
THE
CAMERA
THIS
SIMPLIFIED
MODEL
IGNORES
MULTIPLE
REFLECTIONS
WHICH
OFTEN
OCCUR
IN
REAL
WORLD
SCENES
WAVELENGTHS
L
Λ
THE
INTENSITY
OF
A
LIGHT
SOURCE
FALLS
OFF
WITH
THE
SQUARE
OF
THE
DISTANCE
BETWEEN
THE
SOURCE
AND
THE
OBJECT
BEING
LIT
BECAUSE
THE
SAME
LIGHT
IS
BEING
SPREAD
OVER
A
LARGER
SPHERICAL
AREA
A
LIGHT
SOURCE
MAY
ALSO
HAVE
A
DIRECTIONAL
FALLOFF
DEPENDENCE
BUT
WE
IGNORE
THIS
IN
OUR
SIMPLIFIED
MODEL
AREA
LIGHT
SOURCES
ARE
MORE
COMPLICATED
A
SIMPLE
AREA
LIGHT
SOURCE
SUCH
AS
A
FLUORESCENT
CEILING
LIGHT
FIXTURE
WITH
A
DIFFUSER
CAN
BE
MODELED
AS
A
FINITE
RECTANGULAR
AREA
EMITTING
LIGHT
EQUALLY
IN
ALL
DIRECTIONS
COHEN
AND
WALLACE
SILLION
AND
PUECH
GLASSNER
WHEN
THE
DISTRIBUTION
IS
STRONGLY
DIRECTIONAL
A
FOUR
DIMENSIONAL
LIGHTFIELD
CAN
BE
USED
INSTEAD
ASHDOWN
A
MORE
COMPLEX
LIGHT
DISTRIBUTION
THAT
APPROXIMATES
SAY
THE
INCIDENT
ILLUMINATION
ON
AN
OBJECT
SITTING
IN
AN
OUTDOOR
COURTYARD
CAN
OFTEN
BE
REPRESENTED
USING
AN
ENVIRONMENT
MAP
GREENE
ORIGINALLY
CALLED
A
REFLECTION
MAP
BLINN
AND
NEWELL
THIS
REPRESENTA
TION
MAPS
INCIDENT
LIGHT
DIRECTIONS
Vˆ
TO
COLOR
VALUES
OR
WAVELENGTHS
Λ
L
Vˆ
Λ
AND
IS
EQUIVALENT
TO
ASSUMING
THAT
ALL
LIGHT
SOURCES
ARE
AT
INFINITY
ENVIRONMENT
MAPS
CAN
BE
REPRESENTED
AS
A
COLLECTION
OF
CUBICAL
FACES
GREENE
AS
A
SINGLE
LONGITUDE
LATITUDE
MAP
BLINN
AND
NEWELL
OR
AS
THE
IMAGE
OF
A
REFLECTING
SPHERE
WATT
A
CONVENIENT
WAY
TO
GET
A
ROUGH
MODEL
OF
A
REAL
WORLD
ENVIRONMENT
MAP
IS
TO
TAKE
AN
IMAGE
OF
A
REFLECTIVE
MIRRORED
SPHERE
AND
TO
UNWRAP
THIS
IMAGE
ONTO
THE
DESIRED
ENVIRONMENT
MAP
DEBEVEC
WATT
GIVES
A
NICE
DISCUSSION
OF
ENVIRONMENT
MAPPING
INCLUDING
THE
FORMULAS
NEEDED
TO
MAP
DIRECTIONS
TO
PIXELS
FOR
THE
THREE
MOST
COMMONLY
USED
REPRESENTATIONS
A
B
FIGURE
A
LIGHT
SCATTERS
WHEN
IT
HITS
A
SURFACE
B
THE
BIDIRECTIONAL
REFLECTANCE
DISTRIBUTION
FUNCTION
BRDF
F
ΘI
ΦI
ΘR
ΦR
IS
PARAMETERIZED
BY
THE
ANGLES
THAT
THE
INCI
DENT
VˆI
AND
REFLECTED
VˆR
LIGHT
RAY
DIRECTIONS
MAKE
WITH
THE
LOCAL
SURFACE
COORDINATE
FRAME
DˆX
DˆY
Nˆ
REFLECTANCE
AND
SHADING
WHEN
LIGHT
HITS
AN
OBJECT
SURFACE
IT
IS
SCATTERED
AND
REFLECTED
FIGURE
MANY
DIFFERENT
MODELS
HAVE
BEEN
DEVELOPED
TO
DESCRIBE
THIS
INTERACTION
IN
THIS
SECTION
WE
FIRST
DESCRIBE
THE
MOST
GENERAL
FORM
THE
BIDIRECTIONAL
REFLECTANCE
DISTRIBUTION
FUNCTION
AND
THEN
LOOK
AT
SOME
MORE
SPECIALIZED
MODELS
INCLUDING
THE
DIFFUSE
SPECULAR
AND
PHONG
SHADING
MODELS
WE
ALSO
DISCUSS
HOW
THESE
MODELS
CAN
BE
USED
TO
COMPUTE
THE
GLOBAL
ILLUMINATION
CORRESPONDING
TO
A
SCENE
THE
BIDIRECTIONAL
REFLECTANCE
DISTRIBUTION
FUNCTION
BRDF
THE
MOST
GENERAL
MODEL
OF
LIGHT
SCATTERING
IS
THE
BIDIRECTIONAL
REFLECTANCE
DISTRIBUTION
FUNC
TION
BRDF
RELATIVE
TO
SOME
LOCAL
COORDINATE
FRAME
ON
THE
SURFACE
THE
BRDF
IS
A
FOUR
DIMENSIONAL
FUNCTION
THAT
DESCRIBES
HOW
MUCH
OF
EACH
WAVELENGTH
ARRIVING
AT
AN
INCIDENT
DIRECTION
VˆI
IS
EMITTED
IN
A
REFLECTED
DIRECTION
VˆR
FIGURE
THE
FUNCTION
CAN
BE
WRITTEN
IN
TERMS
OF
THE
ANGLES
OF
THE
INCIDENT
AND
REFLECTED
DIRECTIONS
RELATIVE
TO
THE
SURFACE
FRAME
AS
FR
ΘI
ΦI
ΘR
ΦR
Λ
THE
BRDF
IS
RECIPROCAL
I
E
BECAUSE
OF
THE
PHYSICS
OF
LIGHT
TRANSPORT
YOU
CAN
INTERCHANGE
THE
ROLES
OF
VˆI
AND
VˆR
AND
STILL
GET
THE
SAME
ANSWER
THIS
IS
SOMETIMES
CALLED
HELMHOLTZ
RECIPROCITY
ACTUALLY
EVEN
MORE
GENERAL
MODELS
OF
LIGHT
TRANSPORT
EXIST
INCLUDING
SOME
THAT
MODEL
SPATIAL
VARIATION
ALONG
THE
SURFACE
SUB
SURFACE
SCATTERING
AND
ATMOSPHERIC
EFFECTS
SEE
SECTION
DORSEY
RUSHMEIER
AND
SILLION
WEYRICH
LAWRENCE
LENSCH
ET
AL
MOST
SURFACES
ARE
ISOTROPIC
I
E
THERE
ARE
NO
PREFERRED
DIRECTIONS
ON
THE
SURFACE
AS
FAR
AS
LIGHT
TRANSPORT
IS
CONCERNED
THE
EXCEPTIONS
ARE
ANISOTROPIC
SURFACES
SUCH
AS
BRUSHED
SCRATCHED
ALUMINUM
WHERE
THE
REFLECTANCE
DEPENDS
ON
THE
LIGHT
ORIENTATION
RELATIVE
TO
THE
DIRECTION
OF
THE
SCRATCHES
FOR
AN
ISOTROPIC
MATERIAL
WE
CAN
SIMPLIFY
THE
BRDF
TO
FR
ΘI
ΘR
ΦR
ΦI
Λ
OR
FR
VˆI
VˆR
Nˆ
Λ
SINCE
THE
QUANTITIES
ΘI
ΘR
AND
ΦR
ΦI
CAN
BE
COMPUTED
FROM
THE
DIRECTIONS
VˆI
VˆR
AND
Nˆ
TO
CALCULATE
THE
AMOUNT
OF
LIGHT
EXITING
A
SURFACE
POINT
P
IN
A
DIRECTION
VˆR
UNDER
A
GIVEN
LIGHTING
CONDITION
WE
INTEGRATE
THE
PRODUCT
OF
THE
INCOMING
LIGHT
LI
VˆI
Λ
WITH
THE
BRDF
SOME
AUTHORS
CALL
THIS
STEP
A
CONVOLUTION
TAKING
INTO
ACCOUNT
THE
FORESHORTENING
FACTOR
COS
ΘI
WE
OBTAIN
WHERE
LR
VˆR
Λ
R
LI
VˆI
Λ
FR
VˆI
VˆR
Nˆ
Λ
COS
ΘI
DVˆI
COS
ΘI
MAX
COS
ΘI
IF
THE
LIGHT
SOURCES
ARE
DISCRETE
A
FINITE
NUMBER
OF
POINT
LIGHT
SOURCES
WE
CAN
REPLACE
THE
INTEGRAL
WITH
A
SUMMATION
LR
VˆR
Λ
LI
Λ
FR
VˆI
VˆR
Nˆ
Λ
COS
ΘI
I
BRDFS
FOR
A
GIVEN
SURFACE
CAN
BE
OBTAINED
THROUGH
PHYSICAL
MODELING
TORRANCE
AND
SPARROW
COOK
AND
TORRANCE
GLASSNER
HEURISTIC
MODELING
PHONG
OR
THROUGH
EMPIRICAL
OBSERVATION
WARD
WESTIN
ARVO
AND
TORRANCE
DANA
VAN
GIN
NEKEN
NAYAR
ET
AL
DORSEY
RUSHMEIER
AND
SILLION
WEYRICH
LAWRENCE
LENSCH
ET
AL
TYPICAL
BRDFS
CAN
OFTEN
BE
SPLIT
INTO
THEIR
DIFFUSE
AND
SPECULAR
COMPONENTS
AS
DESCRIBED
BELOW
DIFFUSE
REFLECTION
THE
DIFFUSE
COMPONENT
ALSO
KNOWN
AS
LAMBERTIAN
OR
MATTE
REFLECTION
SCATTERS
LIGHT
UNI
FORMLY
IN
ALL
DIRECTIONS
AND
IS
THE
PHENOMENON
WE
MOST
NORMALLY
ASSOCIATE
WITH
SHADING
E
G
THE
SMOOTH
NON
SHINY
VARIATION
OF
INTENSITY
WITH
SURFACE
NORMAL
THAT
IS
SEEN
WHEN
OB
SERVING
A
STATUE
FIGURE
DIFFUSE
REFLECTION
ALSO
OFTEN
IMPARTS
A
STRONG
BODY
COLOR
TO
THE
LIGHT
SINCE
IT
IS
CAUSED
BY
SELECTIVE
ABSORPTION
AND
RE
EMISSION
OF
LIGHT
INSIDE
THE
OBJECT
MATERIAL
SHAFER
GLASSNER
SEE
HTTP
CS
COLUMBIA
EDU
CAVE
SOFTWARE
CURET
FOR
A
DATABASE
OF
SOME
EMPIRICALLY
SAMPLED
BRDFS
FIGURE
THIS
CLOSE
UP
OF
A
STATUE
SHOWS
BOTH
DIFFUSE
SMOOTH
SHADING
AND
SPECULAR
SHINY
HIGHLIGHT
REFLECTION
AS
WELL
AS
DARKENING
IN
THE
GROOVES
AND
CREASES
DUE
TO
REDUCED
LIGHT
VISIBILITY
AND
INTERREFLECTIONS
PHOTO
COURTESY
OF
THE
CALTECH
VISION
LAB
HTTP
WWW
VISION
CALTECH
EDU
ARCHIVE
HTML
WHILE
LIGHT
IS
SCATTERED
UNIFORMLY
IN
ALL
DIRECTIONS
I
E
THE
BRDF
IS
CONSTANT
FD
VˆI
VˆR
Nˆ
Λ
FD
Λ
THE
AMOUNT
OF
LIGHT
DEPENDS
ON
THE
ANGLE
BETWEEN
THE
INCIDENT
LIGHT
DIRECTION
AND
THE
SURFACE
NORMAL
ΘI
THIS
IS
BECAUSE
THE
SURFACE
AREA
EXPOSED
TO
A
GIVEN
AMOUNT
OF
LIGHT
BECOMES
LARGER
AT
OBLIQUE
ANGLES
BECOMING
COMPLETELY
SELF
SHADOWED
AS
THE
OUTGOING
SURFACE
NORMAL
POINTS
AWAY
FROM
THE
LIGHT
FIGURE
THINK
ABOUT
HOW
YOU
ORIENT
YOURSELF
TOWARDS
THE
SUN
OR
FIREPLACE
TO
GET
MAXIMUM
WARMTH
AND
HOW
A
FLASHLIGHT
PROJECTED
OBLIQUELY
AGAINST
A
WALL
IS
LESS
BRIGHT
THAN
ONE
POINTING
DIRECTLY
AT
IT
THE
SHADING
EQUATION
FOR
DIFFUSE
REFLECTION
CAN
THUS
BE
WRITTEN
AS
LD
VˆR
Λ
LI
Λ
FD
Λ
COS
ΘI
LI
Λ
FD
Λ
VˆI
Nˆ
WHERE
SPECULAR
REFLECTION
I
I
VˆI
Nˆ
MAX
VˆI
Nˆ
THE
SECOND
MAJOR
COMPONENT
OF
A
TYPICAL
BRDF
IS
SPECULAR
GLOSS
OR
HIGHLIGHT
REFLECTION
WHICH
DEPENDS
STRONGLY
ON
THE
DIRECTION
OF
THE
OUTGOING
LIGHT
CONSIDER
LIGHT
REFLECTING
OFF
A
MIRRORED
SURFACE
FIGURE
INCIDENT
LIGHT
RAYS
ARE
REFLECTED
IN
A
DIRECTION
THAT
IS
ROTATED
BY
AROUND
THE
SURFACE
NORMAL
Nˆ
USING
THE
SAME
NOTATION
AS
IN
EQUATIONS
V
I
N
V
I
N
SI
V
I
N
V
I
N
A
B
FIGURE
A
THE
DIMINUTION
OF
RETURNED
LIGHT
CAUSED
BY
FORESHORTENING
DEPENDS
ON
VˆI
Nˆ
THE
COSINE
OF
THE
ANGLE
BETWEEN
THE
INCIDENT
LIGHT
DIRECTION
VˆI
AND
THE
SURFACE
NORMAL
Nˆ
B
MIRROR
SPECULAR
REFLECTION
THE
INCIDENT
LIGHT
RAY
DIRECTION
VˆI
IS
REFLECTED
ONTO
THE
SPECULAR
DIRECTION
SˆI
AROUND
THE
SURFACE
NORMAL
Nˆ
WE
CAN
COMPUTE
THE
SPECULAR
REFLECTION
DIRECTION
SˆI
AS
SˆI
V
I
VI
THE
AMOUNT
OF
LIGHT
REFLECTED
IN
A
GIVEN
DIRECTION
VˆR
THUS
DEPENDS
ON
THE
ANGLE
ΘS
COS
VˆR
SˆI
BETWEEN
THE
VIEW
DIRECTION
VˆR
AND
THE
SPECULAR
DIRECTION
SˆI
FOR
EXAMPLE
THE
PHONG
MODEL
USES
A
POWER
OF
THE
COSINE
OF
THE
ANGLE
FS
ΘS
Λ
KS
Λ
COSKE
ΘS
WHILE
THE
TORRANCE
AND
SPARROW
MICRO
FACET
MODEL
USES
A
GAUSSIAN
FS
ΘS
Λ
KS
Λ
EXP
LARGER
EXPONENTS
KE
OR
INVERSE
GAUSSIAN
WIDTHS
CS
CORRESPOND
TO
MORE
SPECULAR
SURFACES
WITH
DISTINCT
HIGHLIGHTS
WHILE
SMALLER
EXPONENTS
BETTER
MODEL
MATERIALS
WITH
SOFTER
GLOSS
PHONG
SHADING
PHONG
COMBINED
THE
DIFFUSE
AND
SPECULAR
COMPONENTS
OF
REFLECTION
WITH
ANOTHER
TERM
WHICH
HE
CALLED
THE
AMBIENT
ILLUMINATION
THIS
TERM
ACCOUNTS
FOR
THE
FACT
THAT
OBJECTS
ARE
GENERALLY
ILLUMINATED
NOT
ONLY
BY
POINT
LIGHT
SOURCES
BUT
ALSO
BY
A
GENERAL
DIFFUSE
ILLUMINATION
CORRESPONDING
TO
INTER
REFLECTION
E
G
THE
WALLS
IN
A
ROOM
OR
DISTANT
SOURCES
SUCH
AS
THE
AMBIENT
DIFFUSE
EXP
EXP
EXP
A
B
FIGURE
CROSS
SECTION
THROUGH
A
PHONG
SHADING
MODEL
BRDF
FOR
A
FIXED
INCIDENT
ILLU
MINATION
DIRECTION
A
COMPONENT
VALUES
AS
A
FUNCTION
OF
ANGLE
AWAY
FROM
SURFACE
NORMAL
B
POLAR
PLOT
THE
VALUE
OF
THE
PHONG
EXPONENT
KE
IS
INDICATED
BY
THE
EXP
LABELS
AND
THE
LIGHT
SOURCE
IS
AT
AN
ANGLE
OF
AWAY
FROM
THE
NORMAL
BLUE
SKY
IN
THE
PHONG
MODEL
THE
AMBIENT
TERM
DOES
NOT
DEPEND
ON
SURFACE
ORIENTATION
BUT
DEPENDS
ON
THE
COLOR
OF
BOTH
THE
AMBIENT
ILLUMINATION
LA
Λ
AND
THE
OBJECT
KA
Λ
FA
Λ
KA
Λ
LA
Λ
PUTTING
ALL
OF
THESE
TERMS
TOGETHER
WE
ARRIVE
AT
THE
PHONG
SHADING
MODEL
LR
VˆR
Λ
KA
Λ
LA
Λ
KD
Λ
LI
Λ
VˆI
Nˆ
KS
Λ
LI
Λ
VˆR
SˆI
KE
FIGURE
SHOWS
A
TYPICAL
SET
OF
PHONG
SHADING
MODEL
COMPONENTS
AS
A
FUNCTION
OF
THE
ANGLE
AWAY
FROM
THE
SURFACE
NORMAL
IN
A
PLANE
CONTAINING
BOTH
THE
LIGHTING
DIRECTION
AND
THE
VIEWER
TYPICALLY
THE
AMBIENT
AND
DIFFUSE
REFLECTION
COLOR
DISTRIBUTIONS
KA
Λ
AND
KD
Λ
ARE
THE
SAME
SINCE
THEY
ARE
BOTH
DUE
TO
SUB
SURFACE
SCATTERING
BODY
REFLECTION
INSIDE
THE
SURFACE
MATERIAL
SHAFER
THE
SPECULAR
REFLECTION
DISTRIBUTION
KS
Λ
IS
OFTEN
UNIFORM
WHITE
SINCE
IT
IS
CAUSED
BY
INTERFACE
REFLECTIONS
THAT
DO
NOT
CHANGE
THE
LIGHT
COLOR
THE
EXCEPTION
TO
THIS
ARE
METALLIC
MATERIALS
SUCH
AS
COPPER
AS
OPPOSED
TO
THE
MORE
COMMON
DIELECTRIC
MATERIALS
SUCH
AS
PLASTICS
THE
AMBIENT
ILLUMINATION
LA
Λ
OFTEN
HAS
A
DIFFERENT
COLOR
CAST
FROM
THE
DIRECT
LIGHT
SOURCES
LI
Λ
E
G
IT
MAY
BE
BLUE
FOR
A
SUNNY
OUTDOOR
SCENE
OR
YELLOW
FOR
AN
INTERIOR
LIT
WITH
CANDLES
OR
INCANDESCENT
LIGHTS
THE
PRESENCE
OF
AMBIENT
SKY
ILLUMINATION
IN
SHADOWED
AREAS
IS
WHAT
OFTEN
CAUSES
SHADOWS
TO
APPEAR
BLUER
THAN
THE
CORRESPONDING
LIT
PORTIONS
OF
A
SCENE
NOTE
ALSO
THAT
THE
DIFFUSE
COMPONENT
OF
THE
PHONG
MODEL
OR
OF
ANY
SHADING
MODEL
DEPENDS
ON
THE
ANGLE
OF
THE
INCOMING
LIGHT
SOURCE
VˆI
WHILE
THE
SPECULAR
COMPONENT
DEPENDS
ON
THE
RELATIVE
ANGLE
BETWEEN
THE
VIEWER
VR
AND
THE
SPECULAR
REFLECTION
DIRECTION
SˆI
WHICH
ITSELF
DEPENDS
ON
THE
INCOMING
LIGHT
DIRECTION
VˆI
AND
THE
SURFACE
NORMAL
Nˆ
THE
PHONG
SHADING
MODEL
HAS
BEEN
SUPERSEDED
IN
TERMS
OF
PHYSICAL
ACCURACY
BY
A
NUMBER
OF
MORE
RECENTLY
DEVELOPED
MODELS
IN
COMPUTER
GRAPHICS
INCLUDING
THE
MODEL
DEVELOPED
BY
COOK
AND
TORRANCE
BASED
ON
THE
ORIGINAL
MICRO
FACET
MODEL
OF
TORRANCE
AND
SPARROW
UNTIL
RECENTLY
MOST
COMPUTER
GRAPHICS
HARDWARE
IMPLEMENTED
THE
PHONG
MODEL
BUT
THE
RECENT
ADVENT
OF
PROGRAMMABLE
PIXEL
SHADERS
MAKES
THE
USE
OF
MORE
COMPLEX
MODELS
FEASIBLE
DI
CHROMATIC
REFLECTION
MODEL
THE
TORRANCE
AND
SPARROW
MODEL
OF
REFLECTION
ALSO
FORMS
THE
BASIS
OF
SHAFER
DI
CHROMATIC
REFLECTION
MODEL
WHICH
STATES
THAT
THE
APPARENT
COLOR
OF
A
UNIFORM
MATERIAL
LIT
FROM
A
SINGLE
SOURCE
DEPENDS
ON
THE
SUM
OF
TWO
TERMS
LR
VˆR
Λ
LI
VˆR
VˆI
Nˆ
Λ
LB
VˆR
VˆI
Nˆ
Λ
CI
Λ
MI
VˆR
VˆI
Nˆ
CB
Λ
MB
VˆR
VˆI
Nˆ
I
E
THE
RADIANCE
OF
THE
LIGHT
REFLECTED
AT
THE
INTERFACE
LI
AND
THE
RADIANCE
REFLECTED
AT
THE
SUR
FACE
BODY
LB
EACH
OF
THESE
IN
TURN
IS
A
SIMPLE
PRODUCT
BETWEEN
A
RELATIVE
POWER
SPECTRUM
C
Λ
WHICH
DEPENDS
ONLY
ON
WAVELENGTH
AND
A
MAGNITUDE
M
VˆR
VˆI
Nˆ
WHICH
DEPENDS
ONLY
ON
GEOMETRY
THIS
MODEL
CAN
EASILY
BE
DERIVED
FROM
A
GENERALIZED
VERSION
OF
PHONG
MODEL
BY
ASSUMING
A
SINGLE
LIGHT
SOURCE
AND
NO
AMBIENT
ILLUMINATION
AND
RE
ARRANGING
TERMS
THE
DI
CHROMATIC
MODEL
HAS
BEEN
SUCCESSFULLY
USED
IN
COMPUTER
VISION
TO
SEGMENT
SPECULAR
COL
ORED
OBJECTS
WITH
LARGE
VARIATIONS
IN
SHADING
KLINKER
AND
MORE
RECENTLY
HAS
INSPIRED
LOCAL
TWO
COLOR
MODELS
FOR
APPLICATIONS
SUCH
BAYER
PATTERN
DEMOSAICING
BENNETT
UYTTEN
DAELE
ZITNICK
ET
AL
GLOBAL
ILLUMINATION
RAY
TRACING
AND
RADIOSITY
THE
SIMPLE
SHADING
MODEL
PRESENTED
THUS
FAR
ASSUMES
THAT
LIGHT
RAYS
LEAVE
THE
LIGHT
SOURCES
BOUNCE
OFF
SURFACES
VISIBLE
TO
THE
CAMERA
THEREBY
CHANGING
IN
INTENSITY
OR
COLOR
AND
ARRIVE
AT
THE
CAMERA
IN
REALITY
LIGHT
SOURCES
CAN
BE
SHADOWED
BY
OCCLUDERS
AND
RAYS
CAN
BOUNCE
MULTIPLE
TIMES
AROUND
A
SCENE
WHILE
MAKING
THEIR
TRIP
FROM
A
LIGHT
SOURCE
TO
THE
CAMERA
TWO
METHODS
HAVE
TRADITIONALLY
BEEN
USED
TO
MODEL
SUCH
EFFECTS
IF
THE
SCENE
IS
MOSTLY
SPECULAR
THE
CLASSIC
EXAMPLE
BEING
SCENES
MADE
OF
GLASS
OBJECTS
AND
MIRRORED
OR
HIGHLY
POL
ISHED
BALLS
THE
PREFERRED
APPROACH
IS
RAY
TRACING
OR
PATH
TRACING
GLASSNER
AKENINE
MO
LLER
AND
HAINES
SHIRLEY
WHICH
FOLLOWS
INDIVIDUAL
RAYS
FROM
THE
CAMERA
ACROSS
MULTIPLE
BOUNCES
TOWARDS
THE
LIGHT
SOURCES
OR
VICE
VERSA
IF
THE
SCENE
IS
COMPOSED
MOSTLY
OF
UNIFORM
ALBEDO
SIMPLE
GEOMETRY
ILLUMINATORS
AND
SURFACES
RADIOSITY
GLOBAL
ILLUMINATION
TECHNIQUES
ARE
PREFERRED
COHEN
AND
WALLACE
SILLION
AND
PUECH
GLASSNER
COMBINATIONS
OF
THE
TWO
TECHNIQUES
HAVE
ALSO
BEEN
DEVELOPED
WALLACE
COHEN
AND
GREEN
BERG
AS
WELL
AS
MORE
GENERAL
LIGHT
TRANSPORT
TECHNIQUES
FOR
SIMULATING
EFFECTS
SUCH
AS
THE
CAUSTICS
CAST
BY
RIPPLING
WATER
THE
BASIC
RAY
TRACING
ALGORITHM
ASSOCIATES
A
LIGHT
RAY
WITH
EACH
PIXEL
IN
THE
CAMERA
IM
AGE
AND
FINDS
ITS
INTERSECTION
WITH
THE
NEAREST
SURFACE
A
PRIMARY
CONTRIBUTION
CAN
THEN
BE
COMPUTED
USING
THE
SIMPLE
SHADING
EQUATIONS
PRESENTED
PREVIOUSLY
E
G
EQUATION
FOR
ALL
LIGHT
SOURCES
THAT
ARE
VISIBLE
FOR
THAT
SURFACE
ELEMENT
AN
ALTERNATIVE
TECHNIQUE
FOR
COMPUTING
WHICH
SURFACES
ARE
ILLUMINATED
BY
A
LIGHT
SOURCE
IS
TO
COMPUTE
A
SHADOW
MAP
OR
SHADOW
BUFFER
I
E
A
RENDERING
OF
THE
SCENE
FROM
THE
LIGHT
SOURCE
PERSPECTIVE
AND
THEN
COMPARE
THE
DEPTH
OF
PIXELS
BEING
RENDERED
WITH
THE
MAP
WILLIAMS
AKENINE
MO
LLER
AND
HAINES
ADDITIONAL
SECONDARY
RAYS
CAN
THEN
BE
CAST
ALONG
THE
SPECULAR
DIRECTION
TOWARDS
OTHER
OBJECTS
IN
THE
SCENE
KEEPING
TRACK
OF
ANY
ATTENUATION
OR
COLOR
CHANGE
THAT
THE
SPECULAR
REFLECTION
INDUCES
RADIOSITY
WORKS
BY
ASSOCIATING
LIGHTNESS
VALUES
WITH
RECTANGULAR
SURFACE
AREAS
IN
THE
SCENE
INCLUDING
AREA
LIGHT
SOURCES
THE
AMOUNT
OF
LIGHT
INTERCHANGED
BETWEEN
ANY
TWO
MUTUALLY
VISIBLE
AREAS
IN
THE
SCENE
CAN
BE
CAPTURED
AS
A
FORM
FACTOR
WHICH
DEPENDS
ON
THEIR
RELATIVE
ORIENTATION
AND
SURFACE
REFLECTANCE
PROPERTIES
AS
WELL
AS
THE
FALL
OFF
AS
LIGHT
IS
DISTRIBUTED
OVER
A
LARGER
EFFECTIVE
SPHERE
THE
FURTHER
AWAY
IT
IS
COHEN
AND
WALLACE
SILLION
AND
PUECH
GLASSNER
A
LARGE
LINEAR
SYSTEM
CAN
THEN
BE
SET
UP
TO
SOLVE
FOR
THE
FINAL
LIGHTNESS
OF
EACH
AREA
PATCH
USING
THE
LIGHT
SOURCES
AS
THE
FORCING
FUNCTION
RIGHT
HAND
SIDE
ONCE
THE
SYSTEM
HAS
BEEN
SOLVED
THE
SCENE
CAN
BE
RENDERED
FROM
ANY
DESIRED
POINT
OF
VIEW
UNDER
CERTAIN
CIRCUMSTANCES
IT
IS
POSSIBLE
TO
RECOVER
THE
GLOBAL
ILLUMINATION
IN
A
SCENE
FROM
PHOTOGRAPHS
USING
COMPUTER
VISION
TECHNIQUES
YU
DEBEVEC
MALIK
ET
AL
THE
BASIC
RADIOSITY
ALGORITHM
DOES
NOT
TAKE
INTO
ACCOUNT
CERTAIN
NEAR
FIELD
EFFECTS
SUCH
AS
THE
DARKENING
INSIDE
CORNERS
AND
SCRATCHES
OR
THE
LIMITED
AMBIENT
ILLUMINATION
CAUSED
BY
PARTIAL
SHADOWING
FROM
OTHER
SURFACES
SUCH
EFFECTS
HAVE
BEEN
EXPLOITED
IN
A
NUMBER
OF
COMPUTER
VISION
ALGORITHMS
NAYAR
IKEUCHI
AND
KANADE
LANGER
AND
ZUCKER
WHILE
ALL
OF
THESE
GLOBAL
ILLUMINATION
EFFECTS
CAN
HAVE
A
STRONG
EFFECT
ON
THE
APPEARANCE
OF
A
SCENE
AND
HENCE
ITS
INTERPRETATION
THEY
ARE
NOT
COVERED
IN
MORE
DETAIL
IN
THIS
BOOK
BUT
SEE
SECTION
FOR
A
DISCUSSION
OF
RECOVERING
BRDFS
FROM
REAL
SCENES
AND
OBJECTS
OPTICS
ONCE
THE
LIGHT
FROM
A
SCENE
REACHES
THE
CAMERA
IT
MUST
STILL
PASS
THROUGH
THE
LENS
BEFORE
REACHING
THE
SENSOR
ANALOG
FILM
OR
DIGITAL
SILICON
FOR
MANY
APPLICATIONS
IT
SUFFICES
TO
TREAT
THE
LENS
AS
AN
IDEAL
PINHOLE
THAT
SIMPLY
PROJECTS
ALL
RAYS
THROUGH
A
COMMON
CENTER
OF
PROJECTION
FIGURES
AND
HOWEVER
IF
WE
WANT
TO
DEAL
WITH
ISSUES
SUCH
AS
FOCUS
EXPOSURE
VIGNETTING
AND
ABER
FIGURE
A
THIN
LENS
OF
FOCAL
LENGTH
F
FOCUSES
THE
LIGHT
FROM
A
PLANE
A
DISTANCE
ZO
IN
FRONT
OF
THE
LENS
AT
A
DISTANCE
ZI
BEHIND
THE
LENS
WHERE
IF
THE
FOCAL
PLANE
VERTICAL
I
GRAY
LINE
NEXT
TO
C
IS
MOVED
FORWARD
THE
IMAGES
ARE
NO
LONGER
IN
FOCUS
AND
THE
CIRCLE
OF
CONFUSION
C
SMALL
THICK
LINE
SEGMENTS
DEPENDS
ON
THE
DISTANCE
OF
THE
IMAGE
PLANE
MOTION
ZI
RELATIVE
TO
THE
LENS
APERTURE
DIAMETER
D
THE
FIELD
OF
VIEW
F
O
V
DEPENDS
ON
THE
RATIO
BETWEEN
THE
SENSOR
WIDTH
W
AND
THE
FOCAL
LENGTH
F
OR
MORE
PRECISELY
THE
FOCUSING
DISTANCE
ZI
WHICH
IS
USUALLY
QUITE
CLOSE
TO
F
RATION
WE
NEED
TO
DEVELOP
A
MORE
SOPHISTICATED
MODEL
WHICH
IS
WHERE
THE
STUDY
OF
OPTICS
COMES
IN
MO
LLER
HECHT
RAY
FIGURE
SHOWS
A
DIAGRAM
OF
THE
MOST
BASIC
LENS
MODEL
I
E
THE
THIN
LENS
COMPOSED
OF
A
SINGLE
PIECE
OF
GLASS
WITH
VERY
LOW
EQUAL
CURVATURE
ON
BOTH
SIDES
ACCORDING
TO
THE
LENS
LAW
WHICH
CAN
BE
DERIVED
USING
SIMPLE
GEOMETRIC
ARGUMENTS
ON
LIGHT
RAY
REFRACTION
THE
RELATIONSHIP
BETWEEN
THE
DISTANCE
TO
AN
OBJECT
ZO
AND
THE
DISTANCE
BEHIND
THE
LENS
AT
WHICH
A
FOCUSED
IMAGE
IS
FORMED
ZI
CAN
BE
EXPRESSED
AS
ZO
ZI
F
WHERE
F
IS
CALLED
THE
FOCAL
LENGTH
OF
THE
LENS
IF
WE
LET
ZO
I
E
WE
ADJUST
THE
LENS
MOVE
THE
IMAGE
PLANE
SO
THAT
OBJECTS
AT
INFINITY
ARE
IN
FOCUS
WE
GET
ZI
F
WHICH
IS
WHY
WE
CAN
THINK
OF
A
LENS
OF
FOCAL
LENGTH
F
AS
BEING
EQUIVALENT
TO
A
FIRST
APPROXIMATION
TO
A
PINHOLE
A
DISTANCE
F
FROM
THE
FOCAL
PLANE
FIGURE
WHOSE
FIELD
OF
VIEW
IS
GIVEN
BY
IF
THE
FOCAL
PLANE
IS
MOVED
AWAY
FROM
ITS
PROPER
IN
FOCUS
SETTING
OF
ZI
E
G
BY
TWISTING
THE
FOCUS
RING
ON
THE
LENS
OBJECTS
AT
ZO
ARE
NO
LONGER
IN
FOCUS
AS
SHOWN
BY
THE
GRAY
PLANE
IN
FIGURE
THE
AMOUNT
OF
MIS
FOCUS
IS
MEASURED
BY
THE
CIRCLE
OF
CONFUSION
C
SHOWN
AS
SHORT
THICK
BLUE
LINE
SEGMENTS
ON
THE
GRAY
PLANE
THE
EQUATION
FOR
THE
CIRCLE
OF
CONFUSION
CAN
BE
DERIVED
USING
SIMILAR
TRIANGLES
IT
DEPENDS
ON
THE
DISTANCE
OF
TRAVEL
IN
THE
FOCAL
PLANE
ZI
RELATIVE
TO
THE
ORIGINAL
FOCUS
DISTANCE
ZI
AND
THE
DIAMETER
OF
THE
APERTURE
D
SEE
EXERCISE
IF
THE
APERTURE
IS
NOT
COMPLETELY
CIRCULAR
E
G
IF
IT
IS
CAUSED
BY
A
HEXAGONAL
DIAPHRAGM
IT
IS
SOMETIMES
POSSIBLE
TO
SEE
THIS
EFFECT
IN
THE
ACTUAL
BLUR
FUNCTION
LEVIN
FERGUS
DURAND
ET
AL
JOSHI
SZELISKI
AND
KRIEGMAN
OR
IN
THE
GLINTS
THAT
ARE
SEEN
WHEN
SHOOTING
INTO
THE
SUN
A
B
FIGURE
REGULAR
AND
ZOOM
LENS
DEPTH
OF
FIELD
INDICATORS
THE
ALLOWABLE
DEPTH
VARIATION
IN
THE
SCENE
THAT
LIMITS
THE
CIRCLE
OF
CONFUSION
TO
AN
ACCEPT
ABLE
NUMBER
IS
COMMONLY
CALLED
THE
DEPTH
OF
FIELD
AND
IS
A
FUNCTION
OF
BOTH
THE
FOCUS
DISTANCE
AND
THE
APERTURE
AS
SHOWN
DIAGRAMMATICALLY
BY
MANY
LENS
MARKINGS
FIGURE
SINCE
THIS
DEPTH
OF
FIELD
DEPENDS
ON
THE
APERTURE
DIAMETER
D
WE
ALSO
HAVE
TO
KNOW
HOW
THIS
VARIES
WITH
THE
COMMONLY
DISPLAYED
F
NUMBER
WHICH
IS
USUALLY
DENOTED
AS
F
OR
N
AND
IS
DEFINED
AS
F
F
N
D
WHERE
THE
FOCAL
LENGTH
F
AND
THE
APERTURE
DIAMETER
D
ARE
MEASURED
IN
THE
SAME
UNIT
SAY
MILLIMETERS
THE
USUAL
WAY
TO
WRITE
THE
F
NUMBER
IS
TO
REPLACE
THE
IN
F
WITH
THE
ACTUAL
NUMBER
I
E
F
F
F
F
ALTERNATIVELY
WE
CAN
SAY
N
ETC
AN
EASY
WAY
TO
INTERPRET
THESE
NUMBERS
IS
TO
NOTICE
THAT
DIVIDING
THE
FOCAL
LENGTH
BY
THE
F
NUMBER
GIVES
US
THE
DIAMETER
D
SO
THESE
ARE
JUST
FORMULAS
FOR
THE
APERTURE
DIAMETER
NOTICE
THAT
THE
USUAL
PROGRESSION
FOR
F
NUMBERS
IS
IN
FULL
STOPS
WHICH
ARE
MULTIPLES
OF
SINCE
THIS
CORRESPONDS
TO
DOUBLING
THE
AREA
OF
THE
ENTRANCE
PUPIL
EACH
TIME
A
SMALLER
F
NUMBER
IS
SELECTED
THIS
DOUBLING
IS
ALSO
CALLED
CHANGING
THE
EXPOSURE
BY
ONE
EXPOSURE
VALUE
OR
EV
IT
HAS
THE
SAME
EFFECT
ON
THE
AMOUNT
OF
LIGHT
REACHING
THE
SENSOR
AS
DOUBLING
THE
EXPOSURE
DURATION
E
G
FROM
TO
SEE
EXERCISE
NOW
THAT
YOU
KNOW
HOW
TO
CONVERT
BETWEEN
F
NUMBERS
AND
APERTURE
DIAMETERS
YOU
CAN
CONSTRUCT
YOUR
OWN
PLOTS
FOR
THE
DEPTH
OF
FIELD
AS
A
FUNCTION
OF
FOCAL
LENGTH
F
CIRCLE
OF
CONFUSION
C
AND
FOCUS
DISTANCE
ZO
AS
EXPLAINED
IN
EXERCISE
AND
SEE
HOW
WELL
THESE
MATCH
WHAT
YOU
OBSERVE
ON
ACTUAL
LENSES
SUCH
AS
THOSE
SHOWN
IN
FIGURE
OF
COURSE
REAL
LENSES
ARE
NOT
INFINITELY
THIN
AND
THEREFORE
SUFFER
FROM
GEOMETRIC
ABER
RATIONS
UNLESS
COMPOUND
ELEMENTS
ARE
USED
TO
CORRECT
FOR
THEM
THE
CLASSIC
FIVE
SEIDEL
ABERRATIONS
WHICH
ARISE
WHEN
USING
THIRD
ORDER
OPTICS
INCLUDE
SPHERICAL
ABERRATION
COMA
ASTIGMATISM
CURVATURE
OF
FIELD
AND
DISTORTION
MO
LLER
HECHT
RAY
THIS
ALSO
EXPLAINS
WHY
WITH
ZOOM
LENSES
THE
F
NUMBER
VARIES
WITH
THE
CURRENT
ZOOM
FOCAL
LENGTH
SETTING
FIGURE
IN
A
LENS
SUBJECT
TO
CHROMATIC
ABERRATION
LIGHT
AT
DIFFERENT
WAVELENGTHS
E
G
THE
RED
AND
BLUR
ARROWS
IS
FOCUSED
WITH
A
DIFFERENT
FOCAL
LENGTH
F
I
AND
HENCE
A
DIFFERENT
DEPTH
ZII
RESULTING
IN
BOTH
A
GEOMETRIC
IN
PLANE
DISPLACEMENT
AND
A
LOSS
OF
FOCUS
CHROMATIC
ABERRATION
BECAUSE
THE
INDEX
OF
REFRACTION
FOR
GLASS
VARIES
SLIGHTLY
AS
A
FUNCTION
OF
WAVELENGTH
SIM
PLE
LENSES
SUFFER
FROM
CHROMATIC
ABERRATION
WHICH
IS
THE
TENDENCY
FOR
LIGHT
OF
DIFFERENT
COLORS
TO
FOCUS
AT
SLIGHTLY
DIFFERENT
DISTANCES
AND
HENCE
ALSO
WITH
SLIGHTLY
DIFFERENT
MAG
NIFICATION
FACTORS
AS
SHOWN
IN
FIGURE
THE
WAVELENGTH
DEPENDENT
MAGNIFICATION
FAC
TOR
I
E
THE
TRANSVERSE
CHROMATIC
ABERRATION
CAN
BE
MODELED
AS
A
PER
COLOR
RADIAL
DISTORTION
SECTION
AND
HENCE
CALIBRATED
USING
THE
TECHNIQUES
DESCRIBED
IN
SECTION
THE
WAVELENGTH
DEPENDENT
BLUR
CAUSED
BY
LONGITUDINAL
CHROMATIC
ABERRATION
CAN
BE
CALIBRATED
USING
TECHNIQUES
DESCRIBED
IN
SECTION
UNFORTUNATELY
THE
BLUR
INDUCED
BY
LONGITUDINAL
ABERRATION
CAN
BE
HARDER
TO
UNDO
AS
HIGHER
FREQUENCIES
CAN
GET
STRONGLY
ATTENUATED
AND
HENCE
HARD
TO
RECOVER
IN
ORDER
TO
REDUCE
CHROMATIC
AND
OTHER
KINDS
OF
ABERRATIONS
MOST
PHOTOGRAPHIC
LENSES
TODAY
ARE
COMPOUND
LENSES
MADE
OF
DIFFERENT
GLASS
ELEMENTS
WITH
DIFFERENT
COATINGS
SUCH
LENSES
CAN
NO
LONGER
BE
MODELED
AS
HAVING
A
SINGLE
NODAL
POINT
P
THROUGH
WHICH
ALL
OF
THE
RAYS
MUST
PASS
WHEN
APPROXIMATING
THE
LENS
WITH
A
PINHOLE
MODEL
INSTEAD
THESE
LENSES
HAVE
BOTH
A
FRONT
NODAL
POINT
THROUGH
WHICH
THE
RAYS
ENTER
THE
LENS
AND
A
REAR
NODAL
POINT
THROUGH
WHICH
THEY
LEAVE
ON
THEIR
WAY
TO
THE
SENSOR
IN
PRACTICE
ONLY
THE
LOCATION
OF
THE
FRONT
NODAL
POINT
IS
OF
INTEREST
WHEN
PERFORMING
CAREFUL
CAMERA
CALIBRATION
E
G
WHEN
DETERMINING
THE
POINT
AROUND
WHICH
TO
ROTATE
TO
CAPTURE
A
PARALLAX
FREE
PANORAMA
SEE
SECTION
NOT
ALL
LENSES
HOWEVER
CAN
BE
MODELED
AS
HAVING
A
SINGLE
NODAL
POINT
IN
PARTICULAR
VERY
WIDE
ANGLE
LENSES
SUCH
AS
FISHEYE
LENSES
SECTION
AND
CERTAIN
CATADIOPTRIC
IMAGING
SYSTEMS
CONSISTING
OF
LENSES
AND
CURVED
MIRRORS
BAKER
AND
NAYAR
DO
NOT
HAVE
A
SINGLE
POINT
THROUGH
WHICH
ALL
OF
THE
ACQUIRED
LIGHT
RAYS
PASS
IN
SUCH
CASES
IT
IS
PREFERABLE
TO
EXPLICITLY
CONSTRUCT
A
MAPPING
FUNCTION
LOOK
UP
TABLE
BETWEEN
PIXEL
COORDINATES
AND
RAYS
IN
SPACE
GREMBAN
THORPE
AND
KANADE
CHAMPLEBOUX
LAVALLE
E
SAUTOT
ET
AL
FIGURE
THE
AMOUNT
OF
LIGHT
HITTING
A
PIXEL
OF
SURFACE
AREA
ΔI
DEPENDS
ON
THE
SQUARE
OF
THE
RATIO
OF
THE
APERTURE
DIAMETER
D
TO
THE
FOCAL
LENGTH
F
AS
WELL
AS
THE
FOURTH
POWER
OF
THE
OFF
AXIS
ANGLE
Α
COSINE
Α
GROSSBERG
AND
NAYAR
STURM
AND
RAMALINGAM
TARDIF
STURM
TRUDEAU
ET
AL
AS
MENTIONED
IN
SECTION
VIGNETTING
ANOTHER
PROPERTY
OF
REAL
WORLD
LENSES
IS
VIGNETTING
WHICH
IS
THE
TENDENCY
FOR
THE
BRIGHTNESS
OF
THE
IMAGE
TO
FALL
OFF
TOWARDS
THE
EDGE
OF
THE
IMAGE
TWO
KINDS
OF
PHENOMENA
USUALLY
CONTRIBUTE
TO
THIS
EFFECT
RAY
THE
FIRST
IS
CALLED
NATURAL
VIGNETTING
AND
IS
DUE
TO
THE
FORESHORTENING
IN
THE
OBJECT
SURFACE
PROJECTED
PIXEL
AND
LENS
APERTURE
AS
SHOWN
IN
FIGURE
CONSIDER
THE
LIGHT
LEAVING
THE
OBJECT
SURFACE
PATCH
OF
SIZE
ΔO
LOCATED
AT
AN
OFF
AXIS
ANGLE
Α
BECAUSE
THIS
PATCH
IS
FORESHORTENED
WITH
RESPECT
TO
THE
CAMERA
LENS
THE
AMOUNT
OF
LIGHT
REACHING
THE
LENS
IS
REDUCED
BY
A
FACTOR
COS
Α
THE
AMOUNT
OF
LIGHT
REACHING
THE
LENS
IS
ALSO
SUBJECT
TO
THE
USUAL
FALL
OFF
IN
THIS
CASE
THE
DISTANCE
RO
ZO
COS
Α
THE
ACTUAL
AREA
OF
THE
APERTURE
THROUGH
WHICH
THE
LIGHT
PASSES
IS
FORESHORTENED
BY
AN
ADDITIONAL
FACTOR
COS
Α
I
E
THE
APERTURE
AS
SEEN
FROM
POINT
O
IS
AN
ELLIPSE
OF
DIMENSIONS
D
D
COS
Α
PUTTING
ALL
OF
THESE
FACTORS
TOGETHER
WE
SEE
THAT
THE
AMOUNT
OF
LIGHT
LEAVING
O
AND
PASSING
THROUGH
THE
APERTURE
ON
ITS
WAY
TO
THE
IMAGE
PIXEL
LOCATED
AT
I
IS
PROPORTIONAL
TO
ΔO
COS
Α
Π
O
D
COS
Α
ΔO
Π
Α
SINCE
TRIANGLES
OP
Q
AND
IP
J
ARE
SIMILAR
THE
PROJECTED
AREAS
OF
OF
THE
OBJECT
SURFACE
ΔO
AND
IMAGE
PIXEL
ΔI
ARE
IN
THE
SAME
SQUARED
RATIO
AS
ZO
ZI
ΔO
Z
ΔI
PUTTING
THESE
TOGETHER
WE
OBTAIN
THE
FINAL
RELATIONSHIP
BETWEEN
THE
AMOUNT
OF
LIGHT
REACHING
PIXEL
I
AND
THE
APERTURE
DIAMETER
D
THE
FOCUSING
DISTANCE
ZI
F
AND
THE
OFF
AXIS
ANGLE
Α
Π
ΔO
Π
I
Α
ΔI
Α
WHICH
IS
CALLED
THE
FUNDAMENTAL
RADIOMETRIC
RELATION
BETWEEN
THE
SCENE
RADIANCE
L
AND
THE
LIGHT
IRRADIANCE
E
REACHING
THE
PIXEL
SENSOR
Π
D
HORN
NALWA
HECHT
RAY
NOTICE
IN
THIS
EQUATION
HOW
THE
AMOUNT
OF
LIGHT
DEPENDS
ON
THE
PIXEL
SURFACE
AREA
WHICH
IS
WHY
THE
SMALLER
SENSORS
IN
POINT
AND
SHOOT
CAMERAS
ARE
SO
MUCH
NOISIER
THAN
DIGITAL
SINGLE
LENS
REFLEX
SLR
CAMERAS
THE
INVERSE
SQUARE
OF
THE
F
STOP
N
F
D
AND
THE
FOURTH
POWER
OF
THE
Α
OFF
AXIS
FALL
OFF
WHICH
IS
THE
NATURAL
VIGNETTING
TERM
THE
OTHER
MAJOR
KIND
OF
VIGNETTING
CALLED
MECHANICAL
VIGNETTING
IS
CAUSED
BY
THE
INTERNAL
OCCLUSION
OF
RAYS
NEAR
THE
PERIPHERY
OF
LENS
ELEMENTS
IN
A
COMPOUND
LENS
AND
CANNOT
EASILY
BE
DESCRIBED
MATHEMATICALLY
WITHOUT
PERFORMING
A
FULL
RAY
TRACING
OF
THE
ACTUAL
LENS
DESIGN
HOWEVER
UNLIKE
NATURAL
VIGNETTING
MECHANICAL
VIGNETTING
CAN
BE
DECREASED
BY
REDUCING
THE
CAMERA
APERTURE
INCREASING
THE
F
NUMBER
IT
CAN
ALSO
BE
CALIBRATED
ALONG
WITH
NATURAL
VI
GNETTING
USING
SPECIAL
DEVICES
SUCH
AS
INTEGRATING
SPHERES
UNIFORMLY
ILLUMINATED
TARGETS
OR
CAMERA
ROTATION
AS
DISCUSSED
IN
SECTION
THE
DIGITAL
CAMERA
AFTER
STARTING
FROM
ONE
OR
MORE
LIGHT
SOURCES
REFLECTING
OFF
ONE
OR
MORE
SURFACES
IN
THE
WORLD
AND
PASSING
THROUGH
THE
CAMERA
OPTICS
LENSES
LIGHT
FINALLY
REACHES
THE
IMAGING
SENSOR
HOW
ARE
THE
PHOTONS
ARRIVING
AT
THIS
SENSOR
CONVERTED
INTO
THE
DIGITAL
R
G
B
VALUES
THAT
WE
OBSERVE
WHEN
WE
LOOK
AT
A
DIGITAL
IMAGE
IN
THIS
SECTION
WE
DEVELOP
A
SIMPLE
MODEL
THAT
ACCOUNTS
FOR
THE
MOST
IMPORTANT
EFFECTS
SUCH
AS
EXPOSURE
GAIN
AND
SHUTTER
SPEED
NON
LINEAR
MAPPINGS
SAMPLING
AND
ALIASING
AND
NOISE
FIGURE
WHICH
IS
BASED
ON
CAMERA
MODELS
DEVELOPED
BY
HEALEY
AND
KONDEPUDY
TSIN
RAMESH
AND
KANADE
LIU
SZELISKI
KANG
ET
AL
SHOWS
A
SIMPLE
VERSION
OF
THE
PROCESSING
STAGES
THAT
OCCUR
IN
MODERN
DIGITAL
CAMERAS
CHAKRABARTI
SCHARSTEIN
AND
ZICKLER
DEVELOPED
A
SOPHISTI
CATED
PARAMETER
MODEL
THAT
IS
AN
EVEN
BETTER
MATCH
TO
THE
PROCESSING
PERFORMED
IN
TODAY
CAMERAS
THERE
ARE
SOME
EMPIRICAL
MODELS
THAT
WORK
WELL
IN
PRACTICE
KANG
AND
WEISS
ZHENG
LIN
AND
KANG
FIGURE
IMAGE
SENSING
PIPELINE
SHOWING
THE
VARIOUS
SOURCES
OF
NOISE
AS
WELL
AS
TYPICAL
DIGITAL
POST
PROCESSING
STEPS
LIGHT
FALLING
ON
AN
IMAGING
SENSOR
IS
USUALLY
PICKED
UP
BY
AN
ACTIVE
SENSING
AREA
INTE
GRATED
FOR
THE
DURATION
OF
THE
EXPOSURE
USUALLY
EXPRESSED
AS
THE
SHUTTER
SPEED
IN
A
FRACTION
OF
A
SECOND
E
G
AND
THEN
PASSED
TO
A
SET
OF
SENSE
AMPLIFIERS
THE
TWO
MAIN
KINDS
OF
SENSOR
USED
IN
DIGITAL
STILL
AND
VIDEO
CAMERAS
TODAY
ARE
CHARGE
COUPLED
DEVICE
CCD
AND
COMPLEMENTARY
METAL
OXIDE
ON
SILICON
CMOS
IN
A
CCD
PHOTONS
ARE
ACCUMULATED
IN
EACH
ACTIVE
WELL
DURING
THE
EXPOSURE
TIME
THEN
IN
A
TRANSFER
PHASE
THE
CHARGES
ARE
TRANSFERRED
FROM
WELL
TO
WELL
IN
A
KIND
OF
BUCKET
BRIGADE
UNTIL
THEY
ARE
DEPOSITED
AT
THE
SENSE
AMPLIFIERS
WHICH
AMPLIFY
THE
SIGNAL
AND
PASS
IT
TO
AN
ANALOG
TO
DIGITAL
CONVERTER
ADC
OLDER
CCD
SENSORS
WERE
PRONE
TO
BLOOMING
WHEN
CHARGES
FROM
ONE
OVER
EXPOSED
PIXEL
SPILLED
INTO
ADJACENT
ONES
BUT
MOST
NEWER
CCDS
HAVE
ANTI
BLOOMING
TECHNOLOGY
TROUGHS
INTO
WHICH
THE
EXCESS
CHARGE
CAN
SPILL
IN
CMOS
THE
PHOTONS
HITTING
THE
SENSOR
DIRECTLY
AFFECT
THE
CONDUCTIVITY
OR
GAIN
OF
A
PHOTODETECTOR
WHICH
CAN
BE
SELECTIVELY
GATED
TO
CONTROL
EXPOSURE
DURATION
AND
LOCALLY
AM
PLIFIED
BEFORE
BEING
READ
OUT
USING
A
MULTIPLEXING
SCHEME
TRADITIONALLY
CCD
SENSORS
OUTPERFORMED
CMOS
IN
QUALITY
SENSITIVE
APPLICATIONS
SUCH
AS
DIGITAL
SLRS
WHILE
CMOS
WAS
BETTER
FOR
LOW
POWER
APPLICATIONS
BUT
TODAY
CMOS
IS
USED
IN
MOST
DIGITAL
CAMERAS
THE
MAIN
FACTORS
AFFECTING
THE
PERFORMANCE
OF
A
DIGITAL
IMAGE
SENSOR
ARE
THE
SHUTTER
SPEED
SAMPLING
PITCH
FILL
FACTOR
CHIP
SIZE
ANALOG
GAIN
SENSOR
NOISE
AND
THE
RESOLUTION
AND
QUALITY
IN
DIGITAL
STILL
CAMERAS
A
COMPLETE
FRAME
IS
CAPTURED
AND
THEN
READ
OUT
SEQUENTIALLY
AT
ONCE
HOWEVER
IF
VIDEO
IS
BEING
CAPTURED
A
ROLLING
SHUTTER
WHICH
EXPOSES
AND
TRANSFERS
EACH
LINE
SEPARATELY
IS
OFTEN
USED
IN
OLDER
VIDEO
CAMERAS
THE
EVEN
FIELDS
LINES
WERE
SCANNED
FIRST
FOLLOWED
BY
THE
ODD
FIELDS
IN
A
PROCESS
THAT
IS
CALLED
INTERLACING
OF
THE
ANALOG
TO
DIGITAL
CONVERTER
MANY
OF
THE
ACTUAL
VALUES
FOR
THESE
PARAMETERS
CAN
BE
READ
FROM
THE
EXIF
TAGS
EMBEDDED
WITH
DIGITAL
IMAGES
WHILE
OTHERS
CAN
BE
OBTAINED
FROM
THE
CAMERA
MANUFACTURERS
SPECIFICATION
SHEETS
OR
FROM
CAMERA
REVIEW
OR
CALIBRATION
WEB
SITES
SHUTTER
SPEED
THE
SHUTTER
SPEED
EXPOSURE
TIME
DIRECTLY
CONTROLS
THE
AMOUNT
OF
LIGHT
REACHING
THE
SENSOR
AND
HENCE
DETERMINES
IF
IMAGES
ARE
UNDER
OR
OVER
EXPOSED
FOR
BRIGHT
SCENES
WHERE
A
LARGE
APERTURE
OR
SLOW
SHUTTER
SPEED
ARE
DESIRED
TO
GET
A
SHALLOW
DEPTH
OF
FIELD
OR
MOTION
BLUR
NEUTRAL
DENSITY
FILTERS
ARE
SOMETIMES
USED
BY
PHOTOGRAPHERS
FOR
DYNAMIC
SCENES
THE
SHUTTER
SPEED
ALSO
DETERMINES
THE
AMOUNT
OF
MOTION
BLUR
IN
THE
RESULTING
PICTURE
USUALLY
A
HIGHER
SHUTTER
SPEED
LESS
MOTION
BLUR
MAKES
SUBSEQUENT
ANALYSIS
EASIER
SEE
SEC
TION
FOR
TECHNIQUES
TO
REMOVE
SUCH
BLUR
HOWEVER
WHEN
VIDEO
IS
BEING
CAPTURED
FOR
DISPLAY
SOME
MOTION
BLUR
MAY
BE
DESIRABLE
TO
AVOID
STROBOSCOPIC
EFFECTS
SAMPLING
PITCH
THE
SAMPLING
PITCH
IS
THE
PHYSICAL
SPACING
BETWEEN
ADJACENT
SENSOR
CELLS
ON
THE
IMAGING
CHIP
A
SENSOR
WITH
A
SMALLER
SAMPLING
PITCH
HAS
A
HIGHER
SAMPLING
DENSITY
AND
HENCE
PROVIDES
A
HIGHER
RESOLUTION
IN
TERMS
OF
PIXELS
FOR
A
GIVEN
ACTIVE
CHIP
AREA
HOWEVER
A
SMALLER
PITCH
ALSO
MEANS
THAT
EACH
SENSOR
HAS
A
SMALLER
AREA
AND
CANNOT
ACCUMULATE
AS
MANY
PHOTONS
THIS
MAKES
IT
NOT
AS
LIGHT
SENSITIVE
AND
MORE
PRONE
TO
NOISE
FILL
FACTOR
THE
FILL
FACTOR
IS
THE
ACTIVE
SENSING
AREA
SIZE
AS
A
FRACTION
OF
THE
THEORETICALLY
AVAILABLE
SENSING
AREA
THE
PRODUCT
OF
THE
HORIZONTAL
AND
VERTICAL
SAMPLING
PITCHES
HIGHER
FILL
FACTORS
ARE
USUALLY
PREFERABLE
AS
THEY
RESULT
IN
MORE
LIGHT
CAPTURE
AND
LESS
ALIASING
SEE
SECTION
HOWEVER
THIS
MUST
BE
BALANCED
WITH
THE
NEED
TO
PLACE
ADDITIONAL
ELECTRONICS
BETWEEN
THE
ACTIVE
SENSE
AREAS
THE
FILL
FACTOR
OF
A
CAMERA
CAN
BE
DETERMINED
EMPIRICALLY
USING
A
PHOTOMETRIC
CAMERA
CALIBRATION
PROCESS
SEE
SECTION
CHIP
SIZE
VIDEO
AND
POINT
AND
SHOOT
CAMERAS
HAVE
TRADITIONALLY
USED
SMALL
CHIP
AREAS
INCH
TO
INCH
WHILE
DIGITAL
SLR
CAMERAS
TRY
TO
COME
CLOSER
TO
THE
TRADITIONAL
SIZE
OF
A
FILM
FRAME
WHEN
OVERALL
DEVICE
SIZE
IS
NOT
IMPORTANT
HAVING
A
LARGER
CHIP
SIZE
IS
PREFERABLE
SINCE
EACH
SENSOR
CELL
CAN
BE
MORE
PHOTO
SENSITIVE
FOR
COMPACT
CAMERAS
A
SMALLER
CHIP
MEANS
THAT
ALL
OF
THE
OPTICS
CAN
BE
SHRUNK
DOWN
PROPORTIONATELY
HOWEVER
HTTP
WWW
CLARKVISION
COM
IMAGEDETAIL
DIGITAL
SENSOR
PERFORMANCE
SUMMARY
THESE
NUMBERS
REFER
TO
THE
TUBE
DIAMETER
OF
THE
OLD
VIDICON
TUBES
USED
IN
VIDEO
CAMERAS
HTTP
WWW
DPREVIEW
COM
LEARN
GLOSSARY
CAMERA
SYSTEM
SENSOR
SIZES
HTM
THE
SENSOR
ON
THE
CANON
CAM
ERA
ACTUALLY
MEASURES
I
E
A
SIXTH
OF
THE
SIZE
ON
SIDE
OF
A
FULL
FRAME
DSLR
SENSOR
WHEN
A
DSLR
CHIP
DOES
NOT
FILL
THE
FULL
FRAME
IT
RESULTS
IN
A
MULTIPLIER
EFFECT
ON
THE
LENS
FOCAL
LENGTH
FOR
EXAMPLE
A
CHIP
THAT
IS
ONLY
THE
DIMENSION
OF
A
FRAME
WILL
MAKE
A
LENS
IMAGE
THE
SAME
ANGULAR
EXTENT
AS
A
LENS
AS
DEMONSTRATED
IN
LARGER
CHIPS
ARE
MORE
EXPENSIVE
TO
PRODUCE
NOT
ONLY
BECAUSE
FEWER
CHIPS
CAN
BE
PACKED
INTO
EACH
WAFER
BUT
ALSO
BECAUSE
THE
PROBABILITY
OF
A
CHIP
DEFECT
GOES
UP
LINEARLY
WITH
THE
CHIP
AREA
ANALOG
GAIN
BEFORE
ANALOG
TO
DIGITAL
CONVERSION
THE
SENSED
SIGNAL
IS
USUALLY
BOOSTED
BY
A
SENSE
AMPLIFIER
IN
VIDEO
CAMERAS
THE
GAIN
ON
THESE
AMPLIFIERS
WAS
TRADITIONALLY
CONTROLLED
BY
AUTOMATIC
GAIN
CONTROL
AGC
LOGIC
WHICH
WOULD
ADJUST
THESE
VALUES
TO
OBTAIN
A
GOOD
OVERALL
EXPOSURE
IN
NEWER
DIGITAL
STILL
CAMERAS
THE
USER
NOW
HAS
SOME
ADDITIONAL
CONTROL
OVER
THIS
GAIN
THROUGH
THE
ISO
SETTING
WHICH
IS
TYPICALLY
EXPRESSED
IN
ISO
STANDARD
UNITS
SUCH
AS
OR
SINCE
THE
AUTOMATED
EXPOSURE
CONTROL
IN
MOST
CAMERAS
ALSO
ADJUSTS
THE
APERTURE
AND
SHUTTER
SPEED
SETTING
THE
ISO
MANUALLY
REMOVES
ONE
DEGREE
OF
FREEDOM
FROM
THE
CAMERA
CONTROL
JUST
AS
MANUALLY
SPECIFYING
APERTURE
AND
SHUTTER
SPEED
DOES
IN
THEORY
A
HIGHER
GAIN
ALLOWS
THE
CAMERA
TO
PERFORM
BETTER
UNDER
LOW
LIGHT
CONDITIONS
LESS
MOTION
BLUR
DUE
TO
LONG
EXPOSURE
TIMES
WHEN
THE
APERTURE
IS
ALREADY
MAXED
OUT
IN
PRACTICE
HOWEVER
HIGHER
ISO
SETTINGS
USUALLY
AMPLIFY
THE
SENSOR
NOISE
SENSOR
NOISE
THROUGHOUT
THE
WHOLE
SENSING
PROCESS
NOISE
IS
ADDED
FROM
VARIOUS
SOURCES
WHICH
MAY
INCLUDE
FIXED
PATTERN
NOISE
DARK
CURRENT
NOISE
SHOT
NOISE
AMPLIFIER
NOISE
AND
QUANTIZATION
NOISE
HEALEY
AND
KONDEPUDY
TSIN
RAMESH
AND
KANADE
THE
FINAL
AMOUNT
OF
NOISE
PRESENT
IN
A
SAMPLED
IMAGE
DEPENDS
ON
ALL
OF
THESE
QUANTITIES
AS
WELL
AS
THE
INCOMING
LIGHT
CONTROLLED
BY
THE
SCENE
RADIANCE
AND
APERTURE
THE
EXPOSURE
TIME
AND
THE
SENSOR
GAIN
ALSO
FOR
LOW
LIGHT
CONDITIONS
WHERE
THE
NOISE
IS
DUE
TO
LOW
PHOTON
COUNTS
A
POISSON
MODEL
OF
NOISE
MAY
BE
MORE
APPROPRIATE
THAN
A
GAUSSIAN
MODEL
AS
DISCUSSED
IN
MORE
DETAIL
IN
SECTION
LIU
SZELISKI
KANG
ET
AL
USE
THIS
MODEL
ALONG
WITH
AN
EMPIRICAL
DATABASE
OF
CAMERA
RESPONSE
FUNCTIONS
CRFS
OBTAINED
BY
GROSSBERG
AND
NAYAR
TO
ESTIMATE
THE
NOISE
LEVEL
FUNCTION
NLF
FOR
A
GIVEN
IMAGE
WHICH
PREDICTS
THE
OVERALL
NOISE
VARIANCE
AT
A
GIVEN
PIXEL
AS
A
FUNCTION
OF
ITS
BRIGHTNESS
A
SEPARATE
NLF
IS
ESTIMATED
FOR
EACH
COLOR
CHANNEL
AN
ALTERNATIVE
APPROACH
WHEN
YOU
HAVE
ACCESS
TO
THE
CAMERA
BEFORE
TAKING
PICTURES
IS
TO
PRE
CALIBRATE
THE
NLF
BY
TAKING
REPEATED
SHOTS
OF
A
SCENE
CONTAINING
A
VARIETY
OF
COLORS
AND
LUMINANCES
SUCH
AS
THE
MACBETH
COLOR
CHART
SHOWN
IN
FIGURE
MCCAMY
MARCUS
AND
DAVIDSON
WHEN
ESTIMATING
THE
VARIANCE
BE
SURE
TO
THROW
AWAY
OR
DOWNWEIGHT
PIXELS
WITH
LARGE
GRADIENTS
AS
SMALL
SHIFTS
BETWEEN
EXPOSURES
WILL
AFFECT
THE
SENSED
VALUES
AT
SUCH
PIXELS
UNFORTUNATELY
THE
PRE
CALIBRATION
PROCESS
MAY
HAVE
TO
BE
REPEATED
FOR
DIFFERENT
EXPOSURE
TIMES
AND
GAIN
SETTINGS
BECAUSE
OF
THE
COMPLEX
INTERACTIONS
OCCURRING
WITHIN
THE
SENSING
SYSTEM
IN
PRACTICE
MOST
COMPUTER
VISION
ALGORITHMS
SUCH
AS
IMAGE
DENOISING
EDGE
DETECTION
AND
STEREO
MATCHING
ALL
BENEFIT
FROM
AT
LEAST
A
RUDIMENTARY
ESTIMATE
OF
THE
NOISE
LEVEL
BARRING
THE
ABILITY
TO
PRE
CALIBRATE
THE
CAMERA
OR
TO
TAKE
REPEATED
SHOTS
OF
THE
SAME
SCENE
THE
SIMPLEST
APPROACH
IS
TO
LOOK
FOR
REGIONS
OF
NEAR
CONSTANT
VALUE
AND
TO
ESTIMATE
THE
NOISE
VARIANCE
IN
SUCH
REGIONS
LIU
SZELISKI
KANG
ET
AL
ADC
RESOLUTION
THE
FINAL
STEP
IN
THE
ANALOG
PROCESSING
CHAIN
OCCURRING
WITHIN
AN
IMAGING
SENSOR
IS
THE
ANALOG
TO
DIGITAL
CONVERSION
ADC
WHILE
A
VARIETY
OF
TECHNIQUES
CAN
BE
USED
TO
IMPLEMENT
THIS
PROCESS
THE
TWO
QUANTITIES
OF
INTEREST
ARE
THE
RESOLUTION
OF
THIS
PROCESS
HOW
MANY
BITS
IT
YIELDS
AND
ITS
NOISE
LEVEL
HOW
MANY
OF
THESE
BITS
ARE
USEFUL
IN
PRACTICE
FOR
MOST
CAMERAS
THE
NUMBER
OF
BITS
QUOTED
EIGHT
BITS
FOR
COMPRESSED
JPEG
IMAGES
AND
A
NOMINAL
BITS
FOR
THE
RAW
FORMATS
PROVIDED
BY
SOME
DSLRS
EXCEEDS
THE
ACTUAL
NUMBER
OF
USABLE
BITS
THE
BEST
WAY
TO
TELL
IS
TO
SIMPLY
CALIBRATE
THE
NOISE
OF
A
GIVEN
SENSOR
E
G
BY
TAKING
REPEATED
SHOTS
OF
THE
SAME
SCENE
AND
PLOTTING
THE
ESTIMATED
NOISE
AS
A
FUNCTION
OF
BRIGHTNESS
EXERCISE
DIGITAL
POST
PROCESSING
ONCE
THE
IRRADIANCE
VALUES
ARRIVING
AT
THE
SENSOR
HAVE
BEEN
CON
VERTED
TO
DIGITAL
BITS
MOST
CAMERAS
PERFORM
A
VARIETY
OF
DIGITAL
SIGNAL
PROCESSING
DSP
OPERATIONS
TO
ENHANCE
THE
IMAGE
BEFORE
COMPRESSING
AND
STORING
THE
PIXEL
VALUES
THESE
IN
CLUDE
COLOR
FILTER
ARRAY
CFA
DEMOSAICING
WHITE
POINT
SETTING
AND
MAPPING
OF
THE
LUMINANCE
VALUES
THROUGH
A
GAMMA
FUNCTION
TO
INCREASE
THE
PERCEIVED
DYNAMIC
RANGE
OF
THE
SIGNAL
WE
COVER
THESE
TOPICS
IN
SECTION
BUT
BEFORE
WE
DO
WE
RETURN
TO
THE
TOPIC
OF
ALIASING
WHICH
WAS
MENTIONED
IN
CONNECTION
WITH
SENSOR
ARRAY
FILL
FACTORS
SAMPLING
AND
ALIASING
WHAT
HAPPENS
WHEN
A
FIELD
OF
LIGHT
IMPINGING
ON
THE
IMAGE
SENSOR
FALLS
ONTO
THE
ACTIVE
SENSE
AREAS
IN
THE
IMAGING
CHIP
THE
PHOTONS
ARRIVING
AT
EACH
ACTIVE
CELL
ARE
INTEGRATED
AND
THEN
DIGITIZED
HOWEVER
IF
THE
FILL
FACTOR
ON
THE
CHIP
IS
SMALL
AND
THE
SIGNAL
IS
NOT
OTHERWISE
BAND
LIMITED
VISUALLY
UNPLEASING
ALIASING
CAN
OCCUR
TO
EXPLORE
THE
PHENOMENON
OF
ALIASING
LET
US
FIRST
LOOK
AT
A
ONE
DIMENSIONAL
SIGNAL
FIG
URE
IN
WHICH
WE
HAVE
TWO
SINE
WAVES
ONE
AT
A
FREQUENCY
OF
F
AND
THE
OTHER
AT
F
IF
WE
SAMPLE
THESE
TWO
SIGNALS
AT
A
FREQUENCY
OF
F
WE
SEE
THAT
THEY
PRODUCE
THE
SAME
SAMPLES
SHOWN
IN
BLACK
AND
SO
WE
SAY
THAT
THEY
ARE
ALIASED
WHY
IS
THIS
A
BAD
EFFECT
IN
ESSENCE
WE
CAN
NO
LONGER
RECONSTRUCT
THE
ORIGINAL
SIGNAL
SINCE
WE
DO
NOT
KNOW
WHICH
OF
THE
TWO
ORIGINAL
FREQUENCIES
WAS
PRESENT
IN
FACT
SHANNON
SAMPLING
THEOREM
SHOWS
THAT
THE
MINIMUM
SAMPLING
OPPENHEIM
AND
SCHAFER
OPPENHEIM
SCHAFER
AND
BUCK
RATE
REQUIRED
TO
RECONSTRUCT
A
SIGNAL
AN
ALIAS
IS
AN
ALTERNATE
NAME
FOR
SOMEONE
SO
THE
SAMPLED
SIGNAL
CORRESPONDS
TO
TWO
DIFFERENT
ALIASES
F
F
FIGURE
ALIASING
OF
A
ONE
DIMENSIONAL
SIGNAL
THE
BLUE
SINE
WAVE
AT
F
AND
THE
RED
SINE
WAVE
AT
F
HAVE
THE
SAME
DIGITAL
SAMPLES
WHEN
SAMPLED
AT
F
EVEN
AFTER
CONVOLUTION
WITH
A
FILL
FACTOR
BOX
FILTER
THE
TWO
SIGNALS
WHILE
NO
LONGER
OF
THE
SAME
MAGNITUDE
ARE
STILL
ALIASED
IN
THE
SENSE
THAT
THE
SAMPLED
RED
SIGNAL
LOOKS
LIKE
AN
INVERTED
LOWER
MAGNITUDE
VERSION
OF
THE
BLUE
SIGNAL
THE
IMAGE
ON
THE
RIGHT
IS
SCALED
UP
FOR
BETTER
VISIBILITY
THE
ACTUAL
SINE
MAGNITUDES
ARE
AND
OF
THEIR
ORIGINAL
VALUES
FROM
ITS
INSTANTANEOUS
SAMPLES
MUST
BE
AT
LEAST
TWICE
THE
HIGHEST
FREQUENCY
FS
THE
MAXIMUM
FREQUENCY
IN
A
SIGNAL
IS
KNOWN
AS
THE
NYQUIST
FREQUENCY
AND
THE
INVERSE
OF
THE
MINIMUM
SAMPLING
FREQUENCY
RS
FS
IS
KNOWN
AS
THE
NYQUIST
RATE
HOWEVER
YOU
MAY
ASK
SINCE
AN
IMAGING
CHIP
ACTUALLY
AVERAGES
THE
LIGHT
FIELD
OVER
A
FINITE
AREA
ARE
THE
RESULTS
ON
POINT
SAMPLING
STILL
APPLICABLE
AVERAGING
OVER
THE
SENSOR
AREA
DOES
TEND
TO
ATTENUATE
SOME
OF
THE
HIGHER
FREQUENCIES
HOWEVER
EVEN
IF
THE
FILL
FACTOR
IS
AS
IN
THE
RIGHT
IMAGE
OF
FIGURE
FREQUENCIES
ABOVE
THE
NYQUIST
LIMIT
HALF
THE
SAMPLING
FREQUENCY
STILL
PRODUCE
AN
ALIASED
SIGNAL
ALTHOUGH
WITH
A
SMALLER
MAGNITUDE
THAN
THE
CORRESPONDING
BAND
LIMITED
SIGNALS
A
MORE
CONVINCING
ARGUMENT
AS
TO
WHY
ALIASING
IS
BAD
CAN
BE
SEEN
BY
DOWNSAMPLING
A
SIGNAL
USING
A
POOR
QUALITY
FILTER
SUCH
AS
A
BOX
SQUARE
FILTER
FIGURE
SHOWS
A
HIGH
FREQUENCY
CHIRP
IMAGE
SO
CALLED
BECAUSE
THE
FREQUENCIES
INCREASE
OVER
TIME
ALONG
WITH
THE
RESULTS
OF
SAMPLING
IT
WITH
A
FILL
FACTOR
AREA
SENSOR
A
FILL
FACTOR
SENSOR
AND
A
HIGH
QUALITY
TAP
FILTER
ADDITIONAL
EXAMPLES
OF
DOWNSAMPLING
DECIMATION
FILTERS
CAN
BE
FOUND
IN
SECTION
AND
FIGURE
THE
BEST
WAY
TO
PREDICT
THE
AMOUNT
OF
ALIASING
THAT
AN
IMAGING
SYSTEM
OR
EVEN
AN
IMAGE
PROCESSING
ALGORITHM
WILL
PRODUCE
IS
TO
ESTIMATE
THE
POINT
SPREAD
FUNCTION
PSF
WHICH
REPRESENTS
THE
RESPONSE
OF
A
PARTICULAR
PIXEL
SENSOR
TO
AN
IDEAL
POINT
LIGHT
SOURCE
THE
PSF
IS
A
COMBINATION
CONVOLUTION
OF
THE
BLUR
INDUCED
BY
THE
OPTICAL
SYSTEM
LENS
AND
THE
FINITE
INTEGRATION
AREA
OF
A
CHIP
SENSOR
THE
ACTUAL
THEOREM
STATES
THAT
FS
MUST
BE
AT
LEAST
TWICE
THE
SIGNAL
BANDWIDTH
BUT
SINCE
WE
ARE
NOT
DEALING
WITH
MODULATED
SIGNALS
SUCH
AS
RADIO
WAVES
DURING
IMAGE
CAPTURE
THE
MAXIMUM
FREQUENCY
SUFFICES
IMAGING
CHIPS
USUALLY
INTERPOSE
AN
OPTICAL
ANTI
ALIASING
FILTER
JUST
BEFORE
THE
IMAGING
CHIP
TO
REDUCE
OR
CONTROL
THE
AMOUNT
OF
ALIASING
A
B
C
D
FIGURE
ALIASING
OF
A
TWO
DIMENSIONAL
SIGNAL
A
ORIGINAL
FULL
RESOLUTION
IMAGE
B
DOWNSAMPLED
WITH
A
FILL
FACTOR
BOX
FILTER
C
DOWNSAMPLED
WITH
A
FILL
FACTOR
BOX
FILTER
D
DOWNSAMPLED
WITH
A
HIGH
QUALITY
TAP
FILTER
NOTICE
HOW
THE
HIGHER
FREQUENCIES
ARE
ALIASED
INTO
VISIBLE
FREQUENCIES
WITH
THE
LOWER
QUALITY
FILTERS
WHILE
THE
TAP
FILTER
COMPLETELY
REMOVES
THESE
HIGHER
FREQUENCIES
IF
WE
KNOW
THE
BLUR
FUNCTION
OF
THE
LENS
AND
THE
FILL
FACTOR
SENSOR
AREA
SHAPE
AND
SPACING
FOR
THE
IMAGING
CHIP
PLUS
OPTIONALLY
THE
RESPONSE
OF
THE
ANTI
ALIASING
FILTER
WE
CAN
CONVOLVE
THESE
AS
DESCRIBED
IN
SECTION
TO
OBTAIN
THE
PSF
FIGURE
SHOWS
THE
ONE
DIMENSIONAL
CROSS
SECTION
OF
A
PSF
FOR
A
LENS
WHOSE
BLUR
FUNCTION
IS
ASSUMED
TO
BE
A
DISC
OF
A
RADIUS
EQUAL
TO
THE
PIXEL
SPACING
PLUS
A
SENSING
CHIP
WHOSE
HORIZONTAL
FILL
FACTOR
IS
TAKING
THE
FOURIER
TRANSFORM
OF
THIS
PSF
SECTION
WE
OBTAIN
THE
MODULATION
TRANSFER
FUNCTION
MTF
FROM
WHICH
WE
CAN
ESTIMATE
THE
AMOUNT
OF
ALIASING
AS
THE
AREA
OF
THE
FOURIER
MAGNI
TUDE
OUTSIDE
THE
F
FS
NYQUIST
FREQUENCY
IF
WE
DE
FOCUS
THE
LENS
SO
THAT
THE
BLUR
FUNCTION
HAS
A
RADIUS
OF
FIGURE
WE
SEE
THAT
THE
AMOUNT
OF
ALIASING
DECREASES
SIGNIFICANTLY
BUT
SO
DOES
THE
AMOUNT
OF
IMAGE
DETAIL
FREQUENCIES
CLOSER
TO
F
FS
UNDER
LABORATORY
CONDITIONS
THE
PSF
CAN
BE
ESTIMATED
TO
PIXEL
PRECISION
BY
LOOKING
AT
A
POINT
LIGHT
SOURCE
SUCH
AS
A
PIN
HOLE
IN
A
BLACK
PIECE
OF
CARDBOARD
LIT
FROM
BEHIND
HOWEVER
THIS
PSF
THE
ACTUAL
IMAGE
OF
THE
PIN
HOLE
IS
ONLY
ACCURATE
TO
A
PIXEL
RESOLUTION
AND
WHILE
IT
CAN
MODEL
LARGER
BLUR
SUCH
AS
BLUR
CAUSED
BY
DEFOCUS
IT
CANNOT
MODEL
THE
SUB
PIXEL
SHAPE
OF
THE
PSF
AND
PREDICT
THE
AMOUNT
OF
ALIASING
AN
ALTERNATIVE
TECHNIQUE
DESCRIBED
IN
SECTION
IS
TO
LOOK
AT
A
CALIBRATION
PATTERN
E
G
ONE
CONSISTING
OF
SLANTED
STEP
EDGES
REICHENBACH
PARK
AND
NARAYANSWAMY
WILLIAMS
AND
BURNS
JOSHI
SZELISKI
AND
KRIEGMAN
WHOSE
IDEAL
APPEARANCE
CAN
BE
RE
SYNTHESIZED
TO
SUB
PIXEL
PRECISION
IN
ADDITION
TO
OCCURRING
DURING
IMAGE
ACQUISITION
ALIASING
CAN
ALSO
BE
INTRODUCED
IN
VAR
IOUS
IMAGE
PROCESSING
OPERATIONS
SUCH
AS
RESAMPLING
UPSAMPLING
AND
DOWNSAMPLING
SEC
TIONS
AND
DISCUSS
THESE
ISSUES
AND
SHOW
HOW
CAREFUL
SELECTION
OF
FILTERS
CAN
REDUCE
THE
COMPLEX
FOURIER
TRANSFORM
OF
THE
PSF
IS
ACTUALLY
CALLED
THE
OPTICAL
TRANSFER
FUNCTION
OTF
WILLIAMS
ITS
MAGNITUDE
IS
CALLED
THE
MODULATION
TRANSFER
FUNCTION
MTF
AND
ITS
PHASE
IS
CALLED
THE
PHASE
TRANSFER
FUNCTION
PTF
A
B
C
D
FIGURE
SAMPLE
POINT
SPREAD
FUNCTIONS
PSF
THE
DIAMETER
OF
THE
BLUR
DISC
BLUE
IN
A
IS
EQUAL
TO
HALF
THE
PIXEL
SPACING
WHILE
THE
DIAMETER
IN
C
IS
TWICE
THE
PIXEL
SPACING
THE
HORIZONTAL
FILL
FACTOR
OF
THE
SENSING
CHIP
IS
AND
IS
SHOWN
IN
BROWN
THE
CONVOLUTION
OF
THESE
TWO
KERNELS
GIVES
THE
POINT
SPREAD
FUNCTION
SHOWN
IN
GREEN
THE
FOURIER
RESPONSE
OF
THE
PSF
THE
MTF
IS
PLOTTED
IN
B
AND
D
THE
AREA
ABOVE
THE
NYQUIST
FREQUENCY
WHERE
ALIASING
OCCURS
IS
SHOWN
IN
RED
THE
AMOUNT
OF
ALIASING
THAT
OPERATIONS
INJECT
COLOR
IN
SECTION
WE
SAW
HOW
LIGHTING
AND
SURFACE
REFLECTIONS
ARE
FUNCTIONS
OF
WAVELENGTH
WHEN
THE
INCOMING
LIGHT
HITS
THE
IMAGING
SENSOR
LIGHT
FROM
DIFFERENT
PARTS
OF
THE
SPECTRUM
IS
SOMEHOW
INTEGRATED
INTO
THE
DISCRETE
RED
GREEN
AND
BLUE
RGB
COLOR
VALUES
THAT
WE
SEE
IN
A
DIGITAL
IMAGE
HOW
DOES
THIS
PROCESS
WORK
AND
HOW
CAN
WE
ANALYZE
AND
MANIPULATE
COLOR
VALUES
YOU
PROBABLY
RECALL
FROM
YOUR
CHILDHOOD
DAYS
THE
MAGICAL
PROCESS
OF
MIXING
PAINT
COLORS
TO
OBTAIN
NEW
ONES
YOU
MAY
RECALL
THAT
BLUE
YELLOW
MAKES
GREEN
RED
BLUE
MAKES
PURPLE
AND
RED
GREEN
MAKES
BROWN
IF
YOU
REVISITED
THIS
TOPIC
AT
A
LATER
AGE
YOU
MAY
HAVE
LEARNED
THAT
THE
PROPER
SUBTRACTIVE
PRIMARIES
ARE
ACTUALLY
CYAN
A
LIGHT
BLUE
GREEN
MAGENTA
PINK
AND
YELLOW
FIGURE
ALTHOUGH
BLACK
IS
ALSO
OFTEN
USED
IN
FOUR
COLOR
PRINTING
CMYK
IF
YOU
EVER
SUBSEQUENTLY
TOOK
ANY
PAINTING
CLASSES
YOU
LEARNED
THAT
COLORS
CAN
HAVE
EVEN
A
B
FIGURE
PRIMARY
AND
SECONDARY
COLORS
A
ADDITIVE
COLORS
RED
GREEN
AND
BLUE
CAN
BE
MIXED
TO
PRODUCE
CYAN
MAGENTA
YELLOW
AND
WHITE
B
SUBTRACTIVE
COLORS
CYAN
MAGENTA
AND
YELLOW
CAN
BE
MIXED
TO
PRODUCE
RED
GREEN
BLUE
AND
BLACK
MORE
FANCIFUL
NAMES
SUCH
AS
ALIZARIN
CRIMSON
CERULEAN
BLUE
AND
CHARTREUSE
THE
SUBTRACTIVE
COLORS
ARE
CALLED
SUBTRACTIVE
BECAUSE
PIGMENTS
IN
THE
PAINT
ABSORB
CERTAIN
WAVELENGTHS
IN
THE
COLOR
SPECTRUM
LATER
ON
YOU
MAY
HAVE
LEARNED
ABOUT
THE
ADDITIVE
PRIMARY
COLORS
RED
GREEN
AND
BLUE
AND
HOW
THEY
CAN
BE
ADDED
WITH
A
SLIDE
PROJECTOR
OR
ON
A
COMPUTER
MONITOR
TO
PRODUCE
CYAN
MAGENTA
YELLOW
WHITE
AND
ALL
THE
OTHER
COLORS
WE
TYPICALLY
SEE
ON
OUR
TV
SETS
AND
MONITORS
FIGURE
THROUGH
WHAT
PROCESS
IS
IT
POSSIBLE
FOR
TWO
DIFFERENT
COLORS
SUCH
AS
RED
AND
GREEN
TO
INTERACT
TO
PRODUCE
A
THIRD
COLOR
LIKE
YELLOW
ARE
THE
WAVELENGTHS
SOMEHOW
MIXED
UP
TO
PRODUCE
A
NEW
WAVELENGTH
YOU
PROBABLY
KNOW
THAT
THE
CORRECT
ANSWER
HAS
NOTHING
TO
DO
WITH
PHYSICALLY
MIXING
WAVELENGTHS
INSTEAD
THE
EXISTENCE
OF
THREE
PRIMARIES
IS
A
RESULT
OF
THE
TRI
STIMULUS
OR
TRI
CHROMATIC
NATURE
OF
THE
HUMAN
VISUAL
SYSTEM
SINCE
WE
HAVE
THREE
DIFFERENT
KINDS
OF
CONE
EACH
OF
WHICH
RESPONDS
SELECTIVELY
TO
A
DIFFERENT
PORTION
OF
THE
COLOR
SPECTRUM
GLASSNER
WYSZECKI
AND
STILES
FAIRCHILD
REINHARD
WARD
PATTANAIK
ET
AL
LIVINGSTONE
NOTE
THAT
FOR
MACHINE
VISION
APPLICATIONS
SUCH
AS
REMOTE
SENSING
AND
TERRAIN
CLAS
SIFICATION
IT
IS
PREFERABLE
TO
USE
MANY
MORE
WAVELENGTHS
SIMILARLY
SURVEILLANCE
APPLICATIONS
CAN
OFTEN
BENEFIT
FROM
SENSING
IN
THE
NEAR
INFRARED
NIR
RANGE
CIE
RGB
AND
XYZ
TO
TEST
AND
QUANTIFY
THE
TRI
CHROMATIC
THEORY
OF
PERCEPTION
WE
CAN
ATTEMPT
TO
REPRODUCE
ALL
MONOCHROMATIC
SINGLE
WAVELENGTH
COLORS
AS
A
MIXTURE
OF
THREE
SUITABLY
CHOSEN
PRIMARIES
SEE
ALSO
MARK
FAIRCHILD
WEB
PAGE
HTTP
WWW
CIS
RIT
EDU
FAIRCHILD
WHYISCOLOR
BOOKS
LINKS
HTML
A
B
FIGURE
STANDARD
CIE
COLOR
MATCHING
FUNCTIONS
A
R
Λ
G
Λ
B
Λ
COLOR
SPECTRA
OBTAINED
FROM
MATCHING
PURE
COLORS
TO
THE
R
G
AND
B
PRI
MARIES
B
X
Λ
Y
Λ
Z
Λ
COLOR
MATCHING
FUNCTIONS
WHICH
ARE
LINEAR
COMBINATIONS
OF
THE
R
Λ
G
Λ
B
Λ
SPECTRA
PURE
WAVELENGTH
LIGHT
CAN
BE
OBTAINED
USING
EITHER
A
PRISM
OR
SPECIALLY
MANUFACTURED
COLOR
FILTERS
IN
THE
THE
COMMISSION
INTERNATIONALE
D
ECLAIRAGE
CIE
STANDARDIZED
THE
RGB
REPRESENTATION
BY
PERFORMING
SUCH
COLOR
MATCHING
EXPERIMENTS
USING
THE
PRIMARY
COLORS
OF
RED
WAVELENGTH
GREEN
AND
BLUE
FIGURE
SHOWS
THE
RESULTS
OF
PERFORMING
THESE
EXPERIMENTS
WITH
A
STANDARD
OBSERVER
I
E
AVERAGING
PERCEPTUAL
RESULTS
OVER
A
LARGE
NUMBER
OF
SUBJECTS
YOU
WILL
NOTICE
THAT
FOR
CERTAIN
PURE
SPECTRA
IN
THE
BLUE
GREEN
RANGE
A
NEGATIVE
AMOUNT
OF
RED
LIGHT
HAS
TO
BE
ADDED
I
E
A
CERTAIN
AMOUNT
OF
RED
HAS
TO
BE
ADDED
TO
THE
COLOR
BEING
MATCHED
IN
ORDER
TO
GET
A
COLOR
MATCH
THESE
RESULTS
ALSO
PROVIDED
A
SIMPLE
EXPLANATION
FOR
THE
EXISTENCE
OF
METAMERS
WHICH
ARE
COLORS
WITH
DIFFERENT
SPECTRA
THAT
ARE
PERCEPTUALLY
INDISTINGUISHABLE
NOTE
THAT
TWO
FABRICS
OR
PAINT
COLORS
THAT
ARE
METAMERS
UNDER
ONE
LIGHT
MAY
NO
LONGER
BE
SO
UNDER
DIFFERENT
LIGHTING
BECAUSE
OF
THE
PROBLEM
ASSOCIATED
WITH
MIXING
NEGATIVE
LIGHT
THE
CIE
ALSO
DEVELOPED
A
NEW
COLOR
SPACE
CALLED
XYZ
WHICH
CONTAINS
ALL
OF
THE
PURE
SPECTRAL
COLORS
WITHIN
ITS
POSITIVE
OCTANT
IT
ALSO
MAPS
THE
Y
AXIS
TO
THE
LUMINANCE
I
E
PERCEIVED
RELATIVE
BRIGHTNESS
AND
MAPS
PURE
WHITE
TO
A
DIAGONAL
EQUAL
VALUED
VECTOR
THE
TRANSFORMATION
FROM
RGB
TO
XYZ
IS
GIVEN
BY
X
R
WHILE
THE
OFFICIAL
DEFINITION
OF
THE
CIE
XYZ
STANDARD
HAS
THE
MATRIX
NORMALIZED
SO
THAT
THE
Y
VALUE
CORRESPONDING
TO
PURE
RED
IS
A
MORE
COMMONLY
USED
FORM
IS
TO
OMIT
THE
LEADING
FIGURE
CIE
CHROMATICITY
DIAGRAM
SHOWING
COLORS
AND
THEIR
CORRESPONDING
X
Y
VAL
UES
PURE
SPECTRAL
COLORS
ARE
ARRANGED
AROUND
THE
OUTSIDE
OF
THE
CURVE
FRACTION
SO
THAT
THE
SECOND
ROW
ADDS
UP
TO
ONE
I
E
THE
RGB
TRIPLET
MAPS
TO
A
Y
VALUE
OF
LINEARLY
BLENDING
THE
R
Λ
G
Λ
B
Λ
CURVES
IN
FIGURE
ACCORDING
TO
WE
OBTAIN
THE
RESULTING
X
Λ
Y
Λ
Z
Λ
CURVES
SHOWN
IN
FIGURE
NOTICE
HOW
ALL
THREE
SPECTRA
COLOR
MATCHING
FUNCTIONS
NOW
HAVE
ONLY
POSITIVE
VALUES
AND
HOW
THE
Y
Λ
CURVE
MATCHES
THAT
OF
THE
LUMINANCE
PERCEIVED
BY
HUMANS
IF
WE
DIVIDE
THE
XYZ
VALUES
BY
THE
SUM
OF
X
Y
Z
WE
OBTAIN
THE
CHROMATICITY
COORDI
NATES
X
X
Y
Z
Y
Y
X
Y
Z
Z
Z
X
Y
Z
WHICH
SUM
UP
TO
THE
CHROMATICITY
COORDINATES
DISCARD
THE
ABSOLUTE
INTENSITY
OF
A
GIVEN
COLOR
SAMPLE
AND
JUST
REPRESENT
ITS
PURE
COLOR
IF
WE
SWEEP
THE
MONOCHROMATIC
COLOR
Λ
PA
RAMETER
IN
FIGURE
FROM
Λ
TO
Λ
WE
OBTAIN
THE
FAMILIAR
CHROMATICITY
DIAGRAM
SHOWN
IN
FIGURE
THIS
FIGURE
SHOWS
THE
X
Y
VALUE
FOR
EVERY
COLOR
VALUE
PER
CEIVABLE
BY
MOST
HUMANS
OF
COURSE
THE
CMYK
REPRODUCTION
PROCESS
IN
THIS
BOOK
DOES
NOT
ACTUALLY
SPAN
THE
WHOLE
GAMUT
OF
PERCEIVABLE
COLORS
THE
OUTER
CURVED
RIM
REPRESENTS
WHERE
ALL
OF
THE
PURE
MONOCHROMATIC
COLOR
VALUES
MAP
IN
X
Y
SPACE
WHILE
THE
LOWER
STRAIGHT
LINE
WHICH
CONNECTS
THE
TWO
ENDPOINTS
IS
KNOWN
AS
THE
PURPLE
LINE
A
CONVENIENT
REPRESENTATION
FOR
COLOR
VALUES
WHEN
WE
WANT
TO
TEASE
APART
LUMINANCE
AND
CHROMATICITY
IS
THEREFORE
YXY
LUMINANCE
PLUS
THE
TWO
MOST
DISTINCTIVE
CHROMINANCE
COMPONENTS
L
A
B
COLOR
SPACE
WHILE
THE
XYZ
COLOR
SPACE
HAS
MANY
CONVENIENT
PROPERTIES
INCLUDING
THE
ABILITY
TO
SEPARATE
LUMINANCE
FROM
CHROMINANCE
IT
DOES
NOT
ACTUALLY
PREDICT
HOW
WELL
HUMANS
PERCEIVE
DIFFER
ENCES
IN
COLOR
OR
LUMINANCE
BECAUSE
THE
RESPONSE
OF
THE
HUMAN
VISUAL
SYSTEM
IS
ROUGHLY
LOGARITHMIC
WE
CAN
PERCEIVE
RELATIVE
LUMINANCE
DIFFERENCES
OF
ABOUT
THE
CIE
DEFINED
A
NON
LINEAR
RE
MAPPING
OF
THE
XYZ
SPACE
CALLED
L
A
B
ALSO
SOMETIMES
CALLED
CIELAB
WHERE
DIFFERENCES
IN
LUMINANCE
OR
CHROMINANCE
ARE
MORE
PERCEPTUALLY
UNIFORM
THE
L
COMPONENT
OF
LIGHTNESS
IS
DEFINED
AS
L
Y
YN
WHERE
YN
IS
THE
LUMINANCE
VALUE
FOR
NOMINAL
WHITE
FAIRCHILD
AND
T
IS
A
FINITE
SLOPE
APPROXIMATION
TO
THE
CUBE
ROOT
WITH
Δ
THE
RESULTING
SCALE
ROUGHLY
MEASURES
EQUAL
AMOUNTS
OF
LIGHTNESS
PERCEPTIBILITY
IN
A
SIMILAR
FASHION
THE
A
AND
B
COMPONENTS
ARE
DEFINED
AS
A
F
X
F
Y
L
AND
B
F
Y
F
Z
L
WHERE
AGAIN
XN
YN
ZN
IS
THE
MEASURED
WHITE
POINT
FIGURE
K
SHOW
THE
L
A
B
REPRESENTATION
FOR
A
SAMPLE
COLOR
IMAGE
COLOR
CAMERAS
WHILE
THE
PRECEDING
DISCUSSION
TELLS
US
HOW
WE
CAN
UNIQUELY
DESCRIBE
THE
PERCEIVED
TRI
STIMULUS
DESCRIPTION
OF
ANY
COLOR
SPECTRAL
DISTRIBUTION
IT
DOES
NOT
TELL
US
HOW
RGB
STILL
AND
VIDEO
CAMERAS
ACTUALLY
WORK
DO
THEY
JUST
MEASURE
THE
AMOUNT
OF
LIGHT
AT
THE
NOMINAL
WAVELENGTHS
OF
RED
GREEN
AND
BLUE
DO
COLOR
MONITORS
JUST
EMIT
EXACTLY
THESE
WAVELENGTHS
AND
IF
SO
HOW
CAN
THEY
EMIT
NEGATIVE
RED
LIGHT
TO
REPRODUCE
COLORS
IN
THE
CYAN
RANGE
IN
FACT
THE
DESIGN
OF
RGB
VIDEO
CAMERAS
HAS
HISTORICALLY
BEEN
BASED
AROUND
THE
AVAILABIL
ITY
OF
COLORED
PHOSPHORS
THAT
GO
INTO
TELEVISION
SETS
WHEN
STANDARD
DEFINITION
COLOR
TELEVISION
WAS
INVENTED
NTSC
A
MAPPING
WAS
DEFINED
BETWEEN
THE
RGB
VALUES
THAT
WOULD
DRIVE
THE
THREE
COLOR
GUNS
IN
THE
CATHODE
RAY
TUBE
CRT
AND
THE
XYZ
VALUES
THAT
UNAMBIGUOUSLY
DE
FINE
PERCEIVED
COLOR
THIS
STANDARD
WAS
CALLED
ITU
R
BT
WITH
THE
ADVENT
OF
HDTV
AND
NEWER
MONITORS
A
NEW
STANDARD
CALLED
ITU
R
BT
WAS
CREATED
WHICH
SPECIFIES
THE
XYZ
ANOTHER
PERCEPTUALLY
MOTIVATED
COLOR
SPACE
CALLED
L
U
V
WAS
DEVELOPED
AND
STANDARDIZED
SIMULTANEOUSLY
FAIRCHILD
VALUES
OF
EACH
OF
THE
COLOR
PRIMARIES
X
Y
IN
PRACTICE
EACH
COLOR
CAMERA
INTEGRATES
LIGHT
ACCORDING
TO
THE
SPECTRAL
RESPONSE
FUNCTION
OF
ITS
RED
GREEN
AND
BLUE
SENSORS
R
R
L
Λ
SR
Λ
DΛ
G
R
L
Λ
SG
Λ
DΛ
B
R
L
Λ
SB
Λ
DΛ
WHERE
L
Λ
IS
THE
INCOMING
SPECTRUM
OF
LIGHT
AT
A
GIVEN
PIXEL
AND
SR
Λ
SG
Λ
SB
Λ
ARE
THE
RED
GREEN
AND
BLUE
SPECTRAL
SENSITIVITIES
OF
THE
CORRESPONDING
SENSORS
CAN
WE
TELL
WHAT
SPECTRAL
SENSITIVITIES
THE
CAMERAS
ACTUALLY
HAVE
UNLESS
THE
CAMERA
MANUFACTURER
PROVIDES
US
WITH
THIS
DATA
OR
WE
OBSERVE
THE
RESPONSE
OF
THE
CAMERA
TO
A
WHOLE
SPECTRUM
OF
MONOCHROMATIC
LIGHTS
THESE
SENSITIVITIES
ARE
NOT
SPECIFIED
BY
A
STANDARD
SUCH
AS
BT
INSTEAD
ALL
THAT
MATTERS
IS
THAT
THE
TRI
STIMULUS
VALUES
FOR
A
GIVEN
COLOR
PRODUCE
THE
SPECIFIED
RGB
VALUES
THE
MANUFACTURER
IS
FREE
TO
USE
SENSORS
WITH
SENSITIVITIES
THAT
DO
NOT
MATCH
THE
STANDARD
XYZ
DEFINITIONS
SO
LONG
AS
THEY
CAN
LATER
BE
CONVERTED
THROUGH
A
LINEAR
TRANSFORM
TO
THE
STANDARD
COLORS
SIMILARLY
WHILE
TV
AND
COMPUTER
MONITORS
ARE
SUPPOSED
TO
PRODUCE
RGB
VALUES
AS
SPEC
IFIED
BY
EQUATION
THERE
IS
NO
REASON
THAT
THEY
CANNOT
USE
DIGITAL
LOGIC
TO
TRANSFORM
THE
INCOMING
RGB
VALUES
INTO
DIFFERENT
SIGNALS
TO
DRIVE
EACH
OF
THE
COLOR
CHANNELS
PROPERLY
CAL
IBRATED
MONITORS
MAKE
THIS
INFORMATION
AVAILABLE
TO
SOFTWARE
APPLICATIONS
THAT
PERFORM
COLOR
MANAGEMENT
SO
THAT
COLORS
IN
REAL
LIFE
ON
THE
SCREEN
AND
ON
THE
PRINTER
ALL
MATCH
AS
CLOSELY
AS
POSSIBLE
COLOR
FILTER
ARRAYS
WHILE
EARLY
COLOR
TV
CAMERAS
USED
THREE
VIDICONS
TUBES
TO
PERFORM
THEIR
SENSING
AND
LATER
CAMERAS
USED
THREE
SEPARATE
RGB
SENSING
CHIPS
MOST
OF
TODAY
DIGITAL
STILL
AND
VIDEO
CAM
ERAS
CAMERAS
USE
A
COLOR
FILTER
ARRAY
CFA
WHERE
ALTERNATING
SENSORS
ARE
COVERED
BY
DIFFERENT
COLORED
FILTERS
A
NEWER
CHIP
DESIGN
BY
FOVEON
HTTP
WWW
FOVEON
COM
STACKS
THE
RED
GREEN
AND
BLUE
SENSORS
BENEATH
EACH
OTHER
BUT
IT
HAS
NOT
YET
GAINED
WIDESPREAD
ADOPTION
A
B
FIGURE
BAYER
RGB
PATTERN
A
COLOR
FILTER
ARRAY
LAYOUT
B
INTERPOLATED
PIXEL
VALUES
WITH
UNKNOWN
GUESSED
VALUES
SHOWN
AS
LOWER
CASE
THE
MOST
COMMONLY
USED
PATTERN
IN
COLOR
CAMERAS
TODAY
IS
THE
BAYER
PATTERN
BAYER
WHICH
PLACES
GREEN
FILTERS
OVER
HALF
OF
THE
SENSORS
IN
A
CHECKERBOARD
PATTERN
AND
RED
AND
BLUE
FILTERS
OVER
THE
REMAINING
ONES
FIGURE
THE
REASON
THAT
THERE
ARE
TWICE
AS
MANY
GREEN
FILTERS
AS
RED
AND
BLUE
IS
BECAUSE
THE
LUMINANCE
SIGNAL
IS
MOSTLY
DETERMINED
BY
GREEN
VALUES
AND
THE
VISUAL
SYSTEM
IS
MUCH
MORE
SENSITIVE
TO
HIGH
FREQUENCY
DETAIL
IN
LUMINANCE
THAN
IN
CHROMINANCE
A
FACT
THAT
IS
EXPLOITED
IN
COLOR
IMAGE
COMPRESSION
SEE
SECTION
THE
PROCESS
OF
INTERPOLATING
THE
MISSING
COLOR
VALUES
SO
THAT
WE
HAVE
VALID
RGB
VALUES
FOR
ALL
THE
PIXELS
IS
KNOWN
AS
DEMOSAICING
AND
IS
COVERED
IN
DETAIL
IN
SECTION
SIMILARLY
COLOR
LCD
MONITORS
TYPICALLY
USE
ALTERNATING
STRIPES
OF
RED
GREEN
AND
BLUE
FILTERS
PLACED
IN
FRONT
OF
EACH
LIQUID
CRYSTAL
ACTIVE
AREA
TO
SIMULATE
THE
EXPERIENCE
OF
A
FULL
COLOR
DISPLAY
AS
BEFORE
BECAUSE
THE
VISUAL
SYSTEM
HAS
HIGHER
RESOLUTION
ACUITY
IN
LUMINANCE
THAN
CHROMINANCE
IT
IS
POSSIBLE
TO
DIGITALLY
PRE
FILTER
RGB
AND
MONOCHROME
IMAGES
TO
ENHANCE
THE
PERCEPTION
OF
CRISPNESS
BETRISEY
BLINN
DRESEVIC
ET
AL
PLATT
COLOR
BALANCE
BEFORE
ENCODING
THE
SENSED
RGB
VALUES
MOST
CAMERAS
PERFORM
SOME
KIND
OF
COLOR
BALANCING
OPERATION
IN
AN
ATTEMPT
TO
MOVE
THE
WHITE
POINT
OF
A
GIVEN
IMAGE
CLOSER
TO
PURE
WHITE
EQUAL
RGB
VALUES
IF
THE
COLOR
SYSTEM
AND
THE
ILLUMINATION
ARE
THE
SAME
THE
BT
SYSTEM
USES
THE
DAYLIGHT
ILLUMINANT
AS
ITS
REFERENCE
WHITE
THE
CHANGE
MAY
BE
MINIMAL
HOWEVER
IF
THE
ILLUMINANT
IS
STRONGLY
COLORED
SUCH
AS
INCANDESCENT
INDOOR
LIGHTING
WHICH
GENERALLY
RESULTS
IN
A
YELLOW
OR
ORANGE
HUE
THE
COMPENSATION
CAN
BE
QUITE
SIGNIFICANT
A
SIMPLE
WAY
TO
PERFORM
COLOR
CORRECTION
IS
TO
MULTIPLY
EACH
OF
THE
RGB
VALUES
BY
A
DIFFERENT
FACTOR
I
E
TO
APPLY
A
DIAGONAL
MATRIX
TRANSFORM
TO
THE
RGB
COLOR
SPACE
MORE
COMPLICATED
TRANSFORMS
WHICH
ARE
SOMETIMES
THE
RESULT
OF
MAPPING
TO
XYZ
SPACE
AND
BACK
Y
Y
VISIBLE
NOISE
Y
QUANTIZATION
Y
NOISE
FIGURE
GAMMA
COMPRESSION
A
THE
RELATIONSHIP
BETWEEN
THE
INPUT
SIGNAL
LUMINANCE
Y
AND
THE
TRANSMITTED
SIGNAL
Y
I
IS
GIVEN
BY
Y
I
Y
Γ
B
AT
THE
RECEIVER
THE
SIGNAL
Y
I
IS
EXPONENTIATED
BY
THE
FACTOR
Γ
Yˆ
Y
IΓ
NOISE
INTRODUCED
DURING
TRANSMISSION
IS
SQUASHED
IN
THE
DARK
REGIONS
WHICH
CORRESPONDS
TO
THE
MORE
NOISE
SENSITIVE
REGION
OF
THE
VISUAL
SYSTEM
ACTUALLY
PERFORM
A
COLOR
TWIST
I
E
THEY
USE
A
GENERAL
COLOR
TRANSFORM
MATRIX
EXER
CISE
HAS
YOU
EXPLORE
SOME
OF
THESE
ISSUES
GAMMA
IN
THE
EARLY
DAYS
OF
BLACK
AND
WHITE
TELEVISION
THE
PHOSPHORS
IN
THE
CRT
USED
TO
DISPLAY
THE
TV
SIGNAL
RESPONDED
NON
LINEARLY
TO
THEIR
INPUT
VOLTAGE
THE
RELATIONSHIP
BETWEEN
THE
VOLTAGE
AND
THE
RESULTING
BRIGHTNESS
WAS
CHARACTERIZED
BY
A
NUMBER
CALLED
GAMMA
Γ
SINCE
THE
FORMULA
WAS
ROUGHLY
B
V
Γ
WITH
A
Γ
OF
ABOUT
TO
COMPENSATE
FOR
THIS
EFFECT
THE
ELECTRONICS
IN
THE
TV
CAMERA
WOULD
PRE
MAP
THE
SENSED
LUMINANCE
Y
THROUGH
AN
INVERSE
GAMMA
Y
I
Y
Γ
WITH
A
TYPICAL
VALUE
OF
THE
MAPPING
OF
THE
SIGNAL
THROUGH
THIS
NON
LINEARITY
BEFORE
TRANSMISSION
HAD
A
BENEFICIAL
SIDE
EFFECT
NOISE
ADDED
DURING
TRANSMISSION
REMEMBER
THESE
WERE
ANALOG
DAYS
WOULD
BE
REDUCED
AFTER
APPLYING
THE
GAMMA
AT
THE
RECEIVER
IN
THE
DARKER
REGIONS
OF
THE
SIGNAL
WHERE
IT
WAS
MORE
VISIBLE
FIGURE
REMEMBER
THAT
OUR
VISUAL
SYSTEM
IS
ROUGHLY
SENSITIVE
TO
RELATIVE
DIFFERENCES
IN
LUMINANCE
THOSE
OF
YOU
OLD
ENOUGH
TO
REMEMBER
THE
EARLY
DAYS
OF
COLOR
TELEVISION
WILL
NATURALLY
THINK
OF
THE
HUE
ADJUSTMENT
KNOB
ON
THE
TELEVISION
SET
WHICH
COULD
PRODUCE
TRULY
BIZARRE
RESULTS
A
RELATED
TECHNIQUE
CALLED
COMPANDING
WAS
THE
BASIS
OF
THE
DOLBY
NOISE
REDUCTION
SYSTEMS
USED
WITH
AUDIO
TAPES
WHEN
COLOR
TELEVISION
WAS
INVENTED
IT
WAS
DECIDED
TO
SEPARATELY
PASS
THE
RED
GREEN
AND
BLUE
SIGNALS
THROUGH
THE
SAME
GAMMA
NON
LINEARITY
BEFORE
COMBINING
THEM
FOR
ENCODING
TODAY
EVEN
THOUGH
WE
NO
LONGER
HAVE
ANALOG
NOISE
IN
OUR
TRANSMISSION
SYSTEMS
SIGNALS
ARE
STILL
QUANTIZED
DURING
COMPRESSION
SEE
SECTION
SO
APPLYING
INVERSE
GAMMA
TO
SENSED
VALUES
IS
STILL
USEFUL
UNFORTUNATELY
FOR
BOTH
COMPUTER
VISION
AND
COMPUTER
GRAPHICS
THE
PRESENCE
OF
GAMMA
IN
IMAGES
IS
OFTEN
PROBLEMATIC
FOR
EXAMPLE
THE
PROPER
SIMULATION
OF
RADIOMETRIC
PHENOMENA
SUCH
AS
SHADING
SEE
SECTION
AND
EQUATION
OCCURS
IN
A
LINEAR
RADIANCE
SPACE
ONCE
ALL
OF
THE
COMPUTATIONS
HAVE
BEEN
PERFORMED
THE
APPROPRIATE
GAMMA
SHOULD
BE
APPLIED
BEFORE
DISPLAY
UNFORTUNATELY
MANY
COMPUTER
GRAPHICS
SYSTEMS
SUCH
AS
SHADING
MODELS
OPERATE
DIRECTLY
ON
RGB
VALUES
AND
DISPLAY
THESE
VALUES
DIRECTLY
FORTUNATELY
NEWER
COLOR
IMAGING
STANDARDS
SUCH
AS
THE
BIT
SCRGB
USE
A
LINEAR
SPACE
WHICH
MAKES
THIS
LESS
OF
A
PROBLEM
GLASSNER
IN
COMPUTER
VISION
THE
SITUATION
CAN
BE
EVEN
MORE
DAUNTING
THE
ACCURATE
DETERMINATION
OF
SURFACE
NORMALS
USING
A
TECHNIQUE
SUCH
AS
PHOTOMETRIC
STEREO
SECTION
OR
EVEN
A
SIMPLER
OPERATION
SUCH
AS
ACCURATE
IMAGE
DEBLURRING
REQUIRE
THAT
THE
MEASUREMENTS
BE
IN
A
LINEAR
SPACE
OF
INTENSITIES
THEREFORE
IT
IS
IMPERATIVE
WHEN
PERFORMING
DETAILED
QUANTITATIVE
COMPUTATIONS
SUCH
AS
THESE
TO
FIRST
UNDO
THE
GAMMA
AND
THE
PER
IMAGE
COLOR
RE
BALANCING
IN
THE
SENSED
COLOR
VALUES
CHAKRABARTI
SCHARSTEIN
AND
ZICKLER
DEVELOP
A
SOPHISTI
CATED
PARAMETER
MODEL
THAT
IS
A
GOOD
MATCH
TO
THE
PROCESSING
PERFORMED
BY
TODAY
DIGITAL
CAMERAS
THEY
ALSO
PROVIDE
A
DATABASE
OF
COLOR
IMAGES
YOU
CAN
USE
FOR
YOUR
OWN
TESTING
FOR
OTHER
VISION
APPLICATIONS
HOWEVER
SUCH
AS
FEATURE
DETECTION
OR
THE
MATCHING
OF
SIG
NALS
IN
STEREO
AND
MOTION
ESTIMATION
THIS
LINEARIZATION
STEP
IS
OFTEN
NOT
NECESSARY
IN
FACT
DETERMINING
WHETHER
IT
IS
NECESSARY
TO
UNDO
GAMMA
CAN
TAKE
SOME
CAREFUL
THINKING
E
G
IN
THE
CASE
OF
COMPENSATING
FOR
EXPOSURE
VARIATIONS
IN
IMAGE
STITCHING
SEE
EXERCISE
IF
ALL
OF
THESE
PROCESSING
STEPS
SOUND
CONFUSING
TO
MODEL
THEY
ARE
EXERCISE
HAS
YOU
TRY
TO
TEASE
APART
SOME
OF
THESE
PHENOMENA
USING
EMPIRICAL
INVESTIGATION
I
E
TAKING
PICTURES
OF
COLOR
CHARTS
AND
COMPARING
THE
RAW
AND
JPEG
COMPRESSED
COLOR
VALUES
OTHER
COLOR
SPACES
WHILE
RGB
AND
XYZ
ARE
THE
PRIMARY
COLOR
SPACES
USED
TO
DESCRIBE
THE
SPECTRAL
CONTENT
AND
HENCE
TRI
STIMULUS
RESPONSE
OF
COLOR
SIGNALS
A
VARIETY
OF
OTHER
REPRESENTATIONS
HAVE
BEEN
DEVELOPED
BOTH
IN
VIDEO
AND
STILL
IMAGE
CODING
AND
IN
COMPUTER
GRAPHICS
THE
EARLIEST
COLOR
REPRESENTATION
DEVELOPED
FOR
VIDEO
TRANSMISSION
WAS
THE
YIQ
STANDARD
DEVELOPED
FOR
NTSC
VIDEO
IN
NORTH
AMERICA
AND
THE
CLOSELY
RELATED
YUV
STANDARD
DEVELOPED
FOR
PAL
IN
EUROPE
IN
BOTH
OF
THESE
CASES
IT
WAS
DESIRED
TO
HAVE
A
LUMA
CHANNEL
Y
SO
CALLED
HTTP
VISION
MIDDLEBURY
EDU
COLOR
SINCE
IT
ONLY
ROUGHLY
MIMICS
TRUE
LUMINANCE
THAT
WOULD
BE
COMPARABLE
TO
THE
REGULAR
BLACK
AND
WHITE
TV
SIGNAL
ALONG
WITH
TWO
LOWER
FREQUENCY
CHROMA
CHANNELS
IN
BOTH
SYSTEMS
THE
Y
SIGNAL
OR
MORE
APPROPRIATELY
THE
Y
LUMA
SIGNAL
SINCE
IT
IS
GAMMA
COMPRESSED
IS
OBTAINED
FROM
WHERE
R
G
B
IS
THE
TRIPLET
OF
GAMMA
COMPRESSED
COLOR
COMPONENTS
WHEN
USING
THE
NEWER
COLOR
DEFINITIONS
FOR
HDTV
IN
BT
THE
FORMULA
IS
THE
UV
COMPONENTS
ARE
DERIVED
FROM
SCALED
VERSIONS
OF
BI
Y
I
AND
RI
Y
I
NAMELY
U
BI
Y
I
AND
V
RI
Y
I
WHEREAS
THE
IQ
COMPONENTS
ARE
THE
UV
COMPONENTS
ROTATED
THROUGH
AN
ANGLE
OF
IN
COMPOSITE
NTSC
AND
PAL
VIDEO
THE
CHROMA
SIGNALS
WERE
THEN
LOW
PASS
FILTERED
HORIZON
TALLY
BEFORE
BEING
MODULATED
AND
SUPERIMPOSED
ON
TOP
OF
THE
Y
LUMA
SIGNAL
BACKWARD
COMPATIBILITY
WAS
ACHIEVED
BY
HAVING
OLDER
BLACK
AND
WHITE
TV
SETS
EFFECTIVELY
IGNORE
THE
HIGH
FREQUENCY
CHROMA
SIGNAL
BECAUSE
OF
SLOW
ELECTRONICS
OR
AT
WORST
SUPERIMPOSING
IT
AS
A
HIGH
FREQUENCY
PATTERN
ON
TOP
OF
THE
MAIN
SIGNAL
WHILE
THESE
CONVERSIONS
WERE
IMPORTANT
IN
THE
EARLY
DAYS
OF
COMPUTER
VISION
WHEN
FRAME
GRABBERS
WOULD
DIRECTLY
DIGITIZE
THE
COMPOSITE
TV
SIGNAL
TODAY
ALL
DIGITAL
VIDEO
AND
STILL
IMAGE
COMPRESSION
STANDARDS
ARE
BASED
ON
THE
NEWER
YCBCR
CONVERSION
YCBCR
IS
CLOSELY
RELATED
TO
YUV
THE
CB
AND
CR
SIGNALS
CARRY
THE
BLUE
AND
RED
COLOR
DIFFERENCE
SIGNALS
AND
HAVE
MORE
USEFUL
MNEMONICS
THAN
UV
BUT
USES
DIFFERENT
SCALE
FACTORS
TO
FIT
WITHIN
THE
EIGHT
BIT
RANGE
AVAILABLE
WITH
DIGITAL
SIGNALS
FOR
VIDEO
THE
Y
SIGNAL
IS
RE
SCALED
TO
FIT
WITHIN
THE
RANGE
OF
VALUES
WHILE
THE
CB
AND
CR
SIGNALS
ARE
SCALED
TO
FIT
WITHIN
GOMES
AND
VELHO
FAIRCHILD
FOR
STILL
IMAGES
THE
JPEG
STANDARD
USES
THE
FULL
EIGHT
BIT
RANGE
WITH
NO
RESERVED
VALUES
Y
I
RI
CB
CR
GI
WHERE
THE
R
G
B
VALUES
ARE
THE
EIGHT
BIT
GAMMA
COMPRESSED
COLOR
COMPONENTS
I
E
THE
ACTUAL
RGB
VALUES
WE
OBTAIN
WHEN
WE
OPEN
UP
OR
DISPLAY
A
JPEG
IMAGE
FOR
MOST
APPLI
CATIONS
THIS
FORMULA
IS
NOT
THAT
IMPORTANT
SINCE
YOUR
IMAGE
READING
SOFTWARE
WILL
DIRECTLY
PROVIDE
YOU
WITH
THE
EIGHT
BIT
GAMMA
COMPRESSED
R
G
B
VALUES
HOWEVER
IF
YOU
ARE
TRYING
TO
DO
CAREFUL
IMAGE
DEBLOCKING
EXERCISE
THIS
INFORMATION
MAY
BE
USEFUL
ANOTHER
COLOR
SPACE
YOU
MAY
COME
ACROSS
IS
HUE
SATURATION
VALUE
HSV
WHICH
IS
A
PRO
JECTION
OF
THE
RGB
COLOR
CUBE
ONTO
A
NON
LINEAR
CHROMA
ANGLE
A
RADIAL
SATURATION
PERCENTAGE
AND
A
LUMINANCE
INSPIRED
VALUE
IN
MORE
DETAIL
VALUE
IS
DEFINED
AS
EITHER
THE
MEAN
OR
MAXI
MUM
COLOR
VALUE
SATURATION
IS
DEFINED
AS
SCALED
DISTANCE
FROM
THE
DIAGONAL
AND
HUE
IS
DEFINED
AS
THE
DIRECTION
AROUND
A
COLOR
WHEEL
THE
EXACT
FORMULAS
ARE
DESCRIBED
BY
HALL
FOLEY
VAN
DAM
FEINER
ET
AL
SUCH
A
DECOMPOSITION
IS
QUITE
NATURAL
IN
GRAPHICS
APPLICATIONS
SUCH
AS
COLOR
PICKING
IT
APPROXIMATES
THE
MUNSELL
CHART
FOR
COLOR
DESCRIPTION
FIGURE
N
SHOWS
AN
HSV
REPRESENTATION
OF
A
SAMPLE
COLOR
IMAGE
WHERE
SATURATION
IS
ENCODED
USING
A
GRAY
SCALE
SATURATED
DARKER
AND
HUE
IS
DEPICTED
AS
A
COLOR
IF
YOU
WANT
YOUR
COMPUTER
VISION
ALGORITHM
TO
ONLY
AFFECT
THE
VALUE
LUMINANCE
OF
AN
IMAGE
AND
NOT
ITS
SATURATION
OR
HUE
A
SIMPLER
SOLUTION
IS
TO
USE
EITHER
THE
Y
XY
LUMINANCE
CHROMATICITY
COORDINATES
DEFINED
IN
OR
THE
EVEN
SIMPLER
COLOR
RATIOS
R
R
G
B
G
G
R
G
B
B
B
R
G
B
FIGURE
H
AFTER
MANIPULATING
THE
LUMA
E
G
THROUGH
THE
PROCESS
OF
HISTOGRAM
EQUALIZATION
SECTION
YOU
CAN
MULTIPLY
EACH
COLOR
RATIO
BY
THE
RATIO
OF
THE
NEW
TO
OLD
LUMA
TO
OBTAIN
AN
ADJUSTED
RGB
TRIPLET
WHILE
ALL
OF
THESE
COLOR
SYSTEMS
MAY
SOUND
CONFUSING
IN
THE
END
IT
OFTEN
MAY
NOT
MAT
TER
THAT
MUCH
WHICH
ONE
YOU
USE
POYNTON
IN
HIS
COLOR
FAQ
HTTP
WWW
POYNTON
COM
COLORFAQ
HTML
NOTES
THAT
THE
PERCEPTUALLY
MOTIVATED
L
A
B
SYSTEM
IS
QUALITATIVELY
SIMILAR
TO
THE
GAMMA
COMPRESSED
R
G
B
SYSTEM
WE
MOSTLY
DEAL
WITH
SINCE
BOTH
HAVE
A
FRACTIONAL
POWER
SCALING
WHICH
APPROXIMATES
A
LOGARITHMIC
RESPONSE
BETWEEN
THE
ACTUAL
INTENSITY
VAL
UES
AND
THE
NUMBERS
BEING
MANIPULATED
AS
IN
ALL
CASES
THINK
CAREFULLY
ABOUT
WHAT
YOU
ARE
TRYING
TO
ACCOMPLISH
BEFORE
DECIDING
ON
A
TECHNIQUE
TO
USE
COMPRESSION
THE
LAST
STAGE
IN
A
CAMERA
PROCESSING
PIPELINE
IS
USUALLY
SOME
FORM
OF
IMAGE
COMPRESSION
UNLESS
YOU
ARE
USING
A
LOSSLESS
COMPRESSION
SCHEME
SUCH
AS
CAMERA
RAW
OR
PNG
ALL
COLOR
VIDEO
AND
IMAGE
COMPRESSION
ALGORITHMS
START
BY
CONVERTING
THE
SIGNAL
INTO
YCBCR
OR
SOME
CLOSELY
RELATED
VARIANT
SO
THAT
THEY
CAN
COMPRESS
THE
LUMINANCE
SIGNAL
WITH
HIGHER
FIDELITY
THAN
THE
CHROMINANCE
SIGNAL
RECALL
THAT
THE
HUMAN
VISUAL
SYSTEM
HAS
POORER
IF
YOU
ARE
AT
A
LOSS
FOR
QUESTIONS
AT
A
CONFERENCE
YOU
CAN
ALWAYS
ASK
WHY
THE
SPEAKER
DID
NOT
USE
A
PERCEPTUAL
COLOR
SPACE
SUCH
AS
L
A
B
CONVERSELY
IF
THEY
DID
USE
L
A
B
YOU
CAN
ASK
IF
THEY
HAVE
ANY
CONCRETE
EVIDENCE
THAT
THIS
WORKS
BETTER
THAN
REGULAR
COLORS
A
RGB
B
R
C
G
D
B
E
RGB
F
R
G
G
H
B
I
L
J
A
K
B
L
H
M
N
V
FIGURE
COLOR
SPACE
TRANSFORMATIONS
A
D
RGB
E
H
RGB
I
K
L
A
B
L
N
HSV
NOTE
THAT
THE
RGB
L
A
B
AND
HSV
VALUES
ARE
ALL
RE
SCALED
TO
FIT
THE
DYNAMIC
RANGE
OF
THE
PRINTED
PAGE
FREQUENCY
RESPONSE
TO
COLOR
THAN
TO
LUMINANCE
CHANGES
IN
VIDEO
IT
IS
COMMON
TO
SUBSAM
PLE
CB
AND
CR
BY
A
FACTOR
OF
TWO
HORIZONTALLY
WITH
STILL
IMAGES
JPEG
THE
SUBSAMPLING
AVERAGING
OCCURS
BOTH
HORIZONTALLY
AND
VERTICALLY
ONCE
THE
LUMINANCE
AND
CHROMINANCE
IMAGES
HAVE
BEEN
APPROPRIATELY
SUBSAMPLED
AND
SEPARATED
INTO
INDIVIDUAL
IMAGES
THEY
ARE
THEN
PASSED
TO
A
BLOCK
TRANSFORM
STAGE
THE
MOST
COMMON
TECHNIQUE
USED
HERE
IS
THE
DISCRETE
COSINE
TRANSFORM
DCT
WHICH
IS
A
REAL
VALUED
VARIANT
OF
THE
DISCRETE
FOURIER
TRANSFORM
DFT
SEE
SECTION
THE
DCT
IS
A
REASONABLE
APPROXIMATION
TO
THE
KARHUNEN
LOE
VE
OR
EIGENVALUE
DECOMPOSITION
OF
NATURAL
IMAGE
PATCHES
I
E
THE
DECOMPOSITION
THAT
SIMULTANEOUSLY
PACKS
THE
MOST
ENERGY
INTO
THE
FIRST
COEFFICIENTS
AND
DIAGONALIZES
THE
JOINT
COVARIANCE
MATRIX
AMONG
THE
PIXELS
MAKES
TRANSFORM
COEFFICIENTS
FIGURE
IMAGE
COMPRESSED
WITH
JPEG
AT
THREE
QUALITY
SETTINGS
NOTE
HOW
THE
AMOUNT
OF
BLOCK
ARTIFACT
AND
HIGH
FREQUENCY
ALIASING
MOSQUITO
NOISE
INCREASES
FROM
LEFT
TO
RIGHT
STATISTICALLY
INDEPENDENT
BOTH
MPEG
AND
JPEG
USE
DCT
TRANSFORMS
WALLACE
LE
GALL
ALTHOUGH
NEWER
VARIANTS
USE
SMALLER
BLOCKS
OR
ALTERNATIVE
TRANSFORMATIONS
SUCH
AS
WAVELETS
TAUBMAN
AND
MARCELLIN
AND
LAPPED
TRANSFORMS
MALVAR
ARE
NOW
USED
AFTER
TRANSFORM
CODING
THE
COEFFICIENT
VALUES
ARE
QUANTIZED
INTO
A
SET
OF
SMALL
INTEGER
VALUES
THAT
CAN
BE
CODED
USING
A
VARIABLE
BIT
LENGTH
SCHEME
SUCH
AS
A
HUFFMAN
CODE
OR
AN
ARITHMETIC
CODE
WALLACE
THE
DC
LOWEST
FREQUENCY
COEFFICIENTS
ARE
ALSO
ADAPTIVELY
PREDICTED
FROM
THE
PREVIOUS
BLOCK
DC
VALUES
THE
TERM
DC
COMES
FROM
DIRECT
CURRENT
I
E
THE
NON
SINUSOIDAL
OR
NON
ALTERNATING
PART
OF
A
SIGNAL
THE
STEP
SIZE
IN
THE
QUANTIZATION
IS
THE
MAIN
VARIABLE
CONTROLLED
BY
THE
QUALITY
SETTING
ON
THE
JPEG
FILE
FIGURE
WITH
VIDEO
IT
IS
ALSO
USUAL
TO
PERFORM
BLOCK
BASED
MOTION
COMPENSATION
I
E
TO
ENCODE
THE
DIFFERENCE
BETWEEN
EACH
BLOCK
AND
A
PREDICTED
SET
OF
PIXEL
VALUES
OBTAINED
FROM
A
SHIFTED
BLOCK
IN
THE
PREVIOUS
FRAME
THE
EXCEPTION
IS
THE
MOTION
JPEG
SCHEME
USED
IN
OLDER
DV
CAMCORDERS
WHICH
IS
NOTHING
MORE
THAN
A
SERIES
OF
INDIVIDUALLY
JPEG
COMPRESSED
IMAGE
FRAMES
WHILE
BASIC
MPEG
USES
MOTION
COMPENSATION
BLOCKS
WITH
INTEGER
MOTION
VALUES
LE
GALL
NEWER
STANDARDS
USE
ADAPTIVELY
SIZED
BLOCK
SUB
PIXEL
MOTIONS
AND
THE
ABILITY
TO
REFERENCE
BLOCKS
FROM
OLDER
FRAMES
IN
ORDER
TO
RECOVER
MORE
GRACEFULLY
FROM
FAILURES
AND
TO
ALLOW
FOR
RANDOM
ACCESS
TO
THE
VIDEO
STREAM
PREDICTED
P
FRAMES
ARE
INTERLEAVED
AMONG
INDEPENDENTLY
CODED
I
FRAMES
BI
DIRECTIONAL
B
FRAMES
ARE
ALSO
SOMETIMES
USED
THE
QUALITY
OF
A
COMPRESSION
ALGORITHM
IS
USUALLY
REPORTED
USING
ITS
PEAK
SIGNAL
TO
NOISE
RATIO
PSNR
WHICH
IS
DERIVED
FROM
THE
AVERAGE
MEAN
SQUARE
ERROR
M
SE
I
X
Iˆ
X
WHERE
I
X
IS
THE
ORIGINAL
UNCOMPRESSED
IMAGE
AND
Iˆ
X
IS
ITS
COMPRESSED
COUNTERPART
OR
EQUIVALENTLY
THE
ROOT
MEAN
SQUARE
ERROR
RMS
ERROR
WHICH
IS
DEFINED
AS
RM
M
SE
ADDITIONAL
READING
THE
PSNR
IS
DEFINED
AS
IMAX
P
SN
R
MAX
M
SE
RM
WHERE
IMAX
IS
THE
MAXIMUM
SIGNAL
EXTENT
E
G
FOR
EIGHT
BIT
IMAGES
WHILE
THIS
IS
JUST
A
HIGH
LEVEL
SKETCH
OF
HOW
IMAGE
COMPRESSION
WORKS
IT
IS
USEFUL
TO
UNDERSTAND
SO
THAT
THE
ARTIFACTS
INTRODUCED
BY
SUCH
TECHNIQUES
CAN
BE
COMPENSATED
FOR
IN
VARIOUS
COMPUTER
VISION
APPLICATIONS
ADDITIONAL
READING
AS
WE
MENTIONED
AT
THE
BEGINNING
OF
THIS
CHAPTER
IT
PROVIDES
BUT
A
BRIEF
SUMMARY
OF
A
VERY
RICH
AND
DEEP
SET
OF
TOPICS
TRADITIONALLY
COVERED
IN
A
NUMBER
OF
SEPARATE
FIELDS
A
MORE
THOROUGH
INTRODUCTION
TO
THE
GEOMETRY
OF
POINTS
LINES
PLANES
AND
PROJECTIONS
CAN
BE
FOUND
IN
TEXTBOOKS
ON
MULTI
VIEW
GEOMETRY
HARTLEY
AND
ZISSERMAN
FAUGERAS
AND
LUONG
AND
COMPUTER
GRAPHICS
FOLEY
VAN
DAM
FEINER
ET
AL
WATT
OPENGL
ARB
TOPICS
COVERED
IN
MORE
DEPTH
INCLUDE
HIGHER
ORDER
PRIMITIVES
SUCH
AS
QUADRICS
CONICS
AND
CUBICS
AS
WELL
AS
THREE
VIEW
AND
MULTI
VIEW
GEOMETRY
THE
IMAGE
FORMATION
SYNTHESIS
PROCESS
IS
TRADITIONALLY
TAUGHT
AS
PART
OF
A
COMPUTER
GRAPHICS
CURRICULUM
FOLEY
VAN
DAM
FEINER
ET
AL
GLASSNER
WATT
SHIRLEY
BUT
IT
IS
ALSO
STUDIED
IN
PHYSICS
BASED
COMPUTER
VISION
WOLFF
SHAFER
AND
HEALEY
THE
BEHAVIOR
OF
CAMERA
LENS
SYSTEMS
IS
STUDIED
IN
OPTICS
MO
LLER
HECHT
RAY
SOME
GOOD
BOOKS
ON
COLOR
THEORY
HAVE
BEEN
WRITTEN
BY
HEALEY
AND
SHAFER
WYSZECKI
AND
STILES
FAIRCHILD
WITH
LIVINGSTONE
PROVIDING
A
MORE
FUN
AND
INFOR
MAL
INTRODUCTION
TO
THE
TOPIC
OF
COLOR
PERCEPTION
MARK
FAIRCHILD
PAGE
OF
COLOR
BOOKS
AND
LISTS
MANY
OTHER
SOURCES
TOPICS
RELATING
TO
SAMPLING
AND
ALIASING
ARE
COVERED
IN
TEXTBOOKS
ON
SIGNAL
AND
IMAGE
PROCESSING
CRANE
JA
HNE
OPPENHEIM
AND
SCHAFER
OPPENHEIM
SCHAFER
AND
BUCK
PRATT
RUSS
BURGER
AND
BURGE
GONZALES
AND
WOODS
EXERCISES
A
NOTE
TO
STUDENTS
THIS
CHAPTER
IS
RELATIVELY
LIGHT
ON
EXERCISES
SINCE
IT
CONTAINS
MOSTLY
BACKGROUND
MATERIAL
AND
NOT
THAT
MANY
USABLE
TECHNIQUES
IF
YOU
REALLY
WANT
TO
UNDERSTAND
HTTP
WWW
CIS
RIT
EDU
FAIRCHILD
WHYISCOLOR
BOOKS
LINKS
HTML
MULTI
VIEW
GEOMETRY
IN
A
THOROUGH
WAY
I
ENCOURAGE
YOU
TO
READ
AND
DO
THE
EXERCISES
PROVIDED
BY
HARTLEY
AND
ZISSERMAN
SIMILARLY
IF
YOU
WANT
SOME
EXERCISES
RELATED
TO
THE
IMAGE
FORMATION
PROCESS
GLASSNER
BOOK
IS
FULL
OF
CHALLENGING
PROBLEMS
EX
LEAST
SQUARES
INTERSECTION
POINT
AND
LINE
FITTING
ADVANCED
EQUATION
SHOWS
HOW
THE
INTERSECTION
OF
TWO
LINES
CAN
BE
EXPRESSED
AS
THEIR
CROSS
PRODUCT
ASSUMING
THE
LINES
ARE
EXPRESSED
AS
HOMOGENEOUS
COORDINATES
IF
YOU
ARE
GIVEN
MORE
THAN
TWO
LINES
AND
WANT
TO
FIND
A
POINT
X
THAT
MINIMIZES
THE
SUM
OF
SQUARED
DISTANCES
TO
EACH
LINE
D
X
LI
I
HOW
CAN
YOU
COMPUTE
THIS
QUANTITY
HINT
WRITE
THE
DOT
PRODUCT
AS
X
T
LI
AND
TURN
THE
SQUARED
QUANTITY
INTO
A
QUADRATIC
FORM
X
T
AX
TO
FIT
A
LINE
TO
A
BUNCH
OF
POINTS
YOU
CAN
COMPUTE
THE
CENTROID
MEAN
OF
THE
POINTS
AS
WELL
AS
THE
COVARIANCE
MATRIX
OF
THE
POINTS
AROUND
THIS
MEAN
SHOW
THAT
THE
LINE
PASSING
THROUGH
THE
CENTROID
ALONG
THE
MAJOR
AXIS
OF
THE
COVARIANCE
ELLIPSOID
LARGEST
EIGENVECTOR
MINIMIZES
THE
SUM
OF
SQUARED
DISTANCES
TO
THE
POINTS
THESE
TWO
APPROACHES
ARE
FUNDAMENTALLY
DIFFERENT
EVEN
THOUGH
PROJECTIVE
DUALITY
TELLS
US
THAT
POINTS
AND
LINES
ARE
INTERCHANGEABLE
WHY
ARE
THESE
TWO
ALGORITHMS
SO
APPAR
ENTLY
DIFFERENT
ARE
THEY
ACTUALLY
MINIMIZING
DIFFERENT
OBJECTIVES
EX
TRANSFORM
EDITOR
WRITE
A
PROGRAM
THAT
LETS
YOU
INTERACTIVELY
CREATE
A
SET
OF
RECTANGLES
AND
THEN
MODIFY
THEIR
POSE
TRANSFORM
YOU
SHOULD
IMPLEMENT
THE
FOLLOWING
STEPS
OPEN
AN
EMPTY
WINDOW
CANVAS
SHIFT
DRAG
RUBBER
BAND
TO
CREATE
A
NEW
RECTANGLE
SELECT
THE
DEFORMATION
MODE
MOTION
MODEL
TRANSLATION
RIGID
SIMILARITY
AFFINE
OR
PERSPECTIVE
DRAG
ANY
CORNER
OF
THE
OUTLINE
TO
CHANGE
ITS
TRANSFORMATION
THIS
EXERCISE
SHOULD
BE
BUILT
ON
A
SET
OF
PIXEL
COORDINATE
AND
TRANSFORMATION
CLASSES
EITHER
IMPLEMENTED
BY
YOURSELF
OR
FROM
A
SOFTWARE
LIBRARY
PERSISTENCE
OF
THE
CREATED
REPRESENTATION
SAVE
AND
LOAD
SHOULD
ALSO
BE
SUPPORTED
FOR
EACH
RECTANGLE
SAVE
ITS
TRANSFORMATION
EX
VIEWER
WRITE
A
SIMPLE
VIEWER
FOR
POINTS
LINES
AND
POLYGONS
IMPORT
A
SET
OF
POINT
AND
LINE
COMMANDS
PRIMITIVES
AS
WELL
AS
A
VIEWING
TRANSFORM
INTERACTIVELY
MODIFY
THE
OBJECT
OR
CAMERA
TRANSFORM
THIS
VIEWER
CAN
BE
AN
EXTENSION
OF
THE
ONE
YOU
CREATED
IN
EXERCISE
SIMPLY
REPLACE
THE
VIEWING
TRANSFORMATIONS
WITH
THEIR
EQUIVALENTS
OPTIONAL
ADD
A
Z
BUFFER
TO
DO
HIDDEN
SURFACE
REMOVAL
FOR
POLYGONS
OPTIONAL
USE
A
DRAWING
PACKAGE
AND
JUST
WRITE
THE
VIEWER
CONTROL
EX
FOCUS
DISTANCE
AND
DEPTH
OF
FIELD
FIGURE
OUT
HOW
THE
FOCUS
DISTANCE
AND
DEPTH
OF
FIELD
INDICATORS
ON
A
LENS
ARE
DETERMINED
COMPUTE
AND
PLOT
THE
FOCUS
DISTANCE
ZO
AS
A
FUNCTION
OF
THE
DISTANCE
TRAVELED
FROM
THE
FOCAL
LENGTH
ZI
F
ZI
FOR
A
LENS
OF
FOCAL
LENGTH
F
SAY
DOES
THIS
EXPLAIN
THE
HYPERBOLIC
PROGRESSION
OF
FOCUS
DISTANCES
YOU
SEE
ON
A
TYPICAL
LENS
FIGURE
COMPUTE
THE
DEPTH
OF
FIELD
MINIMUM
AND
MAXIMUM
FOCUS
DISTANCES
FOR
A
GIVEN
FOCUS
SETTING
ZO
AS
A
FUNCTION
OF
THE
CIRCLE
OF
CONFUSION
DIAMETER
C
MAKE
IT
A
FRACTION
OF
THE
SENSOR
WIDTH
THE
FOCAL
LENGTH
F
AND
THE
F
STOP
NUMBER
N
WHICH
RELATES
TO
THE
APERTURE
DIAMETER
D
DOES
THIS
EXPLAIN
THE
USUAL
DEPTH
OF
FIELD
MARKINGS
ON
A
LENS
THAT
BRACKET
THE
IN
FOCUS
MARKER
AS
IN
FIGURE
NOW
CONSIDER
A
ZOOM
LENS
WITH
A
VARYING
FOCAL
LENGTH
F
ASSUME
THAT
AS
YOU
ZOOM
THE
LENS
STAYS
IN
FOCUS
I
E
THE
DISTANCE
FROM
THE
REAR
NODAL
POINT
TO
THE
SENSOR
PLANE
ZI
ADJUSTS
ITSELF
AUTOMATICALLY
FOR
A
FIXED
FOCUS
DISTANCE
ZO
HOW
DO
THE
DEPTH
OF
FIELD
INDICATORS
VARY
AS
A
FUNCTION
OF
FOCAL
LENGTH
CAN
YOU
REPRODUCE
A
TWO
DIMENSIONAL
PLOT
THAT
MIMICS
THE
CURVED
DEPTH
OF
FIELD
LINES
SEEN
ON
THE
LENS
IN
FIGURE
EX
F
NUMBERS
AND
SHUTTER
SPEEDS
LIST
THE
COMMON
F
NUMBERS
AND
SHUTTER
SPEEDS
THAT
YOUR
CAMERA
PROVIDES
ON
OLDER
MODEL
SLRS
THEY
ARE
VISIBLE
ON
THE
LENS
AND
SHUT
TER
SPEED
DIALS
ON
NEWER
CAMERAS
YOU
HAVE
TO
LOOK
AT
THE
ELECTRONIC
VIEWFINDER
OR
LCD
SCREEN
INDICATOR
AS
YOU
MANUALLY
ADJUST
EXPOSURES
DO
THESE
FORM
GEOMETRIC
PROGRESSIONS
IF
SO
WHAT
ARE
THE
RATIOS
HOW
DO
THESE
RELATE
TO
EXPOSURE
VALUES
EVS
IF
YOUR
CAMERA
HAS
SHUTTER
SPEEDS
OF
AND
DO
YOU
THINK
THAT
THESE
TWO
SPEEDS
ARE
EXACTLY
A
FACTOR
OF
TWO
APART
OR
A
FACTOR
OF
APART
HOW
ACCURATE
DO
YOU
THINK
THESE
NUMBERS
ARE
CAN
YOU
DEVISE
SOME
WAY
TO
MEASURE
EXACTLY
HOW
THE
APERTURE
AFFECTS
HOW
MUCH
LIGHT
REACHES
THE
SENSOR
AND
WHAT
THE
EXACT
EXPOSURE
TIMES
ACTUALLY
ARE
EX
NOISE
LEVEL
CALIBRATION
ESTIMATE
THE
AMOUNT
OF
NOISE
IN
YOUR
CAMERA
BY
TAKING
RE
PEATED
SHOTS
OF
A
SCENE
WITH
THE
CAMERA
MOUNTED
ON
A
TRIPOD
PURCHASING
A
REMOTE
SHUTTER
RELEASE
IS
A
GOOD
INVESTMENT
IF
YOU
OWN
A
DSLR
ALTERNATIVELY
TAKE
A
SCENE
WITH
CONSTANT
COLOR
REGIONS
SUCH
AS
A
COLOR
CHECKER
CHART
AND
ESTIMATE
THE
VARIANCE
BY
FITTING
A
SMOOTH
FUNCTION
TO
EACH
COLOR
REGION
AND
THEN
TAKING
DIFFERENCES
FROM
THE
PREDICTED
FUNCTION
PLOT
YOUR
ESTIMATED
VARIANCE
AS
A
FUNCTION
OF
LEVEL
FOR
EACH
OF
YOUR
COLOR
CHANNELS
SEPARATELY
CHANGE
THE
ISO
SETTING
ON
YOUR
CAMERA
IF
YOU
CANNOT
DO
THAT
REDUCE
THE
OVERALL
LIGHT
IN
YOUR
SCENE
TURN
OFF
LIGHTS
DRAW
THE
CURTAINS
WAIT
UNTIL
DUSK
DOES
THE
AMOUNT
OF
NOISE
VARY
A
LOT
WITH
ISO
GAIN
COMPARE
YOUR
CAMERA
TO
ANOTHER
ONE
AT
A
DIFFERENT
PRICE
POINT
OR
YEAR
OF
MAKE
IS
THERE
EVIDENCE
TO
SUGGEST
THAT
YOU
GET
WHAT
YOU
PAY
FOR
DOES
THE
QUALITY
OF
DIGITAL
CAMERAS
SEEM
TO
BE
IMPROVING
OVER
TIME
EX
GAMMA
CORRECTION
IN
IMAGE
STITCHING
HERE
A
RELATIVELY
SIMPLE
PUZZLE
ASSUME
YOU
ARE
GIVEN
TWO
IMAGES
THAT
ARE
PART
OF
A
PANORAMA
THAT
YOU
WANT
TO
STITCH
SEE
CHAPTER
THE
TWO
IMAGES
WERE
TAKEN
WITH
DIFFERENT
EXPOSURES
SO
YOU
WANT
TO
ADJUST
THE
RGB
VALUES
SO
THAT
THEY
MATCH
ALONG
THE
SEAM
LINE
IS
IT
NECESSARY
TO
UNDO
THE
GAMMA
IN
THE
COLOR
VALUES
IN
ORDER
TO
ACHIEVE
THIS
EX
SKIN
COLOR
DETECTION
DEVISE
A
SIMPLE
SKIN
COLOR
DETECTOR
FORSYTH
AND
FLECK
JONES
AND
REHG
VEZHNEVETS
SAZONOV
AND
ANDREEVA
KAKUMANU
MAKROGIANNIS
AND
BOURBAKIS
BASED
ON
CHROMATICITY
OR
OTHER
COLOR
PROPERTIES
TAKE
A
VARIETY
OF
PHOTOGRAPHS
OF
PEOPLE
AND
CALCULATE
THE
XY
CHROMATICITY
VALUES
FOR
EACH
PIXEL
CROP
THE
PHOTOS
OR
OTHERWISE
INDICATE
WITH
A
PAINTING
TOOL
WHICH
PIXELS
ARE
LIKELY
TO
BE
SKIN
E
G
FACE
AND
ARMS
CALCULATE
A
COLOR
CHROMATICITY
DISTRIBUTION
FOR
THESE
PIXELS
YOU
CAN
USE
SOMETHING
AS
SIMPLE
AS
A
MEAN
AND
COVARIANCE
MEASURE
OR
AS
COMPLICATED
AS
A
MEAN
SHIFT
SEGMENTA
TION
ALGORITHM
SEE
SECTION
YOU
CAN
OPTIONALLY
USE
NON
SKIN
PIXELS
TO
MODEL
THE
BACKGROUND
DISTRIBUTION
USE
YOUR
COMPUTED
DISTRIBUTION
TO
FIND
THE
SKIN
REGIONS
IN
AN
IMAGE
ONE
EASY
WAY
TO
VISUALIZE
THIS
IS
TO
PAINT
ALL
NON
SKIN
PIXELS
A
GIVEN
COLOR
SUCH
AS
WHITE
OR
BLACK
HOW
SENSITIVE
IS
YOUR
ALGORITHM
TO
COLOR
BALANCE
SCENE
LIGHTING
DOES
A
SIMPLER
CHROMATICITY
MEASUREMENT
SUCH
AS
A
COLOR
RATIO
WORK
JUST
AS
WELL
EX
WHITE
POINT
BALANCING
TRICKY
A
COMMON
IN
CAMERA
OR
POST
PROCESSING
TECH
NIQUE
FOR
PERFORMING
WHITE
POINT
ADJUSTMENT
IS
TO
TAKE
A
PICTURE
OF
A
WHITE
PIECE
OF
PAPER
AND
TO
ADJUST
THE
RGB
VALUES
OF
AN
IMAGE
TO
MAKE
THIS
A
NEUTRAL
COLOR
DESCRIBE
HOW
YOU
WOULD
ADJUST
THE
RGB
VALUES
IN
AN
IMAGE
GIVEN
A
SAMPLE
WHITE
COLOR
OF
RW
GW
BW
TO
MAKE
THIS
COLOR
NEUTRAL
WITHOUT
CHANGING
THE
EXPOSURE
TOO
MUCH
DOES
YOUR
TRANSFORMATION
INVOLVE
A
SIMPLE
PER
CHANNEL
SCALING
OF
THE
RGB
VALUES
OR
DO
YOU
NEED
A
FULL
COLOR
TWIST
MATRIX
OR
SOMETHING
ELSE
CONVERT
YOUR
RGB
VALUES
TO
XYZ
DOES
THE
APPROPRIATE
CORRECTION
NOW
ONLY
DEPEND
ON
THE
XY
OR
XY
VALUES
IF
SO
WHEN
YOU
CONVERT
BACK
TO
RGB
SPACE
DO
YOU
NEED
A
FULL
COLOR
TWIST
MATRIX
TO
ACHIEVE
THE
SAME
EFFECT
IF
YOU
USED
PURE
DIAGONAL
SCALING
IN
THE
DIRECT
RGB
MODE
BUT
END
UP
WITH
A
TWIST
IF
YOU
WORK
IN
XYZ
SPACE
HOW
DO
YOU
EXPLAIN
THIS
APPARENT
DICHOTOMY
WHICH
APPROACH
IS
CORRECT
OR
IS
IT
POSSIBLE
THAT
NEITHER
APPROACH
IS
ACTUALLY
CORRECT
IF
YOU
WANT
TO
FIND
OUT
WHAT
YOUR
CAMERA
ACTUALLY
DOES
CONTINUE
ON
TO
THE
NEXT
EXERCISE
EX
IN
CAMERA
COLOR
PROCESSING
CHALLENGING
IF
YOUR
CAMERA
SUPPORTS
A
RAW
PIXEL
MODE
TAKE
A
PAIR
OF
RAW
AND
JPEG
IMAGES
AND
SEE
IF
YOU
CAN
INFER
WHAT
THE
CAMERA
IS
DOING
WHEN
IT
CONVERTS
THE
RAW
PIXEL
VALUES
TO
THE
FINAL
COLOR
CORRECTED
AND
GAMMA
COMPRESSED
EIGHT
BIT
JPEG
PIXEL
VALUES
DEDUCE
THE
PATTERN
IN
YOUR
COLOR
FILTER
ARRAY
FROM
THE
CORRESPONDENCE
BETWEEN
CO
LOCATED
RAW
AND
COLOR
MAPPED
PIXEL
VALUES
USE
A
COLOR
CHECKER
CHART
AT
THIS
STAGE
IF
IT
MAKES
YOUR
LIFE
EASIER
YOU
MAY
FIND
IT
HELPFUL
TO
SPLIT
THE
RAW
IMAGE
INTO
FOUR
SEPARATE
IMAGES
SUBSAMPLING
EVEN
AND
ODD
COLUMNS
AND
ROWS
AND
TO
TREAT
EACH
OF
THESE
NEW
IMAGES
AS
A
VIRTUAL
SENSOR
EVALUATE
THE
QUALITY
OF
THE
DEMOSAICING
ALGORITHM
BY
TAKING
PICTURES
OF
CHALLENGING
SCENES
WHICH
CONTAIN
STRONG
COLOR
EDGES
SUCH
AS
THOSE
SHOWN
IN
IN
SECTION
IF
YOU
CAN
TAKE
THE
SAME
EXACT
PICTURE
AFTER
CHANGING
THE
COLOR
BALANCE
VALUES
IN
YOUR
CAMERA
COMPARE
HOW
THESE
SETTINGS
AFFECT
THIS
PROCESSING
COMPARE
YOUR
RESULTS
AGAINST
THOSE
PRESENTED
BY
CHAKRABARTI
SCHARSTEIN
AND
ZICKLER
OR
USE
THE
DATA
AVAILABLE
IN
THEIR
DATABASE
OF
COLOR
IMAGES
HTTP
VISION
MIDDLEBURY
EDU
COLOR
CHAPTER
IMAGE
PROCESSING
POINT
OPERATORS
PIXEL
TRANSFORMS
COLOR
TRANSFORMS
COMPOSITING
AND
MATTING
HISTOGRAM
EQUALIZATION
APPLICATION
TONAL
ADJUSTMENT
LINEAR
FILTERING
SEPARABLE
FILTERING
EXAMPLES
OF
LINEAR
FILTERING
BAND
PASS
AND
STEERABLE
FILTERS
MORE
NEIGHBORHOOD
OPERATORS
NON
LINEAR
FILTERING
MORPHOLOGY
DISTANCE
TRANSFORMS
CONNECTED
COMPONENTS
FOURIER
TRANSFORMS
FOURIER
TRANSFORM
PAIRS
TWO
DIMENSIONAL
FOURIER
TRANSFORMS
WIENER
FILTERING
APPLICATION
SHARPENING
BLUR
AND
NOISE
REMOVAL
PYRAMIDS
AND
WAVELETS
INTERPOLATION
DECIMATION
MULTI
RESOLUTION
REPRESENTATIONS
WAVELETS
APPLICATION
IMAGE
BLENDING
GEOMETRIC
TRANSFORMATIONS
PARAMETRIC
TRANSFORMATIONS
MESH
BASED
WARPING
APPLICATION
FEATURE
BASED
MORPHING
GLOBAL
OPTIMIZATION
REGULARIZATION
MARKOV
RANDOM
FIELDS
APPLICATION
IMAGE
RESTORATION
ADDITIONAL
READING
EXERCISES
A
B
C
D
E
F
FIGURE
SOME
COMMON
IMAGE
PROCESSING
OPERATIONS
A
ORIGINAL
IMAGE
B
INCREASED
CONTRAST
C
CHANGE
IN
HUE
D
POSTERIZED
QUANTIZED
COLORS
E
BLURRED
F
ROTATED
NOW
THAT
WE
HAVE
SEEN
HOW
IMAGES
ARE
FORMED
THROUGH
THE
INTERACTION
OF
SCENE
ELEMENTS
LIGHTING
AND
CAMERA
OPTICS
AND
SENSORS
LET
US
LOOK
AT
THE
FIRST
STAGE
IN
MOST
COMPUTER
VISION
APPLICATIONS
NAMELY
THE
USE
OF
IMAGE
PROCESSING
TO
PREPROCESS
THE
IMAGE
AND
CONVERT
IT
INTO
A
FORM
SUITABLE
FOR
FURTHER
ANALYSIS
EXAMPLES
OF
SUCH
OPERATIONS
INCLUDE
EXPOSURE
CORRECTION
AND
COLOR
BALANCING
THE
REDUCTION
OF
IMAGE
NOISE
INCREASING
SHARPNESS
OR
STRAIGHTENING
THE
IMAGE
BY
ROTATING
IT
FIGURE
WHILE
SOME
MAY
CONSIDER
IMAGE
PROCESSING
TO
BE
OUTSIDE
THE
PURVIEW
OF
COMPUTER
VISION
MOST
COMPUTER
VISION
APPLICATIONS
SUCH
AS
COMPUTATIONAL
PHOTOGRAPHY
AND
EVEN
RECOGNITION
REQUIRE
CARE
IN
DESIGNING
THE
IMAGE
PROCESSING
STAGES
IN
ORDER
TO
ACHIEVE
ACCEPTABLE
RESULTS
IN
THIS
CHAPTER
WE
REVIEW
STANDARD
IMAGE
PROCESSING
OPERATORS
THAT
MAP
PIXEL
VALUES
FROM
ONE
IMAGE
TO
ANOTHER
IMAGE
PROCESSING
IS
OFTEN
TAUGHT
IN
ELECTRICAL
ENGINEERING
DEPARTMENTS
AS
A
FOLLOW
ON
COURSE
TO
AN
INTRODUCTORY
COURSE
IN
SIGNAL
PROCESSING
OPPENHEIM
AND
SCHAFER
OPPENHEIM
SCHAFER
AND
BUCK
THERE
ARE
SEVERAL
POPULAR
TEXTBOOKS
FOR
IMAGE
PROCESSING
CRANE
GOMES
AND
VELHO
JA
HNE
PRATT
RUSS
BURGER
AND
BURGE
GONZALES
AND
WOODS
WE
BEGIN
THIS
CHAPTER
WITH
THE
SIMPLEST
KIND
OF
IMAGE
TRANSFORMS
NAMELY
THOSE
THAT
MANIPULATE
EACH
PIXEL
INDEPENDENTLY
OF
ITS
NEIGHBORS
SECTION
SUCH
TRANSFORMS
ARE
OF
TEN
CALLED
POINT
OPERATORS
OR
POINT
PROCESSES
NEXT
WE
EXAMINE
NEIGHBORHOOD
AREA
BASED
OPERATORS
WHERE
EACH
NEW
PIXEL
VALUE
DEPENDS
ON
A
SMALL
NUMBER
OF
NEIGHBORING
INPUT
VALUES
SECTIONS
AND
A
CONVENIENT
TOOL
TO
ANALYZE
AND
SOMETIMES
ACCELERATE
SUCH
NEIGHBORHOOD
OPERATIONS
IS
THE
FOURIER
TRANSFORM
WHICH
WE
COVER
IN
SECTION
NEIGHBOR
HOOD
OPERATORS
CAN
BE
CASCADED
TO
FORM
IMAGE
PYRAMIDS
AND
WAVELETS
WHICH
ARE
USEFUL
FOR
ANALYZING
IMAGES
AT
A
VARIETY
OF
RESOLUTIONS
SCALES
AND
FOR
ACCELERATING
CERTAIN
OPERATIONS
SECTION
ANOTHER
IMPORTANT
CLASS
OF
GLOBAL
OPERATORS
ARE
GEOMETRIC
TRANSFORMATIONS
SUCH
AS
ROTATIONS
SHEARS
AND
PERSPECTIVE
DEFORMATIONS
SECTION
FINALLY
WE
INTRODUCE
GLOBAL
OPTIMIZATION
APPROACHES
TO
IMAGE
PROCESSING
WHICH
INVOLVE
THE
MINIMIZATION
OF
AN
ENERGY
FUNCTIONAL
OR
EQUIVALENTLY
OPTIMAL
ESTIMATION
USING
BAYESIAN
MARKOV
RANDOM
FIELD
MODELS
SECTION
POINT
OPERATORS
THE
SIMPLEST
KINDS
OF
IMAGE
PROCESSING
TRANSFORMS
ARE
POINT
OPERATORS
WHERE
EACH
OUTPUT
PIXEL
VALUE
DEPENDS
ON
ONLY
THE
CORRESPONDING
INPUT
PIXEL
VALUE
PLUS
POTENTIALLY
SOME
GLOBALLY
COLLECTED
INFORMATION
OR
PARAMETERS
EXAMPLES
OF
SUCH
OPERATORS
INCLUDE
BRIGHTNESS
AND
CONTRAST
ADJUSTMENTS
FIGURE
AS
WELL
AS
COLOR
CORRECTION
AND
TRANSFORMATIONS
IN
THE
IMAGE
PROCESSING
LITERATURE
SUCH
OPERATIONS
ARE
ALSO
KNOWN
AS
POINT
PROCESSES
CRANE
WE
BEGIN
THIS
SECTION
WITH
A
QUICK
REVIEW
OF
SIMPLE
POINT
OPERATORS
SUCH
AS
BRIGHTNESS
A
B
C
D
E
F
FIGURE
SOME
LOCAL
IMAGE
PROCESSING
OPERATIONS
A
ORIGINAL
IMAGE
ALONG
WITH
ITS
THREE
COLOR
PER
CHANNEL
HISTOGRAMS
B
BRIGHTNESS
INCREASED
ADDITIVE
OFFSET
B
C
CONTRAST
INCREASED
MULTIPLICATIVE
GAIN
A
D
GAMMA
PARTIALLY
LINEARIZED
Γ
E
FULL
HISTOGRAM
EQUALIZATION
F
PARTIAL
HISTOGRAM
EQUALIZATION
RANGE
A
B
C
D
FIGURE
VISUALIZING
IMAGE
DATA
A
ORIGINAL
IMAGE
B
CROPPED
PORTION
AND
SCANLINE
PLOT
USING
AN
IMAGE
INSPECTION
TOOL
C
GRID
OF
NUMBERS
D
SURFACE
PLOT
FOR
FIGURES
C
D
THE
IMAGE
WAS
FIRST
CONVERTED
TO
GRAYSCALE
SCALING
AND
IMAGE
ADDITION
NEXT
WE
DISCUSS
HOW
COLORS
IN
IMAGES
CAN
BE
MANIPULATED
WE
THEN
PRESENT
IMAGE
COMPOSITING
AND
MATTING
OPERATIONS
WHICH
PLAY
AN
IMPORTANT
ROLE
IN
COMPUTATIONAL
PHOTOGRAPHY
CHAPTER
AND
COMPUTER
GRAPHICS
APPLICATIONS
FINALLY
WE
DESCRIBE
THE
MORE
GLOBAL
PROCESS
OF
HISTOGRAM
EQUALIZATION
WE
CLOSE
WITH
AN
EXAMPLE
APPLI
CATION
THAT
MANIPULATES
TONAL
VALUES
EXPOSURE
AND
CONTRAST
TO
IMPROVE
IMAGE
APPEARANCE
PIXEL
TRANSFORMS
A
GENERAL
IMAGE
PROCESSING
OPERATOR
IS
A
FUNCTION
THAT
TAKES
ONE
OR
MORE
INPUT
IMAGES
AND
PRODUCES
AN
OUTPUT
IMAGE
IN
THE
CONTINUOUS
DOMAIN
THIS
CAN
BE
DENOTED
AS
G
X
H
F
X
OR
G
X
H
X
FN
X
WHERE
X
IS
IN
THE
D
DIMENSIONAL
DOMAIN
OF
THE
FUNCTIONS
USUALLY
D
FOR
IMAGES
AND
THE
FUNCTIONS
F
AND
G
OPERATE
OVER
SOME
RANGE
WHICH
CAN
EITHER
BE
SCALAR
OR
VECTOR
VALUED
E
G
FOR
COLOR
IMAGES
OR
MOTION
FOR
DISCRETE
SAMPLED
IMAGES
THE
DOMAIN
CONSISTS
OF
A
FINITE
NUMBER
OF
PIXEL
LOCATIONS
X
I
J
AND
WE
CAN
WRITE
G
I
J
H
F
I
J
FIGURE
SHOWS
HOW
AN
IMAGE
CAN
BE
REPRESENTED
EITHER
BY
ITS
COLOR
APPEARANCE
AS
A
GRID
OF
NUMBERS
OR
AS
A
TWO
DIMENSIONAL
FUNCTION
SURFACE
PLOT
TWO
COMMONLY
USED
POINT
PROCESSES
ARE
MULTIPLICATION
AND
ADDITION
WITH
A
CONSTANT
G
X
AF
X
B
THE
PARAMETERS
A
AND
B
ARE
OFTEN
CALLED
THE
GAIN
AND
BIAS
PARAMETERS
SOMETIMES
THESE
PARAMETERS
ARE
SAID
TO
CONTROL
CONTRAST
AND
BRIGHTNESS
RESPECTIVELY
FIGURES
C
THE
AN
IMAGE
LUMINANCE
CHARACTERISTICS
CAN
ALSO
BE
SUMMARIZED
BY
ITS
KEY
AVERAGE
LUMINANANCE
AND
RANGE
KOPF
UYTTENDAELE
DEUSSEN
ET
AL
BIAS
AND
GAIN
PARAMETERS
CAN
ALSO
BE
SPATIALLY
VARYING
G
X
A
X
F
X
B
X
E
G
WHEN
SIMULATING
THE
GRADED
DENSITY
FILTER
USED
BY
PHOTOGRAPHERS
TO
SELECTIVELY
DARKEN
THE
SKY
OR
WHEN
MODELING
VIGNETTING
IN
AN
OPTICAL
SYSTEM
MULTIPLICATIVE
GAIN
BOTH
GLOBAL
AND
SPATIALLY
VARYING
IS
A
LINEAR
OPERATION
SINCE
IT
OBEYS
THE
SUPERPOSITION
PRINCIPLE
H
H
H
WE
WILL
HAVE
MORE
TO
SAY
ABOUT
LINEAR
SHIFT
INVARIANT
OPERATORS
IN
SECTION
OPERATORS
SUCH
AS
IMAGE
SQUARING
WHICH
IS
OFTEN
USED
TO
GET
A
LOCAL
ESTIMATE
OF
THE
ENERGY
IN
A
BAND
PASS
FILTERED
SIGNAL
SEE
SECTION
ARE
NOT
LINEAR
ANOTHER
COMMONLY
USED
DYADIC
TWO
INPUT
OPERATOR
IS
THE
LINEAR
BLEND
OPERATOR
G
X
Α
X
X
BY
VARYING
Α
FROM
THIS
OPERATOR
CAN
BE
USED
TO
PERFORM
A
TEMPORAL
CROSS
DISSOLVE
BETWEEN
TWO
IMAGES
OR
VIDEOS
AS
SEEN
IN
SLIDE
SHOWS
AND
FILM
PRODUCTION
OR
AS
A
COMPONENT
OF
IMAGE
MORPHING
ALGORITHMS
SECTION
ONE
HIGHLY
USED
NON
LINEAR
TRANSFORM
THAT
IS
OFTEN
APPLIED
TO
IMAGES
BEFORE
FURTHER
PRO
CESSING
IS
GAMMA
CORRECTION
WHICH
IS
USED
TO
REMOVE
THE
NON
LINEAR
MAPPING
BETWEEN
INPUT
RADIANCE
AND
QUANTIZED
PIXEL
VALUES
SECTION
TO
INVERT
THE
GAMMA
MAPPING
APPLIED
BY
THE
SENSOR
WE
CAN
USE
G
X
F
X
Γ
WHERE
A
GAMMA
VALUE
OF
Γ
IS
A
REASONABLE
FIT
FOR
MOST
DIGITAL
CAMERAS
COLOR
TRANSFORMS
WHILE
COLOR
IMAGES
CAN
BE
TREATED
AS
ARBITRARY
VECTOR
VALUED
FUNCTIONS
OR
COLLECTIONS
OF
INDE
PENDENT
BANDS
IT
USUALLY
MAKES
SENSE
TO
THINK
ABOUT
THEM
AS
HIGHLY
CORRELATED
SIGNALS
WITH
STRONG
CONNECTIONS
TO
THE
IMAGE
FORMATION
PROCESS
SECTION
SENSOR
DESIGN
SECTION
AND
HUMAN
PERCEPTION
SECTION
CONSIDER
FOR
EXAMPLE
BRIGHTENING
A
PICTURE
BY
ADDING
A
CONSTANT
VALUE
TO
ALL
THREE
CHANNELS
AS
SHOWN
IN
FIGURE
CAN
YOU
TELL
IF
THIS
ACHIEVES
THE
DESIRED
EFFECT
OF
MAKING
THE
IMAGE
LOOK
BRIGHTER
CAN
YOU
SEE
ANY
UNDESIRABLE
SIDE
EFFECTS
OR
ARTIFACTS
IN
FACT
ADDING
THE
SAME
VALUE
TO
EACH
COLOR
CHANNEL
NOT
ONLY
INCREASES
THE
APPARENT
IN
TENSITY
OF
EACH
PIXEL
IT
CAN
ALSO
AFFECT
THE
PIXEL
HUE
AND
SATURATION
HOW
CAN
WE
DEFINE
AND
MANIPULATE
SUCH
QUANTITIES
IN
ORDER
TO
ACHIEVE
THE
DESIRED
PERCEPTUAL
EFFECTS
A
B
C
D
FIGURE
IMAGE
MATTING
AND
COMPOSITING
CHUANG
CURLESS
SALESIN
ET
AL
QC
IEEE
A
SOURCE
IMAGE
B
EXTRACTED
FOREGROUND
OBJECT
F
C
ALPHA
MATTE
Α
SHOWN
IN
GRAYSCALE
D
NEW
COMPOSITE
C
AS
DISCUSSED
IN
SECTION
CHROMATICITY
COORDINATES
OR
EVEN
SIMPLER
COLOR
RA
TIOS
CAN
FIRST
BE
COMPUTED
AND
THEN
USED
AFTER
MANIPULATING
E
G
BRIGHTENING
THE
LUMINANCE
Y
TO
RE
COMPUTE
A
VALID
RGB
IMAGE
WITH
THE
SAME
HUE
AND
SATURATION
FIGURE
I
SHOWS
SOME
COLOR
RATIO
IMAGES
MULTIPLIED
BY
THE
MIDDLE
GRAY
VALUE
FOR
BETTER
VISUAL
IZATION
SIMILARLY
COLOR
BALANCING
E
G
TO
COMPENSATE
FOR
INCANDESCENT
LIGHTING
CAN
BE
PER
FORMED
EITHER
BY
MULTIPLYING
EACH
CHANNEL
WITH
A
DIFFERENT
SCALE
FACTOR
OR
BY
THE
MORE
COM
PLEX
PROCESS
OF
MAPPING
TO
XYZ
COLOR
SPACE
CHANGING
THE
NOMINAL
WHITE
POINT
AND
MAPPING
BACK
TO
RGB
WHICH
CAN
BE
WRITTEN
DOWN
USING
A
LINEAR
COLOR
TWIST
TRANSFORM
MATRIX
EXERCISES
AND
HAVE
YOU
EXPLORE
SOME
OF
THESE
ISSUES
ANOTHER
FUN
PROJECT
BEST
ATTEMPTED
AFTER
YOU
HAVE
MASTERED
THE
REST
OF
THE
MATERIAL
IN
THIS
CHAPTER
IS
TO
TAKE
A
PICTURE
WITH
A
RAINBOW
IN
IT
AND
ENHANCE
THE
STRENGTH
OF
THE
RAINBOW
EXERCISE
COMPOSITING
AND
MATTING
IN
MANY
PHOTO
EDITING
AND
VISUAL
EFFECTS
APPLICATIONS
IT
IS
OFTEN
DESIRABLE
TO
CUT
A
FOREGROUND
OBJECT
OUT
OF
ONE
SCENE
AND
PUT
IT
ON
TOP
OF
A
DIFFERENT
BACKGROUND
FIGURE
THE
PROCESS
OF
EXTRACTING
THE
OBJECT
FROM
THE
ORIGINAL
IMAGE
IS
OFTEN
CALLED
MATTING
SMITH
AND
BLINN
WHILE
THE
PROCESS
OF
INSERTING
IT
INTO
ANOTHER
IMAGE
WITHOUT
VISIBLE
ARTIFACTS
IS
CALLED
COMPOSITING
PORTER
AND
DUFF
BLINN
THE
INTERMEDIATE
REPRESENTATION
USED
FOR
THE
FOREGROUND
OBJECT
BETWEEN
THESE
TWO
STAGES
IS
CALLED
AN
ALPHA
MATTED
COLOR
IMAGE
FIGURE
C
IN
ADDITION
TO
THE
THREE
COLOR
RGB
CHANNELS
AN
ALPHA
MATTED
IMAGE
CONTAINS
A
FOURTH
ALPHA
CHANNEL
Α
OR
A
THAT
DESCRIBES
THE
RELATIVE
AMOUNT
OF
OPACITY
OR
FRACTIONAL
COVERAGE
AT
EACH
PIXEL
FIGURES
AND
THE
OPACITY
IS
THE
OPPOSITE
OF
THE
TRANSPARENCY
PIXELS
WITHIN
THE
OBJECT
ARE
FULLY
OPAQUE
Α
WHILE
PIXELS
FULLY
OUTSIDE
THE
OBJECT
ARE
TRANSPARENT
Α
PIXELS
ON
THE
BOUNDARY
OF
THE
OBJECT
VARY
SMOOTHLY
BETWEEN
THESE
TWO
EXTREMES
WHICH
HIDES
THE
PERCEPTUAL
VISIBLE
JAGGIES
B
Α
ΑF
C
A
B
C
D
FIGURE
COMPOSITING
EQUATION
C
Α
B
ΑF
THE
IMAGES
ARE
TAKEN
FROM
A
CLOSE
UP
OF
THE
REGION
OF
THE
HAIR
IN
THE
UPPER
RIGHT
PART
OF
THE
LION
IN
FIGURE
THAT
OCCUR
IF
ONLY
BINARY
OPACITIES
ARE
USED
TO
COMPOSITE
A
NEW
OR
FOREGROUND
IMAGE
ON
TOP
OF
AN
OLD
BACKGROUND
IMAGE
THE
OVER
OPERATOR
FIRST
PROPOSED
BY
PORTER
AND
DUFF
AND
THEN
STUDIED
EXTENSIVELY
BY
BLINN
IS
USED
C
Α
B
ΑF
THIS
OPERATOR
ATTENUATES
THE
INFLUENCE
OF
THE
BACKGROUND
IMAGE
B
BY
A
FACTOR
Α
AND
THEN
ADDS
IN
THE
COLOR
AND
OPACITY
VALUES
CORRESPONDING
TO
THE
FOREGROUND
LAYER
F
AS
SHOWN
IN
FIGURE
IN
MANY
SITUATIONS
IT
IS
CONVENIENT
TO
REPRESENT
THE
FOREGROUND
COLORS
IN
PRE
MULTIPLIED
FORM
I
E
TO
STORE
AND
MANIPULATE
THE
ΑF
VALUES
DIRECTLY
AS
BLINN
SHOWS
THE
PRE
MULTIPLIED
RGBA
REPRESENTATION
IS
PREFERRED
FOR
SEVERAL
REASONS
INCLUDING
THE
ABILITY
TO
BLUR
OR
RESAMPLE
E
G
ROTATE
ALPHA
MATTED
IMAGES
WITHOUT
ANY
ADDITIONAL
COMPLICATIONS
JUST
TREATING
EACH
RGBA
BAND
INDEPENDENTLY
HOWEVER
WHEN
MATTING
USING
LOCAL
COLOR
CONSISTENCY
RUZON
AND
TOMASI
CHUANG
CURLESS
SALESIN
ET
AL
THE
PURE
UN
MULTIPLIED
FOREGROUND
COLORS
F
ARE
USED
SINCE
THESE
REMAIN
CONSTANT
OR
VARY
SLOWLY
IN
THE
VICINITY
OF
THE
OBJECT
EDGE
THE
OVER
OPERATION
IS
NOT
THE
ONLY
KIND
OF
COMPOSITING
OPERATION
THAT
CAN
BE
USED
PORTER
AND
DUFF
DESCRIBE
A
NUMBER
OF
ADDITIONAL
OPERATIONS
THAT
CAN
BE
USEFUL
IN
PHOTO
EDITING
AND
VISUAL
EFFECTS
APPLICATIONS
IN
THIS
BOOK
WE
CONCERN
OURSELVES
WITH
ONLY
ONE
ADDITIONAL
COMMONLY
OCCURRING
CASE
BUT
SEE
EXERCISE
WHEN
LIGHT
REFLECTS
OFF
CLEAN
TRANSPARENT
GLASS
THE
LIGHT
PASSING
THROUGH
THE
GLASS
AND
THE
LIGHT
REFLECTING
OFF
THE
GLASS
ARE
SIMPLY
ADDED
TOGETHER
FIGURE
THIS
MODEL
IS
USE
FUL
IN
THE
ANALYSIS
OF
TRANSPARENT
MOTION
BLACK
AND
ANANDAN
SZELISKI
AVIDAN
AND
ANANDAN
WHICH
OCCURS
WHEN
SUCH
SCENES
ARE
OBSERVED
FROM
A
MOVING
CAMERA
SEE
SECTION
THE
ACTUAL
PROCESS
OF
MATTING
I
E
RECOVERING
THE
FOREGROUND
BACKGROUND
AND
ALPHA
MATTE
VALUES
FROM
ONE
OR
MORE
IMAGES
HAS
A
RICH
HISTORY
WHICH
WE
STUDY
IN
SECTION
FIGURE
AN
EXAMPLE
OF
LIGHT
REFLECTING
OFF
THE
TRANSPARENT
GLASS
OF
A
PICTURE
FRAME
BLACK
AND
ANANDAN
QC
ELSEVIER
YOU
CAN
CLEARLY
SEE
THE
WOMAN
PORTRAIT
INSIDE
THE
PICTURE
FRAME
SUPERIMPOSED
WITH
THE
REFLECTION
OF
A
MAN
FACE
OFF
THE
GLASS
SMITH
AND
BLINN
HAVE
A
NICE
SURVEY
OF
TRADITIONAL
BLUE
SCREEN
MATTING
TECHNIQUES
WHILE
TOYAMA
KRUMM
BRUMITT
ET
AL
REVIEW
DIFFERENCE
MATTING
MORE
RECENTLY
THERE
HAS
BEEN
A
LOT
OF
ACTIVITY
IN
COMPUTATIONAL
PHOTOGRAPHY
RELATING
TO
NATURAL
IMAGE
MATTING
RUZON
AND
TOMASI
CHUANG
CURLESS
SALESIN
ET
AL
WANG
AND
COHEN
WHICH
ATTEMPTS
TO
EXTRACT
THE
MATTES
FROM
A
SINGLE
NATURAL
IMAGE
FIGURE
OR
FROM
EX
TENDED
VIDEO
SEQUENCES
CHUANG
AGARWALA
CURLESS
ET
AL
ALL
OF
THESE
TECHNIQUES
ARE
DESCRIBED
IN
MORE
DETAIL
IN
SECTION
HISTOGRAM
EQUALIZATION
WHILE
THE
BRIGHTNESS
AND
GAIN
CONTROLS
DESCRIBED
IN
SECTION
CAN
IMPROVE
THE
APPEARANCE
OF
AN
IMAGE
HOW
CAN
WE
AUTOMATICALLY
DETERMINE
THEIR
BEST
VALUES
ONE
APPROACH
MIGHT
BE
TO
LOOK
AT
THE
DARKEST
AND
BRIGHTEST
PIXEL
VALUES
IN
AN
IMAGE
AND
MAP
THEM
TO
PURE
BLACK
AND
PURE
WHITE
ANOTHER
APPROACH
MIGHT
BE
TO
FIND
THE
AVERAGE
VALUE
IN
THE
IMAGE
PUSH
IT
TOWARDS
MIDDLE
GRAY
AND
EXPAND
THE
RANGE
SO
THAT
IT
MORE
CLOSELY
FILLS
THE
DISPLAYABLE
VALUES
KOPF
UYTTENDAELE
DEUSSEN
ET
AL
HOW
CAN
WE
VISUALIZE
THE
SET
OF
LIGHTNESS
VALUES
IN
AN
IMAGE
IN
ORDER
TO
TEST
SOME
OF
THESE
HEURISTICS
THE
ANSWER
IS
TO
PLOT
THE
HISTOGRAM
OF
THE
INDIVIDUAL
COLOR
CHANNELS
AND
LUMINANCE
VALUES
AS
SHOWN
IN
FIGURE
FROM
THIS
DISTRIBUTION
WE
CAN
COMPUTE
RELEVANT
STATISTICS
SUCH
AS
THE
MINIMUM
MAXIMUM
AND
AVERAGE
INTENSITY
VALUES
NOTICE
THAT
THE
IMAGE
IN
FIGURE
HAS
BOTH
AN
EXCESS
OF
DARK
VALUES
AND
LIGHT
VALUES
BUT
THAT
THE
MID
RANGE
VALUES
ARE
LARGELY
UNDER
POPULATED
WOULD
IT
NOT
BE
BETTER
IF
WE
COULD
SIMULTANEOUSLY
BRIGHTEN
SOME
THE
HISTOGRAM
IS
SIMPLY
THE
COUNT
OF
THE
NUMBER
OF
PIXELS
AT
EACH
GRAY
LEVEL
VALUE
FOR
AN
EIGHT
BIT
IMAGE
AN
ACCUMULATION
TABLE
WITH
ENTRIES
IS
NEEDED
FOR
HIGHER
BIT
DEPTHS
A
TABLE
WITH
THE
APPROPRIATE
NUMBER
OF
ENTRIES
PROBABLY
FEWER
THAN
THE
FULL
NUMBER
OF
GRAY
LEVELS
SHOULD
BE
USED
A
B
C
D
E
F
FIGURE
HISTOGRAM
ANALYSIS
AND
EQUALIZATION
A
ORIGINAL
IMAGE
B
COLOR
CHANNEL
AND
IN
TENSITY
LUMINANCE
HISTOGRAMS
C
CUMULATIVE
DISTRIBUTION
FUNCTIONS
D
EQUALIZATION
TRANS
FER
FUNCTIONS
E
FULL
HISTOGRAM
EQUALIZATION
F
PARTIAL
HISTOGRAM
EQUALIZATION
DARK
VALUES
AND
DARKEN
SOME
LIGHT
VALUES
WHILE
STILL
USING
THE
FULL
EXTENT
OF
THE
AVAILABLE
DYNAMIC
RANGE
CAN
YOU
THINK
OF
A
MAPPING
THAT
MIGHT
DO
THIS
ONE
POPULAR
ANSWER
TO
THIS
QUESTION
IS
TO
PERFORM
HISTOGRAM
EQUALIZATION
I
E
TO
FIND
AN
INTENSITY
MAPPING
FUNCTION
F
I
SUCH
THAT
THE
RESULTING
HISTOGRAM
IS
FLAT
THE
TRICK
TO
FINDING
SUCH
A
MAPPING
IS
THE
SAME
ONE
THAT
PEOPLE
USE
TO
GENERATE
RANDOM
SAMPLES
FROM
A
PROBABILITY
DENSITY
FUNCTION
WHICH
IS
TO
FIRST
COMPUTE
THE
CUMULATIVE
DISTRIBUTION
FUNCTION
SHOWN
IN
FIGURE
THINK
OF
THE
ORIGINAL
HISTOGRAM
H
I
AS
THE
DISTRIBUTION
OF
GRADES
IN
A
CLASS
AFTER
SOME
EXAM
HOW
CAN
WE
MAP
A
PARTICULAR
GRADE
TO
ITS
CORRESPONDING
PERCENTILE
SO
THAT
STUDENTS
AT
THE
PERCENTILE
RANGE
SCORED
BETTER
THAN
OF
THEIR
CLASSMATES
THE
ANSWER
IS
TO
INTEGRATE
THE
DISTRIBUTION
H
I
TO
OBTAIN
THE
CUMULATIVE
DISTRIBUTION
C
I
C
I
H
I
C
I
H
I
WHERE
N
IS
THE
NUMBER
OF
PIXELS
IN
THE
IMAGE
OR
STUDENTS
IN
THE
CLASS
FOR
ANY
GIVEN
GRADE
OR
INTENSITY
WE
CAN
LOOK
UP
ITS
CORRESPONDING
PERCENTILE
C
I
AND
DETERMINE
THE
FINAL
VALUE
THAT
PIXEL
SHOULD
TAKE
WHEN
WORKING
WITH
EIGHT
BIT
PIXEL
VALUES
THE
I
AND
C
AXES
ARE
RESCALED
FROM
A
B
C
FIGURE
LOCALLY
ADAPTIVE
HISTOGRAM
EQUALIZATION
A
ORIGINAL
IMAGE
B
BLOCK
HISTOGRAM
EQUALIZATION
C
FULL
LOCALLY
ADAPTIVE
EQUALIZATION
FIGURE
SHOWS
THE
RESULT
OF
APPLYING
F
I
C
I
TO
THE
ORIGINAL
IMAGE
AS
WE
CAN
SEE
THE
RESULTING
HISTOGRAM
IS
FLAT
SO
IS
THE
RESULTING
IMAGE
IT
IS
FLAT
IN
THE
SENSE
OF
A
LACK
OF
CONTRAST
AND
BEING
MUDDY
LOOKING
ONE
WAY
TO
COMPENSATE
FOR
THIS
IS
TO
ONLY
PARTIALLY
COMPENSATE
FOR
THE
HISTOGRAM
UNEVENNESS
E
G
BY
USING
A
MAPPING
FUNCTION
F
I
ΑC
I
Α
I
WHICH
IS
A
LINEAR
BLEND
BETWEEN
THE
CUMULATIVE
DISTRIBUTION
FUNCTION
AND
THE
IDENTITY
TRANSFORM
A
STRAIGHT
LINE
AS
YOU
CAN
SEE
IN
FIGURE
THE
RESULTING
IMAGE
MAINTAINS
MORE
OF
ITS
ORIGINAL
GRAYSCALE
DISTRIBUTION
WHILE
HAVING
A
MORE
APPEALING
BALANCE
ANOTHER
POTENTIAL
PROBLEM
WITH
HISTOGRAM
EQUALIZATION
OR
IN
GENERAL
IMAGE
BRIGHTENING
IS
THAT
NOISE
IN
DARK
REGIONS
CAN
BE
AMPLIFIED
AND
BECOME
MORE
VISIBLE
EXERCISE
SUGGESTS
SOME
POSSIBLE
WAYS
TO
MITIGATE
THIS
AS
WELL
AS
ALTERNATIVE
TECHNIQUES
TO
MAINTAIN
CONTRAST
AND
PUNCH
IN
THE
ORIGINAL
IMAGES
LARSON
RUSHMEIER
AND
PIATKO
STARK
LOCALLY
ADAPTIVE
HISTOGRAM
EQUALIZATION
WHILE
GLOBAL
HISTOGRAM
EQUALIZATION
CAN
BE
USEFUL
FOR
SOME
IMAGES
IT
MIGHT
BE
PREFERABLE
TO
APPLY
DIFFERENT
KINDS
OF
EQUALIZATION
IN
DIFFERENT
REGIONS
CONSIDER
FOR
EXAMPLE
THE
IMAGE
IN
FIGURE
WHICH
HAS
A
WIDE
RANGE
OF
LUMINANCE
VALUES
INSTEAD
OF
COMPUTING
A
SINGLE
CURVE
WHAT
IF
WE
WERE
TO
SUBDIVIDE
THE
IMAGE
INTO
M
M
PIXEL
BLOCKS
AND
PERFORM
SEPARATE
HISTOGRAM
EQUALIZATION
IN
EACH
SUB
BLOCK
AS
YOU
CAN
SEE
IN
FIGURE
THE
RESULTING
IMAGE
EXHIBITS
A
LOT
OF
BLOCKING
ARTIFACTS
I
E
INTENSITY
DISCONTINUITIES
AT
BLOCK
BOUNDARIES
ONE
WAY
TO
ELIMINATE
BLOCKING
ARTIFACTS
IS
TO
USE
A
MOVING
WINDOW
I
E
TO
RECOMPUTE
THE
HISTOGRAM
FOR
EVERY
M
M
BLOCK
CENTERED
AT
EACH
PIXEL
THIS
PROCESS
CAN
BE
QUITE
SLOW
M
OPERATIONS
PER
PIXEL
ALTHOUGH
WITH
CLEVER
PROGRAMMING
ONLY
THE
HISTOGRAM
ENTRIES
CORRESPONDING
TO
THE
PIXELS
ENTERING
AND
LEAVING
THE
BLOCK
IN
A
RASTER
SCAN
ACROSS
THE
IMAGE
NEED
TO
BE
UPDATED
M
OPERATIONS
PER
PIXEL
NOTE
THAT
THIS
OPERATION
IS
AN
EXAMPLE
OF
THE
NON
LINEAR
NEIGHBORHOOD
OPERATIONS
WE
STUDY
IN
MORE
DETAIL
IN
SECTION
A
MORE
EFFICIENT
APPROACH
IS
TO
COMPUTE
NON
OVERLAPPED
BLOCK
BASED
EQUALIZATION
FUNC
TIONS
AS
BEFORE
BUT
TO
THEN
SMOOTHLY
INTERPOLATE
THE
TRANSFER
FUNCTIONS
AS
WE
MOVE
BETWEEN
T
T
A
B
FIGURE
LOCAL
HISTOGRAM
INTERPOLATION
USING
RELATIVE
T
COORDINATES
A
BLOCK
BASED
HISTOGRAMS
WITH
BLOCK
CENTERS
SHOWN
AS
CIRCLES
B
CORNER
BASED
SPLINE
HISTOGRAMS
PIXELS
ARE
LOCATED
ON
GRID
INTERSECTIONS
THE
BLACK
SQUARE
PIXEL
TRANSFER
FUNCTION
IS
INTERPOLATED
FROM
THE
FOUR
ADJACENT
LOOKUP
TABLES
GRAY
ARROWS
USING
THE
COMPUTED
T
VALUES
BLOCK
BOUNDARIES
ARE
SHOWN
AS
DASHED
LINES
BLOCKS
THIS
TECHNIQUE
IS
KNOWN
AS
ADAPTIVE
HISTOGRAM
EQUALIZATION
AHE
AND
ITS
CONTRAST
LIMITED
GAIN
LIMITED
VERSION
IS
KNOWN
AS
CLAHE
PIZER
AMBURN
AUSTIN
ET
AL
THE
WEIGHTING
FUNCTION
FOR
A
GIVEN
PIXEL
I
J
CAN
BE
COMPUTED
AS
A
FUNCTION
OF
ITS
HORIZONTAL
AND
VERTICAL
POSITION
T
WITHIN
A
BLOCK
AS
SHOWN
IN
FIGURE
TO
BLEND
THE
FOUR
LOOKUP
FUNCTIONS
A
BILINEAR
BLENDING
FUNCTION
FS
T
I
T
I
T
I
I
I
CAN
BE
USED
SEE
SECTION
FOR
HIGHER
ORDER
GENERALIZATIONS
OF
SUCH
SPLINE
FUNCTIONS
NOTE
THAT
INSTEAD
OF
BLENDING
THE
FOUR
LOOKUP
TABLES
FOR
EACH
OUTPUT
PIXEL
WHICH
WOULD
BE
QUITE
SLOW
WE
CAN
INSTEAD
BLEND
THE
RESULTS
OF
MAPPING
A
GIVEN
PIXEL
THROUGH
THE
FOUR
NEIGH
BORING
LOOKUPS
A
VARIANT
ON
THIS
ALGORITHM
IS
TO
PLACE
THE
LOOKUP
TABLES
AT
THE
CORNERS
OF
EACH
M
M
BLOCK
SEE
FIGURE
AND
EXERCISE
IN
ADDITION
TO
BLENDING
FOUR
LOOKUPS
TO
COMPUTE
THE
FINAL
VALUE
WE
CAN
ALSO
DISTRIBUTE
EACH
INPUT
PIXEL
INTO
FOUR
ADJACENT
LOOKUP
TABLES
DURING
THE
HISTOGRAM
ACCUMULATION
PHASE
NOTICE
THAT
THE
GRAY
ARROWS
IN
FIGURE
POINT
BOTH
WAYS
I
E
HK
L
I
I
J
W
I
J
K
L
WHERE
W
I
J
K
L
IS
THE
BILINEAR
WEIGHTING
FUNCTION
BETWEEN
PIXEL
I
J
AND
LOOKUP
TABLE
K
L
THIS
IS
AN
EXAMPLE
OF
SOFT
HISTOGRAMMING
WHICH
IS
USED
IN
A
VARIETY
OF
OTHER
APPLICA
ALGORITHM
IS
IMPLEMENTED
IN
THE
MATLAB
ADAPTHIST
FUNCTION
TIONS
INCLUDING
THE
CONSTRUCTION
OF
SIFT
FEATURE
DESCRIPTORS
SECTION
AND
VOCABULARY
TREES
SECTION
APPLICATION
TONAL
ADJUSTMENT
ONE
OF
THE
MOST
WIDELY
USED
APPLICATIONS
OF
POINT
WISE
IMAGE
PROCESSING
OPERATORS
IS
THE
MANIPULATION
OF
CONTRAST
OR
TONE
IN
PHOTOGRAPHS
TO
MAKE
THEM
LOOK
EITHER
MORE
ATTRACTIVE
OR
MORE
INTERPRETABLE
YOU
CAN
GET
A
GOOD
SENSE
OF
THE
RANGE
OF
OPERATIONS
POSSIBLE
BY
OPENING
UP
ANY
PHOTO
MANIPULATION
TOOL
AND
TRYING
OUT
A
VARIETY
OF
CONTRAST
BRIGHTNESS
AND
COLOR
MANIPULATION
OPTIONS
AS
SHOWN
IN
FIGURES
AND
EXERCISES
AND
HAVE
YOU
IMPLEMENT
SOME
OF
THESE
OPERATIONS
IN
ORDER
TO
BECOME
FAMILIAR
WITH
BASIC
IMAGE
PROCESSING
OPERATORS
MORE
SOPHISTICATED
TECHNIQUES
FOR
TONAL
ADJUSTMENT
REINHARD
WARD
PATTANAIK
ET
AL
BAE
PARIS
AND
DURAND
ARE
DESCRIBED
IN
THE
SECTION
ON
HIGH
DYNAMIC
RANGE
TONE
MAPPING
SECTION
LINEAR
FILTERING
LOCALLY
ADAPTIVE
HISTOGRAM
EQUALIZATION
IS
AN
EXAMPLE
OF
A
NEIGHBORHOOD
OPERATOR
OR
LOCAL
OPERATOR
WHICH
USES
A
COLLECTION
OF
PIXEL
VALUES
IN
THE
VICINITY
OF
A
GIVEN
PIXEL
TO
DETER
MINE
ITS
FINAL
OUTPUT
VALUE
FIGURE
IN
ADDITION
TO
PERFORMING
LOCAL
TONE
ADJUSTMENT
NEIGHBORHOOD
OPERATORS
CAN
BE
USED
TO
FILTER
IMAGES
IN
ORDER
TO
ADD
SOFT
BLUR
SHARPEN
DE
TAILS
ACCENTUATE
EDGES
OR
REMOVE
NOISE
FIGURE
D
IN
THIS
SECTION
WE
LOOK
AT
LINEAR
FILTERING
OPERATORS
WHICH
INVOLVE
WEIGHTED
COMBINATIONS
OF
PIXELS
IN
SMALL
NEIGHBORHOODS
IN
SECTION
WE
LOOK
AT
NON
LINEAR
OPERATORS
SUCH
AS
MORPHOLOGICAL
FILTERS
AND
DISTANCE
TRANSFORMS
THE
MOST
COMMONLY
USED
TYPE
OF
NEIGHBORHOOD
OPERATOR
IS
A
LINEAR
FILTER
IN
WHICH
AN
OUTPUT
PIXEL
VALUE
IS
DETERMINED
AS
A
WEIGHTED
SUM
OF
INPUT
PIXEL
VALUES
FIGURE
G
I
J
F
I
K
J
L
H
K
L
K
L
THE
ENTRIES
IN
THE
WEIGHT
KERNEL
OR
MASK
H
K
L
ARE
OFTEN
CALLED
THE
FILTER
COEFFICIENTS
THE
ABOVE
CORRELATION
OPERATOR
CAN
BE
MORE
COMPACTLY
NOTATED
AS
G
F
H
A
COMMON
VARIANT
ON
THIS
FORMULA
IS
G
I
J
F
I
K
J
L
H
K
L
F
K
L
H
I
K
J
L
F
X
Y
H
X
Y
G
X
Y
FIGURE
NEIGHBORHOOD
FILTERING
CONVOLUTION
THE
IMAGE
ON
THE
LEFT
IS
CONVOLVED
WITH
THE
FILTER
IN
THE
MIDDLE
TO
YIELD
THE
IMAGE
ON
THE
RIGHT
THE
LIGHT
BLUE
PIXELS
INDICATE
THE
SOURCE
NEIGHBORHOOD
FOR
THE
LIGHT
GREEN
DESTINATION
PIXEL
WHERE
THE
SIGN
OF
THE
OFFSETS
IN
F
HAS
BEEN
REVERSED
THIS
IS
CALLED
THE
CONVOLUTION
OPERATOR
G
F
H
AND
H
IS
THEN
CALLED
THE
IMPULSE
RESPONSE
FUNCTION
THE
REASON
FOR
THIS
NAME
IS
THAT
THE
KERNEL
FUNCTION
H
CONVOLVED
WITH
AN
IMPULSE
SIGNAL
Δ
I
J
AN
IMAGE
THAT
IS
EVERYWHERE
EXCEPT
AT
THE
ORIGIN
REPRODUCES
ITSELF
H
Δ
H
WHEREAS
CORRELATION
PRODUCES
THE
REFLECTED
SIGNAL
TRY
THIS
YOURSELF
TO
VERIFY
THAT
IT
IS
SO
IN
FACT
EQUATION
CAN
BE
INTERPRETED
AS
THE
SUPERPOSITION
ADDITION
OF
SHIFTED
IM
PULSE
RESPONSE
FUNCTIONS
H
I
K
J
L
MULTIPLIED
BY
THE
INPUT
PIXEL
VALUES
F
K
L
CONVOLU
TION
HAS
ADDITIONAL
NICE
PROPERTIES
E
G
IT
IS
BOTH
COMMUTATIVE
AND
ASSOCIATIVE
AS
WELL
THE
FOURIER
TRANSFORM
OF
TWO
CONVOLVED
IMAGES
IS
THE
PRODUCT
OF
THEIR
INDIVIDUAL
FOURIER
TRANS
FORMS
SECTION
BOTH
CORRELATION
AND
CONVOLUTION
ARE
LINEAR
SHIFT
INVARIANT
LSI
OPERATORS
WHICH
OBEY
BOTH
THE
SUPERPOSITION
PRINCIPLE
H
H
H
AND
THE
SHIFT
INVARIANCE
PRINCIPLE
G
I
J
F
I
K
J
L
H
G
I
J
H
F
I
K
J
L
WHICH
MEANS
THAT
SHIFTING
A
SIGNAL
COMMUTES
WITH
APPLYING
THE
OPERATOR
STANDS
FOR
THE
LSI
OPERATOR
ANOTHER
WAY
TO
THINK
OF
SHIFT
INVARIANCE
IS
THAT
THE
OPERATOR
BEHAVES
THE
SAME
EVERYWHERE
THE
CONTINUOUS
VERSION
OF
CONVOLUTION
CAN
BE
WRITTEN
AS
G
X
F
X
U
H
U
DU
A
B
C
D
E
F
G
H
FIGURE
SOME
NEIGHBORHOOD
OPERATIONS
A
ORIGINAL
IMAGE
B
BLURRED
C
SHARPENED
D
SMOOTHED
WITH
EDGE
PRESERVING
FILTER
E
BINARY
IMAGE
F
DILATED
G
DISTANCE
TRANSFORM
H
CONNECTED
COMPONENTS
FOR
THE
DILATION
AND
CONNECTED
COMPONENTS
BLACK
INK
PIXELS
ARE
ASSUMED
TO
BE
ACTIVE
I
E
TO
HAVE
A
VALUE
OF
IN
EQUATIONS
FIGURE
ONE
DIMENSIONAL
SIGNAL
CONVOLUTION
AS
A
SPARSE
MATRIX
VECTOR
MULTIPLY
G
HF
OCCASIONALLY
A
SHIFT
VARIANT
VERSION
OF
CORRELATION
OR
CONVOLUTION
MAY
BE
USED
E
G
G
I
J
F
I
K
J
L
H
K
L
I
J
K
L
WHERE
H
K
L
I
J
IS
THE
CONVOLUTION
KERNEL
AT
PIXEL
I
J
FOR
EXAMPLE
SUCH
A
SPATIALLY
VARYING
KERNEL
CAN
BE
USED
TO
MODEL
BLUR
IN
AN
IMAGE
DUE
TO
VARIABLE
DEPTH
DEPENDENT
DEFOCUS
CORRELATION
AND
CONVOLUTION
CAN
BOTH
BE
WRITTEN
AS
A
MATRIX
VECTOR
MULTIPLY
IF
WE
FIRST
CONVERT
THE
TWO
DIMENSIONAL
IMAGES
F
I
J
AND
G
I
J
INTO
RASTER
ORDERED
VECTORS
F
AND
G
G
HF
WHERE
THE
SPARSE
H
MATRIX
CONTAINS
THE
CONVOLUTION
KERNELS
FIGURE
SHOWS
HOW
A
ONE
DIMENSIONAL
CONVOLUTION
CAN
BE
REPRESENTED
IN
MATRIX
VECTOR
FORM
PADDING
BORDER
EFFECTS
THE
ASTUTE
READER
WILL
NOTICE
THAT
THE
MATRIX
MULTIPLY
SHOWN
IN
FIGURE
SUFFERS
FROM
BOUNDARY
EFFECTS
I
E
THE
RESULTS
OF
FILTERING
THE
IMAGE
IN
THIS
FORM
WILL
LEAD
TO
A
DARKENING
OF
THE
CORNER
PIXELS
THIS
IS
BECAUSE
THE
ORIGINAL
IMAGE
IS
EFFECTIVELY
BEING
PADDED
WITH
VALUES
WHEREVER
THE
CONVOLUTION
KERNEL
EXTENDS
BEYOND
THE
ORIGINAL
IMAGE
BOUNDARIES
TO
COMPENSATE
FOR
THIS
A
NUMBER
OF
ALTERNATIVE
PADDING
OR
EXTENSION
MODES
HAVE
BEEN
DEVELOPED
FIGURE
ZERO
SET
ALL
PIXELS
OUTSIDE
THE
SOURCE
IMAGE
TO
A
GOOD
CHOICE
FOR
ALPHA
MATTED
CUTOUT
IMAGES
CONSTANT
BORDER
COLOR
SET
ALL
PIXELS
OUTSIDE
THE
SOURCE
IMAGE
TO
A
SPECIFIED
BORDER
VALUE
CLAMP
REPLICATE
OR
CLAMP
TO
EDGE
REPEAT
EDGE
PIXELS
INDEFINITELY
CYCLIC
WRAP
REPEAT
OR
TILE
LOOP
AROUND
THE
IMAGE
IN
A
TOROIDAL
CONFIGURATION
ZERO
WRAP
CLAMP
MIRROR
BLURRED
ZERO
NORMALIZED
ZERO
BLURRED
CLAMP
BLURRED
MIRROR
FIGURE
BORDER
PADDING
TOP
ROW
AND
THE
RESULTS
OF
BLURRING
THE
PADDED
IMAGE
BOTTOM
ROW
THE
NORMALIZED
ZERO
IMAGE
IS
THE
RESULT
OF
DIVIDING
NORMALIZING
THE
BLURRED
ZERO
PADDED
RGBA
IMAGE
BY
ITS
CORRESPONDING
SOFT
ALPHA
VALUE
MIRROR
REFLECT
PIXELS
ACROSS
THE
IMAGE
EDGE
EXTEND
EXTEND
THE
SIGNAL
BY
SUBTRACTING
THE
MIRRORED
VERSION
OF
THE
SIGNAL
FROM
THE
EDGE
PIXEL
VALUE
IN
THE
COMPUTER
GRAPHICS
LITERATURE
AKENINE
MO
LLER
AND
HAINES
P
THESE
MECH
ANISMS
ARE
KNOWN
AS
THE
WRAPPING
MODE
OPENGL
OR
TEXTURE
ADDRESSING
MODE
THE
FORMULAS
FOR
EACH
OF
THESE
MODES
ARE
LEFT
TO
THE
READER
EXERCISE
FIGURE
SHOWS
THE
EFFECTS
OF
PADDING
AN
IMAGE
WITH
EACH
OF
THE
ABOVE
MECHANISMS
AND
THEN
BLURRING
THE
RESULTING
PADDED
IMAGE
AS
YOU
CAN
SEE
ZERO
PADDING
DARKENS
THE
EDGES
CLAMP
REPLICATION
PADDING
PROPAGATES
BORDER
VALUES
INWARD
MIRROR
REFLECTION
PADDING
PRE
SERVES
COLORS
NEAR
THE
BORDERS
EXTENSION
PADDING
NOT
SHOWN
KEEPS
THE
BORDER
PIXELS
FIXED
DURING
BLUR
AN
ALTERNATIVE
TO
PADDING
IS
TO
BLUR
THE
ZERO
PADDED
RGBA
IMAGE
AND
TO
THEN
DIVIDE
THE
RESULTING
IMAGE
BY
ITS
ALPHA
VALUE
TO
REMOVE
THE
DARKENING
EFFECT
THE
RESULTS
CAN
BE
QUITE
GOOD
AS
SEEN
IN
THE
NORMALIZED
ZERO
IMAGE
IN
FIGURE
SEPARABLE
FILTERING
THE
PROCESS
OF
PERFORMING
A
CONVOLUTION
REQUIRES
MULTIPLY
ADD
OPERATIONS
PER
PIXEL
WHERE
K
IS
THE
SIZE
WIDTH
OR
HEIGHT
OF
THE
CONVOLUTION
KERNEL
E
G
THE
BOX
FILTER
IN
FIG
K
A
BOX
K
B
BILINEAR
C
GAUSSIAN
D
SOBEL
E
CORNER
FIGURE
SEPARABLE
LINEAR
FILTERS
FOR
EACH
IMAGE
A
E
WE
SHOW
THE
FILTER
KERNEL
TOP
THE
CORRESPONDING
HORIZONTAL
KERNEL
MIDDLE
AND
THE
FILTERED
IMAGE
BOTTOM
THE
FILTERED
SOBEL
AND
CORNER
IMAGES
ARE
SIGNED
SCALED
UP
BY
AND
RESPECTIVELY
AND
ADDED
TO
A
GRAY
OFFSET
BEFORE
DISPLAY
URE
IN
MANY
CASES
THIS
OPERATION
CAN
BE
SIGNIFICANTLY
SPED
UP
BY
FIRST
PERFORMING
A
ONE
DIMENSIONAL
HORIZONTAL
CONVOLUTION
FOLLOWED
BY
A
ONE
DIMENSIONAL
VERTICAL
CONVOLUTION
WHICH
REQUIRES
A
TOTAL
OF
OPERATIONS
PER
PIXEL
A
CONVOLUTION
KERNEL
FOR
WHICH
THIS
IS
POSSIBLE
IS
SAID
TO
BE
SEPARABLE
IT
IS
EASY
TO
SHOW
THAT
THE
TWO
DIMENSIONAL
KERNEL
K
CORRESPONDING
TO
SUCCESSIVE
CON
VOLUTION
WITH
A
HORIZONTAL
KERNEL
H
AND
A
VERTICAL
KERNEL
V
IS
THE
OUTER
PRODUCT
OF
THE
TWO
KERNELS
K
VHT
SEE
FIGURE
FOR
SOME
EXAMPLES
BECAUSE
OF
THE
INCREASED
EFFICIENCY
THE
DESIGN
OF
CONVOLUTION
KERNELS
FOR
COMPUTER
VISION
APPLICATIONS
IS
OFTEN
INFLUENCED
BY
THEIR
SEPARABILITY
HOW
CAN
WE
TELL
IF
A
GIVEN
KERNEL
K
IS
INDEED
SEPARABLE
THIS
CAN
OFTEN
BE
DONE
BY
INSPECTION
OR
BY
LOOKING
AT
THE
ANALYTIC
FORM
OF
THE
KERNEL
FREEMAN
AND
ADELSON
A
MORE
DIRECT
METHOD
IS
TO
TREAT
THE
KERNEL
AS
A
MATRIX
K
AND
TO
TAKE
ITS
SINGULAR
VALUE
DECOMPOSITION
SVD
K
ΣIUIVT
I
SEE
APPENDIX
A
FOR
THE
DEFINITION
OF
THE
SVD
IF
ONLY
THE
FIRST
SINGULAR
VALUE
IS
NON
ZERO
THE
KERNEL
IS
SEPARABLE
AND
AND
PROVIDE
THE
VERTICAL
AND
HORIZONTAL
KERNELS
PERONA
FOR
EXAMPLE
THE
LAPLACIAN
OF
GAUSSIAN
KERNEL
AND
CAN
BE
IMPLEMENTED
AS
THE
SUM
OF
TWO
SEPARABLE
FILTERS
WIEJAK
BUXTON
AND
BUXTON
WHAT
IF
YOUR
KERNEL
IS
NOT
SEPARABLE
AND
YET
YOU
STILL
WANT
A
FASTER
WAY
TO
IMPLEMENT
IT
PERONA
WHO
FIRST
MADE
THE
LINK
BETWEEN
KERNEL
SEPARABILITY
AND
SVD
SUGGESTS
USING
MORE
TERMS
IN
THE
SERIES
I
E
SUMMING
UP
A
NUMBER
OF
SEPARABLE
CONVOLUTIONS
WHETHER
THIS
IS
WORTH
DOING
OR
NOT
DEPENDS
ON
THE
RELATIVE
SIZES
OF
K
AND
THE
NUMBER
OF
SIG
NIFICANT
SINGULAR
VALUES
AS
WELL
AS
OTHER
CONSIDERATIONS
SUCH
AS
CACHE
COHERENCY
AND
MEMORY
LOCALITY
EXAMPLES
OF
LINEAR
FILTERING
NOW
THAT
WE
HAVE
DESCRIBED
THE
PROCESS
FOR
PERFORMING
LINEAR
FILTERING
LET
US
EXAMINE
A
NUMBER
OF
FREQUENTLY
USED
FILTERS
THE
SIMPLEST
FILTER
TO
IMPLEMENT
IS
THE
MOVING
AVERAGE
OR
BOX
FILTER
WHICH
SIMPLY
AVERAGES
THE
PIXEL
VALUES
IN
A
K
K
WINDOW
THIS
IS
EQUIVALENT
TO
CONVOLVING
THE
IMAGE
WITH
A
KERNEL
OF
ALL
ONES
AND
THEN
SCALING
FIGURE
FOR
LARGE
KERNELS
A
MORE
EFFICIENT
IMPLEMENTATION
IS
TO
SLIDE
A
MOVING
WINDOW
ACROSS
EACH
SCANLINE
IN
A
SEPARABLE
FILTER
WHILE
ADDING
THE
NEWEST
PIXEL
AND
SUBTRACTING
THE
OLDEST
PIXEL
FROM
THE
RUNNING
SUM
THIS
IS
RELATED
TO
THE
CONCEPT
OF
SUMMED
AREA
TABLES
WHICH
WE
DESCRIBE
SHORTLY
A
SMOOTHER
IMAGE
CAN
BE
OBTAINED
BY
SEPARABLY
CONVOLVING
THE
IMAGE
WITH
A
PIECEWISE
LINEAR
TENT
FUNCTION
ALSO
KNOWN
AS
A
BARTLETT
FILTER
FIGURE
SHOWS
A
VERSION
OF
THIS
FILTER
WHICH
IS
CALLED
THE
BILINEAR
KERNEL
SINCE
IT
IS
THE
OUTER
PRODUCT
OF
TWO
LINEAR
FIRST
ORDER
SPLINES
SEE
SECTION
CONVOLVING
THE
LINEAR
TENT
FUNCTION
WITH
ITSELF
YIELDS
THE
CUBIC
APPROXIMATING
SPLINE
WHICH
IS
CALLED
THE
GAUSSIAN
KERNEL
FIGURE
IN
BURT
AND
ADELSON
LAPLA
CIAN
PYRAMID
REPRESENTATION
SECTION
NOTE
THAT
APPROXIMATE
GAUSSIAN
KERNELS
CAN
ALSO
BE
OBTAINED
BY
ITERATED
CONVOLUTION
WITH
BOX
FILTERS
WELLS
IN
APPLICATIONS
WHERE
THE
FILTERS
REALLY
NEED
TO
BE
ROTATIONALLY
SYMMETRIC
CAREFULLY
TUNED
VERSIONS
OF
SAMPLED
GAUSSIANS
SHOULD
BE
USED
FREEMAN
AND
ADELSON
EXERCISE
THE
KERNELS
WE
JUST
DISCUSSED
ARE
ALL
EXAMPLES
OF
BLURRING
SMOOTHING
OR
LOW
PASS
KER
NELS
SINCE
THEY
PASS
THROUGH
THE
LOWER
FREQUENCIES
WHILE
ATTENUATING
HIGHER
FREQUENCIES
HOW
GOOD
ARE
THEY
AT
DOING
THIS
IN
SECTION
WE
USE
FREQUENCY
SPACE
FOURIER
ANALYSIS
TO
EXAMINE
THE
EXACT
FREQUENCY
RESPONSE
OF
THESE
FILTERS
WE
ALSO
INTRODUCE
THE
SINC
SIN
X
X
FILTER
WHICH
PERFORMS
IDEAL
LOW
PASS
FILTERING
IN
PRACTICE
SMOOTHING
KERNELS
ARE
OFTEN
USED
TO
REDUCE
HIGH
FREQUENCY
NOISE
WE
HAVE
MUCH
MORE
TO
SAY
ABOUT
USING
VARIANTS
ON
SMOOTHING
TO
REMOVE
NOISE
LATER
SEE
SECTIONS
AND
SURPRISINGLY
SMOOTHING
KERNELS
CAN
ALSO
BE
USED
TO
SHARPEN
IMAGES
USING
A
PROCESS
CALLED
UNSHARP
MASKING
SINCE
BLURRING
THE
IMAGE
REDUCES
HIGH
FREQUENCIES
ADDING
SOME
OF
THE
DIFFERENCE
BETWEEN
THE
ORIGINAL
AND
THE
BLURRED
IMAGE
MAKES
IT
SHARPER
GSHARP
F
Γ
F
HBLUR
F
IN
FACT
BEFORE
THE
ADVENT
OF
DIGITAL
PHOTOGRAPHY
THIS
WAS
THE
STANDARD
WAY
TO
SHARPEN
IMAGES
IN
THE
DARKROOM
CREATE
A
BLURRED
POSITIVE
NEGATIVE
FROM
THE
ORIGINAL
NEGATIVE
BY
MIS
FOCUSING
THEN
OVERLAY
THE
TWO
NEGATIVES
BEFORE
PRINTING
THE
FINAL
IMAGE
WHICH
CORRESPONDS
TO
GUNSHARP
F
ΓHBLUR
F
THIS
IS
NO
LONGER
A
LINEAR
FILTER
BUT
IT
STILL
WORKS
WELL
LINEAR
FILTERING
CAN
ALSO
BE
USED
AS
A
PRE
PROCESSING
STAGE
TO
EDGE
EXTRACTION
SECTION
AND
INTEREST
POINT
DETECTION
SECTION
ALGORITHMS
FIGURE
SHOWS
A
SIMPLE
EDGE
EXTRACTOR
CALLED
THE
SOBEL
OPERATOR
WHICH
IS
A
SEPARABLE
COMBINATION
OF
A
HORIZONTAL
CENTRAL
DIFFERENCE
SO
CALLED
BECAUSE
THE
HORIZONTAL
DERIVATIVE
IS
CENTERED
ON
THE
PIXEL
AND
A
VERTICAL
TENT
FILTER
TO
SMOOTH
THE
RESULTS
AS
YOU
CAN
SEE
IN
THE
IMAGE
BELOW
THE
KERNEL
THIS
FILTER
EFFECTIVELY
EMPHASIZES
HORIZONTAL
EDGES
THE
SIMPLE
CORNER
DETECTOR
FIGURE
LOOKS
FOR
SIMULTANEOUS
HORIZONTAL
AND
VERTICAL
SECOND
DERIVATIVES
AS
YOU
CAN
SEE
HOWEVER
IT
RESPONDS
NOT
ONLY
TO
THE
CORNERS
OF
THE
SQUARE
BUT
ALSO
ALONG
DIAGONAL
EDGES
BETTER
CORNER
DETECTORS
OR
AT
LEAST
INTEREST
POINT
DETECTORS
THAT
ARE
MORE
ROTATIONALLY
INVARIANT
ARE
DESCRIBED
IN
SECTION
BAND
PASS
AND
STEERABLE
FILTERS
THE
SOBEL
AND
CORNER
OPERATORS
ARE
SIMPLE
EXAMPLES
OF
BAND
PASS
AND
ORIENTED
FILTERS
MORE
SOPHISTICATED
KERNELS
CAN
BE
CREATED
BY
FIRST
SMOOTHING
THE
IMAGE
WITH
A
UNIT
AREA
GAUSSIAN
FILTER
G
X
Y
Σ
E
AND
THEN
TAKING
THE
FIRST
OR
SECOND
DERIVATIVES
MARR
WITKIN
FREEMAN
AND
ADELSON
SUCH
FILTERS
ARE
KNOWN
COLLECTIVELY
AS
BAND
PASS
FILTERS
SINCE
THEY
FILTER
OUT
BOTH
LOW
AND
HIGH
FREQUENCIES
THE
UNDIRECTED
SECOND
DERIVATIVE
OF
A
TWO
DIMENSIONAL
IMAGE
F
IS
KNOWN
AS
THE
LAPLACIAN
OPERATOR
BLURRING
AN
IMAGE
WITH
A
GAUSSIAN
AND
THEN
TAKING
ITS
LAPLACIAN
IS
EQUIVALENT
TO
CONVOLVING
DIRECTLY
WITH
THE
LAPLACIAN
OF
GAUSSIAN
LOG
FILTER
A
B
C
FIGURE
SECOND
ORDER
STEERABLE
FILTER
FREEMAN
QC
IEEE
A
ORIGINAL
IMAGE
OF
EINSTEIN
B
ORIENTATION
MAP
COMPUTED
FROM
THE
SECOND
ORDER
ORIENTED
ENERGY
C
ORIGINAL
IMAGE
WITH
ORIENTED
STRUCTURES
ENHANCED
WHICH
HAS
CERTAIN
NICE
SCALE
SPACE
PROPERTIES
WITKIN
WITKIN
TERZOPOULOS
AND
KASS
THE
FIVE
POINT
LAPLACIAN
IS
JUST
A
COMPACT
APPROXIMATION
TO
THIS
MORE
SOPHISTICATED
FILTER
LIKEWISE
THE
SOBEL
OPERATOR
IS
A
SIMPLE
APPROXIMATION
TO
A
DIRECTIONAL
OR
ORIENTED
FILTER
WHICH
CAN
OBTAINED
BY
SMOOTHING
WITH
A
GAUSSIAN
OR
SOME
OTHER
FILTER
AND
THEN
TAKING
A
DIRECTIONAL
DERIVATIVE
Uˆ
WHICH
IS
OBTAINED
BY
TAKING
THE
DOT
PRODUCT
BETWEEN
THE
GRADIENT
FIELD
AND
A
UNIT
DIRECTION
Uˆ
COS
Θ
SIN
Θ
Uˆ
G
F
Uˆ
G
F
UˆG
F
THE
SMOOTHED
DIRECTIONAL
DERIVATIVE
FILTER
G
G
GUˆ
UGX
VGY
U
X
V
Y
WHERE
Uˆ
U
V
IS
AN
EXAMPLE
OF
A
STEERABLE
FILTER
SINCE
THE
VALUE
OF
AN
IMAGE
CONVOLVED
WITH
GUˆ
CAN
BE
COMPUTED
BY
FIRST
CONVOLVING
WITH
THE
PAIR
OF
FILTERS
GX
GY
AND
THEN
STEERING
THE
FILTER
POTENTIALLY
LOCALLY
BY
MULTIPLYING
THIS
GRADIENT
FIELD
WITH
A
UNIT
VECTOR
Uˆ
FREEMAN
AND
ADELSON
THE
ADVANTAGE
OF
THIS
APPROACH
IS
THAT
A
WHOLE
FAMILY
OF
FILTERS
CAN
BE
EVALUATED
WITH
VERY
LITTLE
COST
HOW
ABOUT
STEERING
A
DIRECTIONAL
SECOND
DERIVATIVE
FILTER
Uˆ
UˆGUˆ
WHICH
IS
THE
RESULT
OF
TAKING
A
SMOOTHED
DIRECTIONAL
DERIVATIVE
AND
THEN
TAKING
THE
DIRECTIONAL
DERIVATIVE
AGAIN
FOR
EXAMPLE
GXX
IS
THE
SECOND
DIRECTIONAL
DERIVATIVE
IN
THE
X
DIRECTION
AT
FIRST
GLANCE
IT
WOULD
APPEAR
THAT
THE
STEERING
TRICK
WILL
NOT
WORK
SINCE
FOR
EVERY
DI
RECTION
Uˆ
WE
NEED
TO
COMPUTE
A
DIFFERENT
FIRST
DIRECTIONAL
DERIVATIVE
SOMEWHAT
SURPRISINGLY
FREEMAN
AND
ADELSON
SHOWED
THAT
FOR
DIRECTIONAL
GAUSSIAN
DERIVATIVES
IT
IS
POSSIBLE
A
B
C
D
FIGURE
FOURTH
ORDER
STEERABLE
FILTER
FREEMAN
AND
ADELSON
QC
IEEE
A
TEST
IMAGE
CONTAINING
BARS
LINES
AND
STEP
EDGES
AT
DIFFERENT
ORIENTATIONS
B
AVERAGE
ORIENTED
ENERGY
C
DOMINANT
ORIENTATION
D
ORIENTED
ENERGY
AS
A
FUNCTION
OF
ANGLE
POLAR
PLOT
TO
STEER
ANY
ORDER
OF
DERIVATIVE
WITH
A
RELATIVELY
SMALL
NUMBER
OF
BASIS
FUNCTIONS
FOR
EXAMPLE
ONLY
THREE
BASIS
FUNCTIONS
ARE
REQUIRED
FOR
THE
SECOND
ORDER
DIRECTIONAL
DERIVATIVE
GUˆUˆ
FURTHERMORE
EACH
OF
THE
BASIS
FILTERS
WHILE
NOT
ITSELF
NECESSARILY
SEPARABLE
CAN
BE
COMPUTED
USING
A
LINEAR
COMBINATION
OF
A
SMALL
NUMBER
OF
SEPARABLE
FILTERS
FREEMAN
AND
ADELSON
THIS
REMARKABLE
RESULT
MAKES
IT
POSSIBLE
TO
CONSTRUCT
DIRECTIONAL
DERIVATIVE
FILTERS
OF
IN
CREASINGLY
GREATER
DIRECTIONAL
SELECTIVITY
I
E
FILTERS
THAT
ONLY
RESPOND
TO
EDGES
THAT
HAVE
STRONG
LOCAL
CONSISTENCY
IN
ORIENTATION
FIGURE
FURTHERMORE
HIGHER
ORDER
STEERABLE
FILTERS
CAN
RESPOND
TO
POTENTIALLY
MORE
THAN
A
SINGLE
EDGE
ORIENTATION
AT
A
GIVEN
LOCATION
AND
THEY
CAN
RESPOND
TO
BOTH
BAR
EDGES
THIN
LINES
AND
THE
CLASSIC
STEP
EDGES
FIGURE
IN
ORDER
TO
DO
THIS
HOWEVER
FULL
HILBERT
TRANSFORM
PAIRS
NEED
TO
BE
USED
FOR
SECOND
ORDER
AND
HIGHER
FILTERS
AS
DESCRIBED
IN
FREEMAN
AND
ADELSON
STEERABLE
FILTERS
ARE
OFTEN
USED
TO
CONSTRUCT
BOTH
FEATURE
DESCRIPTORS
SECTION
AND
EDGE
DETECTORS
SECTION
WHILE
THE
FILTERS
DEVELOPED
BY
FREEMAN
AND
ADELSON
ARE
BEST
SUITED
FOR
DETECTING
LINEAR
EDGE
LIKE
STRUCTURES
MORE
RECENT
WORK
BY
KOETHE
SHOWS
HOW
A
COMBINED
BOUNDARY
TENSOR
CAN
BE
USED
TO
ENCODE
BOTH
EDGE
AND
JUNCTION
CORNER
FEATURES
EXERCISE
HAS
YOU
IMPLEMENT
SUCH
STEERABLE
FILTERS
AND
APPLY
THEM
TO
FINDING
BOTH
EDGE
AND
CORNER
FEATURES
SUMMED
AREA
TABLE
INTEGRAL
IMAGE
IF
AN
IMAGE
IS
GOING
TO
BE
REPEATEDLY
CONVOLVED
WITH
DIFFERENT
BOX
FILTERS
AND
ESPECIALLY
FILTERS
OF
DIFFERENT
SIZES
AT
DIFFERENT
LOCATIONS
YOU
CAN
PRECOMPUTE
THE
SUMMED
AREA
TABLE
CROW
A
B
C
FIGURE
SUMMED
AREA
TABLES
A
ORIGINAL
IMAGE
B
SUMMED
AREA
TABLE
C
COMPUTATION
OF
AREA
SUM
EACH
VALUE
IN
THE
SUMMED
AREA
TABLE
I
J
RED
IS
COMPUTED
RECURSIVELY
FROM
ITS
THREE
ADJACENT
BLUE
NEIGHBORS
AREA
SUMS
GREEN
ARE
COMPUTED
BY
COMBINING
THE
FOUR
VALUES
AT
THE
RECTANGLE
CORNERS
PURPLE
POSITIVE
VALUES
ARE
SHOWN
IN
BOLD
AND
NEGATIVE
VALUES
IN
ITALICS
WHICH
IS
JUST
THE
RUNNING
SUM
OF
ALL
THE
PIXEL
VALUES
FROM
THE
ORIGIN
I
J
I
J
F
K
L
K
L
THIS
CAN
BE
EFFICIENTLY
COMPUTED
USING
A
RECURSIVE
RASTER
SCAN
ALGORITHM
I
J
I
J
I
J
I
J
F
I
J
THE
IMAGE
I
J
IS
ALSO
OFTEN
CALLED
AN
INTEGRAL
IMAGE
SEE
FIGURE
AND
CAN
ACTUALLY
BE
COMPUTED
USING
ONLY
TWO
ADDITIONS
PER
PIXEL
IF
SEPARATE
ROW
SUMS
ARE
USED
VIOLA
AND
JONES
TO
FIND
THE
SUMMED
AREA
INTEGRAL
INSIDE
A
RECTANGLE
WE
SIMPLY
COMBINE
FOUR
SAMPLES
FROM
THE
SUMMED
AREA
TABLE
I
J
A
POTENTIAL
DISADVANTAGE
OF
SUMMED
AREA
TABLES
IS
THAT
THEY
REQUIRE
LOG
M
LOG
N
EXTRA
BITS
IN
THE
ACCUMULATION
IMAGE
COMPARED
TO
THE
ORIGINAL
IMAGE
WHERE
M
AND
N
ARE
THE
IMAGE
WIDTH
AND
HEIGHT
EXTENSIONS
OF
SUMMED
AREA
TABLES
CAN
ALSO
BE
USED
TO
APPROXIMATE
OTHER
CONVOLUTION
KERNELS
WOLBERG
SECTION
CONTAINS
A
REVIEW
IN
COMPUTER
VISION
SUMMED
AREA
TABLES
HAVE
BEEN
USED
IN
FACE
DETECTION
VIOLA
AND
JONES
TO
COMPUTE
SIMPLE
MULTI
SCALE
LOW
LEVEL
FEATURES
SUCH
FEATURES
WHICH
CONSIST
OF
ADJACENT
RECTANGLES
OF
POSITIVE
AND
NEGATIVE
VALUES
ARE
ALSO
KNOWN
AS
BOXLETS
SIMARD
BOTTOU
HAFFNER
ET
AL
IN
PRINCIPLE
SUMMED
AREA
TABLES
COULD
ALSO
BE
USED
TO
COMPUTE
THE
SUMS
IN
THE
SUM
OF
SQUARED
DIFFERENCES
SSD
STEREO
AND
MOTION
ALGORITHMS
SECTION
IN
PRACTICE
SEPARABLE
MOVING
AVERAGE
FILTERS
ARE
USUALLY
PREFERRED
KANADE
YOSHIDA
ODA
ET
AL
UNLESS
MANY
DIFFERENT
WINDOW
SHAPES
AND
SIZES
ARE
BEING
CONSIDERED
VEKSLER
RECURSIVE
FILTERING
THE
INCREMENTAL
FORMULA
FOR
THE
SUMMED
AREA
IS
AN
EXAMPLE
OF
A
RECURSIVE
FILTER
I
E
ONE
WHOSE
VALUES
DEPENDS
ON
PREVIOUS
FILTER
OUTPUTS
IN
THE
SIGNAL
PROCESSING
LITERATURE
SUCH
FILTERS
ARE
KNOWN
AS
INFINITE
IMPULSE
RESPONSE
IIR
SINCE
THE
OUTPUT
OF
THE
FILTER
TO
AN
IMPULSE
SINGLE
NON
ZERO
VALUE
GOES
ON
FOREVER
FOR
EXAMPLE
FOR
A
SUMMED
AREA
TABLE
AN
IMPULSE
GENERATES
AN
INFINITE
RECTANGLE
OF
BELOW
AND
TO
THE
RIGHT
OF
THE
IMPULSE
THE
FILTERS
WE
HAVE
PREVIOUSLY
STUDIED
IN
THIS
CHAPTER
WHICH
INVOLVE
THE
IMAGE
WITH
A
FINITE
EXTENT
KERNEL
ARE
KNOWN
AS
FINITE
IMPULSE
RESPONSE
FIR
TWO
DIMENSIONAL
IIR
FILTERS
AND
RECURSIVE
FORMULAS
ARE
SOMETIMES
USED
TO
COMPUTE
QUAN
TITIES
THAT
INVOLVE
LARGE
AREA
INTERACTIONS
SUCH
AS
TWO
DIMENSIONAL
DISTANCE
FUNCTIONS
SEC
TION
AND
CONNECTED
COMPONENTS
SECTION
MORE
COMMONLY
HOWEVER
IIR
FILTERS
ARE
USED
INSIDE
ONE
DIMENSIONAL
SEPARABLE
FILTERING
STAGES
TO
COMPUTE
LARGE
EXTENT
SMOOTHING
KERNELS
SUCH
AS
EFFICIENT
APPROXIMATIONS
TO
GAUS
SIANS
AND
EDGE
FILTERS
DERICHE
NIELSEN
FLORACK
AND
DERICHE
PYRAMID
BASED
ALGORITHMS
SECTION
CAN
ALSO
BE
USED
TO
PERFORM
SUCH
LARGE
AREA
SMOOTHING
COMPUTATIONS
MORE
NEIGHBORHOOD
OPERATORS
AS
WE
HAVE
JUST
SEEN
LINEAR
FILTERS
CAN
PERFORM
A
WIDE
VARIETY
OF
IMAGE
TRANSFORMATIONS
HOWEVER
NON
LINEAR
FILTERS
SUCH
AS
EDGE
PRESERVING
MEDIAN
OR
BILATERAL
FILTERS
CAN
SOMETIMES
PERFORM
EVEN
BETTER
OTHER
EXAMPLES
OF
NEIGHBORHOOD
OPERATORS
INCLUDE
MORPHOLOGICAL
OPER
ATORS
THAT
OPERATE
ON
BINARY
IMAGES
AS
WELL
AS
SEMI
GLOBAL
OPERATORS
THAT
COMPUTE
DISTANCE
TRANSFORMS
AND
FIND
CONNECTED
COMPONENTS
IN
BINARY
IMAGES
FIGURE
H
NON
LINEAR
FILTERING
THE
FILTERS
WE
HAVE
LOOKED
AT
SO
FAR
HAVE
ALL
BEEN
LINEAR
I
E
THEIR
RESPONSE
TO
A
SUM
OF
TWO
SIGNALS
IS
THE
SAME
AS
THE
SUM
OF
THE
INDIVIDUAL
RESPONSES
THIS
IS
EQUIVALENT
TO
SAYING
THAT
EACH
OUTPUT
PIXEL
IS
A
WEIGHTED
SUMMATION
OF
SOME
NUMBER
OF
INPUT
PIXELS
LINEAR
FILTERS
ARE
EASIER
TO
COMPOSE
AND
ARE
AMENABLE
TO
FREQUENCY
RESPONSE
ANALYSIS
SECTION
IN
MANY
CASES
HOWEVER
BETTER
PERFORMANCE
CAN
BE
OBTAINED
BY
USING
A
NON
LINEAR
COM
BINATION
OF
NEIGHBORING
PIXELS
CONSIDER
FOR
EXAMPLE
THE
IMAGE
IN
FIGURE
WHERE
THE
A
B
C
D
E
F
G
H
FIGURE
MEDIAN
AND
BILATERAL
FILTERING
A
ORIGINAL
IMAGE
WITH
GAUSSIAN
NOISE
B
GAUS
SIAN
FILTERED
C
MEDIAN
FILTERED
D
BILATERALLY
FILTERED
E
ORIGINAL
IMAGE
WITH
SHOT
NOISE
F
GAUSSIAN
FILTERED
G
MEDIAN
FILTERED
H
BILATERALLY
FILTERED
NOTE
THAT
THE
BILATERAL
FILTER
FAILS
TO
REMOVE
THE
SHOT
NOISE
BECAUSE
THE
NOISY
PIXELS
ARE
TOO
DIFFERENT
FROM
THEIR
NEIGHBORS
A
MEDIAN
B
Α
MEAN
C
DOMAIN
FILTER
D
RANGE
FILTER
FIGURE
MEDIAN
AND
BILATERAL
FILTERING
A
MEDIAN
PIXEL
GREEN
B
SELECTED
Α
TRIMMED
MEAN
PIXELS
C
DOMAIN
FILTER
NUMBERS
ALONG
EDGE
ARE
PIXEL
DISTANCES
D
RANGE
FILTER
NOISE
RATHER
THAN
BEING
GAUSSIAN
IS
SHOT
NOISE
I
E
IT
OCCASIONALLY
HAS
VERY
LARGE
VALUES
IN
THIS
CASE
REGULAR
BLURRING
WITH
A
GAUSSIAN
FILTER
FAILS
TO
REMOVE
THE
NOISY
PIXELS
AND
INSTEAD
TURNS
THEM
INTO
SOFTER
BUT
STILL
VISIBLE
SPOTS
FIGURE
MEDIAN
FILTERING
A
BETTER
FILTER
TO
USE
IN
THIS
CASE
IS
THE
MEDIAN
FILTER
WHICH
SELECTS
THE
MEDIAN
VALUE
FROM
EACH
PIXEL
NEIGHBORHOOD
FIGURE
MEDIAN
VALUES
CAN
BE
COMPUTED
IN
EXPECTED
LINEAR
TIME
USING
A
RANDOMIZED
SELECT
ALGORITHM
CORMEN
AND
INCREMENTAL
VARIANTS
HAVE
ALSO
BEEN
DEVELOPED
BY
TOMASI
AND
MANDUCHI
AND
BOVIK
SECTION
SINCE
THE
SHOT
NOISE
VALUE
USUALLY
LIES
WELL
OUTSIDE
THE
TRUE
VALUES
IN
THE
NEIGHBORHOOD
THE
MEDIAN
FILTER
IS
ABLE
TO
FILTER
AWAY
SUCH
BAD
PIXELS
FIGURE
ONE
DOWNSIDE
OF
THE
MEDIAN
FILTER
IN
ADDITION
TO
ITS
MODERATE
COMPUTATIONAL
COST
IS
THAT
SINCE
IT
SELECTS
ONLY
ONE
INPUT
PIXEL
VALUE
TO
REPLACE
EACH
OUTPUT
PIXEL
IT
IS
NOT
AS
EFFICIENT
AT
AVERAGING
AWAY
REGULAR
GAUSSIAN
NOISE
HUBER
HAMPEL
RONCHETTI
ROUSSEEUW
ET
AL
STEWART
A
BETTER
CHOICE
MAY
BE
THE
Α
TRIMMED
MEAN
LEE
AND
REDNER
CRANE
P
WHICH
AVERAGES
TOGETHER
ALL
OF
THE
PIXELS
EXCEPT
FOR
THE
Α
FRACTION
THAT
ARE
THE
SMALLEST
AND
THE
LARGEST
FIGURE
ANOTHER
POSSIBILITY
IS
TO
COMPUTE
A
WEIGHTED
MEDIAN
IN
WHICH
EACH
PIXEL
IS
USED
A
NUM
BER
OF
TIMES
DEPENDING
ON
ITS
DISTANCE
FROM
THE
CENTER
THIS
TURNS
OUT
TO
BE
EQUIVALENT
TO
MINIMIZING
THE
WEIGHTED
OBJECTIVE
FUNCTION
W
K
L
F
I
K
J
L
G
I
J
P
K
L
WHERE
G
I
J
IS
THE
DESIRED
OUTPUT
VALUE
AND
P
FOR
THE
WEIGHTED
MEDIAN
THE
VALUE
P
IS
THE
USUAL
WEIGHTED
MEAN
WHICH
IS
EQUIVALENT
TO
CORRELATION
AFTER
NORMALIZING
BY
THE
SUM
OF
THE
WEIGHTS
BOVIK
SECTION
HARALICK
AND
SHAPIRO
SECTION
THE
WEIGHTED
MEAN
ALSO
HAS
DEEP
CONNECTIONS
TO
OTHER
METHODS
IN
ROBUST
STATISTICS
SEE
AP
PENDIX
B
SUCH
AS
INFLUENCE
FUNCTIONS
HUBER
HAMPEL
RONCHETTI
ROUSSEEUW
ET
AL
NON
LINEAR
SMOOTHING
HAS
ANOTHER
PERHAPS
EVEN
MORE
IMPORTANT
PROPERTY
ESPECIALLY
SINCE
SHOT
NOISE
IS
RARE
IN
TODAY
CAMERAS
SUCH
FILTERING
IS
MORE
EDGE
PRESERVING
I
E
IT
HAS
LESS
TENDENCY
TO
SOFTEN
EDGES
WHILE
FILTERING
AWAY
HIGH
FREQUENCY
NOISE
CONSIDER
THE
NOISY
IMAGE
IN
FIGURE
IN
ORDER
TO
REMOVE
MOST
OF
THE
NOISE
THE
GAUSSIAN
FILTER
IS
FORCED
TO
SMOOTH
AWAY
HIGH
FREQUENCY
DETAIL
WHICH
IS
MOST
NOTICEABLE
NEAR
STRONG
EDGES
MEDIAN
FILTERING
DOES
BETTER
BUT
AS
MENTIONED
BEFORE
DOES
NOT
DO
AS
GOOD
A
JOB
AT
SMOOTHING
AWAY
FROM
DISCONTINUITIES
SEE
TOMASI
AND
MANDUCHI
FOR
SOME
ADDITIONAL
REFERENCES
TO
EDGE
PRESERVING
SMOOTHING
TECHNIQUES
WHILE
WE
COULD
TRY
TO
USE
THE
Α
TRIMMED
MEAN
OR
WEIGHTED
MEDIAN
THESE
TECHNIQUES
STILL
HAVE
A
TENDENCY
TO
ROUND
SHARP
CORNERS
SINCE
THE
MAJORITY
OF
PIXELS
IN
THE
SMOOTHING
AREA
COME
FROM
THE
BACKGROUND
DISTRIBUTION
BILATERAL
FILTERING
WHAT
IF
WE
WERE
TO
COMBINE
THE
IDEA
OF
A
WEIGHTED
FILTER
KERNEL
WITH
A
BETTER
VERSION
OF
OUTLIER
REJECTION
WHAT
IF
INSTEAD
OF
REJECTING
A
FIXED
PERCENTAGE
Α
WE
SIMPLY
REJECT
IN
A
SOFT
WAY
PIXELS
WHOSE
VALUES
DIFFER
TOO
MUCH
FROM
THE
CENTRAL
PIXEL
VALUE
THIS
IS
THE
ESSENTIAL
IDEA
IN
BILATERAL
FILTERING
WHICH
WAS
FIRST
POPULARIZED
IN
THE
COMPUTER
VISION
COMMUNITY
BY
TOMASI
AND
MANDUCHI
CHEN
PARIS
AND
DURAND
AND
PARIS
KORNPROBST
TUMBLIN
ET
AL
CITE
SIMILAR
EARLIER
WORK
AURICH
AND
WEULE
SMITH
AND
BRADY
AS
WELL
AS
THE
WEALTH
OF
SUBSEQUENT
APPLICATIONS
IN
COMPUTER
VISION
AND
COMPUTATIONAL
PHOTOGRAPHY
IN
THE
BILATERAL
FILTER
THE
OUTPUT
PIXEL
VALUE
DEPENDS
ON
A
WEIGHTED
COMBINATION
OF
NEIGH
BORING
PIXEL
VALUES
I
J
K
L
F
K
L
W
I
J
K
L
THE
WEIGHTING
COEFFICIENT
W
I
J
K
L
DEPENDS
ON
THE
PRODUCT
OF
A
DOMAIN
KERNEL
FIGURE
D
I
J
K
L
EXP
I
K
J
L
AND
A
DATA
DEPENDENT
RANGE
KERNEL
FIGURE
R
I
J
K
L
EXP
I
J
F
K
L
WHEN
MULTIPLIED
TOGETHER
THESE
YIELD
THE
DATA
DEPENDENT
BILATERAL
WEIGHT
FUNCTION
W
I
J
K
L
EXP
I
K
J
L
I
J
F
K
L
FIGURE
SHOWS
AN
EXAMPLE
OF
THE
BILATERAL
FILTERING
OF
A
NOISY
STEP
EDGE
NOTE
HOW
THE
DO
MAIN
KERNEL
IS
THE
USUAL
GAUSSIAN
THE
RANGE
KERNEL
MEASURES
APPEARANCE
INTENSITY
SIMILARITY
TO
THE
CENTER
PIXEL
AND
THE
BILATERAL
FILTER
KERNEL
IS
A
PRODUCT
OF
THESE
TWO
NOTICE
THAT
THE
RANGE
FILTER
USES
THE
VECTOR
DISTANCE
BETWEEN
THE
CENTER
AND
THE
NEIGHBORING
PIXEL
THIS
IS
IMPORTANT
IN
COLOR
IMAGES
SINCE
AN
EDGE
IN
ANY
ONE
OF
THE
COLOR
BANDS
SIGNALS
A
CHANGE
IN
MATERIAL
AND
HENCE
THE
NEED
TO
DOWNWEIGHT
A
PIXEL
INFLUENCE
TOMASI
AND
MANDUCHI
SHOW
THAT
USING
THE
VECTOR
DISTANCE
AS
OPPOSED
TO
FILTERING
EACH
COLOR
BAND
SEPARATELY
REDUCES
COLOR
FRINGING
EFFECTS
THEY
ALSO
RECOMMEND
TAKING
THE
COLOR
DIFFERENCE
IN
THE
MORE
PERCEPTUALLY
UNIFORM
CIELAB
COLOR
SPACE
SEE
SECTION
A
B
C
D
E
F
FIGURE
BILATERAL
FILTERING
DURAND
AND
DORSEY
QC
ACM
A
NOISY
STEP
EDGE
INPUT
B
DOMAIN
FILTER
GAUSSIAN
C
RANGE
FILTER
SIMILARITY
TO
CENTER
PIXEL
VALUE
D
BILATERAL
FILTER
E
FILTERED
STEP
EDGE
OUTPUT
F
DISTANCE
BETWEEN
PIXELS
SINCE
BILATERAL
FILTERING
IS
QUITE
SLOW
COMPARED
TO
REGULAR
SEPARABLE
FILTERING
A
NUMBER
OF
ACCELERATION
TECHNIQUES
HAVE
BEEN
DEVELOPED
DURAND
AND
DORSEY
PARIS
AND
DURAND
CHEN
PARIS
AND
DURAND
PARIS
KORNPROBST
TUMBLIN
ET
AL
UNFORTUNATELY
THESE
TECHNIQUES
TEND
TO
USE
MORE
MEMORY
THAN
REGULAR
FILTERING
AND
ARE
HENCE
NOT
DIRECTLY
APPLICABLE
TO
FILTERING
FULL
COLOR
IMAGES
ITERATED
ADAPTIVE
SMOOTHING
AND
ANISOTROPIC
DIFFUSION
BILATERAL
AND
OTHER
FILTERS
CAN
ALSO
BE
APPLIED
IN
AN
ITERATIVE
FASHION
ESPECIALLY
IF
AN
APPEAR
ANCE
MORE
LIKE
A
CARTOON
IS
DESIRED
TOMASI
AND
MANDUCHI
WHEN
ITERATED
FILTERING
IS
APPLIED
A
MUCH
SMALLER
NEIGHBORHOOD
CAN
OFTEN
BE
USED
CONSIDER
FOR
EXAMPLE
USING
ONLY
THE
FOUR
NEAREST
NEIGHBORS
I
E
RESTRICTING
K
I
L
J
IN
OBSERVE
THAT
D
I
J
K
L
EXP
I
K
J
L
K
I
L
J
Λ
E
K
I
L
J
WE
CAN
THUS
RE
WRITE
AS
F
T
I
J
Η
F
T
K
L
R
I
J
K
L
T
I
J
Η
K
L
F
T
I
J
Η
ΗR
R
I
J
K
L
R
I
J
K
L
F
T
K
L
F
T
I
J
K
L
WHERE
R
K
L
R
I
J
K
L
K
L
ARE
THE
NEIGHBORS
OF
I
J
AND
WE
HAVE
MADE
THE
ITERATIVE
NATURE
OF
THE
FILTERING
EXPLICIT
AS
BARASH
NOTES
IS
THE
SAME
AS
THE
DISCRETE
ANISOTROPIC
DIFFUSION
EQUATION
FIRST
PROPOSED
BY
PERONA
AND
MALIK
SINCE
ITS
ORIGINAL
INTRODUCTION
ANISOTROPIC
DIF
FUSION
HAS
BEEN
EXTENDED
AND
APPLIED
TO
A
WIDE
RANGE
OF
PROBLEMS
NIELSEN
FLORACK
AND
DE
RICHE
BLACK
SAPIRO
MARIMONT
ET
AL
WEICKERT
TER
HAAR
ROMENY
AND
VIERGEVER
WEICKERT
IT
HAS
ALSO
BEEN
SHOWN
TO
BE
CLOSELY
RELATED
TO
OTHER
ADAPTIVE
SMOOTH
ING
TECHNIQUES
SAINT
MARC
CHEN
AND
MEDIONI
BARASH
BARASH
AND
COMANICIU
AS
WELL
AS
BAYESIAN
REGULARIZATION
WITH
A
NON
LINEAR
SMOOTHNESS
TERM
THAT
CAN
BE
DE
RIVED
FROM
IMAGE
STATISTICS
SCHARR
BLACK
AND
HAUSSECKER
IN
ITS
GENERAL
FORM
THE
RANGE
KERNEL
R
I
J
K
L
R
I
J
F
K
L
WHICH
IS
USUALLY
CALLED
THE
GAIN
OR
EDGE
STOPPING
FUNCTION
OR
DIFFUSION
COEFFICIENT
CAN
BE
ANY
MONOTONICALLY
INCREASING
FUNCTION
WITH
RI
X
AS
X
BLACK
SAPIRO
MARIMONT
ET
AL
SHOW
HOW
ANISOTROPIC
DIFFUSION
IS
EQUIVALENT
TO
MINIMIZING
A
ROBUST
PENALTY
FUNCTION
ON
THE
IMAGE
GRADIENTS
WHICH
WE
DISCUSS
IN
SECTIONS
AND
SCHARR
BLACK
AND
HAUSSECKER
SHOW
HOW
THE
EDGE
STOPPING
FUNCTION
CAN
BE
DERIVED
IN
A
PRINCIPLED
MANNER
FROM
LOCAL
IMAGE
STATISTICS
THEY
ALSO
EXTEND
THE
DIFFUSION
NEIGHBORHOOD
FROM
TO
WHICH
ALLOWS
THEM
TO
CREATE
A
DIFFUSION
OPERATOR
THAT
IS
BOTH
ROTATIONALLY
INVARIANT
AND
INCORPORATES
INFORMATION
ABOUT
THE
EIGENVALUES
OF
THE
LOCAL
STRUCTURE
TENSOR
NOTE
THAT
WITHOUT
A
BIAS
TERM
TOWARDS
THE
ORIGINAL
IMAGE
ANISOTROPIC
DIFFUSION
AND
ITERA
TIVE
ADAPTIVE
SMOOTHING
CONVERGE
TO
A
CONSTANT
IMAGE
UNLESS
A
SMALL
NUMBER
OF
ITERATIONS
IS
USED
E
G
FOR
SPEED
IT
IS
USUALLY
PREFERABLE
TO
FORMULATE
THE
SMOOTHING
PROBLEM
AS
A
JOINT
MINIMIZATION
OF
A
SMOOTHNESS
TERM
AND
A
DATA
FIDELITY
TERM
AS
DISCUSSED
IN
SECTIONS
AND
AND
BY
SCHARR
BLACK
AND
HAUSSECKER
WHICH
INTRODUCE
SUCH
A
BIAS
IN
A
PRINCIPLED
MANNER
MORPHOLOGY
WHILE
NON
LINEAR
FILTERS
ARE
OFTEN
USED
TO
ENHANCE
GRAYSCALE
AND
COLOR
IMAGES
THEY
ARE
ALSO
USED
EXTENSIVELY
TO
PROCESS
BINARY
IMAGES
SUCH
IMAGES
OFTEN
OCCUR
AFTER
A
THRESHOLDING
THE
ΗR
FACTOR
IS
NOT
PRESENT
IN
ANISOTROPIC
DIFFUSION
BUT
BECOMES
NEGLIGIBLE
AS
Η
A
B
C
D
E
F
FIGURE
BINARY
IMAGE
MORPHOLOGY
A
ORIGINAL
IMAGE
B
DILATION
C
EROSION
D
MAJORITY
E
OPENING
F
CLOSING
THE
STRUCTURING
ELEMENT
FOR
ALL
EXAMPLES
IS
A
SQUARE
THE
EFFECTS
OF
MAJORITY
ARE
A
SUBTLE
ROUNDING
OF
SHARP
CORNERS
OPENING
FAILS
TO
ELIMINATE
THE
DOT
SINCE
IT
IS
NOT
WIDE
ENOUGH
OPERATION
IF
F
T
ELSE
E
G
CONVERTING
A
SCANNED
GRAYSCALE
DOCUMENT
INTO
A
BINARY
IMAGE
FOR
FURTHER
PROCESSING
SUCH
AS
OPTICAL
CHARACTER
RECOGNITION
THE
MOST
COMMON
BINARY
IMAGE
OPERATIONS
ARE
CALLED
MORPHOLOGICAL
OPERATIONS
SINCE
THEY
CHANGE
THE
SHAPE
OF
THE
UNDERLYING
BINARY
OBJECTS
RITTER
AND
WILSON
CHAPTER
TO
PERFORM
SUCH
AN
OPERATION
WE
FIRST
CONVOLVE
THE
BINARY
IMAGE
WITH
A
BINARY
STRUCTURING
ELEMENT
AND
THEN
SELECT
A
BINARY
OUTPUT
VALUE
DEPENDING
ON
THE
THRESHOLDED
RESULT
OF
THE
CONVOLUTION
THIS
IS
NOT
THE
USUAL
WAY
IN
WHICH
THESE
OPERATIONS
ARE
DESCRIBED
BUT
I
FIND
IT
A
NICE
SIMPLE
WAY
TO
UNIFY
THE
PROCESSES
THE
STRUCTURING
ELEMENT
CAN
BE
ANY
SHAPE
FROM
A
SIMPLE
BOX
FILTER
TO
MORE
COMPLICATED
DISC
STRUCTURES
IT
CAN
EVEN
CORRESPOND
TO
A
PARTICULAR
SHAPE
THAT
IS
BEING
SOUGHT
FOR
IN
THE
IMAGE
FIGURE
SHOWS
A
CLOSE
UP
OF
THE
CONVOLUTION
OF
A
BINARY
IMAGE
F
WITH
A
STRUC
TURING
ELEMENT
AND
THE
RESULTING
IMAGES
FOR
THE
OPERATIONS
DESCRIBED
BELOW
LET
C
F
BE
THE
INTEGER
VALUED
COUNT
OF
THE
NUMBER
OF
INSIDE
EACH
STRUCTURING
ELEMENT
AS
IT
IS
SCANNED
OVER
THE
IMAGE
AND
BE
THE
SIZE
OF
THE
STRUCTURING
ELEMENT
NUMBER
OF
PIXELS
THE
STANDARD
OPERATIONS
USED
IN
BINARY
MORPHOLOGY
INCLUDE
DILATION
DILATE
F
Θ
C
EROSION
ERODE
F
Θ
C
MAJORITY
MAJ
F
Θ
C
OPENING
OPEN
F
DILATE
ERODE
F
CLOSING
CLOSE
F
ERODE
DILATE
F
AS
WE
CAN
SEE
FROM
FIGURE
DILATION
GROWS
THICKENS
OBJECTS
CONSISTING
OF
WHILE
EROSION
SHRINKS
THINS
THEM
THE
OPENING
AND
CLOSING
OPERATIONS
TEND
TO
LEAVE
LARGE
REGIONS
AND
SMOOTH
BOUNDARIES
UNAFFECTED
WHILE
REMOVING
SMALL
OBJECTS
OR
HOLES
AND
SMOOTHING
BOUNDARIES
WHILE
WE
WILL
NOT
USE
MATHEMATICAL
MORPHOLOGY
MUCH
IN
THE
REST
OF
THIS
BOOK
IT
IS
A
HANDY
TOOL
TO
HAVE
AROUND
WHENEVER
YOU
NEED
TO
CLEAN
UP
SOME
THRESHOLDED
IMAGES
YOU
CAN
FIND
ADDITIONAL
DETAILS
ON
MORPHOLOGY
IN
OTHER
TEXTBOOKS
ON
COMPUTER
VISION
AND
IMAGE
PROCESSING
HARALICK
AND
SHAPIRO
SECTION
BOVIK
SECTION
RITTER
AND
WILSON
SECTION
AS
WELL
AS
ARTICLES
AND
BOOKS
SPECIFICALLY
ON
THIS
TOPIC
SERRA
SERRA
AND
VINCENT
YUILLE
VINCENT
AND
GEIGER
SOILLE
DISTANCE
TRANSFORMS
THE
DISTANCE
TRANSFORM
IS
USEFUL
IN
QUICKLY
PRECOMPUTING
THE
DISTANCE
TO
A
CURVE
OR
SET
OF
POINTS
USING
A
TWO
PASS
RASTER
ALGORITHM
ROSENFELD
AND
PFALTZ
DANIELSSON
BORGE
FORS
PAGLIERONI
BREU
GIL
KIRKPATRICK
ET
AL
FELZENSZWALB
AND
HUTTENLOCHER
FABBRI
COSTA
TORELLI
ET
AL
IT
HAS
MANY
APPLICATIONS
INCLUDING
LEVEL
SETS
SEC
TION
FAST
CHAMFER
MATCHING
BINARY
IMAGE
ALIGNMENT
HUTTENLOCHER
KLANDERMAN
AND
RUCKLIDGE
FEATHERING
IN
IMAGE
STITCHING
AND
BLENDING
SECTION
AND
NEAREST
POINT
ALIGNMENT
SECTION
THE
DISTANCE
TRANSFORM
D
I
J
OF
A
BINARY
IMAGE
B
I
J
IS
DEFINED
AS
FOLLOWS
LET
D
K
L
BE
SOME
DISTANCE
METRIC
BETWEEN
PIXEL
OFFSETS
TWO
COMMONLY
USED
METRICS
INCLUDE
THE
CITY
BLOCK
OR
MANHATTAN
DISTANCE
AND
THE
EUCLIDEAN
DISTANCE
K
L
K
L
K
L
THE
DISTANCE
TRANSFORM
IS
THEN
DEFINED
AS
D
I
J
MIN
K
L
B
K
L
D
I
K
J
L
I
E
IT
IS
THE
DISTANCE
TO
THE
NEAREST
BACKGROUND
PIXEL
WHOSE
VALUE
IS
THE
CITY
BLOCK
DISTANCE
TRANSFORM
CAN
BE
EFFICIENTLY
COMPUTED
USING
A
FORWARD
AND
BACKWARD
PASS
OF
A
SIMPLE
RASTER
SCAN
ALGORITHM
AS
SHOWN
IN
FIGURE
DURING
THE
FORWARD
PASS
EACH
NON
ZERO
PIXEL
IN
B
IS
REPLACED
BY
THE
MINIMUM
OF
THE
DISTANCE
OF
ITS
NORTH
OR
WEST
NEIGHBOR
DURING
THE
BACKWARD
PASS
THE
SAME
OCCURS
EXCEPT
THAT
THE
MINIMUM
IS
BOTH
OVER
THE
CURRENT
VALUE
D
AND
THE
DISTANCE
OF
THE
SOUTH
AND
EAST
NEIGHBORS
FIGURE
A
B
C
D
FIGURE
CITY
BLOCK
DISTANCE
TRANSFORM
A
ORIGINAL
BINARY
IMAGE
B
TOP
TO
BOTTOM
FORWARD
RASTER
SWEEP
GREEN
VALUES
ARE
USED
TO
COMPUTE
THE
ORANGE
VALUE
C
BOTTOM
TO
TOP
BACKWARD
RASTER
SWEEP
GREEN
VALUES
ARE
MERGED
WITH
OLD
ORANGE
VALUE
D
FINAL
DISTANCE
TRANSFORM
EFFICIENTLY
COMPUTING
THE
EUCLIDEAN
DISTANCE
TRANSFORM
IS
MORE
COMPLICATED
HERE
JUST
KEEPING
THE
MINIMUM
SCALAR
DISTANCE
TO
THE
BOUNDARY
DURING
THE
TWO
PASSES
IS
NOT
SUFFICIENT
INSTEAD
A
VECTOR
VALUED
DISTANCE
CONSISTING
OF
BOTH
THE
X
AND
Y
COORDINATES
OF
THE
DISTANCE
TO
THE
BOUNDARY
MUST
BE
KEPT
AND
COMPARED
USING
THE
SQUARED
DISTANCE
HYPOTENUSE
RULE
AS
WELL
LARGER
SEARCH
REGIONS
NEED
TO
BE
USED
TO
OBTAIN
REASONABLE
RESULTS
RATHER
THAN
EXPLAINING
THE
ALGORITHM
DANIELSSON
BORGEFORS
IN
MORE
DETAIL
WE
LEAVE
IT
AS
AN
EXERCISE
FOR
THE
MOTIVATED
READER
EXERCISE
FIGURE
SHOWS
A
DISTANCE
TRANSFORM
COMPUTED
FROM
A
BINARY
IMAGE
NOTICE
HOW
THE
VALUES
GROW
AWAY
FROM
THE
BLACK
INK
REGIONS
AND
FORM
RIDGES
IN
THE
WHITE
AREA
OF
THE
ORIGINAL
IMAGE
BECAUSE
OF
THIS
LINEAR
GROWTH
FROM
THE
STARTING
BOUNDARY
PIXELS
THE
DISTANCE
TRANSFORM
IS
ALSO
SOMETIMES
KNOWN
AS
THE
GRASSFIRE
TRANSFORM
SINCE
IT
DESCRIBES
THE
TIME
AT
WHICH
A
FIRE
STARTING
INSIDE
THE
BLACK
REGION
WOULD
CONSUME
ANY
GIVEN
PIXEL
OR
A
CHAMFER
BECAUSE
IT
RESEMBLES
SIMILAR
SHAPES
USED
IN
WOODWORKING
AND
INDUSTRIAL
DESIGN
THE
RIDGES
IN
THE
DISTANCE
TRANSFORM
BECOME
THE
SKELETON
OR
MEDIAL
AXIS
TRANSFORM
MAT
OF
THE
REGION
WHERE
THE
TRANSFORM
IS
COMPUTED
AND
CONSIST
OF
PIXELS
THAT
ARE
OF
EQUAL
DISTANCE
TO
TWO
OR
MORE
BOUNDARIES
TEK
AND
KIMIA
SEBASTIAN
AND
KIMIA
A
USEFUL
EXTENSION
OF
THE
BASIC
DISTANCE
TRANSFORM
IS
THE
SIGNED
DISTANCE
TRANSFORM
WHICH
COMPUTES
DISTANCES
TO
BOUNDARY
PIXELS
FOR
ALL
THE
PIXELS
LAVALLE
E
AND
SZELISKI
THE
SIMPLEST
WAY
TO
CREATE
THIS
IS
TO
COMPUTE
THE
DISTANCE
TRANSFORMS
FOR
BOTH
THE
ORIGINAL
BI
NARY
IMAGE
AND
ITS
COMPLEMENT
AND
TO
NEGATE
ONE
OF
THEM
BEFORE
COMBINING
BECAUSE
SUCH
DISTANCE
FIELDS
TEND
TO
BE
SMOOTH
IT
IS
POSSIBLE
TO
STORE
THEM
MORE
COMPACTLY
WITH
MINI
MAL
LOSS
IN
RELATIVE
ACCURACY
USING
A
SPLINE
DEFINED
OVER
A
QUADTREE
OR
OCTREE
DATA
STRUCTURE
LAVALLE
E
AND
SZELISKI
SZELISKI
AND
LAVALLE
E
FRISKEN
PERRY
ROCKWOOD
ET
AL
SUCH
PRECOMPUTED
SIGNED
DISTANCE
TRANSFORMS
CAN
BE
EXTREMELY
USEFUL
IN
EFFICIENTLY
ALIGNING
AND
MERGING
CURVES
AND
SURFACES
HUTTENLOCHER
KLANDERMAN
AND
RUCKLIDGE
A
B
C
FIGURE
CONNECTED
COMPONENT
COMPUTATION
A
ORIGINAL
GRAYSCALE
IMAGE
B
HORIZONTAL
RUNS
NODES
CONNECTED
BY
VERTICAL
GRAPH
EDGES
DASHED
BLUE
RUNS
ARE
PSEUDOCOLORED
WITH
UNIQUE
COLORS
INHERITED
FROM
PARENT
NODES
C
RE
COLORING
AFTER
MERGING
ADJACENT
SEGMENTS
SZELISKI
AND
LAVALLE
E
CURLESS
AND
LEVOY
ESPECIALLY
IF
THE
VECTORIAL
VERSION
OF
THE
DISTANCE
TRANSFORM
I
E
A
POINTER
FROM
EACH
PIXEL
OR
VOXEL
TO
THE
NEAREST
BOUNDARY
OR
SURFACE
ELEMENT
IS
STORED
AND
INTERPOLATED
SIGNED
DISTANCE
FIELDS
ARE
ALSO
AN
ESSENTIAL
COM
PONENT
OF
LEVEL
SET
EVOLUTION
SECTION
WHERE
THEY
ARE
CALLED
CHARACTERISTIC
FUNCTIONS
CONNECTED
COMPONENTS
ANOTHER
USEFUL
SEMI
GLOBAL
IMAGE
OPERATION
IS
FINDING
CONNECTED
COMPONENTS
WHICH
ARE
DE
FINED
AS
REGIONS
OF
ADJACENT
PIXELS
THAT
HAVE
THE
SAME
INPUT
VALUE
OR
LABEL
IN
THE
REMAINDER
OF
THIS
SECTION
CONSIDER
PIXELS
TO
BE
ADJACENT
IF
THEY
ARE
IMMEDIATE
NEIGHBORS
AND
THEY
HAVE
THE
SAME
INPUT
VALUE
CONNECTED
COMPONENTS
CAN
BE
USED
IN
A
VARIETY
OF
APPLICATIONS
SUCH
AS
FINDING
INDIVIDUAL
LETTERS
IN
A
SCANNED
DOCUMENT
OR
FINDING
OBJECTS
SAY
CELLS
IN
A
THRESHOLDED
IMAGE
AND
COMPUTING
THEIR
AREA
STATISTICS
CONSIDER
THE
GRAYSCALE
IMAGE
IN
FIGURE
THERE
ARE
FOUR
CONNECTED
COMPONENTS
IN
THIS
FIGURE
THE
OUTERMOST
SET
OF
WHITE
PIXELS
THE
LARGE
RING
OF
GRAY
PIXELS
THE
WHITE
ENCLOSED
REGION
AND
THE
SINGLE
GRAY
PIXEL
THESE
ARE
SHOWN
PSEUDOCOLORED
IN
FIGURE
AS
PINK
GREEN
BLUE
AND
BROWN
TO
COMPUTE
THE
CONNECTED
COMPONENTS
OF
AN
IMAGE
WE
FIRST
CONCEPTUALLY
SPLIT
THE
IMAGE
INTO
HORIZONTAL
RUNS
OF
ADJACENT
PIXELS
AND
THEN
COLOR
THE
RUNS
WITH
UNIQUE
LABELS
RE
USING
THE
LABELS
OF
VERTICALLY
ADJACENT
RUNS
WHENEVER
POSSIBLE
IN
A
SECOND
PHASE
ADJACENT
RUNS
OF
DIFFERENT
COLORS
ARE
THEN
MERGED
WHILE
THIS
DESCRIPTION
IS
A
LITTLE
SKETCHY
IT
SHOULD
BE
ENOUGH
TO
ENABLE
A
MOTIVATED
STU
DENT
TO
IMPLEMENT
THIS
ALGORITHM
EXERCISE
HARALICK
AND
SHAPIRO
SECTION
GIVE
A
MUCH
LONGER
DESCRIPTION
OF
VARIOUS
CONNECTED
COMPONENT
ALGORITHMS
INCLUDING
ONES
THAT
AVOID
THE
CREATION
OF
A
POTENTIALLY
LARGE
RE
COLORING
EQUIVALENCE
TABLE
WELL
DEBUGGED
CONNECTED
COMPONENT
ALGORITHMS
ARE
ALSO
AVAILABLE
IN
MOST
IMAGE
PROCESSING
LIBRARIES
ONCE
A
BINARY
OR
MULTI
VALUED
IMAGE
HAS
BEEN
SEGMENTED
INTO
ITS
CONNECTED
COMPONENTS
IT
IS
OFTEN
USEFUL
TO
COMPUTE
THE
AREA
STATISTICS
FOR
EACH
INDIVIDUAL
REGION
R
SUCH
STATISTICS
INCLUDE
THE
AREA
NUMBER
OF
PIXELS
THE
PERIMETER
NUMBER
OF
BOUNDARY
PIXELS
THE
CENTROID
AVERAGE
X
AND
Y
VALUES
THE
SECOND
MOMENTS
M
X
Y
R
R
X
X
L
X
X
Y
Y
L
FROM
WHICH
THE
MAJOR
AND
MINOR
AXIS
ORIENTATION
AND
LENGTHS
CAN
BE
COMPUTED
USING
EIGENVALUE
ANALYSIS
THESE
STATISTICS
CAN
THEN
BE
USED
FOR
FURTHER
PROCESSING
E
G
FOR
SORTING
THE
REGIONS
BY
THE
AREA
SIZE
TO
CONSIDER
THE
LARGEST
REGIONS
FIRST
OR
FOR
PRELIMINARY
MATCHING
OF
REGIONS
IN
DIFFERENT
IMAGES
FOURIER
TRANSFORMS
IN
SECTION
WE
MENTIONED
THAT
FOURIER
ANALYSIS
COULD
BE
USED
TO
ANALYZE
THE
FREQUENCY
CHARACTERISTICS
OF
VARIOUS
FILTERS
IN
THIS
SECTION
WE
EXPLAIN
BOTH
HOW
FOURIER
ANALYSIS
LETS
US
DETERMINE
THESE
CHARACTERISTICS
OR
EQUIVALENTLY
THE
FREQUENCY
CONTENT
OF
AN
IMAGE
AND
HOW
USING
THE
FAST
FOURIER
TRANSFORM
FFT
LETS
US
PERFORM
LARGE
KERNEL
CONVOLUTIONS
IN
TIME
THAT
IS
INDEPENDENT
OF
THE
KERNEL
SIZE
MORE
COMPREHENSIVE
INTRODUCTIONS
TO
FOURIER
TRANSFORMS
ARE
PROVIDED
BY
BRACEWELL
GLASSNER
OPPENHEIM
AND
SCHAFER
OPPEN
HEIM
SCHAFER
AND
BUCK
HOW
CAN
WE
ANALYZE
WHAT
A
GIVEN
FILTER
DOES
TO
HIGH
MEDIUM
AND
LOW
FREQUENCIES
THE
ANSWER
IS
TO
SIMPLY
PASS
A
SINUSOID
OF
KNOWN
FREQUENCY
THROUGH
THE
FILTER
AND
TO
OBSERVE
BY
HOW
MUCH
IT
IS
ATTENUATED
LET
X
SIN
X
ΦI
SIN
ΩX
ΦI
MOMENTS
CAN
ALSO
BE
COMPUTED
USING
GREEN
THEOREM
APPLIED
TO
THE
BOUNDARY
PIXELS
YANG
AND
ALBREGTSEN
FIGURE
THE
FOURIER
TRANSFORM
AS
THE
RESPONSE
OF
A
FILTER
H
X
TO
AN
INPUT
SINUSOID
X
EJΩX
YIELDING
AN
OUTPUT
SINUSOID
O
X
H
X
X
AEJΩX
Φ
BE
THE
INPUT
SINUSOID
WHOSE
FREQUENCY
IS
F
ANGULAR
FREQUENCY
IS
Ω
AND
PHASE
IS
ΦI
NOTE
THAT
IN
THIS
SECTION
WE
USE
THE
VARIABLES
X
AND
Y
TO
DENOTE
THE
SPATIAL
COORDINATES
OF
AN
IMAGE
RATHER
THAN
I
AND
J
AS
IN
THE
PREVIOUS
SECTIONS
THIS
IS
BOTH
BECAUSE
THE
LETTERS
I
AND
J
ARE
USED
FOR
THE
IMAGINARY
NUMBER
THE
USAGE
DEPENDS
ON
WHETHER
YOU
ARE
READING
COMPLEX
VARIABLES
OR
ELECTRICAL
ENGINEERING
LITERATURE
AND
BECAUSE
IT
IS
CLEARER
HOW
TO
DISTINGUISH
THE
HORIZONTAL
X
AND
VERTICAL
Y
COMPONENTS
IN
FREQUENCY
SPACE
IN
THIS
SECTION
WE
USE
THE
LETTER
J
FOR
THE
IMAGINARY
NUMBER
SINCE
THAT
IS
THE
FORM
MORE
COMMONLY
FOUND
IN
THE
SIGNAL
PROCESSING
LITERATURE
BRACEWELL
OPPENHEIM
AND
SCHAFER
OPPENHEIM
SCHAFER
AND
BUCK
IF
WE
CONVOLVE
THE
SINUSOIDAL
SIGNAL
X
WITH
A
FILTER
WHOSE
IMPULSE
RESPONSE
IS
H
X
WE
GET
ANOTHER
SINUSOID
OF
THE
SAME
FREQUENCY
BUT
DIFFERENT
MAGNITUDE
A
AND
PHASE
ΦO
O
X
H
X
X
A
SIN
ΩX
ΦO
AS
SHOWN
IN
FIGURE
TO
SEE
THAT
THIS
IS
THE
CASE
REMEMBER
THAT
A
CONVOLUTION
CAN
BE
EXPRESSED
AS
A
WEIGHTED
SUMMATION
OF
SHIFTED
INPUT
SIGNALS
AND
THAT
THE
SUMMATION
OF
A
BUNCH
OF
SHIFTED
SINUSOIDS
OF
THE
SAME
FREQUENCY
IS
JUST
A
SINGLE
SINUSOID
AT
THAT
FREQUENCY
THE
NEW
MAGNITUDE
A
IS
CALLED
THE
GAIN
OR
MAGNITUDE
OF
THE
FILTER
WHILE
THE
PHASE
DIFFERENCE
Φ
ΦO
ΦI
IS
CALLED
THE
SHIFT
OR
PHASE
IN
FACT
A
MORE
COMPACT
NOTATION
IS
TO
USE
THE
COMPLEX
VALUED
SINUSOID
X
EJΩX
COS
ΩX
J
SIN
ΩX
IN
THAT
CASE
WE
CAN
SIMPLY
WRITE
O
X
H
X
X
AEJΩX
Φ
IF
H
IS
A
GENERAL
NON
LINEAR
TRANSFORM
ADDITIONAL
HARMONIC
FREQUENCIES
ARE
INTRODUCED
THIS
WAS
TRADITIONALLY
THE
BANE
OF
AUDIOPHILES
WHO
INSISTED
ON
EQUIPMENT
WITH
NO
HARMONIC
DISTORTION
NOW
THAT
DIGITAL
AUDIO
HAS
INTRO
DUCED
PURE
DISTORTION
FREE
SOUND
SOME
AUDIOPHILES
ARE
BUYING
RETRO
TUBE
AMPLIFIERS
OR
DIGITAL
SIGNAL
PROCESSORS
THAT
SIMULATE
SUCH
DISTORTIONS
BECAUSE
OF
THEIR
WARMER
SOUND
THE
FOURIER
TRANSFORM
IS
SIMPLY
A
TABULATION
OF
THE
MAGNITUDE
AND
PHASE
RESPONSE
AT
EACH
FREQUENCY
H
Ω
F
H
X
AEJΦ
I
E
IT
IS
THE
RESPONSE
TO
A
COMPLEX
SINUSOID
OF
FREQUENCY
Ω
PASSED
THROUGH
THE
FILTER
H
X
THE
FOURIER
TRANSFORM
PAIR
IS
ALSO
OFTEN
WRITTEN
AS
H
X
F
H
Ω
UNFORTUNATELY
DOES
NOT
GIVE
AN
ACTUAL
FORMULA
FOR
COMPUTING
THE
FOURIER
TRANSFORM
INSTEAD
IT
GIVES
A
RECIPE
I
E
CONVOLVE
THE
FILTER
WITH
A
SINUSOID
OBSERVE
THE
MAGNITUDE
AND
PHASE
SHIFT
REPEAT
FORTUNATELY
CLOSED
FORM
EQUATIONS
FOR
THE
FOURIER
TRANSFORM
EXIST
BOTH
IN
THE
CONTINUOUS
DOMAIN
AND
IN
THE
DISCRETE
DOMAIN
H
Ω
H
X
E
JΩXDX
N
H
K
H
X
E
J
N
X
WHERE
N
IS
THE
LENGTH
OF
THE
SIGNAL
OR
REGION
OF
ANALYSIS
THESE
FORMULAS
APPLY
BOTH
TO
FILTERS
SUCH
AS
H
X
AND
TO
SIGNALS
OR
IMAGES
SUCH
AS
X
OR
G
X
THE
DISCRETE
FORM
OF
THE
FOURIER
TRANSFORM
IS
KNOWN
AS
THE
DISCRETE
FOURIER
TRANS
FORM
DFT
NOTE
THAT
WHILE
CAN
BE
EVALUATED
FOR
ANY
VALUE
OF
K
IT
ONLY
MAKES
SENSE
FOR
VALUES
IN
THE
RANGE
K
N
N
THIS
IS
BECAUSE
LARGER
VALUES
OF
K
ALIAS
WITH
LOWER
FREQUENCIES
AND
HENCE
PROVIDE
NO
ADDITIONAL
INFORMATION
AS
EXPLAINED
IN
THE
DISCUSSION
ON
ALIASING
IN
SECTION
AT
FACE
VALUE
THE
DFT
TAKES
O
N
OPERATIONS
MULTIPLY
ADDS
TO
EVALUATE
FORTUNATELY
THERE
EXISTS
A
FASTER
ALGORITHM
CALLED
THE
FAST
FOURIER
TRANSFORM
FFT
WHICH
REQUIRES
ONLY
O
N
N
OPERATIONS
BRACEWELL
OPPENHEIM
SCHAFER
AND
BUCK
WE
DO
NOT
EXPLAIN
THE
DETAILS
OF
THE
ALGORITHM
HERE
EXCEPT
TO
SAY
THAT
IT
INVOLVES
A
SERIES
OF
N
STAGES
WHERE
EACH
STAGE
PERFORMS
SMALL
TRANSFORMS
MATRIX
MULTIPLICATIONS
WITH
KNOWN
COEFFICIENTS
FOLLOWED
BY
SOME
SEMI
GLOBAL
PERMUTATIONS
YOU
WILL
OFTEN
SEE
THE
TERM
BUT
TERFLY
APPLIED
TO
THESE
STAGES
BECAUSE
OF
THE
PICTORIAL
SHAPE
OF
THE
SIGNAL
PROCESSING
GRAPHS
INVOLVED
IMPLEMENTATIONS
FOR
THE
FFT
CAN
BE
FOUND
IN
MOST
NUMERICAL
AND
SIGNAL
PROCESSING
LIBRARIES
NOW
THAT
WE
HAVE
DEFINED
THE
FOURIER
TRANSFORM
WHAT
ARE
SOME
OF
ITS
PROPERTIES
AND
HOW
CAN
THEY
BE
USED
TABLE
LISTS
A
NUMBER
OF
USEFUL
PROPERTIES
WHICH
WE
DESCRIBE
IN
A
LITTLE
MORE
DETAIL
BELOW
PROPERTY
SIGNAL
TRANSFORM
SUPERPOSITION
X
X
Ω
Ω
SHIFT
F
X
F
Ω
E
REVERSAL
F
X
F
Ω
CONVOLUTION
F
X
H
X
F
Ω
H
Ω
CORRELATION
F
X
H
X
F
Ω
H
Ω
MULTIPLICATION
F
X
H
X
F
Ω
H
Ω
DIFFERENTIATION
F
I
X
JΩF
Ω
DOMAIN
SCALING
F
AX
AF
Ω
A
REAL
IMAGES
F
X
F
X
F
Ω
F
Ω
PARSEVAL
THEOREM
X
F
X
Ω
F
Ω
TABLE
SOME
USEFUL
PROPERTIES
OF
FOURIER
TRANSFORMS
THE
ORIGINAL
TRANSFORM
PAIR
IS
F
Ω
F
F
X
SUPERPOSITION
THE
FOURIER
TRANSFORM
OF
A
SUM
OF
SIGNALS
IS
THE
SUM
OF
THEIR
FOURIER
TRANSFORMS
THUS
THE
FOURIER
TRANSFORM
IS
A
LINEAR
OPERATOR
SHIFT
THE
FOURIER
TRANSFORM
OF
A
SHIFTED
SIGNAL
IS
THE
TRANSFORM
OF
THE
ORIGINAL
SIGNAL
MULTIPLIED
BY
A
LINEAR
PHASE
SHIFT
COMPLEX
SINUSOID
REVERSAL
THE
FOURIER
TRANSFORM
OF
A
REVERSED
SIGNAL
IS
THE
COMPLEX
CONJUGATE
OF
THE
SIGNAL
TRANSFORM
CONVOLUTION
THE
FOURIER
TRANSFORM
OF
A
PAIR
OF
CONVOLVED
SIGNALS
IS
THE
PRODUCT
OF
THEIR
TRANSFORMS
CORRELATION
THE
FOURIER
TRANSFORM
OF
A
CORRELATION
IS
THE
PRODUCT
OF
THE
FIRST
TRANSFORM
TIMES
THE
COMPLEX
CONJUGATE
OF
THE
SECOND
ONE
MULTIPLICATION
THE
FOURIER
TRANSFORM
OF
THE
PRODUCT
OF
TWO
SIGNALS
IS
THE
CONVOLUTION
OF
THEIR
TRANSFORMS
DIFFERENTIATION
THE
FOURIER
TRANSFORM
OF
THE
DERIVATIVE
OF
A
SIGNAL
IS
THAT
SIGNAL
TRANSFORM
MULTIPLIED
BY
THE
FREQUENCY
IN
OTHER
WORDS
DIFFERENTIATION
LINEARLY
EMPHA
SIZES
MAGNIFIES
HIGHER
FREQUENCIES
DOMAIN
SCALING
THE
FOURIER
TRANSFORM
OF
A
STRETCHED
SIGNAL
IS
THE
EQUIVALENTLY
COM
PRESSED
AND
SCALED
VERSION
OF
THE
ORIGINAL
TRANSFORM
AND
VICE
VERSA
REAL
IMAGES
THE
FOURIER
TRANSFORM
OF
A
REAL
VALUED
SIGNAL
IS
SYMMETRIC
AROUND
THE
ORIGIN
THIS
FACT
CAN
BE
USED
TO
SAVE
SPACE
AND
TO
DOUBLE
THE
SPEED
OF
IMAGE
FFTS
BY
PACKING
ALTERNATING
SCANLINES
INTO
THE
REAL
AND
IMAGINARY
PARTS
OF
THE
SIGNAL
BEING
TRANSFORMED
PARSEVAL
THEOREM
THE
ENERGY
SUM
OF
SQUARED
VALUES
OF
A
SIGNAL
IS
THE
SAME
AS
THE
ENERGY
OF
ITS
FOURIER
TRANSFORM
ALL
OF
THESE
PROPERTIES
ARE
RELATIVELY
STRAIGHTFORWARD
TO
PROVE
SEE
EXERCISE
AND
THEY
WILL
COME
IN
HANDY
LATER
IN
THE
BOOK
E
G
WHEN
DESIGNING
OPTIMUM
WIENER
FILTERS
SECTION
OR
PERFORMING
FAST
IMAGE
CORRELATIONS
SECTION
FOURIER
TRANSFORM
PAIRS
NOW
THAT
WE
HAVE
THESE
PROPERTIES
IN
PLACE
LET
US
LOOK
AT
THE
FOURIER
TRANSFORM
PAIRS
OF
SOME
COMMONLY
OCCURRING
FILTERS
AND
SIGNALS
AS
LISTED
IN
TABLE
IN
MORE
DETAIL
THESE
PAIRS
ARE
AS
FOLLOWS
IMPULSE
THE
IMPULSE
RESPONSE
HAS
A
CONSTANT
ALL
FREQUENCY
TRANSFORM
SHIFTED
IMPULSE
THE
SHIFTED
IMPULSE
HAS
UNIT
MAGNITUDE
AND
LINEAR
PHASE
BOX
FILTER
THE
BOX
MOVING
AVERAGE
FILTER
BOX
IF
X
ELSE
HAS
A
SINC
FOURIER
TRANSFORM
SINC
Ω
SIN
Ω
Ω
WHICH
HAS
AN
INFINITE
NUMBER
OF
SIDE
LOBES
CONVERSELY
THE
SINC
FILTER
IS
AN
IDEAL
LOW
PASS
FILTER
FOR
A
NON
UNIT
BOX
THE
WIDTH
OF
THE
BOX
A
AND
THE
SPACING
OF
THE
ZERO
CROSSINGS
IN
THE
SINC
A
ARE
INVERSELY
PROPORTIONAL
TENT
THE
PIECEWISE
LINEAR
TENT
FUNCTION
TENT
X
MAX
X
HAS
A
FOURIER
TRANSFORM
GAUSSIAN
THE
UNIT
AREA
GAUSSIAN
OF
WIDTH
Σ
G
X
Σ
E
HAS
A
UNIT
HEIGHT
GAUSSIAN
OF
WIDTH
Σ
AS
ITS
FOURIER
TRANSFORM
NAME
SIGNAL
TRANSFORM
IMPULSE
Δ
X
SHIFTED
IMPULSE
Δ
X
U
E
JΩU
BOX
FILTER
BOX
X
A
ASINC
AΩ
TENT
TENT
X
A
AΩ
GAUSSIAN
G
X
Σ
G
Ω
Σ
LAPLACIAN
G
X
Σ
Ω
Σ
COS
Ω
X
G
X
Σ
G
Ω
Ω
Σ
UNSHARP
MASK
Γ
Δ
X
ΓG
X
Σ
Γ
G
Ω
Σ
WINDOWED
SINC
RCOS
X
AW
SINC
X
A
SEE
FIGURE
TABLE
SOME
USEFUL
CONTINUOUS
FOURIER
TRANSFORM
PAIRS
THE
DASHED
LINE
IN
THE
FOURIER
TRANSFORM
OF
THE
SHIFTED
IMPULSE
INDICATES
ITS
LINEAR
PHASE
ALL
OTHER
TRANSFORMS
HAVE
ZERO
PHASE
THEY
ARE
REAL
VALUED
NOTE
THAT
THE
FIGURES
ARE
NOT
NECESSARILY
DRAWN
TO
SCALE
BUT
ARE
DRAWN
TO
ILLUSTRATE
THE
GENERAL
SHAPE
AND
CHARACTERISTICS
OF
THE
FILTER
OR
ITS
RESPONSE
IN
PARTICULAR
THE
LAPLACIAN
OF
GAUSSIAN
IS
DRAWN
INVERTED
BECAUSE
IT
RESEMBLES
MORE
A
MEXICAN
HAT
AS
IT
IS
SOMETIMES
CALLED
LAPLACIAN
OF
GAUSSIAN
THE
SECOND
DERIVATIVE
OF
A
GAUSSIAN
OF
WIDTH
Σ
LOG
X
Σ
G
X
Σ
HAS
A
BAND
PASS
RESPONSE
OF
AS
ITS
FOURIER
TRANSFORM
Σ
Ω
Σ
GABOR
THE
EVEN
GABOR
FUNCTION
WHICH
IS
THE
PRODUCT
OF
A
COSINE
OF
FREQUENCY
AND
A
GAUSSIAN
OF
WIDTH
Σ
HAS
AS
ITS
TRANSFORM
THE
SUM
OF
THE
TWO
GAUSSIANS
OF
WIDTH
Σ
CENTERED
AT
Ω
THE
ODD
GABOR
FUNCTION
WHICH
USES
A
SINE
IS
THE
DIFFERENCE
OF
TWO
SUCH
GAUSSIANS
GABOR
FUNCTIONS
ARE
OFTEN
USED
FOR
ORIENTED
AND
BAND
PASS
FILTERING
SINCE
THEY
CAN
BE
MORE
FREQUENCY
SELECTIVE
THAN
GAUSSIAN
DERIVATIVES
UNSHARP
MASK
THE
UNSHARP
MASK
INTRODUCED
IN
HAS
AS
ITS
TRANSFORM
A
UNIT
RESPONSE
WITH
A
SLIGHT
BOOST
AT
HIGHER
FREQUENCIES
WINDOWED
SINC
THE
WINDOWED
MASKED
SINC
FUNCTION
SHOWN
IN
TABLE
HAS
A
RE
SPONSE
FUNCTION
THAT
APPROXIMATES
AN
IDEAL
LOW
PASS
FILTER
BETTER
AND
BETTER
AS
ADDITIONAL
SIDE
LOBES
ARE
ADDED
W
IS
INCREASED
FIGURE
SHOWS
THE
SHAPES
OF
THESE
SUCH
FIL
TERS
ALONG
WITH
THEIR
FOURIER
TRANSFORMS
FOR
THESE
EXAMPLES
WE
USE
A
ONE
LOBE
RAISED
COSINE
RCOS
X
COS
ΠX
BOX
X
ALSO
KNOWN
AS
THE
HANN
WINDOW
AS
THE
WINDOWING
FUNCTION
WOLBERG
AND
OPPENHEIM
SCHAFER
AND
BUCK
DISCUSS
ADDITIONAL
WINDOWING
FUNCTIONS
WHICH
INCLUDE
THE
LANCZOS
WINDOW
THE
POSITIVE
FIRST
LOBE
OF
A
SINC
FUNCTION
WE
CAN
ALSO
COMPUTE
THE
FOURIER
TRANSFORMS
FOR
THE
SMALL
DISCRETE
KERNELS
SHOWN
IN
FIG
URE
SEE
TABLE
NOTICE
HOW
THE
MOVING
AVERAGE
FILTERS
DO
NOT
UNIFORMLY
DAMPEN
HIGHER
FREQUENCIES
AND
HENCE
CAN
LEAD
TO
RINGING
ARTIFACTS
THE
BINOMIAL
FILTER
GOMES
AND
VELHO
USED
AS
THE
GAUSSIAN
IN
BURT
AND
ADELSON
LAPLACIAN
PYRAMID
SEE
SECTION
DOES
A
DECENT
JOB
OF
SEPARATING
THE
HIGH
AND
LOW
FREQUENCIES
BUT
STILL
LEAVES
A
FAIR
AMOUNT
OF
HIGH
FREQUENCY
DETAIL
WHICH
CAN
LEAD
TO
ALIASING
AFTER
DOWNSAMPLING
THE
SOBEL
EDGE
DETECTOR
AT
FIRST
LINEARLY
ACCENTUATES
FREQUENCIES
BUT
THEN
DECAYS
AT
HIGHER
FRE
QUENCIES
AND
HENCE
HAS
TROUBLE
DETECTING
FINE
SCALE
EDGES
E
G
ADJACENT
BLACK
AND
WHITE
COLUMNS
WE
LOOK
AT
ADDITIONAL
EXAMPLES
OF
SMALL
KERNEL
FOURIER
TRANSFORMS
IN
SECTION
WHERE
WE
STUDY
BETTER
KERNELS
FOR
PRE
FILTERING
BEFORE
DECIMATION
SIZE
REDUCTION
NAME
KERNEL
TRANSFORM
PLOT
BOX
COS
Ω
BOX
COS
Ω
COS
LINEAR
COS
Ω
BINOMIAL
COS
Ω
SOBEL
SIN
Ω
CORNER
COS
Ω
TABLE
FOURIER
TRANSFORMS
OF
THE
SEPARABLE
KERNELS
SHOWN
IN
FIGURE
TWO
DIMENSIONAL
FOURIER
TRANSFORMS
THE
FORMULAS
AND
INSIGHTS
WE
HAVE
DEVELOPED
FOR
ONE
DIMENSIONAL
SIGNALS
AND
THEIR
TRANS
FORMS
TRANSLATE
DIRECTLY
TO
TWO
DIMENSIONAL
IMAGES
HERE
INSTEAD
OF
JUST
SPECIFYING
A
HOR
IZONTAL
OR
VERTICAL
FREQUENCY
ΩX
OR
ΩY
WE
CAN
CREATE
AN
ORIENTED
SINUSOID
OF
FREQUENCY
ΩX
ΩY
X
Y
SIN
ΩXX
ΩYY
THE
CORRESPONDING
TWO
DIMENSIONAL
FOURIER
TRANSFORMS
ARE
THEN
H
ΩX
Ω
R
R
H
X
Y
E
J
ΩXX
ΩY
Y
DX
DY
AND
IN
THE
DISCRETE
DOMAIN
M
N
KXX
KY
Y
H
KX
KY
M
N
X
H
X
Y
E
Y
M
N
WHERE
M
AND
N
ARE
THE
WIDTH
AND
HEIGHT
OF
THE
IMAGE
ALL
OF
THE
FOURIER
TRANSFORM
PROPERTIES
FROM
TABLE
CARRY
OVER
TO
TWO
DIMENSIONS
IF
WE
REPLACE
THE
SCALAR
VARIABLES
X
Ω
AND
A
WITH
THEIR
VECTOR
COUNTERPARTS
X
X
Y
Ω
ΩX
ΩY
AND
A
AX
AY
AND
USE
VECTOR
INNER
PRODUCTS
INSTEAD
OF
MULTIPLICATIONS
WIENER
FILTERING
WHILE
THE
FOURIER
TRANSFORM
IS
A
USEFUL
TOOL
FOR
ANALYZING
THE
FREQUENCY
CHARACTERISTICS
OF
A
FILTER
KERNEL
OR
IMAGE
IT
CAN
ALSO
BE
USED
TO
ANALYZE
THE
FREQUENCY
SPECTRUM
OF
A
WHOLE
CLASS
OF
IMAGES
A
SIMPLE
MODEL
FOR
IMAGES
IS
TO
ASSUME
THAT
THEY
ARE
RANDOM
NOISE
FIELDS
WHOSE
EXPECTED
MAGNITUDE
AT
EACH
FREQUENCY
IS
GIVEN
BY
THIS
POWER
SPECTRUM
PS
ΩX
ΩY
I
E
ΩX
ΩY
PS
ΩX
ΩY
WHERE
THE
ANGLE
BRACKETS
DENOTE
THE
EXPECTED
MEAN
VALUE
OF
A
RANDOM
VARIABLE
TO
GENERATE
SUCH
AN
IMAGE
WE
SIMPLY
CREATE
A
RANDOM
GAUSSIAN
NOISE
IMAGE
ΩX
ΩY
WHERE
EACH
PIXEL
IS
A
ZERO
MEAN
OF
VARIANCE
PS
ΩX
ΩY
AND
THEN
TAKE
ITS
INVERSE
FFT
THE
OBSERVATION
THAT
SIGNAL
SPECTRA
CAPTURE
A
FIRST
ORDER
DESCRIPTION
OF
SPATIAL
STATISTICS
IS
WIDELY
USED
IN
SIGNAL
AND
IMAGE
PROCESSING
IN
PARTICULAR
ASSUMING
THAT
AN
IMAGE
IS
A
THE
NOTATION
E
IS
ALSO
COMMONLY
USED
WE
SET
THE
DC
I
E
CONSTANT
COMPONENT
AT
TO
THE
MEAN
GREY
LEVEL
SEE
ALGORITHM
C
IN
APPENDIX
C
FOR
CODE
TO
GENERATE
GAUSSIAN
NOISE
SAMPLE
FROM
A
CORRELATED
GAUSSIAN
RANDOM
NOISE
FIELD
COMBINED
WITH
A
STATISTICAL
MODEL
OF
THE
MEASUREMENT
PROCESS
YIELDS
AN
OPTIMUM
RESTORATION
FILTER
KNOWN
AS
THE
WIENER
FILTER
TO
DERIVE
THE
WIENER
FILTER
WE
ANALYZE
EACH
FREQUENCY
COMPONENT
OF
A
SIGNAL
FOURIER
TRANSFORM
INDEPENDENTLY
THE
NOISY
IMAGE
FORMATION
PROCESS
CAN
BE
WRITTEN
AS
O
X
Y
X
Y
N
X
Y
WHERE
X
Y
IS
THE
UNKNOWN
IMAGE
WE
ARE
TRYING
TO
RECOVER
N
X
Y
IS
THE
ADDITIVE
NOISE
SIGNAL
AND
O
X
Y
IS
THE
OBSERVED
NOISY
IMAGE
BECAUSE
OF
THE
LINEARITY
OF
THE
FOURIER
TRANS
FORM
WE
CAN
WRITE
O
ΩX
ΩY
ΩX
ΩY
N
ΩX
ΩY
WHERE
EACH
QUANTITY
IN
THE
ABOVE
EQUATION
IS
THE
FOURIER
TRANSFORM
OF
THE
CORRESPONDING
IMAGE
AT
EACH
FREQUENCY
ΩX
ΩY
WE
KNOW
FROM
OUR
IMAGE
SPECTRUM
THAT
THE
UNKNOWN
TRANS
FORM
COMPONENT
ΩX
ΩY
HAS
A
PRIOR
DISTRIBUTION
WHICH
IS
A
ZERO
MEAN
GAUSSIAN
WITH
VARI
ANCE
PS
ΩX
ΩY
WE
ALSO
HAVE
NOISY
MEASUREMENT
O
ΩX
ΩY
WHOSE
VARIANCE
IS
PN
ΩX
ΩY
I
E
THE
POWER
SPECTRUM
OF
THE
NOISE
WHICH
IS
USUALLY
ASSUMED
TO
BE
CONSTANT
WHITE
PN
ΩX
ΩY
ACCORDING
TO
BAYES
RULE
APPENDIX
B
THE
POSTERIOR
ESTIMATE
OF
CAN
BE
WRITTEN
AS
P
O
P
O
P
WHERE
P
O
P
O
P
IS
A
NORMALIZING
CONSTANT
USED
TO
MAKE
THE
P
O
DISTRIBUTION
PROPER
INTEGRATE
TO
THE
PRIOR
DISTRIBUTION
P
IS
GIVEN
BY
Μ
P
E
WHERE
Μ
IS
THE
EXPECTED
MEAN
AT
THAT
FREQUENCY
EVERYWHERE
EXCEPT
AT
THE
ORIGIN
AND
THE
MEASUREMENT
DISTRIBUTION
P
O
IS
GIVEN
BY
O
P
E
GET
TAKING
THE
NEGATIVE
LOGARITHM
OF
BOTH
SIDES
OF
AND
SETTING
Μ
FOR
SIMPLICITY
WE
LOG
P
O
LOG
P
O
LOG
P
C
O
C
WIENER
IS
PRONOUNCED
VEENER
SINCE
IN
GERMAN
THE
W
IS
PRONOUNCED
V
REMEMBER
THAT
NEXT
TIME
YOU
ORDER
WIENER
SCHNITZEL
A
B
FIGURE
ONE
DIMENSIONAL
WIENER
FILTER
A
POWER
SPECTRUM
OF
SIGNAL
PS
F
NOISE
LEVEL
AND
WIENER
FILTER
TRANSFORM
W
F
B
WIENER
FILTER
SPATIAL
KERNEL
WHICH
IS
THE
NEGATIVE
POSTERIOR
LOG
LIKELIHOOD
THE
MINIMUM
OF
THIS
QUANTITY
IS
EASY
TO
COMPUTE
PN
PS
SOPT
P
P
O
P
P
O
P
P
O
THE
QUANTITY
N
N
N
W
ΩX
ΩY
P
Ω
Ω
N
X
Y
IS
THE
FOURIER
TRANSFORM
OF
THE
OPTIMUM
WIENER
FILTER
NEEDED
TO
REMOVE
THE
NOISE
FROM
AN
IMAGE
WHOSE
POWER
SPECTRUM
IS
PS
ΩX
ΩY
NOTICE
THAT
THIS
FILTER
HAS
THE
RIGHT
QUALITATIVE
PROPERTIES
I
E
FOR
LOW
FREQUENCIES
WHERE
PS
IT
HAS
UNIT
GAIN
WHEREAS
FOR
HIGH
FREQUENCIES
IT
ATTENUATES
THE
NOISE
BY
A
FACTOR
PS
FIGURE
SHOWS
THE
ONE
DIMENSIONAL
TRANSFORM
W
F
AND
THE
CORRESPONDING
FILTER
KERNEL
W
X
FOR
THE
COMMONLY
ASSUMED
CASE
OF
P
F
F
FIELD
EXERCISE
HAS
YOU
COMPARE
THE
WIENER
FILTER
AS
A
DENOISING
ALGORITHM
TO
HAND
TUNED
GAUSSIAN
SMOOTHING
THE
METHODOLOGY
GIVEN
ABOVE
FOR
DERIVING
THE
WIENER
FILTER
CAN
EASILY
BE
EXTENDED
TO
THE
CASE
WHERE
THE
OBSERVED
IMAGE
IS
A
NOISY
BLURRED
VERSION
OF
THE
ORIGINAL
IMAGE
O
X
Y
B
X
Y
X
Y
N
X
Y
WHERE
B
X
Y
IS
THE
KNOWN
BLUR
KERNEL
RATHER
THAN
DERIVING
THE
CORRESPONDING
WIENER
FIL
TER
WE
LEAVE
IT
AS
AN
EXERCISE
EXERCISE
WHICH
ALSO
ENCOURAGES
YOU
TO
COMPARE
YOUR
DE
BLURRING
RESULTS
WITH
UNSHARP
MASKING
AND
NA
IVE
INVERSE
FILTERING
MORE
SOPHISTICATED
AL
GORITHMS
FOR
BLUR
REMOVAL
ARE
DISCUSSED
IN
SECTIONS
AND
DISCRETE
COSINE
TRANSFORM
THE
DISCRETE
COSINE
TRANSFORM
DCT
IS
A
VARIANT
OF
THE
FOURIER
TRANSFORM
PARTICULARLY
WELL
SUITED
TO
COMPRESSING
IMAGES
IN
A
BLOCK
WISE
FASHION
THE
ONE
DIMENSIONAL
DCT
IS
COM
PUTED
BY
TAKING
THE
DOT
PRODUCT
OF
EACH
N
WIDE
BLOCK
OF
PIXELS
WITH
A
SET
OF
COSINES
OF
FIGURE
DISCRETE
COSINE
TRANSFORM
DCT
BASIS
FUNCTIONS
THE
FIRST
DC
I
E
CONSTANT
BASIS
IS
THE
HORIZONTAL
BLUE
LINE
THE
SECOND
IS
THE
BROWN
HALF
CYCLE
WAVEFORM
ETC
THESE
BASES
ARE
WIDELY
USED
IN
IMAGE
AND
VIDEO
COMPRESSION
STANDARDS
SUCH
AS
JPEG
DIFFERENT
FREQUENCIES
F
K
N
I
COS
Π
I
N
K
F
I
WHERE
K
IS
THE
COEFFICIENT
FREQUENCY
INDEX
AND
THE
PIXEL
OFFSET
IS
USED
TO
MAKE
THE
BASIS
COEFFICIENTS
SYMMETRIC
WALLACE
SOME
OF
THE
DISCRETE
COSINE
BASIS
FUNCTIONS
ARE
SHOWN
IN
FIGURE
AS
YOU
CAN
SEE
THE
FIRST
BASIS
FUNCTION
THE
STRAIGHT
BLUE
LINE
ENCODES
THE
AVERAGE
DC
VALUE
IN
THE
BLOCK
OF
PIXELS
WHILE
THE
SECOND
ENCODES
A
SLIGHTLY
CURVY
VERSION
OF
THE
SLOPE
IN
TURNS
OUT
THAT
THE
DCT
IS
A
GOOD
APPROXIMATION
TO
THE
OPTIMAL
KARHUNEN
LOE
VE
DECOM
POSITION
OF
NATURAL
IMAGE
STATISTICS
OVER
SMALL
PATCHES
WHICH
CAN
BE
OBTAINED
BY
PERFORMING
A
PRINCIPAL
COMPONENT
ANALYSIS
PCA
OF
IMAGES
AS
DESCRIBED
IN
SECTION
THE
KL
TRANSFORM
DE
CORRELATES
THE
SIGNAL
OPTIMALLY
ASSUMING
THE
SIGNAL
IS
DESCRIBED
BY
ITS
SPECTRUM
AND
THUS
THEORETICALLY
LEADS
TO
OPTIMAL
COMPRESSION
THE
TWO
DIMENSIONAL
VERSION
OF
THE
DCT
IS
DEFINED
SIMILARLY
F
K
L
N
N
COS
Π
I
N
K
COS
Π
J
N
L
F
I
J
I
J
LIKE
THE
FAST
FOURIER
TRANSFORM
THE
DCT
CAN
BE
IMPLEMENTED
SEPARABLY
I
E
FIRST
COMPUTING
THE
DCT
OF
EACH
LINE
IN
THE
BLOCK
AND
THEN
COMPUTING
THE
DCT
OF
EACH
RESULTING
COLUMN
LIKE
THE
FFT
EACH
OF
THE
DCTS
CAN
ALSO
BE
COMPUTED
IN
O
N
LOG
N
TIME
AS
WE
MENTIONED
IN
SECTION
THE
DCT
IS
WIDELY
USED
IN
TODAY
IMAGE
AND
VIDEO
COMPRESSION
ALGORITHMS
ALTHOUGH
IT
IS
SLOWLY
BEING
SUPPLANTED
BY
WAVELET
ALGORITHMS
SI
MONCELLI
AND
ADELSON
AS
DISCUSSED
IN
SECTION
AND
OVERLAPPED
VARIANTS
OF
THE
DCT
MALVAR
WHICH
ARE
USED
IN
THE
NEW
JPEG
XR
STANDARD
THESE
HTTP
WWW
ITU
INT
REC
T
REC
T
I
EN
NEWER
ALGORITHMS
SUFFER
LESS
FROM
THE
BLOCKING
ARTIFACTS
VISIBLE
EDGE
ALIGNED
DISCONTINUITIES
THAT
RESULT
FROM
THE
PIXELS
IN
EACH
BLOCK
TYPICALLY
BEING
TRANSFORMED
AND
QUANTIZED
INDEPENDENTLY
SEE
EXERCISE
FOR
IDEAS
ON
HOW
TO
REMOVE
BLOCKING
ARTIFACTS
FROM
COM
PRESSED
JPEG
IMAGES
APPLICATION
SHARPENING
BLUR
AND
NOISE
REMOVAL
ANOTHER
COMMON
APPLICATION
OF
IMAGE
PROCESSING
IS
THE
ENHANCEMENT
OF
IMAGES
THROUGH
THE
USE
OF
SHARPENING
AND
NOISE
REMOVAL
OPERATIONS
WHICH
REQUIRE
SOME
KIND
OF
NEIGHBORHOOD
PROCESSING
TRADITIONALLY
THESE
KINDS
OF
OPERATION
WERE
PERFORMED
USING
LINEAR
FILTERING
SEE
SECTIONS
AND
SECTION
TODAY
IT
IS
MORE
COMMON
TO
USE
NON
LINEAR
FILTERS
SEC
TION
SUCH
AS
THE
WEIGHTED
MEDIAN
OR
BILATERAL
FILTER
ANISOTROPIC
DIFFUSION
OR
NON
LOCAL
MEANS
BUADES
COLL
AND
MOREL
VARIATIONAL
METHODS
SEC
TION
ESPECIALLY
THOSE
USING
NON
QUADRATIC
ROBUST
NORMS
SUCH
AS
THE
NORM
WHICH
IS
CALLED
TOTAL
VARIATION
ARE
ALSO
OFTEN
USED
FIGURE
SHOWS
SOME
EXAMPLES
OF
LINEAR
AND
NON
LINEAR
FILTERS
BEING
USED
TO
REMOVE
NOISE
WHEN
MEASURING
THE
EFFECTIVENESS
OF
IMAGE
DENOISING
ALGORITHMS
IT
IS
COMMON
TO
REPORT
THE
RESULTS
AS
A
PEAK
SIGNAL
TO
NOISE
RATIO
PSNR
MEASUREMENT
WHERE
I
X
IS
THE
ORIGINAL
NOISE
FREE
IMAGE
AND
Iˆ
X
IS
THE
IMAGE
AFTER
DENOISING
THIS
IS
FOR
THE
CASE
WHERE
THE
NOISY
IMAGE
HAS
BEEN
SYNTHETICALLY
GENERATED
SO
THAT
THE
CLEAN
IMAGE
IS
KNOWN
A
BETTER
WAY
TO
MEASURE
THE
QUALITY
IS
TO
USE
A
PERCEPTUALLY
BASED
SIMILARITY
METRIC
SUCH
AS
THE
STRUCTURAL
SIMILARITY
SSIM
INDEX
WANG
BOVIK
SHEIKH
ET
AL
WANG
BOVIK
AND
SIMONCELLI
EXERCISES
AND
HAVE
YOU
IMPLEMENT
SOME
OF
THESE
OPERATIONS
AND
COMPARE
THEIR
EFFECTIVENESS
MORE
SOPHISTICATED
TECHNIQUES
FOR
BLUR
REMOVAL
AND
THE
RELATED
TASK
OF
SUPER
RESOLUTION
ARE
DISCUSSED
IN
SECTION
PYRAMIDS
AND
WAVELETS
SO
FAR
IN
THIS
CHAPTER
ALL
OF
THE
IMAGE
TRANSFORMATIONS
WE
HAVE
STUDIED
PRODUCE
OUTPUT
IMAGES
OF
THE
SAME
SIZE
AS
THE
INPUTS
OFTEN
HOWEVER
WE
MAY
WISH
TO
CHANGE
THE
RESOLUTION
OF
AN
IMAGE
BEFORE
PROCEEDING
FURTHER
FOR
EXAMPLE
WE
MAY
NEED
TO
INTERPOLATE
A
SMALL
IMAGE
TO
MAKE
ITS
RESOLUTION
MATCH
THAT
OF
THE
OUTPUT
PRINTER
OR
COMPUTER
SCREEN
ALTERNATIVELY
WE
MAY
WANT
TO
REDUCE
THE
SIZE
OF
AN
IMAGE
TO
SPEED
UP
THE
EXECUTION
OF
AN
ALGORITHM
OR
TO
SAVE
ON
STORAGE
SPACE
OR
TRANSMISSION
TIME
SOMETIMES
WE
DO
NOT
EVEN
KNOW
WHAT
THE
APPROPRIATE
RESOLUTION
FOR
THE
IMAGE
SHOULD
BE
CONSIDER
FOR
EXAMPLE
THE
TASK
OF
FINDING
A
FACE
IN
AN
IMAGE
SECTION
SINCE
WE
DO
NOT
KNOW
THE
SCALE
AT
WHICH
THE
FACE
WILL
APPEAR
WE
NEED
TO
GENERATE
A
WHOLE
PYRAMID
OF
DIFFERENTLY
SIZED
IMAGES
AND
SCAN
EACH
ONE
FOR
POSSIBLE
FACES
BIOLOGICAL
VISUAL
SYSTEMS
ALSO
OPERATE
ON
A
HIERARCHY
OF
SCALES
MARR
SUCH
A
PYRAMID
CAN
ALSO
BE
VERY
HELPFUL
IN
ACCELERATING
THE
SEARCH
FOR
AN
OBJECT
BY
FIRST
FINDING
A
SMALLER
INSTANCE
OF
THAT
OBJECT
AT
A
COARSER
LEVEL
OF
THE
PYRAMID
AND
THEN
LOOKING
FOR
THE
FULL
RESOLUTION
OBJECT
ONLY
IN
THE
VICINITY
OF
COARSE
LEVEL
DETECTIONS
SECTION
FINALLY
IMAGE
PYRAMIDS
ARE
EXTREMELY
USEFUL
FOR
PERFORMING
MULTI
SCALE
EDITING
OPERATIONS
SUCH
AS
BLENDING
IMAGES
WHILE
MAINTAINING
DETAILS
IN
THIS
SECTION
WE
FIRST
DISCUSS
GOOD
FILTERS
FOR
CHANGING
IMAGE
RESOLUTION
I
E
UPSAMPLING
INTERPOLATION
SECTION
AND
DOWNSAMPLING
DECIMATION
SECTION
WE
THEN
PRESENT
THE
CONCEPT
OF
MULTI
RESOLUTION
PYRAMIDS
WHICH
CAN
BE
USED
TO
CREATE
A
COMPLETE
HIERARCHY
OF
DIFFERENTLY
SIZED
IMAGES
AND
TO
ENABLE
A
VARIETY
OF
APPLICATIONS
SECTION
A
CLOSELY
RELATED
CONCEPT
IS
THAT
OF
WAVELETS
WHICH
ARE
A
SPECIAL
KIND
OF
PYRAMID
WITH
HIGHER
FREQUENCY
SELECTIVITY
AND
OTHER
USEFUL
PROPERTIES
SECTION
FINALLY
WE
PRESENT
A
USEFUL
APPLICATION
OF
PYRAMIDS
NAMELY
THE
BLENDING
OF
DIFFERENT
IMAGES
IN
A
WAY
THAT
HIDES
THE
SEAMS
BETWEEN
THE
IMAGE
BOUNDARIES
SECTION
INTERPOLATION
IN
ORDER
TO
INTERPOLATE
OR
UPSAMPLE
AN
IMAGE
TO
A
HIGHER
RESOLUTION
WE
NEED
TO
SELECT
SOME
INTERPOLATION
KERNEL
WITH
WHICH
TO
CONVOLVE
THE
IMAGE
G
I
J
F
K
L
H
I
RK
J
RL
K
L
THIS
FORMULA
IS
RELATED
TO
THE
DISCRETE
CONVOLUTION
FORMULA
EXCEPT
THAT
WE
REPLACE
K
AND
L
IN
H
WITH
RK
AND
RL
WHERE
R
IS
THE
UPSAMPLING
RATE
FIGURE
SHOWS
HOW
TO
THINK
OF
THIS
PROCESS
AS
THE
SUPERPOSITION
OF
SAMPLE
WEIGHTED
INTERPOLATION
KERNELS
ONE
CENTERED
AT
EACH
INPUT
SAMPLE
K
AN
ALTERNATIVE
MENTAL
MODEL
IS
SHOWN
IN
FIGURE
WHERE
THE
KERNEL
IS
CENTERED
AT
THE
OUTPUT
PIXEL
VALUE
I
THE
TWO
FORMS
ARE
EQUIVALENT
THE
LATTER
FORM
IS
SOMETIMES
CALLED
THE
POLYPHASE
FILTER
FORM
SINCE
THE
KERNEL
VALUES
H
I
CAN
BE
STORED
AS
R
SEPARATE
KERNELS
EACH
OF
WHICH
IS
SELECTED
FOR
CONVOLUTION
WITH
THE
INPUT
SAMPLES
DEPENDING
ON
THE
PHASE
OF
I
RELATIVE
TO
THE
UPSAMPLED
GRID
WHAT
KINDS
OF
KERNEL
MAKE
GOOD
INTERPOLATORS
THE
ANSWER
DEPENDS
ON
THE
APPLICATION
AND
THE
COMPUTATION
TIME
INVOLVED
ANY
OF
THE
SMOOTHING
KERNELS
SHOWN
IN
TABLES
AND
CAN
BE
USED
AFTER
APPROPRIATE
RE
SCALING
THE
LINEAR
INTERPOLATOR
CORRESPONDING
TO
THE
TENT
KERNEL
PRODUCES
INTERPOLATING
PIECEWISE
LINEAR
CURVES
WHICH
RESULT
IN
UNAPPEALING
CREASES
WHEN
APPLIED
TO
IMAGES
FIGURE
THE
CUBIC
B
SPLINE
WHOSE
DISCRETE
PIXEL
SAM
PLING
APPEARS
AS
THE
BINOMIAL
KERNEL
IN
TABLE
IS
AN
APPROXIMATING
KERNEL
THE
INTERPOLATED
THE
SMOOTHING
KERNELS
IN
TABLE
HAVE
A
UNIT
AREA
TO
TURN
THEM
INTO
INTERPOLATING
KERNELS
WE
SIMPLY
SCALE
THEM
UP
BY
THE
INTERPOLATION
RATE
R
F
K
F
K
F
K
F
K
G
I
R
K
RK
I
R
K
R
K
RK
I
R
K
A
B
FIGURE
SIGNAL
INTERPOLATION
G
I
K
F
K
H
I
RK
A
WEIGHTED
SUMMATION
OF
INPUT
VALUES
B
POLYPHASE
FILTER
INTERPRETATION
IMAGE
DOES
NOT
PASS
THROUGH
THE
INPUT
DATA
POINTS
THAT
PRODUCES
SOFT
IMAGES
WITH
REDUCED
HIGH
FREQUENCY
DETAIL
THE
EQUATION
FOR
THE
CUBIC
B
SPLINE
IS
EASIEST
TO
DERIVE
BY
CONVOLVING
THE
TENT
FUNCTION
LINEAR
B
SPLINE
WITH
ITSELF
WHILE
MOST
GRAPHICS
CARDS
USE
THE
BILINEAR
KERNEL
OPTIONALLY
COMBINED
WITH
A
MIP
MAP
SEE
SECTION
MOST
PHOTO
EDITING
PACKAGES
USE
BICUBIC
INTERPOLATION
THE
CU
BIC
INTERPOLANT
IS
A
DERIVATIVE
CONTINUOUS
PIECEWISE
CUBIC
SPLINE
THE
TERM
SPLINE
IS
SYNONYMOUS
WITH
PIECEWISE
POLYNOMIAL
WHOSE
EQUATION
IS
H
X
A
X
X
IF
X
OTHERWISE
WHERE
A
SPECIFIES
THE
DERIVATIVE
AT
X
PARKER
KENYON
AND
TROXEL
THE
VALUE
OF
A
IS
OFTEN
SET
TO
SINCE
THIS
BEST
MATCHES
THE
FREQUENCY
CHARACTERISTICS
OF
A
SINC
FUNCTION
FIGURE
IT
ALSO
INTRODUCES
A
SMALL
AMOUNT
OF
SHARPENING
WHICH
CAN
BE
VISUALLY
APPEAL
ING
UNFORTUNATELY
THIS
CHOICE
DOES
NOT
LINEARLY
INTERPOLATE
STRAIGHT
LINES
INTENSITY
RAMPS
SO
SOME
VISIBLE
RINGING
MAY
OCCUR
A
BETTER
CHOICE
FOR
LARGE
AMOUNTS
OF
INTERPOLATION
IS
PROB
ABLY
A
WHICH
PRODUCES
A
QUADRATIC
REPRODUCING
SPLINE
IT
INTERPOLATES
LINEAR
AND
QUADRATIC
FUNCTIONS
EXACTLY
WOLBERG
SECTION
FIGURE
SHOWS
THE
A
AND
A
CUBIC
INTERPOLATING
KERNEL
ALONG
WITH
THEIR
FOURIER
TRANSFORMS
FIGURE
AND
C
SHOWS
THEM
BEING
APPLIED
TO
TWO
DIMENSIONAL
INTERPOLATION
SPLINES
HAVE
LONG
BEEN
USED
FOR
FUNCTION
AND
DATA
VALUE
INTERPOLATION
BECAUSE
OF
THE
ABIL
ITY
TO
PRECISELY
SPECIFY
DERIVATIVES
AT
CONTROL
POINTS
AND
EFFICIENT
INCREMENTAL
ALGORITHMS
FOR
THEIR
EVALUATION
BARTELS
BEATTY
AND
BARSKY
FARIN
SPLINES
ARE
WIDELY
USED
IN
GEOMETRIC
MODELING
AND
COMPUTER
AIDED
DESIGN
CAD
APPLICATIONS
ALTHOUGH
THEY
HAVE
THE
TERM
SPLINE
COMES
FROM
THE
DRAUGHTSMAN
WORKSHOP
WHERE
IT
WAS
THE
NAME
OF
A
FLEXIBLE
PIECE
OF
WOOD
OR
METAL
USED
TO
DRAW
SMOOTH
CURVES
A
B
C
D
FIGURE
TWO
DIMENSIONAL
IMAGE
INTERPOLATION
A
BILINEAR
B
BICUBIC
A
C
BICUBIC
A
D
WINDOWED
SINC
NINE
TAPS
WINDOWED
SINC
TENT
CUBIC
A
CUBIC
A
WINDOWED
SINC
CUBIC
A
TENT
CUBIC
A
A
B
FIGURE
A
SOME
WINDOWED
SINC
FUNCTIONS
AND
B
THEIR
LOG
FOURIER
TRANSFORMS
RAISED
COSINE
WINDOWED
SINC
IN
BLUE
CUBIC
INTERPOLATORS
A
AND
A
IN
GREEN
AND
PURPLE
AND
TENT
FUNCTION
IN
BROWN
THEY
ARE
OFTEN
USED
TO
PERFORM
HIGH
ACCURACY
LOW
PASS
FILTERING
OPERATIONS
STARTED
BEING
DISPLACED
BY
SUBDIVISION
SURFACES
ZORIN
SCHRO
DER
AND
SWELDENS
PETERS
AND
REIF
IN
COMPUTER
VISION
SPLINES
ARE
OFTEN
USED
FOR
ELASTIC
IMAGE
DEFORMATIONS
SECTION
MOTION
ESTIMATION
SECTION
AND
SURFACE
INTERPOLATION
SECTION
IN
FACT
IT
IS
POSSIBLE
TO
CARRY
OUT
MOST
IMAGE
PROCESSING
OPERATIONS
BY
REPRESENTING
IMAGES
AS
SPLINES
AND
MANIPULATING
THEM
IN
A
MULTI
RESOLUTION
FRAMEWORK
UNSER
THE
HIGHEST
QUALITY
INTERPOLATOR
IS
GENERALLY
BELIEVED
TO
BE
THE
WINDOWED
SINC
FUNCTION
BECAUSE
IT
BOTH
PRESERVES
DETAILS
IN
THE
LOWER
RESOLUTION
IMAGE
AND
AVOIDS
ALIASING
IT
IS
ALSO
POSSIBLE
TO
CONSTRUCT
A
PIECEWISE
CUBIC
APPROXIMATION
TO
THE
WINDOWED
SINC
BY
MATCHING
ITS
DERIVATIVES
AT
ZERO
CROSSING
SZELISKI
AND
ITO
HOWEVER
SOME
PEOPLE
OBJECT
TO
THE
EXCESSIVE
RINGING
THAT
CAN
BE
INTRODUCED
BY
THE
WINDOWED
SINC
AND
TO
THE
REPETITIVE
NATURE
OF
THE
RINGING
FREQUENCIES
SEE
FIGURE
FOR
THIS
REASON
SOME
PHOTOGRAPHERS
PREFER
TO
REPEATEDLY
INTERPOLATE
IMAGES
BY
A
SMALL
FRACTIONAL
AMOUNT
THIS
TENDS
TO
DE
CORRELATE
THE
ORIGINAL
PIXEL
GRID
WITH
THE
FINAL
IMAGE
ADDITIONAL
POSSIBILITIES
INCLUDE
USING
THE
BILAT
ERAL
FILTER
AS
AN
INTERPOLATOR
KOPF
COHEN
LISCHINSKI
ET
AL
USING
GLOBAL
OPTIMIZATION
SECTION
OR
HALLUCINATING
DETAILS
SECTION
DECIMATION
WHILE
INTERPOLATION
CAN
BE
USED
TO
INCREASE
THE
RESOLUTION
OF
AN
IMAGE
DECIMATION
DOWNSAM
PLING
IS
REQUIRED
TO
REDUCE
THE
RESOLUTION
TO
PERFORM
DECIMATION
WE
FIRST
CONCEPTUALLY
CONVOLVE
THE
IMAGE
WITH
A
LOW
PASS
FILTER
TO
AVOID
ALIASING
AND
THEN
KEEP
EVERY
RTH
SAMPLE
IN
PRACTICE
WE
USUALLY
ONLY
EVALUATE
THE
CONVOLUTION
AT
EVERY
RTH
SAMPLE
G
I
J
F
K
L
H
RI
K
RJ
L
K
L
AS
SHOWN
IN
FIGURE
NOTE
THAT
THE
SMOOTHING
KERNEL
H
K
L
IN
THIS
CASE
IS
OFTEN
A
STRETCHED
AND
RE
SCALED
VERSION
OF
AN
INTERPOLATION
KERNEL
ALTERNATIVELY
WE
CAN
WRITE
G
I
J
F
K
L
H
I
K
R
J
L
R
K
L
AND
KEEP
THE
SAME
KERNEL
H
K
L
FOR
BOTH
INTERPOLATION
AND
DECIMATION
ONE
COMMONLY
USED
R
DECIMATION
FILTER
IS
THE
BINOMIAL
FILTER
INTRODUCED
BY
BURT
AND
ADELSON
AS
SHOWN
IN
TABLE
THIS
KERNEL
DOES
A
DECENT
JOB
OF
SEPARATING
THE
HIGH
AND
LOW
FREQUENCIES
BUT
STILL
LEAVES
A
FAIR
AMOUNT
OF
HIGH
FREQUENCY
DETAIL
WHICH
CAN
LEAD
TO
ALIASING
AFTER
DOWNSAMPLING
HOWEVER
FOR
APPLICATIONS
SUCH
AS
IMAGE
BLENDING
DISCUSSED
LATER
IN
THIS
SECTION
THIS
ALIASING
IS
OF
LITTLE
CONCERN
THE
TERM
DECIMATION
HAS
A
GRUESOME
ETYMOLOGY
RELATING
TO
THE
PRACTICE
OF
KILLING
EVERY
TENTH
SOLDIER
IN
A
ROMAN
UNIT
GUILTY
OF
COWARDICE
IT
IS
GENERALLY
USED
IN
SIGNAL
PROCESSING
TO
MEAN
ANY
DOWNSAMPLING
OR
RATE
REDUCTION
OPERATION
A
B
FIGURE
SIGNAL
DECIMATION
A
THE
ORIGINAL
SAMPLES
ARE
B
CONVOLVED
WITH
A
LOW
PASS
FILTER
BEFORE
BEING
DOWNSAMPLED
IF
HOWEVER
THE
DOWNSAMPLED
IMAGES
WILL
BE
DISPLAYED
DIRECTLY
TO
THE
USER
OR
PERHAPS
BLENDED
WITH
OTHER
RESOLUTIONS
AS
IN
MIP
MAPPING
SECTION
A
HIGHER
QUALITY
FILTER
IS
DESIRED
FOR
HIGH
DOWNSAMPLING
RATES
THE
WINDOWED
SINC
PRE
FILTER
IS
A
GOOD
CHOICE
FIG
URE
HOWEVER
FOR
SMALL
DOWNSAMPLING
RATES
E
G
R
MORE
CAREFUL
FILTER
DESIGN
IS
REQUIRED
TABLE
SHOWS
A
NUMBER
OF
COMMONLY
USED
R
DOWNSAMPLING
FILTERS
WHILE
FIG
URE
SHOWS
THEIR
CORRESPONDING
FREQUENCY
RESPONSES
THESE
FILTERS
INCLUDE
THE
LINEAR
FILTER
GIVES
A
RELATIVELY
POOR
RESPONSE
THE
BINOMIAL
FILTER
CUTS
OFF
A
LOT
OF
FREQUENCIES
BUT
IS
USEFUL
FOR
COMPUTER
VISION
ANALYSIS
PYRAMIDS
THE
CUBIC
FILTERS
FROM
THE
A
FILTER
HAS
A
SHARPER
FALL
OFF
THAN
THE
A
FILTER
FIGURE
TABLE
FILTER
COEFFICIENTS
FOR
DECIMATION
THESE
FILTERS
ARE
OF
ODD
LENGTH
ARE
SYM
METRIC
AND
ARE
NORMALIZED
TO
HAVE
UNIT
DC
GAIN
SUM
UP
TO
SEE
FIGURE
FOR
THEIR
ASSOCIATED
FREQUENCY
RESPONSES
FIGURE
FREQUENCY
RESPONSE
FOR
SOME
DECIMATION
FILTERS
THE
CUBIC
A
FILTER
HAS
THE
SHARPEST
FALL
OFF
BUT
ALSO
A
BIT
OF
RINGING
THE
WAVELET
ANALYSIS
FILTERS
QMF
AND
JPEG
WHILE
USEFUL
FOR
COMPRESSION
HAVE
MORE
ALIASING
A
COSINE
WINDOWED
SINC
FUNCTION
TABLE
THE
QMF
FILTER
OF
SIMONCELLI
AND
ADELSON
IS
USED
FOR
WAVELET
DENOISING
AND
ALIASES
A
FAIR
AMOUNT
NOTE
THAT
THE
ORIGINAL
FILTER
COEFFICIENTS
ARE
NORMALIZED
TO
GAIN
SO
THEY
CAN
BE
SELF
INVERTING
THE
ANALYSIS
FILTER
FROM
JPEG
TAUBMAN
AND
MARCELLIN
PLEASE
SEE
THE
ORIGINAL
PAPERS
FOR
THE
FULL
PRECISION
VALUES
OF
SOME
OF
THESE
COEFFICIENTS
MULTI
RESOLUTION
REPRESENTATIONS
NOW
THAT
WE
HAVE
DESCRIBED
INTERPOLATION
AND
DECIMATION
ALGORITHMS
WE
CAN
BUILD
A
COMPLETE
IMAGE
PYRAMID
FIGURE
AS
WE
MENTIONED
BEFORE
PYRAMIDS
CAN
BE
USED
TO
ACCELERATE
COARSE
TO
FINE
SEARCH
ALGORITHMS
TO
LOOK
FOR
OBJECTS
OR
PATTERNS
AT
DIFFERENT
SCALES
AND
TO
PER
FORM
MULTI
RESOLUTION
BLENDING
OPERATIONS
THEY
ARE
ALSO
WIDELY
USED
IN
COMPUTER
GRAPHICS
HARDWARE
AND
SOFTWARE
TO
PERFORM
FRACTIONAL
LEVEL
DECIMATION
USING
THE
MIP
MAP
WHICH
WE
COVER
IN
SECTION
THE
BEST
KNOWN
AND
PROBABLY
MOST
WIDELY
USED
PYRAMID
IN
COMPUTER
VISION
IS
BURT
AND
ADELSON
LAPLACIAN
PYRAMID
TO
CONSTRUCT
THE
PYRAMID
WE
FIRST
BLUR
AND
SUB
SAMPLE
THE
ORIGINAL
IMAGE
BY
A
FACTOR
OF
TWO
AND
STORE
THIS
IN
THE
NEXT
LEVEL
OF
THE
PYRAMID
FIGURE
BECAUSE
ADJACENT
LEVELS
IN
THE
PYRAMID
ARE
RELATED
BY
A
SAMPLING
RATE
R
THIS
KIND
OF
PYRAMID
IS
KNOWN
AS
AN
OCTAVE
PYRAMID
BURT
AND
ADELSON
ORIGINALLY
PROPOSED
A
COARSE
L
MEDIUM
L
FINE
L
FIGURE
A
TRADITIONAL
IMAGE
PYRAMID
EACH
LEVEL
HAS
HALF
THE
RESOLUTION
WIDTH
AND
HEIGHT
AND
HENCE
A
QUARTER
OF
THE
PIXELS
OF
ITS
PARENT
LEVEL
FIVE
TAP
KERNEL
OF
THE
FORM
WITH
B
AND
C
A
IN
PRACTICE
A
WHICH
RESULTS
IN
THE
FAMILIAR
BINOMIAL
KERNEL
WHICH
IS
PARTICULARLY
EASY
TO
IMPLEMENT
USING
SHIFTS
AND
ADDS
THIS
WAS
IMPORTANT
IN
THE
DAYS
WHEN
MULTIPLIERS
WERE
EXPENSIVE
THE
REASON
THEY
CALL
THEIR
RESULTING
PYRAMID
A
GAUSSIAN
PYRAMID
IS
THAT
REPEATED
CONVOLUTIONS
OF
THE
BINOMIAL
KERNEL
CONVERGE
TO
A
GAUSSIAN
TO
COMPUTE
THE
LAPLACIAN
PYRAMID
BURT
AND
ADELSON
FIRST
INTERPOLATE
A
LOWER
RESOLU
TION
IMAGE
TO
OBTAIN
A
RECONSTRUCTED
LOW
PASS
VERSION
OF
THE
ORIGINAL
IMAGE
FIGURE
THEY
THEN
SUBTRACT
THIS
LOW
PASS
VERSION
FROM
THE
ORIGINAL
TO
YIELD
THE
BAND
PASS
LAPLACIAN
IMAGE
WHICH
CAN
BE
STORED
AWAY
FOR
FURTHER
PROCESSING
THE
RESULTING
PYRAMID
HAS
PERFECT
RECONSTRUCTION
I
E
THE
LAPLACIAN
IMAGES
PLUS
THE
BASE
LEVEL
GAUSSIAN
IN
FIGURE
ARE
SUFFICIENT
TO
EXACTLY
RECONSTRUCT
THE
ORIGINAL
IMAGE
FIGURE
SHOWS
THE
SAME
COM
PUTATION
IN
ONE
DIMENSION
AS
A
SIGNAL
PROCESSING
DIAGRAM
WHICH
COMPLETELY
CAPTURES
THE
COMPUTATIONS
BEING
PERFORMED
DURING
THE
ANALYSIS
AND
RE
SYNTHESIS
STAGES
BURT
AND
ADELSON
ALSO
DESCRIBE
A
VARIANT
ON
THE
LAPLACIAN
PYRAMID
WHERE
THE
LOW
PASS
IMAGE
IS
TAKEN
FROM
THE
ORIGINAL
BLURRED
IMAGE
RATHER
THAN
THE
RECONSTRUCTED
PYRAMID
PIPING
THE
OUTPUT
OF
THE
L
BOX
DIRECTLY
TO
THE
SUBTRACTION
IN
FIGURE
THIS
VARIANT
HAS
LESS
THEN
AGAIN
THIS
IS
TRUE
FOR
ANY
SMOOTHING
KERNEL
WELLS
A
B
FIGURE
THE
GAUSSIAN
PYRAMID
SHOWN
AS
A
SIGNAL
PROCESSING
DIAGRAM
THE
A
ANALYSIS
AND
B
RE
SYNTHESIS
STAGES
ARE
SHOWN
AS
USING
SIMILAR
COMPUTATIONS
THE
WHITE
CIRCLES
IN
DICATE
ZERO
VALUES
INSERTED
BY
THE
UPSAMPLING
OPERATION
NOTICE
HOW
THE
RECONSTRUCTION
FILTER
COEFFICIENTS
ARE
TWICE
THE
ANALYSIS
COEFFICIENTS
THE
COMPUTATION
IS
SHOWN
AS
FLOWING
DOWN
THE
PAGE
REGARDLESS
OF
WHETHER
WE
ARE
GOING
FROM
COARSE
TO
FINE
OR
VICE
VERSA
ALIASING
SINCE
IT
AVOIDS
ONE
DOWNSAMPLING
AND
UPSAMPLING
ROUND
TRIP
BUT
IT
IS
NOT
SELF
INVERTING
SINCE
THE
LAPLACIAN
IMAGES
ARE
NO
LONGER
ADEQUATE
TO
REPRODUCE
THE
ORIGINAL
IMAGE
AS
WITH
THE
GAUSSIAN
PYRAMID
THE
TERM
LAPLACIAN
IS
A
BIT
OF
A
MISNOMER
SINCE
THEIR
BAND
PASS
IMAGES
ARE
REALLY
DIFFERENCES
OF
APPROXIMATE
GAUSSIANS
OR
DOGS
DOG
I
I
I
I
A
LAPLACIAN
OF
GAUSSIAN
WHICH
WE
SAW
IN
IS
ACTUALLY
ITS
SECOND
DERIVATIVE
LOG
I
Σ
GΣ
I
I
WHERE
IS
THE
LAPLACIAN
OPERATOR
OF
A
FUNCTION
FIGURE
SHOWS
HOW
THE
DIFFERENCES
OF
GAUSSIAN
AND
LAPLACIANS
OF
GAUSSIAN
LOOK
IN
BOTH
SPACE
AND
FREQUENCY
LAPLACIANS
OF
GAUSSIAN
HAVE
ELEGANT
MATHEMATICAL
PROPERTIES
WHICH
HAVE
BEEN
WIDELY
STUDIED
IN
THE
SCALE
SPACE
COMMUNITY
WITKIN
WITKIN
TERZOPOULOS
AND
KASS
LINDEBERG
NIELSEN
FLORACK
AND
DERICHE
AND
CAN
BE
USED
FOR
A
VARIETY
OF
APPLI
CATIONS
INCLUDING
EDGE
DETECTION
MARR
AND
HILDRETH
PERONA
AND
MALIK
STEREO
MATCHING
WITKIN
TERZOPOULOS
AND
KASS
AND
IMAGE
ENHANCEMENT
NIELSEN
FLORACK
AND
DERICHE
A
LESS
WIDELY
USED
VARIANT
IS
HALF
OCTAVE
PYRAMIDS
SHOWN
IN
FIGURE
THESE
WERE
FIRST
INTRODUCED
TO
THE
VISION
COMMUNITY
BY
CROWLEY
AND
STERN
WHO
CALL
THEM
DIF
FERENCE
OF
LOW
PASS
DOLP
TRANSFORMS
BECAUSE
OF
THE
SMALL
SCALE
CHANGE
BETWEEN
ADJA
A
B
FIGURE
THE
LAPLACIAN
PYRAMID
A
THE
CONCEPTUAL
FLOW
OF
IMAGES
THROUGH
PROCESSING
STAGES
IMAGES
ARE
HIGH
PASS
AND
LOW
PASS
FILTERED
AND
THE
LOW
PASS
FILTERED
IMAGES
ARE
PRO
CESSED
IN
THE
NEXT
STAGE
OF
THE
PYRAMID
DURING
RECONSTRUCTION
THE
INTERPOLATED
IMAGE
AND
THE
OPTIONALLY
FILTERED
HIGH
PASS
IMAGE
ARE
ADDED
BACK
TOGETHER
THE
Q
BOX
INDICATES
QUANTIZA
TION
OR
SOME
OTHER
PYRAMID
PROCESSING
E
G
NOISE
REMOVAL
BY
CORING
SETTING
SMALL
WAVELET
VALUES
TO
B
THE
ACTUAL
COMPUTATION
OF
THE
HIGH
PASS
FILTER
INVOLVES
FIRST
INTERPOLATING
THE
DOWNSAMPLED
LOW
PASS
IMAGE
AND
THEN
SUBTRACTING
IT
THIS
RESULTS
IN
PERFECT
RECONSTRUCTION
WHEN
Q
IS
THE
IDENTITY
THE
HIGH
PASS
OR
BAND
PASS
IMAGES
ARE
TYPICALLY
CALLED
LAPLACIAN
IMAGES
WHILE
THE
LOW
PASS
IMAGES
ARE
CALLED
GAUSSIAN
IMAGES
SPACE
FREQUENCY
LOW
PASS
LOWER
PASS
FIGURE
THE
DIFFERENCE
OF
TWO
LOW
PASS
FILTERS
RESULTS
IN
A
BAND
PASS
FILTER
THE
DASHED
BLUE
LINES
SHOW
THE
CLOSE
FIT
TO
A
HALF
OCTAVE
LAPLACIAN
OF
GAUSSIAN
CENT
LEVELS
THE
AUTHORS
CLAIM
THAT
COARSE
TO
FINE
ALGORITHMS
PERFORM
BETTER
IN
THE
IMAGE
PROCESSING
COMMUNITY
HALF
OCTAVE
PYRAMIDS
COMBINED
WITH
CHECKERBOARD
SAMPLING
GRIDS
ARE
KNOWN
AS
QUINCUNX
SAMPLING
FEILNER
VAN
DE
VILLE
AND
UNSER
IN
DETECTING
MULTI
SCALE
FEATURES
SECTION
IT
IS
OFTEN
COMMON
TO
USE
HALF
OCTAVE
OR
EVEN
QUARTER
OCTAVE
PYRAMIDS
LOWE
TRIGGS
HOWEVER
IN
THIS
CASE
THE
SUBSAMPLING
ONLY
OCCURS
AT
EVERY
OCTAVE
LEVEL
I
E
THE
IMAGE
IS
REPEATEDLY
BLURRED
WITH
WIDER
GAUSSIANS
UNTIL
A
FULL
OCTAVE
OF
RESOLUTION
CHANGE
HAS
BEEN
ACHIEVED
FIGURE
WAVELETS
WHILE
PYRAMIDS
ARE
USED
EXTENSIVELY
IN
COMPUTER
VISION
APPLICATIONS
SOME
PEOPLE
USE
WAVELET
DECOMPOSITIONS
AS
AN
ALTERNATIVE
WAVELETS
ARE
FILTERS
THAT
LOCALIZE
A
SIGNAL
IN
BOTH
SPACE
AND
FREQUENCY
LIKE
THE
GABOR
FILTER
IN
TABLE
AND
ARE
DEFINED
OVER
A
HIERARCHY
OF
SCALES
WAVELETS
PROVIDE
A
SMOOTH
WAY
TO
DECOMPOSE
A
SIGNAL
INTO
FREQUENCY
COMPONENTS
WITHOUT
BLOCKING
AND
ARE
CLOSELY
RELATED
TO
PYRAMIDS
WAVELETS
WERE
ORIGINALLY
DEVELOPED
IN
THE
APPLIED
MATH
AND
SIGNAL
PROCESSING
COMMUNI
TIES
AND
WERE
INTRODUCED
TO
THE
COMPUTER
VISION
COMMUNITY
BY
MALLAT
STRANG
SIMONCELLI
AND
ADELSON
RIOUL
AND
VETTERLI
CHUI
MEYER
ALL
PROVIDE
NICE
INTRODUCTIONS
TO
THE
SUBJECT
ALONG
WITH
HISTORICAL
REVIEWS
WHILE
CHUI
PRO
VIDES
A
MORE
COMPREHENSIVE
REVIEW
AND
SURVEY
OF
APPLICATIONS
SWELDENS
DESCRIBES
THE
MORE
RECENT
LIFTING
APPROACH
TO
WAVELETS
THAT
WE
DISCUSS
SHORTLY
WAVELETS
ARE
WIDELY
USED
IN
THE
COMPUTER
GRAPHICS
COMMUNITY
TO
PERFORM
MULTI
RESOLUTION
GEOMETRIC
PROCESSING
STOLLNITZ
DEROSE
AND
SALESIN
AND
HAVE
ALSO
BEEN
USED
IN
COM
PUTER
VISION
FOR
SIMILAR
APPLICATIONS
SZELISKI
PENTLAND
GORTLER
AND
COHEN
YAOU
AND
CHANG
LAI
AND
VEMURI
SZELISKI
AS
WELL
AS
FOR
MULTI
SCALE
ORI
ENTED
FILTERING
SIMONCELLI
FREEMAN
ADELSON
ET
AL
AND
DENOISING
PORTILLA
STRELA
COARSE
MEDIUM
L
L
L
COARSE
MEDIUM
L
L
L
FINE
L
FINE
L
A
B
FIGURE
MULTIRESOLUTION
PYRAMIDS
A
PYRAMID
WITH
HALF
OCTAVE
QUINCUNX
SAMPLING
ODD
LEVELS
ARE
COLORED
GRAY
FOR
CLARITY
B
WAVELET
PYRAMID
EACH
WAVELET
LEVEL
STORES
OF
THE
ORIGINAL
PIXELS
USUALLY
THE
HORIZONTAL
VERTICAL
AND
MIXED
GRADIENTS
SO
THAT
THE
TOTAL
NUMBER
OF
WAVELET
COEFFICIENTS
AND
ORIGINAL
PIXELS
IS
THE
SAME
WAINWRIGHT
ET
AL
SINCE
BOTH
IMAGE
PYRAMIDS
AND
WAVELETS
DECOMPOSE
AN
IMAGE
INTO
MULTI
RESOLUTION
DE
SCRIPTIONS
THAT
ARE
LOCALIZED
IN
BOTH
SPACE
AND
FREQUENCY
HOW
DO
THEY
DIFFER
THE
USUAL
ANSWER
IS
THAT
TRADITIONAL
PYRAMIDS
ARE
OVERCOMPLETE
I
E
THEY
USE
MORE
PIXELS
THAN
THE
ORIG
INAL
IMAGE
TO
REPRESENT
THE
DECOMPOSITION
WHEREAS
WAVELETS
PROVIDE
A
TIGHT
FRAME
I
E
THEY
KEEP
THE
SIZE
OF
THE
DECOMPOSITION
THE
SAME
AS
THE
IMAGE
FIGURE
HOWEVER
SOME
WAVELET
FAMILIES
ARE
IN
FACT
OVERCOMPLETE
IN
ORDER
TO
PROVIDE
BETTER
SHIFTABILITY
OR
STEERING
IN
ORIENTATION
SIMONCELLI
FREEMAN
ADELSON
ET
AL
A
BETTER
DISTINCTION
THEREFORE
MIGHT
BE
THAT
WAVELETS
ARE
MORE
ORIENTATION
SELECTIVE
THAN
REGULAR
BAND
PASS
PYRAMIDS
HOW
ARE
TWO
DIMENSIONAL
WAVELETS
CONSTRUCTED
FIGURE
SHOWS
A
HIGH
LEVEL
DIA
GRAM
OF
ONE
STAGE
OF
THE
RECURSIVE
COARSE
TO
FINE
CONSTRUCTION
ANALYSIS
PIPELINE
ALONGSIDE
THE
COMPLEMENTARY
RE
CONSTRUCTION
SYNTHESIS
STAGE
IN
THIS
DIAGRAM
THE
HIGH
PASS
FILTER
FOLLOWED
BY
DECIMATION
KEEPS
OF
THE
ORIGINAL
PIXELS
WHILE
OF
THE
LOW
FREQUENCY
COEF
FICIENTS
ARE
PASSED
ON
TO
THE
NEXT
STAGE
FOR
FURTHER
ANALYSIS
IN
PRACTICE
THE
FILTERING
IS
USUALLY
BROKEN
DOWN
INTO
TWO
SEPARABLE
SUB
STAGES
AS
SHOWN
IN
FIGURE
THE
RESULTING
THREE
WAVELET
IMAGES
ARE
SOMETIMES
CALLED
THE
HIGH
HIGH
HH
HIGH
LOW
HL
AND
LOW
HIGH
LH
IMAGES
THE
HIGH
LOW
AND
LOW
HIGH
IMAGES
ACCENTUATE
THE
HORIZONTAL
AND
VERTICAL
EDGES
AND
GRADIENTS
WHILE
THE
HIGH
HIGH
IMAGE
CONTAINS
THE
LESS
FREQUENTLY
OCCURRING
MIXED
DERIVATIVES
HOW
ARE
THE
HIGH
PASS
H
AND
LOW
PASS
L
FILTERS
SHOWN
IN
FIGURE
CHOSEN
AND
HOW
CAN
THE
CORRESPONDING
RECONSTRUCTION
FILTERS
I
AND
F
BE
COMPUTED
CAN
FILTERS
BE
DESIGNED
A
B
FIGURE
TWO
DIMENSIONAL
WAVELET
DECOMPOSITION
A
HIGH
LEVEL
DIAGRAM
SHOWING
THE
LOW
PASS
AND
HIGH
PASS
TRANSFORMS
AS
SINGLE
BOXES
B
SEPARABLE
IMPLEMENTATION
WHICH
IN
VOLVES
FIRST
PERFORMING
THE
WAVELET
TRANSFORM
HORIZONTALLY
AND
THEN
VERTICALLY
THE
I
AND
F
BOXES
ARE
THE
INTERPOLATION
AND
FILTERING
BOXES
REQUIRED
TO
RE
SYNTHESIZE
THE
IMAGE
FROM
ITS
WAVELET
COMPONENTS
THAT
ALL
HAVE
FINITE
IMPULSE
RESPONSES
THIS
TOPIC
HAS
BEEN
THE
MAIN
SUBJECT
OF
STUDY
IN
THE
WAVELET
COMMUNITY
FOR
OVER
TWO
DECADES
THE
ANSWER
DEPENDS
LARGELY
ON
THE
INTENDED
AP
PLICATION
E
G
WHETHER
THE
WAVELETS
ARE
BEING
USED
FOR
COMPRESSION
IMAGE
ANALYSIS
FEATURE
FINDING
OR
DENOISING
SIMONCELLI
AND
ADELSON
SHOW
IN
TABLE
SOME
GOOD
ODD
LENGTH
QUADRATURE
MIRROR
FILTER
QMF
COEFFICIENTS
THAT
SEEM
TO
WORK
WELL
IN
PRACTICE
SINCE
THE
DESIGN
OF
WAVELET
FILTERS
IS
SUCH
A
TRICKY
ART
IS
THERE
PERHAPS
A
BETTER
WAY
IN
DEED
A
SIMPLER
PROCEDURE
IS
TO
SPLIT
THE
SIGNAL
INTO
ITS
EVEN
AND
ODD
COMPONENTS
AND
THEN
PERFORM
TRIVIALLY
REVERSIBLE
FILTERING
OPERATIONS
ON
EACH
SEQUENCE
TO
PRODUCE
WHAT
ARE
CALLED
LIFTED
WAVELETS
FIGURES
AND
SWELDENS
GIVES
A
WONDERFULLY
UNDERSTANDABLE
INTRODUCTION
TO
THE
LIFTING
SCHEME
FOR
SECOND
GENERATION
WAVELETS
FOLLOWED
BY
A
COMPREHEN
SIVE
REVIEW
SWELDENS
AS
FIGURE
DEMONSTRATES
RATHER
THAN
FIRST
FILTERING
THE
WHOLE
INPUT
SEQUENCE
IMAGE
A
B
FIGURE
ONE
DIMENSIONAL
WAVELET
TRANSFORM
A
USUAL
HIGH
PASS
LOW
PASS
FILTERS
FOL
LOWED
BY
ODD
AND
EVEN
DOWNSAMPLING
B
LIFTED
VERSION
WHICH
FIRST
SELECTS
THE
ODD
AND
EVEN
SUBSEQUENCES
AND
THEN
APPLIES
A
LOW
PASS
PREDICTION
STAGE
L
AND
A
HIGH
PASS
CORRECTION
STAGE
C
IN
AN
EASILY
REVERSIBLE
MANNER
WITH
HIGH
PASS
AND
LOW
PASS
FILTERS
AND
THEN
KEEPING
THE
ODD
AND
EVEN
SUB
SEQUENCES
THE
LIFTING
SCHEME
FIRST
SPLITS
THE
SEQUENCE
INTO
ITS
EVEN
AND
ODD
SUB
COMPONENTS
FILTERING
THE
EVEN
SEQUENCE
WITH
A
LOW
PASS
FILTER
L
AND
SUBTRACTING
THE
RESULT
FROM
THE
EVEN
SEQUENCE
IS
TRIVIALLY
REVERSIBLE
SIMPLY
PERFORM
THE
SAME
FILTERING
AND
THEN
ADD
THE
RESULT
BACK
IN
FURTHERMORE
THIS
OPERATION
CAN
BE
PERFORMED
IN
PLACE
RESULTING
IN
SIGNIFICANT
SPACE
SAVINGS
THE
SAME
APPLIES
TO
FILTERING
THE
EVEN
SEQUENCE
WITH
THE
CORRECTION
FILTER
C
WHICH
IS
USED
TO
ENSURE
THAT
THE
EVEN
SEQUENCE
IS
LOW
PASS
A
SERIES
OF
SUCH
LIFTING
STEPS
CAN
BE
USED
TO
CREATE
MORE
COMPLEX
FILTER
RESPONSES
WITH
LOW
COMPUTATIONAL
COST
AND
GUARANTEED
REVERSIBILITY
THIS
PROCESS
CAN
PERHAPS
BE
MORE
EASILY
UNDERSTOOD
BY
CONSIDERING
THE
SIGNAL
PROCESSING
DIAGRAM
IN
FIGURE
DURING
ANALYSIS
THE
AVERAGE
OF
THE
EVEN
VALUES
IS
SUBTRACTED
FROM
THE
ODD
VALUE
TO
OBTAIN
A
HIGH
PASS
WAVELET
COEFFICIENT
HOWEVER
THE
EVEN
SAMPLES
STILL
CONTAIN
AN
ALIASED
SAMPLE
OF
THE
LOW
FREQUENCY
SIGNAL
TO
COMPENSATE
FOR
THIS
A
SMALL
AMOUNT
OF
THE
HIGH
PASS
WAVELET
IS
ADDED
BACK
TO
THE
EVEN
SEQUENCE
SO
THAT
IT
IS
PROPERLY
LOW
PASS
FILTERED
IT
IS
EASY
TO
SHOW
THAT
THE
EFFECTIVE
LOW
PASS
FILTER
IS
WHICH
IS
IN
A
B
FIGURE
LIFTED
TRANSFORM
SHOWN
AS
A
SIGNAL
PROCESSING
DIAGRAM
A
THE
ANALYSIS
STAGE
FIRST
PREDICTS
THE
ODD
VALUE
FROM
ITS
EVEN
NEIGHBORS
STORES
THE
DIFFERENCE
WAVELET
AND
THEN
COMPENSATES
THE
COARSER
EVEN
VALUE
BY
ADDING
IN
A
FRACTION
OF
THE
WAVELET
B
THE
SYNTHESIS
STAGE
SIMPLY
REVERSES
THE
FLOW
OF
COMPUTATION
AND
THE
SIGNS
OF
SOME
OF
THE
FILTERS
AND
OP
ERATIONS
THE
LIGHT
BLUE
LINES
SHOW
WHAT
HAPPENS
IF
WE
USE
FOUR
TAPS
FOR
THE
PREDICTION
AND
CORRECTION
INSTEAD
OF
JUST
TWO
DEED
A
LOW
PASS
FILTER
DURING
SYNTHESIS
THE
SAME
OPERATIONS
ARE
REVERSED
WITH
A
JUDICIOUS
CHANGE
IN
SIGN
OF
COURSE
WE
NEED
NOT
RESTRICT
OURSELVES
TO
TWO
TAP
FILTERS
FIGURE
SHOWS
AS
LIGHT
BLUE
ARROWS
ADDITIONAL
FILTER
COEFFICIENTS
THAT
COULD
OPTIONALLY
BE
ADDED
TO
THE
LIFTING
SCHEME
WITHOUT
AFFECTING
ITS
REVERSIBILITY
IN
FACT
THE
LOW
PASS
AND
HIGH
PASS
FILTERING
OPERATIONS
CAN
BE
INTERCHANGED
E
G
WE
COULD
USE
A
FIVE
TAP
CUBIC
LOW
PASS
FILTER
ON
THE
ODD
SEQUENCE
PLUS
CENTER
VALUE
FIRST
FOLLOWED
BY
A
FOUR
TAP
CUBIC
LOW
PASS
PREDICTOR
TO
ESTIMATE
THE
WAVELET
ALTHOUGH
I
HAVE
NOT
SEEN
THIS
SCHEME
WRITTEN
DOWN
LIFTED
WAVELETS
ARE
CALLED
SECOND
GENERATION
WAVELETS
BECAUSE
THEY
CAN
EASILY
ADAPT
TO
NON
REGULAR
SAMPLING
TOPOLOGIES
E
G
THOSE
THAT
ARISE
IN
COMPUTER
GRAPHICS
APPLICATIONS
SUCH
AS
MULTI
RESOLUTION
SURFACE
MANIPULATION
SCHRO
DER
AND
SWELDENS
IT
ALSO
TURNS
OUT
THAT
LIFTED
WEIGHTED
WAVELETS
I
E
WAVELETS
WHOSE
COEFFICIENTS
ADAPT
TO
THE
UNDERLYING
PROBLEM
BEING
SOLVED
FATTAL
CAN
BE
EXTREMELY
EFFECTIVE
FOR
LOW
LEVEL
IMAGE
MANIPULATION
TASKS
AND
ALSO
FOR
PRECONDITIONING
THE
KINDS
OF
SPARSE
LINEAR
SYSTEMS
THAT
ARISE
IN
THE
OPTIMIZATION
BASED
APPROACHES
TO
VISION
ALGORITHMS
THAT
WE
DISCUSS
IN
SECTION
SZELISKI
AN
ALTERNATIVE
TO
THE
WIDELY
USED
SEPARABLE
APPROACH
TO
WAVELET
CONSTRUCTION
WHICH
DE
COMPOSES
EACH
LEVEL
INTO
HORIZONTAL
VERTICAL
AND
CROSS
SUB
BANDS
IS
TO
USE
A
REPRESENTATION
THAT
IS
MORE
ROTATIONALLY
SYMMETRIC
AND
ORIENTATIONALLY
SELECTIVE
AND
ALSO
AVOIDS
THE
ALIASING
INHERENT
IN
SAMPLING
SIGNALS
BELOW
THEIR
NYQUIST
FREQUENCY
SIMONCELLI
FREEMAN
ADELSON
ET
AL
INTRODUCE
SUCH
A
REPRESENTATION
WHICH
THEY
CALL
A
PYRAMIDAL
RADIAL
FREQUENCY
SUCH
ALIASING
CAN
OFTEN
BE
SEEN
AS
THE
SIGNAL
CONTENT
MOVING
BETWEEN
BANDS
AS
THE
ORIGINAL
SIGNAL
IS
SLOWLY
SHIFTED
C
A
B
D
FIGURE
STEERABLE
SHIFTABLE
MULTISCALE
TRANSFORMS
SIMONCELLI
FREEMAN
ADELSON
ET
AL
QC
IEEE
A
RADIAL
MULTI
SCALE
FREQUENCY
DOMAIN
DECOMPOSITION
B
ORIGINAL
IMAGE
C
A
SET
OF
FOUR
STEERABLE
FILTERS
D
THE
RADIAL
MULTI
SCALE
WAVELET
DECOMPOSITION
IMPLEMENTATION
OF
SHIFTABLE
MULTI
SCALE
TRANSFORMS
OR
MORE
SUCCINCTLY
STEERABLE
PYRAMIDS
THEIR
REPRESENTATION
IS
NOT
ONLY
OVERCOMPLETE
WHICH
ELIMINATES
THE
ALIASING
PROBLEM
BUT
IS
ALSO
ORIENTATIONALLY
SELECTIVE
AND
HAS
IDENTICAL
ANALYSIS
AND
SYNTHESIS
BASIS
FUNCTIONS
I
E
IT
IS
SELF
INVERTING
JUST
LIKE
REGULAR
WAVELETS
AS
A
RESULT
THIS
MAKES
STEERABLE
PYRAMIDS
A
MUCH
MORE
USEFUL
BASIS
FOR
THE
STRUCTURAL
ANALYSIS
AND
MATCHING
TASKS
COMMONLY
USED
IN
COMPUTER
VISION
FIGURE
SHOWS
HOW
SUCH
A
DECOMPOSITION
LOOKS
IN
FREQUENCY
SPACE
INSTEAD
OF
RE
CURSIVELY
DIVIDING
THE
FREQUENCY
DOMAIN
INTO
SQUARES
WHICH
RESULTS
IN
CHECKERBOARD
HIGH
FREQUENCIES
RADIAL
ARCS
ARE
USED
INSTEAD
FIGURE
ILLUSTRATES
THE
RESULTING
PYRAMID
SUB
BANDS
EVEN
THROUGH
THE
REPRESENTATION
IS
OVERCOMPLETE
I
E
THERE
ARE
MORE
WAVELET
CO
EFFICIENTS
THAN
INPUT
PIXELS
THE
ADDITIONAL
FREQUENCY
AND
ORIENTATION
SELECTIVITY
MAKES
THIS
REPRESENTATION
PREFERABLE
FOR
TASKS
SUCH
AS
TEXTURE
ANALYSIS
AND
SYNTHESIS
PORTILLA
AND
SIMON
CELLI
AND
IMAGE
DENOISING
PORTILLA
STRELA
WAINWRIGHT
ET
AL
LYU
AND
SIMONCELLI
A
B
C
D
FIGURE
LAPLACIAN
PYRAMID
BLENDING
BURT
AND
ADELSON
QC
ACM
A
ORIG
INAL
IMAGE
OF
APPLE
B
ORIGINAL
IMAGE
OF
ORANGE
C
REGULAR
SPLICE
D
PYRAMID
BLEND
APPLICATION
IMAGE
BLENDING
ONE
OF
THE
MOST
ENGAGING
AND
FUN
APPLICATIONS
OF
THE
LAPLACIAN
PYRAMID
PRESENTED
IN
SEC
TION
IS
THE
CREATION
OF
BLENDED
COMPOSITE
IMAGES
AS
SHOWN
IN
FIGURE
BURT
AND
ADELSON
WHILE
SPLICING
THE
APPLE
AND
ORANGE
IMAGES
TOGETHER
ALONG
THE
MIDLINE
PRODUCES
A
NOTICEABLE
CUT
SPLINING
THEM
TOGETHER
AS
BURT
AND
ADELSON
CALLED
THEIR
PROCEDURE
CREATES
A
BEAUTIFUL
ILLUSION
OF
A
TRULY
HYBRID
FRUIT
THE
KEY
TO
THEIR
APPROACH
IS
THAT
THE
LOW
FREQUENCY
COLOR
VARIATIONS
BETWEEN
THE
RED
APPLE
AND
THE
ORANGE
ARE
SMOOTHLY
BLENDED
WHILE
THE
HIGHER
FREQUENCY
TEXTURES
ON
EACH
FRUIT
ARE
BLENDED
MORE
QUICKLY
TO
AVOID
GHOSTING
EFFECTS
WHEN
TWO
TEXTURES
ARE
OVERLAID
TO
CREATE
THE
BLENDED
IMAGE
EACH
SOURCE
IMAGE
IS
FIRST
DECOMPOSED
INTO
ITS
OWN
LAPLA
CIAN
PYRAMID
FIGURE
LEFT
AND
MIDDLE
COLUMNS
EACH
BAND
IS
THEN
MULTIPLIED
BY
A
SMOOTH
WEIGHTING
FUNCTION
WHOSE
EXTENT
IS
PROPORTIONAL
TO
THE
PYRAMID
LEVEL
THE
SIMPLEST
AND
MOST
GENERAL
WAY
TO
CREATE
THESE
WEIGHTS
IS
TO
TAKE
A
BINARY
MASK
IMAGE
FIGURE
AND
TO
CONSTRUCT
A
GAUSSIAN
PYRAMID
FROM
THIS
MASK
EACH
LAPLACIAN
PYRAMID
IMAGE
IS
THEN
A
B
C
D
E
F
G
H
I
J
K
L
FIGURE
LAPLACIAN
PYRAMID
BLENDING
DETAILS
BURT
AND
ADELSON
QC
ACM
THE
FIRST
THREE
ROWS
SHOW
THE
HIGH
MEDIUM
AND
LOW
FREQUENCY
PARTS
OF
THE
LAPLACIAN
PYRAMID
TAKEN
FROM
LEVELS
AND
THE
LEFT
AND
MIDDLE
COLUMNS
SHOW
THE
ORIGINAL
APPLE
AND
ORANGE
IMAGES
WEIGHTED
BY
THE
SMOOTH
INTERPOLATION
FUNCTIONS
WHILE
THE
RIGHT
COLUMN
SHOWS
THE
AVERAGED
CONTRIBUTIONS
A
B
C
D
FIGURE
LAPLACIAN
PYRAMID
BLEND
OF
TWO
IMAGES
OF
ARBITRARY
SHAPE
BURT
AND
ADELSON
QC
ACM
A
FIRST
INPUT
IMAGE
B
SECOND
INPUT
IMAGE
C
REGION
MASK
D
BLENDED
IMAGE
MULTIPLIED
BY
ITS
CORRESPONDING
GAUSSIAN
MASK
AND
THE
SUM
OF
THESE
TWO
WEIGHTED
PYRAMIDS
IS
THEN
USED
TO
CONSTRUCT
THE
FINAL
IMAGE
FIGURE
RIGHT
COLUMN
FIGURE
SHOWS
THAT
THIS
PROCESS
CAN
BE
APPLIED
TO
ARBITRARY
MASK
IMAGES
WITH
SUR
PRISING
RESULTS
IT
IS
ALSO
STRAIGHTFORWARD
TO
EXTEND
THE
PYRAMID
BLEND
TO
AN
ARBITRARY
NUMBER
OF
IMAGES
WHOSE
PIXEL
PROVENANCE
IS
INDICATED
BY
AN
INTEGER
VALUED
LABEL
IMAGE
SEE
EXER
CISE
THIS
IS
PARTICULARLY
USEFUL
IN
IMAGE
STITCHING
AND
COMPOSITING
APPLICATIONS
WHERE
THE
EXPOSURES
MAY
VARY
BETWEEN
DIFFERENT
IMAGES
AS
DESCRIBED
IN
SECTION
GEOMETRIC
TRANSFORMATIONS
IN
THE
PREVIOUS
SECTIONS
WE
SAW
HOW
INTERPOLATION
AND
DECIMATION
COULD
BE
USED
TO
CHANGE
THE
RESOLUTION
OF
AN
IMAGE
IN
THIS
SECTION
WE
LOOK
AT
HOW
TO
PERFORM
MORE
GENERAL
TRANSFOR
MATIONS
SUCH
AS
IMAGE
ROTATIONS
OR
GENERAL
WARPS
IN
CONTRAST
TO
THE
POINT
PROCESSES
WE
SAW
IN
SECTION
WHERE
THE
FUNCTION
APPLIED
TO
AN
IMAGE
TRANSFORMS
THE
RANGE
OF
THE
IMAGE
G
X
H
F
X
F
F
F
F
X
X
X
X
FIGURE
IMAGE
WARPING
INVOLVES
MODIFYING
THE
DOMAIN
OF
AN
IMAGE
FUNCTION
RATHER
THAN
ITS
RANGE
FIGURE
BASIC
SET
OF
GEOMETRIC
IMAGE
TRANSFORMATIONS
HERE
WE
LOOK
AT
FUNCTIONS
THAT
TRANSFORM
THE
DOMAIN
G
X
F
H
X
SEE
FIGURE
WE
BEGIN
BY
STUDYING
THE
GLOBAL
PARAMETRIC
TRANSFORMATION
FIRST
INTRODUCED
IN
SEC
TION
SUCH
A
TRANSFORMATION
IS
CALLED
PARAMETRIC
BECAUSE
IT
IS
CONTROLLED
BY
A
SMALL
NUMBER
OF
PARAMETERS
WE
THEN
TURN
OUR
ATTENTION
TO
MORE
LOCAL
GENERAL
DEFORMATIONS
SUCH
AS
THOSE
DEFINED
ON
MESHES
SECTION
FINALLY
WE
SHOW
HOW
IMAGE
WARPS
CAN
BE
COMBINED
WITH
CROSS
DISSOLVES
TO
CREATE
INTERESTING
MORPHS
IN
BETWEEN
ANIMATIONS
IN
SECTION
FOR
READERS
INTERESTED
IN
MORE
DETAILS
ON
THESE
TOPICS
THERE
IS
AN
EXCELLENT
SURVEY
BY
HECK
BERT
AS
WELL
AS
VERY
ACCESSIBLE
TEXTBOOKS
BY
WOLBERG
GOMES
DARSA
COSTA
ET
AL
AND
AKENINE
MO
LLER
AND
HAINES
NOTE
THAT
HECKBERT
SURVEY
IS
ON
TEX
TURE
MAPPING
WHICH
IS
HOW
THE
COMPUTER
GRAPHICS
COMMUNITY
REFERS
TO
THE
TOPIC
OF
WARPING
IMAGES
ONTO
SURFACES
PARAMETRIC
TRANSFORMATIONS
PARAMETRIC
TRANSFORMATIONS
APPLY
A
GLOBAL
DEFORMATION
TO
AN
IMAGE
WHERE
THE
BEHAVIOR
OF
THE
TRANSFORMATION
IS
CONTROLLED
BY
A
SMALL
NUMBER
OF
PARAMETERS
FIGURE
SHOWS
A
FEW
EX
TRANSFORMATION
MATRIX
DOF
PRESERVES
ICON
TRANSLATION
I
T
ORIENTATION
RIGID
EUCLIDEAN
R
T
L
LENGTHS
SIMILARITY
SR
T
L
ANGLES
AFFINE
A
L
PARALLELISM
PROJECTIVE
H
STRAIGHT
LINES
TABLE
HIERARCHY
OF
COORDINATE
TRANSFORMATIONS
EACH
TRANSFORMATION
ALSO
PRESERVES
THE
PROPERTIES
LISTED
IN
THE
ROWS
BELOW
IT
I
E
SIMILARITY
PRESERVES
NOT
ONLY
ANGLES
BUT
ALSO
PARALLELISM
AND
STRAIGHT
LINES
THE
MATRICES
ARE
EXTENDED
WITH
A
THIRD
ROW
TO
FORM
A
FULL
MATRIX
FOR
HOMOGENEOUS
COORDINATE
TRANSFORMATIONS
AMPLES
OF
SUCH
TRANSFORMATIONS
WHICH
ARE
BASED
ON
THE
GEOMETRIC
TRANSFORMATIONS
SHOWN
IN
FIGURE
THE
FORMULAS
FOR
THESE
TRANSFORMATIONS
WERE
ORIGINALLY
GIVEN
IN
TABLE
AND
ARE
REPRODUCED
HERE
IN
TABLE
FOR
EASE
OF
REFERENCE
IN
GENERAL
GIVEN
A
TRANSFORMATION
SPECIFIED
BY
A
FORMULA
XI
H
X
AND
A
SOURCE
IMAGE
F
X
HOW
DO
WE
COMPUTE
THE
VALUES
OF
THE
PIXELS
IN
THE
NEW
IMAGE
G
X
AS
GIVEN
IN
THINK
ABOUT
THIS
FOR
A
MINUTE
BEFORE
PROCEEDING
AND
SEE
IF
YOU
CAN
FIGURE
IT
OUT
IF
YOU
ARE
LIKE
MOST
PEOPLE
YOU
WILL
COME
UP
WITH
AN
ALGORITHM
THAT
LOOKS
SOMETHING
LIKE
ALGORITHM
THIS
PROCESS
IS
CALLED
FORWARD
WARPING
OR
FORWARD
MAPPING
AND
IS
SHOWN
IN
FIGURE
CAN
YOU
THINK
OF
ANY
PROBLEMS
WITH
THIS
APPROACH
ALGORITHM
FORWARD
WARPING
ALGORITHM
FOR
TRANSFORMING
AN
IMAGE
F
X
INTO
AN
IMAGE
G
XI
THROUGH
THE
PARAMETRIC
TRANSFORM
XI
H
X
X
F
X
X
G
X
A
B
FIGURE
FORWARD
WARPING
ALGORITHM
A
A
PIXEL
F
X
IS
COPIED
TO
ITS
CORRESPONDING
LOCATION
XI
H
X
IN
IMAGE
G
XI
B
DETAIL
OF
THE
SOURCE
AND
DESTINATION
PIXEL
LOCATIONS
IN
FACT
THIS
APPROACH
SUFFERS
FROM
SEVERAL
LIMITATIONS
THE
PROCESS
OF
COPYING
A
PIXEL
F
X
TO
A
LOCATION
XI
IN
G
IS
NOT
WELL
DEFINED
WHEN
XI
HAS
A
NON
INTEGER
VALUE
WHAT
DO
WE
DO
IN
SUCH
A
CASE
WHAT
WOULD
YOU
DO
YOU
CAN
ROUND
THE
VALUE
OF
XI
TO
THE
NEAREST
INTEGER
COORDINATE
AND
COPY
THE
PIXEL
THERE
BUT
THE
RESULTING
IMAGE
HAS
SEVERE
ALIASING
AND
PIXELS
THAT
JUMP
AROUND
A
LOT
WHEN
ANIMATING
THE
TRANSFORMATION
YOU
CAN
ALSO
DISTRIBUTE
THE
VALUE
AMONG
ITS
FOUR
NEAREST
NEIGHBORS
IN
A
WEIGHTED
BILINEAR
FASHION
KEEPING
TRACK
OF
THE
PER
PIXEL
WEIGHTS
AND
NORMALIZING
AT
THE
END
THIS
TECHNIQUE
IS
CALLED
SPLATTING
AND
IS
SOMETIMES
USED
FOR
VOLUME
RENDERING
IN
THE
GRAPHICS
COMMUNITY
LEVOY
AND
WHITTED
LEVOY
WESTOVER
RUSINKIEWICZ
AND
LEVOY
UNFORTUNATELY
IT
SUFFERS
FROM
BOTH
MODERATE
AMOUNTS
OF
ALIASING
AND
A
FAIR
AMOUNT
OF
BLUR
LOSS
OF
HIGH
RESOLUTION
DETAIL
THE
SECOND
MAJOR
PROBLEM
WITH
FORWARD
WARPING
IS
THE
APPEARANCE
OF
CRACKS
AND
HOLES
ESPECIALLY
WHEN
MAGNIFYING
AN
IMAGE
FILLING
SUCH
HOLES
WITH
THEIR
NEARBY
NEIGHBORS
CAN
LEAD
TO
FURTHER
ALIASING
AND
BLURRING
WHAT
CAN
WE
DO
INSTEAD
A
PREFERABLE
SOLUTION
IS
TO
USE
INVERSE
WARPING
ALGORITHM
WHERE
EACH
PIXEL
IN
THE
DESTINATION
IMAGE
G
XI
IS
SAMPLED
FROM
THE
ORIGINAL
IMAGE
F
X
FIGURE
HOW
DOES
THIS
DIFFER
FROM
THE
FORWARD
WARPING
ALGORITHM
FOR
ONE
THING
SINCE
Hˆ
XI
IS
PRESUMABLY
DEFINED
FOR
ALL
PIXELS
IN
G
XI
WE
NO
LONGER
HAVE
HOLES
MORE
IMPORTANTLY
RESAMPLING
AN
IMAGE
AT
NON
INTEGER
LOCATIONS
IS
A
WELL
STUDIED
PROBLEM
GENERAL
IMAGE
INTER
POLATION
SEE
SECTION
AND
HIGH
QUALITY
FILTERS
THAT
CONTROL
ALIASING
CAN
BE
USED
WHERE
DOES
THE
FUNCTION
Hˆ
XI
COME
FROM
QUITE
OFTEN
IT
CAN
SIMPLY
BE
COMPUTED
AS
THE
INVERSE
OF
H
X
IN
FACT
ALL
OF
THE
PARAMETRIC
TRANSFORMS
LISTED
IN
TABLE
HAVE
CLOSED
FORM
SOLUTIONS
FOR
THE
INVERSE
TRANSFORM
SIMPLY
TAKE
THE
INVERSE
OF
THE
MATRIX
SPECIFYING
THE
TRANSFORM
IN
OTHER
CASES
IT
IS
PREFERABLE
TO
FORMULATE
THE
PROBLEM
OF
IMAGE
WARPING
AS
THAT
OF
RE
SAMPLING
A
SOURCE
IMAGE
F
X
GIVEN
A
MAPPING
X
Hˆ
XI
FROM
DESTINATION
PIXELS
XI
TO
SOURCE
PIXELS
X
FOR
EXAMPLE
IN
OPTICAL
FLOW
SECTION
WE
ESTIMATE
THE
FLOW
FIELD
AS
THE
ALGORITHM
INVERSE
WARPING
ALGORITHM
FOR
CREATING
AN
IMAGE
G
XI
FROM
AN
IMAGE
F
X
USING
THE
PARAMETRIC
TRANSFORM
XI
H
X
X
F
X
X
G
X
A
B
FIGURE
INVERSE
WARPING
ALGORITHM
A
A
PIXEL
G
XI
IS
SAMPLED
FROM
ITS
CORRESPONDING
LOCATION
X
Hˆ
XI
IN
IMAGE
F
X
B
DETAIL
OF
THE
SOURCE
AND
DESTINATION
PIXEL
LOCATIONS
LOCATION
OF
THE
SOURCE
PIXEL
WHICH
PRODUCED
THE
CURRENT
PIXEL
WHOSE
FLOW
IS
BEING
ESTIMATED
AS
OPPOSED
TO
COMPUTING
THE
DESTINATION
PIXEL
TO
WHICH
IT
IS
GOING
SIMILARLY
WHEN
CORRECTING
FOR
RADIAL
DISTORTION
SECTION
WE
CALIBRATE
THE
LENS
BY
COMPUTING
FOR
EACH
PIXEL
IN
THE
FINAL
UNDISTORTED
IMAGE
THE
CORRESPONDING
PIXEL
LOCATION
IN
THE
ORIGINAL
DISTORTED
IMAGE
WHAT
KINDS
OF
INTERPOLATION
FILTER
ARE
SUITABLE
FOR
THE
RESAMPLING
PROCESS
ANY
OF
THE
FIL
TERS
WE
STUDIED
IN
SECTION
CAN
BE
USED
INCLUDING
NEAREST
NEIGHBOR
BILINEAR
BICUBIC
AND
WINDOWED
SINC
FUNCTIONS
WHILE
BILINEAR
IS
OFTEN
USED
FOR
SPEED
E
G
INSIDE
THE
INNER
LOOP
OF
A
PATCH
TRACKING
ALGORITHM
SEE
SECTION
BICUBIC
AND
WINDOWED
SINC
ARE
PREFERABLE
WHERE
VISUAL
QUALITY
IS
IMPORTANT
TO
COMPUTE
THE
VALUE
OF
F
X
AT
A
NON
INTEGER
LOCATION
X
WE
SIMPLY
APPLY
OUR
USUAL
FIR
RESAMPLING
FILTER
G
X
Y
F
K
L
H
X
K
Y
L
K
L
WHERE
X
Y
ARE
THE
SUB
PIXEL
COORDINATE
VALUES
AND
H
X
Y
IS
SOME
INTERPOLATING
OR
SMOOTH
ING
KERNEL
RECALL
FROM
SECTION
THAT
WHEN
DECIMATION
IS
BEING
PERFORMED
THE
SMOOTHING
KERNEL
IS
STRETCHED
AND
RE
SCALED
ACCORDING
TO
THE
DOWNSAMPLING
RATE
R
UNFORTUNATELY
FOR
A
GENERAL
NON
ZOOM
IMAGE
TRANSFORMATION
THE
RESAMPLING
RATE
R
IS
NOT
WELL
DEFINED
CONSIDER
A
TRANSFORMATION
THAT
STRETCHES
THE
X
DIMENSIONS
WHILE
SQUASHING
A
B
C
FIGURE
ANISOTROPIC
TEXTURE
FILTERING
A
JACOBIAN
OF
TRANSFORM
A
AND
THE
INDUCED
HORIZONTAL
AND
VERTICAL
RESAMPLING
RATES
AXI
X
AXI
Y
AYI
X
AYI
Y
B
ELLIPTICAL
FOOTPRINT
OF
AN
EWA
SMOOTHING
KERNEL
C
ANISOTROPIC
FILTERING
USING
MULTIPLE
SAMPLES
ALONG
THE
MAJOR
AXIS
IMAGE
PIXELS
LIE
AT
LINE
INTERSECTIONS
THE
Y
DIMENSIONS
THE
RESAMPLING
KERNEL
SHOULD
BE
PERFORMING
REGULAR
INTERPOLATION
ALONG
THE
X
DIMENSION
AND
SMOOTHING
TO
ANTI
ALIAS
THE
BLURRED
IMAGE
IN
THE
Y
DIRECTION
THIS
GETS
EVEN
MORE
COMPLICATED
FOR
THE
CASE
OF
GENERAL
AFFINE
OR
PERSPECTIVE
TRANSFORMS
WHAT
CAN
WE
DO
FORTUNATELY
FOURIER
ANALYSIS
CAN
HELP
THE
TWO
DIMENSIONAL
GENERAL
IZATION
OF
THE
ONE
DIMENSIONAL
DOMAIN
SCALING
LAW
GIVEN
IN
TABLE
IS
G
AX
A
A
T
F
FOR
ALL
OF
THE
TRANSFORMS
IN
TABLE
EXCEPT
PERSPECTIVE
THE
MATRIX
A
IS
ALREADY
DEFINED
FOR
PERSPECTIVE
TRANSFORMATIONS
THE
MATRIX
A
IS
THE
LINEARIZED
DERIVATIVE
OF
THE
PERSPECTIVE
TRANSFORMATION
FIGURE
I
E
THE
LOCAL
AFFINE
APPROXIMATION
TO
THE
STRETCHING
INDUCED
BY
THE
PROJECTION
HECKBERT
WOLBERG
GOMES
DARSA
COSTA
ET
AL
AKENINE
MO
LLER
AND
HAINES
TO
PREVENT
ALIASING
WE
NEED
TO
PRE
FILTER
THE
IMAGE
F
X
WITH
A
FILTER
WHOSE
FREQUENCY
RESPONSE
IS
THE
PROJECTION
OF
THE
FINAL
DESIRED
SPECTRUM
THROUGH
THE
A
T
TRANSFORM
SZELISKI
WINDER
AND
UYTTENDAELE
IN
GENERAL
FOR
NON
ZOOM
TRANSFORMS
THIS
FILTER
IS
NON
SEPARABLE
AND
HENCE
IS
VERY
SLOW
TO
COMPUTE
THEREFORE
A
NUMBER
OF
APPROXIMATIONS
TO
THIS
FILTER
ARE
USED
IN
PRACTICE
INCLUDE
MIP
MAPPING
ELLIPTICALLY
WEIGHTED
GAUSSIAN
AVERAGING
AND
ANISOTROPIC
FILTERING
AKENINE
MO
LLER
AND
HAINES
MIP
MAPPING
MIP
MAPPING
WAS
FIRST
PROPOSED
BY
WILLIAMS
AS
A
MEANS
TO
RAPIDLY
PRE
FILTER
IMAGES
BEING
USED
FOR
TEXTURE
MAPPING
IN
COMPUTER
GRAPHICS
A
MIP
IS
A
STANDARD
IMAGE
THE
TERM
MIP
STANDS
FOR
MULTI
IN
PARVO
MEANING
MANY
IN
ONE
PYRAMID
FIGURE
WHERE
EACH
LEVEL
IS
PRE
FILTERED
WITH
A
HIGH
QUALITY
FILTER
RATHER
THAN
A
POORER
QUALITY
APPROXIMATION
SUCH
AS
BURT
AND
ADELSON
FIVE
TAP
BINOMIAL
TO
RESAMPLE
AN
IMAGE
FROM
A
MIP
MAP
A
SCALAR
ESTIMATE
OF
THE
RESAMPLING
RATE
R
IS
FIRST
COM
PUTED
FOR
EXAMPLE
R
CAN
BE
THE
MAXIMUM
OF
THE
ABSOLUTE
VALUES
IN
A
WHICH
SUPPRESSES
ALIASING
OR
IT
CAN
BE
THE
MINIMUM
WHICH
REDUCES
BLURRING
AKENINE
MO
LLER
AND
HAINES
DISCUSS
THESE
ISSUES
IN
MORE
DETAIL
ONCE
A
RESAMPLING
RATE
HAS
BEEN
SPECIFIED
A
FRACTIONAL
PYRAMID
LEVEL
IS
COMPUTED
USING
THE
BASE
LOGARITHM
L
R
ONE
SIMPLE
SOLUTION
IS
TO
RESAMPLE
THE
TEXTURE
FROM
THE
NEXT
HIGHER
OR
LOWER
PYRAMID
LEVEL
DEPENDING
ON
WHETHER
IT
IS
PREFERABLE
TO
REDUCE
ALIASING
OR
BLUR
A
BETTER
SOLUTION
IS
TO
RE
SAMPLE
BOTH
IMAGES
AND
BLEND
THEM
LINEARLY
USING
THE
FRACTIONAL
COMPONENT
OF
L
SINCE
MOST
MIP
MAP
IMPLEMENTATIONS
USE
BILINEAR
RESAMPLING
WITHIN
EACH
LEVEL
THIS
APPROACH
IS
USU
ALLY
CALLED
TRILINEAR
MIP
MAPPING
COMPUTER
GRAPHICS
RENDERING
APIS
SUCH
AS
OPENGL
AND
HAVE
PARAMETERS
THAT
CAN
BE
USED
TO
SELECT
WHICH
VARIANT
OF
MIP
MAPPING
AND
OF
THE
SAMPLING
RATE
R
COMPUTATION
SHOULD
BE
USED
DEPENDING
ON
THE
DESIRED
TRADEOFF
BETWEEN
SPEED
AND
QUALITY
EXERCISE
HAS
YOU
EXAMINE
SOME
OF
THESE
TRADEOFFS
IN
MORE
DETAIL
ELLIPTICAL
WEIGHTED
AVERAGE
THE
ELLIPTICAL
WEIGHTED
AVERAGE
EWA
FILTER
INVENTED
BY
GREENE
AND
HECKBERT
IS
BASED
ON
THE
OBSERVATION
THAT
THE
AFFINE
MAPPING
X
AXI
DEFINES
A
SKEWED
TWO
DIMENSIONAL
COORDINATE
SYSTEM
IN
THE
VICINITY
OF
EACH
SOURCE
PIXEL
X
FIGURE
FOR
EVERY
DESTINA
TION
PIXEL
XI
THE
ELLIPSOIDAL
PROJECTION
OF
A
SMALL
PIXEL
GRID
IN
XI
ONTO
X
IS
COMPUTED
FIG
URE
THIS
IS
THEN
USED
TO
FILTER
THE
SOURCE
IMAGE
G
X
WITH
A
GAUSSIAN
WHOSE
INVERSE
COVARIANCE
MATRIX
IS
THIS
ELLIPSOID
DESPITE
ITS
REPUTATION
AS
A
HIGH
QUALITY
FILTER
AKENINE
MO
LLER
AND
HAINES
WE
HAVE
FOUND
IN
OUR
WORK
SZELISKI
WINDER
AND
UYTTENDAELE
THAT
BECAUSE
A
GAUSSIAN
KERNEL
IS
USED
THE
TECHNIQUE
SUFFERS
SIMULTANEOUSLY
FROM
BOTH
BLURRING
AND
ALIASING
COMPARED
TO
HIGHER
QUALITY
FILTERS
THE
EWA
IS
ALSO
QUITE
SLOW
ALTHOUGH
FASTER
VARIANTS
BASED
ON
MIP
MAPPING
HAVE
BEEN
PROPOSED
SZELISKI
WINDER
AND
UYTTENDAELE
PROVIDE
SOME
ADDI
TIONAL
REFERENCES
ANISOTROPIC
FILTERING
AN
ALTERNATIVE
APPROACH
TO
FILTERING
ORIENTED
TEXTURES
WHICH
IS
SOMETIMES
IMPLEMENTED
IN
GRAPHICS
HARDWARE
GPUS
IS
TO
USE
ANISOTROPIC
FILTERING
BARKANS
AKENINE
MO
LLER
AND
HAINES
IN
THIS
APPROACH
SEVERAL
SAMPLES
AT
DIFFERENT
RESOLUTIONS
FRACTIONAL
LEVELS
IN
THE
MIP
MAP
ARE
COMBINED
ALONG
THE
MAJOR
AXIS
OF
THE
EWA
GAUSSIAN
FIGURE
F
A
I
INTERPOLATE
X
B
X
WARP
AX
T
C
X
FILTER
X
D
X
SAMPLE
Δ
X
E
I
U
F
U
G
U
H
U
I
U
J
FIGURE
ONE
DIMENSIONAL
SIGNAL
RESAMPLING
SZELISKI
WINDER
AND
UYTTENDAELE
A
ORIGINAL
SAMPLED
SIGNAL
F
I
B
INTERPOLATED
SIGNAL
X
C
WARPED
SIGNAL
X
D
FILTERED
SIGNAL
X
E
SAMPLED
SIGNAL
F
I
I
THE
CORRESPONDING
SPECTRA
ARE
SHOWN
BELOW
THE
SIGNALS
WITH
THE
ALIASED
PORTIONS
SHOWN
IN
RED
MULTI
PASS
TRANSFORMS
THE
OPTIMAL
APPROACH
TO
WARPING
IMAGES
WITHOUT
EXCESSIVE
BLURRING
OR
ALIASING
IS
TO
ADAP
TIVELY
PRE
FILTER
THE
SOURCE
IMAGE
AT
EACH
PIXEL
USING
AN
IDEAL
LOW
PASS
FILTER
I
E
AN
ORIENTED
SKEWED
SINC
OR
LOW
ORDER
E
G
CUBIC
APPROXIMATION
FIGURE
FIGURE
SHOWS
HOW
THIS
WORKS
IN
ONE
DIMENSION
THE
SIGNAL
IS
FIRST
THEORETICALLY
INTERPOLATED
TO
A
CONTINUOUS
WAVEFORM
IDEALLY
LOW
PASS
FILTERED
TO
BELOW
THE
NEW
NYQUIST
RATE
AND
THEN
RE
SAMPLED
TO
THE
FINAL
DESIRED
RESOLUTION
IN
PRACTICE
THE
INTERPOLATION
AND
DECIMATION
STEPS
ARE
CONCATE
NATED
INTO
A
SINGLE
POLYPHASE
DIGITAL
FILTERING
OPERATION
SZELISKI
WINDER
AND
UYTTENDAELE
FOR
PARAMETRIC
TRANSFORMS
THE
ORIENTED
TWO
DIMENSIONAL
FILTERING
AND
RESAMPLING
OPERA
TIONS
CAN
BE
APPROXIMATED
USING
A
SERIES
OF
ONE
DIMENSIONAL
RESAMPLING
AND
SHEARING
TRANS
FORMS
CATMULL
AND
SMITH
HECKBERT
WOLBERG
GOMES
DARSA
COSTA
ET
AL
SZELISKI
WINDER
AND
UYTTENDAELE
THE
ADVANTAGE
OF
USING
A
SERIES
OF
ONE
DIMENSIONAL
TRANSFORMS
IS
THAT
THEY
ARE
MUCH
MORE
EFFICIENT
IN
TERMS
OF
BASIC
ARITHMETIC
OPERATIONS
THAN
LARGE
NON
SEPARABLE
TWO
DIMENSIONAL
FILTER
KERNELS
IN
ORDER
TO
PREVENT
ALIASING
HOWEVER
IT
MAY
BE
NECESSARY
TO
UPSAMPLE
IN
THE
OPPOSITE
DI
RECTION
BEFORE
APPLYING
A
SHEARING
TRANSFORMATION
SZELISKI
WINDER
AND
UYTTENDAELE
FIGURE
SHOWS
THIS
PROCESS
FOR
A
ROTATION
WHERE
A
VERTICAL
UPSAMPLING
STAGE
IS
ADDED
BE
FORE
THE
HORIZONTAL
SHEARING
AND
UPSAMPLING
STAGE
THE
UPPER
IMAGE
SHOWS
THE
APPEARANCE
OF
THE
LETTER
BEING
ROTATED
WHILE
THE
LOWER
IMAGE
SHOWS
ITS
CORRESPONDING
FOURIER
TRANSFORM
VERTICAL
UPSAMPLE
HORIZONTAL
SHEAR
UPSAMPLE
VERTICAL
SHEAR
DOWNSAMPLE
HORIZONTAL
DOWNSAMPLE
A
B
C
D
E
FIGURE
FOUR
PASS
ROTATION
SZELISKI
WINDER
AND
UYTTENDAELE
A
ORIGINAL
PIXEL
GRID
IMAGE
AND
ITS
FOURIER
TRANSFORM
B
VERTICAL
UPSAMPLING
C
HORIZONTAL
SHEAR
AND
UP
SAMPLING
D
VERTICAL
SHEAR
AND
DOWNSAMPLING
E
HORIZONTAL
DOWNSAMPLING
THE
GENERAL
AFFINE
CASE
LOOKS
SIMILAR
EXCEPT
THAT
THE
FIRST
TWO
STAGES
PERFORM
GENERAL
RESAMPLING
MESH
BASED
WARPING
WHILE
PARAMETRIC
TRANSFORMS
SPECIFIED
BY
A
SMALL
NUMBER
OF
GLOBAL
PARAMETERS
HAVE
MANY
USES
LOCAL
DEFORMATIONS
WITH
MORE
DEGREES
OF
FREEDOM
ARE
OFTEN
REQUIRED
CONSIDER
FOR
EXAMPLE
CHANGING
THE
APPEARANCE
OF
A
FACE
FROM
A
FROWN
TO
A
SMILE
FIG
URE
WHAT
IS
NEEDED
IN
THIS
CASE
IS
TO
CURVE
THE
CORNERS
OF
THE
MOUTH
UPWARDS
WHILE
LEAVING
THE
REST
OF
THE
FACE
INTACT
TO
PERFORM
SUCH
A
TRANSFORMATION
DIFFERENT
AMOUNTS
OF
MOTION
ARE
REQUIRED
IN
DIFFERENT
PARTS
OF
THE
IMAGE
FIGURE
SHOWS
SOME
OF
THE
COMMONLY
USED
APPROACHES
THE
FIRST
APPROACH
SHOWN
IN
FIGURE
B
IS
TO
SPECIFY
A
SPARSE
SET
OF
CORRESPONDING
POINTS
THE
DISPLACEMENT
OF
THESE
POINTS
CAN
THEN
BE
INTERPOLATED
TO
A
DENSE
DISPLACEMENT
FIELD
CHAPTER
USING
A
VARIETY
OF
TECHNIQUES
NIELSON
ONE
POSSIBILITY
IS
TO
TRIANGULATE
THE
SET
OF
POINTS
IN
ONE
IMAGE
DE
BERG
CHEONG
VAN
KREVELD
ET
AL
LITWINOWICZ
AND
WILLIAMS
BUCK
FINKELSTEIN
JACOBS
ET
AL
AND
TO
USE
AN
AFFINE
MOTION
MODEL
TABLE
SPECIFIED
BY
THE
THREE
TRIANGLE
VERTICES
INSIDE
EACH
TRIANGLE
IF
THE
DESTINATION
ROWLAND
AND
PERRETT
PIGHIN
HECKER
LISCHINSKI
ET
AL
BLANZ
AND
VETTER
LEYVAND
COHEN
OR
DROR
ET
AL
SHOW
MORE
SOPHISTICATED
EXAMPLES
OF
CHANGING
FACIAL
EXPRESSION
AND
APPEARANCE
A
B
C
D
FIGURE
IMAGE
WARPING
ALTERNATIVES
GOMES
DARSA
COSTA
ET
AL
QC
MORGAN
KAUFMANN
A
SPARSE
CONTROL
POINTS
DEFORMATION
GRID
B
DENSER
SET
OF
CONTROL
POINT
CORRESPONDENCES
C
ORIENTED
LINE
CORRESPONDENCES
D
UNIFORM
QUADRILATERAL
GRID
IMAGE
IS
TRIANGULATED
ACCORDING
TO
THE
NEW
VERTEX
LOCATIONS
AN
INVERSE
WARPING
ALGORITHM
FIGURE
CAN
BE
USED
IF
THE
SOURCE
IMAGE
IS
TRIANGULATED
AND
USED
AS
A
TEXTURE
MAP
COMPUTER
GRAPHICS
RENDERING
ALGORITHMS
CAN
BE
USED
TO
DRAW
THE
NEW
IMAGE
BUT
CARE
MUST
BE
TAKEN
ALONG
TRIANGLE
EDGES
TO
AVOID
POTENTIAL
ALIASING
ALTERNATIVE
METHODS
FOR
INTERPOLATING
A
SPARSE
SET
OF
DISPLACEMENTS
INCLUDE
MOVING
NEARBY
QUADRILATERAL
MESH
VERTICES
AS
SHOWN
IN
FIGURE
USING
VARIATIONAL
ENERGY
MINIMIZING
INTERPOLANTS
SUCH
AS
REGULARIZATION
LITWINOWICZ
AND
WILLIAMS
SEE
SECTION
OR
USING
LOCALLY
WEIGHTED
RADIAL
BASIS
FUNCTION
COMBINATIONS
OF
DISPLACEMENTS
NIELSON
SEE
SECTION
FOR
ADDITIONAL
SCATTERED
DATA
INTERPOLATION
TECHNIQUES
IF
QUADRILATERAL
MESHES
ARE
USED
IT
MAY
BE
DESIRABLE
TO
INTERPOLATE
DISPLACEMENTS
DOWN
TO
INDIVIDUAL
PIXEL
VALUES
USING
A
SMOOTH
INTERPOLANT
SUCH
AS
A
QUADRATIC
B
SPLINE
FARIN
LEE
WOLBERG
CHWA
ET
AL
IN
SOME
CASES
E
G
IF
A
DENSE
DEPTH
MAP
HAS
BEEN
ESTIMATED
FOR
AN
IMAGE
SHADE
GORTLER
HE
ET
AL
WE
ONLY
KNOW
THE
FORWARD
DISPLACEMENT
FOR
EACH
PIXEL
AS
MENTIONED
BEFORE
DRAWING
SOURCE
PIXELS
AT
THEIR
DESTINATION
LOCATION
I
E
FORWARD
WARPING
FIGURE
SUFFERS
FROM
SEVERAL
POTENTIAL
PROBLEMS
INCLUDING
ALIASING
AND
THE
APPEARANCE
OF
SMALL
CRACKS
AN
ALTERNATIVE
TECHNIQUE
IN
THIS
CASE
IS
TO
FORWARD
WARP
THE
DISPLACEMENT
FIELD
OR
DEPTH
MAP
TO
NOTE
THAT
THE
BLOCK
BASED
MOTION
MODELS
USED
BY
MANY
VIDEO
COMPRESSION
STANDARDS
LE
GALL
CAN
BE
THOUGHT
OF
AS
A
ORDER
PIECEWISE
CONSTANT
DISPLACEMENT
FIELD
A
B
C
FIGURE
LINE
BASED
IMAGE
WARPING
BEIER
AND
NEELY
QC
ACM
A
DISTANCE
COMPUTATION
AND
POSITION
TRANSFER
B
RENDERING
ALGORITHM
C
TWO
INTERMEDIATE
WARPS
USED
FOR
MORPHING
ITS
NEW
LOCATION
FILL
SMALL
HOLES
IN
THE
RESULTING
MAP
AND
THEN
USE
INVERSE
WARPING
TO
PERFORM
THE
RESAMPLING
SHADE
GORTLER
HE
ET
AL
THE
REASON
THAT
THIS
GENERALLY
WORKS
BETTER
THAN
FORWARD
WARPING
IS
THAT
DISPLACEMENT
FIELDS
TEND
TO
BE
MUCH
SMOOTHER
THAN
IMAGES
SO
THE
ALIASING
INTRODUCED
DURING
THE
FORWARD
WARPING
OF
THE
DISPLACEMENT
FIELD
IS
MUCH
LESS
NOTICEABLE
A
SECOND
APPROACH
TO
SPECIFYING
DISPLACEMENTS
FOR
LOCAL
DEFORMATIONS
IS
TO
USE
CORRE
SPONDING
ORIENTED
LINE
SEGMENTS
BEIER
AND
NEELY
AS
SHOWN
IN
FIGURES
AND
PIXELS
ALONG
EACH
LINE
SEGMENT
ARE
TRANSFERRED
FROM
SOURCE
TO
DESTINATION
EXACTLY
AS
SPECIFIED
AND
OTHER
PIXELS
ARE
WARPED
USING
A
SMOOTH
INTERPOLATION
OF
THESE
DISPLACEMENTS
EACH
LINE
SEGMENT
CORRESPONDENCE
SPECIFIES
A
TRANSLATION
ROTATION
AND
SCALING
I
E
A
SIMILARITY
TRANS
FORM
TABLE
FOR
PIXELS
IN
ITS
VICINITY
AS
SHOWN
IN
FIGURE
LINE
SEGMENTS
INFLUENCE
THE
OVERALL
DISPLACEMENT
OF
THE
IMAGE
USING
A
WEIGHTING
FUNCTION
THAT
DEPENDS
ON
THE
MINI
MUM
DISTANCE
TO
THE
LINE
SEGMENT
V
IN
FIGURE
IF
U
ELSE
THE
SHORTER
OF
THE
TWO
DISTANCES
TO
P
AND
Q
FOR
EACH
PIXEL
X
THE
TARGET
LOCATION
XI
FOR
EACH
LINE
CORRESPONDENCE
IS
COMPUTED
ALONG
WITH
A
WEIGHT
THAT
DEPENDS
ON
THE
DISTANCE
AND
THE
LINE
SEGMENT
LENGTH
FIGURE
THE
WEIGHTED
AVERAGE
OF
ALL
TARGET
LOCATIONS
XII
THEN
BECOMES
THE
FINAL
DESTINATION
LOCATION
NOTE
THAT
WHILE
BEIER
AND
NEELY
DESCRIBE
THIS
ALGORITHM
AS
A
FORWARD
WARP
AN
EQUIVALENT
ALGORITHM
CAN
BE
WRITTEN
BY
SEQUENCING
THROUGH
THE
DESTINATION
PIXELS
THE
RESULTING
WARPS
ARE
NOT
IDENTICAL
BECAUSE
LINE
LENGTHS
OR
DISTANCES
TO
LINES
MAY
BE
DIFFERENT
EXERCISE
HAS
YOU
IMPLEMENT
THE
BEIER
NEELY
LINE
BASED
WARP
AND
COMPARE
IT
TO
A
NUMBER
OF
OTHER
LOCAL
DEFORMATION
METHODS
YET
ANOTHER
WAY
OF
SPECIFYING
CORRESPONDENCES
IN
ORDER
TO
CREATE
IMAGE
WARPS
IS
TO
USE
SNAKES
SECTION
COMBINED
WITH
B
SPLINES
LEE
WOLBERG
CHWA
ET
AL
THIS
TECH
NIQUE
IS
USED
IN
APPLE
SHAKE
SOFTWARE
AND
IS
POPULAR
IN
THE
MEDICAL
IMAGING
COMMUNITY
FIGURE
IMAGE
MORPHING
GOMES
DARSA
COSTA
ET
AL
QC
MORGAN
KAUFMANN
TOP
ROW
IF
THE
TWO
IMAGES
ARE
JUST
BLENDED
VISIBLE
GHOSTING
RESULTS
BOTTOM
ROW
BOTH
IMAGES
ARE
FIRST
WARPED
TO
THE
SAME
INTERMEDIATE
LOCATION
E
G
HALFWAY
TOWARDS
THE
OTHER
IMAGE
AND
THE
RESULTING
WARPED
IMAGES
ARE
THEN
BLENDED
RESULTING
IN
A
SEAMLESS
MORPH
ONE
FINAL
POSSIBILITY
FOR
SPECIFYING
DISPLACEMENT
FIELDS
IS
TO
USE
A
MESH
SPECIFICALLY
ADAPTED
TO
THE
UNDERLYING
IMAGE
CONTENT
AS
SHOWN
IN
FIGURE
SPECIFYING
SUCH
MESHES
BY
HAND
CAN
INVOLVE
A
FAIR
AMOUNT
OF
WORK
GOMES
DARSA
COSTA
ET
AL
DESCRIBE
AN
INTERACTIVE
SYSTEM
FOR
DOING
THIS
ONCE
THE
TWO
MESHES
HAVE
BEEN
SPECIFIED
INTERMEDIATE
WARPS
CAN
BE
GENERATED
USING
LINEAR
INTERPOLATION
AND
THE
DISPLACEMENTS
AT
MESH
NODES
CAN
BE
INTERPOLATED
USING
SPLINES
APPLICATION
FEATURE
BASED
MORPHING
WHILE
WARPS
CAN
BE
USED
TO
CHANGE
THE
APPEARANCE
OF
OR
TO
ANIMATE
A
SINGLE
IMAGE
EVEN
MORE
POWERFUL
EFFECTS
CAN
BE
OBTAINED
BY
WARPING
AND
BLENDING
TWO
OR
MORE
IMAGES
USING
A
PROCESS
NOW
COMMONLY
KNOWN
AS
MORPHING
BEIER
AND
NEELY
LEE
WOLBERG
CHWA
ET
AL
GOMES
DARSA
COSTA
ET
AL
FIGURE
SHOWS
THE
ESSENCE
OF
IMAGE
MORPHING
INSTEAD
OF
SIMPLY
CROSS
DISSOLVING
BETWEEN
TWO
IMAGES
WHICH
LEADS
TO
GHOSTING
AS
SHOWN
IN
THE
TOP
ROW
EACH
IMAGE
IS
WARPED
TOWARD
THE
OTHER
IMAGE
BEFORE
BLENDING
AS
SHOWN
IN
THE
BOTTOM
ROW
IF
THE
CORRESPONDENCES
HAVE
BEEN
SET
UP
WELL
USING
ANY
OF
THE
TECHNIQUES
SHOWN
IN
FIGURE
CORRESPONDING
FEATURES
ARE
ALIGNED
AND
NO
GHOSTING
RESULTS
THE
ABOVE
PROCESS
IS
REPEATED
FOR
EACH
INTERMEDIATE
FRAME
BEING
GENERATED
DURING
A
MORPH
USING
DIFFERENT
BLENDS
AND
AMOUNTS
OF
DEFORMATION
AT
EACH
INTERVAL
LET
T
BE
THE
TIME
PARAMETER
THAT
DESCRIBES
THE
SEQUENCE
OF
INTERPOLATED
FRAMES
THE
WEIGHTING
FUNC
TIONS
FOR
THE
TWO
WARPED
IMAGES
IN
THE
BLEND
GO
AS
T
AND
T
CONVERSELY
THE
AMOUNT
OF
MOTION
THAT
IMAGE
UNDERGOES
AT
TIME
T
IS
T
OF
THE
TOTAL
AMOUNT
OF
MOTION
THAT
IS
SPECIFIED
BY
THE
CORRESPONDENCES
HOWEVER
SOME
CARE
MUST
BE
TAKEN
IN
DEFINING
WHAT
IT
MEANS
TO
PAR
TIALLY
WARP
AN
IMAGE
TOWARDS
A
DESTINATION
ESPECIALLY
IF
THE
DESIRED
MOTION
IS
FAR
FROM
LINEAR
SEDERBERG
GAO
WANG
ET
AL
EXERCISE
HAS
YOU
IMPLEMENT
A
MORPHING
ALGORITHM
AND
TEST
IT
OUT
UNDER
SUCH
CHALLENGING
CONDITIONS
GLOBAL
OPTIMIZATION
SO
FAR
IN
THIS
CHAPTER
WE
HAVE
COVERED
A
LARGE
NUMBER
OF
IMAGE
PROCESSING
OPERATORS
THAT
TAKE
AS
INPUT
ONE
OR
MORE
IMAGES
AND
PRODUCE
SOME
FILTERED
OR
TRANSFORMED
VERSION
OF
THESE
IMAGES
IN
MANY
APPLICATIONS
IT
IS
MORE
USEFUL
TO
FIRST
FORMULATE
THE
GOALS
OF
THE
DESIRED
TRANSFORMATION
USING
SOME
OPTIMIZATION
CRITERION
AND
THEN
FIND
OR
INFER
THE
SOLUTION
THAT
BEST
MEETS
THIS
CRITERION
IN
THIS
FINAL
SECTION
WE
PRESENT
TWO
DIFFERENT
BUT
CLOSELY
RELATED
VARIANTS
ON
THIS
IDEA
THE
FIRST
WHICH
IS
OFTEN
CALLED
REGULARIZATION
OR
VARIATIONAL
METHODS
SECTION
CON
STRUCTS
A
CONTINUOUS
GLOBAL
ENERGY
FUNCTION
THAT
DESCRIBES
THE
DESIRED
CHARACTERISTICS
OF
THE
SOLUTION
AND
THEN
FINDS
A
MINIMUM
ENERGY
SOLUTION
USING
SPARSE
LINEAR
SYSTEMS
OR
RELATED
ITERATIVE
TECHNIQUES
THE
SECOND
FORMULATES
THE
PROBLEM
USING
BAYESIAN
STATISTICS
MODEL
ING
BOTH
THE
NOISY
MEASUREMENT
PROCESS
THAT
PRODUCED
THE
INPUT
IMAGES
AS
WELL
AS
PRIOR
ASSUMPTIONS
ABOUT
THE
SOLUTION
SPACE
WHICH
ARE
OFTEN
ENCODED
USING
A
MARKOV
RANDOM
FIELD
SECTION
EXAMPLES
OF
SUCH
PROBLEMS
INCLUDE
SURFACE
INTERPOLATION
FROM
SCATTERED
DATA
FIGURE
IMAGE
DENOISING
AND
THE
RESTORATION
OF
MISSING
REGIONS
FIGURE
AND
THE
SEGMENTATION
OF
IMAGES
INTO
FOREGROUND
AND
BACKGROUND
REGIONS
FIGURE
REGULARIZATION
THE
THEORY
OF
REGULARIZATION
WAS
FIRST
DEVELOPED
BY
STATISTICIANS
TRYING
TO
FIT
MODELS
TO
DATA
THAT
SEVERELY
UNDERCONSTRAINED
THE
SOLUTION
SPACE
TIKHONOV
AND
ARSENIN
ENGL
HANKE
AND
NEUBAUER
CONSIDER
FOR
EXAMPLE
FINDING
A
SMOOTH
SURFACE
THAT
PASSES
THROUGH
OR
NEAR
A
SET
OF
MEASURED
DATA
POINTS
FIGURE
SUCH
A
PROBLEM
IS
DESCRIBED
AS
ILL
POSED
BECAUSE
MANY
POSSIBLE
SURFACES
CAN
FIT
THIS
DATA
SINCE
SMALL
CHANGES
IN
THE
INPUT
CAN
SOMETIMES
LEAD
TO
LARGE
CHANGES
IN
THE
FIT
E
G
IF
WE
USE
POLYNOMIAL
INTERPOLATION
SUCH
PROBLEMS
ARE
ALSO
OFTEN
ILL
CONDITIONED
SINCE
WE
ARE
TRYING
TO
RECOVER
THE
UNKNOWN
FUNCTION
F
X
Y
FROM
WHICH
THE
DATA
POINT
D
XI
YI
WERE
SAMPLED
SUCH
PROBLEMS
ARE
ALSO
OFTEN
CALLED
A
B
FIGURE
A
SIMPLE
SURFACE
INTERPOLATION
PROBLEM
A
NINE
DATA
POINTS
OF
VARIOUS
HEIGHT
SCATTERED
ON
A
GRID
B
SECOND
ORDER
CONTROLLED
CONTINUITY
THIN
PLATE
SPLINE
INTERPOLATOR
WITH
A
TEAR
ALONG
ITS
LEFT
EDGE
AND
A
CREASE
ALONG
ITS
RIGHT
SZELISKI
QC
SPRINGER
INVERSE
PROBLEMS
MANY
COMPUTER
VISION
TASKS
CAN
BE
VIEWED
AS
INVERSE
PROBLEMS
SINCE
WE
ARE
TRYING
TO
RECOVER
A
FULL
DESCRIPTION
OF
THE
WORLD
FROM
A
LIMITED
SET
OF
IMAGES
IN
ORDER
TO
QUANTIFY
WHAT
IT
MEANS
TO
FIND
A
SMOOTH
SOLUTION
WE
CAN
DEFINE
A
NORM
ON
THE
SOLUTION
SPACE
FOR
ONE
DIMENSIONAL
FUNCTIONS
F
X
WE
CAN
INTEGRATE
THE
SQUARED
FIRST
DERIVATIVE
OF
THE
FUNCTION
R
F
X
DX
OR
PERHAPS
INTEGRATE
THE
SQUARED
SECOND
DERIVATIVE
R
F
X
DX
HERE
WE
USE
SUBSCRIPTS
TO
DENOTE
DIFFERENTIATION
SUCH
ENERGY
MEASURES
ARE
EXAMPLES
OF
FUNCTIONALS
WHICH
ARE
OPERATORS
THAT
MAP
FUNCTIONS
TO
SCALAR
VALUES
THEY
ARE
ALSO
OFTEN
CALLED
VARIATIONAL
METHODS
BECAUSE
THEY
MEASURE
THE
VARIATION
NON
SMOOTHNESS
IN
A
FUNCTION
IN
TWO
DIMENSIONS
E
G
FOR
IMAGES
FLOW
FIELDS
OR
SURFACES
THE
CORRESPONDING
SMOOTH
NESS
FUNCTIONALS
ARE
R
F
X
Y
F
X
Y
DX
DY
R
F
X
Y
DX
DY
X
Y
AND
R
F
X
Y
X
Y
F
X
Y
DX
DY
WHERE
THE
MIXED
TERM
IS
NEEDED
TO
MAKE
THE
MEASURE
ROTATIONALLY
INVARIANT
GRIMSON
THE
FIRST
DERIVATIVE
NORM
IS
OFTEN
CALLED
THE
MEMBRANE
SINCE
INTERPOLATING
A
SET
OF
DATA
POINTS
USING
THIS
MEASURE
RESULTS
IN
A
TENT
LIKE
STRUCTURE
IN
FACT
THIS
FORMULA
IS
A
SMALL
DEFLECTION
APPROXIMATION
TO
THE
SURFACE
AREA
WHICH
IS
WHAT
SOAP
BUBBLES
MINIMIZE
THE
SECOND
ORDER
NORM
IS
CALLED
THE
THIN
PLATE
SPLINE
SINCE
IT
APPROXIMATES
THE
BEHAVIOR
OF
THIN
PLATES
E
G
FLEXIBLE
STEEL
UNDER
SMALL
DEFORMATIONS
A
BLEND
OF
THE
TWO
IS
CALLED
THE
THIN
PLATE
SPLINE
UNDER
TENSION
VERSIONS
OF
THESE
FORMULAS
WHERE
EACH
DERIVATIVE
TERM
IS
MUL
TIPLIED
BY
A
LOCAL
WEIGHTING
FUNCTION
ARE
CALLED
CONTROLLED
CONTINUITY
SPLINES
TERZOPOULOS
FIGURE
SHOWS
A
SIMPLE
EXAMPLE
OF
A
CONTROLLED
CONTINUITY
INTERPOLATOR
FIT
TO
NINE
SCATTERED
DATA
POINTS
IN
PRACTICE
IT
IS
MORE
COMMON
TO
FIND
FIRST
ORDER
SMOOTHNESS
TERMS
USED
WITH
IMAGES
AND
FLOW
FIELDS
SECTION
AND
SECOND
ORDER
SMOOTHNESS
ASSOCIATED
WITH
SURFACES
SECTION
IN
ADDITION
TO
THE
SMOOTHNESS
TERM
REGULARIZATION
ALSO
REQUIRES
A
DATA
TERM
OR
DATA
PENALTY
FOR
SCATTERED
DATA
INTERPOLATION
NIELSON
THE
DATA
TERM
MEASURES
THE
DIS
TANCE
BETWEEN
THE
FUNCTION
F
X
Y
AND
A
SET
OF
DATA
POINTS
DI
D
XI
YI
ED
F
XI
YI
DI
I
FOR
A
PROBLEM
LIKE
NOISE
REMOVAL
A
CONTINUOUS
VERSION
OF
THIS
MEASURE
CAN
BE
USED
ED
R
F
X
Y
D
X
Y
DX
DY
TO
OBTAIN
A
GLOBAL
ENERGY
THAT
CAN
BE
MINIMIZED
THE
TWO
ENERGY
TERMS
ARE
USUALLY
ADDED
TOGETHER
E
ED
ΛES
WHERE
ES
IS
THE
SMOOTHNESS
PENALTY
OR
SOME
WEIGHTED
BLEND
AND
Λ
IS
THE
REGULARIZA
TION
PARAMETER
WHICH
CONTROLS
HOW
SMOOTH
THE
SOLUTION
SHOULD
BE
IN
ORDER
TO
FIND
THE
MINIMUM
OF
THIS
CONTINUOUS
PROBLEM
THE
FUNCTION
F
X
Y
IS
USUALLY
FIRST
DISCRETIZED
ON
A
REGULAR
GRID
THE
MOST
PRINCIPLED
WAY
TO
PERFORM
THIS
DISCRETIZATION
IS
TO
USE
FINITE
ELEMENT
ANALYSIS
I
E
TO
APPROXIMATE
THE
FUNCTION
WITH
A
PIECEWISE
CONTINUOUS
SPLINE
AND
THEN
PERFORM
THE
ANALYTIC
INTEGRATION
BATHE
FORTUNATELY
FOR
BOTH
THE
FIRST
ORDER
AND
SECOND
ORDER
SMOOTHNESS
FUNCTIONALS
THE
JUDI
CIOUS
SELECTION
OF
APPROPRIATE
FINITE
ELEMENTS
RESULTS
IN
PARTICULARLY
SIMPLE
DISCRETE
FORMS
TERZOPOULOS
THE
CORRESPONDING
DISCRETE
SMOOTHNESS
ENERGY
FUNCTIONS
BECOME
SX
I
J
F
I
J
F
I
J
GX
I
J
I
J
SY
I
J
F
I
J
F
I
J
GY
I
J
AND
H
CX
I
J
F
I
J
I
J
F
I
J
I
J
THE
ALTERNATIVE
OF
USING
KERNEL
BASIS
FUNCTIONS
CENTERED
ON
THE
DATA
POINTS
BOULT
AND
KENDER
NIELSON
IS
DISCUSSED
IN
MORE
DETAIL
IN
SECTION
I
J
F
I
J
F
I
J
F
I
J
F
I
J
CY
I
J
F
I
J
I
J
F
I
J
WHERE
H
IS
THE
SIZE
OF
THE
FINITE
ELEMENT
GRID
THE
H
FACTOR
IS
ONLY
IMPORTANT
IF
THE
ENERGY
IS
BEING
DISCRETIZED
AT
A
VARIETY
OF
RESOLUTIONS
AS
IN
COARSE
TO
FINE
OR
MULTIGRID
TECHNIQUES
THE
OPTIONAL
SMOOTHNESS
WEIGHTS
SX
I
J
AND
SY
I
J
CONTROL
THE
LOCATION
OF
HORIZON
TAL
AND
VERTICAL
TEARS
OR
WEAKNESSES
IN
THE
SURFACE
FOR
OTHER
PROBLEMS
SUCH
AS
COLORIZA
TION
LEVIN
LISCHINSKI
AND
WEISS
AND
INTERACTIVE
TONE
MAPPING
LISCHINSKI
FARBMAN
UYTTENDAELE
ET
AL
THEY
CONTROL
THE
SMOOTHNESS
IN
THE
INTERPOLATED
CHROMA
OR
EXPO
SURE
FIELD
AND
ARE
OFTEN
SET
INVERSELY
PROPORTIONAL
TO
THE
LOCAL
LUMINANCE
GRADIENT
STRENGTH
FOR
SECOND
ORDER
PROBLEMS
THE
CREASE
VARIABLES
CX
I
J
CM
I
J
AND
CY
I
J
CONTROL
THE
LOCATIONS
OF
CREASES
IN
THE
SURFACE
TERZOPOULOS
SZELISKI
THE
DATA
VALUES
GX
I
J
AND
GY
I
J
ARE
GRADIENT
DATA
TERMS
CONSTRAINTS
USED
BY
AL
GORITHMS
SUCH
AS
PHOTOMETRIC
STEREO
SECTION
HDR
TONE
MAPPING
SECTION
FATTAL
LISCHINSKI
AND
WERMAN
POISSON
BLENDING
SECTION
PE
REZ
GANGNET
AND
BLAKE
AND
GRADIENT
DOMAIN
BLENDING
SECTION
LEVIN
ZOMET
PELEG
ET
AL
THEY
ARE
SET
TO
ZERO
WHEN
JUST
DISCRETIZING
THE
CONVENTIONAL
FIRST
ORDER
SMOOTHNESS
FUNCTIONAL
THE
TWO
DIMENSIONAL
DISCRETE
DATA
ENERGY
IS
WRITTEN
AS
ED
W
I
J
F
I
J
D
I
J
I
J
WHERE
THE
LOCAL
WEIGHTS
W
I
J
CONTROL
HOW
STRONGLY
THE
DATA
CONSTRAINT
IS
ENFORCED
THESE
VALUES
ARE
SET
TO
ZERO
WHERE
THERE
IS
NO
DATA
AND
CAN
BE
SET
TO
THE
INVERSE
VARIANCE
OF
THE
DATA
MEASUREMENTS
WHEN
THERE
IS
DATA
AS
DISCUSSED
BY
SZELISKI
AND
IN
SECTION
THE
TOTAL
ENERGY
OF
THE
DISCRETIZED
PROBLEM
CAN
NOW
BE
WRITTEN
AS
A
QUADRATIC
FORM
E
ED
ΛES
XT
AX
B
C
WHERE
X
F
F
M
N
IS
CALLED
THE
STATE
VECTOR
THE
SPARSE
SYMMETRIC
POSITIVE
DEFINITE
MATRIX
A
IS
CALLED
THE
HESSIAN
SINCE
IT
ENCODES
THE
SECOND
DERIVATIVE
OF
THE
ENERGY
FUNCTION
FOR
THE
ONE
DIMENSIONAL
FIRST
ORDER
PROBLEM
A
IS
TRIDIAGONAL
FOR
THE
TWO
DIMENSIONAL
FIRST
ORDER
PROBLEM
IT
IS
MULTI
BANDED
WITH
FIVE
NON
ZERO
ENTRIES
PER
ROW
WE
CALL
B
THE
WEIGHTED
DATA
VECTOR
MINIMIZING
THE
ABOVE
QUADRATIC
FORM
IS
EQUIVALENT
TO
SOLVING
THE
SPARSE
LINEAR
SYSTEM
AX
B
WE
USE
X
INSTEAD
OF
F
BECAUSE
THIS
IS
THE
MORE
COMMON
FORM
IN
THE
NUMERICAL
ANALYSIS
LITERATURE
GOLUB
AND
VAN
LOAN
IN
NUMERICAL
ANALYSIS
A
IS
CALLED
THE
COEFFICIENT
MATRIX
SAAD
IN
FINITE
ELEMENT
ANALYSIS
BATHE
IT
IS
CALLED
THE
STIFFNESS
MATRIX
D
I
J
W
I
J
F
I
J
SY
I
J
F
I
J
F
I
J
SX
I
J
F
I
J
FIGURE
GRAPHICAL
MODEL
INTERPRETATION
OF
FIRST
ORDER
REGULARIZATION
THE
WHITE
CIRCLES
ARE
THE
UNKNOWNS
F
I
J
WHILE
THE
DARK
CIRCLES
ARE
THE
INPUT
DATA
D
I
J
IN
THE
RESISTIVE
GRID
INTERPRETATION
THE
D
AND
F
VALUES
ENCODE
INPUT
AND
OUTPUT
VOLTAGES
AND
THE
BLACK
SQUARES
DENOTE
RESISTORS
WHOSE
CONDUCTANCE
IS
SET
TO
SX
I
J
SY
I
J
AND
W
I
J
IN
THE
SPRING
MASS
SYSTEM
ANALOGY
THE
CIRCLES
DENOTE
ELEVATIONS
AND
THE
BLACK
SQUARES
DENOTE
SPRINGS
THE
SAME
GRAPHICAL
MODEL
CAN
BE
USED
TO
DEPICT
A
FIRST
ORDER
MARKOV
RANDOM
FIELD
FIGURE
WHICH
CAN
BE
DONE
USING
A
VARIETY
OF
SPARSE
MATRIX
TECHNIQUES
SUCH
AS
MULTIGRID
BRIGGS
HENSON
AND
MCCORMICK
AND
HIERARCHICAL
PRECONDITIONERS
SZELISKI
AS
DE
SCRIBED
IN
APPENDIX
A
WHILE
REGULARIZATION
WAS
FIRST
INTRODUCED
TO
THE
VISION
COMMUNITY
BY
POGGIO
TORRE
AND
KOCH
AND
TERZOPOULOS
FOR
PROBLEMS
SUCH
AS
SURFACE
INTERPOLATION
IT
WAS
QUICKLY
ADOPTED
BY
OTHER
VISION
RESEARCHERS
FOR
SUCH
VARIED
PROBLEMS
AS
EDGE
DETECTION
SEC
TION
OPTICAL
FLOW
SECTION
AND
SHAPE
FROM
SHADING
SECTION
POGGIO
TORRE
AND
KOCH
HORN
AND
BROOKS
TERZOPOULOS
BERTERO
POGGIO
AND
TORRE
BROX
BRUHN
PAPENBERG
ET
AL
POGGIO
TORRE
AND
KOCH
ALSO
SHOWED
HOW
THE
DISCRETE
ENERGY
DEFINED
BY
EQUATIONS
COULD
BE
IMPLEMENTED
IN
A
RESISTIVE
GRID
AS
SHOWN
IN
FIGURE
IN
COMPUTATIONAL
PHOTOGRAPHY
CHAPTER
REGULARIZATION
AND
ITS
VARIANTS
ARE
COMMONLY
USED
TO
SOLVE
PROBLEMS
SUCH
AS
HIGH
DYNAMIC
RANGE
TONE
MAPPING
FATTAL
LISCHINSKI
AND
WERMAN
LISCHINSKI
FARBMAN
UYTTENDAELE
ET
AL
POIS
SON
AND
GRADIENT
DOMAIN
BLENDING
PE
REZ
GANGNET
AND
BLAKE
LEVIN
ZOMET
PELEG
ET
AL
AGARWALA
DONTCHEVA
AGRAWALA
ET
AL
COLORIZATION
LEVIN
LISCHINSKI
AND
WEISS
AND
NATURAL
IMAGE
MATTING
LEVIN
LISCHINSKI
AND
WEISS
ROBUST
REGULARIZATION
WHILE
REGULARIZATION
IS
MOST
COMMONLY
FORMULATED
USING
QUADRATIC
NORMS
COMPARE
WITH
THE
SQUARED
DERIVATIVES
IN
AND
SQUARED
DIFFERENCES
IN
IT
CAN
ALSO
BE
FORMULATED
USING
NON
QUADRATIC
ROBUST
PENALTY
FUNCTIONS
APPENDIX
B
FOR
EXAM
PLE
CAN
BE
GENERALIZED
TO
SX
I
J
Ρ
F
I
J
F
I
J
I
J
SY
I
J
Ρ
F
I
J
F
I
J
WHERE
Ρ
X
IS
SOME
MONOTONICALLY
INCREASING
PENALTY
FUNCTION
FOR
EXAMPLE
THE
FAMILY
OF
NORMS
Ρ
X
X
P
IS
CALLED
P
NORMS
WHEN
P
THE
RESULTING
SMOOTHNESS
TERMS
BECOME
MORE
PIECEWISE
CONTINUOUS
THAN
TOTALLY
SMOOTH
WHICH
CAN
BETTER
MODEL
THE
DISCONTINUOUS
NATURE
OF
IMAGES
FLOW
FIELDS
AND
SURFACES
AN
EARLY
EXAMPLE
OF
ROBUST
REGULARIZATION
IS
THE
GRADUATED
NON
CONVEXITY
GNC
ALGO
RITHM
INTRODUCED
BY
BLAKE
AND
ZISSERMAN
HERE
THE
NORMS
ON
THE
DATA
AND
DERIVATIVES
ARE
CLAMPED
TO
A
MAXIMUM
VALUE
Ρ
X
MIN
V
BECAUSE
THE
RESULTING
PROBLEM
IS
HIGHLY
NON
CONVEX
IT
HAS
MANY
LOCAL
MINIMA
A
CONTINUA
TION
METHOD
IS
PROPOSED
WHERE
A
QUADRATIC
NORM
WHICH
IS
CONVEX
IS
GRADUALLY
REPLACED
BY
THE
NON
CONVEX
ROBUST
NORM
ALLGOWER
AND
GEORG
AROUND
THE
SAME
TIME
TERZOPOU
LOS
WAS
ALSO
USING
CONTINUATION
TO
INFER
THE
TEAR
AND
CREASE
VARIABLES
IN
HIS
SURFACE
INTERPOLATION
PROBLEMS
TODAY
IT
IS
MORE
COMMON
TO
USE
THE
P
NORM
WHICH
IS
OFTEN
CALLED
TOTAL
VARIATION
CHAN
OSHER
AND
SHEN
TSCHUMPERLE
AND
DERICHE
TSCHUMPERLE
KAFTORY
SCHECHNER
AND
ZEEVI
OTHER
NORMS
FOR
WHICH
THE
INFLUENCE
DERIVATIVE
MORE
QUICKLY
DECAYS
TO
ZERO
ARE
PRESENTED
BY
BLACK
AND
RANGARAJAN
BLACK
SAPIRO
MARIMONT
ET
AL
AND
DISCUSSED
IN
APPENDIX
B
EVEN
MORE
RECENTLY
HYPER
LAPLACIAN
NORMS
WITH
P
HAVE
GAINED
POPULARITY
BASED
ON
THE
OBSERVATION
THAT
THE
LOG
LIKELIHOOD
DISTRIBUTION
OF
IMAGE
DERIVATIVES
FOLLOWS
A
P
SLOPE
AND
IS
THEREFORE
A
HYPER
LAPLACIAN
DISTRIBUTION
SIMONCELLI
LEVIN
AND
WEISS
WEISS
AND
FREEMAN
KRISHNAN
AND
FERGUS
SUCH
NORMS
HAVE
AN
EVEN
STRONGER
TENDENCY
TO
PREFER
LARGE
DISCONTINUITIES
OVER
SMALL
ONES
SEE
THE
RELATED
DISCUSSION
IN
SECTION
WHILE
LEAST
SQUARES
REGULARIZED
PROBLEMS
USING
NORMS
CAN
BE
SOLVED
USING
LINEAR
SYS
TEMS
OTHER
P
NORMS
REQUIRE
DIFFERENT
ITERATIVE
TECHNIQUES
SUCH
AS
ITERATIVELY
REWEIGHTED
LEAST
SQUARES
IRLS
LEVENBERG
MARQUARDT
OR
ALTERNATION
BETWEEN
LOCAL
NON
LINEAR
SUBPROBLEMS
AND
GLOBAL
QUADRATIC
REGULARIZATION
KRISHNAN
AND
FERGUS
SUCH
TECHNIQUES
ARE
DIS
CUSSED
IN
SECTION
AND
APPENDICES
A
AND
B
MARKOV
RANDOM
FIELDS
AS
WE
HAVE
JUST
SEEN
REGULARIZATION
WHICH
INVOLVES
THE
MINIMIZATION
OF
ENERGY
FUNCTIONALS
DEFINED
OVER
PIECEWISE
CONTINUOUS
FUNCTIONS
CAN
BE
USED
TO
FORMULATE
AND
SOLVE
A
VARIETY
OF
LOW
LEVEL
COMPUTER
VISION
PROBLEMS
AN
ALTERNATIVE
TECHNIQUE
IS
TO
FORMULATE
A
BAYESIAN
MODEL
WHICH
SEPARATELY
MODELS
THE
NOISY
IMAGE
FORMATION
MEASUREMENT
PROCESS
AS
WELL
AS
ASSUMING
A
STATISTICAL
PRIOR
MODEL
OVER
THE
SOLUTION
SPACE
IN
THIS
SECTION
WE
LOOK
AT
PRIORS
BASED
ON
MARKOV
RANDOM
FIELDS
WHOSE
LOG
LIKELIHOOD
CAN
BE
DESCRIBED
USING
LOCAL
NEIGHBORHOOD
INTERACTION
OR
PENALTY
TERMS
KINDERMANN
AND
SNELL
GEMAN
AND
GEMAN
MARROQUIN
MITTER
AND
POGGIO
LI
SZELISKI
ZABIH
SCHARSTEIN
ET
AL
THE
USE
OF
BAYESIAN
MODELING
HAS
SEVERAL
POTENTIAL
ADVANTAGES
OVER
REGULARIZATION
SEE
ALSO
APPENDIX
B
THE
ABILITY
TO
MODEL
MEASUREMENT
PROCESSES
STATISTICALLY
ENABLES
US
TO
EXTRACT
THE
MAXIMUM
INFORMATION
POSSIBLE
FROM
EACH
MEASUREMENT
RATHER
THAN
JUST
GUESSING
WHAT
WEIGHTING
TO
GIVE
THE
DATA
SIMILARLY
THE
PARAMETERS
OF
THE
PRIOR
DISTRIBUTION
CAN
OFTEN
BE
LEARNED
BY
OBSERVING
SAMPLES
FROM
THE
CLASS
WE
ARE
MODELING
ROTH
AND
BLACK
TAPPEN
LI
AND
HUTTENLOCHER
FURTHERMORE
BECAUSE
OUR
MODEL
IS
PROBABILISTIC
IT
IS
POSSIBLE
TO
ESTIMATE
IN
PRINCIPLE
COMPLETE
PROBABILITY
DISTRIBUTIONS
OVER
THE
UNKNOWNS
BEING
RECOVERED
AND
IN
PARTICULAR
TO
MODEL
THE
UNCERTAINTY
IN
THE
SOLUTION
WHICH
CAN
BE
USEFUL
IN
LATTER
PROCESSING
STAGES
FINALLY
MARKOV
RANDOM
FIELD
MODELS
CAN
BE
DEFINED
OVER
DISCRETE
VARIABLES
SUCH
AS
IMAGE
LABELS
WHERE
THE
VARIABLES
HAVE
NO
PROPER
ORDERING
FOR
WHICH
REGULARIZATION
DOES
NOT
APPLY
RECALL
FROM
IN
SECTION
OR
SEE
APPENDIX
B
THAT
ACCORDING
TO
BAYES
RULE
THE
POSTERIOR
DISTRIBUTION
FOR
A
GIVEN
SET
OF
MEASUREMENTS
Y
P
Y
X
COMBINED
WITH
A
PRIOR
P
X
OVER
THE
UNKNOWNS
X
IS
GIVEN
BY
P
X
Y
P
Y
X
P
X
WHERE
P
Y
X
P
Y
X
P
X
IS
A
NORMALIZING
CONSTANT
USED
TO
MAKE
THE
P
X
Y
DISTRIBUTION
PROPER
INTEGRATE
TO
TAKING
THE
NEGATIVE
LOGARITHM
OF
BOTH
SIDES
OF
WE
GET
LOG
P
X
Y
LOG
P
Y
X
LOG
P
X
C
WHICH
IS
THE
NEGATIVE
POSTERIOR
LOG
LIKELIHOOD
TO
FIND
THE
MOST
LIKELY
MAXIMUM
A
POSTERIORI
OR
MAP
SOLUTION
X
GIVEN
SOME
MEASURE
MENTS
Y
WE
SIMPLY
MINIMIZE
THIS
NEGATIVE
LOG
LIKELIHOOD
WHICH
CAN
ALSO
BE
THOUGHT
OF
AS
AN
ENERGY
E
X
Y
ED
X
Y
EP
X
WE
DROP
THE
CONSTANT
C
BECAUSE
ITS
VALUE
DOES
NOT
MATTER
DURING
ENERGY
MINIMIZATION
THE
FIRST
TERM
ED
X
Y
IS
THE
DATA
ENERGY
OR
DATA
PENALTY
IT
MEASURES
THE
NEGATIVE
LOG
LIKELIHOOD
THAT
THE
DATA
WERE
OBSERVED
GIVEN
THE
UNKNOWN
STATE
X
THE
SECOND
TERM
EP
X
IS
THE
PRIOR
ENERGY
IT
PLAYS
A
ROLE
ANALOGOUS
TO
THE
SMOOTHNESS
ENERGY
IN
REGULARIZATION
NOTE
THAT
THE
MAP
ESTIMATE
MAY
NOT
ALWAYS
BE
DESIRABLE
SINCE
IT
SELECTS
THE
PEAK
IN
THE
POSTERIOR
DIS
TRIBUTION
RATHER
THAN
SOME
MORE
STABLE
STATISTIC
SEE
THE
DISCUSSION
IN
APPENDIX
B
AND
BY
LEVIN
WEISS
DURAND
ET
AL
FOR
IMAGE
PROCESSING
APPLICATIONS
THE
UNKNOWNS
X
ARE
THE
SET
OF
OUTPUT
PIXELS
X
F
F
M
N
AND
THE
DATA
ARE
IN
THE
SIMPLEST
CASE
THE
INPUT
PIXELS
Y
D
D
M
N
AS
SHOWN
IN
FIGURE
FOR
A
MARKOV
RANDOM
FIELD
THE
PROBABILITY
P
X
IS
A
GIBBS
OR
BOLTZMANN
DISTRIBUTION
WHOSE
NEGATIVE
LOG
LIKELIHOOD
ACCORDING
TO
THE
HAMMERSLEY
CLIFFORD
THEOREM
CAN
BE
WRIT
TEN
AS
A
SUM
OF
PAIRWISE
INTERACTION
POTENTIALS
EP
X
I
J
K
L
N
VI
J
K
L
F
I
J
F
K
L
WHERE
N
I
J
DENOTES
THE
NEIGHBORS
OF
PIXEL
I
J
IN
FACT
THE
GENERAL
VERSION
OF
THE
THEOREM
SAYS
THAT
THE
ENERGY
MAY
HAVE
TO
BE
EVALUATED
OVER
A
LARGER
SET
OF
CLIQUES
WHICH
DEPEND
ON
THE
ORDER
OF
THE
MARKOV
RANDOM
FIELD
KINDERMANN
AND
SNELL
GEMAN
AND
GEMAN
BISHOP
KOHLI
LADICKY
AND
TORR
KOHLI
KUMAR
AND
TORR
THE
MOST
COMMONLY
USED
NEIGHBORHOOD
IN
MARKOV
RANDOM
FIELD
MODELING
IS
THE
NEIGHBORHOOD
WHERE
EACH
PIXEL
IN
THE
FIELD
F
I
J
INTERACTS
ONLY
WITH
ITS
IMMEDIATE
NEIGH
BORS
THE
MODEL
IN
FIGURE
WHICH
WE
PREVIOUSLY
USED
IN
FIGURE
TO
ILLUSTRATE
THE
DISCRETE
VERSION
OF
FIRST
ORDER
REGULARIZATION
SHOWS
AN
MRF
THE
SX
I
J
AND
SY
I
J
BLACK
BOXES
DENOTE
ARBITRARY
INTERACTION
POTENTIALS
BETWEEN
ADJACENT
NODES
IN
THE
RANDOM
FIELD
AND
THE
W
I
J
DENOTE
THE
DATA
PENALTY
FUNCTIONS
THESE
SQUARE
NODES
CAN
ALSO
BE
INTER
PRETED
AS
FACTORS
IN
A
FACTOR
GRAPH
VERSION
OF
THE
UNDIRECTED
GRAPHICAL
MODEL
BISHOP
WHICH
IS
ANOTHER
NAME
FOR
INTERACTION
POTENTIALS
STRICTLY
SPEAKING
THE
FACTORS
ARE
IMPROPER
PROBABILITY
FUNCTIONS
WHOSE
PRODUCT
IS
THE
UN
NORMALIZED
POSTERIOR
DISTRIBUTION
AS
WE
WILL
SEE
IN
THERE
IS
A
CLOSE
RELATIONSHIP
BETWEEN
THESE
INTERACTION
POTENTIALS
AND
THE
DISCRETIZED
VERSIONS
OF
REGULARIZED
IMAGE
RESTORATION
PROBLEMS
THUS
TO
A
FIRST
APPROXIMATION
WE
CAN
VIEW
ENERGY
MINIMIZATION
BEING
PERFORMED
WHEN
SOLVING
A
REGULARIZED
PROBLEM
AND
THE
MAXIMUM
A
POSTERIORI
INFERENCE
BEING
PERFORMED
IN
AN
MRF
AS
EQUIVALENT
WHILE
NEIGHBORHOODS
ARE
MOST
COMMONLY
USED
IN
SOME
APPLICATIONS
OR
EVEN
HIGHER
ORDER
NEIGHBORHOODS
PERFORM
BETTER
AT
TASKS
SUCH
AS
IMAGE
SEGMENTATION
BECAUSE
D
I
J
W
I
J
F
I
J
SY
I
J
F
I
J
F
I
J
SX
I
J
F
I
J
FIGURE
GRAPHICAL
MODEL
FOR
AN
NEIGHBORHOOD
MARKOV
RANDOM
FIELD
THE
BLUE
EDGES
ARE
ADDED
FOR
AN
NEIGHBORHOOD
THE
WHITE
CIRCLES
ARE
THE
UNKNOWNS
F
I
J
WHILE
THE
DARK
CIRCLES
ARE
THE
INPUT
DATA
D
I
J
THE
SX
I
J
AND
SY
I
J
BLACK
BOXES
DENOTE
ARBI
TRARY
INTERACTION
POTENTIALS
BETWEEN
ADJACENT
NODES
IN
THE
RANDOM
FIELD
AND
THE
W
I
J
DENOTE
THE
DATA
PENALTY
FUNCTIONS
THE
SAME
GRAPHICAL
MODEL
CAN
BE
USED
TO
DEPICT
A
DISCRETE
VERSION
OF
A
FIRST
ORDER
REGULARIZATION
PROBLEM
FIGURE
THEY
CAN
BETTER
MODEL
DISCONTINUITIES
AT
DIFFERENT
ORIENTATIONS
BOYKOV
AND
KOLMOGOROV
ROTHER
KOHLI
FENG
ET
AL
KOHLI
LADICKY
AND
TORR
KOHLI
KUMAR
AND
TORR
BINARY
MRFS
THE
SIMPLEST
POSSIBLE
EXAMPLE
OF
A
MARKOV
RANDOM
FIELD
IS
A
BINARY
FIELD
EXAMPLES
OF
SUCH
FIELDS
INCLUDE
BIT
BLACK
AND
WHITE
SCANNED
DOCUMENT
IMAGES
AS
WELL
AS
IMAGES
SEGMENTED
INTO
FOREGROUND
AND
BACKGROUND
REGIONS
TO
DENOISE
A
SCANNED
IMAGE
WE
SET
THE
DATA
PENALTY
TO
REFLECT
THE
AGREEMENT
BETWEEN
THE
SCANNED
AND
FINAL
IMAGES
ED
I
J
WΔ
F
I
J
D
I
J
AND
THE
SMOOTHNESS
PENALTY
TO
REFLECT
THE
AGREEMENT
BETWEEN
NEIGHBORING
PIXELS
EP
I
J
EX
I
J
EY
I
J
SΔ
F
I
J
F
I
J
SΔ
F
I
J
F
I
J
ONCE
WE
HAVE
FORMULATED
THE
ENERGY
HOW
DO
WE
MINIMIZE
IT
THE
SIMPLEST
APPROACH
IS
TO
PERFORM
GRADIENT
DESCENT
FLIPPING
ONE
STATE
AT
A
TIME
IF
IT
PRODUCES
A
LOWER
ENERGY
THIS
AP
PROACH
IS
KNOWN
AS
CONTEXTUAL
CLASSIFICATION
KITTLER
AND
FO
GLEIN
ITERATED
CONDITIONAL
MODES
ICM
BESAG
OR
HIGHEST
CONFIDENCE
FIRST
HCF
CHOU
AND
BROWN
IF
THE
PIXEL
WITH
THE
LARGEST
ENERGY
DECREASE
IS
SELECTED
FIRST
UNFORTUNATELY
THESE
DOWNHILL
METHODS
TEND
TO
GET
EASILY
STUCK
IN
LOCAL
MINIMA
AN
AL
TERNATIVE
APPROACH
IS
TO
ADD
SOME
RANDOMNESS
TO
THE
PROCESS
WHICH
IS
KNOWN
AS
STOCHASTIC
GRADIENT
DESCENT
METROPOLIS
ROSENBLUTH
ROSENBLUTH
ET
AL
GEMAN
AND
GEMAN
WHEN
THE
AMOUNT
OF
NOISE
IS
DECREASED
OVER
TIME
THIS
TECHNIQUE
IS
KNOWN
AS
SIMULATED
AN
NEALING
KIRKPATRICK
GELATT
AND
VECCHI
CARNEVALI
COLETTI
AND
PATARNELLO
WOL
BERG
AND
PAVLIDIS
SWENDSEN
AND
WANG
AND
WAS
FIRST
POPULARIZED
IN
COMPUTER
VISION
BY
GEMAN
AND
GEMAN
AND
LATER
APPLIED
TO
STEREO
MATCHING
BY
BARNARD
AMONG
OTHERS
EVEN
THIS
TECHNIQUE
HOWEVER
DOES
NOT
PERFORM
THAT
WELL
BOYKOV
VEKSLER
AND
ZABIH
FOR
BINARY
IMAGES
A
MUCH
BETTER
TECHNIQUE
INTRODUCED
TO
THE
COMPUTER
VISION
COM
MUNITY
BY
BOYKOV
VEKSLER
AND
ZABIH
IS
TO
RE
FORMULATE
THE
ENERGY
MINIMIZATION
AS
A
MAX
FLOW
MIN
CUT
GRAPH
OPTIMIZATION
PROBLEM
GREIG
PORTEOUS
AND
SEHEULT
THIS
TECHNIQUE
HAS
INFORMALLY
COME
TO
BE
KNOWN
AS
GRAPH
CUTS
IN
THE
COMPUTER
VISION
COMMUNITY
BOYKOV
AND
KOLMOGOROV
FOR
SIMPLE
ENERGY
FUNCTIONS
E
G
THOSE
WHERE
THE
PENALTY
FOR
NON
IDENTICAL
NEIGHBORING
PIXELS
IS
A
CONSTANT
THIS
ALGORITHM
IS
GUARANTEED
TO
PRODUCE
THE
GLOBAL
MINIMUM
KOLMOGOROV
AND
ZABIH
FORMALLY
CHARACTERIZE
THE
CLASS
OF
BINARY
ENERGY
POTENTIALS
REGULARITY
CONDITIONS
FOR
WHICH
THESE
RESULTS
HOLD
WHILE
NEWER
WORK
BY
KOMODAKIS
TZIRITAS
AND
PARAGIOS
AND
ROTHER
KOLMOGOROV
LEMPITSKY
ET
AL
PROVIDE
GOOD
ALGORITHMS
FOR
THE
CASES
WHEN
THEY
DO
NOT
IN
ADDITION
TO
THE
ABOVE
MENTIONED
TECHNIQUES
A
NUMBER
OF
OTHER
OPTIMIZATION
APPROACHES
HAVE
BEEN
DEVELOPED
FOR
MRF
ENERGY
MINIMIZATION
SUCH
AS
LOOPY
BELIEF
PROPAGATION
AND
DYNAMIC
PROGRAMMING
FOR
ONE
DIMENSIONAL
PROBLEMS
THESE
ARE
DISCUSSED
IN
MORE
DETAIL
IN
APPENDIX
B
AS
WELL
AS
THE
COMPARATIVE
SURVEY
PAPER
BY
SZELISKI
ZABIH
SCHARSTEIN
ET
AL
ORDINAL
VALUED
MRFS
IN
ADDITION
TO
BINARY
IMAGES
MARKOV
RANDOM
FIELDS
CAN
BE
APPLIED
TO
ORDINAL
VALUED
LABELS
SUCH
AS
GRAYSCALE
IMAGES
OR
DEPTH
MAPS
THE
TERM
ORDINAL
INDICATES
THAT
THE
LABELS
HAVE
AN
IMPLIED
ORDERING
E
G
THAT
HIGHER
VALUES
ARE
LIGHTER
PIXELS
IN
THE
NEXT
SECTION
WE
LOOK
AT
UNORDERED
LABELS
SUCH
AS
SOURCE
IMAGE
LABELS
FOR
IMAGE
COMPOSITING
IN
MANY
CASES
IT
IS
COMMON
TO
EXTEND
THE
BINARY
DATA
AND
SMOOTHNESS
PRIOR
TERMS
AS
ED
I
J
W
I
J
ΡD
F
I
J
D
I
J
AND
EP
I
J
SX
I
J
ΡP
F
I
J
F
I
J
SY
I
J
ΡP
F
I
J
F
I
J
WHICH
ARE
ROBUST
GENERALIZATIONS
OF
THE
QUADRATIC
PENALTY
TERMS
AND
FIRST
INTRODUCED
IN
AS
BEFORE
THE
W
I
J
SX
I
J
AND
SY
I
J
WEIGHTS
CAN
BE
USED
TO
LOCALLY
CONTROL
THE
DATA
WEIGHTING
AND
THE
HORIZONTAL
AND
VERTICAL
SMOOTHNESS
INSTEAD
OF
A
B
C
D
FIGURE
GRAYSCALE
IMAGE
DENOISING
AND
INPAINTING
A
ORIGINAL
IMAGE
B
IMAGE
CORRUPTED
BY
NOISE
AND
WITH
MISSING
DATA
BLACK
BAR
C
IMAGE
RESTORED
USING
LOOPY
BE
LIEF
PROPAGATION
D
IMAGE
RESTORED
USING
EXPANSION
MOVE
GRAPH
CUTS
IMAGES
ARE
FROM
HTTP
VISION
MIDDLEBURY
EDU
MRF
RESULTS
SZELISKI
ZABIH
SCHARSTEIN
ET
AL
USING
A
QUADRATIC
PENALTY
HOWEVER
A
GENERAL
MONOTONICALLY
INCREASING
PENALTY
FUNCTION
Ρ
IS
USED
DIFFERENT
FUNCTIONS
CAN
BE
USED
FOR
THE
DATA
AND
SMOOTHNESS
TERMS
FOR
EXAMPLE
ΡP
CAN
BE
A
HYPER
LAPLACIAN
PENALTY
ΡP
D
D
P
P
WHICH
BETTER
ENCODES
THE
DISTRIBUTION
OF
GRADIENTS
MAINLY
EDGES
IN
AN
IMAGE
THAN
EITHER
A
QUADRATIC
OR
LINEAR
TOTAL
VARIATION
PENALTY
LEVIN
AND
WEISS
USE
SUCH
A
PENALTY
TO
SEPARATE
A
TRANSMITTED
AND
REFLECTED
IMAGE
FIGURE
BY
ENCOURAGING
GRADIENTS
TO
LIE
IN
ONE
OR
THE
OTHER
IMAGE
BUT
NOT
BOTH
MORE
RECENTLY
LEVIN
FERGUS
DURAND
ET
AL
USE
THE
HYPER
LAPLACIAN
AS
A
PRIOR
FOR
IMAGE
DECONVOLUTION
DEBLURRING
AND
KRISHNAN
AND
FERGUS
DEVELOP
A
FASTER
ALGORITHM
FOR
SOLVING
SUCH
PROBLEMS
FOR
THE
DATA
PENALTY
ΡD
CAN
BE
QUADRATIC
TO
MODEL
GAUSSIAN
NOISE
OR
THE
LOG
OF
A
CONTAMINATED
GAUSSIAN
APPENDIX
B
WHEN
ΡP
IS
A
QUADRATIC
FUNCTION
THE
RESULTING
MARKOV
RANDOM
FIELD
IS
CALLED
A
GAUSSIAN
MARKOV
RANDOM
FIELD
GMRF
AND
ITS
MINIMUM
CAN
BE
FOUND
BY
SPARSE
LINEAR
SYSTEM
SOLVING
WHEN
THE
WEIGHTING
FUNCTIONS
ARE
UNIFORM
THE
GMRF
BECOMES
A
SPECIAL
CASE
OF
WIENER
FILTERING
SECTION
ALLOWING
THE
WEIGHTING
FUNCTIONS
TO
DEPEND
ON
THE
INPUT
IMAGE
A
SPECIAL
KIND
OF
CONDITIONAL
RANDOM
FIELD
WHICH
WE
DESCRIBE
BELOW
ENABLES
QUITE
SOPHISTICATED
IMAGE
PROCESSING
ALGORITHMS
TO
BE
PERFORMED
INCLUDING
COLORIZATION
LEVIN
LISCHINSKI
AND
WEISS
INTERACTIVE
TONE
MAPPING
LISCHINSKI
FARBMAN
UYTTENDAELE
ET
AL
NATURAL
IMAGE
MATTING
LEVIN
LISCHINSKI
AND
WEISS
AND
IMAGE
RESTORATION
TAPPEN
LIU
FREEMAN
ET
AL
NOTE
THAT
UNLIKE
A
QUADRATIC
PENALTY
THE
SUM
OF
THE
HORIZONTAL
AND
VERTICAL
DERIVATIVE
P
NORMS
IS
NOT
ROTATIONALLY
INVARIANT
A
BETTER
APPROACH
MAY
BE
TO
LOCALLY
ESTIMATE
THE
GRADIENT
DIRECTION
AND
TO
IMPOSE
DIFFERENT
NORMS
ON
THE
PERPENDICULAR
AND
PARALLEL
COMPONENTS
WHICH
ROTH
AND
BLACK
CALL
A
STEERABLE
RANDOM
FIELD
A
INITIAL
LABELING
B
STANDARD
MOVE
C
Α
Β
SWAP
D
Α
EXPANSION
FIGURE
MULTI
LEVEL
GRAPH
OPTIMIZATION
FROM
BOYKOV
VEKSLER
AND
ZABIH
QC
IEEE
A
INITIAL
PROBLEM
CONFIGURATION
B
THE
STANDARD
MOVE
ONLY
CHANGES
ONE
PIXEL
C
THE
Α
Β
SWAP
OPTIMALLY
EXCHANGES
ALL
Α
AND
Β
LABELED
PIXELS
D
THE
Α
EXPANSION
MOVE
OPTIMALLY
SELECTS
AMONG
CURRENT
PIXEL
VALUES
AND
THE
Α
LABEL
WHEN
ΡD
OR
ΡP
ARE
NON
QUADRATIC
FUNCTIONS
GRADIENT
DESCENT
TECHNIQUES
SUCH
AS
NON
LINEAR
LEAST
SQUARES
OR
ITERATIVELY
RE
WEIGHTED
LEAST
SQUARES
CAN
SOMETIMES
BE
USED
AP
PENDIX
A
HOWEVER
IF
THE
SEARCH
SPACE
HAS
LOTS
OF
LOCAL
MINIMA
AS
IS
THE
CASE
FOR
STEREO
MATCHING
BARNARD
BOYKOV
VEKSLER
AND
ZABIH
MORE
SOPHISTICATED
TECHNIQUES
ARE
REQUIRED
THE
EXTENSION
OF
GRAPH
CUT
TECHNIQUES
TO
MULTI
VALUED
PROBLEMS
WAS
FIRST
PROPOSED
BY
BOYKOV
VEKSLER
AND
ZABIH
IN
THEIR
PAPER
THEY
DEVELOP
TWO
DIFFERENT
ALGORITHMS
CALLED
THE
SWAP
MOVE
AND
THE
EXPANSION
MOVE
WHICH
ITERATE
AMONG
A
SERIES
OF
BINARY
LABELING
SUB
PROBLEMS
TO
FIND
A
GOOD
SOLUTION
FIGURE
NOTE
THAT
A
GLOBAL
SOLUTION
IS
GENERALLY
NOT
ACHIEVABLE
AS
THE
PROBLEM
IS
PROVABLY
NP
HARD
FOR
GENERAL
ENERGY
FUNCTIONS
BECAUSE
BOTH
THESE
ALGORITHMS
USE
A
BINARY
MRF
OPTIMIZATION
INSIDE
THEIR
INNER
LOOP
THEY
ARE
SUBJECT
TO
THE
KIND
OF
CONSTRAINTS
ON
THE
ENERGY
FUNCTIONS
THAT
OCCUR
IN
THE
BINARY
LABELING
CASE
KOLMOGOROV
AND
ZABIH
APPENDIX
B
DISCUSSES
THESE
ALGORITHMS
IN
MORE
DETAIL
ALONG
WITH
SOME
MORE
RECENTLY
DEVELOPED
APPROACHES
TO
THIS
PROBLEM
ANOTHER
MRF
INFERENCE
TECHNIQUE
IS
BELIEF
PROPAGATION
BP
WHILE
BELIEF
PROPAGATION
WAS
ORIGINALLY
DEVELOPED
FOR
INFERENCE
OVER
TREES
WHERE
IT
IS
EXACT
PEARL
IT
HAS
MORE
RECENTLY
BEEN
APPLIED
TO
GRAPHS
WITH
LOOPS
SUCH
AS
MARKOV
RANDOM
FIELDS
FREEMAN
PASZ
TOR
AND
CARMICHAEL
YEDIDIA
FREEMAN
AND
WEISS
IN
FACT
SOME
OF
THE
BETTER
PERFORMING
STEREO
MATCHING
ALGORITHMS
USE
LOOPY
BELIEF
PROPAGATION
LBP
TO
PERFORM
THEIR
INFERENCE
SUN
ZHENG
AND
SHUM
LBP
IS
DISCUSSED
IN
MORE
DETAIL
IN
APPENDIX
B
AS
WELL
AS
THE
COMPARATIVE
SURVEY
PAPER
ON
MRF
OPTIMIZATION
SZELISKI
ZABIH
SCHARSTEIN
ET
AL
FIGURE
SHOWS
AN
EXAMPLE
OF
IMAGE
DENOISING
AND
INPAINTING
HOLE
FILLING
USING
A
NON
QUADRATIC
ENERGY
FUNCTION
NON
GAUSSIAN
MRF
THE
ORIGINAL
IMAGE
HAS
BEEN
CORRUPTED
BY
NOISE
AND
A
PORTION
OF
THE
DATA
HAS
BEEN
REMOVED
THE
BLACK
BAR
IN
THIS
CASE
THE
LOOPY
D
I
J
D
I
J
W
I
J
F
I
J
SY
I
J
F
I
J
F
I
J
SX
I
J
F
I
J
FIGURE
GRAPHICAL
MODEL
FOR
A
MARKOV
RANDOM
FIELD
WITH
A
MORE
COMPLEX
MEASUREMENT
MODEL
THE
ADDITIONAL
COLORED
EDGES
SHOW
HOW
COMBINATIONS
OF
UNKNOWN
VALUES
SAY
IN
A
SHARP
IMAGE
PRODUCE
THE
MEASURED
VALUES
A
NOISY
BLURRED
IMAGE
THE
RESULTING
GRAPHICAL
MODEL
IS
STILL
A
CLASSIC
MRF
AND
IS
JUST
AS
EASY
TO
SAMPLE
FROM
BUT
SOME
INFERENCE
ALGORITHMS
E
G
THOSE
BASED
ON
GRAPH
CUTS
MAY
NOT
BE
APPLICABLE
BECAUSE
OF
THE
INCREASED
NETWORK
COMPLEXITY
SINCE
STATE
CHANGES
DURING
THE
INFERENCE
BECOME
MORE
ENTANGLED
AND
THE
POSTERIOR
MRF
HAS
MUCH
LARGER
CLIQUES
BELIEF
PROPAGATION
ALGORITHM
COMPUTES
A
SLIGHTLY
LOWER
ENERGY
AND
ALSO
A
SMOOTHER
IMAGE
THAN
THE
ALPHA
EXPANSION
GRAPH
CUT
ALGORITHM
OF
COURSE
THE
ABOVE
FORMULA
FOR
THE
SMOOTHNESS
TERM
EP
I
J
JUST
SHOWS
THE
SIMPLEST
CASE
IN
MORE
RECENT
WORK
ROTH
AND
BLACK
PROPOSE
A
FIELD
OF
EXPERTS
FOE
MODEL
WHICH
SUMS
UP
A
LARGE
NUMBER
OF
EXPONENTIATED
LOCAL
FILTER
OUTPUTS
TO
ARRIVE
AT
THE
SMOOTHNESS
PENALTY
WEISS
AND
FREEMAN
ANALYZE
THIS
APPROACH
AND
COMPARE
IT
TO
THE
SIMPLER
HYPER
LAPLACIAN
MODEL
OF
NATURAL
IMAGE
STATISTICS
LYU
AND
SIMONCELLI
USE
GAUSSIAN
SCALE
MIXTURES
GSMS
TO
CONSTRUCT
AN
INHOMOGENEOUS
MULTI
SCALE
MRF
WITH
ONE
POSITIVE
EXPONENTIAL
GMRF
MODULATING
THE
VARIANCE
AMPLITUDE
OF
ANOTHER
GAUSSIAN
MRF
IT
IS
ALSO
POSSIBLE
TO
EXTEND
THE
MEASUREMENT
MODEL
TO
MAKE
THE
SAMPLED
NOISE
CORRUPTED
INPUT
PIXELS
CORRESPOND
TO
BLENDS
OF
UNKNOWN
LATENT
IMAGE
PIXELS
AS
IN
FIGURE
THIS
IS
THE
COMMONLY
OCCURRING
CASE
WHEN
TRYING
TO
DE
BLUR
AN
IMAGE
WHILE
THIS
KIND
OF
A
MODEL
IS
STILL
A
TRADITIONAL
GENERATIVE
MARKOV
RANDOM
FIELD
FINDING
AN
OPTIMAL
SOLUTION
CAN
BE
DIFFICULT
BECAUSE
THE
CLIQUE
SIZES
GET
LARGER
IN
SUCH
SITUATIONS
GRADIENT
DESCENT
TECHNIQUES
SUCH
AS
ITERATIVELY
REWEIGHTED
LEAST
SQUARES
CAN
BE
USED
JOSHI
ZITNICK
SZELISKI
ET
AL
EXERCISE
HAS
YOU
EXPLORE
SOME
OF
THESE
ISSUES
FIGURE
AN
UNORDERED
LABEL
MRF
AGARWALA
DONTCHEVA
AGRAWALA
ET
AL
QC
ACM
STROKES
IN
EACH
OF
THE
SOURCE
IMAGES
ON
THE
LEFT
ARE
USED
AS
CONSTRAINTS
ON
AN
MRF
OPTIMIZATION
WHICH
IS
SOLVED
USING
GRAPH
CUTS
THE
RESULTING
MULTI
VALUED
LABEL
FIELD
IS
SHOWN
AS
A
COLOR
OVERLAY
IN
THE
MIDDLE
IMAGE
AND
THE
FINAL
COMPOSITE
IS
SHOWN
ON
THE
RIGHT
UNORDERED
LABELS
ANOTHER
CASE
WITH
MULTI
VALUED
LABELS
WHERE
MARKOV
RANDOM
FIELDS
ARE
OFTEN
APPLIED
ARE
UNORDERED
LABELS
I
E
LABELS
WHERE
THERE
IS
NO
SEMANTIC
MEANING
TO
THE
NUMERICAL
DIFFERENCE
BETWEEN
THE
VALUES
OF
TWO
LABELS
FOR
EXAMPLE
IF
WE
ARE
CLASSIFYING
TERRAIN
FROM
AERIAL
IMAGERY
IT
MAKES
NO
SENSE
TO
TAKE
THE
NUMERIC
DIFFERENCE
BETWEEN
THE
LABELS
ASSIGNED
TO
FOREST
FIELD
WATER
AND
PAVEMENT
IN
FACT
THE
ADJACENCIES
OF
THESE
VARIOUS
KINDS
OF
TERRAIN
EACH
HAVE
DIFFERENT
LIKELIHOODS
SO
IT
MAKES
MORE
SENSE
TO
USE
A
PRIOR
OF
THE
FORM
EP
I
J
SX
I
J
V
L
I
J
L
I
J
SY
I
J
V
L
I
J
L
I
J
WHERE
V
IS
A
GENERAL
COMPATIBILITY
OR
POTENTIAL
FUNCTION
NOTE
THAT
WE
HAVE
ALSO
REPLACED
F
I
J
WITH
L
I
J
TO
MAKE
IT
CLEARER
THAT
THESE
ARE
LABELS
RATHER
THAN
DISCRETE
FUNCTION
SAMPLES
AN
ALTERNATIVE
WAY
TO
WRITE
THIS
PRIOR
ENERGY
BOYKOV
VEKSLER
AND
ZABIH
SZELISKI
ZABIH
SCHARSTEIN
ET
AL
IS
EP
P
Q
N
VP
Q
LP
LQ
WHERE
THE
P
Q
ARE
NEIGHBORING
PIXELS
AND
A
SPATIALLY
VARYING
POTENTIAL
FUNCTION
VP
Q
IS
EVAL
UATED
FOR
EACH
NEIGHBORING
PAIR
AN
IMPORTANT
APPLICATION
OF
UNORDERED
MRF
LABELING
IS
SEAM
FINDING
IN
IMAGE
COMPOSIT
ING
DAVIS
AGARWALA
DONTCHEVA
AGRAWALA
ET
AL
SEE
FIGURE
WHICH
IS
EXPLAINED
IN
MORE
DETAIL
IN
SECTION
HERE
THE
COMPATIBILITY
VP
Q
LP
LQ
MEASURES
THE
QUALITY
OF
THE
VISUAL
APPEARANCE
THAT
WOULD
RESULT
FROM
PLACING
A
PIXEL
P
FROM
IMAGE
LP
NEXT
TO
A
PIXEL
Q
FROM
IMAGE
LQ
AS
WITH
MOST
MRFS
WE
ASSUME
THAT
VP
Q
L
L
I
E
IT
IS
PER
FECTLY
FINE
TO
CHOOSE
CONTIGUOUS
PIXELS
FROM
THE
SAME
IMAGE
FOR
DIFFERENT
LABELS
HOWEVER
FIGURE
IMAGE
SEGMENTATION
BOYKOV
AND
FUNKA
LEA
QC
SPRINGER
THE
USER
DRAWS
A
FEW
RED
STROKES
IN
THE
FOREGROUND
OBJECT
AND
A
FEW
BLUE
ONES
IN
THE
BACKGROUND
THE
SYSTEM
COMPUTES
COLOR
DISTRIBUTIONS
FOR
THE
FOREGROUND
AND
BACKGROUND
AND
SOLVES
A
BINARY
MRF
THE
SMOOTHNESS
WEIGHTS
ARE
MODULATED
BY
THE
INTENSITY
GRADIENTS
EDGES
WHICH
MAKES
THIS
A
CONDITIONAL
RANDOM
FIELD
CRF
THE
COMPATIBILITY
VP
Q
LP
LQ
MAY
DEPEND
ON
THE
VALUES
OF
THE
UNDERLYING
PIXELS
ILP
P
AND
ILQ
Q
CONSIDER
FOR
EXAMPLE
WHERE
ONE
IMAGE
IS
ALL
SKY
BLUE
I
E
P
Q
B
WHILE
THE
OTHER
IMAGE
HAS
A
TRANSITION
FROM
SKY
BLUE
P
B
TO
FOREST
GREEN
Q
G
IN
THIS
CASE
VP
Q
THE
COLORS
AGREE
WHILE
VP
Q
THE
COLORS
DISAGREE
CONDITIONAL
RANDOM
FIELDS
IN
A
CLASSIC
BAYESIAN
MODEL
P
X
Y
P
Y
X
P
X
THE
PRIOR
DISTRIBUTION
P
X
IS
INDEPENDENT
OF
THE
OBSERVATIONS
Y
SOMETIMES
HOWEVER
IT
IS
USEFUL
TO
MODIFY
OUR
PRIOR
ASSUMPTIONS
SAY
ABOUT
THE
SMOOTHNESS
OF
THE
FIELD
WE
ARE
TRYING
TO
ESTIMATE
IN
RESPONSE
TO
THE
SENSED
DATA
WHETHER
THIS
MAKES
SENSE
FROM
A
PROBABILITY
VIEWPOINT
IS
SOMETHING
WE
DISCUSS
ONCE
WE
HAVE
EXPLAINED
THE
NEW
MODEL
CONSIDER
THE
INTERACTIVE
IMAGE
SEGMENTATION
PROBLEM
SHOWN
IN
FIGURE
BOYKOV
AND
FUNKA
LEA
IN
THIS
APPLICATION
THE
USER
DRAWS
FOREGROUND
RED
AND
BACKGROUND
BLUE
STROKES
AND
THE
SYSTEM
THEN
SOLVES
A
BINARY
MRF
LABELING
PROBLEM
TO
ESTIMATE
THE
EXTENT
OF
THE
FOREGROUND
OBJECT
IN
ADDITION
TO
MINIMIZING
A
DATA
TERM
WHICH
MEASURES
THE
POINTWISE
SIMILARITY
BETWEEN
PIXEL
COLORS
AND
THE
INFERRED
REGION
DISTRIBUTIONS
SECTION
THE
MRF
D
I
J
W
I
J
F
I
J
SY
I
J
F
I
J
F
I
J
SX
I
J
F
I
J
FIGURE
GRAPHICAL
MODEL
FOR
A
CONDITIONAL
RANDOM
FIELD
CRF
THE
ADDITIONAL
GREEN
EDGES
SHOW
HOW
COMBINATIONS
OF
SENSED
DATA
INFLUENCE
THE
SMOOTHNESS
IN
THE
UNDERLYING
MRF
PRIOR
MODEL
I
E
SX
I
J
AND
SY
I
J
IN
DEPEND
ON
ADJACENT
D
I
J
VALUES
THESE
ADDITIONAL
LINKS
FACTORS
ENABLE
THE
SMOOTHNESS
TO
DEPEND
ON
THE
INPUT
DATA
HOWEVER
THEY
MAKE
SAMPLING
FROM
THIS
MRF
MORE
COMPLEX
IS
MODIFIED
SO
THAT
THE
SMOOTHNESS
TERMS
SX
X
Y
AND
SY
X
Y
IN
FIGURE
AND
DEPEND
ON
THE
MAGNITUDE
OF
THE
GRADIENT
BETWEEN
ADJACENT
PIXELS
SINCE
THE
SMOOTHNESS
TERM
NOW
DEPENDS
ON
THE
DATA
BAYES
RULE
NO
LONGER
AP
PLIES
INSTEAD
WE
USE
A
DIRECT
MODEL
FOR
THE
POSTERIOR
DISTRIBUTION
P
X
Y
WHOSE
NEGATIVE
LOG
LIKELIHOOD
CAN
BE
WRITTEN
AS
E
X
Y
ED
X
Y
ES
X
Y
VP
XP
Y
VP
Q
XP
XQ
Y
USING
THE
NOTATION
INTRODUCED
IN
THE
RESULTING
PROBABILITY
DISTRIBUTION
IS
CALLED
A
CONDITIONAL
RANDOM
FIELD
CRF
AND
WAS
FIRST
INTRODUCED
TO
THE
COMPUTER
VISION
FIELD
BY
KU
MAR
AND
HEBERT
BASED
ON
EARLIER
WORK
IN
TEXT
MODELING
BY
LAFFERTY
MCCALLUM
AND
PEREIRA
FIGURE
SHOWS
A
GRAPHICAL
MODEL
WHERE
THE
SMOOTHNESS
TERMS
DEPEND
ON
THE
DATA
VALUES
IN
THIS
PARTICULAR
MODEL
EACH
SMOOTHNESS
TERM
DEPENDS
ONLY
ON
ITS
ADJACENT
PAIR
OF
DATA
VALUES
I
E
TERMS
ARE
OF
THE
FORM
VP
Q
XP
XQ
YP
YQ
IN
THE
IDEA
OF
MODIFYING
SMOOTHNESS
TERMS
IN
RESPONSE
TO
INPUT
DATA
IS
NOT
NEW
FOR
EX
AMPLE
BOYKOV
AND
JOLLY
USED
THIS
IDEA
FOR
INTERACTIVE
SEGMENTATION
AS
SHOWN
IN
FIGURE
AND
IT
IS
NOW
WIDELY
USED
IN
IMAGE
SEGMENTATION
SECTION
BLAKE
ROTHER
AN
ALTERNATIVE
FORMULATION
THAT
ALSO
USES
DETECTED
EDGES
TO
MODULATE
THE
SMOOTHNESS
OF
A
DEPTH
OR
MOTION
FIELD
AND
HENCE
TO
INTEGRATE
MULTIPLE
LOWER
LEVEL
VISION
MODULES
IS
PRESENTED
BY
POGGIO
GAMBLE
AND
LITTLE
D
I
J
D
I
J
W
I
J
F
I
J
SY
I
J
F
I
J
F
I
J
SX
I
J
F
I
J
FIGURE
GRAPHICAL
MODEL
FOR
A
DISCRIMINATIVE
RANDOM
FIELD
DRF
THE
ADDITIONAL
GREEN
EDGES
SHOW
HOW
COMBINATIONS
OF
SENSED
DATA
E
G
D
I
J
INFLUENCE
THE
DATA
TERM
FOR
F
I
J
THE
GENERATIVE
MODEL
IS
THEREFORE
MORE
COMPLEX
I
E
WE
CANNOT
JUST
APPLY
A
SIMPLE
FUNCTION
TO
THE
UNKNOWN
VARIABLES
AND
ADD
NOISE
BROWN
ET
AL
ROTHER
KOLMOGOROV
AND
BLAKE
DENOISING
TAPPEN
LIU
FREEMAN
ET
AL
AND
OBJECT
RECOGNITION
SECTION
WINN
AND
SHOTTON
SHOTTON
WINN
ROTHER
ET
AL
IN
STEREO
MATCHING
THE
IDEA
OF
ENCOURAGING
DISPARITY
DISCONTINUITIES
TO
COINCIDE
WITH
INTENSITY
EDGES
GOES
BACK
EVEN
FURTHER
TO
THE
EARLY
DAYS
OF
OPTIMIZATION
AND
MRF
BASED
ALGORITHMS
POGGIO
GAMBLE
AND
LITTLE
FUA
BOBICK
AND
INTILLE
BOYKOV
VEKSLER
AND
ZABIH
AND
IS
DISCUSSED
IN
MORE
DETAIL
IN
SECTION
IN
ADDITION
TO
USING
SMOOTHNESS
TERMS
THAT
ADAPT
TO
THE
INPUT
DATA
KUMAR
AND
HEBERT
ALSO
COMPUTE
A
NEIGHBORHOOD
FUNCTION
OVER
THE
INPUT
DATA
FOR
EACH
VP
XP
Y
TERM
AS
ILLUSTRATED
IN
FIGURE
INSTEAD
OF
USING
THE
CLASSIC
UNARY
MRF
DATA
TERM
VP
XP
YP
SHOWN
IN
FIGURE
BECAUSE
SUCH
NEIGHBORHOOD
FUNCTIONS
CAN
BE
THOUGHT
OF
AS
DIS
CRIMINANT
FUNCTIONS
A
TERM
WIDELY
USED
IN
MACHINE
LEARNING
BISHOP
THEY
CALL
THE
RESULTING
GRAPHICAL
MODEL
A
DISCRIMINATIVE
RANDOM
FIELD
DRF
IN
THEIR
PAPER
KUMAR
AND
HEBERT
SHOW
THAT
DRFS
OUTPERFORM
SIMILAR
CRFS
ON
A
NUMBER
OF
APPLICATIONS
SUCH
AS
STRUCTURE
DETECTION
FIGURE
AND
BINARY
IMAGE
DENOISING
HERE
AGAIN
ONE
COULD
ARGUE
THAT
PREVIOUS
STEREO
CORRESPONDENCE
ALGORITHMS
ALSO
LOOK
AT
A
NEIGHBORHOOD
OF
INPUT
DATA
EITHER
EXPLICITLY
BECAUSE
THEY
COMPUTE
CORRELATION
MEASURES
CRIMINISI
CROSS
BLAKE
ET
AL
AS
DATA
TERMS
OR
IMPLICITLY
BECAUSE
EVEN
PIXEL
WISE
DISPARITY
COSTS
LOOK
AT
SEVERAL
PIXELS
IN
EITHER
THE
LEFT
OR
RIGHT
IMAGE
BARNARD
BOYKOV
VEKSLER
AND
ZABIH
KUMAR
AND
HEBERT
CALL
THE
UNARY
POTENTIALS
VP
XP
Y
ASSOCIATION
POTENTIALS
AND
THE
PAIRWISE
POTENTIALS
VP
Q
XP
YQ
Y
INTERACTION
POTENTIALS
FIGURE
STRUCTURE
DETECTION
RESULTS
USING
AN
MRF
LEFT
AND
A
DRF
RIGHT
KUMAR
AND
HEBERT
QC
SPRINGER
WHAT
THEN
ARE
THE
ADVANTAGES
AND
DISADVANTAGES
OF
USING
CONDITIONAL
OR
DISCRIMINATIVE
RANDOM
FIELDS
INSTEAD
OF
MRFS
CLASSIC
BAYESIAN
INFERENCE
MRF
ASSUMES
THAT
THE
PRIOR
DISTRIBUTION
OF
THE
DATA
IS
IN
DEPENDENT
OF
THE
MEASUREMENTS
THIS
MAKES
A
LOT
OF
SENSE
IF
YOU
SEE
A
PAIR
OF
SIXES
WHEN
YOU
FIRST
THROW
A
PAIR
OF
DICE
IT
WOULD
BE
UNWISE
TO
ASSUME
THAT
THEY
WILL
ALWAYS
SHOW
UP
THEREAFTER
HOWEVER
IF
AFTER
PLAYING
FOR
A
LONG
TIME
YOU
DETECT
A
STATISTICALLY
SIGNIFICANT
BIAS
YOU
MAY
WANT
TO
ADJUST
YOUR
PRIOR
WHAT
CRFS
DO
IN
ESSENCE
IS
TO
SELECT
OR
MODIFY
THE
PRIOR
MODEL
BASED
ON
OBSERVED
DATA
THIS
CAN
BE
VIEWED
AS
MAKING
A
PARTIAL
INFERENCE
OVER
ADDI
TIONAL
HIDDEN
VARIABLES
OR
CORRELATIONS
BETWEEN
THE
UNKNOWNS
SAY
A
LABEL
DEPTH
OR
CLEAN
IMAGE
AND
THE
KNOWNS
OBSERVED
IMAGES
IN
SOME
CASES
THE
CRF
APPROACH
MAKES
A
LOT
OF
SENSE
AND
IS
IN
FACT
THE
ONLY
PLAUSI
BLE
WAY
TO
PROCEED
FOR
EXAMPLE
IN
GRAYSCALE
IMAGE
COLORIZATION
SECTION
LEVIN
LISCHINSKI
AND
WEISS
THE
BEST
WAY
TO
TRANSFER
THE
CONTINUITY
INFORMATION
FROM
THE
INPUT
GRAYSCALE
IMAGE
TO
THE
UNKNOWN
COLOR
IMAGE
IS
TO
MODIFY
LOCAL
SMOOTHNESS
CONSTRAINTS
SIMILARLY
FOR
SIMULTANEOUS
SEGMENTATION
AND
RECOGNITION
WINN
AND
SHOTTON
SHOTTON
WINN
ROTHER
ET
AL
IT
MAKES
A
LOT
OF
SENSE
TO
PERMIT
STRONG
COLOR
EDGES
TO
INFLUENCE
THE
SEMANTIC
IMAGE
LABEL
CONTINUITIES
IN
OTHER
CASES
SUCH
AS
IMAGE
DENOISING
THE
SITUATION
IS
MORE
SUBTLE
USING
A
NON
QUADRATIC
ROBUST
SMOOTHNESS
TERM
AS
IN
PLAYS
A
QUALITATIVELY
SIMILAR
ROLE
TO
SETTING
THE
SMOOTHNESS
BASED
ON
LOCAL
GRADIENT
INFORMATION
IN
A
GAUSSIAN
MRF
GMRF
TAPPEN
LIU
FREEMAN
ET
AL
IN
MORE
RECENT
WORK
TANAKA
AND
OKUTOMI
USE
A
LARGER
NEIGHBORHOOD
AND
FULL
COVARIANCE
MATRIX
ON
A
RELATED
GAUSSIAN
MRF
THE
ADVANTAGE
OF
GAUS
SIAN
MRFS
WHEN
THE
SMOOTHNESS
CAN
BE
CORRECTLY
INFERRED
IS
THAT
THE
RESULTING
QUADRATIC
ENERGY
CAN
BE
MINIMIZED
IN
A
SINGLE
STEP
HOWEVER
FOR
SITUATIONS
WHERE
THE
DISCONTINUITIES
ARE
NOT
SELF
EVIDENT
IN
THE
INPUT
DATA
SUCH
AS
FOR
PIECEWISE
SMOOTH
SPARSE
DATA
INTERPOLATION
BLAKE
AND
ZISSERMAN
TERZOPOULOS
CLASSIC
ROBUST
SMOOTHNESS
ENERGY
MINIMIZA
TION
MAY
BE
PREFERABLE
THUS
AS
WITH
MOST
COMPUTER
VISION
ALGORITHMS
A
CAREFUL
ANALYSIS
OF
THE
PROBLEM
AT
HAND
AND
DESIRED
ROBUSTNESS
AND
COMPUTATION
CONSTRAINTS
MAY
BE
REQUIRED
TO
CHOOSE
THE
BEST
TECHNIQUE
PERHAPS
THE
BIGGEST
ADVANTAGE
OF
CRFS
AND
DRFS
AS
ARGUED
BY
KUMAR
AND
HEBERT
TAPPEN
LIU
FREEMAN
ET
AL
AND
BLAKE
ROTHER
BROWN
ET
AL
IS
THAT
LEARNING
THE
MODEL
PARAMETERS
IS
SOMETIMES
EASIER
WHILE
LEARNING
PARAMETERS
IN
MRFS
AND
THEIR
VARIANTS
IS
NOT
A
TOPIC
THAT
WE
COVER
IN
THIS
BOOK
INTERESTED
READERS
CAN
FIND
MORE
DETAILS
IN
RECENTLY
PUBLISHED
ARTICLES
KUMAR
AND
HEBERT
ROTH
AND
BLACK
TAPPEN
LIU
FREEMAN
ET
AL
TAPPEN
LI
AND
HUTTENLOCHER
APPLICATION
IMAGE
RESTORATION
IN
SECTION
WE
SAW
HOW
TWO
DIMENSIONAL
LINEAR
AND
NON
LINEAR
FILTERS
CAN
BE
USED
TO
REMOVE
NOISE
OR
ENHANCE
SHARPNESS
IN
IMAGES
SOMETIMES
HOWEVER
IMAGES
ARE
DEGRADED
BY
LARGER
PROBLEMS
SUCH
AS
SCRATCHES
AND
BLOTCHES
KOKARAM
IN
THIS
CASE
BAYESIAN
METH
ODS
SUCH
AS
MRFS
WHICH
CAN
MODEL
SPATIALLY
VARYING
PER
PIXEL
MEASUREMENT
NOISE
CAN
BE
USED
INSTEAD
AN
ALTERNATIVE
IS
TO
USE
HOLE
FILLING
OR
INPAINTING
TECHNIQUES
BERTALMIO
SAPIRO
CASELLES
ET
AL
BERTALMIO
VESE
SAPIRO
ET
AL
CRIMINISI
PE
REZ
AND
TOYAMA
AS
DISCUSSED
IN
SECTIONS
AND
FIGURE
SHOWS
AN
EXAMPLE
OF
IMAGE
DENOISING
AND
INPAINTING
HOLE
FILLING
USING
A
MARKOV
RANDOM
FIELD
THE
ORIGINAL
IMAGE
HAS
BEEN
CORRUPTED
BY
NOISE
AND
A
PORTION
OF
THE
DATA
HAS
BEEN
REMOVED
IN
THIS
CASE
THE
LOOPY
BELIEF
PROPAGATION
ALGORITHM
COMPUTES
A
SLIGHTLY
LOWER
ENERGY
AND
ALSO
A
SMOOTHER
IMAGE
THAN
THE
ALPHA
EXPANSION
GRAPH
CUT
ALGO
RITHM
ADDITIONAL
READING
IF
YOU
ARE
INTERESTED
IN
EXPLORING
THE
TOPIC
OF
IMAGE
PROCESSING
IN
MORE
DEPTH
SOME
POPULAR
TEXTBOOKS
HAVE
BEEN
WRITTEN
BY
LIM
CRANE
GOMES
AND
VELHO
JA
HNE
PRATT
RUSS
BURGER
AND
BURGE
GONZALES
AND
WOODS
THE
PRE
EMINENT
CONFERENCE
AND
JOURNAL
IN
THIS
FIELD
ARE
THE
IEEE
CONFERENCE
ON
IMAGE
PRO
CESSSING
AND
THE
IEEE
TRANSACTIONS
ON
IMAGE
PROCESSING
FOR
IMAGE
COMPOSITING
OPERATORS
THE
SEMINAL
REFERENCE
IS
BY
PORTER
AND
DUFF
WHILE
BLINN
B
PROVIDES
A
MORE
DETAILED
TUTORIAL
FOR
IMAGE
COMPOSITING
SMITH
AND
BLINN
WERE
THE
FIRST
TO
BRING
THIS
TOPIC
TO
THE
ATTENTION
OF
THE
GRAPHICS
COMMUNITY
WHILE
WANG
AND
COHEN
PROVIDE
A
RECENT
IN
DEPTH
SURVEY
IN
THE
REALM
OF
LINEAR
FILTERING
FREEMAN
AND
ADELSON
PROVIDE
A
GREAT
INTRODUC
TION
TO
SEPARABLE
AND
STEERABLE
ORIENTED
BAND
PASS
FILTERS
WHILE
PERONA
SHOWS
HOW
TO
ADDITIONAL
READING
APPROXIMATE
ANY
FILTER
AS
A
SUM
OF
SEPARABLE
COMPONENTS
THE
LITERATURE
ON
NON
LINEAR
FILTERING
IS
QUITE
WIDE
AND
VARIED
IT
INCLUDES
SUCH
TOPICS
AS
BILATERAL
FILTERING
TOMASI
AND
MANDUCHI
DURAND
AND
DORSEY
PARIS
AND
DURAND
CHEN
PARIS
AND
DURAND
PARIS
KORNPROBST
TUMBLIN
ET
AL
RELATED
ITERA
TIVE
ALGORITHMS
SAINT
MARC
CHEN
AND
MEDIONI
NIELSEN
FLORACK
AND
DERICHE
BLACK
SAPIRO
MARIMONT
ET
AL
WEICKERT
TER
HAAR
ROMENY
AND
VIERGEVER
WEICK
ERT
BARASH
SCHARR
BLACK
AND
HAUSSECKER
BARASH
AND
COMANICIU
AND
VARIATIONAL
APPROACHES
CHAN
OSHER
AND
SHEN
TSCHUMPERLE
AND
DERICHE
TSCHUMPERLE
KAFTORY
SCHECHNER
AND
ZEEVI
GOOD
REFERENCES
TO
IMAGE
MORPHOLOGY
INCLUDE
HARALICK
AND
SHAPIRO
SECTION
BOVIK
SECTION
RITTER
AND
WILSON
SECTION
SERRA
SERRA
AND
VINCENT
YUILLE
VINCENT
AND
GEIGER
SOILLE
THE
CLASSIC
PAPERS
FOR
IMAGE
PYRAMIDS
AND
PYRAMID
BLENDING
ARE
BY
BURT
AND
ADELSON
B
WAVELETS
WERE
FIRST
INTRODUCED
TO
THE
COMPUTER
VISION
COMMUNITY
BY
MALLAT
AND
GOOD
TUTORIAL
AND
REVIEW
PAPERS
AND
BOOKS
ARE
AVAILABLE
STRANG
SIMONCELLI
AND
ADELSON
RIOUL
AND
VETTERLI
CHUI
MEYER
SWELDENS
WAVELETS
ARE
WIDELY
USED
IN
THE
COMPUTER
GRAPHICS
COMMUNITY
TO
PERFORM
MULTI
RESOLUTION
GEOMET
RIC
PROCESSING
STOLLNITZ
DEROSE
AND
SALESIN
AND
HAVE
BEEN
USED
IN
COMPUTER
VISION
FOR
SIMILAR
APPLICATIONS
SZELISKI
PENTLAND
GORTLER
AND
COHEN
YAOU
AND
CHANG
LAI
AND
VEMURI
SZELISKI
AS
WELL
AS
FOR
MULTI
SCALE
ORIENTED
FILTER
ING
SIMONCELLI
FREEMAN
ADELSON
ET
AL
AND
DENOISING
PORTILLA
STRELA
WAINWRIGHT
ET
AL
WHILE
IMAGE
PYRAMIDS
SECTION
ARE
USUALLY
CONSTRUCTED
USING
LINEAR
FILTERING
OP
ERATORS
SOME
RECENT
WORK
HAS
STARTED
INVESTIGATING
NON
LINEAR
FILTERS
SINCE
THESE
CAN
BETTER
PRESERVE
DETAILS
AND
OTHER
SALIENT
FEATURES
SOME
REPRESENTATIVE
PAPERS
IN
THE
COMPUTER
VISION
LITERATURE
ARE
BY
GLUCKMAN
B
LYU
AND
SIMONCELLI
AND
IN
COMPUTATIONAL
PHO
TOGRAPHY
BY
BAE
PARIS
AND
DURAND
FARBMAN
FATTAL
LISCHINSKI
ET
AL
FATTAL
HIGH
QUALITY
ALGORITHMS
FOR
IMAGE
WARPING
AND
RESAMPLING
ARE
COVERED
BOTH
IN
THE
IM
AGE
PROCESSING
LITERATURE
WOLBERG
DODGSON
GOMES
DARSA
COSTA
ET
AL
SZELISKI
WINDER
AND
UYTTENDAELE
AND
IN
COMPUTER
GRAPHICS
WILLIAMS
HECKBERT
BARKANS
AKENINE
MO
LLER
AND
HAINES
WHERE
THEY
GO
UNDER
THE
NAME
OF
TEXTURE
MAPPING
COMBINATION
OF
IMAGE
WARPING
AND
IMAGE
BLENDING
TECHNIQUES
ARE
USED
TO
ENABLE
MORPHING
BETWEEN
IMAGES
WHICH
IS
COVERED
IN
A
SERIES
OF
SEMINAL
PAPERS
AND
BOOKS
BEIER
AND
NEELY
GOMES
DARSA
COSTA
ET
AL
THE
REGULARIZATION
APPROACH
TO
COMPUTER
VISION
PROBLEMS
WAS
FIRST
INTRODUCED
TO
THE
VI
SION
COMMUNITY
BY
POGGIO
TORRE
AND
KOCH
AND
TERZOPOULOS
B
AND
CONTINUES
TO
BE
A
POPULAR
FRAMEWORK
FOR
FORMULATING
AND
SOLVING
LOW
LEVEL
VISION
PROBLEMS
JU
BLACK
AND
JEPSON
NIELSEN
FLORACK
AND
DERICHE
NORDSTRO
M
BROX
BRUHN
PAPENBERG
ET
AL
LEVIN
LISCHINSKI
AND
WEISS
MORE
DETAILED
MATHE
MATICAL
TREATMENT
AND
ADDITIONAL
APPLICATIONS
CAN
BE
FOUND
IN
THE
APPLIED
MATHEMATICS
AND
STATISTICS
LITERATURE
TIKHONOV
AND
ARSENIN
ENGL
HANKE
AND
NEUBAUER
THE
LITERATURE
ON
MARKOV
RANDOM
FIELDS
IS
TRULY
IMMENSE
WITH
PUBLICATIONS
IN
RELATED
FIELDS
SUCH
AS
OPTIMIZATION
AND
CONTROL
THEORY
OF
WHICH
FEW
VISION
PRACTITIONERS
ARE
EVEN
AWARE
A
GOOD
GUIDE
TO
THE
LATEST
TECHNIQUES
IS
THE
BOOK
EDITED
BY
BLAKE
KOHLI
AND
ROTHER
OTHER
RECENT
ARTICLES
THAT
CONTAIN
NICE
LITERATURE
REVIEWS
OR
EXPERIMENTAL
COMPAR
ISONS
INCLUDE
BOYKOV
AND
FUNKA
LEA
SZELISKI
ZABIH
SCHARSTEIN
ET
AL
KUMAR
VEKSLER
AND
TORR
THE
SEMINAL
PAPER
ON
MARKOV
RANDOM
FIELDS
IS
THE
WORK
OF
GEMAN
AND
GEMAN
WHO
INTRODUCED
THIS
FORMALISM
TO
COMPUTER
VISION
RESEARCHERS
AND
ALSO
INTRODUCED
THE
NO
TION
OF
LINE
PROCESSES
ADDITIONAL
BINARY
VARIABLES
THAT
CONTROL
WHETHER
SMOOTHNESS
PENALTIES
ARE
ENFORCED
OR
NOT
BLACK
AND
RANGARAJAN
SHOWED
HOW
INDEPENDENT
LINE
PROCESSES
COULD
BE
REPLACED
WITH
ROBUST
PAIRWISE
POTENTIALS
BOYKOV
VEKSLER
AND
ZABIH
DEVEL
OPED
ITERATIVE
BINARY
GRAPH
CUT
ALGORITHMS
FOR
OPTIMIZING
MULTI
LABEL
MRFS
KOLMOGOROV
AND
ZABIH
CHARACTERIZED
THE
CLASS
OF
BINARY
ENERGY
POTENTIALS
REQUIRED
FOR
THESE
TECH
NIQUES
TO
WORK
AND
FREEMAN
PASZTOR
AND
CARMICHAEL
POPULARIZED
THE
USE
OF
LOOPY
BELIEF
PROPAGATION
FOR
MRF
INFERENCE
MANY
MORE
ADDITIONAL
REFERENCES
CAN
BE
FOUND
IN
SECTIONS
AND
AND
APPENDIX
B
EXERCISES
EX
COLOR
BALANCE
WRITE
A
SIMPLE
APPLICATION
TO
CHANGE
THE
COLOR
BALANCE
OF
AN
IMAGE
BY
MULTIPLYING
EACH
COLOR
VALUE
BY
A
DIFFERENT
USER
SPECIFIED
CONSTANT
IF
YOU
WANT
TO
GET
FANCY
YOU
CAN
MAKE
THIS
APPLICATION
INTERACTIVE
WITH
SLIDERS
DO
YOU
GET
DIFFERENT
RESULTS
IF
YOU
TAKE
OUT
THE
GAMMA
TRANSFORMATION
BEFORE
OR
AFTER
DOING
THE
MULTIPLICATION
WHY
OR
WHY
NOT
TAKE
THE
SAME
PICTURE
WITH
YOUR
DIGITAL
CAMERA
USING
DIFFERENT
COLOR
BALANCE
SETTINGS
MOST
CAMERAS
CONTROL
THE
COLOR
BALANCE
FROM
ONE
OF
THE
MENUS
CAN
YOU
RECOVER
WHAT
THE
COLOR
BALANCE
RATIOS
ARE
BETWEEN
THE
DIFFERENT
SETTINGS
YOU
MAY
NEED
TO
PUT
YOUR
CAMERA
ON
A
TRIPOD
AND
ALIGN
THE
IMAGES
MANUALLY
OR
AUTOMATICALLY
TO
MAKE
THIS
WORK
ALTERNATIVELY
USE
A
COLOR
CHECKER
CHART
FIGURE
AS
DISCUSSED
IN
SECTIONS
AND
IF
YOU
HAVE
ACCESS
TO
THE
RAW
IMAGE
FOR
THE
CAMERA
PERFORM
THE
DEMOSAICING
YOURSELF
SECTION
OR
DOWNSAMPLE
THE
IMAGE
RESOLUTION
TO
GET
A
TRUE
RGB
IMAGE
DOES
YOUR
CAMERA
PERFORM
A
SIMPLE
LINEAR
MAPPING
BETWEEN
RAW
VALUES
AND
THE
COLOR
BALANCED
VALUES
IN
A
JPEG
SOME
HIGH
END
CAMERAS
HAVE
A
RAW
JPEG
MODE
WHICH
MAKES
THIS
COMPARISON
MUCH
EASIER
CAN
YOU
THINK
OF
ANY
REASON
WHY
YOU
MIGHT
WANT
TO
PERFORM
A
COLOR
TWIST
SEC
TION
ON
THE
IMAGES
SEE
ALSO
EXERCISE
FOR
SOME
RELATED
IDEAS
EX
COMPOSITING
AND
REFLECTIONS
SECTION
DESCRIBES
THE
PROCESS
OF
COMPOSITING
AN
ALPHA
MATTED
IMAGE
ON
TOP
OF
ANOTHER
ANSWER
THE
FOLLOWING
QUESTIONS
AND
OPTIONALLY
VALIDATE
THEM
EXPERIMENTALLY
MOST
CAPTURED
IMAGES
HAVE
GAMMA
CORRECTION
APPLIED
TO
THEM
DOES
THIS
INVALIDATE
THE
BASIC
COMPOSITING
EQUATION
IF
SO
HOW
SHOULD
IT
BE
FIXED
THE
ADDITIVE
PURE
REFLECTION
MODEL
MAY
HAVE
LIMITATIONS
WHAT
HAPPENS
IF
THE
GLASS
IS
TINTED
ESPECIALLY
TO
A
NON
GRAY
HUE
HOW
ABOUT
IF
THE
GLASS
IS
DIRTY
OR
SMUDGED
HOW
COULD
YOU
MODEL
WAVY
GLASS
OR
OTHER
KINDS
OF
REFRACTIVE
OBJECTS
EX
BLUE
SCREEN
MATTING
SET
UP
A
BLUE
OR
GREEN
BACKGROUND
E
G
BY
BUYING
A
LARGE
PIECE
OF
COLORED
POSTERBOARD
TAKE
A
PICTURE
OF
THE
EMPTY
BACKGROUND
AND
THEN
OF
THE
BACK
GROUND
WITH
A
NEW
OBJECT
IN
FRONT
OF
IT
PULL
THE
MATTE
USING
THE
DIFFERENCE
BETWEEN
EACH
COLORED
PIXEL
AND
ITS
ASSUMED
CORRESPONDING
BACKGROUND
PIXEL
USING
ONE
OF
THE
TECHNIQUES
DESCRIBED
IN
SECTION
OR
BY
SMITH
AND
BLINN
EX
DIFFERENCE
KEYING
IMPLEMENT
A
DIFFERENCE
KEYING
ALGORITHM
SEE
SECTION
TOYAMA
KRUMM
BRUMITT
ET
AL
CONSISTING
OF
THE
FOLLOWING
STEPS
COMPUTE
THE
MEAN
AND
VARIANCE
OR
MEDIAN
AND
ROBUST
VARIANCE
AT
EACH
PIXEL
IN
AN
EMPTY
VIDEO
SEQUENCE
FOR
EACH
NEW
FRAME
CLASSIFY
EACH
PIXEL
AS
FOREGROUND
OR
BACKGROUND
SET
THE
BACK
GROUND
PIXELS
TO
RGBA
OPTIONAL
COMPUTE
THE
ALPHA
CHANNEL
AND
COMPOSITE
OVER
A
NEW
BACKGROUND
OPTIONAL
CLEAN
UP
THE
IMAGE
USING
MORPHOLOGY
SECTION
LABEL
THE
CONNECTED
COMPONENTS
SECTION
COMPUTE
THEIR
CENTROIDS
AND
TRACK
THEM
FROM
FRAME
TO
FRAME
USE
THIS
TO
BUILD
A
PEOPLE
COUNTER
EX
PHOTO
EFFECTS
WRITE
A
VARIETY
OF
PHOTO
ENHANCEMENT
OR
EFFECTS
FILTERS
CONTRAST
SO
LARIZATION
QUANTIZATION
ETC
WHICH
ONES
ARE
USEFUL
PERFORM
SENSIBLE
CORRECTIONS
AND
WHICH
ONES
ARE
MORE
CREATIVE
CREATE
UNUSUAL
IMAGES
EX
HISTOGRAM
EQUALIZATION
COMPUTE
THE
GRAY
LEVEL
LUMINANCE
HISTOGRAM
FOR
AN
IM
AGE
AND
EQUALIZE
IT
SO
THAT
THE
TONES
LOOK
BETTER
AND
THE
IMAGE
IS
LESS
SENSITIVE
TO
EXPOSURE
SETTINGS
YOU
MAY
WANT
TO
USE
THE
FOLLOWING
STEPS
CONVERT
THE
COLOR
IMAGE
TO
LUMINANCE
SECTION
COMPUTE
THE
HISTOGRAM
THE
CUMULATIVE
DISTRIBUTION
AND
THE
COMPENSATION
TRANSFER
FUNCTION
SECTION
OPTIONAL
TRY
TO
INCREASE
THE
PUNCH
IN
THE
IMAGE
BY
ENSURING
THAT
A
CERTAIN
FRACTION
OF
PIXELS
SAY
ARE
MAPPED
TO
PURE
BLACK
AND
WHITE
OPTIONAL
LIMIT
THE
LOCAL
GAIN
F
I
I
IN
THE
TRANSFER
FUNCTION
ONE
WAY
TO
DO
THIS
IS
TO
LIMIT
F
I
ΓI
OR
F
I
I
Γ
WHILE
PERFORMING
THE
ACCUMULATION
KEEPING
ANY
UNACCUMULATED
VALUES
IN
RESERVE
I
LL
LET
YOU
FIGURE
OUT
THE
EXACT
DETAILS
COMPENSATE
THE
LUMINANCE
CHANNEL
THROUGH
THE
LOOKUP
TABLE
AND
RE
GENERATE
THE
COLOR
IMAGE
USING
COLOR
RATIOS
OPTIONAL
COLOR
VALUES
THAT
ARE
CLIPPED
IN
THE
ORIGINAL
IMAGE
I
E
HAVE
ONE
OR
MORE
SATURATED
COLOR
CHANNELS
MAY
APPEAR
UNNATURAL
WHEN
REMAPPED
TO
A
NON
CLIPPED
VALUE
EXTEND
YOUR
ALGORITHM
TO
HANDLE
THIS
CASE
IN
SOME
USEFUL
WAY
EX
LOCAL
HISTOGRAM
EQUALIZATION
COMPUTE
THE
GRAY
LEVEL
LUMINANCE
HISTOGRAMS
FOR
EACH
PATCH
BUT
ADD
TO
VERTICES
BASED
ON
DISTANCE
A
SPLINE
BUILD
ON
EXERCISE
LUMINANCE
COMPUTATION
DISTRIBUTE
VALUES
COUNTS
TO
ADJACENT
VERTICES
BILINEAR
CONVERT
TO
CDF
LOOK
UP
FUNCTIONS
OPTIONAL
USE
LOW
PASS
FILTERING
OF
CDFS
INTERPOLATE
ADJACENT
CDFS
FOR
FINAL
LOOKUP
EX
PADDING
FOR
NEIGHBORHOOD
OPERATIONS
WRITE
DOWN
THE
FORMULAS
FOR
COMPUTING
THE
PADDED
PIXEL
VALUES
F
I
J
AS
A
FUNCTION
OF
THE
ORIGINAL
PIXEL
VALUES
F
K
L
AND
THE
IMAGE
WIDTH
AND
HEIGHT
M
N
FOR
EACH
OF
THE
PADDING
MODES
SHOWN
IN
FIGURE
FOR
EXAMPLE
FOR
REPLICATION
CLAMPING
F
I
J
F
K
L
K
MAX
MIN
M
I
L
MAX
MIN
N
J
HINT
YOU
MAY
WANT
TO
USE
THE
MIN
MAX
MOD
AND
ABSOLUTE
VALUE
OPERATORS
IN
ADDITION
TO
THE
REGULAR
ARITHMETIC
OPERATORS
DESCRIBE
IN
MORE
DETAIL
THE
ADVANTAGES
AND
DISADVANTAGES
OF
THESE
VARIOUS
MODES
OPTIONAL
CHECK
WHAT
YOUR
GRAPHICS
CARD
DOES
BY
DRAWING
A
TEXTURE
MAPPED
RECTANGLE
WHERE
THE
TEXTURE
COORDINATES
LIE
BEYOND
THE
RANGE
AND
USING
DIFFERENT
TEXTURE
CLAMPING
MODES
EX
SEPARABLE
FILTERS
IMPLEMENT
CONVOLUTION
WITH
A
SEPARABLE
KERNEL
THE
INPUT
SHOULD
BE
A
GRAYSCALE
OR
COLOR
IMAGE
ALONG
WITH
THE
HORIZONTAL
AND
VERTICAL
KERNELS
MAKE
SURE
YOU
SUPPORT
THE
PADDING
MECHANISMS
DEVELOPED
IN
THE
PREVIOUS
EXERCISE
YOU
WILL
NEED
THIS
FUNCTIONALITY
FOR
SOME
OF
THE
LATER
EXERCISES
IF
YOU
ALREADY
HAVE
ACCESS
TO
SEPARABLE
FILTERING
IN
AN
IMAGE
PROCESSING
PACKAGE
YOU
ARE
USING
SUCH
AS
IPL
SKIP
THIS
EXERCISE
OPTIONAL
USE
PIETRO
PERONA
TECHNIQUE
TO
APPROXIMATE
CONVOLUTION
AS
A
SUM
OF
A
NUMBER
OF
SEPARABLE
KERNELS
LET
THE
USER
SPECIFY
THE
NUMBER
OF
KERNELS
AND
REPORT
BACK
SOME
SENSIBLE
METRIC
OF
THE
APPROXIMATION
FIDELITY
EX
DISCRETE
GAUSSIAN
FILTERS
DISCUSS
THE
FOLLOWING
ISSUES
WITH
IMPLEMENTING
A
DIS
CRETE
GAUSSIAN
FILTER
IF
YOU
JUST
SAMPLE
THE
EQUATION
OF
A
CONTINUOUS
GAUSSIAN
FILTER
AT
DISCRETE
LOCATIONS
WILL
YOU
GET
THE
DESIRED
PROPERTIES
E
G
WILL
THE
COEFFICIENTS
SUM
UP
TO
SIMILARLY
IF
YOU
SAMPLE
A
DERIVATIVE
OF
A
GAUSSIAN
DO
THE
SAMPLES
SUM
UP
TO
OR
HAVE
VANISHING
HIGHER
ORDER
MOMENTS
WOULD
IT
BE
PREFERABLE
TO
TAKE
THE
ORIGINAL
SIGNAL
INTERPOLATE
IT
WITH
A
SINC
BLUR
WITH
A
CONTINUOUS
GAUSSIAN
THEN
PRE
FILTER
WITH
A
SINC
BEFORE
RE
SAMPLING
IS
THERE
A
SIMPLER
WAY
TO
DO
THIS
IN
THE
FREQUENCY
DOMAIN
WOULD
IT
MAKE
MORE
SENSE
TO
PRODUCE
A
GAUSSIAN
FREQUENCY
RESPONSE
IN
THE
FOURIER
DOMAIN
AND
TO
THEN
TAKE
AN
INVERSE
FFT
TO
OBTAIN
A
DISCRETE
FILTER
HOW
DOES
TRUNCATION
OF
THE
FILTER
CHANGE
ITS
FREQUENCY
RESPONSE
DOES
IT
INTRODUCE
ANY
ADDITIONAL
ARTIFACTS
ARE
THE
RESULTING
TWO
DIMENSIONAL
FILTERS
AS
ROTATIONALLY
INVARIANT
AS
THEIR
CONTINUOUS
ANALOGS
IS
THERE
SOME
WAY
TO
IMPROVE
THIS
IN
FACT
CAN
ANY
DISCRETE
SEPARABLE
OR
NON
SEPARABLE
FILTER
BE
TRULY
ROTATIONALLY
INVARIANT
EX
SHARPENING
BLUR
AND
NOISE
REMOVAL
IMPLEMENT
SOME
SOFTENING
SHARPENING
AND
NON
LINEAR
DIFFUSION
SELECTIVE
SHARPENING
OR
NOISE
REMOVAL
FILTERS
SUCH
AS
GAUSSIAN
MEDIAN
AND
BILATERAL
SECTION
AS
DISCUSSED
IN
SECTION
TAKE
BLURRY
OR
NOISY
IMAGES
SHOOTING
IN
LOW
LIGHT
IS
A
GOOD
WAY
TO
GET
BOTH
AND
TRY
TO
IMPROVE
THEIR
APPEARANCE
AND
LEGIBILITY
EX
STEERABLE
FILTERS
IMPLEMENT
FREEMAN
AND
ADELSON
STEERABLE
FILTER
ALGO
RITHM
THE
INPUT
SHOULD
BE
A
GRAYSCALE
OR
COLOR
IMAGE
AND
THE
OUTPUT
SHOULD
BE
A
MULTI
BANDED
IMAGE
CONSISTING
OF
AND
THE
COEFFICIENTS
FOR
THE
FILTERS
CAN
BE
FOUND
IN
THE
PAPER
BY
FREEMAN
AND
ADELSON
TEST
THE
VARIOUS
ORDER
FILTERS
ON
A
NUMBER
OF
IMAGES
OF
YOUR
CHOICE
AND
SEE
IF
YOU
CAN
RELIABLY
FIND
CORNER
AND
INTERSECTION
FEATURES
THESE
FILTERS
WILL
BE
QUITE
USEFUL
LATER
TO
DETECT
ELONGATED
STRUCTURES
SUCH
AS
LINES
SECTION
EX
DISTANCE
TRANSFORM
IMPLEMENT
SOME
RASTER
SCAN
ALGORITHMS
FOR
CITY
BLOCK
AND
EUCLIDEAN
DISTANCE
TRANSFORMS
CAN
YOU
DO
IT
WITHOUT
PEEKING
AT
THE
LITERATURE
DANIELSSON
BORGEFORS
IF
SO
WHAT
PROBLEMS
DID
YOU
COME
ACROSS
AND
RESOLVE
LATER
ON
YOU
CAN
USE
THE
DISTANCE
FUNCTIONS
YOU
COMPUTE
TO
PERFORM
FEATHERING
DURING
IMAGE
STITCHING
SECTION
EX
CONNECTED
COMPONENTS
IMPLEMENT
ONE
OF
THE
CONNECTED
COMPONENT
ALGORITHMS
FROM
SECTION
OR
SECTION
FROM
HARALICK
AND
SHAPIRO
BOOK
AND
DISCUSS
ITS
COMPUTATIONAL
COMPLEXITY
THRESHOLD
OR
QUANTIZE
AN
IMAGE
TO
OBTAIN
A
VARIETY
OF
INPUT
LABELS
AND
THEN
COMPUTE
THE
AREA
STATISTICS
FOR
THE
REGIONS
THAT
YOU
FIND
USE
THE
CONNECTED
COMPONENTS
THAT
YOU
HAVE
FOUND
TO
TRACK
OR
MATCH
REGIONS
IN
DIFFER
ENT
IMAGES
OR
VIDEO
FRAMES
EX
FOURIER
TRANSFORM
PROVE
THE
PROPERTIES
OF
THE
FOURIER
TRANSFORM
LISTED
IN
TA
BLE
AND
DERIVE
THE
FORMULAS
FOR
THE
FOURIER
TRANSFORMS
LISTED
IN
TABLES
AND
THESE
EXERCISES
ARE
VERY
USEFUL
IF
YOU
WANT
TO
BECOME
COMFORTABLE
WORKING
WITH
FOURIER
TRANSFORMS
WHICH
IS
A
VERY
USEFUL
SKILL
WHEN
ANALYZING
AND
DESIGNING
THE
BEHAVIOR
AND
EFFICIENCY
OF
MANY
COMPUTER
VISION
ALGORITHMS
EX
WIENER
FILTERING
ESTIMATE
THE
FREQUENCY
SPECTRUM
OF
YOUR
PERSONAL
PHOTO
COLLEC
TION
AND
USE
IT
TO
PERFORM
WIENER
FILTERING
ON
A
FEW
IMAGES
WITH
VARYING
DEGREES
OF
NOISE
COLLECT
A
FEW
HUNDRED
OF
YOUR
IMAGES
BY
RE
SCALING
THEM
TO
FIT
WITHIN
A
WINDOW
AND
CROPPING
THEM
TAKE
THEIR
FOURIER
TRANSFORMS
THROW
AWAY
THE
PHASE
INFORMATION
AND
AVERAGE
TOGETHER
ALL
OF
THE
SPECTRA
PICK
TWO
OF
YOUR
FAVORITE
IMAGES
AND
ADD
VARYING
AMOUNTS
OF
GAUSSIAN
NOISE
ΣN
GRAY
LEVELS
FOR
EACH
COMBINATION
OF
IMAGE
AND
NOISE
DETERMINE
BY
EYE
WHICH
WIDTH
OF
A
GAUSSIAN
BLURRING
FILTER
ΣS
GIVES
THE
BEST
DENOISED
RESULT
YOU
WILL
HAVE
TO
MAKE
A
SUBJECTIVE
DECISION
BETWEEN
SHARPNESS
AND
NOISE
COMPUTE
THE
WIENER
FILTERED
VERSION
OF
ALL
THE
NOISED
IMAGES
AND
COMPARE
THEM
AGAINST
YOUR
HAND
TUNED
GAUSSIAN
SMOOTHED
IMAGES
OPTIONAL
DO
YOUR
IMAGE
SPECTRA
HAVE
A
LOT
OF
ENERGY
CONCENTRATED
ALONG
THE
HORIZONTAL
AND
VERTICAL
AXES
FX
AND
FY
CAN
YOU
THINK
OF
AN
EXPLANATION
FOR
THIS
DOES
ROTATING
YOUR
IMAGE
SAMPLES
BY
MOVE
THIS
ENERGY
TO
THE
DIAGONALS
IF
NOT
COULD
IT
BE
DUE
TO
EDGE
EFFECTS
IN
THE
FOURIER
TRANSFORM
CAN
YOU
SUGGEST
SOME
TECHNIQUES
FOR
REDUCING
SUCH
EFFECTS
EX
DEBLURRING
USING
WIENER
FILTERING
USE
WIENER
FILTERING
TO
DEBLUR
SOME
IMAGES
MODIFY
THE
WIENER
FILTER
DERIVATION
TO
INCORPORATE
BLUR
DISCUSS
THE
RESULTING
WIENER
FILTER
IN
TERMS
OF
ITS
NOISE
SUPPRESSION
AND
FREQUENCY
BOOSTING
CHARACTERISTICS
ASSUMING
THAT
THE
BLUR
KERNEL
IS
GAUSSIAN
AND
THE
IMAGE
SPECTRUM
FOLLOWS
AN
INVERSE
FREQUENCY
LAW
COMPUTE
THE
FREQUENCY
RESPONSE
OF
THE
WIENER
FILTER
AND
COMPARE
IT
TO
THE
UNSHARP
MASK
SYNTHETICALLY
BLUR
TWO
OF
YOUR
SAMPLE
IMAGES
WITH
GAUSSIAN
BLUR
KERNELS
OF
DIFFERENT
RADII
ADD
NOISE
AND
THEN
PERFORM
WIENER
FILTERING
REPEAT
THE
ABOVE
EXPERIMENT
WITH
A
PILLBOX
DISC
BLURRING
KERNEL
WHICH
IS
CHARAC
TERISTIC
OF
A
FINITE
APERTURE
LENS
SECTION
COMPARE
THESE
RESULTS
TO
GAUSSIAN
BLUR
KERNELS
BE
SURE
TO
INSPECT
YOUR
FREQUENCY
PLOTS
IT
HAS
BEEN
SUGGESTED
THAT
REGULAR
APERTURES
ARE
ANATHEMA
TO
DE
BLURRING
BECAUSE
THEY
INTRODUCE
ZEROS
IN
THE
SENSED
FREQUENCY
SPECTRUM
VEERARAGHAVAN
RASKAR
AGRAWAL
ET
AL
SHOW
THAT
THIS
IS
INDEED
AN
ISSUE
IF
NO
PRIOR
MODEL
IS
ASSUMED
FOR
THE
SIGNAL
I
E
PS
IF
A
REASONABLE
POWER
SPECTRUM
IS
ASSUMED
IS
THIS
STILL
A
PROBLEM
DO
WE
STILL
GET
BANDING
OR
RINGING
ARTIFACTS
EX
HIGH
QUALITY
IMAGE
RESAMPLING
IMPLEMENT
SEVERAL
OF
THE
LOW
PASS
FILTERS
PRE
SENTED
IN
SECTION
AND
ALSO
THE
DISCUSSION
OF
THE
WINDOWED
SINC
SHOWN
IN
TABLE
AND
FIGURE
FEEL
FREE
TO
IMPLEMENT
OTHER
FILTERS
WOLBERG
UNSER
APPLY
YOUR
FILTERS
TO
CONTINUOUSLY
RESIZE
AN
IMAGE
BOTH
MAGNIFYING
INTERPOLATING
AND
MINIFYING
DECIMATING
IT
COMPARE
THE
RESULTING
ANIMATIONS
FOR
SEVERAL
FILTERS
USE
BOTH
A
A
B
C
FIGURE
SAMPLE
IMAGES
FOR
TESTING
THE
QUALITY
OF
RESAMPLING
ALGORITHMS
A
A
SYNTHETIC
CHIRP
B
AND
C
SOME
HIGH
FREQUENCY
IMAGES
FROM
THE
IMAGE
COMPRESSION
COMMUNITY
SYNTHETIC
CHIRP
IMAGE
FIGURE
AND
NATURAL
IMAGES
WITH
LOTS
OF
HIGH
FREQUENCY
DETAIL
FIGURE
C
YOU
MAY
FIND
IT
HELPFUL
TO
WRITE
A
SIMPLE
VISUALIZATION
PROGRAM
THAT
CONTINUOUSLY
PLAYS
THE
ANIMATIONS
FOR
TWO
OR
MORE
FILTERS
AT
ONCE
AND
THAT
LET
YOU
BLINK
BETWEEN
DIFFERENT
RESULTS
DISCUSS
THE
MERITS
AND
DEFICIENCIES
OF
EACH
FILTER
AS
WELL
AS
ITS
TRADEOFF
BETWEEN
SPEED
AND
QUALITY
EX
PYRAMIDS
CONSTRUCT
AN
IMAGE
PYRAMID
THE
INPUTS
SHOULD
BE
A
GRAYSCALE
OR
COLOR
IMAGE
A
SEPARABLE
FILTER
KERNEL
AND
THE
NUMBER
OF
DESIRED
LEVELS
IMPLEMENT
AT
LEAST
THE
FOLLOWING
KERNELS
BLOCK
FILTERING
BURT
AND
ADELSON
BINOMIAL
KERNEL
BURT
AND
ADELSON
A
HIGH
QUALITY
SEVEN
OR
NINE
TAP
FILTER
COMPARE
THE
VISUAL
QUALITY
OF
THE
VARIOUS
DECIMATION
FILTERS
ALSO
SHIFT
YOUR
INPUT
IMAGE
BY
TO
PIXELS
AND
COMPARE
THE
RESULTING
DECIMATED
QUARTER
SIZE
IMAGE
SEQUENCE
EX
PYRAMID
BLENDING
WRITE
A
PROGRAM
THAT
TAKES
AS
INPUT
TWO
COLOR
IMAGES
AND
A
BINARY
MASK
IMAGE
AND
PRODUCES
THE
LAPLACIAN
PYRAMID
BLEND
OF
THE
TWO
IMAGES
CONSTRUCT
THE
LAPLACIAN
PYRAMID
FOR
EACH
IMAGE
CONSTRUCT
THE
GAUSSIAN
PYRAMID
FOR
THE
TWO
MASK
IMAGES
THE
INPUT
IMAGE
AND
ITS
COMPLEMENT
THESE
PARTICULAR
IMAGES
ARE
AVAILABLE
ON
THE
BOOK
WEB
SITE
MULTIPLY
EACH
LAPLACIAN
IMAGE
BY
ITS
CORRESPONDING
MASK
AND
SUM
THE
IMAGES
SEE
FIGURE
RECONSTRUCT
THE
FINAL
IMAGE
FROM
THE
BLENDED
LAPLACIAN
PYRAMID
GENERALIZE
YOUR
ALGORITHM
TO
INPUT
N
IMAGES
AND
A
LABEL
IMAGE
WITH
VALUES
N
THE
VALUE
CAN
BE
RESERVED
FOR
NO
INPUT
DISCUSS
WHETHER
THE
WEIGHTED
SUMMATION
STAGE
STEP
NEEDS
TO
KEEP
TRACK
OF
THE
TOTAL
WEIGHT
FOR
RENORMALIZATION
OR
WHETHER
THE
MATH
JUST
WORKS
OUT
USE
YOUR
ALGORITHM
EITHER
TO
BLEND
TWO
DIFFERENTLY
EXPOSED
IMAGE
TO
AVOID
UNDER
AND
OVER
EXPOSED
REGIONS
OR
TO
MAKE
A
CREATIVE
BLEND
OF
TWO
DIFFERENT
SCENES
EX
WAVELET
CONSTRUCTION
AND
APPLICATIONS
IMPLEMENT
ONE
OF
THE
WAVELET
FAMILIES
DESCRIBED
IN
SECTION
OR
BY
SIMONCELLI
AND
ADELSON
AS
WELL
AS
THE
BASIC
LAPLA
CIAN
PYRAMID
EXERCISE
APPLY
THE
RESULTING
REPRESENTATIONS
TO
ONE
OF
THE
FOLLOWING
TWO
TASKS
COMPRESSION
COMPUTE
THE
ENTROPY
IN
EACH
BAND
FOR
THE
DIFFERENT
WAVELET
IMPLEMEN
TATIONS
ASSUMING
A
GIVEN
QUANTIZATION
LEVEL
SAY
GRAY
LEVEL
TO
KEEP
THE
ROUNDING
ERROR
ACCEPTABLE
QUANTIZE
THE
WAVELET
COEFFICIENTS
AND
RECONSTRUCT
THE
ORIGINAL
IM
AGES
WHICH
TECHNIQUE
PERFORMS
BETTER
SEE
SIMONCELLI
AND
ADELSON
OR
ANY
OF
THE
MULTITUDE
OF
WAVELET
COMPRESSION
PAPERS
FOR
SOME
TYPICAL
RESULTS
DENOISING
AFTER
COMPUTING
THE
WAVELETS
SUPPRESS
SMALL
VALUES
USING
CORING
I
E
SET
SMALL
VALUES
TO
ZERO
USING
A
PIECEWISE
LINEAR
OR
OTHER
FUNCTION
COMPARE
THE
RESULTS
OF
YOUR
DENOISING
USING
DIFFERENT
WAVELET
AND
PYRAMID
REPRESENTATIONS
EX
PARAMETRIC
IMAGE
WARPING
WRITE
THE
CODE
TO
DO
AFFINE
AND
PERSPECTIVE
IMAGE
WARPS
OPTIONALLY
BILINEAR
AS
WELL
TRY
A
VARIETY
OF
INTERPOLANTS
AND
REPORT
ON
THEIR
VISUAL
QUALITY
IN
PARTICULAR
DISCUSS
THE
FOLLOWING
IN
A
MIP
MAP
SELECTING
ONLY
THE
COARSER
LEVEL
ADJACENT
TO
THE
COMPUTED
FRACTIONAL
LEVEL
WILL
PRODUCE
A
BLURRIER
IMAGE
WHILE
SELECTING
THE
FINER
LEVEL
WILL
LEAD
TO
ALIASING
EXPLAIN
WHY
THIS
IS
SO
AND
DISCUSS
WHETHER
BLENDING
AN
ALIASED
AND
A
BLURRED
IMAGE
TRI
LINEAR
MIP
MAPPING
IS
A
GOOD
IDEA
WHEN
THE
RATIO
OF
THE
HORIZONTAL
AND
VERTICAL
RESAMPLING
RATES
BECOMES
VERY
DIFFERENT
ANISOTROPIC
THE
MIP
MAP
PERFORMS
EVEN
WORSE
SUGGEST
SOME
APPROACHES
TO
REDUCE
SUCH
PROBLEMS
EX
LOCAL
IMAGE
WARPING
OPEN
AN
IMAGE
AND
DEFORM
ITS
APPEARANCE
IN
ONE
OF
THE
FOLLOWING
WAYS
CLICK
ON
A
NUMBER
OF
PIXELS
AND
MOVE
DRAG
THEM
TO
NEW
LOCATIONS
INTERPOLATE
THE
RESULTING
SPARSE
DISPLACEMENT
FIELD
TO
OBTAIN
A
DENSE
MOTION
FIELD
SECTIONS
AND
DRAW
A
NUMBER
OF
LINES
IN
THE
IMAGE
MOVE
THE
ENDPOINTS
OF
THE
LINES
TO
SPECIFY
THEIR
NEW
POSITIONS
AND
USE
THE
BEIER
NEELY
INTERPOLATION
ALGORITHM
BEIER
AND
NEELY
DISCUSSED
IN
SECTION
TO
GET
A
DENSE
MOTION
FIELD
OVERLAY
A
SPLINE
CONTROL
GRID
AND
MOVE
ONE
GRID
POINT
AT
A
TIME
OPTIONALLY
SELECT
THE
LEVEL
OF
THE
DEFORMATION
HAVE
A
DENSE
PER
PIXEL
FLOW
FIELD
AND
USE
A
SOFT
PAINTBRUSH
TO
DESIGN
A
HORIZONTAL
AND
VERTICAL
VELOCITY
FIELD
OPTIONAL
PROVE
WHETHER
THE
BEIER
NEELY
WARP
DOES
OR
DOES
NOT
REDUCE
TO
A
SPARSE
POINT
BASED
DEFORMATION
AS
THE
LINE
SEGMENTS
BECOME
SHORTER
REDUCE
TO
POINTS
EX
FORWARD
WARPING
GIVEN
A
DISPLACEMENT
FIELD
FROM
THE
PREVIOUS
EXERCISE
WRITE
A
FORWARD
WARPING
ALGORITHM
WRITE
A
FORWARD
WARPER
USING
SPLATTING
EITHER
NEAREST
NEIGHBOR
OR
SOFT
ACCUMULATION
SECTION
WRITE
A
TWO
PASS
ALGORITHM
WHICH
FORWARD
WARPS
THE
DISPLACEMENT
FIELD
FILLS
IN
SMALL
HOLES
AND
THEN
USES
INVERSE
WARPING
SHADE
GORTLER
HE
ET
AL
COMPARE
THE
QUALITY
OF
THESE
TWO
ALGORITHMS
EX
FEATURE
BASED
MORPHING
EXTEND
THE
WARPING
CODE
YOU
WROTE
IN
EXERCISE
TO
IMPORT
TWO
DIFFERENT
IMAGES
AND
SPECIFY
CORRESPONDENCES
POINT
LINE
OR
MESH
BASED
BE
TWEEN
THE
TWO
IMAGES
CREATE
A
MORPH
BY
PARTIALLY
WARPING
THE
IMAGES
TOWARDS
EACH
OTHER
AND
CROSS
DISSOLVING
SECTION
TRY
USING
YOUR
MORPHING
ALGORITHM
TO
PERFORM
AN
IMAGE
ROTATION
AND
DISCUSS
WHETHER
IT
BEHAVES
THE
WAY
YOU
WANT
IT
TO
EX
IMAGE
EDITOR
EXTEND
THE
PROGRAM
YOU
WROTE
IN
EXERCISE
TO
IMPORT
IMAGES
AND
LET
YOU
CREATE
A
COLLAGE
OF
PICTURES
YOU
SHOULD
IMPLEMENT
THE
FOLLOWING
STEPS
OPEN
UP
A
NEW
IMAGE
IN
A
SEPARATE
WINDOW
FIGURE
THERE
IS
A
FAINT
IMAGE
OF
A
RAINBOW
VISIBLE
IN
THE
RIGHT
HAND
SIDE
OF
THIS
PICTURE
CAN
YOU
THINK
OF
A
WAY
TO
ENHANCE
IT
EXERCISE
SHIFT
DRAG
RUBBER
BAND
TO
CROP
A
SUBREGION
OR
SELECT
WHOLE
IMAGE
PASTE
INTO
THE
CURRENT
CANVAS
SELECT
THE
DEFORMATION
MODE
MOTION
MODEL
TRANSLATION
RIGID
SIMILARITY
AFFINE
OR
PERSPECTIVE
DRAG
ANY
CORNER
OF
THE
OUTLINE
TO
CHANGE
ITS
TRANSFORMATION
OPTIONAL
CHANGE
THE
RELATIVE
ORDERING
OF
THE
IMAGES
AND
WHICH
IMAGE
IS
CURRENTLY
BEING
MANIPULATED
THE
USER
SHOULD
SEE
THE
COMPOSITION
OF
THE
VARIOUS
IMAGES
PIECES
ON
TOP
OF
EACH
OTHER
THIS
EXERCISE
SHOULD
BE
BUILT
ON
THE
IMAGE
TRANSFORMATION
CLASSES
SUPPORTED
IN
THE
SOFT
WARE
LIBRARY
PERSISTENCE
OF
THE
CREATED
REPRESENTATION
SAVE
AND
LOAD
SHOULD
ALSO
BE
SUP
PORTED
FOR
EACH
IMAGE
SAVE
ITS
TRANSFORMATION
EX
TEXTURE
MAPPED
VIEWER
EXTEND
THE
VIEWER
YOU
CREATED
IN
EXERCISE
TO
IN
CLUDE
TEXTURE
MAPPED
POLYGON
RENDERING
AUGMENT
EACH
POLYGON
WITH
U
V
W
COORDINATES
INTO
AN
IMAGE
EX
IMAGE
DENOISING
IMPLEMENT
AT
LEAST
TWO
OF
THE
VARIOUS
IMAGE
DENOISING
TECH
NIQUES
DESCRIBED
IN
THIS
CHAPTER
AND
COMPARE
THEM
ON
BOTH
SYNTHETICALLY
NOISED
IMAGE
SE
QUENCES
AND
REAL
WORLD
LOW
LIGHT
SEQUENCES
DOES
THE
PERFORMANCE
OF
THE
ALGORITHM
DE
PEND
ON
THE
CORRECT
CHOICE
OF
NOISE
LEVEL
ESTIMATE
CAN
YOU
DRAW
ANY
CONCLUSIONS
AS
TO
WHICH
TECHNIQUES
WORK
BETTER
EX
RAINBOW
ENHANCER
CHALLENGING
TAKE
A
PICTURE
CONTAINING
A
RAINBOW
SUCH
AS
FIGURE
AND
ENHANCE
THE
STRENGTH
SATURATION
OF
THE
RAINBOW
DRAW
AN
ARC
IN
THE
IMAGE
DELINEATING
THE
EXTENT
OF
THE
RAINBOW
FIT
AN
ADDITIVE
RAINBOW
FUNCTION
EXPLAIN
WHY
IT
IS
ADDITIVE
TO
THIS
ARC
IT
IS
BEST
TO
WORK
WITH
LINEARIZED
PIXEL
VALUES
USING
THE
SPECTRUM
AS
THE
CROSS
SECTION
AND
ESTIMATING
THE
WIDTH
OF
THE
ARC
AND
THE
AMOUNT
OF
COLOR
BEING
ADDED
THIS
IS
THE
TRICKIEST
PART
OF
THE
PROBLEM
AS
YOU
NEED
TO
TEASE
APART
THE
LOW
FREQUENCY
RAINBOW
PATTERN
AND
THE
NATURAL
IMAGE
HIDING
BEHIND
IT
AMPLIFY
THE
RAINBOW
SIGNAL
AND
ADD
IT
BACK
INTO
THE
IMAGE
RE
APPLYING
THE
GAMMA
FUNCTION
IF
NECESSARY
TO
PRODUCE
THE
FINAL
IMAGE
EX
IMAGE
DEBLOCKING
CHALLENGING
NOW
THAT
YOU
HAVE
SOME
GOOD
TECHNIQUES
TO
DISTINGUISH
SIGNAL
FROM
NOISE
DEVELOP
A
TECHNIQUE
TO
REMOVE
THE
BLOCKING
ARTIFACTS
THAT
OCCUR
WITH
JPEG
AT
HIGH
COMPRESSION
SETTINGS
SECTION
YOUR
TECHNIQUE
CAN
BE
AS
SIMPLE
AS
LOOKING
FOR
UNEXPECTED
EDGES
ALONG
BLOCK
BOUNDARIES
TO
LOOKING
AT
THE
QUANTIZATION
STEP
AS
A
PROJECTION
OF
A
CONVEX
REGION
OF
THE
TRANSFORM
COEFFICIENT
SPACE
ONTO
THE
CORRESPONDING
QUANTIZED
VALUES
DOES
THE
KNOWLEDGE
OF
THE
COMPRESSION
FACTOR
WHICH
IS
AVAILABLE
IN
THE
JPEG
HEADER
INFORMATION
HELP
YOU
PERFORM
BETTER
DEBLOCKING
BECAUSE
THE
QUANTIZATION
OCCURS
IN
THE
DCT
TRANSFORMED
YCBCR
SPACE
IT
MAY
BE
PREFERABLE
TO
PERFORM
THE
ANALYSIS
IN
THIS
SPACE
ON
THE
OTHER
HAND
IMAGE
PRIORS
MAKE
MORE
SENSE
IN
AN
RGB
SPACE
OR
DO
THEY
DECIDE
HOW
YOU
WILL
APPROACH
THIS
DICHOTOMY
AND
DISCUSS
YOUR
CHOICE
WHILE
YOU
ARE
AT
IT
SINCE
THE
YCBCR
CONVERSION
IS
FOLLOWED
BY
A
CHROMINANCE
SUBSAM
PLING
STAGE
BEFORE
THE
DCT
SEE
IF
YOU
CAN
RESTORE
SOME
OF
THE
LOST
HIGH
FREQUENCY
CHROMINANCE
SIGNAL
USING
ONE
OF
THE
BETTER
RESTORATION
TECHNIQUES
DISCUSSED
IN
THIS
CHAPTER
IF
YOUR
CAMERA
HAS
A
RAW
JPEG
MODE
HOW
CLOSE
CAN
YOU
COME
TO
THE
NOISE
FREE
TRUE
PIXEL
VALUES
THIS
SUGGESTION
MAY
NOT
BE
THAT
USEFUL
SINCE
CAMERAS
GENERALLY
USE
REASONABLY
HIGH
QUALITY
SETTINGS
FOR
THEIR
RAW
JPEG
MODELS
EX
INFERENCE
IN
DE
BLURRING
CHALLENGING
WRITE
DOWN
THE
GRAPHICAL
MODEL
CORRE
SPONDING
TO
FIGURE
FOR
A
NON
BLIND
IMAGE
DEBLURRING
PROBLEM
I
E
ONE
WHERE
THE
BLUR
KERNEL
IS
KNOWN
AHEAD
OF
TIME
WHAT
KIND
OF
EFFICIENT
INFERENCE
OPTIMIZATION
ALGORITHMS
CAN
YOU
THINK
OF
FOR
SOLVING
SUCH
PROBLEMS
CHAPTER
FEATURE
DETECTION
AND
MATCHING
POINTS
AND
PATCHES
FEATURE
DETECTORS
FEATURE
DESCRIPTORS
FEATURE
MATCHING
FEATURE
TRACKING
APPLICATION
PERFORMANCE
DRIVEN
ANIMATION
EDGES
EDGE
DETECTION
EDGE
LINKING
APPLICATION
EDGE
EDITING
AND
ENHANCEMENT
LINES
SUCCESSIVE
APPROXIMATION
HOUGH
TRANSFORMS
VANISHING
POINTS
APPLICATION
RECTANGLE
DETECTION
ADDITIONAL
READING
EXERCISES
A
B
C
D
FIGURE
A
VARIETY
OF
FEATURE
DETECTORS
AND
DESCRIPTORS
CAN
BE
USED
TO
ANALYZE
DESCRIBE
AND
MATCH
IMAGES
A
POINT
LIKE
INTEREST
OPERATORS
BROWN
SZELISKI
AND
WINDER
QC
IEEE
B
REGION
LIKE
INTEREST
OPERATORS
MATAS
CHUM
URBAN
ET
AL
QC
ELSEVIER
C
EDGES
ELDER
AND
GOLDBERG
QC
IEEE
D
STRAIGHT
LINES
SINHA
STEEDLY
SZELISKI
ET
AL
QC
ACM
FEATURE
DETECTION
AND
MATCHING
ARE
AN
ESSENTIAL
COMPONENT
OF
MANY
COMPUTER
VISION
APPLI
CATIONS
CONSIDER
THE
TWO
PAIRS
OF
IMAGES
SHOWN
IN
FIGURE
FOR
THE
FIRST
PAIR
WE
MAY
WISH
TO
ALIGN
THE
TWO
IMAGES
SO
THAT
THEY
CAN
BE
SEAMLESSLY
STITCHED
INTO
A
COMPOSITE
MOSAIC
CHAPTER
FOR
THE
SECOND
PAIR
WE
MAY
WISH
TO
ESTABLISH
A
DENSE
SET
OF
CORRESPONDENCES
SO
THAT
A
MODEL
CAN
BE
CONSTRUCTED
OR
AN
IN
BETWEEN
VIEW
CAN
BE
GENERATED
CHAPTER
IN
EITHER
CASE
WHAT
KINDS
OF
FEATURES
SHOULD
YOU
DETECT
AND
THEN
MATCH
IN
ORDER
TO
ESTABLISH
SUCH
AN
ALIGNMENT
OR
SET
OF
CORRESPONDENCES
THINK
ABOUT
THIS
FOR
A
FEW
MOMENTS
BEFORE
READING
ON
THE
FIRST
KIND
OF
FEATURE
THAT
YOU
MAY
NOTICE
ARE
SPECIFIC
LOCATIONS
IN
THE
IMAGES
SUCH
AS
MOUNTAIN
PEAKS
BUILDING
CORNERS
DOORWAYS
OR
INTERESTINGLY
SHAPED
PATCHES
OF
SNOW
THESE
KINDS
OF
LOCALIZED
FEATURE
ARE
OFTEN
CALLED
KEYPOINT
FEATURES
OR
INTEREST
POINTS
OR
EVEN
CORNERS
AND
ARE
OFTEN
DESCRIBED
BY
THE
APPEARANCE
OF
PATCHES
OF
PIXELS
SURROUNDING
THE
POINT
LOCATION
SECTION
ANOTHER
CLASS
OF
IMPORTANT
FEATURES
ARE
EDGES
E
G
THE
PROFILE
OF
MOUNTAINS
AGAINST
THE
SKY
SECTION
THESE
KINDS
OF
FEATURES
CAN
BE
MATCHED
BASED
ON
THEIR
ORIEN
TATION
AND
LOCAL
APPEARANCE
EDGE
PROFILES
AND
CAN
ALSO
BE
GOOD
INDICATORS
OF
OBJECT
BOUND
ARIES
AND
OCCLUSION
EVENTS
IN
IMAGE
SEQUENCES
EDGES
CAN
BE
GROUPED
INTO
LONGER
CURVES
AND
STRAIGHT
LINE
SEGMENTS
WHICH
CAN
BE
DIRECTLY
MATCHED
OR
ANALYZED
TO
FIND
VANISHING
POINTS
AND
HENCE
INTERNAL
AND
EXTERNAL
CAMERA
PARAMETERS
SECTION
IN
THIS
CHAPTER
WE
DESCRIBE
SOME
PRACTICAL
APPROACHES
TO
DETECTING
SUCH
FEATURES
AND
ALSO
DISCUSS
HOW
FEATURE
CORRESPONDENCES
CAN
BE
ESTABLISHED
ACROSS
DIFFERENT
IMAGES
POINT
FEATURES
ARE
NOW
USED
IN
SUCH
A
WIDE
VARIETY
OF
APPLICATIONS
THAT
IT
IS
GOOD
PRACTICE
TO
READ
AND
IMPLEMENT
SOME
OF
THE
ALGORITHMS
FROM
SECTION
EDGES
AND
LINES
PROVIDE
INFORMATION
THAT
IS
COMPLEMENTARY
TO
BOTH
KEYPOINT
AND
REGION
BASED
DESCRIPTORS
AND
ARE
WELL
SUITED
TO
DESCRIBING
OBJECT
BOUNDARIES
AND
MAN
MADE
OBJECTS
THESE
ALTERNATIVE
DESCRIPTORS
WHILE
EXTREMELY
USEFUL
CAN
BE
SKIPPED
IN
A
SHORT
INTRODUCTORY
COURSE
POINTS
AND
PATCHES
POINT
FEATURES
CAN
BE
USED
TO
FIND
A
SPARSE
SET
OF
CORRESPONDING
LOCATIONS
IN
DIFFERENT
IM
AGES
OFTEN
AS
A
PRE
CURSOR
TO
COMPUTING
CAMERA
POSE
CHAPTER
WHICH
IS
A
PREREQUISITE
FOR
COMPUTING
A
DENSER
SET
OF
CORRESPONDENCES
USING
STEREO
MATCHING
CHAPTER
SUCH
CORRE
SPONDENCES
CAN
ALSO
BE
USED
TO
ALIGN
DIFFERENT
IMAGES
E
G
WHEN
STITCHING
IMAGE
MOSAICS
OR
PERFORMING
VIDEO
STABILIZATION
CHAPTER
THEY
ARE
ALSO
USED
EXTENSIVELY
TO
PERFORM
OBJECT
INSTANCE
AND
CATEGORY
RECOGNITION
SECTIONS
AND
A
KEY
ADVANTAGE
OF
KEYPOINTS
IS
THAT
THEY
PERMIT
MATCHING
EVEN
IN
THE
PRESENCE
OF
CLUTTER
OCCLUSION
AND
LARGE
SCALE
AND
ORIENTATION
CHANGES
FEATURE
BASED
CORRESPONDENCE
TECHNIQUES
HAVE
BEEN
USED
SINCE
THE
EARLY
DAYS
OF
STEREO
FIGURE
TWO
PAIRS
OF
IMAGES
TO
BE
MATCHED
WHAT
KINDS
OF
FEATURE
MIGHT
ONE
USE
TO
ESTABLISH
A
SET
OF
CORRESPONDENCES
BETWEEN
THESE
IMAGES
MATCHING
HANNAH
MORAVEC
HANNAH
AND
HAVE
MORE
RECENTLY
GAINED
POP
ULARITY
FOR
IMAGE
STITCHING
APPLICATIONS
ZOGHLAMI
FAUGERAS
AND
DERICHE
BROWN
AND
LOWE
AS
WELL
AS
FULLY
AUTOMATED
MODELING
BEARDSLEY
TORR
AND
ZISSERMAN
SCHAFFALITZKY
AND
ZISSERMAN
BROWN
AND
LOWE
SNAVELY
SEITZ
AND
SZELISKI
THERE
ARE
TWO
MAIN
APPROACHES
TO
FINDING
FEATURE
POINTS
AND
THEIR
CORRESPONDENCES
THE
FIRST
IS
TO
FIND
FEATURES
IN
ONE
IMAGE
THAT
CAN
BE
ACCURATELY
TRACKED
USING
A
LOCAL
SEARCH
TECH
NIQUE
SUCH
AS
CORRELATION
OR
LEAST
SQUARES
SECTION
THE
SECOND
IS
TO
INDEPENDENTLY
DETECT
FEATURES
IN
ALL
THE
IMAGES
UNDER
CONSIDERATION
AND
THEN
MATCH
FEATURES
BASED
ON
THEIR
LOCAL
APPEARANCE
SECTION
THE
FORMER
APPROACH
IS
MORE
SUITABLE
WHEN
IMAGES
ARE
TAKEN
FROM
NEARBY
VIEWPOINTS
OR
IN
RAPID
SUCCESSION
E
G
VIDEO
SEQUENCES
WHILE
THE
LAT
TER
IS
MORE
SUITABLE
WHEN
A
LARGE
AMOUNT
OF
MOTION
OR
APPEARANCE
CHANGE
IS
EXPECTED
E
G
IN
STITCHING
TOGETHER
PANORAMAS
BROWN
AND
LOWE
ESTABLISHING
CORRESPONDENCES
IN
WIDE
BASELINE
STEREO
SCHAFFALITZKY
AND
ZISSERMAN
OR
PERFORMING
OBJECT
RECOGNITION
FERGUS
PERONA
AND
ZISSERMAN
IN
THIS
SECTION
WE
SPLIT
THE
KEYPOINT
DETECTION
AND
MATCHING
PIPELINE
INTO
FOUR
SEPARATE
STAGES
DURING
THE
FEATURE
DETECTION
EXTRACTION
STAGE
SECTION
EACH
IMAGE
IS
SEARCHED
FOR
LOCATIONS
THAT
ARE
LIKELY
TO
MATCH
WELL
IN
OTHER
IMAGES
AT
THE
FEATURE
DESCRIPTION
STAGE
SECTION
EACH
REGION
AROUND
DETECTED
KEYPOINT
LOCATIONS
IS
CONVERTED
INTO
A
MORE
COM
PACT
AND
STABLE
INVARIANT
DESCRIPTOR
THAT
CAN
BE
MATCHED
AGAINST
OTHER
DESCRIPTORS
THE
FIGURE
IMAGE
PAIRS
WITH
EXTRACTED
PATCHES
BELOW
NOTICE
HOW
SOME
PATCHES
CAN
BE
LOCALIZED
OR
MATCHED
WITH
HIGHER
ACCURACY
THAN
OTHERS
FEATURE
MATCHING
STAGE
SECTION
EFFICIENTLY
SEARCHES
FOR
LIKELY
MATCHING
CANDIDATES
IN
OTHER
IMAGES
THE
FEATURE
TRACKING
STAGE
SECTION
IS
AN
ALTERNATIVE
TO
THE
THIRD
STAGE
THAT
ONLY
SEARCHES
A
SMALL
NEIGHBORHOOD
AROUND
EACH
DETECTED
FEATURE
AND
IS
THEREFORE
MORE
SUITABLE
FOR
VIDEO
PROCESSING
A
WONDERFUL
EXAMPLE
OF
ALL
OF
THESE
STAGES
CAN
BE
FOUND
IN
DAVID
LOWE
PAPER
WHICH
DESCRIBES
THE
DEVELOPMENT
AND
REFINEMENT
OF
HIS
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
COMPREHENSIVE
DESCRIPTIONS
OF
ALTERNATIVE
TECHNIQUES
CAN
BE
FOUND
IN
A
SERIES
OF
SURVEY
AND
EVALUATION
PAPERS
COVERING
BOTH
FEATURE
DETECTION
SCHMID
MOHR
AND
BAUCK
HAGE
MIKOLAJCZYK
TUYTELAARS
SCHMID
ET
AL
TUYTELAARS
AND
MIKOLAJCZYK
AND
FEATURE
DESCRIPTORS
MIKOLAJCZYK
AND
SCHMID
SHI
AND
TOMASI
AND
TRIGGS
ALSO
PROVIDE
NICE
REVIEWS
OF
FEATURE
DETECTION
TECHNIQUES
FEATURE
DETECTORS
HOW
CAN
WE
FIND
IMAGE
LOCATIONS
WHERE
WE
CAN
RELIABLY
FIND
CORRESPONDENCES
WITH
OTHER
IMAGES
I
E
WHAT
ARE
GOOD
FEATURES
TO
TRACK
SHI
AND
TOMASI
TRIGGS
LOOK
AGAIN
AT
THE
IMAGE
PAIR
SHOWN
IN
FIGURE
AND
AT
THE
THREE
SAMPLE
PATCHES
TO
SEE
HOW
WELL
THEY
MIGHT
BE
MATCHED
OR
TRACKED
AS
YOU
MAY
NOTICE
TEXTURELESS
PATCHES
ARE
NEARLY
IMPOSSIBLE
TO
LOCALIZE
PATCHES
WITH
LARGE
CONTRAST
CHANGES
GRADIENTS
ARE
EASIER
TO
LOCALIZE
ALTHOUGH
STRAIGHT
LINE
SEGMENTS
AT
A
SINGLE
ORIENTATION
SUFFER
FROM
THE
APERTURE
PROBLEM
HORN
AND
SCHUNCK
LUCAS
AND
KANADE
ANANDAN
I
E
IT
IS
ONLY
POSSIBLE
TO
ALIGN
THE
PATCHES
ALONG
THE
DIRECTION
NORMAL
TO
THE
EDGE
DIRECTION
FIGURE
PATCHES
WITH
A
B
C
FIGURE
APERTURE
PROBLEMS
FOR
DIFFERENT
IMAGE
PATCHES
A
STABLE
CORNER
LIKE
FLOW
B
CLASSIC
APERTURE
PROBLEM
BARBER
POLE
ILLUSION
C
TEXTURELESS
REGION
THE
TWO
IMAGES
YELLOW
AND
RED
ARE
OVERLAID
THE
RED
VECTOR
U
INDICATES
THE
DISPLACEMENT
BETWEEN
THE
PATCH
CENTERS
AND
THE
W
XI
WEIGHTING
FUNCTION
PATCH
WINDOW
IS
SHOWN
AS
A
DARK
CIRCLE
GRADIENTS
IN
AT
LEAST
TWO
SIGNIFICANTLY
DIFFERENT
ORIENTATIONS
ARE
THE
EASIEST
TO
LOCALIZE
AS
SHOWN
SCHEMATICALLY
IN
FIGURE
THESE
INTUITIONS
CAN
BE
FORMALIZED
BY
LOOKING
AT
THE
SIMPLEST
POSSIBLE
MATCHING
CRITERION
FOR
COMPARING
TWO
IMAGE
PATCHES
I
E
THEIR
WEIGHTED
SUMMED
SQUARE
DIFFERENCE
EWSSD
U
W
XI
XI
U
XI
I
WHERE
AND
ARE
THE
TWO
IMAGES
BEING
COMPARED
U
U
V
IS
THE
DISPLACEMENT
VECTOR
W
X
IS
A
SPATIALLY
VARYING
WEIGHTING
OR
WINDOW
FUNCTION
AND
THE
SUMMATION
I
IS
OVER
ALL
THE
PIXELS
IN
THE
PATCH
NOTE
THAT
THIS
IS
THE
SAME
FORMULATION
WE
LATER
USE
TO
ESTIMATE
MOTION
BETWEEN
COMPLETE
IMAGES
SECTION
WHEN
PERFORMING
FEATURE
DETECTION
WE
DO
NOT
KNOW
WHICH
OTHER
IMAGE
LOCATIONS
THE
FEATURE
WILL
END
UP
BEING
MATCHED
AGAINST
THEREFORE
WE
CAN
ONLY
COMPUTE
HOW
STABLE
THIS
METRIC
IS
WITH
RESPECT
TO
SMALL
VARIATIONS
IN
POSITION
U
BY
COMPARING
AN
IMAGE
PATCH
AGAINST
ITSELF
WHICH
IS
KNOWN
AS
AN
AUTO
CORRELATION
FUNCTION
OR
SURFACE
EAC
U
W
XI
XI
U
XI
I
FIGURE
NOTE
HOW
THE
AUTO
CORRELATION
SURFACE
FOR
THE
TEXTURED
FLOWER
BED
FIGURE
AND
THE
RED
CROSS
IN
THE
LOWER
RIGHT
QUADRANT
OF
FIGURE
EXHIBITS
A
STRONG
MINIMUM
INDICATING
THAT
IT
CAN
BE
WELL
LOCALIZED
THE
CORRELATION
SURFACE
CORRESPONDING
TO
THE
ROOF
EDGE
FIGURE
HAS
A
STRONG
AMBIGUITY
ALONG
ONE
DIRECTION
WHILE
THE
CORRELATION
SURFACE
CORRESPONDING
TO
THE
CLOUD
REGION
FIGURE
HAS
NO
STABLE
MINIMUM
STRICTLY
SPEAKING
A
CORRELATION
IS
THE
PRODUCT
OF
TWO
PATCHES
I
M
USING
THE
TERM
HERE
IN
A
MORE
QUALITATIVE
SENSE
THE
WEIGHTED
SUM
OF
SQUARED
DIFFERENCES
IS
OFTEN
CALLED
AN
SSD
SURFACE
SECTION
A
B
C
D
FIGURE
THREE
AUTO
CORRELATION
SURFACES
EAC
U
SHOWN
AS
BOTH
GRAYSCALE
IMAGES
AND
SURFACE
PLOTS
A
THE
ORIGINAL
IMAGE
IS
MARKED
WITH
THREE
RED
CROSSES
TO
DENOTE
WHERE
THE
AUTO
CORRELATION
SURFACES
WERE
COMPUTED
B
THIS
PATCH
IS
FROM
THE
FLOWER
BED
GOOD
UNIQUE
MINIMUM
C
THIS
PATCH
IS
FROM
THE
ROOF
EDGE
ONE
DIMENSIONAL
APERTURE
PROBLEM
AND
D
THIS
PATCH
IS
FROM
THE
CLOUD
NO
GOOD
PEAK
EACH
GRID
POINT
IN
FIGURES
B
D
IS
ONE
VALUE
OF
U
USING
A
TAYLOR
SERIES
EXPANSION
OF
THE
IMAGE
FUNCTION
XI
U
XI
XI
U
LUCAS
AND
KANADE
SHI
AND
TOMASI
WE
CAN
APPROXIMATE
THE
AUTO
CORRELATION
SURFACE
AS
WHERE
EAC
U
W
XI
XI
U
XI
I
W
XI
XI
XI
U
XI
I
W
XI
XI
U
I
UT
A
U
XI
X
Y
XI
IS
THE
IMAGE
GRADIENT
AT
XI
THIS
GRADIENT
CAN
BE
COMPUTED
USING
A
VARIETY
OF
TECHNIQUES
SCHMID
MOHR
AND
BAUCKHAGE
THE
CLASSIC
HARRIS
DETECTOR
HARRIS
AND
STEPHENS
USES
A
FILTER
BUT
MORE
MODERN
VARIANTS
SCHMID
MOHR
AND
BAUCKHAGE
TRIGGS
CONVOLVE
THE
IMAGE
WITH
HORIZONTAL
AND
VERTICAL
DERIVATIVES
OF
A
GAUSSIAN
TYPICALLY
WITH
Σ
THE
AUTO
CORRELATION
MATRIX
A
CAN
BE
WRITTEN
AS
A
W
R
IXIY
L
IXIY
WHERE
WE
HAVE
REPLACED
THE
WEIGHTED
SUMMATIONS
WITH
DISCRETE
CONVOLUTIONS
WITH
THE
WEIGHT
ING
KERNEL
W
THIS
MATRIX
CAN
BE
INTERPRETED
AS
A
TENSOR
MULTIBAND
IMAGE
WHERE
THE
OUTER
PRODUCTS
OF
THE
GRADIENTS
I
ARE
CONVOLVED
WITH
A
WEIGHTING
FUNCTION
W
TO
PROVIDE
A
PER
PIXEL
ESTIMATE
OF
THE
LOCAL
QUADRATIC
SHAPE
OF
THE
AUTO
CORRELATION
FUNCTION
AS
FIRST
SHOWN
BY
ANANDAN
AND
FURTHER
DISCUSSED
IN
SECTION
AND
THE
INVERSE
OF
THE
MATRIX
A
PROVIDES
A
LOWER
BOUND
ON
THE
UNCERTAINTY
IN
THE
LOCATION
OF
A
MATCHING
PATCH
IT
IS
THEREFORE
A
USEFUL
INDICATOR
OF
WHICH
PATCHES
CAN
BE
RELIABLY
MATCHED
THE
EASIEST
WAY
TO
VISUALIZE
AND
REASON
ABOUT
THIS
UNCERTAINTY
IS
TO
PERFORM
AN
EIGENVALUE
ANALYSIS
OF
THE
AUTO
CORRELATION
MATRIX
A
WHICH
PRODUCES
TWO
EIGENVALUES
AND
TWO
EIGENVECTOR
DIRECTIONS
FIGURE
SINCE
THE
LARGER
UNCERTAINTY
DEPENDS
ON
THE
SMALLER
EIGEN
VALUE
I
E
Λ
IT
MAKES
SENSE
TO
FIND
MAXIMA
IN
THE
SMALLER
EIGENVALUE
TO
LOCATE
GOOD
FEATURES
TO
TRACK
SHI
AND
TOMASI
FO
RSTNER
HARRIS
WHILE
ANANDAN
AND
LUCAS
AND
KANADE
WERE
THE
FIRST
TO
ANALYZE
THE
UNCERTAINTY
STRUCTURE
OF
THE
AUTO
CORRELATION
MATRIX
THEY
DID
SO
IN
THE
CONTEXT
OF
ASSO
CIATING
CERTAINTIES
WITH
OPTIC
FLOW
MEASUREMENTS
FO
RSTNER
AND
HARRIS
AND
STEPHENS
FIGURE
UNCERTAINTY
ELLIPSE
CORRESPONDING
TO
AN
EIGENVALUE
ANALYSIS
OF
THE
AUTO
CORRELATION
MATRIX
A
WERE
THE
FIRST
TO
PROPOSE
USING
LOCAL
MAXIMA
IN
ROTATIONALLY
INVARIANT
SCALAR
MEASURES
DERIVED
FROM
THE
AUTO
CORRELATION
MATRIX
TO
LOCATE
KEYPOINTS
FOR
THE
PURPOSE
OF
SPARSE
FEATURE
MATCHING
SCHMID
MOHR
AND
BAUCKHAGE
TRIGGS
GIVE
MORE
DETAILED
HISTORI
CAL
REVIEWS
OF
FEATURE
DETECTION
ALGORITHMS
BOTH
OF
THESE
TECHNIQUES
ALSO
PROPOSED
USING
A
GAUSSIAN
WEIGHTING
WINDOW
INSTEAD
OF
THE
PREVIOUSLY
USED
SQUARE
PATCHES
WHICH
MAKES
THE
DETECTOR
RESPONSE
INSENSITIVE
TO
IN
PLANE
IMAGE
ROTATIONS
THE
MINIMUM
EIGENVALUE
SHI
AND
TOMASI
IS
NOT
THE
ONLY
QUANTITY
THAT
CAN
BE
USED
TO
FIND
KEYPOINTS
A
SIMPLER
QUANTITY
PROPOSED
BY
HARRIS
AND
STEPHENS
IS
DET
A
Α
TRACE
A
Α
WITH
Α
UNLIKE
EIGENVALUE
ANALYSIS
THIS
QUANTITY
DOES
NOT
REQUIRE
THE
USE
OF
SQUARE
ROOTS
AND
YET
IS
STILL
ROTATIONALLY
INVARIANT
AND
ALSO
DOWNWEIGHTS
EDGE
LIKE
FEATURES
WHERE
TRIGGS
SUGGESTS
USING
THE
QUANTITY
SAY
WITH
Α
WHICH
ALSO
REDUCES
THE
RESPONSE
AT
EDGES
WHERE
ALIASING
ERRORS
SOMETIMES
INFLATE
THE
SMALLER
EIGENVALUE
HE
ALSO
SHOWS
HOW
THE
BASIC
HESSIAN
CAN
BE
EXTENDED
TO
PARAMETRIC
MOTIONS
TO
DETECT
POINTS
THAT
ARE
ALSO
ACCURATELY
LOCALIZABLE
IN
SCALE
AND
ROTATION
BROWN
SZELISKI
AND
WINDER
ON
THE
OTHER
HAND
USE
THE
HARMONIC
MEAN
DET
A
TR
A
WHICH
IS
A
SMOOTHER
FUNCTION
IN
THE
REGION
WHERE
FIGURE
SHOWS
ISOCONTOURS
OF
THE
VARIOUS
INTEREST
POINT
OPERATORS
FROM
WHICH
WE
CAN
SEE
HOW
THE
TWO
EIGENVALUES
ARE
BLENDED
TO
DETERMINE
THE
FINAL
INTEREST
VALUE
FIGURE
ISOCONTOURS
OF
POPULAR
KEYPOINT
DETECTION
FUNCTIONS
BROWN
SZELISKI
AND
WINDER
EACH
DETECTOR
LOOKS
FOR
POINTS
WHERE
THE
EIGENVALUES
OF
A
W
I
IT
ARE
BOTH
LARGE
ALGORITHM
OUTLINE
OF
A
BASIC
FEATURE
DETECTION
ALGORITHM
A
B
C
FIGURE
INTEREST
OPERATOR
RESPONSES
A
SAMPLE
IMAGE
B
HARRIS
RESPONSE
AND
C
DOG
RESPONSE
THE
CIRCLE
SIZES
AND
COLORS
INDICATE
THE
SCALE
AT
WHICH
EACH
INTEREST
POINT
WAS
DETECTED
NOTICE
HOW
THE
TWO
DETECTORS
TEND
TO
RESPOND
AT
COMPLEMENTARY
LOCATIONS
THE
STEPS
IN
THE
BASIC
AUTO
CORRELATION
BASED
KEYPOINT
DETECTOR
ARE
SUMMARIZED
IN
ALGO
RITHM
FIGURE
SHOWS
THE
RESULTING
INTEREST
OPERATOR
RESPONSES
FOR
THE
CLASSIC
HARRIS
DETECTOR
AS
WELL
AS
THE
DIFFERENCE
OF
GAUSSIAN
DOG
DETECTOR
DISCUSSED
BELOW
ADAPTIVE
NON
MAXIMAL
SUPPRESSION
ANMS
WHILE
MOST
FEATURE
DETECTORS
SIMPLY
LOOK
FOR
LOCAL
MAXIMA
IN
THE
INTEREST
FUNCTION
THIS
CAN
LEAD
TO
AN
UNEVEN
DISTRIBUTION
OF
FEATURE
POINTS
ACROSS
THE
IMAGE
E
G
POINTS
WILL
BE
DENSER
IN
REGIONS
OF
HIGHER
CONTRAST
TO
MITIGATE
THIS
PROBLEM
BROWN
SZELISKI
AND
WINDER
ONLY
DETECT
FEATURES
THAT
ARE
BOTH
LOCAL
MAXIMA
AND
WHOSE
RESPONSE
VALUE
IS
SIGNIFICANTLY
GREATER
THAN
THAT
OF
ALL
OF
ITS
NEIGH
BORS
WITHIN
A
RADIUS
R
FIGURE
D
THEY
DEVISE
AN
EFFICIENT
WAY
TO
ASSOCIATE
SUPPRESSION
RADII
WITH
ALL
LOCAL
MAXIMA
BY
FIRST
SORTING
THEM
BY
THEIR
RESPONSE
STRENGTH
AND
THEN
CREATING
A
SECOND
LIST
SORTED
BY
DECREASING
SUPPRESSION
RADIUS
BROWN
SZELISKI
AND
WINDER
FIGURE
SHOWS
A
QUALITATIVE
COMPARISON
OF
SELECTING
THE
TOP
N
FEATURES
AND
USING
ANMS
MEASURING
REPEATABILITY
GIVEN
THE
LARGE
NUMBER
OF
FEATURE
DETECTORS
THAT
HAVE
BEEN
DE
VELOPED
IN
COMPUTER
VISION
HOW
CAN
WE
DECIDE
WHICH
ONES
TO
USE
SCHMID
MOHR
AND
BAUCKHAGE
WERE
THE
FIRST
TO
PROPOSE
MEASURING
THE
REPEATABILITY
OF
FEATURE
DETECTORS
WHICH
THEY
DEFINE
AS
THE
FREQUENCY
WITH
WHICH
KEYPOINTS
DETECTED
IN
ONE
IMAGE
ARE
FOUND
WITHIN
E
SAY
E
PIXELS
OF
THE
CORRESPONDING
LOCATION
IN
A
TRANSFORMED
IMAGE
IN
THEIR
PAPER
THEY
TRANSFORM
THEIR
PLANAR
IMAGES
BY
APPLYING
ROTATIONS
SCALE
CHANGES
ILLUMINATION
CHANGES
VIEWPOINT
CHANGES
AND
ADDING
NOISE
THEY
ALSO
MEASURE
THE
INFORMATION
CONTENT
AVAILABLE
AT
EACH
DETECTED
FEATURE
POINT
WHICH
THEY
DEFINE
AS
THE
ENTROPY
OF
A
SET
OF
ROTATION
ALLY
INVARIANT
LOCAL
GRAYSCALE
DESCRIPTORS
AMONG
THE
TECHNIQUES
THEY
SURVEY
THEY
FIND
THAT
THE
IMPROVED
GAUSSIAN
DERIVATIVE
VERSION
OF
THE
HARRIS
OPERATOR
WITH
ΣD
SCALE
OF
THE
DERIVATIVE
GAUSSIAN
AND
ΣI
SCALE
OF
THE
INTEGRATION
GAUSSIAN
WORKS
BEST
A
STRONGEST
B
STRONGEST
C
ANMS
R
D
ANMS
R
FIGURE
ADAPTIVE
NON
MAXIMAL
SUPPRESSION
ANMS
BROWN
SZELISKI
AND
WINDER
QC
IEEE
THE
UPPER
TWO
IMAGES
SHOW
THE
STRONGEST
AND
INTEREST
POINTS
WHILE
THE
LOWER
TWO
IMAGES
SHOW
THE
INTEREST
POINTS
SELECTED
WITH
ADAPTIVE
NON
MAXIMAL
SUP
PRESSION
ALONG
WITH
THE
CORRESPONDING
SUPPRESSION
RADIUS
R
NOTE
HOW
THE
LATTER
FEATURES
HAVE
A
MUCH
MORE
UNIFORM
SPATIAL
DISTRIBUTION
ACROSS
THE
IMAGE
SCALE
INVARIANCE
IN
MANY
SITUATIONS
DETECTING
FEATURES
AT
THE
FINEST
STABLE
SCALE
POSSIBLE
MAY
NOT
BE
APPRO
PRIATE
FOR
EXAMPLE
WHEN
MATCHING
IMAGES
WITH
LITTLE
HIGH
FREQUENCY
DETAIL
E
G
CLOUDS
FINE
SCALE
FEATURES
MAY
NOT
EXIST
ONE
SOLUTION
TO
THE
PROBLEM
IS
TO
EXTRACT
FEATURES
AT
A
VARIETY
OF
SCALES
E
G
BY
PERFORMING
THE
SAME
OPERATIONS
AT
MULTIPLE
RESOLUTIONS
IN
A
PYRAMID
AND
THEN
MATCHING
FEATURES
AT
THE
SAME
LEVEL
THIS
KIND
OF
APPROACH
IS
SUITABLE
WHEN
THE
IMAGES
BEING
MATCHED
DO
NOT
UNDERGO
LARGE
SCALE
CHANGES
E
G
WHEN
MATCHING
SUCCESSIVE
AERIAL
IMAGES
TAKEN
FROM
AN
AIRPLANE
OR
STITCHING
PANORAMAS
TAKEN
WITH
A
FIXED
FOCAL
LENGTH
CAMERA
FIGURE
SHOWS
THE
OUTPUT
OF
ONE
SUCH
APPROACH
THE
MULTI
SCALE
ORIENTED
PATCH
DETECTOR
OF
BROWN
SZELISKI
AND
WINDER
FOR
WHICH
RESPONSES
AT
FIVE
DIFFERENT
SCALES
ARE
SHOWN
HOWEVER
FOR
MOST
OBJECT
RECOGNITION
APPLICATIONS
THE
SCALE
OF
THE
OBJECT
IN
THE
IMAGE
FIGURE
MULTI
SCALE
ORIENTED
PATCHES
MOPS
EXTRACTED
AT
FIVE
PYRAMID
LEVELS
BROWN
SZELISKI
AND
WINDER
QC
IEEE
THE
BOXES
SHOW
THE
FEATURE
ORIENTATION
AND
THE
REGION
FROM
WHICH
THE
DESCRIPTOR
VECTORS
ARE
SAMPLED
IS
UNKNOWN
INSTEAD
OF
EXTRACTING
FEATURES
AT
MANY
DIFFERENT
SCALES
AND
THEN
MATCHING
ALL
OF
THEM
IT
IS
MORE
EFFICIENT
TO
EXTRACT
FEATURES
THAT
ARE
STABLE
IN
BOTH
LOCATION
AND
SCALE
LOWE
MIKOLAJCZYK
AND
SCHMID
EARLY
INVESTIGATIONS
INTO
SCALE
SELECTION
WERE
PERFORMED
BY
LINDEBERG
WHO
FIRST
PROPOSED
USING
EXTREMA
IN
THE
LAPLACIAN
OF
GAUSSIAN
LOG
FUNCTION
AS
INTEREST
POINT
LOCATIONS
BASED
ON
THIS
WORK
LOWE
PROPOSED
COMPUTING
A
SET
OF
SUB
OCTAVE
DIFFERENCE
OF
GAUSSIAN
FILTERS
FIGURE
LOOKING
FOR
SPACE
SCALE
MAXIMA
IN
THE
RE
SULTING
STRUCTURE
FIGURE
AND
THEN
COMPUTING
A
SUB
PIXEL
SPACE
SCALE
LOCATION
USING
A
QUADRATIC
FIT
BROWN
AND
LOWE
THE
NUMBER
OF
SUB
OCTAVE
LEVELS
WAS
DETERMINED
AFTER
CAREFUL
EMPIRICAL
INVESTIGATION
TO
BE
THREE
WHICH
CORRESPONDS
TO
A
QUARTER
OCTAVE
PYRAMID
WHICH
IS
THE
SAME
AS
USED
BY
TRIGGS
AS
WITH
THE
HARRIS
OPERATOR
PIXELS
WHERE
THERE
IS
STRONG
ASYMMETRY
IN
THE
LOCAL
CURVATURE
OF
THE
INDICATOR
FUNCTION
IN
THIS
CASE
THE
DOG
ARE
REJECTED
THIS
IS
IMPLEMENTED
BY
FIRST
COMPUTING
THE
LOCAL
HESSIAN
OF
THE
DIFFERENCE
IMAGE
D
H
DXX
DXY
DXY
DYY
L
AND
THEN
REJECTING
KEYPOINTS
FOR
WHICH
TR
H
DET
H
SCALE
NEXT
OCTAVE
SCALE
FIRST
OCTAVE
GAUSSIAN
DIFFERENCE
OF
GAUSSIAN
DOG
A
B
FIGURE
SCALE
SPACE
FEATURE
DETECTION
USING
A
SUB
OCTAVE
DIFFERENCE
OF
GAUSSIAN
PYRA
MID
LOWE
QC
SPRINGER
A
ADJACENT
LEVELS
OF
A
SUB
OCTAVE
GAUSSIAN
PYRAMID
ARE
SUBTRACTED
TO
PRODUCE
DIFFERENCE
OF
GAUSSIAN
IMAGES
B
EXTREMA
MAXIMA
AND
MINIMA
IN
THE
RESULTING
VOLUME
ARE
DETECTED
BY
COMPARING
A
PIXEL
TO
ITS
NEIGHBORS
WHILE
LOWE
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
PERFORMS
WELL
IN
PRACTICE
IT
IS
NOT
BASED
ON
THE
SAME
THEORETICAL
FOUNDATION
OF
MAXIMUM
SPATIAL
STABILITY
AS
THE
AUTO
CORRELATION
BASED
DETECTORS
IN
FACT
ITS
DETECTION
LOCATIONS
ARE
OFTEN
COMPLEMENTARY
TO
THOSE
PRODUCED
BY
SUCH
TECHNIQUES
AND
CAN
THEREFORE
BE
USED
IN
CONJUNCTION
WITH
THESE
OTHER
APPROACHES
IN
ORDER
TO
ADD
A
SCALE
SELECTION
MECHANISM
TO
THE
HARRIS
CORNER
DETECTOR
MIKOLAJCZYK
AND
SCHMID
EVALUATE
THE
LAPLACIAN
OF
GAUSSIAN
FUNCTION
AT
EACH
DETECTED
HARRIS
POINT
IN
A
MULTI
SCALE
PYRAMID
AND
KEEP
ONLY
THOSE
POINTS
FOR
WHICH
THE
LAPLACIAN
IS
EXTREMAL
LARGER
OR
SMALLER
THAN
BOTH
ITS
COARSER
AND
FINER
LEVEL
VALUES
AN
OPTIONAL
ITERATIVE
REFINEMENT
FOR
BOTH
SCALE
AND
POSITION
IS
ALSO
PROPOSED
AND
EVALUATED
ADDITIONAL
EXAMPLES
OF
SCALE
INVARIANT
REGION
DETECTORS
ARE
DISCUSSED
BY
MIKOLAJCZYK
TUYTELAARS
SCHMID
ET
AL
TUYTELAARS
AND
MIKOLAJCZYK
ROTATIONAL
INVARIANCE
AND
ORIENTATION
ESTIMATION
IN
ADDITION
TO
DEALING
WITH
SCALE
CHANGES
MOST
IMAGE
MATCHING
AND
OBJECT
RECOGNITION
ALGO
RITHMS
NEED
TO
DEAL
WITH
AT
LEAST
IN
PLANE
IMAGE
ROTATION
ONE
WAY
TO
DEAL
WITH
THIS
PROBLEM
IS
TO
DESIGN
DESCRIPTORS
THAT
ARE
ROTATIONALLY
INVARIANT
SCHMID
AND
MOHR
BUT
SUCH
DESCRIPTORS
HAVE
POOR
DISCRIMINABILITY
I
E
THEY
MAP
DIFFERENT
LOOKING
PATCHES
TO
THE
SAME
DESCRIPTOR
FIGURE
A
DOMINANT
ORIENTATION
ESTIMATE
CAN
BE
COMPUTED
BY
CREATING
A
HISTOGRAM
OF
ALL
THE
GRADIENT
ORIENTATIONS
WEIGHTED
BY
THEIR
MAGNITUDES
OR
AFTER
THRESHOLDING
OUT
SMALL
GRADIENTS
AND
THEN
FINDING
THE
SIGNIFICANT
PEAKS
IN
THIS
DISTRIBUTION
LOWE
QC
SPRINGER
A
BETTER
METHOD
IS
TO
ESTIMATE
A
DOMINANT
ORIENTATION
AT
EACH
DETECTED
KEYPOINT
ONCE
THE
LOCAL
ORIENTATION
AND
SCALE
OF
A
KEYPOINT
HAVE
BEEN
ESTIMATED
A
SCALED
AND
ORIENTED
PATCH
AROUND
THE
DETECTED
POINT
CAN
BE
EXTRACTED
AND
USED
TO
FORM
A
FEATURE
DESCRIPTOR
FIGURES
AND
THE
SIMPLEST
POSSIBLE
ORIENTATION
ESTIMATE
IS
THE
AVERAGE
GRADIENT
WITHIN
A
REGION
AROUND
THE
KEYPOINT
IF
A
GAUSSIAN
WEIGHTING
FUNCTION
IS
USED
BROWN
SZELISKI
AND
WINDER
THIS
AVERAGE
GRADIENT
IS
EQUIVALENT
TO
A
FIRST
ORDER
STEERABLE
FILTER
SECTION
I
E
IT
CAN
BE
COMPUTED
USING
AN
IMAGE
CONVOLUTION
WITH
THE
HORIZONTAL
AND
VERTICAL
DERIVATIVES
OF
GAUS
SIAN
FILTER
FREEMAN
AND
ADELSON
IN
ORDER
TO
MAKE
THIS
ESTIMATE
MORE
RELIABLE
IT
IS
USUALLY
PREFERABLE
TO
USE
A
LARGER
AGGREGATION
WINDOW
GAUSSIAN
KERNEL
SIZE
THAN
DETECTION
WINDOW
BROWN
SZELISKI
AND
WINDER
THE
ORIENTATIONS
OF
THE
SQUARE
BOXES
SHOWN
IN
FIGURE
WERE
COMPUTED
USING
THIS
TECHNIQUE
SOMETIMES
HOWEVER
THE
AVERAGED
SIGNED
GRADIENT
IN
A
REGION
CAN
BE
SMALL
AND
THEREFORE
AN
UNRELIABLE
INDICATOR
OF
ORIENTATION
A
MORE
RELIABLE
TECHNIQUE
IS
TO
LOOK
AT
THE
HISTOGRAM
OF
ORIENTATIONS
COMPUTED
AROUND
THE
KEYPOINT
LOWE
COMPUTES
A
BIN
HISTOGRAM
OF
EDGE
ORIENTATIONS
WEIGHTED
BY
BOTH
GRADIENT
MAGNITUDE
AND
GAUSSIAN
DISTANCE
TO
THE
CEN
TER
FINDS
ALL
PEAKS
WITHIN
OF
THE
GLOBAL
MAXIMUM
AND
THEN
COMPUTES
A
MORE
ACCURATE
ORIENTATION
ESTIMATE
USING
A
THREE
BIN
PARABOLIC
FIT
FIGURE
AFFINE
INVARIANCE
WHILE
SCALE
AND
ROTATION
INVARIANCE
ARE
HIGHLY
DESIRABLE
FOR
MANY
APPLICATIONS
SUCH
AS
WIDE
BASELINE
STEREO
MATCHING
PRITCHETT
AND
ZISSERMAN
SCHAFFALITZKY
AND
ZISSERMAN
OR
LOCATION
RECOGNITION
CHUM
PHILBIN
SIVIC
ET
AL
FULL
AFFINE
INVARIANCE
IS
PREFERRED
FIGURE
AFFINE
REGION
DETECTORS
USED
TO
MATCH
TWO
IMAGES
TAKEN
FROM
DRAMATICALLY
DIFFERENT
VIEWPOINTS
MIKOLAJCZYK
AND
SCHMID
QC
SPRINGER
A
A
RXI
FIGURE
AFFINE
NORMALIZATION
USING
THE
SECOND
MOMENT
MATRICES
AS
DESCRIBED
BY
MIKO
LAJCZYK
TUYTELAARS
SCHMID
ET
AL
QC
SPRINGER
AFTER
IMAGE
COORDINATES
ARE
TRANS
FORMED
USING
THE
MATRICES
A
AND
A
THEY
ARE
RELATED
BY
A
PURE
ROTATION
R
WHICH
CAN
BE
ESTIMATED
USING
A
DOMINANT
ORIENTATION
TECHNIQUE
AFFINE
INVARIANT
DETECTORS
NOT
ONLY
RESPOND
AT
CONSISTENT
LOCATIONS
AFTER
SCALE
AND
ORIENTATION
CHANGES
THEY
ALSO
RESPOND
CONSISTENTLY
ACROSS
AFFINE
DEFORMATIONS
SUCH
AS
LOCAL
PERSPECTIVE
FORESHORTENING
FIGURE
IN
FACT
FOR
A
SMALL
ENOUGH
PATCH
ANY
CONTINUOUS
IMAGE
WARPING
CAN
BE
WELL
APPROXIMATED
BY
AN
AFFINE
DEFORMATION
TO
INTRODUCE
AFFINE
INVARIANCE
SEVERAL
AUTHORS
HAVE
PROPOSED
FITTING
AN
ELLIPSE
TO
THE
AUTO
CORRELATION
OR
HESSIAN
MATRIX
USING
EIGENVALUE
ANALYSIS
AND
THEN
USING
THE
PRINCIPAL
AXES
AND
RATIOS
OF
THIS
FIT
AS
THE
AFFINE
COORDINATE
FRAME
LINDEBERG
AND
GARDING
BAUMBERG
MIKOLAJCZYK
AND
SCHMID
MIKOLAJCZYK
TUYTELAARS
SCHMID
ET
AL
TUYTE
LAARS
AND
MIKOLAJCZYK
FIGURE
SHOWS
HOW
THE
SQUARE
ROOT
OF
THE
MOMENT
MATRIX
CAN
BE
USED
TO
TRANSFORM
LOCAL
PATCHES
INTO
A
FRAME
WHICH
IS
SIMILAR
UP
TO
ROTATION
ANOTHER
IMPORTANT
AFFINE
INVARIANT
REGION
DETECTOR
IS
THE
MAXIMALLY
STABLE
EXTREMAL
REGION
MSER
DETECTOR
DEVELOPED
BY
MATAS
CHUM
URBAN
ET
AL
TO
DETECT
MSERS
BINARY
REGIONS
ARE
COMPUTED
BY
THRESHOLDING
THE
IMAGE
AT
ALL
POSSIBLE
GRAY
LEVELS
THE
TECHNIQUE
THEREFORE
ONLY
WORKS
FOR
GRAYSCALE
IMAGES
THIS
OPERATION
CAN
BE
PERFORMED
EFFICIENTLY
BY
FIRST
SORTING
ALL
PIXELS
BY
GRAY
VALUE
AND
THEN
INCREMENTALLY
ADDING
PIXELS
TO
EACH
CONNECTED
COMPONENT
AS
THE
THRESHOLD
IS
CHANGED
NISTE
R
AND
STEWE
NIUS
AS
THE
THRESHOLD
IS
CHANGED
THE
AREA
OF
EACH
COMPONENT
REGION
IS
MONITORED
REGIONS
WHOSE
RATE
OF
CHANGE
OF
AREA
WITH
RESPECT
TO
THE
THRESHOLD
IS
MINIMAL
ARE
DEFINED
AS
MAXIMALLY
STABLE
SUCH
REGIONS
FIGURE
MAXIMALLY
STABLE
EXTREMAL
REGIONS
MSERS
EXTRACTED
AND
MATCHED
FROM
A
NUMBER
OF
IMAGES
MATAS
CHUM
URBAN
ET
AL
QC
ELSEVIER
FIGURE
FEATURE
MATCHING
HOW
CAN
WE
EXTRACT
LOCAL
DESCRIPTORS
THAT
ARE
INVARIANT
TO
INTER
IMAGE
VARIATIONS
AND
YET
STILL
DISCRIMINATIVE
ENOUGH
TO
ESTABLISH
CORRECT
CORRESPON
DENCES
ARE
THEREFORE
INVARIANT
TO
BOTH
AFFINE
GEOMETRIC
AND
PHOTOMETRIC
LINEAR
BIAS
GAIN
OR
SMOOTH
MONOTONIC
TRANSFORMATIONS
FIGURE
IF
DESIRED
AN
AFFINE
COORDINATE
FRAME
CAN
BE
FIT
TO
EACH
DETECTED
REGION
USING
ITS
MOMENT
MATRIX
THE
AREA
OF
FEATURE
POINT
DETECTORS
CONTINUES
TO
BE
VERY
ACTIVE
WITH
PAPERS
APPEARING
EV
ERY
YEAR
AT
MAJOR
COMPUTER
VISION
CONFERENCES
XIAO
AND
SHAH
KOETHE
CARNEIRO
AND
JEPSON
KENNEY
ZULIANI
AND
MANJUNATH
BAY
TUYTELAARS
AND
VAN
GOOL
PLATEL
BALMACHNOVA
FLORACK
ET
AL
ROSTEN
AND
DRUMMOND
MIKOLAJCZYK
TUYTE
LAARS
SCHMID
ET
AL
SURVEY
A
NUMBER
OF
POPULAR
AFFINE
REGION
DETECTORS
AND
PROVIDE
EXPERIMENTAL
COMPARISONS
OF
THEIR
INVARIANCE
TO
COMMON
IMAGE
TRANSFORMATIONS
SUCH
AS
SCAL
ING
ROTATIONS
NOISE
AND
BLUR
THESE
EXPERIMENTAL
RESULTS
CODE
AND
POINTERS
TO
THE
SURVEYED
PAPERS
CAN
BE
FOUND
ON
THEIR
WEB
SITE
AT
HTTP
WWW
ROBOTS
OX
AC
UK
VGG
RESEARCH
AFFINE
OF
COURSE
KEYPOINTS
ARE
NOT
THE
ONLY
FEATURES
THAT
CAN
BE
USED
FOR
REGISTERING
IMAGES
ZOGHLAMI
FAUGERAS
AND
DERICHE
USE
LINE
SEGMENTS
AS
WELL
AS
POINT
LIKE
FEATURES
TO
ESTIMATE
HOMOGRAPHIES
BETWEEN
PAIRS
OF
IMAGES
WHEREAS
BARTOLI
COQUERELLE
AND
STURM
USE
LINE
SEGMENTS
WITH
LOCAL
CORRESPONDENCES
ALONG
THE
EDGES
TO
EXTRACT
STRUCTURE
AND
MOTION
TUYTELAARS
AND
VAN
GOOL
USE
AFFINE
INVARIANT
REGIONS
TO
DETECT
CORRE
SPONDENCES
FOR
WIDE
BASELINE
STEREO
MATCHING
WHEREAS
KADIR
ZISSERMAN
AND
BRADY
DETECT
SALIENT
REGIONS
WHERE
PATCH
ENTROPY
AND
ITS
RATE
OF
CHANGE
WITH
SCALE
ARE
LOCALLY
MAX
IMAL
CORSO
AND
HAGER
USE
A
RELATED
TECHNIQUE
TO
FIT
ORIENTED
GAUSSIAN
KERNELS
TO
HOMOGENEOUS
REGIONS
MORE
DETAILS
ON
TECHNIQUES
FOR
FINDING
AND
MATCHING
CURVES
LINES
AND
REGIONS
CAN
BE
FOUND
LATER
IN
THIS
CHAPTER
FIGURE
MOPS
DESCRIPTORS
ARE
FORMED
USING
AN
SAMPLING
OF
BIAS
AND
GAIN
NOR
MALIZED
INTENSITY
VALUES
WITH
A
SAMPLE
SPACING
OF
FIVE
PIXELS
RELATIVE
TO
THE
DETECTION
SCALE
BROWN
SZELISKI
AND
WINDER
QC
IEEE
THIS
LOW
FREQUENCY
SAMPLING
GIVES
THE
FEATURES
SOME
ROBUSTNESS
TO
INTEREST
POINT
LOCATION
ERROR
AND
IS
ACHIEVED
BY
SAMPLING
AT
A
HIGHER
PYRAMID
LEVEL
THAN
THE
DETECTION
SCALE
FEATURE
DESCRIPTORS
AFTER
DETECTING
FEATURES
KEYPOINTS
WE
MUST
MATCH
THEM
I
E
WE
MUST
DETERMINE
WHICH
FEATURES
COME
FROM
CORRESPONDING
LOCATIONS
IN
DIFFERENT
IMAGES
IN
SOME
SITUATIONS
E
G
FOR
VIDEO
SEQUENCES
SHI
AND
TOMASI
OR
FOR
STEREO
PAIRS
THAT
HAVE
BEEN
RECTIFIED
ZHANG
DERICHE
FAUGERAS
ET
AL
LOOP
AND
ZHANG
SCHARSTEIN
AND
SZELISKI
THE
LO
CAL
MOTION
AROUND
EACH
FEATURE
POINT
MAY
BE
MOSTLY
TRANSLATIONAL
IN
THIS
CASE
SIMPLE
ERROR
METRICS
SUCH
AS
THE
SUM
OF
SQUARED
DIFFERENCES
OR
NORMALIZED
CROSS
CORRELATION
DESCRIBED
IN
SECTION
CAN
BE
USED
TO
DIRECTLY
COMPARE
THE
INTENSITIES
IN
SMALL
PATCHES
AROUND
EACH
FEATURE
POINT
THE
COMPARATIVE
STUDY
BY
MIKOLAJCZYK
AND
SCHMID
DISCUSSED
BELOW
USES
CROSS
CORRELATION
BECAUSE
FEATURE
POINTS
MAY
NOT
BE
EXACTLY
LOCATED
A
MORE
ACCURATE
MATCHING
SCORE
CAN
BE
COMPUTED
BY
PERFORMING
INCREMENTAL
MOTION
REFINEMENT
AS
DESCRIBED
IN
SECTION
BUT
THIS
CAN
BE
TIME
CONSUMING
AND
CAN
SOMETIMES
EVEN
DECREASE
PERFOR
MANCE
BROWN
SZELISKI
AND
WINDER
IN
MOST
CASES
HOWEVER
THE
LOCAL
APPEARANCE
OF
FEATURES
WILL
CHANGE
IN
ORIENTATION
AND
SCALE
AND
SOMETIMES
EVEN
UNDERGO
AFFINE
DEFORMATIONS
EXTRACTING
A
LOCAL
SCALE
ORIENTATION
OR
AFFINE
FRAME
ESTIMATE
AND
THEN
USING
THIS
TO
RESAMPLE
THE
PATCH
BEFORE
FORMING
THE
FEATURE
DESCRIPTOR
IS
THUS
USUALLY
PREFERABLE
FIGURE
EVEN
AFTER
COMPENSATING
FOR
THESE
CHANGES
THE
LOCAL
APPEARANCE
OF
IMAGE
PATCHES
WILL
USUALLY
STILL
VARY
FROM
IMAGE
TO
IMAGE
HOW
CAN
WE
MAKE
IMAGE
DESCRIPTORS
MORE
INVARIANT
TO
SUCH
CHANGES
WHILE
STILL
PRESERVING
DISCRIMINABILITY
BETWEEN
DIFFERENT
NON
CORRESPONDING
PATCHES
FIGURE
MIKOLAJCZYK
AND
SCHMID
REVIEW
SOME
RECENTLY
DEVELOPED
VIEW
INVARIANT
LOCAL
IMAGE
DESCRIPTORS
AND
EXPERIMENTALLY
COMPARE
THEIR
PERFORMANCE
BE
LOW
WE
DESCRIBE
A
FEW
OF
THESE
DESCRIPTORS
IN
MORE
DETAIL
BIAS
AND
GAIN
NORMALIZATION
MOPS
FOR
TASKS
THAT
DO
NOT
EXHIBIT
LARGE
AMOUNTS
OF
FORE
SHORTENING
SUCH
AS
IMAGE
STITCHING
SIMPLE
NORMALIZED
INTENSITY
PATCHES
PERFORM
REASONABLY
WELL
AND
ARE
SIMPLE
TO
IMPLEMENT
BROWN
SZELISKI
AND
WINDER
FIGURE
IN
OR
DER
TO
COMPENSATE
FOR
SLIGHT
INACCURACIES
IN
THE
FEATURE
POINT
DETECTOR
LOCATION
ORIENTATION
AND
SCALE
THESE
MULTI
SCALE
ORIENTED
PATCHES
MOPS
ARE
SAMPLED
AT
A
SPACING
OF
FIVE
PIXELS
RELATIVE
TO
THE
DETECTION
SCALE
USING
A
COARSER
LEVEL
OF
THE
IMAGE
PYRAMID
TO
AVOID
ALIASING
TO
COMPENSATE
FOR
AFFINE
PHOTOMETRIC
VARIATIONS
LINEAR
EXPOSURE
CHANGES
OR
BIAS
AND
GAIN
PATCH
INTENSITIES
ARE
RE
SCALED
SO
THAT
THEIR
MEAN
IS
ZERO
AND
THEIR
VARIANCE
IS
ONE
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
SIFT
FEATURES
ARE
FORMED
BY
COMPUTING
THE
GRADIENT
AT
EACH
PIXEL
IN
A
WINDOW
AROUND
THE
DETECTED
KEYPOINT
USING
THE
APPROPRIATE
LEVEL
OF
THE
GAUSSIAN
PYRAMID
AT
WHICH
THE
KEYPOINT
WAS
DETECTED
THE
GRADIENT
MAGNITUDES
ARE
DOWNWEIGHTED
BY
A
GAUSSIAN
FALL
OFF
FUNCTION
SHOWN
AS
A
BLUE
CIRCLE
IN
FIGURE
IN
ORDER
TO
REDUCE
THE
INFLUENCE
OF
GRADIENTS
FAR
FROM
THE
CENTER
AS
THESE
ARE
MORE
AFFECTED
BY
SMALL
MISREGISTRATIONS
IN
EACH
QUADRANT
A
GRADIENT
ORIENTATION
HISTOGRAM
IS
FORMED
BY
CONCEPTUALLY
ADDING
THE
WEIGHTED
GRADIENT
VALUE
TO
ONE
OF
EIGHT
ORIENTATION
HISTOGRAM
BINS
TO
REDUCE
THE
EFFECTS
OF
LOCATION
AND
DOMINANT
ORIENTATION
MISESTIMATION
EACH
OF
THE
ORIGINAL
WEIGHTED
GRADIENT
MAGNITUDES
IS
SOFTLY
ADDED
TO
HISTOGRAM
BINS
USING
TRILINEAR
INTERPOLATION
SOFTLY
DISTRIBUTING
VALUES
TO
ADJACENT
HISTOGRAM
BINS
IS
GENERALLY
A
GOOD
IDEA
IN
ANY
APPLI
CATION
WHERE
HISTOGRAMS
ARE
BEING
COMPUTED
E
G
FOR
HOUGH
TRANSFORMS
SECTION
OR
LOCAL
HISTOGRAM
EQUALIZATION
SECTION
THE
RESULTING
NON
NEGATIVE
VALUES
FORM
A
RAW
VERSION
OF
THE
SIFT
DESCRIPTOR
VECTOR
TO
REDUCE
THE
EFFECTS
OF
CONTRAST
OR
GAIN
ADDITIVE
VARIATIONS
ARE
ALREADY
REMOVED
BY
THE
GRA
DIENT
THE
D
VECTOR
IS
NORMALIZED
TO
UNIT
LENGTH
TO
FURTHER
MAKE
THE
DESCRIPTOR
ROBUST
TO
OTHER
PHOTOMETRIC
VARIATIONS
VALUES
ARE
CLIPPED
TO
AND
THE
RESULTING
VECTOR
IS
ONCE
AGAIN
RENORMALIZED
TO
UNIT
LENGTH
PCA
SIFT
KE
AND
SUKTHANKAR
PROPOSE
A
SIMPLER
WAY
TO
COMPUTE
DESCRIPTORS
IN
SPIRED
BY
SIFT
IT
COMPUTES
THE
X
AND
Y
GRADIENT
DERIVATIVES
OVER
A
PATCH
AND
THEN
REDUCES
THE
RESULTING
DIMENSIONAL
VECTOR
TO
USING
PRINCIPAL
COMPONENT
ANALYSIS
PCA
SECTION
AND
APPENDIX
A
ANOTHER
POPULAR
VARIANT
OF
SIFT
IS
SURF
BAY
TUYTELAARS
AND
VAN
GOOL
WHICH
USES
BOX
FILTERS
TO
APPROXIMATE
THE
DERIVATIVES
AND
INTEGRALS
USED
IN
SIFT
GRADIENT
LOCATION
ORIENTATION
HISTOGRAM
GLOH
THIS
DESCRIPTOR
DEVELOPED
BY
MIKO
LAJCZYK
AND
SCHMID
IS
A
VARIANT
ON
SIFT
THAT
USES
A
LOG
POLAR
BINNING
STRUCTURE
INSTEAD
OF
THE
FOUR
QUADRANTS
USED
BY
LOWE
FIGURE
THE
SPATIAL
BINS
ARE
OF
RADIUS
A
IMAGE
GRADIENTS
B
KEYPOINT
DESCRIPTOR
FIGURE
A
SCHEMATIC
REPRESENTATION
OF
LOWE
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
A
GRADIENT
ORIENTATIONS
AND
MAGNITUDES
ARE
COMPUTED
AT
EACH
PIXEL
AND
WEIGHTED
BY
A
GAUSSIAN
FALL
OFF
FUNCTION
BLUE
CIRCLE
B
A
WEIGHTED
GRADIENT
ORIENTATION
HISTOGRAM
IS
THEN
COMPUTED
IN
EACH
SUBREGION
USING
TRILINEAR
INTERPOLATION
WHILE
THIS
FIGURE
SHOWS
AN
PIXEL
PATCH
AND
A
DESCRIPTOR
ARRAY
LOWE
ACTUAL
IMPLEMENTATION
USES
PATCHES
AND
A
ARRAY
OF
EIGHT
BIN
HISTOGRAMS
AND
WITH
EIGHT
ANGULAR
BINS
EXCEPT
FOR
THE
CENTRAL
REGION
FOR
A
TOTAL
OF
SPA
TIAL
BINS
AND
ORIENTATION
BINS
THE
DIMENSIONAL
HISTOGRAM
IS
THEN
PROJECTED
ONTO
A
DIMENSIONAL
DESCRIPTOR
USING
PCA
TRAINED
ON
A
LARGE
DATABASE
IN
THEIR
EVALUATION
MIKOLAJCZYK
AND
SCHMID
FOUND
THAT
GLOH
WHICH
HAS
THE
BEST
PERFORMANCE
OVERALL
OUTPERFORMS
SIFT
BY
A
SMALL
MARGIN
STEERABLE
FILTERS
STEERABLE
FILTERS
SECTION
ARE
COMBINATIONS
OF
DERIVATIVE
OF
GAUS
SIAN
FILTERS
THAT
PERMIT
THE
RAPID
COMPUTATION
OF
EVEN
AND
ODD
SYMMETRIC
AND
ANTI
SYMMETRIC
EDGE
LIKE
AND
CORNER
LIKE
FEATURES
AT
ALL
POSSIBLE
ORIENTATIONS
FREEMAN
AND
ADELSON
BECAUSE
THEY
USE
REASONABLY
BROAD
GAUSSIANS
THEY
TOO
ARE
SOMEWHAT
INSENSITIVE
TO
LOCALIZA
TION
AND
ORIENTATION
ERRORS
PERFORMANCE
OF
LOCAL
DESCRIPTORS
AMONG
THE
LOCAL
DESCRIPTORS
THAT
MIKOLAJCZYK
AND
SCHMID
COMPARED
THEY
FOUND
THAT
GLOH
PERFORMED
BEST
FOLLOWED
CLOSELY
BY
SIFT
SEE
FIG
URE
THEY
ALSO
PRESENT
RESULTS
FOR
MANY
OTHER
DESCRIPTORS
NOT
COVERED
IN
THIS
BOOK
THE
FIELD
OF
FEATURE
DESCRIPTORS
CONTINUES
TO
EVOLVE
RAPIDLY
WITH
SOME
OF
THE
NEWER
TECH
NIQUES
LOOKING
AT
LOCAL
COLOR
INFORMATION
VAN
DE
WEIJER
AND
SCHMID
ABDEL
HAKIM
AND
FARAG
WINDER
AND
BROWN
DEVELOP
A
MULTI
STAGE
FRAMEWORK
FOR
FEATURE
DESCRIPTOR
COMPUTATION
THAT
SUBSUMES
BOTH
SIFT
AND
GLOH
FIGURE
AND
ALSO
ALLOWS
THEM
TO
LEARN
OPTIMAL
PARAMETERS
FOR
NEWER
DESCRIPTORS
THAT
OUTPERFORM
PREVIOUS
HAND
TUNED
A
IMAGE
GRADIENTS
B
KEYPOINT
DESCRIPTOR
FIGURE
THE
GRADIENT
LOCATION
ORIENTATION
HISTOGRAM
GLOH
DESCRIPTOR
USES
LOG
POLAR
BINS
INSTEAD
OF
SQUARE
BINS
TO
COMPUTE
ORIENTATION
HISTOGRAMS
MIKOLAJCZYK
AND
SCHMID
DESCRIPTORS
HUA
BROWN
AND
WINDER
EXTEND
THIS
WORK
BY
LEARNING
LOWER
DIMENSIONAL
PROJECTIONS
OF
HIGHER
DIMENSIONAL
DESCRIPTORS
THAT
HAVE
THE
BEST
DISCRIMINATIVE
POWER
BOTH
OF
THESE
PAPERS
USE
A
DATABASE
OF
REAL
WORLD
IMAGE
PATCHES
FIGURE
OBTAINED
BY
SAM
PLING
IMAGES
AT
LOCATIONS
THAT
WERE
RELIABLY
MATCHED
USING
A
ROBUST
STRUCTURE
FROM
MOTION
ALGORITHM
APPLIED
TO
INTERNET
PHOTO
COLLECTIONS
SNAVELY
SEITZ
AND
SZELISKI
GOESELE
SNAVELY
CURLESS
ET
AL
IN
CONCURRENT
WORK
TOLA
LEPETIT
AND
FUA
DEVELOPED
A
SIMILAR
DAISY
DESCRIPTOR
FOR
DENSE
STEREO
MATCHING
AND
OPTIMIZED
ITS
PARAMETERS
BASED
ON
GROUND
TRUTH
STEREO
DATA
WHILE
THESE
TECHNIQUES
CONSTRUCT
FEATURE
DETECTORS
THAT
OPTIMIZE
FOR
REPEATABILITY
ACROSS
ALL
OBJECT
CLASSES
IT
IS
ALSO
POSSIBLE
TO
DEVELOP
CLASS
OR
INSTANCE
SPECIFIC
FEATURE
DETECTORS
THAT
MAXIMIZE
DISCRIMINABILITY
FROM
OTHER
CLASSES
FERENCZ
LEARNED
MILLER
AND
MALIK
FEATURE
MATCHING
ONCE
WE
HAVE
EXTRACTED
FEATURES
AND
THEIR
DESCRIPTORS
FROM
TWO
OR
MORE
IMAGES
THE
NEXT
STEP
IS
TO
ESTABLISH
SOME
PRELIMINARY
FEATURE
MATCHES
BETWEEN
THESE
IMAGES
IN
THIS
SECTION
WE
DIVIDE
THIS
PROBLEM
INTO
TWO
SEPARATE
COMPONENTS
THE
FIRST
IS
TO
SELECT
A
MATCHING
STRATEGY
WHICH
DETERMINES
WHICH
CORRESPONDENCES
ARE
PASSED
ON
TO
THE
NEXT
STAGE
FOR
FURTHER
PROCESS
ING
THE
SECOND
IS
TO
DEVISE
EFFICIENT
DATA
STRUCTURES
AND
ALGORITHMS
TO
PERFORM
THIS
MATCHING
AS
QUICKLY
AS
POSSIBLE
SEE
THE
DISCUSSION
OF
RELATED
TECHNIQUES
IN
SECTION
A
B
FIGURE
SPATIAL
SUMMATION
BLOCKS
FOR
SIFT
GLOH
AND
SOME
NEWLY
DEVELOPED
FEATURE
DESCRIPTORS
WINDER
AND
BROWN
QC
IEEE
A
THE
PARAMETERS
FOR
THE
NEW
FEATURES
E
G
THEIR
GAUSSIAN
WEIGHTS
ARE
LEARNED
FROM
A
TRAINING
DATABASE
OF
B
MATCHED
REAL
WORLD
IMAGE
PATCHES
OBTAINED
FROM
ROBUST
STRUCTURE
FROM
MOTION
APPLIED
TO
INTERNET
PHOTO
COLLEC
TIONS
HUA
BROWN
AND
WINDER
MATCHING
STRATEGY
AND
ERROR
RATES
DETERMINING
WHICH
FEATURE
MATCHES
ARE
REASONABLE
TO
PROCESS
FURTHER
DEPENDS
ON
THE
CONTEXT
IN
WHICH
THE
MATCHING
IS
BEING
PERFORMED
SAY
WE
ARE
GIVEN
TWO
IMAGES
THAT
OVERLAP
TO
A
FAIR
AMOUNT
E
G
FOR
IMAGE
STITCHING
AS
IN
FIGURE
OR
FOR
TRACKING
OBJECTS
IN
A
VIDEO
WE
KNOW
THAT
MOST
FEATURES
IN
ONE
IMAGE
ARE
LIKELY
TO
MATCH
THE
OTHER
IMAGE
ALTHOUGH
SOME
MAY
NOT
MATCH
BECAUSE
THEY
ARE
OCCLUDED
OR
THEIR
APPEARANCE
HAS
CHANGED
TOO
MUCH
ON
THE
OTHER
HAND
IF
WE
ARE
TRYING
TO
RECOGNIZE
HOW
MANY
KNOWN
OBJECTS
APPEAR
IN
A
CLUT
TERED
SCENE
FIGURE
MOST
OF
THE
FEATURES
MAY
NOT
MATCH
FURTHERMORE
A
LARGE
NUMBER
OF
POTENTIALLY
MATCHING
OBJECTS
MUST
BE
SEARCHED
WHICH
REQUIRES
MORE
EFFICIENT
STRATEGIES
AS
DESCRIBED
BELOW
TO
BEGIN
WITH
WE
ASSUME
THAT
THE
FEATURE
DESCRIPTORS
HAVE
BEEN
DESIGNED
SO
THAT
EU
CLIDEAN
VECTOR
MAGNITUDE
DISTANCES
IN
FEATURE
SPACE
CAN
BE
DIRECTLY
USED
FOR
RANKING
POTEN
TIAL
MATCHES
IF
IT
TURNS
OUT
THAT
CERTAIN
PARAMETERS
AXES
IN
A
DESCRIPTOR
ARE
MORE
RELIABLE
THAN
OTHERS
IT
IS
USUALLY
PREFERABLE
TO
RE
SCALE
THESE
AXES
AHEAD
OF
TIME
E
G
BY
DETERMIN
ING
HOW
MUCH
THEY
VARY
WHEN
COMPARED
AGAINST
OTHER
KNOWN
GOOD
MATCHES
HUA
BROWN
AND
WINDER
A
MORE
GENERAL
PROCESS
WHICH
INVOLVES
TRANSFORMING
FEATURE
VECTORS
INTO
A
NEW
SCALED
BASIS
IS
CALLED
WHITENING
AND
IS
DISCUSSED
IN
MORE
DETAIL
IN
THE
CONTEXT
OF
EIGENFACE
BASED
FACE
RECOGNITION
SECTION
GIVEN
A
EUCLIDEAN
DISTANCE
METRIC
THE
SIMPLEST
MATCHING
STRATEGY
IS
TO
SET
A
THRESHOLD
MAXIMUM
DISTANCE
AND
TO
RETURN
ALL
MATCHES
FROM
OTHER
IMAGES
WITHIN
THIS
THRESHOLD
SET
TING
THE
THRESHOLD
TOO
HIGH
RESULTS
IN
TOO
MANY
FALSE
POSITIVES
I
E
INCORRECT
MATCHES
BEING
RETURNED
SETTING
THE
THRESHOLD
TOO
LOW
RESULTS
IN
TOO
MANY
FALSE
NEGATIVES
I
E
TOO
MANY
CORRECT
MATCHES
BEING
MISSED
FIGURE
WE
CAN
QUANTIFY
THE
PERFORMANCE
OF
A
MATCHING
ALGORITHM
AT
A
PARTICULAR
THRESHOLD
BY
FIGURE
RECOGNIZING
OBJECTS
IN
A
CLUTTERED
SCENE
LOWE
QC
SPRINGER
TWO
OF
THE
TRAINING
IMAGES
IN
THE
DATABASE
ARE
SHOWN
ON
THE
LEFT
THESE
ARE
MATCHED
TO
THE
CLUTTERED
SCENE
IN
THE
MIDDLE
USING
SIFT
FEATURES
SHOWN
AS
SMALL
SQUARES
IN
THE
RIGHT
IMAGE
THE
AFFINE
WARP
OF
EACH
RECOGNIZED
DATABASE
IMAGE
ONTO
THE
SCENE
IS
SHOWN
AS
A
LARGER
PARALLELOGRAM
IN
THE
RIGHT
IMAGE
FIGURE
FALSE
POSITIVES
AND
NEGATIVES
THE
BLACK
DIGITS
AND
ARE
FEATURES
BEING
MATCHED
AGAINST
A
DATABASE
OF
FEATURES
IN
OTHER
IMAGES
AT
THE
CURRENT
THRESHOLD
SETTING
THE
SOLID
CIRCLES
THE
GREEN
IS
A
TRUE
POSITIVE
GOOD
MATCH
THE
BLUE
IS
A
FALSE
NEGATIVE
FAILURE
TO
MATCH
AND
THE
RED
IS
A
FALSE
POSITIVE
INCORRECT
MATCH
IF
WE
SET
THE
THRESHOLD
HIGHER
THE
DASHED
CIRCLES
THE
BLUE
BECOMES
A
TRUE
POSITIVE
BUT
THE
BROWN
BECOMES
AN
ADDITIONAL
FALSE
POSITIVE
TRUE
MATCHES
TRUE
NON
MATCHES
PREDICTED
MATCHES
PREDICTED
NON
MATCHES
TABLE
THE
NUMBER
OF
MATCHES
CORRECTLY
AND
INCORRECTLY
ESTIMATED
BY
A
FEATURE
MATCHING
ALGORITHM
SHOWING
THE
NUMBER
OF
TRUE
POSITIVES
TP
FALSE
POSITIVES
FP
FALSE
NEGATIVES
FN
AND
TRUE
NEGATIVES
TN
THE
COLUMNS
SUM
UP
TO
THE
ACTUAL
NUMBER
OF
POSITIVES
P
AND
NEGATIVES
N
WHILE
THE
ROWS
SUM
UP
TO
THE
PREDICTED
NUMBER
OF
POSITIVES
P
AND
NEGATIVES
N
THE
FORMULAS
FOR
THE
TRUE
POSITIVE
RATE
TPR
THE
FALSE
POSITIVE
RATE
FPR
THE
POSITIVE
PREDICTIVE
VALUE
PPV
AND
THE
ACCURACY
ACC
ARE
GIVEN
IN
THE
TEXT
FIRST
COUNTING
THE
NUMBER
OF
TRUE
AND
FALSE
MATCHES
AND
MATCH
FAILURES
USING
THE
FOLLOWING
DEFINITIONS
FAWCETT
TP
TRUE
POSITIVES
I
E
NUMBER
OF
CORRECT
MATCHES
FN
FALSE
NEGATIVES
MATCHES
THAT
WERE
NOT
CORRECTLY
DETECTED
FP
FALSE
POSITIVES
PROPOSED
MATCHES
THAT
ARE
INCORRECT
TN
TRUE
NEGATIVES
NON
MATCHES
THAT
WERE
CORRECTLY
REJECTED
TABLE
SHOWS
A
SAMPLE
CONFUSION
MATRIX
CONTINGENCY
TABLE
CONTAINING
SUCH
NUMBERS
WE
CAN
CONVERT
THESE
NUMBERS
INTO
UNIT
RATES
BY
DEFINING
THE
FOLLOWING
QUANTITIES
FAWCETT
TRUE
POSITIVE
RATE
TPR
FALSE
POSITIVE
RATE
FPR
TPR
TP
TP
FN
FPR
FP
FP
TN
TP
P
FP
N
POSITIVE
PREDICTIVE
VALUE
PPV
PPV
TP
TP
ACCURACY
ACC
TP
FP
P
TP
TN
ACC
P
N
TP
TN
FP
FN
FALSE
POSITIVE
RATE
Θ
D
A
B
FIGURE
ROC
CURVE
AND
ITS
RELATED
RATES
A
THE
ROC
CURVE
PLOTS
THE
TRUE
POSITIVE
RATE
AGAINST
THE
FALSE
POSITIVE
RATE
FOR
A
PARTICULAR
COMBINATION
OF
FEATURE
EXTRACTION
AND
MATCH
ING
ALGORITHMS
IDEALLY
THE
TRUE
POSITIVE
RATE
SHOULD
BE
CLOSE
TO
WHILE
THE
FALSE
POSITIVE
RATE
IS
CLOSE
TO
THE
AREA
UNDER
THE
ROC
CURVE
AUC
IS
OFTEN
USED
AS
A
SINGLE
SCALAR
MEASURE
OF
ALGORITHM
PERFORMANCE
ALTERNATIVELY
THE
EQUAL
ERROR
RATE
IS
SOMETIMES
USED
B
THE
DISTRIBUTION
OF
POSITIVES
MATCHES
AND
NEGATIVES
NON
MATCHES
AS
A
FUNCTION
OF
INTER
FEATURE
DISTANCE
D
AS
THE
THRESHOLD
Θ
IS
INCREASED
THE
NUMBER
OF
TRUE
POSITIVES
TP
AND
FALSE
POSITIVES
FP
INCREASES
IN
THE
INFORMATION
RETRIEVAL
OR
DOCUMENT
RETRIEVAL
LITERATURE
BAEZA
YATES
AND
RIBEIRO
NETO
MANNING
RAGHAVAN
AND
SCHU
TZE
THE
TERM
PRECISION
HOW
MANY
RETURNED
DOCUMENTS
ARE
RELEVANT
IS
USED
INSTEAD
OF
PPV
AND
RECALL
WHAT
FRACTION
OF
RELEVANT
DOCU
MENTS
WAS
FOUND
IS
USED
INSTEAD
OF
TPR
ANY
PARTICULAR
MATCHING
STRATEGY
AT
A
PARTICULAR
THRESHOLD
OR
PARAMETER
SETTING
CAN
BE
RATED
BY
THE
TPR
AND
FPR
NUMBERS
IDEALLY
THE
TRUE
POSITIVE
RATE
WILL
BE
CLOSE
TO
AND
THE
FALSE
POSITIVE
RATE
CLOSE
TO
AS
WE
VARY
THE
MATCHING
THRESHOLD
WE
OBTAIN
A
FAMILY
OF
SUCH
POINTS
WHICH
ARE
COLLECTIVELY
KNOWN
AS
THE
RECEIVER
OPERATING
CHARACTERISTIC
ROC
CURVE
FAWCETT
FIGURE
THE
CLOSER
THIS
CURVE
LIES
TO
THE
UPPER
LEFT
CORNER
I
E
THE
LARGER
THE
AREA
UNDER
THE
CURVE
AUC
THE
BETTER
ITS
PERFORMANCE
FIGURE
SHOWS
HOW
WE
CAN
PLOT
THE
NUMBER
OF
MATCHES
AND
NON
MATCHES
AS
A
FUNCTION
OF
INTER
FEATURE
DISTANCE
D
THESE
CURVES
CAN
THEN
BE
USED
TO
PLOT
AN
ROC
CURVE
EXERCISE
THE
ROC
CURVE
CAN
ALSO
BE
USED
TO
CALCULATE
THE
MEAN
AVERAGE
PRECISION
WHICH
IS
THE
AVERAGE
PRECISION
PPV
AS
YOU
VARY
THE
THRESHOLD
TO
SELECT
THE
BEST
RESULTS
THEN
THE
TWO
TOP
RESULTS
ETC
THE
PROBLEM
WITH
USING
A
FIXED
THRESHOLD
IS
THAT
IT
IS
DIFFICULT
TO
SET
THE
USEFUL
RANGE
FIGURE
FIXED
THRESHOLD
NEAREST
NEIGHBOR
AND
NEAREST
NEIGHBOR
DISTANCE
RATIO
MATCHING
AT
A
FIXED
DISTANCE
THRESHOLD
DASHED
CIRCLES
DESCRIPTOR
DA
FAILS
TO
MATCH
DB
AND
DD
INCORRECTLY
MATCHES
DC
AND
DE
IF
WE
PICK
THE
NEAREST
NEIGHBOR
DA
CORRECTLY
MATCHES
DB
BUT
DD
INCORRECTLY
MATCHES
DC
USING
NEAREST
NEIGHBOR
DISTANCE
RATIO
NNDR
MATCHING
THE
SMALL
NNDR
CORRECTLY
MATCHES
DA
WITH
DB
AND
THE
LARGE
NNDR
CORRECTLY
REJECTS
MATCHES
FOR
DD
OF
THRESHOLDS
CAN
VARY
A
LOT
AS
WE
MOVE
TO
DIFFERENT
PARTS
OF
THE
FEATURE
SPACE
LOWE
MIKOLAJCZYK
AND
SCHMID
A
BETTER
STRATEGY
IN
SUCH
CASES
IS
TO
SIMPLY
MATCH
THE
NEAREST
NEIGHBOR
IN
FEATURE
SPACE
SINCE
SOME
FEATURES
MAY
HAVE
NO
MATCHES
E
G
THEY
MAY
BE
PART
OF
BACKGROUND
CLUTTER
IN
OBJECT
RECOGNITION
OR
THEY
MAY
BE
OCCLUDED
IN
THE
OTHER
IMAGE
A
THRESHOLD
IS
STILL
USED
TO
REDUCE
THE
NUMBER
OF
FALSE
POSITIVES
IDEALLY
THIS
THRESHOLD
ITSELF
WILL
ADAPT
TO
DIFFERENT
REGIONS
OF
THE
FEATURE
SPACE
IF
SUFFICIENT
TRAINING
DATA
IS
AVAILABLE
HUA
BROWN
AND
WINDER
IT
IS
SOMETIMES
POSSIBLE
TO
LEARN
DIFFERENT
THRESHOLDS
FOR
DIFFERENT
FEATURES
OFTEN
HOWEVER
WE
ARE
SIMPLY
GIVEN
A
COLLECTION
OF
IMAGES
TO
MATCH
E
G
WHEN
STITCHING
IMAGES
OR
CONSTRUCTING
MODELS
FROM
UNORDERED
PHOTO
COLLECTIONS
BROWN
AND
LOWE
SNAVELY
SEITZ
AND
SZELISKI
IN
THIS
CASE
A
USEFUL
HEURISTIC
CAN
BE
TO
COMPARE
THE
NEAREST
NEIGHBOR
DISTANCE
TO
THAT
OF
THE
SECOND
NEAREST
NEIGHBOR
PREFERABLY
TAKEN
FROM
AN
IMAGE
THAT
IS
KNOWN
NOT
TO
MATCH
THE
TARGET
E
G
A
DIFFERENT
OBJECT
IN
THE
DATABASE
BROWN
AND
LOWE
LOWE
WE
CAN
DEFINE
THIS
NEAREST
NEIGHBOR
DISTANCE
RATIO
MIKOLAJCZYK
AND
SCHMID
AS
NNDR
DB
DC
WHERE
AND
ARE
THE
NEAREST
AND
SECOND
NEAREST
NEIGHBOR
DISTANCES
DA
IS
THE
TARGET
DESCRIPTOR
AND
DB
AND
DC
ARE
ITS
CLOSEST
TWO
NEIGHBORS
FIGURE
THE
EFFECTS
OF
USING
THESE
THREE
DIFFERENT
MATCHING
STRATEGIES
FOR
THE
FEATURE
DESCRIPTORS
EVALUATED
BY
MIKOLAJCZYK
AND
SCHMID
ARE
SHOWN
IN
FIGURE
AS
YOU
CAN
SEE
THE
NEAREST
NEIGHBOR
AND
NNDR
STRATEGIES
PRODUCE
IMPROVED
ROC
CURVES
GLOH
CROSS
CORRELATION
SIFT
GRADIENT
MOMENTS
PCA
SIFT
COMPLEX
FILTERS
SHAPE
CONTEXT
DIFFERENTIAL
INVARIANTS
SPIN
HES
LAP
GLOH
STEERABLE
FILTERS
PRECISION
A
GLOH
CROSS
CORRELATION
GLOH
CROSS
CORRELATION
SIFT
GRADIENT
MOMENTS
SIFT
GRADIENT
MOMENTS
PCA
SIFT
SHAPE
CONTEXT
COMPLEX
FILTERS
DIFFERENTIAL
INVARIANTS
PCA
SIFT
SHAPE
CONTEXT
COMPLEX
FILTERS
DIFFERENTIAL
INVARIANTS
SPIN
HES
LAP
GLOH
STEERABLE
FILTERS
SPIN
HES
LAP
GLOH
STEERABLE
FILTERS
PRECISION
PRECISION
B
C
FIGURE
PERFORMANCE
OF
THE
FEATURE
DESCRIPTORS
EVALUATED
BY
MIKOLAJCZYK
AND
SCHMID
QC
IEEE
SHOWN
FOR
THREE
MATCHING
STRATEGIES
A
FIXED
THRESHOLD
B
NEAREST
NEIGHBOR
C
NEAREST
NEIGHBOR
DISTANCE
RATIO
NNDR
NOTE
HOW
THE
ORDERING
OF
THE
ALGO
RITHMS
DOES
NOT
CHANGE
THAT
MUCH
BUT
THE
OVERALL
PERFORMANCE
VARIES
SIGNIFICANTLY
BETWEEN
THE
DIFFERENT
MATCHING
STRATEGIES
FIGURE
THE
THREE
HAAR
WAVELET
COEFFICIENTS
USED
FOR
HASHING
THE
MOPS
DESCRIPTOR
DE
VISED
BY
BROWN
SZELISKI
AND
WINDER
ARE
COMPUTED
BY
SUMMING
EACH
NORMALIZED
PATCH
OVER
THE
LIGHT
AND
DARK
GRAY
REGIONS
AND
TAKING
THEIR
DIFFERENCE
EFFICIENT
MATCHING
ONCE
WE
HAVE
DECIDED
ON
A
MATCHING
STRATEGY
WE
STILL
NEED
TO
SEARCH
EFFICIENTLY
FOR
POTEN
TIAL
CANDIDATES
THE
SIMPLEST
WAY
TO
FIND
ALL
CORRESPONDING
FEATURE
POINTS
IS
TO
COMPARE
ALL
FEATURES
AGAINST
ALL
OTHER
FEATURES
IN
EACH
PAIR
OF
POTENTIALLY
MATCHING
IMAGES
UNFORTUNATELY
THIS
IS
QUADRATIC
IN
THE
NUMBER
OF
EXTRACTED
FEATURES
WHICH
MAKES
IT
IMPRACTICAL
FOR
MOST
APPLICATIONS
A
BETTER
APPROACH
IS
TO
DEVISE
AN
INDEXING
STRUCTURE
SUCH
AS
A
MULTI
DIMENSIONAL
SEARCH
TREE
OR
A
HASH
TABLE
TO
RAPIDLY
SEARCH
FOR
FEATURES
NEAR
A
GIVEN
FEATURE
SUCH
INDEXING
STRUC
TURES
CAN
EITHER
BE
BUILT
FOR
EACH
IMAGE
INDEPENDENTLY
WHICH
IS
USEFUL
IF
WE
WANT
TO
ONLY
CONSIDER
CERTAIN
POTENTIAL
MATCHES
E
G
SEARCHING
FOR
A
PARTICULAR
OBJECT
OR
GLOBALLY
FOR
ALL
THE
IMAGES
IN
A
GIVEN
DATABASE
WHICH
CAN
POTENTIALLY
BE
FASTER
SINCE
IT
REMOVES
THE
NEED
TO
IT
ERATE
OVER
EACH
IMAGE
FOR
EXTREMELY
LARGE
DATABASES
MILLIONS
OF
IMAGES
OR
MORE
EVEN
MORE
EFFICIENT
STRUCTURES
BASED
ON
IDEAS
FROM
DOCUMENT
RETRIEVAL
E
G
VOCABULARY
TREES
NISTE
R
AND
STEWE
NIUS
CAN
BE
USED
SECTION
ONE
OF
THE
SIMPLER
TECHNIQUES
TO
IMPLEMENT
IS
MULTI
DIMENSIONAL
HASHING
WHICH
MAPS
DESCRIPTORS
INTO
FIXED
SIZE
BUCKETS
BASED
ON
SOME
FUNCTION
APPLIED
TO
EACH
DESCRIPTOR
VECTOR
AT
MATCHING
TIME
EACH
NEW
FEATURE
IS
HASHED
INTO
A
BUCKET
AND
A
SEARCH
OF
NEARBY
BUCKETS
IS
USED
TO
RETURN
POTENTIAL
CANDIDATES
WHICH
CAN
THEN
BE
SORTED
OR
GRADED
TO
DETERMINE
WHICH
ARE
VALID
MATCHES
A
SIMPLE
EXAMPLE
OF
HASHING
IS
THE
HAAR
WAVELETS
USED
BY
BROWN
SZELISKI
AND
WINDER
IN
THEIR
MOPS
PAPER
DURING
THE
MATCHING
STRUCTURE
CONSTRUCTION
EACH
SCALED
ORIENTED
AND
NORMALIZED
MOPS
PATCH
IS
CONVERTED
INTO
A
THREE
ELEMENT
INDEX
BY
PERFORM
ING
SUMS
OVER
DIFFERENT
QUADRANTS
OF
THE
PATCH
FIGURE
THE
RESULTING
THREE
VALUES
ARE
NORMALIZED
BY
THEIR
EXPECTED
STANDARD
DEVIATIONS
AND
THEN
MAPPED
TO
THE
TWO
OF
B
NEAREST
BINS
THE
THREE
DIMENSIONAL
INDICES
FORMED
BY
CONCATENATING
THE
THREE
QUANTIZED
VALUES
ARE
USED
TO
INDEX
THE
BINS
WHERE
THE
FEATURE
IS
STORED
ADDED
AT
QUERY
TIME
ONLY
THE
PRIMARY
CLOSEST
INDICES
ARE
USED
SO
ONLY
A
SINGLE
THREE
DIMENSIONAL
BIN
NEEDS
TO
A
B
FIGURE
K
D
TREE
AND
BEST
BIN
FIRST
BBF
SEARCH
BEIS
AND
LOWE
QC
IEEE
A
THE
SPATIAL
ARRANGEMENT
OF
THE
AXIS
ALIGNED
CUTTING
PLANES
IS
SHOWN
USING
DASHED
LINES
INDIVIDUAL
DATA
POINTS
ARE
SHOWN
AS
SMALL
DIAMONDS
B
THE
SAME
SUBDIVISION
CAN
BE
REPRE
SENTED
AS
A
TREE
WHERE
EACH
INTERIOR
NODE
REPRESENTS
AN
AXIS
ALIGNED
CUTTING
PLANE
E
G
THE
TOP
NODE
CUTS
ALONG
DIMENSION
AT
VALUE
AND
EACH
LEAF
NODE
IS
A
DATA
POINT
DURING
A
BBF
SEARCH
A
QUERY
POINT
DENOTED
BY
FIRST
LOOKS
IN
ITS
CONTAINING
BIN
D
AND
THEN
IN
ITS
NEAREST
ADJACENT
BIN
B
RATHER
THAN
ITS
CLOSEST
NEIGHBOR
IN
THE
TREE
C
BE
EXAMINED
THE
COEFFICIENTS
IN
THE
BIN
CAN
THEN
BE
USED
TO
SELECT
K
APPROXIMATE
NEAREST
NEIGHBORS
FOR
FURTHER
PROCESSING
SUCH
AS
COMPUTING
THE
NNDR
A
MORE
COMPLEX
BUT
MORE
WIDELY
APPLICABLE
VERSION
OF
HASHING
IS
CALLED
LOCALITY
SEN
SITIVE
HASHING
WHICH
USES
UNIONS
OF
INDEPENDENTLY
COMPUTED
HASHING
FUNCTIONS
TO
INDEX
THE
FEATURES
GIONIS
INDYK
AND
MOTWANI
SHAKHNAROVICH
DARRELL
AND
INDYK
SHAKHNAROVICH
VIOLA
AND
DARRELL
EXTEND
THIS
TECHNIQUE
TO
BE
MORE
SENSITIVE
TO
THE
DISTRIBUTION
OF
POINTS
IN
PARAMETER
SPACE
WHICH
THEY
CALL
PARAMETER
SENSITIVE
HASHING
EVEN
MORE
RECENT
WORK
CONVERTS
HIGH
DIMENSIONAL
DESCRIPTOR
VECTORS
INTO
BINARY
CODES
THAT
CAN
BE
COMPARED
USING
HAMMING
DISTANCES
TORRALBA
WEISS
AND
FERGUS
WEISS
TORRALBA
AND
FERGUS
OR
THAT
CAN
ACCOMMODATE
ARBITRARY
KERNEL
FUNCTIONS
KULIS
AND
GRAUMAN
RAGINSKY
AND
LAZEBNIK
ANOTHER
WIDELY
USED
CLASS
OF
INDEXING
STRUCTURES
ARE
MULTI
DIMENSIONAL
SEARCH
TREES
THE
BEST
KNOWN
OF
THESE
ARE
K
D
TREES
ALSO
OFTEN
WRITTEN
AS
KD
TREES
WHICH
DIVIDE
THE
MULTI
DIMENSIONAL
FEATURE
SPACE
ALONG
ALTERNATING
AXIS
ALIGNED
HYPERPLANES
CHOOSING
THE
THRESHOLD
ALONG
EACH
AXIS
SO
AS
TO
MAXIMIZE
SOME
CRITERION
SUCH
AS
THE
SEARCH
TREE
BALANCE
SAMET
FIGURE
SHOWS
AN
EXAMPLE
OF
A
TWO
DIMENSIONAL
K
D
TREE
HERE
EIGHT
DIFFERENT
DATA
POINTS
A
H
ARE
SHOWN
AS
SMALL
DIAMONDS
ARRANGED
ON
A
TWO
DIMENSIONAL
PLANE
THE
K
D
TREE
RECURSIVELY
SPLITS
THIS
PLANE
ALONG
AXIS
ALIGNED
HORIZONTAL
OR
VERTICAL
CUTTING
PLANES
EACH
SPLIT
CAN
BE
DENOTED
USING
THE
DIMENSION
NUMBER
AND
SPLIT
VALUE
FIGURE
THE
SPLITS
ARE
ARRANGED
SO
AS
TO
TRY
TO
BALANCE
THE
TREE
I
E
TO
KEEP
ITS
MAXIMUM
DEPTH
AS
SMALL
AS
POSSIBLE
AT
QUERY
TIME
A
CLASSIC
K
D
TREE
SEARCH
FIRST
LOCATES
THE
QUERY
POINT
IN
ITS
APPROPRIATE
BIN
D
AND
THEN
SEARCHES
NEARBY
LEAVES
IN
THE
TREE
C
B
UNTIL
IT
CAN
GUARANTEE
THAT
THE
NEAREST
NEIGHBOR
HAS
BEEN
FOUND
THE
BEST
BIN
FIRST
BBF
SEARCH
BEIS
AND
LOWE
SEARCHES
BINS
IN
ORDER
OF
THEIR
SPATIAL
PROXIMITY
TO
THE
QUERY
POINT
AND
IS
THEREFORE
USUALLY
MORE
EFFICIENT
MANY
ADDITIONAL
DATA
STRUCTURES
HAVE
BEEN
DEVELOPED
OVER
THE
YEARS
FOR
SOLVING
NEAREST
NEIGHBOR
PROBLEMS
ARYA
MOUNT
NETANYAHU
ET
AL
LIANG
LIU
XU
ET
AL
HJALTA
SON
AND
SAMET
FOR
EXAMPLE
NENE
AND
NAYAR
DEVELOPED
A
TECHNIQUE
THEY
CALL
SLICING
THAT
USES
A
SERIES
OF
BINARY
SEARCHES
ON
THE
POINT
LIST
SORTED
ALONG
DIFFERENT
DIMEN
SIONS
TO
EFFICIENTLY
CULL
DOWN
A
LIST
OF
CANDIDATE
POINTS
THAT
LIE
WITHIN
A
HYPERCUBE
OF
THE
QUERY
POINT
GRAUMAN
AND
DARRELL
REWEIGHT
THE
MATCHES
AT
DIFFERENT
LEVELS
OF
AN
INDEXING
TREE
WHICH
ALLOWS
THEIR
TECHNIQUE
TO
BE
LESS
SENSITIVE
TO
DISCRETIZATION
ERRORS
IN
THE
TREE
CON
STRUCTION
NISTE
R
AND
STEWE
NIUS
USE
A
METRIC
TREE
WHICH
COMPARES
FEATURE
DESCRIPTORS
TO
A
SMALL
NUMBER
OF
PROTOTYPES
AT
EACH
LEVEL
IN
A
HIERARCHY
THE
RESULTING
QUANTIZED
VISUAL
WORDS
CAN
THEN
BE
USED
WITH
CLASSICAL
INFORMATION
RETRIEVAL
DOCUMENT
RELEVANCE
TECHNIQUES
TO
QUICKLY
WINNOW
DOWN
A
SET
OF
POTENTIAL
CANDIDATES
FROM
A
DATABASE
OF
MILLIONS
OF
IMAGES
SECTION
MUJA
AND
LOWE
COMPARE
A
NUMBER
OF
THESE
APPROACHES
INTRODUCE
A
NEW
ONE
OF
THEIR
OWN
PRIORITY
SEARCH
ON
HIERARCHICAL
K
MEANS
TREES
AND
CONCLUDE
THAT
MUL
TIPLE
RANDOMIZED
K
D
TREES
OFTEN
PROVIDE
THE
BEST
PERFORMANCE
DESPITE
ALL
OF
THIS
PROMISING
WORK
THE
RAPID
COMPUTATION
OF
IMAGE
FEATURE
CORRESPONDENCES
REMAINS
A
CHALLENGING
OPEN
RESEARCH
PROBLEM
FEATURE
MATCH
VERIFICATION
AND
DENSIFICATION
ONCE
WE
HAVE
SOME
HYPOTHETICAL
PUTATIVE
MATCHES
WE
CAN
OFTEN
USE
GEOMETRIC
ALIGNMENT
SECTION
TO
VERIFY
WHICH
MATCHES
ARE
INLIERS
AND
WHICH
ONES
ARE
OUTLIERS
FOR
EXAMPLE
IF
WE
EXPECT
THE
WHOLE
IMAGE
TO
BE
TRANSLATED
OR
ROTATED
IN
THE
MATCHING
VIEW
WE
CAN
FIT
A
GLOBAL
GEOMETRIC
TRANSFORM
AND
KEEP
ONLY
THOSE
FEATURE
MATCHES
THAT
ARE
SUFFICIENTLY
CLOSE
TO
THIS
ESTIMATED
TRANSFORMATION
THE
PROCESS
OF
SELECTING
A
SMALL
SET
OF
SEED
MATCHES
AND
THEN
VERIFYING
A
LARGER
SET
IS
OFTEN
CALLED
RANDOM
SAMPLING
OR
RANSAC
SECTION
ONCE
AN
INITIAL
SET
OF
CORRESPONDENCES
HAS
BEEN
ESTABLISHED
SOME
SYSTEMS
LOOK
FOR
ADDITIONAL
MATCHES
E
G
BY
LOOKING
FOR
ADDITIONAL
CORRESPONDENCES
ALONG
EPIPOLAR
LINES
SECTION
OR
IN
THE
VICINITY
OF
ESTIMATED
LOCATIONS
BASED
ON
THE
GLOBAL
TRANSFORM
THESE
TOPICS
ARE
DISCUSSED
FURTHER
IN
SECTIONS
AND
FEATURE
TRACKING
AN
ALTERNATIVE
TO
INDEPENDENTLY
FINDING
FEATURES
IN
ALL
CANDIDATE
IMAGES
AND
THEN
MATCHING
THEM
IS
TO
FIND
A
SET
OF
LIKELY
FEATURE
LOCATIONS
IN
A
FIRST
IMAGE
AND
TO
THEN
SEARCH
FOR
THEIR
CORRESPONDING
LOCATIONS
IN
SUBSEQUENT
IMAGES
THIS
KIND
OF
DETECT
THEN
TRACK
APPROACH
IS
MORE
WIDELY
USED
FOR
VIDEO
TRACKING
APPLICATIONS
WHERE
THE
EXPECTED
AMOUNT
OF
MOTION
AND
APPEARANCE
DEFORMATION
BETWEEN
ADJACENT
FRAMES
IS
EXPECTED
TO
BE
SMALL
THE
PROCESS
OF
SELECTING
GOOD
FEATURES
TO
TRACK
IS
CLOSELY
RELATED
TO
SELECTING
GOOD
FEATURES
FOR
MORE
GENERAL
RECOGNITION
APPLICATIONS
IN
PRACTICE
REGIONS
CONTAINING
HIGH
GRADIENTS
IN
BOTH
DIRECTIONS
I
E
WHICH
HAVE
HIGH
EIGENVALUES
IN
THE
AUTO
CORRELATION
MATRIX
PROVIDE
STABLE
LOCATIONS
AT
WHICH
TO
FIND
CORRESPONDENCES
SHI
AND
TOMASI
IN
SUBSEQUENT
FRAMES
SEARCHING
FOR
LOCATIONS
WHERE
THE
CORRESPONDING
PATCH
HAS
LOW
SQUARED
DIFFERENCE
OFTEN
WORKS
WELL
ENOUGH
HOWEVER
IF
THE
IMAGES
ARE
UNDERGO
ING
BRIGHTNESS
CHANGE
EXPLICITLY
COMPENSATING
FOR
SUCH
VARIATIONS
OR
USING
NORMALIZED
CROSS
CORRELATION
MAY
BE
PREFERABLE
IF
THE
SEARCH
RANGE
IS
LARGE
IT
IS
ALSO
OFTEN
MORE
EFFICIENT
TO
USE
A
HIERARCHICAL
SEARCH
STRATEGY
WHICH
USES
MATCHES
IN
LOWER
RESOLUTION
IMAGES
TO
PROVIDE
BETTER
INITIAL
GUESSES
AND
HENCE
SPEED
UP
THE
SEARCH
SECTION
ALTERNATIVES
TO
THIS
STRATEGY
INVOLVE
LEARNING
WHAT
THE
APPEARANCE
OF
THE
PATCH
BEING
TRACKED
SHOULD
BE
AND
THEN
SEARCHING
FOR
IT
IN
THE
VICINITY
OF
ITS
PREDICTED
POSITION
AVIDAN
JURIE
AND
DHOME
WILLIAMS
BLAKE
AND
CIPOLLA
THESE
TOPICS
ARE
ALL
COVERED
IN
MORE
DETAIL
IN
SECTION
IF
FEATURES
ARE
BEING
TRACKED
OVER
LONGER
IMAGE
SEQUENCES
THEIR
APPEARANCE
CAN
UNDERGO
LARGER
CHANGES
YOU
THEN
HAVE
TO
DECIDE
WHETHER
TO
CONTINUE
MATCHING
AGAINST
THE
ORIGINALLY
DETECTED
PATCH
FEATURE
OR
TO
RE
SAMPLE
EACH
SUBSEQUENT
FRAME
AT
THE
MATCHING
LOCATION
THE
FORMER
STRATEGY
IS
PRONE
TO
FAILURE
AS
THE
ORIGINAL
PATCH
CAN
UNDERGO
APPEARANCE
CHANGES
SUCH
AS
FORESHORTENING
THE
LATTER
RUNS
THE
RISK
OF
THE
FEATURE
DRIFTING
FROM
ITS
ORIGINAL
LOCATION
TO
SOME
OTHER
LOCATION
IN
THE
IMAGE
SHI
AND
TOMASI
MATHEMATICALLY
SMALL
MIS
REGISTRATION
ERRORS
COMPOUND
TO
CREATE
A
MARKOV
RANDOM
WALK
WHICH
LEADS
TO
LARGER
DRIFT
OVER
TIME
A
PREFERABLE
SOLUTION
IS
TO
COMPARE
THE
ORIGINAL
PATCH
TO
LATER
IMAGE
LOCATIONS
USING
AN
AFFINE
MOTION
MODEL
SECTION
SHI
AND
TOMASI
FIRST
COMPARE
PATCHES
IN
NEIGH
BORING
FRAMES
USING
A
TRANSLATIONAL
MODEL
AND
THEN
USE
THE
LOCATION
ESTIMATES
PRODUCED
BY
THIS
STEP
TO
INITIALIZE
AN
AFFINE
REGISTRATION
BETWEEN
THE
PATCH
IN
THE
CURRENT
FRAME
AND
THE
BASE
FRAME
WHERE
A
FEATURE
WAS
FIRST
DETECTED
FIGURE
IN
THEIR
SYSTEM
FEATURES
ARE
ONLY
DETECTED
INFREQUENTLY
I
E
ONLY
IN
REGIONS
WHERE
TRACKING
HAS
FAILED
IN
THE
USUAL
CASE
AN
AREA
AROUND
THE
CURRENT
PREDICTED
LOCATION
OF
THE
FEATURE
IS
SEARCHED
WITH
AN
INCREMENTAL
REG
ISTRATION
ALGORITHM
SECTION
THE
RESULTING
TRACKER
IS
OFTEN
CALLED
THE
KANADE
LUCAS
TOMASI
KLT
TRACKER
FIGURE
FEATURE
TRACKING
USING
AN
AFFINE
MOTION
MODEL
SHI
AND
TOMASI
QC
IEEE
TOP
ROW
IMAGE
PATCH
AROUND
THE
TRACKED
FEATURE
LOCATION
BOTTOM
ROW
IMAGE
PATCH
AFTER
WARPING
BACK
TOWARD
THE
FIRST
FRAME
USING
AN
AFFINE
DEFORMATION
EVEN
THOUGH
THE
SPEED
SIGN
GETS
LARGER
FROM
FRAME
TO
FRAME
THE
AFFINE
TRANSFORMATION
MAINTAINS
A
GOOD
RESEMBLANCE
BETWEEN
THE
ORIGINAL
AND
SUBSEQUENT
TRACKED
FRAMES
SINCE
THEIR
ORIGINAL
WORK
ON
FEATURE
TRACKING
SHI
AND
TOMASI
APPROACH
HAS
GENERATED
A
STRING
OF
INTERESTING
FOLLOW
ON
PAPERS
AND
APPLICATIONS
BEARDSLEY
TORR
AND
ZISSERMAN
USE
EXTENDED
FEATURE
TRACKING
COMBINED
WITH
STRUCTURE
FROM
MOTION
CHAPTER
TO
INCREMEN
TALLY
BUILD
UP
SPARSE
MODELS
FROM
VIDEO
SEQUENCES
KANG
SZELISKI
AND
SHUM
TIE
TOGETHER
THE
CORNERS
OF
ADJACENT
REGULARLY
GRIDDED
PATCHES
TO
PROVIDE
SOME
ADDITIONAL
STABILITY
TO
THE
TRACKING
AT
THE
COST
OF
POORER
HANDLING
OF
OCCLUSIONS
TOMMASINI
FUSIELLO
TRUCCO
ET
AL
PROVIDE
A
BETTER
SPURIOUS
MATCH
REJECTION
CRITERION
FOR
THE
BASIC
SHI
AND
TOMASI
ALGORITHM
COLLINS
AND
LIU
PROVIDE
IMPROVED
MECHANISMS
FOR
FEATURE
SELEC
TION
AND
DEALING
WITH
LARGER
APPEARANCE
CHANGES
OVER
TIME
AND
SHAFIQUE
AND
SHAH
DEVELOP
ALGORITHMS
FOR
FEATURE
MATCHING
DATA
ASSOCIATION
FOR
VIDEOS
WITH
LARGE
NUMBERS
OF
MOVING
OBJECTS
OR
POINTS
YILMAZ
JAVED
AND
SHAH
AND
LEPETIT
AND
FUA
SURVEY
THE
LARGER
FIELD
OF
OBJECT
TRACKING
WHICH
INCLUDES
NOT
ONLY
FEATURE
BASED
TECHNIQUES
BUT
ALSO
ALTERNATIVE
TECHNIQUES
BASED
ON
CONTOUR
AND
REGION
SECTION
ONE
OF
THE
NEWEST
DEVELOPMENTS
IN
FEATURE
TRACKING
IS
THE
USE
OF
LEARNING
ALGORITHMS
TO
BUILD
SPECIAL
PURPOSE
RECOGNIZERS
TO
RAPIDLY
SEARCH
FOR
MATCHING
FEATURES
ANYWHERE
IN
AN
IMAGE
LEPETIT
PILET
AND
FUA
HINTERSTOISSER
BENHIMANE
NAVAB
ET
AL
ROGEZ
RIHAN
RAMALINGAM
ET
AL
O
ZUYSAL
CALONDER
LEPETIT
ET
AL
BY
TAKING
THE
TIME
TO
TRAIN
CLASSIFIERS
ON
SAMPLE
PATCHES
AND
THEIR
AFFINE
DEFORMATIONS
EXTREMELY
FAST
AND
RELIABLE
FEATURE
DETECTORS
CAN
BE
CONSTRUCTED
WHICH
ENABLES
MUCH
FASTER
MOTIONS
TO
BE
SUPPORTED
FIGURE
COUPLING
SUCH
FEATURES
TO
DEFORMABLE
MODELS
PILET
LEPETIT
AND
FUA
OR
STRUCTURE
FROM
MOTION
ALGORITHMS
KLEIN
AND
MURRAY
CAN
RESULT
IN
EVEN
HIGHER
STABILITY
SEE
ALSO
MY
PREVIOUS
COMMENT
ON
EARLIER
WORK
IN
LEARNING
BASED
TRACKING
AVIDAN
JURIE
AND
DHOME
WILLIAMS
BLAKE
AND
CIPOLLA
FIGURE
REAL
TIME
HEAD
TRACKING
USING
THE
FAST
TRAINED
CLASSIFIERS
OF
LEPETIT
PILET
AND
FUA
QC
IEEE
APPLICATION
PERFORMANCE
DRIVEN
ANIMATION
ONE
OF
THE
MOST
COMPELLING
APPLICATIONS
OF
FAST
FEATURE
TRACKING
IS
PERFORMANCE
DRIVEN
AN
IMATION
I
E
THE
INTERACTIVE
DEFORMATION
OF
A
GRAPHICS
MODEL
BASED
ON
TRACKING
A
USER
MOTIONS
WILLIAMS
LITWINOWICZ
AND
WILLIAMS
LEPETIT
PILET
AND
FUA
BUCK
FINKELSTEIN
JACOBS
ET
AL
PRESENT
A
SYSTEM
THAT
TRACKS
A
USER
FACIAL
EXPRES
SIONS
AND
HEAD
MOTIONS
AND
THEN
USES
THEM
TO
MORPH
AMONG
A
SERIES
OF
HAND
DRAWN
SKETCHES
AN
ANIMATOR
FIRST
EXTRACTS
THE
EYE
AND
MOUTH
REGIONS
OF
EACH
SKETCH
AND
DRAWS
CONTROL
LINES
OVER
EACH
IMAGE
FIGURE
AT
RUN
TIME
A
FACE
TRACKING
SYSTEM
TOYAMA
DETER
MINES
THE
CURRENT
LOCATION
OF
THESE
FEATURES
FIGURE
THE
ANIMATION
SYSTEM
DECIDES
WHICH
INPUT
IMAGES
TO
MORPH
BASED
ON
NEAREST
NEIGHBOR
FEATURE
APPEARANCE
MATCHING
AND
TRIANGULAR
BARYCENTRIC
INTERPOLATION
IT
ALSO
COMPUTES
THE
GLOBAL
LOCATION
AND
ORIENTATION
OF
THE
HEAD
FROM
THE
TRACKED
FEATURES
THE
RESULTING
MORPHED
EYE
AND
MOUTH
REGIONS
ARE
THEN
COMPOSITED
BACK
INTO
THE
OVERALL
HEAD
MODEL
TO
YIELD
A
FRAME
OF
HAND
DRAWN
ANIMATION
FIG
URE
IN
MORE
RECENT
WORK
BARNES
JACOBS
SANDERS
ET
AL
WATCH
USERS
ANIMATE
PAPER
CUTOUTS
ON
A
DESK
AND
THEN
TURN
THE
RESULTING
MOTIONS
AND
DRAWINGS
INTO
SEAMLESS
ANIMA
TIONS
WHERE
WE
HAVE
COMPUTED
THE
INTEGRAL
OVER
Φ
USING
THE
SAME
METHOD
AS
IN
EQUA
TION
UNFORTUNATELY
WE
STILL
CANNOT
COMPUTE
THE
REMAINING
INTEGRAL
IN
CLOSED
FORM
SO
WE
INSTEAD
TAKE
THE
APPROACH
OF
MAXIMIZING
OVER
HIDDEN
VARIABLES
TO
GIVE
AN
APPROXIMATE
EXPRESSION
FOR
THE
MARGINAL
LIKELIHOOD
P
R
W
X
MAX
RNORMW
XT
H
DTT
GAMHD
Ν
Ν
AS
LONG
AS
THE
TRUE
DISTRIBUTION
OVER
THE
HIDDEN
VARIABLES
IS
CONCENTRATED
TIGHTLY
AROUND
THE
MODE
THIS
WILL
BE
A
REASONABLE
APPROXIMATION
WHEN
HD
TAKES
A
LARGE
VALUE
THE
PRIOR
HAS
A
SMALL
VARIANCE
HD
AND
THE
ASSOCIATED
COEFFICIENT
ΦD
WILL
BE
FORCED
TO
BE
CLOSE
TO
ZERO
IN
EFFECT
THIS
MEANS
THAT
THE
DTH
DIMENSION
OF
X
DOES
NOT
CONTRIBUTE
TO
THE
SOLUTION
AND
CAN
BE
DROPPED
FROM
THE
EQUATIONS
THE
GENERAL
APPROACH
TO
FITTING
THE
MODEL
IS
NOW
CLEAR
THERE
ARE
TWO
UNKNOWN
QUANTITIES
THE
VARIANCE
AND
THE
HIDDEN
VARIABLES
H
AND
WE
ALTERNATELY
UPDATE
EACH
TO
MAXIMIZE
THE
LOG
MARGINAL
LIKELIHOOD
TO
UPDATE
THE
HIDDEN
VARIABLES
WE
TAKE
THE
DERIVATIVE
OF
THE
LOG
OF
THIS
EXPRESSION
WITH
RESPECT
TO
H
EQUATE
THE
RESULT
TO
ZERO
AND
RE
ARRANGE
TO
GET
THE
ITERATION
HNEW
HDΣDD
Ν
D
Ν
WHERE
ΜD
IS
THE
DTH
ELEMENT
OF
THE
MEAN
Μ
OF
THE
POSTERIOR
DISTRIBUTION
OVER
THE
WEIGHTS
Φ
AND
ΣDD
IS
THE
DTH
ELEMENT
OF
THE
DIAGONAL
OF
THE
COVARIANCE
Σ
OF
THE
POSTERIOR
DISTRIBUTION
OVER
THE
WEIGHTS
EQUATION
SO
THAT
Μ
A
Σ
A
AND
A
IS
DEFINED
AS
A
XXT
H
TO
UPDATE
THE
VARIANCE
WE
TAKE
THE
DERIVATIVE
OF
THE
LOG
OF
THIS
EXPRESSION
WITH
RESPECT
TO
EQUATE
THE
RESULT
TO
ZERO
AND
SIMPLIFY
TO
GET
NEW
D
H
Σ
W
XΜ
W
XΜ
DETAILS
ABOUT
HOW
THESE
NON
OBVIOUS
UPDATE
EQUATIONS
WERE
GENERATED
CAN
BE
FOUND
IN
SECTION
OF
BISHOP
AND
TIPPING
REGRESSION
MODELS
FIGURE
SPARSE
LINEAR
REGRESSION
A
BAYESIAN
LINEAR
REGRESSION
FROM
TWO
DIMENSIONAL
DATA
THE
BACKGROUND
COLOR
REPRESENTS
THE
MEAN
ΜW
X
OF
THE
GAUSSIAN
PREDICTION
P
R
W
X
FOR
W
THE
VARIANCE
OF
P
R
W
X
IS
NOT
SHOWN
THE
COLOR
OF
THE
DATA
POINTS
INDICATES
THE
TRAINING
VALUE
W
SO
FOR
A
PERFECT
REGRESSION
FIT
THIS
SHOULD
MATCH
EXACTLY
THE
SURROUNDING
COLOR
HERE
THE
ELEMENTS
OF
Φ
TAKE
ARBITRARY
VALUES
AND
SO
THE
GRADIENT
OF
THE
FUNCTION
POINTS
IN
AN
ARBITRARY
DIRECTION
B
SPARSE
LINEAR
REGRESSION
HERE
THE
ELEMENTS
OF
Φ
ARE
ENCOURAGED
TO
BE
ZERO
WHERE
THEY
ARE
NOT
NECESSARY
TO
EXPLAIN
THE
DATA
THE
ALGORITHM
HAS
FOUND
A
GOOD
FIT
WHERE
THE
SECOND
ELEMENT
OF
Φ
IS
ZERO
AND
SO
THERE
IS
NO
DEPENDENCE
ON
THE
VERTICAL
AXIS
IN
BETWEEN
EACH
OF
THESE
UPDATES
THE
POSTERIOR
MEAN
Μ
AND
VARIANCE
Σ
SHOULD
BE
RECALCULATED
IN
PRACTICE
WE
CHOOSE
A
VERY
SMALL
VALUE
FOR
THE
DEGREES
OF
FREEDOM
Ν
TO
ENCOURAGE
SPARSENESS
WE
MAY
ALSO
RESTRICT
THE
MAXIMUM
POSSIBLE
VALUES
OF
THE
HIDDEN
VARIABLES
HI
TO
ENSURE
NUMERICAL
STABILITY
AT
THE
END
OF
THE
TRAINING
ALL
DIMENSIONS
OF
Φ
WHERE
THE
HIDDEN
VARIABLE
HD
IS
LARGE
SAY
ARE
DISCARDED
FIGURE
SHOWS
AN
EXAMPLE
FIT
TO
SOME
TWO
DIMENSIONAL
DATA
THE
SPARSE
SOLUTION
DEPENDS
ONLY
ON
ONE
OF
THE
TWO
POSSIBLE
DIRECTIONS
AND
SO
IS
TWICE
AS
EFFICIENT
IN
PRINCIPLE
A
NONLINEAR
VERSION
OF
THIS
ALGORITHM
CAN
BE
GENERATED
BY
TRANS
FORMING
THE
INPUT
DATA
X
TO
CREATE
THE
VECTOR
Z
F
X
HOWEVER
IF
THE
TRANSFORMED
DATA
Z
IS
VERY
HIGH
DIMENSIONAL
WE
WILL
NEED
CORRESPONDINGLY
MORE
HIDDEN
VARI
ABLES
HD
TO
COPE
WITH
THESE
DIMENSIONS
OBVIOUSLY
THIS
IDEA
WILL
NOT
TRANSFER
TO
KERNEL
FUNCTIONS
WHERE
THE
DIMENSIONALITY
OF
THE
TRANSFORMED
DATA
COULD
BE
INFINITE
TO
RESOLVE
THIS
PROBLEM
WE
WILL
DEVELOP
THE
RELEVANCE
VECTOR
MACHINE
THIS
MODEL
ALSO
IMPOSES
SPARSITY
BUT
IT
DOES
SO
IN
A
WAY
THAT
MAKES
THE
FINAL
PREDICTION
DEPEND
ONLY
ON
A
SPARSE
SUBSET
OF
THE
TRAINING
DATA
RATHER
THAN
A
SPARSE
SUBSET
OF
THE
OBSERVED
DIMENSIONS
BEFORE
WE
CAN
INVESTIGATE
THIS
MODEL
WE
MUST
DEVELOP
A
VERSION
OF
LINEAR
REGRESSION
WHERE
THERE
IS
ONE
PARAMETER
PER
DATA
EXAMPLE
RATHER
THAN
ONE
PER
OBSERVED
DIMENSION
THIS
MODEL
IS
KNOWN
AS
DUAL
LINEAR
REGRESSION
DUAL
LINEAR
REGRESSION
DUAL
LINEAR
REGRESSION
IN
THE
STANDARD
LINEAR
REGRESSION
MODEL
THE
PARAMETER
VECTOR
Φ
CONTAINS
D
ENTRIES
CORRESPONDING
TO
EACH
OF
THE
D
DIMENSIONS
OF
THE
POSSIBLY
TRANSFORMED
INPUT
DATA
IN
THE
DUAL
FORMULATION
WE
RE
PARAMETERIZE
THE
MODEL
IN
TERMS
OF
A
VECTOR
Ψ
WHICH
HAS
I
ENTRIES
WHERE
I
IS
THE
NUMBER
OF
TRAINING
EXAMPLES
THIS
IS
MORE
EFFICIENT
IN
SITUATIONS
WHERE
WE
ARE
TRAINING
A
MODEL
WHERE
THE
INPUT
DATA
ARE
HIGH
DIMENSIONAL
BUT
THE
NUMBER
OF
EXAMPLES
IS
SMALL
I
D
AND
LEADS
TO
OTHER
INTERESTING
MODELS
SUCH
AS
RELEVANCE
VECTOR
REGRESSION
DUAL
MODEL
IN
THE
DUAL
MODEL
WE
RETAIN
THE
ORIGINAL
LINEAR
DEPENDENCE
OF
THE
PREDICTION
W
ON
THE
INPUT
DATA
X
SO
THAT
PROBLEM
P
R
WI
XI
NORMX
ΦT
XI
HOWEVER
WE
NOW
REPRESENT
THE
SLOPE
PARAMETERS
Φ
AS
A
WEIGHTED
SUM
OF
THE
OB
SERVED
DATA
POINTS
SO
THAT
Φ
XΨ
WHERE
Ψ
IS
A
I
VECTOR
REPRESENTING
THE
WEIGHTS
FIGURE
WE
TERM
THIS
THE
DUAL
PARAMETERIZATION
NOTICE
THAT
IF
THERE
ARE
FEWER
DATA
EXAMPLES
THAN
DATA
DIMENSIONS
THEN
THERE
WILL
BE
FEWER
UNKNOWNS
HERE
THAN
IN
THE
STANDARD
LINEAR
REGRESSION
MODEL
AND
HENCE
LEARNING
AND
INFERENCE
WILL
BE
MORE
EFFICIENT
NOTE
THAT
THE
TERM
DUAL
IS
HEAVILY
OVERLOADED
IN
COMPUTER
SCIENCE
AND
THE
READER
SHOULD
BE
CAREFUL
NOT
TO
CONFUSE
THIS
USE
WITH
ITS
OTHER
MEANINGS
IF
THE
DATA
DIMENSIONALITY
D
IS
LESS
THAN
THE
NUMBER
OF
EXAMPLES
I
THEN
WE
CAN
FIND
PARAMETERS
Ψ
TO
REPRESENT
ANY
GRADIENT
VECTOR
Φ
HOWEVER
IF
D
I
OFTEN
TRUE
IN
VISION
WHERE
MEASUREMENTS
CAN
BE
HIGH
DIMENSIONAL
THEN
THE
VECTOR
XΨ
CAN
ONLY
SPAN
A
SUBSPACE
OF
THE
POSSIBLE
GRADIENT
VECTORS
HOWEVER
THIS
IS
NOT
A
PROBLEM
IF
THERE
WAS
NO
VARIATION
IN
THE
DATA
X
IN
A
GIVEN
DIRECTION
IN
SPACE
THEN
THE
GRADIENT
ALONG
THAT
AXIS
SHOULD
BE
ZERO
ANYWAY
SINCE
WE
HAVE
NO
INFORMATION
ABOUT
HOW
THE
WORLD
STATE
W
VARIES
IN
THIS
DIRECTION
MAKING
THE
SUBSTITUTION
FROM
EQUATION
THE
REGRESSION
MODEL
BECOMES
P
R
WI
XI
Θ
NORMXI
ΨT
XT
XI
OR
WRITING
ALL
OF
THE
DATA
LIKELIHOODS
IN
ONE
TERM
P
R
W
X
Θ
NORMW
XT
XΨ
WHERE
THE
PARAMETERS
OF
THE
MODEL
ARE
Θ
Ψ
WE
NOW
CONSIDER
HOW
TO
LEARN
THIS
MODEL
USING
BOTH
THE
MAXIMUM
LIKELIHOOD
AND
BAYESIAN
APPROACHES
FIGURE
DUAL
VARIABLES
TWO
DI
MENSIONAL
TRAINING
DATA
XI
I
AND
ASSOCIATED
WORLD
STATE
WI
I
INDI
CATED
BY
MARKER
COLOR
THE
LINEAR
RE
GRESSION
PARAMETER
Φ
DETERMINES
THE
DIRECTION
IN
THIS
SPACE
IN
WHICH
W
CHANGES
MOST
QUICKLY
WE
CAN
ALTER
NATELY
REPRESENT
THE
GRADIENT
DIREC
TION
AS
A
WEIGHTED
SUM
OF
DATA
EX
AMPLES
HERE
WE
SHOW
THE
CASE
Φ
IN
PRACTICAL
PROBLEMS
THE
DATA
DIMENSIONALITY
D
IS
GREATER
THAN
THE
NUMBER
OF
EXAMPLES
I
SO
WE
TAKE
A
WEIGHTED
SUM
Φ
XΨ
OF
ALL
OF
THE
DATA
POINTS
THIS
IS
THE
DUAL
PARAMETERIZATION
MAXIMUM
LIKELIHOOD
SOLUTION
WE
APPLY
THE
MAXIMUM
LIKELIHOOD
METHOD
TO
ESTIMATE
THE
PARAMETERS
Ψ
IN
THE
DUAL
FORMULATION
TO
THIS
END
WE
MAXIMIZE
THE
LOGARITHM
OF
THE
LIKELIHOOD
EQUATION
ALGORITHM
WITH
RESPECT
TO
Ψ
AND
Σ
SO
THAT
Ψˆ
ARGMAX
Ψ
I
LOG
I
LOG
Σ
W
XT
XΨ
T
W
XT
XΨ
PROBLEM
TO
MAXIMIZE
THIS
EXPRESSION
WE
TAKE
DERIVATIVES
WITH
RESPECT
TO
Ψ
AND
EQUATE
THE
RESULTING
EXPRESSIONS
TO
ZERO
AND
SOLVE
TO
FIND
Ψˆ
XT
X
W
XT
XΨ
T
W
XT
XΨ
I
THIS
SOLUTION
IS
ACTUALLY
THE
SAME
AS
FOR
THE
ORIGINAL
LINEAR
REGRESSION
MODEL
EQUA
TIONS
FOR
EXAMPLE
IF
WE
SUBSTITUTE
IN
THE
DEFINITION
Φ
XΨ
Φˆ
XΨˆ
X
XT
X
XXT
X
XT
X
XXT
WHICH
WAS
THE
ORIGINAL
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
Φ
BAYESIAN
SOLUTION
WE
NOW
EXPLORE
THE
BAYESIAN
APPROACH
TO
THE
DUAL
REGRESSION
MODEL
AS
BEFORE
WE
TREAT
THE
DUAL
PARAMETERS
Ψ
AS
UNCERTAIN
ASSUMING
THAT
THE
NOISE
IS
KNOWN
ONCE
AGAIN
WE
WILL
ESTIMATE
THIS
SEPARATELY
USING
MAXIMUM
LIKELIHOOD
THE
GOAL
OF
THE
BAYESIAN
APPROACH
IS
TO
COMPUTE
THE
POSTERIOR
DISTRIBUTION
P
R
Ψ
X
W
OVER
POSSIBLE
VALUES
OF
THE
PARAMETERS
Ψ
GIVEN
THE
TRAINING
DATA
PAIRS
XI
WI
I
WE
START
BY
DEFINING
A
PRIOR
P
R
Ψ
OVER
THE
PARAMETERS
SINCE
WE
HAVE
NO
PARTICULAR
PRIOR
KNOWLEDGE
WE
CHOOSE
A
NORMAL
DISTRIBUTION
WITH
A
LARGE
SPHERICAL
COVARIANCE
P
R
Ψ
NORMΨ
WE
USE
BAYES
RULE
TO
COMPUTE
THE
POSTERIOR
DISTRIBUTION
OVER
THE
PARAMETERS
P
R
Ψ
X
W
P
R
X
W
Ψ
P
R
Ψ
P
R
X
W
WHICH
CAN
BE
SHOWN
TO
YIELD
THE
CLOSED
FORM
EXPRESSION
WHERE
P
R
Ψ
X
W
NORMΨ
A
XW
A
A
XT
XXT
X
I
P
TO
COMPUTE
THE
PREDICTIVE
DISTRIBUTION
P
R
W
X
WE
TAKE
AN
INFINITE
WEIGHTED
SUM
OVER
THE
PREDICTIONS
OF
THE
MODEL
ASSOCIATED
WITH
EACH
POSSIBLE
VALUE
OF
THE
PARAMETERS
Ψ
P
R
W
X
X
W
P
R
W
X
Ψ
P
R
Ψ
X
W
DΨ
NORMW
X
XA
X
XW
X
XA
X
X
Σ
TO
GENERALIZE
THE
MODEL
TO
THE
NONLINEAR
CASE
WE
REPLACE
THE
TRAINING
DATA
X
XI
WITH
THE
TRANSFORMED
DATA
Z
ZI
AND
THE
TEST
DATA
X
WITH
THE
TRANSFORMED
TEST
DATA
Z
SINCE
THE
RESULTING
EXPRESSION
DEPENDS
ONLY
ON
INNER
PRODUCTS
OF
THE
FORM
ZT
Z
AND
ZT
Z
IT
IS
DIRECTLY
AMENABLE
TO
KERNELIZATION
AS
FOR
THE
ORIGINAL
REGRESSION
MODEL
THE
VARIANCE
PARAMETER
CAN
BE
ESTIMATED
BY
MAXIMIZING
THE
LOG
OF
THE
MARGINAL
LIKELIHOOD
WHICH
IS
GIVEN
BY
P
R
W
X
NORMW
XXT
X
RELEVANCE
VECTOR
REGRESSION
HAVING
DEVELOPED
THE
DUAL
APPROACH
TO
LINEAR
REGRESSION
WE
ARE
NOW
IN
A
POSITION
TO
DEVELOP
A
MODEL
THAT
DEPENDS
ONLY
SPARSELY
ON
THE
TRAINING
DATA
TO
THIS
END
ALGORITHM
ALGORITHM
FIGURE
RELEVANCE
VECTOR
REGRES
SION
A
PRIOR
APPLYING
SPARSENESS
IS
APPLIED
TO
THE
DUAL
PARAMETERS
THIS
MEANS
THAT
THE
FINAL
CLASSIFIER
ONLY
DE
PENDS
ON
A
SUBSET
OF
THE
DATA
POINTS
INDICATED
BY
THE
SIX
LARGER
POINTS
THE
RESULTING
REGRESSION
FUNCTION
IS
CONSIDERABLY
FASTER
TO
EVALUATE
AND
TENDS
TO
BE
SIMPLER
THIS
MEANS
IT
IS
LESS
LIKELY
TO
OVERFIT
TO
RANDOM
STATIS
TICAL
FLUCTUATIONS
IN
THE
TRAINING
DATA
AND
GENERALIZES
BETTER
TO
NEW
DATA
WE
IMPOSE
A
PENALTY
FOR
EVERY
NON
ZERO
WEIGHTED
TRAINING
EXAMPLE
WE
ACHIEVE
THIS
BY
REPLACING
THE
NORMAL
PRIOR
OVER
THE
DUAL
PARAMETERS
Ψ
WITH
A
PRODUCT
OF
ONE
DIMENSIONAL
T
DISTRIBUTIONS
SO
THAT
P
R
Ψ
I
STUDΨI
Ν
I
THIS
MODEL
IS
KNOWN
AS
RELEVANCE
VECTOR
REGRESSION
THIS
SITUATION
IS
EXACTLY
ANALOGOUS
TO
THE
SPARSE
LINEAR
REGRESSION
MODEL
SEC
TION
EXCEPT
THAT
NOW
WE
ARE
WORKING
WITH
DUAL
VARIABLES
AS
FOR
THE
SPARSE
MODEL
IT
IS
NOT
POSSIBLE
TO
MARGINALIZE
OVER
THE
VARIABLES
Ψ
WITH
THE
T
DISTRIBUTED
PRIOR
OUR
APPROACH
WILL
AGAIN
BE
TO
APPROXIMATE
THE
T
DISTRIBUTIONS
BY
MAXI
MIZING
WITH
RESPECT
TO
THEIR
HIDDEN
VARIABLES
RATHER
THAN
MARGINALIZING
OVER
THEM
EQUATION
BY
ANALOGY
WITH
SECTION
THE
MARGINAL
LIKELIHOOD
BECOMES
P
R
W
X
MAX
RNORMW
XT
XH
X
DTT
GAMHD
Ν
Ν
WHERE
THE
MATRIX
H
CONTAINS
THE
HIDDEN
VARIABLES
HI
I
ASSOCIATED
WITH
THE
T
DISTRIBUTION
ON
ITS
DIAGONAL
AND
ZEROS
ELSEWHERE
NOTICE
THAT
THIS
EXPRESSION
IS
SIMILAR
TO
EQUATION
EXCEPT
THAT
INSTEAD
OF
EVERY
DATA
POINT
HAVING
THE
SAME
PRIOR
VARIANCE
THEY
NOW
HAVE
INDIVIDUAL
VARIANCES
THAT
ARE
DETERMINED
BY
THE
HIDDEN
VARIABLES
THAT
FORM
THE
ELEMENTS
OF
THE
DIAGONAL
MATRIX
H
IN
RELEVANCE
VECTOR
REGRESSION
WE
ALTERNATELY
I
OPTIMIZE
THE
MARGINAL
LIKELI
HOOD
WITH
RESPECT
TO
THE
HIDDEN
VARIABLES
AND
II
OPTIMIZE
THE
MARGINAL
LIKELIHOOD
WITH
RESPECT
TO
THE
VARIANCE
PARAMETER
USING
HNEW
HIΣII
Ν
AND
I
Ν
NEW
I
I
HIΣII
W
XT
XΜ
T
W
XT
XΜ
IN
BETWEEN
EACH
STEP
WE
UPDATE
THE
MEAN
Μ
AND
VARIANCE
Σ
OF
THE
POSTERIOR
DIS
TRIBUTION
WHERE
A
IS
DEFINED
AS
Μ
A
XW
Σ
A
A
XT
XXT
X
H
AT
THE
END
OF
THE
TRAINING
ALL
DATA
EXAMPLES
WHERE
THE
HIDDEN
VARIABLE
HI
IS
LARGE
SAY
ARE
DISCARDED
AS
HERE
THE
COEFFICIENTS
ΨI
WILL
BE
VERY
SMALL
AND
CONTRIBUTE
ALMOST
NOTHING
TO
THE
SOLUTION
SINCE
THIS
ALGORITHM
DEPENDS
ONLY
ON
INNER
PRODUCTS
A
NONLINEAR
VERSION
OF
THIS
ALGORITHM
CAN
BE
GENERATED
BY
REPLACING
THE
INNER
PRODUCTS
WITH
A
KERNEL
FUNCTION
K
XI
XJ
IF
THE
KERNEL
ITSELF
CONTAINS
PARAMETERS
THESE
MAY
BE
ALSO
BE
MANIPULATED
TO
IMPROVE
THE
LOG
MARGINAL
VARIANCE
DURING
THE
FITTING
PROCEDURE
FIGURE
SHOWS
AN
EXAMPLE
FIT
USING
THE
RBF
KERNEL
THE
FINAL
SOLUTION
NOW
ONLY
DEPENDS
ON
SIX
DATA
POINTS
BUT
NONETHELESS
STILL
CAPTURES
THE
IMPORTANT
ASPECTS
OF
THE
DATA
REGRESSION
TO
MULTIVARIATE
DATA
THROUGHOUT
THIS
CHAPTER
WE
HAVE
DISCUSSED
PREDICTING
A
SCALAR
VALUE
WI
FROM
MUL
TIVARIATE
DATA
XI
IN
REAL
WORLD
SITUATIONS
SUCH
AS
THE
POSE
REGRESSION
PROBLEM
THE
WORLD
STATES
WI
ARE
MULTIVARIATE
IT
IS
TRIVIAL
TO
EXTEND
THE
MODELS
IN
THIS
CHAPTER
WE
SIMPLY
CONSTRUCT
A
SEPARATE
REGRESSOR
FOR
EACH
DIMENSION
THE
EXCEPTION
TO
THIS
RULE
IS
THE
RELEVANCE
VECTOR
MACHINE
HERE
WE
MIGHT
WANT
TO
ENSURE
THAT
THE
SPARSE
STRUCTURE
IS
COMMON
FOR
EACH
OF
THESE
MODELS
SO
THE
EFFICIENCY
GAINS
ARE
RETAINED
TO
THIS
END
WE
MODIFY
THE
MODEL
SO
THAT
A
SINGLE
SET
OF
HIDDEN
VARIABLES
IS
SHARED
ACROSS
THE
MODEL
FOR
EACH
WORLD
STATE
DIMENSION
APPLICATIONS
REGRESSION
METHODS
ARE
USED
LESS
FREQUENTLY
IN
VISION
THAN
CLASSIFICATION
BUT
NONETHE
LESS
THERE
ARE
MANY
USEFUL
APPLICATIONS
THE
MAJORITY
OF
THESE
INVOLVE
ESTIMATING
THE
POSITION
OR
POSE
OF
OBJECTS
SINCE
THE
UNKNOWNS
IN
SUCH
PROBLEMS
ARE
NATURALLY
TREATED
AS
CONTINUOUS
FIGURE
BODY
POSE
ESTIMATION
RE
SULTS
A
SILHOUETTES
OF
WALKING
AVATAR
B
ESTIMATED
BODY
POSE
BASED
ON
SILHOUETTE
USING
A
RELEVANCE
VECTOR
MACHINE
THE
RVM
USED
RA
DIAL
BASIS
FUNCTIONS
AND
CONSTRUCTED
ITS
FINAL
SOLUTION
FROM
JUST
OF
OF
THE
TRAINING
EXAMPLES
IT
PRODUCED
A
MEAN
TEST
ERROR
OF
AVERAGED
OVER
THE
THREE
JOINT
AN
GLES
FOR
THE
MAIN
BODY
PARTS
AND
THE
OVERALL
COMPASS
DIRECTION
OF
THE
MODEL
ADAPTED
FROM
AGARWAL
TRIGGS
HUMAN
BODY
POSE
ESTIMATION
AGARWAL
TRIGGS
DEVELOPED
A
SYSTEM
BASED
ON
THE
RELEVANCE
VECTOR
MA
CHINE
TO
PREDICT
BODY
POSE
W
FROM
SILHOUETTE
DATA
X
TO
ENCODE
THE
SILHOUETTE
THEY
COMPUTED
A
DIMENSIONAL
SHAPE
CONTEXT
FEATURE
SECTION
AT
EACH
OF
POINTS
ON
THE
BOUNDARY
OF
THE
OBJECT
TO
REDUCE
THE
DATA
DIMENSION
ALITY
THEY
COMPUTED
THE
SIMILARITY
OF
EACH
SHAPE
CONTEXT
FEATURE
TO
EACH
OF
DIFFERENT
PROTOTYPES
FINALLY
THEY
FORMED
A
DIMENSIONAL
HISTOGRAM
CONTAINING
THE
AGGREGATED
DIMENSIONAL
SIMILARITIES
FOR
ALL
OF
THE
BOUNDARY
POINTS
THIS
HISTOGRAM
WAS
USED
AS
THE
DATA
VECTOR
X
THE
BODY
POSE
WAS
ENCODED
BY
THE
JOINT
ANGLES
OF
EACH
OF
THE
MAJOR
BODY
JOINTS
AND
THE
OVERALL
AZIMUTH
COMPASS
HEADING
OF
THE
BODY
THE
RESULTING
DIMENSIONAL
VECTOR
WAS
USED
AS
THE
WORLD
STATE
W
A
RELEVANCE
VECTOR
MACHINE
WAS
TRAINED
USING
DATA
VECTORS
XI
EXTRACTED
FROM
SILHOUETTES
THAT
WERE
RENDERED
USING
THE
COMMERCIAL
PROGRAM
POSER
FROM
KNOWN
MOTION
CAPTURE
DATA
WI
USING
A
RADIAL
BASIS
FUNCTION
KERNEL
THE
RELEVANCE
VECTOR
MACHINE
BASED
ITS
SOLUTION
ON
JUST
OF
THESE
TRAINING
EXAMPLES
THE
BODY
POSE
ANGLES
OF
TEST
DATA
COULD
BE
PREDICTED
TO
WITHIN
AN
AVERAGE
OF
ERROR
FIGURE
THEY
ALSO
DEMONSTRATED
THAT
THE
SYSTEM
WORKED
REASONABLY
WELL
ON
SILHOUETTES
FROM
REAL
IMAGES
FIGURE
SILHOUETTE
INFORMATION
IS
BY
ITS
NATURE
AMBIGUOUS
IT
IS
VERY
HARD
TO
TELL
WHICH
LEG
IS
IN
FRONT
OF
THE
OTHER
BASED
ON
A
SINGLE
SILHOUETTE
AGARWAL
TRIGGS
PARTIALLY
CIRCUMVENTED
THIS
SYSTEM
BY
TRACKING
THE
BODY
POSE
WI
THROUGH
A
VIDEO
SEQUENCE
ESSENTIALLY
THE
AMBIGUITY
AT
A
GIVEN
FRAME
IS
RESOLVED
BY
ENCOURAGING
THE
ESTIMATED
POSE
IN
ADJACENT
FRAMES
IN
THE
SEQUENCE
TO
BE
SIMILAR
INFORMATION
FROM
FRAMES
WHERE
THE
POSE
VECTOR
IS
WELL
DEFINED
IS
PROPAGATED
THROUGH
THE
SEQUENCE
TO
RESOLVE
AMBIGUITIES
IN
OTHER
PARTS
SEE
CHAPTER
HOWEVER
THE
AMBIGUITY
OF
SILHOUETTE
DATA
IS
AN
ARGUMENT
FOR
NOT
USING
THIS
TYPE
OF
CLASSIFIER
THE
REGRESSION
MODELS
IN
THIS
CHAPTER
ARE
DESIGNED
TO
GIVE
A
UNIMODAL
NORMAL
OUTPUT
TO
EFFECTIVELY
CLASSIFY
SINGLE
FRAMES
OF
DATA
WE
SHOULD
USE
A
REGRESSION
METHOD
THAT
PRODUCES
A
MULTI
MODAL
PREDICTION
THAT
CAN
EFFECTIVELY
DESCRIBE
THE
AMBIGUITY
APPLICATIONS
FIGURE
TRACKING
USING
DISPLACEMENT
EXPERTS
THE
GOAL
OF
THE
SYSTEM
IS
TO
PREDICT
A
DISPLACEMENT
VECTOR
INDICATING
THE
MOTION
OF
THE
OBJECT
BASED
ON
THE
PIXEL
DATA
AT
ITS
LAST
KNOWN
POSITION
A
THE
SYSTEM
IS
TRAINED
BY
PERTURBING
THE
BOUNDING
BOX
AROUND
THE
OBJECT
TO
SIMULATE
THE
MOTION
OF
THE
OBJECT
B
THE
SYSTEM
SUCCESSFULLY
TRACKS
A
FACE
EVEN
IN
THE
PRESENCE
C
OF
PARTIAL
OCCLUSIONS
D
IF
THE
SYSTEM
IS
TRAINED
USING
GRADIENT
VECTORS
RATHER
THAN
RAW
PIXEL
VALUES
IT
IS
ALSO
QUITE
ROBUST
TO
CHANGES
IN
ILLUMINATION
ADAPTED
FROM
WILLIAMS
ET
AL
QC
IEEE
DISPLACEMENT
EXPERTS
REGRESSION
MODELS
ARE
ALSO
USED
TO
FORM
DISPLACEMENT
EXPERTS
IN
TRACKING
APPLICA
TIONS
THE
GOAL
IS
TO
TAKE
A
REGION
OF
THE
IMAGE
X
AND
RETURN
A
SET
OF
NUMBERS
W
THAT
INDICATE
THE
CHANGE
IN
POSITION
OF
AN
OBJECT
RELATIVE
TO
THE
WINDOW
THE
WORLD
STATE
W
MIGHT
SIMPLY
CONTAIN
THE
HORIZONTAL
AND
VERTICAL
TRANSLATION
VECTORS
OR
MIGHT
CONTAIN
PARAMETERS
OF
A
MORE
COMPLEX
TRANSFORMATION
CHAPTER
FOR
SIMPLICITY
WE
WILL
DESCRIBE
THE
FORMER
SITUATION
TRAINING
DATA
ARE
EXTRACTED
AS
FOLLOWS
A
BOUNDING
BOX
AROUND
OF
THE
OBJECT
OF
INTEREST
CAR
FACE
ETC
IS
IDENTIFIED
IN
A
NUMBER
OF
FRAMES
FOR
EACH
OF
THESE
FRAMES
THE
BOUNDING
BOX
IS
PERTURBED
BY
A
PRE
DETERMINED
SET
OF
TRANSLATION
VECTORS
TO
SIMULATE
THE
OBJECT
MOVING
IN
THE
OPPOSITE
DIRECTION
FIGURE
IN
THIS
WAY
WE
ASSOCIATE
A
TRANSLATION
VECTOR
WI
WITH
EACH
PERTURBATION
THE
DATA
FROM
THE
PERTURBED
BOUNDING
BOX
ARE
EXTRACTED
RESIZED
TO
A
STANDARD
SHAPE
AND
HISTOGRAM
EQUALIZED
SECTION
TO
INDUCE
A
DEGREE
OF
INVARIANCE
TO
ILLUMINATION
CHANGES
THE
RESULTING
VALUES
ARE
THEN
CONCATENATED
TO
FORM
THE
DATA
VECTOR
XI
WILLIAMS
ET
AL
DESCRIBE
A
SYSTEM
OF
THIS
KIND
IN
WHICH
THE
ELEMENTS
OF
W
WERE
LEARNED
BY
A
SET
OF
INDEPENDENT
RELEVANCE
VECTOR
MACHINES
THEY
INITIALIZE
THE
POSITION
OF
THE
OBJECT
USING
A
STANDARD
OBJECT
DETECTOR
SEE
CHAPTER
IN
THE
SUBSEQUENT
FRAME
THEY
COMPUTE
A
PREDICTION
FOR
THE
DISPLACEMENT
VECTOR
W
USING
THE
RELEVANCE
VECTOR
MACHINES
ON
THE
DATA
X
FROM
THE
ORIGINAL
POSITION
THIS
PREDICTION
IS
COMBINED
IN
A
KALMAN
FILTER
LIKE
SYSTEM
CHAPTER
THAT
IMPOSES
PRIOR
KNOWLEDGE
ABOUT
THE
CONTINUITY
OF
THE
MOTION
TO
CREATE
A
ROBUST
METHOD
FOR
TRACKING
KNOWN
OBJECTS
IN
SCENES
FIGURE
D
SHOW
A
SERIES
OF
TRACKING
RESULTS
FROM
THIS
SYSTEM
REGRESSION
MODELS
DISCUSSION
THE
GOAL
OF
THIS
CHAPTER
WAS
TO
INTRODUCE
DISCRIMINATIVE
APPROACHES
TO
REGRESSION
THESE
HAVE
NICHE
APPLICATIONS
IN
VISION
RELATED
TO
PREDICTING
THE
POSE
AND
POSITION
OF
OBJECTS
HOWEVER
THE
MAIN
REASON
FOR
STUDYING
THESE
MODELS
IS
THAT
THE
CONCEPTS
INVOLVED
SPARSITY
DUAL
VARIABLES
KERNELIZATION
ARE
ALL
IMPORTANT
FOR
DISCRIMINATIVE
CLASSIFICATION
METHODS
THESE
ARE
VERY
WIDELY
USED
BUT
ARE
RATHER
MORE
COMPLEX
AND
ARE
DISCUSSED
IN
THE
FOLLOWING
CHAPTER
NOTES
NOTES
REGRESSION
METHODS
RASMUSSEN
WILLIAMS
IS
A
COMPREHENSIVE
RESOURCE
ON
GAUSSIAN
PROCESSES
THE
RELEVANCE
VECTOR
MACHINE
WAS
FIRST
INTRODUCED
BY
TIPPING
SEVERAL
INNOVATIONS
WITHIN
THE
VISION
COMMUNITY
HAVE
EXTENDED
THESE
MODELS
WILLIAMS
ET
AL
PRESENTED
A
SEMI
SUPERVISED
METHOD
FOR
GAUSSIAN
PROCESS
REGRESSION
IN
WHICH
THE
WORLD
STATE
W
IS
ONLY
KNOWN
FOR
A
SUBSET
OF
EXAMPLES
RANGANATHAN
YANG
PRESENTED
AN
EFFICIENT
ALGORITHM
FOR
ONLINE
LEARNING
OF
GAUSSIAN
PROCESSES
WHEN
THE
KERNEL
MATRIX
IS
SPARSE
THAYANANTHAN
ET
AL
DEVELOPED
A
MULTIVARIATE
VERSION
OF
THE
RELEVANCE
VECTOR
MACHINE
APPLICATIONS
APPLICATIONS
OF
REGRESSION
IN
VISION
INCLUDE
HEAD
POSE
ESTIMATION
WILLIAMS
ET
AL
RANGANATHAN
YANG
RAE
RITTER
BODY
TRACKING
WILLIAMS
ET
AL
AGARWAL
TRIGGS
THAYANANTHAN
ET
AL
EYE
TRACKING
WILLIAMS
ET
AL
AND
TRACKING
OF
OTHER
OBJECTS
WILLIAMS
ET
AL
RANGANATHAN
YANG
MULTIMODAL
POSTERIOR
ONE
OF
THE
DRAWBACKS
OF
USING
THE
METHODS
IN
THIS
CHAPTER
IS
THAT
THEY
ALWAYS
PRODUCE
A
UNIMODAL
NORMALLY
DISTRIBUTED
POSTERIOR
FOR
SOME
PROBLEMS
E
G
BODY
POSE
ESTIMATION
THE
POSTERIOR
PROBABILITY
OVER
THE
WORLD
STATE
MAY
BE
GEN
UINELY
MULTIMODAL
THERE
IS
MORE
THAN
ONE
INTERPRETATION
OF
THE
DATA
ONE
APPROACH
TO
THIS
IS
TO
BUILD
MANY
REGRESSORS
THAT
RELATE
SMALL
PARTS
OF
THE
WORLD
STATE
TO
THE
DATA
THAYANANTHAN
ET
AL
ALTERNATIVELY
IT
IS
POSSIBLE
TO
USE
GENERATIVE
REGRESSION
METH
ODS
IN
WHICH
EITHER
THE
JOINT
DENSITY
IS
MODELED
DIRECTLY
NAVARATNAM
ET
AL
OR
THE
LIKELIHOOD
AND
PRIOR
ARE
MODELED
SEPARATELY
URTASUN
ET
AL
IN
THESE
METHODS
THE
POSTERIOR
DISTRIBUTION
OVER
THE
WORLD
IS
MULTI
MODAL
HOWEVER
THE
COST
OF
THIS
IS
THAT
IT
IS
INTRACTABLE
TO
COMPUTE
EXACTLY
AND
SO
WE
MUST
RELY
ON
OPTIMIZATION
TECHNIQUES
TO
FIND
ITS
MODES
PROBLEMS
PROBLEM
CONSIDER
A
REGRESSION
PROBLEM
WHERE
THE
WORLD
STATE
W
IS
KNOWN
TO
BE
POSITIVE
TO
COPE
WITH
THIS
WE
COULD
CONSTRUCT
A
REGRESSION
MODEL
IN
WHICH
THE
WORLD
STATE
IS
MODELED
AS
A
GAMMA
DISTRIBUTION
WE
COULD
CONSTRAIN
BOTH
PARAMETERS
Α
Β
OF
THE
GAMMA
DISTRIBUTION
TO
BE
THE
SAME
SO
THAT
Α
Β
AND
MAKE
THEM
A
FUNCTION
OF
THE
DATA
X
DESCRIBE
A
MAXIMUM
LIKELIHOOD
APPROACH
TO
FITTING
THIS
MODEL
PROBLEM
CONSIDER
A
ROBUST
REGRESSION
MODEL
BASED
ON
THE
T
DISTRIBUTION
RATHER
THAN
THE
NORMAL
DISTRIBUTION
DEFINE
THIS
MODEL
PRECISELY
IN
MATHEMATICAL
TERMS
AND
SKETCH
OUT
A
MAXIMUM
LIKELIHOOD
APPROACH
TO
FITTING
THE
PARAMETERS
PROBLEM
PROVE
THAT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
FOR
THE
GRADIENT
IN
THE
LINEAR
REGRESSION
MODEL
IS
Φˆ
XXT
PROBLEM
FOR
THE
BAYESIAN
LINEAR
REGRESSION
MODEL
SECTION
SHOW
THAT
THE
POSTERIOR
DISTRIBUTION
OVER
THE
PARAMETERS
Φ
IS
GIVEN
BY
P
R
Φ
X
W
NORMΦ
A
A
WHERE
REGRESSION
MODELS
A
XXT
I
P
PROBLEM
FOR
THE
BAYESIAN
LINEAR
REGRESSION
MODEL
SECTION
SHOW
THAT
THE
PRE
DICTIVE
DISTRIBUTION
FOR
A
NEW
DATA
EXAMPLE
X
IS
GIVEN
BY
P
R
W
X
X
W
NORMW
X
T
A
X
T
A
PROBLEM
USE
THE
MATRIX
INVERSION
LEMMA
APPENDIX
C
TO
SHOW
THAT
A
XXT
ID
XT
X
II
XT
P
P
P
P
PROBLEM
COMPUTE
THE
DERIVATIVE
OF
THE
MARGINAL
LIKELIHOOD
P
R
W
X
NORMW
X
WITH
RESPECT
TO
THE
VARIANCE
PARAMETER
PROBLEM
COMPUTE
A
CLOSED
FORM
EXPRESSION
FOR
THE
APPROXIMATED
T
DISTRIBUTION
USED
TO
IMPOSE
SPARSENESS
Q
H
MAX
NORMΦ
H
GAMH
Ν
Ν
H
PLOT
THIS
FUNCTION
FOR
Ν
PLOT
THE
FUNCTION
Q
Q
FOR
Ν
PROBLEM
DESCRIBE
MAXIMUM
LIKELIHOOD
LEARNING
AND
INFERENCE
ALGORITHMS
FOR
A
NON
LINEAR
REGRESSION
MODEL
BASED
ON
POLYNOMIALS
WHERE
P
R
W
X
NORMW
PROBLEM
I
WISH
TO
LEARN
A
LINEAR
REGRESSION
MODEL
IN
WHICH
I
PREDICT
THE
WORLD
W
FROM
I
EXAMPLES
OF
D
DATA
X
USING
THE
MAXIMUM
LIKELIHOOD
METHOD
IF
I
D
IS
IT
MORE
EFFICIENT
TO
USE
THE
DUAL
PARAMETERIZATION
OR
THE
ORIGINAL
LINEAR
REGRESSION
MODEL
PROBLEM
SHOW
THAT
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
FOR
THE
PARAMETERS
Ψ
IN
THE
DUAL
LINEAR
REGRESSION
MODEL
SECTION
IS
GIVEN
BY
Ψˆ
XT
X
CHAPTER
CLASSIFICATION
MODELS
THIS
CHAPTER
CONCERNS
DISCRIMINATIVE
MODELS
FOR
CLASSIFICATION
THE
GOAL
IS
TO
DI
RECTLY
MODEL
THE
POSTERIOR
PROBABILITY
DISTRIBUTION
P
R
W
X
OVER
A
DISCRETE
WORLD
STATE
W
K
GIVEN
THE
CONTINUOUS
OBSERVED
DATA
VECTOR
X
MODELS
FOR
CLAS
SIFICATION
ARE
VERY
CLOSELY
RELATED
TO
THOSE
FOR
REGRESSION
AND
THE
READER
SHOULD
BE
FAMILIAR
WITH
THE
CONTENTS
OF
CHAPTER
BEFORE
PROCEEDING
TO
MOTIVATE
THE
MODELS
IN
THIS
CHAPTER
WE
WILL
CONSIDER
GENDER
CLASSIFICATION
HERE
WE
OBSERVE
A
RGB
IMAGE
CONTAINING
A
FACE
FIGURE
AND
CONCATENATE
THE
RGB
VALUES
TO
FORM
THE
VECTOR
X
OUR
GOAL
IS
TO
TAKE
THE
VECTOR
X
AND
RETURN
THE
PROBABILITY
DISTRIBUTION
P
R
W
X
OVER
A
LABEL
W
INDICATING
WHETHER
THE
FACE
IS
MALE
W
OR
FEMALE
W
GENDER
CLASSIFICATION
IS
A
BINARY
CLASSIFICATION
TASK
AS
THERE
ARE
ONLY
TWO
POSSIBLE
VALUES
OF
THE
WORLD
STATE
THROUGHOUT
MOST
OF
THIS
CHAPTER
WE
WILL
RESTRICT
OUR
DISCUSSION
TO
BINARY
CLASSIFICATION
WE
DISCUSS
HOW
TO
EXTEND
THESE
MODELS
TO
COPE
WITH
AN
ARBITRARY
NUMBER
OF
CLASSES
IN
SECTION
LOGISTIC
REGRESSION
WE
WILL
START
BY
CONSIDERING
LOGISTIC
REGRESSION
WHICH
DESPITE
ITS
NAME
IS
A
MODEL
THAT
CAN
BE
APPLIED
TO
CLASSIFICATION
LOGISTIC
REGRESSION
FIGURE
IS
A
DISCRIMI
NATIVE
MODEL
WE
SELECT
A
PROBABILITY
DISTRIBUTION
OVER
THE
WORLD
STATE
W
AND
MAKE
ITS
PARAMETERS
CONTINGENT
ON
THE
OBSERVED
DATA
X
SINCE
THE
WORLD
STATE
IS
BINARY
WE
WILL
DESCRIBE
IT
WITH
A
BERNOULLI
DISTRIBUTION
AND
WE
WILL
MAKE
THE
SINGLE
BERNOULLI
PARAMETER
Λ
INDICATING
THE
PROBABILITY
THAT
THE
WORLD
STATE
TAKES
THE
VALUE
W
A
FUNCTION
OF
THE
MEASUREMENTS
X
IN
CONTRAST
TO
THE
REGRESSION
MODEL
WE
CANNOT
SIMPLY
MAKE
THE
PARAMETER
Λ
A
LINEAR
FUNCTION
ΦT
X
OF
THE
MEASUREMENTS
A
LINEAR
FUNCTION
CAN
RETURN
ANY
VALUE
BUT
THE
PARAMETER
Λ
MUST
LIE
BETWEEN
AND
CONSEQUENTLY
WE
FIRST
COMPUTE
THE
LINEAR
FUNCTION
AND
THEN
PASS
THIS
THROUGH
THE
LOGISTIC
SIGMOID
FUNCTION
SIG
THAT
MAPS
THE
RANGE
TO
THE
FINAL
MODEL
IS
HENCE
CLASSIFICATION
MODELS
FIGURE
GENDER
CLASSIFICATION
CON
SIDER
A
PIXEL
IMAGE
OF
A
FACE
WE
CONCATENATE
THE
RGB
VALUES
TO
MAKE
A
DATA
VECTOR
X
THE
GOAL
OF
GENDER
CLASSIFICATION
IS
TO
USE
THE
DATA
X
TO
INFER
A
LABEL
W
IN
DICATING
WHETHER
THE
WINDOW
CONTAINS
A
A
MALE
OR
B
A
FEMALE
FACE
THIS
IS
CHALLENGING
BECAUSE
THE
DIFFERENCES
ARE
SUBTLE
AND
THERE
IS
IMAGE
VARIA
B
TION
DUE
TO
CHANGES
IN
POSE
LIGHTING
AND
EXPRESSION
NOTE
THAT
REAL
SYS
TEMS
WOULD
PREPROCESS
THE
IMAGE
BE
FORE
CLASSIFICATION
BY
REGISTERING
THE
FACES
MORE
CLOSELY
AND
COMPENSATING
IN
SOME
WAY
FOR
LIGHTING
VARIATION
SEE
CHAPTER
P
R
W
Φ
X
BERNW
SIG
A
WHERE
A
IS
TERMED
THE
ACTIVATION
AND
IS
GIVEN
BY
THE
LINEAR
FUNCTION
A
ΦT
X
THE
LOGISTIC
SIGMOID
FUNCTION
SIG
IS
GIVEN
BY
SIG
A
EXP
A
PROBLEM
AS
THE
ACTIVATION
A
TENDS
TO
THIS
FUNCTION
TENDS
TO
ONE
AS
A
TENDS
TO
IT
TENDS
TO
ZERO
WHEN
A
IS
ZERO
THE
LOGISTIC
SIGMOID
FUNCTION
RETURNS
A
VALUE
OF
ONE
HALF
FOR
DATA
X
THE
OVERALL
EFFECT
OF
THIS
TRANSFORMATION
IS
TO
DESCRIBE
A
SIGMOID
CURVE
RELATING
X
TO
Λ
FIGURES
AND
THE
HORIZONTAL
POSITION
OF
THE
SIGMOID
IS
DETERMINED
BY
THE
PLACE
THAT
THE
LINEAR
FUNCTION
A
CROSSES
ZERO
I
E
THE
X
INTERCEPT
AND
THE
STEEPNESS
OF
THE
SIGMOID
DEPENDS
ON
THE
GRADIENT
IN
MORE
THAN
ONE
DIMENSION
THE
RELATIONSHIP
BETWEEN
X
AND
Λ
IS
MORE
COMPLEX
FIGURE
THE
PREDICTED
PARAMETER
Λ
HAS
A
SIGMOID
PROFILE
IN
THE
DIRECTION
OF
THE
GRADIENT
VECTOR
Φ
BUT
IS
CONSTANT
IN
ALL
PERPENDICULAR
DIRECTIONS
THIS
INDUCES
A
LINEAR
DECISION
BOUNDARY
THIS
IS
THE
SET
OF
POSITIONS
IN
DATA
SPACE
X
P
R
W
X
WHERE
THE
POSTERIOR
PROBABILITY
IS
THE
DECISION
BOUNDARY
SEPARATES
THE
REGION
WHERE
THE
WORLD
STATE
W
IS
MORE
LIKELY
TO
BE
FROM
THE
REGION
WHERE
IT
IS
MORE
LIKELY
TO
BE
FOR
LOGISTIC
REGRESSION
THE
DECISION
BOUNDARY
TAKES
THE
FORM
OF
A
HYPERPLANE
WITH
THE
NORMAL
VECTOR
IN
THE
DIRECTION
OF
Φ
AS
FOR
REGRESSION
WE
CAN
SIMPLIFY
THE
NOTATION
BY
ATTACHING
THE
Y
INTERCEPT
TO
THE
START
OF
THE
PARAMETER
VECTOR
Φ
SO
THAT
Φ
ΦT
T
AND
ATTACHING
TO
THE
START
OF
THE
DATA
VECTOR
X
SO
THAT
X
XT
T
AFTER
THESE
CHANGES
THE
ACTIVATION
IS
NOW
A
ΦT
X
AND
THE
FINAL
MODEL
BECOMES
P
R
W
Φ
X
BERNW
EXP
ΦT
X
LOGISTIC
REGRESSION
A
B
C
FIGURE
LOGISTIC
REGRESSION
A
WE
REPRESENT
THE
WORLD
STATE
W
AS
A
BERNOULLI
DISTRIBUTION
AND
MAKE
THE
BERNOULLI
PARAMETER
Λ
A
FUNCTION
OF
THE
OBSERVATIONS
X
B
WE
COMPUTE
THE
ACTIVATION
A
AS
A
LINEAR
SUM
A
OF
THE
OBSERVATIONS
C
THE
BERNOULLI
PARAMETER
Λ
IS
FORMED
BY
PASSING
THE
ACTIVATION
THROUGH
A
LOGISTIC
SIGMOID
FUNCTION
SIG
TO
CONSTRAIN
THE
VALUE
TO
LIE
BETWEEN
AND
GIVING
THE
CHARACTERISTIC
SIGMOID
SHAPE
IN
LEARNING
WE
FIT
PARAMETERS
Θ
USING
TRAINING
PAIRS
XI
WI
IN
INFERENCE
WE
TAKE
A
NEW
DATUM
X
AND
EVALUATE
THE
POSTERIOR
P
R
W
X
OVER
THE
STATE
NOTICE
THAT
THIS
IS
VERY
SIMILAR
TO
THE
LINEAR
REGRESSION
MODEL
SECTION
EXCEPT
FOR
THE
INTRODUCTION
OF
THE
NONLINEAR
LOGISTIC
SIGMOID
FUNCTION
SIG
EXPLAINING
THE
UNFORTUNATE
NAME
LOGISTIC
REGRESSION
HOWEVER
THIS
SMALL
CHANGE
HAS
SERIOUS
IMPLICATIONS
MAXIMUM
LIKELIHOOD
LEARNING
OF
THE
PARAMETERS
Φ
IS
CONSIDERABLY
HARDER
THAN
FOR
LINEAR
REGRESSION
AND
TO
ADOPT
THE
BAYESIAN
APPROACH
WE
WILL
BE
FORCED
TO
MAKE
APPROXIMATIONS
LEARNING
MAXIMUM
LIKELIHOOD
IN
MAXIMUM
LIKELIHOOD
LEARNING
WE
CONSIDER
FITTING
THE
PARAMETERS
Φ
USING
I
PAIRED
EXAMPLES
OF
TRAINING
DATA
X
W
I
FIGURE
ASSUMING
INDEPENDENCE
OF
THE
ALGORITHM
TRAINING
PAIRS
WE
HAVE
P
R
W
X
Φ
I
I
I
I
ΛWI
Λ
WI
I
TT
WI
EXP
ΦT
XI
WI
I
EXP
ΦT
XI
EXP
ΦT
XI
WHERE
X
XI
IS
A
MATRIX
CONTAINING
THE
MEASUREMENTS
AND
W
WI
T
IS
A
VECTOR
CONTAINING
ALL
OF
THE
BINARY
WORLD
STATES
THE
MAXI
MUM
LIKELIHOOD
METHOD
FINDS
PARAMETERS
Φ
WHICH
MAXIMIZE
THIS
EXPRESSION
AS
USUAL
HOWEVER
IT
IS
SIMPLER
TO
MAXIMIZE
THE
LOGARITHM
L
OF
THIS
EXPRESSION
SINCE
THE
LOGARITHM
IS
A
MONOTONIC
TRANSFORMATION
IT
DOES
NOT
CHANGE
THE
POSITION
CLASSIFICATION
MODELS
A
FIGURE
LOGISTIC
REGRESSION
MODEL
FITTED
TO
TWO
DIFFERENT
DATASETS
A
ONE
DIMENSIONAL
DATA
GREEN
POINTS
DENOTE
SET
OF
EXAMPLES
WHERE
W
PINK
POINTS
DENOTE
WHERE
W
NOTE
THAT
IN
THIS
AND
ALL
FUTURE
FIGURES
IN
THIS
CHAPTER
WE
HAVE
ONLY
PLOTTED
THE
PROBABILITY
P
R
W
X
COMPARE
TO
FIGURE
THE
PROBABILITY
P
R
W
X
CAN
BE
COMPUTED
AS
P
R
W
X
B
TWO
DIMENSIONAL
DATA
HERE
THE
MODEL
HAS
A
SIGMOID
PROFILE
IN
THE
DIRECTION
OF
THE
GRADIENT
Φ
AND
P
R
W
X
IS
CONSTANT
IN
THE
ORTHOGONAL
DIRECTIONS
THE
DECISION
BOUNDARY
CYAN
LINE
IS
LINEAR
OF
THE
MAXIMUM
WITH
RESPECT
TO
Φ
APPLYING
THE
LOGARITHM
REPLACES
THE
PRODUCT
WITH
A
SUM
SO
THAT
L
I
WI
LOG
EXP
ΦT
XI
I
WI
LOG
EXP
ΦT
XI
EXP
Φ
XI
PROBLEM
THE
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
L
WITH
RESPECT
TO
THE
PARAMETERS
Φ
IS
L
Φ
I
EXP
ΦT
X
I
XI
I
SIG
AI
WI
XI
UNFORTUNATELY
WHEN
WE
EQUATE
THIS
EXPRESSION
TO
ZERO
THERE
IS
NO
WAY
TO
RE
ARRANGE
TO
GET
A
CLOSED
FORM
SOLUTION
FOR
THE
PARAMETERS
Φ
INSTEAD
WE
MUST
RELY
ON
A
NONLINEAR
OPTIMIZATION
TECHNIQUE
TO
FIND
THE
MAXIMUM
OF
THIS
OBJECTIVE
FUNCTION
OPTIMIZATION
TECHNIQUES
ARE
DISCUSSED
IN
DETAIL
IN
APPENDIX
B
IN
BRIEF
WE
START
WITH
AN
INITIAL
ESTIMATE
OF
THE
SOLUTION
Φ
AND
ITERATIVELY
IMPROVE
IT
UNTIL
NO
MORE
PROGRESS
CAN
BE
MADE
HERE
WE
WILL
APPLY
THE
NEWTON
METHOD
IN
WHICH
WE
BASE
THE
UPDATE
OF
THE
PARAMETERS
ON
THE
FIRST
AND
SECOND
DERIVATIVES
OF
THE
FUNCTION
AT
THE
CURRENT
POSITION
SO
THAT
LOGISTIC
REGRESSION
C
FIGURE
PARAMETER
ESTIMATION
FOR
LOGISTIC
REGRESSION
WITH
DATA
A
IN
MAXIMUM
LIKELIHOOD
LEARNING
WE
SEEK
THE
MAXIMUM
OF
P
R
W
X
Φ
WITH
RESPECT
TO
Φ
B
IN
PRACTICE
WE
INSTEAD
MAXIMIZE
LOG
LIKELIHOOD
NOTICE
THAT
THE
PEAK
IS
IN
THE
SAME
PLACE
CROSSES
SHOW
RESULTS
OF
TWO
ITERATIONS
OF
OPTIMIZATION
USING
NEWTON
METHOD
C
THE
LOGISTIC
SIGMOID
FUNCTIONS
ASSO
CIATED
WITH
THE
PARAMETERS
AT
EACH
OPTIMIZATION
STEP
AS
THE
LOG
LIKELIHOOD
INCREASES
THE
MODEL
FITS
THE
DATA
MORE
CLOSELY
THE
GREEN
POINTS
REPRESENT
DATA
WHERE
W
AND
THE
PURPLE
POINTS
REPRESENT
DATA
WHERE
W
SO
WE
EXPECT
THE
BEST
FITTING
MODEL
TO
INCREASE
FROM
LEFT
TO
RIGHT
JUST
LIKE
CURVE
Φ
T
Φ
T
Α
L
Φ
WHERE
Φ
T
DENOTES
THE
ESTIMATE
OF
THE
PARAMETERS
Φ
AT
ITERATION
T
AND
Α
DETERMINES
HOW
MUCH
WE
CHANGE
THIS
ESTIMATE
AND
IS
USUALLY
CHOSEN
BY
AN
EXPLICIT
SEARCH
AT
EACH
ITERATION
FOR
THE
LOGISTIC
REGRESSION
MODEL
THE
D
VECTOR
OF
FIRST
DERIVATIVES
AND
THE
PROBLEM
D
D
MATRIX
OF
SECOND
DERIVATIVES
ARE
GIVEN
BY
L
SIG
A
W
X
Φ
I
I
I
I
I
L
SIG
A
SIG
A
X
XT
THESE
ARE
KNOWN
AS
THE
GRADIENT
VECTOR
AND
THE
HESSIAN
MATRIX
THE
EXPRESSION
FOR
THE
GRADIENT
VECTOR
HAS
AN
INTUITIVE
EXPLANATION
THE
CON
TRIBUTION
OF
EACH
DATA
POINT
DEPENDS
ON
THE
DIFFERENCE
BETWEEN
THE
ACTUAL
CLASS
WI
AND
THE
PREDICTED
PROBABILITY
Λ
SIG
AI
OF
BEING
IN
CLASS
POINTS
THAT
ARE
CLASSI
FIED
INCORRECTLY
CONTRIBUTE
MORE
TO
THIS
EXPRESSION
AND
HENCE
HAVE
MORE
INFLUENCE
THAT
THESE
ARE
THE
GRADIENT
AND
HESSIAN
OF
THE
LOG
LIKELIHOOD
THAT
WE
AIM
TO
MAXIMIZE
IF
THIS
IS
IMPLEMENTED
USING
A
NONLINEAR
MINIMIZATION
ALGORITHM
WE
SHOULD
MULTIPLY
THE
OBJECTIVE
FUNCTION
GRADIENT
AND
HESSIAN
BY
ON
THE
PARAMETER
VALUES
FIGURE
SHOWS
MAXIMUM
LIKELIHOOD
LEARNING
OF
THE
PARAMETERS
Φ
FOR
DATA
USING
A
SERIES
OF
NEWTON
STEPS
FOR
GENERAL
FUNCTIONS
THE
NEWTON
METHOD
ONLY
FINDS
LOCAL
MAXIMA
AT
THE
END
OF
THE
PROCEDURE
WE
CANNOT
BE
CERTAIN
THAT
THERE
IS
NOT
A
TALLER
PEAK
IN
THE
LIKELIHOOD
FUNCTION
ELSEWHERE
HOWEVER
THE
LOG
LIKELIHOOD
FOR
LOGISTIC
REGRESSION
HAS
A
SPECIAL
PROPERTY
IT
IS
A
CONCAVE
FUNCTION
OF
THE
PARAMETERS
Φ
FOR
CONCAVE
FUNCTIONS
THERE
ARE
NEVER
MULTIPLE
MAXIMA
AND
GRADIENT
BASED
APPROACHES
ARE
GUARANTEED
TO
FIND
THE
GLOBAL
MAXIMUM
IT
IS
POSSIBLE
TO
ESTABLISH
WHETHER
A
FUNCTION
IS
CONCAVE
OR
NOT
BY
EXAMINING
THE
HESSIAN
MATRIX
IF
THIS
IS
NEGATIVE
DEFINITE
FOR
ALL
Φ
THEN
THE
FUNCTION
IS
CONCAVE
THIS
IS
THE
CASE
FOR
LOGISTIC
REGRESSION
AS
THE
HESSIAN
EQUATION
CONSISTS
OF
A
NEGATIVE
WEIGHTED
SUM
OF
OUTER
PROBLEMS
WITH
THE
LOGISTIC
REGRESSION
MODEL
PROBLEM
ALGORITHM
THE
LOGISTIC
REGRESSION
MODEL
WORKS
WELL
FOR
SIMPLE
DATA
SETS
BUT
FOR
MORE
COMPLEX
VISUAL
DATA
IT
WILL
NOT
GENERALLY
SUFFICE
IT
IS
LIMITED
IN
THE
FOLLOWING
WAYS
IT
IS
OVERCONFIDENT
AS
IT
WAS
LEARNED
USING
MAXIMUM
LIKELIHOOD
IT
CAN
ONLY
DESCRIBE
LINEAR
DECISION
BOUNDARIES
IT
IS
INEFFICIENT
AND
PRONE
TO
OVER
FITTING
IN
HIGH
DIMENSIONS
IN
THE
REMAINING
PART
OF
THIS
CHAPTER
WE
WILL
EXTEND
THIS
MODEL
TO
COPE
WITH
THESE
PROBLEMS
FIGURE
BAYESIAN
LOGISTIC
REGRESSION
IN
THE
BAYESIAN
APPROACH
WE
LEARN
A
DISTRIBUTION
P
R
Φ
X
W
OVER
THE
POSSIBLE
PARAMETER
VALUES
Φ
THAT
ARE
COMPATIBLE
WITH
THE
TRAINING
DATA
IN
INFERENCE
WE
OBSERVE
A
NEW
DATA
EXAMPLE
X
AND
USE
THIS
DISTRIBUTION
TO
WEIGHT
THE
PREDICTIONS
FOR
THE
WORLD
STATE
W
GIVEN
BY
EACH
POSSIBLE
ESTIMATE
OF
Φ
IN
LINEAR
REGRESSION
SECTION
THERE
WERE
CLOSED
FORM
EXPRESSIONS
FOR
BOTH
OF
THESE
STEPS
HOWEVER
THE
NONLINEAR
FUNCTION
SIG
IN
LOGISTIC
REGRESSION
MEANS
THAT
THIS
IS
NO
LONGER
THE
CASE
TO
GET
AROUND
THIS
WE
WILL
APPROXIMATE
BOTH
STEPS
SO
THAT
WE
RETAIN
NEAT
CLOSED
FORM
EXPRESSIONS
AND
THE
ALGORITHM
IS
TRACTABLE
LEARNING
WE
START
BY
DEFINING
A
PRIOR
OVER
THE
PARAMETERS
Φ
UNFORTUNATELY
THERE
IS
NO
WE
ARE
CONCERNED
WITH
MINIMIZING
FUNCTIONS
WE
EQUIVALENTLY
CONSIDER
WHETHER
THE
FUNCTION
IS
CONVEX
AND
SO
HAS
ONLY
A
SINGLE
MINIMUM
IF
THE
HESSIAN
MATRIX
IS
POSITIVE
DEFINITE
EVERYWHERE
THEN
THE
FUNCTION
IS
CONVEX
PROBLEM
OVERCONFIDENT
BAYESIAN
FORMULATION
LOGISTIC
REGRESSION
PROBLEM
LINEAR
PROJECT
DATA
THROUGH
NON
LINEARITY
NON
LINEAR
LOGISTIC
REGRESSION
RELEVANCE
VECTOR
CLASSIFICATION
PROBLEM
COMPUTATIONAL
COST
INCREMENTAL
LEARNING
BOOSTING
CLASSIFICATION
TREES
FIGURE
FAMILY
OF
CLASSIFICATION
MODELS
A
IN
THE
REMAINING
PART
OF
THE
CHAPTER
WE
WILL
ADDRESS
SEVERAL
OF
THE
LIMITATIONS
OF
LOGISTIC
REGRESSION
FOR
BINARY
CLASSIFICATION
B
THE
LOGISTIC
REGRESSION
MODEL
WITH
MAXIMUM
LIKE
LIHOOD
LEARNING
IS
OVERCONFIDENT
AND
HENCE
WE
DEVELOP
A
BAYESIAN
VERSION
SECTION
C
IT
IS
UNREALISTIC
TO
ALWAYS
ASSUME
A
LINEAR
RELATIONSHIP
BE
TWEEN
THE
DATA
AND
THE
WORLD
AND
TO
THIS
END
WE
INTRODUCE
A
NONLINEAR
VERSION
SECTION
D
COMBINING
THE
BAYESIAN
AND
NONLINEAR
VERSIONS
OF
REGRESSION
LEADS
TO
GAUSSIAN
PROCESS
CLASSIFICATION
E
THE
LOGISTIC
REGRESSION
MODEL
ALSO
HAS
MANY
PARAMETERS
AND
MAY
REQUIRE
CONSIDERABLE
RESOURCES
TO
LEARN
WHEN
THE
DATA
DIMENSION
IS
HIGH
AND
SO
WE
DEVELOP
RELEVANCE
VECTOR
CLASSIFICATION
WHICH
ENCOURAGES
SPARSITY
F
WE
CAN
ALSO
BUILD
A
SPARSE
MODEL
BY
INCREMENTALLY
ADDING
PARAMETERS
IN
A
BOOSTING
MODEL
G
FINALLY
WE
CONSIDER
A
VERY
FAST
CLASSIFICATION
MODEL
BASED
ON
A
TREE
STRUCTURE
CONJUGATE
PRIOR
FOR
THE
LIKELIHOOD
IN
THE
LOGISTIC
REGRESSION
MODEL
EQUATION
THIS
IS
WHY
THERE
WON
T
BE
CLOSED
FORM
EXPRESSIONS
FOR
THE
LIKELIHOOD
AND
PREDICTIVE
DISTRIBUTION
WITH
NOTHING
ELSE
TO
GUIDE
US
A
REASONABLE
CHOICE
FOR
THE
PRIOR
OVER
THE
CONTINUOUS
PARAMETERS
Φ
IS
A
MULTIVARIATE
NORMAL
DISTRIBUTION
WITH
ZERO
MEAN
AND
A
LARGE
SPHERICAL
COVARIANCE
SO
THAT
P
R
Φ
NORMΦ
TO
COMPUTE
THE
POSTERIOR
PROBABILITY
DISTRIBUTION
P
R
Φ
X
W
OVER
THE
PARAM
ETERS
Φ
GIVEN
THE
TRAINING
DATA
PAIRS
XI
WI
WE
APPLY
BAYES
RULE
P
R
Φ
X
W
P
R
W
X
Φ
P
R
Φ
P
R
W
X
WHERE
THE
LIKELIHOOD
AND
PRIOR
ARE
GIVEN
BY
EQUATIONS
AND
RESPECTIVELY
FIGURE
LAPLACE
APPROXIMATION
A
PROBABILITY
DENSITY
BLUE
CURVE
IS
AP
PROXIMATED
BY
A
NORMAL
DISTRIBUTION
RED
CURVE
THE
MEAN
OF
THE
NORMAL
AND
HENCE
THE
PEAK
IS
CHOSEN
TO
COIN
CIDE
WITH
THE
PEAK
OF
THE
ORIGINAL
PDF
THE
VARIANCE
OF
THE
NORMAL
IS
CHOSEN
SO
THAT
ITS
SECOND
DERIVATIVES
AT
THE
MEAN
MATCH
THE
SECOND
DERIVATIVES
OF
THE
ORIGINAL
PDF
AT
THE
PEAK
PROBLEM
SINCE
WE
ARE
NOT
USING
A
CONJUGATE
PRIOR
THERE
IS
NO
SIMPLE
CLOSED
FORM
EXPRESSION
FOR
THIS
POSTERIOR
AND
SO
WE
ARE
FORCED
TO
MAKE
AN
APPROXIMATION
OF
SOME
KIND
ONE
POSSIBILITY
IS
TO
USE
THE
LAPLACE
APPROXIMATION
FIGURE
WHICH
IS
A
GEN
PROBLEM
ERAL
METHOD
FOR
APPROXIMATING
COMPLEX
PROBABILITY
DISTRIBUTIONS
THE
GOAL
IS
TO
APPROXIMATE
THE
POSTERIOR
DISTRIBUTION
BY
A
MULTIVARIATE
NORMAL
WE
SELECT
THE
PARAMETERS
OF
THIS
NORMAL
SO
THAT
I
THE
MEAN
IS
AT
THE
PEAK
OF
THE
POSTERIOR
DISTRIBUTION
I
E
AT
THE
MAP
ESTIMATE
AND
II
THE
COVARIANCE
IS
SUCH
THAT
THE
SECOND
DERIVATIVES
AT
THE
PEAK
MATCH
THE
SECOND
DERIVATIVES
OF
THE
TRUE
POSTERIOR
DISTRIBUTION
AT
ITS
PEAK
HENCE
TO
MAKE
THE
LAPLACE
APPROXIMATION
WE
FIRST
FIND
THE
MAP
ESTIMATE
OF
THE
PARAMETERS
Φˆ
AND
TO
THIS
END
WE
USE
A
NONLINEAR
OPTIMIZATION
TECHNIQUE
SUCH
AS
NEWTON
METHOD
TO
MAXIMIZE
THE
CRITERION
I
L
LOG
P
R
WI
XI
Φ
LOG
P
R
Φ
I
NEWTON
METHOD
NEEDS
THE
DERIVATIVES
OF
THE
LOG
POSTERIOR
WHICH
ARE
L
SIG
A
W
X
Φ
Φ
I
I
I
I
I
P
L
SIG
A
SIG
A
X
XT
WE
THEN
APPROXIMATE
THE
POSTERIOR
BY
A
MULTIVARIATE
NORMAL
SO
THAT
PROBLEM
P
R
Φ
X
W
Q
Φ
NORMΦ
Μ
Σ
WHERE
THE
MEAN
Μ
IS
SET
TO
THE
MAP
ESTIMATE
Φˆ
AND
THE
COVARIANCE
Σ
IS
CHOSEN
SO
THAT
THE
SECOND
DERIVATIVES
OF
THE
NORMAL
MATCH
THOSE
OF
THE
POSTERIOR
DISTRIBUTION
AT
THE
MAP
ESTIMATE
FIGURE
SO
THAT
BAYESIAN
LOGISTIC
REGRESSION
FIGURE
LAPLACE
APPROXIMATION
FOR
LOGISTIC
REGRESSION
A
THE
PRIOR
P
R
Φ
OVER
THE
PARAMETERS
IS
A
NORMAL
DISTRIBUTION
WITH
MEAN
ZERO
AND
A
LARGE
SPHERICAL
COVARIANCE
B
THE
POSTERIOR
DISTRIBUTION
P
R
Φ
X
W
REPRESENTS
THE
REFINED
STATE
OF
OUR
KNOWLEDGE
AFTER
OBSERVING
THE
DATA
UNFORTUNATELY
THIS
POSTERIOR
CANNOT
BE
EXPRESSED
IN
CLOSED
FORM
C
WE
APPROXIMATE
THE
TRUE
POSTERIOR
WITH
A
NORMAL
DISTRIBUTION
Q
Φ
NORMΦ
Μ
Σ
WHOSE
MEAN
IS
AT
THE
PEAK
OF
THE
POSTERIOR
AND
WHOSE
COVARIANCE
IS
CHOSEN
SO
THAT
THE
SECOND
DERIVATIVES
AT
THE
PEAK
OF
THE
TRUE
POSTERIOR
MATCH
THE
SECOND
DERIVATIVES
AT
THE
PEAK
OF
THE
NORMAL
THIS
IS
TERMED
THE
LAPLACE
APPROXIMATION
Μ
Φˆ
INFERENCE
IN
INFERENCE
WE
AIM
TO
COMPUTE
A
POSTERIOR
DISTRIBUTION
P
R
W
X
X
W
OVER
THE
WORLD
STATE
W
GIVEN
NEW
OBSERVED
DATA
X
TO
THIS
END
WE
COMPUTE
AN
INFI
NITE
WEIGHTED
SUM
I
E
AN
INTEGRAL
OF
THE
PREDICTIONS
P
R
W
X
Φ
GIVEN
BY
EACH
POSSIBLE
VALUE
OF
THE
PARAMETERS
Φ
P
R
W
X
X
W
P
R
W
X
Φ
P
R
Φ
X
W
DΦ
P
R
W
X
Φ
Q
Φ
DΦ
WHERE
THE
WEIGHTS
Q
Φ
ARE
GIVEN
BY
THE
APPROXIMATED
POSTERIOR
DISTRIBUTION
OVER
THE
PARAMETERS
FROM
THE
LEARNING
STAGE
UNFORTUNATELY
THIS
INTEGRAL
CANNOT
BE
COMPUTED
IN
CLOSED
FORM
EITHER
AND
SO
WE
MUST
MAKE
A
FURTHER
APPROXIMATION
CLASSIFICATION
MODELS
FIGURE
APPROXIMATION
OF
ACTIVATION
INTEGRAL
EQUATION
A
ACTUAL
RESULT
OF
INTEGRAL
AS
A
FUNCTION
OF
ΜA
AND
B
THE
NON
OBVIOUS
APPROX
IMATION
FROM
EQUATION
C
THE
ABSOLUTE
DIFFERENCE
BETWEEN
THE
ACTUAL
RESULT
AND
THE
APPROXIMATION
IS
VERY
SMALL
OVER
A
RANGE
OF
REASONABLE
VALUES
WE
FIRST
NOTE
THAT
THE
PREDICTION
P
R
W
X
Φ
DEPENDS
ONLY
ON
A
LINEAR
PRO
JECTION
A
ΦT
X
OF
THE
PARAMETERS
SEE
EQUATION
HENCE
WE
COULD
RE
EXPRESS
THE
PREDICTION
AS
P
R
W
X
X
W
P
R
W
A
P
R
A
DA
THE
PROBABILITY
DISTRIBUTION
P
R
A
CAN
BE
COMPUTED
USING
THE
TRANSFORMATION
PROP
ERTY
OF
THE
NORMAL
DISTRIBUTION
SECTION
AND
IS
GIVEN
BY
P
R
A
P
R
ΦT
X
NORMA
ΜT
X
X
T
ΣX
NORMA
ΜA
WHERE
WE
HAVE
DENOTED
THE
MEAN
AND
VARIANCE
OF
THE
ACTIVATION
BY
ΜA
AND
RE
SPECTIVELY
THE
ONE
DIMENSIONAL
INTEGRATION
IN
EQUATION
CAN
NOW
BE
COMPUTED
USING
NUMERICAL
INTEGRATION
OVER
A
OR
WE
CAN
APPROXIMATE
THE
RESULT
WITH
A
SIMILAR
FUNCTION
SUCH
AS
P
R
W
A
NORMA
ΜA
ΣA
DA
EXP
Μ
IT
IS
NOT
OBVIOUS
BY
INSPECTION
THAT
THIS
FUNCTION
SHOULD
APPROXIMATE
THE
INTEGRAL
WELL
HOWEVER
FIGURE
DEMONSTRATES
THAT
THE
APPROXIMATION
IS
QUITE
ACCURATE
FIGURE
COMPARES
THE
CLASSIFICATION
PREDICTIONS
P
R
W
X
FOR
THE
MAXIMUM
LIKELIHOOD
AND
BAYESIAN
APPROACHES
FOR
LOGISTIC
REGRESSION
THE
BAYESIAN
APPROACH
MAKES
MORE
MODERATE
PREDICTIONS
FOR
THE
FINAL
CLASS
THIS
IS
PARTICULARLY
THE
CASE
IN
REGIONS
OF
DATA
SPACE
THAT
ARE
FAR
FROM
THE
MEAN
NON
LINEAR
LOGISTIC
REGRESSION
A
FIGURE
BAYESIAN
LOGISTIC
REGRESSION
PREDICTIONS
A
THE
BAYESIAN
PREDIC
TION
FOR
THE
CLASS
W
IS
MORE
MODERATE
THAN
THE
MAXIMUM
LIKELIHOOD
PREDIC
TION
B
IN
THE
DECISION
BOUNDARY
IN
THE
BAYESIAN
CASE
BLUE
LINE
IS
STILL
LINEAR
BUT
ISO
PROBABILITY
CONTOURS
AT
LEVELS
OTHER
THAN
ARE
CURVED
COM
PARE
TO
MAXIMUM
LIKELIHOOD
CASE
IN
FIGURE
HERE
TOO
THE
BAYESIAN
SOLUTION
MAKES
MORE
MODERATE
PREDICTIONS
THAN
THE
MAXIMUM
LIKELIHOOD
MODEL
NON
LINEAR
LOGISTIC
REGRESSION
THE
LOGISTIC
REGRESSION
MODEL
DESCRIBED
PREVIOUSLY
CAN
ONLY
CREATE
LINEAR
DECISION
BOUNDARIES
BETWEEN
CLASSES
TO
CREATE
NONLINEAR
DECISION
BOUNDARIES
WE
ADOPT
THE
SAME
APPROACH
AS
WE
DID
FOR
REGRESSION
SECTION
WE
COMPUTE
A
NONLINEAR
TRANSFORMATION
Z
F
X
OF
THE
OBSERVED
DATA
AND
THEN
BUILD
THE
LOGISTIC
REGRESSION
MODEL
SUBSTITUTING
THE
ORIGINAL
DATA
X
FOR
THE
TRANSFORMED
DATA
Z
SO
THAT
P
R
W
X
Φ
BERNW
SIG
ΦT
Z
BERNW
SIG
ΦT
F
X
THE
LOGIC
OF
THIS
APPROACH
IS
THAT
ARBITRARY
NONLINEAR
ACTIVATIONS
CAN
BE
BUILT
AS
A
LINEAR
SUM
OF
NONLINEAR
BASIS
FUNCTIONS
TYPICAL
NONLINEAR
TRANSFORMATIONS
INCLUDE
HEAVISIDE
STEP
FUNCTIONS
OF
PROJECTIONS
ZK
HEAVISIDE
ΑT
X
ARC
TANGENT
FUNCTIONS
OF
PROJECTIONS
ZK
ARCTAN
ΑT
X
AND
RADIAL
BASIS
FUNCTIONS
ZK
EXP
X
ΑK
X
ΑK
WHERE
ZK
DENOTES
THE
KTH
ELEMENT
OF
THE
TRANSFORMED
VECTOR
Z
AND
THE
FUNCTION
HEAVISIDE
RETURNS
ZERO
IF
ITS
ARGUMENT
IS
LESS
THAN
ZERO
AND
ONE
OTHERWISE
IN
THE
FIRST
TWO
CASES
WE
HAVE
ATTACHED
A
TO
THE
START
OF
THE
OBSERVED
DATA
X
WHERE
WE
USE
PROJECTIONS
ΑT
X
TO
AVOID
HAVING
A
SEPARATE
OFFSET
PARAMETER
FIGURES
AND
SHOW
EXAMPLES
OF
NONLINEAR
CLASSIFICATION
USING
ARC
TANGENT
FUNCTIONS
FOR
ONE
AND
TWO
DIMENSIONAL
DATA
RESPECTIVELY
CLASSIFICATION
MODELS
A
B
C
D
FIGURE
NON
LINEAR
CLASSIFICATION
IN
USING
ARC
TANGENT
TRANSFORMATION
WE
CONSIDER
A
COMPLEX
DATA
SET
BOTTOM
OF
ALL
PANELS
WHERE
THE
POSTERIOR
P
R
W
X
CANNOT
EASILY
BE
DESCRIBED
BY
A
SINGLE
SIGMOID
GREEN
CIRCLES
REPRESENT
DATA
XI
WHERE
WI
PINK
CIRCLES
REPRESENT
DATA
XI
WHERE
WI
A
THE
SEVEN
DIMENSIONAL
TRANSFORMED
DATA
VECTORS
ZI
ARE
COMPUTED
BY
EVALUATING
EACH
DATA
EXAMPLE
AGAINST
SEVEN
PREDEFINED
ARC
TANGENT
FUNCTIONS
ZIK
FK
XI
ARCTAN
B
WHEN
WE
LEARN
THE
PARAMETERS
Φ
WE
ARE
LEARNING
WEIGHTS
FOR
THESE
NONLINEAR
ARC
TANGENT
FUNCTIONS
THE
FUNCTIONS
ARE
SHOWN
AFTER
APPLYING
THE
MAXIMUM
LIKELIHOOD
WEIGHTS
Φˆ
C
THE
FINAL
ACTIVATION
A
ΦT
Z
IS
A
WEIGHTED
SUM
OF
THE
NONLINEAR
FUNCTIONS
D
THE
PROBABILITY
P
R
W
X
IS
COMPUTED
BY
PASSING
THE
ACTIVATION
A
THROUGH
THE
LOGISTIC
SIGMOID
FUNCTION
NOTE
THAT
THE
BASIS
FUNCTIONS
ALSO
HAVE
PARAMETERS
FOR
EXAMPLE
IN
THE
ARC
TANGENT
EXAMPLE
THERE
ARE
THE
PROJECTION
DIRECTIONS
ΑK
K
EACH
OF
WHICH
CON
TAINS
AN
OFFSET
AND
A
SET
OF
GRADIENTS
THESE
CAN
ALSO
BE
OPTIMIZED
DURING
THE
FITTING
PROCEDURE
TOGETHER
WITH
THE
WEIGHTS
Φ
WE
FORM
A
NEW
VECTOR
OF
UNKNOWNS
Θ
ΦT
ΑT
ΑT
ΑT
T
AND
OPTIMIZE
THE
MODEL
WITH
RESPECT
TO
ALL
OF
THESE
K
UNKNOWNS
TOGETHER
THE
GRADIENT
VECTOR
AND
THE
HESSIAN
MATRIX
DEPEND
ON
THE
CHOSEN
TRANSFORMATION
F
BUT
CAN
BE
COMPUTED
USING
THE
EXPRESSIONS
FIGURE
NON
LINEAR
CLASSIFICATION
IN
USING
ARC
TANGENT
TRANSFORM
A
THESE
DATA
HAVE
BEEN
SUCCESSFULLY
CLASSIFIED
WITH
NONLINEAR
LOGISTIC
REGRES
SION
NOTE
THE
NONLINEAR
DECISION
BOUNDARY
CYAN
LINE
TO
COMPUTE
THE
POSTERIOR
P
R
W
X
WE
TRANSFORM
THE
DATA
TO
A
NEW
TWO
DIMENSIONAL
SPACE
Z
F
X
WHERE
THE
ELEMENTS
OF
Z
ARE
COMPUTED
BY
EVALUATING
X
AGAINST
THE
ARC
TANGENT
FUNCTIONS
IN
B
AND
C
THE
ARC
TANGENT
ACTIVATIONS
ARE
WEIGHTED
THE
FIRST
BY
A
NEGATIVE
NUMBER
AND
SUMMED
AND
THE
RESULT
IS
PUT
THROUGH
THE
LOGISTIC
SIGMOID
TO
COMPUTE
P
R
W
X
L
W
AI
SIG
A
Θ
I
I
I
I
Θ
T
L
SIG
A
SIG
A
AI
AI
W
AI
SIG
A
WHERE
AI
ΦT
F
XI
THESE
RELATIONS
WERE
ESTABLISHED
USING
THE
CHAIN
RULE
FOR
DERIVATIVES
UNFORTUNATELY
THIS
JOINT
OPTIMIZATION
PROBLEM
IS
GENERALLY
NOT
CONVEX
AND
WILL
BE
PRONE
TO
TERMINATING
IN
LOCAL
MAXIMA
IN
THE
BAYESIAN
CASE
IT
WOULD
BE
TYPICAL
TO
MARGINALIZE
OVER
THE
PARAMETERS
Φ
BUT
MAXIMIZE
OVER
THE
FUNCTION
PARAMETERS
DUAL
LOGISTIC
REGRESSION
THERE
IS
A
POTENTIAL
PROBLEM
WITH
THE
LOGISTIC
REGRESSION
MODELS
AS
DESCRIBED
EARLIER
IN
THE
ORIGINAL
LINEAR
MODEL
THERE
IS
ONE
ELEMENT
OF
THE
GRADIENT
VECTOR
Φ
CORRE
SPONDING
TO
EACH
DIMENSION
OF
THE
OBSERVED
DATA
X
AND
IN
THE
NONLINEAR
EXTENSION
THERE
IS
ONE
ELEMENT
CORRESPONDING
TO
EACH
TRANSFORMED
DATA
DIMENSION
Z
IF
THE
RELEVANT
DATA
X
OR
Z
IS
VERY
HIGH
DIMENSIONAL
THEN
THE
MODEL
WILL
HAVE
A
LARGE
NUMBER
OF
PARAMETERS
THIS
WILL
RENDER
THE
NEWTON
UPDATE
SLOW
OR
EVEN
INTRACTABLE
ALGORITHM
TO
SOLVE
THIS
PROBLEM
WE
SWITCH
TO
THE
DUAL
REPRESENTATION
FOR
SIMPLICITY
WE
WILL
DEVELOP
THIS
MODEL
USING
THE
ORIGINAL
DATA
X
BUT
ALL
OF
THE
IDEAS
TRANSFER
DIRECTLY
TO
THE
NONLINEAR
CASE
WHERE
WE
USE
TRANSFORMED
DATA
Z
IN
THE
DUAL
PARAMETERIZATION
WE
EXPRESS
THE
GRADIENT
PARAMETERS
Φ
AS
A
WEIGHTED
SUM
OF
THE
OBSERVED
DATA
SEE
FIGURE
SO
THAT
Φ
XΨ
WHERE
Ψ
IS
AN
I
VARIABLE
WHERE
EACH
ELEMENT
WEIGHTS
ONE
OF
THE
DATA
EXAMPLES
IF
THE
NUMBER
OF
DATA
POINTS
I
IS
LESS
THAN
THE
DIMENSIONALITY
D
OF
THE
DATA
X
THEN
THE
NUMBER
OF
PARAMETERS
HAS
BEEN
REDUCED
THE
PRICE
THAT
WE
PAY
FOR
THIS
REDUCTION
IS
THAT
WE
CAN
NOW
ONLY
CHOOSE
GRADIENT
VECTORS
Φ
THAT
ARE
IN
THE
SPACE
SPANNED
BY
THE
DATA
EXAMPLES
HOWEVER
THE
GRADIENT
VECTOR
REPRESENTS
THE
DIRECTION
IN
WHICH
THE
FINAL
PROBABILITY
P
R
W
X
CHANGES
FASTEST
AND
THIS
SHOULD
NOT
POINT
IN
A
DIRECTION
IN
WHICH
THERE
WAS
NO
VARIATION
IN
THE
TRAINING
DATA
ANYWAY
SO
THIS
IS
NOT
A
LIMITATION
SUBSTITUTING
EQUATION
INTO
THE
ORIGINAL
LOGISTIC
REGRESSION
MODEL
LEADS
TO
THE
DUAL
LOGISTIC
REGRESSION
MODEL
P
R
W
X
Ψ
ITT
BERNWI
SIG
AI
ITT
BERNWI
ΨT
XT
XI
L
THE
RESULTING
LEARNING
AND
INFERENCE
ALGORITHMS
ARE
VERY
SIMILAR
TO
THOSE
FOR
THE
ORIGINAL
LOGISTIC
REGRESSION
MODEL
SO
WE
COVER
THEM
ONLY
IN
BRIEF
IN
THE
MAXIMUM
LIKELIHOOD
METHOD
WE
LEARN
THE
PARAMETERS
Ψ
BY
NONLINEAR
OPTIMIZATION
OF
THE
LOG
LIKELIHOOD
L
LOG
P
R
W
X
Ψ
USING
THE
NEWTON
METHOD
THIS
OPTIMIZATION
REQUIRES
THE
DERIVATIVES
OF
THE
LOG
LIKELIHOOD
WHICH
ARE
L
SIG
A
W
XT
X
Ψ
I
I
I
I
I
L
SIG
A
SIG
A
XT
X
XT
X
IN
THE
BAYESIAN
APPROACH
WE
USE
A
NORMAL
PRIOR
OVER
THE
PARAMETERS
Ψ
P
R
Ψ
NORMΨ
THE
POSTERIOR
DISTRIBUTION
P
R
Ψ
X
W
OVER
THE
NEW
PARAMETERS
IS
FOUND
USING
BAYES
RULE
AND
ONCE
MORE
THIS
CANNOT
BE
WRITTEN
IN
CLOSED
FORM
SO
WE
APPLY
THE
LAPLACE
APPROXIMATION
WE
FIND
THE
MAP
SOLUTION
Ψˆ
USING
NONLINEAR
OPTIMIZATION
WHICH
REQUIRES
THE
DERIVATIVES
OF
THE
LOG
POSTERIOR
L
LOG
P
R
Ψ
X
W
L
SIG
A
W
XT
X
Ψ
Ψ
I
I
I
I
I
L
SIG
A
SIG
A
XT
X
XT
X
THE
POSTERIOR
IS
NOW
APPROXIMATED
BY
A
MULTIVARIATE
NORMAL
SO
THAT
WHERE
P
R
Ψ
X
W
Q
Ψ
NORMΨ
Μ
Σ
Μ
Ψˆ
IN
INFERENCE
WE
COMPUTE
THE
DISTRIBUTION
OVER
THE
ACTIVATION
P
R
A
P
R
ΨT
XT
X
NORMA
ΜA
NORMA
ΜT
XT
X
X
T
XΣXT
X
AND
THEN
APPROXIMATE
THE
PREDICTIVE
DISTRIBUTION
USING
EQUATION
DUAL
LOGISTIC
REGRESSION
GIVES
IDENTICAL
RESULTS
TO
THE
ORIGINAL
LOGISTIC
REGRESSION
AL
GORITHM
FOR
THE
MAXIMUM
LIKELIHOOD
CASE
AND
VERY
SIMILAR
RESULTS
IN
THE
BAYESIAN
SITUATION
WHERE
THE
DIFFERENCE
RESULTS
FROM
THE
SLIGHTLY
DIFFERENT
PRIORS
HOW
EVER
THE
DUAL
CLASSIFICATION
MODEL
IS
MUCH
FASTER
TO
FIT
IN
HIGH
DIMENSIONS
AS
THE
PARAMETERS
ARE
FEWER
KERNEL
LOGISTIC
REGRESSION
WE
MOTIVATED
THE
DUAL
MODEL
BY
THE
REDUCTION
IN
THE
NUMBER
OF
PARAMETERS
Ψ
IN
THE
MODEL
WHEN
THE
DATA
LIES
IN
A
HIGH
DIMENSIONAL
SPACE
HOWEVER
NOW
THAT
WE
HAVE
DEVELOPED
THE
MODEL
A
FURTHER
ADVANTAGE
IS
EASY
TO
IDENTIFY
BOTH
LEARNING
AND
INFERENCE
IN
THE
DUAL
MODEL
RELY
ONLY
ON
INNER
PRODUCTS
XT
XJ
OF
THAT
DATA
EQUIVALENTLY
THE
NONLINEAR
VERSION
OF
THIS
ALGORITHM
DEPENDS
ONLY
ON
INNER
PRODUCTS
ZT
ZJ
OF
THE
TRANSFORMED
DATA
VECTORS
THIS
MEANS
THAT
THE
ALGORITHM
IS
SUITABLE
FOR
KERNELIZATION
SEE
SECTION
THE
IDEA
OF
KERNELIZATION
IS
TO
DEFINE
A
KERNEL
FUNCTION
K
WHICH
COMPUTES
THE
QUANTITY
ALGORITHM
K
XI
XJ
ZT
ZJ
WHERE
ZI
F
XI
AND
ZJ
F
XJ
ARE
THE
NONLINEAR
TRANSFORMATIONS
OF
THE
TWO
DATA
VECTORS
REPLACING
THE
INNER
PRODUCTS
WITH
THE
KERNEL
FUNCTION
MEANS
THAT
WE
DO
NOT
HAVE
TO
EXPLICITLY
CALCULATE
THE
TRANSFORMED
VECTORS
Z
AND
HENCE
THEY
MAY
BE
OF
VERY
HIGH
OR
EVEN
INFINITE
DIMENSIONS
SEE
SECTION
FOR
A
MORE
DETAILED
DESCRIPTION
OF
KERNEL
FUNCTIONS
THE
KERNEL
LOGISTIC
REGRESSION
MODEL
COMPARE
TO
EQUATION
IS
HENCE
P
R
W
X
Ψ
ITT
BERNWI
SIG
AI
ITT
BERNWI
ΨT
K
X
XI
L
WHERE
THE
NOTATION
K
X
X
I
REPRESENTS
A
COLUMN
VECTOR
OF
DOT
PRODUCTS
WHERE
ELEMENT
K
IS
GIVEN
BY
K
XK
XI
FOR
MAXIMUM
LIKELIHOOD
LEARNING
WE
SIMPLY
OPTIMIZE
THE
LOG
POSTERIOR
PROBA
BILITY
L
WITH
RESPECT
TO
THE
PARAMETERS
WHICH
REQUIRES
THE
DERIVATIVES
L
SIG
A
W
K
X
X
Ψ
I
I
I
I
I
L
SIG
A
SIG
A
K
X
X
K
X
X
PROBLEM
THE
BAYESIAN
FORMULATION
OF
KERNEL
LOGISTIC
REGRESSION
WHICH
IS
SOMETIMES
KNOWN
AS
GAUSSIAN
PROCESS
CLASSIFICATION
PROCEEDS
ALONG
SIMILAR
LINES
WE
FOLLOW
THE
DUAL
FORMULATION
REPLACING
EACH
OF
THE
DOT
PRODUCTS
BETWEEN
DATA
EXAMPLES
WITH
THE
KERNEL
FUNCTION
A
VERY
COMMON
EXAMPLE
OF
A
KERNEL
FUNCTION
IS
THE
RADIAL
BASIS
KERNEL
IN
WHICH
THE
NONLINEAR
TRANSFORMATION
AND
INNER
PRODUCT
OPERATIONS
ARE
REPLACED
BY
K
XI
XJ
EXP
XI
XJ
T
XI
XJ
L
THIS
IS
EQUIVALENT
TO
COMPUTING
TRANSFORMED
VECTORS
ZI
AND
ZJ
OF
INFINITE
LENGTH
WHERE
EACH
ENTRY
EVALUATES
THE
DATA
X
AGAINST
A
RADIAL
BASIS
FUNCTION
AT
A
DIFFERENT
POSITION
AND
THEN
COMPUTING
THE
INNER
PRODUCT
ZT
ZJ
EXAMPLES
OF
THE
KERNEL
LOGISTIC
REGRESSION
WITH
A
RADIAL
BASIS
KERNEL
ARE
SHOWN
IN
FIGURES
AND
RELEVANCE
VECTOR
CLASSIFICATION
ALGORITHM
THE
BAYESIAN
VERSION
OF
THE
KERNEL
LOGISTIC
REGRESSION
MODEL
IS
POWERFUL
BUT
COM
PUTATIONALLY
EXPENSIVE
AS
IT
REQUIRES
US
TO
COMPUTE
DOT
PRODUCTS
BETWEEN
THE
NEW
DATA
EXAMPLE
AND
THE
ALL
OF
THE
TRAINING
EXAMPLES
IN
THE
KERNEL
FUNCTION
IN
EQUA
TION
IT
WOULD
BE
MORE
EFFICIENT
IF
THE
MODEL
DEPENDED
ONLY
SPARSELY
ON
THE
FIGURE
KERNEL
LOGISTIC
REGRESSION
USING
RBF
KERNEL
AND
MAXIMUM
LIKELI
HOOD
LEARNING
A
WITH
A
SMALL
LENGTH
SCALE
Λ
THE
MODEL
DOES
NOT
INTERPOLATE
MUCH
FROM
THE
DATA
EXAMPLES
B
WITH
A
REASONABLE
LENGTH
SCALE
THE
CLAS
SIFIER
DOES
A
GOOD
JOB
OF
MODELING
THE
POSTERIOR
P
R
W
X
C
WITH
A
LARGE
LENGTH
SCALE
THE
ESTIMATED
POSTERIOR
IS
VERY
SMOOTH
AND
THE
MODEL
INTERPOLATES
CONFIDENT
DECISIONS
INTO
REGIONS
SUCH
AS
THE
TOP
LEFT
WHERE
THERE
IS
NO
DATA
FIGURE
KERNEL
LOGISTIC
REGRESSION
WITH
RBF
KERNEL
IN
A
BAYESIAN
SET
TING
WE
NOW
TAKE
ACCOUNT
OF
OUR
UN
CERTAINTY
IN
THE
DUAL
PARAMETERS
Ψ
BY
APPROXIMATING
THEIR
POSTERIOR
DIS
TRIBUTION
USING
LAPLACE
METHOD
AND
MARGINALIZING
THEM
OUT
OF
THE
MODEL
THIS
PRODUCES
A
VERY
SIMILAR
RESULT
TO
THE
MAXIMUM
LIKELIHOOD
CASE
WITH
THE
SAME
LENGTH
SCALE
FIGURE
HOW
EVER
AS
IS
TYPICAL
WITH
BAYESIAN
IMPLE
MENTATIONS
THE
CONFIDENCE
IS
APPRO
PRIATELY
SOMEWHAT
LOWER
TRAINING
DATA
TO
ACHIEVE
THIS
WE
IMPOSE
A
PENALTY
FOR
EVERY
NON
ZERO
WEIGHTED
TRAINING
EXAMPLE
AS
IN
THE
RELEVANCE
REGRESSION
MODEL
SECTION
WE
REPLACE
THE
NORMAL
PRIOR
OVER
THE
DUAL
PARAMETERS
Ψ
EQUATION
WITH
A
PRODUCT
OF
ONE
DIMENSIONAL
T
DISTRIBUTIONS
SO
THAT
P
R
Ψ
I
STUDΨI
Ν
I
APPLYING
THE
BAYESIAN
APPROACH
TO
THIS
MODEL
WITH
RESPECT
TO
THE
PARAMETERS
Ψ
IS
KNOWN
AS
RELEVANCE
VECTOR
CLASSIFICATION
FOLLOWING
THE
ARGUMENT
OF
SECTION
WE
RE
WRITE
EACH
STUDENT
T
DISTRIBUTION
AS
A
MARGINALIZATION
OF
A
JOINT
DISTRIBUTION
P
R
ΨI
HI
P
R
Ψ
ITT
NORMΨI
HI
TT
Ν
Ν
GAMHI
DHI
WHERE
THE
MATRIX
H
CONTAINS
THE
HIDDEN
VARIABLES
HI
I
ON
ITS
DIAGONAL
AND
ZEROS
ELSEWHERE
NOW
WE
CAN
WRITE
THE
MODEL
LIKELIHOOD
AS
P
R
W
X
P
R
W
X
Ψ
P
R
Ψ
DΨ
TTI
SIG
ΨT
K
X
XI
LNORMΨ
H
DTT
GAMHD
Ν
Ν
DHDΨ
NOW
WE
MAKE
TWO
APPROXIMATIONS
FIRST
WE
USE
THE
LAPLACE
APPROXIMATION
TO
DESCRIBE
THE
FIRST
TWO
TERMS
IN
THIS
INTEGRAL
AS
A
NORMAL
DISTRIBUTION
WITH
MEAN
Μ
AND
COVARIANCE
Σ
CENTERED
AT
THE
MAP
PARAMETERS
AND
USE
THE
FOLLOWING
RESULT
FOR
THE
INTEGRAL
OVER
Ψ
Q
Ψ
DΨ
Q
Μ
EXP
Ψ
Μ
T
Σ
Ψ
Μ
L
DΨ
Q
Μ
D
Σ
THIS
YIELDS
THE
EXPRESSION
P
R
W
X
TTI
Ν
Ν
L
WHERE
THE
MATRIX
H
CONTAINS
THE
HIDDEN
VARIABLES
HI
I
ON
THE
DIAGONAL
AND
WE
HAVE
USED
THE
GENERAL
RESULT
FOR
THE
LAPLACE
APPROXIMATION
IN
THE
SECOND
APPROXIMATION
WE
MAXIMIZE
OVER
THE
HIDDEN
VARIABLES
RATHER
THAN
INTEGRATE
OVER
THEM
THIS
YIELDS
THE
EXPRESSION
P
R
W
X
RTTI
Ν
Ν
TO
LEARN
THE
MODEL
WE
NOW
ALTERNATE
BETWEEN
UPDATING
THE
MEAN
AND
VARIANCE
Μ
AND
Σ
OF
THE
POSTERIOR
DISTRIBUTION
AND
UPDATING
THE
HIDDEN
VARIABLES
HI
TO
UPDATE
THE
MEAN
AND
VARIANCE
PARAMETERS
WE
FIND
THE
SOLUTION
Ψ
THAT
MAXIMIZES
FIGURE
RELEVANCE
VECTOR
REGRES
SION
WITH
RBF
KERNEL
WE
PLACE
A
PRIOR
OVER
THE
DUAL
PARAMETERS
Ψ
THAT
ENCOURAGES
SPARSITY
AFTER
LEARNING
THE
POSTERIOR
DISTRIBUTION
OVER
MOST
OF
THE
PARAMETERS
IS
TIGHTLY
CENTERED
AROUND
ZERO
AND
THEY
CAN
BE
DROPPED
FROM
THE
MODEL
LARGE
POINTS
INDI
CATE
DATA
EXAMPLES
ASSOCIATED
WITH
NON
ZERO
DUAL
PARAMETERS
THE
SOLU
TION
HERE
CAN
BE
COMPUTED
FROM
JUST
OF
THE
DATA
POINTS
BUT
NONETHELESS
CLASSIFIES
THE
DATA
ALMOST
AS
WELL
AS
THE
FULL
KERNEL
APPROACH
FIGURE
L
I
LOG
SIG
ΨT
K
X
XI
LL
LOG
NORMΨ
H
USING
THE
DERIVATIVES
L
SIG
A
W
K
X
X
HΨ
Ψ
I
I
I
I
I
L
SIG
A
SIG
A
K
X
X
K
X
X
H
AND
THEN
SET
I
I
I
I
I
Μ
Ψˆ
TO
UPDATE
THE
HIDDEN
VARIABLES
HI
WE
USE
THE
SAME
EXPRESSION
AS
FOR
RELEVANCE
VECTOR
REGRESSION
HNEW
HIΣII
Ν
I
Ν
AS
THIS
OPTIMIZATION
PROCEEDS
SOME
OF
THE
HIDDEN
VARIABLES
HI
WILL
BECOME
VERY
LARGE
THIS
MEANS
THAT
THE
PRIOR
OVER
THE
RELEVANT
PARAMETER
BECOMES
VERY
CONCENTRATED
AROUND
ZERO
AND
THAT
THE
ASSOCIATED
DATA
POINTS
CONTRIBUTE
NOTHING
TO
THE
FINAL
SOLUTION
THESE
CAN
BE
REMOVED
LEAVING
A
KERNELIZED
CLASSIFIER
THAT
DEPENDS
ONLY
SPARSELY
ON
THE
DATA
AND
CAN
HENCE
BE
EVALUATED
VERY
EFFICIENTLY
IN
INFERENCE
WE
AIM
TO
COMPUTE
THE
DISTRIBUTION
OVER
THE
WORLD
STATE
W
GIVEN
A
NEW
DATA
EXAMPLE
X
WE
TAKE
THE
FAMILIAR
STRATEGY
OF
APPROXIMATING
THE
POSTERIOR
DISTRIBUTION
OVER
THE
ACTIVATION
AS
P
R
A
P
R
ΨT
K
XT
X
NORMA
ΜA
R
NORMA
ΜT
K
X
X
K
X
X
ΣK
X
X
AND
THEN
APPROXIMATE
THE
PREDICTIVE
DISTRIBUTION
USING
EQUATION
AN
EXAMPLE
OF
RELEVANCE
VECTOR
CLASSIFICATION
IS
SHOWN
IN
FIGURE
WHICH
SHOWS
THAT
THE
DATA
SET
CAN
BE
DISCRIMINATED
BASED
ON
OF
THE
ORIGINAL
DATA
POINTS
THIS
RESULTS
IN
A
CONSIDERABLE
COMPUTATIONAL
SAVING
AND
THE
SIMPLER
SOLUTION
GUARDS
AGAINST
OVER
FITTING
OF
THE
TRAINING
SET
INCREMENTAL
FITTING
AND
BOOSTING
ALGORITHM
IN
THE
PREVIOUS
SECTION
WE
DEVELOPED
THE
RELEVANCE
VECTOR
CLASSIFICATION
MODEL
IN
WHICH
WE
APPLIED
A
PRIOR
THAT
ENCOURAGES
SPARSITY
IN
THE
DUAL
LOGISTIC
REGRESSION
PARAMETERS
Ψ
AND
HENCE
ENCOURAGED
THE
MODEL
TO
DEPEND
ON
ONLY
A
SUBSET
OF
THE
TRAINING
DATA
IT
IS
SIMILARLY
POSSIBLE
TO
DEVELOP
A
SPARSE
LOGISTIC
REGRESSION
METHOD
BY
PLACING
A
PRIOR
THAT
ENCOURAGES
SPARSITY
IN
THE
ORIGINAL
PARAMETERS
Φ
AND
HENCE
ENCOURAGES
THE
CLASSIFIER
TO
DEPEND
ONLY
ON
A
SUBSET
OF
THE
DATA
DIMENSIONS
THIS
IS
LEFT
AS
AN
EXERCISE
TO
THE
READER
IN
THIS
SECTION
WE
WILL
INVESTIGATE
A
DIFFERENT
APPROACH
TO
INDUCING
SPARSITY
WE
WILL
ADD
ONE
PARAMETER
AT
A
TIME
TO
THE
MODEL
IN
A
GREEDY
FASHION
IN
OTHER
WORDS
WE
ADD
THE
PARAMETER
THAT
IMPROVES
THE
OBJECTIVE
FUNCTION
MOST
AT
EACH
STAGE
AND
THEN
CONSIDER
THIS
FIXED
AS
THE
MOST
DISCRIMINATIVE
PARTS
OF
THE
MODEL
ARE
ADDED
FIRST
IT
IS
POSSIBLE
TO
TRUNCATE
THIS
PROCESS
AFTER
ONLY
A
SMALL
FRACTION
OF
THE
PARAMETERS
ARE
ADDED
AND
STILL
ACHIEVE
GOOD
RESULTS
THE
REMAINING
UNUSED
PARAMETERS
CAN
BE
CONSIDERED
AS
HAVING
A
VALUE
OF
ZERO
AND
SO
THIS
MODEL
ALSO
PROVIDES
A
SPARSE
SOLUTION
WE
TERM
THIS
APPROACH
INCREMENTAL
FITTING
WE
WILL
WORK
WITH
THE
ORIGINAL
FORMULATION
SO
THAT
THE
SPARSITY
IS
OVER
THE
DATA
DIMENSIONS
ALTHOUGH
THESE
IDEAS
CAN
EQUALLY
BE
ADAPTED
TO
THE
DUAL
CASE
TO
DESCRIBE
THE
INCREMENTAL
FITTING
PROCEDURE
LET
US
WORK
WITH
THE
NONLINEAR
FORMULATION
OF
LOGISTIC
REGRESSION
SECTION
WHERE
THE
PROBABILITY
OF
THE
CLASS
GIVEN
THE
DATA
WAS
DESCRIBED
AS
P
R
WI
XI
BERNWI
SIG
AI
WHERE
SIG
IS
THE
LOGISTIC
SIGMOID
FUNCTION
AND
THE
ACTIVATION
AI
IS
GIVEN
BY
AI
ΦT
ZI
ΦT
F
XI
AND
F
IS
A
NONLINEAR
TRANSFORMATION
THAT
RETURNS
THE
TRANSFORMED
VECTOR
ZI
TO
SIMPLIFY
THE
SUBSEQUENT
DESCRIPTION
WE
WILL
NOW
WRITE
THE
ACTIVATION
TERM
IN
A
SLIGHTLY
DIFFERENT
WAY
SO
THAT
THE
DOT
PRODUCT
IS
DESCRIBED
EXPLICITLY
AS
A
WEIGHTED
SUM
OF
INDIVIDUAL
NONLINEAR
FUNCTIONS
OF
THE
DATA
K
AI
ΦKF
XI
ΞK
K
HERE
F
IS
A
FIXED
NONLINEAR
FUNCTION
THAT
TAKES
THE
DATA
VECTOR
XI
AND
SOME
PARAMETERS
ΞK
AND
RETURNS
A
SCALAR
VALUE
IN
OTHER
WORDS
THE
KTH
ENTRY
OF
THE
TRANSFORMED
VECTOR
Z
ARISES
BY
PASSING
THE
DATA
X
THROUGH
THE
FUNCTION
WITH
THE
KTH
PARAMETERS
ΞK
EXAMPLE
FUNCTIONS
F
MIGHT
INCLUDE
RADIAL
BASIS
FUNCTIONS
Ξ
Α
F
X
Ξ
EXP
ARC
TAN
FUNCTIONS
Ξ
Α
X
Α
T
X
Α
AND
F
X
Ξ
ARCTAN
ΑT
X
IN
INCREMENTAL
LEARNING
WE
CONSTRUCT
THE
ACTIVATION
TERM
IN
EQUATION
PIECE
WISE
AT
EACH
STAGE
WE
ADD
A
NEW
TERM
LEAVING
ALL
OF
THE
PREVIOUS
TERMS
UNCHANGED
EXCEPT
THE
ADDITIVE
CONSTANT
SO
AT
THE
FIRST
STAGE
WE
USE
THE
ACTIVATION
AI
XI
AND
LEARN
THE
PARAMETERS
AND
USING
THE
MAXIMUM
LIKELIHOOD
APPROACH
AT
THE
SECOND
STAGE
WE
FIT
THE
FUNCTION
AI
XI
XI
AND
LEARN
THE
PARAMETERS
AND
WHILE
KEEPING
THE
REMAINING
PARAMETERS
AND
CONSTANT
AT
THE
KTH
STAGE
WE
FIT
A
MODEL
WITH
ACTIVATION
K
AI
ΦKF
XI
ΞK
K
AND
LEARN
THE
PARAMETERS
ΦK
AND
ΞK
WHILE
KEEPING
THE
REMAINING
PARAMETERS
ΦK
AND
ΞK
CONSTANT
AT
EACH
STAGE
THE
LEARNING
IS
CARRIED
OUT
USING
THE
MAXIMUM
LIKELIHOOD
AP
PROACH
WE
USE
A
NONLINEAR
OPTIMIZATION
PROCEDURE
TO
MAXIMIZE
THE
LOG
POSTERIOR
PROBABILITY
L
WITH
RESPECT
TO
THE
RELEVANT
PARAMETERS
THE
DERIVATIVES
REQUIRED
BY
THE
OPTIMIZATION
PROCEDURE
DEPEND
ON
THE
CHOICE
OF
NONLINEAR
FUNCTION
BUT
CAN
BE
COMPUTED
USING
THE
CHAIN
RULE
RELATIONS
EQUATION
THIS
PROCEDURE
IS
OBVIOUSLY
SUB
OPTIMAL
AS
WE
DO
NOT
LEARN
THE
PARAMETERS
TOGETHER
OR
EVEN
REVISIT
EARLY
PARAMETERS
ONCE
THEY
HAVE
BEEN
SET
HOWEVER
IT
HAS
THREE
NICE
PROPERTIES
IT
CREATES
SPARSE
MODELS
THE
WEIGHTS
ΦK
TEND
TO
DECREASE
AS
WE
MOVE
THROUGH
THE
SEQUENCE
AND
EACH
SUBSEQUENT
BASIS
FUNCTION
TENDS
TO
HAVE
LESS
INFLUENCE
ON
THE
MODEL
CONSEQUENTLY
THE
SERIES
CAN
BE
TRUNCATED
TO
THE
DESIRED
LENGTH
AND
THE
ASSOCIATED
PERFORMANCE
IS
LIKELY
TO
REMAIN
GOOD
FIGURE
INCREMENTAL
APPROACH
TO
FITTING
NONLINEAR
LOGISTIC
REGRESSION
MODEL
WITH
RBF
FUNCTIONS
A
BEFORE
FITTING
THE
ACTIVATION
AND
HENCE
THE
POSTERIOR
PROBABILITY
IS
UNIFORM
B
POSTERIOR
PROBABILITY
AFTER
FITTING
ONE
FUNCTION
MEAN
AND
SCALE
OF
RBF
SHOWN
IN
BLUE
C
E
AFTER
FITTING
TWO
THREE
AND
FOUR
RBFS
F
AFTER
FITTING
TEN
RBFS
THE
DATA
ARE
NOW
ALL
CLASSIFIED
CORRECTLY
AS
CAN
BE
SEEN
FROM
THE
DECISION
BOUNDARY
CYAN
LINE
THE
PREVIOUS
LOGISTIC
REGRESSION
MODELS
HAVE
BEEN
SUITED
TO
CASES
WHERE
EITHER
THE
DIMENSIONALITY
D
OF
THE
DATA
IS
SMALL
ORIGINAL
FORMULATION
OR
THE
NUM
BER
OF
TRAINING
EXAMPLES
I
IS
SMALL
DUAL
FORMULATION
HOWEVER
IT
IS
QUITE
POSSIBLE
THAT
NEITHER
OF
THESE
THINGS
IS
TRUE
A
STRONG
ADVANTAGE
OF
INCRE
MENTAL
FITTING
IS
THAT
IT
IS
STILL
PRACTICAL
WHEN
THE
DATA
ARE
HIGH
DIMENSIONAL
AND
THERE
ARE
A
LARGE
NUMBER
OF
TRAINING
EXAMPLES
DURING
TRAINING
WE
DO
NOT
NEED
TO
HOLD
ALL
OF
THE
TRANSFORMED
VECTORS
Z
IN
MEMORY
AT
ONCE
AT
THE
KTH
STAGE
WE
NEED
ONLY
THE
KTH
DIMENSION
OF
THE
TRANSFORMED
PARAMETERS
ZK
F
X
ΞK
AND
THE
AGGREGATE
OF
THE
PREVIOUS
CONTRIBUTIONS
TO
ACTIVATION
TERM
K
ΦKF
XI
Ξ
LEARNING
IS
RELATIVELY
INEXPENSIVE
BECAUSE
WE
ONLY
OPTIMIZE
A
FEW
PARAMETERS
AT
EACH
STAGE
FIGURE
ILLUSTRATES
THE
INCREMENTAL
APPROACH
TO
LEARNING
A
DATA
SET
USING
RADIAL
BASIS
FUNCTIONS
NOTICE
THAT
EVEN
AFTER
ONLY
A
FEW
FUNCTIONS
HAVE
BEEN
ADDED
TO
THE
SEQUENCE
THE
CLASSIFICATION
IS
SUBSTANTIALLY
CORRECT
NONETHELESS
IT
IS
WORTH
FIGURE
BOOSTING
A
WE
START
WITH
A
UNIFORM
PREDICTION
P
R
W
X
AND
B
INCREMENTALLY
ADD
A
STEP
FUNCTION
TO
THE
ACTIVATION
GREEN
LINE
INDI
CATES
POSITION
OF
STEP
IN
THIS
CASE
THE
PARAMETERS
OF
THE
STEP
FUNCTION
WERE
CHOSEN
GREEDILY
FROM
A
PRE
DETERMINED
SET
CONTAINING
ANGLES
EACH
WITH
OFFSETS
C
E
AS
SUBSEQUENT
FUNCTIONS
ARE
ADDED
THE
OVERALL
CLASSIFICATION
IMPROVES
F
HOWEVER
THE
FINAL
DECISION
SURFACE
CYAN
LINE
IS
COMPLEX
AND
DOES
NOT
INTERPOLATE
SMOOTHLY
BETWEEN
REGIONS
OF
HIGH
CONFIDENCE
CONTINUING
TO
TRAIN
THIS
MODEL
EVEN
AFTER
THE
TRAINING
DATA
ARE
CLASSIFIED
CORRECTLY
USUALLY
THE
MODEL
CONTINUES
TO
IMPROVE
AND
THE
CLASSIFICATION
PERFORMANCE
ON
TEST
DATA
WILL
CONTINUE
TO
INCREASE
FOR
SOME
TIME
BOOSTING
THERE
IS
A
SPECIAL
CASE
OF
THE
INCREMENTAL
APPROACH
TO
FITTING
NONLINEAR
LOGISTIC
RE
GRESSION
THAT
IS
COMMONLY
USED
IN
VISION
APPLICATIONS
CONSIDER
A
LOGISTIC
REGRESSION
MODEL
BASED
ON
A
SUM
OF
STEP
FUNCTIONS
AI
ΦKHEAVISIDE
ΑT
XI
ALGORITHM
K
WHERE
THE
FUNCTION
HEAVISIDE
RETURNS
IF
ITS
ARGUMENT
IS
LESS
THAN
AND
OTH
ERWISE
AS
USUAL
WE
HAVE
ATTACHED
A
TO
THE
START
OF
THE
DATA
X
SO
THAT
THE
PARAMETERS
ΑK
CONTAIN
BOTH
A
DIRECTION
ΑKD
IN
THE
D
DIMENSIONAL
SPACE
WHICH
DETERMINES
THE
DIRECTION
OF
THE
STEP
FUNCTION
AND
AN
OFFSET
THAT
DETERMINES
WHERE
THE
STEP
OCCURS
ONE
WAY
TO
THINK
ABOUT
THE
STEP
FUNCTIONS
IS
AS
WEAK
CLASSIFIERS
THEY
RETURN
OR
DEPENDING
ON
THE
VALUE
OF
XI
SO
EACH
CLASSIFIES
THE
DATA
THE
MODEL
COMBINES
THESE
WEAK
CLASSIFIERS
TO
COMPUTE
A
FINAL
STRONG
CLASSIFIER
SCHEMES
FOR
COMBINING
WEAK
CLASSIFIERS
IN
THIS
WAY
ARE
GENERICALLY
KNOWN
AS
BOOSTING
AND
THIS
PARTICULAR
MODEL
IS
CALLED
LOGITBOOST
UNFORTUNATELY
WE
CANNOT
SIMPLY
FIT
THIS
MODEL
USING
A
GRADIENT
BASED
OPTIMIZA
TION
APPROACH
BECAUSE
THE
DERIVATIVE
OF
THE
HEAVISIDE
STEP
FUNCTION
WITH
RESPECT
TO
THE
PARAMETERS
ΑK
IS
NOT
SMOOTH
CONSEQUENTLY
IT
IS
USUAL
TO
PREDEFINE
A
LARGE
SET
OF
J
WEAK
CLASSIFIERS
AND
ASSUME
THAT
EACH
PARAMETER
VECTOR
ΑK
IS
TAKEN
FROM
THIS
SET
SO
THAT
ΑK
Α
Α
J
AS
BEFORE
WE
LEARN
THE
LOGITBOOST
MODEL
INCREMENTALLY
BY
ADDING
ONE
TERM
AT
A
TIME
TO
THE
ACTIVATION
EQUATION
HOWEVER
NOW
WE
EXHAUSTIVELY
SEARCH
OVER
THE
WEAK
CLASSIFIERS
Α
Α
J
AND
FOR
EACH
WE
USE
NONLINEAR
OPTIMIZATION
TO
ESTIMATE
THE
WEIGHTS
AND
ΦK
WE
CHOOSE
THE
COMBINATION
ΑK
ΦK
THAT
IMPROVES
THE
LOG
LIKELIHOOD
THE
MOST
THIS
PROCEDURE
MAY
BE
MADE
EVEN
MORE
EFFICIENT
BUT
MORE
APPROXIMATE
BY
CHOOSING
THE
WEAK
CLASSIFIER
BASED
ON
THE
LOG
LIKELIHOOD
AFTER
JUST
A
SINGLE
NEWTON
OR
GRADIENT
DESCENT
STEP
IN
THE
NONLINEAR
OPTIMIZATION
STAGE
WHEN
WE
HAVE
SELECTED
THE
BEST
WEAK
CLASSIFIER
ΑK
WE
CAN
RETURN
AND
PERFORM
THE
FULL
OPTIMIZATION
OVER
THE
OFFSET
AND
WEIGHT
ΦK
NOTE
THAT
AFTER
EACH
CLASSIFIER
IS
ADDED
THE
RELATIVE
IMPORTANCE
OF
EACH
DATA
POINT
IS
EFFECTIVELY
CHANGED
THE
DATA
POINTS
CONTRIBUTE
TO
THE
DERIVATIVE
ACCORDING
TO
HOW
WELL
THEY
ARE
CURRENTLY
PREDICTED
EQUATION
CONSEQUENTLY
THE
LATER
WEAK
CLASSIFIERS
BECOME
MORE
SPECIALIZED
TO
THE
MORE
DIFFICULT
PARTS
OF
THE
DATA
SET
THAT
ARE
NOT
WELL
CLASSIFIED
BY
THE
EARLY
ONES
USUALLY
THESE
ARE
CLOSE
TO
THE
FINAL
DECISION
BOUNDARY
FIGURE
SHOWS
SEVERAL
ITERATIONS
OF
THE
BOOSTING
PROCEDURE
BECAUSE
THE
MODEL
IS
COMPOSED
FROM
STEP
FUNCTIONS
THE
FINAL
CLASSIFICATION
BOUNDARY
IS
IRREGULAR
AND
DOES
NOT
INTERPOLATE
SMOOTHLY
BETWEEN
THE
DATA
EXAMPLES
THIS
IS
A
POTENTIAL
DISADVANTAGE
OF
THIS
APPROACH
IN
GENERAL
A
CLASSIFIER
BASED
ON
ARC
TANGENT
FUNCTIONS
WHICH
ARE
ROUGHLY
SMOOTH
STEP
FUNCTIONS
WILL
HAVE
SUPERIOR
GENERALIZATION
AND
CAN
ALSO
BE
FIT
USING
CONTINUOUS
OPTIMIZATION
CLASSIFICATION
TREES
IN
THE
NONLINEAR
LOGISTIC
REGRESSION
MODEL
WE
CREATED
COMPLEX
DECISION
BOUNDARIES
USING
AN
ACTIVATION
FUNCTION
THAT
IS
A
LINEAR
COMBINATION
ΦT
Z
OF
NONLINEAR
FUNCTIONS
Z
F
X
OF
THE
DATA
X
WE
NOW
INVESTIGATE
AN
ALTERNATIVE
METHOD
TO
INDUCE
COMPLEX
DECISION
BOUNDARIES
WE
PARTITION
DATA
SPACE
INTO
DISTINCT
REGIONS
AND
APPLY
A
DIFFERENT
CLASSIFIER
IN
EACH
REGION
THE
BRANCHING
LOGISTIC
REGRESSION
MODEL
HAS
ACTIVATION
AI
G
XI
Ω
ΦT
XI
G
XI
Ω
ΦT
XI
FIGURE
BRANCHING
LOGISTIC
REGRESSION
A
THIS
DATA
SET
NEEDS
A
NONLINEAR
DECISION
SURFACE
CYAN
LINE
TO
CLASSIFY
THE
DATA
REASONABLY
B
THIS
LINEAR
ACTIVATION
IS
AN
EXPERT
THAT
IS
SPECIALIZED
TO
DESCRIBING
THE
RIGHT
HAND
SIDE
OF
THE
DATA
C
THIS
LINEAR
ACTIVATION
IS
AN
EXPERT
THAT
DESCRIBES
THE
LEFT
HAND
SIDE
OF
THE
DATA
D
A
GATING
FUNCTION
TAKES
THE
DATA
VECTOR
X
AND
RETURNS
A
NUMBER
BETWEEN
AND
WHICH
WE
WILL
USE
TO
DECIDE
WHICH
EXPERT
CONTRIBUTES
AT
EACH
DECISION
E
THE
FINAL
ACTIVATION
CONSISTS
OF
A
WEIGHTED
SUM
OF
THE
ACTIVATION
INDICATED
BY
THE
TWO
EXPERTS
WHERE
THE
WEIGHT
COMES
FROM
THE
GATING
FUNCTION
F
THE
FINAL
CLASSIFIER
PREDICTIONS
P
R
W
X
ARE
GENERATED
BY
PASSING
THIS
ACTIVATION
THROUGH
THE
LOGISTIC
SIGMOID
FUNCTION
THE
TERM
G
IS
A
GATING
FUNCTION
THAT
RETURNS
A
NUMBER
BETWEEN
AND
IF
THIS
GATING
FUNCTION
RETURNS
THEN
THE
ACTIVATION
WILL
BE
WHEREAS
IF
IT
RETURNS
THE
ACTIVATION
WILL
BE
IF
THE
GATING
RETURNS
AN
INTERMEDIATE
VALUE
THEN
THE
ACTIVATION
WILL
BE
A
WEIGHTED
SUM
OF
THESE
TWO
COMPONENTS
THE
GATING
FUNCTION
ITSELF
DEPENDS
ON
THE
DATA
XI
AND
TAKES
PARAMETERS
Ω
THIS
MODEL
INDUCES
A
COMPLEX
NONLINEAR
DECISION
BOUNDARY
FIGURE
WHERE
THE
TWO
LINEAR
FUNCTIONS
AND
ARE
SPECIALIZED
TO
DIFFERENT
REGIONS
OF
THE
DATA
SPACE
IN
THIS
CONTEXT
THEY
ARE
SOMETIMES
REFERRED
TO
AS
EXPERTS
THE
GATING
FUNCTION
COULD
TAKE
MANY
FORMS
BUT
AN
OBVIOUS
POSSIBILITY
IS
TO
USE
A
SECOND
LOGISTIC
REGRESSION
MODEL
IN
OTHER
WORDS
WE
COMPUTE
A
LINEAR
FUNCTION
ΩT
XI
OF
THE
DATA
THAT
IS
PASSED
THROUGH
A
LOGISTIC
SIGMOID
SO
THAT
G
XI
Ω
SIG
ΩT
XI
FIGURE
LOGISTIC
CLASSIFICATION
TREE
DATA
FLOWS
FROM
THE
ROOT
TO
THE
LEAVES
EACH
NODE
IS
A
GATING
FUNCTION
THAT
WEIGHTS
THE
CONTRIBUTIONS
OF
TERMS
IN
THE
SUB
BRANCHES
IN
THE
FINAL
ACTIVATION
THE
GRAY
REGION
INDI
CATES
VARIABLES
THAT
WOULD
BE
LEARNED
TOGETHER
IN
AN
INCREMENTAL
TRAINING
APPROACH
TO
LEARN
THIS
MODEL
WE
MAXIMIZE
THE
LOG
PROBABILITY
L
I
LOG
P
R
WI
XI
PROBLEM
Ω
AS
USUAL
THIS
CAN
BE
ACCOMPLISHED
USING
A
NONLINEAR
OPTIMIZATION
PROCEDURE
THE
PARAMETERS
CAN
BE
ESTIMATED
SIMULTANEOUSLY
OR
USING
A
COORDINATE
ASCENT
APPROACH
IN
WHICH
WE
ALTERNATELY
UPDATE
THE
THREE
SETS
OF
PARAMETERS
WE
CAN
EXTEND
THIS
IDEA
TO
CREATE
A
HIERARCHICAL
TREE
STRUCTURE
BY
NESTING
GATING
FUNCTIONS
FIGURE
FOR
EXAMPLE
CONSIDER
THE
ACTIVATION
AI
G
XI
Ω
XI
G
XI
ΦT
XI
G
XI
ΦT
XIL
G
XI
Ω
XI
G
XI
ΦT
XI
G
XI
ΦT
XIL
THIS
IS
AN
EXAMPLE
OF
A
CLASSIFICATION
TREE
TO
LEARN
THE
PARAMETERS
Θ
Ω
WE
COULD
TAKE
AN
INCREMENTAL
APPROACH
AT
THE
FIRST
STAGE
WE
FIT
THE
TOP
PART
OF
THE
TREE
EQUATION
SETTING
PARAMETERS
Ω
THEN
WE
FIT
THE
LEFT
BRANCH
SET
TING
PARAMETERS
AND
SUBSEQUENTLY
THE
RIGHT
BRANCH
SETTING
PARAMETERS
AND
SO
ON
THE
CLASSIFICATION
TREE
HAS
THE
POTENTIAL
ADVANTAGE
OF
SPEED
IF
EACH
GATING
FUNCTION
PRODUCES
A
BINARY
OUTPUT
LIKE
THE
HEAVISIDE
STEP
FUNCTION
THEN
EACH
DATA
POINT
PASSES
DOWN
JUST
ONE
OF
THE
OUTGOING
EDGES
FROM
EACH
NODE
AND
ENDS
UP
AT
A
SINGLE
LEAF
WHEN
EACH
BRANCH
IN
THE
TREE
IS
A
LINEAR
OPERATION
AS
IN
THIS
EXAMPLE
THESE
OPERATIONS
CAN
BE
AGGREGATED
TO
A
SINGLE
LINEAR
OPERATION
AT
EACH
LEAF
SINCE
EACH
DATA
POINT
RECEIVES
SPECIALIZED
PROCESSING
THE
TREE
NEED
NOT
USUALLY
BE
DEEP
AND
NEW
DATA
CAN
BE
CLASSIFIED
VERY
EFFICIENTLY
MULTI
CLASS
LOGISTIC
REGRESSION
MULTI
CLASS
LOGISTIC
REGRESSION
THROUGHOUT
THIS
CHAPTER
WE
HAVE
DISCUSSED
BINARY
CLASSIFICATION
WE
NOW
DISCUSS
HOW
TO
EXTEND
THESE
MODELS
TO
HANDLE
N
WORLD
STATES
ONE
POSSIBILITY
IS
TO
BUILD
N
ONE
AGAINST
ALL
BINARY
CLASSIFIERS
EACH
OF
WHICH
COMPUTES
THE
PROBABILITY
THAT
THE
NTH
CLASS
IS
PRESENT
AS
OPPOSED
TO
ANY
OF
THE
OTHER
CLASSES
THE
FINAL
LABEL
IS
ASSIGNED
ACCORDING
TO
THE
ONE
AGAINST
ALL
CLASSIFIER
WITH
THE
HIGHEST
PROBABILITY
THE
ONE
AGAINST
ALL
APPROACH
WORKS
IN
PRACTICE
BUT
IS
NOT
VERY
ELEGANT
A
MORE
PRINCIPLED
WAY
TO
COPE
WITH
MULTI
CLASS
CLASSIFICATION
PROBLEMS
IS
TO
DESCRIBE
THE
THE
POSTERIOR
P
R
W
X
AS
A
CATEGORICAL
DISTRIBUTION
WHERE
THE
PARAMETERS
Λ
ΛN
ARE
FUNCTIONS
OF
THE
DATA
X
P
R
W
X
CATW
Λ
X
WHERE
THE
PARAMETERS
ARE
IN
THE
RANGE
ΛN
AND
SUM
TO
ONE
N
ΛN
IN
CONSTRUCTING
THE
FUNCTION
Λ
X
WE
MUST
ENSURE
THAT
WE
OBEY
THESE
CONSTRAINTS
AS
FOR
THE
TWO
CLASS
LOGISTIC
REGRESSION
CASE
WE
WILL
BASE
THE
MODEL
ON
LIN
EAR
FUNCTIONS
OF
THE
DATA
X
AND
PASS
THESE
THROUGH
A
FUNCTION
THAT
ENFORCES
THE
CONSTRAINTS
TO
THIS
END
WE
DEFINE
N
ACTIVATIONS
ONE
FOR
EACH
CLASS
AN
ΦT
X
WHERE
ΦN
ARE
PARAMETER
VECTORS
WE
ASSUME
THAT
AS
USUAL
WE
HAVE
PRE
PENDED
A
TO
EACH
OF
THE
DATA
VECTORS
XI
SO
THAT
THE
FIRST
ENTRY
OF
EACH
PARAMETER
VECTORS
ΦN
REPRESENTS
AN
OFFSET
THE
NTH
ENTRY
OF
THE
FINAL
CATEGORICAL
DISTRIBUTION
IS
NOW
DEFINED
BY
ALGORITHM
Λ
SOFTMAX
A
A
A
EXP
AN
N
N
N
N
M
EXP
AM
THE
FUNCTION
SOFTMAX
TAKES
THE
N
ACTIVATIONS
AN
WHICH
CAN
TAKE
ANY
PROBLEM
REAL
NUMBER
AND
MAPS
THEM
TO
THE
N
PARAMETERS
ΛN
OF
THE
CATEGORICAL
DISTRIBUTION
WHICH
ARE
CONSTRAINED
TO
BE
POSITIVE
AND
SUM
TO
ONE
FIGURE
TO
LEARN
THE
PARAMETERS
Θ
ΦN
GIVEN
TRAINING
PAIRS
WI
XI
WE
OPTI
MIZE
THE
LOG
LIKELIHOOD
OF
THE
TRAINING
DATA
I
L
LOG
P
R
WI
XI
I
AS
FOR
THE
TWO
CLASS
CASE
THERE
IS
NO
CLOSED
FORM
EXPRESSION
FOR
THE
MAXIMUM
LIKELIHOOD
PARAMETERS
HOWEVER
THIS
IS
A
CONVEX
FUNCTION
AND
THE
MAXIMUM
CAN
BE
FOUND
USING
A
NONLINEAR
OPTIMIZATION
TECHNIQUE
SUCH
AS
THE
NEWTON
METHOD
THESE
TECHNIQUES
REQUIRE
THE
FIRST
AND
SECOND
DERIVATIVES
OF
THE
LOG
LIKELIHOOD
WITH
RESPECT
TO
THE
PARAMETERS
WHICH
ARE
GIVEN
BY
PROBLEM
CLASSIFICATION
MODELS
A
B
FIGURE
MULTI
CLASS
LOGISTIC
REGRESSION
A
WE
FORM
ONE
ACTIVATION
FOR
EACH
CLASS
BASED
ON
LINEAR
FUNCTIONS
OF
THE
DATA
B
WE
PASS
THESE
ACTIVATIONS
THROUGH
THE
SOFTMAX
FUNCTION
TO
CREATE
THE
DISTRIBUTION
P
R
W
X
WHICH
IS
SHOWN
HERE
AS
A
FUNCTION
OF
X
THE
SOFTMAX
FUNCTION
TAKES
THE
THREE
REAL
VALUED
ACTIVATIONS
AND
RETURNS
THREE
POSITIVE
VALUES
THAT
SUM
TO
ONE
ENSURING
THAT
THE
DISTRIBUTION
P
R
W
X
IS
A
VALID
PROBABILITY
DISTRIBUTION
FOR
ALL
X
L
ΦN
I
YIN
I
I
Δ
WI
N
XI
L
Y
Δ
M
N
Y
X
XT
WHERE
WE
DEFINE
THE
TERM
YIN
P
R
WI
N
XI
SOFTMAXN
AIN
IT
IS
POSSIBLE
TO
EXTEND
MULTI
CLASS
LOGISTIC
REGRESSION
IN
ALL
OF
THE
WAYS
THAT
WE
EXTENDED
THE
TWO
CLASS
MODEL
WE
CAN
CONSTRUCT
BAYESIAN
NONLINEAR
DUAL
AND
KERNELIZED
VERSIONS
IT
IS
POSSIBLE
TO
TRAIN
INCREMENTALLY
AND
COMBINE
WEAK
CLASSIFIERS
IN
A
BOOSTING
FRAMEWORK
HERE
WE
WILL
CONSIDER
TREE
STRUCTURED
MODELS
AS
THESE
ARE
VERY
COMMON
IN
MODERN
VISION
APPLICATIONS
RANDOM
TREES
FORESTS
AND
FERNS
ALGORITHM
IN
SECTION
WE
INTRODUCED
THE
IDEA
OF
TREE
STRUCTURED
CLASSIFIERS
IN
WHICH
THE
PROCESSING
FOR
EACH
DATA
EXAMPLE
IS
DIFFERENT
AND
BECOMES
STEADILY
MORE
SPECIALIZED
THIS
IDEA
HAS
RECENTLY
BECOME
EXTREMELY
POPULAR
FOR
MULTI
CLASS
PROBLEMS
IN
THE
FORM
OF
RANDOM
CLASSIFICATION
TREES
AS
FOR
THE
TWO
CLASS
CASE
THE
KEY
IDEA
IS
TO
CONSTRUCT
A
BINARY
TREE
WHERE
AT
EACH
NODE
THE
DATA
ARE
EVALUATED
TO
DETERMINE
WHETHER
IT
WILL
PASS
TO
THE
LEFT
OR
THE
RIGHT
BRANCH
UNLIKE
IN
SECTION
WE
WILL
ASSUME
THAT
EACH
DATA
POINT
PASSES
INTO
JUST
ONE
BRANCH
IN
A
RANDOM
CLASSIFICATION
TREE
THE
DATA
ARE
EVALUATED
AGAINST
A
FUNCTION
Q
X
THAT
WAS
RANDOMLY
CHOSEN
FROM
A
PREDEFINED
FAMILY
OF
POSSIBLE
FUNCTIONS
FOR
EXAMPLE
THIS
MIGHT
BE
THE
RESPONSE
OF
A
RANDOMLY
CHOSEN
FILTER
THE
DATA
PROCEEDS
ONE
WAY
IN
THE
TREE
IF
THE
RESPONSE
OF
THIS
FUNCTION
EXCEEDS
A
THRESHOLD
Τ
AND
THE
OTHER
WAY
IF
NOT
WHILE
THE
FUNCTIONS
ARE
CHOSEN
RANDOMLY
THE
THRESHOLD
IS
CAREFULLY
SELECTED
WE
SELECT
THE
THRESHOLD
THAT
MAXIMIZES
THE
LOG
LIKELIHOOD
L
OF
THE
DATA
L
I
HEAVISIDE
Q
XI
Τ
LOG
L
LL
HEAVISIDE
Q
XI
Τ
LOG
R
LL
HERE
THE
FIRST
TERM
REPRESENTS
THE
CONTRIBUTION
OF
THE
DATA
THAT
PASSES
DOWN
THE
LEFT
BRANCH
AND
THE
SECOND
TERM
REPRESENTS
THE
CONTRIBUTION
OF
THE
DATA
THAT
PASSES
DOWN
THE
RIGHT
BRANCH
IN
EACH
CASE
THE
DATA
ARE
EVALUATED
AGAINST
A
CATEGORICAL
DISTRIBUTION
WITH
PARAMETERS
Λ
L
AND
Λ
R
RESPECTIVELY
THESE
PARAMETERS
ARE
SET
USING
MAXIMUM
LIKELIHOOD
Λ
L
I
I
Δ
WI
K
HEAVISIDE
Q
XI
Τ
K
I
I
HEAVISIDE
Q
XI
Τ
Λ
R
I
I
Δ
WI
K
HEAVISIDE
Q
XI
Τ
K
I
I
HEAVISIDE
Q
XI
Τ
THE
LOG
LIKELIHOOD
IS
NOT
A
SMOOTH
FUNCTION
OF
THE
THRESHOLD
Τ
AND
SO
IN
PRACTICE
WE
MAXIMIZE
THE
LOG
LIKELIHOOD
BY
EMPIRICALLY
TRYING
A
NUMBER
OF
DIFFERENT
THRESHOLD
VALUES
AND
CHOOSING
THE
ONE
THAT
GIVES
THE
BEST
RESULT
WE
THEN
PERFORM
THIS
SAME
PROCEDURE
RECURSIVELY
THE
DATA
THAT
PASS
TO
THE
LEFT
BRANCH
HAS
A
NEW
RANDOMLY
CHOSEN
CLASSIFIER
APPLIED
TO
THEM
AND
A
NEW
THRESHOLD
IS
CHOSEN
THAT
SPLITS
IT
AGAIN
THIS
CAN
BE
DONE
WITHOUT
RECOURSE
TO
THE
DATA
IN
THE
RIGHT
BRANCH
WHEN
WE
CLASSIFY
A
NEW
DATA
EXAMPLE
X
WE
PASS
IT
DOWN
THE
TREE
UNTIL
IT
REACHES
ONE
OF
THE
LEAVES
THE
POSTERIOR
DISTRIBUTION
P
R
W
X
OVER
THE
WORLD
STATE
W
IS
SET
TO
CATW
Λ
WHERE
THE
PARAMETERS
Λ
ARE
THE
CATEGORICAL
PARAMETERS
ASSOCIATED
WITH
THIS
LEAF
DURING
THE
TRAINING
PROCESS
THE
RANDOM
CLASSIFICATION
TREE
IS
ATTRACTIVE
BECAUSE
IT
IS
VERY
FAST
TO
TRAIN
AFTER
ALL
MOST
OF
ITS
PARAMETERS
ARE
CHOSEN
RANDOMLY
IT
CAN
ALSO
BE
TRAINED
WITH
VERY
LARGE
AMOUNTS
OF
DATA
AS
ITS
COMPLEXITY
IS
LINEAR
IN
THE
NUMBER
OF
DATA
EXAMPLES
THERE
ARE
TWO
IMPORTANT
VARIATIONS
ON
THIS
MODEL
A
FERN
IS
A
TREE
WHERE
THE
RANDOMLY
CHOSEN
FUNCTIONS
AT
EACH
LEVEL
OF
THE
TREE
ARE
CONSTRAINED
TO
BE
THE
SAME
IN
OTHER
WORDS
THE
DATA
THAT
PASS
THROUGH
THE
LEFT
AND
RIGHT
BRANCHES
AT
ANY
NODE
ARE
SUBSEQUENTLY
ACTED
ON
BY
THE
SAME
FUNCTION
ALTHOUGH
THE
THRESHOLD
LEVEL
MAY
OPTIONALLY
BE
DIFFERENT
IN
EACH
BRANCH
IN
PRACTICE
THIS
MEANS
THAT
EVERY
DATA
POINT
IS
ACTED
ON
BY
THE
SAME
SEQUENCE
OF
FUNCTIONS
THIS
CAN
MAKE
IMPLEMENTATION
EXTREMELY
EFFICIENT
WHEN
WE
ARE
EVALUATING
THE
CLASSIFIER
REPEATEDLY
A
RANDOM
FOREST
IS
A
COLLECTION
OF
RANDOM
TREES
EACH
OF
WHICH
USES
A
DIFFERENT
RANDOMLY
CHOSEN
SET
OF
FUNCTIONS
BY
AVERAGING
TOGETHER
THE
PROBABILITIES
P
R
W
X
PREDICTED
BY
THESE
TREES
A
MORE
ROBUST
CLASSIFIER
IS
PRODUCED
ONE
WAY
TO
THINK
OF
THIS
IS
AS
APPROXIMATING
THE
BAYESIAN
APPROACH
WE
ARE
CONSTRUCTING
THE
FINAL
ANSWER
BY
TAKING
A
WEIGHTED
SUM
OF
THE
PREDICTIONS
SUGGESTED
BY
DIFFERENT
SETS
OF
PARAMETERS
RELATION
TO
NON
PROBABILISTIC
MODELS
IN
THIS
CHAPTER
WE
HAVE
DESCRIBED
A
FAMILY
OF
PROBABILISTIC
ALGORITHMS
FOR
CLASSI
FICATION
EACH
IS
BASED
ON
MAXIMIZING
EITHER
THE
LOG
BERNOULLI
PROBABILITY
OF
THE
TRAINING
CLASS
LABELS
GIVEN
THE
DATA
TWO
CLASS
CASE
OR
THE
LOG
CATEGORICAL
PROBABILITY
OF
THE
TRAINING
CLASS
LABELS
GIVEN
THE
DATA
MULTI
CLASS
CASE
HOWEVER
IT
IS
MORE
COMMON
IN
THE
COMPUTER
VISION
LITERATURE
TO
USE
NON
PROBABILISTIC
CLASSIFICATION
ALGORITHMS
SUCH
AS
THE
MULTILAYER
PERCEPTRON
ADABOOST
OR
SUPPORT
VECTOR
CLASSIFICATION
AT
THEIR
CORE
THESE
ALGORITHMS
OPTIMIZE
DIFFERENT
OBJECTIVE
FUNCTIONS
AND
SO
ARE
NEITHER
DIRECTLY
EQUIVALENT
TO
EACH
OTHER
NOR
TO
THE
MODELS
IN
THIS
CHAPTER
WE
CHOSE
TO
DESCRIBE
THE
LESS
COMMON
PROBABILISTIC
ALGORITHMS
BECAUSE
THEY
HAVE
NO
SERIOUS
DISADVANTAGES
RELATIVE
TO
NON
PROBABILISTIC
TECHNIQUES
THEY
NATURALLY
PRODUCE
ESTIMATES
OF
CERTAINTY
THEY
ARE
EASILY
EXTENSIBLE
TO
THE
MULTI
CLASS
CASE
WHEREAS
NON
PROBABILISTIC
ALGORITHMS
USUALLY
RELY
ON
ONE
AGAINST
ALL
FORMULATIONS
AND
THEY
ARE
MORE
EASILY
RELATED
TO
ONE
ANOTHER
AND
TO
THE
REST
OF
THE
BOOK
IN
SHORT
IT
CAN
REASONABLY
BE
ARGUED
THAT
THE
DOMINANCE
OF
NON
PROBABILISTIC
APPROACHES
TO
CLASSIFICATION
IS
LARGELY
FOR
HISTORICAL
REASONS
WE
WILL
NOW
BRIEFLY
DESCRIBE
THE
RELATIONSHIP
BETWEEN
OUR
MODELS
AND
COMMON
NON
PROBABILISTIC
AP
PROACHES
THE
MULTI
LAYER
PERCEPTRON
OR
NEURAL
NETWORK
IS
VERY
SIMILAR
TO
OUR
NONLIN
EAR
LOGISTIC
REGRESSION
MODEL
IN
THE
SPECIAL
CASE
WHERE
THE
NONLINEAR
TRANSFORM
CONSISTS
OF
A
SET
OF
SIGMOID
FUNCTIONS
APPLIED
TO
LINEAR
PROJECTIONS
OF
DATA
E
G
ZK
ARCTAN
ΑT
X
IN
THE
MLP
LEARNING
IS
KNOWN
AS
BACK
PROPAGATION
AND
THE
TRANSFORMED
VARIABLE
Z
IS
KNOWN
AS
THE
HIDDEN
LAYER
ADABOOST
IS
VERY
CLOSELY
RELATED
TO
THE
THE
LOGITBOOST
MODEL
DESCRIBED
IN
THIS
CHAPTER
BUT
ADABOOST
IS
NOT
PROBABILISTIC
PERFORMANCE
OF
THE
TWO
ALGORITHMS
IS
SIMILAR
THE
SUPPORT
VECTOR
MACHINE
SVM
IS
SIMILAR
TO
RELEVANCE
VECTOR
CLASSIFICATION
IT
IS
A
KERNELIZED
CLASSIFIER
THAT
DEPENDS
SPARSELY
ON
THE
DATA
IT
HAS
THE
ADVANTAGE
THAT
ITS
OBJECTIVE
FUNCTION
IS
CONVEX
WHEREAS
THE
OBJECTIVE
FUNCTION
IN
RELEVANCE
VECTOR
CLASSIFICATION
IS
NON
CONVEX
AND
ONLY
GUARANTEES
TO
CONVERGE
TO
A
LOCAL
MINIMUM
HOWEVER
THE
SVM
HAS
SEVERAL
DISADVANTAGES
IT
DOES
NOT
ASSIGN
CERTAINTY
TO
ITS
CLASS
PREDICTIONS
IT
IS
NOT
SO
EASILY
EXTENDED
TO
THE
MULTI
CLASS
CASE
IT
PRODUCES
SOLUTIONS
THAT
ARE
LESS
SPARSE
THAN
RELEVANCE
VECTOR
CLASSIFICATION
AND
IT
PLACES
MORE
RESTRICTIONS
ON
THE
FORM
OF
THE
KERNEL
FUNCTION
IN
PRACTICE
CLASSIFICATION
PERFORMANCE
OF
THE
TWO
MODELS
IS
AGAIN
SIMILAR
APPLICATIONS
WE
NOW
PRESENT
A
NUMBER
OF
EXAMPLES
OF
THE
USE
OF
CLASSIFICATION
IN
COMPUTER
VISION
FROM
THE
RESEARCH
LITERATURE
IN
MANY
OF
THE
EXAMPLES
THE
METHOD
USED
WAS
NON
PROBABILISTIC
E
G
ADABOOST
BUT
IS
VERY
CLOSELY
RELATED
TO
THE
ALGORITHMS
IN
THIS
CHAPTER
AND
ONE
WOULD
NOT
EXPECT
THE
PERFORMANCE
TO
DIFFER
SIGNIFICANTLY
IF
THESE
WERE
SUBSTITUTED
GENDER
CLASSIFICATION
THE
ALGORITHMS
IN
THIS
CHAPTER
WERE
MOTIVATED
BY
THE
PROBLEM
OF
GENDER
DETECTION
IN
UNCONSTRAINED
FACIAL
IMAGES
THE
GOAL
IS
TO
ASSIGN
A
LABEL
W
INDICATING
WHETHER
A
SMALL
PATCH
OF
AN
IMAGE
X
CONTAINS
A
MALE
OR
A
FEMALE
FACE
PRINCE
AGHAJANIAN
DEVELOPED
A
SYSTEM
OF
THIS
TYPE
FIRST
A
BOUNDING
BOX
AROUND
THE
FACE
WAS
IDENTIFIED
USING
A
FACE
DETECTOR
SEE
NEXT
SECTION
THE
DATA
WITHIN
THIS
BOUNDING
BOX
WAS
RESIZED
TO
CONVERTED
TO
GRAYSCALE
AND
HISTOGRAM
EQUALIZED
THE
RESULTING
IMAGE
WAS
CONVOLVED
WITH
A
BANK
OF
GABOR
FUNCTIONS
AND
THE
FILTERED
IMAGES
SAMPLED
AT
REGULAR
INTERVALS
THAT
WERE
PROPORTIONATE
TO
THE
WAVELENGTH
TO
CREATE
A
FINAL
FEATURE
VECTOR
OF
LENGTH
EACH
DIMENSION
WAS
WHITENED
TO
HAVE
MEAN
ZERO
AND
UNIT
STANDARD
DEVIATION
CHAPTER
CONTAINS
INFORMATION
ABOUT
THESE
AND
OTHER
PREPROCESSING
METHODS
A
TRAINING
DATABASE
OF
EXAMPLES
WAS
USED
TO
LEARN
A
NONLINEAR
LOGISTIC
REGRESSION
MODEL
OF
THE
FORM
P
R
W
X
BERN
L
I
I
WI
EXP
Φ
K
Φ
F
X
Ξ
WHERE
THE
NONLINEAR
FUNCTIONS
F
WERE
ARC
TANGENTS
OF
LINEAR
PROJECTIONS
OF
THE
DATA
SO
THAT
F
XI
ΞK
ARCTAN
ΞT
XI
AS
USUAL
THE
DATA
WERE
AUGMENTED
BY
PREPENDING
A
SO
THE
PROJECTION
VECTORS
ΞK
WERE
OF
LENGTH
D
THIS
MODEL
WAS
LEARNED
USING
AN
INCREMENTAL
APPROACH
SO
THAT
AT
EACH
STAGE
THE
PARAMETERS
ΦK
AND
ΞK
WERE
MODIFIED
THE
SYSTEM
ACHIEVED
PERFORMANCE
WITH
K
ARC
TANGENT
FUNCTIONS
ON
A
CHALLENGING
REAL
WORLD
DATABASE
THAT
CONTAINED
LARGE
VARIATIONS
IN
SCALE
POSE
LIGHTING
AND
EXPRESSION
SIMILAR
TO
THE
FACES
IN
FIGURE
HUMAN
OBSERVERS
MANAGED
ONLY
PERFORMANCE
ON
THE
SAME
DATABASE
USING
THE
RESIZED
FACE
REGION
ALONE
CLASSIFICATION
MODELS
A
B
D
C
FIGURE
FAST
FACE
DETECTION
USING
A
BOOSTING
METHOD
VIOLA
JONES
A
EACH
WEAK
CLASSIFIER
CONSISTS
OF
THE
RESPONSE
OF
THE
IMAGE
TO
A
HAAR
LIKE
FILTER
WHICH
IS
THEN
PASSED
THROUGH
A
STEP
FUNCTION
B
THE
FIRST
TWO
WEAK
CLASSIFIERS
LEARNED
IN
THIS
IMPLEMENTATION
HAVE
CLEAR
INTERPRETATIONS
THE
FIRST
RESPONDS
TO
THE
DARK
HORIZONTAL
REGION
BELONGING
TO
THE
EYES
AND
THE
SECOND
RESPONDS
TO
THE
RELATIVE
BRIGHTNESS
OF
THE
BRIDGE
OF
THE
NOSE
C
THE
DATA
PASSES
THROUGH
A
CASCADE
MOST
REGIONS
CAN
BE
QUICKLY
REJECTED
AFTER
EVALUATING
ONLY
A
FEW
WEAK
CLASSIFIERS
AS
THEY
LOOK
NOTHING
LIKE
FACES
MORE
AMBIGUOUS
REGIONS
UNDERGO
FURTHER
PREPROCESSING
D
EXAMPLE
RESULTS
ADAPTED
FROM
VIOLA
JONES
FACE
AND
PEDESTRIAN
DETECTION
BEFORE
WE
CAN
DETERMINE
THE
GENDER
OF
A
FACE
WE
MUST
FIRST
FIND
IT
IN
FACE
DETECTION
FIGURE
WE
ASSIGN
A
LABEL
W
TO
A
SMALL
REGION
OF
THE
IMAGE
X
INDICATING
WHETHER
A
FACE
IS
PRESENT
W
OR
NOT
W
TO
ENSURE
THAT
THE
FACE
IS
FOUND
THIS
PROCESS
IS
REPEATED
AT
EVERY
POSITION
AND
SCALE
IN
THE
IMAGE
AND
CONSEQUENTLY
THE
CLASSIFIER
MUST
BE
VERY
FAST
VIOLA
JONES
PRESENTED
A
FACE
DETECTION
SYSTEM
BASED
ON
ADABOOST
FIG
URE
THIS
IS
A
NON
PROBABILISTIC
ANALOGUE
OF
THE
BOOSTING
METHODS
DESCRIBED
IN
SECTION
THE
FINAL
CLASSIFICATION
IS
BASED
ON
THE
SIGN
OF
A
SUM
OF
NONLINEAR
FUNCTIONS
OF
THE
DATA
K
A
ΦKF
X
ΞK
K
WHERE
THE
NONLINEAR
FUNCTIONS
F
ARE
HEAVISIDE
STEP
FUNCTIONS
OF
PROJECTIONS
OF
THE
DATA
WEAK
CLASSIFIERS
GIVING
A
RESPONSE
OF
ZERO
OR
ONE
FOR
EACH
POSSIBLE
DATA
VECTOR
X
SO
THAT
F
X
ΞK
HEAVISIDE
ΞT
X
APPLICATIONS
AS
USUAL
THE
DATA
VECTOR
X
WAS
PREPENDED
WITH
A
TO
ACCOUNT
FOR
AN
OFFSET
THE
SYSTEM
WAS
TRAINED
ON
FACES
AND
NON
FACE
REGIONS
EACH
OF
WHICH
WAS
REPRESENTED
AS
A
IMAGE
PATCH
SINCE
THE
MODEL
IS
NOT
SMOOTH
DUE
TO
THE
STEP
FUNCTION
GRADIENT
BASED
OPTIMIZATION
IS
UNSUITABLE
AND
SO
VIOLA
JONES
EXHAUSTIVELY
SEARCHED
THROUGH
A
VERY
LARGE
NUMBER
OF
PRE
DEFINED
PROJECTIONS
ΞK
THERE
WERE
TWO
ASPECTS
OF
THE
DESIGN
THAT
ENSURED
THAT
THE
SYSTEM
RAN
QUICKLY
THE
STRUCTURE
OF
THE
CLASSIFIER
WAS
EXPLOITED
TRAINING
IN
BOOSTING
IS
INCREMEN
TAL
THE
WEAK
CLASSIFIERS
NONLINEAR
FUNCTIONS
OF
THE
DATA
ARE
INCREMENTALLY
ADDED
TO
CREATE
AN
INCREASINGLY
SOPHISTICATED
STRONG
CLASSIFIER
VIOLA
JONES
EXPLOITED
THIS
STRUCTURE
WHEN
THEY
RAN
THE
CLASSIFIER
THEY
REJECT
RE
GIONS
THAT
ARE
VERY
UNLIKELY
TO
BE
FACES
BASED
ON
RESPONSES
OF
THE
FIRST
FEW
WEAK
CLASSIFIERS
AND
ONLY
SUBJECT
MORE
AMBIGUOUS
REGIONS
TO
FURTHER
PROCESS
ING
THIS
IS
KNOWN
AS
A
CASCADE
STRUCTURE
DURING
TRAINING
THE
LATER
STAGES
OF
THE
CASCADE
ARE
TRAINED
WITH
NEW
NEGATIVE
EXAMPLES
THAT
WERE
NOT
REJECTED
BY
THE
EARLIER
STAGES
THE
PROJECTIONS
ΞK
WERE
CAREFULLY
CHOSEN
SO
THAT
THEY
WERE
VERY
FAST
TO
EVAL
UATE
THEY
CONSISTED
OF
HAAR
LIKE
FILTERS
SECTION
WHICH
REQUIRE
ONLY
A
FEW
OPERATIONS
TO
COMPUTE
THE
FINAL
SYSTEM
CONSISTED
OF
WEAK
CLASSIFIERS
DIVIDED
INTO
A
STAGE
CAS
CADE
IT
FOUND
OF
FRONTAL
FACES
ACROSS
IMAGES
WITH
A
FALSE
POSITIVE
RATE
OF
LESS
THAN
PER
FRAME
AND
PROCESSED
IMAGES
IN
FRACTIONS
OF
A
SECOND
VIOLA
ET
AL
DEVELOPED
A
SIMILAR
SYSTEM
FOR
DETECTING
PEDESTRIANS
IN
VIDEO
SEQUENCES
FIGURE
THE
MAIN
MODIFICATION
WAS
TO
EXTEND
THE
SET
OF
WEAK
CLASSIFIERS
TO
ENCOMPASS
FEATURES
THAT
SPAN
MORE
THAN
ONE
FRAME
AND
HENCE
SELECT
FOR
THE
PARTICULAR
TEMPORAL
PATTERNS
ASSOCIATED
WITH
HUMAN
MOTION
TO
THIS
END
THEIR
SYSTEM
USED
NOT
ONLY
THE
IMAGE
DATA
ITSELF
BUT
ALSO
THE
DIFFERENCE
IMAGE
BETWEEN
ADJACENT
FRAMES
AND
SIMILAR
DIFFERENCE
IMAGES
WHEN
TAKEN
AFTER
OFFSETTING
THE
FRAMES
IN
EACH
OF
FOUR
DIRECTIONS
THE
FINAL
SYSTEM
ACHIEVED
AN
DETECTION
RATE
WITH
A
FALSE
ALARM
RATE
OF
WHICH
CORRESPONDS
TO
ONE
FALSE
POSITIVE
FOR
EVERY
TWO
FRAMES
SEMANTIC
SEGMENTATION
THE
GOAL
OF
SEMANTIC
SEGMENTATION
IS
TO
ASSIGN
A
LABEL
W
M
TO
EACH
PIXEL
INDICATING
WHICH
OF
M
OBJECTS
IS
PRESENT
BASED
ON
THE
LOCAL
IMAGE
DATA
X
SHOTTON
ET
AL
DEVELOPED
A
SYSTEM
KNOWN
AS
TEXTONBOOST
THAT
WAS
BASED
ON
A
NON
PROBABILISTIC
BOOSTING
ALGORITHM
CALLED
JOINTBOOST
TORRALBA
ET
AL
THE
DECISION
WAS
BASED
ON
A
ONE
AGAINST
ALL
STRATEGY
IN
WHICH
M
BINARY
CLASSIFIERS
ARE
COMPUTED
BASED
ON
THE
WEIGHTED
SUMS
K
AM
ΦKMF
X
ΞK
K
CLASSIFICATION
MODELS
A
FRAME
FRAME
DIFFERENCE
UP
DOWN
LEFT
RIGHT
B
C
FIGURE
BOOSTING
METHODS
BASED
ON
THE
THRESHOLDED
RESPONSES
OF
HAAR
FUNCTIONS
HAVE
ALSO
BEEN
USED
FOR
PEDESTRIAN
DETECTION
IN
VIDEO
FOOTAGE
A
TO
IMPROVE
DETECTION
RATES
TWO
SUBSEQUENT
FRAMES
ARE
USED
THE
ABSOLUTE
DIFFERENCE
BETWEEN
THE
FRAMES
IS
COMPUTED
AS
IS
THE
DIFFERENCE
WHEN
ONE
OF
THE
FRAMES
IS
OFFSET
IN
EACH
OF
FOUR
DIRECTIONS
THE
SET
OF
POTENTIAL
WEAK
CLASSIFIERS
CONSISTS
OF
HAAR
FUNCTIONS
APPLIED
TO
ALL
SIX
OF
THESE
REPRESENTATIONS
B
C
EXAMPLE
RESULTS
ADAPTED
FROM
VIOLA
ET
AL
QC
SPRINGER
WHERE
THE
NONLINEAR
FUNCTIONS
F
WERE
ONCE
MORE
BASED
ON
HEAVISIDE
STEP
FUNCTIONS
NOTE
THAT
THE
WEIGHTED
SUMS
ASSOCIATED
WITH
EACH
OBJECT
CLASS
SHARE
THE
SAME
NONLINEAR
FUNCTIONS
BUT
WEIGHT
THEM
DIFFERENTLY
AFTER
COMPUTING
THESE
SERIES
THE
DECISION
IS
ASSIGNED
BASED
ON
THE
ACTIVATION
AM
THAT
IS
THE
GREATEST
SHOTTON
ET
AL
BASED
THE
NONLINEAR
FUNCTIONS
ON
A
TEXTON
REPRESENTATION
OF
THE
IMAGE
EACH
PIXEL
IN
THE
IMAGE
IS
REPLACED
BY
A
DISCRETE
INDEX
INDICATING
THE
TYPE
OF
TEXTURE
PRESENT
AT
THAT
POSITION
SEE
SECTION
EACH
NONLINEAR
FUNCTION
CONSIDERS
ONE
OF
THESE
TEXTON
TYPES
AND
COMPUTES
THE
NUMBER
OF
TIMES
THAT
IT
IS
FOUND
WITHIN
A
RECTANGULAR
AREA
THIS
AREA
HAS
A
FIXED
SPATIAL
DISPLACEMENT
FROM
THE
PIXEL
UNDER
CONSIDERATION
FIGURE
F
IF
THIS
DISPLACEMENT
IS
ZERO
THEN
THE
FUNCTION
PROVIDES
EVIDENCE
ABOUT
THE
PIXEL
DIRECTLY
E
G
IT
LOOKS
LIKE
GRASS
IF
THE
SPATIAL
DISPLACEMENT
IS
LARGER
THEN
THE
FUNCTION
PROVIDES
EVIDENCE
OF
THE
LOCAL
CONTEXT
E
G
THERE
IS
GRASS
NEARBY
SO
THIS
PIXEL
MAY
BELONG
TO
A
COW
FOR
EACH
NONLINEAR
FUNCTION
AN
OFFSET
IS
ADDED
TO
THE
TEXTON
COUNT
AND
THE
RESULT
IS
PASSED
THROUGH
A
STEP
FUNCTION
THE
SYSTEM
WAS
LEARNED
INCREMENTALLY
BY
ASSESSING
EACH
OF
A
SET
OF
RANDOMLY
CHOSEN
CLASSIFIERS
DEFINED
BY
THE
CHOICE
OF
TEXTON
RECTANGULAR
REGION
AND
OFFSET
AND
CHOOSING
THE
BEST
AT
THE
CURRENT
STAGE
THE
FULL
SYSTEM
ALSO
INCLUDED
A
POST
PROCESSING
STEP
IN
WHICH
THE
RESULT
WAS
REFINED
USING
A
CONDITIONAL
RANDOM
FIELD
MODEL
SEE
CHAPTER
IT
ACHIEVED
APPLICATIONS
FIGURE
SEMANTIC
IMAGE
LABELING
USING
TEXTONBOOST
A
ORIGINAL
IM
AGE
B
IMAGE
CONVERTED
TO
TEXTONS
A
DISCRETE
VALUE
AT
EACH
PIXEL
INDICATING
THE
TYPE
OF
TEXTURE
THAT
IS
PRESENT
C
THE
SYSTEM
WAS
BASED
ON
WEAK
CLAS
SIFIERS
THAT
COUNT
THE
NUMBER
OF
TEXTONS
OF
A
CERTAIN
TYPE
WITHIN
A
RECTANGLE
THAT
IS
OFFSET
FROM
THE
CURRENT
POSITION
YELLOW
CROSS
D
THIS
PROVIDES
BOTH
INFORMATION
ABOUT
THE
OBJECT
ITSELF
CONTAINS
SHEEP
LIKE
TEXTONS
AND
NEARBY
OBJECTS
NEAR
TO
GRASS
LIKE
TEXTONS
E
F
ANOTHER
EXAMPLE
OF
A
WEAK
CLAS
SIFIER
G
TEST
IMAGE
H
PER
PIXEL
CLASSIFICATION
IS
NOT
VERY
PRECISE
AT
THE
EDGES
OF
OBJECTS
AND
SO
I
A
CONDITIONAL
RANDOM
FIELD
IS
USED
TO
IMPROVE
THE
RESULT
J
EXAMPLES
OF
RESULTS
AND
GROUND
TRUTH
ADAPTED
FROM
SHOTTON
ET
AL
QC
SPRINGER
PERFORMANCE
ON
THE
CHALLENGING
MRSC
DATABASE
THAT
INCLUDES
DIVERSE
OBJECT
CLASSES
INCLUDING
WIRY
OBJECTS
SUCH
AS
BICYCLES
AND
OBJECTS
WITH
A
LARGE
DEGREE
OF
VARIATION
SUCH
AS
DOGS
RECOVERING
SURFACE
LAYOUT
TO
RECOVER
THE
SURFACE
LAYOUT
OF
A
SCENE
WE
ASSIGN
A
LABEL
W
TO
EACH
PIXEL
IN
THE
IMAGE
INDICATING
WHETHER
THE
PIXEL
CONTAINS
A
SUPPORT
OBJECT
E
G
FLOOR
A
VERTICAL
OBJECT
E
G
BUILDING
OR
THE
SKY
THIS
DECISION
IS
BASED
ON
LOCAL
IMAGE
DATA
X
HOIEM
ET
AL
CONSTRUCTED
A
SYSTEM
OF
THIS
TYPE
USING
A
ONE
AGAINST
ALL
PRINCIPLE
EACH
OF
THE
THREE
BINARY
CLASSIFIERS
WAS
BASED
ON
LOGITBOOSTED
CLASSIFICATION
TREES
DIFFERENT
CLASSIFICATION
TREES
ARE
TREATED
AS
WEAK
CLASSIFIERS
AND
THE
RESULTS
ARE
WEIGHTED
TOGETHER
TO
COMPUTE
THE
FINAL
PROBABILITY
HOIEM
ET
AL
WORKED
WITH
THE
INTERMEDIATE
REPRESENTATION
OF
SUPERPIXELS
AN
OVER
SEGMENTATION
OF
THE
SCENE
INTO
SMALL
HOMOGENEOUS
REGIONS
WHICH
ARE
ASSUMED
TO
BELONG
TO
THE
SAME
OBJECT
EACH
SUPERPIXEL
WAS
ASSIGNED
A
LABEL
W
USING
THE
CLASSIFIER
BASED
ON
A
DATA
VECTOR
X
WHICH
CONTAINED
LOCATION
APPEARANCE
TEXTURE
AND
PERSPECTIVE
INFORMATION
ASSOCIATED
WITH
THE
SUPERPIXEL
CLASSIFICATION
MODELS
INPUT
LOCATION
COLOUR
TEXTURE
PERSPECTIVE
ALL
CUES
FIGURE
RECOVERING
SURFACE
LAYOUT
THE
GOAL
IS
TO
TAKE
AN
IMAGE
AND
RETURN
A
LABEL
INDICATING
WHETHER
THE
PIXEL
IS
PART
OF
A
SUPPORT
SURFACE
GREEN
PIXELS
VERTICAL
SURFACE
RED
PIXELS
OR
THE
SKY
BLUE
PIXELS
VERTICAL
SURFACES
WERE
SUB
CLASSIFIED
INTO
PLANAR
OBJECTS
AT
DIFFERENT
ORIENTATIONS
LEFT
ARROWS
UPWARD
ARROWS
AND
RIGHT
ARROWS
DENOTE
LEFT
FACING
FRONTO
PARRALLEL
AND
RIGHT
FACING
SURFACES
AND
NON
PLANAR
OBJECTS
WHICH
CAN
BE
POROUS
MARKED
AS
O
OR
NON
POROUS
MARKED
AS
X
THE
FINAL
CLASSIFICATION
WAS
BASED
ON
I
LOCATION
CUES
WHICH
INCLUDE
POSITION
IN
THE
IMAGE
AND
POSITION
RELATIVE
TO
THE
HORIZON
II
COLOR
CUES
III
TEXTURE
CUES
AND
IV
PERSPECTIVE
CUES
WHICH
WERE
BASED
ON
THE
STATISTICS
OF
LINE
SEGMENTS
IN
THE
REGION
THE
FIGURE
SHOWS
EXAMPLE
CLASSIFICATIONS
FOR
EACH
OF
THESE
CUES
ALONE
AND
WHEN
COMBINED
ADAPTED
FROM
HOIEM
ET
AL
QC
SPRINGER
TO
MITIGATE
AGAINST
THE
POSSIBILITY
THAT
THE
ORIGINAL
SUPERPIXEL
SEGMENTATION
WAS
WRONG
MULTIPLE
SEGMENTATIONS
WERE
COMPUTED
AND
THE
RESULTS
MERGED
TO
PROVIDE
A
FINAL
PER
PIXEL
CLASSIFICATION
FIGURE
IN
THE
FULL
SYSTEM
REGIONS
THAT
WERE
CLAS
SIFIED
AS
VERTICAL
WERE
SUB
CLASSIFIED
INTO
LEFT
FACING
PLANAR
SURFACES
FRONTO
PARALLEL
PLANAR
SURFACES
OR
RIGHT
FACING
PLANAR
SURFACES
OR
NON
PLANAR
SURFACES
WHICH
MAY
BE
POROUS
E
G
TREES
OR
SOLID
THE
SYSTEM
WAS
TRAINED
AND
TESTED
ON
A
DATA
SET
CONSISTING
OF
IMAGES
COLLECTED
FROM
THE
WEB
INCLUDING
DIVERSE
ENVIRONMENTS
FORESTS
CITIES
ROADS
ETC
AND
CONDITIONS
SNOWY
SUNNY
CLOUDY
ETC
THE
DATA
SET
WAS
PRUNED
TO
REMOVE
PHOTOS
WHERE
THE
HORIZON
WAS
NOT
WITHIN
THE
IMAGE
THE
SYSTEM
CORRECTLY
LABELED
OF
PIXELS
CORRECTLY
WITH
RESPECT
TO
THE
MAIN
THREE
CLASSES
AND
CORRECTLY
WITH
RESPECT
TO
THE
SUBCLASSES
OF
THE
VERTICAL
SURFACE
THIS
ALGORITHM
WAS
THE
BASIS
OF
A
REMARKABLE
SYSTEM
FOR
CREATING
A
MODEL
FROM
A
SINGLE
PHOTOGRAPH
HOIEM
ET
AL
D
E
FIGURE
IDENTIFYING
HUMAN
PARTS
A
THE
GOAL
OF
THE
SYSTEM
IS
TO
TAKE
A
DEPTH
IMAGE
X
AND
ASSIGN
A
DISCRETE
LABEL
W
TO
EACH
PIXEL
INDICATING
WHICH
OF
POSSIBLE
BODY
PARTS
IS
PRESENT
THESE
DEPTH
LABELS
ARE
USED
TO
FORM
PROPOSALS
ABOUT
THE
POSITION
OF
JOINTS
B
THE
CLASSIFICATION
IS
BASED
ON
DECISION
TREES
AT
EACH
POINT
IN
THE
TREE
THE
DATA
ARE
DIVIDED
ACCORDING
TO
THE
RELATIVE
DEPTH
AT
TWO
POINTS
RED
CIRCLES
OFFSET
RELATIVE
TO
THE
CURRENT
PIXEL
YELLOW
CROSSES
IN
THIS
EXAMPLE
THIS
DIFFERENCE
IS
LARGE
IN
BOTH
CASES
WHEREAS
IN
C
THIS
DIFFERENCE
IS
SMALL
HENCE
THESE
DIFFERENCES
PROVIDE
INFOR
MATION
ABOUT
THE
POSE
D
E
TWO
MORE
EXAMPLES
OF
DEPTH
IMAGE
LABELING
AND
HYPOTHESIZED
POSE
ADAPTED
FROM
SHOTTON
ET
AL
QC
IEEE
IDENTIFYING
HUMAN
PARTS
SHOTTON
ET
AL
DESCRIBE
A
SYSTEM
THAT
ASSIGNS
A
DISCRETE
LABEL
W
INDICATING
WHICH
OF
BODY
PARTS
IS
PRESENT
AT
EACH
PIXEL
BASED
ON
A
DEPTH
IMAGE
X
THE
RESULTING
DISTRIBUTION
OF
LABELS
IS
AN
INTERMEDIATE
REPRESENTATION
IN
A
SYSTEM
THAT
PROPOSES
A
POSSIBLE
CONFIGURATION
OF
THE
JOINT
POSITIONS
IN
THE
MICROSOFT
KINECT
GAMING
SYSTEM
FIGURE
THE
CLASSIFICATION
WAS
BASED
ON
A
FOREST
OF
DECISION
TREES
THE
FINAL
PROBABILITY
P
R
W
X
IS
AN
AVERAGE
I
E
A
MIXTURE
OF
THE
PREDICTIONS
FROM
A
NUMBER
OF
DIFFERENT
CLASSIFICATION
TREES
THE
GOAL
IS
TO
MITIGATE
AGAINST
BIASES
INTRODUCED
BY
THE
GREEDY
METHOD
WITH
WHICH
A
SINGLE
TREE
IS
TRAINED
WITHIN
EACH
TREE
THE
DECISION
ABOUT
WHICH
BRANCH
A
DATA
POINT
TRAVELS
DOWN
IS
BASED
ON
THE
DIFFERENCE
IN
MEASURED
DEPTHS
AT
TWO
POINTS
EACH
OF
WHICH
IS
SPATIALLY
OFFSET
FROM
THE
CURRENT
PIXEL
THE
OFFSETS
ARE
INVERSELY
SCALED
BY
THE
DISTANCE
TO
THE
PIXEL
ITSELF
WHICH
ENSURES
THAT
THEY
ADDRESS
THE
SAME
RELATIVE
POSITIONS
ON
THE
BODY
WHEN
THE
PERSON
MOVES
CLOSER
OR
FURTHER
AWAY
TO
THE
DEPTH
CAMERA
THE
SYSTEM
WAS
TRAINED
FROM
A
VERY
LARGE
DATA
SET
OF
DEPTH
IMAGES
WHICH
WERE
SYNTHESIZED
BASED
ON
MOTION
CAPTURE
DATA
AND
CONSISTED
OF
THREE
TREES
OF
DEPTH
REMARKABLY
THE
SYSTEM
IS
CAPABLE
OF
ASSIGNING
THE
CORRECT
LABEL
OF
THE
TIME
AND
THIS
PROVIDES
A
VERY
SOLID
BASIS
FOR
THE
SUBSEQUENT
JOINT
PROPOSALS
DISCUSSION
IN
THIS
CHAPTER
WE
HAVE
CONSIDERED
CLASSIFICATION
PROBLEMS
WE
NOTE
THAT
ALL
OF
THE
IDEAS
THAT
WERE
APPLIED
TO
REGRESSION
MODELS
IN
CHAPTER
ARE
ALSO
APPLICABLE
TO
CLASSIFICATION
PROBLEMS
HOWEVER
FOR
CLASSIFICATION
THE
MODEL
INCLUDES
A
NONLINEAR
MAPPING
BETWEEN
THE
DATA
X
AND
THE
PARAMETERS
OF
THE
DISTRIBUTION
P
R
W
X
OVER
THE
WORLD
W
THIS
MEANS
THAT
WE
CANNOT
FIND
THE
MAXIMUM
LIKELIHOOD
SOLUTION
IN
CLOSED
FORM
ALTHOUGH
THE
PROBLEM
IS
STILL
CONVEX
AND
WE
CANNOT
COMPUTE
A
FULL
BAYESIAN
SOLUTION
WITHOUT
MAKING
APPROXIMATIONS
CLASSIFICATION
TECHNIQUES
HAVE
MANY
USES
IN
MACHINE
VISION
NOTICE
THOUGH
THAT
THESE
MODELS
HAVE
NO
DOMAIN
SPECIFIC
INFORMATION
ABOUT
THE
PROBLEM
OTHER
THAN
THAT
PROVIDED
BY
THE
PREPROCESSING
OF
THE
DATA
THIS
IS
BOTH
AN
ADVANTAGE
THEY
FIND
MANY
APPLICATIONS
AND
A
DISADVANTAGE
THEY
CANNOT
TAKE
ADVANTAGE
OF
A
PRIORI
INFORMATION
ABOUT
THE
PROBLEM
IN
THE
REMAINING
PART
OF
THE
BOOK
WE
WILL
EXPLORE
MODELS
THAT
INTRODUCE
INCREASING
AMOUNTS
OF
DOMAIN
SPECIFIC
INFORMATION
TO
THE
PROBLEM
NOTES
NOTES
CLASSIFICATION
IN
VISION
CLASSIFICATION
TECHNIQUES
SUCH
AS
THOSE
DISCUSSED
IN
THIS
CHAPTER
HAVE
BEEN
APPLIED
TO
MANY
PROBLEMS
IN
VISION
INCLUDING
FACE
DETECTION
VIOLA
JONES
SURFACE
LAYOUT
ESTIMATION
HOIEM
ET
AL
BOUNDARY
DETECTION
DOLL
AR
ET
AL
KEYPOINT
MATCHING
LEPETIT
ET
AL
BODY
PART
CLASSIFICATION
HOIEM
ET
AL
SEMANTIC
SEGMENTATION
HE
ET
AL
OBJECT
RECOGNITION
CSURKA
ET
AL
AND
GENDER
CLASSIFICATION
KUMAR
ET
AL
PROBABILISTIC
CLASSIFICATION
MORE
INFORMATION
ABOUT
LOGISTIC
REGRESSION
CAN
BE
FOUND
IN
BISHOP
AND
MANY
OTHER
STATISTICS
TEXTBOOKS
KERNEL
LOGISTIC
REGRESSION
OR
GAUSSIAN
PROCESS
REGRESSION
WAS
PRESENTED
IN
WILLIAMS
BARBER
AND
MORE
INFORMATION
CAN
BE
FOUND
IN
RASMUSSEN
WILLIAMS
A
SPARSE
VERSION
OF
KERNEL
LOGISTIC
REGRESSION
RELEVANCE
VECTOR
CLASSIFICATION
WAS
PRESENTED
BY
TIPPING
AND
A
SPARSE
MULTI
CLASS
VARIANT
WAS
DEVELOPED
BY
BRISHNAPURAM
ET
AL
PROBABILISTIC
INTERPRETATIONS
OF
BOOSTING
WERE
INTRODUCED
BY
FRIEDMAN
ET
AL
RANDOM
FORESTS
OF
MULTINOMIAL
REGRESSORS
WERE
INTRODUCED
IN
PRINZIE
VAN
DEN
POEL
OTHER
CLASSIFICATION
SCHEMES
IN
THIS
CHAPTER
WE
HAVE
PRESENTED
A
FAMILY
OF
PROBA
BILISTIC
CLASSIFICATION
MODELS
BASED
ON
LOGISTIC
REGRESSION
THERE
ARE
OTHER
NON
PROBABILISTIC
TECHNIQUES
FOR
CLASSIFICATION
AND
THESE
INCLUDE
SINGLE
AND
MULTI
LAYER
PERCEPTRONS
ROSEN
BLATT
RUMELHART
ET
AL
SUPPORT
VECTOR
MACHINES
VAPNIK
CRISTIANINI
SHAWE
TAYLOR
AND
ADABOOST
FREUND
SCHAPIRE
A
CRITICAL
DIFFERENCE
BE
TWEEN
THESE
TECHNIQUES
IS
THE
UNDERLYING
OBJECTIVE
FUNCTION
LOGISTIC
REGRESSION
MODELS
OPTIMIZE
THE
LOG
BERNOULLI
PROBABILITY
BUT
THE
OTHER
MODELS
OPTIMIZE
DIFFERENT
CRITERIA
SUCH
AS
THE
HINGE
LOSS
SUPPORT
VECTOR
MACHINES
OR
EXPONENTIAL
ERROR
ADABOOST
IT
IS
DIFFICULT
TO
MAKE
GENERAL
STATEMENTS
ABOUT
THE
RELATIVE
MERITS
OF
THESE
APPROACHES
BUT
IT
IS
PROBABLY
FAIR
TO
SAY
THAT
I
THERE
IS
NO
MAJOR
DISADVANTAGE
TO
USING
THE
PROBABILISTIC
TECHNIQUES
IN
THIS
CHAPTER
AND
II
THE
CHOICE
OF
CLASSIFICATION
METHOD
IS
USUALLY
LESS
IMPOR
TANT
IN
VISION
PROBLEMS
THAN
THE
PREPROCESSING
OF
THE
DATA
METHODS
BASED
ON
BOOSTING
AND
CLASSIFICATION
TREES
ARE
PARTICULARLY
POPULAR
IN
VISION
BECAUSE
OF
THEIR
SPEED
BOOSTING
ADABOOST
WAS
INTRODUCED
BY
FREUND
SCHAPIRE
SINCE
THEN
THERE
HAVE
BEEN
A
LARGE
NUMBER
OF
VARIATIONS
MOST
OF
WHICH
HAVE
BEEN
USED
IN
COMPUTER
VISION
THESE
INCLUDE
DISCRETE
ADABOOST
FREUND
SCHAPIRE
REAL
ADABOOST
SCHAPIRE
SINGER
GENTLEBOOST
FRIEDMAN
ET
AL
LOGITBOOST
FRIEDMAN
ET
AL
FLOATBOOST
LI
ET
AL
KLBOOST
LIU
SHUM
ASYMMETRIC
BOOST
VIOLA
JONES
AND
STATBOOST
PHAM
CHAN
BOOSTING
HAS
ALSO
BEEN
APPLIED
TO
THE
MULTI
CLASS
CASE
SCHAPIRE
SINGER
TORRALBA
ET
AL
AND
FOR
REGRESSION
FRIEDMAN
A
REVIEW
OF
BOOSTING
APPROACHES
CAN
BE
FOUND
IN
MEIR
M
ATSCH
CLASSIFICATION
TREES
CLASSIFICATION
TREES
HAVE
A
LONG
HISTORY
IN
COMPUTER
VISION
DATING
BACK
TO
AT
LEAST
SHEPHERD
MODERN
INTEREST
WAS
STIMULATED
BY
AMIT
GEMAN
AND
BREIMAN
WHO
INVESTIGATED
THE
USE
OF
RANDOM
FORESTS
SINCE
THIS
TIME
CLASSIFICATION
TREES
AND
FORESTS
HAVE
BEEN
APPLIED
TO
KEYPOINT
MATCHING
LEPETIT
ET
AL
SEGMENTATION
YIN
ET
AL
HUMAN
POSE
DETECTION
ROGEZ
ET
AL
SHOTTON
ET
AL
OBJECT
DETECTION
BOSCH
ET
AL
IMAGE
CLASSIFICATION
MOOSMANN
ET
AL
MOOSMANN
ET
AL
DECIDING
IMAGE
SUITABILITY
MAC
AODHA
ET
AL
DETECT
ING
OCCLUSIONS
HUMAYUN
ET
AL
AND
SEMANTIC
IMAGE
SEGMENTATION
SHOTTON
ET
AL
GENDER
CLASSIFICATION
AUTOMATIC
DETERMINATION
OF
GENDER
FROM
A
FACIAL
IMAGE
HAS
VARIOUSLY
BEEN
TACKLED
WITH
NEURAL
NETWORKS
GOLOMB
ET
AL
SUPPORT
VECTOR
MACHINES
MOGHADDAM
YANG
LINEAR
DISCRIMINANT
ANALYSIS
BEKIOS
CALFA
ET
AL
AND
CLASSIFICATION
MODELS
BOTH
ADABOOST
BALUJA
ROWLEY
AND
LOGITBOOST
PRINCE
AGHAJANIAN
A
REVIEW
IS
PROVIDED
BY
M
AKINEN
RAISAMO
AND
QUANTATIVE
COMPARISONS
ARE
PRESENTED
IN
M
AKINEN
RAISAMO
REPRESENTATIVE
EXAMPLES
OF
THE
STATE
OF
THE
ART
CAN
BE
FOUND
IN
KUMAR
ET
AL
AND
SHAN
IN
PRESS
FACE
DETECTION
THE
APPLICATION
OF
BOOSTING
TO
FACE
DETECTION
VIOLA
JONES
USURPED
EARLIER
TECHNIQUES
E
G
OSUNA
ET
AL
SCHNEIDERMAN
KANADE
SINCE
THEN
MANY
BOOSTING
VARIANTS
HAVE
BEEN
APPLIED
TO
THE
PROBLEM
INCLUDING
FLOATBOOST
LI
ET
AL
LI
ZHANG
GENTLEBOOST
LIENHART
ET
AL
REALBOOST
HUANG
ET
AL
WU
ET
AL
ASYMBOOST
PHAM
CHAN
VIOLA
JONES
AND
STATBOOST
PHAM
CHAN
A
RECENT
REVIEW
OF
THIS
AREA
CAN
BE
FOUND
IN
ZHANG
ZHANG
SEMANTIC
SEGMENTATION
THE
AUTHORS
OF
THE
SYSTEM
DESCRIBED
IN
THE
TEXT
SHOTTON
ET
AL
SUBSEQUENTLY
PRESENTED
A
MUCH
FASTER
SYSTEM
BASED
ON
CLASSIFICATION
TREES
SHOTTON
ET
AL
A
RECENT
COMPARISON
OF
QUANTITATIVE
PERFORMANCE
CAN
BE
FOUND
IN
RANGANATHAN
OTHER
WORK
HAS
INVESTIGATED
THE
IMPOSITION
OF
PRIOR
KNOWLEDGE
SUCH
AS
THE
CO
PRESENCE
OF
OBJECT
CLASSES
HE
ET
AL
AND
LIKELY
SPATIAL
CONFIGURATIONS
OF
OBJECTS
HE
ET
AL
PROBLEMS
PROBLEM
THE
LOGISTIC
SIGMOID
FUNCTION
IS
DEFINED
AS
SIG
A
EXP
A
SHOW
THAT
I
SIG
II
SIG
III
SIG
PROBLEM
SHOW
THAT
THE
DERIVATIVE
OF
THE
LOG
POSTERIOR
PROBABILITY
FOR
THE
LOGISTIC
REGRESSION
MODEL
L
I
WI
LOG
EXP
ΦT
XI
I
WI
LOG
EXP
ΦT
XI
EXP
Φ
XI
WITH
RESPECT
TO
THE
PARAMETERS
Φ
IS
GIVEN
BY
L
SIG
A
W
X
PROBLEM
SHOW
THAT
THE
SECOND
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
OF
THE
LOGISTIC
REGRESSION
MODEL
IS
GIVEN
BY
I
L
SIG
A
SIG
A
X
XT
PROBLEM
CONSIDER
FITTING
A
LOGISTIC
REGRESSION
MODEL
TO
DATA
X
WHERE
THE
TWO
CLASSES
ARE
PERFECTLY
SEPARABLE
FOR
EXAMPLE
PERHAPS
ALL
THE
DATA
X
WHERE
THE
WORLD
STATE
W
TAKES
VALUES
LESS
THAN
AND
ALL
THE
DATA
X
WHERE
THE
WORLD
STATE
IS
W
TAKES
NOTES
A
B
C
FIGURE
MIXTURE
OF
TWO
EXPERTS
MODEL
FOR
DATA
PINK
CIRCLES
INDICATE
POSITIVE
EXAMPLES
GREEN
CIRCLES
INDICATE
NEGATIVE
EXAMPLES
A
TWO
EXPERT
IS
SPECIALIZED
TO
MODEL
THE
LEFT
AND
RIGHT
SIDES
OF
THE
DATA
RESPECTIVELY
B
THE
MIXING
WEIGHTS
CHANGE
AS
A
FUNCTION
OF
THE
DATA
C
THE
FINAL
OUTPUT
OF
THE
MODEL
IS
MIXTURE
OF
THE
TWO
CONSTITUENT
EXPERTS
AND
FITS
THE
DATA
WELL
VALUES
GREATER
THAN
HENCE
IT
IS
POSSIBLE
TO
CLASSIFY
THE
TRAINING
DATA
PERFECTLY
WHAT
WILL
HAPPEN
TO
THE
PARAMETERS
OF
THE
MODEL
DURING
LEARNING
HOW
COULD
YOU
RECTIFY
THIS
PROBLEM
PROBLEM
COMPUTE
THE
LAPLACE
APPROXIMATION
TO
A
BETA
DISTRIBUTION
WITH
PARAMETERS
Α
Β
PROBLEM
SHOW
THAT
THE
LAPLACE
APPROXIMATION
TO
A
UNIVARIATE
NORMAL
DISTRIBUTION
WITH
MEAN
Μ
AND
VARIANCE
IS
THE
NORMAL
DISTRIBUTION
ITSELF
PROBLEM
SHOW
THAT
THE
SECOND
DERIVATIVE
OF
THE
LOGARITHM
L
LOG
NORMX
Μ
Σ
OF
A
NORMAL
DISTRIBUTION
EVALUATED
AT
THE
MEAN
Μ
IS
GIVEN
BY
L
LΜ
Σ
PROBLEM
DEVISE
A
METHOD
TO
CHOOSE
THE
SCALE
PARAMETER
IN
THE
RADIAL
BASIS
FUNCTION
IN
KERNEL
LOGISTIC
REGRESSION
EQUATION
PROBLEM
A
MIXTURE
OF
EXPERTS
JORDAN
JACOBS
DIVIDES
SPACE
INTO
DIFFERENT
REGIONS
EACH
OF
WHICH
RECEIVES
SPECIALIZED
ATTENTION
FIGURE
FOR
EXAMPLE
WE
COULD
DESCRIBE
THE
DATA
AS
A
MIXTURE
OF
LOGISTIC
CLASSIFIERS
SO
THAT
P
R
WI
XI
K
ΛK
XI
BERNWI
SIG
ΦT
XI
EACH
LOGISTIC
CLASSIFIER
IS
CONSIDERED
AS
AN
EXPERT
AND
THE
MIXING
WEIGHTS
DECIDE
THE
COM
BINATION
OF
EXPERTS
THAT
ARE
APPLIED
TO
THE
DATA
THE
MIXING
WEIGHTS
WHICH
ARE
POSITIVE
AND
SUM
TO
ONE
DEPEND
ON
THE
DATA
X
FOR
A
TWO
COMPONENT
MODEL
THEY
COULD
BE
BASED
ON
A
SECOND
LOGISTIC
REGRESSION
MODEL
WITH
ACTIVATION
ΩT
X
THIS
MODEL
CAN
BE
EXPRESSED
AS
THE
MARGINALIZATION
OF
A
JOINT
DISTRIBUTION
BETWEEN
WI
AND
A
HIDDEN
VARIABLE
HI
SO
THAT
K
K
P
R
WI
XI
P
R
WI
HI
K
XI
P
R
WI
HI
K
XI
P
R
HI
K
XI
CLASSIFICATION
MODELS
WHERE
P
R
WI
HI
K
XI
BERNWI
P
R
HI
K
XI
BERNH
SIG
ΦT
XI
SIG
ΩT
XI
HOW
DOES
THIS
MODEL
DIFFER
FROM
BRANCHING
LOGISTIC
REGRESSION
SECTION
DEVISE
A
LEARNING
ALGORITHM
FOR
THIS
MODEL
PROBLEM
THE
SOFTMAX
FUNCTION
IS
DEFINED
TO
RETURN
A
MULTIVARIATE
QUAN
TITY
WHERE
THE
K
ELEMENT
IS
GIVEN
BY
SOFTMAX
A
A
A
EXP
AK
K
K
K
K
J
EXP
AJ
SHOW
THAT
SK
AND
THAT
K
SK
PROBLEM
SHOW
THAT
THE
FIRST
DERIVATIVE
OF
THE
LOG
PROBABILITY
OF
THE
MULTI
CLASS
LOGISTIC
REGRESSION
MODEL
IS
GIVEN
BY
EQUATION
PROBLEM
THE
CLASSIFIERS
IN
THIS
CHAPTER
HAVE
ALL
BEEN
BASED
ON
CONTINUOUS
DATA
X
DEVISE
A
MODEL
THAT
CAN
DISTINGUISH
BETWEEN
M
WORLD
STATES
W
M
BASED
ON
A
DISCRETE
OBSERVATION
X
K
AND
DISCUSS
POTENTIAL
LEARNING
ALGORITHMS
PART
III
CONNECTING
LOCAL
MODELS
PART
III
CONNECTING
LOCAL
MODELS
THE
MODELS
IN
CHAPTERS
DESCRIBE
THE
RELATIONSHIP
BETWEEN
A
SET
OF
MEASUREMENTS
AND
THE
WORLD
STATE
THEY
WORK
WELL
WHEN
THE
MEASUREMENTS
AND
THE
WORLD
STATE
ARE
BOTH
LOW
DIMENSIONAL
HOWEVER
THERE
ARE
MANY
SITUATIONS
WHERE
THIS
IS
NOT
THE
CASE
AND
THESE
MODELS
ARE
UNSUITABLE
FOR
EXAMPLE
CONSIDER
THE
SEMANTIC
IMAGE
LABELING
PROBLEM
IN
WHICH
WE
WISH
TO
ASSIGN
A
LABEL
TO
EACH
PIXEL
IN
THE
IMAGE
WHERE
THE
LABEL
DENOTES
THE
OBJECT
CLASS
FOR
EXAMPLE
IN
A
ROAD
SCENE
WE
MIGHT
WISH
TO
LABEL
PIXELS
AS
ROAD
SKY
CAR
TREE
BUILDING
OR
OTHER
FOR
AN
IMAGE
WITH
N
PIXELS
THIS
MEANS
WE
NEED
TO
BUILD
A
MODEL
RELATING
THE
MEASURED
RGB
TRIPLES
TO
POSSIBLE
WORLD
STATES
NONE
OF
THE
MODELS
DISCUSSED
SO
FAR
CAN
COPE
WITH
THIS
CHALLENGE
THE
NUMBER
OF
PARAMETERS
INVOLVED
AND
HENCE
THE
AMOUNT
OF
TRAINING
DATA
AND
THE
COMPUTATIONAL
REQUIREMENTS
OF
THE
LEARNING
AND
INFERENCE
ALGORITHMS
IS
FAR
BEYOND
WHAT
CURRENT
MACHINES
CAN
HANDLE
ONE
POSSIBLE
SOLUTION
TO
THIS
PROBLEM
WOULD
BE
TO
BUILD
A
SET
OF
INDEPENDENT
LOCAL
MODELS
FOR
EXAMPLE
WE
COULD
BUILD
MODELS
THAT
RELATE
EACH
PIXEL
LABEL
SEP
ARATELY
TO
THE
NEARBY
RGB
DATA
HOWEVER
THIS
IS
NOT
IDEAL
AS
THE
IMAGE
MAY
BE
LOCALLY
AMBIGUOUS
FOR
EXAMPLE
A
SMALL
BLUE
IMAGE
PATCH
MIGHT
RESULT
FROM
A
VA
RIETY
OF
SEMANTICALLY
DIFFERENT
CLASSES
SKY
WATER
A
CAR
DOOR
OR
A
PERSON
CLOTHING
IN
GENERAL
IT
IS
INSUFFICIENT
TO
BUILD
INDEPENDENT
LOCAL
MODELS
THE
SOLUTION
TO
THIS
PROBLEM
IS
TO
BUILD
LOCAL
MODELS
THAT
ARE
CONNECTED
TO
ONE
ANOTHER
CONSIDER
AGAIN
THE
SEMANTIC
LABELING
EXAMPLE
GIVEN
THE
WHOLE
IMAGE
WE
CAN
SEE
THAT
WHEN
THE
IMAGE
PATCH
IS
BLUE
AND
IS
FOUND
ABOVE
TREES
AND
MOUNTAINS
AND
ALONGSIDE
SIMILAR
PATCHES
ACROSS
THE
TOP
OF
THE
IMAGE
THEN
THE
CORRECT
CLASS
IS
PROBABLY
SKY
HENCE
TO
SOLVE
THIS
PROBLEM
WE
STILL
MODEL
THE
RELATIONSHIP
BETWEEN
THE
LABEL
AND
ITS
LOCAL
IMAGE
REGION
BUT
WE
ALSO
CONNECT
THESE
MODELS
SO
THAT
NEARBY
ELEMENTS
CAN
HELP
TO
DISAMBIGUATE
ONE
ANOTHER
IN
CHAPTER
WE
INTRODUCE
THE
IDEA
OF
CONDITIONAL
INDEPENDENCE
WHICH
IS
A
WAY
OF
CHARACTERIZING
REDUNDANCIES
IN
THE
MODEL
I
E
THE
LACK
OF
DIRECT
DEPEN
DENCE
BETWEEN
VARIABLES
WE
SHOW
HOW
CONDITIONAL
INDEPENDENCE
RELATIONS
CAN
BE
VISUALIZED
WITH
GRAPHICAL
MODELS
WE
DISTINGUISH
BETWEEN
DIRECTED
AND
UNDIRECTED
GRAPHICAL
MODELS
IN
CHAPTER
WE
DISCUSS
MODELS
IN
WHICH
THE
LOCAL
UNITS
ARE
COMBINED
TOGETHER
TO
FORM
CHAINS
OR
TREES
IN
CHAPTER
WE
EXTEND
THIS
TO
THE
CASE
WHERE
THEY
HAVE
MORE
GENERAL
CONNECTIONS
CHAPTER
GRAPHICAL
MODELS
THE
PREVIOUS
CHAPTERS
DISCUSSED
MODELS
THAT
RELATE
THE
OBSERVED
MEASUREMENTS
TO
SOME
ASPECT
OF
THE
WORLD
THAT
WE
WISH
TO
ESTIMATE
IN
EACH
CASE
THIS
RELATION
SHIP
DEPENDED
ON
A
SET
OF
PARAMETERS
AND
FOR
EACH
MODEL
WE
PRESENTED
A
LEARNING
ALGORITHM
THAT
ESTIMATED
THESE
PARAMETERS
UNFORTUNATELY
THE
UTILITY
OF
THESE
MODELS
IS
LIMITED
BECAUSE
EVERY
ELEMENT
OF
THE
MODEL
DEPENDS
ON
EVERY
OTHER
FOR
EXAMPLE
IN
GENERATIVE
MODELS
WE
MODEL
THE
JOINT
PROBABILITY
OF
THE
OBSERVATIONS
AND
THE
WORLD
STATE
IN
MANY
PROBLEMS
BOTH
OF
THESE
QUANTITIES
MAY
BE
HIGH
DIMENSIONAL
CONSEQUENTLY
THE
NUMBER
OF
PARAMETERS
REQUIRED
TO
CHARACTERIZE
THEIR
JOINT
DENSITY
ACCURATELY
IS
VERY
LARGE
DISCRIMINATIVE
MODELS
SUFFER
FROM
THE
SAME
PATHOLOGY
IF
EVERY
ELEMENT
OF
THE
WORLD
STATE
DEPENDS
ON
EVERY
ELEMENT
OF
THE
DATA
A
LARGE
NUMBER
OF
PARAMETERS
WILL
BE
REQUIRED
TO
CHARACTERIZE
THIS
RELATIONSHIP
IN
PRACTICE
THE
REQUIRED
AMOUNT
OF
TRAINING
DATA
AND
THE
COMPUTATIONAL
BURDEN
OF
LEARNING
AND
INFERENCE
REACH
IMPRACTICAL
LEVELS
THE
SOLUTION
TO
THIS
PROBLEM
IS
TO
REDUCE
THE
DEPENDENCIES
BETWEEN
VARIABLES
IN
THE
MODEL
BY
IDENTIFYING
OR
ASSERTING
SOME
DEGREE
OF
REDUNDANCY
TO
THIS
END
WE
INTRODUCE
THE
IDEA
OF
CONDITIONAL
INDEPENDENCE
WHICH
IS
A
WAY
OF
CHARACTERIZING
THESE
REDUNDANCIES
WE
THEN
INTRODUCE
GRAPHICAL
MODELS
WHICH
ARE
GRAPH
BASED
REPRESENTATIONS
OF
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
WE
DISCUSS
TWO
DIFFER
ENT
TYPES
OF
GRAPHICAL
MODELS
DIRECTED
AND
UNDIRECTED
AND
WE
CONSIDER
THE
IMPLICATIONS
FOR
LEARNING
INFERENCE
AND
DRAWING
SAMPLES
THIS
CHAPTER
DOES
NOT
DEVELOP
SPECIFIC
MODELS
OR
DISCUSS
VISION
APPLICATIONS
THE
GOAL
IS
TO
PROVIDE
THE
THEORETICAL
BACKGROUND
FOR
THE
MODELS
IN
SUBSEQUENT
CHAPTERS
WE
WILL
ILLUSTRATE
THE
IDEAS
WITH
PROBABILITY
DISTRIBUTIONS
WHERE
THE
CONSTITUENT
VARIABLES
ARE
DISCRETE
HOWEVER
ALMOST
ALL
OF
THE
IDEAS
TRANSFER
DIRECTLY
TO
THE
CONTINUOUS
CASE
CONDITIONAL
INDEPENDENCE
WHEN
WE
FIRST
DISCUSSED
PROBABILITY
DISTRIBUTIONS
WE
INTRODUCED
THE
NOTION
OF
INDEPENDENCE
SECTION
TWO
VARIABLES
AND
ARE
INDEPENDENT
IF
THEIR
JOINT
GRAPHICAL
MODELS
D
FIGURE
CONDITIONAL
INDEPENDENCE
A
JOINT
PDF
OF
THREE
DISCRETE
VARI
ABLES
WHICH
TAKE
FOUR
THREE
AND
TWO
POSSIBLE
VALUES
RESPECTIVELY
ALL
PROBABILITY
VALUES
SUM
TO
ONE
B
MARGINALIZING
WE
SEE
THAT
VARIABLES
AND
ARE
DEPENDENT
THE
CONDITIONAL
DISTRIBUTION
OF
IS
DIFFERENT
FOR
DIFFERENT
VALUES
OF
THE
ELEMENTS
IN
EACH
ROW
ARE
NOT
IN
THE
SAME
PROPOR
TIONS
AND
VICE
VERSA
C
VARIABLES
AND
ARE
ALSO
DEPENDENT
D
VARIABLES
AND
ARE
ALSO
DEPENDENT
E
G
HOWEVER
AND
ARE
CONDITIONALLY
INDEPENDENT
GIVEN
FOR
FIXED
TELLS
US
NOTHING
MORE
ABOUT
AND
VICE
VERSA
PROBABILITY
DISTRIBUTION
FACTORIZES
AS
P
R
P
R
P
R
IN
LAYMAN
TERMS
ONE
VARIABLE
PROVIDES
NO
INFORMATION
ABOUT
THE
OTHER
IF
THEY
ARE
INDEPENDENT
WITH
MORE
THAN
TWO
RANDOM
VARIABLES
INDEPENDENCE
RELATIONS
BECOME
MORE
COMPLEX
THE
VARIABLE
IS
SAID
TO
BE
CONDITIONALLY
INDEPENDENT
OF
VARIABLE
GIVEN
VARIABLE
WHEN
AND
ARE
INDEPENDENT
FOR
FIXED
FIGURE
IN
MATHEMATICAL
TERMS
WE
HAVE
P
R
P
R
P
R
P
R
NOTE
THAT
CONDITIONAL
INDEPENDENCE
RELATIONS
ARE
ALWAYS
SYMMETRIC
IF
IS
CONDI
TIONALLY
INDEPENDENT
OF
GIVEN
THEN
IT
IS
ALSO
TRUE
THAT
IS
INDEPENDENT
OF
GIVEN
CONFUSINGLY
THE
CONDITIONAL
INDEPENDENCE
OF
AND
GIVEN
DOES
NOT
MEAN
THAT
AND
ARE
THEMSELVES
INDEPENDENT
IT
MERELY
IMPLIES
THAT
IF
WE
KNOW
VARIABLE
THEN
PROVIDES
NO
FURTHER
INFORMATION
ABOUT
AND
VICE
VERSA
ONE
WAY
THAT
THIS
CAN
OCCUR
IS
IN
A
CHAIN
OF
EVENTS
IF
EVENT
CAUSES
EVENT
AND
CAUSES
THEN
THE
DEPENDENCE
OF
ON
MIGHT
BE
ENTIRELY
MEDIATED
BY
NOW
CONSIDER
DECOMPOSING
THE
JOINT
PROBABILITY
DISTRIBUTION
P
R
INTO
THE
PRODUCT
OF
CONDITIONAL
PROBABILITIES
WHEN
IS
INDEPENDENT
OF
GIVEN
WE
FIND
THAT
P
R
P
R
P
R
P
R
P
R
P
R
P
R
THE
CONDITIONAL
INDEPENDENCE
RELATION
MEANS
THAT
THE
PROBABILITY
DISTRIBUTION
FAC
TORIZES
IN
A
CERTAIN
WAY
AND
IS
HENCE
REDUNDANT
THIS
REDUNDANCY
IMPLIES
THAT
WE
CAN
DESCRIBE
THE
DISTRIBUTION
WITH
FEWER
PARAMETERS
AND
SO
WORKING
WITH
MODELS
WITH
LARGE
NUMBERS
OF
VARIABLES
BECOMES
MORE
TRACTABLE
THROUGHOUT
THIS
CHAPTER
WE
WILL
EXPLORE
THE
RELATIONSHIP
BETWEEN
FACTORIZATION
OF
THE
DISTRIBUTION
AND
CONDITIONAL
INDEPENDENCE
RELATIONS
TO
THIS
END
WE
WILL
INTRODUCE
GRAPHICAL
MODELS
THESE
ARE
GRAPH
BASED
REPRESENTATIONS
THAT
MAKE
BOTH
THE
FACTORIZATION
AND
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
EASY
TO
ESTABLISH
IN
THIS
BOOK
WE
WILL
CONSIDER
TWO
DIFFERENT
TYPES
OF
GRAPHICAL
MODEL
DIRECTED
AND
UNDIRECTED
GRAPHICAL
MODELS
EACH
OF
WHICH
CORRESPONDS
TO
A
DIFFERENT
TYPE
OF
FACTORIZATION
DIRECTED
GRAPHICAL
MODELS
A
DIRECTED
GRAPHICAL
MODEL
OR
BAYESIAN
NETWORK
REPRESENTS
THE
FACTORIZATION
OF
THE
JOINT
PROBABILITY
DISTRIBUTION
INTO
A
PRODUCT
OF
CONDITIONAL
DISTRIBUTIONS
THAT
TAKE
THE
FORM
OF
A
DIRECTED
ACYCLIC
GRAPH
DAG
SO
THAT
WHERE
XN
N
N
P
R
N
P
R
XN
XPA
N
N
REPRESENT
THE
CONSTITUENT
VARIABLES
OF
THE
JOINT
DISTRIBUTION
AND
THE
FUNCTION
PA
N
RETURNS
THE
INDICES
OF
VARIABLES
THAT
ARE
PARENTS
OF
VARIABLE
XN
WE
CAN
VISUALIZE
THE
FACTORIZATION
AS
A
DIRECTED
GRAPHICAL
MODEL
FIGURE
BY
ADDING
ONE
NODE
PER
RANDOM
VARIABLE
AND
DRAWING
AN
ARROW
TO
EACH
VARIABLE
XN
FROM
EACH
OF
ITS
PARENTS
XPA
N
THIS
DIRECTED
GRAPHICAL
MODEL
SHOULD
NEVER
CONTAIN
CYCLES
IF
IT
DOES
THEN
THE
ORIGINAL
FACTORIZATION
WAS
NOT
A
VALID
PROBABILITY
DISTRIBUTION
TO
RETRIEVE
THE
FACTORIZATION
FROM
THE
GRAPHICAL
MODEL
WE
INTRODUCE
ONE
FACTOR
IZATION
TERM
PER
VARIABLE
IN
THE
GRAPH
IF
VARIABLE
XN
IS
INDEPENDENT
OF
ALL
OTHERS
HAS
NO
PARENTS
THEN
WE
WRITE
P
R
XN
OTHERWISE
WE
WRITE
P
R
XN
XPA
N
WHERE
THE
PARENTS
XPA
N
CONSIST
OF
THE
SET
OF
VARIABLES
WITH
ARROWS
THAT
POINT
TO
XN
EXAMPLE
THE
GRAPHICAL
MODEL
IN
FIGURE
REPRESENTS
THE
FACTORIZATION
PROBLEM
PROBLEM
FIGURE
EXAMPLE
A
DIRECTED
GRAPHICAL
MODEL
HAS
ONE
NODE
PER
TERM
IN
THE
FACTORIZATION
OF
THE
JOINT
PROBABILITY
DISTRIBUTION
A
NODE
XN
WITH
NO
INCOMING
CONNECTIONS
REPRE
SENTS
THE
TERM
P
R
XN
A
NODE
XN
WITH
INCOMING
CONNECTIONS
XPA
N
REPRESENTS
THE
TERM
P
R
XN
XPA
N
VARIABLE
XN
IS
CONDITIONALLY
INDE
PENDENT
OF
ALL
OF
THE
OTHERS
GIVEN
ITS
MARKOV
BLANKET
THIS
COMPRISES
ITS
PARENTS
ITS
CHILDREN
AND
OTHER
PAR
ENTS
OF
ITS
CHILDREN
FOR
EXAMPLE
THE
MARKOV
BLANKET
FOR
VARIABLE
IS
INDICATED
BY
THE
SHADED
REGION
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
THE
GRAPHICAL
MODEL
OR
FACTORIZATION
IMPLIES
A
SET
OF
INDEPENDENCE
AND
CONDI
TIONAL
INDEPENDENCE
RELATIONS
BETWEEN
THE
VARIABLES
SOME
STATEMENTS
ABOUT
THESE
RELATIONS
CAN
BE
MADE
BASED
ON
A
SUPERFICIAL
LOOK
AT
THE
GRAPH
FIRST
IF
THERE
IS
NO
DIRECTED
PATH
BETWEEN
TWO
VARIABLES
FOLLOWING
THE
ARROW
DIRECTIONS
AND
THEY
HAVE
NO
COMMON
ANCESTORS
THEN
THEY
ARE
INDEPENDENT
SO
VARIABLE
IN
FIGURE
IS
INDEPENDENT
OF
ALL
OF
THE
OTHER
VARIABLES
AND
VARIABLES
AND
ARE
INDEPENDENT
OF
EACH
OTHER
VARIABLES
AND
ARE
NOT
INDEPENDENT
AS
THEY
SHARE
AN
ANCESTOR
SECOND
ANY
VARIABLE
IS
CONDITIONALLY
INDEPENDENT
OF
ALL
THE
OTHER
VARIABLES
GIVEN
ITS
PARENTS
CHILDREN
AND
THE
OTHER
PARENTS
OF
ITS
CHILDREN
ITS
MARKOV
BLANKET
SO
FOR
EXAMPLE
VARIABLE
IN
FIGURE
IS
CONDITIONALLY
INDEPENDENT
OF
THE
REMAINING
VARIABLES
GIVEN
THOSE
IN
THE
SHADED
AREA
FOR
VISION
APPLICATIONS
THESE
RULES
ARE
USUALLY
SUFFICIENT
TO
GAIN
AN
UNDERSTAND
ING
OF
THE
MAIN
PROPERTIES
OF
A
GRAPHICAL
MODEL
HOWEVER
OCCASIONALLY
WE
MAY
WISH
TO
TEST
WHETHER
ONE
ARBITRARY
SET
OF
NODES
IS
INDEPENDENT
OF
ANOTHER
GIVEN
A
THIRD
THIS
IS
NOT
EASILY
ESTABLISHED
BY
LOOKING
AT
THE
GRAPH
BUT
CAN
BE
TESTED
USING
THE
FOLLOWING
CRITERION
THE
VARIABLES
IN
SET
A
ARE
CONDITIONALLY
INDEPENDENT
OF
THOSE
IN
SET
B
GIVEN
SET
C
IF
ALL
ROUTES
FROM
A
TO
B
ARE
BLOCKED
A
ROUTE
IS
BLOCKED
AT
A
NODE
IF
I
THIS
NODE
IS
IN
C
AND
THE
ARROWS
MEET
HEAD
TO
TAIL
OR
TAIL
TO
TAIL
OR
II
NEITHER
THIS
NODE
NOR
ANY
OF
ITS
DESCENDANTS
ARE
IN
C
AND
THE
ARROWS
MEET
HEAD
TO
HEAD
SEE
KOLLER
FRIEDMAN
FOR
MORE
DETAILS
OF
WHY
THIS
IS
THE
CASE
FIGURE
EXAMPLE
DIRECTED
GRAPHICAL
MODEL
RELATING
VARIABLES
FROM
FIGURE
THIS
MODEL
IMPLIES
THAT
THE
JOINT
PROBABILITY
CAN
BE
BROKEN
DOWN
AS
P
R
P
R
P
R
P
R
EXAMPLE
FIGURE
TELLS
US
THAT
P
R
P
R
P
R
P
R
IN
OTHER
WORDS
THIS
IS
THE
GRAPHICAL
MODEL
CORRESPONDING
TO
THE
DISTRIBUTION
IN
FIGURE
IF
WE
CONDITION
ON
THEN
THE
ONLY
ROUTE
FROM
TO
IS
BLOCKED
AT
THE
ARROWS
MEET
HEAD
TO
TAIL
HERE
AND
SO
MUST
BE
CONDITIONALLY
INDEPENDENT
OF
GIVEN
WE
COULD
HAVE
REACHED
THE
SAME
CONCLUSION
BY
NOTICING
THAT
THE
MARKOV
BLANKET
FOR
VARIABLE
IS
JUST
VARIABLE
IN
THIS
CASE
IT
IS
EASY
TO
PROVE
THIS
CONDITIONAL
INDEPENDENCE
RELATION
ALGE
BRAICALLY
WRITING
OUT
THE
CONDITIONAL
PROBABILITY
OF
GIVEN
AND
P
R
X
X
X
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
WE
SEE
THAT
THE
FINAL
EXPRESSION
DOES
NOT
DEPEND
ON
AND
SO
WE
DEDUCE
THAT
IS
CONDITIONALLY
INDEPENDENT
OF
GIVEN
AS
REQUIRED
NOTICE
THAT
THE
FACTORIZED
DISTRIBUTION
IS
MORE
EFFICIENT
TO
REPRESENT
THAN
THE
FULL
VERSION
THE
ORIGINAL
DISTRIBUTION
P
R
FIGURE
CONTAINS
ENTRIES
HOWEVER
THE
TERMS
P
R
P
R
AND
P
R
CONTAIN
AND
ENTRIES
RESPECTIVELY
GIVING
A
TOTAL
OF
ENTRIES
IN
THIS
CASE
THIS
IS
NOT
A
DRAMATIC
REDUCTION
BUT
IN
MORE
PRACTICAL
SITUATIONS
IT
WOULD
BE
FOR
EXAMPLE
IF
EACH
VARIABLE
TOOK
POSSIBLE
VALUES
THE
FULL
JOINT
DISTRIBUTION
WOULD
HAVE
VALUES
BUT
THE
FACTORIZED
DISTRIBUTION
WOULD
HAVE
ONLY
VALUES
FOR
EVEN
LARGER
SYSTEMS
THIS
CAN
MAKE
A
HUGE
SAVING
ONE
WAY
TO
THINK
ABOUT
CONDITIONAL
INDEPENDENCE
RELATIONS
IS
TO
CONSIDER
THEM
AS
REDUNDANCIES
IN
THE
FULL
JOINT
PROBABILITY
DISTRIBUTION
EXAMPLE
FINALLY
IN
FIGURE
WE
PRESENT
GRAPHICAL
MODELS
FOR
THE
MIXTURE
OF
GAUSSIANS
T
DISTRIBUTION
AND
FACTOR
ANALYSIS
MODELS
FROM
CHAPTER
THESE
DEPICTIONS
IMME
GRAPHICAL
MODELS
A
B
C
FIGURE
EXAMPLE
GRAPHICAL
MODELS
FOR
A
MIXTURE
OF
GAUSSIANS
B
T
DISTRIBUTION
AND
C
FACTOR
ANALYSIS
A
NODE
BLACK
CIRCLE
REPRESENTS
A
RANDOM
VARIABLE
IN
A
GRAPHICAL
MODEL
A
BULLET
REPRESENTS
A
VARIABLE
WHOSE
VALUE
IS
CONSIDERED
TO
BE
FIXED
EACH
VARIABLE
MAY
BE
REPEATED
MANY
TIMES
AND
THIS
IS
INDICATED
BY
A
PLATE
BLUE
RECTANGLE
WHERE
THE
NUMBER
OF
COPIES
IS
INDICATED
IN
THE
LOWER
RIGHT
CORNER
FOR
EXAMPLE
IN
A
THERE
ARE
I
COPIES
OF
THE
TRAINING
DATA
XI
I
AND
I
COPIES
OF
THE
VARIABLE
HI
I
SIMILARLY
THERE
ARE
K
SETS
OF
PARAMETERS
Μ
ΣK
K
BUT
JUST
ONE
WEIGHT
VECTOR
Λ
DIATELY
DEMONSTRATE
THAT
THESE
MODELS
HAVE
VERY
SIMILAR
STRUCTURES
THEY
ALSO
ADD
SEVERAL
NEW
FEATURES
TO
THE
GRAPHICAL
REPRESENTATION
FIRST
THEY
INCLUDE
MULTIDIMENSIONAL
VARIABLES
SECOND
THEY
INCLUDE
VARIABLES
THAT
ARE
CON
SIDERED
AS
FIXED
AND
THESE
ARE
MARKED
BY
A
BULLET
WE
CONDITION
ON
THE
FIXED
VARIABLES
BUT
DO
NOT
DEFINE
A
PROBABILITY
DISTRIBUTION
OVER
THEM
FIGURE
DEPICTS
THE
FACTORIZATION
P
R
HI
XI
P
R
HI
P
R
XI
HI
Μ
Φ
Σ
FINALLY
WE
HAVE
ALSO
USED
PLATE
NOTATION
A
PLATE
IS
DEPICTED
AS
A
RECTANGLE
WITH
A
NUMBER
IN
THE
CORNER
IT
INDICATES
THAT
THE
QUANTITIES
INSIDE
THE
RECTANGLE
SHOULD
BE
REPEATED
THE
GIVEN
NUMBER
OF
TIMES
FOR
EXAMPLE
IN
FIGURE
THERE
ARE
I
COPIES
XI
HI
I
OF
THE
VARIABLES
X
AND
H
BUT
ONLY
ONE
SET
OF
PARAMETERS
Μ
Φ
AND
Σ
SUMMARY
TO
SUMMARIZE
WE
CAN
THINK
ABOUT
THE
STRUCTURE
OF
THE
JOINT
PROBABILITY
DISTRIBUTION
IN
THREE
WAYS
FIRST
WE
CAN
CONSIDER
THE
WAY
THAT
THE
PROBABILITY
DISTRIBUTION
FACTORIZES
SECOND
WE
CAN
EXAMINE
THE
DIRECTED
GRAPHICAL
MODEL
THIRD
WE
CAN
THINK
ABOUT
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
THERE
IS
A
ONE
TO
ONE
MAPPING
BETWEEN
DIRECTED
GRAPHICAL
MODELS
ACYCLIC
DI
RECTED
GRAPHS
OF
CONDITIONAL
PROBABILITY
RELATIONS
AND
FACTORIZATIONS
HOWEVER
THE
RELATIONSHIP
BETWEEN
THE
GRAPHICAL
MODEL
OR
FACTORIZATION
AND
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
IS
MORE
COMPLICATED
A
DIRECTED
GRAPHICAL
MODEL
OR
ITS
EQUIVALENT
FACTORIZATION
DETERMINES
A
SET
OF
CONDITIONAL
INDEPENDENCE
RELATIONS
HOWEVER
AS
WE
SHALL
SEE
LATER
IN
THIS
CHAPTER
THERE
ARE
SOME
SETS
OF
CONDITIONAL
INDEPENDENCE
RELATIONS
THAT
CANNOT
BE
REPRESENTED
BY
DIRECTED
GRAPHICAL
MODELS
UNDIRECTED
GRAPHICAL
MODELS
UNDIRECTED
GRAPHICAL
MODELS
IN
THIS
SECTION
WE
INTRODUCE
A
SECOND
FAMILY
OF
GRAPHICAL
MODELS
UNDIRECTED
GRAPH
ICAL
MODELS
REPRESENT
PROBABILITY
DISTRIBUTIONS
OVER
VARIABLES
XN
FORM
OF
A
PRODUCT
OF
POTENTIAL
FUNCTIONS
Φ
N
SO
THAT
N
N
THAT
TAKE
THE
PROBLEM
PROBLEM
PROBLEM
P
R
X
TT
Φ
X
WHERE
THE
POTENTIAL
FUNCTION
ΦC
N
ALWAYS
RETURNS
A
POSITIVE
NUMBER
SINCE
THE
PROBABILITY
INCREASES
WHEN
ΦC
N
INCREASES
EACH
OF
THESE
FUNCTIONS
MODULATES
THE
TENDENCY
FOR
THE
VARIABLES
N
TO
TAKE
CERTAIN
VALUES
THE
PROBABILITY
IS
GREATEST
WHERE
ALL
OF
THE
FUNCTIONS
C
RETURN
HIGH
VALUES
HOWEVER
IT
SHOULD
BE
EMPHASIZED
THAT
POTENTIAL
FUNCTIONS
ARE
NOT
THE
SAME
AS
CONDITIONAL
PROBABILITIES
AND
THERE
IS
NOT
USUALLY
A
CLEAR
WAY
TO
MAP
FROM
ONE
TO
THE
OTHER
THE
TERM
Z
IS
KNOWN
AS
THE
PARTITION
FUNCTION
AND
NORMALIZES
THE
PRODUCT
OF
THESE
POSITIVE
FUNCTIONS
SO
THAT
THE
TOTAL
PROBABILITY
IS
ONE
IN
THE
DISCRETE
CASE
IT
WOULD
BE
COMPUTED
AS
Z
TT
ΦC
N
FOR
REALISTICALLY
SIZED
SYSTEMS
THIS
SUM
WILL
BE
INTRACTABLE
WE
WILL
NOT
BE
ABLE
TO
COMPUTE
Z
AND
HENCE
WILL
ONLY
BE
ABLE
TO
COMPUTE
THE
OVERALL
PROBABILITY
UP
TO
AN
UNKNOWN
SCALE
FACTOR
WE
CAN
EQUIVALENTLY
WRITE
EQUATION
AS
P
R
N
Z
EXP
C
ΨC
N
WHERE
ΨC
N
LOG
ΦC
N
WHEN
WRITTEN
IN
THIS
FORM
THE
PROBABILITY
IS
REFERRED
TO
AS
A
GIBBS
DISTRIBUTION
THE
TERMS
ΨC
N
ARE
FUNCTIONS
THAT
MAY
RETURN
ANY
REAL
NUMBER
AND
CAN
BE
THOUGHT
OF
AS
REPRESENTING
A
COST
FOR
EVERY
COMBINATION
OF
LABELS
N
AS
THE
COST
INCREASES
THE
PROBABILITY
DECREASES
THE
TOTAL
COST
C
ΨC
N
IS
SOMETIMES
KNOWN
AS
THE
ENERGY
AND
THE
PROCESS
OF
FITTING
THE
MODEL
INCREASING
THE
PROBABILITY
IS
HENCE
SOMETIMES
TERMED
ENERGY
MINIMIZATION
WHEN
EACH
POTENTIAL
FUNCTION
Φ
OR
ALTERNATIVELY
EACH
COST
FUNCTION
Ψ
ADDRESSES
ALL
OF
THE
VARIABLES
N
THE
UNDIRECTED
GRAPHICAL
MODEL
IS
KNOWN
AS
A
PRODUCT
OF
EXPERTS
HOWEVER
IN
COMPUTER
VISION
IT
IS
MORE
COMMON
FOR
EACH
POTENTIAL
FUNCTION
TO
OPERATE
ON
A
SUBSET
OF
THE
VARIABLES
XN
N
THESE
SUBSETS
ARE
CALLED
CLIQUES
AND
IT
IS
THE
CHOICE
OF
THESE
CLIQUES
THAT
DETERMINES
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
DENOTING
THE
CTH
CLIQUE
BY
C
WE
CAN
REWRITE
EQUATION
AS
P
R
X
TT
Φ
FIGURE
EXAMPLE
UNDIRECTED
GRAPHICAL
MODEL
RELATING
VARIABLES
AND
THIS
MODEL
IM
PLIES
THAT
THE
JOINT
PROBABILITY
CAN
BE
FACTORIZED
AS
P
R
IN
OTHER
WORDS
THE
PROBABILITY
DISTRIBUTION
IS
FACTORIZED
INTO
A
PRODUCT
OF
TERMS
EACH
OF
WHICH
ONLY
DEPENDS
ON
A
SUBSET
OF
VARIABLES
IN
THIS
SITUATION
THE
MODEL
IS
SOMETIMES
REFERRED
TO
AS
A
MARKOV
RANDOM
FIELD
TO
VISUALIZE
THE
UNDIRECTED
GRAPHICAL
MODEL
WE
DRAW
ONE
NODE
PER
RANDOM
VARIABLE
THEN
FOR
EVERY
CLIQUE
SC
WE
DRAW
A
CONNECTION
FROM
EVERY
MEMBER
VARIABLE
XI
C
TO
EVERY
OTHER
MEMBER
VARIABLE
MOVING
IN
THE
OPPOSITE
DIRECTION
WE
CAN
TAKE
A
GRAPHICAL
MODEL
AND
ESTABLISH
THE
UNDERLYING
FACTORIZATION
USING
THE
FOLLOWING
METHOD
WE
ADD
ONE
TERM
TO
THE
FACTORIZATION
PER
MAXIMAL
CLIQUE
SEE
FIGURE
A
MAXIMAL
CLIQUE
IS
A
FULLY
CONNECTED
SUBSET
OF
NODES
I
E
A
SUBSET
WHERE
EVERY
NODE
IS
CONNECTED
TO
EVERY
OTHER
WHERE
IT
IS
NOT
POSSIBLE
TO
ADD
ANOTHER
NODE
AND
REMAIN
FULLY
CONNECTED
IT
IS
MUCH
EASIER
TO
ESTABLISH
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
FROM
AN
UNDIRECTED
GRAPHICAL
MODEL
THAN
FOR
DIRECTED
GRAPHICAL
MODELS
THEY
CAN
BE
FOUND
USING
THE
FOLLOWING
PROPERTY
ONE
SET
OF
NODES
IS
CONDITIONALLY
INDEPENDENT
OF
ANOTHER
GIVEN
A
THIRD
IF
THE
THIRD
SET
SEPARATES
THEM
PREVENTS
A
PATH
FROM
THE
FIRST
NODE
TO
THE
SECOND
IT
FOLLOWS
THAT
A
NODE
IS
CONDITIONALLY
INDEPENDENT
OF
ALL
OTHER
NODES
GIVEN
ITS
SET
OF
IMMEDIATE
NEIGHBORS
AND
SO
THESE
NEIGHBORS
FORM
THE
MARKOV
BLANKET
EXAMPLE
CONSIDER
THE
GRAPHICAL
MODEL
IN
FIGURE
THIS
REPRESENTS
THE
FACTORIZATION
P
R
Z
WE
CAN
IMMEDIATELY
SEE
THAT
VARIABLE
IS
CONDITIONALLY
INDEPENDENT
OF
VARIABLE
GIVEN
BECAUSE
SEPARATES
THE
OTHER
TWO
VARIABLES
IT
BLOCKS
THE
PATH
FROM
TO
IN
THIS
CASE
THE
CONDITIONAL
INDEPENDENCE
RELATION
IS
EASY
TO
PROVE
P
R
X
X
X
P
R
P
R
Φ
X
X
Φ
X
X
DX
Φ
X
X
DX
THE
FINAL
EXPRESSION
DOES
NOT
DEPEND
ON
AND
SO
WE
CONCLUDE
THAT
IS
CONDI
TIONALLY
INDEPENDENT
OF
GIVEN
EXAMPLE
FIGURE
EXAMPLE
UNDIRECTED
GRAPHICAL
MODEL
REPRESENTING
VARIABLES
XI
I
THE
ASSOCIATED
PROBABILITY
DISTRIBUTION
FACTORIZES
INTO
A
PRODUCT
OF
ONE
POTENTIAL
FUNCTION
PER
MAXIMAL
CLIQUE
THE
CLIQUE
IS
A
MAXIMAL
CLIQUE
AS
THERE
IS
NO
OTHER
NODE
THAT
WE
CAN
ADD
THAT
CONNECTS
TO
EVERY
NODE
IN
THE
CLIQUE
THE
CLIQUE
IS
NOT
A
MAXIMAL
CLIQUE
AS
IT
IS
POSSIBLE
TO
ADD
NODE
AND
ALL
THREE
NODES
IN
THE
NEW
CLIQUE
ARE
CON
NECTED
TO
EACH
OTHER
CONSIDER
THE
GRAPHICAL
MODEL
IN
FIGURE
THERE
ARE
FOUR
MAXIMAL
CLIQUES
IN
THIS
GRAPH
AND
SO
IT
REPRESENTS
THE
FACTORIZATION
P
R
Z
WE
CAN
DEDUCE
VARIOUS
CONDITIONAL
INDEPENDENCE
RELATIONS
FROM
THE
GRAPHICAL
REPRESENTATION
FOR
EXAMPLE
VARIABLE
IS
CONDITIONALLY
INDEPENDENT
OF
VARIABLES
AND
GIVEN
AND
AND
VARIABLE
IS
INDEPENDENT
OF
VARIABLES
AND
GIVEN
AND
AND
SO
ON
NOTE
ALSO
THAT
THE
FACTORIZATION
P
R
Z
CREATES
THE
SAME
GRAPHICAL
MODEL
THERE
IS
A
MANY
TO
ONE
MAPPING
FROM
FACTORIZA
TIONS
TO
UNDIRECTED
GRAPHICAL
MODELS
AS
OPPOSED
TO
THE
ONE
TO
ONE
MAPPING
FOR
DIRECTED
GRAPHICAL
MODELS
WHEN
WE
COMPUTE
A
FACTORIZATION
FROM
THE
GRAPHICAL
MODEL
BASED
ON
THE
MAXIMAL
CLIQUES
WE
DO
SO
IN
A
CONSERVATIVE
WAY
IT
IS
POSSIBLE
THAT
THERE
ARE
FURTHER
REDUNDANCIES
WHICH
WERE
NOT
MADE
EXPLICIT
BY
THE
UNDIRECTED
GRAPHICAL
MODEL
COMPARING
DIRECTED
AND
UNDIRECTED
GRAPHICAL
MODELS
IN
SECTIONS
AND
WE
HAVE
DISCUSSED
DIRECTED
AND
UNDIRECTED
GRAPHICAL
MOD
ELS
RESPECTIVELY
EACH
GRAPHICAL
MODEL
REPRESENTS
A
FACTORIZATION
OF
THE
PROBABILITY
DISTRIBUTION
WE
HAVE
PRESENTED
METHODS
TO
EXTRACT
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
FROM
EACH
TYPE
OF
GRAPHICAL
MODEL
THE
PURPOSE
OF
THIS
SECTION
IS
TO
ARGUE
THAT
THESE
REPRESENTATIONS
ARE
NOT
EQUIVALENT
THERE
ARE
PATTERNS
OF
CONDI
TIONAL
INDEPENDENCE
THAT
CAN
BE
REPRESENTED
BY
DIRECTED
GRAPHICAL
MODELS
BUT
NOT
UNDIRECTED
GRAPHICAL
MODELS
AND
VICE
VERSA
GRAPHICAL
MODELS
A
B
C
FIGURE
DIRECTED
VS
UNDIRECTED
GRAPHICAL
MODELS
A
DIRECTED
GRAPH
ICAL
MODEL
WITH
THREE
NODES
THERE
IS
ONLY
ONE
CONDITIONAL
INDEPENDENCE
RELATION
IMPLIED
BY
THIS
MODEL
THE
NODE
IS
THE
MARKOV
BLANKET
OF
NODE
SHADED
AREA
AND
SO
WHERE
THE
NOTATION
CAN
BE
READ
AS
IS
INDEPENDENT
OF
B
THIS
UNDIRECTED
GRAPHICAL
MODEL
IMPLIES
THE
SAME
CONDITIONAL
INDEPENDENCE
RELATION
C
SECOND
DIRECTED
GRAPHICAL
MODEL
THE
RELATION
IS
NO
LONGER
TRUE
BUT
AND
ARE
INDEPENDENT
IF
WE
DON
T
CONDITION
ON
SO
WE
CAN
WRITE
THERE
IS
NO
UNDIRECTED
GRAPHICAL
MODEL
WITH
THREE
VARIABLES
THAT
HAS
THIS
PATTERN
OF
INDEPENDENCE
AND
CONDITIONAL
INDEPENDENCE
PROBLEM
PROBLEM
PROBLEM
FIGURES
B
SHOW
AN
UNDIRECTED
AND
DIRECTED
GRAPHICAL
MODEL
THAT
DO
REP
RESENT
THE
SAME
CONDITIONAL
INDEPENDENCE
RELATIONS
HOWEVER
FIGURE
SHOWS
A
DIRECTED
GRAPHICAL
MODEL
FOR
WHICH
THERE
IS
NO
EQUIVALENT
UNDIRECTED
GRAPHICAL
MODEL
THERE
IS
SIMPLY
NO
WAY
TO
INDUCE
THE
SAME
PATTERN
OF
INDEPENDENCE
AND
CONDITIONAL
INDEPENDENCE
WITH
AN
UNDIRECTED
GRAPHICAL
MODEL
CONVERSELY
FIGURE
SHOWS
AN
UNDIRECTED
GRAPHICAL
MODEL
THAT
INDUCES
A
PATTERN
OF
CONDITIONAL
INDEPENDENCE
RELATIONS
THAT
CANNOT
BE
REPLICATED
BY
ANY
DIRECTED
GRAPHICAL
MODEL
FIGURE
SHOWS
A
DIRECTED
GRAPHICAL
MODEL
THAT
IS
CLOSE
BUT
STILL
NOT
EQUIVALENT
THE
MARKOV
BLANKET
OF
IS
DIFFERENT
IN
EACH
MODEL
AND
SO
ARE
ITS
CONDITIONAL
INDEPENDENCE
RELATIONS
WE
CONCLUDE
FROM
THIS
BRIEF
ARGUMENT
THAT
DIRECTED
AND
UNDIRECTED
GRAPHICAL
MODELS
DO
NOT
REPRESENT
THE
SAME
SUBSET
OF
INDEPENDENCE
AND
CONDITIONAL
INDEPEN
DENCE
RELATIONS
AND
SO
WE
CANNOT
ELIMINATE
ONE
OR
THE
OTHER
FROM
OUR
CONSIDERATION
IN
FACT
THERE
ARE
OTHER
PATTERNS
OF
CONDITIONAL
INDEPENDENCE
THAT
CANNOT
BE
REP
RESENTED
BY
EITHER
TYPE
OF
MODEL
HOWEVER
THESE
WILL
NOT
BE
CONSIDERED
IN
THIS
BOOK
FOR
FURTHER
INFORMATION
CONCERNING
THE
FAMILIES
OF
DISTRIBUTIONS
THAT
CAN
BE
REPRESENTED
BY
DIFFERENT
TYPES
OF
GRAPHICAL
MODEL
CONSULT
BARBER
OR
KOLLER
FRIEDMAN
GRAPHICAL
MODELS
IN
COMPUTER
VISION
A
B
FIGURE
DIRECTED
VS
UNDIRECTED
MODELS
A
THIS
UNDIRECTED
GRAPHICAL
MODEL
INDUCES
TWO
CONDITIONAL
INDEPENDENCE
RELATIONS
HOWEVER
THERE
IS
NO
EQUIVALENT
DIRECTED
GRAPHICAL
MODEL
THAT
PRODUCES
THE
SAME
PATTERN
B
THIS
DIRECTED
GRAPHICAL
MODEL
ALSO
INDUCES
TWO
CONDITIONAL
INDEPENDENCE
RELATIONS
BUT
THEY
ARE
NOT
THE
SAME
IN
BOTH
CASES
THE
SHADED
REGION
REPRESENTS
THE
MARKOV
BLANKET
OF
VARIABLE
GRAPHICAL
MODELS
IN
COMPUTER
VISION
WE
WILL
NOW
INTRODUCE
A
NUMBER
OF
COMMON
VISION
MODELS
AND
LOOK
AT
THEIR
ASSOCIATED
GRAPHICAL
MODELS
WE
WILL
DISCUSS
EACH
OF
THESE
IN
DETAIL
IN
SUBSEQUENT
CHAPTERS
HOWEVER
IT
IS
INSTRUCTIVE
TO
SEE
THEM
TOGETHER
FIGURE
SHOWS
THE
GRAPHICAL
MODEL
FOR
A
HIDDEN
MARKOV
MODEL
OR
HMM
PROBLEM
PROBLEM
WE
OBSERVE
A
SEQUENCE
OF
MEASUREMENTS
XN
N
EACH
OF
WHICH
TELLS
US
SOMETHING
ABOUT
THE
CORRESPONDING
DISCRETE
WORLD
STATE
WN
N
ADJACENT
WORLD
STATES
ARE
CONNECTED
TOGETHER
SO
THAT
THE
PREVIOUS
WORLD
STATE
INFLUENCES
THE
CURRENT
ONE
AND
POTENTIALLY
RESOLVES
SITUATIONS
WHERE
THE
MEASUREMENTS
ARE
AMBIGUOUS
A
PROTOTYPICAL
APPLICATION
WOULD
BE
TRACKING
SEQUENCES
OF
SIGN
LANGUAGE
GESTURES
FIGURE
THERE
IS
INFORMATION
AT
EACH
FRAME
ABOUT
WHICH
GESTURE
IS
PRESENT
BUT
IT
MAY
BE
AMBIGUOUS
HOWEVER
WE
CAN
IMPOSE
PRIOR
KNOWLEDGE
THAT
CERTAIN
SIGNS
ARE
MORE
LIKELY
TO
FOLLOW
OTHERS
USING
THE
HMM
AND
GET
AN
IMPROVED
RESULT
FIGURE
REPRESENTS
A
MARKOV
TREE
AGAIN
WE
OBSERVE
A
NUMBER
OF
MEA
SUREMENTS
EACH
OF
WHICH
PROVIDES
INFORMATION
ABOUT
THE
ASSOCIATED
DISCRETE
WORLD
STATE
HOWEVER
THE
WORLD
STATES
ARE
NOW
CONNECTED
IN
A
TREE
STRUCTURE
A
PROTOTYP
ICAL
APPLICATION
WOULD
BE
HUMAN
BODY
FITTING
FIGURE
WHERE
EACH
UNKNOWN
WORLD
STATE
REPRESENTS
A
BODY
PART
THE
PARTS
OF
THE
BODY
NATURALLY
HAVE
A
TREE
STRUCTURE
AND
SO
IT
MAKES
SENSE
TO
BUILD
A
MODEL
THAT
EXPLOITS
THIS
FIGURE
ILLUSTRATES
THE
USE
OF
A
MARKOV
RANDOM
FIELD
OR
MRF
AS
A
PRIOR
THE
MRF
HERE
DESCRIBES
THE
WORLD
STATE
AS
A
GRID
OF
UNDIRECTED
CONNECTIONS
EACH
NODE
MIGHT
CORRESPOND
TO
A
PIXEL
THERE
IS
ALSO
A
MEASUREMENT
VARIABLE
ASSOCIATED
WITH
EACH
WORLD
STATE
VARIABLE
THESE
PAIRS
ARE
CONNECTED
WITH
DIRECTED
LINKS
SO
OVERALL
THIS
IS
A
MIXED
MODEL
PARTLY
DIRECTED
AND
PARTLY
UNDIRECTED
A
PROTOTYPICAL
APPLICATION
OF
AN
MRF
IN
VISION
WOULD
BE
FOR
SEMANTIC
LABELING
FIGURE
THE
A
B
C
D
E
F
G
H
FIGURE
COMMONLY
USED
GRAPHICAL
MODELS
IN
COMPUTER
VISION
A
HIDDEN
MARKOV
MODEL
HMM
B
ONE
POSSIBLE
APPLICATION
OF
THE
HMM
IS
INTER
PRETING
SIGN
LANGUAGE
SEQUENCES
THE
CHOICE
OF
SIGN
AT
TIME
N
DEPENDS
ON
THE
SIGN
AT
TIME
N
C
MARKOV
TREE
D
AN
EXAMPLE
APPLICATION
IS
FITTING
A
TREE
STRUCTURED
BODY
MODEL
E
MARKOV
RANDOM
FIELD
MRF
PRIOR
WITH
IN
DEPENDENT
OBSERVATIONS
F
THE
MRF
IS
OFTEN
USED
AS
A
PRIOR
DISTRIBUTION
IN
SEMANTIC
LABELING
TASKS
HERE
THE
GOAL
IS
TO
INFER
A
BINARY
LABEL
AT
EACH
PIXEL
DETERMINING
WHETHER
IT
BELONGS
TO
THE
COW
OR
THE
GRASS
G
KALMAN
FILTER
AN
EXAMPLE
APPLICATION
IS
TRACKING
AN
OBJECT
THROUGH
A
SEQUENCE
IT
HAS
THE
SAME
GRAPHICAL
MODEL
AS
THE
HMM
BUT
THE
UNKNOWN
QUANTITIES
ARE
CONTINUOUS
AS
OPPOSED
TO
DISCRETE
MEASUREMENTS
CONSTITUTE
THE
RGB
VALUES
AT
EACH
POSITION
THE
WORLD
STATE
AT
EACH
PIXEL
IS
A
DISCRETE
VARIABLE
THAT
DETERMINES
THE
CLASS
OF
OBJECT
PRESENT
I
E
COW
VS
GRASS
THE
MARKOV
RANDOM
FIELD
PRIOR
TIES
TOGETHER
ALL
OF
THE
INDIVIDUAL
CLASSIFIERS
TO
HELP
YIELD
A
SOLUTION
THAT
MAKES
GLOBAL
SENSE
FINALLY
FIGURE
SHOWS
THE
KALMAN
FILTER
THIS
HAS
THE
SAME
GRAPHICAL
MODEL
AS
THE
HIDDEN
MARKOV
MODEL
BUT
IN
THIS
CASE
THE
WORLD
STATE
IS
CONTINUOUS
RATHER
THAN
DISCRETE
A
PROTOTYPICAL
APPLICATION
OF
THE
KALMAN
FILTER
IS
FOR
TRACKING
OBJECTS
THROUGH
A
TIME
SEQUENCE
FIGURE
AT
EACH
TIME
WE
MIGHT
WANT
TO
KNOW
THE
POSITION
SIZE
AND
ORIENTATION
OF
THE
HAND
HOWEVER
IN
A
GIVEN
FRAME
THE
MEASUREMENTS
MIGHT
BE
POOR
THE
FRAME
MAY
BE
BLURRED
OR
THE
OBJECT
MAY
BE
TEMPORARILY
OCCLUDED
BY
BUILDING
A
MODEL
THAT
CONNECTS
THE
ESTIMATES
FROM
ADJACENT
FRAMES
WE
CAN
INCREASE
THE
ROBUSTNESS
TO
THESE
FACTORS
EARLIER
FRAMES
CAN
RESOLVE
THE
UNCERTAINTY
IN
THE
CURRENT
AMBIGUOUS
FRAME
NOTICE
THAT
ALL
OF
THESE
GRAPHICAL
MODELS
HAVE
DIRECTED
LINKS
FROM
THE
WORLD
W
TO
THE
DATA
X
THAT
INDICATE
A
RELATIONSHIP
OF
THE
FORM
P
R
X
W
HENCE
THEY
ALL
CONSTRUCT
A
PROBABILITY
DISTRIBUTION
OVER
THE
DATA
AND
ARE
GENERATIVE
MODELS
WE
WILL
ALSO
CONSIDER
DISCRIMINATIVE
MODELS
BUT
HISTORICALLY
SPEAKING
GENERATIVE
MODELS
OF
THIS
KIND
HAVE
BEEN
MORE
IMPORTANT
EACH
MODEL
IS
QUITE
SPARSELY
CON
NECTED
EACH
DATA
VARIABLE
X
CONNECTS
ONLY
TO
ONE
WORLD
STATE
VARIABLE
W
AND
EACH
WORLD
STATE
VARIABLE
CONNECTS
TO
ONLY
A
FEW
OTHERS
THE
RESULT
OF
THIS
IS
THAT
THERE
ARE
MANY
CONDITIONAL
INDEPENDENCE
RELATIONS
IN
THE
MODEL
WE
WILL
EXPLOIT
THESE
REDUNDANCIES
TO
DEVELOP
EFFICIENT
ALGORITHMS
FOR
LEARNING
AND
INFERENCE
WE
WILL
RETURN
TO
ALL
OF
THESE
MODELS
LATER
IN
THE
BOOK
WE
INVESTIGATE
THE
HIDDEN
MARKOV
MODEL
AND
THE
MARKOV
TREE
IN
CHAPTER
WE
DISCUSS
THE
MARKOV
RANDOM
FIELD
IN
CHAPTER
AND
WE
WILL
PRESENT
THE
KALMAN
FILTER
IN
CHAPTER
THE
REMAINING
PART
OF
THIS
CHAPTER
ANSWERS
TWO
QUESTIONS
I
HOW
CAN
WE
PERFORM
INFERENCE
WHEN
THERE
ARE
A
LARGE
NUMBER
OF
UNKNOWN
WORLD
VARIABLES
II
WHAT
ARE
THE
IMPLICATIONS
OF
USING
A
DIRECTED
GRAPHICAL
MODEL
VS
AN
UNDIRECTED
ONE
INFERENCE
IN
MODELS
WITH
MANY
UNKNOWNS
WE
WILL
NOW
CONSIDER
INFERENCE
IN
THESE
MODELS
IDEALLY
WE
WOULD
COMPUTE
THE
FULL
POSTERIOR
DISTRIBUTION
P
R
N
N
USING
BAYES
RULE
HOWEVER
THE
UNKNOWN
WORLD
STATES
IN
THE
PRECEDING
MODELS
ARE
GENERALLY
MUCH
LARGER
THAN
PREVIOUSLY
CONSIDERED
IN
THIS
BOOK
AND
THIS
MAKES
INFERENCE
CHALLENGING
FOR
EXAMPLE
CONSIDER
THE
SPACE
OF
WORLD
STATES
IN
THE
HMM
EXAMPLE
IF
WE
ARE
GIVEN
FRAMES
OF
VIDEO
AND
THERE
ARE
COMMON
SIGNS
IN
THE
SIGN
LANGUAGE
THEN
THERE
ARE
POSSIBLE
STATES
IT
IS
CLEARLY
NOT
PRACTICAL
TO
COMPUTE
AND
STORE
THE
POSTERIOR
PROBABILITY
ASSOCIATED
WITH
EACH
EVEN
WHEN
THE
WORLD
STATES
ARE
CONTINUOUS
COMPUTING
AND
STORING
THE
PARAMETERS
OF
A
HIGH
DIMENSIONAL
PROB
ABILITY
MODEL
IS
STILL
PROBLEMATIC
FORTUNATELY
THERE
ARE
ALTERNATIVE
APPROACHES
TO
INFERENCE
WHICH
WE
NOW
CONSIDER
IN
TURN
FIGURE
MAP
SOLUTION
VS
MAX
MARGINALS
SOLUTION
THE
MAIN
FIGURE
SHOWS
THE
JOINT
POSTERIOR
DISTRIBUTION
P
R
THE
MAP
SOLU
TION
IS
AT
THE
PEAK
OF
THIS
DISTRIBU
TION
AT
HIGHLIGHTED
IN
GREEN
THE
FIGURE
ALSO
SHOWS
THE
TWO
MARGINAL
DISTRIBUTIONS
P
R
AND
P
R
THE
MAXIMUM
MARGINALS
SOLUTION
IS
COMPUTED
BY
IN
DIVIDUALLY
FINDING
THE
MAXIMUM
OF
EACH
MARGINAL
DISTRIBUTIONS
WHICH
GIVES
THE
SOLUTION
HIGHLIGHTED
IN
RED
FOR
THIS
DISTRI
BUTION
THIS
IS
VERY
UNREPRESENTATIVE
ALTHOUGH
THESE
LABELS
ARE
INDIVIDUALLY
LIKELY
THEY
RARELY
CO
OCCUR
AND
THE
JOINT
POSTERIOR
FOR
THIS
COMBINATION
HAS
LOW
PROBABILITY
FINDING
THE
MAP
SOLUTION
ONE
OBVIOUS
POSSIBILITY
IS
TO
FIND
THE
MAXIMUM
A
POSTERIORI
MAP
SOLUTION
N
ARGMAX
P
R
N
N
N
ARGMAX
P
R
N
N
P
R
N
N
THIS
IS
STILL
FAR
FROM
TRIVIAL
THE
NUMBER
OF
WORLD
STATES
IS
EXTREMELY
LARGE
SO
WE
CANNOT
POSSIBLY
EXPLORE
EVERY
ONE
AND
TAKE
THE
MAXIMUM
WE
MUST
EMPLOY
INTELLIGENT
AND
EFFICIENT
ALGORITHMS
THAT
EXPLOIT
THE
REDUNDANCIES
IN
THE
DISTRIBUTION
TO
FIND
THE
CORRECT
SOLUTION
WHERE
POSSIBLE
HOWEVER
AS
WE
SHALL
SEE
FOR
SOME
MODELS
THERE
IS
NO
KNOWN
POLYNOMIAL
ALGORITHM
TO
FIND
THE
MAP
SOLUTION
FINDING
THE
MARGINAL
POSTERIOR
DISTRIBUTION
AN
ALTERNATIVE
STRATEGY
IS
TO
FIND
THE
MARGINAL
POSTERIOR
DISTRIBUTIONS
P
R
WN
N
P
R
N
N
N
DWN
N
SINCE
EACH
OF
THESE
DISTRIBUTIONS
IS
OVER
A
SINGLE
LABEL
IT
IS
NOT
IMPLAUSIBLE
TO
COMPUTE
AND
STORE
EACH
ONE
SEPARATELY
OBVIOUSLY
IT
IS
NOT
POSSIBLE
TO
DO
THIS
BY
DIRECTLY
COMPUTING
THE
EXTREMELY
LARGE
JOINT
DISTRIBUTION
AND
MARGINALIZING
IT
DIRECTLY
WE
MUST
USE
ALGORITHMS
THAT
EXPLOIT
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
IN
THE
DISTRIBUTION
TO
EFFICIENTLY
COMPUTE
THE
MARGINALS
DRAWING
SAMPLES
MAXIMUM
MARGINALS
IF
WE
WANT
A
SINGLE
ESTIMATE
OF
THE
WORLD
STATE
WE
COULD
RETURN
THE
MAXIMUM
VALUES
OF
THE
MARGINAL
DISTRIBUTIONS
GIVING
THE
CRITERION
WˆN
ARGMAX
P
R
WN
N
WN
THIS
PRODUCES
ESTIMATES
OF
EACH
WORLD
STATE
THAT
ARE
INDIVIDUALLY
MOST
PROBABLE
BUT
WHICH
MAY
NOT
REFLECT
THE
JOINT
STATISTICS
FOR
EXAMPLE
WORLD
STATE
WN
MIGHT
BE
THE
MOST
PROBABLE
VALUE
FOR
THE
NTH
WORLD
STATE
AND
WM
MIGHT
BE
THE
MOST
PROBABLE
VALUE
FOR
THE
MTH
WORLD
STATE
BUT
IT
COULD
BE
THAT
THE
POSTERIOR
PROBABILITY
FOR
THIS
CONFIGURATION
IS
ZERO
ALTHOUGH
THE
STATES
ARE
INDIVIDUALLY
PROBABLE
THEY
NEVER
CO
OCCUR
FIGURE
SAMPLING
THE
POSTERIOR
FOR
SOME
MODELS
IT
IS
INTRACTABLE
TO
COMPUTE
EITHER
THE
MAP
SOLUTION
OR
THE
MARGINAL
DISTRIBUTIONS
ONE
POSSIBILITY
IN
THIS
CIRCUMSTANCE
IS
TO
DRAW
SAMPLES
FROM
THE
POSTERIOR
DISTRIBUTION
METHODS
BASED
ON
SAMPLING
THE
POSTERIOR
WOULD
FALL
UNDER
THE
MORE
GENERAL
CATEGORY
OF
APPROXIMATE
INFERENCE
AS
THEY
DO
NOT
NORMALLY
RETURN
THE
TRUE
ANSWER
HAVING
DRAWN
A
NUMBER
OF
SAMPLES
FROM
THE
POSTERIOR
WE
CAN
APPROXIMATE
THE
POSTERIOR
PROBABILITY
DISTRIBUTION
AS
A
MIXTURE
OF
DELTA
FUNCTIONS
WHERE
THERE
IS
ONE
DELTA
FUNCTION
AT
EACH
OF
THE
SAMPLE
POSITIONS
ALTERNATIVELY
WE
COULD
MAKE
ESTIMATES
OF
MARGINAL
STATISTICS
SUCH
AS
THE
MEAN
AND
VARIANCE
BASED
ON
THE
SAMPLED
VALUES
OR
SELECT
THE
SAMPLE
WITH
THE
HIGHEST
POSTERIOR
PROBABILITY
AS
AN
ESTIMATE
OF
THE
MAP
STATE
THIS
LATTER
APPROACH
HAS
THE
ADVANTAGE
OF
BEING
CONSISTENT
WITH
THE
FULL
POSTERIOR
DISTRIBUTION
AS
OPPOSED
TO
MAXIMUM
MARGINALS
WHICH
IS
NOT
EVEN
IF
WE
CANNOT
BE
SURE
THAT
WE
HAVE
THE
CORRECT
ANSWER
AN
ALTERNATIVE
WAY
TO
COMPUTE
A
POINT
ESTIMATE
FROM
A
SET
OF
SAMPLES
FROM
THE
POSTERIOR
IS
TO
COMPUTE
THE
EMPIRICAL
MAX
MARGINALS
WE
ESTIMATE
THE
MARGINAL
PROBABILITY
DISTRIBUTIONS
BY
LOOKING
AT
THE
MARGINAL
STATISTICS
OF
THE
SAMPLES
IN
OTHER
WORDS
WE
CONSIDER
ONE
VARIABLE
WN
AT
A
TIME
AND
LOOK
AT
THE
DISTRIBUTION
OF
DIFFERENT
VALUES
OBSERVED
FOR
A
DISCRETE
DISTRIBUTION
THIS
INFORMATION
IS
CAPTURED
IN
A
HISTOGRAM
FOR
A
CONTINUOUS
DISTRIBUTION
WE
COULD
FIT
A
UNIVARIATE
MODEL
SUCH
AS
A
NORMAL
DISTRIBUTION
TO
THESE
VALUES
TO
SUMMARIZE
THEM
DRAWING
SAMPLES
WE
HAVE
SEEN
THAT
SOME
OF
THE
APPROACHES
TO
INFERENCE
REQUIRE
US
TO
DRAW
SAMPLES
FROM
THE
POSTERIOR
DISTRIBUTION
WE
WILL
NOW
DISCUSS
HOW
TO
DO
THIS
FOR
BOTH
DIRECTED
AND
UNDIRECTED
MODELS
AND
WE
WILL
SEE
THAT
THIS
IS
GENERALLY
MORE
STRAIGHTFORWARD
IN
DIRECTED
MODELS
GRAPHICAL
MODELS
FIGURE
ANCESTRAL
SAMPLING
WE
WORK
OUR
WAY
THROUGH
THE
GRAPH
IN
AN
ORDER
RED
NUMBER
THAT
GUARANTEES
THAT
THE
PARENTS
OF
EVERY
NODE
ARE
VIS
ITED
BEFORE
THE
NODE
ITSELF
AT
EACH
STEP
WE
DRAW
A
SAMPLE
CONDITIONED
ON
THE
VALUES
OF
THE
SAMPLES
AT
THE
PARENTS
THIS
IS
GUARANTEED
TO
PRO
DUCE
A
VALID
SAMPLE
FROM
THE
FULL
JOINT
DISTRIBUTION
SAMPLING
FROM
DIRECTED
GRAPHICAL
MODELS
DIRECTED
GRAPHICAL
MODELS
TAKE
THE
FORM
OF
DIRECTED
ACYCLIC
GRAPHS
OF
CONDITIONAL
PROBABILITY
RELATIONS
THAT
HAVE
THE
FOLLOWING
ALGEBRAIC
FORM
I
P
R
N
P
R
XN
XPA
N
N
IT
IS
RELATIVELY
EASY
TO
SAMPLE
FROM
A
DIRECTED
GRAPHICAL
MODEL
USING
A
TECHNIQUE
KNOWN
AS
ANCESTRAL
SAMPLING
THE
IDEA
IS
TO
SAMPLE
EACH
VARIABLE
IN
THE
NETWORK
IN
TURN
WHERE
THE
ORDER
IS
SUCH
THAT
ALL
PARENTS
OF
A
NODE
ARE
SAMPLED
BEFORE
THE
NODE
ITSELF
AT
EACH
NODE
WE
CONDITION
ON
THE
OBSERVED
SAMPLE
VALUES
OF
THE
PARENTS
THE
SIMPLEST
WAY
TO
UNDERSTAND
THIS
IS
WITH
AN
EXAMPLE
CONSIDER
THE
DIRECTED
GRAPHICAL
MODEL
IN
FIGURE
WHOSE
PROBABILITY
DISTRIBUTION
FACTORIZES
AS
P
R
P
R
P
R
P
R
P
R
P
R
TO
SAMPLE
FROM
THIS
MODEL
WE
FIRST
IDENTIFY
AS
A
NODE
WITH
NO
PARENTS
AND
DRAW
A
SAMPLE
FROM
THE
DISTRIBUTION
P
R
LET
US
SAY
THE
OBSERVED
SAMPLE
AT
TOOK
VALUE
WE
NOW
TURN
TO
THE
REMAINING
NODES
NODE
IS
THE
ONLY
NODE
IN
THE
NETWORK
WHERE
ALL
OF
THE
PARENTS
HAVE
BEEN
PROCESSED
AND
SO
WE
TURN
OUR
ATTENTION
HERE
NEXT
WE
DRAW
A
SAMPLE
FROM
THE
DISTRIBUTION
P
R
TO
YIELD
A
SAMPLE
WE
NOW
SEE
THAT
WE
ARE
NOT
YET
READY
TO
SAMPLE
FROM
AS
NOT
ALL
OF
ITS
PARENTS
HAVE
BEEN
SAMPLED
BUT
WE
CAN
SAMPLE
FROM
THE
DISTRIBUTION
P
R
TO
YIELD
THE
VALUE
CONTINUING
THIS
PROCESS
WE
DRAW
FROM
P
R
AND
FINALLY
FROM
P
R
THE
RESULTING
VECTOR
W
IS
GUARANTEED
TO
BE
A
VALID
SAMPLE
FROM
THE
FULL
JOINT
DISTRIBUTION
P
R
AN
EQUIVALENT
WAY
TO
THINK
ABOUT
THIS
ALGORITHM
IS
TO
CONSIDER
IT
AS
WORKING
THROUGH
THE
TERMS
IN
THE
FACTORIZED
JOINT
DISTRIBUTION
RIGHT
HAND
SIDE
OF
EQUATION
SAMPLING
FROM
EACH
IN
TURN
CONDITIONED
ON
THE
PREVIOUS
VALUES
LEARNING
SAMPLING
FROM
UNDIRECTED
GRAPHICAL
MODELS
UNFORTUNATELY
IT
IS
MUCH
HARDER
TO
DRAW
SAMPLES
FROM
UNDIRECTED
MODELS
EXCEPT
IN
CERTAIN
SPECIAL
CASES
E
G
WHERE
THE
VARIABLES
ARE
CONTINUOUS
AND
GAUSSIAN
OR
WHERE
THE
GRAPH
STRUCTURE
TAKES
THE
FORM
OF
A
TREE
IN
GENERAL
GRAPHS
WE
CANNOT
USE
ANCESTRAL
SAMPLING
BECAUSE
I
THERE
IS
NO
SENSE
IN
WHICH
ANY
VARIABLE
IS
A
PARENT
TO
ANY
OTHER
SO
WE
DON
T
KNOW
WHICH
ORDER
TO
SAMPLE
IN
AND
II
THE
TERMS
Φ
IN
THE
FACTORIZATION
ARE
NOT
PROBABILITY
DISTRIBUTIONS
ANYWAY
ONE
WAY
TO
GENERATE
SAMPLES
FROM
ANY
COMPLEX
HIGH
DIMENSIONAL
PROBABILITY
DISTRIBUTION
IS
TO
USE
A
MARKOV
CHAIN
MONTE
CARLO
MCMC
METHOD
THE
PRIN
CIPLE
IS
TO
GENERATE
A
SERIES
CHAIN
OF
SAMPLES
FROM
THE
DISTRIBUTION
SO
THAT
EACH
SAMPLE
DEPENDS
DIRECTLY
ON
THE
PREVIOUS
ONE
HENCE
MARKOV
HOWEVER
THE
GENERATION
OF
THE
SAMPLE
IS
NOT
COMPLETELY
DETERMINISTIC
HENCE
MONTE
CARLO
ONE
OF
THE
SIMPLEST
MCMC
METHODS
IS
GIBBS
SAMPLING
WHICH
PROCEEDS
AS
FOLLOWS
FIRST
WE
RANDOMLY
CHOOSE
THE
INITIAL
STATE
X
USING
ANY
METHOD
WE
GENERATE
THE
NEXT
SAMPLE
IN
THE
CHAIN
X
BY
UPDATING
THE
STATE
AT
EACH
DIMENSION
ALGORITHM
XN
N
IN
TURN
IN
ANY
ORDER
TO
UPDATE
THE
NTH
DIMENSION
XN
WE
FIX
THE
OTHER
N
DIMENSIONS
AND
DRAW
FROM
THE
CONDITIONAL
DISTRIBUTION
P
R
XN
N
N
WHERE
THE
SET
N
N
DENOTES
ALL
OF
THE
N
VARIABLES
XN
EXCEPT
XN
HAVING
MODIFIED
EVERY
DIMENSION
IN
THIS
WAY
WE
OBTAIN
THE
SECOND
SAMPLE
IN
THE
CHAIN
THIS
IDEA
IS
ILLUSTRATED
IN
FIGURE
FOR
THE
MULTIVARIATE
NORMAL
DISTRIBUTION
WHEN
THIS
PROCEDURE
IS
REPEATED
A
VERY
LARGE
NUMBER
OF
TIMES
SO
THAT
THE
INITIAL
CONDITIONS
ARE
FORGOTTEN
A
SAMPLE
FROM
THIS
SEQUENCE
CAN
BE
CONSIDERED
AS
A
DRAW
FROM
THE
DISTRIBUTION
P
R
N
ALTHOUGH
THIS
IS
NOT
IMMEDIATELY
OBVIOUS
AND
A
PROOF
IS
BEYOND
THE
SCOPE
OF
THIS
BOOK
THIS
PROCEDURE
DOES
CLEARLY
HAVE
SOME
SENSIBLE
PROPERTIES
SINCE
WE
ARE
SAMPLING
FROM
THE
CONDITIONAL
PROBABILITY
DISTRI
BUTION
AT
EACH
PIXEL
WE
ARE
MORE
LIKELY
TO
CHANGE
THE
CURRENT
VALUE
TO
ONE
WHICH
HAS
AN
OVERALL
HIGHER
PROBABILITY
HOWEVER
THE
STOCHASTIC
UPDATE
RULE
PROVIDES
THE
POSSIBILITY
OF
INFREQUENTLY
VISITING
LESS
PROBABLE
REGIONS
OF
THE
SPACE
FOR
UNDIRECTED
GRAPHICAL
MODELS
THE
CONDITIONAL
DISTRIBUTION
P
R
XN
N
N
CAN
BE
QUITE
EFFICIENT
TO
EVALUATE
BECAUSE
OF
THE
CONDITIONAL
INDEPENDENCE
PROP
ERTIES
VARIABLE
XN
IS
CONDITIONALLY
INDEPENDENT
OF
THE
REST
OF
THE
NODES
GIVEN
ITS
IMMEDIATE
NEIGHBORS
AND
SO
COMPUTING
THIS
TERM
ONLY
INVOLVES
THE
IMMEDIATE
NEIGHBORS
HOWEVER
OVERALL
THIS
METHOD
IS
EXTREMELY
INEFFICIENT
IT
REQUIRES
A
LARGE
AMOUNT
OF
COMPUTATIONAL
EFFORT
TO
GENERATE
EVEN
A
SINGLE
SAMPLE
SAMPLING
FROM
DIRECTED
GRAPHICAL
MODELS
IS
FAR
EASIER
LEARNING
IN
THE
SECTION
WE
ARGUED
THAT
SAMPLING
FROM
DIRECTED
GRAPHICAL
MODELS
IS
CON
SIDERABLY
EASIER
THAN
SAMPLING
FROM
UNDIRECTED
GRAPHICAL
MODELS
IN
THIS
SECTION
WE
CONSIDER
LEARNING
IN
EACH
TYPE
OF
MODEL
AND
COME
TO
A
SIMILAR
CONCLUSION
NOTE
THAT
WE
ARE
NOT
DISCUSSING
THE
LEARNING
OF
THE
GRAPH
STRUCTURE
HERE
WE
ARE
TALKING
ABOUT
LEARNING
THE
PARAMETERS
OF
THE
MODEL
ITSELF
FOR
DIRECTED
GRAPHICAL
MOD
ELS
THESE
PARAMETERS
WOULD
DETERMINE
THE
CONDITIONAL
DISTRIBUTIONS
P
R
XN
XPA
N
GRAPHICAL
MODELS
A
B
FIGURE
GIBBS
SAMPLING
WE
GENERATE
A
CHAIN
OF
SAMPLES
BY
CYCLING
THROUGH
EACH
DIMENSION
IN
TURN
AND
DRAWING
A
SAMPLE
FROM
THE
CONDITIONAL
DISTRIBUTION
OF
THAT
DIMENSION
GIVEN
THAT
THE
OTHERS
ARE
FIXED
A
FOR
THIS
MULTIVARIATE
NORMAL
DISTRIBUTION
WE
START
AT
A
RANDOM
POSITION
X
WE
ALTERNATELY
DRAW
SAMPLES
FROM
THE
CONDITIONAL
DISTRIBUTION
OF
THE
FIRST
DIMEN
SION
KEEPING
THE
SECOND
FIXED
HORIZONTAL
CHANGES
AND
THE
SECOND
DIMENSION
KEEPING
THE
FIRST
FIXED
VERTICAL
CHANGES
FOR
THE
MULTIVARIATE
NORMAL
THESE
CONDITIONAL
DISTRIBUTIONS
ARE
THEMSELVES
NORMAL
SECTION
EACH
TIME
WE
CYCLE
THROUGH
BOTH
OF
THE
DIMENSIONS
WE
CREATE
A
NEW
SAMPLE
X
T
B
MANY
SAMPLES
GENERATED
USING
THIS
METHOD
AND
FOR
UNDIRECTED
GRAPHICAL
MODELS
THEY
WOULD
DETERMINE
THE
POTENTIAL
FUNCTIONS
ΦC
N
LEARNING
IN
DIRECTED
GRAPHICAL
MODELS
ANY
DIRECTED
GRAPHICAL
MODEL
CAN
BE
WRITTEN
IN
THE
FACTORIZED
FORM
N
P
R
XN
P
R
XN
XPA
N
Θ
N
WHERE
THE
CONDITIONAL
PROBABILITY
RELATIONS
FORM
A
DIRECTED
ACYCLIC
GRAPH
AND
Θ
DENOTES
THE
PARAMETERS
OF
THE
MODEL
FOR
EXAMPLE
IN
THE
DISCRETE
DISTRIBUTIONS
THAT
WE
HAVE
FOCUSED
ON
IN
THIS
CHAPTER
AN
INDIVIDUAL
CONDITIONAL
MODEL
MIGHT
BE
P
R
K
ΛK
WHERE
THE
PARAMETERS
HERE
ARE
ΛK
K
IN
GENERAL
THE
PARAMETERS
CAN
BE
LEARNED
USING
THE
MAXIMUM
LIKELIHOOD
METHOD
BY
FINDING
Θˆ
ARGMAX
RTTI
TT
P
R
XI
N
XI
PA
N
Θ
Θ
I
N
ARGMAX
R
I
LOG
P
R
XI
N
XI
PA
N
Θ
Θ
I
N
WHERE
XI
N
REPRESENTS
THE
NTH
DIMENSION
OF
THE
ITH
TRAINING
EXAMPLE
THIS
CRITERION
LEADS
TO
SIMPLE
LEARNING
ALGORITHMS
AND
OFTEN
THE
MAXIMUM
LIKELIHOOD
PARAMETERS
CAN
BE
COMPUTED
IN
CLOSED
FORM
LEARNING
IN
UNDIRECTED
GRAPHICAL
MODELS
AN
UNDIRECTED
GRAPHICAL
MODEL
IS
WRITTEN
AS
P
R
X
TT
Φ
X
Θ
WHERE
X
XN
AND
WE
HAVE
ASSUMED
THAT
THE
TRAINING
SAMPLES
ARE
INDEPENDENT
HOWEVER
IN
THIS
FORM
WE
MUST
CONSTRAIN
THE
PARAMETERS
SO
THAT
THEY
ENSURE
THAT
EACH
ΦC
ALWAYS
RETURNS
A
POSITIVE
NUMBER
A
MORE
PRACTICAL
APPROACH
IS
TO
RE
PARAMETERIZE
THE
UNDIRECTED
GRAPHICAL
MODEL
IN
THE
FORM
OF
THE
GIBBS
DISTRIBUTION
P
R
X
EXP
Z
C
ΨC
N
Θ
SO
THAT
WE
DO
NOT
HAVE
TO
WORRY
ABOUT
CONSTRAINTS
ON
THE
PARAMETERS
GIVEN
I
TRAINING
EXAMPLES
XI
I
WE
AIM
TO
FIT
PARAMETERS
Θ
ASSUMING
THAT
THE
TRAINING
EXAMPLES
ARE
INDEPENDENT
THE
MAXIMUM
LIKELIHOOD
SOLUTION
IS
Θˆ
ARGMAX
EXP
Z
Θ
I
ΨC
XI
Θ
Θ
I
C
ARGMAX
R
I
LOG
Z
Θ
ΨC
XI
Θ
Θ
I
C
WHERE
AS
USUAL
WE
HAVE
TAKEN
THE
LOG
TO
SIMPLIFY
THE
EXPRESSION
TO
MAXIMIZE
THIS
EXPRESSION
WE
CALCULATE
THE
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
L
WITH
RESPECT
TO
THE
PARAMETERS
Θ
I
C
L
I
LOG
Z
Θ
ΨC
XI
Θ
Θ
I
Θ
LOG
XI
I
C
C
C
Θ
Θ
ΨC
XI
Θ
LL
C
ΨC
XI
Θ
Θ
I
C
THE
SECOND
TERM
IS
READILY
COMPUTABLE
BUT
THE
FIRST
TERM
INVOLVES
AN
INTRACTABLE
SUM
OVER
ALL
POSSIBLE
VALUES
OF
THE
VARIABLE
X
WE
CANNOT
COMPUTE
THE
DERIVA
TIVE
WITH
RESPECT
TO
THE
PARAMETERS
FOR
REASONABLE
SIZED
MODELS
AND
SO
LEARNING
IS
DIFFICULT
MOREOVER
WE
CANNOT
EVALUATE
THE
ORIGINAL
PROBABILITY
EXPRESSION
EQUA
TION
AS
THIS
TOO
CONTAINS
AN
INTRACTABLE
SUM
CONSEQUENTLY
WE
CAN
T
COMPUTE
THE
DERIVATIVE
USING
FINITE
DIFFERENCES
EITHER
IN
SHORT
WE
CAN
NEITHER
FIND
AN
ALGEBRAIC
SOLUTION
NOR
USE
A
STRAIGHTFORWARD
OPTIMIZATION
TECHNIQUE
AS
WE
CANNOT
COMPUTE
THE
GRADIENT
THE
BEST
THAT
WE
CAN
DO
IS
TO
APPROXIMATE
THE
GRADIENT
ALGORITHM
CONTRASTIVE
DIVERGENCE
ONE
POSSIBLE
SOLUTION
TO
THIS
PROBLEM
IS
THE
CONTRASTIVE
DIVERGENCE
ALGORITHM
THIS
IS
A
METHOD
FOR
APPROXIMATING
THE
GRADIENT
OF
THE
LOG
LIKELIHOOD
WITH
RESPECT
TO
PARAMETERS
Θ
FOR
FUNCTIONS
WITH
THE
GENERAL
FORM
P
R
X
Z
Θ
F
X
Θ
WHERE
Z
Θ
X
F
X
Θ
IS
THE
NORMALIZING
CONSTANT
AND
THE
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
IS
LOG
P
R
X
Θ
LOG
Z
Θ
Θ
LOG
F
X
Θ
Θ
THE
MAIN
IDEA
BEHIND
CONTRASTIVE
DIVERGENCE
FOLLOWS
FROM
SOME
ALGEBRAIC
MANIPU
LATION
OF
THE
FIRST
TERM
LOG
Z
Θ
Θ
Z
Θ
Z
Θ
Θ
Z
Θ
X
F
X
Θ
Θ
F
X
Θ
F
X
Θ
LOG
F
X
Θ
P
R
X
LOG
F
X
Θ
Θ
X
WHERE
WE
HAVE
USED
THE
RELATION
LOG
F
X
X
F
X
X
F
X
BETWEEN
THE
THIRD
AND
FOURTH
LINES
THE
FINAL
TERM
IN
EQUATION
IS
THE
EXPECTATION
OF
THE
DERIVATIVE
OF
LOG
F
X
Θ
WE
CANNOT
COMPUTE
THIS
EXACTLY
BUT
WE
CAN
APPROXIMATE
IT
BY
DRAWING
J
INDE
PENDENT
SAMPLES
X
FROM
THE
DISTRIBUTION
TO
YIELD
J
LOG
F
X
Θ
LOG
Z
Θ
P
R
X
LOG
F
X
Θ
J
FIGURE
THE
CONTRASTIVE
DIVER
GENCE
ALGORITHM
CHANGES
THE
PARAME
TERS
SO
THAT
THE
UN
NORMALIZED
DISTRI
BUTION
INCREASES
AT
THE
OBSERVED
DATA
POINTS
BLUE
CROSSES
BUT
DECREASES
AT
SAMPLED
DATA
POINTS
FROM
THE
MODEL
THESE
TWO
COMPONENTS
COUNTERBAL
ANCE
ONE
ANOTHER
AND
ENSURE
THAT
THE
LIKELIHOOD
INCREASES
WHEN
THE
MODEL
FITS
THE
DATA
THESE
TWO
FORCES
WILL
CAN
CEL
OUT
AND
THE
PARAMETERS
WILL
REMAIN
CONSTANT
WITH
I
TRAINING
EXAMPLES
XI
I
THE
GRADIENT
OF
THE
LOG
LIKELIHOOD
L
IS
HENCE
J
LOG
F
X
Θ
I
L
I
J
LOG
F
XI
Θ
A
VISUAL
EXPLANATION
OF
THIS
EXPRESSION
IS
PRESENTED
IN
FIGURE
THE
GRADIENT
POINTS
IN
A
DIRECTION
THAT
I
INCREASES
THE
LOGARITHM
OF
THE
UN
NORMALIZED
FUNCTION
AT
THE
DATA
POINTS
XI
BUT
II
DECREASES
THE
SAME
QUANTITY
IN
PLACES
WHERE
THE
MODEL
BELIEVES
THE
DENSITY
IS
HIGH
I
E
THE
SAMPLES
X
J
WHEN
THE
MODEL
FITS
THE
DATA
THESE
TWO
FORCES
CANCEL
OUT
AND
THE
PARAMETERS
WILL
STOP
CHANGING
THIS
ALGORITHM
REQUIRES
US
TO
DRAW
SAMPLES
X
FROM
THE
MODEL
AT
EACH
ITERATION
OF
THE
OPTIMIZATION
PROCEDURE
IN
ORDER
TO
COMPUTE
THE
GRADIENT
UNFORTUNATELY
THE
ONLY
WAY
TO
DRAW
SAMPLES
FROM
GENERAL
UNDIRECTED
GRAPHICAL
MODELS
IS
TO
USE
COSTLY
MARKOV
CHAIN
MONTE
CARLO
METHODS
SUCH
AS
GIBBS
SAMPLING
SECTION
AND
THIS
IS
IMPRACTICALLY
TIME
CONSUMING
IN
PRACTICE
IT
HAS
BEEN
FOUND
THAT
EVEN
APPROXIMATE
SAMPLES
WILL
DO
ONE
METHOD
IS
TO
RE
START
J
I
SAMPLES
AT
THE
DATA
POINTS
AT
EACH
ITERATION
AND
DO
JUST
A
FEW
MCMC
STEPS
SURPRISINGLY
THIS
WORKS
WELL
EVEN
WITH
A
SINGLE
STEP
A
SECOND
APPROACH
IS
TO
START
WITH
THE
SAMPLES
FROM
THE
PREVIOUS
ITERATION
AND
PERFORM
A
FEW
MCMC
STEPS
SO
THAT
THE
SAMPLES
ARE
FREE
TO
WANDER
WITHOUT
RESTARTING
THIS
TECHNIQUE
IS
KNOWN
AS
PERSISTENT
CONTRASTIVE
DIVERGENCE
DISCUSSION
IN
THIS
CHAPTER
WE
INTRODUCED
DIRECTED
AND
UNDIRECTED
GRAPHICAL
MODELS
EACH
REPRESENTS
A
DIFFERENT
TYPE
OF
FACTORIZATION
OF
THE
JOINT
DISTRIBUTION
A
GRAPHICAL
MODEL
IMPLIES
A
SET
OF
INDEPENDENCE
AND
CONDITIONAL
INDEPENDENCE
RELATIONS
THERE
ARE
SOME
SETS
THAT
CAN
ONLY
BE
REPRESENTED
BY
DIRECTED
GRAPHICAL
MODELS
OTHERS
THAT
CAN
ONLY
BE
REPRESENTED
BY
UNDIRECTED
GRAPHICAL
MODELS
SOME
THAT
CAN
BE
REPRESENTED
BY
BOTH
AND
SOME
THAT
CANNOT
BE
REPRESENTED
BY
EITHER
WE
PRESENTED
A
NUMBER
OF
COMMON
VISION
MODELS
AND
EXAMINED
THEIR
GRAPHICAL
MODELS
EACH
HAD
SPARSE
CONNECTIONS
AND
HENCE
MANY
CONDITIONAL
INDEPENDENCE
RELATIONS
IN
SUBSEQUENT
CHAPTERS
WE
WILL
EXPLOIT
THESE
REDUNDANCIES
TO
DEVELOP
EFFICIENT
LEARNING
AND
INFERENCE
ALGORITHMS
THE
WORLD
STATE
IS
USUALLY
VERY
HIGH
DIMENSIONAL
IN
THESE
MODELS
AND
SO
WE
DISCUSSED
ALTERNATIVE
FORMS
OF
INFERENCE
INCLUDING
MAXIMUM
MARGINALS
AND
SAMPLING
FINALLY
WE
LOOKED
AT
THE
IMPLICATIONS
OF
CHOOSING
DIRECTED
OR
UNDIRECTED
GRAPH
ICAL
MODELS
FOR
SAMPLING
AND
FOR
LEARNING
WE
CONCLUDED
THAT
IT
IS
GENERALLY
MORE
STRAIGHTFORWARD
TO
DRAW
SAMPLES
FROM
DIRECTED
GRAPHICAL
MODELS
MOREOVER
IT
IS
ALSO
EASIER
TO
LEARN
DIRECTED
GRAPHICAL
MODELS
THE
BEST
KNOWN
LEARNING
ALGORITHM
FOR
GENERAL
UNDIRECTED
GRAPHICAL
MODELS
REQUIRES
US
TO
DRAW
SAMPLES
WHICH
IS
ITSELF
CHALLENGING
NOTES
NOTES
GRAPHICAL
MODELS
FOR
A
READABLE
INTRODUCTION
TO
GRAPHICAL
MODELS
CONSULT
JORDAN
OR
BISHOP
FOR
A
MORE
COMPREHENSIVE
OVERVIEW
I
WOULD
RECOMMEND
BARBER
FOR
AN
EVEN
MORE
ENCYCLOPAEDIC
RESOURCE
CONSULT
KOLLER
FRIEDMAN
CONTRASTIVE
DIVERGENCE
THE
CONTRASTIVE
DIVERGENCE
ALGORITHM
WAS
INTRODUCED
BY
HIN
TON
FURTHER
INFORMATION
ABOUT
THIS
TECHNIQUE
CAN
BE
FOUND
IN
CARREIRA
PERPIN
AN
HINTON
AND
BENGIO
DELALLEAU
PROBLEMS
PROBLEM
THE
JOINT
PROBABILITY
MODEL
BETWEEN
VARIABLES
XN
FACTORIZES
AS
P
R
P
R
P
R
P
R
P
R
P
R
P
R
P
R
DRAW
A
DIRECTED
GRAPHICAL
MODEL
RELATING
THESE
VARIABLES
WHICH
VARIABLES
FORM
THE
MARKOV
BLANKET
OF
VARIABLE
PROBLEM
WRITE
OUT
THE
FACTORIZATION
CORRESPONDING
TO
THE
DIRECTED
GRAPHICAL
MODEL
IN
FIGURE
A
B
FIGURE
A
GRAPHICAL
MODEL
FOR
PROBLEM
B
GRAPHICAL
MODEL
FOR
PROBLEM
PROBLEM
AN
UNDIRECTED
GRAPHICAL
MODEL
HAS
THE
FORM
P
R
X
X
Φ
X
X
X
Φ
X
X
X
Φ
X
X
Φ
X
X
Z
DRAW
THE
UNDIRECTED
GRAPHICAL
MODEL
THAT
CORRESPONDS
TO
THIS
FACTORIZATION
GRAPHICAL
MODELS
FIGURE
FACTOR
GRAPHS
CONTAIN
ONE
NODE
SQUARE
PER
FACTOR
IN
THE
JOINT
PDF
AS
WELL
AS
ONE
NODE
CIRCLE
PER
VARIABLE
EACH
FACTOR
NODE
IS
CONNECTED
TO
ALL
OF
THE
VARIABLES
THAT
BELONG
TO
THAT
FACTOR
THIS
TYPE
OF
GRAPHICAL
MODEL
CAN
DISTINGUISH
BETWEEN
THE
UNDIRECTED
GRAPHICAL
MODELS
A
P
R
AND
B
P
R
Z
Z
PROBLEM
WRITE
OUT
THE
FACTORIZATION
CORRESPONDING
TO
THE
UNDIRECTED
GRAPHICAL
MODEL
IN
FIGURE
PROBLEM
CONSIDER
THE
UNDIRECTED
GRAPHICAL
MODEL
DEFINED
OVER
BINARY
VALUES
XI
DEFINED
BY
P
R
X
X
X
X
Φ
X
X
Φ
X
X
Φ
X
X
Φ
X
X
Z
WHERE
THE
FUNCTION
Φ
IS
DEFINED
BY
Φ
Φ
Φ
Φ
COMPUTE
THE
PROBABILITY
OF
EACH
OF
THE
POSSIBLE
STATES
OF
THIS
SYSTEM
PROBLEM
WHAT
IS
THE
MARKOV
BLANKET
FOR
EACH
OF
THE
VARIABLES
IN
FIGURES
AND
PROBLEM
SHOW
THAT
THE
STATED
PATTERNS
OF
INDEPENDENCE
AND
CONDITIONAL
INDEPEN
DENCE
IN
FIGURE
AND
FIGURE
ARE
TRUE
PROBLEM
A
FACTOR
GRAPH
IS
A
THIRD
TYPE
OF
GRAPHICAL
MODEL
THAT
DEPICTS
THE
FAC
TORIZATION
OF
A
JOINT
PROBABILITY
AS
USUAL
IT
CONTAINS
A
SINGLE
NODE
PER
VARIABLE
BUT
IT
ALSO
CONTAINS
ONE
NODE
PER
FACTOR
USUALLY
INDICATED
BY
A
SOLID
SQUARE
EACH
FACTOR
VARIABLE
IS
CONNECTED
TO
ALL
OF
THE
VARIABLES
THAT
ARE
CONTAINED
IN
THE
ASSOCIATED
TERM
IN
THE
FACTORIZATION
BY
UNDIRECTED
LINKS
FOR
EXAMPLE
THE
FACTOR
NODE
CORRESPONDING
TO
THE
TERM
P
R
IN
A
DIRECTED
MODEL
WOULD
CONNECT
TO
ALL
THREE
VARIABLES
AND
SIMILARLY
THE
FACTOR
NODE
CORRESPONDING
TO
THE
TERM
IN
AN
UNDIRECTED
MODEL
WOULD
CONNECT
VARIABLES
AND
FIGURE
SHOWS
TWO
EXAMPLES
OF
FACTOR
GRAPHS
DRAW
THE
FACTOR
GRAPHS
CORRESPONDING
TO
THE
GRAPHICAL
MODELS
IN
FIGURES
AND
YOU
MUST
FIRST
ESTABLISH
THE
FACTORIZED
JOINT
DISTRIBUTION
ASSOCIATED
WITH
EACH
GRAPH
PROBLEM
WHAT
IS
THE
MARKOV
BLANKET
OF
VARIABLE
IN
FIGURE
PROBLEM
WHAT
IS
THE
MARKOV
BLANKET
OF
VARIABLE
IN
FIGURE
CHAPTER
MODELS
FOR
CHAINS
AND
TREES
IN
THIS
CHAPTER
WE
MODEL
THE
RELATIONSHIP
BETWEEN
A
MULTIDIMENSIONAL
SET
OF
MEASUREMENTS
XN
N
AND
AN
ASSOCIATED
MULTIDIMENSIONAL
WORLD
STATE
WN
N
WHEN
N
IS
LARGE
IT
IS
NOT
PRACTICAL
TO
DESCRIBE
THE
FULL
SET
OF
DEPENDENCIES
BETWEEN
ALL
OF
THESE
VARIABLES
AS
THE
NUMBER
OF
MODEL
PARAMETERS
WILL
BE
TOO
GREAT
INSTEAD
WE
CONSTRUCT
MODELS
WHERE
WE
ONLY
DIRECTLY
DESCRIBE
THE
PROBABILISTIC
DEPENDENCE
BETWEEN
VARIABLES
IN
SMALL
NEIGHBORHOODS
IN
PARTICULAR
WE
WILL
CONSIDER
MODELS
IN
WHICH
THE
WORLD
VARIABLES
WN
N
ARE
STRUCTURED
AS
CHAINS
OR
TREES
WE
DEFINE
A
CHAIN
MODEL
TO
BE
ONE
IN
WHICH
THE
WORLD
STATE
VARIABLES
WN
N
ARE
CONNECTED
TO
ONLY
THE
PREVIOUS
VARIABLE
AND
THE
SUBSEQUENT
VARIABLE
IN
THE
ASSOCIATED
GRAPHICAL
MODEL
AS
IN
FIGURE
WE
DEFINE
A
TREE
MODEL
TO
BE
ONE
IN
WHICH
THE
WORLD
VARIABLES
HAVE
MORE
COMPLEX
CONNECTIONS
BUT
SO
THAT
THERE
ARE
NO
LOOPS
IN
THE
RESULTING
GRAPHICAL
MODEL
IMPORTANTLY
WE
DISREGARD
THE
DIRECTIONALITY
OF
THE
CONNECTIONS
WHEN
WE
ASSESS
WHETHER
A
DIRECTED
MODEL
IS
A
TREE
HENCE
OUR
DEFINITION
OF
A
TREE
DIFFERS
FROM
THE
STANDARD
COMPUTER
SCIENCE
USAGE
WE
WILL
ALSO
MAKE
THE
FOLLOWING
ASSUMPTIONS
THE
WORLD
STATES
WN
ARE
DISCRETE
THERE
IS
AN
OBSERVED
DATA
VARIABLE
XN
ASSOCIATED
WITH
EACH
WORLD
STATE
WN
THE
NTH
DATA
VARIABLE
XN
IS
CONDITIONALLY
INDEPENDENT
OF
ALL
OTHER
DATA
VARI
ABLES
AND
WORLD
STATES
GIVEN
THE
ASSOCIATED
WORLD
STATE
WN
THESE
ASSUMPTIONS
ARE
NOT
CRITICAL
FOR
THE
DEVELOPMENT
OF
THE
IDEAS
IN
THIS
CHAPTER
BUT
ARE
TYPICAL
FOR
THE
TYPE
OF
COMPUTER
VISION
APPLICATIONS
THAT
WE
CONSIDER
WE
WILL
SHOW
THAT
BOTH
MAXIMUM
A
POSTERIORI
AND
MAXIMUM
MARGINALS
INFERENCE
ARE
TRACTABLE
FOR
THIS
SUB
CLASS
OF
MODELS
AND
WE
WILL
DISCUSS
WHY
THIS
IS
NOT
THE
CASE
WHEN
THE
STATES
ARE
NOT
ORGANIZED
AS
A
CHAIN
OR
A
TREE
TO
MOTIVATE
THESE
MODELS
CONSIDER
THE
PROBLEM
OF
GESTURE
TRACKING
HERE
THE
GOAL
IS
TO
AUTOMATICALLY
INTERPRET
SIGN
LANGUAGE
FROM
A
VIDEO
SEQUENCE
FIGURE
WE
OBSERVE
N
FRAMES
XN
N
OF
A
VIDEO
SEQUENCE
AND
WISH
TO
INFER
THE
N
DISCRETE
VARIABLES
WN
N
THAT
ENCODE
WHICH
SIGN
IS
PRESENT
IN
EACH
OF
THE
N
FRAMES
THE
DATA
AT
TIME
N
TELLS
US
SOMETHING
ABOUT
THE
SIGN
AT
TIME
N
BUT
MAY
BE
INSUFFICIENT
TO
SPECIFY
IT
ACCURATELY
CONSEQUENTLY
WE
ALSO
MODEL
DEPENDENCIES
BETWEEN
ADJACENT
FIGURE
INTERPRETING
SIGN
LANGUAGE
WE
OBSERVE
A
SEQUENCE
OF
IMAGES
OF
A
PERSON
USING
SIGN
LANGUAGE
IN
EACH
FRAME
WE
EXTRACT
A
VECTOR
XN
DESCRIBING
THE
SHAPE
AND
POSITION
OF
THE
HANDS
THE
GOAL
IS
TO
INFER
THE
SIGN
WN
THAT
IS
PRESENT
UNFORTUNATELY
THE
VISUAL
DATA
IN
A
SINGLE
FRAME
MAY
BE
AMBIGUOUS
WE
IMPROVE
MATTERS
BY
DESCRIBING
PROBABILISTIC
CONNECTIONS
BETWEEN
ADJACENT
STATES
WN
AND
WN
WE
IMPOSE
KNOWLEDGE
ABOUT
THE
LIKELY
SEQUENCE
OF
SIGNS
AND
THIS
HELPS
DISAMBIGUATE
ANY
INDIVIDUAL
FRAME
IMAGES
FROM
PURDUE
RVL
SLLL
ASL
DATABASE
WILBUR
KAK
WORLD
STATES
WE
KNOW
THAT
THE
SIGNS
ARE
MORE
LIKELY
TO
APPEAR
IN
SOME
ORDERS
THAN
OTHERS
AND
WE
EXPLOIT
THIS
KNOWLEDGE
TO
HELP
DISAMBIGUATE
THE
SEQUENCE
SINCE
WE
MODEL
PROBABILISTIC
CONNECTIONS
ONLY
BETWEEN
ADJACENT
STATES
IN
THE
TIME
SERIES
THIS
HAS
THE
FORM
OF
A
CHAIN
MODEL
MODELS
FOR
CHAINS
IN
THIS
SECTION
WE
WILL
DESCRIBE
BOTH
A
DIRECTED
AND
AN
UNDIRECTED
MODEL
FOR
DE
SCRIBING
CHAIN
STRUCTURE
AND
SHOW
THAT
THESE
TWO
MODELS
ARE
EQUIVALENT
DIRECTED
MODEL
FOR
CHAINS
THE
DIRECTED
MODEL
DESCRIBES
THE
JOINT
PROBABILITY
OF
A
SET
OF
CONTINUOUS
MEASURE
MENTS
XN
N
AND
A
SET
OF
DISCRETE
WORLD
STATES
WN
N
WITH
THE
GRAPHICAL
MODEL
SHOWN
IN
FIGURE
THE
TENDENCY
TO
OBSERVE
THE
MEASUREMENTS
XN
GIVEN
THAT
STATE
WN
TAKES
VALUE
K
IS
ENCODED
IN
THE
LIKELIHOOD
P
R
XN
WN
K
THE
PRIOR
PROBABILITY
OF
THE
FIRST
STATE
IS
EXPLICITLY
ENCODED
IN
THE
DISCRETE
DISTRIBUTION
P
R
BUT
FOR
SIMPLICITY
WE
ASSUME
THAT
THIS
IS
UNIFORM
AND
OMIT
IT
FROM
MOST
OF
THE
ENSUING
DISCUSSION
THE
REMAINING
STATES
ARE
EACH
DEPENDENT
ON
THE
PREVIOUS
ONE
AND
THIS
INFORMATION
IS
CAPTURED
IN
THE
DISTRIBUTION
P
R
WN
WN
THIS
IS
SOMETIMES
TERMED
THE
MARKOV
ASSUMPTION
HENCE
THE
OVERALL
JOINT
PROBABILITY
IS
P
R
N
N
N
N
P
R
XN
WN
TTN
P
R
WN
WN
THIS
IS
KNOWN
AS
A
HIDDEN
MARKOV
MODEL
HMM
THE
WORLD
STATES
WN
N
IN
THE
DIRECTED
MODEL
HAVE
THE
FORM
OF
A
CHAIN
AND
THE
OVERALL
MODEL
HAS
THE
FORM
OF
A
TREE
AS
WE
SHALL
SEE
THESE
PROPERTIES
ARE
CRITICAL
TO
OUR
ABILITY
TO
PERFORM
INFERENCE
UNDIRECTED
MODEL
FOR
CHAINS
THE
UNDIRECTED
MODEL
SEE
SECTION
DESCRIBES
THE
JOINT
PROBABILITY
OF
THE
MEA
SUREMENTS
XN
N
AND
THE
WORLD
STATES
WN
N
WITH
THE
GRAPHICAL
MODEL
SHOWN
IN
FIGURE
THE
TENDENCY
FOR
THE
MEASUREMENTS
AND
THE
DATA
TO
TAKE
CERTAIN
VALUES
IS
ENCODED
IN
THE
POTENTIAL
FUNCTION
Φ
XN
WN
THIS
FUNCTION
ALWAYS
RETURNS
POSITIVE
VALUES
AND
RETURNS
LARGER
VALUES
WHEN
THE
MEASUREMENTS
AND
THE
WORLD
STATE
ARE
MORE
COMPATIBLE
THE
TENDENCY
FOR
ADJACENT
STATES
TO
TAKE
CERTAIN
VALUES
IS
ENCODED
IN
A
SECOND
POTENTIAL
FUNCTION
Ζ
WN
WN
WHICH
RETURNS
LARGER
VALUES
WHEN
THE
ADJACENT
STATES
ARE
MORE
COMPATIBLE
HENCE
THE
OVERALL
PROBABILITY
IS
N
P
R
N
N
Z
N
Φ
XN
WN
TTN
Ζ
WN
WN
ONCE
MORE
THE
STATES
FORM
A
CHAIN
AND
THE
OVERALL
MODEL
HAS
THE
FORM
OF
A
TREE
THERE
ARE
NO
LOOPS
EQUIVALENCE
OF
MODELS
WHEN
WE
TAKE
A
DIRECTED
MODEL
AND
MAKE
THE
EDGES
UNDIRECTED
WE
USUALLY
CREATE
A
DIFFERENT
MODEL
HOWEVER
COMPARING
EQUATIONS
AND
REVEALS
THAT
THESE
TWO
MODELS
REPRESENT
THE
SAME
FACTORIZATION
OF
THE
JOINT
PROBABILITY
DENSITY
IN
THIS
SPECIAL
CASE
THE
TWO
MODELS
ARE
EQUIVALENT
THIS
EQUIVALENCE
BECOMES
EVEN
MORE
APPARENT
IF
WE
MAKE
THE
SUBSTITUTIONS
P
R
XN
WN
Φ
XN
WN
N
P
R
WN
WN
ZI
Ζ
WN
WN
WHERE
ZN
AND
ZNI
ARE
NORMALIZING
FACTORS
WHICH
FORM
THE
PARTITION
FUNCTION
N
Z
N
TTN
ZNI
SINCE
THE
DIRECTED
AND
UNDIRECTED
VERSIONS
OF
THE
CHAIN
MODEL
ARE
EQUIVALENT
WE
WILL
CONTINUE
OUR
DISCUSSION
IN
TERMS
OF
THE
DIRECTED
MODEL
ALONE
MODELS
FOR
CHAINS
AND
TREES
A
B
FIGURE
MODELS
FOR
CHAINS
A
DIRECTED
MODEL
THERE
IS
ONE
OBSERVATION
VARIABLE
XN
FOR
EACH
STATE
VARIABLE
WN
AND
THESE
ARE
RELATED
BY
THE
CONDI
TIONAL
PROBABILITY
P
R
XN
WN
DOWNWARD
ARROWS
EACH
STATE
WN
IS
RELATED
TO
THE
PREVIOUS
ONE
BY
THE
CONDITIONAL
PROBABILITY
P
R
WN
WN
HORIZONTAL
ARROWS
B
UNDIRECTED
MODEL
HERE
EACH
OBSERVED
VARIABLE
XN
IS
RELATED
TO
ITS
ASSOCIATED
STATE
VARIABLE
WN
VIA
THE
POTENTIAL
FUNCTION
Φ
XN
WN
AND
THE
NEIGHBORING
STATES
ARE
CONNECTED
VIA
THE
POTENTIAL
FUNCTION
Ζ
WN
WN
HIDDEN
MARKOV
MODEL
FOR
SIGN
LANGUAGE
APPLICATION
WE
WILL
NOW
BRIEFLY
DESCRIBE
HOW
THIS
DIRECTED
MODEL
RELATES
TO
THE
SIGN
LANGUAGE
APPLICATION
WE
PREPROCESS
THE
VIDEO
FRAME
TO
CREATE
A
VECTOR
XN
THAT
REPRESENTS
THE
SHAPE
OF
THE
HANDS
FOR
EXAMPLE
WE
MIGHT
JUST
EXTRACT
A
WINDOW
OF
PIXELS
AROUND
EACH
HAND
AND
CONCATENATE
THEIR
RGB
PIXEL
VALUES
WE
NOW
MODEL
THE
LIKELIHOOD
P
R
XN
WN
K
OF
OBSERVING
THIS
MEASUREMENT
VECTOR
GIVEN
THAT
THE
SIGN
WN
IN
THIS
IMAGE
TAKES
VALUE
K
A
VERY
SIMPLE
MODEL
MIGHT
ASSUME
THAT
THE
MEASUREMENTS
HAVE
A
NORMAL
DISTRIBUTION
WITH
PARAMETERS
THAT
ARE
CONTINGENT
ON
WHICH
SIGN
IS
PRESENT
SO
THAT
P
R
XN
WN
K
NORMXN
ΜK
ΣK
WE
MODEL
THE
SIGN
WN
AS
A
BEING
CATEGORICALLY
DISTRIBUTED
WHERE
THE
PARAMETERS
DEPEND
ON
THE
PREVIOUS
SIGN
WN
SO
THAT
P
R
WN
WN
K
CATWN
ΛK
THIS
HIDDEN
MARKOV
MODEL
HAS
PARAMETERS
ΜK
ΣK
ΛK
K
FOR
MOST
OF
THIS
CHAPTER
WE
WILL
ASSUME
THAT
THESE
PARAMETERS
ARE
KNOWN
BUT
WE
RETURN
BRIEFLY
TO
THE
ISSUE
OF
LEARNING
IN
SECTION
WE
NOW
TURN
OUR
FOCUS
TO
INFERENCE
IN
THIS
TYPE
OF
MODEL
MAP
INFERENCE
FOR
CHAINS
CONSIDER
A
CHAIN
WITH
N
UNKNOWN
VARIABLES
WN
N
EACH
OF
WHICH
CAN
TAKE
K
POSSIBLE
VALUES
HERE
THERE
ARE
KN
POSSIBLE
STATES
OF
THE
WORLD
FOR
REAL
WORLD
PROBLEMS
THIS
MEANS
THAT
THERE
ARE
FAR
TOO
MANY
STATES
TO
EVALUATE
EXHAUSTIVELY
FIGURE
DYNAMIC
PROGRAMMING
FORMULATION
EACH
SOLUTION
IS
EQUATED
WITH
A
PARTICULAR
PATH
FROM
LEFT
TO
RIGHT
THROUGH
AN
ACYCLIC
DIRECTED
GRAPH
THE
N
COLUMNS
OF
THE
GRAPH
REPRESENT
VARIABLES
N
AND
THE
K
ROWS
REPRESENT
POSSIBLE
STATES
K
THE
NODES
AND
EDGES
OF
THE
GRAPH
HAVE
COSTS
ASSOCIATED
WITH
THE
UNARY
AND
PAIRWISE
TERMS
RESPECTIVELY
ANY
PATH
FROM
LEFT
TO
RIGHT
THROUGH
THE
GRAPH
HAS
A
COST
THAT
IS
THE
SUM
OF
THE
COSTS
AT
ALL
OF
THE
NODES
AND
EDGES
THAT
IT
PASSES
THROUGH
OPTIMIZING
THE
FUNCTION
IS
NOW
EQUIVALENT
TO
FINDING
THE
PATH
WITH
THE
LEAST
COST
WE
CAN
NEITHER
COMPUTE
THE
FULL
POSTERIOR
DISTRIBUTION
NOR
SEARCH
DIRECTLY
THROUGH
ALL
OF
THE
STATES
TO
FIND
THE
MAXIMUM
A
POSTERIORI
MAP
ESTIMATE
FORTUNATELY
THE
FACTORIZATION
OF
THE
JOINT
PROBABILITY
DISTRIBUTION
THE
CONDI
TIONAL
INDEPENDENCE
STRUCTURE
CAN
BE
EXPLOITED
TO
FIND
MORE
EFFICIENT
ALGORITHMS
FOR
MAP
INFERENCE
THAN
BRUTE
FORCE
SEARCH
THE
MAP
SOLUTION
IS
GIVEN
BY
N
ARGMAX
P
R
N
N
N
ARGMAX
P
R
N
N
N
ARGMIN
LOG
P
R
N
N
N
WHERE
LINE
FOLLOWS
FROM
BAYES
RULE
WE
HAVE
REFORMULATED
THIS
AS
A
MINIMIZATION
PROBLEM
IN
LINE
SUBSTITUTING
IN
THE
EXPRESSION
FOR
THE
LOG
PROBABILITY
EQUATION
WE
GET
N
ARGMIN
N
R
N
LOG
P
R
XN
WN
N
LOG
P
R
WN
WN
WHICH
HAS
THE
GENERAL
FORM
N
ARGMIN
N
N
N
UN
WN
N
PN
WN
WN
WHERE
UN
IS
A
UNARY
TERM
AND
DEPENDS
ONLY
ON
A
SINGLE
VARIABLE
WN
AND
PN
IS
A
PAIRWISE
TERM
DEPENDING
ON
TWO
VARIABLES
WN
AND
WN
IN
THIS
INSTANCE
THE
UNARY
AND
PAIRWISE
TERMS
CAN
BE
DEFINED
AS
UN
WN
LOG
P
R
XN
WN
PN
WN
WN
LOG
P
R
WN
WN
ANY
PROBLEM
THAT
HAS
THE
FORM
OF
EQUATION
CAN
BE
SOLVED
IN
POLYNOMIAL
TIME
USING
THE
VITERBI
ALGORITHM
WHICH
IS
AN
EXAMPLE
OF
DYNAMIC
PROGRAMMING
DYNAMIC
PROGRAMMING
VITERBI
ALGORITHM
TO
OPTIMIZE
THE
COST
FUNCTION
IN
EQUATION
WE
FIRST
VISUALIZE
THE
PROBLEM
WITH
ALGORITHM
A
GRAPH
WITH
VERTICES
V
N
K
N
K
N
K
THE
VERTEX
V
N
K
REPRESENTS
CHOOSING
THE
KTH
WORLD
STATE
AT
THE
NTH
VARIABLE
FIGURE
VERTEX
VN
K
IS
CONNECTED
BY
A
DIRECTED
EDGE
TO
EACH
OF
THE
VERTICES
VN
K
K
AT
THE
NEXT
PIXEL
POSITION
HENCE
THE
ORGANIZATION
OF
THE
GRAPH
IS
SUCH
THAT
EACH
VALID
HORIZONTAL
PATH
FROM
LEFT
TO
RIGHT
REPRESENTS
A
POSSIBLE
SOLUTION
TO
THE
PROBLEM
IT
CORRESPONDS
TO
ASSIGNING
ONE
VALUE
K
K
TO
EACH
VARIABLE
WN
WE
NOW
ATTACH
THE
COSTS
UN
WN
K
TO
THE
VERTICES
VN
K
WE
ALSO
ATTACH
THE
COSTS
PN
WN
K
WN
L
TO
THE
EDGES
JOINING
VERTICES
VN
L
TO
VN
K
WE
DEFINE
THE
TOTAL
COST
OF
A
PATH
FROM
LEFT
TO
RIGHT
AS
THE
SUM
OF
THE
COSTS
OF
THE
EDGES
AND
VERTICES
THAT
MAKE
UP
THE
PATH
NOW
EVERY
HORIZONTAL
PATH
REPRESENTS
A
SOLUTION
AND
THE
COST
OF
THAT
PATH
IS
THE
COST
FOR
THAT
SOLUTION
WE
HAVE
REFORMULATED
THE
PROBLEM
AS
FINDING
THE
MINIMUM
COST
PATH
FROM
LEFT
TO
RIGHT
ACROSS
THE
GRAPH
FINDING
THE
MINIMUM
COST
THE
APPROACH
TO
FINDING
THE
MINIMUM
COST
PATH
IS
SIMPLE
WE
WORK
THROUGH
THE
GRAPH
FROM
LEFT
TO
RIGHT
COMPUTING
AT
EACH
VERTEX
THE
MINIMUM
POSSIBLE
CUMULATIVE
COST
SN
K
TO
ARRIVE
AT
THIS
POINT
BY
ANY
ROUTE
WHEN
WE
REACH
THE
RIGHT
HAND
SIDE
WE
COMPARE
THE
K
VALUES
SN
AND
CHOOSE
THE
MINIMUM
THIS
IS
THE
LOWEST
POSSIBLE
COST
FOR
TRAVERSING
THE
GRAPH
WE
NOW
RETRACE
THE
ROUTE
WE
TOOK
TO
REACH
THIS
POINT
USING
INFORMATION
THAT
WAS
CACHED
DURING
THE
FORWARD
PASS
THE
EASIEST
WAY
TO
UNDERSTAND
THIS
METHOD
IS
WITH
A
CONCRETE
EXAMPLE
FIGURES
AND
THE
READER
IS
ENCOURAGED
TO
SCRUTINIZE
THESE
FIGURES
BEFORE
CONTINUING
A
MORE
FORMAL
DESCRIPTION
IS
AS
FOLLOWS
OUR
GOAL
IS
TO
ASSIGN
THE
MINIMUM
POSSIBLE
CUMULATIVE
COST
SN
K
FOR
REACHING
VERTEX
VN
K
STARTING
AT
THE
LEFT
HAND
SIDE
WE
SET
THE
FIRST
COLUMN
OF
VERTICES
TO
THE
UNARY
COSTS
FOR
THE
FIRST
VARIABLE
K
K
THE
CUMULATIVE
TOTAL
K
FOR
THE
KTH
VERTEX
IN
THE
SECOND
COLUMN
SHOULD
REPRESENT
THE
MINIMUM
POSSIBLE
CUMULATIVE
COST
TO
REACH
THIS
POINT
TO
CALCULATE
THIS
WE
CONSIDER
THE
K
POSSIBLE
PREDECESSORS
AND
COMPUTE
THE
COST
FOR
REACHING
THIS
VERTEX
BY
EACH
POSSIBLE
ROUTE
WE
SET
K
TO
THE
MINIMUM
OF
THESE
VALUES
AND
STORE
THE
ROUTE
BY
WHICH
WE
REACHED
THIS
VERTEX
SO
THAT
MAP
INFERENCE
FOR
CHAINS
A
B
C
FIGURE
DYNAMIC
PROGRAMMING
A
THE
UNARY
COST
UN
WN
K
IS
GIVEN
BY
THE
NUMBER
ABOVE
AND
TO
THE
RIGHT
OF
EACH
NODE
THE
PAIRWISE
COSTS
PN
WN
WN
ARE
ZERO
IF
WN
WN
HORIZONTAL
TWO
IF
WN
WN
AND
OTHERWISE
THIS
FAVORS
A
SOLUTION
THAT
IS
MOSTLY
CONSTANT
BUT
CAN
ALSO
VARY
SMOOTHLY
FOR
CLARITY
WE
HAVE
REMOVED
THE
EDGES
WITH
INFINITE
COST
AS
THEY
CANNOT
BECOME
PART
OF
THE
SOLUTION
WE
NOW
WORK
FROM
LEFT
TO
RIGHT
COMPUTING
THE
MINIMUM
COST
SN
K
FOR
ARRIVING
AT
VERTEX
VN
K
BY
ANY
ROUTE
B
FOR
VERTICES
K
K
THE
MINIMUM
COST
IS
JUST
THE
UNARY
COST
ASSOCIATED
WITH
THAT
VERTEX
WE
HAVE
STORED
THE
VALUES
INSIDE
THE
CIRCLE
REPRESENTING
THE
RESPECTIVE
VERTEX
C
TO
COMPUTE
THE
MINIMUM
COST
AT
VERTEX
N
K
WE
MUST
CONSIDER
TWO
POSSIBLE
ROUTES
THE
PATH
COULD
HAVE
TRAVELED
HORIZONTALLY
FROM
VERTEX
GIVING
A
TOTAL
COST
OF
OR
IT
MAY
HAVE
COME
DIAGONALLY
UPWARD
FROM
VERTEX
WITH
COST
SINCE
THE
FORMER
ROUTE
IS
CHEAPER
WE
USE
THIS
COST
STORING
AT
THE
VERTEX
AND
ALSO
REMEMBERING
THE
PATH
USED
TO
GET
HERE
NOW
WE
REPEAT
THIS
PROCEDURE
AT
VERTEX
WHERE
THERE
ARE
THREE
POSSIBLE
ROUTES
FROM
VERTICES
AND
HERE
IT
TURNS
OUT
THAT
THE
BEST
ROUTE
IS
FROM
AND
HAS
TOTAL
CUMULATIVE
COST
OF
EXAMPLE
CONTINUED
IN
FIGURE
A
B
C
FIGURE
DYNAMIC
PROGRAMMING
WORKED
EXAMPLE
CONTINUED
FROM
FIG
URE
A
HAVING
UPDATED
THE
VERTICES
AT
PIXEL
N
WE
CARRY
OUT
THE
SAME
PROCEDURE
AT
PIXEL
N
ACCUMULATING
AT
EACH
VERTEX
THE
MINIMUM
TOTAL
COST
TO
REACH
THIS
POINT
B
WE
CONTINUE
UPDATING
THE
MINIMUM
CU
MULATIVE
COSTS
SN
K
TO
ARRIVE
AT
PIXEL
N
IN
STATE
K
UNTIL
WE
REACH
THE
RIGHT
HAND
SIDE
C
WE
IDENTIFY
THE
MINIMUM
COST
FROM
AMONG
THE
RIGHT
MOST
VERTICES
IN
THIS
CASE
IT
IS
VERTEX
WHICH
HAS
COST
THIS
IS
THE
MINIMUM
POSSIBLE
COST
FOR
TRAVERSING
THE
GRAPH
BY
TRACING
BACK
THE
ROUTE
THAT
WE
USED
TO
ARRIVE
HERE
RED
ARROWS
WE
FIND
THE
WORLD
STATE
AT
EACH
PIXEL
THAT
WAS
RESPONSIBLE
FOR
THIS
COST
FIGURE
TREE
BASED
MODELS
AS
BEFORE
THERE
IS
ONE
OBSERVATION
XN
FOR
EACH
WORLD
STATE
WN
AND
THESE
ARE
RELATED
BY
THE
CONDITIONAL
PROBABIL
ITY
P
R
XN
WN
HOWEVER
DISREGARD
ING
THE
DIRECTIONALITY
OF
THE
EDGES
THE
WORLD
STATES
ARE
NOW
CONNECTED
AS
A
TREE
VERTEX
HAS
TWO
INCOMING
CONNECTIONS
WHICH
MEANS
THAT
THERE
IS
A
THREE
WISE
TERM
P
R
IN
THE
FACTORIZATION
THE
TREE
STRUC
TURE
MEANS
IT
IS
POSSIBLE
TO
PERFORM
MAP
AND
MAX
MARGINALS
INFERENCE
EFFICIENTLY
K
K
MIN
L
K
L
L
MORE
GENERALLY
TO
CALCULATE
THE
CUMULATIVE
TOTALS
SN
K
WE
USE
THE
RECURSION
SN
K
UN
WN
K
MIN
SN
L
PN
WN
K
WN
L
AND
WE
ALSO
CACHE
THE
ROUTE
BY
WHICH
THIS
MINIMUM
WAS
ACHIEVED
AT
EACH
STAGE
WHEN
WE
REACH
THE
RIGHT
HAND
SIDE
WE
FIND
THE
VALUE
OF
THE
FINAL
VARIABLE
WN
THAT
MINIMIZES
THE
TOTAL
COST
WˆN
ARGMIN
SN
K
K
AND
SET
THE
REMAINING
LABELS
N
ACCORDING
TO
THE
ROUTE
THAT
WE
FOLLOWED
TO
GET
TO
THIS
VALUE
THIS
METHOD
EXPLOITS
THE
FACTORIZATION
STRUCTURE
OF
THE
JOINT
PROBABILITY
BETWEEN
THE
OBSERVATIONS
AND
THE
STATES
TO
MAKE
VAST
COMPUTATIONAL
SAVINGS
THE
COST
OF
THIS
PROCEDURE
IS
N
AS
OPPOSED
TO
KN
FOR
A
BRUTE
FORCE
SEARCH
THROUGH
EVERY
POSSIBLE
SOLUTION
MAP
INFERENCE
FOR
TREES
TO
SHOW
HOW
MAP
INFERENCE
WORKS
IN
TREE
STRUCTURED
MODELS
CONSIDER
THE
MODEL
IN
FIGURE
FOR
THIS
GRAPH
THE
PRIOR
PROBABILITY
OVER
THE
STATES
FACTORIZES
AS
P
R
P
R
P
R
P
R
P
R
P
R
P
R
AND
THE
WORLD
STATES
HAVE
THE
STRUCTURE
OF
A
TREE
DISREGARDING
THE
DIRECTIONALITY
OF
THE
EDGES
ONCE
MORE
WE
CAN
EXPLOIT
THIS
FACTORIZATION
TO
COMPUTE
THE
MAP
SOLUTION
EFFI
CIENTLY
OUR
GOAL
IS
TO
FIND
PROBLEM
PROBLEM
ALGORITHM
PROBLEM
PROBLEM
MODELS
FOR
CHAINS
AND
TREES
A
B
D
C
E
FIGURE
DYNAMIC
PROGRAMMING
EXAMPLE
FOR
TREE
MODEL
IN
FIGURE
A
TABLE
OF
THREE
WISE
COSTS
AT
VERTEX
THIS
IS
A
K
K
K
TABLE
CONSISTING
OF
THE
COSTS
ASSOCIATED
WITH
P
R
PAIRWISE
COSTS
ARE
AS
FOR
THE
EXAMPLE
IN
FIGURE
B
TREE
STRUCTURED
MODEL
WITH
UNARY
AND
PAIRWISE
COSTS
ATTACHED
C
WE
WORK
FROM
THE
LEAVES
FINDING
THE
MINIMAL
POSSIBLE
COST
SN
K
TO
REACH
VERTEX
N
IN
STATE
K
AS
IN
THE
ORIGINAL
DYNAMIC
PROGRAMMING
FORMULATION
D
WHEN
WE
REACH
THE
VERTEX
ABOVE
A
BRANCH
HERE
VERTEX
WE
FIND
THE
MINIMAL
POSSIBLE
COST
CONSIDERING
EVERY
COMBINATION
OF
THE
INCOMING
STATES
E
WE
CONTINUE
UNTIL
WE
REACH
THE
ROOT
THERE
WE
FIND
THE
MINIMUM
OVERALL
COST
AND
TRACE
BACK
MAKING
SURE
TO
SPLIT
AT
THE
JUNCTION
ACCORDING
TO
WHICH
PAIR
OF
STATES
WAS
CHOSEN
ARGMAX
N
LOG
P
R
XN
WN
LOG
P
R
BY
A
SIMILAR
PROCESS
TO
THAT
IN
SECTION
WE
CAN
REWRITE
THIS
AS
A
MINIMIZATION
PROBLEM
WITH
THE
FOLLOWING
COST
FUNCTION
ARGMIN
N
UN
WN
AS
BEFORE
WE
REFORMULATE
THIS
COST
FUNCTION
IN
TERMS
OF
FINDING
A
ROUTE
THROUGH
A
GRAPH
SEE
FIGURE
THE
UNARY
COSTS
UN
ARE
ASSOCIATED
WITH
EACH
VERTEX
THE
PAIRWISE
COSTS
PM
ARE
ASSOCIATED
WITH
EDGES
BETWEEN
PAIRS
OF
ADJACENT
VERTICES
THE
THREE
WISE
COST
IS
ASSOCIATED
WITH
THE
COMBINATION
OF
STATES
AT
THE
POINT
WHERE
THE
TREE
BRANCHES
OUR
GOAL
NOW
IS
TO
FIND
THE
LEAST
COST
PATH
FROM
ALL
OF
THE
LEAVES
SIMULTANEOUSLY
TO
THE
ROOT
WE
WORK
FROM
THE
LEAVES
TO
THE
ROOT
OF
THE
TREE
AT
EACH
STAGE
COMPUTING
SN
K
THE
CUMULATIVE
COST
FOR
ARRIVING
AT
THIS
VERTEX
SEE
WORKED
EXAMPLE
IN
FIGURE
FOR
THE
FIRST
FOUR
VERTICES
WE
PROCEED
AS
IN
STANDARD
DYNAMIC
PROGRAMMING
K
K
K
K
MIN
L
K
L
L
K
K
K
K
MIN
K
L
L
WHEN
WE
COME
TO
THE
BRANCH
IN
THE
TREE
WE
TRY
TO
FIND
THE
BEST
COMBINATION
OF
ROUTES
TO
REACH
THE
NODES
FOR
VARIABLE
WE
MUST
NOW
MINIMIZE
OVER
BOTH
VARIABLES
TO
COMPUTE
THE
NEXT
TERM
IN
OTHER
WORDS
K
K
MIN
L
M
K
L
M
L
M
FINALLY
WE
COMPUTE
THE
LAST
TERMS
AS
NORMAL
SO
THAT
K
K
MIN
L
K
L
L
NOW
WE
FIND
THE
WORLD
STATE
ASSOCIATED
WITH
THE
MINIMUM
OF
THIS
FINAL
SUM
AND
TRACE
BACK
THE
ROUTE
THAT
WE
CAME
BY
AS
BEFORE
SPLITTING
THE
ROUTE
APPROPRIATELY
AT
JUNCTIONS
IN
THE
TREE
DYNAMIC
PROGRAMMING
IN
THIS
TREE
HAS
A
GREATER
COMPUTATIONAL
COMPLEXITY
THAN
DYNAMIC
PROGRAMMING
IN
A
CHAIN
WITH
THE
SAME
NUMBER
OF
VARIABLES
AS
WE
MUST
MINIMIZE
OVER
TWO
VARIABLES
AT
THE
JUNCTION
IN
THE
TREE
THE
OVERALL
COMPLEXITY
IS
PROPORTIONAL
TO
KW
WHERE
W
IS
THE
MAXIMUM
NUMBER
OF
VARIABLES
OVER
WHICH
WE
MUST
MINIMIZE
FOR
DIRECTED
MODELS
W
IS
EQUAL
TO
THE
LARGEST
NUMBER
OF
INCOMING
CONNECTIONS
AT
ANY
VERTEX
FOR
UNDIRECTED
MODELS
W
WILL
BE
THE
SIZE
OF
THE
LARGEST
CLIQUE
IT
SHOULD
BE
NOTED
THAT
FOR
UNDIRECTED
MODELS
THE
CRITICAL
PROPERTY
THAT
ALLOWS
DYNAMIC
PROGRAMMING
SOLUTIONS
IS
THAT
THE
CLIQUES
THEMSELVES
FORM
A
TREE
SEE
FIGURE
MARGINAL
POSTERIOR
INFERENCE
FOR
CHAINS
IN
SECTION
WE
DEMONSTRATED
THAT
IT
IS
POSSIBLE
TO
PERFORM
MAP
INFERENCE
IN
CHAIN
MODELS
EFFICIENTLY
USING
DYNAMIC
PROGRAMMING
IN
THIS
SECTION
WE
WILL
CON
SIDER
A
DIFFERENT
FORM
OF
INFERENCE
WE
WILL
AIM
TO
CALCULATE
THE
MARGINAL
DISTRIBUTION
P
R
WN
N
OVER
EACH
STATE
VARIABLE
WN
SEPARATELY
CONSIDER
COMPUTING
THE
MARGINAL
DISTRIBUTION
OVER
THE
VARIABLE
WN
BY
BAYES
RULE
WE
HAVE
P
R
WN
N
P
R
WN
N
PR
W
P
R
N
N
N
THE
RIGHT
HAND
SIDE
OF
THIS
EQUATION
IS
COMPUTED
BY
MARGINALIZING
OVER
ALL
OF
THE
OTHER
STATE
VARIABLES
EXCEPT
WN
SO
WE
HAVE
P
R
WN
N
P
R
N
N
TTN
P
R
XN
WN
P
R
TTN
P
R
WN
WN
WN
N
N
UNFORTUNATELY
IN
ITS
MOST
BASIC
FORM
THIS
MARGINALIZATION
INVOLVES
SUMMING
OVER
N
DIMENSIONS
OF
THE
N
DIMENSIONAL
PROBABILITY
DISTRIBUTION
SINCE
THIS
DISCRETE
PROBABILITY
DISTRIBUTION
CONTAINS
KN
ENTRIES
COMPUTING
THIS
SUMMATION
DIRECTLY
IS
NOT
PRACTICAL
FOR
REALISTIC
SIZED
PROBLEMS
TO
MAKE
PROGRESS
WE
MUST
AGAIN
EXPLOIT
THE
STRUCTURED
FACTORIZATION
OF
THIS
DISTRIBUTION
COMPUTING
ONE
MARGINAL
DISTRIBUTION
WE
WILL
FIRST
DISCUSS
HOW
TO
COMPUTE
THE
MARGINAL
DISTRIBUTION
P
R
WN
N
FOR
THE
LAST
VARIABLE
IN
THE
CHAIN
WN
IN
THE
FOLLOWING
SECTION
WE
WILL
EXPLOIT
THESE
IDEAS
TO
COMPUTE
ALL
OF
THE
MARGINAL
DISTRIBUTIONS
P
R
WN
N
SIMULTANEOUSLY
WE
OBSERVE
THAT
NOT
EVERY
TERM
IN
THE
PRODUCT
IN
EQUATION
IS
RELEVANT
TO
EVERY
SUMMATION
WE
CAN
RE
ARRANGE
THE
SUMMATION
TERMS
SO
THAT
ONLY
THE
VARIABLES
OVER
WHICH
THEY
SUM
ARE
TO
THE
RIGHT
P
R
WN
N
P
R
XN
WN
P
R
P
R
P
R
P
R
P
R
THEN
WE
PROCEED
FROM
RIGHT
TO
LEFT
COMPUTING
EACH
SUMMATION
IN
TURN
THIS
TECHNIQUE
IS
KNOWN
AS
VARIABLE
ELIMINATION
LET
US
DENOTE
THE
RIGHTMOST
TWO
TERMS
AS
P
R
P
R
THEN
WE
SUM
OVER
TO
COMPUTE
THE
FUNCTION
P
R
P
R
AT
THE
NTH
STAGE
WE
COMPUTE
FN
WN
P
R
XN
WN
P
R
WN
WN
FN
WN
WN
AND
WE
REPEAT
THIS
PROCESS
UNTIL
WE
HAVE
COMPUTED
THE
FULL
EXPRESSION
WE
THEN
NORMALIZE
THE
RESULT
TO
FIND
THE
MARGINAL
POSTERIOR
P
R
WN
N
EQUATION
PROBLEM
THIS
SOLUTION
CONSISTS
OF
N
SUMMATIONS
OVER
K
VALUES
IT
IS
MUCH
MORE
EFFICIENT
TO
COMPUTE
THAN
EXPLICITLY
COMPUTING
ALL
KN
SOLUTIONS
AND
MARGINALIZING
OVER
N
DIMENSIONS
FORWARD
BACKWARD
ALGORITHM
IN
THE
PREVIOUS
SECTION
WE
SHOWED
AN
ALGORITHM
THAT
COULD
COMPUTE
THE
MARGINAL
POSTERIOR
DISTRIBUTION
P
R
WN
N
FOR
THE
LAST
WORLD
STATE
WN
IT
IS
EASY
TO
ADAPT
THIS
METHOD
TO
COMPUTE
THE
MARGINAL
POSTERIOR
P
R
WN
N
OVER
ANY
OTHER
PROBLEM
VARIABLE
WN
HOWEVER
WE
USUALLY
WANT
ALL
OF
THE
MARGINAL
DISTRIBUTIONS
AND
IT
IS
INEFFICIENT
TO
COMPUTE
EACH
SEPARATELY
AS
MUCH
OF
THE
EFFORT
IS
REPLICATED
THE
GOAL
OF
THIS
SECTION
IS
TO
DEVELOP
A
SINGLE
PROCEDURE
THAT
COMPUTES
THE
MARGINAL
POSTERIORS
FOR
ALL
OF
THE
VARIABLES
SIMULTANEOUSLY
AND
EFFICIENTLY
USING
A
TECHNIQUE
KNOWN
AS
THE
FORWARD
BACKWARD
ALGORITHM
THE
PRINCIPLE
IS
TO
DECOMPOSE
THE
MARGINAL
POSTERIOR
INTO
TWO
TERMS
P
R
WN
N
P
R
WN
N
P
R
WN
N
P
R
XN
N
WN
N
P
R
WN
N
P
R
XN
N
WN
WHERE
THE
RELATION
BETWEEN
THE
SECOND
AND
THIRD
LINE
IS
TRUE
BECAUSE
N
AND
XN
N
ARE
CONDITIONALLY
INDEPENDENT
GIVEN
WN
AS
CAN
BE
GLEANED
FROM
FIG
URE
WE
WILL
NOW
FOCUS
ON
FINDING
EFFICIENT
WAYS
TO
CALCULATE
EACH
OF
THESE
TWO
TERMS
PROBLEM
ALGORITHM
FORWARD
RECURSION
LET
US
CONSIDER
THE
FIRST
TERM
P
R
WN
N
WE
CAN
EXPLOIT
THE
RECURSION
P
R
WN
N
P
R
WN
WN
N
WN
P
R
WN
XN
WN
N
P
R
WN
N
WN
P
R
XN
WN
WN
N
P
R
WN
WN
N
P
R
WN
N
WN
P
R
XN
WN
P
R
WN
WN
P
R
WN
N
WN
WHERE
WE
HAVE
AGAIN
APPLIED
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
IMPLIED
BY
THE
GRAPHICAL
MODEL
BETWEEN
THE
LAST
TWO
LINES
THE
TERM
P
R
WN
N
IS
EXACTLY
THE
INTERMEDIATE
FUNCTION
FN
WN
THAT
WE
CAL
CULATED
IN
THE
SOLUTION
FOR
THE
SINGLE
MARGINAL
DISTRIBUTION
IN
THE
PREVIOUS
SECTION
WE
HAVE
REPRODUCED
THE
RECURSION
FN
WN
P
R
XN
WN
P
R
WN
WN
FN
WN
WN
BUT
THIS
TIME
WE
BASED
THE
ARGUMENT
ON
CONDITIONAL
INDEPENDENCE
RATHER
THAN
THE
FACTORIZATION
OF
THE
PROBABILITY
DISTRIBUTION
USING
THIS
RECURSION
WE
CAN
EFFICIENTLY
COMPUTE
THE
FIRST
TERM
OF
EQUATION
FOR
ALL
N
IN
FACT
WE
WERE
ALREADY
DOING
THIS
IN
OUR
SOLUTION
FOR
THE
SINGLE
MARGINAL
DISTRIBUTION
P
R
WN
N
BACKWARD
RECURSION
NOW
CONSIDER
THE
SECOND
TERM
P
R
XN
N
WN
FROM
EQUATION
OUR
GOAL
IS
TO
DEVELOP
A
RECURSIVE
RELATION
FOR
THIS
QUANTITY
SO
THAT
WE
CAN
COMPUTE
IT
EFFICIENTLY
FOR
ALL
N
THIS
TIME
THE
RECURSION
WORKS
BACKWARDS
FROM
THE
END
OF
THE
CHAIN
TO
THE
FRONT
SO
OUR
GOAL
IS
TO
ESTABLISH
AN
EXPRESSION
FOR
P
R
XN
N
WN
IN
TERMS
OF
P
R
XN
N
WN
P
R
XN
N
WN
P
R
XN
N
WN
WN
WN
P
R
XN
N
WN
WN
P
R
WN
WN
WN
P
R
XN
N
XN
WN
WN
P
R
XN
WN
WN
P
R
WN
WN
WN
P
R
XN
N
WN
P
R
XN
WN
P
R
WN
WN
WN
HERE
WE
HAVE
AGAIN
APPLIED
THE
CONDITIONAL
INDEPENDENCE
RELATIONS
IMPLIED
BY
THE
GRAPHICAL
MODEL
BETWEEN
THE
LAST
TWO
LINES
DENOTING
THE
PROBABILITY
P
R
XN
N
WN
AS
BN
WN
WE
SEE
THAT
WE
HAVE
THE
RECURSIVE
RELATION
BN
WN
P
R
XN
WN
P
R
WN
WN
BN
WN
WN
WE
CAN
USE
THIS
TO
COMPUTE
THE
SECOND
TERM
IN
EQUATION
EFFICIENTLY
FOR
ALL
N
FORWARD
BACKWARD
ALGORITHM
WE
CAN
NOW
SUMMARIZE
THE
FORWARD
BACKWARD
ALGORITHM
TO
COMPUTE
THE
MARGINAL
POSTERIOR
PROBABILITY
DISTRIBUTION
FOR
ALL
N
FIRST
WE
OBSERVE
EQUATION
THAT
THE
MARGINAL
DISTRIBUTION
CAN
BE
COMPUTED
AS
P
R
WN
N
P
R
WN
N
P
R
XN
N
WN
FN
WN
BN
WN
WE
RECURSIVELY
COMPUTE
THE
FORWARD
TERMS
USING
THE
RELATION
FN
WN
P
R
XN
WN
P
R
WN
WN
FN
WN
WN
WHERE
WE
SET
P
R
P
R
WE
RECURSIVELY
COMPUTE
THE
BACKWARD
TERMS
USING
THE
RELATION
BN
WN
P
R
XN
WN
P
R
WN
WN
BN
WN
WN
WHERE
WE
SET
BN
WN
TO
THE
CONSTANT
VALUE
K
FINALLY
TO
COMPUTE
THE
NTH
MARGINAL
POSTERIOR
DISTRIBUTION
WE
TAKE
THE
PRODUCT
OF
THE
ASSOCIATED
FORWARD
AND
BACKWARD
TERMS
AND
NORMALIZE
BELIEF
PROPAGATION
THE
FORWARD
BACKWARD
ALGORITHM
CAN
BE
CONSIDERED
A
SPECIAL
CASE
OF
A
MORE
GENERAL
TECHNIQUE
CALLED
BELIEF
PROPAGATION
HERE
THE
INTERMEDIATE
FUNCTIONS
F
AND
B
ARE
CONSIDERED
AS
MESSAGES
THAT
CONVEY
INFORMATION
ABOUT
THE
VARIABLES
IN
THIS
SECTION
WE
DESCRIBE
A
VERSION
OF
BELIEF
PROPAGATION
KNOWN
AS
THE
SUM
PRODUCT
ALGORITHM
THIS
DOES
NOT
COMPUTE
THE
MARGINAL
POSTERIORS
ANY
FASTER
THAN
THE
FORWARD
BACKWARD
ALGORITHM
BUT
IT
IS
MUCH
EASIER
TO
SEE
HOW
TO
EXTEND
IT
TO
MODELS
BASED
ON
TREES
THE
SUM
PRODUCT
ALGORITHM
OPERATES
ON
A
FACTOR
GRAPH
A
FACTOR
GRAPH
IS
A
NEW
TYPE
OF
GRAPHICAL
MODEL
THAT
MAKES
THE
FACTORIZATION
OF
THE
JOINT
PROBABILITY
MORE
EXPLICIT
IT
IS
VERY
SIMPLE
TO
CONVERT
DIRECTED
AND
UNDIRECTED
GRAPHICAL
MODELS
TO
FACTOR
GRAPHS
AS
USUAL
WE
INTRODUCE
ONE
NODE
PER
VARIABLE
FOR
EXAMPLE
VARIABLES
AND
ALL
HAVE
A
VARIABLE
NODE
ASSOCIATED
WITH
THEM
WE
ALSO
INTRO
DUCE
ONE
FUNCTION
NODE
PER
TERM
IN
THE
FACTORIZED
JOINT
PROBABILITY
DISTRIBUTION
IN
A
DIRECTED
MODEL
THIS
WOULD
REPRESENT
A
CONDITIONAL
PROBABILITY
TERM
SUCH
AS
PROBLEM
FIGURE
FACTOR
GRAPH
FOR
CHAIN
MODEL
THERE
IS
ONE
NODE
PER
VARIABLE
CIRCLES
AND
ONE
FUNCTION
NODE
PER
TERM
IN
THE
FACTORIZATION
SQUARES
EACH
FUNCTION
NODE
CONNECTS
TO
ALL
OF
THE
VARIABLES
ASSOCIATED
WITH
THIS
TERM
P
R
AND
IN
AN
UNDIRECTED
MODEL
IT
WOULD
REPRESENT
A
POTENTIAL
FUNCTION
SUCH
AS
Φ
WE
THEN
CONNECT
EACH
FUNCTION
NODE
TO
ALL
OF
THE
VARIABLE
NODES
RELEVANT
TO
THAT
TERM
WITH
UNDIRECTED
LINKS
SO
IN
A
DIRECTED
MODEL
A
TERM
LIKE
P
R
WOULD
RESULT
IN
A
FUNCTION
NODE
THAT
CONNECTS
TO
AND
IN
AN
UNDIRECTED
MODEL
A
TERM
LIKE
WOULD
RESULT
IN
A
FUNCTION
NODE
THAT
CONNECTS
TO
AND
FIGURE
SHOWS
THE
FACTOR
GRAPH
FOR
THE
CHAIN
MODEL
ALGORITHM
SUM
PRODUCT
ALGORITHM
THE
SUM
PRODUCT
ALGORITHM
PROCEEDS
IN
TWO
PHASES
A
FORWARD
PASS
AND
A
BACKWARD
PASS
THE
FORWARD
PASS
DISTRIBUTES
EVIDENCE
THROUGH
THE
GRAPH
AND
THE
BACKWARD
PASS
COLLATES
THIS
EVIDENCE
BOTH
THE
DISTRIBUTION
AND
COLLATION
OF
EVIDENCE
ARE
ACCOMPLISHED
BY
PASSING
MESSAGES
FROM
NODE
TO
NODE
IN
THE
FACTOR
GRAPH
EVERY
EDGE
IN
THE
GRAPH
IS
CONNECTED
TO
EXACTLY
ONE
VARIABLE
NODE
AND
EACH
MESSAGE
IS
DEFINED
OVER
THE
DOMAIN
OF
THIS
VARIABLE
THERE
ARE
THREE
TYPES
OF
MESSAGES
A
MESSAGE
MZP
GQ
FROM
AN
UNOBSERVED
VARIABLE
ZP
TO
A
FUNCTION
NODE
GQ
IS
GIVEN
BY
MZP
GQ
R
NE
P
Q
MGR
ZP
WHERE
NE
P
RETURNS
THE
SET
OF
THE
NEIGHBORS
OF
ZP
IN
THE
GRAPH
AND
SO
THE
EXPRESSION
NE
P
Q
DENOTES
ALL
OF
THE
NEIGHBOURS
EXCEPT
Q
IN
OTHER
WORDS
THE
MESSAGE
FROM
A
VARIABLE
TO
A
FUNCTION
NODE
IS
THE
POINTWISE
PRODUCT
OF
ALL
OTHER
INCOMING
MESSAGES
TO
THE
VARIABLE
IT
IS
THE
COMBINATION
OF
OTHER
BELIEFS
A
MESSAGE
MZP
GQ
FROM
AN
OBSERVED
VARIABLE
ZP
Z
P
TO
A
FUNCTION
NODE
GQ
IS
GIVEN
BY
MZP
GQ
Δ
Z
P
IN
OTHER
WORDS
THE
MESSAGE
FROM
AN
OBSERVED
NODE
TO
A
FUNCTION
CONVEYS
THE
CERTAIN
BELIEF
THAT
THIS
NODE
TOOK
THE
OBSERVED
VALUE
A
MESSAGE
MGP
ZQ
FROM
A
FUNCTION
NODE
GP
TO
A
RECIPIENT
VARIABLE
ZQ
IS
DEFINED
AS
MGP
ZQ
NE
P
Q
GP
NE
P
R
NE
P
Q
MZR
GP
THIS
TAKES
BELIEFS
FROM
ALL
VARIABLES
CONNECTED
TO
THE
FUNCTION
EXCEPT
THE
RECIPIENT
VARIABLE
AND
USES
THE
FUNCTION
GP
TO
CONVERT
THESE
TO
A
BELIEF
ABOUT
THE
RECIPIENT
VARIABLE
IN
THE
FORWARD
PHASE
THE
MESSAGE
PASSING
CAN
PROCEED
IN
ANY
ORDER
AS
LONG
AS
THE
OUTGOING
MESSAGE
FROM
ANY
VARIABLE
OR
FUNCTION
IS
NOT
SENT
UNTIL
ALL
THE
OTHER
INCOMING
MESSAGES
HAVE
ARRIVED
IN
THE
BACKWARD
PASS
THE
MESSAGES
ARE
SENT
IN
THE
OPPOSITE
ORDER
TO
THE
FORWARD
PASS
FINALLY
THE
MARGINAL
DISTRIBUTION
AT
NODE
ZP
CAN
BE
COMPUTED
FROM
A
PRODUCT
OF
ALL
OF
THE
INCOMING
MESSAGES
FROM
BOTH
THE
FORWARD
AND
REVERSE
PASSES
SO
THAT
P
R
ZP
R
NE
P
MGR
ZP
A
PROOF
THAT
THIS
ALGORITHM
IS
CORRECT
IS
BEYOND
THE
SCOPE
OF
THIS
BOOK
HOWEVER
TO
MAKE
THIS
AT
LEAST
PARTIALLY
CONVINCING
AND
MORE
CONCRETE
WE
WILL
WORK
THROUGH
THESE
RULES
FOR
THE
CASE
OF
THE
CHAIN
MODEL
FIGURE
AND
WE
WILL
SHOW
THAT
EXACTLY
THE
SAME
COMPUTATION
OCCURS
AS
FOR
THE
FORWARD
BACKWARD
ALGORITHM
SUM
PRODUCT
ALGORITHM
FOR
CHAIN
MODEL
THE
FACTOR
GRAPH
FOR
THE
CHAIN
SOLUTION
ANNOTATED
WITH
MESSAGES
IS
SHOWN
IN
FIGURE
WE
WILL
NOW
DESCRIBE
THE
SUM
PRODUCT
ALGORITHM
FOR
THE
CHAIN
MODEL
FORWARD
PASS
WE
START
BY
PASSING
A
MESSAGE
FROM
NODE
TO
THE
FUNCTION
NODE
USING
RULE
THIS
MESSAGE
IS
A
DELTA
FUNCTION
AT
THE
OBSERVED
VALUE
X
SO
THAT
Δ
X
NOW
WE
PASS
A
MESSAGE
FROM
FUNCTION
TO
NODE
USING
RULE
WE
HAVE
P
R
Δ
X
P
R
X
BY
RULE
THE
MESSAGE
FROM
NODE
TO
FUNCTION
IS
SIMPLY
THE
PRODUCT
OF
THE
INCOMING
NODES
AND
SINCE
THERE
IS
ONLY
ONE
INCOMING
NODE
THIS
IS
JUST
FIGURE
SUM
PRODUCT
ALGORITHM
FOR
CHAIN
MODEL
FORWARD
PASS
THE
SUM
PRODUCT
ALGORITHM
HAS
TWO
PHASES
IN
THE
FORWARD
PHASE
MESSAGES
ARE
PASSED
THROUGH
THE
GRAPH
IN
AN
ORDER
SUCH
THAT
A
MESSAGE
CANNOT
BE
SENT
UNTIL
ALL
INCOMING
MESSAGES
ARE
RECEIVED
AT
THE
SOURCE
NODE
SO
THE
MESSAGE
CANNOT
BE
SENT
UNTIL
THE
MESSAGES
AND
HAVE
BEEN
RECEIVED
P
R
X
BY
RULE
THE
MESSAGE
FROM
FUNCTION
TO
NODE
IS
COMPUTED
AS
P
R
P
R
CONTINUING
THIS
PROCESS
THE
MESSAGES
FROM
TO
AND
TO
ARE
Δ
X
P
R
X
AND
THE
MESSAGE
FROM
TO
IS
P
R
P
R
P
R
X
A
CLEAR
PATTERN
IS
EMERGING
THE
MESSAGE
FROM
NODE
WN
TO
FUNCTION
GN
N
IS
EQUAL
TO
THE
FORWARD
TERM
FROM
THE
FORWARD
BACKWARD
ALGORITHM
MWN
GN
N
FN
WN
P
R
WN
N
IN
OTHER
WORDS
THE
SUM
PRODUCT
ALGORITHM
IS
PERFORMING
EXACTLY
THE
SAME
COMPU
TATIONS
AS
THE
FORWARD
PASS
OF
THE
FORWARD
BACKWARD
ALGORITHM
FIGURE
FACTOR
GRAPH
CORRE
SPONDING
TO
TREE
MODEL
IN
FIGURE
THERE
IS
ONE
FUNCTION
NODE
CONNECTING
EACH
WORLD
STATE
VARIABLE
TO
ITS
ASSO
CIATED
MEASUREMENT
AND
THESE
CORRE
SPOND
TO
THE
TERMS
P
R
XN
WN
THERE
IS
ONE
FUNCTION
NODE
FOR
EACH
OF
THE
THREE
PAIRWISE
TERMS
P
R
P
R
AND
P
R
AND
THIS
IS
CONNECTED
TO
BOTH
CONTRIBUTING
VARIABLES
THE
FUNCTION
NODE
COR
RESPONDING
TO
THE
THREE
WISE
TERM
P
R
HAS
THREE
NEIGHBORS
AND
BACKWARD
PASS
WHEN
WE
REACH
THE
END
OF
THE
FORWARD
PASS
OF
THE
BELIEF
PROPAGATION
WE
INITIATE
THE
BACKWARD
PASS
THERE
IS
NO
NEED
TO
PASS
MESSAGES
TOWARD
THE
OBSERVED
VARIABLES
XN
SINCE
WE
ALREADY
KNOW
THEIR
VALUES
FOR
CERTAIN
HENCE
WE
CONCENTRATE
ON
THE
HORIZONTAL
CONNECTIONS
BETWEEN
THE
UNOBSERVED
VARIABLES
I
E
ALONG
THE
SPINE
OF
THE
MODEL
THE
MESSAGE
FROM
NODE
WN
TO
FUNCTION
GN
N
IS
GIVEN
BY
MWN
GN
N
P
R
XN
X
N
WN
AND
THE
MESSAGE
FROM
GN
N
TO
WN
IS
GIVEN
BY
MGN
N
WN
P
R
WN
WN
P
R
XN
X
N
WN
WN
IN
GENERAL
WE
HAVE
MGN
N
WN
P
R
WN
WN
P
R
XN
WN
MGN
N
WN
WN
BN
WN
WHICH
IS
EXACTLY
THE
BACKWARD
RECURSION
FROM
THE
FORWARD
BACKWARD
ALGORITHM
COLLATING
EVIDENCE
FINALLY
TO
COMPUTE
THE
MARGINAL
PROBABILITIES
WE
USE
THE
RELATION
P
R
WN
N
M
NE
N
MGM
WN
AND
FOR
THE
GENERAL
CASE
THIS
CONSISTS
OF
THREE
TERMS
P
R
WN
N
MGN
N
WN
MGN
WN
MGN
N
WN
MWN
GN
N
MGN
N
WN
FN
WN
BN
WN
MODELS
FOR
CHAINS
AND
TREES
A
B
FIGURE
CONVERTING
AN
UNDIRECTED
MODEL
TO
A
FACTOR
GRAPH
A
UNDI
RECTED
MODEL
B
CORRESPONDING
FACTOR
GRAPH
THERE
IS
ONE
FUNCTION
NODE
FOR
EACH
MAXIMAL
CLIQUE
EACH
CLIQUE
WHICH
IS
NOT
A
SUBSET
OF
ANOTHER
CLIQUE
ALTHOUGH
THERE
WAS
CLEARLY
A
LOOP
IN
THE
ORIGINAL
GRAPH
THERE
IS
NO
LOOP
IN
THE
FACTOR
GRAPH
AND
SO
THE
SUM
PRODUCT
ALGORITHM
IS
STILL
APPLICABLE
WHERE
IN
THE
SECOND
LINE
WE
HAVE
USED
THE
FACT
THAT
THE
OUTGOING
MESSAGE
FROM
A
VARIABLE
NODE
IS
THE
PRODUCT
OF
THE
INCOMING
MESSAGES
WE
CONCLUDE
THAT
THE
SUM
PRODUCT
ALGORITHM
COMPUTES
THE
POSTERIOR
MARGINALS
IN
EXACTLY
THE
SAME
WAY
AS
THE
FORWARD
BACKWARD
ALGORITHM
MARGINAL
POSTERIOR
INFERENCE
FOR
TREES
PROBLEM
TO
COMPUTE
THE
MARGINALS
IN
TREE
STRUCTURED
MODELS
WE
SIMPLY
APPLY
THE
SUM
PRODUCT
ALGORITHM
TO
THE
NEW
GRAPH
STRUCTURE
THE
FACTOR
GRAPH
FOR
THE
TREE
IN
FIGURE
IS
SHOWN
IN
FIGURE
THE
ONLY
SLIGHT
COMPLICATION
IS
THAT
WE
MUST
ENSURE
THAT
THE
FIRST
TWO
INCOMING
MESSAGES
TO
THE
FUNCTION
RELATING
VARIABLES
AND
HAVE
ARRIVED
BEFORE
SENDING
THE
OUTGOING
MESSAGE
THIS
IS
VERY
SIMILAR
TO
THE
ORDER
OF
OPERATIONS
IN
THE
DYNAMIC
PROGRAMMING
ALGORITHM
FOR
UNDIRECTED
GRAPHS
THE
KEY
PROPERTY
IS
THAT
THE
CLIQUES
NOT
THE
NODES
FORM
A
TREE
FOR
EXAMPLE
THERE
IS
CLEARLY
A
LOOP
IN
THE
UNDIRECTED
MODEL
IN
FIGURE
BUT
WHEN
WE
CONVERT
THIS
TO
A
FACTOR
GRAPH
THE
STRUCTURE
IS
A
TREE
FIGURE
FOR
MODELS
WITH
ONLY
PAIRWISE
CLIQUES
THE
CLIQUES
ALWAYS
FORM
A
TREE
IF
THERE
ARE
NO
LOOPS
IN
THE
ORIGINAL
GRAPHICAL
MODEL
LEARNING
IN
CHAINS
AND
TREES
SO
FAR
WE
HAVE
ONLY
DISCUSSED
INFERENCE
FOR
THESE
MODELS
HERE
WE
BRIEFLY
DISCUSS
LEARNING
WHICH
CAN
BE
DONE
IN
A
SUPERVISED
OR
UNSUPERVISED
CONTEXT
IN
THE
SUPER
VISED
CASE
WE
ARE
GIVEN
A
TRAINING
SET
OF
I
MATCHED
SETS
OF
STATES
WIN
I
N
AND
BEYOND
CHAINS
AND
TREES
DATA
XIN
I
N
IN
THE
UNSUPERVISED
CASE
WE
ONLY
OBSERVE
THE
DATA
XIN
I
N
SUPERVISED
LEARNING
FOR
DIRECTED
MODELS
IS
RELATIVELY
SIMPLE
WE
FIRST
ISOLATE
THE
PART
OF
THE
MODEL
THAT
WE
WANT
TO
LEARN
FOR
EXAMPLE
WE
MIGHT
LEARN
THE
PARAMETERS
Θ
OF
P
R
XN
WN
Θ
FROM
PAIRED
EXAMPLES
OF
XN
AND
WN
WE
CAN
THEN
LEARN
THESE
PARAMETERS
IN
ISOLATION
USING
THE
ML
MAP
OR
BAYESIAN
METHODS
UNSUPERVISED
LEARNING
IS
MORE
CHALLENGING
THE
STATES
WN
ARE
TREATED
AS
HIDDEN
VARIABLES
AND
THE
EM
ALGORITHM
IS
APPLIED
IN
THE
E
STEP
WE
COMPUTE
THE
POSTERIOR
MARGINALS
OVER
THE
STATES
USING
THE
FORWARD
BACKWARD
ALGORITHM
IN
THE
M
STEP
WE
USE
THESE
MARGINALS
TO
UPDATE
THE
MODEL
PARAMETERS
FOR
THE
HIDDEN
MARKOV
MODEL
THE
CHAIN
MODEL
THIS
IS
KNOWN
AS
THE
BAUM
WELCH
ALGORITHM
AS
WE
SAW
IN
THE
PREVIOUS
CHAPTER
LEARNING
IN
UNDIRECTED
MODELS
CAN
BE
CHAL
LENGING
WE
CANNOT
GENERALLY
COMPUTE
THE
NORMALIZATION
CONSTANT
Z
AND
THIS
IN
TURN
PREVENTS
US
FROM
COMPUTING
THE
DERIVATIVE
WITH
RESPECT
TO
THE
PARAMETERS
HOWEVER
FOR
THE
SPECIAL
CASE
OF
TREE
AND
CHAIN
MODELS
IT
IS
POSSIBLE
TO
COMPUTE
Z
EFFICIENTLY
AND
LEARNING
IS
TRACTABLE
TO
SEE
WHY
THIS
IS
THE
CASE
CONSIDER
THE
UNDIRECTED
MODEL
FROM
FIGURE
WHICH
WE
WILL
TREAT
HERE
AS
REPRESENTING
THE
CONDITIONAL
DISTRIBUTION
N
P
R
N
N
Z
N
Φ
XN
WN
TTN
Ζ
WN
WN
SINCE
THE
DATA
NODES
XN
N
ARE
FIXED
THIS
MODEL
IS
KNOWN
AS
A
CONDITIONAL
RANDOM
FIELD
THE
UNKNOWN
CONSTANT
Z
NOW
HAS
THE
FORM
Z
TTN
Φ
XN
WN
TTN
Ζ
WN
WN
WN
N
N
WE
HAVE
ALREADY
SEEN
THAT
IT
IS
POSSIBLE
TO
COMPUTE
THIS
TYPE
OF
SUM
EFFICIENTLY
USING
A
RECURSION
EQUATION
HENCE
Z
CAN
BE
EVALUATED
AND
MAXIMUM
LIKELIHOOD
LEARNING
CAN
BE
PERFORMED
IN
THIS
MODEL
WITHOUT
THE
NEED
FOR
CONTRASTIVE
DIVERGENCE
BEYOND
CHAINS
AND
TREES
UNFORTUNATELY
THERE
ARE
MANY
MODELS
IN
COMPUTER
VISION
THAT
DO
NOT
TAKE
THE
FORM
OF
A
CHAIN
OR
A
TREE
OF
PARTICULAR
IMPORTANCE
ARE
MODELS
THAT
ARE
STRUCTURED
TO
HAVE
ONE
UNKNOWN
WN
FOR
EACH
RGB
PIXEL
XN
IN
THE
IMAGE
THESE
MODELS
ARE
NATURALLY
STRUCTURED
AS
GRIDS
AND
THE
WORLD
STATES
ARE
EACH
CONNECTED
TO
THEIR
FOUR
PIXEL
NEIGHBORS
IN
THE
GRAPHICAL
MODEL
FIGURE
STEREO
VISION
SEGMENTATION
DE
NOISING
SUPER
RESOLUTION
AND
MANY
OTHER
VISION
PROBLEMS
CAN
ALL
BE
FRAMED
IN
THIS
WAY
WE
DEVOTE
THE
WHOLE
OF
THE
NEXT
CHAPTER
TO
GRID
BASED
PROBLEMS
BUT
WE
WILL
BRIEFLY
TAKE
THE
TIME
TO
EXAMINE
WHY
THE
METHODS
DEVELOPED
IN
THIS
CHAPTER
ARE
NOT
SUITABLE
CONSIDER
A
SIMPLE
MODEL
BASED
ON
A
GRID
FIGURE
ILLUSTRATES
WHY
MODELS
FOR
CHAINS
AND
TREES
A
B
FIGURE
GRID
BASED
MODELS
FOR
MANY
VISION
PROBLEMS
THE
NATURAL
DESCRIPTION
IS
A
GRID
BASED
MODEL
WE
OBSERVE
A
GRID
OF
PIXEL
VALUES
XN
N
AND
WISH
TO
INFER
AN
UNKNOWN
WORLD
STATE
WN
N
ASSOCIATED
WITH
EACH
SITE
EACH
WORLD
STATE
IS
CONNECTED
TO
ITS
NEIGHBORS
THESE
CONNECTIONS
ARE
USUALLY
APPLIED
TO
ENSURE
A
SMOOTH
OR
PIECEWISE
SMOOTH
SOLUTION
A
DIRECTED
GRID
MODEL
B
UNDIRECTED
GRID
MODEL
CONDITIONAL
RANDOM
FIELD
FIGURE
DYNAMIC
PROGRAMMING
FAILS
WHEN
THERE
ARE
UNDIRECTED
LOOPS
HERE
WE
SHOW
A
IMAGE
WHERE
WE
HAVE
PERFORMED
A
NA
IIVE
FORWARD
PASS
THROUGH
THE
VARIABLES
ON
RE
TRACING
THE
ROUTE
WE
SEE
THAT
THE
TWO
BRANCHES
DISAGREE
OVER
WHICH
STATE
THE
FIRST
VARIABLE
TOOK
FOR
A
COHERENT
SO
LUTION
THE
CUMULATIVE
MINIMUM
COSTS
AT
NODE
WE
SHOULD
HAVE
FORCED
THE
TWO
PATHS
TO
HAVE
COMMON
ANCESTORS
WITH
A
LARGE
NUMBER
OF
ANCESTORS
THIS
IS
TOO
COMPUTATIONALLY
EXPENSIVE
TO
BE
PRACTICAL
WE
CANNOT
BLINDLY
USE
DYNAMIC
PROGRAMMING
TO
COMPUTE
THE
MAP
SOLUTION
TO
COMPUTE
THE
MINIMUM
CUMULATIVE
COST
SN
AT
EACH
NODE
WE
MIGHT
NA
IVELY
PROCEED
AS
NORMAL
K
K
K
K
MIN
L
K
L
L
K
K
MIN
L
K
L
L
NOW
CONSIDER
THE
FOURTH
TERM
UNFORTUNATELY
FIGURE
PRUNING
GRAPHS
WITH
LOOPS
ONE
APPROACH
TO
DEALING
WITH
MODELS
WITH
LOOPS
IS
SIMPLY
TO
PRUNE
THE
CONNECTIONS
UNTIL
THE
LOOPS
ARE
RE
MOVED
THIS
GRAPHICAL
MODEL
IS
THE
MODEL
FROM
FIGURE
AFTER
SUCH
A
PRUNING
PROCESS
MOST
OF
THE
CON
NECTIONS
ARE
RETAINED
BUT
NOW
THE
RE
MAINING
STRUCTURE
IS
A
TREE
THE
USUAL
APPROACH
TO
PRUNING
IS
TO
ASSOCIATE
A
STRENGTH
WITH
EACH
EDGE
SO
THAT
WEAKER
EDGES
ARE
MORE
DESIRABLE
THEN
WE
COMPUTE
THE
MINIMUM
SPANNING
TREE
BASED
IN
THESE
STRENGTHS
AND
DISCARD
ANY
CONNECTIONS
THAT
DO
NOT
FORM
PART
OF
THE
TREE
K
WK
MIN
L
M
T
K
L
M
L
M
THE
REASON
FOR
THIS
IS
THAT
THE
PARTIAL
CUMULATIVE
SUMS
AND
AT
THE
TWO
PREVIOUS
VERTICES
BOTH
RELY
ON
MINIMIZING
OVER
THE
SAME
VARIABLE
HOWEVER
THEY
DID
NOT
NECESSARILY
CHOOSE
THE
SAME
VALUE
AT
IF
WE
WERE
TO
TRACE
BACK
THE
PATHS
WE
TOOK
THE
TWO
ROUTES
BACK
TO
VERTEX
ONE
MIGHT
PREDICT
A
DIFFERENT
ANSWER
TO
PROPERLY
CALCULATE
THE
MINIMUM
CUMULATIVE
COST
AT
NODE
K
WE
WOULD
HAVE
TO
TAKE
ACCOUNT
OF
ALL
THREE
ANCESTORS
THE
RECURSION
IS
NO
LONGER
VALID
AND
THE
PROBLEM
BECOMES
INTRACTABLE
ONCE
MORE
SIMILARLY
WE
CANNOT
PERFORM
BELIEF
PROPAGATION
ON
THIS
GRAPH
THE
ALGORITHM
REQUIRES
US
TO
SEND
A
MESSAGE
FROM
A
NODE
ONLY
WHEN
ALL
OTHER
INCOMING
MESSAGES
HAVE
BEEN
RECEIVED
HOWEVER
THE
NODES
AND
ALL
SIMULTANEOUSLY
REQUIRE
MESSAGES
FROM
ONE
ANOTHER
AND
SO
THIS
IS
NOT
POSSIBLE
INFERENCE
IN
GRAPHS
WITH
LOOPS
ALTHOUGH
THE
METHODS
OF
THIS
CHAPTER
ARE
NOT
SUITABLE
FOR
MODELS
BASED
ON
GRAPHS
WITH
LOOPS
THERE
ARE
A
NUMBER
OF
WAYS
TO
PROCEED
PRUNE
THE
GRAPH
AN
OBVIOUS
IDEA
IS
TO
PRUNE
THE
GRAPH
BY
REMOVING
EDGES
UNTIL
WHAT
IS
LEFT
HAS
A
TREE
STRUCTURE
FIGURE
THE
CHOICE
OF
WHICH
EDGES
TO
PRUNE
WILL
DEPEND
ON
THE
REAL
WORLD
PROBLEM
COMBINE
VARIABLES
A
SECOND
APPROACH
IS
TO
COMBINE
VARIABLES
TOGETHER
UNTIL
WHAT
REMAINS
HAS
THE
STRUCTURE
OF
A
CHAIN
OR
TREE
FOR
EXAMPLE
IN
FIGURE
WE
COMBINE
THE
VARIABLES
AND
TO
MAKE
A
NEW
VARIABLE
AND
THE
VARIABLES
AND
TO
FORM
CONTINUING
IN
THIS
WAY
WE
FORM
A
MODEL
THAT
HAS
A
CHAIN
STRUCTURE
IF
EACH
OF
THE
ORIGINAL
VARIABLES
HAD
K
STATES
THEN
THE
COMPOUND
VARIABLES
WILL
HAVE
STATES
PROBLEM
FIGURE
COMBINING
VARIABLES
A
A
THIS
GRAPHICAL
MODEL
CONTAINS
LOOPS
B
WE
FORM
THREE
COM
POUND
VARIABLES
EACH
OF
WHICH
CON
SISTS
OF
ALL
OF
THE
VARIABLES
IN
ONE
OF
THE
ORIGINAL
COLUMNS
THESE
ARE
NOW
CONNECTED
BY
A
CHAIN
STRUCTURE
HOWEVER
THE
PRICE
WE
PAY
IS
THAT
IF
THERE
WERE
K
STATES
FOR
EACH
ORIG
INAL
VARIABLE
THE
COMPOUND
VARI
ABLES
NOW
HAVE
K
STATES
AND
SO
INFERENCE
WILL
BE
MORE
EXPENSIVE
IN
GENERAL
THE
MERGING
OF
VARIABLES
CAN
BE
AUTOMATED
USING
THE
JUNCTION
TREE
ALGORITHM
UNFORTUNATELY
THIS
EXAMPLE
ILLUSTRATES
WHY
THIS
APPROACH
WILL
NOT
WORK
FOR
LARGE
GRID
MODELS
WE
MUST
MERGE
TOGETHER
SO
MANY
VARIABLES
THAT
THE
RESULTING
COMPOUND
VARIABLE
HAS
TOO
MANY
STATES
TO
WORK
WITH
LOOPY
BELIEF
PROPAGATION
ANOTHER
IDEA
IS
TO
SIMPLY
APPLY
BELIEF
PROPAGA
TION
REGARDLESS
OF
THE
LOOPS
ALL
MESSAGES
ARE
INITIALIZED
TO
UNIFORM
AND
THEN
THE
MESSAGES
ARE
REPEATEDLY
PASSED
IN
SOME
ORDER
ACCORDING
TO
THE
NORMAL
RULES
THIS
ALGORITHM
IS
NOT
GUARANTEED
TO
CONVERGE
TO
THE
CORRECT
SOLUTION
FOR
THE
MARGINALS
OR
INDEED
TO
CONVERGE
AT
ALL
BUT
IN
PRACTICE
IT
PRODUCES
USEFUL
RESULTS
IN
MANY
SITUATIONS
SAMPLING
APPROACHES
FOR
DIRECTED
GRAPHICAL
MODELS
IT
IS
USUALLY
EASY
TO
DRAW
SAMPLES
FROM
THE
POSTERIOR
THESE
CAN
THEN
BE
AGGREGATED
TO
COMPUTE
AN
EMPIRICAL
ESTIMATE
OF
THE
MARGINAL
DISTRIBUTIONS
OTHER
APPROACHES
THERE
ARE
SEVERAL
OTHER
APPROACHES
FOR
EXACT
OR
AP
PROXIMATE
INFERENCE
IN
GRAPHS
INCLUDING
TREE
REWEIGHTED
MESSAGE
PASSING
AND
GRAPH
CUTS
THE
LATTER
IS
A
PARTICULARLY
IMPORTANT
CLASS
OF
ALGORITHM
AND
WE
DEVOTE
MOST
CHAPTER
TO
DESCRIBING
IT
APPLICATIONS
THE
MODELS
IN
THIS
CHAPTER
ARE
ATTRACTIVE
BECAUSE
THEY
PERMIT
EXACT
MAP
INFERENCE
THEY
HAVE
BEEN
APPLIED
TO
A
NUMBER
OF
PROBLEMS
IN
WHICH
THERE
ARE
ASSUMED
SPATIAL
OR
TEMPORAL
CONNECTIONS
BETWEEN
PARTS
OF
A
MODEL
FIGURE
GESTURE
TRACKING
FROM
STARNER
ET
AL
A
CAMERA
WAS
MOUNTED
ON
A
BASEBALL
CAP
INSET
LOOKING
DOWN
AT
THE
USERS
HANDS
THE
CAMERA
IMAGE
MAIN
FIGURE
WAS
USED
TO
TRACK
THE
HANDS
IN
A
HMM
BASED
SYSTEM
THAT
COULD
ACCURATELY
CLASSIFY
A
WORD
LEXICON
AND
WORKED
IN
REAL
TIME
EACH
WORD
WAS
ASSOCIATED
WITH
FOUR
STATES
IN
THE
HMM
THE
SYS
TEM
WAS
BASED
ON
A
COMPACT
DESCRIP
TION
OF
THE
HAND
POSITION
AND
ORIENTA
TION
WITHIN
EACH
FRAME
ADAPTED
FROM
STARNER
ET
AL
QC
SPRINGER
GESTURE
TRACKING
THE
GOAL
OF
GESTURE
TRACKING
IS
TO
CLASSIFY
THE
POSITION
WN
N
OF
THE
HANDS
WITHIN
EACH
OF
THE
N
CAPTURED
FRAMES
FROM
A
VIDEO
SEQUENCE
INTO
A
DISCRETE
SET
OF
POSSIBLE
GESTURES
WN
K
BASED
ON
EXTRACTED
DATA
XN
N
FROM
THOSE
FRAMES
STARNER
ET
AL
PRESENTED
A
WEARABLE
SYSTEM
FOR
AUTOMATICALLY
INTERPRETING
SIGN
LANGUAGE
GESTURES
A
CAMERA
MOUNTED
IN
THE
USER
HAT
CAPTURED
A
TOP
DOWN
VIEW
OF
THEIR
HANDS
FIGURE
THE
POSITIONS
OF
THE
HANDS
WERE
IDENTIFIED
BY
USING
A
PER
PIXEL
SKIN
SEGMENTATION
TECHNIQUE
SEE
SECTION
THE
STATE
OF
EACH
HAND
WAS
CHARACTERIZED
IN
TERMS
OF
THE
POSITION
AND
SHAPE
OF
A
BOUNDING
ELLIPSE
AROUND
THE
ASSOCIATED
SKIN
REGION
THE
FINAL
EIGHT
DIMENSIONAL
DATA
VECTOR
X
CONCATENATED
THESE
MEASUREMENTS
FROM
EACH
HAND
TO
DESCRIBE
THE
TIME
SEQUENCES
OF
THESE
MEASUREMENTS
STARNER
ET
AL
DEVELOPED
A
HIDDEN
MARKOV
MODEL
BASED
SYSTEM
IN
WHICH
THE
STATES
WN
EACH
REP
RESENTED
A
PART
OF
A
SIGN
LANGUAGE
WORD
EACH
OF
THESE
WORDS
WAS
REPRESENTED
BY
A
PROGRESSION
THROUGH
FOUR
VALUES
OF
THE
STATE
VARIABLE
W
REPRESENTING
THE
VARIOUS
STAGES
IN
THE
ASSOCIATED
GESTURE
FOR
THAT
WORD
THE
PROGRESSION
THROUGH
THESE
STATES
MIGHT
LAST
ANY
NUMBER
OF
TIME
STEPS
EACH
STATE
CAN
BE
FOLLOWED
BY
ITSELF
SO
IT
CAN
CYCLE
INDEFINITELY
BUT
MUST
COME
IN
THE
REQUIRED
ORDER
THEY
TRAINED
THE
SYSTEM
USING
TRAINING
SENTENCES
THEY
USED
A
DYNAMIC
PROGRAMMING
METHOD
TO
ES
TIMATE
THE
MOST
LIKELY
STATES
W
AND
ACHIEVED
RECOGNITION
ACCURACY
OF
USING
A
WORD
LEXICON
WITH
A
TEST
SET
OF
SENTENCES
THEY
FOUND
THAT
PERFORMANCE
WAS
FURTHER
IMPROVED
IF
THEY
IMPOSED
KNOWLEDGE
ABOUT
THE
FIXED
GRAMMAR
OF
EACH
PHRASE
PRONOUN
VERB
NOUN
ADJECTIVE
PRONOUN
REMARKABLY
THE
SYSTEM
WORKED
AT
A
RATE
OF
FRAMES
A
SECOND
STEREO
VISION
IN
DENSE
STEREO
VISION
WE
ARE
GIVEN
TWO
IMAGES
OF
THE
SAME
SCENE
TAKEN
FROM
SLIGHTLY
DIFFERENT
POSITIONS
FOR
OUR
PURPOSES
WE
WILL
ASSUME
THAT
THEY
HAVE
BEEN
PREPROCESSED
SO
THAT
FOR
EACH
PIXEL
IN
IMAGE
THE
CORRESPONDING
PIXEL
IS
ON
THE
SAME
SCANLINE
IN
IMAGE
A
PROCESS
KNOWN
AS
RECTIFICATION
SEE
CHAPTER
THE
FIGURE
DENSE
STEREO
VISION
A
B
TWO
IMAGES
TAKEN
FROM
SLIGHTLY
DIFFERENT
POSITIONS
THE
CORRESPONDING
POINT
FOR
EVERY
PIXEL
IN
THE
FIRST
IMAGE
IS
SOMEWHERE
ON
THE
SAME
SCANLINE
IN
THE
SECOND
IMAGE
THE
HORIZONTAL
OFFSET
IS
KNOWN
AS
THE
DISPARITY
AND
IS
INVERSELY
RELATED
TO
DEPTH
C
GROUND
TRUTH
DISPARITY
MAP
FOR
THIS
IMAGE
D
CLOSE
UP
OF
PART
OF
FIRST
IMAGE
WITH
TWO
PIXELS
HIGHLIGHTED
E
CLOSE
UP
OF
SECOND
IMAGE
WITH
POTENTIAL
CORRESPONDING
PIXELS
HIGHLIGHTED
F
RGB
VALUES
FOR
RED
PIXEL
DASHED
LINES
IN
FIRST
IMAGE
AND
AS
A
FUNCTION
OF
THE
POSITION
IN
SECOND
IMAGE
SOLID
LINES
AT
THE
CORRECT
DISPARITY
THERE
IS
VERY
LITTLE
DIFFERENCE
BETWEEN
THE
RGB
VALUES
IN
THE
TWO
IMAGES
AND
SO
G
THE
LIKELIHOOD
THAT
THIS
DISPARITY
IS
CORRECT
IS
LARGE
H
I
FOR
THE
GREEN
PIXEL
WHICH
IS
IN
A
SMOOTHLY
CHANGING
REGION
OF
THE
IMAGE
THERE
ARE
MANY
POSITIONS
WHERE
THE
RGB
VALUES
IN
THE
SECOND
IMAGE
ARE
SIMILAR
AND
HENCE
MANY
DISPARITIES
HAVE
HIGH
LIKELIHOODS
THE
SOLUTION
IS
AMBIGUOUS
HORIZONTAL
OFFSET
OR
DISPARITY
BETWEEN
CORRESPONDING
POINTS
DEPENDS
ON
THE
DEPTH
OUR
GOAL
IS
TO
FIND
THE
DISCRETE
DISPARITY
FIELD
W
GIVEN
THE
OBSERVED
IMAGES
X
AND
X
FROM
WHICH
THE
DEPTH
OF
EACH
PIXEL
CAN
BE
RECOVERED
FIGURE
WE
ASSUME
THAT
THE
PIXEL
IN
IMAGE
SHOULD
CLOSELY
RESEMBLE
THE
PIXEL
AT
THE
APPROPRIATE
OFFSET
DISPARITY
IN
IMAGE
AND
ANY
REMAINING
SMALL
DIFFERENCES
ARE
TREATED
AS
NOISE
SO
THAT
P
R
X
WM
N
K
NORM
APPLICATIONS
FIGURE
DENSE
STEREO
RESULTS
RECOVERED
DISPARITY
MAPS
FOR
A
INDEPEN
DENT
PIXELS
MODEL
B
INDEPENDENT
SCANLINES
MODEL
AND
C
TREE
BASED
MODEL
OF
VEKSLER
WHERE
W
M
N
IS
THE
DISPARITY
AT
PIXEL
M
N
OF
IMAGE
X
IS
THE
RGB
VECTOR
FROM
PIXEL
M
N
OF
IMAGE
AND
X
IS
THE
RGB
VECTOR
FROM
PIXEL
M
N
OF
IMAGE
UNFORTUNATELY
IF
WE
COMPUTE
THE
MAXIMUM
LIKELIHOOD
DISPARITIES
WM
N
AT
EACH
PIXEL
SEPARATELY
THE
RESULT
IS
EXTREMELY
NOISY
FIGURE
AS
FIGURE
ILLUSTRATES
THE
CHOICE
OF
DISPARITY
IS
AMBIGUOUS
IN
REGIONS
OF
THE
IMAGE
WHERE
THERE
ARE
FEW
VISUAL
CHANGES
IN
LAYMAN
TERMS
IF
THE
NEARBY
PIXELS
ARE
ALL
SIMILAR
IT
IS
DIFFICULT
TO
ESTABLISH
WITH
CERTAINTY
WHICH
CORRESPONDS
TO
A
GIVEN
POSITION
IN
THE
OTHER
IMAGE
TO
RESOLVE
THIS
AMBIGUITY
WE
INTRODUCE
A
PRIOR
P
R
W
THAT
ENCOURAGES
PIECEWISE
SMOOTHNESS
IN
THE
DISPARITY
MAP
WE
ARE
EXPLOITING
THE
FACT
THAT
WE
KNOW
THAT
THE
SCENE
MAINLY
CONSISTS
OF
SMOOTH
SURFACES
WITH
OCCASIONAL
JUMPS
IN
DISPARITY
AT
THE
EDGE
OF
OBJECTS
ONE
POSSIBLE
APPROACH
ATTRIBUTED
ORIGINALLY
TO
OHTA
KANADE
TO
RECOV
ERING
THE
DISPARITY
IS
TO
USE
AN
INDEPENDENT
PRIOR
FOR
EACH
SCANLINE
SO
THAT
M
P
R
W
P
R
WM
M
WHERE
EACH
SCANLINE
WAS
ORGANIZED
INTO
A
CHAIN
MODEL
FIGURE
SO
THAT
N
P
R
WM
P
R
WM
P
R
WM
N
WM
N
N
THE
DISTRIBUTIONS
P
R
WM
N
WM
N
ARE
CHOSEN
SO
THAT
THEY
ALLOT
A
HIGH
PROBABILITY
WHEN
ADJACENT
DISPARITIES
ARE
THE
SAME
AN
INTERMEDIATE
PROBABILITY
WHEN
ADJACENT
DISPARITIES
CHANGE
BY
A
SINGLE
VALUE
AND
A
LOW
PROBABILITY
IF
THEY
TAKE
VALUES
THAT
ARE
MORE
WIDELY
SEPARATED
HENCE
WE
ENCOURAGE
PIECEWISE
SMOOTHNESS
MAP
INFERENCE
CAN
BE
PERFORMED
WITHIN
EACH
SCANLINE
SEPARATELY
USING
THE
DYNAMIC
PROGRAMMING
APPROACH
AND
THE
RESULTS
COMBINED
TO
FORM
THE
FULL
DISPARITY
FIELD
W
ALTHOUGH
THIS
DEFINITELY
IMPROVES
THE
FIDELITY
OF
THE
SOLUTION
IT
RESULTS
IN
A
CHARACTERISTIC
STREAKY
RESULT
FIGURE
THESE
ARTIFACTS
ARE
DUE
TO
THE
ERRONEOUS
ASSUMPTION
THAT
THE
SCANLINES
ARE
INDEPENDENT
TO
GET
AN
IMPROVED
PROBLEM
FIGURE
PICTORIAL
STRUCTURE
THIS
FACE
MODEL
CONSISTS
OF
SEVEN
PARTS
RED
DOTS
WHICH
ARE
CONNECTED
TOGETHER
IN
A
TREE
LIKE
STRUCTURE
RED
LINES
THE
POSSIBLE
POSITIONS
OF
EACH
PART
ARE
INDICATED
BY
THE
YELLOW
BOXES
AL
THOUGH
EACH
PART
CAN
TAKE
SEVERAL
HUN
DRED
PIXEL
POSITIONS
THE
MAP
PO
SITIONS
CAN
BE
INFERRED
EFFICIENTLY
BY
EXPLOITING
THE
TREE
STRUCTURE
OF
THE
GRAPH
USING
A
DYNAMIC
PROGRAMMING
APPROACH
LOCALIZING
FACIAL
FEATURES
IS
A
COMMON
ELEMENT
OF
MANY
FACE
RECOG
NITION
PIPELINES
SOLUTION
WE
SHOULD
SMOOTH
IN
THE
VERTICAL
DIRECTION
AS
WELL
BUT
THE
RESULTING
GRID
BASED
MODEL
WILL
CONTAIN
LOOPS
MAKING
MAP
INFERENCE
PROBLEMATIC
VEKSLER
ADDRESSED
THIS
PROBLEM
BY
PRUNING
THE
FULL
GRID
BASED
MODEL
UNTIL
IT
FORMED
A
TREE
EACH
EDGE
WAS
CHARACTERIZED
BY
A
COST
THAT
INCREASED
IF
THE
ASSOCIATED
PIXELS
WERE
CLOSE
TO
LARGE
CHANGES
IN
THE
IMAGE
AT
THESE
POSITIONS
EITHER
THERE
IS
TEXTURE
IN
THE
IMAGE
AND
SO
THE
DISPARITY
IS
RELATIVELY
WELL
DEFINED
OR
THERE
IS
AN
EDGE
BETWEEN
TWO
OBJECTS
IN
THE
SCENE
IN
EITHER
CASE
THERE
IS
NO
NEED
TO
APPLY
A
SMOOTHING
PRIOR
HERE
HENCE
THE
MINIMUM
SPANNING
TREE
TENDS
TO
RETAIN
EDGES
IN
REGIONS
WHERE
THEY
ARE
MOST
NEEDED
THE
MINIMUM
SPANNING
TREE
CAN
BE
COMPUTED
USING
A
STANDARD
METHOD
SUCH
AS
PRIM
ALGORITHM
SEE
CORMEN
ET
AL
THE
RESULTS
OF
MAP
INFERENCE
USING
THIS
MODEL
ARE
SHOWN
IN
FIGURE
THE
SOLUTION
IS
PIECEWISE
SMOOTH
IN
BOTH
DIRECTIONS
AND
IS
CLEARLY
SUPERIOR
TO
EITHER
THE
INDEPENDENT
PIXELS
MODEL
OR
THE
INDEPENDENT
SCANLINE
APPROACH
HOWEVER
EVEN
THIS
MODEL
IS
AN
UNNECESSARY
APPROXIMATION
WE
WOULD
IDEALLY
LIKE
THE
VARIABLES
TO
BE
FULLY
CONNECTED
IN
A
GRID
STRUCTURE
BUT
THIS
WOULD
OBVIOUSLY
CONTAIN
LOOPS
IN
CHAPTER
WE
CONSIDER
MODELS
OF
THIS
SORT
AND
RE
VISIT
STEREO
VISION
PICTORIAL
STRUCTURES
PICTORIAL
STRUCTURES
ARE
MODELS
FOR
OBJECT
CLASSES
THAT
CONSIST
OF
A
NUMBER
OF
IN
DIVIDUAL
PARTS
THAT
ARE
CONNECTED
TOGETHER
BY
SPRING
LIKE
CONNECTIONS
A
TYPICAL
EXAMPLE
WOULD
BE
A
FACE
MODEL
FIGURE
WHICH
MIGHT
CONSIST
OF
A
NOSE
EYES
AND
MOUTH
THE
SPRING
LIKE
CONNECTIONS
ENCOURAGE
THE
RELATIVE
POSITIONS
OF
THESE
FEATURES
TO
TAKE
SENSIBLE
VALUES
FOR
EXAMPLE
THE
MOUTH
IS
STRONGLY
ENCOURAGED
TO
BE
BELOW
THE
NOSE
PICTORIAL
STRUCTURES
HAVE
A
LONG
HISTORY
IN
COMPUTER
VISION
BUT
WERE
REVIVED
IN
A
MODERN
FORM
BY
FELZENSZWALB
HUTTENLOCHER
WHO
IDENTIFIED
THAT
IF
THE
CONNECTIONS
BETWEEN
PARTS
TAKE
THE
FORM
OF
AN
ACYCLIC
GRAPH
A
TREE
THEN
THEY
CAN
BE
FIT
TO
IMAGES
IN
POLYNOMIAL
TIME
THE
GOAL
OF
MATCHING
A
PICTORIAL
STRUCTURE
TO
AN
IMAGE
IS
TO
IDENTIFY
THE
POSITIONS
WN
N
OF
THE
N
PARTS
BASED
ON
DATA
XN
ASSOCIATED
WITH
EACH
FOR
EXAMPLE
A
SIMPLE
SYSTEM
MIGHT
ASSIGN
A
LIKELIHOOD
P
R
X
WN
K
THAT
IS
A
NORMAL
DISTRIBUTION
FIGURE
PICTORIAL
STRUCTURE
FOR
HUMAN
BODY
A
ORIGINAL
IMAGE
B
AFTER
BACKGROUND
SUBTRACTION
C
F
FOUR
SAMPLES
FROM
THE
POSTERIOR
DISTRIBUTION
OVER
PART
POSITIONS
EACH
PART
POSITION
IS
REPRESENTED
BY
A
RECTANGLE
OF
FIXED
ASPECT
RATIO
AND
CHARACTERIZED
BY
ITS
POSITION
SIZE
AND
ANGLE
ADAPTED
FROM
FELZENSZWALB
HUTTENLOCHER
QC
SPRINGER
OVER
A
PATCH
OF
THE
IMAGE
AT
POSITION
K
THE
RELATIVE
POSITIONS
OF
THE
PARTS
ARE
ENCODED
USING
DISTRIBUTIONS
OF
THE
FORM
P
R
WN
WPA
N
MAP
INFERENCE
IN
THIS
SYSTEM
CAN
BE
ACHIEVED
USING
A
DYNAMIC
PROGRAMMING
TECHNIQUE
FIGURE
SHOWS
A
PICTORIAL
STRUCTURE
FOR
A
FACE
THIS
MODEL
IS
SOMETHING
OF
A
COMPROMISE
IN
THAT
IT
WOULD
BE
PREFERABLE
IF
THE
FEATURES
HAD
MORE
DENSE
CONNECTIONS
FOR
EXAMPLE
THE
LEFT
EYE
PROVIDES
INFORMATION
ABOUT
THE
POSITION
OF
THE
RIGHT
EYE
AS
WELL
AS
THE
NOSE
NONETHELESS
THIS
TYPE
OF
MODEL
CAN
RELIABLY
FIND
FEATURES
ON
FRONTAL
FACES
A
SECOND
APPLICATION
IS
FOR
FITTING
ARTICULATED
MODELS
SUCH
AS
HUMAN
BODIES
FIGURE
THESE
NATURALLY
HAVE
THE
FORM
OF
A
TREE
AND
SO
THE
STRUCTURE
IS
DETERMINED
BY
THE
PROBLEM
ITSELF
FELZENSZWALB
HUTTENLOCHER
DEVELOPED
A
SYSTEM
OF
THIS
SORT
IN
WHICH
EACH
STATE
WN
REPRESENTED
A
JOINT
IN
THE
MODEL
AND
COULD
TAKE
K
POSSIBLE
VALUES
EACH
OF
WHICH
REPRESENTED
A
DIFFERENT
POSITION
AND
SHAPE
OF
AN
ASSOCIATED
RECTANGLE
THE
IMAGE
WAS
PRE
CLASSIFIED
INTO
FOREGROUND
AND
BACKGROUND
USING
A
BACK
GROUND
SUBTRACTION
TECHNIQUE
THE
LIKELIHOOD
P
R
XN
W
K
FOR
A
PARTICULAR
PART
POSITION
WAS
EVALUATED
USING
THIS
BINARY
IMAGE
IN
PARTICULAR
THE
LIKELIHOOD
WAS
CHOSEN
SO
THAT
IT
INCREASED
IF
THE
AREA
WITHIN
THE
RECTANGLE
WAS
CONSIDERED
FORE
GROUND
AND
THE
AREA
SURROUNDING
IT
WAS
CONSIDERED
BACKGROUND
UNFORTUNATELY
MAP
INFERENCE
IN
THIS
MODEL
IS
SOMEWHAT
UNRELIABLE
A
COMMON
FAILURE
MODE
IS
FOR
MORE
THAN
ONE
PART
OF
THE
BODY
TO
BECOME
ASSOCIATED
WITH
THE
SAME
PART
OF
THE
BINARY
IMAGE
THIS
IS
TECHNICALLY
POSSIBLE
AS
THE
LIMBS
MAY
OCCLUDE
EACH
OTHER
BUT
IT
CAN
ALSO
HAPPEN
ERRONEOUSLY
IF
ONE
LIMB
DOMINATES
AND
MODELS
FOR
CHAINS
AND
TREES
FIGURE
SEGMENTATION
USING
SNAKES
A
TWO
POINTS
ARE
FIXED
BUT
THE
REMAINING
POINTS
CAN
TAKE
ANY
POSITION
WITHIN
THEIR
RESPECTIVE
BOXES
THE
POSTERIOR
DISTRIBUTION
FAVORS
POSITIONS
THAT
ARE
ON
IMAGE
CONTOURS
DUE
TO
THE
LIKELIHOOD
TERM
AND
POSITIONS
THAT
ARE
CLOSE
TO
OTHER
POINTS
DUE
TO
THE
PAIRWISE
CONNECTIONS
B
RESULTS
OF
INFERENCE
C
TWO
OTHER
POINTS
ARE
CONSIDERED
FIXED
D
RESULT
OF
INFERENCE
IN
THIS
WAY
A
CLOSED
CONTOUR
IN
THE
IMAGE
IS
IDENTIFIED
ADAPTED
FROM
FELZENSZWALB
ZABIH
C
IEEE
SUPPORTS
THE
RECTANGLE
MODEL
SIGNIFICANTLY
MORE
THAN
THE
OTHERS
FELZENSZWALB
HUTTENLOCHER
DEALT
WITH
THIS
PROBLEM
BY
DRAWING
SAMPLES
FROM
THE
POSTERIOR
DISTRIBUTION
P
R
N
N
OVER
THE
POSITIONS
OF
THE
PARTS
OF
THE
MODEL
AND
USING
A
MORE
COMPLEX
CRITERION
TO
CHOOSE
THE
MOST
PROMISING
SAMPLE
SEGMENTATION
PROBLEM
IN
SECTION
WE
CONSIDERED
SEGMENTATION
AS
THE
PROBLEM
OF
LABELING
PIXELS
AC
CORDING
TO
THE
OBJECT
TO
WHICH
THEY
BELONG
A
DIFFERENT
APPROACH
TO
SEGMENTATION
IS
TO
INFER
THE
POSITION
OF
A
CLOSED
CONTOUR
THAT
DELINEATES
TWO
OBJECTS
IN
INFERENCE
THE
GOAL
IS
USUALLY
TO
INFER
THE
POSITIONS
OF
A
SET
OF
POINTS
WN
ON
THE
BOUNDARY
OF
THIS
CONTOUR
BASED
ON
THE
IMAGE
DATA
X
AS
WE
UPDATE
THESE
POINTS
DURING
AN
ATTEMPT
TO
FIND
THE
MAP
SOLUTION
THE
CONTOUR
MOVES
ACROSS
THE
IMAGE
AND
FOR
THIS
REASON
THIS
TYPE
OF
MODEL
IS
REFERRED
TO
AS
AN
ACTIVE
CONTOUR
OR
SNAKE
MODEL
FIGURE
SHOWS
AN
EXAMPLE
OF
FITTING
THIS
TYPE
OF
MODEL
AT
EACH
ITERATION
THE
POSITIONS
WN
OF
ALL
OF
THE
POINTS
EXCEPT
TWO
ARE
UPDATED
AND
CAN
TAKE
ANY
POSITION
WITHIN
SMALL
REGION
SURROUNDING
THEIR
PREVIOUS
POSITION
THE
LIKELIHOOD
OF
TAKING
A
PARTICULAR
VALUE
WN
K
IS
HIGH
AT
POSITIONS
IN
THE
IMAGE
WHERE
THE
INTENSITY
CHANGES
RAPIDLY
I
E
THE
EDGES
AND
LOW
IN
CONSTANT
REGIONS
IN
ADDITION
NEIGHBORING
POINTS
ARE
CONNECTED
AND
HAVE
AN
ATTRACTIVE
FORCE
THEY
ARE
MORE
LIKELY
TO
BE
CLOSE
TO
ONE
ANOTHER
AS
USUAL
INFERENCE
CAN
BE
CARRIED
OUT
USING
THE
DYNAMIC
PROGRAMMING
METHOD
DURING
INFERENCE
THE
POINTS
TEND
TO
BECOME
CLOSER
TOGETHER
DUE
TO
THEIR
MUTUAL
ATTRACTION
BUT
GET
STUCK
ON
THE
EDGE
OF
AN
OBJECT
IN
THE
FULL
SYSTEM
THIS
PROCESS
IS
REPEATED
BUT
WITH
A
DIFFERENT
PAIR
OF
ADJACENT
APPLICATIONS
POINTS
CHOSEN
TO
BE
FIXED
AT
EACH
STEP
HENCE
THE
DYNAMIC
PROGRAMMING
IS
A
COMPONENT
STEP
OF
A
LARGER
INFERENCE
PROBLEM
AS
THE
INFERENCE
PROCEDURE
CONTINUES
THE
CONTOUR
MOVES
ACROSS
THE
IMAGE
AND
EVENTUALLY
FIXES
ONTO
THE
BOUNDARY
OF
AN
OBJECT
FOR
THIS
REASON
THESE
MODELS
ARE
KNOWN
AS
SNAKES
OR
ACTIVE
CONTOUR
MODELS
THEY
ARE
CONSIDERED
IN
MORE
DETAIL
IN
CHAPTER
DISCUSSION
IN
THIS
CHAPTER
WE
HAVE
CONSIDERED
MODELS
BASED
ON
ACYCLIC
GRAPHS
CHAINS
AND
TREES
IN
THE
CHAPTER
WE
WILL
CONSIDER
GRID
BASED
MODELS
WHICH
CONTAIN
MANY
LOOPS
WE
WILL
SEE
THAT
MAP
INFERENCE
IS
ONLY
TRACTABLE
IN
A
FEW
SPECIAL
CASES
IN
CONTRAST
TO
THIS
CHAPTER
WE
WILL
ALSO
SEE
A
LARGE
DIFFERENCE
BETWEEN
DIRECTED
AND
UNDIRECTED
MODELS
MODELS
FOR
CHAINS
AND
TREES
NOTES
DYNAMIC
PROGRAMMING
DYNAMIC
PROGRAMMING
IS
USED
IN
MANY
VISION
ALGORITHMS
INCLUDING
THOSE
WHERE
THERE
IS
NOT
NECESSARILY
A
CLEAR
PROBABILISTIC
INTERPRETATION
IT
IS
AN
ATTRACTIVE
APPROACH
WHEN
IT
IS
APPLICABLE
BECAUSE
OF
ITS
SPEED
AND
SOME
EFFORTS
HAVE
BEEN
MADE
TO
IMPROVE
THIS
FURTHER
RAPHAEL
INTERESTING
EXAMPLES
INCLUDE
IMAGE
RETARGETING
AVIDAN
SHAMIR
CONTOUR
COMPLETION
SHA
ASHUA
ULLMAN
FITTING
OF
DEFORMABLE
TEMPLATES
AMIT
KONG
COUGHLAN
ET
AL
SHAPE
MATCHING
BASRI
ET
AL
THE
COMPUTATION
OF
SUPERPIXELS
MOORE
ET
AL
AND
SEMANTIC
LABELING
OF
SCENES
WITH
TIERED
STRUCTURE
FELZENSZWALB
VEKSLER
AS
WELL
AS
THE
APPLICATIONS
DESCRIBED
IN
THIS
CHAPTER
FELZENSZWALB
ZABIH
PROVIDE
A
RECENT
REVIEW
OF
DYNAMIC
PROGRAMMING
AND
OTHER
GRAPH
ALGORITHMS
IN
COMPUTER
VISION
STEREO
VISION
DYNAMIC
PROGRAMMING
WAS
VARIOUSLY
APPLIED
TO
STEREO
VISION
BY
BAKER
BINFORD
OHTA
KANADE
WHO
USE
A
MODEL
BASED
ON
EDGES
AND
GEIGER
ET
AL
WHO
USED
A
MODEL
BASED
ON
INTENSITIES
BIRCHFIELD
TOMASI
IMPROVED
THE
SPEED
BY
REMOVING
UNLIKELY
SEARCH
NODES
FROM
THE
DYNAMIC
PROGRAMMING
SOLUTION
AND
INTRODUCED
A
MECHANISM
THAT
MADE
DEPTH
DISCONTINUITIES
MORE
LIKELY
WHERE
THERE
WAS
INTENSITY
VARIATION
TORR
CRIMINISI
DEVELOPED
A
SYSTEM
THAT
INTEGRATED
DYNAMIC
PROGRAMMING
WITH
KNOWN
CONSTRAINTS
SUCH
AS
MATCHED
KEYPOINTS
GONG
YANG
DEVELOPED
A
DYNAMIC
PROGRAMMING
ALGORITHM
THAT
RAN
ON
A
GRAPHICS
PROCESSING
UNIT
GPU
KIM
ET
AL
INTRODUCED
A
METHOD
FOR
IDENTIFYING
DISPARITY
CANDIDATES
AT
EACH
PIXEL
USING
SPATIAL
FILTERS
AND
A
TWO
PASS
METHOD
THAT
PERFORMED
OPTIMIZATION
BOTH
ALONG
AND
ACROSS
THE
SCANLINES
VEKSLER
USED
DYNAMIC
PROGRAMMING
IN
A
TREE
TO
SOLVE
FOR
THE
WHOLE
IMAGE
AT
ONCE
AND
THIS
IDEA
HAS
SUBSEQUENTLY
BEEN
USED
IN
A
METHOD
BASED
ON
LINE
SEGMENTS
DENG
LIN
A
RECENT
QUANTITATIVE
COMPARISON
OF
DYNAMIC
PROGRAMMING
ALGORITHMS
IN
COMPUTER
VISION
CAN
BE
FOUND
IN
SALMEN
ET
AL
ALTERNATIVE
APPROACHES
TO
STEREO
VISION
WHICH
ARE
NOT
BASED
ON
DYNAMIC
PROGRAMMING
ARE
CONSIDERED
IN
CHAPTER
PICTORIAL
STRUCTURES
PICTORIAL
STRUCTURES
WERE
ORIGINALLY
INTRODUCED
BY
FISCHLER
ER
SCHLAGER
BUT
RECENT
INTEREST
WAS
STIMULATED
BY
THE
WORK
OF
FELZENSZWALB
HUT
TENLOCHER
WHO
INTRODUCED
EFFICIENT
METHODS
OF
INFERENCE
BASED
ON
DYNAMIC
PRO
GRAMMING
THERE
HAVE
BEEN
A
NUMBER
OF
ATTEMPTS
TO
IMPROVE
THE
APPEARANCE
LIKELI
HOOD
TERM
OF
THE
MODEL
KUMAR
ET
AL
EICHNER
FERRARI
ANDRILUKA
ET
AL
FELZENSZWALB
ET
AL
MODELS
THAT
DO
NOT
CONFORM
TO
A
TREE
STRUCTURE
HAVE
ALSO
BEEN
INTRODUCED
KUMAR
ET
AL
SIGAL
BLACK
REN
ET
AL
JIANG
MARTIN
AND
HERE
ALTERNATIVE
METHODS
SUCH
AS
LOOPY
PROPAGATION
MUST
BE
USED
FOR
INFERENCE
THESE
MORE
GENERAL
STRUCTURES
ARE
PARTICULARLY
IMPORTANT
FOR
ADDRESSING
PROBLEMS
ASSOCIATED
WITH
OCCLUSIONS
IN
HUMAN
BODY
MODELS
OTHER
AUTHORS
HAVE
BASED
THEIR
MODEL
ON
A
MIXTURE
OF
TREES
EVERINGHAM
ET
AL
FELZENSZWALB
ET
AL
IN
TERMS
OF
APPLICATIONS
RAMANAN
ET
AL
HAVE
DEVELOPED
A
NOTABLE
SYSTEM
FOR
TRACKING
HUMANS
IN
VIDEO
SEQUENCES
BASED
ON
PICTORIAL
STRUCTURES
EVERINGHAM
ET
AL
HAVE
DEVELOPED
A
WIDELY
USED
SYSTEM
FOR
LOCATING
FACIAL
FEATURES
AND
FELZENSZWALB
ET
AL
HAVE
PRESENTED
A
SYSTEM
THAT
IS
USED
FOR
DETECTING
MORE
GENERAL
OBJECTS
HIDDEN
MARKOV
MODELS
HIDDEN
MARKOV
MODELS
ARE
ESSENTIALLY
CHAIN
BASED
MODELS
THAT
ARE
APPLIED
TO
QUANTITIES
EVOLVING
IN
TIME
GOOD
TUTORIALS
ON
THE
SUBJECT
INCLUDING
DETAILS
OF
HOW
TO
LEARN
THEM
IN
THE
UNSUPERVISED
CASE
CAN
BE
FOUND
IN
RABINER
AND
GHAHRAMANI
THEIR
MOST
COMMON
APPLICATION
IN
VISION
IS
FOR
GESTURE
RECOGNITION
STARNER
ET
AL
RIGOLL
ET
AL
AND
SEE
MONI
ALI
FOR
A
RECENT
REVIEW
BUT
THEY
HAVE
ALSO
BEEN
USED
IN
OTHER
CONTEXTS
SUCH
AS
MODELING
INTERACTIONS
OF
PEDESTRIANS
OLIVER
ET
AL
SOME
RECENT
WORK
E
G
BOR
WANG
ET
AL
USES
A
RELATED
DIS
CRIMINATIVE
MODEL
FOR
TRACKING
OBJECTS
IN
TIME
KNOWN
AS
A
CONDITIONAL
RANDOM
FIELD
SEE
CHAPTER
SNAKES
THE
IDEA
OF
A
CONTOUR
EVOLVING
OVER
THE
SURFACE
OF
AN
IMAGE
IS
DUE
TO
KASS
ET
AL
BOTH
AMINI
ET
AL
AND
GEIGER
ET
AL
DESCRIBE
DYNAMIC
PROGRAMMING
APPROACHES
TO
THIS
PROBLEM
THESE
MODELS
ARE
CONSIDERED
FURTHER
IN
CHAPTER
BELIEF
PROPAGATION
THE
SUM
PRODUCT
ALGORITHM
KSCHISCHANG
ET
AL
IS
A
DEVEL
OPMENT
OF
EARLIER
WORK
ON
BELIEF
PROPAGATION
BY
PEARL
THE
FACTOR
GRAPH
REPRE
SENTATION
IS
DUE
TO
FREY
ET
AL
THE
USE
OF
BELIEF
PROPAGATION
FOR
FINDING
MARGINAL
POSTERIORS
AND
MAP
SOLUTIONS
IN
GRAPHS
WITH
LOOPS
HAS
BEEN
INVESTIGATED
BY
MURPHY
ET
AL
AND
WEISS
FREEMAN
RESPECTIVELY
NOTABLE
APPLICATIONS
OF
LOOPY
BELIEF
PROPAGATION
IN
VISION
INCLUDE
STEREO
VISION
SUN
ET
AL
AND
SUPER
RESOLVING
IMAGES
FREEMAN
ET
AL
MORE
INFORMATION
ABOUT
BELIEF
PROPAGATION
CAN
BE
FOUND
IN
MA
CHINE
LEARNING
TEXTBOOKS
SUCH
AS
BISHOP
BARBER
AND
KOLLER
FRIEDMAN
PROBLEMS
PROBLEM
COMPUTE
BY
HAND
THE
LOWEST
POSSIBLE
COST
FOR
TRAVERSING
THE
GRAPH
IN
FIGURE
USING
THE
DYNAMIC
PROGRAMMING
METHOD
FIGURE
DYNAMIC
PROGRAMMING
EXAMPLE
FOR
PROBLEM
PROBLEM
MAP
INFERENCE
IN
CHAIN
MODELS
CAN
ALSO
BE
PERFORMED
BY
RUNNING
DJIKSTRA
ALGORITHM
ON
THE
GRAPH
IN
FIGURE
STARTING
FROM
THE
NODE
ON
THE
LEFT
HAND
SIDE
AND
TERMINATING
WHEN
WE
FIRST
REACH
THE
NODE
ON
THE
RIGHT
HAND
SIDE
IF
THERE
ARE
N
VARIABLES
EACH
OF
WHICH
TAKES
K
VALUES
WHAT
IS
THE
BEST
AND
WORST
CASE
COMPLEXITY
OF
THE
ALGORITHM
DESCRIBE
A
SITUATION
WHERE
DJIKSTRA
ALGORITHM
OUTPERFORMS
DYNAMIC
PROGRAMMING
PROBLEM
CONSIDER
THE
GRAPHICAL
MODEL
IN
FIGURE
WRITE
OUT
THE
COST
FUNCTION
FOR
MAP
ESTIMATION
IN
THE
FORM
OF
EQUATION
DISCUSS
THE
DIFFERENCE
BETWEEN
YOUR
ANSWER
AND
EQUATION
PROBLEM
COMPUTE
THE
SOLUTION
MINIMUM
COST
PATH
TO
THE
DYNAMIC
PROGRAMMING
PROBLEM
ON
THE
TREE
IN
FIGURE
WHICH
CORRESPONDS
TO
THE
GRAPHICAL
MODEL
FROM
FIGURE
FIGURE
GRAPH
CONSTRUCTION
FOR
PROBLEM
THIS
IS
THE
SAME
AS
THE
DYNAMIC
PROGRAMMING
GRAPH
FIGURE
EXCEPT
THAT
I
THERE
ARE
TWO
EXTRA
NODES
AT
THE
START
AND
THE
END
OF
THE
GRAPH
II
THERE
ARE
NO
VERTEX
COSTS
III
THE
COSTS
ASSOCIATED
WITH
THE
LEFT
MOST
EDGES
ARE
K
AND
THE
COSTS
ASSOCIATED
WITH
THE
RIGHT
MOST
EDGES
ARE
THE
GENERAL
EDGE
COST
FOR
PASSING
FROM
LABEL
A
AND
NODE
N
TO
LABEL
B
AT
NODE
N
IS
GIVEN
BY
PN
N
A
B
UN
B
A
B
FIGURE
A
GRAPHICAL
MODEL
FOR
PROBLEM
B
GRAPHICAL
MODEL
FOR
PROBLEM
THE
UNKNOWN
VARIABLES
IN
THIS
MODEL
RECEIVE
CONNECTIONS
FROM
THE
TWO
PRECEDING
VARIABLES
AND
SO
THE
GRAPH
CONTAINS
LOOPS
PROBLEM
MAP
INFERENCE
FOR
THE
CHAIN
MODEL
CAN
BE
EXPRESSED
AS
WˆN
ARGMAX
WN
MAX
MAX
MAX
WN
N
N
LOG
P
R
XN
WN
N
LOG
P
R
WN
WN
SHOW
THAT
IT
IS
POSSIBLE
TO
COMPUTE
THIS
EXPRESSION
PIECEWISE
BY
MOVING
THE
MAXIMIZA
TION
TERMS
THROUGH
THE
SUMMATION
SEQUENCE
IN
A
MANNER
SIMILAR
TO
THAT
DESCRIBED
IN
SECTION
PROBLEM
DEVELOP
AN
ALGORITHM
THAT
CAN
COMPUTE
THE
MARGINAL
DISTRIBUTION
FOR
AN
ARBITRARY
VARIABLE
WN
IN
A
CHAIN
MODEL
NOTES
A
B
FIGURE
DYNAMIC
PROGRAMMING
EXAMPLE
FOR
PROBLEM
PROBLEM
DEVELOP
AN
ALGORITHM
THAT
COMPUTES
THE
JOINT
MARGINAL
DISTRIBUTION
OF
ANY
TWO
VARIABLES
WM
AND
WN
IN
A
CHAIN
MODEL
PROBLEM
CONSIDER
THE
FOLLOWING
TWO
DISTRIBUTIONS
OVER
THREE
VARIABLES
AND
P
R
X
X
X
Φ
X
X
Φ
X
X
Φ
X
X
P
R
X
X
X
Φ
X
X
X
DRAW
I
AN
UNDIRECTED
MODEL
AND
II
A
FACTOR
GRAPH
FOR
EACH
DISTRIBUTION
WHAT
DO
YOU
CONCLUDE
PROBLEM
CONVERT
EACH
OF
THE
GRAPHICAL
MODELS
IN
FIGURE
INTO
THE
FORM
OF
A
FACTOR
GRAPH
WHICH
OF
THE
RESULTING
FACTOR
GRAPHS
TAKE
THE
FORM
OF
A
CHAIN
A
B
C
D
FIGURE
GRAPHICAL
MODELS
FOR
PROBLEM
PROBLEM
FIGURE
SHOWS
A
CHAIN
MODEL
IN
WHICH
EACH
UNKNOWN
VARIABLE
W
DEPENDS
ON
ITS
TWO
PREDECESSORS
DESCRIBE
A
DYNAMIC
PROGRAMMING
APPROACH
TO
FINDING
MODELS
FOR
CHAINS
AND
TREES
THE
MAP
SOLUTION
HINT
YOU
NEED
TO
COMBINE
VARIABLES
IF
THERE
ARE
N
VARIABLES
IN
THE
CHAIN
AND
EACH
TAKES
K
VALUES
WHAT
IS
THE
OVERALL
COMPLEXITY
OF
YOUR
ALGORITHM
PROBLEM
IN
THE
STEREO
VISION
PROBLEM
THE
SOLUTION
WAS
VERY
POOR
WHEN
THE
PIXELS
ARE
TREATED
INDEPENDENTLY
FIGURE
SUGGEST
SOME
IMPROVEMENTS
TO
THIS
METHOD
WHILE
KEEPING
THE
PIXELS
INDEPENDENT
PROBLEM
CONSIDER
A
VARIANT
ON
THE
SEGMENTATION
APPLICATION
FIGURE
IN
WHICH
WE
UPDATE
ALL
OF
THE
CONTOUR
POSITIONS
AT
ONCE
THE
GRAPHICAL
MODEL
FOR
THIS
PROBLEM
IS
A
LOOP
I
E
A
CHAIN
WHERE
THERE
IS
ALSO
A
EDGE
BETWEEN
WN
AND
DEVISE
AN
APPROACH
TO
FINDING
THE
EXACT
MAP
SOLUTION
IN
THIS
MODEL
IF
THERE
ARE
N
VARIABLES
EACH
OF
WHICH
CAN
TAKE
K
VALUES
WHAT
IS
THE
COMPLEXITY
OF
YOUR
ALGORITHM
CHAPTER
MODELS
FOR
GRIDS
IN
CHAPTER
WE
DISCUSSED
MODELS
THAT
WERE
STRUCTURED
AS
CHAINS
OR
TREES
IN
THIS
CHAPTER
WE
CONSIDER
MODELS
THAT
ASSOCIATE
A
LABEL
WITH
EACH
PIXEL
OF
AN
IMAGE
SINCE
THE
UNKNOWN
QUANTITIES
ARE
DEFINED
ON
THE
PIXEL
LATTICE
MODELS
DEFINED
ON
A
GRID
STRUCTURE
ARE
APPROPRIATE
IN
PARTICULAR
WE
WILL
CONSIDER
GRAPHICAL
MODELS
IN
WHICH
EACH
LABEL
HAS
A
DIRECT
PROBABILISTIC
CONNECTION
TO
EACH
OF
ITS
FOUR
NEIGHBOURS
CRITICALLY
THIS
MEANS
THAT
THERE
ARE
LOOPS
IN
THE
UNDERLYING
GRAPHICAL
MODEL
AND
SO
THE
DYNAMIC
PROGRAMMING
AND
BELIEF
PROPAGATION
APPROACHES
OF
THE
PREVIOUS
CHAPTER
ARE
NO
LONGER
APPLICABLE
THESE
GRID
MODELS
ARE
PREDICATED
ON
THE
IDEA
THAT
THE
PIXEL
PROVIDES
ONLY
VERY
AMBIGUOUS
INFORMATION
ABOUT
THE
ASSOCIATED
LABEL
HOWEVER
CERTAIN
SPATIAL
CONFIG
URATIONS
OF
LABELS
ARE
KNOWN
TO
BE
MORE
COMMON
THAN
OTHERS
AND
WE
AIM
TO
EXPLOIT
THIS
KNOWLEDGE
TO
RESOLVE
THE
AMBIGUITY
IN
THIS
CHAPTER
WE
DESCRIBE
THE
RELATIVE
PREFERENCE
FOR
DIFFERENT
CONFIGURATIONS
OF
LABELS
WITH
A
PAIRWISE
MARKOV
RANDOM
FIELD
OR
MRF
AS
WE
SHALL
SEE
MAXIMUM
A
POSTERIORI
INFERENCE
FOR
PAIRWISE
MRFS
IS
TRACTABLE
IN
SOME
CIRCUMSTANCES
USING
A
FAMILY
OF
APPROACHES
KNOWN
COLLECTIVELY
AS
GRAPH
CUTS
TO
MOTIVATE
THE
GRID
MODELS
WE
INTRODUCE
A
REPRESENTATIVE
APPLICATION
IN
IMAGE
DENOISING
WE
OBSERVE
A
CORRUPTED
IMAGE
IN
WHICH
THE
INTENSITIES
AT
A
CERTAIN
PROPORTION
OF
PIXELS
HAVE
BEEN
RANDOMLY
CHANGED
TO
ANOTHER
VALUE
ACCORDING
TO
A
UNIFORM
DISTRIBUTION
FIGURE
OUR
GOAL
IS
TO
RECOVER
THE
ORIGINAL
CLEAN
IMAGE
WE
NOTE
TWO
IMPORTANT
ASPECTS
OF
THE
PROBLEM
MOST
OF
THE
PIXELS
ARE
UNCORRUPTED
SO
THE
DATA
USUALLY
TELL
US
WHICH
INTENSITY
VALUE
TO
PICK
THE
UNCORRUPTED
IMAGE
IS
MAINLY
SMOOTH
WITH
FEW
CHANGES
BETWEEN
INTENSITY
LEVELS
CONSEQUENTLY
OUR
STRATEGY
WILL
BE
TO
CONSTRUCT
A
GENERATIVE
MODEL
WHERE
THE
MAP
SOLUTION
IS
AN
IMAGE
THAT
IS
MOSTLY
THE
SAME
AS
THE
NOISY
VERSION
BUT
IS
SMOOTHER
AS
PART
OF
THIS
SOLUTION
WE
NEED
TO
DEFINE
A
PROBABILITY
DISTRIBUTION
OVER
IMAGES
THAT
FAVORS
SMOOTHNESS
IN
THIS
CHAPTER
WE
WILL
USE
A
DISCRETE
FORMULATION
OF
A
MARKOV
RANDOM
FIELD
TO
FULFIL
THIS
ROLE
MODELS
FOR
GRIDS
A
B
C
D
FIGURE
IMAGE
DENOISING
A
ORIGINAL
BINARY
IMAGE
B
OBSERVED
IMAGE
CREATED
BY
RANDOMLY
FLIPPING
THE
POLARITY
OF
A
FIXED
PROPORTION
OF
PIXELS
OUR
GOAL
IS
TO
RECOVER
THE
ORIGINAL
IMAGE
FROM
THE
CORRUPTED
ONE
C
ORIGINAL
GRAYSCALE
IMAGE
D
OBSERVED
CORRUPTED
IMAGE
IS
CREATED
BY
SETTING
A
CERTAIN
PROPORTION
OF
THE
PIXELS
TO
VALUES
DRAWN
FROM
A
UNIFORM
DISTRIBUTION
ONCE
MORE
WE
AIM
TO
RECOVER
THE
ORIGINAL
IMAGE
MARKOV
RANDOM
FIELDS
A
MARKOV
RANDOM
FIELD
IS
FORMALLY
DETERMINED
BY
A
SET
OF
SITES
N
THESE
WILL
CORRESPOND
TO
THE
N
PIXEL
LOCATIONS
A
SET
OF
RANDOM
VARIABLES
WN
N
ASSOCIATED
WITH
EACH
OF
THE
SITES
A
SET
OF
NEIGHBORS
NN
N
AT
EACH
OF
THE
N
SITES
TO
BE
A
MARKOV
RANDOM
FIELD
THE
MODEL
MUST
OBEY
THE
MARKOV
PROPERTY
P
R
WN
WS
N
P
R
WN
WNN
IN
OTHER
WORDS
THE
MODEL
SHOULD
BE
CONDITIONALLY
INDEPENDENT
OF
ALL
OF
THE
OTHER
VARIABLES
GIVEN
ITS
NEIGHBORS
THIS
PROPERTY
SHOULD
SOUND
FAMILIAR
THIS
IS
EXACTLY
HOW
CONDITIONAL
INDEPENDENCE
WORKS
IN
AN
UNDIRECTED
GRAPHICAL
MODEL
CONSEQUENTLY
WE
CAN
CONSIDER
A
MARKOV
RANDOM
FIELD
MRF
AS
AN
UNDIRECTED
MODEL
SECTION
THAT
DESCRIBES
THE
JOINT
PROBABILITY
OF
THE
VARIABLES
AS
A
PRODUCT
OF
POTENTIAL
FUNCTIONS
SO
THAT
P
R
W
TT
Φ
W
WHERE
ΦJ
IS
THE
JTH
POTENTIAL
FUNCTION
AND
ALWAYS
RETURNS
A
NON
NEGATIVE
VALUE
THIS
VALUE
DEPENDS
ON
THE
STATE
OF
THE
SUBSET
OF
VARIABLES
J
N
IN
THIS
CONTEXT
THIS
SUBSET
IS
KNOWN
AS
A
CLIQUE
THE
TERM
Z
IS
CALLED
THE
PARTITION
FUNCTION
AND
IS
A
NORMALIZING
CONSTANT
THAT
ENSURES
THAT
THE
RESULT
IS
A
VALID
PROBABILITY
DISTRIBUTION
ALTERNATIVELY
WE
CAN
REWRITE
THE
MODEL
AS
A
GIBBS
DISTRIBUTION
FIGURE
GRAPHICAL
MODEL
FOR
WORKED
MRF
EXAMPLE
THE
VARIABLES
FORM
A
GRID
THIS
IS
AN
UNDIRECTED
MODEL
WHERE
EACH
LINK
REPRESENTS
A
POTENTIAL
FUNCTION
DEFINED
OVER
THE
TWO
VARIABLES
THAT
IT
CONNECTS
EACH
POTENTIAL
RETURNS
A
POSITIVE
NUMBER
THAT
INDICATES
THE
TENDENCY
OF
THE
TWO
VARIABLES
TO
TAKE
THESE
PARTICULAR
VALUES
WHERE
Ψ
LOG
Φ
IS
KNOWN
AS
A
COST
FUNCTION
AND
RETURNS
EITHER
POSITIVE
OR
NEGATIVE
VALUES
GRID
EXAMPLE
IN
A
MARKOV
RANDOM
FIELD
EACH
POTENTIAL
FUNCTION
Φ
OR
COST
FUNCTION
Ψ
ADDRESSES
ONLY
A
SMALL
SUBSET
OF
THE
VARIABLES
IN
THIS
CHAPTER
WE
WILL
MAINLY
BE
CONCERNED
WITH
PAIRWISE
MARKOV
RANDOM
FIELDS
IN
WHICH
THE
CLIQUES
SUBSETS
CONSIST
OF
ONLY
NEIGHBORING
PAIRS
IN
A
REGULAR
GRID
STRUCTURE
TO
SEE
HOW
THE
PAIRWISE
MRF
CAN
BE
USED
TO
ENCOURAGE
SMOOTHNESS
IN
AN
IMAGE
CONSIDER
THE
GRAPHICAL
MODEL
FOR
A
IMAGE
FIGURE
HERE
WE
HAVE
DEFINED
THE
PROBABILITY
P
R
OVER
THE
ASSOCIATED
DISCRETE
STATES
AS
A
NORMALIZED
PRODUCT
OF
PAIRWISE
TERMS
P
R
W
Z
WHERE
ΦMN
WM
WN
IS
A
POTENTIAL
FUNCTION
THAT
TAKES
THE
TWO
STATES
WM
AND
WN
AND
RETURNS
A
POSITIVE
NUMBER
LET
US
CONSIDER
THE
SITUATION
WHERE
THE
WORLD
STATE
WN
AT
EACH
PIXEL
IS
BINARY
AND
SO
TAKES
A
VALUE
OF
OR
THE
FUNCTION
ΦMN
WILL
NOW
RETURN
FOUR
POSSIBLE
VALUES
DEPENDING
ON
WHICH
OF
THE
FOUR
CONFIGURATIONS
OF
WM
AND
WN
IS
PRESENT
FOR
SIMPLICITY
WE
WILL
ASSUME
THAT
THE
FUNCTIONS
AND
ARE
IDENTICAL
AND
THAT
FOR
EACH
ΦMN
ΦMN
ΦMN
ΦMN
SINCE
THERE
ARE
ONLY
FOUR
BINARY
STATES
WE
CAN
CALCULATE
THE
CONSTANT
Z
EXPLICITLY
BY
COMPUTING
THE
UN
NORMALIZED
PROBABILITIES
FOR
EACH
OF
THE
POSSIBLE
COMBINATIONS
AND
TAKING
THE
SUM
THE
RESULTING
PROBABILITIES
FOR
EACH
OF
THE
POSSIBLE
STATES
ARE
FIGURE
SAMPLES
FROM
MARKOV
RANDOM
FIELD
PRIOR
FOUR
SAMPLES
FROM
THE
MRF
PRIOR
WHICH
WERE
GENERATED
USING
A
GIBBS
SAMPLING
PROCEDURE
SEE
SECTION
EACH
SAMPLE
IS
A
BI
NARY
IMAGE
THAT
IS
SMOOTH
ALMOST
EV
ERYWHERE
THERE
ARE
ONLY
VERY
OCCA
SIONAL
CHANGES
FROM
BLACK
TO
WHITE
AND
VICE
VERSA
THIS
PRIOR
ENCOURAGES
SMOOTH
SOLUTIONS
LIKE
THE
ORIGINAL
IM
AGE
IN
THE
DENOISING
PROBLEMS
AND
DISCOURAGES
ISOLATED
CHANGES
IN
LABEL
AS
ARE
PRESENT
IN
THE
NOISE
P
R
P
R
P
R
P
R
00471
1111
PROBLEM
THE
POTENTIAL
FUNCTIONS
IN
EQUATION
ENCOURAGE
SMOOTHNESS
THE
FUNCTIONS
ΦMN
RETURN
HIGHER
VALUES
WHEN
THE
NEIGHBORS
TAKE
THE
SAME
STATE
AND
LOWER
VALUES
WHEN
THEY
DIFFER
AND
THIS
IS
REFLECTED
IN
THE
RESULTING
PROBABILITIES
WE
CAN
VISUALIZE
THIS
BY
SCALING
THIS
MODEL
UP
TO
A
LARGER
IMAGE
SIZED
GRID
WHERE
THERE
IS
ONE
NODE
PER
PIXEL
AND
DRAWING
SAMPLES
FROM
THE
RESULTING
PROBABILITY
DISTRIBUTION
FIGURE
THE
RESULTING
BINARY
IMAGES
ARE
MOSTLY
SMOOTH
WITH
ONLY
OCCASIONAL
CHANGES
BETWEEN
THE
TWO
VALUES
IT
SHOULD
BE
NOTED
THAT
FOR
THIS
MORE
REALISTICALLY
SIZED
MODEL
WE
CANNOT
COM
PUTE
THE
NORMALIZING
CONSTANT
Z
BY
BRUTE
FORCE
AS
FOR
THE
CASE
FOR
EXAMPLE
WITH
PIXELS
EACH
TAKING
A
BINARY
VALUE
THE
NORMALIZING
CONSTANT
IS
THE
SUM
OF
TERMS
IN
GENERAL
WE
WILL
HAVE
TO
COPE
WITH
ONLY
KNOWING
THE
PROBABILITIES
UP
TO
AN
UNKNOWN
SCALING
FACTOR
IMAGE
DENOISING
WITH
DISCRETE
PAIRWISE
MRFS
NOW
WE
WILL
APPLY
THE
PAIRWISE
MARKOV
RANDOM
FIELD
MODEL
TO
THE
DENOISING
TASK
OUR
GOAL
IS
TO
RECOVER
THE
ORIGINAL
IMAGE
PIXEL
VALUES
FROM
THE
OBSERVED
NOISY
IMAGE
MORE
PRECISELY
THE
OBSERVED
IMAGE
X
XN
IS
ASSUMED
TO
CON
SIST
OF
DISCRETE
VARIABLES
WHERE
THE
DIFFERENT
POSSIBLE
VALUES
LABELS
REPRESENT
DIF
FERENT
INTENSITIES
OUR
GOAL
IS
TO
RECOVER
THE
ORIGINAL
UNCORRUPTED
IMAGE
W
WN
WHICH
ALSO
CONSISTS
OF
DISCRETE
VARIABLES
REPRESENTING
THE
INTEN
SITY
WE
WILL
INITIALLY
RESTRICT
OUR
DISCUSSION
TO
GENERATIVE
MODELS
AND
COMPUTE
THE
POSTERIOR
PROBABILITY
OVER
THE
UNKNOWN
WORLD
STATE
W
USING
BAYES
RULE
P
R
N
N
N
N
P
R
XN
WN
P
R
N
P
R
N
FIGURE
DENOISING
MODEL
THE
OB
SERVED
DATA
XN
AT
PIXEL
N
IS
CONDI
TIONALLY
DEPENDENT
ON
THE
ASSOCIATED
WORLD
STATE
WN
RED
DIRECTED
EDGES
EACH
WORLD
STATE
WN
HAS
UNDIRECTED
EDGES
TO
ITS
FOUR
CONNECTED
NEIGHBORS
BLUE
UNDIRECTED
EDGES
THIS
IS
HENCE
A
MIXED
MODEL
IT
CONTAINS
BOTH
DI
RECTED
AND
UNDIRECTED
ELEMENTS
TO
GETHER
THE
WORLD
STATES
ARE
CONNECTED
IN
A
MARKOV
RANDOM
FIELD
WITH
CLIQUES
THAT
CONSIST
OF
NEIGHBORING
PAIRS
OF
VARIABLES
FOR
EXAMPLE
VARIABLE
CONTRIBUTES
TO
CLIQUES
WHERE
WE
HAVE
ASSUMED
THAT
THE
CONDITIONAL
PROBABILITY
P
R
N
N
FACTORIZES
INTO
A
PRODUCT
OF
INDIVIDUAL
TERMS
ASSOCIATED
WITH
EACH
PIXEL
WE
WILL
FIRST
CONSIDER
DENOISING
BINARY
IMAGES
IN
WHICH
THE
NOISE
PROCESS
FLIPS
THE
PIXEL
POLARITY
WITH
PROBABILITY
Ρ
SO
THAT
P
R
XN
WN
BERNXN
Ρ
P
R
XN
WN
BERNXN
Ρ
WE
SUBSEQUENTLY
CONSIDER
GRAY
LEVEL
DENOISING
WHERE
THE
OBSERVED
PIXEL
IS
REPLACED
WITH
PROBABILITY
Ρ
BY
A
DRAW
FROM
A
UNIFORM
DISTRIBUTION
WE
NOW
DEFINE
A
PRIOR
THAT
ENCOURAGES
THE
LABELS
WN
TO
BE
SMOOTH
WE
WANT
THEM
TO
MOSTLY
AGREE
WITH
THE
OBSERVED
IMAGE
BUT
TO
DISCOURAGE
CONFIGURATIONS
WITH
ISOLATED
CHANGES
IN
LABEL
TO
THIS
END
WE
MODEL
THE
PRIOR
AS
A
PAIRWISE
MRF
EACH
PAIR
OF
FOUR
CONNECTED
NEIGHBORING
PIXELS
CONTRIBUTES
ONE
CLIQUE
SO
THAT
P
R
W
N
EXP
Z
M
N
C
Ψ
WM
WN
Θ
WHERE
WE
HAVE
ASSUMED
THAT
THE
CLIQUE
COSTS
Ψ
ARE
THE
SAME
FOR
EVERY
WM
WN
THE
PARAMETERS
Θ
DEFINE
THE
COSTS
Ψ
FOR
EACH
COMBINATION
OF
NEIGHBORING
PAIRWISE
VALUES
Ψ
WM
J
WN
K
Θ
ΘJK
SO
WHEN
THE
FIRST
VARIABLE
WM
IN
THE
CLIQUE
TAKES
LABEL
J
AND
THE
SECOND
VARIABLE
WN
TAKES
LABEL
K
WE
PAY
A
PRICE
OF
ΘJK
AS
BEFORE
WE
WILL
CHOOSE
THESE
VALUES
SO
THAT
THERE
IS
A
SMALL
COST
WHEN
NEIGHBORING
LABELS
ARE
THE
SAME
SO
AND
ARE
SMALL
AND
A
LARGER
ONE
WHEN
THE
NEIGHBORING
LABELS
DIFFER
SO
AND
ARE
LARGE
THIS
HAS
THE
EFFECT
OF
ENCOURAGING
SOLUTIONS
THAT
ARE
MOSTLY
SMOOTH
THE
ASSOCIATED
GRAPHICAL
MODEL
IS
ILLUSTRATED
IN
FIGURE
IT
IS
A
MIXED
MODEL
CONTAINING
BOTH
DIRECTED
AND
UNDIRECTED
LINKS
THE
LIKELIHOOD
TERMS
EQUATION
CONTRIBUTE
THE
RED
DIRECTED
LINKS
BETWEEN
THE
OBSERVED
DATA
AND
THE
DENOISED
IMAGE
AT
EACH
PIXEL
AND
THE
MRF
PRIOR
EQUATION
CONTRIBUTES
THE
BLUE
GRID
THAT
CONNECTS
THE
PIXELS
TOGETHER
MAP
INFERENCE
FOR
BINARY
PAIRWISE
MRFS
TO
DENOISE
THE
IMAGE
WE
ESTIMATE
THE
VARIABLES
WN
N
USING
MAP
INFERENCE
WE
AIM
TO
FIND
THE
SET
OF
WORLD
STATES
WN
N
THAT
MAXIMIZES
THE
POSTERIOR
PROBABILITY
P
R
N
N
SO
THAT
N
ARGMAX
P
R
N
N
N
ARGMAX
N
N
N
R
N
P
R
XN
WN
P
R
N
WHERE
WE
HAVE
APPLIED
BAYES
RULE
AND
TRANSFORMED
TO
THE
LOG
DOMAIN
BECAUSE
THE
PRIOR
IS
AN
MRF
WITH
PAIRWISE
CONNECTIONS
WE
CAN
EXPRESS
THIS
AS
N
N
ARGMAX
LOG
P
R
XN
WN
Ψ
WM
WN
Θ
N
N
N
M
N
C
N
ARGMIN
UN
WN
PMN
WM
WN
WHERE
UN
WN
DENOTES
THE
UNARY
TERM
AT
PIXEL
N
THIS
IS
A
COST
FOR
OBSERVING
THE
DATA
AT
PIXEL
N
GIVEN
STATE
WN
AND
IS
THE
NEGATIVE
LOG
LIKELIHOOD
TERM
SIMILARLY
PMN
WM
WN
DENOTES
THE
PAIRWISE
TERM
THIS
IS
A
COST
FOR
PLACING
LABELS
WM
AND
WN
AT
NEIGHBORING
LOCATIONS
M
AND
N
AND
IS
DUE
TO
THE
CLIQUE
COSTS
Ψ
WM
WN
Θ
FROM
THE
MRF
PRIOR
NOTE
THAT
WE
HAVE
OMITTED
THE
TERM
LOG
Z
FROM
THE
MRF
DEFINITION
AS
IT
IS
CONSTANT
WITH
RESPECT
TO
THE
STATES
WN
N
AND
HENCE
DOES
NOT
AFFECT
THE
OPTIMAL
SOLUTION
THE
COST
FUNCTION
IN
EQUATION
CAN
BE
OPTIMIZED
USING
A
SET
OF
TECHNIQUES
KNOWN
COLLECTIVELY
AS
GRAPH
CUTS
WE
WILL
CONSIDER
THREE
CASES
BINARY
MRFS
I
E
WI
WHERE
THE
COSTS
FOR
DIFFERENT
COMBINATIONS
OF
ADJACENT
LABELS
ARE
SUBMODULAR
WE
WILL
EXPLAIN
WHAT
THIS
MEANS
LATER
IN
THE
CHAPTER
EXACT
MAP
INFERENCE
IS
TRACTABLE
HERE
FIGURE
MAX
FLOW
PROBLEM
WE
ARE
GIVEN
A
NETWORK
OF
VERTICES
CONNECTED
BY
DIRECTED
EDGES
EACH
OF
WHICH
HAS
A
NON
NEGATIVE
CAPACITY
CMN
THERE
ARE
TWO
SPECIAL
VERTICES
AND
T
TERMED
THE
SOURCE
AND
SINK
RESPECTIVELY
IN
THE
MAX
FLOW
PROBLEM
WE
SEEK
TO
PUSH
AS
MUCH
FLOW
FROM
SOURCE
TO
SINK
WHILE
RESPECTING
THE
CAPACITIES
OF
THE
EDGES
MULTI
LABEL
MRFS
I
E
WI
K
WHERE
THE
COSTS
ARE
SUBMODULAR
ONCE
MORE
EXACT
MAP
INFERENCE
IS
POSSIBLE
MULTI
LABEL
MRFS
WHERE
THE
COSTS
ARE
MORE
GENERAL
EXACT
MAP
INFERENCE
IS
INTRACTABLE
BUT
GOOD
APPROXIMATE
SOLUTIONS
CAN
BE
FOUND
IN
SOME
CASES
TO
SOLVE
THESE
MAP
INFERENCE
TASKS
WE
WILL
TRANSLATE
THEM
INTO
THE
FORM
OF
MAXIMUM
FLOW
OR
MAX
FLOW
PROBLEMS
MAX
FLOW
PROBLEMS
ARE
WELL
STUDIED
AND
EXACT
POLYNOMIAL
TIME
ALGORITHMS
EXIST
IN
THE
FOLLOWING
SECTION
WE
DESCRIBE
THE
MAX
FLOW
PROBLEM
AND
ITS
SOLUTION
IN
SUBSEQUENT
PARTS
OF
THE
CHAPTER
WE
DESCRIBE
HOW
TO
TRANSLATE
MAP
INFERENCE
IN
MARKOV
RANDOM
FIELDS
INTO
A
MAX
FLOW
PROBLEM
MAX
FLOW
MIN
CUT
CONSIDER
A
GRAPH
G
WITH
VERTICES
AND
DIRECTED
EDGES
CONNECTING
THEM
FIGURE
EACH
EDGE
HAS
A
NON
NEGATIVE
CAPACITY
SO
THAT
THE
EDGE
BETWEEN
VERTICES
M
AND
N
HAS
CAPACITY
CMN
TWO
OF
THE
VERTICES
ARE
TREATED
AS
SPECIAL
AND
ARE
TERMED
THE
SOURCE
AND
THE
SINK
CONSIDER
TRANSFERRING
SOME
QUANTITY
FLOW
THROUGH
THE
NETWORK
FROM
THE
SOURCE
TO
THE
SINK
THE
GOAL
OF
THE
MAX
FLOW
ALGORITHM
IS
TO
COMPUTE
THE
MAXIMUM
AMOUNT
OF
FLOW
THAT
CAN
BE
TRANSFERRED
ACROSS
THE
NETWORK
WITHOUT
EXCEEDING
ANY
OF
THE
EDGE
CAPACITIES
WHEN
THE
MAXIMUM
POSSIBLE
FLOW
IS
BEING
TRANSFERRED
THE
SO
CALLED
MAX
FLOW
SOLUTION
EVERY
PATH
FROM
SOURCE
TO
SINK
MUST
INCLUDE
A
SATURATED
EDGE
ONE
WHERE
THE
CAPACITY
IS
REACHED
IF
NOT
THEN
WE
COULD
PUSH
MORE
FLOW
DOWN
THIS
PATH
AND
SO
BY
DEFINITION
THIS
IS
NOT
THE
MAXIMUM
FLOW
SOLUTION
IT
FOLLOWS
THAT
AN
ALTERNATE
WAY
TO
THINK
ABOUT
THE
PROBLEM
IS
TO
CONSIDER
THE
EDGES
THAT
SATURATE
WE
DEFINE
A
CUT
ON
THE
GRAPH
TO
BE
A
MINIMAL
SET
OF
EDGES
THAT
SEPARATE
THE
SOURCE
FROM
THE
SINK
IN
OTHER
WORDS
WHEN
THESE
EDGES
ARE
REMOVED
THERE
IS
NO
PATH
FROM
THE
SOURCE
TO
THE
SINK
MORE
PRECISELY
A
CUT
PARTITIONS
THE
VERTICES
INTO
TWO
GROUPS
VERTICES
THAT
CAN
BE
REACHED
BY
SOME
PATH
FROM
THE
SOURCE
BUT
CANNOT
REACH
THE
SINK
AND
VERTICES
THAT
CANNOT
BE
REACHED
FROM
THE
SOURCE
BUT
CAN
REACH
THE
SINK
VIA
SOME
PATH
FOR
SHORT
WE
WILL
REFER
TO
A
CUT
AS
SEPARATING
THE
SOURCE
FROM
THE
SINK
EVERY
CUT
IS
GIVEN
AN
ASSOCIATED
COST
WHICH
IS
THE
SUM
OF
THE
CAPACITIES
OF
THE
EXCISED
EDGES
SINCE
THE
SATURATED
EDGES
IN
THE
MAX
FLOW
SOLUTION
SEPARATE
THE
SOURCE
FROM
THE
SINK
THEY
FORM
A
CUT
IN
FACT
THIS
PARTICULAR
CHOICE
OF
CUT
HAS
THE
MINIMUM
POSSIBLE
COST
AND
IS
REFERRED
TO
AS
THE
MIN
CUT
SOLUTION
HENCE
THE
MAXIMUM
FLOW
AND
MINIMUM
CUT
PROBLEMS
CAN
BE
CONSIDERED
INTERCHANGEABLY
AUGMENTING
PATHS
ALGORITHM
FOR
MAXIMUM
FLOW
THERE
ARE
MANY
ALGORITHMS
TO
COMPUTE
THE
MAXIMUM
FLOW
AND
TO
DESCRIBE
THEM
PROPERLY
IS
BEYOND
THE
SCOPE
OF
THIS
VOLUME
HOWEVER
FOR
COMPLETENESS
WE
PRESENT
A
SKETCH
OF
THE
AUGMENTING
PATHS
ALGORITHM
FIGURE
CONSIDER
CHOOSING
ANY
PATH
FROM
THE
SOURCE
TO
THE
SINK
AND
PUSHING
THE
MAX
IMUM
POSSIBLE
AMOUNT
OF
FLOW
ALONG
IT
THIS
FLOW
WILL
BE
LIMITED
BY
THE
EDGE
ON
THAT
PATH
THAT
HAS
THE
SMALLEST
CAPACITY
WHICH
WILL
DULY
SATURATE
WE
REMOVE
THIS
AMOUNT
OF
FLOW
FROM
THE
CAPACITIES
OF
ALL
OF
THE
EDGES
ALONG
THE
PATH
CAUSING
THE
SATURATED
EDGE
TO
HAVE
A
NEW
CAPACITY
OF
ZERO
WE
REPEAT
THIS
PROCEDURE
FINDING
A
SECOND
PATH
FROM
SOURCE
TO
SINK
PUSHING
AS
MUCH
FLOW
AS
POSSIBLE
ALONG
IT
AND
UPDATING
THE
CAPACITIES
WE
CONTINUE
THIS
PROCESS
UNTIL
THERE
IS
NO
PATH
FROM
SOURCE
TO
SINK
WITHOUT
AT
LEAST
ONE
SATURATED
EDGE
THE
TOTAL
FLOW
THAT
WE
HAVE
TRANSFERRED
IS
THE
MAXIMUM
FLOW
AND
THE
SATURATED
EDGES
FORM
THE
MINIMUM
CUT
IN
THE
FULL
ALGORITHM
THERE
ARE
SOME
EXTRA
COMPLICATIONS
FOR
EXAMPLE
IF
THERE
IS
ALREADY
SOME
FLOW
ALONG
EDGE
I
J
IT
MAY
BE
THAT
THERE
IS
A
REMAINING
PATH
FROM
SOURCE
TO
SINK
THAT
INCLUDES
THE
EDGE
J
I
IN
THIS
SITUATION
WE
REDUCE
THE
FLOW
IN
I
J
BEFORE
ADDING
FLOW
TO
J
I
THE
READER
SHOULD
CONSULT
A
SPECIALIZED
TEXT
ON
GRAPH
BASED
ALGORITHMS
FOR
MORE
DETAILS
IF
WE
CHOOSE
THE
PATH
WITH
THE
GREATEST
REMAINING
CAPACITY
AT
EACH
STEP
THE
ALGORITHM
IS
GUARANTEED
TO
CONVERGE
AND
HAS
COMPLEXITY
O
WHERE
IS
THE
NUMBER
OF
EDGES
AND
THE
NUMBER
OF
VERTICES
IN
THE
GRAPH
FROM
NOW
ON
WE
WILL
ASSUME
THAT
THE
MAX
FLOW
MIN
CUT
PROBLEM
CAN
BE
SOLVED
AND
CONCENTRATE
ON
HOW
TO
CONVERT
MAP
ESTIMATION
PROBLEMS
WITH
MRFS
INTO
THIS
FORM
MAP
INFERENCE
BINARY
VARIABLES
ALGORITHM
RECALL
THAT
TO
FIND
THE
MAP
SOLUTION
WE
MUST
FIND
N
ARGMIN
UN
WN
PMN
WM
WN
N
WHERE
UN
WN
DENOTES
THE
UNARY
TERM
AND
PMN
WM
WN
DENOTES
THE
PAIRWISE
TERM
FOR
PEDAGOGICAL
REASONS
WE
WILL
FIRST
CONSIDER
CASES
WHERE
THE
UNARY
TERMS
ARE
POSITIVE
AND
THE
PAIRWISE
TERMS
HAVE
THE
FOLLOWING
ZERO
DIAGONAL
FORM
PMN
PMN
PMN
PMN
WHERE
WE
DISCUSS
THE
MORE
GENERAL
CASE
LATER
IN
THIS
SECTION
T
T
SOURCE
SINK
SOURCE
SINK
T
T
SOURCE
SINK
SOURCE
SINK
F
T
T
SOURCE
SINK
SOURCE
SINK
FIGURE
AUGMENTING
PATHS
ALGORITHM
FOR
MAX
FLOW
THE
NUMBERS
AT
TACHED
TO
THE
EDGES
CORRESPOND
TO
CURRENT
FLOW
CAPACITY
A
WE
CHOOSE
ANY
PATH
FROM
SOURCE
TO
SINK
WITH
SPARE
CAPACITY
AND
PUSH
AS
MUCH
FLOW
AS
POSSIBLE
ALONG
THIS
PATH
THE
EDGE
WITH
THE
SMALLEST
CAPACITY
HERE
EDGE
T
SATURATES
B
WE
THEN
CHOOSE
ANOTHER
PATH
WHERE
THERE
IS
STILL
SPARE
CA
PACITY
AND
PUSH
AS
MUCH
FLOW
AS
POSSIBLE
NOW
EDGE
SATURATES
C
E
WE
REPEAT
THIS
UNTIL
THERE
IS
NO
PATH
FROM
SOURCE
TO
SINK
THAT
DOES
NOT
CONTAIN
A
SATURATED
EDGE
THE
TOTAL
FLOW
PUSHED
IS
THE
MAXIMUM
FLOW
F
IN
THE
MIN
CUT
PROBLEM
WE
SEEK
A
SET
OF
EDGES
THAT
SEPARATE
THE
SOURCE
FROM
THE
SINK
AND
HAVE
MINIMAL
TOTAL
CAPACITY
THE
MIN
CUT
DASHED
LINE
CONSISTS
OF
THE
SATURATED
EDGES
IN
THE
MAX
FLOW
PROBLEM
IN
THIS
EXAMPLE
THE
PATHS
WERE
CHOSEN
ARBITRARILY
BUT
TO
ENSURE
THAT
THIS
ALGORITHM
CONVERGES
IN
THE
GENERAL
CASE
WE
SHOULD
CHOOSE
THE
REMAINING
PATH
WITH
THE
GREATEST
CAPACITY
AT
EACH
STEP
FIGURE
GRAPH
STRUCTURE
FOR
FIND
ING
MAP
SOLUTION
FOR
A
MRF
WITH
BI
NARY
LABELS
AND
PAIRWISE
CONNECTIONS
IN
A
IMAGE
THERE
IS
ONE
VERTEX
PER
PIXEL
AND
NEIGHBORS
IN
THE
PIXEL
GRID
ARE
CONNECTED
BY
RECIPROCAL
PAIRS
OF
DIRECTED
EDGES
EACH
PIXEL
VERTEX
RECEIVES
A
CONNECTION
FROM
THE
SOURCE
AND
SENDS
A
CONNECTION
TO
THE
SINK
TO
SEPARATE
SOURCE
FROM
SINK
THE
CUT
MUST
INCLUDE
ONE
OF
THESE
TWO
EDGES
FOR
EACH
VERTEX
THE
CHOICE
OF
WHICH
EDGE
IS
CUT
WILL
DETERMINE
WHICH
OF
TWO
LABELS
IS
ASSIGNED
TO
THE
PIXEL
SOURCE
SINK
T
FIGURE
GRAPH
CONSTRUCTION
FOR
BINARY
MRF
WITH
DIAGONAL
PAIRWISE
TERMS
USING
SIMPLE
EXAMPLE
AFTER
THE
CUT
VERTICES
ATTACHED
TO
THE
SOURCE
ARE
GIVEN
LABEL
AND
VERTICES
ATTACHED
TO
THE
SINK
ARE
GIVEN
LABEL
WE
HENCE
ATTACH
THE
APPROPRIATE
UNARY
COSTS
TO
THE
LINKS
BETWEEN
THE
SINK
SOURCE
AND
THE
PIXEL
VERTICES
THE
PAIRWISE
COSTS
ARE
ATTACHED
TO
THE
HORIZONTAL
LINKS
BE
TWEEN
PIXELS
AS
SHOWN
THIS
ARRANGE
MENT
ENSURES
THAT
THE
CORRECT
COST
IS
PAID
FOR
EACH
OF
THE
EIGHT
POSSIBLE
SO
LUTIONS
SEE
FIGURE
THE
KEY
IDEA
WILL
BE
TO
SET
UP
A
DIRECTED
GRAPH
AND
ATTACH
WEIGHTS
TO
THE
EDGES
SO
THAT
THE
MINIMUM
CUT
ON
THIS
GRAPH
CORRESPONDS
TO
THE
MAXIMUM
A
POSTERIORI
SOLUTION
IN
PARTICULAR
WE
CONSTRUCT
A
GRAPH
WITH
ONE
VERTEX
PER
PIXEL
AND
A
PAIR
OF
DIRECTED
EDGES
BETWEEN
ADJACENT
VERTICES
IN
THE
PIXEL
GRID
IN
ADDITION
THERE
IS
A
DIRECTED
EDGE
FROM
THE
SOURCE
TO
EVERY
VERTEX
AND
A
DIRECTED
EDGE
FROM
EVERY
VERTEX
TO
THE
SINK
FIGURE
NOW
CONSIDER
A
CUT
ON
THE
GRAPH
IN
ANY
CUT
WE
MUST
EITHER
REMOVE
THE
EDGE
THAT
CONNECTS
THE
SOURCE
TO
A
PIXEL
VERTEX
OR
THE
EDGE
THAT
CONNECTS
THE
PIXEL
VERTEX
TO
THE
SINK
OR
BOTH
IF
WE
DO
NOT
DO
THIS
THEN
THERE
WILL
STILL
BE
A
PATH
FROM
SOURCE
TO
SINK
AND
IT
IS
NOT
A
VALID
CUT
FOR
THE
MINIMUM
CUT
WE
WILL
NEVER
CUT
BOTH
ASSUMING
THE
GENERAL
CASE
WHERE
THE
TWO
EDGES
HAVE
DIFFERENT
CAPACITIES
THIS
IS
UNNECESSARY
AND
WILL
INEVITABLY
INCUR
A
GREATER
COST
THAN
CUTTING
ONE
OR
THE
OTHER
WE
WILL
LABEL
PIXELS
WHERE
THE
EDGE
TO
THE
SOURCE
WAS
CUT
AS
WN
AND
PIXELS
WHERE
THE
EDGE
TO
THE
SINK
WAS
CUT
AS
HAVING
LABEL
WN
SO
EACH
PLAUSIBLE
MINIMUM
CUT
IS
ASSOCIATED
WITH
A
PIXEL
LABELING
OUR
GOAL
IS
NOW
TO
ASSIGN
CAPACITIES
TO
THE
EDGES
SO
THE
COST
OF
EACH
CUT
MATCHES
THE
COST
OF
THE
ASSOCIATED
LABELING
AS
PRESCRIBED
IN
EQUATION
FOR
SIMPLICITY
WE
ILLUSTRATE
THIS
WITH
A
IMAGE
WITH
THREE
PIXELS
FIGURE
BUT
WE
STRESS
THAT
ALL
THE
IDEAS
ARE
ALSO
VALID
FOR
IMAGES
AND
HIGHER
DIMENSIONAL
CONSTRUCTIONS
MAP
INFERENCE
FOR
BINARY
PAIRWISE
MRFS
A
B
C
D
SOLUTION
COST
E
F
SOLUTION
COST
G
SOLUTION
COST
SOLUTION
COST
H
SOLUTION
COST
SOLUTION
COST
SOLUTION
COST
SOLUTION
COST
FIGURE
EIGHT
POSSIBLE
SOLUTIONS
FOR
THREE
PIXEL
EXAMPLE
WHEN
WE
SET
THE
COSTS
AS
IN
FIGURE
EACH
SOLUTION
HAS
THE
APPROPRIATE
COST
A
FOR
EXAMPLE
THE
SOLUTION
A
B
C
REQUIRES
US
TO
CUT
EDGES
A
B
C
AND
PAY
THE
COST
UA
UB
UC
B
FOR
THE
SOLUTION
A
B
C
WE
MUST
CUT
EDGES
A
B
C
T
AND
C
B
TO
PREVENT
FLOW
THROUGH
THE
PATH
C
B
T
THIS
INCURS
A
TOTAL
COST
OF
UA
UB
UC
PBC
C
SIMILARLY
IN
THIS
EXAMPLE
WITH
A
B
C
WE
PAY
THE
APPROPRIATE
COST
UA
UB
UC
PAB
PBC
D
H
THE
OTHER
FIVE
POSSIBLE
CONFIGURATIONS
WE
ATTACH
THE
UNARY
COSTS
UN
AND
UN
TO
THE
EDGES
FROM
THE
PIXEL
TO
THE
SOURCE
AND
SINK
RESPECTIVELY
IF
WE
CUT
THE
EDGE
FROM
THE
SOURCE
TO
A
GIVEN
PIXEL
AND
HENCE
ASSIGN
WN
WE
PAY
THE
COST
UN
CONVERSELY
IF
WE
CUT
THE
EDGE
FROM
THE
PIXEL
TO
THE
SINK
AND
HENCE
ASSIGN
WN
WE
PAY
THE
COST
UN
WE
ATTACH
THE
PAIRWISE
COSTS
PMN
AND
PMN
TO
THE
PAIRS
OF
EDGES
BE
TWEEN
ADJACENT
PIXELS
NOW
IF
ONE
PIXEL
IS
ATTACHED
TO
THE
SOURCE
AND
THE
OTHER
TO
THE
SINK
WE
PAY
EITHER
PMN
OR
PMN
AS
APPROPRIATE
TO
SEPA
PROBLEM
FIGURE
GRAPH
STRUCTURE
FOR
GEN
ERAL
I
E
NON
DIAGONAL
PAIRWISE
COSTS
CONSIDER
THE
SOLUTION
A
B
WE
MUST
BREAK
THE
EDGES
A
AND
B
GIVING
A
TOTAL
COST
OF
UA
UB
PAB
FOR
THE
SOLUTION
A
B
WE
MUST
BREAK
THE
EDGES
A
T
A
B
AND
B
GIVING
A
TOTAL
COST
OF
UA
UB
PAB
SIMI
LARLY
THE
CUTS
CORRESPONDING
TO
THE
SO
LUTIONS
A
B
AND
A
B
ON
THIS
GRAPH
HAVE
PAIRWISE
COSTS
PAB
AND
PAB
RESPECTIVELY
RATE
SOURCE
FROM
SINK
THE
CUTS
CORRESPONDING
TO
ALL
EIGHT
POSSIBLE
CONFIGURATIONS
OF
THE
THREE
PIXEL
MODEL
AND
THEIR
COSTS
ARE
ILLUSTRATED
IN
FIGURE
ANY
CUT
ON
THE
GRAPH
IN
WHICH
EACH
PIXEL
IS
EITHER
SEPARATED
FROM
THE
SOURCE
OR
THE
SINK
NOW
HAS
THE
APPROPRIATE
COST
FROM
EQUATION
IT
FOLLOWS
THAT
THE
MINIMUM
CUT
ON
THIS
GRAPH
WILL
HAVE
THE
MINIMUM
COST
AND
THE
ASSOCIATED
LABELING
N
WILL
CORRESPOND
TO
THE
MAXIMUM
A
POSTERIORI
SOLUTION
GENERAL
PAIRWISE
COSTS
NOW
LET
US
CONSIDER
HOW
TO
USE
THE
MORE
GENERAL
PAIRWISE
COSTS
PMN
PMN
PMN
PMN
PROBLEM
ALGORITHM
TO
ILLUSTRATE
THIS
WE
USE
AN
EVEN
SIMPLER
GRAPH
WITH
ONLY
TWO
PIXELS
FIGURE
NOTICE
THAT
WE
HAVE
ADDED
THE
PAIRWISE
COST
PAB
TO
THE
EDGE
B
WE
WILL
HAVE
TO
PAY
THIS
COST
APPROPRIATELY
IN
THE
CONFIGURATION
WHERE
WA
AND
WB
UNFORTUNATELY
WE
WOULD
ALSO
PAY
IT
IN
THE
CASE
WHERE
WA
AND
WB
HENCE
WE
SUBTRACT
THE
SAME
COST
FROM
THE
EDGE
A
B
WHICH
MUST
ALSO
BE
CUT
IN
THIS
SOLUTION
BY
A
SIMILAR
LOGIC
WE
ADD
PAB
TO
THE
EDGE
A
T
AND
SUBTRACT
IT
FROM
EDGE
A
B
IN
THIS
WAY
WE
ASSOCIATE
THE
CORRECT
COSTS
WITH
EACH
LABELING
REPARAMETERIZATION
THE
PRECEDING
DISCUSSION
ASSUMED
THAT
THE
EDGE
COSTS
ARE
ALL
NON
NEGATIVE
AND
CAN
BE
VALID
CAPACITIES
IN
THE
MAX
FLOW
PROBLEM
IF
THEY
ARE
NOT
THEN
IT
IS
NOT
POSSIBLE
TO
COMPUTE
THE
MAP
SOLUTION
UNFORTUNATELY
IT
IS
OFTEN
THE
CASE
THAT
THEY
ARE
NEGATIVE
EVEN
IF
THE
ORIGINAL
UNARY
AND
PAIRWISE
TERMS
WERE
POSITIVE
THE
EDGE
A
B
IN
FIGURE
WITH
COST
PAB
PAB
PAB
COULD
BE
NEGATIVE
THE
SOLUTION
TO
THIS
PROBLEM
IS
REPARAMETERIZATION
THE
GOAL
OF
REPARAMETERIZATION
IS
TO
MODIFY
THE
COSTS
ASSOCIATED
WITH
THE
EDGES
IN
THE
GRAPH
IN
SUCH
A
WAY
THAT
THE
MAP
SOLUTION
IS
NOT
CHANGED
IN
PARTICULAR
WE
WILL
ADJUST
THE
EDGE
CAPACITIES
SO
THAT
EVERY
POSSIBLE
SOLUTION
HAS
A
CONSTANT
COST
ADDED
TO
IT
THIS
DOES
NOT
CHANGE
WHICH
SOLUTION
HAS
THE
MINIMUM
COST
AND
SO
THE
MAP
LABELING
WILL
BE
UNCHANGED
WE
CONSIDER
TWO
REPARAMETERIZATIONS
FIGURE
FIRST
CONSIDER
ADDING
A
CONSTANT
COST
Α
TO
THE
EDGE
FROM
A
GIVEN
PIXEL
TO
THE
SOURCE
AND
THE
EDGE
FROM
THE
SAME
PIXEL
TO
THE
SINK
SINCE
ANY
SOLUTION
CUTS
EXACTLY
ONE
OF
THESE
EDGES
THE
OVERALL
COST
OF
EVERY
SOLUTION
INCREASES
BY
Α
WE
CAN
USE
THIS
TO
ENSURE
THAT
NONE
OF
THE
EDGES
CONNECTING
THE
PIXELS
TO
THE
SOURCE
AND
SINK
HAVE
NEGATIVE
COSTS
WE
SIMPLY
ADD
A
SUFFICIENTLY
LARGE
POSITIVE
VALUE
Α
TO
MAKE
THEM
ALL
NON
NEGATIVE
A
MORE
SUBTLE
TYPE
OF
REPARAMETERIZATION
IS
ILLUSTRATED
IN
FIGURE
BY
CHANGING
THE
COSTS
IN
THIS
WAY
WE
INCREASE
THE
TOTAL
COST
OF
EACH
POSSIBLE
SOLUTION
BY
Β
FOR
EXAMPLE
IN
THE
ASSIGNMENT
WA
WB
WE
MUST
CUT
THE
LINKS
A
B
A
AND
B
T
GIVING
A
TOTAL
COST
OF
UA
UB
PAB
Β
APPLYING
THE
REPARAMETERIZATION
IN
FIGURE
TO
THE
GENERAL
CONSTRUCTION
IN
FIGURE
WE
MUST
ENSURE
THAT
THE
CAPACITIES
ON
EDGES
BETWEEN
PIXEL
NODES
ARE
NON
NEGATIVE
SO
THAT
Β
Β
ADDING
THESE
EQUATIONS
TOGETHER
WE
CAN
ELIMINATE
Β
TO
GET
A
SINGLE
INEQUALITY
IF
THIS
CONDITION
HOLDS
THE
PROBLEM
IS
TERMED
SUBMODULAR
AND
THE
GRAPH
CAN
BE
REPARAMETERIZED
TO
HAVE
ONLY
NON
NEGATIVE
COSTS
IT
CAN
THEN
BE
SOLVED
IN
POLYNOMIAL
TIME
USING
THE
MAX
FLOW
ALGORITHM
IF
THE
CONDITION
DOES
NOT
HOLD
THEN
THIS
APPROACH
CANNOT
BE
USED
AND
IN
GENERAL
THE
PROBLEM
IS
NP
HARD
FORTUNATELY
THE
FORMER
CASE
IS
COMMON
FOR
VISION
PROBLEMS
WE
GENERALLY
FAVOR
SMOOTH
SOLUTIONS
WHERE
NEIGHBORING
LABELS
ARE
THE
SAME
AND
HENCE
THE
COSTS
FOR
LABELS
DIFFERING
ARE
NATURALLY
GREATER
THAN
THE
COSTS
FOR
THE
LABELS
AGREEING
FIGURE
SHOWS
THE
MAP
SOLUTIONS
TO
THE
BINARY
DENOISING
PROBLEM
WITH
AN
MRF
PRIOR
AS
WE
INCREASE
THE
STRENGTH
OF
THE
COST
FOR
HAVING
ADJACENT
LABELS
THAT
DIFFER
HERE
WE
HAVE
ASSUMED
THAT
THE
COSTS
FOR
ADJACENT
LABELS
BEING
DIFFERENT
ARE
THE
SAME
AND
THAT
THERE
IS
NO
COST
WHEN
NEIGHBORING
LABELS
ARE
THE
SAME
WE
ARE
IN
THE
ZERO
DIAGONAL
REGIMEN
WHEN
THE
MRF
COSTS
ARE
SMALL
THE
SOLUTION
IS
DOMINATED
BY
THE
UNARY
TERMS
AND
THE
MAP
SOLUTION
LOOKS
LIKE
THE
NOISY
IMAGE
AS
THE
COSTS
INCREASE
THE
SOLUTION
CEASES
TO
TOLERATE
ISOLATED
REGIONS
AND
MOST
OF
THE
NOISE
IS
REMOVED
WHEN
THE
COSTS
BECOME
LARGER
DETAILS
SUCH
AS
THE
CENTER
OF
THE
IN
ARE
LOST
AND
EVENTUALLY
NEARBY
REGIONS
ARE
CONNECTED
TOGETHER
WITH
VERY
HIGH
PAIRWISE
COSTS
THE
MAP
SOLUTION
IS
A
UNIFORM
FIELD
OF
LABELS
THE
OVERALL
COST
IS
DOMINATED
BY
THE
PAIRWISE
TERMS
FROM
THE
MRF
AND
THE
UNARY
TERMS
MERELY
DETERMINE
THE
POLARITY
PROBLEM
A
SOURCE
B
SOURCE
C
SOURCE
A
B
A
B
A
B
SINK
T
SINK
T
SINK
T
FIGURE
REPARAMETERIZATION
A
ORIGINAL
GRAPH
CONSTRUCTION
B
REPA
RAMETERIZATION
ADDING
A
CONSTANT
COST
Α
TO
THE
CONNECTIONS
FROM
A
PIXEL
VERTEX
TO
BOTH
THE
SOURCE
AND
SINK
RESULTS
IN
A
PROBLEM
WITH
THE
SAME
MAP
SOLUTION
SINCE
WE
MUST
CUT
EITHER
BUT
NOT
BOTH
OF
THESE
EDGES
EVERY
SO
LUTION
INCREASES
IN
COST
BY
Α
AND
THE
MINIMUM
COST
SOLUTION
REMAINS
THE
SAME
C
REPARAMETERIZATION
MANIPULATING
THE
EDGE
CAPACITIES
IN
THIS
WAY
RESULTS
IN
A
CONSTANT
Β
BEING
ADDED
TO
EVERY
SOLUTION
AND
SO
THE
CHOICE
OF
MINIMUM
COST
SOLUTION
IS
UNAFFECTED
A
B
C
D
E
F
G
H
FIGURE
DENOISING
RESULTS
A
OBSERVED
NOISY
IMAGE
B
H
MAXIMUM
A
POSTERIORI
SOLUTION
AS
WE
INCREASE
ZERO
DIAGONAL
PAIRWISE
COSTS
WHEN
THE
PAIRWISE
COSTS
ARE
LOW
THE
UNARY
TERMS
DOMINATE
AND
THE
MAP
SOLUTION
IS
THE
SAME
AS
THE
OBSERVED
IMAGE
AS
THE
PAIRWISE
COSTS
INCREASE
THE
IMAGE
GETS
MORE
AND
MORE
SMOOTH
UNTIL
EVENTUALLY
IT
BECOMES
UNIFORM
MAP
INFERENCE
FOR
MULTI
LABEL
PAIRWISE
MRFS
B
FIGURE
A
GRAPH
SETUP
FOR
MULTI
LABEL
CASE
FOR
TWO
PIXELS
A
B
AND
FOUR
LABELS
THERE
IS
A
CHAIN
OF
FIVE
VERTICES
ASSOCIATED
WITH
EACH
PIXEL
THE
FOUR
VERTICAL
EDGES
BETWEEN
THESE
VERTICES
ARE
ASSIGNED
THE
UNARY
COSTS
FOR
THE
FOUR
LABELS
THE
MINIMUM
CUT
MUST
BREAK
THIS
CHAIN
TO
SEPARATE
SOURCE
FROM
SINK
AND
THE
LABEL
IS
ASSIGNED
ACCORDING
TO
WHERE
THE
CHAIN
IS
BROKEN
VERTICAL
CONSTRAINT
EDGES
OF
INFINITE
CAPACITY
RUN
BETWEEN
THE
FOUR
VERTICES
IN
THE
OPPOSITE
DIRECTION
THERE
ARE
ALSO
DIAGONAL
EDGES
BETWEEN
THE
ITH
VERTEX
OF
PIXEL
A
AND
THE
JTH
VERTEX
OF
PIXEL
B
WITH
ASSIGNED
COSTS
CAB
I
J
SEE
TEXT
B
THE
VERTICAL
CONSTRAINT
EDGES
PREVENT
SOLUTIONS
LIKE
THIS
EXAMPLE
WITH
THREE
PIXELS
HERE
THE
CHAIN
OF
VERTICES
ASSOCIATED
WITH
THE
CENTRAL
PIXEL
IS
CUT
IN
MORE
THAN
ONE
PLACE
AND
SO
THE
LABELING
HAS
NO
CLEAR
INTERPRETATION
HOWEVER
FOR
THIS
TO
HAPPEN
A
CONSTRAINT
LINK
MUST
BE
CUT
AND
HENCE
THIS
SOLUTION
HAS
AN
INFINITE
COST
MAP
INFERENCE
FOR
MULTI
LABEL
PAIRWISE
MRFS
WE
NOW
INVESTIGATE
MAP
INFERENCE
USING
MRF
PRIORS
WITH
PAIRWISE
CONNECTIONS
WHEN
THE
WORLD
STATE
WN
AT
EACH
PIXEL
CAN
TAKE
MULTIPLE
LABELS
K
TO
SOLVE
THE
MULTI
LABEL
PROBLEM
WE
CHANGE
THE
GRAPH
CONSTRUCTION
FIGURE
WITH
K
LABELS
AND
N
PIXELS
WE
INTRODUCE
K
N
VERTICES
INTO
THE
GRAPH
FOR
EACH
PIXEL
THE
K
ASSOCIATED
VERTICES
ARE
STACKED
THE
TOP
AND
BOTTOM
OF
THE
STACK
ARE
CONNECTED
TO
THE
SOURCE
AND
SINK
BY
EDGES
WITH
INFINITE
CAPACITY
BETWEEN
THE
K
VERTICES
IN
THE
STACK
ARE
K
EDGES
FORMING
A
PATH
FROM
SOURCE
TO
SINK
THESE
EDGES
ARE
ASSOCIATED
WITH
THE
K
UNARY
COSTS
UN
UN
K
TO
SEPARATE
THE
SOURCE
FROM
THE
SINK
WE
MUST
CUT
AT
LEAST
ONE
OF
THE
K
EDGES
IN
THIS
CHAIN
WE
WILL
INTERPRET
A
CUT
AT
THE
KTH
EDGE
IN
THIS
CHAIN
AS
INDICATING
THAT
THE
PIXEL
TAKES
LABEL
K
AND
THIS
INCURS
THE
APPROPRIATE
COST
OF
UN
K
ALGORITHM
MODELS
FOR
GRIDS
COST
COST
FIGURE
EXAMPLE
CUTS
FOR
MULTI
LABEL
CASE
TO
SEPARATE
THE
SOURCE
AND
SINK
WE
MUST
CUT
ALL
OF
THE
LINKS
THAT
PASS
FROM
ABOVE
THE
CHOSEN
LABEL
FOR
PIXEL
A
TO
BELOW
THE
CHOSEN
LABEL
FOR
PIXEL
B
A
PIXEL
A
IS
SET
TO
LABEL
AND
PIXEL
B
IS
SET
TO
LABEL
MEANING
WE
MUST
CUT
THE
LINKS
FROM
VERTEX
TO
NODES
AND
B
PIXEL
A
TAKES
LABEL
AND
PIXEL
B
TAKES
LABEL
PROBLEM
PROBLEM
PROBLEM
TO
ENSURE
THAT
ONLY
A
SINGLE
EDGE
FROM
THE
CHAIN
IS
PART
OF
THE
MINIMUM
CUT
AND
HENCE
THAT
EACH
CUT
CORRESPONDS
TO
ONE
VALID
LABELING
WE
ADD
CONSTRAINT
EDGES
THESE
ARE
EDGES
OF
INFINITE
CAPACITY
THAT
ARE
STRATEGICALLY
PLACED
TO
PREVENT
CERTAIN
CUTS
OCCURRING
IN
THIS
CASE
THE
CONSTRAINT
EDGES
CONNECT
THE
VERTICES
BACKWARDS
ALONG
EACH
CHAIN
ANY
CUT
THAT
CROSSES
THE
CHAIN
MORE
THAN
ONCE
MUST
CUT
ONE
OF
THESE
EDGES
AND
WILL
NEVER
BE
THE
MINIMUM
CUT
SOLUTION
FIGURE
IN
FIGURE
THERE
ARE
ALSO
DIAGONAL
INTER
PIXEL
EDGES
FROM
THE
VERTICES
ASSOCIATED
WITH
PIXEL
A
TO
THOSE
ASSOCIATED
WITH
PIXEL
B
THESE
ARE
ASSIGNED
COSTS
CAB
I
J
WHERE
I
INDEXES
THE
VERTEX
ASSOCIATED
WITH
PIXEL
A
AND
J
INDEXES
THE
VERTEX
ASSOCIATED
WITH
PIXEL
B
WE
CHOOSE
THE
EDGE
COSTS
TO
BE
CAB
I
J
PAB
I
J
PAB
I
J
PAB
I
J
PAB
I
J
MAP
INFERENCE
FOR
MULTI
LABEL
PAIRWISE
MRFS
FIGURE
REPARAMETERIZATION
FOR
MULTI
LABEL
GRAPH
CUTS
THE
ORIGINAL
CONSTRUCTION
A
IS
EQUIVALENT
TO
CONSTRUCTION
B
THE
LABEL
AT
PIXEL
B
DETER
MINES
WHICH
EDGES
THAT
LEAVE
NODE
ARE
CUT
HENCE
WE
CAN
REMOVE
THESE
EDGES
AND
ADD
THE
EXTRA
COSTS
TO
THE
VERTICAL
LINKS
ASSOCIATED
WITH
PIXEL
B
SIMILARLY
THE
COSTS
OF
THE
EDGES
PASSING
INTO
NODE
CAN
BE
ADDED
TO
THE
VERTICAL
EDGES
ASSOCIATED
WITH
PIXEL
A
IF
ANY
OF
THE
RESULTING
VERTICAL
EDGES
ASSOCIATED
WITH
A
PIXEL
ARE
NEGATIVE
WE
CAN
ADD
A
CONSTANT
Α
TO
EACH
SINCE
EXACTLY
ONE
IS
BROKEN
THE
TOTAL
COST
INCREASES
BY
Α
BUT
THE
MAP
SOLUTION
REMAINS
THE
SAME
WHERE
WE
DEFINE
ANY
SUPERFLUOUS
PAIRWISE
COSTS
ASSOCIATED
WITH
THE
NON
EXISTENT
LABELS
OR
K
TO
BE
ZERO
SO
THAT
PAB
I
PAB
I
K
I
K
PAB
J
PAB
K
J
J
K
WHEN
LABEL
I
IS
ASSIGNED
TO
PIXEL
A
AND
LABEL
J
TO
PIXEL
B
WE
MUST
CUT
ALL
OF
THE
LINKS
FROM
VERTICES
AI
TO
THE
VERTICES
BJ
BK
TO
SEPARATE
THE
SOURCE
FROM
THE
SINK
FIGURE
SO
THE
TOTAL
COST
DUE
TO
THE
INTER
PIXEL
EDGES
FOR
ASSIGNING
LABEL
I
TO
PIXEL
A
AND
LABEL
J
TO
PIXEL
B
IS
PROBLEM
K
K
CAB
I
J
PAB
I
J
PAB
I
J
PAB
I
J
PAB
I
J
I
J
J
I
J
J
PAB
I
J
PAB
J
PAB
I
K
PAB
K
PAB
I
J
MODELS
FOR
GRIDS
FIGURE
SUBMODULARITY
CON
STRAINT
FOR
MULTI
LABEL
CASE
COLOR
AT
POSITION
M
N
INDICATES
PAIRWISE
COSTS
PAB
M
N
FOR
ALL
EDGES
IN
THE
GRAPH
TO
BE
POSITIVE
WE
REQUIRE
THAT
THE
PAIRWISE
TERMS
OBEY
PAB
Β
Γ
PAB
Α
Δ
PAB
Β
Δ
PAB
Α
Γ
FOR
ALL
Α
Β
Γ
Δ
SUCH
THAT
Β
Α
AND
Δ
Γ
IN
OTHER
WORDS
FOR
ANY
FOUR
POSITIONS
ARRANGED
IN
A
SQUARE
CONFIGURATION
AS
IN
THE
FIGURE
THE
SUM
OF
THE
TWO
COSTS
ON
THE
DIAGONAL
FROM
TOP
LEFT
TO
BOTTOM
RIGHT
MUST
BE
LESS
THAN
THE
SUM
ON
THE
OFF
DIAGONAL
IF
THIS
CONDITION
HOLDS
THE
PROBLEM
CAN
BE
SOLVED
IN
POLYNOMIAL
TIME
LABEL
ADDING
THE
UNARY
TERMS
THE
TOTAL
COST
IS
UA
I
UB
J
PAB
I
J
AS
REQUIRED
ONCE
MORE
WE
HAVE
IMPLICITLY
MADE
THE
ASSUMPTION
THAT
THE
COSTS
ASSOCIATED
WITH
EDGES
ARE
NON
NEGATIVE
IF
THE
VERTICAL
INTRA
PIXEL
EDGE
TERMS
HAVE
NEGATIVE
COSTS
IT
IS
POSSIBLE
TO
REPARAMETERIZE
THE
GRAPH
BY
ADDING
A
CONSTANT
Α
TO
ALL
OF
THE
UNARY
TERMS
SINCE
THE
FINAL
COST
INCLUDES
EXACTLY
ONE
UNARY
TERM
PER
PIXEL
EVERY
POSSIBLE
SOLUTION
INCREASES
BY
Α
AND
THE
MAP
SOLUTION
IS
UNAFFECTED
THE
DIAGONAL
INTER
PIXEL
EDGES
ARE
MORE
PROBLEMATIC
IT
IS
POSSIBLE
TO
REMOVE
THE
EDGES
THAT
LEAVE
NODE
AND
THE
EDGES
THAT
ARRIVE
AT
BK
BY
ADDING
TERMS
TO
THE
INTRA
PIXEL
EDGES
ASSOCIATED
WITH
THE
UNARY
TERMS
FIGURE
THESE
INTRA
PIXEL
EDGES
CAN
THEN
BE
REPARAMETERIZED
AS
DESCRIBED
ABOVE
IF
NECESSARY
UNFORTUNATELY
WE
CAN
NEITHER
REMOVE
NOR
REPARAMETERIZE
THE
REMAINING
INTER
PIXEL
EDGES
SO
WE
REQUIRE
THAT
CAB
I
J
PAB
I
J
PAB
I
J
PAB
I
J
PAB
I
J
BY
MATHEMATICAL
INDUCTION
WE
GET
THE
MORE
GENERAL
RESULT
FIGURE
PAB
Β
Γ
PAB
Α
Δ
PAB
Β
Δ
PAB
Α
Γ
WHERE
Α
Β
Γ
Δ
ARE
ANY
FOUR
VALUES
OF
THE
STATE
Y
SUCH
THAT
Β
Α
AND
Δ
Γ
THIS
IS
THE
MULTI
LABEL
GENERALIZATION
OF
THE
SUBMODULARITY
CONDITION
EQUATION
AN
IMPORTANT
CLASS
OF
PAIRWISE
COSTS
THAT
ARE
SUBMODULAR
ARE
THOSE
THAT
ARE
CONVEX
IN
THE
ABSOLUTE
DIFFERENCE
WI
WJ
BETWEEN
THE
LABELS
AT
ADJACENT
PIXELS
FIGURE
HERE
SMOOTHNESS
IS
ENCOURAGED
THE
PENALTY
BECOMES
INCREASINGLY
STRINGENT
AS
THE
JUMPS
BETWEEN
LABELS
INCREASE
MULTI
LABEL
MRFS
WITH
NON
CONVEX
POTENTIALS
UNFORTUNATELY
CONVEX
POTENTIALS
ARE
NOT
ALWAYS
APPROPRIATE
FOR
EXAMPLE
IN
THE
DENOISING
TASK
WE
MIGHT
EXPECT
THE
IMAGE
TO
BE
PIECEWISE
SMOOTH
THERE
ARE
SMOOTH
MULTI
LABEL
MRFS
WITH
NON
CONVEX
POTENTIALS
A
B
C
FIGURE
CONVEX
VS
NON
CONVEX
POTENTIALS
THE
METHOD
FOR
MAP
IN
FERENCE
FOR
MULTI
VALUED
VARIABLES
DEPENDS
ON
WHETHER
THE
COSTS
ARE
A
CONVEX
OR
NON
CONVEX
FUNCTION
OF
THE
DIFFERENCE
IN
LABELS
A
QUADRATIC
FUNCTION
CONVEX
PMN
WM
WN
Κ
WM
WN
FOR
CONVEX
FUNCTIONS
IT
IS
POSSIBLE
TO
DRAW
A
CHORD
BETWEEN
ANY
TWO
POINTS
ON
THE
FUNCTION
WITHOUT
INTERSECTING
THE
FUNCTION
ELSEWHERE
E
G
DOTTED
BLUE
LINE
B
TRUNCATED
QUADRATIC
FUNC
TION
NON
CONVEX
PMN
WM
WN
MIN
WM
WN
C
POTTS
MODEL
NON
CONVEX
PMN
WM
WN
Κ
Δ
WM
WN
FIGURE
DENOISING
RESULTS
WITH
CONVEX
QUADRATIC
PAIRWISE
COSTS
A
NOISY
OBSERVED
IMAGE
B
DENOISED
IMAGE
HAS
ARTIWFACTS
WHERE
THERE
ARE
LARGE
INTENSITY
CHANGES
IN
THE
ORIGINAL
IMAGE
CONVEX
COSTS
IMPLY
THAT
THERE
IS
A
LOWER
COST
FOR
A
NUMBER
OF
SMALL
CHANGES
RATHER
THAN
A
SINGLE
LARGE
ONE
REGIONS
CORRESPONDING
TO
OBJECTS
FOLLOWED
BY
ABRUPT
JUMPS
CORRESPONDING
TO
THE
BOUNDARIES
BETWEEN
OBJECTS
A
CONVEX
POTENTIAL
FUNCTION
CANNOT
DESCRIBE
THIS
SITUATION
BECAUSE
IT
PENALIZES
LARGE
JUMPS
MUCH
MORE
THAN
SMALLER
ONES
THE
RESULT
IS
THAT
THE
MAP
SOLUTION
SMOOTHS
OVER
THE
SHARP
EDGES
CHANGING
THE
LABEL
BY
SEVERAL
SMALLER
AMOUNTS
RATHER
THAN
ONE
LARGE
JUMP
FIGURE
TO
SOLVE
THIS
PROBLEM
WE
NEED
TO
WORK
WITH
INTERACTIONS
THAT
ARE
NON
CONVEX
IN
THE
ABSOLUTE
LABEL
DIFFERENCE
SUCH
AS
THE
TRUNCATED
QUADRATIC
FUNCTION
OR
THE
PROBLEM
FIGURE
THE
ALPHA
EXPANSION
AL
GORITHM
BREAKS
THE
PROBLEM
DOWN
INTO
A
SERIES
OF
BINARY
SUB
PROBLEMS
AT
EACH
STEP
WE
CHOOSE
A
LABEL
Α
AND
WE
EXPAND
FOR
EACH
PIXEL
WE
EITHER
LEAVE
THE
LABEL
AS
IT
IS
OR
REPLACE
IT
WITH
Α
THIS
SUB
PROBLEM
IS
SOLVED
IN
SUCH
A
WAY
THAT
IT
IS
GUARANTEED
TO
DECREASE
THE
MULTILABEL
COST
FUNCTION
A
INITIAL
LABELING
B
ORANGE
LABEL
IS
EXPANDED
EACH
LABEL
STAYS
THE
SAME
OR
BECOMES
ORANGE
C
YELLOW
LABEL
IS
EXPANDED
D
RED
LABEL
IS
EXPANDED
A
B
C
D
POTTS
MODEL
FIGURES
C
THESE
FAVOR
SMALL
CHANGES
IN
THE
LABEL
AND
PENALIZE
LARGE
CHANGES
EQUALLY
OR
NEARLY
EQUALLY
THIS
REFLECTS
THE
FACT
THAT
THE
EXACT
SIZE
OF
AN
ABRUPT
JUMP
IN
LABEL
IS
RELATIVELY
UNIMPORTANT
UNFORTUNATELY
THESE
PAIRWISE
COSTS
DO
NOT
SATISFY
THE
SUBMODULARITY
CONSTRAINT
EQUATION
HERE
THE
MAP
SOLUTION
CANNOT
IN
GENERAL
BE
FOUND
EXACTLY
WITH
THE
METHOD
DESCRIBED
PREVIOUSLY
AND
THE
PROBLEM
IS
NP
HARD
FORTUNATELY
THERE
ARE
GOOD
APPROXIMATE
METHODS
FOR
OPTIMIZING
SUCH
PROBLEMS
ONE
OF
WHICH
IS
THE
ALPHA
EXPANSION
ALGORITHM
INFERENCE
ALPHA
EXPANSION
ALGORITHM
THE
ALPHA
EXPANSION
ALGORITHM
WORKS
BY
BREAKING
THE
SOLUTION
DOWN
INTO
A
SERIES
OF
BINARY
PROBLEMS
EACH
OF
WHICH
CAN
BE
SOLVED
EXACTLY
AT
EACH
ITERATION
WE
CHOOSE
ONE
LABEL
VALUE
Α
AND
FOR
EACH
PIXEL
WE
CONSIDER
EITHER
RETAINING
THE
CURRENT
LABEL
OR
SWITCHING
IT
TO
Α
THE
NAME
ALPHA
EXPANSION
DERIVES
FROM
THE
FACT
THAT
THE
SPACE
OCCUPIED
BY
LABEL
Α
IN
THE
SOLUTION
EXPANDS
AT
EACH
ITERATION
FIGURE
THE
PROCESS
IS
ITERATED
UNTIL
NO
CHOICE
OF
Α
CAUSES
ANY
CHANGE
EACH
EXPANSION
MOVE
IS
GUARANTEED
TO
LOWER
THE
OVERALL
OBJECTIVE
FUNCTION
ALTHOUGH
THE
FINAL
RESULT
IS
NOT
GUARANTEED
TO
BE
THE
GLOBAL
MINIMUM
FOR
THE
ALPHA
EXPANSION
ALGORITHM
TO
WORK
WE
REQUIRE
THAT
THE
EDGE
COSTS
FORM
A
METRIC
IN
OTHER
WORDS
WE
REQUIRE
THAT
P
Α
Β
Α
Β
P
Α
Β
P
Β
Α
P
Α
Β
P
Α
Γ
P
Γ
Β
THESE
ASSUMPTIONS
ARE
REASONABLE
FOR
MANY
APPLICATIONS
IN
VISION
AND
ALLOW
US
TO
MODEL
NON
CONVEX
PRIORS
IN
THE
ALPHA
EXPANSION
GRAPH
CONSTRUCTION
FIGURE
THERE
IS
ONE
VERTEX
ASSOCIATED
WITH
EACH
PIXEL
EACH
OF
THESE
VERTICES
IS
CONNECTED
TO
THE
SOURCE
REP
RESENTING
KEEPING
THE
ORIGINAL
LABEL
OR
Α
AND
THE
SINK
REPRESENTING
THE
LABEL
Α
TO
SEPARATE
SOURCE
FROM
SINK
WE
MUST
CUT
ONE
OF
THESE
TWO
EDGES
AT
EACH
PIXEL
THE
CHOICE
OF
EDGE
WILL
DETERMINE
WHETHER
WE
KEEP
THE
ORIGINAL
LABEL
OR
SET
IT
TO
FIGURE
ALPHA
EXPANSION
GRAPH
SETUP
EACH
PIXEL
NODE
A
B
C
D
E
IS
CONNECTED
TO
THE
SOURCE
AND
THE
SINK
BY
EDGES
WITH
COSTS
U
Α
AND
U
Α
RESPECTIVELY
IN
THE
MINIMUM
CUT
EXACTLY
ONE
OF
THESE
LINKS
WILL
BE
CUT
THE
NODES
AND
VERTICES
DESCRIBING
THE
RELATIONSHIP
BETWEEN
NEIGHBORING
PIXELS
DEPEND
ON
THEIR
CURRENT
LABELS
WHICH
MAY
BE
Α
Α
AS
FOR
PIXELS
A
AND
B
Α
Β
AS
FOR
PIXELS
B
AND
C
Β
Β
AS
FOR
PIXELS
C
AND
D
OR
Β
Γ
AS
FOR
PIXELS
D
AND
E
FOR
THE
LAST
CASE
AN
AUXILIARY
NODE
K
MUST
BE
ADDED
TO
THE
GRAPH
Α
ACCORDINGLY
WE
ASSOCIATE
THE
UNARY
COSTS
FOR
EACH
EDGE
BEING
SET
TO
Α
OR
ITS
ORIGINAL
LABEL
WITH
THE
TWO
LINKS
FROM
EACH
PIXEL
IF
THE
PIXEL
ALREADY
HAS
LABEL
Α
THEN
WE
SET
THE
COST
OF
BEING
SET
TO
Α
TO
THE
REMAINING
STRUCTURE
OF
THE
GRAPH
IS
DYNAMIC
IT
CHANGES
AT
EACH
ITERATION
DEPENDING
ON
THE
CHOICE
OF
Α
AND
THE
CURRENT
LABELS
THERE
ARE
FOUR
POSSIBLE
RELATIONSHIPS
BETWEEN
ADJACENT
PIXELS
PIXEL
I
HAS
LABEL
Α
AND
THE
PIXEL
J
HAS
LABEL
Α
HERE
THE
FINAL
CONFIGURATION
IS
INEVITABLY
Α
Α
AND
SO
THE
PAIRWISE
COST
IS
ZERO
AND
THERE
IS
NO
NEED
TO
ADD
FURTHER
EDGES
CONNECTING
NODES
I
AND
J
IN
THE
GRAPH
PIXELS
A
AND
B
IN
FIGURE
HAVE
THIS
RELATIONSHIP
THE
FIRST
PIXEL
HAS
LABEL
Α
BUT
THE
SECOND
PIXEL
HAS
A
DIFFERENT
LABEL
Β
HERE
THE
FINAL
SOLUTION
MAY
BE
Α
Α
WITH
ZERO
PAIRWISE
COST
OR
Α
Β
WITH
PAIRWISE
COST
PIJ
Α
Β
HERE
WE
ADD
A
SINGLE
EDGE
CONNECTING
PIXEL
J
TO
PIXEL
I
WITH
PAIRWISE
COST
PIJ
Α
Β
PIXELS
B
AND
C
IN
FIGURE
HAVE
THIS
RELATIONSHIP
BOTH
PIXELS
I
AND
J
HAVE
THE
SAME
LABEL
Β
HERE
THE
FINAL
SOLUTION
MAY
BE
Α
Α
WITH
ZERO
PAIRWISE
COST
Β
Β
WITH
ZERO
PAIRWISE
COST
Α
Β
WITH
PAIRWISE
COST
PIJ
Α
Β
OR
Β
Α
WITH
PAIRWISE
COST
PIJ
Β
Α
WE
ADD
TWO
EDGES
BETWEEN
THE
PIXELS
REPRESENTING
THE
TWO
NON
ZERO
PAIRWISE
COSTS
PIXELS
C
AND
D
IN
FIGURE
HAVE
THIS
RELATIONSHIP
PIXEL
I
HAS
LABEL
Β
AND
PIXEL
J
HAS
A
SECOND
LABEL
Γ
HERE
THE
FINAL
SOLUTION
MAY
BE
Α
Α
WITH
ZERO
PAIRWISE
COST
Β
Γ
WITH
PAIRWISE
COST
PIJ
Β
Γ
Β
Α
PROBLEM
WITH
PAIRWISE
COST
PIJ
Β
Α
OR
Α
Γ
WITH
PAIRWISE
COST
PIJ
Α
Γ
WE
ADD
A
NEW
VERTEX
K
BETWEEN
VERTICES
I
AND
J
AND
ADD
THE
THREE
NON
ZERO
PAIRWISE
COSTS
TO
EDGES
K
Α
I
K
AND
J
K
RESPECTIVELY
PIXELS
D
AND
E
IN
FIGURE
HAVE
THIS
RELATIONSHIP
THREE
EXAMPLE
CUTS
ARE
SHOWN
IN
FIGURE
NOTE
THAT
THIS
CONSTRUCTION
CRITICALLY
RELIES
ON
THE
TRIANGLE
INEQUALITY
EQUA
TION
FOR
EXAMPLE
CONSIDER
PIXELS
D
AND
E
IN
FIGURE
IF
THE
TRIANGLE
INEQUALITY
DOES
NOT
HOLD
SO
THAT
PDE
Β
Γ
PDE
Β
Α
PDE
Α
Γ
THEN
THE
WRONG
COSTS
WILL
BE
ASSIGNED
RATHER
THAN
THE
LINK
K
Α
THE
TWO
LINKS
D
K
AND
E
K
WILL
BOTH
BE
CUT
AND
THE
WRONG
COST
WILL
BE
ASSIGNED
IN
PRACTICE
IT
IS
SOMETIMES
POSSIBLE
TO
IGNORE
THIS
CONSTRAINT
BY
TRUNCATING
THE
OFFENDING
COST
PIJ
Β
Γ
AND
RUNNING
THE
ALGORITHM
AS
NORMAL
AFTER
THE
CUT
IS
DONE
THE
TRUE
OBJECTIVE
FUNCTION
SUM
OF
THE
UNARY
AND
PAIRWISE
COSTS
CAN
BE
COMPUTED
FOR
THE
NEW
LABEL
MAP
AND
THE
ANSWER
ACCEPTED
IF
THE
COST
HAS
DECREASED
IT
SHOULD
BE
EMPHASIZED
THAT
ALTHOUGH
EACH
STEP
OPTIMALLY
UPDATES
THE
OBJECTIVE
FUNCTION
WITH
RESPECT
TO
EXPANDING
Α
THIS
ALGORITHM
IS
NOT
GUARANTEED
TO
CONVERGE
TO
THE
OVERALL
GLOBAL
MINIMUM
HOWEVER
IT
CAN
BE
PROVEN
THAT
THE
RESULT
IS
WITHIN
A
FACTOR
OF
TWO
OF
THE
MINIMUM
AND
OFTEN
IT
BEHAVES
MUCH
BETTER
FIGURE
SHOWS
AN
EXAMPLE
OF
MULTI
LABEL
DENOISING
USING
THE
ALPHA
EXPANSION
ALGORITHM
ON
EACH
ITERATION
ONE
OF
THE
LABELS
IS
CHOSEN
AND
EXPANDS
AND
THE
AP
PROPRIATE
REGION
IS
DENOISED
SOMETIMES
THE
LABEL
IS
NOT
SUPPORTED
AT
ALL
BY
THE
UNARY
COSTS
AND
NOTHING
HAPPENS
THE
ALGORITHM
TERMINATES
WHEN
NO
CHOICE
OF
Α
CAUSES
ANY
FURTHER
CHANGE
CONDITIONAL
RANDOM
FIELDS
IN
THE
MODELS
PRESENTED
IN
THIS
CHAPTER
THE
MARKOV
RANDOM
FIELDS
HAVE
DESCRIBED
THE
PRIOR
P
R
W
IN
A
GENERATIVE
MODEL
OF
THE
IMAGE
DATA
WE
COULD
ALTERNATIVELY
DESCRIBE
THE
JOINT
PROBABILITY
DISTRIBUTION
P
R
W
X
WITH
THE
UNDIRECTED
MODEL
P
R
W
X
EXP
Ψ
Z
C
C
W
Ζ
W
X
WHERE
THE
FUNCTIONS
Ψ
ENCOURAGE
CERTAIN
CONFIGURATIONS
OF
THE
LABEL
FIELD
AND
THE
FUNCTIONS
Ζ
ENCOURAGE
AGREEMENT
BETWEEN
THE
DATA
AND
THE
LABEL
FIELD
IF
WE
NOW
CONDITION
ON
THE
DATA
I
E
ASSUME
THAT
IT
IS
FIXED
THEN
WE
CAN
USE
THE
RELATION
P
R
W
X
P
R
W
X
TO
WRITE
P
R
W
X
EXP
R
Ψ
W
Ζ
W
X
WHERE
Z
P
R
X
THIS
DISCRIMINATIVE
MODEL
IS
KNOWN
AS
A
CONDITIONAL
RANDOM
FIELD
OR
CRF
WE
CAN
CHOOSE
THE
FUNCTIONS
Ζ
SO
THAT
THEY
EACH
DETERMINE
THE
COMPATIBILITY
OF
ONE
LABEL
WN
TO
ITS
ASSOCIATED
MEASUREMENT
XN
IF
THE
FUNCTIONS
Ψ
ARE
USED
TO
A
SOURCE
A
B
COST
SINK
B
SOURCE
A
B
COST
SINK
C
SOURCE
A
B
C
C
C
D
D
D
STATES
BEFORE
K
E
STATES
AFTER
STATES
BEFORE
K
E
STATES
AFTER
STATES
BEFORE
K
E
COST
SINK
STATES
AFTER
FIGURE
ALPHA
EXPANSION
ALGORITHM
A
C
EXAMPLE
CUTS
ON
THIS
GRAPH
ILLUSTRATE
THAT
THE
APPROPRIATE
UNARY
AND
PAIRWISE
COSTS
ARE
ALWAYS
PAID
FIGURE
ALPHA
EXPANSION
ALGORITHM
FOR
DENOISING
TASK
A
OBSERVED
NOISY
IMAGE
B
LABEL
BLACK
IS
EXPANDED
REMOVING
NOISE
FROM
THE
HAIR
C
F
SUBSEQUENT
ITERATIONS
IN
WHICH
THE
LABELS
CORRESPONDING
TO
THE
BOOTS
TROUSERS
SKIN
AND
BACKGROUND
ARE
EXPANDED
RESPECTIVELY
ENCOURAGE
SMOOTHNESS
BETWEEN
NEIGHBORING
LABELS
THEN
THE
NEGATIVE
LOG
POSTERIOR
PROBABILITY
WILL
AGAIN
BE
THE
SUM
OF
UNARY
AND
PAIRWISE
TERMS
THE
MAP
LABELS
Wˆ
CAN
HENCE
BE
FOUND
BY
MINIMIZING
A
COST
FUNCTION
OF
THE
FORM
N
Wˆ
ARGMIN
UN
WN
PMN
WM
WN
AND
THE
GRAPHICAL
MODEL
WILL
BE
AS
IN
FIGURE
THIS
COST
FUNCTION
CAN
BE
MINIMIZED
USING
THE
GRAPH
CUTS
TECHNIQUES
DESCRIBED
THROUGHOUT
THIS
CHAPTER
FIGURE
GRAPHICAL
MODEL
FOR
CON
DITIONAL
RANDOM
FIELD
COMPARE
TO
FIG
URE
THE
POSTERIOR
PROBABILITY
OF
THE
LABELS
W
IS
A
MARKOV
RANDOM
FIELD
FOR
FIXED
DATA
X
IN
THIS
MODEL
THE
TWO
SETS
OF
CLIQUES
RELATE
I
NEIGH
BOURING
LABELS
AND
II
EACH
LABEL
TO
ITS
ASSOCIATED
MEASUREMENT
SINCE
THIS
MODEL
ONLY
INCLUDES
UNARY
AND
PAIR
WISE
INTERACTIONS
BETWEEN
THE
LABELS
THE
UNKNOWN
LABELS
WN
N
CAN
BE
HIGHER
ORDER
MODELS
OPTIMIZED
USING
GRAPH
CUT
TECHNIQUES
FIGURE
DIRECTED
GRAPHICAL
MODEL
FOR
GRID
ALTHOUGH
THIS
MODEL
APPEARS
SIMILAR
TO
THE
PAIRWISE
MARKOV
RANDOM
FIELD
MODEL
IT
REPRE
SENTS
A
DIFFERENT
FACTORIZATION
OF
THE
JOINT
PROBABILITY
IN
PARTICULAR
THE
FACTORIZATION
CONTAINS
TERMS
INVOLVING
THREE
VARIABLES
SUCH
AS
P
R
W4
THIS
MEANS
THAT
THE
RESULTING
COST
FUNCTION
FOR
MAP
INFERENCE
IS
NO
LONGER
AMENABLE
TO
EXACT
SOLUTION
USING
GRAPH
CUT
METHODS
IN
THIS
CASE
AN
ATTRACTIVE
ALTERNATIVE
IS
TO
USE
SAMPLING
BASED
METHODS
AS
IT
IS
EASY
TO
GENERATE
SAMPLES
FROM
THIS
DIRECTED
MODEL
THE
MODELS
THAT
WE
HAVE
DISCUSSED
SO
FAR
HAVE
ONLY
CONNECTED
IMMEDIATE
NEIGHBORS
HOWEVER
THESE
ONLY
ALLOW
US
TO
MODEL
RELATIVELY
SIMPLE
STATISTICAL
PROPERTIES
OF
THE
LABEL
FIELD
ONE
WAY
TO
IMPROVE
THIS
SITUATION
IS
TO
CONSIDER
EACH
VARIABLE
WN
K
AS
REPRESENTING
THE
INDEX
OF
A
SQUARE
PATCH
OF
LABELS
FROM
A
PREDEFINED
LIBRARY
THE
PAIRWISE
MRF
NOW
ENCODES
THE
AFFINITY
OF
NEIGHBORING
PATCHES
FOR
EACH
OTHER
UNFORTUNATELY
THE
RESULTING
COSTS
ARE
LESS
LIKELY
TO
BE
SUBMODULAR
OR
EVEN
OBEY
THE
TRIANGLE
INEQUALITY
AND
THE
NUMBER
K
OF
PATCHES
IN
THE
LIBRARY
IS
USUALLY
VERY
LARGE
MAKING
GRAPH
CUT
ALGORITHMS
INEFFICIENT
A
SECOND
APPROACH
TO
MODELING
MORE
COMPLEX
STATISTICAL
PROPERTIES
OF
THE
LABEL
FIELD
IS
TO
INCREASE
THE
NUMBER
OF
THE
CONNECTIONS
FOR
THE
UNDIRECTED
MODELS
CRF
MRF
THIS
WOULD
MEAN
INTRODUCING
LARGER
CLIQUES
FOR
EXAMPLE
TO
MODEL
LOCAL
TEXTURE
WE
MIGHT
CONNECT
ALL
OF
THE
VARIABLES
IN
EVERY
REGION
OF
THE
IMAGE
UNFORTUNATELY
INFERENCE
IS
HARD
IN
THESE
MODELS
OPTIMIZING
THE
RESULTING
COMPLEX
COST
FUNCTIONS
IS
STILL
AN
OPEN
RESEARCH
TOPIC
MODELS
FOR
GRIDS
DIRECTED
MODELS
FOR
GRIDS
THE
MARKOV
RANDOM
FIELD
AND
CONDITIONAL
RANDOM
FIELD
MODELS
ARE
ATTRACTIVE
BE
CAUSE
WE
CAN
USE
GRAPH
CUTS
APPROACHES
TO
SEARCH
FOR
THE
MAP
SOLUTION
HOWEVER
THEY
HAVE
THE
DRAWBACK
THAT
IT
IS
VERY
HARD
TO
LEARN
THE
PARAMETERS
OF
THE
MODEL
BECAUSE
THEY
ARE
BASED
ON
UNDIRECTED
MODELS
AN
OBVIOUS
ALTERNATIVE
IS
TO
USE
A
SIMILAR
DIRECTED
MODEL
FIGURE
HERE
LEARNING
IS
RELATIVELY
EASY
BUT
IT
TURNS
OUT
THAT
MAP
INFERENCE
USING
GRAPH
CUTS
IS
NOT
GENERALLY
POSSIBLE
TO
SEE
THIS
CONSIDER
THE
COST
FUNCTION
FOR
MAP
INFERENCE
IN
THIS
MODEL
N
ARGMAX
LOG
P
R
N
N
LOG
P
R
N
N
ARGMAX
N
N
N
LOG
P
R
XN
WN
N
LOG
P
R
WN
WPA
N
ARGMIN
N
N
N
LOG
P
R
XN
WN
N
LOG
P
R
WN
WPA
N
WHERE
WE
HAVE
MULTIPLIED
THE
OBJECTIVE
FUNCTION
BY
MINUS
ONE
AND
NOW
SEEK
THE
MINIMUM
THIS
MINIMIZATION
PROBLEM
NOW
HAS
THE
GENERAL
FORM
N
ARGMIN
N
N
N
UN
WN
N
TN
WN
N
N
WHERE
UN
WN
IS
CALLED
A
UNARY
TERM
REFLECTING
THE
FACT
THAT
IT
ONLY
DEPENDS
ON
A
SINGLE
ELEMENT
WN
OF
THE
LABEL
FIELD
AND
TN
WN
N
N
IS
CALLED
A
THREE
WISE
TERM
REFLECTING
THE
FACT
IN
GENERAL
THE
LABEL
AT
A
PIXEL
IS
CONDITIONED
ON
THE
TWO
PARENTS
N
AND
N
ABOVE
AND
TO
THE
LEFT
OF
THE
CURRENT
POSITION
NOTICE
THAT
THIS
COST
FUNCTION
IS
FUNDAMENTALLY
DIFFERENT
FROM
THE
COST
FUNCTION
FOR
MAP
INFERENCE
IN
A
PAIRWISE
MRF
EQUATION
IT
INCLUDES
THREE
WISE
TERMS
AND
THERE
IS
NO
KNOWN
POLYNOMIAL
ALGORITHM
TO
OPTIMIZE
THIS
CRITERION
HOWEVER
SINCE
THIS
MODEL
IS
A
DIRECTED
GRAPHICAL
MODEL
IT
IS
EASY
TO
GENERATE
SAMPLES
FROM
THIS
MODEL
AND
THIS
CAN
BE
EXPLOITED
FOR
APPROXIMATE
INFERENCE
METHODS
SUCH
AS
COMPUTING
THE
EMPIRICAL
MAX
MARGINALS
APPLICATIONS
THE
MODELS
AND
ALGORITHMS
IN
THIS
CHAPTER
ARE
USED
IN
A
LARGE
NUMBER
OF
COMPUTER
VISION
APPLICATIONS
INCLUDING
STEREO
VISION
MOTION
ESTIMATION
BACKGROUND
SUB
TRACTION
INTERACTIVE
SEGMENTATION
SEMANTIC
SEGMENTATION
IMAGE
EDITING
IMAGE
DENOISING
IMAGE
SUPER
RESOLUTION
AND
BUILDING
MODELS
HERE
WE
REVIEW
A
FEW
KEY
EXAMPLES
WE
CONSIDER
BACKGROUND
SUBTRACTION
WHICH
IS
A
SIMPLE
APPLICATION
WITH
BINARY
LABELS
AND
INTERACTIVE
SEGMENTATION
WHICH
USES
BINARY
LABELS
IN
A
SYS
TEM
THAT
SIMULTANEOUSLY
ESTIMATES
THE
PARAMETERS
IN
THE
LIKELIHOOD
TERMS
THEN
WE
APPLICATIONS
CONSIDER
STEREO
MOTION
ESTIMATION
AND
IMAGE
EDITING
ALL
OF
WHICH
ARE
MULTI
LABEL
GRAPH
CUT
PROBLEMS
WE
CONSIDER
SUPER
RESOLUTION
WHICH
IS
A
MULTI
LABEL
PROBLEM
WHERE
THE
UNITS
ARE
PATCHES
RATHER
THAN
PIXELS
AND
WHICH
THERE
ARE
SO
MANY
LABELS
THAT
THE
ALPHA
EXPANSION
ALGORITHM
IS
NOT
SUITABLE
FINALLY
WE
CONSIDER
DRAWING
SAMPLES
FROM
DIRECTED
GRID
MODELS
TO
GENERATE
NOVEL
IMAGES
BACKGROUND
SUBTRACTION
FIRST
LET
US
REVISIT
THE
BACKGROUND
SUBTRACTION
ALGORITHM
THAT
WE
FIRST
ENCOUNTERED
IN
SECTION
IN
BACKGROUND
SUBTRACTION
THE
GOAL
IS
TO
ASSOCIATE
A
BINARY
LABEL
WN
N
WITH
EACH
OF
THE
N
PIXELS
IN
THE
IMAGE
INDICATING
WHETHER
THIS
PIXEL
BELONGS
TO
THE
FOREGROUND
OR
BACKGROUND
BASED
ON
THE
OBSERVED
RGB
DATA
XN
N
AT
EACH
PIXEL
WHEN
THE
PIXEL
IS
BACKGROUND
WN
THE
DATA
ARE
ASSUMED
TO
BE
GENERATED
FROM
A
NORMAL
DISTRIBUTION
WITH
KNOWN
MEAN
AND
COVARIANCE
WHEN
THE
PIXEL
IS
FOREGROUND
WN
A
UNIFORM
DISTRIBUTION
OVER
THE
DATA
IS
ASSUMED
SO
THAT
P
R
XN
W
NORMXN
ΜN
ΣN
P
R
XN
W
Κ
WHERE
Κ
IS
A
CONSTANT
IN
THE
ORIGINAL
DESCRIPTION
WE
ASSUMED
THAT
THE
MODELS
AT
EACH
PIXEL
WERE
INDEPENDENT
AND
WHEN
WE
INFERRED
THE
LABELS
THE
RESULTS
WERE
NOISY
FIGURE
WE
NOW
PLACE
A
MARKOV
RANDOM
FIELD
PRIOR
OVER
THE
BINARY
LABELS
WHERE
THE
PAIRWISE
CLIQUES
ARE
ORGANIZED
AS
A
GRID
AS
IN
MOST
OF
THE
MODELS
IN
THIS
CHAPTER
AND
WHERE
THE
POTENTIAL
FUNCTIONS
ENCOURAGE
SMOOTHNESS
FIGURE
ILLUSTRATES
THE
RESULTS
OF
PERFORMING
INFERENCE
IN
THIS
MODEL
USING
THE
GRAPH
CUTS
ALGORITHM
THERE
ARE
NOW
FAR
FEWER
ISOLATED
FOREGROUND
REGIONS
AND
FEWER
HOLES
IN
THE
FOREGROUND
OBJECT
THE
MODEL
HAS
STILL
ERRONEOUSLY
DISCOVERED
THE
SHADOW
A
MORE
SOPHISTICATED
MODEL
WOULD
BE
REQUIRED
TO
DEAL
WITH
THIS
PROBLEM
INTERACTIVE
SEGMENTATION
GRABCUT
THE
GOAL
OF
INTERACTIVE
SEGMENTATION
IS
TO
CUT
OUT
THE
FOREGROUND
OBJECT
IN
A
PHOTO
BASED
ON
SOME
INPUT
FROM
THE
USER
FIGURE
MORE
PRECISELY
WE
AIM
TO
AS
SOCIATE
A
BINARY
LABEL
WN
N
TO
EACH
OF
THE
N
PIXELS
IN
THE
IMAGE
INDICATING
WHETHER
THIS
PIXEL
BELONGS
TO
THE
FOREGROUND
OR
BACKGROUND
BASED
ON
THE
OBSERVED
RGB
DATA
XN
N
AT
EACH
PIXEL
HOWEVER
UNLIKE
BACKGROUND
SUBTRACTION
WE
DO
NOT
HAVE
ANY
PRIOR
KNOWLEDGE
OF
EITHER
THE
FOREGROUND
OR
THE
BACKGROUND
IN
THE
GRABCUT
SYSTEM
OF
ROTHER
ET
AL
THE
LIKELIHOODS
OF
OBSERVING
THE
BACKGROUND
W
AND
FOREGROUND
W
ARE
EACH
MODELED
AS
A
MIXTURE
OF
K
GAUSSIANS
SO
THAT
MODELS
FOR
GRIDS
FIGURE
BACKGROUND
SUBTRACTION
REVISITED
A
ORIGINAL
IMAGE
B
MAP
SOLUTION
OF
BACKGROUND
SUBTRACTION
MODEL
WITH
INDEPENDENT
PIXELS
THE
SOLUTION
CONTAINS
NOISE
C
MAP
SOLUTION
OF
BACKGROUND
SUBTRACTION
MODEL
WITH
MARKOV
RANDOM
FIELD
PRIOR
THIS
SMOOTHED
SOLUTION
HAS
ELIMINATED
MOST
OF
THE
NOISE
FIGURE
GRAB
CUT
A
THE
USER
DRAWS
A
BOUNDING
BOX
AROUND
THE
OBJECT
OF
INTEREST
B
THE
ALGORITHM
SEGMENTS
THE
FOREGROUND
FROM
THE
BACKGROUND
BY
ALTERNATING
BETWEEN
BUILDING
COLOR
MODELS
AND
SEGMENTING
THE
IMAGE
C
D
A
SECOND
EXAMPLE
E
F
FAILURE
MODE
THIS
ALGORITHM
DOES
NOT
SEGMENT
WIRY
OBJECTS
WELL
AS
THE
PAIRWISE
COSTS
FOR
TRACING
AROUND
ALL
THE
BOUNDARIES
ARE
PROHIBITIVE
ADAPTED
FROM
ROTHER
ET
AL
QC
ACM
P
R
XN
W
J
K
ΛJKNORMXN
ΜJK
ΣJK
K
AND
THE
PRIOR
OVER
THE
LABELS
IS
MODELED
AS
A
PAIRWISE
CONNECTED
MARKOV
RANDOM
FIELD
WITH
THE
POTENTIALS
CHOSEN
TO
ENCOURAGE
SMOOTHNESS
IN
THIS
APPLICATION
THE
IMAGE
MAY
HAVE
A
WIDE
VARIETY
OF
CONTENT
AND
SO
THERE
IS
NO
SUITABLE
TRAINING
DATA
FROM
WHICH
TO
LEARN
THE
PARAMETERS
ΛJK
ΜJK
K
J
K
OF
THE
FOREGROUND
AND
BACKGROUND
COLOR
MODELS
HOWEVER
WE
NOTE
THAT
I
IF
WE
KNEW
THE
COLOR
MODELS
WE
COULD
PERFORM
THE
SEGMENTATION
VIA
MAP
INFERENCE
WITH
THE
GRAPH
CUTS
ALGORITHM
AND
II
IF
WE
KNEW
THE
SEGMENTATION
THEN
WE
COULD
COMPUTE
THE
FOREGROUND
AND
BACKGROUND
COLOR
MODELS
BASED
ON
THE
PIXELS
ASSIGNED
TO
EACH
CATEGORY
THIS
OBSERVATION
LEADS
TO
AN
ALTERNATING
APPROACH
TO
INFERENCE
IN
THIS
MODEL
IN
WHICH
THE
SEGMENTATION
AND
PARAMETERS
ARE
COMPUTED
IN
TURN
UNTIL
THE
SYSTEM
CONVERGES
IN
THE
GRABCUT
ALGORITHM
THE
USER
DRAWS
A
BOUNDING
BOX
AROUND
THE
DESIRED
OBJECT
TO
BE
SEGMENTED
THIS
EFFECTIVELY
DEFINES
A
ROUGH
SEGMENTATION
PIXELS
WITHIN
THE
BOX
ARE
FOREGROUND
AND
PIXELS
OUTSIDE
ARE
BACKGROUND
FROM
WHICH
THE
SYSTEM
IS
INITIALIZED
IF
THE
SEGMENTATION
IS
NOT
CORRECT
AFTER
THE
ALTERNATING
OPTIMIZATION
ALGORITHM
CONVERGES
THE
USER
MAY
PAINT
REGIONS
OF
THE
IMAGE
WITH
A
FOREGROUND
OR
BACKGROUND
BRUSH
INDICATING
THAT
THESE
MUST
BELONG
TO
THE
APPROPRIATE
CLASS
IN
THE
FINAL
SOLUTION
IN
PRACTICE
THIS
MEANS
THAT
THE
UNARY
COSTS
ARE
SET
TO
ENSURE
THAT
THESE
TAKE
THE
APPROPRIATE
VALUES
AND
THE
ALTERNATING
SOLUTION
IS
RUN
AGAIN
FROM
THIS
POINT
UNTIL
CONVERGENCE
EXAMPLE
RESULTS
ARE
SHOWN
IN
FIGURE
TO
IMPROVE
THE
PERFORMANCE
OF
THIS
ALGORITHM
IT
IS
POSSIBLE
TO
MODIFY
THE
MRF
SO
THAT
THE
PAIRWISE
COST
FOR
CHANGING
FROM
FOREGROUND
TO
BACKGROUND
LABEL
IS
LESS
WHERE
THERE
IS
AN
EDGE
IN
THE
IMAGE
THIS
IS
REFERRED
TO
AS
USING
GEODESIC
DISTANCE
FROM
A
PURE
PROBABILISTIC
VIEWPOINT
THIS
IS
SOMEWHAT
DUBIOUS
AS
THE
MRF
PRIOR
SHOULD
EMBODY
WHAT
WE
KNOW
ABOUT
THE
TASK
BEFORE
SEEING
THE
DATA
AND
HENCE
CANNOT
DEPEND
ON
THE
IMAGE
HOWEVER
THIS
IS
LARGELY
A
PHILOSOPHICAL
OBJECTION
AND
THE
METHOD
WORKS
WELL
IN
PRACTICE
FOR
A
WIDE
VARIETY
OF
OBJECTS
A
NOTABLE
FAILURE
MODE
IS
IN
SEGMENTING
WIRY
OBJECTS
SUCH
AS
TREES
HERE
THE
MODEL
IS
NOT
PREPARED
TO
PAY
THE
EXTENSIVE
PAIRWISE
COSTS
TO
CUT
EXACTLY
AROUND
THE
MANY
EDGES
OF
THE
OBJECT
AND
SO
THE
SEGMENTATION
IS
POOR
STEREO
VISION
IN
STEREO
VISION
THE
GOAL
IS
TO
INFER
A
DISCRETE
MULTIVALUED
LABEL
WN
N
REPRESENT
ING
THE
DISPARITY
HORIZONTAL
SHIFT
AT
EACH
PIXEL
IN
THE
IMAGE
GIVEN
THE
OBSERVED
IMAGE
DATA
XN
N
MORE
DETAILS
ABOUT
THE
LIKELIHOOD
TERMS
IN
THIS
PROBLEM
CAN
BE
FOUND
IN
SECTION
WHERE
WE
DESCRIBED
TREE
BASED
PRIORS
FOR
THE
UNKNOWN
DISPARITIES
A
MORE
SUITABLE
APPROACH
IS
TO
USE
AN
MRF
PRIOR
AS
FOR
THE
DENOISING
EXAMPLE
IT
IS
UNDESIRABLE
TO
USE
AN
MRF
PRIOR
WHERE
THE
COSTS
ARE
A
CONVEX
FUNCTION
OF
THE
DIFFERENCE
IN
NEIGHBORING
LABELS
THIS
RESULTS
IN
A
MAP
SOLUTION
WHERE
THE
EDGES
OF
OBJECTS
ARE
SMOOTHED
HENCE
IT
IS
USUAL
TO
USE
A
NON
CONVEX
PRIOR
SUCH
AS
THE
POTTS
FUNCTION
WHICH
EMBODIES
THE
IDEA
THAT
THE
SCENE
CONSISTS
OF
SMOOTH
SURFACES
WITH
SUDDEN
JUMPS
IN
DEPTH
BETWEEN
THEM
WHERE
THE
SIZE
OF
THE
JUMP
IS
UNIMPORTANT
BOYKOV
ET
AL
USED
THE
ALPHA
EXPANSION
ALGORITHM
TO
PERFORM
APPROX
IMATE
INFERENCE
IN
A
MODEL
OF
THIS
SORT
FIGURE
THE
PERFORMANCE
OF
THIS
ALGORITHM
IS
GOOD
BUT
ERRORS
ARE
FOUND
WHERE
THERE
IS
NO
TRUE
MATCH
IN
THE
OTHER
IMAGE
I
E
WHERE
THE
CORRESPONDING
POINT
IS
OCCLUDED
BY
ANOTHER
OBJECT
KOL
FIGURE
STEREO
VISION
A
ONE
IMAGE
OF
THE
ORIGINAL
STEREO
PAIR
B
DISPARITY
ESTIMATED
USING
THE
METHOD
OF
BOYKOV
ET
AL
C
GROUND
TRUTH
DISPARITY
BLUE
PIXELS
INDICATE
REGIONS
WHICH
ARE
OCCLUDED
IN
THE
SECOND
IMAGE
AND
SO
DO
NOT
HAVE
A
VALID
MATCH
OR
DISPARITY
THE
ALGORITHM
DOES
NOT
TAKE
ACCOUNT
OF
THIS
FACT
AND
PRODUCES
NOISY
ESTIMATES
IN
THESE
REGIONS
ADAPTED
FROM
BOYKOV
ET
AL
MOGOROV
ZABIH
SUBSEQUENTLY
DEVELOPED
A
BESPOKE
GRAPH
FOR
DEALING
WITH
OCCLUSIONS
IN
STEREO
VISION
AND
AN
ALPHA
EXPANSION
ALGORITHM
FOR
OPTIMIZING
THE
ASSOCIATED
COST
FUNCTION
THESE
METHODS
CAN
ALSO
BE
APPLIED
TO
OPTICAL
FLOW
IN
WHICH
WE
ATTEMPT
TO
IDENTIFY
PIXEL
CORRESPONDENCES
BETWEEN
ADJACENT
FRAMES
IN
A
VIDEO
SEQUENCE
UNLIKE
IN
STEREO
VISION
THERE
IS
NO
GUARANTEE
THAT
THE
CORRESPOND
ING
MATCHES
WILL
BE
ON
THE
SAME
SCANLINE
BUT
OTHER
THAN
THIS
THE
PROBLEM
IS
VERY
SIMILAR
REARRANGING
IMAGES
MARKOV
RANDOM
FIELD
MODELS
CAN
ALSO
BE
USED
FOR
REARRANGING
IMAGES
WE
ARE
GIVEN
AN
ORIGINAL
IMAGE
I
AND
WISH
TO
CREATE
A
NEW
IMAGE
I
BY
REARRANGING
THE
PIXELS
FROM
I
IN
SOME
WAY
DEPENDING
ON
THE
APPLICATION
WE
MAY
WISH
TO
CHANGE
THE
DIMENSIONS
OF
THE
ORIGINAL
IMAGE
TERMED
IMAGE
RE
TARGETING
REMOVE
AN
OBJECT
OR
MOVE
AN
OBJECT
FROM
ONE
PLACE
TO
ANOTHER
PRITCH
ET
AL
CONSTRUCTED
A
MODEL
WITH
VARIABLES
W
WN
AT
EACH
OF
THE
N
PIXELS
OF
I
EACH
POSSIBLE
VALUE
OF
WN
K
REPRESENTS
A
RELATIVE
OFFSET
TO
IMAGE
I
THAT
TELLS
US
WHICH
PIXEL
FROM
IMAGE
I
WILL
APPEAR
AT
THE
NTH
PIXEL
OF
THE
NEW
IMAGE
THE
LABEL
MAP
W
IS
HENCE
TERMED
A
SHIFT
MAP
AS
IT
REPRESENTS
SHIFTS
TO
THE
ORIGINAL
IMAGE
EACH
POSSIBLE
SHIFT
MAP
DEFINES
A
DIFFERENT
OUTPUT
IMAGE
I
FIGURE
PRITCH
ET
AL
MODEL
THE
SHIFT
MAP
W
AS
AN
MRF
WITH
PAIRWISE
COSTS
THAT
ENCOURAGE
SMOOTHNESS
THE
RESULT
OF
THIS
IS
THAT
ONLY
SHIFT
MAPS
THAT
ARE
PIECEWISE
CONSTANT
HAVE
HIGH
PROBABILITY
IN
OTHER
WORDS
NEW
IMAGES
WHICH
CONSIST
OF
LARGE
CHUNKS
OF
THE
ORIGINAL
IMAGE
THAT
HAVE
BEEN
COPIED
VERBATIM
ARE
FAVORED
THEY
MODIFY
THE
PAIRWISE
COSTS
SO
THAT
THEY
ARE
LOWER
WHEN
ADJACENT
LABELS
ENCODE
OFFSETS
WITH
SIMILAR
SURROUNDING
REGIONS
THIS
MEANS
THAT
WHERE
THE
LABEL
DOES
CHANGE
IT
DOES
SO
IN
SUCH
A
WAY
THAT
THERE
IS
NO
VISIBLE
SEAM
IN
THE
OUTPUT
IMAGE
THE
REMAINDER
OF
THE
MODEL
DEPENDS
ON
THE
APPLICATION
FIGURE
APPLICATIONS
A
C
D
FIGURE
SHIFT
MAPS
FOR
IMAGE
RE
TARGETING
TO
REDUCE
WIDTH
A
NEW
IMAGE
I
IS
CREATED
FROM
B
THE
ORIGINAL
IMAGE
I
BY
COPYING
PIECEWISE
REGIONS
FIVE
REGIONS
SHOWN
C
THESE
REGIONS
ARE
CAREFULLY
CHOSEN
TO
PRODUCE
A
SEAMLESS
RESULT
D
THE
UNDERLYING
REPRESENTATION
IS
A
SHIFTMAP
A
LABEL
AT
EACH
PIXEL
OF
THE
NEW
IMAGE
THAT
SPECIFIES
THE
OFFSET
TO
THE
POSITION
IN
THE
ORIGINAL
IMAGE
THAT
WILL
BE
COPIED
FROM
AN
MRF
ENCOURAGES
THE
LABELS
TO
BE
PIECEWISE
CONSTANT
AND
HENCE
THE
RESULT
TENDS
TO
CONSIST
OF
LARGE
CHUNKS
COPIED
VERBATIM
FIGURE
SHOWS
METHOD
OF
PRITCH
ET
AL
TO
MOVE
AN
OBJECT
WE
SPECIFY
UNARY
COSTS
IN
THE
NEW
REGION
THAT
ENSURE
THAT
WE
COPY
THE
DESIRED
OBJECT
HERE
THE
REMAINDER
OF
THE
SHIFTS
ARE
LEFT
FREE
TO
VARY
BUT
FAVOR
SMALL
OFFSETS
SO
THAT
PARTS
OF
THE
SCENE
THAT
ARE
FAR
FROM
THE
CHANGE
TEND
TO
BE
UNPERTURBED
TO
REPLACE
AN
AREA
OF
THE
IMAGE
WE
SPECIFY
UNARY
COSTS
SO
THAT
THE
REMAINDER
OF
THE
IMAGE
MUST
HAVE
A
SHIFT
OF
ZERO
VERBATIM
COPYING
AND
THE
SHIFT
IN
THE
MISSING
REGION
MUST
BE
SUCH
THAT
IT
COPIES
FROM
OUTSIDE
THE
REGION
TO
RETARGET
AN
IMAGE
TO
LARGER
WIDTH
WE
SET
THE
UNARY
COSTS
SO
THAT
THE
LEFT
AND
RIGHT
EDGES
OF
THE
NEW
IMAGE
ARE
FORCED
TO
HAVE
SHIFTS
THAT
CORRESPOND
TO
THE
LEFT
AND
RIGHT
OF
THE
ORIGINAL
IMAGE
WE
ALSO
USE
THE
UNARY
COSTS
TO
SPECIFY
THAT
VERTICAL
SHIFTS
MUST
BE
SMALL
TO
RETARGET
AN
IMAGE
TO
A
SMALLER
WIDTH
FIGURE
WE
ADDITIONALLY
SPECIFY
THAT
THE
HORIZONTAL
OFFSET
CAN
ONLY
INCREASE
AS
WE
MOVE
FROM
LEFT
TO
RIGHT
THIS
ENSURES
THAT
THE
NEW
IMAGE
DOES
NOT
CONTAIN
REPLICATED
OBJECTS
AND
THAT
THEIR
HORIZONTAL
ORDER
REMAINS
CONSTANT
IN
EACH
CASE
THE
BEST
SOLUTION
CAN
BE
FOUND
USING
THE
ALPHA
EXPANSION
ALGORITHM
SINCE
THE
PAIRWISE
TERMS
DO
NOT
FORM
A
METRIC
HERE
IT
IS
NECESSARY
TO
TRUNCATE
THE
RELEVANT
COSTS
SEE
SECTION
IN
PRACTICE
THERE
ARE
MANY
LABELS
AND
SO
PRITCH
ET
AL
INTRODUCE
A
COARSE
TO
FINE
SCHEME
IN
WHICH
A
LOW
RESOLUTION
VERSION
OF
THE
IMAGE
IS
INITIALLY
SYNTHESIZED
AND
THE
RESULT
OF
THIS
IS
USED
TO
GUIDE
FURTHER
REFINEMENTS
AT
HIGHER
RESOLUTIONS
MODELS
FOR
GRIDS
FIGURE
APPLICATIONS
OF
SHIFT
MAPS
SHIFT
MAPS
CAN
BE
USED
TO
A
TAKE
AN
OBJECT
FROM
THE
ORIGINAL
IMAGE
B
MOVE
IT
TO
A
NEW
POSITION
AND
C
THEN
FILL
IN
THE
REMAINING
PIXELS
TO
PRODUCE
A
NEW
PICTURE
D
THEY
CAN
ALSO
BE
USED
TO
REMOVE
AN
UNDESIRABLE
OBJECT
E
SPECIFIED
BY
A
MASK
FROM
AN
IMAGE
BY
F
FILLING
IN
THE
MISSING
AREA
G
H
FINALLY
THEY
CAN
BE
USED
TO
RETARGET
AN
ORIGINAL
IMAGE
TO
A
SMALLER
SIZE
OR
I
J
TO
RE
TARGET
AN
ORIGINAL
IMAGE
TO
A
LARGER
SIZE
RESULTS
FROM
METHOD
OF
PRITCH
ET
AL
SUPER
RESOLUTION
IMAGE
SUPER
RESOLUTION
CAN
ALSO
BE
FRAMED
AS
INFERENCE
WITHIN
A
MARKOV
RANDOM
FIELD
MODEL
HERE
THE
BASIC
UNIT
OF
CURRENCY
IS
AN
IMAGE
PATCH
RATHER
THAN
A
PIXEL
FOR
EXAMPLE
CONSIDER
DIVIDING
THE
ORIGINAL
IMAGE
INTO
A
REGULAR
GRID
OF
N
LOW
RESOLUTION
PATCHES
XN
N
THE
GOAL
IS
TO
INFER
A
SET
OF
CORRESPONDING
LABELS
WN
N
AT
EACH
POSITION
IN
THE
GRID
EACH
LABEL
CAN
TAKE
ONE
OF
K
VALUES
EACH
OF
WHICH
CORRESPONDS
TO
A
DIFFERENT
POSSIBLE
HIGH
RESOLUTION
PATCH
THESE
PATCHES
ARE
EXTRACTED
FROM
TRAINING
IMAGES
THE
PAIRWISE
COST
FOR
PLACING
HIGH
RESOLUTION
PATCHES
TOGETHER
IS
DETERMINED
BY
THE
AGREEMENT
AT
THE
ABUTTING
EDGE
THE
UNARY
COST
FOR
CHOOSING
A
PATCH
AT
A
GIVEN
POSITION
DEPENDS
ON
THE
AGREEMENT
BETWEEN
THE
PROPOSED
HIGH
RESOLUTION
PATCH
AND
THE
OBSERVED
LOW
RESOLUTION
PATCH
THIS
CAN
BE
COMPUTED
BY
DOWNSAMPLING
THE
HIGH
RESOLUTION
PATCH
TO
PIXELS
AND
THEN
USING
A
NORMAL
NOISE
MODEL
A
B
C
FIGURE
SUPER
RESOLUTION
A
THE
OBSERVED
IMAGE
WHICH
IS
BROKEN
DOWN
INTO
A
REGULAR
GRID
OF
LOW
RESOLUTION
PATCHES
B
WE
INFER
A
REGULAR
GRID
OF
LABELS
EACH
OF
WHICH
CORRESPONDS
TO
A
HIGH
RESOLUTION
PATCH
AND
QUILT
THESE
TOGETHER
TO
FORM
THE
SUPER
RESOLVED
IMAGE
C
GROUND
TRUTH
ADAPTED
FROM
FREEMAN
ET
AL
QC
SPRINGER
IN
PRINCIPLE
WE
COULD
PERFORM
INFERENCE
IN
THIS
MODEL
WITH
A
GRAPH
CUT
FOR
MULATION
BUT
THERE
ARE
TWO
PROBLEMS
FIRST
THE
RESULTING
COST
FUNCTION
IS
NOT
SUBMODULAR
SECOND
THE
NUMBER
OF
POSSIBLE
HIGH
RESOLUTION
PATCHES
MUST
BE
VERY
LARGE
AND
SO
THE
ALPHA
EXPANSION
ALGORITHM
WHICH
CHOOSES
THESE
IN
TURN
WOULD
BE
EXTREMELY
INEFFICIENT
FREEMAN
ET
AL
USED
LOOPY
BELIEF
PROPAGATION
TO
PERFORM
APPROXIMATE
INFERENCE
IN
A
MODEL
SIMILAR
TO
THIS
TO
MAKE
THIS
RELATIVELY
FAST
THEY
USED
ONLY
A
SUBSET
OF
J
K
POSSIBLE
PATCHES
AT
EACH
POSITION
WHERE
THESE
WERE
CHOSEN
SO
THAT
THEY
WERE
THE
J
PATCHES
WHICH
AGREED
BEST
WITH
THE
OBSERVED
DATA
AND
SO
HAD
THE
LOWEST
UNARY
COSTS
ALTHOUGH
THE
RESULTS
FIGURE
ARE
QUITE
CONVINCING
THEY
ARE
SADLY
FAR
FROM
THE
IMAGINED
FEATS
OF
TV
CRIME
DRAMA
TEXTURE
SYNTHESIS
THE
APPLICATIONS
SO
FAR
HAVE
ALL
BEEN
BASED
ON
PERFORMING
INFERENCE
IN
THE
UNDI
RECTED
MARKOV
RANDOM
FIELD
MODEL
WE
NOW
CONSIDER
THE
DIRECTED
MODEL
INFERENCE
IS
DIFFICULT
IN
THIS
MODEL
DUE
TO
THE
PRESENCE
OF
THREE
WISE
TERMS
IN
THE
ASSOCIATED
COST
FUNCTION
SEE
SECTION
HOWEVER
GENERATION
FROM
THIS
MODEL
IS
RELATIVELY
EASY
SINCE
THIS
IS
A
DIRECTED
MODEL
WE
CAN
USE
AN
ANCESTRAL
SAMPLING
TECHNIQUE
TO
GENERATE
EXAMPLES
ONE
POSSIBLE
APPLICATION
OF
THIS
TECHNIQUE
IS
FOR
TEXTURE
SYNTHESIS
THE
GOAL
OF
TEXTURE
SYNTHESIS
IS
TO
LEARN
A
GENERATIVE
MODEL
FROM
A
SMALL
PATCH
OF
TEXTURE
SUCH
THAT
WHEN
WE
DRAW
SAMPLES
FROM
THE
MODEL
THEY
LOOK
LIKE
EXTENDED
EXAMPLES
OF
THE
SAME
TEXTURE
FIGURE
THE
PARTICULAR
TECHNIQUE
THAT
WE
DESCRIBE
HERE
IS
KNOWN
AS
IMAGE
QUILTING
AND
WAS
ORIGINALLY
DESCRIBED
BY
EFROS
FREEMAN
WE
WILL
FIRST
DESCRIBE
THE
ALGORITHM
AS
IT
WAS
INITIALLY
CONCEIVED
AND
THEN
RELATE
IT
TO
THE
DIRECTED
MODEL
FOR
GRIDS
FIGURE
TEXTURE
SYNTHESIS
A
B
ORIGINAL
TEXTURE
SAMPLES
C
D
SYNTHE
SIZED
TEXTURES
USING
IMAGE
QUILTING
ADAPTED
FROM
EFROS
FREEMAN
ORIGINAL
SAMPLE
A
B
PATCH
LIBRARY
FIGURE
IMAGE
QUILTING
A
ORIGINAL
TEXTURE
SAMPLE
B
LIBRARY
OF
ALL
OVERLAPPING
PATCHES
FROM
THE
ORIGINAL
TEXTURE
SAMPLE
C
THE
FIRST
PATCH
IS
CHOSEN
RANDOMLY
FROM
THE
LIBRARY
D
THE
SECOND
PATCH
IS
CHOSEN
RANDOMLY
FROM
THE
K
LIBRARY
PATCHES
THAT
ARE
MOST
SIMILAR
IN
THE
OVERLAPPING
REGION
E
IN
SUBSEQUENT
ROWS
PATCHES
ARE
CHOSEN
SO
THAT
THE
OVERLAPPING
REGION
AGREES
WITH
THE
PREVIOUSLY
PLACED
PATCHES
TO
THE
LEFT
AND
ABOVE
F
THIS
CONTINUES
UNTIL
WE
REACH
THE
BOTTOM
RIGHT
OF
THE
IMAGE
G
THE
PATCHES
ARE
THEN
BLENDED
TOGETHER
TO
GIVE
THE
FINAL
RESULTS
FIGURE
IMAGE
QUILTING
AS
ANCESTRAL
SAMPLING
FROM
A
GRAPHICAL
MODEL
WHEN
WE
SYNTHESIZE
IMAGES
WE
ARE
EFFECTIVELY
ANCESTRAL
SAMPLING
FROM
A
DIRECTED
GRID
MODEL
WHERE
EACH
HIDDEN
NODE
REPRESENTS
A
PATCH
INDEX
AND
EACH
OBSERVED
VARIABLE
REPRESENTS
THE
PATCH
DATA
THE
FIRST
STEP
SEE
FIGURE
IS
TO
EXTRACT
ALL
POSSIBLE
PATCHES
OF
A
GIVEN
SIZE
FROM
THE
INPUT
TEXTURE
TO
FORM
A
PATCH
LIBRARY
THE
SYNTHESIZED
IMAGE
WILL
CONSIST
OF
A
REGULAR
GRID
OF
THESE
LIBRARY
PATCHES
SUCH
THAT
EACH
OVERLAPS
ITS
NEIGHBORS
BY
A
FEW
PIXELS
A
NEW
TEXTURE
IS
SYNTHESIZED
STARTING
IN
THE
TOP
LEFT
OF
THIS
GRID
AND
PROCEEDING
TO
THE
BOTTOM
RIGHT
AT
EACH
POSITION
A
LIBRARY
PATCH
IS
CHOSEN
SUCH
THAT
IT
IS
VISUALLY
CONSISTENT
WITH
THE
PATCHES
THAT
HAVE
PREVIOUSLY
BEEN
PLACED
ABOVE
AND
TO
THE
LEFT
FOR
THE
TOP
LEFT
POSITION
WE
RANDOMLY
CHOOSE
A
PATCH
FROM
THE
LIBRARY
WE
THEN
CONSIDER
PLACING
A
SECOND
PATCH
TO
THE
RIGHT
OF
THE
FIRST
PATCH
SUCH
THAT
THEY
OVERLAP
BY
ROUGHLY
OF
THEIR
WIDTH
WE
SEARCH
THROUGH
THE
LIBRARY
FOR
THE
J
PATCHES
WHERE
THE
SQUARED
RGB
INTENSITY
DIFFERENCE
IN
THE
OVERLAPPING
REGION
IS
SMALLEST
WE
CHOOSE
ONE
OF
THESE
J
PATCHES
RANDOMLY
AND
PLACE
IT
INTO
THE
IMAGE
AT
THE
SECOND
POSITION
WE
CONTINUE
IN
THIS
WAY
SYNTHESIZING
THE
TOP
ROW
OF
PATCHES
IN
THE
IMAGE
WHEN
WE
REACH
THE
SECOND
ROW
WE
MUST
CONSIDER
THE
OVERLAP
WITH
THE
PATCHES
TO
THE
LEFT
AND
ABOVE
IN
DECIDING
WHETHER
A
CANDIDATE
LIBRARY
PATCH
IS
SUITABLE
WE
CHOOSE
THE
J
PATCHES
WHERE
THE
TOTAL
RGB
DIFFERENCE
BETWEEN
THE
OVERLAPPING
PORTIONS
OF
THE
CANDIDATE
PATCH
AND
THE
PREVIOUSLY
CHOSEN
PATCHES
IS
MINIMAL
THIS
PROCESS
CONTINUES
UNTIL
WE
REACH
THE
BOTTOM
RIGHT
OF
THE
IMAGE
IN
THIS
WAY
WE
SYNTHESIZE
A
NEW
EXAMPLE
OF
THE
TEXTURE
FIGURE
F
BY
FORCING
THE
OVERLAPPING
REGIONS
TO
BE
SIMILAR
WE
ENFORCE
VISUAL
CONSISTENCY
BETWEEN
ADJACENT
PATCHES
BY
CHOOSING
RANDOMLY
FROM
THE
J
BEST
PATCHES
WE
ENSURE
THAT
THE
RESULT
IS
STOCHASTIC
IF
WE
ALWAYS
CHOSE
THE
MOST
VISUALLY
CONSISTENT
PATCH
WE
WOULD
REPLICATE
THE
ORIGINAL
TEXTURE
VERBATIM
AT
THE
END
OF
THIS
PROCESS
IT
IS
COMMON
TO
BLEND
THE
RESULTING
PATCHES
TOGETHER
TO
REMOVE
REMAINING
ARTIFACTS
IN
FIGURE
SYNTHESIZING
NOVEL
FACES
A
A
SAMPLE
IS
DRAWN
FROM
A
SUBSPACE
MODEL
SEE
CHAPTER
THAT
HAS
BEEN
TRAINED
ON
FACIAL
IMAGES
B
TEXTURE
SYNTHESIS
NOW
PROCEEDS
BUT
WITH
TWO
DIFFERENCES
FROM
BEFORE
FIRST
THE
CHOICE
OF
PATCH
MUST
NOW
AGREE
WITH
THE
SAMPLE
FROM
THE
SUBSPACE
MODEL
AS
WELL
AS
THE
PREVIOUSLY
PLACED
PATCHES
SECOND
THE
LIBRARY
PATCHES
ARE
NOW
DIFFERENT
AT
EACH
POSITION
IN
THIS
WAY
WE
ENSURE
THAT
A
NOSE
PATCH
IS
ALWAYS
CHOSEN
IN
THE
CENTER
AND
SO
ON
C
AFTER
COMPLETING
THE
SYNTHESIS
AND
BLENDING
TOGETHER
THE
PATCHES
D
F
THREE
MORE
EXAMPLES
OF
SYNTHESIZED
FACES
ADAPTED
FROM
MOHAMMED
ET
AL
QC
ACM
THE
OVERLAPPING
REGION
FIGURE
IMAGE
QUILTING
CAN
BE
THOUGHT
OF
AS
ANCESTRAL
SAMPLING
FROM
THE
DIRECTED
MODEL
FOR
IMAGES
FIGURE
THE
OBSERVED
DATA
XN
N
ARE
THE
OUTPUT
PATCHES
AND
THE
HIDDEN
LABELS
WN
N
REPRESENT
THE
PATCH
INDEX
THE
LABELS
ARE
CONDITIONED
ON
THEIR
PARENTS
WITH
A
PROBABILITY
DISTRIBUTION
THAT
ALLOTS
A
CONSTANT
PROBABILITY
IF
THE
OVERLAPPING
REGION
IS
ONE
OF
THE
J
CLOSEST
AND
ZERO
OTHERWISE
THE
ONLY
REAL
CHANGE
IS
THAT
THE
RELATIONSHIP
BETWEEN
LABEL
AND
OBSERVED
DATA
IS
NOW
DETERMINISTIC
A
GIVEN
LABEL
ALWAYS
PRODUCES
EXACTLY
THE
SAME
OUTPUT
PATCH
SYNTHESIZING
NOVEL
FACES
MOHAMMED
ET
AL
PRESENTED
A
RELATED
TECHNIQUE
TO
SYNTHESIZE
MORE
COMPLEX
OBJECTS
SUCH
AS
FRONTAL
FACES
FIGURE
BASED
ON
A
LARGE
DATABASE
OF
WEAKLY
ALIGNED
TRAINING
EXAMPLES
FACES
HAVE
A
DISTINCT
SPATIAL
STRUCTURE
AND
WE
MUST
FIGURE
GRAPHICAL
MODEL
FOR
SYN
THESIZING
NOVEL
FACES
WHEN
WE
GENER
ATE
A
NEW
IMAGE
WE
ARE
ANCESTRAL
SAM
PLING
FROM
A
DIRECTED
IMAGE
MODEL
WHERE
EACH
LABEL
W
IS
CONDITIONED
ON
THE
HIDDEN
VARIABLE
H
OF
THE
SUBSPACE
MODEL
ADAPTED
FROM
MOHAMMED
ET
AL
QC
ACM
ENSURE
THAT
OUR
MODEL
ENFORCES
THESE
CONSTRAINTS
TO
THIS
END
WE
BUILD
A
SEPARATE
LIBRARY
OF
PATCHES
FOR
EACH
POSITION
IN
THE
IMAGE
THIS
ENSURES
THAT
THE
FEATURES
HAVE
ROUGHLY
THE
CORRECT
SPATIAL
RELATIONS
THE
NOSE
ALWAYS
APPEARS
IN
THE
CENTER
AND
THE
CHIN
AT
THE
BOTTOM
IN
PRINCIPLE
WE
COULD
NOW
APPLY
A
STANDARD
IMAGE
QUILTING
APPROACH
BY
SYN
THESIZING
PATCHES
STARTING
IN
THE
TOP
LEFT
AND
MOVING
TO
THE
BOTTOM
RIGHT
UNFOR
TUNATELY
THE
RESULTING
FACES
CAN
DRIFT
IN
APPEARANCE
E
G
FROM
MALE
TO
FEMALE
AS
WE
MOVE
THROUGH
THE
IMAGE
TO
PREVENT
THIS
FROM
HAPPENING
WE
CONDITION
THE
PATCH
SYNTHESIS
ON
A
DRAW
FROM
A
FACTOR
ANALYSIS
MODEL
SECTION
WHICH
HAS
BEEN
TRAINED
WITH
FRONTAL
FACES
A
SAMPLE
FROM
THIS
MODEL
LOOKS
LIKE
A
BLURRY
BUT
GLOBALLY
COHERENT
FACE
NOW
WHEN
WE
CHOOSE
POTENTIAL
PATCHES
THEY
MUST
AGREE
WITH
BOTH
THE
PREVIOUSLY
PLACED
PATCHES
TO
THE
LEFT
AND
ABOVE
BUT
ALSO
BE
SIMILAR
TO
THE
APPROPRIATE
PART
OF
THE
BLURRY
SAMPLE
FROM
THE
SUBSPACE
MODEL
THE
GENERATED
IMAGES
FROM
THIS
MODEL
LOOK
LIKE
HIGHLY
REALISTIC
HUMAN
FACES
IN
TERMS
OF
PROBABILITY
THE
LABELS
WN
N
IN
THIS
MODEL
ARE
CONDITIONED
NOT
ONLY
ON
THEIR
ANCESTORS
WPA
BUT
ALSO
ON
THE
HIDDEN
VARIABLE
IN
THE
SUBSPACE
MODEL
H
THIS
CONNECTS
TO
EVERY
PATCH
LABEL
WN
N
AND
GIVES
THE
RESULTING
IMAGE
A
GREATER
VISUAL
COHERENCE
THAN
THE
MARKOV
CONNECTIONS
OF
THE
PATCHES
ALONE
DISCUSSION
MODELS
FOR
GRIDS
ARE
UBIQUITOUS
IN
VISION
THEY
OCCUR
IN
ALMOST
ALL
APPLICATIONS
THAT
ATTEMPT
TO
ASSOCIATE
A
LABEL
WITH
EACH
POSITION
IN
THE
IMAGE
DEPENDING
ON
THE
APPLICATION
THIS
LABEL
MAY
INDICATE
THE
DEPTH
OBJECT
TYPE
SEGMENTATION
MASK
OR
MOTION
AT
THAT
PIXEL
UNFORTUNATELY
MOST
PROBLEMS
OF
THIS
TYPE
ARE
NP
HARD
AND
SO
WE
MUST
RESORT
TO
EFFICIENT
APPROXIMATE
INFERENCE
TECHNIQUES
SUCH
AS
THE
ALPHA
EXPANSION
ALGORITHM
MODELS
FOR
GRIDS
NOTES
MRFS
AND
CRFS
MARKOV
RANDOM
FIELDS
WERE
FIRST
INVESTIGATED
IN
COMPUTER
VISION
BY
GEMAN
GEMAN
ALTHOUGH
MUCH
OF
THE
EARLY
WORK
DEALT
WITH
CONTINUOUS
VARIABLES
RATHER
THAN
THE
DISCRETE
CASE
AS
DISCUSSED
IN
THIS
CHAPTER
A
GOOD
REVIEW
CAN
BE
FOUND
IN
LI
CONDITIONAL
RANDOM
FIELDS
WERE
FIRST
USED
IN
COMPUTER
VISION
BY
KUMAR
HEBERT
AN
OVERVIEW
CAN
BE
FOUND
IN
SUTTON
MCCALLUM
APPLICATIONS
GRID
BASED
MODELS
AND
GRAPH
CUTS
ARE
USED
EXTENSIVELY
IN
VISION
AND
GRAPHICS
A
PARTIAL
LIST
OF
APPLICATIONS
INCLUDES
STEREO
VISION
KOLMOGOROV
ZABIH
WOODFORD
ET
AL
OPTICAL
FLOW
KOLMOGOROV
ZABIH
TEXTURE
SYNTHESIS
KWATRA
ET
AL
PHOTO
MONTAGE
AGARWALA
ET
AL
SUMMARIZING
PHOTO
COLLECTIONS
WITH
COLLAGES
ROTHER
ET
AL
ROTHER
ET
AL
BI
LAYER
SEGMENTATION
KOLMOGOROV
ET
AL
INTERACTIVE
SEGMENTATION
ROTHER
ET
AL
BOYKOV
ET
AL
SUPER
RESOLUTION
FREEMAN
ET
AL
IMAGE
RE
TARGETING
PRITCH
ET
AL
DENOISING
GREIG
ET
AL
OVER
SEGMENTATION
MOORE
ET
AL
VEKSLER
ET
AL
IMAGE
COLORIZATION
LEVIN
ET
AL
SEGMANTIC
SEGMENTATION
SHOTTON
ET
AL
MULTI
VIEW
RECONSTRUCTION
KOLMOGOROV
ZABIH
VOGIATZIS
ET
AL
AND
MATCHING
IMAGE
POINTS
ISACK
BOYKOV
GRAPH
CUTS
THE
FIRST
APPLICATION
OF
GRAPH
CUTS
TO
INFERENCE
IN
AN
MRF
IS
DUE
TO
GREIG
ET
AL
WHO
INVESTIGATED
BINARY
DENOISING
HOWEVER
IT
WAS
NOT
UNTIL
THE
WORK
OF
BOYKOV
ET
AL
THAT
THIS
RESULT
WAS
REDISCOVERED
AND
GRAPH
CUTS
BECAME
WIDELY
USED
ISHIKAWA
PRESENTED
THE
EXACT
SOLUTION
FOR
MULTI
LABEL
GRAPH
CUTS
WITH
CONVEX
POTENTIALS
AND
THIS
WAS
GENERALIZED
BY
SCHLESINGER
FLACH
THE
PRESENTATION
IN
THIS
CHAPTER
IS
A
HYBRID
OF
THESE
TWO
METHODS
BOYKOV
ET
AL
INTRODUCED
THE
IDEA
OF
OPTIMIZING
NON
CONVEX
MULTI
LABEL
ENERGIES
VIA
A
SERIES
OF
BINARY
PROBLEMS
THEY
PROPOSED
TWO
ALGORITHMS
OF
THIS
KIND
THE
ALPHA
BETA
SWAP
IN
WHICH
PAIRS
OF
LABELS
ARE
EXCHANGED
FOR
ONE
ANOTHER
AND
THE
ALPHA
EXPANSION
ALGORITHM
THEY
ALSO
PROVED
THAT
THE
ALPHA
EXPANSION
SOLUTION
IS
GUARANTEED
TO
BE
WITHIN
A
FACTOR
OF
TWO
OF
THE
TRUE
SOLUTION
IN
THE
SAME
SPIRIT
LEMPITSKY
ET
AL
AND
KUMAR
ET
AL
HAVE
PROPOSED
MORE
COMPLEX
MOVES
TARLOW
ET
AL
ELUCIDATES
THE
CONECTION
BETWEEN
GRAPH
CUT
METHODS
AND
MAX
PRODUCT
BELIEF
PROPAGATION
FOR
MORE
DETAILED
OVERVIEWS
OF
GRAPH
CUT
METHODS
CONSULT
BOYKOV
VEKSLER
FELZENSZWALB
ZABIH
AND
BLAKE
ET
AL
MAX
FLOW
GRAPH
CUT
METHODS
RELY
ON
ALGORITHMS
FOR
COMPUTING
MAXIMUM
FLOW
THE
MOST
COMMON
OF
THESE
ARE
THE
AUGMENTING
PATHS
METHOD
OF
FORD
FULKERSON
AND
THE
PUSH
RELABEL
METHOD
OF
GOLDBERG
TARJAN
DETAILS
OF
THESE
AND
OTHER
APPROACHES
TO
THE
SAME
PROBLEM
CAN
BE
FOUND
IN
ANY
STANDARD
TEXTBOOK
ON
ALGORITHMS
SUCH
AS
CORMEN
ET
AL
THE
MOST
COMMON
TECHNIQUE
IN
COMPUTER
VISION
IS
A
MODIFIED
VERSION
OF
THE
AUGMENTED
PATHS
ALGORITHM
DUE
TO
BOYKOV
KOLMOGOROV
THAT
HAS
BEEN
DEMONSTRATED
TO
HAVE
VERY
GOOD
PERFORMANCE
FOR
VISION
PROBLEMS
KOHLI
TORR
JUAN
BOYKOV
AND
ALAHARI
ET
AL
HAVE
ALL
INVESTIGATED
METHODS
FOR
IMPROVING
THE
EFFICIENCY
OF
GRAPH
CUTS
BY
REUSING
SOLUTIONS
TO
SIMILAR
GRAPH
CUT
PROBLEMS
E
G
BASED
ON
THE
SOLUTION
TO
THE
PREVIOUS
FRAMES
IN
A
TIME
SEQUENCE
COST
FUNCTIONS
AND
OPTIMIZATION
KOLMOGOROV
ZABIH
PROVIDE
A
SUMMARY
OF
THE
COST
FUNCTIONS
THAT
CAN
BE
OPTIMIZED
USING
THE
BASIC
GRAPH
CUTS
MAX
FLOW
FORMULATION
WITH
BINARY
VARIABLES
KOLMOGOROV
ROTHER
SUMMARIZE
GRAPH
CUT
APPROACHES
TO
NON
SUBMODULAR
ENERGIES
ROTHER
ET
AL
AND
KOMODAKIS
ET
AL
PRESENT
ALGORITHMS
THAT
CAN
APPROXIMATELY
OPTIMIZE
MORE
GENERAL
COST
FUNCTIONS
CONSTRAINT
EDGES
RECENT
WORK
HAS
INVESTIGATED
BESPOKE
GRAPH
CONSTRUCTIONS
THAT
MAKE
HEAVY
USE
OF
CONSTRAINT
EDGES
EDGES
OF
INFINITE
STRENGTH
TO
ENSURE
THAT
THE
SOLUTION
CONFORMS
TO
A
CERTAIN
STRUCTURE
FOR
EXAMPLE
DELONG
BOYKOV
DEVISED
A
METHOD
THAT
FORCED
CERTAIN
LABELS
TO
SURROUND
OTHERS
AND
MOORE
ET
AL
DESCRIBE
A
METHOD
THAT
FORCES
THE
LABEL
FIELD
TO
CONFORM
TO
A
LATTICE
SEE
ALSO
FELZENSZWALB
VEKSLER
FOR
A
RELATED
SCHEME
BASED
ON
DYNAMIC
PROGRAMMING
HIGHER
ORDER
CLIQUES
ALL
OF
THE
METHODS
DISCUSSED
IN
THIS
CHAPTER
ASSUME
PAIRWISE
CONNECTIONS
THE
CLIQUES
INCLUDE
ONLY
TWO
DISCRETE
VARIABLES
HOWEVER
TO
MODEL
MORE
COMPLEX
STATISTICS
OF
THE
LABEL
FIELD
IT
IS
NECESSARY
TO
INCLUDE
MORE
THAN
TWO
VARIABLES
IN
THE
CLIQUES
AND
THESE
ARE
KNOWN
AS
HIGHER
ORDER
MODELS
ROTH
BLACK
DEMONSTRATED
GOOD
DENOISING
AND
INPAINTING
RESULTS
WITH
A
CONTINUOUS
MRF
MODEL
OF
THIS
KIND
AND
DOMKE
ET
AL
DEMONSTRATED
THE
EFFICACY
OF
A
DIRECTED
MODEL
IN
WHICH
EACH
VARIABLE
WAS
CONDITIONED
ON
A
NUMBER
OF
VARIABLES
ABOVE
AND
TO
THE
RIGHT
IN
THE
IMAGE
THERE
HAS
RECENTLY
BEEN
CONSIDERABLE
INTEREST
IN
DEVELOPING
ALGORITHMS
FOR
MAP
ESTIMATION
IN
MODELS
WITH
DISCRETE
VARIABLES
AND
HIGHER
ORDER
CLIQUES
ISHIKAWA
KOHLI
ET
AL
KOHLI
ET
AL
ROTHER
ET
AL
OTHER
APPROACHES
TO
MAP
ESTIMATION
THERE
ARE
MANY
OTHER
CONTEMPORARY
AP
PROACHES
TO
MAP
ESTIMATION
IN
MRFS
AND
CRFS
THESE
INCLUDE
LOOPY
BELIEF
PROPAGATION
WEISS
FREEMAN
QUADRATIC
PSEUDO
BOOLEAN
OPTIMIZATION
WHICH
IS
USED
IN
NON
SUBMODULAR
COST
FUNCTIONS
KOLMOGOROV
ROTHER
RANDOM
WALKS
GRADY
AND
LINEAR
PROGRAMMING
LP
RELAXATIONS
WEISS
ET
AL
AND
VARIOUS
APPROACHES
TO
MAXIMIZE
THE
LP
LOWER
BOUND
SUCH
AS
TREE
REWEIGHTED
MESSAGE
PASSING
WAINRIGHT
ET
AL
KOLMOGOROV
AN
EXPERIMENTAL
COMPARISON
BETWEEN
DIFFERENT
ENERGY
MINI
MIZATION
METHODS
FOR
MRFS
CAN
BE
FOUND
IN
SZELISKI
ET
AL
TEXTURE
SYNTHESIS
TEXTURE
SYNTHESIS
WAS
ORIGINALLY
INVESTIGATED
AS
A
CONTINUOUS
PROB
LEM
AND
THE
FOCUS
WAS
ON
MODELING
THE
JOINT
STATISTICS
OF
THE
RGB
VALUES
IN
A
SMALL
PATCH
HEEGER
BERGEN
PORTILLA
SIMONCELLI
ALTHOUGH
TEXTURE
SYNTHESIS
AS
A
CON
TINUOUS
PROBLEM
IS
STILL
AN
ACTIVE
RESEARCH
AREA
E
G
HEESS
ET
AL
THESE
EARLY
METHODS
WERE
DISPLACED
BY
METHODS
THAT
REPRESENTED
THE
TEXTURE
IN
TERMS
OF
DISCRETE
VARIABLES
EI
THER
BY
QUANTIZING
THE
RGB
VALUES
INDEXING
PATCHES
OR
USING
A
SHIFT
MAP
REPRESENTATION
THE
RESULTING
ALGORITHMS
E
G
EFROS
LEUNG
WEI
LEVOY
EFROS
FREEMAN
KWATRA
ET
AL
WERE
ORIGINALLY
DESCRIBED
AS
HEURISTIC
APPROACHES
TO
GENERATING
TEXTURES
BUT
CAN
ALSO
BE
INTERPRETED
AS
EXACT
OR
APPROXIMATE
WAYS
TO
DRAW
SAMPLES
FROM
DIRECTED
OR
UNDIRECTED
GRID
MODELS
INTERACTIVE
SEGMENTATION
THE
USE
OF
GRAPH
CUTS
FOR
INTERACTIVE
SEGMENTATION
AL
GORITHMS
WAS
PIONEERED
BY
BOYKOV
JOLLY
IN
EARLY
WORKS
BOYKOV
JOLLY
BOYKOV
FUNKA
LEA
LI
ET
AL
THE
USER
INTERACTED
WITH
THE
IMAGE
BY
PLACING
MARKS
INDICATING
FOREGROUND
AND
BACKGROUND
REGIONS
GRAB
CUT
ROTHER
ET
AL
ALLOWED
THE
USER
TO
DRAW
A
BOX
AROUND
THE
OBJECT
IN
QUESTION
MORE
RECENT
SYSTEMS
LIU
ET
AL
ARE
FAST
ENOUGH
TO
ALLOW
THE
USER
TO
INTERACTIVELY
PAINT
THE
SELECTION
ONTO
THE
IMAGES
CURRENT
INTEREST
IN
GRAPH
CUT
BASED
SEGMENTATION
IS
MAINLY
FOCUSED
ON
DEVELOPING
NOVEL
PRIORS
OVER
THE
SHAPE
THAT
IMPROVE
PERFORMANCE
E
G
MALCOLM
ET
AL
VEKSLER
CHITTAJALLU
ET
AL
FREIMAN
ET
AL
TO
THIS
END
KUMAR
ET
AL
INTRODUCED
A
METHOD
FOR
IMPOSING
HIGH
LEVEL
KNOWLEDGE
ABOUT
THE
ARTICULA
TION
OF
THE
OBJECT
VICENTE
ET
AL
DEVELOPED
AN
ALGORITHM
THAT
IS
SUITED
FOR
CUTTING
OUT
ELONGATED
OBJECTS
AND
LEMPITSKY
ET
AL
USED
A
PRIOR
BASED
ON
A
BOUNDING
BOX
AROUND
THE
OBJECT
STEREO
VISION
MOST
STATE
OF
THE
ART
STEREO
VISION
ALGORITHMS
RELY
ON
MRFS
OR
CRFS
AND
ARE
SOLVED
USING
EITHER
GRAPH
CUTS
E
G
KOLMOGOROV
ZABIH
OR
BELIEF
PROPAGATION
E
G
SUN
ET
AL
COMPARISONS
OF
THESE
APPROACHES
CAN
BE
FOUND
IN
TAPPEN
FREEMAN
AND
SZELISKI
ET
AL
AN
ACTIVE
AREA
OF
RESEARCH
IN
DENSE
STEREO
VISION
IS
THE
FORMULATION
OF
THE
COMPATIBILITY
OF
THE
TWO
IMAGES
GIVEN
A
CERTAIN
DISPARITY
OFFSET
E
G
BLEYER
CHAMBON
HIRSCHMU
LLER
SCHARSTEIN
WHICH
IS
RARELY
BASED
ON
SINGLE
PIXELS
IN
PRACTICE
SEE
YOON
KWEON
TOMBARI
ET
AL
FOR
MORE
INFORMATION
ABOUT
STEREO
VISION
SEE
THE
REVIEWS
BY
SCHARSTEIN
SZELISKI
AND
BROWN
ET
AL
OR
CONSULT
SZELISKI
WHICH
CONTAINS
A
GOOD
MODERN
SUM
MARY
CHAPTER
OF
THIS
BOOK
SUMMARIZES
DYNAMIC
PROGRAMMING
APPROACHES
NOTABLE
STEREO
IMPLEMENTATIONS
INCLUDE
THE
REGION
GROWING
APPROACH
OF
LHUILLIER
QUAN
THE
SYSTEMS
OF
ZITNICK
KANADE
AND
HIRSCHMU
LLER
BOTH
OF
WHICH
ARE
AVAIL
ABLE
ONLINE
AND
THE
EXTREMELY
EFFICIENT
GPU
BASED
SYSTEM
OF
SIZINTSEV
WILDES
FOR
AN
UP
TO
DATE
QUANTITATIVE
COMPARISON
OF
THE
LATEST
STEREO
VISION
ALGORITHMS
CONSULT
THE
MIDDLEBURY
STEREO
VISION
WEBSITE
HTTP
VISION
MIDDLEBURY
EDU
STEREO
PROBLEMS
PROBLEM
CONSIDER
A
MARKOV
RANDOM
FIELD
WITH
THE
STRUCTURE
P
R
X
X
X
X
Φ
X
X
Φ
X
X
Φ
X
X
Φ
X
X
Z
BUT
WHERE
THE
VARIABLES
AND
ARE
CONTINUOUS
AND
THE
POTENTIALS
ARE
DEFINED
AS
Φ
A
B
EXP
A
B
THIS
IS
KNOWN
AS
A
GAUSSIAN
MARKOV
RANDOM
FIELD
SHOW
THAT
THE
JOINT
PROBABILITY
IS
A
NORMAL
DISTRIBUTION
AND
FIND
THE
INFORMATION
MATRIX
INVERSE
COVARIANCE
MATRIX
PROBLEM
COMPUTE
THE
MAP
SOLUTION
TO
THE
THREE
PIXEL
GRAPH
CUT
PROBLEM
IN
FIG
URE
BY
I
COMPUTING
THE
COST
OF
ALL
EIGHT
POSSIBLE
SOLUTIONS
EXPLICITLY
AND
FINDING
THE
ONE
WITH
THE
MINIMUM
COST
II
RUNNING
THE
AUGMENTING
PATHS
ALGORITHM
ON
THIS
GRAPH
BY
HAND
AND
INTERPRETING
THE
MINIMUM
CUT
FIGURE
GRAPH
FOR
PROBLEM
PROBLEM
EXPLICITLY
COMPUTE
THE
COSTS
ASSOCIATED
WITH
THE
FOUR
POSSIBLE
MINIMUM
CUTS
OF
THE
GRAPH
IN
FIGURE
PROBLEM
COMPUTE
THE
COST
FOR
EACH
THE
FOUR
POSSIBLE
CUTS
OF
THE
GRAPH
IN
FIG
URE
PROBLEM
CONSIDER
THE
GRAPH
CONSTRUCTION
IN
FIGURE
WHICH
CONTAINS
A
NUMBER
OF
CONSTRAINT
EDGES
OF
INFINITE
COST
CAPACITY
THERE
ARE
POSSIBLE
MINIMUM
CUTS
ON
THIS
GRAPH
EACH
OF
WHICH
CORRESPONDS
TO
ONE
POSSIBLE
LABELING
OF
THE
TWO
PIXELS
WRITE
OUT
THE
COST
FOR
EACH
LABELING
WHICH
SOLUTIONS
HAVE
FINITE
COST
FOR
THIS
GRAPH
CONSTRUCTION
SOURCE
B
SOURCE
B3
SINK
T
SINK
T
FIGURE
ALTERNATIVE
MULTI
LABEL
GRAPH
CONSTRUCTIONS
EACH
OF
THESE
THESE
GRAPHS
HAS
EXTRA
CONSTRAINT
LINKS
WITH
INFINITE
WEIGHT
THESE
HAVE
THE
EFFECT
OF
GIVING
AN
INFINITE
COST
TO
A
SUBSET
OF
THE
POSSIBLE
SOLUTIONS
PROBLEM
WHICH
OF
THE
POSSIBLE
MINIMUM
CUTS
OF
THE
GRAPH
IN
FIGURE
HAVE
A
FINITE
COST
PROBLEM
CONFIRM
THAT
THE
COSTS
OF
THE
CUTS
IN
FIGURE
ARE
AS
CLAIMED
BY
EXPLICITLY
PERFORMING
THE
SUMMATION
OVER
THE
RELEVANT
TERMS
CIJ
PROBLEM
SHOW
THAT
THE
POTTS
MODEL
FIGURE
IS
NOT
SUBMODULAR
BY
PROVIDING
A
COUNTER
EXAMPLE
TO
THE
REQUIRED
CRITERION
PAB
Β
Γ
PAB
Α
Δ
PAB
Β
Δ
PAB
Α
Γ
PROBLEM
AN
ALTERNATIVE
TO
THE
ALPHA
EXPANSION
ALGORITHM
IS
THE
ALPHA
BETA
SWAP
HERE
A
MULTI
LABEL
MRF
WITH
NON
CONVEX
POTENTIALS
IS
OPTIMIZED
BY
REPEATEDLY
CHOOSING
PAIRS
OF
LABELS
Α
Β
AND
PERFORMING
A
BINARY
GRAPH
CUT
THAT
ALLOWS
THEM
TO
SWAP
IN
SUCH
A
WAY
THAT
THE
OVERALL
COST
FUNCTION
DECREASES
DEVISE
A
GRAPH
STRUCTURE
THAT
CAN
BE
USED
TO
PERFORM
THIS
OPERATION
HINT
CONSIDER
SEPARATE
CASES
FOR
NEIGHBORING
LABELS
Α
Α
Β
Β
Β
Γ
Α
Γ
AND
Γ
Γ
WHERE
Γ
IS
A
LABEL
THAT
IS
NEITHER
Α
NOR
Β
PART
IV
PREPROCESSING
PART
IV
PREPROCESSING
THE
MAIN
FOCUS
OF
THIS
BOOK
IS
ON
STATISTICAL
MODELS
FOR
COMPUTER
VISION
THE
PRE
VIOUS
CHAPTERS
CONCERN
MODELS
THAT
RELATE
VISUAL
MEASUREMENTS
X
TO
THE
WORLD
W
HOWEVER
THERE
HAS
BEEN
LITTLE
DISCUSSION
OF
HOW
THE
MEASUREMENT
VECTOR
X
WAS
CREATED
AND
IT
HAS
OFTEN
BEEN
IMPLIED
THAT
IT
CONTAINS
CONCATENATED
RGB
PIXEL
VALUES
IN
STATE
OF
THE
ART
VISION
SYSTEMS
THE
IMAGE
PIXEL
DATA
ARE
ALMOST
ALWAYS
PREPROCESSED
TO
FORM
THE
MEASUREMENT
VECTOR
WE
DEFINE
PREPROCESSING
TO
BE
ANY
TRANSFORMATION
OF
THE
PIXEL
DATA
PRIOR
TO
BUILDING
THE
MODEL
THAT
RELATES
THE
DATA
TO
THE
WORLD
SUCH
TRANSFORMATIONS
ARE
OFTEN
AD
HOC
HEURISTICS
THEIR
PARAMETERS
ARE
NOT
LEARNED
FROM
TRAINING
DATA
BUT
THEY
ARE
CHOSEN
BASED
ON
EXPERIENCE
OF
WHAT
WORKS
WELL
THE
PHILOSOPHY
BEHIND
IMAGE
PREPROCESSING
IS
EASY
TO
UNDERSTAND
THE
IMAGE
DATA
MAY
BE
CONTINGENT
ON
MANY
ASPECTS
OF
THE
REAL
WORLD
THAT
DO
NOT
PERTAIN
TO
THE
TASK
AT
HAND
FOR
EXAMPLE
IN
AN
OBJECT
DETECTION
TASK
THE
RGB
VALUES
WILL
CHANGE
DEPENDING
ON
THE
CAMERA
GAIN
ILLUMINATION
OBJECT
POSE
AND
PARTICULAR
INSTANCE
OF
THE
OBJECT
THE
GOAL
OF
IMAGE
PREPROCESSING
IS
TO
REMOVE
AS
MUCH
OF
THIS
UNWANTED
VARIATION
AS
POSSIBLE
WHILE
RETAINING
THE
ASPECTS
OF
THE
IMAGE
THAT
ARE
CRITICAL
TO
THE
FINAL
DECISION
IN
A
SENSE
THE
NEED
FOR
PREPROCESSING
REPRESENTS
A
FAILURE
WE
ARE
ADMITTING
THAT
WE
CANNOT
DIRECTLY
MODEL
THE
RELATIONSHIP
BETWEEN
THE
RGB
VALUES
AND
THE
WORLD
STATE
INEVITABLY
WE
MUST
PAY
A
PRICE
FOR
THIS
ALTHOUGH
THE
VARIATION
DUE
TO
EXTRANEOUS
FACTORS
IS
JETTISONED
IT
IS
VERY
PROBABLE
THAT
SOME
OF
THE
TASK
RELATED
IN
FORMATION
IS
ALSO
DISCARDED
FORTUNATELY
IN
THESE
NASCENT
YEARS
OF
COMPUTER
VISION
THIS
RARELY
SEEMS
TO
BE
THE
LIMITING
FACTOR
THAT
GOVERNS
THE
OVERALL
PERFORMANCE
WE
DEVOTE
THE
SINGLE
CHAPTER
IN
THIS
SECTION
TO
DISCUSSING
A
VARIETY
OF
PREPRO
CESSING
TECHNIQUES
ALTHOUGH
THE
TREATMENT
HERE
IS
NOT
EXTENSIVE
IT
SHOULD
BE
EMPHASIZED
THAT
PREPROCESSING
IS
VERY
IMPORTANT
IN
PRACTICE
THE
CHOICE
OF
PREPRO
CESSING
TECHNIQUE
CAN
INFLUENCE
THE
PERFORMANCE
OF
VISION
SYSTEMS
AT
LEAST
AS
MUCH
AS
THE
CHOICE
OF
MODEL
CHAPTER
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
THIS
CHAPTER
PROVIDES
A
BRIEF
OVERVIEW
OF
MODERN
PREPROCESSING
METHODS
FOR
COM
PUTER
VISION
IN
SECTION
WE
INTRODUCE
METHODS
IN
WHICH
WE
REPLACE
EACH
PIXEL
IN
THE
IMAGE
WITH
A
NEW
VALUE
SECTION
CONSIDERS
THE
PROBLEM
OF
FINDING
AND
CHARACTERIZING
EDGES
CORNERS
AND
INTEREST
POINTS
IN
IMAGES
IN
SECTION
WE
DISCUSS
VISUAL
DESCRIPTORS
THESE
ARE
LOW
DIMENSIONAL
VECTORS
THAT
ATTEMPT
TO
CHAR
ACTERIZE
THE
INTERESTING
ASPECTS
OF
AN
IMAGE
REGION
IN
A
COMPACT
WAY
FINALLY
IN
SECTION
WE
DISCUSS
METHODS
FOR
DIMENSIONALITY
REDUCTION
PER
PIXEL
TRANSFORMATIONS
WE
START
OUR
DISCUSSION
OF
PREPROCESSING
WITH
PER
PIXEL
OPERATIONS
THESE
METHODS
RETURN
A
SINGLE
VALUE
CORRESPONDING
TO
EACH
PIXEL
OF
THE
INPUT
IMAGE
WE
DENOTE
THE
ORIGINAL
ARRAY
OF
PIXEL
DATA
AS
P
WHERE
PIJ
IS
THE
ELEMENT
AT
THE
ITH
OF
I
ROWS
AND
THE
JTH
OF
J
COLUMNS
THE
ELEMENT
PIJ
IS
A
SCALAR
REPRESENTING
THE
GRAYSCALE
INTENSITY
PER
PIXEL
OPERATIONS
RETURN
A
NEW
ARRAY
X
OF
THE
SAME
SIZE
AS
P
CONTAINING
ELEMENTS
XIJ
WHITENING
THE
GOAL
OF
WHITENING
FIGURE
IS
TO
PROVIDE
INVARIANCE
TO
FLUCTUATIONS
IN
THE
MEAN
INTENSITY
LEVEL
AND
CONTRAST
OF
THE
IMAGE
SUCH
VARIATION
MAY
ARISE
BECAUSE
OF
A
CHANGE
IN
AMBIENT
LIGHTING
INTENSITY
THE
OBJECT
REFLECTANCE
OR
THE
CAMERA
GAIN
TO
COMPENSATE
FOR
THESE
FACTORS
THE
IMAGE
IS
TRANSFORMED
SO
THAT
THE
RESULTING
PIXEL
VALUES
HAVE
ZERO
MEAN
AND
UNIT
VARIANCE
TO
THIS
END
WE
COMPUTE
THE
MEAN
Μ
AND
VARIANCE
OF
THE
ORIGINAL
GRAYSCALE
IMAGE
P
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
A
B
C
FIGURE
WHITENING
AND
HISTOGRAM
EQUALIZATION
A
A
NUMBER
OF
FACES
WHICH
HAVE
BEEN
CAPTURED
WITH
WIDELY
VARYING
CONTRASTS
AND
MEAN
LEVELS
B
AFTER
WHITENING
THE
IMAGES
HAVE
THE
SAME
MEAN
AND
VARIANCE
C
AFTER
HISTOGRAM
EQUALIZATION
THE
DISTRIBUTION
OF
GRAY
VALUES
IS
APPROXIMATELY
UNI
FORM
BOTH
OF
THESE
TRANSFORMATIONS
REDUCE
THE
AMOUNT
OF
VARIATION
DUE
TO
CONTRAST
AND
INTENSITY
CHANGES
Μ
I
I
I
I
J
J
IJ
J
J
PIJ
PIJ
Μ
IJ
THESE
STATISTICS
ARE
USED
TO
TRANSFORM
EACH
PIXEL
VALUE
SEPARATELY
SO
THAT
XIJ
PIJ
Μ
Σ
FOR
COLOR
IMAGES
THIS
OPERATION
MAY
BE
CARRIED
OUT
BY
COMPUTING
THE
STATISTICS
Μ
AND
FROM
ALL
THREE
CHANNELS
OR
BY
SEPARATELY
TRANSFORMING
EACH
OF
THE
RGB
CHANNELS
BASED
ON
THEIR
OWN
STATISTICS
NOTE
THAT
EVEN
THIS
SIMPLE
TRANSFORM
HAS
THE
POTENTIAL
TO
HAMPER
SUBSEQUENT
INFERENCE
ABOUT
THE
SCENE
DEPENDING
ON
THE
TASK
THE
ABSOLUTE
INTENSITIES
MAY
OR
MAY
NOT
CONTAIN
CRITICAL
INFORMATION
EVEN
THE
SIMPLEST
PREPROCESSING
METHODS
MUST
BE
APPLIED
WITH
CARE
HISTOGRAM
EQUALIZATION
THE
GOAL
OF
HISTOGRAM
EQUALIZATION
FIGURE
IS
TO
MODIFY
THE
STATISTICS
OF
THE
INTENSITY
VALUES
SO
THAT
ALL
OF
THEIR
MOMENTS
TAKE
PREDEFINED
VALUES
TO
THIS
END
A
NONLINEAR
TRANSFORMATION
IS
APPLIED
THAT
FORCES
THE
DISTRIBUTION
OF
PIXEL
INTENSITIES
TO
BE
FLAT
WE
FIRST
COMPUTE
THE
HISTOGRAM
OF
THE
ORIGINAL
INTENSITIES
H
WHERE
THE
KTH
OF
K
ENTRIES
IS
GIVEN
BY
FIGURE
HISTOGRAM
EQUALIZATION
THE
ABSCISSA
INDICATES
THE
PIXEL
INTEN
SITY
THE
ORDINATE
INDICATES
THE
PRO
PORTION
OF
INTENSITIES
THAT
WERE
LESS
THAN
OR
EQUAL
TO
THIS
VALUE
THIS
PLOT
CAN
BE
USED
AS
A
LOOK
UP
TABLE
FOR
HISTOGRAM
EQUALIZING
THE
INTENSITIES
FOR
A
GIVEN
INTENSITY
VALUE
ON
THE
AB
SCISSA
WE
CHOOSE
THE
NEW
INTENSITY
TO
BE
THE
MAXIMUM
OUTPUT
INTENSITY
K
TIMES
THE
VALUE
ON
THE
ORDINATE
AFTER
THIS
TRANSFORMATION
THE
INTENSITIES
ARE
EQUALLY
DISTRIBUTED
IN
THE
EXAMPLE
IMAGE
MANY
OF
THE
PIXELS
ARE
BRIGHT
HISTOGRAM
EQUALIZATION
SPREADS
THESE
BRIGHT
VALUES
OUT
OVER
A
LARGER
INTEN
SITY
RANGE
AND
SO
HAS
THE
EFFECT
OF
IN
CREASING
THE
CONTRAST
IN
THE
BRIGHTER
REGIONS
I
J
HK
Δ
PIJ
K
I
J
WHERE
THE
OPERATION
Δ
RETURNS
ONE
IF
THE
ARGUMENT
IS
ZERO
AND
ZERO
OTHERWISE
WE
THEN
CUMULATIVELY
SUM
THIS
HISTOGRAM
AND
NORMALIZE
BY
THE
TOTAL
NUMBER
OF
PIXELS
TO
COMPUTE
THE
CUMULATIVE
PROPORTION
C
OF
PIXELS
THAT
ARE
LESS
THAN
OR
EQUAL
TO
EACH
INTENSITY
LEVEL
CK
K
L
IJ
HL
FINALLY
WE
USE
THE
CUMULATIVE
HISTOGRAM
AS
A
LOOK
UP
TABLE
TO
COMPUTE
THE
TRANS
FORMED
VALUE
SO
THAT
XIJ
KCPIJ
FOR
EXAMPLE
IN
FIGURE
THE
VALUE
WILL
BE
MAPPED
TO
K
WHERE
K
IS
THE
MAXIMUM
INTENSITY
USUALLY
THE
RESULT
IS
A
CONTINUOUS
NUMBER
RATHER
THAN
A
DISCRETIZED
PIXEL
INTENSITY
BUT
IS
IN
THE
SAME
RANGE
AS
THE
ORIGINAL
DATA
THE
RESULT
CAN
BE
ROUNDED
TO
THE
NEAREST
INTEGER
IF
SUBSEQUENT
PROCESSING
DEMANDS
LINEAR
FILTERING
AFTER
FILTERING
AN
IMAGE
THE
NEW
PIXEL
VALUE
XIJ
CONSISTS
OF
A
WEIGHTED
SUM
OF
THE
INTENSITIES
OF
PIXELS
IN
THE
SURROUNDING
AREA
OF
THE
ORIGINAL
IMAGE
P
THE
WEIGHTS
ARE
STORED
IN
A
FILTER
KERNEL
F
WHICH
HAS
ENTRIES
FM
N
WHERE
M
M
M
AND
N
N
N
MORE
FORMALLY
WHEN
WE
APPLY
A
FILTER
WE
CONVOLVE
THE
P
WITH
THE
FILTER
F
WHERE
TWO
DIMENSIONAL
CONVOLUTION
IS
DEFINED
AS
PROBLEM
FIGURE
IMAGE
BLURRING
A
ORIGINAL
IMAGE
B
RESULT
OF
CONVOLVING
WITH
A
GAUSSIAN
FILTER
FILTER
SHOWN
IN
BOTTOM
RIGHT
OF
IMAGE
EACH
PIXEL
IN
THIS
IMAGE
IS
A
WEIGHTED
SUM
OF
THE
SURROUNDING
PIXELS
IN
THE
ORIGINAL
IMAGE
WHERE
THE
WEIGHTS
ARE
GIVEN
BY
THE
FILTER
THE
RESULT
IS
THAT
THE
IMAGE
IS
SLIGHTLY
BLURRED
C
E
CONVOLVING
WITH
A
FILTER
OF
INCREASING
STANDARD
DEVIATION
CAUSES
THE
RESULTING
IMAGE
TO
BE
INCREASINGLY
BLURRED
XIJ
PI
M
J
NFM
N
PROBLEM
M
M
N
N
NOTICE
THAT
BY
CONVENTION
THE
FILTER
IS
FLIPPED
IN
BOTH
DIRECTIONS
SO
THE
TOP
LEFT
OF
THE
FILTER
F
M
N
WEIGHTS
THE
PIXEL
PI
M
J
N
TO
THE
RIGHT
AND
BELOW
THE
CURRENT
POINT
IN
P
MANY
FILTERS
USED
IN
VISION
ARE
SYMMETRIC
IN
SUCH
A
WAY
THAT
THIS
FLIPPING
MAKES
NO
PRACTICAL
DIFFERENCE
WITHOUT
FURTHER
MODIFICATION
THIS
FORMULATION
WILL
RUN
INTO
PROBLEMS
NEAR
THE
BORDERS
OF
THE
IMAGE
IT
NEEDS
TO
ACCESS
POINTS
THAT
ARE
OUTSIDE
THE
IMAGE
ONE
WAY
TO
DEAL
WITH
THIS
IS
TO
USE
ZERO
PADDING
IN
WHICH
IT
IS
ASSUMED
THAT
THE
VALUE
OF
P
IS
OUTSIDE
THE
DEFINED
IMAGE
REGION
WE
NOW
CONSIDER
A
NUMBER
OF
COMMON
TYPES
OF
FILTER
GAUSSIAN
BLURRING
FILTER
TO
BLUR
AN
IMAGE
WE
CONVOLVE
IT
WITH
A
GAUSSIAN
F
M
N
EXP
PROBLEM
EACH
PIXEL
IN
THE
RESULTING
IMAGE
IS
A
WEIGHTED
SUM
OF
THE
SURROUNDING
PIXELS
WHERE
THE
WEIGHTS
DEPEND
ON
THE
GAUSSIAN
PROFILE
NEARER
PIXELS
CONTRIBUTE
RELATIVELY
MORE
TO
THE
FINAL
OUTPUT
THIS
PROCESS
BLURS
THE
IMAGE
WHERE
THE
DEGREE
OF
BLURRING
IS
DEPENDENT
ON
THE
STANDARD
DEVIATION
Σ
OF
THE
GAUSSIAN
FILTER
FIGURE
THIS
IS
A
SIMPLE
METHOD
TO
REDUCE
NOISE
IN
IMAGES
TAKEN
AT
VERY
LOW
LIGHT
LEVELS
FIRST
DERIVATIVE
FILTERS
AND
EDGE
FILTERS
A
SECOND
USE
FOR
IMAGE
FILTERING
IS
TO
LOCATE
PLACES
IN
THE
IMAGE
WHERE
THE
INTENSITY
CHANGES
ABRUPTLY
CONSIDER
TAKING
THE
FIRST
DERIVATIVE
OF
THE
IMAGE
ALONG
THE
ROWS
WE
COULD
APPROXIMATE
THIS
OPERATION
BY
SIMPLY
COMPUTING
THE
DIFFERENCE
BETWEEN
TWO
OFFSET
PIXELS
ALONG
THE
ROW
THIS
OPERATION
CAN
BE
ACCOMPLISHED
BY
FILTERING
WITH
THE
OPERATOR
F
THIS
FILTER
GIVES
ZERO
RESPONSE
WHEN
THE
IMAGE
IS
FLAT
IN
THE
HORIZONTAL
DIRECTION
IT
IS
HENCE
INVARIANT
TO
CONSTANT
ADDITIVE
LUMINANCE
CHANGES
IT
GIVES
A
NEGATIVE
RESPONSE
WHEN
THE
IMAGE
PIXEL
VALUES
ARE
INCREASING
AS
WE
MOVE
IN
THE
HORIZONTAL
DIRECTION
AND
A
POSITIVE
RESPONSE
WHEN
THEY
ARE
DECREASING
RECALL
THAT
CONVOLUTION
FLIPS
THE
FILTER
BY
AS
SUCH
IT
IS
SELECTIVE
FOR
EDGES
IN
THE
IMAGE
THE
RESPONSE
TO
THE
FILTER
F
IS
NOISY
BECAUSE
OF
ITS
LIMITED
SPATIAL
EXTENT
CONSEQUENTLY
SLIGHTLY
MORE
SOPHISTICATED
FILTERS
ARE
USED
TO
FIND
EDGES
IN
PRACTICE
EXAMPLES
INCLUDE
THE
PREWITT
OPERATORS
FIGURES
B
PROBLEM
PROBLEM
AND
THE
SOBEL
OPERATORS
WHERE
IN
EACH
CASE
THE
FILTER
FX
IS
A
FILTER
SELECTIVE
FOR
EDGES
IN
THE
HORIZONTAL
DIRECTION
AND
FY
IS
A
FILTER
SELECTIVE
FOR
EDGES
IN
THE
VERTICAL
DIRECTION
LAPLACIAN
FILTERS
THE
LAPLACIAN
FILTER
IS
THE
DISCRETE
TWO
DIMENSIONAL
APPROXIMATION
TO
THE
LAPLACIAN
OPERATOR
AND
IS
GIVEN
BY
F
APPLYING
THE
DISCRETIZED
FILTER
F
TO
AN
IMAGE
RESULTS
IN
A
RESPONSE
OF
HIGH
MAG
NITUDE
WHERE
THE
IMAGE
IS
CHANGING
REGARDLESS
OF
THE
DIRECTION
OF
THAT
CHANGE
FIGURE
THE
RESPONSE
IS
ZERO
IN
REGIONS
THAT
ARE
FLAT
AND
SIGNIFICANT
WHERE
EDGES
OCCUR
IN
THE
IMAGE
IT
IS
HENCE
INVARIANT
TO
CONSTANT
ADDITIVE
CHANGES
IN
LUMINANCE
AND
USEFUL
FOR
IDENTIFYING
INTERESTING
REGIONS
OF
THE
IMAGE
LAPLACIAN
OF
GAUSSIAN
FILTERS
IN
PRACTICE
THE
LAPLACIAN
OPERATOR
PRODUCES
NOISY
RESULTS
A
SUPERIOR
APPROACH
IS
TO
FIRST
SMOOTH
THE
IMAGE
WITH
A
GAUSSIAN
FILTER
AND
THEN
APPLY
THE
LAPLACIAN
DUE
TO
THE
ASSOCIATIVE
PROPERTY
OF
CONVOLUTION
WE
CAN
EQUIVALENTLY
CONVOLVE
THE
LAPLACIAN
FILTER
BY
A
GAUSSIAN
AND
APPLY
THE
RESULTING
LAPLACIAN
OF
GAUSSIAN
FILTER
TO
THE
IMAGE
FIGURE
THIS
LAPLACIAN
OF
GAUSSIAN
HAS
THE
ADVANTAGE
THAT
IT
FIGURE
IMAGE
FILTERING
WITH
FIRST
AND
SECOND
DERIVATIVE
OPERATORS
THE
ORIGINAL
IMAGE
IS
SHOWN
IN
FIGURE
A
CONVOLVING
WITH
THE
VERTICAL
PREWITT
FILTER
PRODUCES
A
RESPONSE
THAT
IS
PROPORTIONAL
TO
THE
SIZE
AND
POLARITY
OF
EDGES
IN
THE
VERTICAL
DIRECTION
B
THE
HORIZONTAL
PREWITT
FILTER
PRODUCES
A
RESPONSE
TO
EDGES
IN
THE
HORIZONTAL
DIRECTION
C
THE
LAPLACIAN
FILTER
GIVES
A
SIGNIFICANT
RESPONSE
WHERE
THE
IMAGE
CHANGES
RAPIDLY
REGARDLESS
OF
DIRECTION
D
THE
LAPLACIAN
OF
GAUSSIAN
FILTER
PRODUCES
SIMILAR
RESULTS
BUT
THE
OUTPUT
IS
SMOOTHED
AND
HENCE
LESS
NOISY
E
THE
DIFFERENCE
OF
GAUSSIANS
FILTER
IS
A
COMMON
APPROXIMATION
TO
THE
LAPLACIAN
OF
GAUSSIAN
CAN
BE
TUNED
TO
BE
SELECTIVE
FOR
CHANGES
AT
DIFFERENT
SCALES
DEPENDING
ON
THE
SCALE
OF
THE
GAUSSIAN
COMPONENT
DIFFERENCE
OF
GAUSSIANS
THE
LAPLACIAN
OF
GAUSSIAN
FILTER
IS
VERY
WELL
APPROXIMATED
BY
THE
DIFFERENCE
OF
GAUSSIANS
FILTER
COMPARE
FIGURES
AND
AS
THE
NAME
IMPLIES
THIS
FILTER
IS
CREATED
BY
TAKING
THE
DIFFERENCE
OF
TWO
GAUSSIANS
AT
NEARBY
SCALES
THE
SAME
RESULT
CAN
BE
ACHIEVED
BY
FILTERING
THE
IMAGE
WITH
THE
TWO
GAUSSIANS
SEPARATELY
AND
TAKING
THE
DIFFERENCE
BETWEEN
THE
RESULTS
AGAIN
THIS
FILTER
RESPONDS
STRONGLY
IN
REGIONS
OF
THE
IMAGE
THAT
ARE
CHANGING
AT
A
PREDETERMINED
SCALE
GABOR
FILTERS
GABOR
FILTERS
ARE
SELECTIVE
FOR
BOTH
SCALE
AND
ORIENTATION
THE
GABOR
FUNCTION
IS
THE
PRODUCT
OF
A
GAUSSIAN
WITH
A
SINUSOID
IT
IS
PARAMETERIZED
BY
THE
COVARIANCE
OF
THE
GAUSSIAN
AND
THE
PHASE
Φ
ORIENTATION
Ω
AND
WAVELENGTH
Λ
OF
THE
SINE
WAVE
IF
THE
GAUSSIAN
COMPONENT
IS
SPHERICAL
IT
IS
DEFINED
BY
FMN
EXP
SIN
COS
Ω
M
SIN
Ω
N
Φ
Λ
WHERE
Σ
CONTROLS
THE
SCALE
OF
THE
SPHERICAL
GAUSSIAN
IT
IS
TYPICAL
TO
MAKE
THE
WAVELENGTH
PROPORTIONAL
TO
THE
SCALE
Σ
OF
THE
GAUSSIAN
SO
A
CONSTANT
NUMBER
OF
CYCLES
IS
VISIBLE
THE
GABOR
FILTER
IS
SELECTIVE
FOR
ELEMENTS
WITHIN
THE
IMAGE
AT
A
CERTAIN
FREQUENCY
AND
ORIENTATION
BAND
AND
WITH
A
CERTAIN
PHASE
FIGURE
IT
IS
INVARIANT
TO
CON
STANT
ADDITIVE
CHANGES
IN
LUMINANCE
WHEN
THE
SINUSOIDAL
COMPONENT
IS
ASYMMETRIC
THIS
IS
ALSO
NEARLY
TRUE
FOR
SYMMETRIC
GABOR
FUNCTIONS
AS
LONG
AS
SEVERAL
CYCLES
OF
THE
SINUSOID
ARE
VISIBLE
A
RESPONSE
THAT
IS
INDEPENDENT
OF
PHASE
CAN
EASILY
BE
GENERATED
BY
SQUARING
AND
SUMMING
THE
RESPONSES
OF
TWO
GABOR
FEATURES
WITH
THE
SAME
FREQUENCY
ORIENTATION
AND
SCALE
BUT
WITH
PHASES
THAT
ARE
Π
RADIANS
APART
THE
RESULTING
QUANTITY
IS
TERMED
THE
GABOR
ENERGY
AND
IS
SOMEWHAT
INVARIANT
TO
SMALL
DISPLACEMENTS
OF
THE
IMAGE
FILTERING
WITH
GABOR
FUNCTIONS
IS
MOTIVATED
BY
MAMMALIAN
VISUAL
PERCEPTION
THIS
IS
ONE
OF
THE
FIRST
PROCESSING
OPERATIONS
APPLIED
TO
VISUAL
DATA
IN
THE
BRAIN
MOREOVER
IT
IS
KNOWN
FROM
PSYCHOLOGICAL
STUDIES
THAT
CERTAIN
TASKS
E
G
FACE
DE
TECTION
ARE
PREDOMINANTLY
DEPENDENT
ON
INFORMATION
AT
INTERMEDIATE
FREQUENCIES
THIS
MAY
BE
BECAUSE
HIGH
FREQUENCY
FILTERS
SEE
ONLY
A
SMALL
IMAGE
REGION
AND
ARE
HENCE
NOISY
AND
RELATIVELY
UNINFORMATIVE
AND
LOW
FREQUENCY
FILTERS
ACT
OVER
A
LARGE
REGION
AND
RESPOND
DISPROPORTIONATELY
TO
SLOW
CHANGES
DUE
TO
LIGHTING
HAAR
LIKE
FILTERS
HAAR
LIKE
FILTERS
CONSIST
OF
ADJACENT
RECTANGULAR
REGIONS
THAT
ARE
BALANCED
SO
THAT
THE
AVERAGE
FILTER
VALUE
IS
ZERO
AND
THEY
ARE
INVARIANT
TO
CONSTANT
LUMINANCE
CHANGES
DEPENDING
ON
THE
CONFIGURATION
OF
THESE
REGIONS
THEY
MAY
BE
SIMILAR
TO
DERIVATIVE
OR
GABOR
FILTERS
FIGURE
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
FIGURE
FILTERING
WITH
GABOR
FUNCTIONS
A
ORIGINAL
IMAGE
B
AFTER
FILTERING
WITH
HORIZONTAL
ASYMMETRIC
GABOR
FUNCTION
AT
A
LARGE
SCALE
FILTER
SHOWN
BOTTOM
RIGHT
C
RESULT
OF
FILTERING
WITH
HORIZONTAL
SYMMETRIC
GABOR
FUNCTION
AT
A
LARGE
SCALE
D
FILTERING
WITH
VERTICAL
GABOR
FILTER
RESPONDS
TO
VERTICAL
CHANGES
E
RESPONSE
TO
DIAGONAL
GABOR
FUNCTION
HOWEVER
HAAR
LIKE
FILTERS
ARE
NOISIER
THAN
THE
FILTERS
THEY
APPROXIMATE
THEY
HAVE
SHARP
EDGES
BETWEEN
POSITIVE
AND
NEGATIVE
REGIONS
AND
SO
MOVING
BY
A
SINGLE
PIXEL
NEAR
AN
EDGE
MAY
CHANGE
THE
RESPONSE
SIGNIFICANTLY
THIS
DRAWBACK
IS
COM
PENSATED
FOR
BY
THE
RELATIVE
SPEED
WITH
WHICH
HAAR
FUNCTIONS
CAN
BE
COMPUTED
TO
COMPUTE
HAAR
FUNCTIONS
RAPIDLY
WE
FIRST
FORM
THE
INTEGRAL
IMAGE
FIGURE
THIS
IS
AN
INTERMEDIATE
REPRESENTATION
IN
WHICH
EACH
PIXEL
CONTAINS
THE
SUM
OF
ALL
OF
THE
INTENSITY
VALUES
ABOVE
AND
TO
THE
LEFT
OF
THE
CURRENT
POSITION
SO
THE
VALUE
IN
THE
TOP
LEFT
CORNER
IS
THE
ORIGINAL
PIXEL
VALUE
AT
THAT
POSITION
AND
THE
VALUE
IN
THE
BOTTOM
RIGHT
CORNER
IS
THE
SUM
OF
ALL
OF
THE
PIXEL
VALUES
IN
THE
IMAGE
THE
VALUES
IN
THE
OTHER
PARTS
OF
THE
INTEGRAL
IMAGE
ARE
BETWEEN
THESE
EXTREMES
GIVEN
THE
INTEGRAL
IMAGE
I
IT
IS
POSSIBLE
TO
COMPUTE
THE
SUM
OF
THE
INTENSITIES
IN
ANY
RECTANGULAR
REGION
WITH
JUST
FOUR
OPERATIONS
REGARDLESS
OF
HOW
LARGE
THIS
REGION
IS
CONSIDER
THE
REGION
IS
DEFINED
BY
THE
RANGE
DOWN
THE
COLUMNS
AND
ALONG
THE
ROWS
THE
SUM
OF
THE
INTERNAL
PIXEL
INTENSITIES
IS
PROBLEM
THE
LOGIC
BEHIND
THIS
CALCULATION
IS
ILLUSTRATED
IN
FIGURE
I
SINCE
HAAR
LIKE
FILTERS
ARE
COMPOSED
OF
RECTANGULAR
REGIONS
THEY
CAN
BE
COM
PUTED
USING
A
SIMILAR
TRICK
FOR
A
FILTER
WITH
TWO
ADJACENT
RECTANGULAR
REGIONS
SIX
OPERATIONS
ARE
REQUIRED
WITH
THREE
ADJACENT
RECTANGULAR
REGIONS
EIGHT
OPERA
TIONS
ARE
REQUIRED
WHEN
THE
FILTER
DIMENSIONS
M
AND
N
ARE
LARGE
THIS
APPROACH
COMPARES
VERY
FAVORABLY
TO
A
NA
IVE
IMPLEMENTATION
OF
CONVENTIONAL
FILTERING
WHICH
REQUIRES
O
M
N
OPERATIONS
TO
COMPUTE
THE
FILTER
RESPONSE
DUE
TO
A
M
N
KER
NEL
HAAR
LIKE
FILTERS
ARE
OFTEN
USED
IN
REAL
TIME
APPLICATIONS
SUCH
AS
FACE
DETECTION
BECAUSE
OF
THE
SPEED
WITH
WHICH
THEY
CAN
BE
COMPUTED
PER
PIXEL
TRANSFORMATIONS
FIGURE
HAAR
LIKE
FILTERS
A
D
HAAR
LIKE
FILTERS
CONSIST
OF
RECTANGULAR
REGIONS
CONVOLUTION
WITH
HAAR
LIKE
FILTERS
CAN
BE
DONE
IN
CONSTANT
TIME
E
TO
SEE
WHY
CONSIDER
THE
PROBLEM
OF
FILTERING
WITH
THIS
SINGLE
RECTANGULAR
REGION
F
WE
DENOTE
THE
SUM
OF
THE
PIXEL
VALUES
IN
THESE
FOUR
REGIONS
AS
A
B
C
AND
D
OUR
GOAL
IS
TO
COMPUTE
D
G
THE
INTEGRAL
IMAGE
HAS
A
VALUE
THAT
IS
THE
SUM
OF
THE
INTENSITIES
OF
THE
PIXELS
ABOVE
AND
TO
THE
LEFT
OF
THE
CURRENT
POSITION
THE
INTEGRAL
IMAGE
AT
POSITION
HENCE
HAS
VALUE
A
B
C
D
H
THE
INTEGRAL
IMAGE
AT
HAS
VALUE
A
C
I
THE
INTEGRAL
IMAGE
AT
HAS
VALUE
A
B
J
THE
INTEGRAL
IMAGE
AT
HAS
VALUE
A
THE
SUM
OF
THE
PIXELS
IN
REGION
D
CAN
NOW
BE
COMPUTED
AS
J0
A
B
C
D
A
A
C
A
B
D
THIS
REQUIRES
JUST
FOUR
OPERATIONS
REGARDLESS
OF
THE
SIZE
OF
THE
ORIGINAL
SQUARE
REGION
LOCAL
BINARY
PATTERNS
THE
LOCAL
BINARY
PATTERNS
LBP
OPERATOR
RETURNS
A
DISCRETE
VALUE
AT
EACH
PIXEL
THAT
CHARACTERIZES
THE
LOCAL
TEXTURE
IN
A
WAY
THAT
IS
PARTIALLY
INVARIANT
TO
LUMINANCE
CHANGES
FOR
THIS
REASON
FEATURES
BASED
ON
LOCAL
BINARY
PATTERNS
ARE
COMMONLY
USED
AS
A
SUBSTRATE
FOR
FACE
RECOGNITION
ALGORITHMS
THE
BASIC
LBP
OPERATOR
COMPARES
THE
EIGHT
NEIGHBORING
PIXEL
INTENSITIES
TO
THE
CENTER
PIXEL
INTENSITY
ASSIGNING
A
OR
A
TO
EACH
NEIGHBOR
DEPENDING
ON
WHETHER
THEY
ARE
LESS
THAN
OR
GREATER
THAN
THE
CENTER
VALUE
THESE
BINARY
VALUES
ARE
THEN
CONCATENATED
IN
A
PREDETERMINED
ORDER
AND
CONVERTED
TO
A
SINGLE
DECIMAL
NUMBER
THAT
REPRESENTS
THE
TYPE
OF
LOCAL
IMAGE
STRUCTURE
FIGURE
WITH
FURTHER
PROCESSING
THE
LBP
OPERATOR
CAN
BE
MADE
ORIENTATION
INVARI
ANT
THE
BINARY
REPRESENTATION
IS
REPEATEDLY
SUBJECTED
TO
BIT
WISE
SHIFTS
TO
CREATE
EIGHT
NEW
BINARY
VALUES
AND
THE
MINIMUM
OF
THESE
VALUES
IS
CHOSEN
THIS
REDUCES
THE
NUMBER
OF
POSSIBLE
LBP
VALUES
TO
IN
PRACTICE
IT
HAS
BEEN
FOUND
THAT
THE
DISTRIBUTION
OVER
THESE
LBP
VALUES
IS
DOMINATED
BY
THOSE
THAT
ARE
RELATIVELY
UNI
FORM
IN
OTHER
WORDS
BINARY
STRINGS
WHERE
TRANSITIONS
ARE
ABSENT
E
G
OR
INFREQUENT
E
G
OCCUR
MOST
FREQUENTLY
THE
NUMBER
OF
TEXTURE
CLASSES
CAN
BE
FURTHER
REDUCED
BY
AGGREGATING
ALL
OF
THE
NON
UNIFORM
LBPS
INTO
A
SINGLE
CLASS
NOW
THE
LOCAL
IMAGE
STRUCTURE
IS
CATEGORIZED
INTO
PROBLEM
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
A
B
FIGURE
LOCAL
BINARY
PATTERNS
A
THE
LOCAL
BINARY
PATTERN
LBP
IS
COMPUTED
BY
COMPARING
THE
CENTRAL
PIXEL
TO
EACH
OF
ITS
EIGHT
NEIGHBORS
THE
BINARY
VALUE
ASSOCIATED
WITH
EACH
POSITION
IS
SET
TO
ONE
IF
THAT
NEIGHBOR
IS
GREATER
THAN
OR
EQUAL
TO
THE
CENTRAL
PIXEL
THE
EIGHT
BINARY
VALUES
CAN
BE
READ
OUT
AND
COMBINED
TO
MAKE
A
SINGLE
BIT
NUMBER
B
LOCAL
BINARY
PATTERNS
CAN
BE
COMPUTED
OVER
LARGER
AREAS
BY
COMPARING
THE
CURRENT
PIXELS
TO
THE
INTERPOLATED
IMAGE
AT
POSITIONS
ON
A
CIRCLE
THIS
TYPE
OF
LBP
IS
CHARACTERIZED
BY
THE
NUMBER
OF
SAMPLES
P
AND
THE
RADIUS
OF
THE
CIRCLE
R
NINE
LBP
TYPES
EIGHT
ROTATIONALLY
INVARIANT
UNIFORM
PATTERNS
AND
ONE
NON
UNIFORM
CLASS
THE
LBP
OPERATOR
CAN
BE
EXTENDED
TO
USE
NEIGHBORHOODS
OF
DIFFERENT
SIZES
THE
CENTRAL
PIXEL
IS
COMPARED
TO
POSITIONS
IN
A
CIRCULAR
PATTERN
FIGURE
IN
GENERAL
THESE
POSITIONS
DO
NOT
EXACTLY
COINCIDE
WITH
THE
PIXEL
GRID
AND
THE
INTENSITY
AT
THESE
POSITIONS
MUST
BE
ESTIMATED
USING
BILINEAR
INTERPOLATION
THIS
EXTENDED
LBP
OPERATOR
CAN
CAPTURE
TEXTURE
AT
DIFFERENT
SCALES
IN
THE
IMAGE
TEXTON
MAPS
THE
TERM
TEXTON
STEMS
FROM
THE
STUDY
OF
HUMAN
PERCEPTION
AND
REFERS
TO
A
PRIM
ITIVE
PERCEPTUAL
ELEMENT
OF
TEXTURE
IN
OTHER
WORDS
IT
ROUGHLY
OCCUPIES
THE
ROLE
THAT
A
PHONEME
TAKES
IN
SPEECH
RECOGNITION
IN
A
MACHINE
VISION
CONTEXT
A
TEXTON
IS
A
DISCRETE
VARIABLE
THAT
DESIGNATES
WHICH
ONE
OF
A
FINITE
NUMBER
OF
POSSIBLE
TEXTURE
CLASSES
IS
PRESENT
IN
A
REGION
SURROUNDING
THE
CURRENT
PIXEL
A
TEXTON
MAP
IS
AN
IMAGE
IN
WHICH
THE
TEXTON
IS
COMPUTED
AT
EVERY
PIXEL
FIGURE
TEXTON
ASSIGNMENT
DEPENDS
ON
TRAINING
DATA
A
BANK
OF
N
FILTERS
IS
CONVOLVED
WITH
A
SET
OF
TRAINING
IMAGES
THE
RESPONSES
ARE
CONCATENATED
TO
FORM
ONE
N
VECTOR
FOR
EACH
PIXEL
POSITION
IN
EACH
TRAINING
IMAGE
THESE
VECTORS
ARE
THEN
CLUSTERED
INTO
K
CLASSES
USING
THE
K
MEANS
ALGORITHM
SECTION
TEXTONS
ARE
COMPUTED
FOR
A
NEW
IMAGE
BY
CONVOLVING
IT
WITH
THE
SAME
FILTER
BANK
FOR
EACH
PIXEL
THE
TEXTON
IS
ASSIGNED
BY
NOTING
WHICH
CLUSTER
MEAN
IS
CLOSEST
TO
THE
N
FILTER
OUTPUT
VECTOR
ASSOCIATED
WITH
THE
CURRENT
POSITION
THE
CHOICE
OF
FILTER
BANK
SEEMS
TO
BE
RELATIVELY
UNIMPORTANT
ONE
APPROACH
HAS
BEEN
TO
USE
GAUSSIANS
AT
SCALES
Σ
AND
TO
FILTER
ALL
THREE
COLOR
CHANNELS
AND
DERIVATIVES
OF
GAUSSIANS
AT
SCALES
AND
AND
LAPLACIANS
OF
GAUSSIANS
AT
SCALES
Σ
AND
TO
FILTER
THE
LUMINANCE
FIGURE
IN
THIS
WAY
BOTH
COLOR
AND
FIGURE
TEXTON
MAPS
IN
A
TEX
TON
MAP
EACH
PIXEL
IS
REPLACED
BY
THE
TEXTON
INDEX
THIS
INDEX
CHARACTERIZES
THE
TEXTURE
IN
THE
SURROUNDING
REGION
A
ORIGINAL
IMAGE
B
ASSOCIATED
TEX
TON
MAP
NOTE
HOW
SIMILAR
REGIONS
ARE
ASSIGNED
THE
SAME
TEXTON
INDEX
IN
DICATED
BY
COLOR
C
ORIGINAL
IMAGE
D
ASSOCIATED
TEXTON
MAP
USING
DIF
FERENT
FILTER
BANK
FROM
B
TEXTON
MAPS
ARE
OFTEN
USED
IN
SEMANTIC
IMAGE
SEGMENTATION
ADAPTED
FROM
SHOTTON
ET
AL
QC
SPRINGER
A
C
D
B
FIGURE
TEXTONS
THE
IMAGE
IS
CONVOLVED
WITH
A
FILTER
BANK
TO
YIELD
AN
N
VECTOR
OF
FILTER
RESPONSES
AT
EACH
POSITION
POSSIBLE
CHOICES
FOR
THE
FILTER
BANK
INCLUDE
A
A
COMBINATION
OF
GAUSSIANS
DERIVATIVES
OF
GAUSSIANS
AND
LAPLACIANS
OF
GAUSSIANS
B
ROTATIONALLY
INVARIANT
FILTERS
AND
C
THE
MAXIMUM
RESPONSE
DATABASE
D
IN
TRAINING
THE
N
FILTER
RESPONSE
VECTORS
ARE
CLUSTERED
USING
K
MEANS
FOR
NEW
DATA
THE
TEXTON
INDEX
IS
ASSIGNED
BASED
ON
THE
NEAREST
OF
THESE
CLUSTERS
THUS
THE
FILTER
SPACE
IS
EFFECTIVELY
PARTITIONED
INTO
VORONOI
REGIONS
TEXTURE
INFORMATION
IS
CAPTURED
IT
MAY
BE
DESIRABLE
TO
COMPUTE
TEXTONS
THAT
ARE
INVARIANT
TO
ORIENTATION
ONE
WAY
OF
ACHIEVING
THIS
IS
TO
CHOOSE
ROTATIONALLY
INVARIANT
FILTERS
TO
FORM
THE
FILTER
BANK
FIGURE
HOWEVER
THESE
HAVE
THE
UNDESIRABLE
PROPERTY
OF
NOT
RESPONDING
AT
ALL
TO
ORIENTED
STRUCTURES
IN
THE
IMAGE
THE
MAXIMUM
RESPONSE
FILTER
BANK
IS
DESIGNED
TO
PROVIDE
A
ROTATIONALLY
INVARIANT
MEASURE
OF
LOCAL
TEXTURE
WHICH
DOES
NOT
DISCARD
THIS
INFORMATION
THE
FILTER
BANK
FIGURE
CONSISTS
OF
A
GAUSSIAN
AND
A
LAPLACIAN
OF
GAUSSIAN
FILTER
AN
EDGE
FILTER
AT
THREE
SCALES
AND
A
BAR
FILTER
A
SYMMETRIC
ORIENTED
FILTER
AT
THE
SAME
THREE
SCALES
THE
EDGE
AND
BAR
FILTER
ARE
REPLICATED
AT
ORIENTATIONS
AT
EACH
SCALE
GIVING
A
TOTAL
OF
FILTERS
TO
INDUCE
ROTATIONAL
INVARIANCE
ONLY
THE
MAXIMUM
FILTER
RESPONSE
OVER
ORIENTATION
IS
USED
HENCE
THE
FINAL
VECTOR
OF
FILTER
RESPONSES
CONSISTS
OF
EIGHT
NUMBERS
CORRESPONDING
TO
THE
GAUSSIAN
AND
LAPLACIAN
FILTERS
ALREADY
INVARIANT
AND
THE
MAXIMUM
RESPONSES
OVER
ORIENTATION
OF
THE
EDGE
AND
BAR
FILTERS
AT
EACH
OF
THE
THREE
SCALES
FIGURE
RECONSTRUCTION
FROM
EDGES
A
ORIGINAL
IMAGE
B
EDGE
MAP
EACH
EDGE
PIXEL
HAS
ASSOCIATED
SCALE
AND
ORIENTATION
INFORMATION
AS
WELL
AS
A
RECORD
OF
THE
LUMINANCE
LEVELS
AT
EITHER
SIDE
C
THE
IMAGE
CAN
BE
RECON
STRUCTED
ALMOST
PERFECTLY
FROM
THE
EDGES
AND
THEIR
ASSOCIATED
INFORMATION
ADAPTED
FROM
ELDER
QC
SPRINGER
EDGES
CORNERS
AND
INTEREST
POINTS
IN
THIS
SECTION
WE
CONSIDER
METHODS
THAT
AIM
TO
IDENTIFY
INFORMATIVE
PARTS
OF
THE
IMAGE
IN
EDGE
DETECTION
THE
GOAL
IS
TO
RETURN
A
BINARY
IMAGE
WHERE
A
NON
ZERO
VALUE
DENOTES
THE
PRESENCE
OF
AN
EDGE
IN
THE
IMAGE
EDGE
DETECTORS
OPTIONALLY
ALSO
RETURN
OTHER
INFORMATION
SUCH
AS
THE
ORIENTATION
AND
SCALE
ASSOCIATED
WITH
THE
EDGE
EDGE
MAPS
ARE
A
HIGHLY
COMPACT
REPRESENTATION
OF
AN
IMAGE
AND
IT
HAS
BEEN
SHOWN
THAT
IT
IS
POSSIBLE
TO
RECONSTRUCT
AN
IMAGE
VERY
ACCURATELY
WITH
JUST
INFORMATION
ABOUT
THE
EDGES
IN
THE
SCENE
FIGURE
CORNERS
ARE
POSITIONS
IN
THE
IMAGE
THAT
CONTAIN
RICH
VISUAL
INFORMATION
AND
CAN
BE
FOUND
REPRODUCIBLY
IN
DIFFERENT
IMAGES
OF
THE
SAME
OBJECT
FIGURE
THERE
ARE
MANY
SCHEMES
TO
FIND
CORNERS
BUT
THEY
ALL
AIM
TO
IDENTIFY
POINTS
THAT
ARE
LOCALLY
UNIQUE
CORNER
DETECTION
ALGORITHMS
WERE
ORIGINALLY
DEVELOPED
FOR
GEOMETRIC
COMPUTER
VISION
PROBLEMS
SUCH
AS
WIDE
BASELINE
IMAGE
MATCHING
HERE
WE
SEE
THE
SAME
SCENE
FROM
TWO
DIFFERENT
ANGLES
AND
WISH
TO
IDENTIFY
WHICH
POINTS
CORRESPOND
TO
WHICH
IN
RECENT
YEARS
CORNERS
HAVE
ALSO
BEEN
USED
IN
OBJECT
RECOGNITION
ALGORITHMS
WHERE
THEY
ARE
USUALLY
REFERRED
TO
AS
INTEREST
POINTS
THE
IDEA
HERE
IS
THAT
THE
REGIONS
SURROUNDING
INTEREST
POINTS
CONTAIN
INFORMATION
ABOUT
WHICH
OBJECT
CLASS
IS
PRESENT
CANNY
EDGE
DETECTOR
TO
COMPUTE
EDGES
WITH
THE
CANNY
EDGE
DETECTOR
FIGURE
THE
IMAGE
P
IS
FIRST
BLURRED
AND
THEN
CONVOLVED
WITH
A
PAIR
OF
ORTHOGONAL
DERIVATIVE
FILTERS
SUCH
AS
PRE
WITT
FILTERS
TO
CREATE
IMAGES
H
AND
V
CONTAINING
DERIVATIVES
IN
THE
HORIZONTAL
AND
VERTICAL
DIRECTIONS
RESPECTIVELY
FOR
PIXEL
I
J
THE
ORIENTATION
ΘIJ
AND
MAGNITUDE
AIJ
OF
THE
GRADIENT
IS
COMPUTED
USING
FIGURE
CANNY
EDGE
DETECTION
A
ORIGINAL
IMAGE
B
RESULT
OF
VERTICAL
PREWITT
FILTER
C
RESULTS
OF
HORIZONTAL
PREWITT
FILTER
D
QUANTIZED
ORIEN
TATION
MAP
E
GRADIENT
AMPLITUDE
MAP
F
AMPLITUDES
AFTER
NON
MAXIMAL
SUPPRESSION
G
THRESHOLDING
AT
TWO
LEVELS
THE
WHITE
PIXELS
ARE
ABOVE
THE
HIGHER
THRESHOLD
THE
RED
PIXELS
ARE
ABOVE
THE
LOWER
THRESHOLD
BUT
BELOW
THE
HIGHER
ONE
H
FINAL
EDGE
MAP
AFTER
HYSTERESIS
THRESHOLDING
CONTAINS
ALL
OF
THE
WHITE
PIXELS
FROM
G
AND
THOSE
RED
PIXELS
THAT
CONNECT
TO
THEM
ΘIJ
ARCTAN
VIJ
HIJ
AIJ
A
SIMPLE
APPROACH
WOULD
BE
TO
ASSIGN
AN
EDGE
TO
POSITION
I
J
IF
THE
AMPLITUDE
THERE
EXCEEDS
A
CRITICAL
VALUE
THIS
IS
TERMED
THRESHOLDING
UNFORTUNATELY
IT
PRO
DUCES
POOR
RESULTS
THE
AMPLITUDE
MAP
TAKES
HIGH
VALUES
ON
THE
EDGE
BUT
ALSO
AT
ADJACENT
POSITIONS
THE
CANNY
EDGE
DETECTOR
ELIMINATES
THESE
UNWANTED
RESPONSES
USING
A
METHOD
KNOWN
AS
NON
MAXIMUM
SUPPRESSION
IN
NON
MAXIMUM
SUPPRESSION
THE
GRADIENT
ORIENTATION
IS
QUANTIZED
INTO
ONE
OF
FOUR
ANGLES
WHERE
ANGLES
APART
ARE
TREATED
AS
EQUIVALENT
THE
PIXELS
ASSOCIATED
WITH
EACH
ANGLE
ARE
NOW
TREATED
SEPARATELY
FOR
EACH
PIXEL
THE
AMPLITUDE
IS
SET
TO
ZERO
IF
EITHER
OF
THE
NEIGHBORING
TWO
PIXELS
PERPENDICULAR
TO
THE
GRADIENT
HAVE
HIGHER
VALUES
FOR
EXAMPLE
FOR
A
PIXEL
WHERE
THE
GRADIENT
ORIENTATION
IS
VERTICAL
THE
IMAGE
IS
CHANGING
IN
THE
HORIZONTAL
DIRECTION
THE
PIXELS
TO
THE
LEFT
AND
RIGHT
ARE
EXAMINED
AND
THE
AMPLITUDE
IS
SET
TO
ZERO
IF
EITHER
OF
THESE
FIGURE
HARRIS
CORNER
DETECTOR
A
IMAGE
WITH
DETECTED
CORNERS
THE
CORNER
DETECTION
ALGORITHM
IS
BASED
ON
THE
IMAGE
STRUCTURE
TENSOR
WHICH
CAPTURES
INFORMATION
ABOUT
THE
DISTRIBUTION
OF
GRADIENTS
AROUND
THE
POINT
B
IN
FLAT
REGIONS
BOTH
SINGULAR
VALUES
OF
THE
IMAGE
STRUCTURE
TENSOR
ARE
SMALL
C
ON
EDGES
ONE
IS
SMALL
AND
THE
OTHER
LARGE
D
AT
CORNERS
BOTH
ARE
LARGE
INDICATING
THAT
THE
IMAGE
IS
CHANGING
QUICKLY
IN
BOTH
DIRECTIONS
ARE
GREATER
THAN
THE
CURRENT
VALUE
IN
THIS
WAY
THE
GRADIENTS
AT
THE
MAXIMUM
OF
THE
EDGE
AMPLITUDE
PROFILE
ARE
RETAINED
AND
THOSE
AWAY
FROM
THIS
MAXIMUM
ARE
SUPPRESSED
A
BINARY
EDGE
MAP
CAN
NOW
BE
COMPUTED
BY
COMPARING
THE
REMAINING
NON
ZERO
AMPLITUDES
TO
A
FIXED
THRESHOLD
HOWEVER
FOR
ANY
GIVEN
THRESHOLD
THERE
WILL
BE
MISSES
PLACES
WHERE
THERE
ARE
REAL
EDGES
BUT
THEIR
AMPLITUDE
FALLS
BELOW
THE
THRESHOLD
AND
FALSE
POSITIVES
PIXELS
LABELED
AS
EDGES
WHERE
NONE
EXIST
IN
THE
ORIG
INAL
IMAGE
TO
DECREASE
THESE
UNDESIRABLE
PHENOMENA
KNOWLEDGE
ABOUT
THE
CON
TINUITY
OF
REAL
WORLD
EDGES
IS
EXPLOITED
TWO
THRESHOLDS
ARE
DEFINED
ALL
OF
THE
PIXELS
WHOSE
AMPLITUDE
IS
ABOVE
THE
HIGHER
THRESHOLD
ARE
LABELED
AS
EDGES
AND
THIS
THRESHOLD
IS
CHOSEN
SO
THAT
THERE
ARE
FEW
FALSE
POSITIVES
TO
TRY
TO
DECREASE
THE
NUMBER
OF
MISSES
PIXELS
THAT
ARE
ABOVE
THE
LOWER
AMPLITUDE
THRESHOLD
AND
ARE
CONNECTED
TO
AN
EXISTING
EDGE
PIXEL
ARE
ALSO
LABELED
AS
EDGES
BY
ITERATING
THIS
LAST
STEP
IT
IS
POSSIBLE
TO
TRACE
ALONG
WEAKER
PARTS
OF
STRONG
CONTOURS
THIS
TECHNIQUE
IS
KNOWN
AS
HYSTERESIS
THRESHOLDING
EDGES
CORNERS
AND
INTEREST
POINTS
HARRIS
CORNER
DETECTOR
THE
HARRIS
CORNER
DETECTOR
FIGURE
CONSIDERS
THE
LOCAL
GRADIENTS
IN
THE
HORI
ZONTAL
AND
VERTICAL
DIRECTIONS
AROUND
EACH
POINT
THE
GOAL
IS
TO
FIND
POINTS
IN
THE
IMAGE
WHERE
THE
IMAGE
INTENSITY
IS
VARYING
IN
BOTH
DIRECTIONS
A
CORNER
RATHER
THAN
IN
ONE
DIRECTION
AN
EDGE
OR
NEITHER
A
FLAT
REGION
THE
HARRIS
CORNER
DETECTOR
BASES
THIS
DECISION
ON
THE
IMAGE
STRUCTURE
TENSOR
SIJ
I
D
J
D
WMN
MN
MN
VMNL
M
I
D
N
J
D
HMNVMN
WHERE
SIJ
IS
THE
IMAGE
STRUCTURE
TENSOR
AT
POSITION
I
J
WHICH
IS
COMPUTED
OVER
A
SQUARE
REGION
OF
SIZE
AROUND
THE
CURRENT
POSITION
THE
TERM
HMN
DENOTES
THE
RESPONSE
OF
A
HORIZONTAL
DERIVATIVE
FILTER
SUCH
AS
THE
SOBEL
AT
POSITION
M
N
AND
THE
TERM
VMN
DENOTES
THE
RESPONSE
OF
A
VERTICAL
DERIVATIVE
FILTER
THE
TERM
WMN
IS
A
WEIGHT
THAT
DIMINISHES
THE
CONTRIBUTION
OF
POSITIONS
THAT
ARE
FAR
FROM
THE
CENTRAL
PIXEL
I
J
TO
IDENTIFY
WHETHER
A
CORNER
IS
PRESENT
THE
HARRIS
CORNER
DETECTOR
CONSIDERS
THE
SINGULAR
VALUES
OF
THE
IMAGE
STRUCTURE
TENSOR
IF
BOTH
SINGULAR
VALUES
ARE
SMALL
THEN
THE
REGION
AROUND
THE
POINT
IS
SMOOTH
AND
THIS
POSITION
IS
NOT
CHOSEN
IF
ONE
SINGULAR
VALUE
IS
LARGE
BUT
THE
OTHER
SMALL
THEN
THE
IMAGE
IS
CHANGING
IN
ONE
DIRECTION
BUT
NOT
THE
OTHER
AND
POINT
LIES
ON
OR
NEAR
AN
EDGE
HOWEVER
IF
BOTH
SINGULAR
VALUES
ARE
LARGE
THEN
THIS
IMAGE
IS
CHANGING
RAPIDLY
IN
BOTH
DIRECTIONS
IN
THIS
REGION
AND
THE
POSITION
IS
DEEMED
TO
BE
A
CORNER
IN
FACT
THE
HARRIS
DETECTOR
DOES
NOT
DIRECTLY
COMPUTE
THE
SINGULAR
VALUES
BUT
EVALUATES
A
CRITERION
WHICH
ACCOMPLISHES
THE
SAME
THING
MORE
EFFICIENTLY
CIJ
Κ
DET
SIJ
Κ
TRACE
SIJ
WHERE
Κ
IS
A
CONSTANT
VALUES
BETWEEN
AND
ARE
SENSIBLE
IF
THE
VALUE
OF
CIJ
IS
GREATER
THAN
A
PREDETERMINED
THRESHOLD
THEN
A
CORNER
MAY
BE
ASSIGNED
THERE
IS
USUALLY
AN
ADDITIONAL
NON
MAXIMAL
SUPPRESSION
STAGE
SIMILAR
TO
THAT
IN
THE
CANNY
EDGE
DETECTOR
TO
ENSURE
THAT
ONLY
PEAKS
IN
THE
FUNCTION
CIJ
ARE
RETAINED
SIFT
DETECTOR
THE
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
DETECTOR
IS
A
SECOND
METHOD
FOR
IDEN
TIFYING
INTEREST
POINTS
UNLIKE
THE
HARRIS
CORNER
DETECTOR
IT
ASSOCIATES
A
SCALE
AND
ORIENTATION
TO
EACH
OF
THE
RESULTING
INTEREST
POINTS
TO
FIND
THE
INTEREST
POINTS
A
NUMBER
OF
OPERATIONS
ARE
PERFORMED
IN
TURN
THE
INTENSITY
IMAGE
IS
FILTERED
WITH
A
DIFFERENCE
OF
GAUSSIAN
KERNEL
AT
A
SERIES
OF
K
INCREASINGLY
COARSE
SCALES
FIGURE
THEN
THE
FILTERED
IMAGES
ARE
STACKED
TO
MAKE
A
VOLUME
OF
SIZE
I
J
K
WHERE
I
AND
J
ARE
THE
VERTICAL
AND
HORIZONTAL
SIZE
OF
THE
IMAGE
EXTREMA
ARE
IDENTIFIED
WITHIN
THIS
VOLUME
THESE
ARE
POSITIONS
WHERE
THE
VOXEL
NEIGHBORS
FROM
A
BLOCK
ARE
EITHER
ALL
GREATER
THAN
OR
ALL
LESS
THAN
THE
CURRENT
VALUE
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
FIGURE
THE
SIFT
DETECTOR
A
ORIGINAL
IMAGE
B
H
THE
IMAGE
IS
FILTERED
WITH
DIFFERENCE
OF
GAUSSIAN
KERNELS
AT
A
RANGE
OF
INCREASING
SCALES
I
THE
RESULTING
IMAGES
ARE
STACKED
TO
CREATE
A
VOLUME
POINTS
THAT
ARE
LOCAL
EXTREMA
IN
THE
FILTERED
IMAGE
VOLUME
I
E
ARE
EITHER
GREATER
THAN
OR
LESS
THAN
ALL
NEIGHBORS
ARE
CONSIDERED
TO
BE
CANDIDATES
FOR
INTEREST
POINTS
FIGURE
REFINEMENT
OF
SIFT
DETECTOR
CANDIDATES
A
POSITIONS
OF
EX
TREMA
IN
THE
FILTERED
IMAGE
VOLUME
FIGURE
NOTE
THAT
THE
SCALE
IS
NOT
SHOWN
THESE
ARE
CONSIDERED
CANDIDATES
TO
BE
INTEREST
POINTS
B
RE
MAINING
CANDIDATES
AFTER
ELIMINATING
THOSE
IN
SMOOTH
REGIONS
C
REMAINING
CANDIDATE
POINTS
AFTER
REMOVING
THOSE
ON
EDGES
USING
THE
IMAGE
STRUCTURE
TENSOR
THESE
EXTREMA
ARE
LOCALIZED
TO
SUB
VOXEL
ACCURACY
BY
APPLYING
A
LOCAL
QUADRATIC
APPROXIMATION
AND
RETURNING
THE
POSITION
OF
THE
PEAK
OR
TROUGH
THE
QUADRATIC
APPROXIMATION
IS
MADE
BY
TAKING
A
TAYLOR
EXPANSION
ABOUT
THE
CURRENT
POINT
THIS
PROVIDES
A
POSITION
ESTIMATE
THAT
HAS
SUB
PIXEL
RESOLUTION
AND
AN
ESTIMATE
OF
THE
SCALE
THAT
IS
MORE
ACCURATE
THAN
THE
RESOLUTION
OF
THE
SCALE
SAMPLING
FINALLY
THE
IMAGE
STRUCTURE
TENSOR
SIJ
EQUATION
IS
COMPUTED
AT
THE
LOCATION
AND
SCALE
OF
EACH
POINT
CANDIDATE
POINTS
IN
SMOOTH
REGIONS
AND
ON
EDGES
ARE
REMOVED
BY
CONSIDERING
THE
SINGULAR
VALUES
OF
SIJ
AS
IN
THE
HARRIS
CORNER
DETECTOR
FIGURE
THIS
PROCEDURE
RETURNS
A
SET
OF
INTEREST
POINTS
THAT
ARE
LOCALIZED
TO
SUB
PIXEL
DESCRIPTORS
FIGURE
RESULTS
OF
SIFT
DETEC
TOR
EACH
FINAL
INTEREST
POINT
IS
IN
DICATED
USING
AN
ARROW
THE
LENGTH
OF
THE
ARROW
INDICATES
THE
SCALE
WITH
WHICH
THE
INTEREST
POINT
IS
IDENTIFIED
AND
THE
ANGLE
OF
THE
ARROW
INDICATES
THE
ASSOCIATED
ORIENTATION
NOTICE
THAT
THERE
ARE
SOME
POSITIONS
IN
THE
IMAGE
WHERE
THE
ORIENTATION
WAS
NOT
UNIQUE
AND
HERE
TWO
INTEREST
POINTS
ARE
USED
ONE
ASSOCIATED
WITH
EACH
ORI
ENTATION
AN
EXAMPLE
OF
THIS
IS
ON
THE
RIGHT
SHIRT
COLLAR
SUBSEQUENT
DESCRIP
TORS
THAT
CHARACTERIZE
THE
STRUCTURE
OF
THE
IMAGE
AROUND
THE
INTEREST
POINTS
ARE
COMPUTED
RELATIVE
TO
THIS
SCALE
AND
ORIENTATION
AND
HENCE
INHERIT
SOME
IN
VARIANCE
TO
THESE
FACTORS
ACCURACY
AND
ASSOCIATED
ACCURATELY
WITH
A
PARTICULAR
SCALE
FINALLY
A
UNIQUE
ORI
ENTATION
IS
ALSO
ASSIGNED
TO
EACH
INTEREST
POINT
TO
THIS
END
THE
AMPLITUDE
AND
ORIENTATION
OF
THE
LOCAL
GRADIENTS
ARE
COMPUTED
EQUATIONS
IN
A
REGION
SUR
ROUNDING
THE
INTEREST
POINT
WHOSE
SIZE
IS
PROPORTIONAL
TO
THE
IDENTIFIED
SCALE
AN
ORIENTATION
HISTOGRAM
IS
THEN
COMPUTED
OVER
THIS
REGION
WITH
BINS
COVERING
ALL
OF
ORIENTATION
THE
CONTRIBUTION
TO
THE
HISTOGRAM
DEPENDS
ON
THE
GRADIENT
AMPLITUDE
AND
IS
WEIGHTED
BY
A
GAUSSIAN
PROFILE
CENTERED
AT
THE
LOCATION
OF
THE
INTEREST
POINT
SO
THAT
NEARBY
REGIONS
CONTRIBUTE
MORE
THE
ORIENTATION
OF
THE
IN
TEREST
POINT
IS
ASSIGNED
TO
BE
THE
PEAK
OF
THIS
HISTOGRAM
IF
THERE
IS
A
SECOND
PEAK
WITHIN
OF
THE
MAXIMUM
WE
MAY
CHOOSE
TO
COMPUTE
DESCRIPTORS
AT
TWO
ORIENTA
TIONS
AT
THIS
POINT
THE
FINAL
DETECTED
POINTS
ARE
HENCE
ASSOCIATED
WITH
A
PARTICULAR
ORIENTATION
AND
SCALE
FIGURE
DESCRIPTORS
IN
THIS
SECTION
WE
CONSIDER
DESCRIPTORS
THESE
ARE
COMPACT
REPRESENTATIONS
THAT
SUMMARIZE
THE
CONTENTS
OF
AN
IMAGE
REGION
HISTOGRAMS
THE
SIMPLEST
APPROACH
TO
AGGREGATING
INFORMATION
OVER
A
LARGE
IMAGE
REGION
IS
TO
COMPUTE
A
HISTOGRAM
OF
THE
RESPONSES
IN
THIS
AREA
FOR
EXAMPLE
WE
MIGHT
COLLATE
RGB
PIXEL
INTENSITIES
FILTER
RESPONSES
LOCAL
BINARY
PATTERNS
OR
TEXTONS
INTO
A
HISTOGRAM
DEPENDING
ON
THE
APPLICATION
THE
HISTOGRAM
ENTRIES
CAN
BE
TREATED
AS
DISCRETE
AND
MODELED
WITH
A
CATEGORICAL
DISTRIBUTION
OR
TREATED
AS
A
CONTINUOUS
VECTOR
QUANTITY
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
A
B
FIGURE
SIFT
DESCRIPTOR
A
GRADIENTS
ARE
COMPUTED
FOR
EVERY
PIXEL
WITHIN
A
REGION
AROUND
THE
INTEREST
POINT
B
THIS
REGION
IS
SUBDIVIDED
INTO
CELLS
INFORMATION
IS
POOLED
WITHIN
THESE
CELLS
TO
FORM
AN
HISTOGRAM
THESE
HISTOGRAMS
ARE
CONCATENATED
TO
PROVIDE
A
FINAL
DESCRIPTOR
THAT
POOLS
LOCALLY
TO
PROVIDE
INVARIANCE
TO
SMALL
DEFORMATIONS
BUT
ALSO
RETAINS
SOME
SPATIAL
INFORMATION
ABOUT
THE
IMAGE
GRADIENTS
IN
THIS
FIGURE
INFORMATION
FROM
AN
PIXEL
PATCH
HAS
BEEN
DIVIDED
TO
MAKE
A
GRID
OF
CELLS
IN
THE
ORIGINAL
IMPLEMENTATION
OF
THE
SIFT
DETECTOR
A
PATCH
WAS
DIVIDED
INTO
AT
GRID
OF
CELLS
FOR
CONTINUOUS
QUANTITIES
SUCH
AS
FILTER
RESPONSES
THE
LEVEL
OF
QUANTIZATION
IS
CRITICAL
QUANTIZING
THE
RESPONSES
INTO
MANY
BINS
POTENTIALLY
ALLOWS
FINE
DISCRIMI
NATION
BETWEEN
RESPONSES
HOWEVER
IF
DATA
ARE
SCARCE
THEN
MANY
OF
THESE
BINS
WILL
BE
EMPTY
AND
IT
IS
HARDER
TO
RELIABLY
DETERMINE
THE
STATISTICS
OF
THE
DESCRIPTOR
ONE
APPROACH
IS
TO
USE
AN
ADAPTIVE
CLUSTERING
METHOD
SUCH
AS
K
MEANS
SECTION
TO
AUTOMATICALLY
DETERMINE
THE
BIN
SIZES
AND
SHAPES
HISTOGRAMMING
IS
A
USEFUL
APPROACH
FOR
TASKS
WHERE
SPATIAL
RESOLUTION
IS
NOT
PARAMOUNT
FOR
EXAMPLE
TO
CLASSIFY
A
LARGE
REGION
OF
TEXTURE
IT
MAKES
SENSE
TO
POOL
INFORMATION
HOWEVER
THIS
APPROACH
IS
LARGELY
UNSUITABLE
FOR
CHARACTERIZING
STRUCTURED
OBJECTS
THE
SPATIAL
LAYOUT
OF
THE
OBJECT
IS
IMPORTANT
FOR
IDENTIFICATION
WE
NOW
INTRODUCE
TWO
REPRESENTATIONS
FOR
IMAGE
REGIONS
THAT
RETAIN
SOME
SPATIAL
INFORMATION
BUT
ALSO
POOL
INFORMATION
LOCALLY
AND
THUS
PROVIDE
INVARIANCE
TO
SMALL
DISPLACEMENTS
AND
WARPS
OF
THE
IMAGE
BOTH
THE
SIFT
DESCRIPTOR
SECTION
AND
THE
HOG
DESCRIPTOR
SECTION
CONCATENATE
SEVERAL
HISTOGRAMS
THAT
WERE
COMPUTED
OVER
SPATIALLY
DISTINCT
BLOCKS
SIFT
DESCRIPTORS
THE
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
DESCRIPTOR
FIGURE
CHARACTERIZES
THE
IMAGE
REGION
AROUND
A
GIVEN
POINT
IT
IS
USUALLY
USED
IN
CONJUNCTION
WITH
INTEREST
POINTS
THAT
WERE
FOUND
USING
THE
SIFT
DETECTOR
THESE
INTEREST
POINTS
ARE
ASSOCIATED
WITH
A
PARTICULAR
SCALE
AND
ROTATION
AND
THE
SIFT
DESCRIPTOR
WOULD
TYPICALLY
BE
COMPUTED
OVER
A
SQUARE
REGION
THAT
IS
TRANSFORMED
BY
THESE
VALUES
DESCRIPTORS
FIGURE
HOG
DESCRIPTOR
A
ORIGINAL
IMAGE
B
GRADIENT
ORIENTATION
QUANTIZED
INTO
NINE
BINS
FROM
TO
C
GRADIENT
MAGNITUDE
D
CELL
DESCRIPTORS
ARE
ORIENTATION
HISTOGRAMS
THAT
ARE
COMPUTED
WITHIN
PIXEL
REGIONS
E
BLOCK
DESCRIPTORS
ARE
COMPUTED
BY
CONCATENATING
BLOCKS
OF
CELL
DESCRIPTORS
THE
BLOCK
DESCRIPTORS
ARE
NORMALIZED
THE
FINAL
HOG
DESCRIPTOR
CONSISTS
OF
THE
CONCATENATED
BLOCK
DESCRIPTORS
THE
GOAL
IS
TO
CHARACTERIZE
THE
IMAGE
REGION
IN
A
WAY
THAT
IS
PARTIALLY
INVARIANT
TO
INTENSITY
AND
CONTRAST
CHANGES
AND
SMALL
GEOMETRIC
DEFORMATIONS
TO
COMPUTE
THE
SIFT
DESCRIPTOR
WE
FIRST
COMPUTE
GRADIENT
ORIENTATION
AND
AMPLITUDE
MAPS
EQUATION
AS
FOR
THE
CANNY
EDGE
DETECTOR
OVER
A
PIXEL
REGION
AROUND
THE
INTEREST
POINT
THE
RESULTING
ORIENTATION
IS
QUANTIZED
INTO
EIGHT
BINS
SPREAD
OVER
THE
RANGE
THEN
THE
DETECTOR
REGION
IS
DIVIDED
INTO
A
REGULAR
GRID
OF
NON
OVERLAPPING
CELLS
WITHIN
EACH
OF
THESE
CELLS
AN
EIGHT
DIMENSIONAL
HISTOGRAM
OF
THE
IMAGE
ORIENTATIONS
IS
COMPUTED
EACH
CONTRIBUTION
TO
THE
HISTOGRAM
IS
WEIGHTED
BY
THE
ASSOCIATED
GRADIENT
AMPLITUDE
AND
BY
DISTANCE
SO
THAT
POSITIONS
FURTHER
FROM
THE
INTEREST
POINT
CONTRIBUTE
LESS
THE
HISTOGRAMS
ARE
CONCATENATED
TO
MAKE
A
SINGLE
VECTOR
WHICH
IS
THEN
NORMALIZED
THE
DESCRIPTOR
IS
INVARIANT
TO
CONSTANT
INTENSITY
CHANGES
AS
IT
IS
BASED
ON
GRA
DIENTS
THE
FINAL
NORMALIZATION
PROVIDES
SOME
INVARIANCE
TO
CONTRAST
SMALL
DEFOR
MATIONS
DO
NOT
AFFECT
THE
DESCRIPTOR
TOO
MUCH
AS
IT
POOLS
INFORMATION
WITHIN
EACH
CELL
HOWEVER
BY
KEEPING
THE
INFORMATION
FROM
EACH
CELL
SEPARATE
SOME
SPATIAL
INFORMATION
IS
RETAINED
HISTOGRAM
OF
ORIENTED
GRADIENTS
THE
HISTOGRAM
OF
ORIENTED
GRADIENTS
HOG
DESCRIPTOR
ATTEMPTS
TO
CONSTRUCT
A
MORE
DETAILED
CHARACTERIZATION
OF
THE
SPATIAL
STRUCTURE
WITH
A
SMALL
IMAGE
WIN
DOW
IT
IS
A
USEFUL
PREPROCESSING
STEP
FOR
ALGORITHMS
THAT
DETECT
OBJECTS
WITH
QUASI
REGULAR
STRUCTURE
SUCH
AS
PEDESTRIANS
LIKE
THE
SIFT
DESCRIPTOR
THE
HOG
DESCRIPTOR
CONSISTS
OF
A
COLLECTION
OF
NORMALIZED
HISTOGRAMS
COMPUTED
OVER
SPATIALLY
OFFSET
PATCHES
THE
RESULT
IS
A
DESCRIPTOR
THAT
CAPTURES
COARSE
SPATIAL
STRUCTURE
BUT
IS
INVARIANT
TO
SMALL
LOCAL
DEFORMATIONS
THE
PROCESS
OF
COMPUTING
A
HOG
DESCRIPTOR
SUITABLE
FOR
PEDESTRIAN
DETECTION
CONSISTS
OF
THE
FOLLOWING
STAGES
FIRST
THE
ORIENTATION
AND
AMPLITUDE
OF
THE
IMAGE
GRADIENTS
ARE
COMPUTED
AT
EVERY
PIXEL
IN
A
WINDOW
USING
EQUATION
THE
ORIENTATION
IS
QUANTIZED
INTO
NINE
BINS
SPREAD
OVER
THE
RANGE
THE
DETECTOR
REGION
IS
DIVIDED
INTO
A
REGULAR
GRID
OF
OVERLAPPING
CELLS
A
ORIENTATION
HISTOGRAM
IS
COMPUTED
WITHIN
EACH
CELL
WHERE
THE
CONTRIBUTION
TO
THE
HISTOGRAM
IS
WEIGHTED
BY
THE
GRADIENT
AMPLITUDE
AND
THE
DISTANCE
FROM
THE
CENTER
OF
THE
CELL
SO
THAT
MORE
CENTRAL
PIXELS
CONTRIBUTE
MORE
FOR
EACH
BLOCK
OF
CELLS
THE
DESCRIPTORS
ARE
CONCATENATED
AND
NORMALIZED
TO
FORM
A
BLOCK
DESCRIPTOR
ALL
OF
THE
BLOCK
DESCRIPTORS
ARE
CONCATENATED
TO
FORM
THE
FINAL
HOG
DESCRIPTOR
THE
FINAL
DESCRIPTOR
CONTAINS
SPATIALLY
POOLED
INFORMATION
ABOUT
LOCAL
GRADIENTS
WITHIN
EACH
CELL
BUT
MAINTAINS
SOME
SPATIAL
RESOLUTION
AS
THERE
ARE
MANY
CELLS
IT
CREATES
INVARIANCE
TO
CONTRAST
POLARITY
BY
ONLY
USING
THE
GRADIENT
MAGNITUDES
IT
CREATES
INVARIANCE
TO
LOCAL
CONTRAST
STRENGTH
BY
NORMALIZING
RELATIVE
TO
EACH
BLOCK
THE
HOG
DESCRIPTOR
IS
SIMILAR
IN
SPIRIT
TO
THE
SIFT
DESCRIPTOR
BUT
IS
DISTINGUISHED
BY
BEING
INVARIANT
TO
CONTRAST
POLARITY
HAVING
A
HIGHER
SPATIAL
RESOLUTION
OF
COM
PUTED
HISTOGRAMS
AND
PERFORMING
NORMALIZATION
MORE
LOCALLY
BAG
OF
WORDS
DESCRIPTOR
THE
DESCRIPTORS
DISCUSSED
THUS
FAR
HAVE
BEEN
INTENDED
TO
CHARACTERIZE
SMALL
REGIONS
OF
IMAGES
OFTEN
THESE
REGIONS
HAVE
BEEN
CONNECTED
TO
INTEREST
POINTS
THE
BAG
OF
WORDS
REPRESENTATION
ATTEMPTS
TO
CHARACTERIZE
A
LARGER
REGION
OR
AN
ENTIRE
IMAGE
BY
SUMMARIZING
THE
STATISTICS
OF
THE
DESCRIPTORS
E
G
SIFT
ASSOCIATED
WITH
ALL
OF
THE
INTEREST
POINTS
IN
A
REGION
EACH
OBSERVED
DESCRIPTOR
IS
CONSIDERED
TO
BE
ONE
OF
A
FINITE
VOCABULARY
OF
POS
SIBLE
DESCRIPTORS
TERMED
VISUAL
WORDS
COLLECTIVELY
THIS
VOCABULARY
IS
KNOWN
AS
A
DICTIONARY
THE
BAG
OF
WORDS
DESCRIPTOR
IS
SIMPLY
A
HISTOGRAM
DESCRIBING
THE
FREQUENCY
OF
OBSERVING
THESE
WORDS
GIVING
NO
REGARD
TO
THEIR
POSITION
TO
COMPUTE
THE
DICTIONARY
INTEREST
POINTS
ARE
FOUND
IN
A
LARGE
NUMBER
OF
IMAGES
AND
THE
AS
SOCIATED
DESCRIPTOR
IS
COMPUTED
THESE
DESCRIPTORS
ARE
CLUSTERED
USING
K
MEANS
SECTION
TO
COMPUTE
THE
BAG
OF
WORDS
REPRESENTATION
EACH
DESCRIPTOR
IS
ASSIGNED
TO
THE
NEAREST
WORD
IN
THIS
DICTIONARY
THE
BAG
OF
WORDS
REPRESENTATION
IS
A
REMARKABLY
GOOD
SUBSTRATE
FOR
OBJECT
RECOG
NITION
THIS
IS
SOMEWHAT
SURPRISING
GIVEN
THAT
IT
SURRENDERS
ANY
KNOWLEDGE
ABOUT
THE
SPATIAL
CONFIGURATION
ABOUT
THE
OBJECT
OF
COURSE
THE
DRAWBACK
OF
APPROACHES
BASED
ON
THE
BAG
OF
WORDS
IS
THAT
IT
IS
VERY
HARD
TO
LOCALIZE
THE
OBJECT
AFTER
WE
HAVE
IDENTIFIED
ITS
PRESENCE
OR
TO
DECIDE
HOW
MANY
INSTANCES
ARE
PRESENT
USING
THE
SAME
MODEL
A
D
E
FIGURE
SHAPE
CONTEXT
DESCRIP
TOR
A
OBJECT
SILHOUETTE
B
CON
TOUR
OF
SILHOUETTE
C
POINTS
ARE
PLACED
AT
EQUALLY
SPACED
INTERVALS
AROUND
THE
SILHOUETTE
D
A
LOG
POLAR
SAMPLING
ARRAY
IS
CENTERED
AT
EACH
POINT
E
THE
SHAPE
OF
THE
OBJECT
RELATIVE
TO
THIS
POINT
IS
CAPTURED
BY
THE
HISTOGRAM
OVER
THE
BINS
OF
THE
LOG
POLAR
ARRAY
THE
FINAL
DESCRIPTOR
WOULD
CONSIST
OF
A
CONCATENATION
OF
THE
VALUES
FROM
HIS
TOGRAMS
FROM
MULTIPLE
POINTS
AROUND
THE
EDGE
OF
THE
OBJECT
SHAPE
CONTEXT
DESCRIPTOR
FOR
CERTAIN
VISION
TASKS
THE
SILHOUETTE
OF
THE
OBJECT
CONTAINS
MUCH
MORE
INFORMA
TION
THAN
THE
RGB
VALUES
THEMSELVES
CONSIDER
FOR
EXAMPLE
THE
PROBLEM
OF
BODY
POSE
ESTIMATION
GIVEN
AN
IMAGE
OF
A
HUMAN
BEING
THE
GOAL
IS
TO
ESTIMATE
THE
JOINT
ANGLES
OF
THE
BODY
UNFORTUNATELY
THE
RGB
VALUES
OF
THE
IMAGE
DEPEND
ON
THE
PERSON
CLOTHING
AND
ARE
RELATIVELY
UNINFORMATIVE
IN
SUCH
SITUATIONS
IT
IS
WISER
TO
ATTEMPT
TO
CHARACTERIZE
THE
SHAPE
OF
THE
OBJECT
THE
SHAPE
CONTEXT
DESCRIPTOR
IS
A
FIXED
LENGTH
VECTOR
THAT
CHARACTERIZES
THE
OBJECT
CONTOUR
ESSENTIALLY
IT
ENCODES
THE
RELATIVE
POSITION
OF
POINTS
ON
THE
CONTOUR
IN
COMMON
WITH
THE
SIFT
AND
HOG
DESCRIPTORS
IT
POOLS
INFORMATION
LOCALLY
OVER
SPACE
TO
PROVIDE
A
REPRESENTATION
THAT
CAN
CAPTURE
THE
OVERALL
STRUCTURE
OF
THE
OBJECT
BUT
IS
NOT
AFFECTED
TOO
MUCH
BY
SMALL
SPATIAL
VARIATIONS
TO
COMPUTE
THE
SHAPE
CONTEXT
DESCRIPTOR
FIGURE
A
DISCRETE
SET
OF
POINTS
IS
SAMPLED
ALONG
THE
CONTOUR
OF
THE
OBJECT
A
FIXED
LENGTH
VECTOR
IS
ASSOCIATED
WITH
EACH
POINT
THAT
CHARACTERIZES
THE
RELATIVE
POSITION
OF
THE
OTHER
POINTS
TO
THIS
END
A
LOG
POLAR
SAMPLING
ARRAY
IS
CENTERED
ON
THE
CURRENT
POINT
A
HISTOGRAM
IS
THEN
COMPUTED
WHERE
EACH
BIN
CONTAINS
THE
NUMBER
OF
THE
OTHER
POINTS
ON
THE
SILHOUETTE
THAT
FELL
INTO
EACH
BIN
OF
THE
LOG
POLAR
ARRAY
THE
CHOICE
OF
THE
LOG
POLAR
SCHEME
MEANS
THAT
THE
DESCRIPTOR
IS
VERY
SENSITIVE
TO
LOCAL
CHANGES
IN
THE
SHAPE
BUT
ONLY
CAPTURES
THE
APPROXIMATE
CONFIGURATION
OF
DISTANT
PARTS
THE
COLLECTION
OF
HISTOGRAMS
FOR
ALL
OF
THE
POINTS
ON
THIS
IMAGE
CAPTURES
THE
SHAPE
HOWEVER
TO
DIRECTLY
MATCH
TO
ANOTHER
SHAPE
THE
POINT
CORRESPONDENCE
MUST
BE
ESTABLISHED
IT
IS
POSSIBLE
TO
MAKE
THIS
DESCRIPTOR
INVARIANT
TO
ORIENTATION
BY
EVALUATING
THE
ORIENTATION
OF
THE
CONTOUR
AT
EACH
POINT
AND
ROTATING
THE
LOG
POLAR
SAMPLING
SCHEME
SO
THAT
IT
IS
ALIGNED
WITH
THIS
ORIENTATION
DIMENSIONALITY
REDUCTION
IT
IS
OFTEN
DESIRABLE
TO
REDUCE
THE
DIMENSIONALITY
OF
EITHER
THE
ORIGINAL
OR
PRE
PROCESSED
IMAGE
DATA
IF
WE
CAN
DO
THIS
WITHOUT
LOSING
TOO
MUCH
INFORMATION
THEN
THE
RESULTING
MODELS
WILL
REQUIRE
FEWER
PARAMETERS
AND
BE
FASTER
TO
LEARN
AND
TO
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
B
FIGURE
REDUCTION
TO
A
SINGLE
DIMENSION
A
ORIGINAL
DATA
AND
DIRECTION
Φ
OF
MAXIMUM
VARIANCE
B
THE
DATA
ARE
PROJECTED
ONTO
Φ
TO
PRODUCE
A
ONE
DIMENSIONAL
REPRESENTATION
C
TO
RECONSTRUCT
THE
DATA
WE
RE
MULTIPLY
BY
Φ
MOST
OF
THE
ORIGINAL
VARIATION
IS
RETAINED
PCA
EXTENDS
THIS
MODEL
TO
PROJECT
HIGH
DIMENSIONAL
DATA
ONTO
THE
K
ORTHOGONAL
DIMENSIONS
WITH
THE
MOST
VARIANCE
TO
PRODUCE
A
K
DIMENSIONAL
REPRESENTATION
USE
FOR
INFERENCE
DIMENSIONALITY
REDUCTION
IS
POSSIBLE
BECAUSE
A
GIVEN
TYPE
OF
IMAGE
DATA
E
G
RGB
VALUES
FROM
FACE
IMAGES
USUALLY
LIE
IN
A
TINY
SUBSET
OF
THE
POSSIBLE
DATA
SPACE
NOT
ALL
SETS
OF
RGB
VALUES
LOOK
LIKE
REAL
IMAGES
AND
NOT
ALL
REAL
IMAGES
LOOK
LIKE
FACES
WE
REFER
TO
THE
SUBSET
OF
THE
SPACE
OCCUPIED
BY
A
GIVEN
DATASET
AS
A
MANIFOLD
DIMENSIONALITY
REDUCTION
CAN
HENCE
BE
THOUGHT
OF
AS
A
CHANGE
OF
VARIABLES
WE
MOVE
FROM
THE
ORIGINAL
COORDINATE
SYSTEM
TO
THE
REDUCED
COORDINATE
SYSTEM
WITHIN
THE
MANIFOLD
OUR
GOAL
IS
HENCE
TO
FIND
A
LOW
DIMENSIONAL
OR
HIDDEN
REPRESENTATION
H
WHICH
CAN
APPROXIMATELY
EXPLAIN
THE
DATA
X
SO
THAT
X
F
H
Θ
WHERE
F
IS
A
FUNCTION
THAT
TAKES
THE
HIDDEN
VARIABLE
AND
A
SET
OF
PARAMETERS
Θ
WE
WOULD
LIKE
THE
LOWER
DIMENSIONAL
REPRESENTATION
TO
CAPTURE
ALL
OF
THE
RELE
VANT
VARIATION
IN
THE
ORIGINAL
DATA
HENCE
ONE
POSSIBLE
CRITERION
FOR
CHOOSING
THE
PARAMETERS
IS
TO
MINIMIZE
THE
LEAST
SQUARES
RECONSTRUCTION
ERROR
SO
THAT
Θˆ
I
ARGMIN
Θ
I
I
I
XI
F
HI
Θ
T
XI
F
HI
Θ
WHERE
XI
IS
THE
ITH
OF
I
TRAINING
EXAMPLES
IN
OTHER
WORDS
WE
AIM
TO
FIND
A
SET
OF
LOW
DIMENSIONAL
VARIABLES
HI
I
AND
A
MAPPING
FROM
H
TO
X
SO
THAT
IT
RECONSTRUCTS
THE
ORIGINAL
DATA
AS
CLOSELY
AS
POSSIBLE
IN
A
LEAST
SQUARES
SENSE
DIMENSIONALITY
REDUCTION
APPROXIMATION
WITH
A
SINGLE
NUMBER
LET
US
FIRST
CONSIDER
A
VERY
SIMPLE
MODEL
IN
WHICH
WE
ATTEMPT
TO
REPRESENT
EACH
OBSERVED
DATUM
WITH
A
SINGLE
NUMBER
FIGURE
SO
THAT
XI
ΦHI
Μ
WHERE
THE
PARAMETER
Μ
IS
THE
MEAN
OF
THE
DATA
SET
AND
THE
PARAMETER
Φ
IS
A
BASIS
VECTOR
MAPPING
THE
LOW
DIMENSIONAL
REPRESENTATION
H
BACK
TO
THE
ORIGINAL
DATA
SPACE
X
FOR
SIMPLICITY
WE
WILL
ASSUME
FROM
NOW
ON
THAT
THE
MEAN
OF
THE
DATASET
IS
ZERO
AND
XI
ΦHI
THIS
CAN
BE
ACHIEVED
BY
COMPUTING
THE
EMPIRICAL
MEAN
Μ
AND
SUBTRACTING
IT
FROM
EVERY
EXAMPLE
XI
THE
LEARNING
ALGORITHM
OPTIMIZES
THE
CRITERION
Φˆ
I
ARGMIN
E
ARGMIN
R
I
XI
ΦHI
T
XI
ΦHI
Φ
I
Φ
I
I
CAREFUL
CONSIDERATION
OF
THE
COST
FUNCTION
EQUATION
REVEALS
AN
IMMEDIATE
PROBLEM
THE
SOLUTION
IS
AMBIGUOUS
AS
WE
CAN
MULTIPLY
THE
BASIS
FUNCTION
Φ
BY
ANY
CONSTANT
K
AND
DIVIDE
EACH
OF
THE
HIDDEN
VARIABLES
HI
I
BY
THE
SAME
NUMBER
TO
YIELD
EXACTLY
THE
SAME
COST
TO
RESOLVE
THIS
PROBLEM
WE
FORCE
THE
VECTOR
Φ
TO
HAVE
UNIT
LENGTH
THIS
IS
ACCOMPLISHED
BY
ADDING
IN
A
LAGRANGE
MULTIPLIER
Λ
SO
THAT
THE
COST
FUNCTION
BECOMES
I
E
XI
ΦHI
T
XI
ΦHI
Λ
ΦT
Φ
I
XT
XI
XI
Λ
ΦT
Φ
TO
MINIMIZE
THE
FUNCTION
WE
FIRST
TAKE
THE
DERIVATIVE
WITH
RESPECT
TO
HI
AND
THEN
EQUATE
THE
RESULTING
EXPRESSION
TO
ZERO
TO
YIELD
ˆ
ˆT
HI
Φ
XI
IN
OTHER
WORDS
TO
FIND
THE
REDUCED
DIMENSION
REPRESENTATION
HI
WE
SIMPLY
PROJECT
THE
OBSERVED
DATA
ONTO
THE
VECTOR
Φ
WE
NOW
TAKE
THE
DERIVATIVE
OF
EQUATION
WITH
RESPECT
TO
Φ
SUBSTITUTE
IN
THE
SOLUTION
FOR
HI
EQUATE
THE
RESULT
TO
ZERO
AND
RE
ARRANGE
TO
GET
I
T
ˆ
ˆ
I
OR
IN
MATRIX
FORM
I
XXT
Φˆ
ΛΦˆ
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
WHERE
THE
MATRIX
X
XI
CONTAINS
THE
DATA
EXAMPLES
IN
ITS
COLUMNS
THIS
IS
AN
EIGENVALUE
PROBLEM
TO
FIND
THE
OPTIMAL
VECTOR
WE
COMPUTE
THE
SVD
ULVT
XXT
AND
CHOOSE
THE
FIRST
COLUMN
OF
U
THE
SCATTER
MATRIX
XXT
IS
A
CONSTANT
MULTIPLE
OF
THE
COVARIANCE
MATRIX
AND
SO
THIS
HAS
A
SIMPLE
GEOMETRIC
INTERPRETATION
THE
OPTIMAL
VECTOR
Φ
TO
PROJECT
ONTO
CORRESPONDS
TO
THE
PRINCIPAL
DIRECTION
OF
THE
COVARIANCE
ELLIPSE
THIS
MAKES
INTUITIVE
SENSE
WE
RETAIN
INFORMATION
FROM
THE
DIRECTION
IN
SPACE
WHERE
THE
DATA
VARY
MOST
PRINCIPAL
COMPONENT
ANALYSIS
PRINCIPAL
COMPONENT
ANALYSIS
PCA
GENERALIZES
THE
ABOVE
MODEL
INSTEAD
OF
FIND
ING
A
SCALAR
VARIABLE
HI
THAT
REPRESENTS
THE
ITH
DATA
EXAMPLE
XI
WE
NOW
SEEK
A
K
DIMENSIONAL
VECTOR
HI
THE
RELATION
BETWEEN
THE
HIDDEN
AND
OBSERVED
SPACES
IS
XI
ΦHI
WHERE
THE
MATRIX
Φ
ΦK
CONTAINS
K
BASIS
FUNCTIONS
OR
PRINCIPAL
COMPONENTS
THE
OBSERVED
DATA
ARE
MODELED
AS
A
WEIGHTED
SUM
OFTHE
PRINCIPAL
COMPONENTS
WHERE
THE
KTH
DIMENSION
OF
HI
WEIGHTS
THE
KTH
COMPONENT
THE
SOLUTION
FOR
THE
UNKNOWNS
Φ
AND
I
CAN
NOW
BE
WRITTEN
AS
Φ
I
ARGMIN
E
ARGMIN
R
I
XI
ΦHI
T
XI
ΦHI
Φ
I
Φ
I
I
ONCE
MORE
THE
SOLUTION
TO
THIS
IS
NON
UNIQUE
AS
WE
CAN
POST
MULTIPLY
Φ
BY
ANY
MATRIX
A
AND
PRE
MULTIPLY
EACH
HIDDEN
VARIABLE
HI
BY
THE
INVERSE
A
AND
STILL
GET
THE
SAME
COST
TO
PARTIALLY
RESOLVE
THIS
PROBLEM
WE
ADD
THE
EXTRA
CONSTRAINT
THAT
ΦT
Φ
I
IN
OTHER
WORDS
WE
FORCE
THE
PRINCIPAL
COMPONENTS
TO
BE
ORTHOGONAL
AND
LENGTH
ONE
THIS
GIVES
A
MODIFIED
COST
FUNCTION
OF
I
E
XI
ΦHI
T
XI
ΦHI
Λ
ΦT
Φ
I
I
WHERE
Λ
IS
A
LAGRANGE
MULTIPLIER
WE
NOW
MINIMIZE
THIS
EXPRESSION
WITH
RESPECT
TO
Φ
I
AND
Λ
THE
EXPRESSION
FOR
THE
HIDDEN
VARIABLES
BECOMES
HI
ΦT
XI
THE
K
PRINCIPAL
COMPONENTS
Φ
ΦK
ARE
NOW
FOUND
BY
COMPUTING
THE
SINGULAR
VALUE
DECOMPOSITION
ULVT
XXT
AND
TAKING
THE
FIRST
K
COLUMNS
OF
U
IN
OTHER
WORDS
TO
REDUCE
THE
DIMENSIONALITY
WE
PROJECT
THE
DATA
XI
ONTO
A
HYPERPLANE
DEFINED
BY
THE
K
LARGEST
AXES
OF
THE
COVARIANCE
ELLIPSOID
THIS
ALGORITHM
IS
VERY
CLOSELY
RELATED
TO
PROBABILISTIC
PRINCIPAL
COMPONENT
ANAL
YSIS
SECTION
PROBABILISTIC
PCA
ADDITIONALLY
MODELS
THE
NOISE
THAT
ACCOUNTS
FOR
THE
INEXACT
APPROXIMATION
IN
EQUATION
FACTOR
ANALYSIS
SECTION
IS
ALSO
VERY
SIMILAR
BUT
CONSTRUCTS
A
MORE
SOPHISTICATED
MODEL
OF
THIS
NOISE
DIMENSIONALITY
REDUCTION
DUAL
PRINCIPAL
COMPONENT
ANALYSIS
THE
PRECEDING
METHOD
REQUIRES
US
TO
COMPUTE
THE
SVD
OF
THE
SCATTER
MATRIX
XXT
UNFORTUNATELY
IF
THE
DATA
HAD
DIMENSION
D
THEN
THIS
IS
A
D
D
MATRIX
WHICH
MAY
BE
VERY
LARGE
WE
CAN
SIDESTEP
THIS
PROBLEM
BY
USING
DUAL
VARIABLES
WE
DEFINE
Φ
AS
A
WEIGHTED
SUM
OF
THE
ORIGINAL
DATA
POINTS
SO
THAT
Φ
XΨ
WHERE
Ψ
ΨK
IS
A
I
K
MATRIX
REPRESENTING
THESE
WEIGHTS
THE
ASSOCIATED
COST
FUNCTION
NOW
BECOMES
ALGORITHM
I
E
XI
XΨHI
T
XI
XΨHI
Λ
ΨT
XT
XΨ
I
I
THE
SOLUTION
FOR
THE
HIDDEN
VARIABLES
BECOMES
HI
ΨT
XT
XI
ΦT
XI
AND
THE
K
DUAL
PRINCIPAL
COMPONENTS
Ψ
ΨK
ARE
EXTRACTED
FROM
THE
MATRIX
U
IN
THE
SVD
ULVT
XT
X
THIS
IS
A
SMALLER
PROBLEM
OF
SIZE
I
I
AND
IS
MORE
EFFICIENT
WHEN
THE
NUMBER
OF
DATA
EXAMPLES
I
IS
LESS
THAN
THE
DIMENSIONALITY
OF
THE
OBSERVED
SPACE
D
NOTICE
THAT
THIS
ALGORITHM
DOES
NOT
REQUIRE
THE
ORIGINAL
DATA
POINTS
IT
ONLY
REQUIRES
THE
INNER
PRODUCTS
BETWEEN
THEM
AND
SO
IT
IS
AMENABLE
TO
KERNELIZATION
THIS
RESULTING
METHOD
IS
KNOWN
AS
KERNEL
PCA
THE
K
MEANS
ALGORITHM
A
SECOND
COMMON
APPROACH
TO
DIMENSIONALITY
REDUCTION
IS
TO
ABANDON
A
CONTINUOUS
REPRESENTATION
ALTOGETHER
AND
REPRESENT
EACH
DATA
POINT
USING
ONE
OF
A
LIMITED
SET
OF
PROTOTYPE
VECTORS
IN
THIS
MODEL
THE
DATA
ARE
APPROXIMATED
AS
XI
ΜHI
WHERE
HI
K
IS
AN
INDEX
THAT
IDENTIFIES
WHICH
OF
THE
K
PROTOTYPE
VECTORS
ALGORITHM
ΜK
K
APPROXIMATES
THE
ITH
EXAMPLE
TO
FIND
THE
ASSIGNMENT
INDICES
AND
THE
PROTOTYPE
VECTORS
FIGURE
WE
OPTIMIZE
K
I
ARGMIN
Μ
H
I
I
XI
ΜHI
T
XI
ΜHI
IN
THE
K
MEANS
ALGORITHM
THIS
COST
FUNCTION
IS
MINIMIZED
USING
AN
ALTERNATING
STRATEGY
IN
WHICH
WE
FIRST
ASSIGN
EACH
DATAPOINT
TO
THE
NEAREST
PROTOTYPE
HˆI
ARGMIN
XI
HI
ΜHI
T
X
ΜHI
L
FIGURE
K
MEANS
ALGORITHM
FOR
K
CLUSTERS
A
WE
INITIALIZE
THE
THREE
PROTOTYPE
VECTORS
CROSSES
TO
RANDOM
POSITIONS
WE
ALTERNATELY
B
ASSIGN
THE
DATA
TO
THE
NEAREST
PROTOTYPE
VECTOR
AND
C
UPDATE
THE
PROTOTYPE
VECTORS
TO
BE
EQUAL
TO
THE
MEAN
OF
THE
POINTS
ASSIGNED
TO
THEM
D
I
WE
REPEAT
THESE
STEPS
UNTIL
THERE
IS
NO
FURTHER
CHANGE
AND
THEN
UPDATE
THE
PROTOTYPES
ΜˆK
ARGMIN
ΜK
I
I
XI
ΜHI
T
XI
ΜHI
I
I
I
XIΔ
HI
K
Δ
HI
K
I
WHERE
Δ
IS
A
FUNCTION
THAT
RETURNS
ONE
WHEN
ITS
ARGUMENT
IS
ZERO
AND
ZERO
OTHER
WISE
IN
OTHER
WORDS
THE
NEW
PROTOTYPE
ΜˆK
IS
SIMPLY
THE
AVERAGE
OF
THE
DATA
POINTS
THAT
ARE
ASSIGNED
TO
THIS
CLUSTER
THIS
ALGORITHM
IS
NOT
GUARANTEED
TO
CONVERGE
TO
THE
GLOBAL
MINIMUM
AND
SO
IT
REQUIRES
SENSIBLE
STARTING
CONDITIONS
THE
K
MEANS
ALGORITHM
IS
VERY
CLOSELY
RELATED
TO
THE
MIXTURES
OF
GAUSSIANS
MODEL
SECTION
THE
MAIN
DIFFERENCES
ARE
THAT
THE
MIXTURES
OF
GAUSSIANS
MODEL
IS
PROBABILISTIC
AND
DEFINES
A
DENSITY
OVER
THE
DATA
SPACE
IT
ALSO
ASSIGNS
WEIGHTS
TO
THE
CLUSTERS
AND
DESCRIBES
THEIR
COVARIANCE
CONCLUSION
CAREFUL
READING
OF
THE
INFORMATION
IN
THIS
CHAPTER
SHOULD
CONVINCE
YOU
THAT
THERE
ARE
CERTAIN
RECURRING
IDEAS
IN
IMAGE
PREPROCESSING
TO
MAKE
A
DESCRIPTOR
INVARIANT
TO
INTENSITY
CHANGES
WE
FILTER
THE
IMAGE
AND
NORMALIZE
THE
FILTER
RESPONSES
OVER
THE
REGION
A
UNIQUE
DESCRIPTOR
ORIENTATION
AND
SCALE
CAN
BE
COMPUTED
BY
MAXIMIZING
OVER
RESPONSES
AT
DIFFERENT
ORIENTATIONS
AND
SCALES
TO
CREATE
INVARIANCE
TO
SMALL
SPATIAL
CHANGES
LOCAL
RESPONSES
ARE
POOLED
DESPITE
THE
SIMPLICITY
OF
THESE
IDEAS
IT
IS
REMARKABLE
HOW
MUCH
IMPACT
THEY
HAVE
ON
THE
PERFORMANCE
OF
REAL
SYSTEMS
PROBLEM
PROBLEM
PROBLEM
IMAGE
PREPROCESSING
AND
FEATURE
EXTRACTION
NOTES
IMAGE
PROCESSING
THERE
ARE
NUMEROUS
TEXTS
ON
IMAGE
PROCESSING
WHICH
CONTAIN
FAR
MORE
INFORMATION
THAN
I
COULD
INCLUDE
IN
THIS
CHAPTER
I
WOULD
PARTICULARLY
RECOMMEND
THE
BOOKS
BY
O
GORMAN
ET
AL
GONZALEZ
WOODS
PRATT
AND
NIXON
AGUADO
A
COMPREHENSIVE
RECENT
SUMMARY
OF
LOCAL
IMAGE
FEATURES
CAN
BE
FOUND
IN
LI
ALLINSON
EDGE
AND
CORNER
DETECTION
THE
CANNY
EDGE
DETECTOR
WAS
FIRST
DESCRIBED
IN
CANNY
ELDER
INVESTIGATED
WHETHER
IT
WAS
POSSIBLE
TO
RECONSTRUCT
AN
IMAGE
BASED
ON
EDGE
INFORMATION
ALONE
NOWADAYS
IT
IS
COMMON
TO
USE
MACHINE
LEARNING
METHODS
TO
IDENTIFY
OBJECT
BOUNDARIES
IN
IMAGES
E
G
DOLL
AR
ET
AL
EARLY
WORK
IN
CORNER
DETECTION
INTEREST
POINT
DETECTION
INCLUDES
THAT
OF
MORAVEC
FORSTNER
AND
THE
HARRIS
CORNER
DETECTOR
HARRIS
STEPHENS
WHICH
WE
DE
SCRIBED
IN
THIS
CHAPTER
OTHER
MORE
RECENT
EFFORTS
TO
IDENTIFY
STABLE
POINTS
AND
REGIONS
INCLUDE
THE
SUSAN
CORNER
DETECTOR
SMITH
BRADY
A
SALIENCY
BASED
DESCRIPTOR
KADIR
BRADY
MAXIMALLY
STABLE
EXTREMAL
REGIONS
MATAS
ET
AL
THE
SIFT
DETECTOR
LOWE
AND
THE
FAST
DETECTOR
ROSTEN
DRUMMOND
THERE
HAS
BEEN
CONSIDERABLE
RECENT
INTEREST
IN
AFFINE
INVARIANT
INTEREST
POINT
DETECTION
WHICH
AIMS
TO
FIND
FEATURES
THAT
ARE
STABLE
UNDER
AFFINE
TRANSFORMS
OF
THE
IMAGE
E
G
SCHAFFALITZKY
ZISSERMAN
MIKOLAJCZYK
SCHMID
MIKOLAJCZYK
SCHMID
MIKOLAJCZYK
ET
AL
PRESENT
A
QUANTITATIVE
COMPARISON
OF
DIFFERENT
AFFINE
REGION
DETECTORS
A
RECENT
REVIEW
OF
THIS
AREA
CAN
BE
FOUND
IN
TUYTELAARS
MIKOLAJCZYK
IMAGE
DESCRIPTORS
FOR
ROBUST
OBJECT
RECOGNITION
AND
IMAGE
MATCHING
IT
IS
CRUCIAL
TO
CHARACTERIZE
THE
REGION
AROUND
THE
DETECTED
INTEREST
POINT
IN
A
WAY
THAT
IS
COMPACT
AND
STA
BLE
TO
CHANGES
IN
THE
IMAGE
TO
THIS
END
LOWE
DEVELOPED
THE
SIFT
DESCRIPTOR
DALAL
TRIGGS
DEVELOPED
THE
HOG
DESCRIPTOR
AND
FORSS
EN
LOWE
DEVELOPED
A
DESCRIPTOR
FOR
USE
WITH
MAXIMALLY
STABLE
EXTREMAL
REGIONS
BAY
ET
AL
DEVELOPED
A
VERY
EFFICIENT
VERSION
OF
SIFT
FEATURES
KNOWN
AS
SURF
MIKOLAJCZYK
SCHMID
PRESENT
A
QUANTITATIVE
COMPARISON
OF
REGION
DESCRIPTORS
RECENT
WORK
ON
IMAGE
DESCRIPTORS
HAS
APPLIED
MACHINE
LEARNING
TECHNIQUES
TO
OPTIMIZE
THEIR
PERFORMANCE
BROWN
ET
AL
PHILBIN
ET
AL
MORE
INFORMATION
ABOUT
LOCAL
BINARY
PATTERNS
CAN
BE
FOUND
IN
OJALA
ET
AL
MORE
INFORMATION
ABOUT
THE
SHAPE
CONTEXT
DESCRIPTOR
CAN
BE
FOUND
IN
BELONGIE
ET
AL
DIMENSIONALITY
REDUCTION
PRINCIPAL
COMPONENTS
ANALYSIS
IS
A
LINEAR
DIMENSIONALITY
REDUCTION
METHOD
HOWEVER
THERE
ARE
ALSO
MANY
NONLINEAR
APPROACHES
THAT
DESCRIBE
A
MANIFOLD
OF
IMAGES
IN
HIGH
DIMENSIONS
WITH
FEWER
PARAMETERS
NOTABLE
METHODS
INCLUDE
KERNEL
PCA
SCH
OLKOPF
ET
AL
ISOMAP
TENENBAUM
ET
AL
LOCAL
LINEAR
EMBED
DING
ROWEIS
SAUL
CHARTING
BRAND
THE
GAUSSIAN
PROCESS
LATENT
VARIABLE
MODEL
LAWRENCE
AND
LAPLACIAN
EIGENMAPS
BELKIN
NIYOGI
RECENT
REVIEWS
OF
DIMENSIONALITY
REDUCTION
CAN
BE
FOUND
IN
BURGESS
AND
DE
LA
TORRE
PROBLEMS
PROBLEM
CONSIDER
AN
BIT
IMAGE
IN
WHICH
THE
PIXEL
VALUES
ARE
EVENLY
DISTRIBUTED
IN
THE
RANGE
WITH
NO
PIXELS
TAKING
A
VALUE
OF
OR
LARGER
DRAW
THE
CUMULATIVE
HISTOGRAM
FOR
THIS
IMAGE
SEE
FIGURE
WHAT
WILL
THE
HISTOGRAM
OF
PIXEL
INTENSITIES
FIGURE
CLUSTERING
WITH
THE
K
MEANS
ALGORITHM
IN
THE
PRESENCE
OF
OUTLIERS
PROBLEM
THIS
DATA
SET
CONTAINS
TWO
CLUSTERS
AND
A
SINGLE
OUT
LIER
THE
POINT
ON
THE
RIGHT
HAND
SIDE
THE
OUTLIER
CAUSES
PROBLEMS
FOR
THE
K
MEANS
ALGORITHM
WHEN
K
CLUSTERS
ARE
USED
DUE
TO
THE
IMPLICIT
ASSUMP
TION
THAT
THE
CLUSTERS
CAN
BE
MODELED
AS
NORMAL
DISTRIBUTIONS
WITH
SPHERICAL
COVARIANCE
LOOK
LIKE
AFTER
APPLYING
HISTOGRAM
EQUALIZATION
PROBLEM
CONSIDER
A
CONTINUOUS
IMAGE
P
I
J
AND
A
CONTINUOUS
FILTER
F
N
M
IN
THE
CONTINUOUS
DOMAIN
THE
OPERATION
F
P
OF
CONVOLVING
AN
IMAGE
WITH
THE
FILTER
IS
DEFINED
AS
F
P
P
I
M
J
N
F
M
N
DMDN
NOW
CONSIDER
TWO
FILTERS
F
AND
G
PROVE
THAT
CONVOLVING
THE
IMAGE
FIRST
WITH
F
AND
THEN
WITH
G
HAS
THE
SAME
EFFECT
AS
CONVOLVING
F
WITH
G
AND
THEN
CONVOLVING
THE
IMAGE
WITH
THE
RESULT
IN
OTHER
WORDS
G
F
P
G
F
P
DOES
THIS
RESULT
EXTEND
TO
DISCRETE
IMAGES
PROBLEM
DESCRIBE
THE
SERIES
OF
OPERATIONS
THAT
WOULD
BE
REQUIRED
TO
COMPUTE
THE
HAAR
LIKE
FILTERS
IN
FIGURES
D
FROM
AN
INTEGRAL
IMAGE
HOW
MANY
POINTS
FROM
THE
INTEGRAL
IMAGE
ARE
NEEDED
TO
COMPUTE
EACH
PROBLEM
CONSIDER
A
BLURRING
FILTER
WHERE
EACH
PIXEL
IN
AN
IMAGE
IS
REPLACED
BY
A
WEIGHTED
AVERAGE
OF
LOCAL
INTENSITY
VALUES
BUT
THE
THE
WEIGHTS
DECREASE
IF
THESE
INTENSITY
VALUES
DIFFER
MARKEDLY
FROM
THE
CENTRAL
PIXEL
WHAT
EFFECT
WOULD
THIS
BILATERAL
FILTER
HAVE
WHEN
APPLIED
TO
AN
IMAGE
PROBLEM
DEFINE
A
FILTER
THAT
IS
SPECIALIZED
TO
DETECTING
LUMINANCE
CHANGES
AT
A
ANGLE
AND
GIVES
A
POSITIVE
RESPONSE
WHERE
THE
IMAGE
INTENSITY
INCREASES
FROM
THE
BOTTOM
LEFT
TO
THE
BOTTOM
RIGHT
OF
THE
IMAGE
PROBLEM
DEFINE
A
FILTER
THAT
RESPONDS
TO
THE
SECOND
DERIVATIVE
IN
THE
HORIZONTAL
DIRECTION
BUT
IS
INVARIANT
TO
THE
GRADIENT
AND
ABSOLUTE
INTENSITY
IN
THE
HORIZONTAL
DIRECTION
AND
INVARIANT
TO
ALL
CHANGES
IN
THE
VERTICAL
DIRECTION
PROBLEM
WHY
ARE
MOST
LOCAL
BINARY
PATTERNS
IN
A
NATURAL
IMAGE
TYPICALLY
UNIFORM
OR
NEAR
UNIFORM
PROBLEM
GIVE
ONE
EXAMPLE
OF
A
DATA
SET
WHERE
THE
MIXTURES
OF
GAUSSIANS
MODEL
WILL
SUCCEED
IN
CLUSTERING
THE
DATA
BUT
THE
K
MEANS
ALGORITHM
WILL
FAIL
PROBLEM
CONSIDER
THE
DATA
IN
FIGURE
WHAT
DO
YOU
EXPECT
TO
HAPPEN
IF
WE
RUN
THE
K
MEANS
ALGORITHM
WITH
TWO
CLUSTERS
ON
THIS
DATA
SET
SUGGEST
A
WAY
TO
RESOLVE
THIS
PROBLEM
PROBLEM
AN
ALTERNATIVE
APPROACH
TO
CLUSTERING
THE
DATA
WOULD
BE
TO
FIND
MODES
PEAKS
IN
THE
DENSITY
OF
THE
POINTS
THIS
POTENTIALLY
HAS
THE
ADVANTAGE
OF
ALSO
AUTOMAT
ICALLY
SELECTING
THE
NUMBER
OF
CLUSTERS
PROPOSE
AN
ALGORITHM
TO
FIND
THESE
MODES
PART
V
MODELS
FOR
GEOMETRY
PART
V
MODELS
FOR
GEOMETRY
IN
PART
V
WE
FINALLY
ACKNOWLEDGE
THE
PROCESS
BY
WHICH
REAL
WORLD
IMAGES
ARE
FORMED
LIGHT
IS
EMITTED
FROM
ONE
OR
MORE
SOURCES
AND
TRAVELS
THROUGH
THE
SCENE
INTERACTING
WITH
THE
MATERIALS
VIA
PHYSICAL
PROCESSES
SUCH
AS
REFLECTION
REFRACTION
AND
SCATTERING
SOME
OF
THIS
LIGHT
ENTERS
THE
CAMERA
AND
IS
MEASURED
WE
HAVE
A
VERY
GOOD
UNDERSTANDING
OF
THIS
FORWARD
MODEL
GIVEN
KNOWN
GEOMETRY
LIGHT
SOURCES
AND
MATERIAL
PROPERTIES
COMPUTER
GRAPHICS
TECHNIQUES
CAN
SIMULATE
WHAT
WILL
BE
SEEN
BY
THE
CAMERA
VERY
ACCURATELY
THE
ULTIMATE
GOAL
FOR
A
VISION
ALGORITHM
WOULD
BE
A
COMPLETE
RECONSTRUCTION
IN
WHICH
WE
AIM
TO
INVERT
THIS
FORWARD
MODEL
AND
ESTIMATE
THE
LIGHT
SOURCES
MATERIALS
AND
GEOMETRY
FROM
THE
IMAGE
HERE
WE
AIM
TO
CAPTURE
A
STRUCTURAL
DESCRIPTION
OF
THE
WORLD
WE
SEEK
AN
UNDERSTANDING
OF
WHERE
THINGS
ARE
AND
TO
MEASURE
THEIR
OPTICAL
PROPERTIES
RATHER
THAN
A
SEMANTIC
UNDERSTANDING
SUCH
A
STRUCTURAL
DESCRIPTION
CAN
BE
EXPLOITED
TO
NAVIGATE
AROUND
THE
ENVIRONMENT
OR
BUILD
MODELS
FOR
COMPUTER
GRAPHICS
UNFORTUNATELY
FULL
VISUAL
RECONSTRUCTION
IS
VERY
CHALLENGING
FOR
ONE
THING
THE
SOLUTION
IS
NON
UNIQUE
FOR
EXAMPLE
IF
THE
LIGHT
SOURCE
INTENSITY
INCREASES
BUT
THE
OBJECT
REFLECTANCE
DECREASES
COMMENSURATELY
THE
IMAGE
WILL
REMAIN
UNCHANGED
OF
COURSE
WE
COULD
MAKE
THE
PROBLEM
UNIQUE
BY
IMPOSING
PRIOR
KNOWLEDGE
BUT
EVEN
THEN
RECONSTRUCTION
REMAINS
DIFFICULT
IT
IS
HARD
TO
EFFECTIVELY
PARAMETERIZE
THE
SCENE
AND
THE
PROBLEM
IS
HIGHLY
NON
CONVEX
IN
THIS
PART
OF
THE
BOOK
WE
CONSIDER
A
FAMILY
OF
MODELS
THAT
APPROXIMATE
BOTH
THE
SCENE
AND
THE
OBSERVED
IMAGE
WITH
SPARSE
SETS
OF
VISUAL
PRIMITIVES
POINTS
THE
FORWARD
MODEL
THAT
MAPS
THE
PROXY
REPRESENTATION
OF
THE
WORLD
POINTS
TO
THE
PROXY
REPRESENTATION
OF
THE
IMAGE
POINTS
IS
MUCH
SIMPLER
THAN
THE
FULL
LIGHT
TRANSPORT
MODEL
AND
IS
CALLED
THE
PROJECTIVE
PINHOLE
CAMERA
WE
INVESTIGATE
THE
PROPERTIES
OF
THIS
MODEL
IN
CHAPTER
IN
CHAPTER
WE
CONSIDER
THE
SITUATION
WHERE
THE
PINHOLE
CAMERA
VIEWS
A
PLANE
IN
THE
WORLD
THERE
IS
NOW
A
ONE
TO
ONE
MAPPING
BETWEEN
POINTS
ON
THE
PLANE
AND
POINTS
IN
THE
IMAGE
AND
WE
CHARACTERIZE
THIS
MAPPING
WITH
A
FAMILY
OF
TRANSFORMATIONS
IN
CHAPTER
WE
WILL
FURTHER
EXPLOIT
THE
PINHOLE
CAMERA
MODEL
TO
RECOVER
A
SPARSE
GEOMETRIC
MODEL
OF
THE
SCENE
CHAPTER
THE
PINHOLE
CAMERA
THIS
CHAPTER
INTRODUCES
THE
PINHOLE
OR
PROJECTIVE
CAMERA
THIS
IS
A
PURELY
GEOMETRIC
MODEL
THAT
DESCRIBES
THE
PROCESS
WHEREBY
POINTS
IN
THE
WORLD
ARE
PROJECTED
INTO
THE
IMAGE
CLEARLY
THE
POSITION
IN
THE
IMAGE
DEPENDS
ON
THE
POSITION
IN
THE
WORLD
AND
THE
PINHOLE
CAMERA
MODEL
CAPTURES
THIS
RELATIONSHIP
TO
MOTIVATE
THIS
MODEL
WE
WILL
CONSIDER
THE
PROBLEM
OF
SPARSE
STEREO
RECONSTRUC
TION
FIGURE
WE
ARE
GIVEN
TWO
IMAGES
OF
A
RIGID
OBJECT
TAKEN
FROM
DIFFERENT
POSITIONS
LET
US
ASSUME
THAT
WE
CAN
IDENTIFY
CORRESPONDING
FEATURES
BETWEEN
THE
TWO
IMAGES
POINTS
THAT
ARE
PROJECTED
VERSIONS
OF
THE
SAME
POSITION
IN
THE
WORLD
NOW
THE
GOAL
IS
TO
ESTABLISH
THIS
POSITION
USING
THE
OBSERVED
FEATURE
POINTS
THE
RESULTING
INFORMATION
COULD
BE
USED
BY
A
ROBOT
TO
HELP
IT
NAVIGATE
THROUGH
THE
SCENE
OR
TO
FACILITATE
OBJECT
RECOGNITION
THE
PINHOLE
CAMERA
IN
REAL
LIFE
A
PINHOLE
CAMERA
CONSISTS
OF
A
CLOSED
WITH
A
SMALL
HOLE
THE
PINHOLE
IN
THE
FRONT
FIGURE
RAYS
FROM
AN
OBJECT
IN
THE
WORLD
PASS
THROUGH
THIS
HOLE
TO
FORM
AN
INVERTED
IMAGE
ON
THE
BACK
FACE
OF
THE
BOX
OR
IMAGE
PLANE
OUR
GOAL
IS
TO
BUILD
A
MATHEMATICAL
MODEL
OF
THIS
PROCESS
IT
IS
SLIGHTLY
INCONVENIENT
THAT
THE
IMAGE
FROM
THE
PINHOLE
CAMERA
IS
UPSIDE
DOWN
HENCE
WE
INSTEAD
CONSIDER
THE
VIRTUAL
IMAGE
THAT
WOULD
RESULT
FROM
PLACING
THE
IMAGE
PLANE
IN
FRONT
OF
THE
PINHOLE
OF
COURSE
IT
IS
NOT
PHYSICALLY
POSSIBLE
TO
BUILD
A
CAMERA
THIS
WAY
BUT
IT
IS
MATHEMATICALLY
EQUIVALENT
TO
THE
TRUE
PINHOLE
MODEL
EXCEPT
THAT
THE
IMAGE
IS
THE
RIGHT
WAY
UP
AND
IT
IS
EASIER
TO
THINK
ABOUT
FROM
NOW
ON
WE
WILL
ALWAYS
DRAW
THE
IMAGE
PLANE
IN
FRONT
OF
THE
PINHOLE
FIGURE
ILLUSTRATES
THE
PINHOLE
CAMERA
MODEL
AND
DEFINES
SOME
TERMINOLOGY
THE
PINHOLE
ITSELF
THE
POINT
AT
WHICH
THE
RAYS
CONVERGE
IS
CALLED
THE
OPTICAL
CENTER
WE
WILL
ASSUME
FOR
NOW
THAT
THE
OPTICAL
CENTER
IS
AT
THE
ORIGIN
OF
THE
WORLD
COORDINATE
SYSTEM
IN
WHICH
POINTS
ARE
REPRESENTED
AS
W
U
V
W
T
THE
VIRTUAL
IS
NOT
AN
ACCIDENTAL
CHOICE
OF
WORLD
THE
TERM
CAMERA
IS
DERIVED
FROM
THE
LATIN
WORD
FOR
CHAMBER
FIGURE
SPARSE
STEREO
RECONSTRUCTION
A
B
WE
ARE
GIVEN
TWO
IMAGES
OF
THE
SAME
SCENE
TAKEN
FROM
DIFFERENT
POSITIONS
AND
A
SET
OF
I
PAIRS
OF
POINTS
IN
THESE
IMAGES
THAT
ARE
KNOWN
TO
CORRESPOND
TO
THE
SAME
POINTS
IN
THE
WORLD
E
G
THE
POINTS
CONNECTED
BY
THE
RED
LINE
ARE
A
CORRESPONDING
PAIR
C
OUR
GOAL
IS
TO
ESTABLISH
THE
POSITION
OF
EACH
OF
THE
WORLD
POINTS
HERE
THE
DEPTH
IS
ENCODED
BY
COLOR
SO
THAT
CLOSER
POINTS
ARE
RED
AND
MORE
DISTANT
POINTS
ARE
BLUE
FIGURE
THE
PINHOLE
CAMERA
MODEL
RAYS
FROM
AN
OBJECT
IN
THE
WORLD
PASS
THROUGH
THE
PINHOLE
IN
THE
FRONT
OF
THE
CAMERA
AND
FORM
AN
IMAGE
ON
THE
BACK
PLANE
THE
IMAGE
PLANE
THIS
IMAGE
IS
UPSIDE
DOWN
SO
WE
CAN
ALTERNATIVELY
CONSIDER
THE
VIRTUAL
IMAGE
THAT
WOULD
HAVE
BEEN
CREATED
IF
THE
IMAGE
PLANE
WAS
IN
FRONT
OF
THE
PINHOLE
THIS
IS
NOT
PHYSICALLY
POSSIBLE
BUT
IT
IS
MORE
CONVENIENT
TO
WORK
WITH
IMAGE
IS
CREATED
ON
THE
IMAGE
PLANE
WHICH
IS
DISPLACED
FROM
THE
OPTICAL
CENTER
ALONG
THE
W
AXIS
OR
OPTICAL
AXIS
THE
POINT
WHERE
THE
OPTICAL
AXIS
STRIKES
THE
IMAGE
PLANE
IS
KNOWN
AS
THE
PRINCIPAL
POINT
THE
DISTANCE
BETWEEN
THE
PRINCIPAL
POINT
AND
THE
OPTICAL
CENTER
I
E
THE
DISTANCE
BETWEEN
THE
IMAGE
PLANE
AND
THE
PINHOLE
IS
KNOWN
AS
THE
FOCAL
LENGTH
THE
PINHOLE
CAMERA
MODEL
IS
A
GENERATIVE
MODEL
THAT
DESCRIBES
THE
LIKELIHOOD
P
R
X
W
OF
OBSERVING
A
FEATURE
AT
POSITION
X
X
Y
T
IN
THE
IMAGE
GIVEN
THAT
IT
IS
THE
PROJECTION
OF
A
POINT
W
U
V
W
T
IN
THE
WORLD
ALTHOUGH
LIGHT
TRANSPORT
FIGURE
PIN
HOLE
CAMERA
MODEL
TERMINOLOGY
THE
OPTICAL
CENTER
PINHOLE
IS
PLACED
AT
THE
ORIGIN
OF
THE
WORLD
COORDINATE
SYSTEM
U
V
W
AND
THE
IMAGE
PLANE
WHERE
THE
VIRTUAL
IMAGE
IS
FORMED
IS
DISPLACED
ALONG
THE
W
AXIS
WHICH
IS
ALSO
KNOWN
AS
THE
OPTICAL
AXIS
THE
POSITION
WHERE
THE
OPTICAL
AXIS
STRIKES
THE
IMAGE
PLANE
IS
CALLED
THE
PRINCIPAL
POINT
THE
DISTANCE
BETWEEN
THE
IMAGE
PLANE
AND
THE
OPTICAL
CENTER
IS
CALLED
THE
FOCAL
LENGTH
IS
ESSENTIALLY
DETERMINISTIC
WE
WILL
NONETHELESS
BUILD
A
PROBABILITY
MODEL
THERE
IS
NOISE
IN
THE
SENSOR
AND
UNMODELED
FACTORS
IN
THE
FEATURE
DETECTION
PROCESS
CAN
ALSO
AFFECT
THE
MEASURED
IMAGE
POSITION
HOWEVER
FOR
PEDAGOGICAL
REASONS
WE
WILL
DEFER
A
DISCUSSION
OF
THIS
UNCERTAINTY
UNTIL
LATER
AND
TEMPORARILY
TREAT
THE
IMAGING
PROCESS
AS
IF
IT
WERE
DETERMINISTIC
OUR
TASK
THEN
IS
TO
ESTABLISH
THE
POSITION
X
X
Y
T
WHERE
THE
POINT
W
U
V
W
T
IS
IMAGED
CONSIDERING
FIGURE
IT
IS
CLEAR
HOW
TO
DO
THIS
WE
CONNECT
A
RAY
BETWEEN
W
AND
THE
OPTICAL
CENTER
THE
IMAGE
POSITION
X
CAN
BE
FOUND
BY
OBSERVING
WHERE
THIS
RAY
STRIKES
THE
IMAGE
PLANE
THIS
PROCESS
IS
CALLED
PERSPECTIVE
PROJECTION
IN
THE
NEXT
FEW
SECTIONS
WE
WILL
BUILD
A
MORE
PRECISE
MATHEMATICAL
MODEL
OF
THIS
PROCESS
WE
WILL
START
WITH
A
VERY
SIMPLE
CAMERA
MODEL
THE
NORMALIZED
CAMERA
AND
BUILD
UP
TO
A
FULL
CAMERA
PARAMETERIZATION
THE
NORMALIZED
CAMERA
IN
THE
NORMALIZED
CAMERA
THE
FOCAL
LENGTH
IS
ONE
AND
IT
IS
ASSUMED
THAT
THE
ORIGIN
OF
THE
COORDINATE
SYSTEM
X
Y
ON
THE
IMAGE
PLANE
IS
CENTERED
AT
THE
PRINCIPAL
POINT
FIGURE
SHOWS
A
SLICE
OF
THE
GEOMETRY
OF
THIS
SYSTEM
THE
U
AND
X
AXES
NOW
POINT
UPWARD
OUT
OF
THE
PAGE
AND
CANNOT
BE
SEEN
BY
SIMILAR
TRIANGLES
IT
CAN
EASILY
BE
SEEN
THAT
THE
Y
POSITION
IN
THE
IMAGE
OF
THE
WORLD
POINT
FIGURE
NORMALIZED
CAMERA
THE
FOCAL
LENGTH
IS
ONE
AND
THE
IM
AGE
COORDINATE
SYSTEM
X
Y
IS
CEN
TERED
ON
THE
PRINCIPAL
POINT
ONLY
Y
AXIS
SHOWN
BY
SIMILAR
TRIANGLES
THE
Y
POSITION
IN
THE
IMAGE
OF
A
POINT
AT
U
V
W
IS
GIVEN
BY
V
W
THIS
COR
RESPONDS
TO
OUR
INTUITION
AS
AN
OB
JECT
GETS
MORE
DISTANT
ITS
PROJECTION
BECOMES
CLOSER
TO
THE
CENTER
OF
THE
IMAGE
AT
W
U
V
W
T
IS
GIVEN
BY
V
W
MORE
GENERALLY
IN
THE
NORMALIZED
CAMERA
A
POINT
W
U
V
W
T
IS
PROJECTED
INTO
THE
IMAGE
AT
X
X
Y
T
USING
THE
RELATIONS
U
X
W
V
Y
W
WHERE
X
Y
U
V
AND
W
ARE
MEASURED
IN
THE
SAME
REAL
WORLD
UNITS
E
G
MM
FOCAL
LENGTH
PARAMETERS
THE
NORMALIZED
CAMERA
IS
UNREALISTIC
FOR
ONE
THING
IN
A
REAL
CAMERA
THERE
IS
NO
PARTICULAR
REASON
WHY
THE
FOCAL
LENGTH
SHOULD
BE
ONE
MOREOVER
THE
FINAL
POSITION
IN
THE
IMAGE
IS
MEASURED
IN
PIXELS
NOT
PHYSICAL
DISTANCE
SO
THE
MODEL
MUST
TAKE
INTO
ACCOUNT
THE
PHOTORECEPTOR
SPACING
BOTH
OF
THESE
FACTORS
HAVE
THE
EFFECT
OF
CHANGING
THE
MAPPING
BETWEEN
POINTS
W
U
V
W
T
IN
THE
WORLD
AND
THEIR
POSITIONS
X
X
Y
T
IN
THE
IMAGE
PLANE
BY
A
CONSTANT
SCALING
FACTOR
Φ
FIGURE
SO
THAT
ΦU
X
W
ΦV
Y
W
TO
ADD
A
FURTHER
COMPLICATION
THE
SPACING
OF
THE
PHOTORECEPTORS
MAY
DIFFER
IN
THE
X
AND
Y
DIRECTIONS
AND
SO
THE
SCALING
MAY
BE
DIFFERENT
IN
EACH
DIRECTION
GIVING
THE
RELATIONS
X
ΦXU
W
Y
ΦY
V
W
WHERE
ΦX
AND
ΦY
ARE
SEPARATE
SCALING
FACTORS
FOR
THE
X
AND
Y
DIRECTIONS
THESE
PARAMETERS
ARE
KNOWN
AS
THE
FOCAL
LENGTH
PARAMETERS
IN
THE
X
AND
Y
DIRECTIONS
THE
PINHOLE
CAMERA
FIGURE
FOCAL
LENGTH
AND
PHOTORECEPTOR
SPACING
A
B
CHANGING
THE
DISTANCE
BETWEEN
THE
OPTICAL
CENTER
AND
THE
IMAGE
PLANE
THE
FOCAL
LENGTH
CHANGES
THE
RELATIONSHIP
BETWEEN
THE
WORLD
POINT
W
U
V
W
T
AND
THE
IMAGE
POINT
X
X
Y
T
IN
PARTICULAR
IF
WE
TAKE
THE
ORIGINAL
FOCAL
LENGTH
A
AND
HALVE
IT
B
THE
IMAGE
COORDINATE
IS
ALSO
HALVED
THE
FIELD
OF
VIEW
OF
THE
CAMERA
IS
THE
TOTAL
ANGULAR
RANGE
THAT
IS
IMAGED
USUALLY
DIFFERENT
IN
THE
X
AND
Y
DIRECTIONS
WHEN
THE
FOCAL
LENGTH
DECREASES
THE
FIELD
OF
VIEW
INCREASES
C
D
THE
POSITION
IN
THE
IMAGE
X
X
Y
T
IS
USUALLY
MEASURED
IN
PIXELS
HENCE
THE
POSITION
X
DEPENDS
ON
THE
DENSITY
OF
THE
RECEPTORS
ON
THE
IMAGE
PLANE
IF
WE
TAKE
THE
ORIGINAL
PHOTORECEPTOR
DENSITY
C
AND
HALVE
IT
D
THEN
THE
IMAGE
COORDINATE
IS
ALSO
HALVED
HENCE
THE
PHOTORECEPTOR
SPACING
AND
FOCAL
LENGTH
BOTH
CHANGE
THE
MAPPING
FROM
RAYS
TO
PIXELS
IN
THE
SAME
WAY
BUT
THIS
NAME
IS
SOMEWHAT
MISLEADING
THEY
ACCOUNT
FOR
NOT
JUST
THE
DISTANCE
BETWEEN
THE
OPTICAL
CENTER
AND
THE
PRINCIPAL
POINT
THE
TRUE
FOCAL
LENGTH
BUT
ALSO
THE
PHOTORECEPTOR
SPACING
OFFSET
AND
SKEW
PARAMETERS
THE
MODEL
SO
FAR
IS
STILL
INCOMPLETE
IN
THAT
PIXEL
POSITION
X
T
IS
AT
THE
PRINCIPAL
POINT
WHERE
THE
W
AXIS
INTERSECTS
THE
IMAGE
PLANE
IN
MOST
IMAGING
SYSTEMS
THE
PIXEL
POSITION
X
T
IS
AT
THE
TOP
LEFT
OF
THE
IMAGE
RATHER
THAN
THE
CENTER
TO
COPE
WITH
THIS
WE
ADD
OFFSET
PARAMETERS
ΔX
AND
ΔY
SO
THAT
X
ΦXU
Δ
W
X
Y
ΦY
V
Δ
W
Y
WHERE
ΔX
AND
ΔY
ARE
THE
OFFSETS
IN
PIXELS
FROM
THE
TOP
LEFT
CORNER
OF
THE
IMAGE
TO
THE
POSITION
WHERE
THE
W
AXIS
STRIKES
THE
IMAGE
PLANE
ANOTHER
WAY
TO
THINK
ABOUT
THIS
IS
THAT
THE
VECTOR
ΔX
ΔY
T
IS
THE
POSITION
OF
THE
PRINCIPAL
POINT
IN
PIXELS
IF
THE
IMAGE
PLANE
IS
EXACTLY
CENTERED
ON
THE
W
AXIS
THESE
OFFSET
PARAMETERS
SHOULD
BE
HALF
THE
IMAGE
SIZE
FOR
A
VGA
IMAGE
ΔX
AND
ΔY
WOULD
BE
AND
RESPECTIVELY
HOWEVER
IN
PRACTICE
IT
IS
DIFFICULT
AND
SUPERFLUOUS
TO
MANUFACTURE
CAMERAS
WITH
THE
IMAGING
SENSOR
PERFECTLY
CENTERED
AND
SO
WE
TREAT
THE
OFFSET
PARAMETERS
AS
VARIABLE
QUANTITIES
WE
ALSO
INTRODUCE
A
SKEW
TERM
Γ
WHICH
MODERATES
THE
PROJECTED
POSITION
X
AS
A
FUNCTION
OF
THE
HEIGHT
V
IN
THE
WORLD
THIS
PARAMETER
HAS
NO
CLEAR
PHYSICAL
INTERPRETATION
BUT
CAN
HELP
EXPLAIN
THE
PROJECTION
OF
POINTS
INTO
THE
IMAGE
IN
PRACTICE
THE
RESULTING
CAMERA
MODEL
IS
X
ΦXU
ΓV
Δ
W
X
Y
ΦY
V
Δ
W
Y
POSITION
AND
ORIENTATION
OF
CAMERA
FINALLY
WE
MUST
ACCOUNT
FOR
THE
FACT
THAT
THE
CAMERA
IS
NOT
ALWAYS
CONVENIENTLY
CENTERED
AT
THE
ORIGIN
OF
THE
WORLD
COORDINATE
SYSTEM
WITH
THE
OPTICAL
AXIS
EXACTLY
ALIGNED
WITH
THE
W
AXIS
IN
GENERAL
WE
MAY
WANT
TO
DEFINE
AN
ARBITRARY
WORLD
COORDINATE
SYSTEM
THAT
MAY
BE
COMMON
TO
MORE
THAN
ONE
CAMERA
TO
THIS
END
WE
EXPRESS
THE
WORLD
POINTS
W
IN
THE
COORDINATE
SYSTEM
OF
THE
CAMERA
BEFORE
THEY
ARE
PASSED
THROUGH
THE
PROJECTION
MODEL
USING
THE
COORDINATE
TRANSFORMATION
UI
U
ΤX
WI
W
ΤZ
OR
WI
ΩW
Τ
WHERE
WI
IS
THE
TRANSFORMED
POINT
Ω
IS
A
ROTATION
MATRIX
AND
Τ
IS
A
TRANSLATION
VECTOR
FULL
PINHOLE
CAMERA
MODEL
PROBLEM
PROBLEM
WE
ARE
NOW
IN
A
POSITION
TO
DESCRIBE
THE
FULL
CAMERA
MODEL
BY
COMBINING
EQUATIONS
AND
A
POINT
W
U
V
W
T
IS
PROJECTED
TO
A
POINT
X
X
Y
T
BY
THE
RELATIONS
X
ΦX
ΤX
Γ
ΤY
Δ
ΤZ
X
Y
ΦY
ΤY
Δ
ΤZ
Y
THERE
ARE
TWO
SETS
OF
PARAMETERS
IN
THIS
MODEL
THE
INTRINSIC
OR
CAMERA
PARAMETERS
ΦX
ΦY
Γ
ΔX
ΔY
DESCRIBE
THE
CAMERA
ITSELF
AND
THE
EXTRINSIC
PARAMETERS
Ω
Τ
DESCRIBE
THE
POSITION
AND
ORIENTATION
OF
THE
CAMERA
IN
THE
WORLD
FOR
REASONS
THAT
WILL
BECOME
CLEAR
IN
SECTION
WE
WILL
STORE
THE
INTRINSIC
PARAMETERS
IN
THE
INTRINSIC
MATRIX
Λ
WHERE
ΦX
Γ
ΔX
Λ
ΦY
ΔY
WE
CAN
NOW
ABBREVIATE
THE
FULL
PROJECTION
MODEL
EQUATIONS
BY
JUST
WRITING
X
PINHOLE
W
Λ
Ω
Τ
FINALLY
WE
MUST
ACCOUNT
FOR
THE
FACT
THAT
THE
ESTIMATED
POSITION
OF
A
FEATURE
IN
THE
IMAGE
MAY
DIFFER
FROM
OUR
PREDICTIONS
THERE
ARE
A
NUMBER
OF
REASONS
FOR
THIS
INCLUDING
NOISE
IN
THE
SENSOR
SAMPLING
ISSUES
AND
THE
FACT
THAT
THE
DETECTED
POSITION
IN
THE
IMAGE
MAY
CHANGE
AT
DIFFERENT
VIEWPOINTS
WE
MODEL
THESE
FACTORS
WITH
ADDITIVE
NOISE
THAT
IS
NORMALLY
DISTRIBUTED
WITH
A
SPHERICAL
COVARIANCE
TO
GIVE
THE
FINAL
RELATION
P
R
X
W
Λ
Ω
Τ
NORMX
PINHOLE
W
Λ
Ω
Τ
WHERE
IS
THE
VARIANCE
OF
THE
NOISE
NOTE
THAT
THE
PINHOLE
CAMERA
IS
A
GENERATIVE
MODEL
WE
ARE
DESCRIBING
THE
LIKELIHOOD
P
R
X
W
Λ
Ω
Τ
OF
OBSERVING
A
IMAGE
POINT
X
GIVEN
A
WORLD
POINT
W
AND
THE
PARAMETERS
Λ
Ω
Τ
RADIAL
DISTORTION
IN
THE
PREVIOUS
SECTION
WE
INTRODUCED
THE
PINHOLE
CAMERA
MODEL
HOWEVER
IT
HAS
PROBABLY
NOT
ESCAPED
YOUR
ATTENTION
THAT
REAL
WORLD
CAMERAS
ARE
RARELY
BASED
ON
THE
PINHOLE
THEY
HAVE
A
LENS
OR
POSSIBLY
A
SYSTEM
OF
SEVERAL
LENSES
THAT
COLLECTS
LIGHT
FROM
A
LARGER
AREA
AND
RE
FOCUSES
IT
ON
THE
IMAGE
PLANE
IN
PRACTICE
THIS
LEADS
TO
A
NUMBER
OF
DEVIATIONS
FROM
THE
PINHOLE
MODEL
FOR
EXAMPLE
SOME
PARTS
OF
THE
IMAGE
MAY
BE
OUT
OF
FOCUS
WHICH
ESSENTIALLY
MEANS
THAT
THE
ASSUMPTION
THAT
A
POINT
IN
THE
WORLD
W
MAPS
TO
A
SINGLE
POINT
IN
THE
IMAGE
X
IS
NO
LONGER
VALID
THERE
ARE
MORE
COMPLEX
MATHEMATICAL
MODELS
FOR
CAMERAS
THAT
DEAL
EFFECTIVELY
WITH
THIS
SITUATION
BUT
THEY
ARE
NOT
DISCUSSED
HERE
FIGURE
RADIAL
DISTORTION
THE
PINHOLE
MODEL
IS
ONLY
AN
APPROXIMATION
OF
THE
TRUE
IMAGING
PROCESS
ONE
IMPORTANT
DEVIATION
FROM
THIS
MODEL
IS
A
WARPING
IN
WHICH
POINTS
DEVIATE
FROM
THEIR
EXPECTED
POSITIONS
BY
MOVING
ALONG
RADIAL
LINES
FROM
THE
CENTER
OF
THE
IMAGE
BY
AN
AMOUNT
THAT
DEPENDS
ON
THE
DISTANCE
FROM
THE
CENTER
THIS
IS
KNOWN
AS
RADIAL
DISTORTION
A
AN
IMAGE
THAT
SUFFERS
FROM
RADIAL
DISTORTION
IS
EASILY
SPOTTED
BECAUSE
LINES
THAT
WERE
STRAIGHT
IN
THE
WORLD
ARE
MAPPED
TO
CURVES
IN
THE
IMAGE
E
G
RED
DOTTED
LINE
B
AFTER
APPLYING
THE
INVERSE
RADIAL
DISTORTION
MODEL
STRAIGHT
LINES
IN
THE
WORLD
NOW
CORRECTLY
MAP
TO
STRAIGHT
LINES
IN
THE
IMAGE
THE
DISTORTION
CAUSED
THE
MAGENTA
POINT
TO
MOVE
ALONG
THE
RED
RADIAL
LINE
TO
THE
POSITION
OF
THE
YELLOW
POINT
HOWEVER
THERE
IS
ONE
DEVIATION
FROM
THE
PINHOLE
MODEL
THAT
MUST
BE
ADDRESSED
RADIAL
DISTORTION
IS
A
NONLINEAR
WARPING
OF
THE
IMAGE
THAT
DEPENDS
ON
THE
DISTANCE
FROM
THE
CENTER
OF
THE
IMAGE
IN
PRACTICE
THIS
OCCURS
WHEN
THE
FIELD
OF
VIEW
OF
THE
LENS
SYSTEM
IS
LARGE
IT
CAN
EASILY
BE
DETECTED
IN
AN
IMAGE
BECAUSE
STRAIGHT
LINES
IN
THE
WORLD
NO
LONGER
PROJECT
TO
STRAIGHT
LINES
IN
THE
IMAGE
FIGURE
RADIAL
DISTORTION
IS
COMMONLY
MODELED
AS
A
POLYNOMIAL
FUNCTION
OF
THE
DISTANCE
R
FROM
THE
CENTER
OF
THE
IMAGE
IN
THE
NORMALIZED
CAMERA
THE
FINAL
IMAGE
POSITIONS
XI
YI
ARE
EXPRESSED
AS
FUNCTIONS
OF
THE
ORIGINAL
POSITIONS
X
Y
BY
XI
X
YI
Y
WHERE
THE
PARAMETERS
AND
CONTROL
THE
DEGREE
OF
DISTORTION
THESE
RELATIONS
DESCRIBE
A
FAMILY
OF
POSSIBLE
DISTORTIONS
THAT
APPROXIMATE
THE
TRUE
DISTORTION
CLOSELY
FOR
MOST
COMMON
LENSES
THIS
DISTORTION
IS
IMPLEMENTED
AFTER
PERSPECTIVE
PROJECTION
DIVISION
BY
W
BUT
BEFORE
THE
EFFECT
OF
THE
INTRINSIC
PARAMETERS
FOCAL
LENGTH
OFFSET
ETC
SO
THE
WARP
ING
IS
RELATIVE
TO
THE
OPTICAL
AXIS
AND
NOT
THE
ORIGIN
OF
THE
PIXEL
COORDINATE
SYSTEM
WE
WILL
NOT
DISCUSS
RADIAL
DISTORTION
FURTHER
IN
THIS
VOLUME
HOWEVER
IT
IS
IMPOR
TANT
TO
REALIZE
THAT
FOR
ACCURATE
RESULTS
ALL
OF
THE
ALGORITHMS
IN
THIS
AND
CHAPTERS
AND
SHOULD
ACCOUNT
FOR
RADIAL
DISTORTION
WHEN
THE
FIELD
OF
VIEW
IS
LARGE
IT
IS
PARTICULARLY
CRITICAL
TO
INCORPORATE
THIS
INTO
THE
PINHOLE
CAMERA
MODEL
THREE
GEOMETRIC
PROBLEMS
FIGURE
PROBLEM
LEARNING
EXTRINSIC
PARAMETERS
EXTERIOR
ORIENTATION
PROBLEM
GIVEN
POINTS
WI
I
ON
A
KNOWN
OBJECT
BLUE
LINES
THEIR
PO
SITIONS
XI
I
IN
THE
IMAGE
CIRCLES
ON
IMAGE
PLANE
AND
KNOWN
INTRINSIC
PARAMETERS
Λ
FIND
THE
ROTATION
Ω
AND
TRANSLATION
Τ
RELATING
THE
CAMERA
AND
THE
OBJECT
A
WHEN
THE
ROTATION
OR
TRANSLATION
ARE
WRONG
THE
IMAGE
POINTS
PREDICTED
BY
THE
MODEL
WHERE
THE
RAYS
STRIKE
THE
IMAGE
PLANE
DO
NOT
AGREE
WELL
WITH
THE
OBSERVED
POINTS
XI
B
WHEN
THE
ROTATION
AND
TRANSLATION
ARE
CORRECT
THEY
AGREE
WELL
AND
THE
LIKELIHOOD
P
R
XI
W
Λ
Ω
Τ
WILL
BE
HIGH
THREE
GEOMETRIC
PROBLEMS
NOW
THAT
WE
HAVE
DESCRIBED
THE
PINHOLE
CAMERA
MODEL
WE
WILL
CONSIDER
THREE
IMPORTANT
GEOMETRIC
PROBLEMS
EACH
IS
AN
INSTANCE
OF
LEARNING
OR
INFERENCE
WITHIN
THIS
MODEL
WE
WILL
FIRST
DESCRIBE
THE
PROBLEMS
THEMSELVES
AND
THEN
TACKLE
THEM
ONE
BY
ONE
LATER
IN
THE
CHAPTER
PROBLEM
LEARNING
EXTRINSIC
PARAMETERS
WE
AIM
TO
RECOVER
THE
POSITION
AND
ORIENTATION
OF
THE
CAMERA
RELATIVE
TO
A
KNOWN
SCENE
THIS
IS
SOMETIMES
KNOWN
AS
THE
PERSPECTIVE
N
POINT
PNP
PROBLEM
OR
THE
EXTERIOR
ORIENTATION
PROBLEM
ONE
COMMON
APPLICATION
IS
AUGMENTED
REALITY
WHERE
WE
NEED
TO
KNOW
THIS
RELATIONSHIP
TO
RENDER
VIRTUAL
OBJECTS
THAT
APPEAR
TO
BE
STABLE
PARTS
OF
THE
REAL
SCENE
THE
PROBLEM
CAN
BE
STATED
MORE
FORMALLY
AS
FOLLOWS
WE
ARE
GIVEN
A
KNOWN
OBJECT
WITH
I
DISTINCT
POINTS
WI
I
THEIR
CORRESPONDING
PROJECTIONS
IN
THE
IMAGE
XI
I
AND
KNOWN
INTRINSIC
PARAMETERS
Λ
OUR
GOAL
IS
TO
ESTIMATE
THE
ROTATION
Ω
AND
TRANSLATION
Τ
THAT
MAP
POINTS
IN
THE
COORDINATE
SYSTEM
OF
THE
OBJECT
TO
POINTS
IN
THE
COORDINATE
SYSTEM
OF
THE
CAMERA
SO
THAT
Ωˆ
Τˆ
ARGMAX
Ω
Τ
I
I
LOG
P
R
XI
WI
Λ
Ω
Τ
THIS
IS
A
MAXIMUM
LIKELIHOOD
LEARNING
PROBLEM
IN
WHICH
WE
AIM
TO
FIND
PA
RAMETERS
Ω
Τ
THAT
MAKE
THE
PREDICTIONS
PINHOLE
WI
Λ
Ω
Τ
OF
THE
MODEL
AGREE
WITH
THE
OBSERVED
POINTS
XI
FIGURE
THE
PINHOLE
CAMERA
FIGURE
PROBLEM
LEARNING
INTRINSIC
PARAMETERS
GIVEN
A
SET
OF
POINTS
WI
I
ON
A
KNOWN
OBJECT
IN
THE
WORLD
BLUE
LINES
AND
THE
POSITIONS
X
I
OF
THESE
POINTS
IN
AN
IMAGE
FIND
THE
INTRINSIC
PARAMETERS
Λ
TO
DO
THIS
WE
MUST
ALSO
SIMULTANEOUSLY
ESTIMATE
THE
EXTRINSIC
PARAMETERS
Ω
Τ
A
WHEN
THE
INTRINSIC
OR
EXTRINSIC
PARAMETERS
ARE
WRONG
THE
PREDICTION
OF
THE
PINHOLE
CAMERA
WHERE
RAYS
STRIKE
THE
IMAGE
PLANE
WILL
DEVIATE
SIGNIFICANTLY
FROM
THE
OBSERVED
POINTS
B
WHEN
THE
INTRINSIC
AND
EXTRINSIC
PARAMETERS
ARE
CORRECT
THE
PREDICTION
OF
THE
MODEL
WILL
AGREE
WITH
THE
OBSERVED
IMAGE
PROBLEM
LEARNING
INTRINSIC
PARAMETERS
WE
AIM
TO
ESTIMATE
THE
INTRINSIC
PARAMETERS
Λ
THAT
RELATE
THE
DIRECTION
OF
RAYS
THROUGH
THE
OPTICAL
CENTER
TO
COORDINATES
ON
THE
IMAGE
PLANE
THIS
ESTIMATION
PROCESS
IS
KNOWN
AS
CALIBRATION
KNOWLEDGE
OF
THE
INTRINSIC
PARAMETERS
IS
CRITICAL
IF
WE
WANT
TO
USE
THE
CAMERA
TO
BUILD
MODELS
OF
THE
WORLD
THE
CALIBRATION
PROBLEM
CAN
BE
STATED
MORE
FORMALLY
AS
FOLLOWS
GIVEN
A
KNOWN
OBJECT
WITH
I
DISTINCT
POINTS
WI
I
AND
THEIR
CORRESPONDING
PROJECTIONS
IN
THE
IMAGE
XI
I
ESTIMATE
THE
INTRINSIC
PARAMETERS
Λˆ
ARGMAX
Λ
MAX
Ω
Τ
I
I
LOG
P
R
XI
WI
Λ
Ω
Τ
ONCE
MORE
THIS
IS
A
MAXIMUM
LIKELIHOOD
LEARNING
PROBLEM
IN
WHICH
WE
AIM
TO
FIND
PARAMETERS
Λ
Ω
Τ
THAT
MAKE
THE
PREDICTIONS
OF
THE
MODEL
PINHOLE
WI
Λ
Ω
Τ
AGREE
WITH
THE
OBSERVED
POINTS
XI
FIGURE
WE
DO
NOT
PARTICULARLY
CARE
ABOUT
THE
EXTRINSIC
PARAMETERS
Ω
Τ
FINDING
THESE
IS
JUST
A
MEANS
TO
THE
END
OF
ESTIMATING
THE
INTRINSIC
PARAMETERS
Λ
THE
CALIBRATION
PROCESS
REQUIRES
A
KNOWN
OBJECT
ON
WHICH
DISTINCT
POINTS
CAN
BE
IDENTIFIED
AND
THEIR
PROJECTIONS
IN
THE
IMAGE
FOUND
A
COMMON
APPROACH
IS
TO
CONSTRUCT
A
BESPOKE
CALIBRATION
THAT
ACHIEVES
THESE
GOALS
FIGURE
SHOULD
BE
NOTED
THAT
IN
PRACTICE
CALIBRATION
IS
MORE
USUALLY
BASED
ON
A
NUMBER
OF
VIEWS
OF
A
KNOWN
PLANAR
OBJECT
SEE
SECTION
FIGURE
CAMERA
CALIBRATION
TAR
GET
ONE
WAY
TO
CALIBRATE
THE
CAM
ERA
ESTIMATE
ITS
INTRINSIC
PARAMETERS
IS
TO
VIEW
A
OBJECT
A
CAMERA
CAL
IBRATION
TARGET
FOR
WHICH
THE
GEOME
TRY
IS
KNOWN
THE
MARKS
ON
THE
SUR
FACE
ARE
AT
KNOWN
POSITIONS
IN
THE
FRAME
OF
REFERENCE
OF
THE
OBJECT
AND
ARE
EASY
TO
LOCATE
IN
THE
IMAGE
US
ING
BASIC
IMAGE
PROCESSING
TECHNIQUES
IT
IS
NOW
POSSIBLE
TO
FIND
THE
INTRIN
SIC
AND
EXTRINSIC
PARAMETERS
THAT
OPTI
MALLY
MAP
THE
KNOWN
POSITIONS
TO
THEIR
PROJECTIONS
IN
THE
IMAGE
IM
AGE
FROM
HARTLEY
ZISSERMAN
FIGURE
PROBLEM
INFERRING
WORLD
POINTS
GIVEN
TWO
CAMERAS
WITH
KNOWN
POSITION
AND
ORIENTATION
AND
THE
PROJECTIONS
AND
OF
THE
SAME
POINT
IN
EACH
IMAGE
THE
GOAL
OF
CALIBRATED
STEREO
RECONSTRUCTION
IS
TO
INFER
THE
POSITION
W
OF
THE
WORLD
POINT
A
WHEN
THE
ESTIMATE
OF
THE
WORLD
POINT
RED
CIRCLE
IS
WRONG
THE
PREDICTIONS
OF
THE
PINHOLE
CAMERA
MODELS
WHERE
RAYS
STRIKE
THE
IMAGE
PLANE
WILL
DEVIATE
FROM
THE
OBSERVED
DATA
BROWN
CIRCLES
ON
IMAGE
PLANE
B
WHEN
THE
ESTIMATE
OF
W
IS
CORRECT
THE
PREDICTIONS
OF
THE
MODEL
AGREE
WITH
THE
OBSERVED
DATA
PROBLEM
INFERRING
WORLD
POINTS
WE
AIM
TO
ESTIMATE
THE
POSITION
OF
A
POINT
W
IN
THE
SCENE
GIVEN
ITS
PROJECTIONS
XJ
J
IN
J
CALIBRATED
CAMERAS
WHEN
J
THIS
IS
KNOWN
AS
CALIBRATED
STEREO
RECONSTRUCTION
WITH
J
CALIBRATED
CAMERAS
IT
IS
KNOWN
TO
AS
MULTI
VIEW
RECONSTRUCTION
IF
WE
REPEAT
THIS
PROCESS
FOR
MANY
POINTS
THE
RESULT
IS
A
SPARSE
POINT
CLOUD
THIS
COULD
BE
USED
TO
HELP
AN
AUTONOMOUS
VEHICLE
NAVIGATE
THROUGH
THE
ENVIRONMENT
OR
TO
GENERATE
AN
IMAGE
OF
THE
SCENE
FROM
A
NEW
VIEWPOINT
MORE
FORMALLY
THE
MULTI
VIEW
RECONSTRUCTION
PROBLEM
CAN
BE
STATED
AS
FOL
LOWS
GIVEN
J
CALIBRATED
CAMERAS
IN
KNOWN
POSITIONS
I
E
CAMERAS
WITH
KNOWN
Λ
Ω
Τ
VIEWING
THE
SAME
POINT
W
AND
KNOWING
THE
CORRESPONDING
PROJEC
TIONS
XJ
J
IN
THE
J
IMAGES
ESTABLISH
THE
POSITION
W
OF
THE
POINT
IN
THE
FIGURE
GEOMETRIC
INTERPRETA
TION
OF
HOMOGENEOUS
COORDINATES
THE
DIFFERENT
SCALAR
MULTIPLES
Λ
OF
THE
HO
MOGENEOUS
VECTOR
X
DEFINE
A
RAY
THROUGH
THE
ORIGIN
OF
A
COORDINATE
SPACE
THE
CORRESPONDING
IMAGE
POINT
X
CAN
BE
FOUND
BY
CONSIDERING
THE
POINT
THAT
THIS
RAY
STRIKES
ON
THE
PLANE
AT
Z
AN
INTEREST
ING
SIDE
EFFECT
OF
THIS
REPRESENTATION
IS
THAT
IT
IS
POSSIBLE
TO
REPRESENT
POINTS
AT
INFINITY
KNOWN
AS
IDEAL
POINTS
FOR
EXAMPLE
THE
HOMOGENEOUS
COORDINATE
T
DEFINES
A
RAY
THAT
IS
PARALLEL
TO
Z
AND
SO
NEVER
INTERSECTS
THE
PLANE
IT
REPRESENTS
THE
POINT
AT
INFIN
ITY
IN
DIRECTION
T
WORLD
J
Wˆ
ARGMAX
LOG
P
R
XJ
W
ΛJ
ΩJ
Τ
J
THE
FORM
OF
THIS
INFERENCE
PROBLEM
IS
SIMILAR
TO
THAT
OF
THE
PRECEDING
LEARNING
PROBLEMS
WE
PERFORM
AN
OPTIMIZATION
IN
WHICH
WE
MANIPULATE
THE
VARIABLE
OF
IN
TEREST
W
UNTIL
THE
PREDICTIONS
PINHOLE
W
ΛJ
ΩJ
Τ
J
OF
THE
PINHOLE
CAMERA
MODELS
AGREE
WITH
THE
DATA
XJ
FIGURE
FOR
OBVIOUS
REASONS
THE
PRINCIPLE
BEHIND
RECONSTRUCTION
IS
KNOWN
AS
TRIANGULATION
SOLVING
THE
PROBLEMS
WE
HAVE
INTRODUCED
THREE
GEOMETRIC
PROBLEMS
EACH
OF
WHICH
TOOK
THE
FORM
OF
A
LEARNING
OR
INFERENCE
PROBLEM
USING
THE
PINHOLE
CAMERA
MODEL
WE
FORMULATED
EACH
IN
TERMS
OF
MAXIMUM
LIKELIHOOD
ESTIMATION
AND
IN
EACH
CASE
THIS
RESULTS
IN
AN
OPTIMIZATION
PROBLEM
UNFORTUNATELY
NONE
OF
THE
RESULTING
OBJECTIVE
FUNCTIONS
CAN
BE
OPTIMIZED
IN
CLOSED
FORM
EACH
SOLUTION
REQUIRES
THE
USE
OF
NONLINEAR
OPTIMIZATION
IN
EACH
CASE
IT
IS
CRITICAL
TO
HAVE
A
GOOD
INITIAL
ESTIMATE
OF
THE
UNKNOWN
QUANTITIES
TO
ENSURE
THAT
THE
OPTIMIZATION
PROCESS
CONVERGES
TO
THE
GLOBAL
MAXIMUM
IN
THE
REMAINING
PART
OF
THIS
CHAPTER
WE
DEVELOP
ALGORITHMS
THAT
PROVIDE
THESE
INITIAL
ESTIMATES
THE
GENERAL
APPROACH
IS
TO
CHOOSE
NEW
OBJECTIVE
FUNCTIONS
THAT
CAN
BE
OPTIMIZED
IN
CLOSED
FORM
AND
WHERE
THE
SOLUTION
IS
CLOSE
TO
THE
SOLUTION
OF
THE
TRUE
PROBLEM
HOMOGENEOUS
COORDINATES
HOMOGENEOUS
COORDINATES
TO
GET
GOOD
INITIAL
ESTIMATES
OF
THE
GEOMETRIC
QUANTITIES
IN
THE
PRECEDING
OPTI
MIZATION
PROBLEMS
WE
PLAY
A
SIMPLE
TRICK
WE
CHANGE
THE
REPRESENTATION
OF
BOTH
THE
IMAGE
POINTS
AND
WORLD
POINTS
SO
THAT
THE
PROJECTION
EQUATIONS
BECOME
LINEAR
AFTER
THIS
CHANGE
IT
IS
POSSIBLE
TO
FIND
SOLUTIONS
FOR
THE
UNKNOWN
QUANTI
TIES
IN
CLOSED
FORM
HOWEVER
IT
SHOULD
BE
EMPHASIZED
THAT
THESE
SOLUTIONS
DO
NOT
DIRECTLY
ADDRESS
THE
ORIGINAL
OPTIMIZATION
CRITERIA
THEY
MINIMIZE
MORE
ABSTRACT
OBJECTIVE
FUNCTIONS
BASED
ON
ALGEBRAIC
ERROR
WHOSE
SOLUTIONS
ARE
NOT
GUARANTEED
TO
BE
THE
SAME
AS
THOSE
FOR
THE
ORIGINAL
PROBLEM
HOWEVER
THEY
ARE
GENERALLY
CLOSE
ENOUGH
TO
PROVIDE
A
GOOD
STARTING
POINT
FOR
A
NONLINEAR
OPTIMIZATION
OF
THE
TRUE
COST
FUNCTION
WE
CONVERT
THE
ORIGINAL
CARTESIAN
REPRESENTATION
OF
THE
IMAGE
POINTS
X
TO
A
HOMOGENEOUS
COORDINATE
X
SO
THAT
X
WHERE
Λ
IS
AN
ARBITRARY
SCALING
FACTOR
THIS
IS
A
REDUNDANT
REPRESENTATION
IN
THAT
ANY
SCALAR
MULTIPLE
Λ
REPRESENTS
THE
SAME
POINT
FOR
EXAMPLE
THE
HOMOGENEOUS
PROBLEM
PROBLEM
PROBLEM
VECTORS
X
T
AND
X
T
BOTH
REPRESENT
THE
CARTESIAN
POINT
X
T
WHERE
SCALING
FACTORS
Λ
AND
Λ
HAVE
BEEN
USED
RESPECTIVELY
CONVERTING
BETWEEN
HOMOGENEOUS
AND
CARTESIAN
COORDINATES
IS
EASY
TO
MOVE
TO
HOMOGENEOUS
COORDINATES
WE
CHOOSE
Λ
AND
SIMPLY
APPEND
A
TO
THE
ORIGINAL
CARTESIAN
COORDINATE
TO
RECOVER
THE
CARTESIAN
COORDINATES
WE
DIVIDE
THE
FIRST
TWO
ENTRIES
OF
THE
HOMOGENEOUS
VECTOR
BY
THE
THIRD
SO
THAT
IF
WE
OBSERVE
THE
HOMOGENEOUS
VECTOR
X
X
Y
Z
T
THEN
WE
CAN
RECOVER
THE
CARTESIAN
COORDINATE
X
X
Y
T
AS
PART
I
IMAGE
FORMATION
AND
IMAGE
MODELS
THERE
ARE
MANY
TYPES
OF
IMAGING
DEVICES
FROM
ANIMAL
EYES
TO
VIDEO
CAMERAS
AND
RADIO
TELE
SCOPES
THEY
MAY
OR
MAY
NOT
BE
EQUIPPED
WITH
LENSES
FOR
EXAMPLE
THE
FIRST
MODELS
OF
THE
CAMERA
OBSCURA
LITERALLY
DARK
CHAMBER
INVENTED
IN
THE
CENTURY
DID
NOT
HAVE
LENSES
BUT
INSTEAD
USED
A
PINHOLE
TO
FOCUS
LIGHT
RAYS
ONTO
A
WALL
OR
TRANSLUCENT
PLATE
AND
DEMONSTRATE
THE
LAWS
OF
PERSPECTIVE
DISCOVERED
A
CENTURY
EARLIER
BY
BRUNELLESCHI
PINHOLES
WERE
REPLACED
BY
MORE
AND
MORE
SOPHISTICATED
LENSES
AS
EARLY
AS
AND
THE
MODERN
PHOTOGRAPHIC
OR
DIGITAL
CAMERA
IS
ESSENTIALLY
A
CAMERA
OBSCURA
CAPABLE
OF
RECORDING
THE
AMOUNT
OF
LIGHT
STRIKING
EVERY
SMALL
AREA
OF
ITS
BACKPLANE
FIGURE
FIGURE
IMAGE
FORMATION
ON
THE
BACKPLATE
OF
A
PHOTOGRAPHIC
CAMERA
FIGURE
FROM
US
NAVY
MANUAL
OF
BASIC
OPTICS
AND
OPTICAL
INSTRUMENTS
PREPARED
BY
THE
BUREAU
OF
NAVAL
PERSONNEL
REPRINTED
BY
DOVER
PUBLICATIONS
INC
CAMERAS
CHAP
THE
IMAGING
SURFACE
OF
A
CAMERA
IS
GENERALLY
A
RECTANGLE
BUT
THE
SHAPE
OF
THE
HUMAN
RETINA
IS
MUCH
CLOSER
TO
A
SPHERICAL
SURFACE
AND
PANORAMIC
CAMERAS
MAY
BE
EQUIPPED
WITH
CYLINDRICAL
RETINAS
IMAGING
SENSORS
HAVE
OTHER
CHARACTERISTICS
THEY
MAY
RECORD
A
SPATIALLY
DISCRETE
PICTURE
LIKE
OUR
EYES
WITH
THEIR
RODS
AND
CONES
MM
CAMERAS
WITH
THEIR
GRAIN
AND
DIGITAL
CAMERAS
WITH
THEIR
RECTANGULAR
PICTURE
ELEMENTS
OR
PIXELS
OR
A
CONTINUOUS
ONE
IN
THE
CASE
OF
OLD
FASHIONED
TV
TUBES
FOR
EXAMPLE
THE
SIGNAL
THAT
AN
IMAGING
SENSOR
RECORDS
AT
A
POINT
ON
ITS
RETINA
MAY
BE
DISCRETE
OR
CONTINUOUS
AND
IT
MAY
CONSIST
OF
A
SINGLE
NUMBER
BLACK
AND
WHITE
CAMERA
A
FEW
VALUES
E
G
THE
R
G
B
INTENSITIES
FOR
A
COLOR
CAMERA
OR
THE
RESPONSES
OF
THE
THREE
TYPES
OF
CONES
FOR
THE
HUMAN
EYE
MANY
NUMBERS
E
G
THE
RESPONSES
OF
HYPERSPECTRAL
SENSORS
OR
EVEN
A
CONTINUOUS
FUNCTION
OF
WAVELENGTH
WHICH
IS
ESSENTIALLY
THE
CASE
FOR
SPECTROMETERS
EXAMINING
THESE
CHARACTERISTICS
IS
THE
SUBJECT
OF
THIS
CHAPTER
PINHOLE
CAMERAS
PERSPECTIVE
PROJECTION
IMAGINE
TAKING
A
BOX
PRICKING
A
SMALL
HOLE
IN
ONE
OF
ITS
SIDES
WITH
A
PIN
AND
THEN
REPLACING
THE
OPPOSITE
SIDE
WITH
A
TRANSLUCENT
PLATE
IF
YOU
HOLD
THAT
BOX
IN
FRONT
OF
YOU
IN
A
DIMLY
LIT
ROOM
WITH
THE
PINHOLE
FACING
SOME
LIGHT
SOURCE
SAY
A
CANDLE
YOU
SEE
AN
INVERTED
IMAGE
OF
THE
CANDLE
APPEARING
ON
THE
TRANSLUCENT
PLATE
FIGURE
THIS
IMAGE
IS
FORMED
BY
LIGHT
RAYS
ISSUED
FROM
THE
SCENE
FACING
THE
BOX
IF
THE
PINHOLE
WERE
REALLY
REDUCED
TO
A
POINT
WHICH
IS
OF
COURSE
PHYSICALLY
IMPOSSIBLE
EXACTLY
ONE
LIGHT
RAY
WOULD
PASS
THROUGH
EACH
POINT
IN
THE
PLANE
OF
THE
PLATE
OR
IMAGE
PLANE
THE
PINHOLE
AND
SOME
SCENE
POINT
IN
REALITY
THE
PINHOLE
HAS
A
FINITE
ALBEIT
SMALL
SIZE
AND
EACH
POINT
IN
THE
IMAGE
PLANE
COLLECTS
LIGHT
FROM
A
CONE
OF
RAYS
SUBTENDING
A
FINITE
SOLID
ANGLE
SO
THIS
IDEALIZED
AND
EXTREMELY
SIMPLE
MODEL
OF
THE
IMAGING
GEOMETRY
DOES
NOT
STRICTLY
APPLY
IN
ADDITION
REAL
CAMERAS
ARE
NORMALLY
EQUIPPED
WITH
LENSES
WHICH
FURTHER
COMPLICATES
THINGS
STILL
THE
PINHOLE
PERSPECTIVE
ALSO
CALLED
CENTRAL
PERSPECTIVE
PROJECTION
MODEL
FIRST
PROPOSED
BY
BRUNELLESCHI
AT
THE
BEGINNING
OF
THE
CENTURY
IS
MATHEMATICALLY
CONVENIENT
DESPITE
ITS
SIMPLICITY
IT
OFTEN
PROVIDES
AN
ACCEPTABLE
APPROXIMATION
OF
THE
IMAGING
PROCESS
PERSPECTIVE
PROJECTION
CREATES
INVERTED
IMAGES
AND
IT
IS
SOMETIMES
CONVENIENT
TO
CONSIDER
INSTEAD
A
VIRTUAL
IMAGE
ASSOCIATED
WITH
A
PLANE
LYING
IN
FRONT
OF
THE
PINHOLE
AT
THE
SAME
DISTANCE
FROM
IT
AS
THE
ACTUAL
IMAGE
PLANE
FIGURE
THIS
VIRTUAL
IMAGE
IS
NOT
INVERTED
BUT
IS
OTHERWISE
STRICTLY
EQUIVALENT
TO
THE
ACTUAL
ONE
DEPENDING
ON
THE
CONTEXT
IT
MAY
BE
MORE
CONVENIENT
TO
THINK
ABOUT
ONE
OR
THE
OTHER
FIGURE
A
ILLUSTRATES
AN
OBVIOUS
EFFECT
OF
PERSPECTIVE
PROJECTION
THE
APPARENT
SIZE
OF
OBJECTS
DEPENDS
ON
THEIR
DISTANCE
FOR
EXAMPLE
THE
IMAGES
AND
C
OF
THE
POSTS
B
AND
C
HAVE
THE
SAME
HEIGHT
BUT
A
AND
C
IMAGE
PLANE
FIGURE
THE
PINHOLE
IMAGING
MODEL
SEC
PINHOLE
CAMERAS
A
B
FIGURE
PERSPECTIVE
EFFECTS
A
FAR
OBJECTS
APPEAR
SMALLER
THAN
CLOSE
ONES
THE
DISTANCE
D
FROM
THE
PINHOLE
O
TO
THE
PLANE
CONTAINING
C
IS
HALF
THE
DISTANCE
FROM
O
TO
THE
PLANE
CONTAINING
A
AND
B
B
THE
IMAGES
OF
PARALLEL
LINES
INTERSECT
AT
THE
HORIZON
AFTER
HILBERT
AND
COHN
VOSSEN
FIGURE
NOTE
THAT
THE
IMAGE
PLANE
IS
BEHIND
THE
PINHOLE
IN
A
PHYSICAL
RETINA
AND
IN
FRONT
OF
IT
IN
B
VIRTUAL
IMAGE
PLANE
MOST
OF
THE
DIAGRAMS
IN
THIS
CHAPTER
AND
THE
REST
OF
THIS
BOOK
FEATURE
THE
PHYSICAL
IMAGE
PLANE
BUT
A
VIRTUAL
ONE
IS
ALSO
USED
WHEN
APPROPRIATE
AS
IN
B
ARE
REALLY
HALF
THE
SIZE
OF
B
FIGURE
B
ILLUSTRATES
ANOTHER
WELL
KNOWN
EFFECT
THE
PROJECTIONS
OF
TWO
PARALLEL
LINES
LYING
IN
SOME
PLANE
IT
APPEAR
TO
CONVERGE
ON
A
HORIZON
LINE
H
FORMED
BY
THE
INTERSECTION
OF
THE
IMAGE
PLANE
WITH
THE
PLANE
PARALLEL
TO
IT
AND
PASSING
THROUGH
THE
PINHOLE
NOTE
THAT
THE
LINE
L
IN
IT
THAT
IS
PARALLEL
TO
THE
IMAGE
PLANE
HAS
NO
IMAGE
AT
ALL
THESE
PROPERTIES
ARE
EASY
TO
PROVE
IN
A
PURELY
GEOMETRIC
FASHION
HOWEVER
IT
IS
OFTEN
CONVENIENT
IF
NOT
QUITE
AS
ELEGANT
TO
REASON
IN
TERMS
OF
REFERENCE
FRAMES
COORDINATES
AND
EQUATIONS
CONSIDER
FOR
EXAMPLE
A
COORDINATE
SYSTEM
O
I
J
K
ATTACHED
TO
A
PINHOLE
CAMERA
WHOSE
ORIGIN
O
COINCIDES
WITH
THE
PINHOLE
AND
VECTORS
I
AND
J
FORM
A
BASIS
FOR
A
VECTOR
PLANE
PARALLEL
TO
THE
IMAGE
PLANE
WHICH
IS
LOCATED
AT
A
POSITIVE
DISTANCE
F
FROM
THE
PINHOLE
ALONG
THE
VECTOR
K
FIGURE
THE
LINE
PERPENDICULAR
TO
AND
PASSING
THROUGH
THE
PINHOLE
IS
CALLED
THE
OPTICAL
AXIS
AND
THE
POINT
C
WHERE
IT
PIERCES
IS
CALLED
THE
IMAGE
CENTER
THIS
POINT
CAN
BE
USED
AS
THE
ORIGIN
OF
AN
IMAGE
PLANE
COORDINATE
FRAME
AND
IT
PLAYS
AN
IMPORTANT
ROLE
IN
CAMERA
CALIBRATION
PROCEDURES
LET
P
DENOTE
A
SCENE
POINT
WITH
COORDINATES
X
Y
Z
AND
DENOTE
ITS
IMAGE
WITH
COORDI
NATES
X
SINCE
LIES
IN
THE
IMAGE
PLANE
WE
HAVE
F
SINCE
THE
THREE
POINTS
P
O
CAMERAS
CHAP
FIGURE
THE
PERSPECTIVE
PROJECTION
EQUATIONS
ARE
DERIVED
IN
THIS
SECTION
FROM
THE
COLLINEARITY
OF
THE
POINT
P
ITS
IMAGE
AND
THE
PINHOLE
O
AND
ARE
COLLINEAR
WE
HAVE
O
P
Λ
O
P
FOR
SOME
NUMBER
Λ
SO
AND
THEREFORE
X
ΛX
F
ΛZ
X
Λ
X
X
F
Y
Z
AFFINE
PROJECTION
X
F
Z
F
Y
Z
AS
NOTED
IN
THE
PREVIOUS
SECTION
PINHOLE
PERSPECTIVE
IS
ONLY
AN
APPROXIMATION
OF
THE
GEOMETRY
OF
THE
IMAGING
PROCESS
THIS
SECTION
DISCUSSES
A
CLASS
OF
COARSER
APPROXIMATIONS
CALLED
AFFINE
PROJECTION
MODELS
THAT
ARE
ALSO
USEFUL
ON
OCCASION
WE
FOCUS
ON
TWO
SPECIFIC
AFFINE
MODELS
NAMELY
WEAK
PERSPECTIVE
AND
ORTHOGRAPHIC
PROJECTIONS
A
THIRD
ONE
THE
PARAPERSPECTIVE
MODEL
IS
INTRODUCED
IN
CHAPTER
WHERE
THE
NAME
AFFINE
PROJECTION
IS
ALSO
JUSTIFIED
CONSIDER
THE
FRONTO
PARALLEL
PLANE
DEFINED
BY
Z
FIGURE
FOR
ANY
POINT
P
IN
WE
CAN
REWRITE
THE
PERSPECTIVE
PROJECTION
EQ
AS
X
MX
F
WHERE
M
PHYSICAL
CONSTRAINTS
IMPOSE
THAT
BE
NEGATIVE
THE
PLANE
MUST
BE
IN
FRONT
OF
THE
PINHOLE
SO
THE
MAGNIFICATION
M
ASSOCIATED
WITH
THE
PLANE
IS
POSITIVE
THIS
NAME
IS
JUSTIFIED
BY
THE
FOLLOWING
REMARK
CONSIDER
TWO
POINTS
P
AND
Q
IN
AND
THEIR
IMAGES
AND
FIGURE
OBVIOUSLY
THE
VECTORS
P
Q
AND
P
ARE
PARALLEL
AND
WE
HAVE
P
M
P
Q
THIS
IS
THE
DEPENDENCE
OF
IMAGE
SIZE
ON
OBJECT
DISTANCE
NOTED
EARLIER
WHEN
THE
SCENE
DEPTH
IS
SMALL
RELATIVE
TO
THE
AVERAGE
DISTANCE
FROM
THE
CAMERA
THE
MAG
NIFICATION
CAN
BE
TAKEN
TO
BE
CONSTANT
THIS
PROJECTION
MODEL
IS
CALLED
WEAK
PERSPECTIVE
OR
SCALED
SEC
CAMERAS
WITH
LENSES
FIGURE
WEAK
PERSPECTIVE
PROJECTION
ALL
LINE
SEGMENTS
IN
THE
PLANE
ARE
PROJECTED
WITH
THE
SAME
MAGNIFICATION
FIGURE
ORTHOGRAPHIC
PROJECTION
UNLIKE
OTHER
GEOMETRIC
MODELS
OF
THE
IMAGE
FORMATION
PROCESS
ORTHOGRAPHIC
PROJECTION
DOES
NOT
INVOLVE
A
REVERSAL
OF
IMAGE
FEATURES
ACCORDINGLY
THE
MAGNIFICATION
IS
TAKEN
TO
BE
NEGATIVE
WHICH
IS
A
BIT
UNNATURAL
BUT
SIMPLIFIES
THE
PROJECTION
EQUATIONS
ORTHOGRAPHY
WHEN
IT
IS
A
PRIORI
KNOWN
THAT
THE
CAMERA
ALWAYS
REMAINS
AT
A
ROUGHLY
CONSTANT
DISTANCE
FROM
THE
SCENE
WE
CAN
GO
FURTHER
AND
NORMALIZE
THE
IMAGE
COORDINATES
SO
THAT
M
THIS
IS
ORTHOGRAPHIC
PROJECTION
DEFINED
BY
X
X
Y
WITH
ALL
LIGHT
RAYS
PARALLEL
TO
THE
K
AXIS
AND
ORTHOGONAL
TO
THE
IMAGE
PLANE
FIGURE
ALTHOUGH
WEAK
PERSPECTIVE
PROJECTION
IS
AN
ACCEPTABLE
MODEL
FOR
MANY
IMAGING
CONDITIONS
AS
SUMING
PURE
ORTHOGRAPHIC
PROJECTION
IS
USUALLY
UNREALISTIC
CAMERAS
WITH
LENSES
MOST
CAMERAS
ARE
EQUIPPED
WITH
LENSES
THERE
ARE
TWO
MAIN
REASONS
FOR
THIS
THE
FIRST
ONE
IS
TO
GATHER
LIGHT
SINCE
A
SINGLE
RAY
OF
LIGHT
WOULD
OTHERWISE
REACH
EACH
POINT
IN
THE
IMAGE
PLANE
UNDER
IDEAL
PINHOLE
PROJECTION
REAL
PINHOLES
HAVE
A
FINITE
SIZE
OF
COURSE
SO
EACH
POINT
IN
THE
IMAGE
CAMERAS
CHAP
FIGURE
REFLECTION
AND
REFRACTION
AT
THE
INTERFACE
BETWEEN
TWO
HOMOGE
NEOUS
MEDIA
WITH
INDEXES
OF
REFRACTION
AND
PLANE
IS
ILLUMINATED
BY
A
CONE
OF
LIGHT
RAYS
SUBTENDING
A
FINITE
SOLID
ANGLE
THE
LARGER
THE
HOLE
THE
WIDER
THE
CONE
AND
THE
BRIGHTER
THE
IMAGE
BUT
A
LARGE
PINHOLE
GIVES
BLURRY
PICTURES
SHRINKING
THE
PINHOLE
PRODUCES
SHARPER
IMAGES
BUT
REDUCES
THE
AMOUNT
OF
LIGHT
REACHING
THE
IMAGE
PLANE
AND
MAY
INTRODUCE
DIFFRACTION
EFFECTS
THE
SECOND
MAIN
REASON
FOR
USING
A
LENS
IS
TO
KEEP
THE
PICTURE
IN
SHARP
FOCUS
WHILE
GATHERING
LIGHT
FROM
A
LARGE
AREA
IGNORING
DIFFRACTION
INTERFERENCES
AND
OTHER
PHYSICAL
OPTICS
PHENOMENA
THE
BEHAVIOR
OF
LENSES
IS
DICTATED
BY
THE
LAWS
OF
GEOMETRIC
OPTICS
FIGURE
LIGHT
TRAVELS
IN
STRAIGHT
LINES
LIGHT
RAYS
IN
HOMOGENEOUS
MEDIA
WHEN
A
RAY
IS
REFLECTED
FROM
A
SURFACE
THIS
RAY
ITS
REFLEC
TION
AND
THE
SURFACE
NORMAL
ARE
COPLANAR
AND
THE
ANGLES
BETWEEN
THE
NORMAL
AND
THE
TWO
RAYS
ARE
COMPLEMENTARY
AND
WHEN
A
RAY
PASSES
FROM
ONE
MEDIUM
TO
ANOTHER
IT
IS
REFRACTED
I
E
ITS
DIRECTION
CHANGES
ACCORDING
TO
SNELL
LAW
IF
IS
THE
RAY
INCIDENT
TO
THE
INTERFACE
BETWEEN
TWO
TRANSPARENT
MATERIALS
WITH
INDEXES
OF
REFRACTION
AND
AND
IS
THE
REFRACTED
RAY
THEN
AND
THE
NORMAL
TO
THE
INTERFACE
ARE
COPLANAR
AND
THE
ANGLES
AND
BETWEEN
THE
NORMAL
AND
THE
TWO
RAYS
ARE
RELATED
BY
SIN
SIN
IN
THIS
CHAPTER
WE
ONLY
CONSIDER
THE
EFFECTS
OF
REFRACTION
AND
IGNORE
THOSE
OF
REFLECTION
IN
OTHER
WORDS
WE
CONCENTRATE
ON
LENSES
AS
OPPOSED
TO
CATADIOPTRIC
OPTICAL
SYSTEMS
E
G
TELE
SCOPES
THAT
MAY
INCLUDE
BOTH
REFLECTIVE
MIRRORS
AND
REFRACTIVE
ELEMENTS
TRACING
LIGHT
RAYS
AS
THEY
TRAVEL
THROUGH
A
LENS
IS
SIMPLER
WHEN
THE
ANGLES
BETWEEN
THESE
RAYS
AND
THE
REFRACTING
SURFACES
OF
THE
LENS
ARE
ASSUMED
TO
BE
SMALL
THE
NEXT
SECTION
DISCUSSES
THIS
CASE
PARAXIAL
GEOMETRIC
OPTICS
IN
THIS
SECTION
WE
CONSIDER
PARAXIAL
OR
FIRST
ORDER
GEOMETRIC
OPTICS
WHERE
THE
ANGLES
BETWEEN
ALL
LIGHT
RAYS
GOING
THROUGH
A
LENS
AND
THE
NORMAL
TO
THE
REFRACTIVE
SURFACES
OF
THE
LENS
ARE
SMALL
IN
ADDITION
WE
ASSUME
THAT
THE
LENS
IS
ROTATIONALLY
SYMMETRIC
ABOUT
A
STRAIGHT
LINE
CALLED
ITS
OPTICAL
AXIS
AND
THAT
ALL
REFRACTIVE
SURFACES
ARE
SPHERICAL
THE
SYMMETRY
OF
THIS
SETUP
ALLOWS
US
TO
DETERMINE
THE
PROJECTION
GEOMETRY
BY
CONSIDERING
LENSES
WITH
CIRCULAR
BOUNDARIES
LYING
IN
A
PLANE
THAT
CONTAINS
THE
OPTICAL
AXIS
LET
US
CONSIDER
AN
INCIDENT
LIGHT
RAY
PASSING
THROUGH
A
POINT
ON
THE
OPTICAL
AXIS
AND
REFRACTED
AT
THE
POINT
P
OF
THE
CIRCULAR
INTERFACE
OF
RADIUS
R
SEPARATING
TWO
TRANSPARENT
MEDIA
WITH
INDEXES
OF
REFRACTION
AND
FIGURE
LET
US
ALSO
DENOTE
BY
THE
POINT
WHERE
THE
REFRACTED
RAY
INTERSECTS
THE
OPTICAL
AXIS
A
SECOND
TIME
THE
ROLES
OF
AND
ARE
COMPLETELY
SYMMETRIC
AND
BY
C
THE
CENTER
OF
THE
CIRCULAR
INTERFACE
SEC
CAMERAS
WITH
LENSES
FIGURE
PARAXIAL
REFRACTION
A
LIGHT
RAY
PASSING
THROUGH
THE
POINT
IS
RE
FRACTED
AT
THE
POINT
P
WHERE
IT
INTERSECTS
A
CIRCULAR
INTERFACE
THE
REFRACTED
RAY
INTERSECTS
THE
OPTICAL
AXIS
IN
THE
CENTER
OF
THE
INTERFACE
IS
AT
THE
POINT
C
OF
THE
OPTICAL
AXIS
AND
ITS
RADIUS
IS
R
THE
ANGLES
AND
ARE
ALL
ASSUMED
TO
BE
SMALL
LET
AND
RESPECTIVELY
DENOTE
THE
ANGLES
BETWEEN
THE
TWO
RAYS
AND
THE
CHORD
JOINING
C
TO
P
IF
RESP
IS
THE
ANGLE
BETWEEN
THE
OPTICAL
AXIS
AND
THE
LINE
JOINING
RESP
TO
P
THE
ANGLE
BETWEEN
THE
OPTICAL
AXIS
AND
THE
LINE
JOINING
C
TO
P
IS
AS
SHOWN
BY
FIGURE
Γ
NOW
LET
H
DENOTE
THE
DISTANCE
BETWEEN
P
AND
THE
OPTICAL
AXIS
AND
R
THE
RADIUS
OF
THE
CIRCULAR
INTERFACE
IF
WE
ASSUME
ALL
ANGLES
ARE
SMALL
AND
THUS
TO
FIRST
ORDER
EQUAL
TO
THEIR
SINES
AND
TANGENTS
WE
HAVE
Α
Γ
Β
H
I
AND
Α
Γ
Β
H
I
WRITING
SNELL
LAW
FOR
SMALL
ANGLES
YIELDS
THE
PARAXIAL
REFRACTION
EQUATION
N
Α
N
Α
R
NOTE
THAT
THE
RELATIONSHIP
BETWEEN
AND
DEPENDS
ON
R
AND
BUT
NOT
ON
OR
THIS
IS
THE
MAIN
SIMPLIFICATION
INTRODUCED
BY
THE
PARAXIAL
ASSUMPTION
IT
IS
EASY
TO
SEE
THAT
EQ
REMAINS
VALID
WHEN
SOME
OR
ALL
OF
THE
VALUES
OF
AND
R
BECOME
NEGATIVE
CORRESPONDING
TO
THE
POINTS
OR
C
SWITCHING
SIDES
OF
COURSE
REAL
LENSES
ARE
BOUNDED
BY
AT
LEAST
TWO
REFRACTIVE
SURFACES
THE
CORRESPONDING
RAY
PATHS
CAN
BE
CONSTRUCTED
ITERATIVELY
USING
THE
PARAXIAL
REFRACTION
EQUATION
THE
NEXT
SECTION
ILLUSTRATES
THIS
IDEA
IN
THE
CASE
OF
THIN
LENSES
THIN
LENSES
LET
US
NOW
CONSIDER
A
LENS
WITH
TWO
SPHERICAL
SURFACES
OF
RADIUS
R
AND
INDEX
OF
REFRACTION
N
WE
ASSUME
THAT
THIS
LENS
IS
SURROUNDED
BY
VACUUM
OR
TO
AN
EXCELLENT
APPROXIMATION
BY
AIR
WITH
AN
INDEX
OF
REFRACTION
EQUAL
TO
AND
THAT
IT
IS
THIN
I
E
THAT
A
RAY
ENTERING
THE
LENS
AND
REFRACTED
AT
ITS
RIGHT
BOUNDARY
IS
IMMEDIATELY
REFRACTED
AGAIN
AT
THE
LEFT
BOUNDARY
CONSIDER
A
POINT
P
LOCATED
AT
NEGATIVE
DEPTH
Z
OFF
THE
OPTICAL
AXIS
AND
DENOTE
BY
PO
THE
RAY
PASSING
THROUGH
THIS
POINT
AND
THE
CENTER
O
OF
THE
LENS
FIGURE
AS
SHOWN
IN
THE
EXERCISES
IT
FOLLOWS
FROM
SNELL
LAW
AND
EQ
THAT
THE
RAY
PO
IS
NOT
REFRACTED
AND
THAT
ALL
OTHER
RAYS
PASSING
THROUGH
P
ARE
FOCUSED
BY
THE
THIN
LENS
ON
THE
POINT
WITH
DEPTH
ALONG
CAMERAS
CHAP
FIGURE
A
THIN
LENS
RAYS
PASSING
THROUGH
THE
POINT
O
ARE
NOT
REFRACTED
RAYS
PARALLEL
TO
THE
OPTICAL
AXIS
ARE
FOCUSED
ON
THE
FOCAL
POINT
F
PO
SUCH
THAT
WHERE
F
R
N
Z
IS
THE
FOCAL
LENGTH
OF
THE
LENS
F
PINHOLE
PERSPECTIVE
PROJECTION
IF
WE
TAKE
F
SINCE
P
AND
LIE
ON
A
RAY
PASSING
THROUGH
THE
CENTER
OF
THE
LENS
BUT
THAT
POINTS
LOCATED
AT
A
DISTANCE
Z
FROM
O
ARE
ONLY
IN
SHARP
FOCUS
WHEN
THE
IMAGE
PLANE
IS
LOCATED
AT
A
DISTANCE
FROM
O
ON
THE
OTHER
SIDE
OF
THE
LENS
THAT
SATISFIES
EQ
I
E
THE
THIN
LENS
EQUATION
LETTING
Z
SHOWS
THAT
F
IS
THE
DISTANCE
BETWEEN
THE
CENTER
OF
THE
LENS
AND
THE
PLANE
WHERE
OBJECTS
SUCH
AS
STARS
WHICH
ARE
EFFECTIVELY
LOCATED
AT
Z
FOCUS
THE
TWO
POINTS
F
AND
F
LOCATED
AT
DISTANCE
F
FROM
THE
LENS
CENTER
ON
THE
OPTICAL
AXIS
ARE
CALLED
THE
FOCAL
POINTS
OF
THE
LENS
IN
PRACTICE
OBJECTS
WITHIN
SOME
RANGE
OF
DISTANCES
CALLED
DEPTH
OF
FIELD
OR
DEPTH
OF
FOCUS
ARE
IN
ACCEPTABLE
FOCUS
AS
SHOWN
IN
THE
EXERCISES
THE
DEPTH
OF
FIELD
INCREASES
WITH
THE
F
NUMBER
OF
THE
LENS
I
E
THE
RATIO
BETWEEN
THE
FOCAL
LENGTH
OF
THE
LENS
AND
ITS
DIAMETER
THE
FIELD
OF
VIEW
OF
A
CAMERA
IS
THE
PORTION
OF
SCENE
SPACE
THAT
ACTUALLY
PROJECTS
ONTO
THE
RETINA
OF
THE
CAMERA
IT
IS
NOT
DEFINED
BY
THE
FOCAL
LENGTH
ALONE
BUT
ALSO
DEPENDS
ON
THE
EFFECTIVE
AREA
OF
THE
RETINA
E
G
THE
AREA
OF
FILM
THAT
CAN
BE
EXPOSED
IN
A
PHOTOGRAPHIC
CAMERA
OR
THE
AREA
OF
THE
CCD
SENSOR
IN
A
DIGITAL
CAMERA
FIGURE
FIGURE
THE
FIELD
OF
VIEW
OF
A
CAMERA
IS
WHERE
Φ
DEF
ARCTAN
D
D
IS
THE
DIAMETER
OF
THE
SENSOR
FILM
OR
CCD
CHIP
AND
F
IS
THE
FOCAL
LENGTH
OF
THE
CAMERA
WHEN
F
IS
MUCH
SHORTER
THAN
D
WE
HAVE
A
WIDE
ANGLE
LENS
WITH
RAYS
THAT
CAN
BE
OFF
THE
OPTICAL
AXIS
BY
MORE
THAN
TELEPHOTO
LENSES
HAVE
A
SMALL
FIELD
OF
VIEW
AND
PRODUCE
PICTURES
CLOSER
TO
AFFINE
ONES
SEC
CAMERAS
WITH
LENSES
Y
P
FIGURE
A
SIMPLE
THICK
LENS
WITH
TWO
SPHERICAL
SURFACES
REAL
LENSES
A
MORE
REALISTIC
MODEL
OF
SIMPLE
OPTICAL
SYSTEMS
IS
THE
THICK
LENS
THE
EQUATIONS
DESCRIBING
ITS
BEHAVIOR
ARE
EASILY
DERIVED
FROM
THE
PARAXIAL
REFRACTION
EQUATION
AND
THEY
ARE
THE
SAME
AS
THE
PINHOLE
PERSPECTIVE
AND
THIN
LENS
PROJECTION
EQUATIONS
EXCEPT
FOR
AN
OFFSET
FIGURE
IF
H
AND
H
DENOTE
THE
PRINCIPAL
POINTS
OF
THE
LENS
THEN
EQ
HOLDS
WHEN
Z
RESP
IS
THE
DISTANCE
BETWEEN
P
RESP
AND
THE
PLANE
PERPENDICULAR
TO
THE
OPTICAL
AXIS
AND
PASSING
THROUGH
H
RESP
H
IN
THIS
CASE
THE
ONLY
UNDEFLECTED
RAY
IS
ALONG
THE
OPTICAL
AXIS
SIMPLE
LENSES
SUFFER
FROM
A
NUMBER
OF
ABERRATIONS
TO
UNDERSTAND
WHY
LET
US
REMEMBER
FIRST
THAT
THE
PARAXIAL
REFRACTION
EQ
IS
ONLY
AN
APPROXIMATION
VALID
WHEN
THE
ANGLE
Α
BETWEEN
EACH
RAY
ALONG
THE
OPTICAL
PATH
AND
THE
OPTICAL
AXIS
OF
THE
LENGTH
IS
SMALL
AND
SIN
Α
Α
FOR
LARGER
ANGLES
A
THIRD
ORDER
TAYLOR
EXPANSION
OF
THE
SINE
FUNCTION
YIELDS
THE
FOLLOWING
REFINEMENT
OF
THE
PARAXIAL
EQUATION
I
I
R
R
R
HERE
H
DENOTES
AS
IN
FIGURE
THE
DISTANCE
BETWEEN
THE
OPTICAL
AXIS
AND
THE
POINT
WHERE
THE
INCIDENT
RAY
INTERSECTS
THE
INTERFACE
IN
PARTICULAR
RAYS
STRIKING
THE
INTERFACE
FARTHER
FROM
THE
OPTICAL
AXIS
ARE
FOCUSED
CLOSER
TO
THE
INTERFACE
THE
SAME
PHENOMENON
OCCURS
FOR
A
LENS
AND
IT
IS
THE
SOURCE
OF
TWO
TYPES
OF
SPHERICAL
ABERRATIONS
FIGURE
A
CONSIDER
A
POINT
P
ON
THE
OPTICAL
AXIS
AND
ITS
PARAXIAL
IMAGE
THE
DISTANCE
BETWEEN
AND
THE
INTERSECTION
OF
THE
OPTICAL
AXIS
WITH
A
RAY
ISSUED
FROM
P
AND
REFRACTED
BY
THE
LENS
IS
CALLED
THE
LONGITUDINAL
SPHERICAL
ABERRATION
OF
THAT
RAY
NOTE
THAT
IF
AN
IMAGE
PLANE
WERE
ERECTED
IN
P
THE
RAY
WOULD
INTERSECT
THIS
PLANE
AT
SOME
DISTANCE
FROM
THE
AXIS
CALLED
THE
TRANSVERSE
SPHERICAL
ABERRATION
OF
THAT
RAY
TOGETHER
ALL
RAYS
PASSING
THROUGH
P
AND
REFRACTED
BY
THE
LENS
FORM
A
CIRCLE
OF
CONFUSION
CENTERED
IN
P
AS
THEY
INTERSECT
THE
SIZE
OF
THAT
CIRCLE
CHANGES
WHEN
WE
MOVE
ALONG
THE
OPTICAL
AXIS
THE
CIRCLE
WITH
MINIMUM
DIAMETER
IS
CALLED
THE
CIRCLE
OF
LEAST
CONFUSION
AND
IT
IS
NOT
IN
GENERAL
LOCATED
IN
BESIDES
SPHERICAL
ABERRATION
THERE
ARE
FOUR
OTHER
TYPES
OF
PRIMARY
ABERRATIONS
CAUSED
BY
THE
DIFFERENCES
BETWEEN
FIRST
AND
THIRD
ORDER
OPTICS
NAMELY
COMA
ASTIGMATISM
FIELD
CURVATURE
AND
DISTORTION
A
PRECISE
DEFINITION
OF
THESE
ABERRATIONS
IS
BEYOND
THE
SCOPE
OF
THIS
BOOK
SUFFICE
TO
SAY
THAT
LIKE
SPHERICAL
ABERRATION
THEY
DEGRADE
THE
IMAGE
BY
BLURRING
THE
PICTURE
OF
EVERY
OBJECT
POINT
DISTORTION
PLAYS
A
DIFFERENT
ROLE
AND
CHANGES
THE
SHAPE
OF
THE
IMAGE
AS
A
WHOLE
FIGURE
B
THIS
EFFECT
IS
DUE
TO
THE
FACT
THAT
DIFFERENT
AREAS
OF
A
LENS
HAVE
SLIGHTLY
DIFFERENT
FOCAL
LENGTHS
THE
ABERRATIONS
MENTIONED
SO
FAR
ARE
MONOCHROMATIC
I
E
THEY
ARE
INDEPENDENT
OF
THE
RESPONSE
OF
THE
LENS
TO
VARIOUS
WAVELENGTHS
HOWEVER
THE
INDEX
OF
REFRACTION
OF
A
TRANSPARENT
CAMERAS
CHAP
A
B
C
FIGURE
ABERRATIONS
A
SPHERICAL
ABERRATION
THE
GREY
REGION
IS
THE
PARAX
IAL
ZONE
WHERE
THE
RAYS
ISSUED
FROM
P
INTERSECT
AT
ITS
PARAXIAL
IMAGE
IF
AN
IMAGE
PLANE
IS
ERECTED
IN
THE
IMAGE
OF
IN
THAT
PLANE
FORMS
A
CIRCLE
OF
CONFUSION
OF
DIAMETER
THE
FOCUS
PLANE
YIELDING
THE
CIRCLE
OF
LEAST
CONFUSION
IS
INDICATED
BY
A
DASHED
LINE
B
DISTORTION
FROM
LEFT
TO
RIGHT
THE
NOMINAL
IM
AGE
OF
A
FRONTO
PARALLEL
SQUARE
PINCUSHION
DISTORTION
AND
BARREL
DISTORTION
C
CHROMATIC
ABERRATION
THE
INDEX
OF
REFRACTION
OF
A
TRANSPARENT
MEDIUM
DEPENDS
ON
THE
WAVELENGTH
OR
COLOR
OF
THE
INCIDENT
LIGHT
RAYS
HERE
A
PRISM
DECOMPOSES
WHITE
LIGHT
INTO
A
PALETTE
OF
COLORS
FIGURE
FROM
US
NAVY
MANUAL
OF
BASIC
OPTICS
AND
OPTICAL
INSTRUMENTS
PREPARED
BY
THE
BUREAU
OF
NAVAL
PER
SONNEL
REPRINTED
BY
DOVER
PUBLICATIONS
INC
MEDIUM
DEPENDS
ON
WAVELENGTH
FIGURE
C
AND
IT
FOLLOWS
FROM
THE
THIN
LENS
EQ
THAT
THE
FOCAL
LENGTH
DEPENDS
ON
WAVELENGTH
AS
WELL
THIS
CAUSES
THE
PHENOMENON
OF
CHROMATIC
ABERRATION
REFRACTED
RAYS
CORRESPONDING
TO
DIFFERENT
WAVELENGTHS
INTERSECT
THE
OPTICAL
AXIS
AT
DIFFERENT
POINTS
LONGITUDINAL
CHROMATIC
ABERRATION
AND
FORM
DIFFERENT
CIRCLES
OF
CONFUSION
IN
THE
SAME
IMAGE
PLANE
TRANSVERSE
CHROMATIC
ABERRATION
ABERRATIONS
CAN
BE
MINIMIZED
BY
ALIGNING
SEVERAL
SIMPLE
LENSES
WITH
WELL
CHOSEN
SHAPES
AND
REFRACTION
INDEXES
SEPARATED
BY
APPROPRIATE
STOPS
THESE
COMPOUND
LENSES
CAN
STILL
BE
MOD
ELED
BY
THE
THICK
LENS
EQUATIONS
THEY
SUFFER
FROM
ONE
MORE
DEFECT
RELEVANT
TO
MACHINE
VISION
LIGHT
BEAMS
EMANATING
FROM
OBJECT
POINTS
LOCATED
OFF
AXIS
ARE
PARTIALLY
BLOCKED
BY
THE
VARIOUS
APERTURES
INCLUDING
THE
INDIVIDUAL
LENS
COMPONENTS
POSITIONED
INSIDE
THE
LENS
TO
LIMIT
ABERRA
TIONS
FIGURE
THIS
PHENOMENON
CALLED
VIGNETTING
CAUSES
THE
BRIGHTNESS
TO
DROP
IN
THE
IMAGE
PERIPHERY
VIGNETTING
MAY
POSE
PROBLEMS
TO
AUTOMATED
IMAGE
ANALYSIS
PROGRAMS
BUT
IT
IS
NOT
AS
IMPORTANT
IN
PHOTOGRAPHY
THANKS
TO
THE
HUMAN
EYE
REMARKABLE
INSENSITIVITY
TO
SMOOTH
SEC
THE
HUMAN
EYE
FIGURE
VIGNETTING
EFFECT
IN
A
TWO
LENS
SYSTEM
THE
SHADED
PART
OF
THE
BEAM
NEVER
REACHES
THE
SECOND
LENS
ADDITIONAL
APERTURES
AND
STOPS
IN
A
LENS
FURTHER
CONTRIBUTE
TO
VIGNETTING
BRIGHTNESS
GRADIENTS
SPEAKING
OF
WHICH
IT
IS
TIME
TO
LOOK
AT
THIS
EXTRAORDINARY
ORGAN
IN
A
BIT
MORE
DETAIL
THE
HUMAN
EYE
HERE
WE
GIVE
A
BRIEF
OVERVIEW
OF
THE
ANATOMICAL
STRUCTURE
OF
THE
EYE
IT
IS
LARGELY
BASED
ON
THE
PRESENTATION
IN
WANDELL
AND
THE
INTERESTED
READER
IS
INVITED
TO
READ
THIS
EXCELLENT
BOOK
FOR
MORE
DETAILS
FIGURE
LEFT
IS
A
SKETCH
OF
THE
SECTION
OF
AN
EYEBALL
THROUGH
ITS
VERTICAL
PLANE
OF
SYMMETRY
SHOWING
THE
MAIN
ELEMENTS
OF
THE
EYE
THE
IRIS
AND
THE
PUPIL
WHICH
CONTROL
THE
AMOUNT
OF
LIGHT
PENETRATING
THE
EYEBALL
THE
CORNEA
AND
THE
CRYSTALLINE
LENS
WHICH
TOGETHER
REFRACT
THE
LIGHT
TO
CREATE
THE
RETINAL
IMAGE
AND
FINALLY
THE
RETINA
WHERE
THE
IMAGE
IS
FORMED
DESPITE
ITS
GLOBULAR
SHAPE
THE
HUMAN
EYEBALL
IS
FUNCTIONALLY
SIMILAR
TO
A
CAMERA
WITH
A
FIELD
OF
VIEW
COVERING
A
WIDTH
HEIGHT
AREA
LIKE
ANY
OTHER
OPTICAL
SYSTEM
IT
SUFFERS
NERVE
SCLERA
RETINA
CILIARY
BODY
AND
SHEATH
MACULA
VITREOUS
CAVITY
OPTIC
AXIS
CORNEA
F
F
LUTEA
VISUAL
AXIS
IRIS
FOVEA
LENS
CHORIOID
FIGURE
LEFT
THE
MAIN
COMPONENTS
OF
THE
HUMAN
EYE
REPRODUCED
WITH
PERMISSION
THE
AMERICAN
SOCIETY
FOR
PHOTOGRAMMETRY
AND
REMOTE
SENSING
A
L
NOWICKI
STEREOSCOPY
MANUAL
OF
PHOTOGRAMMETRY
EDITED
BY
M
M
THOMPSON
R
C
ELLER
W
A
RADLINSKI
AND
J
L
SPEERT
THIRD
EDITION
PP
BETHESDA
AMERICAN
SOCIETY
OF
PHOTOGRAMMETRY
RIGHT
HELMOLTZ
SCHEMATIC
EYE
AS
MODIFIED
BY
LAURANCE
AFTER
DRISCOLL
AND
VAUGHAN
THE
DISTANCE
BETWEEN
THE
POLE
OF
THE
CORNEA
AND
THE
ANTERIOR
PRINCIPAL
PLANE
IS
MM
AND
THE
RADII
OF
THE
CORNEA
ANTERIOR
AND
POSTERIOR
SURFACES
OF
THE
LENS
ARE
RESPECTIVELY
MM
MM
AND
MM
CAMERAS
CHAP
FROM
VARIOUS
TYPES
OF
GEOMETRIC
AND
CHROMATIC
ABERRATIONS
SEVERAL
MODELS
OF
THE
EYE
OBEYING
THE
LAWS
OF
FIRST
ORDER
GEOMETRIC
OPTICS
HAVE
BEEN
PROPOSED
AND
FIGURE
RIGHT
SHOWS
ONE
OF
THEM
HELMOLTZ
SCHEMATIC
EYE
THERE
ARE
ONLY
THREE
REFRACTIVE
SURFACES
WITH
AN
INFINITELY
THIN
CORNEA
AND
A
HOMOGENEOUS
LENS
THE
CONSTANTS
GIVEN
IN
FIGURE
ARE
FOR
THE
EYE
FOCUSING
AT
INFINITY
UNACCOMMODATED
EYE
THIS
MODEL
IS
OF
COURSE
ONLY
AN
APPROXIMATION
OF
THE
REAL
OPTICAL
CHARACTERISTICS
OF
THE
EYE
LET
US
HAVE
A
SECOND
LOOK
AT
THE
COMPONENTS
OF
THE
EYE
ONE
LAYER
AT
A
TIME
THE
CORNEA
IS
A
TRANSPARENT
HIGHLY
CURVED
REFRACTIVE
WINDOW
THROUGH
WHICH
LIGHT
ENTERS
THE
EYE
BEFORE
BEING
PARTIALLY
BLOCKED
BY
THE
COLORED
AND
OPAQUE
SURFACE
OF
THE
IRIS
THE
PUPIL
IS
AN
OPENING
AT
THE
CENTER
OF
THE
IRIS
WHOSE
DIAMETER
VARIES
FROM
ABOUT
TO
MM
IN
RESPONSE
TO
ILLUMINATION
CHANGES
DILATING
IN
LOW
LIGHT
TO
INCREASE
THE
AMOUNT
OF
ENERGY
THAT
REACHES
THE
RETINA
AND
CONTRACTING
IN
NORMAL
LIGHTING
CONDITIONS
TO
LIMIT
THE
AMOUNT
OF
IMAGE
BLURRING
DUE
TO
SPHERICAL
ABERRATION
IN
THE
EYE
THE
REFRACTING
POWER
RECIPROCAL
OF
THE
FOCAL
LENGTH
OF
THE
EYE
IS
IN
LARGE
PART
AN
EFFECT
OF
REFRACTION
AT
THE
THE
AIR
CORNEA
INTERFACE
AND
IT
IS
FINE
TUNED
BY
DEFORMATIONS
OF
THE
CRYSTALLINE
LENS
THAT
ACCOMMODATES
TO
BRING
OBJECTS
INTO
SHARP
FOCUS
IN
HEALTHY
ADULTS
IT
VARIES
BETWEEN
UNACCOMMODATED
CASE
AND
DIOPTERS
DIOPTER
M
CORRESPONDING
TO
A
RANGE
OF
FOCAL
LENGTHS
BETWEEN
AND
MM
THE
RETINA
ITSELF
IS
A
THIN
LAYERED
MEMBRANE
POPULATED
BY
TWO
TYPES
OF
PHOTORECEPTORS
RODS
AND
CONES
THAT
RESPOND
TO
LIGHT
IN
THE
TO
NM
WAVELENGTH
RANGE
VIOLET
TO
RED
AS
MENTIONED
IN
CHAPTER
THERE
ARE
THREE
TYPES
OF
CONES
WITH
DIFFERENT
SPECTRAL
SENSITIVITIES
AND
THESE
PLAY
A
KEY
ROLE
IN
THE
PERCEPTION
OF
COLOR
THERE
ARE
ABOUT
MILLION
RODS
AND
MILLION
CONES
IN
A
HUMAN
EYE
THEIR
SPATIAL
DISTRIBUTION
VARIES
ACROSS
THE
RETINA
THE
MACULA
LUTEA
IS
A
REGION
IN
THE
CENTER
OF
THE
RETINA
WHERE
THE
CONCENTRATION
OF
CONES
IS
PARTICULARLY
HIGH
AND
IMAGES
ARE
SHARPLY
FOCUSED
WHENEVER
THE
EYE
FIXES
ITS
ATTENTION
ON
AN
OBJECT
FIGURE
THE
HIGHEST
CONCENTRATION
OF
CONES
OCCURS
IN
THE
FOVEA
A
DEPRESSION
IN
THE
MIDDLE
OF
THE
MACULA
LUTEA
WHERE
IT
PEAKS
AT
WITH
THE
CENTERS
OF
TWO
NEIGHBORING
CONES
SEPARATED
BY
ONLY
HALF
A
MINUTE
OF
VISUAL
ANGLE
FIGURE
CONVERSELY
THERE
ARE
NO
RODS
IN
THE
CENTER
OF
THE
FOVEA
BUT
THE
ROD
DENSITY
INCREASES
TOWARD
THE
PERIPHERY
OF
THE
VISUAL
FIELD
THERE
IS
ALSO
A
BLIND
SPOT
ON
THE
RETINA
WHERE
THE
GANGLION
CELL
AXONS
EXIT
THE
RETINA
AND
FORM
THE
OPTIC
NERVE
THE
RODS
ARE
EXTREMELY
SENSITIVE
PHOTORECEPTORS
THEY
ARE
CAPABLE
OF
RESPONDING
TO
A
SINGLE
PHOTON
BUT
THEY
YIELD
RELATIVELY
POOR
SPATIAL
DETAIL
DESPITE
THEIR
HIGH
NUMBER
BECAUSE
MANY
RODS
CONVERGE
TO
THE
SAME
NEURON
WITHIN
THE
RETINA
IN
CONTRAST
CONES
BECOME
ACTIVE
AT
HIGHER
LIGHT
ANGLE
RELATIVE
TO
FOVEA
DEG
FIGURE
THE
DISTRIBUTION
OF
RODS
AND
CONES
ACROSS
THE
RETINA
REPRINTED
FROM
FOUNDATIONS
OF
VISION
BY
B
WANDELL
SINAUER
ASSOCIATES
INC
QC
SINAUER
ASSOCIATES
INC
SEC
SENSING
LEVELS
BUT
THE
SIGNAL
OUTPUT
BY
EACH
CONE
IN
THE
FOVEA
IS
ENCODED
BY
SEVERAL
NEURONS
YIELDING
A
HIGH
RESOLUTION
IN
THAT
AREA
MORE
GENERALLY
THE
AREA
OF
THE
RETINA
INFLUENCING
A
NEURON
RESPONSE
IS
TRADITIONALLY
CALLED
ITS
RECEPTIVE
FIELD
ALTHOUGH
THIS
TERM
NOW
ALSO
CHARACTERIZES
THE
ACTUAL
ELECTRICAL
RESPONSE
OF
NEURONS
TO
LIGHT
PATTERNS
OF
COURSE
MUCH
MORE
COULD
AND
SHOULD
BE
SAID
ABOUT
THE
HUMAN
EYE
FOR
EXAMPLE
HOW
OUR
TWO
EYES
VERGE
AND
FIXATE
ON
TARGETS
COOPERATE
IN
STEREO
VISION
AND
SO
ON
BESIDES
VISION
ONLY
STARTS
WITH
THIS
CAMERA
OF
OUR
MIND
WHICH
LEADS
TO
THE
FASCINATING
AND
STILL
LARGELY
UNSOLVED
PROBLEM
OF
DECIPHERING
THE
ROLE
OF
THE
VARIOUS
PORTIONS
OF
OUR
BRAIN
IN
HUMAN
VISION
WE
COME
BACK
TO
VARIOUS
ASPECTS
OF
THIS
ENDEAVOR
LATER
IN
THIS
BOOK
SENSING
WHAT
DIFFERENTIATES
A
CAMERA
IN
THE
MODERN
SENSE
OF
THE
WORLD
FROM
THE
PORTABLE
CAMERA
OBSCURA
OF
THE
CENTURY
IS
ITS
ABILITY
TO
RECORD
THE
PICTURES
THAT
FORM
ON
ITS
BACKPLANE
ALTHOUGH
IT
HAD
BEEN
KNOWN
SINCE
AT
LEAST
THE
MIDDLE
AGES
THAT
CERTAIN
SILVER
SALTS
RAPIDLY
DARKEN
UNDER
THE
ACTION
OF
SUNLIGHT
IT
WAS
ONLY
IN
THAT
NIEPCE
OBTAINED
THE
FIRST
TRUE
PHOTOGRAPHS
BY
EXPOSING
PAPER
TREATED
WITH
SILVER
CHLORIDE
TO
THE
LIGHT
RAYS
STRIKING
THE
IMAGE
PLANE
OF
A
CAMERA
OBSCURA
THEN
FIXING
THE
PICTURE
WITH
NITRIC
ACID
THESE
FIRST
IMAGES
WERE
NEGATIVES
AND
NIEPCE
SOON
SWITCHED
TO
OTHER
PHOTOSENSITIVE
CHEMICALS
TO
OBTAIN
POSITIVE
PICTURES
THE
EARLIEST
PHOTOGRAPHS
HAVE
BEEN
LOST
AND
THE
FIRST
ONE
TO
HAVE
BEEN
PRESERVED
IS
LA
TABLE
SERVIE
THE
SET
TABLE
REPRODUCED
IN
FIGURE
NIEPCE
INVENTED
PHOTOGRAPHY
BUT
DAGUERRE
WOULD
BE
THE
ONE
TO
POPULARIZE
IT
AFTER
THE
TWO
BECAME
ASSOCIATES
IN
DAGUERRE
WENT
ON
TO
DEVELOP
HIS
OWN
PHOTOGRAPHIC
PROCESS
USING
MERCURY
FUMES
TO
AMPLIFY
AND
REVEAL
THE
LATENT
IMAGE
FORMED
ON
AN
IODIZED
PLATING
OF
SILVER
ON
COPPER
DAGUERRE
OTYPES
WERE
AN
INSTANT
SUCCESS
WHEN
ARAGO
PRESENTED
DAGUERRE
PROCESS
AT
THE
FRENCH
ACADEMY
OF
SCIENCES
IN
THREE
YEARS
AFTER
NIEPCE
DEATH
OTHER
MILESTONES
IN
THE
LONG
HISTORY
OF
PHOTOGRAPHY
INCLUDE
THE
INTRODUCTION
OF
THE
WET
PLATE
NEGATIVE
POSITIVE
PROCESS
BY
LEGRAY
AND
ARCHER
IN
WHICH
REQUIRED
THE
PICTURES
TO
BE
DEVELOPED
ON
THE
SPOT
BUT
PRODUCED
EXCELLENT
NEGATIVES
THE
INVENTION
OF
THE
GELATIN
PROCESS
BY
MADDOX
IN
WHICH
ELIMINATED
THE
NEED
FOR
IMMEDIATE
DEVELOPMENT
THE
INTRODUCTION
IN
OF
THE
PHOTOGRAPHIC
FILM
THAT
HAS
REPLACED
GLASS
PLATES
IN
MOST
MODERN
APPLICATIONS
BY
EASTMAN
AND
THE
INVENTION
BY
THE
LUMIE
RE
BROTHERS
OF
CINEMA
IN
AND
COLOR
PHOTOGRAPHY
IN
FIGURE
THE
FIRST
PHOTOGRAPH
ON
RECORD
LA
TABLE
SERVIE
OBTAINED
BY
NICE
PHORE
NIEPCE
IN
COLLECTION
HARLINGE
VIOLLET
CAMERAS
CHAP
FIGURE
A
CCD
DEVICE
THE
INVENTION
OF
TELEVISION
IN
THE
BY
PEOPLE
LIKE
BAIRD
FARNSWORTH
AND
ZWORYKIN
WAS
OF
COURSE
A
MAJOR
IMPETUS
FOR
THE
DEVELOPMENT
OF
ELECTRONIC
SENSORS
THE
VIDICON
IS
A
COM
MON
TYPE
OF
TV
VACUUM
TUBE
IT
IS
A
GLASS
ENVELOPE
WITH
AN
ELECTRON
GUN
AT
ONE
END
AND
A
FACEPLATE
AT
THE
OTHER
THE
BACK
OF
THE
FACEPLATE
IS
COATED
WITH
A
THIN
LAYER
OF
PHOTOCONDUCTOR
MATERIAL
LAID
OVER
A
TRANSPARENT
FILM
OF
POSITIVELY
CHARGED
METAL
THIS
DOUBLE
COATING
FORMS
THE
TARGET
THE
TUBE
IS
SURROUNDED
BY
FOCUSING
AND
DEFLECTING
COILS
THAT
ARE
USED
TO
REPEATEDLY
SCAN
THE
TARGET
WITH
THE
ELECTRON
BEAM
GENERATED
BY
THE
GUN
THIS
BEAM
DEPOSITS
A
LAYER
OF
ELECTRONS
ON
THE
TARGET
TO
BALANCE
ITS
POSITIVE
CHARGE
WHEN
A
SMALL
AREA
OF
THE
FACEPLATE
IS
STRUCK
BY
LIGHT
ELECTRONS
FLOW
THROUGH
LOCALLY
DEPLETING
THE
CHARGE
OF
THE
TARGET
AS
THE
ELECTRON
BEAM
SCANS
THIS
AREA
IT
REPLACES
THE
LOST
ELECTRONS
CREATING
A
CURRENT
PROPORTIONAL
TO
THE
INCIDENT
LIGHT
INTENSITY
THE
CURRENT
VARIATIONS
ARE
THEN
TRANSFORMED
INTO
A
VIDEO
SIGNAL
BY
THE
VIDICON
CIRCUITRY
CCD
CAMERAS
LET
US
NOW
TURN
TO
CHARGE
COUPLED
DEVICE
CCD
CAMERAS
THAT
WERE
PROPOSED
IN
AND
HAVE
REPLACED
VIDICON
CAMERAS
IN
MOST
MODERN
APPLICATIONS
FROM
CONSUMER
CAMCORDERS
TO
SPECIAL
PURPOSE
CAMERAS
GEARED
TOWARD
MICROSCOPY
OR
ASTRONOMY
APPLICATIONS
A
CCD
SENSOR
USES
A
RECTANGULAR
GRID
OF
ELECTRON
COLLECTION
SITES
LAID
OVER
A
THIN
SILICON
WAFER
TO
RECORD
A
MEASURE
OF
THE
AMOUNT
OF
LIGHT
ENERGY
REACHING
EACH
OF
THEM
FIGURE
EACH
SITE
IS
FORMED
BY
GROWING
A
LAYER
OF
SILICON
DIOXIDE
ON
THE
WAFER
AND
THEN
DEPOSITING
A
CONDUCTIVE
GATE
STRUCTURE
OVER
THE
DIOXIDE
WHEN
PHOTONS
STRIKE
THE
SILICON
ELECTRON
HOLE
PAIRS
ARE
GENERATED
PHOTO
CONVERSION
AND
THE
ELECTRON
ARE
CAPTURED
BY
THE
POTENTIAL
WELL
FORMED
BY
APPLYING
A
POSITIVE
ELECTRICAL
PO
TENTIAL
TO
THE
CORRESPONDING
GATE
THE
ELECTRONS
GENERATED
AT
EACH
SITE
ARE
COLLECTED
OVER
A
FIXED
PERIOD
OF
TIME
T
AT
THIS
POINT
THE
CHARGES
STORED
AT
THE
INDIVIDUAL
SITES
ARE
MOVED
USING
CHARGE
COUPLING
CHARGE
PACKETS
ARE
TRANSFERED
FROM
SITE
TO
SITE
BY
MANIPULATING
THE
GATE
POTENTIALS
PRESERVING
THE
SEPARATION
OF
THE
PACKETS
THE
IMAGE
IS
READ
OUT
OF
THE
CCD
ONE
ROW
AT
A
TIME
EACH
ROW
BEING
TRANSFERED
IN
PARALLEL
TO
A
SERIAL
OUTPUT
REGISTER
WITH
ONE
ELEMENT
IN
EACH
COLUMN
BETWEEN
TWO
ROW
READS
THE
REGISTER
TRANSFERS
ITS
CHARGES
ONE
AT
A
TIME
TO
AN
OUTPUT
AMPLIFIER
THAT
GENERATES
A
SIGNAL
PROPORTIONAL
TO
THE
CHARGE
IT
RECEIVES
THIS
PROCESS
CONTINUES
UNTIL
THE
ENTIRE
IMAGE
HAS
SEC
SENSING
BEEN
READ
OUT
IT
CAN
BE
REPEATED
TIMES
PER
SECOND
TV
RATE
FOR
VIDEO
APPLICATIONS
OR
AT
A
MUCH
SLOWER
PACE
LEAVING
AMPLE
TIME
SECONDS
MINUTES
EVEN
HOURS
FOR
ELECTRON
COLLECTION
IN
LOW
LIGHT
LEVEL
APPLICATIONS
SUCH
AS
ASTRONOMY
IT
SHOULD
BE
NOTED
THAT
THE
DIGITAL
OUTPUT
OF
MOST
CCD
CAMERAS
IS
TRANSFORMED
INTERNALLY
INTO
AN
ANALOG
VIDEO
SIGNAL
BEFORE
BEING
PASSED
TO
A
FRAME
GRABBER
THAT
CONSTRUCTS
THE
FINAL
DIGITAL
IMAGE
CONSUMER
GRADE
COLOR
CCD
CAMERAS
ESSENTIALLY
USE
THE
SAME
CHIPS
AS
BLACK
AND
WHITE
CAMERAS
EXCEPT
THAT
SUCCESSIVE
ROWS
OR
COLUMNS
OF
SENSORS
ARE
MADE
SENSITIVE
TO
RED
GREEN
OR
BLUE
LIGHT
OFTEN
USING
A
FILTER
COATING
THAT
BLOCKS
THE
COMPLEMENTARY
LIGHT
OTHER
FILTER
PATTERNS
ARE
POSSIBLE
INCLUDING
MOSAICS
OF
BLOCKS
FORMED
BY
TWO
GREEN
ONE
RED
AND
ONE
BLUE
RECEPTORS
BAYER
PATTERNS
THE
SPATIAL
RESOLUTION
OF
SINGLE
CCD
CAMERAS
IS
OF
COURSE
LIMITED
AND
HIGHER
QUALITY
CAMERAS
USE
A
BEAM
SPLITTER
TO
SHIP
THE
IMAGE
TO
THREE
DIFFERENT
CCDS
VIA
COLOR
FILTERS
THE
INDIVIDUAL
COLOR
CHANNELS
ARE
THEN
EITHER
DIGITIZED
SEPARATELY
RGB
OUTPUT
OR
COMBINED
INTO
A
COMPOSITE
COLOR
VIDEO
SIGNAL
NTSC
OUTPUT
IN
THE
UNITED
STATES
SECAM
OR
PAL
IN
EUROPE
AND
JAPAN
OR
INTO
A
COMPONENT
VIDEO
FORMAT
SEPARATING
COLOR
AND
BRIGHTNESS
INFORMATION
SENSOR
MODELS
FOR
SIMPLICITY
WE
RESTRICT
OUR
ATTENTION
IN
THIS
SECTION
TO
BLACK
AND
WHITE
CCD
CAMERAS
COLOR
CAMERAS
CAN
BE
TREATED
IN
A
SIMILAR
FASHION
BY
CONSIDERING
EACH
COLOR
CHANNEL
SEPARATELY
AND
TAKING
THE
EFFECT
OF
THE
ASSOCIATED
FILTER
RESPONSE
EXPLICITLY
INTO
ACCOUNT
THE
NUMBER
I
OF
ELECTRONS
RECORDED
AT
THE
CELL
LOCATED
AT
ROW
R
AND
COLUMN
C
OF
A
CCD
ARRAY
CAN
BE
MODELED
AS
I
R
C
T
E
P
Λ
R
P
Q
Λ
DP
DΛ
Λ
P
R
C
WHERE
T
IS
THE
ELECTRON
COLLECTION
TIME
AND
THE
INTEGRAL
IS
CALCULATED
OVER
THE
SPATIAL
DOMAIN
R
C
OF
THE
CELL
AND
THE
RANGE
OF
WAVELENGTHS
TO
WHICH
THE
CCD
HAS
A
NONZERO
RESPONSE
IN
THIS
INTEGRAL
E
IS
IS
THE
POWER
PER
UNIT
AREA
AND
UNIT
WAVELENGTH
I
E
THE
IRRADIANCE
SEE
CHAPTER
FOR
A
FORMAL
DEFINITION
ARRIVING
AT
THE
POINT
P
R
IS
THE
SPATIAL
RESPONSE
OF
THE
SITE
AND
Q
IS
THE
QUANTUM
EFFICIENCY
OF
THE
DEVICE
I
E
THE
NUMBER
OF
ELECTRONS
GENERATED
PER
UNIT
OF
INCIDENT
LIGHT
ENERGY
IN
GENERAL
BOTH
E
AND
Q
DEPEND
ON
THE
LIGHT
WAVELENGTH
Λ
AND
E
AND
R
DEPEND
ON
THE
POINT
LOCATION
P
WITHIN
R
C
THE
OUTPUT
AMPLIFIER
OF
THE
CCD
TRANSFORMS
THE
CHARGE
COLLECTED
AT
EACH
SITE
INTO
A
MEA
SURABLE
VOLTAGE
IN
MOST
CAMERAS
THIS
VOLTAGE
IS
THEN
TRANSFORMED
INTO
A
LOW
PASS
VIDEO
SIGNAL
BY
THE
CAMERA
ELECTRONICS
WITH
A
MAGNITUDE
PROPORTIONAL
TO
I
THE
ANALOG
IMAGE
CAN
BE
ONCE
AGAIN
TRANSFORMED
INTO
A
DIGITAL
ONE
USING
A
FRAME
GRABBER
THAT
SPATIALLY
SAMPLES
THE
VIDEO
SIGNAL
AND
QUANTIZES
THE
BRIGHTNESS
VALUE
AT
EACH
IMAGE
POINT
OR
PIXEL
FROM
PICTURE
ELEMENT
THERE
ARE
SEVERAL
PHYSICAL
PHENOMENA
THAT
ALTER
THE
IDEAL
CAMERA
MODEL
PRESENTED
EARLIER
BLOOMING
OCCURS
WHEN
THE
LIGHT
SOURCE
ILLUMINATING
A
COLLECTION
SITE
IS
SO
BRIGHT
THAT
THE
CHARGE
STORED
AT
THAT
SITE
OVERFLOWS
INTO
ADJACENT
ONES
IT
CAN
BE
AVOIDED
BY
CONTROLLING
THE
ILLUMINATION
BUT
OTHER
FACTORS
SUCH
AS
FABRICATION
DEFECTS
THERMAL
AND
QUANTUM
EFFECTS
AND
QUANTIZATION
NOISE
ARE
INHERENT
TO
THE
IMAGING
PROCESS
AS
SHOWN
NEXT
THESE
FACTORS
ARE
APPROPRIATELY
CAPTURED
BY
SIMPLE
STATISTICAL
MODELS
QUANTUM
PHYSICS
EFFECTS
INTRODUCE
AN
INHERENT
UNCERTAINTY
IN
THE
PHOTOCONVERSION
PROCESS
AT
EACH
SITE
SHOT
NOISE
MORE
PRECISELY
THE
NUMBER
OF
ELECTRONS
GENERATED
BY
THIS
PROCESS
CAN
BE
MODELED
BY
A
RANDOM
INTEGER
VARIABLE
NI
R
C
OBEYING
A
POISSON
DISTRIBUTION
WITH
MEAN
Β
R
C
I
R
C
WHERE
Β
R
C
IS
A
NUMBER
BETWEEN
AND
THAT
REFLECTS
THE
VARIATION
OF
THE
SPATIAL
RESPONSE
AND
QUANTUM
EFFICIENCY
ACROSS
THE
IMAGE
AND
ALSO
ACCOUNTS
FOR
BAD
PIXELS
ELECTRONS
IS
ROUGHLY
SPEAKING
SPATIALLY
OR
TEMPORALLY
AVERAGED
MORE
ON
THIS
LATER
CAMERAS
CHAP
FREED
FROM
THE
SILICON
BY
THERMAL
ENERGY
ADD
TO
THE
CHARGE
OF
EACH
COLLECTION
SITE
THEIR
CON
TRIBUTION
IS
CALLED
DARK
CURRENT
AND
IT
CAN
BE
MODELED
BY
A
RANDOM
INTEGER
VARIABLE
NDC
R
C
WHOSE
MEAN
ΜDC
R
C
INCREASES
WITH
TEMPERATURE
THE
EFFECT
OF
DARK
CURRENT
CAN
BE
CONTROLLED
BY
COOLING
DOWN
THE
CAMERA
ADDITIONAL
ELECTRONS
ARE
INTRODUCED
BY
THE
CCD
ELECTRONICS
BIAS
AND
THEIR
NUMBER
CAN
ALSO
BE
MODELED
BY
A
POISSON
DISTRIBUTED
RANDOM
VARIABLE
NB
R
C
WITH
MEAN
ΜB
R
C
THE
OUTPUT
AMPLIFIER
ADDS
READ
OUT
NOISE
THAT
CAN
BE
MODELED
BY
A
REAL
VALUED
RANDOM
VARIABLE
R
OBEYING
A
GAUSSIAN
DISTRIBUTION
WITH
MEAN
ΜR
AND
STANDARD
DEVIATION
ΣR
THERE
ARE
OTHER
SOURCES
OF
UNCERTAINTY
E
G
CHARGE
TRANSFER
EFFICIENCY
BUT
THEY
CAN
OFTEN
BE
NEGLECTED
FINALLY
THE
DISCRETIZATION
OF
THE
ANALOG
VOLTAGE
BY
THE
FRAME
GRABBER
INTRODUCES
BOTH
GEOMETRIC
EFFECTS
LINE
JITTER
WHICH
CAN
BE
CORRECTED
VIA
CALIBRATION
AND
A
QUANTIZATION
NOISE
WHICH
CAN
BE
MODELED
AS
A
ZERO
MEAN
RANDOM
VARIABLE
Q
R
C
WITH
A
UNIFORM
DISTRIBUTION
IN
THE
Δ
INTERVAL
AND
A
VARIANCE
OF
WHERE
Δ
IS
THE
QUANTIZATION
STEP
THIS
YIELDS
THE
FOLLOWING
COMPOSITE
MODEL
FOR
THE
DIGITAL
SIGNAL
D
R
C
D
R
C
Γ
NI
R
C
NDC
R
C
NB
R
C
R
R
C
Q
R
C
IN
THIS
EQUATION
Γ
IS
THE
COMBINED
GAIN
OF
THE
AMPLIFIER
AND
CAMERA
CIRCUITRY
THE
STATISTICAL
PROPERTIES
OF
THIS
MODEL
CAN
BE
ESTIMATED
VIA
RADIOMETRIC
CAMERA
CALIBRATION
FOR
EXAMPLE
DARK
CURRENT
CAN
BE
ESTIMATED
BY
TAKING
A
NUMBER
OF
SAMPLE
PICTURES
IN
A
DARK
ENVIRONMENT
I
NOTES
THE
CLASSICAL
TEXTBOOK
BY
HECHT
IS
AN
EXCELLENT
INTRODUCTION
TO
GEOMETRIC
OPTICS
IT
IN
CLUDES
A
DETAILED
DISCUSSION
OF
PARAXIAL
OPTICS
AS
WELL
AS
THE
VARIOUS
ABERRATIONS
BRIEFLY
MEN
TIONED
IN
THIS
CHAPTER
SEE
ALSO
DRISCOLL
AND
VAUGHAN
VIGNETTING
IS
DISCUSSED
IN
HORN
AND
RUSS
WANDELL
GIVES
AN
EXCELLENT
TREATMENT
OF
IMAGE
FORMATION
IN
TABLE
REFERENCE
CARD
CAMERA
MODELS
PROBLEMS
THE
HUMAN
VISUAL
SYSTEM
THE
HELMOLTZ
SCHEMATIC
MODEL
OF
THE
EYE
IS
DETAILED
IN
DRISCOLL
AND
VAUGHAN
CCD
DEVICES
WERE
INTRODUCED
IN
BOYLE
AND
SMITH
AND
AMELIO
ET
AL
SCI
ENTIFIC
APPLICATIONS
OF
CCD
CAMERAS
TO
MICROSCOPY
AND
ASTRONOMY
ARE
DISCUSSED
IN
AIKEN
ET
AL
JANESICK
ET
AL
SNYDER
ET
AL
AND
TYSON
THE
STATISTICAL
SENSOR
MODEL
PRESENTED
IN
THIS
CHAPTER
IS
BASED
ON
SNYDER
ET
AL
WITH
AN
ADDITIONAL
TERM
FOR
THE
QUANTIZATION
NOISE
TAKEN
FROM
HEALEY
AND
KONDEPUDY
THESE
TWO
ARTICLES
CONTAIN
INTER
ESTING
APPLICATIONS
OF
SENSOR
MODELING
TO
IMAGE
RESTORATION
IN
ASTRONOMY
AND
RADIOMETRIC
CAMERA
CALIBRATION
IN
MACHINE
VISION
GIVEN
THE
FUNDAMENTAL
IMPORTANCE
OF
THE
NOTIONS
INTRODUCED
IN
THIS
CHAPTER
THE
MAIN
EQUA
TIONS
DERIVED
IN
ITS
COURSE
HAVE
BEEN
COLLECTED
IN
TABLE
FOR
REFERENCE
PROBLEMS
DERIVE
THE
PERSPECTIVE
EQUATION
PROJECTIONS
FOR
A
VIRTUAL
IMAGE
LOCATED
AT
A
DISTANCE
F
IN
FRONT
OF
THE
PINHOLE
PROVE
GEOMETRICALLY
THAT
THE
PROJECTIONS
OF
TWO
PARALLEL
LINES
LYING
IN
SOME
PLANE
IT
APPEAR
TO
CONVERGE
ON
A
HORIZON
LINE
H
FORMED
BY
THE
INTERSECTION
OF
THE
IMAGE
PLANE
WITH
THE
PLANE
PARALLEL
TO
IT
AND
PASSING
THROUGH
THE
PINHOLE
PROVE
THE
SAME
RESULT
ALGEBRAICALLY
USING
THE
PERSPECTIVE
PROJECTION
EQ
YOU
CAN
ASSUME
FOR
SIMPLICITY
THAT
THE
PLANE
IT
IS
ORTHOGONAL
TO
THE
IMAGE
PLANE
USE
SNELL
LAW
TO
SHOW
THAT
RAYS
PASSING
THROUGH
THE
OPTICAL
CENTER
OF
A
THIN
LENS
ARE
NOT
REFRACTED
AND
DERIVE
THE
THIN
LENS
EQUATION
HINT
CONSIDER
A
RAY
PASSING
THROUGH
THE
POINT
P
AND
CONSTRUCT
THE
RAYS
AND
OBTAINED
RESPECTIVELY
BY
THE
REFRACTION
OF
BY
THE
RIGHT
BOUNDARY
OF
THE
LENS
AND
THE
REFRACTION
OF
BY
ITS
LEFT
BOUNDARY
CONSIDER
A
CAMERA
EQUIPPED
WITH
A
THIN
LENS
WITH
ITS
IMAGE
PLANE
AT
POSITION
AND
THE
PLANE
OF
SCENE
POINTS
IN
FOCUS
AT
POSITION
Z
NOW
SUPPOSE
THAT
THE
IMAGE
PLANE
IS
MOVED
TO
SHOW
THAT
THE
DIAMETER
OF
THE
CORRESPONDING
BLUR
CIRCLE
IS
D
WHERE
D
IS
THE
LENS
DIAMETER
USE
THIS
RESULT
TO
SHOW
THAT
THE
DEPTH
OF
FIELD
I
E
THE
DISTANCE
BETWEEN
THE
NEAR
AND
FAR
PLANES
THAT
WILL
KEEP
THE
DIAMETER
OF
THE
BLUR
CIRCLES
BELOW
SOME
THRESHOLD
Ε
IS
GIVEN
BY
D
D
Z
Z
F
F
AND
CONCLUDE
THAT
FOR
A
FIXED
FOCAL
LENGTH
THE
DEPTH
OF
FIELD
INCREASES
AS
THE
LENS
DIAMETER
DECREASES
AND
THUS
THE
F
NUMBER
INCREASES
HINT
SOLVE
FOR
THE
DEPTH
Zˆ
OF
A
POINT
WHOSE
IMAGE
IS
FOCUSED
ON
THE
IMAGE
PLANE
AT
POSITION
CONSIDERING
BOTH
THE
CASE
WHERE
IS
LARGER
THAN
AND
THE
CASE
WHERE
IT
IS
SMALLER
GIVE
A
GEOMETRIC
CONSTRUCTION
OF
THE
IMAGE
OF
A
POINT
P
GIVEN
THE
TWO
FOCAL
POINTS
F
AND
F
OF
A
THIN
LENS
DERIVE
THE
THICK
LENS
EQUATIONS
IN
THE
CASE
WHERE
BOTH
SPHERICAL
BOUNDARIES
OF
THE
LENS
HAVE
THE
SAME
RADIUS
SURFACES
ARE
BRIGHT
OR
DARK
FOR
TWO
MAIN
REASONS
THEIR
ALBEDO
AND
THE
AMOUNT
OF
LIGHT
THEY
ARE
RE
CEIVING
A
MODEL
OF
HOW
THE
BRIGHTNESS
OF
A
SURFACE
IS
OBTAINED
IS
USUALLY
CALLED
A
SHADING
MODEL
SHADING
MODELS
ARE
IMPORTANT
BECAUSE
WITH
AN
APPROPRIATE
SHADING
MODEL
WE
CAN
INTERPRET
PIXEL
VALUES
IF
THE
RIGHT
SHADING
MODEL
APPLIES
IT
IS
POSSIBLE
TO
RECONSTRUCT
OBJECTS
AND
THEIR
ALBEDOS
USING
JUST
A
FEW
IMAGES
FURTHERMORE
WE
CAN
INTERPRET
SHADOWS
AND
EXPLAIN
THEIR
PUZZLING
AND
SELDOM
NOTICED
ABSENCE
IN
MOST
INDOOR
SCENES
QUALITATIVE
RADIOMETRY
WE
SHOULD
LIKE
TO
KNOW
HOW
BRIGHT
SURFACES
ARE
GOING
TO
BE
UNDER
VARIOUS
LIGHTING
CONDITIONS
AND
HOW
THIS
BRIGHTNESS
DEPENDS
ON
LOCAL
SURFACE
PROPERTIES
ON
SURFACE
SHAPE
AND
ON
ILLUMI
NATION
AS
WE
SAW
IN
CHAPTER
FORESHORTENING
MEANS
THAT
DIFFERENT
SOURCES
CAN
HAVE
THE
SAME
EFFECT
ON
A
SURFACE
THE
MOST
POWERFUL
TOOL
FOR
ANALYZING
THIS
PROBLEM
IS
TO
THINK
ABOUT
WHAT
A
SOURCE
LOOKS
LIKE
FROM
THE
SURFACE
THIS
QUALITATIVE
RADIOMETRY
IS
ONE
OF
THESE
TRICKS
THAT
LOOKS
UNSOPHISTICATED
NO
HARD
MATH
BUT
IS
EXTREMELY
POWERFUL
IN
SOME
CASES
THIS
TECHNIQUE
GIVES
QUALITATIVE
DESCRIPTIONS
OF
BRIGHTNESS
WITHOUT
EVEN
KNOWING
WHAT
THE
TERM
MEANS
RECALL
FROM
SECTION
AND
FIGURE
THAT
A
SURFACE
PATCH
SEES
THE
WORLD
THROUGH
A
HEMISPHERE
OF
DIRECTIONS
AT
THAT
PATCH
THE
RADIATION
ARRIVING
AT
THE
SURFACE
ALONG
A
PARTICULAR
DIRECTION
PASSES
THROUGH
A
POINT
ON
THE
HEMISPHERE
IF
TWO
SURFACE
PATCHES
HAVE
EQUIVALENT
IN
COMING
HEMISPHERES
THEY
MUST
HAVE
THE
SAME
INCOMING
RADIATION
WHATEVER
THE
OUTSIDE
WORLD
LOOKS
LIKE
THIS
MEANS
THAT
ANY
DIFFERENCE
IN
BRIGHTNESS
BETWEEN
PATCHES
WITH
THE
SAME
INCOM
ING
HEMISPHERE
IS
A
RESULT
OF
DIFFERENT
SURFACE
PROPERTIES
IN
PARTICULAR
IF
TWO
SURFACE
PATCHES
WITH
THE
SAME
BRDF
SEE
THE
SAME
INCOMING
HEMISPHERE
THEN
THE
RADIATION
THEY
OUTPUT
MUST
BE
THE
SAME
SEC
SOURCES
AND
THEIR
EFFECTS
A
B
C
FIGURE
A
GEOMETRY
IN
WHICH
A
QUALITATIVE
RADIOMETRIC
SOLUTION
CAN
BE
OB
TAINED
BY
THINKING
ABOUT
WHAT
THE
WORLD
LOOKS
LIKE
FROM
THE
POINT
OF
VIEW
OF
A
PATCH
WE
WISH
TO
KNOW
WHAT
THE
BRIGHTNESS
LOOKS
LIKE
AT
THE
BASE
OF
TWO
DIF
FERENT
INFINITELY
HIGH
WALLS
IN
THIS
GEOMETRY
AN
INFINITELY
HIGH
MATTE
BLACK
WALL
CUTS
OFF
THE
VIEW
OF
THE
OVERCAST
SKY
WHICH
IS
A
HEMISPHERE
OF
INFINITE
RADIUS
AND
UNIFORM
BRIGHTNESS
ON
THE
RIGHT
WE
SHOW
A
REPRESENTATION
OF
THE
DIREC
TIONS
THAT
SEE
OR
DO
NOT
SEE
THE
SOURCE
AT
THE
CORRESPONDING
POINTS
OBTAINED
BY
FLATTENING
THE
HEMISPHERE
TO
A
CIRCLE
OF
DIRECTIONS
OR
EQUIVALENTLY
BY
VIEWING
IT
FROM
ABOVE
SINCE
EACH
POINT
HAS
THE
SAME
INPUT
HEMISPHERE
THE
BRIGHTNESS
MUST
BE
UNIFORM
LAMBERT
DETERMINED
THE
DISTRIBUTION
OF
BRIGHTNESS
ON
A
UNIFORM
PLANE
AT
THE
BASE
OF
AN
INFINITELY
HIGH
BLACK
WALL
ILLUMINATED
BY
AN
OVERCAST
SKY
SEE
FIGURE
IN
THIS
CASE
EVERY
POINT
ON
THE
PLANE
MUST
SEE
THE
SAME
HEMISPHERE
HALF
OF
ITS
VIEWING
SPHERE
IS
CUT
OFF
BY
THE
WALL
AND
THE
OTHER
HALF
CONTAINS
THE
SKY
WHICH
IS
UNIFORM
AND
THE
PLANE
IS
UNIFORM
SO
EVERY
POINT
MUST
HAVE
THE
SAME
BRIGHTNESS
A
SECOND
EXAMPLE
IS
SOMEWHAT
TRICKIER
WE
NOW
HAVE
AN
INFINITELY
THIN
BLACK
WALL
THAT
IS
INFINITELY
LONG
IN
ONLY
ONE
DIRECTION
AND
ON
AN
INFINITE
PLANE
FIGURE
A
QUALITATIVE
DESCRIPTION
WOULD
BE
TO
FIND
WHAT
THE
CURVES
OF
EQUAL
BRIGHTNESS
LOOK
LIKE
IT
IS
FAIRLY
EASY
TO
SEE
THAT
ALL
POINTS
ON
ANY
LINE
PASSING
THROUGH
THE
POINT
P
IN
FIGURE
SEE
THE
SAME
INPUT
HEMISPHERE
AND
SO
MUST
HAVE
THE
SAME
BRIGHTNESS
FURTHERMORE
THE
DISTRIBUTION
OF
BRIGHTNESS
ON
THE
PLANE
MUST
HAVE
A
SYMMETRY
ABOUT
THE
LINE
OF
THE
WALL
WE
EXPECT
THE
BRIGHTEST
POINTS
TO
BE
ALONG
THE
EXTENSION
OF
THE
LINE
OF
THE
WALL
AND
THE
DARKEST
TO
BE
AT
THE
BASE
OF
THE
WALL
SOURCES
AND
THEIR
EFFECTS
RADIOMETRIC
PROPERTIES
OF
LIGHT
SOURCES
WE
DEFINE
A
LIGHT
SOURCE
TO
BE
ANYTHING
THAT
EMITS
LIGHT
THAT
IS
INTERNALLY
GENERATED
I
E
NOT
JUST
REFLECTED
TO
DESCRIBE
A
SOURCE
WE
NEED
A
DESCRIPTION
OF
THE
RADIANCE
IT
EMITS
IN
EACH
DIRECTION
TYPICALLY
INTERNALLY
GENERATED
RADIANCE
IS
DEALT
WITH
SEPARATELY
FROM
REFLECTED
RADIANCE
THIS
IS
BECAUSE
ALTHOUGH
A
SOURCE
MAY
REFLECT
LIGHT
THE
LIGHT
IT
REFLECTS
DEPENDS
ON
THE
ENVIRONMENT
WHEREAS
THE
LIGHT
IT
GENERATES
INTERNALLY
USUALLY
DOES
NOT
SOURCES
SHADOWS
AND
SHADING
CHAP
DARKER
FIGURE
WE
NOW
HAVE
A
MATTE
BLACK
INFINITELY
THIN
HALF
INFINITE
WALL
ON
AN
INFINITE
WHITE
PLANE
SHOWN
ON
THE
LEFT
THIS
GEOMETRY
ALSO
SEES
AN
OVERCAST
SKY
OF
INFINITE
RADIUS
AND
UNIFORM
BRIGHTNESS
IN
THE
TEXT
WE
SHOW
HOW
TO
DETER
MINE
THE
CURVES
OF
SIMILAR
BRIGHTNESS
ON
THE
PLANE
THESE
CURVES
ARE
SHOWN
ON
THE
RIGHT
DEPICTED
ON
AN
OVERHEAD
VIEW
OF
THE
PLANE
THE
THICK
LINE
REPRESENTS
THE
WALL
SUPERIMPOSED
ON
THESE
CURVES
IS
A
REPRESENTATION
OF
THE
INPUT
HEMISPHERE
FOR
SOME
OF
THESE
ISOPHOTES
ALONG
THESE
CURVES
THE
HEMISPHERE
IS
FIXED
BY
A
GEOMETRICAL
ARGUMENT
BUT
THEY
CHANGE
AS
ONE
MOVES
FROM
CURVE
TO
CURVE
WE
SELDOM
NEED
A
COMPLETE
DESCRIPTION
OF
THE
RADIANCE
A
SOURCE
EMITS
IN
EACH
DIRECTION
IT
IS
MORE
USUAL
TO
MODEL
SOURCES
AS
EMITTING
A
CONSTANT
RADIANCE
IN
EACH
DIRECTION
POSSIBLY
WITH
A
FAMILY
OF
DIRECTIONS
ZEROED
LIKE
A
SPOTLIGHT
THE
PROPER
QUANTITY
IN
THIS
CASE
IS
THE
EXITANCE
DEFINED
AS
THE
INTERNALLY
GENERATED
ENERGY
RADIATED
PER
UNIT
TIME
AND
PER
UNIT
AREA
ON
THE
RADIATING
SURFACE
EXITANCE
IS
SIMILAR
TO
RADIOSITY
AND
CAN
BE
COMPUTED
AS
E
P
LE
P
ΘO
ΦO
COS
ΘO
DΩ
TOGETHER
WITH
A
DESCRIPTION
OF
THE
EXITANCE
WE
NEED
A
DESCRIPTION
OF
THE
GEOMETRY
OF
THE
SOURCE
WHICH
HAS
PROFOUND
EFFECTS
ON
THE
SPATIAL
VARIATION
OF
LIGHT
AROUND
THE
SOURCE
AND
ON
THE
SHADOWS
CAST
BY
OBJECTS
NEAR
THE
SOURCE
SOURCES
ARE
USUALLY
MODELED
WITH
QUITE
SIMPLE
GEOMETRIES
FOR
TWO
REASONS
FIRST
MANY
SYNTHETIC
SOURCES
CAN
BE
MODELED
AS
POINT
SOURCES
LINE
SOURCES
OR
AREA
SOURCES
FAIRLY
EFFECTIVELY
SECOND
SOURCES
WITH
SIMPLE
GEOMETRIES
CAN
STILL
YIELD
SURPRISINGLY
COMPLEX
EFFECTS
POINT
SOURCES
A
COMMON
APPROXIMATION
IS
TO
ASSUME
THAT
THE
LIGHT
SOURCE
IS
AN
EXTREMELY
SMALL
SPHERE
IN
FACT
A
POINT
SUCH
A
SOURCE
IS
KNOWN
AS
A
POINT
SOURCE
IT
IS
A
NATURAL
MODEL
TO
USE
BECAUSE
MANY
SOURCES
ARE
PHYSICALLY
SMALL
COMPARED
WITH
THE
ENVIRONMENT
IN
WHICH
THEY
STAND
WE
CAN
OBTAIN
SEC
SOURCES
AND
THEIR
EFFECTS
APPROX
TI
E
D
RADIUS
E
D
E
CONSTANT
RADIANCE
PATCH
DUE
TO
SOURCE
FIGURE
A
SURFACE
PATCH
SEES
A
DISTANT
SPHERE
OF
SMALL
RADIUS
THE
SPHERE
PRODUCES
A
SMALL
ILLUMINATED
PATCH
ON
THE
INPUT
HEMISPHERE
OF
THE
SPHERE
IN
THE
TEXT
BY
REASONING
ABOUT
THE
SCALING
BEHAVIOR
OF
THIS
PATCH
AS
THE
DISTANT
SPHERE
MOVES
FARTHER
AWAY
OR
GETS
BIGGER
WE
OBTAIN
AN
EXPRESSION
FOR
THE
BEHAVIOR
OF
THE
POINT
SOURCE
A
MODEL
FOR
THE
EFFECTS
OF
A
POINT
SOURCE
BY
MODELING
THE
SOURCE
AS
A
VERY
SMALL
SPHERE
THAT
EMITS
LIGHT
AT
EACH
POINT
ON
THE
SPHERE
WITH
AN
EXITANCE
CONSTANT
OVER
THE
SPHERE
ASSUME
THAT
A
SURFACE
PATCH
IS
VIEWING
A
SPHERE
OF
RADIUS
E
AT
A
DISTANCE
R
AWAY
AND
THAT
E
R
FIGURE
THE
ASSUMPTION
THAT
THE
SPHERE
IS
FAR
AWAY
FROM
THE
PATCH
RELATIVE
TO
ITS
RADIUS
ALMOST
ALWAYS
APPLIES
FOR
REAL
SOURCES
NOW
THE
SOLID
ANGLE
THAT
THE
SOURCE
SUBTENDS
IS
THIS
BEHAVES
APPROXIMATELY
PROPORTIONAL
TO
R
THE
PATTERN
OF
ILLUMINATION
THAT
THE
SOURCE
CREATES
ON
THE
HEMISPHERE
ROUGHLY
SCALES
TOO
AS
THE
SPHERE
MOVES
AWAY
THE
RAYS
LEAVING
THE
SURFACE
PATCH
AND
STRIKING
THE
SPHERE
MOVE
CLOSER
TO
GETHER
ROUGHLY
EVENLY
AND
THE
COLLECTION
CHANGES
ONLY
SLIGHTLY
A
SMALL
SET
OF
NEW
RAYS
IS
ADDED
AT
THE
RIM
THE
CONTRIBUTION
FROM
THESE
RAYS
MUST
BE
SMALL
BECAUSE
THEY
COME
FROM
DIRECTIONS
TANGENT
TO
THE
SPHERE
IN
THE
LIMIT
AS
E
TENDS
TO
ZERO
NO
NEW
RAYS
ARE
ADDED
THE
RADIOSITY
DUE
TO
THE
SOURCE
IS
OBTAINED
BY
INTEGRATING
THE
PATTERN
GENERATED
BY
THE
SOURCE
TIMES
COS
ΘI
OVER
THE
PATCH
OF
SOLID
ANGLE
AS
E
TENDS
TO
ZERO
THE
PATCH
SHRINKS
AND
THE
COS
ΘI
IS
CLOSE
TO
CONSTANT
IF
Ρ
IS
THE
SURFACE
ALBEDO
ALL
THIS
MEANS
THE
EXPRESSION
FOR
RADIOSITY
DUE
TO
THE
POINT
SOURCE
IS
SOURCES
SHADOWS
AND
SHADING
CHAP
E
Ρ
R
E
COS
Θ
WHERE
E
IS
A
TERM
IN
THE
EXITANCE
OF
THE
SOURCE
INTEGRATED
OVER
THE
SMALL
PATCH
WE
DO
NOT
NEED
A
MORE
DETAILED
EXPRESSION
FOR
E
TO
DETERMINE
ONE
WE
WOULD
NEED
TO
ACTUALLY
DO
THE
INTEGRAL
WE
HAVE
SHIRKED
A
NEARBY
POINT
SOURCE
THE
ANGLE
TERM
CAN
BE
WRITTEN
IN
TERMS
OF
N
P
THE
UNIT
NORMAL
TO
THE
SURFACE
AND
P
A
VECTOR
FROM
P
TO
THE
SOURCE
WHOSE
LENGTH
IS
TO
YIELD
THE
STANDARD
MODEL
OF
A
NEARBY
POINT
SOURCE
Ρ
P
N
P
P
D
R
P
THIS
IS
AN
EXTREMELY
CONVENIENT
MODEL
BECAUSE
IT
GIVES
AN
EXPLICIT
RELATIONSHIP
BETWEEN
RADIOSITY
AND
SHAPE
THE
NORMAL
TERM
IN
THIS
MODEL
IS
USUALLY
CALLED
THE
SOURCE
VECTOR
IT
IS
COMMON
AND
INCORRECT
TO
OMIT
THE
DEPENDENCY
ON
DISTANCE
TO
THE
SOURCE
FROM
THIS
MODEL
A
POINT
SOURCE
AT
INFINITY
THE
SUN
IS
FAR
AWAY
AS
A
RESULT
THE
TERMS
R
P
AND
P
ARE
ESSENTIALLY
CONSTANT
IN
THIS
CASE
THE
POINT
SOURCE
IS
REFERRED
TO
AS
BEING
A
POINT
SOURCE
AT
INFINITY
IF
ALL
THE
SURFACE
PATCHES
WE
ARE
INTERESTED
IN
ARE
CLOSE
TOGETHER
WITH
RESPECT
TO
THE
DISTANCE
TO
THE
SOURCE
R
P
F
R
P
WHERE
F
R
P
FURTHERMORE
P
F
P
WHERE
F
P
WE
NOW
HAVE
N
P
N
F
P
N
R
P
F
R
P
NOW
BOTH
AND
ARE
CONSTANTS
AND
THERE
IS
NO
PARTICULAR
POINT
IN
KEEPING
THEM
EXPLICITLY
IN
THE
EXPRESSION
INSTEAD
WE
CAN
WRITE
R
AND
OUR
MODEL
FOR
THE
RADIOSITY
DUE
TO
A
POINT
SOURCE
AT
INFINITY
BECOMES
B
P
ΡD
P
N
THE
TERM
IS
AGAIN
KNOWN
AS
THE
SOURCE
VECTOR
TYPICALLY
THIS
MODEL
IS
USED
BY
INFERRING
A
SOURCE
VECTOR
THAT
IS
APPROPRIATE
RATHER
THAN
BY
COMPUTING
ITS
VALUE
FROM
THE
EXITANCE
OF
THE
SOURCE
AND
THE
GEOMETRY
CHOOSING
A
POINT
SOURCE
MODEL
A
POINT
SOURCE
AT
INFINITY
IS
A
GOOD
MODEL
FOR
THE
SUN
FOR
EXAMPLE
BECAUSE
THE
SOLID
ANGLE
THAT
THE
SUN
SUBTENDS
IS
SMALL
AND
ESSENTIALLY
CONSTANT
WHEREVER
IT
APPEARS
IN
THE
FIELD
OF
VIEW
THIS
TEST
MEANS
THAT
OUR
APPROXIMATION
STEP
IS
VALID
IF
WE
USE
LINEAR
SENSORS
WITH
AN
UNKNOWN
GAIN
FOR
EXAMPLE
WE
CAN
ROLL
THE
SOURCE
INTENSITY
AND
THE
UNKNOWN
GAIN
INTO
THE
SOURCE
VECTOR
TERM
THIS
IS
QUITE
OFTEN
DONE
WITHOUT
COMMENT
AS
YOU
SHOULD
EXPECT
FROM
THE
DERIVATION
A
POINT
SOURCE
AT
INFINITY
IS
A
POOR
MODEL
WHEN
THE
DISTANCE
BETWEEN
OBJECTS
IS
SIMILAR
IN
MAGNITUDE
TO
THE
DISTANCE
TO
THE
SOURCE
IN
THIS
CASE
WE
CANNOT
USE
THE
SERIES
APPROXIMATION
TO
PRETEND
THAT
THE
RADIOSITY
DUE
TO
THE
SOURCE
DOES
NOT
GO
DOWN
WITH
DISTANCE
TO
THE
SOURCE
THE
HEART
OF
THE
PROBLEM
IS
EASY
TO
SEE
IF
WE
CONSIDER
WHAT
THE
SOURCE
LOOKS
LIKE
FROM
DIFFERENT
SURFACE
PATCHES
IT
MUST
LOOK
BIGGER
TO
NEARER
SURFACE
PATCHES
HOWEVER
SMALL
ITS
RADIUS
THIS
MEANS
THAT
THE
RADIOSITY
DUE
TO
THE
SOURCE
MUST
GO
UP
IF
THE
SOURCE
IS
SUFFICIENTLY
DISTANT
FOR
EXAMPLE
THE
SUN
WE
CAN
IGNORE
THIS
EFFECT
BECAUSE
THE
SOURCE
DOES
NOT
CHANGE
IN
APPARENT
SIZE
FOR
ANY
PLAUSIBLE
MOTION
HOWEVER
FOR
CONFIGURATIONS
LIKE
A
LIGHT
BULB
IN
THE
CENTER
OF
A
ROOM
THE
SOLID
ANGLE
SUB
TENDED
BY
THE
SOURCE
GOES
UP
AS
THE
INVERSE
SQUARE
OF
THE
DISTANCE
MEANING
THAT
THE
RADIOSITY
SEC
SOURCES
AND
THEIR
EFFECTS
DUE
TO
THE
SOURCE
WILL
DO
SO
TOO
THE
CORRECT
MODEL
TO
USE
IN
THIS
CASE
IS
THE
POINT
SOURCE
OF
SECTION
THE
DIFFICULTY
WITH
THIS
MODEL
IS
THAT
RADIOSITY
CHANGES
SHARPLY
OVER
SPACE
IN
A
WAY
THAT
IS
INCONSISTENT
WITH
EXPERIENCE
FOR
EXAMPLE
IF
A
POINT
SOURCE
IS
PLACED
AT
THE
CENTER
OF
A
CUBE
THEN
THE
THE
MODEL
PREDICTS
THAT
RADIOSITY
IN
THE
CORNERS
IS
ROUGHLY
ONE
THIRD
THAT
AT
THE
CENTER
OF
EACH
FACE
BUT
THE
CORNERS
OF
REAL
ROOMS
ARE
NOWHERE
NEAR
AS
DARK
AS
THAT
IT
IS
QUITE
COMMON
IN
PRACTICE
TO
SUPPRESS
THE
DISTANCE
TERM
IN
THE
NEARBY
POINT
SOURCE
MODEL
TO
ACCOUNT
FOR
THIS
AN
ACTIVITY
THAT
IS
RADIOMETRICALLY
INCORRECT
BUT
THAT
TENDS
TO
YIELD
A
BETTER
MODEL
THE
EXPLANATION
OF
THIS
APPARENT
CONTRADICTION
MUST
WAIT
UNTIL
WE
HAVE
DISCUSSED
SHADING
MODELS
LINE
SOURCES
A
LINE
SOURCE
HAS
THE
GEOMETRY
OF
A
LINE
A
GOOD
EXAMPLE
IS
A
SINGLE
FLUORESCENT
LIGHT
BULB
LINE
SOURCES
ARE
NOT
TERRIBLY
COMMON
IN
NATURAL
SCENES
OR
IN
SYNTHETIC
ENVIRONMENTS
AND
WE
DISCUSS
THEM
ONLY
BRIEFLY
THEIR
MAIN
INTEREST
IS
AS
AN
EXAMPLE
FOR
RADIOMETRIC
PROBLEMS
IN
PARTICULAR
THE
RADIOSITY
OF
PATCHES
REASONABLY
CLOSE
TO
A
LINE
SOURCE
CHANGES
AS
THE
RECIPROCAL
OF
DISTANCE
TO
THE
SOURCE
RATHER
THAN
THE
SQUARE
OF
THE
DISTANCE
THE
REASONING
IS
MORE
INTERESTING
THAN
THE
EFFECT
WE
MODEL
A
LINE
SOURCE
AS
A
THIN
CYLINDER
WITH
DIAMETER
E
ASSUME
FOR
THE
MOMENT
THAT
THE
LINE
SOURCE
IS
INFINITELY
LONG
AND
THAT
WE
ARE
CONSIDERING
A
PATCH
THAT
VIEWS
THE
SOURCE
FRONTALLY
AS
IN
FIGURE
FIGURE
SKETCHES
THE
APPEARANCE
OF
THE
SOURCE
FROM
THE
POINT
OF
VIEW
OF
PATCH
NOW
MOVE
THE
PATCH
CLOSER
AND
CONSIDER
PATCH
THE
WIDTH
OF
THE
REGION
ON
THE
HEMISPHERE
CORRE
SPONDING
TO
THE
SOURCE
CHANGES
BUT
NOT
THE
LENGTH
BECAUSE
THE
SOURCE
IS
INFINITELY
LONG
IN
TURN
BECAUSE
THE
WIDTH
IS
APPROXIMATELY
E
R
THE
RADIOSITY
DUE
TO
THE
SOURCE
MUST
GO
DOWN
WITH
THE
RECIPROCAL
OF
DISTANCE
IT
IS
EASY
TO
SEE
THAT
WITH
A
SOURCE
THAT
IS
NOT
INFINITELY
LONG
THIS
APPLIES
AS
LONG
AS
THE
PATCH
IS
REASONABLY
CLOSE
AREA
SOURCES
AN
AREA
SOURCE
IS
AN
AREA
THAT
RADIATES
LIGHT
AREA
SOURCES
ARE
IMPORTANT
FOR
TWO
REASONS
FIRST
THEY
OCCUR
QUITE
COMMONLY
IN
NATURAL
SCENES
AN
OVERCAST
SKY
IS
A
GOOD
EXAMPLE
AND
IN
SYN
FIGURE
THE
RADIOSITY
DUE
TO
A
LINE
SOURCE
GOES
DOWN
AS
THE
RECIPROCAL
OF
DIS
TANCE
FOR
POINTS
THAT
ARE
REASONABLY
CLOSE
TO
THE
SOURCE
ON
THE
LEFT
TWO
PATCHES
VIEWING
AN
INFINITELY
LONG
NARROW
CYLINDER
WITH
CONSTANT
EXITANCE
ALONG
ITS
SUR
FACE
AND
DIAMETER
E
ON
THE
RIGHT
THE
VIEW
OF
THE
SOURCE
FROM
EACH
PATCH
DRAWN
AS
THE
UNDERSIDE
OF
THE
INPUT
HEMISPHERE
SEEN
FROM
BELOW
NOTICE
THAT
THE
LENGTH
OF
THE
SOURCE
ON
THIS
HEMISPHERE
DOES
NOT
CHANGE
BUT
THE
WIDTH
DOES
AS
E
R
THIS
YIELDS
THE
RESULT
SOURCES
SHADOWS
AND
SHADING
CHAP
THETIC
ENVIRONMENTS
FOR
EXAMPLE
THE
FLUORESCENT
LIGHT
BOXES
FOUND
IN
MANY
INDUSTRIAL
CEILINGS
SECOND
A
STUDY
OF
AREA
SOURCES
ALLOWS
US
TO
EXPLAIN
VARIOUS
SHADOWING
AND
INTERREFLECTION
EFFECTS
AREA
SOURCES
ARE
NORMALLY
MODELED
AS
SURFACE
PATCHES
WHOSE
EMITTED
RADIANCE
IS
INDEPENDENT
OF
POSITION
AND
OF
DIRECTION
THEY
CAN
BE
DESCRIBED
BY
THEIR
EXITANCE
AN
ARGUMENT
SIMILAR
TO
THAT
USED
FOR
LINE
SOURCES
SHOWS
THAT
FOR
POINTS
NOT
TOO
DISTANT
FROM
THE
SOURCE
THE
RADIOSITY
DUE
TO
AN
AREA
SOURCE
DOES
NOT
CHANGE
WITH
DISTANCE
TO
THE
SOURCE
THIS
IS
BECAUSE
IF
THE
AREA
IS
LARGE
ENOUGH
WITH
RESPECT
TO
THE
DISTANCE
TO
THE
SOURCE
THE
AREA
SUBTENDED
BY
THE
SOURCE
ON
SOME
INPUT
HEMISPHERE
IS
ABOUT
THE
SAME
AS
WE
MOVE
TOWARD
AND
AWAY
FROM
THE
SOURCE
THIS
EXPLAINS
THE
WIDESPREAD
USE
OF
AREA
SOURCES
IN
ILLUMINATION
ENGINEERING
THEY
GEN
ERALLY
YIELD
FAIRLY
UNIFORM
ILLUMINATION
FOR
OUR
APPLICATIONS
WE
NEED
A
MORE
EXACT
DESCRIPTION
OF
THE
RADIOSITY
DUE
TO
AN
AREA
SOURCE
SO
WE
NEED
TO
WRITE
OUT
THE
INTEGRAL
THE
EXACT
RADIOSITY
DUE
TO
AN
AREA
SOURCE
ASSUME
WE
HAVE
A
DIFFUSE
SURFACE
PATCH
THAT
IS
ILLUMINATED
BY
AN
AREA
SOURCE
WITH
EXITANCE
E
Q
AT
THE
SOURCE
POINT
Q
INSTEAD
OF
WRITING
ANGLES
IN
COORDINATES
WE
WRITE
Q
P
FOR
THE
DIRECTION
FROM
Q
TO
P
MORE
NOTATION
FIGURE
A
DIFFUSE
SOURCE
ILLUMINATES
A
DIFFUSE
SURFACE
THE
SOURCE
HAS
EXI
TANCE
E
Q
AND
WE
WISH
TO
COMPUTE
THE
RADIOSITY
ON
THE
PATCH
DUE
TO
THE
SOURCE
WE
DO
THIS
BY
TRANSFORMING
THE
INTEGRAL
OF
INCOMING
RADIANCE
AT
THE
SURFACE
INTO
AN
INTEGRAL
OVER
THE
SOURCE
AREA
THIS
TRANSFORMATION
IS
CONVENIENT
BECAUSE
IT
AVOIDS
US
HAVING
TO
USE
DIFFERENT
ANGULAR
DOMAINS
FOR
DIFFERENT
SURFACES
HOW
EVER
IT
STILL
LEADS
TO
AN
INTEGRAL
THAT
IS
USUALLY
IMPOSSIBLE
IN
CLOSED
FORM
SEC
LOCAL
SHADING
MODELS
IS
ILLUSTRATED
IN
FIGURE
THE
RADIOSITY
ON
THE
SURFACE
IS
OBTAINED
BY
SUMMING
THE
INCOMING
RADIANCE
OVER
ALL
INCOMING
DIRECTIONS
THIS
INTEGRAL
CAN
BE
TRANSFORMED
INTO
AN
INTEGRAL
OVER
THE
SOURCE
AS
FOLLOWS
B
P
ΡD
P
ΡD
P
ΡD
P
LI
P
Q
P
COS
ΘI
DΩ
LE
Q
Q
P
COS
ΘI
DΩ
Π
E
Q
COS
ΘI
DΩ
DAQ
ΡD
P
SOURCE
Π
E
Q
COS
ΘI
COS
ΘS
COS
Θ
COS
Θ
R
ΡD
P
SOURCE
E
Q
I
ΠR
DAQ
THE
TRANSFORMATION
WORKS
BECAUSE
RADIANCE
IS
CONSTANT
ALONG
STRAIGHT
LINES
AND
BECAUSE
E
Q
Π
LE
Q
IT
IS
USEFUL
BECAUSE
IT
MEANS
WE
DO
NOT
HAVE
TO
WORRY
ABOUT
CONSISTENT
ANGULAR
COORDINATE
SYSTEMS
HOWEVER
WE
TRANSFORM
THEM
INTEGRALS
DESCRIBING
THE
EFFECT
OF
AREA
SOURCES
ARE
GENERALLY
DIFFICULT
OR
IMPOSSIBLE
TO
DO
IN
CLOSED
FORM
LOCAL
SHADING
MODELS
WE
HAVE
STUDIED
THE
PHYSICS
OF
LIGHT
BECAUSE
WE
WANT
TO
KNOW
HOW
BRIGHT
THINGS
WILL
BE
AND
WHY
IN
THE
HOPE
OF
EXTRACTING
OBJECT
INFORMATION
FROM
THESE
MODELS
CURRENTLY
WE
KNOW
THE
RADIOSITY
AT
A
PATCH
DUE
TO
A
SOURCE
BUT
THIS
IS
NOT
A
SHADING
MODEL
RADIANCE
COULD
ARRIVE
AT
SURFACE
PATCHES
IN
OTHER
WAYS
E
G
IT
COULD
BE
REFLECTED
FROM
OTHER
SURFACE
PATCHES
WE
NEED
TO
KNOW
WHICH
COMPONENTS
TO
ACCOUNT
FOR
THE
EASIEST
MODEL
TO
MANIPULATE
IS
A
LOCAL
SHADING
MODEL
WHICH
MODELS
THE
RADIOSITY
AT
A
SURFACE
PATCH
AS
THE
SUM
OF
THE
RADIOSITY
DUE
ONLY
TO
LIGHT
INTERNALLY
GENERATED
AT
SOURCES
THIS
MEANS
THAT
WE
ASSUME
THAT
LIGHT
IS
NOT
REFLECTED
FROM
SURFACE
TO
SURFACE
BUT
INSTEAD
LEAVES
A
SOURCE
ARRIVES
AT
SOME
SURFACE
AND
PROCEEDS
DIRECTLY
TO
THE
CAMERA
THIS
MODEL
IS
PALPABLY
UNPHYSICAL
BUT
IS
EASY
TO
ANALYZE
THE
MODEL
SUPPORTS
A
VARIETY
OF
ALGORITHMS
AND
THEORIES
SEE
SECTION
UNFORTUNATELY
THIS
MODEL
OFTEN
PRODUCES
WILDLY
INACCURATE
PREDICTIONS
EVEN
WORSE
THERE
IS
LITTLE
RELIABLE
INFORMATION
ABOUT
WHEN
IT
IS
SAFE
TO
USE
THIS
MODEL
AN
ALTERNATE
MODEL
IS
TO
ACCOUNT
FOR
ALL
RADIATION
SECTION
THIS
TAKES
INTO
ACCOUNT
RADIANCE
ARRIVING
FROM
SOURCES
AND
THAT
ARRIVING
FROM
RADIATING
SURFACES
THIS
MODEL
IS
PHYSICALLY
ACCURATE
BUT
USUALLY
HARD
TO
MANIPULATE
LOCAL
SHADING
MODELS
FOR
POINT
SOURCES
THE
LOCAL
SHADING
MODEL
FOR
A
SET
OF
POINT
SOURCES
IS
OBTAINED
BY
WRITING
OUT
THE
RADIOSITY
DUE
TO
LIGHT
INTERNALLY
GENERATED
AT
SOURCES
THIS
GIVES
B
P
SOURCES
VISIBLE
FROM
P
BS
P
WHERE
BS
P
IS
THE
RADIOSITY
DUE
TO
SOURCE
THIS
EXPRESSION
IS
FAIRLY
INNOCUOUS
BUT
NOTICE
THAT
IF
ALL
THE
SOURCES
ARE
POINT
SOURCES
AT
INFINITY
THE
EXPRESSION
BECOMES
SOURCES
SHADOWS
AND
SHADING
CHAP
B
P
SOURCES
VISIBLE
FROM
P
ΡD
P
N
P
SS
SO
THAT
IF
WE
CONFINE
OUR
ATTENTION
TO
A
REGION
WHERE
ALL
POINTS
CAN
SEE
THE
SAME
SOURCES
WE
COULD
ADD
ALL
THE
SOURCE
VECTORS
TO
OBTAIN
A
SINGLE
VIRTUAL
SOURCE
THAT
HAD
THE
SAME
EFFECTS
THE
RELATIONSHIP
BETWEEN
SHAPE
AND
SHADING
IS
PRETTY
DIRECT
HERE
THE
RADIOSITY
IS
A
MEASUREMENT
OF
ONE
COMPONENT
OF
THE
SURFACE
NORMAL
FOR
POINT
SOURCES
THAT
ARE
NOT
AT
INFINITY
THE
MODEL
BECOMES
B
P
Ρ
P
N
P
P
D
SOURCES
VISIBLE
FROM
P
RS
P
WHERE
RS
P
IS
THE
DISTANCE
FROM
THE
SOURCE
TO
P
THE
PRESENCE
OF
THIS
TERM
MEANS
THAT
THE
RELATIONSHIP
BETWEEN
SHAPE
AND
SHADING
IS
SOMEWHAT
MORE
OBSCURE
THE
APPEARANCE
OF
SHADOWS
IN
A
LOCAL
SHADING
MODEL
SHADOWS
OCCUR
WHEN
THE
PATCH
CAN
NOT
SEE
ONE
OR
MORE
SOURCES
IN
THIS
MODEL
POINT
SOURCES
PRODUCE
A
SERIES
OF
SHAD
OWS
WITH
CRISP
BOUNDARIES
SHADOW
REGIONS
WHERE
NO
SOURCE
CAN
BE
SEEN
ARE
PARTICULARLY
DARK
SHADOWS
CAST
WITH
A
SINGLE
SOURCE
CAN
BE
CRISP
AND
BLACK
DEPENDING
ON
THE
SIZE
OF
THE
SOURCE
AND
THE
ALBEDO
OF
OTHER
NEARBY
SURFACES
WHICH
COULD
REFLECT
LIGHT
INTO
THE
SHADOW
AND
SOFTEN
ITS
BOUNDARY
IT
WAS
A
POPULAR
CENTURY
PASTIME
TO
CAST
SUCH
SHADOWS
ONTO
PAPER
AND
THEN
DRAW
THEM
YIELDING
THE
SILHOUETTES
STILL
OCCASIONALLY
FOUND
IN
ANTIQUES
SHOPS
THE
GEOMETRY
OF
THE
SHADOW
CAST
BY
A
POINT
SOURCE
ON
A
PLANE
IS
ANALOGOUS
TO
THE
GEOMETRY
OF
VIEWING
IN
A
PERSPECTIVE
CAMERA
FIGURE
ANY
PATCH
ON
THE
PLANE
IS
IN
SHADOW
IF
A
RAY
FROM
THE
PATCH
TO
THE
SOURCE
PASSES
THROUGH
AN
OBJECT
THIS
MEANS
THAT
THERE
ARE
TWO
KINDS
OF
SHADOW
BOUNDARY
AT
SELF
SHADOW
BOUNDARIES
THE
SURFACE
IS
TURNING
AWAY
FROM
THE
LIGHT
AND
A
RAY
FROM
THE
PATCH
TO
THE
SOURCE
IS
TANGENT
TO
THE
SURFACE
AT
CAST
SHADOW
BOUNDARIES
FROM
THE
PERSPECTIVE
OF
THE
PATCH
THE
SOURCE
SUDDENLY
DISAPPEARS
BEHIND
AN
OCCLUDING
OBJECT
SHADOWS
CAST
ONTO
CURVED
SURFACES
CAN
HAVE
EXTREMELY
COMPLEX
GEOMETRIES
HOWEVER
FIGURE
SHADOWS
CAST
BY
POINT
SOURCES
ON
PLANES
ARE
RELATIVELY
SIMPLE
SELF
SHADOW
BOUNDARIES
OCCUR
WHEN
THE
SURFACE
TURNS
AWAY
FROM
THE
LIGHT
AND
CAST
SHADOW
BOUNDARIES
OCCUR
WHEN
A
DISTANT
SURFACE
OCCLUDES
THE
VIEW
OF
THE
SOURCE
SEC
LOCAL
SHADING
MODELS
IF
THERE
ARE
MANY
SOURCES
THE
SHADOWS
ARE
LESS
DARK
EXCEPT
AT
POINTS
WHERE
NO
SOURCE
IS
VISIBLE
AND
THERE
CAN
BE
MANY
QUALITATIVELY
DISTINCT
SHADOW
REGIONS
EACH
SOURCE
CASTS
ITS
OWN
SHADOW
SOME
POINTS
MAY
NOT
SEE
MORE
THAN
ONE
SOURCE
ONE
EXAMPLE
OF
THIS
EFFECT
OCCURS
IN
TELEVISED
SOCCER
MATCHES
BECAUSE
THE
STADIUM
HAS
MULTIPLE
BRIGHT
DISTANT
POINT
LIKE
ILLUMINANTS
SPACED
EVENLY
AROUND
THE
PERIMETER
OF
THE
STADIUM
THERE
IS
A
SET
OF
SHADOWS
RADIATING
EVENLY
AROUND
EACH
PLAYER
FEET
THESE
SHADOWS
TYPICALLY
BECOME
BRIGHTER
OR
DARKER
AS
THE
PLAYER
MOVES
AROUND
USUALLY
BECAUSE
THE
ILLUMINATION
DUE
TO
OTHER
SOURCES
AND
TO
INTERREFLECTIONS
IN
THE
REGION
OF
THE
SHADOW
INCREASES
OR
DECREASES
AREA
SOURCES
AND
THEIR
SHADOWS
THE
LOCAL
SHADING
MODEL
FOR
A
SET
OF
AREA
SOURCES
IS
SIGNIFICANTLY
MORE
COMPLEX
BECAUSE
IT
IS
POSSIBLE
FOR
PATCHES
TO
SEE
ONLY
A
PORTION
OF
A
GIVEN
SOURCE
THE
MODEL
BECOMES
B
P
ALL
SOURCES
R
VISIBLE
COMPONENT
OF
SOURCE
RADIOSITY
DUE
TO
SOURCEL
E
Q
COS
ΘQ
COS
ΘS
DAQ
USING
THE
TERMINOLOGY
OF
FIGURE
USUALLY
WE
ASSUME
THAT
E
IS
CONSTANT
OVER
THE
SOURCE
AREA
SOURCES
DO
NOT
PRODUCE
DARK
SHADOWS
WITH
CRISP
BOUNDARIES
THIS
IS
BECAUSE
FROM
THE
PERSPECTIVE
OF
A
VIEWING
PATCH
THE
SOURCE
APPEARS
SLOWLY
FROM
BEHIND
THE
OCCLUDING
OBJECT
THINK
OF
AN
ECLIPSE
OF
THE
MOON
IT
IS
AN
EXACT
ANALOGY
IT
IS
COMMON
TO
DISTINGUISH
BETWEEN
POINTS
IN
THE
UMBRA
A
LATIN
WORD
MEANING
SHADOW
WHICH
CANNOT
SEE
THE
SOURCE
AT
ALL
AND
POINTS
IN
THE
PENUMBRA
A
COMPOUND
OF
LATIN
WORDS
MEANING
ALMOST
SHADOW
WHICH
SEE
PART
OF
THE
SOURCE
THE
VAST
MAJORITY
OF
INDOOR
SOURCES
ARE
AREA
SOURCES
OF
ONE
FORM
OR
ANOTHER
SO
THE
EFFECTS
ARE
QUITE
EASY
TO
SEE
HOLD
AN
ARM
QUITE
CLOSE
TO
THE
WALL
AND
LOOK
AT
THE
SHADOW
IT
CASTS
THERE
IS
A
DARK
CORE
WHICH
GETS
LARGER
AS
THE
ARM
GETS
CLOSER
TO
THE
WALL
THIS
IS
THE
UMBRA
SURROUNDED
BY
A
LIGHTER
REGION
WITH
A
FUZZIER
BOUNDARY
THE
PENUMBRA
FIGURE
ILLUSTRATES
THE
GEOMETRY
AMBIENT
ILLUMINATION
ONE
PROBLEM
WITH
LOCAL
SHADING
MODELS
SHOULD
BE
APPARENT
IMMEDIATELY
THEY
PREDICT
THAT
SOME
SHADOW
REGIONS
ARE
ARBITRARILY
DARK
BECAUSE
THEY
CANNOT
SEE
THE
SOURCE
THIS
PREDICTION
IS
INAC
CURATE
IN
ALMOST
EVERY
CASE
BECAUSE
SHADOWS
ARE
ILLUMINATED
BY
LIGHT
FROM
OTHER
DIFFUSE
SURFACES
THIS
EFFECT
CAN
BE
SIGNIFICANT
IN
ROOMS
WITH
LIGHT
WALLS
AND
AREA
SOURCES
IT
IS
POSSIBLE
TO
SEE
SHADOWS
ONLY
BY
HOLDING
OBJECTS
CLOSE
TO
THE
WALL
OR
CLOSE
TO
THE
SOURCE
THIS
IS
BECAUSE
A
PATCH
ON
THE
WALL
SEES
ALL
THE
OTHER
WALLS
IN
THE
ROOM
UNTIL
AN
OBJECT
IS
CLOSE
TO
THE
WALL
IT
BLOCKS
OUT
ONLY
A
SMALL
FRACTION
OF
THE
VISUAL
HEMISPHERE
OF
EACH
PATCH
FOR
SOME
ENVIRONMENTS
THE
TOTAL
IRRADIANCE
A
PATCH
OBTAINS
FROM
OTHER
PATCHES
IS
ROUGHLY
CONSTANT
AND
ROUGHLY
UNIFORMLY
DISTRIBUTED
ACROSS
THE
INPUT
HEMISPHERE
THIS
MUST
BE
TRUE
FOR
THE
INTERIOR
OF
A
SPHERE
WITH
A
CONSTANT
DISTRIBUTION
OF
RADIOSITY
BY
SYMMETRY
AND
BY
ACCEPTING
A
MODEL
OF
A
CUBE
AS
A
SPHERE
IS
ROUGHLY
TRUE
FOR
THE
INTERIOR
OF
A
ROOM
WITH
WHITE
WALLS
IN
SUCH
AN
ENVIRONMENT
IT
IS
SOMETIMES
POSSIBLE
TO
MODEL
THE
EFFECT
OF
OTHER
PATCHES
BY
ADDING
AN
AMBIENT
ILLUMINATION
TERM
TO
EACH
PATCH
RADIOSITY
THERE
ARE
TWO
STRATEGIES
FOR
DETERMINING
THIS
TERM
FIRST
IF
EACH
PATCH
SEES
THE
SAME
PROPORTION
OF
THE
WORLD
E
G
THE
INTERIOR
OF
A
SPHERE
WE
CAN
ADD
THE
SAME
CONSTANT
TERM
TO
THE
RADIOSITY
OF
EACH
PATCH
THE
MAGNITUDE
OF
THIS
TERM
IS
USUALLY
GUESSED
SOURCES
SHADOWS
AND
SHADING
CHAP
AREA
SOURCE
OCCLUDER
FIGURE
AREA
SOURCES
GENERATE
COMPLEX
SHADOWS
WITH
SMOOTH
BOUNDARIES
BECAUSE
FROM
THE
POINT
OF
VIEW
OF
A
SURFACE
PATCH
THE
SOURCE
DISAPPEARS
SLOWLY
BEHIND
THE
OCCLUDER
REGIONS
WHERE
THE
SOURCE
CANNOT
BE
SEEN
AT
ALL
ARE
KNOWN
AS
THE
UMBRA
REGIONS
WHERE
SOME
PORTION
OF
THE
SOURCE
IS
VISIBLE
ARE
KNOWN
AS
THE
PENUMBRA
A
GOOD
MODEL
IS
TO
IMAGINE
LYING
WITH
YOUR
BACK
TO
THE
SURFACE
LOOKING
AT
THE
WORLD
ABOVE
AT
POINT
YOU
CAN
SEE
ALL
OF
THE
SOURCE
AT
POINT
YOU
CAN
SEE
SOME
OF
IT
AND
AT
POINT
YOU
CAN
SEE
NONE
OF
IT
SECOND
IF
SOME
PATCHES
SEE
MORE
OR
LESS
OF
THE
WORLD
THAN
OTHERS
THIS
HAPPENS
IF
REGIONS
OF
THE
WORLD
OCCLUDE
A
PATCH
VIEW
E
G
A
PATCH
AT
THE
BOTTOM
OF
A
GROOVE
THIS
CAN
BE
TAKEN
INTO
ACCOUNT
TO
DO
SO
WE
NEED
A
MODEL
OF
THE
WORLD
FROM
THE
PERSPECTIVE
OF
THE
PATCH
UNDER
CONSIDERATION
A
NATURAL
STRATEGY
IS
TO
MODEL
THE
WORLD
AS
A
LARGE
DISTANT
POLYGON
OF
CONSTANT
RADIOSITY
WHERE
THE
VIEW
OF
THIS
POLYGON
IS
OCCLUDED
AT
SOME
PATCHES
SEE
FIGURE
THE
RESULT
IS
THAT
THE
AMBIENT
TERM
IS
SMALLER
FOR
PATCHES
THAT
SEE
LESS
OF
THE
WORLD
THIS
MODEL
IS
OFTEN
MORE
ACCURATE
THAN
ADDING
A
CONSTANT
AMBIENT
TERM
UNFORTUNATELY
IT
IS
MUCH
MORE
DIFFICULT
TO
EXTRACT
INFORMATION
FROM
THIS
MODEL
POSSIBLY
AS
DIFFICULT
AS
FOR
A
GLOBAL
SHADING
MODEL
APPLICATION
PHOTOMETRIC
STEREO
WE
RECONSTRUCT
A
PATCH
OF
SURFACE
FROM
A
SERIES
OF
PICTURES
OF
THE
SURFACE
TAKEN
UNDER
DIFFER
ENT
ILLUMINANTS
FOR
SIMPLICITY
WE
USE
AN
ORTHOGRAPHIC
CAMERA
AND
CHOOSE
A
COORDINATE
SYSTEM
SUCH
THAT
THE
POINT
X
Y
Z
IN
SPACE
PROJECTS
ONTO
THE
POINT
X
Y
IN
THE
IMAGE
THE
METHOD
WE
DESCRIBE
WORKS
FOR
THE
OTHER
CAMERA
MODELS
DESCRIBED
IN
CHAPTER
IN
THIS
CASE
TO
MEASURE
THE
SHAPE
OF
THE
SURFACE
WE
NEED
TO
OBTAIN
THE
DEPTH
TO
THE
SURFACE
THIS
SUGGESTS
REPRESENTING
THE
SURFACE
AS
X
Y
F
X
Y
A
REPRESENTATION
KNOWN
AS
A
MONGE
PATCH
AFTER
A
FRENCH
MILITARY
ENGINEER
WHO
FIRST
USED
IT
FIGURE
THIS
REPRESENTATION
IS
ATTRAC
TIVE
BECAUSE
WE
CAN
DETERMINE
A
UNIQUE
POINT
ON
THE
SURFACE
BY
GIVING
THE
IMAGE
COORDINATES
NOTICE
THAT
TO
OBTAIN
A
MEASUREMENT
OF
A
SOLID
OBJECT
WE
WOULD
NEED
TO
RECONSTRUCT
MORE
THAN
ONE
PATCH
BECAUSE
WE
NEED
TO
OBSERVE
THE
BACK
OF
THE
OBJECT
PHOTOMETRIC
STEREO
IS
A
METHOD
FOR
RECOVERING
A
REPRESENTATION
OF
THE
MONGE
PATCH
FROM
IMAGE
DATA
THE
METHOD
INVOLVES
REASONING
ABOUT
THE
IMAGE
INTENSITY
VALUES
FOR
SEVERAL
SEC
APPLICATION
PHOTOMETRIC
STEREO
VIEW
FROM
VIEW
FROM
FIGURE
AMBIENT
ILLUMINATION
IS
A
TERM
ADDED
TO
THE
RADIOSITY
PREDICTIONS
OF
LOCAL
SHADING
MODELS
TO
MODEL
THE
EFFECTS
OF
RADIOSITY
FROM
DISTANT
REFLECTING
SURFACES
IN
A
WORLD
LIKE
THE
INTERIOR
OF
A
SPHERE
OR
OF
A
CUBE
THE
CASE
ON
THE
LEFT
WHERE
A
PATCH
SEES
ROUGHLY
THE
SAME
THING
FROM
EACH
POINT
A
CONSTANT
AMBIENT
ILLUMINATION
TERM
IS
OFTEN
ACCEPTABLE
IN
MORE
COMPLEX
WORLDS
SOME
SURFACE
PATCHES
SEE
MUCH
LESS
OF
THE
SURROUNDING
WORLD
THAN
OTHERS
FOR
EXAMPLE
THE
PATCH
AT
THE
BASE
OF
THE
GROOVE
ON
THE
RIGHT
SEES
RELATIVELY
LITTLE
OF
THE
OUTSIDE
WORLD
WHICH
WE
MODEL
AS
AN
INFINITE
POLYGON
OF
CONSTANT
EXITANCE
ITS
INPUT
HEMISPHERE
IS
SHOWN
BELOW
DIRECTION
OF
PROJECTION
HEIGHT
Y
X
FIGURE
A
MONGE
PATCH
IS
A
REPRESENTATION
OF
A
PIECE
OF
SURFACE
AS
A
HEIGHT
FUNCTION
FOR
THE
PHOTOMETRIC
STEREO
EXAMPLE
WE
ASSUME
THAT
AN
ORTHOGRAPHIC
CAMERA
ONE
THAT
MAPS
X
Y
Z
IN
SPACE
TO
X
Y
IN
THE
CAMERA
IS
VIEWING
A
MONGE
PATCH
THIS
MEANS
THAT
THE
SHAPE
OF
THE
SURFACE
CAN
BE
REPRESENTED
AS
A
FUNCTION
OF
POSITION
IN
THE
IMAGE
SOURCES
SHADOWS
AND
SHADING
CHAP
DIFFERENT
IMAGES
OF
A
SURFACE
IN
A
FIXED
VIEW
ILLUMINATED
BY
DIFFERENT
SOURCES
THIS
METHOD
RE
COVERS
THE
HEIGHT
OF
THE
SURFACE
AT
POINTS
CORRESPONDING
TO
EACH
PIXEL
IN
COMPUTER
VISION
CIRCLES
THE
RESULTING
REPRESENTATION
IS
OFTEN
KNOWN
AS
A
HEIGHT
MAP
DEPTH
MAP
OR
DENSE
DEPTH
MAP
FIX
THE
CAMERA
AND
THE
SURFACE
IN
POSITION
AND
ILLUMINATE
THE
SURFACE
USING
A
POINT
SOURCE
THAT
IS
FAR
AWAY
COMPARED
WITH
THE
SIZE
OF
THE
SURFACE
WE
ADOPT
A
LOCAL
SHADING
MODEL
AND
ASSUME
THAT
THERE
IS
NO
AMBIENT
ILLUMINATION
MORE
ABOUT
THIS
LATER
SO
THAT
THE
RADIOSITY
AT
A
POINT
P
ON
THE
SURFACE
IS
B
P
Ρ
P
N
P
WHERE
N
IS
THE
UNIT
SURFACE
NORMAL
AND
IS
THE
SOURCE
VECTOR
WITH
OUR
CAMERA
MODEL
THERE
IS
ONLY
ONE
POINT
P
ON
THE
SURFACE
FOR
EACH
POINT
X
Y
IN
THE
IMAGE
AND
WE
CAN
WRITE
B
X
Y
FOR
B
P
NOW
WE
ASSUME
THAT
THE
RESPONSE
OF
THE
CAMERA
IS
LINEAR
IN
THE
SURFACE
RADIOSITY
SO
THE
VALUE
OF
A
PIXEL
AT
X
Y
IS
I
X
Y
KB
X
Y
KΡ
X
Y
N
X
Y
G
X
Y
WHERE
K
IS
THE
CONSTANT
CONNECTING
THE
CAMERA
RESPONSE
TO
THE
INPUT
RADIANCE
G
X
Y
Ρ
X
Y
N
X
Y
AND
IN
THESE
EQUATIONS
G
X
Y
DESCRIBES
THE
SURFACE
AND
IS
A
PROPERTY
OF
THE
ILLUMINATION
AND
OF
THE
CAMERA
WE
HAVE
A
DOT
PRODUCT
BETWEEN
A
VECTOR
FIELD
G
X
Y
AND
A
VECTOR
WHICH
COULD
BE
MEASURED
WITH
ENOUGH
OF
THESE
DOT
PRODUCTS
WE
COULD
RECONSTRUCT
G
AND
SO
THE
SURFACE
NORMAL
AND
ALBEDO
FROM
MANY
VIEWS
NOW
IF
WE
HAVE
N
SOURCES
FOR
EACH
OF
WHICH
VI
IS
KNOWN
WE
STACK
EACH
OF
THESE
VI
INTO
A
KNOWN
MATRIX
V
WHERE
T
V
FOR
EACH
IMAGE
POINT
WE
STACK
THE
MEASUREMENTS
INTO
A
VECTOR
I
X
Y
X
Y
X
Y
IN
X
Y
T
NOTICE
THAT
WE
HAVE
ONE
VECTOR
PER
IMAGE
POINT
EACH
VECTOR
CONTAINS
ALL
THE
IMAGE
BRIGHTNESSES
OBSERVED
AT
THAT
POINT
FOR
DIFFERENT
SOURCES
NOW
WE
HAVE
I
X
Y
VG
X
Y
AND
G
IS
OBTAINED
BY
SOLVING
THIS
LINEAR
SYSTEM
OR
RATHER
ONE
LINEAR
SYSTEM
PER
POINT
IN
THE
IMAGE
TYPICALLY
N
SO
THAT
A
LEAST
SQUARES
SOLUTION
IS
APPROPRIATE
THIS
HAS
THE
ADVANTAGE
THAT
THE
RESIDUAL
ERROR
IN
THE
SOLUTION
PROVIDES
A
CHECK
ON
OUR
MEASUREMENTS
THE
DIFFICULTY
WITH
THIS
APPROACH
IS
THAT
SUBSTANTIAL
REGIONS
OF
THE
SURFACE
MAY
BE
IN
SHADOW
FOR
ONE
OR
THE
OTHER
LIGHT
SEE
FIGURE
THERE
IS
A
SIMPLE
TRICK
THAT
DEALS
WITH
SHADOWS
IF
THERE
REALLY
IS
NO
AMBIENT
ILLUMINATION
THEN
WE
CAN
FORM
A
MATRIX
FROM
THE
IMAGE
VECTOR
AND
MULTIPLY
BOTH
SIDES
BY
THIS
MATRIX
THIS
ZEROES
OUT
ANY
EQUATIONS
FROM
POINTS
THAT
ARE
IN
SHADOW
SEC
APPLICATION
PHOTOMETRIC
STEREO
FIGURE
FIVE
SYNTHETIC
IMAGES
OF
A
SPHERE
ALL
OBTAINED
IN
AN
ORTHOGRAPHIC
VIEW
FROM
THE
SAME
VIEWING
POSITION
THESE
IMAGES
ARE
SHADED
USING
A
LOCAL
SHADING
MODEL
AND
A
DISTANT
POINT
SOURCE
THIS
IS
A
CONVEX
OBJECT
SO
THE
ONLY
VIEW
WHERE
THERE
IS
NO
VISIBLE
SHADOW
OCCURS
WHEN
THE
SOURCE
DIRECTION
IS
PARAL
LEL
TO
THE
VIEWING
DIRECTION
THE
VARIATIONS
IN
BRIGHTNESS
OCCURING
UNDER
DIFFERENT
SOURCES
CODE
THE
SHAPE
OF
THE
SURFACE
WE
FORM
X
Y
I
X
Y
AND
IN
X
Y
II
IVG
X
Y
AND
HAS
THE
EFFECT
OF
ZEROING
THE
CONTRIBUTIONS
FROM
SHADOWED
REGIONS
BECAUSE
THE
RELEVANT
ELEMENTS
OF
THE
MATRIX
ARE
ZERO
AT
POINTS
THAT
ARE
IN
SHADOW
AGAIN
THERE
IS
ONE
LINEAR
SYSTEM
PER
POINT
IN
THE
IMAGE
AT
EACH
POINT
WE
SOLVE
THIS
LINEAR
SYSTEM
TO
RECOVER
THE
G
VECTOR
AT
THAT
POINT
MEASURING
ALBEDO
WE
CAN
EXTRACT
THE
ALBEDO
FROM
A
MEASUREMENT
OF
G
BECAUSE
N
IS
THE
UNIT
NORMAL
THIS
MEANS
THAT
G
X
Y
Ρ
X
Y
THIS
PROVIDES
A
CHECK
ON
OUR
MEASUREMENTS
AS
WELL
BECAUSE
THE
ALBEDO
IS
IN
THE
RANGE
ZERO
TO
ONE
ANY
PIXELS
WHERE
G
IS
GREATER
THAN
ONE
ARE
SUSPECT
EITHER
THE
PIXEL
IS
NOT
WORKING
OR
IS
INCORRECT
FIGURE
SHOWS
ALBEDO
RECOVERED
USING
THIS
METHOD
FOR
THE
IMAGES
OF
FIGURE
RECOVERING
NORMALS
WE
CAN
EXTRACT
THE
SURFACE
NORMAL
FROM
G
BECAUSE
THE
NORMAL
IS
A
UNIT
VECTOR
N
X
Y
G
X
Y
G
X
Y
FIGURE
SHOWS
NORMAL
VALUES
RECOVERED
FOR
THE
IMAGES
OF
FIGURE
SOURCES
SHADOWS
AND
SHADING
CHAP
FIGURE
THE
MAGNITUDE
OF
THE
VECTOR
FIELD
G
X
Y
RECOVERED
FROM
THE
IN
PUT
DATA
OF
FIGURE
REPRESENTED
AS
AN
IMAGE
THIS
IS
THE
REFLECTANCE
OF
THE
SURFACE
SHAPE
FROM
NORMALS
THE
SURFACE
IS
X
Y
F
X
Y
SO
THE
NORMAL
AS
A
FUNCTION
OF
X
Y
IS
F
F
T
N
X
Y
I
F
F
X
Y
TO
RECOVER
THE
DEPTH
MAP
WE
NEED
TO
DETERMINE
F
X
Y
FROM
MEASURED
VALUES
OF
THE
UNIT
NORMAL
ASSUME
THAT
THE
MEASURED
VALUE
OF
THE
UNIT
NORMAL
AT
SOME
POINT
X
Y
IS
A
X
Y
B
X
Y
C
X
Y
THEN
F
A
X
Y
F
B
X
Y
X
C
X
Y
AND
Y
C
X
Y
FIGURE
THE
NORMAL
FIELD
RECOVERED
FROM
THE
INPUT
DATA
OF
FIGURE
SEC
APPLICATION
PHOTOMETRIC
STEREO
WE
HAVE
ANOTHER
CHECK
ON
OUR
DATA
SET
BECAUSE
F
F
SO
WE
EXPECT
THAT
X
Y
Y
X
A
X
Y
C
X
Y
Y
B
X
Y
C
X
Y
X
SHOULD
BE
SMALL
AT
EACH
POINT
IN
PRINCIPLE
IT
SHOULD
BE
ZERO
BUT
WE
WOULD
HAVE
TO
ESTIMATE
THESE
PARTIAL
DERIVATIVES
NUMERICALLY
AND
SO
SHOULD
BE
WILLING
TO
ACCEPT
SMALL
VALUES
THIS
TEST
IS
KNOWN
AS
A
TEST
OF
INTEGRABILITY
WHICH
IN
VISION
APPLICATIONS
ALWAYS
BOILS
DOWN
TO
CHECKING
THAT
MIXED
SECOND
PARTIALS
ARE
EQUAL
ALGORITHM
PHOTOMETRIC
STEREO
OBTAIN
MANY
IMAGES
IN
A
FIXED
VIEW
UNDER
DIFFERENT
ILLUMINANTS
DETERMINE
THE
MATRIX
FROM
SOURCE
AND
CAMERA
INFORMATION
CREATE
ARRAYS
FOR
ALBEDO
NORMAL
COMPONENTS
P
MEASURED
VALUE
OF
F
AND
Q
MEASURED
VALUE
OF
F
FOR
EACH
POINT
IN
THE
IMAGE
ARRAY
STACK
IMAGE
VALUES
INTO
A
VECTOR
I
CONSTRUCT
THE
DIAGONAL
MATRIX
I
SOLVE
IVG
II
TO
OBTAIN
G
FOR
THIS
POINT
ALBEDO
AT
THIS
POINT
IS
G
NORMAL
AT
THIS
POINT
IS
P
AT
THIS
POINT
IS
Q
AT
THIS
POINT
IS
G
END
CHECK
IS
P
Q
SMALL
EVERYWHERE
TOP
LEFT
CORNER
OF
HEIGHT
MAP
IS
ZERO
FOR
EACH
PIXEL
IN
THE
LEFT
COLUMN
OF
HEIGHT
MAP
HEIGHT
VALUE
PREVIOUS
HEIGHT
VALUE
CORRESPONDING
Q
VALUE
END
FOR
EACH
ROW
FOR
EACH
ELEMENT
OF
THE
ROW
EXCEPT
FOR
LEFTMOST
HEIGHT
VALUE
PREVIOUS
HEIGHT
VALUE
CORRESPONDING
P
VALUE
END
END
SHAPE
BY
INTEGRATION
ASSUMING
THAT
THE
PARTIAL
DERIVATIVES
PASS
THIS
SANITY
TEST
WE
CAN
RECONSTRUCT
THE
SURFACE
UP
TO
SOME
CONSTANT
DEPTH
ERROR
THE
PARTIAL
DERIVATIVE
GIVES
THE
CHANGE
IN
SURFACE
HEIGHT
WITH
A
SMALL
STEP
IN
EITHER
THE
X
OR
THE
Y
DIRECTION
THIS
MEANS
WE
CAN
SOURCES
SHADOWS
AND
SHADING
CHAP
FIGURE
THE
HEIGHT
FIELD
OBTAINED
BY
INTEGRATING
THE
NORMAL
FIELD
OF
FIGURE
USING
THE
METHOD
DESCRIBED
IN
THE
TEXT
GET
THE
SURFACE
BY
SUMMING
THESE
CHANGES
IN
HEIGHT
ALONG
SOME
PATH
IN
PARTICULAR
WE
HAVE
F
F
F
X
Y
X
Y
DL
C
WHERE
C
IS
A
CURVE
STARTING
AT
SOME
FIXED
POINT
AND
ENDING
AT
X
Y
AND
C
IS
A
CONSTANT
OF
INTEGRATION
WHICH
REPRESENTS
THE
UNKNOWN
HEIGHT
OF
THE
SURFACE
AT
THE
START
POINT
THE
RECOVERED
SURFACE
DOES
NOT
DEPEND
ON
THE
CHOICE
OF
CURVE
EXERCISES
FOR
EXAMPLE
WE
CAN
RECONSTRUCT
THE
SURFACE
AT
U
V
BY
STARTING
AT
SUMMING
THE
Y
DERIVATIVE
ALONG
THE
LINE
X
TO
THE
POINT
V
AND
THEN
SUMMING
THE
X
DERIVATIVE
ALONG
THE
LINE
Y
V
TO
THE
POINT
U
V
F
U
V
V
F
Y
DY
U
F
X
X
V
DX
C
THIS
IS
THE
INTEGRATION
PATH
GIVEN
IN
ALGORITHM
ANY
OTHER
SET
OF
PATHS
WOULD
WORK
AS
WELL
ALTHOUGH
IT
IS
PROBABLY
BEST
TO
USE
MANY
DIFFERENT
PATHS
AND
AVERAGE
SO
AS
TO
SPREAD
AROUND
THE
ERROR
IN
THE
DERIVATIVE
ESTIMATES
FIGURE
SHOWS
THE
RECONSTRUCTION
OBTAINED
FOR
THE
DATA
OF
FIGURE
ANOTHER
APPROACH
TO
RECOVERING
SHAPE
IS
TO
CHOOSE
THE
FUNCTION
F
X
Y
WHOSE
PARTIAL
DERIVATIVES
MOST
LOOK
LIKE
THE
MEASURED
PARTIAL
DERIVATIVES
WE
EXPLORE
THIS
APPROACH
FOR
A
SIMILAR
PROBLEM
IN
SECTION
INTERREFLECTIONS
GLOBAL
SHADING
MODELS
LOCAL
SHADING
MODELS
CAN
BE
QUITE
MISLEADING
IN
THE
REAL
WORLD
EACH
SURFACE
PATCH
IS
ILLUMINATED
NOT
ONLY
BY
SOURCES
BUT
ALSO
BY
LIGHT
REFLECTED
OFF
OTHER
SURFACE
PATCHES
A
PHENOMENON
KNOWN
AS
INTERREFLECTION
A
MODEL
THAT
INCORPORATES
INTERREFLECTION
EFFECTS
IS
KNOWN
AS
A
GLOBAL
SHADING
MODEL
INTERREFLECTIONS
LEAD
TO
A
VARIETY
OF
COMPLEX
SHADING
EFFECTS
WHICH
ARE
STILL
QUITE
POORLY
SEC
INTERREFLECTIONS
GLOBAL
SHADING
MODELS
FIGURE
THE
COLUMN
ON
THE
LEFT
SHOWS
DATA
FROM
A
ROOM
WITH
MATTE
BLACK
WALLS
AND
CONTAINING
A
COLLECTION
OF
MATTE
BLACK
POLYHEDRAL
OBJECTS
THAT
ON
THE
RIGHT
SHOWS
DATA
FROM
A
WHITE
ROOM
CONTAINING
WHITE
OBJECTS
THE
IMAGES
ARE
QUALITATIVELY
DIFFERENT
WITH
DARKER
SHADOWS
AND
CRISPER
BOUNDARIES
IN
THE
BLACK
ROOM
AND
BRIGHT
REFLEXES
IN
THE
CONCAVE
CORNERS
IN
THE
WHITE
ROOM
THE
GRAPHS
SHOW
SECTIONS
OF
THE
IMAGE
INTENSITY
ALONG
THE
CORRESPONDING
LINES
IN
THE
IMAGES
FIGURE
FROM
MUTUAL
ILLUMINATION
BY
D
A
FORSYTH
AND
A
P
ZISSERMAN
PROC
CVPR
QC
IEEE
UNDERSTOOD
UNFORTUNATELY
THESE
EFFECTS
OCCUR
WIDELY
AND
IT
IS
STILL
NOT
YET
KNOWN
HOW
TO
SIMPLIFY
GLOBAL
SHADING
MODELS
WITHOUT
LOSING
ESSENTIAL
QUALITATIVE
PROPERTIES
FOR
EXAMPLE
FIGURE
SHOWS
VIEWS
OF
THE
INTERIOR
OF
TWO
ROOMS
ONE
ROOM
HAS
BLACK
WALLS
AND
CONTAINS
BLACK
OBJECTS
THE
OTHER
HAS
WHITE
WALLS
AND
CONTAINS
WHITE
OBJECTS
EACH
IS
ILLUMINATED
APPROXIMATELY
BY
A
DISTANT
POINT
SOURCE
GIVEN
THAT
THE
INTENSITY
OF
THE
SOURCE
IS
ADJUSTED
APPROPRIATELY
THE
LOCAL
SHADING
MODEL
PREDICTS
THAT
THESE
PICTURES
WOULD
BE
INDISTIN
GUISHABLE
IN
FACT
THE
BLACK
ROOM
HAS
MUCH
DARKER
SHADOWS
AND
CRISPER
BOUNDARIES
AT
THE
CREASES
OF
THE
POLYHEDRA
THAN
THE
WHITE
ROOM
THIS
IS
BECAUSE
SURFACES
IN
THE
BLACK
ROOM
REFLECT
LESS
LIGHT
ONTO
OTHER
SURFACES
THEY
ARE
DARKER
WHEREAS
IN
THE
WHITE
ROOM
OTHER
SURFACES
ARE
SIGNIFICANT
SOURCES
OF
RADIATION
THE
SECTIONS
OF
THE
CAMERA
RESPONSE
TO
THE
RADIOSITY
THESE
ARE
PROPORTIONAL
TO
RADIOSITY
FOR
DIFFUSE
SURFACES
SHOWN
IN
THE
FIGURE
ARE
HUGELY
DIFFERENT
QUALITATIVELY
IN
THE
BLACK
ROOM
THE
RADIOSITY
IS
CONSTANT
IN
PATCHES
AS
A
LOCAL
SHADING
MODEL
WOULD
PREDICT
WHEREAS
IN
THE
WHITE
ROOM
SLOW
IMAGE
GRADIENTS
ARE
QUITE
COMMON
THESE
OCCUR
IN
CONCAVE
CORNERS
WHERE
OBJECT
FACES
REFLECT
LIGHT
ONTO
ONE
ANOTHER
THIS
EFFECT
ALSO
EXPLAINS
WHY
A
ROOM
ILLUMINATED
BY
A
POINT
LIGHT
SOURCE
DOES
NOT
SHOW
THE
SHARP
ILLUMINATION
GRADIENTS
THAT
A
LOCAL
SHADING
MODEL
PREDICTS
RECALL
SECTION
THE
WALLS
AND
FLOOR
OF
THE
ROOM
REFLECT
ILLUMINATION
BACK
AND
THIS
TENDS
TO
LIGHT
UP
THE
CORNERS
WHICH
WOULD
OTHERWISE
BE
DARK
AN
INTERREFLECTION
MODEL
IT
IS
WELL
UNDERSTOOD
HOW
TO
PREDICT
THE
RADIOSITY
ON
A
SET
OF
DIFFUSE
SURFACE
PATCHES
THE
TOTAL
RADIOSITY
LEAVING
A
PATCH
IS
ITS
EXITANCE
WHICH
IS
ZERO
FOR
ALL
BUT
SOURCES
PLUS
ALL
THE
RADIOSITY
SOURCES
SHADOWS
AND
SHADING
CHAP
THAT
IS
REFLECTED
FROM
THE
PATCH
B
P
E
P
BREFL
P
FROM
THE
POINT
OF
VIEW
OF
OUR
PATCH
THERE
IS
NO
DISTINCTION
BETWEEN
ENERGY
LEAVING
ANOTHER
PATCH
DUE
TO
EXITANCE
AND
THAT
DUE
TO
REFLECTION
THIS
MEANS
WE
CAN
TAKE
THE
EXPRESSION
FOR
AN
AREA
SOURCE
AND
USE
IT
TO
OBTAIN
AN
EXPRESSION
FOR
BREFL
Q
IN
PARTICULAR
FROM
THE
PERSPECTIVE
OF
OUR
PATCH
THE
PATCH
AT
R
IN
THE
WORLD
IS
EQUIVALENT
TO
AN
AREA
SOURCE
WITH
EXITANCE
B
R
THIS
MEANS
BREFL
P
ΡD
P
WORLD
VISIBLE
P
Q
B
Q
COS
ΘP
COS
ΘQ
PQ
DAQ
ΡD
P
WORLD
VISIBLE
P
Q
K
P
Q
B
Q
DAQ
WHERE
THE
TERMINOLOGY
IS
THAT
OF
FIGURE
AND
VISIBLE
P
Q
IF
P
CAN
SEE
Q
IF
P
CANNOT
SEE
Q
THE
TERM
VISIBLE
P
Q
K
P
Q
IS
USUALLY
REFERRED
TO
AS
THE
INTERREFLECTION
KERNEL
SUBSTI
TUTING
THE
EXPRESSION
FOR
BREFL
P
GIVES
B
P
E
P
ΡD
P
VISIBLE
P
Q
K
P
Q
B
Q
DAQ
WORLD
IN
PARTICULAR
THE
SOLUTION
APPEARS
INSIDE
THE
INTEGRAL
EQUATIONS
OF
THIS
FORM
ARE
KNOWN
AS
FREDHOLM
INTEGRAL
EQUATIONS
OF
THE
SECOND
KIND
THIS
PARTICULAR
EQUATION
IS
A
FAIRLY
NASTY
SAMPLE
OF
THE
TYPE
BECAUSE
THE
INTERREFLECTION
KERNEL
IS
GENERALLY
NOT
CONTINUOUS
AND
MAY
HAVE
SINGULARITIES
SOLUTIONS
OF
THIS
EQUATION
CAN
YIELD
QUITE
GOOD
MODELS
OF
THE
APPEARANCE
OF
DIFFUSE
SURFACES
AND
THE
TOPIC
SUPPORTS
A
SUBSTANTIAL
INDUSTRY
IN
THE
COMPUTER
GRAPHICS
COMMUNITY
GOOD
DAQ
FIGURE
TERMINOLOGY
FOR
EXPRESSION
DERIVED
IN
THE
TEXT
FOR
THE
INTERREFLEC
TION
KERNEL
SEC
INTERREFLECTIONS
GLOBAL
SHADING
MODELS
ILLUMINATION
FROM
AN
INFINITELY
DISTANT
POINT
SOURCE
IN
THIS
DIRECTION
PREDICTED
CORNER
POSITION
P
P
P
POSITION
POSITION
IN
PIXELS
FIGURE
THE
MODEL
DESCRIBED
IN
THE
TEXT
PRODUCES
QUITE
ACCURATE
QUALITA
TIVE
PREDICTIONS
FOR
INTERREFLECTIONS
THE
TOP
FIGURE
SHOWS
A
CONCAVE
RIGHT
ANGLED
GROOVE
ILLUMINATED
BY
A
POINT
SOURCE
AT
INFINITY
WHERE
THE
SOURCE
DIRECTION
IS
PAR
ALLEL
TO
THE
ONE
FACE
ON
THE
LEFT
OF
THE
BOTTOM
ROW
IS
A
SERIES
OF
PREDICTIONS
OF
THE
RADIOSITY
FOR
THIS
CONFIGURATION
THESE
PREDICTIONS
HAVE
BEEN
SCALED
TO
LIE
ON
TOP
OF
ONE
ANOTHER
THE
CASE
Ρ
CORRESPONDS
TO
THE
LOCAL
SHADING
MODEL
ON
THE
RIGHT
AN
OBSERVED
IMAGE
INTENSITY
FOR
AN
IMAGE
OF
THIS
FORM
FOR
A
CORNER
MADE
OF
WHITE
PAPER
SHOWING
THE
ROOF
LIKE
GRADIENT
IN
RADIOSITY
ASSOCIATED
WITH
THE
EDGE
A
LOCAL
SHADING
MODEL
PREDICTS
A
STEP
FIGURE
FROM
MUTUAL
ILLUMINATION
BY
D
A
FORSYTH
AND
A
P
ZISSERMAN
PROC
CVPR
QC
IEEE
PLACES
TO
START
FOR
THIS
TOPIC
ARE
COHEN
AND
WALLACE
OR
SILLION
THE
MODEL
PRODUCES
GOOD
PREDICTIONS
OF
OBSERVED
EFFECTS
FIGURE
SOLVING
FOR
RADIOSITY
WE
SKETCH
ONE
APPROACH
TO
SOLVING
THE
GLOBAL
SHADING
MODEL
TO
ILLUSTRATE
THE
METHODS
SUBDIVIDE
THE
WORLD
INTO
SMALL
FLAT
PATCHES
AND
APPROXIMATE
THE
RADIOSITY
AS
BEING
CONSTANT
OVER
EACH
PATCH
THIS
APPROXIMATION
IS
REASONABLE
BECAUSE
WE
COULD
OBTAIN
AN
ACCURATE
REPRESENTATION
BY
WORKING
WITH
SMALL
PATCHES
NOW
WE
CONSTRUCT
A
VECTOR
B
WHICH
CONTAINS
THE
VALUE
OF
THE
RADIOSITY
FOR
EACH
PATCH
IN
PARTICULAR
THE
I
TH
COMPONENT
OF
B
IS
THE
RADIOSITY
OF
THE
I
TH
PATCH
WE
WRITE
THE
INCOMING
RADIOSITY
AT
THE
I
TH
PATCH
DUE
TO
RADIOSITY
ON
THE
J
TH
PATCH
AS
BJ
I
P
ΡD
P
PATCH
J
VISIBLE
P
Q
K
P
Q
DAQ
BJ
WHERE
P
IS
A
COORDINATE
ON
THE
I
TH
PATCH
AND
R
IS
A
COORDINATE
ON
THE
J
TH
PATCH
NOW
THIS
EXPRESSION
IS
NOT
A
CONSTANT
AND
SO
WE
MUST
AVERAGE
IT
OVER
THE
I
TH
PATCH
TO
GET
SOURCES
SHADOWS
AND
SHADING
CHAP
B
J
I
A
PATCH
I
ΡD
P
PATCH
J
VISIBLE
P
Q
K
P
Q
DAP
DAQ
BJ
WHERE
AI
IS
THE
AREA
OF
THE
I
TH
PATCH
IF
WE
INSIST
THAT
THE
EXITANCE
ON
EACH
PATCH
IS
CONSTANT
TOO
WE
OBTAIN
THE
MODEL
WHERE
BI
EI
B
J
I
ALL
J
EI
KIJ
BJ
ALL
J
KIJ
A
PATCH
I
ΡD
P
PATCH
J
VISIBLE
P
Q
K
P
Q
DAP
DAQ
THE
ELEMENTS
OF
THIS
MATRIX
ARE
SOMETIMES
KNOWN
AS
FORM
FACTORS
THIS
IS
A
SYSTEM
OF
LINEAR
EQUATIONS
IN
BI
ALTHOUGH
AN
AWFULLY
BIG
ONE
KIJ
COULD
BE
A
MILLION
BY
A
MILLION
MATRIX
AND
AS
SUCH
CAN
IN
PRINCIPLE
BE
SOLVED
THE
TRICKS
THAT
ARE
NECESSARY
TO
SOLVE
THE
SYSTEM
EFFICIENTLY
QUICKLY
AND
ACCURATELY
ARE
WELL
BEYOND
OUR
SCOPE
SILLION
IS
AN
EXCELLENT
ACCOUNT
AS
IS
THE
BOOK
OF
COHEN
AND
WALLACE
THE
QUALITATIVE
EFFECTS
OF
INTERREFLECTIONS
WE
SHOULD
LIKE
TO
EXTRACT
SHAPE
INFORMATION
FROM
RADIOSITY
THIS
IS
RELATIVELY
EASY
TO
DO
WITH
A
LOCAL
MODEL
SEE
SECTION
FOR
SOME
DETAILS
BUT
THE
MODEL
DESCRIBES
THE
WORLD
POORLY
AND
LITTLE
IS
KNOWN
ABOUT
HOW
SEVERELY
THIS
AFFECTS
THE
RESULTING
SHAPE
INFORMATION
EXTRACTING
SHAPE
INFORMATION
FROM
A
GLOBAL
SHADING
MODEL
IS
DIFFICULT
FOR
TWO
REASONS
FIRST
THE
RELATIONSHIP
BE
TWEEN
SHAPE
AND
RADIOSITY
IS
COMPLICATED
BECAUSE
IT
IS
GOVERNED
BY
THE
INTERREFLECTION
KERNEL
SECOND
THERE
ARE
ALMOST
ALWAYS
SURFACES
THAT
ARE
NOT
VISIBLE
BUT
RADIATE
TO
THE
OBJECTS
IN
VIEW
THESE
SO
CALLED
DISTANT
SURFACES
MEAN
IT
IS
HARD
TO
ACCOUNT
FOR
ALL
RADIATION
IN
THE
SCENE
USING
AN
INTERREFLECTION
MODEL
BECAUSE
SOME
RADIATORS
ARE
INVISIBLE
AND
WE
MAY
KNOW
LITTLE
OR
NOTHING
ABOUT
THEM
ALL
THIS
SUGGESTS
THAT
UNDERSTANDING
QUALITATIVE
LOCAL
EFFECTS
OF
INTERREFLECTION
IS
IMPORTANT
ARMED
WITH
THIS
UNDERSTANDING
WE
CAN
EITHER
DISCOUNT
THE
EFFECTS
OF
INTERREFLECTION
OR
EXPLOIT
THEM
THIS
TOPIC
REMAINS
LARGELY
AN
OPEN
RESEARCH
TOPIC
BUT
THERE
ARE
SOME
THINGS
WE
CAN
SAY
SMOOTHING
AND
REGIONAL
PROPERTIES
FIRST
INTERREFLECTIONS
HAVE
A
CHARACTERISTIC
SMOOTHING
EFFECT
THIS
IS
MOST
OBVIOUSLY
SEEN
IF
ONE
TRIES
TO
INTERPRET
A
STAINED
GLASS
WINDOW
BY
LOOKING
AT
THE
PATTERN
IT
CASTS
ON
THE
FLOOR
THIS
PATTERN
IS
ALMOST
ALWAYS
A
SET
OF
INDISTINCT
COLORED
BLOBS
THE
EFFECT
IS
SEEN
MOST
EASILY
WITH
THE
CRUDE
MODEL
OF
FIGURE
THE
GEOMETRY
CONSISTS
OF
A
PATCH
WITH
A
FRONTAL
VIEW
OF
AN
INFINITE
PLANE
WHICH
IS
A
UNIT
DISTANCE
AWAY
AND
CARRIES
A
RADIOSITY
SIN
ΩX
THERE
IS
NO
REASON
TO
VARY
THE
DISTANCE
OF
THE
PATCH
FROM
THE
PLANE
BECAUSE
INTERREFLECTION
PROBLEMS
HAVE
SCALE
INVARIANT
SOLUTIONS
THIS
MEANS
THAT
THE
SOLUTION
FOR
A
PATCH
TWO
UNITS
AWAY
CAN
BE
OBTAINED
BY
READING
OUR
GRAPH
AT
THE
PATCH
IS
SMALL
ENOUGH
THAT
ITS
CONTRIBUTION
TO
THE
PLANE
RADIOSITY
CAN
BE
IGNORED
IF
THE
PATCH
IS
SLANTED
BY
Σ
WITH
RESPECT
TO
THE
PLANE
IT
CARRIES
RADIOSITY
THAT
IS
NEARLY
PERIODIC
WITH
SPATIAL
FREQUENCY
Ω
COS
Σ
WE
REFER
TO
THE
AMPLITUDE
OF
THE
COMPONENT
AT
THIS
FREQUENCY
AS
THE
GAIN
OF
THE
PATCH
AND
PLOT
THE
GAIN
IN
FIGURE
THE
IMPORTANT
PROPERTY
OF
THIS
GRAPH
IS
THAT
HIGH
SPATIAL
FREQUENCIES
HAVE
A
DIFFICULT
TIME
JUMPING
THE
GAP
FROM
THE
PLANE
TO
THE
PATCH
THIS
MEANS
THAT
SHADING
EFFECTS
WITH
HIGH
SEC
NOTES
SOURCE
SPATIAL
FREQUENCY
RADIANS
PER
UNIT
LENGTH
FIGURE
A
SMALL
PATCH
VIEWS
A
PLANE
WITH
SINUSOIDAL
RADIOSITY
OF
UNIT
AM
PLITUDE
THIS
PATCH
HAS
A
ROUGHLY
SINUSOIDAL
RADIOSITY
DUE
TO
THE
EFFECTS
OF
THE
PLANE
WE
REFER
TO
THE
AMPLITUDE
OF
THIS
COMPONENT
AS
THE
GAIN
OF
THE
PATCH
THE
GRAPH
SHOWS
NUMERICAL
ESTIMATES
OF
THE
GAIN
FOR
PATCHES
AT
EQUAL
STEPS
IN
SLANT
ANGLE
FROM
TO
Π
AS
A
FUNCTION
OF
SPATIAL
FREQUENCY
ON
THE
PLANE
THE
GAIN
FALLS
EXTREMELY
FAST
MEANING
THAT
LARGE
TERMS
AT
HIGH
SPATIAL
FREQUENCIES
MUST
BE
REGIONAL
EFFECTS
RATHER
THAN
THE
RESULT
OF
DISTANT
RADIATORS
THIS
IS
WHY
IT
IS
HARD
TO
DETERMINE
THE
PATTERN
IN
A
STAINED
GLASS
WINDOW
BY
LOOKING
AT
THE
FLOOR
AT
FOOT
OF
THE
WINDOW
REPRINTED
FROM
SHADING
PRIMITIVES
FINDING
FOLDS
AND
SHALLOW
GROOVES
BY
J
HADDON
AND
D
A
FORSYTH
PROC
INT
CONF
COMPUTER
VISION
QC
IEEE
SPATIAL
FREQUENCY
AND
HIGH
AMPLITUDE
GENERALLY
CANNOT
COME
FROM
DISTANT
SURFACES
UNLESS
THEY
ARE
ABNORMALLY
BRIGHT
THE
EXTREMELY
FAST
FALL
OFF
IN
AMPLITUDE
WITH
SPATIAL
FREQUENCY
OF
TERMS
DUE
TO
DISTANT
SURFACES
MEANS
THAT
IF
ONE
OBSERVES
A
HIGH
AMPLITUDE
TERM
AT
A
HIGH
SPATIAL
FREQUENCY
IT
IS
VERY
UNLIKELY
TO
HAVE
RESULTED
FROM
THE
EFFECTS
OF
DISTANT
PASSIVE
RADIATORS
BECAUSE
THESE
EFFECTS
DIE
AWAY
QUICKLY
THERE
IS
A
CONVENTION
WHICH
WE
SEE
IN
SECTION
THAT
CLASSIFIES
EFFECTS
IN
SHADING
AS
DUE
TO
REFLECTANCE
IF
THEY
ARE
FAST
EDGES
AND
THE
DYNAMIC
RANGE
IS
RELATIVELY
LOW
AND
DUE
TO
ILLUMINATION
OTHERWISE
WE
CAN
EXPAND
THIS
CONVENTION
THERE
IS
A
MID
RANGE
OF
SPATIAL
FREQUENCIES
THAT
ARE
LARGELY
UNAFFECTED
BY
MUTUAL
ILLUMINATION
FROM
DISTANT
SURFACES
BECAUSE
THE
GAIN
IS
SMALL
SPATIAL
FREQUENCIES
IN
THIS
RANGE
CANNOT
BE
TRANSMITTED
BY
DISTANT
PASSIVE
RADIATORS
UNLESS
THESE
RADIATORS
HAVE
IMPROBABLY
HIGH
RADIOSITY
AS
A
RESULT
SPATIAL
FREQUENCIES
IN
THIS
RANGE
CAN
BE
THOUGHT
OF
AS
REGIONAL
PROPERTIES
WHICH
CAN
RESULT
ONLY
FROM
INTERREFLECTION
EFFECTS
WITHIN
A
REGION
THE
MOST
NOTABLE
REGIONAL
PROPERTIES
ARE
PROBABLY
REFLEXES
SMALL
BRIGHT
PATCHES
THAT
AP
PEAR
MAINLY
IN
CONCAVE
REGIONS
ILLUSTRATED
IN
FIGURE
AND
FIGURE
A
SECOND
IMPORTANT
EFFECT
IS
COLOR
BLEEDING
WHERE
A
COLORED
SURFACE
REFLECTS
LIGHT
ONTO
ANOTHER
COLORED
SURFACE
THIS
IS
A
COMMON
EFFECT
THAT
PEOPLE
TEND
NOT
TO
NOTICE
UNLESS
THEY
ARE
CONSCIOUSLY
LOOKING
FOR
IT
IT
IS
QUITE
OFTEN
REPRODUCED
BY
PAINTERS
NOTES
SHADING
MODELS
ARE
HANDLED
IN
A
QUITE
UNSYSTEMATIC
WAY
IN
THE
VISION
LITERATURE
THE
POINT
SOURCE
APPROXIMATION
IS
WIDELY
ABUSED
YOU
SHOULD
USE
IT
WITH
CARE
AND
INSPECT
OTHERS
USE
OF
IT
WITH
SUSPICION
WE
BELIEVE
WE
ARE
THE
FIRST
TO
DRAW
THE
DISTINCTION
BETWEEN
A
THE
PHYSICAL
EFFECTS
OF
SOURCES
AND
B
THE
SHADING
MODEL
SOURCES
SHADOWS
AND
SHADING
CHAP
ILLUMINATION
FROM
AN
INFINITELY
DISTANT
POINT
SOURCE
IN
THIS
DIRECTION
POSITION
POSITION
IN
PIXELS
FIGURE
REFLEXES
AT
CONCAVE
EDGES
ARE
A
COMMON
QUALITATIVE
RESULT
OF
IN
TERREFLECTIONS
THE
FIGURE
ON
THE
TOP
SHOWS
THE
SITUATION
HERE
A
CONCAVE
RIGHT
ANGLED
GROOVE
ILLUMINATED
BY
A
POINT
LIGHT
SOURCE
AT
INFINITY
WHOSE
SOURCE
VECTOR
IS
ALONG
THE
ANGLE
BISECTOR
THE
GRAPH
ON
THE
LEFT
SHOWS
THE
INTENSITY
PREDICTIONS
OF
AN
INTERREFLECTION
MODEL
FOR
THIS
CONFIGURATION
THE
CASE
Ρ
IS
A
LOCAL
SHAD
ING
MODEL
THE
GRAPHS
HAVE
BEEN
LINED
UP
FOR
EASY
COMPARISON
AS
THE
SURFACE
ALBEDO
GOES
UP
A
ROOF
LIKE
STRUCTURE
APPEARS
THE
GRAPH
ON
THE
RIGHT
SHOWS
AN
OBSERVATION
OF
THIS
EFFECT
IN
AN
IMAGE
OF
A
REAL
SCENE
FIGURE
FROM
MUTUAL
IL
LUMINATION
BY
D
A
FORSYTH
AND
A
P
ZISSERMAN
PROC
CVPR
C
IEEE
LOCAL
SHADING
MODELS
THE
GREAT
VIRTUE
OF
LOCAL
SHADING
MODELS
IS
THAT
THE
ANALYSIS
IS
SIMPLE
THE
PRIMARY
CHARACTERISTIC
OF
A
LOCAL
SHADING
MODEL
IS
THAT
ON
A
SURFACE
OF
CONSTANT
ALBEDO
THE
RADIOSITY
OF
A
SURFACE
PATCH
IS
A
FUNCTION
OF
THE
NORMAL
ALONE
THIS
MEANS
THAT
ONE
CAN
AVOID
THE
ABSTRACTION
OF
REFLECTANCE
AND
SOURCES
AND
INSTEAD
SIMPLY
CODE
THE
PROPERTIES
OF
SURFACE
AND
SOURCE
AS
A
REFLECTANCE
MAP
THE
REFLECTANCE
MAP
IS
A
FUNCTION
THAT
TAKES
A
REPRESENTATION
OF
THE
NORMAL
AND
RETURNS
THE
RADIOSITY
TO
BE
EXPECTED
AT
A
POINT
WITH
THAT
NORMAL
HORN
STARTED
THE
SYSTEMATIC
STUDY
OF
SHADING
IN
COMPUTER
VISION
WITH
IMPORTANT
PAPERS
ON
RECOVERING
SHAPE
FROM
A
LOCAL
SHADING
MODEL
USING
A
POINT
SOURCE
HORN
WITH
A
MORE
RECENT
ACCOUNT
IN
HORN
THE
METHODS
DISCUSSED
HAVE
LARGELY
FALLEN
INTO
DISUSE
AT
LEAST
PARTIALLY
BECAUSE
THEY
APPEAR
UNABLE
TO
COPE
WITH
THE
DIFFICULTIES
CREATED
BY
A
GLOBAL
SHADING
MODEL
SO
WE
DO
NOT
SURVEY
THE
VAST
LITERATURE
HERE
A
COMPREHENSIVE
SUMMARY
IS
IN
HORN
AND
BROOKS
SHAPE
AND
ALBEDO
ARE
AMBIGUOUS
WITH
APPROPRIATE
CHANGES
IN
ALBEDO
SURFACES
OF
DIFFERENT
SHAPES
CAN
GENERATE
THE
SAME
IMAGE
BELHUMEUR
KRIEGMAN
AND
YUILLE
KRIEGMAN
AND
BELHUMEUR
BECAUSE
THE
SURFACE
NORMAL
IS
THE
KEY
IN
LOCAL
SHADING
MODELS
SUCH
MODELS
TYPICALLY
YIELD
ELEGANT
LINKS
BETWEEN
SURFACE
SHADING
AND
CURVATURE
KOENDERINK
AND
VAN
DOORN
SEC
NOTES
SELF
SHADOW
A
B
C
PREDICTED
OBSERVED
POSITION
POSITION
IN
PIXELS
FIGURE
REFLEXES
OCCUR
QUITE
WIDELY
THEY
ARE
USUALLY
CAUSED
BY
A
FAVOR
ABLE
VIEW
OF
A
LARGE
REFLECTING
SURFACE
IN
THE
GEOMETRY
SHOWN
ON
THE
TOP
THE
SHADOWED
REGION
OF
THE
CYLINDRICAL
BUMP
SEES
THE
PLANE
BACKGROUND
AT
A
FAIRLY
FAVORABLE
ANGLE
IF
THE
BACKGROUND
IS
LARGE
ENOUGH
NEAR
HALF
THE
HEMISPHERE
OF
THE
PATCH
AT
THE
BASE
OF
THE
BUMP
IS
A
VIEW
OF
THE
PLANE
THIS
MEANS
THERE
WILL
BE
A
REFLEX
WITH
A
LARGE
VALUE
ATTACHED
TO
THE
EDGE
OF
THE
BUMP
AND
INSIDE
THE
CAST
SHADOW
REGION
WHICH
A
LOCAL
MODEL
PREDICTS
AS
BLACK
THERE
IS
ANOTHER
REFLEX
ON
THE
OTHER
SIDE
TOO
AS
THE
SERIES
OF
SOLUTIONS
AGAIN
NORMALIZED
FOR
EASY
COMPARISON
ON
THE
LEFT
SHOW
ON
THE
RIGHT
AN
OBSERVATION
OF
THIS
EFFECT
IN
A
REAL
SCENE
FIGURE
FROM
MUTUAL
ILLUMINATION
BY
D
A
FORSYTH
AND
A
P
ZISSERMAN
PROC
CVPR
QC
IEEE
INTERREFLECTIONS
THE
EFFECTS
OF
GLOBAL
SHADING
ARE
OFTEN
IGNORED
IN
THE
SHADING
LITERATURE
WHICH
CAUSES
A
REFLEX
RESPONSE
OF
HOSTILITY
IN
ONE
OF
THE
AUTHORS
THE
REASON
TO
IGNORE
INTERREFLECTIONS
IS
THAT
THEY
ARE
EXTREMELY
HARD
TO
ANALYZE
PARTICULARLY
FROM
THE
PERSPECTIVE
OF
INFERRING
OBJECT
PROPERTIES
GIVEN
THE
OUTPUT
OF
A
GLOBAL
SHADING
MODEL
IF
INTERREFLECTION
EFFECTS
DO
NOT
CHANGE
THE
OUTPUT
OF
A
METHOD
MUCH
THEN
IT
IS
PROBABLY
ALL
RIGHT
TO
IGNORE
THEM
UNFORTUNATELY
THIS
LINE
OF
REASONING
IS
SELDOM
PURSUED
BECAUSE
IT
IS
QUITE
DIFFICULT
TO
SHOW
THAT
A
METHOD
IS
STABLE
UNDER
INTERREFLECTIONS
THE
DISCUSSION
OF
SPATIAL
FREQUENCY
ISSUES
FOLLOWS
HADDON
AND
FORSYTH
AFTER
AN
IDEA
OF
KOENDERINK
AND
VAN
DOORN
APART
FROM
THIS
THERE
IS
NOT
MUCH
KNOWLEDGE
ABOUT
THE
OVERALL
PROPERTIES
OF
INTERREFLECTED
SHADING
WHICH
IS
AN
IMPORTANT
GAP
IN
OUR
KNOWLEDGE
AN
ALTERNATIVE
STRATEGY
IS
TO
ITERATIVELY
REESTIMATE
SHAPE
USING
A
RENDERING
MODEL
NAYAR
IKEUCHI
AND
KANADE
HORN
IS
ALSO
THE
FIRST
AUTHOR
TO
INDICATE
THE
SIGNIFICANCE
OF
GLOBAL
SHADING
EFFECTS
HORN
KOENDERINK
AND
VAN
DOORN
NOTED
THAT
THE
RADIOSITY
UNDER
A
GLOBAL
MODEL
IS
OB
TAINED
BY
TAKING
THE
RADIOSITY
UNDER
A
LOCAL
MODEL
AND
APPLYING
A
LINEAR
OPERATOR
ONE
THEN
STUDIES
THAT
OPERATOR
IN
SOME
CASES
ITS
EIGENFUNCTIONS
OFTEN
CALLED
GEOMETRICAL
MODES
ARE
INFORMATIVE
FORSYTH
AND
ZISSERMAN
THEN
DEMONSTRATED
A
VARIETY
OF
THE
QUAL
ITATIVE
EFFECTS
DUE
TO
INTERREFLECTIONS
SOURCES
SHADOWS
AND
SHADING
CHAP
PHOTOMETRIC
STEREO
IN
ITS
ORIGINAL
FORM
PHOTOMETRIC
STEREO
IS
DUE
TO
WOODHAM
THERE
ARE
A
NUMBER
OF
VARIANTS
OF
THIS
USEFUL
IDEA
HORN
WOODHAM
AND
SILVER
WOODHAM
THERE
ARE
A
VARIETY
OF
VARIATIONS
ON
PHOTOMETRIC
STEREO
ONE
INTERESTING
IDEA
IS
TO
ILLUMINATE
THE
SURFACE
WITH
THREE
LIGHTS
OF
DIFFERENT
COLORS
AND
IN
DIFFERENT
POSITIONS
AND
USE
A
COLOR
IMAGE
FOR
AN
APPROPRIATE
CHOICE
OF
COLORS
THIS
IS
EQUIVALENT
TO
OBTAINING
THREE
IMAGES
SO
THE
MEASUREMENT
PROCESS
IS
SIMPLIFIED
GENERALLY
PHOTOMETRIC
STEREO
IS
USED
UNDER
CIRCUMSTANCES
WHERE
THE
ILLUMINATION
IS
QUITE
EASILY
CONTROLLED
SO
THAT
IT
IS
POSSIBLE
TO
ENSURE
THAT
NO
AMBIENT
ILLUMINATION
IS
IN
THE
IMAGE
IT
IS
RELATIVELY
SIMPLE
TO
INSERT
AMBIENT
ILLUMINATION
INTO
THE
FORMULATION
GIVEN
WE
EXTEND
THE
MATRIX
BY
ATTACHING
A
COLUMN
OF
ONES
IN
THIS
CASE
G
X
Y
BECOMES
A
FOUR
DIMENSIONAL
VECTOR
AND
THE
FOURTH
COMPONENT
IS
THE
AMBIENT
TERM
HOWEVER
THIS
APPROACH
DOES
NOT
GUARANTEE
THAT
THE
AMBIENT
TERM
IS
CONSTANT
OVER
SPACE
INSTEAD
WE
WOULD
HAVE
TO
CHECK
THAT
THIS
TERM
WAS
CONSTANT
AND
ADJUST
THE
MODEL
IF
IT
WERE
NOT
PHOTOMETRIC
STEREO
DEPENDS
ONLY
ON
ADOPTING
A
LOCAL
SHADING
MODEL
THIS
MODEL
NEED
NOT
BE
A
LAMBERTIAN
SURFACE
ILLUMINATED
BY
A
DISTANT
POINT
SOURCE
IF
THE
RADIOSITY
OF
THE
SURFACE
IS
A
KNOWN
FUNCTION
OF
THE
SURFACE
NORMAL
SATISFYING
A
SMALL
NUMBER
OF
CONSTRAINTS
PHOTOMETRIC
STEREO
IS
STILL
POSSIBLE
THIS
IS
BECAUSE
THE
INTENSITY
OF
A
PIXEL
IN
A
SINGLE
VIEW
DETERMINES
THE
NORMAL
UP
TO
A
ONE
PARAMETER
FAMILY
THIS
MEANS
THAT
TWO
VIEWS
DETERMINE
THE
NORMAL
THE
SIMPLEST
EXAMPLE
OF
THIS
CASE
OCCURS
FOR
A
SURFACE
OF
KNOWN
ALBEDO
ILLUMINATED
BY
A
DISTANT
POINT
SOURCE
IN
FACT
IF
THE
RADIOSITY
OF
THE
SURFACE
IS
A
K
PARAMETER
FUNCTION
OF
THE
SURFACE
NORMAL
PHOTOMETRIC
STEREO
IS
STILL
POSSIBLE
THE
INTENSITY
OF
THE
PIXEL
IN
A
SINGLE
VIEW
DETERMINES
THE
NORMAL
UP
TO
A
K
PARAMETER
FAMILY
AND
K
VIEWS
GIVE
THE
NORMAL
FOR
THIS
APPROACH
TO
WORK
THE
RADIOSITY
NEEDS
TO
BE
GIVEN
BY
A
FUNCTION
FOR
WHICH
OUR
ARITHMETIC
WORKS
E
G
IF
THE
RADIOSITY
OF
THE
SURFACE
IS
A
CONSTANT
FUNCTION
OF
THE
SURFACE
NORMAL
IT
IS
NOT
POSSIBLE
TO
INFER
ANY
CONSTRAINT
ON
THE
NORMAL
FROM
THE
RADIOSITY
ONE
CAN
THEN
RECOVER
SHAPE
AND
REFLECTANCE
MAPS
SIMULTANEOUSLY
GARCIA
BERMEJO
DIAZ
PERNAS
AND
CORONADO
MUKAWA
NAYAR
IKEUCHI
AND
KANADE
AND
TAGARE
AND
DE
FIGUEIREDO
ALTERNATIVE
SHADING
REPRESENTATIONS
INSTEAD
OF
TRYING
TO
EXTRACT
SHAPE
INFORMATION
FROM
THE
SHADING
SIGNAL
ONE
MIGHT
TRY
TO
MATCH
IT
TO
A
COLLECTION
OF
DIFFERENT
POSSIBLE
EXAMPLES
THIS
SUGGESTS
STUDYING
WHAT
KINDS
OF
SHADED
VIEW
A
SURFACE
CAN
GENERATE
THE
COLLECTION
OF
AVAILABLE
SHADINGS
IS
NOTABLY
LIMITED
BELHUMEUR
AND
KRIEGMAN
A
KNOWLEDGE
OF
THIS
COLLECTION
STRUCTURE
IS
VALUABLE
BECAUSE
IT
MAKES
IT
POSSIBLE
TO
UNDERSTAND
HOW
TO
COMPARE
SHADED
IMAGES
WITHOUT
BEING
CONFUSED
BY
ILLUMINATION
CHANGES
ILLUMINATION
CHANGES
ARE
A
PARTICULAR
PROBLEM
IN
FACE
FINDING
AND
RECOGNITION
APPLICA
TIONS
ADINI
MOSES
AND
ULLMAN
PHILLIPS
AND
VARDI
KNOWING
THE
POSSIBLE
VARIA
TIONS
IN
ILLUMINATION
SEEMS
TO
HELP
GEORGHIADES
KRIEGMAN
AND
BELHUMEUR
JACOBS
BELHUMEUR
AND
BASRI
ANOTHER
POSSIBILITY
IS
TO
EXTEND
THE
NOTION
OF
QUALITATIVE
ANALYSIS
OF
INTERREFLECTIONS
TO
OB
TAIN
A
SHADING
PRIMITIVE
A
SHADING
PATTERN
THAT
IS
CHARACTERISTIC
AND
STABLY
LINKED
TO
A
SHAPE
PATTERN
FOR
EXAMPLE
NARROW
GROOVES
AND
DEEP
HOLES
IN
SURFACES
ARE
DARK
AND
CYLINDERS
HAVE
A
CHARACTERISTIC
EXTENDED
PATTERN
OF
SHADING
FEW
SUCH
PRIMITIVES
ARE
KNOWN
BUT
SOME
APPEAR
TO
BE
USEFUL
HADDON
AND
FORSYTH
PROBLEMS
PROBLEMS
WHAT
SHAPES
CAN
THE
SHADOW
OF
A
SPHERE
TAKE
IF
IT
IS
CAST
ON
A
PLANE
AND
THE
SOURCE
IS
A
POINT
SOURCE
WE
HAVE
A
SQUARE
AREA
SOURCE
AND
A
SQUARE
OCCLUDER
BOTH
PARALLEL
TO
A
PLANE
THE
SOURCE
IS
THE
SAME
SIZE
AS
THE
OCCLUDER
AND
THEY
ARE
VERTICALLY
ABOVE
ONE
ANOTHER
WITH
THEIR
CENTERS
ALIGNED
WHAT
IS
THE
SHAPE
OF
THE
UMBRA
WHAT
IS
THE
SHAPE
OF
THE
OUTSIDE
BOUNDARY
OF
THE
PENUMBRA
WE
HAVE
A
SQUARE
AREA
SOURCE
AND
A
SQUARE
OCCLUDER
BOTH
PARALLEL
TO
A
PLANE
THE
EDGE
LENGTH
OF
THE
SOURCE
IS
NOW
TWICE
THAT
OF
THE
OCCLUDER
AND
THEY
ARE
VERTICALLY
ABOVE
ONE
ANOTHER
WITH
THEIR
CENTERS
ALIGNED
WHAT
IS
THE
SHAPE
OF
THE
UMBRA
WHAT
IS
THE
SHAPE
OF
THE
OUTSIDE
BOUNDARY
OF
THE
PENUMBRA
WE
HAVE
A
SQUARE
AREA
SOURCE
AND
A
SQUARE
OCCLUDER
BOTH
PARALLEL
TO
A
PLANE
THE
EDGE
LENGTH
OF
THE
SOURCE
IS
NOW
HALF
THAT
OF
THE
OCCLUDER
AND
THEY
ARE
VERTICALLY
ABOVE
ONE
ANOTHER
WITH
THEIR
CENTERS
ALIGNED
WHAT
IS
THE
SHAPE
OF
THE
UMBRA
WHAT
IS
THE
SHAPE
OF
THE
OUTSIDE
BOUNDARY
OF
THE
PENUMBRA
A
SMALL
SPHERE
CASTS
A
SHADOW
ON
A
LARGER
SPHERE
DESCRIBE
THE
POSSIBLE
SHADOW
BOUNDARIES
THAT
OCCUR
EXPLAIN
WHY
IT
IS
DIFFICULT
TO
USE
SHADOW
BOUNDARIES
TO
INFER
SHAPE
PARTICULARLY
IF
THE
SHADOW
IS
CAST
ONTO
A
CURVED
SURFACE
AN
INFINITESIMAL
PATCH
VIEWS
A
CIRCULAR
AREA
SOURCE
OF
CONSTANT
EXITANCE
FRONTALLY
ALONG
THE
AXIS
OF
SYMMETRY
OF
THE
SOURCE
COMPUTE
THE
RADIOSITY
OF
THE
PATCH
DUE
TO
THE
SOURCE
EXITANCE
E
U
AS
A
FUNCTION
OF
THE
AREA
OF
THE
SOURCE
AND
THE
DISTANCE
BETWEEN
THE
CENTER
OF
THE
SOURCE
AND
THE
PATCH
YOU
MAY
HAVE
TO
LOOK
THE
INTEGRAL
UP
IN
TABLES
IF
YOU
DON
T
YOU
RE
ENTITLED
TO
FEEL
PLEASED
WITH
YOURSELF
BUT
THIS
IS
ONE
OF
FEW
CASES
THAT
CAN
BE
DONE
IN
CLOSED
FORM
IT
IS
EASIER
TO
LOOK
UP
IF
YOU
TRANSFORM
IT
TO
GET
RID
OF
THE
COSINE
TERMS
AS
IN
FIGURE
A
SMALL
PATCH
VIEWS
AN
INFINITE
PLANE
AT
UNIT
DISTANCE
THE
PATCH
IS
SUFFICIENTLY
SMALL
THAT
IT
REFLECTS
A
TRIVIAL
QUANTITY
OF
LIGHT
ONTO
THE
PLANE
THE
PLANE
HAS
RADIOSITY
B
X
Y
SIN
AX
THE
PATCH
AND
THE
PLANE
ARE
PARALLEL
TO
ONE
ANOTHER
WE
MOVE
THE
PATCH
AROUND
PARALLEL
TO
THE
PLANE
AND
CONSIDER
ITS
RADIOSITY
AT
VARIOUS
POINTS
SHOW
THAT
IF
ONE
TRANSLATES
THE
PATCH
ITS
RADIOSITY
VARIES
PERIODICALLY
WITH
ITS
POSITION
IN
X
FIX
THE
PATCH
CENTER
AT
DETERMINE
A
CLOSED
FORM
EXPRESSION
FOR
THE
RADIOSITY
OF
THE
PATCH
AT
THIS
POINT
AS
A
FUNCTION
OF
A
YOU
LL
NEED
A
TABLE
OF
INTEGRALS
FOR
THIS
IF
YOU
DON
T
YOU
RE
ENTITLED
TO
FEEL
VERY
PLEASED
WITH
YOURSELF
IF
ONE
LOOKS
ACROSS
A
LARGE
BAY
IN
THE
DAYTIME
IT
IS
OFTEN
HARD
TO
DISTINGUISH
THE
MOUNTAINS
ON
THE
OPPOSITE
SIDE
NEAR
SUNSET
THEY
ARE
CLEARLY
VISIBLE
THIS
PHENOMENON
HAS
TO
DO
WITH
SCATTERING
OF
LIGHT
BY
AIR
A
LARGE
VOLUME
OF
AIR
IS
ACTUALLY
A
SOURCE
EXPLAIN
WHAT
IS
HAPPENING
WE
HAVE
MODELED
AIR
AS
A
VACUUM
AND
ASSERTED
THAT
NO
ENERGY
IS
LOST
ALONG
A
STRAIGHT
LINE
IN
A
VACUUM
USE
YOUR
EXPLANATION
TO
GIVE
AN
ESTIMATE
OF
THE
KIND
OF
SCALES
OVER
WHICH
THAT
MODEL
IS
ACCEPTABLE
READ
THE
BOOK
COLOUR
AND
LIGHT
IN
NATURE
BY
LYNCH
AND
LIVINGSTONE
PUBLISHED
BY
CAMBRIDGE
UNI
VERSITY
PRESS
PROGRAMMING
ASSIGNMENTS
AN
AREA
SOURCE
CAN
BE
APPROXIMATED
AS
A
GRID
OF
POINT
SOURCES
THE
WEAKNESS
OF
THIS
APPROXIMATION
IS
THAT
THE
PENUMBRA
CONTAINS
QUANTIZATION
ERRORS
WHICH
CAN
BE
QUITE
OFFENSIVE
TO
THE
EYE
EXPLAIN
RENDER
THIS
EFFECT
FOR
A
SQUARE
SOURCE
AND
A
SINGLE
OCCLUDER
CASTING
A
SHADOW
ONTO
AN
INFINITE
PLANE
FOR
A
FIXED
GEOMETRY
YOU
SHOULD
FIND
THAT
AS
THE
NUMBER
OF
POINT
SOURCES
GOES
UP
THE
QUANTIZATION
ERROR
GOES
DOWN
SOURCES
SHADOWS
AND
SHADING
CHAP
THIS
APPROXIMATION
HAS
THE
UNPLEASANT
PROPERTY
THAT
IT
IS
POSSIBLE
TO
PRODUCE
ARBITRARILY
LARGE
QUANTIZATION
ERRORS
WITH
ANY
FINITE
GRID
BY
CHANGING
THE
GEOMETRY
THIS
IS
BECAUSE
THERE
ARE
CON
FIGURATIONS
OF
SOURCE
AND
OCCLUDER
THAT
PRODUCE
LARGE
PENUMBRAE
USE
A
SQUARE
SOURCE
AND
A
SINGLE
OCCLUDER
CASTING
A
SHADOW
ONTO
AN
INFINITE
PLANE
TO
EXPLAIN
THIS
EFFECT
MAKE
A
WORLD
OF
BLACK
OBJECTS
AND
ANOTHER
OF
WHITE
OBJECTS
PAPER
GLUE
AND
SPRAYPAINT
ARE
USEFUL
HERE
AND
OBSERVE
THE
EFFECTS
OF
INTERREFLECTIONS
CAN
YOU
COME
UP
WITH
A
CRITERION
THAT
RELIABLY
TELLS
FROM
AN
IMAGE
WHICH
IS
WHICH
IF
YOU
CAN
PUBLISH
IT
THE
PROBLEM
LOOKS
EASY
BUT
ISN
T
THIS
EXERCISE
REQUIRES
SOME
KNOWLEDGE
OF
NUMERICAL
ANALYSIS
DO
THE
NUMERICAL
INTEGRALS
REQUIRED
TO
REPRODUCE
FIGURE
THESE
INTEGRALS
AREN
T
PARTICULARLY
EASY
IF
ONE
USES
COORDINATES
ON
THE
INFINITE
PLANE
THE
SIZE
OF
THE
DOMAIN
IS
A
NUISANCE
IF
ONE
CONVERTS
TO
COORDINATES
ON
THE
VIEW
HEMISPHERE
OF
THE
PATCH
THE
FREQUENCY
OF
THE
RADIANCE
BECOMES
INFINITE
AT
THE
BOUNDARY
OF
THE
HEMISPHERE
THE
BEST
WAY
TO
ESTIMATE
THESE
INTEGRALS
IS
USING
A
MONTE
CARLO
METHOD
ON
THE
HEMISPHERE
YOU
SHOULD
USE
IMPORTANCE
SAMPLING
BECAUSE
THE
BOUNDARY
CONTRIBUTES
RATHER
LESS
TO
THE
INTEGRAL
THAN
THE
TOP
SET
UP
AND
SOLVE
THE
LINEAR
EQUATIONS
FOR
AN
INTERREFLECTION
SOLUTION
FOR
THE
INTERIOR
OF
A
CUBE
WITH
A
SMALL
SQUARE
SOURCE
IN
THE
CENTER
OF
THE
CEILING
IMPLEMENT
A
PHOTOMETRIC
STEREO
SYSTEM
HOW
ACCURATE
ARE
ITS
MEASUREMENTS
I
E
HOW
WELL
DO
THEY
COMPARE
WITH
KNOWN
SHAPE
INFORMA
TION
DO
INTERREFLECTIONS
AFFECT
THE
ACCURACY
HOW
REPEATABLE
ARE
ITS
MEASUREMENTS
I
E
IF
YOU
OBTAIN
ANOTHER
SET
OF
IMAGES
PERHAPS
UNDER
DIFFERENT
ILLUMINANTS
AND
RECOVER
SHAPE
FROM
THOSE
HOW
DOES
THE
NEW
SHAPE
COMPARE
WITH
THE
OLD
COMPARE
THE
MINIMIZATION
APPROACH
TO
RECONSTRUCTION
WITH
THE
INTEGRATION
APPROACH
WHICH
IS
MORE
ACCURATE
OR
MORE
REPEATABLE
AND
WHY
DOES
THIS
DIFFERENCE
APPEAR
IN
EXPERIMENT
ONE
POSSIBLE
WAY
TO
IMPROVE
THE
INTEGRATION
APPROACH
IS
TO
OBTAIN
DEPTHS
BY
INTEGRATING
OVER
MANY
DIFFERENT
PATHS
AND
THEN
AVERAGE
THESE
DEPTHS
YOU
NEED
TO
BE
A
LITTLE
CAREFUL
ABOUT
CONSTANTS
HERE
DOES
THIS
IMPROVE
THE
ACCURACY
OR
REPEATABILITY
OF
THE
METHOD
THE
ENTERTAINMENT
INDUSTRY
TOUCHES
HUNDREDS
OF
MILLIONS
OF
PEOPLE
EVERY
DAY
AND
SYNTHETIC
PIC
TURES
OF
REAL
SCENES
OFTEN
MIXED
WITH
ACTUAL
FILM
FOOTAGE
ARE
NOW
COMMON
PLACE
IN
COMPUTER
GAMES
SPORTS
BROADCASTING
TV
ADVERTISING
AND
FEATURE
FILMS
CREATING
THESE
IMAGES
IS
WHAT
IMAGE
BASED
RENDERING
DEFINED
HERE
AS
THE
SYNTHESIS
OF
NEW
VIEWS
OF
A
SCENE
FROM
PRERECORDED
PICTURES
IS
ALL
ABOUT
AND
IT
DOES
REQUIRE
THE
RECOVERY
OF
QUANTITATIVE
ALTHOUGH
NOT
NECESSARILY
THREE
DIMENSIONAL
SHAPE
INFORMATION
FROM
IMAGES
THIS
CHAPTER
PRESENTS
A
NUMBER
OF
REPRESEN
TATIVE
APPROACHES
TO
IMAGE
BASED
RENDERING
DIVIDING
THEM
RATHER
ARBITRARILY
INTO
A
TECHNIQUES
THAT
FIRST
RECOVER
A
THREE
DIMENSIONAL
SCENE
MODEL
FROM
A
SEQUENCE
OF
PICTURES
THEN
RENDER
IT
WITH
CLASSICAL
COMPUTER
GRAPHICS
TOOLS
NATURALLY
THESE
APPROACHES
ARE
OFTEN
RELATED
TO
STEREO
AND
MOTION
ANALYSIS
B
METHODS
THAT
DO
NOT
ATTEMPT
TO
RECOVER
THE
CAMERA
OR
SCENE
PARAMETERS
BUT
CONSTRUCT
INSTEAD
AN
EXPLICIT
REPRESENTATION
OF
THE
SET
OF
ALL
POSSIBLE
PICTURES
OF
THE
OBSERVED
SCENE
THEN
USE
THE
IMAGE
POSITION
OF
A
SMALL
NUMBER
OF
TIE
POINTS
TO
SPECIFY
A
NEW
VIEW
OF
THE
SCENE
AND
TRANSFER
ALL
THE
OTHER
POINTS
INTO
THE
NEW
IMAGE
IN
THE
PHOTOGRAMMETRIC
SENSE
ALREADY
MENTIONED
IN
CHAPTER
AND
C
APPROACHES
THAT
MODEL
IMAGES
BY
A
TWO
DIMENSIONAL
SET
OF
LIGHT
RAYS
OR
MORE
PRECISELY
BY
THE
VALUE
OF
THE
RADIANCE
ALONG
THESE
RAYS
AND
THE
SET
OF
ALL
PICTURES
OF
A
SCENE
BY
A
FOUR
DIMENSIONAL
SET
OF
RAYS
THE
LIGHT
FIELD
FIGURE
CONSTRUCTING
MODELS
FROM
IMAGE
SEQUENCES
THIS
SECTION
ADDRESSES
THE
PROBLEM
OF
BUILDING
AND
RENDERING
A
THREE
DIMENSIONAL
OBJECT
MODEL
FROM
A
SEQUENCE
OF
PICTURES
IT
IS
OF
COURSE
POSSIBLE
TO
CONSTRUCT
SUCH
A
MODEL
BY
FUSING
REGIS
TERED
DEPTH
MAPS
ACQUIRED
BY
RANGE
SCANNERS
AS
DESCRIBED
IN
CHAPTER
BUT
WE
FOCUS
HERE
ON
THE
CASE
WHERE
THE
INPUT
IMAGES
ARE
DIGITIZED
PHOTOGRAPHS
OR
FILM
CLIPS
OF
A
RIGID
OR
DYNAMIC
SCENE
SEC
CONSTRUCTING
MODELS
FROM
IMAGE
SEQUENCES
PRE
RECORDED
IMAGES
MODEL
CONTROL
NEW
IMAGE
FIGURE
APPROACHES
TO
IMAGE
BASED
RENDERING
FROM
TOP
TO
BOTTOM
THREE
DIMENSIONAL
MODEL
CONSTRUCTION
FROM
IMAGE
SEQUENCES
TRANSFER
BASED
IMAGE
SYNTHESIS
THE
LIGHT
FIELD
FROM
LEFT
TO
RIGHT
THE
IMAGE
BASED
RENDERING
PIPELINE
A
SCENE
MODEL
THAT
MAY
NOT
BE
THREE
DIMENSIONAL
IS
CONSTRUCTED
FROM
SAMPLE
IMAGES
AND
USED
TO
RENDER
NEW
IMAGES
OF
THE
SCENE
THE
RENDERING
ENGINE
MAY
BE
CONTROLLED
BY
A
JOYSTICK
OR
EQUIVALENTLY
BY
THE
SPECIFICATION
OF
CAMERA
PARAM
ETERS
OR
IN
THE
CASE
OF
TRANSFER
BASED
TECHNIQUES
BY
SETTING
THE
IMAGE
POSITION
OF
A
SMALL
NUMBER
OF
TIE
POINTS
SCENE
MODELING
FROM
REGISTERED
IMAGES
VOLUMETRIC
RECONSTRUCTION
LET
US
ASSUME
THAT
AN
OBJECT
HAS
BEEN
DELINEATED
PER
HAPS
INTERACTIVELY
IN
A
COLLECTION
OF
PHOTOGRAPHS
REGISTERED
IN
THE
SAME
GLOBAL
COORDINATE
SYSTEM
IT
IS
IMPOSSIBLE
TO
UNIQUELY
RECOVER
THE
OBJECT
SHAPE
FROM
THE
IMAGE
CONTOURS
SINCE
AS
OBSERVED
IN
CHAPTER
THE
CONCAVE
PORTIONS
OF
ITS
SURFACE
NEVER
SHOW
UP
ON
THE
IMAGE
CONTOUR
STILL
WE
SHOULD
BE
ABLE
TO
CONSTRUCT
A
REASONABLE
APPROXIMATION
OF
THE
SURFACE
FROM
A
LARGE
ENOUGH
SET
OF
PICTURES
THERE
ARE
TWO
MAIN
GLOBAL
CONSTRAINTS
IMPOSED
ON
A
SOLID
SHAPE
BY
ITS
IMAGE
CONTOURS
A
IT
LIES
IN
THE
VOLUME
DEFINED
BY
THE
INTERSECTION
OF
THE
VIEWING
CONES
ATTACHED
TO
EACH
IMAGE
AND
B
THE
CONES
ARE
TANGENT
TO
ITS
SURFACE
THERE
ARE
OTHER
LOCAL
CONSTRAINTS
E
G
AS
SHOWN
IN
CHAPTER
CONVEX
RESP
CONCAVE
PARTS
OF
THE
CONTOUR
ARE
THE
PROJECTIONS
OF
CONVEX
RESP
SADDLE
SHAPED
PARTS
OF
THE
SURFACE
BAUMGART
EXPLOITED
THE
FIRST
OF
THESE
CONSTRAINTS
IN
HIS
PHD
THESIS
TO
CONSTRUCT
POLYHEDRAL
MODELS
OF
VARIOUS
OBJECTS
BY
INTERSECTING
THE
POLYHEDRAL
CONES
ASSOCIATED
WITH
POLYGONAL
APPROXIMATIONS
OF
THEIR
SILHOUETTES
HIS
IDEAS
HAVE
INSPIRED
A
NUMBER
OF
APPROACHES
TO
OBJECT
MODELING
FROM
SILHOUETTES
INCLUDING
THE
TECHNIQUE
PRESENTED
IN
THE
REST
OF
THIS
SECTION
SULLIVAN
AND
PONCE
THAT
ALSO
INCORPORATES
THE
TANGENCY
CONSTRAINT
ASSOCIATED
WITH
THE
VIEWING
CONES
AS
IN
BAUMGART
SYSTEM
A
POLYHEDRAL
APPROXIMATION
OF
THE
OBSERVED
OBJECT
IS
FIRST
CONSTRUCTED
BY
INTERSECTING
THE
VISUAL
CONES
ASSOCIATED
WITH
A
FEW
PHO
TOGRAPHS
FIGURE
THE
VERTICES
OF
THIS
POLYHEDRON
ARE
THEN
USED
AS
THE
CONTROL
POINTS
OF
A
SMOOTH
SPLINE
SURFACE
WHICH
IS
DEFORMED
UNTIL
IT
IS
TANGENT
TO
THE
VISUAL
RAYS
WE
FOCUS
HERE
ON
THE
CONSTRUCTION
AND
DEFORMATION
OF
THIS
SURFACE
SPLINE
CONSTRUCTION
A
SPLINE
CURVE
IS
A
PIECEWISE
POLYNOMIAL
PARAMETRIC
CURVE
THAT
SATISFIES
CERTAIN
SMOOTHNESS
CONDITIONS
FOR
EXAMPLE
IT
MAY
BE
CK
I
E
DIFFERENTIABLE
WITH
CONTINUOUS
DERIVATIVES
OF
ORDER
UP
TO
K
WITH
K
USUALLY
TAKEN
TO
BE
OR
OR
GK
I
E
NOT
NECESSARILY
DIFFERENTIABLE
EVERYWHERE
BUT
WITH
CONTINUOUS
TANGENTS
IN
THE
CASE
AND
CONTINUOUS
CURVATURES
IN
THE
CASE
SPLINE
CURVES
ARE
USUALLY
CONSTRUCTED
BY
STITCHING
TOGETHER
BE
ZIER
ARCS
A
BE
ZIER
CURVE
OF
DEGREE
N
IS
A
POLYNOMIAL
PARAMETRIC
CURVE
P
DEFINED
AS
THE
BARYCENTRIC
APPLICATION
IMAGE
BASED
RENDERING
CHAP
B
C
FIGURE
CONSTRUCTING
OBJECT
MODELS
BY
INTERSECTING
POLYHEDRAL
VIEWING
CONES
A
SIX
PHOTOGRAPHS
OF
A
TEAPOT
B
THE
RAW
INTERSECTION
OF
THE
CORRESPOND
ING
VIEWING
CONES
C
THE
TRIANGULATION
OBTAINED
BY
SPLITTING
EACH
FACE
INTO
TRI
ANGLES
AND
SIMPLIFYING
THE
RESULTING
MESH
REPRINTED
FROM
AUTOMATIC
MODEL
CONSTRUCTION
POSE
ESTIMATION
AND
OBJECT
RECOGNITION
FROM
PHOTOGRAPHS
USING
TRIANGULAR
SPLINES
BY
SULLIVAN
AND
J
PONCE
IEEE
TRANSACTIONS
ON
PATTERN
ANALYSIS
AND
MACHINE
INTELLIGENCE
OC
IEEE
COMBINATION
N
N
P
T
BI
T
PI
I
OF
N
CONTROL
POINTS
PN
WHERE
THE
WEIGHTS
B
N
T
D
EF
N
TI
T
N
I
ARE
CALLED
THE
BERNSTEIN
POLYNOMIALS
OF
DEGREE
N
I
I
BUT
NOT
THE
OTHER
ONES
FIGURE
AS
SHOWN
IN
THE
EXERCISES
THE
TANGENTS
AT
ITS
ENDPOINTS
ARE
ALONG
THE
FIRST
AND
LAST
LINE
SEGMENTS
OF
THE
CONTROL
POLYGON
FORMED
BY
THE
CONTROL
POINTS
THE
DEFINITION
OF
BE
ZIER
ARCS
AND
SPLINE
CURVES
NATURALLY
EXTENDS
TO
SURFACES
A
TRIANGU
LAR
BE
ZIER
PATCH
OF
DEGREE
N
IS
A
PARAMETRIC
SURFACE
P
DEFINED
AS
THE
BARYCENTRIC
COMBINATION
P
U
V
I
J
K
N
B
N
U
V
U
V
PIJK
OF
A
TRIANGULAR
ARRAY
OF
CONTROL
POINTS
PIJK
WHERE
THE
HOMOGENEOUS
POLYNOMIALS
B
N
U
V
W
D
EF
I
N
UIV
JWK
ARE
THE
TRIVARIATE
BERNSTEIN
POLYNOMIALS
OF
DEGREE
N
IN
THE
REST
OF
THIS
SECTION
WE
J
K
QUARTIC
BE
ZIER
PATCHES
N
EACH
DEFINED
BY
CONTROL
POINTS
FIGURE
THEIR
BOUNDARIES
ARE
THE
QUARTIC
BE
ZIER
CURVES
P
U
P
V
AND
P
U
U
BY
DEFINITION
A
TRIANGULAR
SPLINE
IS
A
NETWORK
OF
TRIANGULAR
BE
ZIER
PATCHES
THAT
SHARE
THE
SAME
TANGENT
PLANE
ALONG
THEIR
COMMON
BOUNDARIES
A
NECESSARY
BUT
NOT
SUFFICIENT
CONDITION
FOR
CONTINUITY
IS
THAT
ALL
CONTROL
POINTS
SURROUNDING
A
COMMON
VERTEX
BE
COPLANAR
WE
FIRST
CONSTRUCT
THESE
POINTS
THEN
PLACE
THE
REMAINING
CONTROL
POINTS
TO
ENSURE
THAT
THE
RESULTING
SPLINE
IS
INDEED
CONTINUOUS
AS
DISCUSSED
IN
LOOP
A
SET
OF
COPLANAR
POINTS
QP
CAN
BE
CREATED
AS
A
BARYCENTRIC
IS
INDEED
A
BARYCENTRIC
COMBINATION
AS
DEFINED
IN
CHAPTER
SINCE
THE
BERNSTEIN
POLYNOMIALS
ARE
EASILY
SHOWN
TO
ALWAYS
ADD
TO
IN
PARTICULAR
BE
ZIER
CURVES
ARE
AFFINE
CONSTRUCTS
A
DESIRABLE
PROPERTY
SINCE
IT
ALLOWS
THE
DEFINITION
OF
THESE
CURVES
PURELY
IN
TERMS
OF
THEIR
CONTROL
POINTS
AND
INDEPENDENTLY
OF
THE
CHOICE
OF
ANY
EXTERNAL
COORDINATE
SYSTEM
SEC
CONSTRUCTING
MODELS
FROM
IMAGE
SEQUENCES
A
B
FIGURE
BE
ZIER
CURVES
AND
PATCHES
A
A
CUBIC
BE
ZIER
CURVE
AND
ITS
CON
TROL
POLYGON
B
A
QUARTIC
TRIANGULAR
BE
ZIER
PATCH
AND
ITS
CONTROL
MESH
TENSOR
PRODUCT
BE
ZIER
PATCHES
CAN
ALSO
BE
DEFINED
USING
A
RECTANGULAR
ARRAY
OF
CONTROL
POINTS
FARIN
TRIANGULAR
PATCHES
ARE
HOWEVER
MORE
APPROPRIATE
FOR
MOD
ELING
FREE
FORM
CLOSED
SURFACES
T
V
CUBIC
CONTROL
POINTS
AI
INTERIOR
QUARTIC
CONTROL
POINTS
DEGREE
RAISED
QUARTIC
CONTROL
POINTS
FIGURE
CONSTRUCTION
OF
A
TRIANGULAR
SPLINE
OVER
A
TRIANGULAR
POLYHEDRAL
MESH
TOP
FROM
LEFT
TO
RIGHT
THE
CUBIC
BOUNDARY
CONTROL
POINTS
THE
BOUNDARY
CURVES
SURROUNDING
A
MESH
VERTEX
AND
THE
CONSTRUCTION
OF
INTERNAL
CONTROL
POINTS
FROM
TANGENT
SPECIFICATION
BOTTOM
SPLITTING
A
PATCH
THREE
WAYS
TO
ENFORCE
CONTINUITY
THE
WHITE
POINTS
ARE
THE
CONTROL
POINTS
OBTAINED
BY
RAISING
THE
DE
GREE
OF
THE
CONTROL
CURVES
AND
THE
GRAY
POINTS
ARE
THE
REMAINING
CONTROL
POINTS
COMPUTED
TO
ENSURE
CONTINUITY
AFTER
SULLIVAN
AND
PONCE
APPLICATION
IMAGE
BASED
RENDERING
CHAP
COMBINATION
OF
P
OTHER
POINTS
CP
IN
GENERAL
POSITION
IN
OUR
CASE
THE
CENTROIDS
OF
THE
P
TRIANGLES
TJ
ADJACENT
TO
A
VERTEX
V
OF
THE
INPUT
TRIANGULATION
FIGURE
TOP
LEFT
AS
Q
COS
Π
COS
I
J
I
Π
C
THIS
CONSTRUCTION
PLACES
THE
POINTS
QI
IN
A
PLANE
PASSING
THROUGH
THE
CENTROID
O
OF
THE
POINTS
CI
TRANSLATING
THIS
PLANE
SO
THAT
O
COINCIDES
WITH
V
YIELDS
A
NEW
SET
OF
POINTS
AI
LYING
IN
A
PLANE
PASSING
THROUGH
V
FIGURE
TOP
CENTER
SINCE
CUBIC
BE
ZIER
CURVES
ARE
DEFINED
BY
FOUR
POINTS
WE
CAN
INTERPRET
TWO
ADJACENT
VERTICES
V
AND
V
I
AND
THE
POINTS
AI
AND
AII
ASSOCIATED
WITH
THE
CORRESPONDING
EDGE
AS
THE
CONTROL
POINTS
OF
A
CUBIC
CURVE
THIS
YIELDS
A
SET
OF
CUBIC
ARCS
THAT
INTERPOLATE
THE
VERTICES
OF
THE
CONTROL
MESH
AND
FORM
THE
BOUNDARIES
OF
TRIANGULAR
PATCHES
ONCE
THESE
CURVES
HAVE
BEEN
CONSTRUCTED
THE
CONTROL
POINTS
ON
BOTH
SIDES
OF
A
BOUNDARY
CAN
BE
CHOSEN
TO
SATISFY
INTERPATCH
CONTINUITY
IN
THIS
CON
STRUCTION
THE
CROSS
BOUNDARY
TANGENT
FIELD
LINEARLY
INTERPOLATES
THE
TANGENTS
AT
THE
TWO
ENDPOINTS
OF
THE
BOUNDARY
CURVE
AT
THE
ENDPOINT
V
THE
TANGENT
T
ACROSS
THE
CURVE
THAT
CONTAINS
THE
POINT
AI
IS
TAKEN
TO
BE
PARALLEL
TO
THE
LINE
JOINING
AI
TO
AI
THE
TANGENT
TI
IS
OBTAINED
BY
A
SIMILAR
CON
STRUCTION
THE
INTERIOR
CONTROL
POINTS
F
F
I
G
AND
GI
FIGURE
TOP
RIGHT
ARE
CONSTRUCTED
BY
SOLVING
THE
SET
OF
LINEAR
EQUATIONS
ASSOCIATED
WITH
THIS
GEOMETRIC
CONDITION
CHIYOKURA
HOWEVER
THERE
ARE
NOT
ENOUGH
DEGREES
OF
FREEDOM
IN
A
QUARTIC
PATCH
TO
ALLOW
THE
SIMULTANE
OUS
SETTING
OF
THE
INTERIOR
POINTS
FOR
ALL
THREE
BOUNDARIES
THUS
EACH
PATCH
MUST
BE
SPLIT
THREE
WAYS
USING
FOR
EXAMPLE
THE
METHOD
OF
SHIRMAN
AND
SEQUIN
TO
ENSURE
CONTINUITY
AMONG
THE
NEW
PATCHES
PERFORMING
DEGREE
ELEVATION
ON
THE
BOUNDARY
CURVES
REPLACES
THEM
BY
QUARTIC
BE
ZIER
CURVES
WITH
THE
SAME
SHAPE
SEE
EXERCISES
THREE
QUARTIC
TRIANGULAR
PATCHES
CAN
THEN
BE
CONSTRUCTED
FROM
THE
BOUNDARIES
AS
SHOWN
IN
FIGURE
BOTTOM
THE
RESULT
IS
A
SET
OF
THREE
QUARTIC
PATCHES
FOR
EACH
MESH
FACE
WHICH
ARE
CONTINUOUS
ACROSS
ALL
BOUNDARIES
SPLINE
DEFORMATION
WE
HAVE
GIVEN
A
METHOD
FOR
CONSTRUCTING
A
CONTINUOUS
TRIANGULAR
SPLINE
APPROXIMATION
OF
A
SURFACE
FROM
A
TRIANGULATION
SUCH
AS
THE
ONE
SHOWN
IN
FIGURE
B
LET
US
NOW
SHOW
HOW
TO
DEFORM
THIS
SPLINE
TO
ENSURE
THAT
IT
IS
TANGENT
TO
THE
VIEWING
CONES
ASSOCIATED
WITH
THE
INPUT
PHOTOGRAPHS
THE
SHAPE
OF
THE
SPLINE
SURFACE
IS
DETERMINED
BY
THE
POSITION
OF
ITS
CONTROL
VERTICES
VP
WE
DENOTE
BY
VJK
K
THE
COORDINATES
OF
THE
POINT
VJ
J
P
IN
SOME
REFERENCE
EUCLIDEAN
COORDINATE
SYSTEM
AND
USE
THESE
P
COEFFICIENTS
AS
SHAPE
PARAMETERS
GIVEN
A
SET
OF
RAYS
RQ
WE
MINIMIZE
THE
ENERGY
FUNCTION
Q
R
RI
Λ
PUU
PUV
PVV
DU
DV
WITH
RESPECT
TO
THE
PARAMETERS
VJK
OF
HERE
D
R
DENOTES
THE
DISTANCE
BETWEEN
THE
RAY
R
AND
THE
SURFACE
THE
INTEGRAL
IS
A
THIN
PLATE
SPLINE
ENERGY
TERM
USED
TO
ENFORCE
SMOOTHNESS
IN
AR
EAS
OF
SPARSE
DATA
AND
Λ
IS
A
CONSTANT
WEIGHT
INTRODUCED
TO
BALANCE
THE
DISTANCE
AND
SMOOTHNESS
TERMS
THE
VARIABLES
U
AND
V
IN
THIS
INTEGRAL
ARE
THE
PATCH
PARAMETERS
AND
THE
SUMMATION
IS
DONE
OVER
THE
R
PATCHES
THAT
FORM
THE
SPLINE
SURFACE
THE
SIGNED
DISTANCE
BETWEEN
A
RAY
AND
A
SURFACE
PATCH
CAN
BE
COMPUTED
USING
NEWTON
METHOD
FOR
RAYS
THAT
DO
NOT
INTERSECT
THE
SURFACE
WE
DE
FINE
D
R
MIN
Q
P
Q
R
P
AND
COMPUTE
THE
DISTANCE
BY
MINIMIZING
Q
P
FOR
THOSE
RAYS
THAT
INTERSECT
THE
SURFACE
WE
FOLLOW
BRUNIE
LAVALLE
E
AND
SZELISKI
AND
MEA
SURE
THE
DISTANCE
TO
THE
FARTHEST
POINT
FROM
THE
RAY
THAT
LIES
ON
THE
SURFACE
IN
THE
DIRECTION
OF
THE
SURFACE
NORMAL
AT
THE
CORRESPONDING
OCCLUDING
CONTOUR
POINT
IN
BOTH
CASES
NEWTON
ITERATIONS
ARE
INITIALIZED
FROM
A
SAMPLING
OF
THE
SURFACE
DURING
SURFACE
FITTING
THE
SPLINE
IS
DEFORMED
SEC
CONSTRUCTING
MODELS
FROM
IMAGE
SEQUENCES
FIGURE
SHADED
AND
TEXTURE
MAPPED
MODELS
OF
A
TEAPOT
GARGOYLE
AND
DI
NOSAUR
THE
TEAPOT
WAS
CONSTRUCTED
FROM
SIX
REGISTERED
PHOTOGRAPHS
THE
GAR
GOYLE
AND
DINOSAUR
MODELS
WERE
EACH
BUILT
FROM
NINE
IMAGES
REPRINTED
FROM
AUTOMATIC
MODEL
CONSTRUCTION
POSE
ESTIMATION
AND
OBJECT
RECOGNITION
FROM
PHOTOGRAPHS
USING
TRIANGULAR
SPLINES
BY
SULLIVAN
AND
J
PONCE
IEEE
TRANSACTIONS
ON
PATTERN
ANALYSIS
AND
MACHINE
INTELLIGENCE
OC
IEEE
TO
MINIMIZE
THE
MEAN
SQUARED
RAY
SURFACE
DISTANCE
USING
A
SIMPLE
GRADIENT
DESCENT
TECHNIQUE
ALTHOUGH
EACH
DISTANCE
IS
COMPUTED
NUMERICALLY
ITS
DERIVATIVES
WITH
RESPECT
TO
THE
SURFACE
PA
RAMETERS
VJK
ARE
EASILY
COMPUTED
BY
DIFFERENTIATING
THE
CONSTRAINTS
SATISFIED
BY
THE
SURFACE
AND
RAY
POINTS
WHERE
THE
DISTANCE
IS
REACHED
THE
THREE
OBJECT
MODELS
SHOWN
IN
FIGURE
HAVE
BEEN
CONSTRUCTED
USING
THE
METHOD
DESCRIBED
IN
THIS
SECTION
THIS
TECHNIQUE
DOES
NOT
REQUIRE
ESTABLISHING
ANY
CORRESPONDENCE
ACROSS
THE
INPUT
PICTURES
BUT
ITS
SCOPE
IS
CURRENTLY
LIMITED
TO
STATIC
SCENES
IN
CONTRAST
THE
APPROACH
PRESENTED
NEXT
IS
BASED
ON
MULTICAMERA
STEREOPSIS
AND
AS
SUCH
REQUIRES
CORRESPONDENCES
BUT
IT
HANDLES
DYNAMIC
SCENES
AS
WELL
AS
STATIC
ONES
VIRTUALIZED
REALITY
KANADE
AND
HIS
COLLEAGUES
HAVE
PROPOSED
THE
CONCEPT
OF
VIRTUALIZED
REALITY
AS
A
NEW
VISUAL
MEDIUM
FOR
MANIPULATING
AND
RENDERING
PRERECORDED
AND
SYNTHETIC
IMAGES
OF
REAL
SCENES
CAPTURED
IN
A
CONTROLLED
ENVIRONMENT
THE
FIRST
PHYSICAL
IMPLE
MENTATION
OF
THIS
CONCEPT
AT
CARNEGIE
MELLON
UNIVERSITY
CONSISTED
OF
A
GEODESIC
DOME
EQUIPPED
WITH
SYNCHRONIZED
VIDEO
CAMERAS
HOOKED
TO
CONSUMER
GRADE
VCRS
AS
OF
THIS
WRITING
THE
LATEST
IMPLEMENTATION
IS
A
ROOM
WHERE
A
VOLUME
OF
CUBIC
FEET
IS
OBSERVED
BY
COLOR
CAMERAS
CONNECTED
TO
A
PC
CLUSTER
AND
REGISTERED
IN
THE
SAME
WORLD
COORDINATE
SYS
TEM
WITH
THE
CAPABILITY
OF
DIGITIZING
IN
REAL
TIME
THE
SYNCHRONIZED
VIDEO
STREAMS
OF
ALL
CAMERAS
THREE
DIMENSIONAL
SCENE
MODELS
ARE
ACQUIRED
BY
FUSING
DENSE
DEPTH
MAPS
ACQUIRED
VIA
MULTIPLE
CAMERA
STEREO
SEE
OKUTAMI
AND
KANADE
CHAPTER
ONE
SUCH
MAP
IS
ACQUIRED
BY
EACH
CAMERA
AND
A
SMALL
NUMBER
OF
ITS
NEIGHBORS
BETWEEN
THREE
AND
SIX
EVERY
RANGE
IMAGE
IS
THEN
APPLICATION
IMAGE
BASED
RENDERING
CHAP
FIGURE
MULTICAMERA
STEREO
FROM
LEFT
TO
RIGHT
THE
RANGE
MAP
ASSOCIATED
WITH
A
CLUSTER
OF
CAMERAS
A
TEXTURE
MAPPED
IMAGE
OF
THE
CORRESPONDING
MESH
OBSERVED
FROM
A
DIFFERENT
VIEWPOINT
NOTE
THE
DARK
AREAS
ASSOCIATED
WITH
DEPTH
DISCONTINUITIES
IN
THE
MAP
A
TEXTURE
MAPPED
IMAGE
CONSTRUCTED
FROM
TWO
ADJA
CENT
CAMERA
CLUSTERS
NOTE
THAT
THE
GAPS
HAVE
BEEN
FILLED
REPRINTED
FROM
VIR
TUALIZED
REALITY
CONSTRUCTING
VIRTUAL
WORLDS
FROM
REAL
SCENES
BY
T
KANADE
RANDER
AND
J
P
NARAYANAN
IEEE
MULTIMEDIA
C
IEEE
B
FIGURE
VIRTUALIZED
REALITY
A
A
SEQUENCE
OF
SYNTHETIC
IMAGES
NOTE
THAT
OCCLUSION
IN
THE
TWO
ELLIPTICAL
REGIONS
OF
THE
FIRST
VIEW
IS
HANDLED
CORRECTLY
THE
CORRESPONDING
MESH
MODEL
REPRINTED
FROM
APPEARANCE
BASED
VIRTUAL
VIEW
GENERATION
OF
TEMPORALLY
VARYING
EVENTS
FROM
MULTI
CAMERA
IMAGES
IN
THE
ROOM
BY
H
SAITO
BABA
M
KIMURA
VEDULA
AND
T
KANADE
TECH
REP
CMU
CS
SCHOOL
OF
COMPUTER
SCIENCE
CARNEGIE
MELLON
UNIVER
SITY
SEC
CONSTRUCTING
MODELS
FROM
IMAGE
SEQUENCES
CONVERTED
TO
A
SURFACE
MESH
THAT
CAN
BE
RENDERED
USING
CLASSICAL
COMPUTER
GRAPHICS
TECHNIQUES
SUCH
AS
TEXTURE
MAPPING
AS
SHOWN
BY
FIGURE
IMAGES
OF
A
SCENE
CONSTRUCTED
FROM
A
SINGLE
DEPTH
MAP
MAY
EXHIBIT
GAPS
THESE
GAPS
CAN
BE
FILLED
BY
RENDERING
IN
THE
SAME
IMAGE
THE
MESHES
CORRESPONDING
TO
ADJACENT
CAMERAS
IT
IS
ALSO
POSSIBLE
TO
DIRECTLY
MERGE
THE
SURFACE
MESHES
ASSOCIATED
WITH
DIFFERENT
CAMERAS
INTO
A
SINGLE
SURFACE
MODEL
THIS
TASK
IS
CHALLENGING
SINCE
A
MULTIPLE
CONFLICTING
MEASURE
MENTS
OF
THE
SAME
SURFACE
PATCHES
ARE
AVAILABLE
IN
AREAS
WHERE
THE
FIELDS
OF
VIEW
OF
SEVERAL
CAMERAS
OVERLAP
AND
B
CERTAIN
SCENE
PATCHES
ARE
NOT
OBSERVED
BY
ANY
CAMERA
BOTH
PROBLEMS
CAN
BE
SOLVED
USING
THE
VOLUMETRIC
TECHNIQUE
FOR
RANGE
IMAGE
FUSION
PROPOSED
BY
CURLESS
AND
LEVOY
AND
INTRODUCED
IN
CHAPTER
ONCE
A
GLOBAL
SURFACE
MODEL
HAS
BEEN
CONSTRUCTED
IT
CAN
OF
COURSE
BE
TEXTURE
MAPPED
AS
BEFORE
SYNTHETIC
ANIMATIONS
CAN
ALSO
BE
OBTAINED
BY
INTER
POLATING
TWO
ARBITRARY
VIEWS
IN
THE
INPUT
SEQUENCE
FIRST
THE
SURFACE
MODEL
IS
USED
TO
ESTABLISH
CORRESPONDENCES
BETWEEN
THESE
TWO
VIEWS
THE
OPTICAL
RAY
PASSING
THROUGH
ANY
POINT
IN
THE
FIRST
IMAGE
IS
INTERSECTED
WITH
THE
MESH
AND
THE
INTERSECTION
POINT
IS
REPROJECTED
IN
THE
SECOND
IMAGE
YIELDING
THE
DESIRED
MATCH
ONCE
THE
CORRESPONDENCES
ARE
KNOWN
NEW
VIEWS
ARE
CONSTRUCTED
BY
LINEARLY
INTERPOLATING
BOTH
THE
POSITIONS
AND
COLORS
OF
MATCHING
POINTS
AS
DISCUSSED
IN
SAITO
ET
AL
THIS
SIMPLE
ALGORITHM
ONLY
PROVIDES
AN
APPROXIMATION
OF
TRUE
PERSPECTIVE
IMAGING
AND
ADDITIONAL
LOGIC
HAS
TO
BE
ADDED
IN
PRACTICE
TO
HANDLE
POINTS
THAT
ARE
VISIBLE
IN
THE
FIRST
IMAGE
BUT
NOT
IN
THE
SECOND
ONE
NEVERTHELESS
IT
CAN
BE
USED
TO
GENERATE
REALISTIC
ANIMATIONS
OF
DYNAMIC
SCENES
WITH
CHANGING
OCCLUSION
PATTERNS
AS
DEMONSTRATED
BY
FIGURE
SCENE
MODELING
FROM
UNREGISTERED
IMAGES
THIS
SECTION
ADDRESSES
AGAIN
THE
PROBLEM
OF
ACQUIRING
AND
RENDERING
THREE
DIMENSIONAL
OBJECT
MODELS
FROM
A
SET
OF
IMAGES
BUT
THIS
TIME
THE
POSITIONS
OF
THE
CAMERAS
OBSERVING
THE
SCENE
ARE
NOT
KNOWN
A
PRIORI
AND
MUST
BE
RECOVERED
FROM
IMAGE
INFORMATION
USING
METHODS
RELATED
TO
THOSE
PRESENTED
IN
CHAPTERS
AND
THE
TECHNIQUES
PRESENTED
IN
THIS
SECTION
ARE
HOWEVER
EXPLICITLY
GEARED
TOWARD
COMPUTER
GRAPHICS
APPLICATIONS
THE
FAC
ADE
SYSTEM
THE
FAC
ADE
SYSTEM
FOR
MODELING
AND
RENDERING
ARCHITECTURAL
SCENES
FROM
DIGITIZED
PHOTOGRAPHS
WAS
DEVELOPED
AT
UC
BERKELEY
BY
DEBEVEC
TAYLOR
AND
MA
LIK
THIS
SYSTEM
TAKES
ADVANTAGE
OF
THE
RELATIVELY
SIMPLE
OVERALL
GEOMETRY
OF
MANY
BUILD
INGS
TO
SIMPLIFY
THE
ESTIMATION
OF
SCENE
STRUCTURE
AND
CAMERA
MOTION
AND
IT
USES
THE
SIMPLE
BUT
POWERFUL
IDEA
OF
MODEL
BASED
STEREOPSIS
TO
BE
DESCRIBED
IN
A
MINUTE
TO
ADD
DETAIL
TO
ROUGH
BUILDING
OUTLINES
FIGURE
SHOWS
AN
EXAMPLE
FAC
ADE
MODELS
ARE
CONSTRAINED
HIERARCHIES
OF
PARAMETRIC
PRIMITIVES
SUCH
AS
BOXES
PRISMS
AND
SOLIDS
OF
REVOLUTION
THESE
PRIMITIVES
ARE
DEFINED
BY
A
SMALL
NUMBER
OF
COEFFICIENTS
E
G
THE
HEIGHT
WIDTH
AND
BREADTH
OF
A
BOX
AND
RELATED
TO
EACH
OTHER
BY
RIGID
TRANSFORMATIONS
ANY
OF
THE
PARAMETERS
DEFINING
A
MODEL
IS
EITHER
A
CONSTANT
OR
VARIABLE
AND
CONSTRAINTS
CAN
BE
SPECIFIED
BETWEEN
THE
VARIOUS
UNKNOWNS
E
G
TWO
BLOCKS
MAY
BE
CONSTRAINED
TO
HAVE
THE
SAME
HEIGHT
MODEL
HIERARCHIES
ARE
DEFINED
INTERACTIVELY
WITH
A
GRAPHICAL
USER
INTERFACE
AND
THE
MAIN
COM
PUTATIONAL
TASK
OF
THE
FAC
ADE
SYSTEM
IS
TO
USE
IMAGE
INFORMATION
TO
ASSIGN
DEFINITE
VALUES
TO
THE
UNKNOWN
MODEL
PARAMETERS
THE
OVERALL
SYSTEM
IS
DIVIDED
INTO
THREE
MAIN
COMPONENTS
THE
FIRST
ONE
OR
PHOTOGRAMMETRIC
MODULE
RECASTS
STRUCTURE
AND
MOTION
ESTIMATION
AS
THE
NONLINEAR
OPTIMIZATION
PROBLEM
OF
MINIMIZING
THE
DISCREPENCY
BETWEEN
LINE
SEGMENTS
SELECTED
BY
HAND
IN
THE
PHOTOGRAPHS
AND
THE
PROJECTIONS
OF
THE
CORRESPONDING
PARTS
OF
THE
PARAMETRIC
MODEL
SEE
EXERCISES
FOR
DETAILS
AS
SHOWN
IN
DEBEVEC
ET
AL
THIS
PROCESS
INVOLVES
RELATIVELY
FEW
NARROW
BASELINE
METHODS
LIKE
CORRELATION
WOULD
BE
INEFFECTIVE
IN
THIS
CONTEXT
SINCE
THE
TWO
VIEWS
MAY
BE
FAR
FROM
EACH
OTHER
A
SIMILAR
METHOD
IS
USED
IN
THE
FAC
ADE
SYSTEM
DESCRIBED
LATER
IN
THIS
CHAPTER
TO
ESTABLISH
CORRESPONDENCES
BETWEEN
WIDELY
SEPARATED
IMAGES
WHEN
THE
ROUGH
SHAPE
OF
THE
OBSERVED
SURFACE
IS
KNOWN
APPLICATION
IMAGE
BASED
RENDERING
CHAP
FIGURE
FAC
ADE
MODEL
OF
THE
BERKELEY
CAMPANILE
FROM
LEFT
TO
RIGHT
A
PHOTOGRAPH
OF
THE
CAMPANILE
WITH
SELECTED
EDGES
OVERLAID
THE
MODEL
RECOVERED
BY
PHOTOGRAMMETRIC
MODELING
REPROJECTION
OF
THE
MODEL
INTO
THE
PHOTOGRAPH
A
TEXTURE
MAPPED
VIEW
OF
THE
MODEL
REPRINTED
FROM
MODELING
AND
RENDERING
ARCHITECTURE
FROM
PHOTOGRAPHS
A
HYBRID
GEOMETRY
AND
IMAGE
BASED
APPROACH
BY
P
DEBEVEC
C
J
TAYLOR
AND
J
MALIK
PROC
SIGGRAPH
OC
ACM
INC
INCLUDED
HERE
BY
PERMISSION
VARIABLES
NAMELY
THE
POSITIONS
AND
ORIENTATIONS
OF
THE
CAMERAS
USED
TO
PHOTOGRAPH
A
BUILDING
AND
THE
PARAMETERS
OF
THE
BUILDING
MODEL
AND
WHEN
THE
ORIENTATION
OF
SOME
OF
THE
MODEL
EDGES
IS
FIXED
RELATIVE
TO
THE
WORLD
COORDINATE
SYSTEM
AN
INITIAL
ESTIMATE
FOR
THESE
PARAMETERS
IS
EASILY
FOUND
USING
LINEAR
LEAST
SQUARES
THE
SECOND
MAIN
COMPONENT
OF
FAC
ADE
IS
THE
VIEW
DEPENDENT
TEXTURE
MAPPING
MODULE
THAT
RENDERS
AN
ARCHITECTURAL
SCENE
BY
MAPPING
DIFFERENT
PHOTOGRAPHS
ONTO
ITS
GEOMETRIC
MODEL
ACCORDING
TO
THE
USER
VIEWPOINT
CONCEPTUALLY
THE
CAMERAS
ARE
REPLACED
BY
SLIDE
PROJECTORS
THAT
PROJECT
THE
ORIGINAL
IMAGES
ONTO
THE
MODEL
OF
COURSE
EACH
CAMERA
ONLY
SEES
A
PORTION
OF
A
BUILDING
AND
SEVERAL
PHOTOGRAPHS
MUST
BE
USED
TO
RENDER
A
COMPLETE
MODEL
IN
GENERAL
PARTS
OF
A
BUILDING
ARE
OBSERVED
BY
SEVERAL
CAMERAS
SO
THE
RENDERER
MUST
NOT
ONLY
PICK
BUT
ALSO
APPROPRIATELY
MERGE
THE
PICTURES
RELEVANT
TO
THE
SYNTHESIS
OF
A
VIRTUAL
VIEW
THE
SOLUTION
ADOPTED
IN
FAC
ADE
IS
TO
ASSIGN
TO
EACH
PIXEL
IN
A
NEW
IMAGE
A
WEIGHTED
AVERAGE
OF
THE
VALUES
PREDICTED
FROM
THE
OVERLAPPING
INPUT
PICTURES
WITH
WEIGHTS
INVERSELY
PROPORTIONAL
TO
THE
ANGLE
BETWEEN
THE
CORRESPONDING
LIGHT
RAYS
IN
THE
INPUT
AND
VIRTUAL
VIEWS
THE
LAST
COMPONENT
OF
FAC
ADE
IS
THE
MODEL
BASED
STEREOPSIS
MODULE
WHICH
USES
STEREO
PAIRS
TO
ADD
FINE
GEOMETRIC
DETAIL
TO
THE
RELATIVELY
ROUGH
SCENE
DESCRIPTION
CONSTRUCTED
BY
THE
PHOTOGRAMMETRIC
MODELING
MODULE
THE
MAIN
DIFFICULTY
IN
USING
STEREO
VISION
IN
THIS
SETTING
IS
THE
WIDE
SEPARATION
OF
THE
CAMERAS
WHICH
PREVENTS
THE
STRAIGHTFORWARD
USE
OF
CORRELATION
BASED
MATCHING
TECHNIQUES
THE
SOLUTION
ADOPTED
IN
FAC
ADE
IS
TO
EXPLOIT
A
PRIORI
SHAPE
INFORMATION
TO
MAP
THE
STEREO
IMAGES
INTO
THE
SAME
REFERENCE
FRAME
FIGURE
TOP
SPECIFICALLY
GIVEN
KEY
AND
OFFSET
PICTURES
THE
OFFSET
IMAGE
CAN
BE
PROJECTED
ONTO
THE
SCENE
MODEL
BEFORE
BEING
RENDERED
FROM
THE
KEY
CAMERA
VIEWPOINT
YIELDING
A
WARPED
OFFSET
PICTURE
SIMILAR
TO
THE
KEY
IMAGE
FIG
URE
BOTTOM
IN
TURN
THIS
ALLOWS
THE
USE
OF
CORRELATION
TO
ESTABLISH
CORRESPONDENCES
BETWEEN
THESE
TWO
IMAGES
AND
THUS
BETWEEN
THE
KEY
AND
OFFSET
IMAGES
AS
WELL
ONCE
THE
MATCHES
BETWEEN
THESE
TWO
PICTURES
HAVE
BEEN
ESTABLISHED
STEREO
RECONSTRUCTION
REDUCES
TO
THE
USUAL
TRIANGULATION
PROCESS
SEC
TRANSFER
BASED
APPROACHES
TO
IMAGE
BASED
RENDERING
FIGURE
MODEL
BASED
STEREOPSIS
TOP
SYNTHESIS
OF
A
WARPED
OFFSET
IMAGE
THE
POINT
PI
IN
THE
OFFSET
IMAGE
IS
MAPPED
ONTO
THE
POINT
Q
OF
THE
SURFACE
MODEL
THEN
REPROJECTED
ONTO
THE
POINT
Q
OF
THE
WARPED
OFFSET
IMAGE
THE
ACTUAL
SURFACE
POINT
P
OBSERVED
BY
BOTH
CAMERAS
PROJECTS
ONTO
THE
POINT
P
OF
THE
KEY
IMAGE
NOTE
THAT
THE
POINT
Q
MUST
LIE
ON
THE
EPIPOLAR
LINE
EP
WHICH
FACILITATES
THE
SEARCH
FOR
MATCHES
AS
IN
THE
CONVENTIONAL
STEREO
CASE
NOTE
ALSO
THAT
THE
DISPARITY
BE
TWEEN
P
AND
Q
ALONG
THE
EPIPOLAR
LINE
MEASURES
THE
DISCREPANCY
BETWEEN
THE
MODELED
AND
ACTUAL
SURFACES
AFTER
DEBEVEC
ET
AL
FIGURE
BOTTOM
FROM
LEFT
TO
RIGHT
A
KEY
IMAGE
AN
OFFSET
IMAGE
AND
THE
CORRESPONDING
WARPED
OFFSET
IMAGE
REPRINTED
FROM
MODELING
AND
RENDERING
ARCHITECTURE
FROM
PHO
TOGRAPHS
A
HYBRID
GEOMETRY
AND
IMAGE
BASED
APPROACH
BY
P
DEBEVEC
C
J
TAYLOR
AND
J
MALIK
PROC
SIGGRAPH
C
ACM
INC
INCLUDED
HERE
BY
PERMISSION
TRANSFER
BASED
APPROACHES
TO
IMAGE
BASED
RENDERING
THIS
SECTION
EXPLORES
A
COMPLETELY
DIFFERENT
APPROACH
TO
IMAGE
BASED
RENDERING
IN
THIS
FRAME
WORK
AN
EXPLICIT
THREE
DIMENSIONAL
SCENE
RECONSTRUCTION
IS
NEVER
PERFORMED
INSTEAD
NEW
IMAGES
ARE
CREATED
DIRECTLY
FROM
A
POSSIBLY
SMALL
SET
OF
VIEWS
AMONG
WHICH
POINT
CORRESPONDENCES
HAVE
BEEN
ESTABLISHED
BY
FEATURE
TRACKING
OR
CONVENTIONAL
STEREO
MATCHING
THIS
APPROACH
IS
RELATED
TO
THE
CLASSICAL
TRANSFER
PROBLEM
FROM
PHOTOGRAMMETRY
ALREADY
MENTIONED
IN
CHAPTER
GIVEN
THE
IMAGE
POSITIONS
OF
A
NUMBER
OF
TIE
POINTS
IN
A
SET
OF
REFERENCE
IMAGES
AND
IN
A
NEW
IMAGE
AND
GIVEN
THE
IMAGE
POSITIONS
OF
A
GROUND
POINT
IN
THE
REFERENCE
IMAGES
PREDICT
THE
POSITION
OF
THAT
POINT
IN
THE
NEW
IMAGE
TRANSFER
BASED
TECHNIQUES
FOR
IMAGE
BASED
RENDERING
WERE
INTRODUCED
IN
THE
PROJECTIVE
SETTING
BY
LAVEAU
AND
FAUGERAS
WHO
PROPOSED
TO
FIRST
ESTIMATE
THE
PAIRWISE
EPIPOLAR
GE
OMETRY
BETWEEN
REFERENCE
VIEWS
THEN
REPROJECT
THE
SCENE
POINTS
INTO
A
VIRTUAL
IMAGE
SPECIFIED
BY
THE
PROJECTIONS
OF
THE
NEW
OPTICAL
CENTER
IN
TWO
REFERENCE
PICTURES
I
E
THE
EPIPOLES
AND
THE
POSITION
OF
FOUR
TIE
POINTS
IN
THE
NEW
VIEW
BY
DEFINITION
THE
EPIPOLAR
GEOMETRY
CONSTRAINS
THE
APPLICATION
IMAGE
BASED
RENDERING
CHAP
FIGURE
AUGMENTED
REALITY
EXPERIMENT
THE
AFFINE
WORLD
COORDINATE
SYSTEM
IS
DEFINED
BY
CORNERS
OF
THE
BLACK
POLYGONS
REPRINTED
FROM
CALIBRATION
FREE
AUGMENTED
REALITY
BY
K
KUTULAKOS
AND
J
VALLINO
IEEE
TRANSACTIONS
ON
VISUALIZATION
AND
COMPUTER
GRAPHICS
OC
IEEE
POSSIBLE
REPROJECTIONS
OF
POINTS
IN
THE
REFERENCE
IMAGES
IN
THE
NEW
VIEW
THE
PROJECTION
OF
THE
SCENE
POINT
IS
AT
THE
INTERSECTION
OF
THE
TWO
EPIPOLAR
LINES
ASSOCIATED
WITH
THE
POINT
AND
TWO
REF
ERENCE
PICTURES
ONCE
THE
FEATURE
POINTS
HAVE
BEEN
REPROJECTED
REALISTIC
PICTURES
ARE
SYNTHESIZED
USING
RAY
TRACING
AND
TEXTURE
MAPPING
AS
NOTED
BY
LAVEAU
AND
FAUGERAS
HOWEVER
SINCE
THE
EU
CLIDEAN
CONSTRAINTS
ASSOCIATED
WITH
CALIBRATED
CAMERAS
ARE
NOT
ENFORCED
THE
RENDERED
IMAGES
ARE
SEPARATED
FROM
CORRECT
PICTURES
BY
ARBITRARY
PLANAR
PROJECTIVE
TRANSFORMATIONS
UNLESS
ADDITIONAL
SCENE
CONSTRAINTS
ARE
TAKEN
INTO
ACCOUNT
THE
REST
OF
THIS
SECTION
EXPLORES
TWO
AFFINE
VARIANTS
OF
THE
TRANSFER
BASED
APPROACH
THAT
CIRCUMVENT
THIS
DIFFICULTY
BOTH
TECHNIQUES
CONSTRUCT
A
PARAM
ETERIZATION
OF
THE
SET
OF
ALL
IMAGES
OF
A
RIGID
SCENE
IN
THE
FIRST
CASE
SECTION
THE
AFFINE
STRUCTURE
OF
THE
SPACE
OF
AFFINE
IMAGES
IS
USED
TO
RENDER
SYNTHETIC
OBJECTS
IN
AN
AUGMENTED
RE
ALITY
SYSTEM
BECAUSE
THE
TIE
POINTS
IN
THIS
CASE
ARE
ALWAYS
GEOMETRICALLY
VALID
IMAGE
FEATURES
E
G
THE
CORNERS
OF
CALIBRATION
POLYGONS
SEE
FIGURE
THE
SYNTHESIZED
IMAGES
ARE
AUTOMAT
ICALLY
EUCLIDEAN
ONES
IN
THE
SECOND
INSTANCE
SECTION
THE
METRIC
CONSTRAINTS
ASSOCIATED
WITH
CALIBRATED
CAMERAS
ARE
EXPLICITLY
TAKEN
INTO
ACCOUNT
IN
THE
IMAGE
SPACE
PARAMETERIZATION
GUARANTEEING
ONCE
AGAIN
THE
SYNTHESIS
OF
CORRECT
EUCLIDEAN
IMAGES
LET
US
NOTE
AGAIN
A
PARTICULARITY
OF
TRANSFER
BASED
APPROACHES
TO
IMAGE
BASED
RENDERING
ALREADY
MENTIONED
IN
THE
INTRODUCTION
BECAUSE
NO
THREE
DIMENSIONAL
MODEL
IS
EVER
CONSTRUCTED
A
JOYSTICK
CANNOT
BE
USED
TO
CONTROL
THE
SYNTHESIS
OF
AN
ANIMATION
INSTEAD
THE
POSITION
OF
TIE
POINTS
MUST
BE
SPECIFIED
INTERACTIVELY
BY
A
USER
THIS
IS
NOT
A
PROBLEM
IN
AN
AUGMENTED
REALITY
CONTEXT
BUT
WHETHER
THIS
IS
A
VIABLE
USER
INTERFACE
FOR
VIRTUAL
REALITY
APPLICATIONS
REMAINS
TO
BE
SHOWN
AFFINE
VIEW
SYNTHESIS
HERE
WE
ADDRESS
THE
PROBLEM
OF
SYNTHESIZING
NEW
AFFINE
IMAGES
OF
A
SCENE
FROM
OLD
ONES
WITH
OUT
SETTING
AN
EXPLICIT
THREE
DIMENSIONAL
EUCLIDEAN
COORDINATE
SYSTEM
RECALL
FROM
CHAPTER
THAT
IF
WE
DENOTE
THE
COORDINATE
VECTOR
OF
A
SCENE
POINT
P
IN
SOME
WORLD
COORDINATE
SYSTEM
BY
P
X
Y
Z
T
AND
DENOTE
BY
P
U
V
T
THE
COORDINATE
VECTOR
OF
THE
PROJECTION
P
OF
P
ONTO
THE
IMAGE
PLANE
THE
AFFINE
CAMERA
MODEL
OF
EQ
CAN
BE
WRITTEN
AS
AT
WHERE
B
IS
THE
POSITION
OF
THE
PROJECTION
INTO
THE
IMAGE
OF
THE
OBJECT
COORDINATE
SYSTEM
ORIGIN
AND
AND
ARE
VECTORS
IN
SEC
TRANSFER
BASED
APPROACHES
TO
IMAGE
BASED
RENDERING
LET
US
CONSIDER
FOUR
NONCOPLANAR
SCENE
POINTS
SAY
AND
WE
CAN
CHOOSE
WITHOUT
LOSS
OF
GENERALITY
THESE
POINTS
AS
AN
AFFINE
REFERENCE
FRAME
SO
THEIR
COORDINATE
VECTORS
ARE
THE
POINTS
P
I
ARE
NOT
IN
GENERAL
AT
A
UNIT
DISTANCE
FROM
P
NOR
ARE
THE
VECTORS
PI
AND
P
PJ
ORTHOGONAL
TO
EACH
OTHER
WHEN
I
J
THIS
IS
IRRELEVANT
SINCE
WE
WORK
IN
AN
AFFINE
SETTING
SINCE
THE
MATRIX
WITH
COLUMNS
AND
IS
THE
IDENTITY
EQ
CAN
BE
REWRITTEN
AS
P
AP
B
Y
Z
B
FINALLY
SINCE
WE
HAVE
CHOSEN
AS
THE
ORIGIN
OF
THE
WORLD
COORDINATE
SYSTEM
WE
HAVE
B
AND
WE
OBTAIN
P
X
Y
Z
X
Y
Z
THIS
RESULT
IS
RELATED
TO
THE
AFFINE
STRUCTURE
OF
AFFINE
IMAGES
AS
DISCUSSED
IN
CHAPTER
IN
THE
CONTEXT
OF
IMAGE
BASED
RENDERING
IT
FOLLOWS
FROM
EQ
THAT
X
Y
AND
Z
CAN
BE
COMPUTED
FROM
M
IMAGES
OF
THE
POINTS
AND
P
THROUGH
LINEAR
LEAST
SQUARES
ONCE
THESE
VALUES
ARE
KNOWN
NEW
IMAGES
CAN
BE
SYNTHESIZED
BY
SPECIFYING
THE
IMAGE
POSITIONS
OF
THE
POINTS
AND
USING
EQ
TO
COMPUTE
ALL
THE
OTHER
POINT
POSITIONS
KUTULAKOS
AND
VALLINO
IN
ADDITION
SINCE
THE
AFFINE
REPRESENTATION
OF
THE
SCENE
IS
TRULY
THREE
DIMENSIONAL
THE
RELATIVE
DEPTH
OF
SCENE
POINTS
CAN
BE
COMPUTED
AND
USED
TO
ELIMINATE
HIDDEN
SURFACES
IN
THE
Z
BUFFER
PART
OF
THE
GRAPHICS
PIPELINE
IT
SHOULD
BE
NOTED
THAT
SPECIFYING
ARBITRARY
POSITIONS
FOR
THE
POINTS
GENERALLY
PRODUCES
AFFINELY
DEFORMED
PICTURES
THIS
IS
NOT
A
PROBLEM
IN
AUGMENTED
REALITY
APPLICATIONS
WHERE
GRAPHICAL
AND
PHYSICAL
OBJECTS
CO
EXIST
IN
THE
IMAGE
IN
THIS
CASE
THE
ANCHOR
POINTS
CAN
BE
CHOSEN
AMONG
TRUE
IMAGE
POINTS
GUARANTEED
TO
BE
IN
THE
CORRECT
EUCLIDEAN
POSITION
FIGURE
SHOWS
AN
EXAMPLE
WHERE
SYNTHETIC
OBJECTS
HAVE
BEEN
OVERLAID
ON
REAL
IMAGES
WHEN
LONGER
IMAGE
SEQUENCES
ARE
AVAILABLE
A
VARIANT
OF
THIS
APPROACH
THAT
TAKES
INTO
AC
COUNT
ALL
SCENE
POINTS
IN
A
UNIFORM
MANNER
CAN
BE
OBTAINED
AS
FOLLOWS
SUPPOSE
WE
OBSERVE
A
FIXED
SET
OF
POINTS
PN
WITH
COORDINATE
VECTORS
PI
I
N
AND
LET
PI
DENOTE
THE
COORDINATE
VECTORS
OF
THE
CORRESPONDING
IMAGE
POINTS
WRITING
EQ
FOR
ALL
THE
SCENE
POINTS
YIELDS
PT
PN
N
B
T
T
N
IN
OTHER
WORDS
THE
SET
OF
ALL
AFFINE
IMAGES
OF
N
FIXED
POINTS
IS
AN
EIGHT
DIMENSIONAL
VECTOR
SPACE
V
EMBEDDED
IN
AND
PARAMETERIZED
BY
THE
VECTORS
AND
B
GIVEN
M
VIEWS
DOES
NOT
CONTRADICT
THE
RESULT
ESTABLISHED
IN
CHAPTER
WHICH
STATES
THAT
THE
SET
OF
M
FIXED
VIEWS
OF
AN
ARBITRARY
COLLECTION
OF
POINTS
IS
A
THREE
DIMENSIONAL
AFFINE
SUBSPACE
OF
APPLICATION
IMAGE
BASED
RENDERING
CHAP
OF
N
POINTS
A
BASIS
FOR
THIS
VECTOR
SPACE
CAN
BE
IDENTIFIED
BY
PERFORMING
THE
SINGULAR
VALUE
DECOMPOSITION
OF
THE
M
MATRIX
M
P
N
P
M
WHERE
P
J
DENOTES
THE
POSITION
OF
THE
IMAGE
POINT
NUMBER
I
IN
FRAME
NUMBER
J
ONCE
A
BASIS
FOR
V
HAS
BEEN
CONSTRUCTED
NEW
IMAGES
CAN
BE
CONSTRUCTED
BY
ASSIGNING
ARBITRARY
VALUES
TO
AND
B
FOR
INTERACTIVE
IMAGE
SYNTHESIS
PURPOSES
A
MORE
INTUITIVE
CONTROL
OF
THE
IMAGING
GEOMETRY
CAN
BE
OBTAINED
BY
SPECIFYING
AS
BEFORE
THE
POSITION
OF
FOUR
IMAGE
POINTS
SOLVING
FOR
THE
CORRESPONDING
VALUES
OF
AND
B
AND
COMPUTING
THE
REMAINING
IMAGE
POSITIONS
EUCLIDEAN
VIEW
SYNTHESIS
AS
DISCUSSED
EARLIER
A
DRAWBACK
OF
THE
METHOD
PRESENTED
IN
THE
PREVIOUS
SECTION
IS
THAT
SPECIFYING
ARBITRARY
POSITIONS
FOR
THE
POINTS
GENERALLY
YIELDS
AFFINELY
DEFORMED
PICTURES
THIS
CAN
BE
AVOIDED
BY
TAKING
INTO
ACCOUNT
FROM
THE
START
THE
EUCLIDEAN
CONSTRAINTS
ASSOCIATED
WITH
CALIBRATED
CAMERAS
WE
SAW
IN
CHAPTER
THAT
A
WEAK
PERSPECTIVE
CAMERA
IS
AN
AFFINE
CAMERA
SATISFYING
THE
TWO
QUADRATIC
CONSTRAINTS
AND
THE
PREVIOUS
SECTION
SHOWED
THAT
THE
AFFINE
IMAGES
OF
A
FIXED
SCENE
FORM
AN
EIGHT
DIMENSIONAL
VECTOR
SPACE
V
NOW
IF
WE
RESTRICT
OUR
ATTENTION
TO
WEAK
PERSPECTIVE
CAMERAS
THE
SET
OF
IM
AGES
BECOMES
THE
SIX
DIMENSIONAL
SUBSPACE
DEFINED
BY
THESE
TWO
POLYNOMIAL
CONSTRAINTS
SIM
ILAR
CONSTRAINTS
APPLY
TO
PARAPERSPECTIVE
AND
TRUE
PERSPECTIVE
PROJECTION
AND
THEY
ALSO
DEFINE
A
SIX
DIMENSIONAL
VARIETY
I
E
A
SUBSPACE
DEFINED
BY
POLYNOMIAL
EQUATIONS
IN
EACH
CASE
LET
US
SUPPOSE
THAT
WE
OBSERVE
THREE
POINTS
WHOSE
IMAGES
ARE
NOT
COLLINEAR
WE
CAN
CHOOSE
WITHOUT
LOSS
OF
GENERALITY
A
EUCLIDEAN
COORDINATE
SYSTEM
SUCH
THAT
THE
COORDINATE
VECTORS
OF
THE
FOUR
POINTS
IN
THIS
SYSTEM
ARE
P
WHERE
P
AND
Q
ARE
NONZERO
BUT
A
PRIORI
UNKNOWN
LET
US
DENOTE
AS
BEFORE
BY
PI
THE
PROJECTION
OF
THE
POINT
PI
I
SINCE
IS
THE
ORIGIN
OF
THE
WORLD
COORDINATE
SYSTEM
WE
HAVE
B
WE
ARE
ALSO
FREE
TO
PICK
AS
THE
ORIGIN
OF
THE
IMAGE
COORDINATE
SYSTEM
THIS
AMOUNTS
TO
SUBMITTING
ALL
IMAGE
POINTS
TO
A
KNOWN
TRANSLATION
SO
EQ
SIMPLIFIES
INTO
AT
P
P
AP
NOW
APPLYING
EQ
TO
AND
P
YIELDS
DEF
DEF
U
U
U
AND
V
V
V
AT
LEAST
EIGHT
IMAGES
MAY
SEEM
LIKE
OVERKILL
SINCE
THE
AFFINE
STRUCTURE
OF
A
SCENE
CAN
BE
RECOVERED
FROM
TWO
PICTURES
AS
SHOWN
IN
CHAPTER
INDEED
AS
SHOWN
IN
THE
EXERCISES
A
BASIS
FOR
V
CAN
IN
FACT
BE
CONSTRUCTED
FROM
TWO
IMAGES
OF
AT
LEAST
FOUR
POINTS
SEC
TRANSFER
BASED
APPROACHES
TO
IMAGE
BASED
RENDERING
WHERE
T
DEF
P
P
Q
X
Y
Z
IN
TURN
THIS
IMPLIES
THAT
WHERE
PT
QU
AND
QV
Q
D
EF
P
Λ
Μ
AND
Λ
P
Q
Μ
Q
Α
Z
Β
Z
Z
Α
X
ΛY
Β
ΜY
USING
EQ
AND
LETTING
DEF
T
THE
WEAK
PERSPECTIVE
CONSTRAINTS
OF
EQ
CAN
BE
REWRITTEN
AS
R
UT
RU
V
RV
WITH
U
RV
Α
Z
Α
Α
Β
Μ
Z
Β
EQUATION
DEFINES
A
PAIR
OF
LINEAR
CONSTRAINTS
ON
THE
COEFFICIENTS
ΞI
I
Α
AND
Β
THESE
CAN
BE
REWRITTEN
AS
T
T
WHERE
D
DEF
DEF
AND
DEF
U
U
U
U
Β
UV
WHEN
THE
FOUR
POINTS
AND
P
ARE
RIGIDLY
ATTACHED
TO
EACH
OTHER
THE
FIVE
STRUCTURE
COEFFICIENTS
Α
AND
Β
ARE
FIXED
FOR
A
RIGID
SCENE
FORMED
BY
N
POINTS
CHOOSING
THREE
OF
THE
POINTS
AS
A
REFERENCE
TRIANGLE
AND
WRITING
EQ
FOR
THE
REMAINING
ONES
YIELDS
A
SET
OF
QUADRATIC
EQUATIONS
IN
UNKNOWNS
WHICH
DEFINE
A
PARAMETERIZATION
OF
THE
SET
OF
ALL
WEAK
PERSPECTIVE
IMAGES
OF
THE
SCENES
THIS
IS
THE
PARAMETERIZED
IMAGE
VARIETY
PIV
OF
GENC
AND
PONCE
NOTE
AGAIN
THAT
THE
WEAK
PERSPECTIVE
CONSTRAINTS
OF
EQ
ARE
LINEAR
IN
THE
FIVE
STRUCTURE
COEFFICIENTS
THUS
GIVEN
A
COLLECTION
OF
IMAGES
AND
POINT
CORRESPONDENCES
THESE
COEFFICIENTS
CAN
BE
ESTIMATED
THROUGH
LINEAR
LEAST
SQUARES
ONCE
THE
VECTOR
Ξ
HAS
BEEN
ESTIMATED
ARBITRARY
IMAGE
APPLICATION
IMAGE
BASED
RENDERING
CHAP
FIGURE
Z
BUFFERING
REPRINTED
FROM
PARAMETERIZED
IMAGE
VARIETIES
A
NOVEL
APPROACH
TO
THE
ANALYSIS
AND
SYNTHESIS
OF
IMAGE
SEQUENCES
BY
Y
GENC
AND
J
PONCE
PROC
INTERNATIONAL
CONFERENCE
ON
COMPUTER
VISION
OC
IEEE
POSITIONS
CAN
BE
ASSIGNED
TO
THE
THREE
REFERENCE
POINTS
EQUATION
YIELDS
FOR
EACH
FEATURE
POINT
TWO
QUADRATIC
CONSTRAINTS
ON
THE
TWO
UNKNOWNS
U
AND
V
ALTHOUGH
THIS
SYSTEM
SHOULD
A
PRIORI
ADMIT
FOUR
SOLUTIONS
IT
ADMITS
AS
SHOWN
IN
THE
EXERCISES
EXACTLY
TWO
REAL
SOLUTIONS
IN
FACT
GIVEN
N
POINT
CORRESPONDENCES
AND
THE
IMAGE
POSITIONS
OF
THE
THREE
TIE
POINTS
IT
CAN
ALSO
BE
SHOWN
GENC
AND
PONCE
THAT
THE
PICTURES
OF
THE
REMAINING
N
POINTS
CAN
BE
DETERMINED
IN
CLOSED
FORM
UP
TO
A
TWO
FOLD
AMBIGUITY
ONCE
THE
POSITIONS
OF
ALL
FEATURE
POINTS
HAVE
BEEN
DETERMINED
THE
SCENE
CAN
BE
RENDERED
BY
TRIANGULATING
THESE
POINTS
AND
TEXTURE
MAPPING
THE
TRIANGLES
INTERESTINGLY
HIDDEN
SURFACE
REMOVAL
CAN
ALSO
BE
PERFORMED
VIA
TRADITIONAL
Z
BUFFER
TECHNIQUES
ALTHOUGH
NO
EXPLICIT
THREE
DIMENSIONAL
RECONSTRUCTION
IS
PERFORMED
THE
IDEA
IS
TO
ASSIGN
RELATIVE
DEPTH
VALUES
TO
THE
VERTICES
OF
THE
TRIANGULATION
AND
IT
IS
CLOSELY
RELATED
TO
THE
METHOD
USED
IN
THE
AFFINE
STRUCTURE
FROM
MOTION
THEOREM
FROM
CHAPTER
LET
Δ
DENOTE
THE
IMAGE
PLANE
OF
ONE
OF
OUR
INPUT
IMAGES
AND
ΔI
THE
IMAGE
PLANE
OF
OUR
SYNTHETIC
IMAGE
TO
RENDER
CORRECTLY
TWO
POINTS
P
AND
Q
THAT
PROJECT
ONTO
THE
SAME
POINT
R
I
IN
THE
SYNTHETIC
IMAGE
WE
MUST
COMPARE
THEIR
DEPTHS
FIGURE
LET
R
DENOTE
THE
INTERSECTION
OF
THE
VIEWING
RAY
JOINING
P
TO
Q
WITH
THE
PLANE
SPANNED
BY
THE
REFERENCE
POINTS
AND
AND
LET
P
Q
R
DENOTE
THE
PROJECTIONS
OF
P
Q
AND
R
INTO
THE
REFERENCE
IMAGE
SUPPOSE
FOR
THE
TIME
BEING
THAT
P
AND
Q
ARE
TWO
OF
THE
POINTS
TRACKED
IN
THE
INPUT
IMAGE
IT
FOLLOWS
THAT
THE
POSITIONS
OF
P
AND
Q
ARE
KNOWN
THE
POSITION
OF
R
IS
EASILY
COMPUTED
BY
REMARKING
THAT
ITS
COORDINATES
IN
THE
AFFINE
BASIS
OF
Δ
FORMED
BY
THE
PRO
JECTIONS
OF
THE
REFERENCE
POINTS
ARE
THE
SAME
AS
THE
COORDINATES
OF
R
IN
THE
AFFINE
BASIS
FORMED
BY
THE
POINTS
IN
THEIR
OWN
PLANE
AND
THUS
ARE
ALSO
THE
SAME
AS
THE
COORDINATES
OF
R
I
IN
THE
AFFINE
BASIS
OF
ΔI
FORMED
BY
THE
PROJECTIONS
OF
THE
REFERENCE
POINTS
THE
RATIO
OF
THE
DEPTHS
OF
P
AND
Q
RELATIVE
TO
THE
PLANE
Δ
IS
SIMPLY
THE
RATIO
PR
QR
NOT
THAT
DECIDING
WHICH
POINT
IS
ACTUALLY
VISIBLE
REQUIRES
ORIENTING
THE
LINE
SUPPORTING
THE
POINTS
P
Q
R
WHICH
IS
SIMPLY
THE
EPIPOLAR
LINE
ASSOCIATED
WITH
THE
POINT
R
I
A
COHERENT
ORIENTATION
SHOULD
BE
CHOSEN
FOR
ALL
EPIPOLAR
LINES
THIS
IS
EASY
SINCE
THEY
ARE
ALL
PARALLEL
TO
EACH
OTHER
NOTE
THAT
THIS
DOES
NOT
REQUIRE
EXPLICITLY
COMPUTING
THE
EPIPOLAR
GEOMETRY
GIVEN
A
FIRST
POINT
PI
ONE
CAN
ORIENT
THE
LINE
PR
AND
THEN
USE
THE
SAME
ORIENTATION
FOR
ALL
OTHER
POINT
CORRESPON
DENCES
THE
ORIENTATIONS
CHOSEN
SHOULD
ALSO
BE
CONSISTENT
OVER
SUCCESSIVE
FRAMES
BUT
THIS
IS
NOT
A
PROBLEM
SINCE
THE
DIRECTION
OF
THE
EPIPOLAR
LINES
CHANGES
SLOWLY
FROM
ONE
FRAME
TO
THE
NEXT
SEC
THE
LIGHT
FIELD
FIGURE
TWO
IMAGES
OF
A
FACE
SYNTHESIZED
USING
PARAMETERIZED
IMAGE
VARIETIES
COURTESY
OF
YAKUP
GENC
AND
ONE
CAN
SIMPLY
CHOOSE
THE
NEW
ORIENTATION
SO
THAT
IT
MAKES
AN
ACUTE
ANGLE
WITH
THE
PRE
VIOUS
ONE
EXAMPLES
OF
SYNTHETIC
PICTURES
CONSTRUCTED
USING
THIS
METHOD
ARE
SHOWN
IN
FIGURE
THE
LIGHT
FIELD
THIS
SECTION
DISCUSSES
A
DIFFERENT
APPROACH
TO
IMAGE
BASED
RENDERING
WHOSE
ONLY
SIMILARITY
WITH
THE
TECHNIQUES
DISCUSSED
IN
THE
PREVIOUS
SECTION
IS
THAT
LIKE
THEM
IT
DOES
NOT
REQUIRE
THE
CONSTRUC
TION
OF
ANY
IMPLICIT
OR
EXPLICIT
MODEL
OF
A
SCENE
LET
US
CONSIDER
FOR
EXAMPLE
A
PANORAMIC
CAMERA
THAT
OPTICALLY
RECORDS
THE
RADIANCE
ALONG
RAYS
PASSING
THROUGH
A
SINGLE
POINT
AND
COVERING
A
FULL
HEMISPHERE
SEE
E
G
PERI
AND
NAYAR
FIGURE
LEFT
IT
IS
POSSIBLE
TO
CREATE
PANORAMIC
CAMERAS
CYLINDRICAL
MOSAICS
FIGURE
CONSTRUCTING
SYNTHETIC
VIEWS
OF
A
SCENE
FROM
A
FIXED
VIEWPOINT
APPLICATION
IMAGE
BASED
RENDERING
CHAP
FIGURE
THE
PLENOPTIC
FUNCTION
AND
THE
LIGHT
FIELD
LEFT
THE
PLENOPTIC
FUNCTION
CAN
BE
PARAMETERIZED
BY
THE
POSITION
P
OF
THE
OBSERVER
AND
THE
VIEWING
DIRECTION
V
RIGHT
THE
LIGHT
FIELD
CAN
BE
PARAMETERIZED
BY
THE
FOUR
PARAMETERS
U
V
T
DEFINING
A
LIGHT
SLAB
IN
PRACTICE
SEVERAL
LIGHT
SLABS
ARE
NECESSARY
TO
MODEL
A
WHOLE
OBJECT
AND
OBTAIN
FULL
SPHERICAL
COVERAGE
ANY
IMAGE
OBSERVED
BY
A
VIRTUAL
CAMERA
WHOSE
PINHOLE
IS
LOCATED
AT
THIS
POINT
BY
MAPPING
THE
ORIGINAL
IMAGE
RAYS
ONTO
VIRTUAL
ONES
THIS
ALLOWS
A
USER
TO
ARBITRARILY
PAN
AND
TILT
THE
VIRTUAL
CAMERA
AND
INTERACTIVELY
EXPLORE
HIS
OR
HER
VISUAL
ENVIRONMENT
SIMILAR
EFFECTS
CAN
BE
OBTAINED
BY
STITCHING
TOGETHER
CLOSE
BY
IMAGES
TAKEN
BY
A
HAND
HELD
CAMCORDER
INTO
A
MOSAIC
SEE
E
G
SHUM
AND
SZELISKI
FIGURE
MIDDLE
OR
BY
COMBINING
THE
PICTURES
TAKEN
BY
A
CAMERA
PANNING
AND
POSSIBLY
TILTING
ABOUT
ITS
OPTICAL
CENTER
INTO
A
CYLINDRICAL
MOSAIC
SEE
E
G
CHEN
FIGURE
RIGHT
THESE
TECHNIQUES
HAVE
THE
DRAWBACK
OF
LIMITING
THE
VIEWER
MOTIONS
TO
PURE
ROTATIONS
ABOUT
THE
OPTICAL
CENTER
OF
THE
CAMERA
A
MORE
POWERFUL
APPROACH
CAN
BE
DEVISED
BY
CONSIDERING
THE
PLENOPTIC
FUNCTION
ADELSON
AND
BERGEN
THAT
ASSOCIATES
WITH
EACH
POINT
IN
SPACE
THE
WAVELENGTH
DEPENDENT
RADIANT
ENERGY
ALONG
A
RAY
PASSING
THROUGH
THIS
POINT
AT
A
GIVEN
TIME
FIGURE
LEFT
THE
LIGHT
FIELD
LEVOY
AND
HANRAHAN
IS
A
SNAPSHOT
OF
THE
PLENOPTIC
FUNCTION
FOR
LIGHT
TRAVELING
IN
VACUUM
IN
THE
ABSENCE
OF
OBSTACLES
THIS
RELAXES
THE
DEPENDENCE
OF
THE
RADIANCE
ON
TIME
AND
ON
THE
POSITION
OF
THE
POINT
OF
INTEREST
ALONG
THE
CORRESPONDING
RAY
SINCE
RADIANCE
IS
CONSTANT
ALONG
STRAIGHT
LINES
IN
A
NONABSORBING
MEDIUM
AND
YIELDS
A
REPRESENTATION
OF
THE
PLENOPTIC
FUNCTION
BY
THE
RADIANCE
ALONG
THE
FOUR
DIMENSIONAL
SET
OF
LIGHT
RAYS
THESE
RAYS
CAN
BE
PARAMETERIZED
IN
MANY
DIFFERENT
WAYS
E
G
USING
THE
PLU
CKER
COORDINATES
INTRODUCED
IN
CHAPTER
BUT
A
CONVENIENT
PARAMETERIZATION
IN
THE
CONTEXT
OF
IMAGE
BASED
RENDERING
IS
THE
LIGHT
SLAB
WHERE
EACH
RAY
IS
SPECIFIED
BY
THE
COORDINATES
OF
ITS
INTERSECTIONS
WITH
TWO
ARBITRARY
PLANES
FIGURE
RIGHT
THE
LIGHT
SLAB
IS
THE
BASIS
FOR
A
TWO
STAGE
APPROACH
TO
IMAGE
BASED
RENDERING
DURING
THE
LEARNING
STAGE
MANY
VIEWS
OF
A
SCENE
ARE
USED
TO
CREATE
A
DISCRETE
VERSION
OF
THE
SLAB
THAT
CAN
BE
THOUGHT
OF
AS
A
FOUR
DIMENSIONAL
LOOKUP
TABLE
AT
SYNTHESIS
TIME
A
VIRTUAL
CAMERA
IS
DEFINED
AND
THE
CORRESPONDING
VIEW
IS
INTERPOLATED
FROM
THE
LOOKUP
TABLE
THE
QUALITY
OF
THE
SYNTHE
SIZED
IMAGES
DEPENDS
ON
THE
NUMBER
OF
REFERENCE
IMAGES
THE
CLOSER
THE
VIRTUAL
VIEW
IS
TO
THE
REFERENCE
IMAGES
THE
BETTER
THE
QUALITY
OF
THE
SYNTHESIZED
IMAGE
NOTE
THAT
CONSTRUCTING
THE
LIGHT
SLAB
MODEL
OF
THE
LIGHT
FIELD
DOES
NOT
REQUIRE
ESTABLISHING
CORRESPONDENCES
BETWEEN
IMAGES
IT
SHOULD
BE
NOTED
THAT
UNLIKE
MOST
METHODS
FOR
IMAGE
BASED
RENDERING
THAT
RELY
ON
TEXTURE
MAPPING
AND
THUS
ASSUME
IMPLICITLY
THAT
THE
OBSERVED
SURFACES
ARE
LAMBERTIAN
LIGHT
FIELD
TECHNIQUES
CAN
BE
USED
TO
RENDER
UNDER
A
FIXED
ILLUMINATION
PICTURES
OF
OBJECTS
WITH
ARBITRARY
BRDFS
SEC
THE
LIGHT
FIELD
FIGURE
THE
ACQUISITION
OF
A
LIGHT
SLAB
FROM
IMAGES
AND
THE
SYNTHESIS
OF
NEW
IMAGES
FROM
A
LIGHT
SLAB
CAN
BE
MODELED
VIA
PROJECTIVE
TRANSFORMATIONS
BETWEEN
THE
X
Y
IMAGE
PLANE
AND
THE
U
V
AND
T
PLANES
DEFINING
THE
SLAB
IN
PRACTICE
A
SAMPLE
OF
THE
LIGHT
FIELD
IS
ACQUIRED
BY
TAKING
A
LARGE
NUMBER
OF
IMAGES
AND
MAPPING
PIXEL
COORDINATES
ONTO
SLAB
COORDINATES
FIGURE
ILLUSTRATES
THE
GENERAL
CASE
THE
MAPPING
BETWEEN
ANY
PIXEL
IN
THE
X
Y
IMAGE
PLANE
AND
THE
CORRESPONDING
AREAS
OF
THE
U
V
AND
T
PLANE
DEFINING
A
LIGHT
SLAB
IS
A
PLANAR
PROJECTIVE
TRANSFORMATION
HARDWARE
OR
SOFTWARE
BASED
TEXTURE
MAPPING
CAN
THUS
BE
USED
TO
POPULATE
THE
LIGHT
FIELD
ON
A
FOUR
DIMENSIONAL
RECTANGULAR
GRID
IN
THE
EXPERIMENTS
DESCRIBED
IN
LEVOY
AND
HANRAHAN
LIGHT
SLABS
ARE
ACQUIRED
IN
THE
SIMPLE
SETTING
OF
A
CAMERA
MOUNTED
ON
A
PLANAR
GANTRY
AND
EQUIPPED
WITH
A
PAN
TILT
HEAD
SO
IT
CAN
ROTATE
ABOUT
ITS
OPTICAL
CENTER
AND
ALWAYS
POINT
TOWARD
THE
CENTER
OF
THE
OBJECT
OF
INTEREST
IN
THIS
CONTEXT
ALL
CALCULATIONS
CAN
BE
SIMPLIFIED
BY
TAKING
THE
U
V
PLANE
TO
BE
THE
PLANE
IN
WHICH
THE
CAMERA
OPTICAL
CENTER
IS
CONSTRAINED
TO
REMAIN
AT
RENDERING
TIME
THE
PROJECTIVE
MAPPING
BETWEEN
THE
VIRTUAL
IMAGE
PLANE
AND
THE
TWO
PLANES
DEFINING
THE
LIGHT
SLAB
CAN
ONCE
AGAIN
BE
USED
TO
EFFICIENTLY
SYNTHESIZE
NEW
IMAGES
FIG
URE
SHOWS
SAMPLE
PICTURES
GENERATED
USING
THE
LIGHT
FIELD
APPROACH
THE
TOP
THREE
IM
AGE
PAIRS
WERE
GENERATED
USING
SYNTHETIC
PICTURES
OF
VARIOUS
OBJECTS
TO
POPULATE
THE
LIGHT
FIELD
THE
LAST
PAIR
OF
IMAGES
WAS
CONSTRUCTED
BY
USING
THE
PLANAR
GANTRY
MENTIONED
EARLIER
TO
ACQUIRE
IMAGES
OF
A
TOY
LION
GROUPED
INTO
FOUR
SLABS
EACH
CONSISTING
OF
IM
AGES
AN
IMPORTANT
ISSUE
IS
THE
SIZE
OF
THE
LIGHT
SLAB
REPRESENTATION
THE
RAW
INPUT
IMAGES
OF
THE
LION
TAKE
OF
DISK
SPACE
THERE
IS
OF
COURSE
MUCH
REDUNDANCY
IN
THESE
PICTURES
AS
IN
THE
CASE
OF
SUCCESSIVE
FRAMES
IN
A
MOTION
SEQUENCE
A
SIMPLE
BUT
EFFECTIVE
TWO
LEVEL
APPROACH
TO
IMAGE
DE
COMPRESSION
IS
PROPOSED
IN
LEVOY
AND
HANRAHAN
THE
LIGHT
SLAB
IS
FIRST
DECOMPOSED
INTO
FOUR
DIMENSIONAL
TILES
OF
COLOR
VALUES
THESE
TILES
ARE
ENCODED
USING
VECTOR
QUANTIZATION
GERSHO
AND
GRAY
A
LOSSY
COMPRESSION
TECHNIQUE
WHERE
THE
DIMENSIONAL
VECTORS
REPRESENTING
THE
RGB
VALUES
AT
THE
CORNERS
OF
THE
ORIGINAL
TILES
ARE
REPLACED
BY
A
REL
ATIVELY
SMALL
SET
OF
REPRODUCTION
VECTORS
CALLED
CODEWORDS
THAT
BEST
APPROXIMATE
IN
THE
MEAN
SQUARED
ERROR
SENSE
THE
INPUT
VECTORS
THE
LIGHT
SLAB
IS
THUS
REPRESENTED
BY
A
SET
OF
INDEXES
IN
THE
CODEBOOK
FORMED
BY
ALL
CODEWORDS
IN
THE
CASE
OF
THE
LION
THE
CODEBOOK
IS
RELATIVELY
SMALL
AND
THE
SIZE
OF
THE
SET
OF
INDEXES
IS
THE
SECOND
COMPRESSION
STAGE
CONSISTS
OF
APPLYING
THE
GZIP
IMPLEMENTATION
OF
ENTROPY
CODING
ZIV
AND
LEMPEL
TO
THE
CODEBOOK
AND
THE
INDEXES
THE
FINAL
SIZE
OF
THE
REPRESENTATION
IS
ONLY
CORRESPONDING
TO
A
COMPRES
SION
RATE
OF
AT
RENDERING
TIME
ENTROPY
DECODING
IS
PERFORMED
AS
THE
FILE
IS
LOADED
IN
MAIN
MEMORY
DEQUANTIZATION
IS
PERFORMED
ON
DEMAND
DURING
DISPLAY
AND
IT
ALLOWS
INTERACTIVE
REFRESH
RATES
APPLICATION
IMAGE
BASED
RENDERING
CHAP
FIGURE
IMAGES
SYNTHESIZED
WITH
THE
LIGHT
FIELD
APPROACH
REPRINTED
FROM
LIGHT
FIELD
RENDERING
BY
M
LEVOY
AND
P
HANRAHAN
PROC
SIGGRAPH
OC
ACM
INC
INCLUDED
HERE
BY
PERMISSION
PROBLEMS
NOTES
IMAGE
BASED
RENDERING
IS
A
QUICKLY
EXPANDING
FIELD
TO
CLOSE
THIS
CHAPTER
LET
US
JUST
MENTION
A
FEW
ALTERNATIVES
TO
THE
APPROACHES
ALREADY
MENTIONED
IN
THE
PREVIOUS
SECTIONS
THE
INTERSECTION
OF
ALL
OF
THE
CONES
THAT
GRAZE
THE
SURFACE
OF
A
SOLID
FORMS
ITS
VISUAL
HULL
LAURENTINI
A
SOLID
IS
ALWAYS
CONTAINED
IN
ITS
VISUAL
HULL
WHICH
IN
TURN
IS
CONTAINED
IN
THE
SOLID
CONVEX
HULL
THE
VOLUMETRIC
APPROACH
TO
OBJECT
MODELING
FROM
REGISTERED
SILHOUETTES
PRESENTED
IN
SECTION
IS
AIMED
AT
CONSTRUCTING
AN
APPROXIMATION
OF
THE
VISUAL
HULL
FROM
A
FINITE
SET
OF
PHOTOGRAPHS
VARIANTS
USE
POLYHEDRA
OR
OCTREES
MARTIN
AND
AGGARWAL
CONNOLLY
AND
STENSTROM
SRIVASTAVA
AND
AHUJA
TO
REPRESENT
THE
CONES
AND
THEIR
INTERSECTION
AND
INCLUDE
A
COMMER
CIAL
SYSTEM
NIEM
AND
BUSCHMANN
FOR
AUTOMATICALLY
CONSTRUCTING
POLYHEDRAL
MODELS
FROM
IMAGES
SEE
ALSO
KUTULAKOS
AND
SEITZ
FOR
A
RELATED
APPROACH
CALLED
SPACE
CARVING
WHERE
EMPTY
VOXELS
ARE
ITERATIVELY
REMOVED
USING
BRIGHTNESS
OR
COLOR
COHERENCE
CON
STRAINTS
THE
TANGENCY
CONSTRAINT
HAS
BEEN
USED
IN
VARIOUS
APPROACHES
FOR
RECONSTRUCTING
A
SURFACE
FROM
A
CONTINUOUS
SEQUENCE
OF
OUTLINES
UNDER
KNOWN
OR
UNKNOWN
CAMERA
MOTIONS
ARBOGAST
AND
MOHR
CIPOLLA
AND
BLAKE
VAILLANT
AND
FAUGERAS
CIPOLLA
ET
AL
BOYER
AND
BERGER
CROSS
ET
AL
JOSHI
ET
AL
VARIANTS
OF
THE
VIEW
INTERPOLATION
METHOD
DISCUSSED
IN
SECTION
INCLUDE
WILLIAMS
AND
CHEN
AND
SEITZ
AND
DYER
TRANSFER
BASED
APPROACHES
TO
IMAGE
BASED
RENDERING
INCLUDE
BESIDES
THOSE
DISCUSSED
IN
SEC
TION
HAVALDAR
ET
AL
AND
AVIDAN
AND
SHASHUA
AS
BRIEFLY
MENTIONED
IN
SEC
TION
A
NUMBER
OF
TECHNIQUES
HAVE
BEEN
DEVELOPED
FOR
INTERACTIVELY
EXPLORING
A
USER
VISUAL
ENVIRONMENT
FROM
A
FIXED
VIEWPOINT
THESE
INCLUDE
A
COMMERCIAL
SYSTEM
QUICKTIME
VR
DE
VELOPED
AT
APPLE
BY
CHEN
AND
ALGORITHMS
THAT
RECONSTRUCT
PINHOLE
PERSPECTIVE
IMAGES
FROM
PANORAMIC
PICTURES
ACQUIRED
BY
SPECIAL
PURPOSE
CAMERAS
SEE
E
G
PERI
AND
NAYAR
SIMILAR
EFFECTS
CAN
BE
OBTAINED
IN
A
LESS
CONTROLLED
SETTING
BY
STITCHING
TOGETHER
CLOSE
BY
IMAGES
TAKEN
BY
A
HAND
HELD
CAMCORDER
INTO
A
MOSAIC
SEE
E
G
IRANI
ET
AL
SHUM
AND
SZELISKI
FOR
IMAGES
OF
DISTANT
TERRAINS
OR
CAMERAS
ROTATING
ABOUT
THEIR
OPTICAL
CENTER
THE
MOSAIC
CAN
BE
CONSTRUCTED
BY
REGISTERING
SUCCESSIVE
PICTURES
VIA
PLANAR
HOMOGRAPHIES
IN
THIS
CONTEXT
ESTIMATING
THE
OPTICAL
FLOW
I
E
THE
VECTOR
FIELD
OF
APPARENT
IMAGE
VELOCITIES
AT
EVERY
IMAGE
POINT
A
NOTION
THAT
HAS
ADMITTEDLY
LARGELY
BEEN
IGNORED
IN
THIS
BOOK
MAY
ALSO
PROVE
IMPORTANT
FOR
FINE
REGISTRATION
AND
DEGHOSTING
SHUM
AND
SZELISKI
VARIANTS
OF
THE
LIGHT
FIELD
APPROACH
DISCUSSED
IN
SECTION
INCLUDE
MCMILLAN
AND
BISHOP
AND
GORTLER
ET
AL
AN
EXCELLENT
INTRODUCTION
TO
BE
ZIER
ARCS
AND
PATCHES
AND
SPLINE
CURVES
AND
SURFACES
CAN
BE
FOUND
IN
FARIN
PROBLEMS
GIVEN
N
POINT
PN
WE
RECURSIVELY
DEFINE
THE
PARAMETRIC
CURVE
PK
T
BY
T
PI
I
I
AND
PK
T
T
PK
T
TPK
T
FOR
K
N
AND
I
N
K
WE
SHOW
IN
THIS
EXERCISE
THAT
PN
T
IS
THE
BE
ZIER
CURVE
OF
DEGREE
N
ASSOCIATED
WITH
THE
N
POINTS
PN
THIS
CONSTRUCTION
OF
A
BE
ZIER
CURVE
IS
CALLED
THE
DE
CASTELJEAU
ALGORITHM
SHOW
THAT
BERNSTEIN
POLYNOMIALS
SATISFY
THE
RECURSION
B
N
T
T
B
N
T
TB
N
T
WITH
B
T
AND
BY
CONVENTION
B
N
T
WHEN
J
OR
J
N
APPLICATION
IMAGE
BASED
RENDERING
CHAP
USE
INDUCTION
TO
SHOW
THAT
K
K
K
P
T
B
T
PI
J
FOR
K
N
AND
I
N
K
CONSIDER
A
BE
ZIER
CURVE
OF
DEGREE
N
DEFINED
BY
N
CONTROL
POINTS
PN
WE
ADDRESS
HERE
THE
PROBLEM
OF
CONSTRUCTING
THE
N
CONTROL
POINTS
QN
OF
A
BE
ZIER
CURVE
OF
DEGREE
N
WITH
THE
SAME
SHAPE
THIS
PROCESS
IS
CALLED
DEGREE
ELEVATION
SHOW
THAT
AND
J
Q
J
N
PJ
J
N
PJ
FOR
J
N
HINT
WRITE
THAT
THE
SAME
POINT
IS
DEFINED
BY
THE
BARYCENTRIC
COMBINATIONS
ASSOCIATED
WITH
THE
TWO
CURVES
AND
EQUATE
THE
POLYNOMIAL
COEFFICIENTS
ON
BOTH
SIDES
OF
THE
EQUATION
SHOW
THAT
THE
TANGENT
TO
THE
BE
ZIER
CURVE
P
T
DEFINED
BY
THE
N
CONTROL
POINTS
PN
IS
N
N
PI
T
N
B
T
PJ
PJ
J
CONCLUDE
THAT
THE
TANGENTS
AT
THE
ENDPOINTS
OF
A
BE
ZIER
ARC
ARE
ALONG
THE
FIRST
AND
LAST
LINE
SEGMENTS
OF
ITS
CONTROL
POLYGON
SHOW
THAT
THE
CONSTRUCTION
OF
THE
POINTS
QI
IN
SECTION
PLACES
THESE
POINTS
IN
A
PLANE
THAT
PASSES
THROUGH
THE
CENTROID
O
OF
THE
POINTS
CI
FAC
ADE
PHOTOGRAMMETRIC
MODULE
WE
SAW
IN
THE
EXERCISES
OF
CHAPTER
THAT
THE
MAPPING
BETWEEN
A
LINE
Δ
WITH
PLU
CKER
COORDINATE
VECTOR
AND
ITS
IMAGE
Δ
WITH
HOMOGENEOUS
COORDINATES
CAN
BE
REPRESENTED
BY
ΡΔ
HERE
IS
A
FUNCTION
OF
THE
MODEL
PARAMETERS
AND
CORRESPONDING
CAMERA
POSITION
AND
ORIENTATION
DEPENDS
ON
THE
ASSUMING
THAT
THE
LINE
Δ
HAS
BEEN
MATCHED
WITH
AN
IMAGE
EDGE
E
OF
LENGTH
L
A
CONVENIENT
MEASURE
OF
THE
DISCREPANCY
BETWEEN
PREDICTED
AND
OBSERVED
DATA
IS
OBTAINED
BY
MULTIPLYING
BY
L
THE
MEAN
SQUARED
DISTANCE
SEPARATING
THE
POINTS
OF
E
FROM
Δ
DEFINING
D
T
AS
THE
SIGNED
DISTANCE
BETWEEN
THE
EDGE
POINT
P
T
T
AND
THE
LINE
Δ
SHOW
THAT
E
T
DT
D
D
D
D
WHERE
AND
DENOTE
THE
SIGNED
DISTANCES
BETWEEN
THE
ENDPOINTS
OF
E
AND
Δ
IF
AND
DENOTE
THE
HOMOGENEOUS
COORDINATE
VECTORS
OF
THESE
POINTS
SHOW
THAT
D
PT
M
AND
D
PT
M
M
M
WHERE
A
DENOTES
THE
VECTOR
FORMED
BY
THE
FIRST
TWO
COORDINATES
OF
THE
VECTOR
A
IN
FORMULATE
THE
RECOVERY
OF
THE
CAMERA
AND
MODEL
PARAMETERS
AS
A
NON
LINEAR
LEAST
SQUARES
PROB
LEM
SHOW
THAT
A
BASIS
FOR
THE
EIGHT
DIMENSIONAL
VECTOR
SPACE
V
FORMED
BY
ALL
AFFINE
IMAGES
OF
A
FIXED
SET
OF
POINTS
PN
CAN
BE
CONSTRUCTED
FROM
AT
LEAST
TWO
IMAGES
OF
THESE
POINTS
WHEN
N
HINT
USE
THE
MATRIX
M
M
U
N
V
N
U
M
N
M
N
WHERE
U
J
V
J
ARE
THE
COORDINATES
OF
THE
PROJECTION
OF
THE
POINT
PI
INTO
IMAGE
NUMBER
J
I
I
SHOW
THAT
THE
SET
OF
ALL
PROJECTIVE
IMAGES
OF
A
FIXED
SCENES
IS
AN
ELEVEN
DIMENSIONAL
VARIETY
PROBLEMS
SHOW
THAT
THE
SET
OF
ALL
PERSPECTIVE
IMAGES
OF
A
FIXED
SCENE
FOR
A
CAMERA
WITH
CONSTANT
INTRINSIC
PARAMETERS
IS
A
SIX
DIMENSIONAL
VARIETY
IN
THIS
EXERCISE
WE
SHOW
THAT
EQ
ONLY
ADMITS
TWO
SOLUTIONS
SHOW
THAT
EQ
CAN
BE
REWRITTEN
AS
R
X
Y
WHERE
E
X
U
Y
V
AND
E
AND
ARE
COEFFICIENTS
DEPENDING
ON
AND
THE
STRUCTURE
PARAMETERS
SHOW
THAT
THE
SOLUTIONS
OF
EQ
ARE
GIVEN
BY
X
I
COS
ARCTAN
E
R
Y
I
J
SIN
J
AND
X
II
Y
II
X
I
Y
I
HINT
USE
A
CHANGE
OF
VARIABLES
TO
REWRITE
EQ
AS
A
SYSTEM
OF
TRIGONOMETRIC
EQUA
TIONS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
AUGUST
ABOUT
THE
INSTRUCTOR
BORN
IN
SOFIA
BULGARIA
GOT
BA
IN
AT
POMONA
COLLEGE
CA
COMPUTER
SCIENCE
MEDIA
STUDIES
GOT
PHD
IN
AT
UNIVERSITY
OF
TEXAS
AT
AUSTIN
COMPUTER
VISION
COURSE
WEBSITE
INSTRUCTOR
ADRIANA
KOVASHKA
KOVASHKA
CS
PITT
EDU
PLEASE
USE
AT
THE
BEGINNING
OF
YOUR
SUBJECT
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
MW
TA
CHANGSHENG
LIU
TO
BE
CONFIRMED
TA
OFFICE
HOURS
TO
BE
DETERMINED
BY
RICHARD
SZELISKI
BY
KRISTEN
GRAUMAN
AND
BASTIAN
LEIBE
MORE
RESOURCES
AVAILABLE
ON
COURSE
WEBPAGE
YOUR
NOTES
FROM
CLASS
ARE
YOUR
BEST
STUDY
MATERIAL
SLIDES
ARE
NOT
COMPLETE
WITH
NOTES
TO
LEARN
ABOUT
THE
BASIC
COMPUTER
VISION
TASKS
AND
APPROACHES
TO
GET
EXPERIENCE
WITH
SOME
COMPUTER
VISION
TECHNIQUES
TO
LEARN
ABSOLUTE
BASICS
OF
MACHINE
LEARNING
TO
THINK
CRITICALLY
ABOUT
VISION
APPROACHES
AND
TO
SEE
CONNECTIONS
BETWEEN
WORKS
AND
POTENTIAL
FOR
IMPROVEMENT
BLITZ
INTRODUCTIONS
WHAT
IS
COMPUTER
VISION
WHY
DO
WE
CARE
WHAT
ARE
THE
CHALLENGES
WHAT
IS
THE
CURRENT
RESEARCH
LIKE
COURSE
STRUCTURE
AND
POLICIES
OVERVIEW
OF
TOPICS
IF
TIME
BLITZ
INTRODUCTIONS
BLITZ
INTRODUCTIONS
SEC
WHAT
IS
YOUR
NAME
TELL
US
ONE
FUN
THING
ABOUT
YOURSELF
I
LL
ASK
YOU
MORE
QUESTIONS
IN
COMPUTER
VISION
DONE
WE
SEE
WITH
OUR
BRAINS
NOT
WITH
OUR
EYES
OLIVER
SACKS
AND
OTHERS
KRISTEN
GRAUMAN
ADAPTED
AUTOMATIC
UNDERSTANDING
OF
IMAGES
AND
VIDEO
COMPUTING
PROPERTIES
OF
THE
WORLD
FROM
VISUAL
DATA
MEASUREMENT
ALGORITHMS
AND
REPRESENTATIONS
TO
ALLOW
A
MACHINE
TO
RECOGNIZE
OBJECTS
PEOPLE
SCENES
AND
ACTIVITIES
PERCEPTION
AND
INTERPRETATION
ALGORITHMS
TO
MINE
SEARCH
AND
INTERACT
WITH
VISUAL
DATA
SEARCH
AND
ORGANIZATION
KRISTEN
GRAUMAN
REAL
TIME
STEREO
STRUCTURE
FROM
MOTION
POLLEFEYS
ET
AL
KRISTEN
GRAUMAN
MULTI
VIEW
STEREO
FOR
COMMUNITY
PHOTO
COLLECTIONS
GOESELE
ET
AL
SLIDE
CREDIT
L
LAZEBNIK
VISION
FOR
PERCEPTION
INTERPRETATION
OBJECTS
ACTIVITIES
SCENES
LOCATIONS
TEXT
WRITING
FACES
GESTURES
MOTIONS
EMOTIONS
VISUAL
SEARCH
ORGANIZATION
QUERY
IMAGE
OR
VIDEO
ARCHIVES
RELEVANT
CONTENT
GRAPHICS
IMAGE
PROCESSING
ARTIFICIAL
INTELLIGENCE
COMPUTER
VISION
ALGORITHMS
MACHINE
LEARNING
COGNITIVE
SCIENCE
IMAGES
VISION
MODEL
GRAPHICS
INVERSE
PROBLEMS
ANALYSIS
AND
SYNTHESIS
AS
IMAGE
SOURCES
MULTIPLY
SO
DO
APPLICATIONS
RELIEVE
HUMANS
OF
BORING
EASY
TASKS
HUMAN
COMPUTER
INTERACTION
PERCEPTION
FOR
ROBOTICS
AUTONOMOUS
AGENTS
ORGANIZE
AND
GIVE
ACCESS
TO
VISUAL
CONTENT
DESCRIPTION
OF
IMAGE
CONTENT
FOR
THE
VISUALLY
IMPAIRED
FUN
APPLICATIONS
E
G
TRANSFER
ART
STYLES
TO
MY
PHOTOS
WHY
VISION
IMAGES
AND
VIDEO
ARE
EVERYWHERE
HOURS
UPLOADED
TO
YOUTUBE
DAILY
MIL
PHOTOS
UPLOADED
TO
FLICKR
DAILY
BIL
IMAGES
INDEXED
BY
GOOGLE
PERSONAL
PHOTO
ALBUMS
SURVEILLANCE
AND
SECURITY
MOVIES
NEWS
SPORTS
MEDICAL
AND
SCIENTIFIC
IMAGES
FACES
AND
DIGITAL
CAMERAS
CAMERA
WAITS
FOR
EVERYONE
TO
SMILE
TO
TAKE
A
PHOTO
CANON
SETTING
CAMERA
FOCUS
VIA
FACE
DETECTION
FACE
RECOGNITION
LINKING
TO
INFO
WITH
A
MOBILE
DEVICE
SITUATED
SEARCH
YEH
ET
AL
MIT
MSR
LINCOLN
KOOABA
EXPLORING
PHOTO
COLLECTIONS
SNAVELY
ET
AL
THE
MATRIX
WHAT
DREAMS
MAY
COME
MOCAP
FOR
PIRATES
OF
THE
CARRIBEAN
INDUSTRIAL
LIGHT
AND
MAGIC
SOURCE
SEITZ
HUMAN
JOYSTICK
NEWSBREAKER
LIVE
ASSISTIVE
TECHNOLOGY
SYSTEMS
CAMERA
MOUSE
BOSTON
COLLEGE
IMAGE
GUIDED
SURGERY
MIT
AI
VISION
GROUP
FMRI
DATA
GOLLAND
ET
AL
NAVIGATION
DRIVER
SAFETY
MONITORING
POOL
POSEIDON
PEDESTRIAN
DETECTION
MERL
VIOLA
ET
AL
SURVEILLANCE
FARMBOT
IO
BY
MYERS
ET
AL
ICCV
PIRSIAVASH
ET
AL
ASSESSING
THE
QUALITY
OF
ACTIONS
ECCV
REED
ET
AL
ICML
RADFORD
ET
AL
ICLR
OBSTACLES
KRISTEN
GRAUMAN
READ
MORE
ABOUT
THE
HISTORY
SZELISKI
SEC
WHY
IS
THIS
PROBLEMATIC
ILL
POSED
PROBLEM
REAL
WORLD
MUCH
MORE
COMPLEX
THAN
WHAT
WE
CAN
MEASURE
IN
IMAGES
IMPOSSIBLE
TO
LITERALLY
INVERT
IMAGE
FORMATION
PROCESS
WITH
LIMITED
INFORMATION
NEED
INFORMATION
OUTSIDE
OF
THIS
PARTICULAR
IMAGE
TO
GENERALIZE
WHAT
IMAGE
PORTRAYS
E
G
TO
RESOLVE
OCCLUSION
ILLUMINATION
OBJECT
POSE
CLUTTER
OCCLUSIONS
INTRA
CLASS
APPEARANCE
VIEWPOINT
THINK
AGAIN
ABOUT
THE
PIXELS
CHALLENGES
COMPLEXITY
THOUSANDS
TO
MILLIONS
OF
PIXELS
IN
AN
IMAGE
HUMAN
RECOGNIZABLE
OBJECT
CATEGORIES
DEGREES
OF
FREEDOM
IN
THE
POSE
OF
ARTICULATED
OBJECTS
HUMANS
BILLIONS
OF
IMAGES
INDEXED
BY
GOOGLE
IMAGE
SEARCH
BILLION
PRINTS
PRODUCED
FROM
DIGITAL
CAMERA
IMAGES
IN
MILLION
CAMERA
PHONES
SOLD
IN
ABOUT
HALF
OF
THE
CEREBRAL
CORTEX
IN
PRIMATES
IS
DEVOTED
TO
PROCESSING
VISUAL
INFORMATION
FELLEMAN
AND
VAN
ESSEN
CHALLENGES
LIMITED
SUPERVISION
LESS
MORE
CHALLENGES
VISION
REQUIRES
REASONING
ANTOL
ET
AL
VQA
VISUAL
QUESTION
ANSWERING
ICCV
OK
CLEARLY
THE
VISION
PROBLEM
IS
DEEP
AND
CHALLENGING
TIME
TO
GIVE
UP
ACTIVE
RESEARCH
AREA
WITH
EXCITING
PROGRESS
KRISTEN
GRAUMAN
DATASETS
TODAY
IMAGENET
CATEGORIES
IMAGES
MICROSOFT
COCO
CATEGORIES
IMAGES
PASCAL
CATEGORIES
IMAGES
SUN
CATEGORIES
IMAGES
SOME
VISUAL
RECOGNITION
PROBLEMS
RECOGNITION
WHAT
IS
THIS
RECOGNITION
WHAT
OBJECTS
DO
YOU
SEE
BUILDING
BALCONY
STREET
TRUCK
CARRIAGE
HORSE
TABLE
PERSON
PERSON
CAR
DETECTION
WHERE
ARE
THE
CARS
ACTIVITY
WHAT
IS
THIS
PERSON
DOING
SCENE
IS
THIS
AN
INDOOR
SCENE
INSTANCE
WHICH
CITY
WHICH
BUILDING
VISUAL
QUESTION
ANSWERING
WHAT
ARE
ALL
THESE
PEOPLE
PARTICIPATING
IN
THE
LATEST
AT
CVPR
CVPR
IEEE
CONFERENCE
ON
COMPUTER
VISION
AND
PATTERN
RECOGNITION
UNIFIED
REAL
TIME
OBJECT
DETECTION
DECODING
PHYSICAL
SENSATION
FROM
A
FIRST
PERSON
VIDEO
QUESTION
ANSWERING
GATYS
ET
AL
CVPR
CONVOLUTIONAL
NEURAL
NETWORKS
IMAGES
STYLES
RESULTS
SEEING
BEHIND
THE
CAMERA
IDENTIFYING
THE
AUTHORSHIP
OF
A
PHOTOGRAPH
THOMAS
AND
KOVASHKA
CVPR
IS
COMPUTER
VISION
SOLVED
GIVEN
AN
IMAGE
WE
CAN
GUESS
WITH
ACCURACY
WHAT
OBJECT
CATEGORIES
ARE
SHOWN
RESNET
BUT
WE
ONLY
ANSWER
WHY
QUESTIONS
ABOUT
IMAGES
WITH
ACCURACY
WHY
DOES
IT
SEEM
THAT
IT
SOLVED
DEEP
LEARNING
MAKES
EXCELLENT
USE
OF
MASSIVE
DATA
LABELED
FOR
THE
TASK
OF
INTEREST
BUT
IT
HARD
TO
UNDERSTAND
HOW
IT
DOES
SO
IT
DOESN
T
WORK
WELL
WHEN
MASSIVE
DATA
IS
NOT
AVAILABLE
AND
YOUR
TASK
IS
DIFFERENT
THAN
TASKS
FOR
WHICH
DATA
IS
AVAILABLE
SOMETIMES
THE
MANNER
IN
WHICH
DEEP
METHODS
WORK
IS
NOT
INTELLECTUALLY
APPEALING
BUT
OUR
SMARTER
MORE
COMPLEX
METHODS
PERFORM
WORSE
COURSE
STRUCTURE
AND
POLICIES
COURSE
WEBSITE
SCHEDULE
INT
RODUCT
ION
TO
COMP
X
E
CS
PITT
EDU
L
R
J
INTRODUCTION
TO
VISION
ODUCTION
TO
COMPUTER
VISION
FALL
LOCATION
SENNOTT
SQUARE
TIM
E
FONDAY
AND
WEDNESDAY
INS
TRU
CT
O
R
ADRIANA
KOVASHKA
EMAIL
KOVASHKA
AT
CS
DOTPITT
DOT
EDU
USE
AT
THE
BEGINNING
OF
THE
SUBJECT
LINE
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
MONDA
Y
AND
WEDNESDAY
TA
TBD
EMAIL
TBD
TA
OFFI
CE
HOURS
TBD
THIS
PAGE
IS
UNDER
CONSTRUCTION
OVERVIEW
COURSE
DESCRIPTION
IN
THIS
CLASS
STUDENTS
WILL
LEARN
THE
BASICS
OF
MODERN
COMPUTER
VISION
THE
FIRST
MAJOR
PART
OF
THE
COURSE
WILL
CO
ER
FUNDAMENTAL
CONCEPTS
SUCH
AS
IMAGE
FORMATION
IMAGE
FILTERING
EDGE
DETECTION
TURE
DESCRIPTION
FEATURE
EXTRACTION
AND
MATCHING
AND
GROUPING
AND
FITTING
A
BRIEF
INTRO
TO
MACHINE
LEARNING
WILL
FOLLM
IN
PREP
ARATION
FOR
THE
SECOND
COURSE
CHAPTER
ON
READINGS
READINGS
FOR
CLASS
N
WILL
BE
POSTED
BY
ON
THE
DAY
OF
CLASS
N
THEY
PROVIDE
BACKGROUND
AND
MORE
DETAILED
EXPLANATIONS
OF
TOPICS
WE
LL
COVER
GENERALLY
THEY
PROVIDE
MORE
INFO
THAN
WHAT
YOU
NEED
FOR
THE
EXAMS
SOMETIMES
I
WILL
POST
RESEARCH
PAPERS
AS
READING
WRITTEN
HW
ASSIGNMENTS
X
EACH
PROGRAMMING
HW
ASSIGNMENTS
X
EACH
MIDTERM
EXAM
FINAL
EXAM
PARTICIPATION
SHORT
ANSWER
OR
MULTIPLE
CHOICE
ANSWERS
WILL
HELP
ENSURE
THAT
YOU
UNDERSTAND
THE
MOST
RECENT
TOPICS
COVERED
SO
YOU
CAN
DO
THE
PROGRAMMING
HOMEWORK
DUE
TWO
DAYS
BEFORE
THE
PROGRAMMING
ASSIGNMENTS
FREE
LATE
DAYS
DO
NOT
APPLY
I
GRADE
THESE
ASSIGNMENTS
IMPLEMENT
A
TECHNIQUE
OR
PRACTICE
CONCEPTS
WE
DISCUSSED
ONE
ASSIGNMENT
ROUGHLY
EVERY
WEEK
PLEASE
COMMENT
YOUR
CODE
FREE
LATE
DAYS
APPLY
SEE
NEXT
SLIDE
TA
GRADES
THESE
ON
PROGRAMMING
ASSIGNMENTS
ONLY
YOU
GET
FREE
LATE
DAYS
I
E
YOU
CAN
SUBMIT
HOMEWORK
A
TOTAL
OF
DAYS
LATE
FOR
EXAMPLE
YOU
CAN
SUBMIT
ONE
PROBLEM
SET
HOURS
LATE
AND
ANOTHER
HOURS
LATE
ONCE
YOU
VE
USED
UP
YOUR
FREE
LATE
DAYS
YOU
WILL
INCUR
A
PENALTY
OF
FROM
THE
TOTAL
PROJECT
CREDIT
POSSIBLE
FOR
EACH
LATE
DAY
A
LATE
DAY
IS
ANYTHING
FROM
MINUTE
TO
HOURS
NAVIGATE
TO
THE
COURSEWEB
PAGE
FOR
CLICK
ON
ASSIGNMENTS
AND
THE
CORRESPONDING
HW
ID
YOUR
WRITTEN
ANSWERS
SHOULD
BE
A
SINGLE
PDF
DOC
DOCX
FILE
YOUR
CODE
SHOULD
BE
A
SINGLE
ZIP
FILE
WITH
M
FILES
AND
IMAGES
RESULTS
IF
REQUESTED
NAME
THE
FILE
EXTENSION
HOMEWORK
IS
DUE
AT
ON
THE
DUE
GRADES
WILL
APPEAR
ON
COURSEWEB
ONE
MID
TERM
AND
ONE
FINAL
EXAM
MIDTERM
COUNTS
FOR
LESS
BECAUSE
BASED
ON
ONLY
MEAT
LECTURES
EXCLUDING
INTRO
MATLAB
REVIEWS
THE
FINAL
EXAM
WILL
FOCUS
ON
THE
LATTER
HALF
OF
THE
COURSE
EXAMS
WILL
BE
PRECEDED
BY
REVIEW
SESSIONS
IF
OUR
SCHEDULE
ALLOWS
IT
THERE
WILL
BE
NO
MAKE
UP
EXAMS
UNLESS
YOU
OR
A
CLOSE
RELATIVE
IS
SERIOUSLY
ILL
EXCLUDES
COLD
FLU
OF
GRADE
WILL
BE
BASED
ON
ATTENDANCE
AND
PARTICIPATION
ANSWER
QUESTIONS
ASKED
BY
INSTRUCTOR
AND
OTHERS
ASK
MEANINGFUL
QUESTIONS
ASK
OR
ANSWER
QUESTIONS
ON
PIAZZA
BRING
IN
RELEVANT
ARTICLES
ABOUT
RECENT
DEVELOPMENTS
IN
COMPUTER
VISION
FEEDBACK
IS
WELCOME
YOU
WILL
WORK
INDIVIDUALLY
THE
WORK
YOU
TURN
IN
MUST
BE
YOUR
OWN
WORK
YOU
CAN
DISCUSS
THE
ASSIGNMENTS
WITH
YOUR
CLASSMATES
BUT
DO
NOT
LOOK
AT
THEIR
CODE
OR
ANSWERS
YOU
CANNOT
USE
POSTED
SOLUTIONS
SEARCH
FOR
CODE
ON
THE
INTERNET
OR
USE
MATLAB
IMPLEMENTATIONS
OF
SOMETHING
YOU
ARE
ASKED
TO
WRITE
WHEN
IN
DOUBT
ASK
THE
INSTRUCTOR
OR
TA
PLAGIARISM
WILL
CAUSE
YOU
TO
FAIL
THE
CLASS
AND
RECEIVE
DISCIPLINARY
PENALTY
IF
YOU
HAVE
A
DISABILITY
FOR
WHICH
YOU
ARE
OR
MAY
BE
REQUESTING
AN
ACCOMMODATION
YOU
ARE
ENCOURAGED
TO
CONTACT
BOTH
YOUR
INSTRUCTOR
AND
DISABILITY
RESOURCES
AND
SERVICES
DRS
WILLIAM
PITT
UNION
FOR
ASL
USERS
AS
EARLY
AS
POSSIBLE
IN
THE
TERM
DRS
WILL
VERIFY
YOUR
DISABILITY
AND
DETERMINE
REASONABLE
ACCOMMODATIONS
FOR
THIS
COURSE
IF
YOU
HAVE
A
MEDICAL
CONDITION
WHICH
WILL
PREVENT
YOU
FROM
DOING
A
CERTAIN
ASSIGNMENT
YOU
MUST
INFORM
THE
INSTRUCTOR
OF
THIS
BEFORE
THE
DEADLINE
YOU
MUST
THEN
SUBMIT
DOCUMENTATION
OF
YOUR
CONDITION
WITHIN
A
WEEK
OF
THE
ASSIGNMENT
DEADLINE
THERE
WILL
BE
NO
MAKE
UP
EXAMS
TO
ENSURE
THE
FREE
AND
OPEN
DISCUSSION
OF
IDEAS
STUDENTS
MAY
NOT
RECORD
CLASSROOM
LECTURES
DISCUSSION
AND
OR
ACTIVITIES
WITHOUT
THE
ADVANCE
WRITTEN
PERMISSION
OF
THE
INSTRUCTOR
AND
ANY
SUCH
RECORDING
PROPERLY
APPROVED
IN
ADVANCE
CAN
BE
USED
SOLELY
FOR
THE
STUDENT
OWN
PRIVATE
USE
WARNINGS
THIS
CLASS
IS
A
LOT
OF
WORK
THIS
TIME
I
VE
OPTED
FOR
SHORTER
MORE
MANAGEABLE
HW
ASSIGNMENTS
BUT
THERE
IS
MORE
OF
THEM
I
EXPECT
YOU
D
BE
SPENDING
HOURS
ON
HOMEWORK
EACH
WEEK
BUT
YOU
GET
TO
UNDERSTAND
ALGORITHMS
AND
CONCEPTS
IN
DETAIL
SOME
PARTS
WILL
BE
HARD
AND
REQUIRE
THAT
YOU
PAY
CLOSE
ATTENTION
I
WILL
USE
THE
WRITTEN
HW
TO
GAUGE
HOW
YOU
RE
DOING
I
WILL
ALSO
PICK
ON
STUDENTS
RANDOMLY
TO
ANSWER
QUESTIONS
USE
INSTRUCTOR
AND
TA
OFFICE
HOURS
YOU
WILL
LEARN
A
LOT
PROGRAMMING
ASSIGNMENTS
WILL
BE
IN
MATLAB
SINCE
THAT
VERY
COMMON
IN
COMPUTER
VISION
AND
IS
OPTIMIZED
FOR
WORK
WITH
MATRICES
MATLAB
ALSO
HAS
GREAT
DOCUMENTATION
IS
JUST
MATLAB
PRACTICE
SOME
PEOPLE
WON
T
LIKE
MATLAB
I
LIKE
IT
YOU
WILL
LEARN
A
NEW
PROGRAMMING
LANGUAGE
EVIDENCE
THAT
YOU
SHOULD
TAKE
MY
WARNINGS
SERIOUSLY
IS
DUE
ON
MONDAY
LABOR
DAY
IS
DUE
ON
WEDNESDAY
SEPT
IF
THIS
DOESN
T
SOUND
LIKE
YOUR
CUP
OF
COFFEE
DROP
DEADLINE
IS
SEPTEMBER
NEXT
FRIDAY
NOTE
TO
WAITLISTED
STUDENTS
KEEP
COMING
TO
CLASS
IF
IT
SOUNDS
INTERESTING
QUESTIONS
OVERVIEW
OF
TOPICS
TRANSFORMING
AND
DESCRIBING
IMAGES
TEXTURES
COLORS
EDGES
KRISTEN
GRAUMAN
DETECTING
DISTINCTIVE
REPEATABLE
FEATURES
DESCRIBING
IMAGES
WITH
LOCAL
STATISTICS
MATCHING
FEATURES
AND
REGIONS
ACROSS
IMAGES
HOW
DOES
LIGHT
IN
WORLD
PROJECT
TO
FORM
IMAGES
HARTLEY
AND
ZISSERMAN
MULTI
VIEW
GEOMETRY
MATCHING
INVARIANT
FEATURES
STEREO
VISION
CLUSTERING
SEGMENTATION
FITTING
WHAT
PARTS
BELONG
TOGETHER
FIG
FROM
SHI
ET
AL
RECOGNIZING
OBJECTS
AND
CATEGORIES
LEARNING
TECHNIQUES
DETECTING
NOVEL
INSTANCES
OF
OBJECTS
CLASSIFYING
REGIONS
AS
ONE
OF
SEVERAL
CATEGORIES
DESCRIBING
THE
HIGH
LEVEL
PROPERTIES
OF
OBJECTS
ALLOWS
RECOGNITION
OF
UNSEEN
OBJECTS
STATE
OF
THE
ART
ON
MANY
RECOGNITION
TASKS
IMAGE
PREDICTION
RECURRENT
NEURAL
NETWORKS
SEQUENCE
PROCESSING
E
G
QUESTION
ANSWERING
MOTION
AND
TRACKING
TRACKING
OBJECTS
VIDEO
ANALYSIS
KRISTEN
GRAUMAN
TOMAS
IZO
POSE
AND
ACTIONS
AUTOMATICALLY
ANNOTATING
HUMAN
POSE
JOINTS
RECOGNIZING
ACTIONS
IN
FIRST
PERSON
VIDEO
MATLAB
TUTORIAL
OUT
WILL
READ
IN
CLASS
DUE
ON
MONDAY
READ
SZELISKI
SEC
ENROLL
FOR
PIAZZA
CS
INTRO
TO
COMPUTER
VISION
MATLAB
TUTORIAL
PLAN
FOR
TODAY
INTRO
COURSE
BASICS
REFRESHER
OVERVIEW
OF
TOPICS
WHAT
ARE
IMAGES
LINEAR
ALGEBRA
LIGHTNING
QUICK
OVER
RE
VIEW
MATLAB
TUTORIAL
OUTRO
OVERVIEW
OF
HOMEWORK
W
AND
P
IF
TIME
COURSE
INFO
COURSE
WEBSITE
INSTRUCTOR
ADRIANA
KOVASHKA
KOVASHKA
CS
PITT
EDU
PLEASE
USE
AT
THE
BEGINNING
OF
YOUR
SUBJECT
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
MW
TAS
TA
GRADER
YUHUAN
JIANG
YUHUAN
CS
PITT
EDU
ADDITIONAL
TAS
OFFICE
HOURS
ONLY
CHRIS
THOMAS
AND
NILS
MURRUGARRA
TAS
OFFICE
HOURS
TBD
YOUR
HOMEWORK
FILL
OUT
THE
DOODLE
AT
IGNORE
DATES
LOOK
AT
DAYS
OF
THE
WEEK
MIN
INCREMENTS
MATLAB
YOU
CAN
GET
IT
FOR
FREE
THROUGH
MY
PITT
EDU
MY
RESOURCES
SOFTWARE
DOWNLOADS
GET
THE
LATEST
VERSION
MOSTLY
BECAUSE
THAT
WHAT
I
USE
MAKE
SURE
TO
CHECK
THE
IMAGE
PROCESSING
TOOLBOX
COMPUTER
VISION
SYSTEM
TOOLBOX
AND
STATISTICS
AND
MACHINE
LEARNING
TOOLBOX
BOXES
DURING
INSTALLATION
EASIEST
TO
INSTALL
ALL
COURSE
COMPONENTS
WRITTEN
HW
ASSIGNMENTS
X
EACH
PROGRAMMING
HW
ASSIGNMENTS
X
EACH
MIDTERM
EXAM
FINAL
EXAM
PARTICIPATION
THE
REST
OF
THE
COURSE
POLICIES
READ
THE
COURSE
WEBSITE
CAREFULLY
WARNINGS
THIS
CLASS
IS
A
LOT
OF
WORK
THIS
TIME
I
VE
OPTED
FOR
SHORTER
MORE
MANAGEABLE
HW
ASSIGNMENTS
BUT
THERE
IS
MORE
OF
THEM
I
EXPECT
YOU
D
BE
SPENDING
HOURS
ON
HOMEWORK
EACH
WEEK
BUT
YOU
GET
TO
UNDERSTAND
ALGORITHMS
AND
CONCEPTS
IN
DETAIL
SOME
PARTS
WILL
BE
HARD
AND
REQUIRE
THAT
YOU
PAY
CLOSE
ATTENTION
I
WILL
USE
THE
WRITTEN
HW
TO
GAUGE
HOW
YOU
RE
DOING
I
WILL
ALSO
PICK
ON
STUDENTS
RANDOMLY
TO
ANSWER
QUESTIONS
USE
INSTRUCTOR
AND
TAS
OFFICE
HOURS
YOU
WILL
LEARN
A
LOT
PROGRAMMING
ASSIGNMENTS
WILL
BE
IN
MATLAB
SINCE
THAT
VERY
COMMON
IN
COMPUTER
VISION
AND
IS
OPTIMIZED
FOR
WORK
WITH
MATRICES
MATLAB
ALSO
HAS
GREAT
DOCUMENTATION
IS
JUST
MATLAB
PRACTICE
SOME
PEOPLE
WON
T
LIKE
MATLAB
I
LIKE
IT
YOU
WILL
LEARN
A
NEW
PROGRAMMING
LANGUAGE
WHAT
IS
THE
ROLE
OF
DATASETS
WHATEVER
OUR
COMPUTER
VISION
ALGORITHMS
LEARN
THEY
WILL
LEARN
FROM
SOME
SET
OF
DATASETS
OF
IMAGES
WE
WILL
ALSO
USE
THE
DATASETS
TO
TEST
OUR
ALGORITHMS
WHY
DID
I
SAY
IT
HARD
TO
UNDERSTAND
DEEP
LEARNING
A
K
A
DEEP
NEURAL
NETWORKS
BECAUSE
MOST
DEEP
LEARNING
METHODS
JUST
TAKE
AN
IMAGE
AS
INPUT
AND
OUTPUT
PREDICTIONS
AND
THEY
LEARN
HOW
TO
REPRESENT
AND
EXAMINE
THE
IMAGE
ON
THEIR
OWN
SO
THEY
APPEAR
TO
BE
BLACK
BOXES
QUESTIONS
OVERVIEW
OF
TOPICS
THE
NEXT
SLIDES
WILL
BE
VERY
QUICK
THEN
WE
LL
SLOW
DOWN
READY
TRANSFORMING
AND
DESCRIBING
IMAGES
TEXTURES
COLORS
EDGES
KRISTEN
GRAUMAN
DETECTING
DISTINCTIVE
REPEATABLE
FEATURES
DESCRIBING
IMAGES
WITH
LOCAL
STATISTICS
MATCHING
FEATURES
AND
REGIONS
ACROSS
IMAGES
HOW
DOES
LIGHT
IN
WORLD
PROJECT
TO
FORM
IMAGES
HARTLEY
AND
ZISSERMAN
MULTI
VIEW
GEOMETRY
MATCHING
INVARIANT
FEATURES
STEREO
VISION
CLUSTERING
SEGMENTATION
FITTING
WHAT
PARTS
BELONG
TOGETHER
FIG
FROM
SHI
ET
AL
RECOGNIZING
OBJECTS
AND
CATEGORIES
LEARNING
TECHNIQUES
DETECTING
NOVEL
INSTANCES
OF
OBJECTS
CLASSIFYING
REGIONS
AS
ONE
OF
SEVERAL
CATEGORIES
DESCRIBING
THE
HIGH
LEVEL
PROPERTIES
OF
OBJECTS
ALLOWS
RECOGNITION
OF
UNSEEN
OBJECTS
STATE
OF
THE
ART
ON
MANY
RECOGNITION
TASKS
IMAGE
PREDICTION
RECURRENT
NEURAL
NETWORKS
SEQUENCE
PROCESSING
E
G
QUESTION
ANSWERING
MOTION
AND
TRACKING
TRACKING
OBJECTS
VIDEO
ANALYSIS
KRISTEN
GRAUMAN
TOMAS
IZO
POSE
AND
ACTIONS
AUTOMATICALLY
ANNOTATING
HUMAN
POSE
JOINTS
RECOGNIZING
ACTIONS
IN
FIRST
PERSON
VIDEO
WHAT
ARE
IMAGES
WHAT
ARE
IMAGES
IN
MATLAB
MATLAB
TREATS
IMAGES
AS
MATRICES
OF
NUMBERS
TO
PROCEED
LET
TALK
VERY
BRIEFLY
ABOUT
HOW
IMAGES
ARE
FORMED
IMAGE
FORMATION
FILM
SLIDE
CREDIT
DEREK
HOIEM
A
DIGITAL
CAMERA
REPLACES
FILM
WITH
A
SENSOR
ARRAY
EACH
CELL
IN
THE
ARRAY
IS
LIGHT
SENSITIVE
DIODE
THAT
CONVERTS
PHOTONS
TO
ELECTRONS
SLIDE
BY
STEVE
SEITZ
SAMPLE
THE
SPACE
ON
A
REGULAR
GRID
QUANTIZE
EACH
SAMPLE
ROUND
TO
NEAREST
INTEGER
SAMPLE
THE
SPACE
ON
A
REGULAR
GRID
QUANTIZE
EACH
SAMPLE
ROUND
TO
NEAREST
INTEGER
WHAT
DOES
QUANTIZING
SIGNAL
LOOK
LIKE
IMAGE
THUS
REPRESENTED
AS
A
MATRIX
OF
INTEGER
VALUES
COLOR
IMAGES
RGB
COLOR
SPACE
SPLIT
IMAGE
INTO
THREE
CHANNELS
R
G
B
ADAPTED
FROM
KRISTEN
GRAUMAN
IMAGES
IN
MATLAB
COLOR
IMAGES
REPRESENTED
AS
A
MATRIX
WITH
MULTIPLE
CHANNELS
IF
GRAYSCALE
SUPPOSE
WE
HAVE
A
NXM
RGB
IMAGE
CALLED
IM
IM
TOP
LEFT
PIXEL
VALUE
IN
R
CHANNEL
IM
Y
X
B
Y
PIXELS
DOWN
X
PIXELS
TO
RIGHT
IN
THE
BTH
CHANNEL
IM
N
M
BOTTOM
RIGHT
PIXEL
IN
B
CHANNEL
IMREAD
FILENAME
RETURNS
A
IMAGE
VALUES
TO
CONVERT
TO
DOUBLE
FORMAT
VALUES
TO
WITH
DOUBLE
ROW
COLUMN
R
ADAPTED
FROM
DEREK
HOIEM
LINEAR
ALGEBRA
VERY
BRIEF
ALL
YOU
NEED
TO
KNOW
FOR
MOST
OF
COURSE
EXCEPTION
LAST
THREE
LECTURES
BEFORE
MIDTERM
SOME
MORE
REVIEW
THEN
RAISE
YOUR
HAND
IF
YOU
VE
HAD
A
LINEAR
ALGEBRA
COURSE
MATLAB
VECTORS
AND
MATRICES
ARE
JUST
COLLECTIONS
OF
ORDERED
NUMBERS
THAT
REPRESENT
SOMETHING
MOVEMENTS
IN
SPACE
SCALING
FACTORS
WORD
COUNTS
MOVIE
RATINGS
PIXEL
BRIGHTNESSES
ETC
WE
LL
DEFINE
SOME
COMMON
USES
AND
STANDARD
OPERATIONS
ON
THEM
A
COLUMN
VECTOR
WHERE
A
ROW
VECTOR
WHERE
DENOTES
THE
TRANSPOSE
OPERATION
YOU
LL
WANT
TO
KEEP
TRACK
OF
THE
ORIENTATION
OF
YOUR
VECTORS
WHEN
PROGRAMMING
IN
MATLAB
YOU
CAN
TRANSPOSE
A
VECTOR
V
IN
MATLAB
BY
WRITING
V
VECTORS
CAN
REPRESENT
AN
OFFSET
IN
OR
SPACE
POINTS
ARE
JUST
VECTORS
FROM
THE
ORIGIN
DATA
CAN
ALSO
BE
TREATED
AS
A
VECTOR
SUCH
VECTORS
DON
T
HAVE
A
GEOMETRIC
INTERPRETATION
BUT
CALCULATIONS
LIKE
DISTANCE
STILL
HAVE
VALUE
A
MATRIX
IS
AN
ARRAY
OF
NUMBERS
WITH
SIZE
𝑚
BY
𝑛
I
E
M
ROWS
AND
N
COLUMNS
IF
WE
SAY
THAT
IS
SQUARE
ADDITION
CAN
ONLY
ADD
MATRICES
WITH
MATCHING
DIMENSIONS
OR
A
SCALAR
TO
A
MATRIX
SCALING
INNER
PRODUCT
DOT
PRODUCT
OF
VECTORS
MULTIPLY
CORRESPONDING
ENTRIES
OF
TWO
VECTORS
AND
ADD
UP
THE
RESULT
WE
WON
T
WORRY
ABOUT
THE
GEOMETRIC
INTERPRETATION
FOR
NOW
MULTIPLICATION
THE
PRODUCT
AB
IS
EACH
ENTRY
IN
THE
RESULT
IS
THAT
ROW
OF
A
DOT
PRODUCT
WITH
THAT
COLUMN
OF
B
MULTIPLICATION
EXAMPLE
EACH
ENTRY
OF
THE
MATRIX
PRODUCT
IS
MADE
BY
TAKING
THE
DOT
PRODUCT
OF
THE
CORRESPONDING
ROW
IN
THE
LEFT
MATRIX
WITH
THE
CORRESPONDING
COLUMN
IN
THE
RIGHT
ONE
TRANSPOSE
FLIP
MATRIX
SO
ROW
BECOMES
COLUMN
A
USEFUL
IDENTITY
SPECIAL
MATRICES
IDENTITY
MATRIX
I
SQUARE
MATRIX
ALONG
DIAGONAL
ELSEWHERE
I
ANOTHER
MATRIX
THAT
MATRIX
DIAGONAL
MATRIX
SQUARE
MATRIX
WITH
NUMBERS
ALONG
DIAGONAL
ELSEWHERE
A
DIAGONAL
ANOTHER
MATRIX
SCALES
THE
ROWS
OF
THAT
MATRIX
MINUTE
BREAK
MATLAB
TUTORIAL
WE
LL
COVER
PARTS
DO
PARTS
AT
HOME
FINISH
THE
MATLAB
TUTORIAL
ON
YOUR
OWN
POST
ON
PIAZZA
IF
QUESTIONS
NO
CLASS
MONDAY
LABOR
DAY
BUT
DUE
WEDNESDAY
IMAGE
FILTERING
DUE
READING
FOR
WEDNESDAY
SZELISKI
SEC
REMINDER
FILL
OUT
DOODLE
CS
INTRO
TO
COMPUTER
VISION
IMAGE
FILTERING
SLIDES
FROM
KRISTEN
GRAUMAN
PLAN
FOR
TODAY
MATLAB
TUTORIAL
PART
IMAGE
FILTERING
OVERVIEW
NOTE
AND
ARE
OUT
QUESTIONS
FROM
GRADED
WITH
COMMENTS
AND
ANSWERS
SEE
COURSEWEB
PRACTICAL
SKILLS
SEE
NEXT
WHAT
ARE
PROGRAMMING
ASSIGNMENTS
LIKE
SEE
WHAT
IS
MY
RESEARCH
LIKE
SEE
OBJECTIVES
BE
MORE
COMFORTABLE
WITH
MATLAB
KNOW
ONE
WAY
TO
REMOVE
NOISE
FROM
IMAGES
FILTERING
KNOW
HOW
TO
RESIZE
IMAGES
IN
A
CONTENT
AWARE
WAY
YOU
WILL
BE
ABLE
TO
REMOVE
NOISE
FROM
IMAGES
DETECT
BASIC
PATTERNS
SUCH
AS
LINES
COMPUTE
A
MEANINGFUL
REPRESENTATION
OF
THE
IMAGE
BEYOND
PIXELS
THAT
ALLOWS
YOU
TO
DO
HIGHER
LEVEL
TASKS
E
G
FINDING
CARS
STITCH
PANORAMAS
FROM
MULTIPLE
IMAGES
OF
THE
SAME
OBJECT
AUTOMATICALLY
RECOGNIZE
DIFFERENT
TYPES
OF
SCENES
E
G
BEACHES
VS
FORESTS
VS
CITIES
DETECT
FACES
IN
IMAGES
TRACK
MOVING
OBJECTS
USE
AND
UNDERSTAND
SUPPORT
VECTOR
MACHINES
USE
NEURAL
NETWORKS
AUTOMATICALLY
GROUP
DATA
ETC
LET
X
BE
AN
AXB
MATRIX
Y
BE
AN
BXC
MATRIX
THEN
Z
X
Y
IS
AN
AXC
MATRIX
SECOND
DIMENSION
OF
FIRST
MATRIX
AND
FIRST
DIMENSION
OF
FIRST
MATRIX
HAVE
TO
BE
THE
SAME
FOR
MATRIX
MULTIPLICATION
TO
BE
POSSIBLE
MATRIX
MULTIPLICATION
IS
NOT
COMMUTATIVE
X
Y
Y
X
BUT
IS
DISTRIBUTIVE
OVER
ADDITION
AND
ASSOCIATIVE
PRACTICE
LET
X
BE
AN
MATRIX
LET
FACTORIZE
IT
INTO
MATRICES
INNER
VS
OUTER
VS
MATRIX
VS
ELEMENT
WISE
PRODUCT
X
Y
COLUMN
VECTORS
X
Y
MATRICES
MXN
X
Y
SCALARS
XT
Y
X
Y
INNER
PRODUCT
X
SCALAR
X
Y
X
YT
OUTER
PRODUCT
X
MATRIX
X
Y
MATRIX
PRODUCT
X
Y
ELEMENT
WISE
PRODUCT
IMAGES
IN
MATLAB
COLOR
IMAGES
REPRESENTED
AS
A
MATRIX
WITH
MULTIPLE
CHANNELS
IF
GRAYSCALE
SUPPOSE
WE
HAVE
A
NXM
RGB
IMAGE
CALLED
IM
IM
TOP
LEFT
PIXEL
VALUE
IN
R
CHANNEL
IM
Y
X
B
Y
PIXELS
DOWN
ROWS
X
PIXELS
TO
RIGHT
COLS
IN
BTH
CHANNEL
IM
N
M
BOTTOM
RIGHT
PIXEL
IN
B
CHANNEL
IMREAD
FILENAME
RETURNS
A
IMAGE
VALUES
TO
ROW
COLUMN
R
66
ADAPTED
FROM
DEREK
HOIEM
MATLAB
TUTORIAL
WE
LL
COVER
PARTS
DO
PARTS
AT
HOME
SLIDING
WINDOW
DETECTOR
ENTER
NOISE
WE
TALKED
ABOUT
HOW
THE
SAME
OBJECT
WILL
LOOK
VERY
DIFFERENT
ACROSS
IMAGES
EVEN
MULTIPLE
IMAGES
OF
THE
SAME
STATIC
SCENE
WILL
NOT
BE
IDENTICAL
HOW
COULD
WE
REDUCE
THE
NOISE
I
E
GIVE
AN
ESTIMATE
OF
THE
TRUE
INTENSITIES
WHAT
IF
THERE
ONLY
ONE
IMAGE
COMMON
TYPES
OF
NOISE
SALT
AND
PEPPER
NOISE
RANDOM
OCCURRENCES
OF
BLACK
AND
WHITE
PIXELS
IMPULSE
NOISE
RANDOM
OCCURRENCES
OF
WHITE
PIXELS
GAUSSIAN
NOISE
VARIATIONS
IN
INTENSITY
DRAWN
FROM
A
GAUSSIAN
NORMAL
DISTRIBUTION
SOURCE
SEITZ
GAUSSIAN
NOISE
NOISE
RANDN
SIZE
IM
SIGMA
OUTPUT
IM
NOISE
WHAT
IS
IMPACT
OF
THE
SIGMA
FIG
M
HEBERT
LET
REPLACE
EACH
PIXEL
WITH
AN
AVERAGE
OF
ALL
THE
VALUES
IN
ITS
NEIGHBORHOOD
ASSUMPTIONS
EXPECT
PIXELS
TO
BE
LIKE
THEIR
NEIGHBORS
EXPECT
NOISE
PROCESSES
TO
BE
INDEPENDENT
FROM
PIXEL
TO
PIXEL
LET
REPLACE
EACH
PIXEL
WITH
AN
AVERAGE
OF
ALL
THE
VALUES
IN
ITS
NEIGHBORHOOD
MOVING
AVERAGE
IN
CAN
ADD
WEIGHTS
TO
OUR
MOVING
AVERAGE
WEIGHTS
NON
UNIFORM
WEIGHTS
BOARD
COMPUTE
A
FUNCTION
OF
THE
LOCAL
NEIGHBORHOOD
AT
EACH
PIXEL
IN
THE
IMAGE
FUNCTION
SPECIFIED
BY
A
FILTER
OR
MASK
SAYING
HOW
TO
COMBINE
VALUES
FROM
NEIGHBORS
ELEMENT
WISE
MULTIPLICATION
USES
OF
FILTERING
ENHANCE
AN
IMAGE
DENOISE
RESIZE
ETC
EXTRACT
INFORMATION
TEXTURE
EDGES
ETC
DETECT
PATTERNS
TEMPLATE
MATCHING
ADAPTED
FROM
DEREK
HOIEM
SAY
THE
AVERAGING
WINDOW
SIZE
IS
X
ATTRIBUTE
UNIFORM
WEIGHT
TO
EACH
PIXEL
LOOP
OVER
ALL
PIXELS
IN
NEIGHBORHOOD
AROUND
IMAGE
PIXEL
F
I
J
NOW
GENERALIZE
TO
ALLOW
DIFFERENT
WEIGHTS
DEPENDING
ON
NEIGHBORING
PIXEL
RELATIVE
POSITION
NON
UNIFORM
WEIGHTS
THIS
IS
CALLED
CROSS
CORRELATION
DENOTED
FILTERING
AN
IMAGE
REPLACE
EACH
PIXEL
WITH
A
LINEAR
COMBINATION
OF
ITS
NEIGHBORS
THE
FILTER
KERNEL
OR
MASK
H
U
V
IS
THE
PRESCRIPTION
FOR
THE
WEIGHTS
IN
THE
LINEAR
COMBINATION
WHAT
VALUES
BELONG
IN
THE
KERNEL
H
FOR
THE
MOVING
AVERAGE
EXAMPLE
BOX
FILTER
DEPICTS
BOX
FILTER
WHITE
HIGH
VALUE
BLACK
LOW
VALUE
ORIGINAL
FILTERED
WHAT
IF
THE
FILTER
SIZE
WAS
X
INSTEAD
OF
X
WHAT
IF
WE
WANT
NEAREST
NEIGHBORING
PIXELS
TO
HAVE
THE
MOST
INFLUENCE
ON
THE
OUTPUT
REMOVES
HIGH
FREQUENCY
COMPONENTS
FROM
THE
IMAGE
LOW
PASS
FILTER
SOURCE
SEITZ
WHAT
PARAMETERS
MATTER
HERE
SIZE
OF
KERNEL
OR
MASK
NOTE
GAUSSIAN
FUNCTION
HAS
INFINITE
SUPPORT
BUT
DISCRETE
FILTERS
USE
FINITE
KERNELS
Σ
WITH
X
KERNEL
Σ
WITH
X
KERNEL
WHAT
PARAMETERS
MATTER
HERE
VARIANCE
OF
GAUSSIAN
DETERMINES
EXTENT
OF
SMOOTHING
Σ
WITH
X
KERNEL
Σ
WITH
X
KERNEL
HOW
BIG
SHOULD
THE
FILTER
BE
VALUES
AT
EDGES
SHOULD
BE
NEAR
ZERO
IMPORTANT
RULE
OF
THUMB
FOR
GAUSSIAN
SET
FILTER
HALF
WIDTH
TO
ABOUT
Σ
SOURCE
DEREK
HOIEM
MESH
H
IMAGESC
H
OUTIM
IMFILTER
IM
H
CORRELATION
IMSHOW
OUTIM
OUTIM
PARAMETER
Σ
IS
THE
SCALE
WIDTH
SPREAD
OF
THE
GAUSSIAN
KERNEL
AND
CONTROLS
THE
AMOUNT
OF
SMOOTHING
FOR
SIGMA
H
FSPECIAL
GAUSSIAN
FSIZE
SIGMA
OUT
IMFILTER
IM
H
IMSHOW
OUT
PAUSE
END
SMOOTHING
VALUES
POSITIVE
SUM
TO
OVERALL
INTENSITY
SAME
AS
INPUT
AMOUNT
OF
SMOOTHING
PROPORTIONAL
TO
MASK
SIZE
REMOVE
HIGH
FREQUENCY
COMPONENTS
LOW
PASS
FILTER
PREDICT
THE
OUTPUTS
USING
CORRELATION
FILTERING
ORIGINAL
ORIGINAL
FILTERED
NO
CHANGE
ORIGINAL
ORIGINAL
SHIFTED
LEFT
BY
PIXEL
WITH
CORRELATION
ORIGINAL
ORIGINAL
BLUR
WITH
A
BOX
FILTER
ORIGINAL
ORIGINAL
SHARPENING
FILTER
ACCENTUATES
DIFFERENCES
WITH
LOCAL
AVERAGE
FINAL
EXAMPLE
SLIDE
CREDIT
DEREK
HOIEM
HOMEWORK
SEAM
CARVING
CONTENT
AWARE
IMAGE
RESIZING
RESIZE
EFFECT
CONTENT
AWARE
RESIZING
TRADITIONAL
RESIZING
INTUITION
CONTENT
AWARE
RESIZING
TO
REDUCE
OR
INCREASE
SIZE
IN
ONE
DIMENSION
REMOVE
IRREGULARLY
SHAPED
NON
STRAIGHT
SEAMS
OPTIMAL
SOLUTION
VIA
DYNAMIC
PROGRAMMING
ENERGY
F
WANT
TO
REMOVE
SEAMS
WHERE
THEY
WON
T
BE
VERY
NOTICEABLE
MEASURE
ENERGY
AS
GRADIENT
MAGNITUDE
HORIZONTAL
VERTICAL
CHANGE
CHOOSE
SEAM
BASED
ON
MINIMUM
TOTAL
ENERGY
PATH
ACROSS
IMAGE
SUBJECT
TO
CONNECTEDNESS
ENERGY
F
LET
A
VERTICAL
SEAM
CONSIST
OF
H
POSITIONS
THAT
FORM
AN
CONNECTED
PATH
LET
THE
COST
OF
A
SEAM
BE
COST
ENERGY
F
I
SI
OPTIMAL
SEAM
MINIMIZES
THIS
COST
MIN
COST
COMPUTE
IT
EFFICIENTLY
WITH
DYNAMIC
PROGRAMMING
COMPUTE
THE
CUMULATIVE
MINIMUM
ENERGY
FOR
ALL
POSSIBLE
CONNECTED
SEAMS
AT
EACH
ENTRY
I
J
M
I
J
ENERGY
I
MIN
M
I
J
M
I
J
M
I
J
ENERGY
MATRIX
GRADIENT
MAGNITUDE
M
MATRIX
CUMULATIVE
MIN
ENERGY
FOR
VERTICAL
SEAMS
THEN
MIN
VALUE
IN
LAST
ROW
OF
M
INDICATES
END
OF
THE
MINIMAL
CONNECTED
VERTICAL
SEAM
BACKTRACK
UP
FROM
THERE
SELECTING
MIN
OF
ABOVE
IN
M
COMPUTING
HORIZONTAL
SEAMS
IS
ANALOGOUS
M
I
J
ENERGY
I
J
MIN
M
I
J
M
I
J
M
I
J
CUMULATIVE
ENERGY
FROM
M
I
J
ENERGY
I
MIN
M
I
J
M
I
J
M
I
J
NOW
BACKTRACK
COMPUTING
CUMULATIVE
ENERGY
AND
FINDING
THE
OPTIMAL
SEAM
WITH
BACK
TRACKING
IS
IMPLEMENTED
FOR
YOU
BUT
YOU
STILL
HAVE
TO
UNDERSTAND
HOW
TO
USE
IT
ORIGINAL
IMAGE
ENERGY
MAP
BLUE
LOW
ENERGY
RED
HIGH
ENERGY
JOHN
PHILLIPS
LINEAR
FILTERS
USEFUL
FOR
ENHANCING
IMAGES
SMOOTHING
REMOVING
NOISE
BOX
FILTER
GAUSSIAN
FILTER
IMPACT
OF
SCALE
WIDTH
OF
SMOOTHING
FILTER
DETECTING
PATTERNS
E
G
GRADIENTS
CONTENT
AWARE
IMAGE
RESIZING
USING
IMAGE
ENERGY
AND
GRADIENTS
MORE
FILTERS
PROPERTIES
OF
FILTERS
APPLICATIONS
TEXTURE
REPRESENTATION
AND
SYNTHESIS
PATTERN
MATCHING
REMINDERS
AND
DUE
MONDAY
CS
INTRO
TO
COMPUTER
VISION
FILTERING
AND
TEXTURE
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
REMINDER
DUE
TONIGHT
TYPES
OF
FILTERS
LINEAR
SMOOTHING
OTHER
NON
LINEAR
MEDIAN
TEXTURE
REPRESENTATION
WITH
FILTERS
COMPUTE
A
FUNCTION
OF
THE
LOCAL
NEIGHBORHOOD
AT
EACH
PIXEL
IN
THE
IMAGE
FUNCTION
SPECIFIED
BY
A
FILTER
OR
MASK
SAYING
HOW
TO
COMBINE
VALUES
FROM
NEIGHBORS
ELEMENT
WISE
MULTIPLICATION
USES
OF
FILTERING
ENHANCE
AN
IMAGE
DENOISE
RESIZE
ETC
EXTRACT
INFORMATION
TEXTURE
EDGES
ETC
DETECT
PATTERNS
TEMPLATE
MATCHING
ADAPTED
FROM
DEREK
HOIEM
SAY
THE
AVERAGING
WINDOW
SIZE
IS
X
ATTRIBUTE
UNIFORM
WEIGHT
LOOP
OVER
ALL
PIXELS
IN
NEIGHBORHOOD
AROUND
TO
EACH
PIXEL
IMAGE
PIXEL
F
I
J
NOW
GENERALIZE
TO
ALLOW
DIFFERENT
WEIGHTS
DEPENDING
ON
NEIGHBORING
PIXEL
RELATIVE
POSITION
NON
UNIFORM
WEIGHTS
THIS
IS
CALLED
CROSS
CORRELATION
DENOTED
FILTERING
AN
IMAGE
REPLACE
EACH
PIXEL
WITH
A
LINEAR
COMBINATION
OF
ITS
NEIGHBORS
THE
FILTER
KERNEL
OR
MASK
H
U
V
IS
THE
PRESCRIPTION
FOR
THE
WEIGHTS
IN
THE
LINEAR
COMBINATION
WHAT
VALUES
BELONG
IN
THE
KERNEL
H
FOR
THE
MOVING
AVERAGE
EXAMPLE
BOX
FILTER
DEPICTS
BOX
FILTER
WHITE
HIGH
VALUE
BLACK
LOW
VALUE
ORIGINAL
FILTERED
WHAT
IF
THE
FILTER
SIZE
WAS
X
INSTEAD
OF
X
F
IMAGE
H
FILTER
BOUNDARY
ISSUES
WHAT
IS
THE
SIZE
OF
THE
OUTPUT
MATLAB
OUTPUT
SIZE
OPTIONS
SHAPE
FULL
OUTPUT
SIZE
IS
LARGER
THAN
THE
SIZE
OF
F
SHAPE
SAME
OUTPUT
SIZE
IS
SAME
AS
F
SHAPE
VALID
OUTPUT
SIZE
IS
DIFFERENCE
OF
SIZES
OF
F
AND
H
DISCONTINUED
FULL
SAME
VALID
ADAPTED
FROM
LAZEBNIK
WHAT
ABOUT
NEAR
THE
EDGE
THE
FILTER
WINDOW
MIGHT
FALL
OFF
THE
EDGE
OF
THE
IMAGE
IN
SAME
OR
FULL
NEED
TO
EXTRAPOLATE
METHODS
CLIP
FILTER
BLACK
WRAP
AROUND
COPY
EDGE
REFLECT
ACROSS
EDGE
WHAT
ABOUT
NEAR
THE
EDGE
THE
FILTER
WINDOW
MIGHT
FALL
OFF
THE
EDGE
OF
THE
IMAGE
IN
SAME
OR
FULL
NEED
TO
EXTRAPOLATE
METHODS
MATLAB
CLIP
FILTER
BLACK
IMFILTER
F
G
WRAP
AROUND
IMFILTER
F
G
CIRCULAR
COPY
EDGE
IMFILTER
F
G
REPLICATE
REFLECT
ACROSS
EDGE
IMFILTER
F
G
SYMMETRIC
CONVOLUTION
FLIP
THE
FILTER
IN
BOTH
DIMENSIONS
BOTTOM
TO
TOP
RIGHT
TO
LEFT
THEN
APPLY
CROSS
CORRELATION
NOTATION
FOR
CONVOLUTION
OPERATOR
KRISTEN
GRAUMAN
CONVOLUTION
CROSS
CORRELATION
FOR
A
GAUSSIAN
OR
BOX
FILTER
HOW
WILL
THE
OUTPUTS
DIFFER
F
CONVOLUTION
U
V
H
I
J
F
CONVOLUTION
U
V
V
H
I
J
F
CONVOLUTION
U
V
V
V
H
I
J
F
CONVOLUTION
U
V
V
V
U
V
H
I
J
CROSS
CORRELATION
U
V
F
H
I
J
CROSS
CORRELATION
U
V
V
F
H
I
J
CROSS
CORRELATION
U
V
V
V
F
H
I
J
CROSS
CORRELATION
F
I
J
U
V
V
V
U
V
H
PROPERTIES
OF
CONVOLUTION
COMMUTATIVE
F
G
G
F
ASSOCIATIVE
F
G
H
F
G
H
DISTRIBUTES
OVER
ADDITION
F
G
H
F
G
F
H
SCALARS
FACTOR
OUT
KF
G
F
KG
K
F
G
IDENTITY
UNIT
IMPULSE
E
F
E
F
SEPARABILITY
IN
SOME
CASES
FILTER
IS
SEPARABLE
AND
WE
CAN
FACTOR
INTO
TWO
STEPS
CONVOLVE
ALL
ROWS
CONVOLVE
ALL
COLUMNS
SEPARABILITY
EXAMPLE
FILTERING
CENTER
LOCATION
ONLY
THE
FILTER
FACTORS
INTO
AN
OUTER
PRODUCT
OF
FILTERS
PERFORM
FILTERING
ALONG
ROWS
FOLLOWED
BY
FILTERING
ALONG
THE
REMAINING
COLUMN
PLAN
FOR
TODAY
FILTERS
MATH
AND
PROPERTIES
TYPES
OF
FILTERS
LINEAR
OTHER
NON
LINEAR
MEDIAN
TEXTURE
REPRESENTATION
WITH
FILTERS
GAUSSIAN
FILTER
WHAT
IF
WE
WANT
NEAREST
NEIGHBORING
PIXELS
TO
HAVE
THE
MOST
INFLUENCE
ON
THE
OUTPUT
SOURCE
SEITZ
SMOOTHING
WITH
A
GAUSSIAN
WHAT
PARAMETERS
MATTER
HERE
SIZE
OF
KERNEL
OR
MASK
NOTE
GAUSSIAN
FUNCTION
HAS
INFINITE
SUPPORT
BUT
DISCRETE
FILTERS
USE
FINITE
KERNELS
Σ
WITH
X
KERNEL
Σ
WITH
X
KERNEL
WHAT
PARAMETERS
MATTER
HERE
VARIANCE
OF
GAUSSIAN
DETERMINES
EXTENT
OF
SMOOTHING
Σ
WITH
X
KERNEL
Σ
WITH
X
KERNEL
HOW
BIG
SHOULD
THE
FILTER
BE
VALUES
AT
EDGES
SHOULD
BE
NEAR
ZERO
IMPORTANT
RULE
OF
THUMB
FOR
GAUSSIAN
SET
FILTER
HALF
WIDTH
TO
ABOUT
Σ
SOURCE
DEREK
HOIEM
MESH
H
IMAGESC
H
OUTIM
IMFILTER
IM
H
CORRELATION
IMSHOW
OUTIM
OUTIM
PARAMETER
Σ
IS
THE
SCALE
WIDTH
SPREAD
OF
THE
GAUSSIAN
KERNEL
AND
CONTROLS
THE
AMOUNT
OF
SMOOTHING
FOR
SIGMA
H
FSPECIAL
GAUSSIAN
FSIZE
SIGMA
OUT
IMFILTER
IM
H
IMSHOW
OUT
PAUSE
END
SMOOTHING
VALUES
POSITIVE
SUM
TO
OVERALL
INTENSITY
SAME
AS
INPUT
AMOUNT
OF
SMOOTHING
PROPORTIONAL
TO
MASK
SIZE
REMOVE
HIGH
FREQUENCY
COMPONENTS
LOW
PASS
FILTER
FILTERS
MATH
AND
PROPERTIES
TYPES
OF
FILTERS
LINEAR
SMOOTHING
NON
LINEAR
MEDIAN
TEXTURE
REPRESENTATION
WITH
FILTERS
PREDICT
THE
OUTPUTS
USING
CORRELATION
FILTERING
KRISTEN
GRAUMAN
ORIGINAL
SHARPENING
FILTER
ACCENTUATES
DIFFERENCES
WITH
LOCAL
AVERAGE
SOURCE
D
LOWE
KRISTEN
GRAUMAN
KRISTEN
GRAUMAN
AUDE
OLIVA
ANTONIO
TORRALBA
PHILIPPE
G
SCHYNS
SIGGRAPH
GAUSSIAN
FILTER
OLIVA
A
TORRALBA
P
G
SCHYNS
SIGGRAPH
LAPLACIAN
FILTER
SHARPENING
UNIT
IMPULSE
GAUSSIAN
LAPLACIAN
OF
GAUSSIAN
KRISTEN
GRAUMAN
FILTERS
FOR
COMPUTING
GRADIENTS
SLIDE
CREDIT
DEREK
HOIEM
FILTERS
MATH
AND
PROPERTIES
TYPES
OF
FILTERS
LINEAR
SMOOTHING
OTHER
TEXTURE
REPRESENTATION
WITH
FILTERS
NO
NEW
PIXEL
VALUES
INTRODUCED
REMOVES
SPIKES
GOOD
FOR
IMPULSE
SALT
PEPPER
NOISE
NON
LINEAR
FILTER
MEDIAN
FILTER
IS
EDGE
PRESERVING
SALT
AND
PEPPER
NOISE
PLOTS
OF
A
ROW
OF
THE
IMAGE
MATLAB
OUTPUT
IM
IM
H
W
MEDIAN
FILTERED
SOURCE
M
HEBERT
MINUTE
BREAK
PLAN
FOR
TODAY
FILTERS
MATH
AND
PROPERTIES
TYPES
OF
FILTERS
LINEAR
SMOOTHING
OTHER
NON
LINEAR
MEDIAN
TEXTURE
WHAT
DEFINES
A
TEXTURE
KRISTEN
GRAUMAN
KRISTEN
GRAUMAN
IMPORTANT
FOR
HOW
WE
PERCEIVE
OBJECTS
OFTEN
INDICATIVE
OF
A
MATERIAL
PROPERTIES
CAN
BE
IMPORTANT
APPEARANCE
CUE
ESPECIALLY
IF
SHAPE
IS
SIMILAR
ACROSS
OBJECTS
TO
REPRESENT
OBJECTS
WE
WANT
A
FEATURE
ONE
STEP
ABOVE
BUILDING
BLOCKS
OF
FILTERS
EDGES
TEXTURES
ARE
MADE
UP
OF
REPEATED
LOCAL
PATTERNS
SO
FIND
THE
PATTERNS
USE
FILTERS
THAT
LOOK
LIKE
PATTERNS
SPOTS
BARS
RAW
PATCHES
CONSIDER
MAGNITUDE
OF
RESPONSE
DESCRIBE
THEIR
STATISTICS
WITHIN
EACH
LOCAL
WINDOW
MEAN
STANDARD
DEVIATION
HISTOGRAM
ORIGINAL
IMAGE
ORIGINAL
IMAGE
ORIGINAL
IMAGE
DIMENSION
MEAN
D
DX
VALUE
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
WITH
PRIMARILY
HORIZONTAL
EDGES
BOTH
WINDOWS
WITH
SMALL
GRADIENT
IN
BOTH
DIRECTIONS
WINDOWS
WITH
PRIMARILY
VERTICAL
EDGES
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
ORIGINAL
IMAGE
KRISTEN
GRAUMAN
DERIVATIVE
FILTER
RESPONSES
SQUARED
FAR
DISSIMILAR
TEXT
CLOSE
SIMILAR
TEXT
DIMENSION
MEAN
D
DX
VALUE
KRISTEN
GRAUMAN
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
D
A
B
DIMENSION
A
B
DIMENSION
DISTANCE
REVEALS
HOW
DISSIMILAR
TEXTURE
B
FROM
WINDOW
A
IS
FROM
TEXTURE
IN
WINDOW
B
OUR
PREVIOUS
EXAMPLE
USED
TWO
FILTERS
AND
RESULTED
IN
A
DIMENSIONAL
FEATURE
VECTOR
TO
DESCRIBE
TEXTURE
IN
A
WINDOW
X
AND
Y
DERIVATIVES
REVEALED
SOMETHING
ABOUT
LOCAL
STRUCTURE
WE
CAN
GENERALIZE
TO
APPLY
A
COLLECTION
OF
MULTIPLE
D
FILTERS
A
FILTER
BANK
THEN
OUR
FEATURE
VECTORS
WILL
BE
D
DIMENSIONAL
STILL
CAN
THINK
OF
NEARNESS
FARNESS
IN
FEATURE
SPACE
COMPUTING
DISTANCES
WITH
D
DIMENSIONAL
FEATURES
D
A
B
EUCLIDEAN
DISTANCE
FILTER
BANKS
SCALES
WHAT
FILTERS
TO
PUT
IN
THE
BANK
TYPICALLY
WE
WANT
A
COMBINATION
OF
SCALES
AND
ORIENTATIONS
DIFFERENT
TYPES
OF
PATTERNS
MATLAB
CODE
AVAILABLE
FOR
THESE
EXAMPLES
FILTER
BANK
D
D
D
D
D
EJ
I
L
L
L
J
LJ
J
MULTIVARIATE
GAUSSIAN
IMAGE
FROM
VECTORS
OF
TEXTURE
RESPONSES
WE
CAN
FORM
A
FEATURE
VECTOR
FROM
THE
LIST
OF
RESPONSES
AT
EACH
PIXEL
GIVES
US
A
REPRESENTATION
OF
THE
PIXEL
IMAGE
YOU
TRY
CAN
YOU
MATCH
THE
TEXTURE
TO
THE
RESPONSE
FILTERS
A
MEAN
ABS
RESPONSES
B
C
DEREK
HOIEM
REPRESENTING
TEXTURE
BY
MEAN
ABS
RESPONSE
FILTERS
MEAN
ABS
RESPONSES
DEREK
HOIEM
CLASSIFYING
MATERIALS
STUFF
FIGURE
BY
VARMA
ZISSERMAN
KRISTEN
GRAUMAN
SUMMARY
FILTERS
USEFUL
FOR
ENHANCING
IMAGES
SMOOTHING
REMOVING
NOISE
E
G
BOX
FILTER
LINEAR
GAUSSIAN
FILTER
LINEAR
MEDIAN
FILTER
DETECTING
PATTERNS
E
G
GRADIENTS
TEXTURE
IS
A
USEFUL
PROPERTY
THAT
IS
OFTEN
INDICATIVE
OF
MATERIALS
APPEARANCE
CUES
TEXTURE
REPRESENTATIONS
ATTEMPT
TO
SUMMARIZE
REPEATING
PATTERNS
OF
LOCAL
STRUCTURE
FILTER
BANKS
USEFUL
TO
MEASURE
REDUNDANT
VARIETY
OF
STRUCTURES
IN
LOCAL
NEIGHBORHOOD
CS
INTRO
TO
COMPUTER
VISION
TEXTURE
REPRESENTATION
IMAGE
PYRAMIDS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
REMINDERS
ANNOUNCEMENTS
DUE
TONIGHT
OUT
SHUFFLE
PLAN
FOR
TODAY
FILTERING
APPLICATION
TO
SUBSAMPLING
IMAGE
PYRAMIDS
DETECTING
INTERESTING
CONTENT
START
CONVOLUTION
VS
CORRELATION
CONVOLUTION
CROSS
CORRELATION
TEXTURE
WHAT
DEFINES
A
TEXTURE
KRISTEN
GRAUMAN
KRISTEN
GRAUMAN
IMPORTANT
FOR
HOW
WE
PERCEIVE
OBJECTS
OFTEN
INDICATIVE
OF
A
MATERIAL
PROPERTIES
CAN
BE
IMPORTANT
APPEARANCE
CUE
ESPECIALLY
IF
SHAPE
IS
SIMILAR
ACROSS
OBJECTS
TO
REPRESENT
OBJECTS
WE
WANT
A
FEATURE
ONE
STEP
ABOVE
BUILDING
BLOCKS
OF
FILTERS
EDGES
TEXTURES
ARE
MADE
UP
OF
REPEATED
LOCAL
PATTERNS
SO
FIND
THE
PATTERNS
USE
FILTERS
THAT
LOOK
LIKE
PATTERNS
SPOTS
BARS
RAW
PATCHES
CONSIDER
MAGNITUDE
OF
RESPONSE
DESCRIBE
THEIR
STATISTICS
WITHIN
EACH
LOCAL
WINDOW
MEAN
STANDARD
DEVIATION
HISTOGRAM
ORIGINAL
IMAGE
ORIGINAL
IMAGE
ORIGINAL
IMAGE
DIMENSION
MEAN
D
DX
VALUE
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
WITH
PRIMARILY
HORIZONTAL
EDGES
BOTH
WINDOWS
WITH
SMALL
GRADIENT
IN
BOTH
DIRECTIONS
WINDOWS
WITH
PRIMARILY
VERTICAL
EDGES
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
ORIGINAL
IMAGE
KRISTEN
GRAUMAN
DERIVATIVE
FILTER
RESPONSES
SQUARED
FAR
DISSIMILAR
TEXT
CLOSE
SIMILAR
TEXT
DIMENSION
MEAN
D
DX
VALUE
KRISTEN
GRAUMAN
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
D
A
B
DIMENSION
A
B
DIMENSION
DISTANCE
REVEALS
HOW
DISSIMILAR
TEXTURE
B
FROM
WINDOW
A
IS
FROM
TEXTURE
IN
WINDOW
B
OUR
PREVIOUS
EXAMPLE
USED
TWO
FILTERS
AND
RESULTED
IN
A
DIMENSIONAL
FEATURE
VECTOR
TO
DESCRIBE
TEXTURE
IN
A
WINDOW
X
AND
Y
DERIVATIVES
REVEALED
SOMETHING
ABOUT
LOCAL
STRUCTURE
WE
CAN
GENERALIZE
TO
APPLY
A
COLLECTION
OF
MULTIPLE
D
FILTERS
A
FILTER
BANK
THEN
OUR
FEATURE
VECTORS
WILL
BE
D
DIMENSIONAL
STILL
CAN
THINK
OF
NEARNESS
FARNESS
IN
FEATURE
SPACE
SCALES
WHAT
FILTERS
TO
PUT
IN
THE
BANK
TYPICALLY
WE
WANT
A
COMBINATION
OF
SCALES
AND
ORIENTATIONS
DIFFERENT
TYPES
OF
PATTERNS
MATLAB
CODE
AVAILABLE
FOR
THESE
EXAMPLES
FILTER
BANK
D
D
D
D
D
EJ
I
L
L
L
J
LJ
J
MULTIVARIATE
GAUSSIAN
IMAGE
FROM
VECTORS
OF
TEXTURE
RESPONSES
WE
CAN
FORM
A
FEATURE
VECTOR
FROM
THE
LIST
OF
RESPONSES
AT
EACH
PIXEL
GIVES
US
A
REPRESENTATION
OF
THE
PIXEL
IMAGE
YOU
TRY
CAN
YOU
MATCH
THE
TEXTURE
TO
THE
RESPONSE
FILTERS
A
B
C
REPRESENTING
TEXTURE
BY
MEAN
ABS
RESPONSE
FILTERS
VECTORS
OF
TEXTURE
RESPONSES
THE
FREQUENCY
OF
IMAGE
PATCH
RESPONSES
IN
AN
IMAGE
TELLS
US
SOMETHING
ABOUT
THE
IMAGE
IF
MY
PATCHES
TEND
TO
RESPOND
STRONGLY
TO
BLOBS
AND
ANOTHER
IMAGE
PATCHES
RESPOND
STRONGLY
TO
VERTICAL
EDGES
THEN
THAT
IMAGE
AND
I
ARE
PROBABLY
OF
DIFFERENT
MATERIALS
HOW
TO
CAPTURE
ONE
IDEA
CONCATENATE
FROM
THE
PREVIOUS
SLIDE
FOR
ALL
IMAGE
PIXELS
PROBLEMS
MEANS
OR
HISTOGRAMS
OF
FEATURE
RESPONSES
INSTEAD
OF
CONCATENATING
COMPUTE
THE
MEAN
ACROSS
ALL
IMAGE
PIXELS
TO
EACH
OF
THE
FILTERS
INSTEAD
OF
CONCATENATING
COUNT
ASSUME
ALL
RESPONSES
BETWEEN
AND
HOW
MANY
TIMES
WITHIN
THE
IMAGE
DID
I
SEE
AN
RESPONSE
BETWEEN
AND
BETWEEN
AND
HOW
MANY
TIMES
DID
I
SEE
RESPONSE
BETWEEN
AND
CONCATENATE
THESE
COUNTS
INTO
A
HISTOGRAM
TRADEOFFS
CLASSIFYING
MATERIALS
STUFF
FIGURE
BY
VARMA
ZISSERMAN
KRISTEN
GRAUMAN
MINUTE
BREAK
PLAN
FOR
TODAY
FILTERING
REPRESENTING
TEXTURE
DETECTING
INTERESTING
CONTENT
START
SAMPLING
WHY
DOES
A
LOWER
RESOLUTION
IMAGE
STILL
MAKE
SENSE
TO
US
WHAT
DO
WE
LOSE
DEREK
HOIEM
IMAGE
SUBSAMPLING
BY
A
FACTOR
OF
THROW
AWAY
EVERY
OTHER
ROW
AND
COLUMN
TO
CREATE
A
SIZE
IMAGE
DEREK
HOIEM
EXAMPLE
SINEWAVE
EXAMPLE
SINEWAVE
SUB
SAMPLING
MAY
BE
DANGEROUS
CHARACTERISTIC
ERRORS
MAY
APPEAR
WAGON
WHEELS
ROLLING
THE
WRONG
WAY
IN
MOVIES
STRIPED
SHIRTS
LOOK
FUNNY
ON
COLOR
TELEVISION
SAMPLING
AND
ALIASING
NYQUIST
SHANNON
SAMPLING
THEOREM
WHEN
SAMPLING
A
SIGNAL
AT
DISCRETE
INTERVALS
THE
SAMPLING
FREQUENCY
MUST
BE
FMAX
FMAX
MAX
FREQUENCY
OF
THE
INPUT
SIGNAL
THIS
WILL
ALLOWS
TO
RECONSTRUCT
THE
ORIGINAL
PERFECTLY
FROM
THE
SAMPLED
VERSION
GOOD
BAD
ANTI
ALIASING
SOLUTIONS
SAMPLE
MORE
OFTEN
GET
RID
OF
ALL
FREQUENCIES
THAT
ARE
GREATER
THAN
HALF
THE
NEW
SAMPLING
FREQUENCY
WILL
LOSE
INFORMATION
BUT
IT
BETTER
THAN
ALIASING
APPLY
A
SMOOTHING
FILTER
ALGORITHM
FOR
DOWNSAMPLING
BY
FACTOR
OF
START
WITH
IMAGE
H
W
APPLY
LOW
PASS
FILTER
IMFILTER
IMAGE
FSPECIAL
GAUSSIAN
SAMPLE
EVERY
OTHER
PIXEL
END
END
GAUSSIAN
FILTER
SAMPLE
ANTI
ALIASING
FORSYTH
AND
PONCE
SUBSAMPLING
WITHOUT
PRE
FILTERING
ZOOM
ZOOM
SUBSAMPLING
WITH
GAUSSIAN
PRE
FILTERING
GAUSSIAN
G
G
SUBSAMPLING
AWAY
WHY
WOULD
WE
WANT
TO
DO
THIS
CAN
WE
RECONSTRUCT
THE
ORIGINAL
FROM
THE
LAPLACIAN
PYRAMID
DEREK
HOIEM
LAPLACIAN
FILTER
UNIT
IMPULSE
GAUSSIAN
LAPLACIAN
OF
GAUSSIAN
PLAN
FOR
TODAY
FILTERING
REPRESENTING
TEXTURE
APPLICATION
TO
SUBSAMPLING
IMAGE
PYRAMIDS
AN
IMAGE
IS
A
SET
OF
PIXELS
NOT
INVARIANT
TO
SMALL
CHANGES
TRANSLATION
ILLUMINATION
ETC
SOME
PARTS
OF
AN
IMAGE
ARE
MORE
IMPORTANT
THAN
OTHERS
WHAT
DO
WE
WANT
TO
REPRESENT
YARBUS
EYE
TRACKING
CHOOSING
DISTINCTIVE
INTEREST
ING
POINTS
IF
YOU
WANTED
TO
MEET
A
FRIEND
WOULD
YOU
SAY
LET
MEET
ON
CAMPUS
LET
MEET
ON
GREEN
STREET
LET
MEET
AT
GREEN
AND
WRIGHT
CORNER
DETECTION
OR
IF
YOU
WERE
IN
A
SECLUDED
AREA
LET
MEET
IN
THE
PLAINS
OF
AKBAR
LET
MEET
ON
THE
SIDE
OF
MT
DOOM
LET
MEET
ON
TOP
OF
MT
DOOM
BLOB
VALLEY
PEAK
DETECTION
WHERE
WOULD
YOU
TELL
YOUR
FRIEND
TO
MEET
YOU
WHERE
WOULD
YOU
TELL
YOUR
FRIEND
TO
MEET
YOU
SUPPOSE
YOU
HAVE
TO
CLICK
ON
SOME
POINT
GO
AWAY
AND
COME
BACK
AFTER
I
DEFORM
THE
IMAGE
AND
CLICK
ON
THE
SAME
POINTS
AGAIN
WHICH
POINTS
WOULD
YOU
CHOOSE
ORIGINAL
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
GRADED
GRADED
WITH
SOME
COMMENTS
ON
SUBMISSION
ITSELF
PLEASE
READ
MY
FEEDBACK
I
DIDN
T
RESPOND
TO
YOU
ON
THE
LAST
QUESTIONS
MY
SILENCE
DOES
NOT
MEAN
YOU
RE
CORRECT
TRY
TO
BE
CONCISE
WITHOUT
FAILING
TO
GIVE
ME
THE
ANSWER
IF
YOU
HAVE
OR
LESS
YOU
SHOULD
BE
CONCERNED
THE
BOOK
IS
OFTEN
A
USEFUL
RESOURCE
LAPLACIAN
VS
SOBEL
M
LOTS
OF
QUESTIONS
ABOUT
IS
IT
OK
IF
I
DO
THIS
YOU
DON
T
HAVE
TO
WORRY
SO
MUCH
ABOUT
DOING
EXACTLY
WHAT
I
SAID
AS
LONG
AS
YOU
DO
THE
CORE
TASK
OF
THE
ASSIGNMENT
AFTER
IT
DOESN
T
MATTER
HOW
YOU
PRINT
OR
WHETHER
YOU
PUT
MULTIPLE
FIGURES
IN
ONE
ETC
HOW
LONG
DID
IT
TAKE
WHAT
TOOK
THE
MOST
TIME
WHAT
WAS
MOST
FUN
WHAT
WAS
MOST
ANNOYING
ENTER
SOCRATIVE
SEPARABILITY
EXAMPLE
FILTERING
CENTER
LOCATION
ONLY
THE
FILTER
FACTORS
INTO
AN
OUTER
PRODUCT
OF
FILTERS
PERFORM
FILTERING
ALONG
ROWS
FOLLOWED
BY
FILTERING
ALONG
THE
REMAINING
COLUMN
KRISTEN
GRAUMAN
PLAN
FOR
TODAY
FEATURE
DETECTION
KEYPOINT
EXTRACTION
CORNER
DETECTION
BLOB
DETECTION
START
FEATURE
DESCRIPTION
OF
DETECTED
FEATURES
PROBLEMS
WITH
PIXEL
REPRESENTATION
NOT
INVARIANT
TO
SMALL
CHANGES
TRANSLATION
ILLUMINATION
ETC
SOME
PARTS
OF
AN
IMAGE
ARE
MORE
IMPORTANT
THAN
OTHERS
WHAT
DO
WE
WANT
TO
REPRESENT
LOCAL
FEATURES
LOCAL
MEANS
THAT
THEY
ONLY
COVER
A
SMALL
PART
OF
THE
IMAGE
THERE
WILL
BE
MANY
LOCAL
FEATURES
DETECTED
IN
AN
IMAGE
LATER
WE
LL
TALK
ABOUT
HOW
TO
USE
THOSE
TO
COMPUTE
A
REPRESENTATION
OF
THE
WHOLE
IMAGE
LOCAL
FEATURES
USUALLY
EXPLOIT
IMAGE
GRADIENTS
RARELY
COLOR
WE
LL
REVISIT
THIS
STATEMENT
WITH
CNNS
LOCAL
FEATURES
DESIRED
PROPERTIES
LOCALITY
A
FEATURE
OCCUPIES
A
RELATIVELY
SMALL
AREA
OF
THE
IMAGE
ROBUST
TO
CLUTTER
AND
OCCLUSION
REPEATABILITY
AND
FLEXIBILITY
THE
SAME
FEATURE
CAN
BE
FOUND
IN
SEVERAL
IMAGES
DESPITE
GEOMETRIC
PHOTOMETRIC
TRANSFORMATIONS
ROBUSTNESS
TO
EXPECTED
VARIATIONS
MAXIMIZE
CORRECT
MATCHES
DISTINCTIVENESS
EACH
FEATURE
HAS
A
DISTINCTIVE
DESCRIPTION
MINIMIZE
WRONG
MATCHES
COMPACTNESS
AND
EFFICIENCY
MANY
FEWER
FEATURES
THAN
IMAGE
PIXELS
INTEREST
ING
POINTS
NOTE
INTEREST
POINTS
KEYPOINTS
ALSO
SOMETIMES
CALLED
FEATURES
MANY
APPLICATIONS
IMAGE
SEARCH
WHICH
POINTS
WOULD
ALLOW
US
TO
MATCH
IMAGES
BETWEEN
QUERY
AND
DATABASE
RECOGNITION
WHICH
PATCHES
ARE
LIKELY
TO
TELL
US
SOMETHING
ABOUT
OBJECT
CATEGORY
RECONSTRUCTION
HOW
TO
FIND
CORRESPONDENCES
ACROSS
DIFFERENT
VIEWS
TRACKING
WHICH
POINTS
ARE
GOOD
TO
TRACK
INTEREST
POINTS
SUPPOSE
YOU
HAVE
TO
CLICK
ON
SOME
POINT
GO
AWAY
AND
COME
BACK
AFTER
I
DEFORM
THE
IMAGE
AND
CLICK
ON
THE
SAME
POINTS
AGAIN
WHICH
POINTS
WOULD
YOU
CHOOSE
ORIGINAL
WHERE
WOULD
YOU
TELL
YOUR
FRIEND
TO
MEET
YOU
CORNER
DETECTION
WHERE
WOULD
YOU
TELL
YOUR
FRIEND
TO
MEET
YOU
BLOB
DETECTION
FIND
A
SET
OF
DISTINCTIVE
KEY
POINTS
DEFINE
A
REGION
AROUND
EACH
KEYPOINT
WINDOW
COMPUTE
A
LOCAL
DESCRIPTOR
FROM
THE
REGION
D
F
A
FB
T
MATCH
DESCRIPTORS
ADAPTED
FROM
K
GRAUMAN
B
LEIBE
WE
WANT
TO
DETECT
POINTS
THAT
ARE
REPEATABLE
AND
DISTINCTIVE
REPEATABLE
SO
THAT
IF
IMAGES
ARE
SLIGHTLY
DIFFERENT
WE
CAN
STILL
RETRIEVE
THEM
DISTINCTIVE
SO
WE
DON
T
RETRIEVE
IRRELEVANT
CONTENT
ADAPTED
FROM
D
HOIEM
STEP
EXTRACT
FEATURES
STEP
MATCH
FEATURES
STEP
EXTRACT
FEATURES
STEP
MATCH
FEATURES
STEP
ALIGN
IMAGES
NO
CHANCE
TO
FIND
TRUE
MATCHES
YET
WE
HAVE
TO
BE
ABLE
TO
RUN
THE
DETECTION
PROCEDURE
INDEPENDENTLY
PER
IMAGE
WE
WANT
TO
DETECT
AT
LEAST
SOME
OF
THE
SAME
POINTS
IN
BOTH
IMAGES
WANT
REPEATABILITY
OF
THE
INTEREST
OPERATOR
WE
WANT
TO
BE
ABLE
TO
RELIABLY
DETERMINE
WHICH
POINT
GOES
WITH
WHICH
WANT
OPERATOR
DISTINCTIVENESS
MUST
PROVIDE
SOME
INVARIANCE
TO
GEOMETRIC
AND
PHOTOMETRIC
DIFFERENCES
BETWEEN
THE
TWO
VIEWS
WITHOUT
FINDING
MANY
FALSE
MATCHES
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
KEYPOINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
KEYPOINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
KEYPOINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
KEYPOINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
KEYPOINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
WHAT
POINTS
WOULD
YOU
CHOOSE
GRAUMAN
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
WINDOW
FUNCTION
W
X
Y
OR
IN
WINDOW
OUTSIDE
GAUSSIAN
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
E
U
V
EXPANDING
I
X
Y
IN
A
TAYLOR
SERIES
EXPANSION
WE
HAVE
FOR
SMALL
SHIFTS
U
V
A
QUADRATIC
APPROXIMATION
TO
THE
ERROR
SURFACE
BETWEEN
A
PATCH
AND
ITSELF
SHIFTED
BY
U
V
WHERE
M
IS
A
MATRIX
COMPUTED
FROM
IMAGE
DERIVATIVES
M
W
X
Y
IX
IX
IX
I
Y
X
Y
I
Y
I
Y
NOTATION
I
I
X
X
I
I
Y
Y
I
X
I
Y
I
I
X
Y
SINCE
M
IS
SYMMETRIC
WE
HAVE
M
X
X
T
MXI
I
XI
THE
EIGENVALUES
OF
M
REVEAL
THE
AMOUNT
OF
INTENSITY
CHANGE
IN
THE
TWO
PRINCIPAL
ORTHOGONAL
GRADIENT
DIRECTIONS
IN
THE
WINDOW
EDGE
CORNER
AND
ARE
LARGE
FLAT
REGION
AND
ARE
SMALL
MEASURE
OF
CORNER
RESPONSE
K
EMPIRICAL
CONSTANT
K
COMPUTE
IMAGE
GRADIENTS
IX
AND
IY
FOR
ALL
PIXELS
FOR
EACH
PIXEL
COMPUTE
BY
LOOPING
OVER
NEIGHBORS
X
Y
COMPUTE
K
EMPIRICAL
CONSTANT
K
FIND
POINTS
WITH
LARGE
CORNER
RESPONSE
FUNCTION
R
R
THRESHOLD
TAKE
THE
POINTS
OF
LOCALLY
MAXIMUM
R
AS
THE
DETECTED
FEATURE
POINTS
I
E
PIXELS
WHERE
R
IS
BIGGER
THAN
FOR
ALL
THE
OR
NEIGHBORS
CORNER
RESPONSE
AT
EVERY
PIXEL
EFFECT
A
VERY
PRECISE
CORNER
DETECTOR
CS
INTRO
TO
COMPUTER
VISION
LOCAL
FEATURE
DETECTION
CONT
D
DESCRIPTION
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
ANNOUNCEMENTS
GRADED
DUE
TONIGHT
PLEASE
READ
THE
ENTIRE
ASSIGNMENT
BEFORE
YOU
START
LET
MAKE
SURE
WE
RE
ON
THE
SAME
PAGE
FOR
PART
I
PROBLEMS
CONCERNS
OUT
PLAN
FOR
TODAY
FEATURE
DETECTION
KEYPOINT
EXTRACTION
CORNER
DETECTION
RECAP
PROPERTIES
BLOB
DETECTION
FEATURE
DESCRIPTION
OF
DETECTED
FEATURES
LOCAL
FEATURES
DESIRED
PROPERTIES
LOCALITY
A
FEATURE
OCCUPIES
A
RELATIVELY
SMALL
AREA
OF
THE
IMAGE
ROBUST
TO
CLUTTER
AND
OCCLUSION
REPEATABILITY
AND
FLEXIBILITY
THE
SAME
FEATURE
CAN
BE
FOUND
IN
SEVERAL
IMAGES
DESPITE
GEOMETRIC
PHOTOMETRIC
TRANSFORMATIONS
ROBUSTNESS
TO
EXPECTED
VARIATIONS
MAXIMIZE
CORRECT
MATCHES
DISTINCTIVENESS
EACH
FEATURE
HAS
A
DISTINCTIVE
DESCRIPTION
MINIMIZE
WRONG
MATCHES
COMPACTNESS
AND
EFFICIENCY
MANY
FEWER
FEATURES
THAN
IMAGE
PIXELS
ADAPTED
FROM
K
GRAUMAN
AND
D
HOIEM
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
KEYPOINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
FLAT
REGION
NO
CHANGE
IN
ALL
DIRECTIONS
EDGE
NO
CHANGE
ALONG
THE
EDGE
DIRECTION
CORNER
SIGNIFICANT
CHANGE
IN
ALL
DIRECTIONS
EFROS
D
FROLOVA
D
SIMAKOV
HARRIS
DETECTOR
ALGORITHM
COMPUTE
IMAGE
GRADIENTS
IX
AND
IY
FOR
ALL
PIXELS
FOR
EACH
PIXEL
COMPUTE
BY
LOOPING
OVER
NEIGHBORS
X
Y
COMPUTE
K
EMPIRICAL
CONSTANT
K
FIND
POINTS
WITH
LARGE
CORNER
RESPONSE
FUNCTION
R
R
THRESHOLD
NON
MAX
SUPPRESSION
TAKE
THE
POINTS
OF
LOCALLY
MAXIMUM
R
AS
THE
DETECTED
FEATURE
POINTS
I
E
PIXELS
WHERE
R
IS
BIGGER
THAN
FOR
ALL
THE
OR
NEIGHBORS
FROLOVA
D
SIMAKOV
CORNER
RESPONSE
AT
EVERY
PIXEL
EFFECT
A
VERY
PRECISE
CORNER
DETECTOR
A
FUNCTION
IS
INVARIANT
UNDER
A
CERTAIN
FAMILY
OF
TRANSFORMATIONS
IF
ITS
VALUE
DOES
NOT
CHANGE
WHEN
A
TRANSFORMATION
FROM
THIS
FAMILY
IS
APPLIED
TO
ITS
ARGUMENT
A
FUNCTION
IS
COVARIANT
WHEN
IT
COMMUTES
WITH
THE
TRANSFORMATION
I
E
APPLYING
THE
TRANSFORMATION
TO
THE
ARGUMENT
OF
THE
FUNCTION
HAS
THE
SAME
EFFECT
AS
APPLYING
THE
TRANSFORMATION
TO
THE
OUTPUT
OF
THE
FUNCTION
FOR
EXAMPLE
THE
AREA
OF
A
SURFACE
IS
INVARIANT
UNDER
ROTATIONS
SINCE
ROTATING
A
SURFACE
DOES
NOT
MAKE
IT
ANY
SMALLER
OR
BIGGER
BUT
THE
ORIENTATION
OF
THE
MAJOR
AXIS
OF
INERTIA
OF
THE
SURFACE
IS
COVARIANT
UNDER
THE
SAME
FAMILY
OF
TRANSFORMATIONS
SINCE
ROTATING
A
SURFACE
WILL
AFFECT
THE
ORIENTATION
OF
ITS
MAJOR
AXIS
IN
EXACTLY
THE
SAME
WAY
LOCAL
INVARIANT
FEATURE
DETECTORS
A
SURVEY
BY
TINNE
TUYTELAARS
AND
KRYSTIAN
MIKOLAJCZYK
IN
FOUNDATIONS
AND
TRENDS
IN
COMPUTER
GRAPHICS
AND
VISION
VOL
NO
CHAPTER
I
A
I
B
ONLY
DERIVATIVES
ARE
USED
INVARIANCE
TO
INTENSITY
SHIFT
I
I
B
INTENSITY
SCALING
I
A
I
R
R
THRESHOLD
X
IMAGE
COORDINATE
X
IMAGE
COORDINATE
DERIVATIVES
AND
WINDOW
FUNCTION
ARE
SHIFT
INVARIANT
SECOND
MOMENT
ELLIPSE
ROTATES
BUT
ITS
SHAPE
I
E
EIGENVALUES
REMAINS
THE
SAME
INVARIANT
TO
IMAGE
SCALE
IMAGE
ZOOMED
IMAGE
CORNER
ALL
POINTS
WILL
BE
CLASSIFIED
AS
EDGES
LAZEBNIK
PROBLEM
HOW
DO
WE
CHOOSE
CORRESPONDING
CIRCLES
INDEPENDENTLY
IN
EACH
IMAGE
DO
OBJECTS
IN
THE
IMAGE
HAVE
A
CHARACTERISTIC
SCALE
THAT
WE
CAN
IDENTIFY
FROLOVA
D
SIMAKOV
SOLUTION
DESIGN
A
FUNCTION
ON
THE
REGION
WHICH
IS
SCALE
INVARIANT
HAS
THE
SAME
SHAPE
EVEN
IF
THE
IMAGE
IS
RESIZED
TAKE
A
LOCAL
MAXIMUM
OF
THIS
FUNCTION
F
F
SCALE
ADAPTED
FROM
A
TORRALBA
REGION
SIZE
REGION
SIZE
A
GOOD
FUNCTION
FOR
SCALE
DETECTION
HAS
ONE
STABLE
SHARP
PEAK
F
F
F
REGION
SIZE
REGION
SIZE
REGION
SIZE
FOR
USUAL
IMAGES
A
GOOD
FUNCTION
WOULD
BE
A
ONE
WHICH
RESPONDS
TO
CONTRAST
SHARP
LOCAL
INTENSITY
CHANGE
TORRALBA
F
IM
X
F
IM
X
HOW
TO
FIND
CORRESPONDING
PATCH
SIZES
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
F
IM
X
F
IM
X
LAPLACIAN
OF
GAUSSIAN
BLOB
DETECTOR
LAPLACIAN
OF
GAUSSIAN
CIRCULARLY
SYMMETRIC
OPERATOR
FOR
BLOB
DETECTION
IN
G
G
G
GRAUMAN
WE
CAN
APPROXIMATE
THE
LAPLACIAN
WITH
A
DIFFERENCE
OF
GAUSSIANS
MORE
EFFICIENT
TO
IMPLEMENT
LAPLACIAN
DOG
G
X
Y
K
G
X
Y
DIFFERENCE
OF
GAUSSIANS
ALLOWS
DETECTION
OF
INCREASINGLY
COARSE
DETAIL
DIFFERENCE
OF
GAUSSIAN
EFFICIENT
COMPUTATION
COMPUTATION
IN
GAUSSIAN
SCALE
PYRAMID
SAMPLING
WITH
STEP
ORIGINAL
IMAGE
K
GRAUMAN
B
LEIBE
FIND
LOCAL
MAXIMA
IN
POSITION
SCALE
SPACE
OF
DIFFERENCE
OF
GAUSSIAN
POSITION
SCALE
SPACE
ADAPTED
FROM
K
GRAUMAN
B
LEIBE
FIND
PLACES
WHERE
X
GREATER
THAN
ALL
OF
ITS
NEIGHBORS
IN
GREEN
LIST
OF
X
Y
RESULTS
DIFFERENCE
OF
GAUSSIAN
K
GRAUMAN
B
LEIBE
ADDITIONAL
REFERENCES
SURVEY
PAPER
ON
LOCAL
FEATURES
LOCAL
INVARIANT
FEATURE
DETECTORS
A
SURVEY
BY
TINNE
TUYTELAARS
AND
KRYSTIAN
MIKOLAJCZYK
IN
FOUNDATIONS
AND
TRENDS
IN
COMPUTER
GRAPHICS
AND
VISION
VOL
NO
MOSTLY
CHAPTERS
MAKING
HARRIS
DETECTION
SCALE
INVARIANT
AND
MORE
ON
SCALE
INVARIANCE
INDEXING
BASED
ON
SCALE
INVARIANT
INTEREST
POINTS
BY
KRYSTIAN
MIKOLAJCZYK
AND
CORDELIA
SCHMID
IN
ICCV
RAW
PATCHES
AS
LOCAL
DESCRIPTORS
THE
SIMPLEST
WAY
TO
DESCRIBE
THE
NEIGHBORHOOD
AROUND
AN
INTEREST
POINT
IS
TO
WRITE
DOWN
THE
LIST
OF
INTENSITIES
TO
FORM
A
FEATURE
VECTOR
BUT
THIS
IS
VERY
SENSITIVE
TO
EVEN
SMALL
SHIFTS
ROTATIONS
E
G
SCALE
TRANSLATION
ROTATION
TUYTELAARS
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
DESCRIPTOR
LOWE
ICCV
HISTOGRAM
OF
ORIENTED
GRADIENTS
CAPTURES
IMPORTANT
TEXTURE
INFORMATION
ROBUST
TO
SMALL
TRANSLATIONS
AFFINE
DEFORMATIONS
K
GRAUMAN
B
LEIBE
COMPUTING
GRADIENTS
L
THE
IMAGE
INTENSITY
TAN
Α
𝑜𝑝𝑝𝑜𝑠𝑖𝑡𝑒
𝑠𝑖𝑑𝑒
𝑎𝑑𝑗𝑎𝑐𝑒𝑛𝑡
𝑠𝑖𝑑𝑒
M
X
Y
SQRT
Θ
X
Y
ATAN
M
X
Y
SQRT
Θ
X
Y
ATAN
M
X
Y
SQRT
Θ
X
Y
ATAN
BASIC
IDEA
TAKE
SQUARE
WINDOW
AROUND
DETECTED
FEATURE
COMPUTE
GRADIENT
ORIENTATION
FOR
EACH
PIXEL
CREATE
HISTOGRAM
OVER
EDGE
ORIENTATIONS
WEIGHTED
BY
MAGNITUDE
THAT
YOUR
FEATURE
DESCRIPTOR
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
QUANTIZE
THE
GRADIENT
ORIENTATIONS
I
E
SNAP
EACH
GRADIENT
TO
ONE
OF
ANGLES
EACH
GRADIENT
CONTRIBUTES
NOT
JUST
BUT
MAGNITUDE
GRADIENT
TO
THE
HISTOGRAM
I
E
STRONGER
GRADIENTS
CONTRIBUTE
MORE
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
FOR
EACH
DETECTED
FEATURE
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
QUANTIZE
THE
GRADIENT
ORIENTATIONS
I
E
SNAP
EACH
GRADIENT
TO
ONE
OF
ANGLES
EACH
GRADIENT
CONTRIBUTES
NOT
JUST
BUT
MAGNITUDE
GRADIENT
TO
THE
HISTOGRAM
I
E
STRONGER
GRADIENTS
CONTRIBUTE
MORE
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
FOR
EACH
DETECTED
FEATURE
NORMALIZE
CLIP
THRESHOLD
NORMALIZE
TO
NORMALIZE
THE
DESCRIPTOR
AFTER
NORMALIZING
WE
HAVE
SUCH
THAT
MAKING
DESCRIPTOR
ROTATION
INVARIANT
ROTATE
PATCH
ACCORDING
TO
ITS
DOMINANT
GRADIENT
ORIENTATION
THIS
PUTS
THE
PATCHES
INTO
A
CANONICAL
ORIENTATION
GRAUMAN
IMAGE
FROM
MATTHEW
BROWN
SIFT
IS
ROBUST
CAN
HANDLE
CHANGES
IN
VIEWPOINT
UP
TO
ABOUT
DEGREE
OUT
OF
PLANE
ROTATION
CAN
HANDLE
SIGNIFICANT
CHANGES
IN
ILLUMINATION
SOMETIMES
EVEN
DAY
VS
NIGHT
BELOW
FAST
AND
EFFICIENT
CAN
RUN
IN
REAL
TIME
CAN
BE
MADE
TO
WORK
WITHOUT
FEATURE
DETECTION
RESULTING
IN
DENSE
SIFT
MORE
POINTS
MEANS
ROBUSTNESS
TO
OCCLUSION
ONE
COMMONLY
USED
IMPLEMENTATION
ADAPTED
FROM
SEITZ
IMAGES
FROM
SEITZ
OBJECT
RECOGNITION
INDEXING
AND
RETRIEVAL
ROBOT
NAVIGATION
RECONSTRUCTION
WIDE
BASELINE
STEREO
MOTION
TRACKING
IMAGE
ALIGNMENT
PANORAMAS
AND
MOSAICS
ADAPTED
FROM
K
GRAUMAN
AND
L
LAZEBNIK
KEYPOINT
DETECTION
REPEATABLE
AND
DISTINCTIVE
CORNERS
BLOBS
STABLE
REGIONS
LAPLACIAN
OF
GAUSSIAN
AUTOMATIC
SCALE
SELECTION
DESCRIPTORS
ROBUST
AND
SELECTIVE
HISTOGRAMS
FOR
ROBUSTNESS
TO
SMALL
SHIFTS
AND
TRANSLATIONS
SIFT
DESCRIPTOR
ADAPTED
FROM
D
HOIEM
K
GRAUMAN
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
POST
MORTEM
MATLAB
OF
YOU
REVIEWED
OF
IT
PLEASE
REVIEW
THE
ENTIRE
TUTORIAL
ASAP
HOW
LONG
DID
TAKE
ANSWER
ON
SOCRATIVE
WHAT
DID
YOU
LEARN
FROM
IT
WHAT
TOOK
THE
MOST
TIME
PLAN
FOR
TODAY
FEATURE
DETECTION
WRAP
UP
MATCHING
FEATURES
INDEXING
FEATURES
VISUAL
WORDS
APPLICATION
TO
IMAGE
RETRIEVAL
MATCHING
LOCAL
FEATURES
IMAGE
IMAGE
TO
GENERATE
CANDIDATE
MATCHES
FIND
PATCHES
THAT
HAVE
THE
MOST
SIMILAR
APPEARANCE
E
G
LOWEST
FEATURE
EUCLIDEAN
DISTANCE
SIMPLEST
APPROACH
COMPARE
THEM
ALL
TAKE
THE
CLOSEST
OR
CLOSEST
K
OR
WITHIN
A
THRESHOLDED
DISTANCE
ROBUST
MATCHING
IMAGE
IMAGE
AT
WHAT
EUCLIDEAN
DISTANCE
VALUE
DO
WE
HAVE
A
GOOD
MATCH
TO
ADD
ROBUSTNESS
TO
MATCHING
CAN
CONSIDER
RATIO
DISTANCE
TO
BEST
MATCH
DISTANCE
TO
SECOND
BEST
MATCH
IF
LOW
FIRST
MATCH
LOOKS
GOOD
IF
HIGH
COULD
BE
AMBIGUOUS
MATCH
MATCHING
SIFT
DESCRIPTORS
NEAREST
NEIGHBOR
EUCLIDEAN
DISTANCE
THRESHOLD
RATIO
OF
NEAREST
TO
NEAREST
DESCRIPTOR
LOWE
IJCV
EFFICIENT
MATCHING
SO
FAR
WE
DISCUSSED
MATCHING
ACROSS
JUST
TWO
IMAGES
WHAT
IF
YOU
WANTED
TO
MATCH
A
QUERY
FEATURE
FROM
ONE
IMAGE
TO
ALL
FRAMES
IN
A
VIDEO
OR
TO
A
GIANT
DATABASE
WITH
POTENTIALLY
THOUSANDS
OF
FEATURES
PER
IMAGE
AND
HUNDREDS
TO
MILLIONS
OF
IMAGES
TO
SEARCH
HOW
TO
EFFICIENTLY
FIND
THOSE
THAT
ARE
RELEVANT
TO
A
NEW
IMAGE
EACH
PATCH
REGION
HAS
A
DESCRIPTOR
WHICH
IS
A
POINT
IN
SOME
HIGH
DIMENSIONAL
FEATURE
SPACE
E
G
SIFT
WHEN
WE
SEE
CLOSE
POINTS
IN
FEATURE
SPACE
WE
HAVE
SIMILAR
DESCRIPTORS
WHICH
INDICATES
SIMILAR
LOCAL
CONTENT
GRAUMAN
DATABASE
IMAGES
INDEXING
LOCAL
FEATURES
INVERTED
FILE
INDEX
FOR
TEXT
DOCUMENTS
AN
EFFICIENT
WAY
TO
FIND
ALL
PAGES
ON
WHICH
A
WORD
OCCURS
IS
TO
USE
AN
INDEX
WE
WANT
TO
FIND
ALL
IMAGES
IN
WHICH
A
FEATURE
OCCURS
TO
USE
THIS
IDEA
WE
LL
NEED
TO
MAP
OUR
FEATURES
TO
VISUAL
WORDS
K
GRAUMAN
EXTRACT
SOME
LOCAL
FEATURES
FROM
A
NUMBER
OF
IMAGES
E
G
SIFT
DESCRIPTOR
SPACE
EACH
POINT
IS
DIMENSIONAL
EACH
POINT
IS
A
LOCAL
DESCRIPTOR
E
G
SIFT
FEATURE
VECTOR
QUANTIZE
THE
SPACE
BY
GROUPING
CLUSTERING
THE
FEATURES
NOTE
FOR
NOW
WE
LL
TREAT
CLUSTERING
AS
A
BLACK
BOX
VISUAL
WORDS
PATCHES
ON
THE
RIGHT
REGIONS
USED
TO
COMPUTE
SIFT
IF
I
GROUP
THESE
EACH
GROUP
OF
PATCHES
WILL
BELONG
TO
THE
SAME
VISUAL
WORD
FIGURE
FROM
SIVIC
ZISSERMAN
ICCV
VISUAL
WORDS
FOR
INDEXING
MAP
HIGH
DIMENSIONAL
DESCRIPTORS
TO
TOKENS
WORDS
BY
QUANTIZING
THE
FEATURE
SPACE
EACH
CLUSTER
HAS
A
CENTER
DETERMINE
WHICH
WORD
TO
ASSIGN
TO
EACH
NEW
IMAGE
REGION
BY
FINDING
THE
CLOSEST
CLUSTER
CENTER
DATABASE
IMAGES
ARE
LOADED
INTO
THE
INDEX
BY
MAPPING
WORDS
TO
IMAGE
NUMBERS
WHEN
WILL
THIS
INDEXING
PROCESS
GIVE
US
A
GAIN
IN
EFFICIENCY
FOR
A
NEW
QUERY
IMAGE
WE
CAN
FIGURE
OUT
WHICH
DATABASE
IMAGES
SHARE
A
WORD
WITH
IT
AND
RETRIEVE
THOSE
IMAGES
AS
MATCHES
WE
CAN
CALL
THIS
RETRIEVAL
PROCESS
INSTANCE
RECOGNITION
ADAPTED
FROM
K
GRAUMAN
OF
ALL
THE
SENSORY
IMPRESSIONS
PROCEEDING
TO
THE
BRAIN
THE
VISUAL
EXPERIENCES
ARE
THE
DOMINANT
ONES
OUR
PERCEPTION
OF
THE
WORLD
AROUND
US
IS
BASED
ESSENTIALLY
ON
THE
MESSAGES
THAT
REACH
THE
BRAIN
FROM
OUR
EYES
FOR
A
LONG
TIME
IT
WAS
THOUGHT
THAT
THE
RETINAL
IMAGE
WAS
TRASNSEMNITTSEOD
PROYI
NTBBRYAPIONIN
T
TO
VISUAL
CENTERS
IN
THVEISBRUAIAN
L
THPE
ECERRCEBERAPL
TCIOORTNEX
WAS
A
MOVIE
SCREEN
SO
TO
SPEAK
UPON
WHICH
THE
IMAGE
INRTEHTEIENYAE
LW
ACSEPRROEJEBCTREADL
TCHORORUTGEHXTH
E
DISCOVERIES
OEF
HYUEB
ELCAENDLLW
OIEPSETLIWCEANLOW
KNOW
THAT
BEHIND
THE
ORIGIN
OF
THE
VISUAL
PERCEPTION
IN
THE
BRAIN
THERE
IS
A
CONSIDERABLY
MORE
COMPLICATHEDUCBOUERSLE
OWF
EIVEESNTES
LBY
FOLLOWING
THE
VISUAL
IMPULSES
ALONG
THEIR
PATH
TO
THE
VARIOUS
CELL
LAYERS
OF
THE
OPTICAL
CORTEX
HUBEL
AND
WIESEL
HAVE
BEEN
ABLE
TO
DEMONSTRATE
THAT
THE
MESSAGE
ABOUT
THE
IMAGE
FALLING
ON
THE
RETINA
UNDERGOES
A
STEP
WISE
ANALYSIS
IN
A
SYSTEM
OF
NERVE
CELLS
STORED
IN
COLUMNS
IN
THIS
SYSTEM
EACH
CELL
HAS
ITS
SPECIFIC
FUNCTION
AND
IS
RESPONSIBLE
FOR
A
SPECIFIC
DETAIL
IN
THE
PATTERN
OF
THE
RETINAL
IMAGE
CHINA
IS
FORECASTING
A
TRADE
SURPLUS
OF
TO
THIS
YEAR
A
THREEFOLD
INCREASE
ON
THE
COMMERCE
MINISTRY
SAID
THE
SURPLUS
WOULD
BE
CREATED
BY
A
PREDICTED
JUMP
IN
EXPORTS
TO
COMPARED
WITH
A
RISE
IN
IMPORTS
TO
THE
FIGCURHESINARAE
LIKTERLAY
DTOEFU
RTHER
ANNOY
THESUUSR
PWHLUICHSH
ACSOLOMNGMARGEURECDETH
AT
CHINA
EXPORTS
ARE
UNFAIRLY
HELPED
BY
A
DELIBERATEELYXUPNODERRTVSAL
UEIMD
YPUAONR
TBSE
IJUINGS
AGREES
TYHEUSAUNRP
LUBS
IAS
NTOKO
HIDGHO
MBUTESSAYTSICTH
E
YUAN
IS
ONLY
ONE
FACTOR
BANK
OF
CHINA
GOVERNOR
ZHOU
XIAOCHUAN
SAID
THE
COUNTRY
ALSO
NEEDED
TO
DTORMAODREET
OVBAOOLUSTEDOMESTIC
DEMAND
SO
MORE
GOODS
STAYED
WITHIN
THE
COUNTRY
CHINA
INCREASED
THE
VALUE
OF
THE
YUAN
AGAINST
THE
DOLLAR
BY
IN
JULY
AND
PERMITTED
IT
TO
TRADE
WITHIN
A
NARROW
BAND
BUT
THE
US
WANTS
THE
YUAN
TO
BE
ALLOWED
TO
TRADE
FREELY
HOWEVER
BEIJING
HAS
MADE
IT
CLEAR
THAT
IT
WILL
TAKE
ITS
TIME
AND
TREAD
CAREFULLY
BEFORE
ALLOWING
THE
YUAN
TO
RISE
FURTHER
IN
VALUE
ICCV
SHORT
COURSE
L
FEI
FEI
SUMMARIZE
ENTIRE
IMAGE
BASED
ON
ITS
DISTRIBUTION
HISTOGRAM
OF
WORD
OCCURRENCES
ANALOGOUS
TO
BAG
OF
WORDS
REPRESENTATION
COMMONLY
USED
FOR
DOCUMENTS
FEATURE
PATCHES
VISUAL
WORDS
RANK
IMAGES
BY
NORMALIZED
SCALAR
PRODUCT
BETWEEN
THEIR
OCCURRENCE
COUNTS
NEAREST
NEIGHBOR
SEARCH
FOR
SIMILAR
IMAGES
𝑠𝑖𝑚
𝑉
𝑑𝑗
𝑖
𝑞
𝑖
D
Q
FOR
VOCABULARY
OF
V
WORDS
SIM
Q
DOT
Q
NORM
NORM
Q
FLEXIBLE
TO
GEOMETRY
DEFORMATIONS
VIEWPOINT
COMPACT
SUMMARY
OF
IMAGE
CONTENT
VERY
GOOD
RESULTS
IN
PRACTICE
BASIC
MODEL
IGNORES
GEOMETRY
MUST
VERIFY
AFTERWARDS
OR
ENCODE
VIA
FEATURES
BACKGROUND
AND
FOREGROUND
MIXED
WHEN
BAG
COVERS
WHOLE
IMAGE
OPTIMAL
VOCABULARY
FORMATION
REMAINS
UNCLEAR
ADAPTED
FROM
K
GRAUMAN
INVERTED
FILE
INDEX
AND
BAGS
OF
WORDS
SIMILARITY
OFFLINE
EXTRACT
FEATURES
IN
DATABASE
IMAGES
CLUSTER
THEM
TO
FIND
WORDS
MAKE
INDEX
EXTRACT
WORDS
IN
QUERY
EXTRACT
FEATURES
AND
MAP
EACH
TO
CLOSEST
CLUSTER
CENTER
USE
INVERTED
FILE
INDEX
TO
FIND
FRAMES
RELEVANT
TO
QUERY
FOR
EACH
RELEVANT
FRAME
RANK
THEM
BY
COMPARING
WORD
COUNTS
OF
QUERY
AND
FRAME
ADAPTED
FROM
K
GRAUMAN
ONE
MORE
TRICK
TF
IDF
WEIGHTING
TERM
FREQUENCY
INVERSE
DOCUMENT
FREQUENCY
DESCRIBE
IMAGE
FRAME
BY
FREQUENCY
OF
EACH
WORD
WITHIN
IT
BUT
DOWNWEIGHT
WORDS
THAT
APPEAR
OFTEN
IN
THE
DATABASE
STANDARD
WEIGHTING
FOR
TEXT
RETRIEVAL
NUMBER
OF
OCCURRENCES
OF
WORD
I
IN
DOCUMENT
D
NUMBER
OF
WORDS
IN
DOCUMENT
D
NORMALIZED
BAG
OF
WORDS
TOTAL
NUMBER
OF
DOCUMENTS
IN
DATABASE
NUMBER
OF
DOCUMENTS
IN
WHICH
WORD
I
OCCURS
ADAPTED
FROM
K
GRAUMAN
BAGS
OF
WORDS
FOR
CONTENT
BASED
IMAGE
RETRIEVAL
SLIDE
FROM
ANDREW
ZISSERMAN
EXAMPLE
RETRIEVED
SHOTS
R
I
ILVK
T
T
J
STRU
T
FNUN
E
KEY
FRAME
END
FRAME
ST
UT
FNUNE
F
KEY
FRAME
F
TI
C
T
R
I
R
SHUT
FRAME
FRA
ME
JI
K
EY
CRAINE
END
FRAME
O
I
T
R
I
I
I
I
J
F
SLIDE
FROM
ANDREW
ZISSERMAN
FRAME
L
R
I
FRAME
KE
FR
E
L
O
R
I
P
KEY
FRAME
ETL
L
F
ENCL
FNUNE
VIDEO
GOOGLE
SYSTEM
COLLECT
ALL
WORDS
WITHIN
QUERY
REGION
INVERTED
FILE
INDEX
TO
FIND
RELEVANT
FRAMES
SKIP
FOR
COMPARE
WORD
COUNTS
BOW
SPATIAL
VERIFICATION
SKIP
SIVIC
ZISSERMAN
ICCV
DEMO
ONLINE
AT
ESEARCH
VGOOGLE
INDEX
HTML
GRAUMAN
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
BOTH
IMAGE
PAIRS
HAVE
MANY
VISUAL
WORDS
IN
COMMON
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
ONLY
SOME
OF
THE
MATCHES
ARE
MUTUALLY
CONSISTENT
EXAMPLE
APPLICATIONS
MOBILE
TOURIST
GUIDE
OBJECT
BUILDING
RECOGNITION
SELF
LOCALIZATION
PHOTO
VIDEO
AUGMENTATION
B
LEIBE
QUACK
LEIBE
VAN
GOOL
CIVR
SCORING
RETRIEVAL
QUALITY
QUERY
DATABASE
SIZE
IMAGES
RELEVANT
TOTAL
IMAGES
E
G
IMAGES
OF
GOLDEN
GATE
RESULTS
ORDERED
PRECISION
RELEVANT
RETURNED
RECALL
RELEVANT
TOTAL
RELEVANT
RECALL
INDEXING
AND
RETRIEVAL
SUMMARY
BAG
OF
WORDS
REPRESENTATION
QUANTIZE
FEATURE
SPACE
TO
MAKE
DISCRETE
SET
OF
VISUAL
WORDS
SUMMARIZE
IMAGE
BY
DISTRIBUTION
OF
WORDS
INDEX
INDIVIDUAL
WORDS
INVERTED
INDEX
PRE
COMPUTE
INDEX
TO
ENABLE
FASTER
SEARCH
AT
QUERY
TIME
RECOGNITION
OF
INSTANCES
MATCH
LOCAL
FEATURES
OPTIONALLY
PERFORM
SPATIAL
VERIFICATION
CS
INTRO
TO
COMPUTER
VISION
AFFINE
AND
PROJECTIVE
TRANSFORMATIONS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
ALIGNMENT
PROBLEM
WE
PREVIOUSLY
DISCUSSED
HOW
TO
MATCH
FEATURES
ACROSS
IMAGES
OF
THE
SAME
OR
DIFFERENT
OBJECTS
NOW
LET
FOCUS
ON
THE
CASE
OF
TWO
IMAGES
OF
THE
SAME
OBJECT
E
G
XI
AND
XI
AND
EXAMINE
IT
IN
MORE
DETAIL
WHAT
TRANSFORMATION
RELATES
XI
AND
XI
IN
ALIGNMENT
WE
WILL
FIT
THE
PARAMETERS
OF
SOME
TRANSFORMATION
ACCORDING
TO
A
SET
OF
MATCHING
FEATURE
PAIRS
CORRESPONDENCES
XI
XI
TWO
QUESTIONS
ALIGNMENT
GIVEN
TWO
IMAGES
WHAT
IS
THE
TRANSFORMATION
BETWEEN
THEM
WARPING
GIVEN
A
SOURCE
IMAGE
AND
A
TRANSFORMATION
WHAT
DOES
THE
TRANSFORMED
OUTPUT
LOOK
LIKE
MOTIVATION
FOR
FEATURE
BASED
ALIGNMENT
IMAGE
MOSAICS
WHAT
ARE
THE
CORRESPONDENCES
COMPARE
CONTENT
IN
LOCAL
PATCHES
FIND
BEST
MATCHES
SIMPLEST
APPROACH
SCAN
XI
WITH
TEMPLATE
FORMED
FROM
A
POINT
IN
XI
AND
COMPUTE
EUCLIDEAN
DISTANCE
A
K
A
SSD
OR
NORMALIZED
CROSS
CORRELATION
BETWEEN
LIST
OF
PIXEL
INTENSITIES
IN
THE
PATCH
WHAT
ARE
THE
TRANSFORMATIONS
EXAMPLES
OF
TRANSFORMATIONS
TRANSLATION
ROTATION
ASPECT
AFFINE
PERSPECTIVE
PARAMETRIC
GLOBAL
WARPING
P
X
Y
P
X
Y
TRANSFORMATION
T
IS
A
COORDINATE
CHANGING
MACHINE
P
T
P
WHAT
DOES
IT
MEAN
THAT
T
IS
GLOBAL
IT
IS
THE
SAME
FOR
ANY
POINT
P
IT
CAN
BE
DESCRIBED
BY
JUST
A
FEW
NUMBERS
PARAMETERS
LET
REPRESENT
T
AS
A
MATRIX
P
MP
X
Y
X
Y
ALYOSHA
EFROS
SCALING
SCALING
A
COORDINATE
MEANS
MULTIPLYING
EACH
OF
ITS
COMPONENTS
BY
A
SCALAR
UNIFORM
SCALING
MEANS
THIS
SCALAR
IS
THE
SAME
FOR
ALL
COMPONENTS
SCALING
NON
UNIFORM
SCALING
DIFFERENT
SCALARS
PER
COMPONENT
X
Y
SCALING
SCALING
OPERATION
OR
IN
MATRIX
FORM
X
AX
Y
BY
X
MX
NY
AX
Y
PX
QY
BY
SCALING
MATRIX
LINEAR
TRANSFORMATIONS
X
Y
A
C
B
X
D
Y
ONLY
LINEAR
TRANSFORMATIONS
CAN
BE
REPRESENTED
WITH
A
MATRIX
LINEAR
TRANSFORMATIONS
ARE
COMBINATIONS
OF
SCALE
ROTATION
SHEAR
AND
MIRROR
WHAT
TRANSFORMS
CAN
WE
WRITE
WITH
A
MATRIX
SCALING
X
X
X
X
SX
X
Y
Y
Y
SY
Y
Y
ROTATE
AROUND
X
COS
X
SIN
Y
X
COS
SIN
X
Y
SIN
X
COS
Y
Y
SIN
COS
Y
SHEAR
X
X
SHX
Y
X
SHX
X
Y
SHY
X
Y
Y
SH
Y
Y
WHAT
TRANSFORMS
CAN
WE
WRITE
WITH
A
MATRIX
MIRROR
ABOUT
Y
AXIS
X
X
X
X
Y
Y
Y
Y
MIRROR
OVER
X
X
X
X
Y
Y
Y
Y
TRANSLATION
X
Y
X
T
X
Y
T
Y
CAN
T
DO
HOMOGENEOUS
COORDINATES
TO
CONVERT
TO
HOMOGENEOUS
COORDINATES
HOMOGENEOUS
IMAGE
COORDINATES
CONVERTING
FROM
HOMOGENEOUS
COORDINATES
TRANSLATION
HOMOGENEOUS
COORDINATES
XX
TX
XX
XX
TTXX
YY
Y
Y
T
TY
Y
Y
TYY
TX
TY
AFFINE
TRANSFORMATIONS
X
A
B
C
X
Y
D
E
F
Y
W
W
AFFINE
TRANSFORMATIONS
ARE
COMBINATIONS
OF
LINEAR
TRANSFORMATIONS
AND
TRANSLATIONS
MAPS
LINES
TO
LINES
PARALLEL
LINES
REMAIN
PARALLEL
FITTING
AN
AFFINE
TRANSFORMATION
ASSUMING
WE
KNOW
THE
CORRESPONDENCES
HOW
DO
WE
GET
THE
TRANSFORMATION
M
XI
XI
Y
M
M
Y
T
M
I
I
ALYOSHA
EFROS
FITTING
AN
AFFINE
TRANSFORMATION
M
XI
YI
XI
X
Y
M
Y
I
I
I
HOW
MANY
MATCHES
CORRESPONDENCE
PAIRS
DO
WE
NEED
TO
SOLVE
FOR
THE
TRANSFORMATION
PARAMETERS
ONCE
WE
HAVE
SOLVED
FOR
THE
PARAMETERS
HOW
DO
WE
COMPUTE
X
NEW
Y
NEW
GIVEN
XNEW
YNEW
WHERE
DO
THE
MATCHES
COME
FROM
PROJECTIVE
TRANSFORMATIONS
X
A
B
C
X
Y
W
D
E
G
H
F
Y
I
W
PROJECTIVE
TRANSFORMATIONS
AFFINE
TRANSFORMATIONS
AND
PROJECTIVE
WARPS
PARALLEL
LINES
DO
NOT
NECESSARILY
REMAIN
PARALLEL
PROJECTION
WORLD
COORD
IMAGE
COORD
P
X
Y
OPTICAL
CENTER
F
Y
Z
X
CAMERA
CENTER
Z
X
X
P
Y
X
Y
MODIFIED
FROM
DEREK
HOIEM
AND
KRISTEN
GRAUMAN
SCENE
POINT
IMAGE
COORDINATES
IMAGE
MOSAICS
GOALS
OBTAIN
A
WIDER
ANGLE
VIEW
BY
COMBINING
MULTIPLE
IMAGES
IMAGE
MOSAICS
CAMERA
SETUP
TWO
IMAGES
WITH
CAMERA
ROTATION
ZOOM
BUT
NO
TRANSLATION
MOSAIC
MANY
VIEWS
OF
SAME
OBJECT
STEVE
SEITZ
MOSAIC
PLANE
THE
MOSAIC
HAS
A
NATURAL
INTERPRETATION
IN
THE
IMAGES
ARE
REPROJECTED
ONTO
A
COMMON
PLANE
THE
MOSAIC
IS
FORMED
ON
THIS
PLANE
MOSAIC
IS
A
SYNTHETIC
WIDE
ANGLE
CAMERA
IMAGE
REPROJECTION
BASIC
QUESTION
HOW
TO
RELATE
TWO
IMAGES
FROM
THE
SAME
CAMERA
CENTER
HOW
TO
MAP
A
PIXEL
FROM
TO
ANSWER
CAST
A
RAY
THROUGH
EACH
PIXEL
IN
DRAW
THE
PIXEL
WHERE
THAT
RAY
INTERSECTS
OBSERVATION
RATHER
THAN
THINKING
OF
THIS
AS
A
REPROJECTION
THINK
OF
IT
AS
A
IMAGE
WARP
FROM
ONE
IMAGE
TO
ANOTHER
PROJECTIVE
TRANSFORMS
A
PROJECTIVE
TRANSFORM
IS
A
MAPPING
BETWEEN
ANY
TWO
PPS
WITH
THE
SAME
CENTER
OF
PROJECTION
RECTANGLE
SHOULD
MAP
TO
ARBITRARY
QUADRILATERAL
PARALLEL
LINES
AREN
T
BUT
PRESERVES
STRAIGHT
LINES
ALSO
CALLED
HOMOGRAPHY
WX
WY
X
Y
W
P
H
P
ADAPTED
FROM
ALYOSHA
EFROS
HOW
TO
STITCH
TOGETHER
A
PANORAMA
A
K
A
MOSAIC
BASIC
PROCEDURE
TAKE
A
SEQUENCE
OF
IMAGES
FROM
THE
SAME
POSITION
ROTATE
THE
CAMERA
ABOUT
ITS
OPTICAL
CENTER
COMPUTE
THE
HOMOGRAPHY
TRANSFORMATION
BETWEEN
SECOND
IMAGE
AND
FIRST
TRANSFORM
THE
SECOND
IMAGE
TO
OVERLAP
WITH
THE
FIRST
BLEND
THE
TWO
TOGETHER
TO
CREATE
A
MOSAIC
IF
THERE
ARE
MORE
IMAGES
REPEAT
COMPUTING
THE
HOMOGRAPHY
XN
YN
XN
YN
TO
COMPUTE
THE
HOMOGRAPHY
GIVEN
PAIRS
OF
CORRESPONDING
POINTS
IN
THE
IMAGES
WE
NEED
TO
SET
UP
AN
EQUATION
WHERE
THE
PARAMETERS
OF
H
ARE
THE
UNKNOWNS
KRISTEN
GRAUMAN
COMPUTING
THE
HOMOGRAPHY
P
HP
WX
A
B
C
X
WY
D
E
F
Y
W
G
H
I
CAN
SET
SCALE
FACTOR
I
SO
THERE
ARE
UNKNOWNS
SET
UP
A
SYSTEM
OF
LINEAR
EQUATIONS
AH
B
WHERE
VECTOR
OF
UNKNOWNS
H
A
B
C
D
E
F
G
H
T
NEED
AT
LEAST
EQS
BUT
THE
MORE
THE
BETTER
SOLVE
FOR
H
IF
OVERCONSTRAINED
SOLVE
USING
LEAST
SQUARES
MIN
AH
B
COMPUTING
THE
HOMOGRAPHY
ASSUME
WE
HAVE
FOUR
MATCHED
POINTS
HOW
DO
WE
COMPUTE
HOMOGRAPHY
H
W
X
H
P
HP
P
W
Y
H
H
H
H
W
H
A
H
H
X
Y
XX
YX
X
H
X
Y
XY
YY
Y
H
H
APPLY
SVD
UDVT
A
U
V
SVD
A
H
VSMALLEST
COLUMN
OF
V
CORR
TO
SMALLEST
SINGULAR
VALUE
DEREK
HOIEM
HOW
TO
STITCH
TOGETHER
A
PANORAMA
A
K
A
MOSAIC
BASIC
PROCEDURE
TAKE
A
SEQUENCE
OF
IMAGES
FROM
THE
SAME
POSITION
ROTATE
THE
CAMERA
ABOUT
ITS
OPTICAL
CENTER
COMPUTE
THE
HOMOGRAPHY
TRANSFORMATION
BETWEEN
SECOND
IMAGE
AND
FIRST
TRANSFORM
THE
SECOND
IMAGE
TO
OVERLAP
WITH
THE
FIRST
BLEND
THE
TWO
TOGETHER
TO
CREATE
A
MOSAIC
IF
THERE
ARE
MORE
IMAGES
REPEAT
TRANSFORMING
THE
SECOND
IMAGE
X
Y
IMAGE
IMAGE
CANVAS
WX
W
WY
W
X
Y
TO
APPLY
A
GIVEN
HOMOGRAPHY
H
COMPUTE
P
HP
REGULAR
MATRIX
MULTIPLY
WX
WY
X
Y
CONVERT
P
FROM
HOMOGENEOUS
TO
IMAGE
COORDINATES
W
MODIFIED
FROM
KRISTEN
GRAUMAN
P
H
P
TRANSFORMING
THE
SECOND
IMAGE
IMAGE
IMAGE
CANVAS
Y
FORWARD
WARPING
SEND
EACH
PIXEL
F
X
Y
TO
ITS
CORRESPONDING
LOCATION
X
Y
H
X
Y
IN
THE
RIGHT
IMAGE
TRANSFORMING
THE
SECOND
IMAGE
Y
X
F
X
Y
X
G
X
Y
FORWARD
WARPING
SEND
EACH
PIXEL
F
X
Y
TO
ITS
CORRESPONDING
LOCATION
X
Y
H
X
Y
IN
THE
RIGHT
IMAGE
Q
WHAT
IF
PIXEL
LANDS
BETWEEN
TWO
PIXELS
A
DISTRIBUTE
COLOR
AMONG
NEIGHBORING
PIXELS
X
Y
TRANSFORMING
THE
SECOND
IMAGE
IMAGE
IMAGE
CANVAS
Y
INVERSE
WARPING
GET
EACH
PIXEL
G
X
Y
FROM
ITS
CORRESPONDING
LOCATION
X
Y
H
X
Y
IN
THE
LEFT
IMAGE
TRANSFORMING
THE
SECOND
IMAGE
Y
X
F
X
Y
X
G
X
Y
INVERSE
WARPING
GET
EACH
PIXEL
G
X
Y
FROM
ITS
CORRESPONDING
LOCATION
X
Y
H
X
Y
IN
THE
LEFT
IMAGE
Q
WHAT
IF
PIXEL
COMES
FROM
BETWEEN
TWO
PIXELS
A
INTERPOLATE
COLOR
VALUE
FROM
NEIGHBORS
HELP
HOMOGRAPHY
EXAMPLE
IMAGE
RECTIFICATION
TO
UNWARP
RECTIFY
AN
IMAGE
SOLVE
FOR
HOMOGRAPHY
H
GIVEN
P
AND
P
P
HP
SUMMARY
WRITE
TRANSFORMATIONS
AS
MATRIX
VECTOR
MULTIPLICATION
INCLUDING
TRANSLATION
WHEN
WE
USE
HOMOGENEOUS
COORDINATES
PROJECTION
EQUATIONS
EXPRESS
HOW
WORLD
POINTS
MAPPED
TO
IMAGE
FITTING
TRANSFORMATIONS
SOLVE
FOR
UNKNOWN
PARAMETERS
GIVEN
CORRESPONDING
POINTS
FROM
TWO
VIEWS
LINEAR
AFFINE
PROJECTIVE
HOMOGRAPHY
MOSAICS
USES
HOMOGRAPHY
AND
IMAGE
WARPING
TO
MERGE
VIEWS
TAKEN
FROM
SAME
CENTER
OF
PROJECTION
PERFORM
IMAGE
WARPING
FORWARD
INVERSE
ADAPTED
FROM
KRISTEN
GRAUMAN
THE
NEXT
HIDDEN
SLIDES
GIVE
SOME
MORE
DETAIL
ABOUT
HOW
IMAGE
FORMATION
WORKS
I
E
HOW
OBJECTS
ARE
MAPPED
TO
IMAGES
PLEASE
REVIEW
ON
YOUR
OWN
TIME
IF
YOU
RE
INTERESTED
CS
INTRO
TO
COMPUTER
VISION
EPIPOLAR
GEOMETRY
AND
STEREO
VISION
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
ANNOUNCEMENT
PLEASE
SEND
ME
THREE
TOPICS
YOU
WANT
ME
TO
REVIEW
NEXT
CLASS
FOR
THE
MIDTERM
THE
MORE
REQUESTS
I
GET
FOR
EACH
TOPIC
THE
MORE
TIME
I
WILL
DEVOTE
TO
IT
IF
NO
ONE
ASKS
ME
TO
REVIEW
A
TOPIC
I
WON
T
REVIEW
IT
EXAM
FORMAT
MULTIPLE
CHOICE
AND
TRUE
FALSE
SHORT
ANSWERS
DEMONSTRATING
AN
ALGORITHM
ON
A
PARTICULAR
EXAMPLE
LAST
CLASS
VS
THIS
CLASS
LAST
CLASS
SAME
CAMERA
CENTER
BUT
CAMERA
ROTATES
THIS
CLASS
CAMERA
CENTER
IS
NOT
THE
SAME
WE
HAVE
MULTIPLE
CAMERAS
EPIPOLAR
GEOMETRY
RELATES
CAMERAS
FROM
TWO
POSITIONS
CAMERAS
STEREO
DEPTH
ESTIMATION
RECOVER
DEPTH
FROM
DISPARITIES
BETWEEN
TWO
IMAGES
ADAPTED
FROM
DEREK
HOIEM
WHY
MULTIPLE
VIEWS
STRUCTURE
AND
DEPTH
ARE
INHERENTLY
AMBIGUOUS
FROM
SINGLE
VIEWS
MULTIPLE
VIEWS
HELP
US
TO
PERCEIVE
SHAPE
AND
DEPTH
STEREO
PHOTOGRAPHY
AND
STEREO
VIEWERS
TAKE
TWO
PICTURES
OF
THE
SAME
SUBJECT
FROM
TWO
SLIGHTLY
DIFFERENT
VIEWPOINTS
AND
DISPLAY
SO
THAT
EACH
EYE
SEES
ONLY
ONE
OF
THE
IMAGES
INVENTED
BY
SIR
CHARLES
WHEATSTONE
IMAGE
FROM
FISHER
PRICE
COM
STEREO
PHOTOGRAPHY
AND
STEREO
VIEWERS
DEPTH
FROM
STEREO
TWO
CAMERAS
SIMULTANEOUS
VIEWS
SINGLE
MOVING
CAMERA
AND
STATIC
SCENE
DEPTH
FROM
STEREO
GOAL
RECOVER
DEPTH
BY
FINDING
IMAGE
COORDINATE
X
THAT
CORRESPONDS
TO
X
X
X
Z
C
BASELINE
C
B
DEPTH
FROM
STEREO
GOAL
RECOVER
DEPTH
BY
FINDING
IMAGE
COORDINATE
X
THAT
CORRESPONDS
TO
X
SUB
PROBLEMS
CALIBRATION
HOW
DO
WE
RECOVER
THE
RELATION
OF
THE
CAMERAS
IF
NOT
ALREADY
KNOWN
CORRESPONDENCE
HOW
DO
WE
SEARCH
FOR
THE
MATCHING
POINT
X
X
GEOMETRY
FOR
A
SIMPLE
STEREO
SYSTEM
ASSUME
PARALLEL
OPTICAL
AXES
KNOWN
CAMERA
PARAMETERS
I
E
CALIBRATED
CAMERAS
WHAT
IS
EXPRESSION
FOR
Z
SIMILAR
TRIANGLES
PL
P
PR
AND
OL
P
OR
T
XL
XR
T
Z
F
Z
DEPTH
IS
INVERSELY
PROPORTIONAL
TO
DISPARITY
DEPTH
DISPARITY
Z
F
T
XR
XL
DEPTH
FROM
DISPARITY
WE
HAVE
TWO
IMAGES
TAKEN
FROM
CAMERAS
WITH
DIFFERENT
INTRINSIC
AND
EXTRINSIC
PARAMETERS
HOW
DO
WE
MATCH
A
POINT
IN
THE
FIRST
IMAGE
TO
A
POINT
IN
THE
SECOND
IMAGE
I
X
Y
DISPARITY
MAP
D
X
Y
IMAGE
I
X
Y
SO
IF
WE
COULD
FIND
THE
CORRESPONDING
POINTS
IN
TWO
IMAGES
WE
COULD
ESTIMATE
RELATIVE
DEPTH
SUMMARY
INTRODUCTION
OF
TERMS
EPIPOLAR
GEOMETRY
EPIPOLES
ARE
INTERSECTION
OF
BASELINE
WITH
IMAGE
PLANES
MATCHING
POINT
IN
SECOND
IMAGE
IS
ON
A
LINE
PASSING
THROUGH
ITS
EPIPOLE
EPIPOLAR
CONSTRAINT
LIMITS
WHERE
POINTS
FROM
ONE
VIEW
WILL
BE
IMAGED
IN
THE
OTHER
WHICH
MAKES
SEARCH
FOR
CORRESPONDENCES
QUICKER
ESSENTIAL
E
AND
FUNDAMENTAL
F
MATRICES
MAP
FROM
A
POINT
IN
ONE
IMAGE
TO
A
LINE
ITS
EPIPOLAR
LINE
IN
THE
OTHER
CAN
SOLVE
FOR
E
F
GIVEN
CORRESPONDING
POINTS
E
G
INTEREST
POINTS
STEREO
DEPTH
ESTIMATION
FIND
CORRESPONDING
POINTS
ALONG
EPIPOLAR
SCANLINE
ESTIMATE
DISPARITY
DEPTH
IS
INVERSE
TO
DISPARITY
ADAPTED
FROM
KRISTEN
GRAUMAN
AND
DEREK
HOIEM
STEREO
CORRESPONDENCE
CONSTRAINTS
GIVEN
P
IN
LEFT
IMAGE
WHERE
CAN
CORRESPONDING
POINT
P
BE
STEREO
CORRESPONDENCE
CONSTRAINTS
EPIPOLAR
CONSTRAINT
GEOMETRY
OF
TWO
VIEWS
CONSTRAINS
WHERE
THE
CORRESPONDING
PIXEL
FOR
SOME
IMAGE
POINT
IN
THE
FIRST
VIEW
MUST
OCCUR
IN
THE
SECOND
VIEW
IT
MUST
BE
ON
THE
LINE
CARVED
OUT
BY
A
PLANE
CONNECTING
THE
WORLD
POINT
AND
OPTICAL
CENTERS
POTENTIAL
MATCHES
FOR
P
HAVE
TO
LIE
ON
THE
CORRESPONDING
LINE
L
POTENTIAL
MATCHES
FOR
P
HAVE
TO
LIE
ON
THE
CORRESPONDING
LINE
L
EPIPOLAR
GEOMETRY
NOTATION
DEREK
HOIEM
BASELINE
LINE
CONNECTING
THE
TWO
CAMERA
CENTERS
EPIPOLES
INTERSECTIONS
OF
BASELINE
WITH
IMAGE
PLANES
PROJECTIONS
OF
THE
OTHER
CAMERA
CENTER
EPIPOLAR
PLANE
PLANE
CONTAINING
BASELINE
EPIPOLAR
LINES
INTERSECTIONS
OF
EPIPOLAR
PLANE
WITH
IMAGE
PLANES
ALWAYS
COME
IN
CORRESPONDING
PAIRS
NOTE
ALL
EPIPOLAR
LINES
INTERSECT
AT
THE
EPIPOLE
EPIPOLAR
CONSTRAINT
THE
EPIPOLAR
CONSTRAINT
IS
USEFUL
BECAUSE
IT
REDUCES
THE
CORRESPONDENCE
PROBLEM
TO
A
SEARCH
ALONG
AN
EPIPOLAR
LINE
KRISTEN
GRAUMAN
IMAGE
FROM
ANDREW
ZISSERMAN
SUMMARY
INTRODUCTION
OF
TERMS
EPIPOLAR
GEOMETRY
EPIPOLES
ARE
INTERSECTION
OF
BASELINE
WITH
IMAGE
PLANES
MATCHING
POINT
IN
SECOND
IMAGE
IS
ON
A
LINE
PASSING
THROUGH
ITS
EPIPOLE
EPIPOLAR
CONSTRAINT
LIMITS
WHERE
POINTS
FROM
ONE
VIEW
WILL
BE
IMAGED
IN
THE
OTHER
WHICH
MAKES
SEARCH
FOR
CORRESPONDENCES
QUICKER
ESSENTIAL
E
AND
FUNDAMENTAL
F
MATRICES
MAP
FROM
A
POINT
IN
ONE
IMAGE
TO
A
LINE
ITS
EPIPOLAR
LINE
IN
THE
OTHER
CAN
SOLVE
FOR
E
F
GIVEN
CORRESPONDING
POINTS
E
G
INTEREST
POINTS
STEREO
DEPTH
ESTIMATION
FIND
CORRESPONDING
POINTS
ALONG
EPIPOLAR
SCANLINE
ESTIMATE
DISPARITY
DEPTH
IS
INVERSE
TO
DISPARITY
ADAPTED
FROM
KRISTEN
GRAUMAN
AND
DEREK
HOIEM
STEREO
GEOMETRY
WITH
CALIBRATED
CAMERAS
IF
THE
STEREO
RIG
IS
CALIBRATED
WE
KNOW
HOW
TO
ROTATE
AND
TRANSLATE
CAMERA
REFERENCE
FRAME
TO
GET
TO
CAMERA
REFERENCE
FRAME
ROTATION
MATRIX
R
TRANSLATION
VECTOR
T
STEREO
GEOMETRY
WITH
CALIBRATED
CAMERAS
IF
THE
STEREO
RIG
IS
CALIBRATED
WE
KNOW
HOW
TO
ROTATE
AND
TRANSLATE
CAMERA
REFERENCE
FRAME
TO
GET
TO
CAMERA
REFERENCE
FRAME
X
C
RXC
T
AN
ASIDE
CROSS
PRODUCT
VECTOR
CROSS
PRODUCT
TAKES
TWO
VECTORS
AND
RETURNS
A
THIRD
VECTOR
THAT
PERPENDICULAR
TO
BOTH
INPUTS
SO
HERE
C
IS
PERPENDICULAR
TO
BOTH
A
AND
B
WHICH
MEANS
THE
DOT
PRODUCT
FROM
GEOMETRY
TO
ALGEBRA
T
X
NORMAL
TO
THE
PLANE
T
RX
CROSS
PRODUCT
OF
VECTOR
WITH
ITSELF
IS
ASIDE
MATRIX
FORM
OF
CROSS
PRODUCT
A
A
B
A
A
B
C
CAN
BE
EXPRESSED
AS
A
MATRIX
MULTIPLICATION
A
A
A
X
ESSENTIAL
MATRIX
X
TX
RX
LET
E
T
X
R
X
EX
X
T
EX
E
IS
CALLED
THE
ESSENTIAL
MATRIX
AND
IT
RELATES
CORRESPONDING
IMAGE
POINTS
BETWEEN
BOTH
CAMERAS
GIVEN
THE
ROTATION
AND
TRANSLATION
BEFORE
WE
SAID
IF
WE
OBSERVE
A
POINT
IN
ONE
IMAGE
ITS
POSITION
IN
OTHER
IMAGE
IS
CONSTRAINED
TO
LIE
ON
LINE
DEFINED
BY
ABOVE
TURNS
OUT
EX
IS
THE
EPIPOLAR
LINE
THROUGH
X
IN
THE
FIRST
IMAGE
CORRESP
TO
X
NOTE
THESE
POINTS
ARE
IN
CAMERA
COORDINATE
SYSTEMS
ESSENTIAL
MATRIX
EXAMPLE
PARALLEL
CAMERAS
R
P
X
Y
F
T
E
T
X
R
P
X
Y
F
P
EP
FOR
THE
PARALLEL
CAMERAS
IMAGE
OF
ANY
POINT
MUST
LIE
ON
SAME
HORIZONTAL
LINE
IN
EACH
IMAGE
PLANE
IMAGE
I
X
Y
DISPARITY
MAP
D
X
Y
X
Y
X
D
X
Y
Y
IMAGE
I
X
Y
WE
CAN
ALSO
FIND
CORRESPONDENCES
AND
DEPTH
IF
CAMERAS
OPTICAL
AXES
ARE
NOT
PARALLEL
NOT
DISCUSSED
HERE
WHAT
IF
WE
DON
T
KNOW
CAMERA
PARAMETERS
R
T
WANT
TO
ESTIMATE
WORLD
GEOMETRY
WITHOUT
REQUIRING
CALIBRATED
CAMERAS
ARCHIVAL
VIDEOS
PHOTOS
FROM
MULTIPLE
UNRELATED
USERS
WEAK
CALIBRATION
ESTIMATE
EPIPOLAR
GEOMETRY
FROM
A
REDUNDANT
SET
OF
POINT
CORRESPONDENCES
BETWEEN
TWO
UNCALIBRATED
CAMERAS
COMPUTING
F
FROM
CORRESPONDENCES
EACH
POINT
CORRESPONDENCE
GENERATES
ONE
CONSTRAINT
ON
F
COLLECT
N
OF
THESE
CONSTRAINTS
IM
RIGHT
IM
LEFT
SOLVE
FOR
F
VECTOR
OF
PARAMETERS
FUNDAMENTAL
MATRIX
RELATES
PIXEL
COORDINATES
IN
THE
TWO
VIEWS
MORE
GENERAL
FORM
THAN
ESSENTIAL
MATRIX
WE
REMOVE
NEED
TO
KNOW
INTRINSIC
PARAMETERS
SUMMARY
INTRODUCTION
OF
TERMS
EPIPOLAR
GEOMETRY
EPIPOLES
ARE
INTERSECTION
OF
BASELINE
WITH
IMAGE
PLANES
MATCHING
POINT
IN
SECOND
IMAGE
IS
ON
A
LINE
PASSING
THROUGH
ITS
EPIPOLE
EPIPOLAR
CONSTRAINT
LIMITS
WHERE
POINTS
FROM
ONE
VIEW
WILL
BE
IMAGED
IN
THE
OTHER
WHICH
MAKES
SEARCH
FOR
CORRESPONDENCES
QUICKER
ESSENTIAL
E
AND
FUNDAMENTAL
F
MATRICES
MAP
FROM
A
POINT
IN
ONE
IMAGE
TO
A
LINE
ITS
EPIPOLAR
LINE
IN
THE
OTHER
CAN
SOLVE
FOR
E
F
GIVEN
CORRESPONDING
POINTS
E
G
INTEREST
POINTS
STEREO
DEPTH
ESTIMATION
USING
EPIPOLAR
GEOMETRY
FOR
STEREO
FUSE
A
CALIBRATED
BINOCULAR
STEREO
PAIR
TO
PRODUCE
A
DEPTH
IMAGE
IMAGE
IMAGE
DENSE
DEPTH
MAP
BASIC
STEREO
MATCHING
ALGORITHM
FOR
EACH
PIXEL
IN
THE
FIRST
IMAGE
FIND
CORRESPONDING
EPIPOLAR
SCANLINE
IN
THE
RIGHT
IMAGE
LEFT
RIGHT
SCANLINE
MATCHING
COST
DISPARITY
SLIDE
A
WINDOW
ALONG
THE
RIGHT
SCANLINE
AND
COMPARE
CONTENTS
OF
THAT
WINDOW
WITH
THE
REFERENCE
WINDOW
IN
THE
LEFT
IMAGE
MATCHING
COST
E
G
EUCLIDEAN
DISTANCE
ASSUME
PARALLEL
OPTICAL
AXES
KNOWN
CAMERA
PARAMETERS
I
E
CALIBRATED
CAMERAS
WHAT
IS
EXPRESSION
FOR
Z
SIMILAR
TRIANGLES
PL
P
PR
AND
OL
P
OR
T
XL
XR
T
Z
F
Z
DEPTH
DISPARITY
Z
F
T
XR
XL
RESULTS
WITH
WINDOW
SEARCH
DATA
WINDOW
BASED
MATCHING
GROUND
TRUTH
HOW
CAN
WE
IMPROVE
UNIQUENESS
FOR
ANY
POINT
IN
ONE
IMAGE
THERE
SHOULD
BE
AT
MOST
ONE
MATCHING
POINT
IN
THE
OTHER
IMAGE
ORDERING
CORRESPONDING
POINTS
SHOULD
BE
IN
THE
SAME
ORDER
IN
BOTH
VIEWS
SMOOTHNESS
WE
EXPECT
DISPARITY
VALUES
TO
CHANGE
SLOWLY
FOR
THE
MOST
PART
MANY
OF
THESE
CONSTRAINTS
CAN
BE
ENCODED
IN
AN
ENERGY
FUNCTION
AND
SOLVED
USING
GRAPH
CUTS
BEFORE
GRAPH
CUTS
GROUND
TRUTH
FOR
THE
LATEST
AND
GREATEST
PROJECTIVE
STRUCTURE
FROM
MOTION
GIVEN
M
IMAGES
OF
N
FIXED
POINTS
XIJ
PI
XJ
I
M
J
N
PROBLEM
ESTIMATE
M
PROJECTION
MATRICES
PI
AND
N
POINTS
XJ
FROM
THE
MN
CORRESPONDING
POINTS
XIJ
XJ
PHOTO
SYNTH
FROM
MULTIPLE
IMAGES
BUILDING
ROME
IN
A
DAY
AGARWAL
ET
AL
RECAP
EPIPOLES
POINT
X
IN
LEFT
IMAGE
CORRESPONDS
TO
EPIPOLAR
LINE
L
IN
RIGHT
IMAGE
EPIPOLAR
LINE
PASSES
THROUGH
THE
EPIPOLE
THE
INTERSECTION
OF
THE
CAMERAS
BASELINE
WITH
THE
IMAGE
PLANE
RECAP
FUNDAMENTAL
MATRIX
FUNDAMENTAL
MATRIX
MAPS
FROM
A
POINT
IN
ONE
IMAGE
TO
A
LINE
IN
THE
OTHER
IF
X
AND
X
CORRESPOND
TO
THE
SAME
POINT
X
ESSENTIAL
MATRIX
IS
LIKE
FUNDAMENTAL
MATRIX
BUT
MORE
CONSTRAINED
RECAP
STEREO
WITH
CALIBRATED
CAMERAS
GIVEN
IMAGE
PAIR
R
T
DETECT
SOME
FEATURES
COMPUTE
ESSENTIAL
MATRIX
E
MATCH
FEATURES
USING
THE
EPIPOLAR
AND
OTHER
CONSTRAINTS
TRIANGULATE
FOR
STRUCTURE
AND
GET
DEPTH
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
REMINDERS
THE
MIDTERM
EXAM
IS
IN
CLASS
ON
THIS
COMING
WEDNESDAY
THERE
WILL
BE
NO
MAKE
UP
EXAMS
UNLESS
YOU
OR
A
CLOSE
RELATIVE
IS
SERIOUSLY
ILL
REVIEW
REQUESTS
I
RECEIVED
TEXTURES
AND
TEXTURE
REPRESENTATIONS
IMAGE
RESPONSES
TO
SIZE
AND
ORIENTATION
OF
GAUSSIAN
FILTER
BANKS
COMPARISONS
CORNER
DETECTION
ALG
HARRIS
INVARIANCE
VS
COVARIANCE
AFFINE
INTENSITY
CHANGE
AND
APPLICATIONS
TO
KNOW
SCALE
INVARIANT
DETECTION
BLOB
DETECTION
HARRIS
AUTOMATIC
SCALE
SELECTION
SIFT
AND
FEATURE
DESCRIPTION
KEYPOINT
MATCHING
ALG
FEATURE
MATCHING
EXAMPLES
OF
HOW
TO
COMPUTE
AND
APPLY
HOMOGRAPHY
EPIPOLAR
GEOMETRY
WHY
IT
MAKES
SENSE
TO
USE
THE
RATIO
DISTANCE
TO
BEST
MATCH
DISTANCE
TO
SECOND
BEST
MATCH
WHEN
MATCHING
FEATURES
ACROSS
IMAGES
SUMMARY
OF
EQUATIONS
STUDENTS
NEED
TO
KNOW
PYRAMIDS
CONVOLUTION
PRACTICAL
USE
FILTERS
FOR
TRANSFORMING
THE
IMAGE
TRANSFORMATIONS
HOMOGRAPHIES
EPIPOLAR
GEOMETRY
X
Y
A
C
B
X
D
Y
ONLY
LINEAR
TRANSFORMATIONS
CAN
BE
REPRESENTED
WITH
A
MATRIX
LINEAR
TRANSFORMATIONS
ARE
COMBINATIONS
OF
SCALE
ROTATION
SHEAR
AND
MIRROR
X
A
B
C
X
Y
D
E
F
Y
W
W
AFFINE
TRANSFORMATIONS
ARE
COMBINATIONS
OF
LINEAR
TRANSFORMATIONS
AND
TRANSLATIONS
MAPS
LINES
TO
LINES
PARALLEL
LINES
REMAIN
PARALLEL
X
A
B
C
X
Y
W
D
E
G
H
F
Y
I
W
PROJECTIVE
TRANSFORMATIONS
AFFINE
TRANSFORMATIONS
AND
PROJECTIVE
WARPS
PARALLEL
LINES
DO
NOT
NECESSARILY
REMAIN
PARALLEL
HOW
TO
STITCH
TOGETHER
A
PANORAMA
A
K
A
MOSAIC
BASIC
PROCEDURE
TAKE
A
SEQUENCE
OF
IMAGES
FROM
THE
SAME
POSITION
ROTATE
THE
CAMERA
ABOUT
ITS
OPTICAL
CENTER
COMPUTE
THE
HOMOGRAPHY
TRANSFORMATION
BETWEEN
SECOND
IMAGE
AND
FIRST
TRANSFORM
THE
SECOND
IMAGE
TO
OVERLAP
WITH
THE
FIRST
BLEND
THE
TWO
TOGETHER
TO
CREATE
A
MOSAIC
IF
THERE
ARE
MORE
IMAGES
REPEAT
XN
YN
XN
YN
TO
COMPUTE
THE
HOMOGRAPHY
GIVEN
PAIRS
OF
CORRESPONDING
POINTS
IN
THE
IMAGES
WE
NEED
TO
SET
UP
AN
EQUATION
WHERE
THE
PARAMETERS
OF
H
ARE
THE
UNKNOWNS
P
HP
WX
A
B
C
X
WY
D
E
F
Y
W
G
H
I
CAN
SET
SCALE
FACTOR
I
SO
THERE
ARE
UNKNOWNS
SET
UP
A
SYSTEM
OF
LINEAR
EQUATIONS
AH
B
WHERE
VECTOR
OF
UNKNOWNS
H
A
B
C
D
E
F
G
H
T
NEED
AT
LEAST
EQS
BUT
THE
MORE
THE
BETTER
SOLVE
FOR
H
IF
OVERCONSTRAINED
SOLVE
USING
LEAST
SQUARES
MIN
AH
B
ASSUME
WE
HAVE
FOUR
MATCHED
POINTS
HOW
DO
WE
COMPUTE
HOMOGRAPHY
H
W
X
H
P
HP
P
W
Y
H
H
H
H
W
H
A
H
H
X
Y
XX
YX
X
H
X
Y
XY
YY
Y
H
H
APPLY
SVD
UDVT
A
U
V
SVD
A
H
VSMALLEST
COLUMN
OF
V
CORR
TO
SMALLEST
SINGULAR
VALUE
DEREK
HOIEM
IMAGE
IMAGE
CANVAS
TEST
POINT
X
Y
WX
W
WY
W
X
Y
TO
APPLY
A
GIVEN
HOMOGRAPHY
H
COMPUTE
P
HP
REGULAR
MATRIX
MULTIPLY
WX
WY
X
Y
CONVERT
P
FROM
HOMOGENEOUS
TO
IMAGE
COORDINATES
W
MODIFIED
FROM
KRISTEN
GRAUMAN
P
H
P
IMAGE
IMAGE
CANVAS
Y
FORWARD
WARPING
SEND
EACH
PIXEL
F
X
Y
TO
ITS
CORRESPONDING
LOCATION
X
Y
H
X
Y
IN
THE
RIGHT
IMAGE
DEPTH
FROM
DISPARITY
WE
HAVE
TWO
IMAGES
TAKEN
FROM
CAMERAS
WITH
DIFFERENT
INTRINSIC
AND
EXTRINSIC
PARAMETERS
HOW
DO
WE
MATCH
A
POINT
IN
THE
FIRST
IMAGE
TO
A
POINT
IN
THE
SECOND
IMAGE
I
X
Y
DISPARITY
MAP
D
X
Y
IMAGE
I
X
Y
SO
IF
WE
COULD
FIND
THE
CORRESPONDING
POINTS
IN
TWO
IMAGES
WE
COULD
ESTIMATE
RELATIVE
DEPTH
EPIPOLAR
GEOMETRY
NOTATION
DEREK
HOIEM
BASELINE
LINE
CONNECTING
THE
TWO
CAMERA
CENTERS
EPIPOLES
INTERSECTIONS
OF
BASELINE
WITH
IMAGE
PLANES
PROJECTIONS
OF
THE
OTHER
CAMERA
CENTER
EPIPOLAR
PLANE
PLANE
CONTAINING
BASELINE
EPIPOLAR
LINES
INTERSECTIONS
OF
EPIPOLAR
PLANE
WITH
IMAGE
PLANES
ALWAYS
COME
IN
CORRESPONDING
PAIRS
NOTE
ALL
EPIPOLAR
LINES
INTERSECT
AT
THE
EPIPOLE
EPIPOLAR
CONSTRAINT
THE
EPIPOLAR
CONSTRAINT
IS
USEFUL
BECAUSE
IT
REDUCES
THE
CORRESPONDENCE
PROBLEM
TO
A
SEARCH
ALONG
AN
EPIPOLAR
LINE
ESSENTIAL
MATRIX
X
TX
RX
LET
E
T
X
R
X
EX
X
T
EX
E
IS
CALLED
THE
ESSENTIAL
MATRIX
AND
IT
RELATES
CORRESPONDING
IMAGE
POINTS
BETWEEN
BOTH
CAMERAS
GIVEN
THE
ROTATION
AND
TRANSLATION
BEFORE
WE
SAID
IF
WE
OBSERVE
A
POINT
IN
ONE
IMAGE
ITS
POSITION
IN
OTHER
IMAGE
IS
CONSTRAINED
TO
LIE
ON
LINE
DEFINED
BY
ABOVE
TURNS
OUT
EX
IS
THE
EPIPOLAR
LINE
THROUGH
X
IN
THE
FIRST
IMAGE
CORRESP
TO
X
NOTE
THESE
POINTS
ARE
IN
CAMERA
COORDINATE
SYSTEMS
BASIC
STEREO
MATCHING
ALGORITHM
FOR
EACH
PIXEL
IN
THE
FIRST
IMAGE
FIND
CORRESPONDING
EPIPOLAR
SCANLINE
IN
THE
RIGHT
IMAGE
LEFT
RIGHT
SCANLINE
MATCHING
COST
DISPARITY
SLIDE
A
WINDOW
ALONG
THE
RIGHT
SCANLINE
AND
COMPARE
CONTENTS
OF
THAT
WINDOW
WITH
THE
REFERENCE
WINDOW
IN
THE
LEFT
IMAGE
MATCHING
COST
E
G
EUCLIDEAN
DISTANCE
ASSUME
PARALLEL
OPTICAL
AXES
KNOWN
CAMERA
PARAMETERS
I
E
CALIBRATED
CAMERAS
WHAT
IS
EXPRESSION
FOR
Z
SIMILAR
TRIANGLES
PL
P
PR
AND
OL
P
OR
T
XL
XR
T
Z
F
Z
DEPTH
DISPARITY
Z
F
T
XR
XL
RESULTS
WITH
WINDOW
SEARCH
DATA
WINDOW
BASED
MATCHING
GROUND
TRUTH
HOW
CAN
WE
IMPROVE
UNIQUENESS
FOR
ANY
POINT
IN
ONE
IMAGE
THERE
SHOULD
BE
AT
MOST
ONE
MATCHING
POINT
IN
THE
OTHER
IMAGE
ORDERING
CORRESPONDING
POINTS
SHOULD
BE
IN
THE
SAME
ORDER
IN
BOTH
VIEWS
SMOOTHNESS
WE
EXPECT
DISPARITY
VALUES
TO
CHANGE
SLOWLY
FOR
THE
MOST
PART
MANY
OF
THESE
CONSTRAINTS
CAN
BE
ENCODED
IN
AN
ENERGY
FUNCTION
AND
SOLVED
USING
GRAPH
CUTS
BEFORE
GRAPH
CUTS
GROUND
TRUTH
FOR
THE
LATEST
AND
GREATEST
PROJECTIVE
STRUCTURE
FROM
MOTION
GIVEN
M
IMAGES
OF
N
FIXED
POINTS
XIJ
PI
XJ
I
M
J
N
PROBLEM
ESTIMATE
M
PROJECTION
MATRICES
PI
AND
N
POINTS
XJ
FROM
THE
MN
CORRESPONDING
POINTS
XIJ
XJ
PHOTO
SYNTH
BUILDING
ROME
IN
A
DAY
AGARWAL
ET
AL
POINT
X
IN
LEFT
IMAGE
CORRESPONDS
TO
EPIPOLAR
LINE
L
IN
RIGHT
IMAGE
EPIPOLAR
LINE
PASSES
THROUGH
THE
EPIPOLE
THE
INTERSECTION
OF
THE
CAMERAS
BASELINE
WITH
THE
IMAGE
PLANE
FUNDAMENTAL
MATRIX
MAPS
FROM
A
POINT
IN
ONE
IMAGE
TO
A
LINE
IN
THE
OTHER
IF
X
AND
X
CORRESPOND
TO
THE
SAME
POINT
X
ESSENTIAL
MATRIX
IS
LIKE
FUNDAMENTAL
MATRIX
BUT
MORE
CONSTRAINED
GIVEN
IMAGE
PAIR
R
T
DETECT
SOME
FEATURES
COMPUTE
ESSENTIAL
MATRIX
E
MATCH
FEATURES
USING
THE
EPIPOLAR
AND
OTHER
CONSTRAINTS
TRIANGULATE
FOR
STRUCTURE
AND
GET
DEPTH
TEXTURE
REPRESENTATIONS
CORRELATION
FILTERING
SAY
THE
AVERAGING
WINDOW
SIZE
IS
X
ATTRIBUTE
UNIFORM
WEIGHT
TO
EACH
PIXEL
LOOP
OVER
ALL
PIXELS
IN
NEIGHBORHOOD
AROUND
IMAGE
PIXEL
F
I
J
NOW
GENERALIZE
TO
ALLOW
DIFFERENT
WEIGHTS
DEPENDING
ON
NEIGHBORING
PIXEL
RELATIVE
POSITION
NON
UNIFORM
WEIGHTS
CONVOLUTION
VS
CORRELATION
F
CONVOLUTION
U
V
H
I
J
SLIDE
CREDIT
DEREK
HOIEM
ORIGINAL
IMAGE
KRISTEN
GRAUMAN
DERIVATIVE
FILTER
RESPONSES
SQUARED
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
FILTER
BANKS
SCALES
WHAT
FILTERS
TO
PUT
IN
THE
BANK
TYPICALLY
WE
WANT
A
COMBINATION
OF
SCALES
AND
ORIENTATIONS
DIFFERENT
TYPES
OF
PATTERNS
MATLAB
CODE
AVAILABLE
FOR
THESE
EXAMPLES
KRISTEN
GRAUMAN
MATCHING
WITH
FILTERS
GOAL
FIND
IN
IMAGE
METHOD
FILTER
THE
IMAGE
WITH
EYE
PATCH
G
M
N
H
K
L
K
L
F
M
K
N
L
F
IMAGE
G
FILTER
INPUT
FILTERED
IMAGE
WHAT
WENT
WRONG
MATCHING
WITH
FILTERS
GOAL
FIND
IN
IMAGE
LIKES
BRIGHT
PIXELS
WHERE
FILTERS
ARE
ABOVE
AVERAGE
DARK
PIXELS
WHERE
FILTERS
ARE
BELOW
AVERAGE
METHOD
FILTER
THE
IMAGE
WITH
ZERO
MEAN
EYE
G
M
N
H
K
L
MEAN
H
K
L
F
M
K
N
L
INPUT
FILTERED
IMAGE
SCALED
THRESHOLDED
IMAGE
REPRESENTING
TEXTURE
BY
MEAN
ABS
RESPONSE
FILTERS
MEAN
ABS
RESPONSES
DEREK
HOIEM
COMPUTING
DISTANCES
USING
TEXTURE
D
A
B
DIMENSION
KRISTEN
GRAUMAN
FEATURE
DETECTION
HARRIS
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
KEYPOINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
FLAT
REGION
NO
CHANGE
IN
ALL
DIRECTIONS
EDGE
NO
CHANGE
ALONG
THE
EDGE
DIRECTION
CORNER
SIGNIFICANT
CHANGE
IN
ALL
DIRECTIONS
EFROS
D
FROLOVA
D
SIMAKOV
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
WINDOW
FUNCTION
W
X
Y
OR
IN
WINDOW
OUTSIDE
GAUSSIAN
EXPANDING
I
X
Y
IN
A
TAYLOR
SERIES
EXPANSION
WE
HAVE
FOR
SMALL
SHIFTS
U
V
A
QUADRATIC
APPROXIMATION
TO
THE
ERROR
SURFACE
BETWEEN
A
PATCH
AND
ITSELF
SHIFTED
BY
U
V
WHERE
M
IS
A
MATRIX
COMPUTED
FROM
IMAGE
DERIVATIVES
M
W
X
Y
IX
IX
IX
I
Y
X
Y
I
Y
I
Y
NOTATION
I
I
X
X
I
I
Y
Y
I
X
I
Y
I
I
X
Y
SINCE
M
IS
SYMMETRIC
WE
HAVE
M
X
X
T
MXI
I
XI
THE
EIGENVALUES
OF
M
REVEAL
THE
AMOUNT
OF
INTENSITY
CHANGE
IN
THE
TWO
PRINCIPAL
ORTHOGONAL
GRADIENT
DIRECTIONS
IN
THE
WINDOW
EDGE
CORNER
AND
ARE
LARGE
FLAT
REGION
AND
ARE
SMALL
COMPUTE
IMAGE
GRADIENTS
IX
AND
IY
FOR
ALL
PIXELS
FOR
EACH
PIXEL
COMPUTE
BY
LOOPING
OVER
NEIGHBORS
X
Y
COMPUTE
K
EMPIRICAL
CONSTANT
K
FIND
POINTS
WITH
LARGE
CORNER
RESPONSE
FUNCTION
R
R
THRESHOLD
TAKE
THE
POINTS
OF
LOCALLY
MAXIMUM
R
AS
THE
DETECTED
FEATURE
POINTS
I
E
PIXELS
WHERE
R
IS
BIGGER
THAN
FOR
ALL
THE
OR
NEIGHBORS
D
FROLOVA
D
SIMAKOV
GRAUMAN
FEATURE
DETECTION
SCALE
INVARIANCE
A
FUNCTION
IS
INVARIANT
UNDER
A
CERTAIN
FAMILY
OF
TRANSFORMATIONS
IF
ITS
VALUE
DOES
NOT
CHANGE
WHEN
A
TRANSFORMATION
FROM
THIS
FAMILY
IS
APPLIED
TO
ITS
ARGUMENT
A
FUNCTION
IS
COVARIANT
WHEN
IT
COMMUTES
WITH
THE
TRANSFORMATION
I
E
APPLYING
THE
TRANSFORMATION
TO
THE
ARGUMENT
OF
THE
FUNCTION
HAS
THE
SAME
EFFECT
AS
APPLYING
THE
TRANSFORMATION
TO
THE
OUTPUT
OF
THE
FUNCTION
FOR
EXAMPLE
THE
AREA
OF
A
SURFACE
IS
INVARIANT
UNDER
ROTATIONS
SINCE
ROTATING
A
SURFACE
DOES
NOT
MAKE
IT
ANY
SMALLER
OR
BIGGER
BUT
THE
ORIENTATION
OF
THE
MAJOR
AXIS
OF
INERTIA
OF
THE
SURFACE
IS
COVARIANT
UNDER
THE
SAME
FAMILY
OF
TRANSFORMATIONS
SINCE
ROTATING
A
SURFACE
WILL
AFFECT
THE
ORIENTATION
OF
ITS
MAJOR
AXIS
IN
EXACTLY
THE
SAME
WAY
LOCAL
INVARIANT
FEATURE
DETECTORS
A
SURVEY
BY
TINNE
TUYTELAARS
AND
KRYSTIAN
MIKOLAJCZYK
IN
FOUNDATIONS
AND
TRENDS
IN
COMPUTER
GRAPHICS
AND
VISION
VOL
NO
CHAPTER
I
A
I
B
ONLY
DERIVATIVES
ARE
USED
INVARIANCE
TO
INTENSITY
SHIFT
I
I
B
INTENSITY
SCALING
I
A
I
R
R
THRESHOLD
X
IMAGE
COORDINATE
X
IMAGE
COORDINATE
DERIVATIVES
AND
WINDOW
FUNCTION
ARE
SHIFT
INVARIANT
SECOND
MOMENT
ELLIPSE
ROTATES
BUT
ITS
SHAPE
I
E
EIGENVALUES
REMAINS
THE
SAME
CORNER
ALL
POINTS
WILL
BE
CLASSIFIED
AS
EDGES
PROBLEM
HOW
DO
WE
CHOOSE
CORRESPONDING
CIRCLES
INDEPENDENTLY
IN
EACH
IMAGE
DO
OBJECTS
IN
THE
IMAGE
HAVE
A
CHARACTERISTIC
SCALE
THAT
WE
CAN
IDENTIFY
FROLOVA
D
SIMAKOV
SOLUTION
DESIGN
A
FUNCTION
ON
THE
REGION
WHICH
IS
SCALE
INVARIANT
HAS
THE
SAME
SHAPE
EVEN
IF
THE
IMAGE
IS
RESIZED
TAKE
A
LOCAL
MAXIMUM
OF
THIS
FUNCTION
F
F
SCALE
ADAPTED
FROM
A
TORRALBA
REGION
SIZE
REGION
SIZE
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
F
IM
X
F
IM
X
LAPLACIAN
OF
GAUSSIAN
BLOB
DETECTOR
WE
CAN
APPROXIMATE
THE
LAPLACIAN
WITH
A
DIFFERENCE
OF
GAUSSIANS
MORE
EFFICIENT
TO
IMPLEMENT
LAPLACIAN
DOG
G
X
Y
K
G
X
Y
DIFFERENCE
OF
GAUSSIANS
DIFFERENCE
OF
GAUSSIAN
EFFICIENT
COMPUTATION
COMPUTATION
IN
GAUSSIAN
SCALE
PYRAMID
SAMPLING
WITH
STEP
ORIGINAL
IMAGE
FIND
LOCAL
MAXIMA
IN
POSITION
SCALE
SPACE
OF
DIFFERENCE
OF
GAUSSIAN
POSITION
SCALE
SPACE
ADAPTED
FROM
K
GRAUMAN
B
LEIBE
FIND
PLACES
WHERE
X
GREATER
THAN
ALL
OF
ITS
NEIGHBORS
IN
GREEN
LIST
OF
X
Y
LAPLACIAN
PYRAMID
EXAMPLE
ALLOWS
DETECTION
OF
INCREASINGLY
COARSE
DETAIL
RESULTS
DIFFERENCE
OF
GAUSSIAN
K
GRAUMAN
B
LEIBE
FEATURE
DESCRIPTION
GRADIENTS
M
X
Y
SQRT
Θ
X
Y
ATAN
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
QUANTIZE
THE
GRADIENT
ORIENTATIONS
I
E
SNAP
EACH
GRADIENT
TO
ONE
OF
ANGLES
EACH
GRADIENT
CONTRIBUTES
NOT
JUST
BUT
MAGNITUDE
GRADIENT
TO
THE
HISTOGRAM
I
E
STRONGER
GRADIENTS
CONTRIBUTE
MORE
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
FOR
EACH
DETECTED
FEATURE
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
QUANTIZE
THE
GRADIENT
ORIENTATIONS
I
E
SNAP
EACH
GRADIENT
TO
ONE
OF
ANGLES
EACH
GRADIENT
CONTRIBUTES
NOT
JUST
BUT
MAGNITUDE
GRADIENT
TO
THE
HISTOGRAM
I
E
STRONGER
GRADIENTS
CONTRIBUTE
MORE
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
FOR
EACH
DETECTED
FEATURE
NORMALIZE
CLIP
THRESHOLD
NORMALIZE
TO
NORMALIZE
THE
DESCRIPTOR
AFTER
NORMALIZING
WE
HAVE
SUCH
THAT
MAKING
DESCRIPTOR
ROTATION
INVARIANT
ROTATE
PATCH
ACCORDING
TO
ITS
DOMINANT
GRADIENT
ORIENTATION
THIS
PUTS
THE
PATCHES
INTO
A
CANONICAL
ORIENTATION
GRAUMAN
IMAGE
FROM
MATTHEW
BROWN
KEYPOINT
MATCHING
MATCHING
LOCAL
FEATURES
IMAGE
IMAGE
TO
GENERATE
CANDIDATE
MATCHES
FIND
PATCHES
THAT
HAVE
THE
MOST
SIMILAR
APPEARANCE
E
G
LOWEST
FEATURE
EUCLIDEAN
DISTANCE
SIMPLEST
APPROACH
COMPARE
THEM
ALL
TAKE
THE
CLOSEST
OR
CLOSEST
K
OR
WITHIN
A
THRESHOLDED
DISTANCE
ROBUST
MATCHING
IMAGE
IMAGE
AT
WHAT
EUCLIDEAN
DISTANCE
VALUE
DO
WE
HAVE
A
GOOD
MATCH
TO
ADD
ROBUSTNESS
TO
MATCHING
CAN
CONSIDER
RATIO
DISTANCE
TO
BEST
MATCH
DISTANCE
TO
SECOND
BEST
MATCH
IF
LOW
FIRST
MATCH
LOOKS
GOOD
IF
HIGH
COULD
BE
AMBIGUOUS
MATCH
RATIO
EXAMPLE
LET
Q
BE
THE
QUERY
FROM
THE
FIRST
IMAGE
BE
THE
CLOSEST
MATCH
IN
THE
SECOND
IMAGE
AND
BE
THE
SECOND
CLOSEST
MATCH
LET
DIST
Q
AND
DIST
Q
BE
THE
DISTANCES
LET
R
DIST
Q
DIST
Q
WHAT
IS
THE
LARGEST
THAT
R
CAN
BE
WHAT
IS
THE
LOWEST
THAT
R
CAN
BE
IF
R
IS
WHAT
DO
WE
KNOW
ABOUT
THE
TWO
DISTANCES
WHAT
ABOUT
WHEN
R
IS
INDEXING
LOCAL
FEATURES
SETUP
WHEN
WE
SEE
CLOSE
POINTS
IN
FEATURE
SPACE
WE
HAVE
SIMILAR
DESCRIPTORS
WHICH
INDICATES
SIMILAR
LOCAL
CONTENT
GRAUMAN
DATABASE
IMAGES
IMAGE
MATCHING
DESCRIBING
IMAGES
W
VISUAL
WORDS
SUMMARIZE
ENTIRE
IMAGE
BASED
ON
ITS
DISTRIBUTION
HISTOGRAM
OF
WORD
OCCURRENCES
ANALOGOUS
TO
BAG
OF
WORDS
REPRESENTATION
COMMONLY
USED
FOR
DOCUMENTS
FEATURE
PATCHES
GRAUMAN
VISUAL
WORDS
BAG
OF
VISUAL
WORDS
TWO
USES
REPRESENT
THE
IMAGE
USING
THAT
REPRESENTATION
LOOK
FOR
SIMILAR
IMAGES
CAN
ALSO
USE
BOW
TO
COMPUTE
AN
INVERTED
INDEX
TO
SIMPLIFY
APPLICATION
EXTRACT
SOME
LOCAL
FEATURES
FROM
A
NUMBER
OF
IMAGES
E
G
SIFT
DESCRIPTOR
SPACE
EACH
POINT
IS
DIMENSIONAL
QUANTIZE
THE
SPACE
BY
GROUPING
CLUSTERING
THE
FEATURES
NOTE
FOR
NOW
WE
LL
TREAT
CLUSTERING
AS
A
BLACK
BOX
INVERTED
FILE
INDEX
AND
BAGS
OF
WORDS
SIMILARITY
OFFLINE
EXTRACT
FEATURES
IN
DATABASE
IMAGES
CLUSTER
THEM
TO
FIND
WORDS
MAKE
INDEX
EXTRACT
WORDS
IN
QUERY
EXTRACT
FEATURES
AND
MAP
EACH
TO
CLOSEST
CLUSTER
CENTER
USE
INVERTED
FILE
INDEX
TO
FIND
FRAMES
RELEVANT
TO
QUERY
FOR
EACH
RELEVANT
FRAME
RANK
THEM
BY
COMPARING
WORD
COUNTS
BOW
OF
QUERY
AND
FRAME
ADAPTED
FROM
K
GRAUMAN
SCORING
RETRIEVAL
QUALITY
QUERY
DATABASE
SIZE
IMAGES
RELEVANT
TOTAL
IMAGES
E
G
IMAGES
OF
GOLDEN
GATE
RESULTS
ORDERED
PRECISION
RETURNED
RELEVANT
RETURNED
RECALL
RETURNED
RELEVANT
TOTAL
RELEVANT
RECALL
ONDREJ
CHUM
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
MIDTERM
EXAM
STATISTICS
EDGES
MORE
LOW
LEVEL
DON
T
NEED
TO
BE
CLOSED
SEGMENTS
IDEALLY
ONE
SEGMENT
FOR
EACH
SEMANTIC
GROUP
OBJECT
SHOULD
INCLUDE
CLOSED
CONTOURS
EDGE
DETECTION
GOAL
MAP
IMAGE
FROM
ARRAY
OF
PIXELS
TO
A
SET
OF
CURVES
OR
LINE
SEGMENTS
OR
CONTOURS
WHY
FIGURE
FROM
J
SHOTTON
ET
AL
PAMI
MAIN
IDEA
LOOK
FOR
STRONG
GRADIENTS
POST
PROCESS
WHAT
CAUSES
AN
EDGE
REFLECTANCE
CHANGE
APPEARANCE
INFORMATION
TEXTURE
DEPTH
DISCONTINUITY
OBJECT
BOUNDARY
CHANGE
IN
SURFACE
ORIENTATION
SHAPE
CAST
SHADOWS
CRITERIA
FOR
A
GOOD
EDGE
DETECTOR
GOOD
DETECTION
FIND
ALL
REAL
EDGES
IGNORING
NOISE
OR
OTHER
ARTIFACTS
GOOD
LOCALIZATION
DETECT
EDGES
AS
CLOSE
AS
POSSIBLE
TO
THE
TRUE
EDGES
RETURN
ONE
POINT
ONLY
FOR
EACH
TRUE
EDGE
POINT
CUES
OF
EDGE
DETECTION
DIFFERENCES
IN
COLOR
INTENSITY
OR
TEXTURE
ACROSS
THE
BOUNDARY
CONTINUITY
AND
CLOSURE
HIGH
LEVEL
KNOWLEDGE
AN
EDGE
IS
A
PLACE
OF
RAPID
CHANGE
IN
THE
IMAGE
INTENSITY
FUNCTION
IMAGE
INTENSITY
FUNCTION
ALONG
HORIZONTAL
SCANLINE
FIRST
DERIVATIVE
EDGES
CORRESPOND
TO
EXTREMA
OF
DERIVATIVE
INTENSITY
PROFILE
GRADIENT
CONSIDER
A
SINGLE
ROW
OR
COLUMN
OF
THE
IMAGE
PLOTTING
INTENSITY
AS
A
FUNCTION
OF
POSITION
GIVES
A
SIGNAL
WHERE
IS
THE
EDGE
DIFFERENCE
FILTERS
RESPOND
STRONGLY
TO
NOISE
IMAGE
NOISE
RESULTS
IN
PIXELS
THAT
LOOK
VERY
DIFFERENT
FROM
THEIR
NEIGHBORS
GENERALLY
THE
LARGER
THE
NOISE
THE
STRONGER
THE
RESPONSE
WHAT
CAN
WE
DO
ABOUT
IT
SOLUTION
SMOOTH
FIRST
F
G
F
G
D
F
DX
G
TO
FIND
EDGES
LOOK
FOR
PEAKS
IN
D
F
DX
G
DERIVATIVE
THEOREM
OF
CONVOLUTION
DIFFERENTIATION
IS
CONVOLUTION
AND
CONVOLUTION
IS
ASSOCIATIVE
D
F
DX
G
F
D
G
DX
THIS
SAVES
US
ONE
OPERATION
F
D
G
DX
F
D
G
DX
IMAGE
WITH
EDGE
DERIVATIVE
OF
GAUSSIAN
EDGE
MAX
OF
DERIVATIVE
GRADIENTS
EDGES
PRIMARY
EDGE
DETECTION
STEPS
SMOOTHING
SUPPRESS
NOISE
EDGE
ENHANCEMENT
FILTER
FOR
CONTRAST
EDGE
LOCALIZATION
DETERMINE
WHICH
LOCAL
MAXIMA
FROM
FILTER
OUTPUT
ARE
ACTUALLY
EDGES
VS
NOISE
THRESHOLD
THIN
THRESHOLDING
CHOOSE
A
THRESHOLD
VALUE
T
SET
ANY
PIXELS
LESS
THAN
T
TO
OFF
SET
ANY
PIXELS
GREATER
THAN
OR
EQUAL
TO
T
TO
ON
ORIGINAL
IMAGE
SOURCE
K
GRAUMAN
GRADIENT
MAGNITUDE
IMAGE
SOURCE
K
GRAUMAN
SOURCE
K
GRAUMAN
SOURCE
K
GRAUMAN
CANNY
EDGE
DETECTOR
FILTER
IMAGE
WITH
DERIVATIVE
OF
GAUSSIAN
FIND
MAGNITUDE
AND
ORIENTATION
OF
GRADIENT
NON
MAXIMUM
SUPPRESSION
THIN
WIDE
RIDGES
DOWN
TO
SINGLE
PIXEL
WIDTH
LINKING
AND
THRESHOLDING
HYSTERESIS
DEFINE
TWO
THRESHOLDS
LOW
AND
HIGH
USE
THE
HIGH
THRESHOLD
TO
START
EDGE
CURVES
AND
THE
LOW
THRESHOLD
TO
CONTINUE
THEM
MATLAB
EDGE
IMAGE
CANNY
HELP
EDGE
SOURCE
D
LOWE
L
FEI
FEI
INPUT
IMAGE
LENA
DERIVATIVE
OF
GAUSSIAN
Y
DERIVATIVE
OF
GAUSSIAN
GRADIENT
MAGNITUDE
NORM
OF
THE
GRADIENT
MAGNITUDE
THRESHOLDING
HOW
TO
TURN
THESE
THICK
REGIONS
OF
THE
GRADIENT
INTO
CURVES
CHECK
IF
PIXEL
IS
LOCAL
MAXIMUM
ALONG
GRADIENT
DIRECTION
SELECT
SINGLE
MAX
ACROSS
WIDTH
OF
THE
EDGE
REQUIRES
CHECKING
INTERPOLATED
PIXELS
P
AND
R
BILINEAR
INTERPOLATION
THE
CANNY
EDGE
DETECTOR
PROBLEM
PIXELS
ALONG
THIS
EDGE
DIDN
T
SURVIVE
THE
THRESHOLDING
THINNING
NON
MAXIMUM
SUPPRESSION
USE
A
HIGH
THRESHOLD
TO
START
EDGE
CURVES
AND
A
LOW
THRESHOLD
TO
CONTINUE
THEM
ORIGINAL
IMAGE
HIGH
THRESHOLD
STRONG
EDGES
LOW
THRESHOLD
WEAK
EDGES
HYSTERESIS
THRESHOLD
HIGH
THRESHOLD
STRONG
EDGES
LOW
THRESHOLD
WEAK
EDGES
HYSTERESIS
THRESHOLD
EFFECT
OF
GAUSSIAN
KERNEL
SPREAD
SIZE
ORIGINAL
CANNY
WITH
CANNY
WITH
THE
CHOICE
OF
DEPENDS
ON
DESIRED
BEHAVIOR
LARGE
DETECTS
LARGE
SCALE
EDGES
SMALL
DETECTS
FINE
FEATURES
SOURCE
SEITZ
LOW
LEVEL
EDGES
VS
PERCEIVED
CONTOURS
IMAGE
HUMAN
SEGMENTATION
GRADIENT
MAGNITUDE
BERKELEY
SEGMENTATION
DATABASE
SOURCE
L
LAZEBNIK
HOW
CAN
WE
DO
BETTER
SO
FAR
WE
HAVE
ONLY
CONSIDERED
CHANGE
IN
INTENSITY
AS
A
CUE
FOR
THE
EXISTENCE
OF
AN
EDGE
BRIGHTNESS
COLOR
TEXTURE
COMBINED
HUMAN
FOR
MORE
IMAGE
HUMAN
SEGMENTATION
GROUP
TOGETHER
SIMILAR
LOOKING
PIXELS
FOR
EFFICIENCY
OF
FURTHER
PROCESSING
SUPERPIXELS
X
REN
AND
J
MALIK
ICCV
OVERSEGMENTATION
UNDERSEGMENTATION
MULTIPLE
SEGMENTATIONS
BOTTOM
UP
GROUP
TOKENS
WITH
SIMILAR
FEATURES
TOP
DOWN
GROUP
TOKENS
THAT
LIKELY
BELONG
TO
THE
SAME
OBJECT
SOURCE
D
HOIEM
LEVIN
AND
WEISS
FIGURE
BY
J
SHI
DETERMINE
IMAGE
REGIONS
JPG
GROUP
VIDEO
FRAMES
INTO
SHOTS
FIGURE
BY
WANG
SUTER
FIGURE
GROUND
FIGURE
BY
GRAUMAN
DARRELL
OBJECT
LEVEL
GROUPING
GOALS
GATHER
FEATURES
THAT
BELONG
TOGETHER
OBTAIN
AN
INTERMEDIATE
REPRESENTATION
THAT
COMPACTLY
DESCRIBES
KEY
IMAGE
VIDEO
PARTS
HARD
TO
MEASURE
SUCCESS
WHAT
IS
INTERESTING
DEPENDS
ON
THE
APPLICATION
GESTALT
PSYCHOLOGY
WHOLE
IS
GREATER
THAN
SUM
OF
ITS
PARTS
RELATIONSHIPS
AMONG
PARTS
CAN
YIELD
NEW
PROPERTIES
FEATURES
PSYCHOLOGISTS
IDENTIFIED
SERIES
OF
FACTORS
THAT
PREDISPOSE
SET
OF
ELEMENTS
TO
BE
GROUPED
BY
HUMAN
VISUAL
SYSTEM
GOOD
INTUITION
AND
BASIC
PRINCIPLES
FOR
GROUPING
SOME
E
G
SYMMETRY
ARE
DIFFICULT
TO
IMPLEMENT
IN
PRACTICE
SOURCE
K
GRAUMAN
THE
MULLER
LYER
ILLUSION
CONTINUITY
EXPLANATION
BY
OCCLUSION
PRINCIPLES
OF
PERCEPTUAL
ORGANIZATION
SOURCE
D
HOIEM
FROM
STEVE
LEHAR
THE
CONSTRUCTIVE
ASPECT
OF
VISUAL
PERCEPTION
SIMILARITY
COMMON
FATE
IMAGE
CREDIT
ARTHUS
BERTRAND
VIA
F
DURAND
SOURCE
K
GRAUMAN
PROXIMITY
SEGMENTATION
AND
GROUPING
INSPIRATION
FROM
HUMAN
PERCEPTION
GESTALT
PROPERTIES
BOTTOM
UP
SEGMENTATION
VIA
CLUSTERING
FEATURES
COLOR
TEXTURE
ALGORITHMS
MODE
FINDING
AND
MEAN
SHIFT
K
MEANS
MEAN
SHIFT
GRAPH
BASED
NORMALIZED
CUTS
IMAGE
SEGMENTATION
TOY
EXAMPLE
BLACK
PIXELS
GRAY
PIXELS
WHITE
PIXELS
INPUT
IMAGE
INTENSITY
THESE
INTENSITIES
DEFINE
THE
THREE
GROUPS
WE
COULD
LABEL
EVERY
PIXEL
IN
THE
IMAGE
ACCORDING
TO
WHICH
OF
THESE
PRIMARY
INTENSITIES
IT
IS
I
E
SEGMENT
THE
IMAGE
BASED
ON
THE
INTENSITY
FEATURE
WHAT
IF
THE
IMAGE
ISN
T
QUITE
SO
SIMPLE
INPUT
IMAGE
INPUT
IMAGE
INTENSITY
INTENSITY
INPUT
IMAGE
INTENSITY
NOW
HOW
TO
DETERMINE
THE
THREE
MAIN
INTENSITIES
THAT
DEFINE
OUR
GROUPS
WE
NEED
TO
CLUSTER
INTENSITY
GOAL
CHOOSE
THREE
CENTERS
AS
THE
REPRESENTATIVE
INTENSITIES
AND
LABEL
EVERY
PIXEL
ACCORDING
TO
WHICH
OF
THESE
CENTERS
IT
IS
NEAREST
TO
BEST
CLUSTER
CENTERS
ARE
THOSE
THAT
MINIMIZE
SSD
BETWEEN
ALL
POINTS
AND
THEIR
NEAREST
CLUSTER
CENTER
CI
CLUSTERING
WITH
THIS
OBJECTIVE
IT
IS
A
CHICKEN
AND
EGG
PROBLEM
IF
WE
KNEW
THE
CLUSTER
CENTERS
WE
COULD
ALLOCATE
POINTS
TO
GROUPS
BY
ASSIGNING
EACH
TO
ITS
CLOSEST
CENTER
IF
WE
KNEW
THE
GROUP
MEMBERSHIPS
WE
COULD
GET
THE
CENTERS
BY
COMPUTING
THE
MEAN
PER
GROUP
K
MEANS
CLUSTERING
BASIC
IDEA
RANDOMLY
INITIALIZE
THE
K
CLUSTER
CENTERS
AND
ITERATE
BETWEEN
THE
TWO
STEPS
WE
JUST
SAW
RANDOMLY
INITIALIZE
THE
CLUSTER
CENTERS
CK
GIVEN
CLUSTER
CENTERS
DETERMINE
POINTS
IN
EACH
CLUSTER
FOR
EACH
POINT
P
FIND
THE
CLOSEST
CI
PUT
P
INTO
CLUSTER
I
GIVEN
POINTS
IN
EACH
CLUSTER
SOLVE
FOR
CI
SET
CI
TO
BE
THE
MEAN
OF
POINTS
IN
CLUSTER
I
IF
CI
HAVE
CHANGED
REPEAT
STEP
PROPERTIES
WILL
ALWAYS
CONVERGE
TO
SOME
SOLUTION
CAN
BE
A
LOCAL
MINIMUM
DOES
NOT
ALWAYS
FIND
THE
GLOBAL
MINIMUM
OF
OBJECTIVE
FUNCTION
SOURCE
STEVE
SEITZ
K
MEANS
ASK
USER
HOW
MANY
CLLUSTERS
THEY
D
NKE
E
G
K
K
MEANS
L
ASK
USER
HOW
CLUSTERS
THEY
D
LLIIKE
E
G
K
GUESS
K
CLLUSTER
CENTER
LOCATIONS
K
MEANS
ASK
USER
HOW
MANY
CLUSTERS
THEY
D
LIKE
E
G
K
GUESS
K
CLUSTER
CE
NT
E
R
LOCAT
IONS
EACH
DLATAPOINT
FINDS
OUT
WH
ICH
CENTER
IT
CLOSEST
TO
THUS
EACH
CENTE
R
A
SET
OF
DATAPO
INTS
L
K
MEANS
ASK
USER
HOW
MANY
CLUSTERS
THEY
D
LIKE
E
G
K
GUESS
K
CLUSTER
CENTER
LOCATIONS
EACH
DATAPOINT
FINDS
OUT
WHICH
CENT
ER
IT
CLOSEST
TO
EACH
CENTER
FINDS
THE
CENTROID
OF
THE
POINTS
IT
OWNS
O
L
K
MEANS
CONVERGES
TO
A
LOCAL
MINIMUM
K
MEANS
CLUSTERING
MATLAB
DEMO
JAVA
DEMOS
K
MEANS
PROS
AND
CONS
PROS
SIMPLE
FAST
TO
COMPUTE
CONVERGES
TO
LOCAL
MINIMUM
OF
WITHIN
CLUSTER
SQUARED
ERROR
CONS
ISSUES
SETTING
K
SENSITIVE
TO
INITIAL
CENTERS
SENSITIVE
TO
OUTLIERS
DETECTS
SPHERICAL
CLUSTERS
ASSUMING
MEANS
CAN
BE
COMPUTED
AN
ASIDE
SMOOTHING
OUT
CLUSTER
ASSIGNMENTS
ASSIGNING
A
CLUSTER
LABEL
PER
PIXEL
MAY
YIELD
OUTLIERS
ORIGINAL
LABELED
BY
CLUSTER
CENTER
INTENSITY
HOW
TO
ENSURE
THEY
ARE
SEGMENTATION
AS
CLUSTERING
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
SIMILARITY
FEATURE
SPACE
INTENSITY
VALUE
D
K
K
QUANTIZATION
OF
THE
FEATURE
SPACE
SEGMENTATION
LABEL
MAP
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
COLOR
SIMILARITY
R
G
B
R
G
B
R
G
R
B
R
G
B
FEATURE
SPACE
COLOR
VALUE
D
SOURCE
K
GRAUMAN
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
SIMILARITY
CLUSTERS
BASED
ON
INTENSITY
SIMILARITY
DON
T
HAVE
TO
BE
SPATIALLY
COHERENT
SOURCE
K
GRAUMAN
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
POSITION
SIMILARITY
SOURCE
K
GRAUMAN
X
BOTH
REGIONS
ARE
BLACK
BUT
IF
WE
ALSO
INCLUDE
POSITION
X
Y
THEN
WE
COULD
GROUP
THE
TWO
INTO
DISTINCT
SEGMENTS
WAY
TO
ENCODE
BOTH
SIMILARITY
PROXIMITY
COLOR
BRIGHTNESS
POSITION
ALONE
ARE
NOT
ENOUGH
TO
DISTINGUISH
ALL
REGIONS
SOURCE
L
LAZEBNIK
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
TEXTURE
SIMILARITY
FILTER
BANK
OF
FILTERS
FEATURE
SPACE
FILTER
BANK
RESPONSES
E
G
D
SOURCE
K
GRAUMAN
FIND
TEXTONS
BY
CLUSTERING
VECTORS
OF
FILTER
BANK
OUTPUTS
DESCRIBE
TEXTURE
IN
A
WINDOW
BASED
ON
TEXTON
HISTOGRAM
IMAGE
TEXTON
MAP
TEXTON
INDEX
MALIK
BELONGIE
LEUNG
AND
SHI
IJCV
TEXTON
INDEX
TEXTON
INDEX
SOURCE
L
LAZEBNIK
IMAGE
SEGMENTATION
EXAMPLE
K
MEANS
PROS
AND
CONS
PROS
SIMPLE
FAST
TO
COMPUTE
CONVERGES
TO
LOCAL
MINIMUM
OF
WITHIN
CLUSTER
SQUARED
ERROR
CONS
ISSUES
SETTING
K
SENSITIVE
TO
INITIAL
CENTERS
SENSITIVE
TO
OUTLIERS
DETECTS
SPHERICAL
CLUSTERS
ASSUMING
MEANS
CAN
BE
COMPUTED
THE
MEAN
SHIFT
ALGORITHM
SEEKS
MODES
OR
LOCAL
MAXIMA
OF
DENSITY
IN
THE
FEATURE
SPACE
IMAGE
FEATURE
SPACE
L
U
V
COLOR
VALUES
ESTIMATED
DENSITY
SEARCH
WINDOW
CENTER
OF
MASS
MEAN
SHIFT
VECTOR
SOURCE
D
HOIEM
CLUSTER
ALL
DATA
POINTS
IN
THE
ATTRACTION
BASIN
OF
A
MODE
ATTRACTION
BASIN
THE
REGION
FOR
WHICH
ALL
TRAJECTORIES
LEAD
TO
THE
SAME
MODE
SLIDE
BY
Y
UKRAINITZ
B
SAREL
COMPUTE
FEATURES
FOR
EACH
POINT
COLOR
TEXTURE
ETC
INITIALIZE
WINDOWS
AT
INDIVIDUAL
FEATURE
POINTS
PERFORM
MEAN
SHIFT
FOR
EACH
WINDOW
UNTIL
CONVERGENCE
MERGE
WINDOWS
THAT
END
UP
NEAR
THE
SAME
PEAK
OR
MODE
SOURCE
D
HOIEM
PROS
MEAN
SHIFT
DOES
NOT
ASSUME
SHAPE
ON
CLUSTERS
ONE
PARAMETER
CHOICE
WINDOW
SIZE
GENERIC
TECHNIQUE
FIND
MULTIPLE
MODES
ROBUST
TO
OUTLIERS
CONS
SELECTION
OF
WINDOW
SIZE
MEAN
SHIFT
READING
NICELY
WRITTEN
MEAN
SHIFT
EXPLANATION
WITH
MATH
INCLUDES
M
CODE
FOR
MEAN
SHIFT
CLUSTERING
MEAN
SHIFT
PAPER
BY
COMANICIU
AND
MEER
ADAPTIVE
MEAN
SHIFT
IN
HIGHER
DIMENSIONS
FULLY
CONNECTED
GRAPH
NODE
VERTEX
FOR
EVERY
PIXEL
LINK
BETWEEN
EVERY
PAIR
OF
PIXELS
P
Q
AFFINITY
WEIGHT
WPQ
FOR
EACH
LINK
EDGE
WPQ
MEASURES
SIMILARITY
SIMILARITY
IS
INVERSELY
PROPORTIONAL
TO
DIFFERENCE
IN
COLOR
AND
POSITION
A
B
C
BREAK
GRAPH
INTO
SEGMENTS
WANT
TO
DELETE
LINKS
THAT
CROSS
BETWEEN
SEGMENTS
EASIEST
TO
BREAK
LINKS
THAT
HAVE
LOW
SIMILARITY
LOW
WEIGHT
SIMILAR
PIXELS
SHOULD
BE
IN
THE
SAME
SEGMENTS
DISSIMILAR
PIXELS
SHOULD
BE
IN
DIFFERENT
SEGMENTS
B
LINK
CUT
SET
OF
LINKS
WHOSE
REMOVAL
MAKES
A
GRAPH
DISCONNECTED
COST
OF
A
CUT
FIND
MINIMUM
CUT
CUT
A
B
W
P
A
Q
B
P
Q
GIVES
YOU
A
SEGMENTATION
FAST
ALGORITHMS
EXIST
FOR
DOING
THIS
K
MEANS
ITERATIVELY
RE
ASSIGN
POINTS
TO
THE
NEAREST
CLUSTER
CENTER
MEAN
SHIFT
CLUSTERING
ESTIMATE
MODES
GRAPH
CUTS
SPLIT
THE
NODES
IN
A
GRAPH
BASED
ON
ASSIGNED
LINKS
WITH
SIMILARITY
WEIGHTS
AGGLOMERATIVE
CLUSTERING
START
WITH
EACH
POINT
AS
ITS
OWN
CLUSTER
AND
ITERATIVELY
MERGE
THE
CLOSEST
CLUSTERS
SUMMARIZING
DATA
LOOK
AT
LARGE
AMOUNTS
OF
DATA
REPRESENT
A
LARGE
CONTINUOUS
VECTOR
WITH
THE
CLUSTER
NUMBER
COUNTING
HISTOGRAMS
OF
TEXTURE
COLOR
SIFT
VECTORS
SEGMENTATION
SEPARATE
THE
IMAGE
INTO
DIFFERENT
MID
LEVEL
REGIONS
FIND
OBJECT
BOUNDARIES
PREDICTION
IMAGES
IN
THE
SAME
CLUSTER
MAY
HAVE
THE
SAME
LABELS
SLIDE
CREDIT
J
HAYS
D
HOIEM
CS
INTRO
TO
COMPUTER
VISION
FITTING
MODELS
HOUGH
TRANSFORM
RANSAC
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
PLAN
FOR
TODAY
LAST
LECTURE
DETECTING
EDGES
THIS
LECTURE
DETECTING
LINES
AND
OTHER
SHAPES
FIND
THE
PARAMETERS
OF
A
LINE
THAT
BEST
FITS
OUR
DATA
LEAST
SQUARES
HOUGH
TRANSFORM
RANSAC
CHARACTERIZING
EDGES
AN
EDGE
IS
A
PLACE
OF
RAPID
CHANGE
IN
THE
IMAGE
INTENSITY
FUNCTION
INTENSITY
FUNCTION
IMAGE
ALONG
HORIZONTAL
SCANLINE
GRADIENT
FIRST
DERIVATIVE
EDGES
CORRESPOND
TO
EXTREMA
OF
GRADIENT
GRADIENTS
EDGES
PRIMARY
EDGE
DETECTION
STEPS
SMOOTHING
SUPPRESS
NOISE
EDGE
ENHANCEMENT
FILTER
FOR
CONTRAST
COMPUTE
GRADIENTS
EDGE
LOCALIZATION
DETERMINE
WHICH
LOCAL
MAXIMA
FROM
FILTER
OUTPUT
ARE
ACTUALLY
EDGES
VS
NOISE
BASIC
STEP
THRESHOLDING
CHOOSE
A
THRESHOLD
VALUE
T
SET
ANY
PIXELS
LESS
THAN
T
OFF
SET
ANY
PIXELS
GREATER
THAN
OR
EQUAL
TO
T
ON
MORE
ADVANCED
STEPS
THINNING
AND
HYSTERESIS
ORIGINAL
IMAGE
SOURCE
K
GRAUMAN
THRESHOLDING
GRADIENT
WITH
A
HIGHER
THRESHOLD
SOURCE
K
GRAUMAN
RELATED
LINE
DETECTION
FITTING
WHY
FIT
LINES
MANY
OBJECTS
CHARACTERIZED
BY
PRESENCE
OF
STRAIGHT
LINES
WHY
AREN
T
WE
DONE
JUST
BY
RUNNING
EDGE
DETECTION
DIFFICULTY
OF
LINE
FITTING
NOISE
IN
MEASURED
EDGE
POINTS
ORIENTATIONS
E
G
EDGES
NOT
COLLINEAR
WHERE
THEY
SHOULD
BE
HOW
TO
DETECT
TRUE
UNDERLYING
PARAMETERS
EXTRA
EDGE
POINTS
CLUTTER
WHICH
POINTS
GO
WITH
WHICH
LINE
IF
ANY
ONLY
SOME
PARTS
OF
EACH
LINE
DETECTED
AND
SOME
PARTS
ARE
MISSING
HOW
TO
FIND
A
LINE
THAT
BRIDGES
MISSING
EVIDENCE
FITTING
OTHER
OBJECTS
WANT
TO
ASSOCIATE
A
MODEL
WITH
OBSERVED
FEATURES
THE
MODEL
COULD
BE
A
LINE
A
CIRCLE
OR
AN
ARBITRARY
SHAPE
FIG
FROM
MARSZALEK
SCHMID
LEAST
SQUARES
LINE
FITTING
DATA
XN
YN
LINE
EQUATION
YI
M
XI
B
FIND
M
B
TO
MINIMIZE
XI
YI
Y
MX
B
WHERE
LINE
YOU
FOUND
TELLS
YOU
POINT
IS
ALONG
Y
AXIS
WHERE
POINT
REALLY
IS
ALONG
Y
AXIS
YOU
WANT
TO
FIND
A
SINGLE
LINE
THAT
EXPLAINS
ALL
OF
THE
POINTS
IN
YOUR
DATA
BUT
DATA
MAY
BE
NOISY
M
M
E
N
X
Y
AP
Y
I
I
B
I
B
XN
Y
N
OUTLIERS
CAN
HURT
THE
QUALITY
OF
OUR
PARAMETER
ESTIMATES
E
G
AN
EDGE
POINT
THAT
IS
NOISE
OR
DOESN
T
BELONG
TO
THE
LINE
WE
ARE
FITTING
TWO
COMMON
WAYS
TO
DEAL
WITH
OUTLIERS
BOTH
BOIL
DOWN
TO
VOTING
HOUGH
TRANSFORM
RANSAC
VOTING
IS
A
GENERAL
TECHNIQUE
WHERE
WE
LET
THE
FEATURES
VOTE
FOR
ALL
MODELS
THAT
ARE
COMPATIBLE
WITH
IT
CYCLE
THROUGH
FEATURES
CAST
VOTES
FOR
MODEL
PARAMETERS
LOOK
FOR
MODEL
PARAMETERS
THAT
RECEIVE
A
LOT
OF
VOTES
NOISE
CLUTTER
FEATURES
THEY
WILL
CAST
VOTES
TOO
BUT
TYPICALLY
THEIR
VOTES
SHOULD
BE
INCONSISTENT
WITH
THE
MAJORITY
OF
GOOD
FEATURES
FITTING
LINES
HOUGH
TRANSFORM
GIVEN
POINTS
THAT
BELONG
TO
A
LINE
WHAT
IS
THE
LINE
HOW
MANY
LINES
ARE
THERE
WHICH
POINTS
BELONG
TO
WHICH
LINES
HOUGH
TRANSFORM
IS
A
VOTING
TECHNIQUE
THAT
CAN
BE
USED
TO
ANSWER
ALL
OF
THESE
QUESTIONS
MAIN
IDEA
RECORD
VOTE
FOR
EACH
POSSIBLE
LINE
ON
WHICH
SOME
EDGE
POINT
LIES
LOOK
FOR
LINES
THAT
GET
MANY
VOTES
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
WHAT
DOES
A
POINT
IN
THE
IMAGE
SPACE
MAP
TO
ANSWER
THE
SOLUTIONS
OF
B
THIS
IS
A
LINE
IN
HOUGH
SPACE
TO
GO
FROM
IMAGE
SPACE
TO
HOUGH
SPACE
GIVEN
A
PAIR
OF
POINTS
X
Y
FIND
ALL
M
B
SUCH
THAT
Y
MX
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
WHAT
ARE
THE
LINE
PARAMETERS
FOR
THE
LINE
THAT
CONTAINS
BOTH
AND
IT
IS
THE
INTERSECTION
OF
THE
LINES
B
AND
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
HOW
CAN
WE
USE
THIS
TO
FIND
THE
MOST
LIKELY
PARAMETERS
M
B
FOR
THE
MOST
PROMINENT
LINE
IN
THE
IMAGE
SPACE
LET
EACH
EDGE
POINT
IN
IMAGE
SPACE
VOTE
FOR
A
SET
OF
POSSIBLE
PARAMETERS
IN
HOUGH
SPACE
ACCUMULATE
VOTES
IN
DISCRETE
SET
OF
BINS
PARAMETERS
WITH
THE
MOST
VOTES
INDICATE
LINE
IN
IMAGE
SPACE
Y
M
X
B
Y
M
X
B
PROBLEMS
WITH
THE
M
B
SPACE
UNBOUNDED
PARAMETER
DOMAINS
VERTICAL
LINES
REQUIRE
INFINITE
M
PROBLEMS
WITH
THE
M
B
SPACE
UNBOUNDED
PARAMETER
DOMAINS
VERTICAL
LINES
REQUIRE
INFINITE
M
ALTERNATIVE
POLAR
REPRESENTATION
Y
SIN
EACH
POINT
X
Y
WILL
ADD
A
SINUSOID
IN
THE
PARAMETER
SPACE
P
V
C
HOUGH
MACHINE
ANALYSIS
OF
BUBBLE
CHAMBER
PICTURES
PROC
INT
CONF
HIGH
ENERGY
ACCELERATORS
AND
INSTRUMENTATION
USE
A
POLAR
REPRESENTATION
FOR
THE
PARAMETER
SPACE
EACH
LINE
IS
A
SINUSOID
IN
HOUGH
PARAMETER
SPACE
Y
X
HOUGH
SPACE
X
COS
YSIN
INITIALIZE
ACCUMULATOR
H
TO
ALL
ZEROS
FOR
EACH
FEATURE
POINT
X
Y
IN
THE
IMAGE
FOR
Θ
TO
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
END
END
FIND
THE
VALUE
OF
Θ
Ρ
WHERE
H
Θ
Ρ
IS
A
LOCAL
MAXIMUM
THE
DETECTED
LINE
IN
THE
IMAGE
IS
GIVEN
BY
Ρ
X
COS
Θ
Y
SIN
Θ
DEREK
HOIEM
X
IMAGE
SPACE
EDGE
COORDINATES
VOTES
Y
D
X
IMAGE
SPACE
EDGE
COORDINATES
VOTES
WHAT
DIFFICULTY
DOES
THIS
PRESENT
FOR
AN
IMPLEMENTATION
FEATURES
VOTES
NEED
TO
ADJUST
GRID
SIZE
OR
SMOOTH
IMAGE
SPACE
EDGE
COORDINATES
VOTES
HERE
EVERYTHING
APPEARS
TO
BE
NOISE
OR
RANDOM
EDGE
POINTS
BUT
WE
STILL
SEE
PEAKS
IN
THE
VOTE
SPACE
INITIALIZE
ACCUMULATOR
H
TO
ALL
ZEROS
FIND
THE
VALUE
OF
Θ
Ρ
WHERE
H
Θ
Ρ
IS
A
LOCAL
MAXIMUM
THE
DETECTED
LINE
IN
THE
IMAGE
IS
GIVEN
BY
Ρ
X
COS
Θ
Y
SIN
Θ
RECALL
WHEN
WE
DETECT
AN
EDGE
POINT
WE
ALSO
KNOW
ITS
GRADIENT
DIRECTION
BUT
THIS
MEANS
THAT
THE
LINE
IS
UNIQUELY
DETERMINED
MODIFIED
HOUGH
TRANSFORM
A
CIRCLE
WITH
RADIUS
R
AND
CENTER
A
B
CAN
BE
DESCRIBED
AS
X
A
R
COS
Θ
Y
B
R
SIN
Θ
X
Y
A
B
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
A
FIXED
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
B
HOUGH
SPACE
A
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
A
FIXED
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
INTERSECTION
MOST
VOTES
FOR
CENTER
OCCUR
HERE
IMAGE
SPACE
HOUGH
SPACE
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
AN
UNKNOWN
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
B
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
AN
UNKNOWN
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
B
IMAGE
SPACE
HOUGH
SPACE
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
AN
UNKNOWN
RADIUS
R
KNOWN
GRADIENT
DIRECTION
IMAGE
SPACE
HOUGH
SPACE
FOR
EVERY
EDGE
PIXEL
X
Y
FOR
EACH
POSSIBLE
RADIUS
VALUE
R
X
A
R
COS
Θ
Y
B
R
SIN
Θ
FOR
EACH
POSSIBLE
GRADIENT
DIRECTION
Θ
OR
USE
ESTIMATED
GRADIENT
AT
X
Y
A
X
R
COS
Θ
COLUMN
B
Y
R
SIN
Θ
ROW
H
A
B
R
END
END
END
ORIGINAL
EDGES
VOTES
PENNY
NOTE
A
DIFFERENT
HOUGH
TRANSFORM
WITH
SEPARATE
ACCUMULATORS
WAS
USED
FOR
EACH
CIRCLE
RADIUS
QUARTERS
VS
PENNY
COMBOINREIGDINDAELTECTIONS
EDGES
VOTES
QUARTER
NOTE
A
DIFFERENT
HOUGH
TRANSFORM
WITH
SEPARATE
ACCUMULATORS
WAS
USED
FOR
EACH
CIRCLE
RADIUS
QUARTERS
VS
PENNY
EXAMPLE
IRIS
DETECTION
GRADIENT
THRESHOLD
HOUGH
SPACE
FIXED
RADIUS
MAX
DETECTIONS
HEMERSON
PISTORI
AND
EDUARDO
ROCHA
COSTA
VOTING
PRACTICAL
TIPS
MINIMIZE
IRRELEVANT
TOKENS
FIRST
REDUCE
NOISE
CHOOSE
A
GOOD
GRID
DISCRETIZATION
TOO
FINE
TOO
COARSE
TOO
COARSE
LARGE
VOTES
OBTAINED
WHEN
TOO
MANY
DIFFERENT
LINES
CORRESPOND
TO
A
SINGLE
BUCKET
TOO
FINE
MISS
LINES
BECAUSE
POINTS
THAT
ARE
NOT
EXACTLY
COLLINEAR
CAST
VOTES
FOR
DIFFERENT
BUCKETS
VOTE
FOR
NEIGHBORS
ALSO
SMOOTHING
IN
ACCUMULATOR
ARRAY
USE
DIRECTION
OF
EDGE
TO
REDUCE
PARAMETERS
BY
TO
READ
BACK
WHICH
POINTS
VOTED
FOR
WINNING
PEAKS
KEEP
TAGS
ON
THE
VOTES
WE
WANT
TO
FIND
A
TEMPLATE
DEFINED
BY
ITS
REFERENCE
POINT
CENTER
AND
SEVERAL
DISTINCT
TYPES
OF
LANDMARK
POINTS
IN
STABLE
SPATIAL
CONFIGURATION
TEMPLATE
TRIANGLE
CIRCLE
DIAMOND
SOME
TYPE
OF
VISUAL
TOKEN
E
G
FEATURE
OR
EDGE
POINT
WHAT
IF
WE
WANT
TO
DETECT
ARBITRARY
SHAPES
SPECIFICALLY
WHERE
IS
THE
REFERENCE
POINT
INTUITION
MODEL
IMAGE
NOVEL
IMAGE
X
VOTE
SPACE
NOW
SUPPOSE
THOSE
COLORS
ENCODE
GRADIENT
DIRECTIONS
DEFINE
A
MODEL
SHAPE
BY
ITS
BOUNDARY
POINTS
AND
A
REFERENCE
POINT
OFFLINE
PROCEDURE
AT
EACH
BOUNDARY
POINT
COMPUTE
DISPLACEMENT
VECTOR
R
A
PI
STORE
THESE
VECTORS
IN
A
TABLE
INDEXED
BY
GRADIENT
ORIENTATION
Θ
KRISTEN
GRAUMAN
DANA
H
BALLARD
GENERALIZING
THE
HOUGH
TRANSFORM
TO
DETECT
ARBITRARY
SHAPES
DETECTION
PROCEDURE
FOR
EACH
EDGE
POINT
USE
ITS
GRADIENT
ORIENTATION
Θ
TO
INDEX
INTO
STORED
TABLE
USE
RETRIEVED
R
VECTORS
TO
VOTE
FOR
REFERENCE
POINT
NOVEL
IMAGE
ASSUMING
TRANSLATION
IS
THE
ONLY
TRANSFORMATION
HERE
I
E
ORIENTATION
AND
SCALE
ARE
FIXED
TEMPLATE
REPRESENTATION
FOR
EACH
TYPE
OF
LANDMARK
POINT
STORE
ALL
POSSIBLE
DISPLACEMENT
VECTORS
TOWARDS
THE
CENTER
TEMPLATE
MODEL
DETECTING
THE
TEMPLATE
FOR
EACH
FEATURE
IN
A
NEW
IMAGE
LOOK
UP
THAT
FEATURE
TYPE
IN
THE
MODEL
AND
VOTE
FOR
THE
POSSIBLE
CENTER
LOCATIONS
ASSOCIATED
WITH
THAT
TYPE
IN
THE
MODEL
TEST
IMAGE
MODEL
SVETLANA
LAZEBNIK
INDEX
DISPLACEMENTS
BY
VISUAL
CODEWORD
TRAINING
IMAGE
VISUAL
CODEWORD
WITH
DISPLACEMENT
VECTORS
LEARNING
IN
COMPUTER
VISION
SVETLANA
LAZEBNIK
HOUGH
TRANSFORM
PROS
AND
CONS
PROS
ALL
POINTS
ARE
PROCESSED
INDEPENDENTLY
SO
CAN
COPE
WITH
OCCLUSION
GAPS
SOME
ROBUSTNESS
TO
NOISE
NOISE
POINTS
UNLIKELY
TO
CONTRIBUTE
CONSISTENTLY
TO
ANY
SINGLE
BIN
CAN
DETECT
MULTIPLE
INSTANCES
OF
A
MODEL
IN
A
SINGLE
PASS
CONS
COMPLEXITY
OF
SEARCH
TIME
FOR
MAXIMA
INCREASES
EXPONENTIALLY
WITH
THE
NUMBER
OF
MODEL
PARAMETERS
IF
PARAMETERS
AND
CHOICES
FOR
EACH
SEARCH
IS
O
NON
TARGET
SHAPES
CAN
PRODUCE
SPURIOUS
PEAKS
IN
PARAMETER
SPACE
QUANTIZATION
CAN
BE
TRICKY
TO
PICK
A
GOOD
GRID
SIZE
ADAPTED
FROM
KRISTEN
GRAUMAN
RANSAC
RANDOM
SAMPLE
CONSENSUS
APPROACH
WE
WANT
TO
AVOID
THE
IMPACT
OF
OUTLIERS
SO
LET
LOOK
FOR
INLIERS
AND
USE
THOSE
ONLY
INTUITION
IF
AN
OUTLIER
IS
CHOSEN
TO
COMPUTE
THE
CURRENT
FIT
THEN
THE
RESULTING
LINE
WON
T
HAVE
MUCH
SUPPORT
FROM
REST
OF
THE
POINTS
RANSAC
GENERAL
FORM
RANSAC
LOOP
RANDOMLY
SELECT
A
SEED
GROUP
OF
POINTS
ON
WHICH
TO
BASE
MODEL
ESTIMATE
FIT
MODEL
TO
THESE
POINTS
FIND
INLIERS
TO
THIS
MODEL
I
E
POINTS
WHOSE
DISTANCE
FROM
THE
LINE
IS
LESS
THAN
T
IF
THERE
ARE
D
OR
MORE
INLIERS
RE
COMPUTE
ESTIMATE
OF
MODEL
ON
ALL
OF
THE
INLIERS
REPEAT
N
TIMES
KEEP
THE
MODEL
WITH
THE
LARGEST
NUMBER
OF
INLIERS
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
RANSAC
RANDOM
SAMPLE
CONSENSUS
FISCHLER
BOLLES
IN
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
RANSAC
RANDOM
SAMPLE
CONSENSUS
FISCHLER
BOLLES
IN
LINE
FITTING
EXAMPLE
N
I
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
RANSAC
RANDOM
SAMPLE
CONSENSUS
FISCHLER
BOLLES
IN
LINE
FITTING
EXAMPLE
ALGORITHM
N
I
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REVIEW
KEYPOINT
MATCHING
FOR
SEARCH
FIND
A
SET
OF
DISTINCTIVE
KEY
POINTS
DEFINE
A
REGION
AROUND
EACH
KEYPOINT
WINDOW
COMPUTE
A
LOCAL
DESCRIPTOR
FROM
THE
REGION
D
F
A
FB
T
MATCH
DESCRIPTORS
ADAPTED
FROM
K
GRAUMAN
B
LEIBE
GIVEN
MATCHED
POINTS
IN
A
AND
B
ESTIMATE
THE
TRANSLATION
OF
THE
OBJECT
X
B
X
A
TX
I
I
I
I
TY
DEREK
HOIEM
TX
TY
LEAST
SQUARES
SOLUTION
X
B
X
A
TX
WRITE
DOWN
OBJECTIVE
FUNCTION
IN
FORM
AX
B
I
I
SOLVE
USING
PSEUDO
INVERSE
OR
EIGENVALUE
DECOMPOSITION
I
I
TY
X
B
X
A
Y
B
Y
A
TX
Y
X
B
X
A
N
N
DEREK
HOIEM
Y
B
Y
A
N
N
TX
TY
PROBLEM
OUTLIERS
RANSAC
SOLUTION
X
B
X
A
TX
SAMPLE
A
SET
OF
MATCHING
POINTS
PAIR
I
I
SOLVE
FOR
TRANSFORMATION
PARAMETERS
SCORE
PARAMETERS
WITH
NUMBER
OF
INLIERS
REPEAT
STEPS
N
TIMES
I
I
TY
TX
TY
PROBLEM
OUTLIERS
MULTIPLE
OBJECTS
AND
OR
MANY
TO
ONE
MATCHES
HOUGH
TRANSFORM
SOLUTION
X
B
X
A
TX
INITIALIZE
A
GRID
OF
PARAMETER
VALUES
I
I
EACH
MATCHED
PAIR
CASTS
A
VOTE
FOR
CONSISTENT
VALUES
FIND
THE
PARAMETERS
WITH
THE
MOST
VOTES
SOLVE
USING
LEAST
SQUARES
WITH
INLIERS
I
I
TY
FITTING
AND
MATCHING
SUMMARY
FITTING
PROBLEMS
REQUIRE
FINDING
ANY
SUPPORTING
EVIDENCE
FOR
A
MODEL
EVEN
WITHIN
CLUTTER
AND
MISSING
FEATURES
VOTING
AND
INLIER
APPROACHES
SUCH
AS
THE
HOUGH
TRANSFORM
AND
RANSAC
MAKE
IT
POSSIBLE
TO
FIND
LIKELY
MODEL
PARAMETERS
WITHOUT
SEARCHING
ALL
COMBINATIONS
OF
FEATURES
CAN
USE
THESE
APPROACHES
TO
COMPUTE
ROBUST
FEATURE
ALIGNMENT
MATCHING
AND
TO
MATCH
OBJECT
TEMPLATES
ADAPTED
FROM
KRISTEN
GRAUMAN
AND
DEREK
HOIEM
THE
REMAINING
SLIDES
SHOW
ANOTHER
MORE
DETAILED
EXAMPLE
OF
RANSAC
AND
PROS
CONS
REVIEW
ON
YOUR
OWN
TIME
LEAST
SQUARES
FIT
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
UNCONTAMINATED
SAMPLE
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
HOW
TO
CHOOSE
PARAMETERS
NUMBER
OF
SAMPLES
N
CHOOSE
N
SO
THAT
WITH
PROBABILITY
P
AT
LEAST
ONE
RANDOM
SAMPLE
IS
FREE
FROM
OUTLIERS
E
G
P
OUTLIER
RATIO
E
NUMBER
OF
SAMPLED
POINTS
MINIMUM
NUMBER
NEEDED
TO
FIT
THE
MODEL
DISTANCE
THRESHOLD
CHOOSE
SO
THAT
A
GOOD
POINT
WITH
NOISE
IS
LIKELY
E
G
PROB
WITHIN
THRESHOLD
EXPLANATION
IN
SZELISKI
MARC
POLLEFEYS
AND
DEREK
HOIEM
RANSAC
PROS
AND
CONS
PROS
SIMPLE
AND
GENERAL
APPLICABLE
TO
MANY
DIFFERENT
PROBLEMS
OFTEN
WORKS
WELL
IN
PRACTICE
CONS
LOTS
OF
PARAMETERS
TO
TUNE
SEE
PREVIOUS
SLIDE
DOESN
T
WORK
WELL
FOR
LOW
INLIER
RATIOS
TOO
MANY
ITERATIONS
OR
CAN
FAIL
COMPLETELY
CAN
T
ALWAYS
GET
A
GOOD
INITIALIZATION
OF
THE
MODEL
BASED
ON
THE
MINIMUM
NUMBER
OF
SAMPLES
COMMON
APPLICATIONS
IMAGE
STITCHING
RELATING
TWO
VIEWS
SPATIAL
VERIFICATION
SVETLANA
LAZEBNIK
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
PLAN
FOR
TODAY
EXAMPLES
OF
VISUAL
RECOGNITION
PROBLEMS
WHAT
SHOULD
WE
RECOGNIZE
RECOGNITION
PIPELINE
FEATURES
DATA
OVERVIEW
OF
SOME
METHODS
FOR
CLASSIFICATION
K
NEAREST
NEIGHBORS
LINEAR
CLASSIFIERS
SOME
TRANSLATIONS
FEATURE
VECTOR
DESCRIPTOR
REPRESENTATION
RECOGNITION
OFTEN
INVOLVES
CLASSIFICATION
CLASSES
CATEGORIES
HENCE
CLASSIFICATION
CATEGORIZATION
TRAINING
LEARNING
A
MODEL
E
G
CLASSIFIER
HAPPENS
AT
TRAINING
TIME
FROM
TRAINING
DATA
CLASSIFICATION
PREDICTION
HAPPENS
AT
TEST
TIME
MACHINE
LEARNING
PROBLEMS
SUPERVISED
LEARNING
UNSUPERVISED
LEARNI
NG
Q
CJ
CL
Q
C
C
GIVEN
A
FEATURE
REPRESENTATION
FOR
IMAGES
HOW
DO
WE
LEARN
A
MODEL
FOR
DISTINGUISHING
FEATURES
FROM
DIFFERENT
CLASSES
DECISION
BOUNDARY
ZEBRA
NON
ZEBRA
ASSIGN
INPUT
VECTOR
TO
ONE
OF
TWO
OR
MORE
CLASSES
ANY
DECISION
RULE
DIVIDES
THE
INPUT
SPACE
INTO
DECISION
REGIONS
SEPARATED
BY
DECISION
BOUNDARIES
EXAMPLE
SPAM
FILTER
EXAMPLES
OF
CATEGORIZATION
IN
VISION
PART
OR
OBJECT
DETECTION
E
G
FOR
EACH
WINDOW
FACE
OR
NON
FACE
SCENE
CATEGORIZATION
INDOOR
VS
OUTDOOR
URBAN
FOREST
KITCHEN
ETC
ACTION
RECOGNITION
PICKING
UP
VS
SITTING
DOWN
VS
STANDING
EMOTION
RECOGNITION
HAPPY
VS
SCARED
VS
SURPRISED
REGION
CLASSIFICATION
LABEL
PIXELS
INTO
DIFFERENT
OBJECT
SURFACE
CATEGORIES
BOUNDARY
CLASSIFICATION
BOUNDARY
VS
NON
BOUNDARY
ETC
ETC
ADAPTED
FROM
D
HOIEM
WHAT
DO
YOU
SEE
IN
THIS
IMAGE
TREES
BEAR
CAMERA
MAN
RABBIT
GRASS
FOREST
SLIDE
CREDIT
D
HOIEM
DESCRIBE
PREDICT
OR
INTERACT
WITH
THE
OBJECT
BASED
ON
VISUAL
CUES
IS
IT
DANGEROUS
HOW
FAST
DOES
IT
RUN
IS
IT
ALIVE
IS
IT
SOFT
DOES
IT
HAVE
A
TAIL
CAN
I
POKE
WITH
IT
SLIDE
CREDIT
D
HOIEM
TWO
CLASS
BINARY
CAT
VS
DOG
MULTI
CLASS
OFTEN
OBJECT
RECOGNITION
CALTECH
AVERAGE
OBJECT
IMAGES
FINE
GRAINED
RECOGNITION
PLACE
RECOGNITION
PLACES
DATABASE
DATING
HISTORICAL
PHOTOS
IMAGE
STYLE
RECOGNITION
SLIDE
CREDIT
D
HOIEM
LAYOUT
PREDICTION
ASSIGN
REGIONS
TO
ORIENTATION
GEOMETRIC
CONTEXT
ASSIGN
REGIONS
TO
DEPTH
MATERIAL
RECOGNITION
A
FARHADI
I
ENDRES
D
HOIEM
AND
D
FORSYTH
DESCRIBING
OBJECTS
BY
THEIR
ATTRIBUTES
CVPR
KOVASHKA
VIJAYANARASIMHAN
AND
K
GRAUMAN
ACTIVELY
SELECTING
ANNOTATIONS
AMONG
OBJECTS
AND
ATTRIBUTES
ICCV
KOVASHKA
D
PARIKH
AND
K
GRAUMAN
WHITTLESEARCH
IMAGE
SEARCH
WITH
RELATIVE
ATTRIBUTE
FEEDBACK
CVPR
GENERIC
CATEGORIZATION
PROBLEM
INSTANCE
LEVEL
RECOGNITION
PROBLEM
JOHN
CAR
WHICH
ONE
DO
YOU
THINK
IS
HARDER
GENERIC
OR
INSTANCE
LEVEL
RECOGNITION
VISUAL
OBJECT
CATEGORIES
WHAT
STUFF
SHOULD
WE
BOTHER
TO
RECOGNIZE
BASIC
LEVEL
CATEGORIES
IN
HUMAN
CATEGORIZATION
ROSCH
LAKOFF
THE
HIGHEST
LEVEL
AT
WHICH
CATEGORY
MEMBERS
HAVE
SIMILAR
PERCEIVED
SHAPE
THE
HIGHEST
LEVEL
AT
WHICH
A
SINGLE
MENTAL
IMAGE
REFLECTS
THE
ENTIRE
CATEGORY
THE
LEVEL
AT
WHICH
HUMAN
SUBJECTS
ARE
USUALLY
FASTEST
AT
IDENTIFYING
CATEGORY
MEMBERS
THE
FIRST
LEVEL
NAMED
AND
UNDERSTOOD
BY
CHILDREN
THE
HIGHEST
LEVEL
AT
WHICH
A
PERSON
USES
SIMILAR
MOTOR
ACTIONS
FOR
INTERACTION
WITH
CATEGORY
MEMBERS
VISUAL
OBJECT
CATEGORIES
BASIC
LEVEL
CATEGORIES
IN
HUMANS
SEEM
TO
BE
DEFINED
PREDOMINANTLY
VISUALLY
THERE
IS
EVIDENCE
THAT
HUMANS
USUALLY
START
WITH
BASIC
LEVEL
CATEGORIZATION
BEFORE
DOING
IDENTIFICATION
BASIC
LEVEL
CATEGORIZATION
IS
EASIER
AND
FASTER
FOR
HUMANS
THAN
OBJECT
IDENTIFICATION
ABSTRACT
LEVELS
ANIMAL
QUADRUPED
BASIC
LEVEL
DOG
CAT
COW
GERMAN
SHEPHERD
DOBERMAN
INDIVIDUAL
LEVEL
FIDO
OBJECT
CATEGORIZATION
TASK
DESCRIPTION
GIVEN
A
SMALL
NUMBER
OF
TRAINING
IMAGES
OF
A
CATEGORY
RECOGNIZE
A
PRIORI
UNKNOWN
INSTANCES
OF
THAT
CATEGORY
AND
ASSIGN
THE
CORRECT
CATEGORY
LABEL
WHICH
CATEGORIES
ARE
FEASIBLE
VISUALLY
FIDO
GERMAN
SHEPHERD
DOG
ANIMAL
LIVING
BEING
HOW
MANY
OBJECT
CATEGORIES
ARE
THERE
SOURCE
FEI
FEI
LI
ROB
FERGUS
ANTONIO
TORRALBA
BIEDERMAN
OTHER
TYPES
OF
CATEGORIES
FUNCTIONAL
CATEGORIES
E
G
CHAIRS
SOMETHING
YOU
CAN
SIT
ON
OTHER
TYPES
OF
CATEGORIES
AD
HOC
CATEGORIES
E
G
SOMETHING
YOU
CAN
FIND
IN
AN
OFFICE
ENVIRONMENT
WHY
RECOGNITION
RECOGNITION
A
FUNDAMENTAL
PART
OF
PERCEPTION
E
G
ROBOTS
AUTONOMOUS
AGENTS
ORGANIZE
AND
GIVE
ACCESS
TO
VISUAL
CONTENT
CONNECT
TO
INFORMATION
DETECT
TRENDS
AND
THEMES
SLIDE
CREDIT
K
GRAUMAN
WHY
IS
RECOGNITION
HARD
RECOGNITION
A
MACHINE
LEARNING
APPROACH
APPLY
A
PREDICTION
FUNCTION
TO
A
FEATURE
REPRESENTATION
OF
THE
IMAGE
TO
GET
THE
DESIRED
OUTPUT
F
APPLE
F
TOMATO
F
COW
Y
F
X
OUTPUT
PREDICTION
FUNCTION
IMAGE
FEATURE
TRAINING
GIVEN
A
TRAINING
SET
OF
LABELED
EXAMPLES
XN
YN
ESTIMATE
THE
PREDICTION
FUNCTION
F
BY
MINIMIZING
THE
PREDICTION
ERROR
ON
THE
TRAINING
SET
TESTING
APPLY
F
TO
A
NEVER
BEFORE
SEEN
TEST
EXAMPLE
X
AND
OUTPUT
THE
PREDICTED
VALUE
Y
F
X
TRAINING
STEPS
TESTING
TEST
IMAGE
SLIDE
CREDIT
D
HOIEM
AND
L
LAZEBNIK
RECOGNIZING
A
BEACH
RECOGNIZING
CLOTH
FABRIC
RECOGNIZING
A
MUG
FINE
GRAINED
RECOGNITION
WHAT
BREED
IS
THIS
DOG
WHAT
ARE
THE
RIGHT
FEATURES
DEPEND
ON
WHAT
YOU
WANT
TO
KNOW
OBJECT
SHAPE
LOCAL
SHAPE
INFO
SHADING
SHADOWS
TEXTURE
MATERIAL
PROPERTIES
ALBEDO
FEEL
HARDNESS
COLOR
TEXTURE
SCENE
GEOMETRIC
LAYOUT
LINEAR
PERSPECTIVE
GRADIENTS
LINE
SEGMENTS
ACTION
MOTION
OPTICAL
FLOW
TRACKED
POINTS
COLOR
L
A
B
COLOR
SPACE
HSV
COLOR
SPACE
TEXTURE
FILTER
BANKS
OR
HOG
OVER
REGIONS
HISTOGRAMS
OF
DESCRIPTORS
SIFT
LOWE
IJCV
BAG
OF
VISUAL
WORDS
BAG
OF
VISUAL
WORDS
IMAGE
PATCHES
CLUSTER
PATCHES
BOW
HISTOGRAM
TRAINING
STEPS
TESTING
TEST
IMAGE
SLIDE
CREDIT
D
HOIEM
AND
L
LAZEBNIK
RECOGNITION
TRAINING
DATA
IMAGES
IN
THE
TRAINING
SET
MUST
BE
ANNOTATED
WITH
THE
CORRECT
ANSWER
THAT
THE
MODEL
IS
EXPECTED
TO
PRODUCE
MOTORBIKE
SLIDE
CREDIT
L
LAZEBNIK
DATASETS
TODAY
IMAGENET
CATEGORIES
IMAGES
MICROSOFT
COCO
CATEGORIES
IMAGES
PASCAL
CATEGORIES
IMAGES
SUN
CATEGORIES
IMAGES
THE
PASCAL
VISUAL
OBJECT
CLASSES
CHALLENGE
CHALLENGE
CLASSES
PERSON
PERSON
ANIMAL
BIRD
CAT
COW
DOG
HORSE
SHEEP
VEHICLE
AEROPLANE
BICYCLE
BOAT
BUS
CAR
MOTORBIKE
TRAIN
INDOOR
BOTTLE
CHAIR
DINING
TABLE
POTTED
PLANT
SOFA
TV
MONITOR
DATASET
SIZE
BY
TRAINING
VALIDATION
IMAGES
BOUNDING
BOXES
SEGMENTATIONS
PASCAL
COMPETITIONS
CLASSIFICATION
FOR
EACH
OF
THE
TWENTY
CLASSES
PREDICTING
PRESENCE
ABSENCE
OF
AN
EXAMPLE
OF
THAT
CLASS
IN
THE
TEST
IMAGE
DETECTION
PREDICTING
THE
BOUNDING
BOX
IF
ANY
AND
LABEL
OF
EACH
OBJECT
FROM
THE
TWENTY
TARGET
CLASSES
IN
THE
TEST
IMAGE
ILLUMINATION
OBJECT
POSE
CLUTTER
OCCLUSIONS
INTRA
CLASS
APPEARANCE
VIEWPOINT
REALISTIC
SCENES
ARE
CROWDED
CLUTTERED
HAVE
OVERLAPPING
OBJECTS
CHALLENGES
IMPORTANCE
OF
CONTEXT
PAINTER
IDENTIFICATION
HOW
WOULD
YOU
LEARN
TO
IDENTIFY
THE
AUTHOR
OF
A
PAINTING
GOYA
KIRCHNER
KLIMT
MARC
MONET
VAN
GOGH
ONE
WAY
TO
THINK
ABOUT
IT
TRAINING
LABELS
DICTATE
THAT
TWO
EXAMPLES
ARE
THE
SAME
OR
DIFFERENT
IN
SOME
SENSE
FEATURES
AND
DISTANCES
DEFINE
VISUAL
SIMILARITY
GOAL
OF
TRAINING
IS
TO
LEARN
FEATURE
WEIGHTS
SO
THAT
VISUAL
SIMILARITY
PREDICTS
LABEL
SIMILARITY
LINEAR
CLASSIFIER
CONFIDENCE
IN
POSITIVE
LABEL
IS
A
WEIGHTED
SUM
OF
FEATURES
WHAT
ARE
THE
WEIGHTS
WE
WANT
THE
SIMPLEST
FUNCTION
THAT
IS
CONFIDENTLY
CORRECT
ADAPTED
FROM
D
HOIEM
NEAREST
NEIGHBOR
CLASSIFIER
TRAINING
EXAMPLES
FROM
CLASS
TEST
EXAMPLE
TRAINING
EXAMPLES
FROM
CLASS
F
X
LABEL
OF
THE
TRAINING
EXAMPLE
NEAREST
TO
X
ALL
WE
NEED
IS
A
DISTANCE
FUNCTION
FOR
OUR
INPUTS
NO
TRAINING
REQUIRED
K
NEAREST
NEIGHBORS
CLASSIFICATION
FOR
A
NEW
POINT
FIND
THE
K
CLOSEST
POINTS
FROM
TRAINING
DATA
LABELS
OF
THE
K
POINTS
VOTE
TO
CLASSIFY
BLACK
NEGATIVE
RED
POSITIVE
K
IF
QUERY
LANDS
HERE
THE
NN
CONSIST
OF
NEGATIVES
AND
POSITIVES
SO
WE
CLASSIFY
IT
AS
NEGATIVE
WHAT
ARE
THE
TRADEOFFS
OF
HAVING
A
TOO
LARGE
K
TOO
SMALL
K
A
NEAREST
NEIGHBOR
RECOGNITION
EXAMPLE
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
JAMES
HAYS
AND
ALEXEI
EFROS
CVPR
WHERE
IN
THE
WORLD
HOW
MUCH
CAN
AN
IMAGE
TELL
ABOUT
ITS
GEOGRAPHIC
LOCATION
NEAREST
NEIGHBORS
ACCORDING
TO
GIST
BAG
OF
SIFT
COLOR
HISTOGRAM
A
FEW
OTHERS
MILLION
GEOTAGGED
PHOTOS
BY
PHOTOGRAPHERS
SLIDES
JAMES
HAYS
EOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
C
SLIDES
JAMES
HAYS
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
SLIDES
JAMES
HAYS
THE
IMPORTANCE
OF
DATA
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
SLIDES
JAMES
HAYS
DISCRIMINATIVE
CLASSIFIERS
LEARN
A
SIMPLE
FUNCTION
OF
THE
INPUT
FEATURES
THAT
CORRECTLY
PREDICTS
THE
TRUE
LABELS
ON
THE
TRAINING
SET
𝑦
𝑓
TRAINING
GOALS
ACCURATE
CLASSIFICATION
OF
TRAINING
DATA
CORRECT
CLASSIFICATIONS
ARE
CONFIDENT
CLASSIFICATION
FUNCTION
IS
SIMPLE
SLIDE
CREDIT
D
HOIEM
LINEAR
CLASSIFIER
WHAT
ABOUT
THIS
LINE
FIND
A
LINEAR
FUNCTION
TO
SEPARATE
THE
CLASSES
F
X
SGN
WDXD
SGN
W
X
NN
VS
LINEAR
CLASSIFIERS
NN
PROS
SIMPLE
TO
IMPLEMENT
DECISION
BOUNDARIES
NOT
NECESSARILY
LINEAR
WORKS
FOR
ANY
NUMBER
OF
CLASSES
NONPARAMETRIC
METHOD
NN
CONS
NEED
GOOD
DISTANCE
FUNCTION
SLOW
AT
TEST
TIME
LARGE
SEARCH
PROBLEM
TO
FIND
NEIGHBORS
STORAGE
OF
DATA
LINEAR
PROS
LOW
DIMENSIONAL
PARAMETRIC
REPRESENTATION
VERY
FAST
AT
TEST
TIME
LINEAR
CONS
WORKS
FOR
TWO
CLASSES
HOW
TO
TRAIN
THE
LINEAR
FUNCTION
WHAT
IF
DATA
IS
NOT
LINEARLY
SEPARABLE
EVALUATING
CLASSIFIERS
ACCURACY
CORRECTLY
CLASSIFIED
ALL
TEST
EXAMPLES
PRECISION
RECALL
PRECISION
RETRIEVED
POSITIVES
RETRIEVED
RECALL
RETRIEVED
POSITIVES
POSITIVES
F
MEASURE
P
R
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
BEYOND
BAGS
OF
FEATURES
SPATIAL
PYRAMID
MATCHING
FOR
RECOGNIZING
NATURAL
SCENE
CATEGORIES
CVPR
SVETLANA
LAZEBNIK
SLAZEBNI
UIUC
EDU
BECKMAN
INSTITUTE
UNIVERSITY
OF
ILLINOIS
AT
URBANA
CHAMPAIGN
CORDELIA
SCHMID
CORDELIA
SCHMID
INRIALPES
FR
INRIA
RHÔNE
ALPES
FRANCE
JEAN
PONCE
PONCE
DI
ENS
FR
ECOLE
NORMALE
SUPÉRIEURE
FRANCE
SCENE
CATEGORY
DATASET
FEI
FEI
PERONA
OLIVA
TORRALBA
BAGS
OF
WORDS
BAG
OF
WORDS
STEPS
EXTRACT
LOCAL
FEATURES
LEARN
VISUAL
VOCABULARY
USING
CLUSTERING
QUANTIZE
LOCAL
FEATURES
USING
VISUAL
VOCABULARY
REPRESENT
IMAGES
BY
FREQUENCIES
OF
VISUAL
WORDS
SLIDE
CREDIT
L
LAZEBNIK
FEATURE
EXTRACTION
ON
WHICH
BOW
IS
BASED
WEAK
FEATURES
STRONG
FEATURES
EDGE
POINTS
AT
SCALES
AND
ORIENTATIONS
VOCABULARY
SIZE
SIFT
DESCRIPTORS
OF
PATCHES
SAMPLED
ON
A
REGULAR
GRID
QUANTIZED
TO
FORM
VISUAL
VOCABULARY
SIZE
SLIDE
CREDIT
L
LAZEBNIK
LEARNING
THE
VISUAL
VOCABULARY
LEARNING
THE
VISUAL
VOCABULARY
IMAGE
CATEGORIZATION
WITH
BAG
OF
WORDS
TRAINING
COMPUTE
BAG
OF
WORDS
REPRESENTATION
FOR
TRAINING
IMAGES
TRAIN
CLASSIFIER
ON
LABELED
EXAMPLES
USING
HISTOGRAM
VALUES
AS
FEATURES
LABELS
ARE
THE
SCENE
TYPES
E
G
MOUNTAIN
VS
FIELD
TESTING
EXTRACT
KEYPOINTS
DESCRIPTORS
FOR
TEST
IMAGES
QUANTIZE
INTO
VISUAL
WORDS
USING
THE
CLUSTERS
COMPUTED
AT
TRAINING
TIME
COMPUTE
VISUAL
WORD
HISTOGRAM
FOR
TEST
IMAGES
COMPUTE
LABELS
ON
TEST
IMAGES
USING
CLASSIFIER
OBTAINED
AT
TRAINING
TIME
MEASURE
ACCURACY
OF
TEST
PREDICTIONS
BY
COMPARING
THEM
TO
GROUND
TRUTH
TEST
LABELS
OBTAINED
FROM
HUMANS
WHAT
ABOUT
SPATIAL
LAYOUT
ALL
OF
THESE
IMAGES
HAVE
THE
SAME
COLOR
HISTOGRAM
SPATIAL
PYRAMID
COMPUTE
HISTOGRAM
IN
EACH
SPATIAL
BIN
SPATIAL
PYRAMID
PYRAMID
MATCHING
INDYK
THAPER
GRAUMAN
DARRELL
MATCHING
USING
PYRAMID
AND
HISTOGRAM
INTERSECTION
FOR
SOME
PARTICULAR
VISUAL
WORD
ORIGINAL
IMAGES
ADAPTED
FROM
L
LAZEBNIK
SCENE
CATEGORY
DATASET
FEI
FEI
PERONA
OLIVA
TORRALBA
MULTI
CLASS
CLASSIFICATION
RESULTS
TRAINING
IMAGES
PER
CLASS
FEI
FEI
PERONA
SCENE
CATEGORY
CONFUSIONS
DIFFICULT
INDOOR
IMAGES
KITCHEN
LIVING
ROOM
BEDROOM
DATASET
FEI
FEI
ET
AL
MULTI
CLASS
CLASSIFICATION
RESULTS
TRAINING
IMAGES
PER
CLASS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
PLAN
FOR
TODAY
SUPPORT
VECTOR
MACHINES
SEPARABLE
CASE
NON
SEPARABLE
CASE
LINEAR
NON
LINEAR
KERNELS
THE
IMPORTANCE
OF
GENERALIZATION
THE
BIAS
VARIANCE
TRADE
OFF
APPLIES
TO
ALL
CLASSIFIERS
LET
A
W
C
X
X
Y
AX
CY
B
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
D
B
DISTANCE
FROM
POINT
TO
LINE
LINES
IN
LET
A
W
C
X
Y
AX
CY
B
W
X
B
B
W
X
B
DISTANCE
FROM
D
W
POINT
TO
LINE
FIND
LINEAR
FUNCTION
TO
SEPARATE
POSITIVE
AND
NEGATIVE
EXAMPLES
XI
POSITIVE
XI
NEGATIVE
W
B
XI
W
B
WHICH
LINE
IS
BEST
DISCRIMINATIVE
CLASSIFIER
BASED
ON
OPTIMAL
SEPARATING
LINE
FOR
CASE
MAXIMIZE
THE
MARGIN
BETWEEN
THE
POSITIVE
AND
NEGATIVE
TRAINING
EXAMPLES
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
SUPPORT
VECTORS
MARGIN
FOR
SUPPORT
VECTORS
XI
W
B
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
AND
LINE
FOR
SUPPORT
VECTORS
XI
W
B
W
SUPPORT
VECTORS
MARGIN
WΤ
X
B
M
WANT
LINE
THAT
MAXIMIZES
THE
MARGIN
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
AND
LINE
XI
W
B
W
SUPPORT
VECTORS
MARGIN
THEREFORE
THE
MARGIN
IS
W
MAXIMIZE
MARGIN
W
CORRECTLY
CLASSIFY
ALL
TRAINING
DATA
POINTS
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
QUADRATIC
OPTIMIZATION
PROBLEM
ONE
CONSTRAINT
FOR
EACH
TRAINING
POINT
NOTE
SIGN
TRICK
SOLUTION
W
I
I
YI
XI
SOLUTION
W
I
I
YI
XI
B
YI
W
XI
FOR
ANY
SUPPORT
VECTOR
CLASSIFICATION
FUNCTION
F
X
SIGN
W
X
B
SIGN
I
I
YI
XI
X
B
IF
F
X
CLASSIFY
AS
NEGATIVE
OTHERWISE
CLASSIFY
AS
POSITIVE
NOTICE
THAT
IT
RELIES
ON
AN
INNER
PRODUCT
BETWEEN
THE
TEST
POINT
X
AND
THE
SUPPORT
VECTORS
XI
SOLVING
THE
OPTIMIZATION
PROBLEM
ALSO
INVOLVES
COMPUTING
THE
INNER
PRODUCTS
XI
XJ
BETWEEN
ALL
PAIRS
OF
TRAINING
POINTS
DATASETS
THAT
ARE
LINEARLY
SEPARABLE
WORK
OUT
GREAT
X
BUT
WHAT
IF
THE
DATASET
IS
JUST
TOO
HARD
X
WE
CAN
MAP
IT
TO
A
HIGHER
DIMENSIONAL
SPACE
GENERAL
IDEA
THE
ORIGINAL
INPUT
SPACE
CAN
ALWAYS
BE
MAPPED
TO
SOME
HIGHER
DIMENSIONAL
FEATURE
SPACE
WHERE
THE
TRAINING
SET
IS
SEPARABLE
NONLINEAR
KERNEL
EXAMPLE
CONSIDER
THE
MAPPING
X
X
X
Y
X
Y
XY
K
X
Y
XY
THE
KERNEL
TRICK
THE
LINEAR
CLASSIFIER
RELIES
ON
DOT
PRODUCT
BETWEEN
VECTORS
K
XI
XJ
XI
XJ
IF
EVERY
DATA
POINT
IS
MAPPED
INTO
HIGH
DIMENSIONAL
SPACE
VIA
SOME
TRANSFORMATION
Φ
XI
Φ
XI
THE
DOT
PRODUCT
BECOMES
K
XI
XJ
Φ
XI
Φ
XJ
A
KERNEL
FUNCTION
IS
SIMILARITY
FUNCTION
THAT
CORRESPONDS
TO
AN
INNER
PRODUCT
IN
SOME
EXPANDED
FEATURE
SPACE
THE
KERNEL
TRICK
INSTEAD
OF
EXPLICITLY
COMPUTING
THE
LIFTING
TRANSFORMATION
Φ
X
DEFINE
A
KERNEL
FUNCTION
K
SUCH
THAT
K
XI
XJ
Φ
XI
Φ
XJ
EXAMPLES
OF
KERNEL
FUNCTIONS
LINEAR
K
XI
X
J
I
J
POLYNOMIALS
OF
DEGREE
UP
TO
D
𝐾
𝑥𝑖
𝑥𝑗
𝑥𝑖𝑇𝑥𝑗
𝑑
GAUSSIAN
RBF
K
XI
X
J
EXP
HISTOGRAM
INTERSECTION
K
XI
X
J
MIN
K
XI
K
X
J
K
THE
W
THAT
MINIMIZES
MAXIMIZE
MARGIN
MISCLASSIFICATION
COST
DATA
SAMPLES
SLACK
VARIABLE
THE
W
THAT
MINIMIZES
MAXIMIZE
MARGIN
MINIMIZE
MISCLASSIFICATION
UNFORTUNATELY
THERE
IS
NO
DEFINITIVE
MULTI
CLASS
SVM
FORMULATION
IN
PRACTICE
WE
HAVE
TO
OBTAIN
A
MULTI
CLASS
SVM
BY
COMBINING
MULTIPLE
TWO
CLASS
SVMS
ONE
VS
OTHERS
TRAINING
LEARN
AN
SVM
FOR
EACH
CLASS
VS
THE
OTHERS
TESTING
APPLY
EACH
SVM
TO
THE
TEST
EXAMPLE
AND
ASSIGN
IT
TO
THE
CLASS
OF
THE
SVM
THAT
RETURNS
THE
HIGHEST
DECISION
VALUE
ONE
VS
ONE
TRAINING
LEARN
AN
SVM
FOR
EACH
PAIR
OF
CLASSES
TESTING
EACH
LEARNED
SVM
VOTES
FOR
A
CLASS
TO
ASSIGN
TO
THE
TEST
EXAMPLE
SVETLANA
LAZEBNIK
ONE
VS
ALL
A
K
A
ONE
VS
OTHERS
TRAIN
K
CLASSIFIERS
IN
EACH
POS
DATA
FROM
CLASS
I
NEG
DATA
FROM
CLASSES
OTHER
THAN
I
THE
CLASS
WITH
THE
MOST
CONFIDENT
PREDICTION
WINS
EXAMPLE
YOU
HAVE
CLASSES
TRAIN
CLASSIFIERS
VS
OTHERS
SCORE
VS
OTHERS
SCORE
VS
OTHERS
SCORE
VS
OTHER
SCORE
FINAL
PREDICTION
CLASS
ONE
VS
ONE
A
K
A
ALL
VS
ALL
TRAIN
K
K
BINARY
CLASSIFIERS
ALL
PAIRS
OF
CLASSES
THEY
ALL
VOTE
FOR
THE
LABEL
EXAMPLE
YOU
HAVE
CLASSES
THEN
TRAIN
CLASSIFIERS
VS
VS
VS
VS
VS
VS
VOTES
FINAL
PREDICTION
IS
CLASS
SVMS
FOR
RECOGNITION
DEFINE
YOUR
REPRESENTATION
FOR
EACH
EXAMPLE
SELECT
A
KERNEL
FUNCTION
COMPUTE
PAIRWISE
KERNEL
VALUES
BETWEEN
LABELED
EXAMPLES
USE
THIS
KERNEL
MATRIX
TO
SOLVE
FOR
SVM
SUPPORT
VECTORS
WEIGHTS
TO
CLASSIFY
A
NEW
EXAMPLE
COMPUTE
KERNEL
VALUES
BETWEEN
NEW
INPUT
AND
SUPPORT
VECTORS
APPLY
WEIGHTS
CHECK
SIGN
OF
OUTPUT
EXAMPLE
LEARNING
GENDER
WITH
SVMS
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
MOGHADDAM
AND
YANG
FACE
GESTURE
LEARNING
GENDER
WITH
SVMS
TRAINING
EXAMPLES
MALES
FEMALES
EXPERIMENT
WITH
VARIOUS
KERNELS
SELECT
GAUSSIAN
RBF
K
XI
XJ
EXP
SUPPORT
FACES
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
GENDER
PERCEPTION
EXPERIMENT
HOW
WELL
CAN
HUMANS
DO
SUBJECTS
PEOPLE
MALE
FEMALE
AGES
MID
TO
MID
TEST
DATA
FACE
IMAGES
MALES
FEMALES
LOW
RES
AND
HIGH
RES
VERSIONS
TASK
CLASSIFY
AS
MALE
OR
FEMALE
FORCED
CHOICE
NO
TIME
LIMIT
GENDER
PERCEPTION
EXPERIMENT
HOW
WELL
CAN
HUMANS
DO
ERROR
ERROR
HUMAN
VS
MACHINE
SVMS
PERFORMED
BETTER
THAN
ANY
SINGLE
HUMAN
TEST
SUBJECT
AT
EITHER
RESOLUTION
KRISTEN
GRAUMAN
SVMS
PROS
AND
CONS
PROS
MANY
PUBLICLY
AVAILABLE
SVM
PACKAGES
OR
USE
BUILT
IN
MATLAB
VERSION
BUT
SLOWER
KERNEL
BASED
FRAMEWORK
IS
VERY
POWERFUL
FLEXIBLE
OFTEN
A
SPARSE
SET
OF
SUPPORT
VECTORS
COMPACT
AT
TEST
TIME
WORK
VERY
WELL
IN
PRACTICE
EVEN
WITH
VERY
SMALL
TRAINING
SAMPLE
SIZES
CONS
NO
DIRECT
MULTI
CLASS
SVM
MUST
COMBINE
TWO
CLASS
SVMS
CAN
BE
TRICKY
TO
SELECT
BEST
KERNEL
FUNCTION
FOR
A
PROBLEM
COMPUTATION
MEMORY
DURING
TRAINING
TIME
MUST
COMPUTE
MATRIX
OF
KERNEL
VALUES
FOR
EVERY
PAIR
OF
EXAMPLES
LEARNING
CAN
TAKE
A
VERY
LONG
TIME
FOR
LARGE
SCALE
PROBLEMS
ADAPTED
FROM
LANA
LAZEBNIK
PRECISION
RECALL
F
MEASURE
TRUE
POSITIVES
IMAGES
THAT
CONTAIN
PEOPLE
TRUE
NEGATIVES
IMAGES
THAT
DO
NOT
CONTAIN
PEOPLE
PREDICTED
POSITIVES
IMAGES
PREDICTED
TO
CONTAIN
PEOPLE
PREDICTED
NEGATIVES
IMAGES
PREDICTED
NOT
TO
CONTAIN
PEOPLE
PRECISION
RECALL
F
MEASURE
ACCURACY
TRAINING
SET
LABELS
KNOWN
TEST
SET
LABELS
UNKNOWN
HOW
WELL
DOES
A
LEARNED
MODEL
GENERALIZE
FROM
THE
DATA
IT
WAS
TRAINED
ON
TO
A
NEW
TEST
SET
COMPONENTS
OF
GENERALIZATION
ERROR
BIAS
HOW
MUCH
THE
AVERAGE
MODEL
OVER
ALL
TRAINING
SETS
DIFFERS
FROM
THE
TRUE
MODEL
ERROR
DUE
TO
INACCURATE
ASSUMPTIONS
SIMPLIFICATIONS
MADE
BY
THE
MODEL
VARIANCE
HOW
MUCH
MODELS
ESTIMATED
FROM
DIFFERENT
TRAINING
SETS
DIFFER
FROM
EACH
OTHER
UNDERFITTING
MODEL
IS
TOO
SIMPLE
TO
REPRESENT
ALL
THE
RELEVANT
CLASS
CHARACTERISTICS
HIGH
BIAS
AND
LOW
VARIANCE
HIGH
TRAINING
ERROR
AND
HIGH
TEST
ERROR
OVERFITTING
MODEL
IS
TOO
COMPLEX
AND
FITS
IRRELEVANT
CHARACTERISTICS
NOISE
IN
THE
DATA
LOW
BIAS
AND
HIGH
VARIANCE
MODELS
WITH
TOO
FEW
PARAMETERS
ARE
INACCURATE
BECAUSE
OF
A
LARGE
BIAS
NOT
ENOUGH
FLEXIBILITY
MODELS
WITH
TOO
MANY
PARAMETERS
ARE
INACCURATE
BECAUSE
OF
A
LARGE
VARIANCE
TOO
MUCH
SENSITIVITY
TO
THE
SAMPLE
UNDERFITTING
OVERFITTING
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
NEED
VALIDATION
SET
VALIDATION
SET
IS
SEPARATE
FROM
THE
TEST
SET
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
NUMBER
OF
TRAINING
EXAMPLES
CHOOSE
A
SIMPLER
CLASSIFIER
USE
FEWER
FEATURES
GET
MORE
TRAINING
DATA
REGULARIZE
THE
PARAMETERS
SLIDE
CREDIT
D
HOIEM
NO
REGULARIZATION
HUGE
REGULARIZATION
FIGURES
FROM
BISHOP
CHARACTERISTICS
OF
VISION
LEARNING
PROBLEMS
LOTS
OF
CONTINUOUS
FEATURES
SPATIAL
PYRAMID
MAY
HAVE
FEATURES
IMBALANCED
CLASSES
OFTEN
LIMITED
POSITIVE
EXAMPLES
PRACTICALLY
INFINITE
NEGATIVE
EXAMPLES
DIFFICULT
PREDICTION
TASKS
RECENTLY
MASSIVE
TRAINING
SETS
BECAME
AVAILABLE
IF
WE
HAVE
A
MASSIVE
TRAINING
SET
WE
WANT
CLASSIFIERS
WITH
LOW
BIAS
HIGH
VARIANCE
IS
OK
AND
REASONABLY
EFFICIENT
TRAINING
REMEMBER
NO
FREE
LUNCH
MACHINE
LEARNING
ALGORITHMS
ARE
TOOLS
THREE
KINDS
OF
ERROR
INHERENT
UNAVOIDABLE
BIAS
DUE
TO
OVER
SIMPLIFICATIONS
VARIANCE
DUE
TO
INABILITY
TO
PERFECTLY
ESTIMATE
PARAMETERS
FROM
LIMITED
DATA
TRY
SIMPLE
CLASSIFIERS
FIRST
BETTER
TO
HAVE
SMART
FEATURES
AND
SIMPLE
CLASSIFIERS
THAN
SIMPLE
FEATURES
AND
SMART
CLASSIFIERS
USE
INCREASINGLY
POWERFUL
CLASSIFIERS
WITH
MORE
TRAINING
DATA
BIAS
VARIANCE
TRADEOFF
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
TODAY
WINDOW
BASED
GENERIC
OBJECT
DETECTION
BASIC
PIPELINE
BOOSTING
CLASSIFIERS
FACE
DETECTION
AS
CASE
STUDY
BASIC
FRAMEWORK
BUILD
TRAIN
OBJECT
MODEL
CHOOSE
A
REPRESENTATION
LEARN
OR
FIT
PARAMETERS
OF
MODEL
CLASSIFIER
GENERATE
CANDIDATES
IN
NEW
IMAGE
SCORE
THE
CANDIDATES
REPRESENTATION
CHOICE
PART
BASED
CONSIDER
EDGES
CONTOURS
AND
ORIENTED
INTENSITY
GRADIENTS
SUMMARIZE
LOCAL
DISTRIBUTION
OF
GRADIENTS
WITH
HISTOGRAM
LOCALLY
ORDERLESS
OFFERS
INVARIANCE
TO
SMALL
SHIFTS
AND
ROTATIONS
GIVEN
THE
REPRESENTATION
TRAIN
A
BINARY
CLASSIFIER
NOY
ENSO
TCAARC
AR
WINDOW
BASED
OBJECT
DETECTION
RECAP
TRAINING
OBTAIN
TRAINING
DATA
DEFINE
FEATURES
DEFINE
CLASSIFIER
GIVEN
NEW
IMAGE
SLIDE
WINDOW
SCORE
BY
CLASSIFIER
FEATURE
EXTRACTION
FACE
DETECTION
AND
RECOGNITION
SALLY
CHALLENGES
OF
FACE
DETECTION
SLIDING
WINDOW
DETECTOR
MUST
EVALUATE
TENS
OF
THOUSANDS
OF
LOCATION
SCALE
COMBINATIONS
FACES
ARE
RARE
PER
IMAGE
A
MEGAPIXEL
IMAGE
HAS
PIXELS
AND
A
COMPARABLE
NUMBER
OF
CANDIDATE
FACE
LOCATIONS
FOR
COMPUTATIONAL
EFFICIENCY
WE
SHOULD
TRY
TO
SPEND
AS
LITTLE
TIME
AS
POSSIBLE
ON
THE
NON
FACE
WINDOWS
TO
AVOID
HAVING
A
FALSE
POSITIVE
IN
EVERY
IMAGE
OUR
FALSE
POSITIVE
RATE
HAS
TO
BE
LESS
THAN
VIOLA
JONES
FACE
DETECTOR
DISCRIMINATIVE
CLASSIFIER
CONSTRUCTION
SLIDE
ADAPTED
FROM
ANTONIO
TORRALBA
WEAK
CLASSIFIER
WEIGHTS
INCREASED
WEAK
CLASSIFIER
WEIGHTS
INCREASED
WEAK
CLASSIFIER
FINAL
CLASSIFIER
IS
A
COMBINATION
OF
WEAK
CLASSIFIERS
BOOSTING
TRAINING
INITIALLY
WEIGHT
EACH
TRAINING
EXAMPLE
EQUALLY
IN
EACH
BOOSTING
ROUND
FIND
THE
WEAK
LEARNER
THAT
ACHIEVES
THE
LOWEST
WEIGHTED
TRAINING
ERROR
RAISE
WEIGHTS
OF
TRAINING
EXAMPLES
MISCLASSIFIED
BY
CURRENT
WEAK
LEARNER
COMPUTE
FINAL
CLASSIFIER
AS
LINEAR
COMBINATION
OF
ALL
WEAK
LEARNERS
WEIGHT
OF
EACH
LEARNER
IS
DIRECTLY
PROPORTIONAL
TO
ITS
ACCURACY
EXACT
FORMULAS
FOR
RE
WEIGHTING
AND
COMBINING
WEAK
LEARNERS
DEPEND
ON
THE
PARTICULAR
BOOSTING
SCHEME
E
G
ADABOOST
VIOLA
JONES
FACE
DETECTOR
MAIN
IDEA
REPRESENT
LOCAL
TEXTURE
WITH
EFFICIENTLY
COMPUTABLE
RECTANGULAR
FEATURES
WITHIN
WINDOW
OF
INTEREST
SELECT
DISCRIMINATIVE
FEATURES
TO
BE
WEAK
CLASSIFIERS
USE
BOOSTED
COMBINATION
OF
THEM
AS
FINAL
CLASSIFIER
FORM
A
CASCADE
OF
SUCH
CLASSIFIERS
REJECTING
CLEAR
NEGATIVES
QUICKLY
VIOLA
JONES
DETECTOR
FEATURES
RECTANGULAR
FILTERS
FEATURE
OUTPUT
IS
DIFFERENCE
BETWEEN
ADJACENT
REGIONS
VALUE
PIXELS
IN
WHITE
AREA
PIXELS
IN
BLACK
AREA
EFFICIENTLY
COMPUTABLE
WITH
INTEGRAL
IMAGE
ANY
SUM
CAN
BE
COMPUTED
IN
CONSTANT
TIME
VALUE
AT
X
Y
IS
SUM
OF
PIXELS
ABOVE
AND
TO
THE
LEFT
OF
X
Y
INTEGRAL
IMAGE
FAST
COMPUTATION
WITH
INTEGRAL
IMAGES
THE
INTEGRAL
IMAGE
COMPUTES
A
VALUE
AT
EACH
PIXEL
X
Y
THAT
IS
THE
SUM
OF
THE
PIXEL
VALUES
ABOVE
AND
TO
THE
LEFT
OF
X
Y
INCLUSIVE
THIS
CAN
QUICKLY
BE
COMPUTED
IN
ONE
PASS
THROUGH
THE
IMAGE
COMPUTING
SUM
WITHIN
A
RECTANGLE
LET
A
B
C
D
BE
THE
VALUES
OF
THE
INTEGRAL
IMAGE
AT
THE
CORNERS
OF
A
THEN
THE
SUM
OF
ORIGINAL
IMAGE
VALUES
WITHIN
THE
RECTANGLE
CAN
BE
COMPUTED
AS
SUM
A
B
C
D
ONLY
ADDITIONS
ARE
REQUIRED
FOR
ANY
SIZE
OF
RECTANGLE
EXAMPLE
SOURCE
RESULT
VIOLA
JONES
DETECTOR
FEATURES
CONSIDERING
ALL
POSSIBLE
FILTER
PARAMETERS
POSITION
SCALE
AND
TYPE
POSSIBLE
FEATURES
ASSOCIATED
WITH
EACH
X
WINDOW
WHICH
SUBSET
OF
THESE
FEATURES
SHOULD
WE
USE
TO
DETERMINE
IF
A
WINDOW
HAS
A
FACE
USE
ADABOOST
BOTH
TO
SELECT
THE
INFORMATIVE
FEATURES
AND
TO
FORM
THE
CLASSIFIER
VIOLA
JONES
DETECTOR
ADABOOST
WANT
TO
SELECT
THE
SINGLE
RECTANGLE
FEATURE
AND
THRESHOLD
THAT
BEST
SEPARATES
POSITIVE
FACES
AND
NEGATIVE
NON
FACES
TRAINING
EXAMPLES
IN
TERMS
OF
WEIGHTED
ERROR
RESULTING
WEAK
CLASSIFIER
OUTPUTS
OF
A
POSSIBLE
RECTANGLE
FEATURE
ON
FACES
AND
NON
FACES
FOR
NEXT
ROUND
REWEIGHT
THE
EXAMPLES
ACCORDING
TO
ERRORS
CHOOSE
ANOTHER
FILTER
THRESHOLD
COMBO
START
WITH
UNIFORM
WEIGHTS
ON
TRAINING
EXAMPLES
FOR
M
ROUNDS
EVALUATE
WEIGHTED
ERROR
FOR
EACH
WEAK
LEARNER
PICK
BEST
LEARNER
FIGURE
FROM
C
BISHOP
NOTES
FROM
K
GRAUMAN
NORMALIZE
THE
WEIGHTS
SO
THEY
SUM
TO
RE
WEIGHT
THE
EXAMPLES
INCORRECTLY
CLASSIFIED
GET
MORE
WEIGHT
CORRECTLY
CLASSIFIED
GET
LESS
WEIGHT
FINAL
CLASSIFIER
IS
COMBINATION
OF
WEAK
ONES
WEIGHTED
ACCORDING
TO
ERROR
THEY
HAD
BOOSTING
FOR
FACE
DETECTION
FIRST
TWO
FEATURES
SELECTED
BY
BOOSTING
THIS
FEATURE
COMBINATION
CAN
YIELD
DETECTION
RATE
AND
FALSE
POSITIVE
RATE
BOOSTING
PROS
AND
CONS
ADVANTAGES
OF
BOOSTING
INTEGRATES
CLASSIFICATION
WITH
FEATURE
SELECTION
COMPLEXITY
OF
TRAINING
IS
LINEAR
IN
THE
NUMBER
OF
TRAINING
EXAMPLES
FLEXIBILITY
IN
THE
CHOICE
OF
WEAK
LEARNERS
BOOSTING
SCHEME
TESTING
IS
FAST
EASY
TO
IMPLEMENT
DISADVANTAGES
NEEDS
MANY
TRAINING
EXAMPLES
OFTEN
FOUND
NOT
TO
WORK
AS
WELL
AS
AN
ALTERNATIVE
DISCRIMINATIVE
CLASSIFIER
SUPPORT
VECTOR
MACHINE
SVM
ARE
WE
DONE
EVEN
IF
THE
FILTERS
ARE
FAST
TO
COMPUTE
EACH
NEW
IMAGE
HAS
A
LOT
OF
POSSIBLE
WINDOWS
TO
SEARCH
HOW
TO
MAKE
THE
DETECTION
MORE
EFFICIENT
CASCADING
CLASSIFIERS
FOR
DETECTION
FORM
A
CASCADE
WITH
LOW
FALSE
NEGATIVE
RATES
EARLY
ON
APPLY
LESS
ACCURATE
BUT
FASTER
CLASSIFIERS
FIRST
TO
IMMEDIATELY
DISCARD
WINDOWS
THAT
CLEARLY
APPEAR
TO
BE
NEGATIVE
TRAIN
WITH
POSITIVES
NEGATIVES
REAL
TIME
DETECTOR
USING
LAYER
CASCADE
FEATURES
IN
ALL
LAYERS
A
SEMINAL
APPROACH
TO
REAL
TIME
OBJECT
DETECTION
TRAINING
IS
SLOW
BUT
DETECTION
IS
VERY
FAST
KEY
IDEAS
INTEGRAL
IMAGES
FOR
FAST
FEATURE
EVALUATION
BOOSTING
FOR
FEATURE
SELECTION
ATTENTIONAL
CASCADE
OF
CLASSIFIERS
FOR
FAST
REJECTION
OF
NON
FACE
WINDOWS
P
VIOLA
AND
M
JONES
CVPR
P
VIOLA
AND
M
JONES
IJCV
MATLAB
DEMO
EXAMPLE
USING
VIOLA
JONES
DETECTOR
FRONTAL
FACES
DETECTED
AND
THEN
TRACKED
CHARACTER
NAMES
INFERRED
WITH
ALIGNMENT
OF
SCRIPT
AND
SUBTITLES
EVERINGHAM
M
SIVIC
J
AND
ZISSERMAN
A
HELLO
MY
NAME
IS
BUFFY
AUTOMATIC
NAMING
OF
CHARACTERS
IN
TV
VIDEO
BMVC
FACE
DETECTION
AND
RECOGNITION
SALLY
CAN
BE
TRAINED
TO
RECOGNIZE
PETS
SLIDE
CREDIT
LANA
LAZEBNIK
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
TODAY
OBJECT
CATEGORY
DETECTION
WINDOW
BASED
APPROACHES
LAST
TIME
VIOLA
JONES
DETECTOR
DALAL
TRIGGS
PEDESTRIAN
DETECTOR
PART
BASED
APPROACHES
IMPLICIT
SHAPE
MODEL
DEFORMABLE
PARTS
MODEL
DPM
IMPROVEMENTS
SPEEDING
UP
DPM
ANALYZING
THE
FAILURES
OF
DPM
DALAL
TRIGGS
PEDESTRIAN
DETECTOR
EXTRACT
FIXED
SIZED
PIXEL
WINDOW
AT
EACH
POSITION
AND
SCALE
COMPUTE
HOG
HISTOGRAM
OF
GRADIENT
FEATURES
WITHIN
EACH
WINDOW
SCORE
THE
WINDOW
WITH
A
LINEAR
SVM
CLASSIFIER
PERFORM
NON
MAXIMA
SUPPRESSION
TO
REMOVE
OVERLAPPING
DETECTIONS
WITH
LOWER
SCORES
HISTOGRAM
OF
GRADIENT
ORIENTATIONS
ORIENTATION
BINS
FOR
UNSIGNED
ANGLES
HISTOGRAMS
IN
PIXEL
CELLS
VOTES
WEIGHTED
BY
MAGNITUDE
CELLS
POS
W
NEG
W
PEDESTRIAN
NON
MAX
SUPPRESSION
ADAPTED
FROM
DEREK
HOIEM
SINGLE
RIGID
TEMPLATE
USUALLY
NOT
ENOUGH
TO
REPRESENT
A
CATEGORY
MANY
OBJECTS
E
G
HUMANS
ARE
ARTICULATED
OR
HAVE
PARTS
THAT
CAN
VARY
IN
CONFIGURATION
MANY
OBJECT
CATEGORIES
LOOK
VERY
DIFFERENT
FROM
DIFFERENT
VIEWPOINTS
OR
FROM
INSTANCE
TO
INSTANCE
SLIDE
BY
N
SNAVELY
IMAGES
FROM
CALTECH
IMAGES
FROM
D
RAMANAN
DATASET
DEFINE
OBJECT
BY
COLLECTION
OF
PARTS
MODELED
BY
APPEARANCE
SPATIAL
CONFIGURATION
ONE
EXTREME
FIXED
TEMPLATE
OBJECT
MODEL
SUM
OF
SCORES
OF
FEATURES
AT
FIXED
POSITIONS
NON
OBJECT
OBJECT
ANOTHER
EXTREME
BAG
OF
WORDS
STAR
SHAPED
MODEL
STAR
SHAPED
MODEL
ARTICULATED
PARTS
MODEL
OBJECT
IS
CONFIGURATION
OF
PARTS
EACH
PART
IS
DETECTABLE
AND
CAN
MOVE
AROUND
ADAPTED
FROM
DEREK
HOIEM
IMAGES
FROM
FELZENSZWALB
VISUAL
VOCABULARY
IS
USED
TO
INDEX
VOTES
FOR
OBJECT
POSITION
A
VISUAL
WORD
PART
TRAINING
IMAGE
ANNOTATED
WITH
OBJECT
LOCALIZATION
INFO
VISUAL
CODEWORD
WITH
DISPLACEMENT
VECTORS
LEARNING
IN
COMPUTER
VISION
LANA
LAZEBNIK
BUILD
VOCABULARY
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
BUILD
VOCABULARY
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MAP
THE
PATCH
AROUND
EACH
INTEREST
POINT
TO
CLOSEST
WORD
BUILD
VOCABULARY
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MAP
THE
PATCH
AROUND
EACH
INTEREST
POINT
TO
CLOSEST
WORD
FOR
EACH
WORD
STORE
ALL
POSITIONS
IT
WAS
FOUND
RELATIVE
TO
OBJECT
CENTER
TEMPLATE
REPRESENTATION
FOR
EACH
TYPE
OF
LANDMARK
POINT
STORE
ALL
POSSIBLE
DISPLACEMENT
VECTORS
TOWARDS
THE
CENTER
TEMPLATE
SVETLANA
LAZEBNIK
MODEL
GIVEN
NEW
TEST
IMAGE
EXTRACT
PATCHES
MATCH
TO
VOCABULARY
WORDS
CAST
VOTES
FOR
POSSIBLE
POSITIONS
OF
OBJECT
CENTER
SEARCH
FOR
MAXIMA
IN
VOTING
SPACE
LANA
LAZEBNIK
DETECTION
RESULTS
QUALITATIVE
PERFORMANCE
RECOGNIZES
DIFFERENT
KINDS
OF
OBJECTS
ROBUST
TO
CLUTTER
OCCLUSION
NOISE
LOW
CONTRAST
K
GRAUMAN
B
LEIBE
ROOT
FILTER
PART
FILTERS
DEFORMATION
WEIGHTS
MULTIPLE
COMPONENTS
THE
SCORE
OF
A
HYPOTHESIS
IS
THE
SUM
OF
APPEARANCE
SCORES
MINUS
THE
SUM
OF
DEFORMATION
COSTS
PART
N
FEATURES
N
DISPLACEMENTS
SCORE
P
P
W
X
P
D
DX
DY
DY
I
I
I
E
HOW
MUCH
THE
PART
PI
MOVED
FROM
ITS
EXPECTED
LOCATION
IN
THE
X
Y
DIRECTIONS
ADAPTED
FROM
LANA
LAZEBNIK
APPEARANCE
WEIGHTS
DEFORMATION
WEIGHTS
HOW
MUCH
WE
LL
PENALIZE
THE
PART
PI
FOR
MOVING
FROM
ITS
EXPECTED
LOCATION
TRAINING
DATA
CONSISTS
OF
IMAGES
WITH
LABELED
BOUNDING
BOXES
NEED
TO
LEARN
THE
WEIGHTS
AND
DEFORMATION
PARAMETERS
F
X
W
H
X
W
ARE
MODEL
PARAMETERS
Z
ARE
LATENT
HYPOTHESES
LATENT
SVM
TRAINING
INITIALIZE
W
AND
ITERATE
FIX
W
AND
FIND
THE
BEST
Z
FOR
EACH
TRAINING
EXAMPLE
FIX
Z
AND
SOLVE
FOR
W
STANDARD
SVM
TRAINING
COMPONENT
COMPONENT
TODAY
OBJECT
CATEGORY
DETECTION
WINDOW
BASED
APPROACHES
LAST
TIME
VIOLA
JONES
DETECTOR
DALAL
TRIGGS
PEDESTRIAN
DETECTOR
PART
BASED
APPROACHES
IMPLICIT
SHAPE
MODEL
DEFORMABLE
PARTS
MODEL
DPM
IMPROVEMENTS
SPEEDING
UP
DPM
ANALYZING
THE
FAILURES
OF
DPM
SPEEDING
UP
DETECTION
RESTRICT
SET
OF
WINDOWS
WE
PASS
THROUGH
SVM
TO
THOSE
W
HIGH
OBJECTNESS
OBJECTNESS
CUE
WHERE
PEOPLE
LOOK
OBJECTNESS
CUE
COLOR
CONTRAST
AT
BOUNDARY
OBJECTNESS
CUE
NO
SEGMENTS
STRADDLING
THE
OBJECT
BOX
BOXES
FOUND
TO
HAVE
HIGH
OBJECTNESS
CYAN
GROUND
TRUTH
BOUNDING
BOXES
YELLOW
CORRECT
AND
RED
INCORRECT
PREDICTIONS
FOR
OBJECTNESS
ONLY
RUN
THE
SHEEP
HORSE
CHAIR
ETC
CLASSIFIER
ON
THE
YELLOW
RED
BOXES
TODAY
OBJECT
CATEGORY
DETECTION
WINDOW
BASED
APPROACHES
LAST
TIME
VIOLA
JONES
DETECTOR
DALAL
TRIGGS
PEDESTRIAN
DETECTOR
PART
BASED
APPROACHES
IMPLICIT
SHAPE
MODEL
DEFORMABLE
PARTS
MODEL
DPM
IMPROVEMENTS
SPEEDING
UP
DPM
ANALYZING
THE
FAILURES
OF
DPM
MOST
ERRORS
THAT
DETECTORS
MAKE
ARE
REASONABLE
LOCALIZATION
ERROR
AND
CONFUSION
WITH
SIMILAR
OBJECTS
MISDETECTION
OF
OCCLUDED
OR
SMALL
OBJECTS
DETECTORS
HAVE
DIFFERENT
SENSITIVITY
TO
DIFFERENT
FACTORS
E
G
LESS
SENSITIVE
TO
TRUNCATION
THAN
TO
SIZE
DIFFERENCES
FAILURE
ANALYSIS
CODE
AND
ANNOTATIONS
AVAILABLE
ONLINE
OTHER
OBJECTS
SIMILAR
OBJECTS
BIRD
BOAT
CAR
BACKGROUND
OTHER
OBJECTS
ADDITIONAL
ANNOTATIONS
FOR
SEVEN
CATEGORIES
OCCLUSION
LEVEL
PARTS
VISIBLE
SIDES
VISIBLE
OCCLUSION
POOR
ROBUSTNESS
TO
OCCLUSION
BUT
LITTLE
IMPACT
ON
OVERALL
PERFORMANCE
EASIER
NONE
HARDER
HEAVY
SIZE
STRONG
PREFERENCE
FOR
AVERAGE
TO
ABOVE
AVERAGE
SIZED
AIRPLANES
LARGE
MEDIUM
X
LARGE
SMALL
X
SMALL
ASPECT
RATIO
BETTER
AT
DETECTING
WIDE
SIDE
VIEWS
THAN
TALL
VIEWS
X
WIDE
WIDE
MEDIUM
X
TALL
TALL
EASIER
WIDE
HARDER
TALL
SIDES
PARTS
BEST
PERFORMANCE
DIRECT
SIDE
VIEW
WITH
ALL
PARTS
VISIBLE
EASIER
SIDE
HARDER
NON
SIDE
DETECTION
IN
OBJECT
DETECTION
SYSTEM
OVERVIEW
OUR
SYSTEM
TAKES
AN
INPUT
IMAGE
EXTRACTS
AROUND
BOTTOM
UP
REGION
PROPOSALS
COMPUTES
FEATURES
FOR
EACH
PROPOSAL
USING
A
LARGE
CONVOLUTIONAL
NEURAL
NETWORK
CNN
AND
THEN
CLASSIFIES
EACH
REGION
USING
CLASS
SPECIFIC
LINEAR
SVMS
R
CNN
ACHIEVES
A
MEAN
AVERAGE
PRECISION
MAP
OF
ON
PASCAL
VOC
FOR
COMPARISON
UIJLINGS
ET
AL
REPORT
MAP
USING
THE
SAME
REGION
PROPOSALS
BUT
WITH
A
SPATIAL
PYRAMID
AND
BAG
OF
VISUAL
WORDS
APPROACH
THE
POPULAR
DEFORMABLE
PART
MODELS
PERFORM
AT
LANA
LAZEBNIK
SUMMARY
WINDOW
BASED
APPROACHES
ASSUME
OBJECT
APPEARS
IN
ROUGHLY
THE
SAME
CONFIGURATION
IN
DIFFERENT
IMAGES
LOOK
FOR
ALIGNMENT
WITH
A
GLOBAL
TEMPLATE
PART
BASED
METHODS
ALLOW
PARTS
TO
MOVE
SOMEWHAT
FROM
THEIR
USUAL
LOCATIONS
LOOK
FOR
GOOD
FITS
IN
APPEARANCE
FOR
BOTH
THE
GLOBAL
TEMPLATE
AND
THE
INDIVIDUAL
PART
TEMPLATES
SPEED
UP
BY
ONLY
SCORING
BOXES
THAT
LOOK
LIKE
ANY
OBJECT
MODELS
PREFER
THAT
OBJECTS
APPEAR
IN
CERTAIN
VIEWS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
ANNOUNCEMENTS
PLEASE
WATCH
THE
VIDEOS
I
SENT
YOU
IF
YOU
HAVEN
T
YET
THAT
YOUR
READING
WE
WON
T
BE
READY
TO
RELEASE
UNTIL
TOMORROW
NIGHT
SO
WE
RE
PUSHING
DEADLINE
UNTIL
THEN
THURSDAY
WILL
BE
DUE
AFTER
THANKSGIVING
WHY
CONVOLUTIONAL
NEURAL
NETWORKS
NEURAL
NETWORK
BASICS
ARCHITECTURE
BIOLOGICAL
INSPIRATION
LOSS
FUNCTIONS
OPTIMIZATION
TRAINING
WITH
BACKPROP
CNNS
SPECIAL
OPERATIONS
COMMON
ARCHITECTURES
UNDERSTANDING
CNNS
VISUALIZATION
SYNTHESIS
STYLE
TRANSFER
BREAKING
CNNS
PRACTICAL
MATTERS
TIPS
AND
TRICKS
FOR
TRAINING
TRANSFER
LEARNING
SOFTWARE
PACKAGES
OBTAINED
STATE
OF
THE
ART
PERFORMANCE
ON
MANY
PROBLEMS
MOST
PAPERS
IN
CVPR
USE
DEEP
LEARNING
RAZAVIAN
ET
AL
CVPR
WORKSHOPS
DENG
ET
AL
CVPR
MILLION
LABELED
IMAGES
CLASSES
IMAGES
GATHERED
FROM
INTERNET
HUMAN
LABELS
VIA
AMAZON
TURK
CHALLENGE
MILLION
TRAINING
IMAGES
CLASSES
ALEXNET
SIMILAR
FRAMEWORK
TO
LECUN
BUT
BIGGER
MODEL
HIDDEN
LAYERS
UNITS
PARAMS
MORE
DATA
VS
IMAGES
GPU
IMPLEMENTATION
SPEEDUP
OVER
CPU
TRAINED
ON
TWO
GPUS
FOR
A
WEEK
BETTER
REGULARIZATION
FOR
TRAINING
DROPOUT
KRIZHEVSKY
ET
AL
ERROR
TOP
NEXT
BEST
NON
CONVNET
ERROR
OBJECT
DETECTION
SYSTEM
OVERVIEW
OUR
SYSTEM
TAKES
AN
INPUT
IMAGE
EXTRACTS
AROUND
BOTTOM
UP
REGION
PROPOSALS
COMPUTES
FEATURES
FOR
EACH
PROPOSAL
USING
A
LARGE
CONVOLUTIONAL
NEURAL
NETWORK
CNN
AND
THEN
CLASSIFIES
EACH
REGION
USING
CLASS
SPECIFIC
LINEAR
SVMS
R
CNN
ACHIEVES
A
MEAN
AVERAGE
PRECISION
MAP
OF
ON
PASCAL
VOC
FOR
COMPARISON
UIJLINGS
ET
AL
REPORT
MAP
USING
THE
SAME
REGION
PROPOSALS
BUT
WITH
A
SPATIAL
PYRAMID
AND
BAG
OF
VISUAL
WORDS
APPROACH
THE
POPULAR
DEFORMABLE
PART
MODELS
PERFORM
AT
FASTER
FASTER
BETTER
USING
VGG
CNN
ON
PASCAL
VOC
DATASET
DETECTION
SEGMENTATION
REGRESSION
POSE
ESTIMATION
SYNTHESIS
AND
MANY
MORE
CONVOLUTIONAL
NEURAL
NETWORKS
ARE
A
TYPE
OF
NEURAL
NETWORK
THE
NEURAL
NETWORK
INCLUDES
LAYERS
THAT
PERFORM
SPECIAL
OPERATIONS
USED
IN
VISION
BUT
TO
A
LESSER
EXTENT
ALSO
IN
NLP
BIOMEDICAL
ETC
OFTEN
THEY
ARE
DEEP
IMAGE
VIDEO
PIXELS
OBJECT
CLASS
FEATURES
ARE
KEY
TO
RECENT
PROGRESS
IN
RECOGNITION
BUT
RESEARCH
SHOWS
THEY
RE
FLAWED
WHERE
NEXT
BETTER
CLASSIFIERS
OR
KEEP
BUILDING
MORE
FEATURES
WHAT
ABOUT
LEARNING
THE
FEATURES
LEARN
A
FEATURE
HIERARCHY
ALL
THE
WAY
FROM
PIXELS
TO
CLASSIFIER
EACH
LAYER
EXTRACTS
FEATURES
FROM
THE
OUTPUT
OF
PREVIOUS
LAYER
TRAIN
ALL
LAYERS
JOINTLY
IMAGE
VIDEO
PIXELS
SIMPLE
CLASSIFIER
SHALLOW
VS
DEEP
ARCHITECTURES
TRADITIONAL
RECOGNITION
SHALLOW
ARCHITECTURE
IMAGE
VIDEO
PIXELS
OBJECT
CLASS
IMAGE
VIDEO
PIXELS
DEEP
LEARNING
DEEP
ARCHITECTURE
OBJECT
CLASS
ACTIVATIONS
NONLINEAR
ACTIVATION
FUNCTION
H
E
G
SIGMOID
TANH
LAYER
FINAL
OUTPUTS
BINARY
FINALLY
MULTICLASS
BINARY
SIGMOID
TANH
TANH
X
MAXOUT
ELU
LEAKY
RELU
MAX
X
RELU
MAX
X
NONLINEAR
CLASSIFIER
CAN
APPROXIMATE
ANY
CONTINUOUS
FUNCTION
TO
ARBITRARY
ACCURACY
GIVEN
SUFFICIENTLY
MANY
HIDDEN
UNITS
NEURONS
ACCEPT
INFORMATION
FROM
MULTIPLE
INPUTS
TRANSMIT
INFORMATION
TO
OTHER
NEURONS
MULTIPLY
INPUTS
BY
WEIGHTS
ALONG
EDGES
APPLY
SOME
FUNCTION
TO
THE
SET
OF
INPUTS
AT
EACH
NODE
IF
OUTPUT
OF
FUNCTION
OVER
THRESHOLD
NEURON
FIRES
INPUT
WEIGHTS
D
LINEAR
NEURON
LOGISTIC
NEURON
PERCEPTRON
Θ
DENOTES
THE
SAME
AS
W
CASCADE
NEURONS
TOGETHER
OUTPUT
FROM
ONE
LAYER
IS
THE
INPUT
TO
THE
NEXT
EACH
LAYER
HAS
ITS
OWN
SETS
OF
WEIGHTS
PREDICTIONS
ARE
FED
FORWARD
THROUGH
THE
NETWORK
TO
CLASSIFY
PREDICTIONS
ARE
FED
FORWARD
THROUGH
THE
NETWORK
TO
CLASSIFY
PREDICTIONS
ARE
FED
FORWARD
THROUGH
THE
NETWORK
TO
CLASSIFY
PREDICTIONS
ARE
FED
FORWARD
THROUGH
THE
NETWORK
TO
CLASSIFY
PREDICTIONS
ARE
FED
FORWARD
THROUGH
THE
NETWORK
TO
CLASSIFY
PREDICTIONS
ARE
FED
FORWARD
THROUGH
THE
NETWORK
TO
CLASSIFY
LOTS
OF
HIDDEN
LAYERS
DEPTH
POWER
USUALLY
IT
INVOLVES
COMPUTING
GRADIENTS
BACKPROPAGATION
BACKPROP
PROPAGATES
ERROR
FROM
OUTPUT
LAYER
TO
INTERMEDIATE
LAYERS
GOAL
IS
TO
ITERATIVELY
FIND
SUCH
A
SET
OF
WEIGHTS
THAT
ALLOW
THE
ACTIVATIONS
TO
MATCH
THE
DESIRED
OUTPUT
TRAINED
WITH
STOCHASTIC
GRADIENT
DESCENT
WE
WANT
TO
MINIMIZE
A
LOSS
FUNCTION
EXAMPLE
DATASET
CIFAR
LABELS
TRAINING
IMAGES
EACH
IMAGE
IS
TEST
IMAGES
F
X
W
NUMBERS
INDICATING
CLASS
SCORES
ARRAY
OF
NUMBERS
NUMBERS
TOTAL
SVM
DECISION
SIGN
WTX
SIGN
WHAT
SHOULD
THE
WEIGHTS
BE
WE
WANT
TO
TRAIN
A
SPAM
OR
NOT
CLASSIFIER
FEATURES
ARE
HOW
MANY
TIMES
EACH
WORD
OCCURS
LET
SAY
YOU
ONLY
HAVE
WORDS
IN
YOUR
VOCABULARY
CASH
BANK
HOMEWORK
BEER
BOOK
SAY
I
WANT
A
HIGH
SCORE
FOR
SPAM
AND
LOW
SCORE
FOR
NOT
SPAM
WHAT
WOULD
THE
WEIGHTS
BE
ON
EACH
ARRAY
OF
NUMBERS
NUMBERS
INDICATING
CLASS
SCORES
PARAMETERS
OR
WEIGHTS
EXAMPLE
WITH
AN
IMAGE
WITH
PIXELS
AND
CLASSES
CAT
DOG
SHIP
GOING
FORWARD
LOSS
FUNCTION
OPTIMIZATION
TODO
DEFINE
A
LOSS
FUNCTION
THAT
QUANTIFIES
OUR
UNHAPPINESS
WITH
THE
SCORES
ACROSS
THE
TRAINING
DATA
COME
UP
WITH
A
WAY
OF
EFFICIENTLY
FINDING
THE
PARAMETERS
THAT
MINIMIZE
THE
LOSS
FUNCTION
OPTIMIZATION
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
CAT
CAR
FROG
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
CAT
CAR
FROG
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
MULTICLASS
SVM
LOSS
GIVEN
AN
EXAMPLE
WHERE
WHERE
IS
THE
IMAGE
AND
IS
THE
INTEGER
LABEL
AND
USING
THE
SHORTHAND
FOR
THE
SCORES
VECTOR
CAT
CAR
FROG
THE
SVM
LOSS
HAS
THE
FORM
AND
THE
FULL
TRAINING
LOSS
IS
THE
MEAN
OVER
ALL
EXAMPLES
IN
THE
TRAINING
DATA
L
LOSSES
WEIGHT
REGULARIZATION
Λ
REGULARIZATION
STRENGTH
HYPERPARAMETER
IN
COMMON
USE
REGULARIZATION
REGULARIZATION
DROPOUT
WILL
SEE
LATER
SCORES
UNNORMALIZED
LOG
PROBABILITIES
OF
THE
CLASSES
WHERE
CAT
CAR
FROG
WANT
TO
MAXIMIZE
THE
LOG
LIKELIHOOD
OR
FOR
A
LOSS
FUNCTION
TO
MINIMIZE
THE
NEGATIVE
LOG
LIKELIHOOD
OF
THE
CORRECT
CLASS
CAT
CAR
FROG
UNNORMALIZED
PROBABILITIES
EXP
NORMALIZE
LOG
UNNORMALIZED
LOG
PROBABILITIES
PROBABILITIES
IN
DIMENSION
THE
DERIVATIVE
OF
A
FUNCTION
IN
MULTIPLE
DIMENSIONS
THE
GRADIENT
IS
THE
VECTOR
OF
PARTIAL
DERIVATIVES
CURRENT
W
LOSS
GRADIENT
DW
CURRENT
W
LOSS
W
H
FIRST
DIM
LOSS
GRADIENT
DW
CURRENT
W
LOSS
W
H
FIRST
DIM
LOSS
GRADIENT
DW
CURRENT
W
LOSS
W
H
SECOND
DIM
LOSS
GRADIENT
DW
CURRENT
W
LOSS
W
H
SECOND
DIM
LOSS
GRADIENT
DW
CURRENT
W
LOSS
W
H
THIRD
DIM
LOSS
GRADIENT
DW
WANT
CURRENT
W
LOSS
DW
SOME
FUNCTION
DATA
AND
W
GRADIENT
DW
NEGATIVE
GRADIENT
DIRECTION
ORIGINAL
W
WE
LL
UPDATE
WEIGHTS
MOVE
IN
DIRECTION
OPPOSITE
TO
GRADIENT
TIME
L
LEARNING
RATE
USE
GRADIENT
DESCENT
ITERATIVELY
SUBTRACT
THE
GRADIENT
WITH
RESPECT
TO
THE
MODEL
PARAMETERS
W
I
E
WE
RE
MOVING
IN
A
DIRECTION
OPPOSITE
TO
THE
GRADIENT
OF
THE
LOSS
I
E
WE
RE
MOVING
TOWARDS
SMALLER
LOSS
ANDREJ
KARPATHY
THE
EFFECTS
OF
STEP
SIZE
OR
LEARNING
RATE
IN
CLASSIC
GRADIENT
DESCENT
WE
COMPUTE
THE
GRADIENT
FROM
THE
LOSS
FOR
ALL
TRAINING
EXAMPLES
COULD
ALSO
ONLY
USE
SOME
OF
THE
DATA
FOR
EACH
GRADIENT
UPDATE
THEN
CYCLE
THROUGH
ALL
TRAINING
SAMPLES
ALLOWS
FASTER
TRAINING
E
G
ON
GPUS
PARALLELIZATION
WE
LL
UPDATE
WEIGHTS
MOVE
IN
DIRECTION
OPPOSITE
TO
GRADIENT
HOW
TO
UPDATE
THE
WEIGHTS
AT
ALL
LAYERS
ANSWER
BACKPROPAGATION
OF
ERROR
FROM
HIGHER
LAYERS
TO
LOWER
LAYERS
USING
SIGMOID
ACTIVATIONS
TWO
LAYER
NET
INITIALIZE
ALL
WEIGHTS
TO
SMALL
RANDOM
VALUES
UNTIL
CONVERGENCE
ERROR
STOPS
DECREASING
REPEAT
FOR
EACH
X
T
CLASS
X
IN
TRAINING
SET
CALCULATE
NETWORK
OUTPUTS
YK
COMPUTE
ERRORS
GRADIENTS
WRT
ACTIVATIONS
FOR
EACH
UNIT
ΔK
YK
YK
TK
YK
FOR
OUTPUT
UNITS
ΔJ
YK
YK
K
WKJ
ΔK
FOR
HIDDEN
UNITS
UPDATE
WEIGHTS
WKJ
WKJ
Η
ΔK
ZJ
FOR
OUTPUT
UNITS
WJI
WJI
Η
ΔJ
XI
FOR
HIDDEN
UNITS
ADAPTED
FROM
REBECCA
HWA
AND
RAY
MOONEY
BACKPROPAGATION
ACTIVATIONS
LOCAL
GRADIENT
F
GRADIENTS
ANDREJ
KARPATHY
ENTIRE
LECTURE
ON
BACKPROPAGATION
KARPATHY
LECTURE
SECOND
VIDEO
I
SENT
YOU
NOT
GUARANTEED
TO
CONVERGE
TO
ZERO
TRAINING
ERROR
MAY
CONVERGE
TO
LOCAL
OPTIMA
OR
OSCILLATE
INDEFINITELY
HOWEVER
IN
PRACTICE
DOES
CONVERGE
TO
LOW
ERROR
FOR
MANY
LARGE
NETWORKS
ON
REAL
DATA
THOUSANDS
OF
EPOCHS
EPOCH
NETWORK
SEES
ALL
TRAINING
DATA
ONCE
MAY
BE
REQUIRED
HOURS
OR
DAYS
TO
TRAIN
TO
AVOID
LOCAL
MINIMA
PROBLEMS
RUN
SEVERAL
TRIALS
STARTING
WITH
DIFFERENT
RANDOM
WEIGHTS
RANDOM
RESTARTS
AND
TAKE
RESULTS
OF
TRIAL
WITH
LOWEST
TRAINING
SET
ERROR
MAY
BE
HARD
TO
SET
LEARNING
RATE
AND
TO
SELECT
NUMBER
OF
HIDDEN
UNITS
AND
LAYERS
NEURAL
NETWORKS
HAD
FALLEN
OUT
OF
FASHION
IN
EARLY
BACK
WITH
A
NEW
NAME
AND
SIGNIFICANTLY
IMPROVED
PERFORMANCE
DEEP
NETWORKS
TRAINED
WITH
DROPOUT
AND
LOTS
OF
DATA
RUNNING
TOO
MANY
EPOCHS
CAN
RESULT
IN
OVER
FITTING
ON
TEST
DATA
TRAINING
EPOCHS
ON
TRAINING
DATA
KEEP
A
HOLD
OUT
VALIDATION
SET
AND
TEST
ACCURACY
ON
IT
AFTER
EVERY
EPOCH
STOP
TRAINING
WHEN
ADDITIONAL
EPOCHS
ACTUALLY
INCREASE
VALIDATION
ERROR
TOO
FEW
HIDDEN
UNITS
PREVENTS
THE
NETWORK
FROM
ADEQUATELY
FITTING
THE
DATA
TOO
MANY
HIDDEN
UNITS
CAN
RESULT
IN
OVER
FITTING
ON
TEST
DATA
HIDDEN
UNITS
ON
TRAINING
DATA
USE
INTERNAL
CROSS
VALIDATION
TO
EMPIRICALLY
DETERMINE
AN
OPTIMAL
NUMBER
OF
HIDDEN
UNITS
MORE
NEURONS
MORE
CAPACITY
DO
NOT
USE
SIZE
OF
NEURAL
NETWORK
AS
A
REGULARIZER
USE
STRONGER
REGULARIZATION
INSTEAD
YOU
CAN
PLAY
WITH
THIS
DEMO
OVER
AT
CONVNETJS
TRAINED
HIDDEN
UNITS
CAN
BE
SEEN
AS
NEWLY
CONSTRUCTED
FEATURES
THAT
MAKE
THE
TARGET
CONCEPT
LINEARLY
SEPARABLE
IN
THE
TRANSFORMED
SPACE
ON
MANY
REAL
DOMAINS
HIDDEN
UNITS
CAN
BE
INTERPRETED
AS
REPRESENTING
MEANINGFUL
FEATURES
SUCH
AS
VOWEL
DETECTORS
OR
EDGE
DETECTORS
ETC
HOWEVER
THE
HIDDEN
LAYER
CAN
ALSO
BECOME
A
DISTRIBUTED
REPRESENTATION
OF
THE
INPUT
IN
WHICH
EACH
INDIVIDUAL
UNIT
IS
NOT
EASILY
INTERPRETABLE
AS
A
MEANINGFUL
FEATURE
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
WHY
CONVOLUTIONAL
NEURAL
NETWORKS
NEURAL
NETWORK
BASICS
ARCHITECTURE
BIOLOGICAL
INSPIRATION
LOSS
FUNCTIONS
OPTIMIZATION
TRAINING
WITH
BACKPROP
CNNS
SPECIAL
OPERATIONS
COMMON
ARCHITECTURES
UNDERSTANDING
CNNS
VISUALIZATION
BREAKING
CNNS
SYNTHESIS
STYLE
TRANSFER
PRACTICAL
MATTERS
TIPS
AND
TRICKS
FOR
TRAINING
TRANSFER
LEARNING
SOFTWARE
PACKAGES
A
BIOLOGICAL
NEURON
AN
ARTIFICIAL
NEURON
HUBEL
AND
WEISEL
ARCHITECTURE
MULTI
LAYER
NEURAL
NETWORK
NEURAL
NETWORK
WITH
SPECIALIZED
CONNECTIVITY
STRUCTURE
STACK
MULTIPLE
STAGES
OF
FEATURE
EXTRACTORS
HIGHER
STAGES
COMPUTE
MORE
GLOBAL
MORE
INVARIANT
MORE
ABSTRACT
FEATURES
CLASSIFICATION
LAYER
AT
THE
END
ADAPTED
FROM
ROB
FERGUS
FEED
FORWARD
FEATURE
EXTRACTION
CONVOLVE
INPUT
WITH
LEARNED
FILTERS
APPLY
NON
LINEARITY
SPATIAL
POOLING
DOWNSAMPLE
SUPERVISED
TRAINING
OF
CONVOLUTIONAL
FILTERS
BY
BACK
PROPAGATING
CLASSIFICATION
ERROR
ADAPTED
FROM
LANA
LAZEBNIK
APPLY
LEARNED
FILTER
WEIGHTS
ONE
FEATURE
MAP
PER
FILTER
STRIDE
CAN
BE
GREATER
THAN
FASTER
LESS
MEMORY
NON
LINEARITY
PER
ELEMENT
INDEPENDENT
OPTIONS
TANH
SIGMOID
EXP
X
RECTIFIED
LINEAR
UNIT
RELU
AVOIDS
SATURATION
ISSUES
SUM
OR
MAX
OVER
NON
OVERLAPPING
OVERLAPPING
REGIONS
ROLE
OF
POOLING
INVARIANCE
TO
SMALL
TRANSFORMATIONS
LARGER
RECEPTIVE
FIELDS
NEURONS
SEE
MORE
OF
INPUT
MAX
SUM
SUM
OR
MAX
OVER
NON
OVERLAPPING
OVERLAPPING
REGIONS
ROLE
OF
POOLING
INVARIANCE
TO
SMALL
TRANSFORMATIONS
LARGER
RECEPTIVE
FIELDS
NEURONS
SEE
MORE
OF
INPUT
IMAGE
HEIGHT
DEPTH
WIDTH
IMAGE
FILTER
CONVOLVE
THE
FILTER
WITH
THE
IMAGE
I
E
SLIDE
OVER
THE
IMAGE
SPATIALLY
COMPUTING
DOT
PRODUCTS
CONVOLUTION
LAYER
IMAGE
FILTER
NUMBER
THE
RESULT
OF
TAKING
A
DOT
PRODUCT
BETWEEN
THE
FILTER
AND
A
SMALL
CHUNK
OF
THE
IMAGE
I
E
DIMENSIONAL
DOT
PRODUCT
BIAS
CONVOLUTION
LAYER
IMAGE
FILTER
ACTIVATION
MAP
CONVOLVE
SLIDE
OVER
ALL
SPATIAL
LOCATIONS
CONVOLUTION
LAYER
CONSIDER
A
SECOND
GREEN
FILTER
IMAGE
FILTER
ACTIVATION
MAPS
CONVOLVE
SLIDE
OVER
ALL
SPATIAL
LOCATIONS
FOR
EXAMPLE
IF
WE
HAD
FILTERS
WE
LL
GET
SEPARATE
ACTIVATION
MAPS
ACTIVATION
MAPS
CONVOLUTION
LAYER
WE
STACK
THESE
UP
TO
GET
A
NEW
IMAGE
OF
SIZE
PREVIEW
CONVNET
IS
A
SEQUENCE
OF
CONVOLUTION
LAYERS
INTERSPERSED
WITH
ACTIVATION
FUNCTIONS
CONV
RELU
E
G
FILTERS
PREVIEW
CONVNET
IS
A
SEQUENCE
OF
CONVOLUTIONAL
LAYERS
INTERSPERSED
WITH
ACTIVATION
FUNCTIONS
CONV
RELU
E
G
FILTERS
CONV
RELU
E
G
FILTERS
CONV
RELU
FROM
RECENT
YANN
LECUN
SLIDES
ONE
FILTER
ONE
ACTIVATION
MAP
EXAMPLE
FILTERS
TOTAL
WE
CALL
THE
LAYER
CONVOLUTIONAL
BECAUSE
IT
IS
RELATED
TO
CONVOLUTION
OF
TWO
SIGNALS
ELEMENTWISE
MULTIPLICATION
AND
SUM
OF
A
FILTER
AND
THE
SIGNAL
IMAGE
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
IMAGE
FILTER
ACTIVATION
MAP
CONVOLVE
SLIDE
OVER
ALL
SPATIAL
LOCATIONS
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
OUTPUT
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
APPLIED
WITH
STRIDE
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
APPLIED
WITH
STRIDE
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
APPLIED
WITH
STRIDE
OUTPUT
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
APPLIED
WITH
STRIDE
A
CLOSER
LOOK
AT
SPATIAL
DIMENSIONS
INPUT
SPATIALLY
ASSUME
FILTER
APPLIED
WITH
STRIDE
DOESN
T
FIT
CANNOT
APPLY
FILTER
ON
INPUT
WITH
STRIDE
CONVOLUTIONS
MORE
DETAIL
N
OUTPUT
SIZE
N
F
STRIDE
N
E
G
N
F
STRIDE
STRIDE
STRIDE
IN
PRACTICE
COMMON
TO
ZERO
PAD
THE
BORDER
E
G
INPUT
FILTER
APPLIED
WITH
STRIDE
PAD
WITH
PIXEL
BORDER
WHAT
IS
THE
OUTPUT
RECALL
N
F
STRIDE
IN
PRACTICE
COMMON
TO
ZERO
PAD
THE
BORDER
E
G
INPUT
FILTER
APPLIED
WITH
STRIDE
PAD
WITH
PIXEL
BORDER
WHAT
IS
THE
OUTPUT
OUTPUT
IN
PRACTICE
COMMON
TO
ZERO
PAD
THE
BORDER
E
G
INPUT
FILTER
APPLIED
WITH
STRIDE
PAD
WITH
PIXEL
BORDER
WHAT
IS
THE
OUTPUT
OUTPUT
IN
GENERAL
COMMON
TO
SEE
CONV
LAYERS
WITH
STRIDE
FILTERS
OF
SIZE
FXF
AND
ZERO
PADDING
WITH
F
WILL
PRESERVE
SIZE
SPATIALLY
E
G
F
ZERO
PAD
WITH
F
ZERO
PAD
WITH
F
ZERO
PAD
WITH
EXAMPLES
TIME
INPUT
VOLUME
FILTERS
WITH
STRIDE
PAD
OUTPUT
VOLUME
SIZE
EXAMPLES
TIME
INPUT
VOLUME
FILTERS
WITH
STRIDE
PAD
OUTPUT
VOLUME
SIZE
SPATIALLY
SO
EXAMPLES
TIME
INPUT
VOLUME
FILTERS
WITH
STRIDE
PAD
NUMBER
OF
PARAMETERS
IN
THIS
LAYER
CONVOLUTIONS
MORE
DETAIL
EXAMPLES
TIME
INPUT
VOLUME
FILTERS
WITH
STRIDE
PAD
NUMBER
OF
PARAMETERS
IN
THIS
LAYER
EACH
FILTER
HAS
PARAMS
FOR
BIAS
CONVOLUTIONS
MORE
DETAIL
PREVIEW
ZEILER
AND
FERGUS
ALEXNET
BUT
CHANGE
FROM
STRIDE
TO
STRIDE
INSTEAD
OF
FILTERS
USE
IMAGENET
TOP
ERROR
SIMONYAN
AND
ZISSERMAN
ONLY
CONV
STRIDE
PAD
AND
MAX
POOL
STRIDE
BEST
MODEL
TOP
ERROR
IN
ILSVRC
TOP
ERROR
SZEGEDY
ET
AL
INCEPTION
MODULE
ILSVRC
WINNER
TOP
ERROR
HE
ET
AL
ILSVRC
WINNER
TOP
ERROR
SLIDE
FROM
KAIMING
HE
RECENT
PRESENTATION
SLIDE
FROM
KAIMING
HE
RECENT
PRESENTATION
HE
ET
AL
ILSVRC
WINNER
TOP
ERROR
WEEKS
OF
TRAINING
ON
GPU
MACHINE
AT
RUNTIME
FASTER
THAN
A
VGGNET
EVEN
THOUGH
IT
HAS
MORE
LAYERS
SLIDE
FROM
KAIMING
HE
RECENT
PRESENTATION
VISUALIZATION
WHAT
ARE
THESE
CNNS
LEARNING
HUBEL
AND
WEISEL
ARCHITECTURE
MULTI
LAYER
NEURAL
NETWORK
ADAPTED
FROM
JIA
BIN
HUANG
PATCHES
FROM
VALIDATION
IMAGES
THAT
GIVE
MAXIMAL
ACTIVATION
OF
A
GIVEN
FEATURE
MAP
LAYER
LAYER
VISUALIZING
AND
UNDERSTANDING
CONVOLUTIONAL
NETWORKS
LAYER
VISUALIZING
AND
UNDERSTANDING
CONVOLUTIONAL
NETWORKS
ZEILER
FERGUS
AS
A
FUNCTION
OF
THE
POSITION
OF
THE
SQUARE
OF
ZEROS
IN
THE
ORIGINAL
IMAGE
ZEILER
FERGUS
AS
A
FUNCTION
OF
THE
POSITION
OF
THE
SQUARE
OF
ZEROS
IN
THE
ORIGINAL
IMAGE
REPEAT
FORWARD
AN
IMAGE
SET
ACTIVATIONS
IN
LAYER
OF
INTEREST
TO
ALL
ZERO
EXCEPT
FOR
A
FOR
A
NEURON
OF
INTEREST
BACKPROP
TO
IMAGE
DO
AN
IMAGE
UPDATE
UNDERSTANDING
NEURAL
NETWORKS
THROUGH
DEEP
VISUALIZATION
YOSINSKI
ET
AL
INTRIGUING
PROPERTIES
OF
NEURAL
NETWORKS
JIA
BIN
HUANG
DEEP
NEURAL
NETWORKS
ARE
EASILY
FOOLED
HIGH
CONFIDENCE
PREDICTIONS
FOR
UNRECOGNIZABLE
IMAGES
FOOLING
A
LINEAR
CLASSIFIER
TO
FOOL
A
LINEAR
CLASSIFIER
ADD
A
SMALL
MULTIPLE
OF
THE
WEIGHT
VECTOR
TO
THE
TRAINING
EXAMPLE
X
X
ΑW
JIA
BIN
HUANG
QUESTION
GIVEN
A
CNN
CODE
IS
IT
POSSIBLE
TO
RECONSTRUCT
THE
ORIGINAL
IMAGE
FIND
AN
IMAGE
SUCH
THAT
ITS
CODE
IS
SIMILAR
TO
A
GIVEN
CODE
IT
LOOKS
NATURAL
IMAGE
PRIOR
REGULARIZATION
UNDERSTANDING
DEEP
IMAGE
REPRESENTATIONS
BY
INVERTING
THEM
MAHENDRAN
AND
VEDALDI
ORIGINAL
IMAGE
RECONSTRUCTIONS
FROM
THE
LOG
PROBABILITIES
FOR
IMAGENET
ILSVRC
CLASSES
RECONSTRUCTIONS
FROM
THE
REPRESENTATION
AFTER
LAST
LAST
POOLING
LAYER
IMMEDIATELY
BEFORE
THE
FIRST
FULLY
CONNECTED
LAYER
DEEPDREAM
MORE
INFO
STANFORD
LECTURE
DEEPDREAM
MODIFIES
THE
IMAGE
IN
A
WAY
THAT
BOOSTS
ALL
ACTIVATIONS
AT
ANY
LAYER
THIS
CREATES
A
FEEDBACK
LOOP
E
G
ANY
SLIGHTLY
DETECTED
DOG
FACE
WILL
BE
MADE
MORE
AND
MORE
DOG
LIKE
OVER
TIME
DEEP
DREAM
GROCERY
TRIP
DEEP
DREAMING
FEAR
LOATHING
IN
LAS
VEGAS
THE
GREAT
SAN
FRANCISCO
ACID
WAVE
SYNTHESIS
STYLE
TRANSFER
A
NEURAL
ALGORITHM
OF
ARTISTIC
STYLE
BY
LEON
A
GATYS
ALEXANDER
ECKER
AND
MATTHIAS
BETHGE
GOOD
IMPLEMENTATION
BY
JUSTIN
JOHNSON
IN
TORCH
H
MAKE
YOUR
OWN
EASILY
ON
DEEPART
IO
STEP
EXTRACT
CONTENT
TARGETS
CONVNET
ACTIVATIONS
OF
ALL
LAYERS
FOR
THE
GIVEN
CONTENT
IMAGE
CONTENT
ACTIVATIONS
E
G
AT
LAYER
WE
WOULD
HAVE
A
ARRAY
OF
TARGET
ACTIVATIONS
STEP
EXTRACT
STYLE
TARGETS
GRAM
MATRICES
OF
CONVNET
ACTIVATIONS
OF
ALL
LAYERS
FOR
THE
GIVEN
STYLE
IMAGE
STYLE
GRAM
MATRICES
E
G
AT
LAYER
WITH
ACTIVATIONS
WOULD
GIVE
A
GRAM
MATRIX
OF
ALL
PAIRWISE
ACTIVATION
COVARIANCES
SUMMED
ACROSS
SPATIAL
LOCATIONS
MORE
INFO
STANFORD
LECTURE
STEP
OPTIMIZE
OVER
IMAGE
TO
HAVE
THE
CONTENT
OF
THE
CONTENT
IMAGE
ACTIVATIONS
MATCH
CONTENT
THE
STYLE
OF
THE
STYLE
IMAGE
GRAM
MATRICES
OF
ACTIVATIONS
MATCH
STYLE
TOTAL
VARIATION
REGULARIZATION
MAYBE
MATCH
CONTENT
MATCH
STYLE
PRACTICAL
MATTERS
USE
MINI
BATCH
USE
REGULARIZATION
USE
GRADIENT
CHECKS
USE
CROSS
VALIDATION
FOR
YOUR
PARAMETERS
USE
RELU
OR
LEAKY
RELU
OR
ELU
DON
T
USE
SIGMOID
CENTER
SUBTRACT
MEAN
FROM
YOUR
DATA
TO
INITIALIZE
USE
XAVIER
INITIALIZATION
LEARNING
RATE
TOO
HIGH
TOO
LOW
RANDOMLY
TURN
OFF
SOME
NEURONS
ALLOWS
INDIVIDUAL
NEURONS
TO
INDEPENDENTLY
BE
RESPONSIBLE
FOR
PERFORMANCE
DROPOUT
A
SIMPLE
WAY
TO
PREVENT
NEURAL
NETWORKS
FROM
OVERFITTING
ADAPTED
FROM
JIA
BIN
HUANG
CREATE
VIRTUAL
TRAINING
SAMPLES
HORIZONTAL
FLIP
RANDOM
CROP
COLOR
CASTING
GEOMETRIC
DISTORTION
JIA
BIN
HUANG
DEEP
IMAGE
TRANSFER
LEARNING
YOU
NEED
A
LOT
OF
A
DATA
IF
YOU
WANT
TO
TRAIN
USE
CNNS
TRAIN
ON
IMAGENET
SMALL
DATASET
FEATURE
EXTRACTOR
FREEZE
THESE
TRAIN
THIS
MEDIUM
DATASET
FINETUNING
MORE
DATA
RETRAIN
MORE
OF
THE
NETWORK
OR
ALL
OF
IT
FREEZE
THESE
TRAIN
THIS
GENERIC
SPECIFIC
SIMPLEST
WAY
TO
USE
CNNS
TAKE
MODEL
TRAINED
ON
E
G
IMAGENET
TRAINING
SET
EASIEST
TAKE
OUTPUTS
OF
E
G
OR
FULLY
CONNECTED
LAYER
AND
PLUG
FEATURES
FROM
EACH
LAYER
INTO
LINEAR
SVM
FEATURES
ARE
NEURON
ACTIVATIONS
AT
THAT
LEVEL
CAN
TRAIN
LINEAR
SVM
FOR
DIFFERENT
TASKS
NOT
JUST
ONE
USED
TO
LEARN
THE
DEEP
NET
BETTER
FINE
TUNE
FEATURES
AND
OR
CLASSIFIER
ON
NEW
DATASET
CLASSIFY
TEST
SET
OF
NEW
DATASET
TRANSFER
LEARNING
WITH
CNNS
IS
PERVASIVE
EXTRACT
PATCH
RUN
THROUGH
A
CNN
CLASSIFY
CENTER
PIXEL
COW
REPEAT
FOR
EVERY
PIXEL
WHO
TOOK
THIS
PHOTOGRAPH
DEEP
NET
FEATURES
ACHIEVE
ACCURACY
CHANCE
IS
LESS
THAN
HUMAN
PERFORMANCE
IS
METHOD
LEARNS
WHAT
PROTO
OBJECTS
SCENES
AUTHORS
SHOOT
THOMAS
AND
KOVASHKA
CVPR
DEA
D
P
LNO
T
O
HERS
WORK
X
HTT
PS
T
HEST
AD
CO
WORL
D
N
EW
P
HOTOGRAPH
FROM
DEA
C
I
I
I
I
UNIVERSITY
OF
PITTSBURGH
EMTLER
AT
PITT
COMPUTE
R
SCIENTISTS
HAVE
RESURRECTED
FA
MED
PHOTOGRAPHE
RS
WORLD
NEW
PH
OTOGRAPHS
FROM
DEAD
PHOTOGR
APHERS
WITH
CONVOLUTI
ONAL
AL
NETWORKS
MARTIN
ANDERSON
THU
NOV
RE
SEARCH
ERS
OUT
OF
THE
UNIVERSITY
OF
PITTSBURGH
HAVE
USED
CON
V
O
L
URTI
ON
AL
NEURAL
NETW
ORKS
CNNS
TO
ID
ENTIFY
TH
E
ST
RAN
GE
OBSESSIONS
AND
UNIQUE
STYLES
OF
KNOWN
PHOTOGRAPHERS
AND
TO
GE
NE
RATE
NEW
PHOTOGRAPHS
WHIICIH
ACCORD
WITH
THEIR
URNIQ
UE
P
E
RSP
E
CTIV
ES
ON
TH
E
WORLD
AROUND
US
AND
WITH
KERAS
LASAGNE
OVERVIEW
NEUROSCIENCE
PERCEPTRON
MULTI
LAYER
NEURAL
NETWORKS
CONVOLUTIONAL
NEURAL
NETWORK
CNN
CONVOLUTION
NONLINEARITY
MAX
POOLING
UNDERSTANDING
AND
VISUALIZING
CNN
FIND
IMAGES
THAT
MAXIMIZE
SOME
CLASS
SCORES
VISUALIZE
INDIVIDUAL
NEURON
ACTIVATION
AND
INPUT
PATTERNS
BREAKING
CNNS
TRAINING
CNN
DROPOUT
DATA
AUGMENTATION
TRANSFER
LEARNING
USING
CNNS
FOR
YOUR
OWN
TASK
BASIC
FIRST
STEP
TRY
THE
PRE
TRAINED
CAFFENET
LAYERS
AS
FEATURES
ADAPTED
FROM
JIA
BIN
HUANG
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
DECEMBER
ANNOUNCEMENTS
NEXT
TIME
REVIEW
FOR
THE
FINAL
EXAM
BY
TUESDAY
AT
NOON
SEND
ME
THREE
TOPICS
YOU
WANT
ME
TO
REVIEW
FOR
PARTICIPATION
CREDIT
PLEASE
DO
OMETS
THANKS
GRADES
BEFORE
FINAL
SEE
COURSEWEB
OVERALL
COLUMN
I
WON
T
NEED
TO
CURVE
PLAN
FOR
TODAY
MOTIVATION
HISTORY
VISION
AND
LANGUAGE
IMAGE
CAPTIONING
TOOLS
RECURRENT
NEURAL
NETWORKS
RECENT
PROBLEM
VISUAL
QUESTION
ANSWERING
SOME
APPROACHES
VISION
AND
LANGUAGE
HUMANS
DON
T
USE
ONLY
THEIR
VISUAL
PROCESSING
ABILITIES
OR
SPEAKING
LISTENING
ABILITIES
IN
ISOLATION
THEY
USE
THEM
TOGETHER
WHILE
COMPUTER
VISION
AND
NATURAL
LANGUAGE
PROCESSING
ARE
SEPARATE
FIELDS
THERE
HAS
BEEN
INCREASED
INTEREST
IN
COMBINING
THEM
A
POPULAR
TASK
IS
IMAGE
CAPTIONING
GIVEN
AN
IMAGE
AUTOMATICALLY
GENERATE
A
CAPTION
FOR
THIS
IMAGE
THAT
AGREES
WELL
WITH
HUMAN
GENERATED
CAPTIONS
IT
WAS
AN
ARRESTING
FACE
POINTED
OF
CHIN
SQUARE
OF
JAW
HER
EYES
WERE
PALE
GREEN
WITHOUT
A
TOUCH
OF
HAZEL
STARRED
WITH
BRISTLY
BLACK
LASHES
AND
SLIGHTLY
TILTED
AT
THE
ENDS
ABOVE
THEM
HER
THICK
BLACK
BROWS
SLANTED
UPWARD
CUTTING
A
STARTLING
OBLIQUE
LINE
IN
HER
MAGNOLIA
WHITE
SKIN
THAT
SKIN
SO
PRIZED
BY
SOUTHERN
WOMEN
AND
SO
CAREFULLY
GUARDED
WITH
BONNETS
VEILS
AND
MITTENS
AGAINST
HOT
GEORGIA
SUNS
SCARLETT
O
HARA
DESCRIBED
IN
GONE
WITH
THE
WIND
MORE
NUANCE
THAN
TRADITIONAL
RECOGNITION
PERSON
CAR
SHOE
CAR
PINK
CAR
ATTRIBUTES
OF
OBJECTS
CAR
ON
ROAD
RELATIONSHIPS
BETWEEN
OBJECTS
LITTLE
PINK
SMART
CAR
PARKED
ON
THE
SIDE
OF
A
ROAD
IN
A
LONDON
SHOPPING
DISTRICT
COMPLEX
STRUCTURED
RECOGNITION
OUTPUTS
TELLING
THE
STORY
OF
AN
IMAGE
THIS
IS
A
PICTURE
OF
ONE
SKY
ONE
ROAD
AND
ONE
SHEEP
THE
GRAY
SKY
IS
OVER
THE
GRAY
ROAD
THE
GRAY
SHEEP
IS
BY
THE
GRAY
ROAD
HERE
WE
SEE
ONE
ROAD
ONE
SKY
AND
ONE
BICYCLE
THE
ROAD
IS
NEAR
THE
BLUE
SKY
AND
NEAR
THE
COLORFUL
BICYCLE
THE
COLORFUL
BICYCLE
IS
WITHIN
THE
BLUE
SKY
THIS
IS
A
PICTURE
OF
TWO
DOGS
THE
FIRST
DOG
IS
NEAR
THE
SECOND
FURRY
DOG
KULKARNI
ET
AL
MISSED
DETECTIONS
SOME
BAD
RESULTS
FALSE
DETECTIONS
INCORRECT
ATTRIBUTES
HERE
WE
SEE
ONE
POTTED
PLANT
THIS
IS
A
PICTURE
OF
ONE
DOG
THERE
ARE
ONE
ROAD
AND
ONE
CAT
THE
FURRY
ROAD
IS
IN
THE
FURRY
CAT
THIS
IS
A
PICTURE
OF
ONE
TREE
ONE
ROAD
AND
ONE
PERSON
THE
RUSTY
TREE
IS
UNDER
THE
RED
ROAD
THE
COLORFUL
PERSON
IS
NEAR
THE
RUSTY
TREE
AND
UNDER
THE
RED
ROAD
THIS
IS
A
PHOTOGRAPH
OF
TWO
SHEEPS
AND
ONE
GRASS
THE
FIRST
BLACK
SHEEP
IS
BY
THE
GREEN
GRASS
AND
BY
THE
SECOND
BLACK
SHEEP
THE
SECOND
BLACK
SHEEP
IS
BY
THE
GREEN
GRASS
THIS
IS
A
PHOTOGRAPH
OF
TWO
HORSES
AND
ONE
GRASS
THE
FIRST
FEATHERED
HORSE
IS
WITHIN
THE
GREEN
GRASS
AND
BY
THE
SECOND
FEATHERED
HORSE
THE
SECOND
FEATHERED
HORSE
IS
WITHIN
THE
GREEN
GRASS
KULKARNI
ET
AL
RESULTS
WITH
RECURRENT
NEURAL
NETWORKS
VANILLA
NEURAL
NETWORKS
E
G
IMAGE
CAPTIONING
IMAGE
SEQUENCE
OF
WORDS
E
G
SENTIMENT
CLASSIFICATION
SEQUENCE
OF
WORDS
SENTIMENT
E
G
MACHINE
TRANSLATION
SEQ
OF
WORDS
SEQ
OF
WORDS
E
G
VIDEO
CLASSIFICATION
ON
FRAME
LEVEL
USUALLY
WANT
TO
OUTPUT
A
PREDICTION
AT
SOME
TIME
STEPS
WE
CAN
PROCESS
A
SEQUENCE
OF
VECTORS
X
BY
APPLYING
A
RECURRENCE
FORMULA
AT
EVERY
TIME
STEP
NEW
STATE
OLD
STATE
INPUT
VECTOR
AT
SOME
TIME
STEP
SOME
FUNCTION
WITH
PARAMETERS
W
WE
CAN
PROCESS
A
SEQUENCE
OF
VECTORS
X
BY
APPLYING
A
RECURRENCE
FORMULA
AT
EVERY
TIME
STEP
NOTICE
THE
SAME
FUNCTION
AND
THE
SAME
SET
OF
PARAMETERS
ARE
USED
AT
EVERY
TIME
STEP
THE
STATE
CONSISTS
OF
A
SINGLE
HIDDEN
VECTOR
H
CHARACTER
LEVEL
LANGUAGE
MODEL
EXAMPLE
VOCABULARY
H
E
L
O
EXAMPLE
TRAINING
SEQUENCE
HELLO
CHARACTER
LEVEL
LANGUAGE
MODEL
EXAMPLE
VOCABULARY
H
E
L
O
EXAMPLE
TRAINING
SEQUENCE
HELLO
CHARACTER
LEVEL
LANGUAGE
MODEL
EXAMPLE
VOCABULARY
H
E
L
O
EXAMPLE
TRAINING
SEQUENCE
HELLO
CHARACTER
LEVEL
LANGUAGE
MODEL
EXAMPLE
VOCABULARY
H
E
L
O
EXAMPLE
TRAINING
SEQUENCE
HELLO
EXPLAIN
IMAGES
WITH
MULTIMODAL
RECURRENT
NEURAL
NETWORKS
MAO
ET
AL
DEEP
VISUAL
SEMANTIC
ALIGNMENTS
FOR
GENERATING
IMAGE
DESCRIPTIONS
KARPATHY
AND
FEI
FEI
SHOW
AND
TELL
A
NEURAL
IMAGE
CAPTION
GENERATOR
VINYALS
ET
AL
LONG
TERM
RECURRENT
CONVOLUTIONAL
NETWORKS
FOR
VISUAL
RECOGNITION
AND
DESCRIPTION
DONAHUE
ET
AL
LEARNING
A
RECURRENT
VISUAL
REPRESENTATION
FOR
IMAGE
CAPTION
GENERATION
CHEN
AND
ZITNICK
RECURRENT
NEURAL
NETWORK
CONVOLUTIONAL
NEURAL
NETWORK
TEST
IMAGE
ANDREJ
KARPATHY
ANDREJ
KARPATHY
WHH
H
WHH
H
WIH
V
CAPTION
GENERATED
STRAW
HAT
SAMPLE
END
TOKEN
FINISH
STA
RT
STRAW
HAT
START
MICROSOFT
COCO
TSUNG
YI
LIN
ET
AL
CURRENTLY
IMAGES
SENTENCES
EACH
TASK
GIVEN
AN
IMAGE
AND
A
NATURAL
LANGUAGE
OPEN
ENDED
QUESTION
GENERATE
A
NATURAL
LANGUAGE
ANSWER
AISHWARYA
AGRAWAL
AN
AID
TO
VISUALLY
IMPAIRED
IS
IT
SAFE
TO
CROSS
THE
STREET
NOW
SURVEILLANCE
WHAT
KIND
OF
CAR
DID
THE
MAN
IN
RED
SHIRT
LEAVE
IN
INTERACTING
WITH
ROBOT
IS
MY
LAPTOP
IN
MY
BEDROOM
UPSTAIRS
IMAGE
EMBEDDING
DIM
NEURAL
NETWORK
SOFTMAX
OVER
TOP
K
ANSWERS
CONVOLUTION
LAYER
NON
LINEARITY
POOLING
LAYER
CONVOLUTION
LAYER
NON
LINEARITY
POOLING
LAYER
FULLY
CONNECTED
MLP
QUESTION
EMBEDDING
HOW
MANY
HORSES
ARE
IN
THIS
IMAGE
DIM
VISUAL
QUESTION
ANSWERING
DEMO
CS
INTRO
TO
COMPUTER
VISION
MOTION
TRACKING
POSE
AND
ACTIONS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
DECEMBER
IN
THIS
SLIDE
DECK
ESTIMATING
HUMAN
POSE
RECOGNIZING
HUMAN
ACTIONS
EVEN
IMPOVERISHED
MOTION
DATA
CAN
EVOKE
A
STRONG
PERCEPT
JOHANSSON
VISUAL
PERCEPTION
OF
BIOLOGICAL
MOTION
AND
A
MODEL
FOR
ITS
ANALYSIS
PERCEPTION
AND
PSYCHOPHYSICS
TRACKING
SOME
APPLICATIONS
BODY
POSE
TRACKING
ACTIVITY
RECOGNITION
CENSUSING
A
BAT
POPULATION
VIDEO
BASED
INTERFACES
MEDICAL
APPS
SURVEILLANCE
EXAMPLE
A
CAMERA
MOUSE
VIDEO
INTERFACE
USE
FEATURE
TRACKING
AS
MOUSE
REPLACEMENT
SPECIALIZED
SOFTWARE
FOR
COMMUNICATION
GAMES
JAMES
GIPS
AND
MARGRIT
BETKE
THINGS
THAT
MAKE
VISUAL
TRACKING
DIFFICULT
ERRATIC
MOVEMENTS
MOVING
VERY
QUICKLY
OCCLUSIONS
LEAVING
AND
COMING
BACK
SURROUNDING
SIMILAR
LOOKING
OBJECTS
TRACKING
BY
REPEATED
DETECTION
WORKS
WELL
IF
OBJECT
IS
EASILY
DETECTABLE
E
G
FACE
OR
COLORED
GLOVE
AND
THERE
IS
ONLY
ONE
NEED
SOME
WAY
TO
LINK
UP
DETECTIONS
BEST
YOU
CAN
DO
IF
YOU
CAN
T
PREDICT
MOTION
KEY
IDEA
BASED
ON
A
MODEL
OF
EXPECTED
MOTION
PREDICT
WHERE
OBJECTS
WILL
OCCUR
IN
NEXT
FRAME
BEFORE
EVEN
SEEING
THE
IMAGE
RESTRICT
SEARCH
FOR
THE
OBJECT
MEASUREMENT
NOISE
IS
REDUCED
BY
TRAJECTORY
SMOOTHNESS
ROBUSTNESS
TO
MISSING
OR
WEAK
OBSERVATIONS
ASSUMPTIONS
CAMERA
IS
NOT
MOVING
INSTANTLY
TO
NEW
VIEWPOINT
OBJECTS
DO
NOT
DISAPPEAR
AND
REAPPEAR
IN
DIFFERENT
PLACES
IN
THE
SCENE
T
T
T
T
KRISTEN
GRAUMAN
DETECTION
WE
DETECT
THE
OBJECT
INDEPENDENTLY
IN
EACH
FRAME
AND
CAN
RECORD
ITS
POSITION
OVER
TIME
E
G
BASED
ON
DETECTION
WINDOW
COORDINATES
ADAPTED
FROM
KRISTEN
GRAUMAN
TRACKING
WITH
DYNAMICS
WE
USE
IMAGE
MEASUREMENTS
TO
ESTIMATE
POSITION
OF
OBJECT
BUT
ALSO
INCORPORATE
POSITION
PREDICTED
BY
DYNAMICS
I
E
OUR
EXPECTATION
OF
OBJECT
MOTION
PATTERN
TIME
T
TIME
T
BELIEF
MEASUREMENT
CORRECTED
PREDICTION
TIME
T
TIME
T
STATE
X
THE
ACTUAL
STATE
OF
THE
MOVING
OBJECT
THAT
WE
WANT
TO
ESTIMATE
BUT
CANNOT
OBSERVE
E
G
POSITION
OBSERVATIONS
Y
OUR
ACTUAL
MEASUREMENT
OR
OBSERVATION
OF
STATE
X
WHICH
CAN
BE
VERY
NOISY
AT
EACH
TIME
T
THE
STATE
CHANGES
TO
XT
AND
WE
GET
A
NEW
OBSERVATION
YT
OUR
GOAL
IS
TO
RECOVER
THE
MOST
LIKELY
STATE
XT
GIVEN
ALL
OBSERVATIONS
SO
FAR
I
E
YT
KNOWLEDGE
ABOUT
DYNAMICS
OF
STATE
TRANSITIONS
P
X
YT
T
P
X
YT
T
CORRECTION
COMPUTE
AN
UPDATED
ESTIMATE
OF
THE
STATE
FROM
PREDICTION
AND
MEASUREMENTS
P
X
YT
YT
YT
YT
WE
HAVE
MODELS
FOR
DETAILS
IN
HIDDEN
SLIDES
THAT
FOLLOW
LIKELIHOOD
OF
NEXT
STATE
GIVEN
CURRENT
STATE
DYNAMICS
MODEL
P
X
X
LIKELIHOOD
OF
OBSERVATION
GIVEN
THE
STATE
OBSERVATION
OR
MEASUREMENT
MODEL
P
Y
X
WE
WANT
TO
RECOVER
FOR
EACH
T
P
X
Y
PREDICTION
DETAILS
IN
HIDDEN
SLIDES
THAT
FOLLOW
P
XT
YT
P
XT
XT
P
XT
YT
DXT
DYNAMICS
MODEL
CORRECTED
ESTIMATE
FROM
PREVIOUS
STEP
CORRECTION
OBSERVATION
MODEL
PREDICTED
ESTIMATE
P
X
Y
Y
P
YT
XT
P
XT
YT
T
T
P
YT
XT
P
XT
YT
DXT
PREDICTION
KNOW
CORRECTED
STATE
FROM
PREVIOUS
TIME
STEP
AND
ALL
MEASUREMENTS
UP
TO
EXCLUDING
THE
CURRENT
ONE
PREDICT
DISTRIBUTION
OVER
NEXT
STATE
ADVANCES
T
P
X
RECEIVE
MEASUREMENT
CORRECTION
KNOW
PREDICTION
OF
STATE
AND
NEXT
MEASUREMENT
UPDATE
DISTRIBUTION
OVER
CURRENT
STATE
P
X
Y
KALMAN
FILTER
PROCESSING
KALMAN
FILTER
PROCESSING
KALMAN
FILTER
PROCESSING
KALMAN
FILTER
PROCESSING
GROUND
TRUTH
OBSERVATION
CORRECTION
AMIN
SADEGHI
IN
THIS
SLIDE
DECK
TRACKING
HOW
AN
OBJECT
MOVES
ACTION
A
TRANSITION
FROM
ONE
STATE
TO
ANOTHER
WHAT
IS
THE
NAME
OF
THE
ACTION
WHO
IS
THE
ACTOR
HOW
IS
THE
STATE
OF
THE
ACTOR
CHANGING
WHAT
IF
ANYTHING
IS
BEING
ACTED
ON
HOW
IS
THAT
THING
CHANGING
WHAT
IS
THE
PURPOSE
OF
THE
ACTION
IF
ANY
MOTION
POSE
HELD
OBJECTS
NEARBY
OBJECTS
JAMIE
SHOTTON
ANDREW
FITZGIBBON
MAT
COOK
TOBY
SHARP
MARK
FINOCCHIO
RICHARD
MOORE
ALEX
KIPMAN
ANDREW
BLAKE
BEST
PAPER
AWARD
AT
CVPR
ADAPTED
FROM
JAMIE
SHOTTON
RECOGNIZE
LARGE
VARIETY
OF
HUMAN
POSES
ALL
SHAPES
SIZES
LIMITED
COMPUTE
BUDGET
SUPER
REAL
TIME
ON
XBOX
TO
ALLOW
GAMES
TO
RUN
CONCURRENTLY
NO
TEMPORAL
INFORMATION
FRAME
BY
FRAME
LOCAL
POSE
ESTIMATE
OF
PARTS
EACH
PIXEL
EACH
BODY
JOINT
TREATED
INDEPENDENTLY
VERY
FAST
SIMPLE
DEPTH
IMAGE
FEATURES
DECISION
FOREST
CLASSIFIER
CAPTURE
DEPTH
IMAGE
REMOVE
BG
INFER
BODY
PARTS
PER
PIXEL
CLUSTER
PIXELS
TO
HYPOTHESIZE
BODY
JOINT
POSITIONS
FIT
MODEL
TRACK
SKELETON
COMPUTE
P
CI
WI
BODY
PARTS
CONSIDERED
BODY
PART
CI
IMAGE
WINDOW
WI
DISCRIMINATIVE
APPROACH
LEARN
CLASSIFIER
P
CI
WI
FROM
TRAINING
DATA
DEPTH
COMPARISONS
VERY
FAST
TO
COMPUTE
𝑑𝐼
X
Δ
TOY
EXAMPLE
DISTINGUISH
LEFT
L
AND
RIGHT
R
SIDES
OF
THE
BODY
TO
CLASSIFY
PIXEL
X
START
HERE
FΘ
I
X
NO
YES
FΘ
I
X
NO
YES
P
C
L
R
P
C
L
R
P
C
L
R
AMIT
GEMAN
BREIMAN
GEURTS
ET
AL
TREE
𝐼
X
𝐼
X
TREE
T
PT
C
C
C
C
TRAINED
ON
DIFFERENT
RANDOM
SUBSET
OF
IMAGES
BAGGING
HELPS
AVOID
OVER
FITTING
𝑇
AVERAGE
TREE
POSTERIORS
𝑃
𝑇
𝑃𝑡
𝑐
𝐼
X
𝑡
NUMBER
OF
TREES
DEPTH
OF
TREES
INPUT
DEPTH
INFERRED
BODY
PARTS
FRONT
VIEW
SIDE
VIEW
TOP
VIEW
JAMIE
SHOTTON
INFERRED
JOINT
POSITIONS
MODES
FOUND
USING
MEAN
SHIFT
NO
TRACKING
OR
SMOOTHING
VIA
THE
POSE
OF
PERSONS
IN
THE
VIDEO
HOW
IT
CHANGES
VIA
TRACKED
POINTS
VIA
SPATIO
TEMPORAL
INTEREST
POINTS
CORNERS
IN
SPACE
TIME
TALK
ON
PHONE
GET
OUT
OF
CAR
SPACE
TIME
INTEREST
POINT
DETECTORS
DESCRIPTORS
HOG
HOF
PYRAMID
HISTOGRAMS
SVMS
WITH
CHI
SQUARED
KERNEL
SPATIO
TEMPORAL
BINNING
INTEREST
POINTS
DETECTING
ACTIVITIES
OF
DAILY
LIVING
IN
FIRST
PERSON
CAMERA
VIEWS
HAMED
PIRSIAVASH
DEVA
RAMANAN
CVPR
WEARABLE
ADL
DETECTION
IT
IS
EASY
TO
COLLECT
NATURAL
DATA
LOW
LEVEL
FEATURES
HIGH
LEVEL
FEATURES
SPACE
TIME
INTEREST
POINTS
LAPTEV
IJCV
HUMAN
POSE
DIFFICULTIES
OF
POSE
DETECTORS
ARE
NOT
ACCURATE
ENOUGH
NOT
USEFUL
IN
FIRST
PERSON
CAMERA
VIEWS
HIGH
LEVEL
FEATURES
APPEARANCE
FEATURE
BAG
OF
OBJECTS
FRIDGE
STOVE
TV
VIDEO
CLIP
FRIDGE
BAG
OF
DETECTED
OBJECTS
STOVE
TV
ACCURACY
ON
ACTION
CATEGORIES
OUR
MODEL
STIP
BASELINE
INTEGRATED
REASONING
HUMAN
POSE
ESTIMATION
INTEGRATED
REASONING
HUMAN
POSE
ESTIMATION
OBJECT
DETECTION
TENNIS
RACKET
INTEGRATED
REASONING
HUMAN
POSE
ESTIMATION
OBJECT
DETECTION
ACTION
CATEGORIZATION
TENNIS
RACKET
HEAD
TORSO
ACTIVITY
TENNIS
FOREHAND
OBJECT
DETECTION
HUMAN
POSE
ESTIMATION
IS
CHALLENGING
YAO
FEI
FEI
FELZENSZWALB
HUTTENLOCHER
REN
ET
AL
RAMANAN
FERRARI
ET
AL
YANG
MORI
ANDRILUKA
ET
AL
EICHNER
FERRARI
HUMAN
POSE
ESTIMATION
OBJECT
DETECTION
FACILITATE
GIVEN
THE
OBJECT
IS
DETECTED
HUMAN
POSE
ESTIMATION
OBJECT
DETECTION
IS
CHALLENGING
FACILITATE
GIVEN
THE
POSE
IS
ESTIMATED
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
DECEMBER
FINAL
INFO
FORMAT
MULTIPLE
CHOICE
TRUE
FALSE
FILL
IN
THE
BLANK
SHORT
ANSWERS
APPLY
AN
ALGORITHM
NON
CUMULATIVE
I
EXPECT
YOU
TO
KNOW
HOW
TO
DO
A
CONVOLUTION
UNLIKE
LAST
TIME
I
LL
HAVE
ONE
HANDOUT
WITH
THE
EXAM
QUESTIONS
AND
A
SEPARATE
ONE
WHERE
YOU
RE
SUPPOSED
TO
WRITE
THE
ANSWERS
ALGORITHMS
YOU
SHOULD
BE
ABLE
TO
APPLY
K
MEANS
APPLY
A
FEW
ITERATIONS
TO
A
SMALL
EXAMPLE
MEAN
SHIFT
TO
SEE
WHERE
A
SINGLE
POINT
ENDS
UP
HOUGH
TRANSFORM
WRITE
PSEUDOCODE
ONLY
HOUGH
TRANSFORM
HOW
CAN
WE
USE
IT
TO
FIND
THE
PARAMETERS
MATRIX
OF
A
TRANSFORMATION
WHEN
WE
HAVE
NOISY
EXAMPLES
COMPUTE
A
SPATIAL
PYRAMID
AT
LEVEL
GRID
FORMULATE
THE
SVM
OBJECTIVE
AND
CONSTRAINTS
IN
MATH
AND
EXPLAIN
IT
WORK
THROUGH
AN
EXAMPLE
FOR
ZERO
SHOT
PREDICTION
BOOSTING
SHOW
HOW
TO
INCREASE
WEIGHTS
PEDESTRIAN
DETECTION
WRITE
HIGH
LEVEL
PSEUDOCODE
ALGORITHMS
ABLE
TO
APPLY
CONT
D
COMPUTE
NEURAL
NETWORK
ACTIVATIONS
COMPUTE
SVM
AND
SOFTMAX
LOSS
SHOW
HOW
TO
USE
WEIGHTS
TO
COMPUTE
LOSS
SHOW
HOW
TO
NUMERICALLY
COMPUTE
GRADIENT
SHOW
ONE
ITERATION
OF
GRADIENT
DESCENT
WITH
GRADIENT
COMPUTED
FOR
YOU
APPLY
CONVOLUTION
RELU
MAX
POOLING
COMPUTE
OUTPUT
DIMENSIONS
FROM
CONVOLUTION
MONDAY
ANYONE
FOR
WHOM
THIS
DOES
NOT
WORK
CONVOLUTIONAL
NEURAL
NETWORKS
REQUESTS
HOUGH
TRANSFORM
SUPPORT
VECTOR
MACHINES
DEFORMABLE
PART
MODELS
ZERO
SHOT
LEARNING
FACE
DETECTION
RECURRENT
NEURAL
NETWORKS
K
MEANS
MEAN
SHIFT
SPATIAL
PYRAMIDS
BACKPROPAGATION
MEANING
OF
WEIGHTS
AND
HOW
COMPUTED
REQUESTS
MATH
FOR
NEURAL
NETWORKS
COMPUTING
ACTIVATIONS
GRADIENTS
GRADIENT
DESCENT
CONVOLUTION
NON
LINEARITY
POOLING
CONVOLUTION
OUTPUT
SIZE
ARCHITECTURES
LOSSES
AND
FINDING
WEIGHTS
THAT
MINIMIZE
THEM
MINIBATCH
ARE
THE
TRAINING
EXAMPLES
CYCLED
OVER
MORE
THAN
ONCE
EFFECT
OF
NUMBER
OF
NEURONS
AND
REGULARIZATION
NEURAL
NETWORKS
NONLINEAR
ACTIVATION
FUNCTION
H
E
G
SIGMOID
TANH
OUTPUTS
BINARY
MULTICLASS
HOW
CAN
I
WRITE
AS
A
FUNCTION
OF
XD
SIGMOID
TANH
TANH
X
RELU
MAX
X
WHEN
DO
I
NEED
TO
COMPUTE
ACTIVATIONS
HOW
MANY
TIMES
DO
I
NEED
TO
DO
THAT
HOW
MANY
TIMES
DO
I
NEED
TO
TRAIN
A
NETWORK
TO
EXTRACT
FEATURES
FROM
IT
ACTIVATIONS
FORWARD
PROPAGATION
START
FROM
INPUTS
COMPUTE
ACTIVATIONS
FROM
INPUTS
TO
OUTPUTS
TRAINING
BACKWARD
PROPAGATION
COMPUTE
A
LOSS
AT
THE
OUTPUTS
BACKPROPAGATE
ERROR
TOWARDS
THE
INPUTS
FIRST
CALCULATE
ERROR
OF
OUTPUT
UNITS
AND
USE
THIS
TO
CHANGE
THE
TOP
LAYER
OF
WEIGHTS
UPDATE
WEIGHTS
INTO
J
OUTPUT
K
HIDDEN
J
INPUT
I
NEXT
CALCULATE
ERROR
FOR
HIDDEN
UNITS
BASED
ON
ERRORS
ON
THE
OUTPUT
UNITS
IT
FEEDS
INTO
OUTPUT
K
HIDDEN
J
INPUT
I
FINALLY
UPDATE
BOTTOM
LAYER
OF
WEIGHTS
BASED
ON
ERRORS
CALCULATED
FOR
HIDDEN
UNITS
OUTPUT
K
UPDATE
WEIGHTS
INTO
I
HIDDEN
J
INPUT
I
DENOTED
AS
DIFF
NOTATIONS
I
E
HOW
DOES
THE
LOSS
CHANGE
AS
A
FUNCTION
OF
THE
WEIGHTS
WE
WANT
TO
FIND
THOSE
WEIGHTS
CHANGE
THE
WEIGHTS
IN
SUCH
A
WAY
THAT
MAKES
THE
LOSS
DECREASE
AS
FAST
AS
POSSIBLE
WE
LL
UPDATE
WEIGHTS
MOVE
IN
DIRECTION
OPPOSITE
TO
GRADIENT
TIME
LEARNING
RATE
IN
DIMENSION
THE
DERIVATIVE
OF
A
FUNCTION
IN
MULTIPLE
DIMENSIONS
THE
GRADIENT
IS
THE
VECTOR
OF
PARTIAL
DERIVATIVES
CURRENT
W
LOSS
GRADIENT
DW
CURRENT
W
LOSS
W
H
FIRST
DIM
LOSS
GRADIENT
DW
CURRENT
W
LOSS
W
H
FIRST
DIM
LOSS
GRADIENT
DW
LOSSES
DEPEND
ON
THE
PREDICTION
FUNCTIONS
SCORES
E
G
FW
X
FOR
CLASS
CAT
ONE
SET
OF
WEIGHTS
FOR
EACH
CLASS
THE
PREDICTION
FUNCTIONS
SCORES
DEPEND
ON
THE
INPUTS
X
AND
THE
MODEL
PARAMETERS
W
HENCE
LOSSES
DEPEND
ON
W
E
G
FOR
A
LINEAR
CLASSIFIER
SCORES
ARE
FOR
A
NEURAL
NETWORK
ARRAY
OF
NUMBERS
NUMBERS
INDICATING
CLASS
SCORES
PARAMETERS
OR
WEIGHTS
IN
THE
SECOND
LAYER
OF
WEIGHTS
ONE
SET
OF
WEIGHTS
TO
COMPUTE
THE
PROBABILITY
OF
EACH
CLASS
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
CAT
CAR
FROG
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
SUPPOSE
TRAINING
EXAMPLES
CLASSES
WITH
SOME
W
THE
SCORES
ARE
MULTICLASS
SVM
LOSS
GIVEN
AN
EXAMPLE
WHERE
WHERE
IS
THE
IMAGE
AND
IS
THE
INTEGER
LABEL
AND
USING
THE
SHORTHAND
FOR
THE
SCORES
VECTOR
CAT
CAR
FROG
THE
SVM
LOSS
HAS
THE
FORM
AND
THE
FULL
TRAINING
LOSS
IS
THE
MEAN
OVER
ALL
EXAMPLES
IN
THE
TRAINING
DATA
L
LOSSES
WEIGHT
REGULARIZATION
Λ
REGULARIZATION
STRENGTH
HYPERPARAMETER
IN
COMMON
USE
REGULARIZATION
REGULARIZATION
DROPOUT
WILL
SEE
LATER
IN
THE
CASE
OF
A
NEURAL
NETWORK
REGULARIZATION
TURNS
SOME
NEURONS
OFF
THEY
DON
T
MATTER
FOR
COMPUTING
AN
ACTIVATION
DO
NOT
USE
SIZE
OF
NEURAL
NETWORK
AS
A
REGULARIZER
USE
STRONGER
REGULARIZATION
INSTEAD
YOU
CAN
PLAY
WITH
THIS
DEMO
OVER
AT
CONVNETJS
MORE
NEURONS
MORE
CAPACITY
CAT
CAR
FROG
UNNORMALIZED
PROBABILITIES
EXP
NORMALIZE
LOG
UNNORMALIZED
LOG
PROBABILITIES
PROBABILITIES
IN
CLASSIC
GRADIENT
DESCENT
WE
COMPUTE
THE
GRADIENT
FROM
THE
LOSS
FOR
ALL
TRAINING
EXAMPLES
COULD
ALSO
ONLY
USE
SOME
OF
THE
DATA
FOR
EACH
GRADIENT
UPDATE
THEN
CYCLE
THROUGH
ALL
TRAINING
SAMPLES
YES
WE
CYCLE
THROUGH
THE
TRAINING
EXAMPLES
MULTIPLE
TIMES
EACH
TIME
WE
VE
CYCLED
THROUGH
ALL
OF
THEM
ONCE
IS
CALLED
AN
EPOCH
ALLOWS
FASTER
TRAINING
E
G
ON
GPUS
PARALLELIZATION
THE
MORE
WEIGHTS
YOU
NEED
TO
LEARN
THE
MORE
DATA
YOU
NEED
THAT
WHY
WITH
A
DEEPER
NETWORK
YOU
NEED
MORE
DATA
FOR
TRAINING
THAN
FOR
A
SHALLOWER
NETWORK
THAT
WHY
IF
YOU
HAVE
SPARSE
DATA
YOU
ONLY
TRAIN
THE
LAST
FEW
LAYERS
OF
A
DEEP
NET
SET
THESE
TO
THE
ALREADY
LEARNED
WEIGHTS
FROM
ANOTHER
NETWORK
LEARN
THESE
ON
YOUR
OWN
TASK
CONVOLUTIONAL
NEURAL
NETWORKS
FEED
FORWARD
FEATURE
EXTRACTION
CONVOLVE
INPUT
WITH
LEARNED
FILTERS
APPLY
NON
LINEARITY
SPATIAL
POOLING
DOWNSAMPLE
ADAPTED
FROM
LANA
LAZEBNIK
APPLY
LEARNED
FILTER
WEIGHTS
ONE
FEATURE
MAP
PER
FILTER
STRIDE
CAN
BE
GREATER
THAN
FASTER
LESS
MEMORY
NON
LINEARITY
PER
ELEMENT
INDEPENDENT
OPTIONS
TANH
SIGMOID
EXP
X
RECTIFIED
LINEAR
UNIT
RELU
AVOIDS
SATURATION
ISSUES
SPATIAL
POOLING
SUM
OR
MAX
OVER
NON
OVERLAPPING
OVERLAPPING
REGIONS
ROLE
OF
POOLING
INVARIANCE
TO
SMALL
TRANSFORMATIONS
LARGER
RECEPTIVE
FIELDS
NEURONS
SEE
MORE
OF
INPUT
CONVOLUTION
LAYER
IMAGE
FILTER
NUMBER
THE
RESULT
OF
TAKING
A
DOT
PRODUCT
BETWEEN
THE
FILTER
AND
A
SMALL
CHUNK
OF
THE
IMAGE
I
E
DIMENSIONAL
DOT
PRODUCT
BIAS
CONVOLUTION
LAYER
IMAGE
FILTER
ACTIVATION
MAP
CONVOLVE
SLIDE
OVER
ALL
SPATIAL
LOCATIONS
FOR
EXAMPLE
IF
WE
HAD
FILTERS
WE
LL
GET
SEPARATE
ACTIVATION
MAPS
ACTIVATION
MAPS
CONVOLUTION
LAYER
WE
STACK
THESE
UP
TO
GET
A
NEW
IMAGE
OF
SIZE
PREVIEW
CONVNET
IS
A
SEQUENCE
OF
CONVOLUTIONAL
LAYERS
INTERSPERSED
WITH
ACTIVATION
FUNCTIONS
CONV
RELU
E
G
FILTERS
CONV
RELU
FILTERS
CONV
RELU
FROM
RECENT
YANN
LECUN
SLIDES
N
OUTPUT
SIZE
N
F
STRIDE
N
E
G
N
F
STRIDE
STRIDE
STRIDE
33
IN
PRACTICE
COMMON
TO
ZERO
PAD
THE
BORDER
E
G
INPUT
FILTER
APPLIED
WITH
STRIDE
PAD
WITH
PIXEL
BORDER
WHAT
IS
THE
OUTPUT
OUTPUT
IN
GENERAL
COMMON
TO
SEE
CONV
LAYERS
WITH
STRIDE
FILTERS
OF
SIZE
FXF
AND
ZERO
PADDING
WITH
F
WILL
PRESERVE
SIZE
SPATIALLY
E
G
F
ZERO
PAD
WITH
F
ZERO
PAD
WITH
F
ZERO
PAD
WITH
N
PADDING
F
STRIDE
HOUGH
TRANSFORM
RANSAC
WON
T
BE
ON
THE
EXAM
LEAST
SQUARES
LINE
FITTING
DATA
XN
YN
LINE
EQUATION
YI
M
XI
B
FIND
M
B
TO
MINIMIZE
XI
YI
Y
MX
B
WHERE
LINE
YOU
FOUND
TELLS
YOU
POINT
IS
ALONG
Y
AXIS
WHERE
POINT
REALLY
IS
ALONG
Y
AXIS
YOU
WANT
TO
FIND
A
SINGLE
LINE
THAT
EXPLAINS
ALL
OF
THE
POINTS
IN
YOUR
DATA
BUT
DATA
MAY
BE
NOISY
VOTING
IS
A
GENERAL
TECHNIQUE
WHERE
WE
LET
THE
FEATURES
VOTE
FOR
ALL
MODELS
THAT
ARE
COMPATIBLE
WITH
IT
CYCLE
THROUGH
FEATURES
CAST
VOTES
FOR
MODEL
PARAMETERS
LOOK
FOR
MODEL
PARAMETERS
THAT
RECEIVE
A
LOT
OF
VOTES
NOISE
CLUTTER
FEATURES
THEY
WILL
CAST
VOTES
TOO
BUT
TYPICALLY
THEIR
VOTES
SHOULD
BE
INCONSISTENT
WITH
THE
MAJORITY
OF
GOOD
FEATURES
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
WHAT
DOES
A
POINT
IN
THE
IMAGE
SPACE
MAP
TO
ANSWER
THE
SOLUTIONS
OF
B
THIS
IS
A
LINE
IN
HOUGH
SPACE
TO
GO
FROM
IMAGE
SPACE
TO
HOUGH
SPACE
GIVEN
A
PAIR
OF
POINTS
X
Y
FIND
ALL
M
B
SUCH
THAT
Y
MX
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
WHAT
ARE
THE
LINE
PARAMETERS
FOR
THE
LINE
THAT
CONTAINS
BOTH
AND
IT
IS
THE
INTERSECTION
OF
THE
LINES
B
AND
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
HOW
CAN
WE
USE
THIS
TO
FIND
THE
MOST
LIKELY
PARAMETERS
M
B
FOR
THE
MOST
PROMINENT
LINE
IN
THE
IMAGE
SPACE
LET
EACH
EDGE
POINT
IN
IMAGE
SPACE
VOTE
FOR
A
SET
OF
POSSIBLE
PARAMETERS
IN
HOUGH
SPACE
ACCUMULATE
VOTES
IN
DISCRETE
SET
OF
BINS
PARAMETERS
WITH
THE
MOST
VOTES
INDICATE
LINE
IN
IMAGE
SPACE
HOUGH
MACHINE
ANALYSIS
OF
BUBBLE
CHAMBER
PICTURES
PROC
INT
CONF
HIGH
ENERGY
ACCELERATORS
AND
INSTRUMENTATION
USE
A
POLAR
REPRESENTATION
FOR
THE
PARAMETER
SPACE
EACH
LINE
IS
A
SINUSOID
IN
HOUGH
PARAMETER
SPACE
Y
X
HOUGH
SPACE
X
COS
YSIN
INITIALIZE
ACCUMULATOR
H
TO
ALL
ZEROS
FOR
EACH
FEATURE
POINT
X
Y
Ρ
IN
THE
IMAGE
Θ
GRADIENT
ORIENTATION
AT
X
Y
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
Θ
END
FIND
THE
VALUE
OF
Θ
Ρ
WHERE
H
Θ
Ρ
IS
A
LOCAL
MAXIMUM
THE
DETECTED
LINE
IN
THE
IMAGE
IS
GIVEN
BY
Ρ
X
COS
Θ
Y
SIN
Θ
HOUGH
TRANSFORM
FOR
CIRCLES
XI
A
YI
B
X
A
R
COS
Θ
Y
B
R
SIN
Θ
FOR
EVERY
EDGE
PIXEL
X
Y
Θ
GRADIENT
ORIENTATION
AT
X
Y
FOR
EACH
POSSIBLE
RADIUS
VALUE
R
A
X
R
COS
Θ
B
Y
R
SIN
Θ
H
A
B
R
END
END
GIVEN
MATCHED
POINTS
IN
A
AND
B
ESTIMATE
THE
TRANSLATION
OF
THE
OBJECT
X
B
X
A
TX
I
I
I
I
T
Y
TX
TY
PROBLEM
OUTLIERS
MULTIPLE
OBJECTS
AND
OR
MANY
TO
ONE
MATCHES
HOUGH
TRANSFORM
SOLUTION
X
B
X
A
TX
INITIALIZE
A
GRID
OF
PARAMETER
VALUES
I
I
EACH
MATCHED
PAIR
CASTS
A
VOTE
FOR
CONSISTENT
VALUES
FIND
THE
PARAMETERS
WITH
THE
MOST
VOTES
I
I
T
Y
SUPPORT
VECTOR
MACHINES
FIND
LINEAR
FUNCTION
TO
SEPARATE
POSITIVE
AND
NEGATIVE
EXAMPLES
XI
POSITIVE
XI
W
B
XI
NEGATIVE
XI
W
B
WHICH
LINE
IS
BEST
DISCRIMINATIVE
CLASSIFIER
BASED
ON
OPTIMAL
SEPARATING
LINE
FOR
CASE
MAXIMIZE
THE
MARGIN
BETWEEN
THE
POSITIVE
AND
NEGATIVE
TRAINING
EXAMPLES
WANT
LINE
THAT
MAXIMIZES
THE
MARGIN
XI
POSITIVE
YI
XI
NEGATIVE
YI
XI
W
B
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
XI
W
B
AND
LINE
FOR
SUPPORT
VECTORS
W
SUPPORT
VECTORS
MARGIN
WΤ
X
B
M
MAXIMIZE
MARGIN
W
CORRECTLY
CLASSIFY
ALL
TRAINING
DATA
POINTS
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
QUADRATIC
OPTIMIZATION
PROBLEM
ONE
CONSTRAINT
FOR
EACH
TRAINING
POINT
NOTE
SIGN
TRICK
SOLUTION
W
I
I
YI
XI
SOLUTION
W
I
I
YI
XI
B
YI
W
XI
FOR
ANY
SUPPORT
VECTOR
CLASSIFICATION
FUNCTION
F
X
SIGN
W
X
B
SIGN
I
I
YI
XI
X
B
IF
F
X
CLASSIFY
AS
NEGATIVE
OTHERWISE
CLASSIFY
AS
POSITIVE
NOTICE
THAT
IT
RELIES
ON
AN
INNER
PRODUCT
BETWEEN
THE
TEST
POINT
X
AND
THE
SUPPORT
VECTORS
XI
THE
KERNEL
TRICK
THE
LINEAR
CLASSIFIER
RELIES
ON
DOT
PRODUCT
BETWEEN
VECTORS
K
XI
XJ
XI
XJ
IF
EVERY
DATA
POINT
IS
MAPPED
INTO
HIGH
DIMENSIONAL
SPACE
VIA
SOME
TRANSFORMATION
Φ
XI
Φ
XI
THE
DOT
PRODUCT
BECOMES
K
XI
XJ
Φ
XI
Φ
XJ
THE
KERNEL
TRICK
INSTEAD
OF
EXPLICITLY
COMPUTING
THE
LIFTING
TRANSFORMATION
Φ
X
DEFINE
A
KERNEL
FUNCTION
K
SUCH
THAT
K
XI
XJ
Φ
XI
Φ
XJ
NONLINEAR
SVMS
DATASETS
THAT
ARE
LINEARLY
SEPARABLE
WORK
OUT
GREAT
X
BUT
WHAT
IF
THE
DATASET
IS
JUST
TOO
HARD
X
WE
CAN
MAP
IT
TO
A
HIGHER
DIMENSIONAL
SPACE
NONLINEAR
KERNEL
EXAMPLE
CONSIDER
THE
MAPPING
X
X
X
Y
X
Y
XY
K
X
Y
XY
EXAMPLES
OF
KERNEL
FUNCTIONS
LINEAR
K
XI
X
J
I
J
POLYNOMIALS
OF
DEGREE
UP
TO
D
𝐾
𝑥𝑖
𝑥𝑗
𝑥𝑖𝑇𝑥𝑗
𝑑
GAUSSIAN
RBF
K
XI
X
J
EXP
HISTOGRAM
INTERSECTION
K
XI
X
J
MIN
XI
K
K
X
J
K
OBJECTIVE
THE
W
THAT
MINIMIZES
CONSTRAINTS
MAXIMIZE
MARGIN
OBJECTIVE
MISCLASSIFICATION
COST
DATA
SAMPLES
SLACK
VARIABLE
THE
W
THAT
MINIMIZES
CONSTRAINTS
MAXIMIZE
MARGIN
MINIMIZE
MISCLASSIFICATION
DEFORMABLE
PART
MODELS
ZERO
SHOT
LEARNING
IMAGE
CLASSIFICATION
TEXTUAL
DESCRIPTIONS
WHICH
IMAGE
SHOWS
AN
AYE
AYE
DESCRIPTION
AYE
AYE
IS
NOCTURNAL
LIVES
IN
TREES
HAS
LARGE
EYES
HAS
LONG
MIDDLE
FINGERS
WE
CAN
CLASSIFY
BASED
ON
TEXTUAL
DESCRIPTIONS
VOCABULARY
OF
ATTRIBUTES
AND
CLASS
DESCRIPTIONS
AYE
AYES
HAVE
PROPERTIES
X
AND
Y
BUT
NOT
Z
TRAIN
CLASSIFIERS
FOR
EACH
ATTIBUTE
X
Y
Z
FROM
VISUAL
EXAMPLES
OF
RELATED
CLASSES
MAKE
IMAGE
ATTRIBUTES
PREDICTIONS
COMBINE
INTO
DECISION
THIS
IMAGE
IS
NOT
AN
AYE
AYE
P
X
IMG
P
Y
IMG
P
Z
IMG
RIPTIONS
BUT
NOT
Z
Z
FROM
VISUAL
EXAMPLES
OF
RELATED
CLASSES
MAKE
IMAGE
ATTRIBUTES
PREDICTIONS
COMBINE
INTO
DECISION
THIS
IMAGE
IS
NOT
AN
AYE
AYE
DEFINE
ATTRIBUTE
PROBABILITY
P
AZ
X
P
AM
X
IF
AZ
M
P
AM
X
OTHERWISE
ASSIGN
A
GIVEN
IMAGE
TO
CLASS
Z
EXAMPLE
CAT
ATTRIBUTES
BEAR
ATTRIBUTES
IMAGE
X
PROBABILITY
OF
THE
ATTRIBUTES
P
ATTRIBUTEI
X
PROBABILITY
THAT
CLASS
X
CAT
PROBABILITY
THAT
CLASS
X
BEAR
UNSUPERVISED
LEARNING
EXAMPLE
CLUSTERING
GROUP
TOGETHER
SIMILAR
EXAMPLES
DI
XI
UNSUPERVISED
LEARNING
DENSITY
ESTIMATION
A
PROBABILITY
DENSITY
OF
A
POINT
IN
THE
TWO
DIMENSIONAL
SPACE
MODEL
USED
HERE
MIXTURE
OF
GAUSSIANS
CS
INTRODUCTION
TO
MACHINE
LEARNING
LECTURE
TOPICS
SUPPORT
VECTOR
MACHINES
CONT
ROC
ANALYSIS
NONPARAMETRIC
METHODS
MILOS
HAUSKRECHT
SENNOTT
SQUARE
LAST
LECTURE
OUTLINE
OUTLINE
ALGORITHMS
FOR
LINEAR
DECISION
BOUNDARY
SUPPORT
VECTOR
MACHINES
MAXIMUM
MARGIN
HYPERPLANE
SUPPORT
VECTORS
SUPPORT
VECTOR
MACHINES
LEARNING
EXTENSIONS
TO
THE
LINEARLY
NON
SEPARABLE
CASE
KERNEL
FUNCTIONS
PROBLEM
THERE
ARE
MULTIPLE
HYPERPLANES
THAT
SEPARATE
THE
DATA
POINTS
WHICH
ONE
TO
CHOOSE
PROBLEM
THERE
ARE
MULTIPLE
HYPERPLANES
THAT
SEPARATE
THE
DATA
POINTS
WHICH
ONE
TO
CHOOSE
THE
DECISION
BOUNDARY
THAT
MAXIMIZES
THE
DISTANCE
OF
THE
AND
POINTS
FROM
IT
FOR
THE
MAXIMUM
MARGIN
HYPERPLANE
ONLY
EXAMPLES
ON
THE
MARGIN
MATTER
ONLY
THESE
AFFECT
THE
DISTANCES
THESE
ARE
CALLED
SUPPORT
VECTORS
DECISION
BOUNDARY
DEFINED
BY
A
SET
OF
SUPPORT
VECTORS
SV
AND
THEIR
ALPHA
VALUES
SUPPORT
VECTORS
A
SUBSET
OF
DATAPOINTS
IN
THE
TRAINING
DATA
THAT
DEFINE
THE
MARGIN
Wˆ
T
X
W
ˆI
I
SV
YI
XI
T
X
W
CLASSIFICATION
DECISION
LAGRANGE
MULTIPLIERS
Yˆ
SIGN
ˆ
Y
X
T
X
W
I
I
I
I
SV
NOTE
THAT
WE
DO
NOT
HAVE
TO
EXPLICITLY
COMPUTE
Wˆ
THIS
WILL
BE
IMPORTANT
FOR
THE
NONLINEAR
KERNEL
CASE
DECISION
ON
A
NEW
X
DEPENDS
ON
THE
INNER
PRODUCT
BETWEEN
TWO
EXAMPLES
THE
DECISION
BOUNDARY
Wˆ
T
X
W
ˆI
I
SV
YI
XI
T
X
W
CLASSIFICATION
DECISION
Yˆ
SIGN
ˆ
Y
X
T
X
W
I
I
I
I
SV
SIMILARLY
THE
OPTIMIZATION
DEPENDS
ON
XI
X
J
J
N
I
I
N
I
J
YI
Y
J
I
J
IDEA
ALLOW
SOME
FLEXIBILITY
ON
CROSSING
THE
SEPARATING
HYPERPLANE
THE
SOLUTION
OF
THE
LINEARLY
NON
SEPARABLE
CASE
HAS
THE
SAME
PROPERTIES
AS
THE
LINEARLY
SEPARABLE
CASE
THE
DECISION
BOUNDARY
IS
DEFINED
ONLY
BY
A
SET
OF
SUPPORT
VECTORS
POINTS
THAT
ARE
ON
THE
MARGIN
OR
THAT
CROSS
THE
MARGIN
THE
DECISION
BOUNDARY
AND
THE
OPTIMIZATION
CAN
BE
EXPRESSED
IN
TERMS
OF
THE
INNER
PRODUCT
IN
BETWEEN
PAIRS
OF
EXAMPLES
Wˆ
T
X
W
ˆI
I
SV
YI
XI
T
X
W
Yˆ
SIGN
Wˆ
T
X
W
SIGN
ˆ
Y
W
N
N
I
I
I
SV
J
I
I
I
J
YI
Y
J
I
J
SO
FAR
WE
HAVE
SEEN
HOW
TO
LEARN
A
LINEAR
DECISION
BOUNDARY
BUT
WHAT
IF
THE
LINEAR
DECISION
BOUNDARY
IS
NOT
GOOD
HOW
WE
CAN
LEARN
A
NON
LINEAR
DECISION
BOUNDARIES
WITH
THE
SVM
THE
NON
LINEAR
CASE
CAN
BE
HANDLED
BY
USING
A
SET
OF
FEATURES
ESSENTIALLY
WE
MAP
INPUT
VECTORS
TO
LARGER
FEATURE
VECTORS
X
Φ
X
EXAMPLE
POLYNOMIAL
EXPANSIONS
NOTE
THAT
FEATURE
EXPANSIONS
ARE
TYPICALLY
HIGH
DIMENSIONAL
GIVEN
THE
NONLINEAR
FEATURE
MAPPINGS
WE
CAN
USE
THE
LINEAR
SVM
ON
THE
EXPANDED
FEATURE
VECTORS
XT
X
Φ
X
T
Φ
X
KERNEL
FUNCTION
MEASURES
SIMILARITY
K
X
X
Φ
X
T
Φ
X
SUPPORT
VECTOR
MACHINES
SOLUTION
FOR
NONLINEAR
DECISION
BOUNDARIES
THE
DECISION
BOUNDARY
Wˆ
T
X
W
ˆI
I
SV
YI
K
XI
X
CLASSIFICATION
Yˆ
SIGN
Wˆ
T
X
W
SIGN
ˆ
Y
K
X
X
W
I
I
I
I
SV
DECISION
ON
A
NEW
X
REQUIRES
TO
COMPUTE
THE
KERNEL
FUNCTION
DEFINING
THE
SIMILARITY
BETWEEN
THE
EXAMPLES
SIMILARLY
THE
OPTIMIZATION
DEPENDS
ON
THE
KERNEL
J
N
I
I
N
I
J
YI
Y
I
J
FEATURE
MAPPING
KERNEL
TRICK
X
Φ
X
KERNEL
FUNCTION
DEFINES
THE
INNER
PRODUCT
IN
THE
EXPANDED
HIGH
DIMENSIONAL
FEATURE
VECTORS
AND
LET
US
USE
THE
SVM
K
X
X
Φ
X
T
Φ
X
PROBLEM
AFTER
EXPANSION
WE
NEED
TO
PERFORM
INNER
PRODUCTS
IN
A
VERY
HIGH
DIMENSIONAL
Φ
X
SPACE
KERNEL
TRICK
IF
WE
CHOOSE
THE
KERNEL
FUNCTION
K
X
X
WISELY
WE
CAN
COMPUTE
LINEAR
SEPARATION
IN
THE
HIGH
DIMENSIONAL
FEATURE
SPACE
IMPLICITLY
BY
WORKING
IN
THE
ORIGINAL
INPUT
SPACE
ASSUME
X
AND
A
FEATURE
MAPPING
THAT
MAPS
THE
INPUT
INTO
A
QUADRATIC
FEATURE
SET
X
Φ
X
X
T
KERNEL
FUNCTION
FOR
THE
FEATURE
SPACE
K
X
X
Φ
X
T
Φ
X
X
X
X
X
X
X
XT
X
THE
COMPUTATION
OF
THE
LINEAR
SEPARATION
IN
THE
HIGHER
DIMENSIONAL
SPACE
IS
PERFORMED
IMPLICITLY
IN
THE
ORIGINAL
INPUT
SPACE
LINEAR
SEPARATOR
IN
THE
EXPANDED
FEATURE
SPACE
LINEAR
KERNEL
K
X
X
XT
X
POLYNOMIAL
KERNEL
K
X
X
RADIAL
BASIS
KERNEL
XT
X
K
K
X
X
EXP
X
X
ML
RESEARCHERS
HAVE
PROPOSED
KERNELS
FOR
COMPARISON
OF
VARIETY
OF
OBJECTS
STRINGS
TREES
GRAPHS
COOL
THING
SVM
ALGORITHM
CAN
BE
NOW
APPLIED
TO
CLASSIFY
A
VARIETY
OF
OBJECTS
EVALUATION
OF
BINARY
CLASSIFIERS
ROC
ANALYSIS
FOR
ANY
DATA
SET
WE
USE
TO
TEST
THE
CLASSIFICATION
MODEL
ON
WE
CAN
BUILD
A
CONFUSION
MATRIX
COUNTS
OF
EXAMPLES
WITH
CLASS
LABEL
J
THAT
ARE
CLASSIFIED
WITH
A
LABEL
I
TARGET
PREDICT
FOR
ANY
DATA
SET
WE
USE
TO
TEST
THE
MODEL
WE
CAN
BUILD
A
CONFUSION
MATRIX
TARGET
PREDICT
ACCURACY
FOR
ANY
DATA
SET
WE
USE
TO
TEST
THE
MODEL
WE
CAN
BUILD
A
CONFUSION
MATRIX
TARGET
PREDICT
ACCURACY
ERROR
ACCURACY
ENTRIES
IN
THE
CONFUSION
MATRIX
FOR
BINARY
CLASSIFICATION
HAVE
NAMES
TARGET
PREDICT
TP
TRUE
POSITIVE
HIT
FP
FALSE
POSITIVE
FALSE
ALARM
TN
TRUE
NEGATIVE
CORRECT
REJECTION
FN
FALSE
NEGATIVE
A
MISS
SENSITIVITY
RECALL
SENS
TP
TP
FN
SPECIFICITY
SPEC
TN
TN
FP
POSITIVE
PREDICTIVE
VALUE
PRECISION
PPT
TP
TP
FP
NEGATIVE
PREDICTIVE
VALUE
NPV
TN
TN
FN
CONFUSION
MATRIX
TARGET
PREDICT
ROW
AND
COLUMN
QUANTITIES
SENSITIVITY
SENS
SPECIFICITY
SPEC
POSITIVE
PREDICTIVE
VALUE
PPV
NEGATIVE
PREDICTIVE
VALUE
NPV
CLASSIFIERS
PROJECT
DATAPOINTS
TO
ONE
DIMENSIONAL
SPACE
DEFINED
FOR
EXAMPLE
BY
WTX
OR
P
Y
X
W
DECISION
BOUNDARY
WTX
WTX
WTX
DECISION
BOUNDARY
WTX
BINARY
DECISIONS
RECEIVER
OPERATING
CURVES
X
PROBABILITIES
SENS
SPEC
THRESHOLD
P
X
P
X
X
X
X
X
RECEIVER
OPERATING
CHARACTERISTIC
ROC
ROC
CURVE
PLOTS
SN
P
X
X
X
SP
P
X
X
X
FOR
DIFFERENT
X
X
SENS
P
X
X
X
SPEC
P
X
X
X
ROC
CURVE
CASE
CASE
CASE
P
X
X
X
P
X
X
X
RECEIVER
OPERATING
CHARACTERISTIC
ROC
SHOWS
THE
DISCRIMINABILITY
BETWEEN
THE
TWO
CLASSES
UNDER
DIFFERENT
DECISION
BIASES
DECISION
BIAS
CAN
BE
CHANGED
USING
DIFFERENT
LOSS
FUNCTION
QUALITY
OF
A
CLASSIFICATION
MODEL
AREA
UNDER
THE
ROC
BEST
VALUE
WORST
NO
DISCRIMINABILITY
NONPARAMETRIC
METHODS
PARAMETRIC
DISTRIBUTION
MODELS
ARE
RESTRICTED
TO
SPECIFIC
FORMS
WHICH
MAY
NOT
ALWAYS
BE
SUITABLE
EXAMPLE
MODELLING
A
MULTIMODAL
DISTRIBUTION
WITH
A
SINGLE
UNIMODAL
MODEL
NONPARAMETRIC
APPROACHES
MAKE
FEW
ASSUMPTIONS
ABOUT
THE
OVERALL
SHAPE
OF
THE
DISTRIBUTION
BEING
MODELLED
CS
MACHINE
LEARNING
NONPARAMETRIC
METHODS
PROBLEM
WE
HAVE
A
SET
D
OF
DATA
POINTS
XI
FOR
I
N
WE
WANT
TO
CALCULATE
P
X
FOR
A
TARGET
VALUE
OF
X
PARAMETRIC
APPROACH
REPRESENTS
P
X
USING
A
PARAMETRIC
DENSITY
MODEL
WITH
PARAMETERS
Θ
FITS
THE
PARAMETERS
Θ
WRT
THE
DATA
NONPARAMETRIC
APPROACH
DOES
NOT
MAKE
ANY
PARAMETRIC
ASSUMPTION
ESTIMATES
P
X
FROM
ALL
DATAPOINTS
IN
D
AS
IF
ALL
D
ARE
PARAMETERS
HISTOGRAM
METHODS
PARTITION
THE
DATA
SPACE
INTO
DISTINCT
BINS
WITH
WIDTHS
I
AND
COUNT
THE
NUMBER
OF
OBSERVATIONS
NI
IN
EACH
BIN
P
NI
I
NΔ
OFTEN
THE
SAME
WIDTH
IS
USED
FOR
ALL
BINS
I
ACTS
AS
A
SMOOTHING
PARAMETER
MD
BINS
ASSUME
OBSERVATIONS
DRAWN
FROM
A
DENSITY
P
X
AND
CONSIDER
A
SMALL
REGION
R
CONTAINING
X
SUCH
THAT
IF
THE
VOLUME
OF
R
V
IS
SUFFICIENTLY
SMALL
P
X
IS
APPROXIMATELY
CONSTANT
OVER
R
AND
P
R
P
X
DX
P
P
X
V
THE
PROBABILITY
THAT
K
OUT
OF
N
OBSERVATIONS
LIE
INSIDE
R
IS
BIN
K
N
P
AND
IF
N
IS
LARGE
THUS
P
X
P
V
K
NP
KERNEL
DENSITY
ESTIMATION
FIX
V
ESTIMATE
K
FROM
THE
DATA
LET
R
BE
A
HYPERCUBE
CENTRED
ON
X
AND
DEFINE
THE
KERNEL
FUNCTION
PARZEN
WINDOW
K
X
XN
XI
XNI
H
I
D
OTHERWISE
H
IT
FOLLOWS
THAT
AND
HENCE
K
N
X
XN
K
H
P
X
N
N
N
HD
X
XN
CS
MACHINE
LEARNING
TO
AVOID
DISCONTINUITIES
IN
P
X
BECAUSE
OF
SHARP
BOUNDARIES
USE
A
SMOOTH
KERNEL
E
G
A
GAUSSIAN
ANY
KERNEL
SUCH
THAT
H
ACTS
AS
A
SMOOTHER
WILL
WORK
NEAREST
NEIGHBOUR
DENSITY
ESTIMATION
FIX
K
ESTIMATE
V
FROM
THE
DATA
CONSIDER
A
HYPER
SPHERE
CENTRED
ON
X
AND
LET
IT
GROW
TO
A
VOLUME
V
THAT
INCLUDES
K
OF
THE
GIVEN
N
DATA
POINTS
THEN
K
ACTS
AS
A
SMOOTHER
NONPARAMETRIC
MODELS
MORE
FLEXIBILITY
NO
DENSITY
MODEL
IS
NEEDED
BUT
REQUIRE
STORING
THE
ENTIRE
DATASET
AND
THE
COMPUTATION
IS
PERFORMED
WITH
ALL
DATA
EXAMPLES
PARAMETRIC
MODELS
ONCE
FITTED
ONLY
PARAMETERS
NEED
TO
BE
STORED
THEY
ARE
MUCH
MORE
EFFICIENT
IN
TERMS
OF
COMPUTATION
BUT
THE
MODEL
NEEDS
TO
BE
PICKED
IN
ADVANCE
WE
HAVE
A
SET
D
OF
X
Y
PAIRS
WE
HAVE
A
NEW
DATA
POINT
X
AND
WANT
TO
ASSIGN
IT
A
CLASS
Y
HOW
ALGORITHM
STEP
ESTIMATE
P
Y
AND
P
Y
STEP
ESTIMATE
P
X
Y
AND
P
X
Y
USING
NONPARAMETRIC
ESTIMATION
METHODS
AND
LABELS
STEP
CHOOSE
A
CLASS
BY
COMPARING
P
X
Y
P
Y
WITH
P
X
Y
P
Y
WE
HAVE
A
SET
D
OF
X
Y
PAIRS
WE
HAVE
A
NEW
DATA
POINT
X
AND
WANT
TO
ASSIGN
IT
A
CLASS
Y
HOW
ALGORITHM
K
NEAREST
NEIGHBORS
STEP
FIND
THE
CLOSEST
K
EXAMPLES
TO
X
STEP
CHOOSE
A
CLASS
BY
CONSIDERING
THE
MAJORITY
OF
THE
CLASS
LABELS
A
SPECIAL
CASE
THE
NEAREST
NEIGHBOUR
ALGORITHM
CS
MACHINE
LEARNING
LECTURE
DENSITY
ESTIMATION
MILOS
HAUSKRECHT
SENNOTT
SQUARE
PROBABILITY
WELL
DEFINED
THEORY
FOR
REPRESENTING
AND
MANIPULATING
UNCERTAINTY
AXIOMS
OF
PROBABILITY
LET
A
AND
B
BE
TWO
EVENTS
THEN
P
A
P
TRUE
AND
P
FALSE
P
A
B
P
A
P
B
P
A
B
PROBABILITY
LET
A
BE
AN
EVENT
AND
A
ITS
COMPLEMENT
THEN
P
A
P
A
P
A
A
P
FALSE
P
A
A
P
TRUE
JOINT
PROBABILITY
JOINT
PROBABILITY
LET
A
AND
B
BE
TWO
EVENTS
THE
PROBABILITY
OF
AN
EVENT
A
B
OCCURRING
JOINTLY
P
A
B
P
A
B
WE
CAN
ADD
MORE
EVENTS
SAY
A
B
C
P
A
B
C
P
A
B
C
INDEPENDENCE
INDEPENDENCE
LET
A
B
BE
TWO
EVENTS
THE
EVENTS
ARE
INDEPENDENT
IF
P
A
B
P
A
P
B
CONDITIONAL
PROBABILITY
CONDITIONAL
PROBABILITY
LET
A
B
BE
TWO
EVENTS
THE
CONDITIONAL
PROBABILITY
OF
A
GIVEN
B
IS
DEFINED
AS
P
A
B
P
A
B
P
B
PRODUCT
RULE
A
REWRITE
OF
THE
CONDITIONAL
PROBABILITY
P
A
B
P
A
B
P
B
CS
MACHINE
LEARNING
BAYES
THEOREM
BAYES
THEOREM
P
A
B
P
B
A
P
A
P
B
WHY
P
A
B
P
A
B
P
B
P
A
B
P
A
B
P
B
A
P
A
P
B
A
P
A
P
B
RANDOM
VARIABLE
A
FUNCTION
THAT
MAPS
OBSERVED
QUANTITIES
TO
REAL
VALUED
OUTCOMES
BINARY
RANDOM
VARIABLES
MAPPED
TO
EXAMPLE
TAIL
MAPPED
TO
HEAD
MAPPED
TO
NOTE
ONLY
ONE
VALUE
FOR
EACH
OUTCOME
EITHER
OR
P
X
P
X
PROBABILITY
OF
TAIL
PROBABILITY
OF
HEAD
PROBABILITY
DISTRIBUTION
P
X
ASSIGNS
A
PROBABILITY
TO
EACH
POSSIBLE
OUTCOME
CS
MACHINE
LEARNING
DISCRETE
RANDOM
VARIABLE
X
BASED
ON
TAIL
HEAD
COIN
TOSS
X
BASED
ON
THE
ROLL
OF
A
DICE
P
X
ASSIGNS
A
PROBABILITY
TO
EACH
POSSIBLE
OUTCOMES
CONTINUOUS
X
HEIGHT
OF
A
PERSON
P
X
DEFINED
IN
TERMS
OF
THE
PROBABILITY
DENSITY
FUNCTION
P
X
DX
CS
MACHINE
LEARNING
DATA
DENSITY
ESTIMATION
D
DN
DI
XI
A
VECTOR
OF
ATTRIBUTE
VALUES
OBJECTIVE
ESTIMATE
THE
UNDERLYING
PROBABILITY
DISTRIBUTION
OVER
VARIABLES
X
P
X
USING
EXAMPLES
IN
D
STANDARD
IID
ASSUMPTIONS
SAMPLES
ARE
INDEPENDENT
OF
EACH
OTHER
COME
FROM
THE
SAME
IDENTICAL
DISTRIBUTION
FIXED
P
X
LEARNING
VIA
PARAMETER
ESTIMATION
IN
THIS
LECTURE
WE
CONSIDER
PARAMETRIC
DENSITY
ESTIMATION
BASIC
SETTINGS
A
SET
OF
RANDOM
VARIABLES
X
XD
A
MODEL
OF
THE
DISTRIBUTION
OVER
VARIABLES
IN
X
WITH
PARAMETERS
Pˆ
X
DATA
D
DN
OBJECTIVE
FIND
PARAMETERS
THE
BEST
SUCH
THAT
P
X
FITS
DATA
D
ML
PARAMETER
ESTIMATION
MODEL
Pˆ
X
P
X
Θ
DATA
D
DN
MAXIMUM
LIKELIHOOD
ML
FIND
THAT
MAXIMIZES
MAX
P
D
P
D
P
D
P
DN
INDEPENDENT
P
P
P
DN
EXAMPLES
I
P
DI
N
LOG
P
D
LOGP
DI
I
PARAMETER
ESTIMATION
COIN
EXAMPLE
COIN
EXAMPLE
WE
HAVE
A
COIN
THAT
CAN
BE
BIASED
OUTCOMES
TWO
POSSIBLE
VALUES
HEAD
OR
TAIL
DATA
D
A
SEQUENCE
OF
OUTCOMES
XI
SUCH
THAT
HEAD
TAIL
XI
XI
MODEL
PROBABILITY
OF
A
HEAD
PROBABILITY
OF
A
TAIL
OBJECTIVE
WE
WOULD
LIKE
TO
ESTIMATE
THE
PROBABILITY
OF
A
HEAD
ˆ
FROM
DATA
PROBABILITY
OF
AN
OUTCOME
DATA
D
A
SEQUENCE
OF
OUTCOMES
XI
SUCH
THAT
HEAD
TAIL
XI
XI
MODEL
PROBABILITY
OF
A
HEAD
PROBABILITY
OF
A
TAIL
ASSUME
WE
KNOW
THE
PROBABILITY
PROBABILITY
OF
AN
OUTCOME
OF
A
COIN
FLIP
XI
P
XI
XI
XI
BERNOULLI
DISTRIBUTION
COMBINES
THE
PROBABILITY
OF
A
HEAD
AND
A
TAIL
SO
THAT
XI
IS
GOING
TO
PICK
ITS
CORRECT
PROBABILITY
GIVES
FOR
XI
GIVES
FOR
XI
PROBABILITY
OF
A
SEQUENCE
OF
OUTCOMES
DATA
D
A
SEQUENCE
OF
OUTCOMES
XI
SUCH
THAT
HEAD
TAIL
XI
XI
MODEL
PROBABILITY
OF
A
HEAD
PROBABILITY
OF
A
TAIL
ASSUME
A
SEQUENCE
OF
COIN
FLIPS
D
H
H
T
H
T
H
ENCODED
AS
D
WHAT
IS
THE
PROBABILITY
OF
OBSERVING
A
DATA
SEQUENCE
D
P
D
MAXIMUM
LIKELIHOOD
ML
ESTIMATE
LIKELIHOOD
OF
DATA
P
D
I
XI
XI
MAXIMUM
LIKELIHOOD
ESTIMATE
ML
ARG
MAX
P
D
OPTIMIZE
LOG
LIKELIHOOD
THE
SAME
AS
MAXIMIZING
LIKELIHOOD
N
L
D
LOG
P
D
LOG
XI
XI
N
I
N
N
XI
I
LOG
XI
LOG
LOG
XI
I
LOG
I
XI
NUMBER
OF
HEADS
SEEN
NUMBER
OF
TAILS
SEEN
MAXIMUM
LIKELIHOOD
ML
ESTIMATE
OPTIMIZE
LOG
LIKELIHOOD
L
D
LOG
LOG
SET
DERIVATIVE
TO
ZERO
L
D
SOLVING
MAXIMUM
LIKELIHOOD
ESTIMATE
EXAMPLE
ASSUME
THE
UNKNOWN
AND
POSSIBLY
BIASED
COIN
PROBABILITY
OF
THE
HEAD
IS
DATA
H
H
T
T
H
H
T
H
T
H
T
T
T
H
T
H
H
H
H
T
H
H
H
H
T
HEADS
TAILS
WHAT
IS
THE
ML
ESTIMATE
OF
THE
PROBABILITY
OF
A
HEAD
AND
A
TAIL
MAXIMUM
LIKELIHOOD
ESTIMATE
EXAMPLE
ASSUME
THE
UNKNOWN
AND
POSSIBLY
BIASED
COIN
PROBABILITY
OF
THE
HEAD
IS
DATA
H
H
T
T
H
H
T
H
T
H
T
T
T
H
T
H
H
H
H
T
H
H
H
H
T
HEADS
TAILS
WHAT
IS
THE
ML
ESTIMATE
OF
THE
PROBABILITY
OF
HEAD
AND
TAIL
HEAD
ML
N
N
N
TAIL
ML
N
N
N
MAXIMUM
A
POSTERIORI
ESTIMATE
MAXIMUM
A
POSTERIORI
ESTIMATE
SELECTS
THE
MODE
OF
THE
POSTERIOR
DISTRIBUTION
MAP
LIKELIHOOD
OF
DATA
ARG
MAX
P
D
PRIOR
P
D
P
D
P
P
D
VIA
BAYES
RULE
NORMALIZING
FACTOR
P
D
I
XI
XI
P
IS
THE
PRIOR
PROBABILITY
ON
HOW
TO
CHOOSE
THE
PRIOR
PROBABILITY
CS
MACHINE
LEARNING
PRIOR
DISTRIBUTION
CHOICE
OF
PRIOR
BETA
DISTRIBUTION
P
BETA
X
A
GAMMA
FUNCTION
X
X
X
FOR
INTEGER
VALUES
OF
X
N
N
WHY
TO
USE
BETA
DISTRIBUTION
BETA
DISTRIBUTION
FITS
BERNOULLI
TRIALS
CONJUGATE
CHOICES
P
D
POSTERIOR
DISTRIBUTION
IS
AGAIN
A
BETA
DISTRIBUTION
P
D
P
D
BETA
P
D
BETA
CS
MACHINE
LEARNING
BETA
DISTRIBUTION
P
BETA
A
B
A
B
A
B
A
B
CS
MACHINE
LEARNING
POSTERIOR
DISTRIBUTION
BETA
BETA
P
D
P
D
BETA
P
D
BETA
N
CS
MACHINE
LEARNING
MAXIMUM
A
POSTERIOR
PROBABILITY
MAXIMUM
A
POSTERIORI
ESTIMATE
SELECTS
THE
MODE
OF
THE
POSTERIOR
DISTRIBUTION
P
D
P
D
BETA
P
D
BETA
N
NOTICE
THAT
PARAMETERS
OF
THE
PRIOR
ACT
LIKE
COUNTS
OF
HEADS
AND
TAILS
SOMETIMES
THEY
ARE
ALSO
REFERRED
TO
AS
PRIOR
COUNTS
CS
MACHINE
LEARNING
MAP
ESTIMATE
EXAMPLE
ASSUME
THE
UNKNOWN
AND
POSSIBLY
BIASED
COIN
PROBABILITY
OF
THE
HEAD
IS
DATA
H
H
T
T
H
H
T
H
T
H
T
T
T
H
T
H
H
H
H
T
H
H
H
H
T
HEADS
TAILS
ASSUME
P
BETA
WHAT
IS
THE
MAP
ESTIMATE
CS
MACHINE
LEARNING
MAP
ESTIMATE
EXAMPLE
ASSUME
THE
UNKNOWN
AND
POSSIBLY
BIASED
COIN
PROBABILITY
OF
THE
HEAD
IS
DATA
H
H
T
T
H
H
T
H
T
H
T
T
T
H
T
H
H
H
H
T
H
H
H
H
T
HEADS
TAILS
ASSUME
P
BETA
WHAT
IS
THE
MAP
ESTIMATE
MAP
N
CS
MACHINE
LEARNING
MAP
ESTIMATE
EXAMPLE
NOTE
THAT
THE
PRIOR
AND
DATA
FIT
DATA
LIKELIHOOD
ARE
COMBINED
THE
MAP
CAN
BE
BIASED
WITH
LARGE
PRIOR
COUNTS
IT
IS
HARD
TO
OVERTURN
IT
WITH
A
SMALLER
SAMPLE
SIZE
DATA
H
H
T
T
H
H
T
H
T
H
T
T
T
H
T
H
H
H
H
T
H
H
H
H
T
HEADS
TAILS
ASSUME
P
P
BETA
BETA
MAP
MAP
CS
MACHINE
LEARNING
NON
LINEAR
EXTENSION
OF
LOGISTIC
REGRESSION
USE
FEATURE
BASIS
FUNCTIONS
TO
MODEL
NONLINEARITIES
THE
SAME
TRICK
AS
USED
FOR
THE
LINEAR
REGRESSION
LINEAR
REGRESSION
LOGISTIC
REGRESSION
F
X
WJ
J
X
J
F
X
G
WJ
J
X
J
J
X
AN
ARBITRARY
FUNCTION
OF
X
X
X
XD
WM
M
X
CS
MACHINE
LEARNING
RULES
FOR
SUBMITTING
PROGRAMS
HTTP
PEOPLE
CS
PITT
EDU
MILOS
COURSES
PROGRAMSUBMISSIONS
HTML
CS
RULES
FOR
SUBMITTING
PROGRAMMING
ASSIGNMENTS
RULES
PROGRAMS
ALL
PROGRAMS
SUBMITTED
TO
US
SHOULD
BE
WRITTEN
IN
MATLAB
FILES
WITH
M
EXTENSION
DATA
PLEASE
AVOID
SENDING
US
BACK
DATA
FILES
UNLESS
YOUR
ARE
EXPLICTLY
ASKED
TO
DO
SO
IN
THE
ASSIGNMENT
SUBMISSION
FORMAT
WINDOWS
SUBMIT
THE
SOURCE
CODE
FOR
ALL
PROGRAMS
AND
FILES
REQUESTED
IN
A
SINGLE
ZIP
FILE
CREATED
WITH
WINZIP
PKZIP
USE
YOURFIRSTNAMEYOURLASTNAME
ZIP
TO
NAME
THE
FILE
FOR
EXAMPLE
JANEDOE
ZIP
UNIX
SUBMIT
ALL
PROGRAMS
AND
FILES
REQUESTED
IN
A
SINGLE
TAR
ARCHIVE
FILE
USE
YOURFIRSTNAMEYOURLASTNAME
TAR
TO
NAME
THE
FILE
FOR
EXAMPLE
JANEDOE
TAR
CREATING
TAR
FILES
IN
UNIX
TO
CREATE
A
TAR
FILE
USE
TAR
CVF
JANEDOE
TAR
IN
UNIX
WHICH
ARCHIVES
ALL
THE
FILES
IN
THE
CURRENT
DIRECTORY
AND
STORES
THEM
IN
THE
FILE
JANEDOE
TAR
TO
ARCHIVE
ONLY
M
FILES
USE
TAR
CVF
JANEDOE
TAR
M
TO
EXTRACT
THE
ARCHIVED
FILES
IN
THE
CURRENT
DIRECTORY
USE
TAR
XVF
JANEDOE
TAR
COMMAND
PLEASE
AVOID
INCLUDING
DIRECTORY
STRUCTURE
INTO
THE
ARCHIVE
FILES
SUBMISSION
THE
FILE
YOURFIRSTNAMEYOURLASTNAME
TAR
OR
ZIP
SHOULD
BE
SUBMITTED
USING
THE
COURSEWEB
SYSTEM
HTTP
COURSEWEB
PITT
EDU
MAINTAINED
BY
THE
UNIVERSITY
OF
PITTSBURGH
ONCE
YOU
LOGIN
TO
THE
SYSTEM
PLEASE
SELECT
CS
FROM
THE
COURSE
MENU
AND
USE
THE
ASSIGNMENTS
BUTTON
TO
SEE
THE
ASSIGNMENT
AND
THE
ASSIGNMENT
SUBMISSION
PAGE
THE
ABOVE
FILE
SHOULD
BE
SUBMITTED
USING
THE
FILE
ATTACHMENT
OPTION
IN
EXCEPTIONAL
CASES
WHEN
COURSEWEB
SYSTEM
DOES
NOT
WORK
YOU
MAY
SUBMIT
YOUR
ARCHIVE
FILE
VIA
EMAIL
SENT
TO
THE
TA
OF
THE
COURSE
LAST
UPDATED
BY
MILOS
ON
UNIVERSITY
OF
PITTSBURGH
INTRODUCTION
TO
MACHINE
LEARNING
HANDOUT
PROFESSOR
MILOS
HAUSKRECHT
MARCH
PROBLEM
ASSIGNMENT
DUE
THURSDAY
MARCH
PROBLEM
BAYESIAN
BELIEF
NETWORKS
ASSUME
THE
BAYESIAN
BELIEF
NETWORK
IN
THE
FIGURE
BELOW
ASSUME
THAT
EVERY
VARIABLE
IN
THE
NETWORK
IS
BINARY
REPRESENTING
T
F
VALUES
EXCEPT
VARIABLE
D
THAT
CAN
TAKE
ON
THREE
POSSIBLE
VALUES
T
F
X
X
STANDS
FOR
UNDECIDED
ASSUME
YOU
WANT
TO
COMPUTE
P
B
T
E
T
SHOW
HOW
WOULD
YOU
COMPUTE
THE
EXPRESSION
MORE
EFFICIENTLY
YOU
DO
NOT
HAVE
TO
PROVIDE
COUNTS
OF
OPERATIONS
IT
IS
SUFFICIENT
TO
ILLUSTRATE
WHAT
YOU
WOULD
DO
TO
IMPROVE
THE
EFFICIENCY
OF
THE
INFERENCE
PROBLEM
PNEUMONIA
DIAGNOSIS
ASSUME
A
BAYESIAN
BELIEF
NETWORK
WITH
THE
NAIVE
BAYES
STRUCTURE
FOR
A
SIMPLIFIED
VERSION
OF
THE
PNEUMONIA
PROBLEM
THE
VARIABLE
PNEUMONIA
IS
AT
THE
ROOT
OF
THE
NAIVE
BAYES
STRUCTURE
AND
FEATURES
ATTRIBUTES
FEVER
PALENESS
COUGH
AND
HIGHWBCCOUNT
ARE
CONDITIONALLY
INDEPENDENT
GIVEN
PNEUMONIA
ASSUME
THAT
RANDOM
VARIABLES
IN
OUR
MODEL
ARE
DISCRETE
WITH
THE
FOLLOWING
SET
OF
VALUES
PNEUMONIA
TRUE
FALSE
FEVER
TRUE
FALSE
PALENESS
TRUE
FALSE
COUGH
TRUE
FALSE
HIGHWBCCOUNT
TRUE
FALSE
PART
A
WRITE
AND
SUBMIT
A
PROGRAM
M
THAT
TAKES
THE
DATA
IN
THE
FILE
PNEUMONIA
TEX
AND
LEARNS
THE
ML
ESTIMATES
OF
PARAMETERS
OF
THE
BBN
NETWORK
THE
PNEUMONIA
FILE
INCLUDES
EXAMPLES
IN
ROWS
THE
FEATURES
ARE
IN
COLUMNS
IN
THE
FOL
LOWING
ORDER
FEVER
PALENESS
COUGH
HIGHWBCCOUNT
LAST
COLUMN
REPRESENTS
PNEUMONIA
VARIABLE
THE
DATA
ARE
REPRESENTED
SUCH
THAT
TRUE
CORRESPONDS
TO
AND
FALSE
TO
REPORT
THE
PARAMETERS
OF
THE
NETWORK
PART
B
ASSUME
THAT
THE
PATIENT
COMES
WITH
THE
FOLLOWING
SET
SYMPTOMS
FEVER
AND
COUGH
ARE
TRUE
PALENESS
AND
HIGHWBCCOUNT
ARE
FALSE
WHAT
IS
THE
PROBABILITY
P
P
NEUMONIA
T
F
EVER
T
P
ALENESS
F
COUGH
T
HIGHW
BCCOUNT
F
THAT
IS
THE
PROBABILITY
THAT
THE
PATIENT
SUFFERS
FROM
PNEUMONIA
GIVEN
THE
SYMPTOMS
SIMPLIFY
THE
EXPRESSION
AS
MUCH
AS
POSSIBLE
BEFORE
PLUGGING
IN
THE
VALUES
PART
C
ASSUME
THAT
THE
PATIENT
REPORTS
COUGH
AND
A
FEVER
THEY
ARE
TRUE
AND
VALUES
OF
PALENESS
AND
HIGHWBCCOUNT
ARE
NOT
KNOWN
SIMPLIFY
THE
EXPRESSION
AS
MUCH
AS
POSSI
BLE
COMPUTE
THE
PROBABILITY
P
P
NEUMONIA
T
F
EVER
T
COUGH
T
OF
THE
PATIENT
SUFFERING
FROM
THE
PNEUMONIA
GIVEN
THE
SYMPTOMS
PART
D
WRITE
AND
SUBMIT
A
MATLAB
PROGRAM
THAT
READS
IN
A
COMBI
NATION
OF
PATIENT
SYMPTOMS
THE
VALUES
OF
FEVER
PALENESS
COUGH
HIGHWBCCOUNT
FROM
THE
FILE
EXAMPLE
TXT
AND
COMPUTES
THE
PROBABILITY
OF
P
P
NEUMONIA
T
RUE
CURRENTSYMPTOMS
THE
VALUES
OF
SYMPTOMS
FOR
THE
CURRENT
PATIENT
CASE
ARE
GIVEN
IN
THE
FOLLOWING
ORDER
F
EVER
P
ALENESS
COUGH
HIGHW
BCCOUNT
THE
VALUES
ARE
ENCODED
AS
FOLLOWS
FOR
FALSE
FOR
TRUE
AND
FOR
NOT
GIVEN
NOT
KNOWN
FOR
EXAMPLE
THE
SYMPTOMS
OF
THE
PATIENT
CASE
IN
PART
B
ARE
ENCODED
AS
A
VECTOR
OF
VALUES
AND
SYMPTOMS
IN
PART
C
AS
NOTE
PLEASE
NOTE
THAT
YOUR
PROGRAM
SHOULD
WORK
ON
AN
ARBITRARY
COMBINATION
OF
INPUT
VALUES
IN
EXAMPLE
TXT
FILE
UNIVERSITY
OF
PITTSBURGH
INTRODUCTION
TO
MACHINE
LEARNING
HANDOUT
PROFESSOR
MILOS
HAUSKRECHT
JANUARY
PROBLEM
ASSIGNMENT
DUE
THURSDAY
JANUARY
PROBLEM
INSTALL
MATLAB
ON
YOUR
COMPUTER
OR
ACCESS
IT
IN
ONE
OF
THE
CSSD
LABS
PROBLEM
EXPLORATORY
DATA
ANALYSIS
IN
THIS
PROBLEM
WE
WILL
EXPLORE
AND
ANALYZE
THE
DATASET
PIMA
TXT
PROVIDED
ON
THE
COURSE
WEB
PAGE
TO
DO
THE
ANALYSIS
YOU
WILL
NEED
TO
WRITE
SHORT
PROGRAMS
KEEP
THE
CODE
YOU
WRITE
FOR
FUTURE
PROBLEM
SETS
THE
PIMA
TXT
IS
DESCRIBED
IN
THE
FILE
PIMA
DESC
TXT
THE
DATASET
CONSISTS
OF
ATTRIBUTES
AND
A
BINARY
ATTRIBUTE
DEFINING
THE
CLASS
LABEL
THE
PRESENCE
OF
DIABETES
DATA
ENTRIES
ARE
ORGANIZED
IN
ROWS
SUCH
THAT
ATTRIBUTES
COME
FIRST
AND
THE
CLASS
LABEL
IS
LAST
ANSWER
THE
FOLLOWING
QUESTIONS
WITH
THE
HELP
OF
MATLAB
A
WHAT
IS
THE
RANGE
MINIMUM
AND
MAXIMUM
VALUE
FOR
EACH
OF
THE
ATTRIBUTES
B
WHAT
ARE
THE
MEANS
AND
VARIANCES
OF
EVERY
ATTRIBUTE
C
CALCULATE
AND
REPORT
CORRELATIONS
BETWEEN
THE
FIRST
ATTRIBUTES
IN
COLUMNS
AND
THE
TARGET
CLASS
ATTRIBUTE
COLUMN
USE
MATLAB
CORRCOEF
FUNCTION
TO
DO
THE
CALCULATIONS
WHAT
IS
THE
ATTRIBUTE
WITH
THE
HIGHEST
POSITIVE
CORRELATION
TO
THE
TARGET
ATTRIBUTE
DO
YOU
THINK
IT
IS
THE
MOST
OR
THE
LEAST
HELPFUL
ATTRIBUTE
IN
PREDICTING
THE
TARGET
CLASS
EXPLAIN
D
CALCULATE
ALL
CORRELATIONS
BETWEEN
ATTRIBUTES
USING
THE
CORRCOEF
FUNCTION
WHICH
TWO
ATTRIBUTES
HAVE
THE
LARGEST
MUTUAL
CORRELATION
IN
THE
DATASET
E
ASSUME
WE
WANT
TO
PREDICT
A
TARGET
CLASS
GIVEN
ALL
ATTRIBUTES
WHAT
DO
YOU
THINK
DOES
IT
HELP
OR
NOT
IN
PREDICTION
TO
HAVE
ATTRIBUTES
THAT
ARE
FULLY
CORRELATED
EXPLAIN
WHILE
THE
ANALYSIS
USING
BASIC
STATISTICS
AS
PERFORMED
ABOVE
CONVEYS
A
LOT
OF
INFORMATION
ABOUT
THE
DATA
AND
LETS
US
MAKE
SOME
CONCLUSIONS
ABOUT
THE
IMPORTANCE
OF
ATTRIBUTES
OR
THEIR
RELATION
IT
IS
OFTEN
VERY
USEFUL
TO
INSPECT
THE
DATA
VISUALLY
AND
GET
MORE
INSIGHT
INTO
VARIOUS
SHAPES
AND
PATTERNS
THEY
HIDE
IN
THE
FOLLOWING
WE
WILL
INSPECT
THE
DATA
USING
HISTOGRAMS
AND
SCATTER
PLOTS
F
HISTOGRAM
ANALYSIS
GIVES
US
MORE
INFORMATION
ABOUT
THE
DISTRIBUTION
OF
ATTRIBUTE
VALUES
WRITE
AND
SUBMIT
A
MATLAB
FUNCTION
HISTOGRAM
ANALYSIS
THAT
TAKES
THE
DATA
FOR
AN
ATTRIBUTE
AS
A
VECTOR
AND
PLOTS
A
HISTOGRAM
WITH
BINS
USING
MATLAB
HIST
FUNCTION
ANALYZE
ATTRIBUTES
IN
THE
DATA
USING
THE
FUNCTION
ANSWER
THE
FOLLOWING
QUESTIONS
WHICH
HISTOGRAM
RESAMBLES
MOST
THE
NORMAL
DISTRIBUTION
IN
YOUR
REPORT
SHOW
AT
LEAST
TWO
HISTOGRAMS
INCLUDING
THE
CHOICE
YOU
PICKED
AS
THE
MOST
NORMALLY
DISTRIBUTED
ATTRIBUTE
G
SCATTER
PLOTS
PLOTS
LET
US
INSPECT
THE
RELATIONS
BETWEEN
PAIRS
OF
ATTRIBUTES
WRITE
AND
SUBMIT
A
FUNCTION
SCATTER
PLOT
THAT
TAKES
PAIRS
OF
VALUES
FOR
TWO
ATTRIBUTES
AND
PLOTS
THEM
AS
POINTS
IN
USE
MATLAB
FUNCTION
SCATTER
TO
DO
THE
PLOT
ANALYZE
THE
PAIRWISE
RELATIONS
BETWEEN
ATTRIBUTES
IN
THE
PIMA
DATASET
USING
THE
SCATTER
PLOT
FUNCTION
ANSWER
THE
FOLLOWING
QUESTIONS
IS
THERE
A
SCATTER
PLOT
THAT
INDICATES
POSSIBLE
LINEAR
DEPENDENCY
BETWEEN
TWO
VARIABLES
SELECT
TWO
RANDOM
SCATTER
PLOTS
AND
INCLUDE
THEM
IN
THE
REPORT
WITH
EVERY
PLOT
INCLUDE
THE
CORRESPONDING
ATTRIBUTE
NAMES
PROBLEM
DATA
PREPROCESSING
BEFORE
APPLYING
LEARNING
ALGORITHMS
SOME
DATA
PREPROCESSING
MAY
BE
NECESSARY
TO
PRACTICE
MATLAB
WE
WILL
WRITE
PROGRAMS
FOR
TWO
POSSIBLE
PREPROCESSING
TASKS
NORMALIZATION
AND
DISCRETIZATION
OF
CONTINUOUS
VALUES
A
WRITE
AND
SUBMIT
A
FUNCTION
NORMALIZE
THAT
TAKES
AN
UNNORMALIZED
VECTOR
OF
ATTRIBUTE
VALUES
AND
RETURNS
THE
VECTOR
OF
VALUES
NORMALIZED
ACCORDING
TO
THE
DATA
MEAN
AND
STANDARD
DEVIATION
THE
NORMALIZED
VALUE
SHOULD
BE
X
ΜX
XNORM
ΣX
WHERE
X
IS
AN
UNNORMALIZED
VALUE
ΜX
IS
THE
MEAN
VALUE
OF
THE
ATTRIBUTE
IN
THE
DATA
AND
ΣX
ITS
STANDARD
DEVIATION
TEST
YOUR
FUNCTION
ON
ATTRIBUTE
OF
THE
PIMA
DATASET
REPORT
NORMALIZED
VALUES
OF
THE
ATTRIBUTE
FOR
THE
FIRST
FIVE
ENTRIES
IN
THE
DATASET
B
WRITE
AND
SUBMIT
A
FUNCTION
DISCRETIZE
ATTRIBUTE
THAT
TAKES
A
VECTOR
OF
ATTRIBUTE
VALUES
A
NUMBER
K
NUMBER
OF
BINS
AND
ASSINGS
EACH
VALUE
TO
ONE
OF
THE
K
BINS
BINS
ARE
OF
EQUAL
LENGTH
AND
SHOULD
COVER
THE
RANGE
OF
VALUES
THAT
IS
DETERMINED
BY
THE
MIN
AND
THE
MAX
OPERATIONS
ON
THE
VECTOR
EVERY
BIN
IS
GIVEN
A
NUMERICAL
LABEL
SUCH
THAT
THE
SMALLEST
VALUE
IS
IN
BIN
AND
THE
LARGEST
ATTRIBUTE
VALUE
IS
IN
BIN
K
THE
BIN
LABEL
REPRESENTS
THE
RESULT
OF
DISCRETIZATION
TEST
YOUR
FUNCTION
ON
ATTRIBUTE
OF
THE
PIMA
DATASET
ASSUME
WE
USE
BINS
REPORT
NEW
DISCRETIZED
VALUES
OF
THE
ATTRIBUTE
FOR
THE
FIRST
FIVE
ENTRIES
IN
THE
DATASET
PROBLEM
DATA
SET
SPLITTING
IN
THIS
PROBLEM
WE
PRACTICE
A
SPLITTING
OF
THE
DATASET
ALONG
AN
ATTRIBUTE
VALUE
AND
B
A
RANDOM
SPLITTING
OF
THE
DATASET
INTO
THE
TRAINING
AND
TESTING
SETS
A
SPLIT
PIMA
TXT
DATA
INTO
TWO
DATA
SUBSETS
ONE
THAT
INCLUDES
ONLY
EXAMPLES
WITH
CLASS
LABEL
THE
OTHER
ONE
WITH
CLASS
VALUES
CALCULATE
AND
REPORT
THE
MEAN
AND
STANDARD
DEVIATION
OF
EACH
ATTRIBUTE
IN
THESE
TWO
SUBSETS
HINT
TRY
TO
USE
MATLAB
FUNCTION
FIND
TO
SPLIT
THE
DATA
B
WRITE
AND
SUBMIT
A
FUNCTION
THAT
TAKES
THE
DATASET
REPRESENTED
AS
A
MATRIX
AND
THE
PROBABILITY
PTRAIN
OF
SELECTING
THE
DATA
ENTRY
A
ROW
IN
THE
MATRIX
INTO
THE
TRAINING
SET
THE
FUNCTION
SHOULD
RETURN
TWO
NONOVERLAPPING
DATASETS
THE
TRAINING
AND
TESTING
DATA
SUCH
THAT
EVERY
ENTRY
IS
SELECTED
TO
THE
TRAINING
SET
RANDOMLY
WITH
PROBABILITY
PTRAIN
TEST
YOUR
DIVIDESET
FUNCTION
ON
THE
PIMA
DATASET
RUN
THE
FUNCTION
TIMES
WITH
PROBABILITY
PTRAIN
AND
REPORT
THE
AVERAGE
LENGTH
OF
THE
TRAINING
DATASET
C
IF
YOUR
CODE
TO
PART
B
IS
CORRECT
YOU
SHOULD
SEE
SOME
VARIATION
IN
THE
SIZE
OF
THE
TRAINING
SETS
WRITE
AND
SUBMIT
A
FUNCTION
THAT
TAKES
THE
DATASET
REPRESENTED
AS
A
MATRIX
AND
THE
PROBABILITY
PTRAIN
AND
RETURNS
TWO
NONOVERLAPPING
DATASETS
THE
TRAINING
AND
TESTING
DATA
THAT
MIMIC
CLOSELY
THE
DISTRIBUTION
DEFINED
BY
PTRAIN
BASICALLY
YOUR
FUNCTION
SHOULD
DECIDE
FIRST
ON
THE
NUMBER
OF
EXAMPLES
THAT
WILL
GO
INTO
TRAINING
AND
TEST
SETS
AND
AFTER
THAT
CHOOSE
RANDOMLY
EXAMPLES
THAT
WILL
GO
INTO
EACH
SET
THE
ALGORITHM
IF
YOU
RUN
IT
SHOULD
ALWAYS
GIVE
YOU
DIFFERENT
TRAINING
AND
TEST
SETS
BUT
THEIR
SIZES
SHOULD
STAY
THE
SAME
FOR
THE
SAME
DATASET
UNIVERSITY
OF
PITTSBURGH
MACHINE
LEARNING
HANDOUT
PROFESSOR
MILOS
HAUSKRECHT
JANUARY
PROBLEM
ASSIGNMENT
DUE
THURSDAY
JANUARY
PROBLEM
MEAN
ESTIMATES
AND
THE
EFFECT
OF
THE
SAMPLE
SIZE
IN
THIS
PROBLEM
WE
STUDY
THE
INFLUENCE
OF
THE
SAMPLE
SIZE
ON
THE
ESTIMATE
OF
THE
MEAN
THE
DATA
FOR
THIS
EXPERIMENTS
ARE
IN
FILE
MEAN
STUDY
DATA
TXT
IN
THE
HOMEWORK
ASSIGNMENT
FOLDER
THE
DATA
WERE
GENERATED
FROM
THE
NORMAL
DISTRIBUTION
WITH
MEAN
AND
STANDARD
DEVIATION
PART
LOAD
THE
DATA
IN
THE
MEAN
STUDY
DATA
TXT
CALCULATE
AND
REPORT
THE
MEAN
AND
STANDARD
DEVIATION
OF
THE
DATA
PART
WRITE
AND
SUBMIT
A
FUNCTION
NEWDATA
SUBSAMPLE
DATA
K
THAT
RANDOMLY
SELECTS
K
INSTANCES
FROM
THE
DATA
IN
THE
MEAN
STUDY
DATA
TXT
PART
USE
THE
ABOVE
FUNCTION
TO
RANDOMLY
GENERATE
SUBSAMPLES
OF
THE
DATA
OF
SIZE
FOR
EACH
SUBSAMPLE
CALCULATE
ITS
MEAN
AND
SAVE
THE
RESULTS
IN
THE
VECTOR
OF
MEANS
PLOT
A
HISTOGRAM
OF
THE
MEAN
VALUES
USING
BINS
PART
INCLUDE
THE
HISTOGRAM
IN
YOUR
REPORT
ANALYZE
THE
MEAN
THAT
WAS
CALCULATED
IN
STEP
ON
ALL
EXAMPLES
IN
THE
DATASET
AND
THE
MEANS
CALCULATED
ON
SUBSAMPLES
OF
SIZE
REPORT
YOUR
OBSERVATIONS
PART
REPEAT
STEP
PART
BUT
NOW
GENERATE
SUBSAMPLES
OF
SIZE
INCLUDE
THE
HISTOGRAM
IN
THE
REPORT
AND
COMPARE
IT
TO
THE
HISTOGRAM
GENERATED
IN
FOR
SUBSAMPLES
OF
SIZE
AND
TO
THE
MEAN
OF
THE
ORIGINAL
DATA
WHAT
ARE
THE
DIFFERENCES
WHAT
CONCLUSIONS
CAN
YOU
MAKE
BY
COMPARING
MEANS
FOR
SUBSAMPLES
OF
SIZE
AND
PROBLEM
TRAIN
TEST
SPLITTING
USING
K
FOLD
CROSSVALIDATION
WHEN
WE
TESTING
THE
PERFORMANCE
OF
A
LEARNING
ALGORITHM
USING
A
SIMPLE
HOLDOUT
METHOD
THE
RESULTS
MAY
BE
BIASED
BY
THE
TRAINING
TESTING
DATA
SPLIT
TO
ALLEVIATE
THE
PROBLEM
VARIOUS
RANDOM
RESAMPLING
SCHEMES
SUCH
AS
K
FOLD
CROSS
VALIDATION
RANDOM
SUBSAMPLING
OR
BOOTSTRAP
SEE
LECTURE
NOTES
FOR
CLASS
CAN
BE
APPLIED
TO
ESTIMATE
THE
STATISTICS
OF
INTEREST
BY
AVERAGING
THE
RESULTS
ACROSS
MULTIPLE
TRAIN
TEST
SPLITS
PLEASE
DO
THE
FOLLOWING
TASKS
PART
PLEASE
WRITE
AND
SUBMIT
THE
FUNCTION
TRAIN
TEST
KFOLD
CROSSVALIDATION
DATA
K
M
THAT
TAKES
THE
DATA
K
THE
NUMBER
OF
FOLDS
AND
M
THE
TARGET
FOLD
AS
INPUTS
AND
RETURNS
THE
TRAINING
AND
TESTING
DATA
SETS
SUCH
THAT
THE
TESTING
SET
CORRESPONDS
TO
M
TH
FOLD
UNDER
THE
K
TH
FOLD
CROSS
VALIDATION
SCHEME
TO
IMPLEMENT
THE
PROCEDURE
PLEASE
PLACE
THE
FOLDS
OVER
INDEXES
OF
THE
DATA
BY
ASSURING
THAT
EACH
FOLD
HAS
EQUAL
NUMBER
OF
ENTRIES
THAT
DO
NOT
OVERLAP
IF
THIS
IS
NOT
POSSIBLE
THE
FOLD
SIZES
NUMBER
OF
INSTANCES
IN
EACH
FOLD
SHOULD
DIFFER
BY
AT
MOST
ONE
THE
FILE
SHOULD
BE
NAMED
KFOLD
CROSSVALIDATION
M
PART
RUN
TEST
YOUR
FUNCTION
ON
DATA
IN
THE
FILE
RESAMPLING
DATA
TXT
MORE
SPECIF
ICALLY
RUN
YOUR
KFOLD
CROSSVALIDATION
FUNCTION
ON
ALL
DATA
IN
THE
FILE
BY
SETTING
K
NUMBER
OF
FOLDS
TO
AND
BY
VARYING
THE
TEST
FOLD
INDEX
PARAMETER
M
FROM
TO
FOR
EACH
TEST
DATA
GENERATED
FOR
THE
DIFFERENT
VALUE
OF
M
THAT
WERE
RETURNED
BY
YOUR
FUNCTION
CALCULATE
THE
MEAN
AND
STD
AND
REPORT
THEM
PROBLEM
FUNCTION
DERIVATIVES
MACHINE
LEARNING
AS
A
FIELD
BUILDS
UPON
KNOWLEDGE
OF
MATH
STATISTICS
CONTROL
AND
DECISION
THEORIES
IN
MANY
CASES
THE
LEARNING
PROCESS
IS
FORMULATED
AS
AN
OPTIMIZATION
PROBLEM
WITH
SOME
OBJECTIVE
FUNCTION
SAY
MINΘ
F
Θ
WHERE
Θ
ARE
PARAMETERS
WE
WANT
TO
OPTIMIZE
IF
THIS
FUNCTION
IS
DIFFERENTIABLE
THE
OPTIMUM
EITHER
LOCAL
OR
GLOBAL
MUST
SATISFY
F
Θ
DΘ
IN
THIS
PROBLEM
WE
PRACTICE
THE
CALCULATION
OF
FUNCTION
DERIVATIVES
PLEASE
DERIVE
A
B
D
DX
D
C
DX
X
D
D
EX
DX
D
DX
SIN
X
E
D
F
DX
X
D
G
DX
X
D
H
J
DX
LN
X
D
DX
D
LN
XI
UNIVERSITY
OF
PITTSBURGH
INTRODUCTION
TO
MACHINE
LEARNING
HANDOUT
PROFESSOR
MILOS
HAUSKRECHT
JANUARY
PROBLEM
ASSIGNMENT
DUE
THURSDAY
FEBRUARY
PROBLEM
BERNOULLI
TRIALS
ASSUME
WE
HAVE
CONDUCTED
A
COIN
TOSS
EXPERIMENT
WITH
COIN
FLIPS
THE
RESULTS
OF
THE
EXPERIMENT
ARE
IN
FILE
COIN
TXT
WHERE
MEANS
A
HEAD
AND
MEANS
A
TAIL
ASSUME
THAT
Θ
REPRESENTS
THE
PROBABILITY
OF
OBSERVING
A
HEAD
A
WHAT
IS
AN
ML
ESTIMATE
OF
Θ
B
ASSUME
THE
PRIOR
ON
Θ
IS
DEFINED
BY
A
BETA
DISTRIBUTION
BETA
Θ
PLOT
AND
REPORT
BOTH
THE
PRIOR
AND
THE
POSTERIOR
DISTRIBUTION
ON
Θ
C
CALCULATE
AND
REPORT
THE
MAP
ESTIMATE
OF
THE
Θ
FOR
THE
PRIOR
IN
PART
B
D
REPEAT
PART
B
AND
C
BY
ASSUMING
THAT
THE
PRIOR
ON
Θ
FOLLOWS
BETA
Θ
PROBLEM
MULTIVARIATE
GAUSSIAN
ASSUME
THE
PAIRS
OF
REAL
VALUED
MEASUREMENTS
IN
FILE
GAUSSIAN
TXT
A
PLOT
THE
DATA
USING
THE
SCATTER
PLOT
MATLAB
FUNCTION
B
CALCULATE
AND
REPORT
THE
ML
ESTIMATE
OF
THE
MEAN
AND
THE
COVARIANCE
MATRIX
FROM
THE
DATA
PLOT
AND
REPORT
THE
RESULTING
GAUSSIAN
DISTRIBUTION
NOTE
YOU
NEED
TO
PLOT
THIS
IN
C
NOW
CONSIDER
EACH
MEASUREMENT
IN
GAUSSIAN
TXT
SEPARATELY
CALCULATE
THE
ML
ESTIMATE
OF
THE
MEAN
AND
VARIANCE
OF
THESE
MEASUREMENTS
PLOT
AND
REPORT
THE
INDI
VIDUAL
DISTRIBUTIONS
D
DO
YOU
BELIEVE
THE
MUTLIVARIATE
GAUSSIAN
MODEL
IS
A
BETTER
THAN
TWO
SEPARATE
UNIVARIATE
GAUSSIAN
MODELS
EXPLAIN
WHY
YES
OR
WHY
NOT
PROBLEM
POISSON
DISTRIBUTION
THE
POISSON
DISTRIBUTION
IS
USED
TO
MODEL
THE
NUMBER
OF
RANDOM
ARRIVALS
TO
A
SYSTEM
OVER
A
FIXED
PERIOD
OF
TIME
EXAMPLES
OF
SYSTEMS
IN
WHICH
EVENTS
ARE
DETERMINED
BY
RANDOM
ARRIVALS
ARE
ARRIVALS
OF
CUSTOMERS
REQUESTING
THE
SERVICE
OCCURENCE
OF
NATURAL
DISASTERS
SUCH
AS
FLOODS
ETC
THE
POISSON
DISTRIBUTION
IS
DEFINED
AS
E
ΛΛX
P
X
Λ
X
FOR
X
WHERE
Λ
IS
A
PARAMETER
THE
MEAN
OF
THE
POISSON
DISTRIBUTION
IS
Λ
ANSWER
THE
FOLLOWING
QUESTIONS
A
PLOT
AND
REPORT
THE
PROBABILITY
FUNCTION
FOR
POISSON
DISTRIBUTIONS
WITH
PARAMETERS
Λ
AND
Λ
NOTE
THAT
THE
POISSON
MODEL
IS
DEFINED
OVER
NONENEGATIVE
INTEGERS
ONLY
B
GIVEN
A
SET
OF
INDEPENDENT
OBSERVATIONS
XN
FROM
A
POISSON
DISTRIBUTION
THE
ML
ESTIMATE
OF
THE
PARAMETER
Λ
IS
N
ΛML
XI
N
I
ASSUME
THE
DATA
IN
POISSON
TXT
FILE
THAT
REPRESENT
THE
NUMBER
OF
INCOMING
PHONE
CALLS
RECEIVED
OVER
A
FIXED
PERIOD
OF
TIME
COMPUTE
AND
REPORT
THE
ML
ESTIMATE
OF
THE
PARAMETER
Λ
ALSO
PLOT
AND
REPORT
THE
PROBABILITY
FUCTION
FOR
THE
ML
PARAMETER
C
THE
CONJUGATE
PRIOR
FOR
Λ
DEFINING
THE
POISSON
DISTRIBUTION
IS
GAMMA
DISTRIBUTION
IT
IS
DEFINED
AS
P
Λ
A
B
BAΓ
A
Λ
A
E
Λ
PLOT
AND
REPORT
THE
GAMMA
DISTRIBUTION
FOR
THE
FOLLOWING
SET
OF
PARAMETERS
A
B
AND
A
B
D
ASSUMING
THE
PRIOR
DISTRIBUTION
ON
Λ
IS
GAMMA
Λ
A
B
THE
POSTERIOR
DISTRIBUTION
FOR
Λ
AFTER
SEEING
OBSERVATIONS
D
XN
IS
AGAIN
GAMMA
DISTRIBUTION
N
P
Λ
D
GAMMA
Λ
A
XI
B
I
NB
PLEASE
USE
DATA
IN
POISSON
TXT
TO
CALCULATE
AND
PLOT
THE
POSTERIOR
DISTRIBUTIONS
OF
Λ
FOR
BOTH
PRIORS
IN
PART
C
NOTE
IT
IS
NOT
NECCESSARY
TO
SUBMIT
ANY
MATLAB
CODE
FOR
THIS
ASSIGNMENT
JUST
INCLUDE
THE
PLOTS
IN
YOUR
REPORT
UNIVERSITY
OF
PITTSBURGH
INTRODUCTION
TO
MACHINE
LEARNING
HANDOUT
PROFESSOR
MILOS
HAUSKRECHT
FEBRUARY
PROBLEM
ASSIGNMENT
DUE
THURSDAY
FEBRUARY
LINEAR
REGRESSION
IN
THIS
PROBLEM
SET
WE
USE
THE
BOSTON
HOUSING
DATASET
FROM
THE
CMU
STATLIB
LIBRARY
THAT
CONCERNS
PRICES
OF
HOUSING
IN
BOSTON
SUBURBS
A
DATA
SAMPLE
CONSISTS
OF
ATTRIBUTE
VALUES
INDICATING
PARAMETERS
LIKE
CRIME
RATE
ACCESSIBILITY
TO
MAJOR
HIGHWAYS
ETC
AND
THE
MEDIAN
VALUE
OF
HOUSING
IN
THOUSANDS
WE
WOULD
LIKE
TO
PREDICT
THE
DATA
ARE
IN
THE
FILE
HOUSING
TXT
THE
DESCRIPTION
OF
THE
DATA
IS
IN
THE
FILE
HOUSING
DESC
TXT
ON
THE
COURSE
WEB
PAGE
PART
EXPLORATORY
DATA
ANALYSIS
EXAMINE
THE
DATASET
HOUSING
TXT
USING
MATLAB
ANSWER
THE
FOLLOWING
QUESTIONS
A
HOW
MANY
BINARY
ATTRIBUTES
ARE
IN
THE
DATA
SET
LIST
THE
ATTRIBUTES
B
CALCULATE
AND
REPORT
CORRELATIONS
IN
BETWEEN
THE
FIRST
ATTRIBUTES
COLUMNS
AND
THE
TARGET
ATTRIBUTE
COLUMN
WHAT
ARE
THE
ATTRIBUTE
NAMES
WITH
THE
HIGHEST
POSITIVE
AND
NEGATIVE
CORRELATIONS
TO
THE
TARGET
ATTRIBUTE
C
NOTE
THAT
THE
CORRELATION
IS
A
LINEAR
MEASURE
OF
SIMILARITY
EXAMINE
SCATTER
PLOTS
FOR
ATTRIBUTES
AND
THE
TARGET
ATTRIBUTE
USING
THE
FUNCTION
YOU
WROTE
IN
PROBLEM
SET
WHICH
SCATTER
PLOT
LOOKS
THE
MOST
LINEAR
AND
WHICH
LOOKS
THE
MOST
NONLINEAR
PLOT
THESE
SCATTER
PLOTS
AND
BRIEFLY
IN
SENTENCES
EXPLAIN
YOUR
CHOICE
D
CALCULATE
ALL
CORRELATIONS
BETWEEN
THE
COLUMNS
USING
THE
CORRCOEF
FUNCTION
WHICH
TWO
ATTRIBUTES
HAVE
THE
LARGEST
MUTUAL
CORRELATION
IN
THE
DATASET
PART
LINEAR
REGRESSION
OUR
GOAL
IS
TO
PREDICT
THE
MEDIAN
VALUE
OF
HOUSING
BASED
ON
THE
VALUES
OF
ATTRIBUTES
FOR
YOUR
CONVENIENCE
THE
DATA
HAS
BEEN
DIVIDED
INTO
TWO
DATASETS
A
TRAINING
DATASET
HOUSING
TRAIN
TXT
YOU
SHOULD
USE
IN
THE
LEARNING
PHASE
AND
A
TESTING
DATASET
HOUSING
TEST
TXT
TO
BE
USED
FOR
TESTING
ASSUME
THAT
WE
CHOOSE
A
LINEAR
REGRESSION
MODEL
TO
PREDICT
THE
TARGET
ATTRIBUTE
USING
MATLAB
A
WRITE
A
FUNCTION
LR
SOLVE
THAT
TAKES
X
AND
Y
COMPONENTS
OF
THE
DATA
X
IS
A
MATRIX
OF
INPUTS
WHERE
ROWS
CORRESPOND
TO
EXAMPLES
AND
RETURNS
A
VECTOR
OF
COEFFI
CIENTS
W
WITH
THE
MINIMAL
MEAN
SQUARE
FIT
HINT
YOU
CAN
USE
BACKSLASH
OPERATOR
TO
DO
THE
LEAST
SQUARES
REGRESSION
DIRECTLY
CHECK
MATLAB
HELP
B
WRITE
A
FUNCTION
LR
PREDICT
THAT
TAKES
INPUT
COMPONENTS
OF
THE
TEST
DATA
X
AND
A
FIXED
SET
OF
WEIGHTS
W
AND
COMPUTES
VECTOR
OF
LINEAR
PREDICTIONS
Y
C
WRITE
AND
SUBMIT
THE
PROGRAM
M
THAT
LOADS
THE
TRAIN
AND
TEST
SET
LEARNS
THE
WEIGHTS
FOR
THE
TRAINING
SET
AND
COMPUTES
THE
MEAN
SQUARED
ERROR
OF
YOUR
PREDICTOR
ON
BOTH
THE
TRAINING
AND
TESTING
DATA
SET
SEE
RULES
FOR
SUBMISSION
OF
PROGRAMS
ON
THE
COURSE
WEBPAGE
D
IN
YOUR
REPORT
PLEASE
LIST
THE
RESULTING
WEIGHTS
AND
BOTH
MEAN
SQUARE
ERRORS
COMPARE
THE
ERRORS
FOR
THE
TRAINING
AND
TESTING
SET
WHICH
ONE
IS
BETTER
PART
ONLINE
GRADIENT
DESCENT
THE
LINEAR
REGRESSION
MODEL
CAN
BE
ALSO
LEARNED
USING
THE
GRADIENT
DESCENT
METHOD
IMPLEMENT
AN
ONLINE
GRADIENT
DESCENT
PROCEDURE
FOR
FINDING
THE
REGRESSION
COEFFICIENTS
YOUR
PROGRAM
SHOULD
START
WITH
ZERO
WEIGHTS
ALL
WEIGHTS
SET
TO
AT
THE
BEGINNING
UPDATE
WEIGHTS
USING
THE
ANNEALED
LEARNING
RATE
T
WHERE
T
DENOTES
THE
T
TH
UPDATE
STEP
THUS
FOR
THE
FIRST
DATA
POINT
THE
LEARNING
RATE
IS
FOR
THE
SECOND
IT
IS
FOR
THE
RD
IS
AND
SO
ON
REPEAT
THE
UPDATE
PROCEDURE
FOR
STEPS
REUSING
THE
EXAMPLES
IN
THE
TRAINING
DATA
IF
NECCESSARY
HINT
THE
INDEX
OF
THE
I
TH
EXAMPLE
IN
THE
TRAINING
SET
OF
SIZE
N
CAN
BE
OBTAINED
BY
I
MOD
N
OPERATION
RETURN
THE
FINAL
SET
OF
WEIGHTS
WRITE
A
PROGRAM
M
THAT
RUNS
THE
GRADIENT
PROCEDURE
ON
THE
DATA
AND
AT
THE
END
PRINTS
THE
MEAN
TEST
AND
TRAIN
ERRORS
YOUR
PROGRAM
SHOULD
NORMALIZE
THE
DATA
BEFORE
RUNNING
THE
METHOD
BY
FINDING
THE
MEAN
AND
STD
OF
THE
INPUT
ATTRIBUTES
ON
THE
TRAIN
DATA
SEE
THE
LECTURE
NOTES
AND
CORRECTING
NORMALIZING
BOTH
THE
TRAIN
AND
TEST
INPUTS
RUN
IT
AND
REPORT
THE
RESULTS
GIVE
THE
MEAN
ERRORS
FOR
BOTH
THE
TRAINING
AND
TEST
SET
IS
THE
RESULT
BETTER
OR
WORSE
THAN
THE
ONE
OBTAINED
BY
SOLVING
THE
REGRESSION
PROBLEM
EXACTLY
RUN
THE
GRADIENT
DESCENT
ON
THE
UN
NORMALIZED
DATASET
WHAT
HAPPENED
MODIFY
M
FROM
PART
B
SUCH
THAT
IT
LETS
YOU
TO
PROGRESSIVELY
OBSERVE
CHANGES
IN
THE
MEAN
TRAIN
AND
TEST
ERRORS
USE
FUNCTIONS
INIT
PROGRESS
GRAPH
AND
ADD
TO
PROGRESS
GRAPH
ON
THE
COURSE
WEB
PAGE
THE
INIT
PROGRESS
GRAPH
INITIALIZES
THE
GRAPH
STRUCTURE
AND
ADD
TO
PROGRESS
GRAPH
LETS
YOU
ADD
NEW
DATA
ENTRIES
ON
FLY
TO
THE
GRAPH
USING
THE
TWO
FUNCTIONS
PLOT
THE
MEAN
SQUARED
ERRORS
FOR
THE
TRAINING
AND
TEST
TEST
FOR
EVERY
ITERATION
STEPS
SUBMIT
THE
PROGRAM
AND
INCLUDE
THE
GRAPH
IN
THE
REPORT
D
EXPERIMENT
WITH
THE
GRADIENT
DESCENT
PROCEDURE
TRY
TO
USE
FIXED
LEARNING
RATE
SAY
OR
DIFFERENT
NUMBER
OF
UPDATE
STEPS
SAY
AND
YOU
MAY
WANT
TO
CHANGE
THE
LEARNING
RATE
SCHEDULE
AS
WELL
TRY
FOR
EXAMPLE
N
REPORT
YOUR
RESULTS
AND
ANY
INTERESTING
BEHAVIORS
YOU
OBSERVE
PART
REGRESSION
WITH
POLYNOMIALS
ASSUME
WE
ARE
NOT
HAPPY
WITH
THE
PREDICTIVE
ACCURACY
OF
THE
LINEAR
MODEL
AND
WE
DECIDED
TO
EXPLORE
A
MORE
COMPLEX
MODEL
FOR
PREDICTING
HOUSING
VALUES
ASSUME
WE
DECIDE
TO
USE
A
QUADRATIC
POLYNOMIAL
TO
MODEL
THE
RELATION
BETWEEN
Y
AND
X
F
X
W
WIXI
WIJXIXJ
I
I
J
I
A
WRITE
A
FUNCTION
EXTENDX
THAT
TAKES
AN
INPUT
X
AND
RETURNS
AN
EXPANDED
X
THAT
INCLUDES
ALL
LINEAR
AND
DEGREE
TWO
POLYNOMIALS
B
WHAT
HAPPENED
TO
THE
BINARY
ATTRIBUTE
AFTER
THE
TRANSFORMATION
C
WRITE
AND
SUBMIT
A
MATLAB
PROGRAM
M
THAT
COMPUTES
THE
REGRESSION
COEFFICIENTS
FOR
THE
EXTENDED
INPUT
AND
BOTH
TRAIN
AND
TEST
ERRORS
FOR
THE
RESULT
D
REPORT
BOTH
ERRORS
IN
YOUR
REPORT
AND
COMPARE
THEM
WITH
THE
RESULTS
IN
PART
WHAT
DO
YOU
SEE
WHICH
METHOD
WOULD
YOU
USE
FOR
THE
PREDICTION
WHY
PLEASE
DO
NOT
TURN
IN
THE
WEIGHTS
FOR
THIS
PART
IN
YOUR
REPORT
UNIVERSITY
OF
PITTSBURGH
INTRODUCTION
TO
MACHINE
LEARNING
HANDOUT
PROFESSOR
MILOS
HAUSKRECHT
FEBRUARY
PROBLEM
ASSIGNMENT
DUE
THURSDAY
FEBRUARY
PROBLEM
DATA
ANALYSIS
THE
DATASET
WE
USE
IN
THIS
PROBLEM
SET
IS
VERY
SIMPLE
AND
CONSISTS
OF
A
TWO
DIMENSIONAL
INPUT
AND
A
CLASS
LABEL
THE
DATA
ARE
AVAILABLE
ON
THE
COURSE
WEB
PAGE
AND
ARE
DI
VIDED
INTO
TWO
FILES
ONE
USED
FOR
TRAINING
CLASSIFICATION
TRAIN
TXT
THE
OTHER
FOR
TESTING
CLASSIFICATION
TEST
TXT
THE
DATA
IN
FILES
ARE
IN
ROWS
SUCH
THAT
FIRST
TWO
COLUMNS
REPRESENT
INPUTS
AND
THE
LAST
THIRD
COLUMN
THE
CLASS
LABEL
OR
SINCE
INPUTS
ARE
ONLY
TWO
DIMENSIONAL
WE
CAN
EASILY
VISUALIZE
THE
DATA
FOR
THE
TWO
CLASSES
IN
A
PLOT
WRITE
A
PROGRAM
THAT
PLOTS
THE
INPUT
DATA
POINTS
IN
CLASSIFICATION
TRAIN
TXT
SUCH
THAT
THE
PLOT
DISTINGUISHES
BETWEEN
DATA
POINTS
WITH
DIFFERENT
CLASS
LABELS
USE
DIFFERENT
COLOR
AND
SYMBOL
FOR
A
POINT
E
G
X
OR
O
INCLUDE
THE
PLOT
IN
YOUR
REPORT
IS
IT
POSSIBLE
TO
SEPARATE
THE
TWO
CLASSES
PERFECTLY
WITH
A
LINEAR
DECISION
BOUNDARY
PROBLEM
LOGISTIC
REGRESSION
WE
ARE
INTERESTED
IN
BUILDING
A
CLASSIFIER
BASED
ON
THE
LOGISTIC
REGRESSION
MODEL
AND
THE
GRADIENT
OPTIMIZATION
METHODS
A
DURING
THE
CLASS
YOU
WERE
GIVEN
THE
EXPRESSION
FOR
THE
GRADIENT
OF
THE
LOGISTIC
REGRESSION
MODEL
USE
THE
LOGLIKELIHOOD
SETUP
FROM
THE
LECTURE
TO
DERIVE
THE
EXPRESSION
SHOW
CLEARLY
THE
STEPS
OF
THE
DERIVATION
PLEASE
REMEMBER
THAT
THE
DEFAULT
GRADIENT
TAKES
INTO
ACCOUNT
ALL
DATAPOINTS
IN
THE
TRAINING
SET
B
WRITE
AND
SUBMIT
A
GRADIENT
PROCEDURE
GLR
M
FOR
UPDATING
THE
PARAMETERS
OF
THE
LOGISTIC
REGRESSION
MODEL
YOUR
GRADIENT
PROCEDURE
SHOULD
START
FROM
UNIT
WEIGHTS
ALL
WEIGHTS
SET
TO
AT
THE
BEGINNING
USE
THE
ANNEALED
LEARNING
RATE
K
EXECUTES
FOR
K
STEPS
WHERE
K
IS
THE
PARAMETER
OF
THE
PROCEDURES
C
WRITE
AND
SUBMIT
A
PROGRAM
M
THAT
RUNS
THE
GLR
FUNCTION
FOR
STEPS
AND
AFTER
THE
TRAINING
COMPUTES
MEAN
MISCLASSIFICATION
ERRORS
FOR
BOTH
THE
TRAINING
AND
TEST
SET
IN
YOUR
REPORT
INCLUDE
THE
RESULTING
WEIGHTS
AND
MISCLASSIFICATION
ERRORS
D
UPDATE
THE
M
WITH
PLOT
FUNCTIONS
THAT
LET
YOU
OBSERVE
THE
PROGRESS
OF
THE
ERRORS
AFTER
EVERY
UPDATE
STEPS
USE
FUNCTIONS
DEFINED
IN
PS
FOR
THIS
PURPOSE
INCLUDE
THE
RESULTING
GRAPH
IN
YOUR
REPORT
E
EXPERIMENT
WITH
THE
GLR
FUNCTION
BY
I
CHANGING
THE
NUMBER
OF
STEPS
K
AND
II
TRYING
DIFFERENT
LEARNING
RATES
IN
PARTICULAR
TRY
SOME
CONSTANT
LEARNING
RATES
AND
K
LEARNING
RATE
SCHEDULE
REPORT
THE
RESULTS
AND
GRAPH
FROM
YOUR
EXPERIMENTS
AND
EXPLANATIONS
OF
BEHAVIORS
YOU
HAVE
OBSERVED
PROBLEM
ONLINE
GRADIENT
DESCENT
A
WRITE
AN
ON
LINE
GRADIENT
PROCEDURE
FOR
LEARNING
THE
WEIGHT
PARAMETERS
THE
PROCEDURE
SHOULD
SELECT
SEQUENTIALLY
EXAMPLES
FROM
THE
TRAINING
SET
REPEATING
THE
EXAMPLES
WHENEVER
NECESSARY
B
WRITE
A
PROGRAM
M
THAT
PERFORMS
THE
ON
LINE
GRADIENT
METHOD
FOR
STEPS
USES
PROGRESS
PLOTS
AND
AT
THE
END
COMPUTES
THE
CONFUSION
MATRICES
AND
MEAN
MISCLASSIFICATIONS
ERRORS
FOR
BOTH
DATASETS
C
REPEAT
THE
ANALYSIS
IN
PROBLEM
COMPARE
THE
RESULTS
OF
PROBLEM
AND
PROBLEM
WHAT
ARE
THE
DIFFERENCES
IN
THE
BEHAVIOR
OF
THE
TWO
GRADIENT
METHODS
PROBLEM
GENERATIVE
CLASSIFICATION
MODEL
AN
ALERNATIVE
APPROACH
IS
TO
LEARN
A
GENERATIVE
MODEL
WITH
CLASS
CONDITIONAL
DENSITIES
AND
CLASS
PRIORS
AND
USE
THE
PARAMETERS
OF
SUCH
A
MODEL
TO
DO
THE
PREDICTION
ASSUME
THAT
AN
INPUT
X
FOR
EACH
CLASS
C
OR
FOLLOWS
A
MULTIVARIATE
NORMAL
DISTRIBUTION
THAT
IS
P
X
C
N
P
X
C
N
FURTHER
ASSUME
THAT
THE
PRIOR
PROBABILITY
OF
A
CLASS
IS
REPRESENTED
BY
A
BERNOULLI
DISTRIBUTION
PARAMETERS
OF
THE
GENERATIVE
MODEL
CAN
BE
COMPUTED
FROM
THE
TRAINING
DATA
USING
THE
DENSITY
ESTIMATION
TECHNIQUES
SUCH
AS
MAXIMUM
LIKELIHOOD
ESTIMATION
ONCE
THIS
IS
AC
COMPLISHED
WE
CAN
USE
THE
ESTIMATES
TO
MAKE
CLASS
PREDICTIONS
FOR
NEW
INPUTS
LET
Θ
Μ
Σ
Μ
Σ
Θ
C
REPRESENT
PARAMETER
ESTIMATES
TO
PREDICT
THE
CLASS
WE
USE
DISCRIMINANT
FUNCTIONS
BASED
ON
THE
POSTERIOR
PROBABILITY
OF
A
CLASS
GIVEN
THE
INPUT
AND
MODEL
PARAMETERS
THIS
CAN
BE
COMPUTED
VIA
BAYES
RULE
G
X
P
C
X
Θ
P
X
Μ
Σ
Θ
C
P
X
Μ
Σ
Θ
C
P
X
Μ
Σ
Θ
C
ASSUME
WE
WANT
TO
USE
A
GENERATIVE
MODEL
IN
WHICH
THE
TWO
CLASS
CONDITIONAL
DENSITIES
SHARE
THE
SAME
COVARIANCE
MATRIX
Σ
THAT
IS
P
X
C
N
Σ
P
X
C
N
Σ
PROVIDE
THE
FOLLOWING
ANSWERS
A
GIVE
THE
FORMULA
FOR
COMPUTING
ML
ESTIMATES
OF
MEANS
OF
CLASS
CONDITIONAL
DEN
SITIES
B
HOW
WOULD
YOU
GO
ABOUT
COMPUTING
THE
ESTIMATE
OF
THE
COVARIANCE
MATRIX
Σ
NOTE
THAT
THE
ESTIMATE
OF
Σ
MUST
COMBINE
BOTH
CLASS
AND
CLASS
EXAMPLES
C
HOW
WOULD
YOU
ESTIMATE
THE
PRIOR
OF
CLASS
Θ
C
D
IMPLEMENT
FUNCTION
M
AX
LIKELIHOOD
THAT
COMPUTES
THE
ESTIMATES
OF
THE
MODEL
PARAMETERS
USING
THE
TRAINING
SET
E
IMPLEMENT
THE
FUNCTION
P
REDICT
CLASS
THAT
CHOOSES
THE
CLASS
USING
THE
DISCRIMINANT
FUNCTIONS
BASED
ON
CLASS
POSTERIORS
F
WRITE
AND
SUBMIT
A
PROGRAM
M
THAT
LEARNS
THE
GENERATIVE
MODEL
AND
THEN
USES
IT
TO
COMPUTE
THE
PREDICTIONS
THE
PROGRAM
SHOULD
COMPUTE
MEAN
MISCLASSIFICA
TION
ERRORS
FOR
BOTH
TRAINING
AND
TESTING
DATASETS
G
REPORT
THE
RESULTS
PARAMETERS
OF
THE
GENERATIVE
MODEL
AND
ERRORS
COMPARE
THEM
TO
THE
RESULTS
OBTAINED
IN
PROBLEM
PROBLEM
THE
NAIVE
BAYES
MODEL
THE
NAIVE
BAYES
MODEL
IS
A
SPECIAL
CASE
OF
THE
GENERATIVE
CLASSIFICATION
MODEL
IN
THIS
CASE
P
X
C
D
P
XI
C
AND
P
X
C
D
P
XI
C
WHERE
P
XI
C
ARE
CLASS
CONDITIONAL
DISTRIBUTIONS
FOR
EACH
INPUT
XI
FOR
THE
PROBLEM
WE
STUDY
IN
THIS
ASSIGN
MENT
ASSUME
THE
CLASS
CONDITIONAL
DISTRIBUTIONS
ARE
UNIVARIATE
GAUSSIANS
WITH
PARAMETERS
ΜI
C
ΣI
C
ΜI
C
ΣI
C
FOR
EVERY
INPUT
XI
A
IMPLEMENT
FUNCTION
M
AX
LIKELIHOOD
N
B
THAT
COMPUTES
THE
ESTIMATES
OF
THE
MODEL
PARAMETERS
USING
THE
TRAINING
SET
B
IMPLEMENT
THE
FUNCTION
P
REDICT
CLASS
N
B
THAT
CHOOSES
THE
CLASS
USING
THE
DIS
CRIMINANT
FUNCTIONS
BASED
ON
CLASS
POSTERIORS
C
WRITE
AND
SUBMIT
A
PROGRAM
M
THAT
LEARNS
THE
NAIVE
BAYES
MODEL
AND
THEN
USES
IT
TO
COMPUTE
THE
PREDICTIONS
THE
PROGRAM
SHOULD
COMPUTE
MEAN
MISCLAS
SIFICATION
ERRORS
FOR
BOTH
THE
TRAINING
AND
TESTING
DATASETS
D
REPORT
THE
RESULTS
PARAMETERS
OF
THE
MODEL
AND
ERRORS
COMPARE
THEM
TO
THE
PREVIOUS
RESULTS
UNIVERSITY
OF
PITTSBURGH
INTRO
TO
MACHINE
LEARNING
HANDOUT
PROFESSOR
MILOS
HAUSKRECHT
FEBRUARY
PROBLEM
ASSIGNMENT
DUE
THURSDAY
FEBRUARY
IN
THIS
PROBLEM
WE
SHALL
INVESTIGATE
THE
PIMA
DATASET
AND
LEARN
CLASSIFICATION
MODELS
FOR
IT
RECALL
WE
PERFORMED
SOME
EXPLORATORY
ANALYSIS
OF
THE
PIMA
DATASET
IN
PROBLEM
SET
YOU
CAN
DOWNLOAD
THE
DATASET
PIMA
TXT
AND
ITS
DESCRIPTION
PIMA
DESC
TXT
FROM
THE
COURSE
WEB
PAGE
IN
ADDITION
TO
THE
COMPLETE
DATASET
PIMA
TXT
YOU
HAVE
PIMA
TRAIN
TXT
AND
PIMA
TEST
TXT
YOU
WILL
NEED
TO
USE
FOR
TRAINING
AND
TESTING
PURPOSES
THE
DATASET
HAS
BEEN
OBTAINED
FROM
THE
UC
IRVINE
MACHINE
LEARNING
REPOSITORY
HTTP
ICS
UCI
EDU
MLEARN
M
LREPOSITORY
HTML
PROBLEM
LOGISTIC
REGRESSION
MODEL
FIRST
WE
TRY
THE
LOGISTIC
REGRESSION
MODEL
IN
COMBINATION
WITH
GRADIENT
METHODS
GIVE
SOLUTIONS
TO
THE
FOLLOWING
TASKS
A
WRITE
A
PROGRAM
THAT
NORMALIZES
INPUTS
IN
THE
PIMA
DATASET
THERE
IS
NO
NEED
TO
NORMALIZE
OUTPUTS
BASED
ON
THE
DATA
IN
THE
TRAINING
SET
APPLY
THE
PROCEDURE
TO
NORMALIZE
BOTH
THE
TRAINING
AND
TEST
SET
DATA
WHILE
GENERATING
TWO
NEW
FILES
PIMA
TRAIN
NORM
TXT
AND
PIMA
TEST
NORM
TXT
B
FAMILIARIZE
YOURSELF
WITH
A
BATCH
MODE
GRADIENT
PROCEDURE
IN
FILE
LOG
REGRESSION
M
IN
WHICH
ALL
DATA
POINTS
ARE
CONSIDERED
AT
THE
SAME
TIME
RECALL
YOU
WERE
ASKED
TO
WRITE
THE
PROCEDURE
IN
PROBLEM
SET
C
IMPLEMENT
AND
SUBMIT
A
PROGRAM
M
THAT
RUNS
THE
GRADIENT
PROCEDURE
ON
THE
TRAINING
DATASET
FOR
I
TERATION
STEPS
ALSO
CALLED
EPOCHS
INITIALIZE
ALL
WEIGHTS
TO
AT
THE
BEGINNING
USE
I
LEARNING
RATE
SCHEDULE
D
INCLUDE
GRAPH
FUNCTIONS
FOR
MONITORING
THE
PROGRESS
OF
ERRORS
IN
M
AS
USED
IN
THE
PREVIOUS
PROBLEM
SET
HW
COMPUTE
MEAN
MISCLASSIFICATION
ERROR
FOR
BOTH
THE
TRAINING
AND
TESTING
DATA
AT
THE
END
IN
THE
REPORT
INCLUDE
FINAL
TRAINING
AND
TEST
MISCLASSIFICATION
ERRORS
CONFUSION
MATRICES
FOR
THE
TRAIN
AND
TEST
SETS
SENSITIVITY
AND
SPECIFICITY
OF
THE
MODEL
ON
THE
TEST
SET
E
EXPERIMENT
WITH
THE
LEARNING
ALGORITHM
BY
CHANGING
INITIAL
WEIGHTS
LEARNING
SCHED
ULE
NUMBER
OF
EPOCHS
REPORT
TRAINING
AND
TEST
MISCLASSIFICATION
ERRORS
WHAT
WAS
THE
BEST
RESULT
YOU
COULD
GET
PROBLEM
NAIVE
BAYES
MODEL
THE
NAIVE
BAYES
MODEL
DEFINES
A
GENERATIVE
CLASSIFIER
MODEL
IN
WHICH
ALL
FEATURES
ARE
INDE
PENDENT
GIVEN
THE
CLASS
LABEL
IN
SUCH
A
CASE
THE
CLASS
CONDITIONAL
DENSITIES
OVER
MANY
INPUT
VARIABLES
CAN
BE
DECOMPOSED
INTO
A
SET
OF
INDEPENDENT
CLASS
CONDITIONAL
DENSITIES
ONE
FOR
EV
ERY
INPUT
VARIABLE
FOR
EXAMPLE
THE
CONDITIONAL
PROBABILITY
OF
AN
INPUT
X
XD
GIVEN
CLASS
IN
THE
NAIVE
BAYES
MODEL
IS
DECOMPOSED
AS
D
P
X
Y
P
XI
Y
I
ONE
IMPORTANT
CONCERN
IS
THE
CHOICE
OF
AN
APPROPRIATE
PARAMETERIZATION
OF
CLASS
CONDITIONAL
DENSITIES
TYPICALLY
WE
DO
NOT
CHOOSE
THE
DISTRIBUTIONS
ARBITRARILY
INSTEAD
WE
WANT
TO
MAKE
A
GOOD
EDUCATED
GUESS
EXPLORATORY
DATA
ANALYSIS
CAN
HELP
US
GREATLY
TO
RECOGNIZE
TYPES
OF
DENSITIES
THAT
APPEAR
TO
MATCH
THE
DATA
THE
BEST
PROBLEM
EXPLORATORY
DATA
ANALYSIS
WE
HAVE
PERFORMED
THE
EXPLORATORY
ANALYSIS
OF
THE
PIMA
DATASET
IN
PROBLEM
SET
HERE
WE
REUSE
THE
PROGRAMS
CREATED
THERE
AND
APPLY
THEM
TO
STUDY
THE
DENSITY
MODELS
WE
CHOOSE
TO
PARAMETERIZE
OUR
NAIVE
BAYES
MODEL
PART
A
WRITE
AND
SUBMIT
A
PROGRAM
M
THAT
DIVIDES
PIMA
TXT
DATA
INTO
TWO
SUBSETS
ONE
WITH
ALL
EXAMPLES
WITH
CLASS
AND
ANOTHER
WITH
ALL
EXAMPLES
WITH
CLASS
ANALYZES
EXAMPLES
IN
TWO
SUBSETS
USING
HISTOGRAMS
HISTOGRAMS
SHOULD
GIVE
YOU
MORE
INFORMATION
ABOUT
THE
SHAPE
OF
THE
DISTRIBUTION
OF
ATTRIBUTES
YOU
CAN
USE
THE
FUNCTION
HISTOGRAM
ANALYSIS
M
FOR
THIS
PURPOSE
PART
B
WHAT
DISTRIBUTION
DENSITY
WOULD
YOU
USE
TO
FIT
THE
VALUES
OF
ATTRIBUTES
TO
IN
THE
PIMA
DATASET
CHOICES
ONE
TYPICALLY
CONSIDERS
ARE
BERNOULLI
BINOMIAL
MULTINOMIAL
NORMAL
POISSON
GAMMA
EXPONENTIAL
DISTRIBUTIONS
PROBLEM
LEARNING
OF
THE
NAIVE
BAYES
CLASSIFIER
THE
LEARNING
OF
THE
NAIVE
BAYES
MODEL
CORRESPONDS
TO
THE
ESTIMATION
OF
PARAMETERS
OF
CLASS
CONDITIONAL
DISTRIBUTIONS
P
XI
Y
P
XI
Y
FOR
ALL
INPUT
COMPONENTS
I
FROM
DATA
AND
ESTIMATION
OF
CLASS
PRIORS
P
Y
P
Y
THUS
THE
LEARNING
BOILS
DOWN
TO
A
NUMBER
OF
SMALLER
DENSITY
ESTIMATION
PROBLEMS
ASSUME
THAT
CLASS
CONDITIONAL
DENSITIES
FOR
PIMA
DATASET
HAVE
THE
FOLLOWING
FORM
CLASS
CONDITIONALS
FOR
INPUTS
TAKE
THE
FORM
OF
EXPONENTIAL
DISTRIBUTION
THE
EXPONENTIAL
DISTRIBUTION
IS
DEFINED
AS
P
X
Μ
EXP
X
WHERE
Μ
IS
THE
PARAMETER
EXPONENTIAL
DISTRIBUTION
IS
A
SPECIAL
CASE
OF
THE
GAMMA
DISTRIBUTION
AND
BELONGS
TO
THE
EXPONENTIAL
FAMILY
CLASS
CONDITIONALS
FOR
INPUTS
FOLLOW
UNIVARIATE
NORMAL
DISTRIBUTIONS
P
X
Μ
Σ
Σ
EXP
X
Μ
WITH
MEAN
AND
STANDARD
DEVIATION
BEING
THE
TWO
PARAMETERS
IN
ADDITION
ASSUME
THAT
PRIORS
ON
CLASSES
FOLLOW
A
BERNOULLI
DISTRIBUTION
Θ
ΘX
Θ
X
FOR
X
PART
A
WRITE
AND
SUBMIT
A
PROGRAM
M
THAT
COMPUTES
AND
RETURNS
THE
ESTIMATES
OF
THE
PARAMETERS
OF
THE
NAIVE
BAYES
MODEL
USING
THE
TRAINING
SET
PIMA
TRAIN
TXT
THE
PARAMETERS
INCLUDE
PRIORS
ON
CLASSES
CLASS
CONDITIONALS
ONE
FOR
EVERY
INPUT
COMPONENT
AND
CLASS
LABEL
TO
FIT
EXPONENTIAL
DISTIBUTIONS
USE
MATLAB
FUNCTION
EXPFIT
TO
FIT
NORMAL
DISTRIBUTIONS
USE
FUNCTION
NORMFIT
SEE
ALSO
MATLAB
HELP
EXAMPLE
APPLICATION
OF
EXPFIT
AND
NORMFIT
FUNCTIONS
ALL
INPUTS
X
WITH
LABEL
CLASS
ALL
INPUTS
X
WITH
LABEL
CLASS
FIT
THE
EXPONENTIAL
CLASS
CONDITIONAL
FOR
INPUT
ATTRIBUTE
AND
CLASS
P
Y
EXPFIT
FIT
THE
EXPONENTIAL
CLASS
CONDITIONAL
FOR
INPUT
ATTRIBUTE
AND
CLASS
P
Y
EXPFIT
FITTING
OF
THE
CLASS
CONDITIONAL
OF
THE
SECOND
ATTRIBUTE
WITH
NORMAL
DISTRIBUTION
CLASS
CONDTIONAL
FOR
CLASS
P
Y
NORMFIT
ETC
PART
B
LIST
PARAMETERS
FOUND
BY
YOUR
PROGRAM
IN
THE
REPORT
PROBLEM
CLASSIFICATION
WITH
THE
NAIVE
BAYES
MODEL
ONCE
THE
PARAMETERS
OF
THE
NAIVE
BAYES
MODEL
ARE
LEARNED
ESTIMATED
THE
DECISION
ABOUT
THE
CLASS
FOR
A
SPECIFIC
INPUT
X
CAN
BE
MADE
BY
DESIGNING
THE
APPROPRIATE
DISCRIMINANT
FUNCTIONS
TYPICALLY
THERE
ARE
BASED
ON
CLASS
POSTERIORS
THUS
A
CLASSIFICATION
PROBLEMS
BOILS
DOWN
TO
THE
PROBLEM
OF
COMPARISON
OF
POSTERIORS
OF
CLASSES
FOR
X
THESE
ARE
COMPUTED
THROUGH
THE
BAYES
RULE
P
Y
X
D
D
I
P
XI
Y
P
Y
D
I
P
XI
Y
L
P
Y
I
P
XI
Y
L
P
Y
NOTE
THAT
IN
ORDER
TO
MAKE
THE
BEST
POSTERIOR
CHOICE
IT
IS
SUFFICIENT
TO
COMPARE
THE
FOLLOWING
DISCRIMINANT
FUNCTIONS
BASED
ON
LOG
POSTERIORS
D
X
LOG
P
XI
Y
LOG
P
Y
I
D
X
LOG
P
XI
Y
LOG
P
Y
I
PART
A
WRITE
AND
SUBMIT
A
PROGRAM
M
THAT
CALLS
A
FUNCTION
PREDICT
N
B
THAT
PREDICTS
CLASS
LABELS
FOR
INPUTS
BASED
ON
CLASS
POSTE
RIOR
THE
DISCRIMINANT
FUNCTIONS
YOU
NEED
TO
USE
HERE
ARE
GIVEN
IN
EXPRESSIONS
AND
AND
USE
PARAMETERS
OBTAINED
IN
PROBLEM
USES
PREDICT
N
B
TO
COMPUTE
THE
MISCLASSIFICATION
ERROR
OF
THE
NAIVE
BAYES
CLASSIFIER
ON
BOTH
TRAINING
AND
TEST
DATASETS
REPORT
THE
ERRORS
CALCULATES
AND
REPORTS
A
CONFUSION
MATRIX
FOR
THE
TEST
AND
TRAINING
SETS
USE
FUNCTION
ACCURACY
M
PART
B
IN
YOUR
REPORT
INCLUDE
TRAINING
AND
TEST
MISCLASSIFICATION
ERRORS
CONFUSION
MATRICES
FOR
THE
TRAIN
AND
TEST
SETS
SENSITIVITY
AND
SPECIFICITY
OF
THE
MODEL
ON
THE
TEST
SET
PART
C
COMPARE
RESULTS
FOR
THE
MEAN
MISCLASSIFICATION
ERRORS
FOR
THE
LOGISTIC
REGRESSION
MODEL
TO
THE
NAIVE
BAYES
CLASSIFIER
PROBLEM
SUPPORT
VECTOR
MACHINES
SUPPORT
VECTOR
MACHINES
REPRESENT
YET
ANOTHER
TECHNIQUE
ONE
CAN
APPLY
TO
THE
PROBLEM
OF
BINARY
CLASSIFICATION
THE
IDEA
IS
TO
FIND
THE
HYPERPLANE
THAT
SEPARATES
THE
EXAMPLES
IN
TWO
CLASSES
THE
BEST
THE
BEST
HYPERPLANE
IS
DEFINED
IN
TERMS
OF
THE
MAXIMUM
MARGIN
THE
LEARNING
PROBLEM
REDUCES
AS
USUALLY
TO
OPTIMIZATION
IN
THIS
CASE
A
QUADRATIC
OPTIMIZATION
PROBLEM
THERE
IS
A
NUMBER
OF
IMPLEMENTATIONS
OF
SVM
ALGORITHMS
WITH
BETTER
OR
WORSE
RUNNING
TIME
PERFORMANCES
HERE
WE
USE
A
MATLAB
CODE
IMPLEMENTING
SVM
SOLVER
FOR
THE
LINEAR
DECISION
BOUNDARY
PROPOSED
BY
O
L
MANGASARIAN
AND
D
MUSICANT
THE
PAPER
DESCRIBING
THIS
METHOD
CAN
BE
DOWNLOADED
ELECTRONICALLY
AT
THE
SVM
SOLVER
IS
IN
FILES
SVML
M
AND
SVML
ITSOL
M
THAT
CAN
BE
DOWNLOADED
FROM
THE
COURSE
WEB
PAGE
SVML
ITSOL
M
IS
A
SLIGHTLY
MODIFIED
VERSION
OF
THE
ORIGINAL
PROGRAM
BY
O
L
MANGASARIAN
AND
MUSICANT
TO
RUN
IT
YOU
CALL
SVML
M
THAT
TAKES
CARE
OF
CONVERTING
OUTPUTS
FROM
CLASS
LABELS
TO
AND
SETS
OTHER
PARAMETERS
OF
THE
LAGRANGIAN
SVM
WRITE
AND
SUBMIT
A
MATLAB
PROGRAM
M
THAT
LOADS
TRAINING
AND
TEST
DATA
CALLS
LINEAR
SVM
SOLVER
TO
LEARN
THE
LINEAR
DECISION
BOUNDARY
COMPUTES
THE
MEAN
MISCLASSIFICATION
ERROR
FOR
BOTH
THE
TRAINING
AND
TEST
DATA
COMPUTES
THE
CONFUSION
MATRIX
FOR
THE
TEST
SET
WRITE
A
SPECIAL
FUNCTION
CONFUSION
MATRIX
THAT
TAKES
CLASS
LABELS
FROM
THE
DATA
AND
COMPARES
THEM
TO
THOSE
COMPUTED
BY
THE
CLASSIFIER
IN
YOUR
REPORT
INCLUDE
THE
MISCLASSIFICATION
ERRORS
AND
CONFUSION
MATRIX
OBTAINED
FOR
THE
TRAIN
AND
TEST
SETS
COMPARE
THE
RESULT
TO
THE
RESULTS
OF
THE
LOGISTIC
REGRESSION
AND
NEURAL
NETWORK
MODELS
OPTIONAL
IF
YOU
ARE
INTERESTED
EXPERIMENTING
WITH
EXISTING
SVMS
TOOLS
INCLUDING
TOOLS
SUPPORTING
NON
LINEAR
KERNELS
PLEASE
CHECK
OUT
THE
FOLLOWING
SOFWARE
PACKAGES
LIBLINEAR
LIBSVM
AND
SVMLIGHT
ALL
THESE
CAN
BE
INTERFACED
WITH
MATLAB
PROBLEM
ROC
ANALYSIS
THE
ROC
ANALYSIS
LET
US
EXPLORE
THE
ABILITY
OF
THE
CLASSIFICATION
MODEL
TO
DISCRIMINATE
IN
BETWEEN
THE
TWO
CLASSES
INCLUDING
POSSIBLE
SENSITIVITY
AND
SPECIFICITY
TRADE
OFFS
IN
THE
ROC
ANALYSIS
WE
ASSUME
A
CHANGING
THRESHOLD
FOR
CALLING
CLASS
BASED
ON
THE
PROJECTION
DEFINED
BY
THE
MODEL
THIS
CAN
BE
EITHER
P
Y
X
FOR
THE
LOGISTIC
REGRESSION
AND
THE
NAIVE
BAYES
OR
WT
X
B
FOR
THE
SVM
PART
A
FAMILIARIZE
YOURSELF
WITH
THE
FUNCTION
PERFCURVE
IN
MATLAB
THAT
LETS
YOU
CALCULATE
COORDINATES
OF
POINTS
DEFINING
THE
ROC
CURVE
AS
WELL
AS
THE
AREA
UNDER
THE
ROC
CURVE
AUROC
PART
B
USE
THE
FUNCTION
PERFCURVE
TO
PLOT
THE
ROC
CURVE
AND
CALCULATE
AUC
ON
THE
TESTING
SET
FOR
THE
MODELS
YOU
BUILD
IN
PROBLEMS
ALL
MODELS
SHOULD
BE
TRAINED
ON
THE
TRAINING
SET
PART
C
PLEASE
INCLUDE
THE
ROC
CURVES
AND
THE
AUC
STATISTICS
IN
THE
REPORT
COMPARE
THE
ROC
CURVES
AND
THEIR
AUC
STATISTICS
WHAT
DO
YOU
THINK
WHICH
MODEL
IS
BETTER
UNIVERSITY
OF
PITTSBURGH
INTRO
TO
MACHINE
LEARNING
HANDOUT
PROFESSOR
MILOS
HAUSKRECHT
FEBRUARY
PROBLEM
ASSIGNMENT
DUE
THURSDAY
MARCH
IN
THIS
PROBLEM
WE
SHALL
CONTINUE
OUR
INVESTIGATION
OF
THE
PIMA
DATASET
USING
NEW
CLASSIFI
CATION
MODELS
MULTILAYER
NEURAL
NETWORKS
DECISION
TREES
AND
THE
NEAREST
NEIGHBOR
CLASSIFIER
YOU
CAN
DOWNLOAD
THE
DATASET
PIMA
TXT
AND
ITS
DESCRIPTION
PIMA
DESC
TXT
FROM
THE
COURSE
WEB
PAGE
IN
ADDITION
TO
THE
COMPLETE
DATASET
PIMA
TXT
YOU
HAVE
PIMA
TRAIN
TXT
AND
PIMA
TEST
TXT
YOU
WILL
NEED
TO
USE
FOR
TRAINING
AND
TESTING
PURPOSES
THE
DATASET
HAS
BEEN
OBTAINED
FROM
THE
UC
IRVINE
MACHINE
LEARNING
REPOSITORY
HTTP
ICS
UCI
EDU
MLEARN
M
LREPOSITORY
HTML
PROBLEM
NEURAL
NETWORK
TOOLBOX
IN
MATLAB
WE
START
WITH
THE
NEURAL
NETWORK
TOOLBOX
PART
A
IN
HOMEWORK
AND
YOU
WERE
ASKED
TO
IMPLEMENT
AND
RUN
A
GRADIENT
AL
GORITHM
FOR
LEARNING
THE
LOGISTIC
REGRESSION
MODEL
HOWEVER
THE
LOGISTIC
REGRESSION
MODEL
IS
ALSO
SUPPORTED
AND
IMPLEMENTED
IN
MATLAB
WITHIN
ITS
NEURAL
NETWORK
TOOL
BOX
PLEASE
FAMILIARIZE
YOURSELF
AND
RUN
LOGISTIC
N
N
M
FUNCTION
THAT
IS
GIVEN
TO
YOU
AND
IMPLEMENTS
THE
LOGISTIC
REGRESSION
MODEL
USING
THE
TOOLBOX
FUNCTIONS
TRY
TO
CHANGE
THE
PARAMETERS
OF
THE
MODEL
SUCH
AS
THE
OPTIMIZATION
METHOD
AND
THE
NUM
BER
OF
EPOCHS
REPORT
THE
WEIGHTS
WITH
THE
BEST
MEAN
MISCLASSIFICATION
RATE
FOR
THE
TEST
SET
AND
ANY
GRAPHS
YOU
HAVE
FOUND
INTERESTING
PART
B
MULTILAYER
NEURAL
NETWORK
THE
LIMITATION
OF
THE
LOGISTIC
REGRESSION
MODEL
IS
THAT
IT
USES
A
LINEAR
DECISION
BOUNDARY
ONE
WAY
AROUND
THIS
IS
PROBLEM
IS
TO
USE
NON
LINEAR
FEATURES
IN
COMBINATION
WITH
A
LINEAR
MODEL
HOWEVER
IN
THIS
CASE
FEATURE
FUNCTION
MUST
BE
FIXED
AND
SELECTED
IN
ADVANCE
MULTILAYER
NEURAL
NETWORKS
ALLOW
US
TO
REPRESENT
NON
LINEAR
MODELS
BY
CASCADING
MULTIPLE
NONLINEAR
UNITS
MULTILAYER
NEURAL
NETWORKS
CAN
BE
BUILT
WITH
THE
NN
MATLAB
TOOLBOX
WRITE
A
PROGRAM
M
THAT
IMPLEMENTS
A
NEURAL
NETWORK
WITH
TWO
HIDDEN
UNITS
THAT
IS
THERE
ARE
TWO
NONLINEAR
UNITS
WE
FEED
THE
INPUT
TO
AND
ONE
UNIT
THAT
COMBINES
THEIR
RESULTS
RUN
THE
PROGRAM
FOR
EPOCHS
CALCULATE
THE
MEAN
MISCLASSIFICATION
ERRORS
FOR
THE
TRAINING
AND
TESTING
DATA
REPORT
ERRORS
AND
COMPARE
THEM
TO
RESULTS
OBTAINED
FOR
THE
LOGISTIC
REGRESSION
MODEL
FOR
PART
A
WHICH
MODEL
IS
BETTER
WHY
PART
C
EXPERIMENT
WITH
NEURAL
NETWORKS
WITH
AND
HIDDEN
UNITS
WHILE
CHANGING
OTHER
LEARNING
PARAMETERS
E
G
THE
OPTIMIZATION
METHOD
OR
THE
NUMBER
OF
EPOCHS
ANALYZE
AND
COMPARE
AND
REPORT
THE
RESULTS
PROBLEM
DECISION
TREES
THE
DECISION
TREE
APPROACH
IS
YET
ANOTHER
CLASSIFICATION
METHODS
WE
COVERED
IN
THE
COURSE
IS
THE
DECISION
TREE
METHOD
THE
METHOD
BUILDS
A
TREE
BY
RECURSIVELY
SPLITTING
THE
TRAINING
SET
USING
ONE
OF
THE
ATTRIBUTES
BY
OPTIMIZING
THE
GAIN
WITH
RESPECT
TO
SOME
IMPURITY
MEASURE
PART
A
THE
SCRIPT
RUN
DT
M
SHOWS
HOW
TO
TRAIN
DISPLAY
AND
APPLY
THE
DECISION
TREE
THE
SCRIPT
FIRST
BUILDS
A
DEFAULT
TREE
WITH
MINIMAL
RESTRICTIONS
ON
ITS
SIZE
AND
AFTER
THAT
THE
TREE
OBTAINED
BY
RESTRICTING
THE
NUMBER
OF
NODES
IN
THE
TREE
PLEASE
RUN
AND
FAMILIARIZE
YOURSELF
WITH
THE
CODE
WHAT
DO
YOU
THINK
WHICH
TREE
IS
BETTER
FOR
PREDICTION
WHY
SHOULD
WE
ALWAYS
TRY
TO
BACKPRUNE
IT
PART
B
EXPERIMENT
WITH
THE
DECISION
TREE
FUNCTION
FITCTREE
M
AND
ITS
OPTIONAL
PA
RAMETERS
MODIFYING
THE
ALGORITHM
AND
THE
TREE
BUILT
REPORT
THE
RESULTS
OF
YOUR
INVESTIGATIONS
IN
THE
REPORT
BY
LISTING
THE
SETTINGS
USED
FOR
THE
TREE
LEARNING
ALGORITHM
AND
OBTAINED
RESULTS
YOU
CAN
FIND
THE
DIFFERENT
SETTINGS
IN
THE
MATLAB
HELP
DOCUMENTS
PROBLEM
K
NEAREST
NEIGHBOR
CLASSIFIER
ANOTHER
CLASSIFICATION
METHOD
COVERED
IN
THE
COURSE
IS
THE
K
NEAREST
NEIGHBOR
CLASSIFIER
KNN
PART
A
THE
SCRIPT
RUN
KN
N
M
SHOWS
HOW
TO
CLASSIFY
EXAMPLES
WITH
THE
KNNCLASSIFY
METHOD
IMPLEMENTED
IN
MATLAB
THE
SCRIPT
USES
EUCLIDEAN
METRIC
AND
THE
NUMBER
OF
NEIGHBORS
IS
SET
TO
PLEASE
RUN
AND
FAMILIARIZE
YOURSELF
WITH
THE
CODE
PLEASE
EXPERIMENT
WITH
KNN
BY
MODIFYING
THE
NUMBER
OF
NEIGHBORS
AND
REPORT
THE
RESULTS
ON
THE
TEST
SET
PLEASE
ATTEMPT
AND
NEIGHBORS
IN
ADDITION
TO
PART
B
PLEASE
NORMALIZE
THE
DATA
BOTH
THE
TRAIN
AND
TEST
SET
BEFORE
RUNNING
THE
KNN
CLASSIFIER
WITH
THE
EUCLIDEAN
DISTANCE
AGAIN
DID
THE
RESULT
IMPROVE
PART
C
THE
KNN
CLASSIFIER
ASSUMES
THAT
ONLY
K
CLOSEST
POINTS
ARE
USED
TO
DETERMINE
THE
CLASS
LABEL
FOR
EACH
TEST
DATA
POINT
MOREOVER
ALL
THESE
NEIGHBORS
CARRY
EQUAL
WEIGHT
THAT
IS
EACH
TRAINING
DATAPOINT
AMONG
THE
TOP
K
NEIGHBORS
CARRIES
WEIGHT
TO
ACCOUNT
FOR
TRAINING
POINT
DIFFERENCES
THE
KNN
CLASSIFIER
CAN
BE
MODIFIED
USING
SMOOTH
KERNELS
SUCH
AS
THE
GAUSSIAN
KERNEL
IN
THIS
CASE
EACH
DATAPOINT
IN
THE
TRAINING
SET
IS
USED
TO
CALCULATE
THE
CLASS
LABEL
AND
ITS
CONTRIBUTION
IS
WEIGHTED
PROPORTIONALLY
TO
ITS
DISTANCE
FROM
THE
TARGET
POINT
WE
WANT
TO
CLASSIFY
WRITE
AND
SUBMIT
FUNCTION
SOFT
NN
M
THAT
TAKES
AS
ARGUMENTS
X
COMPONENT
OF
THE
TRAINING
DATA
D
Y
COMPONENT
OF
D
THE
NEW
DATAPOINT
X
AND
THE
SMOOTHNESS
PARAMETER
H
DEFINING
THE
GAUSSIAN
KERNEL
THE
FUNCTION
RETURNS
THE
LABEL
FOR
THE
NEW
DATAPOINT
X
PLEASE
EXPERIMENT
WITH
THE
NEW
FUNCTION
ON
PIMA
DATASET
WHILE
VARYING
THE
PARAMETER
H
PLEASE
NORMALIZE
THE
DATA
BEFORE
USING
THEM
REPORT
THE
RESULTS
OF
THE
EXPERIMENT
IN
YOUR
REPORT
HOW
DOES
THE
RESULT
COMPARES
TO
THE
KNN
CLASSIFIER
PROBLEM
BAYESIAN
BELIEF
NETWORKS
ASSUME
THE
BAYESIAN
BELIEF
NETWORK
IN
THE
FIGURE
BELOW
ASSUME
THAT
EVERY
VARIABLE
IN
THE
NETWORK
IS
BINARY
REPRESENTING
T
F
VALUES
EXCEPT
VARIABLE
D
THAT
CAN
TAKE
ON
THREE
POSSIBLE
VALUES
T
F
X
X
STANDS
FOR
UNDECIDED
THE
BELIEF
NETWORK
ENCODES
THE
FULL
JOINT
DISTRIBUTION
OVER
RANDOM
VARIABLES
REPRESENTED
BY
NODES
BY
EXPLOITING
CONDITIONAL
INDEPENDENCES
THAT
HOLD
AMONG
VARIABLES
PART
A
GIVE
EXAMPLES
OF
AT
LEAST
FIVE
INDEPENDENCES
MARGINAL
OR
CONDITIONAL
THAT
HOLD
AMONG
VARIABLES
IN
THE
NETWORK
PART
B
WHAT
IS
THE
NUMBER
OF
PARAMETERS
NEEDED
TO
DEFINE
THE
FULL
JOINT
DISTRIBUTION
OVER
VARIABLES
IN
THE
PROBLEM
DOMAIN
WITHOUT
THE
BELIEF
NETWORK
REPRESENTATION
PART
C
SHOW
HOW
TO
COMPUTE
THE
JOINT
PROBABILITY
P
A
T
B
T
C
F
D
F
E
F
F
T
USING
THE
PARAMETERS
OF
THE
BELIEF
NETWORK
MODEL
PART
D
WHAT
IS
THE
NUMBER
OF
PARAMETERS
NEEDED
TO
DEFINE
THE
BELIEF
NETWORK
IN
THE
FIGURE
INTRODUCTION
TO
MACHINE
LEARNING
CS
SPRING
COURSE
DESCRIPTION
THE
GOAL
OF
THE
FIELD
OF
MACHINE
LEARNING
IS
TO
BUILD
COMPUTER
SYSTEMS
THAT
LEARN
FROM
EXPERIENCE
AND
THAT
ARE
CAPABLE
TO
ADAPT
TO
THEIR
ENVIRONMENTS
LEARNING
TECHNIQUES
AND
METHODS
DEVELOPED
BY
RESEARCHERS
IN
THIS
FIELD
HAVE
BEEN
SUCCESSFULLY
APPLIED
TO
A
VARIETY
OF
LEARNING
TASKS
IN
A
BROAD
RANGE
OF
AREAS
INCLUDING
FOR
EXAMPLE
TEXT
CLASSIFICATION
GENE
DISCOVERY
FINANCIAL
FORECASTING
CREDIT
CARD
FRAUD
DETECTION
COLLABORATIVE
FILTERING
DESIGN
OF
ADAPTIVE
WEB
AGENTS
AND
OTHERS
THIS
INTRODUCTORY
MACHINE
LEARNING
COURSE
WILL
GIVE
AN
OVERVIEW
OF
MANY
MODELS
AND
ALGORITHMS
USED
IN
MODERN
MACHINE
LEARNING
INCLUDING
LINEAR
MODELS
MULTI
LAYER
NEURAL
NETWORKS
SUPPORT
VECTOR
MACHINES
DENSITY
ESTIMATION
METHODS
BAYESIAN
BELIEF
NETWORKS
CLUSTERING
ENSEMBLE
METHODS
AND
REINFORCEMENT
OF
LEARNING
THE
COURSE
WILL
GIVE
THE
STUDENT
THE
BASIC
IDEAS
AND
INTUITION
BEHIND
THESE
METHODS
AS
WELL
AS
A
MORE
FORMAL
UNDERSTANDING
OF
HOW
AND
WHY
THEY
WORK
THROUGH
HOMEWORK
ASSIGNMENTS
STUDENTS
WILL
HAVE
AN
OPPORTUNITY
TO
EXPERIMENT
WITH
MANY
MACHINE
LEARNING
TECHNIQUES
AND
APPLY
THEM
TO
VARIOUS
REAL
WORLD
DATASETS
PREREQUISITES
STAT
OR
OR
EQUIVALENT
AND
CS
OR
THE
PERMISSION
OF
THE
INSTRUCTOR
TEXTBOOK
CHRIS
BISHOP
PATTERN
RECOGNITION
AND
MACHINE
LEARNING
SPRINGER
HOMEWORK
ASSIGNMENTS
HOMEWORK
ASSIGNMENTS
WILL
HAVE
MOSTLY
A
CHARACTER
OF
PROJECTS
AND
WILL
REQUIRE
YOU
TO
IMPLEMENT
SOME
OF
THE
LEARNING
ALGORITHMS
COVERED
DURING
LECTURES
PROGRAMMING
ASSIGNMENTS
WILL
BE
IMPLEMENTED
IN
MATLAB
PLEASE
VISIT
TO
SEE
HOW
TO
GET
A
FREE
COPY
OF
MATLAB
LICENSE
FOR
STUDENTS
THE
ASSIGNMENTS
BOTH
REPORTS
AND
PROGRAMMING
PARTS
ARE
DUE
AT
THE
BEGINNING
OF
THE
CLASS
ON
THE
DAY
SPECIFIED
ON
THE
ASSIGNMENT
IN
GENERAL
NO
EXTENSIONS
WILL
BE
GRANTED
POLICY
ON
COLLABORATION
NO
COLLABORATION
ON
HOMEWORK
ASSIGNMENTS
PROGRAMS
AND
EXAMS
UNLESS
YOU
ARE
SPECIFICALLY
INSTRUCTED
TO
WORK
IN
GROUPS
IS
PERMITTED
GRADING
THE
FINAL
GRADE
FOR
THE
COURSE
WILL
BE
DETERMINED
BASED
ON
HOMEWORK
ASSIGNMENTS
EXAMS
AND
YOUR
LECTURE
ATTENDANCE
AND
ACTIVITY
THE
MIDTERM
EXAM
WILL
BE
HELD
PRIOR
TO
THE
SPRING
BREAK
IN
LATE
FEB
EARLY
MARCH
POLICY
ON
CHEATING
CHEATING
AND
ANY
OTHER
ANTI
INTELLECTUAL
BEHAVIOR
INCLUDING
GIVING
YOUR
WORK
TO
SOMEONE
ELSE
WILL
BE
DEALT
WITH
SEVERELY
AND
WILL
RESULT
IN
THE
FAIL
F
GRADE
IF
YOU
FEEL
YOU
MAY
HAVE
VIOLATED
THE
RULES
SPEAK
TO
US
AS
SOON
AS
POSSIBLE
PLEASE
MAKE
SURE
YOU
READ
UNDERSTAND
AND
ABIDE
BY
THE
ACADEMIC
INTEGRITY
CODE
FOR
THE
UNIVERSITY
OF
PITTSBURGH
AND
FACULTY
AND
COLLEGE
OF
ARTS
AND
SCIENCES
STUDENTS
WITH
DISABILITIES
IF
YOU
HAVE
A
DISABILITY
FOR
WHICH
YOU
ARE
OR
MAY
BE
REQUESTING
AN
ACCOMMODATION
YOU
ARE
ENCOURAGED
TO
CONTACT
BOTH
YOUR
INSTRUCTOR
AND
DRS
WWW
DRS
PITT
EDU
WILLIAM
PITT
UNION
FOR
ASL
USERS
AS
EARLY
AS
POSSIBLE
IN
THE
TERM
DRS
WILL
VERIFY
YOUR
DISABILITY
AND
DETERMINE
REASONABLE
ACCOMMODATIONS
FOR
THIS
COURSE
TENTATIVE
SYLLABUS
MACHINE
LEARNING
INTRODUCTION
DENSITY
ESTIMATION
SUPERVISED
LEARNING
LINEAR
AND
LOGISTIC
REGRESSION
GENERATIVE
CLASSIFICATION
MODELS
MULTI
LAYER
NEURAL
NETWORKS
SUPPORT
VECTOR
MACHINES
UNSUPERVISED
LEARNING
BAYESIAN
BELIEF
NETWORKS
BBNS
LEARNING
PARAMETERS
AND
STRUCTURE
OF
BBNS
EXPECTATION
MAXIMIZATION
CLUSTERING
DIMENSIONALITY
REDUCTION
FEATURE
SELECTION
FEATURE
FILTERING
WRAPPER
METHODS
PCA
ENSEMBLE
METHODS
MIXTURES
OF
EXPERTS
BAGGING
AND
BOOSTING
REINFORCEMENT
LEARNING
TABLE
OF
CONTENTS
ARCHITECTURE
OVERVIEW
CONVNET
LAYERS
CONVOLUTIONAL
LAYER
POOLING
LAYER
NORMALIZATION
LAYER
FULLY
CONNECTED
LAYER
CONVERTING
FULLY
CONNECTED
LAYERS
TO
CONVOLUTIONAL
LAYERS
CONVNET
ARCHITECTURES
LAYER
PATTERNS
LAYER
SIZING
PATTERNS
CASE
STUDIES
LENET
ALEXNET
ZFNET
GOOGLENET
VGGNET
COMPUTATIONAL
CONSIDERATIONS
ADDITIONAL
REFERENCES
CONVOLUTIONAL
NEURAL
NETWORKS
CNNS
CONVNETS
CONVOLUTIONAL
NEURAL
NETWORKS
ARE
VERY
SIMILAR
TO
ORDINARY
NEURAL
NETWORKS
FROM
THE
PREVIOUS
CHAPTER
THEY
ARE
MADE
UP
OF
NEURONS
THAT
HAVE
LEARNABLE
WEIGHTS
AND
BIASES
EACH
NEURON
RECEIVES
SOME
INPUTS
PERFORMS
A
DOT
PRODUCT
AND
OPTIONALLY
FOLLOWS
IT
WITH
A
NON
LINEARITY
THE
WHOLE
NETWORK
STILL
EXPRESSES
A
SINGLE
DIFFERENTIABLE
SCORE
FUNCTION
FROM
THE
RAW
IMAGE
PIXELS
ON
ONE
END
TO
CLASS
SCORES
AT
THE
OTHER
AND
THEY
STILL
HAVE
A
LOSS
FUNCTION
E
G
SVM
SOFTMAX
ON
THE
LAST
FULLY
CONNECTED
LAYER
AND
ALL
THE
TIPS
TRICKS
WE
DEVELOPED
FOR
LEARNING
REGULAR
NEURAL
NETWORKS
STILL
APPLY
SO
WHAT
DOES
CHANGE
CONVNET
ARCHITECTURES
MAKE
THE
EXPLICIT
ASSUMPTION
THAT
THE
INPUTS
ARE
IMAGES
WHICH
ALLOWS
US
TO
ENCODE
CERTAIN
PROPERTIES
INTO
THE
ARCHITECTURE
THESE
THEN
MAKE
THE
FORWARD
FUNCTION
MORE
EFકCIENT
TO
IMPLEMENT
AND
VASTLY
REDUCE
THE
AMOUNT
OF
PARAMETERS
IN
THE
NETWORK
ARCHITECTURE
OVERVIEW
RECALL
REGULAR
NEURAL
NETS
AS
WE
SAW
IN
THE
PREVIOUS
CHAPTER
NEURAL
NETWORKS
RECEIVE
AN
INPUT
A
SINGLE
VECTOR
AND
TRANSFORM
IT
THROUGH
A
SERIES
OF
HIDDEN
LAYERS
EACH
HIDDEN
LAYER
IS
MADE
UP
OF
A
SET
OF
NEURONS
WHERE
EACH
NEURON
IS
FULLY
CONNECTED
TO
ALL
NEURONS
IN
THE
PREVIOUS
LAYER
AND
WHERE
NEURONS
IN
A
SINGLE
LAYER
FUNCTION
COMPLETELY
INDEPENDENTLY
AND
DO
NOT
SHARE
ANY
CONNECTIONS
THE
LAST
FULLY
CONNECTED
LAYER
IS
CALLED
THE
OUTPUT
LAYER
AND
IN
CLASSIકCATION
SETTINGS
IT
REPRESENTS
THE
CLASS
SCORES
REGULAR
NEURAL
NETS
DON
T
SCALE
WELL
TO
FULL
IMAGES
IN
CIFAR
IMAGES
ARE
ONLY
OF
SIZE
WIDE
HIGH
COLOR
CHANNELS
SO
A
SINGLE
FULLY
CONNECTED
NEURON
IN
A
કRST
HIDDEN
LAYER
OF
A
REGULAR
NEURAL
NETWORK
WOULD
HAVE
WEIGHTS
THIS
AMOUNT
STILL
SEEMS
MANAGEABLE
BUT
CLEARLY
THIS
FULLY
CONNECTED
STRUCTURE
DOES
NOT
SCALE
TO
LARGER
IMAGES
FOR
EXAMPLE
AN
IMAGE
OF
MORE
RESPECTIBLE
SIZE
E
G
WOULD
LEAD
TO
NEURONS
THAT
HAVE
WEIGHTS
MOREOVER
WE
WOULD
ALMOST
CERTAINLY
WANT
TO
HAVE
SEVERAL
SUCH
NEURONS
SO
THE
PARAMETERS
WOULD
ADD
UP
QUICKLY
CLEARLY
THIS
FULL
CONNECTIVITY
IS
WASTEFUL
AND
THE
HUGE
NUMBER
OF
PARAMETERS
WOULD
QUICKLY
LEAD
TO
OVERકTTING
VOLUMES
OF
NEURONS
CONVOLUTIONAL
NEURAL
NETWORKS
TAKE
ADVANTAGE
OF
THE
FACT
THAT
THE
INPUT
CONSISTS
OF
IMAGES
AND
THEY
CONSTRAIN
THE
ARCHITECTURE
IN
A
MORE
SENSIBLE
WAY
IN
PARTICULAR
UNLIKE
A
REGULAR
NEURAL
NETWORK
THE
LAYERS
OF
A
CONVNET
HAVE
NEURONS
ARRANGED
IN
DIMENSIONS
WIDTH
HEIGHT
DEPTH
NOTE
THAT
THE
WORD
DEPTH
HERE
REFERS
TO
THE
THIRD
DIMENSION
OF
AN
ACTIVATION
VOLUME
NOT
TO
THE
DEPTH
OF
A
FULL
NEURAL
NETWORK
WHICH
CAN
REFER
TO
THE
TOTAL
NUMBER
OF
LAYERS
IN
A
NETWORK
FOR
EXAMPLE
THE
INPUT
IMAGES
IN
CIFAR
ARE
AN
INPUT
VOLUME
OF
ACTIVATIONS
AND
THE
VOLUME
HAS
DIMENSIONS
WIDTH
HEIGHT
DEPTH
RESPECTIVELY
AS
WE
WILL
SOON
SEE
THE
NEURONS
IN
A
LAYER
WILL
ONLY
BE
CONNECTED
TO
A
SMALL
REGION
OF
THE
LAYER
BEFORE
IT
INSTEAD
OF
ALL
OF
THE
NEURONS
IN
A
FULLY
CONNECTED
MANNER
MOREOVER
THE
કNAL
OUTPUT
LAYER
WOULD
FOR
CIFAR
HAVE
DIMENSIONS
BECAUSE
BY
THE
END
OF
THE
CONVNET
ARCHITECTURE
WE
WILL
REDUCE
THE
FULL
IMAGE
INTO
A
SINGLE
VECTOR
OF
CLASS
SCORES
ARRANGED
ALONG
THE
DEPTH
DIMENSION
HERE
IS
A
VISUALIZATION
LEFT
A
REGULAR
LAYER
NEURAL
NETWORK
RIGHT
A
CONVNET
ARRANGES
ITS
NEURONS
IN
THREE
DIMENSIONS
WIDTH
HEIGHT
DEPTH
AS
VISUALIZED
IN
ONE
OF
THE
LAYERS
EVERY
LAYER
OF
A
CONVNET
TRANSFORMS
THE
INPUT
VOLUME
TO
A
OUTPUT
VOLUME
OF
NEURON
ACTIVATIONS
IN
THIS
EXAMPLE
THE
RED
INPUT
LAYER
HOLDS
THE
IMAGE
SO
ITS
WIDTH
AND
HEIGHT
WOULD
BE
THE
DIMENSIONS
OF
THE
IMAGE
AND
THE
DEPTH
WOULD
BE
RED
GREEN
BLUE
CHANNELS
A
CONVNET
IS
MADE
UP
OF
LAYERS
EVERY
LAYER
HAS
A
SIMPLE
API
IT
TRANSFORMS
AN
INPUT
VOLUME
TO
AN
OUTPUT
VOLUME
WITH
SOME
DIFFERENTIABLE
FUNCTION
THAT
MAY
OR
MAY
NOT
HAVE
PARAMETERS
LAYERS
USED
TO
BUILD
CONVNETS
AS
WE
DESCRIBED
ABOVE
A
SIMPLE
CONVNET
IS
A
SEQUENCE
OF
LAYERS
AND
EVERY
LAYER
OF
A
CONVNET
TRANSFORMS
ONE
VOLUME
OF
ACTIVATIONS
TO
ANOTHER
THROUGH
A
DIFFERENTIABLE
FUNCTION
WE
USE
THREE
MAIN
TYPES
OF
LAYERS
TO
BUILD
CONVNET
ARCHITECTURES
CONVOLUTIONAL
LAYER
POOLING
LAYER
AND
FULLY
CONNECTED
LAYER
EXACTLY
AS
SEEN
IN
REGULAR
NEURAL
NETWORKS
WE
WILL
STACK
THESE
LAYERS
TO
FORM
A
FULL
CONVNET
ARCHITECTURE
EXAMPLE
ARCHITECTURE
OVERVIEW
WE
WILL
GO
INTO
MORE
DETAILS
BELOW
BUT
A
SIMPLE
CONVNET
FOR
CIFAR
CLASSIકCATION
COULD
HAVE
THE
ARCHITECTURE
INPUT
CONV
RELU
POOL
FC
IN
MORE
DETAIL
INPUT
WILL
HOLD
THE
RAW
PIXEL
VALUES
OF
THE
IMAGE
IN
THIS
CASE
AN
IMAGE
OF
WIDTH
HEIGHT
AND
WITH
THREE
COLOR
CHANNELS
R
G
B
CONV
LAYER
WILL
COMPUTE
THE
OUTPUT
OF
NEURONS
THAT
ARE
CONNECTED
TO
LOCAL
REGIONS
IN
THE
INPUT
EACH
COMPUTING
A
DOT
PRODUCT
BETWEEN
THEIR
WEIGHTS
AND
A
SMALL
REGION
THEY
ARE
CONNECTED
TO
IN
THE
INPUT
VOLUME
THIS
MAY
RESULT
IN
VOLUME
SUCH
AS
IF
WE
DECIDED
TO
USE
કLTERS
RELU
LAYER
WILL
APPLY
AN
ELEMENTWISE
ACTIVATION
FUNCTION
SUCH
AS
THE
MAX
X
THRESHOLDING
AT
ZERO
THIS
LEAVES
THE
SIZE
OF
THE
VOLUME
UNCHANGED
POOL
LAYER
WILL
PERFORM
A
DOWNSAMPLING
OPERATION
ALONG
THE
SPATIAL
DIMENSIONS
WIDTH
HEIGHT
RESULTING
IN
VOLUME
SUCH
AS
FC
I
E
FULLY
CONNECTED
LAYER
WILL
COMPUTE
THE
CLASS
SCORES
RESULTING
IN
VOLUME
OF
SIZE
WHERE
EACH
OF
THE
NUMBERS
CORRESPOND
TO
A
CLASS
SCORE
SUCH
AS
AMONG
THE
CATEGORIES
OF
CIFAR
AS
WITH
ORDINARY
NEURAL
NETWORKS
AND
AS
THE
NAME
IMPLIES
EACH
NEURON
IN
THIS
LAYER
WILL
BE
CONNECTED
TO
ALL
THE
NUMBERS
IN
THE
PREVIOUS
VOLUME
IN
THIS
WAY
CONVNETS
TRANSFORM
THE
ORIGINAL
IMAGE
LAYER
BY
LAYER
FROM
THE
ORIGINAL
PIXEL
VALUES
TO
THE
કNAL
CLASS
SCORES
NOTE
THAT
SOME
LAYERS
CONTAIN
PARAMETERS
AND
OTHER
DON
T
IN
PARTICULAR
THE
CONV
FC
LAYERS
PERFORM
TRANSFORMATIONS
THAT
ARE
A
FUNCTION
OF
NOT
ONLY
THE
ACTIVATIONS
IN
THE
INPUT
VOLUME
BUT
ALSO
OF
THE
PARAMETERS
THE
WEIGHTS
AND
BIASES
OF
THE
NEURONS
ON
THE
OTHER
HAND
THE
RELU
POOL
LAYERS
WILL
IMPLEMENT
A
કXED
FUNCTION
THE
PARAMETERS
IN
THE
CONV
FC
LAYERS
WILL
BE
TRAINED
WITH
GRADIENT
DESCENT
SO
THAT
THE
CLASS
SCORES
THAT
THE
CONVNET
COMPUTES
ARE
CONSISTENT
WITH
THE
LABELS
IN
THE
TRAINING
SET
FOR
EACH
IMAGE
IN
SUMMARY
A
CONVNET
ARCHITECTURE
IS
IN
THE
SIMPLEST
CASE
A
LIST
OF
LAYERS
THAT
TRANSFORM
THE
IMAGE
VOLUME
INTO
AN
OUTPUT
VOLUME
E
G
HOLDING
THE
CLASS
SCORES
THERE
ARE
A
FEW
DISTINCT
TYPES
OF
LAYERS
E
G
CONV
FC
RELU
POOL
ARE
BY
FAR
THE
MOST
POPULAR
EACH
LAYER
ACCEPTS
AN
INPUT
VOLUME
AND
TRANSFORMS
IT
TO
AN
OUTPUT
VOLUME
THROUGH
A
DIFFERENTIABLE
FUNCTION
EACH
LAYER
MAY
OR
MAY
NOT
HAVE
PARAMETERS
E
G
CONV
FC
DO
RELU
POOL
DON
T
EACH
LAYER
MAY
OR
MAY
NOT
HAVE
ADDITIONAL
HYPERPARAMETERS
E
G
CONV
FC
POOL
DO
RELU
DOESN
T
THE
ACTIVATIONS
OF
AN
EXAMPLE
CONVNET
ARCHITECTURE
THE
INITIAL
VOLUME
STORES
THE
RAW
IMAGE
PIXELS
LEFT
AND
THE
LAST
VOLUME
STORES
THE
CLASS
SCORES
RIGHT
EACH
VOLUME
OF
ACTIVATIONS
ALONG
THE
PROCESSING
PATH
IS
SHOWN
AS
A
COLUMN
SINCE
IT
DIFકCULT
TO
VISUALIZE
VOLUMES
WE
LAY
OUT
EACH
VOLUME
SLICES
IN
ROWS
THE
LAST
LAYER
VOLUME
HOLDS
THE
SCORES
FOR
EACH
CLASS
BUT
HERE
WE
ONLY
VISUALIZE
THE
SORTED
TOP
SCORES
AND
PRINT
THE
LABELS
OF
EACH
ONE
THE
FULL
IS
SHOWN
IN
THE
HEADER
OF
OUR
WEBSITE
THE
ARCHITECTURE
SHOWN
HERE
IS
A
TINY
VGG
NET
WHICH
WE
WILL
DISCUSS
LATER
WE
NOW
DESCRIBE
THE
INDIVIDUAL
LAYERS
AND
THE
DETAILS
OF
THEIR
HYPERPARAMETERS
AND
THEIR
CONNECTIVITIES
CONVOLUTIONAL
LAYER
THE
CONV
LAYER
IS
THE
CORE
BUILDING
BLOCK
OF
A
CONVOLUTIONAL
NETWORK
THAT
DOES
MOST
OF
THE
COMPUTATIONAL
HEAVY
LIFTING
OVERVIEW
AND
INTUITION
WITHOUT
BRAIN
STUFF
LETS
કRST
DISCUSS
WHAT
THE
CONV
LAYER
COMPUTES
WITHOUT
BRAIN
NEURON
ANALOGIES
THE
CONV
LAYER
PARAMETERS
CONSIST
OF
A
SET
OF
LEARNABLE
કLTERS
EVERY
કLTER
IS
SMALL
SPATIALLY
ALONG
WIDTH
AND
HEIGHT
BUT
EXTENDS
THROUGH
THE
FULL
DEPTH
OF
THE
INPUT
VOLUME
FOR
EXAMPLE
A
TYPICAL
કLTER
ON
A
કRST
LAYER
OF
A
CONVNET
MIGHT
HAVE
SIZE
I
E
PIXELS
WIDTH
AND
HEIGHT
AND
BECAUSE
IMAGES
HAVE
DEPTH
THE
COLOR
CHANNELS
DURING
THE
FORWARD
PASS
WE
SLIDE
MORE
PRECISELY
CONVOLVE
EACH
કLTER
ACROSS
THE
WIDTH
AND
HEIGHT
OF
THE
INPUT
VOLUME
AND
COMPUTE
DOT
PRODUCTS
BETWEEN
THE
ENTRIES
OF
THE
કLTER
AND
THE
INPUT
AT
ANY
POSITION
AS
WE
SLIDE
THE
કLTER
OVER
THE
WIDTH
AND
HEIGHT
OF
THE
INPUT
VOLUME
WE
WILL
PRODUCE
A
DIMENSIONAL
ACTIVATION
MAP
THAT
GIVES
THE
RESPONSES
OF
THAT
કLTER
AT
EVERY
SPATIAL
POSITION
INTUITIVELY
THE
NETWORK
WILL
LEARN
કLTERS
THAT
ACTIVATE
WHEN
THEY
SEE
SOME
TYPE
OF
VISUAL
FEATURE
SUCH
AS
AN
EDGE
OF
SOME
ORIENTATION
OR
A
BLOTCH
OF
SOME
COLOR
ON
THE
કRST
LAYER
OR
EVENTUALLY
ENTIRE
HONEYCOMB
OR
WHEEL
LIKE
PATTERNS
ON
HIGHER
LAYERS
OF
THE
NETWORK
NOW
WE
WILL
HAVE
AN
ENTIRE
SET
OF
કLTERS
IN
EACH
CONV
LAYER
E
G
કLTERS
AND
EACH
OF
THEM
WILL
PRODUCE
A
SEPARATE
DIMENSIONAL
ACTIVATION
MAP
WE
WILL
STACK
THESE
ACTIVATION
MAPS
ALONG
THE
DEPTH
DIMENSION
AND
PRODUCE
THE
OUTPUT
VOLUME
THE
BRAIN
VIEW
IF
YOU
RE
A
FAN
OF
THE
BRAIN
NEURON
ANALOGIES
EVERY
ENTRY
IN
THE
OUTPUT
VOLUME
CAN
ALSO
BE
INTERPRETED
AS
AN
OUTPUT
OF
A
NEURON
THAT
LOOKS
AT
ONLY
A
SMALL
REGION
IN
THE
INPUT
AND
SHARES
PARAMETERS
WITH
ALL
NEURONS
TO
THE
LEFT
AND
RIGHT
SPATIALLY
SINCE
THESE
NUMBERS
ALL
RESULT
FROM
APPLYING
THE
SAME
કLTER
WE
NOW
DISCUSS
THE
DETAILS
OF
THE
NEURON
CONNECTIVITIES
THEIR
ARRANGEMENT
IN
SPACE
AND
THEIR
PARAMETER
SHARING
SCHEME
LOCAL
CONNECTIVITY
WHEN
DEALING
WITH
HIGH
DIMENSIONAL
INPUTS
SUCH
AS
IMAGES
AS
WE
SAW
ABOVE
IT
IS
IMPRACTICAL
TO
CONNECT
NEURONS
TO
ALL
NEURONS
IN
THE
PREVIOUS
VOLUME
INSTEAD
WE
WILL
CONNECT
EACH
NEURON
TO
ONLY
A
LOCAL
REGION
OF
THE
INPUT
VOLUME
THE
SPATIAL
EXTENT
OF
THIS
CONNECTIVITY
IS
A
HYPERPARAMETER
CALLED
THE
RECEPTIVE
કELD
OF
THE
NEURON
EQUIVALENTLY
THIS
IS
THE
કLTER
SIZE
THE
EXTENT
OF
THE
CONNECTIVITY
ALONG
THE
DEPTH
AXIS
IS
ALWAYS
EQUAL
TO
THE
DEPTH
OF
THE
INPUT
VOLUME
IT
IS
IMPORTANT
TO
EMPHASIZE
AGAIN
THIS
ASYMMETRY
IN
HOW
WE
TREAT
THE
SPATIAL
DIMENSIONS
WIDTH
AND
HEIGHT
AND
THE
DEPTH
DIMENSION
THE
CONNECTIONS
ARE
LOCAL
IN
SPACE
ALONG
WIDTH
AND
HEIGHT
BUT
ALWAYS
FULL
ALONG
THE
ENTIRE
DEPTH
OF
THE
INPUT
VOLUME
EXAMPLE
FOR
EXAMPLE
SUPPOSE
THAT
THE
INPUT
VOLUME
HAS
SIZE
E
G
AN
RGB
CIFAR
IMAGE
IF
THE
RECEPTIVE
કELD
OR
THE
કLTER
SIZE
IS
THEN
EACH
NEURON
IN
THE
CONV
LAYER
WILL
HAVE
WEIGHTS
TO
A
REGION
IN
THE
INPUT
VOLUME
FOR
A
TOTAL
OF
WEIGHTS
AND
BIAS
PARAMETER
NOTICE
THAT
THE
EXTENT
OF
THE
CONNECTIVITY
ALONG
THE
DEPTH
AXIS
MUST
BE
SINCE
THIS
IS
THE
DEPTH
OF
THE
INPUT
VOLUME
EXAMPLE
SUPPOSE
AN
INPUT
VOLUME
HAD
SIZE
THEN
USING
AN
EXAMPLE
RECEPTIVE
કELD
SIZE
OF
EVERY
NEURON
IN
THE
CONV
LAYER
WOULD
NOW
HAVE
A
TOTAL
OF
CONNECTIONS
TO
THE
INPUT
VOLUME
NOTICE
THAT
AGAIN
THE
CONNECTIVITY
IS
LOCAL
IN
SPACE
E
G
BUT
FULL
ALONG
THE
INPUT
DEPTH
LEFT
AN
EXAMPLE
INPUT
VOLUME
IN
RED
E
G
A
CIFAR
IMAGE
AND
AN
EXAMPLE
VOLUME
OF
NEURONS
IN
THE
કRST
CONVOLUTIONAL
LAYER
EACH
NEURON
IN
THE
CONVOLUTIONAL
LAYER
IS
CONNECTED
ONLY
TO
A
LOCAL
REGION
IN
THE
INPUT
VOLUME
SPATIALLY
BUT
TO
THE
FULL
DEPTH
I
E
ALL
COLOR
CHANNELS
NOTE
THERE
ARE
MULTIPLE
NEURONS
IN
THIS
EXAMPLE
ALONG
THE
DEPTH
ALL
LOOKING
AT
THE
SAME
REGION
IN
THE
INPUT
SEE
DISCUSSION
OF
DEPTH
COLUMNS
IN
TEXT
BELOW
RIGHT
THE
NEURONS
FROM
THE
NEURAL
NETWORK
CHAPTER
REMAIN
UNCHANGED
THEY
STILL
COMPUTE
A
DOT
PRODUCT
OF
THEIR
WEIGHTS
WITH
THE
INPUT
FOLLOWED
BY
A
NON
LINEARITY
BUT
THEIR
CONNECTIVITY
IS
NOW
RESTRICTED
TO
BE
LOCAL
SPATIALLY
SPATIAL
ARRANGEMENT
WE
HAVE
EXPLAINED
THE
CONNECTIVITY
OF
EACH
NEURON
IN
THE
CONV
LAYER
TO
THE
INPUT
VOLUME
BUT
WE
HAVEN
T
YET
DISCUSSED
HOW
MANY
NEURONS
THERE
ARE
IN
THE
OUTPUT
VOLUME
OR
HOW
THEY
ARE
ARRANGED
THREE
HYPERPARAMETERS
CONTROL
THE
SIZE
OF
THE
OUTPUT
VOLUME
THE
DEPTH
STRIDE
AND
ZERO
PADDING
WE
DISCUSS
THESE
NEXT
FIRST
THE
DEPTH
OF
THE
OUTPUT
VOLUME
IS
A
HYPERPARAMETER
IT
CORRESPONDS
TO
THE
NUMBER
OF
કLTERS
WE
WOULD
LIKE
TO
USE
EACH
LEARNING
TO
LOOK
FOR
SOMETHING
DIFFERENT
IN
THE
INPUT
FOR
EXAMPLE
IF
THE
કRST
CONVOLUTIONAL
LAYER
TAKES
AS
INPUT
THE
RAW
IMAGE
THEN
DIFFERENT
NEURONS
ALONG
THE
DEPTH
DIMENSION
MAY
ACTIVATE
IN
PRESENCE
OF
VARIOUS
ORIENTED
EDGED
OR
BLOBS
OF
COLOR
WE
WILL
REFER
TO
A
SET
OF
NEURONS
THAT
ARE
ALL
LOOKING
AT
THE
SAME
REGION
OF
THE
INPUT
AS
A
DEPTH
COLUMN
SOME
PEOPLE
ALSO
PREFER
THE
TERM
કBRE
SECOND
WE
MUST
SPECIFY
THE
STRIDE
WITH
WHICH
WE
SLIDE
THE
કLTER
WHEN
THE
STRIDE
IS
THEN
WE
MOVE
THE
કLTERS
ONE
PIXEL
AT
A
TIME
WHEN
THE
STRIDE
IS
OR
UNCOMMONLY
OR
MORE
THOUGH
THIS
IS
RARE
IN
PRACTICE
THEN
THE
કLTERS
JUMP
PIXELS
AT
A
TIME
AS
WE
SLIDE
THEM
AROUND
THIS
WILL
PRODUCE
SMALLER
OUTPUT
VOLUMES
SPATIALLY
AS
WE
WILL
SOON
SEE
SOMETIMES
IT
WILL
BE
CONVENIENT
TO
PAD
THE
INPUT
VOLUME
WITH
ZEROS
AROUND
THE
BORDER
THE
SIZE
OF
THIS
ZERO
PADDING
IS
A
HYPERPARAMETER
THE
NICE
FEATURE
OF
ZERO
PADDING
IS
THAT
IT
WILL
ALLOW
US
TO
CONTROL
THE
SPATIAL
SIZE
OF
THE
OUTPUT
VOLUMES
MOST
COMMONLY
AS
WE
LL
SEE
SOON
WE
WILL
USE
IT
TO
EXACTLY
PRESERVE
THE
SPATIAL
SIZE
OF
THE
INPUT
VOLUME
SO
THE
INPUT
AND
OUTPUT
WIDTH
AND
HEIGHT
ARE
THE
SAME
WE
CAN
COMPUTE
THE
SPATIAL
SIZE
OF
THE
OUTPUT
VOLUME
AS
A
FUNCTION
OF
THE
INPUT
VOLUME
SIZE
W
THE
RECEPTIVE
કELD
SIZE
OF
THE
CONV
LAYER
NEURONS
F
THE
STRIDE
WITH
WHICH
THEY
ARE
APPLIED
AND
THE
AMOUNT
OF
ZERO
PADDING
USED
P
ON
THE
BORDER
YOU
CAN
CONVINCE
YOURSELF
THAT
THE
CORRECT
FORMULA
FOR
CALCULATING
HOW
MANY
NEURONS
કT
IS
GIVEN
BY
W
F
FOR
EXAMPLE
FOR
A
INPUT
AND
A
કLTER
WITH
STRIDE
AND
PAD
WE
WOULD
GET
A
OUTPUT
WITH
STRIDE
WE
WOULD
GET
A
OUTPUT
LETS
ALSO
SEE
ONE
MORE
GRAPHICAL
EXAMPLE
ILLUSTRATION
OF
SPATIAL
ARRANGEMENT
IN
THIS
EXAMPLE
THERE
IS
ONLY
ONE
SPATIAL
DIMENSION
X
AXIS
ONE
NEURON
WITH
A
RECEPTIVE
કELD
SIZE
OF
F
THE
INPUT
SIZE
IS
W
AND
THERE
IS
ZERO
PADDING
OF
P
LEFT
THE
NEURON
STRIDED
ACROSS
THE
INPUT
IN
STRIDE
OF
GIVING
OUTPUT
OF
SIZE
RIGHT
THE
NEURON
USES
STRIDE
OF
GIVING
OUTPUT
OF
SIZE
NOTICE
THAT
STRIDE
COULD
NOT
BE
USED
SINCE
IT
WOULDN
T
કT
NEATLY
ACROSS
THE
VOLUME
IN
TERMS
OF
THE
EQUATION
THIS
CAN
BE
DETERMINED
SINCE
IS
NOT
DIVISIBLE
BY
THE
NEURON
WEIGHTS
ARE
IN
THIS
EXAMPLE
SHOWN
ON
VERY
RIGHT
AND
ITS
BIAS
IS
ZERO
THESE
WEIGHTS
ARE
SHARED
ACROSS
ALL
YELLOW
NEURONS
SEE
PARAMETER
SHARING
BELOW
USE
OF
ZERO
PADDING
IN
THE
EXAMPLE
ABOVE
ON
LEFT
NOTE
THAT
THE
INPUT
DIMENSION
WAS
AND
THE
OUTPUT
DIMENSION
WAS
EQUAL
ALSO
THIS
WORKED
OUT
SO
BECAUSE
OUR
RECEPTIVE
કELDS
WERE
AND
WE
USED
ZERO
PADDING
OF
IF
THERE
WAS
NO
ZERO
PADDING
USED
THEN
THE
OUTPUT
VOLUME
WOULD
HAVE
HAD
SPATIAL
DIMENSION
OF
ONLY
BECAUSE
THAT
IT
IS
HOW
MANY
NEURONS
WOULD
HAVE
કT
ACROSS
THE
ORIGINAL
INPUT
IN
GENERAL
SETTING
ZERO
PADDING
TO
BE
P
F
WHEN
THE
STRIDE
IS
ENSURES
THAT
THE
INPUT
VOLUME
AND
OUTPUT
VOLUME
WILL
HAVE
THE
SAME
SIZE
SPATIALLY
IT
IS
VERY
COMMON
TO
USE
ZERO
PADDING
IN
THIS
WAY
AND
WE
WILL
DISCUSS
THE
FULL
REASONS
WHEN
WE
TALK
MORE
ABOUT
CONVNET
ARCHITECTURES
CONSTRAINTS
ON
STRIDES
NOTE
AGAIN
THAT
THE
SPATIAL
ARRANGEMENT
HYPERPARAMETERS
HAVE
MUTUAL
CONSTRAINTS
FOR
EXAMPLE
WHEN
THE
INPUT
HAS
SIZE
W
NO
ZERO
PADDING
IS
USED
P
AND
THE
કLTER
SIZE
IS
F
THEN
IT
WOULD
BE
IMPOSSIBLE
TO
USE
STRIDE
SINCE
W
F
I
E
NOT
AN
INTEGER
INDICATING
THAT
THE
NEURONS
DON
T
કT
NEATLY
AND
SYMMETRICALLY
ACROSS
THE
INPUT
THEREFORE
THIS
SETTING
OF
THE
HYPERPARAMETERS
IS
CONSIDERED
TO
BE
INVALID
AND
A
CONVNET
LIBRARY
COULD
THROW
AN
EXCEPTION
OR
ZERO
PAD
THE
REST
TO
MAKE
IT
કT
OR
CROP
THE
INPUT
TO
MAKE
IT
કT
OR
SOMETHING
AS
WE
WILL
SEE
IN
THE
CONVNET
ARCHITECTURES
SECTION
SIZING
THE
CONVNETS
APPROPRIATELY
SO
THAT
ALL
THE
DIMENSIONS
WORK
OUT
CAN
BE
A
REAL
HEADACHE
WHICH
THE
USE
OF
ZERO
PADDING
AND
SOME
DESIGN
GUIDELINES
WILL
SIGNIકCANTLY
ALLEVIATE
REAL
WORLD
EXAMPLE
THE
ARCHITECTURE
THAT
WON
THE
IMAGENET
CHALLENGE
IN
ACCEPTED
IMAGES
OF
SIZE
ON
THE
કRST
CONVOLUTIONAL
LAYER
IT
USED
NEURONS
WITH
RECEPTIVE
કELD
SIZE
F
STRIDE
AND
NO
ZERO
PADDING
P
SINCE
AND
SINCE
THE
CONV
LAYER
HAD
A
DEPTH
OF
K
THE
CONV
LAYER
OUTPUT
VOLUME
HAD
SIZE
EACH
OF
THE
NEURONS
IN
THIS
VOLUME
WAS
CONNECTED
TO
A
REGION
OF
SIZE
IN
THE
INPUT
VOLUME
MOREOVER
ALL
NEURONS
IN
EACH
DEPTH
COLUMN
ARE
CONNECTED
TO
THE
SAME
REGION
OF
THE
INPUT
BUT
OF
COURSE
WITH
DIFFERENT
WEIGHTS
AS
A
FUN
ASIDE
IF
YOU
READ
THE
ACTUAL
PAPER
IT
CLAIMS
THAT
THE
INPUT
IMAGES
WERE
WHICH
IS
SURELY
INCORRECT
BECAUSE
IS
QUITE
CLEARLY
NOT
AN
INTEGER
THIS
HAS
CONFUSED
MANY
PEOPLE
IN
THE
HISTORY
OF
CONVNETS
AND
LITTLE
IS
KNOWN
ABOUT
WHAT
HAPPENED
MY
OWN
BEST
GUESS
IS
THAT
ALEX
USED
ZERO
PADDING
OF
EXTRA
PIXELS
THAT
HE
DOES
NOT
MENTION
IN
THE
PAPER
PARAMETER
SHARING
PARAMETER
SHARING
SCHEME
IS
USED
IN
CONVOLUTIONAL
LAYERS
TO
CONTROL
THE
NUMBER
OF
PARAMETERS
USING
THE
REAL
WORLD
EXAMPLE
ABOVE
WE
SEE
THAT
THERE
ARE
NEURONS
IN
THE
કRST
CONV
LAYER
AND
EACH
HAS
WEIGHTS
AND
BIAS
TOGETHER
THIS
ADDS
UP
TO
PARAMETERS
ON
THE
કRST
LAYER
OF
THE
CONVNET
ALONE
CLEARLY
THIS
NUMBER
IS
VERY
HIGH
IT
TURNS
OUT
THAT
WE
CAN
DRAMATICALLY
REDUCE
THE
NUMBER
OF
PARAMETERS
BY
MAKING
ONE
REASONABLE
ASSUMPTION
THAT
IF
ONE
FEATURE
IS
USEFUL
TO
COMPUTE
AT
SOME
SPATIAL
POSITION
X
Y
THEN
IT
SHOULD
ALSO
BE
USEFUL
TO
COMPUTE
AT
A
DIFFERENT
POSITION
IN
OTHER
WORDS
DENOTING
A
SINGLE
DIMENSIONAL
SLICE
OF
DEPTH
AS
A
DEPTH
SLICE
E
G
A
VOLUME
OF
SIZE
HAS
DEPTH
SLICES
EACH
OF
SIZE
WE
ARE
GOING
TO
CONSTRAIN
THE
NEURONS
IN
EACH
DEPTH
SLICE
TO
USE
THE
SAME
WEIGHTS
AND
BIAS
WITH
THIS
PARAMETER
SHARING
SCHEME
THE
કRST
CONV
LAYER
IN
OUR
EXAMPLE
WOULD
NOW
HAVE
ONLY
UNIQUE
SET
OF
WEIGHTS
ONE
FOR
EACH
DEPTH
SLICE
FOR
A
TOTAL
OF
UNIQUE
WEIGHTS
OR
PARAMETERS
BIASES
ALTERNATIVELY
ALL
NEURONS
IN
EACH
DEPTH
SLICE
WILL
NOW
BE
USING
THE
SAME
PARAMETERS
IN
PRACTICE
DURING
BACKPROPAGATION
EVERY
NEURON
IN
THE
VOLUME
WILL
COMPUTE
THE
GRADIENT
FOR
ITS
WEIGHTS
BUT
THESE
GRADIENTS
WILL
BE
ADDED
UP
ACROSS
EACH
DEPTH
SLICE
AND
ONLY
UPDATE
A
SINGLE
SET
OF
WEIGHTS
PER
SLICE
NOTICE
THAT
IF
ALL
NEURONS
IN
A
SINGLE
DEPTH
SLICE
ARE
USING
THE
SAME
WEIGHT
VECTOR
THEN
THE
FORWARD
PASS
OF
THE
CONV
LAYER
CAN
IN
EACH
DEPTH
SLICE
BE
COMPUTED
AS
A
CONVOLUTION
OF
THE
NEURON
WEIGHTS
WITH
THE
INPUT
VOLUME
HENCE
THE
NAME
CONVOLUTIONAL
LAYER
THIS
IS
WHY
IT
IS
COMMON
TO
REFER
TO
THE
SETS
OF
WEIGHTS
AS
A
કLTER
OR
A
KERNEL
THAT
IS
CONVOLVED
WITH
THE
INPUT
EXAMPLE
કLTERS
LEARNED
BY
KRIZHEVSKY
ET
AL
EACH
OF
THE
કLTERS
SHOWN
HERE
IS
OF
SIZE
AND
EACH
ONE
IS
SHARED
BY
THE
NEURONS
IN
ONE
DEPTH
SLICE
NOTICE
THAT
THE
PARAMETER
SHARING
ASSUMPTION
IS
RELATIVELY
REASONABLE
IF
DETECTING
A
HORIZONTAL
EDGE
IS
IMPORTANT
AT
SOME
LOCATION
IN
THE
IMAGE
IT
SHOULD
INTUITIVELY
BE
USEFUL
AT
SOME
OTHER
LOCATION
AS
WELL
DUE
TO
THE
TRANSLATIONALLY
INVARIANT
STRUCTURE
OF
IMAGES
THERE
IS
THEREFORE
NO
NEED
TO
RELEARN
TO
DETECT
A
HORIZONTAL
EDGE
AT
EVERY
ONE
OF
THE
DISTINCT
LOCATIONS
IN
THE
CONV
LAYER
OUTPUT
VOLUME
NOTE
THAT
SOMETIMES
THE
PARAMETER
SHARING
ASSUMPTION
MAY
NOT
MAKE
SENSE
THIS
IS
ESPECIALLY
THE
CASE
WHEN
THE
INPUT
IMAGES
TO
A
CONVNET
HAVE
SOME
SPECIકC
CENTERED
STRUCTURE
WHERE
WE
SHOULD
EXPECT
FOR
EXAMPLE
THAT
COMPLETELY
DIFFERENT
FEATURES
SHOULD
BE
LEARNED
ON
ONE
SIDE
OF
THE
IMAGE
THAN
ANOTHER
ONE
PRACTICAL
EXAMPLE
IS
WHEN
THE
INPUT
ARE
FACES
THAT
HAVE
BEEN
CENTERED
IN
THE
IMAGE
YOU
MIGHT
EXPECT
THAT
DIFFERENT
EYE
SPECIકC
OR
HAIR
SPECIકC
FEATURES
COULD
AND
SHOULD
BE
LEARNED
IN
DIFFERENT
SPATIAL
LOCATIONS
IN
THAT
CASE
IT
IS
COMMON
TO
RELAX
THE
PARAMETER
SHARING
SCHEME
AND
INSTEAD
SIMPLY
CALL
THE
LAYER
A
LOCALLY
CONNECTED
LAYER
NUMPY
EXAMPLES
TO
MAKE
THE
DISCUSSION
ABOVE
MORE
CONCRETE
LETS
EXPRESS
THE
SAME
IDEAS
BUT
IN
CODE
AND
WITH
A
SPECIકC
EXAMPLE
SUPPOSE
THAT
THE
INPUT
VOLUME
IS
A
NUMPY
ARRAY
THEN
A
DEPTH
COLUMN
OR
A
કBRE
AT
POSITION
WOULD
BE
THE
ACTIVATIONS
A
DEPTH
SLICE
OR
EQUIVALENTLY
AN
ACTIVATION
MAP
AT
DEPTH
WOULD
BE
THE
ACTIVATIONS
CONV
LAYER
EXAMPLE
SUPPOSE
THAT
THE
INPUT
VOLUME
HAS
SHAPE
SUPPOSE
FURTHER
THAT
WE
USE
NO
ZERO
PADDING
P
THAT
THE
કLTER
SIZE
IS
F
AND
THAT
THE
STRIDE
IS
THE
OUTPUT
VOLUME
WOULD
THEREFORE
HAVE
SPATIAL
SIZE
GIVING
A
VOLUME
WITH
WIDTH
AND
HEIGHT
OF
THE
ACTIVATION
MAP
IN
THE
OUTPUT
VOLUME
CALL
IT
THEN
LOOK
AS
FOLLOWS
ONLY
SOME
OF
THE
ELEMENTS
ARE
COMPUTED
IN
THIS
EXAMPLE
WOULD
REMEMBER
THAT
IN
NUMPY
THE
OPERATION
ABOVE
DENOTES
ELEMENTWISE
MULTIPLICATION
BETWEEN
THE
ARRAYS
NOTICE
ALSO
THAT
THE
WEIGHT
VECTOR
IS
THE
WEIGHT
VECTOR
OF
THAT
NEURON
AND
IS
THE
BIAS
HERE
IS
ASSUMED
TO
BE
OF
SHAPE
SINCE
THE
કLTER
SIZE
IS
AND
THE
DEPTH
OF
THE
INPUT
VOLUME
IS
NOTICE
THAT
AT
EACH
POINT
WE
ARE
COMPUTING
THE
DOT
PRODUCT
AS
SEEN
BEFORE
IN
ORDINARY
NEURAL
NETWORKS
ALSO
WE
SEE
THAT
WE
ARE
USING
THE
SAME
WEIGHT
AND
BIAS
DUE
TO
PARAMETER
SHARING
AND
WHERE
THE
DIMENSIONS
ALONG
THE
WIDTH
ARE
INCREASING
IN
STEPS
OF
I
E
THE
STRIDE
TO
CONSTRUCT
A
SECOND
ACTIVATION
MAP
IN
THE
OUTPUT
VOLUME
WE
WOULD
HAVE
V
NP
SUM
X
V
NP
SUM
X
V
NP
SUM
X
V
NP
SUM
X
V
NP
SUM
X
EXAMPLE
OF
GOING
ALONG
Y
V
NP
SUM
X
OR
ALONG
BOTH
WHERE
WE
SEE
THAT
WE
ARE
INDEXING
INTO
THE
SECOND
DEPTH
DIMENSION
IN
AT
INDEX
BECAUSE
WE
ARE
COMPUTING
THE
SECOND
ACTIVATION
MAP
AND
THAT
A
DIFFERENT
SET
OF
PARAMETERS
IS
NOW
USED
IN
THE
EXAMPLE
ABOVE
WE
ARE
FOR
BREVITY
LEAVING
OUT
SOME
OF
THE
OTHER
OPERATIONS
THE
CONV
LAYER
WOULD
PERFORM
TO
કLL
THE
OTHER
PARTS
OF
THE
OUTPUT
ARRAY
V
ADDITIONALLY
RECALL
THAT
THESE
ACTIVATION
MAPS
ARE
OFTEN
FOLLOWED
ELEMENTWISE
THROUGH
AN
ACTIVATION
FUNCTION
SUCH
AS
RELU
BUT
THIS
IS
NOT
SHOWN
HERE
SUMMARY
TO
SUMMARIZE
THE
CONV
LAYER
ACCEPTS
A
VOLUME
OF
SIZE
REQUIRES
FOUR
HYPERPARAMETERS
NUMBER
OF
કLTERS
K
THEIR
SPATIAL
EXTENT
F
THE
STRIDE
THE
AMOUNT
OF
ZERO
PADDING
P
PRODUCES
A
VOLUME
OF
SIZE
WHERE
F
F
I
E
WIDTH
AND
HEIGHT
ARE
COMPUTED
EQUALLY
BY
SYMMETRY
K
WITH
PARAMETER
SHARING
IT
INTRODUCES
F
F
WEIGHTS
PER
કLTER
FOR
A
TOTAL
OF
F
F
K
WEIGHTS
AND
K
BIASES
IN
THE
OUTPUT
VOLUME
THE
D
TH
DEPTH
SLICE
OF
SIZE
IS
THE
RESULT
OF
PERFORMING
A
VALID
CONVOLUTION
OF
THE
D
TH
કLTER
OVER
THE
INPUT
VOLUME
WITH
A
STRIDE
OF
AND
THEN
OFFSET
BY
D
TH
BIAS
A
COMMON
SETTING
OF
THE
HYPERPARAMETERS
IS
F
P
HOWEVER
THERE
ARE
COMMON
CONVENTIONS
AND
RULES
OF
THUMB
THAT
MOTIVATE
THESE
HYPERPARAMETERS
SEE
THE
CONVNET
ARCHITECTURES
SECTION
BELOW
CONVOLUTION
DEMO
BELOW
IS
A
RUNNING
DEMO
OF
A
CONV
LAYER
SINCE
VOLUMES
ARE
HARD
TO
VISUALIZE
ALL
THE
VOLUMES
THE
INPUT
VOLUME
IN
BLUE
THE
WEIGHT
VOLUMES
IN
RED
THE
OUTPUT
VOLUME
IN
GREEN
ARE
VISUALIZED
WITH
EACH
DEPTH
SLICE
STACKED
IN
ROWS
THE
INPUT
VOLUME
IS
OF
SIZE
AND
THE
CONV
LAYER
PARAMETERS
ARE
K
F
P
THAT
IS
WE
HAVE
TWO
કLTERS
OF
SIZE
AND
THEY
ARE
APPLIED
WITH
A
STRIDE
OF
THEREFORE
THE
OUTPUT
VOLUME
SIZE
HAS
SPATIAL
SIZE
MOREOVER
NOTICE
THAT
A
PADDING
OF
P
IS
APPLIED
TO
THE
INPUT
VOLUME
MAKING
THE
OUTER
BORDER
OF
THE
INPUT
VOLUME
ZERO
THE
VISUALIZATION
BELOW
ITERATES
OVER
THE
OUTPUT
ACTIVATIONS
GREEN
AND
SHOWS
THAT
EACH
ELEMENT
IS
COMPUTED
BY
ELEMENTWISE
MULTIPLYING
THE
HIGHLIGHTED
INPUT
BLUE
WITH
THE
કLTER
RED
SUMMING
IT
UP
AND
THEN
OFFSETTING
THE
RESULT
BY
THE
BIAS
INPUT
VOLUME
PAD
X
FILTER
FILTER
OUTPUT
VOL
O
IMPLEMENTATION
AS
MATRIX
MULTIPLICATION
NOTE
THAT
THE
CONVOLUTION
OPERATION
ESSENTIALLY
PERFORMS
DOT
PRODUCTS
BETWEEN
THE
કLTERS
AND
LOCAL
REGIONS
OF
THE
INPUT
A
COMMON
IMPLEMENTATION
PATTERN
OF
THE
CONV
LAYER
IS
TO
TAKE
ADVANTAGE
OF
THIS
FACT
AND
FORMULATE
THE
FORWARD
PASS
OF
A
CONVOLUTIONAL
LAYER
AS
ONE
BIG
MATRIX
MULTIPLY
AS
FOLLOWS
THE
LOCAL
REGIONS
IN
THE
INPUT
IMAGE
ARE
STRETCHED
OUT
INTO
COLUMNS
IN
AN
OPERATION
COMMONLY
CALLED
FOR
EXAMPLE
IF
THE
INPUT
IS
AND
IT
IS
TO
BE
CONVOLVED
WITH
કLTERS
AT
STRIDE
THEN
WE
WOULD
TAKE
BLOCKS
OF
PIXELS
IN
THE
INPUT
AND
STRETCH
EACH
BLOCK
INTO
A
COLUMN
VECTOR
OF
SIZE
ITERATING
THIS
PROCESS
IN
THE
INPUT
AT
STRIDE
OF
GIVES
LOCATIONS
ALONG
BOTH
WIDTH
AND
HEIGHT
LEADING
TO
AN
OUTPUT
MATRIX
OF
OF
SIZE
X
WHERE
EVERY
COLUMN
IS
A
STRETCHED
OUT
RECEPTIVE
કELD
AND
THERE
ARE
OF
THEM
IN
TOTAL
NOTE
THAT
SINCE
THE
RECEPTIVE
કELDS
OVERLAP
EVERY
NUMBER
IN
THE
INPUT
VOLUME
MAY
BE
DUPLICATED
IN
MULTIPLE
DISTINCT
COLUMNS
THE
WEIGHTS
OF
THE
CONV
LAYER
ARE
SIMILARLY
STRETCHED
OUT
INTO
ROWS
FOR
EXAMPLE
IF
THERE
ARE
કLTERS
OF
SIZE
THIS
WOULD
GIVE
A
MATRIX
OF
SIZE
X
THE
RESULT
OF
A
CONVOLUTION
IS
NOW
EQUIVALENT
TO
PERFORMING
ONE
LARGE
MATRIX
MULTIPLY
WHICH
EVALUATES
THE
DOT
PRODUCT
BETWEEN
EVERY
કLTER
AND
EVERY
RECEPTIVE
કELD
LOCATION
IN
OUR
EXAMPLE
THE
OUTPUT
OF
THIS
OPERATION
WOULD
BE
X
GIVING
THE
OUTPUT
OF
THE
DOT
PRODUCT
OF
EACH
કLTER
AT
EACH
LOCATION
THE
RESULT
MUST
કNALLY
BE
RESHAPED
BACK
TO
ITS
PROPER
OUTPUT
DIMENSION
THIS
APPROACH
HAS
THE
DOWNSIDE
THAT
IT
CAN
USE
A
LOT
OF
MEMORY
SINCE
SOME
VALUES
IN
THE
INPUT
VOLUME
ARE
REPLICATED
MULTIPLE
TIMES
IN
HOWEVER
THE
BENEકT
IS
THAT
THERE
ARE
MANY
VERY
EFકCIENT
IMPLEMENTATIONS
OF
MATRIX
MULTIPLICATION
THAT
WE
CAN
TAKE
ADVANTAGE
OF
FOR
EXAMPLE
IN
THE
COMMONLY
USED
API
MOREOVER
THE
SAME
IDEA
CAN
BE
REUSED
TO
PERFORM
THE
POOLING
OPERATION
WHICH
WE
DISCUSS
NEXT
BACKPROPAGATION
THE
BACKWARD
PASS
FOR
A
CONVOLUTION
OPERATION
FOR
BOTH
THE
DATA
AND
THE
WEIGHTS
IS
ALSO
A
CONVOLUTION
BUT
WITH
SPATIALLY
ぱIPPED
કLTERS
THIS
IS
EASY
TO
DERIVE
IN
THE
DIMENSIONAL
CASE
WITH
A
TOY
EXAMPLE
NOT
EXPANDED
ON
FOR
NOW
CONVOLUTION
AS
AN
ASIDE
SEVERAL
PAPERS
USE
CONVOLUTIONS
AS
કRST
INVESTIGATED
BY
SOME
PEOPLE
ARE
AT
કRST
CONFUSED
TO
SEE
CONVOLUTIONS
ESPECIALLY
WHEN
THEY
COME
FROM
SIGNAL
PROCESSING
BACKGROUND
NORMALLY
SIGNALS
ARE
DIMENSIONAL
SO
CONVOLUTIONS
DO
NOT
MAKE
SENSE
IT
JUST
POINTWISE
SCALING
HOWEVER
IN
CONVNETS
THIS
IS
NOT
THE
CASE
BECAUSE
ONE
MUST
REMEMBER
THAT
WE
OPERATE
OVER
DIMENSIONAL
VOLUMES
AND
THAT
THE
કLTERS
ALWAYS
EXTEND
THROUGH
THE
FULL
DEPTH
OF
THE
INPUT
VOLUME
FOR
EXAMPLE
IF
THE
INPUT
IS
THEN
DOING
CONVOLUTIONS
WOULD
EFFECTIVELY
BE
DOING
DIMENSIONAL
DOT
PRODUCTS
SINCE
THE
INPUT
DEPTH
IS
CHANNELS
DILATED
CONVOLUTIONS
A
RECENT
DEVELOPMENT
E
G
SEE
IS
TO
INTRODUCE
ONE
MORE
HYPERPARAMETER
TO
THE
CONV
LAYER
CALLED
THE
DILATION
SO
FAR
WE
VE
ONLY
DICUSSED
CONV
કLTERS
THAT
ARE
CONTIGUOUS
HOWEVER
IT
POSSIBLE
TO
HAVE
કLTERS
THAT
HAVE
SPACES
BETWEEN
EACH
CELL
CALLED
DILATION
AS
AN
EXAMPLE
IN
ONE
DIMENSION
A
કLTER
OF
SIZE
WOULD
COMPUTE
OVER
INPUT
THE
FOLLOWING
N
OF
FOR
DILATION
THE
કLTER
WOULD
INST
IN
OTHER
WORDS
THERE
IS
A
GAP
OF
BETWEEN
THE
APPLICATIONS
THIS
CAN
BE
VERY
USEFUL
IN
SOME
SETTINGS
TO
USE
IN
CONJUNCTION
WITH
DILATED
કLTERS
BECAUSE
IT
ALLOWS
YOU
TO
MERGE
SPATIAL
INFORMATION
ACROSS
THE
INPUTS
MUCH
MORE
AGRESSIVELY
WITH
FEWER
LAYERS
FOR
EXAMPLE
IF
YOU
STACK
TWO
CONV
LAYERS
ON
TOP
OF
EACH
OTHER
THAN
YOU
CAN
CONVINCE
YOURSELF
THAT
THE
NEURONS
ON
THE
LAYER
ARE
A
FUNCTION
OF
A
PATCH
OF
THE
INPUT
WE
WOULD
SAY
THAT
THE
EFFECTIVE
RECEPTIVE
કELD
OF
THESE
NEURONS
IS
IF
WE
USE
DILATED
CONVOLUTIONS
THEN
THIS
EFFECTIVE
RECEPTIVE
કELD
WOULD
GROW
MUCH
QUICKER
POOLING
LAYER
IT
IS
COMMON
TO
PERIODICALLY
INSERT
A
POOLING
LAYER
IN
BETWEEN
SUCCESSIVE
CONV
LAYERS
IN
A
CONVNET
ARCHITECTURE
ITS
FUNCTION
IS
TO
PROGRESSIVELY
REDUCE
THE
SPATIAL
SIZE
OF
THE
REPRESENTATION
TO
REDUCE
THE
AMOUNT
OF
PARAMETERS
AND
COMPUTATION
IN
THE
NETWORK
AND
HENCE
TO
ALSO
CONTROL
OVERકTTING
THE
POOLING
LAYER
OPERATES
INDEPENDENTLY
ON
EVERY
DEPTH
SLICE
OF
THE
INPUT
AND
RESIZES
IT
SPATIALLY
USING
THE
MAX
OPERATION
THE
MOST
COMMON
FORM
IS
A
POOLING
LAYER
WITH
કLTERS
OF
SIZE
APPLIED
WITH
A
STRIDE
OF
DOWNSAMPLES
EVERY
DEPTH
SLICE
IN
THE
INPUT
BY
ALONG
BOTH
WIDTH
AND
HEIGHT
DISCARDING
OF
THE
ACTIVATIONS
EVERY
MAX
OPERATION
WOULD
IN
THIS
CASE
BE
TAKING
A
MAX
OVER
NUMBERS
LITTLE
REGION
IN
SOME
DEPTH
SLICE
THE
DEPTH
DIMENSION
REMAINS
UNCHANGED
MORE
GENERALLY
THE
POOLING
LAYER
ACCEPTS
A
VOLUME
OF
SIZE
REQUIRES
TWO
HYPERPARAMETERS
THEIR
SPATIAL
EXTENT
F
THE
STRIDE
PRODUCES
A
VOLUME
OF
SIZE
WHERE
F
F
INTRODUCES
ZERO
PARAMETERS
SINCE
IT
COMPUTES
A
કXED
FUNCTION
OF
THE
INPUT
NOTE
THAT
IT
IS
NOT
COMMON
TO
USE
ZERO
PADDING
FOR
POOLING
LAYERS
IT
IS
WORTH
NOTING
THAT
THERE
ARE
ONLY
TWO
COMMONLY
SEEN
VARIATIONS
OF
THE
MAX
POOLING
LAYER
FOUND
IN
PRACTICE
A
POOLING
LAYER
WITH
F
ALSO
CALLED
OVERLAPPING
POOLING
AND
MORE
COMMONLY
F
POOLING
SIZES
WITH
LARGER
RECEPTIVE
કELDS
ARE
TOO
DESTRUCTIVE
GENERAL
POOLING
IN
ADDITION
TO
MAX
POOLING
THE
POOLING
UNITS
CAN
ALSO
PERFORM
OTHER
FUNCTIONS
SUCH
AS
AVERAGE
POOLING
OR
EVEN
NORM
POOLING
AVERAGE
POOLING
WAS
OFTEN
USED
HISTORICALLY
BUT
HAS
RECENTLY
FALLEN
OUT
OF
FAVOR
COMPARED
TO
THE
MAX
POOLING
OPERATION
WHICH
HAS
BEEN
SHOWN
TO
WORK
BETTER
IN
PRACTICE
POOLING
LAYER
DOWNSAMPLES
THE
VOLUME
SPATIALLY
INDEPENDENTLY
IN
EACH
DEPTH
SLICE
OF
THE
INPUT
VOLUME
LEFT
IN
THIS
EXAMPLE
THE
INPUT
VOLUME
OF
SIZE
IS
POOLED
WITH
કLTER
SIZE
STRIDE
INTO
OUTPUT
VOLUME
OF
SIZE
NOTICE
THAT
THE
VOLUME
DEPTH
IS
PRESERVED
RIGHT
THE
MOST
COMMON
DOWNSAMPLING
OPERATION
IS
MAX
GIVING
RISE
TO
MAX
POOLING
HERE
SHOWN
WITH
A
STRIDE
OF
THAT
IS
EACH
MAX
IS
TAKEN
OVER
NUMBERS
LITTLE
SQUARE
BACKPROPAGATION
RECALL
FROM
THE
BACKPROPAGATION
CHAPTER
THAT
THE
BACKWARD
PASS
FOR
A
MAX
X
Y
OPERATION
HAS
A
SIMPLE
INTERPRETATION
AS
ONLY
ROUTING
THE
GRADIENT
TO
THE
INPUT
THAT
HAD
THE
HIGHEST
VALUE
IN
THE
FORWARD
PASS
HENCE
DURING
THE
FORWARD
PASS
OF
A
POOLING
LAYER
IT
IS
COMMON
TO
KEEP
TRACK
OF
THE
INDEX
OF
THE
MAX
ACTIVATION
SOMETIMES
ALSO
CALLED
THE
SWITCHES
SO
THAT
GRADIENT
ROUTING
IS
EFકCIENT
DURING
BACKPROPAGATION
GETTING
RID
OF
POOLING
MANY
PEOPLE
DISLIKE
THE
POOLING
OPERATION
AND
THINK
THAT
WE
CAN
GET
AWAY
WITHOUT
IT
FOR
EXAMPLE
PROPOSES
TO
DISCARD
THE
POOLING
LAYER
IN
FAVOR
OF
ARCHITECTURE
THAT
ONLY
CONSISTS
OF
REPEATED
CONV
LAYERS
TO
REDUCE
THE
SIZE
OF
THE
REPRESENTATION
THEY
SUGGEST
USING
LARGER
STRIDE
IN
CONV
LAYER
ONCE
IN
A
WHILE
DISCARDING
POOLING
LAYERS
HAS
ALSO
BEEN
FOUND
TO
BE
IMPORTANT
IN
TRAINING
GOOD
GENERATIVE
MODELS
SUCH
AS
VARIATIONAL
AUTOENCODERS
VAES
OR
GENERATIVE
ADVERSARIAL
NETWORKS
GANS
IT
SEEMS
LIKELY
THAT
FUTURE
ARCHITECTURES
WILL
FEATURE
VERY
FEW
TO
NO
POOLING
LAYERS
NORMALIZATION
LAYER
MANY
TYPES
OF
NORMALIZATION
LAYERS
HAVE
BEEN
PROPOSED
FOR
USE
IN
CONVNET
ARCHITECTURES
SOMETIMES
WITH
THE
INTENTIONS
OF
IMPLEMENTING
INHIBITION
SCHEMES
OBSERVED
IN
THE
BIOLOGICAL
BRAIN
HOWEVER
THESE
LAYERS
HAVE
SINCE
FALLEN
OUT
OF
FAVOR
BECAUSE
IN
PRACTICE
THEIR
CONTRIBUTION
HAS
BEEN
SHOWN
TO
BE
MINIMAL
IF
ANY
FOR
VARIOUS
TYPES
OF
NORMALIZATIONS
SEE
THE
DISCUSSION
IN
ALEX
KRIZHEVSKY
FULLY
CONNECTED
LAYER
NEURONS
IN
A
FULLY
CONNECTED
LAYER
HAVE
FULL
CONNECTIONS
TO
ALL
ACTIVATIONS
IN
THE
PREVIOUS
LAYER
AS
SEEN
IN
REGULAR
NEURAL
NETWORKS
THEIR
ACTIVATIONS
CAN
HENCE
BE
COMPUTED
WITH
A
MATRIX
MULTIPLICATION
FOLLOWED
BY
A
BIAS
OFFSET
SEE
THE
NEURAL
NETWORK
SECTION
OF
THE
NOTES
FOR
MORE
INFORMATION
CONVERTING
FC
LAYERS
TO
CONV
LAYERS
IT
IS
WORTH
NOTING
THAT
THE
ONLY
DIFFERENCE
BETWEEN
FC
AND
CONV
LAYERS
IS
THAT
THE
NEURONS
IN
THE
CONV
LAYER
ARE
CONNECTED
ONLY
TO
A
LOCAL
REGION
IN
THE
INPUT
AND
THAT
MANY
OF
THE
NEURONS
IN
A
CONV
VOLUME
SHARE
PARAMETERS
HOWEVER
THE
NEURONS
IN
BOTH
LAYERS
STILL
COMPUTE
DOT
PRODUCTS
SO
THEIR
FUNCTIONAL
FORM
IS
IDENTICAL
THEREFORE
IT
TURNS
OUT
THAT
IT
POSSIBLE
TO
CONVERT
BETWEEN
FC
AND
CONV
LAYERS
FOR
ANY
CONV
LAYER
THERE
IS
AN
FC
LAYER
THAT
IMPLEMENTS
THE
SAME
FORWARD
FUNCTION
THE
WEIGHT
MATRIX
WOULD
BE
A
LARGE
MATRIX
THAT
IS
MOSTLY
ZERO
EXCEPT
FOR
AT
CERTAIN
BLOCKS
DUE
TO
LOCAL
CONNECTIVITY
WHERE
THE
WEIGHTS
IN
MANY
OF
THE
BLOCKS
ARE
EQUAL
DUE
TO
PARAMETER
SHARING
CONVERSELY
ANY
FC
LAYER
CAN
BE
CONVERTED
TO
A
CONV
LAYER
FOR
EXAMPLE
AN
FC
LAYER
WITH
K
THAT
IS
LOOKING
AT
SOME
INPUT
VOLUME
OF
SIZE
CAN
BE
EQUIVALENTLY
EXPRESSED
AS
A
CONV
LAYER
WITH
F
P
K
IN
OTHER
WORDS
WE
ARE
SETTING
THE
કLTER
SIZE
TO
BE
EXACTLY
THE
SIZE
OF
THE
INPUT
VOLUME
AND
HENCE
THE
OUTPUT
WILL
SIMPLY
BE
SINCE
ONLY
A
SINGLE
DEPTH
COLUMN
કTS
ACROSS
THE
INPUT
VOLUME
GIVING
IDENTICAL
RESULT
AS
THE
INITIAL
FC
LAYER
FC
CONV
CONVERSION
OF
THESE
TWO
CONVERSIONS
THE
ABILITY
TO
CONVERT
AN
FC
LAYER
TO
A
CONV
LAYER
IS
PARTICULARLY
USEFUL
IN
PRACTICE
CONSIDER
A
CONVNET
ARCHITECTURE
THAT
TAKES
A
IMAGE
AND
THEN
USES
A
SERIES
OF
CONV
LAYERS
AND
POOL
LAYERS
TO
REDUCE
THE
IMAGE
TO
AN
ACTIVATIONS
VOLUME
OF
SIZE
IN
AN
ALEXNET
ARCHITECTURE
THAT
WE
LL
SEE
LATER
THIS
IS
DONE
BY
USE
OF
POOLING
LAYERS
THAT
DOWNSAMPLE
THE
INPUT
SPATIALLY
BY
A
FACTOR
OF
TWO
EACH
TIME
MAKING
THE
કNAL
SPATIAL
SIZE
FROM
THERE
AN
ALEXNET
USES
TWO
FC
LAYERS
OF
SIZE
AND
કNALLY
THE
LAST
FC
LAYERS
WITH
NEURONS
THAT
COMPUTE
THE
CLASS
SCORES
WE
CAN
CONVERT
EACH
OF
THESE
THREE
FC
LAYERS
TO
CONV
LAYERS
AS
DESCRIBED
ABOVE
REPLACE
THE
કRST
FC
LAYER
THAT
LOOKS
AT
VOLUME
WITH
A
CONV
LAYER
THAT
USES
કLTER
SIZE
F
GIVING
OUTPUT
VOLUME
REPLACE
THE
SECOND
FC
LAYER
WITH
A
CONV
LAYER
THAT
USES
કLTER
SIZE
F
GIVING
OUTPUT
VOLUME
REPLACE
THE
LAST
FC
LAYER
SIMILARLY
WITH
F
GIVING
કNAL
OUTPUT
EACH
OF
THESE
CONVERSIONS
COULD
IN
PRACTICE
INVOLVE
MANIPULATING
E
G
RESHAPING
THE
WEIGHT
MATRIX
W
IN
EACH
FC
LAYER
INTO
CONV
LAYER
કLTERS
IT
TURNS
OUT
THAT
THIS
CONVERSION
ALLOWS
US
TO
SLIDE
THE
ORIGINAL
CONVNET
VERY
EFકCIENTLY
ACROSS
MANY
SPATIAL
POSITIONS
IN
A
LARGER
IMAGE
IN
A
SINGLE
FORWARD
PASS
FOR
EXAMPLE
IF
IMAGE
GIVES
A
VOLUME
OF
SIZE
I
E
A
REDUCTION
BY
THEN
FORWARDING
AN
IMAGE
OF
SIZE
THROUGH
THE
CONVERTED
ARCHITECTURE
WOULD
GIVE
THE
EQUIVALENT
VOLUME
IN
SIZE
SINCE
FOLLOWING
THROUGH
WITH
THE
NEXT
CONV
LAYERS
THAT
WE
JUST
CONVERTED
FROM
FC
LAYERS
WOULD
NOW
GIVE
THE
કNAL
VOLUME
OF
SIZE
SINCE
NOTE
THAT
INSTEAD
OF
A
SINGLE
VECTOR
OF
CLASS
SCORES
OF
SIZE
WE
RE
NOW
GETTING
AND
ENTIRE
ARRAY
OF
CLASS
SCORES
ACROSS
THE
IMAGE
EVALUATING
THE
ORIGINAL
CONVNET
WITH
FC
LAYERS
INDEPENDENTLY
ACROSS
CROPS
OF
THE
IMAGE
IN
STRIDES
OF
PIXELS
GIVES
AN
IDENTICAL
RESULT
TO
FORWARDING
THE
CONVERTED
CONVNET
ONE
TIME
NATURALLY
FORWARDING
THE
CONVERTED
CONVNET
A
SINGLE
TIME
IS
MUCH
MORE
EFકCIENT
THAN
ITERATING
THE
ORIGINAL
CONVNET
OVER
ALL
THOSE
LOCATIONS
SINCE
THE
EVALUATIONS
SHARE
COMPUTATION
THIS
TRICK
IS
OFTEN
USED
IN
PRACTICE
TO
GET
BETTER
PERFORMANCE
WHERE
FOR
EXAMPLE
IT
IS
COMMON
TO
RESIZE
AN
IMAGE
TO
MAKE
IT
BIGGER
USE
A
CONVERTED
CONVNET
TO
EVALUATE
THE
CLASS
SCORES
AT
MANY
SPATIAL
POSITIONS
AND
THEN
AVERAGE
THE
CLASS
SCORES
LASTLY
WHAT
IF
WE
WANTED
TO
EFકCIENTLY
APPLY
THE
ORIGINAL
CONVNET
OVER
THE
IMAGE
BUT
AT
A
STRIDE
SMALLER
THAN
PIXELS
WE
COULD
ACHIEVE
THIS
WITH
MULTIPLE
FORWARD
PASSES
FOR
EXAMPLE
NOTE
THAT
IF
WE
WANTED
TO
USE
A
STRIDE
OF
PIXELS
WE
COULD
DO
SO
BY
COMBINING
THE
VOLUMES
RECEIVED
BY
FORWARDING
THE
CONVERTED
CONVNET
TWICE
FIRST
OVER
THE
ORIGINAL
IMAGE
AND
SECOND
OVER
THE
IMAGE
BUT
WITH
THE
IMAGE
SHIFTED
SPATIALLY
BY
PIXELS
ALONG
BOTH
WIDTH
AND
HEIGHT
AN
IPYTHON
NOTEBOOK
ON
SHOWS
HOW
TO
PERFORM
THE
CONVERSION
IN
PRACTICE
IN
CODE
USING
CAFFE
CONVNET
ARCHITECTURES
WE
HAVE
SEEN
THAT
CONVOLUTIONAL
NETWORKS
ARE
COMMONLY
MADE
UP
OF
ONLY
THREE
LAYER
TYPES
CONV
POOL
WE
ASSUME
MAX
POOL
UNLESS
STATED
OTHERWISE
AND
FC
SHORT
FOR
FULLY
CONNECTED
WE
WILL
ALSO
EXPLICITLY
WRITE
THE
RELU
ACTIVATION
FUNCTION
AS
A
LAYER
WHICH
APPLIES
ELEMENTWISE
NON
LINEARITY
IN
THIS
SECTION
WE
DISCUSS
HOW
THESE
ARE
COMMONLY
STACKED
TOGETHER
TO
FORM
ENTIRE
CONVNETS
LAYER
PATTERNS
THE
MOST
COMMON
FORM
OF
A
CONVNET
ARCHITECTURE
STACKS
A
FEW
CONV
RELU
LAYERS
FOLLOWS
THEM
WITH
POOL
LAYERS
AND
REPEATS
THIS
PATTERN
UNTIL
THE
IMAGE
HAS
BEEN
MERGED
SPATIALLY
TO
A
SMALL
SIZE
AT
SOME
POINT
IT
IS
COMMON
TO
TRANSITION
TO
FULLY
CONNECTED
LAYERS
THE
LAST
FULLY
CONNECTED
LAYER
HOLDS
THE
OUTPUT
SUCH
AS
THE
CLASS
SCORES
IN
OTHER
WORDS
THE
MOST
COMMON
CONVNET
ARCHITECTURE
FOLLOWS
THE
PATTERN
WHERE
THE
INDICATES
REPETITION
AND
THE
POOL
INDICATES
AN
OPTIONAL
POOLING
LAYER
MOREOVER
AND
USUALLY
K
AND
USUALLY
FOR
EXAMPLE
HERE
ARE
SOME
COMMON
CONVNET
ARCHITECTURES
YOU
MAY
SEE
THAT
FOLLOW
THIS
PATTERN
IMPLEMENTS
A
LINEAR
CLASSIકER
HERE
HERE
WE
SEE
TWO
CONV
LAYERS
STACKED
BEFORE
EVERY
POOL
LAYER
THIS
IS
GENERALLY
A
GOOD
IDEA
FOR
LARGER
AND
DEEPER
NETWORKS
BECAUSE
MULTIPLE
STACKED
CONV
LAYERS
CAN
DEVELOP
MORE
COMPLEX
FEATURES
OF
THE
INPUT
VOLUME
BEFORE
THE
DESTRUCTIVE
POOLING
OPERATION
PREFER
A
STACK
OF
SMALL
કLTER
CONV
TO
ONE
LARGE
RECEPTIVE
કELD
CONV
LAYER
SUPPOSE
THAT
YOU
STACK
THREE
CONV
LAYERS
ON
TOP
OF
EACH
OTHER
WITH
NON
LINEARITIES
IN
BETWEEN
OF
COURSE
IN
THIS
ARRANGEMENT
EACH
NEURON
ON
THE
કRST
CONV
LAYER
HAS
A
VIEW
OF
THE
INPUT
VOLUME
A
NEURON
ON
THE
SECOND
CONV
LAYER
HAS
A
VIEW
OF
THE
કRST
CONV
LAYER
AND
HENCE
BY
EXTENSION
A
VIEW
OF
THE
INPUT
VOLUME
SIMILARLY
A
NEURON
ON
THE
THIRD
CONV
LAYER
HAS
A
VIEW
OF
THE
CONV
LAYER
AND
HENCE
A
VIEW
OF
THE
INPUT
VOLUME
SUPPOSE
THAT
INSTEAD
OF
THESE
THREE
LAYERS
OF
CONV
WE
ONLY
WANTED
TO
USE
A
SINGLE
CONV
LAYER
WITH
RECEPTIVE
કELDS
THESE
NEURONS
WOULD
HAVE
A
RECEPTIVE
કELD
SIZE
OF
THE
INPUT
VOLUME
THAT
IS
IDENTICAL
IN
SPATIAL
EXTENT
BUT
WITH
SEVERAL
DISADVANTAGES
FIRST
THE
NEURONS
WOULD
BE
COMPUTING
A
LINEAR
FUNCTION
OVER
THE
INPUT
WHILE
THE
THREE
STACKS
OF
CONV
LAYERS
CONTAIN
NON
LINEARITIES
THAT
MAKE
THEIR
FEATURES
MORE
EXPRESSIVE
SECOND
IF
WE
SUPPOSE
THAT
ALL
THE
VOLUMES
HAVE
C
CHANNELS
THEN
IT
CAN
BE
SEEN
THAT
THE
SINGLE
CONV
LAYER
WOULD
CONTAIN
C
C
PARAMETERS
WHILE
THE
THREE
CONV
LAYERS
WOULD
ONLY
CONTAIN
C
C
PARAMETERS
INTUITIVELY
STACKING
CONV
LAYERS
WITH
TINY
કLTERS
AS
OPPOSED
TO
HAVING
ONE
CONV
LAYER
WITH
BIG
કLTERS
ALLOWS
US
TO
EXPRESS
MORE
POWERFUL
FEATURES
OF
THE
INPUT
AND
WITH
FEWER
PARAMETERS
AS
A
PRACTICAL
DISADVANTAGE
WE
MIGHT
NEED
MORE
MEMORY
TO
HOLD
ALL
THE
INTERMEDIATE
CONV
LAYER
RESULTS
IF
WE
PLAN
TO
DO
BACKPROPAGATION
RECENT
DEPARTURES
IT
SHOULD
BE
NOTED
THAT
THE
CONVENTIONAL
PARADIGM
OF
A
LINEAR
LIST
OF
LAYERS
HAS
RECENTLY
BEEN
CHALLANGED
IN
GOOGLE
INCEPTION
ARCHITECTURES
AND
ALSO
IN
CURRENT
STATE
OF
THE
ART
RESIDUAL
NETWORKS
FROM
MICROSOFT
RESEARCH
ASIA
BOTH
OF
THESE
SEE
DETAILS
BELOW
IN
CASE
STUDIES
SECTION
FEATURE
MORE
INTRICATE
AND
DIFFERENT
CONNECTIVITY
STRUCTURES
IN
PRACTICE
USE
WHATEVER
WORKS
BEST
ON
IMAGENET
IF
YOU
RE
FEELING
A
BIT
OF
A
FATIGUE
IN
THINKING
ABOUT
THE
ARCHITECTURAL
DECISIONS
YOU
LL
BE
PLEASED
TO
KNOW
THAT
IN
OR
MORE
OF
APPLICATIONS
YOU
SHOULD
NOT
HAVE
TO
WORRY
ABOUT
THESE
I
LIKE
TO
SUMMARIZE
THIS
POINT
AS
DON
T
BE
A
HERO
INSTEAD
OF
ROLLING
YOUR
OWN
ARCHITECTURE
FOR
A
PROBLEM
YOU
SHOULD
LOOK
AT
WHATEVER
ARCHITECTURE
CURRENTLY
WORKS
BEST
ON
IMAGENET
DOWNLOAD
A
PRETRAINED
MODEL
AND
કNETUNE
IT
ON
YOUR
DATA
YOU
SHOULD
RARELY
EVER
HAVE
TO
TRAIN
A
CONVNET
FROM
SCRATCH
OR
DESIGN
ONE
FROM
SCRATCH
I
ALSO
MADE
THIS
POINT
AT
THE
LAYER
SIZING
PATTERNS
UNTIL
NOW
WE
VE
OMITTED
MENTIONS
OF
COMMON
HYPERPARAMETERS
USED
IN
EACH
OF
THE
LAYERS
IN
A
CONVNET
WE
WILL
કRST
STATE
THE
COMMON
RULES
OF
THUMB
FOR
SIZING
THE
ARCHITECTURES
AND
THEN
FOLLOW
THE
RULES
WITH
A
DISCUSSION
OF
THE
NOTATION
THE
INPUT
LAYER
THAT
CONTAINS
THE
IMAGE
SHOULD
BE
DIVISIBLE
BY
MANY
TIMES
COMMON
NUMBERS
INCLUDE
E
G
CIFAR
E
G
STL
OR
E
G
COMMON
IMAGENET
CONVNETS
AND
THE
CONV
LAYERS
SHOULD
BE
USING
SMALL
કLTERS
E
G
OR
AT
MOST
USING
A
STRIDE
OF
AND
CRUCIALLY
PADDING
THE
INPUT
VOLUME
WITH
ZEROS
IN
SUCH
WAY
THAT
THE
CONV
LAYER
DOES
NOT
ALTER
THE
SPATIAL
DIMENSIONS
OF
THE
INPUT
THAT
IS
WHEN
F
THEN
USING
P
WILL
RETAIN
THE
ORIGINAL
SIZE
OF
THE
INPUT
WHEN
F
P
FOR
A
GENERAL
F
IT
CAN
BE
SEEN
THAT
P
F
PRESERVES
THE
INPUT
SIZE
IF
YOU
MUST
USE
BIGGER
કLTER
SIZES
SUCH
AS
OR
SO
IT
IS
ONLY
COMMON
TO
SEE
THIS
ON
THE
VERY
કRST
CONV
LAYER
THAT
IS
LOOKING
AT
THE
INPUT
IMAGE
THE
POOL
LAYERS
ARE
IN
CHARGE
OF
DOWNSAMPLING
THE
SPATIAL
DIMENSIONS
OF
THE
INPUT
THE
MOST
COMMON
SETTING
IS
TO
USE
MAX
POOLING
WITH
RECEPTIVE
કELDS
I
E
F
AND
WITH
A
STRIDE
OF
I
E
NOTE
THAT
THIS
DISCARDS
EXACTLY
OF
THE
ACTIVATIONS
IN
AN
INPUT
VOLUME
DUE
TO
DOWNSAMPLING
BY
IN
BOTH
WIDTH
AND
HEIGHT
ANOTHER
SLIGHTLY
LESS
COMMON
SETTING
IS
TO
USE
RECEPTIVE
કELDS
WITH
A
STRIDE
OF
BUT
THIS
MAKES
IT
IS
VERY
UNCOMMON
TO
SEE
RECEPTIVE
કELD
SIZES
FOR
MAX
POOLING
THAT
ARE
LARGER
THAN
BECAUSE
THE
POOLING
IS
THEN
TOO
LOSSY
AND
AGGRESSIVE
THIS
USUALLY
LEADS
TO
WORSE
PERFORMANCE
REDUCING
SIZING
HEADACHES
THE
SCHEME
PRESENTED
ABOVE
IS
PLEASING
BECAUSE
ALL
THE
CONV
LAYERS
PRESERVE
THE
SPATIAL
SIZE
OF
THEIR
INPUT
WHILE
THE
POOL
LAYERS
ALONE
ARE
IN
CHARGE
OF
DOWN
SAMPLING
THE
VOLUMES
SPATIALLY
IN
AN
ALTERNATIVE
SCHEME
WHERE
WE
USE
STRIDES
GREATER
THAN
OR
DON
T
ZERO
PAD
THE
INPUT
IN
CONV
LAYERS
WE
WOULD
HAVE
TO
VERY
CAREFULLY
KEEP
TRACK
OF
THE
INPUT
VOLUMES
THROUGHOUT
THE
CNN
ARCHITECTURE
AND
MAKE
SURE
THAT
ALL
STRIDES
AND
કLTERS
WORK
OUT
AND
THAT
THE
CONVNET
ARCHITECTURE
IS
NICELY
AND
SYMMETRICALLY
WIRED
WHY
USE
STRIDE
OF
IN
CONV
SMALLER
STRIDES
WORK
BETTER
IN
PRACTICE
ADDITIONALLY
AS
ALREADY
MENTIONED
STRIDE
ALLOWS
US
TO
LEAVE
ALL
SPATIAL
DOWN
SAMPLING
TO
THE
POOL
LAYERS
WITH
THE
CONV
LAYERS
ONLY
TRANSFORMING
THE
INPUT
VOLUME
DEPTH
WISE
WHY
USE
PADDING
IN
ADDITION
TO
THE
AFOREMENTIONED
BENEકT
OF
KEEPING
THE
SPATIAL
SIZES
CONSTANT
AFTER
CONV
DOING
THIS
ACTUALLY
IMPROVES
PERFORMANCE
IF
THE
CONV
LAYERS
WERE
TO
NOT
ZERO
PAD
THE
INPUTS
AND
ONLY
PERFORM
VALID
CONVOLUTIONS
THEN
THE
SIZE
OF
THE
VOLUMES
WOULD
REDUCE
BY
A
SMALL
AMOUNT
AFTER
EACH
CONV
AND
THE
INFORMATION
AT
THE
BORDERS
WOULD
BE
WASHED
AWAY
TOO
QUICKLY
COMPROMISING
BASED
ON
MEMORY
CONSTRAINTS
IN
SOME
CASES
ESPECIALLY
EARLY
IN
THE
CONVNET
ARCHITECTURES
THE
AMOUNT
OF
MEMORY
CAN
BUILD
UP
VERY
QUICKLY
WITH
THE
RULES
OF
THUMB
PRESENTED
ABOVE
FOR
EXAMPLE
કLTERING
A
IMAGE
WITH
THREE
CONV
LAYERS
WITH
કLTERS
EACH
AND
PADDING
WOULD
CREATE
THREE
ACTIVATION
VOLUMES
OF
SIZE
THIS
AMOUNTS
TO
A
TOTAL
OF
ABOUT
MILLION
ACTIVATIONS
OR
OF
MEMORY
PER
IMAGE
FOR
BOTH
ACTIVATIONS
AND
GRADIENTS
SINCE
GPUS
ARE
OFTEN
BOTTLENECKED
BY
MEMORY
IT
MAY
BE
NECESSARY
TO
COMPROMISE
IN
PRACTICE
PEOPLE
PREFER
TO
MAKE
THE
COMPROMISE
AT
ONLY
THE
કRST
CONV
LAYER
OF
THE
NETWORK
FOR
EXAMPLE
ONE
COMPROMISE
MIGHT
BE
TO
USE
A
કRST
CONV
LAYER
WITH
કLTER
SIZES
OF
AND
STRIDE
OF
AS
SEEN
IN
A
ZF
NET
AS
ANOTHER
EXAMPLE
AN
ALEXNET
USES
કLER
SIZES
OF
AND
STRIDE
OF
CASE
STUDIES
THERE
ARE
SEVERAL
ARCHITECTURES
IN
THE
કELD
OF
CONVOLUTIONAL
NETWORKS
THAT
HAVE
A
NAME
THE
MOST
COMMON
ARE
LENET
THE
કRST
SUCCESSFUL
APPLICATIONS
OF
CONVOLUTIONAL
NETWORKS
WERE
DEVELOPED
BY
YANN
LECUN
IN
OF
THESE
THE
BEST
KNOWN
IS
THE
ARCHITECTURE
THAT
WAS
USED
TO
READ
ZIP
CODES
DIGITS
ETC
ALEXNET
THE
કRST
WORK
THAT
POPULARIZED
CONVOLUTIONAL
NETWORKS
IN
COMPUTER
VISION
WAS
THE
DEVELOPED
BY
ALEX
KRIZHEVSKY
ILYA
SUTSKEVER
AND
GEOFF
HINTON
THE
ALEXNET
WAS
SUBMITTED
TO
THE
IN
AND
SIGNIકCANTLY
OUTPERFORMED
THE
SECOND
RUNNER
UP
TOP
ERROR
OF
COMPARED
TO
RUNNER
UP
WITH
ERROR
THE
NETWORK
HAD
A
VERY
SIMILAR
ARCHITECTURE
TO
LENET
BUT
WAS
DEEPER
BIGGER
AND
FEATURED
CONVOLUTIONAL
LAYERS
STACKED
ON
TOP
OF
EACH
OTHER
PREVIOUSLY
IT
WAS
COMMON
TO
ONLY
HAVE
A
SINGLE
CONV
LAYER
ALWAYS
IMMEDIATELY
FOLLOWED
BY
A
POOL
LAYER
ZF
NET
THE
ILSVRC
WINNER
WAS
A
CONVOLUTIONAL
NETWORK
FROM
MATTHEW
ZEILER
AND
ROB
FERGUS
IT
BECAME
KNOWN
AS
THE
SHORT
FOR
ZEILER
FERGUS
NET
IT
WAS
AN
IMPROVEMENT
ON
ALEXNET
BY
TWEAKING
THE
ARCHITECTURE
HYPERPARAMETERS
IN
PARTICULAR
BY
EXPANDING
THE
SIZE
OF
THE
MIDDLE
CONVOLUTIONAL
LAYERS
AND
MAKING
THE
STRIDE
AND
કLTER
SIZE
ON
THE
કRST
LAYER
SMALLER
GOOGLENET
THE
ILSVRC
WINNER
WAS
A
CONVOLUTIONAL
NETWORK
FROM
FROM
GOOGLE
ITS
MAIN
CONTRIBUTION
WAS
THE
DEVELOPMENT
OF
AN
INCEPTION
MODULE
THAT
DRAMATICALLY
REDUCED
THE
NUMBER
OF
PARAMETERS
IN
THE
NETWORK
COMPARED
TO
ALEXNET
WITH
ADDITIONALLY
THIS
PAPER
USES
AVERAGE
POOLING
INSTEAD
OF
FULLY
CONNECTED
LAYERS
AT
THE
TOP
OF
THE
CONVNET
ELIMINATING
A
LARGE
AMOUNT
OF
PARAMETERS
THAT
DO
NOT
SEEM
TO
MATTER
MUCH
THERE
ARE
ALSO
SEVERAL
FOLLOWUP
VERSIONS
TO
THE
GOOGLENET
MOST
RECENTLY
VGGNET
THE
RUNNER
UP
IN
ILSVRC
WAS
THE
NETWORK
FROM
KAREN
SIMONYAN
AND
ANDREW
ZISSERMAN
THAT
BECAME
KNOWN
AS
THE
ITS
MAIN
CONTRIBUTION
WAS
IN
SHOWING
THAT
THE
DEPTH
OF
THE
NETWORK
IS
A
CRITICAL
COMPONENT
FOR
GOOD
PERFORMANCE
THEIR
કNAL
BEST
NETWORK
CONTAINS
CONV
FC
LAYERS
AND
APPEALINGLY
FEATURES
AN
EXTREMELY
HOMOGENEOUS
ARCHITECTURE
THAT
ONLY
PERFORMS
CONVOLUTIONS
AND
POOLING
FROM
THE
BEGINNING
TO
THE
END
THEIR
IS
AVAILABLE
FOR
PLUG
AND
PLAY
USE
IN
CAFFE
A
DOWNSIDE
OF
THE
VGGNET
IS
THAT
IT
IS
MORE
EXPENSIVE
TO
EVALUATE
AND
USES
A
LOT
MORE
MEMORY
AND
PARAMETERS
MOST
OF
THESE
PARAMETERS
ARE
IN
THE
કRST
FULLY
CONNECTED
LAYER
AND
IT
WAS
SINCE
FOUND
THAT
THESE
FC
LAYERS
CAN
BE
REMOVED
WITH
NO
PERFORMANCE
DOWNGRADE
SIGNIકCANTLY
REDUCING
THE
NUMBER
OF
NECESSARY
PARAMETERS
RESNET
DEVELOPED
BY
KAIMING
HE
ET
AL
WAS
THE
WINNER
OF
ILSVRC
IT
FEATURES
SPECIAL
SKIP
CONNECTIONS
AND
A
HEAVY
USE
OF
THE
ARCHITECTURE
IS
ALSO
MISSING
FULLY
CONNECTED
LAYERS
AT
THE
END
OF
THE
NETWORK
THE
READER
IS
ALSO
REFERRED
TO
KAIMING
PRESENTATION
AND
SOME
THAT
REPRODUCE
THESE
NETWORKS
IN
TORCH
RESNETS
ARE
CURRENTLY
BY
FAR
STATE
OF
THE
ART
CONVOLUTIONAL
NEURAL
NETWORK
MODELS
AND
ARE
THE
DEFAULT
CHOICE
FOR
USING
CONVNETS
IN
PRACTICE
AS
OF
MAY
IN
PARTICULAR
ALSO
SEE
MORE
RECENT
DEVELOPMENTS
THAT
TWEAK
VGGNET
IN
DETAIL
LETS
BREAK
DOWN
THE
IN
MORE
DETAIL
AS
A
CASE
STUDY
THE
WHOLE
VGGNET
IS
COMPOSED
OF
CONV
LAYERS
THAT
PERFORM
CONVOLUTIONS
WITH
STRIDE
AND
PAD
AND
OF
POOL
LAYERS
THAT
PERFORM
MAX
POOLING
WITH
STRIDE
AND
NO
PADDING
WE
CAN
WRITE
OUT
THE
SIZE
OF
THE
REPRESENTATION
AT
EACH
STEP
OF
THE
PROCESSING
AND
KEEP
TRACK
OF
BOTH
THE
REPRESENTATION
SIZE
AND
THE
TOTAL
NUMBER
OF
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
MEMORY
WEIGHTS
FC
MEMORY
WEIGHTS
TOTAL
MEMORY
BYTES
IMAGE
ONLY
FORWARD
FOR
BWD
TOTAL
PARAMS
PARAMETERS
AS
IS
COMMON
WITH
CONVOLUTIONAL
NETWORKS
NOTICE
THAT
MOST
OF
THE
MEMORY
AND
ALSO
COMPUTE
TIME
IS
USED
IN
THE
EARLY
CONV
LAYERS
AND
THAT
MOST
OF
THE
PARAMETERS
ARE
IN
THE
LAST
FC
LAYERS
IN
THIS
PARTICULAR
CASE
THE
કRST
FC
LAYER
CONTAINS
WEIGHTS
OUT
OF
A
TOTAL
OF
COMPUTATIONAL
CONSIDERATIONS
THE
LARGEST
BOTTLENECK
TO
BE
AWARE
OF
WHEN
CONSTRUCTING
CONVNET
ARCHITECTURES
IS
THE
MEMORY
BOTTLENECK
MANY
MODERN
GPUS
HAVE
A
LIMIT
OF
MEMORY
WITH
THE
BEST
GPUS
HAVING
ABOUT
OF
MEMORY
THERE
ARE
THREE
MAJOR
SOURCES
OF
MEMORY
TO
KEEP
TRACK
OF
FROM
THE
INTERMEDIATE
VOLUME
SIZES
THESE
ARE
THE
RAW
NUMBER
OF
ACTIVATIONS
AT
EVERY
LAYER
OF
THE
CONVNET
AND
ALSO
THEIR
GRADIENTS
OF
EQUAL
SIZE
USUALLY
MOST
OF
THE
ACTIVATIONS
ARE
ON
THE
EARLIER
LAYERS
OF
A
CONVNET
I
E
કRST
CONV
LAYERS
THESE
ARE
KEPT
AROUND
BECAUSE
THEY
ARE
NEEDED
FOR
BACKPROPAGATION
BUT
A
CLEVER
IMPLEMENTATION
THAT
RUNS
A
CONVNET
ONLY
AT
TEST
TIME
COULD
IN
PRINCIPLE
REDUCE
THIS
BY
A
HUGE
AMOUNT
BY
ONLY
STORING
THE
CURRENT
ACTIVATIONS
AT
ANY
LAYER
AND
DISCARDING
THE
PREVIOUS
ACTIVATIONS
ON
LAYERS
BELOW
FROM
THE
PARAMETER
SIZES
THESE
ARE
THE
NUMBERS
THAT
HOLD
THE
NETWORK
PARAMETERS
THEIR
GRADIENTS
DURING
BACKPROPAGATION
AND
COMMONLY
ALSO
A
STEP
CACHE
IF
THE
OPTIMIZATION
IS
USING
MOMENTUM
ADAGRAD
OR
RMSPROP
THEREFORE
THE
MEMORY
TO
STORE
THE
PARAMETER
VECTOR
ALONE
MUST
USUALLY
BE
MULTIPLIED
BY
A
FACTOR
OF
AT
LEAST
OR
SO
EVERY
CONVNET
IMPLEMENTATION
HAS
TO
MAINTAIN
MISCELLANEOUS
MEMORY
SUCH
AS
THE
IMAGE
DATA
BATCHES
PERHAPS
THEIR
AUGMENTED
VERSIONS
ETC
ONCE
YOU
HAVE
A
ROUGH
ESTIMATE
OF
THE
TOTAL
NUMBER
OF
VALUES
FOR
ACTIVATIONS
GRADIENTS
AND
MISC
THE
NUMBER
SHOULD
BE
CONVERTED
TO
SIZE
IN
GB
TAKE
THE
NUMBER
OF
VALUES
MULTIPLY
BY
TO
GET
THE
RAW
NUMBER
OF
BYTES
SINCE
EVERY
ぱOATING
POINT
IS
BYTES
OR
MAYBE
BY
FOR
DOUBLE
PRECISION
AND
THEN
DIVIDE
BY
MULTIPLE
TIMES
TO
GET
THE
AMOUNT
OF
MEMORY
IN
KB
MB
AND
કNALLY
GB
IF
YOUR
NETWORK
DOESN
T
કT
A
COMMON
HEURISTIC
TO
MAKE
IT
કT
IS
TO
DECREASE
THE
BATCH
SIZE
SINCE
MOST
OF
THE
MEMORY
IS
USUALLY
CONSUMED
BY
THE
ACTIVATIONS
ADDITIONAL
RESOURCES
ADDITIONAL
RESOURCES
RELATED
TO
IMPLEMENTATION
ALLOWS
YOU
TO
PLAY
WITH
CONVNET
ARCHITECTURES
AND
SEE
THE
RESULTS
AND
COMPUTATIONS
IN
REAL
TIME
IN
THE
BROWSER
ONE
OF
THE
POPULAR
CONVNET
LIBRARIES
DISTINCTIVE
IMAGE
FEATURES
FROM
SCALE
INVARIANT
KEYPOINTS
ABSTRACT
THIS
PAPER
PRESENTS
A
METHOD
FOR
EXTRACTING
DISTINCTIVE
INVARIANT
FEATURES
FROM
IMAGES
THAT
CAN
BE
USED
TO
PERFORM
RELIABLE
MATCHING
BETWEEN
DIFFERENT
VIEWS
OF
AN
OBJECT
OR
SCENE
THE
FEATURES
ARE
INVARIANT
TO
IMAGE
SCALE
AND
ROTATION
AND
ARE
SHOWN
TO
PROVIDE
ROBUST
MATCHING
ACROSS
A
A
SUBSTANTIAL
RANGE
OF
AFFINE
DIS
TORTION
CHANGE
IN
VIEWPOINT
ADDITION
OF
NOISE
AND
CHANGE
IN
ILLUMINATION
THE
FEATURES
ARE
HIGHLY
DISTINCTIVE
IN
THE
SENSE
THAT
A
SINGLE
FEATURE
CAN
BE
COR
RECTLY
MATCHED
WITH
HIGH
PROBABILITY
AGAINST
A
LARGE
DATABASE
OF
FEATURES
FROM
MANY
IMAGES
THIS
PAPER
ALSO
DESCRIBES
AN
APPROACH
TO
USING
THESE
FEATURES
FOR
OBJECT
RECOGNITION
THE
RECOGNITION
PROCEEDS
BY
MATCHING
INDIVIDUAL
FEA
TURES
TO
A
DATABASE
OF
FEATURES
FROM
KNOWN
OBJECTS
USING
A
FAST
NEAREST
NEIGHBOR
ALGORITHM
FOLLOWED
BY
A
HOUGH
TRANSFORM
TO
IDENTIFY
CLUSTERS
BELONGING
TO
A
SIN
GLE
OBJECT
AND
FINALLY
PERFORMING
VERIFICATION
THROUGH
LEAST
SQUARES
SOLUTION
FOR
CONSISTENT
POSE
PARAMETERS
THIS
APPROACH
TO
RECOGNITION
CAN
ROBUSTLY
IDENTIFY
OBJECTS
AMONG
CLUTTER
AND
OCCLUSION
WHILE
ACHIEVING
NEAR
REAL
TIME
PERFORMANCE
ACCEPTED
FOR
PUBLICATION
IN
THE
INTERNATIONAL
JOURNAL
OF
COMPUTER
VISION
INTRODUCTION
IMAGE
MATCHING
IS
A
FUNDAMENTAL
ASPECT
OF
MANY
PROBLEMS
IN
COMPUTER
VISION
INCLUDING
OBJECT
OR
SCENE
RECOGNITION
SOLVING
FOR
STRUCTURE
FROM
MULTIPLE
IMAGES
STEREO
CORRESPON
DENCE
AND
MOTION
TRACKING
THIS
PAPER
DESCRIBES
IMAGE
FEATURES
THAT
HAVE
MANY
PROPERTIES
THAT
MAKE
THEM
SUITABLE
FOR
MATCHING
DIFFERING
IMAGES
OF
AN
OBJECT
OR
SCENE
THE
FEATURES
ARE
INVARIANT
TO
IMAGE
SCALING
AND
ROTATION
AND
PARTIALLY
INVARIANT
TO
CHANGE
IN
ILLUMINATION
AND
CAMERA
VIEWPOINT
THEY
ARE
WELL
LOCALIZED
IN
BOTH
THE
SPATIAL
AND
FREQUENCY
DOMAINS
RE
DUCING
THE
PROBABILITY
OF
DISRUPTION
BY
OCCLUSION
CLUTTER
OR
NOISE
LARGE
NUMBERS
OF
FEATURES
CAN
BE
EXTRACTED
FROM
TYPICAL
IMAGES
WITH
EFFICIENT
ALGORITHMS
IN
ADDITION
THE
FEATURES
ARE
HIGHLY
DISTINCTIVE
WHICH
ALLOWS
A
SINGLE
FEATURE
TO
BE
CORRECTLY
MATCHED
WITH
HIGH
PROBABILITY
AGAINST
A
LARGE
DATABASE
OF
FEATURES
PROVIDING
A
BASIS
FOR
OBJECT
AND
SCENE
RECOGNITION
THE
COST
OF
EXTRACTING
THESE
FEATURES
IS
MINIMIZED
BY
TAKING
A
CASCADE
FILTERING
APPROACH
IN
WHICH
THE
MORE
EXPENSIVE
OPERATIONS
ARE
APPLIED
ONLY
AT
LOCATIONS
THAT
PASS
AN
INITIAL
TEST
FOLLOWING
ARE
THE
MAJOR
STAGES
OF
COMPUTATION
USED
TO
GENERATE
THE
SET
OF
IMAGE
FEATURES
SCALE
SPACE
EXTREMA
DETECTION
THE
FIRST
STAGE
OF
COMPUTATION
SEARCHES
OVER
ALL
SCALES
AND
IMAGE
LOCATIONS
IT
IS
IMPLEMENTED
EFFICIENTLY
BY
USING
A
DIFFERENCE
OF
GAUSSIAN
FUNCTION
TO
IDENTIFY
POTENTIAL
INTEREST
POINTS
THAT
ARE
INVARIANT
TO
SCALE
AND
ORIENTATION
KEYPOINT
LOCALIZATION
AT
EACH
CANDIDATE
LOCATION
A
DETAILED
MODEL
IS
FIT
TO
DETERMINE
LOCATION
AND
SCALE
KEYPOINTS
ARE
SELECTED
BASED
ON
MEASURES
OF
THEIR
STABILITY
ORIENTATION
ASSIGNMENT
ONE
OR
MORE
ORIENTATIONS
ARE
ASSIGNED
TO
EACH
KEYPOINT
LO
CATION
BASED
ON
LOCAL
IMAGE
GRADIENT
DIRECTIONS
ALL
FUTURE
OPERATIONS
ARE
PERFORMED
ON
IMAGE
DATA
THAT
HAS
BEEN
TRANSFORMED
RELATIVE
TO
THE
ASSIGNED
ORIENTATION
SCALE
AND
LOCATION
FOR
EACH
FEATURE
THEREBY
PROVIDING
INVARIANCE
TO
THESE
TRANSFORMATIONS
KEYPOINT
DESCRIPTOR
THE
LOCAL
IMAGE
GRADIENTS
ARE
MEASURED
AT
THE
SELECTED
SCALE
IN
THE
REGION
AROUND
EACH
KEYPOINT
THESE
ARE
TRANSFORMED
INTO
A
REPRESENTATION
THAT
ALLOWS
FOR
SIGNIFICANT
LEVELS
OF
LOCAL
SHAPE
DISTORTION
AND
CHANGE
IN
ILLUMINATION
THIS
APPROACH
HAS
BEEN
NAMED
THE
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
AS
IT
TRANSFORMS
IMAGE
DATA
INTO
SCALE
INVARIANT
COORDINATES
RELATIVE
TO
LOCAL
FEATURES
AN
IMPORTANT
ASPECT
OF
THIS
APPROACH
IS
THAT
IT
GENERATES
LARGE
NUMBERS
OF
FEATURES
THAT
DENSELY
COVER
THE
IMAGE
OVER
THE
FULL
RANGE
OF
SCALES
AND
LOCATIONS
A
TYPICAL
IMAGE
OF
SIZE
PIXELS
WILL
GIVE
RISE
TO
ABOUT
STABLE
FEATURES
ALTHOUGH
THIS
NUMBER
DEPENDS
ON
BOTH
IMAGE
CONTENT
AND
CHOICES
FOR
VARIOUS
PARAMETERS
THE
QUANTITY
OF
FEATURES
IS
PARTIC
ULARLY
IMPORTANT
FOR
OBJECT
RECOGNITION
WHERE
THE
ABILITY
TO
DETECT
SMALL
OBJECTS
IN
CLUTTERED
BACKGROUNDS
REQUIRES
THAT
AT
LEAST
FEATURES
BE
CORRECTLY
MATCHED
FROM
EACH
OBJECT
FOR
RELI
ABLE
IDENTIFICATION
FOR
IMAGE
MATCHING
AND
RECOGNITION
SIFT
FEATURES
ARE
FIRST
EXTRACTED
FROM
A
SET
OF
REF
ERENCE
IMAGES
AND
STORED
IN
A
DATABASE
A
NEW
IMAGE
IS
MATCHED
BY
INDIVIDUALLY
COMPARING
EACH
FEATURE
FROM
THE
NEW
IMAGE
TO
THIS
PREVIOUS
DATABASE
AND
FINDING
CANDIDATE
MATCH
ING
FEATURES
BASED
ON
EUCLIDEAN
DISTANCE
OF
THEIR
FEATURE
VECTORS
THIS
PAPER
WILL
DISCUSS
FAST
NEAREST
NEIGHBOR
ALGORITHMS
THAT
CAN
PERFORM
THIS
COMPUTATION
RAPIDLY
AGAINST
LARGE
DATABASES
THE
KEYPOINT
DESCRIPTORS
ARE
HIGHLY
DISTINCTIVE
WHICH
ALLOWS
A
SINGLE
FEATURE
TO
FIND
ITS
CORRECT
MATCH
WITH
GOOD
PROBABILITY
IN
A
LARGE
DATABASE
OF
FEATURES
HOWEVER
IN
A
CLUTTERED
IMAGE
MANY
FEATURES
FROM
THE
BACKGROUND
WILL
NOT
HAVE
ANY
CORRECT
MATCH
IN
THE
DATABASE
GIVING
RISE
TO
MANY
FALSE
MATCHES
IN
ADDITION
TO
THE
CORRECT
ONES
THE
CORRECT
MATCHES
CAN
BE
FILTERED
FROM
THE
FULL
SET
OF
MATCHES
BY
IDENTIFYING
SUBSETS
OF
KEYPOINTS
THAT
AGREE
ON
THE
OBJECT
AND
ITS
LOCATION
SCALE
AND
ORIENTATION
IN
THE
NEW
IMAGE
THE
PROBABILITY
THAT
SEVERAL
FEATURES
WILL
AGREE
ON
THESE
PARAMETERS
BY
CHANCE
IS
MUCH
LOWER
THAN
THE
PROBABILITY
THAT
ANY
INDIVIDUAL
FEATURE
MATCH
WILL
BE
IN
ERROR
THE
DETERMINATION
OF
THESE
CONSISTENT
CLUSTERS
CAN
BE
PERFORMED
RAPIDLY
BY
USING
AN
EFFICIENT
HASH
TABLE
IMPLEMENTATION
OF
THE
GENERALIZED
HOUGH
TRANSFORM
EACH
CLUSTER
OF
OR
MORE
FEATURES
THAT
AGREE
ON
AN
OBJECT
AND
ITS
POSE
IS
THEN
SUBJECT
TO
FURTHER
DETAILED
VERIFICATION
FIRST
A
LEAST
SQUARED
ESTIMATE
IS
MADE
FOR
AN
AFFINE
APPROXI
MATION
TO
THE
OBJECT
POSE
ANY
OTHER
IMAGE
FEATURES
CONSISTENT
WITH
THIS
POSE
ARE
IDENTIFIED
AND
OUTLIERS
ARE
DISCARDED
FINALLY
A
DETAILED
COMPUTATION
IS
MADE
OF
THE
PROBABILITY
THAT
A
PARTICULAR
SET
OF
FEATURES
INDICATES
THE
PRESENCE
OF
AN
OBJECT
GIVEN
THE
ACCURACY
OF
FIT
AND
NUMBER
OF
PROBABLE
FALSE
MATCHES
OBJECT
MATCHES
THAT
PASS
ALL
THESE
TESTS
CAN
BE
IDENTIFIED
AS
CORRECT
WITH
HIGH
CONFIDENCE
RELATED
RESEARCH
THE
DEVELOPMENT
OF
IMAGE
MATCHING
BY
USING
A
SET
OF
LOCAL
INTEREST
POINTS
CAN
BE
TRACED
BACK
TO
THE
WORK
OF
MORAVEC
ON
STEREO
MATCHING
USING
A
CORNER
DETECTOR
THE
MORAVEC
DETECTOR
WAS
IMPROVED
BY
HARRIS
AND
STEPHENS
TO
MAKE
IT
MORE
REPEATABLE
UNDER
SMALL
IMAGE
VARIATIONS
AND
NEAR
EDGES
HARRIS
ALSO
SHOWED
ITS
VALUE
FOR
EFFICIENT
MOTION
TRACKING
AND
STRUCTURE
FROM
MOTION
RECOVERY
HARRIS
AND
THE
HARRIS
CORNER
DETECTOR
HAS
SINCE
BEEN
WIDELY
USED
FOR
MANY
OTHER
IMAGE
MATCHING
TASKS
WHILE
THESE
FEATURE
DETECTORS
ARE
USUALLY
CALLED
CORNER
DETECTORS
THEY
ARE
NOT
SELECTING
JUST
CORNERS
BUT
RATHER
ANY
IMAGE
LOCATION
THAT
HAS
LARGE
GRADIENTS
IN
ALL
DIRECTIONS
AT
A
PREDETERMINED
SCALE
THE
INITIAL
APPLICATIONS
WERE
TO
STEREO
AND
SHORT
RANGE
MOTION
TRACKING
BUT
THE
APPROACH
WAS
LATER
EXTENDED
TO
MORE
DIFFICULT
PROBLEMS
ZHANG
ET
AL
SHOWED
THAT
IT
WAS
POSSI
BLE
TO
MATCH
HARRIS
CORNERS
OVER
A
LARGE
IMAGE
RANGE
BY
USING
A
CORRELATION
WINDOW
AROUND
EACH
CORNER
TO
SELECT
LIKELY
MATCHES
OUTLIERS
WERE
THEN
REMOVED
BY
SOLVING
FOR
A
FUNDA
MENTAL
MATRIX
DESCRIBING
THE
GEOMETRIC
CONSTRAINTS
BETWEEN
THE
TWO
VIEWS
OF
RIGID
SCENE
AND
REMOVING
MATCHES
THAT
DID
NOT
AGREE
WITH
THE
MAJORITY
SOLUTION
AT
THE
SAME
TIME
A
SIMILAR
APPROACH
WAS
DEVELOPED
BY
TORR
FOR
LONG
RANGE
MOTION
MATCHING
IN
WHICH
GEOMETRIC
CONSTRAINTS
WERE
USED
TO
REMOVE
OUTLIERS
FOR
RIGID
OBJECTS
MOVING
WITHIN
AN
IMAGE
THE
GROUND
BREAKING
WORK
OF
SCHMID
AND
MOHR
SHOWED
THAT
INVARIANT
LOCAL
FEA
TURE
MATCHING
COULD
BE
EXTENDED
TO
GENERAL
IMAGE
RECOGNITION
PROBLEMS
IN
WHICH
A
FEATURE
WAS
MATCHED
AGAINST
A
LARGE
DATABASE
OF
IMAGES
THEY
ALSO
USED
HARRIS
CORNERS
TO
SELECT
INTEREST
POINTS
BUT
RATHER
THAN
MATCHING
WITH
A
CORRELATION
WINDOW
THEY
USED
A
ROTATIONALLY
INVARIANT
DESCRIPTOR
OF
THE
LOCAL
IMAGE
REGION
THIS
ALLOWED
FEATURES
TO
BE
MATCHED
UNDER
ARBITRARY
ORIENTATION
CHANGE
BETWEEN
THE
TWO
IMAGES
FURTHERMORE
THEY
DEMONSTRATED
THAT
MULTIPLE
FEATURE
MATCHES
COULD
ACCOMPLISH
GENERAL
RECOGNITION
UNDER
OCCLUSION
AND
CLUTTER
BY
IDENTIFYING
CONSISTENT
CLUSTERS
OF
MATCHED
FEATURES
THE
HARRIS
CORNER
DETECTOR
IS
VERY
SENSITIVE
TO
CHANGES
IN
IMAGE
SCALE
SO
IT
DOES
NOT
PROVIDE
A
GOOD
BASIS
FOR
MATCHING
IMAGES
OF
DIFFERENT
SIZES
EARLIER
WORK
BY
THE
AUTHOR
LOWE
EXTENDED
THE
LOCAL
FEATURE
APPROACH
TO
ACHIEVE
SCALE
INVARIANCE
THIS
WORK
ALSO
DESCRIBED
A
NEW
LOCAL
DESCRIPTOR
THAT
PROVIDED
MORE
DISTINCTIVE
FEATURES
WHILE
BEING
LESS
SENSITIVE
TO
LOCAL
IMAGE
DISTORTIONS
SUCH
AS
VIEWPOINT
CHANGE
THIS
CURRENT
PAPER
PROVIDES
A
MORE
IN
DEPTH
DEVELOPMENT
AND
ANALYSIS
OF
THIS
EARLIER
WORK
WHILE
ALSO
PRESENTING
A
NUMBER
OF
IMPROVEMENTS
IN
STABILITY
AND
FEATURE
INVARIANCE
THERE
IS
A
CONSIDERABLE
BODY
OF
PREVIOUS
RESEARCH
ON
IDENTIFYING
REPRESENTATIONS
THAT
ARE
STABLE
UNDER
SCALE
CHANGE
SOME
OF
THE
FIRST
WORK
IN
THIS
AREA
WAS
BY
CROWLEY
AND
PARKER
WHO
DEVELOPED
A
REPRESENTATION
THAT
IDENTIFIED
PEAKS
AND
RIDGES
IN
SCALE
SPACE
AND
LINKED
THESE
INTO
A
TREE
STRUCTURE
THE
TREE
STRUCTURE
COULD
THEN
BE
MATCHED
BETWEEN
IMAGES
WITH
ARBITRARY
SCALE
CHANGE
MORE
RECENT
WORK
ON
GRAPH
BASED
MATCHING
BY
SHOKOUFANDEH
MARSIC
AND
DICKINSON
PROVIDES
MORE
DISTINCTIVE
FEATURE
DESCRIPTORS
USING
WAVELET
CO
EFFICIENTS
THE
PROBLEM
OF
IDENTIFYING
AN
APPROPRIATE
AND
CONSISTENT
SCALE
FOR
FEATURE
DETECTION
HAS
BEEN
STUDIED
IN
DEPTH
BY
LINDEBERG
HE
DESCRIBES
THIS
AS
A
PROBLEM
OF
SCALE
SELECTION
AND
WE
MAKE
USE
OF
HIS
RESULTS
BELOW
RECENTLY
THERE
HAS
BEEN
AN
IMPRESSIVE
BODY
OF
WORK
ON
EXTENDING
LOCAL
FEATURES
TO
BE
INVARIANT
TO
FULL
AFFINE
TRANSFORMATIONS
BAUMBERG
TUYTELAARS
AND
VAN
GOOL
MIKOLAJCZYK
AND
SCHMID
SCHAFFALITZKY
AND
ZISSERMAN
BROWN
AND
LOWE
THIS
ALLOWS
FOR
INVARIANT
MATCHING
TO
FEATURES
ON
A
PLANAR
SURFACE
UNDER
CHANGES
IN
ORTHO
GRAPHIC
PROJECTION
IN
MOST
CASES
BY
RESAMPLING
THE
IMAGE
IN
A
LOCAL
AFFINE
FRAME
HOW
EVER
NONE
OF
THESE
APPROACHES
ARE
YET
FULLY
AFFINE
INVARIANT
AS
THEY
START
WITH
INITIAL
FEATURE
SCALES
AND
LOCATIONS
SELECTED
IN
A
NON
AFFINE
INVARIANT
MANNER
DUE
TO
THE
PROHIBITIVE
COST
OF
EXPLORING
THE
FULL
AFFINE
SPACE
THE
AFFINE
FRAMES
ARE
ARE
ALSO
MORE
SENSITIVE
TO
NOISE
THAN
THOSE
OF
THE
SCALE
INVARIANT
FEATURES
SO
IN
PRACTICE
THE
AFFINE
FEATURES
HAVE
LOWER
REPEATABILITY
THAN
THE
SCALE
INVARIANT
FEATURES
UNLESS
THE
AFFINE
DISTORTION
IS
GREATER
THAN
ABOUT
A
DEGREE
TILT
OF
A
PLANAR
SURFACE
MIKOLAJCZYK
WIDER
AFFINE
INVARIANCE
MAY
NOT
BE
IMPORTANT
FOR
MANY
APPLICATIONS
AS
TRAINING
VIEWS
ARE
BEST
TAKEN
AT
LEAST
EVERY
DEGREES
ROTATION
IN
VIEW
POINT
MEANING
THAT
RECOGNITION
IS
WITHIN
DEGREES
OF
THE
CLOSEST
TRAINING
VIEW
IN
ORDER
TO
CAPTURE
NON
PLANAR
CHANGES
AND
OCCLUSION
EFFECTS
FOR
OBJECTS
WHILE
THE
METHOD
TO
BE
PRESENTED
IN
THIS
PAPER
IS
NOT
FULLY
AFFINE
INVARIANT
A
DIFFERENT
APPROACH
IS
USED
IN
WHICH
THE
LOCAL
DESCRIPTOR
ALLOWS
RELATIVE
FEATURE
POSITIONS
TO
SHIFT
SIGNIF
ICANTLY
WITH
ONLY
SMALL
CHANGES
IN
THE
DESCRIPTOR
THIS
APPROACH
NOT
ONLY
ALLOWS
THE
DESCRIP
TORS
TO
BE
RELIABLY
MATCHED
ACROSS
A
CONSIDERABLE
RANGE
OF
AFFINE
DISTORTION
BUT
IT
ALSO
MAKES
THE
FEATURES
MORE
ROBUST
AGAINST
CHANGES
IN
VIEWPOINT
FOR
NON
PLANAR
SURFACES
OTHER
ADVANTAGES
INCLUDE
MUCH
MORE
EFFICIENT
FEATURE
EXTRACTION
AND
THE
ABILITY
TO
IDENTIFY
LARGER
NUMBERS
OF
FEATURES
ON
THE
OTHER
HAND
AFFINE
INVARIANCE
IS
A
VALUABLE
PROPERTY
FOR
MATCHING
PLANAR
SURFACES
UNDER
VERY
LARGE
VIEW
CHANGES
AND
FURTHER
RESEARCH
SHOULD
BE
PERFORMED
ON
THE
BEST
WAYS
TO
COMBINE
THIS
WITH
NON
PLANAR
VIEWPOINT
INVARIANCE
IN
AN
EFFICIENT
AND
STABLE
MANNER
MANY
OTHER
FEATURE
TYPES
HAVE
BEEN
PROPOSED
FOR
USE
IN
RECOGNITION
SOME
OF
WHICH
COULD
BE
USED
IN
ADDITION
TO
THE
FEATURES
DESCRIBED
IN
THIS
PAPER
TO
PROVIDE
FURTHER
MATCHES
UNDER
DIFFERING
CIRCUMSTANCES
ONE
CLASS
OF
FEATURES
ARE
THOSE
THAT
MAKE
USE
OF
IMAGE
CONTOURS
OR
REGION
BOUNDARIES
WHICH
SHOULD
MAKE
THEM
LESS
LIKELY
TO
BE
DISRUPTED
BY
CLUTTERED
BACK
GROUNDS
NEAR
OBJECT
BOUNDARIES
MATAS
ET
AL
HAVE
SHOWN
THAT
THEIR
MAXIMALLY
STABLE
EXTREMAL
REGIONS
CAN
PRODUCE
LARGE
NUMBERS
OF
MATCHING
FEATURES
WITH
GOOD
STABILITY
MIKO
LAJCZYK
ET
AL
HAVE
DEVELOPED
A
NEW
DESCRIPTOR
THAT
USES
LOCAL
EDGES
WHILE
IGNORING
UNRELATED
NEARBY
EDGES
PROVIDING
THE
ABILITY
TO
FIND
STABLE
FEATURES
EVEN
NEAR
THE
BOUNDARIES
OF
NARROW
SHAPES
SUPERIMPOSED
ON
BACKGROUND
CLUTTER
NELSON
AND
SELINGER
HAVE
SHOWN
GOOD
RESULTS
WITH
LOCAL
FEATURES
BASED
ON
GROUPINGS
OF
IMAGE
CONTOURS
SIMILARLY
POPE
AND
LOWE
USED
FEATURES
BASED
ON
THE
HIERARCHICAL
GROUPING
OF
IMAGE
CONTOURS
WHICH
ARE
PARTICULARLY
USEFUL
FOR
OBJECTS
LACKING
DETAILED
TEXTURE
THE
HISTORY
OF
RESEARCH
ON
VISUAL
RECOGNITION
CONTAINS
WORK
ON
A
DIVERSE
SET
OF
OTHER
IMAGE
PROPERTIES
THAT
CAN
BE
USED
AS
FEATURE
MEASUREMENTS
CARNEIRO
AND
JEPSON
DESCRIBE
PHASE
BASED
LOCAL
FEATURES
THAT
REPRESENT
THE
PHASE
RATHER
THAN
THE
MAGNITUDE
OF
LOCAL
SPATIAL
FREQUENCIES
WHICH
IS
LIKELY
TO
PROVIDE
IMPROVED
INVARIANCE
TO
ILLUMINATION
SCHIELE
AND
CROWLEY
HAVE
PROPOSED
THE
USE
OF
MULTIDIMENSIONAL
HISTOGRAMS
SUMMARIZING
THE
DISTRIBUTION
OF
MEASUREMENTS
WITHIN
IMAGE
REGIONS
THIS
TYPE
OF
FEATURE
MAY
BE
PARTICULARLY
USEFUL
FOR
RECOGNITION
OF
TEXTURED
OBJECTS
WITH
DEFORMABLE
SHAPES
BASRI
AND
JACOBS
HAVE
DEMONSTRATED
THE
VALUE
OF
EXTRACTING
LOCAL
REGION
BOUNDARIES
FOR
RECOGNITION
OTHER
USEFUL
PROPERTIES
TO
INCORPORATE
INCLUDE
COLOR
MOTION
FIGURE
GROUND
DISCRIMINATION
REGION
SHAPE
DESCRIPTORS
AND
STEREO
DEPTH
CUES
THE
LOCAL
FEATURE
APPROACH
CAN
EASILY
INCORPORATE
NOVEL
FEATURE
TYPES
BECAUSE
EXTRA
FEATURES
CONTRIBUTE
TO
ROBUSTNESS
WHEN
THEY
PROVIDE
CORRECT
MATCHES
BUT
OTHERWISE
DO
LITTLE
HARM
OTHER
THAN
THEIR
COST
OF
COMPUTATION
THEREFORE
FUTURE
SYSTEMS
ARE
LIKELY
TO
COMBINE
MANY
FEATURE
TYPES
DETECTION
OF
SCALE
SPACE
EXTREMA
AS
DESCRIBED
IN
THE
INTRODUCTION
WE
WILL
DETECT
KEYPOINTS
USING
A
CASCADE
FILTERING
APPROACH
THAT
USES
EFFICIENT
ALGORITHMS
TO
IDENTIFY
CANDIDATE
LOCATIONS
THAT
ARE
THEN
EXAMINED
IN
FURTHER
DETAIL
THE
FIRST
STAGE
OF
KEYPOINT
DETECTION
IS
TO
IDENTIFY
LOCATIONS
AND
SCALES
THAT
CAN
BE
REPEATABLY
ASSIGNED
UNDER
DIFFERING
VIEWS
OF
THE
SAME
OBJECT
DETECTING
LOCATIONS
THAT
ARE
INVARIANT
TO
SCALE
CHANGE
OF
THE
IMAGE
CAN
BE
ACCOMPLISHED
BY
SEARCHING
FOR
STABLE
FEATURES
ACROSS
ALL
POSSIBLE
SCALES
USING
A
CONTINUOUS
FUNCTION
OF
SCALE
KNOWN
AS
SCALE
SPACE
WITKIN
IT
HAS
BEEN
SHOWN
BY
KOENDERINK
AND
LINDEBERG
THAT
UNDER
A
VARIETY
OF
REASONABLE
ASSUMPTIONS
THE
ONLY
POSSIBLE
SCALE
SPACE
KERNEL
IS
THE
GAUSSIAN
FUNCTION
THERE
FORE
THE
SCALE
SPACE
OF
AN
IMAGE
IS
DEFINED
AS
A
FUNCTION
L
X
Y
Σ
THAT
IS
PRODUCED
FROM
THE
CONVOLUTION
OF
A
VARIABLE
SCALE
GAUSSIAN
G
X
Y
Σ
WITH
AN
INPUT
IMAGE
I
X
Y
L
X
Y
Σ
G
X
Y
Σ
I
X
Y
WHERE
IS
THE
CONVOLUTION
OPERATION
IN
X
AND
Y
AND
G
X
Y
Σ
E
TO
EFFICIENTLY
DETECT
STABLE
KEYPOINT
LOCATIONS
IN
SCALE
SPACE
WE
HAVE
PROPOSED
LOWE
USING
SCALE
SPACE
EXTREMA
IN
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
CONVOLVED
WITH
THE
IMAGE
D
X
Y
Σ
WHICH
CAN
BE
COMPUTED
FROM
THE
DIFFERENCE
OF
TWO
NEARBY
SCALES
SEPARATED
BY
A
CONSTANT
MULTIPLICATIVE
FACTOR
K
D
X
Y
Σ
G
X
Y
KΣ
G
X
Y
Σ
I
X
Y
L
X
Y
KΣ
L
X
Y
Σ
THERE
ARE
A
NUMBER
OF
REASONS
FOR
CHOOSING
THIS
FUNCTION
FIRST
IT
IS
A
PARTICULARLY
EFFICIENT
FUNCTION
TO
COMPUTE
AS
THE
SMOOTHED
IMAGES
L
NEED
TO
BE
COMPUTED
IN
ANY
CASE
FOR
SCALE
SPACE
FEATURE
DESCRIPTION
AND
D
CAN
THEREFORE
BE
COMPUTED
BY
SIMPLE
IMAGE
SUBTRACTION
SCALE
NEXT
OCTAVE
SCALE
FIRST
OCTAVE
GAUSSIAN
DIFFERENCE
OF
GAUSSIAN
DOG
FIGURE
FOR
EACH
OCTAVE
OF
SCALE
SPACE
THE
INITIAL
IMAGE
IS
REPEATEDLY
CONVOLVED
WITH
GAUSSIANS
TO
PRODUCE
THE
SET
OF
SCALE
SPACE
IMAGES
SHOWN
ON
THE
LEFT
ADJACENT
GAUSSIAN
IMAGES
ARE
SUBTRACTED
TO
PRODUCE
THE
DIFFERENCE
OF
GAUSSIAN
IMAGES
ON
THE
RIGHT
AFTER
EACH
OCTAVE
THE
GAUSSIAN
IMAGE
IS
DOWN
SAMPLED
BY
A
FACTOR
OF
AND
THE
PROCESS
REPEATED
IN
ADDITION
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
PROVIDES
A
CLOSE
APPROXIMATION
TO
THE
SCALE
NORMALIZED
LAPLACIAN
OF
GAUSSIAN
AS
STUDIED
BY
LINDEBERG
LINDEBERG
SHOWED
THAT
THE
NORMALIZATION
OF
THE
LAPLACIAN
WITH
THE
FACTOR
IS
REQUIRED
FOR
TRUE
SCALE
INVARIANCE
IN
DETAILED
EXPERIMENTAL
COMPARISONS
MIKOLAJCZYK
FOUND
THAT
THE
MAXIMA
AND
MINIMA
OF
PRODUCE
THE
MOST
STABLE
IMAGE
FEATURES
COMPARED
TO
A
RANGE
OF
OTHER
POSSIBLE
IMAGE
FUNCTIONS
SUCH
AS
THE
GRADIENT
HESSIAN
OR
HARRIS
CORNER
FUNCTION
THE
RELATIONSHIP
BETWEEN
D
AND
CAN
BE
UNDERSTOOD
FROM
THE
HEAT
DIFFUSION
EQUA
TION
PARAMETERIZED
IN
TERMS
OF
Σ
RATHER
THAN
THE
MORE
USUAL
T
G
Σ
Σ
FROM
THIS
WE
SEE
THAT
CAN
BE
COMPUTED
FROM
THE
FINITE
DIFFERENCE
APPROXIMATION
TO
G
Σ
USING
THE
DIFFERENCE
OF
NEARBY
SCALES
AT
KΣ
AND
Σ
Σ
G
G
X
Y
KΣ
G
X
Y
Σ
AND
THEREFORE
Σ
KΣ
Σ
G
X
Y
KΣ
G
X
Y
Σ
K
THIS
SHOWS
THAT
WHEN
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
HAS
SCALES
DIFFERING
BY
A
CON
STANT
FACTOR
IT
ALREADY
INCORPORATES
THE
SCALE
NORMALIZATION
REQUIRED
FOR
THE
SCALE
INVARIANT
FIGURE
MAXIMA
AND
MINIMA
OF
THE
DIFFERENCE
OF
GAUSSIAN
IMAGES
ARE
DETECTED
BY
COMPARING
A
PIXEL
MARKED
WITH
X
TO
ITS
NEIGHBORS
IN
REGIONS
AT
THE
CURRENT
AND
ADJACENT
SCALES
MARKED
WITH
CIRCLES
LAPLACIAN
THE
FACTOR
K
IN
THE
EQUATION
IS
A
CONSTANT
OVER
ALL
SCALES
AND
THEREFORE
DOES
NOT
INFLUENCE
EXTREMA
LOCATION
THE
APPROXIMATION
ERROR
WILL
GO
TO
ZERO
AS
K
GOES
TO
BUT
IN
PRACTICE
WE
HAVE
FOUND
THAT
THE
APPROXIMATION
HAS
ALMOST
NO
IMPACT
ON
THE
STABILITY
OF
EXTREMA
DETECTION
OR
LOCALIZATION
FOR
EVEN
SIGNIFICANT
DIFFERENCES
IN
SCALE
SUCH
AS
K
AN
EFFICIENT
APPROACH
TO
CONSTRUCTION
OF
D
X
Y
Σ
IS
SHOWN
IN
FIGURE
THE
INITIAL
IMAGE
IS
INCREMENTALLY
CONVOLVED
WITH
GAUSSIANS
TO
PRODUCE
IMAGES
SEPARATED
BY
A
CONSTANT
FACTOR
K
IN
SCALE
SPACE
SHOWN
STACKED
IN
THE
LEFT
COLUMN
WE
CHOOSE
TO
DIVIDE
EACH
OCTAVE
OF
SCALE
SPACE
I
E
DOUBLING
OF
Σ
INTO
AN
INTEGER
NUMBER
OF
INTERVALS
SO
K
WE
MUST
PRODUCE
IMAGES
IN
THE
STACK
OF
BLURRED
IMAGES
FOR
EACH
OCTAVE
SO
THAT
FINAL
EXTREMA
DETECTION
COVERS
A
COMPLETE
OCTAVE
ADJACENT
IMAGE
SCALES
ARE
SUBTRACTED
TO
PRODUCE
THE
DIFFERENCE
OF
GAUSSIAN
IMAGES
SHOWN
ON
THE
RIGHT
ONCE
A
COMPLETE
OCTAVE
HAS
BEEN
PROCESSED
WE
RESAMPLE
THE
GAUSSIAN
IMAGE
THAT
HAS
TWICE
THE
INITIAL
VALUE
OF
Σ
IT
WILL
BE
IMAGES
FROM
THE
TOP
OF
THE
STACK
BY
TAKING
EVERY
SECOND
PIXEL
IN
EACH
ROW
AND
COLUMN
THE
ACCURACY
OF
SAMPLING
RELATIVE
TO
Σ
IS
NO
DIFFERENT
THAN
FOR
THE
START
OF
THE
PREVIOUS
OCTAVE
WHILE
COMPUTATION
IS
GREATLY
REDUCED
LOCAL
EXTREMA
DETECTION
IN
ORDER
TO
DETECT
THE
LOCAL
MAXIMA
AND
MINIMA
OF
D
X
Y
Σ
EACH
SAMPLE
POINT
IS
COMPARED
TO
ITS
EIGHT
NEIGHBORS
IN
THE
CURRENT
IMAGE
AND
NINE
NEIGHBORS
IN
THE
SCALE
ABOVE
AND
BELOW
SEE
FIGURE
IT
IS
SELECTED
ONLY
IF
IT
IS
LARGER
THAN
ALL
OF
THESE
NEIGHBORS
OR
SMALLER
THAN
ALL
OF
THEM
THE
COST
OF
THIS
CHECK
IS
REASONABLY
LOW
DUE
TO
THE
FACT
THAT
MOST
SAMPLE
POINTS
WILL
BE
ELIMINATED
FOLLOWING
THE
FIRST
FEW
CHECKS
AN
IMPORTANT
ISSUE
IS
TO
DETERMINE
THE
FREQUENCY
OF
SAMPLING
IN
THE
IMAGE
AND
SCALE
DO
MAINS
THAT
IS
NEEDED
TO
RELIABLY
DETECT
THE
EXTREMA
UNFORTUNATELY
IT
TURNS
OUT
THAT
THERE
IS
NO
MINIMUM
SPACING
OF
SAMPLES
THAT
WILL
DETECT
ALL
EXTREMA
AS
THE
EXTREMA
CAN
BE
ARBITRAR
ILY
CLOSE
TOGETHER
THIS
CAN
BE
SEEN
BY
CONSIDERING
A
WHITE
CIRCLE
ON
A
BLACK
BACKGROUND
WHICH
WILL
HAVE
A
SINGLE
SCALE
SPACE
MAXIMUM
WHERE
THE
CIRCULAR
POSITIVE
CENTRAL
REGION
OF
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
MATCHES
THE
SIZE
AND
LOCATION
OF
THE
CIRCLE
FOR
A
VERY
ELONGATED
ELLIPSE
THERE
WILL
BE
TWO
MAXIMA
NEAR
EACH
END
OF
THE
ELLIPSE
AS
THE
LOCATIONS
OF
MAXIMA
ARE
A
CONTINUOUS
FUNCTION
OF
THE
IMAGE
FOR
SOME
ELLIPSE
WITH
INTERMEDIATE
ELONGATION
THERE
WILL
BE
A
TRANSITION
FROM
A
SINGLE
MAXIMUM
TO
TWO
WITH
THE
MAXIMA
ARBITRARILY
CLOSE
TO
NUMBER
OF
SCALES
SAMPLED
PER
OCTAVE
NUMBER
OF
SCALES
SAMPLED
PER
OCTAVE
FIGURE
THE
TOP
LINE
OF
THE
FIRST
GRAPH
SHOWS
THE
PERCENT
OF
KEYPOINTS
THAT
ARE
REPEATABLY
DETECTED
AT
THE
SAME
LOCATION
AND
SCALE
IN
A
TRANSFORMED
IMAGE
AS
A
FUNCTION
OF
THE
NUMBER
OF
SCALES
SAMPLED
PER
OCTAVE
THE
LOWER
LINE
SHOWS
THE
PERCENT
OF
KEYPOINTS
THAT
HAVE
THEIR
DESCRIPTORS
CORRECTLY
MATCHED
TO
A
LARGE
DATABASE
THE
SECOND
GRAPH
SHOWS
THE
TOTAL
NUMBER
OF
KEYPOINTS
DETECTED
IN
A
TYPICAL
IMAGE
AS
A
FUNCTION
OF
THE
NUMBER
OF
SCALE
SAMPLES
EACH
OTHER
NEAR
THE
TRANSITION
THEREFORE
WE
MUST
SETTLE
FOR
A
SOLUTION
THAT
TRADES
OFF
EFFICIENCY
WITH
COMPLETENESS
IN
FACT
AS
MIGHT
BE
EXPECTED
AND
IS
CONFIRMED
BY
OUR
EXPERIMENTS
EXTREMA
THAT
ARE
CLOSE
TOGETHER
ARE
QUITE
UNSTABLE
TO
SMALL
PERTURBATIONS
OF
THE
IMAGE
WE
CAN
DETERMINE
THE
BEST
CHOICES
EXPERIMENTALLY
BY
STUDYING
A
RANGE
OF
SAMPLING
FREQUENCIES
AND
USING
THOSE
THAT
PROVIDE
THE
MOST
RELIABLE
RESULTS
UNDER
A
REALISTIC
SIMULATION
OF
THE
MATCHING
TASK
FREQUENCY
OF
SAMPLING
IN
SCALE
THE
EXPERIMENTAL
DETERMINATION
OF
SAMPLING
FREQUENCY
THAT
MAXIMIZES
EXTREMA
STABILITY
IS
SHOWN
IN
FIGURES
AND
THESE
FIGURES
AND
MOST
OTHER
SIMULATIONS
IN
THIS
PAPER
ARE
BASED
ON
A
MATCHING
TASK
USING
A
COLLECTION
OF
REAL
IMAGES
DRAWN
FROM
A
DIVERSE
RANGE
INCLUDING
OUTDOOR
SCENES
HUMAN
FACES
AERIAL
PHOTOGRAPHS
AND
INDUSTRIAL
IMAGES
THE
IMAGE
DOMAIN
WAS
FOUND
TO
HAVE
ALMOST
NO
INFLUENCE
ON
ANY
OF
THE
RESULTS
EACH
IMAGE
WAS
THEN
SUBJECT
TO
A
RANGE
OF
TRANSFORMATIONS
INCLUDING
ROTATION
SCALING
AFFINE
STRETCH
CHANGE
IN
BRIGHTNESS
AND
CONTRAST
AND
ADDITION
OF
IMAGE
NOISE
BECAUSE
THE
CHANGES
WERE
SYNTHETIC
IT
WAS
POSSIBLE
TO
PRECISELY
PREDICT
WHERE
EACH
FEATURE
IN
AN
ORIGINAL
IMAGE
SHOULD
APPEAR
IN
THE
TRANSFORMED
IMAGE
ALLOWING
FOR
MEASUREMENT
OF
CORRECT
REPEATABILITY
AND
POSITIONAL
ACCURACY
FOR
EACH
FEATURE
FIGURE
SHOWS
THESE
SIMULATION
RESULTS
USED
TO
EXAMINE
THE
EFFECT
OF
VARYING
THE
NUMBER
OF
SCALES
PER
OCTAVE
AT
WHICH
THE
IMAGE
FUNCTION
IS
SAMPLED
PRIOR
TO
EXTREMA
DETECTION
IN
THIS
CASE
EACH
IMAGE
WAS
RESAMPLED
FOLLOWING
ROTATION
BY
A
RANDOM
ANGLE
AND
SCALING
BY
A
RANDOM
AMOUNT
BETWEEN
OF
TIMES
THE
ORIGINAL
SIZE
KEYPOINTS
FROM
THE
REDUCED
RESOLUTION
IMAGE
WERE
MATCHED
AGAINST
THOSE
FROM
THE
ORIGINAL
IMAGE
SO
THAT
THE
SCALES
FOR
ALL
KEYPOINTS
WOULD
BE
BE
PRESENT
IN
THE
MATCHED
IMAGE
IN
ADDITION
IMAGE
NOISE
WAS
ADDED
MEANING
THAT
EACH
PIXEL
HAD
A
RANDOM
NUMBER
ADDED
FROM
THE
UNIFORM
INTERVAL
WHERE
PIXEL
VALUES
ARE
IN
THE
RANGE
EQUIVALENT
TO
PROVIDING
SLIGHTLY
LESS
THAN
BITS
OF
ACCURACY
FOR
IMAGE
PIXELS
PRIOR
SMOOTHING
FOR
EACH
OCTAVE
SIGMA
FIGURE
THE
TOP
LINE
IN
THE
GRAPH
SHOWS
THE
PERCENT
OF
KEYPOINT
LOCATIONS
THAT
ARE
REPEATABLY
DETECTED
IN
A
TRANSFORMED
IMAGE
AS
A
FUNCTION
OF
THE
PRIOR
IMAGE
SMOOTHING
FOR
THE
FIRST
LEVEL
OF
EACH
OCTAVE
THE
LOWER
LINE
SHOWS
THE
PERCENT
OF
DESCRIPTORS
CORRECTLY
MATCHED
AGAINST
A
LARGE
DATABASE
THE
TOP
LINE
IN
THE
FIRST
GRAPH
OF
FIGURE
SHOWS
THE
PERCENT
OF
KEYPOINTS
THAT
ARE
DETECTED
AT
A
MATCHING
LOCATION
AND
SCALE
IN
THE
TRANSFORMED
IMAGE
FOR
ALL
EXAMPLES
IN
THIS
PAPER
WE
DEFINE
A
MATCHING
SCALE
AS
BEING
WITHIN
A
FACTOR
OF
OF
THE
CORRECT
SCALE
AND
A
MATCHING
LOCATION
AS
BEING
WITHIN
Σ
PIXELS
WHERE
Σ
IS
THE
SCALE
OF
THE
KEYPOINT
DEFINED
FROM
EQUATION
AS
THE
STANDARD
DEVIATION
OF
THE
SMALLEST
GAUSSIAN
USED
IN
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
THE
LOWER
LINE
ON
THIS
GRAPH
SHOWS
THE
NUMBER
OF
KEYPOINTS
THAT
ARE
CORRECTLY
MATCHED
TO
A
DATABASE
OF
KEYPOINTS
USING
THE
NEAREST
NEIGHBOR
MATCHING
PROCEDURE
TO
BE
DESCRIBED
IN
SECTION
THIS
SHOWS
THAT
ONCE
THE
KEYPOINT
IS
REPEATABLY
LOCATED
IT
IS
LIKELY
TO
BE
USEFUL
FOR
RECOGNITION
AND
MATCHING
TASKS
AS
THIS
GRAPH
SHOWS
THE
HIGHEST
REPEATABILITY
IS
OBTAINED
WHEN
SAMPLING
SCALES
PER
OCTAVE
AND
THIS
IS
THE
NUMBER
OF
SCALE
SAMPLES
USED
FOR
ALL
OTHER
EXPERIMENTS
THROUGHOUT
THIS
PAPER
IT
MIGHT
SEEM
SURPRISING
THAT
THE
REPEATABILITY
DOES
NOT
CONTINUE
TO
IMPROVE
AS
MORE
SCALES
ARE
SAMPLED
THE
REASON
IS
THAT
THIS
RESULTS
IN
MANY
MORE
LOCAL
EXTREMA
BEING
DETECTED
BUT
THESE
EXTREMA
ARE
ON
AVERAGE
LESS
STABLE
AND
THEREFORE
ARE
LESS
LIKELY
TO
BE
DETECTED
IN
THE
TRANSFORMED
IMAGE
THIS
IS
SHOWN
BY
THE
SECOND
GRAPH
IN
FIGURE
WHICH
SHOWS
THE
AVERAGE
NUMBER
OF
KEYPOINTS
DETECTED
AND
CORRECTLY
MATCHED
IN
EACH
IMAGE
THE
NUMBER
OF
KEYPOINTS
RISES
WITH
INCREASED
SAMPLING
OF
SCALES
AND
THE
TOTAL
NUMBER
OF
CORRECT
MATCHES
ALSO
RISES
SINCE
THE
SUCCESS
OF
OBJECT
RECOGNITION
OFTEN
DEPENDS
MORE
ON
THE
QUANTITY
OF
CORRECTLY
MATCHED
KEYPOINTS
AS
OPPOSED
TO
THEIR
PERCENTAGE
CORRECT
MATCHING
FOR
MANY
APPLICATIONS
IT
WILL
BE
OPTIMAL
TO
USE
A
LARGER
NUMBER
OF
SCALE
SAMPLES
HOWEVER
THE
COST
OF
COMPUTATION
ALSO
RISES
WITH
THIS
NUMBER
SO
FOR
THE
EXPERIMENTS
IN
THIS
PAPER
WE
HAVE
CHOSEN
TO
USE
JUST
SCALE
SAMPLES
PER
OCTAVE
TO
SUMMARIZE
THESE
EXPERIMENTS
SHOW
THAT
THE
SCALE
SPACE
DIFFERENCE
OF
GAUSSIAN
FUNC
TION
HAS
A
LARGE
NUMBER
OF
EXTREMA
AND
THAT
IT
WOULD
BE
VERY
EXPENSIVE
TO
DETECT
THEM
ALL
FORTUNATELY
WE
CAN
DETECT
THE
MOST
STABLE
AND
USEFUL
SUBSET
EVEN
WITH
A
COARSE
SAMPLING
OF
SCALES
FREQUENCY
OF
SAMPLING
IN
THE
SPATIAL
DOMAIN
JUST
AS
WE
DETERMINED
THE
FREQUENCY
OF
SAMPLING
PER
OCTAVE
OF
SCALE
SPACE
SO
WE
MUST
DE
TERMINE
THE
FREQUENCY
OF
SAMPLING
IN
THE
IMAGE
DOMAIN
RELATIVE
TO
THE
SCALE
OF
SMOOTHING
GIVEN
THAT
EXTREMA
CAN
BE
ARBITRARILY
CLOSE
TOGETHER
THERE
WILL
BE
A
SIMILAR
TRADE
OFF
BETWEEN
SAMPLING
FREQUENCY
AND
RATE
OF
DETECTION
FIGURE
SHOWS
AN
EXPERIMENTAL
DETERMINATION
OF
THE
AMOUNT
OF
PRIOR
SMOOTHING
Σ
THAT
IS
APPLIED
TO
EACH
IMAGE
LEVEL
BEFORE
BUILDING
THE
SCALE
SPACE
REPRESENTATION
FOR
AN
OCTAVE
AGAIN
THE
TOP
LINE
IS
THE
REPEATABILITY
OF
KEYPOINT
DETECTION
AND
THE
RESULTS
SHOW
THAT
THE
REPEATABILITY
CONTINUES
TO
INCREASE
WITH
Σ
HOWEVER
THERE
IS
A
COST
TO
USING
A
LARGE
Σ
IN
TERMS
OF
EFFICIENCY
SO
WE
HAVE
CHOSEN
TO
USE
Σ
WHICH
PROVIDES
CLOSE
TO
OPTIMAL
REPEATABILITY
THIS
VALUE
IS
USED
THROUGHOUT
THIS
PAPER
AND
WAS
USED
FOR
THE
RESULTS
IN
FIGURE
OF
COURSE
IF
WE
PRE
SMOOTH
THE
IMAGE
BEFORE
EXTREMA
DETECTION
WE
ARE
EFFECTIVELY
DIS
CARDING
THE
HIGHEST
SPATIAL
FREQUENCIES
THEREFORE
TO
MAKE
FULL
USE
OF
THE
INPUT
THE
IMAGE
CAN
BE
EXPANDED
TO
CREATE
MORE
SAMPLE
POINTS
THAN
WERE
PRESENT
IN
THE
ORIGINAL
WE
DOU
BLE
THE
SIZE
OF
THE
INPUT
IMAGE
USING
LINEAR
INTERPOLATION
PRIOR
TO
BUILDING
THE
FIRST
LEVEL
OF
THE
PYRAMID
WHILE
THE
EQUIVALENT
OPERATION
COULD
EFFECTIVELY
HAVE
BEEN
PERFORMED
BY
US
ING
SETS
OF
SUBPIXEL
OFFSET
FILTERS
ON
THE
ORIGINAL
IMAGE
THE
IMAGE
DOUBLING
LEADS
TO
A
MORE
EFFICIENT
IMPLEMENTATION
WE
ASSUME
THAT
THE
ORIGINAL
IMAGE
HAS
A
BLUR
OF
AT
LEAST
Σ
THE
MINIMUM
NEEDED
TO
PREVENT
SIGNIFICANT
ALIASING
AND
THAT
THEREFORE
THE
DOUBLED
IMAGE
HAS
Σ
RELATIVE
TO
ITS
NEW
PIXEL
SPACING
THIS
MEANS
THAT
LITTLE
ADDITIONAL
SMOOTHING
IS
NEEDED
PRIOR
TO
CREATION
OF
THE
FIRST
OCTAVE
OF
SCALE
SPACE
THE
IMAGE
DOUBLING
INCREASES
THE
NUMBER
OF
STABLE
KEYPOINTS
BY
ALMOST
A
FACTOR
OF
BUT
NO
SIGNIFICANT
FURTHER
IMPROVEMENTS
WERE
FOUND
WITH
A
LARGER
EXPANSION
FACTOR
ACCURATE
KEYPOINT
LOCALIZATION
ONCE
A
KEYPOINT
CANDIDATE
HAS
BEEN
FOUND
BY
COMPARING
A
PIXEL
TO
ITS
NEIGHBORS
THE
NEXT
STEP
IS
TO
PERFORM
A
DETAILED
FIT
TO
THE
NEARBY
DATA
FOR
LOCATION
SCALE
AND
RATIO
OF
PRINCIPAL
CURVATURES
THIS
INFORMATION
ALLOWS
POINTS
TO
BE
REJECTED
THAT
HAVE
LOW
CONTRAST
AND
ARE
THEREFORE
SENSITIVE
TO
NOISE
OR
ARE
POORLY
LOCALIZED
ALONG
AN
EDGE
THE
INITIAL
IMPLEMENTATION
OF
THIS
APPROACH
LOWE
SIMPLY
LOCATED
KEYPOINTS
AT
THE
LOCATION
AND
SCALE
OF
THE
CENTRAL
SAMPLE
POINT
HOWEVER
RECENTLY
BROWN
HAS
DEVELOPED
A
METHOD
BROWN
AND
LOWE
FOR
FITTING
A
QUADRATIC
FUNCTION
TO
THE
LOCAL
SAMPLE
POINTS
TO
DETERMINE
THE
INTERPOLATED
LOCATION
OF
THE
MAXIMUM
AND
HIS
EXPERIMENTS
SHOWED
THAT
THIS
PROVIDES
A
SUBSTANTIAL
IMPROVEMENT
TO
MATCHING
AND
STABILITY
HIS
APPROACH
USES
THE
TAYLOR
EXPANSION
UP
TO
THE
QUADRATIC
TERMS
OF
THE
SCALE
SPACE
FUNCTION
D
X
Y
Σ
SHIFTED
SO
THAT
THE
ORIGIN
IS
AT
THE
SAMPLE
POINT
D
X
D
D
T
X
XT
X
X
WHERE
D
AND
ITS
DERIVATIVES
ARE
EVALUATED
AT
THE
SAMPLE
POINT
AND
X
X
Y
Σ
T
IS
THE
OFFSET
FROM
THIS
POINT
THE
LOCATION
OF
THE
EXTREMUM
Xˆ
IS
DETERMINED
BY
TAKING
THE
DERIVATIVE
OF
THIS
FUNCTION
WITH
RESPECT
TO
X
AND
SETTING
IT
TO
ZERO
GIVING
D
Xˆ
X
A
FIGURE
THIS
FIGURE
SHOWS
THE
STAGES
OF
KEYPOINT
SELECTION
A
THE
PIXEL
ORIGINAL
IMAGE
B
THE
INITIAL
KEYPOINTS
LOCATIONS
AT
MAXIMA
AND
MINIMA
OF
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
KEYPOINTS
ARE
DISPLAYED
AS
VECTORS
INDICATING
SCALE
ORIENTATION
AND
LOCATION
C
AFTER
APPLYING
A
THRESHOLD
ON
MINIMUM
CONTRAST
KEYPOINTS
REMAIN
D
THE
FINAL
KEYPOINTS
THAT
REMAIN
FOLLOWING
AN
ADDITIONAL
THRESHOLD
ON
RATIO
OF
PRINCIPAL
CURVATURES
AS
SUGGESTED
BY
BROWN
THE
HESSIAN
AND
DERIVATIVE
OF
D
ARE
APPROXIMATED
BY
USING
DIF
FERENCES
OF
NEIGHBORING
SAMPLE
POINTS
THE
RESULTING
LINEAR
SYSTEM
CAN
BE
SOLVED
WITH
MINIMAL
COST
IF
THE
OFFSET
Xˆ
IS
LARGER
THAN
IN
ANY
DIMENSION
THEN
IT
MEANS
THAT
THE
EX
TREMUM
LIES
CLOSER
TO
A
DIFFERENT
SAMPLE
POINT
IN
THIS
CASE
THE
SAMPLE
POINT
IS
CHANGED
AND
THE
INTERPOLATION
PERFORMED
INSTEAD
ABOUT
THAT
POINT
THE
FINAL
OFFSET
Xˆ
IS
ADDED
TO
THE
LOCATION
OF
ITS
SAMPLE
POINT
TO
GET
THE
INTERPOLATED
ESTIMATE
FOR
THE
LOCATION
OF
THE
EXTREMUM
THE
FUNCTION
VALUE
AT
THE
EXTREMUM
D
Xˆ
IS
USEFUL
FOR
REJECTING
UNSTABLE
EXTREMA
WITH
LOW
CONTRAST
THIS
CAN
BE
OBTAINED
BY
SUBSTITUTING
EQUATION
INTO
GIVING
D
Xˆ
D
D
T
Xˆ
X
FOR
THE
EXPERIMENTS
IN
THIS
PAPER
ALL
EXTREMA
WITH
A
VALUE
OF
D
Xˆ
LESS
THAN
WERE
DISCARDED
AS
BEFORE
WE
ASSUME
IMAGE
PIXEL
VALUES
IN
THE
RANGE
FIGURE
SHOWS
THE
EFFECTS
OF
KEYPOINT
SELECTION
ON
A
NATURAL
IMAGE
IN
ORDER
TO
AVOID
TOO
MUCH
CLUTTER
A
LOW
RESOLUTION
BY
PIXEL
IMAGE
IS
USED
AND
KEYPOINTS
ARE
SHOWN
AS
VECTORS
GIVING
THE
LOCATION
SCALE
AND
ORIENTATION
OF
EACH
KEYPOINT
ORIENTATION
ASSIGNMENT
IS
DESCRIBED
BELOW
FIGURE
A
SHOWS
THE
ORIGINAL
IMAGE
WHICH
IS
SHOWN
AT
REDUCED
CONTRAST
BEHIND
THE
SUBSEQUENT
FIGURES
FIGURE
B
SHOWS
THE
KEYPOINTS
AT
ALL
DETECTED
MAXIMA
AND
MINIMA
OF
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
WHILE
C
SHOWS
THE
KEYPOINTS
THAT
REMAIN
FOLLOWING
REMOVAL
OF
THOSE
WITH
A
VALUE
OF
D
Xˆ
LESS
THAN
PART
D
WILL
BE
EXPLAINED
IN
THE
FOLLOWING
SECTION
ELIMINATING
EDGE
RESPONSES
FOR
STABILITY
IT
IS
NOT
SUFFICIENT
TO
REJECT
KEYPOINTS
WITH
LOW
CONTRAST
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
WILL
HAVE
A
STRONG
RESPONSE
ALONG
EDGES
EVEN
IF
THE
LOCATION
ALONG
THE
EDGE
IS
POORLY
DETERMINED
AND
THEREFORE
UNSTABLE
TO
SMALL
AMOUNTS
OF
NOISE
A
POORLY
DEFINED
PEAK
IN
THE
DIFFERENCE
OF
GAUSSIAN
FUNCTION
WILL
HAVE
A
LARGE
PRINCIPAL
CURVATURE
ACROSS
THE
EDGE
BUT
A
SMALL
ONE
IN
THE
PERPENDICULAR
DIRECTION
THE
PRINCIPAL
CURVA
TURES
CAN
BE
COMPUTED
FROM
A
HESSIAN
MATRIX
H
COMPUTED
AT
THE
LOCATION
AND
SCALE
OF
THE
KEYPOINT
H
DXX
DXY
DXY
DYY
THE
DERIVATIVES
ARE
ESTIMATED
BY
TAKING
DIFFERENCES
OF
NEIGHBORING
SAMPLE
POINTS
THE
EIGENVALUES
OF
H
ARE
PROPORTIONAL
TO
THE
PRINCIPAL
CURVATURES
OF
D
BORROWING
FROM
THE
APPROACH
USED
BY
HARRIS
AND
STEPHENS
WE
CAN
AVOID
EXPLICITLY
COMPUTING
THE
EIGENVALUES
AS
WE
ARE
ONLY
CONCERNED
WITH
THEIR
RATIO
LET
Α
BE
THE
EIGENVALUE
WITH
THE
LARGEST
MAGNITUDE
AND
Β
BE
THE
SMALLER
ONE
THEN
WE
CAN
COMPUTE
THE
SUM
OF
THE
EIGENVALUES
FROM
THE
TRACE
OF
H
AND
THEIR
PRODUCT
FROM
THE
DETERMINANT
TR
H
DXX
DYY
Α
Β
DET
H
DXXDYY
DXY
ΑΒ
IN
THE
UNLIKELY
EVENT
THAT
THE
DETERMINANT
IS
NEGATIVE
THE
CURVATURES
HAVE
DIFFERENT
SIGNS
SO
THE
POINT
IS
DISCARDED
AS
NOT
BEING
AN
EXTREMUM
LET
R
BE
THE
RATIO
BETWEEN
THE
LARGEST
MAGNITUDE
EIGENVALUE
AND
THE
SMALLER
ONE
SO
THAT
Α
RΒ
THEN
TR
H
DET
H
Α
Β
ΑΒ
RΒ
Β
R
R
WHICH
DEPENDS
ONLY
ON
THE
RATIO
OF
THE
EIGENVALUES
RATHER
THAN
THEIR
INDIVIDUAL
VALUES
THE
QUANTITY
R
R
IS
AT
A
MINIMUM
WHEN
THE
TWO
EIGENVALUES
ARE
EQUAL
AND
IT
INCREASES
WITH
R
THEREFORE
TO
CHECK
THAT
THE
RATIO
OF
PRINCIPAL
CURVATURES
IS
BELOW
SOME
THRESHOLD
R
WE
ONLY
NEED
TO
CHECK
TR
H
DET
H
R
R
THIS
IS
VERY
EFFICIENT
TO
COMPUTE
WITH
LESS
THAN
FLOATING
POINT
OPERATIONS
REQUIRED
TO
TEST
EACH
KEYPOINT
THE
EXPERIMENTS
IN
THIS
PAPER
USE
A
VALUE
OF
R
WHICH
ELIMINATES
KEYPOINTS
THAT
HAVE
A
RATIO
BETWEEN
THE
PRINCIPAL
CURVATURES
GREATER
THAN
THE
TRANSITION
FROM
FIGURE
C
TO
D
SHOWS
THE
EFFECTS
OF
THIS
OPERATION
ORIENTATION
ASSIGNMENT
BY
ASSIGNING
A
CONSISTENT
ORIENTATION
TO
EACH
KEYPOINT
BASED
ON
LOCAL
IMAGE
PROPERTIES
THE
KEYPOINT
DESCRIPTOR
CAN
BE
REPRESENTED
RELATIVE
TO
THIS
ORIENTATION
AND
THEREFORE
ACHIEVE
IN
VARIANCE
TO
IMAGE
ROTATION
THIS
APPROACH
CONTRASTS
WITH
THE
ORIENTATION
INVARIANT
DESCRIPTORS
OF
SCHMID
AND
MOHR
IN
WHICH
EACH
IMAGE
PROPERTY
IS
BASED
ON
A
ROTATIONALLY
INVARIANT
MEASURE
THE
DISADVANTAGE
OF
THAT
APPROACH
IS
THAT
IT
LIMITS
THE
DESCRIPTORS
THAT
CAN
BE
USED
AND
DISCARDS
IMAGE
INFORMATION
BY
NOT
REQUIRING
ALL
MEASURES
TO
BE
BASED
ON
A
CONSISTENT
ROTATION
FOLLOWING
EXPERIMENTATION
WITH
A
NUMBER
OF
APPROACHES
TO
ASSIGNING
A
LOCAL
ORIENTATION
THE
FOLLOWING
APPROACH
WAS
FOUND
TO
GIVE
THE
MOST
STABLE
RESULTS
THE
SCALE
OF
THE
KEYPOINT
IS
USED
TO
SELECT
THE
GAUSSIAN
SMOOTHED
IMAGE
L
WITH
THE
CLOSEST
SCALE
SO
THAT
ALL
COMPU
TATIONS
ARE
PERFORMED
IN
A
SCALE
INVARIANT
MANNER
FOR
EACH
IMAGE
SAMPLE
L
X
Y
AT
THIS
SCALE
THE
GRADIENT
MAGNITUDE
M
X
Y
AND
ORIENTATION
Θ
X
Y
IS
PRECOMPUTED
USING
PIXEL
DIFFERENCES
M
X
Y
I
L
X
Y
L
X
Y
L
X
Y
L
X
Y
Θ
X
Y
TAN
L
X
Y
L
X
Y
L
X
Y
L
X
Y
AN
ORIENTATION
HISTOGRAM
IS
FORMED
FROM
THE
GRADIENT
ORIENTATIONS
OF
SAMPLE
POINTS
WITHIN
A
REGION
AROUND
THE
KEYPOINT
THE
ORIENTATION
HISTOGRAM
HAS
BINS
COVERING
THE
DEGREE
RANGE
OF
ORIENTATIONS
EACH
SAMPLE
ADDED
TO
THE
HISTOGRAM
IS
WEIGHTED
BY
ITS
GRADIENT
MAGNI
TUDE
AND
BY
A
GAUSSIAN
WEIGHTED
CIRCULAR
WINDOW
WITH
A
Σ
THAT
IS
TIMES
THAT
OF
THE
SCALE
OF
THE
KEYPOINT
PEAKS
IN
THE
ORIENTATION
HISTOGRAM
CORRESPOND
TO
DOMINANT
DIRECTIONS
OF
LOCAL
GRADIENTS
THE
HIGHEST
PEAK
IN
THE
HISTOGRAM
IS
DETECTED
AND
THEN
ANY
OTHER
LOCAL
PEAK
THAT
IS
WITHIN
OF
THE
HIGHEST
PEAK
IS
USED
TO
ALSO
CREATE
A
KEYPOINT
WITH
THAT
ORIENTATION
THEREFORE
FOR
LOCATIONS
WITH
MULTIPLE
PEAKS
OF
SIMILAR
MAGNITUDE
THERE
WILL
BE
MULTIPLE
KEYPOINTS
CREATED
AT
THE
SAME
LOCATION
AND
SCALE
BUT
DIFFERENT
ORIENTATIONS
ONLY
ABOUT
OF
POINTS
ARE
ASSIGNED
MULTIPLE
ORIENTATIONS
BUT
THESE
CONTRIBUTE
SIGNIFICANTLY
TO
THE
STABILITY
OF
MATCHING
FINALLY
A
PARABOLA
IS
FIT
TO
THE
HISTOGRAM
VALUES
CLOSEST
TO
EACH
PEAK
TO
INTERPOLATE
THE
PEAK
POSITION
FOR
BETTER
ACCURACY
FIGURE
SHOWS
THE
EXPERIMENTAL
STABILITY
OF
LOCATION
SCALE
AND
ORIENTATION
ASSIGNMENT
UNDER
DIFFERING
AMOUNTS
OF
IMAGE
NOISE
AS
BEFORE
THE
IMAGES
ARE
ROTATED
AND
SCALED
BY
RANDOM
AMOUNTS
THE
TOP
LINE
SHOWS
THE
STABILITY
OF
KEYPOINT
LOCATION
AND
SCALE
ASSIGN
MENT
THE
SECOND
LINE
SHOWS
THE
STABILITY
OF
MATCHING
WHEN
THE
ORIENTATION
ASSIGNMENT
IS
ALSO
REQUIRED
TO
BE
WITHIN
DEGREES
AS
SHOWN
BY
THE
GAP
BETWEEN
THE
TOP
TWO
LINES
THE
ORIENTATION
ASSIGNMENT
REMAINS
ACCURATE
OF
THE
TIME
EVEN
AFTER
ADDITION
OF
PIXEL
NOISE
EQUIVALENT
TO
A
CAMERA
PROVIDING
LESS
THAN
BITS
OF
PRECISION
THE
MEASURED
VARI
ANCE
OF
ORIENTATION
FOR
THE
CORRECT
MATCHES
IS
ABOUT
DEGREES
RISING
TO
DEGREES
FOR
NOISE
THE
BOTTOM
LINE
IN
FIGURE
SHOWS
THE
FINAL
ACCURACY
OF
CORRECTLY
MATCHING
A
KEYPOINT
DESCRIPTOR
TO
A
DATABASE
OF
KEYPOINTS
TO
BE
DISCUSSED
BELOW
AS
THIS
GRAPH
SHOWS
THE
SIFT
FEATURES
ARE
RESISTANT
TO
EVEN
LARGE
AMOUNTS
OF
PIXEL
NOISE
AND
THE
MAJOR
CAUSE
OF
ERROR
IS
THE
INITIAL
LOCATION
AND
SCALE
DETECTION
IMAGE
NOISE
FIGURE
THE
TOP
LINE
IN
THE
GRAPH
SHOWS
THE
PERCENT
OF
KEYPOINT
LOCATIONS
AND
SCALES
THAT
ARE
REPEAT
ABLY
DETECTED
AS
A
FUNCTION
OF
PIXEL
NOISE
THE
SECOND
LINE
SHOWS
THE
REPEATABILITY
AFTER
ALSO
REQUIRING
AGREEMENT
IN
ORIENTATION
THE
BOTTOM
LINE
SHOWS
THE
FINAL
PERCENT
OF
DESCRIPTORS
CORRECTLY
MATCHED
TO
A
LARGE
DATABASE
THE
LOCAL
IMAGE
DESCRIPTOR
THE
PREVIOUS
OPERATIONS
HAVE
ASSIGNED
AN
IMAGE
LOCATION
SCALE
AND
ORIENTATION
TO
EACH
KEY
POINT
THESE
PARAMETERS
IMPOSE
A
REPEATABLE
LOCAL
COORDINATE
SYSTEM
IN
WHICH
TO
DESCRIBE
THE
LOCAL
IMAGE
REGION
AND
THEREFORE
PROVIDE
INVARIANCE
TO
THESE
PARAMETERS
THE
NEXT
STEP
IS
TO
COMPUTE
A
DESCRIPTOR
FOR
THE
LOCAL
IMAGE
REGION
THAT
IS
HIGHLY
DISTINCTIVE
YET
IS
AS
INVARIANT
AS
POSSIBLE
TO
REMAINING
VARIATIONS
SUCH
AS
CHANGE
IN
ILLUMINATION
OR
VIEWPOINT
ONE
OBVIOUS
APPROACH
WOULD
BE
TO
SAMPLE
THE
LOCAL
IMAGE
INTENSITIES
AROUND
THE
KEY
POINT
AT
THE
APPROPRIATE
SCALE
AND
TO
MATCH
THESE
USING
A
NORMALIZED
CORRELATION
MEASURE
HOWEVER
SIMPLE
CORRELATION
OF
IMAGE
PATCHES
IS
HIGHLY
SENSITIVE
TO
CHANGES
THAT
CAUSE
MIS
REGISTRATION
OF
SAMPLES
SUCH
AS
AFFINE
OR
VIEWPOINT
CHANGE
OR
NON
RIGID
DEFORMATIONS
A
BETTER
APPROACH
HAS
BEEN
DEMONSTRATED
BY
EDELMAN
INTRATOR
AND
POGGIO
THEIR
PRO
POSED
REPRESENTATION
WAS
BASED
UPON
A
MODEL
OF
BIOLOGICAL
VISION
IN
PARTICULAR
OF
COMPLEX
NEURONS
IN
PRIMARY
VISUAL
CORTEX
THESE
COMPLEX
NEURONS
RESPOND
TO
A
GRADIENT
AT
A
PARTICULAR
ORIENTATION
AND
SPATIAL
FREQUENCY
BUT
THE
LOCATION
OF
THE
GRADIENT
ON
THE
RETINA
IS
ALLOWED
TO
SHIFT
OVER
A
SMALL
RECEPTIVE
FIELD
RATHER
THAN
BEING
PRECISELY
LOCALIZED
EDELMAN
ET
AL
HYPOTH
ESIZED
THAT
THE
FUNCTION
OF
THESE
COMPLEX
NEURONS
WAS
TO
ALLOW
FOR
MATCHING
AND
RECOGNITION
OF
OBJECTS
FROM
A
RANGE
OF
VIEWPOINTS
THEY
HAVE
PERFORMED
DETAILED
EXPERIMENTS
USING
COMPUTER
MODELS
OF
OBJECT
AND
ANIMAL
SHAPES
WHICH
SHOW
THAT
MATCHING
GRADIENTS
WHILE
ALLOWING
FOR
SHIFTS
IN
THEIR
POSITION
RESULTS
IN
MUCH
BETTER
CLASSIFICATION
UNDER
ROTATION
FOR
EXAMPLE
RECOGNITION
ACCURACY
FOR
OBJECTS
ROTATED
IN
DEPTH
BY
DEGREES
INCREASED
FROM
FOR
CORRELATION
OF
GRADIENTS
TO
USING
THE
COMPLEX
CELL
MODEL
OUR
IMPLEMENTATION
DESCRIBED
BELOW
WAS
INSPIRED
BY
THIS
IDEA
BUT
ALLOWS
FOR
POSITIONAL
SHIFT
USING
A
DIFFERENT
COMPUTATIONAL
MECHANISM
IMAGE
GRADIENTS
KEYPOINT
DESCRIPTOR
FIGURE
A
KEYPOINT
DESCRIPTOR
IS
CREATED
BY
FIRST
COMPUTING
THE
GRADIENT
MAGNITUDE
AND
ORIENTATION
AT
EACH
IMAGE
SAMPLE
POINT
IN
A
REGION
AROUND
THE
KEYPOINT
LOCATION
AS
SHOWN
ON
THE
LEFT
THESE
ARE
WEIGHTED
BY
A
GAUSSIAN
WINDOW
INDICATED
BY
THE
OVERLAID
CIRCLE
THESE
SAMPLES
ARE
THEN
ACCUMULATED
INTO
ORIENTATION
HISTOGRAMS
SUMMARIZING
THE
CONTENTS
OVER
SUBREGIONS
AS
SHOWN
ON
THE
RIGHT
WITH
THE
LENGTH
OF
EACH
ARROW
CORRESPONDING
TO
THE
SUM
OF
THE
GRADIENT
MAGNITUDES
NEAR
THAT
DIRECTION
WITHIN
THE
REGION
THIS
FIGURE
SHOWS
A
DESCRIPTOR
ARRAY
COMPUTED
FROM
AN
SET
OF
SAMPLES
WHEREAS
THE
EXPERIMENTS
IN
THIS
PAPER
USE
DESCRIPTORS
COMPUTED
FROM
A
SAMPLE
ARRAY
DESCRIPTOR
REPRESENTATION
FIGURE
ILLUSTRATES
THE
COMPUTATION
OF
THE
KEYPOINT
DESCRIPTOR
FIRST
THE
IMAGE
GRADIENT
MAG
NITUDES
AND
ORIENTATIONS
ARE
SAMPLED
AROUND
THE
KEYPOINT
LOCATION
USING
THE
SCALE
OF
THE
KEYPOINT
TO
SELECT
THE
LEVEL
OF
GAUSSIAN
BLUR
FOR
THE
IMAGE
IN
ORDER
TO
ACHIEVE
ORIENTATION
INVARIANCE
THE
COORDINATES
OF
THE
DESCRIPTOR
AND
THE
GRADIENT
ORIENTATIONS
ARE
ROTATED
RELATIVE
TO
THE
KEYPOINT
ORIENTATION
FOR
EFFICIENCY
THE
GRADIENTS
ARE
PRECOMPUTED
FOR
ALL
LEVELS
OF
THE
PYRAMID
AS
DESCRIBED
IN
SECTION
THESE
ARE
ILLUSTRATED
WITH
SMALL
ARROWS
AT
EACH
SAMPLE
LOCATION
ON
THE
LEFT
SIDE
OF
FIGURE
A
GAUSSIAN
WEIGHTING
FUNCTION
WITH
Σ
EQUAL
TO
ONE
HALF
THE
WIDTH
OF
THE
DESCRIPTOR
WIN
DOW
IS
USED
TO
ASSIGN
A
WEIGHT
TO
THE
MAGNITUDE
OF
EACH
SAMPLE
POINT
THIS
IS
ILLUSTRATED
WITH
A
CIRCULAR
WINDOW
ON
THE
LEFT
SIDE
OF
FIGURE
ALTHOUGH
OF
COURSE
THE
WEIGHT
FALLS
OFF
SMOOTHLY
THE
PURPOSE
OF
THIS
GAUSSIAN
WINDOW
IS
TO
AVOID
SUDDEN
CHANGES
IN
THE
DESCRIPTOR
WITH
SMALL
CHANGES
IN
THE
POSITION
OF
THE
WINDOW
AND
TO
GIVE
LESS
EMPHASIS
TO
GRADIENTS
THAT
ARE
FAR
FROM
THE
CENTER
OF
THE
DESCRIPTOR
AS
THESE
ARE
MOST
AFFECTED
BY
MISREGISTRATION
ERRORS
THE
KEYPOINT
DESCRIPTOR
IS
SHOWN
ON
THE
RIGHT
SIDE
OF
FIGURE
IT
ALLOWS
FOR
SIGNIFICANT
SHIFT
IN
GRADIENT
POSITIONS
BY
CREATING
ORIENTATION
HISTOGRAMS
OVER
SAMPLE
REGIONS
THE
FIGURE
SHOWS
EIGHT
DIRECTIONS
FOR
EACH
ORIENTATION
HISTOGRAM
WITH
THE
LENGTH
OF
EACH
ARROW
CORRESPONDING
TO
THE
MAGNITUDE
OF
THAT
HISTOGRAM
ENTRY
A
GRADIENT
SAMPLE
ON
THE
LEFT
CAN
SHIFT
UP
TO
SAMPLE
POSITIONS
WHILE
STILL
CONTRIBUTING
TO
THE
SAME
HISTOGRAM
ON
THE
RIGHT
THEREBY
ACHIEVING
THE
OBJECTIVE
OF
ALLOWING
FOR
LARGER
LOCAL
POSITIONAL
SHIFTS
IT
IS
IMPORTANT
TO
AVOID
ALL
BOUNDARY
AFFECTS
IN
WHICH
THE
DESCRIPTOR
ABRUPTLY
CHANGES
AS
A
SAMPLE
SHIFTS
SMOOTHLY
FROM
BEING
WITHIN
ONE
HISTOGRAM
TO
ANOTHER
OR
FROM
ONE
ORIENTATION
TO
ANOTHER
THEREFORE
TRILINEAR
INTERPOLATION
IS
USED
TO
DISTRIBUTE
THE
VALUE
OF
EACH
GRADIENT
SAMPLE
INTO
ADJACENT
HISTOGRAM
BINS
IN
OTHER
WORDS
EACH
ENTRY
INTO
A
BIN
IS
MULTIPLIED
BY
A
WEIGHT
OF
D
FOR
EACH
DIMENSION
WHERE
D
IS
THE
DISTANCE
OF
THE
SAMPLE
FROM
THE
CENTRAL
VALUE
OF
THE
BIN
AS
MEASURED
IN
UNITS
OF
THE
HISTOGRAM
BIN
SPACING
THE
DESCRIPTOR
IS
FORMED
FROM
A
VECTOR
CONTAINING
THE
VALUES
OF
ALL
THE
ORIENTATION
HIS
TOGRAM
ENTRIES
CORRESPONDING
TO
THE
LENGTHS
OF
THE
ARROWS
ON
THE
RIGHT
SIDE
OF
FIGURE
THE
FIGURE
SHOWS
A
ARRAY
OF
ORIENTATION
HISTOGRAMS
WHEREAS
OUR
EXPERIMENTS
BELOW
SHOW
THAT
THE
BEST
RESULTS
ARE
ACHIEVED
WITH
A
ARRAY
OF
HISTOGRAMS
WITH
ORIENTATION
BINS
IN
EACH
THEREFORE
THE
EXPERIMENTS
IN
THIS
PAPER
USE
A
ELEMENT
FEATURE
VECTOR
FOR
EACH
KEYPOINT
FINALLY
THE
FEATURE
VECTOR
IS
MODIFIED
TO
REDUCE
THE
EFFECTS
OF
ILLUMINATION
CHANGE
FIRST
THE
VECTOR
IS
NORMALIZED
TO
UNIT
LENGTH
A
CHANGE
IN
IMAGE
CONTRAST
IN
WHICH
EACH
PIXEL
VALUE
IS
MULTIPLIED
BY
A
CONSTANT
WILL
MULTIPLY
GRADIENTS
BY
THE
SAME
CONSTANT
SO
THIS
CONTRAST
CHANGE
WILL
BE
CANCELED
BY
VECTOR
NORMALIZATION
A
BRIGHTNESS
CHANGE
IN
WHICH
A
CONSTANT
IS
ADDED
TO
EACH
IMAGE
PIXEL
WILL
NOT
AFFECT
THE
GRADIENT
VALUES
AS
THEY
ARE
COMPUTED
FROM
PIXEL
DIFFERENCES
THEREFORE
THE
DESCRIPTOR
IS
INVARIANT
TO
AFFINE
CHANGES
IN
ILLUMINATION
HOWEVER
NON
LINEAR
ILLUMINATION
CHANGES
CAN
ALSO
OCCUR
DUE
TO
CAMERA
SATURATION
OR
DUE
TO
ILLUMINATION
CHANGES
THAT
AFFECT
SURFACES
WITH
DIFFERING
ORIENTATIONS
BY
DIFFERENT
AMOUNTS
THESE
EFFECTS
CAN
CAUSE
A
LARGE
CHANGE
IN
RELATIVE
MAGNITUDES
FOR
SOME
GRADIENTS
BUT
ARE
LESS
LIKELY
TO
AFFECT
THE
GRADIENT
ORIENTATIONS
THEREFORE
WE
REDUCE
THE
INFLUENCE
OF
LARGE
GRADIENT
MAGNITUDES
BY
THRESHOLDING
THE
VALUES
IN
THE
UNIT
FEATURE
VECTOR
TO
EACH
BE
NO
LARGER
THAN
AND
THEN
RENORMALIZING
TO
UNIT
LENGTH
THIS
MEANS
THAT
MATCHING
THE
MAGNITUDES
FOR
LARGE
GRADIENTS
IS
NO
LONGER
AS
IMPORTANT
AND
THAT
THE
DISTRIBUTION
OF
ORIENTATIONS
HAS
GREATER
EMPHASIS
THE
VALUE
OF
WAS
DETERMINED
EXPERIMENTALLY
USING
IMAGES
CONTAINING
DIFFERING
ILLUMINATIONS
FOR
THE
SAME
OBJECTS
DESCRIPTOR
TESTING
THERE
ARE
TWO
PARAMETERS
THAT
CAN
BE
USED
TO
VARY
THE
COMPLEXITY
OF
THE
DESCRIPTOR
THE
NUMBER
OF
ORIENTATIONS
R
IN
THE
HISTOGRAMS
AND
THE
WIDTH
N
OF
THE
N
N
ARRAY
OF
ORIENTATION
HISTOGRAMS
THE
SIZE
OF
THE
RESULTING
DESCRIPTOR
VECTOR
IS
AS
THE
COMPLEXITY
OF
THE
DESCRIPTOR
GROWS
IT
WILL
BE
ABLE
TO
DISCRIMINATE
BETTER
IN
A
LARGE
DATABASE
BUT
IT
WILL
ALSO
BE
MORE
SENSITIVE
TO
SHAPE
DISTORTIONS
AND
OCCLUSION
FIGURE
SHOWS
EXPERIMENTAL
RESULTS
IN
WHICH
THE
NUMBER
OF
ORIENTATIONS
AND
SIZE
OF
THE
DESCRIPTOR
WERE
VARIED
THE
GRAPH
WAS
GENERATED
FOR
A
VIEWPOINT
TRANSFORMATION
IN
WHICH
A
PLANAR
SURFACE
IS
TILTED
BY
DEGREES
AWAY
FROM
THE
VIEWER
AND
IMAGE
NOISE
IS
ADDED
THIS
IS
NEAR
THE
LIMITS
OF
RELIABLE
MATCHING
AS
IT
IS
IN
THESE
MORE
DIFFICULT
CASES
THAT
DESCRIPTOR
PERFORMANCE
IS
MOST
IMPORTANT
THE
RESULTS
SHOW
THE
PERCENT
OF
KEYPOINTS
THAT
FIND
A
CORRECT
MATCH
TO
THE
SINGLE
CLOSEST
NEIGHBOR
AMONG
A
DATABASE
OF
KEYPOINTS
THE
GRAPH
SHOWS
THAT
A
SINGLE
ORIENTATION
HISTOGRAM
N
IS
VERY
POOR
AT
DISCRIMINATING
BUT
THE
RESULTS
CONTINUE
TO
IMPROVE
UP
TO
A
ARRAY
OF
HISTOGRAMS
WITH
ORIENTATIONS
AFTER
THAT
ADDING
MORE
ORIENTATIONS
OR
A
LARGER
DESCRIPTOR
CAN
ACTUALLY
HURT
MATCHING
BY
MAKING
THE
DESCRIPTOR
MORE
SENSITIVE
TO
DISTORTION
THESE
RESULTS
WERE
BROADLY
SIMILAR
FOR
OTHER
DEGREES
OF
VIEW
POINT
CHANGE
AND
NOISE
ALTHOUGH
IN
SOME
SIMPLER
CASES
DISCRIMINATION
CONTINUED
TO
IMPROVE
FROM
ALREADY
HIGH
LEVELS
WITH
AND
HIGHER
DESCRIPTOR
SIZES
THROUGHOUT
THIS
PAPER
WE
USE
A
DESCRIPTOR
WITH
ORIENTATIONS
RESULTING
IN
FEATURE
VECTORS
WITH
DIMENSIONS
WHILE
THE
DIMENSIONALITY
OF
THE
DESCRIPTOR
MAY
SEEM
HIGH
WE
HAVE
FOUND
THAT
IT
CONSISTENTLY
PERFORMS
BETTER
THAN
LOWER
DIMENSIONAL
DESCRIPTORS
ON
A
RANGE
OF
MATCHING
TASKS
AND
THAT
THE
COMPUTATIONAL
COST
OF
MATCHING
REMAINS
LOW
WHEN
USING
THE
APPROXIMATE
NEAREST
NEIGHBOR
METHODS
DESCRIBED
BELOW
WIDTH
N
OF
DESCRIPTOR
ANGLE
DEG
NOISE
FIGURE
THIS
GRAPH
SHOWS
THE
PERCENT
OF
KEYPOINTS
GIVING
THE
CORRECT
MATCH
TO
A
DATABASE
OF
KEYPOINTS
AS
A
FUNCTION
OF
WIDTH
OF
THE
N
N
KEYPOINT
DESCRIPTOR
AND
THE
NUMBER
OF
ORIENTATIONS
IN
EACH
HISTOGRAM
THE
GRAPH
IS
COMPUTED
FOR
IMAGES
WITH
AFFINE
VIEWPOINT
CHANGE
OF
DEGREES
AND
ADDITION
OF
NOISE
SENSITIVITY
TO
AFFINE
CHANGE
THE
SENSITIVITY
OF
THE
DESCRIPTOR
TO
AFFINE
CHANGE
IS
EXAMINED
IN
FIGURE
THE
GRAPH
SHOWS
THE
RELIABILITY
OF
KEYPOINT
LOCATION
AND
SCALE
SELECTION
ORIENTATION
ASSIGNMENT
AND
NEAREST
NEIGHBOR
MATCHING
TO
A
DATABASE
AS
A
FUNCTION
OF
ROTATION
IN
DEPTH
OF
A
PLANE
AWAY
FROM
A
VIEWER
IT
CAN
BE
SEEN
THAT
EACH
STAGE
OF
COMPUTATION
HAS
REDUCED
REPEATABILITY
WITH
INCREAS
ING
AFFINE
DISTORTION
BUT
THAT
THE
FINAL
MATCHING
ACCURACY
REMAINS
ABOVE
OUT
TO
A
DEGREE
CHANGE
IN
VIEWPOINT
TO
ACHIEVE
RELIABLE
MATCHING
OVER
A
WIDER
VIEWPOINT
ANGLE
ONE
OF
THE
AFFINE
INVARIANT
DETECTORS
COULD
BE
USED
TO
SELECT
AND
RESAMPLE
IMAGE
REGIONS
AS
DISCUSSED
IN
SECTION
AS
MENTIONED
THERE
NONE
OF
THESE
APPROACHES
IS
TRULY
AFFINE
INVARIANT
AS
THEY
ALL
START
FROM
INITIAL
FEATURE
LOCATIONS
DETERMINED
IN
A
NON
AFFINE
INVARIANT
MANNER
IN
WHAT
APPEARS
TO
BE
THE
MOST
AFFINE
INVARIANT
METHOD
MIKOLAJCZYK
HAS
PROPOSED
AND
RUN
DETAILED
EXPERIMENTS
WITH
THE
HARRIS
AFFINE
DETECTOR
HE
FOUND
THAT
ITS
KEYPOINT
REPEATABILITY
IS
BELOW
THAT
GIVEN
HERE
OUT
TO
ABOUT
A
DEGREE
VIEWPOINT
ANGLE
BUT
THAT
IT
THEN
RETAINS
CLOSE
TO
REPEATABILITY
OUT
TO
AN
ANGLE
OF
DEGREES
WHICH
PROVIDES
BETTER
PERFORMANCE
FOR
EXTREME
AFFINE
CHANGES
THE
DISADVANTAGES
ARE
A
MUCH
HIGHER
COMPUTATIONAL
COST
A
REDUCTION
IN
THE
NUMBER
OF
KEYPOINTS
AND
POORER
STABILITY
FOR
SMALL
AFFINE
CHANGES
DUE
TO
ERRORS
IN
ASSIGNING
A
CONSISTENT
AFFINE
FRAME
UNDER
NOISE
IN
PRACTICE
THE
ALLOWABLE
RANGE
OF
ROTATION
FOR
OBJECTS
IS
CONSIDERABLY
LESS
THAN
FOR
PLANAR
SURFACES
SO
AFFINE
INVARIANCE
IS
USUALLY
NOT
THE
LIMITING
FACTOR
IN
THE
ABILITY
TO
MATCH
ACROSS
VIEWPOINT
CHANGE
IF
A
WIDE
RANGE
OF
AFFINE
INVARIANCE
IS
DESIRED
SUCH
AS
FOR
A
SURFACE
THAT
IS
KNOWN
TO
BE
PLANAR
THEN
A
SIMPLE
SOLUTION
IS
TO
ADOPT
THE
APPROACH
OF
PRITCHARD
AND
HEIDRICH
IN
WHICH
ADDITIONAL
SIFT
FEATURES
ARE
GENERATED
FROM
AFFINE
TRANSFORMED
VERSIONS
OF
THE
TRAINING
IMAGE
CORRESPONDING
TO
DEGREE
VIEWPOINT
CHANGES
THIS
ALLOWS
FOR
THE
USE
OF
STANDARD
SIFT
FEATURES
WITH
NO
ADDITIONAL
COST
WHEN
PROCESSING
THE
IMAGE
TO
BE
RECOGNIZED
BUT
RESULTS
IN
AN
INCREASE
IN
THE
SIZE
OF
THE
FEATURE
DATABASE
BY
A
FACTOR
OF
VIEWPOINT
ANGLE
DEGREES
FIGURE
THIS
GRAPH
SHOWS
THE
STABILITY
OF
DETECTION
FOR
KEYPOINT
LOCATION
ORIENTATION
AND
FINAL
MATCHING
TO
A
DATABASE
AS
A
FUNCTION
OF
AFFINE
DISTORTION
THE
DEGREE
OF
AFFINE
DISTORTION
IS
EXPRESSED
IN
TERMS
OF
THE
EQUIVALENT
VIEWPOINT
ROTATION
IN
DEPTH
FOR
A
PLANAR
SURFACE
MATCHING
TO
LARGE
DATABASES
AN
IMPORTANT
REMAINING
ISSUE
FOR
MEASURING
THE
DISTINCTIVENESS
OF
FEATURES
IS
HOW
THE
RE
LIABILITY
OF
MATCHING
VARIES
AS
A
FUNCTION
OF
THE
NUMBER
OF
FEATURES
IN
THE
DATABASE
BEING
MATCHED
MOST
OF
THE
EXAMPLES
IN
THIS
PAPER
ARE
GENERATED
USING
A
DATABASE
OF
IMAGES
WITH
ABOUT
KEYPOINTS
FIGURE
SHOWS
HOW
THE
MATCHING
RELIABILITY
VARIES
AS
A
FUNC
TION
OF
DATABASE
SIZE
THIS
FIGURE
WAS
GENERATED
USING
A
LARGER
DATABASE
OF
IMAGES
WITH
A
VIEWPOINT
DEPTH
ROTATION
OF
DEGREES
AND
IMAGE
NOISE
IN
ADDITION
TO
THE
USUAL
RANDOM
IMAGE
ROTATION
AND
SCALE
CHANGE
THE
DASHED
LINE
SHOWS
THE
PORTION
OF
IMAGE
FEATURES
FOR
WHICH
THE
NEAREST
NEIGHBOR
IN
THE
DATABASE
WAS
THE
CORRECT
MATCH
AS
A
FUNCTION
OF
DATABASE
SIZE
SHOWN
ON
A
LOGARITHMIC
SCALE
THE
LEFTMOST
POINT
IS
MATCHING
AGAINST
FEATURES
FROM
ONLY
A
SINGLE
IMAGE
WHILE
THE
RIGHTMOST
POINT
IS
SELECTING
MATCHES
FROM
A
DATABASE
OF
ALL
FEATURES
FROM
THE
IMAGES
IT
CAN
BE
SEEN
THAT
MATCHING
RELIABILITY
DOES
DECREASE
AS
A
FUNCTION
OF
THE
NUMBER
OF
DISTRACTORS
YET
ALL
INDICATIONS
ARE
THAT
MANY
CORRECT
MATCHES
WILL
CONTINUE
TO
BE
FOUND
OUT
TO
VERY
LARGE
DATABASE
SIZES
THE
SOLID
LINE
IS
THE
PERCENTAGE
OF
KEYPOINTS
THAT
WERE
IDENTIFIED
AT
THE
CORRECT
MATCH
ING
LOCATION
AND
ORIENTATION
IN
THE
TRANSFORMED
IMAGE
SO
IT
IS
ONLY
THESE
POINTS
THAT
HAVE
ANY
CHANCE
OF
HAVING
MATCHING
DESCRIPTORS
IN
THE
DATABASE
THE
REASON
THIS
LINE
IS
FLAT
IS
THAT
THE
TEST
WAS
RUN
OVER
THE
FULL
DATABASE
FOR
EACH
VALUE
WHILE
ONLY
VARYING
THE
PORTION
OF
THE
DATABASE
USED
FOR
DISTRACTORS
IT
IS
OF
INTEREST
THAT
THE
GAP
BETWEEN
THE
TWO
LINES
IS
SMALL
INDICATING
THAT
MATCHING
FAILURES
ARE
DUE
MORE
TO
ISSUES
WITH
INITIAL
FEATURE
LOCALIZATION
AND
ORIENTATION
ASSIGNMENT
THAN
TO
PROBLEMS
WITH
FEATURE
DISTINCTIVENESS
EVEN
OUT
TO
LARGE
DATABASE
SIZES
NUMBER
OF
KEYPOINTS
IN
DATABASE
LOG
SCALE
FIGURE
THE
DASHED
LINE
SHOWS
THE
PERCENT
OF
KEYPOINTS
CORRECTLY
MATCHED
TO
A
DATABASE
AS
A
FUNCTION
OF
DATABASE
SIZE
USING
A
LOGARITHMIC
SCALE
THE
SOLID
LINE
SHOWS
THE
PERCENT
OF
KEYPOINTS
ASSIGNED
THE
CORRECT
LOCATION
SCALE
AND
ORIENTATION
IMAGES
HAD
RANDOM
SCALE
AND
ROTATION
CHANGES
AN
AFFINE
TRANSFORM
OF
DEGREES
AND
IMAGE
NOISE
OF
ADDED
PRIOR
TO
MATCHING
APPLICATION
TO
OBJECT
RECOGNITION
THE
MAJOR
TOPIC
OF
THIS
PAPER
IS
THE
DERIVATION
OF
DISTINCTIVE
INVARIANT
KEYPOINTS
AS
DESCRIBED
ABOVE
TO
DEMONSTRATE
THEIR
APPLICATION
WE
WILL
NOW
GIVE
A
BRIEF
DESCRIPTION
OF
THEIR
USE
FOR
OBJECT
RECOGNITION
IN
THE
PRESENCE
OF
CLUTTER
AND
OCCLUSION
MORE
DETAILS
ON
APPLICATIONS
OF
THESE
FEATURES
TO
RECOGNITION
ARE
AVAILABLE
IN
OTHER
PAPERS
LOWE
LOWE
SE
LOWE
AND
LITTLE
OBJECT
RECOGNITION
IS
PERFORMED
BY
FIRST
MATCHING
EACH
KEYPOINT
INDEPENDENTLY
TO
THE
DATABASE
OF
KEYPOINTS
EXTRACTED
FROM
TRAINING
IMAGES
MANY
OF
THESE
INITIAL
MATCHES
WILL
BE
INCORRECT
DUE
TO
AMBIGUOUS
FEATURES
OR
FEATURES
THAT
ARISE
FROM
BACKGROUND
CLUTTER
THEREFORE
CLUSTERS
OF
AT
LEAST
FEATURES
ARE
FIRST
IDENTIFIED
THAT
AGREE
ON
AN
OBJECT
AND
ITS
POSE
AS
THESE
CLUSTERS
HAVE
A
MUCH
HIGHER
PROBABILITY
OF
BEING
CORRECT
THAN
INDIVIDUAL
FEATURE
MATCHES
THEN
EACH
CLUSTER
IS
CHECKED
BY
PERFORMING
A
DETAILED
GEOMETRIC
FIT
TO
THE
MODEL
AND
THE
RESULT
IS
USED
TO
ACCEPT
OR
REJECT
THE
INTERPRETATION
KEYPOINT
MATCHING
THE
BEST
CANDIDATE
MATCH
FOR
EACH
KEYPOINT
IS
FOUND
BY
IDENTIFYING
ITS
NEAREST
NEIGHBOR
IN
THE
DATABASE
OF
KEYPOINTS
FROM
TRAINING
IMAGES
THE
NEAREST
NEIGHBOR
IS
DEFINED
AS
THE
KEYPOINT
WITH
MINIMUM
EUCLIDEAN
DISTANCE
FOR
THE
INVARIANT
DESCRIPTOR
VECTOR
AS
WAS
DESCRIBED
IN
SECTION
HOWEVER
MANY
FEATURES
FROM
AN
IMAGE
WILL
NOT
HAVE
ANY
CORRECT
MATCH
IN
THE
TRAINING
DATABASE
BECAUSE
THEY
ARISE
FROM
BACKGROUND
CLUTTER
OR
WERE
NOT
DETECTED
IN
THE
TRAINING
IM
AGES
THEREFORE
IT
WOULD
BE
USEFUL
TO
HAVE
A
WAY
TO
DISCARD
FEATURES
THAT
DO
NOT
HAVE
ANY
GOOD
MATCH
TO
THE
DATABASE
A
GLOBAL
THRESHOLD
ON
DISTANCE
TO
THE
CLOSEST
FEATURE
DOES
NOT
PERFORM
WELL
AS
SOME
DESCRIPTORS
ARE
MUCH
MORE
DISCRIMINATIVE
THAN
OTHERS
A
MORE
EF
FECTIVE
MEASURE
IS
OBTAINED
BY
COMPARING
THE
DISTANCE
OF
THE
CLOSEST
NEIGHBOR
TO
THAT
OF
THE
RATIO
OF
DISTANCES
CLOSEST
NEXT
CLOSEST
FIGURE
THE
PROBABILITY
THAT
A
MATCH
IS
CORRECT
CAN
BE
DETERMINED
BY
TAKING
THE
RATIO
OF
DISTANCE
FROM
THE
CLOSEST
NEIGHBOR
TO
THE
DISTANCE
OF
THE
SECOND
CLOSEST
USING
A
DATABASE
OF
KEYPOINTS
THE
SOLID
LINE
SHOWS
THE
PDF
OF
THIS
RATIO
FOR
CORRECT
MATCHES
WHILE
THE
DOTTED
LINE
IS
FOR
MATCHES
THAT
WERE
INCORRECT
SECOND
CLOSEST
NEIGHBOR
IF
THERE
ARE
MULTIPLE
TRAINING
IMAGES
OF
THE
SAME
OBJECT
THEN
WE
DEFINE
THE
SECOND
CLOSEST
NEIGHBOR
AS
BEING
THE
CLOSEST
NEIGHBOR
THAT
IS
KNOWN
TO
COME
FROM
A
DIFFERENT
OBJECT
THAN
THE
FIRST
SUCH
AS
BY
ONLY
USING
IMAGES
KNOWN
TO
CONTAIN
DIFFERENT
OB
JECTS
THIS
MEASURE
PERFORMS
WELL
BECAUSE
CORRECT
MATCHES
NEED
TO
HAVE
THE
CLOSEST
NEIGHBOR
SIGNIFICANTLY
CLOSER
THAN
THE
CLOSEST
INCORRECT
MATCH
TO
ACHIEVE
RELIABLE
MATCHING
FOR
FALSE
MATCHES
THERE
WILL
LIKELY
BE
A
NUMBER
OF
OTHER
FALSE
MATCHES
WITHIN
SIMILAR
DISTANCES
DUE
TO
THE
HIGH
DIMENSIONALITY
OF
THE
FEATURE
SPACE
WE
CAN
THINK
OF
THE
SECOND
CLOSEST
MATCH
AS
PROVIDING
AN
ESTIMATE
OF
THE
DENSITY
OF
FALSE
MATCHES
WITHIN
THIS
PORTION
OF
THE
FEATURE
SPACE
AND
AT
THE
SAME
TIME
IDENTIFYING
SPECIFIC
INSTANCES
OF
FEATURE
AMBIGUITY
FIGURE
SHOWS
THE
VALUE
OF
THIS
MEASURE
FOR
REAL
IMAGE
DATA
THE
PROBABILITY
DENSITY
FUNCTIONS
FOR
CORRECT
AND
INCORRECT
MATCHES
ARE
SHOWN
IN
TERMS
OF
THE
RATIO
OF
CLOSEST
TO
SECOND
CLOSEST
NEIGHBORS
OF
EACH
KEYPOINT
MATCHES
FOR
WHICH
THE
NEAREST
NEIGHBOR
WAS
A
CORRECT
MATCH
HAVE
A
PDF
THAT
IS
CENTERED
AT
A
MUCH
LOWER
RATIO
THAN
THAT
FOR
INCORRECT
MATCHES
FOR
OUR
OBJECT
RECOGNITION
IMPLEMENTATION
WE
REJECT
ALL
MATCHES
IN
WHICH
THE
DISTANCE
RATIO
IS
GREATER
THAN
WHICH
ELIMINATES
OF
THE
FALSE
MATCHES
WHILE
DISCARDING
LESS
THAN
OF
THE
CORRECT
MATCHES
THIS
FIGURE
WAS
GENERATED
BY
MATCHING
IMAGES
FOLLOWING
RANDOM
SCALE
AND
ORIENTATION
CHANGE
A
DEPTH
ROTATION
OF
DEGREES
AND
ADDITION
OF
IMAGE
NOISE
AGAINST
A
DATABASE
OF
KEYPOINTS
EFFICIENT
NEAREST
NEIGHBOR
INDEXING
NO
ALGORITHMS
ARE
KNOWN
THAT
CAN
IDENTIFY
THE
EXACT
NEAREST
NEIGHBORS
OF
POINTS
IN
HIGH
DI
MENSIONAL
SPACES
THAT
ARE
ANY
MORE
EFFICIENT
THAN
EXHAUSTIVE
SEARCH
OUR
KEYPOINT
DESCRIPTOR
HAS
A
DIMENSIONAL
FEATURE
VECTOR
AND
THE
BEST
ALGORITHMS
SUCH
AS
THE
K
D
TREE
FRIEDMAN
ET
AL
PROVIDE
NO
SPEEDUP
OVER
EXHAUSTIVE
SEARCH
FOR
MORE
THAN
ABOUT
DIMENSIONAL
SPACES
THEREFORE
WE
HAVE
USED
AN
APPROXIMATE
ALGORITHM
CALLED
THE
BEST
BIN
FIRST
BBF
ALGORITHM
BEIS
AND
LOWE
THIS
IS
APPROXIMATE
IN
THE
SENSE
THAT
IT
RETURNS
THE
CLOSEST
NEIGHBOR
WITH
HIGH
PROBABILITY
THE
BBF
ALGORITHM
USES
A
MODIFIED
SEARCH
ORDERING
FOR
THE
K
D
TREE
ALGORITHM
SO
THAT
BINS
IN
FEATURE
SPACE
ARE
SEARCHED
IN
THE
ORDER
OF
THEIR
CLOSEST
DISTANCE
FROM
THE
QUERY
LOCATION
THIS
PRIORITY
SEARCH
ORDER
WAS
FIRST
EXAMINED
BY
ARYA
AND
MOUNT
AND
THEY
PROVIDE
FURTHER
STUDY
OF
ITS
COMPUTATIONAL
PROPERTIES
IN
ARYA
ET
AL
THIS
SEARCH
ORDER
REQUIRES
THE
USE
OF
A
HEAP
BASED
PRIORITY
QUEUE
FOR
EFFICIENT
DETERMINATION
OF
THE
SEARCH
ORDER
AN
APPROXIMATE
ANSWER
CAN
BE
RETURNED
WITH
LOW
COST
BY
CUTTING
OFF
FURTHER
SEARCH
AFTER
A
SPECIFIC
NUMBER
OF
THE
NEAREST
BINS
HAVE
BEEN
EXPLORED
IN
OUR
IMPLEMENTATION
WE
CUT
OFF
SEARCH
AFTER
CHECKING
THE
FIRST
NEAREST
NEIGHBOR
CANDIDATES
FOR
A
DATABASE
OF
KEYPOINTS
THIS
PROVIDES
A
SPEEDUP
OVER
EXACT
NEAREST
NEIGHBOR
SEARCH
BY
ABOUT
ORDERS
OF
MAGNITUDE
YET
RESULTS
IN
LESS
THAN
A
LOSS
IN
THE
NUMBER
OF
CORRECT
MATCHES
ONE
REASON
THE
BBF
ALGORITHM
WORKS
PARTICULARLY
WELL
FOR
THIS
PROBLEM
IS
THAT
WE
ONLY
CONSIDER
MATCHES
IN
WHICH
THE
NEAREST
NEIGHBOR
IS
LESS
THAN
TIMES
THE
DISTANCE
TO
THE
SECOND
NEAREST
NEIGHBOR
AS
DESCRIBED
IN
THE
PREVIOUS
SECTION
AND
THEREFORE
THERE
IS
NO
NEED
TO
EXACTLY
SOLVE
THE
MOST
DIFFICULT
CASES
IN
WHICH
MANY
NEIGHBORS
ARE
AT
VERY
SIMILAR
DISTANCES
CLUSTERING
WITH
THE
HOUGH
TRANSFORM
TO
MAXIMIZE
THE
PERFORMANCE
OF
OBJECT
RECOGNITION
FOR
SMALL
OR
HIGHLY
OCCLUDED
OBJECTS
WE
WISH
TO
IDENTIFY
OBJECTS
WITH
THE
FEWEST
POSSIBLE
NUMBER
OF
FEATURE
MATCHES
WE
HAVE
FOUND
THAT
RELIABLE
RECOGNITION
IS
POSSIBLE
WITH
AS
FEW
AS
FEATURES
A
TYPICAL
IMAGE
CONTAINS
OR
MORE
FEATURES
WHICH
MAY
COME
FROM
MANY
DIFFERENT
OBJECTS
AS
WELL
AS
BACKGROUND
CLUTTER
WHILE
THE
DISTANCE
RATIO
TEST
DESCRIBED
IN
SECTION
WILL
ALLOW
US
TO
DISCARD
MANY
OF
THE
FALSE
MATCHES
ARISING
FROM
BACKGROUND
CLUTTER
THIS
DOES
NOT
REMOVE
MATCHES
FROM
OTHER
VALID
OBJECTS
AND
WE
OFTEN
STILL
NEED
TO
IDENTIFY
CORRECT
SUBSETS
OF
MATCHES
CONTAINING
LESS
THAN
INLIERS
AMONG
OUTLIERS
MANY
WELL
KNOWN
ROBUST
FITTING
METHODS
SUCH
AS
RANSAC
OR
LEAST
MEDIAN
OF
SQUARES
PERFORM
POORLY
WHEN
THE
PERCENT
OF
INLIERS
FALLS
MUCH
BELOW
FORTUNATELY
MUCH
BETTER
PERFORMANCE
CAN
BE
OBTAINED
BY
CLUSTERING
FEATURES
IN
POSE
SPACE
USING
THE
HOUGH
TRANSFORM
HOUGH
BALLARD
GRIMSON
THE
HOUGH
TRANSFORM
IDENTIFIES
CLUSTERS
OF
FEATURES
WITH
A
CONSISTENT
INTERPRETATION
BY
USING
EACH
FEATURE
TO
VOTE
FOR
ALL
OBJECT
POSES
THAT
ARE
CONSISTENT
WITH
THE
FEATURE
WHEN
CLUSTERS
OF
FEATURES
ARE
FOUND
TO
VOTE
FOR
THE
SAME
POSE
OF
AN
OBJECT
THE
PROBABILITY
OF
THE
INTERPRETATION
BEING
CORRECT
IS
MUCH
HIGHER
THAN
FOR
ANY
SINGLE
FEATURE
EACH
OF
OUR
KEYPOINTS
SPECIFIES
PARAMETERS
LOCATION
SCALE
AND
ORIENTATION
AND
EACH
MATCHED
KEYPOINT
IN
THE
DATABASE
HAS
A
RECORD
OF
THE
KEYPOINT
PARAMETERS
RELATIVE
TO
THE
TRAINING
IMAGE
IN
WHICH
IT
WAS
FOUND
THEREFORE
WE
CAN
CREATE
A
HOUGH
TRANSFORM
ENTRY
PREDICTING
THE
MODEL
LOCATION
ORIENTATION
AND
SCALE
FROM
THE
MATCH
HYPOTHESIS
THIS
PREDICTION
HAS
LARGE
ERROR
BOUNDS
AS
THE
SIMILARITY
TRANSFORM
IMPLIED
BY
THESE
PARAMETERS
IS
ONLY
AN
APPROXIMATION
TO
THE
FULL
DEGREE
OF
FREEDOM
POSE
SPACE
FOR
A
OBJECT
AND
ALSO
DOES
NOT
ACCOUNT
FOR
ANY
NON
RIGID
DEFORMATIONS
THEREFORE
WE
USE
BROAD
BIN
SIZES
OF
DEGREES
FOR
ORIENTATION
A
FACTOR
OF
FOR
SCALE
AND
TIMES
THE
MAXIMUM
PROJECTED
TRAINING
IMAGE
DIMENSION
USING
THE
PREDICTED
SCALE
FOR
LOCATION
TO
AVOID
THE
PROBLEM
OF
BOUNDARY
EFFECTS
IN
BIN
ASSIGNMENT
EACH
KEYPOINT
MATCH
VOTES
FOR
THE
CLOSEST
BINS
IN
EACH
DIMENSION
GIVING
A
TOTAL
OF
ENTRIES
FOR
EACH
HYPOTHESIS
AND
FURTHER
BROADENING
THE
POSE
RANGE
IN
MOST
IMPLEMENTATIONS
OF
THE
HOUGH
TRANSFORM
A
MULTI
DIMENSIONAL
ARRAY
IS
USED
TO
REPRESENT
THE
BINS
HOWEVER
MANY
OF
THE
POTENTIAL
BINS
WILL
REMAIN
EMPTY
AND
IT
IS
DIFFICULT
TO
COMPUTE
THE
RANGE
OF
POSSIBLE
BIN
VALUES
DUE
TO
THEIR
MUTUAL
DEPENDENCE
FOR
EXAMPLE
THE
DEPENDENCY
OF
LOCATION
DISCRETIZATION
ON
THE
SELECTED
SCALE
THESE
PROBLEMS
CAN
BE
AVOIDED
BY
USING
A
PSEUDO
RANDOM
HASH
FUNCTION
OF
THE
BIN
VALUES
TO
INSERT
VOTES
INTO
A
ONE
DIMENSIONAL
HASH
TABLE
IN
WHICH
COLLISIONS
ARE
EASILY
DETECTED
SOLUTION
FOR
AFFINE
PARAMETERS
THE
HOUGH
TRANSFORM
IS
USED
TO
IDENTIFY
ALL
CLUSTERS
WITH
AT
LEAST
ENTRIES
IN
A
BIN
EACH
SUCH
CLUSTER
IS
THEN
SUBJECT
TO
A
GEOMETRIC
VERIFICATION
PROCEDURE
IN
WHICH
A
LEAST
SQUARES
SOLUTION
IS
PERFORMED
FOR
THE
BEST
AFFINE
PROJECTION
PARAMETERS
RELATING
THE
TRAINING
IMAGE
TO
THE
NEW
IMAGE
AN
AFFINE
TRANSFORMATION
CORRECTLY
ACCOUNTS
FOR
ROTATION
OF
A
PLANAR
SURFACE
UNDER
ORTHOGRAPHIC
PROJECTION
BUT
THE
APPROXIMATION
CAN
BE
POOR
FOR
ROTATION
OF
NON
PLANAR
OBJECTS
A
MORE
GENERAL
SOLUTION
WOULD
BE
TO
SOLVE
FOR
THE
FUNDAMENTAL
MATRIX
LUONG
AND
FAUGERAS
HARTLEY
AND
ZISSERMAN
HOWEVER
A
FUNDAMENTAL
MATRIX
SOLUTION
REQUIRES
AT
LEAST
POINT
MATCHES
AS
COMPARED
TO
ONLY
FOR
THE
AFFINE
SOLUTION
AND
IN
PRACTICE
REQUIRES
EVEN
MORE
MATCHES
FOR
GOOD
STABILITY
WE
WOULD
LIKE
TO
PERFORM
RECOGNITION
WITH
AS
FEW
AS
FEATURE
MATCHES
SO
THE
AFFINE
SOLUTION
PROVIDES
A
BETTER
STARTING
POINT
AND
WE
CAN
ACCOUNT
FOR
ERRORS
IN
THE
AFFINE
APPROXIMATION
BY
ALLOWING
FOR
LARGE
RESIDUAL
ERRORS
IF
WE
IMAGINE
PLACING
A
SPHERE
AROUND
AN
OBJECT
THEN
ROTATION
OF
THE
SPHERE
BY
DEGREES
WILL
MOVE
NO
POINT
WITHIN
THE
SPHERE
BY
MORE
THAN
TIMES
THE
PROJECTED
DIAMETER
OF
THE
SPHERE
FOR
THE
EXAMPLES
OF
TYPICAL
OBJECTS
USED
IN
THIS
PAPER
AN
AFFINE
SOLUTION
WORKS
WELL
GIVEN
THAT
WE
ALLOW
RESIDUAL
ERRORS
UP
TO
TIMES
THE
MAXIMUM
PROJECTED
DIMENSION
OF
THE
OBJECT
A
MORE
GENERAL
APPROACH
IS
GIVEN
IN
BROWN
AND
LOWE
IN
WHICH
THE
INITIAL
SOLUTION
IS
BASED
ON
A
SIMILARITY
TRANSFORM
WHICH
THEN
PROGRESSES
TO
SOLUTION
FOR
THE
FUNDAMENTAL
MATRIX
IN
THOSE
CASES
IN
WHICH
A
SUFFICIENT
NUMBER
OF
MATCHES
ARE
FOUND
THE
AFFINE
TRANSFORMATION
OF
A
MODEL
POINT
X
Y
T
TO
AN
IMAGE
POINT
U
V
T
CAN
BE
WRITTEN
AS
U
X
TX
WHERE
THE
MODEL
TRANSLATION
IS
TX
TY
T
AND
THE
AFFINE
ROTATION
SCALE
AND
STRETCH
ARE
REPRE
SENTED
BY
THE
MI
PARAMETERS
WE
WISH
TO
SOLVE
FOR
THE
TRANSFORMATION
PARAMETERS
SO
THE
EQUATION
ABOVE
CAN
BE
REWRIT
TEN
TO
GATHER
THE
UNKNOWNS
INTO
A
COLUMN
VECTOR
X
Y
U
V
TX
THIS
EQUATION
SHOWS
A
SINGLE
MATCH
BUT
ANY
NUMBER
OF
FURTHER
MATCHES
CAN
BE
ADDED
WITH
EACH
MATCH
CONTRIBUTING
TWO
MORE
ROWS
TO
THE
FIRST
AND
LAST
MATRIX
AT
LEAST
MATCHES
ARE
NEEDED
TO
PROVIDE
A
SOLUTION
WE
CAN
WRITE
THIS
LINEAR
SYSTEM
AS
AX
B
FIGURE
THE
TRAINING
IMAGES
FOR
TWO
OBJECTS
ARE
SHOWN
ON
THE
LEFT
THESE
CAN
BE
RECOGNIZED
IN
A
CLUTTERED
IMAGE
WITH
EXTENSIVE
OCCLUSION
SHOWN
IN
THE
MIDDLE
THE
RESULTS
OF
RECOGNITION
ARE
SHOWN
ON
THE
RIGHT
A
PARALLELOGRAM
IS
DRAWN
AROUND
EACH
RECOGNIZED
OBJECT
SHOWING
THE
BOUNDARIES
OF
THE
ORIGINAL
TRAINING
IMAGE
UNDER
THE
AFFINE
TRANSFORMATION
SOLVED
FOR
DURING
RECOGNITION
SMALLER
SQUARES
INDICATE
THE
KEYPOINTS
THAT
WERE
USED
FOR
RECOGNITION
THE
LEAST
SQUARES
SOLUTION
FOR
THE
PARAMETERS
X
CAN
BE
DETERMINED
BY
SOLVING
THE
CORRESPOND
ING
NORMAL
EQUATIONS
X
ATA
WHICH
MINIMIZES
THE
SUM
OF
THE
SQUARES
OF
THE
DISTANCES
FROM
THE
PROJECTED
MODEL
LOCATIONS
TO
THE
CORRESPONDING
IMAGE
LOCATIONS
THIS
LEAST
SQUARES
APPROACH
COULD
READILY
BE
EXTENDED
TO
SOLVING
FOR
POSE
AND
INTERNAL
PARAMETERS
OF
ARTICULATED
AND
FLEXIBLE
OBJECTS
LOWE
OUTLIERS
CAN
NOW
BE
REMOVED
BY
CHECKING
FOR
AGREEMENT
BETWEEN
EACH
IMAGE
FEATURE
AND
THE
MODEL
GIVEN
THE
MORE
ACCURATE
LEAST
SQUARES
SOLUTION
WE
NOW
REQUIRE
EACH
MATCH
TO
AGREE
WITHIN
HALF
THE
ERROR
RANGE
THAT
WAS
USED
FOR
THE
PARAMETERS
IN
THE
HOUGH
TRANSFORM
BINS
IF
FEWER
THAN
POINTS
REMAIN
AFTER
DISCARDING
OUTLIERS
THEN
THE
MATCH
IS
REJECTED
AS
OUTLIERS
ARE
DISCARDED
THE
LEAST
SQUARES
SOLUTION
IS
RE
SOLVED
WITH
THE
REMAINING
POINTS
AND
THE
PROCESS
ITERATED
IN
ADDITION
A
TOP
DOWN
MATCHING
PHASE
IS
USED
TO
ADD
ANY
FURTHER
MATCHES
THAT
AGREE
WITH
THE
PROJECTED
MODEL
POSITION
THESE
MAY
HAVE
BEEN
MISSED
FROM
THE
HOUGH
TRANSFORM
BIN
DUE
TO
THE
SIMILARITY
TRANSFORM
APPROXIMATION
OR
OTHER
ERRORS
THE
FINAL
DECISION
TO
ACCEPT
OR
REJECT
A
MODEL
HYPOTHESIS
IS
BASED
ON
A
DETAILED
PROBABILIS
TIC
MODEL
GIVEN
IN
A
PREVIOUS
PAPER
LOWE
THIS
METHOD
FIRST
COMPUTES
THE
EXPECTED
NUMBER
OF
FALSE
MATCHES
TO
THE
MODEL
POSE
GIVEN
THE
PROJECTED
SIZE
OF
THE
MODEL
THE
NUMBER
OF
FEATURES
WITHIN
THE
REGION
AND
THE
ACCURACY
OF
THE
FIT
A
BAYESIAN
ANALYSIS
THEN
GIVES
THE
PROBABILITY
THAT
THE
OBJECT
IS
PRESENT
BASED
ON
THE
ACTUAL
NUMBER
OF
MATCHING
FEATURES
FOUND
WE
ACCEPT
A
MODEL
IF
THE
FINAL
PROBABILITY
FOR
A
CORRECT
INTERPRETATION
IS
GREATER
THAN
FOR
OBJECTS
THAT
PROJECT
TO
SMALL
REGIONS
OF
AN
IMAGE
FEATURES
MAY
BE
SUFFICIENT
FOR
RELI
ABLE
RECOGNITION
FOR
LARGE
OBJECTS
COVERING
MOST
OF
A
HEAVILY
TEXTURED
IMAGE
THE
EXPECTED
NUMBER
OF
FALSE
MATCHES
IS
HIGHER
AND
AS
MANY
AS
FEATURE
MATCHES
MAY
BE
NECESSARY
RECOGNITION
EXAMPLES
FIGURE
SHOWS
AN
EXAMPLE
OF
OBJECT
RECOGNITION
FOR
A
CLUTTERED
AND
OCCLUDED
IMAGE
CON
TAINING
OBJECTS
THE
TRAINING
IMAGES
OF
A
TOY
TRAIN
AND
A
FROG
ARE
SHOWN
ON
THE
LEFT
FIGURE
THIS
EXAMPLE
SHOWS
LOCATION
RECOGNITION
WITHIN
A
COMPLEX
SCENE
THE
TRAINING
IMAGES
FOR
LOCATIONS
ARE
SHOWN
AT
THE
UPPER
LEFT
AND
THE
PIXEL
TEST
IMAGE
TAKEN
FROM
A
DIFFERENT
VIEWPOINT
IS
ON
THE
UPPER
RIGHT
THE
RECOGNIZED
REGIONS
ARE
SHOWN
ON
THE
LOWER
IMAGE
WITH
KEYPOINTS
SHOWN
AS
SQUARES
AND
AN
OUTER
PARALLELOGRAM
SHOWING
THE
BOUNDARIES
OF
THE
TRAINING
IMAGES
UNDER
THE
AFFINE
TRANSFORM
USED
FOR
RECOGNITION
THE
MIDDLE
IMAGE
OF
SIZE
PIXELS
CONTAINS
INSTANCES
OF
THESE
OBJECTS
HIDDEN
BEHIND
OTHERS
AND
WITH
EXTENSIVE
BACKGROUND
CLUTTER
SO
THAT
DETECTION
OF
THE
OBJECTS
MAY
NOT
BE
IM
MEDIATE
EVEN
FOR
HUMAN
VISION
THE
IMAGE
ON
THE
RIGHT
SHOWS
THE
FINAL
CORRECT
IDENTIFICATION
SUPERIMPOSED
ON
A
REDUCED
CONTRAST
VERSION
OF
THE
IMAGE
THE
KEYPOINTS
THAT
WERE
USED
FOR
RECOGNITION
ARE
SHOWN
AS
SQUARES
WITH
AN
EXTRA
LINE
TO
INDICATE
ORIENTATION
THE
SIZES
OF
THE
SQUARES
CORRESPOND
TO
THE
IMAGE
REGIONS
USED
TO
CONSTRUCT
THE
DESCRIPTOR
AN
OUTER
PARALLEL
OGRAM
IS
ALSO
DRAWN
AROUND
EACH
INSTANCE
OF
RECOGNITION
WITH
ITS
SIDES
CORRESPONDING
TO
THE
BOUNDARIES
OF
THE
TRAINING
IMAGES
PROJECTED
UNDER
THE
FINAL
AFFINE
TRANSFORMATION
DETERMINED
DURING
RECOGNITION
ANOTHER
POTENTIAL
APPLICATION
OF
THE
APPROACH
IS
TO
PLACE
RECOGNITION
IN
WHICH
A
MOBILE
DEVICE
OR
VEHICLE
COULD
IDENTIFY
ITS
LOCATION
BY
RECOGNIZING
FAMILIAR
LOCATIONS
FIGURE
GIVES
AN
EXAMPLE
OF
THIS
APPLICATION
IN
WHICH
TRAINING
IMAGES
ARE
TAKEN
OF
A
NUMBER
OF
LOCATIONS
AS
SHOWN
ON
THE
UPPER
LEFT
THESE
CAN
EVEN
BE
OF
SUCH
SEEMINGLY
NON
DISTINCTIVE
ITEMS
AS
A
WOODEN
WALL
OR
A
TREE
WITH
TRASH
BINS
THE
TEST
IMAGE
OF
SIZE
BY
PIXELS
ON
THE
UPPER
RIGHT
WAS
TAKEN
FROM
A
VIEWPOINT
ROTATED
ABOUT
DEGREES
AROUND
THE
SCENE
FROM
THE
ORIGINAL
POSITIONS
YET
THE
TRAINING
IMAGE
LOCATIONS
ARE
EASILY
RECOGNIZED
ALL
STEPS
OF
THE
RECOGNITION
PROCESS
CAN
BE
IMPLEMENTED
EFFICIENTLY
SO
THE
TOTAL
TIME
TO
RECOGNIZE
ALL
OBJECTS
IN
FIGURES
OR
IS
LESS
THAN
SECONDS
ON
A
PENTIUM
PROCESSOR
WE
HAVE
IMPLEMENTED
THESE
ALGORITHMS
ON
A
LAPTOP
COMPUTER
WITH
ATTACHED
VIDEO
CAMERA
AND
HAVE
TESTED
THEM
EXTENSIVELY
OVER
A
WIDE
RANGE
OF
CONDITIONS
IN
GENERAL
TEXTURED
PLANAR
SURFACES
CAN
BE
IDENTIFIED
RELIABLY
OVER
A
ROTATION
IN
DEPTH
OF
UP
TO
DEGREES
IN
ANY
DIRECTION
AND
UNDER
ALMOST
ANY
ILLUMINATION
CONDITIONS
THAT
PROVIDE
SUFFICIENT
LIGHT
AND
DO
NOT
PRODUCE
EXCESSIVE
GLARE
FOR
OBJECTS
THE
RANGE
OF
ROTATION
IN
DEPTH
FOR
RELIABLE
RECOGNITION
IS
ONLY
ABOUT
DEGREES
IN
ANY
DIRECTION
AND
ILLUMINATION
CHANGE
IS
MORE
DISRUPTIVE
FOR
THESE
REASONS
OBJECT
RECOGNITION
IS
BEST
PERFORMED
BY
INTEGRATING
FEATURES
FROM
MULTIPLE
VIEWS
SUCH
AS
WITH
LOCAL
FEATURE
VIEW
CLUSTERING
LOWE
THESE
KEYPOINTS
HAVE
ALSO
BEEN
APPLIED
TO
THE
PROBLEM
OF
ROBOT
LOCALIZATION
AND
MAP
PING
WHICH
HAS
BEEN
PRESENTED
IN
DETAIL
IN
OTHER
PAPERS
SE
LOWE
AND
LITTLE
IN
THIS
APPLICATION
A
TRINOCULAR
STEREO
SYSTEM
IS
USED
TO
DETERMINE
ESTIMATES
FOR
KEYPOINT
LOCA
TIONS
KEYPOINTS
ARE
USED
ONLY
WHEN
THEY
APPEAR
IN
ALL
IMAGES
WITH
CONSISTENT
DISPARITIES
RESULTING
IN
VERY
FEW
OUTLIERS
AS
THE
ROBOT
MOVES
IT
LOCALIZES
ITSELF
USING
FEATURE
MATCHES
TO
THE
EXISTING
MAP
AND
THEN
INCREMENTALLY
ADDS
FEATURES
TO
THE
MAP
WHILE
UPDATING
THEIR
POSITIONS
USING
A
KALMAN
FILTER
THIS
PROVIDES
A
ROBUST
AND
ACCURATE
SOLUTION
TO
THE
PROBLEM
OF
ROBOT
LOCALIZATION
IN
UNKNOWN
ENVIRONMENTS
THIS
WORK
HAS
ALSO
ADDRESSED
THE
PROBLEM
OF
PLACE
RECOGNITION
IN
WHICH
A
ROBOT
CAN
BE
SWITCHED
ON
AND
RECOGNIZE
ITS
LOCATION
ANYWHERE
WITHIN
A
LARGE
MAP
SE
LOWE
AND
LITTLE
WHICH
IS
EQUIVALENT
TO
A
IMPLEMENTATION
OF
OBJECT
RECOGNITION
CONCLUSIONS
THE
SIFT
KEYPOINTS
DESCRIBED
IN
THIS
PAPER
ARE
PARTICULARLY
USEFUL
DUE
TO
THEIR
DISTINCTIVE
NESS
WHICH
ENABLES
THE
CORRECT
MATCH
FOR
A
KEYPOINT
TO
BE
SELECTED
FROM
A
LARGE
DATABASE
OF
OTHER
KEYPOINTS
THIS
DISTINCTIVENESS
IS
ACHIEVED
BY
ASSEMBLING
A
HIGH
DIMENSIONAL
VECTOR
REPRESENTING
THE
IMAGE
GRADIENTS
WITHIN
A
LOCAL
REGION
OF
THE
IMAGE
THE
KEYPOINTS
HAVE
BEEN
SHOWN
TO
BE
INVARIANT
TO
IMAGE
ROTATION
AND
SCALE
AND
ROBUST
ACROSS
A
SUBSTANTIAL
RANGE
OF
AFFINE
DISTORTION
ADDITION
OF
NOISE
AND
CHANGE
IN
ILLUMINATION
LARGE
NUMBERS
OF
KEYPOINTS
CAN
BE
EXTRACTED
FROM
TYPICAL
IMAGES
WHICH
LEADS
TO
ROBUSTNESS
IN
EXTRACTING
SMALL
OBJECTS
AMONG
CLUTTER
THE
FACT
THAT
KEYPOINTS
ARE
DETECTED
OVER
A
COMPLETE
RANGE
OF
SCALES
MEANS
THAT
SMALL
LOCAL
FEATURES
ARE
AVAILABLE
FOR
MATCHING
SMALL
AND
HIGHLY
OCCLUDED
OBJECTS
WHILE
LARGE
KEYPOINTS
PERFORM
WELL
FOR
IMAGES
SUBJECT
TO
NOISE
AND
BLUR
THEIR
COMPUTATION
IS
EFFICIENT
SO
THAT
SEVERAL
THOUSAND
KEYPOINTS
CAN
BE
EXTRACTED
FROM
A
TYPICAL
IMAGE
WITH
NEAR
REAL
TIME
PERFORMANCE
ON
STANDARD
PC
HARDWARE
THIS
PAPER
HAS
ALSO
PRESENTED
METHODS
FOR
USING
THE
KEYPOINTS
FOR
OBJECT
RECOGNITION
THE
APPROACH
WE
HAVE
DESCRIBED
USES
APPROXIMATE
NEAREST
NEIGHBOR
LOOKUP
A
HOUGH
TRANSFORM
FOR
IDENTIFYING
CLUSTERS
THAT
AGREE
ON
OBJECT
POSE
LEAST
SQUARES
POSE
DETERMINATION
AND
FI
NAL
VERIFICATION
OTHER
POTENTIAL
APPLICATIONS
INCLUDE
VIEW
MATCHING
FOR
RECONSTRUCTION
MOTION
TRACKING
AND
SEGMENTATION
ROBOT
LOCALIZATION
IMAGE
PANORAMA
ASSEMBLY
EPIPOLAR
CALIBRATION
AND
ANY
OTHERS
THAT
REQUIRE
IDENTIFICATION
OF
MATCHING
LOCATIONS
BETWEEN
IMAGES
THERE
ARE
MANY
DIRECTIONS
FOR
FURTHER
RESEARCH
IN
DERIVING
INVARIANT
AND
DISTINCTIVE
IMAGE
FEATURES
SYSTEMATIC
TESTING
IS
NEEDED
ON
DATA
SETS
WITH
FULL
VIEWPOINT
AND
ILLUMINATION
CHANGES
THE
FEATURES
DESCRIBED
IN
THIS
PAPER
USE
ONLY
A
MONOCHROME
INTENSITY
IMAGE
SO
FUR
THER
DISTINCTIVENESS
COULD
BE
DERIVED
FROM
INCLUDING
ILLUMINATION
INVARIANT
COLOR
DESCRIPTORS
FUNT
AND
FINLAYSON
BROWN
AND
LOWE
SIMILARLY
LOCAL
TEXTURE
MEASURES
APPEAR
TO
PLAY
AN
IMPORTANT
ROLE
IN
HUMAN
VISION
AND
COULD
BE
INCORPORATED
INTO
FEATURE
DESCRIPTORS
IN
A
MORE
GENERAL
FORM
THAN
THE
SINGLE
SPATIAL
FREQUENCY
USED
BY
THE
CURRENT
DESCRIPTORS
AN
ATTRACTIVE
ASPECT
OF
THE
INVARIANT
LOCAL
FEATURE
APPROACH
TO
MATCHING
IS
THAT
THERE
IS
NO
NEED
TO
SELECT
JUST
ONE
FEATURE
TYPE
AND
THE
BEST
RESULTS
ARE
LIKELY
TO
BE
OBTAINED
BY
USING
MANY
DIFFERENT
FEATURES
ALL
OF
WHICH
CAN
CONTRIBUTE
USEFUL
MATCHES
AND
IMPROVE
OVERALL
ROBUSTNESS
ANOTHER
DIRECTION
FOR
FUTURE
RESEARCH
WILL
BE
TO
INDIVIDUALLY
LEARN
FEATURES
THAT
ARE
SUITED
TO
RECOGNIZING
PARTICULAR
OBJECTS
CATEGORIES
THIS
WILL
BE
PARTICULARLY
IMPORTANT
FOR
GENERIC
OBJECT
CLASSES
THAT
MUST
COVER
A
BROAD
RANGE
OF
POSSIBLE
APPEARANCES
THE
RESEARCH
OF
WE
BER
WELLING
AND
PERONA
AND
FERGUS
PERONA
AND
ZISSERMAN
HAS
SHOWN
THE
POTENTIAL
OF
THIS
APPROACH
BY
LEARNING
SMALL
SETS
OF
LOCAL
FEATURES
THAT
ARE
SUITED
TO
RECOGNIZ
ING
GENERIC
CLASSES
OF
OBJECTS
IN
THE
LONG
TERM
FEATURE
SETS
ARE
LIKELY
TO
CONTAIN
BOTH
PRIOR
AND
LEARNED
FEATURES
THAT
WILL
BE
USED
ACCORDING
TO
THE
AMOUNT
OF
TRAINING
DATA
THAT
HAS
BEEN
AVAILABLE
FOR
VARIOUS
OBJECT
CLASSES
PEEKABOOM
A
GAME
FOR
LOCATING
OBJECTS
IN
IMAGES
LUIS
VON
AHN
RUORAN
LIU
AND
MANUEL
BLUM
COMPUTER
SCIENCE
DEPARTMENT
CARNEGIE
MELLON
UNIVERSITY
FORBES
AVENUE
PITTSBURGH
PA
BIGLOU
ROYLIU
MBLUM
CS
CMU
EDU
ABSTRACT
WE
INTRODUCE
PEEKABOOM
AN
ENTERTAINING
WEB
BASED
GAME
THAT
CAN
HELP
COMPUTERS
LOCATE
OBJECTS
IN
IMAGES
PEOPLE
PLAY
THE
GAME
BECAUSE
OF
ITS
ENTERTAINMENT
VALUE
AND
AS
A
SIDE
EFFECT
OF
THEM
PLAYING
WE
COLLECT
VALUABLE
IMAGE
METADATA
SUCH
AS
WHICH
PIXELS
BELONG
TO
WHICH
OBJECT
IN
THE
IMAGE
THE
COLLECTED
DATA
COULD
BE
APPLIED
TOWARDS
CONSTRUCTING
MORE
ACCURATE
COMPUTER
VISION
ALGORITHMS
WHICH
REQUIRE
MASSIVE
AMOUNTS
OF
TRAINING
AND
TESTING
DATA
NOT
CURRENTLY
AVAILABLE
PEEKABOOM
HAS
BEEN
PLAYED
BY
THOUSANDS
OF
PEOPLE
SOME
OF
WHOM
HAVE
SPENT
OVER
HOURS
A
DAY
PLAYING
AND
THUS
FAR
HAS
GENERATED
MILLIONS
OF
DATA
POINTS
IN
ADDITION
TO
ITS
PURELY
UTILITARIAN
ASPECT
PEEKABOOM
IS
AN
EXAMPLE
OF
A
NEW
EMERGING
CLASS
OF
GAMES
WHICH
NOT
ONLY
BRING
PEOPLE
TOGETHER
FOR
LEISURE
PURPOSES
BUT
ALSO
EXIST
TO
IMPROVE
ARTIFICIAL
INTELLIGENCE
SUCH
GAMES
APPEAL
TO
A
GENERAL
AUDIENCE
WHILE
PROVIDING
ANSWERS
TO
PROBLEMS
THAT
COMPUTERS
CANNOT
YET
SOLVE
AUTHOR
KEYWORDS
DISTRIBUTED
KNOWLEDGE
ACQUISITION
OBJECT
SEGMENTATION
OBJECT
RECOGNITION
COMPUTER
VISION
WEB
BASED
GAMES
ACM
CLASSIFICATION
KEYWORDS
LEARNING
KNOWLEDGE
ACQUISITION
H
HCI
WEB
BASED
INTERACTION
INTRODUCTION
HUMANS
UNDERSTAND
AND
ANALYZE
EVERYDAY
IMAGES
WITH
LITTLE
EFFORT
WHAT
OBJECTS
ARE
IN
THE
IMAGE
WHERE
THEY
ARE
LOCATED
WHAT
IS
THE
BACKGROUND
WHAT
IS
THE
FOREGROUND
ETC
COMPUTERS
ON
THE
OTHER
HAND
STILL
HAVE
TROUBLE
WITH
SUCH
BASIC
VISUAL
TASKS
AS
READING
DISTORTED
TEXT
OR
FINDING
WHERE
IN
THE
IMAGE
A
SIMPLE
OBJECT
IS
LOCATED
ALTHOUGH
RESEARCHERS
HAVE
PROPOSED
AND
TESTED
MANY
IMPRESSIVE
ALGORITHMS
FOR
COMPUTER
VISION
NONE
HAVE
BEEN
MADE
TO
WORK
RELIABLY
AND
GENERALLY
MOST
OF
THE
BEST
APPROACHES
FOR
COMPUTER
VISION
E
G
RELY
ON
MACHINE
LEARNING
TRAIN
AN
ALGORITHM
TO
PERFORM
A
VISUAL
TASK
BY
SHOWING
IT
EXAMPLE
IMAGES
IN
WHICH
THE
TASK
HAS
ALREADY
BEEN
PERFORMED
FOR
EXAMPLE
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
OR
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
CHI
APRIL
MONTRÉAL
QUÉBEC
CANADA
COPYRIGHT
ACM
TRAINING
AN
ALGORITHM
FOR
TESTING
WHETHER
AN
IMAGE
CONTAINS
A
DOG
WOULD
INVOLVE
PRESENTING
IT
WITH
MULTIPLE
IMAGES
OF
DOGS
EACH
ANNOTATED
WITH
THE
PRECISE
LOCATION
OF
THE
DOG
IN
THE
IMAGE
AFTER
PROCESSING
ENOUGH
IMAGES
THE
ALGORITHM
LEARNS
TO
FIND
DOGS
IN
ARBITRARY
IMAGES
A
MAJOR
PROBLEM
WITH
THIS
APPROACH
HOWEVER
IS
THE
LACK
OF
TRAINING
DATA
WHICH
OBVIOUSLY
MUST
BE
PREPARED
BY
HAND
DATABASES
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
CURRENTLY
HAVE
HUNDREDS
OR
AT
BEST
A
FEW
THOUSAND
IMAGES
ORDERS
OF
MAGNITUDE
LESS
THAN
WHAT
IS
REQUIRED
IN
THIS
PAPER
WE
ADDRESS
THE
PROBLEM
OF
CONSTRUCTING
A
MASSIVELY
LARGE
DATABASE
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
THE
TARGET
DATABASE
WILL
CONTAIN
MILLIONS
OF
IMAGES
ALL
FULLY
ANNOTATED
WITH
INFORMATION
ABOUT
WHAT
OBJECTS
ARE
IN
THE
IMAGE
WHERE
EACH
OBJECT
IS
LOCATED
AND
HOW
MUCH
OF
THE
IMAGE
IS
NECESSARY
TO
RECOGNIZE
IT
OUR
DATABASE
WILL
BE
SIMILAR
TO
THOSE
PREVIOUSLY
SHOWN
TO
BE
USEFUL
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
E
G
TO
CONSTRUCT
SUCH
A
DATABASE
WE
FOLLOW
THE
APPROACH
TAKEN
BY
THE
ESP
GAME
AND
INTRODUCE
A
NEW
GAME
CALLED
PEEKABOOM
PEEKABOOM
IS
AN
EXTREMELY
ENJOYABLE
NETWORKED
GAME
IN
WHICH
SIMPLY
BY
PLAYING
PEOPLE
HELP
CONSTRUCT
A
DATABASE
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
WE
GUARANTEE
THE
DATABASE
CORRECTNESS
EVEN
IF
THE
PEOPLE
PLAYING
THE
GAME
DON
T
INTEND
IT
AS
WE
WILL
SHOW
IN
THIS
PAPER
OUR
GAME
IS
ALSO
VERY
ENJOYABLE
WITH
SOME
PEOPLE
HAVING
PLAYED
OVER
HOURS
A
WEEK
WE
WILL
FURTHER
SHOW
THAT
THIS
GAME
CAN
BE
USED
TO
IMPROVE
IMAGE
SEARCH
RESULTS
AND
TO
CALCULATE
OBJECT
BOUNDING
BOXES
SIMILAR
TO
THOSE
IN
FLICKR
SEE
FIGURE
THE
ESP
GAME
IS
AN
INTERACTIVE
SYSTEM
THAT
ALLOWS
PEOPLE
TO
LABEL
IMAGES
WHILE
HAVING
FUN
THE
ESP
GAME
COLLECTS
RANDOM
IMAGES
FROM
THE
WEB
AND
OUTPUTS
WORD
LABELS
DESCRIBING
THE
CONTENTS
OF
THE
IMAGES
THE
GAME
HAS
ALREADY
COLLECTED
MILLIONS
OF
LABELS
FOR
ARBITRARY
IMAGES
GIVEN
AN
IMAGE
THE
ESP
GAME
CAN
BE
USED
TO
DETERMINE
WHAT
OBJECTS
ARE
IN
THE
IMAGE
BUT
CANNOT
BE
USED
TO
DETERMINE
WHERE
IN
THE
IMAGE
EACH
OBJECT
IS
LOCATED
SUCH
LOCATION
INFORMATION
IS
NECESSARY
FOR
TRAINING
AND
TESTING
COMPUTER
VISION
ALGORITHMS
SO
THE
DATA
COLLECTED
BY
THE
ESP
GAME
IS
NOT
SUFFICIENT
FOR
OUR
PURPOSES
THE
GAME
INTRODUCED
IN
THIS
PAPER
PEEKABOOM
IMPROVES
ON
THE
DATA
COLLECTED
BY
THE
ESP
GAME
AND
FOR
EACH
OBJECT
IN
THE
IMAGE
OUTPUTS
PRECISE
LOCATION
INFORMATION
AS
WELL
AS
OTHER
INFORMATION
USEFUL
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
BY
PLAYING
A
GAME
PEOPLE
HELP
US
COLLECT
DATA
FIGURE
PEEK
AND
BOOM
BOOM
GETS
AN
IMAGE
ALONG
WITH
A
WORD
RELATED
TO
IT
AND
MUST
REVEAL
PARTS
OF
THE
IMAGE
FOR
PEEK
TO
GUESS
THE
CORRECT
WORD
PEEK
CAN
ENTER
MULTIPLE
GUESSES
THAT
BOOM
CAN
SEE
NOT
BECAUSE
THEY
WANT
TO
BE
HELPFUL
BUT
BECAUSE
THEY
HAVE
FUN
INDEED
PEEKABOOM
OR
THE
ESP
GAME
OR
ANY
GAME
BUILT
ON
THIS
PREMISE
CAN
BE
TREATED
AS
A
HUMAN
ALGORITHM
ON
INPUT
AN
IMAGE
IT
OUTPUTS
WITH
ARBITRARILY
HIGH
PROBABILITY
A
CORRECT
ANNOTATION
OF
THE
IMAGE
INSTEAD
OF
USING
A
SILICON
PROCESSOR
THIS
ALGORITHM
RUNS
ON
A
PROCESSOR
CONSISTING
OF
REGULAR
HUMANS
INTERACTING
THROUGHOUT
THE
WEB
IN
ADDITION
TO
APPLICATIONS
IN
COMPUTER
VISION
AND
IMAGE
SEARCH
OUR
SYSTEM
MAKES
A
SIGNIFICANT
CONTRIBUTION
TO
HCI
BECAUSE
OF
THE
WAY
IT
ADDRESSES
THE
PROBLEM
PEEKABOOM
PRESENTS
AN
EXAMPLE
OF
A
NEW
LINE
OF
RESEARCH
DEVOTED
TO
SOLVING
LARGE
SCALE
PROBLEMS
WITH
HUMAN
COMPUTING
POWER
WHERE
PEOPLE
INTERACT
WITH
COMPUTERS
TO
EXTEND
THE
COMPUTATIONAL
ABILITIES
OF
MACHINES
BASIC
GAME
PLAY
PEEKABOOM
AS
THE
NAME
MAY
SUGGEST
IS
A
GAME
WITH
TWO
MAIN
COMPONENTS
PEEK
AND
BOOM
TWO
RANDOM
PLAYERS
FROM
THE
WEB
PARTICIPATE
BY
TAKING
DIFFERENT
ROLES
IN
THE
GAME
WHEN
ONE
PLAYER
IS
PEEK
THE
OTHER
IS
BOOM
PEEK
STARTS
OUT
WITH
A
BLANK
SCREEN
WHILE
BOOM
STARTS
WITH
AN
IMAGE
AND
A
WORD
RELATED
TO
IT
SEE
FIGURE
THE
GOAL
OF
THE
GAME
IS
FOR
BOOM
TO
REVEAL
PARTS
OF
THE
IMAGE
TO
PEEK
SO
THAT
PEEK
CAN
GUESS
THE
ASSOCIATED
WORD
BOOM
REVEALS
CIRCULAR
AREAS
OF
THE
IMAGE
BY
CLICKING
A
CLICK
REVEALS
AN
AREA
WITH
A
PIXEL
RADIUS
PEEK
ON
THE
OTHER
HAND
CAN
ENTER
GUESSES
OF
WHAT
BOOM
WORD
IS
BOOM
CAN
SEE
PEEK
GUESSES
AND
CAN
INDICATE
WHETHER
THEY
ARE
HOT
OR
COLD
WHEN
PEEK
CORRECTLY
GUESSES
THE
WORD
THE
PLAYERS
GET
POINTS
AND
SWITCH
ROLES
PLAY
THEN
PROCEEDS
ON
A
NEW
IMAGE
WORD
PAIR
IF
THE
IMAGE
WORD
PAIR
IS
TOO
DIFFICULT
THE
TWO
PLAYERS
CAN
PASS
OR
OPT
OUT
OF
THE
CURRENT
IMAGE
PASSING
CREATES
THE
SAME
EFFECT
AS
A
CORRECT
GUESS
FROM
PEEK
EXCEPT
THAT
THE
PLAYERS
GET
NO
POINTS
TO
MAXIMIZE
POINTS
BOOM
HAS
AN
INCENTIVE
TO
REVEAL
ONLY
THE
AREAS
OF
THE
IMAGE
NECESSARY
FOR
PEEK
TO
GUESS
THE
CORRECT
WORD
FOR
EXAMPLE
IF
THE
IMAGE
CONTAINS
A
CAR
AND
A
DOG
AND
THE
WORD
ASSOCIATED
TO
THE
IMAGE
IS
DOG
THEN
BOOM
WILL
REVEAL
ONLY
THOSE
PARTS
OF
THE
IMAGE
THAT
CONTAIN
THE
DOG
THUS
GIVEN
AN
IMAGE
WORD
PAIR
DATA
FROM
THE
GAME
YIELD
THE
AREA
OF
THE
IMAGE
PERTAINING
TO
THE
WORD
PINGS
ANOTHER
COMPONENT
OF
THE
GAME
ARE
PINGS
RIPPLES
THAT
APPEAR
ON
PEEK
SCREEN
WHEN
BOOM
RIGHT
CLICKS
ON
THE
IMAGE
SEE
FIGURE
IF
TWO
PLAYERS
WERE
PLAYING
WITH
THE
IMAGE
ON
FIGURE
THEN
MANY
CORRECT
WORDS
ARE
POSSIBLE
FROM
PEEK
POINT
OF
VIEW
ELEPHANT
TRUNK
TUSK
EAR
SUPPOSE
THE
CORRECT
WORD
IS
TRUNK
TO
GET
PEEK
TO
GUESS
CORRECTLY
BOOM
CAN
PING
THE
TRUNK
OF
THE
ELEPHANT
BY
RIGHT
CLICKING
ON
IT
IN
DOING
SO
BOOM
HELPS
TO
DISAMBIGUATE
THE
TRUNK
FROM
THE
REST
OF
THE
ELEPHANT
FIGURE
PINGS
TO
HELP
PEEK
BOOM
CAN
PING
PARTS
OF
THE
IMAGE
BY
RIGHT
CLICKING
ON
THEM
FIGURE
HINTS
BOOM
CAN
FURTHER
HELP
PEEK
BY
GIVING
HINTS
ABOUT
HOW
THE
WORD
RELATES
TO
THE
IMAGE
IS
IT
A
NOUN
DESCRIBING
SOMETHING
IN
THE
IMAGE
A
NOUN
RELATED
TO
THE
IMAGE
TEXT
ON
THE
IMAGE
OR
A
VERB
HINTS
ANOTHER
FEATURE
OF
THE
GAME
ARE
BUTTONS
THAT
ALLOW
BOOM
TO
GIVE
HINTS
TO
PEEK
ABOUT
HOW
THE
WORD
RELATES
TO
THE
IMAGE
SEE
FIGURES
AND
UPON
BOOM
PRESSING
OF
ONE
OF
THE
HINT
BUTTONS
A
CORRESPONDING
FLASHING
PLACARD
APPEARS
ON
PEEK
SCREEN
THE
REASON
FOR
HAVING
HINTS
IS
THAT
OFTEN
THE
WORDS
CAN
RELATE
TO
THE
IMAGE
IN
MULTIPLE
WAYS
AS
NOUNS
VERBS
TEXT
OR
RELATED
NOUNS
SOMETHING
NOT
IN
THE
IMAGE
BUT
RELATED
TO
IT
THE
ORIGIN
OF
IMAGES
AND
LABELS
ALL
WORDS
PRESENTED
TO
THE
PLAYERS
ARE
RELATED
TO
THEIR
CORRESPONDING
IMAGE
ON
INPUT
AN
IMAGE
WORD
PAIR
PEEKABOOM
OUTPUTS
A
REGION
OF
THE
IMAGE
THAT
IS
RELATED
TO
THE
WORD
WE
OBTAIN
MILLIONS
OF
IMAGES
WITH
ASSOCIATED
KEYWORD
LABELS
FROM
THE
ESP
GAME
WHICH
WE
NOW
DESCRIBE
IN
MORE
DETAIL
AS
MENTIONED
BEFORE
THE
ESP
GAME
IS
A
TWO
PLAYER
ONLINE
GAME
THAT
PAIRS
RANDOM
PLAYERS
FROM
THE
WEB
FROM
THE
PLAYER
PERSPECTIVE
THE
GOAL
OF
THE
ESP
GAME
IS
TO
GUESS
THE
WORD
THAT
THEIR
PARTNER
IS
TYPING
FOR
EACH
IMAGE
ONCE
BOTH
PLAYERS
HAVE
TYPED
THE
SAME
STRING
THEY
MOVE
ON
TO
A
NEXT
IMAGE
SINCE
THE
PLAYERS
CAN
T
COMMUNICATE
AND
DON
T
KNOW
ANYTHING
ABOUT
EACH
OTHER
THE
EASIEST
WAY
FOR
BOTH
TO
TYPE
THE
SAME
STRING
IS
BY
TYPING
SOMETHING
RELATED
TO
THE
COMMON
IMAGE
THE
STRING
UPON
WHICH
THE
TWO
PLAYERS
AGREE
IS
A
VERY
GOOD
LABEL
FOR
THE
IMAGE
WE
USE
THE
LABELS
COLLECTED
FROM
THE
ESP
GAME
AS
THE
WORDS
WE
PRESENT
TO
THE
PLAYERS
IN
PEEKABOOM
GAME
POINTS
AND
THE
BONUS
ROUND
ALTHOUGH
THE
EXACT
NUMBER
OF
POINTS
GIVEN
TO
THE
PLAYERS
FOR
DIFFERENT
ACTIONS
IS
NOT
IMPORTANT
WE
MENTION
IT
TO
SHOW
THE
RELATIVE
PROPORTIONS
FURTHERMORE
WE
MENTION
THE
DIFFERENT
POINT
STRATEGIES
USED
BY
PEEKABOOM
TO
KEEP
PLAYERS
ENGAGED
POINTS
ARE
GIVEN
TO
BOTH
PEEK
AND
BOOM
EQUALLY
WHENEVER
PEEK
GUESSES
THE
CORRECT
WORD
IN
THE
CURRENT
IMPLEMENTATION
BOTH
OBTAIN
POINTS
POINTS
ARE
NOT
SUBTRACTED
FOR
PASSING
POINTS
ARE
ALSO
GIVEN
TO
BOTH
PEEK
AND
BOOM
FOR
USING
THE
HINT
BUTTONS
ALTHOUGH
THIS
MIGHT
APPEAR
COUNTERINTUITIVE
SINCE
USING
HINTS
DEDUCTS
POINTS
IN
MANY
OTHER
GAMES
WE
ACTUALLY
WANT
THE
PLAYERS
TO
USE
THE
HINT
BUTTONS
AS
MENTIONED
ABOVE
HINTS
GIVE
US
ADDITIONAL
INFORMATION
ABOUT
THE
RELATIONSHIP
BETWEEN
THE
WORD
AND
THE
IMAGE
AND
THEREFORE
WE
ENCOURAGE
PLAYERS
TO
USE
THEM
TWENTY
FIVE
EXTRA
POINTS
ARE
GIVEN
TO
BOTH
PEEK
AND
BOOM
WHENEVER
PEEK
GUESSES
THE
CORRECT
WORD
AND
BOOM
HAD
USED
A
HINT
POINTS
ARE
NOT
GIVEN
FOR
USAGE
OF
THE
HOT
COLD
BUTTONS
EVERY
TIME
THE
PLAYERS
CORRECTLY
COMPLETE
FOUR
IMAGES
THEY
ARE
SENT
TO
A
BONUS
ROUND
THE
BONUS
ROUND
IS
DIFFERENT
IN
NATURE
FROM
THE
REST
OF
THE
GAME
AND
ALLOWS
PLAYERS
TO
OBTAIN
UP
TO
POINTS
IN
THE
BONUS
ROUND
SEE
FIGURE
PLAYERS
SIMPLY
CLICK
ON
AN
OBJECT
IN
THE
IMAGE
THE
CLOSER
THEY
ARE
TO
EACH
OTHER
CLICKS
THE
MORE
POINTS
THEY
GET
FOR
EXAMPLE
BOTH
PLAYERS
COULD
OBTAIN
AN
IMAGE
OF
A
CAR
AND
BE
TOLD
CLICK
ON
THE
CAR
FIGURE
THE
PEEKABOOM
BONUS
ROUND
PLAYERS
MUST
CLICK
ON
THE
SPECIFIED
OBJECT
WITHIN
THE
IMAGE
THEY
OBTAIN
POINTS
PROPORTIONAL
TO
HOW
CLOSE
THEIR
CLICKS
ARE
TO
EACH
OTHER
CLICKS
PLAYERS
OBTAIN
BETWEEN
AND
POINTS
FOR
EVERY
CLICK
IN
THE
BONUS
ROUND
DEPENDING
ON
HOW
FAR
THE
CLICK
IS
FROM
THEIR
PARTNER
CORRESPONDING
CLICK
THE
BONUS
ROUND
IS
TIMED
PLAYERS
HAVE
TO
CLICK
ON
THE
SAME
PLACE
AS
THEIR
PARTNER
AS
MANY
TIMES
AS
THEY
CAN
IN
SECONDS
IF
THE
OBJECT
IS
NOT
IN
THE
IMAGE
PLAYERS
CAN
PASS
BECAUSE
SOME
IMAGES
DO
NOT
CONTAIN
THE
OBJECT
RELATED
TO
THE
WORD
PASSING
IN
THE
BONUS
ROUND
GENERATES
POINTS
FOR
BOTH
PLAYERS
SO
WE
CAN
LEARN
WHETHER
THE
OBJECT
IS
THERE
PLAYERS
CANNOT
PASS
AFTER
THEY
HAVE
CLICKED
ON
THE
IMAGE
THERE
ARE
TWO
REASONS
FOR
THE
PEEKABOOM
BONUS
ROUND
FIRST
BY
GIVING
PLAYERS
BITE
SIZE
MILESTONES
GETTING
FOUR
IMAGES
CORRECTLY
WE
REINFORCE
THEIR
INCREMENTAL
SUCCESS
IN
THE
GAME
AND
THUS
ENCOURAGE
THEM
TO
CONTINUE
PLAYING
SECOND
THE
BONUS
ROUND
IS
AN
ALTERNATIVE
APPROACH
TO
COLLECTING
TRAINING
DATA
FOR
COMPUTER
VISION
IN
IT
PLAYERS
CLICK
INSIDE
SPECIFIC
OBJECTS
WITHIN
AN
IMAGE
SUCH
CLICKS
GIVE
ADDITIONAL
INFORMATION
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
IN
THIS
PAPER
WE
DO
NOT
CONCERN
OURSELVES
WITH
SUCH
INFORMATION
BUT
REMARK
THAT
IT
IS
ALSO
USEFUL
COLLECTING
IMAGE
METADATA
OUR
GOAL
IS
TO
CONSTRUCT
A
DATABASE
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
HERE
WE
DISCUSS
EXACTLY
WHAT
INFORMATION
IS
COLLECTED
BY
PEEKABOOM
AND
HOW
IT
IS
COLLECTED
ON
INPUT
AN
IMAGE
WORD
PAIR
COMING
DIRECTLY
FROM
THE
ESP
GAME
PEEKABOOM
COLLECTS
THE
FOLLOWING
INFORMATION
HOW
THE
WORD
RELATES
TO
THE
IMAGE
IS
IT
AN
OBJECT
PERSON
OR
ANIMAL
IN
THE
IMAGE
IS
IT
TEXT
IN
THE
IMAGE
IS
IT
A
VERB
DESCRIBING
AN
ACTION
IN
THE
IMAGE
IS
IT
AN
OBJECT
PERSON
OR
ANIMAL
NOT
IN
THE
IMAGE
BUT
RELATED
TO
IT
THE
ESP
GAME
ASSOCIATES
WORDS
TO
IMAGES
BUT
DOES
NOT
SAY
HOW
THE
WORD
IS
RELATED
TO
THE
IMAGE
FIGURE
FOR
INSTANCE
SHOWS
MULTIPLE
WAYS
IN
WHICH
A
WORD
CAN
BE
RELATED
TO
AN
IMAGE
HINT
BUTTONS
IN
PEEKABOOM
ALLOW
US
TO
DETERMINE
THE
RELATION
OF
THE
WORD
TO
THE
IMAGE
THIS
IS
USEFUL
IN
MULTIPLE
WAYS
BUT
FOR
THE
PURPOSES
OF
CONSTRUCTING
TRAINING
SETS
FOR
COMPUTER
VISION
IT
ALLOWS
US
TO
WEED
OUT
RELATED
NOUNS
AND
TO
TREAT
TEXT
SEPARATELY
PIXELS
NECESSARY
TO
GUESS
THE
WORD
WHEN
PEEK
ENTERS
THE
CORRECT
WORD
THE
AREA
THAT
BOOM
HAS
REVEALED
IS
PRECISELY
ENOUGH
TO
GUESS
THE
WORD
THAT
IS
WE
CAN
LEARN
EXACTLY
WHAT
CONTEXT
IS
NECESSARY
TO
DETERMINE
WHAT
THE
WORD
REFERS
TO
THIS
CONTEXT
INFORMATION
IS
ABSOLUTELY
NECESSARY
WHEN
ATTEMPTING
TO
DETERMINE
WHAT
TYPE
OF
OBJECT
A
SET
OF
PIXELS
CONSTITUTES
SEE
FIGURE
THE
PIXELS
INSIDE
THE
OBJECT
ANIMAL
OR
PERSON
IF
THE
WORD
IS
A
NOUN
DIRECTLY
REFERRING
TO
SOMETHING
IN
THE
IMAGE
PINGS
GIVE
US
PIXELS
THAT
ARE
INSIDE
THE
OBJECT
PERSON
OR
ANIMAL
FIGURE
THE
IMAGE
ON
THE
LEFT
CONTAINS
A
CAR
DRIVING
THROUGH
THE
STREET
WHILE
THE
ONE
ON
THE
RIGHT
HAS
A
PERSON
CROSSING
THE
SAME
STREET
BOTH
THE
CAR
AND
THE
PERSON
ARE
EXACTLY
THE
SAME
SET
OF
PIXELS
UP
TO
A
ROTATION
BY
DEGREES
EXAMPLE
TAKEN
FROM
THE
MOST
SALIENT
ASPECTS
OF
THE
OBJECTS
IN
THE
IMAGE
BY
INSPECTING
THE
SEQUENCE
OF
BOOM
CLICKS
WE
GAIN
INFORMATION
ABOUT
WHAT
PARTS
OF
THE
IMAGE
ARE
SALIENT
WITH
RESPECT
TO
THE
WORD
BOOM
TYPICALLY
REVEALS
THE
MOST
SALIENT
PARTS
OF
THE
IMAGE
FIRST
E
G
FACE
OF
A
DOG
INSTEAD
OF
THE
LEGS
ETC
ELIMINATION
OF
POOR
IMAGE
WORD
PAIRS
IF
MANY
INDEPENDENT
PAIRS
OF
PLAYERS
AGREE
TO
PASS
ON
AN
IMAGE
WITHOUT
TAKING
ACTION
ON
IT
THEN
LIKELY
THEY
FOUND
IT
IMPOSSIBLY
HARD
BECAUSE
OF
POOR
PICTURE
QUALITY
OR
A
DUBIOUS
RELATION
BETWEEN
THE
IMAGE
AND
ITS
LABEL
BY
IMPLEMENTING
AN
EVICTION
POLICY
FOR
IMAGES
THAT
WE
DISCOVER
ARE
BAD
WE
CAN
IMPROVE
THE
QUALITY
OF
THE
DATA
COLLECTED
AS
WELL
AS
THE
FUN
LEVEL
OF
THE
GAME
WHEN
MULTIPLE
PLAYERS
HAVE
GONE
THROUGH
THE
SAME
IMAGE
THESE
PIECES
OF
INFORMATION
CAN
BE
COMBINED
INTELLIGENTLY
TO
GIVE
EXTREMELY
ACCURATE
AND
USEFUL
ANNOTATIONS
FOR
COMPUTER
VISION
LATER
IN
THE
PAPER
FOR
EXAMPLE
WE
SHOW
HOW
A
SIMPLE
ALGORITHM
CAN
USE
THE
DATA
PRODUCED
BY
PEEKABOOM
TO
CALCULATE
ACCURATE
OBJECT
BOUNDING
BOXES
SEE
FIGURE
THE
SINGLE
PLAYER
GAME
PEEKABOOM
IS
A
TWO
PLAYER
GAME
OFTENTIMES
HOWEVER
THERE
WILL
BE
AN
ODD
NUMBER
OF
PEOPLE
ATTEMPTING
TO
PLAY
THE
GAME
SO
THE
REMAINING
PERSON
CANNOT
BE
PAIRED
TO
PREVENT
THEIR
FRUSTRATION
WE
ALSO
HAVE
A
SINGLE
PLAYER
VERSION
OF
THE
GAME
IN
WHICH
THE
PLAYER
IS
MATCHED
WITH
A
SERVER
SIDE
BOT
OUR
BOT
ACTS
INTELLIGENTLY
TO
SIMULATE
A
HUMAN
PLAYER
BY
BEING
BASED
ON
PRE
RECORDED
GAMES
IN
OTHER
WORDS
WE
TAKE
DATA
COLLECTED
FROM
PAIRS
OF
HUMANS
AND
USE
IT
AS
THE
BASIS
FOR
THE
COMPUTER
PLAYER
LOGIC
EMULATING
A
BOOM
PLAYER
IS
FAIRLY
SIMPLE
THE
BOT
CAN
REGURGITATE
THE
SEQUENCE
OF
RECORDED
CLICKS
TO
THE
HUMAN
EMULATING
PEEK
IS
MUCH
MORE
COMPLICATED
THE
BOT
NEEDS
TO
HAVE
SOME
CONCEPT
OF
CLOSENESS
OF
THE
HUMAN
CLICKS
TO
THE
SET
OF
RECORDED
CLICKS
FOR
INSTANCE
IF
THE
HUMAN
DOES
NOT
REVEAL
THE
DOG
IN
THE
PICTURE
THE
BOT
SHOULD
NOT
GUESS
DOG
OUR
BOT
ONLY
REVEALS
A
CERTAIN
PRE
RECORDED
GUESS
IF
ENOUGH
AREA
HAS
BEEN
REVEALED
TOWARDS
THIS
END
IT
EMPLOYS
A
SPATIAL
DATA
STRUCTURE
WHOSE
MEMBERS
ARE
CIRCLES
EACH
OF
WHICH
CORRESPONDS
TO
A
CLICK
ELEMENTS
OF
THE
DATA
STRUCTURE
ARE
REMOVED
AS
THEY
ARE
CLICKED
ON
BY
THE
HUMAN
PLAYER
WHEN
THE
DATA
STRUCTURE
BECOMES
EMPTY
THE
BOT
GIVES
THE
CORRECT
ANSWER
MOREOVER
IT
HAS
THE
ABILITY
TO
MAKE
INCORRECT
GUESSES
ALONG
THE
WAY
BASED
ON
THE
RELATIVE
EMPTINESS
OF
THE
SPATIAL
DATA
STRUCTURE
CHEATING
PEEKABOOM
IS
A
COLLABORATIVE
GAME
PARTNERS
WORK
TOGETHER
TO
MAXIMIZE
THEIR
SCORE
WHEN
BOTH
PARTNERS
DO
NOT
COMMUNICATE
OUTSIDE
THE
GAME
ENVIRONMENT
WE
OBTAIN
CORRECT
INFORMATION
HOWEVER
IF
THE
TWO
PARTNERS
COLLUDE
TO
CHEAT
ON
THE
GAME
THE
DATA
COULD
BE
POISONED
FOR
INSTANCE
IF
BOOM
AND
PEEK
KNOW
EACH
OTHER
AND
HAVE
AN
OUTSIDE
MEANS
OF
COMMUNICATION
THEN
BOOM
CAN
SIMPLY
TELL
PEEK
WHAT
WORDS
TO
TYPE
PEEKABOOM
CONTAINS
MULTIPLE
ANTI
CHEATING
MECHANISMS
THROUGH
A
COMBINATION
OF
ONLINE
IN
GAME
ENFORCEMENT
AND
OFFLINE
ANALYSIS
WE
ARE
ABLE
TO
DETECT
AND
DEAL
WITH
CHEATING
BEFORE
DETAILING
PEEKABOOM
ANTI
CHEATING
MEASURES
WE
MENTION
THAT
CHEATING
ATTEMPTS
ARE
UNCOMMON
ALTHOUGH
A
MINORITY
OF
PLAYERS
MIGHT
OBTAIN
SATISFACTION
FROM
GAMING
THE
SYSTEM
THE
MAJORITY
OF
THEM
JUST
WANT
TO
PLAY
THE
GAME
HONESTLY
INDEED
AS
ANECDOTAL
EVIDENCE
WHEN
PEEKABOOM
WAS
TESTED
IN
A
ROOM
WITH
CHILDREN
OF
AGES
THEY
WOULD
COVER
THE
WORD
WITH
THEIR
HAND
TO
PREVENT
OTHERS
IN
THE
ROOM
FROM
SEEING
THE
ANSWERS
NEVERTHELESS
PEEKABOOM
DOES
HAVE
A
FULL
SET
OF
MEASURES
TO
PREVENT
COLLUSION
THE
PLAYER
QUEUE
WHEN
PLAYERS
LOG
ON
TO
THE
GAME
SERVER
THEY
ARE
NOT
IMMEDIATELY
PAIRED
OFF
INSTEAD
THE
SERVER
MAKES
THEM
WAIT
N
SECONDS
WHERE
N
IS
THE
NUMBER
OF
SECONDS
UNTIL
THE
NEXT
MATCHING
INTERVAL
CURRENTLY
MATCHING
INTERVALS
HAPPEN
EVERY
SECONDS
AND
WHEN
THEY
DO
THE
SERVER
MATCHES
EVERYONE
IN
THE
QUEUE
WITH
A
PARTNER
ANY
ODD
PERSON
OUT
WILL
BE
PAIRED
WITH
A
BOT
WITH
A
LARGE
NUMBER
OF
PLAYERS
IN
THE
SYSTEM
WE
CAN
ENSURE
THAT
A
PLAYER
PARTNER
IS
RANDOM
AND
PREVENT
COLLUDERS
FROM
GETTING
MATCHED
JUST
BECAUSE
THEY
CLICKED
START
PLAYING
AT
THE
SAME
TIME
IP
ADDRESS
CHECKS
WE
ALSO
CHECK
PLAYER
IP
ADDRESSES
TO
ENSURE
THAT
THEY
ARE
NOT
PAIRED
WITH
THEMSELVES
OR
WITH
OTHERS
THAT
HAVE
A
SIMILAR
ADDRESS
SIMILARITY
IN
IP
ADDRESSES
CAN
IMPLY
GEOGRAPHICAL
PROXIMITY
SEED
IMAGES
BECAUSE
OUR
SYSTEM
IS
A
WEB
BASED
GAME
ONE
POINT
OF
CONCERN
IS
THAT
BOTS
I
E
AUTOMATED
PLAYERS
MIGHT
PLAY
THE
GAME
AND
POLLUTE
THE
POOL
OF
COLLECTED
DATA
TO
DETECT
THEM
WE
INTRODUCE
SEED
IMAGES
INTO
THE
SYSTEM
IN
OTHER
WORDS
THOSE
FOR
WHICH
WE
HAVE
HAND
VERIFIED
METADATA
ON
BEING
PRESENTED
SEED
IMAGES
IF
A
PLAYER
CONSISTENTLY
FAILS
TO
CLICK
ON
THE
RELEVANT
PARTS
WHEN
PLAYING
BOOM
OR
TO
GUESS
THE
CORRECT
WORDS
WHEN
PLAYING
PEEK
THEY
WILL
BE
ADDED
TO
A
BLACKLIST
WE
DISCARD
ALL
CURRENT
AND
FUTURE
GAME
PLAY
DATA
ASSOCIATED
WITH
ANYONE
ON
THE
BLACKLIST
NOTICE
THAT
ALMOST
BY
DEFINITION
A
COMPUTER
PROGRAM
CANNOT
SUCCESSFULLY
PLAY
PEEKABOOM
IF
IT
WERE
ABLE
TO
DO
SO
THEN
IT
WOULD
BE
ABLE
TO
RECOGNIZE
THE
OBJECTS
IN
THE
IMAGES
THEREFORE
THIS
STRATEGY
PREVENTS
BOTS
AS
WELL
AS
OTHERWISE
MALICIOUS
PLAYERS
FROM
POISONING
OUR
DATA
LIMITED
FREEDOM
TO
ENTER
GUESSES
SINCE
BOOM
CAN
SEE
ALL
OF
PEEK
GUESSES
THE
GAME
ALLOWS
A
LIMITED
FORM
OF
COMMUNICATION
BETWEEN
THE
PLAYERS
INDEED
MANY
OF
THE
PEEKABOOM
PLAYERS
USE
THE
GUESS
FIELD
AS
A
WAY
TO
COMMUNICATE
WITH
THEIR
PARTNER
IT
IS
NOT
UNCOMMON
FOR
THE
FIRST
GUESS
IN
A
GAME
TO
BE
HI
OR
FOR
THE
FIRST
GUESS
AFTER
PASSING
ON
AN
IMAGE
TO
BE
THE
CORRECT
WORD
ASSOCIATED
WITH
THE
PREVIOUS
IMAGE
IT
IS
ALSO
NOT
UNCOMMON
FOR
PLAYERS
TO
TYPE
SORRY
AFTER
TAKING
TOO
LONG
ON
AN
IMAGE
A
POSSIBLE
CHEATING
STRATEGY
IS
TO
EXCHANGE
IM
SCREEN
NAMES
THROUGH
THE
GUESS
FIELD
AND
THEN
USING
IM
COMMUNICATE
THE
CORRECT
WORDS
ALTHOUGH
WE
HAVE
NEVER
OBSERVED
ATTEMPTS
TO
EXECUTE
SUCH
A
STRATEGY
WE
CAN
MITIGATE
IT
BY
NOT
ALLOWING
PEEK
TO
ENTER
ANY
NON
ALPHABETICAL
CHARACTERS
SUCH
AS
NUMBERS
SIMILARLY
WE
CAN
PREVENT
BOOM
FROM
SEEING
ANY
GUESSES
THAT
ARE
NOT
WORDS
IN
THE
DICTIONARY
CURRENTLY
WE
DO
ALLOW
BOOM
TO
SEE
SUCH
GUESSES
BECAUSE
WE
HAVE
NOT
SEEN
PLAYERS
ATTEMPT
TO
CHEAT
IN
THIS
WAY
HOWEVER
EVEN
IF
PLAYERS
ARE
SUCCESSFUL
IN
SUCH
A
STRATEGY
THE
OTHER
ANTI
COLLUSION
MECHANISMS
CAN
DEAL
WITH
THE
CORRUPTED
DATA
AGGREGATING
DATA
FROM
MULTIPLE
PLAYERS
IN
ADDITION
TO
THE
ABOVE
STRATEGIES
WE
AGGREGATE
DATA
FROM
MULTIPLE
PLAYERS
FOR
A
GIVEN
IMAGE
WORD
PAIR
BY
DOING
THIS
WE
CAN
ELIMINATE
OUTLIERS
IMPLEMENTATION
WE
IMPLEMENTED
THE
ARCHITECTURE
OF
THE
GAME
UNDER
THE
CLIENT
SERVER
MODEL
THE
CLIENT
APPLICATION
IS
DELIVERED
AS
A
JAVA
APPLET
WHILE
THE
SERVER
IS
WRITTEN
PURELY
IN
JAVA
APPLETS
CONNECT
TO
A
SERVER
WHICH
THEN
MATCHES
THE
PLAYERS
WITH
GAMES
OF
PEEKABOOM
UPON
TWO
PLAYERS
COMPLETION
OF
A
MATCH
THE
SERVER
WRITES
THEIR
GAME
PLAY
DATA
AND
SCORES
TO
DISK
WE
THEN
COMPILE
THE
COLLECTED
DATA
INTO
DESIRED
FORMATS
OUR
IMPLEMENTATION
OF
THE
GAME
CONTAINS
MANY
FEATURES
TO
IMPROVE
GAME
PLAY
SPELLING
CHECK
INCORRECTLY
SPELLED
WORDS
ARE
DISPLAYED
IN
A
DIFFERENT
COLOR
TO
NOTIFY
PLAYERS
THIS
IS
IMPORTANT
BECAUSE
THE
PEEK
PLAYER
USUALLY
TYPES
MULTIPLE
GUESSES
IN
A
SHORT
TIME
OFTEN
MAKING
SPELLING
MISTAKES
INAPPROPRIATE
WORD
REPLACEMENT
SINCE
BOOM
CAN
SEE
PEEK
GUESSES
WE
DO
NOT
ALLOW
PEEK
TO
ENTER
INAPPROPRIATE
WORDS
WHENEVER
ONE
OF
PEEK
GUESSES
IS
AMONG
A
LIST
OF
POSSIBLE
INAPPROPRIATE
WORDS
WE
SUBSTITUTE
IT
WITH
ANOTHER
WORD
CHOSEN
FROM
A
LIST
OF
INNOCENT
WORDS
SUCH
AS
LOVE
CARING
ILUVPEEKABOOM
ETC
TOP
SCORES
LIST
AND
RANKS
THE
PEEKABOOM
WEBSITE
PROMINENTLY
DISPLAYS
THE
CUMULATIVE
TOP
SCORES
OF
THE
DAY
AS
WELL
AS
THE
TOP
SCORES
OF
ALL
TIME
FURTHERMORE
PLAYERS
ARE
GIVEN
A
RANK
BASED
ON
THE
TOTAL
NUMBER
OF
POINTS
THEY
HAVE
ACCUMULATED
THROUGHOUT
TIME
SEE
FIGURE
THE
DIFFERENT
RANKS
ARE
FRESH
MEAT
POINTS
NEWBIE
POINTS
PLAYER
POINTS
GANGSTER
POINTS
AND
GODFATHER
OR
MORE
POINTS
WE
REMARK
THAT
RANKS
HAVE
PROVEN
AN
IMPORTANT
COMPONENT
OF
PEEKABOOM
INCENTIVE
STRATEGY
OF
THE
PLAYERS
THAT
HAVE
OBTAINED
AN
ACCOUNT
OF
THEM
HAVE
SCORES
THAT
FALL
WITHIN
POINTS
OF
THE
RANK
CUTOFFS
GIVEN
THAT
THESE
INTERVALS
COVER
LESS
THAN
OF
THE
SPACE
OF
POSSIBLE
CUMULATIVE
SCORES
THIS
STRONGLY
SUGGESTS
THAT
MANY
PLAYERS
SIMPLY
PLAY
TO
REACH
A
NEW
RANK
FIGURE
TOP
SCORES
AND
PLAYER
RANKS
PLAYERS
ARE
SHOWN
THEIR
CURRENT
RANK
AND
THE
NUMBER
OF
POINTS
REMAINING
FOR
THE
NEXT
RANK
ADDITIONAL
APPLICATIONS
BEFORE
GOING
TO
THE
EVALUATION
SECTION
WE
MENTION
TWO
ADDITIONAL
APPLICATIONS
FOR
THE
DATA
COLLECTED
BY
PEEKABOOM
A
BENEFIT
OF
THESE
APPLICATIONS
IS
THAT
THEY
ARE
DIRECT
IN
THAT
THEY
DO
NOT
REQUIRE
THE
TRAINING
OF
MACHINE
LEARNING
ALGORITHMS
IMPROVING
IMAGE
SEARCH
RESULTS
PEEKABOOM
GIVES
AN
ACCURATE
ESTIMATE
OF
THE
FRACTION
OF
THE
IMAGE
RELATED
TO
THE
WORD
IN
QUESTION
THIS
ESTIMATE
CAN
BE
CALCULATED
FROM
THE
AREA
REVEALED
BY
BOOM
THE
FRACTION
OF
THE
IMAGE
RELATED
TO
A
WORD
CAN
BE
USED
TO
ORDER
IMAGE
SEARCH
RESULTS
IMAGES
IN
WHICH
THE
WORD
REFERS
TO
A
HIGHER
FRACTION
OF
THE
TOTAL
PIXELS
SHOULD
BE
RANKED
HIGHER
MUCH
LIKE
THE
GOAL
OF
THE
ESP
GAME
IS
TO
LABEL
ALL
IMAGES
ON
THE
WEB
WE
CAN
IMAGINE
PEEKABOOM
DOING
THE
SAME
AND
THUS
FURTHER
IMPROVING
IMAGE
SEARCH
OBJECT
BOUNDING
BOXES
IN
THE
SAME
VEIN
PEEKABOOM
CAN
BE
USED
TO
DIRECTLY
CALCULATE
OBJECT
BOUNDING
BOXES
SIMILAR
TO
THOSE
USED
IN
FLICKR
SEE
FIGURE
FLICKR
IS
A
PHOTO
SHARING
SERVICE
THAT
ALLOWS
USERS
TO
TAG
IMAGES
WITH
KEYWORDS
AND
TO
ASSOCIATE
KEYWORDS
WITH
RECTANGULAR
AREAS
IN
THE
IMAGE
THE
AREAS
AND
TAGS
HOWEVER
ARE
NOT
GUARANTEED
TO
BE
CORRECT
SINCE
A
USER
CAN
ENTER
ANYTHING
THEY
WISH
FOR
THEIR
OWN
IMAGES
TO
EXHIBIT
THE
POWER
OF
THE
DATA
COLLECTED
BY
PEEKABOOM
WE
SHOW
HOW
TO
USE
IT
CALCULATE
SUCH
RECTANGLES
WE
EMPHASIZE
HOWEVER
THAT
THE
DATA
COLLECTED
BY
PEEKABOOM
IS
SIGNIFICANTLY
RICHER
AND
THAT
TO
CALCULATE
THE
RECTANGLES
WE
DISCARD
VAST
AMOUNTS
OF
THE
INFORMATION
COLLECTED
BY
OUR
GAME
SINCE
PEEKABOOM
ANNOTATES
ARBITRARY
IMAGES
ON
THE
WEB
ITS
DATA
ALLOWS
FOR
AN
IMAGE
SEARCH
ENGINE
IN
WHICH
THE
RESULTS
ARE
HIGHLIGHTED
SIMILAR
TO
THE
HIGHLIGHTED
WORDS
IN
GOOGLE
TEXT
SEARCH
RESULTS
USING
THE
DATA
OBTAINED
IN
THE
FIRST
TWO
WEEKS
OF
GAME
PLAY
WE
HAVE
IMPLEMENTED
A
PROTOTYPE
OF
SUCH
A
SEARCH
ENGINE
SEE
FIGURE
THE
SEARCH
ENGINE
CAN
BE
ACCESSED
FROM
THE
PEEKABOOM
WEBSITE
THE
BOUNDING
BOXES
WERE
CALCULATED
AS
FOLLOWS
FOR
A
SINGLE
PLAY
OF
AN
IMAGE
WORD
PAIR
WE
CREATE
A
MATRIX
OF
AND
THE
DIMENSIONS
OF
THE
MATRIX
ARE
THE
SAME
AS
THE
DIMENSIONS
OF
THE
IMAGE
IN
PIXELS
AT
FIRST
EVERY
ENTRY
IN
THE
MATRIX
IS
A
WE
ADD
A
IN
EVERY
PIXEL
CLICKED
BY
BOOM
AS
WELL
AS
IN
THE
CIRCLE
OF
RADIUS
PIXELS
AROUND
THE
CLICK
WE
THUS
OBTAIN
A
MATRIX
OF
AND
CORRESPONDING
TO
THE
EXACT
AREA
THAT
WAS
REVEALED
IN
A
SINGLE
GAME
PLAY
NEXT
WE
COMBINE
DIFFERENT
PLAYS
OF
THE
SAME
IMAGE
WORD
PAIR
BY
ADDING
THEIR
CORRESPONDING
MATRICES
THIS
GIVES
A
MATRIX
WHOSE
ENTRIES
ARE
INTEGERS
CORRESPONDING
TO
THE
NUMBER
OF
DIFFERENT
PLAYERS
THAT
REVEALED
EACH
PIXEL
OF
THE
IMAGE
ON
THIS
COMBINED
MATRIX
WE
APPLY
A
THRESHOLD
OF
MEANING
THAT
WE
SUBSTITUTE
EVERY
VALUE
LESS
THAN
WITH
AND
EVERY
VALUE
GREATER
THAN
WITH
THIS
GIVES
A
MATRIX
CORRESPONDING
TO
ALL
THE
PIXELS
THAT
HAVE
BEEN
REVEALED
BY
AT
LEAST
PLAYERS
NEXT
WE
CLUSTER
THESE
PIXELS
AND
CALCULATE
FIGURE
OBJECT
BOUNDING
BOXES
OBTAINED
FROM
PEEKABOOM
DATA
THE
BOUNDING
BOXES
BY
TAKING
FOR
EACH
CLUSTER
THE
LEFTMOST
RIGHTMOST
TOPMOST
AND
BOTTOMMOST
POINTS
THIS
ALGORITHM
MAY
PRODUCE
MULTIPLE
BOUNDING
BOXES
FOR
A
SINGLE
IMAGE
WORD
PAIR
FOR
INSTANCE
IN
FIGURE
WE
CAN
SEE
THAT
MANY
OF
THE
RESULTS
FOR
EYES
HAVE
TWO
BOUNDING
BOXES
ONE
CORRESPONDING
TO
EACH
EYE
AS
WE
WILL
SEE
THE
RESULTS
PRODUCED
BY
THIS
SIMPLISTIC
ALGORITHM
ARE
EXTREMELY
ACCURATE
SUCH
RESULTS
COULD
BE
IMPROVED
BY
MAKING
INTELLIGENT
USE
OF
THE
ADDITIONAL
DATA
GIVEN
BY
PEEKABOOM
SUCH
AS
PINGS
THE
PRECISE
ORDER
OF
THE
AREAS
REVEALED
ETC
BUT
FOR
THE
PURPOSES
OF
THIS
PAPER
WE
USE
THE
SIMPLISTIC
ALGORITHM
ALTERNATIVE
USING
PING
DATA
FOR
POINTING
INSTEAD
OF
SHOWING
BOUNDING
BOXES
CALCULATED
FROM
REVEALED
AREAS
WE
COULD
SHOW
ARROWS
OR
LINES
POINTING
TO
THE
OBJECTS
SEE
FIGURE
SUCH
POINTERS
CAN
BE
EASILY
CALCULATED
FROM
THE
PING
DATA
THE
SIMPLEST
ALGORITHM
FOR
DOING
SO
IS
TO
SELECT
A
PING
AT
RANDOM
AND
ASSUME
IT
IS
A
GOOD
POINTER
FOR
THE
OBJECT
WE
WILL
SHOW
THAT
THIS
SIMPLISTIC
ALGORITHM
GIVES
VERY
ACCURATE
RESULTS
FIGURE
SHOWS
AN
IMAGE
IN
WHICH
THE
DIFFERENT
OBJECTS
HAVE
BEEN
LOCATED
USING
PING
DATA
MORE
ELABORATE
ALGORITHMS
COULD
GIVE
EVEN
BETTER
RESULTS
WE
REMARK
HOWEVER
THAT
SIMPLY
AVERAGING
THE
PINGS
OVER
MULTIPLE
PLAYERS
TO
OBTAIN
A
SINGLE
POINTER
DOES
NOT
GIVE
ACCURATE
RESULTS
FOR
INSTANCE
IF
THE
OBJECT
WAS
EYES
AVERAGING
THE
PINGS
GIVES
A
POINTER
TO
A
REGION
THAT
IS
NOT
AN
EYE
FIGURE
CALCULATION
OF
OBJECT
POINTERS
USING
PINGS
EVALUATION
USER
STATISTICS
THE
EVALUATION
OF
OUR
CLAIMS
CONSISTS
OF
TWO
PARTS
FIRST
WE
MUST
SHOW
THAT
THE
GAME
IS
INDEED
ENJOYABLE
SECOND
WE
MUST
SHOW
THAT
THE
DATA
PRODUCED
BY
THE
GAME
IS
ACCURATE
IT
IS
DIFFICULT
TO
EVALUATE
HOW
ENJOYABLE
A
GAME
REALLY
IS
ONE
APPROACH
IS
TO
ASK
PARTICIPANTS
A
SERIES
OF
QUESTIONS
REGARDING
HOW
MUCH
THEY
ENJOYED
PLAYING
THE
GAME
OUR
DATA
FOR
SUCH
AN
APPROACH
WERE
EXTREMELY
POSITIVE
BUT
WE
FOLLOW
A
DIFFERENT
APPROACH
IN
THIS
PAPER
WE
PRESENT
USAGE
STATISTICS
FROM
ARBITRARY
PEOPLE
PLAYING
OUR
GAME
ONLINE
THIS
SAME
APPROACH
WAS
USED
BY
THE
ESP
GAME
USAGE
STATISTICS
PEEKABOOM
WAS
RELEASED
TO
A
GENERAL
AUDIENCE
ON
AUGUST
OF
WE
PRESENT
THE
USAGE
STATISTICS
FROM
THE
PERIOD
STARTING
AUGUST
AND
ENDING
SEPTEMBER
A
TOTAL
OF
DIFFERENT
PEOPLE
PLAYED
THE
GAME
DURING
THIS
TIME
GENERATING
PIECES
OF
DATA
BY
DIFFERENT
PEOPLE
WE
MEAN
DIFFERENT
USER
IDS
BY
A
PIECE
OF
DATA
WE
MEAN
A
SUCCESSFUL
ROUND
OF
PEEKABOOM
IN
WHICH
PEEK
CORRECTLY
GUESSED
THE
WORD
GIVEN
BOOM
REVEALED
REGION
WE
MENTION
THAT
AN
IMAGE
WORD
PAIR
CAN
HAVE
MULTIPLE
PIECES
OF
DATA
ASSOCIATED
TO
IT
IF
IT
OCCURS
IN
MULTIPLE
GAMES
IF
PEOPLE
GAVE
US
PIECES
OF
DATA
THEN
ON
AVERAGE
EACH
PERSON
PLAYED
ON
IMAGES
SINCE
EACH
SESSION
OF
THE
GAME
LASTS
MINUTES
AND
ON
AVERAGE
PLAYERS
GO
THROUGH
IMAGES
DURING
A
SESSION
IN
THIS
ONE
MONTH
PERIOD
EACH
PERSON
PLAYED
ON
AVERAGE
MINUTES
WITHOUT
COUNTING
TIME
SPENT
WAITING
FOR
A
PARTNER
ETC
OVER
OF
THE
PEOPLE
PLAYED
ON
MORE
THAN
ONE
OCCASION
THAT
IS
MORE
THAN
OF
THE
PEOPLE
PLAYED
ON
DIFFERENT
DATES
FURTHERMORE
EVERY
PLAYER
IN
THE
TOP
SCORES
LIST
PLAYED
OVER
GAMES
THAT
OVER
HOURS
WITHOUT
INCLUDING
THE
TIME
THEY
SPENT
WAITING
FOR
A
PARTNER
THIS
UNDOUBTEDLY
ATTESTS
TO
HOW
ENJOYABLE
THE
GAME
IS
USER
COMMENTS
TO
GIVE
A
FURTHER
SENSE
FOR
HOW
MUCH
THE
PLAYERS
ENJOYED
THE
GAME
WE
INCLUDE
BELOW
SOME
QUOTES
TAKEN
FROM
COMMENTS
SUBMITTED
BY
PLAYERS
USING
A
LINK
ON
THE
WEBSITE
THE
GAME
ITSELF
IS
EXTREMELY
ADDICTIVE
AS
THERE
IS
AN
ELEMENT
OF
PRESSURE
INVOLVED
IN
BEATING
THE
CLOCK
A
DRIVE
TO
SCORE
MORE
POINTS
THE
FEELING
THAT
YOU
COULD
ALWAYS
DO
BETTER
NEXT
TIME
AND
A
CURIOSITY
ABOUT
WHAT
IS
GOING
TO
COME
UP
NEXT
I
WOULD
SAY
THAT
IT
GIVES
THE
SAME
GUT
FEELING
AS
COMBINING
GAMBLING
WITH
CHARADES
WHILE
RIDING
ON
A
ROLLER
COASTER
THE
GOOD
POINTS
ARE
THAT
YOU
INCREASE
AND
STIMULATE
YOUR
INTELLIGENCE
YOU
DON
T
LOSE
ALL
YOUR
MONEY
AND
YOU
DON
T
FALL
OFF
THE
RIDE
THE
BAD
POINT
IS
THAT
YOU
LOOK
AT
YOUR
WATCH
AND
EIGHT
HOURS
HAVE
JUST
DISAPPEARED
ONE
UNFORTUNATE
SIDE
EFFECT
OF
PLAYING
SO
MUCH
IN
SUCH
A
SHORT
TIME
WAS
A
MILD
CASE
OF
CARPAL
TUNNEL
SYNDROME
IN
MY
RIGHT
HAND
AND
FOREARM
BUT
THAT
DISSIPATED
QUICKLY
THIS
GAME
IS
LIKE
CRACK
I
VE
BEEN
PEEKABOOM
FREE
FOR
HOURS
UNLIKE
OTHER
GAMES
PEEKABOOM
IS
COOPERATIVE
RATHER
THAN
COMPETITIVE
EVALUATION
ACCURACY
OF
COLLECTED
DATA
THE
USEFULNESS
OF
PEEKABOOM
AS
A
DATA
COLLECTION
METHOD
RESTS
IN
THE
QUALITY
OF
THE
DATA
WE
COLLECT
ALTHOUGH
THE
DESIGN
OF
THE
GAME
INHERENTLY
ENSURES
CORRECTNESS
OF
THE
DATA
WE
WANTED
TO
TEST
WHETHER
IT
IS
AS
GOOD
AS
WHAT
WOULD
BE
COLLECTED
DIRECTLY
FROM
VOLUNTEERS
IN
A
NON
GAME
SETTING
TO
DO
SO
WE
CONDUCTED
TWO
EXPERIMENTS
TO
TEST
FIRST
THE
ACCURACY
OF
THE
BOUNDING
BOXES
WE
DEFINED
AND
SECOND
THE
UTILITY
OF
THE
POINTING
BEHAVIOR
IN
THE
GAME
NOTICE
THAT
THESE
EXPERIMENTS
ARE
MEANT
TO
ANALYZE
THE
CORRECTNESS
OF
THE
DATA
AND
NOT
WHETHER
SUCH
DATA
CAN
BE
USED
TO
TRAIN
COMPUTER
VISION
ALGORITHMS
THE
USEFULNESS
OF
DATA
ABOUT
LOCATION
OF
OBJECTS
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
HAS
BEEN
PREVIOUSLY
ESTABLISHED
EXPERIMENT
ACCURACY
OF
BOUNDING
BOXES
IN
THE
FIRST
EXPERIMENT
WE
TESTED
WHETHER
THE
BOUNDING
BOXES
FOR
OBJECTS
WITHIN
AN
IMAGE
THAT
ARE
CALCULATED
FROM
PEEKABOOM
ARE
AS
GOOD
AS
BOUNDING
BOXES
PEOPLE
WOULD
MAKE
AROUND
AN
OBJECT
IN
A
NON
GAME
SETTING
WE
SELECTED
AT
RANDOM
IMAGE
WORD
PAIRS
FROM
THE
DATA
POOL
THAT
HAD
BEEN
SUCCESSFULLY
PLAYED
ON
BY
AT
LEAST
TWO
INDEPENDENT
PAIRS
OF
PEOPLE
THE
IMAGES
SELECTED
ALL
HAD
NOUNS
AS
THEIR
WORD
AS
OPPOSED
TO
TEXT
IN
THE
IMAGE
OR
AN
ADJECTIVE
ETC
SEE
FIGURE
ALL
THE
IMAGES
CHOSEN
HAD
THE
WORD
REFER
TO
A
SINGLE
OBJECT
IN
THE
IMAGE
FOR
EACH
IMAGE
PEEKABOOM
DATA
WAS
USED
TO
CALCULATE
OBJECT
BOUNDING
BOXES
USING
THE
METHOD
EXPLAINED
IN
PREVIOUS
SECTIONS
WE
THEN
HAD
FOUR
VOLUNTEERS
MAKE
BOUNDING
BOXES
AROUND
THE
OBJECTS
FOR
EACH
IMAGE
PROVIDING
US
WITH
BOUNDING
BOXES
DRAWN
BY
VOLUNTEERS
THE
VOLUNTEERS
WERE
ASKED
FOR
EACH
IMAGE
TO
DRAW
A
BOUNDING
BOX
AROUND
THE
OBJECT
THAT
THE
WORD
REFERRED
TO
WE
THEN
SELECTED
AT
RANDOM
ONE
OF
THE
FOUR
VOLUNTEER
BOUNDING
BOXES
FOR
EACH
IMAGE
SO
AS
TO
END
UP
WITH
ONE
VOLUNTEER
GENERATED
BOUNDING
BOX
FOR
EVERY
ONE
OF
THE
IMAGES
FINALLY
WE
TESTED
THE
AMOUNT
OF
OVERLAP
BETWEEN
THE
BOUNDING
BOXES
GENERATED
BY
PEEKABOOM
AND
THOSE
GENERATED
BY
OUR
VOLUNTEERS
THE
AMOUNT
OF
OVERLAP
WAS
DETERMINED
USING
THE
FORMULA
OVERLAP
A
B
AREA
A
B
AREA
A
B
WHERE
A
AND
B
ARE
THE
BOUNDING
BOXES
NOTICE
THAT
IF
A
B
THEN
OVERLAP
A
B
AND
IF
A
IS
DISJOINT
FROM
B
THEN
OVERLAP
A
B
WE
CALCULATED
THE
AVERAGE
OVERLAP
ACROSS
THE
IMAGES
AS
WELL
AS
THE
STANDARD
DEVIATION
RESULTS
ON
AVERAGE
THE
OVERLAP
BETWEEN
THE
PEEKABOOM
BOUNDING
BOXES
AND
THE
VOLUNTEER
GENERATED
ONES
WAS
WITH
STANDARD
DEVIATION
THIS
MEANS
THAT
THE
PEEKABOOM
BOUNDING
BOXES
WERE
VERY
CLOSE
TO
THOSE
GENERATED
BY
THE
VOLUNTEERS
TO
ILLUSTRATE
WE
SHOW
IN
FIGURE
THE
BOUNDING
BOX
THAT
OBTAINED
THE
LOWEST
OVERLAP
SCORE
FIGURE
EXPERIMENT
IMAGE
WITH
LOWEST
OVERLAP
BETWEEN
A
VOLUNTEER
GENERATED
BOUNDING
BOX
SOLID
LINES
AND
ONE
GENERATED
BY
PEEKABOOM
DASHED
LINES
GIVEN
THAT
PEEKABOOM
WAS
NOT
DIRECTLY
BUILT
TO
CALCULATE
BOUNDING
BOXES
THIS
SHOWS
THE
WIDE
APPLICABILITY
OF
THE
DATA
COLLECTED
EXPERIMENT
ACCURACY
OF
PINGS
IN
THE
SECOND
EXPERIMENT
WE
TESTED
WHETHER
THE
OBJECT
POINTERS
THAT
ARE
CALCULATED
FROM
PEEKABOOM
ARE
INDEED
INSIDE
THE
OBJECTS
AS
IN
THE
PREVIOUS
EXPERIMENT
WE
SELECTED
AT
RANDOM
IMAGE
LABEL
PAIRS
FROM
THE
DATA
POOL
THAT
HAVE
BEEN
SUCCESSFULLY
PLAYED
ON
BY
AT
LEAST
TWO
INDEPENDENT
PAIRS
OF
PEOPLE
THE
IMAGES
SELECTED
ALL
HAD
THE
WORD
AS
A
NOUN
AS
OPPOSED
TO
AS
TEXT
IN
THE
IMAGE
OR
AN
ADJECTIVE
ETC
SEE
FIGURE
ALL
THE
IMAGES
CHOSEN
HAD
THE
WORD
REFER
TO
A
SINGLE
OBJECT
IN
THE
IMAGE
FOR
EACH
IMAGE
PEEKABOOM
DATA
WAS
USED
TO
CALCULATE
OBJECT
POINTERS
USING
THE
METHOD
EXPLAINED
IN
PREVIOUS
SECTIONS
WE
THEN
ASKED
THREE
VOLUNTEER
RATERS
TO
DETERMINE
FOR
EACH
POINTER
WHETHER
IT
WAS
INSIDE
THE
OBJECT
OR
NOT
THE
RATERS
WERE
SHOWN
EXAMPLES
OF
POINTERS
INSIDE
AND
OUTSIDE
THE
OBJECT
AND
WERE
TOLD
THAT
NEAR
AN
OBJECT
DOES
NOT
COUNT
AS
INSIDE
THE
OBJECT
RESULTS
ACCORDING
TO
ALL
THE
RATERS
OF
THE
POINTERS
WERE
INSIDE
THE
OBJECT
REFERRED
TO
BY
THE
WORD
THIS
GIVES
POSITIVE
EVIDENCE
THAT
PING
DATA
IS
ACCURATE
ESPECIALLY
SINCE
IT
WAS
CALCULATED
USING
SUCH
A
SIMPLISTIC
ALGORITHM
GENERALIZING
OUR
APPROACH
THE
APPROACH
PRESENTED
IN
THIS
PAPER
SOLVING
A
PROBLEM
BY
HAVING
PEOPLE
PLAY
GAMES
ONLINE
CAN
BE
GENERALIZED
TO
MANY
OTHER
PROBLEMS
IN
ARTIFICIAL
INTELLIGENCE
IN
FOLLOW
UP
WORK
FOR
EXAMPLE
WE
HAVE
CREATED
TWO
OTHER
GAMES
VERBOSITY
AND
PHETCH
IN
WHICH
PLAYERS
SOLVE
PROBLEMS
THAT
COMPUTERS
CANNOT
YET
SOLVE
VERBOSITY
COLLECTS
COMMON
SENSE
FACTS
TO
TRAIN
REASONING
ALGORITHMS
FOR
INSTANCE
FOR
THE
WORD
MILK
THE
GAME
OUTPUTS
FACTS
SUCH
AS
IT
IS
WHITE
PEOPLE
USUALLY
EAT
CEREAL
WITH
IT
ETC
VERBOSITY
IS
A
TWO
PLAYER
GAME
IN
WHICH
ONE
PLAYER
ATTEMPTS
TO
MAKE
THE
OTHER
SAY
A
TARGET
WORD
E
G
MILK
WITHOUT
USING
THE
WORD
THEY
DO
SO
BY
SAYING
MANY
FACTS
WITHOUT
USING
THE
WORD
ITSELF
IN
THEIR
STATEMENTS
E
G
IT
IS
A
WHITE
LIQUID
THE
UNDERLYING
GAME
MECHANISM
OF
VERBOSITY
IS
SIMILAR
IN
NATURE
TO
THAT
OF
PEEKABOOM
MUCH
LIKE
DESIGNING
AN
ALGORITHM
TO
SOLVE
A
PROBLEM
DESIGNING
A
GAME
TO
HARNESS
VALUABLE
HUMAN
CYCLES
IS
TO
A
LARGE
EXTENT
AN
ART
PROBLEMS
USUALLY
REQUIRE
A
SPECIFICALLY
TAILORED
GAME
IN
ADDITION
TO
AN
ORIGINAL
IDEA
CREATING
SUCH
A
GAME
ALSO
DEPENDS
ON
A
BROADER
SET
OF
CRITERIA
INCLUDING
LOOKS
THE
FLUIDITY
OF
THE
GAME
GRAPHICS
EASE
OF
USE
AN
INTUITIVE
USER
INTERFACE
COGNITIVE
LOAD
THE
AMOUNT
OF
USER
ATTENTION
REQUIRED
TO
PLAY
THE
GAME
AND
ACTION
THE
EXTENT
TO
WHICH
THE
GAME
ABSORBS
THE
USER
IN
THE
EXPERIENCE
ALL
OF
THESE
ASPECTS
HAVE
BEEN
TREATED
IN
THIS
PAPER
AND
WE
BELIEVE
MANY
OF
THE
TECHNIQUES
HERE
PRESENTED
GENERALIZE
TO
CREATING
OTHER
GAMES
WITH
A
PURPOSE
FINALLY
WE
BELIEVE
THAT
THESE
DESIGN
PRINCIPLES
LIKE
THE
SCIENTIFIC
METHOD
DON
T
JUST
PROVIDE
IDEAS
BUT
A
WAY
OF
THINKING
GAMES
PROVIDE
A
VALUABLE
VEHICLE
TO
SOLVE
PROBLEMS
THAT
COMPUTERS
CANNOT
YET
SOLVE
ETHICAL
CONSIDERATIONS
AS
WITH
ALL
SYSTEMS
SOLICITING
INPUT
FROM
HUMANS
WE
MUST
ADDRESS
THE
ETHICAL
ISSUES
BEHIND
THE
USAGE
OF
THE
COLLECTED
DATA
TOWARDS
THIS
END
WE
INFORM
THE
PLAYERS
OF
THE
GAME
PURPOSE
ON
THE
PEEKABOOM
WEBSITE
PLAYERS
PARTICIPATE
WILLINGLY
AND
KNOWINGLY
INDEED
MANY
PEOPLE
PLAY
BECAUSE
THEY
LIKE
THE
FACT
THAT
THE
GAME
HAS
A
PURPOSE
FURTHERMORE
WE
STATE
ON
THE
RECORD
THAT
THE
GAME
PURPOSE
IS
TO
OBTAIN
ACCURATE
SEGMENTATIONS
OF
OBJECTS
FROM
BACKGROUNDS
AND
TO
TRAIN
COMPUTER
VISION
ALGORITHMS
TO
RECOGNIZE
SIMPLE
OBJECTS
WE
HAVE
NO
INTENTION
OF
APPLYING
OUR
DATA
TOWARDS
FOR
EXAMPLE
MILITARY
SURVEILLANCE
RELATED
WORK
WE
HAVE
PRESENTED
A
METHOD
FOR
ANNOTATING
ARBITRARY
IMAGES
AND
WE
HAVE
PRESENTED
EVIDENCE
THAT
IT
PRODUCES
HIGH
QUALITY
DATA
WE
NOW
SURVEY
THE
RELATED
WORK
THE
ESP
GAME
AS
MENTIONED
BEFORE
THE
ESP
GAME
IS
TWO
PLAYER
GAME
THAT
COLLECTS
WORD
LABELS
FOR
ARBITRARY
IMAGES
PEEKABOOM
IS
SIMILAR
TO
THE
ESP
GAME
AND
IN
FACT
WAS
INSPIRED
BY
IT
WE
CONSIDER
PEEKABOOM
AN
EXTENSION
OF
ESP
WHEREAS
ESP
GIVES
DATA
TO
DETERMINE
WHICH
OBJECTS
ARE
IN
THE
IMAGE
PEEKABOOM
CAN
AUGMENT
THIS
DATA
WITH
INFORMATION
ABOUT
WHERE
IN
THE
IMAGE
OBJECTS
ARE
LOCATED
IN
TERMS
OF
GAME
MECHANICS
PEEKABOOM
IS
DIFFERENT
FROM
THE
ESP
GAME
IN
SEVERAL
WAYS
FIRST
PEEKABOOM
IS
ASYMMETRIC
WHEREAS
BOTH
PLAYERS
IN
THE
ESP
GAME
ARE
PERFORMING
THE
SAME
ROLE
PLAYERS
OF
PEEKABOOM
ALTERNATE
IN
PERFORMING
DIFFERENT
ROLES
SECOND
PEEKABOOM
ALLOWS
A
SIGNIFICANTLY
HIGHER
LEVEL
OF
INTERACTION
AMONG
THE
PLAYERS
WHEREAS
IN
THE
ESP
GAME
PLAYERS
CANNOT
COMMUNICATE
AT
ALL
IN
PEEKABOOM
ONE
OF
THE
PLAYERS
CAN
FREELY
COMMUNICATE
WITH
THE
OTHER
THIRD
THE
USAGE
OF
HINT
BUTTONS
HAS
PROVEN
VERY
SUCCESSFUL
IN
PEEKABOOM
AND
SUCH
BUTTONS
COULD
AS
WELL
BE
INCORPORATED
INTO
ESP
SUCH
DIFFERENCES
IN
GAME
MECHANICS
REFLECT
THE
DIFFERENCE
IN
PURPOSE
OF
PEEKABOOM
AND
ESP
THE
OPEN
MIND
INITIATIVE
PERHAPS
LESS
SO
PEEKABOOM
IS
ALSO
SIMILAR
AT
LEAST
IN
SPIRIT
TO
THE
OPEN
MIND
INITIATIVE
E
G
A
WORLDWIDE
EFFORT
TO
DEVELOP
INTELLIGENT
SOFTWARE
OPEN
MIND
COLLECTS
DATA
FROM
REGULAR
INTERNET
USERS
REFERRED
TO
AS
NETIZENS
AND
FEEDS
IT
TO
MACHINE
LEARNING
ALGORITHMS
VOLUNTEERS
PARTICIPATE
BY
ANSWERING
QUESTIONS
AND
TEACHING
CONCEPTS
TO
COMPUTER
PROGRAMS
PEEKABOOM
IS
SIMILAR
TO
OPEN
MIND
IN
THAT
WE
USE
REGULAR
PEOPLE
ON
THE
INTERNET
TO
ANNOTATE
IMAGES
HOWEVER
AS
WITH
THE
ESP
GAME
WE
PUT
MUCH
GREATER
EMPHASIS
ON
OUR
METHOD
BEING
FUN
WE
DON
T
EXPECT
VOLUNTEERS
TO
ANNOTATE
MILLIONS
OF
IMAGES
ON
THE
WEB
WE
EXPECT
IMAGES
TO
BE
ANNOTATED
BECAUSE
PEOPLE
WANT
TO
PLAY
OUR
GAME
WHEREAS
A
TYPICAL
OPEN
MIND
ACTIVITY
WOULD
ASK
PARTICIPANTS
TO
POINT
TO
THE
OBJECT
IN
QUESTION
WE
TRANSFORM
THE
ACTIVITY
INTO
A
TWO
PLAYER
GAME
IN
WHICH
PLAYERS
ARE
NOT
EVEN
ASKED
TO
POINT
TO
THE
OBJECT
THEY
DO
SO
ONLY
AS
A
SIDE
EFFECT
OF
PLAYING
THE
GAME
LABELME
LABELME
IS
A
WEB
BASED
TOOL
FOR
IMAGE
ANNOTATION
ANYBODY
CAN
ANNOTATE
DATA
USING
THIS
TOOL
AND
THUS
CONTRIBUTE
TO
CONSTRUCTING
A
LARGE
DATABASE
OF
ANNOTATED
OBJECTS
THE
INCENTIVE
TO
ANNOTATE
DATA
IS
THE
DATA
ITSELF
YOU
CAN
ONLY
HAVE
ACCESS
TO
THE
DATABASE
ONCE
YOU
HAVE
ANNOTATED
A
CERTAIN
NUMBER
OF
IMAGES
THE
MAIN
DIFFERENCE
BETWEEN
PEEKABOOM
AND
LABELME
IS
THE
GAME
ASPECT
WHEREAS
LABELME
SIMPLY
ASKS
USERS
TO
ANNOTATE
AN
IMAGE
PEEKABOOM
TRANSFORMS
THE
PROCESS
INTO
AN
ENJOYABLE
GAME
LABELME
RELIES
ON
PEOPLE
DESIRE
TO
HELP
AND
THUS
ASSUMES
THAT
THE
ENTERED
DATA
IS
CORRECT
ON
THE
OTHER
HAND
PEEKABOOM
HAS
MULTIPLE
MECHANISMS
TO
PREVENT
PLAYERS
FROM
POLLUTING
THE
DATA
INTERACTIVE
MACHINE
LEARNING
ANOTHER
AREA
OF
RELATED
WORK
IS
THAT
OF
INTERACTIVELY
TRAINING
MACHINE
LEARNING
ALGORITHMS
E
G
IN
THESE
SYSTEMS
A
USER
IS
GIVEN
IMMEDIATE
FEEDBACK
ABOUT
HOW
WELL
AN
ALGORITHM
IS
LEARNING
FROM
THE
EXAMPLES
PROVIDED
BY
THEM
AS
WITH
LABELME
PEEKABOOM
DIFFERS
FROM
THESE
SYSTEMS
IN
THE
GAMING
ASPECT
AS
WELL
AS
IN
THE
ASSUMPTION
THAT
OUR
USERS
ARE
INTERESTED
IN
TRAINING
AN
ALGORITHM
CONCLUSIONS
AND
FUTURE
WORK
PEEKABOOM
IS
A
NOVEL
COMPLETE
GAME
ARCHITECTURE
FOR
COLLECTING
IMAGE
METADATA
SEGMENTING
OBJECTS
IN
IMAGES
IS
A
UNIQUE
CHALLENGE
AND
WE
HAVE
TAILORED
A
GAME
SPECIFICALLY
TO
THIS
END
IN
THE
VERY
NEAR
FUTURE
WE
WOULD
LIKE
TO
MAKE
OUR
PIECES
OF
DATA
AVAILABLE
TO
THE
WORLD
BY
FORMATTING
IT
AS
AN
IMAGE
SEGMENTATION
LIBRARY
LIKE
THE
ESP
GAME
PEEKABOOM
ENCOMPASSES
MUCH
MORE
THAN
JUST
A
JAVA
APPLET
DELIVERED
FROM
A
WEBSITE
RATHER
THE
IDEAS
BEHIND
THE
DESIGN
AND
IMPLEMENTATION
OF
THE
GAME
GENERALIZE
TO
A
WAY
OF
HARNESSING
AND
DIRECTING
THE
POWER
OF
THE
MOST
INTRICATE
COMPUTING
DEVICE
IN
THE
WORLD
THE
HUMAN
MIND
SOME
DAY
COMPUTERS
WILL
BE
ABLE
TO
SEGMENT
OBJECTS
IN
IMAGES
UNASSISTED
BUT
THAT
DAY
IS
NOT
TODAY
TODAY
WE
HAVE
ENGINES
LIKE
PEEKABOOM
THAT
USE
THE
WISDOM
OF
HUMANS
TO
HELP
NAÏVE
COMPUTERS
GET
TO
THAT
POINT
THE
ACTUAL
PROCESS
OF
MAKING
COMPUTERS
SMARTER
GIVEN
SEGMENTATION
METADATA
IS
BEYOND
THE
SCOPE
OF
THIS
PAPER
SINCE
IT
WOULD
REQUIRE
A
FAR
MORE
SOPHISTICATED
INTERPRETATION
OF
THE
DATA
THAN
THE
SIMPLE
BOUNDING
BOX
DERIVATION
WE
HAVE
PRESENTED
THUS
WE
SEE
GREAT
POTENTIAL
IN
FUTURE
WORK
AT
THE
CROSSROADS
OF
HUMAN
COMPUTER
INTERACTION
AND
ARTIFICIAL
INTELLIGENCE
WHERE
THE
OUTPUT
OF
OUR
INTERACTIVE
SYSTEM
HELPS
ADVANCE
THE
STATE
OF
THE
ART
IN
COMPUTER
VISION
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
COURSE
INFO
COURSE
WEBSITE
INSTRUCTOR
ADRIANA
KOVASHKA
KOVASHKA
CS
PITT
EDU
PLEASE
USE
AT
THE
BEGINNING
OF
YOUR
SUBJECT
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
TUESDAY
AND
THURSDAY
GRADER
NILS
MURRUGARRA
LLERENA
NINEIL
CS
PITT
EDU
TEXTBOOKS
BY
RICHARD
SZELISKI
BY
KRISTEN
GRAUMAN
AND
BASTIAN
LEIBE
COURSE
GOALS
TO
LEARN
ABOUT
THE
BASIC
COMPUTER
VISION
TASKS
AND
APPROACHES
TO
GET
EXPERIENCE
WITH
SOME
COMPUTER
VISION
TECHNIQUES
TO
LEARN
ABSOLUTE
BASICS
OF
MACHINE
LEARNING
TO
THINK
CRITICALLY
ABOUT
VISION
APPROACHES
AND
TO
SEE
CONNECTIONS
BETWEEN
WORKS
AND
POTENTIAL
FOR
IMPROVEMENT
PLAN
FOR
TODAY
INTRODUCTIONS
WHAT
IS
COMPUTER
VISION
WHY
DO
WE
CARE
WHAT
ARE
THE
CHALLENGES
COURSE
STRUCTURE
AND
POLICIES
OVERVIEW
OF
TOPICS
INTRODUCTIONS
INTRODUCTIONS
WHAT
IS
YOUR
NAME
WHAT
IS
YOUR
DEPARTMENT
MAJOR
AND
YEAR
WHAT
ONE
THING
OUTSIDE
OF
SCHOOL
ARE
YOU
PASSIONATE
ABOUT
WHAT
DO
YOU
HOPE
TO
GET
OUT
OF
THIS
CLASS
WHAT
DO
YOU
PLAN
TO
DO
WHEN
YOU
GRADUATE
COMPUTER
VISION
WHAT
IS
COMPUTER
VISION
DONE
WE
SEE
WITH
OUR
BRAINS
NOT
WITH
OUR
EYES
OLIVER
SACKS
AND
OTHERS
KRISTEN
GRAUMAN
ADAPTED
WHAT
IS
COMPUTER
VISION
AUTOMATIC
UNDERSTANDING
OF
IMAGES
AND
VIDEO
COMPUTING
PROPERTIES
OF
THE
WORLD
FROM
VISUAL
DATA
MEASUREMENT
ALGORITHMS
AND
REPRESENTATIONS
TO
ALLOW
A
MACHINE
TO
RECOGNIZE
OBJECTS
PEOPLE
SCENES
AND
ACTIVITIES
PERCEPTION
AND
INTERPRETATION
ALGORITHMS
TO
MINE
SEARCH
AND
INTERACT
WITH
VISUAL
DATA
SEARCH
AND
ORGANIZATION
KRISTEN
GRAUMAN
VISION
FOR
MEASUREMENT
MULTI
VIEW
STEREO
FOR
REAL
TIME
STEREO
STRUCTURE
FROM
MOTION
POLLEFEYS
ET
AL
KRISTEN
GRAUMAN
COMMUNITY
PHOTO
COLLECTIONS
GOESELE
ET
AL
SLIDE
CREDIT
L
LAZEBNIK
VISION
FOR
PERCEPTION
INTERPRETATION
OBJECTS
ACTIVITIES
SCENES
LOCATIONS
TEXT
WRITING
FACES
GESTURES
MOTIONS
EMOTIONS
VISUAL
SEARCH
ORGANIZATION
QUERY
IMAGE
OR
VIDEO
ARCHIVES
RELEVANT
CONTENT
RELATED
DISCIPLINES
GRAPHICS
IMAGE
PROCESSING
ARTIFICIAL
INTELLIGENCE
COMPUTER
VISION
ALGORITHMS
MACHINE
LEARNING
COGNITIVE
SCIENCE
VISION
AND
GRAPHICS
IMAGES
VISION
MODEL
GRAPHICS
INVERSE
PROBLEMS
ANALYSIS
AND
SYNTHESIS
WHY
VISION
AS
IMAGE
SOURCES
MULTIPLY
SO
DO
APPLICATIONS
RELIEVE
HUMANS
OF
BORING
EASY
TASKS
HUMAN
COMPUTER
INTERACTION
PERCEPTION
FOR
ROBOTICS
AUTONOMOUS
AGENTS
ORGANIZE
AND
GIVE
ACCESS
TO
VISUAL
CONTENT
WHY
VISION
IMAGES
AND
VIDEO
ARE
EVERYWHERE
PERSONAL
PHOTO
ALBUMS
SURVEILLANCE
AND
SECURITY
MOVIES
NEWS
SPORTS
MEDICAL
AND
SCIENTIFIC
IMAGES
FACES
AND
DIGITAL
CAMERAS
CAMERA
WAITS
FOR
EVERYONE
TO
SMILE
TO
TAKE
A
PHOTO
CANON
SETTING
CAMERA
FOCUS
VIA
FACE
DETECTION
FACE
RECOGNITION
LINKING
TO
INFO
WITH
A
MOBILE
DEVICE
SITUATED
SEARCH
YEH
ET
AL
MIT
MSR
LINCOLN
KOOABA
EXPLORING
PHOTO
COLLECTIONS
SNAVELY
ET
AL
SPECIAL
VISUAL
EFFECTS
THE
MATRIX
WHAT
DREAMS
MAY
COME
MOCAP
FOR
PIRATES
OF
THE
CARRIBEAN
INDUSTRIAL
LIGHT
AND
MAGIC
SOURCE
SEITZ
INTERACTIVE
SYSTEMS
VIDEO
BASED
INTERFACES
HUMAN
JOYSTICK
NEWSBREAKER
LIVE
ASSISTIVE
TECHNOLOGY
SYSTEMS
CAMERA
MOUSE
BOSTON
COLLEGE
VISION
FOR
MEDICAL
NEUROIMAGES
IMAGE
GUIDED
SURGERY
MIT
AI
VISION
GROUP
FMRI
DATA
GOLLAND
ET
AL
SAFETY
SECURITY
NAVIGATION
DRIVER
SAFETY
MONITORING
POOL
POSEIDON
PEDESTRIAN
DETECTION
MERL
VIOLA
ET
AL
SURVEILLANCE
OBSTACLES
WHAT
THE
COMPUTER
GETS
WHY
IS
VISION
DIFFICULT
ILL
POSED
PROBLEM
REAL
WORLD
MUCH
MORE
COMPLEX
THAN
WHAT
WE
CAN
MEASURE
IN
IMAGES
IMPOSSIBLE
TO
LITERALLY
INVERT
IMAGE
FORMATION
PROCESS
CHALLENGES
MANY
NUISANCE
PARAMETERS
ILLUMINATION
OBJECT
POSE
CLUTTER
OCCLUSIONS
INTRA
CLASS
APPEARANCE
VIEWPOINT
CHALLENGES
INTRA
CLASS
VARIATION
CHALLENGES
IMPORTANCE
OF
CONTEXT
CHALLENGES
COMPLEXITY
THOUSANDS
TO
MILLIONS
OF
PIXELS
IN
AN
IMAGE
HUMAN
RECOGNIZABLE
OBJECT
CATEGORIES
DEGREES
OF
FREEDOM
IN
THE
POSE
OF
ARTICULATED
OBJECTS
HUMANS
BILLIONS
OF
IMAGES
INDEXED
BY
GOOGLE
IMAGE
SEARCH
BILLION
PRINTS
PRODUCED
FROM
DIGITAL
CAMERA
IMAGES
IN
MILLION
CAMERA
PHONES
SOLD
IN
ABOUT
HALF
OF
THE
CEREBRAL
CORTEX
IN
PRIMATES
IS
DEVOTED
TO
PROCESSING
VISUAL
INFORMATION
FELLEMAN
AND
VAN
ESSEN
CHALLENGES
LIMITED
SUPERVISION
LESS
MORE
OK
CLEARLY
THE
VISION
PROBLEM
IS
DEEP
AND
CHALLENGING
TIME
TO
GIVE
UP
ACTIVE
RESEARCH
AREA
WITH
EXCITING
PROGRESS
DATASETS
TODAY
IMAGENET
CATEGORIES
IMAGES
MICROSOFT
COCO
CATEGORIES
IMAGES
PASCAL
CATEGORIES
IMAGES
SUN
CATEGORIES
IMAGES
SOME
VISUAL
RECOGNITION
PROBLEMS
RECOGNITION
WHAT
IS
THIS
RECOGNITION
WHAT
IS
THIS
BUILDING
BALCONY
STREET
TRUCK
CARRIAGE
HORSE
TABLE
PERSON
PERSON
CAR
DETECTION
WHERE
ARE
THE
CARS
ACTIVITY
WHAT
IS
THIS
PERSON
DOING
SCENE
IS
THIS
AN
INDOOR
SCENE
INSTANCE
WHICH
CITY
WHICH
BUILDING
VISUAL
PERSUASION
EVALUATING
ACTION
QUALITY
TRANSFERRING
ART
STYLE
ANSWERING
VISUAL
QUESTIONS
COURSE
STRUCTURE
AND
POLICIES
COURSE
COMPONENTS
HOMEWORK
PROBLEM
SETS
MIDTERM
EXAM
FINAL
EXAM
IN
CLASS
PARTICIPATION
COURSE
SCHEDULE
HOMEWORK
SUBMISSION
WE
WILL
USE
COURSEWEB
NAVIGATE
TO
THE
COURSEWEB
PAGE
FOR
CLICK
ON
ASSIGNMENTS
AND
THE
CORRESPONDING
HW
ATTACH
A
ZIP
FILE
WITH
YOUR
WRITTEN
RESPONSES
AND
CODE
NAME
THE
FILE
AS
ZIP
OR
TAR
HOMEWORK
IS
DUE
AT
ON
THE
DUE
GRADES
WILL
APPEAR
ON
COURSEWEB
EXAMS
ONE
MID
TERM
AND
ONE
FINAL
EXAM
THE
FINAL
EXAM
WILL
FOCUS
ON
THE
LATTER
HALF
OF
THE
COURSE
EXAMS
WILL
BE
PRECEDED
BY
REVIEW
SESSIONS
IF
OUR
SCHEDULE
ALLOWS
IT
EXAM
IS
TENTATIVELY
SCHEDULED
FOR
MONDAY
DECEMBER
PARTICIPATION
OF
GRADE
WILL
BE
BASED
ON
ATTENDANCE
AND
PARTICIPATION
TWO
FREE
ABSENCES
LET
ME
KNOW
AND
EXPLAIN
BEYOND
THAT
ANSWER
QUESTIONS
ASKED
BY
INSTRUCTOR
AND
OTHERS
ASK
MEANINGFUL
QUESTIONS
BRING
IN
RELEVANT
ARTICLES
ABOUT
RECENT
DEVELOPMENTS
IN
COMPUTER
VISION
FEEDBACK
IS
WELCOME
LATE
POLICY
YOU
GET
FREE
LATE
DAYS
I
E
YOU
CAN
SUBMIT
HOMEWORK
A
TOTAL
OF
DAYS
LATE
FOR
EXAMPLE
YOU
CAN
SUBMIT
ONE
PROBLEM
SET
HOURS
LATE
AND
ANOTHER
HOURS
LATE
ONCE
YOU
VE
USED
UP
YOUR
FREE
LATE
DAYS
YOU
WILL
INCUR
A
PENALTY
OF
FROM
THE
TOTAL
PROJECT
CREDIT
POSSIBLE
FOR
EACH
LATE
DAY
A
LATE
DAY
IS
ANYTHING
FROM
MINUTE
TO
HOURS
COLLABORATION
POLICY
YOU
WILL
WORK
INDIVIDUALLY
THE
WORK
YOU
TURN
IN
MUST
BE
YOUR
OWN
WORK
YOU
CAN
DISCUSS
THE
PROBLEM
SETS
WITH
YOUR
CLASSMATES
BUT
DO
NOT
LOOK
AT
THEIR
CODE
YOU
CANNOT
USE
POSTED
SOLUTIONS
SEARCH
FOR
CODE
ON
THE
INTERNET
OR
USE
MATLAB
IMPLEMENTATIONS
OF
SOMETHING
YOU
ARE
ASKED
TO
WRITE
WHEN
IN
DOUBT
ASK
THE
INSTRUCTOR
PLAGIARISM
WILL
CAUSE
YOU
TO
FAIL
THE
CLASS
AND
RECEIVE
DISCIPLINARY
PENALTY
DISABILITIES
IF
YOU
HAVE
A
DISABILITY
FOR
WHICH
YOU
ARE
OR
MAY
BE
REQUESTING
AN
ACCOMMODATION
YOU
ARE
ENCOURAGED
TO
CONTACT
BOTH
YOUR
INSTRUCTOR
AND
DISABILITY
RESOURCES
AND
SERVICES
DRS
WILLIAM
PITT
UNION
FOR
ASL
USERS
AS
EARLY
AS
POSSIBLE
IN
THE
TERM
DRS
WILL
VERIFY
YOUR
DISABILITY
AND
DETERMINE
REASONABLE
ACCOMMODATIONS
FOR
THIS
COURSE
MEDICAL
CONDITIONS
IF
YOU
HAVE
A
MEDICAL
CONDITION
WHICH
WILL
PREVENT
YOU
FROM
DOING
A
CERTAIN
ASSIGNMENT
OR
COMING
TO
CLASS
YOU
MUST
INFORM
THE
INSTRUCTOR
OF
THIS
BEFORE
THE
DEADLINE
YOU
MUST
THEN
SUBMIT
DOCUMENTATION
OF
YOUR
CONDITION
WITHIN
A
WEEK
OF
THE
ASSIGNMENT
DEADLINE
QUESTIONS
OVERVIEW
OF
TOPICS
FEATURES
AND
FILTERS
TRANSFORMING
AND
DESCRIBING
IMAGES
TEXTURES
COLORS
EDGES
KRISTEN
GRAUMAN
FEATURES
AND
FILTERS
DETECTING
REPEATABLE
FEATURES
DESCRIBING
IMAGES
WITH
LOCAL
STATISTICS
INDEXING
AND
SEARCH
MATCHING
FEATURES
AND
REGIONS
ACROSS
IMAGES
GROUPING
AND
FITTING
CLUSTERING
SEGMENTATION
FITTING
WHAT
PARTS
BELONG
TOGETHER
IMAGE
FORMATION
HOW
DOES
LIGHT
IN
WORLD
PROJECT
TO
FORM
IMAGES
MULTIPLE
VIEWS
MULTI
VIEW
GEOMETRY
MATCHING
INVARIANT
FEATURES
STEREO
VISION
HARTLEY
AND
ZISSERMAN
VISUAL
RECOGNITION
RECOGNIZING
OBJECTS
AND
CATEGORIES
LEARNING
TECHNIQUES
OBJECT
DETECTION
DETECTING
NOVEL
INSTANCES
OF
OBJECTS
CLASSIFYING
REGIONS
AS
ONE
OF
SEVERAL
CATEGORIES
ATTRIBUTE
BASED
SEARCH
DESCRIBING
THE
HIGH
LEVEL
PROPERTIES
OF
OBJECTS
SEARCHING
FOR
OBJECTS
WITH
RELATIVE
ATTRIBUTES
CROWDSOURCING
ANNOTATIONS
USING
NON
EXPERT
LABELERS
TO
COLLECT
DATA
ACTIVELY
REQUESTING
LABELS
DEEP
LEARNING
ALEXNET
GOOGLE
INCEPTIONISM
MOTION
AND
TRACKING
TRACKING
OBJECTS
VIDEO
ANALYSIS
LOW
LEVEL
MOTION
OPTICAL
FLOW
KRISTEN
GRAUMAN
TOMAS
IZO
POSE
AND
ACTIONS
AUTOMATICALLY
ANNOTATING
A
HUMAN
POSE
RECOGNIZING
ACTIONS
IN
FIRST
PERSON
VIDEO
NEXT
TIME
MATLAB
TUTORIAL
OUT
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
PLAN
FOR
TODAY
COURSE
BASICS
REFRESHER
MATLAB
TUTORIAL
OVERVIEW
OF
HOMEWORK
START
ON
IMAGE
FILTERING
COURSE
WEBSITE
INSTRUCTOR
ADRIANA
KOVASHKA
KOVASHKA
CS
PITT
EDU
PLEASE
USE
AT
THE
BEGINNING
OF
YOUR
SUBJECT
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
TUESDAY
AND
THURSDAY
GRADER
NILS
MURRUGARRA
LLERENA
NINEIL
CS
PITT
EDU
BY
RICHARD
SZELISKI
BY
KRISTEN
GRAUMAN
AND
BASTIAN
LEIBE
PLEASE
DO
THE
ASSIGNED
READINGS
BEFORE
CLASS
WE
WILL
USE
COURSEWEB
NAVIGATE
TO
THE
COURSEWEB
PAGE
FOR
CLICK
ON
ASSIGNMENTS
AND
THE
CORRESPONDING
HW
ATTACH
A
ZIP
FILE
WITH
YOUR
WRITTEN
RESPONSES
AND
CODE
NAME
THE
FILE
AS
ZIP
OR
TAR
HOMEWORK
IS
DUE
AT
ON
THE
DUE
GRADES
WILL
APPEAR
ON
COURSEWEB
YOU
GET
FREE
LATE
DAYS
I
E
YOU
CAN
SUBMIT
HOMEWORK
A
TOTAL
OF
DAYS
LATE
FOR
EXAMPLE
YOU
CAN
SUBMIT
ONE
PROBLEM
SET
HOURS
LATE
AND
ANOTHER
HOURS
LATE
ONCE
YOU
VE
USED
UP
YOUR
FREE
LATE
DAYS
YOU
WILL
INCUR
A
PENALTY
OF
FROM
THE
TOTAL
PROJECT
CREDIT
POSSIBLE
FOR
EACH
LATE
DAY
A
LATE
DAY
IS
ANYTHING
FROM
MINUTE
TO
HOURS
YOU
WILL
WORK
INDIVIDUALLY
THE
WORK
YOU
TURN
IN
MUST
BE
YOUR
OWN
WORK
YOU
CAN
DISCUSS
THE
PROBLEM
SETS
WITH
YOUR
CLASSMATES
BUT
DO
NOT
LOOK
AT
THEIR
CODE
YOU
CANNOT
USE
POSTED
SOLUTIONS
SEARCH
FOR
CODE
ON
THE
INTERNET
OR
USE
MATLAB
IMPLEMENTATIONS
OF
SOMETHING
YOU
ARE
ASKED
TO
WRITE
WHEN
IN
DOUBT
ASK
THE
INSTRUCTOR
PLAGIARISM
WILL
CAUSE
YOU
TO
FAIL
THE
CLASS
AND
RECEIVE
DISCIPLINARY
PENALTY
PART
I
MATLAB
EXERCISE
TODAY
LECTURE
PART
II
SHORT
ANSWERS
LECTURES
ON
AND
PART
III
IMPLEMENTING
IMAGE
PYRAMIDS
LECTURE
ON
PART
IV
IMPLEMENTING
SEAM
CARVING
TODAY
LECTURE
MATLAB
TUTORIAL
IMAGE
FORMATION
DIGITAL
CAMERA
A
DIGITAL
CAMERA
REPLACES
FILM
WITH
A
SENSOR
ARRAY
EACH
CELL
IN
THE
ARRAY
IS
LIGHT
SENSITIVE
DIODE
THAT
CONVERTS
PHOTONS
TO
ELECTRONS
DIGITAL
IMAGES
DIGITAL
IMAGES
SAMPLE
THE
SPACE
ON
A
REGULAR
GRID
QUANTIZE
EACH
SAMPLE
ROUND
TO
NEAREST
INTEGER
IMAGE
THUS
REPRESENTED
AS
A
MATRIX
OF
INTEGER
VALUES
DIGITAL
COLOR
IMAGES
DIGITAL
COLOR
IMAGES
COLOR
IMAGES
RGB
COLOR
SPACE
R
G
B
SLIDE
CREDIT
KRISTEN
GRAUMAN
IMAGES
IN
MATLAB
IMAGES
REPRESENTED
AS
A
MATRIX
SUPPOSE
WE
HAVE
A
NXM
RGB
IMAGE
CALLED
IM
IM
TOP
LEFT
PIXEL
VALUE
IN
R
CHANNEL
IM
Y
X
B
Y
PIXELS
DOWN
X
PIXELS
TO
RIGHT
IN
THE
BTH
CHANNEL
IM
N
M
BOTTOM
RIGHT
PIXEL
IN
B
CHANNEL
IMREAD
FILENAME
RETURNS
A
IMAGE
VALUES
TO
CONVERT
TO
DOUBLE
FORMAT
VALUES
TO
WITH
ROW
COLUMN
R
SLIDE
CREDIT
DEREK
HOIEM
HOMEWORK
SEAM
CARVING
CONTENT
AWARE
RESIZING
TRADITIONAL
RESIZING
INTUITION
CONTENT
AWARE
RESIZING
TO
REDUCE
OR
INCREASE
SIZE
IN
ONE
DIMENSION
REMOVE
IRREGULARLY
SHAPED
SEAMS
OPTIMAL
SOLUTION
VIA
DYNAMIC
PROGRAMMING
ENERGY
F
WANT
TO
REMOVE
SEAMS
WHERE
THEY
WON
T
BE
VERY
NOTICEABLE
MEASURE
ENERGY
AS
GRADIENT
MAGNITUDE
CHOOSE
SEAM
BASED
ON
MINIMUM
TOTAL
ENERGY
PATH
ACROSS
IMAGE
SUBJECT
TO
CONNECTEDNESS
ENERGY
F
LET
A
VERTICAL
SEAM
CONSIST
OF
H
POSITIONS
THAT
FORM
AN
CONNECTED
PATH
LET
THE
COST
OF
A
SEAM
BE
COST
ENERGY
F
I
SI
OPTIMAL
SEAM
MINIMIZES
THIS
COST
MIN
COST
COMPUTE
IT
EFFICIENTLY
WITH
DYNAMIC
PROGRAMMING
HOW
TO
IDENTIFY
THE
MINIMUM
COST
SEAM
FIRST
CONSIDER
A
GREEDY
APPROACH
KRISTEN
GRAUMAN
UT
AUSTIN
ENERGY
MATRIX
GRADIENT
MAGNITUDE
SEAM
CARVING
ALGORITHM
COMPUTE
THE
CUMULATIVE
MINIMUM
ENERGY
FOR
ALL
POSSIBLE
CONNECTED
SEAMS
AT
EACH
ENTRY
I
J
M
I
J
ENERGY
I
J
MIN
M
I
J
M
I
J
M
I
J
ENERGY
MATRIX
GRADIENT
MAGNITUDE
M
MATRIX
CUMULATIVE
MIN
ENERGY
FOR
VERTICAL
SEAMS
THEN
MIN
VALUE
IN
LAST
ROW
OF
M
INDICATES
END
OF
THE
MINIMAL
CONNECTED
VERTICAL
SEAM
BACKTRACK
UP
FROM
THERE
SELECTING
MIN
OF
ABOVE
IN
M
M
I
J
ENERGY
I
J
MIN
M
I
J
M
I
J
M
I
J
M
I
J
ENERGY
I
MIN
M
I
J
M
I
J
M
I
J
ORIGINAL
IMAGE
ENERGY
MAP
BLUE
LOW
ENERGY
RED
HIGH
ENERGY
OTHER
NOTES
ON
SEAM
CARVING
ANALOGOUS
PROCEDURE
FOR
HORIZONTAL
SEAMS
CAN
ALSO
INSERT
SEAMS
TO
INCREASE
SIZE
OF
IMAGE
IN
EITHER
DIMENSION
DUPLICATE
OPTIMAL
SEAM
AVERAGED
WITH
NEIGHBORS
OTHER
ENERGY
FUNCTIONS
MAY
BE
PLUGGED
IN
E
G
COLOR
BASED
INTERACTIVE
CAN
USE
COMBINATION
OF
VERTICAL
AND
HORIZONTAL
SEAMS
EXAMPLE
RESULTS
FROM
CLASSES
AT
UT
AUSTIN
RESULTS
FROM
EUNHO
YANG
RESULTS
FROM
SUYOG
JAIN
ORIGINAL
IMAGE
CONVENTIONAL
RESIZE
SEAM
CARVING
RESULT
ORIGINAL
IMAGE
CONVENTIONAL
RESIZE
SEAM
CARVING
RESULT
ORIGINAL
IMAGE
BY
CONVENTIONAL
RESIZE
BY
SEAM
CARVING
BY
RESULTS
FROM
JAY
HENNIG
NEXT
TIME
IMAGE
FILTERING
READING
FOR
TUESDAY
SZELISKI
SEC
READING
FOR
TODAY
WAS
SZELISKI
SEC
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
SLIDES
FROM
KRISTEN
GRAUMAN
PLAN
FOR
TODAY
FILTERS
CONT
D
TEXTURE
DESCRIPTION
TEXTURE
SYNTHESIS
SAMPLING
TEMPLATE
MATCHING
FILTERING
EXAMPLE
IMAGE
FILTERING
COMPUTE
A
FUNCTION
OF
THE
LOCAL
NEIGHBORHOOD
AT
EACH
PIXEL
IN
THE
IMAGE
FUNCTION
SPECIFIED
BY
A
FILTER
OR
MASK
SAYING
HOW
TO
COMBINE
VALUES
FROM
NEIGHBORS
USES
OF
FILTERING
ENHANCE
AN
IMAGE
DENOISE
RESIZE
ETC
EXTRACT
INFORMATION
TEXTURE
EDGES
ETC
DETECT
PATTERNS
TEMPLATE
MATCHING
MOTIVATION
NOISE
REDUCTION
EVEN
MULTIPLE
IMAGES
OF
THE
SAME
STATIC
SCENE
WILL
NOT
BE
IDENTICAL
COMMON
TYPES
OF
NOISE
SALT
AND
PEPPER
NOISE
RANDOM
OCCURRENCES
OF
BLACK
AND
WHITE
PIXELS
IMPULSE
NOISE
RANDOM
OCCURRENCES
OF
WHITE
PIXELS
GAUSSIAN
NOISE
VARIATIONS
IN
INTENSITY
DRAWN
FROM
A
GAUSSIAN
NORMAL
DISTRIBUTION
SOURCE
SEITZ
GAUSSIAN
NOISE
NOISE
RANDN
SIZE
IM
SIGMA
OUTPUT
IM
NOISE
WHAT
IS
IMPACT
OF
THE
SIGMA
FIG
M
HEBERT
EVEN
MULTIPLE
IMAGES
OF
THE
SAME
STATIC
SCENE
WILL
NOT
BE
IDENTICAL
HOW
COULD
WE
REDUCE
THE
NOISE
I
E
GIVE
AN
ESTIMATE
OF
THE
TRUE
INTENSITIES
WHAT
IF
THERE
ONLY
ONE
IMAGE
LET
REPLACE
EACH
PIXEL
WITH
AN
AVERAGE
OF
ALL
THE
VALUES
IN
ITS
NEIGHBORHOOD
ASSUMPTIONS
EXPECT
PIXELS
TO
BE
LIKE
THEIR
NEIGHBORS
EXPECT
NOISE
PROCESSES
TO
BE
INDEPENDENT
FROM
PIXEL
TO
PIXEL
LET
REPLACE
EACH
PIXEL
WITH
AN
AVERAGE
OF
ALL
THE
VALUES
IN
ITS
NEIGHBORHOOD
MOVING
AVERAGE
IN
CAN
ADD
WEIGHTS
TO
OUR
MOVING
AVERAGE
WEIGHTS
NON
UNIFORM
WEIGHTS
SAY
THE
AVERAGING
WINDOW
SIZE
IS
X
ATTRIBUTE
UNIFORM
WEIGHT
TO
EACH
PIXEL
LOOP
OVER
ALL
PIXELS
IN
NEIGHBORHOOD
AROUND
IMAGE
PIXEL
F
I
J
NOW
GENERALIZE
TO
ALLOW
DIFFERENT
WEIGHTS
DEPENDING
ON
NEIGHBORING
PIXEL
RELATIVE
POSITION
NON
UNIFORM
WEIGHTS
THIS
IS
CALLED
CROSS
CORRELATION
DENOTED
FILTERING
AN
IMAGE
REPLACE
EACH
PIXEL
WITH
A
LINEAR
COMBINATION
OF
ITS
NEIGHBORS
THE
FILTER
KERNEL
OR
MASK
H
U
V
IS
THE
PRESCRIPTION
FOR
THE
WEIGHTS
IN
THE
LINEAR
COMBINATION
WHAT
VALUES
BELONG
IN
THE
KERNEL
H
FOR
THE
MOVING
AVERAGE
EXAMPLE
BOX
FILTER
DEPICTS
BOX
FILTER
WHITE
HIGH
VALUE
BLACK
LOW
VALUE
ORIGINAL
FILTERED
WHAT
IF
THE
FILTER
SIZE
WAS
X
INSTEAD
OF
X
WHAT
IS
THE
SIZE
OF
THE
OUTPUT
MATLAB
OUTPUT
SIZE
SHAPE
OPTIONS
SHAPE
FULL
OUTPUT
SIZE
IS
SUM
OF
SIZES
OF
F
AND
G
SHAPE
SAME
OUTPUT
SIZE
IS
SAME
AS
F
SHAPE
VALID
OUTPUT
SIZE
IS
DIFFERENCE
OF
SIZES
OF
F
AND
G
FULL
SAME
VALID
WHAT
ABOUT
NEAR
THE
EDGE
THE
FILTER
WINDOW
FALLS
OFF
THE
EDGE
OF
THE
IMAGE
NEED
TO
EXTRAPOLATE
METHODS
CLIP
FILTER
BLACK
WRAP
AROUND
COPY
EDGE
REFLECT
ACROSS
EDGE
WHAT
ABOUT
NEAR
THE
EDGE
THE
FILTER
WINDOW
FALLS
OFF
THE
EDGE
OF
THE
IMAGE
NEED
TO
EXTRAPOLATE
METHODS
MATLAB
CLIP
FILTER
BLACK
IMFILTER
F
G
WRAP
AROUND
IMFILTER
F
G
CIRCULAR
COPY
EDGE
IMFILTER
F
G
REPLICATE
REFLECT
ACROSS
EDGE
IMFILTER
F
G
SYMMETRIC
GAUSSIAN
FILTER
WHAT
IF
WE
WANT
NEAREST
NEIGHBORING
PIXELS
TO
HAVE
THE
MOST
INFLUENCE
ON
THE
OUTPUT
REMOVES
HIGH
FREQUENCY
COMPONENTS
FROM
THE
IMAGE
LOW
PASS
FILTER
SOURCE
SEITZ
SMOOTHING
WITH
A
GAUSSIAN
GAUSSIAN
FILTERS
WHAT
PARAMETERS
MATTER
HERE
SIZE
OF
KERNEL
OR
MASK
NOTE
GAUSSIAN
FUNCTION
HAS
INFINITE
SUPPORT
BUT
DISCRETE
FILTERS
USE
FINITE
KERNELS
Σ
WITH
X
KERNEL
Σ
WITH
X
KERNEL
PRACTICAL
MATTERS
HOW
BIG
SHOULD
THE
FILTER
BE
VALUES
AT
EDGES
SHOULD
BE
NEAR
ZERO
IMPORTANT
RULE
OF
THUMB
FOR
GAUSSIAN
SET
FILTER
HALF
WIDTH
TO
ABOUT
Σ
SOURCE
DEREK
HOIEM
GAUSSIAN
FILTERS
WHAT
PARAMETERS
MATTER
HERE
VARIANCE
OF
GAUSSIAN
DETERMINES
EXTENT
OF
SMOOTHING
Σ
WITH
X
KERNEL
Σ
WITH
X
KERNEL
MATLAB
HSIZE
SIGMA
H
FSPECIAL
GAUSSIAN
HSIZE
SIGMA
MESH
H
IMAGESC
H
OUTIM
IMFILTER
IM
H
CORRELATION
IMSHOW
OUTIM
OUTIM
SMOOTHING
WITH
A
GAUSSIAN
PARAMETER
Σ
IS
THE
SCALE
WIDTH
SPREAD
OF
THE
GAUSSIAN
KERNEL
AND
CONTROLS
THE
AMOUNT
OF
SMOOTHING
FOR
SIGMA
H
FSPECIAL
GAUSSIAN
FSIZE
SIGMA
OUT
IMFILTER
IM
H
IMSHOW
OUT
PAUSE
END
PROPERTIES
OF
SMOOTHING
FILTERS
SMOOTHING
VALUES
POSITIVE
SUM
TO
CONSTANT
REGIONS
SAME
AS
INPUT
AMOUNT
OF
SMOOTHING
PROPORTIONAL
TO
MASK
SIZE
REMOVE
HIGH
FREQUENCY
COMPONENTS
LOW
PASS
FILTER
CONVOLUTION
CONVOLUTION
FLIP
THE
FILTER
IN
BOTH
DIMENSIONS
BOTTOM
TO
TOP
RIGHT
TO
LEFT
THEN
APPLY
CROSS
CORRELATION
NOTATION
FOR
CONVOLUTION
OPERATOR
CONVOLUTION
VS
CORRELATION
CONVOLUTION
CROSS
CORRELATION
FOR
A
GAUSSIAN
OR
BOX
FILTER
HOW
WILL
THE
OUTPUTS
DIFFER
PREDICT
THE
OUTPUTS
USING
CORRELATION
FILTERING
ORIGINAL
ORIGINAL
FILTERED
NO
CHANGE
ORIGINAL
ORIGINAL
SHIFTED
LEFT
BY
PIXEL
WITH
CORRELATION
ORIGINAL
ORIGINAL
BLUR
WITH
A
BOX
FILTER
ORIGINAL
ORIGINAL
SHARPENING
FILTER
ACCENTUATES
DIFFERENCES
WITH
LOCAL
AVERAGE
FILTERING
EXAMPLES
SHARPENING
SHIFT
INVARIANT
OPERATOR
BEHAVES
THE
SAME
EVERYWHERE
I
E
THE
VALUE
OF
THE
OUTPUT
DEPENDS
ON
THE
PATTERN
IN
THE
IMAGE
NEIGHBORHOOD
NOT
THE
POSITION
OF
THE
NEIGHBORHOOD
SUPERPOSITION
H
H
H
COMMUTATIVE
F
G
G
F
ASSOCIATIVE
F
G
H
F
G
H
DISTRIBUTES
OVER
ADDITION
F
G
H
F
G
F
H
SCALARS
FACTOR
OUT
KF
G
F
KG
K
F
G
IDENTITY
UNIT
IMPULSE
E
F
E
F
IN
SOME
CASES
FILTER
IS
SEPARABLE
AND
WE
CAN
FACTOR
INTO
TWO
STEPS
CONVOLVE
ALL
ROWS
CONVOLVE
ALL
COLUMNS
SEPARABILITY
EXAMPLE
F
G
H
F
G
H
FILTERING
CENTER
LOCATION
ONLY
THE
FILTER
FACTORS
INTO
A
PRODUCT
OF
FILTERS
PERFORM
FILTERING
ALONG
ROWS
FOLLOWED
BY
FILTERING
ALONG
THE
REMAINING
COLUMN
EFFECT
OF
SMOOTHING
FILTERS
ADDITIVE
GAUSSIAN
NOISE
SALT
AND
PEPPER
NOISE
NO
NEW
PIXEL
VALUES
INTRODUCED
REMOVES
SPIKES
GOOD
FOR
IMPULSE
SALT
PEPPER
NOISE
NON
LINEAR
FILTER
SALT
AND
PEPPER
NOISE
PLOTS
OF
A
ROW
OF
THE
IMAGE
MATLAB
OUTPUT
IM
IM
H
W
MEDIAN
FILTERED
SOURCE
M
HEBERT
MEDIAN
FILTER
IS
EDGE
PRESERVING
AUDE
OLIVA
ANTONIO
TORRALBA
PHILIPPE
G
SCHYNS
SIGGRAPH
GAUSSIAN
FILTER
OLIVA
A
TORRALBA
P
G
SCHYNS
SIGGRAPH
LAPLACIAN
FILTER
UNIT
IMPULSE
GAUSSIAN
LAPLACIAN
OF
GAUSSIAN
FILTERS
SUMMARY
LINEAR
FILTERS
AND
CONVOLUTION
USEFUL
FOR
ENHANCING
IMAGES
SMOOTHING
REMOVING
NOISE
BOX
FILTER
GAUSSIAN
FILTER
IMPACT
OF
SCALE
WIDTH
OF
SMOOTHING
FILTER
DETECTING
FEATURES
SEPARABLE
FILTERS
ARE
MORE
EFFICIENT
MEDIAN
FILTER
A
NON
LINEAR
FILTER
EDGE
PRESERVING
PLAN
FOR
TODAY
FILTERS
CONT
D
TEXTURE
DESCRIPTION
TEXTURE
SYNTHESIS
SAMPLING
TEMPLATE
MATCHING
TEXTURE
WHAT
DEFINES
A
TEXTURE
SHAPE
FROM
TEXTURE
ESTIMATE
SURFACE
ORIENTATION
OR
SHAPE
FROM
IMAGE
TEXTURE
USE
DEFORMATION
OF
TEXTURE
FROM
POINT
TO
POINT
TO
ESTIMATE
SURFACE
SHAPE
PICS
FROM
A
LOH
SHAPE
FROM
TEXTURE
ESTIMATE
SURFACE
ORIENTATION
OR
SHAPE
FROM
IMAGE
TEXTURE
SEGMENTATION
CLASSIFICATION
FROM
TEXTURE
CUES
ANALYZE
REPRESENT
TEXTURE
GROUP
IMAGE
REGIONS
WITH
CONSISTENT
TEXTURE
SYNTHESIS
GENERATE
NEW
TEXTURE
PATCHES
IMAGES
GIVEN
SOME
EXAMPLES
WHY
ANALYZE
TEXTURE
IMAGES
BILL
FREEMAN
A
EFROS
WHY
ANALYZE
TEXTURE
IMPORTANCE
TO
PERCEPTION
OFTEN
INDICATIVE
OF
A
MATERIAL
PROPERTIES
CAN
BE
IMPORTANT
APPEARANCE
CUE
ESPECIALLY
IF
SHAPE
IS
SIMILAR
ACROSS
OBJECTS
AIM
TO
DISTINGUISH
BETWEEN
SHAPE
BOUNDARIES
AND
TEXTURE
TECHNICALLY
REPRESENTATION
WISE
WE
WANT
A
FEATURE
ONE
STEP
ABOVE
BUILDING
BLOCKS
OF
FILTERS
EDGES
PSYCHOPHYSICS
OF
TEXTURE
SOME
TEXTURES
DISTINGUISHABLE
WITH
PREATTENTIVE
PERCEPTION
WITHOUT
SCRUTINY
EYE
MOVEMENTS
JULESZ
SAME
OR
DIFFERENT
LR
LR
II
T
L
LL
I
T
T
LR
T
II
T
LTT
LT
LR
LT
LL
L
T
L
L
L
T
LR
L
LL
L
LT
LR
LR
LR
LR
LR
LR
LR
LR
R
LI
LR
LR
II
T
L
LL
I
T
T
LR
T
II
T
LTT
LT
LR
LT
LL
L
T
L
L
L
T
LR
L
LL
L
LT
LR
LR
LR
LR
LR
LR
LR
LR
R
LR
LR
CAPTURING
THE
LOCAL
PATTERNS
WITH
IMAGE
MEASUREMENTS
BERGEN
ADELSON
NATURE
SCALE
OF
PATTERNS
INFLUENCES
DISCRIMINABILITY
SIZE
TUNED
LINEAR
FILTERS
TEXTURE
REPRESENTATION
TEXTURES
ARE
MADE
UP
OF
REPEATED
LOCAL
PATTERNS
SO
FIND
THE
PATTERNS
USE
FILTERS
THAT
LOOK
LIKE
PATTERNS
SPOTS
BARS
RAW
PATCHES
CONSIDER
MAGNITUDE
OF
RESPONSE
DESCRIBE
THEIR
STATISTICS
WITHIN
EACH
LOCAL
WINDOW
MEAN
STANDARD
DEVIATION
HISTOGRAM
HISTOGRAM
OF
PROTOTYPICAL
FEATURE
OCCURRENCES
KRISTEN
GRAUMAN
ORIGINAL
IMAGE
ORIGINAL
IMAGE
ORIGINAL
IMAGE
ORIGINAL
IMAGE
DIMENSION
MEAN
D
DX
VALUE
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
WITH
PRIMARILY
HORIZONTAL
EDGES
BOTH
WINDOWS
WITH
SMALL
GRADIENT
IN
BOTH
DIRECTIONS
WINDOWS
WITH
PRIMARILY
VERTICAL
EDGES
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
ORIGINAL
IMAGE
KRISTEN
GRAUMAN
DERIVATIVE
FILTER
RESPONSES
SQUARED
FAR
DISSIMILAR
TEXT
CLOSE
SIMILAR
TEXT
DIMENSION
MEAN
D
DX
VALUE
KRISTEN
GRAUMAN
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
D
A
B
DIMENSION
A
B
DIMENSION
DISTANCE
REVEALS
HOW
DISSIMILAR
TEXTU
B
FROM
WINDOW
A
IS
FROM
TEXTURE
IN
WINDOW
B
TEXTURE
REPRESENTATION
WINDOW
SCALE
WE
RE
ASSUMING
WE
KNOW
THE
RELEVANT
WINDOW
SIZE
FOR
WHICH
WE
COLLECT
THESE
STATISTICS
POSSIBLE
TO
PERFORM
SCALE
SELECTION
BY
LOOKING
FOR
WINDOW
SCALE
WHERE
TEXTURE
DESCRIPTION
NOT
CHANGING
OUR
PREVIOUS
EXAMPLE
USED
TWO
FILTERS
AND
RESULTED
IN
A
DIMENSIONAL
FEATURE
VECTOR
TO
DESCRIBE
TEXTURE
IN
A
WINDOW
X
AND
Y
DERIVATIVES
REVEALED
SOMETHING
ABOUT
LOCAL
STRUCTURE
WE
CAN
GENERALIZE
TO
APPLY
A
COLLECTION
OF
MULTIPLE
D
FILTERS
A
FILTER
BANK
THEN
OUR
FEATURE
VECTORS
WILL
BE
D
DIMENSIONAL
STILL
CAN
THINK
OF
NEARNESS
FARNESS
IN
FEATURE
SPACE
SCALES
WHAT
FILTERS
TO
PUT
IN
THE
BANK
TYPICALLY
WE
WANT
A
COMBINATION
OF
SCALES
AND
ORIENTATIONS
DIFFERENT
TYPES
OF
PATTERNS
MATLAB
CODE
AVAILABLE
FOR
THESE
EXAMPLES
MULTIVARIATE
GAUSSIAN
FILTER
BANK
D
D
D
D
D
EJ
I
L
L
L
J
LJ
J
IMAGE
FROM
YOU
TRY
CAN
YOU
MATCH
THE
TEXTURE
TO
THE
RESPONSE
FILTERS
A
MEAN
ABS
RESPONSES
B
C
DEREK
HOIEM
REPRESENTING
TEXTURE
BY
MEAN
ABS
RESPONSE
FILTERS
MEAN
ABS
RESPONSES
DEREK
HOIEM
WE
CAN
FORM
A
FEATURE
VECTOR
FROM
THE
LIST
OF
RESPONSES
AT
EACH
PIXEL
D
DIMENSIONAL
FEATURES
D
A
B
EUCLIDEAN
DISTANCE
EXAMPLE
USES
OF
TEXTURE
IN
VISION
ANALYSIS
CLASSIFYING
MATERIALS
STUFF
FIGURE
BY
VARMA
ZISSERMAN
TEXTURE
FEATURES
FOR
IMAGE
RETRIEVAL
Y
RUBNER
C
TOMASI
AND
L
J
GUIBAS
THE
EARTH
MOVER
DISTANCE
AS
A
METRIC
FOR
IMAGE
RETRIEVAL
INTERNATIONAL
JOURNAL
OF
COMPUTER
VISION
NOVEMBER
CHARACTERIZING
SCENE
CATEGORIES
BY
TEXTURE
L
W
RENNINGER
AND
J
MALIK
WHEN
IS
SCENE
IDENTIFICATION
JUST
TEXTURE
RECOGNITION
VISION
RESEARCH
KRISTEN
GRAUMAN
SEGMENTING
AERIAL
IMAGERY
BY
TEXTURES
KRISTEN
GRAUMAN
CS
INTRO
TO
COMPUTER
VISION
TEXTURE
AND
OTHER
USES
OF
FILTERS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
SLIDES
FROM
KRISTEN
GRAUMAN
AND
DEREK
HOIEM
PLAN
FOR
TODAY
TEXTURE
CONT
D
REVIEW
OF
TEXTURE
DESCRIPTION
TEXTURE
SYNTHESIS
USES
OF
FILTERS
SAMPLING
TEMPLATE
MATCHING
READING
FOR
TODAY
SZELISKI
SEC
FOR
NEXT
TIME
SZELISKI
SEC
PAGES
GET
STARTED
NOW
ON
READING
FOR
PAGES
I
WILL
FINALIZE
THE
READING
FOR
EACH
CLASS
BY
THE
DAY
OF
THE
CLASS
PRECEDING
IT
READINGS
FINALIZED
UNTIL
INCLUSIVE
F
CONVOLUTION
U
V
H
I
J
F
CONVOLUTION
U
V
V
H
I
J
F
CONVOLUTION
U
V
V
V
H
I
J
F
CONVOLUTION
U
V
V
V
U
V
H
I
J
CROSS
CORRELATION
U
V
F
H
I
J
CROSS
CORRELATION
U
V
V
F
H
I
J
CROSS
CORRELATION
U
V
V
V
F
H
I
J
CROSS
CORRELATION
F
I
J
U
V
V
V
U
V
H
NO
NEW
PIXEL
VALUES
INTRODUCED
REMOVES
SPIKES
GOOD
FOR
IMPULSE
SALT
PEPPER
NOISE
NON
LINEAR
FILTER
MEDIAN
FILTER
IS
EDGE
PRESERVING
SALT
AND
PEPPER
NOISE
PLOTS
OF
A
ROW
OF
THE
IMAGE
MATLAB
OUTPUT
IM
IM
H
W
MEDIAN
FILTERED
SOURCE
M
HEBERT
WHAT
DEFINES
A
TEXTURE
TEXTURE
REPRESENTATION
TEXTURES
ARE
MADE
UP
OF
REPEATED
LOCAL
PATTERNS
SO
FIND
THE
PATTERNS
USE
FILTERS
THAT
LOOK
LIKE
PATTERNS
SPOTS
BARS
RAW
PATCHES
CONSIDER
MAGNITUDE
OF
RESPONSE
DESCRIBE
THEIR
STATISTICS
WITHIN
EACH
LOCAL
WINDOW
MEAN
STANDARD
DEVIATION
HISTOGRAM
KRISTEN
GRAUMAN
ORIGINAL
IMAGE
ORIGINAL
IMAGE
ORIGINAL
IMAGE
DIMENSION
MEAN
D
DX
VALUE
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
WITH
PRIMARILY
HORIZONTAL
EDGES
BOTH
WINDOWS
WITH
SMALL
GRADIENT
IN
BOTH
DIRECTIONS
WINDOWS
WITH
PRIMARILY
VERTICAL
EDGES
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
ORIGINAL
IMAGE
KRISTEN
GRAUMAN
DERIVATIVE
FILTER
RESPONSES
SQUARED
OUR
PREVIOUS
EXAMPLE
USED
TWO
FILTERS
AND
RESULTED
IN
A
DIMENSIONAL
FEATURE
VECTOR
TO
DESCRIBE
TEXTURE
IN
A
WINDOW
X
AND
Y
DERIVATIVES
REVEALED
SOMETHING
ABOUT
LOCAL
STRUCTURE
WE
CAN
GENERALIZE
TO
APPLY
A
COLLECTION
OF
MULTIPLE
FILTERS
A
FILTER
BANK
THEN
OUR
FEATURE
VECTORS
WILL
BE
D
DIMENSIONAL
SCALES
WHAT
FILTERS
TO
PUT
IN
THE
BANK
TYPICALLY
WE
WANT
A
COMBINATION
OF
SCALES
AND
ORIENTATIONS
DIFFERENT
TYPES
OF
PATTERNS
MATLAB
CODE
AVAILABLE
FOR
THESE
EXAMPLES
REPRESENTING
TEXTURE
BY
MEAN
ABS
RESPONSE
FILTERS
MEAN
ABS
RESPONSES
DEREK
HOIEM
WE
CAN
FORM
A
FEATURE
VECTOR
FROM
THE
LIST
OF
RESPONSES
AT
EACH
PIXEL
KRISTEN
GRAUMAN
TEXTURE
RELATED
TASKS
SHAPE
FROM
TEXTURE
ESTIMATE
SURFACE
ORIENTATION
OR
SHAPE
FROM
IMAGE
TEXTURE
SEGMENTATION
CLASSIFICATION
FROM
TEXTURE
CUES
ANALYZE
REPRESENT
TEXTURE
GROUP
IMAGE
REGIONS
WITH
CONSISTENT
TEXTURE
TEXTURE
SYNTHESIS
GOAL
CREATE
NEW
SAMPLES
OF
A
GIVEN
TEXTURE
MANY
APPLICATIONS
VIRTUAL
ENVIRONMENTS
HOLE
FILLING
TEXTURING
SURFACES
THE
CHALLENGE
NEED
TO
MODEL
THE
WHOLE
SPECTRUM
FROM
REPEATED
TO
STOCHASTIC
TEXTURE
ALEXEI
A
EFROS
AND
THOMAS
K
LEUNG
TEXTURE
SYNTHESIS
BY
NON
PARAMETRIC
SAMPLING
PROC
INTERNATIONAL
CONFERENCE
ON
COMPUTER
VISION
ICCV
REPEATED
STOCHASTIC
BOTH
MARKOV
CHAIN
A
SEQUENCE
OF
RANDOM
VARIABLES
IS
THE
STATE
OF
THE
MODEL
AT
TIME
T
A
DOG
IS
A
MAN
BEST
FRIEND
IT
A
DOG
EAT
DOG
WORLD
OUT
THERE
A
DOG
IS
MAN
BEST
FRIEND
IT
EAT
WORLD
OUT
THERE
A
CREATE
PLAUSIBLE
LOOKING
POETRY
LOVE
LETTERS
TERM
PAPERS
ETC
MOST
BASIC
ALGORITHM
BUILD
PROBABILITY
HISTOGRAM
FIND
ALL
BLOCKS
OF
N
CONSECUTIVE
WORDS
LETTERS
IN
TRAINING
DOCUMENTS
WE
NEED
TO
EAT
CAKE
RESULTS
AS
I
VE
COMMENTED
BEFORE
REALLY
RELATING
TO
SOMEONE
INVOLVES
STANDING
NEXT
TO
IMPOSSIBLE
ONE
MORNING
I
SHOT
AN
ELEPHANT
IN
MY
ARMS
AND
KISSED
HIM
I
SPENT
AN
INTERESTING
EVENING
RECENTLY
WITH
A
GRAIN
OF
SALT
DEWDNEY
A
POTPOURRI
OF
PROGRAMMED
PROSE
AND
PROSODY
SCIENTIFIC
AMERICAN
SLIDE
FROM
ALYOSHA
EFROS
ICCV
SYNTHESIZING
COMPUTER
VISION
TEXT
WHAT
DO
WE
GET
IF
WE
EXTRACT
THE
PROBABILITIES
FROM
A
CHAPTER
ON
LINEAR
FILTERS
AND
THEN
SYNTHESIZE
NEW
STATEMENTS
CHECK
OUT
YISONG
YUE
WEBSITE
IMPLEMENTING
TEXT
GENERATION
BUILD
YOUR
OWN
TEXT
MARKOV
CHAIN
FOR
A
GIVEN
TEXT
CORPUS
SYNTHESIZED
TEXT
THIS
MEANS
WE
CANNOT
OBTAIN
A
SEPARATE
COPY
OF
THE
BEST
STUDIED
REGIONS
IN
THE
SUM
ALL
THIS
ACTIVITY
WILL
RESULT
IN
THE
PRIMATE
VISUAL
SYSTEM
THE
RESPONSE
IS
ALSO
GAUSSIAN
AND
HENCE
ISN
T
BANDLIMITED
INSTEAD
WE
NEED
TO
KNOW
ONLY
ITS
RESPONSE
TO
ANY
DATA
VECTOR
WE
NEED
TO
APPLY
A
LOW
PASS
FILTER
THAT
STRONGLY
REDUCES
THE
CONTENT
OF
THE
FOURIER
TRANSFORM
OF
A
VERY
LARGE
STANDARD
DEVIATION
IT
IS
CLEAR
HOW
THIS
INTEGRAL
EXIST
IT
IS
SUFFICIENT
FOR
ALL
PIXELS
WITHIN
A
REQUIRED
FOR
THE
IMAGES
SEPARATELY
MARKOV
RANDOM
FIELD
A
MARKOV
RANDOM
FIELD
MRF
GENERALIZATION
OF
MARKOV
CHAINS
TO
TWO
OR
MORE
DIMENSIONS
FIRST
ORDER
MRF
PROBABILITY
THAT
PIXEL
X
TAKES
A
CERTAIN
VALUE
GIVEN
THE
VALUES
OF
NEIGHBORS
A
B
C
AND
D
SOURCE
SEITZ
TEXTURE
SYNTHESIS
CAN
APPLY
VERSION
OF
TEXT
SYNTHESIS
TEXTURE
CORPUS
SAMPLE
OUTPUT
TEXTURE
SYNTHESIS
INTUITION
BEFORE
WE
INSERTED
THE
NEXT
WORD
BASED
ON
EXISTING
NEARBY
WORDS
NOW
WE
WANT
TO
INSERT
PIXEL
INTENSITIES
BASED
ON
EXISTING
NEARBY
PIXEL
VALUES
CORPUS
PLACE
WE
WANT
TO
INSERT
NEXT
DISTRIBUTION
OF
A
VALUE
OF
A
PIXEL
IS
CONDITIONED
ON
ITS
NEIGHBORS
ALONE
SYNTHESIZING
ONE
PIXEL
P
INPUT
IMAGE
SYNTHESIZED
IMAGE
WHAT
IS
FIND
ALL
THE
WINDOWS
IN
THE
IMAGE
THAT
MATCH
THE
NEIGHBORHOOD
TO
SYNTHESIZE
X
PICK
ONE
MATCHING
WINDOW
AT
RANDOM
ASSIGN
X
TO
BE
THE
CENTER
PIXEL
OF
THAT
WINDOW
AN
EXACT
NEIGHBOURHOOD
MATCH
MIGHT
NOT
BE
PRESENT
SO
FIND
THE
BEST
MATCHES
USING
SSD
ERROR
AND
RANDOMLY
CHOOSE
BETWEEN
THEM
PREFERRING
BETTER
MATCHES
WITH
HIGHER
PROBABILITY
SLIDE
FROM
ALYOSHA
EFROS
ICCV
INPUT
INCREASING
WINDOW
SIZE
FRENCH
CANVAS
RAFIA
WEAVE
WHITE
BREAD
BRICK
WALL
GROWING
TEXTURE
STARTING
FROM
THE
INITIAL
IMAGE
GROW
THE
TEXTURE
ONE
PIXEL
AT
A
TIME
HOLE
FILLING
EXTRAPOLATION
TEXTURE
SUMMARY
TEXTURE
IS
A
USEFUL
PROPERTY
THAT
IS
OFTEN
INDICATIVE
OF
MATERIALS
APPEARANCE
CUES
TEXTURE
REPRESENTATIONS
ATTEMPT
TO
SUMMARIZE
REPEATING
PATTERNS
OF
LOCAL
STRUCTURE
FILTER
BANKS
USEFUL
TO
MEASURE
REDUNDANT
VARIETY
OF
STRUCTURES
IN
LOCAL
NEIGHBORHOOD
FEATURE
SPACES
CAN
BE
MULTI
DIMENSIONAL
NEIGHBORHOOD
STATISTICS
CAN
BE
EXPLOITED
TO
SAMPLE
OR
SYNTHESIZE
NEW
TEXTURE
REGIONS
EXAMPLE
BASED
TECHNIQUE
KRISTEN
GRAUMAN
PLAN
FOR
TODAY
TEXTURE
CONT
D
REVIEW
OF
TEXTURE
DESCRIPTION
TEXTURE
SYNTHESIS
USES
OF
FILTERS
SAMPLING
TEMPLATE
MATCHING
SAMPLING
WHY
DOES
A
LOWER
RESOLUTION
IMAGE
STILL
MAKE
SENSE
TO
US
WHAT
DO
WE
LOSE
IMAGE
THROW
AWAY
EVERY
OTHER
ROW
AND
COLUMN
TO
CREATE
A
SIZE
IMAGE
EXAMPLE
SINEWAVE
EXAMPLE
SINEWAVE
SUB
SAMPLING
MAY
BE
DANGEROUS
CHARACTERISTIC
ERRORS
MAY
APPEAR
WAGON
WHEELS
ROLLING
THE
WRONG
WAY
IN
MOVIES
CHECKERBOARDS
DISINTEGRATE
IN
RAY
TRACING
STRIPED
SHIRTS
LOOK
FUNNY
ON
COLOR
TELEVISION
SAMPLING
AND
ALIASING
WHEN
SAMPLING
A
SIGNAL
AT
DISCRETE
INTERVALS
THE
SAMPLING
FREQUENCY
MUST
BE
FMAX
FMAX
MAX
FREQUENCY
OF
THE
INPUT
SIGNAL
THIS
WILL
ALLOWS
TO
RECONSTRUCT
THE
ORIGINAL
PERFECTLY
FROM
THE
SAMPLED
VERSION
GOOD
BAD
SOLUTIONS
SAMPLE
MORE
OFTEN
GET
RID
OF
ALL
FREQUENCIES
THAT
ARE
GREATER
THAN
HALF
THE
NEW
SAMPLING
FREQUENCY
WILL
LOSE
INFORMATION
BUT
IT
BETTER
THAN
ALIASING
APPLY
A
SMOOTHING
FILTER
ALGORITHM
FOR
DOWNSAMPLING
BY
FACTOR
OF
START
WITH
IMAGE
H
W
APPLY
LOW
PASS
FILTER
IMFILTER
IMAGE
FSPECIAL
GAUSSIAN
SAMPLE
EVERY
OTHER
PIXEL
END
END
ANTI
ALIASING
FORSYTH
AND
PONCE
SUBSAMPLING
WITHOUT
PRE
FILTERING
ZOOM
ZOOM
SUBSAMPLING
WITH
GAUSSIAN
PRE
FILTERING
GAUSSIAN
G
G
TEXTURE
CONT
D
REVIEW
OF
TEXTURE
DESCRIPTION
TEXTURE
SYNTHESIS
USES
OF
FILTERS
SAMPLING
TEMPLATE
MATCHING
GOAL
FIND
IN
IMAGE
MAIN
CHALLENGE
WHAT
IS
A
GOOD
SIMILARITY
OR
DISTANCE
MEASURE
BETWEEN
TWO
PATCHES
CORRELATION
ZERO
MEAN
CORRELATION
SUM
SQUARE
DIFFERENCE
NORMALIZED
CROSS
CORRELATION
GOAL
FIND
IN
IMAGE
METHOD
FILTER
THE
IMAGE
WITH
EYE
PATCH
H
M
N
G
K
L
K
L
F
M
K
N
L
F
IMAGE
G
FILTER
WHAT
WENT
WRONG
GOAL
FIND
IN
IMAGE
METHOD
FILTER
THE
IMAGE
WITH
ZERO
MEAN
EYE
H
M
N
G
K
L
G
F
M
K
N
L
K
L
MEAN
OF
TEMPLATE
G
GOAL
FIND
IN
IMAGE
METHOD
SSD
H
M
N
G
K
L
K
L
F
M
K
N
L
GOAL
FIND
IN
IMAGE
METHOD
SSD
WHAT
THE
POTENTIAL
DOWNSIDE
OF
SSD
H
M
N
G
K
L
K
L
F
M
K
N
L
GOAL
FIND
IN
IMAGE
METHOD
NORMALIZED
CROSS
CORRELATION
MEAN
TEMPLATE
MEAN
IMAGE
PATCH
H
M
N
G
K
L
K
L
G
F
M
K
N
L
FM
N
G
K
L
G
F
M
K
N
L
M
N
K
L
K
L
MATLAB
TEMPLATE
IM
GOAL
FIND
IN
IMAGE
METHOD
NORMALIZED
CROSS
CORRELATION
GOAL
FIND
IN
IMAGE
METHOD
NORMALIZED
CROSS
CORRELATION
A
DEPENDS
ZERO
MEAN
FILTER
FASTEST
BUT
NOT
A
GREAT
MATCHER
SSD
NEXT
FASTEST
SENSITIVE
TO
OVERALL
INTENSITY
NORMALIZED
CROSS
CORRELATION
SLOWEST
INVARIANT
TO
LOCAL
AVERAGE
INTENSITY
AND
CONTRAST
Q
WHAT
IF
WE
WANT
TO
FIND
LARGER
OR
SMALLER
EYES
A
IMAGE
PYRAMID
SAMPLING
GAUSSIAN
FILTER
SAMPLE
SOURCE
FORSYTH
INPUT
IMAGE
TEMPLATE
MATCH
TEMPLATE
AT
CURRENT
SCALE
DOWNSAMPLE
IMAGE
IN
PRACTICE
SCALE
STEP
OF
TO
REPEAT
UNTIL
IMAGE
IS
VERY
SMALL
TAKE
RESPONSES
ABOVE
SOME
THRESHOLD
LAPLACIAN
FILTER
UNIT
IMPULSE
GAUSSIAN
LAPLACIAN
OF
GAUSSIAN
CAN
WE
RECONSTRUCT
THE
ORIGINAL
FROM
THE
LAPLACIAN
PYRAMID
IMAGE
SMOOTH
THEN
DOWNSAMPLE
DOWNSAMPLE
SMOOTH
DOWNSAMPLE
SMOOTH
G
GN
LN
SMOOTH
UPSAMPLE
SMOOTH
UPSAMPLE
SMOOTH
UPSAMPLE
USE
SAME
FILTER
FOR
SMOOTHING
IN
EACH
STEP
E
G
GAUSSIAN
WITH
𝜎
DOWNSAMPLE
UPSAMPLE
WITH
NEAREST
INTERPOLATION
APPLICATION
HYBRID
IMAGES
AUDE
OLIVA
ANTONIO
TORRALBA
PHILIPPE
G
SCHYNS
SIGGRAPH
APPLICATION
HYBRID
IMAGES
GAUSSIAN
FILTER
A
OLIVA
A
TORRALBA
P
G
SCHYNS
SIGGRAPH
LAPLACIAN
FILTER
UNIT
IMPULSE
GAUSSIAN
LAPLACIAN
OF
GAUSSIAN
SLIDE
CREDIT
KRISTEN
GRAUMAN
USES
OF
FILTERS
SUMMARY
TEXTURE
DESCRIPTION
TEXTURE
SYNTHESIS
IMAGE
COMPRESSION
IMAGE
PYRAMIDS
TEMPLATE
MATCHING
USES
IN
OBJECT
RECOGNITION
DETECTING
STABLE
INTEREST
POINTS
SCALE
SEARCH
NEXT
TIME
EDGE
DETECTION
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
EDGE
DETECTION
BINARY
IMAGE
ANALYSIS
DUE
ON
REVIEW
SLIDES
FROM
REGARDING
HOW
TO
DO
PART
IV
THERE
ARE
DIFFERENT
WAYS
TO
COMPUTE
IMAGE
GRADIENTS
OFFICE
HOURS
FOR
WILL
BE
AT
ON
EDGE
DETECTION
GOAL
MAP
IMAGE
FROM
ARRAY
OF
PIXELS
TO
A
SET
OF
CURVES
OR
LINE
SEGMENTS
OR
CONTOURS
WHY
FIGURE
FROM
J
SHOTTON
ET
AL
PAMI
MAIN
IDEA
LOOK
FOR
STRONG
GRADIENTS
POST
PROCESS
ORIGIN
OF
EDGES
SURFACE
NORMAL
DISCONTINUITY
DEPTH
DISCONTINUITY
SURFACE
COLOR
DISCONTINUITY
ILLUMINATION
DISCONTINUITY
EDGES
ARE
CAUSED
BY
A
VARIETY
OF
FACTORS
WHAT
CAUSES
AN
EDGE
REFLECTANCE
CHANGE
APPEARANCE
INFORMATION
TEXTURE
DEPTH
DISCONTINUITY
OBJECT
BOUNDARY
CHANGE
IN
SURFACE
ORIENTATION
SHAPE
CAST
SHADOWS
EDGES
GRADIENTS
AND
INVARIANCE
AN
EDGE
IS
A
PLACE
OF
RAPID
CHANGE
IN
THE
IMAGE
INTENSITY
FUNCTION
IMAGE
INTENSITY
FUNCTION
ALONG
HORIZONTAL
SCANLINE
FIRST
DERIVATIVE
EDGES
CORRESPOND
TO
EXTREMA
OF
DERIVATIVE
INTENSITY
PROFILE
GRADIENT
CONSIDER
A
SINGLE
ROW
OR
COLUMN
OF
THE
IMAGE
PLOTTING
INTENSITY
AS
A
FUNCTION
OF
POSITION
GIVES
A
SIGNAL
WHERE
IS
THE
EDGE
DIFFERENCE
FILTERS
RESPOND
STRONGLY
TO
NOISE
IMAGE
NOISE
RESULTS
IN
PIXELS
THAT
LOOK
VERY
DIFFERENT
FROM
THEIR
NEIGHBORS
GENERALLY
THE
LARGER
THE
NOISE
THE
STRONGER
THE
RESPONSE
WHAT
CAN
WE
DO
ABOUT
IT
SOLUTION
SMOOTH
FIRST
F
G
F
G
D
F
DX
G
TO
FIND
EDGES
LOOK
FOR
PEAKS
IN
D
F
DX
G
DIFFERENTIATION
IS
CONVOLUTION
AND
CONVOLUTION
IS
ASSOCIATIVE
D
F
DX
G
F
D
G
DX
THIS
SAVES
US
ONE
OPERATION
F
D
G
DX
F
D
G
DX
IS
THIS
FILTER
SEPARABLE
TRADEOFF
BETWEEN
SMOOTHING
AND
LOCALIZATION
PIXEL
PIXELS
PIXELS
SMOOTHED
DERIVATIVE
REMOVES
NOISE
BUT
BLURS
EDGE
ALSO
FINDS
EDGES
AT
DIFFERENT
SCALES
SOURCE
D
FORSYTH
DESIGNING
AN
EDGE
DETECTOR
CRITERIA
FOR
A
GOOD
EDGE
DETECTOR
GOOD
DETECTION
FIND
ALL
REAL
EDGES
IGNORING
NOISE
OR
OTHER
ARTIFACTS
GOOD
LOCALIZATION
DETECT
EDGES
AS
CLOSE
AS
POSSIBLE
TO
THE
TRUE
EDGES
RETURN
ONE
POINT
ONLY
FOR
EACH
TRUE
EDGE
POINT
CUES
OF
EDGE
DETECTION
DIFFERENCES
IN
COLOR
INTENSITY
OR
TEXTURE
ACROSS
THE
BOUNDARY
CONTINUITY
AND
CLOSURE
HIGH
LEVEL
KNOWLEDGE
SOURCE
L
FEI
FEI
GRADIENTS
EDGES
PRIMARY
EDGE
DETECTION
STEPS
SMOOTHING
SUPPRESS
NOISE
EDGE
ENHANCEMENT
FILTER
FOR
CONTRAST
EDGE
LOCALIZATION
DETERMINE
WHICH
LOCAL
MAXIMA
FROM
FILTER
OUTPUT
ARE
ACTUALLY
EDGES
VS
NOISE
THRESHOLD
THIN
THRESHOLDING
CHOOSE
A
THRESHOLD
VALUE
T
SET
ANY
PIXELS
LESS
THAN
T
TO
ZERO
OFF
SET
ANY
PIXELS
GREATER
THAN
OR
EQUAL
TO
T
TO
ONE
ON
ORIGINAL
IMAGE
SOURCE
K
GRAUMAN
GRADIENT
MAGNITUDE
IMAGE
SOURCE
K
GRAUMAN
SOURCE
K
GRAUMAN
SOURCE
K
GRAUMAN
CANNY
EDGE
DETECTOR
THIS
IS
PROBABLY
THE
MOST
WIDELY
USED
EDGE
DETECTOR
IN
COMPUTER
VISION
THEORETICAL
MODEL
STEP
EDGES
CORRUPTED
BY
ADDITIVE
GAUSSIAN
NOISE
CANNY
HAS
SHOWN
THAT
THE
FIRST
DERIVATIVE
OF
THE
GAUSSIAN
CLOSELY
APPROXIMATES
THE
OPERATOR
THAT
OPTIMIZES
THE
PRODUCT
OF
SIGNAL
TO
NOISE
RATIO
AND
LOCALIZATION
CANNY
IEEE
TRANS
PATTERN
ANALYSIS
AND
MACHINE
INTELLIGENCE
SOURCE
L
FEI
FEI
CANNY
EDGE
DETECTOR
FILTER
IMAGE
WITH
DERIVATIVE
OF
GAUSSIAN
FIND
MAGNITUDE
AND
ORIENTATION
OF
GRADIENT
NON
MAXIMUM
SUPPRESSION
THIN
WIDE
RIDGES
DOWN
TO
SINGLE
PIXEL
WIDTH
LINKING
AND
THRESHOLDING
HYSTERESIS
DEFINE
TWO
THRESHOLDS
LOW
AND
HIGH
USE
THE
HIGH
THRESHOLD
TO
START
EDGE
CURVES
AND
THE
LOW
THRESHOLD
TO
CONTINUE
THEM
MATLAB
EDGE
IMAGE
CANNY
HELP
EDGE
SOURCE
D
LOWE
L
FEI
FEI
INPUT
IMAGE
LENA
DERIVATIVE
OF
GAUSSIAN
Y
DERIVATIVE
OF
GAUSSIAN
GRADIENT
MAGNITUDE
THRESHOLD
AT
MINIMUM
LEVEL
GET
ORIENTATION
THETA
GY
GX
NORM
OF
THE
GRADIENT
THRESHOLDING
HOW
TO
TURN
THESE
THICK
REGIONS
OF
THE
GRADIENT
INTO
CURVES
CHECK
IF
PIXEL
IS
LOCAL
MAXIMUM
ALONG
GRADIENT
DIRECTION
SELECT
SINGLE
MAX
ACROSS
WIDTH
OF
THE
EDGE
REQUIRES
CHECKING
INTERPOLATED
PIXELS
P
AND
R
BILINEAR
INTERPOLATION
THE
CANNY
EDGE
DETECTOR
PROBLEM
PIXELS
ALONG
THIS
EDGE
DIDN
T
SURVIVE
THE
THRESHOLDING
THINNING
NON
MAXIMUM
SUPPRESSION
USE
A
HIGH
THRESHOLD
TO
START
EDGE
CURVES
AND
A
LOW
THRESHOLD
TO
CONTINUE
THEM
ORIGINAL
IMAGE
HIGH
THRESHOLD
STRONG
EDGES
LOW
THRESHOLD
WEAK
EDGES
HYSTERESIS
THRESHOLD
HIGH
THRESHOLD
STRONG
EDGES
LOW
THRESHOLD
WEAK
EDGES
HYSTERESIS
THRESHOLD
FILTER
IMAGE
WITH
DERIVATIVE
OF
GAUSSIAN
FIND
MAGNITUDE
AND
ORIENTATION
OF
GRADIENT
NON
MAXIMUM
SUPPRESSION
THIN
WIDE
RIDGES
DOWN
TO
SINGLE
PIXEL
WIDTH
LINKING
AND
THRESHOLDING
HYSTERESIS
DEFINE
TWO
THRESHOLDS
LOW
AND
HIGH
USE
THE
HIGH
THRESHOLD
TO
START
EDGE
CURVES
AND
THE
LOW
THRESHOLD
TO
CONTINUE
THEM
MATLAB
EDGE
IMAGE
CANNY
HELP
EDGE
EFFECT
OF
GAUSSIAN
KERNEL
SPREAD
SIZE
ORIGINAL
CANNY
WITH
CANNY
WITH
THE
CHOICE
OF
DEPENDS
ON
DESIRED
BEHAVIOR
LARGE
DETECTS
LARGE
SCALE
EDGES
SMALL
DETECTS
FINE
FEATURES
SOURCE
SEITZ
LOW
LEVEL
EDGES
VS
PERCEIVED
CONTOURS
BACKGROUND
TEXTURE
SHADOWS
LOW
LEVEL
EDGES
VS
PERCEIVED
CONTOURS
IMAGE
HUMAN
SEGMENTATION
GRADIENT
MAGNITUDE
BERKELEY
SEGMENTATION
DATABASE
PB
BOUNDARY
DETECTOR
FIGURE
FROM
FOWLKES
LEARN
FROM
HUMANS
WHICH
COMBINATION
OF
FEATURES
IS
MOST
INDICATIVE
OF
A
GOOD
CONTOUR
D
MARTIN
ET
AL
PAMI
HUMAN
MARKED
SEGMENT
BOUNDARIES
PB
BOUNDARY
DETECTOR
FIGURE
FROM
FOWLKES
BRIGHTNESS
COLOR
TEXTURE
COMBINED
HUMAN
FOR
MORE
PLAN
FOR
TODAY
EDGE
DETECTION
BINARY
IMAGE
ANALYSIS
BINARY
IMAGES
BINARY
IMAGE
ANALYSIS
BASIC
STEPS
CONVERT
THE
IMAGE
INTO
BINARY
FORM
THRESHOLDING
CLEAN
UP
THE
THRESHOLDED
IMAGE
MORPHOLOGICAL
OPERATORS
EXTRACT
SEPARATE
BLOBS
CONNECTED
COMPONENTS
DESCRIBE
THE
BLOBS
WITH
REGION
PROPERTIES
TWO
PIXEL
VALUES
FOREGROUND
AND
BACKGROUND
MARK
REGION
OF
INTEREST
GRAYSCALE
BINARY
MASK
USEFUL
IF
OBJECT
OF
INTEREST
INTENSITY
DISTRIBUTION
IS
DISTINCT
FROM
BACKGROUND
SIMPLEBINARY
HTML
GIVEN
A
GRAYSCALE
IMAGE
OR
AN
INTERMEDIATE
MATRIX
THRESHOLD
TO
CREATE
A
BINARY
OUTPUT
EXAMPLE
EDGE
DETECTION
GRADIENT
MAGNITUDE
FIND
T
LOOKING
FOR
PIXELS
WHERE
GRADIENT
IS
STRONG
GIVEN
A
GRAYSCALE
IMAGE
OR
AN
INTERMEDIATE
MATRIX
THRESHOLD
TO
CREATE
A
BINARY
OUTPUT
EXAMPLE
BACKGROUND
SUBTRACTION
LOOKING
FOR
PIXELS
THAT
DIFFER
SIGNIFICANTLY
FROM
THE
EMPTY
BACKGROUND
SOURCE
K
GRAUMAN
FIND
DIFF
T
GIVEN
A
GRAYSCALE
IMAGE
OR
AN
INTERMEDIATE
MATRIX
THRESHOLD
TO
CREATE
A
BINARY
OUTPUT
EXAMPLE
INTENSITY
BASED
DETECTION
FIND
IM
LOOKING
FOR
DARK
PIXELS
GIVEN
A
GRAYSCALE
IMAGE
OR
AN
INTERMEDIATE
MATRIX
THRESHOLD
TO
CREATE
A
BINARY
OUTPUT
EXAMPLE
COLOR
BASED
DETECTION
FIND
HUE
HUE
LOOKING
FOR
PIXELS
WITHIN
A
CERTAIN
HUE
RANGE
A
NICE
CASE
BIMODAL
INTENSITY
HISTOGRAMS
IDEAL
HISTOGRAM
LIGHT
OBJECT
ON
DARK
BACKGROUND
ACTUAL
OBSERVED
HISTOGRAM
WITH
NOISE
ISSUES
WHAT
TO
DO
WITH
NOISY
BINARY
OUTPUTS
HOLES
EXTRA
SMALL
FRAGMENTS
HOW
TO
DEMARCATE
MULTIPLE
REGIONS
OF
INTEREST
COUNT
OBJECTS
COMPUTE
FURTHER
FEATURES
PER
OBJECT
MORPHOLOGICAL
OPERATORS
CHANGE
THE
SHAPE
OF
THE
FOREGROUND
REGIONS
VIA
INTERSECTION
UNION
OPERATIONS
BETWEEN
A
SCANNING
STRUCTURING
ELEMENT
AND
BINARY
IMAGE
USEFUL
TO
CLEAN
UP
RESULT
FROM
THRESHOLDING
BASIC
OPERATORS
ARE
DILATION
EROSION
EXPANDS
CONNECTED
COMPONENTS
GROW
FEATURES
FILL
HOLES
BEFORE
DILATION
AFTER
DILATION
ERODE
CONNECTED
COMPONENTS
SHRINK
FEATURES
REMOVE
BRIDGES
BRANCHES
NOISE
BEFORE
EROSION
AFTER
EROSION
MASKS
OF
VARYING
SHAPES
AND
SIZES
USED
TO
PERFORM
MORPHOLOGY
FOR
EXAMPLE
SCAN
MASK
ACROSS
FOREGROUND
PIXELS
TO
TRANSFORM
THE
BINARY
IMAGE
HELP
STREL
AT
EACH
POSITION
DILATION
IF
CURRENT
PIXEL
IS
FOREGROUND
OR
THE
STRUCTURING
ELEMENT
WITH
THE
INPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELE
G
X
F
X
SE
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
NOTE
THAT
THE
OBJECT
GETS
BIGGER
AND
HOLES
ARE
FILLED
HELP
IMDILATE
SOURCE
SHAPIRO
AND
STOCKMAN
AT
EACH
POSITION
DILATION
IF
CURRENT
PIXEL
IS
FOREGROUND
OR
THE
STRUCTURING
ELEMENT
WITH
THE
INPUT
IMAGE
EROSION
IF
EVERY
PIXEL
UNDER
THE
STRUCTURING
ELEMENT
NONZERO
ENTRIES
IS
FOREGROUND
OR
THE
CURRENT
PIXEL
WITH
INPUT
IMAGE
STRUCTURING
ELEMENT
G
X
F
X
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
G
X
F
X
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
INPUT
IMAGE
STRUCTURING
ELEMENT
OUTPUT
IMAGE
NOTE
THAT
THE
OBJECT
GETS
SMALLER
HELP
IMERODE
ERODE
THEN
DILATE
REMOVE
SMALL
OBJECTS
KEEP
ORIGINAL
SHAPE
BEFORE
OPENING
AFTER
OPENING
CLOSING
DILATE
THEN
ERODE
FILL
HOLES
BUT
KEEP
ORIGINAL
SHAPE
BEFORE
CLOSING
AFTER
CLOSING
APPLET
CAN
USE
FILTERS
TO
PROCESS
AND
DESCRIBE
LOCAL
NEIGHBORHOOD
DERIVATIVES
TO
LOCATE
GRADIENTS
CONVOLUTION
PROPERTIES
WILL
INFLUENCE
EFFICIENCY
EDGE
DETECTION
PROCESSES
THE
IMAGE
GRADIENT
TO
FIND
CURVES
CLEAN
UP
THRESHOLDING
OUTPUTS
WITH
BINARY
IMAGE
MORPHOLOGY
OPERATIONS
COMPUTING
IMAGE
FEATURES
DETECTING
AND
DESCRIBING
KEYPOINTS
CS
INTRO
TO
COMPUTER
VISION
LOCAL
IMAGE
FEATURES
EXTRACTION
AND
DESCRIPTION
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
WHAT
CS
WELCOME
PARTY
WHEN
FRIDAY
SEPTEMBER
AT
WHERE
SENSQ
WHY
TO
LEARN
ABOUT
THE
CS
DEPARTMENT
THE
CS
MAJOR
AND
OTHER
INTERESTING
USEFUL
THINGS
WHY
TO
MEET
OTHER
STUDENTS
AND
FACULTY
WHY
TO
HAVE
SOME
PIZZA
FEATURE
EXTRACTION
KEYPOINT
DETECTION
FEATURE
DESCRIPTION
HOMEWORK
DUE
ON
TUESDAY
OFFICE
HOURS
TODAY
MOVED
TO
TOMORROW
AN
IMAGE
IS
A
SET
OF
PIXELS
ADAPTED
FROM
NARASIMHAN
PROBLEMS
WITH
PIXEL
REPRESENTATION
NOT
INVARIANT
TO
SMALL
CHANGES
TRANSLATION
ILLUMINATION
ETC
SOME
PARTS
OF
AN
IMAGE
ARE
MORE
IMPORTANT
THAN
OTHERS
WHAT
DO
WE
WANT
TO
REPRESENT
NOTE
INTEREST
POINTS
KEYPOINTS
ALSO
SOMETIMES
CALLED
FEATURES
MANY
APPLICATIONS
TRACKING
WHICH
POINTS
ARE
GOOD
TO
TRACK
RECOGNITION
FIND
PATCHES
LIKELY
TO
TELL
US
SOMETHING
ABOUT
OBJECT
CATEGORY
RECONSTRUCTION
FIND
CORRESPONDENCES
ACROSS
DIFFERENT
VIEWS
YARBUS
EYE
TRACKING
SUPPOSE
YOU
HAVE
TO
CLICK
ON
SOME
POINT
GO
AWAY
AND
COME
BACK
AFTER
I
DEFORM
THE
IMAGE
AND
CLICK
ON
THE
SAME
POINTS
AGAIN
WHICH
POINTS
WOULD
YOU
CHOOSE
ORIGINAL
IF
YOU
WANTED
TO
MEET
A
FRIEND
WOULD
YOU
SAY
LET
MEET
ON
CAMPUS
LET
MEET
ON
GREEN
STREET
LET
MEET
AT
GREEN
AND
WRIGHT
CORNER
DETECTION
OR
IF
YOU
WERE
IN
A
SECLUDED
AREA
LET
MEET
IN
THE
PLAINS
OF
AKBAR
LET
MEET
ON
THE
SIDE
OF
MT
DOOM
LET
MEET
ON
TOP
OF
MT
DOOM
BLOB
VALLEY
PEAK
DETECTION
WHERE
WOULD
YOU
TELL
YOUR
FRIEND
TO
MEET
YOU
WHERE
WOULD
YOU
TELL
YOUR
FRIEND
TO
MEET
YOU
WHAT
POINTS
WOULD
YOU
CHOOSE
OVERVIEW
OF
KEYPOINT
MATCHING
D
F
A
FB
T
GRAUMAN
B
LEIBE
FIND
A
SET
OF
DISTINCTIVE
KEY
POINTS
DEFINE
A
REGION
AROUND
EACH
KEYPOINT
EXTRACT
AND
NORMALIZE
THE
REGION
CONTENT
COMPUTE
A
LOCAL
DESCRIPTOR
FROM
THE
NORMALIZED
REGION
MATCH
LOCAL
DESCRIPTORS
GOALS
FOR
KEYPOINTS
DETECT
POINTS
THAT
ARE
REPEATABLE
AND
DISTINCTIVE
KEY
TRADE
OFFS
DETECTION
MORE
REPEATABLE
MORE
POINTS
PRECISE
LOCALIZATION
ROBUST
TO
OCCLUSION
DESCRIPTION
MORE
DISTINCTIVE
MORE
FLEXIBLE
MINIMIZE
WRONG
MATCHES
ROBUST
TO
EXPECTED
VARIATIONS
MAXIMIZE
CORRECT
MATCHES
MOTIVATION
PANORAMA
STITCHING
WE
HAVE
TWO
IMAGES
HOW
DO
WE
COMBINE
THEM
MOTIVATION
PANORAMA
STITCHING
WE
HAVE
TWO
IMAGES
HOW
DO
WE
COMBINE
THEM
STEP
EXTRACT
FEATURES
STEP
MATCH
FEATURES
MOTIVATION
PANORAMA
STITCHING
WE
HAVE
TWO
IMAGES
HOW
DO
WE
COMBINE
THEM
STEP
EXTRACT
FEATURES
STEP
MATCH
FEATURES
STEP
ALIGN
IMAGES
GOAL
INTEREST
OPERATOR
REPEATABILITY
WE
WANT
TO
DETECT
AT
LEAST
SOME
OF
THE
SAME
POINTS
IN
BOTH
IMAGES
NO
CHANCE
TO
FIND
TRUE
MATCHES
YET
WE
HAVE
TO
BE
ABLE
TO
RUN
THE
DETECTION
PROCEDURE
INDEPENDENTLY
PER
IMAGE
GOAL
DESCRIPTOR
DISTINCTIVENESS
WE
WANT
TO
BE
ABLE
TO
RELIABLY
DETERMINE
WHICH
POINT
GOES
WITH
WHICH
MUST
PROVIDE
SOME
INVARIANCE
TO
GEOMETRIC
AND
PHOTOMETRIC
DIFFERENCES
BETWEEN
THE
TWO
VIEWS
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
WINDOW
FUNCTION
W
X
Y
OR
IN
WINDOW
OUTSIDE
GAUSSIAN
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
E
U
V
EXPANDING
I
X
Y
IN
A
TAYLOR
SERIES
EXPANSION
WE
HAVE
FOR
SMALL
SHIFTS
U
V
A
QUADRATIC
APPROXIMATION
TO
THE
ERROR
SURFACE
BETWEEN
A
PATCH
AND
ITSELF
SHIFTED
BY
U
V
WHERE
M
IS
A
MATRIX
COMPUTED
FROM
IMAGE
DERIVATIVES
M
W
X
Y
I
X
I
X
I
X
I
Y
X
Y
I
Y
I
Y
NOTATION
I
I
X
X
I
I
Y
Y
IX
I
Y
I
I
X
Y
FIRST
CONSIDER
AN
AXIS
ALIGNED
CORNER
FIRST
CONSIDER
AN
AXIS
ALIGNED
CORNER
I
I
I
M
I
I
X
Y
I
THIS
MEANS
DOMINANT
GRADIENT
DIRECTIONS
ALIGN
WITH
X
OR
Y
AXIS
LOOK
FOR
LOCATIONS
WHERE
BOTH
Λ
ARE
LARGE
IF
EITHER
Λ
IS
CLOSE
TO
THEN
THIS
IS
NOT
CORNER
LIKE
WHAT
IF
WE
HAVE
A
CORNER
THAT
IS
NOT
ALIGNED
WITH
THE
IMAGE
AXES
WHAT
DOES
THIS
MATRIX
REVEAL
T
SINCE
M
IS
SYMMETRIC
WE
HAVE
M
X
X
MXI
I
XI
THE
EIGENVALUES
OF
M
REVEAL
THE
AMOUNT
OF
INTENSITY
CHANGE
IN
THE
TWO
PRINCIPAL
ORTHOGONAL
GRADIENT
DIRECTIONS
IN
THE
WINDOW
CORNER
RESPONSE
FUNCTION
EDGE
CORNER
AND
ARE
LARGE
FLAT
REGION
AND
ARE
SMALL
HARRIS
DETECTOR
MATHEMATICS
MEASURE
OF
CORNER
RESPONSE
K
EMPIRICAL
CONSTANT
K
HARRIS
DETECTOR
SUMMARY
COMPUTE
IMAGE
GRADIENTS
IX
AND
IY
FOR
ALL
PIXELS
FOR
EACH
PIXEL
COMPUTE
BY
LOOPING
OVER
NEIGHBORS
X
Y
COMPUTE
K
EMPIRICAL
CONSTANT
K
FIND
POINTS
WITH
LARGE
CORNER
RESPONSE
FUNCTION
R
R
THRESHOLD
TAKE
THE
POINTS
OF
LOCALLY
MAXIMUM
R
AS
THE
DETECTED
FEATURE
POINTS
I
E
PIXELS
WHERE
R
IS
BIGGER
THAN
FOR
ALL
THE
OR
NEIGHBORS
COMPUTE
CORNER
RESPONSE
AT
EVERY
PIXEL
HARRIS
DETECTOR
RESPONSES
EFFECT
A
VERY
PRECISE
CORNER
DETECTOR
HARRIS
DETECTOR
RESPONSES
I
A
I
B
ONLY
DERIVATIVES
ARE
USED
INVARIANCE
TO
INTENSITY
SHIFT
I
I
B
INTENSITY
SCALING
I
A
I
R
R
THRESHOLD
X
IMAGE
COORDINATE
X
IMAGE
COORDINATE
DERIVATIVES
AND
WINDOW
FUNCTION
ARE
SHIFT
INVARIANT
SECOND
MOMENT
ELLIPSE
ROTATES
BUT
ITS
SHAPE
I
E
EIGENVALUES
REMAINS
THE
SAME
INVARIANT
TO
IMAGE
SCALE
IMAGE
ZOOMED
IMAGE
CORNER
ALL
POINTS
WILL
BE
CLASSIFIED
AS
EDGES
LAZEBNIK
SCALE
INVARIANT
DETECTION
THE
PROBLEM
HOW
DO
WE
CHOOSE
CORRESPONDING
CIRCLES
INDEPENDENTLY
IN
EACH
IMAGE
DO
OBJECTS
IN
THE
IMAGE
HAVE
A
CHARACTERISTIC
SCALE
THAT
WE
CAN
IDENTIFY
FROLOVA
D
SIMAKOV
F
IM
X
F
IM
X
HOW
TO
FIND
CORRESPONDING
PATCH
SIZES
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
F
IM
X
F
IM
X
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
F
IM
X
F
IM
X
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
F
IM
X
F
IM
X
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
F
IM
X
F
IM
X
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
F
IM
X
F
IM
X
FUNCTION
RESPONSES
FOR
INCREASING
SCALE
SCALE
SIGNATURE
F
IM
X
F
IM
X
F
D
G
DX
F
D
G
DX
EDGE
DERIVATIVE
OF
GAUSSIAN
EDGE
MAX
OF
DERIVATIVE
F
D
G
D
F
G
DX
EDGE
SECOND
DERIVATIVE
OF
GAUSSIAN
EDGE
ZERO
CROSSING
OF
DERIVATIVE
EDGE
RIPPLE
BLOB
SUPERPOSITION
OF
TWO
RIPPLES
MAXIMUM
SPATIAL
SELECTION
THE
MAGNITUDE
OF
THE
LAPLACIAN
RESPONSE
WILL
ACHIEVE
A
MAXIMUM
AT
THE
CENTER
OF
THE
BLOB
PROVIDED
THE
SCALE
OF
THE
LAPLACIAN
IS
MATCHED
TO
THE
SCALE
OF
THE
BLOB
L
LAZEBNIK
LAPLACIAN
OF
GAUSSIAN
CIRCULARLY
SYMMETRIC
OPERATOR
FOR
BLOB
DETECTION
IN
G
G
G
GRAUMAN
WHAT
IS
A
USEFUL
SIGNATURE
FUNCTION
LAPLACIAN
OF
GAUSSIAN
BLOB
DETECTOR
K
GRAUMAN
B
LEIBE
WE
CAN
APPROXIMATE
THE
LAPLACIAN
WITH
A
DIFFERENCE
OF
GAUSSIANS
MORE
EFFICIENT
TO
IMPLEMENT
LAPLACIAN
DOG
G
X
Y
K
G
X
Y
DIFFERENCE
OF
GAUSSIANS
COMPUTATION
IN
GAUSSIAN
SCALE
PYRAMID
SAMPLING
WITH
STEP
ORIGINAL
IMAGE
FIND
LOCAL
MAXIMA
IN
POSITION
SCALE
SPACE
OF
DIFFERENCE
OF
GAUSSIAN
LXX
LYY
LIST
OF
X
Y
LOCAL
FEATURES
DESIRED
PROPERTIES
REPEATABILITY
THE
SAME
FEATURE
CAN
BE
FOUND
IN
SEVERAL
IMAGES
DESPITE
GEOMETRIC
AND
PHOTOMETRIC
TRANSFORMATIONS
SALIENCY
EACH
FEATURE
HAS
A
DISTINCTIVE
DESCRIPTION
COMPACTNESS
AND
EFFICIENCY
MANY
FEWER
FEATURES
THAN
IMAGE
PIXELS
LOCALITY
A
FEATURE
OCCUPIES
A
RELATIVELY
SMALL
AREA
OF
THE
IMAGE
ROBUST
TO
CLUTTER
AND
OCCLUSION
PLAN
FOR
TODAY
FEATURE
EXTRACTION
KEYPOINT
DETECTION
CORNERS
BLOBS
FEATURE
DESCRIPTION
E
G
SCALE
TRANSLATION
ROTATION
TUYTELAARS
THE
IDEAL
DESCRIPTOR
SHOULD
BE
ROBUST
DISTINCTIVE
COMPACT
EFFICIENT
MOST
AVAILABLE
DESCRIPTORS
FOCUS
ON
EDGE
GRADIENT
INFORMATION
CAPTURE
TEXTURE
INFORMATION
COLOR
RARELY
USED
LOWE
ICCV
HISTOGRAM
OF
ORIENTED
GRADIENTS
CAPTURES
IMPORTANT
TEXTURE
INFORMATION
ROBUST
TO
SMALL
TRANSLATIONS
AFFINE
DEFORMATIONS
RUN
DIFFERENCE
OF
GAUSSIAN
KEYPOINT
DETECTOR
FIND
MAXIMA
IN
LOCATION
SCALE
SPACE
FIND
ALL
MAJOR
ORIENTATIONS
BIN
ORIENTATIONS
WEIGHT
BY
GRADIENT
MAGNITUDE
WEIGHT
BY
DISTANCE
TO
CENTER
GAUSSIAN
WEIGHTED
MEAN
RETURN
ORIENTATIONS
WITHIN
OF
PEAK
FOR
EACH
X
Y
SCALE
ORIENTATION
CREATE
DESCRIPTOR
SAMPLE
GRADIENT
MAGNITUDE
AND
RELATIVE
ORIENTATION
BIN
SAMPLES
INTO
HISTOGRAMS
THRESHOLD
VALUES
TO
MAX
OF
DIVIDE
BY
NORM
FINAL
DESCRIPTOR
NORMALIZED
HISTOGRAMS
LOWE
IJCV
SCALE
INVARIANT
FEATURE
TRANSFORM
BASIC
IDEA
TAKE
SQUARE
WINDOW
AROUND
DETECTED
FEATURE
COMPUTE
GRADIENT
ORIENTATION
FOR
EACH
PIXEL
CREATE
HISTOGRAM
OVER
EDGE
ORIENTATIONS
WEIGHTED
BY
MAGNITUDE
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
COMPUTE
AN
ORIENTATION
HISTOGRAM
FOR
EACH
CELL
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
COMPUTE
AN
ORIENTATION
HISTOGRAM
FOR
EACH
CELL
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
THRESHOLD
NORMALIZE
THE
DESCRIPTOR
SUCH
THAT
MAKING
DESCRIPTOR
ROTATION
INVARIANT
ROTATE
PATCH
ACCORDING
TO
ITS
DOMINANT
GRADIENT
ORIENTATION
THIS
PUTS
THE
PATCHES
INTO
A
CANONICAL
ORIENTATION
GRAUMAN
IMAGE
FROM
MATTHEW
BROWN
SIFT
DESCRIPTOR
LOWE
EXTRAORDINARILY
ROBUST
MATCHING
TECHNIQUE
CAN
HANDLE
CHANGES
IN
VIEWPOINT
UP
TO
ABOUT
DEGREE
OUT
OF
PLANE
ROTATION
CAN
HANDLE
SIGNIFICANT
CHANGES
IN
ILLUMINATION
SOMETIMES
EVEN
DAY
VS
NIGHT
BELOW
FAST
AND
EFFICIENT
CAN
RUN
IN
REAL
TIME
LOTS
OF
CODE
AVAILABLE
SEITZ
MATCHING
LOCAL
FEATURES
IMAGE
IMAGE
TO
GENERATE
CANDIDATE
MATCHES
FIND
PATCHES
THAT
HAVE
THE
MOST
SIMILAR
APPEARANCE
E
G
LOWEST
SSD
SIMPLEST
APPROACH
COMPARE
THEM
ALL
TAKE
THE
CLOSEST
OR
CLOSEST
K
OR
WITHIN
A
THRESHOLDED
DISTANCE
GRAUMAN
AMBIGUOUS
MATCHES
IMAGE
IMAGE
AT
WHAT
SSD
VALUE
DO
WE
HAVE
A
GOOD
MATCH
TO
ADD
ROBUSTNESS
TO
MATCHING
CAN
CONSIDER
RATIO
DISTANCE
TO
BEST
MATCH
DISTANCE
TO
SECOND
BEST
MATCH
IF
LOW
FIRST
MATCH
LOOKS
GOOD
K
GRAUMAN
IF
HIGH
COULD
BE
AMBIGUOUS
MATCH
NEAREST
NEIGHBOR
EUCLIDEAN
DISTANCE
THRESHOLD
RATIO
OF
NEAREST
TO
NEAREST
DESCRIPTOR
EXTRACT
FEATURES
EXTRACT
FEATURES
COMPUTE
PUTATIVE
MATCHES
EXTRACT
FEATURES
COMPUTE
PUTATIVE
MATCHES
LOOP
HYPOTHESIZE
TRANSFORMATION
T
SMALL
GROUP
OF
PUTATIVE
MATCHES
THAT
ARE
RELATED
BY
T
EXTRACT
FEATURES
COMPUTE
PUTATIVE
MATCHES
LOOP
HYPOTHESIZE
TRANSFORMATION
T
SMALL
GROUP
OF
PUTATIVE
MATCHES
THAT
ARE
RELATED
BY
T
VERIFY
TRANSFORMATION
SEARCH
FOR
OTHER
MATCHES
CONSISTENT
WITH
T
EXTRACT
FEATURES
COMPUTE
PUTATIVE
MATCHES
LOOP
HYPOTHESIZE
TRANSFORMATION
T
SMALL
GROUP
OF
PUTATIVE
MATCHES
THAT
ARE
RELATED
BY
T
VERIFY
TRANSFORMATION
SEARCH
FOR
OTHER
MATCHES
CONSISTENT
WITH
T
APPLICATIONS
OF
LOCAL
INVARIANT
FEATURES
IMAGE
ALIGNMENT
INDEXING
AND
RETRIEVAL
RECOGNITION
ROBOT
NAVIGATION
RECONSTRUCTION
WIDE
BASELINE
STEREO
PANORAMAS
MOTION
TRACKING
RECOGNITION
OF
SPECIFIC
OBJECTS
SCENES
SCHMID
AND
MOHR
SIVIC
AND
ZISSERMAN
LOWE
GRAUMA
IMAGE
FROM
T
TUYTELAARS
ECCV
TUTORIAL
SUMMARY
KEYPOINT
DETECTION
REPEATABLE
AND
DISTINCTIVE
CORNERS
BLOBS
STABLE
REGIONS
LAPLACIAN
OF
GAUSSIAN
AUTOMATIC
SCALE
SELECTION
DESCRIPTORS
ROBUST
AND
SELECTIVE
HISTOGRAMS
FOR
ROBUSTNESS
TO
SMALL
SHIFTS
AND
TRANSLATIONS
SIFT
DESCRIPTOR
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
TODAY
REVIEW
SIFT
FEATURES
PHYSICS
AND
PERCEPTION
OF
COLOR
COLOR
MATCHING
COLOR
SPACES
USES
OF
COLOR
IN
COMPUTER
VISION
ANNOUNCEMENT
HOMEWORK
RELEASED
SMALL
CHANGES
MADE
HOMEWORK
DUE
TONIGHT
AT
REVIEW
LATE
POLICY
REMINDER
DO
NOT
LOOK
FOR
OR
USE
EXISTING
IMPLEMENTATIONS
HARRIS
DETECTOR
SUMMARY
COMPUTE
IMAGE
GRADIENTS
IX
AND
IY
FOR
ALL
PIXELS
FOR
EACH
PIXEL
COMPUTE
BY
LOOPING
OVER
NEIGHBORS
X
Y
COMPUTE
K
EMPIRICAL
CONSTANT
K
FIND
POINTS
WITH
LARGE
CORNER
RESPONSE
FUNCTION
R
R
THRESHOLD
TAKE
THE
POINTS
OF
LOCALLY
MAXIMUM
R
AS
THE
DETECTED
FEATURE
POINTS
I
E
PIXELS
WHERE
R
IS
BIGGER
THAN
FOR
ALL
THE
OR
NEIGHBORS
D
FROLOVA
D
SIMAKOV
EXAMPLE
OF
HARRIS
APPLICATION
GRAUMAN
LOWE
ICCV
HISTOGRAM
OF
ORIENTED
GRADIENTS
CAPTURES
IMPORTANT
TEXTURE
INFORMATION
ROBUST
TO
SMALL
TRANSLATIONS
AFFINE
DEFORMATIONS
K
GRAUMAN
B
LEIBE
TAN
Α
𝑜𝑝𝑝𝑜𝑠𝑖𝑡𝑒
𝑠𝑖𝑑𝑒
𝑎𝑑𝑗𝑎𝑐𝑒𝑛𝑡
𝑠𝑖𝑑𝑒
M
X
Y
SQRT
Θ
X
Y
ATAN
M
X
Y
SQRT
Θ
X
Y
ATAN
M
X
Y
SQRT
Θ
X
Y
ATAN
SCALE
INVARIANT
FEATURE
TRANSFORM
BASIC
IDEA
TAKE
SQUARE
WINDOW
AROUND
DETECTED
FEATURE
COMPUTE
GRADIENT
ORIENTATION
FOR
EACH
PIXEL
CREATE
HISTOGRAM
OVER
EDGE
ORIENTATIONS
WEIGHTED
BY
MAGNITUDE
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
COMPUTE
AN
ORIENTATION
HISTOGRAM
FOR
EACH
CELL
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
COMPUTE
AN
ORIENTATION
HISTOGRAM
FOR
EACH
CELL
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
THRESHOLD
NORMALIZE
THE
DESCRIPTOR
SUCH
THAT
MAKING
DESCRIPTOR
ROTATION
INVARIANT
ROTATE
PATCH
ACCORDING
TO
ITS
DOMINANT
GRADIENT
ORIENTATION
THIS
PUTS
THE
PATCHES
INTO
A
CANONICAL
ORIENTATION
GRAUMAN
IMAGE
FROM
MATTHEW
BROWN
TODAY
REVIEW
SIFT
FEATURES
PHYSICS
AND
PERCEPTION
OF
COLOR
COLOR
MATCHING
COLOR
SPACES
USES
OF
COLOR
IN
COMPUTER
VISION
COLOR
AND
LIGHT
COLOR
OF
LIGHT
ARRIVING
AT
CAMERA
DEPENDS
ON
SPECTRAL
REFLECTANCE
OF
THE
SURFACE
LIGHT
IS
LEAVING
SPECTRAL
RADIANCE
OF
LIGHT
FALLING
ON
THAT
PATCH
COLOR
PERCEIVED
DEPENDS
ON
PHYSICS
OF
LIGHT
VISUAL
SYSTEM
RECEPTORS
BRAIN
PROCESSING
ENVIRONMENT
GRAUMAN
THE
HUMAN
EYE
IS
A
CAMERA
LENS
CHANGES
SHAPE
BY
USING
CILIARY
MUSCLES
TO
FOCUS
ON
OBJECTS
AT
DIFFERENT
DISTANCES
PUPIL
THE
HOLE
APERTURE
WHOSE
SIZE
IS
CONTROLLED
BY
THE
IRIS
IRIS
COLORED
ANNULUS
WITH
RADIAL
MUSCLES
RETINA
PHOTORECEPTOR
CELLS
SLIDE
BY
STEVE
SEITZ
LIGHT
D
HOIEM
COLOR
SENSING
IN
CAMERAS
BAYER
GRID
ESTIMATE
RGB
AT
EACH
CELL
FROM
NEIGHBORING
VALUES
SLIDE
BY
STEVE
SEITZ
TWO
TYPES
OF
LIGHT
SENSITIVE
RECEPTORS
CONES
CONE
SHAPED
LESS
SENSITIVE
OPERATE
IN
HIGH
LIGHT
COLOR
VISION
RODS
ROD
SHAPED
HIGHLY
SENSITIVE
OPERATE
AT
NIGHT
GRAY
SCALE
VISION
SLOWER
TO
RESPOND
ROD
CONE
SENSITIVITY
DISTRIBUTION
OF
RODS
AND
CONES
RODS
RESPONSIBLE
FOR
INTENSITY
CONES
RESPONSIBLE
FOR
COLOR
FOVEA
SMALL
REGION
OR
AT
THE
CENTER
OF
THE
VISUAL
FIELD
CONTAINING
THE
HIGHEST
DENSITY
OF
CONES
AND
NO
RODS
LESS
VISUAL
ACUITY
IN
THE
PERIPHERY
NIGHT
SKY
WHY
ARE
THERE
MORE
STARS
OFF
CENTER
ADAPTED
FROM
A
EFROS
K
GRAUMAN
SEITZ
P
DUYGULU
ELECTROMAGNETIC
SPECTRUM
HUMAN
LUMINANCE
SENSITIVITY
FUNCTION
GRAUMAN
IMAGE
CREDIT
NASA
GOV
THE
PHYSICS
OF
LIGHT
ANY
PATCH
OF
LIGHT
CAN
BE
COMPLETELY
DESCRIBED
PHYSICALLY
BY
ITS
SPECTRUM
THE
NUMBER
OF
PHOTONS
PER
TIME
UNIT
AT
EACH
WAVELENGTH
NM
PHOTONS
PER
MS
WAVELENGTH
NM
STEPHEN
E
PALMER
THE
PHYSICS
OF
LIGHT
SOME
EXAMPLES
OF
THE
SPECTRA
OF
LIGHT
SOURCES
RUBY
LASER
B
GALLIUM
PHOSPHIDE
CRYSTAL
WAVELENGTH
NM
WAVELENGTH
NM
TUNGSTEN
LIGHTBULB
NORMAL
DAYLIGHT
STEPHEN
E
PALMER
THE
PHYSICS
OF
LIGHT
SOME
EXAMPLES
OF
THE
REFLECTANCE
SPECTRA
OF
SURFACES
WAVELENGTH
NM
STEPHEN
E
PALMER
PHYSIOLOGY
OF
COLOR
VISION
THREE
KINDS
OF
CONES
NM
M
L
WAVELENGTH
NM
WHY
ARE
M
AND
L
CONES
SO
CLOSE
STEPHEN
E
PALMER
IS
BETTER
THAN
M
AND
L
ON
THE
X
CHROMOSOME
WHY
MEN
ARE
MORE
LIKELY
TO
BE
COLOR
BLIND
SEE
WHAT
IT
LIKE
L
HAS
HIGH
VARIATION
SO
SOME
WOMEN
ARE
TETRACHROMATIC
SOME
ANIMALS
HAVE
NIGHT
ANIMALS
E
G
DOGS
FISH
BIRDS
PIGEONS
SOME
REPTILES
AMPHIBIANS
OR
EVEN
MANTIS
SHRIMP
D
HOIEM
HUMAN
PHOTORECEPTORS
POSSIBLE
EVOLUTIONARY
PRESSURE
FOR
DEVELOPING
RECEPTORS
FOR
DIFFERENT
WAVELENGTHS
IN
PRIMATES
OSORIO
VOROBYEV
K
GRAUMAN
MEASURING
SPECTRA
SPECTRORADIOMETER
SEPARATE
INPUT
LIGHT
INTO
ITS
DIFFERENT
WAVELENGTHS
AND
MEASURE
THE
ENERGY
AT
EACH
K
GRAUMAN
FOUNDATIONS
OF
VISION
B
WANDELL
METAMERS
SPECTRAL
REFLECTANCES
FOR
SOME
NATURAL
OBJECTS
HOW
MUCH
OF
EACH
WAVELENGTH
IS
REFLECTED
FOR
THAT
SURFACE
GRAUMAN
FORSYTH
PONCE
MEASUREMENTS
BY
E
KOIVISTO
WE
DON
T
PERCEIVE
A
SPECTRUM
OR
EVEN
RGB
WE
PERCEIVE
HUE
MEAN
WAVELENGTH
COLOR
SATURATION
VARIANCE
VIVIDNESS
INTENSITY
TOTAL
AMOUNT
OF
LIGHT
SAME
PERCEIVED
COLOR
CAN
BE
RECREATED
WITH
COMBINATIONS
OF
THREE
PRIMARY
COLORS
TRICHROMACY
D
HOIEM
COLOR
MIXING
CARTOON
SPECTRA
FOR
COLOR
NAMES
ADDITIVE
COLOR
MIXING
COLORS
COMBINE
BY
ADDING
COLOR
SPECTRA
LIGHT
ADDS
TO
BLACK
EXAMPLES
OF
ADDITIVE
COLOR
SYSTEMS
CRT
PHOSPHORS
MULTIPLE
PROJECTORS
GRAUMAN
SUBTRACTIVE
COLOR
MIXING
COLORS
COMBINE
BY
MULTIPLYING
COLOR
SPECTRA
PIGMENTS
REMOVE
COLOR
FROM
INCIDENT
LIGHT
WHITE
SOURCE
W
FREEMAN
EXAMPLES
OF
SUBTRACTIVE
COLOR
SYSTEMS
PRINTING
ON
PAPER
CRAYONS
MOST
PHOTOGRAPHIC
FILM
GRAUMAN
FUN
WITH
COLOR
BRIGHTNESS
PERCEPTION
EDWARD
ADELSON
EDWARD
ADELSON
EDWARD
ADELSON
COLOR
CONSTANCY
INTERPRET
SURFACE
IN
TERMS
OF
TRUE
COLOR
RATHER
THAN
OBSERVED
INTENSITY
HUMANS
ARE
GOOD
AT
IT
COMPUTERS
ARE
NOT
NEARLY
AS
GOOD
LOOK
AT
BLUE
SQUARES
CONTENT
R
BEAU
LOTTO
LOOK
AT
YELLOW
SQUARES
CONTENT
R
BEAU
LOTTO
CONTENT
R
BEAU
LOTTO
CONTENT
R
BEAU
LOTTO
CONTENT
R
BEAU
LOTTO
CONTENT
R
BEAU
LOTTO
NAME
THAT
COLOR
HIGH
LEVEL
INTERACTIONS
AFFECT
PERCEPTION
AND
PROCESSING
REASONS
FOR
ILLUSIONS
CHROMATIC
ADAPTATION
WE
ADAPT
TO
A
PARTICULAR
ILLUMINANT
ASSIMILATION
CONTRAST
EFFECTS
CHROMATIC
INDUCTION
NEARBY
COLORS
AFFECT
WHAT
IS
PERCEIVED
RECEPTOR
EXCITATIONS
INTERACT
ACROSS
IMAGE
AND
TIME
AFTERIMAGES
TIRED
RECEPTORS
PRODUCE
NEGATIVE
RESPONSE
COLOR
MATCHING
COLOR
APPEARANCE
PHYSICS
OF
LIGHT
PERCEPTION
OF
LIGHT
CHROMATIC
ADAPTATION
THE
VISUAL
SYSTEM
CHANGES
ITS
SENSITIVITY
DEPENDING
ON
THE
LUMINANCES
PREVAILING
IN
THE
VISUAL
FIELD
THE
EXACT
MECHANISM
IS
POORLY
UNDERSTOOD
ADAPTING
TO
DIFFERENT
BRIGHTNESS
LEVELS
CHANGING
THE
SIZE
OF
THE
IRIS
OPENING
I
E
THE
APERTURE
CHANGES
THE
AMOUNT
OF
LIGHT
THAT
CAN
ENTER
THE
EYE
THINK
OF
WALKING
INTO
A
BUILDING
FROM
FULL
SUNSHINE
ADAPTING
TO
DIFFERENT
COLOR
TEMPERATURE
THE
RECEPTIVE
CELLS
ON
THE
RETINA
CHANGE
THEIR
SENSITIVITY
FOR
EXAMPLE
IF
THERE
IS
AN
INCREASED
AMOUNT
OF
RED
LIGHT
THE
CELLS
RECEPTIVE
TO
RED
DECREASE
THEIR
SENSITIVITY
UNTIL
THE
SCENE
LOOKS
WHITE
AGAIN
WE
ACTUALLY
ADAPT
BETTER
IN
BRIGHTER
SCENES
THIS
IS
WHY
CANDLELIT
SCENES
STILL
LOOK
YELLOW
TODAY
REVIEW
SIFT
FEATURES
PHYSICS
AND
PERCEPTION
OF
COLOR
COLOR
MATCHING
COLOR
SPACES
USES
OF
COLOR
IN
COMPUTER
VISION
GOAL
FIND
OUT
WHAT
SPECTRAL
RADIANCES
PRODUCE
SAME
RESPONSE
IN
HUMAN
OBSERVERS
OBSERVER
ADJUSTS
WEIGHT
INTENSITY
FOR
PRIMARY
LIGHTS
FIXED
SPD
TO
MATCH
APPEARANCE
OF
TEST
LIGHT
FOUNDATIONS
OF
VISION
BY
BRIAN
WANDELL
SINAUER
ASSOC
AFTER
JUDD
WYSZECKI
GOAL
FIND
OUT
WHAT
SPECTRAL
RADIANCES
PRODUCE
SAME
RESPONSE
IN
HUMAN
OBSERVERS
ASSUMPTION
SIMPLE
VIEWING
CONDITIONS
WHERE
WE
SAY
TEST
LIGHT
ALONE
AFFECTS
PERCEPTION
IGNORING
ADDITIONAL
FACTORS
FOR
NOW
LIKE
ADAPTATION
COMPLEX
SURROUNDING
SCENES
ETC
K
GRAUMAN
SLIDE
CREDIT
FREEMAN
THE
PRIMARY
COLOR
AMOUNTS
NEEDED
FOR
A
MATCH
WE
SAY
A
NEGATIVE
AMOUNT
OF
WAS
NEEDED
TO
MAKE
THE
MATCH
BECAUSE
WE
ADDED
IT
TO
THE
TEST
COLOR
SIDE
THE
PRIMARY
COLOR
AMOUNTS
NEEDED
FOR
A
MATCH
WHAT
MUST
WE
REQUIRE
OF
THE
PRIMARY
LIGHTS
CHOSEN
HOW
ARE
THREE
NUMBERS
ENOUGH
TO
REPRESENT
ENTIRE
SPECTRUM
TRICHROMACY
IN
COLOR
MATCHING
EXPERIMENTS
MOST
PEOPLE
CAN
MATCH
ANY
GIVEN
LIGHT
WITH
THREE
PRIMARIES
PRIMARIES
MUST
BE
INDEPENDENT
FOR
THE
SAME
LIGHT
AND
SAME
PRIMARIES
MOST
PEOPLE
SELECT
THE
SAME
WEIGHTS
EXCEPTION
COLOR
BLINDNESS
TRICHROMATIC
COLOR
THEORY
THREE
NUMBERS
SEEM
TO
BE
SUFFICIENT
FOR
ENCODING
COLOR
DATES
BACK
TO
CENTURY
THOMAS
YOUNG
GRASSMAN
LAWS
IF
TWO
TEST
LIGHTS
CAN
BE
MATCHED
WITH
THE
SAME
SET
OF
WEIGHTS
THEN
THEY
MATCH
EACH
OTHER
SUPPOSE
A
AND
B
THEN
A
B
IF
WE
SCALE
THE
TEST
LIGHT
THEN
THE
MATCHES
GET
SCALED
BY
THE
SAME
AMOUNT
SUPPOSE
A
THEN
KA
IF
WE
MIX
TWO
TEST
LIGHTS
THEN
MIXING
THE
MATCHES
WILL
MATCH
THE
RESULT
SUPERPOSITION
SUPPOSE
A
AND
B
THEN
A
B
HERE
MEANS
MATCHES
HOW
DO
WE
COMPUTE
THE
WEIGHTS
THAT
WILL
YIELD
A
PERCEPTUAL
MATCH
FOR
ANY
TEST
LIGHT
USING
A
GIVEN
SET
OF
PRIMARIES
SELECT
PRIMARIES
ESTIMATE
THEIR
COLOR
MATCHING
FUNCTIONS
OBSERVER
MATCHES
SERIES
OF
MONOCHROMATIC
LIGHTS
ONE
AT
EACH
WAVELENGTH
C
C
COLOR
MATCHING
FUNCTIONS
FOR
A
PARTICULAR
SET
OF
PRIMARIES
NM
NM
NM
ROWS
OF
MATRIX
C
FOUNDATIONS
OF
VISION
BY
BRIAN
WANDELL
SINAUER
ASSOC
SLIDE
CREDIT
W
FREEMAN
I
MATCHES
I
I
I
NOW
HAVE
MATCHING
FUNCTIONS
FOR
ALL
MONOCHROMATIC
LIGHT
SOURCES
SO
WE
KNOW
HOW
TO
MATCH
A
UNIT
OF
EACH
WAVELENGTH
ARBITRARY
NEW
SPECTRAL
SIGNAL
IS
A
LINEAR
COMBINATION
OF
THE
MONOCHROMATIC
SOURCES
T
T
K
GRAUMAN
T
SO
GIVEN
ANY
SET
OF
PRIMARIES
AND
THEIR
ASSOCIATED
MATCHING
FUNCTIONS
C
WE
CAN
COMPUTE
WEIGHTS
E
NEEDED
ON
EACH
PRIMARY
TO
GIVE
A
PERCEPTUAL
MATCH
TO
ANY
TEST
LIGHT
T
SPECTRAL
SIGNAL
GRAUMAN
FIG
FROM
B
WANDELL
WHY
IS
COMPUTING
THE
COLOR
MATCH
FOR
ANY
COLOR
SIGNAL
FOR
A
GIVEN
SET
OF
PRIMARIES
USEFUL
WANT
TO
PAINT
A
CARTON
OF
KODAK
FILM
WITH
THE
KODAK
YELLOW
COLOR
WANT
TO
MATCH
SKIN
COLOR
OF
A
PERSON
IN
A
PHOTOGRAPH
PRINTED
ON
AN
INK
JET
PRINTER
TO
THEIR
TRUE
SKIN
COLOR
WANT
THE
COLORS
IN
THE
WORLD
ON
A
MONITOR
AND
IN
A
PRINT
FORMAT
TO
ALL
LOOK
THE
SAME
ADAPTED
FROM
W
FREEMAN
IMAGE
CREDIT
PBS
ORG
TODAY
REVIEW
SIFT
FEATURES
PHYSICS
AND
PERCEPTION
OF
COLOR
COLOR
MATCHING
COLOR
SPACES
USES
OF
COLOR
IN
COMPUTER
VISION
WHY
SPECIFY
COLOR
NUMERICALLY
ACCURATE
COLOR
REPRODUCTION
IS
COMMERCIALLY
VALUABLE
MANY
PRODUCTS
ARE
IDENTIFIED
BY
COLOR
GOLDEN
ARCHES
FEW
COLOR
NAMES
ARE
WIDELY
RECOGNIZED
BY
ENGLISH
SPEAKERS
BLACK
BLUE
BROWN
GREY
GREEN
ORANGE
PINK
PURPLE
RED
WHITE
AND
YELLOW
OTHER
LANGUAGES
HAVE
FEWER
MORE
COMMON
TO
DISAGREE
ON
APPROPRIATE
COLOR
NAMES
COLOR
REPRODUCTION
PROBLEMS
INCREASED
BY
PREVALENCE
OF
DIGITAL
IMAGING
E
G
DIGITAL
LIBRARIES
OF
ART
HOW
TO
ENSURE
THAT
EVERYONE
PERCEIVES
THE
SAME
COLOR
FORSYTH
PONCE
STANDARD
COLOR
SPACES
USE
A
COMMON
SET
OF
PRIMARIES
COLOR
MATCHING
FUNCTIONS
LINEAR
COLOR
SPACE
EXAMPLES
RGB
CIE
XYZ
NON
LINEAR
COLOR
SPACE
HSV
LINEAR
COLOR
SPACES
DEFINED
BY
A
CHOICE
OF
THREE
PRIMARIES
THE
COORDINATES
OF
A
COLOR
ARE
GIVEN
BY
THE
WEIGHTS
OF
THE
PRIMARIES
USED
TO
MATCH
IT
MIXING
TWO
LIGHTS
PRODUCES
COLORS
THAT
LIE
ALONG
A
STRAIGHT
LINE
IN
COLOR
SPACE
MIXING
THREE
LIGHTS
PRODUCES
COLORS
THAT
LIE
WITHIN
THE
TRIANGLE
THEY
DEFINE
IN
COLOR
SPACE
DEFAULT
COLOR
SPACE
SOME
DRAWBACKS
STRONGLY
CORRELATED
CHANNELS
NON
PERCEPTUAL
HOIEM
R
G
B
G
R
B
B
R
G
IMAGE
FROM
HTTP
EN
WIKIPEDIA
ORG
WIKI
FILE
PNG
PERCEPTUAL
EQUIVALENTS
WITH
RGB
PERCEPTUAL
EQUIVALENTS
WITH
CIE
XYZ
RGB
PORTION
IS
IN
TRIANGLE
ARE
DISTANCES
BETWEEN
POINTS
IN
A
COLOR
SPACE
PERCEPTUALLY
MEANINGFUL
NOT
NECESSARILY
CIE
XYZ
IS
NOT
A
UNIFORM
COLOR
SPACE
SO
MAGNITUDE
OF
DIFFERENCES
IN
COORDINATES
ARE
POOR
INDICATOR
OF
COLOR
DISTANCE
MCADAM
ELLIPSES
JUST
NOTICEABLE
DIFFERENCES
IN
COLOR
ATTEMPT
TO
CORRECT
THIS
LIMITATION
BY
REMAPPING
COLOR
SPACE
SO
THAT
JUST
NOTICEABLE
DIFFERENCES
ARE
CONTAINED
BY
CIRCLES
DISTANCES
MORE
PERCEPTUALLY
MEANINGFUL
EXAMPLES
CIE
U
V
CIE
LAB
CIE
XYZ
CIE
U
V
COLOR
SPACES
CIE
L
A
B
PERCEPTUALLY
UNIFORM
COLOR
SPACE
LUMINANCE
BRIGHTNESS
CHROMINANCE
COLOR
L
A
B
A
L
B
B
L
A
INTUITIVE
COLOR
SPACE
H
V
H
V
V
H
HSV
COLOR
SPACE
HUE
SATURATION
VALUE
BRIGHTNESS
NONLINEAR
REFLECTS
TOPOLOGY
OF
COLORS
BY
CODING
HUE
AS
AN
ANGLE
MATLAB
GRAUMAN
IMAGE
FROM
MATHWORKS
COM
TODAY
REVIEW
SIFT
FEATURES
PHYSICS
AND
PERCEPTION
OF
COLOR
COLOR
MATCHING
COLOR
SPACES
USES
OF
COLOR
IN
COMPUTER
VISION
COLOR
AS
A
LOW
LEVEL
CUE
FOR
CBIR
R
G
B
COLOR
INTENSITY
COLOR
HISTOGRAMS
USE
DISTRIBUTION
OF
COLORS
TO
DESCRIBE
IMAGE
NO
SPATIAL
INFO
INVARIANT
TO
TRANSLATION
ROTATION
SCALE
K
GRAUMAN
COLOR
AS
A
LOW
LEVEL
CUE
FOR
CBIR
R
G
B
GIVEN
TWO
HISTOGRAM
VECTORS
SUM
THE
MINIMUM
COUNTS
PER
BIN
N
I
X
Y
MIN
I
XI
YI
GRAUMAN
GIVEN
COLLECTION
DATABASE
OF
IMAGES
EXTRACT
AND
STORE
ONE
COLOR
HISTOGRAM
PER
IMAGE
GIVEN
NEW
QUERY
IMAGE
EXTRACT
ITS
COLOR
HISTOGRAM
FOR
EACH
DATABASE
IMAGE
COMPUTE
INTERSECTION
BETWEEN
QUERY
HISTOGRAM
AND
DATABASE
HISTOGRAM
SORT
INTERSECTION
VALUES
HIGHEST
SCORE
MOST
SIMILAR
RANK
DATABASE
ITEMS
RELATIVE
TO
QUERY
BASED
ON
THIS
SORTED
ORDER
EXAMPLE
DATABASE
GRAUMAN
M
JONES
AND
J
REHG
STATISTICAL
COLOR
MODELS
WITH
APPLICATION
TO
SKIN
DETECTION
IJCV
COLOR
BASED
SEGMENTATION
FOR
ROBOT
SOCCER
TOWARDS
ELIMINATING
MANUAL
COLOR
CALIBRATION
AT
ROBOCUP
MOHAN
SRIDHARAN
AND
PETER
STONE
ROBOCUP
ROBOT
SOCCER
WORLD
CUP
IX
SPRINGER
VERLAG
HTTP
WWW
CS
UTEXAS
EDU
USERS
AUSTINVILLA
P
RESEARCH
COLOR
PERCEPTION
DIFFERS
FROM
THE
PHYSICS
OF
COLOR
VARIOUS
COLOR
SPACES
EXIST
WITH
DIFFERENT
STRENGTHS
AND
WEAKNESSES
COLOR
HAS
LIMITED
APPLICATION
IN
COMPUTER
VISION
SEGMENTATION
CLUSTERING
FIGURE
BY
J
SHI
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
GOALS
GROUPING
IN
VISION
GATHER
FEATURES
THAT
BELONG
TOGETHER
OBTAIN
AN
INTERMEDIATE
REPRESENTATION
THAT
COMPACTLY
DESCRIBES
KEY
IMAGE
PARTS
EXAMPLES
OF
GROUPING
IN
VISION
FIGURE
BY
J
SHI
DETERMINE
IMAGE
REGIONS
JPG
GROUP
VIDEO
FRAMES
INTO
SHOTS
FIGURE
BY
WANG
SUTER
FIGURE
GROUND
FIGURE
BY
GRAUMAN
DARRELL
OBJECT
LEVEL
GROUPING
IMAGE
HUMAN
SEGMENTATION
GROUP
TOGETHER
SIMILAR
LOOKING
PIXELS
FOR
EFFICIENCY
OF
FURTHER
PROCESSING
SUPERPIXELS
X
REN
AND
J
MALIK
ICCV
OVERSEGMENTATION
UNDERSEGMENTATION
MULTIPLE
SEGMENTATIONS
BOTTOM
UP
GROUP
TOKENS
WITH
SIMILAR
FEATURES
TOP
DOWN
GROUP
TOKENS
THAT
LIKELY
BELONG
TO
THE
SAME
OBJECT
SOURCE
D
HOIEM
LEVIN
AND
WEISS
GOALS
GROUPING
IN
VISION
GATHER
FEATURES
THAT
BELONG
TOGETHER
OBTAIN
AN
INTERMEDIATE
REPRESENTATION
THAT
COMPACTLY
DESCRIBES
KEY
IMAGE
VIDEO
PARTS
TOP
DOWN
VS
BOTTOM
UP
SEGMENTATION
TOP
DOWN
PIXELS
BELONG
TOGETHER
BECAUSE
THEY
ARE
FROM
THE
SAME
OBJECT
BOTTOM
UP
PIXELS
BELONG
TOGETHER
BECAUSE
THEY
LOOK
SIMILAR
HARD
TO
MEASURE
SUCCESS
WHAT
IS
INTERESTING
DEPENDS
ON
THE
APP
BOTTOM
UP
SEGMENTATION
VIA
CLUSTERING
FEATURES
COLOR
TEXTURE
QUANTIZATION
FOR
TEXTURE
SUMMARIES
ALGORITHMS
MODE
FINDING
AND
MEAN
SHIFT
K
MEANS
MEAN
SHIFT
GRAPH
BASED
NORMALIZED
CUTS
OTHER
GESTALT
WHOLE
IS
GREATER
THAN
SUM
OF
ITS
PARTS
RELATIONSHIPS
AMONG
PARTS
CAN
YIELD
NEW
PROPERTIES
FEATURES
PSYCHOLOGISTS
IDENTIFIED
SERIES
OF
FACTORS
THAT
PREDISPOSE
SET
OF
ELEMENTS
TO
BE
GROUPED
BY
HUMAN
VISUAL
SYSTEM
GESTALTISM
THE
MULLER
LYER
ILLUSION
WE
PERCEIVE
THE
INTERPRETATION
NOT
THE
SENSES
SOURCE
D
HOIEM
FROM
STEVE
LEHAR
THE
CONSTRUCTIVE
ASPECT
OF
VISUAL
PERCEPTION
GESTALTISTS
DO
NOT
BELIEVE
IN
COINCIDENCE
GROUPING
BY
INVISIBLE
COMPLETION
SOURCE
D
HOIEM
FROM
STEVE
LEHAR
THE
CONSTRUCTIVE
ASPECT
OF
VISUAL
PERCEPTION
CONTINUITY
EXPLANATION
BY
OCCLUSION
FIGURE
GROUND
IN
VISION
D
MARR
FROM
J
L
MARROQUIN
HUMAN
VISUAL
PERCEPTION
OF
STRUCTURE
GESTALT
CUES
GOOD
INTUITION
AND
BASIC
PRINCIPLES
FOR
GROUPING
BASIS
FOR
MANY
IDEAS
IN
SEGMENTATION
AND
OCCLUSION
REASONING
SOME
E
G
SYMMETRY
ARE
DIFFICULT
TO
IMPLEMENT
IN
PRACTICE
SOURCE
D
HOIEM
SIMILARITY
COMMON
FATE
IMAGE
CREDIT
ARTHUS
BERTRAND
VIA
F
DURAND
SOURCE
K
GRAUMAN
PROXIMITY
TODAY
INSPIRATION
FROM
HUMAN
PERCEPTION
GESTALT
PROPERTIES
BOTTOM
UP
SEGMENTATION
VIA
CLUSTERING
FEATURES
COLOR
TEXTURE
QUANTIZATION
FOR
TEXTURE
SUMMARIES
ALGORITHMS
MODE
FINDING
AND
MEAN
SHIFT
K
MEANS
MEAN
SHIFT
GRAPH
BASED
NORMALIZED
CUTS
OTHER
IMAGE
SEGMENTATION
TOY
EXAMPLE
BLACK
PIXELS
GRAY
PIXELS
WHITE
PIXELS
INPUT
IMAGE
INTENSITY
THESE
INTENSITIES
DEFINE
THE
THREE
GROUPS
WE
COULD
LABEL
EVERY
PIXEL
IN
THE
IMAGE
ACCORDING
TO
WHICH
OF
THESE
PRIMARY
INTENSITIES
IT
IS
I
E
SEGMENT
THE
IMAGE
BASED
ON
THE
INTENSITY
FEATURE
WHAT
IF
THE
IMAGE
ISN
T
QUITE
SO
SIMPLE
INPUT
IMAGE
INPUT
IMAGE
INTENSITY
INTENSITY
INPUT
IMAGE
INTENSITY
NOW
HOW
TO
DETERMINE
THE
THREE
MAIN
INTENSITIES
THAT
DEFINE
OUR
GROUPS
WE
NEED
TO
CLUSTER
INTENSITY
GOAL
CHOOSE
THREE
CENTERS
AS
THE
REPRESENTATIVE
INTENSITIES
AND
LABEL
EVERY
PIXEL
ACCORDING
TO
WHICH
OF
THESE
CENTERS
IT
IS
NEAREST
TO
BEST
CLUSTER
CENTERS
ARE
THOSE
THAT
MINIMIZE
SSD
BETWEEN
ALL
POINTS
AND
THEIR
NEAREST
CLUSTER
CENTER
CI
CLUSTERING
WITH
THIS
OBJECTIVE
IT
IS
A
CHICKEN
AND
EGG
PROBLEM
IF
WE
KNEW
THE
CLUSTER
CENTERS
WE
COULD
ALLOCATE
POINTS
TO
GROUPS
BY
ASSIGNING
EACH
TO
ITS
CLOSEST
CENTER
IF
WE
KNEW
THE
GROUP
MEMBERSHIPS
WE
COULD
GET
THE
CENTERS
BY
COMPUTING
THE
MEAN
PER
GROUP
K
MEANS
CLUSTERING
BASIC
IDEA
RANDOMLY
INITIALIZE
THE
K
CLUSTER
CENTERS
AND
ITERATE
BETWEEN
THE
TWO
STEPS
WE
JUST
SAW
RANDOMLY
INITIALIZE
THE
CLUSTER
CENTERS
CK
GIVEN
CLUSTER
CENTERS
DETERMINE
POINTS
IN
EACH
CLUSTER
FOR
EACH
POINT
P
FIND
THE
CLOSEST
CI
PUT
P
INTO
CLUSTER
I
GIVEN
POINTS
IN
EACH
CLUSTER
SOLVE
FOR
CI
SET
CI
TO
BE
THE
MEAN
OF
POINTS
IN
CLUSTER
I
IF
CI
HAVE
CHANGED
REPEAT
STEP
PROPERTIES
WILL
ALWAYS
CONVERGE
TO
SOME
SOLUTION
CAN
BE
A
LOCAL
MINIMUM
DOES
NOT
ALWAYS
FIND
THE
GLOBAL
MINIMUM
OF
OBJECTIVE
FUNCTION
SOURCE
STEVE
SEITZ
K
MEANS
ASK
USER
HOW
MANY
CLLUSTERS
THEY
D
NKE
E
G
K
K
MEANS
L
ASK
USER
HOW
CLUSTERS
THEY
D
LLIIKE
E
G
K
GUESS
K
CLLUSTER
CENTER
LOCATIONS
K
MEANS
ASK
USER
HOW
MANY
CLUSTERS
THEY
D
LIKE
E
G
K
GUESS
K
CLUSTER
CE
NT
E
R
LOCAT
IONS
EACH
DLATAPOINT
FINDS
OUT
WH
ICH
CENTER
IT
CLOSEST
TO
THUS
EACH
CENTE
R
A
SET
OF
DATAPO
INTS
L
K
MEANS
ASK
USER
HOW
MANY
CLUSTERS
THEY
D
LIKE
E
G
K
GUESS
K
CLUSTER
CENTER
LOCATIONS
EACH
DATAPOINT
FINDS
OUT
WHICH
CENT
ER
IT
CLOSEST
TO
EACH
CENTER
FINDS
THE
CENTROID
OF
THE
POINTS
IT
OWNS
O
L
K
MEANS
CLUSTERING
MATLAB
DEMO
JAVA
DEMOS
TODAY
INSPIRATION
FROM
HUMAN
PERCEPTION
GESTALT
PROPERTIES
BOTTOM
UP
SEGMENTATION
VIA
CLUSTERING
FEATURES
COLOR
TEXTURE
QUANTIZATION
FOR
TEXTURE
SUMMARIES
ALGORITHMS
GRAPH
BASED
NORMALIZED
CUTS
OTHER
K
MEANS
PROS
AND
CONS
PROS
SIMPLE
FAST
TO
COMPUTE
CONVERGES
TO
LOCAL
MINIMUM
OF
WITHIN
CLUSTER
SQUARED
ERROR
CONS
ISSUES
SETTING
K
SENSITIVE
TO
INITIAL
CENTERS
SENSITIVE
TO
OUTLIERS
DETECTS
SPHERICAL
CLUSTERS
ASSUMING
MEANS
CAN
BE
COMPUTED
AN
ASIDE
SMOOTHING
OUT
CLUSTER
ASSIGNMENTS
ASSIGNING
A
CLUSTER
LABEL
PER
PIXEL
MAY
YIELD
OUTLIERS
ORIGINAL
LABELED
BY
CLUSTER
CENTER
INTENSITY
HOW
TO
ENSURE
THEY
ARE
SEGMENTATION
AS
CLUSTERING
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
SIMILARITY
FEATURE
SPACE
INTENSITY
VALUE
D
K
K
QUANTIZATION
OF
THE
FEATURE
SPACE
SEGMENTATION
LABEL
MAP
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
COLOR
SIMILARITY
R
G
B
R
G
B
R
G
R
B
R
G
B
FEATURE
SPACE
COLOR
VALUE
D
SOURCE
K
GRAUMAN
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
SIMILARITY
CLUSTERS
BASED
ON
INTENSITY
SIMILARITY
DON
T
HAVE
TO
BE
SPATIALLY
COHERENT
SOURCE
K
GRAUMAN
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
POSITION
SIMILARITY
SOURCE
K
GRAUMAN
X
BOTH
REGIONS
ARE
BLACK
BUT
IF
WE
ALSO
INCLUDE
POSITION
X
Y
THEN
WE
COULD
GROUP
THE
TWO
INTO
DISTINCT
SEGMENTS
WAY
TO
ENCODE
BOTH
SIMILARITY
PROXIMITY
SEGMENTATION
AS
CLUSTERING
COLOR
BRIGHTNESS
POSITION
ALONE
ARE
NOT
ENOUGH
TO
DISTINGUISH
ALL
REGIONS
SOURCE
L
LAZEBNIK
SEGMENTATION
AS
CLUSTERING
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
TEXTURE
SIMILARITY
FILTER
BANK
OF
FILTERS
FEATURE
SPACE
FILTER
BANK
RESPONSES
E
G
D
SOURCE
K
GRAUMAN
RECALL
TEXTURE
REPRESENTATION
WINDOWS
WITH
PRIMARILY
HORIZONTAL
EDGES
BOTH
SOURCE
K
GRAUMAN
WINDOWS
WITH
SMALL
GRADIENT
IN
BOTH
DIRECTIONS
WINDOWS
WITH
PRIMARILY
VERTICAL
EDGES
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
FIND
TEXTONS
BY
CLUSTERING
VECTORS
OF
FILTER
BANK
OUTPUTS
DESCRIBE
TEXTURE
IN
A
WINDOW
BASED
ON
TEXTON
HISTOGRAM
IMAGE
TEXTON
MAP
TEXTON
INDEX
TEXTON
INDEX
MALIK
BELONGIE
LEUNG
AND
SHI
IJCV
SOURCE
L
LAZEBNIK
PIXEL
PROPERTIES
VS
NEIGHBORHOOD
PROPERTIES
THESE
LOOK
VERY
SIMILAR
IN
TERMS
OF
THEIR
COLOR
DISTRIBUTIONS
HISTOGRAMS
HOW
WOULD
THEIR
TEXTURE
DISTRIBUTIONS
COMPARE
FOR
AN
IMAGE
OF
A
SINGLE
TEXTURE
WE
CAN
CLASSIFY
IT
ACCORDING
TO
ITS
GLOBAL
IMAGE
WIDE
TEXTON
HISTOGRAM
FIGURE
FROM
VARMA
ZISSERMAN
IJCV
SOURCE
K
GRAUMAN
NEAREST
NEIGHBOR
CLASSIFICATION
LABEL
THE
INPUT
ACCORDING
TO
THE
NEAREST
KNOWN
EXAMPLE
LABEL
BOARD
SOURCE
K
GRAUMAN
MANIK
VARMA
K
MEANS
PROS
AND
CONS
PROS
SIMPLE
FAST
TO
COMPUTE
CONVERGES
TO
LOCAL
MINIMUM
OF
WITHIN
CLUSTER
SQUARED
ERROR
CONS
ISSUES
SETTING
K
SENSITIVE
TO
INITIAL
CENTERS
SENSITIVE
TO
OUTLIERS
DETECTS
SPHERICAL
CLUSTERS
ASSUMING
MEANS
CAN
BE
COMPUTED
THE
MEAN
SHIFT
ALGORITHM
SEEKS
MODES
OR
LOCAL
MAXIMA
OF
DENSITY
IN
THE
FEATURE
SPACE
IMAGE
FEATURE
SPACE
L
U
V
COLOR
VALUES
ESTIMATED
DENSITY
SEARCH
WINDOW
CENTER
OF
MASS
MEAN
SHIFT
VECTOR
SOURCE
D
HOIEM
CLUSTER
ALL
DATA
POINTS
IN
THE
ATTRACTION
BASIN
OF
A
MODE
ATTRACTION
BASIN
THE
REGION
FOR
WHICH
ALL
TRAJECTORIES
LEAD
TO
THE
SAME
MODE
SLIDE
BY
Y
UKRAINITZ
B
SAREL
COMPUTE
FEATURES
FOR
EACH
POINT
COLOR
TEXTURE
ETC
INITIALIZE
WINDOWS
AT
INDIVIDUAL
FEATURE
POINTS
PERFORM
MEAN
SHIFT
FOR
EACH
WINDOW
UNTIL
CONVERGENCE
MERGE
WINDOWS
THAT
END
UP
NEAR
THE
SAME
PEAK
OR
MODE
SOURCE
D
HOIEM
PROS
MEAN
SHIFT
DOES
NOT
ASSUME
SHAPE
ON
CLUSTERS
ONE
PARAMETER
CHOICE
WINDOW
SIZE
GENERIC
TECHNIQUE
FIND
MULTIPLE
MODES
ROBUST
TO
OUTLIERS
CONS
SELECTION
OF
WINDOW
SIZE
DOES
NOT
SCALE
WELL
WITH
DIMENSION
OF
FEATURE
SPACE
MEAN
SHIFT
READING
NICELY
WRITTEN
MEAN
SHIFT
EXPLANATION
WITH
MATH
INCLUDES
M
CODE
FOR
MEAN
SHIFT
CLUSTERING
MEAN
SHIFT
PAPER
BY
COMANICIU
AND
MEER
ADAPTIVE
MEAN
SHIFT
IN
HIGHER
DIMENSIONS
TODAY
INSPIRATION
FROM
HUMAN
PERCEPTION
GESTALT
PROPERTIES
BOTTOM
UP
SEGMENTATION
VIA
CLUSTERING
FEATURES
COLOR
TEXTURE
QUANTIZATION
FOR
TEXTURE
SUMMARIES
ALGORITHMS
MODE
FINDING
AND
MEAN
SHIFT
K
MEANS
MEAN
SHIFT
OTHER
FULLY
CONNECTED
GRAPH
NODE
VERTEX
FOR
EVERY
PIXEL
LINK
BETWEEN
EVERY
PAIR
OF
PIXELS
P
Q
AFFINITY
WEIGHT
WPQ
FOR
EACH
LINK
EDGE
WPQ
MEASURES
SIMILARITY
SIMILARITY
IS
INVERSELY
PROPORTIONAL
TO
DIFFERENCE
IN
COLOR
AND
POSITION
A
B
C
BREAK
GRAPH
INTO
SEGMENTS
WANT
TO
DELETE
LINKS
THAT
CROSS
BETWEEN
SEGMENTS
EASIEST
TO
BREAK
LINKS
THAT
HAVE
LOW
SIMILARITY
LOW
WEIGHT
SIMILAR
PIXELS
SHOULD
BE
IN
THE
SAME
SEGMENTS
DISSIMILAR
PIXELS
SHOULD
BE
IN
DIFFERENT
SEGMENTS
B
LINK
CUT
SET
OF
LINKS
WHOSE
REMOVAL
MAKES
A
GRAPH
DISCONNECTED
COST
OF
A
CUT
FIND
MINIMUM
CUT
CUT
A
B
W
P
A
Q
B
P
Q
GIVES
YOU
A
SEGMENTATION
FAST
ALGORITHMS
EXIST
FOR
DOING
THIS
MINIMUM
CUT
PROBLEM
WITH
MINIMUM
CUT
WEIGHT
OF
CUT
PROPORTIONAL
TO
NUMBER
OF
EDGES
IN
THE
CUT
TENDS
TO
PRODUCE
SMALL
ISOLATED
COMPONENTS
SHI
MALIK
PAMI
SOURCE
K
GRAUMAN
B
NORMALIZED
CUT
FIX
BIAS
OF
MIN
CUT
BY
NORMALIZING
FOR
SIZE
OF
SEGMENTS
CUT
A
B
ASSOC
A
V
CUT
A
B
ASSOC
B
V
ASSOC
A
V
SUM
OF
WEIGHTS
OF
ALL
EDGES
THAT
TOUCH
A
NCUT
VALUE
SMALL
WHEN
WE
GET
TWO
CLUSTERS
WITH
MANY
EDGES
WITH
HIGH
WEIGHTS
AND
FEW
EDGES
OF
LOW
WEIGHT
BETWEEN
THEM
APPROXIMATE
SOLUTION
FOR
MINIMIZING
THE
NCUT
VALUE
EIGENVALUE
PROBLEM
SHI
AND
J
MALIK
CVPR
SOURCE
STEVE
SEITZ
NORMALIZED
CUTS
PROS
AND
CONS
PROS
GENERIC
FRAMEWORK
FLEXIBLE
TO
CHOICE
OF
FUNCTION
THAT
COMPUTES
WEIGHTS
AFFINITIES
BETWEEN
NODES
DOES
NOT
REQUIRE
MODEL
OF
THE
DATA
DISTRIBUTION
CONS
TIME
COMPLEXITY
CAN
BE
HIGH
DENSE
HIGHLY
CONNECTED
GRAPHS
MANY
AFFINITY
COMPUTATIONS
SOLVING
EIGENVALUE
PROBLEM
PREFERENCE
FOR
BALANCED
PARTITIONS
TODAY
INSPIRATION
FROM
HUMAN
PERCEPTION
GESTALT
PROPERTIES
BOTTOM
UP
SEGMENTATION
VIA
CLUSTERING
FEATURES
COLOR
TEXTURE
QUANTIZATION
FOR
TEXTURE
SUMMARIES
ALGORITHMS
MODE
FINDING
AND
MEAN
SHIFT
K
MEANS
MEAN
SHIFT
GRAPH
BASED
NORMALIZED
CUTS
E
BORENSTEIN
AND
ULLMAN
ECCV
LEVIN
AND
Y
WEISS
ECCV
SLIDE
CREDIT
LANA
LAZEBNIK
DEFINE
A
LABELING
L
AS
AN
ASSIGNMENT
OF
EACH
PIXEL
WITH
A
LABEL
BACKGROUND
OR
FOREGROUND
FIND
THE
LABELING
L
THAT
MINIMIZES
DATA
TERM
SMOOTHNESS
TERM
HOW
SIMILAR
IS
EACH
LABELED
PIXEL
TO
THE
FOREGROUND
OR
BACKGROUND
ENCOURAGE
SPATIALLY
COHERENT
SEGMENTS
SLIDE
CREDIT
N
SNAVELY
IMAGE
GRADIENT
WATERSHED
BOUNDARIES
IMAGE
PARSING
OR
SEMANTIC
SEGMENTATION
J
TIGHE
AND
LAZEBNIK
ECCV
IJCV
SEGMENTATION
TO
FIND
OBJECT
BOUNDARIES
OR
MID
LEVEL
REGIONS
BOTTOM
UP
SEGMENTATION
VIA
CLUSTERING
GENERAL
CHOICES
FEATURES
AFFINITY
FUNCTIONS
AND
CLUSTERING
ALGORITHMS
GROUPING
ALSO
USEFUL
FOR
QUANTIZATION
CAN
CREATE
NEW
FEATURE
SUMMARIES
TEXTON
HISTOGRAMS
FOR
TEXTURE
WITHIN
LOCAL
REGION
EXAMPLE
CLUSTERING
METHODS
K
MEANS
MEAN
SHIFT
GRAPH
CUT
NORMALIZED
CUTS
FINDING
CORRESPONDENCES
BETWEEN
FEATURES
LINE
AND
MODEL
FITTING
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
SEPTEMBER
TODAY
FITTING
MODELS
LINES
TO
POINTS
I
E
FIND
THE
PARAMETERS
OF
A
MODEL
THAT
BEST
FITS
THE
DATA
LEAST
SQUARES
HOUGH
TRANSFORM
RANSAC
MATCHING
FINDING
CORRESPONDENCES
BETWEEN
POINTS
I
E
FIND
THE
PARAMETERS
OF
THE
TRANSFORMATION
THAT
BEST
ALIGNS
POINTS
HOMEWORK
IS
DUE
FITTING
WANT
TO
ASSOCIATE
A
MODEL
WITH
OBSERVED
FEATURES
FIG
FROM
MARSZALEK
SCHMID
FOR
EXAMPLE
THE
MODEL
COULD
BE
A
LINE
A
CIRCLE
OR
AN
ARBITRARY
SHAPE
EXAMPLE
LINE
FITTING
WHY
FIT
LINES
MANY
OBJECTS
CHARACTERIZED
BY
PRESENCE
OF
STRAIGHT
LINES
WHY
AREN
T
WE
DONE
JUST
BY
RUNNING
EDGE
DETECTION
DIFFICULTY
OF
LINE
FITTING
EXTRA
EDGE
POINTS
CLUTTER
MULTIPLE
MODELS
WHICH
POINTS
GO
WITH
WHICH
LINE
IF
ANY
ONLY
SOME
PARTS
OF
EACH
LINE
DETECTED
AND
SOME
PARTS
ARE
MISSING
HOW
TO
FIND
A
LINE
THAT
BRIDGES
MISSING
EVIDENCE
NOISE
IN
MEASURED
EDGE
POINTS
ORIENTATIONS
HOW
TO
DETECT
TRUE
UNDERLYING
PARAMETERS
LEAST
SQUARES
LINE
FITTING
DATA
XN
YN
LINE
EQUATION
YI
M
XI
B
FIND
M
B
TO
MINIMIZE
XI
YI
Y
MX
B
M
M
E
N
X
Y
AP
Y
I
I
B
I
B
X
Y
N
N
HYPOTHESIZE
AND
TEST
PROPOSE
PARAMETERS
TRY
ALL
POSSIBLE
EACH
POINT
VOTES
FOR
ALL
CONSISTENT
PARAMETERS
REPEATEDLY
SAMPLE
ENOUGH
POINTS
TO
SOLVE
FOR
PARAMETERS
SCORE
THE
GIVEN
PARAMETERS
NUMBER
OF
CONSISTENT
POINTS
POSSIBLY
WEIGHTED
BY
DISTANCE
CHOOSE
FROM
AMONG
THE
SET
OF
PARAMETERS
GLOBAL
OR
LOCAL
MAXIMUM
OF
SCORES
POSSIBLY
REFINE
PARAMETERS
USING
INLIERS
VOTING
IT
NOT
FEASIBLE
TO
CHECK
ALL
COMBINATIONS
OF
FEATURES
BY
FITTING
A
MODEL
TO
EACH
POSSIBLE
SUBSET
VOTING
IS
A
GENERAL
TECHNIQUE
WHERE
WE
LET
THE
FEATURES
VOTE
FOR
ALL
MODELS
THAT
ARE
COMPATIBLE
WITH
IT
CYCLE
THROUGH
FEATURES
CAST
VOTES
FOR
MODEL
PARAMETERS
LOOK
FOR
MODEL
PARAMETERS
THAT
RECEIVE
A
LOT
OF
VOTES
NOISE
CLUTTER
FEATURES
THEY
WILL
CAST
VOTES
TOO
BUT
TYPICALLY
THEIR
VOTES
SHOULD
BE
INCONSISTENT
WITH
THE
MAJORITY
OF
GOOD
FEATURES
FITTING
LINES
HOUGH
TRANSFORM
GIVEN
POINTS
THAT
BELONG
TO
A
LINE
WHAT
IS
THE
LINE
HOW
MANY
LINES
ARE
THERE
WHICH
POINTS
BELONG
TO
WHICH
LINES
HOUGH
TRANSFORM
IS
A
VOTING
TECHNIQUE
THAT
CAN
BE
USED
TO
ANSWER
ALL
OF
THESE
QUESTIONS
MAIN
IDEA
RECORD
VOTE
FOR
EACH
POSSIBLE
LINE
ON
WHICH
EACH
EDGE
POINT
LIES
LOOK
FOR
LINES
THAT
GET
MANY
VOTES
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
WHAT
DOES
A
POINT
IN
THE
IMAGE
SPACE
MAP
TO
ANSWER
THE
SOLUTIONS
OF
B
THIS
IS
A
LINE
IN
HOUGH
SPACE
TO
GO
FROM
IMAGE
SPACE
TO
HOUGH
SPACE
GIVEN
A
SET
OF
POINTS
X
Y
FIND
ALL
M
B
SUCH
THAT
Y
MX
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
WHAT
ARE
THE
LINE
PARAMETERS
FOR
THE
LINE
THAT
CONTAINS
BOTH
AND
IT
IS
THE
INTERSECTION
OF
THE
LINES
B
AND
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
HOW
CAN
WE
USE
THIS
TO
FIND
THE
MOST
LIKELY
PARAMETERS
M
B
FOR
THE
MOST
PROMINENT
LINE
IN
THE
IMAGE
SPACE
LET
EACH
EDGE
POINT
IN
IMAGE
SPACE
VOTE
FOR
A
SET
OF
POSSIBLE
PARAMETERS
IN
HOUGH
SPACE
ACCUMULATE
VOTES
IN
DISCRETE
SET
OF
BINS
PARAMETERS
WITH
THE
MOST
VOTES
INDICATE
LINE
IN
IMAGE
SPACE
HOUGH
TRANSFORM
Y
M
X
B
Y
M
X
B
PROBLEMS
WITH
THE
M
B
SPACE
UNBOUNDED
PARAMETER
DOMAINS
VERTICAL
LINES
REQUIRE
INFINITE
M
PROBLEMS
WITH
THE
M
B
SPACE
UNBOUNDED
PARAMETER
DOMAINS
VERTICAL
LINES
REQUIRE
INFINITE
M
ALTERNATIVE
POLAR
REPRESENTATION
Y
SIN
EACH
POINT
X
Y
WILL
ADD
A
SINUSOID
IN
THE
PARAMETER
SPACE
HOUGH
TRANSFORM
HOUGH
MACHINE
ANALYSIS
OF
BUBBLE
CHAMBER
PICTURES
PROC
INT
CONF
HIGH
ENERGY
ACCELERATORS
AND
INSTRUMENTATION
USE
A
POLAR
REPRESENTATION
FOR
THE
PARAMETER
SPACE
Y
X
HOUGH
SPACE
X
COS
YSIN
ALGORITHM
OUTLINE
INITIALIZE
ACCUMULATOR
H
TO
ALL
ZEROS
FOR
EACH
FEATURE
POINT
X
Y
IN
THE
IMAGE
FOR
Θ
TO
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
END
END
FIND
THE
VALUE
OF
Θ
Ρ
WHERE
H
Θ
Ρ
IS
A
LOCAL
MAXIMUM
THE
DETECTED
LINE
IN
THE
IMAGE
IS
GIVEN
BY
Ρ
X
COS
Θ
Y
SIN
Θ
HOUGH
TRANSFORM
EXAMPLE
DEREK
HOIEM
X
IMAGE
SPACE
EDGE
COORDINATES
VOTES
Y
D
X
IMAGE
SPACE
EDGE
COORDINATES
VOTES
WHAT
DIFFICULTY
DOES
THIS
PRESENT
FOR
AN
IMPLEMENTATION
FEATURES
VOTES
NEED
TO
ADJUST
GRID
SIZE
OR
SMOOTH
IMAGE
SPACE
EDGE
COORDINATES
VOTES
HERE
EVERYTHING
APPEARS
TO
BE
NOISE
OR
RANDOM
EDGE
POINTS
BUT
WE
STILL
SEE
PEAKS
IN
THE
VOTE
SPACE
INITIALIZE
ACCUMULATOR
H
TO
ALL
ZEROS
FOR
EACH
FEATURE
POINT
X
Y
IN
THE
IMAGE
FOR
Θ
TO
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
END
END
FIND
THE
VALUE
OF
Θ
Ρ
WHERE
H
Θ
Ρ
IS
A
LOCAL
MAXIMUM
THE
DETECTED
LINE
IN
THE
IMAGE
IS
GIVEN
BY
Ρ
X
COS
Θ
Y
SIN
Θ
RECALL
WHEN
WE
DETECT
AN
EDGE
POINT
WE
ALSO
KNOW
ITS
GRADIENT
DIRECTION
BUT
THIS
MEANS
THAT
THE
LINE
IS
UNIQUELY
DETERMINED
MODIFIED
HOUGH
TRANSFORM
FOR
EACH
EDGE
POINT
X
Y
Θ
GRADIENT
ORIENTATION
AT
X
Y
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
END
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
A
FIXED
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
B
HOUGH
SPACE
A
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
A
FIXED
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
INTERSECTION
MOST
VOTES
FOR
CENTER
OCCUR
HERE
IMAGE
SPACE
HOUGH
SPACE
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
AN
UNKNOWN
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
B
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
AN
UNKNOWN
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
B
IMAGE
SPACE
HOUGH
SPACE
FOR
EVERY
EDGE
PIXEL
X
Y
FOR
ALL
A
FOR
ALL
B
R
H
A
B
R
END
END
END
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
AN
UNKNOWN
RADIUS
R
KNOWN
GRADIENT
DIRECTION
IMAGE
SPACE
HOUGH
SPACE
KRISTEN
GRAUMAN
A
CIRCLE
WITH
RADIUS
R
AND
CENTER
A
B
CAN
BE
DESCRIBED
AS
X
A
R
COS
Θ
Y
B
R
SIN
Θ
A
FOR
EVERY
EDGE
PIXEL
X
Y
FOR
EACH
POSSIBLE
RADIUS
VALUE
R
X
A
R
COS
Θ
Y
B
R
SIN
Θ
FOR
EACH
POSSIBLE
GRADIENT
DIRECTION
Θ
OR
USE
ESTIMATED
GRADIENT
AT
X
Y
A
X
R
COS
Θ
COLUMN
B
Y
R
SIN
Θ
ROW
H
A
B
R
END
END
END
MODIFIED
FROM
KRISTEN
GRAUMAN
ORIGINAL
EDGES
VOTES
PENNY
NOTE
A
DIFFERENT
HOUGH
TRANSFORM
WITH
SEPARATE
ACCUMULATORS
WAS
USED
FOR
EACH
CIRCLE
RADIUS
QUARTERS
VS
PENNY
COMBOINREIGDINDAELTECTIONS
EDGES
VOTES
QUARTER
NOTE
A
DIFFERENT
HOUGH
TRANSFORM
WITH
SEPARATE
ACCUMULATORS
WAS
USED
FOR
EACH
CIRCLE
RADIUS
QUARTERS
VS
PENNY
EXAMPLE
IRIS
DETECTION
GRADIENT
THRESHOLD
HOUGH
SPACE
FIXED
RADIUS
MAX
DETECTIONS
HEMERSON
PISTORI
AND
EDUARDO
ROCHA
COSTA
VOTING
PRACTICAL
TIPS
MINIMIZE
IRRELEVANT
TOKENS
FIRST
CHOOSE
A
GOOD
GRID
DISCRETIZATION
TOO
FINE
TOO
COARSE
TOO
COARSE
LARGE
VOTES
OBTAINED
WHEN
TOO
MANY
DIFFERENT
LINES
CORRESPOND
TO
A
SINGLE
BUCKET
TOO
FINE
MISS
LINES
BECAUSE
POINTS
THAT
ARE
NOT
EXACTLY
COLLINEAR
CAST
VOTES
FOR
DIFFERENT
BUCKETS
VOTE
FOR
NEIGHBORS
ALSO
SMOOTHING
IN
ACCUMULATOR
ARRAY
USE
DIRECTION
OF
EDGE
TO
REDUCE
PARAMETERS
BY
TO
READ
BACK
WHICH
POINTS
VOTED
FOR
WINNING
PEAKS
KEEP
TAGS
ON
THE
VOTES
WE
WANT
TO
FIND
A
TEMPLATE
DEFINED
BY
ITS
REFERENCE
POINT
CENTER
AND
SEVERAL
DISTINCT
TYPES
OF
LANDMARK
POINTS
IN
STABLE
SPATIAL
CONFIGURATION
TEMPLATE
WHAT
IF
WE
WANT
TO
DETECT
ARBITRARY
SHAPES
INTUITION
MODEL
IMAGE
NOVEL
IMAGE
X
VOTE
SPACE
NOW
SUPPOSE
THOSE
COLORS
ENCODE
GRADIENT
DIRECTIONS
DEFINE
A
MODEL
SHAPE
BY
ITS
BOUNDARY
POINTS
AND
A
REFERENCE
POINT
OFFLINE
PROCEDURE
AT
EACH
BOUNDARY
POINT
COMPUTE
DISPLACEMENT
VECTOR
R
A
PI
STORE
THESE
VECTORS
IN
A
TABLE
INDEXED
BY
GRADIENT
ORIENTATION
Θ
KRISTEN
GRAUMAN
DANA
H
BALLARD
GENERALIZING
THE
HOUGH
TRANSFORM
TO
DETECT
ARBITRARY
SHAPES
DETECTION
PROCEDURE
FOR
EACH
EDGE
POINT
USE
ITS
GRADIENT
ORIENTATION
Θ
TO
INDEX
INTO
STORED
TABLE
USE
RETRIEVED
R
VECTORS
TO
VOTE
FOR
REFERENCE
POINT
NOVEL
IMAGE
ASSUMING
TRANSLATION
IS
THE
ONLY
TRANSFORMATION
HERE
I
E
ORIENTATION
AND
SCALE
ARE
FIXED
TEMPLATE
REPRESENTATION
FOR
EACH
TYPE
OF
LANDMARK
POINT
STORE
ALL
POSSIBLE
DISPLACEMENT
VECTORS
TOWARDS
THE
CENTER
TEMPLATE
MODEL
DETECTING
THE
TEMPLATE
FOR
EACH
FEATURE
IN
A
NEW
IMAGE
LOOK
UP
THAT
FEATURE
TYPE
IN
THE
MODEL
AND
VOTE
FOR
THE
POSSIBLE
CENTER
LOCATIONS
ASSOCIATED
WITH
THAT
TYPE
IN
THE
MODEL
TEST
IMAGE
MODEL
SVETLANA
LAZEBNIK
TRAINING
IMAGE
VISUAL
CODEWORD
WITH
DISPLACEMENT
VECTORS
TEST
IMAGE
BUILD
CODEBOOK
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MORE
ON
THIS
LATER
IN
THE
COURSE
TEMPLATE
REPRESENTATION
FOR
EACH
TYPE
OF
LANDMARK
POINT
STORE
ALL
POSSIBLE
DISPLACEMENT
VECTORS
TOWARDS
THE
CENTER
TEMPLATE
MODEL
BUILD
CODEBOOK
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MAP
THE
PATCH
AROUND
EACH
INTEREST
POINT
TO
CLOSEST
CODEBOOK
ENTRY
BUILD
CODEBOOK
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MAP
THE
PATCH
AROUND
EACH
INTEREST
POINT
TO
CLOSEST
CODEBOOK
ENTRY
FOR
EACH
CODEBOOK
ENTRY
STORE
ALL
POSITIONS
IT
WAS
FOUND
RELATIVE
TO
OBJECT
CENTER
HOUGH
TRANSFORM
PROS
AND
CONS
PROS
ALL
POINTS
ARE
PROCESSED
INDEPENDENTLY
SO
CAN
COPE
WITH
OCCLUSION
GAPS
SOME
ROBUSTNESS
TO
NOISE
NOISE
POINTS
UNLIKELY
TO
CONTRIBUTE
CONSISTENTLY
TO
ANY
SINGLE
BIN
CAN
DETECT
MULTIPLE
INSTANCES
OF
A
MODEL
IN
A
SINGLE
PASS
CONS
COMPLEXITY
OF
SEARCH
TIME
INCREASES
EXPONENTIALLY
WITH
THE
NUMBER
OF
MODEL
PARAMETERS
NON
TARGET
SHAPES
CAN
PRODUCE
SPURIOUS
PEAKS
IN
PARAMETER
SPACE
QUANTIZATION
CAN
BE
TRICKY
TO
PICK
A
GOOD
GRID
SIZE
TODAY
FITTING
MODELS
LINES
TO
POINTS
I
E
FIND
THE
PARAMETERS
OF
A
MODEL
THAT
BEST
FITS
THE
DATA
LEAST
SQUARES
HOUGH
TRANSFORM
RANSAC
MATCHING
FINDING
CORRESPONDENCES
BETWEEN
POINTS
I
E
FIND
THE
PARAMETERS
OF
THE
TRANSFORMATION
THAT
BEST
ALIGNS
POINTS
HOMEWORK
IS
DUE
OUTLIERS
OUTLIERS
CAN
HURT
THE
QUALITY
OF
OUR
PARAMETER
ESTIMATES
E
G
AN
ERRONEOUS
PAIR
OF
MATCHING
POINTS
FROM
TWO
IMAGES
AN
EDGE
POINT
THAT
IS
NOISE
OR
DOESN
T
BELONG
TO
THE
LINE
WE
ARE
FITTING
RANDOM
SAMPLE
CONSENSUS
APPROACH
WE
WANT
TO
AVOID
THE
IMPACT
OF
OUTLIERS
SO
LET
LOOK
FOR
INLIERS
AND
USE
THOSE
ONLY
INTUITION
IF
AN
OUTLIER
IS
CHOSEN
TO
COMPUTE
THE
CURRENT
FIT
THEN
THE
RESULTING
LINE
WON
T
HAVE
MUCH
SUPPORT
FROM
REST
OF
THE
POINTS
RANSAC
GENERAL
FORM
RANSAC
LOOP
RANDOMLY
SELECT
A
SEED
GROUP
OF
POINTS
ON
WHICH
TO
BASE
MODEL
ESTIMATE
FIT
MODEL
TO
THESE
POINTS
FIND
INLIERS
TO
THIS
MODEL
I
E
POINTS
WHOSE
DISTANCE
FROM
THE
LINE
IS
LESS
THAN
T
IF
THERE
ARE
D
OR
MORE
INLIERS
RE
COMPUTE
ESTIMATE
OF
MODEL
ON
ALL
OF
THE
INLIERS
REPEAT
N
TIMES
KEEP
THE
MODEL
WITH
THE
LARGEST
NUMBER
OF
INLIERS
LEAST
SQUARES
FIT
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
UNCONTAMINATED
SAMPLE
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
FISCHLER
BOLLES
IN
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
RANSAC
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
N
I
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
ALGORITHM
N
I
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
HOW
TO
CHOOSE
PARAMETERS
NUMBER
OF
SAMPLES
N
CHOOSE
N
SO
THAT
WITH
PROBABILITY
P
AT
LEAST
ONE
RANDOM
SAMPLE
IS
FREE
FROM
OUTLIERS
E
G
P
OUTLIER
RATIO
E
NUMBER
OF
SAMPLED
POINTS
MINIMUM
NUMBER
NEEDED
TO
FIT
THE
MODEL
DISTANCE
THRESHOLD
CHOOSE
SO
THAT
A
GOOD
POINT
WITH
NOISE
IS
LIKELY
E
G
PROB
WITHIN
THRESHOLD
EXPLANATION
IN
SZELISKI
RANSAC
PROS
AND
CONS
PROS
SIMPLE
AND
GENERAL
APPLICABLE
TO
MANY
DIFFERENT
PROBLEMS
OFTEN
WORKS
WELL
IN
PRACTICE
CONS
LOTS
OF
PARAMETERS
TO
TUNE
DOESN
T
WORK
WELL
FOR
LOW
INLIER
RATIOS
TOO
MANY
ITERATIONS
OR
CAN
FAIL
COMPLETELY
CAN
T
ALWAYS
GET
A
GOOD
INITIALIZATION
OF
THE
MODEL
BASED
ON
THE
MINIMUM
NUMBER
OF
SAMPLES
COMMON
APPLICATIONS
IMAGE
STITCHING
RELATING
TWO
VIEWS
SVETLANA
LAZEBNIK
CS
INTRO
TO
COMPUTER
VISION
FEATURE
MATCHING
INDEXING
AND
RETRIEVAL
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
TODAY
MATCHING
POINTS
RETRIEVING
OBJECT
INSTANCES
INDEXING
BY
VISUAL
WORDS
SPATIAL
VERIFICATION
FITTING
VS
MATCHING
ALIGNMENT
FITTING
MODELS
LINES
TO
POINTS
I
E
FIND
THE
PARAMETERS
OF
A
MODEL
THAT
BEST
FITS
THE
DATA
LEAST
SQUARES
HOUGH
TRANSFORM
RANSAC
MATCHING
FINDING
CORRESPONDENCES
BETWEEN
POINTS
I
E
FIND
THE
PARAMETERS
OF
THE
TRANSFORMATION
THAT
BEST
ALIGNS
POINTS
LEAST
SQUARES
LINE
FITTING
DATA
XN
YN
LINE
EQUATION
YI
M
XI
B
FIND
M
B
TO
MINIMIZE
XI
YI
Y
MX
B
M
M
E
N
X
Y
AP
Y
I
I
B
I
B
X
Y
N
N
MODIFIED
FROM
SVETLANA
LAZEBNIK
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
WHAT
ARE
THE
LINE
PARAMETERS
FOR
THE
LINE
THAT
CONTAINS
BOTH
AND
IT
IS
THE
INTERSECTION
OF
THE
LINES
B
AND
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
HOW
CAN
WE
USE
THIS
TO
FIND
THE
MOST
LIKELY
PARAMETERS
M
B
FOR
THE
MOST
PROMINENT
LINE
IN
THE
IMAGE
SPACE
LET
EACH
EDGE
POINT
IN
IMAGE
SPACE
VOTE
FOR
A
SET
OF
POSSIBLE
PARAMETERS
IN
HOUGH
SPACE
ACCUMULATE
VOTES
IN
DISCRETE
SET
OF
BINS
PARAMETERS
WITH
THE
MOST
VOTES
INDICATE
LINE
IN
IMAGE
SPACE
PROBLEMS
WITH
THE
M
B
SPACE
UNBOUNDED
PARAMETER
DOMAINS
VERTICAL
LINES
REQUIRE
INFINITE
M
ALTERNATIVE
POLAR
REPRESENTATION
Y
SIN
EACH
POINT
X
Y
WILL
ADD
A
SINUSOID
IN
THE
PARAMETER
SPACE
HOUGH
TRANSFORM
HOUGH
MACHINE
ANALYSIS
OF
BUBBLE
CHAMBER
PICTURES
PROC
INT
CONF
HIGH
ENERGY
ACCELERATORS
AND
INSTRUMENTATION
USE
A
POLAR
REPRESENTATION
FOR
THE
PARAMETER
SPACE
Y
X
HOUGH
SPACE
X
COS
YSIN
INITIALIZE
ACCUMULATOR
H
TO
ALL
ZEROS
FOR
EACH
FEATURE
POINT
X
Y
IN
THE
IMAGE
FOR
Θ
TO
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
END
END
FIND
THE
VALUE
OF
Θ
Ρ
WHERE
H
Θ
Ρ
IS
A
LOCAL
MAXIMUM
THE
DETECTED
LINE
IN
THE
IMAGE
IS
GIVEN
BY
Ρ
X
COS
Θ
Y
SIN
Θ
RECALL
WHEN
WE
DETECT
AN
EDGE
POINT
WE
ALSO
KNOW
ITS
GRADIENT
DIRECTION
BUT
THIS
MEANS
THAT
THE
LINE
IS
UNIQUELY
DETERMINED
MODIFIED
HOUGH
TRANSFORM
FOR
EACH
EDGE
POINT
X
Y
Θ
GRADIENT
ORIENTATION
AT
X
Y
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
END
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
A
FIXED
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
B
HOUGH
SPACE
A
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
A
FIXED
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
INTERSECTION
MOST
VOTES
FOR
CENTER
OCCUR
HERE
IMAGE
SPACE
HOUGH
SPACE
FOR
EVERY
EDGE
PIXEL
X
Y
FOR
EACH
POSSIBLE
RADIUS
VALUE
R
FOR
EACH
POSSIBLE
GRADIENT
DIRECTION
Θ
OR
USE
ESTIMATED
GRADIENT
AT
X
Y
A
X
R
COS
Θ
COLUMN
B
Y
R
SIN
Θ
ROW
H
A
B
R
END
END
END
GENERALIZED
HOUGH
TRANSFORM
DEFINE
A
MODEL
SHAPE
BY
ITS
BOUNDARY
POINTS
AND
A
REFERENCE
POINT
OFFLINE
PROCEDURE
AT
EACH
BOUNDARY
POINT
COMPUTE
DISPLACEMENT
VECTOR
R
A
PI
STORE
THESE
VECTORS
IN
A
TABLE
INDEXED
BY
GRADIENT
ORIENTATION
Θ
KRISTEN
GRAUMAN
DANA
H
BALLARD
GENERALIZING
THE
HOUGH
TRANSFORM
TO
DETECT
ARBITRARY
SHAPES
HOUGH
TRANSFORM
PROS
AND
CONS
PROS
ALL
POINTS
ARE
PROCESSED
INDEPENDENTLY
SO
CAN
COPE
WITH
OCCLUSION
GAPS
SOME
ROBUSTNESS
TO
NOISE
NOISE
POINTS
UNLIKELY
TO
CONTRIBUTE
CONSISTENTLY
TO
ANY
SINGLE
BIN
CAN
DETECT
MULTIPLE
INSTANCES
OF
A
MODEL
IN
A
SINGLE
PASS
CONS
COMPLEXITY
OF
SEARCH
TIME
INCREASES
EXPONENTIALLY
WITH
THE
NUMBER
OF
MODEL
PARAMETERS
IF
PARAMETERS
AND
CHOICES
FOR
EACH
SEARCH
IS
O
NON
TARGET
SHAPES
CAN
PRODUCE
SPURIOUS
PEAKS
IN
PARAMETER
SPACE
QUANTIZATION
CAN
BE
TRICKY
TO
PICK
A
GOOD
GRID
SIZE
MODIFIED
FROM
KRISTEN
GRAUMAN
FISCHLER
BOLLES
IN
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
RANSAC
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
N
I
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
ALGORITHM
N
I
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
PROS
RANSAC
PROS
AND
CONS
SIMPLE
AND
GENERAL
APPLICABLE
TO
MANY
DIFFERENT
PROBLEMS
OFTEN
WORKS
WELL
IN
PRACTICE
CONS
LOTS
OF
PARAMETERS
TO
TUNE
DOESN
T
WORK
WELL
FOR
LOW
INLIER
RATIOS
TOO
MANY
ITERATIONS
OR
CAN
FAIL
COMPLETELY
CAN
T
ALWAYS
GET
A
GOOD
INITIALIZATION
OF
THE
MODEL
BASED
ON
THE
MINIMUM
NUMBER
OF
SAMPLES
COMMON
APPLICATIONS
IMAGE
STITCHING
RELATING
TWO
VIEWS
TODAY
REVIEW
FITTING
HOUGH
TRANSFORM
RANSAC
RETRIEVING
OBJECT
INSTANCES
INDEXING
BY
VISUAL
WORDS
SPATIAL
VERIFICATION
ALIGNMENT
PROBLEM
WE
HAVE
PREVIOUSLY
CONSIDERED
HOW
TO
FIT
A
MODEL
TO
IMAGE
EVIDENCE
E
G
A
LINE
TO
EDGE
POINTS
IN
ALIGNMENT
WE
WILL
FIT
THE
PARAMETERS
OF
SOME
TRANSFORMATION
ACCORDING
TO
A
SET
OF
MATCHING
FEATURE
PAIRS
CORRESPONDENCES
DIFFICULTIES
XI
NOISE
OUTLIERS
XI
EXAMPLES
OF
PARAMETRIC
WARPS
TRANSLATION
ROTATION
ASPECT
AFFINE
PERSPECTIVE
P
X
Y
P
X
Y
TRANSFORMATION
T
IS
A
COORDINATE
CHANGING
MACHINE
P
T
P
WHAT
DOES
IT
MEAN
THAT
T
IS
GLOBAL
IS
THE
SAME
FOR
ANY
POINT
P
CAN
BE
DESCRIBED
BY
JUST
A
FEW
NUMBERS
PARAMETERS
LET
REPRESENT
T
AS
A
MATRIX
P
MP
X
Y
X
Y
SCALING
A
COORDINATE
MEANS
MULTIPLYING
EACH
OF
ITS
COMPONENTS
BY
A
SCALAR
UNIFORM
SCALING
MEANS
THIS
SCALAR
IS
THE
SAME
FOR
ALL
COMPONENTS
NON
UNIFORM
SCALING
DIFFERENT
SCALARS
PER
COMPONENT
X
Y
SCALING
OPERATION
OR
IN
MATRIX
FORM
X
AX
Y
BY
XX
YY
A
XX
B
YY
SCALING
MATRIX
X
Y
A
C
B
X
D
Y
ONLY
LINEAR
TRANSFORMATIONS
CAN
BE
REPRESENTED
WITH
A
MATRIX
LINEAR
TRANSFORMATIONS
ARE
COMBINATIONS
OF
SCALE
ROTATION
SHEAR
AND
MIRROR
X
A
B
C
X
AFFINE
TRANSFORMATIONS
ARE
COMBINATIONS
OF
LINEAR
TRANSFORMATIONS
AND
Y
D
E
F
TRANSLATIONS
PROPERTIES
OF
AFFINE
TRANSFORMATIONS
LINES
MAP
TO
LINES
OR
PARALLEL
LINES
REMAIN
PARALLEL
RATIOS
ARE
PRESERVED
X
Y
A
B
D
E
C
X
F
Y
CLOSED
UNDER
COMPOSITION
ASSUMING
WE
KNOW
THE
CORRESPONDENCES
HOW
DO
WE
GET
THE
TRANSFORMATION
M
XI
XI
Y
M
M
Y
T
M
I
I
ALYOSHA
EFROS
WHAT
ARE
THE
CORRESPONDENCES
COMPARE
CONTENT
IN
LOCAL
PATCHES
FIND
BEST
MATCHES
E
G
SIMPLEST
APPROACH
SCAN
WITH
TEMPLATE
AND
COMPUTE
SSD
OR
CORRELATION
BETWEEN
LIST
OF
PIXEL
INTENSITIES
IN
THE
PATCH
KRISTEN
GRAUMAN
N
PIXELS
D
F
A
FB
T
GRAUMAN
B
LEIBE
FIND
A
SET
OF
DISTINCTIVE
KEY
POINTS
DEFINE
A
REGION
AROUND
EACH
KEYPOINT
EXTRACT
AND
NORMALIZE
THE
REGION
CONTENT
COMPUTE
A
LOCAL
DESCRIPTOR
FROM
THE
NORMALIZED
REGION
MATCH
LOCAL
DESCRIPTORS
GIVEN
MATCHED
POINTS
IN
A
AND
B
ESTIMATE
THE
TRANSLATION
OF
THE
OBJECT
X
B
X
A
TX
I
I
I
I
T
Y
DEREK
HOIEM
TX
TY
LEAST
SQUARES
SOLUTION
X
B
X
A
TX
WRITE
DOWN
OBJECTIVE
FUNCTION
IN
FORM
AX
B
I
I
SOLVE
USING
PSEUDO
INVERSE
OR
EIGENVALUE
DECOMPOSITION
I
I
T
Y
X
B
X
A
Y
B
Y
A
TX
Y
X
B
X
A
N
N
DEREK
HOIEM
Y
B
Y
A
N
N
TX
TY
PROBLEM
OUTLIERS
RANSAC
SOLUTION
X
B
X
A
TX
SAMPLE
A
SET
OF
MATCHING
POINTS
PAIR
I
I
SOLVE
FOR
TRANSFORMATION
PARAMETERS
SCORE
PARAMETERS
WITH
NUMBER
OF
INLIERS
REPEAT
STEPS
N
TIMES
SOLVE
USING
LEAST
SQUARES
WITH
INLIERS
I
I
T
Y
TX
TY
PROBLEM
OUTLIERS
MULTIPLE
OBJECTS
AND
OR
MANY
TO
ONE
MATCHES
HOUGH
TRANSFORM
SOLUTION
X
B
X
A
TX
INITIALIZE
A
GRID
OF
PARAMETER
VALUES
I
I
EACH
MATCHED
PAIR
CASTS
A
VOTE
FOR
CONSISTENT
VALUES
FIND
THE
PARAMETERS
WITH
THE
MOST
VOTES
SOLVE
USING
LEAST
SQUARES
WITH
INLIERS
I
I
BOARD
T
Y
EACH
FEATURE
MATCH
GIVES
AN
ALIGNMENT
HYPOTHESIS
FOR
SCALE
TRANSLATION
AND
ORIENTATION
OF
MODEL
IN
IMAGE
ASSUMING
WE
USE
SCALE
ROTATION
AND
TRANSLATION
INVARIANT
LOCAL
FEATURES
A
HYPOTHESIS
GENERATED
BY
A
SINGLE
MATCH
MAY
BE
UNRELIABLE
SO
LET
EACH
MATCH
VOTE
FOR
A
HYPOTHESIS
IN
HOUGH
SPACE
GEN
HOUGH
TRANSFORM
DETAILS
LOWE
SYSTEM
TRAINING
PHASE
FOR
EACH
MODEL
FEATURE
RECORD
LOCATION
SCALE
AND
ORIENTATION
OF
MODEL
RELATIVE
TO
NORMALIZED
FEATURE
FRAME
TEST
PHASE
LET
EACH
MATCH
BETWEEN
A
TEST
SIFT
FEATURE
AND
A
MODEL
FEATURE
VOTE
IN
A
HOUGH
SPACE
X
Y
LOCATION
ORIENTATION
SCALE
FIND
ALL
BINS
WITH
AT
LEAST
THREE
VOTES
AND
PERFORM
GEOMETRIC
VERIFICATION
ESTIMATE
LEAST
SQUARES
AFFINE
TRANSFORMATION
SEARCH
FOR
ADDITIONAL
FEATURES
THAT
AGREE
WITH
THE
ALIGNMENT
OBJECT
FOUND
IF
AT
LEAST
T
MATCHED
POINTS
FOUND
DAVID
G
LOWE
IJCV
PP
EXAMPLE
RESULT
BACKGROUND
SUBTRACT
FOR
MODEL
BOUNDARIES
LOWE
OBJECTS
RECOGNIZED
RECOGNITION
IN
SPITE
OF
OCCLUSION
FITTING
AND
MATCHING
SUMMARY
FITTING
PROBLEMS
REQUIRE
FINDING
ANY
SUPPORTING
EVIDENCE
FOR
A
MODEL
EVEN
WITHIN
CLUTTER
AND
MISSING
FEATURES
VOTING
AND
INLIER
APPROACHES
SUCH
AS
THE
HOUGH
TRANSFORM
AND
RANSAC
MAKE
IT
POSSIBLE
TO
FIND
LIKELY
MODEL
PARAMETERS
WITHOUT
SEARCHING
ALL
COMBINATIONS
OF
FEATURES
CAN
USE
THESE
APPROACHES
TO
COMPUTE
ROBUST
FEATURE
ALIGNMENT
MATCHING
AND
TO
MATCH
OBJECT
TEMPLATES
ADAPTED
FROM
KRISTEN
GRAUMAN
AND
DEREK
HOIEM
JOHN
PHILLIPS
TODAY
REVIEW
FITTING
HOUGH
TRANSFORM
RANSAC
MATCHING
POINTS
SPATIAL
VERIFICATION
EACH
PATCH
REGION
HAS
A
DESCRIPTOR
WHICH
IS
A
POINT
IN
SOME
HIGH
DIMENSIONAL
FEATURE
SPACE
E
G
SIFT
KRISTEN
GRAUMAN
WHEN
WE
SEE
CLOSE
POINTS
IN
FEATURE
SPACE
WE
HAVE
SIMILAR
DESCRIPTORS
WHICH
INDICATES
SIMILAR
LOCAL
CONTENT
KRISTEN
GRAUMAN
DATABASE
IMAGES
WITH
POTENTIALLY
THOUSANDS
OF
FEATURES
PER
IMAGE
AND
HUNDREDS
TO
MILLIONS
OF
IMAGES
TO
SEARCH
HOW
TO
EFFICIENTLY
FIND
THOSE
THAT
ARE
RELEVANT
TO
A
NEW
IMAGE
KRISTEN
GRAUMAN
INDEXING
LOCAL
FEATURES
INVERTED
FILE
INDEX
FOR
TEXT
DOCUMENTS
AN
EFFICIENT
WAY
TO
FIND
ALL
PAGES
ON
WHICH
A
WORD
OCCURS
IS
TO
USE
AN
INDEX
WE
WANT
TO
FIND
ALL
IMAGES
IN
WHICH
A
FEATURE
OCCURS
TO
USE
THIS
IDEA
WE
LL
NEED
TO
MAP
OUR
FEATURES
TO
VISUAL
WORDS
KRISTEN
GRAUMAN
EXTRACT
SOME
LOCAL
FEATURES
FROM
A
NUMBER
OF
IMAGES
E
G
SIFT
DESCRIPTOR
SPACE
EACH
POINT
IS
DIMENSIONAL
EACH
POINT
IS
A
LOCAL
DESCRIPTOR
E
G
SIFT
VECTOR
EXAMPLE
EACH
VISUAL
WORDS
GROUP
OF
PATCHES
BELONGS
TO
THE
SAME
VISUAL
WORD
FIGURE
FROM
SIVIC
ZISSERMAN
ICCV
INDEX
DISPLACEMENTS
BY
VISUAL
CODEWORD
TEST
IMAGE
LEARNING
IN
COMPUTER
VISION
BUILD
CODEBOOK
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MORE
ON
THIS
LATER
IN
THE
COURSE
VISUAL
WORDS
MAP
HIGH
DIMENSIONAL
DESCRIPTORS
TO
TOKENS
WORDS
BY
QUANTIZING
THE
FEATURE
SPACE
QUANTIZE
VIA
CLUSTERING
LET
CLUSTER
CENTERS
BE
THE
PROTOTYPE
WORDS
DETERMINE
WHICH
WORD
TO
ASSIGN
TO
EACH
NEW
IMAGE
REGION
BY
FINDING
THE
CLOSEST
CLUSTER
CENTER
DATABASE
IMAGES
ARE
LOADED
INTO
THE
INDEX
MAPPING
WORDS
TO
IMAGE
NUMBERS
WHEN
WILL
THIS
GIVE
US
A
SIGNIFICANT
GAIN
IN
EFFICIENCY
NEW
QUERY
IMAGE
IS
MAPPED
TO
INDICES
OF
DATABASE
IMAGES
THAT
SHARE
A
WORD
WE
CAN
CALL
THIS
RETRIEVAL
PROCESS
INSTANCE
RECOGNITION
INSTANCE
RECOGNITION
REMAINING
ISSUES
HOW
TO
SUMMARIZE
THE
CONTENT
OF
AN
ENTIRE
IMAGE
AND
GAUGE
OVERALL
SIMILARITY
HOW
LARGE
SHOULD
THE
VOCABULARY
BE
HOW
TO
PERFORM
QUANTIZATION
EFFICIENTLY
IS
HAVING
THE
SAME
SET
OF
VISUAL
WORDS
ENOUGH
TO
IDENTIFY
THE
OBJECT
SCENE
HOW
TO
VERIFY
SPATIAL
AGREEMENT
ANALOGY
TO
DOCUMENTS
OF
ALL
THE
SENSORY
IMPRESSIONS
PROCEEDING
TO
THE
BRAIN
THE
VISUAL
EXPERIENCES
ARE
THE
DOMINANT
ONES
OUR
PERCEPTION
OF
THE
WORLD
AROUND
US
IS
BASED
ESSENTIALLY
ON
THE
MESSAGES
THAT
REACH
THE
BRAIN
FROM
OUR
EYES
FOR
A
LONG
TIME
IT
WAS
THOUGHT
THAT
THE
RETINAL
IMAGE
WAS
TRASNSEMNITTSEOD
PROYI
NTBBRYAPIONIN
T
TO
VISUAL
CENTERS
IN
THVEISBRUAIAN
L
THPE
ECERRCEBERAPL
TCIOORTNEX
WAS
A
MOVIE
SCREEN
SO
TO
SPEAK
UPON
WHICH
THE
IMAGE
INRTEHTEIENYAE
LW
ACSEPRROEJEBCTREADL
TCHORORUTGEHXTH
E
DISCOVERIES
OEF
HYUEB
ELCAENDLLW
OIEPSETLIWCEANLOW
KNOW
THAT
BEHIND
THE
ORIGIN
OF
THE
VISUAL
PERCEPTION
IN
THE
BRAIN
THERE
IS
A
CONSIDERABLY
MORE
COMPLICATHEDUCBOUERSLE
OWF
EIVEESNTES
LBY
FOLLOWING
THE
VISUAL
IMPULSES
ALONG
THEIR
PATH
TO
THE
VARIOUS
CELL
LAYERS
OF
THE
OPTICAL
CORTEX
HUBEL
AND
WIESEL
HAVE
BEEN
ABLE
TO
DEMONSTRATE
THAT
THE
MESSAGE
ABOUT
THE
IMAGE
FALLING
ON
THE
RETINA
UNDERGOES
A
STEP
WISE
ANALYSIS
IN
A
SYSTEM
OF
NERVE
CELLS
STORED
IN
COLUMNS
IN
THIS
SYSTEM
EACH
CELL
HAS
ITS
SPECIFIC
FUNCTION
AND
IS
RESPONSIBLE
FOR
A
SPECIFIC
DETAIL
IN
THE
PATTERN
OF
THE
RETINAL
IMAGE
CHINA
IS
FORECASTING
A
TRADE
SURPLUS
OF
TO
THIS
YEAR
A
THREEFOLD
INCREASE
ON
THE
COMMERCE
MINISTRY
SAID
THE
SURPLUS
WOULD
BE
CREATED
BY
A
PREDICTED
JUMP
IN
EXPORTS
TO
COMPARED
WITH
A
RISE
IN
IMPORTS
TO
THE
FIGCURHESINARAE
LIKTERLAY
DTOEFU
RTHER
ANNOY
THESUUSR
PWHLUICHSH
ACSOLOMNGMARGEURECDETH
AT
CHINA
EXPORTS
ARE
UNFAIRLY
HELPED
BY
A
DELIBERATEELYXUPNODERRTVSAL
UEIMD
YPUAONR
TBSE
IJUINGS
AGREES
TYHEUSAUNRP
LUBS
IAS
NTOKO
HIDGHO
MBUTESSAYTSICTH
E
YUAN
IS
ONLY
ONE
FACTOR
BANK
OF
CHINA
GOVERNOR
ZHOU
XIAOCHUAN
SAID
THE
COUNTRY
ALSO
NEEDED
TO
DTORMAODREET
OVBAOOLUSTEDOMESTIC
DEMAND
SO
MORE
GOODS
STAYED
WITHIN
THE
COUNTRY
CHINA
INCREASED
THE
VALUE
OF
THE
YUAN
AGAINST
THE
DOLLAR
BY
IN
JULY
AND
PERMITTED
IT
TO
TRADE
WITHIN
A
NARROW
BAND
BUT
THE
US
WANTS
THE
YUAN
TO
BE
ALLOWED
TO
TRADE
FREELY
HOWEVER
BEIJING
HAS
MADE
IT
CLEAR
THAT
IT
WILL
TAKE
ITS
TIME
AND
TREAD
CAREFULLY
BEFORE
ALLOWING
THE
YUAN
TO
RISE
FURTHER
IN
VALUE
ICCV
SHORT
COURSE
L
FEI
FEI
BAGS
OF
VISUAL
WORDS
SUMMARIZE
ENTIRE
IMAGE
BASED
ON
ITS
DISTRIBUTION
HISTOGRAM
OF
WORD
OCCURRENCES
ANALOGOUS
TO
BAG
OF
WORDS
REPRESENTATION
COMMONLY
USED
FOR
DOCUMENTS
COMPARING
BAGS
OF
WORDS
RANK
FRAMES
BY
NORMALIZED
SCALAR
PRODUCT
BETWEEN
THEIR
POSSIBLY
WEIGHTED
OCCURRENCE
COUNTS
NEAREST
NEIGHBOR
SEARCH
FOR
SIMILAR
IMAGES
𝑠𝑖𝑚
𝑉
𝑑𝑗
𝑖
𝑞
𝑖
FOR
VOCABULARY
OF
V
WORDS
J
KRISTEN
GRAUMAN
BAGS
OF
WORDS
PROS
AND
CONS
FLEXIBLE
TO
GEOMETRY
DEFORMATIONS
VIEWPOINT
COMPACT
SUMMARY
OF
IMAGE
CONTENT
VERY
GOOD
RESULTS
IN
PRACTICE
BASIC
MODEL
IGNORES
GEOMETRY
MUST
VERIFY
AFTERWARDS
OR
ENCODE
VIA
FEATURES
BACKGROUND
AND
FOREGROUND
MIXED
WHEN
BAG
COVERS
WHOLE
IMAGE
OPTIMAL
VOCABULARY
FORMATION
REMAINS
UNCLEAR
ADAPTED
FROM
KRISTEN
GRAUMAN
INVERTED
FILE
INDEX
AND
BAGS
OF
WORDS
SIMILARITY
EXTRACT
WORDS
IN
QUERY
INVERTED
FILE
INDEX
TO
FIND
RELEVANT
FRAMES
COMPARE
WORD
COUNTS
ADAPTED
FROM
KRISTEN
GRAUMAN
TF
IDF
WEIGHTING
TERM
FREQUENCY
INVERSE
DOCUMENT
FREQUENCY
DESCRIBE
FRAME
BY
FREQUENCY
OF
EACH
WORD
WITHIN
IT
DOWNWEIGHT
WORDS
THAT
APPEAR
OFTEN
IN
THE
DATABASE
STANDARD
WEIGHTING
FOR
TEXT
RETRIEVAL
NUMBER
OF
OCCURRENCES
OF
WORD
I
IN
DOCUMENT
D
NUMBER
OF
WORDS
IN
DOCUMENT
D
TOTAL
NUMBER
OF
DOCUMENTS
IN
DATABASE
NUMBER
OF
DOCUMENTS
WORD
I
OCCURS
IN
IN
WHOLE
DATABASE
KRISTEN
GRAUMAN
BAGS
OF
WORDS
FOR
CONTENT
BASED
IMAGE
RETRIEVAL
SLIDE
FROM
ANDREW
ZISSERMAN
EXAMPLE
RETRIEVED
SHOTS
R
I
ILVK
T
T
J
STRU
T
FNUN
E
KEY
FRAME
END
FRAME
ST
UT
FNUNE
F
KEY
FRAME
F
TI
C
T
R
I
R
SHUT
FRAME
FRA
ME
JI
K
EY
CRAINE
END
FRAME
O
I
T
R
I
I
I
I
J
F
SLIDE
FROM
ANDREW
ZISSERMAN
FRAME
L
R
I
FRAME
KE
FR
E
L
O
R
I
P
KEY
FRAME
ETL
L
F
ENCL
FNUNE
VIDEO
GOOGLE
SYSTEM
COLLECT
ALL
WORDS
WITHIN
QUERY
REGION
INVERTED
FILE
INDEX
TO
FIND
RELEVANT
FRAMES
COMPARE
WORD
COUNTS
SPATIAL
VERIFICATION
SIVIC
ZISSERMAN
ICCV
DEMO
ONLINE
AT
ESEARCH
VGOOGLE
INDEX
HTML
K
GRAUMAN
B
LEIBE
SCORING
RETRIEVAL
QUALITY
QUERY
DATABASE
SIZE
IMAGES
RELEVANT
TOTAL
IMAGES
RESULTS
ORDERED
PRECISION
RELEVANT
RETURNED
RECALL
RELEVANT
TOTAL
RELEVANT
RECALL
INSTANCE
RECOGNITION
REMAINING
ISSUES
HOW
TO
SUMMARIZE
THE
CONTENT
OF
AN
ENTIRE
IMAGE
AND
GAUGE
OVERALL
SIMILARITY
HOW
LARGE
SHOULD
THE
VOCABULARY
BE
HOW
TO
PERFORM
QUANTIZATION
EFFICIENTLY
IS
HAVING
THE
SAME
SET
OF
VISUAL
WORDS
ENOUGH
TO
IDENTIFY
THE
OBJECT
SCENE
HOW
TO
VERIFY
SPATIAL
AGREEMENT
VOCABULARY
SIZE
RESULTS
FOR
RECOGNITION
TASK
WITH
IMAGES
INFLUENCE
ON
PERFORMANCE
SPARSITY
NISTER
STEWENIUS
CVPR
VOCABULARY
TREES
HIERARCHICAL
CLUSTERING
FOR
LARGE
VOCABULARIES
TREE
CONSTRUCTION
NISTER
STEWENIUS
CVPR
SLIDE
CREDIT
DAVID
NISTER
VOCABULARY
TREE
TRAINING
FILLING
THE
TREE
NISTER
STEWENIUS
CVPR
VOCABULARY
TREE
TRAINING
FILLING
THE
NISTER
STEWENIUS
CVPR
VOCABULARY
TREE
TRAINING
FILLING
THE
TREE
NISTER
STEWENIUS
CVPR
VOCABULARY
TREE
RECOGNITION
NISTER
STEWENIUS
CVPR
SLIDE
CREDIT
DAVID
NISTER
COMPLEXITY
WHAT
IS
THE
COMPUTATIONAL
ADVANTAGE
OF
THE
HIERARCHICAL
REPRESENTATION
BAG
OF
WORDS
VS
A
FLAT
VOCABULARY
COMPLEXITY
DEPENDS
ON
BRANCHING
FACTOR
AND
NUMBER
OF
LEVELS
INSTANCE
RECOGNITION
REMAINING
ISSUES
HOW
TO
SUMMARIZE
THE
CONTENT
OF
AN
ENTIRE
IMAGE
AND
GAUGE
OVERALL
SIMILARITY
HOW
LARGE
SHOULD
THE
VOCABULARY
BE
HOW
TO
PERFORM
QUANTIZATION
EFFICIENTLY
IS
HAVING
THE
SAME
SET
OF
VISUAL
WORDS
ENOUGH
TO
IDENTIFY
THE
OBJECT
SCENE
HOW
TO
VERIFY
SPATIAL
AGREEMENT
TODAY
REVIEW
FITTING
HOUGH
TRANSFORM
RANSAC
MATCHING
POINTS
RETRIEVING
OBJECT
INSTANCES
INDEXING
BY
VISUAL
WORDS
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
BOTH
IMAGE
PAIRS
HAVE
MANY
VISUAL
WORDS
IN
COMMON
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
ONLY
SOME
OF
THE
MATCHES
ARE
MUTUALLY
CONSISTENT
SPATIAL
VERIFICATION
TWO
BASIC
STRATEGIES
RANSAC
TYPICALLY
SORT
BY
BOW
SIMILARITY
AS
INITIAL
FILTER
VERIFY
BY
CHECKING
SUPPORT
INLIERS
FOR
POSSIBLE
TRANSFORMATIONS
E
G
SUCCESS
IF
FIND
A
TRANSFORMATION
WITH
N
INLIER
CORRESPONDENCES
GENERALIZED
HOUGH
TRANSFORM
LET
EACH
MATCHED
FEATURE
CAST
A
VOTE
ON
LOCATION
SCALE
ORIENTATION
OF
THE
MODEL
OBJECT
VERIFY
PARAMETERS
WITH
ENOUGH
VOTES
VIDEO
GOOGLE
SYSTEM
COLLECT
ALL
WORDS
WITHIN
QUERY
REGION
INVERTED
FILE
INDEX
TO
FIND
RELEVANT
FRAMES
COMPARE
WORD
COUNTS
SPATIAL
VERIFICATION
SIVIC
ZISSERMAN
ICCV
DEMO
ONLINE
AT
ESEARCH
VGOOGLE
INDEX
HTML
KRISTEN
GRAUMAN
EXAMPLE
APPLICATIONS
MOBILE
TOURIST
GUIDE
SELF
LOCALIZATION
OBJECT
BUILDING
RECOGNITION
PHOTO
VIDEO
AUGMENTATION
B
LEIBE
QUACK
LEIBE
VAN
GOOL
CIVR
INDEXING
AND
RETRIEVAL
SUMMARY
BAG
OF
WORDS
REPRESENTATION
QUANTIZE
FEATURE
SPACE
TO
MAKE
DISCRETE
SET
OF
VISUAL
WORDS
SUMMARIZE
IMAGE
BY
DISTRIBUTION
OF
WORDS
INDEX
INDIVIDUAL
WORDS
INVERTED
INDEX
PRE
COMPUTE
INDEX
TO
ENABLE
FASTER
SEARCH
AT
QUERY
TIME
RECOGNITION
OF
INSTANCES
VIA
ALIGNMENT
MATCHING
LOCAL
FEATURES
FOLLOWED
BY
SPATIAL
VERIFICATION
ROBUST
FITTING
RANSAC
GHT
ADAPTED
FROM
KRISTEN
GRAUMAN
CS
INTRO
TO
COMPUTER
VISION
PROJECTIVE
TRANSFORMATIONS
IMAGE
STITCHING
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
PART
III
B
DEGREE
BINS
DEGREE
BINS
DUE
ON
THURSDAY
WRAP
UP
INDEXING
AND
RETRIEVAL
IMAGE
TRANSFORMATIONS
LINEAR
AND
AFFINE
IMAGE
FORMATION
IMAGE
STITCHING
COMPUTING
HOMOGRAPHIES
INDEXING
LOCAL
FEATURES
INVERTED
FILE
INDEX
FOR
TEXT
DOCUMENTS
AN
EFFICIENT
WAY
TO
FIND
ALL
PAGES
ON
WHICH
A
WORD
OCCURS
IS
TO
USE
AN
INDEX
WE
WANT
TO
FIND
ALL
IMAGES
IN
WHICH
A
FEATURE
OCCURS
TO
USE
THIS
IDEA
WE
LL
NEED
TO
MAP
OUR
FEATURES
TO
VISUAL
WORDS
KRISTEN
GRAUMAN
EXTRACT
SOME
LOCAL
FEATURES
FROM
A
NUMBER
OF
IMAGES
E
G
SIFT
DESCRIPTOR
SPACE
EACH
POINT
IS
DIMENSIONAL
EXAMPLE
EACH
GROUP
OF
PATCHES
BELONGS
TO
THE
SAME
VISUAL
WORD
FIGURE
FROM
SIVIC
ZISSERMAN
ICCV
MAP
HIGH
DIMENSIONAL
DESCRIPTORS
TO
TOKENS
WORDS
BY
QUANTIZING
THE
FEATURE
SPACE
QUANTIZE
VIA
CLUSTERING
LET
CLUSTER
CENTERS
BE
THE
PROTOTYPE
WORDS
DETERMINE
WHICH
WORD
TO
ASSIGN
TO
EACH
NEW
IMAGE
REGION
BY
FINDING
THE
CLOSEST
CLUSTER
CENTER
COMPARING
BAGS
OF
WORDS
RANK
FRAMES
BY
NORMALIZED
SCALAR
PRODUCT
BETWEEN
THEIR
POSSIBLY
WEIGHTED
OCCURRENCE
COUNTS
NEAREST
NEIGHBOR
SEARCH
FOR
SIMILAR
IMAGES
𝑠𝑖𝑚
𝑉
𝑑𝑗
𝑖
𝑞
𝑖
FOR
VOCABULARY
OF
V
WORDS
J
INVERTED
FILE
INDEX
AND
BAGS
OF
WORDS
SIMILARITY
EXTRACT
WORDS
IN
QUERY
INVERTED
FILE
INDEX
TO
FIND
RELEVANT
FRAMES
COMPARE
WORD
COUNTS
BAGS
OF
WORDS
FOR
CONTENT
BASED
IMAGE
RETRIEVAL
SLIDE
FROM
ANDREW
ZISSERMAN
EXAMPLE
RETRIEVED
SHOTS
R
I
ILVK
T
T
J
STRU
T
FNUN
E
KEY
FRAME
END
FRAME
ST
UT
FNUNE
F
KEY
FRAME
F
TI
C
T
R
I
R
SHUT
FRAME
FRA
ME
JI
K
EY
CRAINE
END
FRAME
O
I
T
R
I
I
I
I
J
F
SLIDE
FROM
ANDREW
ZISSERMAN
FRAME
L
R
I
FRAME
KE
FR
E
L
O
R
I
P
KEY
FRAME
ETL
L
F
ENCL
FNUNE
VIDEO
GOOGLE
SYSTEM
COLLECT
ALL
WORDS
WITHIN
QUERY
REGION
INVERTED
FILE
INDEX
TO
FIND
RELEVANT
FRAMES
COMPARE
WORD
COUNTS
SPATIAL
VERIFICATION
SIVIC
ZISSERMAN
ICCV
DEMO
ONLINE
AT
ESEARCH
VGOOGLE
INDEX
HTML
K
GRAUMAN
B
LEIBE
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
BOTH
IMAGE
PAIRS
HAVE
MANY
VISUAL
WORDS
IN
COMMON
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
ONLY
SOME
OF
THE
MATCHES
ARE
MUTUALLY
CONSISTENT
SPATIAL
VERIFICATION
TWO
BASIC
STRATEGIES
RANSAC
TYPICALLY
SORT
BY
BOW
SIMILARITY
AS
INITIAL
FILTER
VERIFY
BY
CHECKING
SUPPORT
INLIERS
FOR
POSSIBLE
TRANSFORMATIONS
E
G
SUCCESS
IF
FIND
A
TRANSFORMATION
WITH
N
INLIER
CORRESPONDENCES
GENERALIZED
HOUGH
TRANSFORM
LET
EACH
MATCHED
FEATURE
CAST
A
VOTE
ON
LOCATION
SCALE
ORIENTATION
OF
THE
TRANSFORMATION
VERIFY
PARAMETERS
WITH
ENOUGH
VOTES
TX
TY
PROBLEM
OUTLIERS
RANSAC
SOLUTION
X
B
X
A
TX
SAMPLE
A
SET
OF
MATCHING
POINTS
PAIR
I
I
SOLVE
FOR
TRANSFORMATION
PARAMETERS
SCORE
PARAMETERS
WITH
NUMBER
OF
INLIERS
REPEAT
STEPS
N
TIMES
I
I
T
Y
TX
TY
PROBLEM
OUTLIERS
MULTIPLE
OBJECTS
AND
OR
MANY
TO
ONE
MATCHES
HOUGH
TRANSFORM
SOLUTION
X
B
X
A
TX
INITIALIZE
A
GRID
OF
PARAMETER
VALUES
I
I
EACH
MATCHED
PAIR
CASTS
A
VOTE
FOR
CONSISTENT
VALUES
FIND
THE
PARAMETERS
WITH
THE
MOST
VOTES
SOLVE
USING
LEAST
SQUARES
WITH
INLIERS
I
I
T
Y
SCORING
RETRIEVAL
QUALITY
QUERY
DATABASE
SIZE
IMAGES
RELEVANT
TOTAL
IMAGES
RESULTS
ORDERED
PRECISION
RELEVANT
RETURNED
RECALL
RELEVANT
TOTAL
RELEVANT
RECALL
EXAMPLE
APPLICATIONS
MOBILE
TOURIST
GUIDE
OBJECT
BUILDING
RECOGNITION
SELF
LOCALIZATION
PHOTO
VIDEO
AUGMENTATION
B
LEIBE
QUACK
LEIBE
VAN
GOOL
CIVR
INDEXING
AND
RETRIEVAL
SUMMARY
BAG
OF
WORDS
REPRESENTATION
QUANTIZE
FEATURE
SPACE
TO
MAKE
DISCRETE
SET
OF
VISUAL
WORDS
SUMMARIZE
IMAGE
BY
DISTRIBUTION
OF
WORDS
INDEX
INDIVIDUAL
WORDS
INVERTED
INDEX
PRE
COMPUTE
INDEX
TO
ENABLE
FASTER
SEARCH
AT
QUERY
TIME
RECOGNITION
OF
INSTANCES
VIA
ALIGNMENT
MATCHING
LOCAL
FEATURES
FOLLOWED
BY
SPATIAL
VERIFICATION
ROBUST
FITTING
RANSAC
GHT
TODAY
WRAP
UP
INDEXING
AND
RETRIEVAL
IMAGE
FORMATION
IMAGE
STITCHING
COMPUTING
HOMOGRAPHIES
EXAMPLES
OF
PARAMETRIC
WARPS
TRANSLATION
ROTATION
ASPECT
AFFINE
PERSPECTIVE
P
X
Y
P
X
Y
TRANSFORMATION
T
IS
A
COORDINATE
CHANGING
MACHINE
P
T
P
WHAT
DOES
IT
MEAN
THAT
T
IS
GLOBAL
IS
THE
SAME
FOR
ANY
POINT
P
CAN
BE
DESCRIBED
BY
JUST
A
FEW
NUMBERS
PARAMETERS
LET
REPRESENT
T
AS
A
MATRIX
P
MP
X
Y
X
Y
SCALING
X
X
X
X
SX
X
Y
Y
Y
SY
Y
Y
ROTATE
AROUND
X
COS
X
SIN
Y
X
COS
SIN
X
Y
SIN
X
COS
Y
Y
SIN
COS
Y
SHEAR
X
X
SHX
Y
X
SHX
X
Y
SH
X
Y
Y
SH
Y
Y
Y
MIRROR
ABOUT
Y
AXIS
X
X
X
X
Y
Y
Y
Y
MIRROR
OVER
X
X
X
X
Y
Y
Y
Y
TRANSLATION
X
Y
X
TX
Y
T
Y
NO
LINEAR
TRANSFORMATIONS
X
Y
A
C
B
X
D
Y
ONLY
LINEAR
TRANSFORMATIONS
CAN
BE
REPRESENTED
WITH
A
MATRIX
LINEAR
TRANSFORMATIONS
ARE
COMBINATIONS
OF
SCALE
ROTATION
SHEAR
AND
MIRROR
HOMOGENEOUS
COORDINATES
TO
CONVERT
TO
HOMOGENEOUS
COORDINATES
HOMOGENEOUS
IMAGE
COORDINATES
CONVERTING
FROM
HOMOGENEOUS
COORDINATES
HOMOGENEOUS
COORDINATES
XX
TX
XX
XX
TTXX
YY
Y
Y
T
TY
Y
Y
TYY
TX
TY
X
A
B
C
X
Y
D
E
F
Y
W
W
AFFINE
TRANSFORMATIONS
ARE
COMBINATIONS
OF
LINEAR
TRANSFORMATIONS
AND
TRANSLATIONS
PARALLEL
LINES
REMAIN
PARALLEL
ASSUMING
WE
KNOW
THE
CORRESPONDENCES
HOW
DO
WE
GET
THE
TRANSFORMATION
M
XI
XI
Y
M
M
Y
T
M
I
I
ALYOSHA
EFROS
M
XI
YI
XI
X
Y
M
Y
I
I
I
HOW
MANY
MATCHES
CORRESPONDENCE
PAIRS
DO
WE
NEED
TO
SOLVE
FOR
THE
TRANSFORMATION
PARAMETERS
ONCE
WE
HAVE
SOLVED
FOR
THE
PARAMETERS
HOW
DO
WE
COMPUTE
X
NEW
Y
NEW
GIVEN
XNEW
YNEW
WHERE
DO
THE
MATCHES
COME
FROM
RECALL
FITTING
AN
AFFINE
TRANSFORMATION
THE
AFFINE
MODEL
APPROXIMATES
PERSPECTIVE
PROJECTION
PROJECTIVE
TRANSFORMATIONS
X
A
B
C
X
Y
W
D
E
G
H
F
Y
I
W
PROJECTIVE
TRANSFORMATIONS
AFFINE
TRANSFORMATIONS
AND
PROJECTIVE
WARPS
PARALLEL
LINES
DO
NOT
NECESSARILY
REMAIN
PARALLEL
MAIN
QUESTIONS
ALIGNMENT
GIVEN
TWO
IMAGES
WHAT
IS
THE
TRANSFORMATION
BETWEEN
THEM
WARPING
GIVEN
A
SOURCE
IMAGE
AND
A
TRANSFORMATION
WHAT
DOES
THE
TRANSFORMED
OUTPUT
LOOK
LIKE
MOTIVATION
FOR
FEATURE
BASED
ALIGNMENT
IMAGE
MOSAICS
TODAY
WRAP
UP
INDEXING
AND
RETRIEVAL
IMAGE
TRANSFORMATIONS
LINEAR
AND
AFFINE
IMAGE
STITCHING
COMPUTING
HOMOGRAPHIES
HOW
ARE
OBJECTS
IN
THE
WORLD
CAPTURED
IN
AN
IMAGE
LET
DESIGN
A
CAMERA
IDEA
PUT
A
PIECE
OF
FILM
IN
FRONT
OF
AN
OBJECT
DO
WE
GET
A
REASONABLE
IMAGE
IDEA
ADD
A
BARRIER
TO
BLOCK
OFF
MOST
OF
THE
RAYS
THIS
REDUCES
BLURRING
THE
OPENING
IS
KNOWN
AS
THE
APERTURE
HOW
DOES
THIS
TRANSFORM
THE
IMAGE
PINHOLE
CAMERA
IS
A
SIMPLE
MODEL
TO
APPROXIMATE
IMAGING
PROCESS
PERSPECTIVE
PROJECTION
IF
WE
TREAT
PINHOLE
AS
A
POINT
ONLY
ONE
RAY
FROM
ANY
GIVEN
POINT
CAN
ENTER
THE
CAMERA
CAMERA
OBSCURA
THE
PRE
CAMERA
CAMERA
OBSCURA
DARK
ROOM
FIRST
IDEA
MO
TI
CHINA
TO
FIRST
BUILT
ALHAZEN
IRAQ
EGYPT
TO
ILLUSTRATION
OF
CAMERA
OBSCURA
FREESTANDING
CAMERA
OBSCURA
AT
UNC
CHAPEL
HILL
MODIFIED
FROM
DEREK
HOIEM
PHOTO
BY
SETH
ILYS
CAMERA
OBSCURA
AT
HOME
HOW
CAN
YOU
MAKE
YOUR
OWN
PORTABLE
CAMERA
SKETCH
FROM
CAMERA
OBSCURA
USED
FOR
TRACING
LENS
BASED
CAMERA
OBSCURA
DEREK
HOIEM
CAMERA
OBSCURA
JETTY
AT
MARGATE
ENGLAND
AN
ATTRACTION
IN
THE
LATE
CENTURY
ADAPTED
FROM
R
DURAISWAMI
AROUND
OLDEST
SURVIVING
PHOTOGRAPH
TOOK
HOURS
ON
PEWTER
PLATE
JOSEPH
NIEPCE
PHOTOGRAPH
OF
THE
FIRST
PHOTOGRAPH
STORED
AT
UT
AUSTIN
DEREK
HOIEM
BOULEVARD
DU
TEMPLE
LOUIS
DAGUERRE
DIMENSIONALITY
REDUCTION
MACHINE
TO
WORLD
IMAGE
POINT
OF
OBSERVATION
PROJECTION
CAN
BE
TRICKY
HONDA
COMMERCIAL
FAR
AWAY
OBJECTS
APPEAR
SMALLER
FORSYTH
AND
PONCE
PARALLEL
LINES
IN
THE
SCENE
INTERSECT
IN
THE
IMAGE
CONVERGE
IN
IMAGE
ON
HORIZON
LINE
IMAGE
PLANE
VIRTUAL
PINHOLE
SCENE
MANY
TO
ONE
ANY
POINTS
ALONG
SAME
RAY
MAP
TO
SAME
POINT
IN
IMAGE
POINTS
POINTS
LINES
LINES
COLLINEARITY
PRESERVED
DISTANCES
AND
ANGLES
ARE
NOT
PRESERVED
DEGENERATE
CASES
LINE
THROUGH
FOCAL
POINT
PROJECTS
TO
A
POINT
PLANE
THROUGH
FOCAL
POINT
PROJECTS
TO
LINE
PLANE
PERPENDICULAR
TO
IMAGE
PLANE
PROJECTS
TO
PART
OF
THE
IMAGE
PROJECTION
WORLD
COORDINATES
IMAGE
COORDINATES
P
X
Y
OPTICAL
CENTER
Y
F
Z
X
CAMERA
CENTER
Z
X
X
P
Y
X
Y
SCENE
POINT
IMAGE
COORDINATES
HOMOGENEOUS
COORDINATES
IS
THIS
A
LINEAR
TRANSFORMATION
NO
DIVISION
BY
Z
IS
NONLINEAR
TRICK
ADD
ONE
MORE
COORDINATE
HOMOGENEOUS
IMAGE
COORDINATES
HOMOGENEOUS
SCENE
COORDINATES
CONVERTING
FROM
HOMOGENEOUS
COORDINATES
HOMOGENEOUS
COORDINATES
INVARIANT
TO
SCALING
X
KX
KX
X
K
Y
KY
KW
W
KY
Y
W
KW
KW
W
HOMOGENEOUS
COORDINATES
CARTESIAN
COORDINATES
POINT
IN
CARTESIAN
IS
RAY
IN
HOMOGENEOUS
PERSPECTIVE
PROJECTION
MATRIX
PROJECTION
IS
A
MATRIX
MULTIPLICATION
USING
HOMOGENEOUS
COORDINATES
X
Y
X
X
Y
Y
F
F
F
Z
Z
F
Z
Z
DIVIDE
BY
THE
THIRD
COORDINATE
TO
CONVERT
BACK
TO
NON
HOMOGENEOUS
COORDINATES
WEAK
PERSPECTIVE
ASSUMES
SCENE
DEPTH
AVERAGE
DISTANCE
TO
CAMERA
ADDING
A
LENS
A
LENS
FOCUSES
LIGHT
ONTO
THE
FILM
RAYS
PASSING
THROUGH
THE
CENTER
ARE
NOT
DEVIATED
ALL
PARALLEL
RAYS
CONVERGE
TO
ONE
POINT
ON
A
PLANE
LOCATED
AT
THE
FOCAL
LENGTH
F
SLIDE
BY
STEVE
SEITZ
CAMERAS
WITH
LENSES
F
FOCAL
POINT
OPTICAL
CENTER
CENTER
OF
PROJECTION
A
LENS
FOCUSES
PARALLEL
RAYS
ONTO
A
SINGLE
FOCAL
POINT
GATHER
MORE
LIGHT
WHILE
KEEPING
FOCUS
MAKE
PINHOLE
PERSPECTIVE
PROJECTION
PRACTICAL
TODAY
WRAP
UP
INDEXING
AND
RETRIEVAL
IMAGE
TRANSFORMATIONS
LINEAR
AND
AFFINE
IMAGE
FORMATION
MOSAICS
OBTAIN
A
WIDER
ANGLE
VIEW
BY
COMBINING
MULTIPLE
IMAGES
HOW
TO
STITCH
TOGETHER
A
PANORAMA
A
K
A
MOSAIC
BASIC
PROCEDURE
TAKE
A
SEQUENCE
OF
IMAGES
FROM
THE
SAME
POSITION
ROTATE
THE
CAMERA
ABOUT
ITS
OPTICAL
CENTER
COMPUTE
TRANSFORMATION
BETWEEN
SECOND
IMAGE
AND
FIRST
TRANSFORM
THE
SECOND
IMAGE
TO
OVERLAP
WITH
THE
FIRST
BLEND
THE
TWO
TOGETHER
TO
CREATE
A
MOSAIC
IF
THERE
ARE
MORE
IMAGES
REPEAT
USE
THE
GEOMETRY
OF
THE
SCENE
COMBINE
TWO
OR
MORE
OVERLAPPING
IMAGES
TO
MAKE
ONE
LARGER
IMAGE
CAMERA
CENTER
MOSAICS
GENERATING
SYNTHETIC
VIEWS
REAL
CAMERA
SYNTHETIC
CAMERA
CAN
GENERATE
ANY
SYNTHETIC
CAMERA
VIEW
AS
LONG
AS
IT
HAS
THE
SAME
CENTER
OF
PROJECTION
MOSAIC
PP
THE
MOSAIC
HAS
A
NATURAL
INTERPRETATION
IN
THE
IMAGES
ARE
REPROJECTED
ONTO
A
COMMON
PLANE
THE
MOSAIC
IS
FORMED
ON
THIS
PLANE
MOSAIC
IS
A
SYNTHETIC
WIDE
ANGLE
CAMERA
BASIC
QUESTION
HOW
TO
RELATE
TWO
IMAGES
FROM
THE
SAME
CAMERA
CENTER
HOW
TO
MAP
A
PIXEL
FROM
TO
ANSWER
CAST
A
RAY
THROUGH
EACH
PIXEL
IN
DRAW
THE
PIXEL
WHERE
THAT
RAY
INTERSECTS
OBSERVATION
RATHER
THAN
THINKING
OF
THIS
AS
A
REPROJECTION
THINK
OF
IT
AS
A
IMAGE
WARP
FROM
ONE
IMAGE
TO
ANOTHER
A
PROJECTIVE
TRANSFORM
IS
A
MAPPING
BETWEEN
ANY
TWO
PPS
WITH
THE
SAME
CENTER
OF
PROJECTION
RECTANGLE
SHOULD
MAP
TO
ARBITRARY
QUADRILATERAL
PARALLEL
LINES
AREN
T
BUT
MUST
PRESERVE
STRAIGHT
LINES
CALLED
HOMOGRAPHY
WX
WY
X
Y
W
P
H
P
HOMOGRAPHY
XN
YN
XN
YN
TO
COMPUTE
THE
HOMOGRAPHY
GIVEN
PAIRS
OF
CORRESPONDING
POINTS
IN
THE
IMAGES
WE
NEED
TO
SET
UP
AN
EQUATION
WHERE
THE
PARAMETERS
OF
H
ARE
THE
UNKNOWNS
SOLVING
FOR
HOMOGRAPHIES
P
HP
WX
A
B
C
X
WY
D
E
F
Y
W
G
H
I
CAN
SET
SCALE
FACTOR
I
SO
THERE
ARE
UNKNOWNS
SET
UP
A
SYSTEM
OF
LINEAR
EQUATIONS
AH
B
WHERE
VECTOR
OF
UNKNOWNS
H
A
B
C
D
E
F
G
H
T
NEED
AT
LEAST
EQS
BUT
THE
MORE
THE
BETTER
SOLVE
FOR
H
IF
OVERCONSTRAINED
SOLVE
USING
LEAST
SQUARES
MIN
AH
B
HELP
LMDIVIDE
X
Y
HOMOGRAPHY
WX
W
WY
W
X
Y
TO
APPLY
A
GIVEN
HOMOGRAPHY
H
COMPUTE
P
HP
REGULAR
MATRIX
MULTIPLY
CONVERT
P
FROM
HOMOGENEOUS
TO
IMAGE
WX
WY
W
X
Y
COORDINATES
KRISTEN
GRAUMAN
P
H
P
Y
GIVEN
A
COORDINATE
TRANSFORM
AND
A
SOURCE
IMAGE
F
X
Y
HOW
DO
WE
COMPUTE
A
TRANSFORMED
IMAGE
G
X
Y
F
T
X
Y
Y
SEND
EACH
PIXEL
F
X
Y
TO
ITS
CORRESPONDING
LOCATION
X
Y
T
X
Y
IN
THE
SECOND
IMAGE
Q
WHAT
IF
PIXEL
LANDS
BETWEEN
TWO
PIXELS
Y
X
F
X
Y
X
G
X
Y
SEND
EACH
PIXEL
F
X
Y
TO
ITS
CORRESPONDING
LOCATION
X
Y
T
X
Y
IN
THE
SECOND
IMAGE
Q
WHAT
IF
PIXEL
LANDS
BETWEEN
TWO
PIXELS
A
DISTRIBUTE
COLOR
AMONG
NEIGHBORING
PIXELS
X
Y
KNOWN
AS
SPLATTING
Y
GET
EACH
PIXEL
G
X
Y
FROM
ITS
CORRESPONDING
LOCATION
X
Y
T
X
Y
IN
THE
FIRST
IMAGE
Q
WHAT
IF
PIXEL
COMES
FROM
BETWEEN
TWO
PIXELS
Y
X
F
X
Y
X
G
X
Y
GET
EACH
PIXEL
G
X
Y
FROM
ITS
CORRESPONDING
LOCATION
X
Y
T
X
Y
IN
THE
FIRST
IMAGE
Q
WHAT
IF
PIXEL
COMES
FROM
BETWEEN
TWO
PIXELS
A
INTERPOLATE
COLOR
VALUE
FROM
NEIGHBORS
NEAREST
NEIGHBOR
BILINEAR
HELP
RECAP
HOW
TO
STITCH
TOGETHER
A
PANORAMA
A
K
A
MOSAIC
BASIC
PROCEDURE
TAKE
A
SEQUENCE
OF
IMAGES
FROM
THE
SAME
POSITION
ROTATE
THE
CAMERA
ABOUT
ITS
OPTICAL
CENTER
COMPUTE
TRANSFORMATION
HOMOGRAPHY
BETWEEN
SECOND
IMAGE
AND
FIRST
USING
CORRESPONDING
POINTS
TRANSFORM
THE
SECOND
IMAGE
TO
OVERLAP
WITH
THE
FIRST
BLEND
THE
TWO
TOGETHER
TO
CREATE
A
MOSAIC
IF
THERE
ARE
MORE
IMAGES
REPEAT
HOMOGRAPHY
EXAMPLE
IMAGE
RECTIFICATION
TO
UNWARP
RECTIFY
AN
IMAGE
SOLVE
FOR
HOMOGRAPHY
H
GIVEN
P
AND
P
P
HP
ASSUME
WE
HAVE
FOUR
MATCHED
POINTS
HOW
DO
WE
COMPUTE
HOMOGRAPHY
H
DIRECT
LINEAR
TRANSFORMATION
DLT
W
X
P
W
Y
H
H
H
H
P
HP
A
X
Y
W
XX
YX
X
H
X
Y
XY
YY
Y
H
H
H
APPLY
SVD
UDVT
A
H
H
VSMALLEST
COLUMN
OF
V
CORR
TO
SMALLEST
SINGULAR
VALUE
H
ASSUME
WE
HAVE
MATCHED
POINTS
WITH
OUTLIERS
HOW
DO
WE
COMPUTE
HOMOGRAPHY
H
AUTOMATIC
HOMOGRAPHY
ESTIMATION
WITH
RANSAC
CHOOSE
NUMBER
OF
SAMPLES
N
CHOOSE
RANDOM
POTENTIAL
MATCHES
COMPUTE
H
USING
NORMALIZED
DLT
PROJECT
POINTS
FROM
X
TO
X
FOR
EACH
POTENTIALLY
MATCHING
PAIR
X
I
HXI
COUNT
POINTS
WITH
PROJECTED
DISTANCE
T
E
G
T
PIXELS
REPEAT
STEPS
N
TIMES
CHOOSE
H
WITH
MOST
INLIERS
DEREK
HOIEM
COMPUTE
INTEREST
POINTS
ON
EACH
IMAGE
FIND
CANDIDATE
MATCHES
ESTIMATE
HOMOGRAPHY
H
USING
MATCHED
POINTS
AND
RANSAC
WITH
NORMALIZED
DLT
PROJECT
EACH
IMAGE
ONTO
THE
SAME
SURFACE
AND
BLEND
MATLAB
MAKETFORM
IMTRANSFORM
SUMMARY
WRITE
TRANSFORMATIONS
AS
MATRIX
VECTOR
MULTIPLICATION
INCLUDING
TRANSLATION
WHEN
WE
USE
HOMOGENEOUS
COORDINATES
PROJECTION
EQUATIONS
EXPRESS
HOW
WORLD
POINTS
MAPPED
TO
IMAGE
PERFORM
IMAGE
WARPING
FORWARD
INVERSE
FITTING
TRANSFORMATIONS
SOLVE
FOR
UNKNOWN
PARAMETERS
GIVEN
CORRESPONDING
POINTS
FROM
TWO
VIEWS
AFFINE
PROJECTIVE
HOMOGRAPHY
MOSAICS
USES
HOMOGRAPHY
AND
IMAGE
WARPING
TO
MERGE
VIEWS
TAKEN
FROM
SAME
CENTER
OF
PROJECTION
CS
INTRO
TO
COMPUTER
VISION
EPIPOLAR
GEOMETRY
AND
STEREO
VISION
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
TODAY
REVIEW
PROJECTIVE
TRANSFORMS
IMAGE
STITCHING
HOMOGRAPHY
EPIPOLAR
GEOMETRY
MULTIPLE
VIEWS
FROM
DIFFERENT
CAMERAS
STEREO
VISION
ESTIMATING
DEPTH
FROM
DISPARITIES
EXAM
AND
HOMEWORK
INFO
X
Y
A
C
B
X
D
Y
ONLY
LINEAR
TRANSFORMATIONS
CAN
BE
REPRESENTED
WITH
A
MATRIX
LINEAR
TRANSFORMATIONS
ARE
COMBINATIONS
OF
SCALE
ROTATION
SHEAR
AND
MIRROR
X
A
B
C
X
Y
D
E
F
Y
W
W
AFFINE
TRANSFORMATIONS
ARE
COMBINATIONS
OF
LINEAR
TRANSFORMATIONS
AND
TRANSLATIONS
PARALLEL
LINES
REMAIN
PARALLEL
X
A
B
C
X
Y
W
D
E
G
H
F
Y
I
W
PROJECTIVE
TRANSFORMATIONS
AFFINE
TRANSFORMATIONS
AND
PROJECTIVE
WARPS
PARALLEL
LINES
DO
NOT
NECESSARILY
REMAIN
PARALLEL
M
XI
YI
XI
X
Y
M
Y
I
I
I
HOW
MANY
MATCHES
CORRESPONDENCE
PAIRS
DO
WE
NEED
TO
SOLVE
FOR
THE
TRANSFORMATION
PARAMETERS
ONCE
WE
HAVE
SOLVED
FOR
THE
PARAMETERS
HOW
DO
WE
COMPUTE
X
NEW
Y
NEW
GIVEN
XNEW
YNEW
X
K
R
T
X
X
IMAGE
COORDINATES
U
V
K
INTRINSIC
MATRIX
R
ROTATION
T
TRANSLATION
X
WORLD
COORDINATES
X
Y
Z
EXTRINSIC
PARAMS
R
T
INTRINSIC
PARAMS
K
FOCAL
LENGTH
PIXEL
SIZES
MM
ETC
WE
LL
ASSUME
THAT
THESE
PARAMETERS
ARE
GIVEN
AND
FIXED
INTRINSIC
ASSUMPTIONS
UNIT
ASPECT
RATIO
OPTICAL
CENTER
AT
NO
SKEW
EXTRINSIC
ASSUMPTIONS
NO
ROTATION
CAMERA
AT
K
U
F
X
Y
X
K
I
X
W
V
F
Z
U
F
U
R
R
R
T
X
X
Y
X
K
R
T
X
W
V
TY
R
R
R
T
Z
Z
X
K
R
T
X
WU
X
Y
WV
Z
W
OBTAIN
A
WIDER
ANGLE
VIEW
BY
COMBINING
MULTIPLE
IMAGES
TWO
IMAGES
WITH
ROTATION
ZOOM
BUT
NO
TRANSLATION
DEREK
HOIEM
CAMERA
CENTER
HOW
TO
STITCH
TOGETHER
A
PANORAMA
A
K
A
MOSAIC
BASIC
PROCEDURE
TAKE
A
SEQUENCE
OF
IMAGES
FROM
THE
SAME
POSITION
ROTATE
THE
CAMERA
ABOUT
ITS
OPTICAL
CENTER
COMPUTE
THE
HOMOGRAPHY
TRANSFORMATION
BETWEEN
SECOND
IMAGE
AND
FIRST
TRANSFORM
THE
SECOND
IMAGE
TO
OVERLAP
WITH
THE
FIRST
BLEND
THE
TWO
TOGETHER
TO
CREATE
A
MOSAIC
IF
THERE
ARE
MORE
IMAGES
REPEAT
MODIFIED
FROM
STEVE
SEITZ
MOSAIC
PLANE
THE
MOSAIC
HAS
A
NATURAL
INTERPRETATION
IN
THE
IMAGES
ARE
REPROJECTED
ONTO
A
COMMON
PLANE
THE
MOSAIC
IS
FORMED
ON
THIS
PLANE
MOSAIC
IS
A
SYNTHETIC
WIDE
ANGLE
CAMERA
A
PROJECTIVE
TRANSFORM
IS
A
MAPPING
BETWEEN
ANY
TWO
PPS
WITH
THE
SAME
CENTER
OF
PROJECTION
RECTANGLE
SHOULD
MAP
TO
ARBITRARY
QUADRILATERAL
PARALLEL
LINES
AREN
T
BUT
MUST
PRESERVE
STRAIGHT
LINES
CALLED
HOMOGRAPHY
WX
WY
X
Y
W
P
H
P
XN
YN
XN
YN
TO
COMPUTE
THE
HOMOGRAPHY
GIVEN
PAIRS
OF
CORRESPONDING
POINTS
IN
THE
IMAGES
WE
NEED
TO
SET
UP
AN
EQUATION
WHERE
THE
PARAMETERS
OF
H
ARE
THE
UNKNOWNS
P
HP
WX
A
B
C
X
WY
D
E
F
Y
W
G
H
I
CAN
SET
SCALE
FACTOR
I
SO
THERE
ARE
UNKNOWNS
SET
UP
A
SYSTEM
OF
LINEAR
EQUATIONS
AH
B
WHERE
VECTOR
OF
UNKNOWNS
H
A
B
C
D
E
F
G
H
T
NEED
AT
LEAST
EQS
BUT
THE
MORE
THE
BETTER
SOLVE
FOR
H
IF
OVERCONSTRAINED
SOLVE
USING
LEAST
SQUARES
MIN
AH
B
HELP
LMDIVIDE
HOW
TO
STITCH
TOGETHER
A
PANORAMA
A
K
A
MOSAIC
BASIC
PROCEDURE
TAKE
A
SEQUENCE
OF
IMAGES
FROM
THE
SAME
POSITION
ROTATE
THE
CAMERA
ABOUT
ITS
OPTICAL
CENTER
COMPUTE
THE
HOMOGRAPHY
TRANSFORMATION
BETWEEN
SECOND
IMAGE
AND
FIRST
TRANSFORM
THE
SECOND
IMAGE
TO
OVERLAP
WITH
THE
FIRST
BLEND
THE
TWO
TOGETHER
TO
CREATE
A
MOSAIC
IF
THERE
ARE
MORE
IMAGES
REPEAT
X
Y
IMAGE
IMAGE
CANVAS
WX
W
WY
W
X
Y
TO
APPLY
A
GIVEN
HOMOGRAPHY
H
COMPUTE
P
HP
REGULAR
MATRIX
MULTIPLY
WX
WY
X
Y
CONVERT
P
FROM
HOMOGENEOUS
TO
IMAGE
COORDINATES
W
MODIFIED
FROM
KRISTEN
GRAUMAN
P
H
P
IMAGE
IMAGE
CANVAS
Y
FORWARD
WARPING
SEND
EACH
PIXEL
F
X
Y
TO
ITS
CORRESPONDING
LOCATION
X
Y
H
X
Y
IN
THE
RIGHT
IMAGE
IMAGE
IMAGE
CANVAS
Y
INVERSE
WARPING
GET
EACH
PIXEL
G
X
Y
FROM
ITS
CORRESPONDING
LOCATION
X
Y
H
X
Y
IN
THE
LEFT
IMAGE
Q
WHAT
IF
PIXEL
COMES
FROM
BETWEEN
TWO
PIXELS
A
INTERPOLATE
COLOR
VALUE
FROM
NEIGHBORS
RANSAC
FOR
HOMOGRAPHY
TODAY
REVIEW
PROJECTIVE
TRANSFORMS
IMAGE
STITCHING
HOMOGRAPHY
STEREO
VISION
ESTIMATING
DEPTH
FROM
DISPARITIES
EXAM
AND
HOMEWORK
INFO
LAST
CLASS
VS
THIS
CLASS
LAST
CLASS
SAME
CAMERA
CENTER
BUT
CAMERA
ROTATES
THIS
CLASS
CAMERA
CENTER
IS
NOT
THE
SAME
WE
HAVE
MULTIPLE
CAMERAS
EPIPOLAR
GEOMETRY
RELATES
CAMERAS
FROM
TWO
POSITIONS
STEREO
DEPTH
ESTIMATION
RECOVER
DEPTH
FROM
TWO
IMAGES
ADAPTED
FROM
DEREK
HOIEM
WHY
MULTIPLE
VIEWS
STRUCTURE
AND
DEPTH
ARE
INHERENTLY
AMBIGUOUS
FROM
SINGLE
VIEWS
MULTIPLE
VIEWS
HELP
US
TO
PERCEIVE
SHAPE
AND
DEPTH
TAKE
TWO
PICTURES
OF
THE
SAME
SUBJECT
FROM
TWO
SLIGHTLY
DIFFERENT
VIEWPOINTS
AND
DISPLAY
SO
THAT
EACH
EYE
SEES
ONLY
ONE
OF
THE
IMAGES
INVENTED
BY
SIR
CHARLES
WHEATSTONE
IMAGE
FROM
FISHER
PRICE
COM
STEREO
VISION
TWO
CAMERAS
SIMULTANEOUS
VIEWS
SINGLE
MOVING
CAMERA
AND
STATIC
SCENE
GOAL
RECOVER
DEPTH
BY
FINDING
IMAGE
COORDINATE
X
THAT
CORRESPONDS
TO
X
X
X
Z
C
BASELINE
C
B
GOAL
RECOVER
DEPTH
BY
FINDING
IMAGE
COORDINATE
X
THAT
CORRESPONDS
TO
X
SUB
PROBLEMS
CALIBRATION
HOW
DO
WE
RECOVER
THE
RELATION
OF
THE
CAMERAS
IF
NOT
ALREADY
KNOWN
CORRESPONDENCE
HOW
DO
WE
SEARCH
FOR
THE
MATCHING
POINT
X
X
GEOMETRY
FOR
A
SIMPLE
STEREO
SYSTEM
ASSUME
PARALLEL
OPTICAL
AXES
KNOWN
CAMERA
PARAMETERS
I
E
CALIBRATED
CAMERAS
WHAT
IS
EXPRESSION
FOR
Z
SIMILAR
TRIANGLES
PL
P
PR
AND
OL
P
OR
T
XL
XR
T
Z
F
Z
DEPTH
IS
INVERSELY
PROPORTIONAL
TO
DISPARITY
DEPTH
DISPARITY
Z
F
T
XR
XL
DEPTH
FROM
DISPARITY
WE
HAVE
TWO
IMAGES
TAKEN
FROM
CAMERAS
WITH
DIFFERENT
INTRINSIC
AND
EXTRINSIC
PARAMETERS
HOW
DO
WE
MATCH
A
POINT
IN
THE
FIRST
IMAGE
TO
A
POINT
IN
THE
SECOND
IMAGE
I
X
Y
DISPARITY
MAP
D
X
Y
IMAGE
I
X
Y
SO
IF
WE
COULD
FIND
THE
CORRESPONDING
POINTS
IN
TWO
IMAGES
WE
COULD
ESTIMATE
RELATIVE
DEPTH
GIVEN
P
IN
LEFT
IMAGE
WHERE
CAN
CORRESPONDING
POINT
P
BE
GEOMETRY
OF
TWO
VIEWS
CONSTRAINS
WHERE
THE
CORRESPONDING
PIXEL
FOR
SOME
IMAGE
POINT
IN
THE
FIRST
VIEW
MUST
OCCUR
IN
THE
SECOND
VIEW
IT
MUST
BE
ON
THE
LINE
CARVED
OUT
BY
A
PLANE
CONNECTING
THE
WORLD
POINT
AND
OPTICAL
CENTERS
POTENTIAL
MATCHES
FOR
P
HAVE
TO
LIE
ON
THE
CORRESPONDING
LINE
L
POTENTIAL
MATCHES
FOR
P
HAVE
TO
LIE
ON
THE
CORRESPONDING
LINE
L
EPIPOLAR
GEOMETRY
NOTATION
DEREK
HOIEM
BASELINE
LINE
CONNECTING
THE
TWO
CAMERA
CENTERS
EPIPOLES
INTERSECTIONS
OF
BASELINE
WITH
IMAGE
PLANES
PROJECTIONS
OF
THE
OTHER
CAMERA
CENTER
EPIPOLAR
PLANE
PLANE
CONTAINING
BASELINE
EPIPOLAR
LINES
INTERSECTIONS
OF
EPIPOLAR
PLANE
WITH
IMAGE
PLANES
ALWAYS
COME
IN
CORRESPONDING
PAIRS
NOTE
ALL
EPIPOLAR
LINES
INTERSECT
AT
THE
EPIPOLE
EPIPOLAR
CONSTRAINT
THIS
IS
USEFUL
BECAUSE
IT
REDUCES
THE
CORRESPONDENCE
PROBLEM
TO
A
SEARCH
ALONG
AN
EPIPOLAR
LINE
KRISTEN
GRAUMAN
IMAGE
FROM
ANDREW
ZISSERMAN
IF
THE
STEREO
RIG
IS
CALIBRATED
WE
KNOW
HOW
TO
ROTATE
AND
TRANSLATE
CAMERA
REFERENCE
FRAME
TO
GET
TO
CAMERA
REFERENCE
FRAME
ROTATION
MATRIX
R
TRANSLATION
VECTOR
T
IF
THE
STEREO
RIG
IS
CALIBRATED
WE
KNOW
HOW
TO
ROTATE
AND
TRANSLATE
CAMERA
REFERENCE
FRAME
TO
GET
TO
CAMERA
REFERENCE
FRAME
X
C
RXC
T
AN
ASIDE
CROSS
PRODUCT
VECTOR
CROSS
PRODUCT
TAKES
TWO
VECTORS
AND
RETURNS
A
THIRD
VECTOR
THAT
PERPENDICULAR
TO
BOTH
INPUTS
SO
HERE
C
IS
PERPENDICULAR
TO
BOTH
A
AND
B
WHICH
MEANS
THE
DOT
PRODUCT
FROM
GEOMETRY
TO
ALGEBRA
X
RX
T
X
T
X
X
T
RX
T
X
NORMAL
TO
THE
PLANE
T
RX
ANOTHER
ASIDE
MATRIX
FORM
OF
CROSS
PRODUCT
A
A
B
A
A
B
C
CAN
BE
EXPRESSED
AS
A
MATRIX
MULTIPLICATION
A
A
A
X
FROM
GEOMETRY
TO
ALGEBRA
X
RX
T
X
T
X
X
T
RX
T
X
T
RX
T
T
NORMAL
TO
THE
PLANE
T
RX
ESSENTIAL
MATRIX
X
TX
RX
LET
E
T
X
R
X
T
EX
E
IS
CALLED
THE
ESSENTIAL
MATRIX
AND
IT
RELATES
CORRESPONDING
IMAGE
POINTS
BETWEEN
BOTH
CAMERAS
GIVEN
THE
ROTATION
AND
TRANSLATION
IF
WE
OBSERVE
A
POINT
IN
ONE
IMAGE
ITS
POSITION
IN
OTHER
IMAGE
IS
CONSTRAINED
TO
LIE
ON
LINE
DEFINED
BY
ABOVE
EX
IS
THE
EPIPOLAR
LINE
THROUGH
X
IN
THE
FIRST
IMAGE
CORRESPONDING
TO
X
NOTE
THESE
POINTS
ARE
IN
CAMERA
COORDINATE
SYSTEMS
ESSENTIAL
MATRIX
EXAMPLE
PARALLEL
CAMERAS
R
P
X
Y
F
T
E
T
X
R
P
X
Y
F
P
EP
FOR
THE
PARALLEL
CAMERAS
IMAGE
OF
ANY
POINT
MUST
LIE
ON
SAME
HORIZONTAL
LINE
IN
EACH
IMAGE
PLANE
IMAGE
I
X
Y
DISPARITY
MAP
D
X
Y
X
Y
X
D
X
Y
Y
IMAGE
I
X
Y
WHAT
ABOUT
WHEN
CAMERAS
OPTICAL
AXES
ARE
NOT
PARALLEL
REPROJECT
IMAGE
PLANES
ONTO
A
COMMON
PLANE
PARALLEL
TO
THE
LINE
BETWEEN
CAMERA
CENTERS
PIXEL
MOTION
IS
HORIZONTAL
AFTER
THIS
TRANSFORMATION
TWO
HOMOGRAPHIES
TRANSFORM
ONE
FOR
EACH
INPUT
IMAGE
REPROJECTION
AND
PATTERN
RECOGNITION
STEREO
IMAGE
RECTIFICATION
EXAMPLE
WHAT
IF
WE
DON
T
KNOW
THE
CAMERA
PARAMETERS
WANT
TO
ESTIMATE
WORLD
GEOMETRY
WITHOUT
REQUIRING
CALIBRATED
CAMERAS
ARCHIVAL
VIDEOS
PHOTOS
FROM
MULTIPLE
UNRELATED
USERS
WEAK
CALIBRATION
ESTIMATE
EPIPOLAR
GEOMETRY
FROM
A
REDUNDANT
SET
OF
POINT
CORRESPONDENCES
BETWEEN
TWO
UNCALIBRATED
CAMERAS
COMPUTING
F
FROM
CORRESPONDENCES
EACH
POINT
CORRESPONDENCE
GENERATES
ONE
CONSTRAINT
ON
F
COLLECT
N
OF
CONSTRAINTS
SOLVE
FOR
F
IM
RIGHT
IM
LEFT
FUNDAMENTAL
MATRIX
RELATES
PIXEL
COORDINATES
IN
THE
TWO
VIEWS
MORE
GENERAL
FORM
THAN
ESSENTIAL
MATRIX
WE
REMOVE
NEED
TO
KNOW
INTRINSIC
PARAMETERS
PROPERTIES
OF
THE
FUNDAMENTAL
MATRIX
XT
F
X
WITH
F
K
T
EK
F
X
IS
THE
EPIPOLAR
LINE
ASSOCIATED
WITH
X
L
F
X
FTX
IS
THE
EPIPOLAR
LINE
ASSOCIATED
WITH
X
L
FTX
F
E
AND
FTE
F
IS
SINGULAR
RANK
TWO
DET
F
F
HAS
SEVEN
DEGREES
OF
FREEDOM
ENTRIES
BUT
DEFINED
UP
TO
SCALE
DET
F
LET
RECAP
TODAY
REVIEW
PROJECTIVE
TRANSFORMS
IMAGE
STITCHING
HOMOGRAPHY
EPIPOLAR
GEOMETRY
MULTIPLE
VIEWS
FROM
DIFFERENT
CAMERAS
EXAM
AND
HOMEWORK
INFO
MOVING
ON
TO
STEREO
FUSE
A
CALIBRATED
BINOCULAR
STEREO
PAIR
TO
PRODUCE
A
DEPTH
IMAGE
IMAGE
IMAGE
DENSE
DEPTH
MAP
BASIC
STEREO
MATCHING
ALGORITHM
FOR
EACH
PIXEL
IN
THE
FIRST
IMAGE
FIND
CORRESPONDING
EPIPOLAR
SCANLINE
IN
THE
RIGHT
IMAGE
IF
NECESSARY
RECTIFY
THE
TWO
STEREO
IMAGES
TO
TRANSFORM
EPIPOLAR
LINES
INTO
SCANLINES
SEARCH
ALONG
EPIPOLAR
LINE
AND
PICK
THE
BEST
MATCH
X
COMPUTE
DISPARITY
X
X
AND
SET
DEPTH
X
F
T
X
X
CORRESPONDENCE
SEARCH
LEFT
RIGHT
SCANLINE
MATCHING
COST
DISPARITY
SLIDE
A
WINDOW
ALONG
THE
RIGHT
SCANLINE
AND
COMPARE
CONTENTS
OF
THAT
WINDOW
WITH
THE
REFERENCE
WINDOW
IN
THE
LEFT
IMAGE
MATCHING
COST
SSD
OR
NORMALIZED
CORRELATION
GEOMETRY
FOR
A
SIMPLE
STEREO
SYSTEM
ASSUME
PARALLEL
OPTICAL
AXES
KNOWN
CAMERA
PARAMETERS
I
E
CALIBRATED
CAMERAS
WHAT
IS
EXPRESSION
FOR
Z
SIMILAR
TRIANGLES
PL
P
PR
AND
OL
P
OR
T
XL
XR
T
Z
F
Z
DEPTH
DISPARITY
Z
F
T
XR
XL
KRISTEN
GRAUMAN
RESULTS
WITH
WINDOW
SEARCH
DATA
WINDOW
BASED
MATCHING
GROUND
TRUTH
HOW
CAN
WE
IMPROVE
UNIQUENESS
FOR
ANY
POINT
IN
ONE
IMAGE
THERE
SHOULD
BE
AT
MOST
ONE
MATCHING
POINT
IN
THE
OTHER
IMAGE
ORDERING
CORRESPONDING
POINTS
SHOULD
BE
IN
THE
SAME
ORDER
IN
BOTH
VIEWS
SMOOTHNESS
WE
EXPECT
DISPARITY
VALUES
TO
CHANGE
SLOWLY
FOR
THE
MOST
PART
MANY
OF
THESE
CONSTRAINTS
CAN
BE
ENCODED
IN
AN
ENERGY
FUNCTION
AND
SOLVED
USING
GRAPH
CUTS
BEFORE
GRAPH
CUTS
GROUND
TRUTH
FOR
THE
LATEST
AND
GREATEST
PROJECTIVE
STRUCTURE
FROM
MOTION
GIVEN
M
IMAGES
OF
N
FIXED
POINTS
XIJ
PI
XJ
I
M
J
N
PROBLEM
ESTIMATE
M
PROJECTION
MATRICES
PI
AND
N
POINTS
XJ
FROM
THE
MN
CORRESPONDING
POINTS
XIJ
XJ
PHOTO
SYNTH
BUILDING
ROME
IN
A
DAY
AGARWAL
ET
AL
POINT
X
IN
LEFT
IMAGE
CORRESPONDS
TO
EPIPOLAR
LINE
L
IN
RIGHT
IMAGE
EPIPOLAR
LINE
PASSES
THROUGH
THE
EPIPOLE
THE
INTERSECTION
OF
THE
CAMERAS
BASELINE
WITH
THE
IMAGE
PLANE
FUNDAMENTAL
MATRIX
MAPS
FROM
A
POINT
IN
ONE
IMAGE
TO
A
LINE
IN
THE
OTHER
IF
X
AND
X
CORRESPOND
TO
THE
SAME
POINT
X
RECAP
STEREO
WITH
CALIBRATED
CAMERAS
GIVEN
IMAGE
PAIR
R
T
DETECT
SOME
FEATURES
COMPUTE
ESSENTIAL
MATRIX
E
MATCH
FEATURES
USING
THE
EPIPOLAR
AND
OTHER
CONSTRAINTS
TRIANGULATE
FOR
STRUCTURE
AND
GET
DEPTH
KRISTEN
GRAUMAN
SUMMARY
EPIPOLAR
GEOMETRY
EPIPOLES
ARE
INTERSECTION
OF
BASELINE
WITH
IMAGE
PLANES
MATCHING
POINT
IN
SECOND
IMAGE
IS
ON
A
LINE
PASSING
THROUGH
ITS
EPIPOLE
EPIPOLAR
CONSTRAINT
LIMITS
WHERE
POINTS
FROM
ONE
VIEW
WILL
BE
IMAGED
IN
THE
OTHER
WHICH
MAKES
SEARCH
FOR
CORRESPONDENCES
QUICKER
FUNDAMENTAL
MATRIX
MAPS
FROM
A
POINT
IN
ONE
IMAGE
TO
A
LINE
ITS
EPIPOLAR
LINE
IN
THE
OTHER
CAN
SOLVE
FOR
F
GIVEN
CORRESPONDING
POINTS
E
G
INTEREST
POINTS
STEREO
DEPTH
ESTIMATION
FIND
CORRESPONDING
POINTS
ALONG
EPIPOLAR
SCANLINE
ESTIMATE
DISPARITY
DEPTH
IS
INVERSE
TO
DISPARITY
MODIFIED
FROM
KRISTEN
GRAUMAN
AND
DEREK
HOIEM
TODAY
REVIEW
PROJECTIVE
TRANSFORMS
IMAGE
STITCHING
HOMOGRAPHY
EPIPOLAR
GEOMETRY
MULTIPLE
VIEWS
FROM
DIFFERENT
CAMERAS
STEREO
VISION
ESTIMATING
DEPTH
FROM
DISPARITIES
NEXT
THURSDAY
MIDTERM
EXAM
IN
CLASS
REVIEW
ON
TUESDAY
EMAIL
ME
WITH
TOPICS
YOU
WANT
ME
TO
REVIEW
OR
WITH
QUESTIONS
FORMAT
MOSTLY
SHORT
ANSWER
QUESTIONS
FROM
EASIER
SHORTER
TO
LONGER
HARDER
SOME
EXERCISES
TO
SHOW
YOU
CAN
APPLY
SOME
OF
THE
CLUSTERING
AND
MATCHING
ALGORITHMS
WE
DISCUSSED
HOMEWORK
GRADES
DUE
TONIGHT
REVIEW
LATE
POLICY
BEYOND
FREE
LATE
DAYS
TOTAL
FOR
THE
CLASS
MINUTE
LATE
LATE
DAY
PENALTY
NOTES
ON
PART
III
A
THE
X
Y
SCORES
YOU
OUTPUT
SHOULD
CORRESPOND
TO
THE
FINAL
SET
OF
KEYPOINTS
AFTER
NON
MAX
SUPPRESSION
IF
YOU
RE
GETTING
A
NEGATIVE
MEAN
R
YOU
CAN
IGNORE
THE
THRESHOLD
AND
OUTPUT
THE
TOP
N
KEYPOINTS
E
G
TOP
MATLAB
TIPS
RELEASED
DUE
OCTOBER
PART
I
HOUGH
TRANSFORM
FOR
CIRCLES
PART
II
VIDEO
GOOGLE
SYSTEM
DATA
AND
STARTER
CODE
PROVIDED
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
IMAGE
FORMATION
DIGITAL
IMAGES
DIGITAL
IMAGES
SAMPLE
THE
SPACE
ON
A
REGULAR
GRID
QUANTIZE
EACH
SAMPLE
ROUND
TO
NEAREST
INTEGER
IMAGE
THUS
REPRESENTED
AS
A
MATRIX
OF
INTEGER
VALUES
DIGITAL
COLOR
IMAGES
DIGITAL
COLOR
IMAGES
COLOR
IMAGES
RGB
COLOR
SPACE
R
G
B
SLIDE
CREDIT
KRISTEN
GRAUMAN
IMAGES
IN
MATLAB
IMAGES
REPRESENTED
AS
A
MATRIX
SUPPOSE
WE
HAVE
A
NXM
RGB
IMAGE
CALLED
IM
IM
TOP
LEFT
PIXEL
VALUE
IN
R
CHANNEL
IM
Y
X
B
Y
PIXELS
DOWN
X
PIXELS
TO
RIGHT
IN
THE
BTH
CHANNEL
IM
N
M
BOTTOM
RIGHT
PIXEL
IN
B
CHANNEL
IMREAD
FILENAME
RETURNS
A
IMAGE
VALUES
TO
CONVERT
TO
DOUBLE
FORMAT
VALUES
TO
WITH
ROW
COLUMN
R
SLIDE
CREDIT
DEREK
HOIEM
SALT
AND
PEPPER
NOISE
RANDOM
OCCURRENCES
OF
BLACK
AND
WHITE
PIXELS
IMPULSE
NOISE
RANDOM
OCCURRENCES
OF
WHITE
PIXELS
GAUSSIAN
NOISE
VARIATIONS
IN
INTENSITY
DRAWN
FROM
A
GAUSSIAN
NORMAL
DISTRIBUTION
SOURCE
SEITZ
LET
REPLACE
EACH
PIXEL
WITH
AN
AVERAGE
OF
ALL
THE
VALUES
IN
ITS
NEIGHBORHOOD
MOVING
AVERAGE
IN
CAN
ADD
WEIGHTS
TO
OUR
MOVING
AVERAGE
WEIGHTS
NON
UNIFORM
WEIGHTS
DEPICTS
BOX
FILTER
WHITE
HIGH
VALUE
BLACK
LOW
VALUE
ORIGINAL
FILTERED
WHAT
IF
THE
FILTER
SIZE
WAS
X
INSTEAD
OF
X
SLIDE
CREDIT
KRISTEN
GRAUMAN
GAUSSIAN
FILTER
WHAT
IF
WE
WANT
NEAREST
NEIGHBORING
PIXELS
TO
HAVE
THE
MOST
INFLUENCE
ON
THE
OUTPUT
REMOVES
HIGH
FREQUENCY
COMPONENTS
FROM
THE
IMAGE
LOW
PASS
FILTER
SOURCE
SEITZ
SMOOTHING
WITH
A
GAUSSIAN
SIZE
OF
KERNEL
OR
MASK
NOTE
GAUSSIAN
FUNCTION
HAS
INFINITE
SUPPORT
BUT
DISCRETE
FILTERS
USE
FINITE
KERNELS
Σ
WITH
X
KERNEL
Σ
WITH
X
KERNEL
VARIANCE
OF
GAUSSIAN
DETERMINES
EXTENT
OF
SMOOTHING
Σ
WITH
X
KERNEL
Σ
WITH
X
KERNEL
PRACTICAL
MATTERS
HOW
BIG
SHOULD
THE
FILTER
BE
VALUES
AT
EDGES
SHOULD
BE
NEAR
ZERO
IMPORTANT
RULE
OF
THUMB
FOR
GAUSSIAN
SET
FILTER
HALF
WIDTH
TO
ABOUT
Σ
SOURCE
DEREK
HOIEM
SMOOTHING
WITH
A
GAUSSIAN
PARAMETER
Σ
IS
THE
SCALE
WIDTH
SPREAD
OF
THE
GAUSSIAN
KERNEL
AND
CONTROLS
THE
AMOUNT
OF
SMOOTHING
FOR
SIGMA
H
FSPECIAL
GAUSSIAN
FSIZE
SIGMA
OUT
IMFILTER
IM
H
IMSHOW
OUT
PAUSE
END
SLIDE
CREDIT
KRISTEN
GRAUMAN
PREDICT
THE
OUTPUTS
USING
CORRELATION
FILTERING
SLIDE
CREDIT
KRISTEN
GRAUMA
ORIGINAL
ORIGINAL
FILTERED
NO
CHANGE
ORIGINAL
ORIGINAL
SHIFTED
LEFT
BY
PIXEL
WITH
CORRELATION
ORIGINAL
ORIGINAL
BLUR
WITH
A
BOX
FILTER
ORIGINAL
ORIGINAL
SHARPENING
FILTER
ACCENTUATES
DIFFERENCES
WITH
LOCAL
AVERAGE
SEPARABILITY
EXAMPLE
F
G
H
F
G
H
FILTERING
CENTER
LOCATION
ONLY
THE
FILTER
FACTORS
INTO
A
PRODUCT
OF
FILTERS
PERFORM
FILTERING
ALONG
ROWS
FOLLOWED
BY
FILTERING
ALONG
THE
REMAINING
COLUMN
NO
NEW
PIXEL
VALUES
INTRODUCED
REMOVES
SPIKES
GOOD
FOR
IMPULSE
SALT
PEPPER
NOISE
NON
LINEAR
FILTER
MEDIAN
FILTER
IS
EDGE
PRESERVING
WHAT
DEFINES
A
TEXTURE
ORIGINAL
IMAGE
ORIGINAL
IMAGE
ORIGINAL
IMAGE
ORIGINAL
IMAGE
DIMENSION
MEAN
D
DX
VALUE
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
WINDOWS
WITH
PRIMARILY
HORIZONTAL
EDGES
BOTH
WINDOWS
WITH
SMALL
GRADIENT
IN
BOTH
DIRECTIONS
WINDOWS
WITH
PRIMARILY
VERTICAL
EDGES
STATISTICS
TO
SUMMARIZE
PATTERNS
IN
SMALL
ORIGINAL
IMAGE
KRISTEN
GRAUMAN
DERIVATIVE
FILTER
RESPONSES
SQUARED
D
A
B
DIMENSION
KRISTEN
GRAUMAN
FILTER
BANKS
SCALES
WHAT
FILTERS
TO
PUT
IN
THE
BANK
TYPICALLY
WE
WANT
A
COMBINATION
OF
SCALES
AND
ORIENTATIONS
DIFFERENT
TYPES
OF
PATTERNS
MATLAB
CODE
AVAILABLE
FOR
THESE
EXAMPLES
SLIDE
CREDIT
KRISTEN
GRAUMAN
FILTER
BANK
D
D
D
D
D
EJ
I
L
L
L
J
LJ
J
SLIDE
CREDIT
KRISTEN
GRAUMAN
250
250
250
WE
CAN
FORM
A
FEATURE
VECTOR
FROM
THE
LIST
OF
RESPONSES
AT
EACH
PIXEL
CLASSIFYING
MATERIALS
STUFF
FIGURE
BY
VARMA
ZISSERMAN
TEXTURE
SYNTHESIS
GOAL
CREATE
NEW
SAMPLES
OF
A
GIVEN
TEXTURE
MANY
APPLICATIONS
VIRTUAL
ENVIRONMENTS
HOLE
FILLING
TEXTURING
SURFACES
SLIDE
CREDIT
KRISTEN
GRAUMAN
MARKOV
RANDOM
FIELD
A
MARKOV
RANDOM
FIELD
MRF
GENERALIZATION
OF
MARKOV
CHAINS
TO
TWO
OR
MORE
DIMENSIONS
FIRST
ORDER
MRF
PROBABILITY
THAT
PIXEL
X
TAKES
A
CERTAIN
VALUE
GIVEN
THE
VALUES
OF
NEIGHBORS
A
B
C
AND
D
SOURCE
SEITZ
TEXTURE
SYNTHESIS
INTUITION
BEFORE
WE
INSERTED
THE
NEXT
WORD
BASED
ON
EXISTING
NEARBY
WORDS
NOW
WE
WANT
TO
INSERT
PIXEL
INTENSITIES
BASED
ON
EXISTING
NEARBY
PIXEL
VALUES
CORPUS
PLACE
WE
WANT
TO
INSERT
NEXT
DISTRIBUTION
OF
A
VALUE
OF
A
PIXEL
IS
CONDITIONED
ON
ITS
NEIGHBORS
ALONE
SLIDE
CREDIT
KRISTEN
GRAUMAN
SYNTHESIZING
ONE
PIXEL
P
INPUT
IMAGE
SYNTHESIZED
IMAGE
WHAT
IS
FIND
ALL
THE
WINDOWS
IN
THE
IMAGE
THAT
MATCH
THE
NEIGHBORHOOD
TO
SYNTHESIZE
X
PICK
ONE
MATCHING
WINDOW
AT
RANDOM
ASSIGN
X
TO
BE
THE
CENTER
PIXEL
OF
THAT
WINDOW
AN
EXACT
NEIGHBOURHOOD
MATCH
MIGHT
NOT
BE
PRESENT
SO
FIND
THE
BEST
MATCHES
USING
SSD
ERROR
AND
RANDOMLY
CHOOSE
BETWEEN
THEM
PREFERRING
BETTER
MATCHES
WITH
HIGHER
PROBABILITY
VARYING
WINDOW
SIZE
INCREASING
WINDOW
SIZE
THROW
AWAY
EVERY
OTHER
ROW
AND
COLUMN
TO
CREATE
A
SIZE
IMAGE
SLIDE
CREDIT
DEREK
HOIEM
EXAMPLE
SINEWAVE
EXAMPLE
SINEWAVE
WHEN
SAMPLING
A
SIGNAL
AT
DISCRETE
INTERVALS
THE
SAMPLING
FREQUENCY
MUST
BE
FMAX
FMAX
MAX
FREQUENCY
OF
THE
INPUT
SIGNAL
THIS
WILL
ALLOWS
TO
RECONSTRUCT
THE
ORIGINAL
PERFECTLY
FROM
THE
SAMPLED
VERSION
GOOD
BAD
GOAL
FIND
IN
IMAGE
MAIN
CHALLENGE
WHAT
IS
A
GOOD
SIMILARITY
OR
DISTANCE
MEASURE
BETWEEN
TWO
PATCHES
CORRELATION
ZERO
MEAN
CORRELATION
SUM
SQUARE
DIFFERENCE
NORMALIZED
CROSS
CORRELATION
GOAL
FIND
IN
IMAGE
METHOD
FILTER
THE
IMAGE
WITH
EYE
PATCH
H
M
N
G
K
L
K
L
F
M
K
N
L
F
IMAGE
G
FILTER
INPUT
FILTERED
IMAGE
WHAT
WENT
WRONG
GOAL
FIND
IN
IMAGE
METHOD
SSD
H
M
N
G
K
L
K
L
F
M
K
N
L
INPUT
SQRT
SSD
THRESHOLDED
IMAGE
GOAL
FIND
IN
IMAGE
METHOD
NORMALIZED
CROSS
CORRELATION
MEAN
TEMPLATE
MEAN
IMAGE
PATCH
H
M
N
G
K
L
K
L
G
F
M
K
N
L
FM
N
G
K
L
G
F
M
K
N
L
M
N
K
L
K
L
MATLAB
TEMPLATE
IM
GOAL
FIND
IN
IMAGE
METHOD
NORMALIZED
CROSS
CORRELATION
INPUT
NORMALIZED
X
CORRELATION
THRESHOLDED
IMAGE
GAUSSIAN
PYRAMID
LAPLACIAN
FILTER
UNIT
IMPULSE
GAUSSIAN
LAPLACIAN
OF
GAUSSIAN
LAPLACIAN
PYRAMID
WHAT
CAUSES
AN
EDGE
REFLECTANCE
CHANGE
APPEARANCE
INFORMATION
TEXTURE
DEPTH
DISCONTINUITY
OBJECT
BOUNDARY
CHANGE
IN
SURFACE
ORIENTATION
SHAPE
CAST
SHADOWS
EDGES
GRADIENTS
AND
INVARIANCE
CHARACTERIZING
EDGES
AN
EDGE
IS
A
PLACE
OF
RAPID
CHANGE
IN
THE
IMAGE
INTENSITY
FUNCTION
IMAGE
INTENSITY
FUNCTION
ALONG
HORIZONTAL
SCANLINE
FIRST
DERIVATIVE
EDGES
CORRESPOND
TO
EXTREMA
OF
DERIVATIVE
INTENSITY
PROFILE
WITH
A
LITTLE
GAUSSIAN
NOISE
GRADIENT
SOLUTION
SMOOTH
FIRST
F
G
F
G
D
F
DX
G
TO
FIND
EDGES
LOOK
FOR
PEAKS
IN
D
F
DX
G
DERIVATIVE
THEOREM
OF
CONVOLUTION
DIFFERENTIATION
IS
CONVOLUTION
AND
CONVOLUTION
IS
ASSOCIATIVE
D
F
DX
G
F
D
G
DX
THIS
SAVES
US
ONE
OPERATION
F
D
G
DX
F
D
G
DX
GRADIENTS
EDGES
PRIMARY
EDGE
DETECTION
STEPS
SMOOTHING
SUPPRESS
NOISE
EDGE
ENHANCEMENT
FILTER
FOR
CONTRAST
EDGE
LOCALIZATION
DETERMINE
WHICH
LOCAL
MAXIMA
FROM
FILTER
OUTPUT
ARE
ACTUALLY
EDGES
VS
NOISE
THRESHOLD
THIN
THRESHOLDING
CHOOSE
A
THRESHOLD
VALUE
T
SET
ANY
PIXELS
LESS
THAN
T
TO
ZERO
OFF
SET
ANY
PIXELS
GREATER
THAN
OR
EQUAL
TO
T
TO
ONE
ON
ORIGINAL
IMAGE
SOURCE
K
GRAUMAN
GRADIENT
MAGNITUDE
IMAGE
SOURCE
K
GRAUMAN
SOURCE
K
GRAUMAN
SOURCE
K
GRAUMAN
THRESHOLD
AT
MINIMUM
LEVEL
GET
ORIENTATION
THETA
GY
GX
NORM
OF
THE
GRADIENT
THRESHOLDING
HOW
TO
TURN
THESE
THICK
REGIONS
OF
THE
GRADIENT
INTO
CURVES
CHECK
IF
PIXEL
IS
LOCAL
MAXIMUM
ALONG
GRADIENT
DIRECTION
SELECT
SINGLE
MAX
ACROSS
WIDTH
OF
THE
EDGE
REQUIRES
CHECKING
INTERPOLATED
PIXELS
P
AND
R
BRIGHTNESS
COLOR
TEXTURE
COMBINED
HUMAN
GIVEN
A
GRAYSCALE
IMAGE
OR
AN
INTERMEDIATE
MATRIX
THRESHOLD
TO
CREATE
A
BINARY
OUTPUT
EXAMPLE
EDGE
DETECTION
GRADIENT
MAGNITUDE
FIND
T
LOOKING
FOR
PIXELS
WHERE
GRADIENT
IS
STRONG
GIVEN
A
GRAYSCALE
IMAGE
OR
AN
INTERMEDIATE
MATRIX
THRESHOLD
TO
CREATE
A
BINARY
OUTPUT
EXAMPLE
INTENSITY
BASED
DETECTION
FIND
IM
LOOKING
FOR
DARK
PIXELS
GIVEN
A
GRAYSCALE
IMAGE
OR
AN
INTERMEDIATE
MATRIX
THRESHOLD
TO
CREATE
A
BINARY
OUTPUT
EXAMPLE
COLOR
BASED
DETECTION
FIND
HUE
HUE
LOOKING
FOR
PIXELS
WITHIN
A
CERTAIN
HUE
RANGE
STRUCTURING
ELEMENTS
MASKS
OF
VARYING
SHAPES
AND
SIZES
USED
TO
PERFORM
MORPHOLOGY
FOR
EXAMPLE
SCAN
MASK
ACROSS
FOREGROUND
PIXELS
TO
TRANSFORM
THE
BINARY
IMAGE
HELP
STREL
EXPANDS
CONNECTED
COMPONENTS
GROW
FEATURES
FILL
HOLES
BEFORE
DILATION
AFTER
DILATION
ERODE
CONNECTED
COMPONENTS
SHRINK
FEATURES
REMOVE
BRIDGES
BRANCHES
NOISE
BEFORE
EROSION
AFTER
EROSION
SOURCE
SHAPIRO
AND
STOCKMAN
ERODE
THEN
DILATE
REMOVE
SMALL
OBJECTS
KEEP
ORIGINAL
SHAPE
BEFORE
OPENING
AFTER
OPENING
CLOSING
DILATE
THEN
ERODE
FILL
HOLES
BUT
KEEP
ORIGINAL
SHAPE
BEFORE
CLOSING
AFTER
CLOSING
APPLET
SUPPOSE
YOU
HAVE
TO
CLICK
ON
SOME
POINT
GO
AWAY
AND
COME
BACK
AFTER
I
DEFORM
THE
IMAGE
AND
CLICK
ON
THE
SAME
POINTS
AGAIN
WHICH
POINTS
WOULD
YOU
CHOOSE
ORIGINAL
HOIEM
D
F
A
FB
T
GRAUMAN
B
LEIBE
FIND
A
SET
OF
DISTINCTIVE
KEY
POINTS
DEFINE
A
REGION
AROUND
EACH
KEYPOINT
EXTRACT
AND
NORMALIZE
THE
REGION
CONTENT
COMPUTE
A
LOCAL
DESCRIPTOR
FROM
THE
NORMALIZED
REGION
MATCH
LOCAL
DESCRIPTORS
GOALS
FOR
KEYPOINTS
DETECT
POINTS
THAT
ARE
REPEATABLE
AND
DISTINCTIVE
ADAPTED
FROM
D
HOIEM
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
CORNERS
AS
DISTINCTIVE
INTEREST
POINTS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
WINDOW
FUNCTION
W
X
Y
OR
IN
WINDOW
OUTSIDE
GAUSSIAN
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
E
U
V
EXPANDING
I
X
Y
IN
A
TAYLOR
SERIES
EXPANSION
WE
HAVE
FOR
SMALL
SHIFTS
U
V
A
QUADRATIC
APPROXIMATION
TO
THE
ERROR
SURFACE
BETWEEN
A
PATCH
AND
ITSELF
SHIFTED
BY
U
V
WHERE
M
IS
A
MATRIX
COMPUTED
FROM
IMAGE
DERIVATIVES
EDGE
CORNER
AND
ARE
LARGE
FLAT
REGION
AND
ARE
SMALL
COMPUTE
IMAGE
GRADIENTS
IX
AND
IY
FOR
ALL
PIXELS
FOR
EACH
PIXEL
COMPUTE
BY
LOOPING
OVER
NEIGHBORS
X
Y
COMPUTE
K
EMPIRICAL
CONSTANT
K
FIND
POINTS
WITH
LARGE
CORNER
RESPONSE
FUNCTION
R
R
THRESHOLD
TAKE
THE
POINTS
OF
LOCALLY
MAXIMUM
R
AS
THE
DETECTED
FEATURE
POINTS
I
E
PIXELS
WHERE
R
IS
BIGGER
THAN
FOR
ALL
THE
OR
NEIGHBORS
FROLOVA
D
SIMAKOV
COMPUTE
CORNER
RESPONSE
AT
EVERY
PIXEL
I
A
I
B
ONLY
DERIVATIVES
ARE
USED
INVARIANCE
TO
INTENSITY
SHIFT
I
I
B
INTENSITY
SCALING
I
A
I
R
R
THRESHOLD
X
IMAGE
COORDINATE
X
IMAGE
COORDINATE
DERIVATIVES
AND
WINDOW
FUNCTION
ARE
SHIFT
INVARIANT
SECOND
MOMENT
ELLIPSE
ROTATES
BUT
ITS
SHAPE
I
E
EIGENVALUES
REMAINS
THE
SAME
CORNER
ALL
POINTS
WILL
BE
CLASSIFIED
AS
EDGES
SCALE
INVARIANT
DETECTION
THE
PROBLEM
HOW
DO
WE
CHOOSE
CORRESPONDING
CIRCLES
INDEPENDENTLY
IN
EACH
IMAGE
DO
OBJECTS
IN
THE
IMAGE
HAVE
A
CHARACTERISTIC
SCALE
THAT
WE
CAN
IDENTIFY
F
D
G
DX
F
D
G
DX
EDGE
DERIVATIVE
OF
GAUSSIAN
EDGE
MAX
OF
DERIVATIVE
F
D
G
D
F
G
DX
EDGE
SECOND
DERIVATIVE
OF
GAUSSIAN
EDGE
ZERO
CROSSING
OF
DERIVATIVE
EDGE
RIPPLE
BLOB
SUPERPOSITION
OF
TWO
RIPPLES
MAXIMUM
SPATIAL
SELECTION
THE
MAGNITUDE
OF
THE
LAPLACIAN
RESPONSE
WILL
ACHIEVE
A
MAXIMUM
AT
THE
CENTER
OF
THE
BLOB
PROVIDED
THE
SCALE
OF
THE
LAPLACIAN
IS
MATCHED
TO
THE
SCALE
OF
THE
BLOB
LAZEBNIK
WHAT
IS
A
USEFUL
SIGNATURE
FUNCTION
LAPLACIAN
OF
GAUSSIAN
BLOB
DETECTOR
DIFFERENCE
OF
GAUSSIAN
LAPLACIAN
WE
CAN
APPROXIMATE
THE
LAPLACIAN
WITH
A
DIFFERENCE
OF
GAUSSIANS
MORE
EFFICIENT
TO
IMPLEMENT
LAPLACIAN
DOG
G
X
Y
K
G
X
Y
DIFFERENCE
OF
GAUSSIANS
FIND
LOCAL
MAXIMA
IN
POSITION
SCALE
SPACE
OF
DIFFERENCE
OF
GAUSSIAN
LXX
LYY
LIST
OF
X
Y
LOWE
ICCV
HISTOGRAM
OF
ORIENTED
GRADIENTS
CAPTURES
IMPORTANT
TEXTURE
INFORMATION
ROBUST
TO
SMALL
TRANSLATIONS
AFFINE
DEFORMATIONS
GRAUMAN
B
LEIBE
SCALE
INVARIANT
FEATURE
TRANSFORM
BASIC
IDEA
TAKE
SQUARE
WINDOW
AROUND
DETECTED
FEATURE
COMPUTE
GRADIENT
ORIENTATION
FOR
EACH
PIXEL
CREATE
HISTOGRAM
OVER
EDGE
ORIENTATIONS
WEIGHTED
BY
MAGNITUDE
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
COMPUTE
AN
ORIENTATION
HISTOGRAM
FOR
EACH
CELL
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
FULL
VERSION
DIVIDE
THE
WINDOW
INTO
A
GRID
OF
CELLS
CASE
SHOWN
BELOW
COMPUTE
AN
ORIENTATION
HISTOGRAM
FOR
EACH
CELL
CELLS
ORIENTATIONS
DIMENSIONAL
DESCRIPTOR
THRESHOLD
NORMALIZE
THE
DESCRIPTOR
SUCH
THAT
MAKING
DESCRIPTOR
ROTATION
INVARIANT
ROTATE
PATCH
ACCORDING
TO
ITS
DOMINANT
GRADIENT
ORIENTATION
THIS
PUTS
THE
PATCHES
INTO
A
CANONICAL
ORIENTATION
GRAUMAN
IMAGE
FROM
MATTHEW
BROWN
LOCAL
FEATURES
DESIRED
PROPERTIES
REPEATABILITY
THE
SAME
FEATURE
CAN
BE
FOUND
IN
SEVERAL
IMAGES
DESPITE
GEOMETRIC
AND
PHOTOMETRIC
TRANSFORMATIONS
SALIENCY
EACH
FEATURE
HAS
A
DISTINCTIVE
DESCRIPTION
COMPACTNESS
AND
EFFICIENCY
MANY
FEWER
FEATURES
THAN
IMAGE
PIXELS
LOCALITY
A
FEATURE
OCCUPIES
A
RELATIVELY
SMALL
AREA
OF
THE
IMAGE
ROBUST
TO
CLUTTER
AND
OCCLUSION
MATCHING
LOCAL
FEATURES
IMAGE
IMAGE
TO
GENERATE
CANDIDATE
MATCHES
FIND
PATCHES
THAT
HAVE
THE
MOST
SIMILAR
APPEARANCE
E
G
LOWEST
SSD
SIMPLEST
APPROACH
COMPARE
THEM
ALL
TAKE
THE
CLOSEST
OR
CLOSEST
K
OR
WITHIN
A
THRESHOLDED
DISTANCE
AMBIGUOUS
MATCHES
IMAGE
IMAGE
AT
WHAT
SSD
VALUE
DO
WE
HAVE
A
GOOD
MATCH
TO
ADD
ROBUSTNESS
TO
MATCHING
CAN
CONSIDER
RATIO
DISTANCE
TO
BEST
MATCH
DISTANCE
TO
SECOND
BEST
MATCH
IF
LOW
FIRST
MATCH
LOOKS
GOOD
K
GRAUMAN
IF
HIGH
COULD
BE
AMBIGUOUS
MATCH
EXTRACT
FEATURES
EXTRACT
FEATURES
COMPUTE
PUTATIVE
MATCHES
EXTRACT
FEATURES
COMPUTE
PUTATIVE
MATCHES
LOOP
HYPOTHESIZE
TRANSFORMATION
T
SMALL
GROUP
OF
PUTATIVE
MATCHES
THAT
ARE
RELATED
BY
T
EXTRACT
FEATURES
COMPUTE
PUTATIVE
MATCHES
LOOP
HYPOTHESIZE
TRANSFORMATION
T
SMALL
GROUP
OF
PUTATIVE
MATCHES
THAT
ARE
RELATED
BY
T
VERIFY
TRANSFORMATION
SEARCH
FOR
OTHER
MATCHES
CONSISTENT
WITH
T
EXTRACT
FEATURES
COMPUTE
PUTATIVE
MATCHES
LOOP
HYPOTHESIZE
TRANSFORMATION
T
SMALL
GROUP
OF
PUTATIVE
MATCHES
THAT
ARE
RELATED
BY
T
VERIFY
TRANSFORMATION
SEARCH
FOR
OTHER
MATCHES
CONSISTENT
WITH
T
TWO
TYPES
OF
LIGHT
SENSITIVE
RECEPTORS
CONES
CONE
SHAPED
LESS
SENSITIVE
OPERATE
IN
HIGH
LIGHT
COLOR
VISION
RODS
ROD
SHAPED
HIGHLY
SENSITIVE
OPERATE
AT
NIGHT
GRAY
SCALE
VISION
SLOWER
TO
RESPOND
SLIDE
CREDIT
EFROS
THE
PHYSICS
OF
LIGHT
SOME
EXAMPLES
OF
THE
SPECTRA
OF
LIGHT
SOURCES
A
RUBY
LASER
B
GALLIUM
PHOSPHIDE
CRYSTAL
WAVELENGTH
NM
WAVELENGTH
NM
TUNGSTEN
LIGHTBULB
NORMAL
DAYLIGHT
STEPHEN
E
PALMER
THE
PHYSICS
OF
LIGHT
SOME
EXAMPLES
OF
THE
REFLECTANCE
SPECTRA
OF
SURFACES
700
WAVELENGTH
NM
STEPHEN
E
PALMER
PHYSIOLOGY
OF
COLOR
VISION
THREE
KINDS
OF
CONES
NM
M
L
WAVELENGTH
NM
WHY
ARE
M
AND
L
CONES
SO
CLOSE
STEPHEN
E
PALMER
METAMERS
SPECTRAL
REFLECTANCES
FOR
SOME
NATURAL
OBJECTS
HOW
MUCH
OF
EACH
WAVELENGTH
IS
REFLECTED
FOR
THAT
SURFACE
GRAUMAN
FORSYTH
PONCE
MEASUREMENTS
BY
E
KOIVISTO
COLOR
CONSTANCY
INTERPRET
SURFACE
IN
TERMS
OF
TRUE
COLOR
RATHER
THAN
OBSERVED
INTENSITY
HUMANS
ARE
GOOD
AT
IT
COMPUTERS
ARE
NOT
NEARLY
AS
GOOD
HOW
DO
WE
COMPUTE
THE
WEIGHTS
THAT
WILL
YIELD
A
PERCEPTUAL
MATCH
FOR
ANY
TEST
LIGHT
USING
A
GIVEN
SET
OF
PRIMARIES
SELECT
PRIMARIES
ESTIMATE
THEIR
COLOR
MATCHING
FUNCTIONS
OBSERVER
MATCHES
SERIES
OF
MONOCHROMATIC
LIGHTS
ONE
AT
EACH
WAVELENGTH
C
C
I
MATCHES
I
I
I
NOW
HAVE
MATCHING
FUNCTIONS
FOR
ALL
MONOCHROMATIC
LIGHT
SOURCES
SO
WE
KNOW
HOW
TO
MATCH
A
UNIT
OF
EACH
WAVELENGTH
ARBITRARY
NEW
SPECTRAL
SIGNAL
IS
A
LINEAR
COMBINATION
OF
THE
MONOCHROMATIC
SOURCES
T
T
T
IMAGE
HUMAN
SEGMENTATION
GROUP
TOGETHER
SIMILAR
LOOKING
PIXELS
FOR
EFFICIENCY
OF
FURTHER
PROCESSING
SUPERPIXELS
X
REN
AND
J
MALIK
ICCV
OVERSEGMENTATION
UNDERSEGMENTATION
MULTIPLE
SEGMENTATIONS
IMAGE
SEGMENTATION
TOY
EXAMPLE
BLACK
PIXELS
GRAY
PIXELS
WHITE
PIXELS
INPUT
IMAGE
INTENSITY
THESE
INTENSITIES
DEFINE
THE
THREE
GROUPS
WE
COULD
LABEL
EVERY
PIXEL
IN
THE
IMAGE
ACCORDING
TO
WHICH
OF
THESE
PRIMARY
INTENSITIES
IT
IS
I
E
SEGMENT
THE
IMAGE
BASED
ON
THE
INTENSITY
FEATURE
WHAT
IF
THE
IMAGE
ISN
T
QUITE
SO
SIMPLE
INPUT
IMAGE
INPUT
IMAGE
INTENSITY
INTENSITY
INPUT
IMAGE
INTENSITY
NOW
HOW
TO
DETERMINE
THE
THREE
MAIN
INTENSITIES
THAT
DEFINE
OUR
GROUPS
WE
NEED
TO
CLUSTER
INTENSITY
GOAL
CHOOSE
THREE
CENTERS
AS
THE
REPRESENTATIVE
INTENSITIES
AND
LABEL
EVERY
PIXEL
ACCORDING
TO
WHICH
OF
THESE
CENTERS
IT
IS
NEAREST
TO
BEST
CLUSTER
CENTERS
ARE
THOSE
THAT
MINIMIZE
SSD
BETWEEN
ALL
POINTS
AND
THEIR
NEAREST
CLUSTER
CENTER
CI
CLUSTERING
WITH
THIS
OBJECTIVE
IT
IS
A
CHICKEN
AND
EGG
PROBLEM
IF
WE
KNEW
THE
CLUSTER
CENTERS
WE
COULD
ALLOCATE
POINTS
TO
GROUPS
BY
ASSIGNING
EACH
TO
ITS
CLOSEST
CENTER
IF
WE
KNEW
THE
GROUP
MEMBERSHIPS
WE
COULD
GET
THE
CENTERS
BY
COMPUTING
THE
MEAN
PER
GROUP
K
MEANS
CLUSTERING
BASIC
IDEA
RANDOMLY
INITIALIZE
THE
K
CLUSTER
CENTERS
AND
ITERATE
BETWEEN
THE
TWO
STEPS
WE
JUST
SAW
RANDOMLY
INITIALIZE
THE
CLUSTER
CENTERS
CK
GIVEN
CLUSTER
CENTERS
DETERMINE
POINTS
IN
EACH
CLUSTER
FOR
EACH
POINT
P
FIND
THE
CLOSEST
CI
PUT
P
INTO
CLUSTER
I
GIVEN
POINTS
IN
EACH
CLUSTER
SOLVE
FOR
CI
SET
CI
TO
BE
THE
MEAN
OF
POINTS
IN
CLUSTER
I
IF
CI
HAVE
CHANGED
REPEAT
STEP
PROPERTIES
WILL
ALWAYS
CONVERGE
TO
SOME
SOLUTION
CAN
BE
A
LOCAL
MINIMUM
DOES
NOT
ALWAYS
FIND
THE
GLOBAL
MINIMUM
OF
OBJECTIVE
FUNCTION
SOURCE
STEVE
SEITZ
K
MEANS
L
ASK
USER
HOW
MANY
CLLUSTERS
THEY
D
NKE
E
G
K
K
MEANS
L
ASK
USER
HOW
CLUSTERS
THEY
D
LLIIKE
E
G
K
GUESS
K
CLLUSTER
CENTER
LOCATIONS
K
MEANS
ASK
USER
HOW
MANY
CLUSTERS
THEY
D
LIKE
E
G
K
GUESS
K
CLUSTER
CE
NT
E
R
LOCAT
IONS
EACH
DLATAPOINT
FINDS
OUT
WH
ICH
CENTER
IT
CLOSEST
TO
THUS
EACH
CENTE
R
A
SET
OF
DATAPO
INTS
L
K
MEANS
ASK
USER
HOW
MANY
CLUSTERS
THEY
D
LIKE
E
G
K
GUESS
K
CLUSTER
CENTER
LOCATIONS
EACH
DATAPOINT
FINDS
OUT
WHICH
CENT
ER
IT
CLOSEST
TO
EACH
CENTER
FINDS
THE
CENTROID
OF
THE
POINTS
IT
OWNS
O
L
SEGMENTATION
AS
CLUSTERING
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
SIMILARITY
FEATURE
SPACE
INTENSITY
VALUE
D
K
K
QUANTIZATION
OF
THE
FEATURE
SPACE
SEGMENTATION
LABEL
MAP
SEGMENTATION
AS
CLUSTERING
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
POSITION
SIMILARITY
SOURCE
K
GRAUMAN
X
BOTH
REGIONS
ARE
BLACK
BUT
IF
WE
ALSO
INCLUDE
POSITION
X
Y
THEN
WE
COULD
GROUP
THE
TWO
INTO
DISTINCT
SEGMENTS
WAY
TO
ENCODE
BOTH
SIMILARITY
PROXIMITY
SEGMENTATION
AS
CLUSTERING
COLOR
BRIGHTNESS
POSITION
ALONE
ARE
NOT
ENOUGH
TO
DISTINGUISH
ALL
REGIONS
SOURCE
L
LAZEBNIK
SEGMENTATION
AS
CLUSTERING
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
TEXTURE
SIMILARITY
FILTER
BANK
OF
FILTERS
FEATURE
SPACE
FILTER
BANK
RESPONSES
E
G
D
SOURCE
K
GRAUMAN
FIND
TEXTONS
BY
CLUSTERING
VECTORS
OF
FILTER
BANK
OUTPUTS
DESCRIBE
TEXTURE
IN
A
WINDOW
BASED
ON
TEXTON
HISTOGRAM
IMAGE
TEXTON
MAP
TEXTON
INDEX
TEXTON
INDEX
MALIK
BELONGIE
LEUNG
AND
SHI
IJCV
SOURCE
L
LAZEBNIK
THE
MEAN
SHIFT
ALGORITHM
SEEKS
MODES
OR
LOCAL
MAXIMA
OF
DENSITY
IN
THE
FEATURE
SPACE
IMAGE
FEATURE
SPACE
L
U
V
COLOR
VALUES
SOURCE
K
GRAUMAN
SEARCH
WINDOW
CENTER
OF
MASS
MEAN
SHIFT
VECTOR
SOURCE
D
HOIEM
CLUSTER
ALL
DATA
POINTS
IN
THE
ATTRACTION
BASIN
OF
A
MODE
ATTRACTION
BASIN
THE
REGION
FOR
WHICH
ALL
TRAJECTORIES
LEAD
TO
THE
SAME
MODE
SLIDE
BY
Y
UKRAINITZ
B
SAREL
FULLY
CONNECTED
GRAPH
NODE
VERTEX
FOR
EVERY
PIXEL
LINK
BETWEEN
EVERY
PAIR
OF
PIXELS
P
Q
AFFINITY
WEIGHT
WPQ
FOR
EACH
LINK
EDGE
WPQ
MEASURES
SIMILARITY
SIMILARITY
IS
INVERSELY
PROPORTIONAL
TO
DIFFERENCE
IN
COLOR
AND
POSITION
A
B
C
BREAK
GRAPH
INTO
SEGMENTS
WANT
TO
DELETE
LINKS
THAT
CROSS
BETWEEN
SEGMENTS
EASIEST
TO
BREAK
LINKS
THAT
HAVE
LOW
SIMILARITY
LOW
WEIGHT
SIMILAR
PIXELS
SHOULD
BE
IN
THE
SAME
SEGMENTS
DISSIMILAR
PIXELS
SHOULD
BE
IN
DIFFERENT
SEGMENTS
B
NORMALIZED
CUT
FIX
BIAS
OF
MIN
CUT
BY
NORMALIZING
FOR
SIZE
OF
SEGMENTS
CUT
A
B
ASSOC
A
V
CUT
A
B
ASSOC
B
V
ASSOC
A
V
SUM
OF
WEIGHTS
OF
ALL
EDGES
THAT
TOUCH
A
NCUT
VALUE
SMALL
WHEN
WE
GET
TWO
CLUSTERS
WITH
MANY
EDGES
WITH
HIGH
WEIGHTS
AND
FEW
EDGES
OF
LOW
WEIGHT
BETWEEN
THEM
APPROXIMATE
SOLUTION
FOR
MINIMIZING
THE
NCUT
VALUE
EIGENVALUE
PROBLEM
SHI
AND
J
MALIK
CVPR
SOURCE
STEVE
SEITZ
DIFFICULTY
OF
LINE
FITTING
EXTRA
EDGE
POINTS
CLUTTER
MULTIPLE
MODELS
WHICH
POINTS
GO
WITH
WHICH
LINE
IF
ANY
ONLY
SOME
PARTS
OF
EACH
LINE
DETECTED
AND
SOME
PARTS
ARE
MISSING
HOW
TO
FIND
A
LINE
THAT
BRIDGES
MISSING
EVIDENCE
NOISE
IN
MEASURED
EDGE
POINTS
ORIENTATIONS
HOW
TO
DETECT
TRUE
UNDERLYING
PARAMETERS
KRISTEN
GRAUMAN
LEAST
SQUARES
LINE
FITTING
DATA
XN
YN
LINE
EQUATION
YI
M
XI
B
FIND
M
B
TO
MINIMIZE
XI
YI
Y
MX
B
M
M
E
N
X
Y
AP
Y
I
I
B
I
B
X
Y
N
N
MODIFIED
FROM
SVETLANA
LAZEBNIK
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
WHAT
DOES
A
POINT
IN
THE
IMAGE
SPACE
MAP
TO
ANSWER
THE
SOLUTIONS
OF
B
THIS
IS
A
LINE
IN
HOUGH
SPACE
TO
GO
FROM
IMAGE
SPACE
TO
HOUGH
SPACE
GIVEN
A
SET
OF
POINTS
X
Y
FIND
ALL
M
B
SUCH
THAT
Y
MX
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
WHAT
ARE
THE
LINE
PARAMETERS
FOR
THE
LINE
THAT
CONTAINS
BOTH
AND
IT
IS
THE
INTERSECTION
OF
THE
LINES
B
AND
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
HOW
CAN
WE
USE
THIS
TO
FIND
THE
MOST
LIKELY
PARAMETERS
M
B
FOR
THE
MOST
PROMINENT
LINE
IN
THE
IMAGE
SPACE
LET
EACH
EDGE
POINT
IN
IMAGE
SPACE
VOTE
FOR
A
SET
OF
POSSIBLE
PARAMETERS
IN
HOUGH
SPACE
ACCUMULATE
VOTES
IN
DISCRETE
SET
OF
BINS
PARAMETERS
WITH
THE
MOST
VOTES
INDICATE
LINE
IN
IMAGE
SPACE
PROBLEMS
WITH
THE
M
B
SPACE
UNBOUNDED
PARAMETER
DOMAINS
VERTICAL
LINES
REQUIRE
INFINITE
M
ALTERNATIVE
POLAR
REPRESENTATION
Y
SIN
EACH
POINT
X
Y
WILL
ADD
A
SINUSOID
IN
THE
PARAMETER
SPACE
HOUGH
TRANSFORM
HOUGH
MACHINE
ANALYSIS
OF
BUBBLE
CHAMBER
PICTURES
PROC
INT
CONF
HIGH
ENERGY
ACCELERATORS
AND
INSTRUMENTATION
USE
A
POLAR
REPRESENTATION
FOR
THE
PARAMETER
SPACE
Y
X
HOUGH
SPACE
X
COS
YSIN
INITIALIZE
ACCUMULATOR
H
TO
ALL
ZEROS
FOR
EACH
FEATURE
POINT
X
Y
IN
THE
IMAGE
FOR
Θ
TO
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
END
END
FIND
THE
VALUE
OF
Θ
Ρ
WHERE
H
Θ
Ρ
IS
A
LOCAL
MAXIMUM
THE
DETECTED
LINE
IN
THE
IMAGE
IS
GIVEN
BY
Ρ
X
COS
Θ
Y
SIN
Θ
RECALL
WHEN
WE
DETECT
AN
EDGE
POINT
WE
ALSO
KNOW
ITS
GRADIENT
DIRECTION
BUT
THIS
MEANS
THAT
THE
LINE
IS
UNIQUELY
DETERMINED
MODIFIED
HOUGH
TRANSFORM
FOR
EACH
EDGE
POINT
X
Y
Θ
GRADIENT
ORIENTATION
AT
X
Y
Ρ
X
COS
Θ
Y
SIN
Θ
H
Θ
Ρ
H
Θ
Ρ
END
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
A
FIXED
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
B
HOUGH
SPACE
A
HOUGH
TRANSFORM
FOR
CIRCLES
CIRCLE
CENTER
A
B
AND
RADIUS
R
XI
A
YI
B
FOR
A
FIXED
RADIUS
R
UNKNOWN
GRADIENT
DIRECTION
INTERSECTION
MOST
VOTES
FOR
CENTER
OCCUR
HERE
IMAGE
SPACE
HOUGH
SPACE
RANSAC
GENERAL
FORM
RANSAC
LOOP
RANDOMLY
SELECT
A
SEED
GROUP
OF
POINTS
ON
WHICH
TO
BASE
MODEL
ESTIMATE
FIT
MODEL
TO
THESE
POINTS
FIND
INLIERS
TO
THIS
MODEL
I
E
POINTS
WHOSE
DISTANCE
FROM
THE
LINE
IS
LESS
THAN
T
IF
THERE
ARE
D
OR
MORE
INLIERS
RE
COMPUTE
ESTIMATE
OF
MODEL
ON
ALL
OF
THE
INLIERS
REPEAT
N
TIMES
KEEP
THE
MODEL
WITH
THE
LARGEST
NUMBER
OF
INLIERS
LEAST
SQUARES
FIT
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
UNCONTAMINATED
SAMPLE
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
RANDOMLY
SELECT
MINIMAL
SUBSET
OF
POINTS
HYPOTHESIZE
A
MODEL
COMPUTE
ERROR
FUNCTION
SELECT
POINTS
CONSISTENT
WITH
MODEL
REPEAT
HYPOTHESIZE
AND
VERIFY
LOOP
ALIGNMENT
PROBLEM
WE
HAVE
PREVIOUSLY
CONSIDERED
HOW
TO
FIT
A
MODEL
TO
IMAGE
EVIDENCE
E
G
A
LINE
TO
EDGE
POINTS
IN
ALIGNMENT
WE
WILL
FIT
THE
PARAMETERS
OF
SOME
TRANSFORMATION
ACCORDING
TO
A
SET
OF
MATCHING
FEATURE
PAIRS
CORRESPONDENCES
DIFFICULTIES
XI
NOISE
OUTLIERS
XI
KRISTEN
GRAUMAN
AND
DEREK
HOIEM
GIVEN
MATCHED
POINTS
IN
A
AND
B
ESTIMATE
THE
TRANSLATION
OF
THE
OBJECT
X
B
X
A
TX
I
I
I
I
T
Y
TX
TY
PROBLEM
OUTLIERS
RANSAC
SOLUTION
X
B
X
A
TX
SAMPLE
A
SET
OF
MATCHING
POINTS
PAIR
I
I
SOLVE
FOR
TRANSFORMATION
PARAMETERS
SCORE
PARAMETERS
WITH
NUMBER
OF
INLIERS
REPEAT
STEPS
N
TIMES
I
I
T
Y
TX
TY
PROBLEM
OUTLIERS
MULTIPLE
OBJECTS
AND
OR
MANY
TO
ONE
MATCHES
HOUGH
TRANSFORM
SOLUTION
X
B
X
A
TX
INITIALIZE
A
GRID
OF
PARAMETER
VALUES
I
I
EACH
MATCHED
PAIR
CASTS
A
VOTE
FOR
CONSISTENT
VALUES
FIND
THE
PARAMETERS
WITH
THE
MOST
VOTES
I
I
T
Y
INDEXING
LOCAL
FEATURES
INVERTED
FILE
INDEX
FOR
TEXT
DOCUMENTS
AN
EFFICIENT
WAY
TO
FIND
ALL
PAGES
ON
WHICH
A
WORD
OCCURS
IS
TO
USE
AN
INDEX
WE
WANT
TO
FIND
ALL
IMAGES
IN
WHICH
A
FEATURE
OCCURS
TO
USE
THIS
IDEA
WE
LL
NEED
TO
MAP
OUR
FEATURES
TO
VISUAL
WORDS
KRISTEN
GRAUMAN
EXTRACT
SOME
LOCAL
FEATURES
FROM
A
NUMBER
OF
IMAGES
E
G
SIFT
DESCRIPTOR
SPACE
EACH
POINT
IS
DIMENSIONAL
EACH
POINT
IS
A
LOCAL
DESCRIPTOR
E
G
SIFT
VECTOR
EXAMPLE
EACH
VISUAL
WORDS
GROUP
OF
PATCHES
BELONGS
TO
THE
SAME
VISUAL
WORD
FIGURE
FROM
SIVIC
ZISSERMAN
ICCV
ADAPTED
FROM
KRISTEN
GRAUMAN
INVERTED
FILE
INDEX
AND
BAGS
OF
WORDS
SIMILARITY
EXTRACT
WORDS
IN
QUERY
INVERTED
FILE
INDEX
TO
FIND
RELEVANT
FRAMES
COMPARE
WORD
COUNTS
ADAPTED
FROM
KRISTEN
GRAUMAN
BAGS
OF
VISUAL
WORDS
SUMMARIZE
ENTIRE
IMAGE
BASED
ON
ITS
DISTRIBUTION
HISTOGRAM
OF
WORD
OCCURRENCES
ANALOGOUS
TO
BAG
OF
WORDS
REPRESENTATION
COMMONLY
USED
FOR
DOCUMENTS
COMPARING
BAGS
OF
WORDS
RANK
FRAMES
BY
NORMALIZED
SCALAR
PRODUCT
BETWEEN
THEIR
POSSIBLY
WEIGHTED
OCCURRENCE
COUNTS
NEAREST
NEIGHBOR
SEARCH
FOR
SIMILAR
IMAGES
𝑠𝑖𝑚
𝑉
𝑑𝑗
𝑖
𝑞
𝑖
FOR
VOCABULARY
OF
V
WORDS
J
KRISTEN
GRAUMAN
TF
IDF
WEIGHTING
TERM
FREQUENCY
INVERSE
DOCUMENT
FREQUENCY
DESCRIBE
FRAME
BY
FREQUENCY
OF
EACH
WORD
WITHIN
IT
DOWNWEIGHT
WORDS
THAT
APPEAR
OFTEN
IN
THE
DATABASE
STANDARD
WEIGHTING
FOR
TEXT
RETRIEVAL
NUMBER
OF
OCCURRENCES
OF
WORD
I
IN
DOCUMENT
D
NUMBER
OF
WORDS
IN
DOCUMENT
D
TOTAL
NUMBER
OF
DOCUMENTS
IN
DATABASE
NUMBER
OF
DOCUMENTS
WORD
I
OCCURS
IN
IN
WHOLE
DATABASE
KRISTEN
GRAUMAN
VIDEO
GOOGLE
SYSTEM
COLLECT
ALL
WORDS
WITHIN
QUERY
REGION
INVERTED
FILE
INDEX
TO
FIND
RELEVANT
FRAMES
COMPARE
WORD
COUNTS
SPATIAL
VERIFICATION
SIVIC
ZISSERMAN
ICCV
DEMO
ONLINE
AT
ESEARCH
VGOOGLE
INDEX
HTML
GRAUMAN
B
LEIBE
SCORING
RETRIEVAL
QUALITY
QUERY
DATABASE
SIZE
IMAGES
RELEVANT
TOTAL
IMAGES
RESULTS
ORDERED
PRECISION
RELEVANT
RETURNED
RECALL
RELEVANT
TOTAL
RELEVANT
RECALL
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
BOTH
IMAGE
PAIRS
HAVE
MANY
VISUAL
WORDS
IN
COMMON
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
DB
IMAGE
WITH
HIGH
BOW
SIMILARITY
ONLY
SOME
OF
THE
MATCHES
ARE
MUTUALLY
CONSISTENT
EXAMPLES
OF
PARAMETRIC
WARPS
TRANSLATION
ROTATION
ASPECT
AFFINE
PERSPECTIVE
P
X
Y
P
X
Y
TRANSFORMATION
T
IS
A
COORDINATE
CHANGING
MACHINE
P
T
P
WHAT
DOES
IT
MEAN
THAT
T
IS
GLOBAL
IS
THE
SAME
FOR
ANY
POINT
P
CAN
BE
DESCRIBED
BY
JUST
A
FEW
NUMBERS
PARAMETERS
LET
REPRESENT
T
AS
A
MATRIX
P
MP
X
Y
X
Y
WHAT
TRANSFORMATIONS
CAN
BE
REPRESENTED
WITH
A
MATRIX
SCALING
X
X
X
X
SX
X
Y
Y
Y
SY
Y
Y
ROTATE
AROUND
X
COS
X
SIN
Y
X
COS
SIN
X
Y
SIN
X
COS
Y
Y
SIN
COS
Y
SHEAR
X
X
SHX
Y
X
SHX
X
Y
SH
X
Y
Y
SH
Y
Y
Y
HOMOGENEOUS
COORDINATES
TO
CONVERT
TO
HOMOGENEOUS
COORDINATES
HOMOGENEOUS
IMAGE
COORDINATES
CONVERTING
FROM
HOMOGENEOUS
COORDINATES
HOMOGENEOUS
COORDINATES
XX
TX
XX
XX
TTXX
YY
Y
Y
T
TY
Y
Y
TYY
TX
TY
M
XI
YI
XI
X
Y
M
Y
I
I
I
HOW
MANY
MATCHES
CORRESPONDENCE
PAIRS
DO
WE
NEED
TO
SOLVE
FOR
THE
TRANSFORMATION
PARAMETERS
ONCE
WE
HAVE
SOLVED
FOR
THE
PARAMETERS
HOW
DO
WE
COMPUTE
X
NEW
Y
NEW
GIVEN
XNEW
YNEW
WHERE
DO
THE
MATCHES
COME
FROM
Y
SEND
EACH
PIXEL
F
X
Y
TO
ITS
CORRESPONDING
LOCATION
X
Y
T
X
Y
IN
THE
SECOND
IMAGE
Q
WHAT
IF
PIXEL
LANDS
BETWEEN
TWO
PIXELS
Y
X
F
X
Y
X
G
X
Y
SEND
EACH
PIXEL
F
X
Y
TO
ITS
CORRESPONDING
LOCATION
X
Y
T
X
Y
IN
THE
SECOND
IMAGE
Q
WHAT
IF
PIXEL
LANDS
BETWEEN
TWO
PIXELS
A
DISTRIBUTE
COLOR
AMONG
NEIGHBORING
PIXELS
X
Y
KNOWN
AS
SPLATTING
Y
GET
EACH
PIXEL
G
X
Y
FROM
ITS
CORRESPONDING
LOCATION
X
Y
T
X
Y
IN
THE
FIRST
IMAGE
Q
WHAT
IF
PIXEL
COMES
FROM
BETWEEN
TWO
PIXELS
Y
X
F
X
Y
X
G
X
Y
GET
EACH
PIXEL
G
X
Y
FROM
ITS
CORRESPONDING
LOCATION
X
Y
T
X
Y
IN
THE
FIRST
IMAGE
Q
WHAT
IF
PIXEL
COMES
FROM
BETWEEN
TWO
PIXELS
A
INTERPOLATE
COLOR
VALUE
FROM
NEIGHBORS
NEAREST
NEIGHBOR
BILINEAR
HELP
OBTAIN
A
WIDER
ANGLE
VIEW
BY
COMBINING
MULTIPLE
IMAGES
KRISTEN
GRAUMAN
TWO
IMAGES
WITH
ROTATION
ZOOM
BUT
NO
TRANSLATION
DEREK
HOIEM
CAMERA
CENTER
HOW
TO
STITCH
TOGETHER
A
PANORAMA
A
K
A
MOSAIC
BASIC
PROCEDURE
TAKE
A
SEQUENCE
OF
IMAGES
FROM
THE
SAME
POSITION
ROTATE
THE
CAMERA
ABOUT
ITS
OPTICAL
CENTER
COMPUTE
THE
HOMOGRAPHY
TRANSFORMATION
BETWEEN
SECOND
IMAGE
AND
FIRST
TRANSFORM
THE
SECOND
IMAGE
TO
OVERLAP
WITH
THE
FIRST
BLEND
THE
TWO
TOGETHER
TO
CREATE
A
MOSAIC
IF
THERE
ARE
MORE
IMAGES
REPEAT
MODIFIED
FROM
STEVE
SEITZ
A
PROJECTIVE
TRANSFORM
IS
A
MAPPING
BETWEEN
ANY
TWO
PPS
WITH
THE
SAME
CENTER
OF
PROJECTION
RECTANGLE
SHOULD
MAP
TO
ARBITRARY
QUADRILATERAL
PARALLEL
LINES
AREN
T
BUT
MUST
PRESERVE
STRAIGHT
LINES
CALLED
HOMOGRAPHY
WX
WY
X
Y
W
P
H
P
ALYOSHA
EFROS
XN
YN
XN
YN
TO
COMPUTE
THE
HOMOGRAPHY
GIVEN
PAIRS
OF
CORRESPONDING
POINTS
IN
THE
IMAGES
WE
NEED
TO
SET
UP
AN
EQUATION
WHERE
THE
PARAMETERS
OF
H
ARE
THE
UNKNOWNS
P
HP
WX
A
B
C
X
WY
D
E
F
Y
W
G
H
I
CAN
SET
SCALE
FACTOR
I
SO
THERE
ARE
UNKNOWNS
SET
UP
A
SYSTEM
OF
LINEAR
EQUATIONS
AH
B
WHERE
VECTOR
OF
UNKNOWNS
H
A
B
C
D
E
F
G
H
T
NEED
AT
LEAST
EQS
BUT
THE
MORE
THE
BETTER
SOLVE
FOR
H
IF
OVERCONSTRAINED
SOLVE
USING
LEAST
SQUARES
MIN
AH
B
HELP
LMDIVIDE
X
Y
IMAGE
IMAGE
CANVAS
WX
W
WY
W
X
Y
TO
APPLY
A
GIVEN
HOMOGRAPHY
H
COMPUTE
P
HP
REGULAR
MATRIX
MULTIPLY
WX
WY
X
Y
CONVERT
P
FROM
HOMOGENEOUS
TO
IMAGE
COORDINATES
W
MODIFIED
FROM
KRISTEN
GRAUMAN
P
H
P
HOMOGRAPHY
EXAMPLE
IMAGE
RECTIFICATION
TO
UNWARP
RECTIFY
AN
IMAGE
SOLVE
FOR
HOMOGRAPHY
H
GIVEN
P
AND
P
P
HP
DEREK
HOIEM
STEREO
PHOTOGRAPHY
AND
STEREO
VIEWERS
KRISTEN
GRAUMAN
GOAL
RECOVER
DEPTH
BY
FINDING
IMAGE
COORDINATE
X
THAT
CORRESPONDS
TO
X
X
X
Z
C
BASELINE
C
B
BASELINE
LINE
CONNECTING
THE
TWO
CAMERA
CENTERS
EPIPOLES
INTERSECTIONS
OF
BASELINE
WITH
IMAGE
PLANES
PROJECTIONS
OF
THE
OTHER
CAMERA
CENTER
EPIPOLAR
PLANE
PLANE
CONTAINING
BASELINE
EPIPOLAR
LINES
INTERSECTIONS
OF
EPIPOLAR
PLANE
WITH
IMAGE
PLANES
ALWAYS
COME
IN
CORRESPONDING
PAIRS
NOTE
ALL
EPIPOLAR
LINES
INTERSECT
AT
THE
EPIPOLE
EPIPOLAR
CONSTRAINT
THIS
IS
USEFUL
BECAUSE
IT
REDUCES
THE
CORRESPONDENCE
PROBLEM
TO
A
SEARCH
ALONG
AN
EPIPOLAR
LINE
ESSENTIAL
MATRIX
X
TX
RX
LET
E
T
X
R
X
T
EX
E
IS
CALLED
THE
ESSENTIAL
MATRIX
AND
IT
RELATES
CORRESPONDING
IMAGE
POINTS
BETWEEN
BOTH
CAMERAS
GIVEN
THE
ROTATION
AND
TRANSLATION
IF
WE
OBSERVE
A
POINT
IN
ONE
IMAGE
ITS
POSITION
IN
OTHER
IMAGE
IS
CONSTRAINED
TO
LIE
ON
LINE
DEFINED
BY
ABOVE
EX
IS
THE
EPIPOLAR
LINE
THROUGH
X
IN
THE
FIRST
IMAGE
CORRESPONDING
TO
X
NOTE
THESE
POINTS
ARE
IN
CAMERA
COORDINATE
SYSTEMS
BASIC
STEREO
MATCHING
ALGORITHM
FOR
EACH
PIXEL
IN
THE
FIRST
IMAGE
FIND
CORRESPONDING
EPIPOLAR
SCANLINE
IN
THE
RIGHT
IMAGE
IF
NECESSARY
RECTIFY
THE
TWO
STEREO
IMAGES
TO
TRANSFORM
EPIPOLAR
LINES
INTO
SCANLINES
SEARCH
ALONG
EPIPOLAR
LINE
AND
PICK
THE
BEST
MATCH
X
COMPUTE
DISPARITY
X
X
AND
SET
DEPTH
X
F
T
X
X
CORRESPONDENCE
SEARCH
LEFT
RIGHT
SCANLINE
MATCHING
COST
DISPARITY
SLIDE
A
WINDOW
ALONG
THE
RIGHT
SCANLINE
AND
COMPARE
CONTENTS
OF
THAT
WINDOW
WITH
THE
REFERENCE
WINDOW
IN
THE
LEFT
IMAGE
MATCHING
COST
SSD
OR
NORMALIZED
CORRELATION
IMAGE
I
X
Y
DISPARITY
MAP
D
X
Y
X
Y
X
D
X
Y
Y
IMAGE
I
X
Y
WHAT
ABOUT
WHEN
CAMERAS
OPTICAL
AXES
ARE
NOT
PARALLEL
KRISTEN
GRAUMAN
RESULTS
WITH
WINDOW
SEARCH
DATA
DEREK
HOIEM
WINDOW
BASED
MATCHING
GROUND
TRUTH
PROJECTIVE
STRUCTURE
FROM
MOTION
GIVEN
M
IMAGES
OF
N
FIXED
POINTS
XIJ
PI
XJ
I
M
J
N
PROBLEM
ESTIMATE
M
PROJECTION
MATRICES
PI
AND
N
POINTS
XJ
FROM
THE
MN
CORRESPONDING
POINTS
XIJ
XJ
SVETLANA
LAZEBNIK
PHOTO
SYNTH
CS
INTRO
TO
COMPUTER
VISION
INTRO
TO
MACHINE
LEARNING
VISUAL
RECOGNITION
PART
I
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
OUT
OF
MEDIAN
MEAN
DUE
OCTOBER
DUE
NOVEMBER
UP
TO
POINTS
OF
EXTRA
CREDIT
OVERVIEW
OF
SOME
METHODS
FOR
CLASSIFICATION
CHALLENGES
AND
TRADE
OFFS
FULL
SYSTEM
SPATIAL
PYRAMID
MATCH
SCENE
RECOGNITION
BAG
OF
WORDS
FEATURE
VECTOR
DESCRIPTOR
REPRESENTATION
RECOGNITION
OFTEN
INVOLVES
CLASSIFICATION
CLASSES
CATEGORIES
HENCE
CLASSIFICATION
CATEGORIZATION
TRAINING
LEARNING
A
MODEL
E
G
CLASSIFIER
HAPPENS
AT
TRAINING
TIME
FROM
TRAINING
DATA
CLASSIFICATION
PREDICTION
HAPPENS
AT
TEST
TIME
MACHINE
LEARNING
PROBLEMS
SUPERVISED
LEARNING
UNSUPERVISED
LEARNI
NG
Q
CJ
CL
Q
C
C
CLASSIFICATION
GIVEN
A
FEATURE
REPRESENTATION
FOR
IMAGES
HOW
DO
WE
LEARN
A
MODEL
FOR
DISTINGUISHING
FEATURES
FROM
DIFFERENT
CLASSES
DECISION
BOUNDARY
ZEBRA
NON
ZEBRA
CLASSIFICATION
ASSIGN
INPUT
VECTOR
TO
ONE
OF
TWO
OR
MORE
CLASSES
ANY
DECISION
RULE
DIVIDES
THE
INPUT
SPACE
INTO
DECISION
REGIONS
SEPARATED
BY
DECISION
BOUNDARIES
EXAMPLE
SPAM
FILTER
EXAMPLES
OF
CATEGORIZATION
IN
VISION
PART
OR
OBJECT
DETECTION
E
G
FOR
EACH
WINDOW
FACE
OR
NON
FACE
SCENE
CATEGORIZATION
INDOOR
VS
OUTDOOR
URBAN
FOREST
KITCHEN
ETC
ACTION
RECOGNITION
PICKING
UP
VS
SITTING
DOWN
VS
STANDING
EMOTION
RECOGNITION
REGION
CLASSIFICATION
LABEL
PIXELS
INTO
DIFFERENT
OBJECT
SURFACE
CATEGORIES
BOUNDARY
CLASSIFICATION
BOUNDARY
VS
NON
BOUNDARY
ETC
ETC
WHAT
DO
YOU
SEE
IN
THIS
IMAGE
TREES
BEAR
CAMERA
MAN
RABBIT
GRASS
FOREST
DESCRIBE
PREDICT
OR
INTERACT
WITH
THE
OBJECT
BASED
ON
VISUAL
CUES
IS
IT
DANGEROUS
HOW
FAST
DOES
IT
RUN
IS
IT
ALIVE
IS
IT
SOFT
DOES
IT
HAVE
A
TAIL
CAN
I
POKE
WITH
IT
SLIDE
CREDIT
D
HOIEM
CAT
VS
DOG
OBJECT
RECOGNITION
CALTECH
AVERAGE
OBJECT
IMAGES
FINE
GRAINED
RECOGNITION
PLACE
RECOGNITION
PLACES
DATABASE
DATING
HISTORICAL
PHOTOS
IMAGE
STYLE
RECOGNITION
SLIDE
CREDIT
D
HOIEM
LAYOUT
PREDICTION
ASSIGN
REGIONS
TO
ORIENTATION
GEOMETRIC
CONTEXT
ASSIGN
REGIONS
TO
DEPTH
MATERIAL
RECOGNITION
A
FARHADI
I
ENDRES
D
HOIEM
AND
D
FORSYTH
DESCRIBING
OBJECTS
BY
THEIR
ATTRIBUTES
CVPR
KOVASHKA
VIJAYANARASIMHAN
AND
K
GRAUMAN
ACTIVELY
SELECTING
ANNOTATIONS
AMONG
OBJECTS
AND
ATTRIBUTES
ICCV
KOVASHKA
D
PARIKH
AND
K
GRAUMAN
WHITTLESEARCH
IMAGE
SEARCH
WITH
RELATIVE
ATTRIBUTE
FEEDBACK
CVPR
GENERIC
CATEGORIZATION
PROBLEM
INSTANCE
LEVEL
RECOGNITION
PROBLEM
JOHN
CAR
VISUAL
OBJECT
CATEGORIES
BASIC
LEVEL
CATEGORIES
IN
HUMAN
CATEGORIZATION
ROSCH
LAKOFF
THE
HIGHEST
LEVEL
AT
WHICH
CATEGORY
MEMBERS
HAVE
SIMILAR
PERCEIVED
SHAPE
THE
HIGHEST
LEVEL
AT
WHICH
A
SINGLE
MENTAL
IMAGE
REFLECTS
THE
ENTIRE
CATEGORY
THE
LEVEL
AT
WHICH
HUMAN
SUBJECTS
ARE
USUALLY
FASTEST
AT
IDENTIFYING
CATEGORY
MEMBERS
THE
FIRST
LEVEL
NAMED
AND
UNDERSTOOD
BY
CHILDREN
THE
HIGHEST
LEVEL
AT
WHICH
A
PERSON
USES
SIMILAR
MOTOR
ACTIONS
FOR
INTERACTION
WITH
CATEGORY
MEMBERS
VISUAL
OBJECT
CATEGORIES
BASIC
LEVEL
CATEGORIES
IN
HUMANS
SEEM
TO
BE
DEFINED
PREDOMINANTLY
VISUALLY
THERE
IS
EVIDENCE
THAT
HUMANS
USUALLY
START
WITH
BASIC
LEVEL
CATEGORIZATION
BEFORE
DOING
IDENTIFICATION
BASIC
LEVEL
CATEGORIZATION
IS
EASIER
AND
FASTER
FOR
HUMANS
THAN
OBJECT
IDENTIFICATION
HOW
DOES
THIS
TRANSFER
TO
AUTOMATIC
ABSTRACT
LEVELS
ANIMAL
QUADRUPED
CLASSIFICATION
ALGORITHMS
BASIC
LEVEL
DOG
CAT
COW
GERMAN
SHEPHERD
DOBERMAN
INDIVIDUAL
LEVEL
FIDO
OBJECT
CATEGORIZATION
TASK
DESCRIPTION
GIVEN
A
SMALL
NUMBER
OF
TRAINING
IMAGES
OF
A
CATEGORY
RECOGNIZE
A
PRIORI
UNKNOWN
INSTANCES
OF
THAT
CATEGORY
AND
ASSIGN
THE
CORRECT
CATEGORY
LABEL
WHICH
CATEGORIES
ARE
FEASIBLE
VISUALLY
FIDO
GERMAN
SHEPHERD
DOG
ANIMAL
LIVING
BEING
HOW
MANY
OBJECT
CATEGORIES
ARE
THERE
SOURCE
FEI
FEI
LI
ROB
FERGUS
ANTONIO
TORRALBA
BIEDERMAN
OTHER
TYPES
OF
CATEGORIES
FUNCTIONAL
CATEGORIES
E
G
CHAIRS
SOMETHING
YOU
CAN
SIT
ON
OTHER
TYPES
OF
CATEGORIES
AD
HOC
CATEGORIES
E
G
SOMETHING
YOU
CAN
FIND
IN
AN
OFFICE
ENVIRONMENT
PROTOTYPE
OR
SUM
OF
EXEMPLARS
PROTOTYPE
MODEL
EXEMPLARS
MODEL
CATEGORY
JUDGMENTS
ARE
MADE
BY
COMPARING
A
NEW
EXEMPLAR
TO
THE
PROTOTYPE
CATEGORY
JUDGMENTS
ARE
MADE
BY
COMPARING
A
NEW
EXEMPLAR
TO
ALL
THE
OLD
EXEMPLARS
OF
A
CATEGORY
OR
TO
THE
EXEMPLAR
THAT
IS
THE
MOST
APPROPRIATE
SLIDE
CREDIT
TORRALBA
WHY
RECOGNITION
RECOGNITION
A
FUNDAMENTAL
PART
OF
PERCEPTION
E
G
ROBOTS
AUTONOMOUS
AGENTS
ORGANIZE
AND
GIVE
ACCESS
TO
VISUAL
CONTENT
CONNECT
TO
INFORMATION
DETECT
TRENDS
AND
THEMES
SLIDE
CREDIT
K
GRAUMAN
RECOGNITION
A
MACHINE
LEARNING
APPROACH
APPLY
A
PREDICTION
FUNCTION
TO
A
FEATURE
REPRESENTATION
OF
THE
IMAGE
TO
GET
THE
DESIRED
OUTPUT
F
APPLE
F
TOMATO
F
COW
Y
F
X
OUTPUT
PREDICTION
FUNCTION
IMAGE
FEATURE
TRAINING
GIVEN
A
TRAINING
SET
OF
LABELED
EXAMPLES
XN
YN
ESTIMATE
THE
PREDICTION
FUNCTION
F
BY
MINIMIZING
THE
PREDICTION
ERROR
ON
THE
TRAINING
SET
TESTING
APPLY
F
TO
A
NEVER
BEFORE
SEEN
TEST
EXAMPLE
X
AND
OUTPUT
THE
PREDICTED
VALUE
Y
F
X
TRAINING
STEPS
TESTING
TEST
IMAGE
SLIDE
CREDIT
D
HOIEM
AND
L
LAZEBNIK
POPULAR
GLOBAL
IMAGE
FEATURES
RAW
PIXELS
AND
SIMPLE
FUNCTIONS
OF
RAW
PIXELS
OLIVA
AND
TORRALBA
HISTOGRAMS
BAGS
OF
FEATURES
HOG
DALAL
AND
TRIGGS
SLIDE
CREDIT
L
LAZEBNIK
RECOGNIZING
A
BEACH
RECOGNIZING
CLOTH
FABRIC
RECOGNIZING
A
MUG
FINE
GRAINED
RECOGNITION
WHAT
BREED
IS
THIS
DOG
WHAT
ARE
THE
RIGHT
FEATURES
DEPEND
ON
WHAT
YOU
WANT
TO
KNOW
OBJECT
SHAPE
LOCAL
SHAPE
INFO
SHADING
SHADOWS
TEXTURE
SCENE
GEOMETRIC
LAYOUT
LINEAR
PERSPECTIVE
GRADIENTS
LINE
SEGMENTS
MATERIAL
PROPERTIES
ALBEDO
FEEL
HARDNESS
COLOR
TEXTURE
ACTION
MOTION
OPTICAL
FLOW
TRACKED
POINTS
COLOR
L
A
B
COLOR
SPACE
HSV
COLOR
SPACE
TEXTURE
FILTER
BANKS
OR
HOG
OVER
REGIONS
HISTOGRAMS
OF
DESCRIPTORS
SIFT
LOWE
IJCV
BAG
OF
VISUAL
WORDS
BAG
OF
VISUAL
WORDS
IMAGE
PATCHES
CLUSTER
PATCHES
GET
CODEWORDS
BOW
HISTOGRAM
CODEWORDS
TRAINING
STEPS
TESTING
TEST
IMAGE
SLIDE
CREDIT
D
HOIEM
AND
L
LAZEBNIK
RECOGNITION
TRAINING
DATA
IMAGES
IN
THE
TRAINING
SET
MUST
BE
ANNOTATED
WITH
THE
CORRECT
ANSWER
THAT
THE
MODEL
IS
EXPECTED
TO
PRODUCE
MOTORBIKE
SLIDE
CREDIT
L
LAZEBNIK
DATASETS
TODAY
IMAGENET
CATEGORIES
IMAGES
MICROSOFT
COCO
CATEGORIES
IMAGES
PASCAL
CATEGORIES
IMAGES
SUN
CATEGORIES
IMAGES
THE
PASCAL
VISUAL
OBJECT
CLASSES
CHALLENGE
CHALLENGE
CLASSES
PERSON
PERSON
ANIMAL
BIRD
CAT
COW
DOG
HORSE
SHEEP
VEHICLE
AEROPLANE
BICYCLE
BOAT
BUS
CAR
MOTORBIKE
TRAIN
INDOOR
BOTTLE
CHAIR
DINING
TABLE
POTTED
PLANT
SOFA
TV
MONITOR
DATASET
SIZE
BY
TRAINING
VALIDATION
IMAGES
BOUNDING
BOXES
SEGMENTATIONS
CLASSIFICATION
FOR
EACH
OF
THE
TWENTY
CLASSES
PREDICTING
PRESENCE
ABSENCE
OF
AN
EXAMPLE
OF
THAT
CLASS
IN
THE
TEST
IMAGE
DETECTION
PREDICTING
THE
BOUNDING
BOX
AND
LABEL
OF
EACH
OBJECT
FROM
THE
TWENTY
TARGET
CLASSES
IN
THE
TEST
IMAGE
SEGMENTATION
GENERATING
PIXEL
WISE
SEGMENTATIONS
GIVING
THE
CLASS
OF
THE
OBJECT
VISIBLE
AT
EACH
PIXEL
OR
BACKGROUND
OTHERWISE
PERSON
LAYOUT
PREDICTING
THE
BOUNDING
BOX
AND
LABEL
OF
EACH
PART
OF
A
PERSON
HEAD
HANDS
FEET
SLIDE
CREDIT
L
LAZEBNIK
ILLUMINATION
OBJECT
POSE
CLUTTER
OCCLUSIONS
INTRA
CLASS
APPEARANCE
VIEWPOINT
REALISTIC
SCENES
ARE
CROWDED
CLUTTERED
HAVE
OVERLAPPING
OBJECTS
CHALLENGES
COMPLEXITY
THOUSANDS
TO
MILLIONS
OF
PIXELS
IN
AN
IMAGE
HUMAN
RECOGNIZABLE
OBJECT
CATEGORIES
DEGREES
OF
FREEDOM
IN
THE
POSE
OF
ARTICULATED
OBJECTS
HUMANS
BILLIONS
OF
IMAGES
INDEXED
BY
GOOGLE
IMAGE
SEARCH
BILLION
PRINTS
PRODUCED
FROM
DIGITAL
CAMERA
IMAGES
IN
MILLION
CAMERA
PHONES
SOLD
IN
ABOUT
HALF
OF
THE
CEREBRAL
CORTEX
IN
PRIMATES
IS
DEVOTED
TO
PROCESSING
VISUAL
INFORMATION
FELLEMAN
AND
VAN
ESSEN
CHALLENGES
LEARNING
WITH
MINIMAL
SUPERVISION
LESS
CS
INTRO
TO
COMPUTER
VISION
INTRO
TO
MACHINE
LEARNING
VISUAL
RECOGNITION
PART
II
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
HOMEWORK
PUSH
BACK
TO
NOV
PUSH
BACK
TO
NOV
MAKE
HALF
LENGTH
STILL
OF
OVERALL
GRADE
OUT
NOV
DUE
DEC
TAKE
OUT
A
SMALL
PIECE
OF
PAPER
AND
VOTE
YES
NO
ON
THIS
PROPOSAL
OTHER
ANNOUNCEMENTS
PIAZZA
CAN
GET
PARTICIPATION
CREDIT
BY
ASKING
ANSWERING
FEEDBACK
FROM
SURVEYS
PLAN
FOR
TODAY
OVERVIEW
OF
SOME
METHODS
FOR
CLASSIFICATION
CHALLENGES
AND
TRADE
OFFS
SOME
TRANSLATIONS
FEATURE
VECTOR
DESCRIPTOR
REPRESENTATION
RECOGNITION
OFTEN
INVOLVES
CLASSIFICATION
CLASSES
CATEGORIES
HENCE
CLASSIFICATION
CATEGORIZATION
TRAINING
LEARNING
A
MODEL
E
G
CLASSIFIER
HAPPENS
AT
TRAINING
TIME
FROM
TRAINING
DATA
CLASSIFICATION
PREDICTION
HAPPENS
AT
TEST
TIME
CLASSIFICATION
GIVEN
A
FEATURE
REPRESENTATION
FOR
IMAGES
LEARN
A
MODEL
FOR
DISTINGUISHING
FEATURES
FROM
DIFFERENT
CLASSES
DECISION
BOUNDARY
ZEBRA
NON
ZEBRA
SLIDE
CREDIT
L
LAZEBNIK
IMAGE
CATEGORIZATION
CAT
VS
DOG
IMAGE
CATEGORIZATION
OBJECT
RECOGNITION
CALTECH
AVERAGE
OBJECT
IMAGES
IMAGE
CATEGORIZATION
PLACE
RECOGNITION
PLACES
DATABASE
REGION
CATEGORIZATION
MATERIAL
RECOGNITION
RECOGNITION
A
MACHINE
LEARNING
APPROACH
THE
MACHINE
LEARNING
FRAMEWORK
APPLY
A
PREDICTION
FUNCTION
TO
A
FEATURE
REPRESENTATION
OF
THE
IMAGE
TO
GET
THE
DESIRED
OUTPUT
F
APPLE
F
TOMATO
F
COW
THE
MACHINE
LEARNING
FRAMEWORK
Y
F
X
OUTPUT
PREDICTION
FUNCTION
IMAGE
FEATURE
TRAINING
GIVEN
A
TRAINING
SET
OF
LABELED
EXAMPLES
XN
YN
ESTIMATE
THE
PREDICTION
FUNCTION
F
BY
MINIMIZING
THE
PREDICTION
ERROR
ON
THE
TRAINING
SET
TESTING
APPLY
F
TO
A
NEVER
BEFORE
SEEN
TEST
EXAMPLE
X
AND
OUTPUT
THE
PREDICTED
VALUE
Y
F
X
TRAINING
STEPS
BOARD
TESTING
TEST
IMAGE
SLIDE
CREDIT
D
HOIEM
AND
L
LAZEBNIK
POPULAR
GLOBAL
IMAGE
FEATURES
RAW
PIXELS
AND
SIMPLE
FUNCTIONS
OF
RAW
PIXELS
OLIVA
AND
TORRALBA
HISTOGRAMS
BAGS
OF
FEATURES
HOG
DALAL
AND
TRIGGS
SLIDE
CREDIT
L
LAZEBNIK
WHAT
KIND
OF
THINGS
DO
WE
COMPUTE
HISTOGRAMS
OF
COLOR
L
A
B
COLOR
SPACE
HSV
COLOR
SPACE
TEXTURE
FILTER
BANKS
OR
HOG
OVER
REGIONS
WHAT
KIND
OF
THINGS
DO
WE
COMPUTE
HISTOGRAMS
OF
HISTOGRAMS
OF
DESCRIPTORS
SIFT
LOWE
IJCV
BAG
OF
VISUAL
WORDS
TRAINING
STEPS
TESTING
TEST
IMAGE
SLIDE
CREDIT
D
HOIEM
AND
L
LAZEBNIK
RECOGNITION
TRAINING
DATA
IMAGES
IN
THE
TRAINING
SET
MUST
BE
ANNOTATED
WITH
THE
CORRECT
ANSWER
THAT
THE
MODEL
IS
EXPECTED
TO
PRODUCE
MOTORBIKE
CHALLENGES
ROBUSTNESS
ILLUMINATION
OBJECT
POSE
CLUTTER
OCCLUSIONS
INTRA
CLASS
APPEARANCE
VIEWPOINT
CHALLENGES
IMPORTANCE
OF
CONTEXT
PAINTER
IDENTIFICATION
HOW
WOULD
YOU
LEARN
TO
IDENTIFY
THE
AUTHOR
OF
A
PAINTING
GOYA
KIRCHNER
KLIMT
MARC
MONET
VAN
GOGH
PLAN
FOR
TODAY
VISUAL
RECOGNITION
PROBLEMS
RECOGNITION
PIPELINE
FEATURES
AND
DATA
CHALLENGES
ONE
WAY
TO
THINK
ABOUT
IT
TRAINING
LABELS
DICTATE
THAT
TWO
EXAMPLES
ARE
THE
SAME
OR
DIFFERENT
IN
SOME
SENSE
FEATURES
AND
DISTANCES
DEFINE
VISUAL
SIMILARITY
GOAL
OF
TRAINING
IS
TO
LEARN
FEATURE
WEIGHTS
SO
THAT
VISUAL
SIMILARITY
PREDICTS
LABEL
SIMILARITY
LINEAR
CLASSIFIER
CONFIDENCE
IN
POSITIVE
LABEL
IS
A
WEIGHTED
SUM
OF
FEATURES
WHAT
ARE
THE
WEIGHTS
BOARD
WE
WANT
THE
SIMPLEST
FUNCTION
THAT
IS
CONFIDENTLY
CORRECT
SUPERVISED
CLASSIFICATION
GIVEN
A
COLLECTION
OF
LABELED
EXAMPLES
COME
UP
WITH
A
FUNCTION
THAT
WILL
PREDICT
THE
LABELS
OF
NEW
EXAMPLES
FOUR
NINE
TRAINING
EXAMPLES
NOVEL
INPUT
HOW
GOOD
IS
SOME
FUNCTION
THAT
WE
COME
UP
WITH
TO
DO
THE
CLASSIFICATION
DEPENDS
ON
MISTAKES
MADE
COST
ASSOCIATED
WITH
THE
MISTAKES
SUPERVISED
CLASSIFICATION
GIVEN
A
COLLECTION
OF
LABELED
EXAMPLES
COME
UP
WITH
A
FUNCTION
THAT
WILL
PREDICT
THE
LABELS
OF
NEW
EXAMPLES
CONSIDER
THE
TWO
CLASS
BINARY
DECISION
PROBLEM
L
LOSS
OF
CLASSIFYING
A
AS
A
L
LOSS
OF
CLASSIFYING
A
AS
A
RISK
OF
A
CLASSIFIER
IS
EXPECTED
LOSS
R
PR
USING
L
PR
USING
L
WE
WANT
TO
CHOOSE
A
CLASSIFIER
SO
AS
TO
MINIMIZE
THIS
TOTAL
RISK
SUPERVISED
CLASSIFICATION
OPTIMAL
CLASSIFIER
WILL
MINIMIZE
TOTAL
RISK
FEATURE
VALUE
X
AT
DECISION
BOUNDARY
EITHER
CHOICE
OF
LABEL
YIELDS
SAME
EXPECTED
LOSS
IF
WE
CHOOSE
CLASS
FOUR
AT
BOUNDARY
EXPECTED
LOSS
IS
P
CLASS
IS
X
L
IF
WE
CHOOSE
CLASS
NINE
AT
BOUNDARY
EXPECTED
LOSS
IS
P
CLASS
IS
X
L
SO
BEST
DECISION
BOUNDARY
IS
AT
POINT
X
WHERE
P
CLASS
IS
X
L
P
CLASS
IS
X
L
SUPERVISED
CLASSIFICATION
OPTIMAL
CLASSIFIER
WILL
MINIMIZE
TOTAL
RISK
FEATURE
VALUE
X
AT
DECISION
BOUNDARY
EITHER
CHOICE
OF
LABEL
YIELDS
SAME
EXPECTED
LOSS
TO
CLASSIFY
A
NEW
POINT
CHOOSE
CLASS
WITH
LOWEST
EXPECTED
LOSS
I
E
CHOOSE
FOUR
IF
P
X
L
P
X
L
LOSS
FOR
CHOOSING
FOUR
LOSS
FOR
CHOOSING
NINE
DISCLAIMERS
WE
WILL
OFTEN
ASSUME
THE
SAME
LOSS
FOR
ALL
POSSIBLE
TYPES
OF
MISCLASSIFICATIONS
WE
WON
T
ALWAYS
BUILD
PROBABILITY
DISTRIBUTIONS
OFTEN
WE
LL
JUST
FIND
A
DECISION
BOUNDARY
USING
DISCRIMINATIVE
METHODS
WHAT
THE
SIMPLEST
CLASSIFIER
YOU
CAN
THINK
OF
NEAREST
NEIGHBOR
CLASSIFIER
TRAINING
EXAMPLES
FROM
CLASS
TEST
EXAMPLE
TRAINING
EXAMPLES
FROM
CLASS
F
X
LABEL
OF
THE
TRAINING
EXAMPLE
NEAREST
TO
X
ALL
WE
NEED
IS
A
DISTANCE
FUNCTION
FOR
OUR
INPUTS
NO
TRAINING
REQUIRED
K
NEAREST
NEIGHBORS
CLASSIFICATION
FOR
A
NEW
POINT
FIND
THE
K
CLOSEST
POINTS
FROM
TRAINING
DATA
LABELS
OF
THE
K
POINTS
VOTE
TO
CLASSIFY
BLACK
NEGATIVE
RED
POSITIVE
K
IF
QUERY
LANDS
HERE
THE
NN
CONSIST
OF
NEGATIVES
AND
POSITIVES
SO
WE
CLASSIFY
IT
AS
NEGATIVE
NEAREST
NEIGHBOR
NEAREST
NEIGHBOR
NEAREST
NEIGHBOR
WHAT
ARE
THE
TRADEOFFS
OF
HAVING
A
TOO
LARGE
K
TOO
SMALL
K
A
NEAREST
NEIGHBOR
RECOGNITION
EXAMPLE
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
JAMES
HAYS
AND
ALEXEI
EFROS
CVPR
WHERE
IN
THE
WORLD
WHERE
IN
THE
WORLD
WHERE
IN
THE
WORLD
HOW
MUCH
CAN
AN
IMAGE
TELL
ABOUT
ITS
GEOGRAPHIC
LOCATION
NEAREST
NEIGHBORS
ACCORDING
TO
GIST
BAG
OF
SIFT
COLOR
HISTOGRAM
A
FEW
OTHERS
MILLION
GEOTAGGED
PHOTOS
BY
PHOTOGRAPHERS
SLIDES
JAMES
HAYS
SPATIAL
ENVELOPE
THEORY
OF
SCENE
REPRESENTATION
OLIVA
TORRALBA
SLIDE
CREDIT
AUDE
OLIVIA
SCENE
MATCHES
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
SLIDES
JAMES
HAYS
SLIDES
JAMES
HAYS
SCENE
MATCHES
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
SLIDES
JAMES
HAYS
EOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
C
SCENE
MATCHES
THE
IMPORTANCE
OF
DATA
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
SLIDES
JAMES
HAYS
NEAREST
NEIGHBOR
CLASSIFIER
TRAINING
EXAMPLES
FROM
CLASS
TEST
EXAMPLE
TRAINING
EXAMPLES
FROM
CLASS
F
X
LABEL
OF
THE
TRAINING
EXAMPLE
NEAREST
TO
X
ALL
WE
NEED
IS
A
DISTANCE
FUNCTION
FOR
OUR
INPUTS
NO
TRAINING
REQUIRED
SLIDE
CREDIT
L
LAZEBNIK
EVALUATING
CLASSIFIERS
ACCURACY
CORRECTLY
CLASSIFIED
ALL
TEST
EXAMPLES
PRECISION
RECALL
PRECISION
RETRIEVED
POSITIVES
RETRIEVED
RECALL
RETRIEVED
POSITIVES
POSITIVES
F
MEASURE
P
R
DISCRIMINATIVE
CLASSIFIERS
LEARN
A
SIMPLE
FUNCTION
OF
THE
INPUT
FEATURES
THAT
CORRECTLY
PREDICTS
THE
TRUE
LABELS
ON
THE
TRAINING
SET
𝑦
𝑓
TRAINING
GOALS
ACCURATE
CLASSIFICATION
OF
TRAINING
DATA
CORRECT
CLASSIFICATIONS
ARE
CONFIDENT
CLASSIFICATION
FUNCTION
IS
SIMPLE
SLIDE
CREDIT
D
HOIEM
LINEAR
CLASSIFIER
WHAT
ABOUT
THIS
LINE
FIND
A
LINEAR
FUNCTION
TO
SEPARATE
THE
CLASSES
F
X
SGN
WDXD
SGN
W
X
NN
VS
LINEAR
CLASSIFIERS
NN
PROS
SIMPLE
TO
IMPLEMENT
DECISION
BOUNDARIES
NOT
NECESSARILY
LINEAR
WORKS
FOR
ANY
NUMBER
OF
CLASSES
NONPARAMETRIC
METHOD
NN
CONS
NEED
GOOD
DISTANCE
FUNCTION
SLOW
AT
TEST
TIME
LARGE
SEARCH
PROBLEM
TO
FIND
NEIGHBORS
STORAGE
OF
DATA
LINEAR
PROS
LOW
DIMENSIONAL
PARAMETRIC
REPRESENTATION
VERY
FAST
AT
TEST
TIME
LINEAR
CONS
WORKS
FOR
TWO
CLASSES
HOW
TO
TRAIN
THE
LINEAR
FUNCTION
WHAT
IF
DATA
IS
NOT
LINEARLY
SEPARABLE
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
OCTOBER
HOMEWORK
DUE
NOV
NOTE
YOU
NEED
TO
PERFORM
K
MEANS
ON
FEATURES
FROM
MULTIPLE
FRAMES
JOINED
TOGETHER
NOT
ONE
K
MEANS
PER
FRAME
DUE
NOV
HALF
LENGTH
STILL
OF
GRADE
OUT
NOV
DUE
DEC
PLAN
FOR
TODAY
SUPPORT
VECTOR
MACHINES
BIAS
VARIANCE
TRADEOFF
SCENE
RECOGNITION
SPATIAL
PYRAMID
MATCHING
THE
MACHINE
LEARNING
FRAMEWORK
Y
F
X
OUTPUT
PREDICTION
FUNCTION
IMAGE
FEATURE
TRAINING
GIVEN
A
TRAINING
SET
OF
LABELED
EXAMPLES
XN
YN
ESTIMATE
THE
PREDICTION
FUNCTION
F
BY
MINIMIZING
THE
PREDICTION
ERROR
ON
THE
TRAINING
SET
TESTING
APPLY
F
TO
A
NEVER
BEFORE
SEEN
TEST
EXAMPLE
X
AND
OUTPUT
THE
PREDICTED
VALUE
Y
F
X
NEAREST
NEIGHBOR
CLASSIFIER
TRAINING
EXAMPLES
FROM
CLASS
TEST
EXAMPLE
TRAINING
EXAMPLES
FROM
CLASS
F
X
LABEL
OF
THE
TRAINING
EXAMPLE
NEAREST
TO
X
ALL
WE
NEED
IS
A
DISTANCE
FUNCTION
FOR
OUR
INPUTS
NO
TRAINING
REQUIRED
K
NEAREST
NEIGHBORS
CLASSIFICATION
FOR
A
NEW
POINT
FIND
THE
K
CLOSEST
POINTS
FROM
TRAINING
DATA
LABELS
OF
THE
K
POINTS
VOTE
TO
CLASSIFY
BLACK
NEGATIVE
RED
POSITIVE
K
IF
QUERY
LANDS
HERE
THE
NN
CONSIST
OF
NEGATIVES
AND
POSITIVES
SO
WE
CLASSIFY
IT
AS
NEGATIVE
SCENE
MATCHES
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
SLIDES
JAMES
HAYS
NEAREST
NEIGHBORS
PROS
AND
CONS
NN
PROS
SIMPLE
TO
IMPLEMENT
DECISION
BOUNDARIES
NOT
NECESSARILY
LINEAR
WORKS
FOR
ANY
NUMBER
OF
CLASSES
NONPARAMETRIC
METHOD
NN
CONS
NEED
GOOD
DISTANCE
FUNCTION
SLOW
AT
TEST
TIME
LARGE
SEARCH
PROBLEM
TO
FIND
NEIGHBORS
STORAGE
OF
DATA
ADAPTED
FROM
L
LAZEBNIK
DISCRIMINATIVE
CLASSIFIERS
LEARN
A
SIMPLE
FUNCTION
OF
THE
INPUT
FEATURES
THAT
CORRECTLY
PREDICTS
THE
TRUE
LABELS
ON
THE
TRAINING
SET
𝑦
𝑓
TRAINING
GOALS
ACCURATE
CLASSIFICATION
OF
TRAINING
DATA
CORRECT
CLASSIFICATIONS
ARE
CONFIDENT
CLASSIFICATION
FUNCTION
IS
SIMPLE
SLIDE
CREDIT
D
HOIEM
LINEAR
CLASSIFIER
FIND
A
LINEAR
FUNCTION
TO
SEPARATE
THE
CLASSES
F
X
SGN
WDXD
SGN
W
X
SVETLANA
LAZEBNIK
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
D
B
DISTANCE
FROM
POINT
TO
LINE
LINES
IN
LET
A
W
C
X
Y
AX
CY
B
W
X
B
B
W
X
B
DISTANCE
FROM
D
W
POINT
TO
LINE
LINEAR
CLASSIFIERS
FIND
LINEAR
FUNCTION
TO
SEPARATE
POSITIVE
AND
NEGATIVE
EXAMPLES
XI
POSITIVE
XI
NEGATIVE
W
B
XI
W
B
WHICH
LINE
IS
BEST
SUPPORT
VECTOR
MACHINES
DISCRIMINATIVE
CLASSIFIER
BASED
ON
OPTIMAL
SEPARATING
LINE
FOR
CASE
MAXIMIZE
THE
MARGIN
BETWEEN
THE
POSITIVE
AND
NEGATIVE
TRAINING
EXAMPLES
SUPPORT
VECTOR
MACHINES
WANT
LINE
THAT
MAXIMIZES
THE
MARGIN
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
SUPPORT
VECTORS
MARGIN
FOR
SUPPORT
VECTORS
XI
W
B
SUPPORT
VECTOR
MACHINES
WANT
LINE
THAT
MAXIMIZES
THE
MARGIN
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
AND
LINE
FOR
SUPPORT
VECTORS
XI
W
B
W
SUPPORT
VECTORS
MARGIN
WΤ
X
B
M
SUPPORT
VECTOR
MACHINES
WANT
LINE
THAT
MAXIMIZES
THE
MARGIN
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
AND
LINE
XI
W
B
W
SUPPORT
VECTORS
MARGIN
THEREFORE
THE
MARGIN
IS
W
FINDING
THE
MAXIMUM
MARGIN
LINE
MAXIMIZE
MARGIN
W
CORRECTLY
CLASSIFY
ALL
TRAINING
DATA
POINTS
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
QUADRATIC
OPTIMIZATION
PROBLEM
ONE
CONSTRAINT
FOR
EACH
TRAINING
POINT
NOTE
SIGN
TRICK
FINDING
THE
MAXIMUM
MARGIN
LINE
SOLUTION
W
I
I
YI
XI
FINDING
THE
MAXIMUM
MARGIN
LINE
SOLUTION
W
I
I
YI
XI
B
YI
W
XI
FOR
ANY
SUPPORT
VECTOR
CLASSIFICATION
FUNCTION
F
X
SIGN
W
X
B
SIGN
I
I
YI
XI
X
B
IF
F
X
CLASSIFY
AS
NEGATIVE
OTHERWISE
CLASSIFY
AS
POSITIVE
NOTICE
THAT
IT
RELIES
ON
AN
INNER
PRODUCT
BETWEEN
THE
TEST
POINT
X
AND
THE
SUPPORT
VECTORS
XI
SOLVING
THE
OPTIMIZATION
PROBLEM
ALSO
INVOLVES
COMPUTING
THE
INNER
PRODUCTS
XI
XJ
BETWEEN
ALL
PAIRS
OF
TRAINING
POINTS
NONLINEAR
SVMS
DATASETS
THAT
ARE
LINEARLY
SEPARABLE
WORK
OUT
GREAT
X
BUT
WHAT
IF
THE
DATASET
IS
JUST
TOO
HARD
X
WE
CAN
MAP
IT
TO
A
HIGHER
DIMENSIONAL
SPACE
NONLINEAR
SVMS
GENERAL
IDEA
THE
ORIGINAL
INPUT
SPACE
CAN
ALWAYS
BE
MAPPED
TO
SOME
HIGHER
DIMENSIONAL
FEATURE
SPACE
WHERE
THE
TRAINING
SET
IS
SEPARABLE
NONLINEAR
KERNEL
EXAMPLE
CONSIDER
THE
MAPPING
X
X
X
Y
X
Y
XY
K
X
Y
XY
THE
KERNEL
TRICK
THE
LINEAR
CLASSIFIER
RELIES
ON
DOT
PRODUCT
BETWEEN
VECTORS
K
XI
XJ
XI
XJ
IF
EVERY
DATA
POINT
IS
MAPPED
INTO
HIGH
DIMENSIONAL
SPACE
VIA
SOME
TRANSFORMATION
Φ
XI
Φ
XI
THE
DOT
PRODUCT
BECOMES
K
XI
XJ
Φ
XI
Φ
XJ
A
KERNEL
FUNCTION
IS
SIMILARITY
FUNCTION
THAT
CORRESPONDS
TO
AN
INNER
PRODUCT
IN
SOME
EXPANDED
FEATURE
SPACE
THE
KERNEL
TRICK
INSTEAD
OF
EXPLICITLY
COMPUTING
THE
LIFTING
TRANSFORMATION
Φ
X
DEFINE
A
KERNEL
FUNCTION
K
SUCH
THAT
K
XI
XJ
Φ
XI
Φ
XJ
EXAMPLES
OF
KERNEL
FUNCTIONS
LINEAR
K
XI
X
J
I
J
GAUSSIAN
RBF
K
XI
X
J
EXP
HISTOGRAM
INTERSECTION
K
XI
X
J
MIN
K
XI
K
X
J
K
ALLOWING
MISCLASSIFICATIONS
MISCLASSIFICATION
COST
DATA
SAMPLES
SLACK
VARIABLE
THE
W
THAT
MINIMIZES
MAXIMIZE
MARGIN
MINIMIZE
MISCLASSIFICATION
WHAT
ABOUT
MULTI
CLASS
SVMS
UNFORTUNATELY
THERE
IS
NO
DEFINITIVE
MULTI
CLASS
SVM
FORMULATION
IN
PRACTICE
WE
HAVE
TO
OBTAIN
A
MULTI
CLASS
SVM
BY
COMBINING
MULTIPLE
TWO
CLASS
SVMS
ONE
VS
OTHERS
TRAINING
LEARN
AN
SVM
FOR
EACH
CLASS
VS
THE
OTHERS
TESTING
APPLY
EACH
SVM
TO
THE
TEST
EXAMPLE
AND
ASSIGN
IT
TO
THE
CLASS
OF
THE
SVM
THAT
RETURNS
THE
HIGHEST
DECISION
VALUE
ONE
VS
ONE
TRAINING
LEARN
AN
SVM
FOR
EACH
PAIR
OF
CLASSES
TESTING
EACH
LEARNED
SVM
VOTES
FOR
A
CLASS
TO
ASSIGN
TO
THE
TEST
EXAMPLE
SVMS
FOR
RECOGNITION
DEFINE
YOUR
REPRESENTATION
FOR
EACH
EXAMPLE
SELECT
A
KERNEL
FUNCTION
COMPUTE
PAIRWISE
KERNEL
VALUES
BETWEEN
LABELED
EXAMPLES
USE
THIS
KERNEL
MATRIX
TO
SOLVE
FOR
SVM
SUPPORT
VECTORS
WEIGHTS
TO
CLASSIFY
A
NEW
EXAMPLE
COMPUTE
KERNEL
VALUES
BETWEEN
NEW
INPUT
AND
SUPPORT
VECTORS
APPLY
WEIGHTS
CHECK
SIGN
OF
OUTPUT
EXAMPLE
LEARNING
GENDER
WITH
SVMS
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
MOGHADDAM
AND
YANG
FACE
GESTURE
LEARNING
GENDER
WITH
SVMS
TRAINING
EXAMPLES
MALES
FEMALES
EXPERIMENT
WITH
VARIOUS
KERNELS
SELECT
GAUSSIAN
RBF
K
XI
XJ
EXP
SUPPORT
FACES
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
GENDER
PERCEPTION
EXPERIMENT
HOW
WELL
CAN
HUMANS
DO
SUBJECTS
PEOPLE
MALE
FEMALE
AGES
MID
TO
MID
TEST
DATA
FACE
IMAGES
MALES
FEMALES
LOW
RES
AND
HIGH
RES
VERSIONS
TASK
CLASSIFY
AS
MALE
OR
FEMALE
FORCED
CHOICE
NO
TIME
LIMIT
GENDER
PERCEPTION
EXPERIMENT
HOW
WELL
CAN
HUMANS
DO
ERROR
ERROR
HUMAN
VS
MACHINE
SVMS
PERFORMED
BETTER
THAN
ANY
SINGLE
HUMAN
TEST
SUBJECT
AT
EITHER
RESOLUTION
KRISTEN
GRAUMAN
HARDEST
EXAMPLES
FOR
HUMANS
MOGHADDAM
AND
YANG
FACE
GESTURE
SVMS
PROS
AND
CONS
PROS
MANY
PUBLICLY
AVAILABLE
SVM
PACKAGES
OR
USE
BUILT
IN
MATLAB
VERSION
BUT
SLOWER
KERNEL
BASED
FRAMEWORK
IS
VERY
POWERFUL
FLEXIBLE
OFTEN
A
SPARSE
SET
OF
SUPPORT
VECTORS
COMPACT
AT
TEST
TIME
WORK
VERY
WELL
IN
PRACTICE
EVEN
WITH
VERY
SMALL
TRAINING
SAMPLE
SIZES
CONS
NO
DIRECT
MULTI
CLASS
SVM
MUST
COMBINE
TWO
CLASS
SVMS
CAN
BE
TRICKY
TO
SELECT
BEST
KERNEL
FUNCTION
FOR
A
PROBLEM
COMPUTATION
MEMORY
DURING
TRAINING
TIME
MUST
COMPUTE
MATRIX
OF
KERNEL
VALUES
FOR
EVERY
PAIR
OF
EXAMPLES
LEARNING
CAN
TAKE
A
VERY
LONG
TIME
FOR
LARGE
SCALE
PROBLEMS
ADAPTED
FROM
LANA
LAZEBNIK
CS
INTRO
TO
COMPUTER
VISION
INTERACTIVE
IMAGE
SEARCH
WITH
ATTRIBUTES
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
HOURS
OF
VIDEO
UPLOADED
TO
YOUTUBE
DAILY
MILLION
PHOTOS
UPLOADED
TO
FLICKR
DAILY
MILLION
TOP
LEVEL
DOMAINS
THIS
DATA
POOL
IS
TOO
BIG
TO
SIMPLY
BROWSE
WHO
DID
THE
WITNESS
SEE
AT
THE
CRIME
SCENE
FIND
ALL
IMAGES
DEPICTING
SHINY
OBJECTS
WHICH
PLANT
IS
THIS
KEYWORDS
WORK
WELL
FOR
CATEGORIES
WITH
KNOWN
NAME
UNCLEAR
HOW
TO
SEARCH
WITHOUT
NAME
OR
IMAGE
KEYWORDS
ARE
NOT
ENOUGH
POTENTIAL
TO
COMMUNICATE
MORE
PRECISELY
THE
DESIRED
VISUAL
CONTENT
ITERATIVELY
REFINE
THE
SET
OF
RETRIEVED
IMAGES
KEY
QUESTIONS
HOW
TO
OPEN
UP
COMMUNICATION
HOW
IS
INTERACTIVE
SEARCH
DONE
TODAY
KEYWORDS
BINARY
RELEVANCE
FEEDBACK
TRADITIONAL
BINARY
FEEDBACK
IMPRECISE
ALLOWS
ONLY
COARSE
COMMUNICATION
BETWEEN
USER
AND
SYSTEM
RUI
ET
AL
ZHOU
ET
AL
TONG
CHANG
COX
ET
AL
FERECATU
GEMAN
OUR
IDEA
SEARCH
VIA
COMPARISONS
LIKE
THIS
BUT
WITH
CURLIER
HAIR
ALLOW
USER
TO
WHITTLE
AWAY
IRRELEVANT
IMAGES
VIA
COMPARATIVE
FEEDBACK
ON
PROPERTIES
OF
RESULTS
KOVASHKA
PARIKH
AND
GRAUMAN
CVPR
VISUAL
ATTRIBUTES
HIGH
LEVEL
SEMANTIC
PROPERTIES
SHARED
BY
OBJECTS
METALLIC
LONG
HAIR
ORNAMENTS
RED
WE
WANT
TO
BE
ABLE
TO
INFER
SOMETHING
ABOUT
UNFAMILIAR
OBJECTS
IF
WE
CAN
INFER
CATEGORY
NAMES
FAMILIAR
OBJECTS
NEW
OBJECT
CAT
HORSE
DOG
FARHADI
ET
AL
CVPR
SLIDE
CREDIT
DEREK
HOIEM
WE
WANT
TO
BE
ABLE
TO
INFER
SOMETHING
ABOUT
UNFAMILIAR
OBJECTS
IF
WE
CAN
INFER
PROPERTIES
FAMILIAR
OBJECTS
NEW
OBJECT
HAS
STRIPES
HAS
EARS
HAS
EYES
HAS
FOUR
LEGS
HAS
MANE
HAS
TAIL
HAS
SNOUT
BROWN
MUSCULAR
HAS
SNOUT
HAS
STRIPES
LIKE
CAT
HAS
MANE
AND
TAIL
LIKE
HORSE
HAS
SNOUT
LIKE
HORSE
AND
DOG
FARHADI
ET
AL
CVPR
SLIDE
CREDIT
DEREK
HOIEM
BRIGHT
NOT
BRIGHT
SMILING
NOT
SMILING
NATURAL
NOT
NATURAL
WE
NEED
ABILITY
TO
COMPARE
IMAGES
BY
ATTRIBUTE
STRENGTH
BRIGHT
AT
TEST
TIME
PREDICT
ATTRIBUTE
STRENGTH
OF
EACH
DATABASE
IMAGE
INPUT
IMAGE
FEATURES
X
OUTPUT
REAL
VALUED
ATTRIBUTE
STRENGTH
AM
X
AT
TRAINING
TIME
LEARN
A
MAPPING
BETWEEN
IMAGE
FEATURES
AND
ATTRIBUTE
STRENGTH
INPUT
PAIRS
OF
ORDERED
IMAGES
WITH
FEATURES
OUTPUT
RANKING
FUNCTIONS
AM
PARIKH
AND
GRAUMAN
ICCV
WE
WANT
TO
LEARN
A
SPECTRUM
RANKING
MODEL
FOR
AN
ATTRIBUTE
E
G
BRIGHTNESS
SUPERVISION
FROM
HUMAN
ANNOTATORS
CONSISTS
OF
ORDERED
PAIRS
SIMILAR
PAIRS
PARIKH
AND
GRAUMAN
ICCV
LEARN
A
RANKING
FUNCTION
IMAGE
FEATURES
LEARNED
PARAMETERS
THAT
BEST
SATISFIES
THE
CONSTRAINTS
MAX
MARGIN
LEARNING
TO
RANK
FORMULATION
IMAGE
RELATIVE
ATTRIBUTE
SCORE
PARIKH
AND
GRAUMAN
ICCV
JOACHIMS
KDD
RELATIVE
ATTRIBUTES
WE
NEED
ABILITY
TO
COMPARE
IMAGES
BY
ATTRIBUTE
STRENGTH
BRIGHT
KOVASHKA
PARIKH
AND
GRAUMAN
CVPR
I
WANT
SOMETHING
MORE
NATURAL
THAN
THIS
I
WANT
SOMETHING
LESS
NATURAL
THAN
THIS
NATURAL
I
WANT
SOMETHING
WITH
MORE
PERSPECTIVE
THAN
THIS
KOVASHKA
PARIKH
AND
GRAUMAN
CVPR
QUERY
I
WANT
A
BRIGHT
OPEN
SHOE
THAT
IS
SHORT
ON
THE
LEG
MORE
OPEN
THAN
SELECTED
FE
ROUND
LESS
ORNA
ROUND
MATCH
EN
THAN
DATASETS
DATA
FROM
USERS
SHOES
SHOE
IMAGES
ATTRIBUTES
POINTY
BRIGHT
HIGH
HEELED
FEMININE
ETC
OSR
SCENE
IMAGES
ATTRIBUTES
NATURAL
PERSPECTIVE
OPEN
AIR
CLOSE
DEPTH
ETC
PUBFIG
FACE
IMAGES
ATTRIBUTES
MASCULINE
YOUNG
SMILING
ROUND
FACE
ETC
WHITTLESEARCH
RESULTS
SUMMARY
BINARY
FEEDBACK
REPRESENTS
STATUS
QUO
RUI
ET
AL
COX
ET
AL
FERECATU
GEMAN
WHITTLESEARCH
FINDS
RELEVANT
RESULTS
FASTER
THAN
TRADITIONAL
BINARY
FEEDBACK
WHITTLESEARCH
DEMO
CKTIME
PLAYER
FILE
EDIT
VI
EW
WI
NDOW
HELP
BIIL
O
A
J
WED
AM
Q
WLD
L
FIND
VISUAL
LY
APPEALING
SHOES
O
UR
RECOMMENDATIONS
ARE
BASED
ON
APP
EAR
ANC
E
NOT
D
ICK
TRACKING
WOMENS
GIRLS
MENS
BOYS
USERS
RETRIEVE
FONTS
THAT
MATCH
REQUESTED
ATTRIBUTES
FONTS
SORTED
BY
RELATIVE
ATTRIBUTE
SCORES
O
DONOVAN
ET
AL
EXPLORATORY
FONT
SELECTION
USING
CROWDSOURCED
ATTRIBUTES
SIGGRAPH
THE
MOST
RELEVANT
IMAGES
MIGHT
NOT
BE
MOST
INFORMATIVE
EXISTING
ACTIVE
METHODS
LARGELY
FOCUS
ON
BINARY
RELEVANCE
FEEDBACK
AND
SUFFER
FROM
EXPENSIVE
SELECTION
PROCEDURES
TONG
CHANG
LI
ET
AL
COX
ET
AL
FERECATU
GEMAN
RELATIVE
QUESTIONS
GAME
SELECT
SERIES
OF
MOST
INFORMATIVE
VISUAL
COMPARISONS
THAT
USER
SHOULD
MAKE
TO
HELP
DEDUCE
TARGET
KOVASHKA
AND
GRAUMAN
ICCV
ARE
THE
SHOES
YOU
SEEK
MORE
OR
LESS
FEMININE
THAN
TRADITIONAL
ACTIVE
APPROACH
USING
PIVOTS
POINTY
OPEN
BRIGHT
X
M
FEMININE
N
POINTY
OPEN
BRIGHT
M
FEMININE
PIVOTS
O
MN
COMPARISONS
TOTAL
M
N
O
M
COMPARISONS
TOTAL
HOW
TO
ACCOUNT
FOR
AMBIGUITY
OF
SEARCH
TERMS
STANDARD
ATTRIBUTE
LEARNING
ONE
GENERIC
MODEL
FORMAL
NOT
FORMAL
LEARN
A
GENERIC
MODEL
BY
POOLING
TRAINING
DATA
REGARDLESS
OF
THE
ANNOTATOR
IDENTITY
INTER
ANNOTATOR
DISAGREEMENT
TREATED
AS
NOISE
PROBLEM
ONE
MODEL
DOES
NOT
FIT
ALL
BINARY
ATTRIBUTE
RELATIVE
ATTRIBUTE
THERE
MAY
BE
VALID
PERCEPTUAL
DIFFERENCES
WITHIN
AN
ATTRIBUTE
YET
EXISTING
METHODS
ASSUME
MONOLITHIC
ATTRIBUTE
SUFFICIENT
LAMPERT
ET
AL
CVPR
FARHADI
ET
AL
CVPR
BRANSON
ET
AL
ECCV
KUMAR
ET
AL
PAMI
SCHEIRER
ET
AL
CVPR
PARIKH
GRAUMAN
ICCV
PERSONALIZATION
FROM
SCRATCH
FORMAL
NOT
FORMAL
COLLECT
DATA
FROM
THE
USER
AND
TRAIN
A
CLASSIFIER
IDEA
LEARN
USER
SPECIFIC
ATTRIBUTES
TREAT
LEARNING
PERCEIVED
ATTRIBUTES
AS
AN
ADAPTATION
PROBLEM
ADAPT
GENERIC
ATTRIBUTE
MODEL
WITH
MINIMAL
USER
SPECIFIC
LABELED
EXAMPLES
KOVASHKA
AND
GRAUMAN
ICCV
ADAPTING
BINARY
ATTRIBUTE
CLASSIFIERS
GIVEN
USER
LABELED
DATA
AND
GENERIC
MODEL
LEARN
ADAPTED
MODEL
J
YANG
ET
AL
ICDM
FORMAL
NOT
FORMAL
SUN
ATTRIBUTES
SCENE
IMAGES
ATTRIBUTES
SAILING
HIKING
VACATIONING
OPEN
AREA
VEGETATION
ETC
SHOES
SHOE
IMAGES
ATTRIBUTES
POINTY
BRIGHT
HIGH
HEELED
FEMININE
ETC
RESULT
OVER
DATASETS
ATTRIBUTES
AND
TOTAL
USERS
OUR
USER
ADAPTIVE
METHOD
MOST
ACCURATELY
CAPTURES
PERCEIVED
ATTRIBUTES
WITH
ADAPTED
ATTRIBUTES
SHOES
BINARY
SUN
GENERIC
GENERIC
USER
EXCLUSIVE
USER
ADAPTIVE
WHICH
IMAGES
MOST
INFLUENCE
ADAPTATION
OPEN
FEMININE
SAILING
CAMPING
VEGETATION
HORIZON
FAR
ATTRIBUTE
MODEL
SPECTRUM
NO
PERSONALIZATION
PERSONALIZED
TO
EACH
USER
ASSUMES
ALL
USERS
HAVE
THE
SAME
NOTION
OF
THE
ATTRIBUTE
ASSUMES
EACH
USER
HAS
A
UNIQUE
NOTION
OF
THE
ATTRIBUTE
ROBUSTNESS
TO
NOISE
VIA
MAJORITY
VOTE
NO
ROBUSTNESS
TO
NOISE
PROBLEM
USER
SPECIFIC
EXTREME
DIFFERENT
GROUPS
OF
USERS
MIGHT
SUBSCRIBE
TO
DIFFERENT
SHADES
OF
MEANING
OF
AN
ATTRIBUTE
TERM
HOW
CAN
WE
DISCOVER
THESE
SHADES
AUTOMATICALLY
COLOR
IS
OUR
IDEA
DISCOVERING
SHADES
OF
ATTRIBUTES
DISCOVER
SCHOOLS
OF
THOUGHT
AMONG
USERS
BASED
ON
LATENT
FACTORS
BEHIND
THEIR
USE
OF
ATTRIBUTE
TERMS
ALLOWS
DISCOVERY
OF
THE
ATTRIBUTE
SHADES
OF
MEANING
KOVASHKA
AND
GRAUMAN
IJCV
SHOW
USERS
DEFINITIONS
BUT
NO
EXAMPLE
IMAGES
ASK
THEM
TO
LABEL
IMAGES
WITH
PRESENCE
OF
ATTRIBUTE
ALSO
ASK
FOR
EXPLANATIONS
OF
SOME
RESPONSES
IS
THE
ATTRIBUTE
OPEN
PRESENT
IN
THE
IMAGE
GIVEN
A
PARTIALLY
OBSERVED
ATTRIBUTE
SPECIFIC
LABEL
MATRIX
WANT
TO
RECOVER
ITS
LATENT
FACTORS
VIA
MATRIX
FACTORIZATION
GIVES
A
REPRESENTATION
OF
EACH
USER
ATTRIBUTE
OPEN
CLUSTER
USERS
IN
THE
SPACE
OF
LATENT
FACTOR
REPRESENTATIONS
USE
K
MEANS
SELECT
K
AUTOMATICALLY
GIVES
A
REPRESENTATION
OF
EACH
SHADE
FOR
THIS
ATTRIBUTE
APPROACH
USING
SHADES
TO
PREDICT
ATTRIBUTES
RESULTS
ACCURACY
OF
ATTRIBUTE
PREDICTION
USER
ADP
KOVASHKA
AND
GRAUMAN
ICCV
ATTR
DISC
RASTEGARI
ET
AL
ECCV
SHADES
MAKE
PERSONALIZATION
MORE
ROBUST
SHADES
ADVANTAGE
TRANSFERS
TO
MORE
ACCURATE
SEARCH
LIU
AND
KOVASHKA
WACV
IDEA
LEARN
MODEL
FOR
NEW
DOMAIN
BY
ADAPTING
A
MODEL
FOR
AN
EXISTING
DOMAIN
IDEA
RELY
ON
FEATURES
THAT
ARE
SIMILAR
BETWEEN
THE
TWO
DOMAINS
TO
MAKE
THE
ADAPTATION
POSSIBLE
LIU
AND
KOVASHKA
WACV
LIU
AND
KOVASHKA
WACV
A
NAÏVE
ADAPTATION
METHOD
FAILS
TO
OUTPERFORM
A
METHOD
THAT
LEARNS
FROM
SCRATCH
BUT
FEATURE
SELECTION
MAKES
OUR
METHOD
MOST
USEFUL
WHEN
DATA
IS
SCARCE
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
TODAY
WINDOW
BASED
GENERIC
OBJECT
DETECTION
BASIC
PIPELINE
BOOSTING
CLASSIFIERS
FACE
DETECTION
AS
CASE
STUDY
BASIC
FRAMEWORK
BUILD
TRAIN
OBJECT
MODEL
CHOOSE
A
REPRESENTATION
LEARN
OR
FIT
PARAMETERS
OF
MODEL
CLASSIFIER
GENERATE
CANDIDATES
IN
NEW
IMAGE
SCORE
THE
CANDIDATES
REPRESENTATION
CHOICE
PART
BASED
SIMPLE
HOLISTIC
DESCRIPTIONS
OF
IMAGE
CONTENT
GRAYSCALE
COLOR
HISTOGRAM
VECTOR
OF
PIXEL
INTENSITIES
PIXEL
BASED
REPRESENTATIONS
SENSITIVE
TO
SMALL
SHIFTS
COLOR
OR
GRAYSCALE
BASED
APPEARANCE
DESCRIPTION
CAN
BE
SENSITIVE
TO
ILLUMINATION
AND
INTRA
CLASS
APPEARANCE
VARIATION
SUMMARIZE
LOCAL
DISTRIBUTION
OF
GRADIENTS
WITH
HISTOGRAM
LOCALLY
ORDERLESS
OFFERS
INVARIANCE
TO
SMALL
SHIFTS
AND
ROTATIONS
CONTRAST
NORMALIZATION
TRY
TO
CORRECT
FOR
VARIABLE
ILLUMINATION
GIVEN
THE
REPRESENTATION
TRAIN
A
BINARY
CLASSIFIER
NOY
ENSO
TCAARC
AR
DISCRIMINATIVE
CLASSIFIER
CONSTRUCTION
SLIDE
ADAPTED
FROM
ANTONIO
TORRALBA
WINDOW
BASED
MODELS
GENERATING
AND
SCORING
CANDIDATES
WINDOW
BASED
OBJECT
DETECTION
RECAP
TRAINING
OBTAIN
TRAINING
DATA
DEFINE
FEATURES
DEFINE
CLASSIFIER
GIVEN
NEW
IMAGE
SLIDE
WINDOW
SCORE
BY
CLASSIFIER
FEATURE
EXTRACTION
FACE
DETECTION
AND
RECOGNITION
SALLY
CHALLENGES
OF
FACE
DETECTION
SLIDING
WINDOW
DETECTOR
MUST
EVALUATE
TENS
OF
THOUSANDS
OF
LOCATION
SCALE
COMBINATIONS
FACES
ARE
RARE
PER
IMAGE
A
MEGAPIXEL
IMAGE
HAS
PIXELS
AND
A
COMPARABLE
NUMBER
OF
CANDIDATE
FACE
LOCATIONS
FOR
COMPUTATIONAL
EFFICIENCY
WE
SHOULD
TRY
TO
SPEND
AS
LITTLE
TIME
AS
POSSIBLE
ON
THE
NON
FACE
WINDOWS
TO
AVOID
HAVING
A
FALSE
POSITIVE
IN
EVERY
IMAGE
OUR
FALSE
POSITIVE
RATE
HAS
TO
BE
LESS
THAN
VIOLA
JONES
FACE
DETECTOR
WEAK
CLASSIFIER
WEIGHTS
INCREASED
WEAK
CLASSIFIER
WEIGHTS
INCREASED
WEAK
CLASSIFIER
FINAL
CLASSIFIER
IS
A
COMBINATION
OF
WEAK
CLASSIFIERS
BOOSTING
TRAINING
INITIALLY
WEIGHT
EACH
TRAINING
EXAMPLE
EQUALLY
IN
EACH
BOOSTING
ROUND
FIND
THE
WEAK
LEARNER
THAT
ACHIEVES
THE
LOWEST
WEIGHTED
TRAINING
ERROR
RAISE
WEIGHTS
OF
TRAINING
EXAMPLES
MISCLASSIFIED
BY
CURRENT
WEAK
LEARNER
COMPUTE
FINAL
CLASSIFIER
AS
LINEAR
COMBINATION
OF
ALL
WEAK
LEARNERS
WEIGHT
OF
EACH
LEARNER
IS
DIRECTLY
PROPORTIONAL
TO
ITS
ACCURACY
EXACT
FORMULAS
FOR
RE
WEIGHTING
AND
COMBINING
WEAK
LEARNERS
DEPEND
ON
THE
PARTICULAR
BOOSTING
SCHEME
E
G
ADABOOST
VIOLA
JONES
FACE
DETECTOR
MAIN
IDEA
REPRESENT
LOCAL
TEXTURE
WITH
EFFICIENTLY
COMPUTABLE
RECTANGULAR
FEATURES
WITHIN
WINDOW
OF
INTEREST
SELECT
DISCRIMINATIVE
FEATURES
TO
BE
WEAK
CLASSIFIERS
USE
BOOSTED
COMBINATION
OF
THEM
AS
FINAL
CLASSIFIER
FORM
A
CASCADE
OF
SUCH
CLASSIFIERS
REJECTING
CLEAR
NEGATIVES
QUICKLY
VIOLA
JONES
DETECTOR
FEATURES
RECTANGULAR
FILTERS
FEATURE
OUTPUT
IS
DIFFERENCE
BETWEEN
ADJACENT
REGIONS
VALUE
PIXELS
IN
WHITE
AREA
PIXELS
IN
BLACK
AREA
EFFICIENTLY
COMPUTABLE
WITH
INTEGRAL
IMAGE
ANY
SUM
CAN
BE
COMPUTED
IN
CONSTANT
TIME
VALUE
AT
X
Y
IS
SUM
OF
PIXELS
ABOVE
AND
TO
THE
LEFT
OF
X
Y
INTEGRAL
IMAGE
SOURCE
RESULT
THE
INTEGRAL
IMAGE
COMPUTES
A
VALUE
AT
EACH
PIXEL
X
Y
THAT
IS
THE
SUM
OF
THE
PIXEL
VALUES
ABOVE
AND
TO
THE
LEFT
OF
X
Y
INCLUSIVE
THIS
CAN
QUICKLY
BE
COMPUTED
IN
ONE
PASS
THROUGH
THE
IMAGE
CUMULATIVE
ROW
SUM
X
Y
X
Y
I
X
Y
INTEGRAL
IMAGE
II
X
Y
II
X
Y
X
Y
COMPUTING
SUM
WITHIN
A
RECTANGLE
LET
A
B
C
D
BE
THE
VALUES
OF
THE
INTEGRAL
IMAGE
AT
THE
CORNERS
OF
A
THEN
THE
SUM
OF
ORIGINAL
IMAGE
VALUES
WITHIN
THE
RECTANGLE
CAN
BE
COMPUTED
AS
SUM
A
B
C
D
ONLY
ADDITIONS
ARE
REQUIRED
FOR
ANY
SIZE
OF
RECTANGLE
VIOLA
JONES
DETECTOR
FEATURES
CONSIDERING
ALL
POSSIBLE
FILTER
PARAMETERS
POSITION
SCALE
AND
TYPE
POSSIBLE
FEATURES
ASSOCIATED
WITH
EACH
X
WINDOW
WHICH
SUBSET
OF
THESE
FEATURES
SHOULD
WE
USE
TO
DETERMINE
IF
A
WINDOW
HAS
A
FACE
USE
ADABOOST
BOTH
TO
SELECT
THE
INFORMATIVE
FEATURES
AND
TO
FORM
THE
CLASSIFIER
VIOLA
JONES
DETECTOR
ADABOOST
WANT
TO
SELECT
THE
SINGLE
RECTANGLE
FEATURE
AND
THRESHOLD
THAT
BEST
SEPARATES
POSITIVE
FACES
AND
NEGATIVE
NON
FACES
TRAINING
EXAMPLES
IN
TERMS
OF
WEIGHTED
ERROR
RESULTING
WEAK
CLASSIFIER
OUTPUTS
OF
A
POSSIBLE
RECTANGLE
FEATURE
ON
FACES
AND
NON
FACES
FOR
NEXT
ROUND
REWEIGHT
THE
EXAMPLES
ACCORDING
TO
ERRORS
CHOOSE
ANOTHER
FILTER
THRESHOLD
COMBO
ADABOOST
ALGORITHM
START
WITH
UNIFORM
WEIGHTS
ON
TRAINING
EXAMPLES
FOR
M
ROUNDS
XN
EVALUATE
WEIGHTED
ERROR
FOR
EACH
FEATURE
PICK
BEST
CLASSIFIER
RE
WEIGHT
THE
EXAMPLES
INCORRECTLY
CLASSIFIED
MORE
WEIGHT
CORRECTLY
CLASSIFIED
LESS
WEIGHT
FINAL
CLASSIFIER
IS
COMBINATION
OF
WEAK
ONES
WEIGHTED
ACCORDING
TO
ERROR
THEY
HAD
SLIDE
ADAPTED
FROM
K
GRAUMAN
FIGURE
FROM
SZELISKI
BOOSTING
FOR
FACE
DETECTION
FIRST
TWO
FEATURES
SELECTED
BY
BOOSTING
THIS
FEATURE
COMBINATION
CAN
YIELD
DETECTION
RATE
AND
FALSE
POSITIVE
RATE
BOOSTING
PROS
AND
CONS
ADVANTAGES
OF
BOOSTING
INTEGRATES
CLASSIFICATION
WITH
FEATURE
SELECTION
COMPLEXITY
OF
TRAINING
IS
LINEAR
IN
THE
NUMBER
OF
TRAINING
EXAMPLES
FLEXIBILITY
IN
THE
CHOICE
OF
WEAK
LEARNERS
BOOSTING
SCHEME
TESTING
IS
FAST
EASY
TO
IMPLEMENT
DISADVANTAGES
NEEDS
MANY
TRAINING
EXAMPLES
OFTEN
FOUND
NOT
TO
WORK
AS
WELL
AS
AN
ALTERNATIVE
DISCRIMINATIVE
CLASSIFIER
SUPPORT
VECTOR
MACHINE
SVM
ARE
WE
DONE
EVEN
IF
THE
FILTERS
ARE
FAST
TO
COMPUTE
EACH
NEW
IMAGE
HAS
A
LOT
OF
POSSIBLE
WINDOWS
TO
SEARCH
HOW
TO
MAKE
THE
DETECTION
MORE
EFFICIENT
CASCADING
CLASSIFIERS
FOR
DETECTION
FORM
A
CASCADE
WITH
LOW
FALSE
NEGATIVE
RATES
EARLY
ON
APPLY
LESS
ACCURATE
BUT
FASTER
CLASSIFIERS
FIRST
TO
IMMEDIATELY
DISCARD
WINDOWS
THAT
CLEARLY
APPEAR
TO
BE
NEGATIVE
WE
START
WITH
SIMPLE
CLASSIFIERS
WHICH
REJECT
MANY
OF
THE
NEGATIVE
SUB
WINDOWS
WHILE
DETECTING
ALMOST
ALL
POSITIVE
SUB
WINDOWS
POSITIVE
RESPONSE
FROM
THE
FIRST
CLASSIFIER
TRIGGERS
THE
EVALUATION
OF
A
SECOND
MORE
COMPLEX
CLASSIFIER
AND
SO
ON
A
NEGATIVE
OUTCOME
AT
ANY
POINT
LEADS
TO
THE
IMMEDIATE
REJECTION
OF
THE
SUB
WINDOW
THE
DETECTION
RATE
AND
THE
FALSE
POSITIVE
RATE
OF
THE
CASCADE
ARE
FOUND
BY
MULTIPLYING
THE
RESPECTIVE
RATES
OF
THE
INDIVIDUAL
STAGES
A
DETECTION
RATE
OF
AND
A
FALSE
POSITIVE
RATE
ON
THE
ORDER
OF
CAN
BE
ACHIEVED
BY
A
STAGE
CASCADE
IF
EACH
STAGE
HAS
A
DETECTION
RATE
OF
AND
A
FALSE
POSITIVE
RATE
OF
ABOUT
EXAMPLE
PROBLEM
DECIDE
WHETHER
TO
WAIT
FOR
A
TABLE
AT
A
RESTAURANT
BASED
ON
THE
FOLLOWING
ATTRIBUTES
ALTERNATE
IS
THERE
AN
ALTERNATIVE
RESTAURANT
NEARBY
BAR
IS
THERE
A
COMFORTABLE
BAR
AREA
TO
WAIT
IN
FRI
SAT
IS
TODAY
FRIDAY
OR
SATURDAY
HUNGRY
ARE
WE
HUNGRY
PATRONS
NUMBER
OF
PEOPLE
IN
THE
RESTAURANT
NONE
SOME
FULL
PRICE
PRICE
RANGE
RAINING
IS
IT
RAINING
OUTSIDE
RESERVATION
HAVE
WE
MADE
A
RESERVATION
TYPE
KIND
OF
RESTAURANT
FRENCH
ITALIAN
THAI
BURGER
WAITESTIMATE
ESTIMATED
WAITING
TIME
TRAIN
WITH
POSITIVES
NEGATIVES
REAL
TIME
DETECTOR
USING
LAYER
CASCADE
FEATURES
IN
ALL
LAYERS
ADAPTED
FROM
KRISTEN
GRAUMAN
A
SEMINAL
APPROACH
TO
REAL
TIME
OBJECT
DETECTION
TRAINING
IS
SLOW
BUT
DETECTION
IS
VERY
FAST
KEY
IDEAS
INTEGRAL
IMAGES
FOR
FAST
FEATURE
EVALUATION
BOOSTING
FOR
FEATURE
SELECTION
ATTENTIONAL
CASCADE
OF
CLASSIFIERS
FOR
FAST
REJECTION
OF
NON
FACE
WINDOWS
P
VIOLA
AND
M
JONES
CVPR
VIOLA
AND
M
JONES
IJCV
KRISTEN
GRAUMAN
EXAMPLE
USING
VIOLA
JONES
DETECTOR
FRONTAL
FACES
DETECTED
AND
THEN
TRACKED
CHARACTER
NAMES
INFERRED
WITH
ALIGNMENT
OF
SCRIPT
AND
SUBTITLES
EVERINGHAM
M
SIVIC
J
AND
ZISSERMAN
A
HELLO
MY
NAME
IS
BUFFY
AUTOMATIC
NAMING
OF
CHARACTERS
IN
TV
VIDEO
BMVC
ET
ASIA
RE
TTDLNOLOGY
MEANS
BLLI
I
N
SS
HOME
I
IN
SIGHT
IREVIEWS
I
T
EC
TTGU
I
DES
IJOBS
IBIOGS
IVIDEOS
I
COMM
UNITY
IDOWNLOADS
IIT
L
JIBRARY
I
SO
FT
W
AR
E
HARDW
AR
E
SECUR
ITY
COMMUNICAT
ION
BUS
INESS
IN
T
E
RN
ET
PHATO
I
SEA
RCH
ZDNET
SIA
NEW
I
NT
ERN
ET
GOOGLE
NOW
VIEW
BY
ELIN
OR
MILLS
CNET
NEWS
COM
FRID
AY
A
U
GUST
PT
I
GOO
G
LE
H
A
GOTL
E
A
LO
T
O
F
FL
AC
K
FRO
M
P
RI
VA
CY
AD
RO
C
AT
E
FO
R
PHOT
P
HRIRNGI
F
AC
E
ANCLL
NEW
FRO
M
CO
UNT
RI
ESJREG
ION
I
HAILAND
I
INDON
ES
IA
WH
AT
ILO
T
LATEST
NEW
IS
EBAY
FA
CINGI
ELLER
REV
OIT
I
A
IA
PAC
IFII
L
I
O
E
N
E
PL
AT
E
N
UMBE
R
AN
D
D
I
SPL
AYI
T
H
E
M
T
H
E
T
R
E
ET
VI
EW
I
N
GOO
MAPS
O
LIIG
I
N
ALL
Y
T
H
E
C
O
RNPANY
SAID
O
NLY
P
E
O
P
L
E
V
LH
O
I
D
E
N
T
I
FI
ED
U
E
M
SE
V
E
C
O
U
A
SK
TH
E
C
O
MP
ANY
TO
R
E
M
O
V
E
T
H
EI
R
IM
A
G
E
BUT
OO
GLE
H
AS
QUIETLY
CHANGED
POLI
CY
PARTLY
IN
RES
PON
SE
TO
CRITICI
SM
AND
NOW
ANYON
E
CAN
ALERT
THE
COMP
ANY
AND
HAVE
AN
IM
AGE
OF
A
LI
CEN
SE
PLATE
OR
A
REC
OGNIZABLE
FACE
REM
OVE
D
NOT
JUST
THE
O
ER
OF
TH
E
FACE
OR
CAR
AYS
MARISS
A
N
A
YER
VI
CE
PR
ESIDENT
OF
SEA
RCH
PRO
DUCTS
AND
US
ER
EXP
ERIEN
CE
AT
OOG
LE
REPO
RT
A
MAZ
ON
MAY
AGAIN
B
E
MULLIN
P
NETFLIX
BU
MO
Z
ILLA
MAP
OUT
JETP
AC
K
ADD
AN
TR
ANS
ITION
PL
AN
GA
A
GLE
BEGINS
EARCH
FAR
MIDDL
E
EAST
LABBY
IST
GOO
GLE
ST
ILL
THINKS
IT
CA
N
CHANG
E
CHINA
ADVERTISEMENT
BROUG
TH
A
GOOD
POLI
CY
FOR
US
ER
AND
ALSO
CLARIFIES
THE
INTENT
OF
TH
E
PROD
UC
T
SHE
SAID
IN
AN
INTE
F
I
E
N
FOLL
O
I
I
N
G
HER
KEYNOT
E
AT
THE
SE
ARCH
ENGIN
E
STRATEGIES
CONF
ERENCE
IN
SA
N
J
OS
E
CALIF
N
EDNES
DAY
TH
E
POLI
CY
CH
ANG
E
L
A
M
A
DE
A
BO
UT
DA
Y
AFTER
THE
LAUNCH
OF
THE
PROD
UCT
IN
LA
TE
RL
A
Y
BUT
L
A
NOT
PUBLI
CLY
ANNOUN
CED
AC
CORD
ING
TO
R
YER
THE
COM
PANY
I
REM
OVING
IM
AGES
ON
LY
W
HEN
ORN
E
ON
E
NOTIFI
E
THEM
AND
NOT
PRO
A
CTIVELY
SHE
SAID
WA
DEFIN
ITELY
A
BIG
PO
LI
CY
CHANGE
IN
SIDE
CISCO
COLLOBORATLON
SOLUF
RTORR
FACE
DETECTION
AND
RECOGNITION
SALLY
CAN
BE
TRAINED
TO
RECOGNIZE
PETS
SLIDE
CREDIT
LANA
LAZEBNIK
SURVEILLANCE
DEREK
HOIEM
CS
INTRO
TO
COMPUTER
VISION
DETECTION
II
DEFORMABLE
PART
MODELS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
TODAY
OBJECT
CATEGORY
DETECTION
WINDOW
BASED
APPROACHES
REVIEW
VIOLA
JONES
DETECTOR
DALAL
TRIGGS
PEDESTRIAN
DETECTOR
PART
BASED
APPROACHES
IMPLICIT
SHAPE
MODEL
DEFORMABLE
PARTS
MODEL
OBJECT
CATEGORY
DETECTION
FOCUS
ON
OBJECT
SEARCH
WHERE
IS
IT
BUILD
TEMPLATES
THAT
QUICKLY
DIFFERENTIATE
OBJECT
PATCH
FROM
BACKGROUND
PATCH
DOG
OR
NON
DOG
DEREK
HOIEM
RECTANGULAR
FILTERS
FEATURE
OUTPUT
IS
DIFFERENCE
BETWEEN
ADJACENT
REGIONS
VALUE
PIXELS
IN
WHITE
AREA
PIXELS
IN
BLACK
AREA
EFFICIENTLY
COMPUTABLE
WITH
INTEGRAL
IMAGE
ANY
SUM
CAN
BE
COMPUTED
IN
CONSTANT
TIME
VALUE
AT
X
Y
IS
SUM
OF
PIXELS
ABOVE
AND
TO
THE
LEFT
OF
X
Y
INTEGRAL
IMAGE
ADAPTED
FROM
KRISTEN
GRAUMAN
AND
LANA
LAZEBNIK
CONSIDERING
ALL
POSSIBLE
FILTER
PARAMETERS
POSITION
SCALE
AND
TYPE
POSSIBLE
FEATURES
ASSOCIATED
WITH
EACH
X
WINDOW
WHICH
SUBSET
OF
THESE
FEATURES
SHOULD
WE
USE
TO
DETERMINE
IF
A
WINDOW
HAS
A
FACE
USE
ADABOOST
BOTH
TO
SELECT
THE
INFORMATIVE
FEATURES
AND
TO
FORM
THE
CLASSIFIER
VIOLA
JONES
DETECTOR
ADABOOST
WANT
TO
SELECT
THE
SINGLE
RECTANGLE
FEATURE
AND
THRESHOLD
THAT
BEST
SEPARATES
POSITIVE
FACES
AND
NEGATIVE
NON
FACES
TRAINING
EXAMPLES
IN
TERMS
OF
WEIGHTED
ERROR
RESULTING
WEAK
CLASSIFIER
OUTPUTS
OF
A
POSSIBLE
RECTANGLE
FEATURE
ON
FACES
AND
NON
FACES
FOR
NEXT
ROUND
REWEIGHT
THE
EXAMPLES
ACCORDING
TO
ERRORS
CHOOSE
ANOTHER
FILTER
THRESHOLD
COMBO
CASCADING
CLASSIFIERS
FOR
DETECTION
DEMO
FORM
A
CASCADE
WITH
LOW
FALSE
NEGATIVE
RATES
EARLY
ON
APPLY
LESS
ACCURATE
BUT
FASTER
CLASSIFIERS
FIRST
TO
IMMEDIATELY
DISCARD
WINDOWS
THAT
CLEARLY
APPEAR
TO
BE
NEGATIVE
EXTRACT
FIXED
SIZED
PIXEL
WINDOW
AT
EACH
POSITION
AND
SCALE
COMPUTE
HOG
HISTOGRAM
OF
GRADIENT
FEATURES
WITHIN
EACH
WINDOW
SCORE
THE
WINDOW
WITH
A
LINEAR
SVM
CLASSIFIER
PERFORM
NON
MAXIMA
SUPPRESSION
TO
REMOVE
OVERLAPPING
DETECTIONS
WITH
LOWER
SCORES
NAVNEET
DALAL
AND
BILL
TRIGGS
HISTOGRAMS
OF
ORIENTED
GRADIENTS
FOR
HUMAN
DETECTION
NON
MAX
SUPPRESSION
ADAPTED
FROM
DEREK
HOIEM
MAP
EACH
GRID
CELL
IN
THE
INPUT
WINDOW
TO
A
HISTOGRAM
COUNTING
THE
GRADIENTS
PER
ORIENTATION
TRAIN
A
LINEAR
SVM
USING
TRAINING
SET
OF
PEDESTRIAN
VS
NON
PEDESTRIAN
WINDOWS
CODE
AVAILABLE
SLIDE
BY
PETE
BARNUM
KRISTEN
GRAUMAN
NAVNEET
DALAL
AND
BILL
TRIGGS
HISTOGRAMS
OF
ORIENTED
GRADIENTS
FOR
HUMAN
DETECTION
HISTOGRAM
OF
GRADIENT
ORIENTATIONS
ORIENTATION
BINS
FOR
UNSIGNED
ANGLES
HISTOGRAMS
IN
PIXEL
CELLS
VOTES
WEIGHTED
BY
MAGNITUDE
BILINEAR
INTERPOLATION
BETWEEN
CELLS
SLIDE
BY
PETE
BARNUM
NAVNEET
DALAL
AND
BILL
TRIGGS
HISTOGRAMS
OF
ORIENTED
GRADIENTS
FOR
HUMAN
DETECTION
CELLS
POS
W
NEG
W
PEDESTRIAN
SINGLE
RIGID
TEMPLATE
USUALLY
NOT
ENOUGH
TO
REPRESENT
A
CATEGORY
MANY
OBJECTS
E
G
HUMANS
ARE
ARTICULATED
OR
HAVE
PARTS
THAT
CAN
VARY
IN
CONFIGURATION
MANY
OBJECT
CATEGORIES
LOOK
VERY
DIFFERENT
FROM
DIFFERENT
VIEWPOINTS
OR
FROM
INSTANCE
TO
INSTANCE
SLIDE
BY
N
SNAVELY
IMAGES
FROM
CALTECH
IMAGES
FROM
D
RAMANAN
DATASET
DEFINE
OBJECT
BY
COLLECTION
OF
PARTS
MODELED
BY
APPEARANCE
SPATIAL
CONFIGURATION
ONE
EXTREME
FIXED
TEMPLATE
OBJECT
MODEL
SUM
OF
SCORES
OF
FEATURES
AT
FIXED
POSITIONS
NON
OBJECT
OBJECT
ANOTHER
EXTREME
BAG
OF
WORDS
STAR
SHAPED
MODEL
STAR
SHAPED
MODEL
ARTICULATED
PARTS
MODEL
OBJECT
IS
CONFIGURATION
OF
PARTS
EACH
PART
IS
DETECTABLE
AND
CAN
MOVE
AROUND
ADAPTED
FROM
DEREK
HOIEM
IMAGES
FROM
FELZENSZWALB
VISUAL
VOCABULARY
IS
USED
TO
INDEX
VOTES
FOR
OBJECT
POSITION
A
VISUAL
WORD
PART
TRAINING
IMAGE
ANNOTATED
WITH
OBJECT
LOCALIZATION
INFO
VISUAL
CODEWORD
WITH
DISPLACEMENT
VECTORS
LEARNING
IN
COMPUTER
VISION
LANA
LAZEBNIK
VISUAL
VOCABULARY
IS
USED
TO
INDEX
VOTES
FOR
OBJECT
POSITION
A
VISUAL
WORD
PART
TEST
IMAGE
LEARNING
IN
COMPUTER
VISION
BUILD
VOCABULARY
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
BUILD
VOCABULARY
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MAP
THE
PATCH
AROUND
EACH
INTEREST
POINT
TO
CLOSEST
WORD
BUILD
VOCABULARY
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MAP
THE
PATCH
AROUND
EACH
INTEREST
POINT
TO
CLOSEST
WORD
FOR
EACH
WORD
STORE
ALL
POSITIONS
IT
WAS
FOUND
RELATIVE
TO
OBJECT
CENTER
GIVEN
NEW
TEST
IMAGE
EXTRACT
PATCHES
MATCH
TO
VOCABULARY
WORDS
CAST
VOTES
FOR
POSSIBLE
POSITIONS
OF
OBJECT
CENTER
SEARCH
FOR
MAXIMA
IN
VOTING
SPACE
EXAMPLE
RESULTS
ON
COWS
K
GRAUMAN
B
LEIBE
DETECTION
RESULTS
QUALITATIVE
PERFORMANCE
RECOGNIZES
DIFFERENT
KINDS
OF
OBJECTS
ROBUST
TO
CLUTTER
OCCLUSION
NOISE
LOW
CONTRAST
K
GRAUMAN
B
LEIBE
ROOT
FILTER
PART
FILTERS
DEFORMATION
WEIGHTS
MULTIPLE
COMPONENTS
THE
SCORE
OF
A
HYPOTHESIS
IS
THE
SUM
OF
APPEARANCE
SCORES
MINUS
THE
SUM
OF
DEFORMATION
COSTS
SUBWINDOW
N
FEATURES
N
DISPLACEMENTS
SCORE
P
P
F
H
P
D
DX
DY
N
I
I
I
I
I
I
I
I
I
APPEARANCE
WEIGHTS
DEFORMATION
WEIGHTS
TRAINING
DATA
CONSISTS
OF
IMAGES
WITH
LABELED
BOUNDING
BOXES
NEED
TO
LEARN
THE
FILTERS
AND
DEFORMATION
PARAMETERS
F
X
W
H
X
W
ARE
MODEL
PARAMETERS
Z
ARE
LATENT
HYPOTHESES
LATENT
SVM
TRAINING
INITIALIZE
W
AND
ITERATE
FIX
W
AND
FIND
THE
BEST
Z
FOR
EACH
TRAINING
EXAMPLE
FIX
Z
AND
SOLVE
FOR
W
STANDARD
SVM
TRAINING
COMPONENT
COMPONENT
OBJECT
DETECTION
SYSTEM
OVERVIEW
OUR
SYSTEM
TAKES
AN
INPUT
IMAGE
EXTRACTS
AROUND
BOTTOM
UP
REGION
PROPOSALS
COMPUTES
FEATURES
FOR
EACH
PROPOSAL
USING
A
LARGE
CONVOLUTIONAL
NEURAL
NETWORK
CNN
AND
THEN
CLASSIFIES
EACH
REGION
USING
CLASS
SPECIFIC
LINEAR
SVMS
R
CNN
ACHIEVES
A
MEAN
AVERAGE
PRECISION
MAP
OF
ON
PASCAL
VOC
FOR
COMPARISON
UIJLINGS
ET
AL
REPORT
MAP
USING
THE
SAME
REGION
PROPOSALS
BUT
WITH
A
SPATIAL
PYRAMID
AND
BAG
OF
VISUAL
WORDS
APPROACH
THE
POPULAR
DEFORMABLE
PART
MODELS
PERFORM
AT
LANA
LAZEBNIK
SUMMARY
WINDOW
BASED
APPROACHES
ASSUME
OBJECT
APPEARS
IN
ROUGHLY
THE
SAME
CONFIGURATION
IN
DIFFERENT
IMAGES
LOOK
FOR
ALIGNMENT
WITH
A
GLOBAL
TEMPLATE
PART
BASED
METHODS
ALLOW
PARTS
TO
MOVE
SOMEWHAT
FROM
THEIR
USUAL
LOCATIONS
LOOK
FOR
GOOD
FITS
IN
APPEARANCE
FOR
BOTH
THE
GLOBAL
TEMPLATE
AND
THE
INDIVIDUAL
PART
TEMPLATES
NEXT
TIME
ANALYZING
AND
DEBUGGING
DETECTION
METHODS
CS
INTRO
TO
COMPUTER
VISION
DETECTION
III
ANALYZING
AND
DEBUGGING
DETECTION
METHODS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
TODAY
IN
WHAT
WAYS
DOES
DETECTION
FAIL
HOW
CAN
WE
VISUALIZE
FEATURES
AND
MODELS
DEFINE
OBJECT
BY
COLLECTION
OF
PARTS
MODELED
BY
APPEARANCE
SPATIAL
CONFIGURATION
STAR
SHAPED
MODEL
BUILD
VOCABULARY
OF
PATCHES
AROUND
EXTRACTED
INTEREST
POINTS
USING
CLUSTERING
MAP
THE
PATCH
AROUND
EACH
INTEREST
POINT
TO
CLOSEST
WORD
FOR
EACH
WORD
STORE
ALL
POSITIONS
IT
WAS
FOUND
RELATIVE
TO
OBJECT
CENTER
GIVEN
NEW
TEST
IMAGE
EXTRACT
PATCHES
MATCH
TO
VOCABULARY
WORDS
CAST
VOTES
FOR
POSSIBLE
POSITIONS
OF
OBJECT
CENTER
SEARCH
FOR
MAXIMA
IN
VOTING
SPACE
BIN
GRADIENTS
FROM
PIXEL
NEIGHBORHOODS
INTO
ORIENTATIONS
DALAL
TRIGGS
CVPR
ROOT
FILTER
PART
FILTERS
DEFORMATION
WEIGHTS
P
FELZE
LANA
LAZEBNIK
THE
SCORE
OF
A
HYPOTHESIS
IS
THE
SUM
OF
APPEARANCE
SCORES
MINUS
THE
SUM
OF
DEFORMATION
COSTS
SUBWINDOW
N
FEATURES
N
DISPLACEMENTS
SCORE
P
P
F
H
P
D
DX
DY
N
I
I
I
I
I
I
I
I
I
APPEARANCE
WEIGHTS
DEFORMATION
WEIGHTS
ADAPTED
FROM
LANA
LAZEBNIK
WHAT
IS
AN
OBJECT
B
ALEXE
T
DESELAERS
AND
V
FERRARI
COMPUTER
VISION
AND
PATTERN
RECOGNITION
CVPR
SPEEDING
UP
DETECTION
RESTRICT
SET
OF
WINDOWS
WE
PASS
THROUGH
SVM
TO
THOSE
W
HIGH
OBJECTNESS
OBJECTNESS
CUE
WHERE
PEOPLE
LOOK
OBJECTNESS
CUE
COLOR
CONTRAST
AT
BOUNDARY
OBJECTNESS
CUE
NO
SEGMENTS
STRADDLING
THE
OBJECT
BOX
BOXES
FOUND
TO
HAVE
HIGH
OBJECTNESS
CYAN
GROUND
TRUTH
BOUNDING
BOXES
YELLOW
CORRECT
AND
RED
INCORRECT
PREDICTIONS
FOR
OBJECTNESS
ONLY
RUN
THE
SHEEP
HORSE
CHAIR
ETC
CLASSIFIER
ON
THE
YELLOW
RED
BOXES
TODAY
REVIEW
DEFORMABLE
PART
MODELS
HOW
CAN
WE
SPEED
UP
DETECTION
HOW
CAN
WE
VISUALIZE
FEATURES
AND
DETECTIONS
DIAGNOSING
ERROR
IN
OBJECT
DETECTORS
D
HOIEM
Y
CHODPATHUMWAN
AND
Q
DAI
EUROPEAN
CONFERENCE
ON
COMPUTER
VISION
ECCV
INTRA
CLASS
VARIATION
FOR
AIRPLANE
OCCLUSION
SHAPE
VIEWPOINT
DISTANCE
CONFUSING
DISTRACTORS
FOR
AIRPLANE
BACKGROUND
SIMILAR
CATEGORIES
DISSIMILAR
CATEGORIES
LOCALIZATION
ERROR
OTHER
OBJECTS
SIMILAR
OBJECTS
BIRD
BOAT
CAR
BACKGROUND
OTHER
OBJECTS
ADDITIONAL
ANNOTATIONS
FOR
SEVEN
CATEGORIES
OCCLUSION
LEVEL
PARTS
VISIBLE
SIDES
VISIBLE
OCCLUSION
POOR
ROBUSTNESS
TO
OCCLUSION
BUT
LITTLE
IMPACT
ON
OVERALL
PERFORMANCE
EASIER
NONE
HARDER
HEAVY
SIZE
STRONG
PREFERENCE
FOR
AVERAGE
TO
ABOVE
AVERAGE
SIZED
AIRPLANES
LARGE
MEDIUM
X
LARGE
SMALL
SMALL
ASPECT
RATIO
BETTER
AT
DETECTING
WIDE
SIDE
VIEWS
THAN
TALL
VIEWS
X
WIDE
WIDE
MEDIUM
X
TALL
TALL
EASIER
WIDE
HARDER
TALL
SIDES
PARTS
BEST
PERFORMANCE
DIRECT
SIDE
VIEW
WITH
ALL
PARTS
VISIBLE
EASIER
SIDE
HARDER
NON
SIDE
MOST
ERRORS
THAT
DETECTORS
MAKE
ARE
REASONABLE
LOCALIZATION
ERROR
AND
CONFUSION
WITH
SIMILAR
OBJECTS
MISDETECTION
OF
OCCLUDED
OR
SMALL
OBJECTS
DETECTORS
HAVE
DIFFERENT
SENSITIVITY
TO
DIFFERENT
FACTORS
E
G
LESS
SENSITIVE
TO
TRUNCATION
THAN
TO
SIZE
DIFFERENCES
CODE
AND
ANNOTATIONS
ARE
AVAILABLE
ONLINE
TODAY
REVIEW
DEFORMABLE
PART
MODELS
HOW
CAN
WE
SPEED
UP
DETECTION
IN
WHAT
WAYS
DOES
DETECTION
FAIL
HOGGLES
VISUALIZING
OBJECT
DETECTION
FEATURES
C
VONDRICK
A
KHOSLA
T
MALISIEWICZ
AND
A
TORRALBA
INTERNATIONAL
CONFERENCE
ON
COMPUTER
VISION
ICCV
WHY
DID
THE
DETECTOR
FAIL
CAR
VONDRICK
ET
AL
ICCV
WHAT
INFORMATION
IS
LOST
VONDRICK
ET
AL
ICCV
WHAT
INFORMATION
IS
LOST
VONDRICK
ET
AL
ICCV
RECOVERING
IMAGE
FROM
NEIGHBORS
IMAGE
HOG
TOP
DETECTIONS
VONDRICK
ET
AL
ICCV
RECOVERING
IMAGE
FROM
NEIGHBORS
IMAGE
HOG
TOP
DETECTIONS
VONDRICK
ET
AL
ICCV
RECOVERING
IMAGE
FROM
NEIGHBORS
IMAGE
HOG
TOP
DETECTIONS
VONDRICK
ET
AL
ICCV
RECOVERING
IMAGE
FROM
NEIGHBORS
IMAGE
HOG
TOP
DETECTIONS
VONDRICK
ET
AL
ICCV
BETTER
RECOVERY
USING
PAIRED
DICTIONARY
VONDRICK
ET
AL
ICCV
A
MICROSCOPE
TO
VIEW
HOG
MORE
INTUITIVE
VONDRICK
ET
AL
ICCV
VS
HUMAN
VISION
HOG
VISION
VONDRICK
ET
AL
ICCV
VONDRICK
ET
AL
ICCV
VONDRICK
ET
AL
ICCV
VONDRICK
ET
AL
ICCV
VONDRICK
ET
AL
ICCV
VONDRICK
ET
AL
ICCV
VONDRICK
ET
AL
ICCV
THE
HOGGLES
CHAL
ENGE
HUMANS
DETECT
DPMS
DETECT
VONDRICK
ET
AL
ICCV
THE
HOGGLES
CHALLENGE
HUMANS
MISS
DPM
MISS
VONDRICK
ET
AL
ICCV
CHAIR
DETECTIONS
VONDRICK
ET
AL
ICCV
CHAIR
DETECTIONS
VONDRICK
ET
AL
ICCV
CAR
DETECTIONS
VONDRICK
ET
AL
ICCV
CAR
DETECTIONS
VONDRICK
ET
AL
ICCV
HOG
HUMAN
DETECTOR
CHAIR
LOSS
DUE
TO
RGB
HOG
RECALL
HOG
DPM
HOG
HUMAN
RGB
HUMAN
VONDRICK
ET
AL
ICCV
WHY
DID
THE
DETECTOR
FAIL
CAR
VONDRICK
ET
AL
ICCV
WHY
DID
THE
DETECTOR
FAIL
CAR
VONDRICK
ET
AL
ICCV
WHY
DID
THE
DETECTOR
FAIL
CAR
VONDRICK
ET
AL
ICCV
VISUALIZING
LEARNED
MODELS
CAR
PERSON
BOTTLE
BICYCLE
MOTORBIKE
CHAIR
TV
HORSE
VONDRICK
ET
AL
ICCV
WE
CAN
SPEED
UP
OBJECT
DETECTION
BY
USING
THE
NOTION
OF
OBJECTNESS
TO
PRUNE
WINDOWS
UNLIKELY
TO
CONTAIN
ANY
OBJECT
SOME
FAILURE
MODES
ARE
MORE
IMPORTANT
THAN
OTHERS
AND
FIXING
THEM
COULD
INCREASE
THE
OVERALL
DETECTION
PERFORMANCE
EVEN
HUMANS
CANNOT
PRODUCE
CORRECT
CLASSIFICATIONS
WITH
IMPERFECT
FEATURES
CS
INTRO
TO
COMPUTER
VISION
HUMAN
FACTORS
CROWDSOURCING
LABELS
GAZE
SALIENCY
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
TODAY
GAMES
WITH
A
PURPOSE
ESP
GAME
PEEKABOOM
VISUAL
SALIENCY
PREDICT
WHERE
PEOPLE
WILL
LOOK
AMAZON
MECHANICAL
TURK
WORKERS
ANNOTATION
PROTOCOLS
TYPE
KEYWORDS
SELECT
RELEVANT
IMAGES
CLICK
ON
LANDMARKS
OUTLINE
SOMETHING
ANYTHING
ELSE
CUSTOM
ANNOTATIONS
X
LARGE
SCALE
LOW
PRICE
G
A
M
AZON
M
ECHAN
I
CA
L
TURK
X
AMAZONMECHANICAL
TURK
ARTIFICIAL
ARTIFICIAL
INTELLIGENCE
YOUR
ACCOUNT
HITS
QUALIFICAT
IONS
INTRODUCTION
I
DASHBOARD
I
STATUS
I
ACCOUNT
SETTINGS
MECHANICAL
TURK
IS
A
MARKETPLACE
FOR
WORK
D
ALREADY
HAVE
AN
ACCOUNT
SIGN
IN
AS
A
WORKER
I
REQUESTER
WE
GIVE
BUSINESSES
AND
DEVELOPERS
ACCESS
TO
AN
ON
DEMAND
SCALABLE
WORKFORCE
WORKERS
SELECT
FROM
MOUSANDS
OF
TASKS
A
A
WORK
WHENEVER
IT
CONVENIENT
HITS
AVAILAB
LE
VIEW
THEM
NOW
MAKE
MONEY
BY
WORKING
ON
HITS
H
TS
HUM
AN
I
NTELLIGENCE
TASKS
ARE
INDIVIDUAL
TASKS
THAT
YOU
WORK
ON
FIND
HITS
NO
W
AS
A
MECHANICAL
TURK
WORKER
YOU
CAN
WORK
FROM
HOME
CHOOSE
YOUR
OWN
WORK
H
OURS
GET
PAID
FOR
DOING
GOOD
W
ORK
GE
RESUL
FROM
MECHANICAL
TURK
WORKERS
ASK
WORKERS
TO
COMPLETE
HITS
HUMAN
I
NTELLIGENCE
TASKS
AND
GET
R
ESULT
U
SING
MECHANICAL
TURK
GET
STARTED
AS
A
MECHANICAL
TURK
REQUESTER
YOU
HAVE
ACCESS
TO
A
GLOBAL
ON
DEMAND
X
WORKFORCE
GET
THOUSANDS
OF
HITS
COMPLETED
IN
MINUTES
PAY
ONLY
WHEN
YOU
RE
SATISFIED
WIT
H
TH
E
RESULTS
FIND
AN
INTERESTING
TASK
WORK
EARN
MONEY
FIND
HITS
NOW
FUND
YOUR
LOAD
YOUR
ACCOUNT
TASKS
GET
STARTED
GET
RESULTS
OR
L
EARN
MORE
ABOUT
BEING
A
WORKER
FAQ
I
CO
NTACT
US
I
CA
R
EERS
AT
MEC
H
ANICAL
TURK
I
DEV
ELOP
ERS
I
PRESS
I
POLICIES
I
STATE
LICENSING
I
BLOG
I
SE
RV
ICE
HEA
LT
H
D
ASHBO
AR
D
AMA
W
N
COM
I
N
C
OR
ITS
AFF
ILI
AT
ES
AN
AMA
ON
COM
COMPANY
ISSUES
QUALITY
HOW
GOOD
IS
IT
HOW
TO
BE
SURE
PRICE
HOW
TO
PRICE
IT
ENSURING
ANNOTATION
QUALITY
CONSENSUS
MULTIPLE
ANNOTATION
WISDOM
OF
THE
CROWD
QUALIFICATION
EXAM
GOLD
STANDARD
QUESTIONS
GRADING
TASKS
A
SECOND
TIER
OF
WORKERS
WHO
GRADE
OTHERS
CROWDSOURCING
OBJECT
BOUNDING
BOXES
PRICING
TRADE
OFF
BETWEEN
THROUGHPUT
AND
COST
HIGHER
PAY
CAN
ACTUALLY
ATTRACT
SCAMMERS
SOME
STUDIES
FIND
THAT
THE
MOST
ACCURATE
RESULTS
ARE
ACHIEVED
IF
TURKERS
DO
TASKS
FOR
FREE
C
P
Q
Q
O
U
CL
U
Q
TI
I
O
OF
RRECT
R
TECT
IO
N
O
C
ERROR
P
U
O
U
U
I
ERROR
CL
TI
C
I
ERROR
ERROR
F
RATE
O
CORRECT
N
C
A
II
T
ERIROR
ER
ROR
I
ER
ROR
II
C
I
O
ER
ROR
LO
C
P
U
Q
O
Q
C
E
RRO
R
E
RRO
R
I
ERRO
R
O
C
ERROR
P
U
DJ
DJ
O
U
DJ
E
RRO
R
U
DJ
C
C
I
E
RRO
R
L
O
RATE
OF
RRECT
R
TECT
IO
N
O
C
E
RRO
R
P
Q
Q
E
RRO
R
O
U
Q
U
Q
C
E
RRO
R
E
RRO
R
OF
RRECT
R
TECTIO
N
O
C
P
U
T
O
ERIROR
I
ERROR
Q
C
ADVER
ARIES
I
ERROR
ERROR
RATEOF
RRECT
R
TECT
IO
N
O
TODAY
COLLECTING
ANNOTATIONS
FROM
HUMANS
MTURK
PIPELINE
AND
CHALLENGES
VISUAL
SALIENCY
PREDICT
WHERE
PEOPLE
WILL
LOOK
LUIS
VON
AHN
ASSOCIATE
PROFESSOR
AT
CMU
ONE
OF
THE
FATHERS
OF
CROWDSOURCING
CREATED
THE
ESP
GAME
PEEKABOOM
AND
SEVERAL
OTHER
GAMES
WITH
A
PURPOSE
TWO
PLAYER
ONLINE
GAME
PARTNERS
DON
T
KNOW
EACH
OTHER
AND
CAN
T
COMMUNICATE
OBJECT
OF
THE
GAME
TYPE
THE
SAME
WORD
THE
ONLY
THING
IN
COMMON
IS
AN
IMAGE
LUIS
VON
AHN
AND
LAURA
DABBISH
LABELING
IMAGES
WITH
A
COMPUTER
GAME
CHI
PLAYER
PLAYER
GUESSING
CAR
GUESSING
BOY
GUESSING
HAT
GUESSING
KID
SUCCESS
YOU
AGREE
ON
CAR
GUESSING
CAR
SUCCESS
YOU
AGREE
ON
CAR
THE
ESP
GAME
IS
FUN
MILLION
LABELS
WITH
PLAYERS
THERE
ARE
MANY
PEOPLE
THAT
PLAY
OVER
HOURS
A
WEEK
LABELING
THE
ENTIRE
WEB
PEOPLE
PLAYING
SIMULTANEOUSLY
CAN
LABEL
ALL
IMAGES
ON
GOOGLE
IN
DAYS
INDIVIDUAL
GAMES
IN
YAHOO
AND
MSN
AVERAGE
OVER
PLAYERS
AT
A
TIME
A
FEW
MILLION
LABELS
CAN
IMPROVE
IMAGE
SEARCH
CAN
BE
USED
TO
IMPROVE
COMPUTER
VISION
SINGLE
PLAYER
GAME
A
SINGLE
PERSON
CAN
PLAY
WITH
PRE
RECORDED
ACTIONS
AS
THEIR
PARTNER
CAR
HAT
BOY
WE
EMULATE
PARTNER
BY
PLAYING
PRE
RECORDED
MOVES
KID
CAR
NOTICE
THAT
THIS
WHEN
PEOPLE
PLAY
WE
RECORD
EVERY
ACTION
WITH
TIMING
INFORMATION
DOESN
T
STOP
THE
LABELING
PROCESS
WHAT
ABOUT
CHEATING
IF
A
PAIR
PLAYS
TOO
FAST
WE
DON
T
RECORD
THE
WORDS
THEY
AGREE
ON
WHAT
ABOUT
CHEATING
WE
GIVE
PLAYERS
TEST
IMAGES
FOR
WHICH
WE
KNOW
ALL
THE
COMMON
LABELS
WE
ONLY
STORE
A
PLAYER
GUESSES
IF
THEY
SUCCESSFULLY
LABEL
THE
TEST
IMAGES
WHY
DO
PEOPLE
LIKE
THE
ESP
GAME
THE
ESP
GAME
GIVES
ITS
PLAYERS
A
WEIRD
AND
BEAUTIFUL
SENSE
OF
ANONYMOUS
INTIMACY
ON
THE
ONE
HAND
YOU
HAVE
NO
IDEA
WHO
YOUR
PARTNER
IS
ON
THE
OTHER
HAND
THE
TWO
OF
YOU
ARE
BRINGING
YOUR
MINDS
TOGETHER
IN
A
WAY
THAT
LOVERS
WOULD
ENVY
STRANGELY
ADDICTIVE
IT
SO
MUCH
FUN
TRYNG
TO
GUESS
WHAT
OTHERS
THINK
YOU
HAVE
TO
STEP
OUTSIDE
OF
YOURSELF
TO
MATCH
IT
FAST
PACED
HELPS
ME
LEARN
ENGLISH
THERE
ARE
MANY
FASCINATING
THINGS
ABOUT
THE
ESP
GAME
HOW
IT
USES
A
GAME
STRUCTURE
TO
BUILD
AN
INDEX
OF
IMAGES
ON
THE
WEB
WHAT
WORDS
BECOME
TABOO
FROM
FREQUENT
USE
HOW
INTERESTING
SOME
OF
THE
IMAGES
PULLED
RANDOMLY
FROM
THE
WEB
ARE
BUT
BY
FAR
THE
MOST
INTRIGUING
ASPECT
OF
THE
GAME
IS
HOW
OFTEN
YOUR
RANDOM
UNKNOWN
PARTNER
IS
A
COMPLETE
IDIOT
OTHER
GAMES
LOCATING
OBJECTS
IN
IMAGES
THE
ESP
GAME
TELLS
US
IF
AN
IMAGE
CONTAINS
A
SPECIFIC
OBJECT
BUT
DOESN
T
SAY
WHERE
IN
THE
IMAGE
THE
OBJECT
IS
SUCH
INFORMATION
WOULD
BE
EXTREMELY
USEFUL
FOR
COMPUTER
VISION
RESEARCH
PLAYERS
SHOOT
AT
OBJECTS
ON
THE
IMAGE
SHOOT
THE
CAR
WE
GIVE
POINTS
AND
CHECK
ACCURACY
BY
GIVING
PLAYERS
IMAGES
FOR
WHICH
WE
ALREADY
KNOW
WHERE
THE
OBJECT
IS
X
OF
IMAGES
X
OF
IMAGES
DON
T
KNOW
GUESSER
REVEALER
GUESS
CAR
BCRAURSH
PARTNER
GUESS
THE
REVEALER
CLICKS
ON
PARTS
OF
THE
IMAGE
AND
SHOWS
THEM
TO
THE
GUESSER
THE
GUESSER
GUESSES
FLOWER
PETAL
BUTTERFLY
SERVER
CORRECT
BUTTERFLY
WHY
PEEKABOOM
WORKS
BY
GETTING
THE
GUESSER
TO
GUESS
CORRECTLY
THE
REVEALER
LOCATES
OBJECTS
BY
CLICKING
ON
THE
RELEVANT
PARTS
OF
THE
IMAGE
SUMMARY
COLLECTING
ANNOTATIONS
FROM
HUMANS
CROWDSOURCING
ALLOWS
VERY
CHEAP
DATA
COLLECTION
GETTING
HIGH
QUALITY
ANNOTATIONS
CAN
BE
TRICKY
BUT
THERE
ARE
MANY
WAYS
TO
ENSURE
QUALITY
ONE
WAY
TO
OBTAIN
HIGH
QUALITY
DATA
FAST
IS
BY
PHRASING
YOUR
DATA
COLLECTION
AS
A
GAME
TODAY
COLLECTING
ANNOTATIONS
FROM
HUMANS
MTURK
PIPELINE
AND
CHALLENGES
GAMES
WITH
A
PURPOSE
ESP
GAME
PEEKABOOM
NATURAL
IMAGES
FROM
FLICKR
AND
LABELME
TILKE
JUDD
CHRIS
THOMAS
EYE
TRACKING
EXPERIMENTS
SCREEN
RESOLUTION
EACH
IMAGE
SHOWN
FOR
SECONDS
USER
RESTS
HEAD
IN
CHIN
REST
EYE
TRACKER
MEASURES
LOCATION
OF
EYE
FIXATION
SEVERAL
TIMES
A
SECOND
PEOPLE
FREE
VIEWED
THE
IMAGES
ACTUALLY
DONE
IN
DARK
ROOM
CALIBRATE
CHECK
EVERY
IMAGES
DIVIDE
THE
EXPERIMENT
INTO
TWO
SESSIONS
OF
RANDOMLY
ORDERED
IMAGES
INITIAL
FIXATION
IS
DISCARDED
TILKE
JUDD
CHRIS
THOMAS
HOW
DO
EYE
TRACKERS
WORK
JUST
CONVOLVE
A
GAUSSIAN
OVER
THE
FIXATION
POSITIONS
CAN
DO
THIS
FOR
A
SINGLE
PERSON
OR
AN
AGGREGATE
OF
MANY
PEOPLE
CAN
ALSO
THRESHOLD
CHOOSE
TOP
N
YIELDS
A
BINARY
MAP
LABELED
FACES
TEXT
WITH
BOUNDING
BOX
AND
HORIZON
LINE
IF
ANY
BASED
ON
THE
ANNOTATIONS
FIXATION
ON
FACES
ON
TEXT
OF
FIXATIONS
ARE
WITHIN
THE
CENTER
OF
THE
IMAGE
ANIMALS
CARS
HUMAN
BODY
PARTS
FEATURES
LABELS
PREDICT
WHERE
PEOPLE
WILL
LOOK
FOR
A
NEW
IMAGE
BY
RUNNING
MODEL
ON
EVERY
PIXEL
OF
THE
IMAGE
SALIENCY
MODEL
FEATURES
ILLUMINATION
COLOR
ORIENTATION
HORIZON
LINE
FACE
DETECTOR
VIOLA
JONES
PERSON
DETECTOR
DPM
DISTANCE
TO
CENTER
SALIENT
SAMPLES
FROM
TOP
NON
SALIENT
SAMPLES
FROM
BOTTOM
FROM
EACH
OF
TRAINING
IMAGES
LEARNS
WEIGHTS
FOR
EACH
FEATURE
THAT
BEST
PREDICT
A
SALIENCY
LABEL
FOR
EACH
PIXEL
NON
PHOTOREALISTIC
RENDERING
APPLICATION
SALIENCY
MAP
FROM
FIXATIONS
OF
ONE
PERSON
IS
THRESHOLDED
TO
BECOME
A
BINARY
CLASSIFIER
IF
PIXEL
AT
X
Y
OVER
THRESHOLD
OTHERWISE
BY
VARYING
THE
THRESHOLD
WE
CAN
GET
A
ROC
CURVE
RECEIVER
OPERATING
CHARACTERISTIC
CURVE
AVERAGE
OVER
ALL
USERS
AND
ALL
IMAGES
RECEIVER
OPERATING
CHARACTERISTIC
CURVE
PERCENT
SALIENT
HUMAN
FIXATIONS
WHITE
CLOUD
PREDICTED
SALIENCY
STRENGTH
RED
DOTS
TRUE
HUMAN
FIXATIONS
RECEIVER
OPERATING
CHARACTERISTIC
CURVE
THRESHOLDED
SALIENCY
MAP
PERCENT
SALIENT
WE
CALCULATE
THE
PERCENTAGE
OF
FIXATIONS
THAT
LIE
WITHIN
THE
SALIENT
PORTION
OF
THE
MAP
WHITE
CLOUD
PREDICTED
SALIENCY
STRENGTH
RED
DOTS
TRUE
HUMAN
FIXATIONS
RECEIVER
OPERATING
CHARACTERISTIC
CURVE
THRESHOLDED
SALIENCY
MAP
PERCENT
SALIENT
WE
CALCULATE
THE
PERCENTAGE
OF
FIXATIONS
THAT
LIE
WITHIN
THE
SALIENT
PORTION
OF
THE
MAP
WHITE
CLOUD
PREDICTED
SALIENCY
STRENGTH
RED
DOTS
TRUE
HUMAN
FIXATIONS
RECEIVER
OPERATING
CHARACTERISTIC
CURVE
THRESHOLDED
SALIENCY
MAP
WE
CALCULATE
THE
PERCENTAGE
OF
FIXATIONS
THAT
LIE
WITHIN
THE
SALIENT
PORTION
OF
THE
MAP
WHITE
CLOUD
PREDICTED
SALIENCY
STRENGTH
RED
DOTS
TRUE
HUMAN
FIXATIONS
RECEIVER
OPERATING
CHARACTERISTIC
CURVE
THRESHOLDED
SALIENCY
MAP
PERCENT
SALIENT
WE
CALCULATE
THE
PERCENTAGE
OF
FIXATIONS
THAT
LIE
WITHIN
THE
SALIENT
PORTION
OF
THE
MAP
SUMMARY
VISUAL
SALIENCY
COLLECT
HUMAN
GAZE
DATA
AND
TRAIN
A
CLASSIFIER
TO
PREDICT
WHICH
PIXELS
WILL
BE
FIXATED
FIXATED
REGIONS
INTERESTING
REGIONS
CAN
USE
THIS
TO
RESTRICT
REGION
OF
IMAGE
THAT
WE
WANT
TO
EXAMINE
E
G
FOR
OBJECT
DETECTION
ALSO
USEFUL
FOR
GRAPHICS
APPLICATIONS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
NOVEMBER
ANNOUNCEMENTS
HOMEWORK
DUE
TONIGHT
JUST
SAVE
SOME
POSITIVE
DETECTIONS
NOT
ALL
HOMEWORK
OUT
DUE
DECEMBER
WORTH
POINTS
TOTAL
I
E
HALF
THE
WORK
STILL
OF
FINAL
GRADE
UP
TO
POINTS
OF
EXTRA
CREDIT
FIXATIONS
FOR
ONE
PERSON
FIXATION
MAP
JUST
CONVOLVE
A
GAUSSIAN
OVER
THE
FIXATION
POSITIONS
CAN
DO
THIS
FOR
A
SINGLE
PERSON
OR
AN
AGGREGATE
OF
MANY
PEOPLE
CAN
ALSO
THRESHOLD
CHOOSE
TOP
N
YIELDS
A
BINARY
MAP
WHERE
DO
PEOPLE
ACTUALLY
LOOK
LABELED
FACES
TEXT
WITH
BOUNDING
BOX
AND
HORIZON
LINE
IF
ANY
BASED
ON
THE
ANNOTATIONS
FIXATION
ON
FACES
ON
TEXT
OF
FIXATIONS
ARE
WITHIN
THE
CENTER
OF
THE
IMAGE
ANIMALS
CARS
HUMAN
BODY
PARTS
LEARNING
A
CLASSIFIER
FROM
EYE
TRACKING
DATA
FEATURES
LABELS
PREDICT
WHERE
PEOPLE
WILL
LOOK
FOR
A
NEW
IMAGE
BY
RUNNING
MODEL
ON
EVERY
PIXEL
OF
THE
IMAGE
SALIENCY
MODEL
FEATURES
ILLUMINATION
COLOR
ORIENTATION
HORIZON
LINE
FACE
DETECTOR
VIOLA
JONES
PERSON
DETECTOR
DPM
DISTANCE
TO
CENTER
SALIENT
SAMPLES
FROM
TOP
NON
SALIENT
SAMPLES
FROM
BOTTOM
FROM
EACH
OF
TRAINING
IMAGES
LEARNS
WEIGHTS
FOR
EACH
FEATURE
THAT
BEST
PREDICT
A
SALIENCY
LABEL
FOR
EACH
PIXEL
EVALUATING
SALIENCY
MAPS
AMAZON
MECHANICAL
TURK
WORKERS
THE
ESP
GAME
TWO
PLAYER
ONLINE
GAME
PARTNERS
DON
T
KNOW
EACH
OTHER
AND
CAN
T
COMMUNICATE
OBJECT
OF
THE
GAME
TYPE
THE
SAME
WORD
THE
ONLY
THING
IN
COMMON
IS
AN
IMAGE
LUIS
VON
AHN
AND
LAURA
DABBISH
LABELING
IMAGES
WITH
A
COMPUTER
GAME
CHI
THE
ESP
GAME
PLAYER
PLAYER
GUESSING
CAR
GUESSING
BOY
GUESSING
HAT
GUESSING
KID
SUCCESS
YOU
AGREE
ON
CAR
GUESSING
CAR
SUCCESS
YOU
AGREE
ON
CAR
REVEALING
IMAGES
GUESSER
REVEALER
GUESS
CAR
BCRAURSH
PARTNER
GUESS
USING
GAZE
TO
COLLECT
OBJECT
BOUNDING
BOXES
VIEWING
TASK
WAS
TO
DETERMINE
WHICH
OBJECT
CATEGORY
IS
SHOWN
COMPUTE
A
BOUNDING
BOX
FROM
LOCATIONS
THAT
WERE
FIXATED
BY
USERS
FASTER
THAN
DRAWING
BOUNDING
BOXES
PAPADOPOULOS
ET
AL
TRAINING
OBJECT
CLASS
DETECTORS
FROM
EYE
TRACKING
DATA
ECCV
LEARNING
ATTRIBUTES
USING
HUMAN
GAZE
USE
HUMAN
GAZE
TO
LEARN
WHERE
ATTRIBUTES
LIVE
TO
LEARN
ATTRIBUTE
MODELS
EXTRACT
FEATURES
ONLY
FROM
FIXATED
REGIONS
TODAY
WHAT
TO
DO
WHEN
DATA
IS
EXPENSIVE
TO
OBTAIN
HUMAN
IN
THE
LOOP
RECOGNITION
CHOOSE
LABELING
QUESTIONS
AT
TEST
TIME
DEVELOP
UNSUPERVISED
METHODS
THAT
DON
T
REQUIRE
LABELS
NEXT
TIME
OVERVIEW
OF
RECENT
RESEARCH
CONCEPTS
WILL
GO
FAST
CROWDSOURCING
TRAINING
JAMES
HAYS
ACTIVE
LEARNING
TRADITIONAL
ACTIVE
LEARNING
REDUCES
SUPERVISION
BY
OBTAINING
LABELS
FOR
THE
MOST
INFORMATIVE
OR
UNCERTAIN
EXAMPLES
FIRST
MACKAY
FREUND
ET
AL
TONG
KOLLER
LINDENBAUM
ET
AL
KAPOOR
ET
AL
MULTI
LEVEL
ACTIVE
LEARNING
CHOOSE
NOT
ONLY
WHICH
IMAGES
TO
LABEL
BUT
AT
WHAT
LEVEL
TO
LABEL
THEM
WEAK
LABELS
INFORMING
ABOUT
PRESENCE
OF
AN
OBJECT
STRONG
LABELS
OUTLINES
DEMARKING
THE
OBJECT
STRONGER
LABELS
INFORMING
ABOUT
LABELS
OF
PARTS
OF
OBJECTS
APPROACH
MULTI
LEVEL
ACTIVE
VISUAL
LEARNING
BEST
USE
OF
MANUAL
RESOURCES
MAY
CALL
FOR
COMBINATION
OF
ANNOTATIONS
AT
DIFFERENT
LEVELS
CHOICE
MUST
BALANCE
COST
OF
VARYING
ANNOTATIONS
WITH
THEIR
INFORMATION
GAIN
REQUIREMENTS
THE
APPROACH
REQUIRES
A
CLASSIFIER
THAT
CAN
DEAL
WITH
ANNOTATIONS
AT
MULTIPLE
LEVELS
AN
ACTIVE
LEARNING
CRITERION
TO
DEAL
WITH
MULTIPLE
TYPES
OF
ANNOTATION
QUERIES
VARIABLE
COST
ASSOCIATED
WITH
DIFFERENT
QUERIES
RESULTS
CATEGORY
AJAXORANGE
CATEGORY
APPLE
CATEGORY
BANANA
COST
COST
COST
CATEGORY
CHECKEREDSCARF
CATEGORY
COKECAN
CATEGORY
DIRTYWORKGLOVES
SAMPLE
LEARNING
CURVES
PER
CLASS
EACH
AVERAGED
OVER
FIVE
TRIALS
MULTI
LEVEL
ACTIVE
SELECTION
PERFORMS
THE
BEST
FOR
MOST
CLASSES
TODAY
WHAT
TO
DO
WHEN
DATA
IS
EXPENSIVE
TO
OBTAIN
USE
DATA
INTELLIGENTLY
ONLY
LABEL
USEFUL
DATA
ACTIVE
LEARNING
CHOOSE
LABELING
QUESTIONS
AT
TRAINING
TIME
DEVELOP
UNSUPERVISED
METHODS
THAT
DON
T
REQUIRE
LABELS
NEXT
TIME
HUMAN
IN
THE
LOOP
RECOGNITION
TEST
IMAGE
TESTING
ADAPTED
FROM
JAMES
HAYS
VISUAL
RECOGNITION
WITH
HUMANS
IN
THE
LOOP
ECCV
CRETE
GREECE
STEVE
BRANSON
CATHERINE
WAH
FLORIAN
SCHROFF
BORIS
BABENKO
SERGE
BELONGIE
PETER
WELINDER
PIETRO
PERONA
FIELD
GUIDES
DIFFICULT
FOR
AVERAGE
USERS
COMPUTER
VISION
DOESN
T
WORK
PERFECTLY
YET
RESEARCH
MOSTLY
ON
BASIC
LEVEL
CATEGORIES
WHAT
TYPE
OF
BIRD
IS
THIS
VISUAL
RECOGNITION
WITH
HUMANS
IN
THE
LOOP
PARAKEET
AUKLET
MOTIVATION
SUPPLEMENT
VISUAL
RECOGNITION
WITH
THE
HUMAN
CAPACITY
FOR
VISUAL
FEATURE
EXTRACTION
TO
TACKLE
DIFFICULT
FINE
GRAINED
RECOGNITION
PROBLEMS
TYPICAL
PROGRESS
IS
VIEWED
AS
INCREASING
DATA
DIFFICULTY
WHILE
MAINTAINING
FULL
AUTONOMY
HERE
THE
AUTHORS
VIEW
PROGRESS
AS
REDUCTION
IN
HUMAN
EFFORT
ON
DIFFICULT
DATA
BRIAN
O
NEILL
CATEGORIES
OF
RECOGNITION
BASIC
LEVEL
SUBORDINATE
PARTS
ATTRIBUTES
EASY
FOR
HUMANS
HARD
FOR
COMPUTERS
HARD
FOR
HUMANS
HARD
FOR
COMPUTERS
EASY
FOR
HUMANS
HARD
FOR
COMPUTERS
VISUAL
QUESTIONS
GAME
HARD
CLASSIFICATION
PROBLEMS
CAN
BE
TURNED
INTO
A
SEQUENCE
OF
EASY
ONES
RECOGNITION
WITH
HUMANS
IN
THE
LOOP
COMPUTERS
REDUCE
NUMBER
OF
REQUIRED
QUESTIONS
HUMANS
DRIVE
UP
ACCURACY
OF
VISION
ALGORITHMS
IMPLEMENTATION
ASSEMBLED
VISUAL
QUESTIONS
ENCOMPASSING
VISUAL
ATTRIBUTES
EXTRACTED
FROM
MECHANICAL
TURK
USERS
ASKED
TO
ANSWER
QUESTIONS
AND
PROVIDE
CONFIDENCE
SCORES
BRIAN
O
NEILL
EXAMPLE
QUESTIONS
EXAMPLE
QUESTIONS
EXAMPLE
QUESTIONS
BASIC
ALGORITHM
INPUT
IMAGE
X
COMPUTER
VISION
MAX
EXPECTED
INFORMATION
GAIN
QUESTION
A
NO
P
C
X
IS
THE
BELLY
BLACK
MAX
EXPECTED
INFORMATION
GAIN
QUESTION
A
YES
P
C
X
IS
THE
BILL
HOOKED
P
C
X
SOME
DEFINITIONS
Q
QN
SET
OF
POSSIBLE
QUESTIONS
BOARD
AI
AI
RI
V
POSSIBLE
ANSWERS
TO
QUESTION
I
POSSIBLE
CONFIDENCE
IN
ANSWER
I
GUESSING
PROBABLY
DEFINITELY
UI
U
T
AI
RI
USER
RESPONSE
HISTORY
OF
USER
RESPONSES
AT
TIME
T
QUESTION
SELECTION
BOARD
SEEK
THE
QUESTION
E
G
WHAT
COLOR
IS
THE
BELLY
OF
THE
BIRD
THAT
GIVES
THE
MAXIMUM
INFORMATION
GAIN
ENTROPY
REDUCTION
GIVEN
THE
IMAGE
AND
THE
SET
OF
PREVIOUS
USER
RESPONSES
I
C
U
X
U
T
P
U
X
U
T
H
C
X
U
U
T
H
C
X
U
T
I
I
I
UI
AI
V
PROBABILITY
OF
OBTAINING
RESPONSE
UI
TO
EVALUATED
QUESTION
GIVEN
IMAGE
AND
RESPONSE
HISTORY
ENTROPY
WHEN
RESPONSE
IS
ADDED
TO
HISTORY
ENTROPY
AT
THIS
ITERATION
BEFORE
RESPONSE
TO
EVALUATED
QUESTION
IS
ADDED
TO
HISTORY
WHERE
H
C
X
U
T
C
P
C
X
U
T
LOG
P
C
X
U
T
BIRDS
DATASET
CLASSES
IMAGES
BINARY
ATTRIBUTES
BLACK
FOOTED
ALBATROSS
GROOVE
BILLED
ANI
PARAKEET
AUKLET
FIELD
SPARROW
VESPER
SPARROW
ARCTIC
TERN
FORSTER
TERN
COMMON
TERN
BAIRD
SPARROW
HENSLOW
SPARROW
RESULTS
USERS
DRIVE
PERFORMANCE
FEWER
QUESTIONS
ASKED
IF
CV
USED
JUST
COMPUTER
VISION
EXAMPLES
USER
INPUT
HELPS
CORRECT
COMPUTER
VISION
MAGNOLIA
WARBLER
COMMON
YELLOWTHROAT
COMPUTER
VISION
COMMON
YELLOWTHROAT
IS
THE
BREAST
PATTERN
SOLID
NO
DEFINITELY
MAGNOLIA
WARBLER
SUMMARY
HUMAN
IN
THE
LOOP
TRAINING
AND
TESTING
TO
MAKE
INTELLIGENT
USE
OF
THE
HUMAN
LABELING
EFFORT
DURING
TRAINING
HAVE
THE
COMPUTER
VISION
ALGORITHM
LEARN
ACTIVELY
BY
SELECTING
THOSE
QUESTIONS
THAT
ARE
MOST
INFORMATIVE
TO
COMBINE
STRENGTHS
OF
HUMAN
AND
IMPERFECT
VISION
ALGORITHMS
USE
A
HUMAN
IN
THE
LOOP
AT
RECOGNITION
TIME
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
DECEMBER
TODAY
DEEP
NEURAL
NETWORKS
ARCHITECTURES
AND
BASIC
OPERATIONS
APPLICATIONS
VISUALIZING
DEEP
NEURAL
NETWORKS
BREAKING
DEEP
LEARNING
PACKAGES
TAKE
MODEL
TRAINED
ON
E
G
IMAGENET
TRAINING
SET
TAKE
OUTPUTS
OF
OR
LAYER
OPTIONAL
FINE
TUNE
FEATURES
AND
OR
CLASSIFIER
ON
NEW
DATASET
TO
TRAIN
MODEL
FROM
SCRATCH
NEED
LOTS
OF
DATA
CLASSIFY
TEST
SET
OF
NEW
DATASET
WHY
NOW
WE
HAVE
LOTS
OF
DATA
AND
DEEP
NETS
CAN
BE
TRAINED
IN
REASONABLE
TIME
WITH
GPUS
LANGUAGE
DEEP
LEARNING
DEEP
NEURAL
NETS
CONVOLUTIONAL
NEURAL
NETS
IMAGE
VIDEO
PIXELS
OBJECT
CLASS
FEATURES
ARE
KEY
TO
RECENT
PROGRESS
IN
RECOGNITION
BUT
FLAWED
WHERE
NEXT
BETTER
CLASSIFIERS
OR
KEEP
BUILDING
MORE
FEATURES
WHAT
ABOUT
LEARNING
THE
FEATURES
LEARN
A
FEATURE
HIERARCHY
ALL
THE
WAY
FROM
PIXELS
TO
CLASSIFIER
EACH
LAYER
EXTRACTS
FEATURES
FROM
THE
OUTPUT
OF
PREVIOUS
LAYER
TRAIN
ALL
LAYERS
JOINTLY
IMAGE
VIDEO
PIXELS
SIMPLE
CLASSIFIER
SHALLOW
VS
DEEP
ARCHITECTURES
TRADITIONAL
RECOGNITION
SHALLOW
ARCHITECTURE
IMAGE
VIDEO
PIXELS
OBJECT
CLASS
IMAGE
VIDEO
PIXELS
DEEP
LEARNING
DEEP
ARCHITECTURE
OBJECT
CLASS
BACKGROUND
PERCEPTRONS
INPUT
WEIGHTS
D
BACKGROUND
MULTI
LAYER
NEURAL
NETWORKS
NONLINEAR
CLASSIFIER
TRAINING
FIND
NETWORK
WEIGHTS
W
TO
MINIMIZE
THE
ERROR
BETWEEN
TRUE
TRAINING
LABELS
YI
AND
ESTIMATED
LABELS
FW
XI
N
E
W
YI
I
FW
XI
MINIMIZATION
CAN
BE
DONE
BY
GRADIENT
DESCENT
PROVIDED
F
IS
DIFFERENTIABLE
THIS
TRAINING
METHOD
IS
CALLED
BACK
PROPAGATION
BIOLOGICAL
NEURON
AND
PERCEPTRONS
A
BIOLOGICAL
NEURON
AN
ARTIFICIAL
NEURON
PERCEPTRON
A
LINEAR
CLASSIFIER
JIA
BIN
HUANG
HUBEL
WIESEL
ARCHITECTURE
D
HUBEL
AND
T
WIESEL
NOBEL
PRIZE
VISUAL
CORTEX
CONSISTS
OF
A
HIERARCHY
OF
SIMPLE
COMPLEX
AND
HYPER
COMPLEX
CELLS
HIERARCHY
OF
FEATURE
DETECTORS
IN
THE
VISUAL
CORTEX
WITH
HIGHER
LEVEL
FEATURES
RESPONDING
TO
PATTERNS
OF
ACTIVATION
IN
LOWER
LEVEL
CELLS
AND
PROPAGATING
ACTIVATION
UPWARDS
TO
STILL
HIGHER
LEVEL
CELLS
LANA
LAZEBNIK
JIA
BIN
HUANG
HUBEL
WIESEL
ARCHITECTURE
AND
MULTI
LAYER
NEURAL
NETWORK
HUBEL
AND
WEISEL
ARCHITECTURE
MULTI
LAYER
NEURAL
NETWORK
A
NON
LINEAR
CLASSIFIER
JIA
BIN
HUANG
TODAY
DEEP
NEURAL
NETWORKS
BACKGROUND
APPLICATIONS
VISUALIZING
DEEP
NEURAL
NETWORKS
BREAKING
DEEP
LEARNING
PACKAGES
NEURAL
NETWORK
WITH
SPECIALIZED
CONNECTIVITY
STRUCTURE
STACK
MULTIPLE
STAGES
OF
FEATURE
EXTRACTORS
HIGHER
STAGES
COMPUTE
MORE
GLOBAL
MORE
INVARIANT
MORE
ABSTRACT
FEATURES
CLASSIFICATION
LAYER
AT
THE
END
ADAPTED
FROM
ROB
FERGUS
CONVOLVE
INPUT
WITH
LEARNED
FILTERS
APPLY
NON
LINEARITY
SPATIAL
POOLING
DOWNSAMPLE
NORMALIZATION
OPTIONAL
SUPERVISED
TRAINING
OF
CONVOLUTIONAL
FILTERS
BY
BACK
PROPAGATING
CLASSIFICATION
ERROR
ADAPTED
FROM
LANA
LAZEBNIK
CONVOLUTION
APPLY
LEARNED
FILTER
WEIGHTS
ONE
FEATURE
MAP
PER
FILTER
STRIDE
CAN
BE
GREATER
THAN
FASTER
LESS
MEMORY
NON
LINEARITY
PER
ELEMENT
INDEPENDENT
OPTIONS
TANH
SIGMOID
EXP
X
RECTIFIED
LINEAR
UNIT
RELU
SIMPLIFIES
BACKPROPAGATION
MAKES
LEARNING
FASTER
AVOIDS
SATURATION
ISSUES
PREFERRED
OPTION
WORKS
WELL
SPATIAL
POOLING
SUM
OR
MAX
NON
OVERLAPPING
OVERLAPPING
REGIONS
ROLE
OF
POOLING
INVARIANCE
TO
SMALL
TRANSFORMATIONS
LARGER
RECEPTIVE
FIELDS
SEE
MORE
OF
INPUT
NORMALIZATION
WITHIN
OR
ACROSS
FEATURE
MAPS
BEFORE
OR
AFTER
SPATIAL
POOLING
FEATURE
MAPS
FEATURE
MAPS
AFTER
CONTRAST
NORMALIZATION
IMAGE
PIXELS
LOWE
IJCV
FEATURE
VECTOR
TRAINING
CONVOLUTIONAL
NEURAL
NETWORKS
BACKPROPAGATION
STOCHASTIC
GRADIENT
DESCENT
INITIALIZATION
TRANSFER
LEARNING
DROPOUT
DATA
AUGMENTATION
RANDOMLY
TURN
OFF
SOME
NEURONS
ALLOWS
INDIVIDUAL
NEURONS
TO
INDEPENDENTLY
BE
RESPONSIBLE
FOR
PERFORMANCE
DROPOUT
A
SIMPLE
WAY
TO
PREVENT
NEURAL
NETWORKS
FROM
OVERFITTING
CREATE
VIRTUAL
TRAINING
SAMPLES
HORIZONTAL
FLIP
RANDOM
CROP
COLOR
CASTING
GEOMETRIC
DISTORTION
DEEP
IMAGE
TODAY
DEEP
NEURAL
NETWORKS
BACKGROUND
ARCHITECTURES
AND
BASIC
OPERATIONS
VISUALIZING
DEEP
NEURAL
NETWORKS
BREAKING
DEEP
LEARNING
PACKAGES
HANDWRITTEN
TEXT
DIGITS
MNIST
ERROR
CIRESAN
ET
AL
ARABIC
CHINESE
CIRESAN
ET
AL
SIMPLER
RECOGNITION
BENCHMARKS
CIFAR
ERROR
WAN
ET
AL
TRAFFIC
SIGN
RECOGNITION
ERROR
VS
FOR
HUMANS
CIRESAN
ET
AL
BUT
UNTIL
RECENTLY
LESS
GOOD
AT
MORE
COMPLEX
DATASETS
CALTECH
FEW
TRAINING
EXAMPLES
DENG
ET
AL
CVPR
MILLION
LABELED
IMAGES
CLASSES
IMAGES
GATHERED
FROM
INTERNET
HUMAN
LABELS
VIA
AMAZON
TURK
CHALLENGE
MILLION
TRAINING
IMAGES
CLASSES
ALEXNET
SIMILAR
FRAMEWORK
TO
LECUN
BUT
BIGGER
MODEL
HIDDEN
LAYERS
UNITS
PARAMS
MORE
DATA
VS
IMAGES
GPU
IMPLEMENTATION
SPEEDUP
OVER
CPU
TRAINED
ON
TWO
GPUS
FOR
A
WEEK
BETTER
REGULARIZATION
FOR
TRAINING
DROPOUT
KRIZHEVSKY
ET
AL
ERROR
TOP
NEXT
BEST
NON
CONVNET
ERROR
IMAGENET
CHALLENGE
BEST
NON
CONVNET
IN
JIA
BIN
HUANG
CALTECH
SAMPLES
PER
CLASS
CALTECH
UCSD
BIRDS
DECAF
SUN
DATASET
DECAF
MIT
INDOOR
SCENES
DATASET
OVERFEAT
J
DONAHUE
Y
J
A
RAZAVIAN
H
AZIZPOUR
J
SULLIVAN
LANA
LAZEBNIK
OBJECT
DETECTION
SYSTEM
OVERVIEW
OUR
SYSTEM
TAKES
AN
INPUT
IMAGE
EXTRACTS
AROUND
BOTTOM
UP
REGION
PROPOSALS
COMPUTES
FEATURES
FOR
EACH
PROPOSAL
USING
A
LARGE
CONVOLUTIONAL
NEURAL
NETWORK
CNN
AND
THEN
CLASSIFIES
EACH
REGION
USING
CLASS
SPECIFIC
LINEAR
SVMS
R
CNN
ACHIEVES
A
MEAN
AVERAGE
PRECISION
MAP
OF
ON
PASCAL
VOC
FOR
COMPARISON
UIJLINGS
ET
AL
REPORT
MAP
USING
THE
SAME
REGION
PROPOSALS
BUT
WITH
A
SPATIAL
PYRAMID
AND
BAG
OF
VISUAL
WORDS
APPROACH
THE
POPULAR
DEFORMABLE
PART
MODELS
PERFORM
AT
LANA
LAZEBNIK
IMPROVEMENT
OF
LEARNING
IN
A
NEW
TASK
THROUGH
THE
TRANSFER
OF
KNOWLEDGE
FROM
A
RELATED
TASK
THAT
HAS
ALREADY
BEEN
LEARNED
JIA
BIN
HUANG
LEARNING
AND
TRANSFERRING
MID
LEVEL
IMAGE
REPRESENTATIONS
USING
CONVOLUTIONAL
NEURAL
NETWORKS
DETECTION
SEGMENTATION
REGRESSION
POSE
ESTIMATION
MATCHING
PATCHES
SYNTHESIS
AND
MANY
MORE
JIA
BIN
HUANG
WHO
TOOK
THIS
PHOTOGRAPH
DEEP
NET
FEATURES
ACHIEVE
ACCURACY
CHANCE
IS
LESS
THAN
HUMAN
PERFORMANCE
IS
METHOD
LEARNS
WHAT
PROTO
OBJECTS
SCENES
AUTHORS
SHOOT
CAN
USE
THIS
TO
DEVELOP
FIELD
GUIDES
FOR
HUMAN
USE
CAN
GENERATE
NOVEL
PHOTOGRAPHS
BY
GIVEN
AUTHOR
SOME
MISCLASSIFICATIONS
DEAD
PHO
TOG
RA
P
HERS
WORK
X
TH
STACH
LOT
I
I
I
I
I
UNIVERSITY
OF
PITTSBURGH
EMTLER
AT
PITT
COMPUTE
R
SCIENTISTS
HAVE
RESURRECTED
FA
MED
PHOTOGRAPHE
RS
WORLD
NEW
T
O
G
TR
O
MDEAD
PHOTO
GRAPHERS
WITH
CONVOLUTIONAL
NEUR
AL
NETWORKS
ARTIN
ANDERSON
THU
NOV
RESEARCHERS
OUT
OF
THE
UNIVERSITY
OF
P
IRTT
SBUR
GH
HAVE
USED
CONVOLUTIONAL
NEURAIL
NETWORKS
CNNS
TO
ID
ENTIFY
TH
E
ST
RAN
G
E
OBSESSIONS
AND
UNIQUE
STYLES
O
F
WEU
KNOWN
PHOTOGRAPHERS
AND
EVEN
TO
GENERATE
NEW
PHOTOGRAPHS
WHKH
ACCORD
WITH
THEIR
UNIQUE
TODAY
DEEP
NEURAL
NETWORKS
BACKGROUND
ARCHITECTURES
AND
BASIC
OPERATIONS
APPLICATIONS
PACKAGES
PATCHES
FROM
VALIDATION
IMAGES
THAT
GIVE
MAXIMAL
ACTIVATION
OF
A
GIVEN
FEATURE
MAP
LAYER
LAYER
VISUALIZING
AND
UNDERSTANDING
CONVOLUTIONAL
NETWORKS
LAYER
LAYER
AND
VISUALIZING
AND
UNDERSTANDING
CONVOLUTIONAL
NETWORKS
LANA
LAZEBNIK
LANA
LAZEBNIK
DIAGNOSING
PROBLEMS
VISUALIZATION
OF
KRIZHEVSKY
ET
AL
ARCHITECTURE
SHOWED
SOME
PROBLEMS
WITH
LAYERS
AND
LARGE
STRIDE
OF
USED
ALTER
ARCHITECTURE
SMALLER
STRIDE
FILTER
SIZE
VISUALIZATIONS
LOOK
BETTER
PERFORMANCE
IMPROVES
LANA
LAZEBNI
ARCHITECTURE
OF
KRIZHEVSKY
ET
AL
LAYERS
TOTAL
TRAINED
ON
IMAGENET
TOP
ERROR
LAYER
CONV
POOL
LANA
LAZEBNIK
REMOVE
TOP
FULLY
CONNECTED
LAYER
LAYER
DROP
MILLION
PARAMETERS
ONLY
DROP
IN
PERFORMANCE
LAYER
CONV
POOL
LANA
LAZEBNIK
REMOVE
BOTH
FULLY
CONNECTED
LAYERS
LAYER
DROP
MILLION
PARAMETERS
DROP
IN
PERFORMANCE
LAYER
CONV
POOL
LANA
LAZEBNIK
NOW
TRY
REMOVING
UPPER
FEATURE
EXTRACTOR
LAYERS
LAYERS
DROP
MILLION
PARAMETERS
DROP
IN
PERFORMANCE
LAYER
CONV
POOL
LAYER
CONV
POOL
LAYER
CONV
POOL
LANA
LAZEBNIK
INPUT
IMAGE
NOW
TRY
REMOVING
UPPER
FEATURE
EXTRACTOR
LAYERS
FULLY
CONNECTED
LAYERS
NOW
ONLY
LAYERS
DROP
IN
PERFORMANCE
LAYER
CONV
POOL
LAYER
CONV
POOL
DEPTH
OF
NETWORK
IS
KEY
LAYER
CONV
POOL
LANA
LAZEBNIK
INPUT
IMAGE
TAPPING
OFF
FEATURES
AT
EACH
LAYER
PLUG
FEATURES
FROM
EACH
LAYER
INTO
LINEAR
SVM
FEATURES
ARE
NEURON
ACTIVATIONS
AT
THAT
LEVEL
ADAPTED
FROM
MATT
ZEILER
JIA
BIN
HUANG
INTRIGUING
PROPERTIES
OF
NEURAL
NETWORKS
JIA
BIN
HUANG
DEEP
NEURAL
NETWORKS
ARE
EASILY
FOOLED
HIGH
CONFIDENCE
PREDICTIONS
FOR
UNRECOGNIZABLE
IMAGES
FOOLING
A
LINEAR
CLASSIFIER
TO
FOOL
A
LINEAR
CLASSIFIER
ADD
A
SMALL
MULTIPLE
OF
THE
WEIGHT
VECTOR
TO
THE
TRAINING
EXAMPLE
X
X
ΑW
JIA
BIN
HUANG
TODAY
DEEP
NEURAL
NETWORKS
BACKGROUND
ARCHITECTURES
AND
BASIC
OPERATIONS
APPLICATIONS
VISUALIZING
DEEP
NEURAL
NETWORKS
BREAKING
DEEP
LEARNING
NEW
VERSION
OF
DECAF
THINGS
TO
REMEMBER
OVERVIEW
NEUROSCIENCE
PERCEPTRON
MULTI
LAYER
NEURAL
NETWORKS
CONVOLUTIONAL
NEURAL
NETWORK
CNN
CONVOLUTION
NONLINEARITY
MAX
POOLING
CNN
FOR
CLASSIFICATION
AND
BEYOND
UNDERSTANDING
AND
VISUALIZING
CNN
FIND
IMAGES
THAT
MAXIMIZE
SOME
CLASS
SCORES
VISUALIZE
INDIVIDUAL
NEURON
ACTIVATION
AND
INPUT
PATTERNS
BREAKING
CNNS
TRAINING
CNN
DROPOUT
DATA
AUGMENTATION
TRANSFER
LEARNING
USING
CNNS
FOR
YOUR
OWN
TASK
BASIC
FIRST
STEP
TRY
THE
PRE
TRAINED
CAFFENET
LAYERS
AS
FEATURES
ADAPTED
FROM
JIA
BIN
HUANG
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
DECEMBER
TODAY
TRACKING
TRACKING
HOW
AN
OBJECT
MOVES
EXAMPLES
OF
TRACKING
APPLICATIONS
PROBABILISTIC
APPROACH
TO
TRACKING
KALMAN
FILTERS
TRACKING
CHALLENGES
TA
WILL
HAVE
GRADED
ON
TUESDAY
DUE
NEXT
THURSDAY
USE
A
RANDOM
SAMPLE
OF
IMAGES
TO
TRAIN
THE
ATTRIBUTE
CLASSIFIERS
EVEN
IMPOVERISHED
MOTION
DATA
CAN
EVOKE
A
STRONG
PERCEPT
JOHANSSON
VISUAL
PERCEPTION
OF
BIOLOGICAL
MOTION
AND
A
MODEL
FOR
ITS
ANALYSIS
PERCEPTION
AND
PSYCHOPHYSICS
TRAFFIC
SOCCER
FACE
BODY
EYE
GAZE
HOW
WOULD
YOU
DO
IT
BODY
POSE
TRACKING
ACTIVITY
RECOGNITION
CENSUSING
A
BAT
POPULATION
VIDEO
BASED
INTERFACES
MEDICAL
APPS
SURVEILLANCE
EXTRACT
VISUAL
FEATURES
CORNERS
TEXTURED
AREAS
AND
TRACK
THEM
OVER
MULTIPLE
FRAMES
MANY
PROBLEMS
E
G
STRUCTURE
FROM
MOTION
REQUIRE
MATCHING
POINTS
MANY
CHALLENGES
E
G
POINTS
MAY
CHANGE
APPEARANCE
OVER
TIME
POINTS
MAY
APPEAR
OR
DISAPPEAR
I
X
Y
T
I
X
Y
T
GIVEN
TWO
SUBSEQUENT
FRAMES
ESTIMATE
THE
POINT
TRANSLATION
THE
LUCAS
KANADE
OPTICAL
FLOW
METHOD
ASSUMES
BRIGHTNESS
CONSTANCY
PROJECTION
OF
THE
SAME
POINT
LOOKS
THE
SAME
IN
EVERY
FRAME
SMALL
MOTION
POINTS
DO
NOT
MOVE
VERY
FAR
SPATIAL
COHERENCE
POINTS
MOVE
LIKE
THEIR
NEIGHBORS
RECOGNIZING
HUMAN
FACIAL
EXPRESSION
BY
YASER
YACOOB
LARRY
DAVIS
KRISTEN
GRAUMAN
FEATURES
OPTICAL
FLOW
WITHIN
A
REGION
OF
INTEREST
THE
PIXEL
MAN
EFROS
BERG
MORI
MALIK
KRISTEN
GRAUMAN
INTERESTING
POINT
BEST
MATCHING
NEIGHBORHOOD
SEARCH
WINDOW
TIME
T
TIME
T
SEARCH
WINDOW
IS
CENTERED
AT
THE
POINT
WHERE
WE
LAST
SAW
THE
FEATURE
IN
IMAGE
BEST
MATCH
POSITION
WHERE
WE
HAVE
THE
HIGHEST
NORMALIZED
CROSS
CORRELATION
VALUE
KRISTEN
GRAUMAN
VIDEO
INTERFACE
USE
FEATURE
TRACKING
AS
MOUSE
REPLACEMENT
USER
CLICKS
ON
THE
FEATURE
TO
BE
TRACKED
TAKE
THE
PIXEL
SQUARE
OF
THE
FEATURE
IN
THE
NEXT
IMAGE
DO
A
SEARCH
TO
FIND
THE
REGION
WITH
THE
HIGHEST
CORRELATION
MOVE
THE
MOUSE
POINTER
ACCORDINGLY
REPEAT
IN
THE
BACKGROUND
EVERY
OF
A
SECOND
JAMES
GIPS
AND
MARGRIT
BETKE
KRISTEN
GRAUMAN
SPECIALIZED
SOFTWARE
FOR
COMMUNICATION
GAMES
JAMES
GIPS
AND
MARGRIT
BETKE
FEATURE
BASED
MATCHING
FOR
MOTION
FOR
A
DISCRETE
MATCHING
SEARCH
WHAT
ARE
THE
TRADEOFFS
OF
THE
CHOSEN
SEARCH
WINDOW
SIZE
WHICH
PATCHES
TO
TRACK
SELECT
INTEREST
POINTS
E
G
CORNERS
WHERE
SHOULD
THE
SEARCH
WINDOW
BE
PLACED
NEAR
MATCH
AT
PREVIOUS
FRAME
MORE
GENERALLY
TAKING
INTO
ACCOUNT
THE
EXPECTED
DYNAMICS
OF
THE
OBJECT
THINGS
THAT
MAKE
VISUAL
TRACKING
DIFFICULT
SMALL
FEW
VISUAL
FEATURES
ERRATIC
MOVEMENTS
MOVING
VERY
QUICKLY
OCCLUSIONS
LEAVING
AND
COMING
BACK
SURROUNDING
SIMILAR
LOOKING
OBJECTS
TRACKING
BY
REPEATED
DETECTION
WORKS
WELL
IF
OBJECT
IS
EASILY
DETECTABLE
E
G
FACE
OR
COLORED
GLOVE
AND
THERE
IS
ONLY
ONE
NEED
SOME
WAY
TO
LINK
UP
DETECTIONS
BEST
YOU
CAN
DO
IF
YOU
CAN
T
PREDICT
MOTION
KEY
IDEA
BASED
ON
A
MODEL
OF
EXPECTED
MOTION
PREDICT
WHERE
OBJECTS
WILL
OCCUR
IN
NEXT
FRAME
BEFORE
EVEN
SEEING
THE
IMAGE
RESTRICT
SEARCH
FOR
THE
OBJECT
MEASUREMENT
NOISE
IS
REDUCED
BY
TRAJECTORY
SMOOTHNESS
ROBUSTNESS
TO
MISSING
OR
WEAK
OBSERVATIONS
ASSUMPTIONS
CAMERA
IS
NOT
MOVING
INSTANTLY
TO
NEW
VIEWPOINT
OBJECTS
DO
NOT
DISAPPEAR
AND
REAPPEAR
IN
DIFFERENT
PLACES
IN
THE
SCENE
T
T
T
T
DETECTION
WE
DETECT
THE
OBJECT
INDEPENDENTLY
IN
EACH
FRAME
AND
CAN
RECORD
ITS
POSITION
OVER
TIME
E
G
BASED
ON
BLOB
CENTROID
OR
DETECTION
WINDOW
COORDINATES
TRACKING
WITH
DYNAMICS
WE
USE
IMAGE
MEASUREMENTS
TO
ESTIMATE
POSITION
OF
OBJECT
BUT
ALSO
INCORPORATE
POSITION
PREDICTED
BY
DYNAMICS
I
E
OUR
EXPECTATION
OF
OBJECT
MOTION
PATTERN
TIME
T
TIME
T
BELIEF
MEASUREMENT
CORRECTED
PREDICTION
TIME
T
TIME
T
STATE
X
THE
ACTUAL
STATE
OF
THE
MOVING
OBJECT
THAT
WE
WANT
TO
ESTIMATE
BUT
CANNOT
OBSERVE
STATE
COULD
BE
ANY
COMBINATION
OF
POSITION
POSE
VIEWPOINT
VELOCITY
ACCELERATION
ETC
OBSERVATIONS
Y
OUR
ACTUAL
MEASUREMENT
OR
OBSERVATION
OF
STATE
X
WHICH
CAN
BE
VERY
NOISY
AT
EACH
TIME
T
THE
STATE
CHANGES
TO
XT
AND
WE
GET
A
NEW
OBSERVATION
YT
AMIN
SADEGHI
OUR
GOAL
RECOVER
MOST
LIKELY
STATE
XT
GIVEN
ALL
OBSERVATIONS
SEEN
SO
FAR
I
E
YT
KNOWLEDGE
ABOUT
DYNAMICS
OF
STATE
TRANSITIONS
P
X
YT
T
P
X
YT
T
CORRECTION
COMPUTE
AN
UPDATED
ESTIMATE
OF
THE
STATE
FROM
PREDICTION
AND
MEASUREMENTS
P
X
YT
YT
YT
YT
ONLY
THE
IMMEDIATE
PAST
MATTERS
P
X
X
T
DYNAMICS
MODEL
ONLY
THE
IMMEDIATE
PAST
MATTERS
P
X
X
T
DYNAMICS
MODEL
MEASUREMENTS
DEPEND
ONLY
ON
THE
CURRENT
STATE
P
Y
X
X
T
YT
X
OBS
EL
ONLY
THE
IMMEDIATE
PAST
MATTERS
P
X
X
T
DYNAMICS
MODEL
MEASUREMENTS
DEPEND
ONLY
ON
THE
CURRENT
STATE
P
Y
X
X
T
YT
X
P
Y
X
OBSERVATION
MODEL
WE
HAVE
MODELS
FOR
LIKELIHOOD
OF
NEXT
STATE
GIVEN
CURRENT
STATE
DYNAMICS
MODEL
P
X
X
LIKELIHOOD
OF
OBSERVATION
GIVEN
THE
STATE
OBSERVATION
OR
MEASUREMENT
MODEL
P
Y
X
WE
WANT
TO
RECOVER
FOR
EACH
T
P
X
Y
NOTATION
REMINDER
X
N
Μ
Σ
RANDOM
VARIABLE
WITH
GAUSSIAN
PROBABILITY
DISTRIBUTION
THAT
HAS
THE
MEAN
VECTOR
Μ
AND
COVARIANCE
MATRIX
Σ
X
AND
Μ
ARE
D
DIMENSIONAL
Σ
IS
D
X
D
D
D
IF
X
IS
D
WE
JUST
HAVE
ONE
Σ
PARAMETER
THE
VARIANCE
KRISTEN
GRAUMAN
DYNAMICS
AND
OBSERVATION
MODELS
DYNAMICS
MODEL
REPRESENTS
EVOLUTION
OF
STATE
OVER
TIME
P
XT
XT
OBSERVATION
OR
MEASUREMENT
MODEL
AT
EVERY
TIME
STEP
WE
GET
A
NOISY
MEASUREMENT
OF
THE
STATE
P
YT
X
T
THIS
IS
HOW
THESE
MODELS
ARE
DEFINED
IN
THE
KALMAN
FILTER
THE
KALMAN
FILTER
LINEAR
DYNAMICS
MODEL
STATE
UNDERGOES
LINEAR
TRANSFORMATION
PLUS
GAUSSIAN
NOISE
OBSERVATION
MODEL
MEASUREMENT
IS
LINEARLY
TRANSFORMED
STATE
PLUS
GAUSSIAN
NOISE
THE
PREDICTED
CORRECTED
STATE
DISTRIBUTIONS
ARE
GAUSSIAN
YOU
ONLY
NEED
TO
MAINTAIN
THE
MEAN
AND
COVARIANCE
THE
CALCULATIONS
ARE
EASY
TIME
STATE
VECTOR
POSITION
P
AND
VELOCITY
V
X
PT
PT
T
V
T
X
T
PT
NOISE
T
V
T
MEASUREMENT
IS
POSITION
ONLY
Y
MX
NOISE
PT
NOISE
T
T
V
T
BASE
CASE
START
WITH
INITIAL
PRIOR
THAT
PREDICTS
STATE
IN
ABSENCE
OF
ANY
EVIDENCE
P
FOR
THE
FIRST
FRAME
CORRECT
THIS
GIVEN
THE
FIRST
MEASUREMENT
BASE
CASE
START
WITH
INITIAL
PRIOR
THAT
PREDICTS
STATE
IN
ABSENCE
OF
ANY
EVIDENCE
P
FOR
THE
FIRST
FRAME
CORRECT
THIS
GIVEN
THE
FIRST
MEASUREMENT
P
X
Y
Y
P
X
P
X
P
BASE
CASE
START
WITH
INITIAL
PRIOR
THAT
PREDICTS
STATE
IN
ABSENCE
OF
ANY
EVIDENCE
P
FOR
THE
FIRST
FRAME
CORRECT
THIS
GIVEN
THE
FIRST
MEASUREMENT
GIVEN
CORRECTED
ESTIMATE
FOR
FRAME
T
PREDICT
FOR
FRAME
T
P
X
YT
OBSERVE
YT
CORRECT
FOR
FRAME
T
P
X
T
YT
YT
PREDICT
CORRECT
PREDICTION
INVOLVES
REPRESENTING
P
X
T
GIVEN
T
T
P
X
Y
T
P
X
X
T
T
T
LAW
OF
TOTAL
PROBABILITY
P
X
Y
T
P
X
X
T
T
T
P
X
T
X
T
YT
P
X
T
YT
DX
T
P
X
Y
T
P
X
X
T
T
T
P
X
T
YT
DX
T
P
X
T
YT
DX
T
INDEPENDENCE
ASSUMPTION
ONLY
IMMEDIATE
PAST
STATE
MATTERS
ADAPTED
FROM
AMIN
SADEGHI
P
X
Y
T
P
X
X
T
T
T
P
X
T
X
T
YT
P
X
T
YT
DX
T
P
X
T
X
T
P
X
T
YT
DX
T
DYNAMICS
MODEL
CORRECTED
ESTIMATE
FROM
PREVIOUS
STEP
P
CORRECTION
CORRECTION
INVOLVES
COMPUTING
P
X
Y
GIVEN
PREDICTED
VALUE
P
P
X
T
P
Y
YT
P
X
T
YT
P
YT
X
T
P
X
T
YT
P
YT
YT
CORRECTION
CORRECTION
INVOLVES
COMPUTING
P
X
Y
GIVEN
PREDICTED
VALUE
P
X
Y
T
P
X
Y
Y
P
YT
X
T
YT
P
X
Y
Y
P
YT
YT
T
T
P
YT
X
T
P
X
T
YT
P
YT
YT
P
YT
X
T
P
X
T
YT
P
YT
X
T
P
X
T
YT
DX
T
CONDITIONING
ON
XT
AMIN
SADEGHI
CORRECTION
INVOLVES
COMPUTING
P
X
Y
GIVEN
PREDICTED
VALUE
P
X
Y
T
P
X
Y
Y
P
YT
X
T
YT
P
X
Y
Y
P
YT
YT
T
T
P
YT
X
T
P
X
T
YT
OBSERVATION
P
YT
YT
PREDICTED
MODEL
P
YT
X
T
P
X
T
YT
ESTIMATE
P
YT
X
T
P
X
T
YT
DX
T
NORMALIZATION
FACTOR
AMIN
SADEGHI
PREDICTION
KNOW
CORRECTED
STATE
FROM
PREVIOUS
TIME
STEP
AND
ALL
MEASUREMENTS
UP
TO
EXCLUDING
THE
CURRENT
ONE
PREDICT
DISTRIBUTION
OVER
NEXT
STATE
ADVANCES
T
P
X
RECEIVE
MEASUREMENT
CORRECTION
KNOW
PREDICTION
OF
STATE
AND
NEXT
MEASUREMENT
UPDATE
DISTRIBUTION
OVER
CURRENT
STATE
P
X
Y
PREDICTION
P
XT
YT
P
XT
XT
P
XT
YT
DXT
DYNAMICS
MODEL
CORRECTED
ESTIMATE
FROM
PREVIOUS
STEP
CORRECTION
OBSERVATION
MODEL
PREDICTED
ESTIMATE
P
X
Y
Y
P
YT
XT
P
XT
YT
T
T
P
YT
XT
P
XT
YT
DXT
KALMAN
FILTER
PROCESSING
KALMAN
FILTER
PROCESSING
KALMAN
FILTER
PROCESSING
KALMAN
FILTER
PROCESSING
HOW
TO
PREDICT
IF
MISSING
A
DETECTION
IN
THE
CURRENT
FRAME
PREDICTION
ONLY
DEPENDS
ON
PREVIOUS
FRAMES
HOW
TO
CORRECT
IF
MISSING
DETECTION
IN
THE
CURRENT
FRAME
DON
T
CORRECT
JUST
USE
PREDICTION
WHAT
IF
WE
HAVE
TWO
DETECTIONS
IN
A
FRAME
DETERMINE
WHICH
OF
THE
TWO
IS
MORE
LIKELY
CORRECT
EXAMPLE
KALMAN
FILTER
GROUND
TRUTH
OBSERVATION
CORRECTION
AMIN
SADEGHI
A
BAT
CENSUS
KRISTEN
GRAUMAN
TRACKING
ISSUES
INITIALIZATION
MANUAL
BACKGROUND
SUBTRACTION
DETECTION
INITIALIZATION
OBTAINING
OBSERVATION
AND
DYNAMICS
MODELS
OBSERVATION
MODEL
MATCH
A
TEMPLATE
OR
USE
A
TRAINED
DETECTOR
DYNAMICS
MODEL
USUALLY
SPECIFY
USING
DOMAIN
KNOWLEDGE
INITIALIZATION
OBTAINING
OBSERVATION
AND
DYNAMICS
MODELS
UNCERTAINTY
OF
PREDICTION
VS
CORRECTION
IF
THE
DYNAMICS
MODEL
IS
TOO
STRONG
WILL
END
UP
IGNORING
THE
DATA
IF
THE
OBSERVATION
MODEL
IS
TOO
STRONG
TRACKING
IS
REDUCED
TO
REPEATED
DETECTION
TOO
STRONG
DYNAMICS
MODEL
TOO
STRONG
OBSERVATION
MODEL
INITIALIZATION
GETTING
OBSERVATION
AND
DYNAMICS
MODELS
PREDICTION
VS
CORRECTION
DATA
ASSOCIATION
WHEN
TRACKING
MULTIPLE
OBJECTS
NEED
TO
ASSIGN
RIGHT
OBJECTS
TO
RIGHT
TRACKS
PARTICLE
FILTERS
GOOD
FOR
THIS
INITIALIZATION
GETTING
OBSERVATION
AND
DYNAMICS
MODELS
PREDICTION
VS
CORRECTION
DATA
ASSOCIATION
DRIFT
ERRORS
CAN
ACCUMULATE
OVER
TIME
D
RAMANAN
D
FORSYTH
AND
A
ZISSERMAN
TPAMI
TRACKING
OBJECTS
DETECTION
PREDICTION
PROBABILISTIC
FRAMEWORK
PREDICT
NEXT
STATE
UPDATE
CURRENT
STATE
BASED
ON
OBSERVATION
TWO
SIMPLE
BUT
EFFECTIVE
METHODS
KALMAN
FILTERS
GAUSSIAN
DISTRIBUTION
PARTICLE
FILTERS
MULTIMODAL
DISTRIBUTION
FOR
MULTIPLE
OBJECTS
CHALLENGES
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
DECEMBER
TODAY
HUMAN
POSE
AND
ACTIONS
INTRODUCTION
ESTIMATING
HUMAN
POSE
RECOGNIZING
HUMAN
ACTIONS
USING
SPECIALIZED
FEATURES
USING
POSE
USING
OBJECTS
FROM
EGO
CENTRIC
VIDEO
NEXT
TIME
LAST
CLASS
REVIEW
FOR
THE
FINAL
EXAM
OMETS
BY
WEDNESDAY
NIGHT
POST
ON
PIAZZA
QUESTIONS
OR
ANYTHING
YOU
WANT
ME
TO
REVIEW
FOR
PARTICIPATION
CREDIT
EXTRA
OFFICE
HOURS
ON
FRIDAY
FINAL
EXAM
MONDAY
DEC
SAME
ROOM
SENNOTT
SQUARE
SIMILAR
TO
MIDTERM
EXAM
MOSTLY
SHORT
QUESTIONS
AND
A
FEW
PROBLEMS
BUT
LONGER
POINTS
WILL
ONLY
COVER
TOPICS
DISCUSSED
AFTER
MIDTERM
BUT
SOME
OF
THESE
USE
TOPICS
FROM
FIRST
HALF
MEAN
MEDIAN
MAX
DUE
THURSDAY
SEE
PIAZZA
FOR
CORRECTION
ABOUT
HOW
TO
GET
PROBABILITIES
FOR
PART
III
TENTATIVE
GRADES
ENTERED
ON
COURSEWEB
MEDIAN
IS
ACTION
A
TRANSITION
FROM
ONE
STATE
TO
ANOTHER
WHO
IS
THE
ACTOR
HOW
IS
THE
STATE
OF
THE
ACTOR
CHANGING
WHAT
IF
ANYTHING
IS
BEING
ACTED
ON
HOW
IS
THAT
THING
CHANGING
WHAT
IS
THE
PURPOSE
OF
THE
ACTION
IF
ANY
COULD
BE
MORE
OR
LESS
COMPLEX
NO
UNIVERSAL
TERMINOLOGY
BUT
APPROXIMATELY
ACTIONS
ATOMIC
MOTION
PATTERNS
OFTEN
GESTURE
LIKE
SINGLE
CLEAR
CUT
TRAJECTORY
SINGLE
NAMEABLE
BEHAVIOR
E
G
SIT
WAVE
ARMS
ACTIVITY
SERIES
OR
COMPOSITION
OF
ACTIONS
E
G
INTERACTIONS
BETWEEN
PEOPLE
EVENT
COMBINATION
OF
ACTIVITIES
OR
ACTIONS
E
G
A
FOOTBALL
GAME
A
TRAFFIC
ACCIDENT
CATEGORIES
WALKING
HAMMERING
DANCING
SKIING
SITTING
DOWN
STANDING
UP
JUMPING
POSES
NOUNS
AND
PREDICATES
MAN
SWINGS
HAMMER
MAN
HITS
NAIL
W
HAMMER
MOTION
POSE
HELD
OBJECTS
NEARBY
OBJECTS
TODAY
HUMAN
POSE
AND
ACTIONS
INTRODUCTION
RECOGNIZING
HUMAN
ACTIONS
USING
SPECIALIZED
FEATURES
USING
POSE
USING
OBJECTS
FROM
EGO
CENTRIC
VIDEO
JAMIE
SHOTTON
ANDREW
FITZGIBBON
MAT
COOK
TOBY
SHARP
MARK
FINOCCHIO
RICHARD
MOORE
ALEX
KIPMAN
ANDREW
BLAKE
BEST
PAPER
AWARD
AT
CVPR
ADAPTED
FROM
JAMIE
SHOTTON
RECOGNIZE
LARGE
VARIETY
OF
HUMAN
POSES
ALL
SHAPES
SIZES
LIMITED
COMPUTE
BUDGET
SUPER
REAL
TIME
ON
XBOX
TO
ALLOW
GAMES
TO
RUN
CONCURRENTLY
NO
TEMPORAL
INFORMATION
FRAME
BY
FRAME
LOCAL
POSE
ESTIMATE
OF
PARTS
EACH
PIXEL
EACH
BODY
JOINT
TREATED
INDEPENDENTLY
REDUCED
TRAINING
DATA
AND
COMPUTATION
TIME
VERY
FAST
SIMPLE
DEPTH
IMAGE
FEATURES
PARALLEL
DECISION
FOREST
CLASSIFIER
CAPTURE
DEPTH
IMAGE
REMOVE
BG
INFER
BODY
PARTS
PER
PIXEL
CLUSTER
PIXELS
TO
HYPOTHESIZE
BODY
JOINT
POSITIONS
FIT
MODEL
TRACK
SKELETON
COMPUTE
P
CI
WI
PIXELS
I
X
Y
BODY
PART
CI
IMAGE
WINDOW
WI
DISCRIMINATIVE
APPROACH
LEARN
CLASSIFIER
P
CI
WI
FROM
TRAINING
DATA
TRAIN
INVARIANCE
TO
DEPTH
COMPARISONS
VERY
FAST
TO
COMPUTE
𝑑𝐼
X
Δ
TOY
EXAMPLE
DISTINGUISH
LEFT
L
AND
RIGHT
R
SIDES
OF
THE
BODY
TO
CLASSIFY
PIXEL
X
START
HERE
FΘ
I
X
NO
YES
FΘ
I
X
NO
YES
P
C
L
R
P
C
L
R
P
C
L
R
INPUT
DEPTH
GROUND
TRUTH
PARTS
INFERRED
PARTS
SOFT
DEPTH
DEPTH
OF
TREES
DEPTH
OF
TREES
AMIT
GEMAN
BREIMAN
GEURTS
ET
AL
TREE
𝐼
X
𝐼
X
TREE
T
PT
C
C
C
C
TRAINED
ON
DIFFERENT
RANDOM
SUBSET
OF
IMAGES
BAGGING
HELPS
AVOID
OVER
FITTING
𝑇
AVERAGE
TREE
POSTERIORS
𝑃
𝑇
𝑃𝑡
𝑐
𝐼
X
𝑡
NUMBER
OF
TREES
INPUT
DEPTH
INFERRED
BODY
PARTS
FRONT
VIEW
SIDE
VIEW
TOP
VIEW
JAMIE
SHOTTON
INFERRED
JOINT
POSITIONS
MODES
FOUND
USING
MEAN
SHIFT
NO
TRACKING
OR
SMOOTHING
INPUT
DEPTH
INFERRED
BODY
PARTS
FRONT
VIEW
SIDE
VIEW
TOP
VIEW
JAMIE
SHOTTON
INFERRED
JOINT
POSITIONS
MODES
FOUND
USING
MEAN
SHIFT
NO
TRACKING
OR
SMOOTHING
TODAY
HUMAN
POSE
AND
ACTIONS
INTRODUCTION
ESTIMATING
HUMAN
POSE
USING
POSE
USING
OBJECTS
FROM
EGO
CENTRIC
VIDEO
TRACKED
POINTS
SPACE
TIME
INTEREST
POINTS
CORNER
DETECTORS
IN
SPACE
TIME
TALK
ON
PHONE
GET
OUT
OF
CAR
SPACE
TIME
INTEREST
POINT
DETECTORS
DESCRIPTORS
HOG
HOF
PYRAMID
HISTOGRAMS
SVMS
WITH
CHI
SQUARED
KERNEL
SPATIO
TEMPORAL
BINNING
INTEREST
POINTS
TODAY
HUMAN
POSE
AND
ACTIONS
INTRODUCTION
ESTIMATING
HUMAN
POSE
RECOGNIZING
HUMAN
ACTIONS
USING
SPECIALIZED
FEATURES
FROM
EGO
CENTRIC
VIDEO
INTEGRATED
REASONING
HUMAN
POSE
ESTIMATION
INTEGRATED
REASONING
HUMAN
POSE
ESTIMATION
OBJECT
DETECTION
TENNIS
RACKET
INTEGRATED
REASONING
HUMAN
POSE
ESTIMATION
OBJECT
DETECTION
ACTION
CATEGORIZATION
TENNIS
RACKET
HEAD
TORSO
ACTIVITY
TENNIS
FOREHAND
HUMAN
POSE
ESTIMATION
IS
CHALLENGING
DIFFICULT
PART
APPEARANCE
SELF
OCCLUSION
IMAGE
REGION
LOOKS
LIKE
A
BODY
PART
HUMAN
POSE
ESTIMATION
IS
CHALLENGING
HUMAN
POSE
ESTIMATION
OBJECT
DETECTION
FACILITATE
GIVEN
THE
OBJECT
IS
DETECTED
OBJECT
DETECTION
IS
CHALLENGING
VIOLA
JONES
LAMPERT
ET
AL
DIVVALA
ET
AL
VEDALDI
ET
AL
OBJECT
DETECTION
IS
CHALLENGING
VIOLA
JONES
LAMPERT
ET
AL
DIVVALA
ET
AL
VEDALDI
ET
AL
FACILITATE
GIVEN
THE
POSE
IS
ESTIMATED
LEARNING
RESULTS
TENNIS
FOREHAND
TENNIS
SERVE
VOLLEYBALL
SMASH
ACTIVITY
CLASSIFICATION
RESULTS
OUR
MODEL
GUPTA
ET
AL
BAG
OF
WORDS
SIFT
SVM
CRICKET
SHOT
TENNIS
FOREHAND
TODAY
HUMAN
POSE
AND
ACTIONS
INTRODUCTION
ESTIMATING
HUMAN
POSE
RECOGNIZING
HUMAN
ACTIONS
USING
SPECIALIZED
FEATURES
USING
POSE
USING
OBJECTS
DETECTING
ACTIVITIES
OF
DAILY
LIVING
IN
FIRST
PERSON
CAMERA
VIEWS
HAMED
PIRSIAVASH
DEVA
RAMANAN
CVPR
HAMED
PIRSIAVASH
MOTIVATION
A
SAMPLE
VIDEO
OF
ACTIVITIES
OF
DAILY
LIVING
TELE
REHABILITATION
LONG
TERM
AT
HOME
MONITORING
KOPP
ET
AL
ARCH
OF
PHYSICAL
MEDICINE
AND
REHABILITATION
CATZ
ET
AL
SPINAL
CORD
LIFE
LOGGING
SO
FAR
MOSTLY
WRITE
ONLY
MEMORY
THIS
IS
THE
RIGHT
TIME
FOR
COMPUTER
VISION
COMMUNITY
TO
GET
INVOLVED
GEMMELL
ET
AL
MYLIFEBITS
A
PERSONAL
DATABASE
FOR
EVERYTHING
COMMUNICATIONS
OF
THE
ACM
HODGES
ET
AL
SENSECAM
A
RETROSPECTIVE
MEMORY
AID
UBICOMP
WEARABLE
ADL
DETECTION
IT
IS
EASY
TO
COLLECT
NATURAL
DATA
LOW
LEVEL
FEATURES
WEAK
SEMANTICS
HIGH
LEVEL
FEATURES
STRONG
SEMANTICS
SPACE
TIME
INTEREST
POINTS
LAPTEV
IJCV
HUMAN
POSE
DIFFICULTIES
OF
POSE
DETECTORS
ARE
NOT
ACCURATE
ENOUGH
NOT
USEFUL
IN
FIRST
PERSON
CAMERA
VIEWS
HIGH
LEVEL
FEATURES
STRONG
SEMANTICS
LONG
SCALE
TEMPORAL
STRUCTURE
CLASSIC
DATA
BOXING
WEARABLE
DATA
MAKING
TEA
APPEARANCE
FEATURE
BAG
OF
OBJECTS
FRIDGE
STOVE
TV
VIDEO
CLIP
FRIDGE
BAG
OF
DETECTED
OBJECTS
STOVE
TV
TEMPORAL
PYRAMID
COARSE
TO
FINE
CORRESPONDENCE
MATCHING
WITH
A
MULTI
LAYER
PYRAMID
INSPIRED
BY
SPATIAL
PYRAMID
CVPR
AND
PYRAMID
MATCH
KERNELS
ICCV
VIDEO
CLIP
TEMPORAL
PYRAMID
DESCRIPTOR
HAMED
PIRSIAVASH
TIME
ACCURACY
ON
ACTION
CATEGORIES
OUR
MODEL
STIP
BASELINE
HAMED
PIRSIAVASH
SUMMARY
HUMAN
ACTIONS
ACTION
RECOGNITION
STILL
AN
OPEN
PROBLEM
HOW
TO
REPRESENT
ACTIONS
TYPES
OF
DATA
ATOMIC
AND
MORE
COMPLEX
ACTIONS
EGO
CENTRIC
VIDEO
COMMON
REPRESENTATIONS
SPACE
TIME
INTEREST
POINTS
POSE
OBJECTS
AND
TEMPORAL
PYRAMIDS
OF
OBJECTS
POSE
CAN
BE
APPROACHED
AS
A
CLASSIFICATION
PROBLEM
USING
DEPTH
DATA
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
DECEMBER
TODAY
OMETS
MIN
REVIEW
QUIZ
RETURNED
AVERAGE
WAS
SOLUTIONS
POSTED
ON
COURSEWEB
EXTENDED
TO
FRIDAY
TOMORROW
AT
HWS
SUBMITTED
ON
TUESDAY
HAVE
BEEN
GRADED
ASK
ME
IF
YOU
NEED
TO
KNOW
HOW
MANY
FREE
LATE
DAYS
LEFT
ANYONE
WORKING
ON
PART
I
DESCRIBE
EACH
SEARCH
FINAL
EXAM
MONDAY
DEC
REVIEW
QUIZZES
MAKE
SURE
YOU
KNOW
AND
UNDERSTAND
SVMS
I
AM
NOT
REVIEWING
TRACKING
AND
ACTIONS
POSE
TODAY
SINCE
WE
COVERED
THEM
RECENTLY
BUT
THEY
WILL
BE
ON
THE
EXAM
EXTRA
OFFICE
HOURS
TOMORROW
AT
RECOGNITION
SOME
TRANSLATIONS
FEATURE
VECTOR
DESCRIPTOR
REPRESENTATION
RECOGNITION
OFTEN
INVOLVES
CLASSIFICATION
CLASSES
CATEGORIES
HENCE
CLASSIFICATION
CATEGORIZATION
TRAINING
LEARNING
A
MODEL
E
G
CLASSIFIER
HAPPENS
AT
TRAINING
TIME
FROM
TRAINING
DATA
CLASSIFICATION
PREDICTION
HAPPENS
AT
TEST
TIME
GROUND
TRUTH
LABELS
FOR
TEST
IMAGES
THEIR
TRUE
LABELS
THAT
WE
CANNOT
OBSERVE
WHEN
DEVELOPING
OUR
SYSTEM
MACHINE
LEARNING
PROBLEMS
SUPERVISED
LEARNING
UNSUPERVISED
LEARNI
NG
Q
CJ
CL
Q
C
C
CLASSIFICATION
GIVEN
A
FEATURE
REPRESENTATION
FOR
IMAGES
HOW
DO
WE
LEARN
A
MODEL
FOR
DISTINGUISHING
FEATURES
FROM
DIFFERENT
CLASSES
DECISION
BOUNDARY
ZEBRA
NON
ZEBRA
CLASSIFICATION
ASSIGN
INPUT
VECTOR
TO
ONE
OF
TWO
OR
MORE
CLASSES
ANY
DECISION
RULE
DIVIDES
THE
INPUT
SPACE
INTO
DECISION
REGIONS
SEPARATED
BY
DECISION
BOUNDARIES
WHAT
DO
YOU
SEE
IN
THIS
IMAGE
TREES
BEAR
CAMERA
MAN
RABBIT
GRASS
FOREST
SLIDE
CREDIT
D
HOIEM
DESCRIBE
PREDICT
OR
INTERACT
WITH
THE
OBJECT
BASED
ON
VISUAL
CUES
IS
IT
DANGEROUS
HOW
FAST
DOES
IT
RUN
IS
IT
ALIVE
IS
IT
SOFT
DOES
IT
HAVE
A
TAIL
CAN
I
POKE
WITH
IT
SLIDE
CREDIT
D
HOIEM
BINARY
CAT
VS
DOG
OR
CAT
VS
NO
CAT
OBJECT
RECOGNITION
CALTECH
AVERAGE
OBJECT
IMAGES
FINE
GRAINED
RECOGNITION
PLACE
RECOGNITION
PLACES
DATABASE
OBJECT
CATEGORIZATION
TASK
DESCRIPTION
GIVEN
A
SMALL
NUMBER
OF
TRAINING
IMAGES
OF
A
CATEGORY
RECOGNIZE
A
PRIORI
UNKNOWN
INSTANCES
OF
THAT
CATEGORY
AND
ASSIGN
THE
CORRECT
CATEGORY
LABEL
CATEGORIZE
AT
WHICH
LEVEL
FIDO
GERMAN
SHEPHERD
DOG
GRAUMAN
B
LEIBE
ANIMAL
LIVING
BEING
APPLY
A
PREDICTION
FUNCTION
TO
A
FEATURE
REPRESENTATION
OF
THE
IMAGE
TO
GET
THE
DESIRED
OUTPUT
F
APPLE
F
TOMATO
F
COW
Y
F
X
OUTPUT
PREDICTION
FUNCTION
IMAGE
FEATURE
TRAINING
GIVEN
A
TRAINING
SET
OF
LABELED
EXAMPLES
XN
YN
ESTIMATE
THE
PREDICTION
FUNCTION
F
BY
MINIMIZING
THE
PREDICTION
ERROR
ON
THE
TRAINING
SET
TESTING
APPLY
F
TO
A
NEVER
BEFORE
SEEN
TEST
EXAMPLE
X
AND
OUTPUT
THE
PREDICTED
VALUE
Y
F
X
TRAINING
STEPS
TESTING
TEST
IMAGE
SLIDE
CREDIT
D
HOIEM
AND
L
LAZEBNIK
POPULAR
GLOBAL
IMAGE
FEATURES
RAW
PIXELS
AND
SIMPLE
FUNCTIONS
OF
RAW
PIXELS
OLIVA
AND
TORRALBA
HISTOGRAMS
BAGS
OF
FEATURES
E
G
SIFT
HOG
DALAL
AND
TRIGGS
RECOGNIZING
A
BEACH
RECOGNIZING
CLOTH
FABRIC
DATASETS
TODAY
IMAGENET
CATEGORIES
IMAGES
MICROSOFT
COCO
CATEGORIES
IMAGES
PASCAL
CATEGORIES
IMAGES
SUN
CATEGORIES
IMAGES
CHALLENGES
ROBUSTNESS
ILLUMINATION
OBJECT
POSE
CLUTTER
OCCLUSIONS
INTRA
CLASS
APPEARANCE
VIEWPOINT
NEAREST
NEIGHBOR
CLASSIFIER
TRAINING
EXAMPLES
FROM
CLASS
TEST
EXAMPLE
TRAINING
EXAMPLES
FROM
CLASS
F
X
LABEL
OF
THE
TRAINING
EXAMPLE
NEAREST
TO
X
ALL
WE
NEED
IS
A
DISTANCE
FUNCTION
FOR
OUR
INPUTS
NO
TRAINING
REQUIRED
K
NEAREST
NEIGHBORS
CLASSIFICATION
FOR
A
NEW
POINT
FIND
THE
K
CLOSEST
POINTS
FROM
TRAINING
DATA
LABELS
OF
THE
K
POINTS
VOTE
TO
CLASSIFY
BLACK
NEGATIVE
RED
POSITIVE
K
IF
QUERY
LANDS
HERE
THE
NN
CONSIST
OF
NEGATIVES
AND
POSITIVES
SO
WE
CLASSIFY
IT
AS
NEGATIVE
NEAREST
NEIGHBORS
PROS
AND
CONS
NN
PROS
SIMPLE
TO
IMPLEMENT
DECISION
BOUNDARIES
NOT
NECESSARILY
LINEAR
WORKS
FOR
ANY
NUMBER
OF
CLASSES
NONPARAMETRIC
METHOD
NN
CONS
NEED
GOOD
DISTANCE
FUNCTION
SLOW
AT
TEST
TIME
LARGE
SEARCH
PROBLEM
TO
FIND
NEIGHBORS
STORAGE
OF
DATA
DISCRIMINATIVE
CLASSIFIERS
LEARN
A
SIMPLE
FUNCTION
OF
THE
INPUT
FEATURES
THAT
CORRECTLY
PREDICTS
THE
TRUE
LABELS
ON
THE
TRAINING
SET
𝑦
𝑓
TRAINING
GOALS
ACCURATE
CLASSIFICATION
OF
TRAINING
DATA
CORRECT
CLASSIFICATIONS
ARE
CONFIDENT
CLASSIFICATION
FUNCTION
IS
SIMPLE
LINEAR
CLASSIFIER
FIND
A
LINEAR
FUNCTION
TO
SEPARATE
THE
CLASSES
CONFIDENCE
IN
POSITIVE
LABEL
IS
A
WEIGHTED
SUM
OF
FEATURES
F
X
SGN
WDXD
SGN
W
X
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
LINES
IN
LET
A
W
C
X
Y
AX
CY
B
W
X
B
D
B
W
X
B
DISTANCE
FROM
W
POINT
TO
LINE
FIND
LINEAR
FUNCTION
TO
SEPARATE
POSITIVE
AND
NEGATIVE
EXAMPLES
XI
POSITIVE
XI
NEGATIVE
W
B
XI
W
B
WHICH
LINE
IS
BEST
DISCRIMINATIVE
CLASSIFIER
BASED
ON
OPTIMAL
SEPARATING
LINE
FOR
CASE
MAXIMIZE
THE
MARGIN
BETWEEN
THE
POSITIVE
AND
NEGATIVE
TRAINING
EXAMPLES
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
SUPPORT
VECTORS
MARGIN
FOR
SUPPORT
VECTORS
XI
W
B
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
AND
LINE
FOR
SUPPORT
VECTORS
XI
W
B
W
SUPPORT
VECTORS
MARGIN
WΤ
X
B
M
WANT
LINE
THAT
MAXIMIZES
THE
MARGIN
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
AND
LINE
XI
W
B
W
SUPPORT
VECTORS
MARGIN
THEREFORE
THE
MARGIN
IS
W
MAXIMIZE
MARGIN
W
CORRECTLY
CLASSIFY
ALL
TRAINING
DATA
POINTS
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
QUADRATIC
OPTIMIZATION
PROBLEM
ONE
CONSTRAINT
FOR
EACH
TRAINING
POINT
NOTE
SIGN
TRICK
SOLUTION
W
I
I
YI
XI
SOLUTION
W
I
I
YI
XI
B
YI
W
XI
FOR
ANY
SUPPORT
VECTOR
CLASSIFICATION
FUNCTION
F
X
SIGN
W
X
B
SIGN
I
I
YI
XI
X
B
IF
F
X
CLASSIFY
AS
NEGATIVE
OTHERWISE
CLASSIFY
AS
POSITIVE
NOTICE
THAT
IT
RELIES
ON
AN
INNER
PRODUCT
BETWEEN
THE
TEST
POINT
X
AND
THE
SUPPORT
VECTORS
XI
NONLINEAR
SVMS
DATASETS
THAT
ARE
LINEARLY
SEPARABLE
WORK
OUT
GREAT
X
BUT
WHAT
IF
THE
DATASET
IS
JUST
TOO
HARD
X
WE
CAN
MAP
IT
TO
A
HIGHER
DIMENSIONAL
SPACE
ANDREW
MOORE
NONLINEAR
KERNEL
EXAMPLE
CONSIDER
THE
MAPPING
X
X
X
Y
X
Y
XY
K
X
Y
XY
THE
KERNEL
TRICK
THE
LINEAR
CLASSIFIER
RELIES
ON
DOT
PRODUCT
BETWEEN
VECTORS
K
XI
XJ
XI
XJ
IF
EVERY
DATA
POINT
IS
MAPPED
INTO
HIGH
DIMENSIONAL
SPACE
VIA
SOME
TRANSFORMATION
Φ
XI
Φ
XI
THE
DOT
PRODUCT
BECOMES
K
XI
XJ
Φ
XI
Φ
XJ
A
KERNEL
FUNCTION
IS
SIMILARITY
FUNCTION
THAT
CORRESPONDS
TO
AN
INNER
PRODUCT
IN
SOME
EXPANDED
FEATURE
SPACE
THE
KERNEL
TRICK
INSTEAD
OF
EXPLICITLY
COMPUTING
THE
LIFTING
TRANSFORMATION
Φ
X
DEFINE
A
KERNEL
FUNCTION
K
SUCH
THAT
K
XI
XJ
Φ
XI
Φ
XJ
EXAMPLES
OF
KERNEL
FUNCTIONS
LINEAR
K
XI
X
J
I
J
GAUSSIAN
RBF
K
XI
X
J
EXP
HISTOGRAM
INTERSECTION
K
XI
X
J
MIN
K
XI
K
X
J
K
ALLOWING
MISCLASSIFICATIONS
MISCLASSIFICATION
COST
DATA
SAMPLES
SLACK
VARIABLE
THE
W
THAT
MINIMIZES
MAXIMIZE
MARGIN
MINIMIZE
MISCLASSIFICATION
WHAT
ABOUT
MULTI
CLASS
SVMS
UNFORTUNATELY
THERE
IS
NO
DEFINITIVE
MULTI
CLASS
SVM
FORMULATION
IN
PRACTICE
WE
HAVE
TO
OBTAIN
A
MULTI
CLASS
SVM
BY
COMBINING
MULTIPLE
TWO
CLASS
SVMS
ONE
VS
OTHERS
TRAINING
LEARN
AN
SVM
FOR
EACH
CLASS
VS
THE
OTHERS
TESTING
APPLY
EACH
SVM
TO
THE
TEST
EXAMPLE
AND
ASSIGN
IT
TO
THE
CLASS
OF
THE
SVM
THAT
RETURNS
THE
HIGHEST
DECISION
VALUE
ONE
VS
ONE
TRAINING
LEARN
AN
SVM
FOR
EACH
PAIR
OF
CLASSES
TESTING
EACH
LEARNED
SVM
VOTES
FOR
A
CLASS
TO
ASSIGN
TO
THE
TEST
EXAMPLE
SVMS
FOR
RECOGNITION
DEFINE
YOUR
REPRESENTATION
FOR
EACH
EXAMPLE
SELECT
A
KERNEL
FUNCTION
COMPUTE
PAIRWISE
KERNEL
VALUES
BETWEEN
LABELED
EXAMPLES
USE
THIS
KERNEL
MATRIX
TO
SOLVE
FOR
SVM
SUPPORT
VECTORS
WEIGHTS
TO
CLASSIFY
A
NEW
EXAMPLE
COMPUTE
KERNEL
VALUES
BETWEEN
NEW
INPUT
AND
SUPPORT
VECTORS
APPLY
WEIGHTS
CHECK
SIGN
OF
OUTPUT
EXAMPLE
LEARNING
GENDER
WITH
SVMS
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
MOGHADDAM
AND
YANG
FACE
GESTURE
SUPPORT
FACES
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
HUMAN
VS
MACHINE
SVMS
PERFORMED
BETTER
THAN
ANY
SINGLE
HUMAN
TEST
SUBJECT
AT
EITHER
RESOLUTION
KRISTEN
GRAUMAN
SVMS
PROS
AND
CONS
PROS
MANY
PUBLICLY
AVAILABLE
SVM
PACKAGES
OR
USE
BUILT
IN
MATLAB
VERSION
BUT
SLOWER
KERNEL
BASED
FRAMEWORK
IS
VERY
POWERFUL
FLEXIBLE
OFTEN
A
SPARSE
SET
OF
SUPPORT
VECTORS
COMPACT
AT
TEST
TIME
WORK
VERY
WELL
IN
PRACTICE
EVEN
WITH
VERY
SMALL
TRAINING
SAMPLE
SIZES
CONS
NO
DIRECT
MULTI
CLASS
SVM
MUST
COMBINE
TWO
CLASS
SVMS
CAN
BE
TRICKY
TO
SELECT
BEST
KERNEL
FUNCTION
FOR
A
PROBLEM
COMPUTATION
MEMORY
DURING
TRAINING
TIME
MUST
COMPUTE
MATRIX
OF
KERNEL
VALUES
FOR
EVERY
PAIR
OF
EXAMPLES
LEARNING
CAN
TAKE
A
VERY
LONG
TIME
FOR
LARGE
SCALE
PROBLEMS
ADAPTED
FROM
LANA
LAZEBNIK
EVALUATING
CLASSIFIERS
ACCURACY
CORRECTLY
CLASSIFIED
ALL
TEST
EXAMPLES
PRECISION
RECALL
PRECISION
PREDICTED
TRUE
POS
PREDICTED
POS
RECALL
PREDICTED
TRUE
POS
TRUE
POS
F
MEASURE
P
R
WANT
EVALUATION
METRIC
TO
BE
IN
SOME
RANGE
E
G
WORST
POSSIBLE
CLASSIFIER
BEST
POSSIBLE
CLASSIFIER
TRAINING
SET
LABELS
KNOWN
TEST
SET
LABELS
UNKNOWN
HOW
WELL
DOES
A
LEARNED
MODEL
GENERALIZE
FROM
THE
DATA
IT
WAS
TRAINED
ON
TO
A
NEW
TEST
SET
COMPONENTS
OF
GENERALIZATION
ERROR
BIAS
HOW
MUCH
THE
AVERAGE
MODEL
OVER
ALL
TRAINING
SETS
DIFFERS
FROM
THE
TRUE
MODEL
ERROR
DUE
TO
INACCURATE
ASSUMPTIONS
SIMPLIFICATIONS
MADE
BY
THE
MODEL
VARIANCE
HOW
MUCH
MODELS
ESTIMATED
FROM
DIFFERENT
TRAINING
SETS
DIFFER
FROM
EACH
OTHER
UNDERFITTING
MODEL
IS
TOO
SIMPLE
TO
REPRESENT
ALL
THE
RELEVANT
CLASS
CHARACTERISTICS
HIGH
BIAS
AND
LOW
VARIANCE
OVERFITTING
MODEL
IS
TOO
COMPLEX
AND
FITS
IRRELEVANT
CHARACTERISTICS
NOISE
IN
THE
DATA
LOW
BIAS
AND
HIGH
VARIANCE
MODELS
WITH
TOO
FEW
PARAMETERS
ARE
INACCURATE
BECAUSE
OF
A
LARGE
BIAS
NOT
ENOUGH
FLEXIBILITY
MODELS
WITH
TOO
MANY
PARAMETERS
ARE
INACCURATE
BECAUSE
OF
A
LARGE
VARIANCE
TOO
MUCH
SENSITIVITY
TO
THE
SAMPLE
UNDERFITTING
OVERFITTING
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
NEED
VALIDATION
SET
VALIDATION
SET
IS
SEPARATE
FROM
THE
TEST
SET
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
NUMBER
OF
TRAINING
EXAMPLES
HOW
TO
REDUCE
VARIANCE
CHOOSE
A
SIMPLER
CLASSIFIER
USE
FEWER
FEATURES
GET
MORE
TRAINING
DATA
REGULARIZE
THE
PARAMETERS
WANT
TO
MINIMIZE
OVERALL
ABSOLUTE
VALUE
OF
WEIGHTS
SLIDE
CREDIT
D
HOIEM
REMEMBER
NO
FREE
LUNCH
MACHINE
LEARNING
ALGORITHMS
ARE
TOOLS
THREE
KINDS
OF
ERROR
INHERENT
UNAVOIDABLE
BIAS
DUE
TO
OVER
SIMPLIFICATIONS
VARIANCE
DUE
TO
INABILITY
TO
PERFECTLY
ESTIMATE
PARAMETERS
FROM
LIMITED
DATA
TRY
SIMPLE
CLASSIFIERS
FIRST
BETTER
TO
HAVE
SMART
FEATURES
AND
SIMPLE
CLASSIFIERS
THAN
SIMPLE
FEATURES
AND
SMART
CLASSIFIERS
USE
INCREASINGLY
POWERFUL
CLASSIFIERS
WITH
MORE
TRAINING
DATA
BIAS
VARIANCE
TRADEOFF
ADAPTED
FROM
D
HOIEM
BAG
OF
FEATURES
STEPS
EXTRACT
LOCAL
FEATURES
LEARN
VISUAL
VOCABULARY
USING
CLUSTERING
QUANTIZE
LOCAL
FEATURES
USING
VISUAL
VOCABULARY
REPRESENT
IMAGES
BY
FREQUENCIES
OF
VISUAL
WORDS
SLIDE
CREDIT
L
LAZEBNIK
LEARNING
THE
VISUAL
VOCABULARY
COMPUTE
HISTOGRAM
IN
EACH
SPATIAL
BIN
ATTRIBUTES
WE
WANT
TO
BE
ABLE
TO
INFER
SOMETHING
ABOUT
UNFAMILIAR
OBJECTS
IF
WE
CAN
INFER
PROPERTIES
FAMILIAR
OBJECTS
NEW
OBJECT
HAS
STRIPES
HAS
EARS
HAS
EYES
HAS
FOUR
LEGS
HAS
MANE
HAS
TAIL
HAS
SNOUT
BROWN
MUSCULAR
HAS
SNOUT
HAS
STRIPES
LIKE
CAT
HAS
MANE
AND
TAIL
LIKE
HORSE
HAS
SNOUT
LIKE
HORSE
AND
DOG
WHITTLESEARCH
SEARCH
VIA
COMPARISONS
LIKE
THIS
BUT
WITH
CURLIER
HAIR
ALLOW
USER
TO
WHITTLE
AWAY
IRRELEVANT
IMAGES
VIA
COMPARATIVE
FEEDBACK
ON
PROPERTIES
OF
RESULTS
RELATIVE
ATTRIBUTES
WE
NEED
ABILITY
TO
COMPARE
IMAGES
BY
ATTRIBUTE
STRENGTH
BRIGHT
LEARNING
RELATIVE
ATTRIBUTES
WE
WANT
TO
LEARN
A
SPECTRUM
RANKING
MODEL
FOR
AN
ATTRIBUTE
E
G
BRIGHTNESS
SUPERVISION
FROM
HUMAN
ANNOTATORS
CONSISTS
OF
ORDERED
PAIRS
SIMILAR
PAIRS
LEARNING
USER
SPECIFIC
ATTRIBUTES
TREAT
LEARNING
PERCEIVED
ATTRIBUTES
AS
AN
ADAPTATION
PROBLEM
ADAPT
GENERIC
ATTRIBUTE
MODEL
WITH
MINIMAL
USER
SPECIFIC
LABELED
EXAMPLES
GENERIC
CATEGORY
RECOGNITION
REPRESENTATION
CHOICE
PART
BASED
CONSIDER
EDGES
CONTOURS
AND
ORIENTED
INTENSITY
GRADIENTS
SUMMARIZE
LOCAL
DISTRIBUTION
OF
GRADIENTS
WITH
HISTOGRAM
GIVEN
THE
REPRESENTATION
TRAIN
A
BINARY
CLASSIFIER
NOY
ENSO
TCAARC
AR
FACE
DETECTION
AND
RECOGNITION
SALLY
BOOSTING
TRAINING
INITIALLY
WEIGHT
EACH
TRAINING
EXAMPLE
EQUALLY
IN
EACH
BOOSTING
ROUND
FIND
THE
WEAK
LEARNER
THAT
ACHIEVES
THE
LOWEST
WEIGHTED
TRAINING
ERROR
RAISE
WEIGHTS
OF
TRAINING
EXAMPLES
MISCLASSIFIED
BY
CURRENT
WEAK
LEARNER
COMPUTE
FINAL
CLASSIFIER
AS
LINEAR
COMBINATION
OF
ALL
WEAK
LEARNERS
WEIGHT
OF
EACH
LEARNER
IS
DIRECTLY
PROPORTIONAL
TO
ITS
ACCURACY
REVIEW
ILLUSTRATION
WITH
RED
BLUE
CIRCLES
OF
DIFFERENT
SIZES
VIOLA
JONES
FACE
DETECTOR
MAIN
IDEA
REPRESENT
LOCAL
TEXTURE
WITH
EFFICIENTLY
COMPUTABLE
RECTANGULAR
FEATURES
WITHIN
WINDOW
OF
INTEREST
SELECT
DISCRIMINATIVE
FEATURES
TO
BE
WEAK
CLASSIFIERS
USE
BOOSTED
COMBINATION
OF
THEM
AS
FINAL
CLASSIFIER
FORM
A
CASCADE
OF
SUCH
CLASSIFIERS
REJECTING
CLEAR
NEGATIVES
QUICKLY
VIOLA
JONES
DETECTOR
FEATURES
RECTANGULAR
FILTERS
FEATURE
OUTPUT
IS
DIFFERENCE
BETWEEN
ADJACENT
REGIONS
VALUE
PIXELS
IN
WHITE
AREA
PIXELS
IN
BLACK
AREA
EFFICIENTLY
COMPUTABLE
WITH
INTEGRAL
IMAGE
ANY
SUM
CAN
BE
COMPUTED
IN
CONSTANT
TIME
VALUE
AT
X
Y
IS
SUM
OF
PIXELS
ABOVE
AND
TO
THE
LEFT
OF
X
Y
INTEGRAL
IMAGE
EXAMPLE
SOURCE
RESULT
COMPUTING
SUM
WITHIN
A
RECTANGLE
LET
A
B
C
D
BE
THE
VALUES
OF
THE
INTEGRAL
IMAGE
AT
THE
CORNERS
OF
A
THEN
THE
SUM
OF
ORIGINAL
IMAGE
VALUES
WITHIN
THE
RECTANGLE
CAN
BE
COMPUTED
AS
SUM
A
B
C
D
ONLY
ADDITIONS
ARE
REQUIRED
FOR
ANY
SIZE
OF
RECTANGLE
VIOLA
JONES
DETECTOR
FEATURES
CONSIDERING
ALL
POSSIBLE
FILTER
PARAMETERS
POSITION
SCALE
AND
TYPE
POSSIBLE
FEATURES
ASSOCIATED
WITH
EACH
X
WINDOW
WHICH
SUBSET
OF
THESE
FEATURES
SHOULD
WE
USE
TO
DETERMINE
IF
A
WINDOW
HAS
A
FACE
USE
ADABOOST
BOTH
TO
SELECT
THE
INFORMATIVE
FEATURES
AND
TO
FORM
THE
CLASSIFIER
VIOLA
JONES
DETECTOR
ADABOOST
WANT
TO
SELECT
THE
SINGLE
RECTANGLE
FEATURE
AND
THRESHOLD
THAT
BEST
SEPARATES
POSITIVE
FACES
AND
NEGATIVE
NON
FACES
TRAINING
EXAMPLES
IN
TERMS
OF
WEIGHTED
ERROR
RESULTING
WEAK
CLASSIFIER
OUTPUTS
OF
A
POSSIBLE
RECTANGLE
FEATURE
ON
FACES
AND
NON
FACES
FOR
NEXT
ROUND
REWEIGHT
THE
EXAMPLES
ACCORDING
TO
ERRORS
CHOOSE
ANOTHER
FILTER
THRESHOLD
COMBO
CASCADING
CLASSIFIERS
FOR
DETECTION
FORM
A
CASCADE
WITH
LOW
FALSE
NEGATIVE
RATES
EARLY
ON
APPLY
LESS
ACCURATE
BUT
FASTER
CLASSIFIERS
FIRST
TO
IMMEDIATELY
DISCARD
WINDOWS
THAT
CLEARLY
APPEAR
TO
BE
NEGATIVE
DALAL
TRIGGS
PEDESTRIAN
DETECTOR
EXTRACT
FIXED
SIZED
PIXEL
WINDOW
AT
EACH
POSITION
AND
SCALE
COMPUTE
HOG
HISTOGRAM
OF
GRADIENT
FEATURES
WITHIN
EACH
WINDOW
SCORE
THE
WINDOW
WITH
A
LINEAR
SVM
CLASSIFIER
PERFORM
NON
MAXIMA
SUPPRESSION
TO
REMOVE
OVERLAPPING
DETECTIONS
WITH
LOWER
SCORES
HISTOGRAM
OF
GRADIENT
ORIENTATIONS
ORIENTATION
BINS
FOR
UNSIGNED
ANGLES
HISTOGRAMS
IN
PIXEL
CELLS
HISTOGRAMS
OF
ORIENTED
GRADIENTS
HOG
CELLS
DALAL
AND
B
TRIGGS
CVPR
IMAGE
CREDIT
N
SNAVELY
IMAGES
FROM
CALTECH
DEFINE
OBJECT
BY
COLLECTION
OF
PARTS
MODELED
BY
APPEARANCE
SPATIAL
CONFIGURATION
OBJECT
MODEL
SUM
OF
SCORES
OF
FEATURES
AT
FIXED
POSITIONS
NON
OBJECT
OBJECT
STAR
SHAPED
MODEL
ROOT
FILTER
PART
FILTERS
DEFORMATION
WEIGHTS
P
FELZE
THE
SCORE
OF
A
HYPOTHESIS
IS
THE
SUM
OF
APPEARANCE
SCORES
MINUS
THE
SUM
OF
DEFORMATION
COSTS
SUBWINDOW
N
FEATURES
N
DISPLACEMENTS
SCORE
P
P
F
H
P
D
DX
DY
N
I
I
I
I
I
I
I
I
I
APPEARANCE
WEIGHTS
DEFORMATION
WEIGHTS
ADAPTED
FROM
LANA
LAZEBNIK
TRAINING
DATA
CONSISTS
OF
IMAGES
WITH
LABELED
BOUNDING
BOXES
NEED
TO
LEARN
THE
FILTERS
AND
DEFORMATION
PARAMETERS
F
X
W
H
X
W
ARE
MODEL
PARAMETERS
Z
ARE
LATENT
HYPOTHESES
LATENT
SVM
TRAINING
INITIALIZE
W
AND
ITERATE
FIX
W
AND
FIND
THE
BEST
Z
FOR
EACH
TRAINING
EXAMPLE
FIX
Z
AND
SOLVE
FOR
W
STANDARD
SVM
TRAINING
OBJECT
DETECTION
SYSTEM
OVERVIEW
OUR
SYSTEM
TAKES
AN
INPUT
IMAGE
EXTRACTS
AROUND
BOTTOM
UP
REGION
PROPOSALS
COMPUTES
FEATURES
FOR
EACH
PROPOSAL
USING
A
LARGE
CONVOLUTIONAL
NEURAL
NETWORK
CNN
AND
THEN
CLASSIFIES
EACH
REGION
USING
CLASS
SPECIFIC
LINEAR
SVMS
R
CNN
ACHIEVES
A
MEAN
AVERAGE
PRECISION
MAP
OF
ON
PASCAL
VOC
FOR
COMPARISON
UIJLINGS
ET
AL
REPORT
MAP
USING
THE
SAME
REGION
PROPOSALS
BUT
WITH
A
SPATIAL
PYRAMID
AND
BAG
OF
VISUAL
WORDS
APPROACH
THE
POPULAR
DEFORMABLE
PART
MODELS
PERFORM
AT
LANA
LAZEBNIK
HOG
HUMAN
DETECTOR
CHAIR
LOSS
DUE
TO
RGB
HOG
RECALL
HOG
DPM
HOG
HUMAN
RGB
HUMAN
VONDRICK
ET
AL
ICCV
WHY
DID
THE
DETECTOR
FAIL
CAR
VONDRICK
ET
AL
ICCV
WHY
DID
THE
DETECTOR
FAIL
CAR
VONDRICK
ET
AL
ICCV
SUMMARY
DETECTION
WINDOW
BASED
APPROACHES
ASSUME
OBJECT
APPEARS
IN
ROUGHLY
THE
SAME
CONFIGURATION
IN
DIFFERENT
IMAGES
LOOK
FOR
ALIGNMENT
WITH
A
GLOBAL
TEMPLATE
PART
BASED
METHODS
ALLOW
PARTS
TO
MOVE
SOMEWHAT
FROM
THEIR
USUAL
LOCATIONS
LOOK
FOR
GOOD
FITS
IN
APPEARANCE
FOR
BOTH
THE
GLOBAL
TEMPLATE
AND
THE
INDIVIDUAL
PART
TEMPLATES
AMAZON
MECHANICAL
TURK
WORKERS
ANNOTATION
PROTOCOLS
TYPE
KEYWORDS
SELECT
RELEVANT
IMAGES
CLICK
ON
LANDMARKS
OUTLINE
SOMETHING
ANYTHING
ELSE
ENSURING
ANNOTATION
QUALITY
CONSENSUS
MULTIPLE
ANNOTATION
WISDOM
OF
THE
CROWD
QUALIFICATION
EXAM
GOLD
STANDARD
QUESTIONS
GRADING
TASKS
A
SECOND
TIER
OF
WORKERS
WHO
GRADE
OTHERS
THE
ESP
GAME
PLAYER
PLAYER
GUESSING
CAR
GUESSING
BOY
GUESSING
HAT
GUESSING
KID
SUCCESS
YOU
AGREE
ON
CAR
GUESSING
CAR
SUCCESS
YOU
AGREE
ON
CAR
REVEALING
IMAGES
GUESSER
REVEALER
GUESS
CAR
BCRAURSH
PARTNER
GUESS
SUMMARY
COLLECTING
ANNOTATIONS
FROM
HUMANS
CROWDSOURCING
ALLOWS
VERY
CHEAP
DATA
COLLECTION
GETTING
HIGH
QUALITY
ANNOTATIONS
CAN
BE
TRICKY
BUT
THERE
ARE
MANY
WAYS
TO
ENSURE
QUALITY
ONE
WAY
TO
OBTAIN
HIGH
QUALITY
DATA
FAST
IS
BY
PHRASING
YOUR
DATA
COLLECTION
AS
A
GAME
CROWDSOURCING
TRAINING
JAMES
HAYS
ACTIVE
LEARNING
TRADITIONAL
ACTIVE
LEARNING
REDUCES
SUPERVISION
BY
OBTAINING
LABELS
FOR
THE
MOST
INFORMATIVE
OR
UNCERTAIN
EXAMPLES
FIRST
BOARD
ENTROPY
MACKAY
FREUND
ET
AL
TONG
KOLLER
LINDENBAUM
ET
AL
KAPOOR
ET
AL
HUMAN
IN
THE
LOOP
RECOGNITION
TEST
IMAGE
TESTING
BASIC
ALGORITHM
INPUT
IMAGE
X
COMPUTER
VISION
MAX
EXPECTED
INFORMATION
GAIN
QUESTION
A
NO
P
C
X
HIGH
INFORMATION
GAIN
LOW
ENTROPY
IS
THE
BELLY
BLACK
MAX
EXPECTED
INFORMATION
GAIN
QUESTION
A
YES
P
C
X
IS
THE
BILL
HOOKED
P
C
X
FEATURES
LABELS
PREDICT
WHERE
PEOPLE
WILL
LOOK
FOR
A
NEW
IMAGE
BY
RUNNING
MODEL
ON
EVERY
PIXEL
OF
THE
IMAGE
SALIENCY
MODEL
FEATURES
ILLUMINATION
COLOR
ORIENTATION
HORIZON
LINE
FACE
DETECTOR
VIOLA
JONES
PERSON
DETECTOR
DPM
DISTANCE
TO
CENTER
SALIENT
SAMPLES
FROM
TOP
NON
SALIENT
SAMPLES
FROM
BOTTOM
FROM
EACH
OF
TRAINING
IMAGES
LEARNS
WEIGHTS
FOR
EACH
FEATURE
THAT
BEST
PREDICT
A
SALIENCY
LABEL
FOR
EACH
PIXEL
TAKE
MODEL
TRAINED
ON
E
G
IMAGENET
TRAINING
SET
TAKE
OUTPUTS
OF
OR
LAYER
OPTIONAL
FINE
TUNE
FEATURES
AND
OR
CLASSIFIER
ON
NEW
DATASET
TO
TRAIN
MODEL
FROM
SCRATCH
NEED
LOTS
OF
DATA
CLASSIFY
TEST
SET
OF
NEW
DATASET
WHY
NOW
WE
HAVE
LOTS
OF
DATA
AND
DEEP
NETS
CAN
BE
TRAINED
IN
REASONABLE
TIME
WITH
GPUS
LANGUAGE
DEEP
LEARNING
DEEP
NEURAL
NETS
CONVOLUTIONAL
NEURAL
NETS
SHALLOW
VS
DEEP
ARCHITECTURES
TRADITIONAL
RECOGNITION
SHALLOW
ARCHITECTURE
IMAGE
VIDEO
PIXELS
OBJECT
CLASS
IMAGE
VIDEO
PIXELS
DEEP
LEARNING
DEEP
ARCHITECTURE
OBJECT
CLASS
BACKGROUND
PERCEPTRONS
INPUT
WEIGHTS
D
NEURAL
NETWORK
WITH
SPECIALIZED
CONNECTIVITY
STRUCTURE
STACK
MULTIPLE
STAGES
OF
FEATURE
EXTRACTORS
HIGHER
STAGES
COMPUTE
MORE
GLOBAL
MORE
INVARIANT
MORE
ABSTRACT
FEATURES
CLASSIFICATION
LAYER
AT
THE
END
ADAPTED
FROM
ROB
FERGUS
CONVOLVE
INPUT
WITH
LEARNED
FILTERS
APPLY
NON
LINEARITY
SPATIAL
POOLING
DOWNSAMPLE
NORMALIZATION
OPTIONAL
SUPERVISED
TRAINING
OF
CONVOLUTIONAL
FILTERS
BY
BACK
PROPAGATING
CLASSIFICATION
ERROR
ADAPTED
FROM
LANA
LAZEBNIK
CONVOLUTION
APPLY
LEARNED
FILTER
WEIGHTS
ONE
FEATURE
MAP
PER
FILTER
STRIDE
CAN
BE
GREATER
THAN
FASTER
LESS
MEMORY
NON
LINEARITY
PER
ELEMENT
INDEPENDENT
OPTIONS
TANH
SIGMOID
EXP
X
RECTIFIED
LINEAR
UNIT
RELU
SIMPLIFIES
BACKPROPAGATION
MAKES
LEARNING
FASTER
AVOIDS
SATURATION
ISSUES
PREFERRED
OPTION
WORKS
WELL
MAX
POOLING
SPATIAL
POOLING
DOWNSAMPLING
SUM
OR
MAX
NON
OVERLAPPING
OVERLAPPING
REGIONS
ROLE
OF
POOLING
INVARIANCE
TO
SMALL
TRANSFORMATIONS
LARGER
RECEPTIVE
FIELDS
SEE
MORE
OF
INPUT
KRIZHEVSKY
ET
AL
ERROR
TOP
NEXT
BEST
NON
CONVNET
ERROR
MACHINE
LEARNING
PROCEEDINGS
OF
THE
THIRTEENTH
INTERNATIONAL
CONFERENCE
EXPERIMENTS
WITH
A
NEW
BOOSTING
ALGORITHM
YOAV
FREUND
ROBERT
E
SCHAPIRE
AT
T
LABORATORIES
MOUNTAIN
AVENUE
MURRAY
HILL
NJ
FYOAV
ABSTRACT
IN
AN
EARLIER
PAPER
WE
INTRODUCED
A
NEW
BOOSTING
ALGORITHM
CALLED
ADABOOST
WHICH
THEORETICALLY
CAN
BE
USED
TO
SIGNIFICANTLY
REDUCE
THE
ERROR
OF
ANY
LEARNING
ALGORITHM
THAT
CON
SISTENTLY
GENERATES
CLASSIFIERS
WHOSE
PERFORMANCE
IS
A
LITTLE
BETTER
THAN
RANDOM
GUESSING
WE
ALSO
INTRODUCED
THE
RELATED
NOTION
OF
A
PSEUDO
LOSS
WHICH
IS
A
METHOD
FOR
FORCING
A
LEARNING
ALGORITHM
OF
MULTI
LABEL
CONCEPTSTO
CONCENTRATE
ON
THE
LABELS
THAT
ARE
HARDEST
TO
DISCRIMINATE
IN
THIS
PAPER
WE
DESCRIBE
EXPERIMENTS
WE
CARRIED
OUT
TO
ASSESS
HOW
WELL
ADABOOST
WITH
AND
WITHOUT
PSEUDO
LOSS
PERFORMS
ON
REAL
LEARNING
PROBLEMS
WE
PERFORMED
TWO
SETS
OF
EXPERIMENTS
THE
FIRST
SET
COMPARED
BOOSTING
TO
BREIMAN
BAGGING
METHOD
WHEN
USED
TO
AGGREGATE
VARIOUS
CLASSIFIERS
INCLUDING
DECISION
TREES
AND
SINGLE
ATTRIBUTE
VALUE
TESTS
WE
COMPARED
THE
PERFORMANCE
OF
THE
TWO
METHODS
ON
A
COLLECTION
OF
MACHINE
LEARNING
BENCHMARKS
IN
THE
SECOND
SET
OF
EXPERIMENTS
WE
STUDIED
IN
MORE
DETAIL
THE
PERFORMANCE
OF
BOOSTING
USING
A
NEAREST
NEIGHBOR
CLASSIFIER
ON
AN
OCR
PROBLEM
INTRODUCTION
BOOSTING
IS
A
GENERAL
METHOD
FOR
IMPROVING
THE
PERFOR
MANCE
OF
ANY
LEARNING
ALGORITHM
IN
THEORY
BOOSTINGCAN
BE
USED
TO
SIGNIFICANTLY
REDUCE
THE
ERROR
OF
ANY
WEAK
LEARNING
ALGORITHM
THAT
CONSISTENTLY
GENERATES
CLASSIFIERS
WHICH
NEED
ONLY
BE
A
LITTLE
BIT
BETTER
THAN
RANDOM
GUESSING
DESPITE
THE
POTENTIAL
BENEFITS
OF
BOOSTING
PROMISED
BY
THE
THEORET
ICAL
RESULTS
THE
TRUE
PRACTICAL
VALUE
OF
BOOSTING
CAN
ONLY
BE
ASSESSED
BY
TESTING
THE
METHOD
ON
REAL
MACHINE
LEARNING
PROBLEMS
IN
THIS
PAPER
WE
PRESENT
SUCH
AN
EXPERIMENTAL
ASSESSMENT
OF
A
NEW
BOOSTING
ALGORITHM
CALLED
ADABOOST
BOOSTING
WORKS
BY
REPEATEDLY
RUNNING
A
GIVEN
LEARNING
ALGORITHM
ON
VARIOUS
DISTRIBUTIONS
OVER
THE
TRAIN
ING
DATA
AND
THEN
COMBINING
THE
CLASSIFIERS
PRODUCED
BY
THE
WEAK
LEARNER
INTO
A
SINGLE
COMPOSITE
CLASSIFIER
THE
FIRST
PROVABLY
EFFECTIVE
BOOSTING
ALGORITHMS
WERE
PRESENTED
BY
SCHAPIRE
AND
FREUND
MORE
RECENTLY
WE
DE
SCRIBED
AND
ANALYZED
ADABOOST
AND
WE
ARGUED
THAT
THIS
NEW
BOOSTING
ALGORITHM
HAS
CERTAIN
PROPERTIES
WHICH
MAKE
IT
MORE
PRACTICAL
AND
EASIER
TO
IMPLEMENT
THAN
ITS
PREDE
CESSORS
THIS
ALGORITHM
WHICH
WE
USED
IN
ALL
OUR
EXPERIMENTS
IS
DESCRIBED
IN
DETAIL
IN
SECTION
HOME
PAGE
EXPECTED
TO
CHANGE
TO
SOME
TIME
IN
THE
NEAR
FUTURE
FOR
UID
EFYOAV
SCHAPIREG
USE
THE
TERM
WEAK
LEARNING
ALGORITHM
EVEN
THOUGH
IN
PRACTICE
BOOSTING
MIGHT
BE
COMBINED
WITH
A
QUITE
STRONG
LEARNING
ALGORITHM
SUCH
AS
THIS
PAPER
DESCRIBES
TWO
DISTINCT
SETS
OF
EXPERIMENTS
IN
THE
FIRST
SET
OF
EXPERIMENTS
DESCRIBED
IN
SECTION
WE
COMPARED
BOOSTING
TO
BAGGING
A
METHOD
DESCRIBED
BY
BREIMAN
WHICH
WORKS
IN
THE
SAME
GENERAL
FASHION
I
E
BY
REPEATEDLY
RERUNNING
A
GIVEN
WEAK
LEARNING
ALGORITHM
AND
COMBINING
THE
COMPUTED
CLASSIFIERS
BUT
WHICH
CON
STRUCTS
EACH
DISTRIBUTION
IN
A
SIMPLER
MANNER
DETAILS
GIVEN
BELOW
WE
COMPARED
BOOSTING
WITH
BAGGING
BECAUSE
BOTH
METHODS
WORK
BY
COMBINING
MANY
CLASSIFIERS
THIS
COM
PARISON
ALLOWS
US
TO
SEPARATE
OUT
THE
EFFECT
OF
MODIFYING
THE
DISTRIBUTION
ON
EACH
ROUND
WHICH
IS
DONE
DIFFERENTLY
BY
EACH
ALGORITHM
FROM
THE
EFFECT
OF
VOTING
MULTIPLECLASSIFIERS
WHICH
IS
DONE
THE
SAME
BY
EACH
IN
OUR
EXPERIMENTS
WE
COMPARED
BOOSTING
TO
BAGGING
USING
A
NUMBER
OF
DIFFERENT
WEAK
LEARNING
ALGORITHMS
OF
VARYING
LEVELS
OF
SOPHISTICATION
THESE
INCLUDE
AN
ALGORITHM
THAT
SEARCHES
FOR
VERY
SIMPLE
PREDICTION
RULES
WHICH
TEST
ON
A
SINGLE
ATTRIBUTE
SIMILAR
TO
HOLTE
VERY
SIM
PLE
CLASSIFICATION
RULES
AN
ALGORITHM
THAT
SEARCHES
FOR
A
SINGLE
GOOD
DECISION
RULE
THAT
TESTS
ON
A
CONJUNCTION
OF
ATTRIBUTE
TESTS
SIMILAR
IN
FLAVOR
TO
THE
RULE
FORMATION
PART
OF
COHEN
RIPPER
ALGORITHM
AND
FU
RNKRANZ
AND
WIDMER
IREP
ALGORITHM
AND
QUINLAN
DECISION
TREE
ALGORITHM
WE
TESTED
THESE
ALGORITHMS
ON
A
COLLECTION
OF
BENCHMARK
LEARNING
PROBLEMS
TAKEN
FROM
THE
UCI
REPOSITORY
THE
MAIN
CONCLUSION
OF
OUR
EXPERIMENTS
IS
THAT
BOOST
ING
PERFORMS
SIGNIFICANTLY
AND
UNIFORMLY
BETTER
THAN
BAG
GING
WHEN
THE
WEAK
LEARNING
ALGORITHM
GENERATES
FAIRLY
SIMPLE
CLASSIFIERS
ALGORITHMS
AND
ABOVE
WHEN
COMBINED
WITH
BOOSTING
STILL
SEEMS
TO
OUTPERFORM
BAGGING
SLIGHTLY
BUT
THE
RESULTS
ARE
LESS
COMPELLING
WE
ALSO
FOUND
THAT
BOOSTING
CAN
BE
USED
WITH
VERY
SIM
PLE
RULES
ALGORITHM
TO
CONSTRUCT
CLASSIFIERS
THAT
ARE
QUITE
GOOD
RELATIVE
SAY
TO
KEARNS
AND
MANSOUR
ARGUE
THAT
CAN
ITSELF
BE
VIEWED
AS
A
KIND
OF
BOOSTING
ALGO
RITHM
SO
A
COMPARISON
OF
ADABOOST
AND
CAN
BE
SEEN
AS
A
COMPARISON
OF
TWO
COMPETING
BOOSTINGALGORITHMS
SEE
DIETTERICH
KEARNS
AND
MANSOUR
PAPER
FOR
MORE
DETAIL
ON
THIS
POINT
IN
THE
SECOND
SET
OF
EXPERIMENTS
WE
TEST
THE
PERFOR
MANCE
OF
BOOSTING
ON
A
NEAREST
NEIGHBOR
CLASSIFIER
FOR
HAND
WRITTEN
DIGIT
RECOGNITION
IN
THIS
CASE
THE
WEAK
LEARNING
ALGORITHM
IS
VERY
SIMPLE
AND
THIS
LETS
US
GAIN
SOME
INSIGHT
INTO
THE
INTERACTION
BETWEEN
THE
BOOSTING
ALGORITHM
AND
THE
NEAREST
NEIGHBOR
CLASSIFIER
WE
SHOW
THAT
THE
BOOSTING
AL
GORITHM
IS
AN
EFFECTIVE
WAY
FOR
FINDING
A
SMALL
SUBSET
OF
PROTOTYPES
THAT
PERFORMS
ALMOST
AS
WELL
AS
THE
COMPLETE
SET
WE
ALSO
SHOW
THAT
IT
COMPARES
FAVORABLY
TO
THE
STANDARD
METHOD
OF
CONDENSED
NEAREST
NEIGHBOR
IN
TERMS
OF
ITS
TEST
ERROR
THERE
SEEM
TO
BE
TWO
SEPARATE
REASONS
FOR
THE
IMPROVE
MENT
IN
PERFORMANCE
THAT
IS
ACHIEVED
BY
BOOSTING
THE
FIRST
AND
BETTER
UNDERSTOOD
EFFECT
OF
BOOSTING
IS
THAT
IT
GENERATES
A
HYPOTHESIS
WHOSE
ERROR
ON
THE
TRAINING
SET
IS
SMALL
BY
COM
BINING
MANY
HYPOTHESES
WHOSE
ERROR
MAY
BE
LARGE
BUT
STILL
BETTER
THAN
RANDOM
GUESSING
IT
SEEMS
THAT
BOOSTINGMAY
BE
HELPFUL
ON
LEARNING
PROBLEMS
HAVING
EITHER
OF
THE
FOLLOWING
TWO
PROPERTIES
THE
FIRST
PROPERTY
WHICH
HOLDS
FOR
MANY
REAL
WORLD
PROBLEMS
IS
THAT
THE
OBSERVED
EXAMPLES
TEND
TO
HAVE
VARYING
DEGREES
OF
HARDNESS
FOR
SUCH
PROBLEMS
THE
BOOSTING
ALGORITHM
TENDS
TO
GENERATE
DISTRIBUTIONS
THAT
CON
CENTRATE
ON
THE
HARDER
EXAMPLES
THUS
CHALLENGING
THE
WEAK
LEARNING
ALGORITHM
TO
PERFORM
WELL
ON
THESE
HARDER
PARTS
OF
ALGORITHM
ADABOOST
INPUT
SEQUENCE
OF
MEXAMPLES
H
XM
YM
I
WITH
LABELS
YIEY
KG
WEAK
LEARNING
ALGORITHM
WEAKLEARN
INTEGER
TSPECIFYING
NUMBER
OF
ITERATIONS
INITIALIZE
I
MFOR
ALL
I
DO
FOR
T
T
CALL
WEAKLEARN
PROVIDING
IT
WITH
THE
DISTRIBUTION
DT
GET
BACK
A
HYPOTHESIS
HT
X
Y
CALCULATE
THE
ERROR
OF
HT
TT
DT
I
I
HT
XI
Y
I
IF
TT
THEN
SET
T
T
AND
ABORT
LOOP
SET
TT
TT
UPDATE
DISTRIBUTION
DT
DT
I
DT
I
O
HT
XI
Y
I
WHERE
ZTIS
A
NORMALIZATION
CONSTANT
CHOSEN
SO
THAT
DT
WILL
BE
A
DISTRIBUTION
OUTPUT
THE
FINAL
HYPOTHESIS
THE
SAMPLE
SPACE
THE
SECOND
PROPERTY
IS
THAT
THE
LEARNING
ALGORITHM
BE
SENSITIVE
TO
CHANGES
IN
THE
TRAINING
EXAMPLES
H
N
X
ARG
MAX
YEY
T
HX
T
X
Y
LOG
SO
THAT
SIGNIFICANTLY
DIFFERENT
HYPOTHESES
ARE
GENERATED
FOR
DIFFERENT
TRAINING
SETS
IN
THIS
SENSE
BOOSTING
IS
SIMILAR
TO
BREIMAN
BAGGING
WHICH
PERFORMS
BEST
WHEN
THE
WEAK
LEARNER
EXHIBITS
SUCH
UNSTABLE
BEHAVIOR
HOWEVER
UNLIKE
BAGGING
BOOSTING
TRIES
ACTIVELY
TO
FORCE
THE
WEAK
LEARNING
ALGORITHM
TO
CHANGE
ITS
HYPOTHESES
BY
CHANGING
THE
DISTRI
BUTION
OVER
THE
TRAINING
EXAMPLES
AS
A
FUNCTION
OF
THE
ERRORS
MADE
BY
PREVIOUSLY
GENERATED
HYPOTHESES
THE
SECOND
EFFECT
OF
BOOSTING
HAS
TO
DO
WITH
VARIANCE
RE
DUCTION
INTUITIVELY
TAKING
A
WEIGHTED
MAJORITY
OVER
MANY
HYPOTHESES
ALL
OF
WHICH
WERE
TRAINED
ON
DIFFERENT
SAMPLES
TAKEN
OUT
OF
THE
SAME
TRAINING
SET
HAS
THE
EFFECT
OF
RE
DUCING
THE
RANDOM
VARIABILITY
OF
THE
COMBINED
HYPOTHESIS
THUS
LIKE
BAGGING
BOOSTING
MAY
HAVE
THE
EFFECT
OF
PRODUC
ING
A
COMBINED
HYPOTHESIS
WHOSE
VARIANCE
IS
SIGNIFICANTLY
LOWER
THAN
THOSE
PRODUCED
BY
THE
WEAK
LEARNER
HOWEVER
UNLIKE
BAGGING
BOOSTING
MAY
ALSO
REDUCE
THE
BIAS
OF
THE
LEARNING
ALGORITHM
AS
DISCUSSED
ABOVE
SEE
KONG
AND
DI
ETTERICH
FOR
FURTHER
DISCUSSION
OF
THE
BIAS
AND
VARIANCE
REDUCING
EFFECTS
OF
VOTING
MULTIPLE
HYPOTHESES
AS
WELL
AS
BREIMAN
VERY
RECENT
WORK
COMPARING
BOOSTING
AND
BAGGING
IN
TERMS
OF
THEIR
EFFECTS
ON
BIAS
AND
VARIANCE
IN
OUR
FIRST
SET
OF
EXPERIMENTS
WE
COMPARE
BOOSTING
AND
BAG
GING
AND
TRY
TO
USE
THAT
COMPARISON
TO
SEPARATE
BETWEEN
THE
BIAS
AND
VARIANCE
REDUCING
EFFECTS
OF
BOOSTING
PREVIOUS
WORK
DRUCKER
SCHAPIRE
AND
SIMARD
PERFORMED
THE
FIRST
EXPERIMENTS
USING
A
BOOSTING
ALGORITHM
THEY
USED
SCHAPIRE
ORIGINAL
BOOSTING
ALGORITHM
COM
BINED
WITH
A
NEURAL
NET
FOR
AN
OCR
PROBLEM
FOLLOW
UP
COMPARISONS
TO
OTHER
ENSEMBLE
METHODS
WERE
DONE
BY
DRUCKER
ET
AL
MORE
RECENTLY
DRUCKER
AND
CORTES
USED
ADABOOST
WITH
A
DECISION
TREE
ALGORITHM
FOR
AN
OCR
TASK
JACKSON
AND
CRAVEN
USED
ADABOOST
TO
LEARN
CLASSIFIERS
REPRESENTED
BY
SPARSE
PERCEPTRONS
AND
TESTED
THE
ALGORITHM
ON
A
SET
OF
BENCHMARKS
FINALLY
QUINLAN
RECENTLY
CONDUCTED
AN
INDEPENDENT
COMPARISON
OF
BOOSTING
AND
BAGGING
COMBINED
WITH
ON
A
COLLECTION
OF
UCI
BENCHMARKS
FIGURE
THE
ALGORITHM
ADABOOST
THE
BOOSTING
ALGORITHM
IN
THIS
SECTION
WE
DESCRIBE
OUR
BOOSTING
ALGORITHM
CALLED
ADABOOST
SEE
OUR
EARLIER
PAPER
FOR
MORE
DETAILS
ABOUT
THE
ALGORITHM
AND
ITS
THEORETICAL
PROPERTIES
WE
DESCRIBE
TWO
VERSIONS
OF
THE
ALGORITHM
WHICH
WE
DENOTE
ADABOOST
AND
ADABOOST
THE
TWO
VER
SIONS
ARE
EQUIVALENT
FOR
BINARY
CLASSIFICATION
PROBLEMS
AND
DIFFER
ONLY
IN
THEIR
HANDLING
OF
PROBLEMS
WITH
MORE
THAN
TWO
CLASSES
ADABOOST
WE
BEGIN
WITH
THE
SIMPLER
VERSION
ADABOOST
THE
BOOSTING
ALGORITHM
TAKES
AS
INPUT
A
TRAINING
SET
OF
MEXAM
PLES
H
XM
YM
IWHERE
XIIS
AN
INSTANCE
DRAWN
FROM
SOME
SPACE
XAND
REPRESENTED
IN
SOME
MAN
NER
TYPICALLY
A
VECTOR
OF
ATTRIBUTE
VALUES
AND
YIEYIS
THE
CLASS
LABEL
ASSOCIATED
WITH
XI
IN
THIS
PAPER
WE
AL
WAYS
ASSUME
THAT
THE
SET
OF
POSSIBLE
LABELS
YIS
OF
FINITE
CARDINALITY
K
IN
ADDITION
THE
BOOSTING
ALGORITHMHAS
ACCESS
TO
ANOTHER
UNSPECIFIED
LEARNING
ALGORITHM
CALLED
THE
WEAK
LEARNING
ALGORITHM
WHICH
IS
DENOTED
GENERICALLY
AS
WEAKLEARN
THE
BOOSTING
ALGORITHM
CALLS
WEAKLEARN
REPEATEDLY
IN
A
SERIES
OF
ROUNDS
ON
ROUND
T
THE
BOOSTER
PROVIDES
WEAKLEARN
WITH
A
DISTRIBUTION
DTOVER
THE
TRAINING
SET
IN
RESPONSE
WEAKLEARN
COMPUTES
A
CLASSIFIER
OR
HY
POTHESIS
HT
X
YWHICH
SHOULD
CORRECTLY
CLASSIFY
A
FRACTION
OF
THE
TRAINING
SET
THAT
HAS
LARGE
PROBABILITY
WITH
RESPECT
TO
DT
THAT
IS
THE
WEAK
LEARNER
GOAL
IS
TO
FIND
A
HYPOTHESIS
HTWHICH
MINIMIZES
THE
TRAINING
ERROR
IT
PRI
DTHT
XI
YI
NOTE
THAT
THIS
ERROR
IS
MEASURED
WITH
RESPECT
TO
THE
DISTRIBUTION
DTTHAT
WAS
PROVIDED
TO
THE
WEAK
LEARNER
THIS
PROCESS
CONTINUES
FOR
TROUNDS
AND
AT
LAST
THE
BOOSTER
COMBINES
THE
WEAK
HYPOTHESES
HT
INTO
A
SINGLE
FINAL
HYPOTHESIS
H
N
ALGORITHM
ADABOOST
INPUT
SEQUENCE
OF
MEXAMPLES
H
XM
YM
I
WITH
LABELS
YIEY
KG
WEAK
LEARNING
ALGORITHM
WEAKLEARN
THEN
THE
FOLLOWING
UPPER
BOUND
HOLDS
ON
THE
ERROR
OF
THE
FINAL
HYPOTHESIS
HFIN
JFI
HFIN
X
YGJTT
INITIALIZE
I
Y
JBJFOR
I
Y
EB
DO
FOR
T
T
CALL
WEAKLEARN
PROVIDING
IT
WITH
MISLABEL
DISTRIBUTION
DT
GET
BACK
A
HYPOTHESIS
HT
XOY
CALCULATE
THE
PSEUDO
LOSS
OF
HT
THEOREM
IMPLIES
THAT
THE
TRAINING
ERROR
OF
THE
FINAL
HY
POTHESIS
GENERATED
BY
ADABOOST
IS
SMALL
THIS
DOES
NOT
NECESSARILY
IMPLY
THAT
THE
TEST
ERROR
IS
SMALL
HOWEVER
IF
THE
WEAK
HYPOTHESES
ARE
SIMPLE
AND
T
NOT
TOO
LARGE
TT
I
X
Y
EB
DT
I
Y
HT
XI
YI
THT
XI
Y
THEN
THE
DIFFERENCE
BETWEEN
THE
TRAINING
AND
TEST
ERRORS
CAN
ALSO
BE
THEORETICALLY
BOUNDED
SEE
OUR
EARLIER
PAPER
FOR
SET
TT
TT
UPDATE
DT
D
I
Y
DT
I
Y
HT
XI
YI
HT
XI
Y
MORE
ON
THIS
SUBJECT
THE
EXPERIMENTS
IN
THIS
PAPER
INDICATE
THAT
THE
THEORETI
CAL
BOUND
ON
THE
TRAINING
ERROR
IS
OFTEN
WEAK
BUT
GENERALLY
T
ZTT
CORRECT
QUALITATIVELY
HOWEVER
THE
TEST
ERROR
TENDS
TO
BE
WHERE
ZTIS
A
NORMALIZATION
CONSTANT
CHOSEN
SO
THAT
DT
WILL
BE
A
DISTRIBUTION
OUTPUT
THE
FINAL
HYPOTHESIS
HFIN
X
ARG
MAX
X
LOG
HH
X
Y
MUCH
BETTER
THAN
THE
THEORY
WOULD
SUGGEST
INDICATING
A
CLEAR
DEFECT
IN
OUR
THEORETICAL
UNDERSTANDING
THE
MAIN
DISADVANTAGE
OF
ADABOOST
IS
THAT
IT
IS
UNABLE
TO
HANDLE
WEAK
HYPOTHESES
WITH
ERROR
GREATER
THAN
YEY
T
THE
EXPECTED
ERROR
OF
A
HYPOTHESIS
WHICH
RANDOMLY
GUESSES
THE
LABEL
IS
WHERE
KIS
THE
NUMBER
OF
FIGURE
THE
ALGORITHM
ADABOOST
STILL
UNSPECIFIED
ARE
THE
MANNER
IN
WHICH
DTIS
COMPUTED
ON
EACH
ROUND
AND
HOW
H
NIS
COMPUTED
DIFFERENT
BOOSTING
SCHEMES
ANSWER
THESE
TWO
QUESTIONS
IN
DIFFERENT
WAYS
ADABOOST
USES
THE
SIMPLE
RULE
SHOWN
IN
FIGURE
THE
INITIAL
DISTRIBUTION
IS
UNIFORM
OVER
SSO
I
ALL
I
TO
COMPUTE
DISTRIBUTION
DT
FROM
DTAND
THE
LAST
WEAK
HYPOTHESIS
HT
WE
MULTIPLY
THE
WEIGHT
OF
EXAMPLE
IBY
SOME
NUMBER
JJTE
IF
HTCLASSIFIES
XI
CORRECTLY
AND
OTHERWISE
THE
WEIGHT
IS
LEFT
UNCHANGED
THE
WEIGHTS
ARE
THEN
RENORMALIZED
BY
DIVIDING
BY
THE
NORMAL
IZATION
CONSTANT
ZT
EFFECTIVELY
EASY
EXAMPLES
THAT
ARE
CORRECTLY
CLASSIFIED
BY
MANY
OF
THE
PREVIOUS
WEAK
HYPOTHE
SES
GET
LOWER
WEIGHT
AND
HARD
EXAMPLES
WHICH
TEND
OFTEN
TO
BE
MISCLASSIFIED
GET
HIGHER
WEIGHT
THUS
ADABOOST
FO
CUSES
THE
MOST
WEIGHT
ON
THE
EXAMPLES
WHICH
SEEM
TO
BE
HARDEST
FOR
WEAKLEARN
THE
NUMBER
JJTIS
COMPUTED
AS
SHOWN
IN
THE
FIGURE
AS
A
FUNCTION
OF
IT
THE
FINAL
HYPOTHESIS
H
NIS
A
WEIGHTED
VOTE
I
E
A
WEIGHTED
LINEAR
THRESHOLD
OF
THE
WEAK
HYPOTHESES
THAT
IS
FOR
A
GIVEN
INSTANCE
X
H
NOUTPUTS
THE
LABEL
YTHAT
MAXIMIZES
THE
SUM
OF
THE
WEIGHTS
OF
THE
WEAK
HYPOTHESES
PREDICTING
THAT
LABEL
THE
WEIGHT
OF
HYPOTHESIS
HTIS
DEFINED
TO
BE
LOG
SO
THAT
GREATER
WEIGHT
IS
GIVEN
TO
HYPOTHESES
WITH
LOWER
ERROR
THE
IMPORTANTTHEORETICAL
PROPERTYABOUT
ADABOOST
IS
STATED
IN
THE
FOLLOWING
THEOREM
THIS
THEOREM
SHOWS
THAT
IF
THE
WEAK
HYPOTHESES
CONSISTENTLY
HAVE
ERROR
ONLY
SLIGHTLY
BETTER
THAN
THEN
THE
TRAINING
ERROR
OF
THE
FINAL
HYPOTHESIS
H
NDROPS
TO
ZERO
EXPONENTIALLY
FAST
FOR
BINARY
CLASSIFI
CATION
PROBLEMS
THIS
MEANS
THAT
THE
WEAK
HYPOTHESES
NEED
BE
ONLY
SLIGHTLY
BETTER
THAN
RANDOM
THEOREM
SUPPOSE
THE
WEAK
LEARNING
ALGORITHM
WEAKLEARN
WHEN
CALLED
BY
ADABOOST
GENERATES
HY
POTHESES
WITH
ERRORS
IT
WHERE
ITIS
AS
DEFINED
IN
FIGURE
ASSUME
EACH
IT
AND
LET
T
IT
POSSIBLE
LABELS
THUS
FOR
K
THE
WEAK
HYPOTHESES
NEED
TO
BE
JUST
SLIGHTLY
BETTER
THAN
RANDOM
GUESSING
BUT
WHEN
K
THE
REQUIREMENT
THAT
THE
ERROR
BE
LESS
THAN
IS
QUITE
STRONG
AND
MAY
OFTEN
BE
HARD
TO
MEET
ADABOOST
THE
SECOND
VERSION
OF
ADABOOST
ATTEMPTS
TO
OVERCOME
THIS
DIFFICULTY
BY
EXTENDING
THE
COMMUNICATION
BETWEEN
THE
BOOSTING
ALGORITHM
AND
THE
WEAK
LEARNER
FIRST
WE
ALLOW
THE
WEAK
LEARNER
TO
GENERATE
MORE
EXPRESSIVE
HYPOTHESES
WHICH
RATHER
THAN
IDENTIFYING
A
SINGLE
LABEL
IN
Y
INSTEAD
CHOOSE
A
SET
OF
PLAUSIBLE
LABELS
THIS
MAY
OFTEN
BE
EASIER
THAN
CHOOSING
JUST
ONE
LABEL
FOR
INSTANCE
IN
AN
OCR
SETTING
IT
MAY
BE
HARD
TO
TELL
IF
A
PARTICULAR
IMAGE
IS
OR
A
BUT
EASY
TO
ELIMINATE
ALL
OF
THE
OTHER
POSSIBILITIES
IN
THIS
CASE
RATHER
THAN
CHOOSING
BETWEEN
AND
THE
HYPOTHESIS
MAY
OUTPUT
THE
SET
THAT
BOTH
LABELS
ARE
PLAUSIBLE
WE
ALSO
ALLOW
THE
WEAK
LEARNER
TO
INDICATE
A
DEGREE
OF
PLAUSIBILITY
THUS
EACH
WEAK
HYPOTHESIS
OUTPUTS
A
VECTOR
K
WHERE
THE
COMPONENTS
WITH
VALUES
CLOSE
TO
OR
CORRESPOND
TO
THOSE
LABELS
CONSIDERED
TO
BE
PLAUSIBLE
OR
IMPLAUSIBLE
RESPECTIVELY
NOTE
THAT
THIS
VECTOR
OF
VALUES
IS
NOT
A
PROBABILITY
VECTOR
I
E
THE
COMPONENTS
NEED
NOT
SUM
TO
ONE
WHILE
WE
GIVE
THE
WEAK
LEARNING
ALGORITHM
MORE
EX
PRESSIVE
POWER
WE
ALSO
PLACE
A
MORE
COMPLEX
REQUIREMENT
ON
THE
PERFORMANCE
OF
THE
WEAK
HYPOTHESES
RATHER
THAN
USING
THE
USUAL
PREDICTION
ERROR
WE
ASK
THAT
THE
WEAK
HY
POTHESES
DO
WELL
WITH
RESPECT
TO
A
MORE
SOPHISTICATED
ERROR
MEASURE
THAT
WE
CALL
THE
PSEUDO
LOSS
UNLIKE
ORDINARY
ERROR
WHICH
IS
COMPUTED
WITH
RESPECT
TO
A
DISTRIBUTIONOVER
EXAM
PLES
PSEUDO
LOSS
IS
COMPUTED
WITH
RESPECT
TO
A
DISTRIBUTION
DELIBERATELY
USE
THE
TERM
PLAUSIBLE
RATHER
THAN
PROB
ABLE
TO
EMPHASIZE
THE
FACT
THAT
THESE
NUMBERS
SHOULD
NOT
BE
INTERPRETED
AS
THE
PROBABILITY
OF
A
GIVEN
LABEL
OVER
THE
SET
OF
ALL
PAIRS
OF
EXAMPLES
AND
INCORRECT
LABELS
BY
MANIPULATING
THIS
DISTRIBUTION
THE
BOOSTING
ALGORITHM
CAN
FOCUS
THE
WEAK
LEARNER
NOT
ONLY
ON
HARD
TO
CLASSIFY
EX
AMPLES
BUT
MORE
SPECIFICALLY
ON
THE
INCORRECT
LABELS
THAT
ARE
HARDEST
TO
DISCRIMINATE
WE
WILL
SEE
THAT
THE
BOOSTING
ALGORITHM
ADABOOST
WHICH
IS
BASED
ON
THESE
IDEAS
ACHIEVES
BOOSTING
IF
EACH
WEAK
HYPOTHESIS
HAS
PSEUDO
LOSS
SLIGHTLY
BETTER
THAN
RANDOM
GUESSING
MORE
FORMALLY
A
MISLABEL
IS
A
PAIR
I
Y
WHERE
IIS
THE
INDEX
OF
A
TRAINING
EXAMPLE
AND
YIS
AN
INCORRECT
LABEL
ASSOCIATED
WITH
EXAMPLE
I
LET
BBE
THE
SET
OF
ALL
MISLA
BELS
B
F
I
Y
MG
Y
YIG
A
MISLABEL
DISTRIBUTION
IS
A
DISTRIBUTION
DEFINED
OVER
THE
SET
BOF
ALL
MISLABELS
ONLY
THAT
THE
WEAK
HYPOTHESES
HAVE
PSEUDO
LOSS
LESS
THAN
I
E
ONLY
SLIGHTLY
BETTER
THAN
A
TRIVIAL
CONSTANT
VALUED
HYPOTHESIS
REGARDLESS
OF
THE
NUMBER
OF
CLASSES
ALSO
AL
THOUGH
THE
WEAK
HYPOTHESES
HTARE
EVALUATED
WITH
RESPECT
TO
THE
PSEUDO
LOSS
WE
OF
COURSE
EVALUATE
THE
FINAL
HYPOTHESIS
H
NUSING
THE
ORDINARY
ERROR
MEASURE
THEOREM
SUPPOSE
THE
WEAK
LEARNING
ALGORITHM
WEAKLEARN
WHEN
CALLED
BY
ADABOOST
GENERATES
HY
POTHESES
WITH
PSEUDO
LOSSES
IT
WHERE
ITIS
AS
DE
FINED
IN
FIGURE
LET
T
IT
THEN
THE
FOLLOWING
UPPER
BOUND
HOLDS
ON
THE
ERROR
OF
THE
FINAL
HYPOTHESIS
HFIN
JFI
H
X
YGJ
ON
EACH
ROUND
TOF
BOOSTING
ADABOOST
FIGURE
FIN
I
M
I
K
SUPPLIES
THE
WEAK
LEARNER
WITH
A
MISLABEL
DISTRIBUTION
DT
IN
RESPONSE
THE
WEAK
LEARNER
COMPUTES
A
HYPOTHESIS
HTOF
T
T
THE
FORM
HT
XRY
THERE
IS
NORESTRICTION
ON
YHT
X
Y
IN
PARTICULAR
THE
PREDICTION
VECTOR
DOES
NOT
HAVE
TO
DEFINE
A
PROBABILITY
DISTRIBUTION
INTUITIVELY
WE
INTERPRET
EACH
MISLABEL
I
Y
AS
REPRE
SENTING
A
BINARY
QUESTION
OF
THE
FORM
DO
YOU
PREDICT
THAT
THE
LABEL
ASSOCIATED
WITH
EXAMPLE
XIIS
YI
THE
CORRECT
K
EXP
WHERE
KIS
THE
NUMBER
OF
CLASSES
BOOSTING
AND
BAGGING
T
T
LABEL
OR
Y
ONE
OF
THE
INCORRECT
LABELS
WITH
THIS
IN
TERPRETATION
THE
WEIGHT
DT
I
Y
ASSIGNED
TO
THIS
MISLABEL
REPRESENTS
THE
IMPORTANCE
OF
DISTINGUISHING
INCORRECT
LABEL
YON
EXAMPLE
XI
A
WEAK
HYPOTHESIS
HTIS
THEN
INTERPRETED
IN
THE
FOLLOWING
MANNER
IF
HT
XI
YI
AND
HT
XI
Y
THEN
HTHAS
CORRECTLY
PREDICTED
THAT
XI
LABEL
IS
YI
NOT
Y
SINCE
HT
DEEMS
YITO
BE
PLAUSIBLE
AND
Y
IMPLAUSIBLE
SIMILARLY
IF
HT
XI
YI
AND
HT
XI
Y
THEN
HTHAS
INCORRECTLY
MADE
THE
OPPOSITE
PREDICTION
IF
HT
XI
YI
HT
XI
Y
THEN
HT
PREDICTION
IS
TAKEN
TO
BE
A
RANDOM
GUESS
VALUES
FOR
HTIN
ARE
INTERPRETED
PROBABILISTICALLY
THIS
INTERPRETATION
LEADS
US
TO
DEFINE
THE
PSEUDO
LOSS
OF
HYPOTHESIS
HTWITH
RESPECT
TO
MISLABEL
DISTRIBUTION
DTBY
THE
FORMULA
IN
THIS
SECTION
WE
DESCRIBE
OUR
EXPERIMENTS
COMPARING
BOOSTING
AND
BAGGING
ON
THE
UCI
BENCHMARKS
WE
FIRST
MENTION
BRIEFLY
A
SMALL
IMPLEMENTATION
ISSUE
MANY
LEARNING
ALGORITHMS
CAN
BE
MODIFIED
TO
HANDLE
EX
AMPLES
THAT
ARE
WEIGHTED
BY
A
DISTRIBUTION
SUCH
AS
THE
ONE
CREATED
BY
THE
BOOSTING
ALGORITHM
WHEN
THIS
IS
POSSI
BLE
THE
BOOSTER
DISTRIBUTION
DTIS
SUPPLIED
DIRECTLY
TO
THE
WEAK
LEARNING
ALGORITHM
A
METHOD
WE
CALL
BOOSTING
BY
REWEIGHTING
HOWEVER
SOME
LEARNING
ALGORITHMS
REQUIRE
AN
UNWEIGHTED
SET
OF
EXAMPLES
FOR
SUCH
A
WEAK
LEARN
ING
ALGORITHM
WE
INSTEAD
CHOOSE
A
SET
OF
EXAMPLES
FROM
INDEPENDENTLY
AT
RANDOM
ACCORDING
TO
THE
DISTRIBUTION
DT
WITH
REPLACEMENT
THE
NUMBER
OF
EXAMPLES
TO
BE
CHOSEN
ON
EACH
ROUND
IS
A
MATTER
OF
DISCRETION
IN
OUR
EXPERIMENTS
WE
CHOSE
MEXAMPLES
ON
EACH
ROUND
WHERE
MIS
THE
SIZE
OF
THE
ORIGINAL
TRAINING
SET
WE
REFER
TO
THIS
METHOD
AS
IT
I
Y
EB
DT
I
Y
HT
XI
YI
HT
XI
Y
BOOSTING
BY
RESAMPLING
BOOSTING
BY
RESAMPLING
IS
ALSO
POSSIBLE
WHEN
USING
THE
SPACE
LIMITATIONS
PREVENT
US
FROM
GIVING
A
COMPLETE
DERIVA
TION
OF
THIS
FORMULA
WHICH
IS
EXPLAINED
IN
DETAIL
IN
OUR
EARLIER
PAPER
IT
CAN
BE
VERIFIED
THOUGH
THAT
THE
PSEUDO
LOSS
IS
MINIMIZED
WHEN
CORRECT
LABELS
YIARE
ASSIGNED
THE
VALUE
AND
INCORRECT
LABELS
Y
YIASSIGNED
THE
VALUE
FUR
THER
NOTE
THAT
PSEUDO
LOSS
IS
TRIVIALLY
ACHIEVED
BY
ANY
CONSTANT
VALUED
HYPOTHESIS
HT
THE
WEAK
LEARNER
GOAL
IS
TO
FIND
A
WEAK
HYPOTHESIS
HTWITH
SMALL
PSEUDO
LOSS
THUS
STANDARD
OFF
THE
SHELF
LEARNING
ALGORITHMS
MAY
NEED
SOME
MODIFICATION
TO
BE
USED
IN
THIS
MANNER
ALTHOUGH
THIS
MODIFICATION
IS
OFTEN
STRAIGHT
FORWARD
AFTER
RECEIVING
HT
THE
MISLABEL
DISTRIBUTION
IS
UP
DATED
USING
A
RULE
SIMILAR
TO
THE
ONE
USED
IN
ADABOOST
THE
FINAL
HYPOTHESIS
H
NOUTPUTS
FOR
A
GIVEN
INSTANCE
X
THE
LABEL
YTHAT
MAXIMIZES
A
WEIGHTED
AVERAGE
OF
THE
WEAK
HYPOTHESIS
VALUES
HT
X
Y
THE
FOLLOWING
THEOREM
GIVES
A
BOUND
ON
THE
TRAINING
ER
ROR
OF
THE
FINAL
HYPOTHESIS
NOTE
THAT
THIS
THEOREM
REQUIRES
PSEUDO
LOSS
IN
THIS
CASE
A
SET
OF
MISLABELS
ARE
CHOSEN
FROM
THE
SET
BOF
ALL
MISLABELS
WITH
REPLACEMENT
ACCORDING
TO
THE
GIVEN
DISTRIBUTION
DT
SUCH
A
PROCEDURE
IS
CONSISTENT
WITH
THE
INTERPRETATION
OF
MISLABELS
DISCUSSED
IN
SECTION
IN
OUR
EXPERIMENTS
WE
CHOSE
A
SAMPLE
OF
SIZE
JBJ
M
K
ON
EACH
ROUND
WHEN
USING
THE
RESAMPLING
METHOD
THE
WEAK
LEARNING
ALGORITHMS
AS
MENTIONED
IN
THE
INTRODUCTION
WE
USED
THREE
WEAK
LEARN
ING
ALGORITHMS
IN
THESE
EXPERIMENTS
IN
ALL
CASES
THE
EXAM
PLES
ARE
DESCRIBED
BY
A
VECTOR
OF
VALUES
WHICH
CORRESPONDS
TO
A
FIXED
SET
OF
FEATURES
OR
ATTRIBUTES
THESE
VALUES
MAY
BE
DISCRETE
OR
CONTINUOUS
SOME
OF
THE
EXAMPLES
MAY
HAVE
MISSING
VALUES
ALL
THREE
OF
THE
WEAK
LEARNERS
BUILD
HY
POTHESES
WHICH
CLASSIFY
EXAMPLES
BY
REPEATEDLY
TESTING
THE
VALUES
OF
CHOSEN
ATTRIBUTES
THE
FIRST
AND
SIMPLEST
WEAK
LEARNER
WHICH
WE
CALL
FINDATTRTEST
SEARCHES
FOR
THE
SINGLE
ATTRIBUTE
VALUE
TEST
BOOSTING
FINDATTRTEST
BOOSTING
FINDDECRULE
BAGGING
FINDATTRTEST
BAGGING
FINDDECRULE
PSEUDO
LOSS
TABLE
THE
BENCHMARK
MACHINE
LEARNING
PROBLEMS
USED
IN
THE
EXPERIMENTS
WITH
MINIMUM
ERROR
OR
PSEUDO
LOSS
ON
THE
TRAINING
SET
MORE
PRECISELY
FINDATTRTEST
COMPUTES
A
CLASSIFIER
WHICH
IS
DEFINED
BY
AN
ATTRIBUTE
A
A
VALUE
VAND
THREE
PREDICTIONS
AND
P
THIS
CLASSIFIER
CLASSIFIES
A
NEW
EXAMPLE
X
AS
FOLLOWS
IF
THE
VALUE
OF
ATTRIBUTE
AIS
MISSING
ON
X
THEN
PREDICT
P
IF
ATTRIBUTE
AIS
DISCRETE
AND
ITS
VALUE
ON
EXAMPLE
XIS
EQUAL
TO
V
OR
IF
ATTRIBUTE
AIS
CONTINUOUS
AND
ITS
VALUE
ON
XIS
AT
MOST
V
THEN
PREDICT
OTHERWISE
PREDICT
IF
USING
ORDINARY
ERROR
ADABOOST
THESE
PREDICTIONS
P
WOULD
BE
SIMPLE
CLASSIFICATIONS
FOR
PSEUDO
LOSS
THE
PREDICTIONS
WOULD
BE
VECTORS
IN
K
WHERE
KIS
THE
NUMBER
OF
CLASSES
THE
ALGORITHM
FINDATTRTEST
SEARCHES
EXHAUSTIVELY
FOR
THE
CLASSIFIER
OF
THE
FORM
GIVEN
ABOVE
WITH
MINIMUM
ERROR
OR
PSEUDO
LOSS
WITH
RESPECT
TO
THE
DISTRIBUTION
PROVIDED
BY
THE
BOOSTER
IN
OTHER
WORDS
ALL
POSSIBLE
VALUES
OF
A
V
AND
P
ARE
CONSIDERED
WITH
SOME
PREPROCESSING
THIS
SEARCH
CAN
BE
CARRIED
OUT
FOR
THE
ERROR
BASED
IMPLEMENTATION
IN
O
NM
TIME
WHERE
NIS
THE
NUMBER
OF
ATTRIBUTES
AND
MTHE
NUMBER
OF
EXAMPLES
AS
IS
TYPICAL
THE
PSEUDO
LOSS
IMPLEMENTATION
ADDS
A
FACTOR
OF
O
K
WHERE
KIS
THE
NUMBER
OF
CLASS
LABELS
FOR
THIS
ALGORITHM
WE
USED
BOOSTING
WITH
REWEIGHTING
THE
SECOND
WEAK
LEARNER
DOES
A
SOMEWHAT
MORE
SOPHIS
TICATED
SEARCH
FOR
A
DECISION
RULE
THAT
TESTS
ON
A
CONJUNCTION
OF
ATTRIBUTE
VALUE
TESTS
WE
SKETCH
THE
MAIN
IDEAS
OF
THIS
ALGORITHM
WHICH
WE
CALL
FINDDECRULE
BUT
OMIT
SOME
OF
THE
FINER
DETAILS
FOR
LACK
OF
SPACE
THESE
DETAILS
WILL
BE
PROVIDED
IN
THE
FULL
PAPER
FIRST
THE
ALGORITHM
REQUIRES
AN
UNWEIGHTED
TRAINING
SET
SO
WE
USE
THE
RESAMPLING
VERSION
OF
BOOSTING
THE
GIVEN
TRAINING
SET
IS
RANDOMLY
DIVIDED
INTO
A
GROWING
SET
USING
OF
THE
DATA
AND
A
PRUNING
SET
WITH
THE
REMAINING
FIGURE
COMPARISON
OF
USING
PSEUDO
LOSS
VERSUS
ORDINARY
ERROR
ON
MULTI
CLASS
PROBLEMS
FOR
BOOSTING
AND
BAGGING
IN
THE
FIRST
PHASE
THE
GROWING
SET
IS
USED
TO
GROW
A
LIST
OF
ATTRIBUTE
VALUE
TESTS
EACH
TEST
COMPARES
AN
ATTRIBUTE
ATO
A
VALUE
V
SIMILAR
TO
THE
TESTS
USED
BY
FINDATTRTEST
WE
USE
AN
ENTROPY
BASED
POTENTIAL
FUNCTION
TO
GUIDE
THE
GROWTH
OF
THE
LIST
OF
TESTS
THE
LIST
IS
INITIALLY
EMPTY
AND
ONE
TEST
IS
ADDED
AT
A
TIME
EACH
TIME
CHOOSING
THE
TEST
THAT
WILL
CAUSE
THE
GREATEST
DROP
IN
POTENTIAL
AFTER
THE
TEST
IS
CHOSEN
ONLY
ONE
BRANCH
IS
EXPANDED
NAMELY
THE
BRANCH
WITH
THE
HIGHEST
REMAINING
POTENTIAL
THE
LIST
CONTINUES
TO
BE
GROWN
IN
THIS
FASHION
UNTIL
NO
TEST
REMAINS
WHICH
WILL
FURTHER
REDUCE
THE
POTENTIAL
IN
THE
SECOND
PHASE
THE
LIST
IS
PRUNED
BY
SELECTING
THE
PREFIX
OF
THE
LIST
WITH
MINIMUM
ERROR
OR
PSEUDO
LOSS
ON
THE
PRUNING
SET
THE
THIRD
WEAK
LEARNER
IS
QUINLAN
DECISION
TREE
ALGORITHM
WE
USED
ALL
THE
DEFAULT
OPTIONS
WITH
PRUNING
TURNED
ON
SINCE
EXPECTS
AN
UNWEIGHTED
TRAINING
SAM
PLE
WE
USED
RESAMPLING
ALSO
WE
DID
NOT
ATTEMPT
TO
USE
ADABOOST
SINCE
IS
DESIGNED
TO
MINIMIZE
ERROR
NOT
PSEUDO
LOSS
FURTHERMORE
WE
DID
NOT
EXPECT
PSEUDO
LOSS
TO
BE
HELPFUL
WHEN
USING
A
WEAK
LEARNING
ALGORITHM
AS
STRONG
AS
SINCE
SUCH
AN
ALGORITHM
WILL
USUALLY
BE
ABLE
TO
FIND
A
HYPOTHESIS
WITH
ERROR
LESS
THAN
BAGGING
WE
COMPARED
BOOSTING
TO
BREIMAN
BOOTSTRAP
AGGRE
GATING
OR
BAGGING
METHOD
FOR
TRAINING
AND
COMBINING
MULTIPLE
COPIES
OF
A
LEARNING
ALGORITHM
BRIEFLY
THE
METHOD
WORKS
BY
TRAINING
EACH
COPY
OF
THE
ALGORITHM
ON
A
BOOTSTRAP
SAMPLE
I
E
A
SAMPLE
OF
SIZE
MCHOSEN
UNIFORMLY
AT
RANDOM
WITH
REPLACEMENT
FROM
THE
ORIGINAL
TRAINING
SET
OF
SIZE
M
THE
MULTIPLE
HYPOTHESES
THAT
ARE
COMPUTED
ARE
THEN
COMBINED
USING
SIMPLE
VOTING
THAT
IS
THE
FINAL
COMPOSITE
HYPOTHESIS
CLASSIFIES
AN
EXAMPLE
XTO
THE
CLASS
MOST
OFTEN
ASSIGNED
BY
THE
UNDERLYING
WEAK
HYPOTHESES
SEE
HIS
PAPER
FOR
MORE
DETAILS
THE
METHOD
CAN
BE
QUITE
EFFECTIVE
ESPECIALLY
ACCORDING
TO
BREIMAN
FOR
UNSTABLE
LEARNING
ALGORITHMS
FOR
WHICH
A
SMALL
CHANGE
IN
THE
DATA
EFFECTS
A
LARGE
CHANGE
IN
THE
COMPUTED
HYPOTHESIS
IN
ORDER
TO
COMPARE
ADABOOST
WHICH
USES
PSEUDO
LOSS
TO
BAGGING
WE
ALSO
EXTENDED
BAGGING
IN
A
NATURAL
WAY
FOR
USE
WITH
A
WEAK
LEARNING
ALGORITHM
THAT
MINIMIZES
PSEUDO
LOSS
RATHER
THAN
ORDINARY
ERROR
AS
DESCRIBED
IN
SECTION
SUCH
A
WEAK
LEARNING
ALGORITHM
EXPECTS
TO
BE
PROVIDED
WITH
A
DISTRIBUTION
OVER
THE
SET
BOF
ALL
MISLABELS
ON
EACH
ROUND
OF
BAGGING
WE
CONSTRUCT
THIS
DISTRIBUTION
USING
THE
BOOTSTRAP
METHOD
THAT
IS
WE
SELECT
JBJMISLABELS
FROM
B
CHOSEN
UNIFORMLY
AT
RANDOM
WITH
REPLACEMENT
FINDATTRTEST
FINDDECRULE
BOOSTING
BOOSTING
FINDATTRTEST
BOOSTING
FINDDECRULE
BOOSTING
BAGGING
FIGURE
COMPARISON
OF
BOOSTING
AND
BAGGING
FOR
EACH
OF
THE
WEAK
LEARNERS
AND
ASSIGN
EACH
MISLABEL
WEIGHT
THE
NUMBER
OF
TIMES
IT
WAS
CHOSEN
THE
HYPOTHESES
HTCOMPUTED
IN
THIS
MANNER
ARE
THEN
COMBINED
USING
VOTING
IN
A
NATURAL
MANNER
NAMELY
GIVEN
X
THE
COMBINED
HYPOTHESIS
OUTPUTS
THE
LABEL
YWHICH
MAXIMIZES
THT
X
Y
FOR
EITHER
ERROR
OR
PSEUDO
LOSS
THE
DIFFERENCES
BETWEEN
BAGGING
AND
BOOSTING
CAN
BE
SUMMARIZED
AS
FOLLOWS
BAGGING
ALWAYS
USES
RESAMPLING
RATHER
THAN
REWEIGHTING
BAGGING
DOES
NOT
MODIFY
THE
DISTRIBUTION
OVER
EXAMPLES
OR
MISLABELS
BUT
INSTEAD
ALWAYS
USES
THE
UNIFORM
DISTRIBUTION
AND
IN
FORMING
THE
FINAL
HYPOTHESIS
BAGGING
GIVES
EQUAL
WEIGHT
TO
EACH
OF
THE
WEAK
HYPOTHESES
THE
EXPERIMENTS
WE
CONDUCTED
OUR
EXPERIMENTS
ON
A
COLLECTION
OF
MACHINE
LEARNING
DATASETS
AVAILABLE
FROM
THE
REPOSITORY
AT
UNIVERSITY
OF
CALIFORNIA
AT
IRVINE
A
SUMMARY
OF
SOME
OF
THE
PROPER
TIES
OF
THESE
DATASETS
IS
GIVEN
IN
TABLE
SOME
DATASETS
ARE
PROVIDED
WITH
A
TEST
SET
FOR
THESE
WE
RERAN
EACH
ALGORITHM
TIMES
SINCE
SOME
OF
THE
ALGORITHMS
ARE
RANDOMIZED
AND
AVERAGED
THE
RESULTS
FOR
DATASETS
WITH
NO
PROVIDED
TEST
SET
WE
USED
FOLD
CROSS
VALIDATION
AND
AVERAGED
THE
RE
SULTS
OVER
RUNS
FOR
A
TOTAL
OF
RUNS
OF
EACH
ALGORITHM
ON
EACH
DATASET
IN
ALL
OUR
EXPERIMENTS
WE
SET
THE
NUMBER
OF
ROUNDS
OF
BOOSTING
OR
BAGGING
TO
BE
T
RESULTS
AND
DISCUSSION
THE
RESULTS
OF
OUR
EXPERIMENTS
ARE
SHOWN
IN
TABLE
THE
FIGURES
INDICATE
TEST
ERROR
RATE
AVERAGED
OVER
MUL
TIPLE
RUNS
OF
EACH
ALGORITHM
COLUMNS
INDICATE
WHICH
WEAK
LEARNING
ALGORITHM
WAS
USED
AND
WHETHER
PSEUDO
LOSS
ADABOOST
OR
ERROR
ADABOOST
WAS
USED
NOTE
THAT
PSEUDO
LOSS
WAS
NOT
USED
ON
ANY
TWO
CLASS
PROB
LEMS
SINCE
THE
RESULTING
ALGORITHM
WOULD
BE
IDENTICAL
TO
THE
CORRESPONDING
ERROR
BASED
ALGORITHM
COLUMNS
LABELED
INDICATE
THAT
THE
WEAK
LEARNING
ALGORITHM
WAS
USED
BY
ITSELF
WITH
NO
BOOSTING
OR
BAGGING
COLUMNS
USING
BOOSTING
OR
BAGGING
ARE
MARKED
BOOST
AND
BAG
RESPECTIVELY
ONE
OF
OUR
GOALS
IN
CARRYING
OUT
THESE
EXPERIMENTS
WAS
TO
DETERMINE
IF
BOOSTING
USING
PSEUDO
LOSS
RATHER
THAN
ER
ROR
IS
WORTHWHILE
FIGURE
SHOWS
HOW
THE
DIFFERENT
AL
GORITHMS
PERFORMED
ON
EACH
OF
THE
MANY
CLASS
K
PROBLEMS
USING
PSEUDO
LOSS
VERSUS
ERROR
EACH
POINT
IN
THE
SCATTER
PLOT
REPRESENTS
THE
ERROR
ACHIEVED
BY
THE
TWO
COMPET
ING
ALGORITHMS
ON
A
GIVEN
BENCHMARK
SO
THERE
IS
ONE
POINT
HTML
FIGURE
COMPARISON
OF
VERSUS
VARIOUS
OTHER
BOOSTING
AND
BAGGING
METHODS
FOR
EACH
BENCHMARK
THESE
EXPERIMENTS
INDICATE
THAT
BOOST
ING
USING
PSEUDO
LOSS
CLEARLY
OUTPERFORMS
BOOSTING
USING
ERROR
USING
PSEUDO
LOSS
DID
DRAMATICALLY
BETTER
THAN
ERROR
ON
EVERY
NON
BINARY
PROBLEM
EXCEPT
IT
DID
SLIGHTLY
WORSE
ON
IRIS
WITH
THREE
CLASSES
BECAUSE
ADABOOST
DID
SO
MUCH
BETTER
THAN
ADABOOST
WE
WILL
ONLY
DISCUSS
ADABOOST
HENCEFORTH
AS
THE
FIGURE
SHOWS
USING
PSEUDO
LOSS
WITH
BAGGING
GAVE
MIXED
RESULTS
IN
COMPARISON
TO
ORDINARY
ERROR
OVER
ALL
PSEUDO
LOSS
GAVE
BETTER
RESULTS
BUT
OCCASIONALLY
USING
PSEUDO
LOSS
HURT
CONSIDERABLY
FIGURE
SHOWS
SIMILAR
SCATTERPLOTS
COMPARING
THE
PER
FORMANCE
OF
BOOSTING
AND
BAGGING
FOR
ALL
THE
BENCHMARKS
AND
ALL
THREE
WEAK
LEARNER
FOR
BOOSTING
WE
PLOTTED
THE
ER
ROR
RATE
ACHIEVED
USING
PSEUDO
LOSS
TO
PRESENT
BAGGING
IN
THE
BEST
POSSIBLE
LIGHT
WE
USED
THE
ERROR
RATE
ACHIEVED
USING
EITHER
ERROR
OR
PSEUDO
LOSS
WHICHEVER
GAVE
THE
BETTER
RESULT
ON
THAT
PARTICULAR
BENCHMARK
FOR
THE
BINARY
PROBLEMS
AND
EXPERIMENTS
WITH
ONLY
ERROR
WAS
USED
FOR
THE
SIMPLER
WEAK
LEARNING
ALGORITHMS
FINDATTR
TEST
AND
FINDDECRULE
BOOSTING
DID
SIGNIFICANTLY
AND
UNI
FORMLY
BETTER
THAN
BAGGING
THE
BOOSTING
ERROR
RATE
WAS
WORSE
THAN
THE
BAGGING
ERROR
RATE
USING
EITHER
PSEUDO
LOSS
OR
ERROR
ON
A
VERY
SMALL
NUMBER
OF
BENCHMARK
PROBLEMS
AND
ON
THESE
THE
DIFFERENCE
IN
PERFORMANCE
WAS
QUITE
SMALL
ON
AVERAGE
FOR
FINDATTRTEST
BOOSTING
IMPROVED
THE
ERROR
RATE
OVER
USING
FINDATTRTEST
ALONE
BY
COMPARED
TO
BAGGING
WHICH
GAVE
AN
IMPROVEMENT
OF
ONLY
USING
PSEUDO
LOSS
OR
USING
ERROR
FOR
FINDDECRULE
BOOST
ING
IMPROVED
THE
ERROR
RATE
BY
BAGGING
BY
ONLY
USING
PSEUDO
LOSS
USING
ERROR
WHEN
USING
AS
THE
WEAK
LEARNING
ALGORITHM
BOOST
ING
AND
BAGGING
SEEM
MORE
EVENLY
MATCHED
ALTHOUGH
BOOSTING
STILL
SEEMS
TO
HAVE
A
SLIGHT
ADVANTAGE
ON
AV
ERAGE
BOOSTING
IMPROVED
THE
ERROR
RATE
BY
BAGGING
BY
BOOSTING
BEAT
BAGGING
BY
MORE
THAN
ON
OF
THE
BENCHMARKS
WHILE
BAGGING
DID
NOT
BEAT
BOOSTINGBY
THIS
AMOUNT
ON
ANY
BENCHMARK
FOR
THE
REMAINING
BENCH
MARKS
THE
DIFFERENCE
IN
PERFORMANCE
WAS
LESS
THAN
FIGURE
SHOWS
IN
A
SIMILAR
MANNER
HOW
PERFORMED
COMPARED
TO
BAGGING
WITH
AND
COMPARED
TO
BOOSTING
WITH
EACH
OF
THE
WEAK
LEARNERS
USING
PSEUDO
LOSS
FOR
THE
NON
BINARY
PROBLEMS
AS
THE
FIGURE
SHOWS
USING
BOOSTING
WITH
FINDATTRTEST
DOES
QUITE
WELL
AS
A
LEARNING
ALGORITHM
IN
ITS
OWN
RIGHT
IN
COMPARISON
TO
THIS
ALGORITHM
BEAT
ON
OF
THE
BENCHMARKS
BY
AT
LEAST
TIED
ON
AND
LOST
ON
AS
MENTIONED
ABOVE
ITS
AVERAGE
PERFORMANCE
RELATIVE
TO
USING
FINDATTRTEST
BY
ITSELF
WAS
IN
COMPARISON
IMPROVEMENT
IN
PERFORMANCE
TABLE
TEST
ERROR
RATES
OF
VARIOUS
ALGORITHMS
ON
BENCHMARK
PROBLEMS
OVER
FINDATTRTEST
WAS
USING
BOOSTING
WITH
FINDDECRULE
DID
SOMEWHAT
BET
TER
THE
WIN
TIE
LOSE
NUMBERS
FOR
THIS
ALGORITHM
COMPARED
TO
WERE
AND
ITS
AVERAGE
IMPROVEMENT
OVER
FINDATTRTEST
WAS
BOOSTING
A
NEAREST
NEIGHBOR
CLASSIFIER
IN
THIS
SECTION
WE
STUDY
THE
PERFORMANCE
OF
A
LEARNING
AL
GORITHM
WHICH
COMBINES
ADABOOST
AND
A
VARIANT
OF
THE
NEAREST
NEIGHBOR
CLASSIFIER
WE
TEST
THE
COMBINED
ALGORITHM
ON
THE
PROBLEM
OF
RECOGNIZING
HANDWRITTEN
DIGITS
OUR
GOAL
IS
NOT
TO
IMPROVE
ON
THE
ACCURACY
OF
THE
NEAREST
NEIGHBOR
CLASSIFIER
BUT
RATHER
TO
SPEED
IT
UP
SPEED
UP
IS
ACHIEVED
BY
REDUCING
THE
NUMBER
OF
PROTOTYPES
IN
THE
HYPOTHESIS
AND
THUS
THE
REQUIRED
NUMBER
OF
DISTANCE
CALCULATIONS
WITHOUT
INCREASING
THE
ERROR
RATE
IT
IS
A
SIMILAR
APPROACH
TO
THAT
OF
NEAREST
NEIGHBOR
EDITING
IN
WHICH
ONE
TRIES
TO
FIND
THE
MINIMAL
SET
OF
PROTOTYPES
THAT
IS
SUFFICIENT
TO
LABEL
ALL
THE
TRAINING
SET
CORRECTLY
THE
DATASET
COMES
FROM
THE
US
POSTAL
SERVICE
USPS
AND
CONSISTS
OF
TRAINING
EXAMPLES
AND
TEST
EXAM
PLES
THE
TRAINING
AND
TEST
EXAMPLES
ARE
EVIDENTLY
DRAWN
FROM
RATHER
DIFFERENT
DISTRIBUTIONS
AS
THERE
IS
A
VERY
SIGNIFI
CANT
IMPROVEMENT
IN
THE
PERFORMANCE
IF
THE
PARTITION
OF
THE
DATA
INTO
TRAINING
AND
TESTING
IS
DONE
AT
RANDOM
RATHER
THAN
USING
THE
GIVEN
PARTITION
WE
REPORT
RESULTS
BOTH
ON
THE
ORIGINAL
PARTITIONING
AND
ON
A
TRAINING
SET
AND
A
TEST
SET
OF
THE
SAME
SIZES
THAT
WERE
GENERATED
BY
RANDOMLY
PARTITIONING
THE
UNION
OF
THE
ORIGINAL
TRAINING
AND
TEST
SETS
EACH
IMAGE
IS
REPRESENTED
BY
A
MATRIX
OF
BIT
PIXELS
THE
METRIC
THAT
WE
USE
FOR
IDENTIFYING
THE
NEAR
EST
NEIGHBOR
AND
HENCE
FOR
CLASSIFYING
AN
INSTANCE
IS
THE
STANDARD
EUCLIDEAN
DISTANCE
BETWEEN
THE
IMAGES
VIEWED
AS
VECTORS
IN
R
THIS
IS
A
VERY
NAIVE
METRIC
BUT
IT
GIVES
REASONABLY
GOOD
PERFORMANCE
A
NEAREST
NEIGHBOR
CLASSIFIER
WHICH
USES
ALL
THE
TRAINING
EXAMPLES
AS
PROTOTYPES
ACHIEVES
A
TEST
ERROR
OF
ON
RANDOMLY
PARTITIONED
DATA
USING
THE
MORE
SOPHISTICATED
TANGENT
DISTANCE
IS
IN
OUR
FUTURE
PLANS
EACH
WEAK
HYPOTHESIS
IS
DEFINED
BY
A
SUBSET
POF
THE
TRAINING
EXAMPLES
AND
A
MAPPING
P
K
GIVEN
A
NEW
TEST
POINT
X
SUCH
A
WEAK
HYPOTHESIS
PREDICTS
THE
VECTOR
WHERE
EPIS
THE
CLOSEST
POINT
TO
X
ON
EACH
ROUND
OF
BOOSTING
A
WEAK
HYPOTHESIS
IS
GENER
ATED
BY
ADDING
ONE
PROTOTYPE
AT
A
TIME
TO
THE
SET
PUNTIL
THE
SET
REACHES
A
PRESPECIFIED
SIZE
GIVEN
ANY
SET
P
WE
ALWAYS
CHOOSE
THE
MAPPING
MINIMIZES
THE
PSEUDO
LOSS
OF
THE
RESULTING
WEAK
HYPOTHESIS
WITH
RESPECT
TO
THE
GIVEN
MISLABEL
DISTRIBUTION
INITIALLY
THE
SET
OF
PROTOTYPES
PIS
EMPTY
NEXT
TEN
CANDIDATE
PROTOTYPES
ARE
SELECTED
AT
RANDOM
ACCORDING
TO
THE
CURRENT
MARGINAL
DISTRIBUTION
OVER
THE
TRAINING
EXAM
PLES
OF
THESE
CANDIDATES
THE
ONE
THAT
CAUSES
THE
LARGEST
DECREASE
IN
THE
PSEUDO
LOSS
IS
ADDED
TO
THE
SET
P
AND
THE
PROCESS
IS
REPEATED
THE
BOOSTING
PROCESS
THUS
INFLUENCES
THE
WEAK
LEARNING
ALGORITHM
IN
TWO
WAYS
FIRST
BY
CHANGING
THE
WAY
THE
TEN
RANDOM
EXAMPLES
ARE
SELECTED
AND
SECOND
BY
CHANGING
THE
CALCULATION
OF
THE
PSEUDO
LOSS
IT
OFTEN
HAPPENS
THAT
ON
THE
FOLLOWING
ROUND
OF
BOOST
ING
THE
SAME
SET
PWILL
HAVE
PSEUDO
LOSS
SIGNIFICANTLY
LESS
THAN
WITH
RESPECT
TO
THE
NEW
MISLABEL
DISTRIBUTION
BUT
POSSIBLY
USING
A
DIFFERENT
MAPPING
IN
THIS
CASE
RATHER
THAN
CHOOSING
A
NEW
SET
OF
PROTOTYPES
WE
REUSE
THE
SAME
SET
PIN
ADDITIONAL
BOOSTING
STEPS
UNTIL
THE
ADVANTAGE
THAT
CAN
BE
GAINED
FROM
THE
GIVEN
PARTITION
IS
EXHAUSTED
DETAILS
FIGURE
A
SAMPLE
OF
THE
EXAMPLES
THAT
HAVE
THE
LARGEST
WEIGHT
AFTER
OF
THE
BOOSTING
ITERATIONS
THE
FIRST
LINE
IS
AFTER
ITERA
TION
THE
SECOND
AFTER
ITERATION
AND
THE
THIRD
AFTER
ITERATION
UNDERNEATH
EACH
IMAGE
WE
HAVE
A
LINE
OF
THE
FORM
D
WHERE
DIS
THE
LABEL
OF
THE
EXAMPLE
AND
ARE
THE
LABELS
THAT
GET
THE
HIGHEST
AND
SECOND
HIGHEST
VOTE
FROM
THE
COMBINED
HY
POTHESIS
AT
THAT
POINT
IN
THE
RUN
OF
THE
ALGORITHM
AND
ARE
THE
CORRESPONDING
NORMALIZED
VOTES
OMITTED
WE
RAN
ITERATIONS
OF
THE
BOOSTING
ALGORITHM
AND
THE
NUMBER
OF
PROTOTYPES
WE
USED
WERE
FOR
THE
FIRST
WEAK
HYPOTHESIS
FOR
THE
SECOND
FOR
THE
THIRD
FOR
THE
NEXT
FIVE
AND
FOR
THE
REMAINING
TWENTY
TWO
WEAK
HYPOTHESES
THESE
SIZES
WERE
CHOSEN
SO
THAT
THE
ERRORS
OF
ALL
OF
THE
WEAK
HYPOTHESES
ARE
APPROXIMATELY
EQUAL
WE
COMPARED
THE
PERFORMANCE
OF
OUR
ALGORITHM
TO
A
STRAWMAN
ALGORITHM
WHICH
USES
A
SINGLE
SET
OF
PROTOTYPES
SIMILAR
TO
OUR
ALGORITHM
THE
PROTOTYPE
SET
IS
GENERATED
IN
CREMENTALLY
COMPARING
TEN
PROTOTYPE
CANDIDATES
AT
EACH
STEP
AND
ALWAYS
CHOOSING
THE
ONE
THAT
MINIMIZES
THE
EM
PIRICAL
ERROR
WE
COMPARED
THE
PERFORMANCE
OF
THE
BOOSTING
ALGORITHM
TO
THAT
OF
THE
STRAWMAN
HYPOTHESIS
THAT
USES
THE
SAME
NUMBER
OF
PROTOTYPES
WE
ALSO
COMPARED
OUR
PER
FORMANCE
TO
THAT
OF
THE
CONDENSED
NEAREST
NEIGHBOR
RULE
CNN
A
GREEDY
METHOD
FOR
FINDING
A
SMALL
SET
OF
PROTOTYPES
WHICH
CORRECTLY
CLASSIFY
THE
ENTIRE
TRAINING
SET
RESULTS
AND
DISCUSSION
THE
RESULTS
OF
OUR
EXPERIMENTS
ARE
SUMMARIZED
IN
TA
BLE
AND
FIGURE
TABLE
DESCRIBES
THE
RESULTS
FROM
EX
PERIMENTS
WITH
ADABOOST
EACH
EXPERIMENT
WAS
REPEATED
TIMES
USING
DIFFERENT
RANDOM
SEEDS
THE
STRAWMAN
AL
GORITHM
EACH
REPEATED
TIMES
AND
CNN
TIMES
WE
COMPARE
THE
RESULTS
USING
A
RANDOM
PARTITION
OF
THE
DATA
INTO
TRAINING
AND
TESTING
AND
USING
THE
PARTITION
THAT
WAS
DEFINED
BY
USPS
WE
SEE
THAT
IN
BOTH
CASES
AFTER
MORE
THAN
EXAMPLES
THE
TRAINING
ERROR
OF
ADABOOST
IS
MUCH
BETTER
THAN
THAT
OF
THE
STRAWMAN
ALGORITHM
THE
PERFORMANCE
ON
THE
TEST
SET
IS
SIMILAR
WITH
A
SLIGHT
ADVANTAGE
TO
ADABOOST
WHEN
THE
HYPOTHESES
INCLUDE
MORE
THAN
EXAMPLES
BUT
A
SLIGHT
ADVANTAGE
TO
STRAWMAN
IF
FEWER
ROUNDS
OF
BOOSTINGARE
USED
AFTER
EXAMPLES
THE
ERROR
OF
ADABOOST
ON
THE
RANDOM
PARTITION
IS
ON
AVERAGE
WHILE
THE
ERROR
ACHIEVED
BY
USING
THE
WHOLE
TRAINING
SET
IS
ON
THE
RANDOM
PARTITION
THE
FINAL
ERROR
IS
WHILE
THE
ERROR
USING
THE
WHOLE
TRAINING
SET
IS
ERR
FIGURE
GRAPHS
OF
THE
PERFORMANCE
OF
THE
BOOSTING
ALGORITHM
ON
A
RANDOMLY
PARTITIONED
USPS
DATASET
THE
HORIZONTAL
AXIS
INDICATES
THE
TOTAL
NUMBER
OF
PROTOTYPES
THAT
WERE
ADDED
TO
THE
COMBINED
HYPOTHESIS
AND
THE
VERTICAL
AXIS
INDICATES
ERROR
THE
TOPMOST
JAGGED
LINE
INDICATES
THE
ERROR
OF
THE
WEAK
HYPOTHESIS
THAT
IS
TRAINED
AT
THIS
POINT
ON
THE
WEIGHTED
TRAINING
SET
THE
BOLD
CURVE
IS
THE
BOUND
ON
THE
TRAINING
ERROR
CALCULATED
USING
THEOREM
THE
LOWEST
THIN
CURVE
AND
THE
MEDIUM
BOLD
CURVE
SHOW
THE
PERFORMANCE
OF
THE
COMBINED
HYPOTHESIS
ON
THE
TRAINING
SET
AND
TEST
SET
RESPECTIVELY
COMPARING
TO
CNN
WE
SEE
THAT
BOTH
THE
STRAWMAN
ALGORITHM
AND
ADABOOST
PERFORM
BETTER
THAN
CNN
EVEN
WHEN
THEY
USE
ABOUT
EXAMPLES
IN
THEIR
HYPOTHESES
LARGER
HYPOTHESES
GENERATED
BY
ADABOOST
OR
STRAWMAN
ARE
MUCH
BETTER
THAN
THAT
GENERATED
BY
CNN
THE
MAIN
PROBLEM
WITH
CNN
SEEMS
TO
BE
ITS
TENDENCY
TO
OVERFIT
THE
TRAINING
DATA
ADABOOST
AND
THE
STRAWMAN
ALGORITHM
SEEM
TO
SUFFER
LESS
FROM
OVERFITTING
FIGURE
SHOWS
A
TYPICAL
RUN
OF
ADABOOST
THE
UPPER
MOST
JAGGED
LINE
IS
A
CONCATENATION
OF
THE
ERRORS
OF
THE
WEAK
HYPOTHESES
WITH
RESPECT
TO
THE
MISLABEL
DISTRIBUTION
EACH
PEAK
FOLLOWED
BY
A
VALLEY
CORRESPONDS
TO
THE
BEGINNING
AND
END
ERRORS
OF
A
WEAK
HYPOTHESIS
AS
IT
IS
BEING
CONSTRUCTED
ONE
PROTOTYPE
AT
A
TIME
THE
WEIGHTED
ERROR
ALWAYS
STARTS
AROUND
AT
THE
BEGINNING
OF
A
BOOSTING
ITERATION
AND
DROPS
TO
AROUND
THE
HEAVIEST
LINE
DESCRIBES
THE
UPPER
BOUND
ON
THE
TRAINING
ERROR
THAT
IS
GUARANTEED
BY
THE
OREM
AND
THE
TWO
BOTTOM
LINES
DESCRIBE
THE
TRAINING
AND
TEST
ERROR
OF
THE
FINAL
COMBINED
HYPOTHESIS
IT
IS
INTERESTING
THAT
THE
PERFORMANCE
OF
THE
BOOSTING
ALGORITHM
ON
THE
TEST
SET
IMPROVED
SIGNIFICANTLY
AFTER
THE
ERROR
ON
THE
TRAINING
SET
HAS
ALREADY
BECOME
ZERO
THIS
IS
SURPRISING
BECAUSE
AN
OCCAM
RAZOR
ARGUMENT
WOULD
PREDICT
THAT
INCREASING
THE
COMPLEXITY
OF
THE
HYPOTHESIS
AFTER
THE
ERROR
HAS
BEEN
REDUCED
TO
ZERO
IS
LIKELY
TO
DEGRADE
THE
PERFORMANCE
ON
THE
TEST
SET
FIGURE
SHOWS
A
SAMPLE
OF
THE
EXAMPLES
THAT
ARE
GIVEN
LARGE
WEIGHTS
BY
THE
BOOSTING
ALGORITHM
ON
A
TYPICAL
RUN
THERE
SEEM
TO
BE
TWO
TYPES
OF
HARD
EXAMPLES
FIRST
ARE
EXAMPLES
WHICH
ARE
VERY
ATYPICAL
OR
WRONGLY
LABELED
SUCH
AS
EXAMPLE
ON
THE
FIRST
LINE
AND
EXAMPLES
AND
ON
THE
SECOND
LINE
THE
SECOND
TYPE
WHICH
TENDS
TO
DOMINATE
ON
LATER
ITERATIONS
CONSISTS
OF
EXAMPLES
THAT
ARE
VERY
SIMILAR
TO
EACH
OTHER
BUT
HAVE
DIFFERENT
LABELS
SUCH
AS
EXAMPLES
VERSUS
ON
THE
THIRD
LINE
ALTHOUGH
THE
ALGORITHM
AT
THIS
POINT
WAS
CORRECT
ON
ALL
TRAINING
EXAMPLES
IT
IS
CLEAR
FROM
THE
VOTES
IT
ASSIGNED
TO
DIFFERENT
LABELS
FOR
THESE
EXAMPLE
PAIRS
THAT
IT
WAS
STILL
TRYING
TO
IMPROVE
THE
DISCRIMINATION
TABLE
AVERAGE
ERROR
RATES
ON
TRAINING
AND
TEST
SETS
IN
PERCENT
FOR
COLUMNS
LABELED
RANDOM
PARTITION
A
RANDOM
PARTITION
OF
THE
UNION
OF
THE
TRAINING
AND
TEST
SETS
WAS
USED
USPS
PARTITION
MEANS
THE
USPS
PROVIDED
PARTITION
INTO
TRAINING
AND
TEST
SETS
WAS
USED
COLUMNS
LABELED
THEORY
GIVE
THEORETICAL
UPPER
BOUNDS
ON
TRAINING
ERROR
CALCULATED
USING
THEOREM
SIZE
INDICATES
NUMBER
OF
PROTOTYPES
DEFINING
THE
FINAL
HYPOTHESIS
BETWEEN
SIMILAR
EXAMPLES
THIS
AGREES
WITH
OUR
INTUITION
THAT
THE
PSEUDO
LOSS
IS
A
MECHANISM
THAT
CAUSES
THE
BOOSTING
ALGORITHM
TO
CONCENTRATE
ON
THE
HARD
TO
DISCRIMINATE
LABELS
OF
HARD
EXAMPLES
CONCLUSIONS
WE
HAVE
DEMONSTRATED
THAT
ADABOOST
CAN
BE
USED
IN
MANY
SETTINGS
TO
IMPROVE
THE
PERFORMANCE
OF
A
LEARNING
ALGORITHM
WHEN
STARTING
WITH
RELATIVELY
SIMPLE
CLASSIFIERS
THE
IM
PROVEMENT
CAN
BE
ESPECIALLY
DRAMATIC
AND
CAN
OFTEN
LEAD
TO
A
COMPOSITE
CLASSIFIER
THAT
OUTPERFORMS
MORE
COMPLEX
ONE
SHOT
LEARNING
ALGORITHMS
LIKE
THIS
IMPROVEMENT
IS
FAR
GREATER
THAN
CAN
BE
ACHIEVED
WITH
BAGGING
NOTE
HOW
EVER
THAT
FOR
NON
BINARY
CLASSIFICATION
PROBLEMS
BOOSTING
SIMPLE
CLASSIFIERS
CAN
ONLY
BE
DONE
EFFECTIVELY
IF
THE
MORE
SOPHISTICATED
PSEUDO
LOSS
IS
USED
WHEN
STARTING
WITH
A
COMPLEX
ALGORITHM
LIKE
BOOSTING
CAN
ALSO
BE
USED
TO
IMPROVE
PERFORMANCE
BUT
DOES
NOT
HAVE
SUCH
A
COMPELLING
ADVANTAGE
OVER
BAGGING
BOOSTING
COMBINED
WITH
A
COMPLEX
ALGORITHM
MAY
GIVE
THE
GREATEST
IMPROVEMENT
IN
PERFORMANCE
WHEN
THERE
IS
A
REA
SONABLY
LARGE
AMOUNT
OF
DATA
AVAILABLE
NOTE
FOR
INSTANCE
BOOSTING
PERFORMANCE
ON
THE
LETTER
RECOGNITION
PROBLEM
WITH
TRAINING
EXAMPLES
NATURALLY
ONE
NEEDS
TO
CONSIDER
WHETHER
THE
IMPROVEMENT
IN
ERROR
IS
WORTH
THE
AD
DITIONAL
COMPUTATION
TIME
ALTHOUGH
WE
USED
ROUNDS
OF
BOOSTING
QUINLAN
GOT
GOOD
RESULTS
USING
ONLY
ROUNDS
BOOSTING
MAY
HAVE
OTHER
APPLICATIONS
BESIDES
REDUCING
THE
ERROR
OF
A
CLASSIFIER
FOR
INSTANCE
WE
SAW
IN
SECTION
THAT
BOOSTING
CAN
BE
USED
TO
FIND
A
SMALL
SET
OF
PROTOTYPES
FOR
A
NEAREST
NEIGHBOR
CLASSIFIER
AS
DESCRIBED
IN
THE
INTRODUCTION
BOOSTING
COMBINES
TWO
EFFECTS
IT
REDUCES
THE
BIAS
OF
THE
WEAK
LEARNER
BY
FORCING
THE
WEAK
LEARNER
TO
CONCENTRATE
ON
DIFFERENT
PARTS
OF
THE
INSTANCE
SPACE
AND
IT
ALSO
REDUCES
THE
VARIANCE
OF
THE
WEAK
LEARNER
BY
AVERAGING
SEVERAL
HYPOTHESES
THAT
WERE
GENERATED
FROM
DIFFERENT
SUBSAMPLES
OF
THE
TRAINING
SET
WHILE
THERE
IS
GOOD
THEORY
TO
EXPLAIN
THE
BIAS
REDUCING
EFFECTS
THERE
IS
NEED
FOR
A
BETTER
THEORY
OF
THE
VARIANCE
REDUCTION
WHEN
LOGISTIC
REGRESSION
FAILS
EXAMPLE
IN
WHICH
THE
LOGISTIC
REGRESSION
MODEL
FAILS
CS
MACHINE
LEARNING
MULTICLASS
CLASSIFICATION
APPROACH
VS
VS
VS
CS
MACHINE
LEARNING
DISCRIMINATIVE
APPROACH
METHODS
BASED
ON
BINARY
CLASSIFICATION
METHODS
ASSUME
WE
HAVE
CLASSES
LABELED
APPROACH
A
BINARY
LOGISTIC
REGRESSION
ON
ALL
PAIRS
VS
VS
VS
D
CLASS
DECISION
CLASS
LABEL
BASED
ON
WHO
GETS
THE
MAJORITY
DOES
NOT
WORK
ALL
THE
TIME
CS
MACHINE
LEARNING
MULTICLASS
CLASSIFICATION
APPROACH
VS
VS
VS
CS
MACHINE
LEARNING
LEARNING
OF
THE
SOFTMAX
MODEL
LEARNING
OF
PARAMETERS
W
STATISTICAL
VIEW
X
SOFTMAX
NETWORK
P
Y
X
K
P
Y
K
X
MULTI
WAY
Y
COIN
TOSS
ASSUME
OUTPUTS
Y
ARE
TRANSFORMED
AS
FOLLOWS
Y
K
Y
CS
MACHINE
LEARNING
ON
LINE
LEARNING
EXAMPLE
CS
MACHINE
LEARNING
PRACTICAL
CONCERNS
INPUT
NORMALIZATION
INPUT
NORMALIZATION
MAKES
THE
DATA
VARY
ROUGHLY
ON
THE
SAME
SCALE
CAN
MAKE
A
HUGE
DIFFERENCE
IN
ON
LINE
LEARNING
ASSUME
ON
LINE
UPDATE
DELTA
RULE
FOR
TWO
WEIGHTS
J
K
WJ
WJ
I
YI
F
XI
XI
J
W
W
I
Y
F
X
X
CHANGE
DEPENDS
ON
THE
MAGNITUDE
OF
THE
INPUT
K
K
I
I
I
K
FOR
INPUTS
WITH
A
LARGE
MAGNITUDE
THE
CHANGE
IN
THE
WEIGHT
IS
HUGE
CHANGES
TO
THE
INPUTS
WITH
HIGH
MAGNITUDE
DISPROPORTIONAL
AS
IF
THE
INPUT
WAS
MORE
IMPORTANT
CS
MACHINE
LEARNING
CS
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
MILOS
COURSES
MACHINE
LEARNING
ISSP
TIME
MONDAY
WEDNESDAY
LOCATION
SENNOTT
SQUARE
ROOM
ANNOUNCEMENTS
COURSE
SYLLABUS
MATLAB
TUTORIAL
THE
FINAL
EXAM
FOR
THE
COURSE
WILL
BE
ON
WEDNESDAY
APRIL
THE
EXAM
IS
A
CLOSED
BOOK
CUMULATIVE
EXAM
WITH
A
SLIGHT
BIAS
TOWARDS
THE
MATERIAL
COVERED
IN
THE
SECOND
HALF
OF
THE
COURSE
THE
TERM
PROJECT
PRESENTATIONS
WILL
BE
HELD
ON
MONDAY
APRIL
AND
WEDNESDAY
APRIL
THE
SLIDE
PRESENTATIONS
SHOULD
BE
MINUTES
LONG
AND
SHOULD
BRIEFLY
SUMMARIZE
THE
PROBLEM
THE
DATA
USED
THE
METHODOLOGY
LEARNING
METHODS
THE
RESULTS
OBTAINED
DURING
THE
INVESTIGATIONS
AND
MAIN
CONCLUSIONS
THE
PRESENTATION
ORDER
THAT
WAS
GENERATED
WITH
THE
HELP
OF
RANDPERM
FUNCTION
CAN
BE
FOUND
HERE
PLEASE
SUBMIT
THE
PRESENTATIONS
IN
PPT
PPTX
OR
PDF
FORMAT
BY
OF
THE
DAY
OF
YOUR
PRESENTATIONVIA
COURSEWEB
THE
TERM
PROJECTS
ARE
DUE
ON
WEDNESDAY
APRIL
AT
THE
PROJECT
REPORT
SHOULD
BE
ORGANIZED
LIKE
A
CONFERENCE
PAPERS
WITH
ABSTRACT
INTRODUCTION
RELATED
WORK
METHODOLOGY
EXPERIMENTAL
RESULTS
DISCUSSION
OF
RESULTS
AND
CONCLUSION
SECTIONS
THE
REPORTS
SHOULD
BE
SELFEXPLANATORY
THE
METHODOLOGY
SECTION
SHOULD
EXPLAIN
BRIEFLY
THE
METHODS
USED
AND
PROVIDE
RELEVANT
REFERENCES
HOMEWORK
ASSIGNMENTS
THE
PRINTED
REPORTS
SHOULD
BE
SUBMITTED
IN
THE
CLASS
BEFORE
THE
LECTURE
PROGRAMS
SHOULD
BE
SUBMITTED
ELECTRONICALLY
VIA
COURSE
WEB
BEFORE
PLEASE
FOLLOW
THE
RULES
FOR
THE
SUBMISSION
OF
PROGRAMS
HOMEWORK
ASSIGNMENT
SOLUTIONS
HOMEWORK
ASSIGNMENT
HOMEWORK
ASSIGNMENT
HOMEWORK
ASSIGNMENT
CS
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
MILOS
COURSES
HOMEWORK
ASSIGNMENT
HOMEWORK
ASSIGNMENT
HOMEWORK
ASSIGNMENT
HOMEWORK
ASSIGNMENT
HOMEWORK
ASSIGNMENT
HOMEWORK
ASSIGNMENT
HOMEWORK
ASSIGNMENT
LINKS
COURSE
DESCRIPTION
LECTURES
HOMEWORKS
TERM
PROJECTS
MATLAB
ABSTRACT
THE
GOAL
OF
THE
FIELD
OF
MACHINE
LEARNING
IS
TO
BUILD
COMPUTER
SYSTEMS
THAT
LEARN
FROM
EXPERIENCE
AND
THAT
ARE
CAPABLE
TO
ADAPT
TO
THEIR
ENVIRONMENTS
LEARNING
TECHNIQUES
AND
METHODS
DEVELOPED
BY
RESEARCHERS
IN
THIS
FIELD
HAVE
BEEN
SUCCESSFULLY
APPLIED
TO
A
VARIETY
OF
LEARNING
TASKS
IN
A
BROAD
RANGE
OF
AREAS
INCLUDING
FOR
EXAMPLE
TEXT
CLASSIFICATION
GENE
DISCOVERY
FINANCIAL
FORECASTING
CREDIT
CARD
FRAUD
DETECTION
COLLABORATIVE
FILTERING
DESIGN
OF
ADAPTIVE
WEB
AGENTS
AND
OTHERS
THIS
INTRODUCTORY
MACHINE
LEARNING
COURSE
WILL
GIVE
AN
OVERVIEW
OF
MANY
MODELS
AND
ALGORITHMS
USED
IN
MODERN
MACHINE
LEARNING
INCLUDING
LINEAR
MODELS
MULTILAYER
NEURAL
NETWORKS
SUPPORT
VECTOR
MACHINES
DENSITY
ESTIMATION
METHODS
BAYESIAN
BELIEF
NETWORKS
MIXTURE
MODELS
CLUSTERING
ENSAMBLE
METHODS
AND
REINFORCEMENT
LEARNING
THE
COURSE
WILL
GIVE
THE
STUDENT
THE
BASIC
IDEAS
AND
INTUITION
BEHIND
THESE
METHODS
AS
WELL
AS
A
MORE
FORMAL
UNDERSTANDING
OF
HOW
AND
WHY
THEY
WORK
STUDENTS
WILL
HAVE
AN
OPPORTUNITY
TO
EXPERIMENT
WITH
MACHINE
LEARNING
TECHNIQUES
AND
APPLY
THEM
A
SELECTED
PROBLEM
IN
THE
CONTEXT
OF
A
TERM
PROJECT
COURSE
SYLLABUS
PREREQUISITES
KNOWLEDGE
OF
MATRICES
AND
LINEAR
ALGEBRA
PROBABILITY
CS
AND
STATISTICS
CS
PROGRAMMING
CS
OR
EQUIVALENT
OR
THE
PERMISSION
OF
THE
INSTRUCTOR
TEXTBOOK
CS
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
MILOS
COURSES
CHRIS
BISHOP
PATTERN
RECOGNITION
AND
MACHINE
LEARNING
SPRINGER
OTHER
ML
BOOKS
K
MURPHY
MACHINE
LEARNING
A
PROBABILISTIC
PERSPECTIVE
MIT
PRESS
R
O
DUDA
P
E
HART
D
G
STORK
PATTERN
CLASSIFICATION
SECOND
EDITION
JOHN
WILEY
AND
SONS
J
HAN
M
KAMBER
DATA
MINING
CONCEPTS
AND
TECHNIQUES
MORGAN
KAUFFMAN
T
MITCHELL
MACHINE
LEARNING
MC
GRAW
HILL
LECTURES
LECTURES
TOPIC
ASSIGNMENTS
JANUARY
INTRODUCTION
TO
MACHINE
LEARNING
READINGS
BISHOP
CHAPTER
JANUARY
INTRODUCTION
TO
MACHINE
LEARNING
READINGS
BISHOP
CHAPTER
JANUARY
DENSITY
ESTIMATION
I
READINGS
BISHOP
CHAPTER
JANUARY
MATLAB
TUTORIAL
READINGS
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
JANUARY
DENSITY
ESTIMATION
II
READINGS
BISHOP
CHAPTER
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
JANUARY
DENSITY
ESTIMATION
III
READINGS
BISHOP
CHAPTER
JANUARY
LINEAR
REGRESSION
READINGS
BISHOP
CHAPTER
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
FEBRUARY
CLASSIFICATION
LEARNING
LOGISTIC
REGRESSION
GENERATIVE
CLASSIFICATION
MODELS
READINGS
BISHOP
CHAPTER
FEBRUARY
CLASSIFICATION
LEARNING
II
EVALUATION
OF
CLASSIFIERS
READINGS
BISHOP
CHAPTER
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
FEBRUARY
FISHER
LINEAR
DISCRIMINANT
SUPPORT
VECTOR
MACHINES
READINGS
BISHOP
CHAPTER
CHAPTER
CS
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
MILOS
COURSES
FEBRUARY
SUPPORT
VECTOR
MACHINES
FOR
REGRESSION
NONPARAMETRIC
INSTANCE
BASED
CLASSIFICATION
METHODS
READINGS
BISHOP
CHAPTER
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
FEBRUARY
MULTILAYER
NEURAL
NETWORKS
READINGS
BISHOP
CHAPTER
FEBRUARY
MULTICLASS
CLASSIFICATION
DECISION
TREES
READINGS
BISHOP
CHAPTER
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
FEBRUARY
BAYESIAN
BELIEF
NETWORKS
READINGS
BISHOP
CHAPTER
FEBRUARY
BAYESIAN
BELIEF
NETWORKS
INFERENCE
AND
LEARNING
READINGS
BISHOP
CHAPTER
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
MARCH
MIDTERM
EXAM
READINGS
EVERYTHING
COVERED
BEFORE
OR
ON
FEBRUARY
MARCH
EXPECTATION
MAXIMIZATION
ALGORITHM
READINGS
BISHOP
CHAPTER
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
MARCH
EXPECTATION
MAXIMIZATION
ALGORITHM
MIXTURE
OF
GAUSSIANS
READINGS
BISHOP
CHAPTER
MARCH
CLUSTERING
READINGS
BISHOP
CHAPTER
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
MARCH
ENSEMBLE
METHODS
MIXTURE
OF
EXPERTS
BAGGING
READINGS
BISHOP
CHAPTER
L
BREIMAN
ARCING
CLASSIFIERS
Y
FREUND
R
SCHAPIRE
EXPERIMENTS
WITH
A
NEW
BOOSTING
ALGORITHM
MARCH
ENSEMBLE
METHODS
BOOSTING
READINGS
BISHOP
CHAPTER
L
BREIMAN
ARCING
CLASSIFIERS
Y
FREUND
R
SCHAPIRE
EXPERIMENTS
WITH
A
NEW
BOOSTING
ALGORITHM
HOMEWORK
ASSIGNMENT
DATA
FOR
HOMEWORK
CS
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
MILOS
COURSES
MARCH
DIMENSIONALITY
REDUCTION
FEATURE
SELECTION
READINGS
BISHOP
CHAPTER
APRIL
DIMENSIONALITY
REDUCTION
II
REINFORCEMENT
LEARNING
READINGS
BISHOP
CHAPTER
KAEBLING
LITTMAN
MOORE
REINFORCEMENT
LEARNING
A
SURVEY
APRIL
REINFORCEMENT
LEARNING
READINGS
KAEBLING
LITTMAN
MOORE
REINFORCEMENT
LEARNING
A
SURVEY
APRIL
CONCEPT
LEARNING
READINGS
APRIL
FINAL
EXAM
READINGS
ALL
SEMESTER
APRIL
AND
TERM
PROJECT
PRESENTATIONS
READINGS
ALL
SEMESTER
HOMEWORKS
THE
HOMEWORK
ASSIGNMENTS
WILL
HAVE
MOSTLY
A
CHARACTER
OF
PROJECTS
AND
WILL
REQUIRE
YOU
TO
IMPLEMENT
SOME
OF
THE
LEARNING
ALGORITHMS
COVERED
DURING
LECTURES
PROGRAMMING
ASSIGNMENTS
WILL
BE
IMPLEMENTED
IN
MATLAB
SEE
RULES
FOR
THE
SUBMISSION
OF
PROGRAMS
THE
ASSIGNMENTS
BOTH
WRITTEN
AND
PROGRAMMING
PARTS
ARE
DUE
AT
THE
BEGINNING
OF
THE
CLASS
ON
THE
DAY
SPECIFIED
ON
THE
ASSIGNMENT
IN
GENERAL
NO
EXTENSIONS
WILL
BE
GRANTED
COLLABORATIONS
NO
COLLABORATION
ON
HOMEWORK
ASSIGNMENTS
PROGRAMS
AND
EXAMS
IS
PERMITTED
UNLESS
YOU
ARE
SPECIFICALLY
INSTRUCTED
TO
WORK
IN
GROUPS
TERM
PROJECTS
THE
TERM
PROJECT
IS
DUE
AT
THE
END
OF
THE
SEMESTER
AND
ACCOUNTS
FOR
A
SIGNIFICANT
PORTION
OF
YOUR
GRADE
MATLAB
CS
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
MILOS
COURSES
MATLAB
IS
A
MATHEMATICAL
TOOL
FOR
NUMERICAL
COMPUTATION
AND
MANIPULATION
WITH
EXCELLENT
GRAPHING
CAPABILITIES
IT
PROVIDES
A
GREAT
DEAL
OF
SUPPORT
AND
CAPABILITIES
FOR
THINGS
YOU
WILL
NEED
TO
RUN
MACHINE
LEARNING
EXPERIMENTS
THE
CSSD
AT
UPITT
OFFERS
STUDENT
LICENSES
FOR
MATLAB
TO
OBTAIN
THE
LICENCE
PLEASE
CHECK
THE
FOLLOWING
LINK
TO
THE
MATLAB
CSSD
PAGE
IN
ADDITION
UPITT
HAS
A
NUMBER
OF
MATLAB
LICENCES
RUNNING
ON
BOTH
UNIX
AND
WINDOWS
PLATFORMS
SEE
THE
FOLLOWING
WEB
PAGE
FOR
THE
DETAILS
MATLAB
TUTORIAL
FILE
OTHER
MATLAB
RESOURCES
ON
THE
WEB
ONLINE
MATLAB
DOCUMENTATION
ONLINE
MATHWORKS
DOCUMENTATION
INCLUDING
MATLAB
TOOLBOXES
CHEATING
POLICY
CHEATING
AND
ANY
OTHER
ANTIINTELLECTUAL
BEHAVIOR
INCLUDING
GIVING
YOUR
WORK
TO
SOMEONE
ELSE
WILL
BE
DEALT
WITH
SEVERELY
AND
WILL
RESULT
IN
THE
FAIL
F
GRADE
IF
YOU
FEEL
YOU
MAY
HAVE
VIOLATED
THE
RULES
SPEAK
TO
US
AS
SOON
AS
POSSIBLE
PLEASE
MAKE
SURE
YOU
READ
UNDERSTAND
AND
ABIDE
BY
THE
ACADEMIC
INTEGRITY
CODE
FOR
THE
FACULTY
AND
COLLEGE
OF
ARTS
AND
SCIENCES
STUDENTS
WITH
DISABILITIES
IF
YOU
HAVE
A
DISABILITY
FOR
WHICH
YOU
ARE
OR
MAY
BE
REQUESTING
AN
ACCOMMODATION
YOU
ARE
ENCOURAGED
TO
CONTACT
BOTH
YOUR
INSTRUCTOR
AND
DISABILITY
RESOURCES
AND
SERVICES
WILLIAM
PITT
UNION
AS
EARLY
AS
POSSIBLE
IN
THE
TERM
DRS
WILL
VERIFY
YOUR
DISABILITY
AND
DETERMINE
REASONABLE
ACCOMODATIONS
FOR
THIS
COURSE
COURSE
WEBPAGES
FROM
SPRING
SPRING
SPRING
SPRING
SPRING
AND
SPRING
LAST
UPDATED
BY
MILOS
ON
MACHINE
LEARNING
CS
ISSP
SPRING
LECTURE
MEETING
TIME
MONDAYS
WEDNESDAYS
AM
PM
CLASSROOM
SENNOTT
SQUARE
SENSQ
COURSE
DESCRIPTION
THE
GOAL
OF
THE
FIELD
OF
MACHINE
LEARNING
IS
TO
BUILD
COMPUTER
SYSTEMS
THAT
LEARN
FROM
EXPERIENCE
AND
THAT
ARE
CAPABLE
TO
ADAPT
TO
THEIR
ENVIRONMENTS
LEARNING
TECHNIQUES
AND
METHODS
DEVELOPED
BY
RESEARCHERS
IN
THIS
FIELD
HAVE
BEEN
SUCCESSFULLY
APPLIED
TO
A
VARIETY
OF
LEARNING
TASKS
IN
A
BROAD
RANGE
OF
AREAS
INCLUDING
FOR
EXAMPLE
TEXT
CLASSIFICATION
GENE
DISCOVERY
FINANCIAL
FORECASTING
CREDIT
CARD
FRAUD
DETECTION
COLLABORATIVE
FILTERING
DESIGN
OF
ADAPTIVE
WEB
AGENTS
AND
OTHERS
THIS
INTRODUCTORY
MACHINE
LEARNING
COURSE
WILL
GIVE
AN
OVERVIEW
OF
MANY
MODELS
AND
ALGORITHMS
USED
IN
MODERN
MACHINE
LEARNING
INCLUDING
LINEAR
MODELS
MULTI
LAYER
NEURAL
NETWORKS
SUPPORT
VECTOR
MACHINES
DENSITY
ESTIMATION
METHODS
BAYESIAN
BELIEF
NETWORKS
MIXTURE
MODELS
CLUSTERING
ENSEMBLE
METHODS
AND
REINFORCEMENT
LEARNING
THE
COURSE
WILL
GIVE
THE
STUDENT
THE
BASIC
IDEAS
AND
INTUITION
BEHIND
THESE
METHODS
AS
WELL
AS
A
MORE
FORMAL
UNDERSTANDING
OF
HOW
AND
WHY
THEY
WORK
STUDENTS
WILL
HAVE
AN
OPPORTUNITY
TO
EXPERIMENT
WITH
MACHINE
LEARNING
TECHNIQUES
AND
APPLY
THEM
A
SELECTED
PROBLEM
IN
THE
CONTEXT
OF
A
TERM
PROJECT
PREREQUISITES
KNOWLEDGE
OF
MATRICES
AND
LINEAR
ALGEBRA
CS
PROBABILITY
CS
STATISTICS
CS
PROGRAMMING
CS
OR
EQUIVALENT
OR
THE
PERMISSION
OF
THE
INSTRUCTOR
TEXTBOOK
CHRIS
BISHOP
PATTERN
RECOGNITION
AND
MACHINE
LEARNING
SPRINGER
HOMEWORK
ASSIGNMENTS
HOMEWORK
ASSIGNMENTS
WILL
HAVE
MOSTLY
A
CHARACTER
OF
PROJECTS
AND
WILL
REQUIRE
YOU
TO
IMPLEMENT
SOME
OF
THE
LEARNING
ALGORITHMS
COVERED
DURING
LECTURES
PROGRAMMING
ASSIGNMENTS
WILL
BE
IMPLEMENTED
IN
MATLAB
PLEASE
VISIT
TO
OBTAIN
A
MATLAB
LICENSE
FOR
STUDENTS
THE
ASSIGNMENTS
BOTH
WRITTEN
AND
PROGRAMMING
PARTS
ARE
DUE
AT
THE
BEGINNING
OF
THE
CLASS
ON
THE
DAY
SPECIFIED
ON
THE
ASSIGNMENT
IN
GENERAL
NO
EXTENSIONS
WILL
BE
GRANTED
COLLABORATIONS
NO
COLLABORATION
ON
HOMEWORK
ASSIGNMENTS
PROGRAMS
TERM
PROJECTS
AND
EXAMS
UNLESS
YOU
ARE
SPECIFICALLY
INSTRUCTED
TO
WORK
IN
GROUPS
IS
PERMITTED
GRADING
THE
FINAL
GRADE
FOR
THE
COURSE
WILL
BE
DETERMINED
BASED
ON
HOMEWORK
ASSIGNMENTS
EXAMS
THE
TERM
PROJECT
AND
YOUR
LECTURE
ATTENDANCE
AND
ACTIVITY
THE
MIDTERM
EXAM
WILL
BE
IN
EARLY
MARCH
AND
THE
FINAL
EXAM
WILL
BE
THE
WEEK
OF
APRIL
THE
TERM
PROJECT
PRESENTATIONS
WILL
BE
HELD
THE
WEEK
OF
APRIL
POLICY
ON
CHEATING
CHEATING
AND
ANY
OTHER
ANTI
INTELLECTUAL
BEHAVIOR
INCLUDING
GIVING
YOUR
WORK
TO
SOMEONE
ELSE
WILL
BE
DEALT
WITH
SEVERELY
AND
WILL
RESULT
IN
THE
FAIL
F
GRADE
IF
YOU
FEEL
YOU
MAY
HAVE
VIOLATED
THE
RULES
SPEAK
TO
US
AS
SOON
AS
POSSIBLE
PLEASE
MAKE
SURE
YOU
READ
UNDERSTAND
AND
ABIDE
BY
THE
ACADEMIC
INTEGRITY
CODE
FOR
THE
FACULTY
AND
COLLEGE
OF
ARTS
AND
SCIENCES
STUDENTS
WITH
DISABILITIES
IF
YOU
HAVE
A
DISABILITY
FOR
WHICH
YOU
ARE
OR
MAY
BE
REQUESTING
AN
ACCOMMODATION
YOU
ARE
ENCOURAGED
TO
CONTACT
BOTH
YOUR
INSTRUCTOR
AND
DISABILITY
RESOURCES
AND
SERVICES
WILLIAM
PITT
UNION
TTY
AS
EARLY
AS
POSSIBLE
IN
THE
TERM
DRS
WILL
VERIFY
YOUR
DISABILITY
AND
DETERMINE
REASONABLE
ACCOMMODATIONS
FOR
THIS
COURSE
TENTATIVE
SYLLABUS
MACHINE
LEARNING
INTRODUCTION
DENSITY
ESTIMATION
SUPERVISED
LEARNING
LINEAR
AND
LOGISTIC
REGRESSION
GENERATIVE
CLASSIFICATION
MODELS
MULTI
LAYER
NEURAL
NETWORKS
SUPPORT
VECTOR
MACHINES
UNSUPERVISED
LEARNING
BAYESIAN
BELIEF
NETWORKS
BBNS
LEARNING
PARAMETERS
AND
STRUCTURE
OF
BBNS
EXPECTATION
MAXIMIZATION
CLUSTERING
DIMENSIONALITY
REDUCTION
FEATURE
SELECTION
FEATURE
FILTERING
WRAPPER
METHODS
PCA
ENSEMBLE
METHODS
MIXTURES
OF
EXPERTS
BAGGING
AND
BOOSTING
REINFORCEMENT
LEARNING
CHANGSHENG
LIU
UNIVERSITY
OF
APRIL
PLAN
FOR
TODAY
REVIEW
SOME
QUESDONS
FROM
HW
DENSITY
ESDMADON
MIXTURE
OF
GAUSSIAN
NAÏVE
BAYESIAN
HW
PLEASE
SEE
WHITEBOARD
MAXIMUM
LIKELIHOOD
MAXIMUM
A
POSTERIORI
ESDMADON
A
SET
OF
RANDOM
VARIABLES
X
XD
A
MODEL
OF
DISTRIBUDON
OVER
VARIABLES
IN
X
WITH
PARAMETERS
Θ
P
X
Θ
DATA
D
DN
OBJECDVE
FIND
PARAMETER
Θ
THAT
P
X
Θ
FITS
DATA
D
THE
BEST
MAXIMUM
LIKELIHOOD
MAXIMIZE
P
D
Θ
Ξ
MAXIMUM
A
POSTERIORI
PROBABILITY
MAP
A
MODEL
OF
DISTRIBUDON
OVER
VARIABLES
IN
X
WITH
PARAMETERS
Θ
P
Θ
D
Ξ
A
BIASED
COIN
WITH
THE
PROBABILITY
OF
A
HEAD
Θ
DATA
HHTTHHTHTHTTTHTHHHHTHHHHT
HEADS
TAILS
WHAT
IS
A
GOOD
ESDMATE
OF
Θ
USE
THE
FREQUENCY
OF
OCCURRENCES
THIS
IS
THE
MAXIMUM
LIKELIHOOD
ESDMATE
THE
LIKELIHOOD
OF
THE
DATA
MAXIMUM
LIKELIHOOD
CHOOSE
FROM
THE
SAME
FAMILY
FOR
CONVIENENCE
DIAGONAL
COVARIANCE
MATRIX
COVARIANCE
MATRIX
PROPORDONAL
TO
THE
IDENDTY
MATRIX
OLD
FAITHFUL
DATA
SET
SINGLE
GAUSSIAN
MIXTURE
OF
TWO
GAUSSIANS
COMBINE
SIMPLE
MODELS
INTO
A
COMPLEX
MODEL
MIXING
COEFFICIENT
K
DIRECTED
ACYCLIC
GRAPH
DAG
NODES
ARE
RANDOM
VARIABLES
EDGES
INDICATE
CAUSAL
INFLUENCES
EACH
NODE
HAS
A
CONDI
ONAL
PROBABILITY
TABLE
CPT
THAT
GIVES
THE
PROBABILITY
OF
EACH
OF
ITS
VALUES
GIVEN
EVERY
POSSIBLE
COMBINADON
OF
VALUES
FOR
ITS
PARENTS
CONDIDONING
CASE
ROOTS
SOURCES
OF
THE
DAG
THAT
HAVE
NO
PARENTS
ARE
GIVEN
PRIOR
PROBABILIDES
A
IS
INDEPENDENT
OF
B
GIVEN
C
EQUIVALENTLY
NOTADON
CONDI
ONALLY
INDEPENDENT
VIA
D
SEPARA
ON
D
SEPARA
ON
IN
THE
GRAPH
LET
X
Y
AND
Z
BE
THREE
SETS
OF
NODES
IF
X
AND
Y
ARE
D
SEPARATED
BY
Z
THEN
X
AND
Y
ARE
CONDIDONALLY
INDEPENDENT
GIVE
Z
D
SEPARA
ON
A
IS
D
SEPARATED
FROM
B
GIVE
C
IF
EVERY
UNDIRECTED
PATH
BETWEEN
THEM
IS
BLOCKED
WITH
C
NAÏVE
BAYES
AS
A
BAYES
NET
NAÏVE
BAYES
IS
A
SIMPLE
BAYES
NET
PRIORS
P
Y
AND
CONDIDONALS
P
XI
Y
FOR
NAÏVE
BAYES
PROVIDE
CPTS
FOR
THE
NETWORK
SLIDE
CREDIT
RAY
MOONEY
A
COURSE
IN
MACHINE
LEARNING
HAL
DAUMÉ
III
UNSUPERVISED
LEARNING
IF
YOU
HAVE
ACCESS
TO
LABELED
TRAINING
DATA
YOU
KNOW
WHAT
TO
DO
THIS
IS
THE
SUPERVISED
SETTING
IN
WHICH
YOU
HAVE
A
TEACHER
TELLING
YOU
THE
RIGHT
ANSWERS
UNFORTUNATELY
FINDING
SUCH
A
TEACHER
IS
OFTEN
DIFFICULT
EXPENSIVE
OR
DOWN
RIGHT
IMPOSSIBLE
IN
THOSE
CASES
YOU
MIGHT
STILL
WANT
TO
BE
ABLE
TO
ANALYZE
YOUR
DATA
EVEN
THOUGH
YOU
DO
NOT
HAVE
LABELS
UNSUPERVISED
LEARNING
IS
LEARNING
WITHOUT
A
TEACHER
ONE
BASIC
THING
THAT
YOU
MIGHT
WANT
TO
DO
WITH
DATA
IS
TO
VISUALIZE
IT
SADLY
IT
IS
DIFFICULT
TO
VISUALIZE
THINGS
IN
MORE
THAN
TWO
OR
THREE
DIMENSIONS
AND
MOST
DATA
IS
IN
HUNDREDS
OF
DIMENSIONS
OR
MORE
DIMENSION
ALITY
REDUCTION
IS
THE
PROBLEM
OF
TAKING
HIGH
DIMENSIONAL
DATA
AND
EMBEDDING
IT
IN
A
LOWER
DIMENSION
SPACE
ANOTHER
THING
YOU
MIGHT
WANT
TO
DO
IS
AUTOMATICALLY
DERIVE
A
PARTITIONING
OF
THE
DATA
INTO
CLUSTERS
YOU
VE
ALREADY
LEARNED
A
BASIC
APPROACH
FOR
DOING
THIS
THE
K
MEANS
ALGORITHM
CHAPTER
HERE
YOU
WILL
ANALYZE
THIS
ALGORITHM
TO
SEE
WHY
IT
WORKS
YOU
WILL
ALSO
LEARN
MORE
ADVANCED
CLUSTERING
APPROACHES
K
MEANS
CLUSTERING
REVISITED
THE
K
MEANS
CLUSTERING
ALGORITHM
IS
RE
PRESENTED
IN
ALGORITHM
THERE
ARE
TWO
VERY
BASIC
QUESTIONS
ABOUT
THIS
ALGORITHM
DOES
IT
CONVERGE
AND
IF
SO
HOW
QUICKLY
HOW
SENSITIVE
IT
IS
TO
INITIALIZA
TION
THE
ANSWERS
TO
THESE
QUESTIONS
DETAILED
BELOW
ARE
YES
IT
CONVERGES
AND
IT
CONVERGES
VERY
QUICKLY
IN
PRACTICE
THOUGH
SLOWLY
IN
THEORY
YES
IT
IS
SENSITIVE
TO
INITIALIZATION
BUT
THERE
ARE
GOOD
WAYS
TO
INITIALIZE
IT
CONSIDER
THE
QUESTION
OF
CONVERGENCE
THE
FOLLOWING
THEOREM
STATES
THAT
THE
K
MEANS
ALGORITHM
CONVERGES
THOUGH
IT
DOES
NOT
SAY
HOW
QUICKLY
IT
HAPPENS
THE
METHOD
OF
PROVING
THE
CONVERGENCE
IS
TO
SPECIFY
A
CLUSTERING
QUALITY
OBJECTIVE
FUNCTION
AND
THEN
TO
SHOW
THAT
THE
K
MEANS
ALGORITHM
CONVERGES
TO
A
LOCAL
OPTIMUM
OF
THAT
OBJECTIVE
FUNCTION
THE
PARTICULAR
OBJECTIVE
FUNCTION
THAT
K
MEANS
DEPENDENCIES
UNSUPERVISED
LEARNING
ALGORITHM
K
MEANS
D
K
FOR
K
TO
K
DO
ΜK
SOME
RANDOM
LOCATION
RANDOMLY
INITIALIZE
MEAN
FOR
KTH
CLUSTER
END
FOR
REPEAT
FOR
N
TO
N
DO
ZN
ARGMINK
ΜK
XN
ASSIGN
EXAMPLE
N
TO
CLOSEST
CENTER
END
FOR
FOR
K
TO
K
DO
ΜK
MEAN
XN
ZN
K
RE
ESTIMATE
MEAN
OF
CLUSTER
K
END
FOR
UNTIL
CONVERGED
RETURN
Z
RETURN
CLUSTER
ASSIGNMENTS
IS
OPTIMIZING
IS
THE
SUM
OF
SQUARED
DISTANCES
FROM
ANY
DATA
POINT
TO
ITS
ASSIGNED
CENTER
THIS
IS
A
NATURAL
GENERALIZATION
OF
THE
DEFINITION
OF
A
MEAN
THE
MEAN
OF
A
SET
OF
POINTS
IS
THE
SINGLE
POINT
THAT
MINIMIZES
THE
SUM
OF
SQUARED
DISTANCES
FROM
THE
MEAN
TO
EVERY
POINT
IN
THE
DATA
FORMALLY
THE
K
MEANS
OBJECTIVE
IS
L
Z
Μ
D
ΜZN
K
N
ZN
K
XN
ΜK
THEOREM
K
MEANS
CONVERGENCE
THEOREM
FOR
ANY
DATASET
D
AND
ANY
NUMBER
OF
CLUSTERS
K
THE
K
MEANS
ALGORITHM
CONVERGES
IN
A
FINITE
NUM
BER
OF
ITERATIONS
WHERE
CONVERGENCE
IS
MEASURED
BY
L
CEASING
THE
CHANGE
PROOF
OF
THEOREM
THE
PROOF
WORKS
AS
FOLLOWS
THERE
ARE
ONLY
TWO
POINTS
IN
WHICH
THE
K
MEANS
ALGORITHM
CHANGES
THE
VALUES
OF
Μ
OR
Z
LINES
AND
WE
WILL
SHOW
THAT
BOTH
OF
THESE
OPERATIONS
CAN
NEVER
INCREASE
THE
VALUE
OF
L
ASSUMING
THIS
IS
TRUE
THE
REST
OF
THE
ARGU
MENT
IS
AS
FOLLOWS
AFTER
THE
FIRST
PASS
THROUGH
THE
DATA
THERE
ARE
ARE
ONLY
FINITELY
MANY
POSSIBLE
ASSIGNMENTS
TO
Z
AND
Μ
BECAUSE
Z
IS
DISCRETE
AND
BECAUSE
Μ
CAN
ONLY
TAKE
ON
A
FINITE
NUMBER
OF
VALUES
MEANS
OF
SOME
SUBSET
OF
THE
DATA
FURTHERMORE
L
IS
LOWER
BOUNDED
BY
ZERO
TOGETHER
THIS
MEANS
THAT
L
CANNOT
DECREASE
MORE
THAN
A
FINITE
NUMBER
OF
TIMES
THUS
IT
MUST
STOP
DECREASING
AT
SOME
POINT
AND
AT
THAT
POINT
THE
ALGORITHM
HAS
CONVERGED
IT
REMAINS
TO
SHOW
THAT
LINES
AND
DECREASE
L
FOR
LINE
WHEN
LOOKING
AT
EXAMPLE
N
SUPPOSE
THAT
THE
PREVIOUS
VALUE
OF
ZN
IS
A
AND
THE
NEW
VALUE
IS
B
IT
MUST
BE
THE
CASE
THAT
XN
ΜB
XN
ΜB
THUS
CHANGING
FROM
A
TO
B
CAN
ONLY
DECREASE
L
FOR
LINE
CONSIDER
THE
SECOND
FORM
OF
L
LINE
COMPUTES
ΜK
AS
THE
MEAN
OF
THE
DATA
POINTS
FOR
WHICH
ZN
K
WHICH
IS
PRECISELY
THE
POINT
THAT
MINIMIZES
SQUARED
SITANCES
THUS
THIS
UPDATE
TO
ΜK
CAN
ONLY
DECREASE
L
THERE
ARE
SEVERAL
ASPECTS
OF
K
MEANS
THAT
ARE
UNFORTUNATE
FIRST
THE
CONVERGENCE
IS
ONLY
TO
A
LOCAL
OPTIMUM
OF
L
IN
PRACTICE
THIS
A
COURSE
IN
MACHINE
LEARNING
MEANS
THAT
YOU
SHOULD
USUALLY
RUN
IT
TIMES
WITH
DIFFERENT
INITIAL
IZATIONS
AND
PICK
THE
ONE
WITH
MINIMAL
RESULTING
L
SECOND
ONE
CAN
SHOW
THAT
THERE
ARE
INPUT
DATASETS
AND
INITIALIZATIONS
ON
WHICH
IT
MIGHT
TAKE
AN
EXPONENTIAL
AMOUNT
OF
TIME
TO
CONVERGE
FORTU
NATELY
THESE
CASES
ALMOST
NEVER
HAPPEN
IN
PRACTICE
AND
IN
FACT
IT
HAS
RECENTLY
BEEN
SHOWN
THAT
ROUGHLY
IF
YOU
LIMIT
THE
FLOATING
POINT
PRE
CISION
OF
YOUR
MACHINE
K
MEANS
WILL
CONVERGE
IN
POLYNOMIAL
TIME
THOUGH
STILL
ONLY
TO
A
LOCAL
OPTIMUM
USING
TECHNIQUES
OF
SMOOTHED
ANALYSIS
THE
BIGGEST
PRACTICAL
ISSUE
IN
K
MEANS
IS
INITIALIZATION
IF
THE
CLUS
TER
MEANS
ARE
INITIALIZED
POORLY
YOU
OFTEN
GET
CONVERGENCE
TO
UNINTER
ESTING
SOLUTIONS
A
USEFUL
HEURISTIC
IS
THE
FURTHEST
FIRST
HEURISTIC
THIS
GIVES
A
WAY
TO
PERFORM
A
SEMI
RANDOM
INITIALIZATION
THAT
ATTEMPTS
TO
PICK
INITIAL
MEANS
AS
FAR
FROM
EACH
OTHER
AS
POSSIBLE
THE
HEURISTIC
IS
SKETCHED
BELOW
PICK
A
RANDOM
EXAMPLE
M
AND
SET
XM
FOR
K
K
FIND
THE
EXAMPLE
M
THAT
IS
AS
FAR
AS
POSSIBLE
FROM
ALL
PREVI
OUSLY
SELECTED
MEANS
NAMELY
M
ARG
MAXM
MINK
K
XM
ΜK
AND
SET
ΜK
XM
IN
THIS
HEURISTIC
THE
ONLY
BIT
OF
RANDOMNESS
IS
THE
SELECTION
OF
THE
FIRST
DATA
POINT
AFTER
THAT
IT
IS
COMPLETELY
DETERMINISTIC
EXCEPT
IN
THE
RARE
CASE
THAT
THERE
ARE
MULTIPLE
EQUIDISTANT
POINTS
IN
STEP
IT
IS
EXTREMELY
IMPORTANT
THAT
WHEN
SELECTING
THE
MEAN
YOU
SELECT
THAT
POINT
THAT
MAXIMIZES
THE
MINIMUM
DISTANCE
TO
THE
CLOSEST
OTHER
MEAN
YOU
WANT
THE
POINT
THAT
AS
FAR
AWAY
FROM
ALL
PREVIOUS
MEANS
AS
POSSIBLE
THE
FURTHEST
FIRST
HEURISTIC
IS
JUST
THAT
A
HEURISTIC
IT
WORKS
VERY
WELL
IN
PRACTICE
THOUGH
CAN
BE
SOMEWHAT
SENSITIVE
TO
OUTLIERS
WHICH
WILL
OFTEN
GET
SELECTED
AS
SOME
OF
THE
INITIAL
MEANS
HOWEVER
THIS
OUTLIER
SENSITIVITY
IS
USUALLY
REDUCED
AFTER
ONE
ITERATION
THROUGH
THE
K
MEANS
ALGORITHM
DESPITE
BEING
JUST
A
HEURISTIC
IT
IS
QUITE
USEFUL
IN
PRACTICE
YOU
CAN
TURN
THE
HEURISTIC
INTO
AN
ALGORITHM
BY
ADDING
A
BIT
MORE
RANDOMNESS
THIS
IS
THE
IDEA
OF
THE
K
MEANS
ALGORITHM
WHICH
IS
A
SIMPLE
RANDOMIZED
TWEAK
ON
THE
FURTHEST
FIRST
HEURISTIC
THE
IDEA
IS
THAT
WHEN
YOU
SELECT
THE
KTH
MEAN
INSTEAD
OF
CHOOSING
THE
ABSOLUTE
FURTHEST
DATA
POINT
YOU
CHOOSE
A
DATA
POINT
AT
RANDOM
WITH
PROBABILITY
PROPORTIONAL
TO
ITS
DISTANCE
SQUARED
THIS
IS
MADE
FORMAL
IN
ALGORITHM
IF
YOU
USE
K
MEANS
AS
AN
INITIALIZATION
FOR
K
MEANS
THEN
YOU
ARE
ABLE
TO
ACHIEVE
AN
APPROXIMATION
GUARANTEE
ON
THE
FINAL
VALUE
UNSUPERVISED
LEARNING
ALGORITHM
K
MEANS
D
K
XM
FOR
M
CHOSEN
UNIFORMLY
AT
RANDOM
RANDOMLY
INITIALIZE
FIRST
POINT
FOR
K
TO
K
DO
DN
MINK
K
XN
ΜK
N
COMPUTE
DISTANCES
P
D
NORMALIZE
TO
PROBABILITY
DISTRIBUTION
M
RANDOM
SAMPLE
FROM
P
PICK
AN
EXAMPLE
AT
RANDOM
ΜK
XM
END
FOR
RUN
K
MEANS
USING
Μ
AS
INITIAL
CENTERS
OF
THE
OBJECTIVE
THIS
DOESN
T
TELL
YOU
THAT
YOU
WILL
REACH
THE
GLOBAL
OPTIMUM
BUT
IT
DOES
TELL
YOU
THAT
YOU
WILL
GET
REASONABLY
CLOSE
IN
PARTICULAR
IF
Lˆ
IS
THE
VALUE
OBTAINED
BY
RUNNING
K
MEANS
THEN
THIS
WILL
NOT
BE
TOO
FAR
FROM
L
OPT
THE
TRUE
GLOBAL
MINIMUM
THEOREM
K
MEANS
APPROXIMATION
GUARANTEE
THE
EXPECTED
VALUE
OF
THE
OBJECTIVE
RETURNED
BY
K
MEANS
IS
NEVER
MORE
THAN
O
LOG
K
FROM
OPTIMAL
AND
CAN
BE
AS
CLOSE
AS
O
FROM
OPTIMAL
EVEN
IN
THE
FORMER
CASE
WITH
RANDOM
RESTARTS
ONE
RESTART
WILL
BE
O
FROM
OPTIMAL
WITH
HIGH
PROBABILITY
FORMALLY
E
LOG
K
MOREOVER
IF
THE
DATA
IS
WELL
SUITED
FOR
CLUSTERING
THEN
E
Lˆ
O
L
OPT
THE
NOTION
OF
WELL
SUITED
FOR
CLUSTERING
INFORMALLY
STATES
THAT
THE
ADVANTAGE
OF
GOING
FROM
K
CLUSTERS
TO
K
CLUSTERS
IS
LARGE
FORMALLY
IT
MEANS
THAT
LK
OPT
OPT
WHERE
LK
OPT
IS
THE
OPTIMAL
VALUE
FOR
CLUSTERING
WITH
K
CLUSTERS
AND
IS
THE
DESIRED
DEGREE
OF
APPROXIMATION
THE
IDEA
IS
THAT
IF
THIS
CONDITION
DOES
NOT
HOLD
THEN
YOU
SHOULDN
T
BOTHER
CLUSTERING
THE
DATA
ONE
OF
THE
BIGGEST
PRACTICAL
ISSUES
WITH
K
MEANS
CLUSTERING
IS
CHOOSING
K
NAMELY
IF
SOMEONE
JUST
HANDS
YOU
A
DATASET
AND
ASKS
YOU
TO
CLUSTER
IT
HOW
MANY
CLUSTERS
SHOULD
YOU
PRODUCE
THIS
IS
DIFFICULT
BECAUSE
INCREASING
K
WILL
ALWAYS
DECREASE
LK
OPT
UNTIL
K
N
AND
SO
SIMPLY
USING
L
AS
A
NOTION
OF
GOODNESS
IS
INSUFFI
CIENT
ANALOGOUS
TO
OVERFITTING
IN
A
SUPERVISED
SETTING
A
NUMBER
OF
INFORMATION
CRITERIA
HAVE
BEEN
PROPOSED
TO
TRY
TO
ADDRESS
THIS
PROBLEM
THEY
ALL
EFFECTIVELY
BOIL
DOWN
TO
REGULARIZING
K
SO
THAT
THE
MODEL
CANNOT
GROW
TO
BE
TOO
COMPLICATED
THE
TWO
MOST
POPULAR
ARE
THE
BAYES
INFORMATION
CRITERIA
BIC
AND
THE
AKAIKE
INFORMATION
CRITERIA
AIC
DEFINED
BELOW
IN
THE
CONTEXT
OF
K
MEANS
BIC
ARG
MIN
ˆK
K
LOG
D
K
AIC
ARG
MIN
ˆK
K
THE
INFORMAL
INTUITION
BEHIND
THESE
CRITERIA
IS
THAT
INCREASING
K
IS
GOING
TO
MAKE
LK
GO
DOWN
HOWEVER
IF
IT
DOESN
T
GO
DOWN
BY
ENOUGH
THEN
IT
NOT
WORTH
DOING
IN
THE
CASE
OF
BIC
BY
ENOUGH
A
COURSE
IN
MACHINE
LEARNING
MEANS
BY
AN
AMOUNT
PROPORTIONAL
TO
LOG
D
IN
THE
CASE
OF
AIC
IT
PROPORTIONAL
TO
THUS
AIC
PROVIDES
A
MUCH
STRONGER
PENALTY
FOR
MANY
CLUSTERS
THAN
DOES
BIC
ESPECIALLY
IN
HIGH
DIMENSIONS
A
MORE
FORMAL
INTUITION
FOR
BIC
IS
THE
FOLLOWING
YOU
ASK
YOURSELF
THE
QUESTION
IF
I
WANTED
TO
SEND
THIS
DATA
ACROSS
A
NETWORK
HOW
MANY
BITS
WOULD
I
NEED
TO
SEND
CLEARLY
YOU
COULD
SIMPLY
SEND
ALL
OF
THE
N
EXAMPLES
EACH
OF
WHICH
WOULD
TAKE
ROUGHLY
LOG
D
BITS
TO
SEND
THIS
GIVES
N
LOG
D
TO
SEND
ALL
THE
DATA
ALTERNATIVELY
YOU
COULD
FIRST
CLUSTER
THE
DATA
AND
SEND
THE
CLUSTER
CENTERS
THIS
WILL
TAKE
K
LOG
D
BITS
THEN
FOR
EACH
DATA
POINT
YOU
SEND
ITS
CENTER
AS
WELL
AS
ITS
DEVIATION
FROM
THAT
CENTER
IT
TURNS
OUT
THIS
WILL
COST
EXACTLY
LˆK
BITS
THEREFORE
THE
BIC
IS
PRECISELY
MEASURING
HOW
MANY
BITS
IT
WILL
TAKE
TO
SEND
YOUR
DATA
USING
K
CLUSTERS
THE
K
THAT
MINIMIZES
THIS
NUMBER
OF
BITS
IS
THE
OPTIMAL
VALUE
LINEAR
DIMENSIONALITY
REDUCTION
DIMENSIONALITY
REDUCTION
IS
THE
TASK
OF
TAKING
A
DATASET
IN
HIGH
DI
MENSIONS
SAY
AND
REDUCING
IT
TO
LOW
DIMENSIONS
SAY
WHILE
RETAINING
THE
IMPORTANT
CHARACTERISTICS
OF
THE
DATA
SINCE
THIS
IS
AN
UNSUPERVISED
SETTING
THE
NOTION
OF
IMPORTANT
CHARACTERISTICS
IS
DIFFICULT
TO
DEFINE
CONSIDER
THE
DATASET
IN
FIGURE
WHICH
LIVES
IN
HIGH
DIMENSIONS
TWO
AND
YOU
WANT
TO
REDUCE
TO
LOW
DIMENSIONS
ONE
IN
THE
CASE
OF
LINEAR
DIMENSIONALITY
REDUCTION
THE
ONLY
THING
YOU
CAN
DO
IS
TO
PROJECT
THE
DATA
ONTO
A
VECTOR
AND
USE
THE
PROJECTED
DISTANCES
AS
THE
EMBEDDINGS
FIGURE
SHOWS
A
PROJECTION
OF
THIS
DATA
ONTO
THE
VECTOR
THAT
POINTS
IN
THE
DIRECTION
OF
MAXIMAL
VARIANCE
OF
THE
ORIGINAL
DATASET
INTUITIVELY
THIS
IS
A
REASONABLE
NOTION
OF
IMPORTANCE
SINCE
THIS
IS
THE
DIRECTION
IN
WHICH
MOST
INFORMATION
IS
ENCODED
IN
THE
DATA
FOR
THE
REST
OF
THIS
SECTION
ASSUME
THAT
THE
DATA
IS
CENTERED
NAMELY
THE
MEAN
OF
ALL
THE
DATA
IS
AT
THE
ORIGIN
THIS
WILL
SIM
PLY
MAKE
THE
MATH
EASIER
SUPPOSE
THE
TWO
DIMENSIONAL
DATA
IS
XN
AND
YOU
RE
LOOKING
FOR
A
VECTOR
U
THAT
POINTS
IN
THE
DIREC
TION
OF
MAXIMAL
VARIANCE
YOU
CAN
COMPUTE
THIS
BY
PROJECTING
EACH
POINT
ONTO
U
AND
LOOKING
AT
THE
VARIANCE
OF
THE
RESULT
IN
ORDER
FOR
THE
PROJECTION
TO
MAKE
SENSE
YOU
NEED
TO
CONSTRAIN
U
IN
THIS
CASE
THE
PROJECTIONS
ARE
U
XN
U
CALL
THESE
VALUES
PN
THE
GOAL
IS
TO
COMPUTE
THE
VARIANCE
OF
THE
PN
AND
THEN
CHOOSE
U
TO
MAXIMIZE
THIS
VARIANCE
TO
COMPUTE
THE
VARIANCE
YOU
FIRST
NEED
TO
COMPUTE
THE
MEAN
BECAUSE
THE
MEAN
OF
THE
XNS
WAS
ZERO
THE
UNSUPERVISED
LEARNING
FIGURE
MEAN
OF
THE
PS
IS
ALSO
ZERO
THIS
CAN
BE
SEEN
AS
FOLLOWS
PN
XN
U
XN
U
U
THE
VARIANCE
OF
THE
PN
IS
THEN
JUST
N
FINDING
THE
OPTIMAL
U
FROM
THE
PERSPECTIVE
OF
VARIANCE
MAXIMIZATION
REDUCES
TO
THE
FOLLOWING
OPTIMIZATION
PROBLEM
MAX
U
XN
U
SUBJ
TO
U
IN
THIS
PROBLEM
IT
BECOMES
APPARENT
WHY
KEEPING
U
UNIT
LENGTH
IS
IMPORTANT
IF
NOT
U
WOULD
SIMPLY
STRETCH
TO
HAVE
INFINITE
LENGTH
TO
MAXIMIZE
THE
OBJECTIVE
IT
IS
NOW
HELPFUL
TO
WRITE
THE
COLLECTION
OF
DATAPOINTS
XN
AS
A
N
D
MATRIX
X
IF
YOU
TAKE
THIS
MATRIX
X
AND
MULTIPLY
IT
BY
U
WHICH
HAS
DIMENSIONS
D
YOU
END
UP
WITH
A
N
VECTOR
WHOSE
VALUES
ARE
EXACTLY
THE
VALUES
P
THE
OBJECTIVE
IN
EQ
IS
THEN
JUST
THE
SQUARED
NORM
OF
P
THIS
SIMPLIFIES
EQ
TO
MAX
U
XU
SUBJ
TO
U
WHERE
THE
CONSTRAINT
HAS
BEEN
REWRITTEN
TO
MAKE
IT
AMENABLE
TO
CON
STRUCTING
THE
LAGRANGIAN
DOING
SO
AND
TAKING
GRADIENTS
YIELDS
L
U
Λ
XU
Λ
U
UL
ΛU
XTX
U
YOU
CAN
SOLVE
THIS
EXPRESSION
ΛU
XTXU
BY
COMPUTING
THE
FIRST
EIGENVECTOR
AND
EIGENVALUE
OF
THE
MATRIX
XTX
THIS
GIVES
YOU
THE
SOLUTION
TO
A
PROJECTION
INTO
A
ONE
DIMENSIONAL
SPACE
TO
GET
A
SECOND
DIMENSION
YOU
WANT
TO
FIND
A
NEW
VECTOR
V
ON
WHICH
THE
DATA
HAS
MAXIMAL
VARIANCE
HOWEVER
TO
AVOID
REDUNDANCY
YOU
WANT
V
TO
BE
ORTHOGONAL
TO
U
NAMELY
U
V
THIS
GIVES
MAX
V
XV
SUBJ
TO
V
AND
U
V
FOLLOWING
THE
SAME
PROCEDURE
AS
BEFORE
YOU
CAN
CONSTRUCT
A
LA
A
COURSE
IN
MACHINE
LEARNING
ALGORITHM
PCA
D
K
Μ
M
EAN
X
COMPUTE
DATA
MEAN
FOR
CENTERING
ΛK
UK
TOP
K
EIGENVALUES
EIGENVECTORS
OF
D
RETURN
X
U
PROJECT
DATA
USING
U
GRANGIAN
AND
DIFFERENTIATE
L
V
XV
U
V
UL
XTX
V
HOWEVER
YOU
KNOW
THAT
U
IS
THE
FIRST
EIGENVECTOR
OF
XTX
SO
THE
SOLUTION
TO
THIS
PROBLEM
FOR
AND
V
IS
GIVEN
BY
THE
SECOND
EIGEN
VALUE
EIGENVECTOR
PAIR
OF
XTX
REPEATING
THIS
ANALYSIS
INDUCTIVELY
TELLS
YOU
THAT
IF
YOU
WANT
TO
PROJECT
ONTO
K
MUTUALLY
ORTHOGONAL
DIMENSIONS
YOU
SIMPLY
NEED
TO
TAKE
THE
FIRST
K
EIGENVECTORS
OF
THE
MATRIX
XTX
THIS
MATRIX
IS
OFTEN
CALLED
THE
DATA
COVARIANCE
MATRIX
BECAUSE
XTX
I
J
N
M
XN
I
XM
J
WHICH
IS
THE
SAMPLE
COVARIANCE
BETWEEN
FEATURES
I
AND
J
THIS
LEADS
TO
THE
TECHNIQUE
OF
PRINCIPLE
COMPONENTS
ANALYSIS
OR
PCA
FOR
COMPLETENESS
THE
IS
DEPICTED
IN
ALGORITHM
THE
IMPORTANT
THING
TO
NOTE
IS
THAT
THE
EIGENANALYSIS
ONLY
GIVES
YOU
THE
PROJECTION
DIRECTIONS
IT
DOES
NOT
GIVE
YOU
THE
EMBEDDED
DATA
TO
EMBED
A
DATA
POINT
X
YOU
NEED
TO
COMPUTE
ITS
EMBEDDING
AS
X
X
X
UK
IF
YOU
WRITE
U
FOR
THE
D
K
MATRIX
OF
US
THEN
THIS
IS
JUST
XU
THERE
IS
AN
ALTERNATIVE
DERIVATION
OF
PCA
THAT
CAN
BE
INFORMATIVE
BASED
ON
RECONSTRUCTION
ERROR
CONSIDER
THE
ONE
DIMENSIONAL
CASE
AGAIN
WHERE
YOU
ARE
LOOKING
FOR
A
SINGLE
PROJECTION
DIRECTION
U
IF
YOU
WERE
TO
USE
THIS
DIRECTION
YOUR
PROJECTED
DATA
WOULD
BE
Z
XU
EACH
ZN
GIVES
THE
POSITION
OF
THE
NTH
DATAPOINT
ALONG
U
YOU
CAN
PROJECT
THIS
ONE
DIMENSIONAL
DATA
BACK
INTO
THE
ORIGINAL
SPACE
BY
MULTIPLYING
IT
BY
UT
THIS
GIVES
YOU
RECONSTRUCTED
VALUES
ZUT
INSTEAD
OF
MAXIMIZING
VARIANCE
YOU
MIGHT
INSTEAD
WANT
TO
MINIMIZE
THE
RECONSTRUCTION
ERROR
DEFINED
BY
DEFINITION
OF
Z
X
XUUT
QUADRATIC
RULE
UNSUPERVISED
LEARNING
X
XUUT
QUADRATIC
RULE
X
X
U
IS
A
UNIT
VECTOR
C
XU
JOIN
CONSTANTS
REWRITE
LAST
TERM
MINIMIZING
THIS
FINAL
TERM
IS
EQUIVALENT
TO
MAXIMIZING
XU
WHICH
IS
EXACTLY
THE
FORM
OF
THE
MAXIMUM
VARIANCE
DERIVATION
OF
PCA
THUS
YOU
CAN
SEE
THAT
MAXIMIZING
VARIANCE
IS
IDENTICAL
TO
MINIMIZ
ING
RECONSTRUCTION
ERROR
THE
SAME
QUESTION
OF
WHAT
SHOULD
K
BE
ARISES
IN
DIMENSION
ALITY
REDUCTION
AS
IN
CLUSTERING
IF
THE
PURPOSE
OF
DIMENSIONALITY
REDUCTION
IS
TO
VISUALIZE
THEN
K
SHOULD
BE
OR
HOWEVER
AN
ALTER
NATIVE
PURPOSE
OF
DIMENSIONALITY
REDUCTION
IS
TO
AVOID
THE
CURSE
OF
DIMENSIONALITY
FOR
INSTANCE
EVEN
IF
YOU
HAVE
LABELED
DATA
IT
MIGHT
BE
WORTHWHILE
TO
REDUCE
THE
DIMENSIONALITY
BEFORE
APPLYING
SUPER
VISED
LEARNING
ESSENTIALLY
AS
A
FORM
OF
REGULARIZATION
IN
THIS
CASE
THE
QUESTION
OF
AN
OPTIMAL
K
COMES
UP
AGAIN
IN
THIS
CASE
THE
SAME
CRITERIA
AIC
AND
BIC
THAT
CAN
BE
USED
FOR
CLUSTERING
CAN
BE
USED
FOR
PCA
THE
ONLY
DIFFERENCE
IS
THE
QUALITY
MEASURE
CHANGES
FROM
A
SUM
OF
SQUARED
DISTANCES
TO
MEANS
FOR
CLUSTERING
TO
A
SUM
OF
SQUARED
DISTANCES
TO
ORIGINAL
DATA
POINTS
FOR
PCA
IN
PARTICULAR
FOR
BIC
YOU
GET
THE
RECONSTRUCTION
ERROR
PLUS
K
LOG
D
FOR
AIC
YOU
GET
THE
RECON
STRUCTION
ERROR
PLUS
MANIFOLDS
AND
GRAPHS
WHAT
IS
A
MANIFOLD
GRAPH
CONSTRUCTION
NON
LINEAR
DIMENSIONALITY
REDUCTION
ISOMAP
LLE
MVU
MDS
NON
LINEAR
CLUSTERING
SPECTRAL
METHODS
WHAT
IS
A
SPECTRUM
SPECTRAL
CLUSTERING
A
COURSE
IN
MACHINE
LEARNING
EXERCISES
EXERCISE
TODO
COMPUTER
VISION
ALGORITHMS
AND
APPLICATIONS
HTTP
SZELISKI
ORG
BOOK
COMPUTER
VISION
ALGORITHMS
AND
APPLICATIONS
RICHARD
SZELISKI
WELCOME
TO
THE
WEB
SITE
HTTP
SZELISKI
ORG
BOOK
FOR
MY
COMPUTER
VISION
TEXTBOOK
WHICH
YOU
CAN
NOW
PURCHASE
AT
A
VARIETY
OF
LOCATIONS
INCLUDING
SPRINGER
SPRINGERLINK
DOI
AMAZON
AND
BARNES
NOBLE
THE
BOOK
IS
ALSO
AVAILABLE
IN
CHINESE
AND
JAPANESE
TRANSLATED
BY
PROF
TORU
TAMAKI
THIS
BOOK
IS
LARGELY
BASED
ON
THE
COMPUTER
VISION
COURSES
THAT
I
HAVE
COTAUGHT
AT
THE
UNIVERSITY
OF
WASHINGTON
AND
STANFORD
WITH
STEVE
SEITZ
AND
DAVID
FLEET
YOU
ARE
WELCOME
TO
DOWNLOAD
THE
PDF
FROM
THIS
WEB
SITE
FOR
PERSONAL
USE
BUT
NOT
TO
REPOST
IT
ON
ANY
OTHER
WEB
SITE
PLEASE
POST
A
LINK
TO
THIS
URL
HTTP
SZELISKI
ORG
BOOK
INSTEAD
AN
ELECTRONIC
VERSION
OF
THIS
MANUSCRIPT
WILL
CONTINUE
TO
BE
AVAILABLE
EVEN
AFTER
THE
BOOK
IS
PUBLISHED
NOTE
HOWEVER
THAT
WHILE
THE
CONTENT
OF
THE
ELECTRONIC
AND
HARDCOPY
VERSIONS
ARE
THE
SAME
THE
PAGE
LAYOUT
PAGINATION
IS
DIFFERENT
SINCE
THE
ELECTRONIC
VERSION
IS
OPTIMIZED
FOR
ONLINE
READING
COMPUTER
VISION
ALGORITHMS
AND
APPLICATIONS
HTTP
SZELISKI
ORG
BOOK
THE
PDFS
SHOULD
BE
ENABLED
FOR
COMMENTING
DIRECTLY
IN
YOUR
VIEWER
ALSO
HYPERLINKS
TO
SECTIONS
EQUATIONS
AND
REFERENCES
ARE
ENABLED
TO
GET
BACK
TO
WHERE
YOU
WERE
USE
ALTLEFTARROW
IN
ACROBAT
IF
YOU
HAVE
ANY
COMMENTS
OR
FEEDBACK
ON
THE
BOOK
PLEASE
SEND
ME
EMAIL
THIS
WEB
SITE
WILL
ALSO
EVENTUALLY
CONTAIN
SUPPLEMENTARY
MATERIALS
FOR
THE
TEXTBOOK
SUCH
AS
FIGURES
AND
IMAGES
FROM
THE
BOOK
SLIDES
SETS
POINTERS
TO
SOFTWARE
AND
A
BIBLIOGRAPHY
ELECTRONIC
DRAFT
SEPTEMBER
ERRATA
SEE
HERE
FOR
A
LIST
OF
ERRORS
THAT
PEOPLE
HAVE
NOTICED
AND
REPORTED
LAST
UPDATED
PROF
TORU
TAMAKI
HAS
COMPILED
A
MORE
EXTENSIVE
LIST
DURING
HIS
TRANSLATION
OF
THE
BOOK
INTO
JAPANESE
SLIDE
SETS
THERE
ARE
NOT
YET
ANY
SLIDE
SETS
TO
GO
WITH
THE
BOOK
PLEASE
FEEL
FREE
TO
LOOK
AT
THE
UNIVERSITY
OF
WASHINGTON
CSE
GRADUATE
COMPUTER
VISION
SLIDES
THAT
STEVE
SEITZ
AND
I
HAVE
PUT
TOGETHER
ADDITIONAL
GOOD
SOURCES
FOR
RELATED
SLIDES
INCLUDE
TREVOR
DARRELL
CS
COMPUTER
VISION
CLASS
AT
BERKELEY
ANTONIO
TORRALBA
ADVANCES
IN
COMPUTER
VISION
CLASS
AT
MIT
MICHAEL
BLACK
CS
INTRODUCTION
TO
COMPUTER
VISION
CLASS
AT
BROWN
KRISTEN
GRAUMAN
CS
COMPUTER
VISION
CLASS
AT
UT
AUSTIN
ALYOSHA
EFROS
COMPUTATIONAL
PHOTOGRAPHY
AND
LEARNINGBASED
METHODS
IN
VISION
CLASSES
AT
CARNEGIE
MELLON
PASCAL
FUA
INTRODUCTION
TO
COMPUTER
VISION
CLASS
AT
EPFL
CS
MACHINE
LEARNING
PROJECT
SUGGESTIONS
HTTPS
WWW
CS
UTEXAS
EDU
MOONEY
PROJECTTOPICS
HTML
CS
MACHINE
LEARNING
PROJECT
SUGGESTIONS
PAGE
PROJECT
PROPOSAL
DUE
NOV
FINAL
PROJECT
REPORT
DUE
DECEMBER
GENERAL
INFORMATION
AND
RESOURCES
THESE
ARE
JUST
SUGGESTIONS
GATHERED
FROM
VARIOUS
ONGOING
UT
RESEARCH
PROJECTS
RELATED
TO
MACHINE
LEARNING
FEEL
FREE
TO
PROPOSE
YOUR
OWN
IDEA
PARTICULARLY
ONE
THAT
RELATES
TO
YOUR
OWN
ONGOING
RESEARCH
INTERESTS
AND
PROJECTS
I
ENCOURAGE
YOU
TO
TALK
TO
ME
ABOUT
YOUR
PROJECT
IDEAS
DURING
OFFICE
HOURS
THE
ONLY
REAL
RESTRICTION
IS
THAT
PROJECTS
SHOULD
INVOLVE
DOING
SOME
RESEARCH
IN
MACHINE
LEARNING
RATHER
THAN
A
WRITING
A
SURVEY
OR
DISCUSSION
PAPER
THE
IDEAL
GOAL
IS
TO
PRODUCE
A
RESEARCH
RESULT
THAT
COULD
BE
PUBLISHED
AS
A
SCIENTIFIC
CONFERENCE
PAPER
LIKE
THE
ONE
YOU
READ
FOR
HOMEWORK
PERHAPS
WITH
SOME
ADDTIONAL
FOLLOWUP
WORK
IN
ADDITION
TO
WEKA
JAVA
ML
CODE
THERE
IS
C
CODE
FOR
DECISION
TREES
IN
U
MOONEY
FOIL
C
CODE
FOR
ILP
IN
U
MOONEY
FOIL
PROLOG
CODE
FOR
ILP
AVAILABLE
FOR
ALEPH
AND
ML
SYSTEMS
IN
C
AVAILABLE
IN
MLC
ALSO
CHECKOUT
DATA
RESOURCES
AVAILABLE
AT
THE
IRVINE
ML
REPOSITORY
THE
KDD
CUP
REPOSITORY
AND
THE
CONLL
SHARED
TASK
DESCRIPTIONS
FOR
THE
CONFERENCE
ON
NATURAL
LANGUAGE
LEARNING
MARKOV
LOGIC
NETWORKS
THE
ALCHEMY
SYSTEM
RECENTLY
DEVELOPED
AT
THE
UNIV
OF
WASHINGTON
COMBINES
STATISTICAL
AND
RELATIONAL
LEARNING
TO
CONSTRUCT
MARKOV
LOGIC
NETWORKS
A
CROSS
BETWEEN
LOGIC
PROGRAMS
AND
MARKOV
NETS
LOCAL
CONTACT
IS
CS
PH
D
STUDENT
LILY
MIHALKOVA
LILYANAM
CS
UTEXAS
EDU
IMPROVE
THE
SPEED
OR
ACCURACY
OF
ALCHEMY
CURRENT
GREEDY
BEAM
SEARCH
ALGORITHM
IMPLEMENT
AND
TEST
VARIOUS
WAYS
OF
LIMITING
THE
SEARCH
SPACE
OR
IMPROVING
THE
OUTPUT
OF
THE
BEAM
SEARCH
ALGORITHM
IMPLEMENT
AND
TEST
COSTSENSITIVE
DISCRIMINATIVE
WEIGHT
TRAINING
OF
MLNS
BY
ADAPTING
THE
ALGORITHM
FROM
SEN
AND
GETOOR
FOR
USE
IN
ALCHEMY
BIOINFORMATICS
THERE
IS
AN
ACTIVE
LOCAL
GROUP
OF
RESEARCHERS
WORKING
ON
DATA
MINING
FOR
BIOINFORMATICS
IN
PARTICULAR
THERE
IS
A
GROUP
COMPETING
IN
A
SCIENTIFIC
COMPETITION
CALLED
MOUSEFUNC
I
FOR
PREDICTING
THE
FUNCTION
OF
MOUSE
GENES
FROM
VARIOUS
SOURCES
OF
INFORMATION
ABOUT
THE
GENE
THAT
IS
INTERESTED
IN
HELP
LOCAL
CONTACTS
ARE
PROF
EDWARD
MARCOTTE
IN
BIOCHEMISTRY
MARCOTTE
ICMB
UTEXAS
EDU
AND
ECE
PH
D
STUDENT
CHASE
KRUMPLEMAN
KRUMP
LANS
ECE
UTEXAS
EDU
BELOW
IS
ANOTHER
PROJECT
IDEA
FROM
PROF
EDWARD
MARCOTTE
AND
BIOLOGY
POSTDOC
CHRISTINE
VOGEL
CVOGEL
MAIL
UTEXAS
EDU
CONTACT
THEM
FOR
DETAILS
WE
ARE
INTERESTED
IN
A
PARTICULAR
VERSION
OF
THESE
CALLED
SYNTHETIC
LETHAL
INTERACTIONS
IN
WHICH
EITHER
GENE
CAN
BE
REMOVED
INDIVIDUALLY
WITHOUT
AFFECTING
THE
CELL
BUT
THE
REMOVAL
OF
BOTH
KILLS
THE
CELL
IN
PARTICULAR
THE
HUBS
IN
A
SYNTHETIC
LETHAL
INTERACTION
NETWORK
ARE
IMPORTANT
THEY
COMPENSATE
FOR
THE
LOSS
OF
MANY
OTHER
GENES
I
SUSPECT
THESE
MAY
OFTEN
BE
GENES
CRITICAL
FOR
SUPPRESSING
TUMOR
FORMATION
ALTHOUGH
WE
HAVEN
T
FORMALLY
TESTED
THIS
IDEA
A
GOOD
PROJECT
WOULD
BE
TO
TRY
TO
PREDICT
WHICH
GENES
ARE
HUBS
IN
THESE
NETWORKSTHAT
IS
LEARN
WHAT
PROPERTIES
SEPARATE
HUBS
FROM
NONHUBS
WITH
FEATURES
BASED
ON
EXPRESSION
DATA
INTERACTION
DATA
FUNCTIONS
ETC
THIS
WOULD
REQUIRE
PLAYING
WITH
VARIOUS
CLASSIFIERS
ETC
ON
FUNCTIONAL
GENOMICS
DATA
IF
ANYONE
GOT
NICE
RESULTS
ON
THIS
WE
HAVE
A
COLLABORATOR
IN
ENGLAND
THAT
WOULD
BE
CS
MACHINE
LEARNING
PROJECT
SUGGESTIONS
HTTPS
WWW
CS
UTEXAS
EDU
MOONEY
PROJECTTOPICS
HTML
WILLING
TO
EXPERIMENTALLY
ASSAY
THE
GENES
FOR
THEIR
PARTICIPATION
IN
SYNTHETIC
LETHAL
INTERACTIONS
SO
THE
PROJECT
WOULD
CONTINUE
ON
BEYOND
THE
CLASS
WITH
REAL
RESULTS
COMPUTER
GRAPHICS
BELOW
IS
AN
IDEA
FROM
CS
PROF
OKAN
ARIKAN
OKAN
CS
UTEXAS
EDU
FOR
APPLYING
MACHINE
LEARNING
TO
COMPUTER
GRAPHICS
PLEASE
CONTACT
HIM
FOR
MORE
DETAILS
GRAPHICS
APPLICATIONS
FILMS
GAMES
ETC
REQUIRE
VERY
DETAILED
DIGITAL
CONTENT
SUCH
AS
GEOMETRIC
MODELS
TEXTURES
ANIMATIONS
CREATING
SUCH
CONTENT
OFTEN
INVOLVES
REPETITIVE
OPERATIONS
FOR
EXAMPLE
IF
WE
RE
CREATING
AN
OLD
DECREPIT
HOUSE
THE
ARTIST
WOULD
HAVE
TO
CREATE
THE
SAME
DUSTY
ERODED
APPEARANCE
WITH
SPIDER
WEBS
DISTRIBUTED
ACCORDINGLY
EVERYWHERE
IN
THE
HOUSE
THE
IDEA
OF
THIS
PROJECT
IS
TO
LEARN
A
MODEL
OF
THE
USER
EDITS
AND
GENERALIZING
THESE
EDITS
TO
CUT
DOWN
THE
CREATION
TIME
FOR
DIGITAL
ENVIRONMENTS
IN
OUR
OLD
DECREPIT
HOUSE
EXAMPLE
IF
WE
LET
THE
USER
CREATE
A
SINGLE
ROOM
OF
THIS
HOUSE
CAN
WE
LEARN
FROM
THIS
WHAT
IT
MEANS
TO
BE
OLD
AND
DECREPIT
AND
APPLY
THIS
TO
THE
REST
OF
THE
HOUSE
AUTOMATICALLY
CAN
WE
LEARN
AND
HELP
THE
USER
AS
THE
USER
IS
PERFORMING
THESE
EDITS
CAN
WE
EVER
REACH
A
STATE
WHERE
AFTER
SUFFICIENT
USE
OF
THIS
SYSTEM
WE
CAN
DEVELOP
A
MODEL
OF
USER
DESIRED
APPEARANCE
AND
RECREATE
IT
FROM
VERY
SIMPLE
INPUTS
ON
NOVEL
ENVIRONMENTS
COMPUTER
ARCHITECTURE
THERE
IS
A
LARGE
ARCHITECTURE
PROJECT
IN
THE
DEPARTMENT
CALLED
TRIPS
CS
PROF
KATHRYN
MCKINLEY
MCKINLEY
CS
UTEXAS
EDU
HAS
PROPOSED
TWO
POTENTIAL
TOPICS
FOR
MACHINE
LEARNING
RESEARCH
IN
THIS
AREA
PLEASE
CONTACT
HER
FOR
DETAILS
WE
HAVE
A
BUNCH
OF
DATA
FROM
SIMULATED
ANNEALING
OF
DIFFERENT
SCHEDULES
ON
TRIPS
WE
EXAMINED
THIS
DATA
BY
HAND
TO
EXTRACT
SCHEDULING
HEURISTICS
IT
WOULD
BE
INTERESTED
TO
SEE
IF
LEARNING
COULD
DO
BETTER
I
WOULD
ALSO
WOULD
LIKE
TO
USE
THE
SAME
FRAMEWORK
TO
GENERATE
REGISTER
BANK
ALLOCATION
AND
SCHEDULING
DATA
FROM
SIMULATED
ANNEALING
AND
SEE
IF
WE
CAN
DERIVE
SOME
REGISTER
BANK
ASSIGNMENT
HEURISTICS
EITHER
INDEPENDENTLY
OR
TOGETHER
WITH
SCHEDULES
BUSINESS
APPLICATIONS
PROF
MAYTAL
SAARTSECHANSKY
MAYTAL
SAARTSECHANSKY
MCCOMBS
UTEXAS
EDU
IN
THE
SCHOOL
OF
BUSINESS
WORKS
IN
MACHINE
LEARNING
AND
DATA
MINING
AND
HAS
PROPOSED
THE
FOLLOWING
TOPICS
PLEASE
SEE
HER
FOR
DETAILS
ACTIVE
INFORMATION
ACQUISITION
PREDICTIVE
MODELS
PLAY
A
DOMINANT
ROLE
IN
NUMEROUS
BUSINESS
INTELLIGENCE
TASKS
A
CRITICAL
FACTOR
AFFECTING
THE
KNOWLEDGE
CAPTURED
BY
SUCH
A
MODEL
IS
THE
QUALITY
OF
THE
INFORMATION
I
E
THE
TRAINING
DATA
FROM
WHICH
THE
MODEL
IS
INDUCED
FOR
MANY
TASKS
POTENTIALLY
PERTINENT
INFORMATION
IS
NOT
IMMEDIATELY
AVAILABLE
BUT
CAN
BE
ACQUIRED
AT
A
COST
TRADITIONALLY
INFORMATION
ACQUISITION
AND
MODELING
ARE
ADDRESSED
INDEPENDENTLY
DATA
ARE
COLLECTED
IRRESPECTIVE
OF
THE
MODELING
OBJECTIVES
HOWEVER
INFORMATION
ACQUISITION
AND
PREDICTIVE
MODELING
IN
FACT
ARE
MUTUALLY
DEPENDENT
NEWLY
ACQUIRED
INFORMATION
AFFECTS
THE
MODEL
INDUCED
FROM
THE
DATA
AND
THE
KNOWLEDGE
CAPTURED
BY
THE
MODEL
CAN
HELP
DETERMINE
WHAT
NEW
INFORMATION
WOULD
BE
MOST
USEFUL
TO
ACQUIRE
INFORMATION
ACQUISITION
POLICIES
TAKE
ADVANTAGE
OF
THIS
RELATIONSHIP
TO
PRODUCING
ACQUISITION
SCHEDULES
AN
ACQUISITION
SCHEDULE
IS
A
RANKING
OF
POTENTIAL
INFORMATION
ACQUISITIONS
IN
THIS
CASE
CURRENTLY
UNKNOWN
FEATURE
VALUES
OR
MISSING
LABELS
TARGET
VARIABLES
AN
IDEAL
ACQUISITION
SCHEDULE
WOULD
RANK
MOST
HIGHLY
THOSE
ACQUISITIONS
THAT
WOULD
YIELD
THE
LARGEST
IMPROVEMENT
IN
MODEL
QUALITY
PER
UNIT
COST
PRIOR
WORK
PROPOSED
ALGORITHMS
FOR
ACQUISITION
OF
EITHER
CLASS
LABELS
I
E
DEPENDENT
TARGET
VARIABLES
OF
TRAINING
EXAMPLES
OR
OF
FEATURE
VALUES
IN
THIS
PROJECT
WE
PROPOSE
AND
EVALUATE
NEW
COMPREHENSIVE
APPROACHES
FOR
INFORMATION
ACQUISITION
TO
RANK
THE
ACQUISITION
OF
BOTH
LABELS
AND
FEATURE
VALUES
THAT
MAY
BE
MISSING
WE
WILL
EVALUATE
NEW
APPROACHES
ON
SEVERAL
BUSINESS
DATA
SETS
WHERE
FEATURE
VALUES
AND
CLASS
LABELS
ARE
COSTLY
TO
ACQUIRE
INFORMATION
ACQUISITION
FOR
COMPLIANCE
MANAGEMENT
DOMAINS
COMPLIANCE
MANAGEMENT
PERTAINS
TO
THE
SELECTION
OF
TAX
REPORTS
TO
BE
AUDITED
OR
HEALTH
CARE
CLAIMS
TO
BE
SCRUTINIZED
FOR
FRAUD
NONCOMPLIANCE
IS
A
CS
MACHINE
LEARNING
PROJECT
SUGGESTIONS
HTTPS
WWW
CS
UTEXAS
EDU
MOONEY
PROJECTTOPICS
HTML
SUBSTANTIAL
SOURCE
OF
REVENUE
LOSS
THAT
AFFLICTS
A
VARIETY
OF
INDUSTRIES
THE
U
DEPARTMENT
OF
HEALTH
AND
HUMAN
SERVICES
REPORTED
IN
THAT
MEDICARE
ALONE
LOST
BILLION
TO
FRAUD
OR
OTHER
IMPROPER
PAYMENTS
TO
PROVIDERS
IN
THE
INTERNAL
REVENUE
SERVICE
REPORTED
RECENTLY
THAT
THE
GAP
BETWEEN
TAXES
PAID
AND
TAXES
OWED
WAS
BETWEEN
BILLION
AND
BILLION
IN
THE
LATEST
YEAR
FOR
WHICH
FIGURES
ARE
AVAILABLE
WITH
ABOUT
ONE
SIXTH
OF
THIS
AMOUNT
EVENTUALLY
COLLECTED
SUBSTANTIAL
LOSSES
HAVE
ALSO
BEEN
REPORTED
BY
THE
AUTO
INSURANCE
INDUSTRY
ADDING
BILLIONS
OF
DOLLARS
TO
AUTO
PREMIUMS
EACH
YEAR
IN
ALL
THESE
SCENARIOS
AT
EACH
POINT
IN
TIME
AN
ANALYST
MUST
DECIDE
WHETHER
TO
AUDIT
A
CASE
IN
ORDER
TO
RECOVER
OR
PREVENT
A
REVENUE
LOSS
SUCH
DECISIONS
ARE
INCREASINGLY
BEING
GUIDED
BY
PREDICTIVE
MODELS
BUILT
BASED
ON
HISTORICAL
AUDIT
OUTCOMES
DUE
TO
THE
VAST
TRANSACTION
VOLUME
REVIEW
OF
ALL
CASES
THAT
MIGHT
BE
RELATED
TO
FRAUD
IS
PROHIBITIVELY
EXPENSIVE
THUS
EFFECTIVE
PREDICTIVE
MODELS
THAT
IDENTIFY
STRONG
LEADS
FOR
FURTHER
INVESTIGATION
ARE
ESSENTIAL
HOWEVER
THERE
EXISTS
A
HURDLE
SHARED
BY
ALL
COMPLIANCE
MANAGEMENT
SECTORS
THAT
RENDERS
MODEL
INDUCTION
IN
THIS
DOMAIN
PARTICULARLY
CHALLENGING
AS
IN
ALL
SUPERVISED
LEARNING
SETTINGS
IN
ORDER
TO
INDUCE
A
MODEL
IT
IS
NECESSARY
FOR
TRAINING
EXAMPLES
TO
BE
LABELED
MEANING
THAT
THE
VALUE
OF
THE
TARGET
VARIABLE
E
G
WHETHER
OR
NOT
A
PARTICULAR
CLAIM
IS
FRAUDULENT
MUST
BE
ACQUIRED
HOWEVER
AUDITS
ARE
CAREFULLY
CHOSEN
TO
TARGET
ONLY
CASES
WHICH
ARE
PREDICTED
TO
HAVE
A
HIGH
PROBABILITY
OF
BEING
FRAUDULENT
AND
THUS
LEADING
TO
REVENUE
RECOVERY
THIS
LEADS
TO
A
SEVERELY
BIASED
TRAINING
SAMPLE
WHICH
CRIPPLES
MODEL
INDUCTION
AND
REVENUE
COLLECTION
SUCH
MODELS
DO
NOT
DETECT
NEW
POCKETS
OF
NONCOMPLIANCE
EFFECTIVELY
FURTHERMORE
RATHER
THAN
HELPING
IMPROVE
THE
MODEL
NEW
AUDIT
OUTCOMES
MERELY
REINFORCE
EXISTING
PERCEPTIONS
AND
DO
NOT
PROVIDE
USEFUL
INFORMATION
AN
ALTERNATIVE
APPROACH
IS
TO
COMPLEMENT
THE
BIASED
SAMPLE
WITH
ADDITIONAL
AUDITS
CAREFULLY
SELECTED
TO
SIGNIFICANTLY
IMPROVE
THE
MODEL
ITSELF
RATHER
THAN
TO
AVOID
IMMINENT
LOSSES
BECAUSE
AUDITS
ARE
COSTLY
IT
IS
ESSENTIAL
TO
DEVISE
SELECTIVE
INFORMATION
ACQUISITION
MECHANISM
THAT
WOULD
IDENTIFY
INFORMATIVE
AUDITS
THAT
WILL
PARTICULARLY
IMPROVE
MODEL
PERFORMANCE
FOR
A
GIVEN
ACQUISITION
COST
EXISTING
ACTIVE
LEARNING
POLICIES
EITHER
ASSUME
THAT
NO
DATA
IS
AVAILABLE
EXANTE
AND
THAT
THE
SAMPLE
IS
CONSTRUCTED
EXCLUSIVELY
BY
THE
ACTIVE
LEARNER
OR
THAT
A
REPRESENTATIVE
SAMPLE
OF
THE
UNDERLYING
POPULATION
IS
AVAILABLE
EXANTE
THESE
ASSUMPTIONS
ARE
FUNDAMENTAL
TO
THE
ACQUISITION
POLICIES
SUBSEQUENT
ACTIONS
AND
ARE
SEVERALLY
VIOLATED
IN
THE
COMPLIANCE
MANAGEMENT
DOMAIN
DUE
TO
THE
BIASED
AUDIT
DATA
THUS
THE
ACTIVE
LEARNER
OUGHT
TO
LEVERAGE
THIS
KNOWLEDGE
OF
BIASNESS
TO
HELP
IDENTIFY
NEW
INFORMATIVE
ACQUISITIONS
THAT
CAN
IMPROVE
THE
MODEL
PERFORMANCE
WE
HAVE
OBTAINED
A
REAL
DATA
SET
ON
COMPANIES
SALES
TAX
AUDITS
WHICH
WE
WILL
USE
IN
THIS
STUDY
TO
EVALUATE
NEW
POLICIES
THE
DATA
INCLUDE
INFORMATION
ABOUT
FIRMS
IN
A
GIVEN
STATE
SALES
TAX
AUDIT
RESULTS
AND
THE
AMOUNTS
OF
MONEY
PAID
BY
COMPANIES
FOLLOWING
THE
AUDITS
COMPUTATIONAL
LINGUISTICS
NATURAL
LANGUAGE
PROCESSING
PROF
KATRIN
ERK
KATRIN
ERK
GMAIL
COM
IN
THE
LINGUISTICS
DEPARTMENT
HAS
SUGGESTED
THE
FOLLOWING
TWO
PROJECTS
CONTACT
HER
FOR
DETAILS
GUESS
WORD
SENSES
FOR
ITEMS
OUTSIDE
THE
LEXICON
SUPPOSE
YOU
WANT
TO
TAG
EACH
CONTENT
WORD
IN
YOUR
TEXT
WITH
A
SENSE
BUT
YOUR
LEXICON
IS
LACKING
ENTRIES
A
LOT
OF
WORDS
ARE
NOT
COVERED
HOWEVER
THE
SENSES
THAT
YOU
HAVE
IN
THE
LEXICON
ARE
ACTUALLY
DESCRIBED
AS
SETS
OF
WORDS
FOR
EXAMPLE
FOR
THE
WORD
BANK
YOU
MIGHT
HAVE
THE
SENSES
DEPOSITORY
FINANCIAL
INSTITUTION
BANK
BANKING
CONCERN
BANKING
COMPANY
AND
BANK
CANT
CAMBER
SO
IF
YOU
ENCOUNTER
A
WORD
THAT
IS
NOT
COVERED
BY
THE
LEXICON
WHAT
YOU
CAN
DO
IS
TO
TRY
TO
FIND
ANOTHER
SIMILAR
WORD
THAT
IS
COVERED
BY
THE
LEXICON
AND
JUST
USE
ITS
SENSE
THE
IDEA
WOULD
BE
TO
USE
A
SEMANTIC
SIMILARITY
METHOD
THAT
YOU
LEARN
FROM
CORPUS
DATA
E
G
THE
ONE
PROPOSED
BY
LIN
FOR
THIS
YOU
CAN
EVALUATE
YOUR
METHOD
ON
WORDS
THAT
ARE
COVERED
BY
THE
LEXICON
BY
PRETENDING
THAT
THEY
ARE
NOT
COVERED
AND
SEEING
WHICH
SIMILAR
WORDS
ARE
PROPOSED
DETECT
OCCURRENCES
OF
AN
IDIOM
IT
IS
BECOMING
MORE
AND
MORE
OBVIOUS
LATELY
THAT
MULTIWORD
EXPRESSIONS
AND
IDIOMS
ARE
NOT
WEIRD
EXCEPTIONS
THEY
OCCUR
A
LOT
AND
IF
YOU
CANNOT
SPOT
THEM
YOUR
MEANING
ANALYSIS
OF
THE
TEXT
WILL
GO
WRONG
A
TEXT
TALKING
ABOUT
LETTING
THE
CAT
OUT
OF
THE
BAG
NEED
NOT
BE
ABOUT
CATS
NOW
SUPPOSE
YOU
HAVE
TRAINING
DATA
LABELED
FOR
WHETHER
A
MULTIWORD
EXPRESSION
OR
IDIOM
IS
PRESENT
CAN
YOU
BUILD
A
CLASSIFIER
THAT
WILL
SPOT
THE
IDIOM
IN
NEW
TEXT
THE
IDEA
WOULD
BE
TO
USE
AN
EXISTING
PARSER
AS
A
BASIS
OCCURRENCES
OF
AN
IDIOM
MAY
VARY
SYNTACTICALLY
EITHER
BECAUSE
THEY
ARE
SYNTACTICALLY
VARIABLE
LET
THE
CAT
OUT
OF
THE
BAG
CAT
IS
OUT
OF
THE
BAG
OR
BECAUSE
THE
PARSER
DOESN
T
TREAT
OCCURRENCES
UNIFORMLY
IDIOMS
MAY
ALSO
VARY
LEXICALLY
FOR
EXAMPLE
YOU
WILL
FIND
OCCURRENCES
OF
LET
THE
SECRET
OUT
OF
THE
BAG
OR
EVEN
LET
CATBERT
OUT
OF
THE
CS
MACHINE
LEARNING
PROJECT
SUGGESTIONS
HTTPS
WWW
CS
UTEXAS
EDU
MOONEY
PROJECTTOPICS
HTML
BAG
THIS
IS
AN
ATTESTED
OCCURRENCE
FEATURES
OF
THE
CLASSIFIER
COULD
BE
SYNTACTIC
SUBTREES
LOCAL
TO
THE
IDIOM
IN
THE
TRAINING
DATA
AND
SEMANTIC
CLASSES
OF
WORDS
THAT
TEND
TO
OCCUR
WITH
THE
IDIOM
DATA
MINING
THE
FOLLOWING
SUGGESTIONS
COME
FROM
PROF
INDERJIT
DHILLON
DATA
MINING
RESEARCH
GROUP
NONNEGATIVE
MATRIX
FACTORIZATION
NONNEGATIVE
MATRIX
FACTORIZATION
ATTEMPTS
TO
FIND
A
FACTORIZATION
OF
A
MATRIX
A
INTO
THE
PRODUCT
OF
TWO
MATRICES
B
AND
C
WHERE
THE
ENTRIES
OF
B
AND
C
ARE
NONNEGATIVE
THIS
HAS
BEEN
USED
RECENTLY
IN
UNSUPERVISED
LEARNING
FOR
VARIOUS
APPLICATIONS
ONE
PROJECT
POSSIBILITY
IS
TO
CODE
UP
AND
USE
NONNEGATIVE
MATRIX
FACTORIZATION
FOR
SOME
APPLICATION
POSSIBILITIES
INCLUDE
TEXT
ANALYSIS
MODELLING
TOPICS
IN
TEXT
IMAGE
FACE
PROCESSING
OR
FOR
PROBLEMS
IN
BIOINFORMATICS
A
SECOND
IDEA
WOULD
BE
EXPLORE
SPARSE
NONNEGATIVE
MATRIX
FACTORIZATION
WHICH
HAS
BEEN
RECENTLY
PROPOSED
THE
CONTACT
FOR
THESE
PROJECTS
IS
SUVRIT
SRA
SUVRIT
CS
UTEXAS
EDU
POWERLAW
GRAPHS
MUCH
LARGESCALE
GRAPH
DATA
HAS
NODE
DEGREES
THAT
ARE
POWERLAW
DISTRIBUTED
A
STANDARD
EXAMPLE
IS
A
WEB
GRAPH
CLUSTERING
OF
SUCH
GRAPHS
IS
SOMEWHAT
DIFFICULT
DUE
TO
THESE
DISTRIBUTIONSONE
PROJECT
IDEA
WOULD
BE
TO
EXPLORE
CLUSTERING
SUCH
GRAPHS
SOME
WORK
HAS
BEEN
DONE
RECENTLY
USING
MIN
BALANCED
CUTS
TO
CLUSTER
THESE
GRAPHS
AND
THERE
HAS
BEEN
PROMISE
IN
USING
NORMALIZED
CUTS
IN
THIS
DOMAIN
AN
OPTION
WOULD
BE
TO
COMPARE
GRACLUS
SOFTWARE
FOR
COMPUTING
THE
NORMALIZED
CUT
IN
A
GRAPH
TO
OTHER
GRAPH
CLUSTERING
METHODS
TO
DETERMINE
THE
MOST
EFFECTIVE
WAYS
TO
COMPUTE
CLUSTERS
IN
POWERLAW
GRAPHS
THE
CONTACT
FOR
THIS
PROJECT
IS
BRIAN
KULIS
KULIS
CS
UTEXAS
EDU
EXPERIMENTS
WITH
SVMS
SEVERAL
PROJECTS
ARE
POSSIBLE
FOR
SUPPORT
VECTOR
MACHINES
ONE
POSSIBILITY
IS
TO
FIND
A
GOOD
APPLICATION
CREATE
A
SUPPORT
VECTOR
MACHINE
IMPLEMENTATION
AND
PERFORM
EXPERIMENTS
ANOTHER
POSSIBILITY
IS
TO
COMPARE
DIFFERENT
EXISTING
IMPLEMENTATIONS
OF
SVMS
INCLUDING
SVM
CODE
DEVELOPED
AT
UT
IN
TERMS
OF
SPEED
AND
ACCURACY
A
THIRD
IDEA
IS
TO
EXPLORE
DIFFERENT
METHODS
OF
REGULARIZATION
FOR
SVMS
THIS
WOULD
REQUIRE
SOME
KNOWLEDGE
OF
OPTIMIZATION
AND
COMPARE
PERFORMANCE
CONTACTS
FOR
THIS
PROJECT
ARE
DONGMIN
KIM
DMKIM
CS
UTEXAS
EDU
SUVRIT
SRA
SUVRIT
CS
UTEXAS
EDU
AND
BRIAN
KULIS
KULIS
CS
UTEXAS
EDU
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
HOMEWORK
DUE
INSTRUCTIONS
PLEASE
PROVIDE
YOUR
CODE
AND
YOUR
WRITTEN
ANSWERS
YOUR
WRITTEN
ANSWERS
SHOULD
BE
IN
THE
FORM
OF
A
PDF
OR
WORD
DOCUMENT
DOC
OR
DOCX
YOUR
CODE
SHOULD
BE
WRITTEN
IN
MATLAB
ZIP
OR
TAR
YOUR
WRITTEN
ANSWERS
AND
M
FILES
AND
UPLOAD
THE
ZIP
OR
TAR
FILE
ON
COURSEWEB
ASSIGNMENTS
HOMEWORK
NAME
THE
FILE
ZIP
OR
TAR
IF
YOU
DO
NOT
SEE
THE
ASSIGNMENTS
BUTTON
NOTIFY
THE
INSTRUCTOR
NOTE
IF
YOU
ARE
ASKED
TO
IMPLEMENT
SOMETHING
BY
YOURSELF
IT
IS
NOT
OK
TO
USE
OR
EVEN
LOOK
AT
EXISTING
MATLAB
CODE
FOR
ANYTHING
YOU
ARE
NOT
ASKED
TO
IMPLEMENT
FEEL
FREE
TO
LOOK
UP
RELEVANT
MATLAB
FUNCTIONS
IF
YOU
HAVE
QUESTIONS
ABOUT
WHAT
YOU
CAN
USE
ASK
THE
INSTRUCTOR
OR
THE
TA
PART
I
SHORT
ANSWERS
POINTS
A
PTS
BISHOP
EXERCISES
AND
B
PTS
PROPOSE
A
NEW
PROBLEM
THAT
CAN
BE
SOLVED
WITH
MACHINE
LEARNING
ONE
THAT
WE
HAVE
NOT
DISCUSSED
IN
CLASS
AND
DESCRIBE
HOW
YOU
WOULD
GO
ABOUT
SOLVING
IT
MAKE
SURE
TO
DESCRIBE
BOTH
THE
ALGORITHM
DESIGN
AS
WELL
AS
THE
DATA
COLLECTION
AND
EVALUATION
OF
YOUR
ALGORITHM
OF
COURSE
YOU
KNOW
VERY
LITTLE
ABOUT
MACHINE
LEARNING
AT
THIS
POINT
UNLESS
YOU
HAVE
PRIOR
EXPERIENCE
WITH
MACHINE
LEARNING
WHICH
WILL
BE
TAKEN
INTO
ACCOUNT
WHEN
GRADING
WHAT
ARE
SOME
POTENTIAL
PROBLEMS
WITH
YOUR
APPROACH
C
PTS
DESCRIBE
WHAT
OVERFITTING
IS
HOW
WE
CAN
DETECT
IT
IF
WE
HAD
ACCESS
TO
THE
TEST
DATA
WHICH
USUALLY
WE
DON
T
WHY
IT
IS
A
PROBLEM
AND
DISCUSS
THREE
POSSIBLE
WAYS
TO
DEAL
WITH
IT
PART
II
MATLAB
BASICS
POINTS
GENERATE
TWO
FIVE
THOUSAND
BY
ONE
VECTORS
OF
RANDOM
NUMBERS
FROM
A
GAUSSIAN
DISTRIBUTION
WITH
MEAN
APPROXIMATELY
AND
STANDARD
DEVIATION
APPROXIMATELY
USE
MATLAB
RANDN
FUNCTION
ADD
THE
NTH
VALUE
OF
THE
SECOND
VECTOR
TO
THE
NTH
VALUE
OF
THE
FIRST
VECTOR
BY
USING
A
LOOP
TO
GET
THE
NUMBER
OF
LOOPS
YOU
NEED
TO
RUN
USE
MATLAB
SIZE
FUNCTION
TIME
THE
ABOVE
OPERATION
PRINT
THE
NUMBER
AND
WRITE
THE
TOTAL
TIME
TAKEN
IN
YOUR
ANSWER
SHEET
USE
MATLAB
TIC
AND
TOC
FUNCTIONS
NOW
DO
THE
SAME
ADDITION
BUT
WITHOUT
USING
A
LOOP
TIME
THIS
OPERATION
PRINT
THE
TIME
AND
WRITE
IT
DOWN
READ
IN
THIS
IMAGE
INTO
MATLAB
AS
A
MATRIX
AND
WRITE
DOWN
ITS
DIMENSIONS
COMPUTE
THE
MEAN
ALONG
THE
THIRD
DIMENSION
AND
SAVE
THE
RESULTING
MATRIX
IN
A
MATRIX
VARIABLE
FIND
THE
LARGEST
VALUE
IN
THE
MATRIX
AND
WRITE
IT
YOUR
ANSWER
SHEET
CONVERT
THE
MATRIX
TO
A
VECTOR
FIRST
AND
USE
MATLAB
FUNCTION
REPLACE
ANY
OF
THE
PIXELS
EQUAL
TO
THE
LARGEST
PIXEL
VALUE
AND
ITS
NEIGHBORS
BY
THE
VALUE
WITHOUT
USING
A
LOOP
USE
MATLAB
FUNCTION
REPMAT
FIND
THE
INDICES
IN
THE
RESULTING
MATRIX
THAT
ARE
LARGER
THAN
AND
REPORT
THEIR
COUNT
IN
YOUR
WRITEUP
PLOT
A
HISTOGRAM
OF
THE
VALUES
IN
THE
MATRIX
AND
INCLUDE
THE
FIGURE
IN
YOUR
WRITEUP
PART
III
COMPUTING
AN
IMAGE
REPRESENTATION
POINTS
IN
THIS
PROBLEM
YOU
WILL
COMPUTE
A
DESCRIPTOR
FOR
AN
IMAGE
THE
X
IN
MACHINE
LEARNING
NOTATION
TO
DO
SO
YOU
WILL
COMPUTE
THE
RESPONSE
OF
AN
IMAGE
TO
EACH
OF
A
SET
OF
FILTERS
AND
THEN
COMPUTE
A
HISTOGRAM
OVER
THE
RESPONSE
VALUES
DOWNLOAD
THE
LEUNGMALIK
FILTER
BANK
CODE
ONE
FUNCTION
AND
READ
THE
DESCRIPTION
AT
THE
TOP
OF
HOW
TO
RUN
IT
RUN
THE
CODE
TO
OBTAIN
THE
FILTER
BANK
F
AND
APPLY
EACH
OF
FILTERS
I
E
EACH
F
I
TO
AN
IMAGE
OF
YOUR
CHOICE
RESULTING
IN
ONE
RESP
IM
F
I
VALID
VALUE
FOR
EACH
FILTER
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
I
NOTE
THAT
IM
IMREAD
IMAGE
FILENAME
IM
IM
IS
AN
IMAGE
READ
INTO
MATLAB
AND
CONVERTED
TO
GRAYSCALE
COMPUTE
A
HISTOGRAM
FOR
EACH
OF
THE
I
RESPONSES
WITH
BIN
SIZES
OF
YOUR
CHOICE
AND
CONCATENATE
THE
HISTOGRAMS
THIS
IS
THE
FINAL
REPRESENTATION
FOR
YOUR
IMAGE
REPORT
ITS
DIMENSIONALITY
IN
YOUR
WRITEUP
WHAT
TEST
CAN
YOU
RUN
TO
DETERMINE
IF
THE
REPRESENTATION
YOU
COMPUTED
IS
A
USEFUL
REPRESENTATION
INCLUDE
THE
ANSWER
IN
YOUR
WRITEUP
OPTIONAL
IF
YOU
WANT
TO
VISUALIZE
EACH
RESPONSE
CALL
FIGURE
IMSHOW
RESP
PART
IV
THE
CLASSIFICATION
PIPELINE
POINTS
IN
THIS
PROBLEM
YOU
WILL
TRAIN
A
MULTICLASS
MODEL
TO
DISTINGUISH
BETWEEN
DIFFERENT
ANIMALS
FIRST
COPY
THE
ANIMALS
WITH
ATTRIBUTES
DATASET
ORIGINALLY
APPEARING
HERE
FROM
THE
FOLLOWING
PITT
AFS
DIRECTORY
YOU
NEED
TO
BE
ON
CAMPUS
OR
USE
VPN
TO
ACCESS
THE
LOCAL
COPY
WHICH
CONTAINS
FOUR
FEATURE
TYPE
FOLDERS
PLUS
A
CLASSES
TXT
FILE
WHICH
YOU
CAN
ALSO
DOWNLOAD
FROM
THE
ORIGINAL
BASE
PACKAGE
THE
DATASET
INCLUDES
ANIMAL
CATEGORIES
ATTRIBUTES
AND
IMAGES
FOR
THIS
PROBLEM
WE
WILL
NOT
USE
THE
ATTRIBUTES
YOU
CAN
USE
TEXTREAD
CLASSES
TXT
U
TO
READ
IN
THE
CLASS
NAMES
FOUR
FEATURE
TYPES
ARE
PROVIDED
CQHIST
WHICH
ARE
COLOR
HISTOGRAMS
PHOGHIST
AND
SIFTHIST
WHICH
ARE
TWO
TYPES
OF
HISTOGRAMS
OF
GRADIENTS
AND
DECAF
WHICH
ARE
FEATURES
EXTRACTED
FROM
A
DEEP
NEURAL
NETWORK
PICK
ANY
ONE
FEATURE
TYPE
TO
USE
FOR
ANY
FEATURE
TYPE
THERE
IS
ONE
TEXT
FILE
FOR
EVERY
IMAGE
AND
FILES
ARE
ORGANIZED
BY
ANIMAL
CATEGORIES
NORMALIZE
YOUR
FEATURES
DIVIDE
ALL
VALUES
IN
THE
DESCRIPTOR
FOR
IMAGE
I
BY
THE
SUM
OF
VALUES
IN
THAT
DESCRIPTOR
AND
DO
THIS
FOR
ALL
IMAGES
SEPARATELY
SPLIT
THE
IMAGES
FEATURES
AVAILABLE
FOR
EACH
ANIMAL
INTO
THREE
GROUPS
A
TRAINING
SET
A
VALIDATION
SET
AND
A
TEST
SET
NOW
YOU
WILL
TRAIN
A
MULTIWAY
SVM
ANIMAL
CLASSIFIER
YOU
FILL
USE
MATLAB
FUNCTION
MODEL
FITCECOC
WHERE
OF
SIZE
NXD
ARE
YOUR
FEATURES
AND
OF
SIZE
ARE
THE
GROUND
TRUTH
LABELS
FOR
THE
TRAINING
SAMPLES
THE
LABEL
VALUES
SHOULD
BE
INTEGERS
BETWEEN
AND
FOR
ANIMALS
TO
USE
THE
MODEL
YOU
JUST
LEARNED
YOU
WILL
CALL
LABEL
PREDICT
MODEL
WHERE
OF
SIZE
MXD
IS
THE
DESCRIPTOR
FOR
THE
M
SAMPLES
WHOSE
LABELS
YOU
WANT
TO
PREDICT
YOU
MIGHT
WANT
TO
USE
A
TEMPLATE
SVM
T
TEMPLATESVM
STANDARDIZE
MODEL
FITCECOC
LEARNERS
T
YOU
HAVE
SO
FAR
USED
THE
DEFAULT
MISCLASSIFICATION
PARAMETERS
YOU
WILL
NOW
AUTOMATICALLY
CHOOSE
THE
BEST
VALUE
FOR
ONE
PARTICULAR
PARAMETER
THE
MISCLASSIFICATION
COST
SOMETHING
WE
WILL
DISCUSS
LATER
YOU
WILL
SET
ALL
OFFDIAGONAL
VALUES
IN
THE
COST
MATRIX
TO
THE
SAME
VALUE
C
THAT
PARAMETER
CAN
BE
MODIFIED
BY
ADDING
COST
AT
THE
END
OF
YOUR
CALL
TO
FITCECOC
WHERE
REPMAT
C
LOGICAL
EYE
SIZE
THE
COST
CAN
BE
MODIFIED
BY
ADDING
BOXCONSTRAINT
AT
THE
END
OF
YOUR
CALL
TO
TEMPLATESVM
TRY
THREE
DIFFERENT
VALUES
OF
C
FOR
EXAMPLE
AND
AND
PICK
THAT
VALUE
FOR
YOUR
FINAL
MODEL
THAT
GIVES
THE
BEST
PERFORMANCE
ACCURACY
ON
THE
VALIDATION
SET
TO
DETERMINE
THE
ACCURACY
OF
A
MODEL
ON
SOME
SET
COMPUTE
WHAT
FRACTION
OF
THE
IMAGES
IN
THAT
SET
WERE
ASSIGNED
THE
CORRECT
LABEL
I
E
THE
GROUND
TRUTH
LABEL
THAT
CAME
WITH
THE
DATASET
ONCE
YOU
HAVE
FIXED
THE
COST
PARAMETER
RUN
YOUR
MODEL
ON
THE
TEST
DATA
AND
REPORT
THAT
FINAL
ACCURACY
IN
YOUR
WRITEUP
ALSO
REPORT
THE
ERROR
ON
THE
TRAINING
DATA
FINALLY
EXPERIMENT
WITH
DIFFERENT
AMOUNTS
OF
TRAINING
DATA
AND
REPORT
HOW
THE
ERROR
ON
THE
TEST
DATA
AND
TRAINING
DATA
CHANGES
AS
YOU
ADD
MORE
TRAINING
DATA
INCLUDE
A
PLOT
IN
YOUR
WRITEUP
WITH
AT
LEAST
FIVE
DIFFERENT
VALUES
FOR
THE
SIZE
OF
THE
TRAINING
DATA
ON
THE
XAXIS
AND
ACCURACY
ON
THE
YAXIS
FOR
THE
TRAINING
AND
TEST
SETS
SEPARATELY
I
E
SHOW
TWO
CURVES
EXPLAIN
WHAT
YOU
ARE
OBSERVING
AND
WHY
IT
MIGHT
BE
HAPPENING
GRADING
RUBRIC
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
A
LOADING
THE
FEATURES
SPLITTING
THE
DATA
AND
RUNNING
THE
TRAINING
TESTING
WITH
DEFAULT
PARAMETERS
PTS
B
USING
THE
VALIDATION
SET
TO
PICK
THE
BEST
C
VALUE
PTS
C
DEMONSTRATING
HOW
TRAIN
TEST
ERROR
CHANGES
AS
MORE
TRAINING
DATA
IS
ADDED
PTS
D
YOUR
WRITEUP
INCLUDING
EXPLANATIONS
PTS
PART
V
SEGMENTATION
VIA
CLUSTERING
POINTS
FOR
THIS
PROBLEM
YOU
WILL
PERFORM
IMAGE
CLUSTERING
WITH
YOUR
FAVORITE
CLUSTERING
METHOD
WHICH
YOU
WILL
IMPLEMENT
YOU
HAVE
GREAT
FREEDOM
IN
HOW
YOU
GO
ABOUT
THIS
PROBLEM
WE
ONLY
CARE
HOW
GOOD
QUALITATIVELY
YOUR
SEGMENTATION
RESULTS
ARE
THE
FEATURES
AND
SEGMENTATION
METHOD
YOU
USE
WILL
BE
RELATIVELY
SIMPLE
SO
OF
COURSE
WE
DON
T
EXPECT
THE
RESULTS
TO
BE
ANYWHERE
CLOSE
TO
PERFECT
DOWNLOAD
BOTH
THE
IMAGES
AND
OPTIONALLY
FOR
QUALITATIVE
EVALUATION
HUMAN
SEGMENTATIONS
FROM
THE
THE
BERKELEY
SEGMENTATION
DATASET
AND
BENCHMARK
TO
PERFORM
SEGMENTATION
YOU
NEED
A
REPRESENTATION
FOR
EVERY
IMAGE
PIXEL
FOR
SIMPLICITY
THE
ONLY
TWO
REPRESENTATIONS
YOU
USE
WILL
BE
THE
R
G
AND
B
VALUES
OF
EACH
PIXEL
AND
THE
VERTICAL
AND
HORIZONTAL
GRADIENTS
OF
EACH
PIXEL
FOR
THE
LATTER
YOU
CAN
CONVERT
THE
IMAGE
TO
GRAYSCALE
AND
AT
EACH
PIXEL
I
I
J
COMPUTE
THE
DIFFERENCE
IN
INTENSITY
BETWEEN
I
I
AND
I
I
J
AS
THE
HORIZONTAL
GRADIENT
AND
BETWEEN
I
J
AND
I
I
J
AS
THE
VERTICAL
GRADIENT
EXPERIMENT
WITH
DIFFERENT
COMBINATIONS
OF
RGB
VALUES
AND
GRADIENTS
AS
YOUR
PIXEL
REPRESENTATION
YOU
CAN
INCLUDE
THE
X
Y
LOCATION
OF
EACH
PIXEL
IN
YOUR
CLUSTERING
REPRESENTATION
IF
YOU
WISH
IMPLEMENT
AND
PERFORM
CLUSTERING
OVER
THE
RESULTING
REPRESENTATION
SPACE
AND
PLACE
EACH
PIXEL
IN
SOME
CLUSTER
YOU
CAN
CHOOSE
TO
IMPLEMENT
KMEANS
MEAN
SHIFT
GRAPH
CUTS
OR
AGGLOMERATIVE
CLUSTERING
IN
YOUR
WRITEUP
EXPLAIN
HOW
YOU
CHOSE
WHICH
METHOD
TO
IMPLEMENT
EASINESS
IS
A
GOOD
REASON
TO
PICK
A
METHOD
JUST
MAKE
SURE
TO
EXPLAIN
WHY
YOU
THINK
ONE
IS
EASIER
THAN
ANOTHER
SEE
THE
FOLLOWING
TWO
FUNCTIONS
FOR
FAST
IMPLEMENTATIONS
OF
DISTANCE
COMPUTATIONS
AND
DISTSQR
IF
YOUR
METHOD
IS
STILL
VERY
SLOW
FEEL
FREE
TO
DOWNSAMPLE
THE
IMAGE
YOU
CAN
GENERATE
AS
MANY
CLUSTERS
AS
YOU
D
LIKE
AND
WHATEVER
DISTANCE
FUNCTION
YOU
LIKE
FINALLY
RECOLOR
THE
INPUT
IMAGES
ACCORDING
TO
THE
CLUSTER
THEY
BELONG
TO
FOR
EXAMPLE
YOU
CAN
COLOR
ALL
THE
PIXELS
IN
A
CLUSTER
WITH
THE
MEAN
COLOR
OF
PIXELS
IN
THAT
CLUSTER
INCLUDE
THE
RECOLORING
RESULT
FOR
IMAGES
IN
YOUR
WRITEUP
DISCUSS
THE
QUALITY
OF
THE
SEGMENTATION
EXPLAIN
ANY
CHOICES
YOU
MADE
IN
HOW
YOU
IMPLEMENTED
THE
CLUSTERING
METHOD
AND
WHAT
PARAMETERS
E
G
NUMBER
OF
CLUSTERS
AND
PIXEL
REPRESENTATION
YOU
PICKED
GRADING
RUBRIC
A
THE
CORRECTNESS
OF
YOUR
CLUSTERING
METHOD
IMPLEMENTATION
PTS
B
APPLYING
YOUR
CLUSTERING
METHOD
ON
IMAGES
INCLUDING
REPRESENTING
PIXELS
AND
RECOLORING
DEPENDING
ON
CLUSTER
MEMBERSHIP
PTS
C
YOUR
WRITEUP
INCLUDING
EXPLANATIONS
PTS
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
HOMEWORK
DUE
INSTRUCTIONS
PLEASE
PROVIDE
YOUR
CODE
AND
YOUR
WRITTEN
ANSWERS
YOUR
WRITTEN
ANSWERS
SHOULD
BE
IN
THE
FORM
OF
A
PDF
OR
WORD
DOCUMENT
DOC
OR
DOCX
YOUR
CODE
SHOULD
BE
WRITTEN
IN
MATLAB
ZIP
OR
TAR
YOUR
WRITTEN
ANSWERS
AND
M
FILES
AND
UPLOAD
THE
ZIP
OR
TAR
FILE
ON
COURSEWEB
ASSIGNMENTS
HOMEWORK
NAME
THE
FILE
ZIP
OR
TAR
NOTE
IF
YOU
ARE
ASKED
TO
IMPLEMENT
SOMETHING
BY
YOURSELF
IT
IS
NOT
OK
TO
USE
OR
EVEN
LOOK
AT
EXISTING
MATLAB
CODE
FOR
ANYTHING
YOU
ARE
NOT
ASKED
TO
IMPLEMENT
FEEL
FREE
TO
LOOK
UP
RELEVANT
MATLAB
FUNCTIONS
IF
YOU
HAVE
QUESTIONS
ABOUT
WHAT
YOU
CAN
USE
ASK
THE
INSTRUCTOR
OR
THE
TA
PART
I
KNEAREST
NEIGHBORS
POINTS
IN
THIS
EXAMPLE
YOU
WILL
EXPLORE
KNN
CLASSIFICATION
USING
MATLAB
BUILTIN
KNN
FUNCTION
FIRST
READ
ABOUT
HOW
KNN
CLASSIFICATION
WORKS
IN
MATLAB
HERE
HOWEVER
DO
NOT
MATLAB
BUILTIN
KFOLD
CROSSVALIDATION
FUNCTIONALITY
YOU
WILL
USE
THE
PIMA
INDIANS
DIABETES
DATASET
SAME
AS
USED
IN
THE
VERSION
OF
CS
TAUGHT
BY
PROF
HAUSKRECHT
YOU
WILL
USE
THE
PIMAINDIANSDIABETES
DATA
FILE
AND
SHOULD
ALSO
READ
THE
PIMAINDIANSDIABETES
NAMES
FILE
BOTH
FOUND
AT
THE
DATA
FOLDER
YELLOW
LINK
AT
THE
TOP
THE
LAST
VALUE
IN
EACH
ROW
CONTAINS
THE
TARGET
VALUE
Y
FOR
THAT
ROW
BEFORE
YOU
BEGIN
SPLIT
THE
DATA
INTO
APPROXIMATELY
EQUALLYSIZED
FOLDS
YOUR
RESULTS
REPORTED
BELOW
SHOULD
BE
AN
AVERAGE
OF
THE
RESULTS
WHEN
YOU
TRAIN
ON
THE
FIRST
FOLDS
AND
TEST
ON
THE
REMAINING
THEN
IF
YOU
TRAIN
ON
THE
FOLDS
NUMBERED
THROUGH
AND
THE
FOLD
AND
TESTING
ON
THE
FOLD
ETC
FOR
SIMPLICITY
YOU
CAN
ALSO
JUST
USE
FOLDS
OF
SIZE
AND
DROP
THE
REMAINING
INSTANCES
HINT
YOU
CAN
USE
THE
MATLAB
FUNCTION
SETDIFF
TO
FIND
WHICH
FOLD
YOU
RE
SUPPOSED
TO
USE
FOR
TRAINING
GIVEN
WHAT
YOUR
TEST
FOLD
IS
YOU
CAN
GET
THE
INDICES
OF
DATA
POINTS
IN
A
FOLD
AS
FOLLOWS
IF
Z
IS
THE
SIZE
OF
A
FOLD
AND
I
IS
THE
FOLD
ID
Z
Z
ALSO
BEFORE
YOU
BEGIN
MAKE
SURE
TO
NORMALIZE
THE
DATA
X
BY
SUBTRACTING
THE
MEAN
AND
DIVIDING
BY
THE
STANDARD
DEVIATION
OVER
EACH
DIMENSION
NOTE
THAT
YOU
SHOULD
COMPUTE
THE
MEAN
AND
STDEV
USING
THE
TRAINING
DATA
ONLY
AND
THEN
APPLY
THEM
ON
THE
TEST
DATA
THIS
IS
BECAUSE
IN
A
REAL
APPLICATION
WE
DO
NOT
SEE
THE
TEST
DATA
UNTIL
AFTER
WE
SHIP
OFF
OUR
PROGRAM
CODE
FIRST
USE
THE
DEFAULT
PARAMETER
VALUES
E
G
THE
VALUE
OF
K
AND
THE
CHOICE
OF
DISTANCE
FUNCTIONS
AND
COMPUTE
THE
ACCURACY
ON
YOUR
TEST
SET
HINT
USE
MODEL
FITCKNN
X
Y
PLOT
THE
TEST
ACCURACY
AS
A
FUNCTION
OF
K
USE
THE
FOLLOWING
VALUES
OF
K
EXPLAIN
WHAT
YOU
OBSERVE
HINT
FIT
THE
MODEL
JUST
ONCE
AND
THEN
USE
MODEL
NUMNEIGHBORS
K
BEFORE
APPLYING
IT
FOR
EACH
VALUE
OF
K
RUNNING
THIS
ENTIRE
PART
SHOULD
TAKE
LESS
THAN
A
MINUTE
SO
FAR
WE
HAVE
BEEN
WEIGHING
NEIGHBORS
EQUALLY
NOW
WE
WANT
TO
EXPERIMENT
WITH
WEIGHING
THEM
ACCORDING
TO
THEIR
DISTANCE
TO
THE
TEST
SAMPLE
OF
INTEREST
IMPLEMENT
A
GAUSSIANWEIGHED
KNN
CLASSIFIER
USING
THE
EQUATION
GIVEN
IN
CLASS
EXPERIMENT
WITH
THREE
DIFFERENT
VALUES
OF
THE
BANDWIDTH
PARAMETER
Σ
FROM
THE
EQUATIONS
ON
THE
BOARD
AND
REPORT
THE
RESULTS
CONSIDER
MULTIPLYING
THE
DENOMINATOR
IN
THE
EXPONENTIAL
BY
IF
YOU
WANT
TO
USE
THE
EXACT
SAME
FORM
AS
A
GAUSSIAN
KERNEL
EQUIVALENT
TO
SCALING
Σ
BY
FOR
ONE
FIXED
CHOICE
OF
PARAMETER
VALUES
USE
JUST
ONE
FOLD
AS
OPPOSED
TO
NINE
FOLDS
AS
TRAINING
DATA
AND
REPORT
THE
ACCURACY
WHY
DO
YOU
OBSERVE
WHAT
YOU
OBSERVE
DON
T
JUST
SAY
MORE
DATA
IS
BETTER
BUT
YOU
CAN
INFORMALLY
TALK
ABOUT
WHY
MORE
DATA
INCREASES
THE
CHANCE
OF
FINDING
GOOD
REPRESENTATIVES
FOR
THE
TEST
DATA
GRADING
RUBRIC
MAKE
SURE
TO
INCLUDE
THE
RESULTS
IN
YOUR
WRITTEN
REPORT
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
A
POINTS
DEFAULT
PARAMETER
VALUES
SOLUTION
INCLUDING
CODE
TO
SET
UP
CROSSVALIDATION
AND
COMPUTE
AVERAGE
RESULT
OVER
TESTING
ON
TEST
FOLDS
B
POINTS
PLOT
USING
DIFFERENT
VALUES
OF
K
DISCUSSION
C
POINTS
GAUSSIAN
WEIGHING
SOLUTION
D
POINTS
LESS
DATA
EXPERIMENT
DISCUSSION
PART
II
LINEAR
REGRESSION
POINTS
WE
LL
DISCUSS
THIS
TOPIC
ON
IN
THIS
PROBLEM
YOU
WILL
SOLVE
A
REGRESSION
PROBLEM
IN
TWO
WAYS
USING
THE
DIRECT
LEASTSQUARES
SOLUTION
OR
USING
GRADIENT
DESCENT
YOU
WILL
USE
THE
WINE
QUALITY
DATASET
USE
ONLY
THE
RED
WINE
DATA
THE
GOAL
IS
TO
FIND
THE
QUALITY
SCORE
OF
SOME
WINE
BASED
ON
ITS
ATTRIBUTES
FIRST
DIVIDE
THE
DATA
INTO
A
TRAINING
AND
TEST
SET
USING
APPROXIMATELY
FOR
TRAINING
YOU
DON
T
NEED
TO
USE
CROSSVALIDATION
FOR
THIS
PROBLEM
NORMALIZE
THE
DATA
BEFORE
YOU
BEGIN
USE
THE
LINEAR
SYSTEM
OF
EQUATIONS
LEAST
SQUARES
SOLUTION
IN
MATLAB
USING
THE
BACKSLASH
OPERATOR
AX
B
X
A
B
WHAT
ARE
A
X
AND
B
IN
THE
CASE
OF
LINEAR
REGRESSION
USE
THE
RESULTING
SOLUTION
TO
FIND
THE
WINE
QUALITY
SCORES
ON
THE
TEST
DATA
THEN
MEASURE
AND
REPORT
IN
YOUR
WRITTEN
ANSWERS
THE
DISTANCE
BETWEEN
THE
TRUE
AND
PREDICTED
SCORES
NOW
IMPLEMENT
THE
GRADIENT
DESCENT
SOLUTION
FOR
THIS
YOU
WILL
NEED
TO
INITIALIZE
THE
WEIGHTS
IN
SOME
WAY
USE
EITHER
RANDOM
VALUES
OR
ALL
ZEROS
THEN
YOU
REPEAT
THE
FOLLOWING
SOME
NUMBER
OF
TIMES
FOR
THIS
PROBLEM
REPEAT
TIMES
IN
EACH
ITERATION
COMPUTE
THE
ERROR
FUNCTION
GRADIENT
USING
ALL
TRAINING
DATA
POINTS
THEN
ADJUST
THE
WEIGHTS
IN
THE
DIRECTION
OPPOSITE
TO
THE
GRADIENT
APPLY
THE
SOLUTION
TO
THE
TEST
SET
THEN
COMPUTE
AND
REPORT
THE
DISTANCE
AS
ABOVE
EXPERIMENT
WITH
DIFFERENT
LEARNING
RATES
E
G
ONES
IN
THE
RANGE
I
E
AND
REPORT
YOUR
OBSERVATIONS
GRADING
RUBRIC
A
POINTS
DATA
SET
UP
AND
LEAST
SQUARES
SOLUTION
B
POINTS
GRADIENT
DESCENT
SOLUTION
PART
III
FISHER
LINEAR
DISCRIMINANT
POINTS
WE
LL
DISCUSS
THIS
TOPIC
ON
YOU
HAVE
THE
FOLLOWING
TWODIMENSIONAL
DATA
THE
FIRST
FIVE
DATA
POINTS
BELONG
TO
ONE
CLASS
AND
THE
SECOND
SET
OF
FIVE
TO
A
SECOND
CLASS
DOWNLOAD
THE
STARTER
CODE
HERE
ADD
THE
FOLLOWING
TO
THAT
CODE
COMPUTE
THE
DIRECTION
OF
THE
W
VECTOR
CORRESPONDING
TO
THE
FISHER
LINEAR
DISCRIMINANT
OF
THE
DATA
POINTS
RUN
THE
STARTER
CODE
WHICH
WILL
PLOT
THE
POINTS
IN
ALONG
WITH
THE
FISHER
DISCRIMINANT
DIRECTION
SAVE
THE
FIGURE
AND
INCLUDE
IT
IN
YOUR
WRITTEN
REPORT
PART
IV
SHORT
ANSWERS
POINTS
WE
LL
DISCUSS
THESE
TOPICS
ROUGHLY
ON
AND
A
POINTS
BISHOP
EXERCISE
B
POINTS
IN
THE
DISCUSSION
OF
SVMS
WE
DESCRIBE
HOW
WE
CAN
ACCOUNT
FOR
NONLINEARLYSEPARABLE
DATA
USING
SLACK
VARIABLES
SEE
EQUATIONS
AND
IN
BISHOP
ONE
CAN
ALSO
USE
SLACK
VARIABLES
FOR
REGRESSION
WRITE
DOWN
THE
OPTIMIZATION
OBJECTIVE
AND
CONSTRAINTS
FOR
LINEAR
REGRESSION
SIMILAR
TO
EQUATIONS
AND
FOR
CLASSIFICATION
THAT
RESULTS
IF
YOU
ADD
SLACK
VARIABLES
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
HOMEWORK
DUE
INSTRUCTIONS
PLEASE
PROVIDE
YOUR
CODE
AND
YOUR
WRITTEN
ANSWERS
YOUR
WRITTEN
ANSWERS
SHOULD
BE
IN
THE
FORM
OF
A
PDF
OR
WORD
DOCUMENT
DOC
OR
DOCX
YOUR
CODE
SHOULD
BE
WRITTEN
IN
MATLAB
ZIP
OR
TAR
YOUR
WRITTEN
ANSWERS
AND
M
FILES
AND
UPLOAD
THE
ZIP
OR
TAR
FILE
ON
COURSEWEB
ASSIGNMENTS
HOMEWORK
NAME
THE
FILE
ZIP
OR
TAR
NOTE
IF
YOU
ARE
ASKED
TO
IMPLEMENT
SOMETHING
BY
YOURSELF
IT
IS
NOT
OK
TO
USE
OR
EVEN
LOOK
AT
EXISTING
MATLAB
CODE
FOR
ANYTHING
YOU
ARE
NOT
ASKED
TO
IMPLEMENT
FEEL
FREE
TO
LOOK
UP
RELEVANT
MATLAB
FUNCTIONS
IF
YOU
HAVE
QUESTIONS
ABOUT
WHAT
YOU
CAN
USE
ASK
THE
INSTRUCTOR
OR
THE
TA
DO
NOT
DISCUSS
THE
SOLUTIONS
OF
THIS
ASSIGNMENT
UNTIL
A
WEEK
AFTER
THE
DEADLINE
PART
I
SHORT
ANSWERS
POINTS
WHAT
YOU
NEED
FOR
PARTS
A
D
HAS
BEEN
DISCUSSED
ALREADY
AND
WHAT
YOU
NEED
FOR
PARTS
E
G
WILL
BE
DISCUSSED
ON
AND
A
PTS
BISHOP
EXERCISE
HINTS
EXPAND
THE
NORM
NOTATION
REMEMBERING
THAT
THE
NORM
OF
A
VECTOR
X
IS
XTX
THAT
XTX
IS
THE
SAME
AS
AN
INNER
PRODUCT
OF
X
WITH
ITSELF
AND
TRANSPOSE
DISTRIBUTION
PROPERTIES
THEN
EXPRESS
THE
EXPANDED
FORM
WITH
KERNEL
NOTATION
WHAT
TYPE
OF
KERNEL
DO
YOU
SEE
B
PTS
BISHOP
EXERCISE
C
PTS
BISHOP
EXERCISE
HINTS
THIS
IS
A
TWOCLASS
PROBLEM
YOU
HAVE
TWO
CONSTRAINTS
TOTAL
ONE
FOR
THE
POSITIVE
INSTANCE
AND
ONE
FOR
THE
NEGATIVE
INSTANCE
AND
THESE
CONSTRAINTS
ARE
EQUALITIES
BECAUSE
YOUR
POSITIVE
AND
NEGATIVE
POINTS
HAVE
TO
LIE
ON
THE
MARGIN
AND
BE
SUPPORT
VECTORS
USE
LAGRANGE
MULTIPLIERS
TO
MAKE
THE
CONSTRAINTS
PART
OF
THE
OPTIMIZATION
PROBLEM
AND
FIND
WHAT
W
AND
B
EQUAL
D
PTS
SHOW
THAT
LINEAR
REGRESSION
WITH
AN
OBJECTIVE
TO
MINIMIZE
THE
SQUARED
DIFFERENCE
BETWEEN
TRUE
AND
PREDICTED
LABELS
TN
WHEN
REGULARIZED
IS
EQUIVALENT
TO
MAXIMUMAPOSTERIORI
MAP
ESTIMATION
WE
LL
MODEL
THE
NOISE
BETWEEN
IN
THE
TARGET
LABELS
TN
WITH
A
GAUSSIAN
DISTRIBUTION
SEE
FIGURE
IN
BISHOP
EXAMINE
THE
FOLLOWING
QUESTIONS
IN
BISHOP
WHEN
LOG
IS
TAKEN
AND
WHICH
COMBINES
AND
SHOW
HOW
THESE
EQUATIONS
CAN
BE
USED
TO
ACHIEVE
THE
MAP
OBJECTIVE
IN
EQUATION
NO
NEED
TO
COMPUTE
ITS
DERIVATIVE
WHAT
LOG
BASE
YOU
USE
DOES
NOT
MATTER
E
PTS
THE
FIRST
PART
ONLY
OF
BISHOP
EXERCISE
HINT
TRANSFORM
THE
RIGHTHAND
SIDE
INTO
THE
LEFTHAND
SIDE
AND
WORK
WITH
DISCRETE
RANDOM
VARIABLES
F
PTS
BISHOP
EXERCISE
G
PTS
JUST
THE
LAST
PART
OF
BISHOP
EXERCISE
DRAW
THE
GRAPHICAL
MODEL
CORRESPONDING
TO
THE
JOINT
PROBABILITY
PART
II
SVM
AS
A
QUADRATIC
PROGRAM
POINTS
WHAT
YOU
NEED
HAS
BEEN
DISCUSSED
ALREADY
IN
THIS
PART
YOU
WILL
IMPLEMENT
AN
SVM
AS
A
QUADRATIC
PROGRAM
QP
USING
MATLAB
QUADPROG
FUNCTION
BUT
NOT
THE
BUILTIN
MATLAB
SVM
YOU
WILL
USE
THE
PIMA
INDIANS
DIABETES
DATASET
FROM
READ
UNDER
DESCRIPTION
FOR
QUADPROG
WHAT
QP
MATLAB
EXPECTS
YOUR
JOB
IS
TO
DEFINE
THE
MATRIX
H
DISCUSSED
IN
CLASS
ON
AND
IN
MY
NOTES
FOR
THAT
DAY
ON
PAGE
AND
TO
TRANSFORM
A
MAXIMIZATION
INTO
A
MINIMIZATION
WHAT
IS
THE
FT
VECTOR
IN
THE
CASE
OF
AN
SVM
FIRST
ADJUST
YOUR
DATA
BY
REPLACING
LABELS
OF
BY
LABELS
OF
SINCE
THE
PIMA
DATASET
DENOTES
NEGATIVE
LABELS
BY
AND
WE
WANT
THEM
TO
BE
DO
CROSSVALIDATION
AS
IN
INCLUDE
A
BIAS
BY
APPENDING
A
IN
FRONT
OF
THE
FEATURE
REPRESENTATION
FOR
EVERY
DATA
SAMPLE
THAT
A
LAZY
WAY
BUT
NOT
FULLY
CORRECT
WAY
TO
NOT
WORRY
ABOUT
THE
BIAS
EXPLICITLY
AND
YOU
DON
T
HAVE
TO
DO
IT
YOU
CAN
DEAL
WITH
THE
BIAS
EXPLICITLY
INSTEAD
AS
WE
DID
IN
CLASS
AND
IN
MY
NOTES
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
MAKE
SURE
TO
STANDARDIZE
THE
DATA
YOU
WILL
CREATE
A
SOFTMARGIN
SVM
ASSUMING
A
NONLINEARLY
SEPARABLE
DATASET
WHICH
CORRESPONDS
TO
SETTING
THE
BOUNDS
ON
THE
ALPHAS
TO
BE
Α
C
WHERE
THE
SCALAR
C
IS
THE
MISCLASSIFICATION
COST
USE
C
USE
THE
CALL
X
QUADPROG
H
F
A
B
AEQ
BEQ
LB
UB
AND
SET
THE
THIRD
THROUGH
SIXTH
ARGUMENT
TO
YOU
CAN
ALSO
USE
AEQ
BEQ
TO
DEFINE
THE
CONSTRAINT
THAT
THE
ALPHAS
TIMES
THE
LABELS
SHOULD
SUM
TO
I
FORGOT
ABOUT
THIS
CONSTRAINT
WHEN
I
WROTE
THIS
UP
SO
IT
FINE
TO
SKIP
IT
NOTE
THAT
YOU
MIGHT
FIND
THAT
ALL
OF
YOUR
TRAINING
SAMPLES
ARE
SUPPORT
VECTORS
THAT
OK
IT
ALSO
OK
IF
YOU
GET
WARNINGS
FROM
MATLAB
COMPUTE
AND
REPORT
THE
ACCURACY
ON
THE
TEST
SET
AND
ON
THE
TRAINING
SET
THE
LATTER
IS
JUST
FOR
COMPARISON
NOW
TRAIN
AND
APPLY
JUST
ON
THE
TEST
SET
THE
BUILTIN
MATLAB
SVM
HOWEVER
REMOVE
ANY
CHANGES
TO
THE
FEATURE
REPRESENTATION
THAT
WERE
MADE
TO
ACCOUNT
FOR
THE
BIAS
USE
THE
SAME
C
VALUE
AS
BEFORE
REPORT
THE
BUILTIN
MATLAB
SVM
ACCURACY
ON
THE
TEST
SET
IF
THE
WTX
FOR
SOME
TEST
INSTANCE
IS
EXACTLY
ZERO
SET
ITS
SIGN
TO
TIME
HOW
LONG
IT
TAKES
TO
LEARN
THE
MODEL
SOLVE
FOR
W
WITH
THE
QP
APPROACH
VERSUS
THE
BUILTIN
MATLAB
SVM
USING
THE
QP
APPROACH
TO
TRAINING
AN
SVM
CREATE
A
FUNCTION
WITH
THE
SIGNATURE
FUNCTION
W
C
AS
WELL
AS
ANOTHER
FUNCTION
FUNCTION
ACCURACY
W
THE
ARE
THE
PREDICTED
LABELS
AND
ARE
THE
THE
RAW
WTX
SCORES
BEFORE
TAKING
THE
SIGN
OF
THAT
EXPRESSION
NAME
YOUR
SCRIPT
FOR
PART
II
M
MAKE
CALLS
TO
YOUR
AND
FUNCTIONS
IN
YOUR
SCRIPT
YOUR
ACCURACY
SHOULD
BE
AROUND
OR
OVER
USEFUL
MATLAB
FUNCTIONS
FOR
THIS
AND
THE
NEXT
PART
QUADPROG
SIGN
SIZE
LENGTH
FPRINTF
TIC
TOC
FIND
RANDPERM
STRCMP
UNIQUE
SETDIFF
NCHOOSEK
MODE
YOU
CAN
IGNORE
OUTPUTS
FROM
A
FUNCTION
USING
E
G
A
B
PART
III
MULTICLASS
SVM
OUT
OF
TWOCLASS
SVMS
POINTS
WHAT
YOU
NEED
HAS
BEEN
DISCUSSED
ALREADY
IN
THIS
PART
YOU
WILL
USE
YOUR
CODE
FROM
PART
II
TO
SOLVE
A
MULTICLASS
PREDICTION
TASK
ON
THE
MULTICLASS
IRIS
DATASET
THIS
DATASET
HAS
THE
SAME
FORMAT
AS
THE
PIMA
INDIANS
DATASET
BUT
YOU
NEED
TO
READ
THE
DATA
DIFFERENTLY
USE
THE
IRIS
DATA
FILE
READ
IT
AS
CLASSES
TEXTREAD
IRIS
DATA
F
F
F
F
WHICH
WILL
GIVE
YOU
VECTORS
OF
REAL
NUMBERS
THROUGH
WHICH
YOU
CONCATENATE
HORIZONTALLY
TO
FORM
YOUR
FEATURE
MATRIX
X
AND
A
VECTOR
OF
STRINGS
CLASSES
FIRST
YOU
NEED
TO
MAP
THE
LAST
ATTRIBUTE
IN
EACH
ROW
TO
AN
INTEGER
BETWEEN
AND
THAT
DENOTES
THE
CLASS
LABEL
FIND
THE
UNIQUE
CLASS
NAMES
AND
THEN
FOR
EACH
SAMPLE
CALL
Y
I
FIND
STRCMP
CLASSES
I
USE
A
SINGLE
TRAINING
TEST
SPLIT
RATHER
THAN
AND
CHOOSE
IMAGES
FOR
TRAINING
AND
FOR
TESTING
HOWEVER
REPEAT
THE
EXPERIMENT
TIMES
I
E
WITH
DIFFERENT
TRAIN
TEST
SPLITS
AND
REPORT
THE
AVERAGE
RESULTS
BUILD
TWO
TYPES
OF
MULTICLASS
SVMS
ONEVSALL
AND
ONEVSONE
THE
METHODOLOGY
FOR
CONSTRUCTING
ONEVSALL
AND
ONEVSONE
CAN
BE
FOUND
IN
THE
SLIDE
DECK
FROM
TOWARDS
THE
END
FOR
THE
ONEVSONE
SVM
DON
T
FORGET
TO
MAP
THE
LABELS
BACK
TO
LABELS
FOR
THE
IRIS
CLASSES
FOR
EXAMPLE
YOU
CAN
SAY
LABELS
LABELS
LABELS
LABELS
THEN
RECORD
THE
VOTES
IN
A
X
NUMBER
OF
ONEVSONE
CLASSIFIERS
MATRIX
AND
CALL
MODE
VOTES
NOTE
THAT
FOR
ONEVSALL
YOU
NEED
TO
USE
THE
DECISION
VALUES
TO
GET
THE
ARGMAX
IN
EACH
ROW
OF
A
X
NUMBER
OF
ONEVSALL
CLASSIFIERS
SCORES
MATRIX
USE
INDS
MAX
SCORES
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
WRITE
YOUR
TWO
CLASSIFIERS
IN
TWO
SEPARATE
SCRIPTS
ONE
WITH
A
SIGNATURE
ACCURACY
C
AND
THE
OTHER
ACCURACY
C
SAME
INPUT
OUTPUT
BUT
DIFFERENT
FUNCTION
NAME
THEN
CALL
THESE
FUNCTIONS
IN
A
SCRIPT
CALLED
M
REPORT
THE
ACCURACY
OF
BOTH
THE
ONEVSALL
AND
ONEVSONE
SVMS
AVERAGED
OVER
THE
RUNS
ALSO
OUTPUT
WHAT
THE
PERFORMANCE
OF
A
CLASSIFIER
THAT
GUESSES
RANDOMLY
WHAT
THE
CLASS
IS
FOR
COMPARISON
THERE
NO
NEED
TO
IMPLEMENT
THIS
RANDOM
CLASSIFIER
JUST
GIVE
THE
EXPECTED
NUMBER
PART
IV
PERCEPTRON
POINTS
WHAT
YOU
NEED
HAS
BEEN
DISCUSSED
ALREADY
IN
THIS
PART
YOU
WILL
TRACE
THROUGH
THE
A
RUN
OF
THE
PERCEPTRON
ALGORITHM
USE
THE
FOLLOWING
DATA
X
Y
ONES
ONES
THIS
DATA
IS
LINEARLY
SEPARABLE
AND
CAN
BE
PLOTTED
IN
USING
ITS
TWO
FEATURE
DIMENSIONS
YOUR
GOAL
IS
TO
CREATE
FIGURES
SIMILAR
TO
FIGURE
IN
BISHOP
WE
ARE
PROVIDING
CODE
THAT
YOU
CAN
USE
TO
PLOT
THE
POINTS
W
AND
THE
DECISION
BOUNDARY
WHICH
PASSES
THROUGH
THE
ORIGIN
AND
IS
PERPENDICULAR
TO
W
DOWNLOAD
THIS
CODE
HERE
IT
WILL
PLOT
POSITIVE
POINTS
AS
HOLLOW
CIRCLES
AND
NEGATIVES
AS
FILLED
CIRCLES
IT
WILL
ALSO
PLOT
CORRECTLY
CLASSIFIED
POINTS
IN
GREEN
AND
MISCLASSIFIED
ONES
IN
RED
YOUR
GOAL
IS
TO
IMPLEMENT
THE
PERCEPTRON
ALGORITHM
AND
USE
THE
PROVIDED
CODE
TO
TRACE
THROUGH
SEVERAL
ITERATIONS
OF
THE
METHOD
USE
ALL
THE
DATA
FOR
TRAINING
FOR
THE
PURPOSES
OF
THIS
EXERCISE
SET
Η
TO
REMEMBER
THAT
IN
EACH
STEP
THE
WEIGHT
VECTOR
W
IS
ADJUSTED
USING
ONE
MISCLASSIFIED
EXAMPLE
YOU
WILL
USE
THE
EQUATIONS
FROM
THE
SLIDES
FOR
PERCEPTRON
ON
INSTEAD
OF
A
BASIS
FUNCTION
YOU
WILL
JUST
USE
X
I
E
Φ
X
X
YOU
CAN
USE
YOUR
FUNCTION
TO
COMPUTE
THE
PREDICTION
AND
ACCURACY
ON
A
TEST
SAMPLE
GIVEN
A
W
OF
COURSE
YOU
HAVE
TO
CALL
THAT
ON
THE
TRAINING
SET
IN
YOUR
CODE
IN
EACH
ITERATION
OUTPUT
THE
ITERATION
ID
AND
THE
TWO
FEATURE
DIMENSIONS
FOR
THE
MISCLASSIFIED
EXAMPLE
THAT
IS
BEING
USED
TO
CORRECT
THE
W
USE
THE
FEATURE
DIMENSIONS
TO
IDENTIFY
WHICH
POINT
IS
BEING
USED
IN
YOUR
WRITEUP
WORD
OR
PDF
FILE
FOR
THREE
CONSECUTIVE
ITERATIONS
OF
THE
METHOD
INCLUDE
A
PLOT
OF
THE
MISCLASSIFIED
POINTS
AND
CURRENT
W
THEN
POINT
OUT
THE
POINT
THAT
WAS
USED
FOR
THE
UPDATE
OF
W
BRIEFLY
DESCRIBE
WHAT
YOU
ARE
OBSERVING
IF
TOO
MANY
OR
TOO
FEW
ITERATIONS
ARE
TAKING
PLACE
TERMINATE
THE
RUN
AND
START
ANOTHER
RUN
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
HOMEWORK
DUE
INSTRUCTIONS
PLEASE
PROVIDE
YOUR
CODE
AND
YOUR
WRITTEN
ANSWERS
YOUR
WRITTEN
ANSWERS
SHOULD
BE
IN
THE
FORM
OF
A
PDF
OR
WORD
DOCUMENT
DOC
OR
DOCX
YOUR
CODE
SHOULD
BE
WRITTEN
IN
MATLAB
ZIP
OR
TAR
YOUR
WRITTEN
ANSWERS
AND
M
FILES
AND
UPLOAD
THE
ZIP
OR
TAR
FILE
ON
COURSEWEB
ASSIGNMENTS
HOMEWORK
NAME
THE
FILE
ZIP
OR
TAR
NOTE
IF
YOU
ARE
ASKED
TO
IMPLEMENT
SOMETHING
BY
YOURSELF
IT
IS
NOT
OK
TO
USE
OR
EVEN
LOOK
AT
EXISTING
MATLAB
CODE
FOR
ANYTHING
YOU
ARE
NOT
ASKED
TO
IMPLEMENT
FEEL
FREE
TO
LOOK
UP
RELEVANT
MATLAB
FUNCTIONS
IF
YOU
HAVE
QUESTIONS
ABOUT
WHAT
YOU
CAN
USE
ASK
THE
INSTRUCTOR
OR
THE
TA
DO
NOT
DISCUSS
THE
SOLUTIONS
OF
THIS
ASSIGNMENT
UNTIL
A
WEEK
AFTER
THE
DEADLINE
PART
I
SHORT
ANSWERS
POINTS
WHAT
YOU
NEED
HAS
BEEN
DISCUSSED
ALREADY
A
PTS
CONSIDER
THE
FOLLOWING
GRAPHICAL
MODEL
THE
VARIABLES
IN
THIS
MODEL
ARE
THE
FOLLOWING
R
CAN
TAKE
VALUES
OR
DEPENDING
ON
WHETHER
IT
RAINED
OR
NOT
CAN
TAKE
VALUES
OR
DEPENDING
ON
WHETHER
THE
SPRINKLER
IN
TOM
YARD
WAS
ON
OR
NOT
T
CAN
TAKE
VALUES
OR
DEPENDING
ON
WHETHER
THE
GRASS
IN
TOM
YARD
IS
WET
OR
NOT
J
CAN
TAKE
VALUES
OR
DEPENDING
ON
WHETHER
THE
GRASS
IN
JULIA
YARD
IS
WET
OR
NOT
LET
THE
PRIOR
PROBABILITY
THAT
IT
RAINED
BE
AND
THE
PRIOR
PROBABILITY
THAT
THE
SPRINKLER
IN
TOM
YARD
WAS
ON
BE
WE
ALSO
HAVE
THE
FOLLOWING
CONDITIONAL
PROBABILITY
TABLES
R
J
T
F
R
T
T
T
T
F
F
T
F
F
COMPUTE
THE
PROBABILITY
THAT
THE
SPRINKLER
WAS
ON
GIVEN
THAT
TOM
GRASS
IS
WET
KEEP
IN
MIND
HOW
YOU
CAN
COMPUTE
THE
PROBABILITY
OF
INDIVIDUAL
VARIABLES
FROM
A
JOINT
PROBABILITY
HOW
YOU
EXPRESS
A
JOINT
PROBABILITY
FROM
A
GRAPHICAL
MODEL
AND
THE
AXIOMS
OF
PROBABILITY
ALSO
COMPUTE
THE
PROBABILITY
THAT
THE
SPRINKLER
WAS
ON
GIVEN
THAT
THE
GRASS
IN
BOTH
TOM
AND
JULIA
YARDS
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
IS
WET
HOW
DO
THESE
TWO
PROBABILITIES
COMPARE
WHAT
IS
THE
INTUITIVE
EXPLANATION
FOR
THEIR
DIFFERENCE
B
PTS
IN
THIS
EXERCISE
WE
LL
DO
SOME
CROSSDOMAIN
RECOMMENDATION
WHERE
WE
ASSUME
THAT
THERE
IS
A
CORRELATION
BETWEEN
A
USER
TASTE
IN
MUSIC
AND
FILM
WE
LL
ONLY
CONSIDER
ONE
MUSIC
GENRE
NAMELY
JAZZ
WHICH
WE
LL
DENOTE
BY
J
AND
FOUR
FILMS
WAKING
LIFE
DENOTED
BY
W
BORAT
DENOTED
BY
B
CINEMA
PARADISO
DENOTED
BY
C
AND
REQUIEM
FOR
A
DREAM
DENOTED
BY
R
WE
LL
ASSUME
THAT
CONDITIONED
ON
WHETHER
THE
USER
LIKES
JAZZ
THE
MOVIE
LIKES
DISLIKES
ARE
INDEPENDENT
THE
PRIOR
PROBABILITY
OF
LIKING
JAZZ
IS
WE
VE
DEFINED
THE
FOLLOWING
COMBINED
CONDITIONAL
PROBABILITY
TABLE
WHERE
MEANS
LIKES
J
W
B
C
R
T
F
WHAT
IS
THE
PROBABILITY
THE
USER
LIKES
JAZZ
GIVEN
THAT
SHE
LIKES
THE
FIRST
AND
FOURTH
MOVIES
BUT
DISLIKES
THE
SECOND
AND
THIRD
HOW
ABOUT
THE
PROBABILITY
THAT
THE
USER
LIKES
JAZZ
GIVEN
THAT
SHE
LIKES
ALL
THE
MOVIES
C
PTS
BISHOP
EXERCISE
PART
II
HIDDEN
MARKOV
MODEL
FOR
PARTOFSPEECH
TAGGING
POINTS
WHAT
YOU
NEED
WILL
BE
DISCUSSED
ON
MARCH
WE
LL
USE
THE
HMM
FROM
OUR
INCLASS
PARTOFSPEECH
TAGGING
EXAMPLE
WHOSE
STATES
ARE
PROPNOUN
NOUN
VERB
DET
WE
LL
DENOTE
PROPNOUN
AS
A
STATE
NOUN
AS
A
STATE
VERB
AS
A
STATE
AND
DET
AS
A
STATE
REMEMBER
THAT
WE
ALSO
HAVE
THE
START
STATE
AND
END
STATE
SF
THE
TRANSITION
PROBABILITIES
ARE
SIMILAR
TO
THOSE
GIVEN
IN
PAGE
OF
THIS
SLIDE
DECK
BUT
WITH
ONE
CORRECTION
THEY
CAN
BE
OBTAINED
IN
THIS
FILE
YOU
CAN
CHANGE
HOW
THE
PROBABILITIES
ARE
STORED
IF
YOU
PREFER
WE
LL
DEFINE
THE
OBSERVATION
PROBABILITIES
AS
FOLLOWS
THESE
ARE
ALSO
GIVEN
IN
THE
FILE
STATE
OBSERVATION
JOHN
MARY
CAT
SAW
ATE
A
THE
PROPNOUN
NOUN
VERB
DET
A
PTS
WRITE
CODE
TO
COMPUTE
THE
PROBABILITY
OF
OBSERVING
THE
FOLLOWING
SENTENCES
IN
TWO
WAYS
USING
THE
NAIVE
SOLUTION
AND
USING
THE
EFFICIENT
SOLUTION
YOU
CAN
MAP
EACH
WORD
TO
A
NUMBER
THAT
IS
ITS
INDEX
INTO
OUR
VOCABULARY
THE
UNION
OF
THE
COLUMN
HEADERS
EXCEPT
THE
FIRST
ONE
THEN
A
SENTENCE
IS
JUST
A
VECTOR
OF
NUMBERS
JOHN
SAW
THE
CAT
OR
USING
OUR
MAPPING
TO
NUMBERS
SENT
JOHN
ATE
JOHN
SAW
MARY
MARY
SAW
JOHN
CAT
SAW
THE
JOHN
JOHN
SAW
THE
SAW
JOHN
ATE
THE
CAT
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
INSTRUCTIONS
CREATE
TWO
FUNCTIONS
FUNCTION
PROB
A
B
N
M
SENT
AND
A
FUNCTION
WITH
THE
SAME
SIGNATURE
THE
INPUTS
ARE
DEFINED
IN
OUR
SLIDES
YOU
CAN
USE
THIS
CODE
TO
GET
COMBINATIONS
WITH
REPLACEMENT
TO
GET
YOUR
LIST
OF
POSSIBLE
STATE
SEQUENCES
ALSO
YOU
CAN
USE
FPRINTF
P
T
N
VOCAB
SENT
PROB
TO
PRINT
THE
PROBABILITY
OF
A
SENTENCE
WITH
DECIMAL
PLACES
IF
YOU
INCLUDE
THE
PROBABILITY
OF
TRANSFERING
TO
THE
END
STATE
IN
THE
EFFICIENT
SOLUTION
MAKE
SURE
YOU
ALSO
INCLUDE
IT
IN
THE
NAIVE
SOLUTION
INCLUDE
THE
PROBABILITIES
OF
THE
ABOVE
SENTENCES
FROM
BOTH
METHODS
AND
DISCUSS
SOME
GREATER
THAN
LESS
THAN
RELATIONS
BETWEEN
THE
RESULTING
PROBABILITIES
FOR
THE
DIFFERENT
SENTENCES
INCLUDE
ANY
SCRIPTS
YOU
USED
B
PTS
WRITE
OUT
THE
EFFICIENT
SOLUTION
BY
HAND
JUST
FOR
THE
SENTENCE
JOHN
ATE
AND
SHOW
YOUR
WORK
THEN
CHECK
YOUR
ANSWER
WITH
THE
ANSWER
YOU
GOT
FROM
YOUR
PROGRAM
PART
III
ADABOOST
POINTS
WHAT
YOU
NEED
WILL
BE
DISCUSSED
ON
MARCH
AND
IN
THIS
EXERCISE
YOU
LL
IMPLEMENT
THE
ADABOOST
METHOD
DEFINED
ON
PAGES
IN
BISHOP
SECTION
USE
DECISION
STUMPS
AS
YOUR
WEAK
CLASSIFIERS
FOR
EACH
DECISION
STUMP
USE
AT
LEAST
THRESHOLDS
USE
THE
PIMA
DATASET
FROM
AND
AND
THE
CODE
FOR
CROSSVALIDATION
AND
FEATURE
NORMALIZATION
FROM
SUBMIT
THE
FOLLOWING
CODE
YOU
CAN
ADD
INPUTS
OUTPUTS
TO
THE
SIGNATURES
FUNCTION
MISTAKES
WEIGHTS
WHERE
ARE
THE
LABELS
ON
THE
TEST
SET
OUTPUT
BY
YOUR
BEST
DECISION
STUMP
MISTAKES
IS
THE
INDICATOR
VALUE
VECTOR
USED
TO
COMPUTE
THE
QUANTITY
JM
FROM
BISHOP
EQ
AND
WEIGHTS
ARE
THE
WN
M
FUNCTION
ACCURACY
ADABOOST
M
WHERE
ACCURACY
ARE
THE
FINAL
ACCURACY
AND
LABELS
ON
THE
TEST
DATA
A
SCRIPT
M
TO
SET
UP
YOUR
TRAIN
TEST
SPLITS
RUN
YOUR
ADABOOST
FUNCTION
AND
EVALUATE
ACCURACY
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
MACHINE
LEARNING
ANNOUNCEMENTS
OVERVIEW
POLICIES
SCHEDULE
RESOURCES
MACHINE
LEARNING
SPRING
LOCATION
SENNOTT
SQUARE
TIME
MONDAY
AND
WEDNESDAY
INSTRUCTOR
ADRIANA
KOVASHKA
EMAIL
KOVASHKA
AT
CS
DOT
PITT
DOT
EDU
USE
AT
THE
BEGINNING
OF
THE
SUBJECT
LINE
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
MONDAY
AND
WEDNESDAY
TA
CHANGSHENG
LIU
EMAIL
CHANGSHENG
AT
CS
DOT
PITT
DOT
EDU
USE
AT
THE
BEGINNING
OF
THE
SUBJECT
LINE
TA
OFFICE
SENNOTT
SQUARE
TA
OFFICE
HOURS
WEDNESDAY
AND
THURSDAY
ANNOUNCEMENTS
THE
PROJECTS
GRADING
RUBRIC
HAS
BEEN
POSTED
THE
PRESENTATION
SCHEDULE
HAS
BEEN
POSTED
IS
OUT
AND
IS
DUE
TOP
OVERVIEW
COURSE
DESCRIPTION
THE
COURSE
WILL
COVER
THE
FOLLOWING
TOPICS
LEARNING
BASICS
UNSUPERVISED
LEARNING
SUPERVISED
LEARNING
CLASSIFICATION
REGRESSION
CLUSTERING
DIMENSIONALITY
REDUCTION
NEAREST
NEIGHBOR
CLASSIFICATION
SUPPORT
VECTOR
MACHINES
DENSITY
ESTIMATION
BAYESIAN
BELIEF
NETWORKS
HIDDEN
MARKOV
MODELS
EXPECTATION
MAXIMIZATION
DECISION
TREES
ENSEMBLES
DEEP
LEARNING
ACTIVE
AND
TRANSFER
LEARNING
AND
INFORMATION
RETRIEVAL
THE
COURSE
WILL
INCLUDE
MANY
EXAMPLES
OF
HOW
MACHINE
LEARNING
IS
USED
IN
COMPUTER
VISION
THE
HOMEWORK
ASSIGNMENTS
WILL
HAVE
SOME
BIAS
TOWARDS
APPLYING
MACHINE
LEARNING
TECHNIQUES
TO
COMPUTER
VISION
PROBLEMS
AND
DATASETS
THERE
WILL
BE
TWO
EXAMS
AND
A
FINAL
PROJECT
PREREQUISITES
KNOWLEDGE
OF
MATRICES
AND
LINEAR
ALGEBRA
CS
PROBABILITY
STAT
STATISTICS
STAT
PROGRAMMING
AND
ALGORITHM
DEVELOPMENT
AND
ANALYSIS
CS
OR
EQUIVALENT
OR
THE
PERMISSION
OF
THE
INSTRUCTOR
PROGRAMMING
HOMEWORK
ASSIGNMENTS
WILL
BE
WRITTEN
IN
MATLAB
THE
FINAL
PROJECT
CAN
BE
WRITTEN
IN
ANY
LANGUAGE
TEXTBOOK
CHRISTOPHER
M
BISHOP
PATTERN
RECOGNITION
AND
MACHINE
LEARNING
SPRINGER
BOOK
RESOURCES
TOP
POLICIES
GRADING
GRADING
WILL
BE
BASED
ON
THE
FOLLOWING
COMPONENTS
HOMEWORK
PROJECT
STATUS
PRESENTATION
AND
REPORT
FINAL
PRESENTATION
AND
REPORT
MIDTERM
EXAM
FINAL
EXAM
PARTICIPATION
HOMEWORK
THERE
WILL
BE
FOUR
HOMEWORK
ASSIGNMENTS
YOU
WILL
SUBMIT
YOUR
HOMEWORK
USING
COURSEWEB
NAVIGATE
TO
THE
COURSEWEB
PAGE
FOR
THEN
CLICK
ON
ASSIGNMENTS
ON
THE
LEFT
AND
THE
CORRESPONDING
HOMEWORK
NUMBER
ATTACH
IN
A
SINGLE
ZIP
FILE
YOUR
WRITTEN
RESPONSES
AND
CODE
NAME
THE
FILE
AS
ZIP
OR
TAR
HOMEWORK
IS
DUE
AT
ON
THE
DUE
DATE
PROJECT
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
STUDENTS
ARE
ENCOURAGED
TO
WORK
IN
GROUPS
OF
TWO
SEE
EXCEPTIONS
BELOW
FOR
THEIR
FINAL
PROJECT
A
PROJECT
CAN
BE
DESIGN
OF
A
NEW
METHOD
FOR
SOME
PROBLEM
WHICH
MAY
OR
MAY
NOT
EXTEND
EXISTING
METHODS
YOU
DON
T
HAVE
TO
INVENT
A
NEW
SVM
BUT
CAN
FOR
EXAMPLE
SHOW
AN
ALGORITHM
FOR
SOLVING
SOME
PROBLEM
THAT
INCORPORATES
OR
MODIFIES
SVMS
OR
OTHER
CLASSIFIERS
ORGANIZED
AND
INTERLEAVED
IN
SOME
FASHION
AN
IMPLEMENATION
OF
SOME
METHOD
DISCUSSED
IN
CLASS
OR
OTHER
METHOD
WITH
INSTRUCTOR
APPROVAL
NOTE
THAT
YOU
SHOULD
ALSO
DISCUSS
ALGORITHMIC
CHOICES
AND
TRADEOFFS
AN
APPLICATION
OF
TECHNIQUES
WE
STUDIED
IN
CLASS
OR
ANOTHER
METHOD
WITH
INSTRUCTOR
APPROVAL
TO
A
NEW
PROBLEM
THAT
WE
HAVE
NOT
DISCUSSED
IN
CLASS
NOTE
THAT
YOU
SHOULD
ALSO
DESCRIBE
WHY
THIS
PROBLEM
IS
IMPORTANT
AND
CHALLENGING
I
EXPECT
THIS
TO
BE
A
SIGNIFICANT
AMOUNT
OF
WORK
AND
NOT
JUST
A
STRAIGHTUP
RUN
OF
SOME
PACKAGE
ON
EXISTING
DATA
AND
YOU
ARE
ALLOWED
TO
USE
EXISTING
CODE
FOR
KNOWN
METHODS
AN
IMPLEMENTATION
OF
A
REAL
WORKING
SYSTEM
E
G
AN
APP
THAT
CAN
SOLVE
SOME
MACHINE
LEARNING
TASK
NOTE
YOU
SHOULD
ALSO
DISCUSS
DESIGN
CHALLENGES
AND
EVALUATE
YOUR
SYSTEM
QUANTITATIVELY
AND
QUALITATIVELY
WITH
REAL
USERS
EXPERIMENTAL
COMPARISON
OF
A
NUMBER
OF
EXISTING
TECHNIQUES
ON
A
KNOWN
PROBLEM
AND
DETAILED
DISCUSSION
AND
ANALYSIS
OF
THE
RESULTS
THIS
ONE
CAN
ONLY
BE
DONE
BY
STUDENTS
WORKING
INDIVIDUALLY
AN
EXTENSIVE
LITERATURE
REVIEW
AND
ANALYSIS
ON
ONE
OF
THE
TOPICS
COVERED
IN
CLASS
THIS
ONE
CAN
ONLY
BE
DONE
BY
STUDENTS
WORKING
INDIVIDUALLY
OTHER
SPEAK
WITH
THE
INSTRUCTOR
ALL
PROJECTS
SHOULD
INCLUDE
SOME
EXPERIMENTAL
RESULTS
THAT
VALIDATE
YOUR
METHOD
APPLICATION
ALSO
THINK
ABOUT
WHAT
DATA
AND
OR
CODE
YOU
WILL
USE
A
PROJECT
CAN
BECOME
A
SUBSEQUENT
CONFERENCE
PUBLICATION
IDEAS
FOR
COMPUTER
VISION
PROJECT
IDEAS
YOU
CAN
LOOK
AT
THE
LIST
OF
DATASETS
AND
TASKS
BELOW
FOR
INSPIRATION
OR
READ
SOME
PAPER
ABSTRACTS
ON
THIS
PAGE
FOR
NLP
PROJECT
IDEAS
SEE
THIS
PAGE
FROM
CHRISTOPHER
MANNING
ALSO
LOOK
AT
THE
FOLLOWING
LIST
OF
PROJECT
SUGGESTIONS
FROM
RAY
MOONEY
BUT
PLEASE
DO
NOT
CONTACT
ANY
OF
THE
CONTACTS
GIVEN
THIS
ONE
FROM
CARLOS
GUESTRIN
THIS
ONE
FROM
ANDREAS
KRAUSE
AND
THIS
ONE
FROM
ANDREW
NG
TIMELINE
AND
DELIVERABLES
YOU
WILL
SUBMIT
A
PROJECT
PROPOSAL
IN
FEBRUARY
AND
RECEIVE
FEEDBACK
FROM
THE
INSTRUCTOR
IN
THE
PROPOSAL
DESCRIBE
WHAT
TECHNIQUES
AND
DATA
YOU
PLAN
TO
USE
AND
WHAT
EXISTING
WORK
THERE
IS
ON
THE
SUBJECT
IN
LATE
MARCH
YOU
WILL
PRESENT
YOUR
PROGRESS
TO
YOUR
CLASSMATES
FOR
FEEDBACK
DESCRIBE
YOUR
PROGRESS
ON
THE
PROJECT
AND
ANY
PROBLEMS
ENCOUNTERED
ALONG
THE
WAY
AT
THE
END
OF
THE
SEMESTER
YOU
WILL
PRESENT
YOUR
FINAL
PROJECT
AND
SUBMIT
A
FINAL
PROJECT
REPORT
USING
THE
CVPR
LATEX
TEMPLATE
THE
FINAL
REPORT
SHOULD
RESEMBLE
A
CONFERENCE
PAPER
AND
SHOULD
INCLUDE
CLEAR
PROBLEM
DEFINITION
AND
ARGUMENTATION
OF
WHY
THIS
PROBLEM
IS
IMPORTANT
OVERVIEW
OF
RELATED
WORK
DETAILED
EXPLANATION
OF
THE
APPROACH
WELLMOTIVATED
EXPERIMENTAL
EVALUATION
INCLUDING
SETUP
DESCRIPTION
AND
A
DESCRIPTION
OF
WHAT
EACH
TEAM
MEMBER
DID
IN
THE
FINAL
PRESENTATION
DESCRIBE
YOUR
APPROACH
AND
EXPERIMENTAL
FINDINGS
IN
A
CLEAR
AND
ENGAGING
FASHION
PLEASE
LOOK
AT
THIS
PROJECT
GRADING
RUBRIC
ALL
PROJECT
WRITTEN
ITEMS
ARE
DUE
AT
ON
COURSEWEB
STATUS
REPORT
PERSENTATIONS
WILL
BE
MINUTES
LONG
AND
FINAL
PRESENTATIONS
WILL
BE
MINUTES
LONG
IF
YOU
DESCRIBED
SOMETHING
IN
THE
STATUS
REPORT
PRESENTATION
DON
T
REPEAT
IT
EXCEPT
MAYBE
WITH
ONE
SENTENCE
IN
THE
FINAL
PRESENTATION
EXAMS
THERE
WILL
BE
BOTH
A
MIDTERM
EXAM
AND
A
FINAL
EXAM
THE
LATTER
OF
WHICH
WILL
MOSTLY
FOCUS
ON
MATERIAL
FROM
THE
SECOND
PART
OF
THE
CLASS
BUT
WILL
BE
CUMULATIVE
PARTICIPATION
STUDENTS
ARE
EXPECTED
TO
REGULARLY
ATTEND
THE
CLASS
LECTURES
AND
SHOULD
ACTIVELY
ENGAGE
IN
INCLASS
DISCUSSIONS
YOUR
PARTICIPATION
GRADE
WILL
BE
BASED
ON
HOW
ACTIVELY
YOU
PARTICIPATED
IN
CLASS
YOU
CAN
ACTIVELY
PARTICIPATE
BY
FOR
EXAMPLE
RESPONDING
TO
THE
INSTRUCTOR
OR
OTHERS
QUESTIONS
ASKING
QUESTIONS
OR
MAKING
MEANINGFUL
REMARKS
AND
COMMENTS
ABOUT
THE
LECTURE
OR
POSTING
QUESTIONS
OR
RESPONSES
ON
PIAZZA
YOU
ARE
ALSO
ENCOURAGED
TO
BRING
IN
RELEVANT
ARTICLES
YOU
SAW
IN
THE
NEWS
LATE
POLICY
YOU
GET
FREE
LATE
DAYS
I
E
YOU
CAN
SUBMIT
HOMEWORK
A
TOTAL
OF
DAYS
LATE
FOR
EXAMPLE
YOU
CAN
SUBMIT
ONE
PROBLEM
SET
HOURS
LATE
AND
ANOTHER
HOURS
LATE
ONCE
YOU
VE
USED
UP
YOUR
FREE
LATE
DAYS
YOU
WILL
INCUR
A
PENALTY
OF
FROM
THE
TOTAL
PROJECT
CREDIT
POSSIBLE
FOR
EACH
LATE
DAY
A
LATE
DAY
IS
ANYTHING
FROM
MINUTE
TO
HOURS
COLLABORATION
POLICY
AND
ACADEMIC
HONESTY
YOU
WILL
DO
YOUR
WORK
EXAMS
AND
HOMEWORK
INDIVIDUALLY
THE
WORK
YOU
TURN
IN
MUST
BE
YOUR
OWN
WORK
YOU
ARE
ALLOWED
TO
DISCUSS
THE
PROBLEM
SETS
WITH
YOUR
CLASSMATES
BUT
DO
NOT
LOOK
AT
CODE
THEY
MIGHT
HAVE
WRITTEN
FOR
THE
PROBLEM
SETS
YOU
ARE
ALSO
NOT
ALLOWED
TO
SEARCH
FOR
CODE
ON
THE
INTERNET
USE
SOLUTIONS
POSTED
ONLINE
UNLESS
YOU
ARE
EXPLICITLY
ALLOWED
TO
LOOK
AT
THOSE
OR
TO
USE
MATLAB
IMPLEMENTATION
IF
YOU
ARE
ASKED
TO
WRITE
YOUR
OWN
CODE
WHEN
IN
DOUBT
ABOUT
WHAT
YOU
CAN
OR
CANNOT
USE
ASK
THE
INSTRUCTOR
PLAGIARISM
WILL
CAUSE
YOU
TO
FAIL
THE
CLASS
AND
RECEIVE
DISCIPLINARY
PENALTY
PLEASE
CONSULT
THE
UNIVERSITY
GUIDELINES
ON
ACADEMIC
INTEGRITY
NOTE
ON
DISABILITIES
IF
YOU
HAVE
A
DISABILITY
FOR
WHICH
YOU
ARE
OR
MAY
BE
REQUESTING
AN
ACCOMMODATION
YOU
ARE
ENCOURAGED
TO
CONTACT
BOTH
YOUR
INSTRUCTOR
AND
DISABILITY
RESOURCES
AND
SERVICES
DRS
WILLIAM
PITT
UNION
DRSRECEP
PITT
EDU
FOR
ASL
USERS
AS
EARLY
AS
POSSIBLE
IN
THE
TERM
DRS
WILL
VERIFY
YOUR
DISABILITY
AND
DETERMINE
REASONABLE
ACCOMMODATIONS
FOR
THIS
COURSE
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
NOTE
ON
MEDICAL
CONDITIONS
IF
YOU
HAVE
A
MEDICAL
CONDITION
WHICH
WILL
PREVENT
YOU
FROM
DOING
A
CERTAIN
ASSIGNMENT
OR
COMING
TO
CLASS
YOU
MUST
INFORM
THE
INSTRUCTOR
OF
THIS
BEFORE
THE
DEADLINE
YOU
MUST
THEN
SUBMIT
DOCUMENTATION
OF
YOUR
CONDITION
WITHIN
A
WEEK
OF
THE
ASSIGNMENT
DEADLINE
STATEMENT
ON
CLASSROOM
RECORDING
TO
ENSURE
THE
FREE
AND
OPEN
DISCUSSION
OF
IDEAS
STUDENTS
MAY
NOT
RECORD
CLASSROOM
LECTURES
DISCUSSION
AND
OR
ACTIVITIES
WITHOUT
THE
ADVANCE
WRITTEN
PERMISSION
OF
THE
INSTRUCTOR
AND
ANY
SUCH
RECORDING
PROPERLY
APPROVED
IN
ADVANCE
CAN
BE
USED
SOLELY
FOR
THE
STUDENT
OWN
PRIVATE
USE
TOP
SCHEDULE
DATE
CHAPTER
TOPIC
READINGS
LECTURE
SLIDES
DUE
BASICS
INTRODUCTION
AND
ADMINISTRATIVIA
THIS
WEBSITE
PPTX
PDF
OUT
MATLAB
ML
TASKS
NOTATION
AND
CHALLENGES
BISHOP
CH
SEC
PPTX
PDF
MORE
MATLAB
BIASVARIANCE
TRADEOFF
PPTX
PDF
NO
CLASS
MLK
DAY
BIASVARIANCE
TRADEOFF
CONT
D
DATA
REPRESENTATIONS
SZELISKI
SEC
GRAUMAN
LEIBE
CH
PPTX
PDF
UNSUPERVISED
LEARNING
CLUSTERING
BISHOP
CH
PPTX
PDF
DIMENSIONALITY
REDUCTION
BISHOP
SEC
DAUME
PPTX
PDF
SUPERVISED
LEARNING
INTRO
AND
LINEAR
MODELS
NEAREST
NEIGHBORS
BISHOP
SEC
PPTX
PDF
NOTES
DUE
OUT
NEAREST
NEIGHBORS
CONT
D
LINEAR
ALGEBRA
REVIEW
PPTX
PDF
REVIEW
CONT
D
BISHOP
SEC
PPTX
PDF
LINEAR
REGRESSION
BISHOP
SEC
PPTX
PDF
LINEAR
MODELS
FOR
CLASSIFICATION
BISHOP
SEC
PPTX
PDF
NOTES
LINEAR
MODELS
FOR
CLASSIFICATION
CONT
D
SUPPORT
VECTOR
MACHINES
BISHOP
SEC
PPTX
PDF
NOTES
PROPOSAL
DUE
SUPPORT
VECTOR
MACHINES
CONT
D
SUPPORT
VECTOR
MACHINES
OPTIMIZATION
SOLUTION
NOTES
CLASSIFICATION
PROBABILISTIC
MODELS
PROBABILITY
REVIEW
DENSITY
ESTIMATION
BISHOP
SEC
PPTX
PDF
PPTX
PDF
DUE
OUT
MIDTERM
EXAM
NO
CLASS
SPRING
BREAK
BAYESIAN
BELIEF
NETWORKS
BISHOP
SEC
PPTX
PDF
BAYESIAN
BELIEF
NETWORKS
CONT
D
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
MARKOV
RANDOM
FIELDS
HIDDEN
MARKOV
MODELS
BISHOP
SEC
SKIP
EXTRA
JURAFSKY
MARTIN
PPT
PDF
HIDDEN
MARKOV
MODELS
CONT
D
PPTX
PDF
DUE
OUT
HIDDEN
MARKOV
MODELS
CONT
D
PPTX
PDF
EXPECTATION
MAXIMIZATION
BISHOP
SEC
PPTX
PDF
REVIEW
CHANGSHENG
PPTX
PDF
PROJECT
STATUS
REPORT
PRESENTATIONS
STATUS
REPORT
DUE
CLASSIFICATION
OTHER
TOPICS
ENSEMBLES
BAGGING
AND
BOOSTING
DECISION
TREES
BISHOP
SEC
SKIP
PPTX
PDF
NEURAL
NETWORKS
BISHOP
SEC
SKIP
PPTX
PDF
NEURAL
NETWORKS
CONT
D
ACTIVE
LEARNING
AND
CROWDSOURCING
PPTX
PDF
DUE
FINAL
EXAM
PROJECT
PRESENTATIONS
FINAL
REPORT
DUE
FRIDAY
TOP
RESOURCES
THIS
COURSE
WAS
INSPIRED
BY
THE
FOLLOWING
COURSES
MACHINE
LEARNING
BY
MILOS
HAUSKRECHT
UNIVERSITY
OF
PITTSBURGH
SPRING
INTRODUCTION
TO
MACHINE
LEARNING
BY
DHRUV
BATRA
VIRGINIA
TECH
SPRING
MACHINE
LEARNING
BY
TOMMI
JAAKKOLA
MIT
MACHINE
LEARNING
BY
SUBHRANSU
MAJI
UMASS
AMHREST
SPRING
MACHINE
LEARNING
BY
ERIK
SUDDERTH
BROWN
UNIVERSITY
FALL
COMPUTER
VISION
BY
KRISTEN
GRAUMAN
UT
AUSTIN
SPRING
COMPUTER
VISION
BY
DEREK
HOIEM
UIUC
SPRING
NATURAL
LANGUAGE
PROCESSING
BY
RAY
MOONEY
UT
AUSTIN
TUTORIALS
MATLAB
TUTORIAL
LINEAR
ALGEBRA
REVIEW
BY
FEIFEI
LI
BRIEF
MACHINE
LEARNING
INTRO
BY
ADITYA
KHOSLA
AND
JOSEPH
LIM
RESOURCES
LIST
COMPILED
BY
DEVI
PARIKH
SOME
COMPUTER
VISION
DATASETS
AND
TASKS
MICROSOFT
COCO
COMMON
OBJECTS
IN
CONTEXT
OBJECT
RECOGNITION
SEGMENTATION
IMAGE
DESCRIPTION
IMAGENET
OBJECT
RECOGNITION
SUN
DATABASE
SCENES
CALTECHUCSD
BIRDS
FINEGRAINED
OBJECT
RECOGNITION
MSRC
ANNOTATIONS
ACTIVE
LEARNING
ANIMALS
WITH
ATTRIBUTES
ATTRIBUTEBASED
RECOGNITION
APASCAL
AYAHOO
ATTRIBUTEBASED
RECOGNITION
SHOES
ATTRIBUTEBASED
SEARCH
INRIA
MOVIE
ACTIONS
ACTION
RECOGNITION
ADL
EGOCENTRIC
ACTION
RECOGNITION
ACTION
QUALITY
EVALUATING
ACTION
QUALITY
CARDB
HISTORICAL
CARS
STYLE
CLASSIFICATION
OF
CARS
RECOGNIZING
IMAGE
STYLE
PHOTOGRAPHIC
STYLE
CLASSIFICATION
JUDD
GAZE
VISUAL
SALIENCY
PREDICTION
VISUAL
PERSUASION
PREDICTING
SUBTLE
MESSAGES
IN
IMAGES
VQA
VISUAL
QUESTIONANSWERING
RECOGNITION
DATASETS
LIST
COMPILED
BY
KRISTEN
GRAUMAN
HUMAN
ACTIVITY
DATASETS
LIST
COMPILED
BY
CHAOYEH
CHEN
SOME
CODE
OF
INTEREST
LIBSVM
BY
CHIHCHUNG
CHANG
AND
CHIHJEN
LIN
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
SVM
LIGHT
BY
THORSTEN
JOACHIMS
VLFEAT
FEATURE
EXTRACTION
TUTORIALS
AND
MORE
BY
ANDREA
VEDALDI
GIST
FEATURE
EXTRACTION
BY
AUDE
OLIVA
AND
ANTONIO
TORRALBA
CAFFE
DEEP
LEARNING
CODE
BY
YANGQING
JIA
ET
AL
TOP
NOTES
ON
K
NEAREST
NEIGHBORS
FEBRUARY
FORMAL
DEFINITION
LET
X
BE
OUR
TEST
DATA
POINT
AND
NK
X
BE
THE
INDICES
OF
THE
K
NEAREST
NEIGHBORS
OF
X
IN
RD
CLASSIFICATION
IN
CLASSIFICATION
WE
ARE
TRYING
TO
PREDICT
SOME
DISCRETE
TARGET
LABEL
E
G
FOR
SPAM
NO
SPAM
OR
IN
MULTI
CLASS
PROBLEMS
SOME
CLASS
ID
E
G
THIS
MOVIE
IS
A
DRAMA
I
E
CLASS
ID
ACTION
I
E
CLASS
ID
INDIE
I
E
CLASS
ID
ETC
THEN
Y
ARGMAXC
YI
C
WHERE
C
IS
THE
CLASS
ID
THIS
CAN
ALSO
BE
WRITTEN
AS
Y
ARGMAXC
I
NK
X
I
YI
C
REGRESSION
IN
REGRESSION
WE
WANT
TO
PREDICT
SOME
CONTINUOUS
LABEL
E
G
PRICE
OF
A
HOUSE
BASED
ON
ITS
SIZE
YEAR
OF
CONSTRUCTION
ETC
THEN
Y
I
NK
X
YI
DISTANCE
METRICS
EUCLIDEAN
D
D
MAHALANOBIS
WE
WANT
TO
WEIGH
DIFFERENT
DIMENSIONS
DIFFERENTLY
E
G
BECAUSE
IN
SOME
DIMEN
SIONS
POINTS
ARE
NATURALLY
SPREAD
OUT
SO
DISTANCES
ARE
ON
AVERAGE
LARGED
THEN
WE
CAN
DEFINE
A
DISTANCE
AS
D
X
Z
D
I
TH
DIMENSION
XI
ZI
WHERE
Σ
I
IS
THE
VARIANCE
IN
THE
MORE
GENERALLY
WE
CAN
WRITE
D
X
Z
X
Z
TA
X
Z
WHERE
A
IS
A
DIAG
ONAL
MATRIX
WITH
THE
ΣI
ALONG
THE
DIAGONAL
THIS
IS
AN
EXAMPLE
OF
A
MAHALANOBIS
DISTANCE
FOR
A
DISTANCE
OF
THIS
FORM
TO
BE
A
MAHALANOBIS
DISTANCE
A
HAS
TO
BE
POSITIVE
SEMI
DEFININE
I
E
IT
HAS
TO
BE
A
SYMMETRIC
MATRIX
FOR
WHICH
XTAX
FOR
ALL
X
RD
MINKOWSKI
WE
DEFINE
D
X
Z
D
XI
ZI
P
P
NOTE
THAT
IF
P
THIS
BECOMES
THE
MANHATTAN
DISTANCE
AND
IF
P
IT
IS
THE
EUCLIDEAN
DISTANCE
WEIGHTED
K
NN
FOR
THIS
GENERALIZATION
WE
WILL
USE
ALL
N
TRAINING
POINTS
I
E
K
N
LET
WI
BE
THE
WEIGHT
OF
THE
I
TH
INSTANCE
NOTE
THAT
THIS
WI
DEPENDS
ON
THE
PARTICULAR
QUERY
SAMPLE
X
ONE
CHOICE
OF
A
WEIGHTING
FUNCTION
IS
A
GAUSSIAN
KERNEL
WI
X
XI
E
WE
NEED
A
MINUS
SIGN
BECAUSE
WE
WANT
A
POINT
CLOSE
TO
X
I
E
A
POINT
WITH
A
SMALL
DISTANCE
TO
X
TO
GET
A
HIGH
WEIGHT
NOTE
THAT
Σ
IS
THE
BANDWIDTH
PARAMETER
AND
IT
EXPRESSES
HOW
QUICKLY
OUR
WEIGHT
FUNCTION
DROPS
OFF
AS
POINTS
BECOMES
FURTHER
AND
FURTHER
FROM
THE
QUERY
X
CLASSIFICATION
THE
WEIGHTED
PREDICTION
BECOMES
Y
ARGMAXC
N
I
WII
YI
C
REGRESSION
THE
WEIGHTED
AVERAGE
BECOMES
Y
N
WIYI
WI
VOLUME
OF
A
SHELL
WITH
WIDTH
E
TO
BE
DISCUSSED
ON
WEDNESDAY
THE
VOLUME
OF
A
SPHERE
WITH
RADIUS
R
IN
D
DIMENSIONS
IS
KDRD
SEE
BISHOP
SECTION
SAY
WE
HAVE
A
SPHERE
WITH
RADIUS
AND
A
THIN
SHELL
OF
THE
SPHERE
WITH
THICKNESS
E
THEN
V
SHELL
KD
D
KD
E
D
KD
D
E
D
THIS
TENDS
TO
AS
D
TENDS
TO
INFINITY
THUS
MOST
OF
THE
VOLUME
OF
THE
SPHERE
IS
IN
THAT
THIN
OUTER
SHELL
WHICH
IS
COUNTER
INTUITIVE
ACKNOWLEDGEMENT
THESE
NOTES
ARE
BASED
ON
DHRUV
BATRA
NOTES
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
FEBRUARY
SOLUTION
VIA
LEAST
SQUARES
SOLUTION
VIA
GRADIENT
DESCENT
REGULARIZED
LEAST
SQUARES
STATISTICAL
VIEW
OF
REGRESSION
DEALING
WITH
OUTLIERS
SOMETIMES
WANT
TO
ADD
A
BIAS
TERM
CAN
ADD
AS
SUCH
THAT
X
XD
FIGURE
FROM
MILOS
HAUSKRECHT
F
X
T
AT
TRAINING
TIME
USE
GIVEN
XN
TN
TO
ESTIMATE
MAPPING
FUNCTION
F
OBJECTIVE
MINIMIZE
F
XI
TI
FOR
ALL
I
N
XI
ARE
THE
INPUT
FEATURES
D
DIMENSIONAL
TI
IS
THE
TARGET
OUTPUT
LABEL
GIVEN
BY
HUMAN
ORACLE
AT
TEST
TIME
USE
F
TO
MAKE
PREDICTION
FOR
SOME
NEW
XTEST
PROBLEM
IS
CALLED
CLASSIFICATION
IF
DOES
YOUR
PATIENT
HAVE
CANCER
SHOULD
YOUR
BANK
GIVE
THIS
PERSON
A
CREDIT
CARD
IS
IT
GOING
TO
RAIN
TOMORROW
WHAT
ANIMAL
IS
IN
THIS
IMAGE
PROBLEM
IS
CALLED
REGRESSION
IF
HOW
MUCH
SHOULD
YOU
ASK
FOR
THIS
HOUSE
WHAT
IS
THE
TEMPERATURE
GOING
TO
BE
TOMORROW
WHAT
SCORE
SHOULD
YOUR
SYSTEM
GIVE
TO
THIS
PERSON
FIGURE
SKATING
PERFORMANCE
Y
AND
T
WILL
BE
USED
INTERCHANGEABLY
REGRESSION
THE
GOAL
IS
TO
MAKE
QUANTITATIVE
REAL
VALUED
PREDICTIONS
ON
THE
BASIS
OF
A
VECTOR
OF
FEATURES
OR
ATTRIBUTES
EXAMPLE
PREDICTING
VEHICLE
FUEL
EFFICIENCY
MPG
FROM
ATTRIBUTES
WE
NEED
TO
SPECIFY
THE
CLASS
OF
FUNCTIONS
E
G
LINEAR
SELECT
HOW
TO
MEASURE
PRED
ICT
IO
N
LOSS
SOLVE
THE
RESULTING
MINIMIZATION
PROBLEM
LINEAR
REGRESSION
X
WE
BEGIN
BY
CONSIDERING
LINEAR
REGRESSION
EASY
TO
EXTEND
TO
MORE
COM
PLEX
PREDICT
IONS
LAT
ER
ON
F
R
R
F
A
R
F
X
W
WO
W
F
X
W
WO
W
X
WDXD
WHERE
W
WO
W
W
D
T
ARE
PARAMETERS
WE
NEED
TO
SET
FIT
LINE
TO
POINTS
USE
PARAMETERS
OF
LINE
TO
PREDICT
THE
Y
COORDINATE
OF
A
NEW
DATA
POINT
XNEW
FIND
PARAMETERS
OF
PLANE
LINEAR
REGRESSION
SQUARED
LOSS
X
F
X
W
WO
F
X
W
WO
W
X
WDXD
WE
CAN
MEASURE
THE
PREDICTION
LOSS
IN
TERMS
OF
SQUARED
ERROR
LOSS
Y
Y
Y
Y
SO
THAT
THE
EMPIRICAL
LOSS
ON
N
TRAINING
SAMPLES
BECOMES
MEAN
SQUARED
ERROR
L
N
W
I
L
YI
J
XI
W
TOMMI
JAAKKOLA
MIT
CSAIL
PLAN
FOR
TODAY
LINEAR
REGRESSION
DEFINITION
REGULARIZED
LEAST
SQUARES
STATISTICAL
VIEW
OF
REGRESSION
DEALING
WITH
OUTLIERS
LINEAR
REGRESSION
ESTIMATION
WE
HAVE
TO
MINIMIZE
THE
EMPIRICAL
SQUARED
LOSS
OPTIMALITY
CONDITIONS
DERIVATION
A
A
L
N
JN
W
YI
W
I
W
W
N
I
L
NOW
USING
D
DIMENSIONS
J
A
SYSTEM
OF
EQUATIONS
IN
W
USING
D
L
DIMENSIONS
LINEAR
REGRESSION
MATRIX
NOTATION
WE
CAN
EXPRESS
THE
SOLUTION
A
BIT
MORE
GENERALLY
BY
RESORTING
TO
A
MATRIX
NOTATION
Y
YN
SO
THAT
X
W
N
N
YN
N
LLY
N
INEAR
REGRESSION
SO
UTION
BY
SETTING
THE
DERIVATIVES
OF
IIY
N
TO
ZERO
WE
GET
THE
SAME
OPTIMALITY
CONDITIONS
AS
BEFORE
NOW
EXPRESSED
IN
A
MATRIX
FORM
T
N
L
Y
XWLL
W
N
Y
XW
Y
XW
WHICH
GIVES
THE
SOLUTION
IS
A
LINEAR
FUNCTION
OF
THE
OUTPUTS
Y
NOW
USING
D
DIMENSIONS
ID
E
IVA
T
IVE
OF
LOSS
L
W
Y
XW
T
Y
XW
L
W
YT
WT
XT
Y
X
W
ID
E
IRI
V
A
T
I
V
E
OF
LOSS
L
W
Y
XW
T
Y
XW
L
W
YT
W
T
XT
Y
X
W
W
T
T
T
T
T
T
W
J
T
W
Y
Y
W
X
Y
Y
XW
W
X
XW
L
LL
E
AST
AIRE
SOLL
U
T
I
O
A
L
W
X
T
Y
X
T
X
W
CHALLENGES
COMPUTING
THE
PSEUDOINVERSE
MIGHT
BE
SLOW
FOR
LARGE
MATRICES
CUBIC
IN
NUMBER
OF
FEATURES
D
DUE
TO
PSEUDOINVERSE
LINEAR
IN
NUMBER
OF
SAMPLES
N
WE
MIGHT
WANT
TO
ADJUST
SOLUTION
AS
NEW
EXAMPLES
COME
IN
WITHOUT
RECOMPUTING
THE
PSEUDOINVERSE
FOR
EACH
NEW
SAMPLE
THAT
COMES
IN
ANOTHER
SOLUTION
GRADIENT
DESCENT
COST
LINEAR
IN
BOTH
D
AND
N
IF
D
USE
GRADIENT
DESCENT
THE
SAME
THING
BUT
DO
IT
FOR
EACH
TRAINING
SAMPLE
SEPARATELY
RATHER
THAN
AS
A
BATCH
A
K
A
SEQUENTIAL
A
K
A
ONLINE
W
Τ
W
Τ
Α
JNI
W
Τ
Α
YI
W
Τ
TXI
XI
GLOBAL
VS
LOCAL
MINIMA
WHAT
HAPPENS
WHEN
WE
GET
INTO
A
LOCAL
MINIMUM
LINEAR
REGRESSION
DEFINITION
SOLUTION
VIA
LEAST
SQUARES
SOLUTION
VIA
GRADIENT
DESCENT
DEALING
WITH
OUTLIERS
EXAMPLE
POLYNOMIAL
CURVE
FITTING
M
Y
X
W
W
J
X
W
J
X
J
O
WHERE
Q
X
ARE
KNOWN
AS
BASIS
FUNCTIONS
TYPICA
LY
X
SO
THAT
ACTS
AS
A
BIAS
LEST
WE
F
UNC
TIONS
J
X
XD
CONSIDER
THE
ERROR
FUNCTION
DATA
TERM
REGULARIZATION
TERM
WITH
THE
SUM
OF
SQUARES
ERROR
FUNCTION
AND
A
QUADRATIC
REGULARIZER
WE
GET
WHICH
IS
MINIMIZED
BY
WITH
A
MORE
GENERAL
REGULARIZER
WE
HAVE
ISOSURFACES
W
P
CONSTANT
LASSO
QUADRATIC
LASSO
TENDS
TO
GENERATE
SPARSER
SOLUTIONS
THAN
A
QUADRATIC
REGULARIZER
STATISTICAL
VIEW
OF
REGRESSION
THE
MEAN
SQUARED
PREDICTION
ERROR
SETTING
IS
EQUIVALENT
TO
THE
MAXIMUM
LIKELIHOOD
ESTIMATION
SETTING
SEE
HIDDEN
SLIDES
FOR
MORE
PLAN
FOR
TODAY
LINEAR
REGRESSION
DEFINITION
SOLUTION
VIA
LEAST
SQUARES
SOLUTION
VIA
GRADIENT
DESCENT
REGULARIZED
LEAST
SQUARES
STATISTICAL
VIEW
OF
REGRESSION
HYPOTHESIZE
AND
TEST
TRY
ALL
POSSIBLE
PARAMETER
COMBINATIONS
REPEATEDLY
SAMPLE
ENOUGH
POINTS
TO
SOLVE
FOR
PARAMETERS
EACH
POINT
VOTES
FOR
ALL
CONSISTENT
PARAMETERS
E
G
EACH
POINT
VOTES
FOR
ALL
POSSIBLE
LINES
ON
WHICH
IT
MIGHT
LIE
SCORE
THE
GIVEN
PARAMETERS
NUMBER
OF
CONSISTENT
POINTS
CHOOSE
FROM
AMONG
THE
SET
OF
PARAMETERS
NOISE
CLUTTER
FEATURES
THEY
WILL
CAST
VOTES
TOO
BUT
TYPICALLY
THEIR
VOTES
SHOULD
BE
INCONSISTENT
WITH
THE
MAJORITY
OF
GOOD
FEATURES
TWO
METHODS
HOUGH
TRANSFORM
AND
RANSAC
ADAPTED
FROM
DEREK
HOIEM
AND
KRISTEN
GRAUMAN
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
WHAT
DOES
A
POINT
IN
THE
IMAGE
SPACE
MAP
TO
ANSWER
THE
SOLUTIONS
OF
B
THIS
IS
A
LINE
IN
HOUGH
SPACE
TO
GO
FROM
IMAGE
SPACE
TO
HOUGH
SPACE
GIVEN
A
SET
OF
POINTS
X
Y
FIND
ALL
M
B
SUCH
THAT
Y
MX
B
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
HOW
CAN
WE
USE
THIS
TO
FIND
THE
MOST
LIKELY
PARAMETERS
M
B
FOR
THE
MOST
PROMINENT
LINE
IN
THE
IMAGE
SPACE
LET
EACH
EDGE
POINT
IN
IMAGE
SPACE
VOTE
FOR
A
SET
OF
POSSIBLE
PARAMETERS
IN
HOUGH
SPACE
ACCUMULATE
VOTES
IN
DISCRETE
SET
OF
BINS
PARAMETERS
WITH
THE
MOST
VOTES
INDICATE
LINE
IN
IMAGE
SPACE
RANSAC
RANDOM
SAMPLE
CONSENSUS
APPROACH
WE
WANT
TO
AVOID
THE
IMPACT
OF
OUTLIERS
SO
LET
LOOK
FOR
INLIERS
AND
USE
THOSE
ONLY
INTUITION
IF
AN
OUTLIER
IS
CHOSEN
TO
COMPUTE
THE
CURRENT
FIT
THEN
THE
RESULTING
LINE
WON
T
HAVE
MUCH
SUPPORT
FROM
REST
OF
THE
POINTS
RANSAC
GENERAL
FORM
RANSAC
LOOP
RANDOMLY
SELECT
A
SEED
GROUP
OF
POINTS
ON
WHICH
TO
BASE
MODEL
ESTIMATE
FIT
MODEL
TO
THESE
POINTS
FIND
INLIERS
TO
THIS
MODEL
I
E
POINTS
WHOSE
DISTANCE
FROM
THE
LINE
IS
LESS
THAN
T
IF
THERE
ARE
D
OR
MORE
INLIERS
RE
COMPUTE
ESTIMATE
OF
MODEL
ON
ALL
OF
THE
INLIERS
REPEAT
N
TIMES
KEEP
THE
MODEL
WITH
THE
LARGEST
NUMBER
OF
INLIERS
FISCHLER
BOLLES
IN
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
RANSAC
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
N
I
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
ALGORITHM
N
I
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
FEBRUARY
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
PERCEPTRON
LOGISTIC
REGRESSION
MULTI
WAY
CLASSIFICATION
GENERATIVE
VS
DISCRIMINATIVE
MODELS
CLASSIFICATION
EXAMPLE
DIGIT
RECOGNITION
BINARY
DIGITS
BINARY
DIGIT
ACTUAL
LABEL
TARGET
LABEL
IN
LEARNING
II
CLASSIFICATION
VIA
REGRESSION
SUPPOSE
WE
IGNORE
THE
FACT
THAT
THE
TARGET
OUTPUT
Y
IS
BINARY
E
G
RATHER
THAN
A
CONTINUOUS
VARIABLE
SO
WE
WILL
ESTIMATE
A
LINEAR
REGRESSION
FUNCTION
J
X
W
WQ
WDXD
WO
XT
W
BASED
ON
THE
AVAILABLE
DATA
AS
BEFORE
ASSUMING
Y
F
X
W
E
E
RV
N
O
A
THEN
THE
ML
OBJECTIVE
FOR
THE
PARAMETERS
W
REDUCES
TO
LEAST
SQUARES
FITTING
WE
CAN
USE
THE
RESULTING
REGRESSION
FUNCTION
TO
CLASSIFY
ANY
NEW
TEST
EXAMPLE
X
ACCORDING
TO
LABEL
IF
F
X
W
AND
LABEL
OTHERWISE
F
X
W
THEREFORE
DEFINES
A
LINEAR
DECISION
BOUNDARY
THAT
PARTITIONS
THE
INPUT
SPACE
INTO
TWO
CLASS
SPECIFIC
REGIONS
HALF
SPACES
GIVEN
THE
DISSOCIATION
BETWEEN
THE
OBJECTIVE
CLASSIFICATION
AND
THE
ESTIMATION
CRITERION
REGRESSION
IT
IS
NOT
CLEAR
THAT
THIS
APPROACH
LEADS
TO
SENS
IBLE
RESULTS
FN
SOMETIMES
GOOD
SOMETIMES
BAD
THE
EFFECT
OF
OUTLIERS
MAGENTA
LEAST
SQUARES
GREEN
LOGISTIC
REGRESSION
WITH
THREE
CLASSES
LEFT
LEAST
SQUARES
RIGHT
LOGISTIC
REGRESSION
PROJECTED
POINT
IN
R
Z
XI
W
W
WE
CAN
STUDY
HOW
WELL
THE
PROJECTED
POINTS
ZN
VIEWED
AS
FUNCTIONS
OF
W
ARE
SEPARATED
ACROSS
THE
CLASSES
WE
CAN
STUDY
HOW
WELL
THE
PROJECTED
POINTS
ZN
VIEWED
AS
FUNCTIONS
OF
ARE
SEPARATED
ACROSS
THE
CLASSES
BY
VARYING
W
WE
GET
DIFFERENT
LEVELS
OF
SEPARATION
BETWEEN
THE
PROJECTED
POINTS
WE
WOULD
LIKE
TO
FIND
THAT
SOMEHOW
MAXIMIZES
THE
SEPARATION
OF
THE
PROJECTED
POINTS
ACROSS
CLASSES
WE
CAN
QUANTIFY
THE
SEPARATION
OVERLAP
IN
TERMS
OF
MEANS
AND
VARIANCES
OF
THE
RESULTING
DIMENSIONAL
CLASS
DISTRIBUTIONS
R
CLASS
DESCRIPTIONS
IN
RD
CLASS
N
SAMPLES
MEAN
COVARIANCE
O
CLASS
N
SAMPLES
MEAN
COVARIANCE
PROJECTED
CLASS
DESCRIPTIONS
IN
R
CLASS
N
SAMPLES
MEAN
VARIANCE
W
F
CLASS
N
SAMPLES
MEAN
ΜF
W
VARIANCE
W
F
W
ESTIMATION
CRITERION
WE
FIND
THAT
MAXIMIZES
SEPARATION
OF
PROJECTED
MEANS
J
FI
SH
ER
W
SUM
OF
WITHIN
CLASS
VARIANCES
ΜF
SOLUTION
CLASS
SEPARATION
FISHER
LINEAR
DISCRIMINANT
FIGURES
FROM
BISHOP
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
LOGISTIC
REGRESSION
MULTI
WAY
CLASSIFICATION
GENERATIVE
VS
DISCRIMINATIVE
MODELS
ROSENBLATT
PREDICTION
RULE
WHERE
LOSS
JUST
USING
THE
MISCLASSIFIED
EXAMPLES
LOSS
LEARNING
ALGORITHM
UPDATE
RULE
INTERPRETATION
IF
SAMPLE
IS
BEING
MISCLASSIFIED
MAKE
THE
WEIGHT
VECTOR
MORE
LIKE
IT
FIGURES
FROM
BISHOP
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
PERCEPTRON
MULTI
WAY
CLASSIFICATION
GENERATIVE
VS
DISCRIMINATIVE
MODELS
SUPPOSE
WE
KNOW
THE
CLASS
CONDITIONAL
DENSIT
IES
P
XLY
FOR
Y
L
AS
WELL
AS
THE
OVERALL
CLASS
FREQUENCIES
P
Y
HOW
DO
WE
DECIDE
WHICH
CLASS
A
NEW
EXAMPLE
X
BELONGS
TO
SO
AS
TO
MINIMIZE
THE
OVERALL
PROBABILITY
OF
ERROR
SUPPOSE
WE
KNOW
THE
CLASS
CONDITIONAL
DENSITIES
P
XLY
FOR
Y
L
AS
WELL
AS
THE
OVERALL
CLASS
FREQUENCIES
P
Y
HOW
DO
WE
DECIDE
WHICH
CLASS
A
NEW
EXAMPLE
X
BELONGS
TO
SO
AS
TO
MINIMIZE
THE
OVERALL
PROBABILITY
OF
ERROR
THE
MINIMUM
PROBABILITY
OF
ERROR
DECISIONS
ARE
GIVEN
BY
Y
ARGMAX
P
X
LY
P
Y
Y
O
L
ARG
MAX
P
YLX
Y
O
L
THE
OPTIMAL
DECISIONS
ARE
BASED
ON
THE
POSTERIOR
CLASS
PROBABILIT
IESP
Y
LX
FOR
BINARY
CLASSIFICATION
PROBLEMS
WE
CAN
WRITE
THESE
DECISIONS
AS
P
Y
L
L
X
Y
IF
LOG
P
Y
OXL
AND
Y
OTHERWISE
THE
OPTIMAL
DECISIONS
ARE
BASED
ON
THE
POSTERIOR
CLASS
PROBABILITIES
P
Y
LX
FOR
BINARY
CLASSIFICATION
PROBLEMS
WE
CAN
WRITE
THESE
DECISIONS
AS
P
Y
L
L
X
Y
IF
LOG
P
Y
OXL
AND
Y
OTHERWISE
WE
GENERALLY
DON
T
KNOW
P
YLX
BUT
WE
CAN
PARAMETERIZE
THE
POSSIBLE
DECISIONS
ACCORDING
TO
P
Y
LLX
T
LOG
W
W
W
PY
OX
OUR
LOG
ODDS
MODEL
P
Y
L
LX
T
LOG
P
Y
OXL
W
X
W
GIVES
RISE
TO
A
SPECIFIC
FORM
FOR
THE
CONDITIONAL
PROBABILITY
OVER
THE
LABELS
THE
LOGISTIC
MODEL
P
Y
L
L
X
W
G
WO
X
T
WHERE
G
Z
EXP
Z
IS
A
LOGISTIC
SQUASHING
FUNCTION
THAT
TURNS
LINEAR
PREDICTIONS
INT
O
PROBABILIT
IES
Z
LOGISTIC
REGRESSION
MODELS
IMPLY
A
LINEAR
DECISION
BOUNDARY
P
Y
LLX
WO
XT
P
Y
OLX
T
O
OO
O
OOO
AO
O
RR
I
O
O
L
J
O
O
O
O
O
F
O
J
O
O
I
A
O
O
CF
O
D
O
O
O
O
O
OO
R
OD
O
FL
ODB
QO
I
OO
CXI
CXI
O
A
A
AO
ES
A
O
A
CB
OOO
O
RL
CLASS
AS
WITH
THE
LINEAR
REGRESSION
MODELS
WE
CAN
FIT
THE
LOGISTIC
MODELS
USING
THE
MAXIMUM
CONDITIONAL
LOG
LIKELIHOOD
CRITERION
WHERE
N
L
D
W
LOGP
YI
LX
I
W
I
L
P
Y
L
LX
W
G
WO
STOCHASTIC
GRADIENT
ASCENT
WE
CAN
TRY
TO
MAXIMIZE
THE
LOG
LIKELIHOOD
IN
AN
ON
LINE
OR
INCREMENTAL
FASHION
GIVEN
EACH
TRAINING
INPUT
XI
AND
THE
BINARY
LABEL
YI
WE
CAN
CHANGE
THE
PARAMETERS
W
SLIGHTLY
TO
INCREASE
THE
CORRESPONDING
LOG
PROBABILITY
W
W
RJ
AW
LOG
P
YILXI
W
W
TJ
YI
P
YI
L
LX
I
W
PREDICTION
ERROR
WHERE
TJ
IS
THE
LEARNING
RATE
GRADIENT
ASCENT
OF
THE
LOG
LIKELIHOOD
WE
CAN
ALSO
PERFORM
GRADIENT
ASCENT
STEPS
ON
THE
LOG
LIKELIHOOD
OF
ALL
THE
TRAINING
LABELS
GIVEN
EXAMPLES
AT
THE
SAME
TIME
IN
OTHER
WORDS
A
W
W
OWL
D
W
W
T
YI
P
YI
LLXI
W
I
I
L
WE
HAVE
A
PROBABILISTIC
MODEL
M
OF
SOME
PHENOMENA
BUT
WE
DO
NOT
KNOW
ITS
PARAMETERS
EACH
EXECUTION
OF
M
PRODUCES
AN
OBSERVATION
X
I
ACCORDING
TO
THE
UNKNOWN
DISTRIBUTION
INDUCED
BY
M
GOAL
AFTER
OBSERVING
X
X
N
ESTIMATE
THE
MODEL
PARAMETERS
THAT
GENERATED
THE
OBSERVED
DATA
THIS
VECTOR
PARAMETER
CAN
BE
USED
TO
PREDICT
FUTURE
DATA
MLE
PRINCIPLE
CHOOSE
PARAMETERS
THAT
MAXIMIZE
THE
LIKELIHOOD
OF
THE
DATA
THE
LIKELIHOOD
OF
THE
OBSERVED
DATA
GIVEN
THE
MODEL
PARAMETERS
IS
THE
CONDITIONAL
PROBABILITY
THAT
THE
MODEL
M
WITH
PARAMETERS
PRODUCES
X
X
N
L
PR
X
X
N
M
IN
MLE
WE
SEEK
THE
MODEL
PARAMETERS
THAT
MAXIMIZE
THE
LIKELIHOOD
WHEN
TOSSED
IT
CAN
LAND
IN
ONE
OF
TWO
POSITIONS
HEAD
H
OR
TAIL
T
WE
DENOTE
BY
THE
UNKNOWN
PROBABILITY
P
H
HOW
GOOD
IS
A
PARTICULAR
IT
DEPENDS
ON
HOW
LIKELY
IT
IS
TO
GENERATE
THE
OBSERVED
DATA
LD
P
D
P
X
M
M
THE
LIKELIHOOD
FOR
THE
SEQUENCE
H
T
T
H
H
IS
LD
LD
NH
LOG
NT
LOG
TAKING
DERIVATIVE
AND
EQUATING
IT
TO
WE
GET
NH
NT
ˆ
NH
NH
NT
OVERCONFIDENCE
BETTER
MAXIMUM
A
POSTERIORI
MAP
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
PERCEPTRON
LOGISTIC
REGRESSION
GENERATIVE
VS
DISCRIMINATIVE
MODELS
INSTEAD
OF
JUST
TWO
CLASSES
WE
NOW
HAVE
C
CLASSES
E
G
PREDICT
WHICH
MOVIE
GENRE
A
VIEWER
LIKES
BEST
POSSIBLE
ANSWERS
ACTION
DRAMA
INDIE
THRILLER
ETC
TWO
APPROACHES
ONE
VS
ALL
ONE
VS
ONE
ONE
VS
ALL
A
K
A
ONE
VS
OTHERS
TRAIN
K
CLASSIFIERS
IN
EACH
POS
DATA
FROM
CLASS
I
NEG
DATA
FROM
CLASSES
OTHER
THAN
I
THE
CLASS
WITH
THE
MOST
CONFIDENT
PREDICTION
WINS
EXAMPLE
YOU
HAVE
CLASSES
TRAIN
CLASSIFIERS
VS
OTHERS
SCORE
VS
OTHERS
SCORE
VS
OTHERS
SCORE
VS
OTHER
SCORE
FINAL
PREDICTION
CLASS
ISSUES
ONE
VS
ONE
A
K
A
ALL
VS
ALL
TRAIN
K
K
BINARY
CLASSIFIERS
ALL
PAIRS
OF
CLASSES
THEY
ALL
VOTE
FOR
THE
LABEL
EXAMPLE
YOU
HAVE
CLASSES
THEN
TRAIN
CLASSIFIERS
VS
VS
VS
VS
VS
VS
VOTES
FINAL
PREDICTION
IS
CLASS
WHAT
ARE
SOME
PROBLEMS
WITH
THIS
APPROACH
TO
DOING
MULTI
CLASS
THERE
ARE
NATIVELY
MULTI
CLASS
METHODS
FIGURES
FROM
BISHOP
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
PERCEPTRON
LOGISTIC
REGRESSION
MULTI
WAY
CLASSIFICATION
BINARY
CASE
MUTLI
CLASS
CASE
WHY
ARE
THESE
CALLED
GENERATIVE
CAN
USE
THEM
TO
GENERATE
NEW
SAMPLES
X
PERHAPS
THIS
IS
OVERKILL
CONSIDER
AGAIN
A
BINARY
CLASSIFICATION
TASK
WITH
Y
L
LABELS
NOT
AS
BEFORE
AND
LINEAR
DISCRIMINANT
FUNCTIONS
F
X
W
WO
XT
PARAMETERIZED
BY
WA
AND
WD
T
THE
PREDICTED
LABEL
IS
SIMPLY
GIVEN
BY
THE
SIGN
OF
THE
DISCRIMINANT
FUNCTION
Y
SIGN
F
X
W
WE
ARE
ONLY
INTERESTED
IN
GETTING
THE
LABELS
CORRECT
NO
PROBABILITIES
ARE
ASSOCIATED
WITH
THE
PREDICTIONS
WHEN
THE
TRAINING
SET
XN
YN
IS
LINEARLY
SEPARABLE
WE
CAN
FIND
PARAMETERS
W
SUCH
THAT
YI
WO
XF
W
I
N
I
E
THE
SIGN
OF
THE
DISCRIMINANT
FUNCTION
AGREES
WITH
THE
LABEL
THERE
ARE
MANY
POSSIBLE
SOLUTIONS
PERHAPS
WE
CAN
FIND
A
BETTER
DISCRIMINANT
BOUNDARY
BY
REQUIRING
THAT
THE
TRAINING
EXAMPLES
ARE
SEPARATED
WITH
A
FIXED
MARGIN
YI
WO
XF
W
I
N
WE
MINIMIZE
THE
REGULARIZATION
PENALTY
D
L
WI
SUBJECT
TO
THE
CLASSIFICATION
CONSTRAINTS
I
L
X
X
X
X
X
X
X
Q
X
X
X
X
X
FOR
I
N
X
X
X
X
X
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
FEBRUARY
HOMEWORK
DEADLINE
IS
NOW
WE
LL
HAVE
COVERED
EVERYTHING
YOU
NEED
TODAY
OR
AT
THE
LATEST
ON
MONDAY
PROJECT
PROPOSAL
DUE
TONIGHT
ON
COURSEWEB
HOW
MANY
OF
YOU
WANT
ME
TO
PRINT
HANDOUTS
FOR
NEXT
TIME
LINEAR
SUPPORT
VECTOR
MACHINES
NON
LINEAR
SVMS
AND
THE
KERNEL
TRICK
SOFT
MARGIN
SVMS
EXAMPLE
USE
OF
SVMS
ADVANCED
TOPICS
VERY
BRIEFLY
STRUCTURED
SVMS
LATENT
VARIABLES
HOW
TO
SOLVE
THE
SVM
PROBLEM
NEXT
CLASS
LET
A
W
C
X
X
Y
AX
CY
B
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
LINES
IN
LET
A
W
C
X
X
Y
AX
CY
B
W
X
B
D
B
DISTANCE
FROM
POINT
TO
LINE
LINES
IN
LET
A
W
C
X
Y
AX
CY
B
W
X
B
B
W
X
B
DISTANCE
FROM
D
W
POINT
TO
LINE
FIND
LINEAR
FUNCTION
TO
SEPARATE
POSITIVE
AND
NEGATIVE
EXAMPLES
XI
POSITIVE
XI
NEGATIVE
W
B
XI
W
B
WHICH
LINE
IS
BEST
DISCRIMINATIVE
CLASSIFIER
BASED
ON
OPTIMAL
SEPARATING
LINE
FOR
CASE
MAXIMIZE
THE
MARGIN
BETWEEN
THE
POSITIVE
AND
NEGATIVE
TRAINING
EXAMPLES
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
SUPPORT
VECTORS
MARGIN
FOR
SUPPORT
VECTORS
XI
W
B
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
AND
LINE
FOR
SUPPORT
VECTORS
XI
W
B
W
SUPPORT
VECTORS
MARGIN
WΤ
X
B
M
WANT
LINE
THAT
MAXIMIZES
THE
MARGIN
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
FOR
SUPPORT
VECTORS
XI
W
B
DISTANCE
BETWEEN
POINT
AND
LINE
XI
W
B
W
SUPPORT
VECTORS
MARGIN
THEREFORE
THE
MARGIN
IS
W
MAXIMIZE
MARGIN
W
CORRECTLY
CLASSIFY
ALL
TRAINING
DATA
POINTS
XI
POSITIVE
YI
XI
W
B
XI
NEGATIVE
YI
XI
W
B
QUADRATIC
OPTIMIZATION
PROBLEM
ONE
CONSTRAINT
FOR
EACH
TRAINING
POINT
NOTE
SIGN
TRICK
SOLUTION
W
I
I
YI
XI
SOLUTION
W
I
I
YI
XI
B
YI
W
XI
FOR
ANY
SUPPORT
VECTOR
CLASSIFICATION
FUNCTION
F
X
SIGN
W
X
B
SIGN
I
YI
XI
X
B
IF
F
X
CLASSIFY
AS
NEGATIVE
OTHERWISE
CLASSIFY
AS
POSITIVE
NOTICE
THAT
IT
RELIES
ON
AN
INNER
PRODUCT
BETWEEN
THE
TEST
POINT
X
AND
THE
SUPPORT
VECTORS
XI
SOLVING
THE
OPTIMIZATION
PROBLEM
ALSO
INVOLVES
COMPUTING
THE
INNER
PRODUCTS
XI
XJ
BETWEEN
ALL
PAIRS
OF
TRAINING
POINTS
F
X
SIGN
W
X
B
SIGN
I
I
YI
XI
X
B
ADAPTED
FROM
MILOS
HAUSKRECHT
PLAN
FOR
TODAY
LINEAR
SUPPORT
VECTOR
MACHINES
SOFT
MARGIN
SVMS
EXAMPLE
USE
OF
SVMS
ADVANCED
TOPICS
VERY
BRIEFLY
STRUCTURED
SVMS
LATENT
VARIABLES
HOW
TO
SOLVE
THE
SVM
PROBLEM
NEXT
CLASS
DATASETS
THAT
ARE
LINEARLY
SEPARABLE
WORK
OUT
GREAT
X
BUT
WHAT
IF
THE
DATASET
IS
JUST
TOO
HARD
X
WE
CAN
MAP
IT
TO
A
HIGHER
DIMENSIONAL
SPACE
GENERAL
IDEA
THE
ORIGINAL
INPUT
SPACE
CAN
ALWAYS
BE
MAPPED
TO
SOME
HIGHER
DIMENSIONAL
FEATURE
SPACE
WHERE
THE
TRAINING
SET
IS
SEPARABLE
CONSIDER
THE
MAPPING
X
X
X
Y
X
Y
XY
K
X
Y
XY
SVETLANA
LAZEBNIK
THE
KERNEL
TRICK
THE
LINEAR
CLASSIFIER
RELIES
ON
DOT
PRODUCT
BETWEEN
VECTORS
K
XI
XJ
XI
XJ
IF
EVERY
DATA
POINT
IS
MAPPED
INTO
HIGH
DIMENSIONAL
SPACE
VIA
SOME
TRANSFORMATION
Φ
XI
Φ
XI
THE
DOT
PRODUCT
BECOMES
K
XI
XJ
Φ
XI
Φ
XJ
A
KERNEL
FUNCTION
IS
SIMILARITY
FUNCTION
THAT
CORRESPONDS
TO
AN
INNER
PRODUCT
IN
SOME
EXPANDED
FEATURE
SPACE
THE
KERNEL
TRICK
INSTEAD
OF
EXPLICITLY
COMPUTING
THE
LIFTING
TRANSFORMATION
Φ
X
DEFINE
A
KERNEL
FUNCTION
K
SUCH
THAT
K
XI
XJ
Φ
XI
Φ
XJ
ANDREW
MOORE
EXAMPLES
OF
KERNEL
FUNCTIONS
LINEAR
K
XI
X
J
I
J
POLYNOMIALS
OF
DEGREE
UP
TO
D
𝐾
𝑥𝑖
𝑥𝑗
𝑥𝑖𝑇𝑥𝑗
𝑑
GAUSSIAN
RBF
K
XI
X
J
EXP
HISTOGRAM
INTERSECTION
K
XI
X
J
MIN
K
XI
K
X
J
K
ANDREW
MOORE
CARLOS
GUESTRIN
PROBLLEM
CHEDKI
N
G
IF
A
G
I
VEN
K
X
X
X
JR
F
U
LLFI
LLLS
THE
CONDIT
I
O
N
FOR
A
LKERNELL
I
D
IFFICUIT
WE
NEED
TO
PROVE
OR
DI
SPR
O
VE
FOR
ANY
SET
W
ORK
AR
OU
N
D
L
TIK
XI
XJ
TJ
I
J
L
IT
I
EASY
TO
CONSTRUCT
FUNCTI
O
N
K
T
H
A
T
ARE
P
OSIT
I
VE
DEFINITE
LKERNELLS
WE
CAN
CONSTRUCT
KERNE
LS
FR
O
M
SCRATCH
FOR
ANY
CP
X
JRM
K
X
X
CP
X
CP
X
NTM
I
A
LKERNE
I
IF
D
X
X
X
JR
I
A
DISTANCE
FU
NCT
ION
I
E
D
X
X
FOR
A
LLL
X
X
E
X
D
X
X
ONLLY
FOR
X
X
D
X
X
D
X
X
FOR
ALLLL
X
X
E
X
D
X
X
D
X
X
D
X
X
FOR
ALLLL
X
X
UX
E
X
THEN
K
X
X
EXP
D
X
X
I
A
KERNELL
WE
CAN
CONSTRUCT
KERNELS
FR
O
M
OTHER
KERNELS
IF
K
I
A
LKERNELL
AND
A
THEN
AK
A
ND
K
A
ARE
LKERNELLS
IF
ARE
KERNE
LS
THEN
AND
ARE
LKERNELLS
PLAN
FOR
TODAY
LINEAR
SUPPORT
VECTOR
MACHINES
NON
LINEAR
SVMS
AND
THE
KERNEL
TRICK
ADVANCED
TOPICS
VERY
BRIEFLY
STRUCTURED
SVMS
LATENT
VARIABLES
HOW
TO
SOLVE
THE
SVM
PROBLEM
NEXT
CLASS
THE
W
THAT
MINIMIZES
MAXIMIZE
MARGIN
MISCLASSIFICATION
COST
DATA
SAMPLES
SLACK
VARIABLE
THE
W
THAT
MINIMIZES
MAXIMIZE
MARGIN
MINIMIZE
MISCLASSIFICATION
BOARD
WHAT
ABOUT
MULTI
CLASS
SVMS
IN
PRACTICE
WE
OBTAIN
A
MULTI
CLASS
SVM
BY
COMBINING
TWO
CLASS
SVMS
ONE
VS
OTHERS
TRAINING
LEARN
AN
SVM
FOR
EACH
CLASS
VS
THE
OTHERS
TESTING
APPLY
EACH
SVM
TO
THE
TEST
EXAMPLE
AND
ASSIGN
IT
TO
THE
CLASS
OF
THE
SVM
THAT
RETURNS
THE
HIGHEST
DECISION
VALUE
ONE
VS
ONE
TRAINING
LEARN
AN
SVM
FOR
EACH
PAIR
OF
CLASSES
TESTING
EACH
LEARNED
SVM
VOTES
FOR
A
CLASS
TO
ASSIGN
TO
THE
TEST
EXAMPLE
THERE
ARE
ALSO
NATIVELY
MULTI
CLASS
FORMULATIONS
CRAMMER
AND
SINGER
JMLR
SVMS
FOR
RECOGNITION
DEFINE
YOUR
REPRESENTATION
FOR
EACH
EXAMPLE
SELECT
A
KERNEL
FUNCTION
COMPUTE
PAIRWISE
KERNEL
VALUES
BETWEEN
LABELED
EXAMPLES
USE
THIS
KERNEL
MATRIX
TO
SOLVE
FOR
SVM
SUPPORT
VECTORS
WEIGHTS
TO
CLASSIFY
A
NEW
EXAMPLE
COMPUTE
KERNEL
VALUES
BETWEEN
NEW
INPUT
AND
SUPPORT
VECTORS
APPLY
WEIGHTS
CHECK
SIGN
OF
OUTPUT
EXAMPLE
LEARNING
GENDER
WITH
SVMS
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
MOGHADDAM
AND
YANG
FACE
GESTURE
SUPPORT
FACES
MOGHADDAM
AND
YANG
LEARNING
GENDER
WITH
SUPPORT
FACES
TPAMI
HUMAN
VS
MACHINE
SVMS
PERFORMED
BETTER
THAN
ANY
SINGLE
HUMAN
TEST
SUBJECT
AT
EITHER
RESOLUTION
KRISTEN
GRAUMAN
PLAN
FOR
TODAY
LINEAR
SUPPORT
VECTOR
MACHINES
NON
LINEAR
SVMS
AND
THE
KERNEL
TRICK
SOFT
MARGIN
SVMS
EXAMPLE
USE
OF
SVMS
HOW
TO
SOLVE
THE
SVM
PROBLEM
NEXT
CLASS
Y
IS
A
VECTOR
TSOCHANTARIDIS
ET
AL
JMLR
ADAPTED
FROM
NOWOZIN
AND
C
LAMPERT
SVM
VS
LOGISTIC
REGRESSION
WHEN
VIEWED
FROM
THE
POINT
OF
VIEW
OF
REGULARIZED
EMPIRICAL
LOSS
MINIMIZATION
SVM
AND
LOGISTIC
REGRESSION
APPEAR
QUITE
SIMILAR
SVM
LOGP
YI
LX
W
L
N
LOGISTIC
N
L
LOGG
YI
WO
XF
W
J
I
L
WHERE
G
Z
EXP
Z
IS
THE
LOGISTIC
FUNCTION
NOTE
THAT
WE
HAVE
TRANSFORMED
THE
PROBLEM
MAXIMIZING
THE
PENALIZED
LOG
LIKELIHOOD
INTO
MINIMIZING
NEGATIVE
PENALIZED
LOG
LIKELIHOOD
SVM
VS
LOGISTIC
REGRESSION
CONT
D
THE
DIFFERENCE
COMES
FROM
HOW
WE
PENALIZE
ERRORS
N
AZ
BOTH
N
L
LOSS
I
WO
LL
I
L
SVM
LOSS
Z
Z
REGULARIZED
LOGISTIC
REG
LOSS
Z
LOG
L
EXP
Z
O
L
J
J
J
Z
PROS
SVMS
PROS
AND
CONS
KERNEL
BASED
FRAMEWORK
IS
VERY
POWERFUL
FLEXIBLE
OFTEN
A
SPARSE
SET
OF
SUPPORT
VECTORS
COMPACT
AT
TEST
TIME
WORK
VERY
WELL
IN
PRACTICE
EVEN
WITH
VERY
SMALL
TRAINING
SAMPLE
SIZES
SOLUTION
CAN
BE
FORMULATED
AS
A
QUADRATIC
PROGRAM
NEXT
TIME
MANY
PUBLICLY
AVAILABLE
SVM
PACKAGES
E
G
LIBSVM
LIBLINEAR
SVMLIGHT
OR
USE
BUILT
IN
MATLAB
VERSION
BUT
SLOWER
CONS
CAN
BE
TRICKY
TO
SELECT
BEST
KERNEL
FUNCTION
FOR
A
PROBLEM
COMPUTATION
MEMORY
AT
TRAINING
TIME
MUST
COMPUTE
KERNEL
VALUES
FOR
ALL
EXAMPLE
PAIRS
LEARNING
CAN
TAKE
A
VERY
LONG
TIME
FOR
LARGE
SCALE
PROBLEMS
ADAPTED
FROM
LANA
LAZEBNIK
MIN
LL
SUBJECT
TO
YI
WO
XF
I
N
YI
WO
XF
I
N
LET
START
BY
REPRESENTING
THE
CONSTRAINTS
AS
LOSSES
MAX
A
A
O
YI
WO
XIT
YI
WO
XF
OO
OT
YI
WO
XF
I
N
LET
START
BY
REPRESENTING
THE
CONSTRAINTS
AS
LOSSES
MAX
A
YI
WO
XIT
YI
WO
XF
A
O
OO
OT
AND
REWRITE
THE
MINIMIZATION
PROBLEM
IN
TERMS
OF
THESE
MIN
W
MIN
LL
SUBJECT
TO
YI
WO
XF
W
I
N
LET
START
BY
REPRESENTING
THE
CONSTRAINTS
AS
LOSSES
MAX
A
YI
WO
XI
T
YI
WO
XF
A
O
OO
OT
AND
REWRITE
THE
MINIMIZATION
PROBLEM
IN
TERMS
OF
THESE
MAX
MIN
O
I
O
W
N
LAI
YI
WO
XF
I
L
J
W
O
AS
A
RESULT
WE
HAVE
TO
BE
ABLE
TO
MINIMIZE
J
W
A
WITH
RESPECT
TO
PARAMETERS
W
FOR
ANY
FIXED
SETTING
OF
THE
LAGRANGE
MULTIPLIERS
AI
MAX
AI
O
W
N
L
CXI
L
YI
WO
XF
I
L
J
W
A
WE
CAN
FIND
THE
OPTIMAL
W
AS
A
FUNCTION
OF
AI
BY
SETTING
THE
DERIVATIVES
TO
ZERO
A
J
W
A
A
J
W
A
WO
L
AIYIXI
I
L
N
LAIYI
I
L
WE
CAN
THEN
SUBSTITUTE
THE
SOLUTION
A
N
J
W
A
A
L
O
IYIXI
I
L
N
J
W
A
AIYI
OWO
I
L
BACK
INTO
THE
OBJECTIVE
AND
GET
AFTER
SOME
ALGEBRA
N
MAX
A
L
AI
L
YI
WO
XF
I
L
I
I
A
IY
I
MAX
A
I
I
O
IYI
SUBJECT
TO
AI
AND
EI
AIYI
ONLY
A
CORRESPONDING
TO
PPORT
VECTORS
WILL
BE
NON
ZERO
SUBJECT
TO
AI
AND
EI
AIYI
ONLY
A
CORRESPONDING
TO
PPORT
VECTORS
WILL
BE
NON
ZERO
WE
CAN
MAKE
PREDICTIONS
ON
ANY
NEW
EXAMPLE
X
ACCORDING
TO
THE
SIGN
OF
THE
DISCRIMINANT
FUNCTION
SUBJECT
TO
AI
AND
EI
AIYI
ONLY
A
CORRESPONDING
TO
PPORT
VECTORS
WILL
BE
NON
ZERO
WE
CAN
MAKE
PREDICTIONS
ON
ANY
NEW
EXAMPLE
X
ACCORDING
TO
THE
SIGN
OF
THE
DISCRIMINANT
FUNCTION
W
XT
WA
XT
L
IYIXI
I
L
AI
BY
MAXIMIZING
SUBJECT
TO
AI
AND
EI
AIYI
ONLY
A
CORRESPONDING
TO
PPORT
VECTORS
WILL
BE
NON
ZERO
WE
CAN
MAKE
PREDICTIONS
ON
ANY
NEW
EXAMPLE
X
ACCORDING
TO
THE
SIGN
OF
THE
DISCRIMINANT
FUNCTION
WA
XT
WA
XT
L
IYIXI
WO
L
IY
I
XT
X
I
I
L
IESV
SEQUENTIAL
MINIMAL
OPTIMIZATION
CONVERGENCE
ALL
ΑI
SATISFY
KARUSH
KUHN
TUCKER
KKT
CONDITIONS
USED
TO
DETERMINE
IF
AT
OPTIMAL
SOLUTION
REPEAT
UNTIL
CONVERGENCE
PICK
ΑI
THAT
VIOLATES
THE
CONDITIONS
PICK
ANOTHER
ΑJ
RECOMPUTE
NEW
VALUES
FOR
ΑI
AND
ΑJ
PROPOSED
BY
JOHN
PLATT
IN
FAST
TRAINING
OF
SUPPORT
VECTOR
MACHINES
USING
SEQUENTIAL
MINIMAL
OPTIMIZATION
FURTHER
READING
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
FEBRUARY
PLAN
FOR
TODAY
AND
NEXT
TWO
CLASSES
PROBABILITY
REVIEW
DENSITY
ESTIMATION
NAÏVE
BAYES
AND
BAYESIAN
BELIEF
NETWORKS
PROCEDURAL
VIEW
TRAINING
STAGE
RAW
DATA
X
FEATURE
EXTRACTION
TRAINING
DATA
X
Y
F
LEARNING
TESTING
STAGE
RAW
DATA
X
FEATURE
EXTRACTION
TEST
DATA
X
F
X
APPLY
FUNCTION
EVALUATE
ERROR
STATISTICAL
ESTIMATION
VIEW
PROBABILITIES
TO
RESCUE
X
AND
Y
ARE
RANDOM
VARIABLES
D
XN
YN
P
X
Y
IID
INDEPENDENT
IDENTICALLY
DISTRIBUTED
BOTH
TRAINING
TESTING
DATA
SAMPLED
IID
FROM
P
X
Y
LEARN
ON
TRAINING
SET
HAVE
SOME
HOPE
OF
GENERALIZING
TO
TEST
SET
PROBABILITY
A
IS
NON
DETERMINISTIC
EVENT
CAN
THINK
OF
A
AS
A
BOOLEAN
VALUED
VARIABLE
EXAMPLES
A
YOUR
NEXT
PATIENT
HAS
CANCER
A
RAFAEL
NADAL
WINS
US
OPEN
INTERPRETING
PROBABILITIES
WHAT
DOES
P
A
MEAN
FREQUENTIST
VIEW
LIMIT
N
A
IS
TRUE
N
LIMITING
FREQUENCY
OF
A
REPEATING
NON
DETERMINISTIC
EVENT
BAYESIAN
VIEW
P
A
IS
YOUR
BELIEF
ABOUT
A
MARKET
DESIGN
VIEW
P
A
TELLS
YOU
HOW
MUCH
YOU
WOULD
BET
AXIOMS
OF
PROBABILITY
THEORY
ALL
PROBABILITIES
BETWEEN
AND
P
A
TRUE
PROPOSITION
HAS
PROBABILITY
FALSE
HAS
PROBABILITY
P
TRUE
P
FALSE
THE
PROBABILITY
OF
DISJUNCTION
IS
P
A
B
P
A
P
B
P
A
B
SLIDE
CREDIT
RAY
MOONEY
P
A
P
FALSE
P
TRUE
P
A
V
B
P
A
P
B
P
A
B
EVENT
SPACE
OF
ALL
POSSIBLE
WORLDS
ITS
AREA
IS
P
A
AREA
OF
REDDISH
OVAL
P
A
P
FALSE
P
TRUE
P
A
V
B
P
A
P
B
P
A
B
THE
AREA
OF
A
CAN
T
GET
ANY
SMALLER
THAN
AND
A
ZERO
AREA
WOULD
MEAN
NO
WORLD
COULD
EVER
HAVE
A
TRUE
P
A
P
FALSE
P
TRUE
P
A
V
B
P
A
P
B
P
A
B
THE
AREA
OF
A
CAN
T
GET
ANY
BIGGER
THAN
AND
AN
AREA
OF
WOULD
MEAN
ALL
WORLDS
WILL
HAVE
A
TRUE
P
A
P
FALSE
P
TRUE
P
A
V
B
P
A
P
B
P
A
B
SIMPLE
ADDITION
AND
SUBTRACTION
THE
JOINT
PROBABILITY
DISTRIBUTION
FOR
A
SET
OF
RANDOM
VARIABLES
XN
GIVES
THE
PROBABILITY
OF
EVERY
COMBINATION
OF
VALUES
AN
N
DIMENSIONAL
ARRAY
WITH
VN
VALUES
IF
ALL
VARIABLES
ARE
DISCRETE
WITH
V
VALUES
ALL
VN
VALUES
MUST
SUM
TO
P
XN
POSITIVE
NEGATIVE
THE
PROBABILITY
OF
ALL
POSSIBLE
CONJUNCTIONS
ASSIGNMENTS
OF
VALUES
TO
SOME
SUBSET
OF
VARIABLES
CAN
BE
CALCULATED
BY
SUMMING
THE
APPROPRIATE
SUBSET
OF
VALUES
FROM
THE
JOINT
DISTRIBUTION
P
RED
P
RED
CIRCLE
THEREFORE
ALL
CONDITIONAL
PROBABILITIES
CAN
ALSO
BE
CALCULATED
P
POSITIVE
RED
SLIDE
CREDIT
RAY
MOONEY
CIRCLE
Y
Z
SUM
RULE
DHRUV
BATRA
SLIDE
CREDIT
ERIK
SUDDHERTH
P
A
B
IN
WORLDS
WHERE
B
IS
TRUE
FRACTION
WHERE
A
IS
TRUE
P
A
B
P
A
B
P
B
EXAMPLE
H
HAVE
A
HEADACHE
F
COMING
DOWN
WITH
FLU
P
H
P
F
P
H
F
HEADACHES
ARE
RARE
AND
FLU
IS
RARER
BUT
IF
YOU
RE
COMING
DOWN
WITH
FLU
THERE
A
CHANCE
YOU
LL
HAVE
A
HEADACHE
P
Y
Y
X
X
WHAT
DO
YOU
BELIEVE
ABOUT
Y
Y
IF
I
TELL
YOU
X
X
P
RAFAEL
NADAL
WINS
US
OPEN
WHAT
IF
I
TELL
YOU
HE
HAS
WON
THE
US
OPEN
TWICE
NOVAK
DJOKOVIC
IS
RANKED
JUST
WON
AUSTRALIAN
OPEN
PRODUCT
RULE
DHRUV
BATRA
SLIDE
CREDIT
ERIK
SUDDERTH
GENERALIZED
PRODUCT
RULE
EXAMPLE
A
AND
B
ARE
INDEPENDENT
IFF
P
A
P
B
B
P
A
P
B
THESE
TWO
CONSTRAINTS
ARE
LOGICALLY
EQUIVALENT
THEREFORE
IF
A
AND
B
ARE
INDEPENDENT
P
A
B
P
A
B
P
A
P
A
B
P
B
P
A
P
B
MARGINAL
P
SATISFIES
X
Y
IF
AND
ONLY
IF
P
X
X
Y
Y
P
X
X
P
Y
Y
X
VAL
X
Y
VAL
Y
CONDITIONAL
P
SATISFIES
X
Y
Z
IF
AND
ONLY
IF
P
X
Y
Z
P
X
Z
P
Y
Z
X
VAL
X
Y
VAL
Y
Z
VAL
Z
EXPECTATION
VARIANCE
COVARIANCE
EQUATIONS
FROM
BISHOP
BAYES
THEOREM
P
H
E
P
E
H
P
H
P
E
SIMPLE
PROOF
FROM
DEFINITION
OF
CONDITIONAL
PROBABILITY
P
H
E
P
H
E
DEF
COND
PROB
P
E
P
E
H
P
H
E
DEF
COND
PROB
P
H
E
P
H
P
E
H
P
H
P
H
E
P
E
QED
P
H
E
P
E
H
P
H
ADAPTED
FROM
RAY
MOONEY
P
E
PROBABILISTIC
CLASSIFICATION
LET
Y
BE
THE
RANDOM
VARIABLE
FOR
THE
CLASS
WHICH
TAKES
VALUES
YM
LET
X
BE
THE
RANDOM
VARIABLE
DESCRIBING
AN
INSTANCE
CONSISTING
OF
A
VECTOR
OF
VALUES
FOR
N
FEATURES
XN
LET
XK
BE
A
POSSIBLE
VALUE
FOR
X
AND
XIJ
A
POSSIBLE
VALUE
FOR
XI
FOR
CLASSIFICATION
WE
NEED
TO
COMPUTE
P
Y
YI
X
XK
FOR
I
M
HOWEVER
GIVEN
NO
OTHER
ASSUMPTIONS
THIS
REQUIRES
A
TABLE
GIVING
THE
PROBABILITY
OF
EACH
CATEGORY
FOR
EACH
POSSIBLE
INSTANCE
IN
THE
INSTANCE
SPACE
WHICH
IS
IMPOSSIBLE
TO
ACCURATELY
ESTIMATE
FROM
A
REASONABLY
SIZED
TRAINING
SET
ASSUMING
Y
AND
ALL
XI
ARE
BINARY
WE
NEED
ENTRIES
TO
SPECIFY
P
Y
POS
X
XK
FOR
EACH
OF
THE
POSSIBLE
XK
SINCE
P
Y
NEG
X
XK
P
Y
POS
X
XK
COMPARED
TO
ENTRIES
FOR
THE
JOINT
DISTRIBUTION
P
Y
XN
SLIDE
CREDIT
RAY
MOONEY
BAYESIAN
CATEGORIZATION
DETERMINE
CATEGORY
OF
XK
BY
DETERMINING
FOR
EACH
YI
PRIOR
LIKELIHOOD
P
Y
Y
X
X
P
Y
YI
P
X
XK
Y
YI
I
K
POSTERIOR
P
X
XK
P
X
XK
CAN
BE
DETERMINED
SINCE
CATEGORIES
ARE
COMPLETE
AND
DISJOINT
M
P
X
XK
P
Y
I
YI
P
X
XK
Y
YI
P
Y
Y
X
X
P
Y
YI
P
X
XK
Y
YI
I
I
K
I
P
X
XK
BAYESIAN
CATEGORIZATION
CONT
NEED
TO
KNOW
PRIORS
P
Y
YI
CONDITIONALS
LIKELIHOOD
P
X
XK
Y
YI
P
Y
YI
ARE
EASILY
ESTIMATED
FROM
DATA
IF
NI
OF
THE
EXAMPLES
IN
D
ARE
IN
YI
THEN
P
Y
YI
NI
D
TOO
MANY
POSSIBLE
INSTANCES
E
G
FOR
BINARY
FEATURES
TO
ESTIMATE
ALL
P
X
XK
Y
YI
NEED
TO
MAKE
SOME
SORT
OF
INDEPENDENCE
ASSUMPTIONS
ABOUT
THE
FEATURES
TO
MAKE
LEARNING
TRACTABLE
MORE
DETAILS
LATER
A
HYPOTHESIS
IS
DENOTED
AS
H
IT
IS
ONE
MEMBER
OF
THE
HYPOTHESIS
SPACE
H
A
SET
OF
TRAINING
EXAMPLES
IS
DENOTED
AS
D
A
COLLECTION
OF
X
Y
PAIRS
FOR
TRAINING
PR
H
THE
PRIOR
PROBABILITY
OF
THE
HYPOTHESIS
WITHOUT
OBSERVING
ANY
TRAINING
DATA
WHAT
THE
PROBABILITY
THAT
H
IS
THE
TARGET
FUNCTION
WE
WANT
PR
D
THE
PRIOR
PROBABILITY
OF
THE
OBSERVED
DATA
CHANCE
OF
GETTING
THE
PARTICULAR
SET
OF
TRAINING
EXAMPLES
D
PR
H
D
THE
POSTERIOR
PROBABILITY
OF
H
WHAT
IS
THE
PROBABILITY
THAT
H
IS
THE
TARGET
GIVEN
THAT
WE
VE
OBSERVED
D
PR
D
H
THE
PROBABILITY
OF
GETTING
D
IF
H
WERE
TRUE
A
K
A
LIKELIHOOD
OF
THE
DATA
PR
H
D
PR
D
H
PR
H
PR
D
MAXIMUM
A
POSTERIORI
MAP
ESTIMATION
HMAP
ARGMAXH
PR
H
D
ARGMAXH
PR
D
H
PR
H
PR
D
ARGMAXH
PR
D
H
PR
H
MAXIMUM
LIKELIHOOD
ESTIMATION
MLE
HML
ARGMAX
PR
D
H
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
MARCH
BASIC
BUILDING
BLOCKS
NEED
TO
DETERMINE
GIVEN
CURVE
FITTING
COIN
FLIPPING
HEADS
TAILS
BERNOULLI
DISTRIBUTION
N
COIN
FLIPS
BINOMIAL
DISTRIBUTION
ML
FOR
BERNOULLI
GIVEN
EXAMPLE
PREDICTION
ALL
FUTURE
TOSSES
WILL
LAND
HEADS
UP
OVERFITTING
TO
D
DISTRIBUTION
OVER
THE
BETA
DISTRIBUTION
PROVIDES
THE
CONJUGATE
PRIOR
FOR
THE
BERNOULLI
DISTRIBUTION
THE
HYPERPARAMETERS
AN
AND
BN
ARE
THE
EFFECTIVE
NUMBER
OF
OBSERVATIONS
OF
X
AND
X
NEED
NOT
BE
INTEGERS
THE
POSTERIOR
DISTRIBUTION
IN
TURN
CAN
ACT
AS
A
PRIOR
AS
MORE
DATA
IS
OBSERVED
INTERPRETATION
L
N
M
THE
FRACTION
OF
REAL
AND
FICTITIOUS
PRIOR
OBSERVATIONS
CORRESPONDING
TO
X
PRIOR
LIKELIHOOD
POSTERIOR
OF
K
CODING
SCHEME
GIVEN
CONJUGATE
PRIOR
FOR
THE
MULTINOMIAL
DISTRIBUTION
DIAGONAL
COVARIANCE
MATRIX
COVARIANCE
MATRIX
PROPORTIONAL
TO
THE
IDENTITY
MATRIX
GIVEN
I
I
D
DATA
THE
LOG
LIKELI
HOOD
FUNCTION
IS
GIVEN
BY
SET
THE
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
FUNCTION
TO
ZERO
AND
SOLVE
TO
OBTAIN
SIMILARLY
MIXTURES
OF
GAUSSIANS
OLD
FAITHFUL
DATA
SET
SINGLE
GAUSSIAN
MIXTURE
OF
TWO
GAUSSIANS
COMBINE
SIMPLE
MODELS
INTO
A
COMPLEX
MODEL
MIXING
COEFFICIENT
K
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
MARCH
PLAN
FOR
TODAY
AND
NEXT
WEEK
TODAY
AND
NEXT
TIME
BAYESIAN
NETWORKS
BISHOP
SEC
CONDITIONAL
INDEPENDENCE
BISHOP
SEC
NEXT
WEEK
MARKOV
RANDOM
FIELDS
BISHOP
SEC
HIDDEN
MARKOV
MODELS
BISHOP
SEC
EXPECTATION
MAXIMIZATION
BISHOP
CH
GRAPHICAL
MODELS
IF
NO
ASSUMPTION
OF
INDEPENDENCE
IS
MADE
THEN
AN
EXPONENTIAL
NUMBER
OF
PARAMETERS
MUST
BE
ESTIMATED
FOR
SOUND
PROBABILISTIC
INFERENCE
NO
REALISTIC
AMOUNT
OF
TRAINING
DATA
IS
SUFFICIENT
TO
ESTIMATE
SO
MANY
PARAMETERS
IF
A
BLANKET
ASSUMPTION
OF
CONDITIONAL
INDEPENDENCE
IS
MADE
EFFICIENT
TRAINING
AND
INFERENCE
IS
POSSIBLE
BUT
SUCH
A
STRONG
ASSUMPTION
IS
RARELY
WARRANTED
GRAPHICAL
MODELS
USE
DIRECTED
OR
UNDIRECTED
GRAPHS
OVER
A
SET
OF
RANDOM
VARIABLES
TO
EXPLICITLY
SPECIFY
VARIABLE
DEPENDENCIES
AND
ALLOW
FOR
LESS
RESTRICTIVE
INDEPENDENCE
ASSUMPTIONS
WHILE
LIMITING
THE
NUMBER
OF
PARAMETERS
THAT
MUST
BE
ESTIMATED
BAYESIAN
NETWORKS
DIRECTED
ACYCLIC
GRAPHS
INDICATE
CAUSAL
STRUCTURE
MARKOV
NETWORKS
UNDIRECTED
GRAPHS
CAPTURE
GENERAL
DEPENDENCIES
STRUCTURE
LEARNING
LEARN
THE
GRAPHICAL
STRUCTURE
OF
THE
NETWORK
PARAMETER
LEARNING
LEARN
THE
REAL
VALUED
PARAMETERS
OF
THE
NETWORK
CPTS
FOR
BAYES
NETS
POTENTIAL
FUNCTIONS
FOR
MARKOV
NETS
IF
VALUES
FOR
ALL
VARIABLES
ARE
AVAILABLE
DURING
TRAINING
THEN
PARAMETER
ESTIMATES
CAN
BE
DIRECTLY
ESTIMATED
USING
FREQUENCY
COUNTS
OVER
THE
TRAINING
DATA
IF
THERE
ARE
HIDDEN
VARIABLES
SOME
FORM
OF
GRADIENT
DESCENT
OR
EXPECTATION
MAXIMIZATION
EM
MUST
BE
USED
TO
ESTIMATE
DISTRIBUTIONS
FOR
HIDDEN
VARIABLES
DIRECTED
ACYCLIC
GRAPH
DAG
DIRECTED
ACYCLIC
GRAPH
DAG
NODES
ARE
RANDOM
VARIABLES
EDGES
INDICATE
CAUSAL
INFLUENCES
EACH
NODE
HAS
A
CONDITIONAL
PROBABILITY
TABLE
CPT
THAT
GIVES
THE
PROBABILITY
OF
EACH
OF
ITS
VALUES
GIVEN
EVERY
POSSIBLE
COMBINATION
OF
VALUES
FOR
ITS
PARENTS
CONDITIONING
CASE
ROOTS
SOURCES
OF
THE
DAG
THAT
HAVE
NO
PARENTS
ARE
GIVEN
PRIOR
PROBABILITIES
PROBABILITY
OF
FALSE
NOT
GIVEN
SINCE
ROWS
MUST
ADD
TO
EXAMPLE
REQUIRES
PARAMETERS
RATHER
THAN
FOR
SPECIFYING
THE
FULL
JOINT
DISTRIBUTION
NUMBER
OF
PARAMETERS
IN
THE
CPT
FOR
A
NODE
IS
EXPONENTIAL
IN
THE
NUMBER
OF
PARENTS
GIVEN
KNOWN
VALUES
FOR
SOME
EVIDENCE
VARIABLES
DETERMINE
THE
POSTERIOR
PROBABILITY
OF
SOME
QUERY
VARIABLES
EXAMPLE
GIVEN
THAT
JOHN
CALLS
WHAT
IS
THE
PROBABILITY
THAT
THERE
IS
A
BURGLARY
JOHN
CALLS
OF
THE
TIME
THERE
BURGLARY
EARTHQUAKE
ALARM
JOHNCALLS
MARYCALLS
IS
AN
ALARM
AND
THE
ALARM
DETECTS
OF
BURGLARIES
SO
PEOPLE
GENERALLY
THINK
IT
SHOULD
BE
FAIRLY
HIGH
HOWEVER
THIS
IGNORES
THE
PRIOR
PROBABILITY
OF
JOHN
CALLING
EXAMPLE
GIVEN
THAT
JOHN
CALLS
WHAT
IS
THE
PROBABILITY
THAT
THERE
IS
A
BURGLARY
JOHN
ALSO
CALLS
OF
THE
TIME
WHEN
THERE
IS
NO
ALARM
SO
OVER
DAYS
WE
EXPECT
BURGLARY
AND
JOHN
WILL
PROBABLY
CALL
HOWEVER
HE
WILL
ALSO
CALL
WITH
A
FALSE
REPORT
TIMES
ON
AVERAGE
SO
THE
CALL
IS
ABOUT
TIMES
MORE
LIKELY
A
FALSE
REPORT
P
BURGLARY
JOHNCALLS
POLYNOMIAL
PLATE
INPUT
VARIABLES
AND
EXPLICIT
HYPERPARAMETERS
CONDITION
ON
DATA
PREDICTIVE
DISTRIBUTION
WHERE
GENERATIVE
APPROACH
MODEL
USE
BAYES
THEOREM
DISCRIMINATIVE
APPROACH
MODEL
DIRECTLY
CAUSAL
PROCESS
FOR
GENERATING
IMAGES
GENERAL
JOINT
DISTRIBUTION
K
PARAMETERS
INDEPENDENT
JOINT
DISTRIBUTION
K
PARAMETERS
GENERAL
JOINT
DISTRIBUTION
OVER
M
VARIABLES
KM
PARAMETERS
M
NODE
MARKOV
CHAIN
K
M
K
K
PARAMETERS
A
IS
INDEPENDENT
OF
B
GIVEN
C
EQUIVALENTLY
NOTATION
NODE
C
IS
TAIL
TO
TAIL
FOR
PATH
FROM
A
TO
B
PATH
MAKES
A
AND
B
DEPENDENT
NODE
C
IS
TAIL
TO
TAIL
FOR
PATH
FROM
A
TO
B
C
BLOCKS
THE
PATH
THUS
MAKING
A
AND
B
CONDITIONALLY
INDEPENDENT
NODE
C
IS
HEAD
TO
TAIL
FOR
PATH
FROM
A
TO
B
PATH
MAKES
A
AND
B
DEPENDENT
NODE
C
IS
HEAD
TO
TAIL
FOR
PATH
FROM
A
TO
B
C
BLOCKS
THE
PATH
THUS
MAKING
A
AND
B
CONDITIONALLY
INDEPENDENT
NODE
C
IS
HEAD
TO
HEAD
FOR
PATH
FROM
A
TO
B
C
BLOCKS
THE
PATH
THUS
MAKING
A
AND
B
INDEPENDENT
NOTE
THIS
IS
THE
OPPOSITE
OF
EXAMPLE
WITH
C
UNOBSERVED
NODE
C
IS
HEAD
TO
HEAD
FOR
PATH
FROM
A
TO
B
C
UNBLOCKS
THE
PATH
THUS
MAKING
A
AND
B
CONDITIONALLY
DEPENDENT
NOTE
THIS
IS
THE
OPPOSITE
OF
EXAMPLE
WITH
C
OBSERVED
B
BATTERY
FLAT
FULLY
CHARGED
F
FUEL
TANK
EMPTY
FULL
G
FUEL
GAUGE
READING
EMPTY
FULL
PROBABILITY
OF
AN
EMPTY
TANK
INCREASED
BY
OBSERVING
G
PROBABILITY
OF
AN
EMPTY
TANK
REDUCED
BY
OBSERVING
B
THIS
REFERRED
TO
AS
EXPLAINING
AWAY
D
SEPARATION
A
B
AND
C
ARE
NON
INTERSECTING
SUBSETS
OF
NODES
IN
A
DIRECTED
GRAPH
A
PATH
FROM
A
TO
B
IS
BLOCKED
IF
IT
CONTAINS
A
NODE
SUCH
THAT
EITHER
THE
ARROWS
ON
THE
PATH
MEET
EITHER
HEAD
TO
TAIL
OR
TAIL
TO
TAIL
AT
THE
NODE
AND
THE
NODE
IS
IN
THE
SET
C
OR
THE
ARROWS
MEET
HEAD
TO
HEAD
AT
THE
NODE
AND
NEITHER
THE
NODE
NOR
ANY
OF
ITS
DESCENDANTS
ARE
IN
THE
SET
C
IF
ALL
PATHS
FROM
A
TO
B
ARE
BLOCKED
A
IS
SAID
TO
BE
D
SEPARATED
FROM
B
BY
C
IF
A
IS
D
SEPARATED
FROM
B
BY
C
THE
JOINT
DISTRIBUTION
OVER
ALL
VARIABLES
IN
THE
GRAPH
SATISFIES
THE
XI
CONDITIONALLY
INDEPENDENT
ARE
THE
XI
MARGINALLY
INDEPENDENT
CONDITIONED
ON
THE
CLASS
Z
THE
DISTRIBUTIONS
OF
THE
INPUT
VARIABLES
XD
ARE
INDEPENDENT
ARE
THE
XD
MARGINALLY
INDEPENDENT
FACTORS
INDEPENDENT
OF
XI
CANCEL
BETWEEN
NUMERATOR
AND
DENOMINATOR
THE
PARENTS
CHILDREN
AND
CO
PARENTS
OF
XI
FORM
ITS
MARKOV
BLANKET
THE
MINIMAL
SET
OF
NODES
THAT
ISOLATE
XI
FROM
THE
REST
OF
THE
GRAPH
SLIDE
FROM
BISHOP
BAYES
NETS
REPRESENT
A
SUBCLASS
OF
JOINT
DISTRIBUTIONS
THAT
CAPTURE
NON
CYCLIC
CAUSAL
DEPENDENCIES
BETWEEN
VARIABLES
A
MARKOV
NET
CAN
REPRESENT
ANY
JOINT
DISTRIBUTION
SLIDE
CREDIT
RAY
MOONEY
IN
GENERAL
FIRST
ORDER
MARKOV
CHAIN
SECOND
ORDER
MARKOV
CHAIN
UNDIRECTED
GRAPH
OVER
A
SET
OF
RANDOM
VARIABLES
WHERE
AN
EDGE
REPRESENTS
A
DEPENDENCY
THE
MARKOV
BLANKET
OF
A
NODE
X
IN
A
MARKOV
NET
IS
THE
SET
OF
ITS
NEIGHBORS
IN
THE
GRAPH
NODES
THAT
HAVE
AN
EDGE
CONNECTING
TO
X
EVERY
NODE
IN
A
MARKOV
NET
IS
CONDITIONALLY
INDEPENDENT
OF
EVERY
OTHER
NODE
GIVEN
ITS
MARKOV
BLANKET
SLIDE
CREDIT
RAY
MOONEY
MARKOV
BLANKET
A
NODE
IS
CONDITIONALLY
INDEPENDENT
OF
ALL
OTHER
NODES
CONDITIONED
ONLY
ON
THE
NEIGHBORING
NODES
CLIQUE
MAXIMAL
CLIQUE
DISTRIBUTION
FOR
A
MARKOV
NETWORK
THE
DISTRIBUTION
OF
A
MARKOV
NET
IS
MOST
COMPACTLY
DESCRIBED
IN
TERMS
OF
A
SET
OF
POTENTIAL
FUNCTIONS
ΦK
FOR
EACH
CLIQUE
K
IN
THE
GRAPH
FOR
EACH
JOINT
ASSIGNMENT
OF
VALUES
TO
THE
VARIABLES
IN
CLIQUE
K
ΦK
ASSIGNS
A
NON
NEGATIVE
REAL
VALUE
THAT
REPRESENTS
THE
COMPATIBILITY
OF
THESE
VALUES
THE
JOINT
DISTRIBUTION
OF
A
MARKOV
IS
THEN
DEFINED
BY
P
XN
Z
K
K
X
K
WHERE
X
K
REPRESENTS
THE
JOINT
ASSIGNMENT
OF
THE
VARIABLES
IN
CLIQUE
K
AND
Z
IS
A
NORMALIZING
CONSTANT
THAT
MAKES
A
JOINT
DISTRIBUTION
THAT
SUMS
TO
Z
K
X
K
X
K
SLIDE
CREDIT
RAY
MOONEY
ORIGINAL
IMAGE
NOISY
IMAGE
YI
IN
LABELS
IN
OBSERVED
NOISY
IMAGE
XI
IN
LABELS
IN
NOISE
FREE
IMAGE
I
IS
THE
INDEX
OVER
PIXELS
NOISY
IMAGE
RESTORED
IMAGE
ICM
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
MARCH
ALL
SLIDES
ARE
FROM
RAY
MOONEY
MOTIVATING
EXAMPLE
PART
OF
SPEECH
TAGGING
ANNOTATE
EACH
WORD
IN
A
SENTENCE
WITH
A
PART
OF
SPEECH
MARKER
LOWEST
LEVEL
OF
SYNTACTIC
ANALYSIS
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NNP
VBD
DT
NN
CC
VBD
TO
VB
PRP
IN
DT
NN
USEFUL
FOR
SUBSEQUENT
SYNTACTIC
PARSING
AND
WORD
SENSE
DISAMBIGUATION
ENGLISH
PARTS
OF
SPEECH
NOUN
PERSON
PLACE
OR
THING
SINGULAR
NN
DOG
FORK
PLURAL
NNS
DOGS
FORKS
PROPER
NNP
NNPS
JOHN
SPRINGFIELDS
PERSONAL
PRONOUN
PRP
I
YOU
HE
SHE
IT
WH
PRONOUN
WP
WHO
WHAT
VERB
ACTIONS
AND
PROCESSES
BASE
INFINITIVE
VB
EAT
PAST
TENSE
VBD
ATE
GERUND
VBG
EATING
PAST
PARTICIPLE
VBN
EATEN
NON
PERSON
SINGULAR
PRESENT
TENSE
VBP
EAT
PERSON
SINGULAR
PRESENT
TENSE
VBZ
EATS
MODAL
MD
SHOULD
CAN
TO
TO
TO
TO
EAT
ENGLISH
PARTS
OF
SPEECH
CONT
ADJECTIVE
MODIFY
NOUNS
BASIC
JJ
RED
TALL
COMPARATIVE
JJR
REDDER
TALLER
SUPERLATIVE
JJS
REDDEST
TALLEST
ADVERB
MODIFY
VERBS
BASIC
RB
QUICKLY
COMPARATIVE
RBR
QUICKER
SUPERLATIVE
RBS
QUICKEST
PREPOSITION
IN
ON
IN
BY
TO
WITH
DETERMINER
BASIC
DT
A
AN
THE
WH
DETERMINER
WDT
WHICH
THAT
COORDINATING
CONJUNCTION
CC
AND
BUT
OR
PARTICLE
RP
OFF
TOOK
OFF
UP
PUT
UP
AMBIGUITY
IN
POS
TAGGING
LIKE
CAN
BE
A
VERB
OR
A
PREPOSITION
I
LIKE
VBP
CANDY
TIME
FLIES
LIKE
IN
AN
ARROW
AROUND
CAN
BE
A
PREPOSITION
PARTICLE
OR
ADVERB
I
BOUGHT
IT
AT
THE
SHOP
AROUND
IN
THE
CORNER
I
NEVER
GOT
AROUND
RP
TO
GETTING
A
CAR
A
NEW
PRIUS
COSTS
AROUND
RB
CLASSIFICATION
LEARNING
TYPICAL
MACHINE
LEARNING
ADDRESSES
THE
PROBLEM
OF
CLASSIFYING
A
FEATURE
VECTOR
DESCRIPTION
INTO
A
FIXED
NUMBER
OF
CLASSES
THERE
ARE
MANY
STANDARD
LEARNING
METHODS
FOR
THIS
TASK
DECISION
TREES
AND
RULE
LEARNING
NAÏVE
BAYES
AND
BAYESIAN
NETWORKS
LOGISTIC
REGRESSION
MAXIMUM
ENTROPY
MAXENT
PERCEPTRON
AND
NEURAL
NETWORKS
SUPPORT
VECTOR
MACHINES
SVMS
NEAREST
NEIGHBOR
INSTANCE
BASED
BEYOND
CLASSIFICATION
LEARNING
STANDARD
CLASSIFICATION
PROBLEM
ASSUMES
INDIVIDUAL
CASES
ARE
DISCONNECTED
AND
INDEPENDENT
I
I
D
INDEPENDENTLY
AND
IDENTICALLY
DISTRIBUTED
MANY
NLP
PROBLEMS
DO
NOT
SATISFY
THIS
ASSUMPTION
AND
INVOLVE
MAKING
MANY
CONNECTED
DECISIONS
EACH
RESOLVING
A
DIFFERENT
AMBIGUITY
BUT
WHICH
ARE
MUTUALLY
DEPENDENT
MORE
SOPHISTICATED
LEARNING
AND
INFERENCE
TECHNIQUES
ARE
NEEDED
TO
HANDLE
SUCH
SITUATIONS
IN
GENERAL
SEQUENCE
LABELING
PROBLEM
MANY
NLP
PROBLEMS
CAN
VIEWED
AS
SEQUENCE
LABELING
EACH
TOKEN
IN
A
SEQUENCE
IS
ASSIGNED
A
LABEL
LABELS
OF
TOKENS
ARE
DEPENDENT
ON
THE
LABELS
OF
OTHER
TOKENS
IN
THE
SEQUENCE
PARTICULARLY
THEIR
NEIGHBORS
NOT
I
I
D
FOO
BAR
BLAM
ZONK
ZONK
BAR
BLAM
IDENTIFY
PHRASES
IN
LANGUAGE
THAT
REFER
TO
SPECIFIC
TYPES
OF
ENTITIES
AND
RELATIONS
IN
TEXT
NAMED
ENTITY
RECOGNITION
IS
TASK
OF
IDENTIFYING
NAMES
OF
PEOPLE
PLACES
ORGANIZATIONS
ETC
IN
TEXT
PEOPLE
ORGANIZATIONS
PLACES
MICHAEL
DELL
IS
THE
CEO
OF
DELL
COMPUTER
CORPORATION
AND
LIVES
IN
AUSTIN
TEXAS
EXTRACT
PIECES
OF
INFORMATION
RELEVANT
TO
A
SPECIFIC
APPLICATION
E
G
USED
CAR
ADS
MAKE
MODEL
YEAR
MILEAGE
PRICE
FOR
SALE
TOYOTA
PRIUS
MI
OR
BEST
OFFER
AVAILABLE
STARTING
JULY
FOR
EACH
CLAUSE
DETERMINE
THE
SEMANTIC
ROLE
PLAYED
BY
EACH
NOUN
PHRASE
THAT
IS
AN
ARGUMENT
TO
THE
VERB
AGENT
PATIENT
SOURCE
DESTINATION
INSTRUMENT
JOHN
DROVE
MARY
FROM
AUSTIN
TO
DALLAS
IN
HIS
TOYOTA
PRIUS
THE
HAMMER
BROKE
THE
WINDOW
ALSO
REFERRED
TO
A
CASE
ROLE
ANALYSIS
THEMATIC
ANALYSIS
AND
SHALLOW
SEMANTIC
PARSING
SEQUENCE
LABELING
ALSO
VALUABLE
IN
LABELING
GENETIC
SEQUENCES
IN
GENOME
ANALYSIS
EXTRON
INTRON
AGCTAACGTTCGATACGGATTACAGCCT
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NNP
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VBD
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
DT
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NN
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
CC
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VBD
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
TO
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VB
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
PRP
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
IN
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
DT
CLASSIFY
EACH
TOKEN
INDEPENDENTLY
BUT
USE
AS
INPUT
FEATURES
INFORMATION
ABOUT
THE
SURROUNDING
TOKENS
SLIDING
WINDOW
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NN
SEQUENCE
LABELING
AS
CLASSIFICATION
USING
OUTPUTS
AS
INPUTS
BETTER
INPUT
FEATURES
ARE
USUALLY
THE
CATEGORIES
OF
THE
SURROUNDING
TOKENS
BUT
THESE
ARE
NOT
AVAILABLE
YET
CAN
USE
CATEGORY
OF
EITHER
THE
PRECEDING
OR
SUCCEEDING
TOKENS
BY
GOING
FORWARD
OR
BACK
AND
USING
PREVIOUS
OUTPUT
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NNP
NNP
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VBD
NNP
VBD
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
DT
NNP
VBD
DT
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NN
NNP
VBD
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
CC
NNP
VBD
DT
NN
CC
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VBD
NNP
VBD
DT
NN
CC
VBD
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
TO
NNP
VBD
DT
NN
CC
VBD
TO
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VB
NNP
VBD
DT
NN
CC
VBD
TO
VB
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
PRP
NNP
VBD
DT
NN
CC
VBD
TO
VB
PRP
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
IN
NNP
VBD
DT
NN
CC
VBD
TO
VB
PRP
IN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
DT
NNP
VBD
DT
NN
CC
VBD
TO
VB
PRP
IN
DT
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NN
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NN
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
DT
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
IN
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
PRP
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
PRP
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VB
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
VB
PRP
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
TO
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
TO
VB
PRP
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VBD
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
VBD
TO
VB
PRP
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
CC
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
CC
VBD
TO
VB
PRP
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NN
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
VBD
CC
VBD
TO
VB
PRP
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
DT
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
DT
VBD
CC
VBD
TO
VB
PRP
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
VBD
DISAMBIGUATING
TO
IN
THIS
CASE
WOULD
BE
EVEN
EASIER
BACKWARD
VBD
DT
VBD
CC
VBD
TO
VB
PRP
IN
DT
NN
JOHN
SAW
THE
SAW
AND
DECIDED
TO
TAKE
IT
TO
THE
TABLE
NNP
PROBLEMS
WITH
SEQUENCE
LABELING
AS
CLASSIFICATION
NOT
EASY
TO
INTEGRATE
INFORMATION
FROM
CATEGORY
OF
TOKENS
ON
BOTH
SIDES
DIFFICULT
TO
PROPAGATE
UNCERTAINTY
BETWEEN
DECISIONS
AND
COLLECTIVELY
DETERMINE
THE
MOST
LIKELY
JOINT
ASSIGNMENT
OF
CATEGORIES
TO
ALL
OF
THE
TOKENS
IN
A
SEQUENCE
PROBABILISTIC
SEQUENCE
MODELS
ALLOW
INTEGRATING
UNCERTAINTY
OVER
MULTIPLE
INTERDEPENDENT
CLASSIFICATIONS
AND
COLLECTIVELY
DETERMINE
THE
MOST
LIKELY
GLOBAL
ASSIGNMENT
TWO
STANDARD
MODELS
HIDDEN
MARKOV
MODEL
HMM
CONDITIONAL
RANDOM
FIELD
CRF
A
FINITE
STATE
MACHINE
WITH
PROBABILISTIC
STATE
TRANSITIONS
MAKES
MARKOV
ASSUMPTION
THAT
NEXT
STATE
ONLY
DEPENDS
ON
THE
CURRENT
STATE
AND
INDEPENDENT
OF
PREVIOUS
HISTORY
DET
NOUN
STOP
START
PROPNOUN
VERB
DET
NOUN
STOP
START
PROPNOUN
VERB
P
PROPNOUN
VERB
DET
NOUN
PROBABILISTIC
GENERATIVE
MODEL
FOR
SEQUENCES
ASSUME
AN
UNDERLYING
SET
OF
HIDDEN
UNOBSERVED
STATES
IN
WHICH
THE
MODEL
CAN
BE
E
G
PARTS
OF
SPEECH
ASSUME
PROBABILISTIC
TRANSITIONS
BETWEEN
STATES
OVER
TIME
E
G
TRANSITION
FROM
POS
TO
ANOTHER
POS
AS
SEQUENCE
IS
GENERATED
ASSUME
A
PROBABILISTIC
GENERATION
OF
TOKENS
FROM
STATES
E
G
WORDS
GENERATED
FOR
EACH
POS
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
START
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
START
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
START
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
START
JOHN
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
START
JOHN
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
START
JOHN
BIT
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
START
JOHN
BIT
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
START
JOHN
BIT
THE
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
START
JOHN
BIT
THE
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
START
JOHN
BIT
THE
APPLE
THE
CAT
A
A
THE
DOG
BED
THE
A
THE
CAR
APPLE
THAT
DET
PEN
NOUN
BIT
SAW
PLAYED
STOP
TOM
HIT
GAVE
JOHNMARY
ALICE
JERRY
PROPNOUN
VERB
START
JOHN
BIT
THE
APPLE
A
SET
OF
N
STATES
SN
SF
DISTINGUISHED
START
STATE
DISTINGUISHED
FINAL
STATE
SF
A
SET
OF
M
POSSIBLE
OBSERVATIONS
V
VM
A
STATE
TRANSITION
PROBABILITY
DISTRIBUTION
A
AIJ
AIJ
N
P
QT
J
QT
SI
I
J
N
AND
I
J
F
AIJ
J
AIF
I
N
OBSERVATION
PROBABILITY
DISTRIBUTION
FOR
EACH
STATE
J
B
BJ
K
BJ
K
P
VK
AT
T
QT
J
J
N
K
M
TOTAL
PARAMETER
SET
Λ
A
B
TO
GENERATE
A
SEQUENCE
OF
T
OBSERVATIONS
O
OT
SET
INITIAL
STATE
FOR
T
TO
T
TRANSIT
TO
ANOTHER
STATE
QT
SJ
BASED
ON
TRANSITION
DISTRIBUTION
AIJ
FOR
STATE
QT
PICK
AN
OBSERVATION
OT
VK
BASED
ON
BEING
IN
STATE
QT
USING
DISTRIBUTION
BQT
K
OBSERVATION
LIKELIHOOD
TO
CLASSIFY
AND
ORDER
SEQUENCES
MOST
LIKELY
STATE
SEQUENCE
DECODING
TO
TAG
EACH
TOKEN
IN
A
SEQUENCE
WITH
A
LABEL
MAXIMUM
LIKELIHOOD
TRAINING
LEARNING
TO
TRAIN
MODELS
TO
FIT
EMPIRICAL
TRAINING
DATA
GIVEN
A
SEQUENCE
OF
OBSERVATIONS
O
AND
A
MODEL
WITH
A
SET
OF
PARAMETERS
Λ
WHAT
IS
THE
PROBABILITY
THAT
THIS
OBSERVATION
WAS
GENERATED
BY
THIS
MODEL
P
O
Λ
ALLOWS
HMM
TO
BE
USED
AS
A
LANGUAGE
MODEL
A
FORMAL
PROBABILISTIC
MODEL
OF
A
LANGUAGE
THAT
ASSIGNS
A
PROBABILITY
TO
EACH
STRING
SAYING
HOW
LIKELY
THAT
STRING
WAS
TO
HAVE
BEEN
GENERATED
BY
THE
LANGUAGE
USEFUL
FOR
TWO
TASKS
SEQUENCE
CLASSIFICATION
MOST
LIKELY
SEQUENCE
ASSUME
AN
HMM
IS
AVAILABLE
FOR
EACH
CATEGORY
I
E
LANGUAGE
WHAT
IS
THE
MOST
LIKELY
CATEGORY
FOR
A
GIVEN
OBSERVATION
SEQUENCE
I
E
WHICH
CATEGORY
HMM
IS
MOST
LIKELY
TO
HAVE
GENERATED
IT
USED
IN
SPEECH
RECOGNITION
TO
FIND
MOST
LIKELY
WORD
MODEL
TO
HAVE
GENERATE
A
GIVEN
SOUND
OR
PHONEME
SEQUENCE
AUSTIN
P
O
AUSTIN
P
O
BOSTON
BOSTON
OF
TWO
OR
MORE
POSSIBLE
SEQUENCES
WHICH
ONE
WAS
MOST
LIKELY
GENERATED
BY
A
GIVEN
MODEL
USED
TO
SCORE
ALTERNATIVE
WORD
SEQUENCE
INTERPRETATIONS
IN
SPEECH
RECOGNITION
ORDINARY
ENGLISH
P
O
ORDENGLISH
P
O
ORDENGLISH
NAÏVE
SOLUTION
CONSIDER
ALL
POSSIBLE
STATE
SEQUENCES
Q
OF
LENGTH
T
THAT
THE
MODEL
COULD
HAVE
TRAVERSED
IN
GENERATING
THE
GIVEN
OBSERVATION
SEQUENCE
COMPUTE
THE
PROBABILITY
OF
A
GIVEN
STATE
SEQUENCE
FROM
A
AND
MULTIPLY
IT
BY
THE
PROBABILITIES
OF
GENERATING
EACH
OF
GIVEN
OBSERVATIONS
IN
EACH
OF
THE
CORRESPONDING
STATES
IN
THIS
SEQUENCE
TO
GET
P
O
Q
Λ
P
O
Q
Λ
P
Q
Λ
SUM
THIS
OVER
ALL
POSSIBLE
STATE
SEQUENCES
TO
GET
P
O
Λ
COMPUTATIONALLY
COMPLEX
O
TNT
EFFICIENT
SOLUTION
DUE
TO
THE
MARKOV
ASSUMPTION
THE
PROBABILITY
OF
BEING
IN
ANY
STATE
AT
ANY
GIVEN
TIME
T
ONLY
RELIES
ON
THE
PROBABILITY
OF
BEING
IN
EACH
OF
THE
POSSIBLE
STATES
AT
TIME
T
FORWARD
ALGORITHM
USES
DYNAMIC
PROGRAMMING
TO
EXPLOIT
THIS
FACT
TO
EFFICIENTLY
COMPUTE
OBSERVATION
LIKELIHOOD
IN
O
TIME
COMPUTE
A
FORWARD
TRELLIS
THAT
COMPACTLY
AND
IMPLICITLY
ENCODES
INFORMATION
ABOUT
ALL
POSSIBLE
STATE
PATHS
LET
T
J
BE
THE
PROBABILITY
OF
BEING
IN
STATE
J
AFTER
SEEING
THE
FIRST
T
OBSERVATIONS
BY
SUMMING
OVER
ALL
INITIAL
PATHS
LEADING
TO
J
T
J
P
OT
QT
J
CONSIDER
ALL
POSSIBLE
WAYS
OF
GETTING
TO
SJ
AT
TIME
T
BY
COMING
FROM
ALL
POSSIBLE
STATES
SI
AND
DETERMINE
PROBABILITY
OF
EACH
J
SUM
THESE
TO
GET
THE
TOTAL
PROBABILITY
OF
BEING
IN
STATE
SJ
AT
N
TIME
T
WHILE
ACCOUNTING
FOR
THE
T
I
T
I
FIRST
T
OBSERVATIONS
THEN
MULTIPLY
BY
THE
PROBABILITY
OF
ACTUALLY
OBSERVING
OT
IN
SJ
SF
TT
TT
CONTINUE
FORWARD
IN
TIME
UNTIL
REACHING
FINAL
TIME
POINT
AND
SUM
PROBABILITY
OF
ENDING
IN
FINAL
STATE
INITIALIZATION
J
RECURSION
JBJ
J
N
J
N
I
A
B
O
J
N
T
T
T
I
T
IJ
J
T
TERMINATION
N
P
O
T
SF
T
I
AIF
REQUIRES
ONLY
O
TIME
TO
COMPUTE
THE
PROBABILITY
OF
AN
OBSERVED
SEQUENCE
GIVEN
A
MODEL
EXPLOITS
THE
FACT
THAT
ALL
STATE
SEQUENCES
MUST
MERGE
INTO
ONE
OF
THE
N
POSSIBLE
STATES
AT
ANY
POINT
IN
TIME
AND
THE
MARKOV
ASSUMPTION
THAT
ONLY
THE
LAST
STATE
EFFECTS
THE
NEXT
ONE
GIVEN
AN
OBSERVATION
SEQUENCE
O
AND
A
MODEL
Λ
WHAT
IS
THE
MOST
LIKELY
STATE
SEQUENCE
Q
QT
THAT
GENERATED
THIS
SEQUENCE
FROM
THIS
MODEL
USED
FOR
SEQUENCE
LABELING
ASSUMING
EACH
STATE
CORRESPONDS
TO
A
TAG
IT
DETERMINES
THE
GLOBALLY
BEST
ASSIGNMENT
OF
TAGS
TO
ALL
TOKENS
IN
A
SEQUENCE
USING
A
PRINCIPLED
APPROACH
GROUNDED
IN
PROBABILITY
THEORY
GIVEN
AN
OBSERVATION
SEQUENCE
O
AND
A
MODEL
Λ
WHAT
IS
THE
MOST
LIKELY
STATE
SEQUENCE
Q
QT
THAT
GENERATED
THIS
SEQUENCE
FROM
THIS
MODEL
USED
FOR
SEQUENCE
LABELING
ASSUMING
EACH
STATE
CORRESPONDS
TO
A
TAG
IT
DETERMINES
THE
GLOBALLY
BEST
ASSIGNMENT
OF
TAGS
TO
ALL
TOKENS
IN
A
SEQUENCE
USING
A
PRINCIPLED
APPROACH
GROUNDED
IN
PROBABILITY
THEORY
GIVEN
AN
OBSERVATION
SEQUENCE
O
AND
A
MODEL
Λ
WHAT
IS
THE
MOST
LIKELY
STATE
SEQUENCE
Q
QT
THAT
GENERATED
THIS
SEQUENCE
FROM
THIS
MODEL
USED
FOR
SEQUENCE
LABELING
ASSUMING
EACH
STATE
CORRESPONDS
TO
A
TAG
IT
DETERMINES
THE
GLOBALLY
BEST
ASSIGNMENT
OF
TAGS
TO
ALL
TOKENS
IN
A
SEQUENCE
USING
A
PRINCIPLED
APPROACH
GROUNDED
IN
PROBABILITY
THEORY
GIVEN
AN
OBSERVATION
SEQUENCE
O
AND
A
MODEL
Λ
WHAT
IS
THE
MOST
LIKELY
STATE
SEQUENCE
Q
QT
THAT
GENERATED
THIS
SEQUENCE
FROM
THIS
MODEL
USED
FOR
SEQUENCE
LABELING
ASSUMING
EACH
STATE
CORRESPONDS
TO
A
TAG
IT
DETERMINES
THE
GLOBALLY
BEST
ASSIGNMENT
OF
TAGS
TO
ALL
TOKENS
IN
A
SEQUENCE
USING
A
PRINCIPLED
APPROACH
GROUNDED
IN
PROBABILITY
THEORY
GIVEN
AN
OBSERVATION
SEQUENCE
O
AND
A
MODEL
Λ
WHAT
IS
THE
MOST
LIKELY
STATE
SEQUENCE
Q
QT
THAT
GENERATED
THIS
SEQUENCE
FROM
THIS
MODEL
USED
FOR
SEQUENCE
LABELING
ASSUMING
EACH
STATE
CORRESPONDS
TO
A
TAG
IT
DETERMINES
THE
GLOBALLY
BEST
ASSIGNMENT
OF
TAGS
TO
ALL
TOKENS
IN
A
SEQUENCE
USING
A
PRINCIPLED
APPROACH
GROUNDED
IN
PROBABILITY
THEORY
GIVEN
AN
OBSERVATION
SEQUENCE
O
AND
A
MODEL
Λ
WHAT
IS
THE
MOST
LIKELY
STATE
SEQUENCE
Q
QT
THAT
GENERATED
THIS
SEQUENCE
FROM
THIS
MODEL
USED
FOR
SEQUENCE
LABELING
ASSUMING
EACH
STATE
CORRESPONDS
TO
A
TAG
IT
DETERMINES
THE
GLOBALLY
BEST
ASSIGNMENT
OF
TAGS
TO
ALL
TOKENS
IN
A
SEQUENCE
USING
A
PRINCIPLED
APPROACH
GROUNDED
IN
PROBABILITY
THEORY
GIVEN
AN
OBSERVATION
SEQUENCE
O
AND
A
MODEL
Λ
WHAT
IS
THE
MOST
LIKELY
STATE
SEQUENCE
Q
QT
THAT
GENERATED
THIS
SEQUENCE
FROM
THIS
MODEL
USED
FOR
SEQUENCE
LABELING
ASSUMING
EACH
STATE
CORRESPONDS
TO
A
TAG
IT
DETERMINES
THE
GLOBALLY
BEST
ASSIGNMENT
OF
TAGS
TO
ALL
TOKENS
IN
A
SEQUENCE
USING
A
PRINCIPLED
APPROACH
GROUNDED
IN
PROBABILITY
THEORY
HMM
MOST
LIKELY
STATE
SEQUENCE
EFFICIENT
SOLUTION
OBVIOUSLY
COULD
USE
NAÏVE
ALGORITHM
BASED
ON
EXAMINING
EVERY
POSSIBLE
STATE
SEQUENCE
OF
LENGTH
T
DYNAMIC
PROGRAMMING
CAN
ALSO
BE
USED
TO
EXPLOIT
THE
MARKOV
ASSUMPTION
AND
EFFICIENTLY
DETERMINE
THE
MOST
LIKELY
STATE
SEQUENCE
FOR
A
GIVEN
OBSERVATION
AND
MODEL
STANDARD
PROCEDURE
IS
CALLED
THE
VITERBI
ALGORITHM
VITERBI
AND
ALSO
HAS
O
TIME
COMPLEXITY
VITERBI
SCORES
RECURSIVELY
COMPUTE
THE
PROBABILITY
OF
THE
MOST
LIKELY
SUBSEQUENCE
OF
STATES
THAT
ACCOUNTS
FOR
THE
FIRST
T
OBSERVATIONS
AND
ENDS
IN
STATE
SJ
VT
J
MAX
QT
P
QT
OT
QT
J
ALSO
RECORD
BACKPOINTERS
THAT
SUBSEQUENTLY
ALLOW
BACKTRACING
THE
MOST
PROBABLE
STATE
SEQUENCE
BTT
J
STORES
THE
STATE
AT
TIME
T
THAT
MAXIMIZES
THE
PROBABILITY
THAT
SYSTEM
WAS
IN
STATE
SJ
AT
TIME
T
GIVEN
THE
OBSERVED
SEQUENCE
COMPUTING
THE
VITERBI
SCORES
INITIALIZATION
J
RECURSION
N
JBJ
J
N
VT
J
MAX
I
VT
I
AIJBJ
OT
J
N
T
T
TERMINATION
N
P
VT
SF
MAX
I
VT
I
AIF
ANALOGOUS
TO
FORWARD
ALGORITHM
EXCEPT
TAKE
MAX
INSTEAD
OF
SUM
COMPUTING
THE
VITERBI
BACKPOINTERS
INITIALIZATION
J
J
N
RECURSION
N
BTT
J
ARGMAX
I
VT
I
AIJBJ
OT
J
N
T
T
TERMINATION
N
QT
BTT
SF
ARGMAX
I
VT
I
AIF
FINAL
STATE
IN
THE
MOST
PROBABLE
STATE
SEQUENCE
FOLLOW
BACKPOINTERS
TO
INITIAL
STATE
TO
CONSTRUCT
FULL
SEQUENCE
SF
TT
TT
SF
TT
TT
MOST
LIKELY
SEQUENCE
SN
SF
SUPERVISED
LEARNING
ALL
TRAINING
SEQUENCES
ARE
COMPLETELY
LABELED
TAGGED
UNSUPERVISED
LEARNING
ALL
TRAINING
SEQUENCES
ARE
UNLABELLED
BUT
GENERALLY
KNOW
THE
NUMBER
OF
TAGS
I
E
STATES
SEMISUPERVISED
LEARNING
SOME
TRAINING
SEQUENCES
ARE
LABELED
MOST
ARE
UNLABELED
IF
TRAINING
SEQUENCES
ARE
LABELED
TAGGED
WITH
THE
UNDERLYING
STATE
SEQUENCES
THAT
GENERATED
THEM
THEN
THE
PARAMETERS
Λ
A
B
CAN
ALL
BE
ESTIMATED
DIRECTLY
TRAINING
SEQUENCES
DET
NOUN
PROPNOUN
VERB
ESTIMATE
STATE
TRANSITION
PROBABILITIES
BASED
ON
TAG
BIGRAM
AND
UNIGRAM
STATISTICS
IN
THE
LABELED
DATA
A
C
QT
SI
QT
J
C
QT
SI
ESTIMATE
THE
OBSERVATION
PROBABILITIES
BASED
ON
TAG
WORD
CO
OCCURRENCE
STATISTICS
IN
THE
LABELED
DATA
B
K
C
QI
J
OI
VK
C
QI
J
USE
APPROPRIATE
SMOOTHING
IF
TRAINING
DATA
IS
SPARSE
USE
A
CORPUS
OF
LABELED
SEQUENCE
DATA
TO
EASILY
CONSTRUCT
AN
HMM
USING
SUPERVISED
TRAINING
GIVEN
A
NOVEL
UNLABELED
TEST
SEQUENCE
TO
TAG
USE
THE
VITERBI
ALGORITHM
TO
PREDICT
THE
MOST
LIKELY
GLOBALLY
OPTIMAL
TAG
SEQUENCE
UNSUPERVISED
MAXIMUM
LIKELIHOOD
TRAINING
TRAINING
SEQUENCES
AUSTIN
MAXIMUM
LIKELIHOOD
TRAINING
GIVEN
AN
OBSERVATION
SEQUENCE
O
WHAT
SET
OF
PARAMETERS
Λ
FOR
A
GIVEN
MODEL
MAXIMIZES
THE
PROBABILITY
THAT
THIS
DATA
WAS
GENERATED
FROM
THIS
MODEL
P
O
Λ
USED
TO
TRAIN
AN
HMM
MODEL
AND
PROPERLY
INDUCE
ITS
PARAMETERS
FROM
A
SET
OF
TRAINING
DATA
ONLY
NEED
TO
HAVE
AN
UNANNOTATED
OBSERVATION
SEQUENCE
OR
SET
OF
SEQUENCES
GENERATED
FROM
THE
MODEL
DOES
NOT
NEED
TO
KNOW
THE
CORRECT
STATE
SEQUENCE
FOR
THE
OBSERVATION
SEQUENCE
IN
THIS
SENSE
IT
IS
UNSUPERVISED
HMM
MAXIMUM
LIKELIHOOD
TRAINING
EFFICIENT
SOLUTION
THERE
IS
NO
KNOWN
EFFICIENT
ALGORITHM
FOR
FINDING
THE
PARAMETERS
Λ
THAT
TRULY
MAXIMIZES
P
O
Λ
HOWEVER
USING
ITERATIVE
RE
ESTIMATION
THE
BAUM
WELCH
ALGORITHM
A
K
A
FORWARD
BACKWARD
A
VERSION
OF
A
STANDARD
STATISTICAL
PROCEDURE
CALLED
EXPECTATION
MAXIMIZATION
EM
IS
ABLE
TO
LOCALLY
MAXIMIZE
P
O
Λ
IN
PRACTICE
EM
IS
ABLE
TO
FIND
A
GOOD
SET
OF
PARAMETERS
THAT
PROVIDE
A
GOOD
FIT
TO
THE
TRAINING
DATA
IN
MANY
CASES
EM
ALGORITHM
ITERATIVE
METHOD
FOR
LEARNING
PROBABILISTIC
CATEGORIZATION
MODEL
FROM
UNSUPERVISED
DATA
INITIALLY
ASSUME
RANDOM
ASSIGNMENT
OF
EXAMPLES
TO
CATEGORIES
LEARN
AN
INITIAL
PROBABILISTIC
MODEL
BY
ESTIMATING
MODEL
PARAMETERS
FROM
THIS
RANDOMLY
LABELED
DATA
ITERATE
FOLLOWING
TWO
STEPS
UNTIL
CONVERGENCE
EXPECTATION
E
STEP
COMPUTE
P
CI
E
FOR
EACH
EXAMPLE
GIVEN
THE
CURRENT
MODEL
AND
PROBABILISTICALLY
RE
LABEL
THE
EXAMPLES
BASED
ON
THESE
POSTERIOR
PROBABILITY
ESTIMATES
MAXIMIZATION
M
STEP
RE
ESTIMATE
THE
MODEL
PARAMETERS
FROM
THE
PROBABILISTICALLY
RE
LABELED
DATA
INITIALIZE
ASSIGN
RANDOM
PROBABILISTIC
LABELS
TO
UNLABELED
DATA
UNLABELED
EXAMPLES
INITIALIZE
GIVE
SOFT
LABELED
TRAINING
DATA
TO
A
PROBABILISTIC
LEARNER
INITIALIZE
PRODUCE
A
PROBABILISTIC
CLASSIFIER
E
STEP
RELABEL
UNLABELED
DATA
USING
THE
TRAINED
CLASSIFIER
M
STEP
RETRAIN
CLASSIFIER
ON
RELABELED
DATA
CONTINUE
EM
ITERATIONS
UNTIL
PROBABILISTIC
LABELS
ON
UNLABELED
DATA
CONVERGE
SKETCH
OF
BAUM
WELCH
EM
ALGORITHM
FOR
TRAINING
HMMS
ASSUME
AN
HMM
WITH
N
STATES
RANDOMLY
SET
ITS
PARAMETERS
Λ
A
B
MAKING
SURE
THEY
REPRESENT
LEGAL
DISTRIBUTIONS
UNTIL
CONVERGE
I
E
Λ
NO
LONGER
CHANGES
DO
E
STEP
USE
THE
FORWARD
BACKWARD
PROCEDURE
TO
DETERMINE
THE
PROBABILITY
OF
VARIOUS
POSSIBLE
STATE
SEQUENCES
FOR
GENERATING
THE
TRAINING
DATA
M
STEP
USE
THESE
PROBABILITY
ESTIMATES
TO
RE
ESTIMATE
VALUES
FOR
ALL
OF
THE
PARAMETERS
Λ
BACKWARD
PROBABILITIES
LET
T
I
BE
THE
PROBABILITY
OF
OBSERVING
THE
FINAL
SET
OF
OBSERVATIONS
FROM
TIME
T
TO
T
GIVEN
THAT
ONE
IS
IN
STATE
I
AT
TIME
T
T
I
P
OT
OT
OT
QT
SI
COMPUTING
THE
BACKWARD
PROBABILITIES
INITIALIZATION
T
I
AIF
I
N
RECURSION
N
T
I
AIJBJ
OT
T
J
J
I
N
T
T
TERMINATION
N
P
O
T
SF
JBJ
J
J
ESTIMATING
PROBABILITY
OF
STATE
TRANSITIONS
LET
T
I
J
BE
THE
PROBABILITY
OF
BEING
IN
STATE
I
AT
TIME
T
AND
STATE
J
AT
TIME
T
T
I
J
P
QT
SI
QT
J
O
I
P
QT
SI
QT
J
O
T
I
AIJBJ
OT
T
J
T
P
O
P
O
A
B
O
J
ANI
SN
T
I
IJ
J
T
SN
T
T
T
T
RE
ESTIMATING
A
Aˆ
EXPECTED
NUMBER
OF
TRANSITIO
NS
FROM
STATE
I
TO
J
IJ
EXPECTED
NUMBER
T
OF
TRANSITIO
NS
FROM
STATE
I
T
I
J
AˆIJ
T
T
N
T
I
J
T
J
ESTIMATING
OBSERVATION
PROBABILITIES
LET
T
I
BE
THE
PROBABILITY
OF
BEING
IN
STATE
I
AT
TIME
T
GIVEN
THE
OBSERVATIONS
AND
THE
MODEL
J
P
Q
O
P
QT
J
O
T
J
T
J
T
T
J
P
O
P
O
RE
ESTIMATING
B
Bˆ
V
EXPECTED
NUMBER
OF
TIMES
IN
STATE
J
OBSERVING
VK
J
K
EXPECTED
NUMBER
OF
TIMES
IN
STATE
J
BˆJ
VK
T
J
T
T
OT
VK
T
T
J
T
PSEUDOCODE
FOR
BAUM
WELCH
EM
ALGORITHM
FOR
TRAINING
HMMS
ASSUME
AN
HMM
WITH
N
STATES
RANDOMLY
SET
ITS
PARAMETERS
Λ
A
B
MAKING
SURE
THEY
REPRESENT
LEGAL
DISTRIBUTIONS
UNTIL
CONVERGE
I
E
Λ
NO
LONGER
CHANGES
DO
E
STEP
M
STEP
COMPUTE
VALUES
FOR
T
J
AND
T
I
J
USING
CURRENT
VALUES
FOR
PARAMETERS
A
AND
B
RE
ESTIMATE
PARAMETERS
AIJ
AˆIJ
BJ
VK
BˆJ
VK
EM
PROPERTIES
EACH
ITERATION
CHANGES
THE
PARAMETERS
IN
A
WAY
THAT
IS
GUARANTEED
TO
INCREASE
THE
LIKELIHOOD
OF
THE
DATA
P
O
ANYTIME
ALGORITHM
CAN
STOP
AT
ANY
TIME
PRIOR
TO
CONVERGENCE
TO
GET
APPROXIMATE
SOLUTION
CONVERGES
TO
A
LOCAL
MAXIMUM
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
MARCH
MIXTURES
OF
GAUSSIANS
FORM
LET
Z
LATENT
VARIABLE
BE
OF
K
REPRESENTATION
THEN
RESPONSIBILITY
OF
COMPONENT
K
FOR
EXPLAINING
X
GENERATING
SAMPLES
SAMPLE
VALUE
Z
FROM
P
Z
THEN
SAMPLE
A
VALUE
FOR
X
FROM
P
X
Z
COLOR
GENERATED
SAMPLES
USING
Z
LEFT
COLOR
SAMPLES
USING
RESPONSIBILITIES
RIGHT
FINDING
PARAMETERS
OF
MIXTURE
WANT
TO
MAXIMIZE
SET
DERIVATIVE
WITH
RESPECT
TO
MEANS
TO
GET
FINDING
PARAMETERS
OF
MIXTURE
SET
DERIVATIVE
WRT
COVARIANCE
TO
SET
DERIVATIVE
WRT
MIXING
COEFFICIENTS
TO
REMINDER
RESPONSIBILITIES
SO
PARAMETERS
OF
GAUSSIAN
DEPEND
ON
RESPONSIBILITIES
AND
VICE
VERSA
REMEMBER
K
MEANS
ITERATIVE
ALGORITHM
FROM
BISHOP
FROM
BISHOP
A
LB
C
D
E
F
FROM
BISHOP
GENERAL
ALGORITHM
CS
MACHINE
LEARNING
ENSEMBLES
BAGGING
AND
BOOSTING
DECISION
TREES
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
APRIL
PLAN
FOR
TODAY
ENSEMBLE
METHODS
BAGGING
BOOSTING
BOOSTING
APPLICATION
FACE
DETECTION
DECISION
TREES
LEARN
MULTIPLE
ALTERNATIVE
DEFINITIONS
OF
A
CONCEPT
USING
DIFFERENT
TRAINING
DATA
OR
DIFFERENT
LEARNING
ALGORITHMS
TRAIN
SEVERAL
CLASSIFIERS
SVM
KNN
LOGISTIC
REGRESSION
DECISION
TREE
NEURAL
NETWORK
ETC
CALL
THESE
CLASSIFIERS
X
X
FM
X
TAKE
MAJORITY
OF
PREDICTIONS
Y
MAJORITY
X
X
FM
X
FOR
REGRESSION
USE
MEAN
OR
MEDIAN
OF
THE
PREDICTIONS
AVERAGING
IS
A
FORM
OF
REGULARIZATION
EACH
MODEL
CAN
INDIVIDUALLY
OVERFIT
BUT
THE
AVERAGE
IS
ABLE
TO
OVERCOME
THE
OVERFITTING
LEARN
MULTIPLE
ALTERNATIVE
DEFINITIONS
OF
A
CONCEPT
USING
DIFFERENT
TRAINING
DATA
OR
DIFFERENT
LEARNING
ALGORITHMS
COMBINE
DECISIONS
OF
MULTIPLE
DEFINITIONS
WHEN
COMBING
MULTIPLE
INDEPENDENT
AND
DIVERSE
DECISIONS
EACH
OF
WHICH
IS
AT
LEAST
MORE
ACCURATE
THAN
RANDOM
GUESSING
RANDOM
ERRORS
CANCEL
EACH
OTHER
OUT
CORRECT
DECISIONS
ARE
REINFORCED
HUMAN
ENSEMBLES
ARE
DEMONSTRABLY
BETTER
HOW
MANY
JELLY
BEANS
IN
THE
JAR
INDIVIDUAL
ESTIMATES
VS
GROUP
AVERAGE
WHO
WANTS
TO
BE
A
MILLIONAIRE
EXPERT
FRIEND
VS
AUDIENCE
VOTE
USE
A
SINGLE
ARBITRARY
LEARNING
ALGORITHM
BUT
MANIPULATE
TRAINING
DATA
TO
MAKE
IT
LEARN
MULTIPLE
MODELS
DATA
M
LEARNER
M
DIFFERENT
METHODS
FOR
CHANGING
TRAINING
DATA
BAGGING
RESAMPLE
TRAINING
DATA
BOOSTING
REWEIGHT
TRAINING
DATA
CREATE
ENSEMBLES
BY
REPEATEDLY
RANDOMLY
RESAMPLING
THE
TRAINING
DATA
BRIEMAN
GIVEN
A
TRAINING
SET
OF
SIZE
N
CREATE
M
SAMPLES
OF
SIZE
N
BY
DRAWING
N
EXAMPLES
FROM
THE
ORIGINAL
DATA
WITH
REPLACEMENT
COMBINE
THE
M
RESULTING
MODELS
USING
SIMPLE
MAJORITY
VOTE
DECREASES
ERROR
BY
DECREASING
THE
VARIANCE
IN
THE
RESULTS
DUE
TO
UNSTABLE
LEARNERS
ALGORITHMS
LIKE
DECISION
TREES
WHOSE
OUTPUT
CAN
CHANGE
DRAMATICALLY
WHEN
THE
TRAINING
DATA
IS
SLIGHTLY
CHANGED
HOWEVER
OFTEN
THE
ERRORS
OF
THE
DIFFERENT
MODELS
ARE
CORRELATED
WHICH
DEFIES
THE
PURPOSE
OF
BAGGING
ORIGINALLY
DEVELOPED
BY
COMPUTATIONAL
LEARNING
THEORISTS
TO
GUARANTEE
PERFORMANCE
IMPROVEMENTS
ON
FITTING
TRAINING
DATA
FOR
A
WEAK
LEARNER
THAT
ONLY
NEEDS
TO
GENERATE
A
HYPOTHESIS
WITH
A
TRAINING
ACCURACY
GREATER
THAN
SCHAPIRE
REVISED
TO
BE
A
PRACTICAL
ALGORITHM
ADABOOST
FOR
BUILDING
ENSEMBLES
THAT
EMPIRICALLY
IMPROVES
GENERALIZATION
PERFORMANCE
FREUND
SHAPIRE
EXAMPLES
ARE
GIVEN
WEIGHTS
AT
EACH
ITERATION
A
NEW
HYPOTHESIS
IS
LEARNED
AND
THE
EXAMPLES
ARE
REWEIGHTED
TO
FOCUS
THE
SYSTEM
ON
EXAMPLES
THAT
THE
MOST
RECENTLY
LEARNED
CLASSIFIER
GOT
WRONG
GENERAL
LOOP
SET
ALL
EXAMPLES
TO
HAVE
EQUAL
UNIFORM
WEIGHTS
FOR
M
FROM
TO
M
DO
FIND
THE
WEAK
LEARNER
HM
THAT
ACHIEVES
LOWEST
WEIGHTED
TRAINING
ERROR
INCREASE
THE
WEIGHTS
OF
EXAMPLES
THAT
HM
CLASSIFIES
INCORRECTLY
DURING
TESTING
EACH
OF
THE
M
CLASSIFIERS
GETS
A
WEIGHTED
VOTE
PROPORTIONAL
TO
ITS
ACCURACY
ON
THE
TRAINING
DATA
FINAL
CLASSIFIER
IS
A
LINEAR
COMBINATION
OF
ALL
WEAK
LEARNERS
BASE
WEAK
LEARNER
MUST
FOCUS
ON
CORRECTLY
CLASSIFYING
THE
MOST
HIGHLY
WEIGHTED
EXAMPLES
WHILE
STRONGLY
AVOIDING
OVER
FITTING
WEAK
LEARNERS
MUST
PERFORM
BETTER
THAN
CHANCE
WEAK
CLASSIFIER
WEIGHTS
INCREASED
WEAK
CLASSIFIER
WEIGHTS
INCREASED
WEAK
CLASSIFIER
FINAL
CLASSIFIER
IS
A
COMBINATION
OF
WEAK
CLASSIFIERS
START
WITH
UNIFORM
WEIGHTS
ON
TRAINING
EXAMPLES
FOR
M
ROUNDS
EVALUATE
WEIGHTED
ERROR
FOR
EACH
WEAK
LEARNER
PICK
BEST
LEARNER
FIGURE
FROM
C
BISHOP
NOTES
FROM
K
GRAUMAN
NORMALIZE
THE
WEIGHTS
SO
THEY
SUM
TO
RE
WEIGHT
THE
EXAMPLES
INCORRECTLY
CLASSIFIED
GET
MORE
WEIGHT
CORRECTLY
CLASSIFIED
GET
LESS
WEIGHT
FINAL
CLASSIFIER
IS
COMBINATION
OF
WEAK
ONES
WEIGHTED
ACCORDING
TO
ERROR
THEY
HAD
LEARNING
WITH
WEIGHTED
EXAMPLES
GENERIC
APPROACH
IS
TO
REPLICATE
EXAMPLES
IN
THE
TRAINING
SET
PROPORTIONAL
TO
THEIR
WEIGHTS
E
G
REPLICAS
OF
AN
EXAMPLE
WITH
A
WEIGHT
OF
AND
FOR
ONE
WITH
WEIGHT
MOST
ALGORITHMS
CAN
BE
ENHANCED
TO
EFFICIENTLY
INCORPORATE
WEIGHTS
DIRECTLY
IN
THE
LEARNING
ALGORITHM
SO
THAT
THE
EFFECT
IS
THE
SAME
FOR
DECISION
TREES
FOR
CALCULATING
INFORMATION
GAIN
WHEN
COUNTING
EXAMPLE
I
SIMPLY
INCREMENT
THE
CORRESPONDING
COUNT
BY
WI
RATHER
THAN
BY
EXPERIMENTAL
RESULTS
ON
ENSEMBLES
FREUND
SCHAPIRE
QUINLAN
ENSEMBLES
HAVE
BEEN
USED
TO
IMPROVE
GENERALIZATION
ACCURACY
ON
A
WIDE
VARIETY
OF
PROBLEMS
ON
AVERAGE
BOOSTING
PROVIDES
A
LARGER
INCREASE
IN
ACCURACY
THAN
BAGGING
BOOSTING
ON
RARE
OCCASIONS
CAN
DEGRADE
ACCURACY
BAGGING
MORE
CONSISTENTLY
PROVIDES
A
MODEST
IMPROVEMENT
BOOSTING
IS
PARTICULARLY
SUBJECT
TO
OVER
FITTING
WHEN
THERE
IS
SIGNIFICANT
NOISE
IN
THE
TRAINING
DATA
ISSUES
IN
ENSEMBLES
PARALLELISM
IN
ENSEMBLES
BAGGING
IS
EASILY
PARALLELIZED
BOOSTING
IS
NOT
VARIANTS
OF
BOOSTING
TO
HANDLE
NOISY
DATA
HOW
WEAK
SHOULD
A
BASE
LEARNER
FOR
BOOSTING
BE
WHAT
IS
THE
THEORETICAL
EXPLANATION
OF
BOOSTING
ABILITY
TO
IMPROVE
GENERALIZATION
EXACTLY
HOW
DOES
THE
DIVERSITY
OF
ENSEMBLES
AFFECT
THEIR
GENERALIZATION
PERFORMANCE
COMBINING
BOOSTING
AND
BAGGING
SLIDING
WINDOW
DETECTOR
MUST
EVALUATE
TENS
OF
THOUSANDS
OF
LOCATION
SCALE
COMBINATIONS
FACES
ARE
RARE
PER
IMAGE
A
MEGAPIXEL
IMAGE
HAS
PIXELS
AND
A
COMPARABLE
NUMBER
OF
CANDIDATE
FACE
LOCATIONS
FOR
COMPUTATIONAL
EFFICIENCY
WE
SHOULD
TRY
TO
SPEND
AS
LITTLE
TIME
AS
POSSIBLE
ON
THE
NON
FACE
WINDOWS
MAIN
IDEA
REPRESENT
LOCAL
TEXTURE
WITH
EFFICIENTLY
COMPUTABLE
RECTANGULAR
FEATURES
WITHIN
WINDOW
OF
INTEREST
SELECT
DISCRIMINATIVE
FEATURES
TO
BE
WEAK
CLASSIFIERS
USE
BOOSTED
COMBINATION
OF
THEM
AS
FINAL
CLASSIFIER
FORM
A
CASCADE
OF
SUCH
CLASSIFIERS
REJECTING
CLEAR
NEGATIVES
QUICKLY
NOT
DISCUSSED
SEE
HIDDEN
SLIDES
RECTANGULAR
FILTERS
FEATURE
OUTPUT
IS
DIFFERENCE
BETWEEN
ADJACENT
REGIONS
VALUE
PIXELS
IN
WHITE
AREA
PIXELS
IN
BLACK
AREA
EFFICIENTLY
COMPUTABLE
WITH
INTEGRAL
IMAGE
ANY
SUM
CAN
BE
COMPUTED
IN
CONSTANT
TIME
VALUE
AT
X
Y
IS
SUM
OF
PIXELS
ABOVE
AND
TO
THE
LEFT
OF
X
Y
INTEGRAL
IMAGE
CONSIDERING
ALL
POSSIBLE
FILTER
PARAMETERS
POSITION
SCALE
AND
TYPE
POSSIBLE
FEATURES
ASSOCIATED
WITH
EACH
X
WINDOW
WHICH
SUBSET
OF
THESE
FEATURES
SHOULD
WE
USE
TO
DETERMINE
IF
A
WINDOW
HAS
A
FACE
USE
ADABOOST
BOTH
TO
SELECT
THE
INFORMATIVE
FEATURES
AND
TO
FORM
THE
CLASSIFIER
WANT
TO
SELECT
THE
SINGLE
RECTANGLE
FEATURE
AND
THRESHOLD
THAT
BEST
SEPARATES
POSITIVE
FACES
AND
NEGATIVE
NON
FACES
TRAINING
EXAMPLES
IN
TERMS
OF
WEIGHTED
ERROR
RESULTING
WEAK
CLASSIFIER
OUTPUTS
OF
A
POSSIBLE
RECTANGLE
FEATURE
ON
FACES
AND
NON
FACES
FOR
NEXT
ROUND
REWEIGHT
THE
EXAMPLES
ACCORDING
TO
ERRORS
CHOOSE
ANOTHER
FILTER
THRESHOLD
COMBO
FIRST
TWO
FEATURES
SELECTED
BY
BOOSTING
THIS
FEATURE
COMBINATION
CAN
YIELD
DETECTION
RATE
AND
FALSE
POSITIVE
RATE
LIKE
THE
THRESHOLDED
FEATURES
CLASSIFIERS
IN
FACE
DETECTION
A
SINGLE
LEVEL
DECISION
TREE
DISCUSSED
NEXT
FOR
USE
A
TOTAL
OF
OF
THESE
AND
DON
T
WORRY
ABOUT
THEM
HAVING
BETTER
THAN
CHANCE
PERFORMANCE
FIGURE
FROM
WIKIPEDIA
ENSEMBLE
METHODS
BAGGING
BOOSTING
BOOSTING
APPLICATION
FACE
DETECTION
TREE
BASED
CLASSIFIERS
FOR
INSTANCES
REPRESENTED
AS
FEATURE
VECTORS
NODES
TEST
FEATURES
THERE
IS
ONE
BRANCH
FOR
EACH
VALUE
OF
THE
FEATURE
AND
LEAVES
SPECIFY
THE
CATEGORY
SHAPE
COLOR
RED
BLUE
GREEN
NEG
POS
SHAPE
COLOR
RED
BLUE
GREEN
B
C
CIRCLE
SQUARE
TRIANGLE
POS
NEG
NEG
CIRCLE
SQUARE
TRIANGLE
A
B
C
CAN
REPRESENT
ARBITRARY
CONJUNCTION
AND
DISJUNCTION
CAN
REPRESENT
ANY
CLASSIFICATION
FUNCTION
OVER
DISCRETE
FEATURE
VECTORS
CAN
BE
REWRITTEN
AS
A
SET
OF
RULES
RED
CIRCLE
POS
RED
CIRCLE
A
BLUE
B
RED
SQUARE
B
GREEN
C
RED
TRIANGLE
C
CONTINUOUS
REAL
VALUED
FEATURES
CAN
BE
HANDLED
BY
ALLOWING
NODES
TO
SPLIT
A
REAL
VALUED
FEATURE
INTO
TWO
RANGES
BASED
ON
A
THRESHOLD
E
G
LENGTH
AND
LENGTH
CLASSIFICATION
TREES
HAVE
DISCRETE
CLASS
LABELS
AT
THE
LEAVES
RECURSIVELY
BUILD
A
TREE
TOP
DOWN
BY
DIVIDE
AND
CONQUER
BIG
RED
CIRCLE
SMALL
RED
CIRCLE
SMALL
RED
SQUARE
BIG
BLUE
CIRCLE
COLOR
RED
BLUE
GREEN
BIG
RED
CIRCLE
SMALL
RED
CIRCLE
SMALL
RED
SQUARE
RECURSIVELY
BUILD
A
TREE
TOP
DOWN
BY
DIVIDE
AND
CONQUER
BIG
RED
CIRCLE
BIG
RED
CIRCLE
SMALL
RED
CIRCLE
SMALL
RED
SQUARE
BIG
BLUE
CIRCLE
COLOR
RED
BLUE
GREEN
SMALL
RED
CIRCLE
SHAPE
NEG
NEG
SMALL
RED
SQUARE
CIRCLE
POS
BIG
RED
CIRCLE
SMALL
RED
CIRCLE
BIG
BLUE
CIRCLE
SQUARE
TRIANGLE
NEG
POS
SMALL
RED
SQUARE
DTREE
EXAMPLES
FEATURES
RETURNS
A
TREE
IF
ALL
EXAMPLES
ARE
IN
ONE
CATEGORY
RETURN
A
LEAF
NODE
WITH
THAT
CATEGORY
LABEL
ELSE
IF
THE
SET
OF
FEATURES
IS
EMPTY
RETURN
A
LEAF
NODE
WITH
THE
CATEGORY
LABEL
THAT
IS
THE
MOST
COMMON
IN
EXAMPLES
ELSE
PICK
A
FEATURE
F
AND
CREATE
A
NODE
R
FOR
IT
FOR
EACH
POSSIBLE
VALUE
VI
OF
F
LET
EXAMPLESI
BE
THE
SUBSET
OF
EXAMPLES
THAT
HAVE
VALUE
VI
FOR
F
ADD
AN
OUT
GOING
EDGE
E
TO
NODE
R
LABELED
WITH
THE
VALUE
VI
IF
EXAMPLESI
IS
EMPTY
THEN
ATTACH
A
LEAF
NODE
TO
EDGE
E
LABELED
WITH
THE
CATEGORY
THAT
IS
THE
MOST
COMMON
IN
EXAMPLES
ELSE
CALL
DTREE
EXAMPLESI
FEATURES
F
AND
ATTACH
THE
RESULTING
TREE
AS
THE
SUBTREE
UNDER
EDGE
E
RETURN
THE
SUBTREE
ROOTED
AT
R
GOAL
IS
TO
HAVE
THE
RESULTING
TREE
BE
AS
SMALL
AS
POSSIBLE
PER
OCCAM
RAZOR
FINDING
A
MINIMAL
DECISION
TREE
NODES
LEAVES
OR
DEPTH
IS
AN
NP
HARD
OPTIMIZATION
PROBLEM
TOP
DOWN
DIVIDE
AND
CONQUER
METHOD
DOES
A
GREEDY
SEARCH
FOR
A
SIMPLE
TREE
BUT
DOES
NOT
GUARANTEE
TO
FIND
THE
SMALLEST
THAT
OK
WANT
TO
PICK
A
FEATURE
THAT
CREATES
SUBSETS
OF
EXAMPLES
THAT
ARE
RELATIVELY
PURE
IN
A
SINGLE
CLASS
SO
THEY
ARE
CLOSER
TO
BEING
LEAF
NODES
THERE
ARE
A
VARIETY
OF
HEURISTICS
FOR
PICKING
A
GOOD
TEST
A
POPULAR
ONE
IS
BASED
ON
INFORMATION
GAIN
THAT
ORIGINATED
WITH
THE
SYSTEM
OF
QUINLAN
ENTROPY
DISORDER
IMPURITY
OF
A
SET
OF
EXAMPLES
RELATIVE
TO
A
BINARY
CLASSIFICATION
IS
ENTROPY
LOG
LOG
WHERE
IS
THE
FRACTION
OF
POSITIVE
EXAMPLES
IN
AND
IS
THE
FRACTION
OF
NEGATIVES
IF
ALL
EXAMPLES
ARE
IN
ONE
CATEGORY
ENTROPY
IS
ZERO
WE
DEFINE
LOG
IF
EXAMPLES
ARE
EQUALLY
MIXED
ENTROPY
IS
A
MAXIMUM
OF
FOR
MULTI
CLASS
PROBLEMS
WITH
C
CATEGORIES
ENTROPY
GENERALIZES
TO
C
ENTROPY
I
PI
LOG
PI
THE
INFORMATION
GAIN
OF
A
FEATURE
F
IS
THE
EXPECTED
REDUCTION
IN
ENTROPY
RESULTING
FROM
SPLITTING
ON
THIS
FEATURE
GAIN
F
ENTROPY
V
VALUES
F
ENTROPY
SV
WHERE
SV
IS
THE
SUBSET
OF
HAVING
VALUE
V
FOR
FEATURE
F
ENTROPY
OF
EACH
RESULTING
SUBSET
WEIGHTED
BY
ITS
RELATIVE
SIZE
EXAMPLE
BIG
RED
CIRCLE
SMALL
RED
CIRCLE
SMALL
RED
SQUARE
BIG
BLUE
CIRCLE
E
SIZE
BIG
SMALL
E
E
GAIN
SLIDE
CREDIT
RAY
MOONEY
E
COLOR
RED
BLUE
E
E
GAIN
E
SHAPE
CIRCLE
SQUARE
E
E
GAIN
EXAMPLE
PROBLEM
DECIDE
WHETHER
TO
WAIT
FOR
A
TABLE
AT
A
RESTAURANT
BASED
ON
THE
FOLLOWING
ATTRIBUTES
ALTERNATE
IS
THERE
AN
ALTERNATIVE
RESTAURANT
NEARBY
BAR
IS
THERE
A
COMFORTABLE
BAR
AREA
TO
WAIT
IN
FRI
SAT
IS
TODAY
FRIDAY
OR
SATURDAY
HUNGRY
ARE
WE
HUNGRY
PATRONS
NUMBER
OF
PEOPLE
IN
THE
RESTAURANT
NONE
SOME
FULL
PRICE
PRICE
RANGE
RAINING
IS
IT
RAINING
OUTSIDE
RESERVATION
HAVE
WE
MADE
A
RESERVATION
TYPE
KIND
OF
RESTAURANT
FRENCH
ITALIAN
THAI
BURGER
WAITESTIMATE
ESTIMATED
WAITING
TIME
LEARNING
A
TREE
THAT
CLASSIFIES
THE
TRAINING
DATA
PERFECTLY
MAY
NOT
LEAD
TO
THE
TREE
WITH
THE
BEST
GENERALIZATION
TO
UNSEEN
DATA
THERE
MAY
BE
NOISE
IN
THE
TRAINING
DATA
THAT
THE
TREE
IS
ERRONEOUSLY
FITTING
THE
ALGORITHM
MAY
BE
MAKING
POOR
DECISIONS
TOWARDS
THE
LEAVES
OF
THE
TREE
THAT
ARE
BASED
ON
VERY
LITTLE
DATA
AND
MAY
NOT
REFLECT
RELIABLE
TRENDS
ON
TRAINING
DATA
ON
TEST
DATA
HYPOTHESIS
COMPLEXITY
CATEGORY
OR
FEATURE
NOISE
CAN
EASILY
CAUSE
OVERFITTING
ADD
NOISY
INSTANCE
MEDIUM
BLUE
CIRCLE
POS
BUT
REALLY
NEG
COLOR
RED
GREEN
BLUE
SHAPE
NEG
NEG
CIRCLE
SQUARE
TRIANGLE
POS
NEG
POS
CATEGORY
OR
FEATURE
NOISE
CAN
EASILY
CAUSE
OVERFITTING
ADD
NOISY
INSTANCE
MEDIUM
BLUE
CIRCLE
POS
BUT
REALLY
NEG
COLOR
SHAPE
RED
GREEN
NEG
BLUE
BIG
BLUE
CIRCLE
MEDIUM
BLUE
CIRCLE
CIRCLE
SQUARE
TRIANGLE
SMALL
MED
BIG
POS
NEG
POS
NEG
POS
NEG
NOISE
CAN
ALSO
CAUSE
DIFFERENT
INSTANCES
OF
THE
SAME
FEATURE
VECTOR
TO
HAVE
DIFFERENT
CLASSES
IMPOSSIBLE
TO
FIT
THIS
DATA
AND
MUST
LABEL
LEAF
WITH
THE
MAJORITY
CLASS
BIG
RED
CIRCLE
NEG
BUT
REALLY
POS
CONFLICTING
EXAMPLES
CAN
ALSO
ARISE
IF
THE
FEATURES
ARE
INCOMPLETE
AND
INADEQUATE
TO
DETERMINE
THE
CLASS
OR
IF
THE
TARGET
CONCEPT
IS
NON
DETERMINISTIC
OVERFITTING
PREVENTION
PRUNING
METHODS
TWO
BASIC
APPROACHES
FOR
DECISION
TREES
PREPRUNING
STOP
GROWING
TREE
AS
SOME
POINT
DURING
TOP
DOWN
CONSTRUCTION
WHEN
THERE
IS
NO
LONGER
SUFFICIENT
DATA
TO
MAKE
RELIABLE
DECISIONS
POSTPRUNING
GROW
THE
FULL
TREE
THEN
REMOVE
SUBTREES
THAT
DO
NOT
HAVE
SUFFICIENT
EVIDENCE
LABEL
LEAF
RESULTING
FROM
PRUNING
WITH
THE
MAJORITY
CLASS
OF
THE
REMAINING
DATA
OR
A
CLASS
PROBABILITY
DISTRIBUTION
METHOD
FOR
DETERMINING
WHICH
SUBTREES
TO
PRUNE
CROSS
VALIDATION
RESERVE
SOME
TRAINING
DATA
AS
A
HOLD
OUT
SET
VALIDATION
SET
TO
EVALUATE
UTILITY
OF
SUBTREES
STATISTICAL
TEST
USE
A
STATISTICAL
TEST
ON
THE
TRAINING
DATA
TO
DETERMINE
IF
ANY
OBSERVED
REGULARITY
CAN
BE
DISMISSES
AS
LIKELY
DUE
TO
RANDOM
CHANCE
MINIMUM
DESCRIPTION
LENGTH
MDL
DETERMINE
IF
THE
ADDITIONAL
COMPLEXITY
OF
THE
HYPOTHESIS
IS
LESS
COMPLEX
THAN
JUST
EXPLICITLY
REMEMBERING
ANY
EXCEPTIONS
RESULTING
FROM
PRUNING
CS
MACHINE
LEARNING
ACTIVE
LEARNING
AND
CROWDSOURCING
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
APRIL
COLLECTING
DATA
ON
AMAZON
MECHANICAL
TURK
WORKERS
ANNOTATION
PROTOCOLS
TYPE
KEYWORDS
SELECT
RELEVANT
IMAGES
CLICK
ON
LANDMARKS
OUTLINE
SOMETHING
ANYTHING
ELSE
TYPE
KEYWORDS
SELECT
EXAMPLES
OUTLINE
SOMETHING
MOTIVATION
CUSTOM
ANNOTATIONS
X
LARGE
SCALE
LOW
PRICE
G
A
M
AZON
M
ECHAN
I
CA
L
TURK
X
AMAZONMECHANICAL
TURK
ARTIFICIAL
ARTIFICIAL
INTELLIGENCE
YOUR
ACCOUNT
HITS
QUALIFICAT
IONS
INTRODUCTION
I
DASHBOARD
I
STATUS
I
ACCOUNT
SETTINGS
MECHANICAL
TURK
IS
A
MARKETPLACE
FOR
WORK
D
ALREADY
HAVE
AN
ACCOUNT
SIGN
IN
AS
A
WORKER
I
REQUESTER
WE
GIVE
BUSINESSES
AND
DEVELOPERS
ACCESS
TO
AN
ON
DEMAND
SCALABLE
WORKFORCE
WORKERS
SELECT
FROM
MOUSANDS
OF
TASKS
A
A
WORK
WHENEVER
IT
CONVENIENT
HITS
AVAILAB
LE
VIEW
THEM
NOW
MAKE
MONEY
BY
WORKING
ON
HITS
H
TS
HUM
AN
I
NTELLIGENCE
TASKS
ARE
INDIVIDUAL
TASKS
THAT
YOU
WORK
ON
FIND
HITS
NO
W
AS
A
MECHANICAL
TURK
WORKER
YOU
CAN
WORK
FROM
HOME
CHOOSE
YOUR
OWN
WORK
H
OURS
GET
PAID
FOR
DOING
GOOD
W
ORK
GE
RESUL
FROM
MECHANICAL
TURK
WORKERS
ASK
WORKERS
TO
COMPLETE
HITS
HUMAN
I
NTELLIGENCE
TASKS
AND
GET
R
ESULT
U
SING
MECHANICAL
TURK
GET
STARTED
AS
A
MECHANICAL
TURK
REQUESTER
YOU
HAVE
ACCESS
TO
A
GLOBAL
ON
DEMAND
X
WORKFORCE
GET
THOUSANDS
OF
HITS
COMPLETED
IN
MINUTES
PAY
ONLY
WHEN
YOU
RE
SATISFIED
WIT
H
TH
E
RESULTS
FIND
AN
INTERESTING
TASK
WORK
EARN
MONEY
FIND
HITS
NOW
FUND
YOUR
LOAD
YOUR
ACCOUNT
TASKS
GET
STARTED
GET
RESULTS
OR
L
EARN
MORE
ABOUT
BEING
A
WORKER
FAQ
I
CO
NTACT
US
I
CA
R
EERS
AT
MEC
H
ANICAL
TURK
I
DEV
ELOP
ERS
I
PRESS
I
POLICIES
I
STATE
LICENSING
I
BLOG
I
SE
RV
ICE
HEA
LT
H
D
ASHBO
AR
D
AMA
W
N
COM
I
N
C
OR
ITS
AFF
ILI
AT
ES
AN
AMA
ON
COM
COMPANY
ISSUES
QUALITY
HOW
GOOD
IS
IT
HOW
TO
BE
SURE
PRICE
HOW
TO
PRICE
IT
ENSURING
ANNOTATION
QUALITY
CONSENSUS
MULTIPLE
ANNOTATION
WISDOM
OF
THE
CROWD
QUALIFICATION
EXAM
GOLD
STANDARD
QUESTIONS
GRADING
TASKS
A
SECOND
TIER
OF
WORKERS
WHO
GRADE
OTHERS
PRICING
TRADE
OFF
BETWEEN
THROUGHPUT
AND
COST
HIGHER
PAY
CAN
ACTUALLY
ATTRACT
SCAMMERS
SOME
STUDIES
FIND
THAT
THE
MOST
ACCURATE
RESULTS
ARE
ACHIEVED
IF
TURKERS
DO
TASKS
FOR
FREE
GAMES
WITH
A
PURPOSE
LUIS
VON
AHN
ASSOCIATE
PROFESSOR
AT
CMU
ONE
OF
THE
FATHERS
OF
CROWDSOURCING
CREATED
THE
ESP
GAME
PEEKABOOM
AND
SEVERAL
OTHER
GAMES
WITH
A
PURPOSE
THE
ESP
GAME
TWO
PLAYER
ONLINE
GAME
PARTNERS
DON
T
KNOW
EACH
OTHER
AND
CAN
T
COMMUNICATE
OBJECT
OF
THE
GAME
TYPE
THE
SAME
WORD
THE
ONLY
THING
IN
COMMON
IS
AN
IMAGE
LUIS
VON
AHN
AND
LAURA
DABBISH
LABELING
IMAGES
WITH
A
COMPUTER
GAME
CHI
THE
ESP
GAME
PLAYER
PLAYER
GUESSING
CAR
GUESSING
BOY
GUESSING
HAT
GUESSING
KID
SUCCESS
YOU
AGREE
ON
CAR
GUESSING
CAR
SUCCESS
YOU
AGREE
ON
CAR
THE
ESP
GAME
IS
FUN
MILLION
LABELS
WITH
PLAYERS
THERE
ARE
MANY
PEOPLE
THAT
PLAY
OVER
HOURS
A
WEEK
WHY
DO
PEOPLE
LIKE
THE
ESP
GAME
THE
ESP
GAME
GIVES
ITS
PLAYERS
A
WEIRD
AND
BEAUTIFUL
SENSE
OF
ANONYMOUS
INTIMACY
ON
THE
ONE
HAND
YOU
HAVE
NO
IDEA
WHO
YOUR
PARTNER
IS
ON
THE
OTHER
HAND
THE
TWO
OF
YOU
ARE
BRINGING
YOUR
MINDS
TOGETHER
IN
A
WAY
THAT
LOVERS
WOULD
ENVY
STRANGELY
ADDICTIVE
IT
SO
MUCH
FUN
TRYNG
TO
GUESS
WHAT
OTHERS
THINK
YOU
HAVE
TO
STEP
OUTSIDE
OF
YOURSELF
TO
MATCH
IT
FAST
PACED
HELPS
ME
LEARN
ENGLISH
LOCATING
OBJECTS
IN
IMAGES
THE
ESP
GAME
TELLS
US
IF
AN
IMAGE
CONTAINS
A
SPECIFIC
OBJECT
BUT
DOESN
T
SAY
WHERE
IN
THE
IMAGE
THE
OBJECT
IS
SUCH
INFORMATION
WOULD
BE
EXTREMELY
USEFUL
FOR
COMPUTER
VISION
RESEARCH
PAINTBALL
GAME
PLAYERS
SHOOT
AT
OBJECTS
ON
THE
IMAGE
SHOOT
THE
CAR
WE
GIVE
POINTS
AND
CHECK
ACCURACY
BY
GIVING
PLAYERS
IMAGES
FOR
WHICH
WE
ALREADY
KNOW
WHERE
THE
OBJECT
IS
REVEALING
IMAGES
GUESSER
REVEALER
GUESS
CAR
BCRAURSH
PARTNER
GUESS
SUMMARY
COLLECTING
ANNOTATIONS
FROM
HUMANS
CROWDSOURCING
ALLOWS
VERY
CHEAP
DATA
COLLECTION
GETTING
HIGH
QUALITY
ANNOTATIONS
CAN
BE
TRICKY
BUT
THERE
ARE
MANY
WAYS
TO
ENSURE
QUALITY
ONE
WAY
TO
OBTAIN
HIGH
QUALITY
DATA
FAST
IS
BY
PHRASING
YOUR
DATA
COLLECTION
AS
A
GAME
WHAT
TO
DO
WHEN
DATA
IS
EXPENSIVE
TO
OBTAIN
CROWDSOURCING
TRAINING
JAMES
HAYS
ACTIVE
LEARNING
TRADITIONAL
ACTIVE
LEARNING
REDUCES
SUPERVISION
BY
OBTAINING
LABELS
FOR
THE
MOST
INFORMATIVE
OR
UNCERTAIN
EXAMPLES
FIRST
MACKAY
FREUND
ET
AL
TONG
KOLLER
LINDENBAUM
ET
AL
KAPOOR
ET
AL
VISUAL
RECOGNITION
WITH
HUMANS
IN
THE
LOOP
ECCV
CRETE
GREECE
STEVE
BRANSON
CATHERINE
WAH
FLORIAN
SCHROFF
BORIS
BABENKO
SERGE
BELONGIE
PETER
WELINDER
PIETRO
PERONA
FIELD
GUIDES
DIFFICULT
FOR
AVERAGE
USERS
COMPUTER
VISION
DOESN
T
WORK
PERFECTLY
YET
RESEARCH
MOSTLY
ON
BASIC
LEVEL
CATEGORIES
WHAT
TYPE
OF
BIRD
IS
THIS
VISUAL
RECOGNITION
WITH
HUMANS
IN
THE
LOOP
PARAKEET
AUKLET
MOTIVATION
SUPPLEMENT
VISUAL
RECOGNITION
WITH
THE
HUMAN
CAPACITY
FOR
VISUAL
FEATURE
EXTRACTION
TO
TACKLE
DIFFICULT
FINE
GRAINED
RECOGNITION
PROBLEMS
TYPICAL
PROGRESS
IS
VIEWED
AS
INCREASING
DATA
DIFFICULTY
WHILE
MAINTAINING
FULL
AUTONOMY
HERE
THE
AUTHORS
VIEW
PROGRESS
AS
REDUCTION
IN
HUMAN
EFFORT
ON
DIFFICULT
DATA
BRIAN
O
NEILL
CATEGORIES
OF
RECOGNITION
BASIC
LEVEL
SUBORDINATE
PARTS
ATTRIBUTES
EASY
FOR
HUMANS
HARD
FOR
COMPUTERS
HARD
FOR
HUMANS
HARD
FOR
COMPUTERS
EASY
FOR
HUMANS
HARD
FOR
COMPUTERS
VISUAL
QUESTIONS
GAME
HARD
CLASSIFICATION
PROBLEMS
CAN
BE
TURNED
INTO
A
SEQUENCE
OF
EASY
ONES
RECOGNITION
WITH
HUMANS
IN
THE
LOOP
COMPUTERS
REDUCE
NUMBER
OF
REQUIRED
QUESTIONS
HUMANS
DRIVE
UP
ACCURACY
OF
VISION
ALGORITHMS
EXAMPLE
QUESTIONS
EXAMPLE
QUESTIONS
EXAMPLE
QUESTIONS
BASIC
ALGORITHM
INPUT
IMAGE
X
COMPUTER
VISION
MAX
EXPECTED
INFORMATION
GAIN
QUESTION
A
NO
P
C
X
IS
THE
BELLY
BLACK
MAX
EXPECTED
INFORMATION
GAIN
QUESTION
A
YES
P
C
X
IS
THE
BILL
HOOKED
P
C
X
SOME
DEFINITIONS
Q
QN
SET
OF
POSSIBLE
QUESTIONS
AI
AI
RI
V
POSSIBLE
ANSWERS
TO
QUESTION
I
POSSIBLE
CONFIDENCE
IN
ANSWER
I
GUESSING
PROBABLY
DEFINITELY
UI
U
T
AI
RI
USER
RESPONSE
HISTORY
OF
USER
RESPONSES
AT
TIME
T
QUESTION
SELECTION
SEEK
THE
QUESTION
E
G
WHAT
COLOR
IS
THE
BELLY
OF
THE
BIRD
THAT
GIVES
THE
MAXIMUM
INFORMATION
GAIN
ENTROPY
REDUCTION
GIVEN
THE
IMAGE
AND
THE
SET
OF
PREVIOUS
USER
RESPONSES
I
C
U
X
U
T
P
U
X
U
T
H
C
X
U
U
T
H
C
X
U
T
I
I
I
UI
AI
V
PROBABILITY
OF
OBTAINING
RESPONSE
UI
TO
EVALUATED
QUESTION
GIVEN
IMAGE
AND
RESPONSE
HISTORY
ENTROPY
WHEN
RESPONSE
IS
ADDED
TO
HISTORY
ENTROPY
AT
THIS
ITERATION
BEFORE
RESPONSE
TO
EVALUATED
QUESTION
IS
ADDED
TO
HISTORY
WHERE
H
C
X
U
T
C
P
C
X
U
T
LOG
P
C
X
U
T
RESULTS
USERS
DRIVE
PERFORMANCE
FEWER
QUESTIONS
ASKED
IF
CV
USED
JUST
COMPUTER
VISION
ADAPTED
FROM
STEVE
BRANSON
SUMMARY
HUMAN
IN
THE
LOOP
LEARNING
TO
MAKE
INTELLIGENT
USE
OF
THE
HUMAN
LABELING
EFFORT
DURING
TRAINING
HAVE
THE
COMPUTER
VISION
ALGORITHM
LEARN
ACTIVELY
BY
SELECTING
THOSE
QUESTIONS
THAT
ARE
MOST
INFORMATIVE
TO
COMBINE
STRENGTHS
OF
HUMAN
AND
IMPERFECT
VISION
ALGORITHMS
USE
A
HUMAN
IN
THE
LOOP
AT
RECOGNITION
TIME
VISUAL
OBJECT
RECOGNITION
SYNTHESIS
LECTURES
ON
ARTIFICIAL
INTELLIGENCE
AND
MACHINE
LEARNING
APRIL
PAGES
DOI
KRISTEN
GRAUMAN
UNIVERSITY
OF
TEXAS
AT
AUSTIN
BASTIAN
LEIBE
RWTH
AACHEN
UNIVERSITY
ABSTRACT
THE
VISUAL
RECOGNITION
PROBLEM
IS
CENTRAL
TO
COMPUTER
VISION
RESEARCH
FROM
ROBOTICS
TO
INFORMATION
RETRIEVAL
MANY
DESIRED
APPLICATIONS
DEMAND
THE
ABILITY
TO
IDENTIFY
AND
LOCALIZE
CATEGORIES
PLACES
AND
OBJECTS
THIS
TUTORIAL
OVERVIEWS
COMPUTER
VISION
ALGORITHMS
FOR
VISUAL
OBJECT
RECOGNITION
AND
IMAGE
CLASSIFICATION
WE
INTRODUCE
PRIMARY
REPRESENTATIONS
AND
LEARNING
APPROACHES
WITH
AN
EMPHASIS
ON
RECENT
ADVANCES
IN
THE
FIELD
THE
TARGET
AUDIENCE
CONSISTS
OF
RESEARCHERS
OR
STUDENTS
WORKING
IN
AI
ROBOTICS
OR
VISION
WHO
WOULD
LIKE
TO
UNDERSTAND
WHAT
METHODS
AND
REPRESENTATIONS
ARE
AVAILABLE
FOR
THESE
PROBLEMS
THIS
LECTURE
SUMMARIZES
WHAT
IS
AND
ISN
T
POSSIBLE
TO
DO
RELIABLY
TODAY
AND
OVERVIEWS
KEY
CONCEPTS
THAT
COULD
BE
EMPLOYED
IN
SYSTEMS
REQUIRING
VISUAL
CATEGORIZATION
TABLE
OF
CONTENTS
INTRODUCTION
OVERVIEW
RECOGNITION
OF
SPECIFIC
OBJECTS
LOCAL
FEATURES
DETECTION
AND
DESCRIPTION
MATCHING
LOCAL
FEATURES
GEOMETRIC
VERIFICATION
OF
MATCHED
FEATURES
EXAMPLE
SYSTEMS
SPECIFICOBJECT
RECOGNITION
OVERVIEW
RECOGNITION
OF
GENERIC
OBJECT
CATEGORIES
REPRESENTATIONS
FOR
OBJECT
CATEGORIES
GENERIC
OBJECT
DETECTION
FINDING
AND
SCORING
CANDIDATES
LEARNING
GENERIC
OBJECT
CATEGORY
MODELS
EXAMPLE
SYSTEMS
GENERIC
OBJECT
RECOGNITION
OTHER
CONSIDERATIONS
AND
CURRENT
CHALLENGES
CONCLUSIONS
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
HOMEWORK
DUE
INSTRUCTIONS
SEE
HOMEWORK
SUBMISSION
MECHANICS
ON
THE
MAIN
COURSE
PAGE
IF
YOU
DO
NOT
SEE
THE
ASSIGNMENTS
BUTTON
NOTIFY
THE
INSTRUCTOR
NOTE
IF
YOU
ARE
ASKED
TO
IMPLEMENT
SOMETHING
BY
YOURSELF
IT
IS
NOT
OK
TO
USE
OR
EVEN
LOOK
AT
EXISTING
MATLAB
OR
PYTHON
CODE
FOR
ANYTHING
YOU
ARE
NOT
ASKED
TO
IMPLEMENT
FEEL
FREE
TO
LOOK
UP
RELEVANT
FUNCTIONS
IF
YOU
HAVE
QUESTIONS
ABOUT
WHAT
YOU
CAN
USE
ASK
THE
INSTRUCTOR
OR
THE
TA
PART
I
SHORT
ANSWERS
POINTS
PROPOSE
THREE
NEW
PROBLEMS
THAT
CAN
BE
SOLVED
WITH
MACHINE
LEARNING
ONES
THAT
WE
HAVE
NOT
DISCUSSED
IN
CLASS
AND
DESCRIBE
HOW
YOU
WOULD
GO
ABOUT
SOLVING
EACH
FOR
EACH
PROBLEM
DISCUSS
WHAT
FEATURES
WOULD
YOU
USE
WHAT
WOULD
THE
LABELS
BE
HOW
WOULD
YOU
COLLECT
DATA
WHY
MIGHT
THE
PROBLEM
TURN
OUT
TO
BE
CHALLENGING
PROVIDE
YOUR
ANSWERS
IN
A
TEXT
FILE
TITLED
TXT
PART
II
THE
CLASSIFICATION
PIPELINE
POINTS
IN
THIS
PROBLEM
YOU
WILL
TRAIN
A
MULTICLASS
MODEL
TO
DISTINGUISH
BETWEEN
DIFFERENT
TYPES
OF
FLOWERS
YOU
WILL
USE
THE
IRIS
DATASET
FROM
THE
UCI
MACHINE
LEARNING
REPOSITORY
THE
DATA
IS
CONTAINED
THE
IRIS
DATA
FILE
UNDER
DATA
FOLDER
WHILE
THE
FILE
IRIS
NAMES
CONTAINS
A
DESCRIPTION
OF
THE
DATA
THE
FEATURES
X
ARE
GIVEN
AS
THE
FIRST
FOUR
COMMASEPARATED
VALUES
IN
EACH
ROW
IN
THE
DATA
FILE
THE
LABELS
Y
ARE
THE
LAST
ENTRY
IN
EACH
ROW
BUT
YOU
SHOULD
CONVERT
THE
STRINGS
TO
INTEGER
IDS
FIRST
SPLIT
THE
DATA
INTO
THREE
GROUPS
A
TRAINING
SET
A
VALIDATION
SET
AND
A
TEST
SET
YOU
WILL
TRAIN
A
MULTIWAY
SVM
CLASSIFIER
WE
WILL
TALK
ABOUT
SVM
CLASSIFIERS
IN
GREAT
LENGTH
LATER
IN
THE
CLASS
THE
GOAL
IN
THIS
ASSIGNMENT
IS
TO
USE
SVMS
AS
A
BLACK
BOX
SO
THAT
YOU
CAN
EXPERIMENT
WITH
THE
MACHINE
LEARNING
PIPELINE
WE
DISCUSSED
IN
THE
INTRODUCTION
SVM
IS
ONE
OF
THE
MOST
POPULAR
CLASSIFIERS
YOU
MIGHT
NEED
TO
USE
IT
FOR
YOUR
PROJECT
AND
IT
MIGHT
BE
ONE
OF
THE
FEW
THINGS
YOU
REMEMBER
HOW
TO
USE
AFTER
THIS
CLASS
IS
OVER
PICK
AN
SVM
SOFTWARE
PACKAGE
TO
USE
AMONG
LIBSVM
LIBLINEAR
SVM
LIGHT
THE
SVM
BUILTIN
TO
MATLAB
OR
THE
SVM
IN
SCIKITLEARN
FOR
PYTHON
LOOK
AT
DIFFERENT
PACKAGES
TO
SEE
WHICH
ONE
YOU
WOULD
FEEL
MOST
COMFORTABLE
USING
READ
THE
DOCUMENTATION
AND
FIND
WHICH
FUNCTION
IN
THE
PACKAGE
YOU
ARE
USING
PERFORMS
LEARNING
FITTING
TRAINING
AND
WHICH
FUNCTION
PERFORMS
PREDICTION
CLASSIFICATION
TESTING
YOU
CAN
ALSO
LOOK
FOR
EXAMPLES
OF
HOW
THESE
FUNCTIONS
ARE
USED
INCLUDE
A
TEXT
FILE
TXT
IN
YOUR
SUBMISSION
IN
WHICH
YOU
DESCRIBE
WHY
YOU
CHOSE
THE
PACKAGE
THAT
YOU
CHOSE
TO
USE
THEN
COPYPASTE
THE
PARTS
OF
THE
DOCUMENTATION
THAT
SHOW
HOW
TO
TRAIN
AND
USE
AN
SVM
PICK
SOME
SVM
PARAMETER
THAT
THE
PACKAGE
ALLOWS
YOU
TO
TUNE
OR
SPECIFY
VALUES
FOR
FOR
NOW
YOU
WON
T
KNOW
WHAT
THESE
PARAMETERS
DO
BUT
THEY
WILL
AFFECT
THE
SUCCESS
OF
YOUR
LEARNING
ALGORITHM
IN
SOME
WAY
YOUR
GOAL
IS
TO
PICK
THE
BEST
VALUE
OF
THE
PARAMETER
BY
TUNING
YOUR
MODEL
ON
A
VALIDATION
SET
IN
OTHER
WORDS
YOU
WILL
TRAIN
ONE
MODEL
FOR
EVERY
VALUE
OF
THE
PARAMETER
OF
YOUR
CHOICE
TRY
DIFFERENT
VALUES
AND
PICK
WHICH
VALUE
IS
BEST
ACCORDING
TO
THE
ACCURACY
ON
THE
VALIDATION
SET
YOU
WILL
THEN
USE
THE
CHOSEN
VALUE
OF
THE
PARAMETER
AND
APPLY
IT
TO
CLASSIFY
THE
SAMPLES
IN
THE
TEST
SET
IN
THE
TEXT
FILE
MENTIONED
ABOVE
REPORT
THE
FINAL
ACCURACY
ON
THE
TRAINING
SET
AND
THE
TEST
SET
THE
DIFFERENCE
BETWEEN
THESE
TWO
IS
CALLED
THE
GENERALIZATION
ERROR
IF
THE
CODE
YOU
ARE
USING
DOES
NOT
INCLUDE
A
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
FUNCTION
TO
COMPUTE
ACCURACY
YOU
SHOULD
WRITE
SUCH
A
FUNCTION
YOURSELF
IT
WILL
ONLY
BE
A
FEW
LINES
TO
COMPUTE
ACCURACY
YOU
WILL
COMPARE
THE
PREDICTED
LABELS
FOR
YOUR
TEST
SAMPLES
TO
THE
GROUNDTRUTH
PROVIDED
WITH
THE
DATASET
LABELS
FOR
THOSE
SAME
SAMPLES
AND
COMPUTE
THE
FRACTION
OF
SAMPLES
THAT
WERE
LABELED
CORRECTLY
FINALLY
EXPERIMENT
WITH
DIFFERENT
AMOUNTS
OF
TRAINING
DATA
AND
REPORT
HOW
THE
ERROR
ON
THE
TRAINING
DATA
AND
TEST
DATA
CHANGES
AS
YOU
ADD
MORE
TRAINING
DATA
INCLUDE
A
PLOT
IN
YOUR
WRITEUP
WITH
AT
LEAST
DIFFERENT
VALUES
FOR
THE
SIZE
OF
THE
TRAINING
DATA
ON
THE
XAXIS
AND
ACCURACY
ON
THE
YAXIS
FOR
THE
TRAINING
AND
TEST
SETS
SEPARATELY
I
E
SHOW
TWO
CURVES
IN
THE
TEXT
FILE
EXPLAIN
WHAT
YOU
ARE
OBSERVING
AND
WHY
IT
MIGHT
BE
HAPPENING
GRADING
RUBRIC
A
LOADING
AND
SPLITTING
THE
DATA
INTO
TRAIN
TEST
VALIDATION
AND
X
Y
PARTS
PTS
B
PACKAGE
CHOICE
AND
DOCUMENTATION
EXCERPTS
PTS
C
TRAINING
WITH
DIFFERENT
VALUES
OF
A
PARAMETER
AND
USING
THE
VALIDATION
SET
TO
PICK
THE
BEST
VALUE
OF
THAT
PARAMETER
PTS
D
DEMONSTRATING
HOW
TRAIN
TEST
ERROR
CHANGES
AS
MORE
TRAINING
DATA
IS
ADDED
INCLUDING
PLOT
PTS
PART
III
SEGMENTATION
VIA
CLUSTERING
POINTS
FOR
THIS
PROBLEM
YOU
WILL
IMPLEMENT
THE
KMEANS
ALGORITHM
YOU
WILL
THEN
USE
IT
TO
PERFORM
IMAGE
CLUSTERING
TO
TEST
YOUR
IMPLEMENTATION
WRITE
CODE
TO
PERFORM
CLUSTERING
OVER
A
AN
NXD
DATA
MATRIX
WHERE
N
IS
THE
NUMBER
OF
SAMPLES
AND
D
IS
THE
DIMENSIONALITY
OF
YOUR
FEATURE
REPRESENTATION
THAT
YOU
RECEIVE
AS
INPUT
FROM
THE
USER
YOUR
CODE
SHOULD
OUTPUT
AN
OUTPUT
CONTAINING
THE
DATA
MEMBERSHIPS
OF
EACH
SAMPLE
DENOTED
BY
AN
INDEX
FROM
TO
K
WHERE
K
IS
THE
NUMBER
OF
CLUSTERS
A
KXD
MATRIX
CONTAINING
THE
MEAN
CENTER
FOR
EACH
CLUSTER
AND
THE
FINAL
SSD
ERROR
OF
THE
CLUSTERING
I
E
THE
SUM
OF
THE
SQUARED
DISTANCES
BETWEEN
POINTS
AND
THEIR
ASSIGNED
MEANS
SUMMED
OVER
ALL
CLUSTERS
IN
YOUR
KMEANS
FUNCTION
TRY
RANDOM
RESTARTS
AND
RETURN
THE
CLUSTERING
WITH
THE
LOWEST
SSD
ERROR
YOU
WILL
NEXT
TEST
YOUR
IMPLEMENTATION
BY
APPLYING
CLUSTERING
TO
SEGMENT
AND
RECOLOR
AN
IMAGE
DOWNLOAD
IMAGES
FROM
THE
THE
BERKELEY
SEGMENTATION
DATASET
AND
BENCHMARK
TO
MAKE
SURE
RUNNING
YOUR
METHOD
DOESN
T
TAKE
A
LONG
TIME
DOWNSAMPLE
REDUCE
THE
SIZE
OF
YOUR
CHOSEN
IMAGES
TO
PERFORM
SEGMENTATION
YOU
NEED
A
REPRESENTATION
FOR
EVERY
IMAGE
PIXEL
FOR
SIMPLICITY
YOU
WILL
USE
A
THREEDIMENSIONAL
FEATURE
REPRESENTATION
FOR
EACH
PIXEL
CONSISTING
OF
THE
R
G
AND
B
VALUES
OF
EACH
PIXEL
YOU
CAN
ALSO
INCLUDE
THE
X
Y
LOCATION
OF
EACH
PIXEL
IN
THE
FEATURE
REPRESENTATION
IF
YOU
WISH
PERFORM
CLUSTERING
OVER
THE
PIXELS
OF
THE
IMAGE
THEN
RECOLOR
THE
PIXELS
OF
EACH
IMAGE
ACCORDING
TO
THEIR
CLUSTER
MEMBERSHIP
IN
PARTICULAR
REPLACE
EACH
PIXEL
WITH
THE
AVERAGE
R
G
B
VALUES
FOR
THE
CENTER
TO
WHICH
THE
PIXEL
BELONGS
INCLUDE
THE
RECOLORING
RESULT
FOR
IMAGES
AND
DIFFERENT
VALUES
OF
K
FOR
EACH
IMAGE
IN
YOUR
SUBMISSION
GRADING
RUBRIC
A
THE
CORRECTNESS
OF
YOUR
CLUSTERING
METHOD
IMPLEMENTATION
PTS
B
APPLYING
YOUR
CLUSTERING
METHOD
ON
IMAGES
AND
RECOLORING
DEPENDING
ON
CLUSTER
MEMBERSHIP
PTS
PART
IV
LINEAR
REGRESSION
POINTS
IN
THIS
PROBLEM
YOU
WILL
SOLVE
A
REGRESSION
PROBLEM
IN
TWO
WAYS
USING
THE
DIRECT
LEASTSQUARES
SOLUTION
AND
USING
GRADIENT
DESCENT
YOU
WILL
USE
THE
WINE
QUALITY
DATASET
USE
ONLY
THE
RED
WINE
DATA
THE
GOAL
IS
TO
FIND
THE
QUALITY
SCORE
OF
SOME
WINE
BASED
ON
ITS
ATTRIBUTES
FIRST
DIVIDE
THE
DATA
INTO
A
TRAINING
AND
TEST
SET
USING
APPROXIMATELY
FOR
TRAINING
YOU
DON
T
NEED
TO
USE
CROSSVALIDATION
FOR
THIS
PROBLEM
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
USE
THE
LINEAR
SYSTEM
OF
EQUATIONS
LEAST
SQUARES
SOLUTION
FOR
THE
LANGUAGE
OF
YOUR
CHOICE
IN
MATLAB
THAT
THE
BACKSLASH
OPERATOR
AX
B
X
A
B
YOU
NEED
TO
DECIDE
WHAT
ARE
A
X
AND
B
IN
THE
CASE
OF
LINEAR
REGRESSION
USE
THE
RESULTING
SOLUTION
TO
FIND
THE
WINE
QUALITY
SCORES
ON
THE
TEST
DATA
THEN
MEASURE
AND
REPORT
IN
A
FILE
TXT
THE
DISTANCE
BETWEEN
THE
TRUE
AND
PREDICTED
SCORES
NOW
IMPLEMENT
THE
GRADIENT
DESCENT
SOLUTION
FOR
THIS
YOU
WILL
NEED
TO
INITIALIZE
THE
WEIGHTS
IN
SOME
WAY
USE
EITHER
RANDOM
VALUES
OR
ALL
ZEROS
THEN
YOU
REPEAT
THE
FOLLOWING
SOME
NUMBER
OF
TIMES
FOR
THIS
PROBLEM
REPEAT
TIMES
IN
EACH
ITERATION
COMPUTE
THE
ERROR
FUNCTION
GRADIENT
USING
ALL
TRAINING
DATA
POINTS
THEN
ADJUST
THE
WEIGHTS
IN
THE
DIRECTION
OPPOSITE
TO
THE
GRADIENT
APPLY
THE
SOLUTION
TO
THE
TEST
SET
THEN
COMPUTE
AND
REPORT
THE
DISTANCE
AS
ABOVE
EXPERIMENT
WITH
DIFFERENT
LEARNING
RATES
E
G
ONES
IN
THE
RANGE
I
E
AND
REPORT
YOUR
OBSERVATIONS
GRADING
RUBRIC
A
DATA
SET
UP
AND
LEAST
SQUARES
SOLUTION
PTS
B
GRADIENT
DESCENT
SOLUTION
PTS
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
HOMEWORK
DUE
NOTE
IF
YOU
ARE
ASKED
TO
IMPLEMENT
SOMETHING
BY
YOURSELF
IT
IS
NOT
OK
TO
USE
OR
EVEN
LOOK
AT
EXISTING
MATLAB
OR
PYTHON
CODE
UNLESS
IT
UTILITY
CODE
IF
YOU
HAVE
QUESTIONS
ABOUT
WHAT
YOU
CAN
USE
ASK
THE
INSTRUCTOR
OR
THE
TA
PART
I
KNEAREST
NEIGHBORS
POINTS
IN
THIS
EXAMPLE
YOU
WILL
IMPLEMENT
AND
EXPLORE
KNN
CLASSIFICATION
YOU
WILL
USE
THE
PIMA
INDIANS
DIABETES
DATASET
SEE
THE
PIMAINDIANSDIABETES
DATA
FILE
THE
LAST
VALUE
IN
EACH
ROW
CONTAINS
THE
TARGET
LABEL
FOR
THAT
ROW
AND
THE
PIMAINDIANSDIABETES
NAMES
FILE
BOTH
FOUND
AT
THE
DATA
FOLDER
YELLOW
LINK
AT
THE
TOP
BEFORE
YOU
BEGIN
SPLIT
THE
DATA
INTO
APPROXIMATELY
EQUALLYSIZED
FOLDS
YOUR
RESULTS
REPORTED
BELOW
SHOULD
BE
AN
AVERAGE
OF
THE
RESULTS
WHEN
YOU
TRAIN
ON
THE
FIRST
FOLDS
AND
TEST
ON
THE
REMAINING
THEN
IF
YOU
TRAIN
ON
THE
FOLDS
NUMBERED
THROUGH
AND
THE
FOLD
AND
TESTING
ON
THE
FOLD
ETC
FOR
SIMPLICITY
YOU
CAN
ALSO
JUST
USE
FOLDS
OF
SIZE
AND
DROP
THE
REMAINING
INSTANCES
MAKE
SURE
TO
NORMALIZE
THE
DATA
X
BY
SUBTRACTING
THE
MEAN
AND
DIVIDING
BY
THE
STANDARD
DEVIATION
OVER
EACH
DIMENSION
NOTE
THAT
YOU
SHOULD
COMPUTE
THE
MEAN
AND
STDEV
USING
THE
TRAINING
DATA
ONLY
AND
THEN
APPLY
THEM
ON
THE
TEST
DATA
THIS
IS
BECAUSE
IN
A
REAL
APPLICATION
WE
DO
NOT
SEE
THE
TEST
DATA
UNTIL
AFTER
WE
SHIP
OFF
OUR
PROGRAM
CODE
IMPLEMENT
KNN
YOUR
FUNCTION
SHOULD
TAKE
IN
AS
INPUTS
A
SCALAR
K
AND
MATRICES
VECTORS
OF
SIZE
NTRAINXD
WHERE
NTRAIN
IS
THE
NUMBER
OF
TRAINING
INSTANCES
AND
D
IS
THE
FEATURE
DIMENSION
OF
SIZE
CONTAINING
THE
LABELS
OF
THE
TRAINING
INSTANCES
AND
OF
SIZE
NTESTXD
IT
SHOULD
OUTPUT
A
VECTOR
OF
SIZE
FOR
EACH
TEST
INSTANCE
COMPUTE
ITS
DISTANCE
TO
ALL
TRAINING
INSTANCES
PICK
THE
CLOSEST
K
TRAINING
INSTANCES
PICK
THE
MOST
COMMON
AMONG
THEIR
LABELS
AND
RETURN
IT
AS
THE
LABEL
FOR
THAT
TEST
INSTANCE
IT
OK
TO
USE
BUILTIN
FUNCTIONS
THAT
COMPUTE
DISTANCES
SORT
COMPUTE
THE
MOST
COMMON
MEMBER
OF
A
LIST
ETC
IN
YOUR
SUBMISSION
REPORT
IN
A
TEXT
FILE
THE
TEST
ACCURACY
WHEN
K
REMEMBER
TO
AVERAGE
THE
TEST
ACCURACY
OVER
THE
FOLDS
SO
FAR
WE
HAVE
BEEN
WEIGHING
NEIGHBORS
EQUALLY
NOW
WE
WANT
TO
EXPERIMENT
WITH
WEIGHING
THEM
ACCORDING
TO
THEIR
DISTANCE
TO
THE
TEST
SAMPLE
OF
INTEREST
IMPLEMENT
A
GAUSSIANWEIGHED
KNN
CLASSIFIER
USING
THE
EQUATION
GIVEN
IN
CLASS
EXPERIMENT
WITH
DIFFERENT
VALUES
OF
THE
BANDWIDTH
PARAMETER
Σ
FROM
THE
EQUATIONS
ON
THE
BOARD
AND
REPORT
THE
RESULTS
REMEMBER
THAT
YOUR
PLOT
SHOULD
BE
AVERAGED
OVER
THE
TEST
FOLDS
PART
II
FISHER
LINEAR
DISCRIMINANT
POINTS
YOU
HAVE
THE
FOLLOWING
TWODIMENSIONAL
DATA
THE
FIRST
FIVE
DATA
POINTS
BELONG
TO
ONE
CLASS
AND
THE
SECOND
SET
OF
FIVE
TO
A
SECOND
CLASS
WRITE
A
FUNCTION
THAT
COMPUTES
THE
DIRECTION
OF
THE
W
VECTOR
CORRESPONDING
TO
THE
FISHER
LINEAR
DISCRIMINANT
OF
THE
DATA
POINTS
APPLY
THE
FUNCTION
TO
THE
GIVEN
DATA
PLOT
BOTH
THE
POINTS
AND
THE
DIRECTION
ALONG
WHICH
THEY
ARE
PROJECTED
SAVE
THE
FIGURE
AND
INCLUDE
IT
IN
YOUR
SUBMISSION
HINT
THE
SLOPE
OF
THE
LINE
W
ALONG
WHICH
THE
POINTS
ARE
PLOTTED
IS
W
W
HAVE
A
LOOK
AT
THIS
FILE
FOR
STARTER
PLOTTING
CODE
DO
NOT
HESITATE
TO
ASK
THE
TA
FOR
HELP
WITH
PLOTTING
PART
III
PERCEPTRON
POINTS
IN
THIS
PART
YOU
WILL
TRACE
THROUGH
A
RUN
OF
THE
PERCEPTRON
ALGORITHM
USE
THE
DATA
SAMPLES
AND
LABELS
FROM
PART
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
II
BUT
SUBTRACT
FROM
THEIR
COORDINATES
TO
CENTER
THEM
AROUND
THE
ORIGIN
OR
ADD
A
BIAS
THIS
DATA
IS
LINEARLY
SEPARABLE
AND
CAN
BE
PLOTTED
IN
USING
ITS
TWO
FEATURE
DIMENSIONS
YOUR
GOAL
IS
TO
CREATE
FIGURES
SIMILAR
TO
FIGURE
IN
BISHOP
IMPLEMENT
THE
PERCEPTRON
ALGORITHM
AND
USE
IT
ALONG
WITH
SOME
PLOTTING
CODE
YOU
WRITE
TO
TRACE
THROUGH
SEVERAL
ITERATIONS
OF
THE
METHOD
USE
THE
EQUATIONS
FROM
THE
SLIDES
FOR
PERCEPTRON
ON
USE
ALL
THE
DATA
FOR
TRAINING
YOU
NEED
TO
KEEP
TRACK
OF
WHICH
INSTANCES
ARE
MISCLASSIFIED
IN
EACH
STEP
THE
WEIGHT
VECTOR
W
IS
ADJUSTED
USING
ONE
MISCLASSIFIED
EXAMPLE
INSTEAD
OF
A
BASIS
FUNCTION
YOU
WILL
JUST
USE
X
I
E
Φ
X
X
SET
Η
TO
IN
YOUR
SUBMISSION
ZIP
FILE
FOR
THREE
CONSECUTIVE
ITERATIONS
OF
THE
METHOD
SHOW
WHICH
POINTS
ARE
MISCLASSIFIED
AND
THE
CURRENT
W
IN
EACH
ITERATION
OUTPUT
THE
ITERATION
ID
AND
THE
TWO
FEATURE
DIMENSIONS
FOR
THE
MISCLASSIFIED
EXAMPLE
THAT
IS
BEING
USED
TO
CORRECT
THE
W
USE
THE
FEATURE
DIMENSIONS
TO
IDENTIFY
WHICH
POINT
IS
BEING
USED
PLOT
POSITIVE
POINTS
AS
HOLLOW
CIRCLES
AND
NEGATIVES
AS
FILLED
CIRCLES
PLOT
CORRECTLY
CLASSIFIED
POINTS
IN
GREEN
AND
MISCLASSIFIED
ONES
IN
RED
IF
TOO
MANY
OR
TOO
FEW
ITERATIONS
ARE
TAKING
PLACE
TERMINATE
THE
RUN
AND
START
ANOTHER
RUN
DO
NOT
HESITATE
TO
ASK
THE
TA
FOR
HELP
WITH
PLOTTING
REFER
TO
THE
PLOTTING
CODE
FROM
PART
II
PART
IV
SHORT
ANSWERS
SUPPORT
VECTOR
MACHINES
POINTS
PTS
BISHOP
EXERCISE
PTS
BISHOP
EXERCISE
HINTS
EXPAND
THE
NORM
NOTATION
REMEMBERING
THAT
THE
NORM
OF
A
VECTOR
X
IS
XTX
THAT
XTX
IS
THE
SAME
AS
AN
INNER
PRODUCT
OF
X
WITH
ITSELF
AND
TRANSPOSE
DISTRIBUTION
PROPERTIES
THEN
EXPRESS
THE
EXPANDED
FORM
WITH
KERNEL
NOTATION
WHAT
TYPE
OF
KERNEL
DO
YOU
SEE
PTS
BISHOP
EXERCISE
PTS
BISHOP
EXERCISE
HINTS
THIS
IS
A
TWOCLASS
PROBLEM
YOU
HAVE
TWO
CONSTRAINTS
TOTAL
ONE
FOR
THE
POSITIVE
INSTANCE
AND
ONE
FOR
THE
NEGATIVE
INSTANCE
AND
THESE
CONSTRAINTS
ARE
EQUALITIES
BECAUSE
YOUR
POSITIVE
AND
NEGATIVE
POINTS
HAVE
TO
LIE
ON
THE
MARGIN
AND
BE
SUPPORT
VECTORS
USE
LAGRANGE
MULTIPLIERS
TO
MAKE
THE
CONSTRAINTS
PART
OF
THE
OPTIMIZATION
PROBLEM
AND
FIND
WHAT
W
AND
B
EQUAL
PTS
EXAMINE
THE
MATLAB
FUNCTION
QUADPROG
IT
CAN
BE
USED
TRAIN
AN
SVM
FIND
THE
OPTIMAL
W
CONSIDER
THE
INPUT
VARIABLES
H
F
A
B
AEQ
BEQ
LB
UB
WRITE
PSEUDOCODE
THAT
SHOWS
HOW
YOU
SHOULD
SET
EACH
OF
THEM
SO
THAT
THE
QUADRATIC
PROGRAM
THAT
IS
SOLVED
IS
THE
SOLUTION
TO
AN
SVM
ALSO
INCLUDE
PSEUDOCODE
THAT
SHOWS
HOW
TO
COMPUTE
THE
WEIGHT
VECTOR
W
FROM
THE
OUTPUT
OF
QUADPROG
MAKE
SURE
TO
EXPLAIN
IN
YOUR
PSEUDOCODE
WHAT
NOTATION
YOU
ARE
USING
FOR
THE
TRAIN
TEST
FEATURE
LABEL
MATRICES
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
HOMEWORK
DUE
NOTE
IF
YOU
ARE
ASKED
TO
IMPLEMENT
SOMETHING
BY
YOURSELF
IT
IS
NOT
OK
TO
USE
OR
EVEN
LOOK
AT
EXISTING
MATLAB
OR
PYTHON
CODE
UNLESS
IT
UTILITY
CODE
IF
YOU
HAVE
QUESTIONS
ABOUT
WHAT
YOU
CAN
USE
ASK
THE
INSTRUCTOR
OR
THE
TA
PART
I
NEURAL
NETWORKS
POINTS
IN
THIS
EXERCISE
YOU
WILL
TRAIN
AND
EVALUATE
A
VERY
SIMPLE
NEURAL
NETWORK
YOU
WILL
TRAIN
A
NETWORK
WITH
A
SINGLE
HIDDEN
LAYER
YOUR
NETWORK
WILL
HAVE
A
SINGLE
OUTPUT
DIMENSION
I
E
K
YOUR
NETWORK
SHOULD
HAVE
A
TANH
ACTIVATION
FUNCTION
AT
THE
HIDDEN
LAYER
AND
IDENTITY
ACTIVATION
FUNCTION
I
E
YK
AK
AT
THE
OUTPUT
LAYER
THIS
IS
EXACTLY
THE
FUNCTION
WE
USED
TO
ILLUSTRATE
THE
BACKPROP
ALGORITHM
IN
OTHER
WORDS
IF
YOU
DIRECTLY
FOLLOW
THE
SLIDES
FROM
CLASS
SLIDES
FROM
SLIDE
DECK
THIS
WILL
BE
VERY
QUICK
TO
IMPLEMENT
THE
NETWORK
WILL
BE
TRAINED
FOR
A
REGRESSION
TASK
USING
THE
WINE
QUALITY
DATASET
FROM
INCLUDE
ONE
FUNCTION
WHICH
COMPUTES
ACTIVATIONS
FORWARD
PASS
AND
ANOTHER
FUNCTION
WHICH
PERFORMS
TRAINING
USING
BACKPROPAGATION
AND
CALLS
THE
ACTIVATIONCOMPUTATION
FUNCTION
AS
IT
ITERATES
ALSO
USE
THE
FORWARD
PASS
FUNCTION
TO
EVALUATE
YOUR
NETWORK
AFTER
TRAINING
CALL
BOTH
OF
THESE
FUNCTIONS
FROM
A
MAIN
FUNCTION
WHICH
SETS
UP
YOUR
TRAIN
TEST
SPLITS
TRAINS
THE
NEURAL
NETWORK
COMPUTES
PREDICTIONS
ON
THE
TRAIN
TEST
DATA
AND
PRINTS
YOUR
ACCURACY
ON
THE
TRAINING
AND
TEST
SETS
INITIALIZE
YOUR
WEIGHTS
TO
SMALL
RANDOM
NUMBERS
E
G
ON
A
SCALE
OF
EXPERIMENT
WITH
DIFFERENT
VALUES
OF
THE
NUMBER
OF
HIDDEN
NEURONS
M
THE
NUMBER
OF
ITERATIONS
BEFORE
YOU
TERMINATE
TRAINING
THE
LEARNING
RATE
SHOW
PLOTS
IN
YOUR
SUBMISSION
THAT
DEMONSTRATE
WHAT
HAPPENS
AS
YOU
VARY
EACH
OF
THESE
THREE
FACTORS
WHILE
KEEPING
THE
OTHER
TWO
FACTORS
THE
SAME
YOU
CAN
START
WITH
THE
FOLLOWING
VALUES
M
LR
PART
II
CONVOLUTIONAL
NEURAL
NETWORKS
POINTS
YOU
DON
T
NEED
TO
WRITE
ANY
CODE
FOR
THIS
JUST
DO
IT
BY
HAND
IN
THIS
PART
YOU
WILL
COMPUTE
THE
OUTPUT
FROM
APPLYING
A
SINGLE
SET
OF
CONVOLUTION
NONLINEARITY
AND
POOLING
OPERATIONS
ON
A
TOY
EXAMPLE
BELOW
ARE
YOUR
IMAGE
WITH
SIZE
N
AND
YOUR
FILTER
WITH
SIZE
F
A
FIRST
SHOW
THE
OUTPUT
OF
APPLYING
CONVOLUTION
USE
NO
PADDING
AND
A
STRIDE
OF
IN
BOTH
THE
HORIZONTAL
AND
VERTICAL
DIRECTIONS
HOMEWORK
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
HTM
B
SECOND
SHOW
THE
OUTPUT
OF
APPLYING
A
RECTIFIED
LINEAR
UNIT
RELU
ACTIVATION
C
THIRD
SHOW
THE
OUTPUT
OF
APPLYING
MAX
POOLING
OVER
REGIONS
PART
III
ADABOOST
POINTS
IN
THIS
EXERCISE
YOU
WILL
IMPLEMENT
THE
ADABOOST
METHOD
DEFINED
ON
PAGES
IN
BISHOP
SECTION
USE
THE
PIMA
DATASET
FROM
USE
DECISION
STUMPS
AS
YOUR
WEAK
CLASSIFIERS
EACH
DECISION
STUMP
OPERATES
ON
SOME
FEATURE
DIMENSION
AND
USES
SOME
THRESHOLD
OVER
THAT
FEATURE
DIMENSION
TO
MAKE
POSITIVE
NEGATIVE
PREDICTIONS
FOR
EACH
DECISION
STUMP
USE
AT
LEAST
THRESHOLDS
INCLUDE
ONE
FUNCTION
WHICH
DETERMINES
THE
BEST
DECISION
STUMP
THE
ONE
WITH
THE
LOWEST
WEIGHTED
ERROR
ON
THE
TRAINING
DATA
INCLUDE
ANOTHER
FUNCTION
WHICH
IMPLEMENTS
THE
ADABOOST
LOOP
USING
DECISION
STUMPS
AND
OUTPUTS
THE
FINAL
SET
OF
CLASSIFIERS
AND
WEIGHTS
ALPHA
ASSOCIATED
WITH
EACH
ALSO
INCLUDE
A
THIRD
FUNCTION
WHICH
SETS
UP
YOUR
TRAIN
TEST
SPLITS
CALLS
THE
ADABOOST
LOOP
TO
TRAIN
COMPUTES
PREDICTIONS
ON
THE
TRAIN
TEST
DATA
AND
PRINTS
ACCURACY
ON
THE
TRAINING
AND
TEST
SETS
PART
IV
PROBABILITY
REVIEW
POINTS
A
BISHOP
EXERCISE
B
BISHOP
EXERCISE
C
BISHOP
EXERCISE
FIRST
PART
ONLY
HINT
TRANSFORM
THE
RIGHTHAND
SIDE
INTO
THE
LEFTHAND
SIDE
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
MACHINE
LEARNING
OVERVIEW
POLICIES
PROJECT
SCHEDULE
RESOURCES
MACHINE
LEARNING
SPRING
OVERVIEW
COURSE
DESCRIPTION
THE
COURSE
WILL
COVER
THE
FOLLOWING
TOPICS
LEARNING
BASICS
UNSUPERVISED
LEARNING
SUPERVISED
LEARNING
CLASSIFICATION
REGRESSION
CLUSTERING
DIMENSIONALITY
REDUCTION
NEAREST
NEIGHBOR
CLASSIFICATION
SUPPORT
VECTOR
MACHINES
NEURAL
NETWORKS
DENSITY
ESTIMATION
BAYESIAN
BELIEF
NETWORKS
HIDDEN
MARKOV
MODELS
EXPECTATION
MAXIMIZATION
DECISION
TREES
AND
ENSEMBLES
THERE
WILL
BE
HOMEWORK
ASSIGNMENTS
TWO
EXAMS
AND
A
FINAL
PROJECT
PREREQUISITES
MATH
STAT
STAT
THE
EXPECTATION
IS
THAT
YOU
CAN
PROGRAM
AND
ANALYZE
THE
EFFICIENCY
AND
PERFORMANCE
OF
PROGRAMS
YOU
SHOULD
ALSO
BE
ABLE
TO
COMPUTE
DERIVATIVES
OF
FUNCTIONS
FURTHER
SOME
EXPERIENCE
WITH
LINEAR
ALGEBRA
MATRIX
AND
VECTOR
OPERATIONS
AND
PROBABILITY
IS
EXPECTED
PIAZZA
SIGN
UP
FOR
IT
HERE
NOTE
THAT
WE
WILL
USE
PIAZZA
FOR
TWO
MAIN
PURPOSES
FOR
ANNOUNCEMENTS
AND
FOR
CLASSMATETOCLASSMATES
DISCUSSION
OF
HOMEWORK
PROBLEMS
ETC
THE
INSTRUCTOR
WILL
MONITOR
PIAZZA
INFREQUENTLY
THE
TIME
WHEN
YOU
SHOULD
ASK
THE
INSTRUCTOR
OR
TA
QUESTIONS
IS
DURING
OFFICE
HOURS
PROGRAMMING
LANGUAGES
FOR
HOMEWORK
ASSIGNMENTS
YOU
CAN
USE
MATLAB
OR
PYTHON
FOR
THE
COURSE
PROJECT
YOU
CAN
USE
ANY
LANGUAGE
OF
YOUR
CHOICE
TEXTBOOKS
WE
WILL
HAVE
REQUIRED
READINGS
FROM
TWO
TEXTBOOKS
IN
SOME
CASES
THE
READINGS
WILL
BE
OVERLAPPING
BUT
IT
HELPS
TO
READ
TWO
PHRASINGS
OF
THE
SAME
IDEA
IN
OTHER
CASES
ONE
READING
IS
MORE
COMPLETE
OR
SOMETIMES
MORE
INTUITIVE
THAN
THE
OTHER
CHRISTOPHER
M
BISHOP
PATTERN
RECOGNITION
AND
MACHINE
LEARNING
SPRINGER
KEVIN
P
MURPHY
MACHINE
LEARNING
A
PROBABILISTIC
PERSPECTIVE
MIT
PRESS
FULL
TEXT
AVAILABLE
ONLINE
THROUGH
THE
PITT
LIBRARY
YOU
CAN
ALSO
REFER
TO
THE
FOLLOWING
TWO
TEXTBOOKS
FOR
ADDITIONAL
EXAMPLES
AND
EXPLANATIONS
TREVOR
HASTIE
ROBERT
TIBSHIRANI
AND
JEROME
FRIEDMAN
THE
ELEMENTS
OF
STATISTICAL
LEARNING
SPRINGER
AVAILABLE
ONLINE
ON
THE
SECOND
AUTHOR
PAGE
DAVID
BARBER
BAYESIAN
REASONING
AND
MACHINE
LEARNING
CAMBRIDGE
UNIVERSITY
PRESS
AVAILABLE
ONLINE
ON
THE
AUTHOR
PAGE
TOP
POLICIES
GRADING
GRADING
WILL
BE
BASED
ON
THE
FOLLOWING
COMPONENTS
HOMEWORK
ASSIGNMENTS
ASSIGNMENTS
X
EACH
COURSE
PROJECT
MIDTERM
AND
FINAL
EXAM
MIDTERM
FINAL
PARTICIPATION
HOMEWORK
SUBMISSION
MECHANICS
YOU
WILL
SUBMIT
YOUR
HOMEWORK
USING
COURSEWEB
NAVIGATE
TO
THE
COURSEWEB
PAGE
FOR
THEN
CLICK
ON
ASSIGNMENTS
ON
THE
LEFT
AND
THE
CORRESPONDING
HOMEWORK
ID
YOUR
WRITTEN
ANSWERS
SHOULD
BE
A
SINGLE
PDF
DOC
DOCX
FILE
YOUR
SOURCE
CODE
SHOULD
BE
A
SINGLE
ZIP
FILE
ALSO
INCLUDING
IMAGES
RESULTS
IF
REQUESTED
NAME
THE
FILE
EXTENSION
PLEASE
COMMENT
YOUR
CODE
HOMEWORK
IS
DUE
AT
ON
THE
DUE
DATE
GRADES
WILL
BE
POSTED
ON
COURSEWEB
EXAMS
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
THERE
WILL
BE
ONE
INCLASS
MIDTERM
EXAM
AND
A
FINAL
EXAM
WHICH
WILL
FOCUS
ON
MATERIAL
FROM
THE
LATTER
PART
OF
THE
COURSE
THERE
WILL
BE
NO
MAKEUP
EXAMS
UNLESS
YOU
OR
A
CLOSE
RELATIVE
IS
SERIOUSLY
ILL
PARTICIPATION
STUDENTS
ARE
EXPECTED
TO
REGULARLY
ATTEND
THE
CLASS
LECTURES
AND
SHOULD
ACTIVELY
ENGAGE
IN
INCLASS
DISCUSSIONS
ATTENDANCE
WILL
NOT
BE
TAKEN
BUT
KEEP
IN
MIND
THAT
IF
YOU
DON
T
ATTEND
YOU
CANNOT
PARTICIPATE
YOU
CAN
ACTIVELY
PARTICIPATE
BY
FOR
EXAMPLE
RESPONDING
TO
THE
INSTRUCTOR
OR
OTHERS
QUESTIONS
ASKING
QUESTIONS
OR
MAKING
MEANINGFUL
REMARKS
AND
COMMENTS
ABOUT
THE
LECTURE
AND
ANSWERING
OTHERS
QUESTIONS
ON
PIAZZA
YOU
ARE
ALSO
ENCOURAGED
TO
BRING
IN
RELEVANT
ARTICLES
YOU
SAW
IN
THE
NEWS
LATE
POLICY
ON
YOUR
PROGRAMMING
ASSIGNMENTS
ONLY
YOU
GET
FREE
LATE
DAYS
COUNTED
IN
MINUTES
I
E
YOU
CAN
SUBMIT
A
TOTAL
OF
HOURS
LATE
FOR
EXAMPLE
YOU
CAN
SUBMIT
ONE
HOMEWORK
HOURS
LATE
AND
ANOTHER
HOURS
LATE
ONCE
YOU
VE
USED
UP
YOUR
FREE
LATE
DAYS
YOU
WILL
INCUR
A
PENALTY
OF
FROM
THE
TOTAL
ASSIGNMENT
CREDIT
POSSIBLE
FOR
EACH
LATE
DAY
A
LATE
DAY
IS
ANYTHING
FROM
MINUTE
TO
HOURS
NOTE
THIS
POLICY
DOES
NOT
APPLY
TO
COMPONENTS
OF
THE
PROJECT
COLLABORATION
POLICY
AND
ACADEMIC
HONESTY
YOU
WILL
DO
YOUR
WORK
EXAMS
AND
HOMEWORK
INDIVIDUALLY
THE
ONLY
EXCEPTION
IS
THE
PROJECT
WHICH
CAN
BE
DONE
IN
PAIRS
THE
WORK
YOU
TURN
IN
MUST
BE
YOUR
OWN
WORK
YOU
ARE
ALLOWED
TO
DISCUSS
THE
ASSIGNMENTS
WITH
YOUR
CLASSMATES
BUT
DO
NOT
LOOK
AT
CODE
THEY
MIGHT
HAVE
WRITTEN
FOR
THE
ASSIGNMENTS
OR
AT
THEIR
WRITTEN
ANSWERS
YOU
ARE
ALSO
NOT
ALLOWED
TO
SEARCH
FOR
CODE
ON
THE
INTERNET
USE
SOLUTIONS
POSTED
ONLINE
UNLESS
YOU
ARE
EXPLICITLY
ALLOWED
TO
LOOK
AT
THOSE
OR
TO
USE
MATLAB
OR
PYTHON
IMPLEMENTATION
IF
YOU
ARE
ASKED
TO
WRITE
YOUR
OWN
CODE
WHEN
IN
DOUBT
ABOUT
WHAT
YOU
CAN
OR
CANNOT
USE
ASK
THE
INSTRUCTOR
PLAGIARISM
WILL
CAUSE
YOU
TO
FAIL
THE
CLASS
AND
RECEIVE
DISCIPLINARY
PENALTY
PLEASE
CONSULT
THE
UNIVERSITY
GUIDELINES
ON
ACADEMIC
INTEGRITY
NOTE
ON
DISABILITIES
IF
YOU
HAVE
A
DISABILITY
FOR
WHICH
YOU
ARE
OR
MAY
BE
REQUESTING
AN
ACCOMMODATION
YOU
ARE
ENCOURAGED
TO
CONTACT
BOTH
YOUR
INSTRUCTOR
AND
DISABILITY
RESOURCES
AND
SERVICES
DRS
WILLIAM
PITT
UNION
DRSRECEP
PITT
EDU
FOR
ASL
USERS
AS
EARLY
AS
POSSIBLE
IN
THE
TERM
DRS
WILL
VERIFY
YOUR
DISABILITY
AND
DETERMINE
REASONABLE
ACCOMMODATIONS
FOR
THIS
COURSE
NOTE
ON
MEDICAL
CONDITIONS
IF
YOU
HAVE
A
MEDICAL
CONDITION
WHICH
WILL
PREVENT
YOU
FROM
DOING
A
CERTAIN
ASSIGNMENT
YOU
MUST
INFORM
THE
INSTRUCTOR
OF
THIS
BEFORE
THE
DEADLINE
YOU
MUST
THEN
SUBMIT
DOCUMENTATION
OF
YOUR
CONDITION
WITHIN
A
WEEK
OF
THE
ASSIGNMENT
DEADLINE
STATEMENT
ON
CLASSROOM
RECORDING
TO
ENSURE
THE
FREE
AND
OPEN
DISCUSSION
OF
IDEAS
STUDENTS
MAY
NOT
RECORD
CLASSROOM
LECTURES
DISCUSSION
AND
OR
ACTIVITIES
WITHOUT
THE
ADVANCE
WRITTEN
PERMISSION
OF
THE
INSTRUCTOR
AND
ANY
SUCH
RECORDING
PROPERLY
APPROVED
IN
ADVANCE
CAN
BE
USED
SOLELY
FOR
THE
STUDENT
OWN
PRIVATE
USE
TOP
PROJECT
A
PROJECT
CAN
BE
TYPE
A
DESIGN
OF
A
NEW
METHOD
FOR
AN
EXISTING
PROBLEM
OR
AN
APPLICATION
OF
TECHNIQUES
WE
STUDIED
IN
CLASS
OR
ANOTHER
METHOD
TO
A
NEW
PROBLEM
THAT
WE
HAVE
NOT
DISCUSSED
IN
CLASS
TYPE
B
EXPERIMENTAL
COMPARISON
OF
A
NUMBER
OF
EXISTING
TECHNIQUES
ON
A
KNOWN
PROBLEM
AND
DETAILED
DISCUSSION
AND
ANALYSIS
OF
THE
RESULTS
TYPE
C
AN
EXTENSIVE
LITERATURE
REVIEW
AND
ANALYSIS
ON
ONE
OF
THE
TOPICS
COVERED
IN
CLASS
MILESTONES
FOR
THE
PROJECT
PROPOSAL
NOT
FOR
A
GRADE
BUT
SHOULD
BE
SUBMITTED
AIM
FOR
AT
LEAST
PAGES
THE
BETTER
THOUGHTOUT
THIS
IS
THE
MORE
FEEDBACK
THE
INSTRUCTOR
CAN
GIVE
ALSO
THINK
ABOUT
WHAT
DATA
AND
OR
CODE
YOU
WILL
USE
DRAFT
OF
FINAL
GRADE
THIS
SHOULD
BE
LIKE
A
FINAL
VERSION
OF
YOUR
FINAL
REPORT
AND
SHOULD
HAVE
ALL
SECTIONS
THAT
YOUR
FINAL
REPORT
WOULD
HAVE
ALTHOUGH
SOME
OF
THEM
WILL
BE
INCOMPLETE
YET
SHOWING
AS
MUCH
PROGRESS
AS
YOU
CAN
THE
EXPECTATION
IS
THAT
AT
THIS
POINT
YOU
HAVE
DONE
OR
OF
THE
REQUIRED
WORK
FOR
THIS
PROJECT
PRESENTATION
OF
FINAL
GRADE
AIM
TO
BE
CLEAR
ENTHUSIASTIC
AND
CONCISE
YOU
NEED
TO
SUBMIT
ON
COURSEWEB
THE
PRESENTATION
FILE
ON
THE
DAY
OF
YOUR
PRESENTATION
FOR
THE
INSTRUCTOR
REFERENCE
FINAL
REPORT
OF
FINAL
GRADE
USE
SOME
EXISTING
CONFERENCE
TEMPLATE
THE
FINAL
REPORT
SHOULD
RESEMBLE
A
CONFERENCE
PAPER
AND
SHOULD
INCLUDE
AS
APPLICABLE
CLEAR
PROBLEM
DEFINITION
AND
ARGUMENTATION
OF
WHY
THIS
PROBLEM
IS
IMPORTANT
OVERVIEW
OF
RELATED
WORK
DETAILED
EXPLANATION
OF
THE
APPROACH
WELLMOTIVATED
EXPERIMENTAL
EVALUATION
INCLUDING
SETUP
DESCRIPTION
AND
A
DESCRIPTION
OF
WHAT
EACH
TEAM
MEMBER
DID
GENERAL
RULES
STUDENTS
ARE
ENCOURAGED
TO
WORK
IN
GROUPS
OF
TWO
FOR
THEIR
FINAL
PROJECT
THE
ONLY
EXCEPTION
IS
THE
LITERATURE
REVIEW
WHICH
CAN
ONLY
BE
DONE
BY
STUDENTS
WORKING
INDIVIDUALLY
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
THE
PROJECT
SHOULD
INCLUDE
SOME
AMOUNT
OF
NOVELTY
YOU
ARE
ENCOURAGED
TO
USE
ANY
EXTERNAL
EXPERTISE
YOU
MIGHT
HAVE
E
G
BIOLOGY
PHYSICS
ETC
SO
THAT
YOUR
PROJECT
MAKES
THE
BEST
USE
OF
AREAS
YOU
KNOW
WELL
AND
IS
AS
INTERESTING
AS
POSSIBLE
COMBINING
YOUR
FINAL
PROJECT
FOR
THIS
CLASS
AND
ANOTHER
CLASS
IS
GENERALLY
PERMITTED
BUT
THE
PROJECT
PROPOSAL
AND
FINAL
REPORT
SHOULD
CLEARLY
OUTLINE
WHAT
PART
OF
THE
WORK
WAS
DONE
TO
GET
CREDIT
IN
THIS
CLASS
AND
THE
INSTRUCTOR
SHOULD
APPROVE
THE
PROPOSED
BREAKDOWN
OF
WORK
BETWEEN
THIS
AND
ANOTHER
CLASS
THE
FINAL
REPORT
SHOULD
BE
SELFCONTAINED
I
E
THE
INSTRUCTOR
SHOULD
NOT
HAVE
TO
READ
ANY
OTHER
PAPERS
TO
UNDERSTAND
WHAT
YOU
DID
ALL
PROJECT
WRITTEN
ITEMS
ARE
DUE
AT
ON
COURSEWEB
IF
YOU
ARE
PROPOSING
A
NEW
PROBLEM
OR
A
NEW
SOLUTION
TO
AN
EXISTING
PROBLEM
THE
PROJECT
SHOULD
INCLUDE
SOME
AMOUNT
OF
NOVELTY
FOR
EXAMPLE
YOU
CANNOT
JUST
REIMPLEMENT
AN
EXISTING
PAPER
OR
PROJECT
YOU
SHOULD
COME
UP
WITH
A
NEW
METHOD
OR
APPLY
AN
EXISTING
METHOD
FOR
A
NEW
PROBLEM
DO
NOT
RELY
ON
DATA
COLLECTION
TO
BE
THE
NOVEL
COMPONENT
OF
YOUR
WORK
IF
YOU
ARE
PROPOSING
TO
TACKLE
A
NEW
PROBLEM
YOU
MIGHT
NEED
TO
COLLECT
DATA
BUT
WHILE
THIS
IS
A
CONTRIBUTION
IT
WILL
NOT
BE
ENOUGH
TO
EARN
A
GOOD
PROJECT
GRADE
YOU
STILL
HAVE
TO
COME
UP
WITH
A
SOLID
METHOD
IDEA
I
E
YOUR
PROJECT
HAS
TO
HAVE
SUFFICIENT
TECHNICAL
NOVELTY
YOU
MUST
SHOW
THAT
YOUR
METHOD
IS
IN
SOME
SENSE
BETTER
QUANTITATIVELY
THAN
AT
LEAST
SOME
RELATIVELY
RECENT
EXISTING
METHODS
FOR
EXAMPLE
YOU
CAN
SHOW
THAT
YOUR
METHOD
ACHIEVES
SUPERIOR
ACCURACY
IN
SOME
PREDICTION
TASK
COMPARED
TO
PRIOR
METHODS
OR
THAT
IT
ACHIEVES
COMPARABLE
ACCURACY
BUT
IS
FASTER
THIS
OUTCOME
IS
NOT
GUARANTEED
TO
COME
OUT
THE
WAY
YOU
INTENDED
DURING
THE
LIMITED
TIMESPAN
OF
A
COURSE
PROJECT
SO
WHETHER
OR
NOT
YOUR
OUTPERFORM
THE
STATE
OF
THE
ART
WILL
ONLY
BE
A
SMALL
COMPONENT
OF
YOUR
GRADE
FURTHER
IF
YOU
PROPOSE
A
SUFFICIENTLY
INTERESTING
METHOD
RATHER
THAN
AN
EXTREMELY
SIMPLE
METHOD
IT
WILL
BE
LESS
OF
A
PROBLEM
IF
YOUR
METHOD
DOES
NOT
OUTPERFORM
OTHER
EXISTING
APPROACHES
TO
THE
PROBLEM
EACH
OF
THE
FOLLOWING
COMPONENTS
WILL
BE
GRADED
HOW
WELL
YOU
INTRODUCED
AND
MOTIVATED
THE
PROBLEM
IN
YOUR
PRESENTATION
AND
FINAL
REPORT
HOW
WELL
YOU
RESEARCHED
AND
PRESENTED
THE
RELEVANT
WORK
IN
THE
AREA
YOU
ARE
TACKLING
HOW
TECHNICALLY
SOLID
AND
NOVEL
YOUR
METHOD
IS
HOW
WELL
YOU
EXPERIMENTALLY
TESTED
YOUR
METHOD
AND
ANALYTICALLY
DISCUSSED
YOUR
EXPERIMENTAL
FINDINGS
HOW
WELL
YOU
WERE
ABLE
TO
DRAW
CONCLUSIONS
FROM
YOUR
WORK
AND
DISCUSS
POTENTIAL
FUTURE
WORK
TO
FURTHER
IMPROVE
ON
THE
PROBLEM
YOU
PROPOSED
TO
TACKLE
YOU
ARE
ALLOWED
TO
USE
EXISTING
CODE
FOR
KNOWN
METHODS
BUT
AGAIN
NOTICE
THAT
YOUR
PROJECT
IS
EXPECTED
TO
BE
A
SIGNIFICANT
AMOUNT
OF
WORK
AND
NOT
JUST
A
STRAIGHTUP
RUN
OF
SOME
PACKAGE
THIS
TYPE
OF
PROJECT
HAS
THE
HIGHEST
CHANCE
OF
TURNING
INTO
A
PUBLISHED
WORKSHOP
OR
CONFERENCE
PAPER
EVEN
FOR
THIS
TYPE
OF
PROJECT
YOU
SHOULD
PRESENT
A
VERY
BRIEF
LITERATURE
REVIEW
DURING
YOUR
PRESENTATION
SO
YOUR
CLASSMATES
KNOW
THE
SPACE
IN
WHICH
YOU
ARE
WORKING
A
GOOD
SOURCE
FOR
LEARNING
ABOUT
WHAT
WORK
HAS
BEEN
DONE
IN
YOUR
DOMAIN
OF
INTEREST
ARE
SEARCH
ENGINES
GOOGLE
SCHOLAR
AND
ARXIV
ORG
IF
YOU
ARE
PROPOSING
A
LITERATURE
REVIEW
OR
ARE
PROPOSING
TO
EXPERIMENTALLY
COMPARE
EXISTING
SOLUTIONS
TO
A
KNOWN
PROBLEM
WHAT
YOU
ARE
PROPOSING
TO
DO
SHOULD
NOT
ALREADY
HAVE
BEEN
DONE
IN
ANOTHER
PUBLISHED
PAPER
INCLUDING
PAPERS
ON
ARXIV
ORG
YOU
HAVE
TO
PROPERLY
INTRODUCE
AND
MOTIVATE
THE
PROBLEM
YOU
CHOSE
TO
STUDY
I
E
WHY
IT
IS
IMPORTANT
AND
WHY
IT
IS
CHALLENGING
FOR
EXPERIMENTAL
COMPARISONS
YOU
STILL
NEED
TO
PRESENT
A
DETAILED
LITERATURE
REVIEW
FOR
THE
TOPIC
AT
HAND
YOU
MUST
REVIEW
AND
INCLUDE
DETAILED
DESCRIPTIONS
IN
YOUR
FINAL
REPORT
OF
AT
LEAST
PAPERS
IF
CODE
IS
NOT
AVAILABLE
FOR
MOST
OF
THE
PAPERS
YOU
CHOSE
TO
IMPLEMENT
YOU
NEED
TO
EXPERIMENTALLY
COMPARE
AT
LEAST
PAPERS
IN
YOUR
IMPLEMENTATION
OF
PAPERS
WITHOUT
CODE
YOU
DO
NOT
HAVE
TO
FOLLOW
THE
PAPERS
IN
EVERY
DETAIL
BUT
YOUR
IMPLEMENTATION
SHOULD
BE
FAITHFUL
TO
THE
PAPER
YOU
ARE
IMPLEMENTING
IN
SPIRIT
YOU
SHOULD
IMPLEMENT
RATHER
THAN
USE
EXISTING
CODE
FOR
AT
LEAST
ONE
OF
THE
METHODS
YOU
COMPARE
AGAINST
E
G
YOU
MIGHT
USE
CODE
FOR
PAPERS
AND
IMPLEMENT
ADDITIONAL
PAPER
MAKE
SURE
TO
INCLUDE
A
CAREFUL
JUSTIFICATION
WHY
THESE
ARE
THE
ONES
YOU
CHOSE
TO
IMPLEMENT
MAKE
SURE
TO
INCLUDE
A
DETAILED
ANALYSIS
OF
THE
STRENGTHS
AND
WEAKNESSES
OF
EACH
PAPER
YOU
CHOSE
TO
COMPARE
BASED
ON
BOTH
THE
PUBLISHED
PAPERS
AS
WELL
AS
THE
EXPERIMENTAL
FINDINGS
YOU
COLLECTED
OVER
THE
COURSE
OF
THE
PROJECT
FOR
LITERATURE
REVIEWS
YOUR
FINAL
REPORT
SHOULD
INCLUDE
AT
LEAST
REFERENCES
IT
SHOULD
SHOW
A
SENSIBLE
ORGANIZATION
OF
THESE
REFERENCES
AND
AT
LEAST
ONE
PARAGRAPH
CONTAINING
DETAILS
ABOUT
EACH
PAPER
INCLUDING
AT
LEAST
SENTENCES
DESCRIBING
THE
METHOD
IN
EACH
OF
THE
REFERENCED
WORKS
MAKE
SURE
TO
DESCRIBE
BOTH
THE
TECHNICAL
DETAILS
AND
THE
EXPERIMENTAL
TECHNIQUES
USED
IN
EACH
OF
THE
PAPERS
YOU
PRESENT
MAKE
SURE
TO
DISCUSS
SOME
STRENGTHS
AND
WEAKNESSES
OF
EACH
PAPER
YOU
INCLUDE
IN
YOUR
REVIEW
ALSO
INCLUDE
A
SYNTHESIS
SUMMARY
OF
WHAT
HAS
BEEN
ACCOMPLISHED
IN
THE
COMMUNITY
ON
THE
PROBLEM
YOU
CHOSE
TO
STUDY
GROUPED
BY
THE
THEMES
OF
THE
PAPERS
AND
WHAT
FUTURE
WORK
MIGHT
BE
LITERATURE
REVIEWS
CAN
ONLY
BE
DONE
IN
TEAMS
OF
ONE
FOR
SOURCES
OF
IDEAS
FOR
COMPUTER
VISION
PROJECT
IDEAS
YOU
CAN
LOOK
AT
THE
LIST
OF
DATASETS
AND
TASKS
BELOW
FOR
INSPIRATION
OR
READ
SOME
PAPER
ABSTRACTS
ON
THIS
PAGE
FOR
NLP
PROJECT
IDEAS
SEE
THIS
PAGE
FROM
CHRISTOPHER
MANNING
ALSO
LOOK
AT
THE
FOLLOWING
LIST
OF
PROJECT
SUGGESTIONS
FROM
RAY
MOONEY
BUT
PLEASE
DO
NOT
CONTACT
ANY
OF
THE
CONTACTS
GIVEN
THIS
ONE
FROM
CARLOS
GUESTRIN
THIS
ONE
FROM
ANDREAS
KRAUSE
AND
THIS
ONE
FROM
ANDREW
NG
TOP
SCHEDULE
DATE
CHAPTER
TOPIC
READINGS
LECTURE
SLIDES
DUE
INTRODUCTION
MURPHY
CH
BISHOP
SEC
PPTX
PDF
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
INTRO
LINEAR
ALGEBRA
AND
MATLAB
PPTX
PDF
UNSUPERVISED
LEARNING
CLUSTERING
BISHOP
SEC
MURPHY
SEC
PPTX
PDF
OUT
DIMENSIONALITY
REDUCTION
BISHOP
SEC
MURPHY
SEC
PPTX
PDF
NOTES
REGRESSION
LINE
FITTING
BIASVARIANCE
BISHOP
SEC
PPTX
PDF
LINEAR
REGRESSION
BISHOP
SEC
MURPHY
SEC
SEC
PPTX
PDF
NOTES
CLASSIFICATION
INTRO
AND
LINEAR
MODELS
NEAREST
NEIGHBORS
BISHOP
SEC
LINEAR
MODELS
FOR
CLASSIFICATION
BISHOP
SEC
MURPHY
CH
DUE
SUPPORT
VECTOR
MACHINES
BISHOP
SEC
MURPHY
SEC
MIDTERM
EXAM
CLASSIFICATION
NONLINEAR
MODELS
NEURAL
NETWORKS
BISHOP
SEC
SKIP
MURPHY
SEC
CONVOLUTIONAL
NEURAL
NETWORKS
KARPATHY
NOTES
MODULE
PROJECT
PROPOSAL
DUE
SPRING
BREAK
NO
CLASS
RECURRENT
NEURAL
NETWORKS
DECISION
TREES
BAGGING
AND
BOOSTING
BISHOP
SEC
SKIP
MURPHY
SEC
CLASSIFICATION
PROBABILISTIC
MODELS
PROBABILITY
REVIEW
DENSITY
ESTIMATION
BISHOP
SEC
MURPHY
SEC
BAYESIAN
BELIEF
NETWORKS
BISHOP
SEC
MURPHY
CH
MARKOV
RANDOM
FIELDS
HIDDEN
MARKOV
MODELS
BISHOP
SEC
JURAFSKY
MARTIN
SKIP
MURPHY
SEC
PROJECT
DRAFT
DUE
EXPECTATION
MAXIMIZATION
BISHOP
SEC
MURPHY
CH
WRAPUP
PROJECTS
EXAM
RESEARCH
TOPICS
PROJECT
PRESENTATIONS
PROJECT
FINAL
REPORT
DUE
MACHINE
LEARNING
HTTP
PEOPLE
CS
PITT
EDU
KOVASHKA
FINAL
EXAM
TOP
RESOURCES
THIS
COURSE
WAS
INSPIRED
BY
THE
FOLLOWING
COURSES
MACHINE
LEARNING
BY
MILOS
HAUSKRECHT
UNIVERSITY
OF
PITTSBURGH
SPRING
INTRODUCTION
TO
MACHINE
LEARNING
BY
DHRUV
BATRA
VIRGINIA
TECH
SPRING
MACHINE
LEARNING
BY
TOMMI
JAAKKOLA
MIT
MACHINE
LEARNING
BY
SUBHRANSU
MAJI
UMASS
AMHREST
SPRING
MACHINE
LEARNING
BY
ERIK
SUDDERTH
BROWN
UNIVERSITY
FALL
COMPUTER
VISION
BY
KRISTEN
GRAUMAN
UT
AUSTIN
SPRING
COMPUTER
VISION
BY
DEREK
HOIEM
UIUC
SPRING
NATURAL
LANGUAGE
PROCESSING
BY
RAY
MOONEY
UT
AUSTIN
TUTORIALS
MATLAB
TUTORIAL
LINEAR
ALGEBRA
REVIEW
BY
FEIFEI
LI
BRIEF
MACHINE
LEARNING
INTRO
BY
ADITYA
KHOSLA
AND
JOSEPH
LIM
RESOURCES
LIST
INCLUDING
CODE
AND
DATA
TUTORIALS
AND
OTHER
RELATED
COURSES
COMPILED
BY
DEVI
PARIKH
SOME
COMPUTER
VISION
DATASETS
MICROSOFT
COCO
COMMON
OBJECTS
IN
CONTEXT
OBJECT
RECOGNITION
SEGMENTATION
IMAGE
DESCRIPTION
IMAGENET
OBJECT
RECOGNITION
SUN
DATABASE
SCENES
CALTECHUCSD
BIRDS
FINEGRAINED
OBJECT
RECOGNITION
MSRC
ANNOTATIONS
ACTIVE
LEARNING
ANIMALS
WITH
ATTRIBUTES
ATTRIBUTEBASED
RECOGNITION
APASCAL
AYAHOO
ATTRIBUTEBASED
RECOGNITION
SHOES
ATTRIBUTEBASED
SEARCH
INRIA
MOVIE
ACTIONS
ACTION
RECOGNITION
ADL
EGOCENTRIC
ACTION
RECOGNITION
ACTION
QUALITY
EVALUATING
ACTION
QUALITY
CARDB
HISTORICAL
CARS
STYLE
CLASSIFICATION
OF
CARS
RECOGNIZING
IMAGE
STYLE
PHOTOGRAPHIC
STYLE
CLASSIFICATION
JUDD
GAZE
VISUAL
SALIENCY
PREDICTION
VISUAL
PERSUASION
PREDICTING
SUBTLE
MESSAGES
IN
IMAGES
VQA
VISUAL
QUESTIONANSWERING
RECOGNITION
DATASETS
LIST
COMPILED
BY
KRISTEN
GRAUMAN
HUMAN
ACTIVITY
DATASETS
LIST
COMPILED
BY
CHAOYEH
CHEN
SOME
CODE
OF
INTEREST
LIBSVM
BY
CHIHCHUNG
CHANG
AND
CHIHJEN
LIN
SVM
LIGHT
BY
THORSTEN
JOACHIMS
VLFEAT
FEATURE
EXTRACTION
TUTORIALS
AND
MORE
BY
ANDREA
VEDALDI
CAFFE
DEEP
LEARNING
CODE
BY
YANGQING
JIA
ET
AL
TOP
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
JANUARY
ABOUT
THE
INSTRUCTOR
BORN
IN
SOFIA
BULGARIA
GOT
BA
IN
AT
POMONA
COLLEGE
CA
COMPUTER
SCIENCE
MEDIA
STUDIES
GOT
PHD
IN
AT
UNIVERSITY
OF
TEXAS
AT
AUSTIN
COMPUTER
VISION
COURSE
INFO
COURSE
WEBSITE
INSTRUCTOR
ADRIANA
KOVASHKA
KOVASHKA
CS
PITT
EDU
USE
AT
THE
BEGINNING
OF
YOUR
SUBJECT
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
TUE
THU
TA
LONGHAO
LI
CS
PITT
EDU
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
TBD
DO
THE
DOODLE
BY
THE
END
OF
FRIDAY
NOTE
LONGHAO
IS
OUT
OF
THE
COUNTRY
UNTIL
JAN
PLEASE
EMAIL
HIM
ANY
QUESTIONS
TEXTBOOKS
CHRISTOPHER
M
BISHOP
PATTERN
RECOGNITION
AND
MACHINE
LEARNING
SPRINGER
KEVIN
P
MURPHY
MACHINE
LEARNING
A
PROBABILISTIC
PERSPECTIVE
MIT
PRESS
MORE
RESOURCES
AVAILABLE
ON
COURSE
WEBPAGE
YOUR
NOTES
FROM
CLASS
ARE
YOUR
BEST
STUDY
MATERIAL
SLIDES
ARE
NOT
COMPLETE
WITH
NOTES
COURSE
GOALS
TO
LEARN
THE
BASIC
MACHINE
LEARNING
TECHNIQUES
BOTH
FROM
A
THEORETICAL
AND
PRACTICAL
PERSPECTIVE
TO
LEARN
HOW
TO
APPLY
THESE
TECHNIQUES
ON
TOY
PROBLEMS
TO
GET
EXPERIENCE
WITH
THESE
TECHNIQUES
ON
A
REAL
WORLD
PROBLEM
POLICIES
AND
SCHEDULE
SHOULD
I
TAKE
THIS
CLASS
IT
WILL
BE
A
LOT
OF
WORK
BUT
YOU
WILL
LEARN
A
LOT
SOME
PARTS
WILL
BE
HARD
AND
REQUIRE
THAT
YOU
PAY
CLOSE
ATTENTION
BUT
I
WILL
HAVE
PERIODIC
UNGRADED
POP
QUIZZES
TO
SEE
HOW
YOU
RE
DOING
I
WILL
ALSO
PICK
ON
STUDENTS
RANDOMLY
TO
ANSWER
QUESTIONS
USE
INSTRUCTOR
AND
TA
OFFICE
HOURS
QUESTIONS
INTRODUCTIONS
WHAT
IS
MACHINE
LEARNING
EXAMPLE
PROBLEMS
AND
TASKS
ML
IN
A
NUTSHELL
CHALLENGES
MEASURING
PERFORMANCE
WHAT
IS
YOUR
NAME
WHAT
ONE
THING
OUTSIDE
OF
SCHOOL
ARE
YOU
PASSIONATE
ABOUT
DO
YOU
HAVE
ANY
PRIOR
EXPERIENCE
WITH
MACHINE
LEARNING
WHAT
DO
YOU
HOPE
TO
GET
OUT
OF
THIS
CLASS
EVERY
TIME
YOU
SPEAK
PLEASE
REMIND
ME
YOUR
NAME
FINDING
PATTERNS
AND
RELATIONSHIPS
IN
DATA
WE
CAN
APPLY
THESE
PATTERNS
TO
MAKE
USEFUL
PREDICTIONS
E
G
WE
CAN
PREDICT
HOW
MUCH
A
USER
WILL
LIKE
A
MOVIE
EVEN
THOUGH
THAT
USER
NEVER
RATED
THAT
MOVIE
NETFLIX
CHALLENGE
GIVEN
LOTS
OF
DATA
ABOUT
HOW
USERS
RATED
MOVIES
TRAINING
DATA
BUT
WE
DON
T
KNOW
HOW
USER
I
WILL
RATE
MOVIE
J
AND
WANT
TO
PREDICT
THAT
TEST
DATA
SPAM
OR
NOT
VS
WEATHER
PREDICTION
WHO
WILL
WIN
CONTEST
OF
YOUR
CHOICE
MACHINE
TRANSLATION
SPEECH
RECOGNITION
POSE
ESTIMATION
FACE
RECOGNITION
IMAGE
CATEGORIZATION
PIZZA
WINE
STOVE
IS
IT
DANGEROUS
HOW
FAST
DOES
IT
RUN
IS
IT
ALIVE
IS
IT
SOFT
SLIDE
CREDIT
DEREK
HOIEM
DOES
IT
HAVE
A
TAIL
CAN
I
POKE
WITH
IT
ATTRIBUTE
BASED
IMAGE
RETRIEVAL
DATING
CAR
PHOTOGRAPHS
INFERRING
VISUAL
PERSUASION
ANSWERING
QUESTIONS
ABOUT
IMAGES
WHAT
ELSE
WHAT
ARE
SOME
PROBLEMS
IN
YOUR
AREA
OF
RESEARCH
OR
FROM
YOUR
EVERYDAY
LIFE
THAT
CAN
BE
HELPED
BY
MACHINE
LEARNING
TENS
OF
THOUSANDS
OF
MACHINE
LEARNING
ALGORITHMS
DECADES
OF
ML
RESEARCH
OVERSIMPLIFIED
LEARN
A
MAPPING
FROM
INPUT
TO
OUTPUT
F
X
Y
X
EMAILS
Y
SPAM
NOTSPAM
SLIDE
CREDIT
PEDRO
DOMINGOS
Y
F
X
OUTPUT
PREDICTION
FUNCTION
FEATURES
TRAINING
GIVEN
A
TRAINING
SET
OF
LABELED
EXAMPLES
XN
YN
ESTIMATE
THE
PREDICTION
FUNCTION
F
BY
MINIMIZING
THE
PREDICTION
ERROR
ON
THE
TRAINING
SET
TESTING
APPLY
F
TO
A
NEVER
BEFORE
SEEN
TEST
EXAMPLE
X
AND
OUTPUT
THE
PREDICTED
VALUE
Y
F
X
APPLY
A
PREDICTION
FUNCTION
TO
A
FEATURE
REPRESENTATION
OF
THE
IMAGE
TO
GET
THE
DESIRED
OUTPUT
F
APPLE
F
TOMATO
F
COW
TRAINING
TESTING
TEST
IMAGE
SLIDE
CREDIT
D
HOIEM
AND
L
LAZEBNIK
TRAINING
VS
TESTING
WHAT
DO
WE
WANT
HIGH
ACCURACY
ON
TRAINING
DATA
NO
HIGH
ACCURACY
ON
UNSEEN
NEW
TEST
DATA
WHY
IS
THIS
TRICKY
TRAINING
DATA
FEATURES
X
AND
LABELS
Y
USED
TO
LEARN
MAPPING
F
TEST
DATA
FEATURES
USED
TO
MAKE
A
PREDICTION
LABELS
ONLY
USED
TO
SEE
HOW
WELL
WE
VE
LEARNED
F
VALIDATION
DATA
HELD
OUT
SET
OF
THE
TRAINING
DATA
CAN
USE
BOTH
FEATURES
AND
LABELS
TO
TUNE
PARAMETERS
OF
THE
MODEL
WE
RE
LEARNING
WHY
DO
WE
HOPE
THIS
WOULD
WORK
STATISTICAL
ESTIMATION
VIEW
X
AND
Y
ARE
RANDOM
VARIABLES
D
XN
YN
P
X
Y
BOTH
TRAINING
TESTING
DATA
SAMPLED
IID
FROM
P
X
Y
IID
INDEPENDENT
AND
IDENTICALLY
DISTRIBUTED
LEARN
ON
TRAINING
SET
HAVE
SOME
HOPE
OF
GENERALIZING
TO
TEST
SET
ML
IN
A
NUTSHELL
EVERY
MACHINE
LEARNING
ALGORITHM
HAS
DATA
REPRESENTATION
X
Y
PROBLEM
REPRESENTATION
EVALUATION
OBJECTIVE
FUNCTION
OPTIMIZATION
DATA
REPRESENTATION
LET
BRAINSTORM
WHAT
OUR
X
SHOULD
BE
FOR
VARIOUS
Y
PREDICTION
TASKS
PROBLEM
REPRESENTATION
DECISION
TREES
SETS
OF
RULES
LOGIC
PROGRAMS
INSTANCES
GRAPHICAL
MODELS
BAYES
MARKOV
NETS
NEURAL
NETWORKS
SUPPORT
VECTOR
MACHINES
MODEL
ENSEMBLES
ETC
EVALUATION
OBJECTIVE
FUNCTION
ACCURACY
PRECISION
AND
RECALL
SQUARED
ERROR
LIKELIHOOD
POSTERIOR
PROBABILITY
COST
UTILITY
MARGIN
ENTROPY
K
L
DIVERGENCE
ETC
OPTIMIZATION
DISCRETE
COMBINATORIAL
OPTIMIZATION
E
G
GRAPH
ALGORITHMS
CONTINUOUS
OPTIMIZATION
E
G
LINEAR
PROGRAMMING
TYPES
OF
LEARNING
SUPERVISED
LEARNING
TRAINING
DATA
INCLUDES
DESIRED
OUTPUTS
UNSUPERVISED
LEARNING
TRAINING
DATA
DOES
NOT
INCLUDE
DESIRED
OUTPUTS
WEAKLY
OR
SEMI
SUPERVISED
LEARNING
TRAINING
DATA
INCLUDES
A
FEW
DESIRED
OUTPUTS
REINFORCEMENT
LEARNING
REWARDS
FROM
SEQUENCE
OF
ACTIONS
SUPERVISED
LEARNING
X
Y
DISCRETE
X
Y
CONTINUOUS
UNSUPERVISED
LEARNING
X
X
DISCRETE
ID
X
X
CONTINUOUS
IMPROVE
ON
TASK
T
WITH
RESPECT
TO
PERFORMANCE
METRIC
P
BASED
ON
EXPERIENCE
E
T
CATEGORIZE
EMAIL
MESSAGES
AS
SPAM
OR
LEGITIMATE
P
PERCENTAGE
OF
EMAIL
MESSAGES
CORRECTLY
CLASSIFIED
E
DATABASE
OF
EMAILS
SOME
WITH
HUMAN
GIVEN
LABELS
T
RECOGNIZING
HAND
WRITTEN
WORDS
P
PERCENTAGE
OF
WORDS
CORRECTLY
CLASSIFIED
E
DATABASE
OF
HUMAN
LABELED
IMAGES
OF
HANDWRITTEN
WORDS
T
PLAYING
CHECKERS
P
PERCENTAGE
OF
GAMES
WON
AGAINST
AN
ARBITRARY
OPPONENT
E
PLAYING
PRACTICE
GAMES
AGAINST
ITSELF
T
DRIVING
ON
FOUR
LANE
HIGHWAYS
USING
VISION
SENSORS
P
AVERAGE
DISTANCE
TRAVELED
BEFORE
A
HUMAN
JUDGED
ERROR
E
A
SEQUENCE
OF
IMAGES
AND
STEERING
COMMANDS
RECORDED
WHILE
OBSERVING
A
HUMAN
DRIVER
SPAM
OR
NOT
VS
SPAM
EMAILS
A
LOT
OF
WORDS
LIKE
MONEY
FREE
BANK
ACCOUNT
REGULAR
EMAILS
WORD
USAGE
PATTERN
IS
MORE
SPREAD
OUT
SIMPLE
STRATEGY
LET
COUNT
THIS
IS
X
THIS
IS
Y
ОR
WEIGH
COUNTS
AND
SUM
TO
GET
PREDICTION
WHY
THESE
WORDS
KLINGON
VS
MLINGON
CLASSIFICATION
TRAINING
DATA
KLINGON
KLIX
KOUR
KOOP
MLINGON
MOO
MAA
MOU
TESTING
DATA
KAP
WHICH
LANGUAGE
WHY
BOARD
WHY
NOT
JUST
HAND
CODE
THESE
WEIGHTS
WE
RE
LETTING
THE
DATA
DO
THE
WORK
RATHER
THAN
DEVELOP
HAND
CODE
CLASSIFICATION
RULES
THE
MACHINE
IS
LEARNING
TO
PROGRAM
ITSELF
BUT
THERE
ARE
CHALLENGES
I
SAW
HER
DUCK
WITH
A
TELESCOPE
WHAT
HUMANS
SEE
WHAT
COMPUTERS
SEE
SOME
CHALLENGES
AMBIGUITY
AND
CONTEXT
MACHINES
TAKE
DATA
REPRESENTATIONS
TOO
LITERALLY
HUMANS
ARE
MUCH
BETTER
THAN
MACHINES
AT
GENERALIZATION
WHICH
IS
NEEDED
SINCE
TEST
DATA
WILL
RARELY
LOOK
EXACTLY
LIKE
THE
TRAINING
DATA
WHY
MIGHT
IT
BE
HARD
TO
PREDICT
IF
A
VIEWER
WILL
LIKE
A
MOVIE
RECOGNIZE
CARS
IN
IMAGES
TRANSLATE
BETWEEN
LANGUAGES
MANY
BASIC
EFFECTIVE
AND
EFFICIENT
ALGORITHMS
AVAILABLE
LARGE
AMOUNTS
OF
ON
LINE
DATA
AVAILABLE
LARGE
AMOUNTS
OF
COMPUTATIONAL
RESOURCES
AVAILABLE
IF
Y
IS
DISCRETE
ACCURACY
CORRECTLY
CLASSIFIED
ALL
TEST
EXAMPLES
TRUE
POSITIVE
FALSE
POSITIVE
TRUE
NEGATIVE
FALSE
NEGATIVE
WEIGHTED
MISCLASSIFICATION
VIA
A
CONFUSION
MATRIX
IF
Y
IS
DISCRETE
PRECISION
RECALL
PRECISION
TP
TP
FP
PREDICTED
TRUE
POS
PREDICTED
POS
RECALL
TP
TP
FN
PREDICTED
TRUE
POS
TRUE
POS
F
MEASURE
P
R
WANT
EVALUATION
METRIC
TO
BE
IN
SOME
RANGE
E
G
WORST
POSSIBLE
CLASSIFIER
BEST
POSSIBLE
CLASSIFIER
TRUE
POSITIVES
IMAGES
THAT
CONTAIN
PEOPLE
TRUE
NEGATIVES
IMAGES
THAT
DO
NOT
CONTAIN
PEOPLE
PREDICTED
POSITIVES
IMAGES
PREDICTED
TO
CONTAIN
PEOPLE
PREDICTED
NEGATIVES
IMAGES
PREDICTED
NOT
TO
CONTAIN
PEOPLE
PRECISION
RECALL
F
MEASURE
ACCURACY
IF
Y
IS
CONTINUOUS
SUM
OF
SQUARED
DIFFERENCES
SSD
ERROR
BETWEEN
PREDICTED
AND
TRUE
Y
E
N
I
F
XI
YI
FILL
OUT
DOODLE
READ
ENTIRE
COURSE
WEBSITE
DO
FIRST
READING
LINEAR
ALGEBRA
REVIEW
MATLAB
TUTORIAL
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
JANUARY
ANNOUNCEMENT
TA
WON
T
BE
BACK
UNTIL
THE
LAST
WEEK
OF
JANUARY
SKYPE
OFFICE
HOURS
TUESDAY
WEDNESDAY
TO
USERNAME
PLEASE
DO
DOODLE
LINEAR
ALGEBRA
PRIMER
PROFESSOR
FEI
FEI
LI
STANFORD
VECTORS
AND
MATRICES
VECTORS
AND
MATRICES
ARE
JUST
COLLECTIONS
OF
ORDERED
NUMBERS
THAT
REPRESENT
SOMETHING
MOVEMENTS
IN
SPACE
SCALING
FACTORS
WORD
COUNTS
MOVIE
RATINGS
PIXEL
BRIGHTNESSES
ETC
WE
LL
DEFINE
SOME
COMMON
USES
AND
STANDARD
OPERATIONS
ON
THEM
A
COLUMN
VECTOR
WHERE
A
ROW
VECTOR
WHERE
DENOTES
THE
TRANSPOSE
OPERATION
YOU
LL
WANT
TO
KEEP
TRACK
OF
THE
ORIENTATION
OF
YOUR
VECTORS
WHEN
PROGRAMMING
IN
MATLAB
YOU
CAN
TRANSPOSE
A
VECTOR
V
IN
MATLAB
BY
WRITING
V
VECTORS
CAN
REPRESENT
AN
OFFSET
IN
OR
SPACE
POINTS
ARE
JUST
VECTORS
FROM
THE
ORIGIN
DATA
CAN
ALSO
BE
TREATED
AS
A
VECTOR
SUCH
VECTORS
DON
T
HAVE
A
GEOMETRIC
INTERPRETATION
BUT
CALCULATIONS
LIKE
DISTANCE
STILL
HAVE
VALUE
A
MATRIX
IS
AN
ARRAY
OF
NUMBERS
WITH
SIZE
𝑚
BY
𝑛
I
E
M
ROWS
AND
N
COLUMNS
IF
WE
SAY
THAT
IS
SQUARE
ADDITION
CAN
ONLY
ADD
A
MATRIX
WITH
MATCHING
DIMENSIONS
OR
A
SCALAR
SCALING
INNER
PRODUCT
DOT
PRODUCT
OF
VECTORS
MULTIPLY
CORRESPONDING
ENTRIES
OF
TWO
VECTORS
AND
ADD
UP
THE
RESULT
X
Y
IS
ALSO
X
Y
COS
THE
ANGLE
BETWEEN
X
AND
Y
INNER
PRODUCT
DOT
PRODUCT
OF
VECTORS
IF
B
IS
A
UNIT
VECTOR
THEN
A
B
GIVES
THE
LENGTH
OF
A
WHICH
LIES
IN
THE
DIRECTION
OF
B
PROJECTION
IF
B
IS
UNIT
LENGTH
HENCE
NORM
IS
NORM
NORM
LP
NORM
FOR
REAL
NUMBERS
P
MULTIPLICATION
THE
PRODUCT
AB
IS
EACH
ENTRY
IN
THE
RESULT
IS
THAT
ROW
OF
A
DOT
PRODUCT
WITH
THAT
COLUMN
OF
B
MULTIPLICATION
EXAMPLE
EACH
ENTRY
OF
THE
MATRIX
PRODUCT
IS
MADE
BY
TAKING
THE
DOT
PRODUCT
OF
THE
CORRESPONDING
ROW
IN
THE
LEFT
MATRIX
WITH
THE
CORRESPONDING
COLUMN
IN
THE
RIGHT
ONE
TRANSPOSE
FLIP
MATRIX
SO
ROW
BECOMES
COLUMN
A
USEFUL
IDENTITY
IDENTITY
MATRIX
I
SQUARE
MATRIX
ALONG
DIAGONAL
ELSEWHERE
I
ANOTHER
MATRIX
THAT
MATRIX
DIAGONAL
MATRIX
SQUARE
MATRIX
WITH
NUMBERS
ALONG
DIAGONAL
ELSEWHERE
A
DIAGONAL
ANOTHER
MATRIX
SCALES
THE
ROWS
OF
THAT
MATRIX
SYMMETRIC
MATRIX
GIVEN
A
MATRIX
A
ITS
INVERSE
A
IS
A
MATRIX
SUCH
THAT
AA
A
I
E
G
INVERSE
DOES
NOT
ALWAYS
EXIST
IF
A
EXISTS
A
IS
INVERTIBLE
OR
NON
SINGULAR
OTHERWISE
IT
SINGULAR
MATLAB
EXAMPLE
SAY
YOU
HAVE
THE
MATRIX
EQUATION
AX
B
WHERE
A
AND
B
ARE
KNOWN
AND
YOU
WANT
TO
SOLVE
FOR
X
YOU
COULD
USE
MATLAB
TO
CALCULATE
THE
INVERSE
AND
PREMULTIPLY
BY
IT
A
A
X
A
MATLAB
COMMAND
WOULD
BE
INV
A
B
BUT
CALCULATING
THE
INVERSE
FOR
LARGE
MATRICES
OFTEN
BRINGS
PROBLEMS
WITH
COMPUTER
FLOATING
POINT
RESOLUTION
OR
YOUR
MATRIX
MIGHT
NOT
EVEN
HAVE
AN
INVERSE
FORTUNATELY
THERE
ARE
WORKAROUNDS
INSTEAD
OF
TAKING
AN
INVERSE
DIRECTLY
ASK
MATLAB
TO
SOLVE
FOR
X
IN
AX
B
BY
TYPING
A
B
MATLAB
WILL
TRY
SEVERAL
APPROPRIATE
NUMERICAL
METHODS
INCLUDING
THE
PSEUDOINVERSE
IF
THE
INVERSE
DOESN
T
EXIST
MATLAB
WILL
RETURN
THE
VALUE
OF
X
WHICH
SOLVES
THE
EQUATION
IF
THERE
IS
NO
EXACT
SOLUTION
IT
WILL
RETURN
THE
CLOSEST
ONE
IF
THERE
ARE
MANY
SOLUTIONS
IT
WILL
RETURN
THE
SMALLEST
ONE
MATRIX
ADDITION
IS
COMMUTATIVE
AND
ASSOCIATIVE
A
B
B
A
A
B
C
A
B
C
MATRIX
MULTIPLICATION
IS
ASSOCIATIVE
AND
DISTRIBUTIVE
BUT
NOT
COMMUTATIVE
A
B
C
A
B
C
A
B
C
A
B
A
C
A
B
B
A
SUPPOSE
WE
HAVE
A
SET
OF
VECTORS
VN
IF
WE
CAN
EXPRESS
AS
A
LINEAR
COMBINATION
OF
THE
OTHER
VECTORS
VN
THEN
IS
LINEARLY
DEPENDENT
ON
THE
OTHER
VECTORS
THE
DIRECTION
CAN
BE
EXPRESSED
AS
A
COMBINATION
OF
THE
DIRECTIONS
VN
E
G
IF
NO
VECTOR
IS
LINEARLY
DEPENDENT
ON
THE
REST
OF
THE
SET
THE
SET
IS
LINEARLY
INDEPENDENT
COMMON
CASE
A
SET
OF
VECTORS
VN
IS
ALWAYS
LINEARLY
INDEPENDENT
IF
EACH
VECTOR
IS
PERPENDICULAR
TO
EVERY
OTHER
VECTOR
AND
NON
ZERO
LINEARLY
INDEPENDENT
SET
NOT
LINEARLY
INDEPENDENT
COLUMN
ROW
RANK
COLUMN
RANK
ALWAYS
EQUALS
ROW
RANK
MATRIX
RANK
IF
A
MATRIX
IS
NOT
FULL
RANK
INVERSE
DOESN
T
EXIST
INVERSE
ALSO
DOESN
T
EXIST
FOR
NON
SQUARE
MATRICES
THERE
ARE
SEVERAL
COMPUTER
ALGORITHMS
THAT
CAN
FACTOR
A
MATRIX
REPRESENTING
IT
AS
THE
PRODUCT
OF
SOME
OTHER
MATRICES
THE
MOST
USEFUL
OF
THESE
IS
THE
SINGULAR
VALUE
DECOMPOSITION
REPRESENTS
ANY
MATRIX
A
AS
A
PRODUCT
OF
THREE
MATRICES
UΣVT
MATLAB
COMMAND
U
V
SVD
A
UΣVT
A
WHERE
U
AND
V
ARE
ROTATION
MATRICES
AND
Σ
IS
A
SCALING
MATRIX
FOR
EXAMPLE
IN
GENERAL
IF
A
IS
M
X
N
THEN
U
WILL
BE
M
X
M
Σ
WILL
BE
M
X
N
AND
VT
WILL
BE
N
X
N
U
AND
V
ARE
ALWAYS
ROTATION
MATRICES
GEOMETRIC
ROTATION
MAY
NOT
BE
AN
APPLICABLE
CONCEPT
DEPENDING
ON
THE
MATRIX
SO
WE
CALL
THEM
UNITARY
MATRICES
EACH
COLUMN
IS
A
UNIT
VECTOR
Σ
IS
A
DIAGONAL
MATRIX
THE
NUMBER
OF
NONZERO
ENTRIES
RANK
OF
A
THE
ALGORITHM
ALWAYS
SORTS
THE
ENTRIES
HIGH
TO
LOW
M
UΣVT
ILLUSTRATION
FROM
WIKIPEDIA
WE
VE
DISCUSSED
SVD
IN
TERMS
OF
GEOMETRIC
TRANSFORMATION
MATRICES
BUT
SVD
OF
A
DATA
MATRIX
CAN
ALSO
BE
VERY
USEFUL
TO
UNDERSTAND
THIS
WE
LL
LOOK
AT
A
LESS
GEOMETRIC
INTERPRETATION
OF
WHAT
SVD
IS
DOING
SVD
APPLICATIONS
LOOK
AT
HOW
THE
MULTIPLICATION
WORKS
OUT
LEFT
TO
RIGHT
COLUMN
OF
U
GETS
SCALED
BY
THE
FIRST
VALUE
FROM
Σ
THE
RESULTING
VECTOR
GETS
SCALED
BY
ROW
OF
VT
TO
PRODUCE
A
CONTRIBUTION
TO
THE
COLUMNS
OF
A
SVD
APPLICATIONS
EACH
PRODUCT
OF
COLUMN
I
OF
U
VALUE
I
FROM
Σ
ROW
I
OF
VT
PRODUCES
A
COMPONENT
OF
THE
FINAL
A
SVD
APPLICATIONS
WE
RE
BUILDING
A
AS
A
LINEAR
COMBINATION
OF
THE
COLUMNS
OF
U
USING
ALL
COLUMNS
OF
U
WE
LL
REBUILD
THE
ORIGINAL
MATRIX
PERFECTLY
BUT
IN
REAL
WORLD
DATA
OFTEN
WE
CAN
JUST
USE
THE
FIRST
FEW
COLUMNS
OF
U
AND
WE
LL
GET
SOMETHING
CLOSE
E
G
THE
FIRST
APARTIAL
ABOVE
SVD
APPLICATIONS
WE
CAN
CALL
THOSE
FIRST
FEW
COLUMNS
OF
U
THE
PRINCIPAL
COMPONENTS
OF
THE
DATA
THEY
SHOW
THE
MAJOR
PATTERNS
THAT
CAN
BE
ADDED
TO
PRODUCE
THE
COLUMNS
OF
THE
ORIGINAL
MATRIX
THE
ROWS
OF
VT
SHOW
HOW
THE
PRINCIPAL
COMPONENTS
ARE
MIXED
TO
PRODUCE
THE
COLUMNS
OF
THE
MATRIX
SVD
APPLICATIONS
WE
CAN
LOOK
AT
Σ
TO
SEE
THAT
THE
FIRST
COLUMN
HAS
A
LARGE
EFFECT
WHILE
THE
SECOND
COLUMN
HAS
A
MUCH
SMALLER
EFFECT
IN
THIS
EXAMPLE
PRINCIPAL
COMPONENT
ANALYSIS
REMEMBER
COLUMNS
OF
U
ARE
THE
PRINCIPAL
COMPONENTS
OF
THE
DATA
THE
MAJOR
PATTERNS
THAT
CAN
BE
ADDED
TO
PRODUCE
THE
COLUMNS
OF
THE
ORIGINAL
MATRIX
ONE
USE
OF
THIS
IS
TO
CONSTRUCT
A
MATRIX
WHERE
EACH
COLUMN
IS
A
SEPARATE
DATA
SAMPLE
RUN
SVD
ON
THAT
MATRIX
AND
LOOK
AT
THE
FIRST
FEW
COLUMNS
OF
U
TO
SEE
PATTERNS
THAT
ARE
COMMON
AMONG
THE
COLUMNS
THIS
IS
CALLED
PRINCIPAL
COMPONENT
ANALYSIS
OR
PCA
OF
THE
DATA
SAMPLES
PRINCIPAL
COMPONENT
ANALYSIS
OFTEN
RAW
DATA
SAMPLES
HAVE
A
LOT
OF
REDUNDANCY
AND
PATTERNS
PCA
CAN
ALLOW
YOU
TO
REPRESENT
DATA
SAMPLES
AS
WEIGHTS
ON
THE
PRINCIPAL
COMPONENTS
RATHER
THAN
USING
THE
ORIGINAL
RAW
FORM
OF
THE
DATA
BY
REPRESENTING
EACH
SAMPLE
AS
JUST
THOSE
WEIGHTS
YOU
CAN
REPRESENT
JUST
THE
MEAT
OF
WHAT
DIFFERENT
BETWEEN
SAMPLES
THIS
MINIMAL
REPRESENTATION
MAKES
MACHINE
LEARNING
AND
OTHER
ALGORITHMS
MUCH
MORE
EFFICIENT
EXAMPLE
EIGENFACES
IMAGES
OF
FACES
REPRESENT
EACH
AS
THE
CONCATENATION
OF
ITS
PIXELS
COLUMN
VECTORS
STACK
TOGETHER
HORIZONTALLY
TO
GET
MATRIX
A
U
V
SVD
A
FIRST
FOUR
COLUMNS
OF
U
CAN
REPRESENT
EACH
FACE
AS
A
LINEAR
COMBINATION
OF
THE
FIRST
FEW
COLUMNS
OF
U
IMAGE
FROM
ALEXANDER
IHLER
ADDENDUM
HOW
IS
SVD
COMPUTED
FOR
THIS
CLASS
TELL
MATLAB
TO
DO
IT
USE
THE
RESULT
BUT
IF
YOU
RE
INTERESTED
ONE
COMPUTER
ALGORITHM
TO
DO
IT
MAKES
USE
OF
EIGENVECTORS
THE
FOLLOWING
MATERIAL
IS
PRESENTED
TO
MAKE
SVD
LESS
OF
A
MAGICAL
BLACK
BOX
BUT
YOU
WILL
DO
FINE
IN
THIS
CLASS
IF
YOU
TREAT
SVD
AS
A
MAGICAL
BLACK
BOX
AS
LONG
AS
YOU
REMEMBER
ITS
PROPERTIES
FROM
THE
PREVIOUS
SLIDES
SUPPOSE
WE
HAVE
A
SQUARE
MATRIX
A
WE
CAN
SOLVE
FOR
VECTOR
X
AND
SCALAR
Λ
SUCH
THAT
AX
ΛX
IN
OTHER
WORDS
FIND
VECTORS
WHERE
IF
WE
TRANSFORM
THEM
WITH
A
THE
ONLY
EFFECT
IS
TO
SCALE
THEM
WITH
NO
CHANGE
IN
DIRECTION
THESE
VECTORS
ARE
CALLED
EIGENVECTORS
GERMAN
FOR
SELF
VECTOR
OF
THE
MATRIX
AND
THE
SCALING
FACTORS
Λ
ARE
CALLED
EIGENVALUES
AN
M
X
M
MATRIX
WILL
HAVE
M
EIGENVECTORS
WHERE
Λ
IS
NONZERO
COMPUTERS
CAN
FIND
AN
X
SUCH
THAT
AX
ΛX
USING
THIS
ITERATIVE
ALGORITHM
X
RANDOM
UNIT
VECTOR
WHILE
X
HASN
T
CONVERGED
X
AX
NORMALIZE
X
X
WILL
QUICKLY
CONVERGE
TO
AN
EIGENVECTOR
SOME
SIMPLE
MODIFICATIONS
WILL
LET
THIS
ALGORITHM
FIND
ALL
EIGENVECTORS
EIGENVECTORS
ARE
FOR
SQUARE
MATRICES
BUT
SVD
IS
FOR
ALL
MATRICES
TO
DO
SVD
A
COMPUTERS
CAN
DO
THIS
TAKE
EIGENVECTORS
OF
AAT
MATRIX
IS
ALWAYS
SQUARE
THESE
EIGENVECTORS
ARE
THE
COLUMNS
OF
U
SQUARE
ROOT
OF
EIGENVALUES
ARE
THE
SINGULAR
VALUES
THE
ENTRIES
OF
Σ
TAKE
EIGENVECTORS
OF
ATA
MATRIX
IS
ALWAYS
SQUARE
THESE
EIGENVECTORS
ARE
COLUMNS
OF
V
OR
ROWS
OF
VT
MORAL
OF
THE
STORY
SVD
IS
FAST
EVEN
FOR
LARGE
MATRICES
IT
USEFUL
FOR
A
LOT
OF
STUFF
THERE
ARE
ALSO
OTHER
ALGORITHMS
TO
COMPUTE
SVD
OR
PART
OF
THE
SVD
MATLAB
SVD
COMMAND
HAS
OPTIONS
TO
EFFICIENTLY
COMPUTE
ONLY
WHAT
YOU
NEED
IF
PERFORMANCE
BECOMES
AN
ISSUE
MATLAB
TUTORIAL
PLEASE
COVER
WHATEVER
WE
DON
T
FINISH
AT
HOME
TUTORIALS
AND
EXERCISES
DO
PROBLEMS
MOST
ALSO
HAVE
SOLUTIONS
ASK
THE
TA
IF
YOU
HAVE
ANY
PROBLEMS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
JANUARY
GROUPING
ITEMS
THAT
BELONG
TOGETHER
I
E
HAVE
SIMILAR
FEATURES
UNSUPERVISED
WE
ONLY
USE
THE
FEATURES
X
NOT
THE
LABELS
Y
THIS
IS
USEFUL
BECAUSE
WE
MAY
NOT
HAVE
ANY
LABELS
BUT
WE
CAN
STILL
DETECT
PATTERNS
SUMMARIZING
DATA
LOOK
AT
LARGE
AMOUNTS
OF
DATA
REPRESENT
A
LARGE
CONTINUOUS
VECTOR
WITH
THE
CLUSTER
NUMBER
COUNTING
COMPUTING
FEATURE
HISTOGRAMS
PREDICTION
DATA
POINTS
IN
THE
SAME
CLUSTER
MAY
HAVE
THE
SAME
LABELS
SLIDE
CREDIT
J
HAYS
D
HOIEM
COUNTING
AND
CLASSIFICATION
VIA
CLUSTERING
COMPUTE
A
HISTOGRAM
TO
SUMMARIZE
THE
DATA
AFTER
CLUSTERING
ASK
A
HUMAN
TO
LABEL
EACH
GROUP
CLUSTER
CAT
PANDA
GROUP
ID
GIRAFFE
IMAGE
SEGMENTATION
VIA
CLUSTERING
SEPARATE
IMAGE
INTO
COHERENT
OBJECTS
IMAGE
HUMAN
SEGMENTATION
UNSUPERVISED
DISCOVERY
WE
DON
T
KNOW
WHAT
THE
OBJECTS
IN
RED
BOXES
ARE
BUT
WE
KNOW
THEY
TEND
TO
OCCUR
IN
SIMILAR
CONTEXT
IF
FEATURES
THE
CONTEXT
OBJECTS
IN
RED
WILL
CLUSTER
TOGETHER
THEN
ASK
HUMAN
FOR
A
LABEL
ON
ONE
EXAMPLE
FROM
THE
CLUSTER
AND
KEEP
LEARNING
NEW
OBJECT
CATEGORIES
ITERATIVELY
TODAY
AND
NEXT
CLASS
CLUSTERING
MOTIVATION
AND
APPLICATIONS
ALGORITHMS
MEAN
SHIFT
FIND
MODES
IN
THE
DATA
HIERARCHICAL
CLUSTERING
START
WITH
ALL
POINTS
IN
SEPARATE
CLUSTERS
AND
MERGE
NORMALIZED
CUTS
SPLIT
NODES
IN
A
GRAPH
BASED
ON
SIMILARITY
IMAGE
SEGMENTATION
TOY
EXAMPLE
BLACK
PIXELS
GRAY
PIXELS
WHITE
PIXELS
INPUT
IMAGE
INTENSITY
THESE
INTENSITIES
DEFINE
THE
THREE
GROUPS
WE
COULD
LABEL
EVERY
PIXEL
IN
THE
IMAGE
ACCORDING
TO
WHICH
OF
THESE
PRIMARY
INTENSITIES
IT
IS
I
E
SEGMENT
THE
IMAGE
BASED
ON
THE
INTENSITY
FEATURE
WHAT
IF
THE
IMAGE
ISN
T
QUITE
SO
SIMPLE
INPUT
IMAGE
INTENSITY
INTENSITY
GOAL
CHOOSE
THREE
CENTERS
AS
THE
REPRESENTATIVE
INTENSITIES
AND
LABEL
EVERY
PIXEL
ACCORDING
TO
WHICH
OF
THESE
CENTERS
IT
IS
NEAREST
TO
BEST
CLUSTER
CENTERS
ARE
THOSE
THAT
MINIMIZE
SSD
BETWEEN
ALL
POINTS
AND
THEIR
NEAREST
CLUSTER
CENTER
CI
CLUSTERING
WITH
THIS
OBJECTIVE
IT
IS
A
CHICKEN
AND
EGG
PROBLEM
IF
WE
KNEW
THE
CLUSTER
CENTERS
WE
COULD
ALLOCATE
POINTS
TO
GROUPS
BY
ASSIGNING
EACH
TO
ITS
CLOSEST
CENTER
IF
WE
KNEW
THE
GROUP
MEMBERSHIPS
WE
COULD
GET
THE
CENTERS
BY
COMPUTING
THE
MEAN
PER
GROUP
K
MEANS
CLUSTERING
BASIC
IDEA
RANDOMLY
INITIALIZE
THE
K
CLUSTER
CENTERS
AND
ITERATE
BETWEEN
THE
TWO
STEPS
WE
JUST
SAW
RANDOMLY
INITIALIZE
THE
CLUSTER
CENTERS
CK
GIVEN
CLUSTER
CENTERS
DETERMINE
POINTS
IN
EACH
CLUSTER
FOR
EACH
POINT
P
FIND
THE
CLOSEST
CI
PUT
P
INTO
CLUSTER
I
GIVEN
POINTS
IN
EACH
CLUSTER
SOLVE
FOR
CI
SET
CI
TO
BE
THE
MEAN
OF
POINTS
IN
CLUSTER
I
IF
CI
HAVE
CHANGED
REPEAT
STEP
PROPERTIES
WILL
ALWAYS
CONVERGE
TO
SOME
SOLUTION
CAN
BE
A
LOCAL
MINIMUM
DOES
NOT
ALWAYS
FIND
THE
GLOBAL
MINIMUM
OF
OBJECTIVE
FUNCTION
SOURCE
STEVE
SEITZ
K
MEANS
L
ASK
USER
HOW
MANY
CLLUSTERS
THEY
D
NKE
E
G
K
K
MEANS
L
ASK
USER
HOW
CLUSTERS
THEY
D
LLIIKE
E
G
K
GUESS
K
CLLUSTER
CENTER
LOCATIONS
K
MEANS
ASK
USER
HOW
MANY
CLUSTERS
THEY
D
LIKE
E
G
K
GUESS
K
CLUSTER
CE
NT
E
R
LOCAT
IONS
EACH
DLATAPOINT
FINDS
OUT
WH
ICH
CENTER
IT
CLOSEST
TO
THUS
EACH
CENTE
R
A
SET
OF
DATAPO
INTS
L
K
MEANS
ASK
USER
HOW
MANY
CLUSTERS
THEY
D
LIKE
E
G
K
GUESS
K
CLUSTER
CENTER
LOCATIONS
EACH
DATAPOINT
FINDS
OUT
WHICH
CENT
ER
IT
CLOSEST
TO
EACH
CENTER
FINDS
THE
CENTROID
OF
THE
POINTS
IT
OWNS
O
L
K
MEANS
CONVERGES
TO
A
LOCAL
MINIMUM
K
MEANS
CLUSTERING
JAVA
DEMO
MATLAB
DEMO
TIME
COMPLEXITY
LET
N
NUMBER
OF
INSTANCES
D
DIMENSIONALITY
OF
THE
FEATURES
K
NUMBER
OF
CLUSTERS
ASSUME
COMPUTING
DISTANCE
BETWEEN
TWO
INSTANCES
IS
O
D
REASSIGNING
CLUSTERS
O
KN
DISTANCE
COMPUTATIONS
OR
O
KND
COMPUTING
CENTROIDS
EACH
INSTANCE
VECTOR
GETS
ADDED
ONCE
TO
A
CENTROID
O
ND
ASSUME
THESE
TWO
STEPS
ARE
EACH
DONE
ONCE
FOR
A
FIXED
NUMBER
OF
ITERATIONS
I
O
IKND
LINEAR
IN
ALL
RELEVANT
FACTORS
ADAPTED
FROM
RAY
MOONEY
ANOTHER
WAY
OF
WRITING
OBJECTIVE
K
MEANS
LET
RNK
IF
INSTANCE
N
BELONGS
TO
CLUSTER
K
OTHERWISE
K
MEDOIDS
MORE
GENERAL
DISTANCES
DISTANCE
METRICS
EUCLIDIAN
DISTANMCE
NORM
NORM
X
Y
XI
I
M
YI
X
Y
I
XI
YI
COSINE
SIMILARITY
TRANSFORM
TO
A
DISTANCE
BY
SUBTRACTING
FROM
X
Y
X
Y
SEGMENTATION
AS
CLUSTERING
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
INTENSITY
SIMILARITY
FEATURE
SPACE
INTENSITY
VALUE
D
K
K
QUANTIZATION
OF
THE
FEATURE
SPACE
SEGMENTATION
LABEL
MAP
SEGMENTATION
AS
CLUSTERING
DEPENDING
ON
WHAT
WE
CHOOSE
AS
THE
FEATURE
SPACE
WE
CAN
GROUP
PIXELS
IN
DIFFERENT
WAYS
GROUPING
PIXELS
BASED
ON
COLOR
SIMILARITY
R
G
B
R
G
B
R
G
R
B
R
G
B
FEATURE
SPACE
COLOR
VALUE
D
SOURCE
K
GRAUMAN
K
MEANS
PROS
AND
CONS
PROS
SIMPLE
FAST
TO
COMPUTE
CONVERGES
TO
LOCAL
MINIMUM
OF
WITHIN
CLUSTER
SQUARED
ERROR
CONS
ISSUES
SETTING
K
ONE
WAY
SILHOUETTE
COEFFICIENT
SENSITIVE
TO
INITIAL
CENTERS
USE
HEURISTICS
OR
OUTPUT
OF
ANOTHER
METHOD
SENSITIVE
TO
OUTLIERS
DETECTS
SPHERICAL
CLUSTERS
ADAPTED
FROM
K
GRAUMAN
TODAY
CLUSTERING
MOTIVATION
AND
APPLICATIONS
ALGORITHMS
K
MEANS
ITERATE
BETWEEN
FINDING
CENTERS
AND
ASSIGNING
POINTS
HIERARCHICAL
CLUSTERING
START
WITH
ALL
POINTS
IN
SEPARATE
CLUSTERS
AND
MERGE
NORMALIZED
CUTS
SPLIT
NODES
IN
A
GRAPH
BASED
ON
SIMILARITY
THE
MEAN
SHIFT
ALGORITHM
SEEKS
MODES
OR
LOCAL
MAXIMA
OF
DENSITY
IN
THE
FEATURE
SPACE
IMAGE
FEATURE
SPACE
L
U
V
COLOR
VALUES
KERNEL
ESTIMATED
DENSITY
DATA
D
SEARCH
WINDOW
CENTER
OF
MASS
MEAN
SHIFT
VECTOR
SOURCE
D
HOIEM
CLUSTER
ALL
DATA
POINTS
IN
THE
ATTRACTION
BASIN
OF
A
MODE
ATTRACTION
BASIN
THE
REGION
FOR
WHICH
ALL
TRAJECTORIES
LEAD
TO
THE
SAME
MODE
SLIDE
BY
Y
UKRAINITZ
B
SAREL
SIMPLE
MEAN
SHIFT
PROCEDURE
COMPUTE
MEAN
SHIFT
VECTOR
TRANSLATE
THE
KERNEL
WINDOW
BY
M
X
ADAPTED
FROM
Y
UKRAINITZ
B
SAREL
COMPUTE
FEATURES
FOR
EACH
POINT
INTENSITY
WORD
COUNTS
ETC
INITIALIZE
WINDOWS
AT
INDIVIDUAL
FEATURE
POINTS
PERFORM
MEAN
SHIFT
FOR
EACH
WINDOW
UNTIL
CONVERGENCE
MERGE
WINDOWS
THAT
END
UP
NEAR
THE
SAME
PEAK
OR
MODE
SOURCE
D
HOIEM
PROS
MEAN
SHIFT
DOES
NOT
ASSUME
SHAPE
ON
CLUSTERS
ROBUST
TO
OUTLIERS
CONS
ISSUES
NEED
TO
CHOOSE
WINDOW
SIZE
EXPENSIVE
O
I
D
SEARCH
FOR
NEIGHBORS
COULD
BE
SPED
UP
IN
LOWER
DIMENSIONS
DOES
NOT
SCALE
WELL
WITH
DIMENSION
OF
FEATURE
SPACE
MEAN
SHIFT
READING
NICELY
WRITTEN
MEAN
SHIFT
EXPLANATION
WITH
MATH
INCLUDES
M
CODE
FOR
MEAN
SHIFT
CLUSTERING
MEAN
SHIFT
PAPER
BY
COMANICIU
AND
MEER
ADAPTIVE
MEAN
SHIFT
IN
HIGHER
DIMENSIONS
SOURCE
K
GRAUMAN
TODAY
CLUSTERING
MOTIVATION
AND
APPLICATIONS
ALGORITHMS
K
MEANS
ITERATE
BETWEEN
FINDING
CENTERS
AND
ASSIGNING
POINTS
MEAN
SHIFT
FIND
MODES
IN
THE
DATA
NORMALIZED
CUTS
SPLIT
NODES
IN
A
GRAPH
BASED
ON
SIMILARITY
ASSUMES
A
SIMILARITY
FUNCTION
FOR
DETERMINING
THE
SIMILARITY
OF
TWO
INSTANCES
STARTS
WITH
ALL
INSTANCES
IN
SEPARATE
CLUSTERS
AND
THEN
REPEATEDLY
JOINS
THE
TWO
CLUSTERS
THAT
ARE
MOST
SIMILAR
UNTIL
THERE
IS
ONLY
ONE
CLUSTER
THE
HISTORY
OF
MERGING
FORMS
A
BINARY
TREE
OR
HIERARCHY
START
WITH
ALL
INSTANCES
IN
THEIR
OWN
CLUSTER
UNTIL
THERE
IS
ONLY
ONE
CLUSTER
AMONG
THE
CURRENT
CLUSTERS
DETERMINE
THE
TWO
CLUSTERS
CI
AND
CJ
THAT
ARE
MOST
SIMILAR
REPLACE
CI
AND
CJ
WITH
A
SINGLE
CLUSTER
CI
CJ
HOW
MANY
CLUSTERS
CLUSTERING
CREATES
A
DENDROGRAM
A
TREE
TO
GET
FINAL
CLUSTERS
PICK
A
THRESHOLD
MAX
NUMBER
OF
CLUSTERS
OR
MAX
DISTANCE
WITHIN
CLUSTERS
Y
AXIS
HOW
TO
COMPUTE
SIMILARITY
OF
TWO
CLUSTERS
EACH
POSSIBLY
CONTAINING
MULTIPLE
INSTANCES
SINGLE
LINK
SIMILARITY
OF
TWO
MOST
SIMILAR
MEMBERS
SIM
CI
C
J
MAX
X
CI
Y
C
J
SIM
X
Y
COMPLETE
LINK
SIMILARITY
OF
TWO
LEAST
SIMILAR
MEMBERS
SIM
CI
C
J
MIN
X
CI
Y
C
J
SIM
X
Y
GROUP
AVERAGE
AVERAGE
SIMILARITY
BETWEEN
MEMBERS
TODAY
CLUSTERING
MOTIVATION
AND
APPLICATIONS
ALGORITHMS
K
MEANS
ITERATE
BETWEEN
FINDING
CENTERS
AND
ASSIGNING
POINTS
MEAN
SHIFT
FIND
MODES
IN
THE
DATA
HIERARCHICAL
CLUSTERING
START
WITH
ALL
POINTS
IN
SEPARATE
CLUSTERS
AND
MERGE
FULLY
CONNECTED
GRAPH
NODE
VERTEX
FOR
EVERY
PIXEL
LINK
BETWEEN
EVERY
PAIR
OF
PIXELS
P
Q
AFFINITY
WEIGHT
WPQ
FOR
EACH
LINK
EDGE
WPQ
MEASURES
SIMILARITY
SIMILARITY
IS
INVERSELY
PROPORTIONAL
TO
DIFFERENCE
IN
COLOR
AND
POSITION
A
B
C
BREAK
GRAPH
INTO
SEGMENTS
WANT
TO
DELETE
LINKS
THAT
CROSS
BETWEEN
SEGMENTS
EASIEST
TO
BREAK
LINKS
THAT
HAVE
LOW
SIMILARITY
LOW
WEIGHT
SIMILAR
PIXELS
SHOULD
BE
IN
THE
SAME
SEGMENTS
DISSIMILAR
PIXELS
SHOULD
BE
IN
DIFFERENT
SEGMENTS
B
LINK
CUT
SET
OF
LINKS
WHOSE
REMOVAL
MAKES
A
GRAPH
DISCONNECTED
COST
OF
A
CUT
FIND
MINIMUM
CUT
CUT
A
B
W
P
A
Q
B
P
Q
GIVES
YOU
A
SEGMENTATION
FAST
ALGORITHMS
EXIST
FOR
DOING
THIS
MINIMUM
CUT
PROBLEM
WITH
MINIMUM
CUT
WEIGHT
OF
CUT
PROPORTIONAL
TO
NUMBER
OF
EDGES
IN
THE
CUT
TENDS
TO
PRODUCE
SMALL
ISOLATED
COMPONENTS
SHI
MALIK
PAMI
CUTS
IN
A
GRAPH
NORMALIZED
CUT
B
NORMALIZE
FOR
SIZE
OF
SEGMENTS
CUT
A
B
WP
Q
P
A
Q
B
CUT
A
B
ASSOC
A
V
CUT
A
B
ASSOC
B
V
ASSOC
A
V
SUM
OF
WEIGHTS
OF
ALL
EDGES
THAT
TOUCH
A
NCUT
VALUE
SMALL
WHEN
WE
GET
TWO
CLUSTERS
WITH
MANY
EDGES
WITH
HIGH
WEIGHTS
WITHIN
THEM
AND
FEW
EDGES
OF
LOW
WEIGHT
BETWEEN
THEM
SHI
AND
J
MALIK
CVPR
ADAPTED
FROM
STEVE
SEITZ
MIGHT
DEPEND
ON
APPLICATION
PURITY
WHERE
IS
THE
SET
OF
CLUSTERS
AND
IS
THE
SET
OF
CLASSES
SEE
MURPHY
SEC
FOR
ANOTHER
TWO
METRICS
RAND
INDEX
AND
MUTUAL
INFORMATION
CLUSTERING
STRATEGIES
K
MEANS
ITERATIVELY
RE
ASSIGN
POINTS
TO
THE
NEAREST
CLUSTER
CENTER
MEAN
SHIFT
CLUSTERING
ESTIMATE
MODES
GRAPH
CUTS
SPLIT
THE
NODES
IN
A
GRAPH
BASED
ON
ASSIGNED
LINKS
WITH
SIMILARITY
WEIGHTS
AGGLOMERATIVE
CLUSTERING
START
WITH
EACH
POINT
AS
ITS
OWN
CLUSTER
AND
ITERATIVELY
MERGE
THE
CLOSEST
CLUSTERS
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
JANUARY
PLAN
FOR
TODAY
DIMENSIONALITY
REDUCTION
MOTIVATION
PRINCIPAL
COMPONENT
ANALYSIS
PCA
APPLICATIONS
OF
PCA
OTHER
METHODS
FOR
DIMENSIONALITY
REDUCTION
WHY
REDUCE
DIMENSIONALITY
DATA
MAY
INTRINSICALLY
LIVE
IN
A
LOWER
DIM
SPACE
TOO
MANY
FEATURES
AND
TOO
FEW
DATA
LOWER
COMPUTATIONAL
EXPENSE
MEMORY
TRAIN
TEST
TIME
WANT
TO
VISUALIZE
THE
DATA
IN
A
LOWER
DIM
SPACE
WANT
TO
USE
DATA
OF
DIFFERENT
DIMENSIONALITY
INPUT
DATA
IN
A
HIGH
DIM
FEATURE
SPACE
OUTPUT
PROJECTION
OF
SAME
DATA
INTO
A
LOWER
DIM
SPACE
F
HIGH
DIM
X
LOW
DIM
X
SLIDE
CREDIT
ERIK
SUDDERTH
FIND
A
PROJECTION
WHERE
THE
DATA
HAS
LOW
RECONSTRUCTION
ERROR
HIGH
VARIANCE
OF
THE
DATA
SEE
HAND
WRITTEN
NOTES
FOR
HOW
WE
FIND
THE
OPTIMAL
PROJECTION
SLIDE
CREDIT
SUBHRANSU
MAJI
DEMO
WITH
EIGENFACES
COVARIANCE
MATRIX
IS
HUGE
FOR
D
PIXELS
BUT
TYPICALLY
EXAMPLES
N
D
SIMPLE
TRICK
X
IS
NXD
MATRIX
OF
NORMALIZED
TRAINING
DATA
SOLVE
FOR
EIGENVECTORS
U
OF
XXT
INSTEAD
OF
XTX
THEN
XU
IS
EIGENVECTOR
OF
COVARIANCE
XTX
NEED
TO
NORMALIZE
EACH
VECTOR
OF
XU
INTO
UNIT
LENGTH
ADAPTED
FROM
DEREK
HOIEM
ONE
GOAL
CAN
BE
TO
PICK
K
SUCH
THAT
P
OF
THE
VARIANCE
OF
THE
DATA
IS
PRESERVED
E
G
LET
Λ
A
VECTOR
CONTAINING
THE
EIGENVALUES
OF
THE
COVARIANCE
MATRIX
TOTAL
VARIANCE
CAN
BE
OBTAINED
FROM
ENTRIES
OF
Λ
SUM
Λ
TAKE
AS
MANY
OF
THESE
ENTRIES
AS
NEEDED
K
FIND
CUMSUM
Λ
P
VARIANCE
PRESERVED
AT
I
TH
EIGENVALUE
FIGURE
A
FROM
BISHOP
APPLICATION
FACE
RECOGNITION
FACE
RECOGNITION
ONCE
YOU
VE
DETECTED
AND
CROPPED
A
FACE
TRY
TO
RECOGNIZE
IT
SALLY
VERIFICATION
A
PERSON
IS
CLAIMING
A
PARTICULAR
IDENTITY
VERIFY
WHETHER
THAT
IS
TRUE
E
G
SECURITY
CLOSED
WORLD
IDENTIFICATION
ASSIGN
A
FACE
TO
ONE
PERSON
FROM
AMONG
A
KNOWN
SET
GENERAL
IDENTIFICATION
ASSIGN
A
FACE
TO
A
KNOWN
PERSON
OR
TO
UNKNOWN
WHEN
VIEWED
AS
VECTORS
OF
PIXEL
VALUES
FACE
IMAGES
ARE
EXTREMELY
HIGH
DIMENSIONAL
IMAGE
DIMENSIONS
SLOW
AND
LOTS
OF
STORAGE
BUT
VERY
FEW
DIMENSIONAL
VECTORS
ARE
VALID
FACE
IMAGES
WE
WANT
TO
EFFECTIVELY
MODEL
THE
SUBSPACE
OF
FACE
IMAGES
FACE
X
IN
FACE
SPACE
COORDINATES
RECONSTRUCTION
X
Μ
PROCESS
LABELED
TRAINING
IMAGES
FIND
MEAN
Μ
AND
COVARIANCE
MATRIX
Σ
FIND
K
PRINCIPAL
COMPONENTS
EIGENVECTORS
OF
Σ
UK
PROJECT
EACH
TRAINING
IMAGE
XI
ONTO
SUBSPACE
SPANNED
BY
PRINCIPAL
COMPONENTS
WIK
U
TXI
U
TXI
GIVEN
NOVEL
IMAGE
X
PROJECT
ONTO
SUBSPACE
WK
U
TX
U
TX
CLASSIFY
AS
CLOSEST
TRAINING
FACE
IN
K
DIMENSIONAL
SUBSPACE
M
TURK
AND
A
PENTLAND
CVPR
SINGULAR
VALUE
DECOMPOSITION
ALTERNATIVE
METHOD
TO
CALCULATE
STILL
SUBTRACT
MEAN
DECOMPOSE
X
U
VT
ORTHOGONAL
XR
X
VS
VR
V
D
VR
X
XR
U
LJT
U
D
LJT
U
MATRIX
PROVIDES
COEFFICIENTS
EXAMPLE
XI
UI
GIVES
THE
LEAST
SQUARES
APPROXIMATION
TO
X
OF
THIS
FORM
I
K
KI
I
K
EIGEN
X
REPRESENT
X
USING
PCA
EX
VIOLA
JONES
DATA
SET
IMAGES
OF
FACES
DIMENSIONAL
MEASUREMENTS
EIGEN
X
REPRESENT
X
USING
PCA
EX
VIOLA
JONES
DATA
SET
IMAGES
OF
FACES
DIMENSIONAL
MEASUREMENTS
TAKE
FIRST
K
PCA
COMPONENTS
N
X
L
MEAN
V
L
V
V
V
EIGEN
X
REPRESENT
X
USING
PCA
EX
VIOLA
JONES
DATA
SET
IMAGES
OF
FACES
DIMENSIONAL
MEASUREMENTS
TAKE
FIRST
K
PCA
COMPONENTS
MEAN
DIR
K
K
DIR
DIR
DIR
K
COLLABORATIVE
FILTERING
NETFLIX
OF
BELLKOR
TEAM
CD
USERS
I
JI
K
J
LATENT
SPACE
MODELS
MODEL
RATINGS
MATRIX
AS
USER
AND
MOVIE
POSITIONS
CJ
USERS
INFER
VALUES
FROM
KNOWN
RATINGS
EXTRAPOLATE
TO
UNRANKED
USERS
D
CJ
LATENT
SPACE
MODELS
OUS
BRAVEHEART
THE
COLOR
PURPLE
I
AMADEUS
CHICK
FLICKS
THE
PRINCESS
DIARIES
THE
LION
KING
INDEPENDENCE
DAY
ESCAPIST
DUMB
AND
DUMBER
TEXT
REPRESENTATIONS
BAG
OF
WORDS
REMEMBER
WORD
COUNTS
BUT
NOT
ORDER
EXAMPLE
RAIN
AND
CHILLY
WEATHER
DIDN
T
KEEP
THOUSANDS
OF
PARADEGOERS
FROM
CAMPING
OUT
FRIDAY
NIGHT
FOR
THE
TOURNAMENT
OF
ROSES
SPIRITS
WERE
HIGH
AMONG
THE
STREET
PARTY
CROWD
AS
THEY
SET
UP
FOR
CURBSIDE
SEATS
FOR
TODAY
PARADE
I
WANT
TO
PARTY
ALL
NIGHT
SAID
TYNE
GAUDIELLE
OF
GLENDALE
WHO
SPENT
THE
LAST
NIGHT
OF
THE
YEAR
ALONG
COLORADO
BOULEVARD
WITH
A
GROUP
OF
FRIENDS
WHETHER
THEY
CAME
FOR
THE
PARTYING
OR
THE
PARADE
CAMPERS
WERE
IN
FOR
A
LONG
NIGHT
RAIN
CONTINUED
INTO
THE
EVENING
AND
TERN
PERATURES
WERE
EXPECTED
TO
DIP
DOWN
INTO
THE
LOW
BAG
OF
WORDS
REMEMBER
WORD
COUNTS
BUT
NOT
ORDER
EXAMPLE
EXAMPLEL
TXT
RAIN
AND
CHILLY
WEATHER
DIDN
T
KEEP
THOUSA
RA
N
PARADEGOERS
FROM
CAMPING
OUT
FRIDAY
NIG
CHILLY
OF
ROSES
WEATHER
DIDN
SPIRITS
WERE
HIGH
AMONG
THE
STREET
PARTY
ER
KEEP
FOR
CURBSIDE
SEATS
FOR
TODAY
PARADE
AN
DS
PARAD
EGOERS
I
WANT
TO
PARTY
ALL
NIGHT
SAID
TYNE
GAU
CAMPING
GLENDALE
WHO
SPENT
THE
LAST
NIGHT
OF
THE
YE
OUT
BOULEVARD
WITH
A
GROUP
OF
FRIENDS
FRIDAY
NIGHT
WHETHER
THEY
CAME
FOR
THE
PARTYING
OR
THE
L
L
L
TH
IN
FOR
A
LONG
NIGHT
RAIN
CONTINUED
INTO
THE
TOURNAMENT
TERN
PERATURES
WERE
EXPECTED
TO
DIP
DOWN
IN
ROSES
SPIRITS
PCA
FOR
TEXT
DATA
CREATE
A
GIANT
MATRIX
OF
WORDS
IN
DOCS
WORD
J
APPEARS
FEATURE
XJ
IN
DOCUMENT
I
DATA
EXAMPLE
I
WORDJ
DOC
I
HUGE
MATRIX
MOSTLY
ZEROS
TYPICALLY
NORMALIZE
BY
E
G
SUM
OVER
J
TO
CONTROL
FOR
SHORT
DOCS
TYPICALLY
DON
T
SUBTRACT
MEAN
OR
NORMALIZE
BY
VARIANCE
MIGHT
TRANSFORM
COUNTS
IN
SOME
WAY
LOG
ETC
PCA
ON
THIS
MATRIX
PROVIDES
A
NEW
REPRESENTATION
DOCUMENT
COMPARISON
FUZZY
SEARCH
CONCEPT
INSTEAD
OF
WORD
MATCHING
PLAN
FOR
TODAY
DIMENSIONALITY
REDUCTION
MOTIVATION
PRINCIPAL
COMPONENT
ANALYSIS
PCA
APPLICATIONS
OF
PCA
PCA
GENERAL
DIMENSIONALITY
REDUCTION
TECHNIQUE
PRESERVES
MOST
OF
VARIANCE
WITH
A
MUCH
MORE
COMPACT
REPRESENTATION
LOWER
STORAGE
REQUIREMENTS
EIGENVECTORS
A
FEW
NUMBERS
PER
FACE
FASTER
MATCHING
WHAT
ARE
SOME
PROBLEMS
THE
DIRECTION
OF
MAXIMUM
VARIANCE
IS
NOT
ALWAYS
GOOD
FOR
CLASSIFICATION
PCA
PRESERVES
MAXIMUM
VARIANCE
A
MORE
DISCRIMINATIVE
SUBSPACE
FISHER
LINEAR
DISCRIMINANTS
FLD
PRESERVES
DISCRIMINATION
FIND
PROJECTION
THAT
MAXIMIZES
SCATTER
BETWEEN
CLASSES
AND
MINIMIZES
SCATTER
WITHIN
CLASSES
FISHER
LINEAR
DISCRIMINANT
USING
TWO
CLASSES
AS
EXAMPLE
POOR
PROJECTION
GOOD
COMPARISON
WITH
PCA
OTHER
DIMENSIONALITY
REDUCTION
METHODS
NON
LINEAR
KERNEL
PCA
SCHÖLKOPF
ET
AL
NEURAL
COMPUTATION
INDEPENDENT
COMPONENT
ANALYSIS
COMON
SIGNAL
PROCESSING
LLE
LOCALLY
LINEAR
EMBEDDING
ROWEIS
AND
SAUL
SCIENCE
ISOMAP
ISOMETRIC
FEATURE
MAPPING
TENENBAUM
ET
AL
SCIENCE
T
SNE
T
DISTRIBUTED
STOCHASTIC
NEIGHBOR
EMBEDDING
VAN
DER
MAATEN
AND
HINTON
JMLR
FIGURE
FROM
GENEVIEVE
PATTERSON
IJCV
CS
MACHINE
LEARNING
LINE
FITTING
BIAS
VARIANCE
TRADE
OFF
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
JANUARY
GENERALIZATION
TRAINING
SET
LABELS
KNOWN
TEST
SET
LABELS
UNKNOWN
HOW
WELL
DOES
A
LEARNED
MODEL
GENERALIZE
FROM
THE
DATA
IT
WAS
TRAINED
ON
TO
A
NEW
TEST
SET
SLIDE
CREDIT
L
LAZEBNIK
GENERALIZATION
COMPONENTS
OF
EXPECTED
LOSS
NOISE
IN
OUR
OBSERVATIONS
UNAVOIDABLE
BIAS
HOW
MUCH
THE
AVERAGE
MODEL
OVER
ALL
TRAINING
SETS
DIFFERS
FROM
THE
TRUE
MODEL
ERROR
DUE
TO
INACCURATE
ASSUMPTIONS
SIMPLIFICATIONS
MADE
BY
THE
MODEL
VARIANCE
HOW
MUCH
MODELS
ESTIMATED
FROM
DIFFERENT
TRAINING
SETS
DIFFER
FROM
EACH
OTHER
UNDERFITTING
MODEL
IS
TOO
SIMPLE
TO
REPRESENT
ALL
THE
RELEVANT
CLASS
CHARACTERISTICS
HIGH
BIAS
AND
LOW
VARIANCE
HIGH
TRAINING
ERROR
AND
HIGH
TEST
ERROR
OVERFITTING
MODEL
IS
TOO
COMPLEX
AND
FITS
IRRELEVANT
CHARACTERISTICS
NOISE
IN
THE
DATA
LOW
BIAS
AND
HIGH
VARIANCE
LOW
TRAINING
ERROR
AND
HIGH
TEST
ERROR
ADAPTED
FROM
L
LAZEBNIK
MODELS
WITH
TOO
FEW
PARAMETERS
ARE
INACCURATE
BECAUSE
OF
A
LARGE
BIAS
NOT
ENOUGH
FLEXIBILITY
MODELS
WITH
TOO
MANY
PARAMETERS
ARE
INACCURATE
BECAUSE
OF
A
LARGE
VARIANCE
TOO
MUCH
SENSITIVITY
TO
THE
SAMPLE
PURPLE
DOTS
POSSIBLE
TEST
POINTS
RED
DOTS
TRAINING
DATA
ALL
THAT
WE
SEE
BEFORE
WE
SHIP
OFF
OUR
MODEL
GREEN
CURVE
TRUE
UNDERLYING
MODEL
BLUE
CURVE
OUR
PREDICTED
MODEL
FIT
ADAPTED
FROM
D
HOIEM
ROOT
MEAN
SQUARE
RMS
ERROR
ORDER
POLYNOMIAL
ORDER
POLYNOMIAL
PENALIZE
LARGE
COEFFICIENT
VALUES
REMEMBER
WE
WANT
TO
MINIMIZE
THIS
EXPRESSION
NO
REGULARIZATION
HUGE
REGULARIZATION
REGULARIZATION
VS
TRAINING
VS
TEST
ERROR
UNDERFITTING
OVERFITTING
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
NUMBER
OF
TRAINING
EXAMPLES
CHOOSING
THE
TRADE
OFF
BETWEEN
BIAS
AND
VARIANCE
NEED
VALIDATION
SET
SEPARATE
FROM
THE
TEST
SET
HIGH
BIAS
LOW
VARIANCE
LOW
BIAS
HIGH
VARIANCE
FIGURE
FROM
CHRIS
BISHOP
GET
MORE
TRAINING
DATA
REGULARIZE
THE
PARAMETERS
CHOOSE
A
SIMPLER
CLASSIFIER
SLIDE
CREDIT
D
HOIEM
REMEMBER
THREE
KINDS
OF
ERROR
INHERENT
UNAVOIDABLE
BIAS
DUE
TO
OVER
SIMPLIFICATIONS
VARIANCE
DUE
TO
INABILITY
TO
PERFECTLY
ESTIMATE
PARAMETERS
FROM
LIMITED
DATA
TRY
SIMPLE
CLASSIFIERS
FIRST
USE
INCREASINGLY
POWERFUL
CLASSIFIERS
WITH
MORE
TRAINING
DATA
BIAS
VARIANCE
TRADE
OFF
ADAPTED
FROM
D
HOIEM
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
JANUARY
YOUR
TA
WILL
BE
BACK
THIS
WEDNESDAY
OFFICE
HOURS
TUESDAY
AND
THURSDAY
FRIDAY
OFFICE
HOURS
START
THIS
THURSDAY
SOLUTION
VIA
LEAST
SQUARES
SOLUTION
VIA
GRADIENT
DESCENT
REGULARIZED
LEAST
SQUARES
DEALING
WITH
OUTLIERS
SOMETIMES
WANT
TO
ADD
A
BIAS
TERM
CAN
ADD
AS
SUCH
THAT
X
XD
FIGURE
FROM
MILOS
HAUSKRECHT
F
X
W
X
Y
AT
TRAINING
TIME
USE
GIVEN
XN
YN
TO
ESTIMATE
MAPPING
FUNCTION
F
OBJECTIVE
MINIMIZE
YI
F
W
XI
FOR
ALL
I
N
XI
ARE
THE
INPUT
FEATURES
D
DIMENSIONAL
YI
IS
THE
TARGET
CONTINUOUS
OUTPUT
LABEL
GIVEN
BY
HUMAN
ORACLE
AT
TEST
TIME
USE
F
TO
MAKE
PREDICTION
FOR
SOME
NEW
XTEST
PROBLEM
IS
CALLED
CLASSIFICATION
IF
Y
IS
DISCRETE
DOES
YOUR
PATIENT
HAVE
CANCER
SHOULD
YOUR
BANK
GIVE
THIS
PERSON
A
CREDIT
CARD
IS
IT
GOING
TO
RAIN
TOMORROW
WHAT
ANIMAL
IS
IN
THIS
IMAGE
PROBLEM
IS
CALLED
REGRESSION
IF
Y
IS
CONTINUOUS
WHAT
PRICE
SHOULD
YOU
ASK
FOR
THIS
HOUSE
WHAT
IS
THE
TEMPERATURE
GOING
TO
BE
TOMORROW
WHAT
SCORE
SHOULD
YOUR
SYSTEM
GIVE
TO
THIS
PERSON
FIGURE
SKATING
PERFORMANCE
LINEAR
REGRESSION
R
R
X
WE
BEGIN
BY
CONSIDERING
LINEAR
REGRESSION
EASY
TO
EXTEND
TO
MORE
COMPLEX
PREDICT
IONS
LATER
ON
F
R
R
F
X
W
W
O
W
B
MX
TOMMI
JAAKKOLA
MIT
CSAIL
FIT
LINE
TO
POINTS
USE
PARAMETERS
OF
LINE
TO
PREDICT
THE
Y
COORDINATE
OF
A
NEW
DATA
POINT
XNEW
FIND
PARAMETERS
OF
PLANE
DERIVATION
ON
BOARD
TEST
TEST
CHALLENGES
COMPUTING
THE
PSEUDOINVERSE
MIGHT
BE
SLOW
FOR
LARGE
MATRICES
CUBIC
IN
NUMBER
OF
FEATURES
D
DUE
TO
SVD
LINEAR
IN
NUMBER
OF
SAMPLES
N
WE
MIGHT
WANT
TO
ADJUST
SOLUTION
AS
NEW
EXAMPLES
COME
IN
WITHOUT
RECOMPUTING
THE
PSEUDOINVERSE
FOR
EACH
NEW
SAMPLE
THAT
COMES
IN
ANOTHER
SOLUTION
GRADIENT
DESCENT
COST
LINEAR
IN
BOTH
D
AND
N
IF
D
USE
GRADIENT
DESCENT
GLOBAL
VS
LOCAL
MINIMA
FORTUNATELY
LEAST
SQUARES
IS
CONVEX
LINEAR
REGRESSION
DEFINITION
SOLUTION
VIA
LEAST
SQUARES
SOLUTION
VIA
GRADIENT
DESCENT
DEALING
WITH
OUTLIERS
EXAMPLE
POLYNOMIAL
CURVE
FITTING
M
Y
X
W
W
J
X
W
J
X
J
O
WHERE
Q
X
ARE
KNOWN
AS
BASIS
FUNCTIONS
TYPICA
LY
X
SO
THAT
ACTS
AS
A
BIAS
LEST
WE
F
UNC
TIONS
J
X
XD
CONSIDER
THE
ERROR
FUNCTION
DATA
TERM
REGULARIZATION
TERM
WITH
THE
SUM
OF
SQUARES
ERROR
FUNCTION
AND
A
QUADRATIC
REGULARIZER
WE
GET
WHICH
IS
MINIMIZED
BY
WITH
A
MORE
GENERAL
REGULARIZER
WE
HAVE
ISOSURFACES
W
Q
CONSTANT
E
G
LASSO
QUADRATIC
LASSO
TENDS
TO
GENERATE
SPARSER
SOLUTIONS
THAN
A
QUADRATIC
REGULARIZER
PLAN
FOR
TODAY
LINEAR
REGRESSION
DEFINITION
SOLUTION
VIA
LEAST
SQUARES
SOLUTION
VIA
GRADIENT
DESCENT
REGULARIZED
LEAST
SQUARES
HYPOTHESIZE
AND
TEST
TRY
ALL
POSSIBLE
PARAMETER
COMBINATIONS
REPEATEDLY
SAMPLE
ENOUGH
POINTS
TO
SOLVE
FOR
PARAMETERS
EACH
POINT
VOTES
FOR
ALL
CONSISTENT
PARAMETERS
E
G
EACH
POINT
VOTES
FOR
ALL
POSSIBLE
LINES
ON
WHICH
IT
MIGHT
LIE
SCORE
THE
GIVEN
PARAMETERS
NUMBER
OF
CONSISTENT
POINTS
CHOOSE
THE
OPTIMAL
PARAMETERS
USING
THE
SCORES
NOISE
CLUTTER
FEATURES
THEY
WILL
CAST
VOTES
TOO
BUT
TYPICALLY
THEIR
VOTES
SHOULD
BE
INCONSISTENT
WITH
THE
MAJORITY
OF
GOOD
FEATURES
TWO
METHODS
HOUGH
TRANSFORM
AND
RANSAC
ADAPTED
FROM
DEREK
HOIEM
AND
KRISTEN
GRAUMAN
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
CONNECTION
BETWEEN
IMAGE
X
Y
AND
HOUGH
M
B
SPACES
A
LINE
IN
THE
IMAGE
CORRESPONDS
TO
A
POINT
IN
HOUGH
SPACE
WHAT
DOES
A
POINT
IN
THE
IMAGE
SPACE
MAP
TO
ANSWER
THE
SOLUTIONS
OF
B
THIS
IS
A
LINE
IN
HOUGH
SPACE
Y
B
X
M
IMAGE
SPACE
HOUGH
PARAMETER
SPACE
HOW
CAN
WE
USE
THIS
TO
FIND
THE
MOST
LIKELY
PARAMETERS
M
B
FOR
THE
MOST
PROMINENT
LINE
IN
THE
IMAGE
SPACE
LET
EACH
EDGE
POINT
IN
IMAGE
SPACE
VOTE
FOR
A
SET
OF
POSSIBLE
PARAMETERS
IN
HOUGH
SPACE
ACCUMULATE
VOTES
IN
DISCRETE
SET
OF
BINS
PARAMETERS
WITH
THE
MOST
VOTES
INDICATE
LINE
IN
IMAGE
SPACE
RANDOM
SAMPLE
CONSENSUS
RANSAC
RANSAC
LOOP
RANDOMLY
SELECT
A
SEED
GROUP
OF
POINTS
ON
WHICH
TO
BASE
MODEL
ESTIMATE
FIT
MODEL
TO
THESE
POINTS
FIND
INLIERS
TO
THIS
MODEL
I
E
POINTS
WHOSE
DISTANCE
FROM
THE
LINE
IS
LESS
THAN
T
IF
THERE
ARE
D
OR
MORE
INLIERS
RE
COMPUTE
ESTIMATE
OF
MODEL
ON
ALL
OF
THE
INLIERS
REPEAT
N
TIMES
KEEP
THE
MODEL
WITH
THE
LARGEST
NUMBER
OF
INLIERS
FISCHLER
BOLLES
IN
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
RANSAC
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
N
I
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
ALGORITHM
N
I
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
FEBRUARY
JUST
FOR
TODAY
MY
OFFICE
HOURS
WILL
BE
SLIGHTLY
SHIFTED
BASIC
FORMULATION
OF
THE
SIMPLEST
CLASSIFIER
K
NEAREST
NEIGHBORS
EXAMPLE
USE
GENERALIZING
THE
DISTANCE
METRIC
AND
WEIGHTING
NEIGHBORS
DIFFERENTLY
PROBLEMS
THE
CURSE
OF
DIMENSIONALITY
PICKING
K
APPROXIMATION
STRATEGIES
A
TYPE
OF
SUPERVISED
LEARNING
WE
WANT
TO
LEARN
TO
PREDICT
FOR
A
NEW
DATA
POINT
X
ITS
LABEL
Y
E
G
SPAM
NOT
SPAM
DON
T
LEARN
AN
EXPLICIT
FUNCTION
F
X
Y
KEEP
ALL
TRAINING
DATA
X
Y
FOR
A
TEST
EXAMPLE
X
FIND
THE
TRAINING
EXAMPLE
XI
CLOSEST
TO
IT
E
G
USING
EUCLIDEAN
DISTANCE
THEN
COPY
THE
TARGET
LABEL
YI
AS
THE
LABEL
FOR
X
INSTANCE
BASED
METHODS
EXEMPLAR
METHODS
MEMORY
BASED
METHODS
NON
PARAMETRIC
METHODS
FOUR
THINGS
MAKE
A
MEMORY
BASED
LEARNER
A
DISTANCE
METRIC
HOW
MANY
NEARBY
NEIGHBORS
TO
LOOK
AT
A
WEIGHTING
FUNCTION
OPTIONAL
HOW
TO
FIT
WITH
THE
LOCAL
POINTS
FOUR
THINGS
MAKE
A
MEMORY
BASED
LEARNER
A
DISTANCE
METRIC
EUCLIDEAN
AND
OTHERS
HOW
MANY
NEARBY
NEIGHBORS
TO
LOOK
AT
A
WEIGHTING
FUNCTION
OPTIONAL
NOT
USED
HOW
TO
FIT
WITH
THE
LOCAL
POINTS
JUST
PREDICT
THE
SAME
OUTPUT
AS
THE
NEAREST
NEIGHBOR
TRAINING
EXAMPLES
FROM
CLASS
TEST
EXAMPLE
TRAINING
EXAMPLES
FROM
CLASS
F
X
LABEL
OF
THE
TRAINING
EXAMPLE
NEAREST
TO
X
FOUR
THINGS
MAKE
A
MEMORY
BASED
LEARNER
A
DISTANCE
METRIC
EUCLIDEAN
AND
OTHERS
HOW
MANY
NEARBY
NEIGHBORS
TO
LOOK
AT
K
A
WEIGHTING
FUNCTION
OPTIONAL
NOT
USED
HOW
TO
FIT
WITH
THE
LOCAL
POINTS
JUST
PREDICT
THE
AVERAGE
OUTPUT
AMONG
THE
NEAREST
NEIGHBORS
FOR
A
NEW
POINT
FIND
THE
K
CLOSEST
POINTS
FROM
TRAINING
DATA
E
G
K
LABELS
OF
THE
K
POINTS
VOTE
TO
CLASSIFY
BLACK
NEGATIVE
RED
POSITIVE
IF
QUERY
LANDS
HERE
THE
NN
CONSIST
OF
NEGATIVES
AND
POSITIVES
SO
WE
CLASSIFY
IT
AS
NEGATIVE
WHAT
ARE
THE
TRADEOFFS
OF
HAVING
A
TOO
LARGE
K
TOO
SMALL
K
LET
X
BE
OUR
TEST
DATA
POINT
AND
NK
X
BE
THE
INDICES
OF
THE
K
NEAREST
NEIGHBORS
OF
X
CLASSIFICATION
REGRESSION
MILLION
GEOTAGGED
PHOTOS
BY
PHOTOGRAPHERS
SCENE
MATCHES
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
SCENE
MATCHES
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
HAYS
AND
EFROS
ESTIMATING
GEOGRAPHIC
INFORMATION
FROM
A
SINGLE
IMAGE
CVPR
SCENE
MATCHES
THE
IMPORTANCE
OF
DATA
K
NEAREST
NEIGHBOR
FOUR
THINGS
MAKE
A
MEMORY
BASED
LEARNER
HOW
MANY
NEARBY
NEIGHBORS
TO
LOOK
AT
K
HOW
TO
FIT
WITH
THE
LOCAL
POINTS
JUST
PREDICT
THE
AVERAGE
OUTPUT
AMONG
THE
NEAREST
NEIGHBORS
DISTANCES
SUPPOSE
I
WANT
TO
CHARGE
MY
OVERALL
DISTANCE
MORE
FOR
DIFFERENCES
IN
DIRECTION
AS
OPPOSED
TO
DIRECTION
SETUP
A
EQUAL
WEIGHING
ON
ALL
DIRECTIONS
SETUP
B
MORE
WEIGHT
ON
DIRECTION
WILL
MY
NEIGHBORHOODS
BE
LONGER
IN
THE
OR
DIRECTION
VORONOI
PARTITIONING
NEAREST
NEIGHBOR
REGIONS
ALL
POINTS
IN
A
REGION
ARE
CLOSER
TO
THE
SEED
IN
THAT
REGION
THAN
TO
ANY
OTHER
SEED
BLACK
DOTS
SEEDS
FIGURE
FROM
WIKIPEDIA
MULTIVARIATE
DISTANCE
METRICS
SUPPOSE
THE
INPUT
VECTORS
XN
ARE
TWO
DIMENSIONAL
XN
X
X
DIST
XI
XJ
X
X
X
X
DIST
XI
XJ
X
X
THE
RELATIVE
SCALINGS
IN
THE
DISTANCE
METRIC
AFFECT
REGION
SHAPES
ADAPTED
FROM
CARLOS
GUESTRIN
EUCLIDEAN
MINKOWSKI
MAHALANOBIS
WHERE
A
IS
A
POSITIVE
SEMIDEFINITE
MATRIX
SYMMETRIC
MATRIX
WITH
ALL
NON
NEGATIVE
EIGENVALUES
MANHATTAN
FIGURES
FROM
WIKIPEDIA
NEIGHBORS
WEIGHTED
DIFFERENTLY
USE
ALL
SAMPLES
I
E
K
N
WEIGHT
ON
I
TH
SAMPLE
Σ
THE
BANDWIDHT
PARAMETER
EXPRESSES
HOW
QUICKLY
OUR
WEIGHT
FUNCTION
DROPS
OFF
AS
POINTS
GET
FURTHER
AND
FURTHER
FROM
THE
QUERY
X
CLASSIFICATION
REGRESSION
EXTREMES
BANDWIDTH
INFINITY
PREDICTION
IS
DATASET
AVERAGE
BANDWIDTH
ZERO
PREDICTION
BECOMES
NN
KERNEL
REGRESSION
CLASSIFICATION
FOUR
THINGS
MAKE
A
MEMORY
BASED
LEARNER
A
DISTANCE
METRIC
EUCLIDEAN
AND
OTHERS
HOW
MANY
NEARBY
NEIGHBORS
TO
LOOK
AT
ALL
OF
THEM
A
WEIGHTING
FUNCTION
OPTIONAL
WI
EXP
D
XI
QUERY
NEARBY
POINTS
TO
THE
QUERY
ARE
WEIGHTED
STRONGLY
FAR
POINTS
WEAKLY
THE
Σ
PARAMETER
IS
THE
KERNEL
WIDTH
BANDWIDTH
HOW
TO
FIT
WITH
THE
LOCAL
POINTS
PREDICT
THE
WEIGHTED
AVERAGE
OF
THE
OUTPUTS
PROBLEMS
WITH
INSTANCE
BASED
LEARNING
TOO
MANY
FEATURES
DOESN
T
WORK
WELL
IF
LARGE
NUMBER
OF
IRRELEVANT
FEATURES
DISTANCES
OVERWHELMED
BY
NOISY
FEATURES
DISTANCES
BECOME
MEANINGLESS
IN
HIGH
DIMENSIONS
THE
CURSE
OF
DIMENSIONALITY
WHAT
IS
THE
IMPACT
OF
THE
VALUE
OF
K
EXPENSIVE
NO
LEARNING
MOST
REAL
WORK
DONE
DURING
TESTING
FOR
EVERY
TEST
SAMPLE
MUST
SEARCH
THROUGH
ALL
DATASET
VERY
SLOW
MUST
USE
TRICKS
LIKE
APPROXIMATE
NEAREST
NEIGHBOR
SEARCH
NEED
TO
STORE
ALL
TRAINING
DATA
CONSIDER
SPHERE
OF
RADIUS
IN
D
DIMS
CONSIDER
AN
OUTER
Ε
SHELL
IN
THIS
SPHERE
WHAT
IS
SHELL
VOLUME
SPHERE
VOLUME
SLIDE
CREDIT
DHRUV
BATRA
THE
VOLUME
OF
A
SPHERE
WITH
RADIUS
R
IN
D
DIMENSIONS
IS
KDRD
BISHOP
SEC
WHAT
IS
SHELL
VOLUME
SPHERE
VOLUME
AS
D
TENDS
TO
INFINITY
THIS
RATIO
TENDS
TO
I
E
MOST
OF
THE
VOLUME
OF
THE
SPHERE
IS
IN
THAT
THIN
OUTER
SHELL
WHICH
IS
COUNTER
INTUITIVE
FIGURE
FROM
BISHOP
PROBLEM
IN
VERY
HIGH
DIMENSIONS
ALL
POINTS
ARE
EQUALLY
CLOSE
THIS
PROBLEM
APPLIES
TO
ALL
TYPES
OF
CLASSIFIERS
NOT
JUST
K
NN
PROBLEMS
WITH
INSTANCE
BASED
LEARNING
TOO
MANY
FEATURES
DOESN
T
WORK
WELL
IF
LARGE
NUMBER
OF
IRRELEVANT
FEATURES
DISTANCES
OVERWHELMED
BY
NOISY
FEATURES
DISTANCES
BECOME
MEANINGLESS
IN
HIGH
DIMENSIONS
THE
CURSE
OF
DIMENSIONALITY
WHAT
IS
THE
IMPACT
OF
THE
VALUE
OF
K
EXPENSIVE
NO
LEARNING
MOST
REAL
WORK
DONE
DURING
TESTING
FOR
EVERY
TEST
SAMPLE
MUST
SEARCH
THROUGH
ALL
DATASET
VERY
SLOW
MUST
USE
TRICKS
LIKE
APPROXIMATE
NEAREST
NEIGHBOR
SEARCH
NEED
TO
STORE
ALL
TRAINING
DATA
ADAPTED
FROM
DHRUV
BATRA
SLIDE
CREDIT
ALEXANDER
IHLER
INCREASING
K
SIMPLIFIES
DECISION
BOUNDARY
MAJORITY
VOTING
MEANS
LESS
EMPHASIS
ON
INDIVIDUAL
POINTS
K
K
INCREASING
K
SIMPLIFIES
DECISION
BOUNDARY
MAJORITY
VOTING
MEANS
LESS
EMPHASIS
ON
INDIVIDUAL
POINTS
K
K
IKNN
DECISION
BOUNDARY
INCREASING
K
SIMPLIFIES
DECISION
BOUNDARY
MAJORITY
VOTING
MEANS
LESS
EMPHASIS
ON
INDIVIDUAL
POINTS
K
USE
A
VALIDATION
SET
TO
PICK
K
TOO
COMPLEX
SLIDE
CREDIT
ALEXANDER
IHLER
PROBLEMS
WITH
INSTANCE
BASED
LEARNING
TOO
MANY
FEATURES
DOESN
T
WORK
WELL
IF
LARGE
NUMBER
OF
IRRELEVANT
FEATURES
DISTANCES
OVERWHELMED
BY
NOISY
FEATURES
DISTANCES
BECOME
MEANINGLESS
IN
HIGH
DIMENSIONS
THE
CURSE
OF
DIMENSIONALITY
WHAT
IS
THE
IMPACT
OF
THE
VALUE
OF
K
EXPENSIVE
NO
LEARNING
MOST
REAL
WORK
DONE
DURING
TESTING
FOR
EVERY
TEST
SAMPLE
MUST
SEARCH
THROUGH
ALL
DATASET
VERY
SLOW
MUST
USE
TRICKS
LIKE
APPROXIMATE
NEAREST
NEIGHBOR
SEARCH
NEED
TO
STORE
ALL
TRAINING
DATA
ADAPTED
FROM
DHRUV
BATRA
AN
APPROXIMATE
DISTANCE
METHOD
BUILD
A
BALANCED
TREE
OF
DATA
POINTS
KD
TREE
SPLITTING
ALONG
DIFFERENT
DIMENSIONS
GO
DOWN
TREE
STARTING
FROM
ROOT
TO
FIND
IN
WHICH
BIN
A
QUERY
POINT
LIVES
DECLARE
THAT
LEAF
CURRENT
BEST
NEIGHBOR
GO
UP
THE
TREE
CHECKING
FOR
AND
EXPLORING
BRANCHES
AS
NEEDED
WHEN
A
CLOSER
NEIGHBOR
COULD
EXIST
IN
THAT
BRANCH
CHECK
FOR
POSSIBILITY
OF
BETTER
NEIGHBOR
BY
INTERSECTING
REGIONS
WITH
HYPERSPHERE
DEFINED
BY
RADIUS
OF
CURRENT
BEST
ELIMINATE
PARTS
OF
THE
SEARCH
SPACE
IF
THEY
CANNOT
CONTAIN
A
BETTER
CURRENT
BEST
ONLY
SEARCH
FOR
NEIGHBORS
UP
UNTIL
SOME
BUDGET
EXHAUSTED
K
D
TREE
O
LOG
N
QUERY
TIME
FRUIT
DATA
I
I
I
I
C
CA
K
D
TREE
O
LOG
N
QUERY
TIME
FRUIT
DATA
I
I
I
I
I
I
SP
LIT
AT
THE
MEDIAN
C
CA
C
K
D
TREE
O
LOG
N
QUERY
TIME
E
A
C
A
C
K
D
TREE
O
LOG
N
QUERY
TIME
E
A
C
A
C
K
D
TREE
O
LOG
N
QUERY
TIME
FRUIT
DATA
I
I
I
I
I
SP
LIT
AT
THE
MEDIAN
E
A
J
C
C
A
I
I
I
K
D
TREE
O
LOG
N
QUERY
TIME
FRUIT
DATA
I
I
I
I
SP
LIT
AT
THE
MEDIAN
E
A
J
C
O
A
C
I
I
FIGURE
FROM
SZELISKI
SUMMARY
K
NEAREST
NEIGHBOR
IS
THE
MOST
BASIC
AND
SIMPLEST
TO
IMPLEMENT
CLASSIFIER
CHEAP
AT
TRAINING
TIME
EXPENSIVE
AT
TEST
TIME
UNLIKE
OTHER
METHODS
WE
LL
SEE
LATER
NATURALLY
WORKS
FOR
ANY
NUMBER
OF
CLASSES
PICK
K
THROUGH
A
VALIDATION
SET
USE
APPROXIMATE
METHODS
FOR
FINDING
NEIGHBORS
SUCCESS
OF
CLASSIFICATION
DEPENDS
ON
THE
AMOUNT
OF
DATA
AND
THE
MEANINGFULNESS
OF
THE
DISTANCE
FUNCTION
PROF
ADRIANA
KOVASHKA
UNIVERSITY
OF
PITTSBURGH
FEBRUARY
PLAN
FOR
NEXT
TWO
LECTURES
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
PERCEPTRON
LOGISTIC
REGRESSION
MULTI
WAY
CLASSIFICATION
GENERATIVE
VS
DISCRIMINATIVE
MODELS
CLASSIFICATION
VIA
REGRESSION
SUPPOSE
WE
IGNORE
THE
FACT
THAT
THE
TARGET
OUTPUT
Y
IS
BINARY
E
G
RATHER
THAN
A
CONTINUOUS
VARIABLE
SO
WE
WILL
ESTIMATE
A
LINEAR
REGRESSION
FUNCTION
J
X
W
WQ
WDXD
WO
XT
W
BASED
ON
THE
AVAILABLE
DATA
AS
BEFORE
OBJECTIVE
WE
WANT
TO
MINIMIZE
CLASSIFICATION
VIA
REGRESSION
CONT
D
WE
CAN
USE
THE
RESULTING
REGRESSION
FUNCTION
TO
CLASSIFY
ANY
NEW
TEST
EXAMPLE
X
ACCORDING
TO
LABEL
IF
F
X
W
AND
LABEL
OTHERWISE
F
X
W
THEREFORE
DEFINES
A
LINEAR
DECISION
BOUNDARY
THAT
PARTITIONS
THE
INPUT
SPACE
INTO
TWO
CLASS
SPECIFIC
REGIONS
HALF
SPACES
FIGURES
ADAPTED
FROM
FROM
ANDREW
NG
CLASSIFICATION
VIA
REGRESSION
CONT
D
GIVEN
THE
DISSOCIATION
BETWEEN
THE
OBJECTIVE
CLASSIFICATION
AND
THE
ESTIMATION
CRITERION
REGRESSION
IT
IS
NOT
CLEAR
THAT
THIS
APPROACH
LEADS
TO
SENS
IBLE
RESULTS
FN
SOMETIMES
GOOD
SOMETIMES
BAD
TOMMI
JAAKKOLA
MIT
CSAIL
THE
EFFECT
OF
OUTLIERS
ANOTHER
EXAMPLE
MAGENTA
LEAST
SQUARES
GREEN
LOGISTIC
REGRESSION
PROJECTED
POINT
IN
R
Y
XI
Y
Y
N
XNT
WE
CAN
STUDY
HOW
WELL
THE
PROJECTED
POINTS
YN
VIEWED
AS
FUNCTIONS
OF
ARE
SEPARATED
ACROSS
THE
CLASSES
BY
VARYING
W
WE
GET
DIFFERENT
LEVELS
OF
SEPARATION
BETWEEN
THE
PROJECTED
POINTS
WE
WOULD
LIKE
TO
FIND
THAT
SOMEHOW
MAXIMIZES
THE
SEPARATION
OF
THE
PROJECTED
POINTS
ACROSS
CLASSES
WE
CAN
QUANTIFY
THE
SEPARATION
OVERLAP
IN
TERMS
OF
MEANS
AND
VARIANCES
OF
THE
RESULTING
DIMENSIONAL
CLASS
DISTRIBUTIONS
ESTIMATION
CRITERION
WE
FIND
THAT
MAXIMIZES
SEPARATION
OF
PROJECTED
MEANS
J
FI
SHER
W
SUM
OF
WITHIN
CLASS
VARIANCES
FISHER
LINEAR
DISCRIMINANT
FIGURES
FROM
BISHOP
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
LOGISTIC
REGRESSION
MULTI
WAY
CLASSIFICATION
GENERATIVE
VS
DISCRIMINATIVE
MODELS
ROSENBLATT
PREDICTION
RULE
WHERE
WANT
LOSS
OR
JUST
USING
THE
MISCLASSIFIED
EXAMPLES
LOSS
LEARNING
ALGORITHM
UPDATE
RULE
INTERPRETATION
IF
SAMPLE
IS
BEING
MISCLASSIFIED
MAKE
THE
WEIGHT
VECTOR
MORE
LIKE
IT
FIGURES
FROM
BISHOP
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
PERCEPTRON
MULTI
WAY
CLASSIFICATION
GENERATIVE
VS
DISCRIMINATIVE
MODELS
SUPPOSE
WE
KNOW
THE
CLASS
CONDITIONAL
DENSIT
IES
P
XLY
FOR
Y
L
AS
WELL
AS
THE
OVERALL
CLASS
FREQUENCIES
P
Y
HOW
DO
WE
DECIDE
WHICH
CLASS
A
NEW
EXAMPLE
X
BELONGS
TO
SO
AS
TO
MINIMIZE
THE
OVERALL
PROBABILITY
OF
ERROR
SUPPOSE
WE
KNOW
THE
CLASS
CONDITIONAL
DENSITIES
P
XLY
FOR
Y
L
AS
WELL
AS
THE
OVERALL
CLASS
FREQUENCIES
P
Y
HOW
DO
WE
DECIDE
WHICH
CLASS
A
NEW
EXAMPLE
X
BELONGS
TO
SO
AS
TO
MINIMIZE
THE
OVERALL
PROBABILITY
OF
ERROR
THE
MINIMUM
PROBABILITY
OF
ERROR
DECISIONS
ARE
GIVEN
BY
Y
ARGMAX
P
X
LY
P
Y
Y
O
L
ARG
MAX
P
YLX
Y
O
L
THE
OPTIMAL
DECISIONS
ARE
BASED
ON
THE
POSTERIOR
CLASS
PROBABILIT
IESP
Y
LX
FOR
BINARY
CLASSIFICATION
PROBLEMS
WE
CAN
WRITE
THESE
DECISIONS
AS
P
Y
L
L
X
Y
IF
LOG
P
Y
OXL
AND
Y
OTHERWISE
THE
OPTIMAL
DECISIONS
ARE
BASED
ON
THE
POSTERIOR
CLASS
PROBABILITIES
P
Y
LX
FOR
BINARY
CLASSIFICATION
PROBLEMS
WE
CAN
WRITE
THESE
DECISIONS
AS
P
Y
L
L
X
Y
IF
LOG
P
Y
OXL
AND
Y
OTHERWISE
WE
GENERALLY
DON
T
KNOW
P
YLX
BUT
WE
CAN
PARAMETERIZE
THE
POSSIBLE
DECISIONS
ACCORDING
TO
P
Y
LLX
T
LOG
W
W
W
PY
OX
OUR
LOG
ODDS
MODEL
P
Y
L
LX
T
LOG
P
Y
OXL
W
X
W
GIVES
RISE
TO
A
SPECIFIC
FORM
FOR
THE
CONDITIONAL
PROBABILITY
OVER
THE
LABELS
THE
LOGISTIC
MODEL
P
Y
L
L
X
W
A
W
O
XT
WHERE
A
Z
EXP
Z
IS
A
LOGISTIC
SQUASHING
FUNCTION
THAT
TURNS
LINEAR
PREDICTIONS
INT
O
PROBABILIT
IES
Z
LOGISTIC
REGRESSION
MODELS
IMPLY
A
LINEAR
DECISION
BOUNDARY
P
Y
LLX
WO
XT
P
Y
OLX
T
O
OO
O
OOO
AO
O
RR
I
O
O
L
J
O
O
O
O
O
F
O
J
O
O
I
A
O
O
CF
O
D
O
O
O
O
O
OO
R
OD
O
FL
ODB
QO
I
OO
CXI
CXI
O
A
A
AO
ES
A
O
A
CB
OOO
O
RL
CLASS
AS
WITH
THE
LINEAR
REGRESSION
MODELS
WE
CAN
FIT
THE
LOGISTIC
MODELS
USING
THE
MAXIMUM
CONDITIONAL
LOG
LIKELIHOOD
CRITERION
WHERE
N
L
D
W
LOGP
YI
LX
I
W
I
L
STOCHASTIC
GRADIENT
ASCENT
WE
CAN
TRY
TO
MAXIMIZE
THE
LOG
LIKELIHOOD
IN
AN
ON
LINE
OR
INCREMENTAL
FASHION
GIVEN
EACH
TRAINING
INPUT
XI
AND
THE
BINARY
LABEL
YI
WE
CAN
CHANGE
THE
PARAMETERS
W
SLIGHTLY
TO
INCREASE
THE
CORRESPONDING
LOG
PROBABILITY
W
W
RJ
AW
LOG
P
YILXI
W
W
TJ
YI
P
YI
L
LX
I
W
PREDICTION
ERROR
WHERE
TJ
IS
THE
LEARNING
RATE
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
PERCEPTRON
LOGISTIC
REGRESSION
GENERATIVE
VS
DISCRIMINATIVE
MODELS
INSTEAD
OF
JUST
TWO
CLASSES
WE
NOW
HAVE
C
CLASSES
E
G
PREDICT
WHICH
MOVIE
GENRE
A
VIEWER
LIKES
BEST
POSSIBLE
ANSWERS
ACTION
DRAMA
INDIE
THRILLER
ETC
TWO
APPROACHES
ONE
VS
ALL
ONE
VS
ONE
ONE
VS
ALL
A
K
A
ONE
VS
OTHERS
TRAIN
C
CLASSIFIERS
IN
EACH
POS
DATA
FROM
CLASS
I
NEG
DATA
FROM
CLASSES
OTHER
THAN
I
THE
CLASS
WITH
THE
MOST
CONFIDENT
PREDICTION
WINS
EXAMPLE
YOU
HAVE
CLASSES
TRAIN
CLASSIFIERS
VS
OTHERS
SCORE
VS
OTHERS
SCORE
VS
OTHERS
SCORE
VS
OTHER
SCORE
FINAL
PREDICTION
CLASS
ISSUES
ONE
VS
ONE
A
K
A
ALL
VS
ALL
TRAIN
C
C
BINARY
CLASSIFIERS
ALL
PAIRS
OF
CLASSES
THEY
ALL
VOTE
FOR
THE
LABEL
EXAMPLE
YOU
HAVE
CLASSES
THEN
TRAIN
CLASSIFIERS
VS
VS
VS
VS
VS
VS
VOTES
FINAL
PREDICTION
IS
CLASS
WHAT
ARE
SOME
PROBLEMS
WITH
THIS
APPROACH
TO
DOING
MULTI
CLASS
THERE
ARE
NATIVELY
MULTI
CLASS
METHODS
FIGURES
FROM
BISHOP
SINGLE
K
CLASS
CLASSIFIER
FK
X
WKT
X
PREDICTION
ASSIGN
X
TO
CLASS
CK
IF
FK
X
FJ
X
FOR
ALL
J
K
BENEFIT
DECISION
REGIONS
WOULD
NOT
OVERLAP
PLAN
FOR
TODAY
REGRESSION
FOR
CLASSIFICATION
FISHER
LINEAR
DISCRIMINANT
PERCEPTRON
LOGISTIC
REGRESSION
MULTI
WAY
CLASSIFICATION
BINARY
CASE
MULTI
CLASS
CASE
WHY
ARE
THESE
CALLED
GENERATIVE
MODEL
PROBABILITY
OF
THE
DATA
FEATURES
X
CAN
USE
THEM
TO
GENERATE
NEW
SAMPLES
X
CAN
MODEL
WHICH
DATA
POINTS
ARE
OUTLIERS
DON
T
ALWAYS
NEED
TO
MODEL
THE
PROBABILITY
OF
THE
FEATURES
IN
SVMS
COMING
NEXT
WE
DON
T
TRADEOFFS
IN
DISCUSSED
METHODS
REGRESSION
FOR
CLASSIFICATION
GENERALLY
NOT
A
GOOD
IDEA
FISHER
LINEAR
DISCRIMINANT
FINDS
A
DISCRIMINATIVE
PROJECTION
OF
THE
DATA
CAN
BE
COUPLED
WITH
A
THRESHOLD
TO
ALLOW
CLASSIFICATION
PERCEPTRON
DECISION
BOUNDARY
CORRESPONDS
TO
WT
X
SIMPLE
UPDATE
RULE
WON
T
CONVERGE
FOR
NON
LINEARLY
SEPARABLE
DATA
LOGISTIC
REGRESSION
A
CLASSIFICATION
METHOD
SAME
DECISION
BOUNDARY
AS
PERCEPTRON
MODELS
THE
PROBABILITY
OF
A
LABEL
GIVEN
THE
DATA
DATA
DRIVEN
VISUAL
SIMILARITY
FOR
CROSS
DOMAIN
IMAGE
MATCHING
ABHINAV
SHRIVASTAVA
CARNEGIE
MELLON
UNIVERSITY
TOMASZ
MALISIEWICZ
MIT
ABHINAV
GUPTA
CARNEGIE
MELLON
UNIVERSITY
ALEXEI
A
EFROS
CARNEGIE
MELLON
UNIVERSITY
FIGURE
IN
THIS
PAPER
WE
ARE
INTERESTED
IN
DEFINING
VISUAL
SIMILARITY
BETWEEN
IMAGES
ACROSS
DIFFERENT
DOMAINS
SUCH
AS
PHOTOS
TAKEN
IN
DIFFERENT
SEASONS
PAINTINGS
SKETCHES
ETC
WHAT
MAKES
THIS
CHALLENGING
IS
THAT
THE
VISUAL
CONTENT
IS
ONLY
SIMILAR
ON
THE
HIGHER
SCENE
LEVEL
BUT
QUITE
DISSIMILAR
ON
THE
PIXEL
LEVEL
HERE
WE
PRESENT
AN
APPROACH
THAT
WORKS
WELL
ACROSS
DIFFERENT
VISUAL
DOMAINS
ABSTRACT
THE
GOAL
OF
THIS
WORK
IS
TO
FIND
VISUALLY
SIMILAR
IMAGES
EVEN
IF
THEY
APPEAR
QUITE
DIFFERENT
AT
THE
RAW
PIXEL
LEVEL
THIS
TASK
IS
PARTICU
LARLY
IMPORTANT
FOR
MATCHING
IMAGES
ACROSS
VISUAL
DOMAINS
SUCH
AS
PHOTOS
TAKEN
OVER
DIFFERENT
SEASONS
OR
LIGHTING
CONDITIONS
PAINT
INGS
HAND
DRAWN
SKETCHES
ETC
WE
PROPOSE
A
SURPRISINGLY
SIMPLE
METHOD
THAT
ESTIMATES
THE
RELATIVE
IMPORTANCE
OF
DIFFERENT
FEATURES
IN
A
QUERY
IMAGE
BASED
ON
THE
NOTION
OF
DATA
DRIVEN
UNIQUENESS
WE
EMPLOY
STANDARD
TOOLS
FROM
DISCRIMINATIVE
OBJECT
DETECTION
IN
A
NOVEL
WAY
YIELDING
A
GENERIC
APPROACH
THAT
DOES
NOT
DEPEND
ON
A
PARTICULAR
IMAGE
REPRESENTATION
OR
A
SPECIFIC
VISUAL
DOMAIN
OUR
APPROACH
SHOWS
GOOD
PERFORMANCE
ON
A
NUMBER
OF
DIFFICULT
CROSS
DOMAIN
VISUAL
TASKS
E
G
MATCHING
PAINTINGS
OR
SKETCHES
TO
REAL
PHOTOGRAPHS
THE
METHOD
ALSO
ALLOWS
US
TO
DEMONSTRATE
NOVEL
AP
PLICATIONS
SUCH
AS
INTERNET
RE
PHOTOGRAPHY
AND
WHILE
AT
PRESENT
THE
TECHNIQUE
IS
TOO
COMPUTATIONALLY
INTENSIVE
TO
BE
PRACTICAL
FOR
INTERACTIVE
IMAGE
RETRIEVAL
WE
HOPE
THAT
SOME
OF
THE
IDEAS
WILL
EVENTUALLY
BECOME
APPLICABLE
TO
THAT
DOMAIN
AS
WELL
CR
CATEGORIES
I
ARTIFICIAL
INTELLIGENCE
VISION
AND
SCENE
UNDERSTANDING
LEARNING
I
IMAGE
PROCESSING
AND
COMPUTER
VISION
IMAGE
REPRESENTATION
STATISTICAL
KEYWORDS
IMAGE
MATCHING
VISUAL
SIMILARITY
SALIENCY
IMAGE
RE
TRIEVAL
PAINTINGS
SKETCHES
RE
PHOTOGRAPHY
VISUAL
MEMEX
LINKS
INTRODUCTION
POWERED
BY
THE
AVAILABILITY
OF
INTERNET
SCALE
IMAGE
AND
VIDEO
COL
LECTIONS
COUPLED
WITH
GREATER
PROCESSING
SPEEDS
THE
LAST
DECADE
HAS
WITNESSED
THE
RISE
OF
DATA
DRIVEN
APPROACHES
IN
COMPUTER
GRAPHICS
AND
COMPUTATIONAL
PHOTOGRAPHY
UNLIKE
TRADITIONAL
METHODS
WHICH
EMPLOY
PARAMETRIC
MODELS
TO
CAPTURE
VISUAL
PHENOMENA
THE
DATA
DRIVEN
APPROACHES
USE
VISUAL
DATA
DIRECTLY
WITHOUT
AN
EXPLICIT
INTER
MEDIATE
REPRESENTATION
THESE
APPROACHES
HAVE
SHOWN
PROMISING
RESULTS
ON
A
WIDE
RANGE
OF
CHALLENGING
COMPUTER
GRAPHICS
PROBLEMS
INCLUDING
SUPER
RESOLUTION
AND
DE
NOISING
FREEMAN
ET
AL
BUADES
ET
AL
HACOHEN
ET
AL
TEXTURE
AND
VIDEO
SYN
THESIS
EFROS
AND
FREEMAN
SCHODL
ET
AL
IMAGE
ANALO
GIES
HERTZMANN
ET
AL
AUTOMATIC
COLORIZATION
TORRALBA
ET
AL
SCENE
AND
VIDEO
COMPLETION
WEXLER
ET
AL
HAYS
AND
EFROS
WHYTE
ET
AL
PHOTO
RESTORATION
DALE
ET
AL
AS
SEMBLING
PHOTO
REALISTIC
VIRTUAL
SPACES
KANEVA
ET
AL
CHEN
ET
AL
AND
EVEN
MAKING
CG
IMAGERY
MORE
REALISTIC
JOHNSON
ET
AL
TO
GIVE
BUT
A
FEW
EXAMPLES
THE
CENTRAL
ELEMENT
COMMON
TO
ALL
THE
ABOVE
APPROACHES
IS
SEARCH
ING
A
LARGE
DATASET
TO
FIND
VISUALLY
SIMILAR
MATCHES
TO
A
GIVEN
QUERY
BE
IT
AN
IMAGE
PATCH
A
FULL
IMAGE
OR
A
SPATIO
TEMPORAL
BLOCK
HOWEVER
DEFINING
A
GOOD
VISUAL
SIMILARITY
METRIC
TO
USE
FOR
MATCH
ING
CAN
OFTEN
BE
SURPRISINGLY
DIFFICULT
GRANTED
IN
MANY
SITUATIONS
WHERE
THE
DATA
IS
REASONABLY
HOMOGENEOUS
E
G
DIFFERENT
PATCHES
WITHIN
THE
SAME
TEXTURE
IMAGE
EFROS
AND
FREEMAN
OR
DIF
FERENT
FRAMES
WITHIN
THE
SAME
VIDEO
SCHODL
ET
AL
A
SIMPLE
PIXEL
WISE
SUM
OF
SQUARED
DIFFERENCES
MATCHING
WORKS
QUITE
WELL
BUT
WHAT
ABOUT
THE
CASES
WHEN
THE
VISUAL
CONTENT
IS
ONLY
SIM
ILAR
ON
THE
HIGHER
SCENE
LEVEL
BUT
QUITE
DISSIMILAR
ON
THE
PIXEL
LEVEL
FOR
INSTANCE
METHODS
THAT
USE
SCENE
MATCHING
E
G
HAYS
AND
EFROS
DALE
ET
AL
OFTEN
NEED
TO
MATCH
IMAGES
ACROSS
DIFFERENT
ILLUMINATIONS
DIFFERENT
SEASONS
DIFFERENT
CAMERAS
ETC
LIKEWISE
RETEXTURING
AN
IMAGE
IN
THE
STYLE
OF
A
PAINTING
HERTZMANN
ET
AL
EFROS
AND
FREEMAN
REQUIRES
MAKING
VISUAL
CORRESPON
DENCE
BETWEEN
TWO
VERY
DIFFERENT
DOMAINS
PHOTOS
AND
PAINTINGS
CROSS
DOMAIN
MATCHING
IS
EVEN
MORE
CRITICAL
FOR
APPLICATIONS
SUCH
AS
CHEN
ET
AL
AND
JOHNSON
ET
AL
WHICH
AIM
TO
BRING
DOMAINS
AS
DIFFERENT
AS
SKETCHES
AND
CG
RENDERINGS
INTO
CORRESPONDENCE
WITH
NATURAL
PHOTOGRAPHS
IN
ALL
OF
THESE
CASES
PIXEL
WISE
MATCHING
FARES
QUITE
POORLY
BECAUSE
SMALL
PERCEPTUAL
DIFFERENCES
CAN
RESULT
IN
ARBITRARILY
LARGE
PIXEL
WISE
DIF
FERENCES
WHAT
IS
NEEDED
IS
A
VISUAL
METRIC
THAT
CAN
CAPTURE
THE
IMPORTANT
VISUAL
STRUCTURES
THAT
MAKE
TWO
IMAGES
APPEAR
SIMILAR
YET
SHOW
ROBUSTNESS
TO
SMALL
UNIMPORTANT
VISUAL
DETAILS
THIS
IS
PRECISELY
WHAT
MAKES
THIS
PROBLEM
SO
DIFFICULT
THE
VISUAL
SIMILAR
ITY
ALGORITHM
SOMEHOW
NEEDS
TO
KNOW
WHICH
VISUAL
STRUCTURES
ARE
IMPORTANT
FOR
A
HUMAN
OBSERVER
AND
WHICH
ARE
NOT
CURRENTLY
THE
WAY
RESEARCHERS
ADDRESS
THIS
PROBLEM
IS
BY
USING
VAR
FIGURE
IN
DETERMINING
VISUAL
SIMILARITY
THE
CENTRAL
QUESTION
IS
WHICH
VISUAL
STRUCTURES
ARE
IMPORTANT
FOR
A
HUMAN
OBSERVER
AND
WHICH
ARE
NOT
IN
THE
PAINTING
ABOVE
THE
BRUSH
STROKES
IN
THE
SKY
ARE
AS
THICK
AS
THOSE
ON
THE
GROUND
YET
ARE
PERCEIVED
AS
LESS
IMPORTANT
IN
THIS
PAPER
WE
PROPOSE
A
SIMPLE
DATA
DRIVEN
LEARNING
METHOD
FOR
DETERMINING
WHICH
PARTS
OF
A
GIVEN
IMAGE
ARE
MORE
INFORMATIVE
FOR
VISUAL
MATCHING
IOUS
IMAGE
FEATURE
REPRESENTATIONS
SIFT
LOWE
GIST
OLIVA
AND
TORRALBA
HOG
DALAL
AND
TRIGGS
WAVELETS
ETC
THAT
AIM
TO
CAPTURE
THE
LOCALLY
SALIENT
I
E
HIGH
GRADIENT
AND
HIGH
CONTRAST
PARTS
OF
THE
IMAGE
WHILE
DOWNPLAYING
THE
REST
SUCH
REP
RESENTATIONS
HAVE
CERTAINLY
BEEN
VERY
HELPFUL
IN
IMPROVING
IMAGE
MATCHING
ACCURACY
FOR
A
NUMBER
OF
APPLICATIONS
E
G
HAYS
AND
EFROS
KANEVA
ET
AL
DALE
ET
AL
JOHNSON
ET
AL
HOWEVER
WHAT
THESE
FEATURES
ENCODE
ARE
PURELY
LOCAL
TRANS
FORMATIONS
MAPPING
PIXEL
PATCHES
FROM
ONE
FEATURE
SPACE
INTO
AN
OTHER
INDEPENDENT
OF
THE
GLOBAL
IMAGE
CONTENT
THE
PROBLEM
IS
THAT
THE
SAME
LOCAL
FEATURE
MIGHT
BE
UNIMPORTANT
IN
ONE
CONTEXT
BUT
CRU
CIALLY
IMPORTANT
IN
ANOTHER
CONSIDER
FOR
EXAMPLE
THE
PAINTING
IN
FIGURE
IN
LOCAL
APPEARANCE
THE
BRUSH
STROKES
ON
THE
ALLEYWAY
ON
THE
GROUND
ARE
VIRTUALLY
THE
SAME
AS
THE
BRUSH
STROKES
ON
THE
SKY
YET
THE
FORMER
ARE
CLEARLY
MUCH
MORE
INFORMATIVE
AS
TO
THE
CONTENT
OF
THE
IMAGE
THAN
THE
LATTER
AND
SHOULD
BE
GIVEN
A
HIGHER
IMPORTANCE
WHEN
MATCHING
FIGURE
TO
DO
THIS
ALGORITHMICALLY
REQUIRES
NOT
ONLY
CONSIDERING
THE
LOCAL
FEATURES
WITHIN
THE
CONTEXT
OF
A
GIVEN
QUERY
IMAGE
BUT
ALSO
HAVING
A
GOOD
WAY
OF
ESTIMATING
THE
IMPORTANCE
OF
EACH
FEATURE
WITH
RESPECT
TO
THE
PARTICULAR
SCENE
OVERALL
VISUAL
IMPRESSION
WHAT
WE
PRESENT
IN
THIS
PAPER
IS
A
VERY
SIMPLE
YET
SURPRISINGLY
EFFECTIVE
APPROACH
TO
VISUAL
MATCHING
WHICH
IS
PARTICULARLY
WELL
SUITED
FOR
MATCHING
IMAGES
ACROSS
DIFFERENT
DOMAINS
WE
DO
NOT
PROPOSE
ANY
NEW
IMAGE
DESCRIPTORS
OR
FEATURE
REPRESENTATIONS
IN
STEAD
GIVEN
AN
IMAGE
REPRESENTED
BY
SOME
FEATURES
WE
WILL
BE
US
ING
THE
SPATIALLY
RIGID
HOG
DALAL
AND
TRIGGS
DESCRIPTOR
FOR
MOST
OF
THIS
PAPER
THE
AIM
IS
TO
FOCUS
THE
MATCHING
ON
THE
FEATURES
THAT
ARE
THE
MOST
VISUALLY
IMPORTANT
FOR
THIS
PARTICULAR
IMAGE
THE
CENTRAL
IDEA
IS
THE
NOTION
OF
DATA
DRIVEN
UNIQUENESS
WE
HYPOTHE
SIZE
FOLLOWING
BOIMAN
AND
IRANI
THAT
THE
IMPORTANT
PARTS
OF
THE
IMAGE
ARE
THOSE
THAT
ARE
MORE
UNIQUE
OR
RARE
WITHIN
THE
VISUAL
WORLD
REPRESENTED
HERE
BY
A
LARGE
DATASET
FOR
EXAMPLE
IN
FIG
URE
THE
TOWERS
OF
THE
TEMPLE
ARE
VERY
UNIQUE
WHEREAS
THE
WISPY
CLOUDS
IN
THE
SKY
ARE
QUITE
COMMON
HOWEVER
SINCE
THE
SAME
LOCAL
FEATURES
COULD
REPRESENT
VERY
DIFFERENT
VISUAL
CONTENT
DEPENDING
OF
CONTEXT
UNLIKE
BOIMAN
AND
IRANI
OUR
NOTION
OF
UNIQUENESS
IS
SCENE
DEPENDENT
I
E
EACH
QUERY
IMAGE
DECIDES
WHAT
IS
THE
BEST
WAY
TO
WEIGHT
ITS
CONSTITUENT
PARTS
FIGURE
DEMONSTRATES
THE
DIF
FERENCE
BETWEEN
IMAGE
MATCHING
USING
A
STANDARD
UNIFORM
FEATURE
WEIGHTING
VS
OUR
UNIQUENESS
BASED
WEIGHTING
WE
OPERATIONALIZE
THIS
DATA
DRIVEN
UNIQUENESS
BY
USING
IDEAS
FROM
MACHINE
LEARNING
TRAINING
A
DISCRIMINATIVE
CLASSIFIER
TO
DISCOVER
WHICH
PARTS
OF
AN
IMAGE
ARE
MOST
DISCRIMINATIVE
IN
RELATIONSHIP
TO
THE
REST
OF
THE
DATASET
THIS
SIMPLE
APPROACH
RESULTS
IN
VISUAL
MATCHING
THAT
IS
SURPRISINGLY
VERSATILE
AND
ROBUST
BY
FOCUSING
ON
THE
GLOBALLY
SALIENT
PARTS
OF
THE
IMAGE
THE
APPROACH
CAN
BE
SUC
CESSFULLY
USED
FOR
GENERIC
CROSS
DOMAIN
MATCHING
WITHOUT
MAKING
FIGURE
EXAMPLE
OF
IMAGE
MATCHING
USING
THE
SIFT
DESCRIPTOR
WHILE
SIFT
WORKS
VERY
WELL
AT
MATCHING
FINE
IMAGE
STRUCTURE
LEFT
IT
FAILS
MISERABLY
WHEN
THERE
IS
TOO
MUCH
LOCAL
CHANGE
SUCH
AS
A
CHANGE
OF
SEASON
RIGHT
ANY
DOMAIN
SPECIFIC
CHANGES
AS
SHOWN
ON
FIGURE
THE
REST
OF
THE
PAPER
IS
ORGANIZED
AS
FOLLOWS
WE
FIRST
GIVE
A
BRIEF
OVERVIEW
OF
THE
RELATED
WORK
SECTION
THEN
DESCRIBE
OUR
APPROACH
IN
DETAIL
SECTION
PRESENT
AN
EVALUATION
ON
SEVERAL
PUBLIC
DATASETS
SEC
TION
AND
FINALLY
SHOW
SOME
OF
THE
APPLICATIONS
THAT
OUR
ALGORITHM
MAKES
POSSIBLE
SECTION
BACKGROUND
IN
GENERAL
VISUAL
MATCHING
APPROACHES
CAN
BE
DIVIDED
INTO
THREE
BROAD
CLASSES
WITH
DIFFERENT
TECHNIQUES
TAILORED
FOR
EACH
EXACT
MATCHING
FOR
FINDING
MORE
IMAGES
OF
THE
EXACT
SAME
PHYS
ICAL
OBJECT
E
G
A
PEPSI
CAN
OR
SCENE
E
G
ANOTHER
PHOTO
OF
EIF
FEL
TOWER
UNDER
SIMILAR
ILLUMINATION
RESEARCHERS
TYPICALLY
USE
THE
GENERAL
BAG
OF
WORDS
PARADIGM
INTRODUCED
BY
THE
VIDEO
GOOGLE
WORK
SIVIC
AND
ZISSERMAN
WHERE
A
LARGE
HISTOGRAM
OF
QUAN
TIZED
LOCAL
IMAGE
PATCHES
USUALLY
ENCODED
WITH
THE
SIFT
DESCRIP
TOR
LOWE
IS
USED
FOR
IMAGE
RETRIEVAL
THIS
PARADIGM
GENER
ALLY
WORKS
EXTREMELY
WELL
ESPECIALLY
FOR
HEAVILY
TEXTURED
OBJECTS
AND
HAS
LED
TO
MANY
SUCCESSFUL
APPLICATIONS
SUCH
AS
GOOGLE
GOG
GLES
HOWEVER
THESE
METHODS
USUALLY
FAIL
WHEN
TASKED
WITH
FIND
ING
SIMILAR
BUT
NOT
IDENTICAL
OBJECTS
E
G
TRY
USING
GOOGLE
GOG
GLES
APP
TO
FIND
A
CUP
OR
A
CHAIR
THIS
IS
BECAUSE
SIFT
BEING
A
LOCAL
DESCRIPTOR
CAPTURES
THE
MINUTE
DETAILS
OF
A
PARTICULAR
OBJECT
WELL
BUT
NOT
ITS
OVERALL
GLOBAL
PROPERTIES
AS
SEEN
IN
FIGURE
APPROXIMATE
MATCHING
THE
TASK
OF
FINDING
IMAGES
THAT
ARE
MERELY
VISUALLY
SIMILAR
TO
A
QUERY
IMAGE
IS
SIGNIFICANTLY
MORE
DIFFICULT
AND
NONE
OF
THE
CURRENT
APPROACHES
CAN
CLAIM
TO
BE
PAR
TICULARLY
SUCCESSFUL
MOST
FOCUS
ON
EMPLOYING
VARIOUS
IMAGE
REP
RESENTATIONS
THAT
AIM
TO
CAPTURE
THE
IMPORTANT
SALIENT
PARTS
OF
THE
IMAGE
SOME
OF
THE
POPULAR
ONES
INCLUDE
THE
GIST
OLIVA
AND
TOR
RALBA
DESCRIPTOR
THE
HISTOGRAM
OF
GRADIENTS
HOG
DESCRIP
TOR
DALAL
AND
TRIGGS
VARIOUS
OTHER
WAVELET
AND
GRADIENT
BASED
DECOMPOSITIONS
OR
AGGLOMERATIONS
SUCH
AS
THE
SPATIAL
PYRA
MID
LAZEBNIK
ET
AL
OF
VISUAL
WORDS
ALSO
RELATED
IS
THE
VAST
FIELD
OF
CONTENT
BASED
IMAGE
RETRIEVAL
CBIR
SEE
DATTA
ET
AL
FOR
OVERVIEW
HOWEVER
IN
CBIR
THE
GOALS
ARE
SOMEWHAT
DIFFERENT
THE
AIM
IS
TO
RETRIEVE
SEMANTICALLY
RELEVANT
IMAGES
EVEN
IF
THEY
DO
NOT
APPEAR
TO
BE
VISUALLY
SIMILAR
E
G
A
STEAM
ENGINE
WOULD
BE
CONSIDERED
SEMANTICALLY
VERY
SIMILAR
TO
A
BULLET
TRAIN
EVEN
THOUGH
VISUALLY
THERE
IS
LITTLE
IN
COMMON
AS
A
RESULT
MOST
MODERN
CBIR
METHODS
COMBINE
VISUAL
INFORMATION
WITH
TEXTUAL
ANNOTATIONS
AND
USER
INPUT
CROSS
DOMAIN
MATCHING
A
NUMBER
OF
METHODS
EXISTS
FOR
MATCH
ING
BETWEEN
PARTICULAR
DOMAINS
SUCH
AS
SKETCHES
TO
PHOTOGRAPHS
E
G
CHEN
ET
AL
EITZ
ET
AL
DRAWINGS
PAINTINGS
TO
PHOTOGRAPHS
E
G
RUSSELL
ET
AL
OR
PHOTOS
UNDER
DIFFERENT
ILLUMINANTS
E
G
CHONG
ET
AL
ETC
HOWEVER
THESE
TYPICALLY
PRESENT
VERY
DOMAIN
SPECIFIC
SOLUTIONS
THAT
DO
NOT
EASILY
GENERALIZE
ACROSS
MULTIPLE
DOMAINS
OF
THE
GENERAL
SOLUTIONS
THE
MOST
AM
BITIOUS
IS
WORK
BY
SHECHTMAN
AND
IRANI
WHICH
PROPOSES
TO
DESCRIBE
AN
IMAGE
IN
TERMS
OF
LOCAL
SELF
SIMILARITY
DESCRIPTORS
THAT
ARE
INVARIANT
ACROSS
VISUAL
DOMAINS
THIS
WORK
IS
COMPLEMENTARY
TO
OURS
SINCE
IT
FOCUSES
ON
THE
DESIGN
OF
A
CROSS
DOMAIN
LOCAL
DESCRIP
TOR
WHILE
WE
CONSIDER
RELATIVE
WEIGHTING
BETWEEN
THE
DESCRIPTORS
FOR
A
GIVEN
IMAGE
SO
IT
MIGHT
BE
INTERESTING
TO
COMBINE
BOTH
WITHIN
THE
TEXT
RETRIEVAL
COMMUNITY
THE
TF
IDF
NORMALIZA
TION
BAEZA
YATES
AND
RIBEIRO
NETO
USED
IN
THE
BAG
OF
WORDS
APPROACHES
SHARES
THE
SAME
GOALS
AS
OUR
WORK
TRYING
TO
RE
WEIGHT
THE
DIFFERENT
FEATURES
WORDS
IN
TEXT
OR
VISUAL
WORDS
IN
IM
AGES
SIVIC
AND
ZISSERMAN
BASED
ON
THEIR
RELATIVE
FREQUENCY
THE
MAIN
DIFFERENCE
IS
THAT
IN
TF
IDF
EACH
WORD
IS
RE
WEIGHTED
INDE
PENDENTLY
OF
ALL
THE
OTHERS
WHEREAS
OUR
METHOD
TAKES
THE
INTERAC
TIONS
BETWEEN
ALL
OF
THE
FEATURES
INTO
ACCOUNT
MOST
CLOSELY
RELATED
TO
OURS
ARE
APPROACHES
THAT
TRY
TO
LEARN
THE
STATISTICAL
STRUCTURE
OF
NATURAL
IMAGES
BY
USING
LARGE
UNLABELED
IM
AGE
SETS
AS
A
WAY
TO
DEFINE
A
BETTER
VISUAL
SIMILARITY
IN
THE
CONTEXT
OF
IMAGE
RETRIEVAL
HOIEM
ET
AL
ESTIMATE
THE
UN
CONDITIONAL
PROBABILITY
DENSITY
OF
IMAGES
OFF
LINE
AND
USE
IT
IN
A
BAYESIAN
FRAMEWORK
TO
FIND
CLOSE
MATCHES
TIEU
AND
VIOLA
USE
BOOSTING
AT
QUERY
TIME
TO
DISCRIMINATIVELY
LEARN
QUERY
SPECIFIC
FEATURES
HOWEVER
THESE
SYSTEMS
REQUIRE
MULTIPLE
POSITIVE
QUERY
IMAGES
AND
OR
USER
GUIDANCE
WHEREAS
MOST
VISUAL
MATCHING
TASKS
THAT
WE
ARE
INTERESTED
IN
NEED
TO
WORK
AUTOMATICALLY
AND
WITH
ONLY
A
SINGLE
INPUT
IMAGE
FORTUNATELY
RECENT
WORK
IN
VISUAL
RECOGNITION
HAS
SHOWN
THAT
IT
POSSIBLE
TO
TRAIN
A
DISCRIMINATIVE
CLASSIFIER
USING
A
SINGLE
POSITIVE
INSTANCE
AND
A
LARGE
BODY
OF
NEGATIVES
WOLF
ET
AL
MALISIEWICZ
ET
AL
PROVIDED
THAT
THE
NEGATIVES
DO
NOT
CONTAIN
ANY
IMAGES
SIMILAR
TO
THE
POSITIVE
INSTANCE
IN
THIS
WORK
WE
ADAPT
THIS
IDEA
TO
IMAGE
RETRIEVAL
WHERE
ONE
CANNOT
GUARANTEE
THAT
THE
NEGATIVE
SET
WILL
NOT
CONTAIN
IMAGES
SIMILAR
TO
THE
QUERY
ON
THE
CONTRARY
IT
MOST
PROBABLY
WILL
WHAT
WE
SHOW
IS
THAT
SURPRIS
INGLY
THIS
ASSUMPTION
CAN
BE
RELAXED
WITHOUT
ADVERSELY
IMPACTING
THE
PERFORMANCE
APPROACH
THE
PROBLEM
CONSIDERED
IN
THIS
PAPER
IS
THE
FOLLOWING
HOW
TO
COM
PUTE
VISUAL
SIMILARITY
BETWEEN
IMAGES
WHICH
WOULD
BE
MORE
CON
SISTENT
WITH
HUMAN
EXPECTATIONS
ONE
WAY
TO
ATTACK
THIS
IS
BY
DE
SIGNING
A
NEW
MORE
POWERFUL
IMAGE
REPRESENTATION
HOWEVER
WE
BELIEVE
THAT
EXISTING
REPRESENTATIONS
ARE
ALREADY
SUFFICIENTLY
POWER
FUL
BUT
THAT
THE
MAIN
DIFFICULTY
IS
IN
DEVELOPING
THE
RIGHT
SIMILARITY
DISTANCE
FUNCTION
WHICH
CAN
PICK
WHICH
PARTS
OF
THE
REPRESENTA
TION
ARE
MOST
IMPORTANT
FOR
MATCHING
IN
OUR
VIEW
THERE
ARE
TWO
REQUIREMENTS
FOR
A
GOOD
VISUAL
SIMILARITY
FUNCTION
IT
HAS
TO
FO
CUS
ON
THE
CONTENT
OF
THE
IMAGE
THE
WHAT
RATHER
THAT
THE
STYLE
THE
HOW
E
G
THE
IMAGES
ON
FIGURE
SHOULD
EXHIBIT
HIGH
VI
SUAL
SIMILARITY
DESPITE
LARGE
PIXEL
WISE
DIFFERENCES
IT
SHOULD
BE
SCENE
DEPENDENT
THAT
IS
EACH
IMAGE
SHOULD
HAVE
ITS
OWN
UNIQUE
SIMILARITY
FUNCTION
THAT
DEPENDS
ON
ITS
GLOBAL
CONTENT
THIS
IS
IM
PORTANT
SINCE
THE
SAME
LOCAL
FEATURE
CAN
REPRESENT
VASTLY
DIFFERENT
VISUAL
CONTENT
DEPENDING
ON
WHAT
ELSE
IS
DEPICTED
IN
THE
IMAGE
DATA
DRIVEN
UNIQUENESS
THE
VISUAL
SIMILARITY
FUNCTION
THAT
WE
PROPOSE
IS
BASED
ON
THE
IDEA
OF
DATA
DRIVEN
THE
LOW
RANKED
LAST
SAMPLES
WHICH
ARE
EITHER
HIGHLY
OCCLUDED
BLURRED
OR
NOT
GOOD
REP
RESENTATIVES
OF
SIDE
FACING
CLASS
SOURCE
MOTORBIKE
TARGET
BICYCLE
TEST
SET
PASCAL
TEST
PROCEDURE
PASCAL
SIDE
ONLY
IN
ALL
THE
TABLES
TEST
CONFIGURATION
INFORMATION
IS
GIVEN
SIMILAR
TO
THE
LINE
ABOVE
THE
VALUES
FOR
THE
SOURCE
AND
TARGET
ARE
THE
AP
SCORES
OF
THE
SOURCE
I
E
MOTORBIKE
AND
FULL
TARGET
I
E
BICYCLE
TRAINED
WITH
ALL
AVAILABLE
SAMPLES
DETECTORS
ON
THE
TARGET
TASK
TABLE
AVERAGE
PRECISION
AP
COMPARISON
OF
BASELINE
AND
TRANSFER
SVMS
ON
THE
ONE
SHOT
LEARNING
TASK
MODELS
ARE
LEARNED
USING
ONE
SAMPLE
OF
BICYCLE
CLASS
AND
THE
MOTORBIKE
CLAS
SIFIER
AS
THE
SOURCE
THE
TOP
ROW
DISPLAYS
THE
AVERAGE
AP
RESULTS
USING
ONE
OF
THE
TOP
HIGH
RANKED
SAMPLES
RANKED
BY
THE
SOURCE
CLASSIFIER
NEXT
ROWS
DISPLAY
THE
NEXT
IN
THE
RANKING
NOTE
THE
TREMENDOUS
BOOST
OBTAINED
BY
THE
TRANSFER
METHOD
COMPARED
TO
THE
BASE
SVM
WITHOUT
TRANSFER
OUTPERFORMS
PMT
SVM
WE
CONCLUDE
THAT
PMT
SVM
IS
HIGHLY
SENSITIVE
TO
BAD
LOW
RANKED
SAMPLES
DA
SVM
PER
FORMS
VERY
SIMILARLY
TO
A
SVM
FOR
ALL
THE
SAMPLES
TO
OUR
KNOWLEDGE
THERE
IS
NO
PREVIOUS
WORK
ON
HOW
TO
SELECT
OR
WEIGHT
SAMPLES
OF
THE
TARGET
CLASS
WHILE
PERFORM
ING
TRANSFER
LEARNING
UNDER
THE
ASSUMPTION
THAT
THE
SOURCE
CLASS
IS
VISUALLY
SIMILAR
TO
THE
TARGET
CLASS
RANKING
SAMPLES
WITH
THE
SOURCE
DETECTOR
PROVIDES
AN
IDEA
ABOUT
THE
QUAL
ITY
OF
AVAILABLE
SAMPLES
NOTE
THE
RANKING
OF
SAMPLES
USING
THE
SOURCE
AND
FULL
TARGET
I
E
TRAINED
WITH
ALL
AVAILABLE
SAM
PLES
CLASSIFIER
HAS
OVERLAP
IN
THE
TOP
SAMPLES
AND
OVERLAP
FOR
THE
LAST
SAMPLES
THIS
SOURCE
RANKING
CAN
HELP
DURING
THE
SAMPLE
SELECTION
OR
WEIGHTING
OF
THE
SAMPLES
FOR
TRANSFER
LEARNING
SINCE
BAD
SAMPLES
CLEARLY
DE
TERIORATE
THE
PERFORMANCE
SEE
TABLE
AND
LOW
SCORED
SAMPLES
EITHER
SHOULD
BE
REMOVED
OR
AT
LEAST
SHOULD
BE
AS
SIGNED
SMALL
WEIGHTS
SOURCE
COW
TARGET
HORSE
TEST
SET
PASCAL
TEST
PROCEDURE
PASCAL
SIDE
ONLY
TABLE
AP
COMPARISON
OF
BASELINE
AND
TRANSFER
SVMS
ON
THE
ONE
SHOT
LEARNING
TASK
MODELS
ARE
LEARNED
USING
ONE
SAMPLE
OF
HORSE
CLASS
AND
THE
COW
CLASSIFIER
AS
THE
SOURCE
SOURCE
COW
TARGET
HORSE
TEST
SET
PASCAL
TEST
PROCEDURE
PASCAL
SIDE
ONLY
A
SOURCE
MOTORBIKE
TARGET
BICYCLE
TEST
SET
PASCAL
TEST
PROCEDURE
PASCAL
SIDE
ONLY
B
SOURCE
HORSE
TARGET
BICYCLE
TEST
SET
PASCAL
TEST
PROCEDURE
PASCAL
SIDE
ONLY
C
TABLE
AP
RESULTS
OF
BASELINE
SVM
AND
MODEL
TRANSFER
METH
ODS
TRANSFERS
ARE
PERFORMED
A
FROM
COW
TO
HORSE
B
FROM
MO
TORBIKE
TO
BICYCLE
AND
C
FROM
HORSE
TO
BICYCLE
NEGATIVE
TRANSFER
LEFTMOST
COLUMN
DISPLAYS
THE
NUMBER
OF
POSITIVE
SAMPLES
USED
FOR
LEARNING
IN
THIS
AND
ALL
THE
FOLLOWING
TABLES
AND
FIGURES
THE
EX
PERIMENTS
ARE
PERFORMED
FIVE
TIMES
WITH
DIFFERENT
RANDOMIZED
OR
DERINGS
OF
THE
POSITIVE
SAMPLES
MULTIPLE
SHOT
LEARNING
FOR
THE
EXPERIMENTS
WE
USE
A
FIXED
BUT
RANDOM
ORDER
ING
FOR
FOUR
LEARNING
METHODS
TARGET
SAMPLES
ONLY
TRANS
FER
FROM
THE
RIGID
SOURCE
TEMPLATE
USING
A
SVM
AND
PMT
SVM
AND
TRANSFER
FROM
THE
DEFORMABLE
SOURCE
TEMPLATE
US
ING
DA
SVM
EACH
EXPERIMENT
IS
REPEATED
TIMES
WITH
A
DIFFERENT
RANDOM
ORDER
THE
APS
ARE
AVERAGED
FOR
EACH
NUMBER
OF
SAMPLES
THE
AVERAGE
REMOVES
ANY
IDIOSYN
CRASIES
DUE
TO
PARTICULARLY
GOOD
OR
BAD
SAMPLES
TURNING
UP
EARLY
IN
THE
TRAINING
THESE
FLUCTUATIONS
IN
AP
CAN
BE
SEEN
IN
THE
STANDARD
DEVIATIONS
OF
THE
RESULTS
GIVEN
IN
TABLE
THE
TRANSITION
OF
THE
LEARNED
TEMPLATE
IS
ILLUSTRATED
IN
FIGURE
AS
IS
CLEAR
FROM
TABLE
TRANSFER
FROM
THE
SOURCE
CLASS
USING
A
SVM
DA
SVM
AND
PMT
SVM
PERFORMS
SIGNIFI
CANTLY
BETTER
THAN
THE
BASELINE
SVM
ESPECIALLY
FOR
A
SMALL
NUMBER
OF
POSITIVE
SAMPLES
NOTE
THAT
THE
STANDARD
DEVI
ATIONS
OF
ALL
THREE
METHODS
ARE
SMALLER
THAN
THE
BASELINE
SVM
SHOWING
THAT
THE
IDIOSYNCRASIES
DUE
TO
GOOD
AND
BAD
TRAINING
SAMPLES
ARE
BETTER
TOLERATED
AS
THE
NUMBER
OF
POSITIVE
SAMPLES
INCREASES
THE
IMPROVEMENTS
FROM
TRANSFER
LEARNING
METHODS
OVER
THE
BASELINE
DECREASES
AS
CAN
BE
SEEN
FROM
TABLE
PMT
SVM
WORKS
BETTER
FOR
A
SMALL
NUMBER
OF
SAMPLES
IN
TABLE
A
ONE
SAM
PLE
PMT
SVM
APPEARS
WORSE
THAN
A
SVM
IN
FACT
THIS
IS
CAUSED
BY
ONE
OF
THE
ORDERINGS
WHERE
A
BAD
SAMPLE
IS
INTRO
DUCED
WHEN
WE
REMOVE
THAT
ORDERING
AND
AVERAGE
THE
OTHER
ORDERINGS
FOR
THE
ONE
SAMPLE
CASE
PMT
SVM
CLEARLY
OUTPERFORMS
A
SVM
WITH
PERFORMANCE
TO
RESPECTIVELY
THIS
INCIDENT
ALSO
SHOWS
THAT
PMT
SVM
IS
GOOD
FOR
ONE
SHOT
LEARNING
BUT
IT
IS
HIGHLY
SENSITIVE
TO
SAM
PLE
QUALITY
PMT
SVM
ALSO
DOESN
T
PERFORM
WELL
WITH
LARGE
NUMBER
OF
SAMPLES
PROBABLY
CAUSED
BY
THE
INTRODUCTION
OF
BAD
SAMPLES
IN
ADDITION
TO
POSITIVE
TRANSFER
EXPERIMENTS
WE
ALSO
COMPARED
THE
TRANSFER
METHODS
IN
A
NEGATIVE
TRANSFER
CASE
WHERE
WE
LEARN
A
BICYCLE
CLASSIFIER
USING
A
HORSE
CLASSIFIER
AS
THE
SOURCE
AS
CAN
BE
SEEN
FROM
TABLE
C
A
SVM
AND
DA
SVM
PERFORM
WORSE
THAN
THE
BASELINE
SINCE
HORSE
IS
NOT
A
SUITABLE
CLASS
FOR
TRANSFER
LEARNING
OF
A
BICYCLE
CLASS
THE
DETERIORATION
OF
A
SVM
AND
DA
SVM
IS
EXPECTED
HOW
EVER
PMT
SVM
STILL
MANAGES
TO
PERFORM
SOME
BOOST
OVER
THE
BASELINE
SVM
WHICH
SHOWS
THAT
THIS
WEAKER
MODEL
CAN
TRANSFER
AT
THE
COARSE
OBJECTNESS
LEVEL
AS
WELL
AS
ANOTHER
TYPE
OF
BASELINE
FOR
THE
TRANSFER
EXPERIMENTS
WE
INCLUDE
THE
PERFORMANCE
OF
THE
SOURCE
CLASSIFIER
WITHOUT
TRANSFER
FOR
DETECTING
THE
TARGET
CATEGORY
THIS
MEASURES
THE
CONFUSION
BETWEEN
THE
TWO
CATEGORIES
AS
SHOWN
IN
TA
BLE
IF
MORE
THAN
POSITIVE
SAMPLE
IS
USED
THEN
THE
TRANSFER
METHODS
OUTPERFORM
THE
SOURCE
CLASSIFIER
FOR
THE
TARGET
CAT
EGORY
DETECTION
WE
ALSO
EVALUATED
A
SVM
AND
DA
SVM
USING
A
LARGER
SCALE
OF
POSITIVE
SAMPLES
ON
THE
PASCAL
COMPLETE
TEST
SET
SEE
TABLE
AND
FIGURE
IN
ALL
THE
CASES
A
SVM
AND
DA
SVM
PERFORM
BETTER
THAT
THE
BASELINE
SVM
DA
SVM
IS
SUPERIOR
TO
A
SVM
EXCEPT
FOR
THE
SAMPLES
CASE
IN
SUMMARY
THE
RESULTS
SHOW
A
SIGNIFICANT
IMPROVEMENT
THROUGH
TRANSFER
LEARNING
IN
TERMS
OF
HIGHER
START
AND
HIGHER
SLOPE
REFER
TO
FIGURE
MULTIPLE
SHOT
LEARNING
WITH
MULTIPLE
COMPONENTS
ASPECTS
THESE
EXPERIMENTS
ARE
CONDUCTED
ON
CLASSIFIERS
TRAINED
WITH
MULTIPLE
COMPONENTS
ASPECTS
SIMILAR
TO
WE
COMPARE
TWO
METHODS
BASELINE
SVM
A
CLASSIFIER
WITH
MULTIPLE
COM
SOURCE
MOTORBIKE
PASCAL
SIDE
ONLY
PASCAL
DEFAULT
TARGET
BICYCLE
PASCAL
SIDE
ONLY
PASCAL
DEFAULT
TEST
SET
PASCAL
COMPLETE
TABLE
AP
RESULTS
OF
BASELINE
SVM
AND
MODEL
TRANSFER
METHODS
FOR
THE
BICYCLE
DETECTION
TASK
SOURCE
MOTORBIKE
TARGET
BICYCLE
TEST
SET
PASCAL
COMPLETE
TEST
PROCEDURE
PASCAL
SIDE
ONLY
TABLE
AP
RESULTS
OF
MULTIPLE
COMPONENT
BASELINE
SVM
AND
A
SVM
FOR
BICYCLE
DETECTION
TASK
FOR
A
SVM
THE
TRANSFER
IS
PERFORMED
FROM
A
MULTIPLE
COMPONENT
MOTORBIKE
CLASSIFIER
SOURCE
MOTORBIKE
TARGET
BICYCLE
TEST
SET
PASCAL
COMPLETE
TEST
PROCEDURE
PASCAL
SIDE
ONLY
FIGURE
AP
COMPARISON
BETWEEN
BASELINE
SVM
AND
MODEL
TRANS
FER
METHODS
ON
BICYCLE
DETECTION
TASK
PONENTS
FOR
THE
ROOT
FILTER
LEARNED
FROM
THE
POSITIVE
SAMPLES
ONLY
AND
A
SVM
A
MULTIPLE
COMPONENT
CLASSIFIER
LEARNED
BY
TRANSFERRING
FROM
A
MULTIPLE
COMPONENT
SOURCE
CLASSIFIER
THE
EXPERIMENTAL
SETTINGS
ARE
AS
ABOVE
USING
THE
PASCAL
COMPLETE
TEST
SET
EXCEPT
NOW
IN
TRAINING
POSITIVE
TRAIN
ING
SAMPLES
ARE
SELECTED
FROM
ALL
POSES
NOT
JUST
THOSE
OF
THE
SIDE
VIEWS
A
MODEL
WITH
TWO
DISTINCT
COMPONENTS
CORRE
SPONDING
TO
FOUR
COMPONENTS
ONCE
MIRRORED
IS
USED
FOR
THE
SOURCE
AND
THE
TARGET
CLASSIFIERS
TRANSFER
IS
PERFORMED
FROM
THE
MOTORBIKE
CLASS
TO
BICYCLE
CLASS
IN
THE
TRANSFER
TRAINING
EACH
POSITIVE
TRAINING
SAMPLE
IS
ASSIGNED
TO
ONE
OF
THE
COMPONENTS
OF
THE
SOURCE
CLASSIFIER
DEPENDING
ON
THE
SCORE
OF
THE
SAMPLE
OBTAINED
FROM
THE
SOURCE
COMPONENTS
THEN
TRAINING
IS
PERFORMED
IN
A
SIMILAR
FASHION
TO
EXCEPT
WE
USE
TRANSFER
SVM
TRAINING
INSTEAD
OF
CLASSICAL
SVM
TRAINING
SIMILARLY
TO
THE
OTHER
TRANSFER
EXPERIMENTS
TRANSFER
METHODS
ACHIEVE
A
GOOD
PERFORMANCE
IMPROVING
OVER
THE
CLASSICAL
SVM
TRAINING
PARTICULARLY
FOR
A
SMALL
NUMBER
OF
SAMPLES
SEE
TABLE
THE
BOOST
GRADU
ALLY
DECREASES
WHEN
WE
INCREASE
THE
NUMBER
OF
SAMPLES
SPECIALIZATION
SUPERIOR
TO
SUBORDINATE
CATE
GORY
TRANSFER
THESE
TRANSFER
EXPERIMENTS
ARE
CONDUCTED
ON
THE
HORSE
COW
AND
SHEEP
CATEGORIES
OF
PASCAL
VOC
A
QUADRUPED
CATEGORY
DETECTOR
IS
TRAINED
FROM
RANDOMLY
SELECTED
EXAMPLES
FROM
THE
HORSE
COW
AND
SHEEP
CATE
GORIES
IT
IS
THEN
SPECIALIZED
TO
ONE
OF
THOSE
CATEGORIES
BY
TRANSFER
LEARNING
USING
THE
MODEL
TRANSFER
METHODS
WE
OMITTED
PMT
SVM
SINCE
IT
DOESN
T
PERFORM
WELL
FOR
A
LARGE
NUMBER
OF
SAMPLES
THE
EXPERIMENTAL
SETTINGS
AND
PROCE
DURES
ARE
THE
SAME
AS
MULTIPLE
SHOT
LEARNING
EXPERIMENTS
ON
PASCAL
COMPLETE
TEST
SET
TABLE
SHOWS
THAT
SPECIALIZING
FROM
THE
QUADRUPED
CLASS
TO
A
SUBORDINATE
CLASS
USING
MODEL
TRANSFER
AGAIN
GIVES
A
SIG
NIFICANT
PERFORMANCE
IMPROVEMENT
OVER
THE
BASELINE
SVM
ESPECIALLY
FOR
A
SMALL
NUMBER
OF
POSITIVE
SAMPLES
OCCA
SIONALLY
USING
A
LARGE
NUMBER
OF
SAMPLES
THE
BASELINE
SVM
CAN
PERFORM
BETTER
THAN
TRANSFER
METHODS
IN
OUR
EXPERI
MENTS
THE
TRANSFER
PARAMETER
Γ
IS
FIXED
DECREASING
Γ
WHEN
WE
HAVE
LARGE
NUMBER
OF
SAMPLES
WOULD
SOLVE
THE
PROBLEM
SINCE
OUR
MODELS
CONVERGES
TO
THE
CLASSICAL
SVM
FORMULA
TION
WHEN
Γ
DISCUSSION
THE
BENEFITS
OF
TRAINING
A
SUPERIOR
CLASS
QUADRUPED
IN
THIS
CASE
ARE
THAT
THE
TRAINING
SAMPLES
CAN
COME
FROM
MULTIPLE
SUBORDINATE
CLASSES
THIS
IS
SIMILAR
TO
THE
CASE
OF
ATTRIBUTE
TRAINING
INDEED
THE
SUPERIOR
CLASS
NEED
NOT
INVOLVE
ANY
TRAINING
FROM
THE
TARGET
CLASS
HOWEVER
WE
ARE
RESTRICTED
HERE
IN
USING
PASCAL
VOC
SINCE
THERE
ARE
SO
FEW
CATEGORIES
IT
IS
NOT
POSSIBLE
TO
TRAIN
A
SUPERIOR
CLASS
WITHOUT
ALSO
INCLUDING
ALL
SUBORDINATE
CLASSES
NEVERTHE
LESS
THE
BENEFIT
OF
SPECIALIZING
BY
TRANSFER
LEARNING
IS
WELL
DEMONSTRATED
CONCLUSIONS
AND
FUTURE
WORK
ALMOST
ALL
OBJECT
CATEGORY
DETECTION
METHODS
TO
DATE
LEARN
THE
CLASSIFIER
FROM
SCRATCH
TABULA
RASA
WE
HAVE
PROPOSED
A
STRAIGHTFORWARD
MODIFICATION
OF
THE
LEARNING
OB
JECTIVE
FUNCTION
WHICH
RETAINS
THE
BENEFITS
OF
I
CONVEXITY
II
OPTIMIZATION
METHODS
HONED
TO
THE
SPECIAL
STRUCTURE
OF
AN
SVM
AND
ALSO
BRINGS
THE
BENEFIT
OF
LEARNING
WITH
FEWER
TRAINING
SAMPLES
THE
MODEL
TRANSFER
METHODS
CAN
ACT
AS
A
POWER
BOOST
PLUG
IN
TO
ANY
SVM
TRAINING
SCHEME
THERE
ARE
A
NUMBER
CLEAR
EXTENSIONS
TO
THE
MODEL
I
SO
FAR
THE
TRANSFER
LEARNING
HAS
BEEN
APPLIED
TO
THE
ROOT
FILTER
AND
MULTIPLE
COMPONENT
SCENARIO
OF
THE
NEXT
STEP
IS
TO
EXTEND
IT
TO
MULTIPLE
PARTS
II
SO
FAR
A
SINGLE
FEATURE
TYPE
HAS
BEEN
USED
BUT
THE
MODEL
CAN
ALSO
BE
EXTENDED
TO
MULTI
PLE
FEATURES
SUCH
AS
ARE
USED
IN
ACKNOWLEDGEMENTS
WE
ARE
VERY
GRATEFUL
TO
ANDREA
VEDALDI
FOR
INSIGHTFUL
DISCUSSIONS
AND
THE
VLFEAT
LI
BRARY
FINANCIAL
SUPPORT
WAS
PROVIDED
BY
THE
ROYAL
ACADEMY
OF
ENGINEERING
MICROSOFT
AND
ERC
GRANT
VIS
REC
NO
BIG
DATA
PHUONG
PHAM
APRIL
LEARNING
EVERYTHING
ABOUT
ANYTHING
WEBLY
SUPERVISED
VISUAL
CONCEPT
LEARNING
BASED
ON
SANTOSH
DIVVALA
SLIDE
HORSE
REARING
HORSE
ROLLING
HORSE
REINING
HORSE
EYE
HORSE
WHAT
ARE
THESE
WITHOUT
HUMAN
SUPERVISION
BIASED
NON
COMPREHENSIVE
CONCEPT
SPECIFIC
EXPERTISE
SCALABILITY
APPROACH
OVERALL
QUESTIONS
OR
WE
WILL
GO
INTO
DETAIL
OF
EACH
COMPONENT
RETRIEVE
CONCEPT
VARIATIONS
APPROXIMATELY
N
GRAMS
PER
CONCEPT
SEVERAL
VISUALLY
NON
SALIENT
N
GRAMS
E
G
LAST
HORSE
PARTICULAR
HORSE
CLASSIFIER
BASED
PRUNING
TRAIN
TEST
THUMBNAIL
IMAGES
IF
A
P
THRESHOLD
DISCARD
N
GRAM
FIND
GOOD
QUALITY
N
GRAM
QUALITY
COVERAGE
MERGE
INTO
IF
I
J
ARE
DIFFERENT
NOT
FOR
FINDING
SUPER
NGRAMS
MERGING
SIMILAR
GOOD
QUALITY
NGRAMS
TRAIN
SEPARATE
DPM
PER
SUPER
NGRAM
PRUNING
NOISY
COMPONENTS
MERGING
SIMILAR
APPEARANCE
CLUSTERING
PRUNING
NOISY
COMPONENTS
NOISY
COMPONENTS
PRUNED
BASED
ON
A
P
THRESHOLD
AND
FREQUENCY
MERGING
SIMILAR
COMPONENTS
AS
SUPER
NGRAM
THEIR
SYSTEM
IS
COMPARABLE
TO
BETTER
IN
AVG
IS
THE
BEST
MODEL
AT
THE
COST
OF
FULLY
SUPERVISED
LEARNING
LIMITATION
OF
ANNOTATION
THE
WEBLY
LEARNING
GIVE
REASONABLE
GOOD
RESULT
ADVANTAGES
CUT
ANNOTATION
COST
COMBINE
LINGUISTIC
AND
VISUAL
SEMANTIC
MAY
LEAD
TO
MORE
INTERESTING
RESEARCH
TOPICS
DISADVANTAGES
STILL
NOT
DEFEAT
STATE
OF
THE
ART
EVEN
WITH
TRAINING
SET
AT
LARGE
SCALE
QUESTIONS
SCENE
COMPLETION
USING
MILLIONS
OF
PHOTOGRAPHS
BASED
ON
JAMES
HAYS
SLIDES
INPUT
IMAGE
SCENE
DESCRIPTOR
IMAGE
COLLECTION
COMPLETIONS
CONTEXT
MATCHING
BLENDING
MATCHES
GIST
SCENE
DESCRIPTOR
FROM
ORIENTATION
AND
SCALES
FIND
ITS
NEAREST
NEIGHBORS
IN
MILLION
IMAGES
TOTAL
CONTEXT
MATCHING
GRAPH
CUT
POISSON
BLENDING
HAYS
AND
EFROS
SIGGRAPH
FOR
MISSING
REGIONS
OF
THE
EXISTING
IMAGE
FOR
REGIONS
OF
THE
IMAGE
NOT
COVERED
BE
THE
SCENE
MATCH
FOR
OTHER
PIXELS
WE
ASSIGN
EACH
OF
THE
RESULTS
A
SCORE
WHICH
IS
THE
SUM
OF
THE
SCENE
MATCHING
DISTANCE
THE
CONTEXT
MATCHING
DISTANCE
COLOR
TEXTURE
THE
GRAPH
CUT
COST
OUR
BASELINE
DISCUSSIONS
TOTAL
RECALL
AUTOMATIC
QUERY
EXPANSION
WITH
A
GENERATIVE
FEATURE
MODEL
FOR
OBJECT
RETRIEVAL
ONDˇREJ
JAMES
JOSEF
MICHAEL
AND
ANDREW
GEOMETRY
GROUP
DEPARTMENT
OF
ENGINEERING
SCIENCE
UNIVERSITY
OF
OXFORD
RESEARCH
SILICON
VALLEY
ONDRA
JAMES
JOSEF
AZ
ROBOTS
OX
AC
UK
ABSTRACT
GIVEN
A
QUERY
IMAGE
OF
AN
OBJECT
OUR
OBJECTIVE
IS
TO
RE
TRIEVE
ALL
INSTANCES
OF
THAT
OBJECT
IN
A
LARGE
IMAGE
DATABASE
WE
ADOPT
THE
BAG
OF
VISUAL
WORDS
ARCHITECTURE
WHICH
HAS
PROVEN
SUCCESSFUL
IN
ACHIEVING
HIGH
PRECISION
AT
LOW
RECALL
UNFORTUNATELY
FEATURE
DETECTION
AND
QUANTIZA
TION
ARE
NOISY
PROCESSES
AND
THIS
CAN
RESULT
IN
VARIATION
IN
THE
PARTICULAR
VISUAL
WORDS
THAT
APPEAR
IN
DIFFERENT
IMAGES
OF
THE
SAME
OBJECT
LEADING
TO
MISSED
RESULTS
IN
THE
TEXT
RETRIEVAL
LITERATURE
A
STANDARD
METHOD
FOR
IM
PROVING
PERFORMANCE
IS
QUERY
EXPANSION
A
NUMBER
OF
THE
HIGHLY
RANKED
DOCUMENTS
FROM
THE
ORIGINAL
QUERY
ARE
REIS
SUED
AS
A
NEW
QUERY
IN
THIS
WAY
ADDITIONAL
RELEVANT
TERMS
CAN
BE
ADDED
TO
THE
QUERY
THIS
IS
A
FORM
OF
BLIND
RELE
VANCE
FEEDBACK
AND
IT
CAN
FAIL
IF
OUTLIER
FALSE
POSITIVE
DOCUMENTS
ARE
INCLUDED
IN
THE
REISSUED
QUERY
IN
THIS
PAPER
WE
BRING
QUERY
EXPANSION
INTO
THE
VISUAL
DOMAIN
VIA
TWO
NOVEL
CONTRIBUTIONS
FIRSTLY
STRONG
SPATIAL
CONSTRAINTS
BETWEEN
THE
QUERY
IMAGE
AND
EACH
RESULT
ALLOW
US
TO
ACCURATELY
VERIFY
EACH
RETURN
SUPPRESSING
THE
FALSE
POSITIVES
WHICH
TYPICALLY
RUIN
TEXT
BASED
QUERY
EXPANSION
SECONDLY
THE
VERIFIED
IMAGES
CAN
BE
USED
TO
LEARN
A
LATENT
FEATURE
MODEL
TO
ENABLE
THE
CONTROLLED
CONSTRUCTION
OF
EX
PANDED
QUERIES
WE
ILLUSTRATE
THESE
IDEAS
ON
THE
ANNOTATED
IM
AGE
OXFORD
BUILDING
DATABASE
TOGETHER
WITH
MORE
THAN
FLICKR
IMAGES
WE
SHOW
THAT
THE
PRECISION
IS
SUBSTANTIALLY
BOOSTED
ACHIEVING
TOTAL
RECALL
IN
MANY
CASES
INTRODUCTION
THE
LEADING
METHODS
FOR
OBJECT
RETRIEVAL
FROM
LARGE
IM
AGE
CORPORA
ALL
RELY
ON
VARIANTS
OF
THE
SAME
TECHNIQUE
FIRST
EACH
IMAGE
IN
THE
CORPUS
IS
PROCESSED
TO
EXTRACT
FEATURES
IN
SOME
HIGH
DIMENSIONAL
DESCRIPTOR
SPACE
THESE
DESCRIPTORS
ARE
QUANTIZED
OR
CLUSTERED
TO
MAP
EVERY
FEATURE
TO
A
VISUAL
WORD
IN
SOME
MUCH
SMALLER
DISCRETE
FIGURE
A
SAMPLE
OF
CHALLENGING
RESULTS
RETURNED
BY
OUR
METHOD
IN
ANSWER
TO
A
VISUAL
QUERY
FOR
THE
TOM
TOWER
CHRIST
CHURCH
COL
LEGE
OXFORD
TOP
LEFT
WHICH
WEREN
T
FOUND
BY
A
SIMPLE
BAG
OF
VISUAL
WORDS
METHOD
THIS
QUERY
WAS
PERFORMED
ON
A
LARGE
DATASET
OF
IMAGES
VOCABULARY
THE
CORPUS
IS
THEN
SUMMARIZED
USING
AN
INDEX
WHERE
EACH
IMAGE
IS
REPRESENTED
BY
THE
VISUAL
WORDS
THAT
IT
CONTAINS
AT
QUERY
TIME
THE
SYSTEM
IS
PRESENTED
WITH
A
QUERY
IN
THE
FORM
OF
AN
IMAGE
REGION
THIS
REGION
IS
IT
SELF
PROCESSED
TO
EXTRACT
FEATURE
DESCRIPTORS
THAT
ARE
MAPPED
ONTO
THE
VISUAL
WORD
VOCABULARY
AND
THESE
WORDS
ARE
USED
TO
QUERY
THE
INDEX
THE
RESPONSE
SET
OF
THE
QUERY
IS
A
SET
OF
IMAGES
FROM
THE
CORPUS
THAT
CONTAIN
A
LARGE
NUMBER
OF
VISUAL
WORDS
IN
COMMON
WITH
THE
QUERY
REGION
THESE
RE
SPONSE
IMAGES
MAY
SUBSEQUENTLY
BE
RANKED
USING
SPATIAL
INFORMATION
TO
ENSURE
THAT
THE
RESPONSE
AND
THE
QUERY
NOT
ONLY
CONTAIN
SIMILAR
FEATURES
BUT
THAT
THE
FEATURES
OCCUR
IN
COMPATIBLE
SPATIAL
CONFIGURATIONS
THIS
PROCEDURE
CAN
BE
INTERPRETED
PROBABILISTICALLY
AS
FOLLOWS
THE
SYSTEM
EXTRACTS
A
GENERATIVE
MODEL
OF
AN
OB
JECT
FROM
THE
QUERY
REGION
THEN
FORMS
THE
RESPONSE
SET
FROM
THOSE
IMAGES
IN
THE
CORPUS
THAT
ARE
LIKELY
TO
HAVE
BEEN
GEN
ERATED
FROM
THAT
MODEL
THE
GENERATIVE
MODEL
IN
THIS
CASE
IS
A
SPATIAL
CONFIGURATION
OF
VISUAL
WORDS
EXTRACTED
FROM
THE
QUERY
REGION
TOGETHER
WITH
A
BACKGROUND
DISTRIBUTION
OF
WORDS
THAT
ENCODES
THE
OVERALL
FREQUENCY
STATISTICS
OF
THE
CORPUS
IN
THIS
PAPER
WE
EXPLORE
WAYS
TO
DERIVE
BETTER
OBJECT
MODELS
GIVEN
THE
QUERY
REGION
IN
ORDER
TO
IMPROVE
RETRIEVAL
PERFORMANCE
WE
KEEP
THE
FORM
OF
THE
MODEL
FIXED
IT
IS
STILL
A
CONFIGURATION
OF
VISUAL
WORDS
HOWEVER
RATHER
THAN
SIMPLY
EXTRACTING
THE
MODEL
FROM
THE
SINGLE
INPUT
QUERY
RE
GION
WE
ENRICH
IT
WITH
ADDITIONAL
INFORMATION
FROM
THE
COR
PUS
WE
REFER
TO
THIS
AS
A
LATENT
MODEL
OF
THE
OBJECT
THIS
RICHER
MODEL
ACHIEVES
SUBSTANTIALLY
BETTER
RETRIEVAL
PERFOR
MANCE
THAN
THE
STATE
OF
THE
ART
ON
THE
OXFORD
BUILDINGS
DATASET
THE
LATENT
MODEL
IS
A
GENERALIZATION
OF
THE
IDEA
OF
QUERY
EXPANSION
A
WELL
KNOWN
TECHNIQUE
FROM
THE
FIELD
OF
TEXT
BASED
INFORMATION
RETRIEVAL
IN
TEXT
BASED
QUERY
EX
PANSION
A
NUMBER
OF
THE
HIGH
RANKED
DOCUMENTS
FROM
THE
ORIGINAL
RESPONSE
SET
ARE
USED
TO
GENERATE
A
NEW
QUERY
THAT
CAN
BE
USED
TO
OBTAIN
A
NEW
RESPONSE
SET
THIS
IS
A
FORM
OF
BLIND
RELEVANCE
FEEDBACK
IN
THAT
IT
ALLOWS
ADDITIONAL
RELEVANT
TERMS
TO
BE
ADDED
TO
THE
QUERY
IT
IS
PARTICULARLY
WELL
SUITED
TO
OUR
PROBLEM
DOMAIN
FOR
TWO
REASONS
FIRST
THE
SPATIAL
STRUCTURE
OF
IMAGES
ALLOWS
US
TO
BE
VERY
ROBUST
TO
FALSE
POSITIVES
IN
TEXT
RETRIEVAL
RELEVANCE
FEED
BACK
ATTEMPTS
TO
CONSTRUCT
A
TOPIC
MODEL
OF
RELEVANCE
BASED
ON
TERMS
IN
THE
DOCUMENTS
DUE
TO
THE
COMPLEXITIES
OF
NATURAL
LANGUAGE
THE
RELEVANT
TERMS
MAY
BE
SPREAD
ARBI
TRARILY
THROUGHOUT
THE
RETURNED
DOCUMENTS
AND
THE
TASK
IS
COMPLICATED
BY
THE
DRAMATIC
CHANGES
IN
MEANING
THAT
CAN
ARISE
FROM
SUBTLE
REARRANGEMENT
OF
LANGUAGE
TERMS
CONSE
QUENTLY
THERE
IS
SUBSTANTIAL
DANGER
OF
TOPIC
DRIFT
WHERE
AN
INCORRECT
MODEL
IS
INFERRED
FROM
THE
INITIAL
RESULT
SET
LEAD
ING
TO
DIVERGENCE
AS
THE
PROCESS
IS
ITERATED
IN
THE
IMAGE
RETRIEVAL
CASE
WE
ARE
GREATLY
ASSISTED
BY
THE
FACT
THAT
WE
CAN
CONSTRUCT
A
MODEL
OF
A
REGION
RATHER
THAN
THE
WHOLE
IMAGE
AND
THAT
THE
IMAGE
DATA
WITHIN
THE
REGION
IS
VERY
LIKELY
TO
CORRESPOND
TO
THE
OBJECT
OF
INTEREST
WHILE
THERE
MAY
BE
OCCLUSIONS
OBSCURING
PARTS
OF
SOME
MATCHING
REGIONS
IT
IS
REASONABLE
TO
EXPECT
THEM
TO
BE
INDEPENDENT
IN
DIFFERENT
RE
SPONSE
IMAGES
SIMPLIFYING
THE
TASK
OF
INFERRING
THE
LATENT
MODEL
SECOND
THE
BASELINE
IMAGE
SEARCH
WITHOUT
QUERY
EXPAN
SION
SUFFERS
MORE
ACUTELY
FROM
FALSE
NEGATIVES
THAN
MOST
TEXT
RETRIEVAL
SYSTEMS
BECAUSE
THE
VISUAL
WORDS
USED
TO
INDEX
IMAGES
ARE
A
SYNTHETIC
PROJECTION
FROM
A
HIGH
DIMENSIONAL
DESCRIPTOR
SPACE
THEY
SUFFER
FROM
SUBSTANTIAL
NOISE
AND
DROP
OUTS
TWO
VERY
SIMILAR
IMAGE
INSTANCES
OF
THE
SAME
OBJECT
TYPICALLY
HAVE
ONLY
PARTIAL
OVERLAP
OF
THEIR
VISUAL
WORDS
ESPECIALLY
WHEN
THE
FEATURES
ARE
SAMPLED
SPARSELY
AS
IS
COMMON
TO
MANY
SYSTEMS
FOR
PERFORMANCE
REASONS
CONSEQUENTLY
AS
WE
SHOW
IN
SECTION
WE
CAN
SUBSTANTIALLY
IMPROVE
RECALL
AT
A
GIVEN
THRESHOLD
OF
PRECISION
SIMPLY
BY
FORMING
THE
UNION
OF
FEATURES
COMMON
TO
A
TRANSITIVE
CLOSURE
OF
THE
RESPONSE
IMAGES
AN
OUTLINE
OF
OUR
APPROACH
IS
AS
FOLLOWS
GIVEN
A
QUERY
REGION
SEARCH
THE
CORPUS
AND
RETRIEVE
A
SET
OF
IMAGE
REGIONS
THAT
MATCH
THE
QUERY
OBJECT
WE
USE
BAG
OF
VISUAL
WORDS
RETRIEVAL
TOGETHER
WITH
SPATIAL
VERIFICATION
HOWEVER
THE
APPROACH
WOULD
APPLY
TO
RE
TRIEVAL
SYSTEMS
THAT
USE
DIFFERENT
OBJECT
MODELS
COMBINE
THE
RETRIEVED
REGIONS
ALONG
WITH
THE
ORIGINAL
QUERY
TO
FORM
A
RICHER
LATENT
MODEL
OF
THE
OBJECT
OF
IN
TEREST
RE
QUERY
THE
CORPUS
USING
THIS
EXPANDED
MODEL
TO
RE
TRIEVE
AN
EXPANDED
SET
OF
MATCHING
REGIONS
REPEAT
THE
PROCESS
AS
NECESSARY
ALTERNATING
BETWEEN
MODEL
REFINEMENT
AND
RE
QUERYING
IN
THE
FOLLOWING
WE
BRIEFLY
OUTLINE
OUR
IMPLEMENTATION
OF
THE
BAG
OF
VISUAL
WORDS
RETRIEVAL
IN
SECTION
AND
SPATIAL
VERIFICATION
IN
SECTION
SECTION
THEN
DESCRIBES
SEVERAL
AL
TERNATIVE
MECHANISMS
FOR
CONSTRUCTING
LATENT
MODELS
IN
THE
ITERATIVE
FRAMEWORK
DESCRIBED
ABOVE
IN
SECTION
THE
PER
FORMANCE
OF
THESE
MECHANISMS
IS
ASSESSED
ON
A
VERY
CHAL
LENGING
DATASET
OF
OVER
FLICKR
IMAGES
SINCE
OUR
GENERATIVE
MODEL
OUTPUTS
ONLY
VISUAL
WORDS
OUR
SYSTEM
PRESENTS
THE
RESULTS
TO
THE
USER
AS
A
SET
OF
MATCH
ING
IMAGE
REGIONS
FROM
THE
CORPUS
HOWEVER
AS
WE
ARGUE
IN
SECTION
THERE
IS
A
NATURAL
AVENUE
OF
EXTENSIONS
TO
THIS
WORK
THAT
LEAD
TOWARD
MORE
COMPLEX
MODELS
THAT
MIGHT
INCLUDE
DETAILED
INTENSITY
OR
STRUCTURAL
INFORMATION
ABOUT
THE
OBJECT
WITH
THESE
MORE
SOPHISTICATED
MODELS
WE
COULD
IMAGINE
RE
TURNING
A
SYNTHESIS
OF
THE
QUERIED
OBJECT
DIRECTLY
RATHER
THAN
A
SET
OF
MATCHING
IMAGES
REAL
TIME
OBJECT
RETRIEVAL
THIS
SECTION
OVERVIEWS
OUR
BAG
OF
VISUAL
WORDS
REAL
TIME
OBJECT
RETRIEVAL
ENGINE
FURTHER
DETAILS
CAN
BE
FOUND
IN
IMAGE
DESCRIPTION
FOR
EACH
IMAGE
IN
THE
DATASET
SEE
SEC
TION
WE
FIND
MULTI
SCALE
HESSIAN
INTEREST
POINTS
AND
FIT
AN
AFFINE
INVARIANT
REGION
TO
EACH
USING
THE
SEMI
LOCAL
SECOND
MOMENT
MATRIX
ON
AVERAGE
THERE
ARE
REGIONS
DETECTED
ON
AN
IMAGE
OF
SIZE
FOR
EACH
OF
THESE
AFFINE
REGIONS
WE
COMPUTE
DIMENSIONAL
SIFT
DESCRIP
TORS
THE
NUMBER
OF
DESCRIPTORS
GENERATED
FOR
EACH
OF
OUR
DATASETS
IS
SHOWN
IN
TABLE
QUANTIZATION
A
VISUAL
VOCABULARY
OF
WORDS
IS
GENER
ATED
USING
AN
APPROXIMATE
K
MEANS
CLUSTERING
METHOD
BASED
ON
RANDOMIZED
TREES
THIS
PRODUCES
VISUAL
VOCABU
LARIES
WHICH
PERFORM
AS
WELL
AS
THOSE
GENERATED
BY
EXACT
K
MEANS
AT
A
FRACTION
OF
THE
COMPUTATIONAL
COST
EACH
VISUAL
DESCRIPTOR
IS
ASSIGNED
VIA
APPROXIMATE
NEAREST
NEIGHBOUR
SEARCH
TO
A
SINGLE
CLUSTER
CENTRE
GIVING
A
STANDARD
BAG
OF
FIGURE
SAMPLE
OF
QUERY
IMAGES
USED
IN
THE
GROUND
TRUTH
EVAL
UATION
FOR
ALL
QUERY
IMAGES
SEE
VISUAL
WORDS
MODEL
THESE
QUANTIZED
VISUAL
FEATURES
ARE
THEN
USED
TO
INDEX
THE
IMAGES
FOR
THE
SEARCH
ENGINE
SEARCH
ENGINE
OUR
SEARCH
ENGINE
USES
THE
VECTOR
SPACE
MODEL
OF
INFORMATION
RETRIEVAL
THE
QUERY
AND
EACH
DOC
UMENT
IN
THE
CORPUS
IS
REPRESENTED
AS
A
SPARSE
VECTOR
OF
TERM
VISUAL
WORD
OCCURRENCES
AND
SEARCH
THEN
PROCEEDS
BY
CALCULATING
THE
SIMILARITY
BETWEEN
THE
QUERY
VECTOR
AND
EACH
DOCUMENT
VECTOR
WE
USE
THE
STANDARD
TF
IDF
WEIGHTING
SCHEME
WHICH
DOWN
WEIGHTS
THE
CONTRIBUTION
THAT
COM
MONLY
OCCURRING
AND
THEREFORE
LESS
DISCRIMINATIVE
WORDS
MAKE
TO
THE
RELEVANCE
SCORE
FOR
COMPUTATIONAL
SPEED
THE
ENGINE
STORES
WORD
OCCUR
RENCES
IN
AN
INDEX
WHICH
MAPS
INDIVIDUAL
WORDS
TO
THE
DOC
UMENTS
IN
WHICH
THEY
OCCUR
FOR
SPARSE
QUERIES
THIS
CAN
RESULT
IN
A
SUBSTANTIAL
SPEEDUP
OVER
EXAMINING
EVERY
DOC
UMENT
VECTOR
AS
ONLY
DOCUMENTS
WHICH
CONTAIN
COMMON
TO
THE
QUERY
WORDS
NEED
TO
BE
EXAMINED
THE
SCORES
FOR
EACH
DOCUMENT
ARE
ACCUMULATED
SO
THAT
THEY
ARE
IDENTICAL
TO
EXPLICITLY
COMPUTING
THE
SIMILARITY
WITH
LARGE
CORPORA
OF
IMAGES
MEMORY
USAGE
BECOMES
A
MAJOR
CONCERN
TO
HELP
AMELIORATE
THIS
PROBLEM
THE
IN
VERTED
FILE
IS
STORED
IN
A
SPACE
EFFICIENT
BINARY
PACKED
STRUC
TURE
ADDITIONALLY
WHEN
MAIN
MEMORY
IS
EXHAUSTED
THE
EN
GINE
CAN
BE
SWITCHED
TO
USE
AN
INVERTED
FILE
FLATTENED
TO
DISK
WHICH
CACHES
THE
DATA
FOR
THE
MOST
FREQUENTLY
REQUESTED
WORDS
SPATIAL
VERIFICATION
THE
OUTPUT
FROM
PERFORMING
A
QUERY
ON
THE
INVERTED
FILE
DESCRIBED
PREVIOUSLY
IS
A
RANKED
LIST
OF
IMAGES
FOR
A
SIG
NIFICANT
SECTION
OF
THE
CORPUS
UNTIL
NOW
WE
HAVE
CONSID
ERED
THE
FEATURES
IN
EACH
IMAGE
AS
A
VISUAL
BAG
OF
WORDS
AND
HAVE
IGNORED
THEIR
SPATIAL
CONFIGURATIONS
IT
IS
VITAL
FOR
QUERY
EXPANSION
THAT
WE
DO
NOT
EXPAND
USING
FALSE
POSITIVES
OR
USE
FEATURES
WHICH
OCCUR
IN
THE
RESULT
IMAGE
BUT
NOT
ON
THE
OBJECT
OF
INTEREST
TO
ACHIEVE
THIS
WE
USE
A
FAST
RO
BUST
HYPOTHESIZE
AND
VERIFY
PROCEDURE
TO
ESTIMATE
AN
AFFINE
HOMOGRAPHY
BETWEEN
A
QUERY
REGION
AND
TARGET
IMAGE
EACH
INTEREST
POINT
HAS
AN
AFFINE
INVARIANT
SEMI
LOCAL
RE
GION
ASSOCIATED
WITH
IT
AND
WE
USE
THIS
EXTRA
INFORMATION
TO
HYPOTHESIZE
TRANSFORMATIONS
USING
SINGLE
CORRESPONDENCES
THIS
MAKES
OUR
PROCEDURE
BOTH
FAST
THE
NUMBER
OF
HY
POTHESES
TO
TEST
IS
SIMPLY
THE
NUMBER
OF
PUTATIVE
CORRESPON
DENCES
AND
DETERMINISTIC
WE
EXAMINE
EVERY
POSSIBLE
HY
POTHESIS
A
RANSAC
LIKE
SCORING
MECHANISM
IS
USED
TO
SELECT
THE
HYPOTHESIS
WITH
THE
GREATEST
NUMBER
OF
INLIERS
EACH
SINGLE
CORRESPONDENCE
HYPOTHESIZES
A
THREE
DEGREE
OF
FREEDOM
DOF
TRANSFORMATION
ISOTROPIC
SCALE
TRANS
LATION
FOR
A
TYPICAL
QUERY
OF
FEATURES
WITH
A
DIS
CRIMINATIVE
VOCABULARY
THE
NUMBER
OF
CORRESPONDENCES
AND
HENCE
HYPOTHESES
TO
TEST
WILL
BE
OF
THE
ORDER
OF
A
FEW
THOU
SAND
THE
NUMBER
OF
INLIERS
TO
THIS
TRANSFORMATION
IS
FOUND
USING
A
SYMMETRIC
TRANSFER
ERROR
COUPLED
WITH
A
SCALE
THRESHOLD
WHICH
PREVENTS
MIS
SIZED
REGIONS
FROM
SCORING
AS
INLIERS
EACH
HYPOTHESIS
IS
STORED
IN
A
PRIORITY
QUEUE
KEYED
BY
THE
NUMBER
OF
INLIERS
FOR
THE
TOP
HYPOTHESES
FOUND
WE
ITERATIVELY
USE
A
LEAST
SQUARES
RE
ESTIMATION
METHOD
ON
THE
INITIALLY
FOUND
INLIERS
TO
GENERATE
A
FULL
DOF
AFFINE
TRANS
FORMATION
RETURNING
THE
BEST
HYPOTHESIS
AS
THE
ONE
WITH
THE
MOST
INLIERS
AFTER
RE
ESTIMATION
EMPIRICALLY
WE
FIND
THAT
RESULTS
WITH
MORE
THAN
INLIERS
RELIABLY
CONTAIN
THE
OBJECT
BEING
SOUGHT
FOR
WE
CALL
SUCH
RESULTS
SPATIALLY
VERI
FIED
THE
SPATIAL
VERIFICATION
IS
APPLIED
UP
TO
A
MAXIMUM
OF
THE
TOP
RESULTS
RETURNED
FROM
THE
SEARCH
ENGINE
AT
EACH
RESULT
A
DECISION
IS
MADE
ABOUT
WHETHER
TO
PROCEED
WITH
THE
VERIFICATION
FURTHER
DOWN
THE
RANKED
LIST
BASED
ON
HOW
RECENTLY
A
VERIFIED
IMAGE
HAS
BEEN
SEEN
IF
NO
VERIFIED
RESULT
HAS
BEEN
SEEN
IN
THE
LAST
RANKED
IMAGES
THEN
WE
STOP
RETURNING
THE
VERIFIED
IMAGES
SEEN
SO
FAR
EMPIRICALLY
WE
FIND
THAT
INCREASING
THIS
THRESHOLD
FURTHER
DOES
NOT
SIG
NIFICANTLY
INCREASE
THE
NUMBER
OF
POSITIVELY
VERIFIED
RESULTS
THIS
PREVENTS
US
FROM
NEEDLESSLY
VERIFYING
IMAGES
FOR
RE
SULTS
WHERE
ALL
THE
TRUE
POSITIVE
IMAGES
HAVE
ALREADY
BEEN
SEEN
OR
FROM
PREMATURELY
BAILING
OUT
OF
VERIFICATION
WHEN
THERE
ARE
MORE
TRUE
POSITIVES
WAITING
TO
BE
FOUND
THE
OUT
PUT
IS
A
LIST
OF
IMAGES
RANKED
IN
NON
INCREASING
ORDER
OF
THE
NUMBER
OF
INLIERS
THE
THRESHOLD
OF
INLIERS
IS
USED
TO
PRODUCE
A
LIST
OF
VERIFIED
RESULTS
AND
THEIR
ASSOCIATED
TRANS
FORMATIONS
THIS
LIST
OF
KNOWN
GOOD
RESULTS
IS
ESSENTIAL
FOR
THE
QUERY
EXPANSION
GENERATIVE
MODEL
IN
THIS
SECTION
WE
DESCRIBE
SEVERAL
METHODS
FOR
COM
PUTING
LATENT
OBJECT
MODELS
THESE
ARE
BASED
ON
GENERATIVE
MODELS
OF
THE
FEATURES
AND
THEIR
CONFIGURATION
WITH
DIFFERENT
LEVELS
OF
COMPLEXITY
WE
ACCOUNT
FOR
QUANTIZATION
AND
DE
TECTION
NOISE
AND
THE
EFFECT
OF
DIFFERENT
IMAGE
RESOLUTIONS
EACH
METHOD
STARTS
BY
EVALUATING
THE
ORIGINAL
QUERY
COMPOSED
OF
ALL
THE
VISUAL
WORDS
WHICH
FALL
INSIDE
THE
QUERY
REGION
A
LATENT
MODEL
IS
THEN
CONSTRUCTED
FROM
THE
VERIFIED
IMAGES
RETURNED
FROM
AND
A
NEW
QUERY
OR
SEVERAL
NEW
QUERIES
ISSUED
THIS
IMMEDIATELY
RAISES
TWO
ISSUES
I
HOW
FAR
SHOULD
THIS
SEQUENCE
EXTEND
SHOULD
A
NEW
LATENT
MODEL
BE
BUILT
FROM
THE
RETURNS
OF
AND
ANOTHER
QUERY
ISSUED
ETC
II
HOW
SHOULD
THE
RANKED
LISTS
RETURNED
FROM
BE
COMBINED
WE
EXPLORE
BOTH
THESE
QUESTIONS
NOTE
THAT
THE
BAG
OF
VISUAL
WORD
RESULT
SET
FROM
MUST
BE
VERIFIED
AGAINST
FOR
EXAMPLE
CANNOT
BE
USED
FOR
VERIFICATION
SINCE
WE
ARE
AIMING
TO
OBTAIN
IMAGES
THAT
WERE
NOT
VERIFIED
AGAINST
METHODS
THE
METHODS
CAN
BE
DIVIDED
INTO
THOSE
THAT
ISSUE
A
SINGLE
NEW
QUERY
AND
THOSE
THAT
ISSUE
MULTIPLE
QUERIES
IN
THE
LATTER
CASE
IT
IS
NECESSARY
TO
COMBINE
THE
RETURNED
RANKED
LISTS
FOR
EACH
QUERY
QUERY
EXPANSION
BASELINE
THIS
METHOD
IS
A
STRAIGHT
FOR
WARD
NA
IVE
APPLICATION
OF
QUERY
EXPANSION
AS
IS
USED
IN
TEXT
RETRIEVAL
WE
TAKE
THE
TOP
M
RESULTS
FROM
THE
ORIGI
NAL
QUERY
WITHOUT
SPATIAL
VERIFICATION
AVERAGE
THE
TERM
FREQUENCY
VECTORS
COMPUTED
FROM
THE
ENTIRE
RESULT
IMAGE
AND
REQUERY
ONCE
THE
RESULTS
OF
ARE
APPENDED
TO
THOSE
OF
THE
TOP
TRANSITIVE
CLOSURE
EXPANSION
A
PRIORITY
QUEUE
OF
VERIFIED
IMAGES
IS
KEYED
BY
THE
NUMBER
OF
INLIERS
THEN
AN
IMAGE
IS
TAKEN
FROM
THE
TOP
OF
THE
QUEUE
AND
THE
REGION
CORRESPOND
ING
TO
THE
ORIGINAL
QUERY
REGION
IS
USED
TO
ISSUE
A
NEW
QUERY
VERIFIED
RESULTS
OF
THE
EXPANDED
QUERY
THAT
HAVE
NOT
BEEN
INSERTED
TO
THE
QUEUE
BEFORE
ARE
INSERTED
AGAIN
IN
THE
ORDER
OF
THE
NUMBER
OF
INLIERS
THE
PROCEDURE
REPEATS
UNTIL
THE
QUEUE
IS
EMPTY
THE
IMAGES
IN
THE
FINAL
RESULT
ARE
IN
THE
SAME
ORDER
IN
WHICH
THEY
ENTERED
THE
QUEUE
AVERAGE
QUERY
EXPANSION
A
NEW
QUERY
IS
CONSTRUCTED
BY
AVERAGING
VERIFIED
RESULTS
OF
THE
ORIGINAL
QUERY
FIRST
THE
TOP
M
VERIFIED
RESULTS
RETURNED
BY
THE
SEARCH
ENGINE
ARE
SELECTED
A
NEW
QUERY
QAVG
IS
THEN
FORMED
BY
TAKING
THE
AVERAGE
OF
THE
ORIGINAL
QUERY
AND
THE
M
RESULTS
RECURSIVE
AVERAGE
QUERY
EXPANSION
THIS
METHOD
IM
PROVES
ON
THE
AVERAGE
QUERY
EXPANSION
METHOD
BY
RECUR
SIVELY
GENERATING
QUERIES
QI
FROM
ALL
SPATIALLY
VERIFIED
RE
SULTS
RETURNED
SO
FAR
THE
METHOD
STOPS
ONCE
MORE
THAN
VERIFIED
IMAGES
HAVE
BEEN
FOUND
OR
AFTER
NO
NEW
IMAGES
HAVE
BEEN
POSITIVELY
VERIFIED
MULTIPLE
IMAGE
RESOLUTION
EXPANSION
THE
GENERATIVE
MODEL
IN
THIS
CASE
ALSO
TAKES
ACCOUNT
OF
THE
PROBABILITY
OF
OBSERVING
A
FEATURE
GIVEN
AN
IMAGE
OF
AN
OBJECT
AND
ITS
RES
OLUTION
FEATURES
COVERING
A
SMALL
AREA
OF
THE
OBJECT
ARE
SEEN
ONLY
IN
CLOSE
UP
IMAGES
OR
IMAGES
WITH
HIGH
RESOLU
TION
SIMILARLY
FEATURES
COVERING
THE
WHOLE
OBJECT
ARE
NOT
SEEN
ON
DETAILED
VIEWS
THE
LATENT
IMAGE
IS
CONSTRUCTED
AS
BEFORE
BY
BACK
PRO
JECTING
VERIFIED
REGIONS
OF
USING
THE
HI
TRANSFORMATIONS
THE
NUMBER
OF
PIXELS
OF
THE
PROJECTED
REGION
DEFINES
THE
RESOLUTION
OF
EACH
RESULT
IMAGE
AN
IMAGE
WITH
MEDIAN
RESOLUTION
IS
CHOSEN
AS
A
RESOLUTION
REFERENCE
IMAGE
AND
A
RELATIVE
CHANGE
OF
THE
RESOLUTION
WITH
RESPECT
TO
THE
RESOLU
TION
REFERENCE
IMAGE
IS
COMPUTED
FOR
EACH
RESULT
IMAGE
THE
RESOLUTION
BANDS
ARE
GIVEN
BY
THE
RELATIVE
RESOLUTION
CHANGE
AS
AND
WE
CONSTRUCT
AN
AVERAGE
QUERY
FOR
EACH
OF
THE
THREE
DIFFERENT
RESOLUTION
BANDS
USING
ONLY
IMAGES
THAT
HAVE
RESOLUTION
WITHIN
THAT
SCALE
BAND
THE
QUERIES
ARE
EXECUTED
INDEPENDENTLY
AND
THE
RESULTS
ARE
MERGED
VERIFIED
IMAGES
FROM
ARE
RETURNED
FIRST
RESULTS
FROM
EXPANDED
QUERIES
FOLLOW
IN
ORDER
OF
THE
NUMBER
OF
INLIERS
THE
MAXIMUM
IS
TAKEN
IF
AN
IMAGE
IS
RE
TRIEVED
IN
MORE
THAN
ONE
RESOLUTION
BAND
EXPERIMENTS
TO
EVALUATE
OUR
SYSTEM
WE
USE
THE
OXFORD
DATASET
AVAIL
ABLE
FROM
THIS
IS
A
RELATIVELY
SMALL
SET
OF
IMAGES
WITH
AN
EXTENSIVE
ASSOCIATED
GROUND
TRUTH
WE
ALSO
USE
TWO
ADDITIONAL
UNLABELED
DATASETS
AND
WHICH
ARE
ASSUMED
NOT
TO
CONTAIN
IMAGES
OF
THE
GROUND
TRUTH
LAND
MARKS
THESE
ADDITIONAL
DATASETS
ARE
USED
AS
DISTRACTORS
FOR
THE
SYSTEM
AND
PROVIDE
AN
IMPORTANT
TEST
FOR
THE
SCAL
ABILITY
OF
OUR
METHOD
THESE
THREE
DATASETS
ARE
DESCRIBED
BELOW
AND
COMPARED
IN
TABLE
THE
SET
OF
IMAGES
DOWN
LOADED
FROM
TWO
OR
MORE
OF
FLICKR
TAGS
WILL
NOT
IN
GENERAL
BE
DISJOINT
SO
WE
REMOVE
EXACT
DUPLICATE
IMAGES
FROM
ALL
DAVG
M
DI
I
OUR
DATASETS
THE
OXFORD
DATASET
THIS
DATASET
WAS
CRAWLED
FROM
WHERE
IS
THE
NORMALIZED
TF
VECTOR
OF
THE
QUERY
REGION
AND
DI
IS
THE
NORMALIZED
TF
VECTOR
OF
THE
I
TH
RESULT
FOR
THIS
AVERAGE
WE
TAKE
THE
UNION
OF
FEATURES
OF
THE
ORIGINAL
QUERY
COMBINED
WITH
REGIONS
BACK
PROJECTED
INTO
THE
QUERY
REGION
BY
HI
THE
ESTIMATED
TRANSFORMATION
THIS
IS
THE
SIMPLEST
FORM
OF
LATENT
MODEL
SINCE
NO
ACCOUNT
IS
TAKEN
OF
THE
STABIL
ITY
OF
THE
FEATURES
OR
THE
RESOLUTION
OF
THE
IMAGES
AGAIN
WE
REQUERY
ONCE
AND
THE
RESULTS
OF
QAVG
ARE
APPENDED
TO
THOSE
TOP
M
OF
FLICKR
USING
QUERIES
FOR
FAMOUS
OXFORD
LANDMARKS
SUCH
AS
OXFORD
CHRIST
CHURCH
AND
OXFORD
RADCLIFFE
CAMERA
IT
CONSISTS
OF
HIGH
RESOLUTION
IMAGES
GROUND
TRUTH
LABELLING
IS
PROVIDED
FOR
LANDMARKS
WITH
FOUR
POSSIBLE
LABELS
AS
FOLLOWS
GOOD
A
NICE
CLEAR
PICTURE
OF
THE
OBJECT
BUILDING
OK
MORE
THAN
OF
THE
OBJECT
IS
CLEARLY
VISIBLE
BAD
THE
OBJECT
IS
NOT
PRESENT
JUNK
LESS
THAN
OF
THE
OBJECT
IS
VISIBLE
OR
THERE
IS
A
VERY
HIGH
LEVEL
OF
OCCLUSION
OR
DISTORTION
FOR
EACH
TABLE
THE
NUMBER
OF
DESCRIPTORS
FOR
EACH
DATASET
LANDMARK
FIVE
STANDARD
QUERIES
ARE
DEFINED
FOR
EVALUATION
A
SAMPLE
OF
QUERY
IMAGES
IS
SHOWN
IN
FIGURE
FOR
THE
REST
SEE
DATASET
THIS
DATASET
WAS
CRAWLED
FROM
FLICKR
MOST
POPULAR
TAGS
AND
CONSISTS
OF
HIGH
RESOLU
TION
IMAGES
OUR
SEARCH
ENGINE
CAN
QUERY
THE
COMBINED
DATASETS
OF
OXFORD
AND
FLICKR
CONSISTING
OF
IMAGES
IN
AROUND
FOR
A
TYPICAL
QUERY
AND
THE
INDEX
CONSUMES
OF
MAIN
MEMORY
DATASET
THIS
DATASET
CONSISTS
OF
MEDIUM
RESOLUTION
DOWNLOADED
FROM
FLICKR
MOST
POPULAR
TAGS
THE
INDEX
FOR
THE
COMBINED
OXFORD
AND
CORPUS
IS
SO
WE
USE
AN
OFFLINE
VERSION
OF
THE
INDEX
WHICH
DOES
NOT
HAVE
TO
SIT
IN
MAIN
MEMORY
QUERYING
THIS
CORPUS
FROM
DISK
TAKES
AROUND
FOR
A
TYPICAL
QUERY
EVALUATION
PROCEDURE
TO
EVALUATE
PERFORMANCE
WE
USE
AVERAGE
PRECISION
AP
COMPUTED
AS
THE
AREA
UNDER
THE
PRECISION
RECALL
CURVE
PRE
CISION
IS
THE
NUMBER
OF
RETRIEVED
POSITIVE
IMAGES
RELATIVE
TO
THE
TOTAL
NUMBER
OF
IMAGES
RETRIEVED
RECALL
IS
THE
NUMBER
OF
RETRIEVED
POSITIVE
IMAGES
RELATIVE
TO
THE
TOTAL
NUMBER
OF
POSITIVES
IN
THE
CORPUS
AN
IDEAL
PRECISION
RECALL
CURVE
HAS
PRECISION
OVER
ALL
RECALL
LEVELS
WHICH
CORRESPONDS
TO
AN
AVERAGE
PRECISION
OF
NOTE
A
PRECISION
RECALL
CURVE
DOES
NOT
HAVE
TO
BE
MONOTONICALLY
DECREASING
TO
ILLUSTRATE
THIS
SAY
THERE
ARE
POSITIVES
OUT
OF
THE
FIRST
RETRIEVED
WHICH
CORRESPONDS
TO
PRECISION
THEN
IF
THE
NEXT
IMAGE
IS
POSITIVE
THE
PRECISION
INCREASES
TO
WE
COMPUTE
AN
AVERAGE
PRECISION
SCORE
FOR
EACH
OF
THE
QUERIES
FOR
A
LANDMARK
AND
THEN
AVERAGE
THESE
TO
OBTAIN
A
MEAN
AVERAGE
PRECISION
MAP
FOR
THE
LANDMARK
FOR
SOME
EXPERIMENTS
IN
ADDITION
TO
THE
MAP
WE
ALSO
DISPLAY
PRECISION
RECALL
CURVES
WHICH
CAN
SOMETIMES
BETTER
ILLUS
TRATE
THE
SUCCESS
OF
OUR
SYSTEM
IN
IMPROVING
RECALL
IN
THE
EVALUATION
THE
GOOD
AND
OK
IMAGES
ARE
TREATED
AS
POSITIVES
BAD
IMAGES
AS
NEGATIVE
AND
JUNK
IMAGES
AS
DON
T
CARE
THE
DON
T
CARE
IMAGES
ARE
HAN
DLED
AS
IF
THEY
WERE
NOT
PRESENT
IN
THE
CORPUS
SO
THAT
IF
OUR
SYSTEM
RETURNS
THEM
THE
SCORE
IS
NOT
AFFECTED
WE
EVALUATE
OUR
SYSTEM
ON
TWO
DATABASES
COMPOSED
OF
OXFORD
DATASETS
IMAGES
AND
OX
FORD
DATASETS
IMAGES
THE
EFFECT
OF
THE
SIZE
OF
THE
DATABASE
ON
THE
PERFORMANCE
IS
DIS
CUSSED
IN
SECTION
FIGURE
PRECISION
RECALL
CURVES
BEFORE
LEFT
AND
AFTER
RIGHT
QUERY
EXPANSION
ON
EXPERIMENT
THESE
RESULTS
ARE
FOR
RESO
LUTION
EXPANSION
OUR
BEST
METHOD
IN
EACH
CASE
THE
FIVE
CURVES
CORRESPOND
TO
THE
FIVE
QUERIES
FOR
THAT
LANDMARK
RETRIEVAL
PERFORMANCE
IN
THIS
SECTION
WE
DISCUSS
SOME
QUANTITATIVE
RESULTS
OF
OUR
METHOD
EVALUATED
AGAINST
THE
GROUND
TRUTH
GATHERED
FROM
THE
OXFORD
DATASET
TABLE
SUMMARIZES
THE
RESULTS
OF
USING
OUR
DIFFERENT
QUERY
EXPANSION
METHODS
MEASURING
THEIR
RELATIVE
PERFOR
MANCE
IN
TERMS
OF
THE
MAP
SCORE
FROM
THE
TABLE
WE
CAN
TABLE
SUMMARY
OF
GROUND
TRUTH
AND
THE
RELATIVE
PERFORMANCE
OF
THE
DIFFERENT
EXPANSION
METHODS
THE
METHODS
ARE
AS
FOLLOWS
ORI
ORIGINAL
QUERY
QEB
QUERY
EXPANSION
BASELINE
TRC
TRANSITIVE
CLOSURE
AVG
AVERAGE
QUERY
EXPANSION
REC
RECURSIVE
AVERAGE
QUERY
EXPANSION
SCA
RESOLUTION
EXPANSION
THE
SHADE
OF
EACH
CELL
SHOWS
RELATIVE
PERFORMANCE
TO
THE
WORST
DARK
AND
THE
BEST
WHITE
RESULT
FOR
A
PARTICULAR
QUERY
ROW
AP
FIGURE
HISTOGRAMS
OF
THE
AVERAGE
PRECISION
FOR
ALL
QUERIES
IN
EXPERIMENT
NOTE
THAT
THE
QUERY
EXPANSION
MOVES
THE
MASS
OF
THE
HISTOGRAM
TOWARDS
THE
RIGHT
HAND
SIDE
I
E
TOWARDS
TOTAL
RECALL
FIGURE
SOME
FALSE
POSITIVE
IMAGES
FOR
MAGDALEN
TOWER
QUERY
THE
TOWER
SHOWN
IS
ACTUALLY
PART
OF
MERTON
COLLEGE
CHAPEL
SEE
THAT
ALL
PROPOSED
QUERY
EXPANSION
WITH
THE
EXCEPTION
OF
THE
QUERY
EXPANSION
BASELINE
METHODS
PERFORM
MUCH
BET
TER
THAN
THE
ORIGINAL
BAG
OF
VISUAL
WORDS
METHOD
SHOWING
A
GAIN
IN
THE
MAP
FROM
TO
ON
AND
FROM
TO
ON
WITH
THE
BEST
METHOD
FIGURE
SHOWS
SELECTED
PRECISION
RECALL
CURVES
FOR
THE
PLAIN
BAG
OF
WORDS
METHOD
ON
THE
LEFT
WITH
THE
CURVES
FROM
THE
RESOLUTION
BASED
QUERY
EXPANSION
SHOWN
ON
THE
RIGHT
IN
ALMOST
ALL
CASES
THE
PRECISION
RECALL
CURVE
HUGS
TO
THE
RIGHT
OF
THE
GRAPH
MUCH
MORE
AFTER
THE
QUERY
EXPANSION
DEMON
STRATING
THE
METHOD
POWER
IN
DRAMATICALLY
IMPROVING
THE
RECALL
OF
A
QUERY
ADDITIONALLY
IN
THE
ORIGINAL
BAG
OF
WORDS
QUERY
EACH
INDIVIDUAL
QUERY
FOR
THE
LANDMARKS
SHOWS
CON
SIDERABLE
VARIANCE
IN
THE
PRECISION
RECALL
PLOTS
WHEREAS
AF
TER
QUERY
EXPANSION
HAS
BEEN
APPLIED
IN
MOST
CASES
THIS
VARIANCE
HAS
BEEN
REDUCED
IMPROVING
ALL
THE
COMPONENT
QUERIES
TO
A
SIMILAR
LEVEL
OF
RETRIEVAL
PERFORMANCE
THE
PLOT
FOR
MAGDALEN
IN
FIGURE
SHOWS
FAILURE
IN
ACHIEVING
TOTAL
RECALL
IF
THE
INITIAL
RESULTS
RETURNED
FROM
THE
BAG
OF
WORDS
METHOD
ARE
TOO
BAD
SO
THAT
THERE
ARE
NO
VERIFIED
IMAGES
WITH
WHICH
TO
QUERY
EXPAND
OUR
METHOD
IS
UNABLE
TO
IMPROVE
THE
PERFORMANCE
THIS
OCCURS
FOR
TWO
OF
THE
MAGDALEN
QUERIES
NOTE
THAT
SINCE
WE
ARE
MEASURING
THE
MAP
OVER
ALL
QUERIES
THE
AVERAGE
RESULT
IN
TABLE
IS
LOWERED
BY
SUCH
NON
EXPANDABLE
QUERIES
TO
ELIMINATE
THE
AVERAGING
EFFECT
WE
STUDY
EACH
QUERY
INDEPENDENTLY
FIGURE
COMPARES
TWO
HISTOGRAMS
OF
AP
FOR
EACH
OF
THE
QUERIES
ON
EXPERIMENT
THE
TOP
HISTOGRAM
DISPLAYS
RESULTS
OF
THE
ORIGINAL
QUERY
AND
THE
BOTTOM
RESULTS
OF
THE
BEST
QUERY
EXPANSION
METHOD
THE
PLOT
CLEARLY
SHOWS
THE
SIGNIFICANT
IMPROVEMENT
BROUGHT
BY
THE
QUERY
EXPANSION
THE
PERFORMANCE
OF
THE
SYSTEM
IS
HURT
BY
INCORRECTLY
VER
IFIED
RETRIEVALS
NO
VERIFICATION
METHOD
IS
PERFECT
ESPECIALLY
WHEN
ONE
HAS
TO
DEAL
WITH
PARTIAL
OCCLUSIONS
SOME
OF
THE
FALSE
POSITIVES
ARE
INDEED
DIFFICULT
TO
DISTINGUISH
EVEN
FOR
A
HUMAN
AS
DEMONSTRATED
IN
FIGURE
FIGURE
SHOWS
SOME
EXAMPLE
IMAGES
RETURNED
BY
OUR
METHOD
WHICH
WERE
NOT
FOUND
IN
THE
ORIGINAL
BAG
OF
WORDS
QUERY
AFTER
QUERY
EXPANSION
WE
GET
MANY
MORE
EXAMPLES
FIGURE
DEMONSTRATING
THE
PERFORMANCE
OF
THE
METHOD
ON
A
NUMBER
OF
DIFFERENT
QUERIES
THE
IMAGE
TO
THE
LEFT
SHOWS
THE
ORIGINAL
QUERY
IMAGE
THE
FOUR
IMAGES
IN
THE
MIDDLE
SHOW
THE
FIRST
FOUR
RESULTS
RETURNED
BY
THE
ORIGINAL
QUERY
BEFORE
QUERY
EXPANSION
THE
IMAGES
TO
THE
RIGHT
SHOW
TRUE
POSITIVE
IMAGES
RETURNED
AFTER
QUERY
EXPANSION
WHICH
WERE
NOT
FOUND
FROM
THE
BAG
OF
WORDS
METHOD
OF
THE
OBJECT
SOME
OF
WHICH
WOULD
BE
EXTREMELY
CHALLENG
ING
TO
THE
TRADITIONAL
METHOD
WITH
IN
SOME
CASES
VERY
HIGH
LEVELS
OF
OCCLUSION
OR
LARGE
SCALE
CHANGES
METHOD
COMPARISON
WE
NOW
COMPARE
OUR
DIFFERENT
QUERY
EXPANSION
METHODS
REFERRING
TO
TABLE
FOR
THE
RELATIVE
PERFORMANCES
QUERY
EXPANSION
BASELINE
QEB
THIS
METHOD
DOES
WORSE
THAN
NOT
USING
QUERY
EXPANSION
AT
ALL
AS
EXPECTED
BLINDLY
CHOOSING
THE
TOP
M
DOCUMENTS
FOR
EXPANSION
DOES
NOT
TAKE
INTO
ACCOUNT
WHETHER
OR
NOT
ANY
OF
THE
TOP
M
ARE
CORRECT
SO
THE
METHOD
SUFFERS
FROM
SERIOUS
DRIFT
WE
CAN
SEE
THIS
BY
NOTING
THAT
QUERIES
WHICH
RETURN
LOTS
OF
TRUE
POSITIVES
FROM
THE
INITIAL
QUERY
SUCH
AS
RADCLIFFE
CAMERA
AND
HERT
FORD
PERFORM
MUCH
BETTER
THAN
THOSE
WITH
FEWER
INITIAL
TRUE
POSITIVES
SUCH
AS
ASHMOLEAN
AND
KEBLE
TRANSITIVE
CLOSURE
TRC
THE
METHOD
USES
A
SINGLE
IMAGE
TO
QUERY
WITH
EACH
TIME
SINCE
BOTH
THE
FEATURE
DETECTION
AND
VOCABULARY
GENERATION
ARE
NOISY
PROCESSES
TRANSITIVE
CLO
SURE
HAS
LOWER
PERFORMANCE
THAN
METHODS
CONSTRUCTING
LA
TENT
IMAGE
REPRESENTATION
FROM
SEVERAL
IMAGES
THIS
METHOD
THE
SLOWEST
SINCE
IT
GENERATES
BY
FAR
THE
HIGHEST
NUMBER
OF
QUERY
REISSUES
AVERAGE
QUERY
EXPANSION
AVG
THIS
METHOD
PERFORMS
SIGNIFICANTLY
BETTER
THAN
JUST
USING
THE
RESULTS
FROM
THE
STAN
DARD
BAG
OF
WORDS
METHODS
SCORING
ON
AVERAGE
AS
OPPOSED
TO
IN
THE
CASE
OF
ADDITIONALLY
THE
METHOD
IMPROVES
THE
RESULTS
FOR
EVERY
QUERY
IN
OUR
SCORING
THIS
METHOD
PERFORMS
SO
MUCH
BETTER
MAINLY
BECAUSE
THE
SPATIAL
VERIFICATION
ALLOWS
US
TO
EXCLUDE
FALSE
POSITIVES
FROM
RESULTS
TO
THE
ORIGINAL
QUERY
PREVENTING
THE
DRIFT
WHICH
RUINED
THE
BASELINE
METHOD
RECURSIVE
AVERAGE
QUERY
EXPANSION
REC
THIS
METHOD
IMPROVES
ON
THE
AVG
METHOD
BY
RECURSIVELY
GENERATING
AND
QUERYING
THE
SYSTEM
WITH
SPATIALLY
VERIFIED
RESULTS
BY
QUERYING
RECURSIVELY
WE
CAN
MORE
THOROUGHLY
EXPLORE
THE
SPACE
OF
OBJECT
FEATURES
GIVING
US
INSTANCES
OF
THE
OBJECT
WHOSE
VISUAL
APPEARANCE
CAN
DIFFER
GREATLY
FROM
THE
ORIGI
NAL
QUERY
RESOLUTION
EXPANSION
SCA
THE
RESOLUTION
EXPANSION
METHOD
PERFORMS
THE
BEST
ON
OUR
DATA
BY
GROUPING
RE
SULTS
BASED
ON
THE
RESOLUTION
OF
THE
OBJECT
OF
INTEREST
WE
QUERY
EXPAND
USING
ONLY
FEATURES
WHICH
RELIABLY
FIRE
ON
THE
OBJECT
AT
A
PARTICULAR
RESOLUTION
THIS
PREVENTS
US
FROM
IN
CLUDING
FEATURES
WHICH
FIRE
AT
DIFFERENT
SCALES
WHICH
CAN
RAISE
THE
CHANCE
OF
A
FALSE
POSITIVE
IMAGE
BEING
VERIFIED
THIS
METHOD
GETS
AN
MAP
SCORE
OF
ON
AND
ON
AND
MOST
OF
THE
QUERIES
EXHIBIT
NEAR
TOTAL
RE
CALL
SEE
FIGURE
THE
PERCENTAGE
IS
BROUGHT
DOWN
BY
A
FEW
QUERIES
WHICH
DUE
TO
THE
INITIAL
BAD
PERFORMANCE
OF
THE
BAG
OF
WORDS
METHOD
ARE
UNABLE
TO
BE
SUCCESSFULLY
EX
PANDED
SUCH
QUERIES
LIE
ON
THE
LEFT
HAND
SIDE
OF
THE
LOWER
HISTOGRAM
IN
FIGURE
ALSO
NOTE
THAT
THE
MERGING
STRATEGY
DOES
NOT
RANK
ALL
IM
AGES
IN
THE
DATABASE
THIS
CAN
BE
OBSERVED
ON
THE
PRECISION
RECALL
FIGURE
WHERE
THE
CURVE
DOES
NOT
REACH
THE
RIGHT
SIDE
OF
THE
PLOT
DATASET
COMPARISON
VS
THE
AVERAGE
PRECISION
MEASURE
IS
DESIGNED
TO
CAPTURE
QUALITY
OF
RETRIEVAL
WITH
STRONG
EMPHASIS
ON
THE
TOP
RANKED
RESULTS
NOTE
THAT
ADDITIONAL
NEGATIVE
IMAGES
CAN
ONLY
DE
CREASE
OR
LEAVE
UNCHANGED
THE
AVERAGE
PRECISION
MEASURE
IN
THE
BEST
CASE
IF
ALL
OF
THE
ADDITIONAL
NEGATIVE
IMAGES
WERE
CORRECTLY
CLASSIFIED
THEY
COULD
BE
APPENDED
AT
THE
TAIL
OF
THE
RESULTS
WHICH
WOULD
LEAVE
AVERAGE
PRECISION
UNCHANGED
HOWEVER
CORRECT
CLASSIFICATION
OF
ALL
IMAGES
RARELY
HAPPENS
OUR
EXPERIMENTS
SHOW
VARYING
DROP
OF
PERFORMANCE
AFTER
INCREASING
THE
SIZE
OF
THE
DATABASE
NEGATIVE
IMAGES
TIMES
THE
DECREASE
IN
PERFORMANCE
RELATIVE
AND
ABSOLUTE
IS
LOWER
FOR
QUERY
EXPANSION
METHODS
THAN
FOR
THE
ORIGINAL
METHOD
DISCUSSION
GIVEN
THE
SET
OF
RETRIEVED
IMAGES
WHICH
OFTEN
COVER
A
VARIETY
OF
VIEWPOINTS
WE
NOW
HAVE
THE
POTENTIAL
TO
CON
STRUCT
MUCH
RICHER
LATENT
FEATURE
MODELS
OF
THE
QUERY
RE
GION
MUCH
PREVIOUS
WORK
FERRARI
ET
AL
LOWE
ROTHGANGER
ET
AL
HAS
EXPLORED
COMBINING
FEATURES
FROM
MULTIPLE
VIEWS
AND
THIS
CAN
NOW
BE
HARNESSED
FOR
LA
TENT
MODEL
CONSTRUCTION
IT
IS
ALSO
POSSIBLE
TO
MOVE
FROM
FEATURES
TO
SURFACES
WHERE
THE
LATENT
MODEL
WOULD
CONSIST
OF
A
TEXTURED
SURFACE
RECONSTRUCTION
WHICH
CAN
BE
BUILT
USING
STANDARD
METHODS
WE
VIEW
IMAGE
RETRIEVAL
SYSTEMS
SUCH
AS
VIDEO
GOOGLE
AS
ONE
EXTREME
AND
THE
PHOTO
TOURISM
SYSTEM
AS
ANOTHER
OF
EXAMPLES
DRAWN
FROM
A
SPECTRUM
OF
POSSIBLE
IMAGE
BASED
OBJECT
RETRIEVAL
TECHNIQUES
THE
COMMON
FEA
TURE
UNIFYING
THIS
FAMILY
OF
METHODS
IS
THAT
THEY
CONSTRUCT
A
LATENT
MODEL
OF
THE
QUERY
OBJECT
WITH
THE
AID
OF
THE
IMAGE
CORPUS
AND
RETURN
TO
THE
USER
SOME
REPRESENTATION
OF
THAT
LATENT
MODEL
THE
WORK
OF
THIS
PAPER
DEFINES
ANOTHER
POINT
ON
THE
SPECTRUM
ACKNOWLEDGEMENTS
WE
ARE
GRATEFUL
FOR
SUPPORT
FROM
AN
EPSRC
PLATFORM
GRANT
THE
ROYAL
ACADEMY
OF
ENGINEERING
EU
PROJECT
CLASS
AND
MICROSOFT
CLASSIFICATION
EXPERIMENTS
JANUARY
VISUAL
RECOGNITION
BHAVIN
MODI
BAG
OF
FEATURES
BAG
OF
FEATURES
OUTLINE
EXTRACT
FEATURES
LEARN
VISUAL
VOCABULARY
QUANTIZE
FEATURES
USING
VISUAL
VOCABULARY
REPRESENT
IMAGES
BY
FREQUENCIES
OF
VISUAL
WORDS
SLIDE
CREDITS
LI
FEI
FEI
BAG
OF
FEATURES
SUMMARY
WHAT
ABOUT
SPATIAL
INFORMATION
SLIDE
CREDITS
CORDELIA
SCHMID
BEYOND
BAG
OF
FEATURES
SLIDE
CREDITS
LI
FEI
FEI
SPATIAL
PYRAMID
MATCHING
IMAGE
REPRESENTATION
SLIDE
CREDITS
LI
FEI
FEI
KERNEL
FUNCTION
HISTOGRAM
INTERSECTION
FUNCTION
FINAL
KERNEL
IS
THE
SUM
OF
THE
SEPARATE
CHANNELS
SPATIAL
PYRAMID
VECTOR
DIMENSIONS
WEAKNESS
OF
THE
MODEL
EXPERIMENTS
CONDUCTED
DATASETS
USED
SCENE
CALTECH
AND
GRAZ
STRONG
FEATURES
SIFT
DESCRIPTORS
OF
PIXEL
PATCHES
COMPUTED
OVER
A
GRID
WITH
SPACING
OF
PIXELS
WEAK
FEATURES
ORIENTED
EDGE
POINTS
I
E
POINTS
WHOSE
GRADIENT
MAGNITUDE
IN
A
GIVEN
DIRECTION
EXCEEDS
A
MINIMUM
THRESHOLD
DICTIONARY
SIZE
AND
LEVELS
ARE
TESTED
FOR
DIFFERENT
VALUES
M
AND
L
NOT
IN
ALL
CASES
SCENE
ONE
OF
THE
MOST
COMPLETE
SCENE
CATEGORIES
AT
THE
TIME
EACH
CATEGORY
HAS
TO
IMAGES
CONCLUSIONS
MADE
USING
ALL
LEVELS
TOGETHER
CONFERS
A
STATISTICALLY
SIGNIFICANT
BENEFIT
FOR
STRONG
FEATURES
SINGLE
LEVEL
PERFORMANCE
DROPS
AS
WE
GO
FROM
L
TO
L
WHILE
WEAK
FEATURES
IMPROVE
PERFORMANCE
AT
L
AND
L
IS
ALMOST
EQUIVALENT
MOVING
FROM
M
TO
M
HAS
A
VERY
SMALL
PERFORMANCE
INCREASE
PERFORMS
BETTER
WITH
CLASSES
THAN
CLASSES
AT
L
CALTECH
HAS
GEOMETRIC
STABILITY
AND
LACK
OF
CLUTTER
CONTAINS
TO
IMAGES
PER
CATEGORY
SLIDE
CREDITS
CORDELIA
SCHMID
CALTECH
CONCLUSIONS
PRONE
TO
INTRA
CLASS
VARIATIONS
RESULTS
SHOWN
FOR
M
M
SHOWS
NO
SIGNIFICANT
IMPROVEMENT
BEST
PERFORMANCE
WITH
L
M
WITH
STRONG
FEATURES
BEST
CLASSIFICATION
RATE
FOR
SCENE
WAS
AND
IT
IS
FOR
CALTECH
GRAZ
DATASET
HAS
OBJECT
CATEGORIES
BIKES
AND
PEOPLE
WITH
HEAVY
CLUTTER
AND
POSE
CHANGES
M
L
AND
L
FOR
STRONG
FEATURES
CONCLUSIONS
IMPROVEMENT
FOR
L
TO
L
IS
RELATIVELY
SMALL
SINCE
IT
IS
DIFFICULT
TO
FIND
USEFUL
GLOBAL
FEATURES
PERFORMANCE
AT
IS
HIGHER
THAN
SCENE
AND
CALTECH
NEW
EXPERIMENTS
CONDUCTED
USED
THE
CALTECH
DATASET
CATEGORIES
TO
CHECK
IF
PERFORMANCE
DECREASES
ON
INCREASING
THE
NUMBER
OF
CLASSES
VARY
THE
SIZE
OF
DICTIONARY
M
TO
SEE
THE
EFFECTS
ON
ACCURACY
VALUES
USED
M
AND
IS
SAID
TO
BE
THE
OPTIMAL
CONTROL
PARAMETERS
PRESENT
DEFAULT
SHOWN
IMAGE
SIZE
GRID
SPACING
PATCH
SIZE
DICTIONARY
SIZE
NUMBER
OF
TEXTON
IMAGES
PYRAMID
LEVELS
WHY
CALTECH
CALTECH
WEAKNESSES
THE
DATASET
IS
TOO
CLEAN
IMAGES
ARE
VERY
UNIFORM
IN
PRESENTATION
ALIGNED
FROM
LEFT
TO
RIGHT
AND
USUALLY
NOT
OCCLUDED
LIMITED
NUMBER
OF
CATEGORIES
SOME
CATEGORIES
CONTAIN
FEW
IMAGES
CERTAIN
CATEGORIES
ARE
NOT
REPRESENTED
AS
WELL
AS
OTHERS
CONTAINING
AS
FEW
AS
IMAGES
FOR
EXAMPLE
BINOCULAR
WILD
CAT
CALTECH
IS
ANOTHER
IMAGE
DATASET
CREATED
AT
THE
CALIFORNIA
INSTITUTE
OF
TECHNOLOGY
IN
A
SUCCESSOR
TO
CALTECH
IT
IS
INTENDED
TO
ADDRESS
SOME
OF
THE
WEAKNESSES
INHERENT
TO
CALTECH
005_0140
SLIDE
CREDITS
VISION
CALTECH
EDU
EXPERIMENT
RESULTS
DATASET
CALTECH
MULTIPLE
CATEGORIES
CONSIDERED
TRAINING
IMAGES
PER
CATEGORY
TEST
IMAGES
PER
CATEGORY
L
M
EXPERIMENT
SAME
AS
ABOVE
BUT
CATEGORIES
CONSIDERED
M
M
M
NUMBER
OF
CATEGORIES
M
DICTIONARY
SIZE
PROBLEMS
AS
WE
CAN
SEE
THE
ACCURACY
IS
VERY
LOW
WHICH
LEADS
TO
BELIEVE
THAT
THERE
IS
SOME
ERROR
IN
IMPLEMENTATION
SO
WE
TRY
TO
FIGURE
OUT
THE
REASON
BY
PERFORMING
THREE
DEBUGGING
STEPS
ALL
DEBUGGING
IS
DONE
ON
THE
CATECH
DATASET
FOR
CATEGORIES
M
L
NO
OF
TRAINING
IMAGES
PER
CATEGORY
NO
OF
TESTING
IMAGES
PER
CATEGORY
ACCURACY
ON
TEST
SET
ACCURACY
ON
TRAIN
SET
COMPUTE
THE
BIG
KERNEL
USING
THE
INBUILT
LINEAR
KERNEL
AND
RBF
KERNEL
CALCULATING
KERNEL
MEANS
VALUES
DEBUGGING
RESULTS
CALCULATING
THE
BIG
KERNEL
ACCURACY
NO
CHANGE
USING
A
LINEAR
OR
RBF
KERNEL
ON
THE
TEST
DATA
AND
DOING
A
SANITY
CHECK
ON
THE
TRAINING
DATA
CALCULATING
THE
RATIO
OF
THE
MEAN
K
SAMPLE
OTHER
SAMPLES
FROM
SAME
CLASS
VALUES
AND
THE
MEAN
K
SAMPLE
SAMPLES
FROM
DIFFERENT
CLASSES
RATIO
VALUES
FOR
BOTH
THE
TRAIN
AND
TEST
KERNELS
DEBUGGING
II
WE
CHECK
THE
PREDICTED
LABELS
ON
THE
TEST
SET
TO
SEE
THE
WHICH
CATEGORY
WAS
ASSIGNED
TO
MAJORITY
OF
THE
IMAGES
WE
SEE
CATEGORY
THAT
BASKETBALL
HOOPS
AND
DRINKING
STRAW
HAVE
MORE
THAN
IMAGES
ASSIGNED
TO
THESE
TWO
CATEGORIES
EVALUATION
ON
OTHER
DATASETS
SLIDE
CREDITS
CORDELIA
SCHMID
SUMMARY
DISCUSSION
SPATIAL
PYRAMID
REPRESENTATION
APPEARANCE
OF
LOCAL
IMAGE
PATCHES
COARSE
GLOBAL
POSITION
INFORMATION
SUBSTANTIAL
IMPROVEMENT
OVER
BAG
OF
FEATURES
DEPENDS
ON
THE
SIMILARITY
OF
IMAGE
LAYOUT
FUTURE
WORK
DONE
PACKING
MORE
INFORMATION
IN
THE
PYRAMID
BOSCH
ET
AL
USED
DESCRIPTORS
PHOW
AND
PHOG
GERMETT
ET
AL
KERNEL
CODEBOOK
USES
A
GAUSSIAN
KERNEL
OVER
EVERY
CENTROID
W
EVERY
BIN
GETS
IF
DESCRIPTOR
RI
IS
ASSIGNED
NEAREST
TO
ITS
CENTROID
W
EVERY
DESCRIPTOR
CONTRIBUTES
SOME
INFORMATION
TO
EVERY
BIN
DEPENDING
ON
Σ
SHENGYE
YAN
ET
AL
BEYOND
SPATIAL
PYRAMID
USES
A
TWO
LEVEL
FEATURE
EXTRACTION
METHOD
USING
ENCODING
AND
POOLING
PROCEDURES
ON
THE
WINDOW
BASED
FEATURES
TO
ACQUIRE
NEW
IMAGE
FEATURES
THANK
YOU
PEEKABOOM
A
GAME
FOR
LOCATING
OBJECTS
IN
IMAGES
LUIS
VON
AHN
RUORAN
LIU
AND
MANUEL
BLUM
COMPUTER
SCIENCE
DEPARTMENT
CARNEGIE
MELLON
UNIVERSITY
PRESENTED
BY
NILS
MURRUGARRA
UNIVERSITY
OF
PITTSBURGH
OBJECT
LOCATION
IN
IMAGES
GIVEN
AN
IMAGE
DETERMINE
WHAT
OBJECTS
ARE
PRESENT
IN
THE
IMAGE
AND
LOCATE
THEM
WOMAN
MAN
UMBRELLA
TREE
SAILBOAT
DOG
LET
USE
HUMAN
POWER
MATH
IS
HARD
LET
GO
SHOPPING
BARBIE
ON
SIMILAR
LINE
OF
THINKING
PROGRAMMING
COMPUTERS
TO
LOCATE
OBJECTS
IN
IMAGES
IS
HARD
SO
LET
NOT
THINK
ABOUT
THAT
INSTEAD
HUMANS
CAN
DO
THE
WORK
FOR
US
PROBLEMS
WAIT
HUMAN
PROBABLY
WANTS
ENJOYMENT
THEY
WANT
TO
HAVE
A
GOOD
TIME
INCENTIVES
THEY
WANT
SOMETHING
IN
RETURN
HOW
TO
ADDRESS
THEM
A
GAME
PEOPLE
CAN
DO
THE
WORK
FOR
US
BY
PLAYING
A
GAME
MANY
QUESTIONS
APPEARS
WHAT
WILL
BE
THE
CORE
IDEA
OF
THE
GAME
HOW
DO
WE
COLLECT
DATA
HOW
DO
WE
ENSURE
THE
QUALITY
OF
THE
DATA
AN
EARLIER
IDEA
LUIS
VON
AHN
ESP
GAME
CORE
IDEA
TWO
PLAYERS
WITHOUT
COMMUNICATION
WATCH
A
PARTICULAR
IMAGE
EACH
ONE
TRIES
TO
GUESS
WHAT
THE
OTHER
IS
THINKING
ABOUT
THE
IMAGE
IF
THEY
AGREE
ON
A
WORD
THE
GAME
MOVES
ON
AND
INCREASES
BOTH
PLAYERS
SCORES
A
SAMPLE
RUN
PLAYER
GUESSES
PANTS
LADY
PLAYER
GUESSES
WOMAN
SHIRT
GIRL
MODEL
SERVER
AGREED
MODEL
WHY
ESP
WORKS
DATA
COLLECTION
AND
QUALITY
WHEN
TWO
PLAYERS
AGREES
SAY
WHAT
IT
IS
IN
OTHER
WORDS
THIS
IS
A
LABEL
TO
THE
SHOWN
IMAGE
THE
FACT
THAT
TWO
PLAYERS
AGREE
ON
A
LABEL
MEANS
THAT
THIS
LABEL
HAS
A
HIGH
QUALITY
LIMITATIONS
OF
ESP
THE
ESP
GAME
CAN
LABEL
IMAGES
WHAT
IN
THEM
BUT
IT
CANNOT
WHERE
THE
OBJECTS
ARE
DETERMINE
THE
WAY
IN
WHICH
THE
OBJECT
APPEARS
DOES
THE
LABEL
CAR
REFER
TO
THE
TEXT
CAR
OR
AN
ACTUAL
CAR
IN
THE
IMAGE
COMPLETING
THE
IMAGE
CYCLE
UNLABELED
IMAGES
ESP
GAME
SERVER
LABELED
IMAGES
A
NEW
IDEA
PEEKABOOM
CORE
IDEA
TWO
PLAYERS
ARE
ASSIGNED
THE
ROLES
OF
REVEALER
BOOM
AND
GUESSER
PEEK
THE
REVEALER
SEES
AN
IMAGE
WITH
A
LABEL
THE
GUESSER
SEES
NOTHING
THE
REVEALER
SHOWS
THE
GUESSER
PARTS
OF
THE
IMAGE
IF
THE
GUESSER
GUESSES
CORRECTLY
THE
GAME
CONTINUES
WITH
NEW
IMAGES
PEEKABOOM
INTERFACE
PEEK
GUESSER
BOOM
REVEALER
STATEMENT
OF
PURPOSE
THE
AUTHORS
WOULD
LIKE
TO
COLLECT
DATA
OF
A
LOT
IMAGES
AUTOMATICALLY
THE
AUTHORS
HOPE
THAT
THESE
DATA
CAN
BE
USED
TO
TRAIN
COMPUTER
VISION
ALGORITHMS
LET
DO
AN
EXAMPLE
THE
REVEALER
CLICKS
ON
PARTS
OF
THE
IMAGE
AND
SHOWS
THEM
TO
THE
GUESSER
THE
GUESSER
GUESSES
FLOWER
PETAL
BUTTERFLY
SERVER
CORRECT
BUTTERFLY
LET
PLAY
HTTPS
WGCM
FEATURE
YOUTU
BE
T
WHY
PEEKABOOM
WORKS
TO
HELP
AS
MUCH
AS
POSSIBLE
THE
GUESSER
TO
GUESS
CORRECTLY
THE
REVEALER
LOCATES
RELEVANT
PARTS
OF
THE
OBJECT
IN
THE
IMAGE
BUT
WAIT
THERE
MORE
PEEKABOOM
NOT
ONLY
LOCATES
OBJECTS
IT
GIVES
THE
CONTEXT
NECESSARY
TO
IDENTIFY
THEM
IT
CLASSIFIES
THE
IMAGE
AS
TEXT
NOUN
OR
VERB
USING
THE
HINTS
OPTION
LET
LEARN
MORE
ABOUT
THESE
FUNCTIONALITIES
OBJECT
CONTEXT
THE
LABEL
NOSE
PINGS
HELP
SEPARATE
THE
CONTEXT
OF
OBJECT
WITH
THE
OBJECT
ITSELF
THEY
HELP
THE
GUESSER
DISTINGUISH
NOSE
FROM
OTHER
POSSIBLY
CORRECT
LABELS
LIKE
ELEPHANT
AND
EAR
HINTS
THE
ROLE
OF
HINTS
HOW
TO
INVOLVE
MORE
PARTICIPANTS
IN
THE
GAME
GAME
POINTS
GAME
POINTS
PEEK
GUESSES
THE
CORRECT
WORD
POINTS
ARE
NOT
SUBTRACTED
FOR
PASSING
PEEK
GUESSES
THE
CORRECT
WORD
AND
BOOM
HAD
USED
A
HINT
EXTRA
POINTS
ARE
NOT
GIVEN
FOR
USAGE
OF
THE
HOT
COLD
BUTTONS
BONUS
POINTS
OBTAIN
UP
TO
GET
POINTS
POINTS
DEPEND
ON
HOW
FAR
ONE
PARTICIPANT
CLICK
IS
FROM
HIS
HER
PARTNER
CORRESPONDING
CLICK
LF
THE
OBJECT
ARE
NOT
IN
THE
IMAGE
PLAYERS
CAN
PASS
COLLECTING
IMAGE
METADATA
DATA
COLLECTION
DATA
FROM
AREA
REVEALED
WHICH
PIXELS
ARE
NECESSARY
TO
GUESS
THE
WORD
DATA
FROM
HINTS
WHAT
IS
THE
RELATION
BETWEEN
WORD
AND
IMAGE
DATA
FROM
PINGS
WHICH
PIXELS
ARE
INSIDE
THE
OBJECT
DATA
FROM
SEQUENCE
OF
BOOM
CLICKS
WHAT
ARE
THE
MOST
RELEVANT
ASPECTS
OF
THE
OBJECT
DATA
FROM
PASS
BUTTON
ELIMINATION
OF
POOR
DIFFICULT
IMAGE
WORD
PAIRS
CHEATING
DATA
QUALITY
WHY
TO
WORRIED
IF
THE
TWO
PLAYERS
CHEAT
ON
THE
GAME
THE
DATA
IS
NOT
RELIABLE
MULTIPLE
ANTI
CHEATING
MECHANISMS
TO
AVOID
MATCH
PARTICIPANTS
THAT
START
AT
THE
SAME
TIME
THE
PLAYER
QUEUE
TO
AVOID
GEOGRAPHICALLY
PROXIMITY
IP
ADDRESS
CHECKS
TO
AVOID
BOTS
BLACKLISTS
AFTER
CONSISTENT
FAILURE
ON
SEED
IMAGES
TO
AVOID
CHEATING
COMMUNICATION
LIMITED
FREEDOM
TO
ENTER
GUESSES
APPLICATIONS
IMPROVING
IMAGE
SEARCH
RESULTS
OBJECT
BOUNDING
BOXES
GIVEN
AN
IMAGE
CREATE
A
MATRIX
OF
FOR
EACH
CLICK
IN
ITS
SURROUNDING
AREA
RADIUS
PIXELS
ADD
TO
THE
MATRIX
POSITION
COMBINE
DIFFERENT
GAMES
FOR
THE
SAME
IMAGE
WORD
PAIR
APPLY
A
THRESHOLD
OF
AT
LEAST
PLAYERS
AGREE
CLUSTER
THE
PIXELS
TO
GET
BOUNDING
BOXES
USING
PING
DATA
FOR
POINTING
SELECT
A
RANDOM
PING
EVALUATION
IS
THIS
AN
EFFECTIVE
WAY
TO
COLLECT
DATA
YES
GAME
IS
ENJOYABLE
EACH
PERSON
PLAYED
AVERAGE
OF
IMAGES
THAT
MINUTES
PER
PERSON
IN
ONE
MONTH
USER
REVIEWS
USAGE
STATISTICS
AUGUST
SEPTEMBER
PEOPLE
AND
PIECES
OF
DATA
EVALUATION
ACCURACY
OF
COLLECTED
DATA
ACCURACY
OF
BOUNDING
BOXES
ARE
THEY
GOOD
COMPARED
TO
BOUNDING
BOXES
COLLECTED
IN
A
NON
GAME
SETUP
IT
WAS
PERFORMED
IN
IMAGE
WORD
NOUNS
PAIRS
GIVEN
A
WORD
FOUR
VOLUNTEERS
WERE
ASKED
TO
DRAW
A
BOUNDING
BOX
AROUND
THE
OBJECT
THAT
THE
WORD
REFERS
TO
AVERAGE
OVERLAP
STANDARD
DEVIATION
ACCURACY
OF
PINGS
IT
WAS
VERIFIED
IF
THE
PEEKABOOM
OBJECT
POINTERS
ARE
INDEED
INSIDE
THE
OBJECTS
GIVEN
A
POINTER
THREE
VOLUNTEER
DETERMINE
IF
IT
IS
INSIDE
THE
OBJECT
OR
NOT
OF
THE
POINTERS
WERE
INSIDE
THE
OBJECT
REFERRED
BY
THE
WORD
DISCUSSION
WHAT
ARE
SOME
DISADVANTAGES
WEAKNESSES
OF
PEEKABOOM
CAN
YOU
THINK
OF
ANY
OTHER
APPLICATIONS
OF
PEEKABOOM
CONCLUSION
PEEKABOOM
IS
AN
ENJOYABLE
GAME
TO
COLLECT
IMAGE
DATA
ACHIEVING
LOW
COSTS
ONE
GAME
SERVER
DATA
WITH
GOOD
QUALITY
ACCURATELY
LOCATE
OBJECTS
IN
IMAGES
LARGE
QUANTITY
OF
DATA
LOCATE
OBJECTS
IN
MILLIONS
OF
IMAGES
QUESTIONS
REFERENCES
VON
AHN
L
LIU
R
BLUM
M
APRIL
PEEKABOOM
A
GAME
FOR
LOCATING
OBJECTS
IN
IMAGES
IN
PROCEEDINGS
OF
THE
SIGCHI
CONFERENCE
ON
HUMAN
FACTORS
IN
COMPUTING
SYSTEMS
PP
ACM
SLIDES
VERSION
OF
PEEKABOOM
A
GAME
FOR
LOCATING
OBJECTS
IN
IMAGES
SOURCE
OBJECTS
IN
IMAGES
SLIDES
VERSION
OF
PEEKABOOM
A
GAME
FOR
LOCATING
OBJECTS
IN
IMAGES
SOURCE
SLIDES
VERSION
OF
PEEKABOOM
A
GAME
FOR
LOCATING
OBJECTS
IN
IMAGES
SOURCE
HTTP
CGIT
NUTN
EDU
TW
CGIT
PPTDL
PDF
SLIDES
VERSION
OF
PEEKABOOM
A
GAME
FOR
LOCATING
OBJECTS
IN
IMAGES
SOURCE
VIDEO
HUMAN
COMPUTATION
SOURCE
HTTPS
CROWDSOURCING
ANNOTATIONS
FOR
VISUAL
OBJECT
DETECTION
HAO
SU
JIA
DENG
LI
FEI
FEI
COMPUTER
SCIENCE
DEPARTMENT
STANFORD
UNIVERSITY
PRESENTED
BY
NILS
MURRUGARRA
UNIVERSITY
OF
PITTSBURGH
MOTIVATION
MOTIVATION
A
LARGE
QUANTITY
OF
PRECISE
BOUNDING
BOXES
ARE
REQUIRED
TO
LEARN
GOOD
OBJECT
DETECTORS
GOAL
CROWD
SOURCE
BOUNDING
BOXES
ANNOTATIONS
CHALLENGES
CONTROL
THE
DATA
QUALITY
WITH
MINIMAL
COST
METHOD
OVERVIEW
QUALIFICATION
TEST
QUALIFICATION
CONTROL
GOOD
BOUNDING
BOXES
BAD
BOUNDING
BOXES
METHOD
DRAWING
TASK
METHOD
DRAWING
TASK
METHOD
DRAWING
TASK
METHOD
QUALITY
VERIFICATION
TASK
GOOD
ANNOTATION
BAD
ANNOTATION
METHOD
COVERAGE
VERIFICATION
TASK
EVALUATION
DATASET
IMAGES
WERE
SELECTED
OVER
CATEGORIES
ON
THE
IMAGENET
DATABASE
OVERALL
QUALITY
IT
WAS
MANUALLY
INSPECTED
OF
IMAGES
ARE
COMPLETELY
COVERED
WITH
BOUNDING
BOXES
THE
REMAINING
ARE
DIFFICULT
CASES
ARE
ACCURATE
TIGHT
AS
POSSIBLE
OVERALL
COST
THE
PROPOSED
METHOD
IS
CHEAPER
CONSENSUS
IS
MORE
EXPENSIVE
EVALUATION
QUALITY
CONTROL
DRAWING
TASK
ACCEPTANCE
RATIO
QUALITY
VERIFICATION
TASK
IT
WAS
EMPLOYED
A
GOLD
STANDARD
VALIDATION
IMAGES
ACCEPTANCE
RATIO
COVERAGE
VERIFICATION
TASK
IT
WAS
EMPLOYED
A
GOLD
STANDARD
VALIDATION
IMAGES
ACCEPTANCE
RATIO
EFFECTIVENESS
OF
WORKER
TRAINING
CONCLUSION
IT
WAS
PRESENTED
A
METHOD
THAT
COLLECTS
BOUNDING
BOXES
ANNOTATION
USING
CROWDSOURCING
IT
IS
COMPOSED
BY
TASKS
DRAWING
TASK
QUALITY
VERIFICATION
TASK
COVERAGE
VERIFICATION
TASK
IT
ACHIEVES
HIGH
QUALITY
DATA
WITH
LOW
COST
BEYOND
BAGS
OF
FEATURES
SPATIAL
PYRAMID
MATCHING
FOR
RECOGNIZING
NATURAL
SCENE
CATEGORIES
SVETLANA
INSTITUTE
UNIVERSITY
OF
ILLINOIS
CORDELIA
RHOˆNE
ALPES
MONTBONNOT
FRANCE
JEAN
NORMALE
SUPE
RIEURE
PARIS
FRANCE
ABSTRACT
THIS
PAPER
PRESENTS
A
METHOD
FOR
RECOGNIZING
SCENE
CAT
EGORIES
BASED
ON
APPROXIMATE
GLOBAL
GEOMETRIC
CORRESPON
DENCE
THIS
TECHNIQUE
WORKS
BY
PARTITIONING
THE
IMAGE
INTO
INCREASINGLY
FINE
SUB
REGIONS
AND
COMPUTING
HISTOGRAMS
OF
LOCAL
FEATURES
FOUND
INSIDE
EACH
SUB
REGION
THE
RESULT
ING
SPATIAL
PYRAMID
IS
A
SIMPLE
AND
COMPUTATIONALLY
EFFI
CIENT
EXTENSION
OF
AN
ORDERLESS
BAG
OF
FEATURES
IMAGE
REP
RESENTATION
AND
IT
SHOWS
SIGNIFICANTLY
IMPROVED
PERFOR
MANCE
ON
CHALLENGING
SCENE
CATEGORIZATION
TASKS
SPECIFI
CALLY
OUR
PROPOSED
METHOD
EXCEEDS
THE
STATE
OF
THE
ART
ON
THE
CALTECH
DATABASE
AND
ACHIEVES
HIGH
ACCURACY
ON
A
LARGE
DATABASE
OF
FIFTEEN
NATURAL
SCENE
CATEGORIES
THE
SPA
TIAL
PYRAMID
FRAMEWORK
ALSO
OFFERS
INSIGHTS
INTO
THE
SUCCESS
OF
SEVERAL
RECENTLY
PROPOSED
IMAGE
DESCRIPTIONS
INCLUDING
TORRALBA
GIST
AND
LOWE
SIFT
DESCRIPTORS
INTRODUCTION
IN
THIS
PAPER
WE
CONSIDER
THE
PROBLEM
OF
RECOGNIZING
THE
SEMANTIC
CATEGORY
OF
AN
IMAGE
FOR
EXAMPLE
WE
MAY
WANT
TO
CLASSIFY
A
PHOTOGRAPH
AS
DEPICTING
A
SCENE
FOREST
STREET
OFFICE
ETC
OR
AS
CONTAINING
A
CERTAIN
OBJECT
OF
IN
TEREST
FOR
SUCH
WHOLE
IMAGE
CATEGORIZATION
TASKS
BAG
OF
FEATURES
METHODS
WHICH
REPRESENT
AN
IMAGE
AS
AN
ORDERLESS
COLLECTION
OF
LOCAL
FEATURES
HAVE
RECENTLY
DEMONSTRATED
IM
PRESSIVE
LEVELS
OF
PERFORMANCE
HOWEVER
BECAUSE
THESE
METHODS
DISREGARD
ALL
INFORMATION
ABOUT
THE
SPATIAL
LAYOUT
OF
THE
FEATURES
THEY
HAVE
SEVERELY
LIMITED
DE
SCRIPTIVE
ABILITY
IN
PARTICULAR
THEY
ARE
INCAPABLE
OF
CAPTUR
ING
SHAPE
OR
OF
SEGMENTING
AN
OBJECT
FROM
ITS
BACKGROUND
UNFORTUNATELY
OVERCOMING
THESE
LIMITATIONS
TO
BUILD
EFFEC
TIVE
STRUCTURAL
OBJECT
DESCRIPTIONS
HAS
PROVEN
TO
BE
QUITE
CHALLENGING
ESPECIALLY
WHEN
THE
RECOGNITION
SYSTEM
MUST
BE
MADE
TO
WORK
IN
THE
PRESENCE
OF
HEAVY
CLUTTER
OCCLU
SION
OR
LARGE
VIEWPOINT
CHANGES
APPROACHES
BASED
ON
GENERATIVE
PART
MODELS
AND
GEOMETRIC
CORRESPONDENCE
SEARCH
ACHIEVE
ROBUSTNESS
AT
SIGNIFICANT
COMPUTA
TIONAL
EXPENSE
A
MORE
EFFICIENT
APPROACH
IS
TO
AUGMENT
A
BASIC
BAG
OF
FEATURES
REPRESENTATION
WITH
PAIRWISE
RELATIONS
BETWEEN
NEIGHBORING
LOCAL
FEATURES
BUT
EXISTING
IMPLEMEN
TATIONS
OF
THIS
IDEA
HAVE
YIELDED
INCONCLUSIVE
RE
SULTS
ONE
OTHER
STRATEGY
FOR
INCREASING
ROBUSTNESS
TO
GEO
METRIC
DEFORMATIONS
IS
TO
INCREASE
THE
LEVEL
OF
INVARIANCE
OF
LOCAL
FEATURES
E
G
BY
USING
AFFINE
INVARIANT
DETECTORS
BUT
A
RECENT
LARGE
SCALE
EVALUATION
SUGGESTS
THAT
THIS
STRAT
EGY
USUALLY
DOES
NOT
PAY
OFF
THOUGH
WE
REMAIN
SYMPATHETIC
TO
THE
GOAL
OF
DEVELOP
ING
ROBUST
AND
GEOMETRICALLY
INVARIANT
STRUCTURAL
OBJECT
REP
RESENTATIONS
WE
PROPOSE
IN
THIS
PAPER
TO
REVISIT
GLOBAL
NON
INVARIANT
REPRESENTATIONS
BASED
ON
AGGREGATING
STATIS
TICS
OF
LOCAL
FEATURES
OVER
FIXED
SUBREGIONS
WE
INTRODUCE
A
KERNEL
BASED
RECOGNITION
METHOD
THAT
WORKS
BY
COMPUTING
ROUGH
GEOMETRIC
CORRESPONDENCE
ON
A
GLOBAL
SCALE
USING
AN
EFFICIENT
APPROXIMATION
TECHNIQUE
ADAPTED
FROM
THE
PYRAMID
MATCHING
SCHEME
OF
GRAUMAN
AND
DARRELL
OUR
METHOD
INVOLVES
REPEATEDLY
SUBDIVIDING
THE
IMAGE
AND
COMPUTING
HISTOGRAMS
OF
LOCAL
FEATURES
AT
INCREASINGLY
FINE
RESOLUTIONS
AS
SHOWN
BY
EXPERIMENTS
IN
SECTION
THIS
SIMPLE
OPER
ATION
SUFFICES
TO
SIGNIFICANTLY
IMPROVE
PERFORMANCE
OVER
A
BASIC
BAG
OF
FEATURES
REPRESENTATION
AND
EVEN
OVER
METH
ODS
BASED
ON
DETAILED
GEOMETRIC
CORRESPONDENCE
PREVIOUS
RESEARCH
HAS
SHOWN
THAT
STATISTICAL
PROPERTIES
OF
THE
SCENE
CONSIDERED
IN
A
HOLISTIC
FASHION
WITHOUT
ANY
ANAL
YSIS
OF
ITS
CONSTITUENT
OBJECTS
YIELD
A
RICH
SET
OF
CUES
TO
ITS
SEMANTIC
CATEGORY
OUR
OWN
EXPERIMENTS
CONFIRM
THAT
GLOBAL
REPRESENTATIONS
CAN
BE
SURPRISINGLY
EFFECTIVE
NOT
ONLY
FOR
IDENTIFYING
THE
OVERALL
SCENE
BUT
ALSO
FOR
CATEGORIZING
IMAGES
AS
CONTAINING
SPECIFIC
OBJECTS
EVEN
WHEN
THESE
OB
JECTS
ARE
EMBEDDED
IN
HEAVY
CLUTTER
AND
VARY
SIGNIFICANTLY
IN
POSE
AND
APPEARANCE
THIS
SAID
WE
DO
NOT
ADVOCATE
THE
DIRECT
USE
OF
A
GLOBAL
METHOD
FOR
OBJECT
RECOGNITION
EXCEPT
FOR
VERY
RESTRICTED
SORTS
OF
IMAGERY
INSTEAD
WE
ENVISION
A
SUBORDINATE
ROLE
FOR
THIS
METHOD
IT
MAY
BE
USED
TO
CAPTURE
THE
GIST
OF
AN
IMAGE
AND
TO
INFORM
THE
SUBSEQUENT
SEARCH
FOR
SPECIFIC
OBJECTS
E
G
IF
THE
IMAGE
BASED
ON
ITS
GLOBAL
DESCRIPTION
IS
LIKELY
TO
BE
A
HIGHWAY
WE
HAVE
A
HIGH
PROBABILITY
OF
FINDING
A
CAR
BUT
NOT
A
TOASTER
IN
ADDITION
THE
SIMPLICITY
AND
EFFICIENCY
OF
OUR
METHOD
IN
COMBINA
TION
WITH
ITS
TENDENCY
TO
YIELD
UNEXPECTEDLY
HIGH
RECOGNI
TION
RATES
ON
CHALLENGING
DATA
COULD
MAKE
IT
A
GOOD
BASE
LINE
FOR
CALIBRATING
NEW
DATASETS
AND
FOR
EVALUATING
MORE
SOPHISTICATED
RECOGNITION
APPROACHES
PREVIOUS
WORK
IN
COMPUTER
VISION
HISTOGRAMS
HAVE
A
LONG
HISTORY
AS
A
METHOD
FOR
IMAGE
DESCRIPTION
SEE
E
G
KOEN
DERINK
AND
VAN
DOORN
HAVE
GENERALIZED
HISTOGRAMS
TO
LOCALLY
ORDERLESS
IMAGES
OR
HISTOGRAM
VALUED
SCALE
SPACES
I
E
FOR
EACH
GAUSSIAN
APERTURE
AT
A
GIVEN
LOCATION
AND
SCALE
THE
LOCALLY
ORDERLESS
IMAGE
RETURNS
THE
HISTOGRAM
OF
IMAGE
FEATURES
AGGREGATED
OVER
THAT
APERTURE
OUR
SPATIAL
PYRAMID
APPROACH
CAN
BE
THOUGHT
OF
AS
AN
ALTERNATIVE
FOR
MULATION
OF
A
LOCALLY
ORDERLESS
IMAGE
WHERE
INSTEAD
OF
A
GAUSSIAN
SCALE
SPACE
OF
APERTURES
WE
DEFINE
A
FIXED
HIER
ARCHY
OF
RECTANGULAR
WINDOWS
KOENDERINK
AND
VAN
DOORN
HAVE
ARGUED
PERSUASIVELY
THAT
LOCALLY
ORDERLESS
IMAGES
PLAY
AN
IMPORTANT
ROLE
IN
VISUAL
PERCEPTION
OUR
RETRIEVAL
EXPER
IMENTS
FIG
CONFIRM
THAT
SPATIAL
PYRAMIDS
CAN
CAPTURE
PERCEPTUALLY
SALIENT
FEATURES
AND
SUGGEST
THAT
LOCALLY
OR
DERLESS
MATCHING
MAY
BE
A
POWERFUL
MECHANISM
FOR
ESTI
MATING
OVERALL
PERCEPTUAL
SIMILARITY
BETWEEN
IMAGES
IT
IS
IMPORTANT
TO
CONTRAST
OUR
PROPOSED
APPROACH
WITH
MULTIRESOLUTION
HISTOGRAMS
WHICH
INVOLVE
REPEATEDLY
SUBSAMPLING
AN
IMAGE
AND
COMPUTING
A
GLOBAL
HISTOGRAM
OF
PIXEL
VALUES
AT
EACH
NEW
LEVEL
IN
OTHER
WORDS
A
MUL
TIRESOLUTION
HISTOGRAM
VARIES
THE
RESOLUTION
AT
WHICH
THE
FEA
TURES
INTENSITY
VALUES
ARE
COMPUTED
BUT
THE
HISTOGRAM
RES
OLUTION
INTENSITY
SCALE
STAYS
FIXED
WE
TAKE
THE
OPPOSITE
SUBDIVISION
SCHEME
ALTHOUGH
A
REGULAR
GRID
SEEMS
TO
BE
THE
MOST
POPULAR
IMPLEMENTATION
CHOICE
AND
WHAT
IS
THE
RIGHT
BALANCE
BETWEEN
SUBDIVIDING
AND
DISORDERING
THE
SPATIAL
PYRAMID
FRAMEWORK
SUGGESTS
A
POSSIBLE
WAY
TO
ADDRESS
THIS
ISSUE
NAMELY
THE
BEST
RESULTS
MAY
BE
ACHIEVED
WHEN
MULTIPLE
RESOLUTIONS
ARE
COMBINED
IN
A
PRINCIPLED
WAY
IT
ALSO
SUGGESTS
THAT
THE
REASON
FOR
THE
EMPIRICAL
SUCCESS
OF
SUBDIVIDE
AND
DISORDER
TECHNIQUES
IS
THE
FACT
THAT
THEY
AC
TUALLY
PERFORM
APPROXIMATE
GEOMETRIC
MATCHING
SPATIAL
PYRAMID
MATCHING
WE
FIRST
DESCRIBE
THE
ORIGINAL
FORMULATION
OF
PYRAMID
MATCHING
AND
THEN
INTRODUCE
OUR
APPLICATION
OF
THIS
FRAMEWORK
TO
CREATE
A
SPATIAL
PYRAMID
IMAGE
REPRESENTATION
PYRAMID
MATCH
KERNELS
LET
X
AND
Y
BE
TWO
SETS
OF
VECTORS
IN
A
D
DIMENSIONAL
FEATURE
SPACE
GRAUMAN
AND
DARRELL
PROPOSE
PYRAMID
MATCHING
TO
FIND
AN
APPROXIMATE
CORRESPONDENCE
BETWEEN
THESE
TWO
SETS
INFORMALLY
PYRAMID
MATCHING
WORKS
BY
PLACING
A
SEQUENCE
OF
INCREASINGLY
COARSER
GRIDS
OVER
THE
FEATURE
SPACE
AND
TAKING
A
WEIGHTED
SUM
OF
THE
NUMBER
OF
MATCHES
THAT
OCCUR
AT
EACH
LEVEL
OF
RESOLUTION
AT
ANY
FIXED
RESOLUTION
TWO
POINTS
ARE
SAID
TO
MATCH
IF
THEY
FALL
INTO
THE
SAME
CELL
OF
THE
GRID
MATCHES
FOUND
AT
FINER
RESOLUTIONS
ARE
WEIGHTED
MORE
HIGHLY
THAN
MATCHES
FOUND
AT
COARSER
RESOLU
TIONS
MORE
SPECIFICALLY
LET
US
CONSTRUCT
A
SEQUENCE
OF
GRIDS
AT
RESOLUTIONS
L
SUCH
THAT
THE
GRID
AT
LEVEL
HAS
CELLS
ALONG
EACH
DIMENSION
FOR
A
TOTAL
OF
D
CELLS
LET
F
AND
HF
DENOTE
THE
HISTOGRAMS
OF
X
AND
Y
AT
THIS
RES
OLUTION
SO
THAT
HF
I
AND
HF
I
ARE
THE
NUMBERS
OF
POINTS
APPROACH
OF
FIXING
THE
RESOLUTION
AT
WHICH
THE
FEATURES
ARE
X
Y
COMPUTED
BUT
VARYING
THE
SPATIAL
RESOLUTION
AT
WHICH
THEY
ARE
AGGREGATED
THIS
RESULTS
IN
A
HIGHER
DIMENSIONAL
REP
RESENTATION
THAT
PRESERVES
MORE
INFORMATION
E
G
AN
IMAGE
CONSISTING
OF
THIN
BLACK
AND
WHITE
STRIPES
WOULD
RETAIN
TWO
MODES
AT
EVERY
LEVEL
OF
A
SPATIAL
PYRAMID
WHEREAS
IT
WOULD
BECOME
INDISTINGUISHABLE
FROM
A
UNIFORMLY
GRAY
IMAGE
AT
ALL
BUT
THE
FINEST
LEVELS
OF
A
MULTIRESOLUTION
HISTOGRAM
FI
NALLY
UNLIKE
A
MULTIRESOLUTION
HISTOGRAM
A
SPATIAL
PYRAMID
WHEN
EQUIPPED
WITH
AN
APPROPRIATE
KERNEL
CAN
BE
USED
FOR
FROM
X
AND
Y
THAT
FALL
INTO
THE
ITH
CELL
OF
THE
GRID
THEN
THE
NUMBER
OF
MATCHES
AT
LEVEL
IS
GIVEN
BY
THE
HISTOGRAM
INTERSECTION
FUNCTION
F
F
F
F
X
Y
X
Y
I
IN
THE
FOLLOWING
WE
WILL
ABBREVIATE
I
HF
HF
TO
IF
THE
OPERATION
OF
SUBDIVIDE
AND
DISORDER
I
E
PAR
TITION
THE
IMAGE
INTO
SUBBLOCKS
AND
COMPUTE
HISTOGRAMS
OR
HISTOGRAM
STATISTICS
SUCH
AS
MEANS
OF
LOCAL
FEATURES
IN
THESE
SUBBLOCKS
HAS
BEEN
PRACTICED
NUMEROUS
TIMES
IN
COMPUTER
VISION
BOTH
FOR
GLOBAL
IMAGE
DESCRIPTION
AND
FOR
LOCAL
DESCRIPTION
OF
INTEREST
REGIONS
THUS
THOUGH
THE
OPERATION
ITSELF
SEEMS
FUNDAMENTAL
PRE
VIOUS
METHODS
LEAVE
OPEN
THE
QUESTION
OF
WHAT
IS
THE
RIGHT
NOTE
THAT
THE
NUMBER
OF
MATCHES
FOUND
AT
LEVEL
ALSO
IN
CLUDES
ALL
THE
MATCHES
FOUND
AT
THE
FINER
LEVEL
THERE
FORE
THE
NUMBER
OF
NEW
MATCHES
FOUND
AT
LEVEL
IS
GIVEN
BY
F
F
FOR
L
THE
WEIGHT
ASSOCIATED
WITH
LEVEL
IS
SET
TO
WHICH
IS
INVERSELY
PROPORTIONAL
TO
CELL
WIDTH
AT
THAT
LEVEL
INTUITIVELY
WE
WANT
TO
PENALIZE
MATCHES
FOUND
IN
LARGER
CELLS
BECAUSE
THEY
INVOLVE
INCREAS
INGLY
DISSIMILAR
FEATURES
PUTTING
ALL
THE
PIECES
TOGETHER
WE
GET
THE
FOLLOWING
DEFINITION
OF
A
PYRAMID
MATCH
KERNEL
L
LEVEL
LEVEL
LEVEL
ΚL
X
Y
I
IF
IF
F
L
IF
BOTH
THE
HISTOGRAM
INTERSECTION
AND
THE
PYRAMID
MATCH
KER
NEL
ARE
MERCER
KERNELS
SPATIAL
MATCHING
SCHEME
AS
INTRODUCED
IN
A
PYRAMID
MATCH
KERNEL
WORKS
WITH
AN
ORDERLESS
IMAGE
REPRESENTATION
IT
ALLOWS
FOR
PRE
CISE
MATCHING
OF
TWO
COLLECTIONS
OF
FEATURES
IN
A
HIGH
DIMENSIONAL
APPEARANCE
SPACE
BUT
DISCARDS
ALL
SPATIAL
IN
FORMATION
THIS
PAPER
ADVOCATES
AN
ORTHOGONAL
APPROACH
PERFORM
PYRAMID
MATCHING
IN
THE
TWO
DIMENSIONAL
IMAGE
SPACE
AND
USE
TRADITIONAL
CLUSTERING
TECHNIQUES
IN
FEATURE
SPACE
SPECIFICALLY
WE
QUANTIZE
ALL
FEATURE
VECTORS
INTO
M
DISCRETE
TYPES
AND
MAKE
THE
SIMPLIFYING
ASSUMPTION
THAT
ONLY
FEATURES
OF
THE
SAME
TYPE
CAN
BE
MATCHED
TO
ONE
AN
OTHER
EACH
CHANNEL
M
GIVES
US
TWO
SETS
OF
TWO
DIMENSIONAL
VECTORS
XM
AND
YM
REPRESENTING
THE
COORDINATES
OF
FEA
TURES
OF
TYPE
M
FOUND
IN
THE
RESPECTIVE
IMAGES
THE
FINAL
KERNEL
IS
THEN
THE
SUM
OF
THE
SEPARATE
CHANNEL
KERNELS
M
KL
X
Y
ΚL
XM
YM
M
THIS
APPROACH
HAS
THE
ADVANTAGE
OF
MAINTAINING
CONTINUITY
WITH
THE
POPULAR
VISUAL
VOCABULARY
PARADIGM
IN
FACT
IT
REDUCES
TO
A
STANDARD
BAG
OF
FEATURES
WHEN
L
BECAUSE
THE
PYRAMID
MATCH
KERNEL
IS
SIMPLY
A
WEIGHTED
SUM
OF
HISTOGRAM
INTERSECTIONS
AND
BECAUSE
C
MIN
A
B
MIN
CA
CB
FOR
POSITIVE
NUMBERS
WE
CAN
IMPLEMENT
KL
AS
A
SINGLE
HISTOGRAM
INTERSECTION
OF
LONG
VECTORS
FORMED
BY
CONCATENATING
THE
APPROPRIATELY
WEIGHTED
HISTOGRAMS
OF
ALL
CHANNELS
AT
ALL
RESOLUTIONS
FIG
FOR
L
LEVELS
AND
M
CHANNELS
THE
RESULTING
VECTOR
HAS
DIMEN
SIONALITY
M
L
M
SEVERAL
EXPERI
FIGURE
TOY
EXAMPLE
OF
CONSTRUCTING
A
THREE
LEVEL
PYRAMID
THE
IMAGE
HAS
THREE
FEATURE
TYPES
INDICATED
BY
CIRCLES
DIAMONDS
AND
CROSSES
AT
THE
TOP
WE
SUBDIVIDE
THE
IMAGE
AT
THREE
DIFFERENT
LEV
ELS
OF
RESOLUTION
NEXT
FOR
EACH
LEVEL
OF
RESOLUTION
AND
EACH
CHAN
NEL
WE
COUNT
THE
FEATURES
THAT
FALL
IN
EACH
SPATIAL
BIN
FINALLY
WE
WEIGHT
EACH
SPATIAL
HISTOGRAM
ACCORDING
TO
EQ
THE
FINAL
IMPLEMENTATION
ISSUE
IS
THAT
OF
NORMALIZATION
FOR
MAXIMUM
COMPUTATIONAL
EFFICIENCY
WE
NORMALIZE
ALL
HISTOGRAMS
BY
THE
TOTAL
WEIGHT
OF
ALL
FEATURES
IN
THE
IMAGE
IN
EFFECT
FORCING
THE
TOTAL
NUMBER
OF
FEATURES
IN
ALL
IMAGES
TO
BE
THE
SAME
BECAUSE
WE
USE
A
DENSE
FEATURE
REPRESENTATION
SEE
SECTION
AND
THUS
DO
NOT
NEED
TO
WORRY
ABOUT
SPURI
OUS
FEATURE
DETECTIONS
RESULTING
FROM
CLUTTER
THIS
PRACTICE
IS
SUFFICIENT
TO
DEAL
WITH
THE
EFFECTS
OF
VARIABLE
IMAGE
SIZE
FEATURE
EXTRACTION
THIS
SECTION
BRIEFLY
DESCRIBES
THE
TWO
KINDS
OF
FEATURES
USED
IN
THE
EXPERIMENTS
OF
SECTION
FIRST
WE
HAVE
SO
CALLED
WEAK
FEATURES
WHICH
ARE
ORIENTED
EDGE
POINTS
I
E
POINTS
WHOSE
GRADIENT
MAGNITUDE
IN
A
GIVEN
DIRECTION
EX
CEEDS
A
MINIMUM
THRESHOLD
WE
EXTRACT
EDGE
POINTS
AT
TWO
SCALES
AND
EIGHT
ORIENTATIONS
FOR
A
TOTAL
OF
M
CHAN
NELS
WE
DESIGNED
THESE
FEATURES
TO
OBTAIN
A
REPRESENTATION
SIMILAR
TO
THE
GIST
OR
TO
A
GLOBAL
SIFT
DESCRIPTOR
OF
THE
IMAGE
FOR
BETTER
DISCRIMINATIVE
POWER
WE
ALSO
UTILIZE
HIGHER
DIMENSIONAL
STRONG
FEATURES
WHICH
ARE
SIFT
DESCRIPTORS
OF
PIXEL
PATCHES
COMPUTED
OVER
A
GRID
WITH
SPACING
MENTS
REPORTED
IN
SECTION
USE
THE
SETTINGS
OF
M
AND
L
RESULTING
IN
DIMENSIONAL
HISTOGRAM
IN
TERSECTIONS
HOWEVER
THESE
OPERATIONS
ARE
EFFICIENT
BECAUSE
THE
HISTOGRAM
VECTORS
ARE
EXTREMELY
SPARSE
IN
FACT
JUST
AS
IN
THE
COMPUTATIONAL
COMPLEXITY
OF
THE
KERNEL
IS
LINEAR
IN
THE
NUMBER
OF
FEATURES
IT
MUST
ALSO
BE
NOTED
THAT
WE
DID
NOT
OBSERVE
ANY
SIGNIFICANT
INCREASE
IN
PERFORMANCE
BEYOND
M
AND
L
WHERE
THE
CONCATENATED
HISTOGRAMS
ARE
ONLY
DIMENSIONAL
PRINCIPLE
IT
IS
POSSIBLE
TO
INTEGRATE
GEOMETRIC
INFORMATION
DIRECTLY
INTO
THE
ORIGINAL
PYRAMID
MATCHING
FRAMEWORK
BY
TREATING
IMAGE
COORDI
NATES
AS
TWO
EXTRA
DIMENSIONS
IN
THE
FEATURE
SPACE
OF
PIXELS
OUR
DECISION
TO
USE
A
DENSE
REGULAR
GRID
IN
STEAD
OF
INTEREST
POINTS
WAS
BASED
ON
THE
COMPARATIVE
EVALU
ATION
OF
FEI
FEI
AND
PERONA
WHO
HAVE
SHOWN
THAT
DENSE
FEATURES
WORK
BETTER
FOR
SCENE
CLASSIFICATION
INTUITIVELY
A
DENSE
IMAGE
DESCRIPTION
IS
NECESSARY
TO
CAPTURE
UNIFORM
RE
GIONS
SUCH
AS
SKY
CALM
WATER
OR
ROAD
SURFACE
TO
DEAL
WITH
LOW
CONTRAST
REGIONS
WE
SKIP
THE
USUAL
SIFT
NORMALIZATION
PROCEDURE
WHEN
THE
OVERALL
GRADIENT
MAGNITUDE
OF
THE
PATCH
IS
TOO
WEAK
WE
PERFORM
K
MEANS
CLUSTERING
OF
A
RANDOM
SUBSET
OF
PATCHES
FROM
THE
TRAINING
SET
TO
FORM
A
VISUAL
VO
CABULARY
TYPICAL
VOCABULARY
SIZES
FOR
OUR
EXPERIMENTS
ARE
M
AND
M
OFFICE
KITCHEN
LIVING
ROOM
BEDROOM
STORE
INDUSTRIAL
TALL
BUILDING
INSIDE
CITY
STREET
HIGHWAY
COAST
OPEN
COUNTRY
MOUNTAIN
FOREST
SUBURB
FIGURE
EXAMPLE
IMAGES
FROM
THE
SCENE
CATEGORY
DATABASE
THE
STARRED
CATEGORIES
ORIGINATE
FROM
OLIVA
AND
TORRALBA
TABLE
CLASSIFICATION
RESULTS
FOR
THE
SCENE
CATEGORY
DATABASE
SEE
TEXT
THE
HIGHEST
RESULTS
FOR
EACH
KIND
OF
FEATURE
ARE
SHOWN
IN
BOLD
EXPERIMENTS
IN
THIS
SECTION
WE
REPORT
RESULTS
ON
THREE
DIVERSE
DATASETS
FIFTEEN
SCENE
CATEGORIES
CALTECH
AND
GRAZ
WE
PERFORM
ALL
PROCESSING
IN
GRAYSCALE
EVEN
WHEN
COLOR
IMAGES
ARE
AVAILABLE
ALL
EXPERIMENTS
ARE
RE
PEATED
TEN
TIMES
WITH
DIFFERENT
RANDOMLY
SELECTED
TRAINING
AND
TEST
IMAGES
AND
THE
AVERAGE
OF
PER
CLASS
RECOGNITION
IS
RECORDED
FOR
EACH
RUN
THE
FINAL
RESULT
IS
REPORTED
AS
THE
MEAN
AND
STANDARD
DEVIATION
OF
THE
RESULTS
FROM
THE
IN
DIVIDUAL
RUNS
MULTI
CLASS
CLASSIFICATION
IS
DONE
WITH
A
SUP
PORT
VECTOR
MACHINE
SVM
TRAINED
USING
THE
ONE
VERSUS
ALL
RULE
A
CLASSIFIER
IS
LEARNED
TO
SEPARATE
EACH
CLASS
FROM
THE
REST
AND
A
TEST
IMAGE
IS
ASSIGNED
THE
LABEL
OF
THE
CLASSIFIER
WITH
THE
HIGHEST
RESPONSE
ALTERNATIVE
PERFORMANCE
MEASURE
THE
PERCENTAGE
OF
ALL
TEST
IM
AGES
CLASSIFIED
CORRECTLY
CAN
BE
BIASED
IF
TEST
SET
SIZES
FOR
DIFFERENT
CLASSES
VARY
SIGNIFICANTLY
THIS
IS
ESPECIALLY
TRUE
OF
THE
CALTECH
DATASET
WHERE
SOME
OF
THE
EASIEST
CLASSES
ARE
DISPROPORTIONATELY
LARGE
SCENE
CATEGORY
RECOGNITION
OUR
FIRST
DATASET
FIG
IS
COMPOSED
OF
FIFTEEN
SCENE
CAT
EGORIES
THIRTEEN
WERE
PROVIDED
BY
FEI
FEI
AND
PERONA
EIGHT
OF
THESE
WERE
ORIGINALLY
COLLECTED
BY
OLIVA
AND
TOR
RALBA
AND
TWO
INDUSTRIAL
AND
STORE
WERE
COLLECTED
BY
OURSELVES
EACH
CATEGORY
HAS
TO
IMAGES
AND
AV
ERAGE
IMAGE
SIZE
IS
PIXELS
THE
MAJOR
SOURCES
OF
THE
PICTURES
IN
THE
DATASET
INCLUDE
THE
COREL
COLLECTION
PERSONAL
PHOTOGRAPHS
AND
GOOGLE
IMAGE
SEARCH
THIS
IS
ONE
OF
THE
MOST
COMPLETE
SCENE
CATEGORY
DATASET
USED
IN
THE
LITERATURE
THUS
FAR
TABLE
SHOWS
DETAILED
RESULTS
OF
CLASSIFICATION
EXPERI
MENTS
USING
IMAGES
PER
CLASS
FOR
TRAINING
AND
THE
REST
FOR
TESTING
THE
SAME
SETUP
AS
FIRST
LET
US
EXAMINE
THE
PERFORMANCE
OF
STRONG
FEATURES
FOR
L
AND
M
CORRESPONDING
TO
A
STANDARD
BAG
OF
FEATURES
OUR
CLASSI
FICATION
RATE
IS
FOR
THE
CLASSES
INHERITED
FROM
FEI
FEI
AND
PERONA
WHICH
IS
MUCH
HIGHER
THAN
THEIR
BEST
RESULTS
OF
ACHIEVED
WITH
AN
ORDERLESS
METHOD
AND
A
FEATURE
SET
COMPARABLE
TO
OURS
WE
CONJECTURE
THAT
FEI
FEI
AND
PERONA
APPROACH
IS
DISADVANTAGED
BY
ITS
RE
OFFICE
KITCHEN
LIVING
ROOM
BEDROOM
STORE
INDUSTRIAL
TALL
BUILDING
INSIDE
CITY
STREET
HIGHWAY
COAST
OPEN
COUNTRY
MOUNTAIN
FOREST
SUBURB
FIGURE
CONFUSION
TABLE
FOR
THE
SCENE
CATEGORY
DATASET
AVERAGE
CLASSIFICATION
RATES
FOR
INDIVIDUAL
CLASSES
ARE
LISTED
ALONG
THE
DIAG
ONAL
THE
ENTRY
IN
THE
ITH
ROW
AND
JTH
COLUMN
IS
THE
PERCENTAGE
OF
IMAGES
FROM
CLASS
I
THAT
WERE
MISIDENTIFIED
AS
CLASS
J
LIANCE
ON
LATENT
DIRICHLET
ALLOCATION
LDA
WHICH
IS
ESSENTIALLY
AN
UNSUPERVISED
DIMENSIONALITY
REDUCTION
TECH
NIQUE
AND
AS
SUCH
IS
NOT
NECESSARILY
CONDUCIVE
TO
ACHIEV
ING
THE
HIGHEST
CLASSIFICATION
ACCURACY
TO
VERIFY
THIS
WE
HAVE
EXPERIMENTED
WITH
PROBABILISTIC
LATENT
SEMANTIC
ANALY
SIS
PLSA
WHICH
ATTEMPTS
TO
EXPLAIN
THE
DISTRIBUTION
OF
FEATURES
IN
THE
IMAGE
AS
A
MIXTURE
OF
A
FEW
SCENE
TOPICS
OR
ASPECTS
AND
PERFORMS
VERY
SIMILARLY
TO
LDA
IN
PRAC
TICE
FOLLOWING
THE
SCHEME
OF
QUELHAS
ET
AL
WE
RUN
PLSA
IN
AN
UNSUPERVISED
SETTING
TO
LEARN
A
ASPECT
MODEL
OF
HALF
THE
TRAINING
IMAGES
NEXT
WE
APPLY
THIS
MODEL
TO
THE
OTHER
HALF
TO
OBTAIN
PROBABILITIES
OF
TOPICS
GIVEN
EACH
IMAGE
THUS
REDUCING
THE
DIMENSIONALITY
OF
THE
FEATURE
SPACE
FROM
TO
FINALLY
WE
TRAIN
THE
SVM
ON
THESE
REDUCED
FEATURES
AND
USE
THEM
TO
CLASSIFY
THE
TEST
SET
IN
THIS
SETUP
OUR
AVERAGE
CLASSIFICATION
RATE
DROPS
TO
FROM
THE
ORIGINAL
FOR
THE
CLASSES
INHERITED
FROM
FEI
FEI
AND
PERONA
IT
DROPS
TO
FROM
WHICH
IS
NOW
VERY
SIMILAR
TO
THEIR
RESULTS
THUS
WE
CAN
SEE
THAT
LA
TENT
FACTOR
ANALYSIS
TECHNIQUES
CAN
ADVERSELY
AFFECT
CLASSIFI
CATION
PERFORMANCE
WHICH
IS
ALSO
CONSISTENT
WITH
THE
RESULTS
OF
QUELHAS
ET
AL
NEXT
LET
US
EXAMINE
THE
BEHAVIOR
OF
SPATIAL
PYRAMID
MATCHING
FOR
COMPLETENESS
TABLE
LISTS
THE
PERFORMANCE
ACHIEVED
USING
JUST
THE
HIGHEST
LEVEL
OF
THE
PYRAMID
THE
SINGLE
LEVEL
COLUMNS
AS
WELL
AS
THE
PERFORMANCE
OF
THE
COMPLETE
MATCHING
SCHEME
USING
MULTIPLE
LEVELS
THE
PYRA
MID
COLUMNS
FOR
ALL
THREE
KINDS
OF
FEATURES
RESULTS
IM
PROVE
DRAMATICALLY
AS
WE
GO
FROM
L
TO
A
MULTI
LEVEL
SETUP
THOUGH
MATCHING
AT
THE
HIGHEST
PYRAMID
LEVEL
SEEMS
TO
ACCOUNT
FOR
MOST
OF
THE
IMPROVEMENT
USING
ALL
THE
LEVELS
TOGETHER
CONFERS
A
STATISTICALLY
SIGNIFICANT
BENEFIT
FOR
STRONG
FEATURES
SINGLE
LEVEL
PERFORMANCE
ACTUALLY
DROPS
AS
WE
GO
FROM
L
TO
L
THIS
MEANS
THAT
THE
HIGHEST
LEVEL
OF
THE
L
PYRAMID
IS
TOO
FINELY
SUBDIVIDED
WITH
INDIVID
UAL
BINS
YIELDING
TOO
FEW
MATCHES
DESPITE
THE
DIMINISHED
DISCRIMINATIVE
POWER
OF
THE
HIGHEST
LEVEL
THE
PERFORMANCE
OF
THE
ENTIRE
L
PYRAMID
REMAINS
ESSENTIALLY
IDENTICAL
TO
THAT
OF
THE
L
PYRAMID
THIS
THEN
IS
THE
MAIN
ADVANTAGE
OF
THE
SPATIAL
PYRAMID
REPRESENTATION
BECAUSE
IT
COMBINES
MULTIPLE
RESOLUTIONS
IN
A
PRINCIPLED
FASHION
IT
IS
ROBUST
TO
FAILURES
AT
INDIVIDUAL
LEVELS
IT
IS
ALSO
INTERESTING
TO
COMPARE
PERFORMANCE
OF
DIFFER
ENT
FEATURE
SETS
AS
EXPECTED
WEAK
FEATURES
DO
NOT
PER
FORM
AS
WELL
AS
STRONG
FEATURES
THOUGH
IN
COMBINATION
WITH
THE
SPATIAL
PYRAMID
THEY
CAN
ALSO
ACHIEVE
ACCEPTABLE
LEVELS
OF
ACCURACY
NOTE
THAT
BECAUSE
WEAK
FEATURES
HAVE
A
MUCH
HIGHER
DENSITY
AND
MUCH
SMALLER
SPATIAL
EXTENT
THAN
STRONG
FEATURES
THEIR
PERFORMANCE
CONTINUES
TO
IMPROVE
AS
WE
GO
FROM
L
TO
L
INCREASING
THE
VISUAL
VOCABULARY
SIZE
FROM
M
TO
M
RESULTS
IN
A
SMALL
PERFOR
MANCE
INCREASE
AT
L
BUT
THIS
DIFFERENCE
IS
ALL
BUT
ELIM
INATED
AT
HIGHER
PYRAMID
LEVELS
THUS
WE
CAN
CONCLUDE
THAT
THE
COARSE
GRAINED
GEOMETRIC
CUES
PROVIDED
BY
THE
PYRAMID
HAVE
MORE
DISCRIMINATIVE
POWER
THAN
AN
ENLARGED
VISUAL
VO
CABULARY
OF
COURSE
THE
OPTIMAL
WAY
TO
EXPLOIT
STRUCTURE
BOTH
IN
THE
IMAGE
AND
IN
THE
FEATURE
SPACE
MAY
BE
TO
COM
BINE
THEM
IN
A
UNIFIED
MULTIRESOLUTION
FRAMEWORK
THIS
IS
SUBJECT
FOR
FUTURE
RESEARCH
FIG
SHOWS
A
CONFUSION
TABLE
BETWEEN
THE
FIFTEEN
SCENE
CATEGORIES
NOT
SURPRISINGLY
CONFUSION
OCCURS
BETWEEN
THE
INDOOR
CLASSES
KITCHEN
BEDROOM
LIVING
ROOM
AND
ALSO
BE
TWEEN
SOME
NATURAL
CLASSES
SUCH
AS
COAST
AND
OPEN
COUNTRY
FIG
SHOWS
EXAMPLES
OF
IMAGE
RETRIEVAL
USING
THE
SPATIAL
PYRAMID
KERNEL
AND
STRONG
FEATURES
WITH
M
THESE
EXAMPLES
GIVE
A
SENSE
OF
THE
KIND
OF
VISUAL
INFORMATION
CAP
TURED
BY
OUR
APPROACH
IN
PARTICULAR
SPATIAL
PYRAMIDS
SEEM
SUCCESSFUL
AT
CAPTURING
THE
ORGANIZATION
OF
MAJOR
PICTORIAL
ELEMENTS
OR
BLOBS
AND
THE
DIRECTIONALITY
OF
DOMINANT
LINES
AND
EDGES
BECAUSE
THE
PYRAMID
IS
BASED
ON
FEATURES
COM
PUTED
AT
THE
ORIGINAL
IMAGE
RESOLUTION
EVEN
HIGH
FREQUENCY
DETAILS
CAN
BE
PRESERVED
FOR
EXAMPLE
QUERY
IMAGE
B
SHOWS
WHITE
KITCHEN
CABINET
DOORS
WITH
DARK
BORDERS
THREE
OF
THE
RETRIEVED
KITCHEN
IMAGES
CONTAIN
SIMILAR
CABINETS
THE
OFFICE
IMAGE
SHOWS
A
WALL
PLASTERED
WITH
WHITE
DOCU
MENTS
IN
DARK
FRAMES
AND
THE
INSIDE
CITY
IMAGE
SHOWS
A
WHITE
BUILDING
WITH
DARKER
WINDOW
FRAMES
CALTECH
OUR
SECOND
SET
OF
EXPERIMENTS
IS
ON
THE
CALTECH
DATABASE
FIG
THIS
DATABASE
CONTAINS
FROM
TO
IMAGES
PER
CATEGORY
MOST
IMAGES
ARE
MEDIUM
RESOLU
TION
I
E
ABOUT
PIXELS
CALTECH
IS
PROBABLY
THE
MOST
DIVERSE
OBJECT
DATABASE
AVAILABLE
TODAY
THOUGH
IT
KITCHEN
LIVING
ROOM
LIVING
ROOM
LIVING
ROOM
OFFICE
LIVING
ROOM
LIVING
ROOM
LIVING
ROOM
LIVING
ROOM
KITCHEN
OFFICE
INSIDE
CITY
STORE
MOUNTAIN
FOREST
TALL
BLDG
INSIDE
CITY
INSIDE
CITY
TALL
BLDG
INSIDE
CITY
MOUNTAIN
MOUNTAIN
MOUNTAIN
INSIDE
CITY
TALL
BLDG
STREET
FIGURE
RETRIEVAL
FROM
THE
SCENE
CATEGORY
DATABASE
THE
QUERY
IMAGES
ARE
ON
THE
LEFT
AND
THE
EIGHT
IMAGES
GIVING
THE
HIGHEST
VALUES
OF
THE
SPATIAL
PYRAMID
KERNEL
FOR
L
M
ARE
ON
THE
RIGHT
THE
ACTUAL
CLASS
OF
INCORRECTLY
RETRIEVED
IMAGES
IS
LISTED
BELOW
THEM
IS
NOT
WITHOUT
SHORTCOMINGS
NAMELY
MOST
IMAGES
FEATURE
RELATIVELY
LITTLE
CLUTTER
AND
THE
OBJECTS
ARE
CENTERED
AND
OC
CUPY
MOST
OF
THE
IMAGE
IN
ADDITION
A
NUMBER
OF
CATEGORIES
SUCH
AS
MINARET
SEE
FIG
ARE
AFFECTED
BY
CORNER
ARTI
FACTS
RESULTING
FROM
ARTIFICIAL
IMAGE
ROTATION
THOUGH
THESE
ARTIFACTS
ARE
SEMANTICALLY
IRRELEVANT
THEY
CAN
PROVIDE
STABLE
CUES
RESULTING
IN
MISLEADINGLY
HIGH
RECOGNITION
RATES
WE
FOLLOW
THE
EXPERIMENTAL
SETUP
OF
GRAUMAN
AND
DAR
RELL
AND
J
ZHANG
ET
AL
NAMELY
WE
TRAIN
ON
IM
AGES
PER
CLASS
AND
TEST
ON
THE
REST
FOR
EFFICIENCY
WE
LIMIT
THE
NUMBER
OF
TEST
IMAGES
TO
PER
CLASS
NOTE
THAT
BE
CAUSE
SOME
CATEGORIES
ARE
VERY
SMALL
WE
MAY
END
UP
WITH
JUST
A
SINGLE
TEST
IMAGE
PER
CLASS
TABLE
GIVES
A
BREAK
DOWN
OF
CLASSIFICATION
RATES
FOR
DIFFERENT
PYRAMID
LEVELS
FOR
WEAK
FEATURES
AND
STRONG
FEATURES
WITH
M
THE
RESULTS
FOR
M
ARE
NOT
SHOWN
BECAUSE
JUST
AS
FOR
THE
SCENE
CATEGORY
DATABASE
THEY
DO
NOT
BRING
ANY
SIGNIFI
CANT
IMPROVEMENT
FOR
L
STRONG
FEATURES
GIVE
WHICH
IS
SLIGHTLY
BELOW
THE
REPORTED
BY
GRAUMAN
AND
DARRELL
OUR
BEST
RESULT
IS
ACHIEVED
WITH
STRONG
FEA
TURES
AT
L
THIS
EXCEEDS
THE
HIGHEST
CLASSIFICATION
RATE
PREVIOUSLY
PUBLISHED
THAT
OF
REPORTED
BY
J
ZHANG
ET
AL
BERG
ET
AL
REPORT
ACCURACY
USING
TRAINING
IMAGES
PER
CLASS
OUR
AVERAGE
RECOGNITION
RATE
WITH
THIS
SETUP
IS
THE
BEHAVIOR
OF
WEAK
FEATURES
ON
THIS
DATABASE
IS
ALSO
NOTEWORTHY
FOR
L
THEY
GIVE
A
CLAS
SIFICATION
RATE
OF
WHICH
IS
CONSISTENT
WITH
A
NAIVE
GRAYLEVEL
CORRELATION
BASELINE
BUT
IN
CONJUNCTION
WITH
A
FOUR
LEVEL
SPATIAL
PYRAMID
THEIR
PERFORMANCE
RISES
TO
ON
PAR
WITH
THE
BEST
RESULTS
IN
THE
LITERATURE
FIG
SHOWS
A
FEW
OF
THE
EASIEST
AND
HARDEST
OBJECT
CLASSES
FOR
OUR
METHOD
THE
SUCCESSFUL
CLASSES
ARE
EITHER
DOMINATED
BY
ROTATION
ARTIFACTS
LIKE
MINARET
HAVE
VERY
LIT
TLE
CLUTTER
LIKE
WINDSOR
CHAIR
OR
REPRESENT
COHERENT
NATURAL
SCENES
LIKE
JOSHUA
TREE
AND
OKAPI
THE
LEAST
SUCCESS
FUL
CLASSES
ARE
EITHER
TEXTURELESS
ANIMALS
LIKE
BEAVER
AND
COUGAR
ANIMALS
THAT
CAMOUFLAGE
WELL
IN
THEIR
ENVIRONMENT
HOWEVER
H
ZHANG
ET
AL
IN
THESE
PROCEEDINGS
FOR
AN
AL
GORITHM
THAT
YIELDS
A
CLASSIFICATION
RATE
OF
FOR
TRAINING
EXAMPLES
AND
FOR
EXAMPLES
MINARET
WINDSOR
CHAIR
JOSHUA
TREE
OKAPI
COUGAR
BODY
BEAVER
CROCODILE
ANT
FIGURE
CALTECH
RESULTS
TOP
SOME
CLASSES
ON
WHICH
OUR
METHOD
L
M
ACHIEVED
HIGH
PERFORMANCE
BOTTOM
SOME
CLASSES
ON
WHICH
OUR
METHOD
PERFORMED
POORLY
TABLE
CLASSIFICATION
RESULTS
FOR
THE
CALTECH
DATABASE
TABLE
TOP
FIVE
CONFUSIONS
FOR
OUR
METHOD
L
M
ON
THE
CALTECH
DATABASE
TABLE
RESULTS
OF
OUR
METHOD
M
FOR
THE
GRAZ
DATABASE
AND
COMPARISON
WITH
TWO
EXISTING
METHODS
LIKE
CROCODILE
OR
THIN
OBJECTS
LIKE
ANT
TABLE
SHOWS
THE
TOP
FIVE
OF
OUR
METHOD
CONFUSIONS
ALL
OF
WHICH
ARE
BETWEEN
CLOSELY
RELATED
CLASSES
TO
SUMMARIZE
OUR
METHOD
HAS
OUTPERFORMED
BOTH
STATE
OF
THE
ART
ORDERLESS
METHODS
AND
METHODS
BASED
ON
PRECISE
GEOMETRIC
CORRESPONDENCE
SIGNIFICANTLY
ALL
THESE
METHODS
RELY
ON
SPARSE
FEATURES
INTEREST
POINTS
OR
SPARSELY
SAMPLED
EDGE
POINTS
HOWEVER
BECAUSE
OF
THE
GEOMETRIC
STABILITY
AND
LACK
OF
CLUTTER
OF
CALTECH
DENSE
FEATURES
COMBINED
WITH
GLOBAL
SPATIAL
RELATIONS
SEEM
TO
CAP
TURE
MORE
DISCRIMINATIVE
INFORMATION
ABOUT
THE
OBJECTS
THE
GRAZ
DATASET
AS
SEEN
FROM
SECTIONS
AND
OUR
PROPOSED
AP
PROACH
DOES
VERY
WELL
ON
GLOBAL
SCENE
CLASSIFICATION
TASKS
OR
ON
OBJECT
RECOGNITION
TASKS
IN
THE
ABSENCE
OF
CLUTTER
WITH
MOST
OF
THE
OBJECTS
ASSUMING
CANONICAL
POSES
HOWEVER
IT
WAS
NOT
DESIGNED
TO
COPE
WITH
HEAVY
CLUTTER
AND
POSE
CHANGES
IT
IS
INTERESTING
TO
SEE
HOW
WELL
OUR
ALGORITHM
CAN
DO
BY
EXPLOITING
THE
GLOBAL
SCENE
CUES
THAT
STILL
REMAIN
UNDER
THESE
CONDITIONS
ACCORDINGLY
OUR
FINAL
SET
OF
EX
PERIMENTS
IS
ON
THE
GRAZ
DATASET
FIG
WHICH
IS
CHARACTERIZED
BY
HIGH
INTRA
CLASS
VARIATION
THIS
DATASET
HAS
TWO
OBJECT
CLASSES
BIKES
IMAGES
AND
PERSONS
IM
AGES
AND
A
BACKGROUND
CLASS
IMAGES
THE
IMAGE
RES
OLUTION
IS
AND
THE
RANGE
OF
SCALES
AND
POSES
AT
WHICH
EXEMPLARS
ARE
PRESENTED
IS
VERY
DIVERSE
E
G
A
PER
SON
IMAGE
MAY
SHOW
A
PEDESTRIAN
IN
THE
DISTANCE
A
SIDE
VIEW
OF
A
COMPLETE
BODY
OR
JUST
A
CLOSEUP
OF
A
HEAD
FOR
THIS
DATABASE
WE
PERFORM
TWO
CLASS
DETECTION
OBJECT
VS
BACK
GROUND
USING
AN
EXPERIMENTAL
SETUP
CONSISTENT
WITH
THAT
OF
OPELT
ET
AL
NAMELY
WE
TRAIN
DETECTORS
FOR
PERSONS
AND
BIKES
ON
POSITIVE
AND
NEGATIVE
IMAGES
OF
WHICH
ARE
DRAWN
FROM
THE
OTHER
OBJECT
CLASS
AND
FROM
THE
BACK
GROUND
AND
TEST
ON
A
SIMILARLY
DISTRIBUTED
SET
WE
GENERATE
ROC
CURVES
BY
THRESHOLDING
RAW
SVM
OUTPUT
AND
REPORT
THE
ROC
EQUAL
ERROR
RATE
AVERAGED
OVER
TEN
RUNS
TABLE
SUMMARIZES
OUR
RESULTS
FOR
STRONG
FEATURES
WITH
M
NOTE
THAT
THE
STANDARD
DEVIATION
IS
QUITE
HIGH
BE
CAUSE
THE
IMAGES
IN
THE
DATABASE
VARY
GREATLY
IN
THEIR
LEVEL
OF
DIFFICULTY
SO
THE
PERFORMANCE
FOR
ANY
SINGLE
RUN
IS
DEPEN
DENT
ON
THE
COMPOSITION
OF
THE
TRAINING
SET
IN
PARTICULAR
FOR
L
THE
PERFORMANCE
FOR
BIKES
RANGES
FROM
TO
FOR
THIS
DATABASE
THE
IMPROVEMENT
FROM
L
TO
L
IS
RELATIVELY
SMALL
THIS
MAKES
INTUITIVE
SENSE
WHEN
A
CLASS
IS
CHARACTERIZED
BY
HIGH
GEOMETRIC
VARIABILITY
IT
IS
DIFFICULT
TO
FIND
USEFUL
GLOBAL
FEATURES
DESPITE
THIS
DISADVANTAGE
OF
OUR
METHOD
WE
STILL
ACHIEVE
RESULTS
VERY
CLOSE
TO
THOSE
OF
OPELT
ET
AL
WHO
USE
A
SPARSE
LOCALLY
INVARIANT
FEATURE
REPRESENTATION
IN
THE
FUTURE
WE
PLAN
TO
COMBINE
SPATIAL
PYRAMIDS
WITH
INVARIANT
FEATURES
FOR
IMPROVED
ROBUSTNESS
AGAINST
GEOMETRIC
CHANGES
DISCUSSION
THIS
PAPER
HAS
PRESENTED
A
HOLISTIC
APPROACH
FOR
IMAGE
CATEGORIZATION
BASED
ON
A
MODIFICATION
OF
PYRAMID
MATCH
KERNELS
OUR
METHOD
WHICH
WORKS
BY
REPEATEDLY
SUB
DIVIDING
AN
IMAGE
AND
COMPUTING
HISTOGRAMS
OF
IMAGE
FEA
TURES
OVER
THE
RESULTING
SUBREGIONS
HAS
SHOWN
PROMISING
RE
BIKE
PERSON
BACKGROUND
FIGURE
THE
GRAZ
DATABASE
SULTS
ON
THREE
LARGE
SCALE
DIVERSE
DATASETS
DESPITE
THE
SIM
PLICITY
OF
OUR
METHOD
AND
DESPITE
THE
FACT
THAT
IT
WORKS
NOT
BY
CONSTRUCTING
EXPLICIT
OBJECT
MODELS
BUT
BY
USING
GLOBAL
CUES
AS
INDIRECT
EVIDENCE
ABOUT
THE
PRESENCE
OF
AN
OBJECT
IT
CONSISTENTLY
ACHIEVES
AN
IMPROVEMENT
OVER
AN
ORDERLESS
IMAGE
REPRESENTATION
THIS
IS
NOT
A
TRIVIAL
ACCOMPLISHMENT
GIVEN
THAT
A
WELL
DESIGNED
BAG
OF
FEATURES
METHOD
CAN
OUT
PERFORM
MORE
SOPHISTICATED
APPROACHES
BASED
ON
PARTS
AND
RELATIONS
OUR
RESULTS
ALSO
UNDERSCORE
THE
SURPRISING
AND
UBIQUITOUS
POWER
OF
GLOBAL
SCENE
STATISTICS
EVEN
IN
HIGHLY
VARIABLE
DATASETS
SUCH
AS
GRAZ
THEY
CAN
STILL
PROVIDE
USEFUL
DISCRIMINATIVE
INFORMATION
IT
IS
IMPORTANT
TO
DEVELOP
METHODS
THAT
TAKE
FULL
ADVANTAGE
OF
THIS
INFORMATION
EI
THER
AS
STAND
ALONE
SCENE
CATEGORIZERS
AS
CONTEXT
MOD
ULES
WITHIN
LARGER
OBJECT
RECOGNITION
SYSTEMS
OR
AS
TOOLS
FOR
EVALUATING
BIASES
PRESENT
IN
NEWLY
COLLECTED
DATASETS
HISTOGRAMS
OF
ORIENTED
GRADIENTS
FOR
HUMAN
DETECTION
NAVNEET
DALAL
AND
BILL
TRIGGS
INRIA
RHOˆNE
ALPS
AVENUE
DE
L
EUROPE
MONTBONNOT
FRANCE
NAVNEET
DALAL
BILL
TRIGGS
INRIALPES
FR
ABSTRACT
WE
STUDY
THE
QUESTION
OF
FEATURE
SETS
FOR
ROBUST
VISUAL
OB
JECT
RECOGNITION
ADOPTING
LINEAR
SVM
BASED
HUMAN
DETEC
TION
AS
A
TEST
CASE
AFTER
REVIEWING
EXISTING
EDGE
AND
GRA
DIENT
BASED
DESCRIPTORS
WE
SHOW
EXPERIMENTALLY
THAT
GRIDS
OF
HISTOGRAMS
OF
ORIENTED
GRADIENT
HOG
DESCRIPTORS
SIG
NIFICANTLY
OUTPERFORM
EXISTING
FEATURE
SETS
FOR
HUMAN
DETEC
TION
WE
STUDY
THE
INFLUENCE
OF
EACH
STAGE
OF
THE
COMPUTATION
ON
PERFORMANCE
CONCLUDING
THAT
FINE
SCALE
GRADIENTS
FINE
ORIENTATION
BINNING
RELATIVELY
COARSE
SPATIAL
BINNING
AND
HIGH
QUALITY
LOCAL
CONTRAST
NORMALIZATION
IN
OVERLAPPING
DE
SCRIPTOR
BLOCKS
ARE
ALL
IMPORTANT
FOR
GOOD
RESULTS
THE
NEW
APPROACH
GIVES
NEAR
PERFECT
SEPARATION
ON
THE
ORIGINAL
MIT
PEDESTRIAN
DATABASE
SO
WE
INTRODUCE
A
MORE
CHALLENGING
DATASET
CONTAINING
OVER
ANNOTATED
HUMAN
IMAGES
WITH
A
LARGE
RANGE
OF
POSE
VARIATIONS
AND
BACKGROUNDS
INTRODUCTION
DETECTING
HUMANS
IN
IMAGES
IS
A
CHALLENGING
TASK
OWING
TO
THEIR
VARIABLE
APPEARANCE
AND
THE
WIDE
RANGE
OF
POSES
THAT
THEY
CAN
ADOPT
THE
FIRST
NEED
IS
A
ROBUST
FEATURE
SET
THAT
ALLOWS
THE
HUMAN
FORM
TO
BE
DISCRIMINATED
CLEANLY
EVEN
IN
CLUTTERED
BACKGROUNDS
UNDER
DIFFICULT
ILLUMINATION
WE
STUDY
THE
ISSUE
OF
FEATURE
SETS
FOR
HUMAN
DETECTION
SHOWING
THAT
LO
CALLY
NORMALIZED
HISTOGRAM
OF
ORIENTED
GRADIENT
HOG
DE
SCRIPTORS
PROVIDE
EXCELLENT
PERFORMANCE
RELATIVE
TO
OTHER
EX
ISTING
FEATURE
SETS
INCLUDING
WAVELETS
THE
PROPOSED
DESCRIPTORS
ARE
REMINISCENT
OF
EDGE
ORIENTATION
HISTOGRAMS
SIFT
DESCRIPTORS
AND
SHAPE
CONTEXTS
BUT
THEY
ARE
COMPUTED
ON
A
DENSE
GRID
OF
UNIFORMLY
SPACED
CELLS
AND
THEY
USE
OVERLAPPING
LOCAL
CONTRAST
NORMALIZATIONS
FOR
IM
PROVED
PERFORMANCE
WE
MAKE
A
DETAILED
STUDY
OF
THE
EFFECTS
OF
VARIOUS
IMPLEMENTATION
CHOICES
ON
DETECTOR
PERFORMANCE
TAKING
PEDESTRIAN
DETECTION
THE
DETECTION
OF
MOSTLY
VISIBLE
PEOPLE
IN
MORE
OR
LESS
UPRIGHT
POSES
AS
A
TEST
CASE
FOR
SIM
PLICITY
AND
SPEED
WE
USE
LINEAR
SVM
AS
A
BASELINE
CLASSIFIER
THROUGHOUT
THE
STUDY
THE
NEW
DETECTORS
GIVE
ESSENTIALLY
PER
FECT
RESULTS
ON
THE
MIT
PEDESTRIAN
TEST
SET
SO
WE
HAVE
CREATED
A
MORE
CHALLENGING
SET
CONTAINING
OVER
PEDES
TRIAN
IMAGES
WITH
A
LARGE
RANGE
OF
POSES
AND
BACKGROUNDS
ONGOING
WORK
SUGGESTS
THAT
OUR
FEATURE
SET
PERFORMS
EQUALLY
WELL
FOR
OTHER
SHAPE
BASED
OBJECT
CLASSES
WE
BRIEFLY
DISCUSS
PREVIOUS
WORK
ON
HUMAN
DETECTION
IN
GIVE
AN
OVERVIEW
OF
OUR
METHOD
DESCRIBE
OUR
DATA
SETS
IN
AND
GIVE
A
DETAILED
DESCRIPTION
AND
EXPERIMENTAL
EVALUATION
OF
EACH
STAGE
OF
THE
PROCESS
IN
THE
MAIN
CONCLUSIONS
ARE
SUMMARIZED
IN
PREVIOUS
WORK
THERE
IS
AN
EXTENSIVE
LITERATURE
ON
OBJECT
DETECTION
BUT
HERE
WE
MENTION
JUST
A
FEW
RELEVANT
PAPERS
ON
HUMAN
DETEC
TION
SEE
FOR
A
SURVEY
PAPAGEORGIOU
ET
AL
DESCRIBE
A
PEDESTRIAN
DETECTOR
BASED
ON
A
POLYNOMIAL
SVM
USING
RECTIFIED
HAAR
WAVELETS
AS
INPUT
DESCRIPTORS
WITH
A
PARTS
SUBWINDOW
BASED
VARIANT
IN
DEPOORTERE
ET
AL
GIVE
AN
OPTIMIZED
VERSION
OF
THIS
GAVRILA
PHILOMEN
TAKE
A
MORE
DIRECT
APPROACH
EXTRACTING
EDGE
IMAGES
AND
MATCHING
THEM
TO
A
SET
OF
LEARNED
EXEMPLARS
USING
CHAMFER
DISTANCE
THIS
HAS
BEEN
USED
IN
A
PRACTICAL
REAL
TIME
PEDES
TRIAN
DETECTION
SYSTEM
VIOLA
ET
AL
BUILD
AN
EFFICIENT
MOVING
PERSON
DETECTOR
USING
ADABOOST
TO
TRAIN
A
CHAIN
OF
PROGRESSIVELY
MORE
COMPLEX
REGION
REJECTION
RULES
BASED
ON
HAAR
LIKE
WAVELETS
AND
SPACE
TIME
DIFFERENCES
RONFARD
ET
AL
BUILD
AN
ARTICULATED
BODY
DETECTOR
BY
INCORPORATING
SVM
BASED
LIMB
CLASSIFIERS
OVER
AND
ORDER
GAUSSIAN
FILTERS
IN
A
DYNAMIC
PROGRAMMING
FRAMEWORK
SIMILAR
TO
THOSE
OF
FELZENSZWALB
HUTTENLOCHER
AND
IOFFE
FORSYTH
MIKOLAJCZYK
ET
AL
USE
COMBINATIONS
OF
ORIENTATION
POSITION
HISTOGRAMS
WITH
BINARY
THRESHOLDEDGRADIENT
MAGNI
TUDES
TO
BUILD
A
PARTS
BASED
METHOD
CONTAINING
DETECTORS
FOR
FACES
HEADS
AND
FRONT
AND
SIDE
PROFILES
OF
UPPER
AND
LOWER
BODY
PARTS
IN
CONTRAST
OUR
DETECTOR
USES
A
SIMPLER
ARCHI
TECTURE
WITH
A
SINGLE
DETECTION
WINDOW
BUT
APPEARS
TO
GIVE
SIGNIFICANTLY
HIGHER
PERFORMANCE
ON
PEDESTRIAN
IMAGES
OVERVIEW
OF
THE
METHOD
THIS
SECTION
GIVES
AN
OVERVIEW
OF
OUR
FEATURE
EXTRACTION
CHAIN
WHICH
IS
SUMMARIZED
IN
FIG
IMPLEMENTATION
DETAILS
ARE
POSTPONED
UNTIL
THE
METHOD
IS
BASED
ON
EVALUATING
WELL
NORMALIZED
LOCAL
HISTOGRAMS
OF
IMAGE
GRADIENT
ORIENTA
TIONS
IN
A
DENSE
GRID
SIMILAR
FEATURES
HAVE
SEEN
INCREASING
USE
OVER
THE
PAST
DECADE
THE
BASIC
IDEA
IS
THAT
LOCAL
OBJECT
APPEARANCE
AND
SHAPE
CAN
OFTEN
BE
CHARACTERIZED
RATHER
WELL
BY
THE
DISTRIBUTION
OF
LOCAL
INTENSITY
GRADIENTS
OR
INPUT
IMAGE
PERSON
NON
PERSON
CLASSIFICATION
FIGURE
AN
OVERVIEW
OF
OUR
FEATURE
EXTRACTION
AND
OBJECT
DETECTION
CHAIN
THE
DETECTOR
WINDOW
IS
TILED
WITH
A
GRID
OF
OVERLAPPING
BLOCKS
IN
WHICH
HISTOGRAM
OF
ORIENTED
GRADIENT
FEATURE
VECTORS
ARE
EXTRACTED
THE
COMBINED
VECTORS
ARE
FED
TO
A
LINEAR
SVM
FOR
OBJECT
NON
OBJECT
CLASSIFICATION
THE
DETECTION
WINDOW
IS
SCANNED
ACROSS
THE
IMAGE
AT
ALL
POSITIONS
AND
SCALES
AND
CONVENTIONAL
NON
MAXIMUM
SUPPRESSION
IS
RUN
ON
THE
OUTPUT
PYRAMID
TO
DETECT
OBJECT
INSTANCES
BUT
THIS
PAPER
CONCENTRATES
ON
THE
FEATURE
EXTRACTION
PROCESS
EDGE
DIRECTIONS
EVEN
WITHOUT
PRECISE
KNOWLEDGE
OF
THE
COR
RESPONDING
GRADIENT
OR
EDGE
POSITIONS
IN
PRACTICE
THIS
IS
IM
PLEMENTED
BY
DIVIDING
THE
IMAGE
WINDOW
INTO
SMALL
SPATIAL
REGIONS
CELLS
FOR
EACH
CELL
ACCUMULATING
A
LOCAL
D
HIS
TOGRAM
OF
GRADIENT
DIRECTIONS
OR
EDGE
ORIENTATIONS
OVER
THE
PIXELS
OF
THE
CELL
THE
COMBINED
HISTOGRAM
ENTRIES
FORM
THE
REPRESENTATION
FOR
BETTER
INVARIANCE
TO
ILLUMINATION
SHAD
OWING
ETC
IT
IS
ALSO
USEFUL
TO
CONTRAST
NORMALIZE
THE
LOCAL
RESPONSES
BEFORE
USING
THEM
THIS
CAN
BE
DONE
BY
ACCUMU
LATING
A
MEASURE
OF
LOCAL
HISTOGRAM
ENERGY
OVER
SOMEWHAT
LARGER
SPATIAL
REGIONS
BLOCKS
AND
USING
THE
RESULTS
TO
NOR
MALIZE
ALL
OF
THE
CELLS
IN
THE
BLOCK
WE
WILL
REFER
TO
THE
NOR
MALIZED
DESCRIPTOR
BLOCKS
AS
HISTOGRAM
OF
ORIENTED
GRADI
ENT
HOG
DESCRIPTORS
TILING
THE
DETECTION
WINDOW
WITH
A
DENSE
IN
FACT
OVERLAPPING
GRID
OF
HOG
DESCRIPTORS
AND
USING
THE
COMBINED
FEATURE
VECTOR
IN
A
CONVENTIONAL
SVM
BASED
WINDOW
CLASSIFIER
GIVES
OUR
HUMAN
DETECTION
CHAIN
SEE
FIG
THE
USE
OF
ORIENTATION
HISTOGRAMS
HAS
MANY
PRECURSORS
BUT
IT
ONLY
REACHED
MATURITY
WHEN
COMBINED
WITH
LOCAL
SPATIAL
HISTOGRAMMING
AND
NORMALIZATION
IN
LOWE
SCALE
INVARIANT
FEATURE
TRANSFORMATION
SIFT
APPROACH
TO
WIDE
BASELINE
IMAGE
MATCHING
IN
WHICH
IT
PROVIDES
THE
UNDERLYING
IMAGE
PATCH
DESCRIPTOR
FOR
MATCHING
SCALE
INVARIANT
KEYPOINTS
SIFT
STYLE
APPROACHES
PERFORM
REMARK
ABLY
WELL
IN
THIS
APPLICATION
THE
SHAPE
CONTEXT
WORK
STUDIED
ALTERNATIVE
CELL
AND
BLOCK
SHAPES
ALBEIT
INI
TIALLY
USING
ONLY
EDGE
PIXEL
COUNTS
WITHOUT
THE
ORIENTATION
HISTOGRAMMING
THAT
MAKES
THE
REPRESENTATION
SO
EFFECTIVE
THE
SUCCESS
OF
THESE
SPARSE
FEATURE
BASED
REPRESENTATIONS
HAS
SOMEWHAT
OVERSHADOWED
THE
POWER
AND
SIMPLICITY
OF
HOG
AS
DENSE
IMAGE
DESCRIPTORS
WE
HOPE
THAT
OUR
STUDY
WILL
HELP
TO
RECTIFY
THIS
IN
PARTICULAR
OUR
INFORMAL
EXPERIMENTS
SUG
GEST
THAT
EVEN
THE
BEST
CURRENT
KEYPOINT
BASED
APPROACHES
ARE
LIKELY
TO
HAVE
FALSE
POSITIVE
RATES
AT
LEAST
ORDERS
OF
MAG
NITUDE
HIGHER
THAN
OUR
DENSE
GRID
APPROACH
FOR
HUMAN
DETEC
TION
MAINLY
BECAUSE
NONE
OF
THE
KEYPOINT
DETECTORS
THAT
WE
ARE
AWARE
OF
DETECT
HUMAN
BODY
STRUCTURES
RELIABLY
THE
HOG
SIFT
REPRESENTATION
HAS
SEVERAL
ADVANTAGES
IT
CAPTURES
EDGE
OR
GRADIENT
STRUCTURE
THAT
IS
VERY
CHARACTERISTIC
OF
LOCAL
SHAPE
AND
IT
DOES
SO
IN
A
LOCAL
REPRESENTATION
WITH
COARSE
SPATIAL
SAMPLING
FINE
ORIENTATION
SAMPLING
AND
STRONG
LOCAL
PHOTOMETRIC
NORMALIZATION
TURNS
OUT
TO
BE
THE
BEST
STRAT
EGY
PRESUMABLY
BECAUSE
IT
PERMITS
LIMBS
AND
BODY
SEGMENTS
TO
CHANGE
APPEARANCE
AND
MOVE
FROM
SIDE
TO
SIDE
QUITE
A
LOT
PROVIDED
THAT
THEY
MAINTAIN
A
ROUGHLY
UPRIGHT
ORIENTATION
DATA
SETS
AND
METHODOLOGY
DATASETS
WE
TESTED
OUR
DETECTOR
ON
TWO
DIFFERENT
DATA
SETS
THE
FIRST
IS
THE
WELL
ESTABLISHED
MIT
PEDESTRIAN
DATABASE
CONTAINING
TRAINING
AND
TEST
IMAGES
OF
PEDESTRI
ANS
IN
CITY
SCENES
PLUS
LEFT
RIGHT
REFLECTIONS
OF
THESE
IT
CON
TAINS
ONLY
FRONT
OR
BACK
VIEWS
WITH
A
RELATIVELY
LIMITED
RANGE
OF
POSES
OUR
BEST
DETECTORS
GIVE
ESSENTIALLY
PERFECT
RESULTS
ON
THIS
DATA
SET
SO
WE
PRODUCED
A
NEW
AND
SIGNIFICANTLY
MORE
CHALLENGING
DATA
SET
INRIA
CONTAINING
IM
AGES
OF
HUMANS
CROPPED
FROM
A
VARIED
SET
OF
PERSONAL
PHO
TOS
FIG
SHOWS
SOME
SAMPLES
THE
PEOPLE
ARE
USUALLY
STANDING
BUT
APPEAR
IN
ANY
ORIENTATION
AND
AGAINST
A
WIDE
VARIETY
OF
BACKGROUND
IMAGE
INCLUDING
CROWDS
MANY
ARE
BYSTANDERS
TAKEN
FROM
THE
IMAGE
BACKGROUNDS
SO
THERE
IS
NO
PARTICULAR
BIAS
ON
THEIR
POSE
THE
DATABASE
IS
AVAILABLE
FROM
FOR
RESEARCH
PURPOSES
METHODOLOGY
WE
SELECTED
OF
THE
IMAGES
AS
POSITIVE
TRAINING
EXAMPLES
TOGETHER
WITH
THEIR
LEFT
RIGHT
REFLECTIONS
IMAGES
IN
ALL
A
FIXED
SET
OF
PATCHES
SAMPLED
RANDOMLY
FROM
PERSON
FREE
TRAINING
PHOTOS
PROVIDED
THE
INITIAL
NEGATIVE
SET
FOR
EACH
DETECTOR
AND
PARAMETER
COM
BINATION
A
PRELIMINARY
DETECTOR
IS
TRAINED
AND
THE
NEGA
TIVE
TRAINING
PHOTOS
ARE
SEARCHED
EXHAUSTIVELY
FOR
FALSE
POSI
TIVES
HARD
EXAMPLES
THE
METHOD
IS
THEN
RE
TRAINED
USING
THIS
AUGMENTED
SET
INITIAL
HARD
EXAMPLES
TO
PRO
DUCE
THE
FINAL
DETECTOR
THE
SET
OF
HARD
EXAMPLES
IS
SUBSAM
PLED
IF
NECESSARY
SO
THAT
THE
DESCRIPTORS
OF
THE
FINAL
TRAINING
SET
FIT
INTO
GB
OF
RAM
FOR
SVM
TRAINING
THIS
RETRAIN
ING
PROCESS
SIGNIFICANTLY
IMPROVES
THE
PERFORMANCE
OF
EACH
DETECTOR
BY
AT
FALSE
POSITIVES
PER
WINDOW
TESTED
FPPW
FOR
OUR
DEFAULT
DETECTOR
BUT
ADDITIONAL
ROUNDS
OF
RETRAINING
MAKE
LITTLE
DIFFERENCE
SO
WE
DO
NOT
USE
THEM
TO
QUANTIFY
DETECTOR
PERFORMANCE
WE
PLOT
DETECTION
ER
ROR
TRADEOFF
DET
CURVES
ON
A
LOG
LOG
SCALE
I
E
MISS
RATE
AN
EASILY
CONTROLLABLE
DEGREE
OF
INVARIANCE
TO
LOCAL
GEOMETRIC
FALSENEG
TRUEPOS
FALSENEG
VERSUS
FPPW
LOWER
VAL
AND
PHOTOMETRIC
TRANSFORMATIONS
TRANSLATIONS
OR
ROTATIONS
MAKE
LITTLE
DIFFERENCE
IF
THEY
ARE
MUCH
SMALLER
THAT
THE
LOCAL
SPATIAL
OR
ORIENTATION
BIN
SIZE
FOR
HUMAN
DETECTION
RATHER
UES
ARE
BETTER
DET
PLOTS
ARE
USED
EXTENSIVELY
IN
SPEECH
AND
IN
NIST
EVALUATIONS
THEY
PRESENT
THE
SAME
INFORMATION
AS
RECEIVER
OPERATING
CHARACTERISTICS
ROC
BUT
ALLOW
SMALL
FIGURE
SOME
SAMPLE
IMAGES
FROM
OUR
NEW
HUMAN
DETECTION
DATABASE
THE
SUBJECTS
ARE
ALWAYS
UPRIGHT
BUT
WITH
SOME
PARTIAL
OCCLUSIONS
AND
A
WIDE
RANGE
OF
VARIATIONS
IN
POSE
APPEARANCE
CLOTHING
ILLUMINATION
AND
BACKGROUND
PROBABILITIES
TO
BE
DISTINGUISHED
MORE
EASILY
WE
WILL
OFTEN
USE
MISS
RATE
AT
AS
A
REFERENCE
POINT
FOR
RESULTS
THIS
IS
ARBITRARY
BUT
NO
MORE
SO
THAN
E
G
AREA
UNDER
ROC
IN
A
MULTISCALE
DETECTOR
IT
CORRESPONDS
TO
A
RAW
ERROR
RATE
OF
ABOUT
FALSE
POSITIVES
PER
IMAGE
TESTED
THE
FULL
DETECTOR
HAS
AN
EVEN
LOWER
FALSE
POSITIVE
RATE
OWING
TO
NON
MAXIMUM
SUPPRESSION
OUR
DET
CURVES
ARE
USUALLY
QUITE
SHALLOW
SO
EVEN
VERY
SMALL
IMPROVEMENTS
IN
MISS
RATE
ARE
EQUIVALENT
TO
LARGE
GAINS
IN
FPPW
AT
CONSTANT
MISS
RATE
FOR
EXAMPLE
FOR
OUR
DEFAULT
DETECTOR
AT
FPPW
EVERY
ABSOLUTE
RELATIVE
REDUCTION
IN
MISS
RATE
IS
EQUIVALENT
TO
REDUCING
THE
FPPW
AT
CONSTANT
MISS
RATE
BY
A
FACTOR
OF
OVERVIEW
OF
RESULTS
BEFORE
PRESENTING
OUR
DETAILED
IMPLEMENTATION
AND
PER
FORMANCE
ANALYSIS
WE
COMPARE
THE
OVERALL
PERFORMANCE
OF
OUR
FINAL
HOG
DETECTORS
WITH
THAT
OF
SOME
OTHER
EXISTING
METHODS
DETECTORS
BASED
ON
RECTANGULAR
R
HOG
OR
CIR
CULAR
LOG
POLAR
C
HOG
BLOCKS
AND
LINEAR
OR
KERNEL
SVM
ARE
COMPARED
WITH
OUR
IMPLEMENTATIONS
OF
THE
HAAR
WAVELET
PCA
SIFT
AND
SHAPE
CONTEXT
APPROACHES
BRIEFLY
THESE
AP
PROACHES
ARE
AS
FOLLOWS
GENERALIZED
HAAR
WAVELETS
THIS
IS
AN
EXTENDED
SET
OF
ORI
ENTED
HAAR
LIKE
WAVELETS
SIMILAR
TO
BUT
BETTER
THAN
THAT
USED
IN
THE
FEATURES
ARE
RECTIFIED
RESPONSES
FROM
AND
ORIENTED
AND
DERIVATIVE
BOX
FILTERS
AT
INTER
STRENGTH
AND
EDGE
PRESENCE
BASED
VOTING
WERE
TESTED
WITH
THE
EDGE
THRESHOLD
CHOSEN
AUTOMATICALLY
TO
MAXIMIZE
DETEC
TION
PERFORMANCE
THE
VALUES
SELECTED
WERE
SOMEWHAT
VARI
ABLE
IN
THE
REGION
OF
GRAYLEVELS
RESULTS
FIG
SHOWS
THE
PERFORMANCE
OF
THE
VARIOUS
DETEC
TORS
ON
THE
MIT
AND
INRIA
DATA
SETS
THE
HOG
BASED
DE
TECTORS
GREATLY
OUTPERFORM
THE
WAVELET
PCA
SIFT
AND
SHAPE
CONTEXT
ONES
GIVING
NEAR
PERFECT
SEPARATION
ON
THE
MIT
TEST
SET
AND
AT
LEAST
AN
ORDER
OF
MAGNITUDE
REDUCTION
IN
FPPW
ON
THE
INRIA
ONE
OUR
HAAR
LIKE
WAVELETS
OUTPERFORM
MIT
WAVELETS
BECAUSE
WE
ALSO
USE
ORDER
DERIVATIVES
AND
CON
TRAST
NORMALIZE
THE
OUTPUT
VECTOR
FIG
A
ALSO
SHOWS
MIT
BEST
PARTS
BASED
AND
MONOLITHIC
DETECTORS
THE
POINTS
ARE
IN
TERPOLATED
FROM
HOWEVER
BEWARE
THAT
AN
EXACT
COMPAR
ISON
IS
NOT
POSSIBLE
AS
WE
DO
NOT
KNOW
HOW
THE
DATABASE
IN
WAS
DIVIDED
INTO
TRAINING
AND
TEST
PARTS
AND
THE
NEGA
TIVE
IMAGES
USED
ARE
NOT
AVAILABLE
THE
PERFORMANCES
OF
THE
FINAL
RECTANGULAR
R
HOG
AND
CIRCULAR
C
HOG
DETECTORS
ARE
VERY
SIMILAR
WITH
C
HOG
HAVING
THE
SLIGHT
EDGE
AUG
MENTING
R
HOG
WITH
PRIMITIVE
BAR
DETECTORS
ORIENTED
DERIVATIVES
HOG
DOUBLES
THE
FEATURE
DIMENSION
BUT
FURTHER
IMPROVES
THE
PERFORMANCE
BY
AT
FPPW
REPLACING
THE
LINEAR
SVM
WITH
A
GAUSSIAN
KERNEL
ONE
IM
PROVES
PERFORMANCE
BY
ABOUT
AT
FPPW
AT
THE
COST
OF
MUCH
HIGHER
RUN
USING
BINARY
EDGE
VOTING
EC
HOG
INSTEAD
OF
GRADIENT
MAGNITUDE
WEIGHTED
VOTING
C
VALS
AND
THE
CORRESPONDING
DERIVATIVE
XY
FILTER
HOG
DECREASES
PERFORMANCE
BY
AT
FPPW
WHILE
OMITTING
ORIENTATION
INFORMATION
DECREASES
IT
BY
MUCH
MORE
PCA
SIFT
THESE
DESCRIPTORS
ARE
BASED
ON
PROJECTING
GRADI
ENT
IMAGES
ONTO
A
BASIS
LEARNED
FROM
TRAINING
IMAGES
USING
PCA
KE
SUKTHANKAR
FOUND
THAT
THEY
OUTPERFORMED
SIFT
FOR
KEY
POINT
BASED
MATCHING
BUT
THIS
IS
CONTROVERSIAL
OUR
IMPLEMENTATION
USES
BLOCKS
WITH
THE
SAME
DERIVATIVE
SCALE
OVERLAP
ETC
SETTINGS
AS
OUR
HOG
DESCRIP
TORS
THE
PCA
BASIS
IS
CALCULATED
USING
POSITIVE
TRAINING
IM
AGES
SHAPE
CONTEXTS
THE
ORIGINAL
SHAPE
CONTEXTS
USED
BI
NARY
EDGE
PRESENCE
VOTING
INTO
LOG
POLAR
SPACED
BINS
IRRE
SPECTIVE
OF
EDGE
ORIENTATION
WE
SIMULATE
THIS
USING
OUR
C
HOG
DESCRIPTOR
SEE
BELOW
WITH
JUST
ORIENTATION
BIN
ANGULAR
AND
RADIAL
INTERVALS
WITH
INNER
RADIUS
PIXELS
AND
OUTER
RADIUS
PIXELS
GAVE
THE
BEST
RESULTS
BOTH
GRADIENT
EVEN
IF
ADDITIONAL
SPATIAL
OR
RADIAL
BINS
ARE
ADDED
BY
AT
FPPW
FOR
BOTH
EDGES
E
SHAPEC
AND
GRADIENTS
G
SHAPEC
PCA
SIFT
ALSO
PERFORMS
POORLY
ONE
REASON
IS
THAT
IN
COMPARISON
TO
MANY
MORE
OF
PRINCIPAL
VECTORS
HAVE
TO
BE
RETAINED
TO
CAPTURE
THE
SAME
PROPORTION
OF
THE
VARIANCE
THIS
MAY
BE
BECAUSE
THE
SPATIAL
REGISTRATION
IS
WEAKER
WHEN
THERE
IS
NO
KEYPOINT
DETECTOR
IMPLEMENTATION
AND
PERFORMANCE
STUDY
WE
NOW
GIVE
DETAILS
OF
OUR
HOG
IMPLEMENTATIONS
AND
SYSTEMATICALLY
STUDY
THE
EFFECTS
OF
THE
VARIOUS
CHOICES
ON
DE
USE
THE
HARD
EXAMPLES
GENERATED
BY
LINEAR
R
HOG
TO
TRAIN
THE
KER
NEL
R
HOG
DETECTOR
AS
KERNEL
R
HOG
GENERATES
SO
FEW
FALSE
POSITIVES
THAT
ITS
HARD
EXAMPLE
SET
IS
TOO
SPARSE
TO
IMPROVE
THE
GENERALIZATION
SIGNIFICANTLY
DET
DIFFERENT
DESCRIPTORS
ON
MIT
DATABASE
DET
DIFFERENT
DESCRIPTORS
ON
INRIA
DATABASE
FALSE
POSITIVES
PER
WINDOW
FPPW
FALSE
POSITIVES
PER
WINDOW
FPPW
FIGURE
THE
PERFORMANCE
OF
SELECTED
DETECTORS
ON
LEFT
MIT
AND
RIGHT
INRIA
DATA
SETS
SEE
THE
TEXT
FOR
DETAILS
TECTOR
PERFORMANCE
THROUGHOUT
THIS
SECTION
WE
REFER
RESULTS
TO
OUR
DEFAULT
DETECTOR
WHICH
HAS
THE
FOLLOWING
PROPERTIES
DESCRIBED
BELOW
RGB
COLOUR
SPACE
WITH
NO
GAMMA
COR
RECTION
GRADIENT
FILTER
WITH
NO
SMOOTHING
LINEAR
GRADIENT
VOTING
INTO
ORIENTATION
BINS
IN
TIVE
MASKS
SEVERAL
SMOOTHING
SCALES
WERE
TESTED
INCLUD
ING
Σ
NONE
MASKS
TESTED
INCLUDED
VARIOUS
D
POINT
DERIVATIVES
UNCENTRED
CENTRED
AND
CUBIC
CORRECTED
AS
WELL
AS
SOBEL
MASKS
AND
DIAGONAL
ONES
THE
MOST
COMPACT
CEN
PIXEL
BLOCKS
OF
FOUR
PIXEL
CELLS
GAUSSIAN
SPATIAL
WIN
DOW
WITH
Σ
PIXEL
HYS
LOWE
STYLE
CLIPPED
NORM
BLOCK
NORMALIZATION
BLOCK
SPACING
STRIDE
OF
PIXELS
HENCE
FOLD
COVERAGE
OF
EACH
CELL
DETECTION
WINDOW
LINEAR
SVM
CLASSIFIER
FIG
SUMMARIZES
THE
EFFECTS
OF
THE
VARIOUS
HOG
PARAM
ETERS
ON
OVERALL
DETECTION
PERFORMANCE
THESE
WILL
BE
EXAM
INED
IN
DETAIL
BELOW
THE
MAIN
CONCLUSIONS
ARE
THAT
FOR
GOOD
PERFORMANCE
ONE
SHOULD
USE
FINE
SCALE
DERIVATIVES
ESSEN
TIALLY
NO
SMOOTHING
MANY
ORIENTATION
BINS
AND
MODERATELY
SIZED
STRONGLY
NORMALIZED
OVERLAPPING
DESCRIPTOR
BLOCKS
GAMMA
COLOUR
NORMALIZATION
WE
EVALUATED
SEVERAL
INPUT
PIXEL
REPRESENTATIONS
INCLUD
ING
GRAYSCALE
RGB
AND
LAB
COLOUR
SPACES
OPTIONALLY
WITH
POWER
LAW
GAMMA
EQUALIZATION
THESE
NORMALIZATIONS
HAVE
ONLY
A
MODEST
EFFECT
ON
PERFORMANCE
PERHAPS
BECAUSE
THE
SUBSEQUENT
DESCRIPTOR
NORMALIZATION
ACHIEVES
SIMILAR
RESULTS
WE
DO
USE
COLOUR
INFORMATION
WHEN
AVAILABLE
RGB
AND
LAB
COLOUR
SPACES
GIVE
COMPARABLE
RESULTS
BUT
RESTRICTING
TO
GRAYSCALE
REDUCES
PERFORMANCE
BY
AT
FPPW
SQUARE
ROOT
GAMMA
COMPRESSION
OF
EACH
COLOUR
CHANNEL
IM
PROVES
PERFORMANCE
AT
LOW
FPPW
BY
AT
FPPW
BUT
LOG
COMPRESSION
IS
TOO
STRONG
AND
WORSENS
IT
BY
AT
FPPW
GRADIENT
COMPUTATION
DETECTOR
PERFORMANCE
IS
SENSITIVE
TO
THE
WAY
IN
WHICH
GRADIENTS
ARE
COMPUTED
BUT
THE
SIMPLEST
SCHEME
TURNS
OUT
TO
BE
THE
BEST
WE
TESTED
GRADIENTS
COMPUTED
USING
GAUS
SIAN
SMOOTHING
FOLLOWED
BY
ONE
OF
SEVERAL
DISCRETE
DERIVA
TRED
D
DERIVATIVE
MASKS
SIMPLE
D
MASKS
AT
Σ
WORK
BEST
USING
LARGER
MASKS
ALWAYS
SEEMS
TO
DE
CREASE
PERFORMANCE
AND
SMOOTHING
DAMAGES
IT
SIGNIFICANTLY
FOR
GAUSSIAN
DERIVATIVES
MOVING
FROM
Σ
TO
Σ
REDUCES
THE
RECALL
RATE
FROM
TO
AT
FPPW
AT
Σ
CUBIC
CORRECTED
D
WIDTH
FILTERS
ARE
ABOUT
WORSE
THAN
AT
FPPW
WHILE
THE
DIAGONAL
MASKS
ARE
WORSE
USING
UNCENTRED
DERIVATIVE
MASKS
ALSO
DECREASES
PERFORMANCE
BY
AT
FPPW
PRESUM
ABLY
BECAUSE
ORIENTATION
ESTIMATION
SUFFERS
AS
A
RESULT
OF
THE
X
AND
Y
FILTERS
BEING
BASED
AT
DIFFERENT
CENTRES
FOR
COLOUR
IMAGES
WE
CALCULATE
SEPARATE
GRADIENTS
FOR
EACH
COLOUR
CHANNEL
AND
TAKE
THE
ONE
WITH
THE
LARGEST
NORM
AS
THE
PIXEL
GRADIENT
VECTOR
SPATIAL
ORIENTATION
BINNING
THE
NEXT
STEP
IS
THE
FUNDAMENTAL
NONLINEARITY
OF
THE
DE
SCRIPTOR
EACH
PIXEL
CALCULATES
A
WEIGHTED
VOTE
FOR
AN
EDGE
ORIENTATION
HISTOGRAM
CHANNEL
BASED
ON
THE
ORIENTATION
OF
THE
GRADIENT
ELEMENT
CENTRED
ON
IT
AND
THE
VOTES
ARE
ACCUMU
LATED
INTO
ORIENTATION
BINS
OVER
LOCAL
SPATIAL
REGIONS
THAT
WE
CALL
CELLS
CELLS
CAN
BE
EITHER
RECTANGULAR
OR
RADIAL
LOG
POLAR
SECTORS
THE
ORIENTATION
BINS
ARE
EVENLY
SPACED
OVER
UNSIGNED
GRADIENT
OR
SIGNED
GRADIENT
TO
REDUCE
ALIASING
VOTES
ARE
INTERPOLATED
BILINEARLY
BETWEEN
THE
NEIGHBOURING
BIN
CENTRES
IN
BOTH
ORIENTATION
AND
POSI
TION
THE
VOTE
IS
A
FUNCTION
OF
THE
GRADIENT
MAGNITUDE
AT
THE
PIXEL
EITHER
THE
MAGNITUDE
ITSELF
ITS
SQUARE
ITS
SQUARE
ROOT
OR
A
CLIPPED
FORM
OF
THE
MAGNITUDE
REPRESENTING
SOFT
PRES
ENCE
ABSENCE
OF
AN
EDGE
AT
THE
PIXEL
IN
PRACTICE
USING
THE
DET
EFFECT
OF
GRADIENT
SCALE
DET
EFFECT
OF
NUMBER
OF
ORIENTATION
BINS
DET
EFFECT
OF
NORMALIZATION
METHODS
FALSE
POSITIVES
PER
WINDOW
FPPW
FALSE
POSITIVES
PER
WINDOW
FPPW
FALSE
POSITIVES
PER
WINDOW
FPPW
B
C
DET
EFFECT
OF
OVERLAP
CELL
SIZE
NUM
CELL
WT
FALSE
POSITIVES
PER
WINDOW
FPPW
DET
EFFECT
OF
WINDOW
SIZE
FALSE
POSITIVES
PER
WINDOW
FPPW
DET
EFFECT
OF
KERNEL
WIDTH
ON
KERNEL
SVM
FALSE
POSITIVES
PER
WINDOW
FPPW
E
F
FIGURE
FOR
DETAILS
SEE
THE
TEXT
A
USING
FINE
DERIVATIVE
SCALE
SIGNIFICANTLY
INCREASES
THE
PERFORMANCE
C
COR
IS
THE
CUBIC
CORRECTED
POINT
DERIVATIVE
B
INCREASING
THE
NUMBER
OF
ORIENTATION
BINS
INCREASES
PERFORMANCE
SIGNIFICANTLY
UP
TO
ABOUT
BINS
SPACED
OVER
C
THE
EFFECT
OF
DIFFERENT
BLOCK
NORMALIZATION
SCHEMES
SEE
D
USING
OVERLAPPING
DESCRIPTOR
BLOCKS
DECREASES
THE
MISS
RATE
BY
AROUND
E
REDUCING
THE
PIXEL
MARGIN
AROUND
THE
DETECTION
WINDOW
DECREASES
THE
PERFORMANCE
BY
ABOUT
F
USING
A
GAUSSIAN
KERNEL
SVM
EXP
Γ
IMPROVES
THE
PERFORMANCE
BY
ABOUT
MAGNITUDE
ITSELF
GIVES
THE
BEST
RESULTS
TAKING
THE
SQUARE
ROOT
REDUCES
PERFORMANCE
SLIGHTLY
WHILE
USING
BINARY
EDGE
PRES
ENCE
VOTING
DECREASES
IT
SIGNIFICANTLY
BY
AT
FPPW
FINE
ORIENTATION
CODING
TURNS
OUT
TO
BE
ESSENTIAL
FOR
GOOD
PERFORMANCE
WHEREAS
SEE
BELOW
SPATIAL
BINNING
CAN
BE
RATHER
COARSE
AS
FIG
B
SHOWS
INCREASING
THE
NUMBER
OF
ORIENTATION
BINS
IMPROVES
PERFORMANCE
SIGNIFICANTLY
UP
TO
ABOUT
BINS
BUT
MAKES
LITTLE
DIFFERENCE
BEYOND
THIS
THIS
IS
FOR
BINS
SPACED
OVER
I
E
THE
SIGN
OF
THE
GRADI
ENT
IS
IGNORED
INCLUDING
SIGNED
GRADIENTS
ORIENTATION
RANGE
AS
IN
THE
ORIGINAL
SIFT
DESCRIPTOR
DECREASES
THE
PERFORMANCE
EVEN
WHEN
THE
NUMBER
OF
BINS
IS
ALSO
DOUBLED
TO
PRESERVE
THE
ORIGINAL
ORIENTATION
RESOLUTION
FOR
HUMANS
CELL
SIZE
PIXELS
BLOCK
SIZE
CELLS
THE
WIDE
RANGE
OF
CLOTHING
AND
BACKGROUND
COLOURS
PRESUM
ABLY
MAKES
THE
SIGNS
OF
CONTRASTS
UNINFORMATIVE
HOWEVER
NOTE
THAT
INCLUDING
SIGN
INFORMATION
DOES
HELP
SUBSTANTIALLY
IN
SOME
OTHER
OBJECT
RECOGNITION
TASKS
E
G
CARS
MOTORBIKES
NORMALIZATION
AND
DESCRIPTOR
BLOCKS
GRADIENT
STRENGTHS
VARY
OVER
A
WIDE
RANGE
OWING
TO
LOCAL
VARIATIONS
IN
ILLUMINATION
AND
FOREGROUND
BACKGROUND
CON
TRAST
SO
EFFECTIVE
LOCAL
CONTRAST
NORMALIZATION
TURNS
OUT
TO
BE
ESSENTIAL
FOR
GOOD
PERFORMANCE
WE
EVALUATED
A
NUM
FIGURE
THE
MISS
RATE
AT
FPPW
AS
THE
CELL
AND
BLOCK
SIZES
CHANGE
THE
STRIDE
BLOCK
OVERLAP
IS
FIXED
AT
HALF
OF
THE
BLOCK
SIZE
BLOCKS
OF
PIXEL
CELLS
PERFORM
BEST
WITH
MISS
RATE
BER
OF
DIFFERENT
NORMALIZATION
SCHEMES
MOST
OF
THEM
ARE
BASED
ON
GROUPING
CELLS
INTO
LARGER
SPATIAL
BLOCKS
AND
CON
TRAST
NORMALIZING
EACH
BLOCK
SEPARATELY
THE
FINAL
DESCRIPTOR
IS
THEN
THE
VECTOR
OF
ALL
COMPONENTS
OF
THE
NORMALIZED
CELL
RESPONSES
FROM
ALL
OF
THE
BLOCKS
IN
THE
DETECTION
WINDOW
IN
FACT
WE
TYPICALLY
OVERLAP
THE
BLOCKS
SO
THAT
EACH
SCALAR
CELL
RESPONSE
CONTRIBUTES
SEVERAL
COMPONENTS
TO
THE
FINAL
DE
SCRIPTOR
VECTOR
EACH
NORMALIZED
WITH
RESPECT
TO
A
DIFFERENT
BLOCK
THIS
MAY
SEEM
REDUNDANT
BUT
GOOD
NORMALIZATION
IS
CRITICAL
AND
INCLUDING
OVERLAP
SIGNIFICANTLY
IMPROVES
THE
PER
FORMANCE
FIG
D
SHOWS
THAT
PERFORMANCE
INCREASES
BY
AT
FPPW
AS
WE
INCREASE
THE
OVERLAP
FROM
NONE
STRIDE
TO
FOLD
AREA
FOLD
LINEAR
COVERAGE
STRIDE
WE
EVALUATED
TWO
CLASSES
OF
BLOCK
GEOMETRIES
SQUARE
OR
RECTANGULAR
ONES
PARTITIONED
INTO
GRIDS
OF
SQUARE
OR
RECTANGU
LAR
SPATIAL
CELLS
AND
CIRCULAR
BLOCKS
PARTITIONED
INTO
CELLS
IN
LOG
POLAR
FASHION
WE
WILL
REFER
TO
THESE
TWO
ARRANGEMENTS
AS
R
HOG
AND
C
HOG
FOR
RECTANGULAR
AND
CIRCULAR
HOG
R
HOG
R
HOG
BLOCKS
HAVE
MANY
SIMILARITIES
TO
SIFT
DE
SCRIPTORS
BUT
THEY
ARE
USED
QUITE
DIFFERENTLY
THEY
ARE
COMPUTED
IN
DENSE
GRIDS
AT
A
SINGLE
SCALE
WITHOUT
DOMINANT
ORIENTATION
ALIGNMENT
AND
USED
AS
PART
OF
A
LARGER
CODE
VECTOR
THAT
IMPLICITLY
ENCODES
SPATIAL
POSITION
RELATIVE
TO
THE
DETEC
TION
WINDOW
WHEREAS
SIFT
ARE
COMPUTED
AT
A
SPARSE
SET
OF
SCALE
INVARIANT
KEY
POINTS
ROTATED
TO
ALIGN
THEIR
DOMINANT
ORIENTATIONS
AND
USED
INDIVIDUALLY
SIFT
ARE
OPTIMIZED
FOR
SPARSE
WIDE
BASELINE
MATCHING
R
HOG
FOR
DENSE
ROBUST
CODING
OF
SPATIAL
FORM
OTHER
PRECURSORS
INCLUDE
THE
EDGE
ORIENTATION
HISTOGRAMS
OF
FREEMAN
ROTH
WE
USUALLY
USE
SQUARE
R
HOG
I
E
Σ
Σ
GRIDS
OF
Η
Η
PIXEL
CELLS
EACH
CONTAINING
Β
ORIENTATION
BINS
WHERE
Σ
Η
Β
ARE
PARAMETERS
FIG
PLOTS
THE
MISS
RATE
AT
FPPW
W
R
T
CELL
SIZE
AND
BLOCK
SIZE
IN
CELLS
FOR
HUMAN
DETECTION
CELL
BLOCKS
OF
PIXEL
CELLS
PERFORM
BEST
WITH
MISS
RATE
AT
FPPW
IN
FACT
PIXEL
WIDE
CELLS
DO
BEST
IRRESPEC
TIVE
OF
THE
BLOCK
SIZE
AN
INTERESTING
COINCIDENCE
AS
HUMAN
LIMBS
ARE
ABOUT
PIXELS
ACROSS
IN
OUR
IMAGES
AND
BLOCKS
WORK
BEST
BEYOND
THIS
THE
RESULTS
DETERIORATE
ADAPTIVITY
TO
LOCAL
IMAGING
CONDITIONS
IS
WEAKENED
WHEN
THE
BLOCK
BECOMES
TOO
BIG
AND
WHEN
IT
IS
TOO
SMALL
BLOCK
NORMALIZATION
OVER
ORIENTATIONS
ALONE
VALUABLE
SPATIAL
IN
FORMATION
IS
SUPPRESSED
AS
IN
IT
IS
USEFUL
TO
DOWNWEIGHT
PIXELS
NEAR
THE
EDGES
OF
THE
BLOCK
BY
APPLYING
A
GAUSSIAN
SPATIAL
WINDOW
TO
EACH
PIXEL
BEFORE
ACCUMULATING
ORIENTATION
VOTES
INTO
CELLS
THIS
IMPROVES
PERFORMANCE
BY
AT
FPPW
FOR
A
GAUSSIAN
C
HOG
OUR
CIRCULAR
BLOCK
C
HOG
DESCRIPTORS
ARE
REM
INISCENT
OF
SHAPE
CONTEXTS
EXCEPT
THAT
CRUCIALLY
EACH
SPATIAL
CELL
CONTAINS
A
STACK
OF
GRADIENT
WEIGHTED
ORIENTA
TION
CELLS
INSTEAD
OF
A
SINGLE
ORIENTATION
INDEPENDENT
EDGE
PRESENCE
COUNT
THE
LOG
POLAR
GRID
WAS
ORIGINALLY
SUGGESTED
BY
THE
IDEA
THAT
IT
WOULD
ALLOW
FINE
CODING
OF
NEARBY
STRUC
TURE
TO
BE
COMBINED
WITH
COARSER
CODING
OF
WIDER
CONTEXT
AND
THE
FACT
THAT
THE
TRANSFORMATION
FROM
THE
VISUAL
FIELD
TO
THE
CORTEX
IN
PRIMATES
IS
LOGARITHMIC
HOWEVER
SMALL
DESCRIPTORS
WITH
VERY
FEW
RADIAL
BINS
TURN
OUT
TO
GIVE
THE
BEST
PERFORMANCE
SO
IN
PRACTICE
THERE
IS
LITTLE
INHOMOGENEITY
OR
CONTEXT
IT
IS
PROBABLY
BETTER
TO
THINK
OF
C
HOG
SIMPLY
AS
AN
ADVANCED
FORM
OF
CENTRE
SURROUND
CODING
WE
EVALUATED
TWO
VARIANTS
OF
THE
C
HOG
GEOMETRY
ONES
WITH
A
SINGLE
CIRCULAR
CENTRAL
CELL
SIMILAR
TO
THE
GLOH
FEATURE
OF
AND
ONES
WHOSE
CEN
TRAL
CELL
IS
DIVIDED
INTO
ANGULAR
SECTORS
AS
IN
SHAPE
CONTEXTS
WE
PRESENT
RESULTS
ONLY
FOR
THE
CIRCULAR
CENTRE
VARIANTS
AS
THESE
HAVE
FEWER
SPATIAL
CELLS
THAN
THE
DIVIDED
CENTRE
ONES
AND
GIVE
THE
SAME
PER
FORMANCE
IN
PRACTICE
A
TECHNICAL
REPORT
WILL
PROVIDE
FUR
THER
DETAILS
THE
C
HOG
LAYOUT
HAS
FOUR
PARAMETERS
THE
NUMBERS
OF
ANGULAR
AND
RADIAL
BINS
THE
RADIUS
OF
THE
CENTRAL
BIN
IN
PIXELS
AND
THE
EXPANSION
FACTOR
FOR
SUBSEQUENT
RADII
AT
LEAST
TWO
RADIAL
BINS
A
CENTRE
AND
A
SURROUND
AND
FOUR
ANGULAR
BINS
QUARTERING
ARE
NEEDED
FOR
GOOD
PERFORMANCE
INCLUDING
ADDITIONAL
RADIAL
BINS
DOES
NOT
CHANGE
THE
PERFOR
MANCE
MUCH
WHILE
INCREASING
THE
NUMBER
OF
ANGULAR
BINS
DECREASES
PERFORMANCE
BY
AT
FPPW
WHEN
GO
ING
FROM
TO
ANGULAR
BINS
PIXELS
IS
THE
BEST
RADIUS
FOR
THE
CENTRAL
BIN
BUT
AND
GIVE
SIMILAR
RESULTS
INCREAS
ING
THE
EXPANSION
FACTOR
FROM
TO
LEAVES
THE
PERFORMANCE
ESSENTIALLY
UNCHANGED
WITH
THESE
PARAMETERS
NEITHER
GAUS
SIAN
SPATIAL
WEIGHTING
NOR
INVERSE
WEIGHTING
OF
CELL
VOTES
BY
CELL
AREA
CHANGES
THE
PERFORMANCE
BUT
COMBINING
THESE
TWO
REDUCES
SLIGHTLY
THESE
VALUES
ASSUME
FINE
ORIENTATION
SAM
PLING
SHAPE
CONTEXTS
ORIENTATION
BIN
REQUIRE
MUCH
FINER
SPATIAL
SUBDIVISION
TO
WORK
WELL
BLOCK
NORMALIZATION
SCHEMES
WE
EVALUATED
FOUR
DIFFER
ENT
BLOCK
NORMALIZATION
SCHEMES
FOR
EACH
OF
THE
ABOVE
HOG
GEOMETRIES
LET
V
BE
THE
UNNORMALIZED
DESCRIPTOR
VECTOR
LVLK
BE
ITS
K
NORM
FOR
K
AND
E
BE
A
SMALL
CONSTANT
WITH
Σ
BLOCK
WIDTH
THE
SCHEMES
ARE
A
NORM
V
V
B
WE
ALSO
TRIED
INCLUDING
MULTIPLE
BLOCK
TYPES
WITH
DIFFER
HYS
NORM
FOLLOWED
BY
CLIPPING
LIMITING
THE
MAXI
ENT
CELL
AND
BLOCK
SIZES
IN
THE
OVERALL
DESCRIPTOR
THIS
SLIGHTLY
MUM
VALUES
OF
V
TO
AND
RENORMALIZING
AS
IN
C
IMPROVES
PERFORMANCE
BY
AROUND
AT
FPPW
AT
THE
NORM
V
V
E
AND
D
SQRT
NORM
FOL
COST
OF
GREATLY
INCREASED
DESCRIPTOR
SIZE
BESIDES
SQUARE
R
HOG
BLOCKS
WE
ALSO
TESTED
VERTICAL
CELL
AND
HORIZONTAL
CELL
BLOCKS
AND
A
COMBINED
DESCRIPTOR
INCLUDING
BOTH
VERTICAL
AND
HORIZONTAL
PAIRS
VERTI
CAL
AND
VERTICAL
HORIZONTAL
PAIRS
ARE
SIGNIFICANTLY
BETTER
THAN
HORIZONTAL
PAIRS
ALONE
BUT
NOT
AS
GOOD
AS
BLOCKS
WORSE
AT
FPPW
LOWED
BY
SQUARE
ROOT
V
V
E
WHICH
AMOUNTS
TO
TREATING
THE
DESCRIPTOR
VECTORS
AS
PROBABILITY
DISTRIBUTIONS
AND
USING
THE
BHATTACHARYA
DISTANCE
BETWEEN
THEM
FIG
C
SHOWS
THAT
HYS
NORM
AND
SQRT
ALL
PERFORM
EQUALLY
WELL
WHILE
SIMPLE
NORM
REDUCES
PERFORMANCE
BY
AND
OMITTING
NORMALIZATION
ENTIRELY
REDUCES
IT
BY
AT
FPPW
SOME
REGULARIZATION
E
IS
NEEDED
AS
WE
EVALU
ATE
DESCRIPTORS
DENSELY
INCLUDING
ON
EMPTY
PATCHES
BUT
THE
RESULTS
ARE
INSENSITIVE
TO
E
VALUE
OVER
A
LARGE
RANGE
CENTRE
SURROUND
NORMALIZATION
WE
ALSO
INVESTIGATED
AN
ALTERNATIVE
CENTRE
SURROUND
STYLE
CELL
NORMALIZATION
SCHEME
IN
WHICH
THE
IMAGE
IS
TILED
WITH
A
GRID
OF
CELLS
AND
FOR
EACH
CELL
THE
TOTAL
ENERGY
IN
THE
CELL
AND
ITS
SURROUNDING
RE
GION
SUMMED
OVER
ORIENTATIONS
AND
POOLED
USING
GAUSSIAN
WEIGHTING
IS
USED
TO
NORMALIZE
THE
CELL
HOWEVER
AS
FIG
C
WINDOW
NORM
SHOWS
THIS
DECREASES
PERFORMANCE
RELATIVE
TO
THE
CORRESPONDING
BLOCK
BASED
SCHEME
BY
AT
FPPW
FOR
POOLING
WITH
Σ
CELL
WIDTHS
ONE
REASON
IS
THAT
THERE
ARE
NO
LONGER
ANY
OVERLAPPING
BLOCKS
SO
EACH
CELL
IS
CODED
ONLY
ONCE
IN
THE
FINAL
DESCRIPTOR
INCLUDING
SEVERAL
NORMALIZATIONS
FOR
EACH
CELL
BASED
ON
DIFFERENT
POOLING
SCALES
Σ
PROVIDES
NO
PERCEPTIBLE
CHANGE
IN
PERFORMANCE
SO
IT
SEEMS
THAT
IT
IS
THE
EXISTENCE
OF
SEVERAL
POOLING
REGIONS
WITH
DIFFER
ENT
SPATIAL
OFFSETS
RELATIVE
TO
THE
CELL
THAT
IS
IMPORTANT
HERE
NOT
THE
POOLING
SCALE
TO
CLARIFY
THIS
POINT
CONSIDER
THE
R
HOG
DETECTOR
WITH
OVERLAPPING
BLOCKS
THE
COEFFICIENTS
OF
THE
TRAINED
LINEAR
SVM
GIVE
A
MEASURE
OF
HOW
MUCH
WEIGHT
EACH
CELL
OF
EACH
BLOCK
CAN
HAVE
IN
THE
FINAL
DISCRIMINATION
DECISION
CLOSE
EX
AMINATION
OF
FIG
B
F
SHOWS
THAT
THE
MOST
IMPORTANT
CELLS
ARE
THE
ONES
THAT
TYPICALLY
CONTAIN
MAJOR
HUMAN
CONTOURS
ES
PECIALLY
THE
HEAD
AND
SHOULDERS
AND
THE
FEET
NORMALIZED
W
R
T
BLOCKS
LYING
OUTSIDE
THE
CONTOUR
IN
OTHER
WORDS
DESPITE
THE
COMPLEX
CLUTTERED
BACKGROUNDS
THAT
ARE
COM
MON
IN
OUR
TRAINING
SET
THE
DETECTOR
CUES
MAINLY
ON
THE
CONTRAST
OF
SILHOUETTE
CONTOURS
AGAINST
THE
BACKGROUND
NOT
ON
INTERNAL
EDGES
OR
ON
SILHOUETTE
CONTOURS
AGAINST
THE
FORE
GROUND
PATTERNED
CLOTHING
AND
POSE
VARIATIONS
MAY
MAKE
INTERNAL
REGIONS
UNRELIABLE
AS
CUES
OR
FOREGROUND
TO
CONTOUR
TRANSITIONS
MAY
BE
CONFUSED
BY
SMOOTH
SHADING
AND
SHAD
OWING
EFFECTS
SIMILARLY
FIG
C
G
ILLUSTRATE
THAT
GRADIENTS
INSIDE
THE
PERSON
ESPECIALLY
VERTICAL
ONES
TYPICALLY
COUNT
AS
NEGATIVE
CUES
PRESUMABLY
BECAUSE
THIS
SUPPRESSES
FALSE
POS
ITIVES
IN
WHICH
LONG
VERTICAL
LINES
TRIGGER
VERTICAL
HEAD
AND
LEG
CELLS
DETECTOR
WINDOW
AND
CONTEXT
OUR
DETECTION
WINDOW
INCLUDES
ABOUT
PIXELS
OF
MARGIN
AROUND
THE
PERSON
ON
ALL
FOUR
SIDES
FIG
E
SHOWS
THAT
THIS
BORDER
PROVIDES
A
SIGNIFICANT
AMOUNT
OF
CON
TEXT
THAT
HELPS
DETECTION
DECREASING
IT
FROM
TO
PIXELS
DETECTION
WINDOW
DECREASES
PERFORMANCE
BY
AT
FPPW
KEEPING
A
WINDOW
BUT
INCREASING
THE
PERSON
SIZE
WITHIN
IT
AGAIN
DECREASING
THE
BORDER
CAUSES
A
SIMILAR
LOSS
OF
PERFORMANCE
EVEN
THOUGH
THE
RESOLUTION
OF
THE
PERSON
IS
ACTUALLY
INCREASED
CLASSIFIER
BY
DEFAULT
WE
USE
A
SOFT
C
LINEAR
SVM
TRAINED
WITH
SVMLIGHT
SLIGHTLY
MODIFIED
TO
REDUCE
MEMORY
USAGE
FOR
PROBLEMS
WITH
LARGE
DENSE
DESCRIPTOR
VECTORS
US
ING
A
GAUSSIAN
KERNEL
SVM
INCREASES
PERFORMANCE
BY
ABOUT
AT
FPPW
AT
THE
COST
OF
A
MUCH
HIGHER
RUN
TIME
DISCUSSION
OVERALL
THERE
ARE
SEVERAL
NOTABLE
FINDINGS
IN
THIS
WORK
THE
FACT
THAT
HOG
GREATLY
OUT
PERFORMS
WAVELETS
AND
THAT
ANY
SIGNIFICANT
DEGREE
OF
SMOOTHING
BEFORE
CALCULATING
GRA
DIENTS
DAMAGES
THE
HOG
RESULTS
EMPHASIZES
THAT
MUCH
OF
THE
AVAILABLE
IMAGE
INFORMATION
IS
FROM
ABRUPT
EDGES
AT
FINE
SCALES
AND
THAT
BLURRING
THIS
IN
THE
HOPE
OF
REDUCING
THE
SEN
SITIVITY
TO
SPATIAL
POSITION
IS
A
MISTAKE
INSTEAD
GRADIENTS
SHOULD
BE
CALCULATED
AT
THE
FINEST
AVAILABLE
SCALE
IN
THE
CURRENT
PYRAMID
LAYER
RECTIFIED
OR
USED
FOR
ORIENTATION
VOTING
AND
ONLY
THEN
BLURRED
SPATIALLY
GIVEN
THIS
RELATIVELY
COARSE
SPA
TIAL
QUANTIZATION
SUFFICES
PIXEL
CELLS
ONE
LIMB
WIDTH
ON
THE
OTHER
HAND
AT
LEAST
FOR
HUMAN
DETECTION
IT
PAYS
TO
SAMPLE
ORIENTATION
RATHER
FINELY
BOTH
WAVELETS
AND
SHAPE
CONTEXTS
LOSE
OUT
SIGNIFICANTLY
HERE
SECONDLY
STRONG
LOCAL
CONTRAST
NORMALIZATION
IS
ESSEN
TIAL
FOR
GOOD
RESULTS
AND
TRADITIONAL
CENTRE
SURROUND
STYLE
SCHEMES
ARE
NOT
THE
BEST
CHOICE
BETTER
RESULTS
CAN
BE
ACHIEVED
BY
NORMALIZING
EACH
ELEMENT
EDGE
CELL
SEVERAL
TIMES
WITH
RESPECT
TO
DIFFERENT
LOCAL
SUPPORTS
AND
TREATING
THE
RESULTS
AS
INDEPENDENT
SIGNALS
IN
OUR
STANDARD
DETECTOR
EACH
HOG
CELL
APPEARS
FOUR
TIMES
WITH
DIFFERENT
NORMALIZA
TIONS
AND
INCLUDING
THIS
REDUNDANT
INFORMATION
IMPROVES
PERFORMANCE
FROM
TO
AT
FPPW
SUMMARY
AND
CONCLUSIONS
WE
HAVE
SHOWN
THAT
USING
LOCALLY
NORMALIZED
HISTOGRAM
OF
GRADIENT
ORIENTATIONS
FEATURES
SIMILAR
TO
SIFT
DESCRIPTORS
IN
A
DENSE
OVERLAPPING
GRID
GIVES
VERY
GOOD
RESULTS
FOR
PERSON
DETECTION
REDUCING
FALSE
POSITIVE
RATES
BY
MORE
THAN
AN
ORDER
OF
MAGNITUDE
RELATIVE
TO
THE
BEST
HAAR
WAVELET
BASED
DETECTOR
FROM
WE
STUDIED
THE
INFLUENCE
OF
VARIOUS
DE
SCRIPTOR
PARAMETERS
AND
CONCLUDED
THAT
FINE
SCALE
GRADIENTS
FINE
ORIENTATION
BINNING
RELATIVELY
COARSE
SPATIAL
BINNING
AND
HIGH
QUALITY
LOCAL
CONTRAST
NORMALIZATION
IN
OVERLAPPING
DESCRIPTOR
BLOCKS
ARE
ALL
IMPORTANT
FOR
GOOD
PERFORMANCE
WE
ALSO
INTRODUCED
A
NEW
AND
MORE
CHALLENGING
PEDESTRIAN
DATABASE
WHICH
IS
PUBLICLY
AVAILABLE
FUTURE
WORK
ALTHOUGH
OUR
CURRENT
LINEAR
SVM
DETECTOR
IS
REASONABLY
EFFICIENT
PROCESSING
A
SCALE
SPACE
IM
AGE
DETECTION
WINDOWS
IN
LESS
THAN
A
SECOND
THERE
IS
STILL
ROOM
FOR
OPTIMIZATION
AND
TO
FURTHER
SPEED
UP
DETECTIONS
IT
WOULD
BE
USEFUL
TO
DEVELOP
A
COARSE
TO
FINE
OR
REJECTION
CHAIN
STYLE
DETECTOR
BASED
ON
HOG
DESCRIPTORS
WE
ARE
ALSO
WORKING
ON
HOG
BASED
DETECTORS
THAT
INCORPORATE
MOTION
IN
FORMATION
USING
BLOCK
MATCHING
OR
OPTICAL
FLOW
FIELDS
FI
NALLY
ALTHOUGH
THE
CURRENT
FIXED
TEMPLATE
STYLE
DETECTOR
HAS
PROVEN
DIFFICULT
TO
BEAT
FOR
FULLY
VISIBLE
PEDESTRIANS
HUMANS
ARE
HIGHLY
ARTICULATED
AND
WE
BELIEVE
THAT
INCLUDING
A
PARTS
BASED
MODEL
WITH
A
GREATER
DEGREE
OF
LOCAL
SPATIAL
INVARIANCE
A
B
C
D
E
F
G
FIGURE
OUR
HOG
DETECTORS
CUE
MAINLY
ON
SILHOUETTE
CONTOURS
ESPECIALLY
THE
HEAD
SHOULDERS
AND
FEET
THE
MOST
ACTIVE
BLOCKS
ARE
CENTRED
ON
THE
IMAGE
BACKGROUND
JUST
OUTSIDE
THE
CONTOUR
A
THE
AVERAGE
GRADIENT
IMAGE
OVER
THE
TRAINING
EXAMPLES
B
EACH
PIXEL
SHOWS
THE
MAXIMUM
POSITIVE
SVM
WEIGHT
IN
THE
BLOCK
CENTRED
ON
THE
PIXEL
C
LIKEWISE
FOR
THE
NEGATIVE
SVM
WEIGHTS
D
A
TEST
IMAGE
IT
COMPUTED
R
HOG
DESCRIPTOR
F
G
THE
R
HOG
DESCRIPTOR
WEIGHTED
BY
RESPECTIVELY
THE
POSITIVE
AND
THE
NEGATIVE
SVM
WEIGHTS
WOULD
HELP
TO
IMPROVE
THE
DETECTION
RESULTS
IN
MORE
GENERAL
SITUATIONS
SCENE
SEMANTICS
FROM
LONG
TERM
OBSERVATION
OF
PEOPLE
VINCENT
DAVID
F
IVAN
JOSEF
ABHINAV
AND
ALEXEI
A
E
COLE
NORMALE
SUP
ERIEURE
PARIS
MELLON
UNIVERSITY
ABSTRACT
OUR
EVERYDAY
OBJECTS
SUPPORT
VARIOUS
TASKS
AND
CAN
BE
USED
BY
PEOPLE
FOR
DIFFERENT
PURPOSES
WHILE
OBJECT
CLASSIFICATION
IS
A
WIDELY
STUDIED
TOPIC
IN
COMPUTER
VISION
RECOGNITION
OF
OBJECT
FUNCTION
I
E
WHAT
PEOPLE
CAN
DO
WITH
AN
OBJECT
AND
HOW
THEY
DO
IT
IS
RARELY
ADDRESSED
IN
THIS
PAPER
WE
CONSTRUCT
A
FUNCTIONAL
OBJECT
DESCRIPTION
WITH
THE
AIM
TO
RECOGNIZE
OBJECTS
BY
THE
WAY
PEOPLE
INTERACT
WITH
THEM
WE
DESCRIBE
SCENE
OBJECTS
SOFAS
TABLES
CHAIRS
BY
ASSOCIATED
HUMAN
POSES
AND
OB
JECT
APPEARANCE
OUR
MODEL
IS
LEARNED
DISCRIMINATIVELY
FROM
AUTOMAT
ICALLY
ESTIMATED
BODY
POSES
IN
MANY
REALISTIC
SCENES
IN
PARTICULAR
WE
MAKE
USE
OF
TIME
LAPSE
VIDEOS
FROM
YOUTUBE
PROVIDING
A
RICH
SOURCE
OF
COMMON
HUMAN
OBJECT
INTERACTIONS
AND
MINIMIZING
THE
EFFORT
OF
MAN
UAL
OBJECT
ANNOTATION
WE
SHOW
HOW
THE
MODELS
LEARNED
FROM
HUMAN
OBSERVATIONS
SIGNIFICANTLY
IMPROVE
OBJECT
RECOGNITION
AND
ENABLE
PREDIC
TION
OF
CHARACTERISTIC
HUMAN
POSES
IN
NEW
SCENES
RESULTS
ARE
SHOWN
ON
A
DATASET
OF
MORE
THAN
FRAMES
OBTAINED
FROM
TIME
LAPSE
VIDEOS
OF
CHALLENGING
AND
REALISTIC
INDOOR
SCENES
INTRODUCTION
WHAT
ARE
PEOPLE
EXPECTED
TO
DO
WITH
A
CHRISTMAS
TREE
JUST
SET
UP
IN
A
LIVING
ROOM
IS
IT
COMMON
TO
SEE
A
PERSON
SITTING
ON
A
STOVE
CURRENT
COMPUTER
VISION
METHODS
PROVIDE
NO
ANSWERS
TO
SUCH
QUESTIONS
MEANWHILE
RESOLVING
THESE
AND
MANY
OTHER
QUESTIONS
BY
RECOGNIZING
FUNCTIONAL
PROPERTIES
OF
OBJECTS
AND
SCENES
WOULD
BE
HIGHLY
RELEVANT
FOR
ADDRESSING
THE
TASKS
OF
ABNORMAL
EVENT
DETECTION
AND
PREDICTING
FUTURE
EVENTS
IN
IMAGE
AND
VIDEO
DATA
OBJECT
FUNCTIONS
CAN
BE
DERIVED
FROM
THE
KNOWN
ASSOCIATIONS
BETWEEN
OBJECT
CATEGORIES
AND
HUMAN
ACTIONS
THE
MEDIATED
PERCEPTION
OF
FUNCTION
APPROACH
FOR
EXAMPLE
CHAIR
SITTABLE
WINDOW
OPENABLE
ACTIONS
SUCH
AS
SITTING
HOWEVER
CAN
BE
REALIZED
IN
MANY
DIFFERENT
FORMS
WHICH
CAN
BE
CHARACTERISTIC
FOR
SOME
OBJECTS
BUT
NOT
FOR
OTHERS
AS
ILLUSTRATED
IN
FIGURE
MOREOVER
SOME
OBJECTS
MAY
NOT
SUPPORT
THE
COMMON
FUNCTION
ASSOCIATED
WITH
THEIR
CATEGORY
FOR
EXAM
PLE
WINDOWS
IN
AIRPLANES
ARE
USUALLY
NOT
OPENABLE
THESE
AND
NUMEROUS
OTHER
EXAMPLES
SUGGEST
THAT
THE
CATEGORY
LEVEL
ASSOCIATION
BETWEEN
OBJECTS
AND
THEIR
FUNCTIONS
IS
NOT
LIKELY
TO
SCALE
WELL
TO
THE
VERY
RICH
VARIETY
OF
THE
TYPES
AND
FORMS
OF
PERSON
OBJECT
INTERACTIONS
INSTEAD
WE
ARGUE
THAT
THE
FUNCTIONAL
DESCRIPTIONS
OF
OBJECTS
SHOULD
BE
LEARNED
DIRECTLY
FROM
OBSERVATIONS
OF
VISUAL
DATA
FIG
DIFFERENT
WAYS
OF
USING
OBJECTS
WHILE
ALL
PEOPLE
DEPICTED
ON
THE
LEFT
ARE
SITTING
THEIR
SITTING
POSES
CAN
BE
RATHER
UNAMBIGUOUSLY
ASSOCIATED
WITH
THE
OBJECTS
ON
THE
RIGHT
IN
THIS
PAPER
WE
BUILD
ON
THIS
OBSERVATION
AND
LEARN
OBJECT
DESCRIPTIONS
IN
TERMS
OF
CHARACTERISTIC
BODY
POSES
IN
THIS
WORK
WE
DESIGN
OBJECT
DESCRIPTIONS
BY
LEARNING
ASSOCIATIONS
BETWEEN
OBJECTS
AND
SPATIALLY
CO
OCCURRING
HUMAN
POSES
TO
CAPTURE
THE
RICH
VARIETY
OF
PERSON
OBJECT
INTERACTIONS
WE
AUTOMATICALLY
DETECT
PEOPLE
AND
ESTIMATE
BODY
POSES
IN
LONG
TERM
OBSERVATIONS
OF
REALISTIC
INDOOR
SCENES
USING
THE
STATE
OF
THE
ART
METHOD
OF
WHILE
RELIABLE
POSE
ESTIMATION
IS
STILL
A
CHALLENGING
PROBLEM
WE
CIRCUMVENT
THE
NOISE
IN
POSE
ESTIMATION
BY
OBSERVING
MANY
PERSON
INTERAC
TIONS
WITH
THE
SAME
INSTANCES
OF
OBJECTS
FOR
THIS
PURPOSE
WE
USE
VIDEOS
FROM
HOURS
LASTING
EVENTS
PARTIES
HOUSE
CLEANING
RECORDED
WITH
A
STATIC
CAMERA
AND
SUMMARIZED
INTO
TIME
STATIC
OBJECTS
IN
TIME
LAPSES
E
G
SOFAS
CAN
BE
READILY
ASSOCIATED
WITH
HUNDREDS
OF
CO
OCCURRING
HUMAN
POSES
SPANNING
THE
TYP
ICAL
INTERACTIONS
OF
PEOPLE
WITH
THESE
OBJECTS
SEE
FIGURES
EQUIPPED
WITH
THIS
DATA
WE
CONSTRUCT
STATISTICAL
OBJECT
DESCRIPTORS
WHICH
COMBINE
THE
SIGNA
TURES
OF
OBJECT
SPECIFIC
BODY
POSES
AS
WELL
AS
THE
OBJECT
APPEARANCE
THE
MODEL
IS
LEARNED
DISCRIMINATIVELY
FROM
MANY
TIME
LAPSE
VIDEOS
OF
VARIETY
OF
SCENES
TO
SUMMARIZE
OUR
CONTRIBUTIONS
WE
PROPOSE
A
NEW
STATISTICAL
MODEL
DE
SCRIBING
OBJECTS
IN
TERMS
OF
DISTRIBUTIONS
OF
ASSOCIATED
HUMAN
POSES
NOTABLY
WE
DO
NOT
REQUIRE
HUMAN
POSES
TO
BE
ANNOTATED
DURING
TRAINING
AND
LEARN
THE
RICH
VARIETY
OF
PERSON
OBJECT
INTERACTIONS
AUTOMATICALLY
FROM
LONG
TERM
OBSERVA
TIONS
OF
PEOPLE
OUR
FUNCTIONAL
OBJECT
DESCRIPTION
GENERALIZES
ACROSS
REALISTIC
AND
CHALLENGING
SCENES
PROVIDES
SIGNIFICANT
IMPROVEMENTS
IN
OBJECT
RECOGNITION
AND
SUPPORTS
PREDICTION
OF
HUMAN
POSES
IN
NEW
SCENES
BACKGROUND
SEMANTIC
OBJECT
LABELING
AND
SEGMENTATION
HAS
BEEN
MAINLY
CON
SIDERED
FOR
OUTDOOR
SCENES
E
G
FOR
INDOOR
SCENES
THE
FOCUS
HAS
BEEN
ON
RECOVERING
SPATIAL
LAYOUT
POSSIBLY
SINCE
MANY
INDOOR
OBJECTS
ARE
OFTEN
BET
TER
DEFINED
BY
THEIR
FUNCTION
RATHER
THAN
APPEARANCE
TIME
LAPSE
IS
A
COMMON
MEDIA
TYPE
USED
TO
SUMMARIZE
RECORDINGS
OF
LONG
EVENTS
INTO
SHORT
VIDEO
CLIPS
BY
TEMPORAL
SUB
SAMPLING
WE
USE
TIME
LAPSES
WIDELY
AVAILABLE
ON
PUBLIC
VIDEO
SHARING
WEB
SITES
SUCH
AS
YOUTUBE
WHICH
ARE
TYPICALLY
SAMPLED
AT
ONE
FRAME
PER
SECONDS
THE
INTERPLAY
BETWEEN
PEOPLE
AND
OBJECTS
HAS
RECENTLY
ATTRACTED
SIGNIFICANT
ATTENTION
INTERACTIONS
BETWEEN
PEOPLE
AND
SEMANTIC
OBJECTS
HAS
BEEN
STUDIED
IN
STILL
IMAGES
WITH
THE
FOCUS
ON
IMPROVING
ACTION
RECOGNITION
OBJECT
LOCAL
IZATION
AND
DISCOVERY
AS
WELL
AS
POSE
ESTIMATION
IN
VIDEO
CONSTRAINTS
BETWEEN
HUMAN
ACTIONS
AND
OBJECTS
E
G
DRINKING
FROM
A
COFFEE
CUP
HAVE
BEEN
INVESTIGATED
IN
RESTRICTED
LABORATORY
SETUPS
OR
EGO
CENTRIC
SCENARIOS
IN
BOTH
STILL
IMAGES
AND
VIDEO
THE
FOCUS
HAS
BEEN
TYPICALLY
ON
SMALL
OBJECTS
MANIPULATED
BY
HANDS
E
G
COFFEE
CUPS
FOOTBALLS
TENNIS
RACKETS
RATHER
THAN
SCENE
OBJECTS
SUCH
AS
CHAIRS
SOFAS
OR
TABLES
WHICH
EXHIBIT
LARGE
INTRA
CLASS
VARIABILITY
IN
ADDITION
MANUAL
ANNOTATION
OF
ACTION
CATEGORIES
OR
HUMAN
POSES
IN
THE
TRAINING
DATA
IS
OFTEN
REQUIRED
AND
THE
MODELS
TYPICALLY
DO
NOT
ALLOW
PREDICTING
POSES
IN
NEW
SCENES
WITHOUT
PEOPLE
FUNCTIONAL
SCENE
DESCRIPTIONS
HAVE
BEEN
DEVELOPED
FOR
SURVEILLANCE
SETUPS
E
G
BUT
THE
MODELS
ARE
USUALLY
DESIGNED
FOR
SPECIFIC
SCENE
INSTANCES
AND
USE
ONLY
COARSE
LEVEL
OBSERVATIONS
OF
OBJECT
PERSON
TRACKS
OR
APPROXIMATE
PERSON
SEGMENTS
OBTAINED
FROM
BACKGROUND
SUBTRACTION
IN
CONTRAST
OUR
METHOD
GENERALIZES
TO
NEW
CHALLENGING
SCENES
AND
USES
FINER
GRAIN
DESCRIPTORS
OF
ESTIMATED
BODY
CONFIGURATION
ENABLING
DISCRIMINATION
BETWEEN
OBJECT
CLASSES
SUCH
AS
SOFAS
AND
CHAIRS
RECENT
ATTEMPTS
HAVE
INFERRED
FUNCTIONS
OR
AFFORDANCES
FROM
AU
TOMATICALLY
OBTAINED
NOISY
RECONSTRUCTIONS
OF
INDOOR
SCENES
THESE
METHODS
INFER
AFFORDANCE
BASED
ON
THE
GEOMETRY
AND
PHYSICAL
PROPERTIES
OF
THE
SPACE
FOR
EXAMPLE
THEY
FIND
PLACES
WHERE
A
PERSON
CAN
SIT
BY
FITTING
A
HUMAN
SKELETON
IN
A
PARTICULAR
POSE
AT
A
PARTICULAR
LOCATION
IN
THE
SCENE
WHILE
PEOPLE
CAN
SIT
AT
MANY
PLACES
THEY
TEND
TO
SIT
IN
SOFAS
MORE
OFTEN
THAN
ON
TABLES
MOREOVER
THEY
MAY
SIT
ON
SOFAS
IN
A
DIFFERENT
WAY
THAN
ON
A
FLOOR
OR
ON
A
CHAIR
IN
THIS
WORK
WE
AIM
TO
LEVERAGE
THESE
OBSERVATIONS
AND
FOCUS
ON
STATISTICAL
AFFORDANCES
BY
LEARNING
TYPICAL
HUMAN
POSES
ASSOCIATED
WITH
EACH
OBJECT
IN
A
SIMILAR
SETUP
TO
OURS
FOUHEY
ET
AL
HAVE
LOOKED
AT
PEOPLE
ACTIONS
AS
A
CUE
FOR
A
COARSE
BOX
LIKE
GEOMETRY
OF
INDOOR
SCENES
HERE
WE
INVESTIGATE
THE
INTERPLAY
BETWEEN
OBJECT
FUNCTION
AND
OBJECT
SEMANTICS
RATHER
THAN
SCENE
GEOMETRY
IN
ADDITION
IN
THE
GEOMETRIC
PERSON
SCENE
RELATIONS
ARE
DESIGNED
MANUALLY
IN
THIS
WORK
WE
LEARN
SEMANTIC
PERSON
OBJECT
INTERACTIONS
FROM
DATA
METHOD
OVERVIEW
IN
THIS
SECTION
WE
GIVE
A
BRIEF
OVERVIEW
OF
THE
PROPOSED
APPROACH
OUR
MAIN
GOAL
IS
TO
LEARN
FUNCTIONAL
OBJECT
DESCRIPTIONS
FROM
REALISTIC
OBSERVATIONS
OF
PERSON
OBJECT
INTERACTIONS
TO
SIMPLIFY
THE
LEARNING
TASK
WE
ASSUME
INPUT
VIDEOS
TO
CONTAIN
STATIC
OBJECTS
WITH
FIXED
LOCATIONS
IN
EACH
FRAME
OF
THE
VIDEO
ANNOTA
TION
OF
SUCH
OBJECTS
IN
THE
WHOLE
VIDEO
CAN
BE
SIMPLY
DONE
BY
OUTLINING
OBJECT
BOUNDARY
IN
ONE
VIDEO
FRAME
AS
ILLUSTRATED
IN
FIGURE
MOREOVER
PERSON
INTER
ACTIONS
WITH
STATIC
OBJECTS
CAN
BE
AUTOMATICALLY
RECORDED
BY
DETECTING
PEOPLE
IN
THE
SPATIAL
PROXIMITY
OF
ANNOTATED
OBJECTS
WE
START
BY
OVER
SEGMENTING
INPUT
SCENES
INTO
SUPER
PIXELS
WHICH
WILL
FORM
THE
CANDIDATE
OBJECT
REGIONS
DETAILS
GIVEN
IN
SECTION
FOR
EACH
OBJECT
REGION
POSE
ESTIMATION
APPEARANCE
POSE
DISTRIBUTION
APPEARANCE
DISTRIBUTION
BOF
LOCATION
LOCATION
DISTRIBUTION
H
R
FIG
OVERVIEW
OF
THE
PROPOSED
PERSON
BASED
OBJECT
DESCRIPTION
INPUT
SCENES
ARE
OVER
SEGMENTED
INTO
SUPER
PIXELS
EACH
SUPER
PIXEL
DENOTED
R
HERE
IS
DESCRIBED
BY
THE
DISTRIBUTION
OF
CO
OCCURRING
HUMAN
POSES
OVER
TIME
AS
WELL
AS
BY
THE
APPEARANCE
AND
LOCATION
OF
THE
SUPER
PIXEL
IN
THE
IMAGE
R
WE
CONSTRUCT
A
DESCRIPTOR
VECTOR
H
R
TO
BE
USED
FOR
SUBSEQUENT
LEARNING
AND
RECOGNITION
THE
PARTICULAR
NOVELTY
OF
OUR
METHOD
IS
A
NEW
DESCRIPTOR
REPRESENT
ING
AN
OBJECT
REGION
BY
THE
TEMPORAL
STATISTICS
HP
R
OF
CO
OCCURRING
PEOPLE
SECTION
THIS
DESCRIPTOR
CONTAINS
A
DISTRIBUTION
OF
HUMAN
BODY
POSES
AND
THEIR
RELATIVE
LOCATION
WITH
RESPECT
TO
THE
OBJECT
REGION
WE
ALSO
REPRESENT
EACH
OBJECT
REGION
BY
APPEARANCE
FEATURES
DENOTED
HA
R
AND
THE
ABSOLUTE
LOCATION
IN
THE
FRAME
DENOTED
HL
R
AS
DESCRIBED
IN
SECTION
GIVEN
DESCRIPTOR
VECTORS
ONE
FOR
EACH
OBJECT
REGION
CONTAINING
STATISTICS
OF
CHARACTERISTIC
POSES
APPEARANCE
AND
IMAGE
LOCATIONS
A
LINEAR
SUPPORT
VECTOR
MA
CHINE
SVM
CLASSIFIER
IS
LEARNT
FOR
EACH
OBJECT
CLASS
FROM
THE
LABELLED
TRAINING
DATA
IN
A
DISCRIMINATIVE
MANNER
AT
TEST
TIME
THE
SAME
FUNCTIONAL
AND
APPEAR
ANCE
REPRESENTATION
IS
EXTRACTED
FROM
CANDIDATE
OBJECT
REGIONS
OF
THE
TESTING
VIDEO
INDIVIDUAL
CANDIDATE
OBJECT
REGIONS
ARE
THEN
CLASSIFIED
AS
BELONGING
TO
ONE
OF
THE
SEMANTIC
OBJECT
CLASSES
MODELING
LONG
TERM
PERSON
OBJECT
INTERACTIONS
THIS
SECTION
PRESENTS
OUR
MODEL
OF
THE
RELATIONSHIP
BETWEEN
OBJECTS
AND
SUR
ROUNDING
PEOPLE
WE
START
BY
INTRODUCING
A
NEW
REPRESENTATION
DESCRIBING
AN
OBJECT
BY
THE
STATISTICS
OF
CO
OCCURRING
HUMAN
POSES
WE
THEN
EXPLAIN
THE
DE
TAILS
OF
THE
EXTRACTION
AND
QUANTIZATION
OF
HUMAN
POSES
IN
TIME
LAPSES
DESCRIBING
AN
OBJECT
BY
A
DISTRIBUTION
OF
POSES
WE
WISH
TO
CHARACTERIZE
OBJECTS
BY
THE
TYPICAL
LOCATIONS
AND
POSES
OF
SURROUNDING
PEOPLE
WHILE
REASONING
ABOUT
PEOPLE
AND
SCENES
HAS
SOME
ADVANTAGES
RELIABLE
ESTIMATION
OF
SCENE
GEOMETRY
AND
HUMAN
POSES
IN
IS
STILL
AN
OPEN
PROBLEM
MOREOVER
DERIVING
RICH
PERSON
OBJECT
CO
OCCURRENCES
FROM
A
SINGLE
IM
AGE
IS
DIFFICULT
DUE
TO
THE
TYPICALLY
LIMITED
NUMBER
OF
PEOPLE
IN
THE
SCENE
AND
THE
NOISE
OF
AUTOMATIC
HUMAN
POSE
ESTIMATION
TO
CIRCUMVENT
THESE
PROBLEMS
WE
FIG
CAPTURING
PERSON
OBJECT
INTERACTIONS
AN
OBJECT
REGION
R
IS
DESCRIBED
BY
A
DISTRIBUTION
HISTOGRAM
OVER
POSES
K
LEFT
JOINTS
J
MIDDLE
AND
CELLS
C
RIGHT
THE
GRID
OF
CELLS
C
IS
PLACED
AROUND
EACH
JOINT
TO
CAPTURE
THE
RELATIVE
POSITION
OF
AN
OBJECT
REGION
R
WITH
RESPECT
TO
JOINT
J
THE
PIXEL
OVERLAP
BETWEEN
THE
GRID
CELL
C
AND
THE
OBJECT
REGION
R
WEIGHTS
THE
CONTRIBUTION
OF
THE
JTH
JOINT
AND
THE
KTH
POSE
CLUSTER
TAKE
ADVANTAGE
OF
THE
SPATIAL
CO
OCCURRENCE
OF
OBJECTS
AND
PEOPLE
IN
THE
IMAGE
PLANE
MOREOVER
WE
ACCUMULATE
MANY
HUMAN
POSES
BY
OBSERVING
SCENES
OVER
AN
EXTENDED
PERIOD
OF
TIME
IN
OUR
SETUP
WE
ASSUME
A
STATIC
CAMERA
AND
CONSIDER
LARGER
OBJECTS
SUCH
AS
SOFAS
AND
TABLES
WHICH
ARE
LESS
LIKELY
TO
CHANGE
LOCATIONS
OVER
TIME
WE
DESCRIBE
OBJECT
REGION
R
IN
THE
IMAGE
BY
THE
TEMPORAL
STATISTICS
HP
OF
CO
OCCURRING
HUMAN
POSES
EACH
PERSON
DETECTION
D
IS
REPRESENTED
BY
THE
LOCATIONS
OF
J
BODY
JOINTS
INDEXED
BY
J
AND
THE
ASSIGNMENT
QD
OF
D
POSE
TO
A
VOCABULARY
OF
KP
DISCRETE
POSE
CLUSTERS
SEE
FIGURE
AND
SECTIONS
FOR
DETAILS
TO
MEASURE
THE
CO
OCCURRENCE
OF
PEOPLE
AND
OBJECTS
WE
DEFINE
A
SPATIAL
GRID
OF
CELLS
C
AROUND
EACH
BODY
JOINT
J
WE
MEASURE
THE
OVERLAP
BETWEEN
THE
OBJECT
REGION
R
AND
THE
GRID
CELL
BD
BY
THE
NORMALIZED
AREA
OF
THEIR
INTERSECTION
B
R
BJ
C
R
BJ
C
WE
THEN
ACCUMULATE
OVERLAPS
FROM
ALL
PERSON
DETECTIONS
D
IN
A
GIVEN
VIDEO
AND
COMPUTE
ONE
ENTRY
HP
R
OF
THE
HISTOGRAM
DESCRIPTOR
HP
R
FOR
REGION
R
AS
HP
R
D
J
C
R
QD
K
J
C
D
D
EXP
WHERE
K
J
AND
C
INDEX
POSE
CLUSTERS
BODY
JOINTS
AND
GRID
CELLS
RESPECTIVELY
THE
CONTRIBUTION
OF
EACH
PERSON
DETECTION
IN
IS
WEIGHTED
BY
THE
DETECTION
SCORE
SD
THE
VALUES
OF
QD
INDICATE
THE
SIMILARITY
OF
THE
PERSON
DETECTION
D
WITH
A
POSE
CLUSTER
K
IN
THE
CASE
OF
THE
HARD
ASSIGNMENT
OF
D
TO
THE
POSE
CLUSTER
K
QD
FOR
K
K
AND
QD
OTHERWISE
IN
OUR
EXPERIMENTS
WE
FOUND
THAT
BETTER
RESULTS
CAN
BE
OBTAINED
USING
SOFT
POSE
ASSIGNMENT
AS
DESCRIBED
IN
THE
NEXT
SECTION
FIG
POSE
CLUSTER
AND
DETECTION
EXAMPLES
LEFT
EXAMPLE
CLUSTER
MEANS
FROM
OUR
POSE
VOCABULARY
RIGHT
PERSON
DETECTIONS
IN
MULTIPLE
FRAMES
OF
TIME
LAPSE
VIDEOS
ASSIGNED
TO
THE
POSE
CLUSTERS
ON
THE
LEFT
BUILDING
A
VOCABULARY
OF
POSES
WE
REPRESENT
OBJECT
SPECIFIC
HUMAN
ACTIONS
BY
A
DISTRIBUTION
OF
QUANTIZED
HUMAN
POSES
TO
COMPUTE
POSE
QUANTIZATION
WE
BUILD
A
VOCABULARY
OF
POSES
FROM
PERSON
DETECTIONS
IN
THE
TRAINING
SET
BY
UNSUPERVISED
CLUSTERING
IN
ORDER
TO
BUILD
THE
POSE
VOCABULARY
WE
FIRST
CONVERT
EACH
DETECTION
D
IN
THE
TRAINING
VIDEO
INTO
A
DIMENSIONAL
POSE
VECTOR
XD
BY
CONCATENATING
MID
POINT
COORDINATES
OF
ALL
DETECTED
BODY
JOINTS
WE
CENTER
AND
NORMALIZE
ALL
POSE
VECTORS
IN
THE
TRAINING
VIDEOS
AND
CLUSTER
THEM
BY
FITTING
A
GAUSSIAN
MIXTURE
MODEL
GMM
WITH
KP
COMPONENTS
VIA
EXPECTATION
MAXIMIZATION
EM
THE
COMPONENTS
ARE
INITIALIZED
BY
THE
RESULT
OF
A
K
MEANS
CLUSTERING
AND
DURING
FITTING
WE
CONSTRAIN
THE
COVARIANCES
TO
BE
DIAGONAL
THE
RESULTING
MEAN
VECTORS
ΜK
DIAGONAL
COVARIANCE
MATRICES
ΣK
AND
WEIGHTS
ΠK
FOR
EACH
POSE
CLUSTER
K
KP
FORM
OUR
VOCABULARY
OF
POSES
SEE
FIGURE
A
POSE
VECTOR
XD
FOR
A
DETECTION
D
CAN
BE
DESCRIBED
BY
A
SOFT
ASSIGNMENT
TO
EACH
OF
THE
ΜK
BY
COMPUTING
THE
POSTERIOR
PROBABILITY
VECTOR
QD
WHERE
QK
P
XD
ΜK
ΣK
ΠK
KP
P
XD
Μ
Σ
Π
PERSON
DETECTION
AND
POSE
ESTIMATION
WE
FOCUS
ON
DETECTING
PEOPLE
IN
THREE
BODY
CONFIGURATIONS
COMMON
IN
INDOOR
SCENES
STANDING
SITTING
AND
REACHING
WE
USE
THE
PERSON
DETECTOR
FROM
YANG
AND
RAMANAN
WHICH
WAS
SHOWN
TO
PERFORM
VERY
WELL
AT
BOTH
PEOPLE
DETECTION
AND
POSE
ESTIMATION
AND
TRAIN
THREE
SEPARATE
MODELS
ONE
FOR
EACH
BODY
CONFIGURATION
WE
FOUND
THAT
TRAINING
SEPARATE
MODELS
IMPROVED
POSE
ESTIMATION
PERFORMANCE
OVER
USING
A
SINGLE
GENERIC
POSE
ESTIMATOR
SECTION
THE
THREE
DETECTORS
ARE
RUN
SEPARATELY
ON
ALL
FRAMES
OF
EACH
TIME
LAPSE
VIDEO
IN
A
SLIDING
WINDOW
MANNER
AT
MULTIPLE
SCALES
AS
ALL
OUR
VIDEOS
HAVE
FIXED
VIEW
POINT
WE
USE
BACKGROUND
SUBTRACTION
SECTION
TO
REMOVE
SOME
FALSE
POSITIVE
DETECTIONS
ADDITIONAL
FALSE
POSITIVES
CAN
BE
REMOVED
VIA
GEOMETRIC
FILTERING
WE
USE
THE
VANISHING
POINT
ESTIMATION
METHOD
PROPOSED
IN
TO
COMPUTE
THE
HORI
ZON
HEIGHT
YH
WE
THEN
ASSUME
A
LINEAR
RELATIONSHIP
HP
YP
Α
YP
YH
BETWEEN
A
PERSON
HEIGHT
HP
AND
THE
FEET
Y
COORDINATE
YP
IN
THE
IMAGE
AND
LEARN
THE
SCALING
COEFFICIENT
Α
VIA
RANSAC
AND
ROBUST
LEAST
SQUARE
FITTING
WE
DISCARD
DETECTIONS
FOR
WHICH
THE
DIFFERENCE
BETWEEN
THE
DETECTED
PERSON
HEIGHT
AND
THE
EXPECTED
PERSON
HEIGHT
IS
GREATER
THAN
A
GIVEN
THRESHOLD
E
FINALLY
WE
NORMALIZE
THE
OUTPUT
OF
THE
DETECTORS
BY
MAKING
THE
MEAN
AND
STANDARD
DEVIATION
OF
THE
DETECTION
SCORES
EQUAL
TO
AND
ON
TRAINING
VIDEOS
RESPECTIVELY
THE
FILTERING
AND
NORMALIZATION
IS
PERFORMED
SEPARATELY
FOR
EACH
DETECTOR
TO
OBTAIN
THE
FINAL
SET
OF
DETECTIONS
WE
PERFORM
STANDARD
NON
MAXIMA
SUP
PRESSION
ON
THE
COMBINED
OUTPUTS
OF
THE
THREE
DETECTORS
IN
EACH
FRAME
IF
BOUND
ING
BOXES
OF
SEVERAL
PERSON
DETECTIONS
OVERLAP
I
E
HAVE
INTERSECTION
OVER
UNION
BIGGER
THAN
THE
DETECTION
WITH
THE
HIGHEST
NORMALIZED
RESPONSE
IS
KEPT
THIS
LEADS
TO
A
SET
DI
OF
CONFIDENT
PERSON
DETECTIONS
FOR
THE
ITH
VIDEO
EACH
DETECTION
D
DI
IS
REPRESENTED
BY
AN
ASSOCIATED
NORMALIZED
SCORE
SD
AND
AN
ESTIMATED
LIMB
CONFIGURATION
CONSISTING
OF
J
BOUNDING
BOXES
BD
J
J
CORRESPONDING
TO
J
LOCATIONS
OF
BODY
JOINTS
AS
OUR
TIME
LAPSE
VIDEOS
ARE
SPARSELY
SAMPLED
IN
TIME
THE
REASONING
ABOUT
TEMPORAL
EVOLUTION
OF
HUMAN
POSES
IS
NOT
STRAIGHTFORWARD
WE
THEREFORE
CUR
RENTLY
DISCARD
ANY
TEMPORAL
INFORMATION
ABOUT
DETECTED
PEOPLE
NEVERTHELESS
THE
TEMPORAL
RE
OCCURRENCE
OF
CHARACTERISTIC
BODY
POSES
FOR
PARTICULAR
OBJECTS
IS
A
VERY
POWERFUL
CUE
WHICH
WE
EXPLOIT
TO
I
REDUCE
THE
NOISE
IN
POSE
ESTIMATION
AND
II
TO
SPAN
THE
RICH
VARIETY
OF
PERSON
OBJECT
INTERACTIONS
MODELING
APPEARANCE
AND
LOCATION
IN
ADDITION
TO
THE
DISTRIBUTION
OF
POSES
WE
ALSO
MODEL
THE
APPEARANCE
AND
AB
SOLUTE
POSITION
OF
IMAGE
REGIONS
WE
BUILD
ON
THE
ORDERLESS
BAG
OF
FEATURES
REP
RESENTATION
AND
DESCRIBE
THE
APPEARANCE
OF
IMAGE
REGIONS
BY
A
DISTRIBUTION
OF
VISUAL
WORDS
WE
FIRST
DENSELY
EXTRACT
SIFT
DESCRIPTORS
F
K
FROM
IMAGE
PATCHES
BF
OF
MULTIPLE
SIZES
SK
FOR
K
FOR
ALL
TRAINING
VIDEOS
AND
QUANTIZE
THEM
INTO
VISUAL
WORDS
BY
FITTING
A
GMM
WITH
KA
COMPONENTS
EACH
FEATURE
F
IS
THEN
SOFT
ASSIGNED
TO
THIS
VOCABULARY
IN
THE
SAME
MANNER
AS
DESCRIBED
IN
EQ
THIS
RESULTS
IN
AN
ASSIGNMENT
VECTOR
QF
FOR
EACH
FEATURE
THE
KA
DIMENSIONAL
APPEARANCE
HISTOGRAM
HA
R
FOR
REGION
R
IS
COMPUTED
AS
A
WEIGHTED
SUM
OF
ASSIGNMENT
VECTORS
QF
A
F
F
K
K
F
FK
WHERE
BF
R
IS
THE
NUMBER
OF
PIXELS
BELONGING
TO
BOTH
OBJECT
REGION
R
AND
FEATURE
PATCH
BF
SIMILAR
TO
WE
ALSO
REPRESENT
THE
ABSOLUTE
POSITION
OF
REGIONS
R
WITHIN
THE
VIDEO
FRAME
THIS
IS
ACHIEVED
BY
SPATIALLY
DISCRETIZING
THE
VIDEO
INTO
A
GRID
OF
M
N
CELLS
RESULTING
IN
A
M
N
DIMENSIONAL
HISTOGRAM
HL
R
FOR
EACH
REGION
R
HERE
THE
ITH
BIN
OF
HL
R
IS
SIMPLY
THE
PROPORTION
OF
PIXELS
OF
THE
ITH
CELL
OF
THE
GRID
FALLING
INTO
R
LEARNING
FROM
LONG
TERM
OBSERVATIONS
WE
NOW
DETAIL
HOW
WE
OBTAIN
CANDIDATE
OBJECT
REGIONS
FROM
MULTIPLE
SUPER
PIXEL
SEGMENTATIONS
AND
LEARN
THE
MODEL
OF
PERSON
OBJECT
INTERACTIONS
WE
THEN
SHOW
HOW
TO
RECOGNIZE
OBJECTS
IN
TESTING
VIDEOS
AND
PREDICT
LIKELY
POSES
IN
NEW
SCENES
OBTAINING
CANDIDATE
OBJECT
REGIONS
AS
DESCRIBED
IN
PREVIOUS
SECTIONS
WE
REPRESENT
OBJECTS
BY
ACCUMULATING
STATISTICS
OF
HUMAN
POSES
IMAGE
APPEARANCE
AND
LOCATION
AT
OBJECT
REGIONS
R
CANDIDATE
OBJECT
REGIONS
ARE
OBTAINED
BY
OVER
SEGMENTING
VIDEO
FRAMES
INTO
SUPER
PIXELS
USING
THE
METHOD
AND
ON
LINE
IMPLE
MENTATION
OF
AS
INDIVIDUAL
VIDEO
FRAMES
MAY
CONTAIN
MANY
PEOPLE
OCCLUD
ING
THE
OBJECTS
IN
THE
SCENE
WE
REPRESENT
EACH
VIDEO
USING
A
SINGLE
BACKGROUND
FRAME
CONTAINING
ALMOST
NO
PEOPLE
SECTION
RATHER
THAN
RELYING
ON
A
SINGLE
SEGMENTATION
WE
FOLLOW
AND
COMPUTE
MULTIPLE
OVERLAPPING
SEGMENTATIONS
BY
VARYING
THE
PARAMETERS
OF
THE
SEGMENTATION
ALGORITHM
LEARNING
OBJECT
MODEL
WE
TRAIN
A
CLASSIFIER
FOR
EACH
OBJECT
CLASS
IN
A
ONE
VERSUS
ALL
MANNER
THE
TRAINING
DATA
FOR
EACH
CLASSIFIER
IS
OBTAINED
BY
COLLECTING
ALL
POTENTIALLY
OVERLAPPING
SUPER
PIXELS
RI
FOR
I
N
FROM
ALL
TRAINING
VIDEOS
FOR
EACH
REGION
WE
EXTRACT
THEIR
CORRESPONDING
POSE
APPEARANCE
AND
LOCATION
HISTOGRAMS
AS
DESCRIBED
IN
SECTIONS
AND
THE
HISTOGRAMS
ARE
SEPA
RATELY
NORMALIZED
AND
CONCATENATED
INTO
A
SINGLE
K
DIMENSIONAL
FEATURE
VEC
TOR
X
H
P
R
H
A
R
H
L
RI
WHERE
H
DENOTES
NORMALIZED
HISTOGRAM
H
AN
OBJECT
LABEL
YI
IS
THEN
ASSIGNED
TO
EACH
SUPER
PIXEL
BASED
ON
THE
SUR
FACE
OVERLAP
WITH
THE
PROVIDED
GROUND
TRUTH
OBJECT
SEGMENTATION
IN
THE
TRAINING
VIDEOS
USING
THE
SURFACE
OVERLAP
THRESHOLD
OF
EACH
SUPER
PIXEL
CAN
BE
AS
SIGNED
UP
TO
TWO
GROUND
TRUTH
OBJECT
LABELS
FINALLY
WE
TRAIN
A
BINARY
SUPPORT
VECTOR
MACHINE
SVM
CLASSIFIER
WITH
THE
HELLINGER
KERNEL
FOR
EACH
OBJECT
CLASS
USING
THE
LABELLED
SUPER
PIXELS
AS
TRAINING
DATA
THE
HELLINGER
KERNEL
IS
EFFICIENTLY
IMPLEMENTED
USING
THE
EXPLICIT
FEATURE
MAP
Φ
XI
XI
XI
AND
A
LINEAR
CLASSIFIER
FINALLY
THE
OUTPUTS
OF
INDIVIDUAL
SVM
CLASSIFIERS
ARE
CALIBRATED
WITH
RESPECT
TO
EACH
OTHER
BY
FITTING
A
MULTINOMIAL
REGRESSION
MODEL
FROM
THE
CLAS
SIFIERS
OUTPUT
TO
THE
SUPER
PIXEL
LABELS
THE
OUTPUT
OF
THE
LEARNING
STAGE
IS
A
K
DIMENSIONAL
WEIGHT
VECTOR
WY
OF
THE
CALIBRATED
LINEAR
CLASSIFIER
FOR
EACH
OBJECT
CLASS
Y
AT
TEST
TIME
MULTIPLE
SUPER
PIXEL
SEGMENTATIONS
ARE
EXTRACTED
FROM
THE
BACK
GROUND
FRAME
OF
THE
TEST
VIDEO
AND
THE
INDIVIDUAL
CLASSIFIERS
ARE
APPLIED
TO
EACH
SUPER
PIXEL
THIS
LEADS
TO
A
CONFIDENCE
MEASURE
FOR
EACH
LABEL
AND
SUPER
PIXEL
THE
CONFIDENCE
OF
A
SINGLE
IMAGE
PIXEL
IS
THEN
THE
MEAN
OF
THE
CONFIDENCES
OF
ALL
THE
SUPER
PIXELS
IT
BELONGS
TO
INFERRING
PROBABLE
POSE
HERE
WE
WISH
TO
PREDICT
THE
MOST
LIKELY
POSE
WITHIN
A
MANUALLY
PROVIDED
BOUNDING
BOX
IN
AN
IMAGE
GIVEN
AN
OBJECT
LAYOUT
SEGMEN
TATION
OF
THE
SCENE
THIS
IS
ACHIEVED
BY
CHOOSING
THE
POSE
CLUSTER
FOR
WHICH
THE
SUM
OF
LEARNT
OBJECT
WEIGHTS
FOR
ALL
JOINTS
MOST
AGREE
WITH
THE
GIVEN
PER
PIXEL
OBJECT
LABELS
IN
THE
IMAGE
MORE
FORMALLY
DENOTING
WY
K
J
C
THE
WEIGHT
LEARNT
FOR
LABEL
Y
POSE
CLUSTER
K
JOINT
J
AND
GRID
CELL
C
WE
SELECT
THE
POSE
CLUSTER
Kˆ
THAT
MAXIMIZES
THE
SUM
OF
PER
PIXEL
WEIGHTS
UNDER
EACH
JOINT
GRID
CELL
BK
J
Kˆ
ARG
MAX
WYI
K
J
C
WHERE
YI
IS
THE
LABEL
FOR
PIXEL
I
TIME
LAPSE
DATASET
WE
EXTEND
THE
DATASET
OF
TO
TIME
LAPSE
VIDEOS
CONTAINING
A
TOTAL
OF
AROUND
FRAMES
EACH
VIDEO
SEQUENCE
SHOWS
HUMAN
ACTORS
INTERACTING
WITH
AN
INDOOR
SCENE
OVER
A
PERIOD
OF
TIME
RANGING
FROM
A
FEW
MINUTES
TO
SEVERAL
HOURS
THE
CAPTURED
EVENTS
INCLUDE
PARTIES
WORKING
IN
AN
OFFICE
COOKING
OR
ROOM
CLEANING
THE
VIDEOS
WERE
DOWNLOADED
FROM
YOUTUBE
BY
PLACING
QUERIES
SUCH
AS
TIME
LAPSE
PARTY
SEARCH
RESULTS
WERE
MANUALLY
VERIFIED
TO
CONTAIN
ONLY
VIDEOS
CAPTURED
WITH
A
STATIONARY
CAMERA
AND
SHOWING
AN
INDOOR
SCENE
ALL
VIDEOS
ARE
SPARSELY
SAMPLED
IN
TIME
WITH
LIMITED
TEMPORAL
CONTINUITY
BETWEEN
CONSECUTIVE
FRAMES
THE
DATASET
REPRESENTS
A
CHALLENGING
UNCONTROLLED
SETUP
WHERE
PEOPLE
PERFORM
NATURAL
NON
STAGED
INTERACTIONS
WITH
OBJECTS
IN
A
VARIETY
OF
REAL
INDOOR
SCENES
WE
MANUALLY
ANNOTATED
EACH
VIDEO
WITH
GROUND
TRUTH
SEGMENTATION
MASKS
OF
EIGHT
FREQUENTLY
OCCURRING
SEMANTIC
OBJECT
CLASSES
BED
SOFA
ARMCHAIR
COF
FEE
TABLE
CHAIR
TABLE
WARDROBE
CUPBOARD
CHRISTMAS
TREE
AND
OTHER
SIMILAR
TO
THE
OTHER
CLASS
CONTAINS
VARIOUS
FOREGROUND
ROOM
CLUTTER
SUCH
AS
CLOTHES
ON
THE
FLOOR
OR
OBJECTS
E
G
LAMPS
BOTTLES
OR
DISHES
ON
TABLES
IN
ADDI
TION
TO
OBJECTS
WE
ALSO
ANNOTATED
THREE
ROOM
BACKGROUND
CLASSES
WALL
CEILING
AND
FLOOR
AS
THE
CAMERA
AND
MAJORITY
OF
THE
OBJECTS
ARE
STATIC
WE
CAN
COLLECT
HUNDREDS
OR
EVEN
THOUSANDS
OF
REALISTIC
PERSON
OBJECT
INTERACTIONS
THROUGHOUT
THE
WHOLE
TIME
LAPSE
SEQUENCE
BY
PROVIDING
A
SINGLE
OBJECT
ANNOTATION
PER
VIDEO
THE
DATASET
IS
DIVIDED
INTO
SPLITS
OF
AROUND
VIDEOS
WITH
APPROXIMATELY
THE
SAME
PROPORTION
OF
LABELS
FOR
DIFFERENT
OBJECTS
THE
DATASET
INCLUDING
THE
ANNOTATIONS
IS
AVAILABLE
AT
EXPERIMENTS
IN
THIS
SECTION
WE
GIVE
THE
IMPLEMENTATION
DETAILS
AND
THEN
SHOW
RESULTS
FOR
I
POSE
ESTIMATION
II
SEMANTIC
LABELING
OF
OBJECTS
IN
TIME
LAPSE
VIDEOS
AND
III
PREDICTING
LIKELY
POSES
FOR
NEW
SCENES
IMPLEMENTATION
DETAILS
THE
FOREGROUND
BACKGROUND
SEGMENTATION
IN
EACH
VIDEO
FRAME
IS
ESTIMATED
USING
A
PIXEL
WISE
ADAPTIVE
MIXTURE
OF
GAUSSIAN
WITH
COMPONENTS
WITH
Α
AND
T
WE
ALSO
COMPUTE
A
SINGLE
BACKGROUND
IMAGE
FOR
EACH
VIDEO
THAT
CONTAINS
NO
PEOPLE
BY
TAKING
THE
MEDIAN
OF
BACKGROUND
SEGMENTS
ACROSS
ALL
VIDEO
FRAMES
PERSON
DETECTIONS
AND
HUMAN
POSE
ESTIMATES
IN
EACH
FRAME
ARE
OBTAINED
USING
THE
METHOD
AND
CODE
OF
DETECTIONS
IN
THE
BACKGROUND
SEGMENTS
AND
WITH
CONFIDENCE
SMALLER
THAN
ARE
REMOVED
THE
THRESHOLD
E
FOR
THE
GROUND
PLANE
BASED
GEOMETRIC
FILTER
IS
SET
TO
SUPER
PIXELS
FOR
EACH
VIDEO
ARE
GENERATED
USING
THE
CODE
OF
WITH
PARAMETERS
Σ
K
AND
MIN
SIFT
FEATURES
ARE
EXTRACTED
FROM
PATCHES
OF
SIZE
PIXELS
WITH
SPATIAL
OVERLAP
TO
TRAIN
THE
PROPOSED
MODEL
WE
USE
SPLITS
OF
THE
DATASET
SEE
SECTION
TO
CROSS
VALIDATE
THE
C
PARAMETER
OF
THE
SVM
AND
USE
THE
SPLIT
TO
CALIBRATE
THE
OUTPUTS
OF
THE
INDIVIDUAL
CLASSIFIERS
THE
RESULTING
MODEL
IS
TESTED
ON
THE
SPLIT
THIS
IS
REPEATED
FIVE
TIMES
FOR
THE
DIFFERENT
TEST
SPLITS
TO
OBTAIN
THE
MEAN
AND
STANDARD
DEVIATION
OF
THE
CLASSIFICATION
PERFORMANCE
POSE
ESTIMATION
TO
EVALUATE
PERSON
DETECTION
AND
POSE
ESTIMATION
PERFOR
MANCE
WE
HAVE
ANNOTATED
POSES
OF
AT
LEAST
TEN
RANDOMLY
CHOSEN
PERSON
OCCUR
RENCES
IN
EACH
VIDEO
RESULTING
IN
POSE
ANNOTATIONS
PERSON
BOUNDING
BOX
DETECTION
PERFORMANCE
IS
MEASURED
USING
THE
STANDARD
AVERAGE
PRECISION
AP
AND
POSE
ESTIMATION
PERFORMANCE
IS
MEASURED
BY
THE
PERCENTAGE
OF
CORRECT
PARTS
PCP
SCORE
AMONG
THE
DETECTED
PEOPLE
AS
PROPOSED
IN
WE
FIRST
COMPARE
OUR
INDIVIDUALLY
TRAINED
POSE
ESTIMATORS
FOR
EACH
ACTION
SEE
SECTION
WITH
A
SINGLE
MODEL
TRAINED
ON
IMAGES
FROM
ALL
ACTION
CLASSES
BOTH
HAVE
A
SIMILAR
RE
CALL
OF
AROUND
BUT
THE
INDIVIDUALLY
TRAINED
MODELS
ACHIEVE
AN
AVERAGE
PCP
OF
COMPARED
TO
FOR
THE
SINGLE
MODEL
WE
THEN
EVALUATE
THE
EFFECT
OF
THE
BACKGROUND
SUBTRACTION
AND
GEOMETRIC
FILTERING
FOR
PERSON
DETECTION
THE
INDI
VIDUALLY
TRAINED
MODELS
ACHIEVE
AN
AP
OF
WHICH
IS
SIGNIFICANTLY
IMPROVED
BY
BACKGROUND
SUBTRACTION
AND
GEOMETRIC
FILTERING
SEMANTIC
LABELING
OF
OBJECTS
SEMANTIC
LABELING
PERFORMANCE
IS
MEASURED
BY
PIXEL
WISE
PRECISION
RECALL
CURVE
AND
AVERAGE
PRECISION
AP
FOR
EACH
OBJECT
TABLE
SHOWS
THE
AVERAGE
PRECISION
FOR
DIFFERENT
OBJECT
AND
ROOM
BACKGROUND
CLASSES
FOR
DIFFERENT
FEATURE
COMBINATIONS
OF
OUR
METHOD
PERFORMANCE
IS
COM
PARED
TO
TWO
BASELINES
THE
METHOD
OF
TRAINED
ON
OUR
DATA
WITH
SEMANTIC
OBJECT
ANNOTATIONS
AND
THE
DEFORMABLE
PART
MODEL
DPM
OF
TRAINED
OVER
MANUALLY
DEFINED
BOUNDING
BOXES
FOR
EACH
CLASS
AT
TEST
TIME
THE
DPM
BOUND
ING
BOXES
ARE
CONVERTED
TO
SEGMENTATION
MASKS
BY
ASSIGNING
TO
EACH
TESTING
PIXEL
THE
MAXIMUM
SCORE
OF
ANY
OVERLAPPING
DETECTION
NOTE
THAT
COMBINING
THE
PRO
POSED
POSE
FEATURES
WITH
APPEARANCE
A
P
RESULTS
IN
A
SIGNIFICANT
IMPROVEMENT
TABLE
AVERAGE
PRECISION
AP
FOR
BASELINES
OF
FELZENSZWALBET
AL
AND
HEDAUET
AL
COMPARED
TO
FOUR
DIFFERENT
SETTINGS
OF
OUR
METHOD
APPEARANCE
AND
LOCATION
FEATURES
ONLY
A
L
PERSON
FEATURES
ONLY
P
APPEARANCE
AND
PERSON
FEATURES
A
P
APPEARANCE
LOCATION
AND
PERSON
FEATURES
COMBINED
A
L
P
IN
OVERALL
PERFORMANCE
BUT
FURTHER
ADDING
LOCATION
FEATURES
A
L
P
BRINGS
LITTLE
ADDITIONAL
BENEFIT
WHICH
SUGGESTS
THAT
SPATIAL
INFORMATION
IN
THE
SCENE
IS
LARGELY
CAPTURED
BY
THE
SPATIAL
RELATION
TO
THE
HUMAN
POSE
THE
PROPOSED
METHOD
A
L
P
ALSO
SIGNIFICANTLY
OUTPERFORMS
BOTH
BASELINES
EXAMPLE
CLASSIFICATION
RESULTS
FOR
THE
PROPOSED
METHOD
ARE
SHOWN
IN
FIGURE
FINALLY
LEARNT
WEIGHTS
FOR
DIFFERENT
OBJECTS
ARE
VISUALIZED
IN
FIGURE
WE
HAVE
ALSO
EVALUATED
OUR
MODEL
ON
FUNCTIONAL
SURFACE
ESTIMATION
FOR
TRAIN
ING
AND
TESTING
WE
HAVE
PROVIDED
GROUND
TRUTH
FUNCTIONAL
SURFACE
MASKS
FOR
THE
DATASET
OF
OUR
MODEL
ACHIEVES
AP
OF
AND
FOR
WALKABLE
SIT
TABLE
AND
REACHABLE
SURFACES
RESPECTIVELY
AVERAGING
A
GAIN
OF
COMPARED
TO
WHICH
COULD
BE
ATTRIBUTED
TO
THE
DISCRIMINATIVE
NATURE
OF
OUR
MODEL
PREDICTING
POSES
IN
NEW
SCENES
FIGURE
SHOWS
QUALITATIVE
RESULTS
OF
PRE
DICTING
LIKELY
HUMAN
POSES
IN
NEW
SCENES
GIVEN
A
PERSON
BOUNDING
BOX
AND
THE
MANUALLY
LABELLED
OBJECT
REGIONS
THE
MOST
LIKELY
POSE
IS
PREDICTED
USING
EQ
AS
CAN
BE
SEEN
THE
AUTOMATICALLY
GENERATED
POSES
ARE
CONSISTENT
WITH
OBJECT
CLASSES
AS
WELL
AS
WITH
THE
SCENE
GEOMETRY
DESPITE
NO
EXPLICIT
REASONING
IS
INCLUDED
IN
OUR
MODEL
DISCUSSION
WE
HAVE
PROPOSED
A
STATISTICAL
DESCRIPTOR
OF
PERSON
OBJECT
INTERACTIONS
AND
HAVE
DEMONSTRATED
ITS
BENEFITS
FOR
RECOGNIZING
OBJECTS
AND
PREDICTING
HUMAN
BODY
POSES
IN
NEW
SCENES
NOTABLY
OUR
METHOD
REQUIRES
VERY
LITTLE
ANNOTATION
AND
RELIES
ON
LONG
TERM
OBSERVATIONS
OF
PEOPLE
IN
TIME
LAPSE
VIDEOS
GIVEN
THE
MUTUAL
DEPENDENCE
OF
OBJECTS
AND
HUMAN
POSES
THE
CURRENT
METHOD
CAN
BE
FURTHER
EXTENDED
TO
PERFORM
JOINT
POSE
ESTIMATION
AND
OBJECT
RECOGNITION
STYLE
AWARE
MID
LEVEL
REPRESENTATION
FOR
DISCOVERING
VISUAL
CONNECTIONS
IN
SPACE
AND
TIME
PAPER
PRESENTATION
BY
BHAVIN
MODI
SLIDES
BY
YONG
JAE
LEE
ALEXEI
A
EFROS
AND
MARTIAL
HEBERT
CARNEGIE
MELLON
UNIVERSITY
UC
BERKELEY
ICCV
LONG
BEFORE
THE
AGE
OF
DATA
MINING
WHEN
HISTORICAL
DATING
WHERE
BOTANY
GEOGRAPHY
THE
VIEW
FROM
YOUR
WINDOW
CHALLENGE
LOW
LEVEL
VISUAL
WORDS
SIVIC
ZISSERMAN
LAPTEV
LINDEBERG
CZURKA
ET
AL
VISUAL
WORLD
OBJECT
CATEGORY
DISCOVERY
SIVIC
ET
AL
GRAUMAN
DARRELL
RUSSELL
ET
AL
LEE
GRAUMAN
PAYET
TODOROVIC
FAKTOR
IRANI
KANG
ET
AL
MOST
APPROACHES
MINE
GLOBALLY
CONSISTENT
PATTERNS
MID
LEVEL
VISUAL
ELEMENTS
DOERSCH
ET
AL
ENDRES
ET
AL
JUNEJA
ET
AL
FOUHEY
ET
AL
DOERSCH
ET
AL
RECENT
METHODS
DISCOVER
SPECIFIC
VISUAL
PATTERNS
MUCH
IN
OUR
VISUAL
WORLD
UNDERGOES
A
GRADUAL
CHANGE
TEMPORAL
MUCH
IN
OUR
VISUAL
WORLD
UNDERGOES
A
GRADUAL
CHANGE
SPATIAL
MINE
MID
LEVEL
VISUAL
ELEMENTS
IN
TEMPORALLY
AND
SPATIALLY
VARYING
DATA
AND
MODEL
THEIR
VISUAL
STYLE
WHEN
HISTORICAL
DATING
OF
CARS
KIM
ET
AL
FU
ET
AL
PALERMO
ET
AL
WHERE
GEOLOCALIZATION
OF
STREETVIEW
IMAGES
CRISTANI
ET
AL
HAYS
EFROS
KNOPP
ET
AL
CHEN
GRAUMAN
SCHINDLER
ET
AL
ESTABLISH
CONNECTIONS
CLOSED
WORLD
MODEL
STYLE
SPECIFIC
DIFFERENCES
APPROACH
UNSUPERVISED
DISCOVERY
OF
MID
LEVEL
DISCRIMINATIVE
PATCHES
FIG
THE
TOP
TWO
DETECTED
VISUAL
WORDS
BO
TTOM
VS
MID
LEVEL
DISCRIM
INATIVE
PAT
CHES
T
OP
T
RAINED
WIT
HOUT
ANY
SUPERVISIO
N
A
ND
ON
T
HE
E
LARGE
UNLABELED
DATASET
CAN
WE
GET
NICE
PARTS
WITHOUT
SUPERVISION
IDEA
K
MEANS
CLUSTERING
IN
HOG
SPACE
STILL
NOT
GOOD
ENOUGH
THE
SVM
MEMORIZES
BAD
EXAMPLES
AND
STILL
SCORES
THEM
HIGHLY
HOWEVER
THE
SPACE
OF
BAD
EXAMPLES
IS
MUCH
MORE
DIVERSE
SO
WE
CAN
AVOID
OVERFITTING
IF
WE
TRAIN
ON
A
TRAINING
SUBSET
BUT
LOOK
FOR
PATCHES
ON
A
VALIDATION
SUBSET
WHY
K
MEANS
ON
HOG
FAILS
CHICKEN
EGG
PROBLEM
IF
WE
KNOW
THAT
A
SET
OF
PATCHES
ARE
VISUALLY
SIMILAR
WE
CAN
EASILY
LEARN
A
DISTANCE
METRIC
FOR
THEM
IF
WE
KNOW
THE
DISTANCE
METRIC
WE
CAN
EASILY
FIND
OTHER
MEMBERS
START
WITH
K
MEANS
TRAIN
A
DISCRIMINATIVE
CLASSIFIER
FOR
THE
DISTANCE
FUNCTION
USING
ALL
OTHER
CLASSES
AS
NEGATIVE
EXAMPLES
RE
ASSIGN
PATCHES
TO
CLUSTERS
WHOSE
CLASSIFIER
GIVES
HIGHEST
SCORE
REPEAT
START
WITH
K
MEANS
OR
KNN
TRAIN
A
DISCRIMINATIVE
CLASSIFIER
FOR
THE
DISTANCE
FUNCTION
USING
DETECTION
DETECT
THE
PATCHES
AND
ASSIGN
TO
TOP
K
CLUSTERS
REPEAT
CAN
WE
GET
GOOD
PARTS
WITHOUT
SUPERVISION
WHAT
MAKES
A
GOOD
PART
MUST
OCCUR
FREQUENTLY
IN
ONE
CLASS
REPRESENTATIVE
MUST
NOT
OCCUR
FREQUENTLY
IN
ALL
CLASSES
DISCRIMINATIVE
SPLIT
THE
DISCOVERY
DATASET
INTO
TWO
EQUAL
PARTS
TRAINING
AND
VALIDATION
TRAIN
ON
THE
TRAINING
SUBSET
RUN
THE
TRAINED
CLASSIFIER
ON
THE
VALIDATION
SET
TO
COLLECT
EXAMPLES
EXCHANGE
TRAINING
AND
VALIDATION
SETS
REPEAT
ALGORIT
HM
DI
COVER
TOP
N
DISC
INAT
IVE
PAT
CHES
REQUIRE
DISCOVERY
SET
V
ATURAL
WORLD
ET
N
D
N
L
DIV
ID
D
N
INT
O
EQUAL
SIZED
DISJOINT
SE
RAND
PL
T
SAMPLE
RANDOM
PATCHE
FROM
K
KM
ANS
L
CLUST
ER
PCT
CHES
U
I
NG
K
LEANS
WHILE
NOT
CONVERG
D
DO
U
FOR
ALL
I
SUCH
THA
SIZ
K
I
DO
L
PRUNE
OUT
SMALL
ONES
CNEW
I
SVM
RAIN
K
I
T
T
RAIN
CLASSIFIER
FOR
EACH
CLUST
ER
KNEW
I
DET
ECT
OP
C
I
I
FIND
OP
M
NEVL
MEMBER
IN
OT
HER
SET
END
FOR
K
KNEW
C
CNEW
W
A
P
SWA
P
T
SWAP
THE
HVO
SETS
END
WHILE
A
I
PURIT
Y
K
I
X
DI
SCRIM
IN
ATI
VEN
ESS
K
I
VI
T
COMPUT
E
SCOR
RET
URN
SELE
CT
OP
C
A
N
T
SORT
ACCORDING
O
SCORES
AND
SELECT
OP
N
PATCHES
DOUBLETS
DISCOVER
SECOND
ORDER
RELATIONSHIPS
START
WITH
HIGH
SCORING
PATCHES
FIND
SPATIAL
CORRELATIONS
TO
OTHER
WEAKER
PATCHES
RANK
THE
POTENTIAL
DOUBLETS
ON
VALIDATION
SET
DOUBLETS
AP
ON
MIT
INDOOR
SCENE
RECOGNITION
DATASET
COMING
BACK
SAMPLE
PATCHES
AND
COMPUTE
NEAREST
NEIGHBORS
DALAL
TRIGGS
HOG
STYLE
SENSITIVE
STYLE
INSENSITIVE
TIGHT
UNIFORM
PEAKY
LOW
ENTROPY
CLUSTERS
UNIFORM
HIGH
ENTROPY
CLUSTERS
TAKE
TOP
RANKED
CLUSTERS
TO
BUILD
CORRESPONDENCES
DATASET
TRAIN
A
DETECTOR
HOG
LINEAR
SVM
SINGH
ET
AL
NATURAL
WORLD
BACKGROUND
DATASET
TOP
DETECTION
PER
DECADE
SINGH
ET
AL
WE
EXPECT
STYLE
TO
CHANGE
GRADUALLY
NATURAL
WORLD
BACKGROUND
DATASET
TOP
DETECTION
PER
DECADE
INITIAL
MODEL
FINAL
MODEL
INITIAL
MODEL
FINAL
MODEL
REGRESSION
MODEL
REGRESSION
MODEL
SUPPORT
VECTOR
REGRESSORS
WITH
GAUSSIAN
KERNELS
INPUT
HOG
OUTPUT
DATE
GEO
LOCATION
DETECTOR
REGRESSION
OUTPUT
DETECTOR
REGRESSION
OUTPUT
TRAIN
IMAGE
LEVEL
REGRESSION
MODEL
USING
OUTPUTS
OF
VISUAL
ELEMENT
DETECTORS
AND
REGRESSORS
AS
FEATURES
FIGURE
WE
IS
ALIZE
HE
TYLE
HAT
A
STYLE
JW
ARE
REORES
OR
L
LEARNED
B
Y
AVERAOIN
THE
FOR
E
CH
D
ECADE
RESULTS
RESULTS
DATE
GEO
LOCATION
PREDICTION
CRAWLED
FROM
CRAWLED
FROM
GOOGLE
STREET
VIEW
IMAGES
TAGGED
WITH
YEAR
IMAGES
TAGGED
WITH
GPS
COORDINATE
N
CAROLINA
TO
GEORGIA
CARL
B
DATE
PR
EDID
II
O
N
ERROR
EDB
GEO
L
O
C
ATIIION
PR
EDIC
FI
ON
ERROR
Q
I
LO
I
T
I
WLO
II
C
I
I
I
B
SINGH
E
TI
AL
SIP
BOW
OURS
SP
SINGH
ET
AL
IBOW
FIGURE
BOX
PLOTS
SHOWING
DATE
AND
LOCATION
TION
ERROR
ON
THE
CARDB
AND
ED
B
DA
TASETS
RESPECTIVELY
LOWER
VALUE
ARE
BET
TER
QUR
APPROACH
THE
UBTLE
STYLISTIC
DIFFERENCES
FOR
EACH
DISCOVERED
ENT
IN
THE
DATA
WHICH
LEADS
TO
LOWER
ERROR
RATES
RESULTS
DATE
GEO
LOCATION
PREDICTION
CRAWLED
FROM
CRAWLED
FROM
GOOGLE
STREET
VIEW
RESULTS
LEARNED
STYLES
AVERAGE
OF
TOP
PREDICTIONS
PER
DECADE
EXTRA
FINE
GRAINED
RECOGNITION
MEAN
CLASSIFICATION
ACCURACY
ON
CALTECH
UCSD
BIRDS
DATASET
WEAK
SUPERVISION
STRONG
SUPERVISION
CONCLUSIONS
MODELS
VISUAL
STYLE
APPEARANCE
CORRELATED
WITH
TIME
SPACE
FIRST
ESTABLISH
VISUAL
CONNECTIONS
TO
CREATE
A
CLOSED
WORLD
THEN
FOCUS
ON
STYLE
SPECIFIC
DIFFERENCES
EXPERIMENT
PRESENTATION
FOR
VISUAL
RECOGNITION
PRESENTER
ZITAO
LIU
UNIVERSITY
OF
PITTSBURGH
MARCH
GOAL
DISCOVER
CONNECTIONS
GOAL
DISCOVER
CONNECTIONS
BETWEEN
RECURRING
MID
LEVEL
VISUAL
ELEMENTS
GOAL
DISCOVER
CONNECTIONS
BETWEEN
RECURRING
MID
LEVEL
VISUAL
ELEMENTS
IN
HISTORIC
TEMPORAL
AND
GEOGRAPHIC
SPATIAL
IMAGE
COLLECTIONS
GOAL
DISCOVER
CONNECTIONS
BETWEEN
RECURRING
MID
LEVEL
VISUAL
ELEMENTS
IN
HISTORIC
TEMPORAL
AND
GEOGRAPHIC
SPATIAL
IMAGE
COLLECTIONS
AND
ATTEMPTS
TO
CAPTURE
THE
UNDERLYING
VISUAL
STYLE
VISUAL
STYLE
APPEARANCE
VARIATIONS
OF
THE
SAME
VISUAL
ELEMENT
DUE
TO
CHANGE
IN
TIME
OR
LOCATION
THREE
STEPS
MINING
STYLE
SENSITIVE
VISUAL
ELEMENTS
ESTABLISHING
CORRESPONDENCES
TRAINING
STYLE
AWARE
REGRESSION
MODELS
DATA
THREE
DATASETS
ARE
USED
IN
THE
PAPER
AND
ONE
DATASET
CARDB
CAN
BE
DOWNLOADED
FROM
THE
AUTHOR
WEBSITE
CARDB
WITH
PHOTOS
FOR
TRAINING
AND
FOR
TESTING
CARDB
CONTAINS
CARS
MADE
IN
CRAWLED
FROM
EXTERNAL
PACKAGES
VOC
EXTRACTING
FEATURES
LIBSVM
MAKING
CLASSIFICATION
AND
REGRESSION
DECADES
IN
TOTAL
GOAL
FIND
VISUAL
ELEMENTS
WHOSE
APPEARANCE
CORRELATES
WITH
THE
DATE
LABEL
WHAT
CODE
DOES
SELECT
QUERY
IMAGES
FROM
EACH
CATEGORY
RANDOM
SAMPLE
PATCHES
WITH
VARIOUS
SCALES
AND
LOCATIONS
FROM
EACH
QUERY
IMAGE
EACH
PATCH
IS
REPRESENTED
BY
A
HISTOGRAM
OF
GRADIENTS
HOG
GOAL
FIND
VISUAL
ELEMENTS
WHOSE
APPEARANCE
CORRELATES
WITH
THE
DATE
LABEL
WHAT
CODE
DOES
FIND
TOP
N
N
NEAREST
NEIGHBOR
PATCHES
IN
THE
DATABASE
FOREACH
QUERY
DECADE
IMAGE
SUBSETS
QUERYDEC
FOREACH
QUERY
IMAGE
IN
QUERYDEC
FOREACH
MATCH
DECADE
IMAGE
SUBSETS
MATCHDEC
FOREACH
IMAGE
IN
MATCHDEC
I
FOREACH
DETECTOR
IN
IMAGE
I
MORE
THAN
IMAGES
GOAL
FIND
VISUAL
ELEMENTS
WHOSE
APPEARANCE
CORRELATES
WITH
THE
DATE
LABEL
WHAT
CODE
DOES
FOR
EACH
CLUSTER
COMPUTE
THE
TEMPORAL
DISTRIBUTION
OF
LABELS
COMPUTE
ENTROPY
BASED
ON
YEAR
DISTRIBUTION
FIND
GOOD
CLUSTERS
WITH
PEAKY
DISTRIBUTIONS
RANK
GOOD
CLUSTERS
AND
SELECT
TOP
M
M
AS
THE
DISCOVERED
STYLE
SENSITIVE
VISUAL
ELEMENTS
PEAKY
EXAMPLES
ETC
FLAT
EXAMPLES
ETC
GOAL
MODEL
THE
CHANGE
IN
STYLE
OF
THE
SAME
VISUAL
ELEMENT
OVER
THE
ENTIRE
LABEL
SPACE
WHAT
CODE
DOES
TRAIN
A
LINEAR
SVM
WITH
INITIAL
CLUSTER
IMAGES
NEGATIVE
DATA
ARE
SAMPLED
FROM
RANDOM
FLICKR
IMAGES
RUN
THE
LEARNED
SVM
ON
A
NEW
SUBSET
OF
THE
DATA
THAT
SLIGHTLY
BROADER
RANGE
IN
LABEL
SPACE
KEEP
THE
MOST
CONFIDENT
PREDICTION
EXAMPLE
AND
CONTINUE
CROSS
VALIDATION
IS
USED
IN
EACH
STEP
OF
THE
INCREMENTAL
REVISION
GOOD
POSITIVE
TRAINING
EXAMPLES
1972
GOAL
PREDICTION
WHAT
CODE
DOES
TRAIN
A
SVM
WITH
RBF
KERNELS
ON
THE
CORRESPONDENCES
OBTAINED
FROM
STEP
EACH
TRAINING
EXAMPLE
IS
WEIGHTED
BY
A
TRANSFORMED
DETECTION
SCORE
KEEP
IN
MIND
WE
HAVE
DECADES
CLUSTERS
FOR
EACH
DECADES
NEIGHBORS
IN
EACH
CLUSTER
THANK
YOU
Q
A
UNSUPERVISED
DISCOVERY
OF
MID
LEVEL
DISCRIMINATIVE
PATCHES
SAURABH
SINGH
ABHINAV
GUPTA
AND
ALEXEI
A
EFROS
CARNEGIE
MELLON
UNIVERSITY
PITTSBURGH
PA
USA
HTTP
GRAPHICS
CS
CMU
EDU
PROJECTS
DISCRIMINATIVEPATCHES
ABSTRACT
THE
GOAL
OF
THIS
PAPER
IS
TO
DISCOVER
A
SET
OF
DISCRIMINATIVE
PATCHES
WHICH
CAN
SERVE
AS
A
FULLY
UNSUPERVISED
MID
LEVEL
VISUAL
REPRE
SENTATION
THE
DESIRED
PATCHES
NEED
TO
SATISFY
TWO
REQUIREMENTS
TO
BE
REPRESENTATIVE
THEY
NEED
TO
OCCUR
FREQUENTLY
ENOUGH
IN
THE
VISUAL
WORLD
TO
BE
DISCRIMINATIVE
THEY
NEED
TO
BE
DI
ERENT
ENOUGH
FROM
THE
REST
OF
THE
VISUAL
WORLD
THE
PATCHES
COULD
CORRESPOND
TO
PARTS
OBJECTS
VISUAL
PHRASES
ETC
BUT
ARE
NOT
RESTRICTED
TO
BE
ANY
ONE
OF
THEM
WE
POSE
THIS
AS
AN
UNSUPERVISED
DISCRIMINATIVE
CLUSTERING
PROBLEM
ON
A
HUGE
DATASET
OF
IMAGE
PATCHES
WE
USE
AN
ITERATIVE
PROCEDURE
WHICH
ALTERNATES
BETWEEN
CLUSTERING
AND
TRAINING
DISCRIMINATIVE
CLASSI
ERS
WHILE
APPLYING
CAREFUL
CROSS
VALIDATION
AT
EACH
STEP
TO
PREVENT
OVER
TTING
THE
PAPER
EX
PERIMENTALLY
DEMONSTRATES
THE
E
ECTIVENESS
OF
DISCRIMINATIVE
PATCHES
AS
AN
UNSUPERVISED
MID
LEVEL
VISUAL
REPRESENTATION
SUGGESTING
THAT
IT
COULD
BE
USED
IN
PLACE
OF
VISUAL
WORDS
FOR
MANY
TASKS
FURTHERMORE
DISCRIM
INATIVE
PATCHES
CAN
ALSO
BE
USED
IN
A
SUPERVISED
REGIME
SUCH
AS
SCENE
CLASSI
CATION
WHERE
THEY
DEMONSTRATE
STATE
OF
THE
ART
PERFORMANCE
ON
THE
MIT
INDOOR
DATASET
INTRODUCTION
CONSIDER
THE
IMAGE
IN
FIGURE
SHOWN
IN
GREEN
ARE
THE
TWO
MOST
CON
DENT
VISUAL
WORDS
DETECTED
IN
THIS
IMAGE
AND
THE
CORRESPONDING
VISUAL
WORD
CLUSTERS
SHOWN
IN
RED
ARE
THE
TWO
MOST
CON
DENT
DETECTIONS
USING
OUR
PROPOSED
MID
LEVEL
DISCRIMINATIVE
PATCHES
COMPUTED
ON
THE
SAME
LARGE
UNLABELED
IMAGE
DATASET
AS
THE
VISUAL
WORDS
WITHOUT
ANY
SUPERVISION
FOR
MOST
PEOPLE
THE
REPRESENTATION
AT
THE
TOP
SEEMS
INSTANTLY
MORE
INTUITIVE
AND
REASONABLE
IN
THIS
PAPER
WE
WILL
SHOW
THAT
IT
IS
ALSO
SIMPLE
TO
COMPUTE
AND
O
ERS
VERY
GOOD
DISCRIMINABILITY
BROAD
COVERAGE
BETTER
PURITY
AND
IMPROVED
PERFORMANCE
COMPARED
TO
VISUAL
WORD
FEATURES
FINALLY
WE
WILL
ALSO
SHOW
HOW
OUR
APPROACH
CAN
BE
USED
IN
A
SUPERVISED
SETTING
WHERE
IT
DEMONSTRATES
STATE
OF
THE
ART
PERFORMANCE
ON
SCENE
CLASSI
CATION
BEATING
BAG
OF
WORDS
SPATIAL
PYRAMIDS
OBJECTBANK
AND
SCENE
DEFORMABLE
PARTS
MODELS
ON
THE
MIT
INDOOR
DATASET
WHAT
ARE
THE
RIGHT
PRIMITIVES
FOR
REPRESENTING
VISUAL
INFORMATION
THIS
IS
A
QUESTION
AS
OLD
AS
THE
COMPUTER
VISION
DISCIPLINE
ITSELF
AND
IS
UNLIKELY
TO
BE
SETTLED
ANYTIME
SOON
OVER
THE
YEARS
RESEARCHERS
HAVE
PROPOSED
A
PLETHORA
OF
DI
ERENT
VISUAL
FEATURES
SPANNING
A
WIDE
SPECTRUM
FROM
VERY
LOCAL
TO
FULL
IMAGE
SINGH
ET
AL
OUR
DISCRIMINA
VE
PATCHES
VISUAL
WORDS
FIG
THE
TOP
TWO
DETECTED
VISUAL
WORDS
BOTTOM
VS
MID
LEVEL
DISCRIMINATIVE
PATCHES
TOP
TRAINED
WITHOUT
ANY
SUPERVISION
AND
ON
THE
SAME
LARGE
UNLABELED
DATASET
AND
FROM
LOW
LEVEL
BOTTOM
UP
TO
SEMANTIC
TOP
DOWN
IN
TERMS
OF
SPATIAL
RESOLUTION
ONE
EXTREME
IS
USING
THE
PIXEL
ITSELF
AS
A
PRIMITIVE
HOWEVER
THERE
IS
GENERALLY
NOT
ENOUGH
INFORMATION
AT
A
PIXEL
LEVEL
TO
MAKE
A
USEFUL
FEATURE
IT
WILL
RE
ALL
THE
TIME
AT
THE
OTHER
EXTREME
ONE
CAN
USE
THE
WHOLE
IMAGE
AS
A
PRIMITIVE
WHICH
WHILE
SHOWING
GREAT
PROMISE
IN
SOME
APPLICATIONS
REQUIRES
EXTRAORDINARILY
LARGE
AMOUNTS
OF
TRAINING
DATA
SINCE
ONE
NEEDS
TO
REPRESENT
ALL
POSSIBLE
SPATIAL
CON
GURATIONS
OF
OBJECTS
IN
THE
WORLD
EXPLICITLY
AS
A
RESULT
MOST
RESEARCHERS
HAVE
CONVERGED
ON
USING
FEATURES
AT
AN
INTERMEDIATE
SCALE
THAT
OF
AN
IMAGE
PATCH
BUT
EVEN
IF
WE
X
THE
RESOLUTION
OF
THE
PRIMITIVE
THERE
IS
STILL
A
WIDE
RANGE
OF
CHOICES
TO
BE
MADE
REGARDING
WHAT
THIS
PRIMITIVE
AIMS
TO
REPRESENT
FROM
THE
LOW
LEVEL
BOTTOM
UP
POINT
OF
VIEW
AN
IMAGE
PATCH
SIMPLY
REPRESENTS
THE
APPEARANCE
AT
THAT
POINT
EITHER
DIRECTLY
WITH
RAW
PIXELS
OR
TRANSFORMED
INTO
A
DI
ERENT
REPRESENTATION
LTERBANK
RESPONSE
VECTOR
BLURRED
OR
SPATIALLY
BINNED
FEATURE
ETC
AT
A
SLIGHTLY
HIGHER
LEVEL
COMBINING
SUCH
PATCHES
TOGETHER
TYPICALLY
BY
CLUSTERING
AND
HISTOGRAMMING
ALLOWS
ONE
TO
REPRESENT
TEXTURE
INFORMATION
E
G
TEXTONS
DENSE
BAG
OF
WORDS
ETC
A
BIT
HIGHER
STILL
ARE
APPROACHES
THAT
ENCODE
IMAGE
PATCHES
ONLY
AT
SPARSE
INTEREST
POINTS
IN
A
SCALE
AND
ROTATION
INVARIANT
WAY
SUCH
AS
IN
SIFT
MATCHING
OVERALL
THE
BOTTOM
UP
APPROACHES
WORK
VERY
WELL
FOR
MOST
PROBLEMS
INVOLVING
EXACT
INSTANCE
MATCHING
BUT
THEIR
RECORD
FOR
GENERALIZATION
I
E
NDING
SIMILAR
INSTANCES
IS
MORE
MIXED
ONE
EXPLANATION
IS
THAT
AT
THE
LOW
LEVEL
IT
IS
VERY
HARD
TO
KNOW
WHICH
PARTS
OF
THE
REPRESENTATION
ARE
THE
IMPORTANT
ONES
AND
WHICH
COULD
BE
SAFELY
IGNORED
AS
A
RESULT
RECENTLY
SOME
RESEARCHERS
HAVE
STARTED
LOOKING
AT
HIGH
LEVEL
FEA
TURES
WHICH
ARE
ALREADY
IMPREGNATED
WITH
SEMANTIC
INFORMATION
NEEDED
TO
GEN
ERALIZE
WELL
FOR
EXAMPLE
A
NUMBER
OF
PAPERS
HAVE
USED
FULL
BLOWN
OBJECT
DETEC
TORS
E
G
AS
FEATURES
TO
DESCRIBE
AND
REASON
ABOUT
IMAGES
E
G
OTHERS
HAVE
EMPLOYED
DISCRIMINATIVE
PART
DETECTORS
SUCH
AS
POSELETS
AT
TRIBUTE
DETECTORS
VISUAL
PHRASES
OR
STU
DETECTORS
AS
FEATURES
UNSUPERVISED
DISCOVERY
OF
MID
LEVEL
DISCRIMINATIVE
PATCHES
HOWEVER
THERE
ARE
SIGNI
CANT
PRACTICAL
BARRIERS
TO
THE
WIDE
SPREAD
ADAPTATION
OF
SUCH
TOP
DOWN
SEMANTIC
TECHNIQUES
FIRST
THEY
ALL
REQUIRE
NON
TRIVIAL
AMOUNTS
OF
HAND
LABELED
TRAINING
DATA
PER
EACH
SEMANTIC
ENTITY
OBJECT
PART
ATTRIBUTE
ETC
SECOND
MANY
SEMANTIC
ENTITIES
ARE
JUST
NOT
DISCRIMINATIVE
ENOUGH
VISUALLY
TO
ACT
AS
GOOD
FEATURES
FOR
EXAMPLE
WALL
IS
A
WELL
DE
NED
SEMANTIC
CATEGORY
WITH
PLENTY
OF
TRAINING
DATA
AVAILABLE
BUT
IT
MAKES
A
LOUSY
DETECTOR
SIMPLY
BECAUSE
WALLS
ARE
USUALLY
PLAIN
AND
THUS
NOT
EASILY
DISCRIMINABLE
IN
THIS
PAPER
WE
CONSIDER
MID
LEVEL
VISUAL
PRIMITIVES
WHICH
ARE
MORE
ADAPT
ABLE
TO
THE
APPEARANCE
DISTRIBUTIONS
IN
THE
REAL
WORLD
THAN
THE
LOW
LEVEL
FEATURES
BUT
DO
NOT
REQUIRE
THE
SEMANTIC
GROUNDING
OF
THE
HIGH
LEVEL
ENTITIES
WE
PROPOSE
A
REPRESENTATION
CALLED
MID
LEVEL
DISCRIMINATIVE
PATCHES
THESE
PATCHES
COULD
CORRESPOND
TO
PARTS
OBJECTS
VISUAL
PHRASES
ETC
BUT
ARE
NOT
RESTRICTED
TO
BE
ANY
ONE
OF
THEM
WHAT
DE
NES
THEM
IS
THEIR
REPRESENTATIVE
AND
DISCRIMINATIVE
PROPERTY
THAT
IS
THAT
THEY
CAN
BE
DETECTED
IN
A
LARGE
NUMBER
OF
IMAGES
WITH
HIGH
RECALL
AND
PRECISION
BUT
UNLIKE
OTHER
DISCRIMINATIVE
METHODS
WHICH
ARE
WEAKLY
SUPERVISED
EITHER
WITH
IMAGE
LABELS
E
G
OR
BOUNDING
BOX
LABELS
E
G
OUR
DISCRIMINATIVE
PATCHES
CAN
BE
DISCOVERED
IN
A
FULLY
UNSUPERVISED
MANNER
GIVEN
ONLY
A
LARGE
PILE
OF
UNLABELED
THE
KEY
INSIGHT
OF
THIS
PAPER
IS
TO
POSE
THIS
AS
AN
UNSUPERVISED
DISCRIMINATIVE
CLUSTERING
PROBLEM
ON
A
HUGE
UNLA
BELED
DATASET
OF
IMAGE
PATCHES
WE
USE
AN
ITERATIVE
PROCEDURE
WHICH
ALTERNATES
BETWEEN
CLUSTERING
AND
TRAINING
DISCRIMINATIVE
CLASSI
ERS
LINEAR
SVMS
WHILE
APPLYING
CAREFUL
CROSS
VALIDATION
AT
EACH
STEP
TO
PREVENT
OVER
TTING
SOME
OF
THE
RESULTING
DISCRIMINATIVE
PATCHES
ARE
SHOWN
IN
FIGURE
PRIORWORK
OUR
GOALS
ARE
VERY
MUCH
IN
COMMON
WITH
PRIOR
WORK
ON
NDING
GOOD
MID
LEVEL
FEATURE
REPRESENTATIONS
MOST
NOTABLY
THE
ORIGINAL
VISUAL
WORDS
APPROACH
GIVEN
SPARSE
KEY
POINT
DETECTIONS
OVER
A
LARGE
DATASET
THE
IDEA
IS
TO
CLUSTER
THEM
IN
SIFT
SPACE
IN
AN
E
ORT
TO
YIELD
MEANINGFUL
COMMON
UNITS
OF
VISUAL
MEANING
AKIN
TO
WORDS
IN
TEXT
HOWEVER
IN
PRACTICE
IT
TURNS
OUT
THAT
WHILE
SOME
VISUAL
WORDS
DO
CAPTURE
HIGH
LEVEL
OBJECT
PARTS
MOST
OTHERS
END
UP
ENCODING
SIMPLE
ORIENTED
BARS
AND
CORNERS
AND
MIGHT
MORE
APPROPRIATELY
BE
CALLED
VISUAL
PHONEMES
OR
EVEN
VISUAL
LETTERS
THE
WAY
ADDRESSED
THESE
SHORTCOMINGS
WAS
BY
USING
IMAGE
SEGMENTS
AS
A
MID
LEVEL
UNIT
FOR
NDING
COMMONALITY
SINCE
THEN
THERE
HAS
BEEN
A
LARGE
BODY
OF
WORK
IN
THE
GENERAL
AREA
OF
UNSUPERVISED
OBJECT
DISCOVERY
WHILE
WE
SHARE
SOME
OF
THE
SAME
CONCEPTUAL
GOALS
OUR
WORK
IS
QUITE
DI
ERENT
IN
THAT
WE
DO
NOT
EXPLICITLY
AIM
TO
DISCOVER
WHOLE
SEMANTIC
UNITS
LIKE
OBJECTS
OR
PARTS
UNLIKE
WE
DO
NOT
ASSUME
A
SINGLE
OBJECT
PER
IMAGE
WHEREAS
IN
OBJECT
DISCOVERY
THERE
IS
NO
SEPARATE
TRAINING
AND
TEST
SET
WE
EXPLICITLY
AIM
TO
DISCOVER
PATCHES
THAT
ARE
DETECTABLE
IN
NOVEL
IMAGES
BECAUSE
ONLY
VISUAL
WORDS
HAVE
ALL
THE
ABOVE
PROPERTIES
THAT
WILL
BE
OUR
MAIN
POINT
OF
COMPARISON
OUR
PAPER
IS
VERY
MUCH
INSPIRED
BY
POSELETS
BOTH
IN
ITS
GOAL
OF
NDING
REPRESENTATIVE
YET
DISCRIMINATIVE
REGIONS
AND
ITS
USE
OF
HOG
DESCRIPTORS
AND
N
B
THE
TERM
UNSUPERVISED
HAS
CHANGED
ITS
MEANING
OVER
THE
YEARS
E
G
WHILE
THE
AWARD
WINNING
PAPER
OF
FERGUS
ET
AL
HAD
UNSUPERVISED
IN
ITS
TITLE
IT
WOULD
NOW
BE
CONSIDERED
A
WEAKLY
SUPERVISED
METHOD
SINGH
ET
AL
FIG
EXAMPLES
OF
DISCOVERED
DISCRIMINATIVE
PATCHES
THAT
WERE
HIGHLY
RANKED
LINEAR
SVMS
HOWEVER
POSELETS
IS
A
HEAVILY
SUPERVISED
METHOD
EMPLOYING
LABELS
AT
THE
IMAGE
BOUNDING
BOX
AND
PART
LEVELS
WHEREAS
OUR
APPROACH
AIMS
TO
SOLVE
A
MUCH
HARDER
PROBLEM
WITHOUT
ANY
SUPERVISION
AT
ALL
SO
DIRECT
COMPARISONS
BETWEEN
THE
TWO
WOULD
NOT
BE
MEANINGFUL
OUR
WORK
IS
ALSO
INFORMED
BY
WHO
SHOW
THAT
DISCRIMINATIVE
MACHINERY
SUCH
AS
A
LINEAR
SVM
COULD
BE
SUCCESSFULLY
USED
IN
A
FULLY
UNSUPERVISED
MANNER
DISCOVERING
DISCRIMINATIVE
PATCHES
GIVEN
AN
ARBITRARY
SET
OF
UNLABELED
IMAGES
THE
DISCOVERY
DATASET
D
OUR
GOAL
IS
TO
DISCOVER
A
RELATIVELY
SMALL
NUMBER
OF
DISCRIMINATIVE
PATCHES
AT
ARBITRARY
RESOLUTION
WHICH
CAN
CAPTURE
THE
ESSENCE
OF
THAT
DATA
THE
CHALLENGE
IS
THAT
THE
SPACE
OF
POTENTIAL
PATCHES
REPRESENTED
IN
THIS
PAPER
BY
HOG
FEATURES
IS
EXTREMELY
LARGE
SINCE
EVEN
A
SINGLE
IMAGE
CAN
GENERATE
TENS
OF
THOUSANDS
OF
PATCHES
AT
MULTIPLE
SCALES
UNSUPERVISED
DISCOVERY
OF
MID
LEVEL
DISCRIMINATIVE
PATCHES
APPROACH
MOTIVATION
OF
OUR
TWO
KEY
REQUIREMENTS
FOR
GOOD
DISCRIMINATIVE
PATCHES
TO
OCCUR
FRE
QUENTLY
AND
TO
BE
SU
CIENTLY
DI
ERENT
FROM
THE
REST
OF
THE
VISUAL
WORLD
THE
RST
ONE
IS
ACTUALLY
COMMON
TO
MOST
OTHER
OBJECT
DISCOVERY
APPROACHES
THE
STANDARD
SOLUTION
IS
TO
EMPLOY
SOME
FORM
OF
UNSUPERVISED
CLUSTERING
SUCH
AS
K
MEANS
EITHER
ON
THE
ENTIRE
DATASET
OR
ON
A
RANDOMLY
SAMPLED
SUBSET
HOWEVER
RUNNING
K
MEANS
ON
OUR
MID
LEVEL
PATCHES
DOES
NOT
PRODUCE
VERY
GOOD
CLUSTERS
AS
SHOWN
ON
FIGURE
INITIAL
KMEANS
THE
REASON
IS
THAT
UNSUPERVISED
CLUSTERING
LIKE
K
MEANS
HAS
NO
CHOICE
BUT
TO
USE
A
LOW
LEVEL
DISTANCE
METRIC
E
G
EUCLIDEAN
CROSS
CORRELATION
WHICH
DOES
NOT
WORK
WELL
FOR
MEDIUM
SIZED
PATCHES
OF
TEN
COMBINING
INSTANCES
WHICH
ARE
IN
NO
WAY
VISUALLY
SIMILAR
OF
COURSE
IF
WE
SOMEHOW
KNEW
THAT
A
SET
OF
PATCHES
WERE
VISUALLY
SIMILAR
WE
COULD
EASILY
TRAIN
A
DISCRIMINATIVE
CLASSI
ER
SUCH
AS
A
LINEAR
SVM
TO
PRODUCE
AN
APPROPRIATE
SIM
ILARITY
METRIC
FOR
THESE
PATCHES
IT
WOULD
SEEM
WE
HAVE
A
CLASSIC
CHICKEN
AND
EGG
PROBLEM
THE
CLUSTERING
OF
THE
PATCHES
DEPENDS
ON
A
GOOD
SIMILARITY
BUT
LEARNING
A
SIMILARITY
DEPENDS
ON
OBTAINING
GOOD
CLUSTERS
BUT
NOTICE
THAT
WE
CAN
POSE
THIS
PROBLEM
AS
A
TYPE
OF
ITERATIVE
DISCRIMINATIVE
CLUSTERING
IN
A
TYPICAL
INSTANTIATION
E
G
AN
INITIAL
CLUSTERING
OF
DATA
IS
FOLLOWED
BY
LEARNING
A
DISCRIMINATIVE
CLASSI
ER
FOR
EACH
CLUSTER
BASED
ON
THE
DISCRIMINATIVELY
LEARNED
SIMILARITY
NEW
CLUSTER
MEMBERSHIPS
CAN
BE
COMPUTED
BY
REASSIGNING
DATA
POINTS
TO
EACH
CLUSTER
ETC
IN
PRINCIPLE
THIS
PROCEDURE
WILL
SATISFY
BOTH
OF
OUR
REQUIREMENTS
THE
CLUSTERING
STEP
WILL
LATCH
ONTO
FREQUENTLY
OCCURRING
PATCHES
WHILE
THE
CLASSI
CATION
STEP
WILL
MAKE
SURE
THAT
THE
PATCHES
IN
THE
CLUSTERS
ARE
DI
ERENT
ENOUGH
FROM
THE
REST
AND
THUS
DISCRIMINATIVE
HOWEVER
THIS
APPROACH
WILL
NOT
WORK
ON
OUR
PROBLEM
AS
IS
SINCE
IT
IS
INFEASIBLE
TO
USE
A
DISCOVERY
DATASET
LARGE
ENOUGH
TO
BE
REPRESENTATIVE
OF
THE
ENTIRE
VISUAL
WORLD
IT
WILL
REQUIRE
TOO
MANY
CLUSTERS
TO
ADDRESS
THIS
WE
TURN
THE
CLASSI
CATION
STEP
OF
DISCRIMINATIVE
CLUSTERING
INTO
A
DETECTION
STEP
MAKING
EACH
PATCH
CLUSTER
INTO
A
DETECTOR
TRAINED
USING
A
LINEAR
SVM
TO
ND
OTHER
PATCHES
LIKE
THOSE
IT
ALREADY
OWNS
THIS
MEANS
THAT
EACH
CLUSTER
IS
NOW
TRAINED
TO
BE
DISCRIMINATIVE
NOT
JUST
AGAINST
THE
OTHER
CLUSTERS
IN
THE
DISCOVERY
DATASET
D
BUT
AGAINST
THE
REST
OF
THE
VISUAL
WORLD
WHICH
WE
PROPOSE
TO
MODEL
BY
A
NATURAL
WORLD
DATASET
N
THE
ONLY
REQUIREMENT
OF
N
IS
THAT
IT
BE
VERY
LARGE
THOUSANDS
OF
IMAGES
CONTAINING
TENS
OF
MILLIONS
OF
PATCHES
AND
DRAWN
FROM
A
REASONABLY
RANDOM
IMAGE
DISTRIBUTION
WE
FOLLOW
IN
SIMPLY
USING
RANDOM
PHOTOS
FROM
THE
INTERNET
NOTE
THAT
N
IS
NOT
A
NEGATIVE
SET
AS
IT
CAN
AND
MOST
LIKELY
WILL
CONTAIN
VISUAL
PATTERNS
ALSO
FOUND
IN
D
WE
ALSO
EXPERIMENTED
WITH
D
N
IT
IS
INTERESTING
TO
NOTE
THE
SIMILARITY
BETWEEN
THIS
VERSION
OF
DISCRIMINATIVE
CLUSTERING
AND
THE
ROOT
LTER
LATENT
UPDATES
OF
THERE
TOO
A
CLUSTER
OF
PATCHES
REPRESENTING
AN
OBJECT
CATEGORY
IS
BEING
ITERATIVELY
RE
NED
BY
MAKING
IT
MORE
DISCRIMINATIVE
AGAINST
MILLIONS
OF
OTHER
IMAGE
PATCHES
HOWEVER
WHEREAS
IMPOSES
OVERLAP
CONSTRAINTS
PREVENTING
THE
CLUSTER
FROM
MOVING
TOO
FAR
FROM
THE
SUPERVISED
INITIALIZATION
IN
OUR
UNSUPERVISED
FORMULATION
THE
CLUSTERS
ARE
COMPLETELY
UNCONSTRAINED
SINGH
ET
AL
INI
AL
KMEANS
DISCRIMINA
VE
ITER
ITER
CLUSTERING
OUR
APPROACH
FIG
FEW
EXAMPLES
TO
SHOW
HOW
OUR
ITERATIVE
APPROACH
STARTING
WITH
INITIAL
K
MEANS
CLUSTERING
CONVERGES
TO
CONSISTENT
CLUSTERS
ITER
WHILE
STANDARD
DISCRIMINATIVE
CLUS
TERING
APPROACH
SECOND
ROW
ALSO
CONVERGES
IN
SOME
CASES
COLUMN
IN
VAST
MAJOR
ITY
OF
CASES
IT
MEMORIZES
AND
OVER
TS
NOTE
THAT
OUR
APPROACH
ALLOWS
CLUSTERS
TO
MOVE
AROUND
IN
X
Y
AND
SCALE
SPACE
TO
ND
BETTER
MEMBERS
OVAL
IN
COLUMN
ALAS
OUR
PROPOSED
DISCRIMINATIVE
CLUSTERING
PROCEDURE
IS
STILL
NOT
QUITE
ENOUGH
CONSIDER
FIGURE
WHICH
SHOWS
THREE
EXAMPLE
CLUSTERS
THE
TOP
ROW
IS
SIMPLE
INI
TIALIZATION
USING
K
MEANS
WHILE
THE
SECOND
ROW
SHOWS
THE
RESULTS
OF
THE
DISCRIMI
NATIVE
CLUSTERING
DESCRIBED
ABOVE
THE
LEFT
MOST
CLUSTER
SHOWS
GOOD
IMPROVEMENT
COMPARED
TO
INITIALIZATION
BUT
THE
OTHER
TWO
CLUSTERS
SEE
LITTLE
CHANGE
THE
CUL
PRIT
SEEMS
TO
BE
THE
SVM
IT
IS
SO
GOOD
AT
MEMORIZING
THE
TRAINING
DATA
THAT
IT
IS
OFTEN
UNWILLING
TO
BUDGE
FROM
THE
INITIAL
CLUSTER
CON
GURATION
TO
COM
BAT
THIS
WE
PROPOSE
AN
EXTREMELY
SIMPLE
BUT
SURPRISINGLY
E
ECTIVE
SOLUTION
CROSS
VALIDATION
TRAINING
INSTEAD
OF
TRAINING
AND
CLASSIFYING
THE
SAME
DATA
WE
DIVIDE
OUR
INPUT
DATASET
INTO
TWO
EQUAL
NON
OVERLAPPING
SUBSETS
WE
PERFORM
A
STEP
OF
DISCRIMINATIVE
CLUSTERING
ON
THE
TRAINING
SUBSET
BUT
THEN
APPLY
OUR
LEARNED
DISCRIMINATIVE
PATCHES
ON
THE
VALIDATION
SUBSET
TO
FORM
CLUSTERS
THERE
IN
THIS
WAY
WE
ARE
ABLE
TO
ACHIEVE
BETTER
GENERALIZATION
SINCE
THE
ERRORS
IN
THE
TRAINING
SET
ARE
LARGELY
UNCORRELATED
WITH
ERRORS
IN
THE
VALIDATION
SET
AND
HENCE
THE
SVM
IS
NOT
ABLE
TO
OVER
T
TO
THEM
WE
THEN
EXCHANGE
THE
ROLES
OF
TRAINING
AND
VALIDATION
AND
REPEAT
THE
WHOLE
PROCESS
UNTIL
CONVERGENCE
FIGURE
SHOWS
THE
ITERATIONS
OF
OUR
ALGORITHM
FOR
THE
THREE
INITIAL
PATCH
CLUSTERS
SHOWING
TOP
PATCHES
IN
EACH
CLUSTER
NOTE
HOW
THE
CONSISTENCY
OF
THE
CLUSTERS
IMPROVES
SIGNI
CANTLY
AFTER
EACH
ITERATION
NOTE
ALSO
THAT
THE
CLUSTERS
CAN
MOVE
AROUND
IN
X
Y
AND
SCALE
SPACE
TO
LATCH
ONTO
THE
MORE
DISCRIMINATIVE
PARTS
OF
THE
VISUAL
SPACE
SEE
THE
CIRCLED
TRAIN
IN
THE
RIGHT
MOST
COLUMN
APPROACH
DETAILS
INITIALIZATION
THE
INPUT
TO
OUR
DISCOVERY
ALGORITHM
IS
A
DISCOVERY
DATASET
D
OF
UNLABELED
IMAGES
AS
WELL
AS
A
MUCH
LARGER
NATURAL
WORLD
DATASET
N
IN
THIS
UNSUPERVISED
DISCOVERY
OF
MID
LEVEL
DISCRIMINATIVE
PATCHES
ALGORITHM
DISCOVER
TOP
N
DISCRIMINATIVE
PATCHES
REQUIRE
DISCOVERY
SET
D
NATURAL
WORLD
SET
N
D
N
DIVIDE
D
N
INTO
EQUAL
SIZED
DISJOINT
SETS
RAND
SAMPLE
SAMPLE
RANDOM
PATCHES
FROM
K
KMEANS
CLUSTER
PATCHES
USING
KMEANS
WHILE
NOT
CONVERGED
DO
FOR
ALL
I
SUCH
THAT
SIZE
K
I
DO
PRUNE
OUT
SMALL
ONES
CNEW
I
SVM
TRAIN
K
I
TRAIN
CLASSI
ER
FOR
EACH
CLUSTER
KNEW
I
DETECT
TOP
C
I
M
FIND
TOP
M
NEW
MEMBERS
IN
OTHER
SET
END
FOR
K
KNEW
C
CNEW
SWAP
SWAP
SWAP
THE
TWO
SETS
END
WHILE
A
I
PURITY
K
I
DISCRIMINATIVENESS
K
I
I
COMPUTE
SCORES
RETURN
SELECT
TOP
C
A
N
SORT
ACCORDING
TO
SCORES
AND
SELECT
TOP
N
PATCHES
PAPER
WE
USED
IMAGES
RANDOMLY
SAMPLED
FROM
FLICKR
COM
FIRST
WE
DIVIDE
BOTH
D
AND
N
INTO
TWO
EQUAL
NON
OVERLAPPING
SUBSETS
AND
FOR
CROSS
VALIDATION
FOR
ALL
IMAGES
IN
WE
COMPUTE
HOG
DESCRIPTORS
AT
MULTIPLE
RESOLUTIONS
AT
DI
ERENT
SCALES
TO
INITIALIZE
OUR
ALGORITHM
WE
RANDOMLY
SAMPLE
PATCHES
FROM
ABOUT
PER
IMAGE
DISALLOWING
HIGHLY
OVERLAPPING
PATCHES
OR
PATCHES
WITH
NO
GRADIENT
ENERGY
E
G
SKY
PATCHES
AND
THEN
RUN
STANDARD
K
MEANS
CLUSTERING
IN
HOG
SPACE
SINCE
WE
DO
NOT
TRUST
K
MEANS
TO
GENERALIZE
WELL
WE
SET
K
QUITE
HIGH
K
PRODUCING
TENS
OF
THOUSANDS
OF
CLUSTERS
MOST
WITH
VERY
FEW
MEMBERS
WE
REMOVE
CLUSTERS
WITH
LESS
THAN
PATCHES
ELIMINATING
OF
THE
CLUSTERS
ENDING
UP
WITH
ABOUT
PATCHES
PER
IMAGE
STILL
ACTIVE
ITERATIVE
ALGORITHM
GIVEN
AN
INITIAL
SET
OF
CLUSTERS
K
WE
TRAIN
A
LINEAR
SVM
CLASSI
ER
FOR
EACH
CLUSTER
USING
PATCHES
WITHIN
THE
CLUSTER
AS
POSITIVE
EXAMPLES
AND
ALL
PATCHES
OF
AS
NEGATIVE
EXAMPLES
ITERATIVE
HARD
MINING
IS
USED
TO
HANDLE
THE
COMPLEXITY
IF
WE
EXCLUDE
NEAR
DUPLICATES
FROM
BY
NORMALIZED
CROSS
CORRELATION
THE
TRAINED
DISCRIMINATIVE
CLASSI
ERS
ARE
THEN
RUN
ON
THE
HELD
OUT
VALIDATION
SET
AND
NEW
CLUSTERS
ARE
FORMED
FROM
THE
TOP
M
RINGS
OF
EACH
DETECTOR
WE
CONSIDER
ALL
SVM
SCORES
ABOVE
TO
BE
RINGS
WE
LIMIT
THE
NEW
CLUSTERS
TO
ONLY
M
MEMBERS
TO
KEEP
CLUSTER
PURITY
HIGH
USING
MORE
PRODUCES
MUCH
LESS
HOMOGENEOUS
CLUSTERS
ON
THE
OTHER
HAND
IF
A
CLUSTER
DETECTOR
RES
LESS
THAN
TIMES
ON
THE
VALIDATION
SET
THIS
SUGGESTS
THAT
IT
MIGHT
NOT
BE
VERY
DISCRIMINATIVE
AND
IS
KILLED
THE
VALIDATION
SET
NOW
BECOMES
THE
TRAINING
SET
AND
THE
PROCEDURE
IS
REPEATED
UNTIL
CONVERGENCE
I
E
THE
TOP
M
PATCHES
IN
A
CLUSTER
DO
NOT
CHANGE
IN
PRACTICE
THE
ALGORITHM
CONVERGES
IN
ITERATIONS
THE
FULL
APPROACH
IS
SUMMARIZED
IN
ALGORITHM
PARAMETERS
THE
SIZE
OF
OUR
HOG
DESCRIPTOR
IS
CELLS
WITH
A
STRIDE
OF
PIXELS
CELL
SO
THE
MINIMUM
POSSIBLE
PATCH
IS
PIXELS
WHILE
THE
MAXIMUM
COULD
BE
AS
LARGE
AS
FULL
IMAGE
WE
USE
A
LINEAR
SVM
C
WITH
ITERATIONS
OF
HARD
NEGATIVE
MINING
FOR
MORE
DETAILS
CONSULT
THE
SOURCE
CODE
ON
THE
WEBSITE
SINGH
ET
AL
FIG
VISUALIZING
IMAGES
LEFT
IN
TERMS
OF
THEIR
MOST
DISCRIMINATIVE
PATCHES
RIGHT
THE
PATCH
DETECTORS
WERE
RED
ON
A
NOVEL
IMAGE
AND
THE
HIGH
SCORING
PATCH
DETECTIONS
WERE
AVERAGED
TOGETHER
WEIGHTED
BY
THEIR
SCORES
RANKING
DISCRIMINATIVE
PATCHES
OUR
ALGORITHM
PRODUCES
A
DICTIONARY
OF
A
FEW
THOUSAND
DISCRIMINATIVE
PATCHES
OF
VARYING
QUALITY
OUR
NEXT
TASK
IS
TO
RANK
THEM
TO
ND
A
SMALL
NUMBER
OF
THE
MOST
DISCRIMINATIVE
ONES
OUR
CRITERIA
FOR
RANKING
CONSISTS
OF
TWO
TERMS
PURITY
IDEALLY
A
GOOD
CLUSTER
SHOULD
HAVE
ALL
ITS
MEMBER
PATCHES
COME
FROM
THE
SAME
VISUAL
CONCEPT
HOWEVER
MEASURING
PURITY
IN
AN
UNSUPERVISED
SETTING
IS
IMPOSSIBLE
THEREFORE
WE
APPROXIMATE
THE
PURITY
OF
EACH
CLUSTER
IN
TERMS
OF
THE
CLASSI
ER
CON
DENCE
OF
THE
CLUSTER
MEMBERS
ASSUMING
THAT
CROSS
VALIDATION
REMOVED
OVER
TTING
THUS
THE
PURITY
SCORE
FOR
A
CLUSTER
IS
COMPUTED
BY
SUMMING
UP
THE
SVM
DETECTION
SCORES
OF
TOP
R
CLUSTER
MEMBERS
WHERE
R
M
TO
EVALUATE
THE
GENERALIZATION
OF
THE
CLUSTER
BEYOND
THE
M
TRAINING
PATCHES
DISCRIMINATIVENESS
IN
AN
UNSUPERVISED
SETTING
THE
ONLY
THING
WE
CAN
SAY
IS
THAT
A
HIGHLY
DISCRIMINATIVE
PATCH
SHOULD
RE
RARELY
IN
THE
NATURAL
WORLD
THEREFORE
WE
DE
NE
DISCRIMINATIVENESS
OF
A
PATCH
AS
THE
RATIO
OF
THE
NUMBER
OF
RINGS
ON
D
TO
THE
NUMBER
OF
RINGS
ON
D
N
OF
COURSE
WE
DO
NOT
WANT
PATCHES
THAT
NEVER
RE
AT
ALL
BUT
THESE
WOULD
HAVE
ALREADY
BEEN
REMOVED
IN
CROSS
VALIDATION
TRAINING
ALL
CLUSTERS
ARE
RANKED
USING
A
LINEAR
COMBINATION
OF
THE
ABOVE
TWO
SCORES
FIGURE
SHOWS
A
SET
OF
TOP
RANKED
DISCRIMINATIVE
PATCH
CLUSTERS
DISCOVERED
WITH
OUR
APPROACH
NOTE
HOW
SOMETIMES
THE
PATCHES
CORRESPOND
TO
OBJECT
PARTS
SUCH
AS
HORSE
LEGS
AND
HORSE
MUZZLE
SOMETIMES
TO
WHOLE
OBJECTS
SUCH
AS
PLATES
AND
SOMETIMES
THEY
ARE
JUST
DISCRIMINATIVE
PORTIONS
OF
AN
OBJECT
SIMILAR
TO
POSE
LETS
E
G
SEE
THE
CORNER
OF
TRAINS
ALSO
NOTE
THAT
THEY
EXHIBIT
SURPRISINGLY
GOOD
VISUAL
CONSISTENCY
FOR
A
FULLY
UNSUPERVISED
APPROACH
THE
ABILITY
OF
DISCRIMINA
TIVE
PATCHES
TO
RE
ON
VISUALLY
SIMILAR
IMAGE
REGIONS
IS
FURTHER
DEMONSTRATED
IN
FIGURE
WHERE
THE
PATCH
DETECTORS
ARE
APPLIED
TO
A
NOVEL
IMAGE
AND
HIGH
SCORING
DETECTIONS
ARE
DISPLAYED
WITH
THE
AVERAGE
PATCH
FROM
THAT
CLUSTER
IN
A
WAY
THE
GURE
SHOWS
WHAT
OUR
REPRESENTATION
CAPTURES
ABOUT
THE
IMAGE
DISCOVERING
DOUBLETS
WHILE
OUR
DISCRIMINATIVE
PATCH
DISCOVERY
APPROACH
IS
ABLE
TO
PRODUCE
A
NUMBER
OF
VISUALLY
GOOD
HIGHLY
RANKED
DISCRIMINATIVE
PATCHES
SOME
OTHER
POTENTIALLY
PROMISING
ONES
DO
NOT
MAKE
IT
TO
THE
TOP
DUE
TO
LOW
PURITY
THIS
HAPPENS
WHEN
UNSUPERVISED
DISCOVERY
OF
MID
LEVEL
DISCRIMINATIVE
PATCHES
A
CLEAN
CLUSTER
B
NOISY
CLUSTER
C
CLEANED
UP
FIG
CLUSTER
CLEAN
UP
USING
DOUBLETS
A
VISUALLY
NON
HOMOGENEOUS
CLUSTER
B
THAT
HAS
LEARNED
MORE
THAN
ONE
CONCEPT
WHEN
COUPLED
INTO
A
DOUBLET
WITH
A
HIGH
QUALITY
CLUSTER
A
GETS
CLEANED
UP
C
FIG
EXAMPLES
OF
DISCOVERED
DISCRIMINATIVE
DOUBLETS
THAT
WERE
HIGHLY
RANKED
A
CLUSTER
CONVERGES
TO
TWO
OR
MORE
CONCEPTS
BECAUSE
THE
UNDERLYING
CLASSI
ER
IS
ABLE
TO
GENERALIZE
TO
BOTH
CONCEPTS
SIMULTANEOUSLY
E
G
FIGURE
HOWEVER
OFTEN
THE
TWO
CONCEPTS
HAVE
DI
ERENT
RING
PATTERNS
WITH
RESPECT
TO
SOME
OTHER
MID
LEVEL
PATCH
IN
THE
DICTIONARY
E
G
MOTORCYCLE
WHEEL
IN
FIGURE
THEREFORE
WE
PROPOSE
TO
EMPLOY
SECOND
ORDER
SPATIAL
CO
OCCURRENCE
RELATIONSHIPS
AMONG
OUR
DISCRIMINATIVE
PATCHES
AS
A
WAY
OF
CLEANING
THEM
UP
FIGURE
MOREOVER
DISCOVERING
THESE
SECOND
ORDER
RELATIONSHIPS
CAN
PROVIDE
US
WITH
DOUBLETS
WHICH
COULD
BE
FURTHER
GENERALIZED
TO
GROUPLETS
THAT
CAN
THEMSELVES
BE
HIGHLY
DISCRIMINATIVE
AND
USEFUL
AS
MID
LEVEL
FEATURES
IN
THEIR
OWN
RIGHT
TO
DISCOVER
DOUBLETS
WE
START
WITH
A
LIST
OF
HIGHLY
DISCRIMINATIVE
PATCHES
THAT
WILL
SERVE
AS
HIGH
QUALITY
ROOTS
FOR
EACH
ROOT
PATCH
WE
SEARCH
OVER
ALL
THE
OTHER
DISCOVERED
DISCRIMINATIVE
PATCHES
EVEN
POOR
QUALITY
ONES
AND
RECORD
THEIR
RELATIVE
SPATIAL
CON
GURATION
IN
EACH
IMAGE
WHERE
THEY
BOTH
RE
THE
PAIRS
THAT
EXHIBIT
A
HIGHLY
SPATIALLY
CORRELATED
RING
PATTERN
BECOME
POTENTIAL
DOUBLETS
WE
RANK
THE
DOUBLETS
BY
APPLYING
THEM
ON
THE
UNLABELED
VALIDATION
SET
THE
DOUBLETS
ARE
RANKED
HIGH
IF
IN
IMAGES
WHERE
BOTH
PATCHES
RE
THEIR
RELATIVE
SPATIAL
CON
GURATION
IS
CONSISTENT
WITH
WHAT
WAS
OBSERVED
IN
THE
TRAINING
SET
IN
FIGURE
WE
SHOW
SOME
EXAMPLES
OF
HIGHLY
DISCRIMINATIVE
DOUBLETS
NOTICE
THAT
NOT
ONLY
IS
THE
QUALITY
OF
DISCRIMINATIVE
PATCHES
GOOD
BUT
ALSO
THE
SPATIAL
RELATIONSHIPS
WITHIN
THE
DOUBLET
ARE
INTUITIVE
QUANTITATIVE
EVALUATION
AS
WITH
OTHER
UNSUPERVISED
DISCOVERY
APPROACHES
EVALUATION
IS
DI
CULT
WE
HAVE
SHOWN
A
NUMBER
OF
QUALITATIVE
RESULTS
FIGURES
AND
THERE
ARE
MANY
MORE
ON
THE
WEBSITE
FOR
THE
RST
SET
OF
QUANTITATIVE
EVALUATIONS
AS
WELL
AS
FOR
ALL
THE
QUALITATIVE
RESULTS
EXCEPT
FIGURE
WE
HAVE
CHOSEN
A
SUBSET
OF
OF
PASCAL
VOC
IMAGES
AS
OUR
DISCOVERY
DATASET
WE
PICKED
PASCAL
SINGH
ET
AL
VOC
BECAUSE
IT
IS
A
WELL
KNOWN
AND
DI
CULT
DATASET
WITH
RICH
VISUAL
DIVERSITY
AND
SCENE
CLUTTER
MOREOVER
IT
PROVIDES
ANNOTATIONS
FOR
A
NUMBER
OF
OBJECT
CLASSES
WHICH
COULD
BE
USED
TO
EVALUATE
OUR
UNSUPERVISED
DISCOVERY
HOWEVER
SINCE
OUR
DISCOVERED
PATCHES
ARE
NOT
MEANT
TO
CORRESPOND
TO
SEMANTIC
OBJECTS
THIS
EVALUATION
METRIC
SHOULD
BE
TAKEN
WITH
QUITE
A
FEW
GRAINS
OF
SALT
ONE
WAY
TO
EVALUATE
THE
QUALITY
OF
OUR
DISCRIMINATIVE
PATCH
CLUSTERS
IS
BY
USING
THE
STANDARD
UNSUPERVISED
DISCOVERY
MEASURES
OF
PURITY
AND
COVERAGE
E
G
PURITY
IS
DE
NED
BY
WHAT
PERCENTAGE
OF
CLUSTER
MEMBERS
CORRESPOND
TO
THE
SAME
VISUAL
ENTITY
IN
OUR
CASE
WE
WILL
USE
PASCAL
SEMANTIC
CATEGORY
ANNOTATIONS
AS
A
SURROGATE
FOR
VISUAL
SIMILARITY
FOR
EACH
OF
THE
TOP
DISCOV
ERED
PATCHES
WE
RST
ASSIGN
IT
TO
ONE
OF
THE
SEMANTIC
CATEGORIES
USING
MAJORITY
MEMBERSHIP
WE
THEN
MEASURE
PURITY
AS
PERCENTAGE
OF
PATCHES
ASSIGNED
TO
THE
SAME
PASCAL
SEMANTIC
LABEL
COVERAGE
IS
DE
NED
AS
THE
NUMBER
OF
IMAGES
IN
THE
DATASET
COVERED
RED
ON
BY
A
GIVEN
CLUSTER
FIGURE
REPORTS
THE
PURITY
AND
COVERAGE
OF
OUR
APPROACH
AND
A
NUMBER
OF
BASELINES
FOR
EACH
ONE
THE
GRAPHS
SHOW
THE
CUMULATIVE
PURITY
COVERAGE
AS
NUMBER
OF
CLUSTERS
BEING
CONSIDERED
IS
INCREASED
THE
CLUSTERS
ARE
SORTED
IN
THE
DECREASING
ORDER
OF
PURITY
WE
COMPARE
OUR
APPROACH
WITH
VISUAL
WORDS
AND
RUSSELL
ET
AL
BASELINE
PLUS
A
NUMBER
OF
INTERMEDIATE
RESULTS
OF
OUR
METHOD
HOG
K
MEANS
VISUAL
WORD
ANALOG
FOR
HOG
FEATURES
INITIAL
CLUSTERING
SVMS
TRAINED
ON
THE
K
MEANS
CLUSTERS
WITHOUT
DISCRIMINATIVE
RE
CLUSTERING
AND
NO
CROSS
VALIDATION
ITERATIVE
DISCRIMINATIVELY
TRAINED
CLUSTERS
BUT
WITH
OUT
CROSS
VALIDATION
IN
EACH
CASE
THE
NUMBERS
INDICATE
AREA
UNDER
THE
CURVE
AUC
FOR
EACH
METHOD
OVERALL
OUR
APPROACH
DEMONSTRATES
SUBSTANTIAL
GAIN
IN
PURITY
WITHOUT
SACRI
CING
MUCH
COVERAGE
AS
COMPARED
TO
THE
ESTABLISHED
AP
PROACHES
MOREOVER
EACH
STEP
OF
OUR
ALGORITHM
IMPROVES
PURITY
NOTE
IN
PAR
TICULAR
THE
SUBSTANTIAL
IMPROVEMENT
A
ORDED
BY
THE
CROSS
VALIDATION
TRAINING
PROCEDURE
COMPARED
TO
STANDARD
TRAINING
AS
WE
MENTIONED
HOWEVER
THE
EXPERIMENT
ABOVE
UNDER
REPORTS
THE
PURITY
OF
OUR
CLUSTERS
SINCE
SEMANTIC
EQUIVALENCE
IS
NOT
THE
SAME
AS
VISUAL
SIMILARITY
THEREFORE
WE
PERFORMED
AN
INFORMAL
PERCEPTUAL
EXPERIMENT
WITH
HUMAN
SUB
JECTS
MEASURING
THE
VISUAL
PURITY
OF
OUR
CLUSTERS
WE
SELECTED
THE
TOP
CLUSTERS
FROM
THE
DATASET
FOR
EACH
CLUSTER
WE
ASKED
HUMAN
LABELERS
TO
MARK
WHICH
OF
THE
CLUSTER
TOP
TEN
RINGS
ON
THE
VALIDATION
SET
ARE
VISUALLY
CONSISTENT
WITH
THE
CLUSTER
BASED
ON
THIS
MEASURE
AVERAGE
VISUAL
PURITY
FOR
THESE
CLUSTERS
WAS
SUPERVISED
IMAGE
CLASSI
CATION
UNSUPERVISED
CLUSTERING
APPROACHES
SUCH
AS
VISUAL
WORDS
HAVE
LONG
BEEN
USED
AS
FEATURES
FOR
SUPERVISED
TASKS
SUCH
AS
CLASSI
CATION
IN
PARTICULAR
BAG
OF
VISUAL
WORDS
AND
SPATIAL
PYRAMIDS
ARE
SOME
OF
THE
MOST
POPULAR
CURRENT
METHODS
FOR
IMAGE
CLASSI
CATION
SINCE
OUR
MID
LEVEL
PATCHES
COULD
BE
CONSIDERED
THE
TRUE
VISUAL
WORDS
AS
OPPOSED
TO
VISUAL
LETTERS
IT
MAKES
SENSE
TO
SEE
HOW
THEY
WOULD
PERFORM
ON
A
SUPERVISED
CLASSI
CATION
TASK
WE
EVALUATE
THEM
IN
TWO
DI
ERENT
SETTINGS
UNSUPERVISED
DISCOVERY
SUPERVISED
CLASSI
CATION
AND
SUPERVISED
DISCOVERY
SUPERVISED
CLASSI
CATION
UNSUPERVISED
DISCOVERY
OF
MID
LEVEL
DISCRIMINATIVE
PATCHES
NUMBER
OF
CLUSTERS
CUMULATIVE
PURITY
PURITY
VISUAL
WORDS
RUSSEL
ET
AL
HOG
KMEANS
INIT
CLUST
NO
CROSSVAL
OUR
APPROACH
NUMBER
OF
CLUSTERS
CUMULATIVE
COVERAGE
COVERAGE
VISUAL
WORDS
RUSSEL
ET
AL
HOG
KMEANS
INIT
CLUST
NO
CROSSVAL
OUR
APPROACH
FIG
QUANTITATIVE
COMPARISON
OF
DISCRIMINATIVE
PATCHES
COMPARED
TO
THE
BASELINE
APPROACHES
QUALITY
OF
CLUSTERING
IS
EVALUATED
IN
TERMS
OF
THE
AREA
UNDER
THE
CURVE
FOR
CUMULATIVE
PURITY
AND
COVERAGE
GIST
SPATIAL
PYRAMID
HOG
SPHOG
Y
SPATIAL
PYRAMID
SIFT
SP
Y
ROI
GIST
SCENE
DPM
MM
SCENE
OBJECT
BANK
OURS
OURS
GIST
OURS
SP
OURS
GIST
SP
OURS
DPM
OURS
GIST
DPM
OURS
SP
DPM
GIST
SP
DPM
OURS
GIST
SP
DPM
TABLE
QUANTITATIVE
EVALUATION
AVERAGE
CLASSI
CATION
ON
MIT
INDOOR
DATASET
CURRENT
STATE
OF
THE
ART
YBEST
PERFORMANCE
FROM
VARIOUS
VOCABULARY
SIZES
UNSUPERVISED
DISCRIMINATIVE
PATCHES
USING
THE
DISCRIMINATIVE
PATCHES
DISCOVERED
FROM
THE
SAME
PASCAL
VOC
DIS
COVERY
DATASET
AS
BEFORE
WE
WOULD
LIKE
TO
SEE
IF
THEY
COULD
MAKE
BETTER
VISUAL
WORDS
FOR
A
SUPERVISED
IMAGE
CLASSI
CATION
TASK
OUR
BASELINE
IS
THE
STANDARD
SPATIAL
PYRAMID
OF
VISUAL
WORDS
USING
VISUAL
WORDS
USING
THEIR
PUBLIC
CODE
FOR
OUR
APPROACH
WE
CONSTRUCT
SPATIAL
PYRAMID
USING
TOP
DIS
CRIMINATIVE
PATCHES
CLASSI
CATION
WAS
PERFORMED
USING
A
SIMPLE
LINEAR
SVM
AND
PERFORMANCE
WAS
EVALUATED
USING
AVERAGE
PRECISION
STANDARD
VISUAL
WORDS
SCORED
AP
WHILE
USING
OUR
DISCRIMINATIVE
PATCHES
THE
SCORE
WAS
AP
WE
FURTHER
EXPANDED
OUR
FEATURE
REPRESENTATION
BY
ADDING
THE
TOP
RANKING
DOUBLETS
AS
EXTRA
VISUAL
WORDS
RESULTING
IN
A
SLIGHT
IMPROVEMENT
TO
AP
SUPERVISED
DISCRIMINATIVE
PATCHES
WE
FURTHER
WANT
TO
EVALUATE
THE
PERFORMANCE
OF
OUR
APPROACH
WHEN
IT
IS
ALLOWED
TO
UTILIZE
MORE
SUPERVISION
FOR
A
FAIR
COMPARISON
WITH
SEVERAL
EXISTING
SUPERVISED
APPROACHES
INSTEAD
OF
DISCOVERING
THE
DISCRIMINATIVE
PATCHES
FROM
A
COMMON
POOL
OF
ALL
THE
IMAGES
WE
CAN
ALSO
DISCOVER
THEM
ON
A
PER
CATEGORY
BASIS
IN
THIS
EXPERIMENT
WE
PERFORM
SUPERVISED
SCENE
CLASSI
CATION
USING
THE
CHALLENGING
MIT
INDOOR
DATASET
CONTAINING
SCENE
CATEGORIES
USING
THE
PROVIDED
SCENE
SINGH
ET
AL
CHURCH
CLOSET
BOWLING
BOOKSTORE
BAKERY
BATHROOM
WINE
CELLAR
SUBWAY
GAME
ROOM
SHOE
SHOP
OFFICE
LAUNDROMAT
HAIR
SALON
DINING
ROOM
STAIRCASE
AUDITORIUM
GROCERY
STORE
MEEBNG
ROOM
FIG
TOP
DISCRIMINATIVE
PATCHES
FOR
A
SAMPLING
OF
SCENES
IN
THE
MIT
INDOOR
SCENE
DATASET
NOTE
HOW
THESE
CAPTURE
VARIOUS
VISUAL
ASPECTS
OF
A
TYPICAL
SCENE
LABELS
WE
DISCOVER
DISCRIMINATIVE
PATCHES
FOR
EACH
SCENE
INDEPENDENTLY
WHILE
TREATING
ALL
OTHER
IMAGES
IN
THE
DATASET
AS
THE
NATURAL
WORLD
FIGURE
SHOWS
TOP
FEW
MOST
DISCRIMINATIVE
PATCHES
DISCOVERED
THIS
WAY
FOR
A
NUMBER
OF
CATEGORIES
FROM
THE
DATASET
IT
IS
INTERESTING
TO
SEE
THAT
THE
DISCRIMINA
TIVE
PATCHES
CAPTURE
ASPECTS
OF
SCENES
THAT
SEEM
VERY
INTUITIVE
TO
US
IN
PARTICULAR
UNSUPERVISED
DISCOVERY
OF
MID
LEVEL
DISCRIMINATIVE
PATCHES
THE
DISCRIMINATIVE
PATCHES
FOR
THE
CHURCH
CATEGORY
CAPTURE
THE
ARCHES
AND
THE
BENCHES
THE
ONES
FOR
THE
MEETING
ROOM
CAPTURE
THE
CENTER
TABLE
AND
THE
SEATS
THESE
DISCRIMINATIVE
PATCHES
ARE
THEREFORE
CAPTURING
THE
ESSENCE
OF
THE
SCENE
IN
TERMS
OF
THESE
HIGHLY
CONSISTENT
AND
REPEATING
PATTERNS
AND
HENCE
PROVIDING
A
SIMPLE
YET
HIGHLY
E
ECTIVE
MID
LEVEL
REPRESENTATION
INSPIRED
BY
THESE
RESULTS
WE
HAVE
ALSO
APPLIED
A
SIMILAR
APPROACH
TO
DISCOVERING
WHAT
MAKES
PARIS
LOOK
LIKE
PARIS
USING
GEOGRAPHIC
LABELS
AS
THE
WEAK
SUPERVISORY
SIGNAL
TO
PERFORM
CLASSI
CATION
TOP
DISCOVERED
PATCHES
OF
EACH
SCENE
ARE
AG
GREGATED
INTO
A
SPATIAL
PYRAMID
USING
MAXPOOLING
OVER
THE
DISCRIMINATIVE
PATCH
SCORES
AS
IN
WE
AGAIN
USE
A
LINEAR
SVM
IN
A
ONE
VS
ALL
CLASSI
CATION
THE
RESULTS
ARE
REPORTED
IN
TABLE
COMPARISON
WITH
HOG
VISUAL
WORDS
SPHOG
SHOWS
THE
HUGE
PERFORMANCE
GAIN
RESULTING
FROM
OUR
ALGORITHM
WHEN
OPERATING
IN
THE
SAME
FEATURE
SPACE
FURTHER
OUR
SIMPLE
METHOD
BY
ITSELF
OUTPERFORMS
ALL
OTHERS
THAT
HAVE
BEEN
TESTED
ON
THIS
DATASET
MOREOVER
COMBINING
OUR
METHOD
WITH
THE
CURRENTLY
BEST
PERFORMING
COMBINATION
APPROACH
OF
YIELDS
PERFORMANCE
WHICH
TO
OUR
KNOWLEDGE
IS
THE
BEST
ON
THIS
DATASET
ACKNOWLEDGMENTS
THE
AUTHORS
WOULD
LIKE
TO
THANK
MARTIAL
HEBERT
TOMASZ
MALISIEWICZ
ABHINAV
SHRIVASTAVA
AND
CARL
DOERSCH
FOR
MANY
HELPFUL
DISCUSSIONS
THIS
WORK
WAS
SUPPORTED
BY
ONR
GRANT
CS
VISUAL
RECOGNITION
DESCRIBING
IMAGES
WITH
FEATURES
ADRIANA
KOVASHKA
DEPARTMENT
OF
COMPUTER
SCIENCE
JANUARY
PLAN
FOR
TODAY
PRESENTATION
ASSIGNMENTS
SCHEDULE
CHANGES
IMAGE
FILTERING
FEATURE
DETECTION
FEATURE
DESCRIPTION
FEATURE
MATCHING
NEXT
TIME
CLASSIFICATION
AND
DETECTION
ADRIANA
RESEARCH
ANNOUNCEMENTS
OPEN
DOOR
POLICY
FIXED
OFFICE
HOURS
ADRIANA
TRAVEL
CLARIFICATION
OF
EXPERIMENT
PRESENTATIONS
PRESENTATION
ASSIGNMENTS
IMAGE
DESCRIPTION
AN
IMAGE
IS
A
SET
OF
PIXELS
PROBLEMS
WITH
PIXEL
REPRESENTATION
NOT
INVARIANT
TO
SMALL
CHANGES
TRANSLATION
ILLUMINATION
ETC
SOME
PARTS
OF
AN
IMAGE
ARE
MORE
IMPORTANT
THAN
OTHERS
WHAT
DO
WE
WANT
TO
REPRESENT
PREPROCESSING
IMAGE
FILTERING
IMAGE
FILTERING
COMPUTE
A
FUNCTION
OF
THE
LOCAL
NEIGHBORHOOD
AT
EACH
PIXEL
IN
THE
IMAGE
FUNCTION
SPECIFIED
BY
A
FILTER
OR
MASK
SAYING
HOW
TO
COMBINE
VALUES
FROM
NEIGHBORS
USES
OF
FILTERING
ENHANCE
AN
IMAGE
DENOISE
RESIZE
ETC
EXTRACT
INFORMATION
TEXTURE
EDGES
ETC
DETECT
PATTERNS
TEMPLATE
MATCHING
MOTIVATION
NOISE
REDUCTION
EVEN
MULTIPLE
IMAGES
OF
THE
SAME
STATIC
SCENE
WILL
NOT
BE
IDENTICAL
COMMON
TYPES
OF
NOISE
SALT
AND
PEPPER
NOISE
RANDOM
OCCURRENCES
OF
BLACK
AND
WHITE
PIXELS
IMPULSE
NOISE
RANDOM
OCCURRENCES
OF
WHITE
PIXELS
GAUSSIAN
NOISE
VARIATIONS
IN
INTENSITY
DRAWN
FROM
A
GAUSSIAN
NORMAL
DISTRIBUTION
MOTIVATION
NOISE
REDUCTION
HOW
COULD
WE
REDUCE
THE
NOISE
I
E
GIVE
AN
ESTIMATE
OF
THE
TRUE
INTENSITIES
WHAT
IF
THERE
ONLY
ONE
IMAGE
LET
REPLACE
EACH
PIXEL
WITH
AN
AVERAGE
OF
ALL
THE
VALUES
IN
ITS
NEIGHBORHOOD
ASSUMPTIONS
EXPECT
PIXELS
TO
BE
LIKE
THEIR
NEIGHBORS
EXPECT
NOISE
PROCESSES
TO
BE
INDEPENDENT
FROM
PIXEL
TO
PIXEL
LET
REPLACE
EACH
PIXEL
WITH
AN
AVERAGE
OF
ALL
THE
VALUES
IN
ITS
NEIGHBORHOOD
MOVING
AVERAGE
IN
CAN
ADD
WEIGHTS
TO
OUR
MOVING
AVERAGE
WEIGHTS
NON
UNIFORM
WEIGHTS
SAY
THE
AVERAGING
WINDOW
SIZE
IS
X
ATTRIBUTE
UNIFORM
WEIGHT
TO
EACH
PIXEL
LOOP
OVER
ALL
PIXELS
IN
NEIGHBORHOOD
AROUND
IMAGE
PIXEL
F
I
J
NOW
GENERALIZE
TO
ALLOW
DIFFERENT
WEIGHTS
DEPENDING
ON
NEIGHBORING
PIXEL
RELATIVE
POSITION
NON
UNIFORM
WEIGHTS
THIS
IS
CALLED
CROSS
CORRELATION
DENOTED
FILTERING
AN
IMAGE
REPLACE
EACH
PIXEL
WITH
A
LINEAR
COMBINATION
OF
ITS
NEIGHBORS
THE
FILTER
KERNEL
OR
MASK
H
U
V
IS
THE
PRESCRIPTION
FOR
THE
WEIGHTS
IN
THE
LINEAR
COMBINATION
WHAT
IF
WE
WANT
NEAREST
NEIGHBORING
PIXELS
TO
HAVE
THE
MOST
INFLUENCE
ON
THE
OUTPUT
REMOVES
HIGH
FREQUENCY
COMPONENTS
FROM
THE
IMAGE
LOW
PASS
FILTER
DALI
MARILYN
EINSTEIN
DESCRIBING
IMAGES
WITH
FEATURES
FEATURE
DETECTION
REPEATABILITY
THE
SAME
FEATURE
CAN
BE
FOUND
IN
SEVERAL
IMAGES
DESPITE
GEOMETRIC
AND
PHOTOMETRIC
TRANSFORMATIONS
SALIENCY
EACH
FEATURE
HAS
A
DISTINCTIVE
DESCRIPTION
COMPACTNESS
AND
EFFICIENCY
MANY
FEWER
FEATURES
THAN
IMAGE
PIXELS
LOCALITY
A
FEATURE
OCCUPIES
A
RELATIVELY
SMALL
AREA
OF
THE
IMAGE
ROBUST
TO
CLUTTER
AND
OCCLUSION
WE
WANT
TO
DETECT
AT
LEAST
SOME
OF
THE
SAME
POINTS
IN
BOTH
IMAGES
NO
CHANCE
TO
FIND
TRUE
MATCHES
YET
WE
HAVE
TO
BE
ABLE
TO
RUN
THE
DETECTION
PROCEDURE
INDEPENDENTLY
PER
IMAGE
WE
WANT
TO
BE
ABLE
TO
RELIABLY
DETERMINE
WHICH
POINT
GOES
WITH
WHICH
MUST
PROVIDE
SOME
INVARIANCE
TO
GEOMETRIC
AND
PHOTOMETRIC
DIFFERENCES
BETWEEN
THE
TWO
VIEWS
WE
SHOULD
EASILY
RECOGNIZE
THE
POINT
BY
LOOKING
THROUGH
A
SMALL
WINDOW
SHIFTING
A
WINDOW
IN
ANY
DIRECTION
SHOULD
GIVE
A
LARGE
CHANGE
IN
INTENSITY
FLAT
REGION
NO
CHANGE
IN
ALL
DIRECTIONS
EDGE
NO
CHANGE
ALONG
THE
EDGE
DIRECTION
CORNER
SIGNIFICANT
CHANGE
IN
ALL
DIRECTIONS
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
WINDOW
FUNCTION
W
X
Y
OR
IN
WINDOW
OUTSIDE
GAUSSIAN
WINDOW
AVERAGED
SQUARED
CHANGE
OF
INTENSITY
INDUCED
BY
SHIFTING
THE
IMAGE
DATA
BY
U
V
E
U
V
EXPANDING
I
X
Y
IN
A
TAYLOR
SERIES
EXPANSION
WE
HAVE
FOR
SMALL
SHIFTS
U
V
A
QUADRATIC
APPROXIMATION
TO
THE
ERROR
SURFACE
BETWEEN
A
PATCH
AND
ITSELF
SHIFTED
BY
U
V
WHERE
M
IS
A
MATRIX
COMPUTED
FROM
IMAGE
DERIVATIVES
M
W
X
Y
I
X
I
X
I
X
I
Y
X
Y
I
Y
I
Y
NOTATION
I
I
X
X
I
I
Y
Y
I
X
I
Y
I
I
X
Y
SINCE
M
IS
SYMMETRIC
WE
HAVE
M
X
X
T
MXI
I
XI
THE
EIGENVALUES
OF
M
REVEAL
THE
AMOUNT
OF
INTENSITY
CHANGE
IN
THE
TWO
PRINCIPAL
ORTHOGONAL
GRADIENT
DIRECTIONS
IN
THE
WINDOW
EDGE
CORNER
AND
ARE
LARGE
FLAT
REGION
AND
ARE
SMALL
MEASURE
OF
CORNER
RESPONSE
K
EMPIRICAL
CONSTANT
K
COMPUTE
IMAGE
GRADIENTS
IX
AND
IY
FOR
ALL
PIXELS
FOR
EACH
PIXEL
COMPUTE
BY
LOOPING
OVER
NEIGHBORS
X
Y
COMPUTE
FIND
POINTS
WITH
LARGE
CORNER
RESPONSE
FUNCTION
R
R
THRESHOLD
TAKE
THE
POINTS
OF
LOCALLY
MAXIMUM
R
AS
THE
DETECTED
FEATURE
POINTS
I
E
PIXELS
WHERE
R
IS
BIGGER
THAN
FOR
ALL
THE
OR
NEIGHBORS
PARTIAL
INVARIANCE
TO
ADDITIVE
AND
MULTIPLICATIVE
INTENSITY
CHANGES
ONLY
DERIVATIVES
ARE
USED
INVARIANCE
TO
INTENSITY
SHIFT
INTENSITY
SCALING
FINE
EXCEPT
FOR
THE
THRESHOLD
THAT
USED
TO
SPECIFY
WHEN
R
IS
LARGE
ENOUGH
R
R
THRESHOLD
X
IMAGE
COORDINATE
X
IMAGE
COORDINATE
INVARIANT
TO
IMAGE
SCALE
IMAGE
ZOOMED
IMAGE
NOT
INVARIANT
TO
IMAGE
SCALE
ALL
POINTS
WILL
BE
CLASSIFIED
AS
EDGES
CORNER
THE
PROBLEM
HOW
DO
WE
CHOOSE
CORRESPONDING
CIRCLES
INDEPENDENTLY
IN
EACH
IMAGE
DO
OBJECTS
IN
THE
IMAGE
HAVE
A
CHARACTERISTIC
SCALE
THAT
WE
CAN
IDENTIFY
SOLUTION
DESIGN
A
FUNCTION
ON
THE
REGION
CIRCLE
WHICH
IS
SCALE
INVARIANT
THE
SAME
FOR
CORRESPONDING
REGIONS
EVEN
IF
THEY
ARE
AT
DIFFERENT
SCALES
TAKE
A
LOCAL
MAXIMUM
OF
THIS
FUNCTION
F
F
SCALE
REGION
SIZE
REGION
SIZE
A
GOOD
FUNCTION
FOR
SCALE
DETECTION
HAS
ONE
STABLE
SHARP
PEAK
F
F
F
REGION
SIZE
REGION
SIZE
REGION
SIZE
FOR
USUAL
IMAGES
A
GOOD
FUNCTION
WOULD
BE
A
ONE
WHICH
RESPONDS
TO
CONTRAST
SHARP
LOCAL
INTENSITY
CHANGE
FUNCTIONS
FOR
DETERMINING
SCALE
KERNELS
LAPLACIAN
DERIVATIVE
OF
GAUSSIAN
DIFFERENCE
OF
GAUSSIANS
WHERE
GAUSSIAN
HARRIS
LAPLACIAN
FIND
LOCAL
MAXIMUM
OF
HARRIS
CORNER
DETECTOR
IN
SPACE
IMAGE
COORDINATES
LAPLACIAN
IN
SCALE
SCALE
HARRIS
X
DESCRIBING
IMAGES
WITH
FEATURES
FEATURE
DESCRIPTION
RAW
PATCHES
AS
LOCAL
DESCRIPTORS
THE
SIMPLEST
WAY
TO
DESCRIBE
THE
NEIGHBORHOOD
AROUND
AN
INTEREST
POINT
IS
TO
WRITE
DOWN
THE
LIST
OF
INTENSITIES
TO
FORM
A
FEATURE
VECTOR
BUT
THIS
IS
VERY
SENSITIVE
TO
EVEN
SMALL
SHIFTS
ROTATIONS
GEOMETRIC
TRANSFORMATIONS
E
G
SCALE
TRANSLATION
ROTATION
PHOTOMETRIC
TRANSFORMATIONS
SIFT
DESCRIPTOR
LOWE
USE
HISTOGRAMS
TO
BIN
PIXELS
WITHIN
SUB
PATCHES
ACCORDING
TO
THEIR
ORIENTATION
MAKING
THE
DESCRIPTOR
ROTATION
INVARIANT
ROTATE
THE
PATCH
ACCORDING
TO
ITS
DOMINANT
GRADIENT
ORIENTATION
THIS
PUTS
THE
PATCHES
INTO
A
CANONICAL
ORIENTATION
HISTOGRAMS
OF
ORIENTED
GRADIENTS
HOG
BIN
GRADIENTS
FROM
PIXEL
NEIGHBORHOODS
INTO
ORIENTATIONS
DALAL
TRIGGS
CVPR
FILTER
BANKS
D
D
D
D
D
EJ
I
L
L
L
J
LJ
J
KRISTEN
GRAUMAN
IMAGE
FROM
250
250
CAN
YOU
MATCH
THE
TEXTURE
TO
THE
RESPONSE
A
B
C
DEREK
HOIEM
MEAN
RESPONSES
REPRESENTING
TEXTURE
BY
MEAN
RESPONSE
FILTERS
DEREK
HOIEM
MEAN
RESPONSES
WE
CAN
FORM
A
FEATURE
VECTOR
FROM
THE
LIST
OF
RESPONSES
AT
EACH
PIXEL
SHAPE
CONTEXT
BELONGIE
MALIK
AND
PUZICHA
PAMI
REPRESENTATION
OF
THE
LOCAL
SHAPE
AROUND
A
FEATURE
LOCATION
AS
HISTOGRAM
OF
EDGE
POINTS
IN
AN
IMAGE
RELATIVE
TO
THAT
LOCATION
COMPUTED
BY
COUNTING
EDGE
POINTS
IN
LOG
POLAR
SPACE
COLOR
HISTOGRAMS
REPRESENTATION
OF
THE
DISTRIBUTION
OF
COLORS
IN
AN
IMAGE
DERIVED
BY
COUNTING
THE
NUMBER
OF
PIXELS
OF
EACH
OF
GIVEN
SET
OF
COLOR
RANGES
IN
A
TYPICALLY
COLOR
SPACE
RGB
HSV
ETC
GIST
OLIVA
AND
TORRALBA
IJCV
CAPTURES
THE
GLOBAL
ENERGY
OF
THE
SCENE
COMPUTES
EDGE
ORIENTATION
RESPONSES
FOR
MULTIPLE
ORIENTATIONS
AND
SCALES
DESCRIBING
IMAGES
WITH
FEATURES
FEATURE
MATCHING
CORRESPONDENCE
MATCHING
POINTS
PATCHES
EDGES
OR
REGIONS
ACROSS
IMAGES
ALIGNMENT
FIND
THE
PARAMETERS
OF
THE
TRANSFORMATION
THAT
BEST
ALIGN
MATCHED
POINTS
FITTING
FIND
THE
PARAMETERS
OF
A
MODEL
THAT
BEST
FIT
THE
DATA
P
V
C
HOUGH
MACHINE
ANALYSIS
OF
BUBBLE
CHAMBER
PICTURES
PROC
INT
CONF
HIGH
ENERGY
ACCELERATORS
AND
INSTRUMENTATION
GIVEN
A
SET
OF
POINTS
FIND
THE
CURVE
OR
LINE
THAT
EXPLAINS
THE
DATA
POINTS
BEST
Y
M
X
Y
M
X
B
B
HOUGH
SPACE
M
Y
B
X
HOUGH
TRANSFORM
Y
M
X
B
Y
M
X
B
FISCHLER
BOLLES
IN
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
RANSAC
LINE
FITTING
EXAMPLE
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
LINE
FITTING
EXAMPLE
N
I
ALGORITHM
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
REPEAT
UNTIL
THE
BEST
MODEL
IS
FOUND
WITH
HIGH
CONFIDENCE
ALGORITHM
N
I
SAMPLE
RANDOMLY
THE
NUMBER
OF
POINTS
REQUIRED
TO
FIT
THE
MODEL
SOLVE
FOR
MODEL
PARAMETERS
USING
SAMPLES
SCORE
BY
THE
FRACTION
OF
INLIERS
WITHIN
A
PRESET
THRESHOLD
OF
THE
MODEL
GIVEN
MATCHED
POINTS
IN
A
AND
B
ESTIMATE
THE
TRANSLATION
OF
THE
OBJECT
X
B
X
A
TX
I
I
I
I
T
Y
TX
TY
X
B
X
A
TX
I
I
LEAST
SQUARES
SOLUTION
WRITE
DOWN
OBJECTIVE
FUNCTION
I
I
T
Y
X
B
X
A
WRITE
IN
FORM
AX
B
SOLVE
USING
PSEUDO
INVERSE
OR
EIGENVALUE
TX
Y
B
Y
A
DECOMPOSITION
T
Y
X
B
X
A
N
N
DEREK
HOIEM
Y
B
Y
A
N
N
TX
TY
PROBLEM
OUTLIERS
MULTIPLE
OBJECTS
AND
OR
MANY
TO
ONE
MATCHES
HOUGH
TRANSFORM
SOLUTION
X
B
X
A
TX
INITIALIZE
A
GRID
OF
PARAMETER
VALUES
I
I
EACH
MATCHED
PAIR
CASTS
A
VOTE
FOR
CONSISTENT
VALUES
FIND
THE
PARAMETERS
WITH
THE
MOST
VOTES
SOLVE
USING
LEAST
SQUARES
WITH
INLIERS
I
I
T
Y
TX
TY
RANSAC
SOLUTION
PROBLEM
OUTLIERS
X
B
X
A
TX
SAMPLE
A
SET
OF
MATCHING
POINTS
PAIR
I
I
SOLVE
FOR
TRANSFORMATION
PARAMETERS
SCORE
PARAMETERS
WITH
NUMBER
OF
INLIERS
REPEAT
STEPS
N
TIMES
I
I
T
Y
LOCAL
FEATURES
MAIN
COMPONENTS
DETECTION
IDENTIFY
THE
INTEREST
POINTS
DESCRIPTION
EXTRACT
VECTOR
FEATURE
DESCRIPTOR
SURROUNDING
EACH
INTEREST
POINT
X
X
X
MATCHING
DETERMINE
CORRESPONDENCE
BETWEEN
DESCRIPTORS
IN
TWO
VIEWS
X
X
X
NEXT
TIME
CLASSIFICATION
AND
DETECTION
ADRIANA
RESEARCH
FOUNDATIONS
AND
TRENDSQR
IN
COMPUTER
GRAPHICS
AND
VISION
VOL
NO
QC
T
TUYTELAARS
AND
K
MIKOLAJCZYK
DOI
LOCAL
INVARIANT
FEATURE
DETECTORS
A
SURVEY
TINNE
AND
KRYSTIAN
DEPARTMENT
OF
ELECTRICAL
ENGINEERING
KATHOLIEKE
UNIVERSITEIT
LEUVEN
KASTEELPARK
ARENBERG
B
LEUVEN
BELGIUM
SCHOOL
OF
ELECTRONICS
AND
PHYSICAL
SCIENCES
UNIVERSITY
OF
SURREY
GUILDFORD
SURREY
UK
ABSTRACT
IN
THIS
SURVEY
WE
GIVE
AN
OVERVIEW
OF
INVARIANT
INTEREST
POINT
DETECTORS
HOW
THEY
EVOLVED
OVER
TIME
HOW
THEY
WORK
AND
WHAT
THEIR
RESPECTIVE
STRENGTHS
AND
WEAKNESSES
ARE
WE
BEGIN
WITH
DEFINING
THE
PROPERTIES
OF
THE
IDEAL
LOCAL
FEATURE
DETECTOR
THIS
IS
FOLLOWED
BY
AN
OVERVIEW
OF
THE
LITERATURE
OVER
THE
PAST
FOUR
DECADES
ORGANIZED
IN
DIFFERENT
CATEGORIES
OF
FEATURE
EXTRACTION
METHODS
WE
THEN
PROVIDE
A
MORE
DETAILED
ANALYSIS
OF
A
SELECTION
OF
METHODS
WHICH
HAD
A
PARTICULARLY
SIGNIFICANT
IMPACT
ON
THE
RESEARCH
FIELD
WE
CONCLUDE
WITH
A
SUMMARY
AND
PROMISING
FUTURE
RESEARCH
DIRECTIONS
INTRODUCTION
IN
THIS
SECTION
WE
DISCUSS
THE
VERY
NATURE
OF
LOCAL
INVARIANT
FEA
TURES
WHAT
DO
WE
MEAN
WITH
THIS
TERM
WHAT
IS
THE
ADVANTAGE
OF
USING
LOCAL
FEATURES
WHAT
CAN
WE
DO
WITH
THEM
WHAT
WOULD
THE
IDEAL
LOCAL
FEATURE
LOOK
LIKE
THESE
ARE
SOME
OF
THE
QUESTIONS
WE
ATTEMPT
TO
ANSWER
WHAT
ARE
LOCAL
FEATURES
A
LOCAL
FEATURE
IS
AN
IMAGE
PATTERN
WHICH
DIFFERS
FROM
ITS
IMMEDIATE
NEIGHBORHOOD
IT
IS
USUALLY
ASSOCIATED
WITH
A
CHANGE
OF
AN
IMAGE
PROP
ERTY
OR
SEVERAL
PROPERTIES
SIMULTANEOUSLY
ALTHOUGH
IT
IS
NOT
NECESSARILY
LOCALIZED
EXACTLY
ON
THIS
CHANGE
THE
IMAGE
PROPERTIES
COMMONLY
CON
SIDERED
ARE
INTENSITY
COLOR
AND
TEXTURE
FIGURE
SHOWS
SOME
EXAM
PLES
OF
LOCAL
FEATURES
IN
A
CONTOUR
IMAGE
LEFT
AS
WELL
AS
IN
A
GRAYVALUE
IMAGE
RIGHT
LOCAL
FEATURES
CAN
BE
POINTS
BUT
ALSO
EDGELS
OR
SMALL
IMAGE
PATCHES
TYPICALLY
SOME
MEASUREMENTS
ARE
TAKEN
FROM
A
REGION
CENTERED
ON
A
LOCAL
FEATURE
AND
CONVERTED
INTO
DESCRIPTORS
THE
DESCRIP
TORS
CAN
THEN
BE
USED
FOR
VARIOUS
APPLICATIONS
WHY
LOCAL
FEATURES
FIG
IMPORTANCE
OF
CORNERS
AND
JUNCTIONS
IN
VISUAL
RECOGNITION
AND
AN
IMAGE
EXAMPLE
WITH
INTEREST
POINTS
PROVIDED
BY
A
CORNER
DETECTOR
CF
SECTION
WHY
LOCAL
FEATURES
AS
DISCUSSED
SHORTLY
IN
THE
PREFACE
LOCAL
INVARIANT
FEATURES
ARE
A
POWERFUL
TOOL
THAT
HAS
BEEN
APPLIED
SUCCESSFULLY
IN
A
WIDE
RANGE
OF
SYSTEMS
AND
APPLICATIONS
IN
THE
FOLLOWING
WE
DISTINGUISH
THREE
BROAD
CATEGORIES
OF
FEATURE
DETECTORS
BASED
ON
THEIR
POSSIBLE
USAGE
IT
IS
NOT
EXHAUSTIVE
OR
THE
ONLY
WAY
OF
CATEGORIZING
THE
DETECTORS
BUT
IT
EMPHASIZES
DIFFERENT
PROPER
TIES
REQUIRED
BY
THE
USAGE
SCENARIOS
FIRST
ONE
MIGHT
BE
INTERESTED
IN
A
SPECIFIC
TYPE
OF
LOCAL
FEATURES
AS
THEY
MAY
HAVE
A
SPECIFIC
SEMAN
TIC
INTERPRETATION
IN
THE
LIMITED
CONTEXT
OF
A
CERTAIN
APPLICATION
FOR
INSTANCE
EDGES
DETECTED
IN
AERIAL
IMAGES
OFTEN
CORRESPOND
TO
ROADS
BLOB
DETECTION
CAN
BE
USED
TO
IDENTIFY
IMPURITIES
IN
SOME
INSPECTION
TASK
ETC
THESE
WERE
THE
FIRST
APPLICATIONS
FOR
WHICH
LOCAL
FEATURE
DETEC
TORS
HAVE
BEEN
PROPOSED
SECOND
ONE
MIGHT
BE
INTERESTED
IN
LOCAL
FEA
TURES
SINCE
THEY
PROVIDE
A
LIMITED
SET
OF
WELL
LOCALIZED
AND
INDIVIDUALLY
IDENTIFIABLE
ANCHOR
POINTS
WHAT
THE
FEATURES
ACTUALLY
REPRESENT
IS
NOT
REALLY
RELEVANT
AS
LONG
AS
THEIR
LOCATION
CAN
BE
DETERMINED
ACCURATELY
AND
IN
A
STABLE
MANNER
OVER
TIME
THIS
IS
FOR
INSTANCE
THE
SITUATION
IN
MOST
MATCHING
OR
TRACKING
APPLICATIONS
AND
ESPECIALLY
FOR
CAMERA
CAL
IBRATION
OR
RECONSTRUCTION
OTHER
APPLICATION
DOMAINS
INCLUDE
POSE
ESTIMATION
IMAGE
ALIGNMENT
OR
MOSAICING
A
TYPICAL
EXAMPLE
HERE
ARE
THE
FEATURES
USED
IN
THE
KLT
TRACKER
FINALLY
A
SET
OF
LOCAL
FEATURES
CAN
BE
USED
AS
A
ROBUST
IMAGE
REPRESENTATION
THAT
ALLOWS
TO
RECOGNIZE
OBJECTS
OR
SCENES
WITHOUT
THE
NEED
FOR
SEGMENTATION
HERE
AGAIN
IT
DOES
NOT
REALLY
MATTER
WHAT
THE
FEATURES
ACTUALLY
REPRESENT
THEY
DO
NOT
EVEN
HAVE
TO
BE
LOCALIZED
PRECISELY
SINCE
THE
GOAL
IS
NOT
TO
MATCH
THEM
ON
AN
INDIVIDUAL
BASIS
BUT
RATHER
TO
ANALYZE
THEIR
STATISTICS
THIS
WAY
OF
EXPLOITING
LOCAL
FEATURES
WAS
FIRST
REPORTED
IN
THE
SEMINAL
WORK
OF
AND
AND
SOON
BECAME
VERY
POPULAR
ESPECIALLY
IN
THE
CONTEXT
OF
OBJECT
RECOGNITION
BOTH
FOR
SPECIFIC
OBJECTS
AS
WELL
AS
FOR
CATEGORY
LEVEL
RECOGNITION
OTHER
APPLICATION
DOMAINS
INCLUDE
SCENE
CLASSIFICATION
TEXTURE
ANALYSIS
IMAGE
RETRIEVAL
AND
VIDEO
MINING
CLEARLY
EACH
OF
THE
ABOVE
THREE
CATEGORIES
IMPOSES
ITS
OWN
CON
STRAINTS
AND
A
GOOD
FEATURE
FOR
ONE
APPLICATION
MAY
BE
USELESS
IN
THE
CONTEXT
OF
A
DIFFERENT
PROBLEM
THESE
CATEGORIES
CAN
BE
CONSIDERED
WHEN
SEARCHING
FOR
SUITABLE
FEATURE
DETECTORS
FOR
AN
APPLICATION
AT
HAND
IN
THIS
SURVEY
WE
MAINLY
FOCUS
ON
THE
SECOND
AND
ESPECIALLY
THE
THIRD
APPLICATION
SCENARIO
FINALLY
IT
IS
WORTH
NOTING
THAT
THE
IMPORTANCE
OF
LOCAL
FEATURES
HAS
ALSO
BEEN
DEMONSTRATED
IN
THE
CONTEXT
OF
OBJECT
RECOGNITION
BY
THE
HUMAN
VISUAL
SYSTEM
MORE
PRECISELY
EXPERIMENTS
HAVE
SHOWN
THAT
REMOVING
THE
CORNERS
FROM
IMAGES
IMPEDES
HUMAN
RECOGNITION
WHILE
REMOVING
MOST
OF
THE
STRAIGHT
EDGE
INFORMATION
DOES
NOT
THIS
IS
ILLUSTRATED
IN
FIGURE
A
FEW
NOTES
ON
TERMINOLOGY
BEFORE
WE
DISCUSS
FEATURE
DETECTORS
IN
MORE
DETAIL
LET
US
EXPLAIN
SOME
TERMINOLOGY
COMMONLY
USED
IN
THE
LITERATURE
DETECTOR
OR
EXTRACTOR
TRADITIONALLY
THE
TERM
DETECTOR
HAS
BEEN
USED
TO
REFER
TO
THE
TOOL
THAT
EXTRACTS
THE
FEATURES
FROM
THE
IMAGE
E
G
A
CORNER
BLOB
OR
EDGE
DETEC
TOR
HOWEVER
THIS
ONLY
MAKES
SENSE
IF
IT
IS
A
PRIORI
CLEAR
WHAT
THE
CORNERS
BLOBS
OR
EDGES
IN
THE
IMAGE
ARE
SO
ONE
CAN
SPEAK
OF
FALSE
DETECTIONS
OR
MISSED
DETECTIONS
THIS
ONLY
HOLDS
IN
THE
FIRST
USAGE
A
FEW
NOTES
ON
TERMINOLOGY
SCENARIO
MENTIONED
EARLIER
NOT
FOR
THE
LAST
TWO
WHERE
EXTRACTOR
WOULD
PROBABLY
BE
SEMANTICALLY
MORE
CORRECT
STILL
THE
TERM
DETECTOR
IS
WIDELY
USED
WE
THEREFORE
ALSO
STICK
TO
THIS
TERMINOLOGY
INVARIANT
OR
COVARIANT
A
SIMILAR
DISCUSSION
HOLDS
FOR
THE
USE
OF
INVARIANT
OR
COVARIANT
A
FUNCTION
IS
INVARIANT
UNDER
A
CERTAIN
FAMILY
OF
TRANSFORMATIONS
IF
ITS
VALUE
DOES
NOT
CHANGE
WHEN
A
TRANSFORMATION
FROM
THIS
FAMILY
IS
APPLIED
TO
ITS
ARGUMENT
A
FUNCTION
IS
COVARIANT
WHEN
IT
COMMUTES
WITH
THE
TRANSFORMATION
I
E
APPLYING
THE
TRANSFORMATION
TO
THE
ARGU
MENT
OF
THE
FUNCTION
HAS
THE
SAME
EFFECT
AS
APPLYING
THE
TRANSFORMATION
TO
THE
OUTPUT
OF
THE
FUNCTION
A
FEW
EXAMPLES
MAY
HELP
TO
EXPLAIN
THE
DIFFERENCE
THE
AREA
OF
A
SURFACE
IS
INVARIANT
UNDER
ROTATIONS
SINCE
ROTATING
A
SURFACE
DOES
NOT
MAKE
IT
ANY
SMALLER
OR
BIGGER
BUT
THE
ORIENTATION
OF
THE
MAJOR
AXIS
OF
INERTIA
OF
THE
SURFACE
IS
COVARIANT
UNDER
THE
SAME
FAMILY
OF
TRANSFORMATIONS
SINCE
ROTATING
A
SUR
FACE
WILL
AFFECT
THE
ORIENTATION
OF
ITS
MAJOR
AXIS
IN
EXACTLY
THE
SAME
WAY
BASED
ON
THESE
DEFINITIONS
IT
IS
CLEAR
THAT
THE
SO
CALLED
LOCAL
SCALE
AND
OR
AFFINE
INVARIANT
FEATURES
ARE
IN
FACT
ONLY
COVARIANT
THE
DESCRIP
TORS
DERIVED
FROM
THEM
ON
THE
OTHER
HAND
ARE
USUALLY
INVARIANT
DUE
TO
A
NORMALIZATION
STEP
SINCE
THE
TERM
LOCAL
INVARIANT
FEATURE
IS
SO
WIDELY
USED
WE
NEVERTHELESS
USE
INVARIANT
IN
THIS
SURVEY
ROTATION
INVARIANT
OR
ISOTROPIC
A
FUNCTION
IS
ISOTROPIC
AT
A
PARTICULAR
POINT
IF
IT
BEHAVES
THE
SAME
IN
ALL
DIRECTIONS
THIS
IS
A
TERM
THAT
APPLIES
TO
E
G
TEXTURES
AND
SHOULD
NOT
BE
CONFUSED
WITH
ROTATIONAL
INVARIANCE
INTEREST
POINT
REGION
OR
LOCAL
FEATURE
IN
A
WAY
THE
IDEAL
LOCAL
FEATURE
WOULD
BE
A
POINT
AS
DEFINED
IN
GEOMETRY
HAVING
A
LOCATION
IN
SPACE
BUT
NO
SPATIAL
EXTENT
IN
PRACTICE
HOWEVER
IMAGES
ARE
DISCRETE
WITH
THE
SMALLEST
SPATIAL
UNIT
BEING
A
PIXEL
AND
DISCRETIZATION
EFFECTS
PLAYING
AN
IMPORTANT
ROLE
TO
LOCALIZE
FEATURES
IN
IMAGES
A
LOCAL
NEIGHBORHOOD
OF
PIXELS
NEEDS
TO
BE
ANALYZED
GIVING
ALL
LOCAL
FEATURES
SOME
IMPLICIT
SPATIAL
EXTENT
FOR
SOME
APPLICATIONS
E
G
CAMERA
CALIBRATION
OR
RECONSTRUCTION
THIS
SPATIAL
EXTENT
IS
COMPLETELY
IGNORED
IN
FURTHER
PROCESSING
AND
ONLY
THE
LOCATION
DERIVED
FROM
THE
FEATURE
EXTRACTION
PROCESS
IS
USED
WITH
THE
LOCATION
SOMETIMES
DETERMINED
UP
TO
SUB
PIXEL
ACCURACY
IN
THOSE
CASES
ONE
TYPICALLY
USES
THE
TERM
INTEREST
POINT
HOWEVER
IN
MOST
APPLICATIONS
THOSE
FEATURES
ALSO
NEED
TO
BE
DESCRIBED
SUCH
THAT
THEY
CAN
BE
IDENTIFIED
AND
MATCHED
AND
THIS
AGAIN
CALLS
FOR
A
LOCAL
NEIGHBORHOOD
OF
PIXELS
OFTEN
THIS
NEIGHBORHOOD
IS
TAKEN
EQUAL
TO
THE
NEIGHBORHOOD
USED
TO
LOCALIZE
THE
FEATURE
BUT
THIS
NEED
NOT
BE
THE
CASE
IN
THIS
CONTEXT
ONE
TYPICALLY
USES
THE
TERM
REGION
INSTEAD
OF
INTEREST
POINT
HOWEVER
BEWARE
WHEN
A
LOCAL
NEIGHBORHOOD
OF
PIXELS
IS
USED
TO
DESCRIBE
AN
INTEREST
POINT
THE
FEATURE
EXTRACTION
PROCESS
HAS
TO
DETERMINE
NOT
ONLY
THE
LOCATION
OF
THE
INTEREST
POINT
BUT
ALSO
THE
SIZE
AND
POSSIBLY
THE
SHAPE
OF
THIS
LOCAL
NEIGHBORHOOD
ESPECIALLY
IN
CASE
OF
GEOMETRIC
DEFORMATIONS
THIS
SIGNIFICANTLY
COMPLI
CATES
THE
PROCESS
AS
THE
SIZE
AND
SHAPE
HAVE
TO
BE
DETERMINED
IN
AN
INVARIANT
COVARIANT
WAY
IN
THIS
SURVEY
WE
PREFER
THE
USE
OF
THE
TERM
LOCAL
FEATURE
WHICH
CAN
BE
EITHER
POINTS
REGIONS
OR
EVEN
EDGE
SEGMENTS
PROPERTIES
OF
THE
IDEAL
LOCAL
FEATURE
LOCAL
FEATURES
TYPICALLY
HAVE
A
SPATIAL
EXTENT
I
E
THE
LOCAL
NEIGH
BORHOOD
OF
PIXELS
MENTIONED
ABOVE
IN
CONTRAST
TO
CLASSICAL
SEGMEN
TATION
THIS
CAN
BE
ANY
SUBSET
OF
AN
IMAGE
THE
REGION
BOUNDARIES
DO
NOT
HAVE
TO
CORRESPOND
TO
CHANGES
IN
IMAGE
APPEARANCE
SUCH
AS
COLOR
OR
TEXTURE
ALSO
MULTIPLE
REGIONS
MAY
OVERLAP
AND
UNINTER
ESTING
PARTS
OF
THE
IMAGE
SUCH
AS
HOMOGENEOUS
AREAS
CAN
REMAIN
UNCOVERED
IDEALLY
ONE
WOULD
LIKE
SUCH
LOCAL
FEATURES
TO
CORRESPOND
TO
SEMAN
TICALLY
MEANINGFUL
OBJECT
PARTS
IN
PRACTICE
HOWEVER
THIS
IS
UNFEASIBLE
AS
THIS
WOULD
REQUIRE
HIGH
LEVEL
INTERPRETATION
OF
THE
SCENE
CONTENT
WHICH
IS
NOT
AVAILABLE
AT
THIS
EARLY
STAGE
INSTEAD
DETECTORS
SELECT
LOCAL
FEATURES
DIRECTLY
BASED
ON
THE
UNDERLYING
INTENSITY
PATTERNS
PROPERTIES
OF
THE
IDEAL
LOCAL
FEATURE
GOOD
FEATURES
SHOULD
HAVE
THE
FOLLOWING
PROPERTIES
REPEATABILITY
GIVEN
TWO
IMAGES
OF
THE
SAME
OBJECT
OR
SCENE
TAKEN
UNDER
DIFFERENT
VIEWING
CONDITIONS
A
HIGH
PERCENTAGE
OF
THE
FEATURES
DETECTED
ON
THE
SCENE
PART
VISIBLE
IN
BOTH
IMAGES
SHOULD
BE
FOUND
IN
BOTH
IMAGES
DISTINCTIVENESS
INFORMATIVENESS
THE
INTENSITY
PATTERNS
UNDERLYING
THE
DETECTED
FEATURES
SHOULD
SHOW
A
LOT
OF
VARIA
TION
SUCH
THAT
FEATURES
CAN
BE
DISTINGUISHED
AND
MATCHED
LOCALITY
THE
FEATURES
SHOULD
BE
LOCAL
SO
AS
TO
REDUCE
THE
PROBABILITY
OF
OCCLUSION
AND
TO
ALLOW
SIMPLE
MODEL
APPROX
IMATIONS
OF
THE
GEOMETRIC
AND
PHOTOMETRIC
DEFORMATIONS
BETWEEN
TWO
IMAGES
TAKEN
UNDER
DIFFERENT
VIEWING
CONDITIONS
E
G
BASED
ON
A
LOCAL
PLANARITY
ASSUMPTION
QUANTITY
THE
NUMBER
OF
DETECTED
FEATURES
SHOULD
BE
SUFFI
CIENTLY
LARGE
SUCH
THAT
A
REASONABLE
NUMBER
OF
FEATURES
ARE
DETECTED
EVEN
ON
SMALL
OBJECTS
HOWEVER
THE
OPTIMAL
NUMBER
OF
FEATURES
DEPENDS
ON
THE
APPLICATION
IDEALLY
THE
NUMBER
OF
DETECTED
FEATURES
SHOULD
BE
ADAPTABLE
OVER
A
LARGE
RANGE
BY
A
SIMPLE
AND
INTUITIVE
THRESHOLD
THE
DENSITY
OF
FEATURES
SHOULD
REFLECT
THE
INFORMATION
CONTENT
OF
THE
IMAGE
TO
PROVIDE
A
COMPACT
IMAGE
REPRESENTATION
ACCURACY
THE
DETECTED
FEATURES
SHOULD
BE
ACCURATELY
LOCAL
IZED
BOTH
IN
IMAGE
LOCATION
AS
WITH
RESPECT
TO
SCALE
AND
POSSIBLY
SHAPE
EFFICIENCY
PREFERABLY
THE
DETECTION
OF
FEATURES
IN
A
NEW
IMAGE
SHOULD
ALLOW
FOR
TIME
CRITICAL
APPLICATIONS
REPEATABILITY
ARGUABLY
THE
MOST
IMPORTANT
PROPERTY
OF
ALL
CAN
BE
ACHIEVED
IN
TWO
DIFFERENT
WAYS
EITHER
BY
INVARIANCE
OR
BY
ROBUSTNESS
INVARIANCE
WHEN
LARGE
DEFORMATIONS
ARE
TO
BE
EXPECTED
THE
PREFERRED
APPROACH
IS
TO
MODEL
THESE
MATHEMATICALLY
IF
POSSIBLE
AND
THEN
DEVELOP
METHODS
FOR
FEATURE
DETECTION
THAT
ARE
UNAFFECTED
BY
THESE
MATHEMATICAL
TRANSFORMATIONS
ROBUSTNESS
IN
CASE
OF
RELATIVELY
SMALL
DEFORMATIONS
IT
OFTEN
SUFFICES
TO
MAKE
FEATURE
DETECTION
METHODS
LESS
SENSITIVE
TO
SUCH
DEFORMATIONS
I
E
THE
ACCURACY
OF
THE
DETECTION
MAY
DECREASE
BUT
NOT
DRASTICALLY
SO
TYPICAL
DEFORMATIONS
THAT
ARE
TACKLED
USING
ROBUSTNESS
ARE
IMAGE
NOISE
DISCRETIZATION
EFFECTS
COMPRESSION
ARTIFACTS
BLUR
ETC
ALSO
GEOMETRIC
AND
PHOTOMETRIC
DEVIATIONS
FROM
THE
MATHEMATICAL
MODEL
USED
TO
OBTAIN
INVARIANCE
ARE
OFTEN
OVERCOME
BY
INCLUDING
MORE
ROBUSTNESS
DISCUSSION
CLEARLY
THE
IMPORTANCE
OF
THESE
DIFFERENT
PROPERTIES
DEPENDS
ON
THE
ACTUAL
APPLICATION
AND
SETTINGS
AND
COMPROMISES
NEED
TO
BE
MADE
REPEATABILITY
IS
REQUIRED
IN
ALL
APPLICATION
SCENARIOS
AND
IT
DIRECTLY
DEPENDS
ON
THE
OTHER
PROPERTIES
LIKE
INVARIANCE
ROBUSTNESS
QUANTITY
ETC
DEPENDING
ON
THE
APPLICATION
INCREASING
OR
DECREASING
THEM
MAY
RESULT
IN
HIGHER
REPEATABILITY
DISTINCTIVENESS
AND
LOCALITY
ARE
COMPETING
PROPERTIES
AND
CANNOT
BE
FULFILLED
SIMULTANEOUSLY
THE
MORE
LOCAL
A
FEATURE
THE
LESS
INFORMATION
IS
AVAILABLE
IN
THE
UNDERLYING
INTENSITY
PATTERN
AND
THE
HARDER
IT
BECOMES
TO
MATCH
IT
CORRECTLY
ESPECIALLY
IN
DATABASE
APPLICATIONS
WHERE
THERE
ARE
MANY
CANDIDATE
FEATURES
TO
MATCH
TO
ON
THE
OTHER
HAND
IN
CASE
OF
PLANAR
OBJECTS
AND
OR
PURELY
ROTATING
CAMERAS
E
G
IN
IMAGE
MOSAICING
APPLICATIONS
IMAGES
ARE
RELATED
BY
A
GLOBAL
HOMOGRAPHY
AND
THERE
ARE
NO
PROBLEMS
WITH
OCCLUSIONS
OR
DEPTH
DISCONTINUITIES
UNDER
THESE
CON
DITIONS
THE
SIZE
OF
THE
LOCAL
FEATURES
CAN
BE
INCREASED
WITHOUT
PROBLEMS
RESULTING
IN
A
HIGHER
DISTINCTIVENESS
SIMILARLY
AN
INCREASED
LEVEL
OF
INVARIANCE
TYPICALLY
LEADS
TO
A
REDUCED
DISTINCTIVENESS
AS
SOME
OF
THE
IMAGE
MEASUREMENTS
ARE
USED
TO
LIFT
THE
DEGREES
OF
FREEDOM
OF
THE
TRANSFORMATION
A
SIMILAR
RULE
HOLDS
FOR
ROBUSTNESS
VERSUS
DISTINCTIVENESS
AS
TYPICALLY
SOME
INFORMATION
IS
DISREGARDED
CONSIDERED
AS
NOISE
IN
ORDER
TO
ACHIEVE
ROBUSTNESS
AS
A
RESULT
IT
IS
IMPORTANT
TO
HAVE
A
CLEAR
IDEA
ON
THE
REQUIRED
LEVEL
OF
INVARIANCE
OR
ROBUSTNESS
FOR
A
GIVEN
APPLICATION
IT
IS
HARD
TO
ACHIEVE
HIGH
INVARIANCE
AND
ROBUSTNESS
AT
THE
SAME
TIME
AND
INVARIANCE
WHICH
IS
NOT
ADAPTED
TO
THE
APPLICATION
MAY
HAVE
A
NEGATIVE
IMPACT
ON
THE
RESULTS
ACCURACY
IS
ESPECIALLY
IMPORTANT
IN
WIDE
BASELINE
MATCHING
REGIS
TRATION
AND
STRUCTURE
FROM
MOTION
APPLICATIONS
WHERE
PRECISE
CORRE
SPONDENCES
ARE
NEEDED
TO
E
G
ESTIMATE
THE
EPIPOLAR
GEOMETRY
OR
TO
CALIBRATE
THE
CAMERA
SETUP
QUANTITY
IS
PARTICULARLY
USEFUL
IN
SOME
CLASS
LEVEL
OBJECT
OR
SCENE
RECOGNITION
METHODS
WHERE
IT
IS
VITAL
TO
DENSELY
COVER
THE
OBJECT
OF
INTEREST
ON
THE
OTHER
HAND
A
HIGH
NUMBER
OF
FEATURES
HAS
IN
MOST
CASES
A
NEGATIVE
IMPACT
ON
THE
COMPUTATION
TIME
AND
IT
SHOULD
BE
KEPT
WITHIN
LIMITS
ALSO
ROBUSTNESS
IS
ESSENTIAL
FOR
OBJECT
CLASS
RECOGNITION
AS
IT
IS
IMPOSSIBLE
TO
MODEL
THE
INTRA
CLASS
VARIATIONS
MATHEMATICALLY
SO
FULL
INVARIANCE
IS
IMPOSSIBLE
FOR
THESE
APPLICATIONS
AN
ACCURATE
LOCAL
IZATION
IS
LESS
IMPORTANT
THE
EFFECT
OF
INACCURATE
LOCALIZATION
OF
A
FEA
TURE
DETECTOR
CAN
BE
COUNTERED
UP
TO
SOME
POINT
BY
HAVING
AN
EXTRA
ROBUST
DESCRIPTOR
WHICH
YIELDS
A
FEATURE
VECTOR
THAT
IS
NOT
AFFECTED
BY
SMALL
LOCALIZATION
ERRORS
GLOBAL
VERSUS
LOCAL
FEATURES
LOCAL
INVARIANT
FEATURES
NOT
ONLY
ALLOW
TO
FIND
CORRESPONDENCES
IN
SPITE
OF
LARGE
CHANGES
IN
VIEWING
CONDITIONS
OCCLUSIONS
AND
IMAGE
CLUTTER
WIDE
BASELINE
MATCHING
BUT
ALSO
YIELD
AN
INTERESTING
DESCRIPTION
OF
THE
IMAGE
CONTENT
FOR
IMAGE
RETRIEVAL
AND
OBJECT
OR
SCENE
RECOGNITION
TASKS
BOTH
FOR
SPECIFIC
OBJECTS
AS
WELL
AS
CATEGORIES
TO
PUT
THIS
INTO
CONTEXT
WE
BRIEFLY
SUMMARIZE
SOME
ALTERNATIVE
STRATEGIES
TO
COMPUTE
IMAGE
REPRESENTATIONS
INCLUDING
GLOBAL
FEATURES
IMAGE
SEGMENTS
AND
EXHAUSTIVE
AND
RANDOM
SAMPLING
OF
FEATURES
GLOBAL
FEATURES
IN
THE
FIELD
OF
IMAGE
RETRIEVAL
MANY
GLOBAL
FEATURES
HAVE
BEEN
PROPOSED
TO
DESCRIBE
THE
IMAGE
CONTENT
WITH
COLOR
HISTOGRAMS
AND
VARIATIONS
THEREOF
AS
A
TYPICAL
EXAMPLE
THIS
APPROACH
WORKS
SURPRISINGLY
WELL
AT
LEAST
FOR
IMAGES
WITH
DISTINCTIVE
COLORS
AS
LONG
AS
IT
IS
THE
OVERALL
COMPOSITION
OF
THE
IMAGE
AS
A
WHOLE
THAT
THE
USER
IS
INTERESTED
IN
RATHER
THAN
THE
FOREGROUND
OBJECT
INDEED
GLOBAL
FEATURES
CANNOT
DISTINGUISH
FOREGROUND
FROM
BACKGROUND
AND
MIX
INFORMATION
FROM
BOTH
PARTS
TOGETHER
GLOBAL
FEATURES
HAVE
ALSO
BEEN
USED
FOR
OBJECT
RECOGNITION
RESULT
ING
IN
THE
FIRST
APPEARANCE
BASED
APPROACHES
TO
TACKLE
THIS
CHALLENGING
PROBLEM
TURK
AND
PENTLAND
AND
LATER
MURASE
AND
NAYAR
PROPOSED
TO
COMPUTE
A
PRINCIPAL
COMPONENT
ANALYSIS
OF
A
SET
OF
MODEL
IMAGES
AND
TO
USE
THE
PROJECTIONS
ONTO
THE
FIRST
FEW
PRINCIPAL
COMPONENTS
AS
DESCRIPTORS
COMPARED
TO
THE
PURELY
GEOMETRY
BASED
APPROACHES
TRIED
BEFORE
THE
RESULTS
OF
THE
NOVEL
APPEARANCE
BASED
APPROACH
WERE
STRIKING
A
WHOLE
NEW
RANGE
OF
NATURAL
OBJECTS
COULD
SUDDENLY
BE
RECOGNIZED
HOWEVER
BEING
BASED
ON
A
GLOBAL
DESCRIPTION
IMAGE
CLUTTER
AND
OCCLUSIONS
AGAIN
FORM
A
MAJOR
PROBLEM
LIMITING
THE
USEFULNESS
OF
THE
SYSTEM
TO
CASES
WITH
CLEAN
BACKGROUNDS
OR
WHERE
THE
OBJECT
CAN
BE
SEGMENTED
OUT
E
G
RELYING
ON
MOTION
INFORMATION
IMAGE
SEGMENTS
AN
APPROACH
TO
OVERCOME
THE
LIMITATIONS
OF
THE
GLOBAL
FEATURES
IS
TO
SEGMENT
THE
IMAGE
IN
A
LIMITED
NUMBER
OF
REGIONS
OR
SEGMENTS
WITH
EACH
SUCH
REGION
CORRESPONDING
TO
A
SINGLE
OBJECT
OR
PART
THEREOF
THE
BEST
KNOWN
EXAMPLE
OF
THIS
APPROACH
IS
THE
BLOBWORLD
SYSTEM
PRO
POSED
IN
WHICH
SEGMENTS
THE
IMAGE
BASED
ON
COLOR
AND
TEXTURE
THEN
SEARCHES
A
DATABASE
FOR
IMAGES
WITH
SIMILAR
IMAGE
BLOBS
AN
EXAMPLE
BASED
ON
TEXTURE
SEGMENTATION
IS
THE
WIDE
BASELINE
MATCHING
WORK
DESCRIBED
IN
HOWEVER
THIS
RAISES
A
CHICKEN
AND
EGG
PROBLEM
AS
IMAGE
SEGMEN
TATION
IS
A
VERY
CHALLENGING
TASK
IN
ITSELF
WHICH
IN
GENERAL
REQUIRES
A
HIGH
LEVEL
UNDERSTANDING
OF
THE
IMAGE
CONTENT
FOR
GENERIC
OBJECTS
COLOR
AND
TEXTURE
CUES
ARE
INSUFFICIENT
TO
OBTAIN
MEANINGFUL
SEGMENTATIONS
SAMPLED
FEATURES
A
WAY
TO
DEAL
WITH
THE
PROBLEMS
ENCOUNTERED
WITH
GLOBAL
FEATURES
OR
IMAGE
SEGMENTATIONS
IS
TO
EXHAUSTIVELY
SAMPLE
DIFFERENT
SUBPARTS
OF
THE
IMAGE
AT
EACH
LOCATION
AND
SCALE
FOR
EACH
SUCH
IMAGE
SUBPART
GLOBAL
FEATURES
CAN
THEN
BE
COMPUTED
THIS
APPROACH
IS
ALSO
REFERRED
TO
AS
A
SLIDING
WINDOW
BASED
APPROACH
IT
HAS
BEEN
ESPECIALLY
POPU
LAR
IN
THE
CONTEXT
OF
FACE
DETECTION
BUT
HAS
ALSO
BEEN
APPLIED
FOR
THE
RECOGNITION
OF
SPECIFIC
OBJECTS
OR
PARTICULAR
OBJECT
CLASSES
SUCH
AS
PEDES
TRIANS
OR
CARS
BY
FOCUSING
ON
SUBPARTS
OF
THE
IMAGE
THESE
METHODS
ARE
ABLE
TO
FIND
SIMILARITIES
BETWEEN
THE
QUERIES
AND
THE
MODELS
IN
SPITE
OF
CHANGING
BACKGROUNDS
AND
EVEN
IF
THE
OBJECT
COVERS
ONLY
A
SMALL
PERCENTAGE
OF
THE
TOTAL
IMAGE
AREA
ON
THE
DOWNSIDE
THEY
STILL
DO
NOT
MANAGE
TO
COPE
WITH
PARTIAL
OCCLUSIONS
AND
THE
ALLOWED
SHAPE
VARIABILITY
IS
SMALLER
THAN
WHAT
IS
FEASIBLE
WITH
A
LOCAL
FEATURES
BASED
APPROACH
HOWEVER
BY
FAR
THE
BIGGEST
DRAWBACK
IS
THE
INEFFICIENCY
OF
THIS
APPROACH
EACH
AND
EVERY
SUBPART
OF
THE
IMAGE
MUST
BE
ANALYZED
RESULTING
IN
THOUSANDS
OR
EVEN
MILLIONS
OF
FEATURES
PER
IMAGE
THIS
REQUIRES
EXTREMELY
EFFICIENT
METHODS
WHICH
SIGNIFICANTLY
LIMITS
THE
SCOPE
OF
POSSIBLE
APPLICATIONS
TO
OVERCOME
THE
COMPLEXITY
PROBLEMS
MORE
SPARSE
FIXED
GRID
SAM
PLING
OF
IMAGE
PATCHES
WAS
USED
E
G
IT
IS
HOWEVER
DIFFICULT
TO
ACHIEVE
INVARIANCE
TO
GEOMETRIC
DEFORMATIONS
FOR
SUCH
FEA
TURES
THE
APPROACH
CAN
TOLERATE
SOME
DEFORMATIONS
DUE
TO
DENSE
SAM
PLING
OVER
POSSIBLE
LOCATIONS
SCALES
POSES
ETC
BUT
THE
INDIVIDUAL
FEATURES
ARE
NOT
INVARIANT
AN
EXAMPLE
OF
SUCH
APPROACH
ARE
MULTI
SCALE
INTEREST
POINTS
AS
A
RESULT
THEY
CANNOT
BE
USED
WHEN
THE
GOAL
IS
TO
FIND
PRECISE
CORRESPONDENCES
BETWEEN
IMAGES
HOWEVER
FOR
SOME
APPLI
CATIONS
SUCH
AS
SCENE
CLASSIFICATION
OR
TEXTURE
RECOGNITION
THEY
MAY
WELL
BE
SUFFICIENT
IN
BETTER
RESULTS
ARE
REPORTED
WITH
A
FIXED
GRID
OF
PATCHES
THAN
WITH
PATCHES
CENTERED
ON
INTEREST
POINTS
IN
THE
CONTEXT
OF
SCENE
CLASSIFICATION
WORK
THIS
CAN
BE
EXPLAINED
BY
THE
DENSE
COVER
AGE
AS
WELL
AS
THE
FACT
THAT
HOMOGENEOUS
AREAS
E
G
SKY
ARE
ALSO
TAKEN
INTO
ACCOUNT
IN
THE
FIXED
GRID
APPROACH
WHICH
MAKES
THE
REPRESENTATION
MORE
COMPLETE
THIS
DENSE
COVERAGE
IS
ALSO
EXPLOITED
IN
WHERE
A
FIXED
GRID
OF
PATCHES
WAS
USED
ON
TOP
OF
A
SET
OF
LOCAL
INVARIANT
FEATURES
IN
THE
CONTEXT
OF
SPECIFIC
OBJECT
RECOGNITION
WHERE
THE
LATTER
SUPPLY
AN
INITIAL
SET
OF
CORRESPONDENCES
WHICH
THEN
GUIDE
THE
CONSTRUCTION
OF
CORRESPONDENCES
FOR
THE
FORMER
IN
A
SIMILAR
VEIN
RATHER
THAN
USING
A
FIXED
GRID
OF
PATCHES
A
RANDOM
SAMPLING
OF
IMAGE
PATCHES
CAN
ALSO
BE
USED
E
G
THIS
GIVES
A
LARGER
FLEXIBILITY
IN
THE
NUMBER
OF
PATCHES
THE
RANGE
OF
SCALES
OR
SHAPES
AND
THEIR
SPATIAL
DISTRIBUTION
GOOD
SCENE
RECOGNITION
RESULTS
ARE
SHOWN
IN
BASED
ON
RANDOM
IMAGE
PATCHES
AS
IN
THE
CASE
OF
FIXED
GRID
SAMPLING
THIS
CAN
BE
EXPLAINED
BY
THE
DENSE
COVERAGE
WHICH
IGNORES
THE
LOCALIZATION
PROPERTIES
OF
FEATURES
RANDOM
PATCHES
ARE
IN
FACT
A
SUBSET
OF
THE
DENSE
PATCHES
AND
ARE
USED
MOSTLY
TO
REDUCE
THE
COMPLEXITY
THEIR
REPEATABILITY
IS
POOR
HENCE
THEY
WORK
BETTER
AS
AN
ADDITION
TO
THE
REGULAR
FEATURES
RATHER
THAN
AS
A
STAND
ALONE
METHOD
FINALLY
TO
OVERCOME
THE
COMPLEXITY
PROBLEMS
WHILE
STILL
PROVIDING
A
LARGE
NUMBER
OF
FEATURES
WITH
BETTER
THAN
RANDOM
LOCALIZATION
PROPOSED
TO
SAMPLE
FEATURES
UNIFORMLY
FROM
EDGES
THIS
PROVED
USEFUL
FOR
DEALING
WITH
WIRY
OBJECTS
WELL
REPRESENTED
BY
EDGES
AND
CURVES
OVERVIEW
OF
THIS
SURVEY
THIS
SURVEY
ARTICLE
CONSISTS
OF
TWO
PARTS
FIRST
IN
SECTION
WE
REVIEW
LOCAL
INVARIANT
FEATURE
DETECTORS
IN
THE
LITERATURE
FROM
THE
EARLY
DAYS
IN
COMPUTER
VISION
UP
TO
THE
MOST
RECENT
EVOLUTIONS
NEXT
WE
DESCRIBE
A
FEW
SELECTED
REPRESENTATIVE
METHODS
IN
MORE
DETAIL
WE
HAVE
STRUCTURED
THE
METHODS
IN
A
RELATIVELY
INTUITIVE
MANNER
BASED
ON
THE
TYPE
OF
FEATURE
EXTRACTED
IN
THE
IMAGE
DOING
SO
WE
DISTINGUISH
BETWEEN
CORNER
DETECTORS
SECTION
BLOB
DETECTORS
SECTION
AND
REGION
DETECTORS
SECTION
ADDITIONALLY
WE
ADDED
A
SECTION
ON
VARIOUS
DETECTORS
THAT
HAVE
BEEN
DESIGNED
IN
A
COMPUTATIONALLY
EFFICIENT
MANNER
SECTION
WITH
THIS
STRUCTURE
WE
HOPE
THE
READER
CAN
EASILY
FIND
THE
TYPE
OF
DETECTOR
MOST
USEFUL
FOR
HIS
HER
APPLICATION
WE
CONCLUDE
THE
SURVEY
WITH
A
QUALITATIVE
COMPARISON
OF
THE
DIFFERENT
METHODS
AND
A
DISCUSSION
OF
FUTURE
WORK
SECTION
TO
THE
NOVICE
READER
WHO
IS
NOT
VERY
FAMILIAR
WITH
LOCAL
INVARIANT
FEATURE
DETECTORS
YET
WE
ADVICE
TO
SKIP
SECTION
AT
FIRST
THIS
SECTION
HAS
BEEN
ADDED
MAINLY
FOR
THE
MORE
ADVANCED
READER
TO
GIVE
FURTHER
INSIGHT
IN
HOW
THIS
FIELD
EVOLVED
AND
WHAT
WERE
THE
MOST
IMPORTANT
TRENDS
AND
TO
ADD
POINTERS
TO
EARLIER
WORK
LOCAL
FEATURES
IN
THE
LITERATURE
IN
THIS
SECTION
WE
GIVE
AN
OVERVIEW
OF
LOCAL
FEATURE
DETECTORS
PROPOSED
IN
THE
LITERATURE
STARTING
FROM
THE
EARLY
DAYS
OF
IMAGE
PROCESSING
AND
PATTERN
RECOGNITION
UP
TO
THE
CURRENT
STATE
OF
THE
ART
INTRODUCTION
THE
LITERATURE
ON
LOCAL
FEATURE
DETECTION
IS
VAST
AND
GOES
BACK
AS
FAR
AS
WHEN
IT
WAS
FIRST
OBSERVED
BY
ATTNEAVE
THAT
INFORMATION
ON
SHAPE
IS
CONCENTRATED
AT
DOMINANT
POINTS
HAVING
HIGH
CURVATURE
IT
IS
IMPOSSIBLE
TO
DESCRIBE
EACH
AND
EVERY
CONTRIBUTION
TO
OVER
YEARS
OF
RESEARCH
IN
DETAIL
INSTEAD
WE
PROVIDE
POINTERS
TO
THE
LITERATURE
WHERE
THE
INTERESTED
READER
CAN
FIND
OUT
MORE
THE
MAIN
GOAL
OF
THIS
SECTION
IS
TO
MAKE
THE
READER
AWARE
OF
THE
VARIOUS
GREAT
IDEAS
THAT
HAVE
BEEN
PROPOSED
ESPECIALLY
IN
THE
PRE
INTERNET
ERA
ALL
TOO
OFTEN
THESE
ARE
OVERLOOKED
AND
THEN
RE
INVENTED
WE
WOULD
LIKE
TO
GIVE
PROPER
CREDIT
TO
ALL
THOSE
RESEARCHERS
WHO
CONTRIBUTED
TO
THE
CURRENT
STATE
OF
THE
ART
EARLY
WORK
ON
LOCAL
FEATURES
IT
IS
IMPORTANT
TO
MENTION
THE
BEGINNINGS
OF
THIS
RESEARCH
AREA
AND
THE
FIRST
PUBLICATIONS
WHICH
APPEARED
AFTER
THE
OBSERVATION
ON
THE
IMPORTANCE
OF
CORNERS
AND
JUNCTIONS
IN
VISUAL
RECOGNITION
SEE
FIGURE
SINCE
THEN
A
LARGE
NUMBER
OF
ALGORITHMS
HAVE
BEEN
SUG
GESTED
FOR
EXTRACTING
INTEREST
POINTS
AT
THE
EXTREMA
OF
VARIOUS
FUNCTIONS
COMPUTED
ON
THE
DIGITAL
SHAPE
ALSO
IT
HAS
BEEN
UNDERSTOOD
EARLY
ON
IN
THE
IMAGE
PROCESSING
AND
VISUAL
PATTERN
RECOGNITION
FIELD
THAT
INTER
SECTIONS
OF
STRAIGHT
LINES
AND
STRAIGHT
CORNERS
ARE
STRONG
INDICATIONS
OF
MAN
MADE
STRUCTURES
SUCH
FEATURES
HAVE
BEEN
USED
IN
A
FIRST
SERIES
OF
APPLICATIONS
FROM
LINE
DRAWING
IMAGES
AND
PHOTOMOSAICS
FIRST
MONOGRAPHS
ON
DIGITAL
IMAGE
PROCESSING
BY
ROSENFELD
AND
BY
DUDA
AND
HART
AS
WELL
AS
THEIR
LATER
EDITIONS
SERVED
TO
ESTABLISH
THE
FIELD
ON
A
SOUND
THEORETICAL
FOUNDATION
OVERVIEW
WE
IDENTIFIED
A
NUMBER
OF
IMPORTANT
RESEARCH
DIRECTIONS
AND
STRUC
TURED
THE
SUBSECTIONS
OF
THIS
SECTION
ACCORDINGLY
FIRST
MANY
AUTHORS
HAVE
STUDIED
THE
CURVATURE
OF
CONTOURS
TO
FIND
CORNERS
THEIR
WORK
IS
DESCRIBED
IN
SECTION
OTHERS
DIRECTLY
ANALYZE
THE
IMAGE
INTENSITIES
E
G
BASED
ON
DERIVATIVES
OR
REGIONS
WITH
HIGH
VARIANCE
THIS
IS
THE
TOPIC
OF
SECTION
ANOTHER
LINE
OF
RESEARCH
HAS
BEEN
INSPIRED
BY
THE
HUMAN
VISUAL
SYSTEM
AND
AIMS
AT
REPRODUCING
THE
PROCESSES
IN
THE
HUMAN
BRAIN
SEE
SECTION
METHODS
FOCUSSING
ON
THE
EXPLOITATION
OF
COLOR
INFORMATION
ARE
DISCUSSED
IN
SECTION
WHILE
SECTION
DESCRIBES
MODEL
BASED
APPROACHES
MORE
RECENTLY
THERE
HAS
BEEN
A
TREND
TOWARD
FEATURE
DETECTION
WITH
INVARIANCE
AGAINST
VARIOUS
GEOMETRIC
TRANSFOR
MATIONS
INCLUDING
MULTI
SCALE
APPROACHES
AND
SCALE
OR
AFFINE
INVARIANT
METHODS
THESE
ARE
DISCUSSED
IN
SECTION
IN
SECTION
WE
FOCUS
ON
SEGMENTATION
BASED
METHODS
AND
SECTION
DESCRIBES
METHODS
WHICH
BUILD
ON
MACHINE
LEARNING
TECHNIQUES
FINALLY
SECTION
GIVES
AN
OVERVIEW
OF
DIFFERENT
EVALUATION
AND
COMPARISON
SCHEMES
PROPOSED
IN
THE
LITERATURE
CONTOUR
CURVATURE
BASED
METHODS
A
FIRST
CATEGORY
OF
INTEREST
POINT
DETECTORS
ARE
THE
CONTOUR
CURVATURE
BASED
METHODS
ORIGINALLY
THESE
WERE
MAINLY
APPLIED
TO
LINE
DRAWINGS
PIECEWISE
CONSTANT
REGIONS
AND
CAD
CAM
IMAGES
RATHER
THAN
NATURAL
SCENES
THE
FOCUS
WAS
ESPECIALLY
ON
THE
ACCURACY
OF
POINT
LOCALIZATION
THEY
WERE
MOST
POPULAR
OF
THE
END
OF
THE
AND
MOST
OF
THE
HIGH
CURVATURE
POINTS
CONTOUR
INTERSECTIONS
AND
JUNCTIONS
OFTEN
RESULT
IN
BI
DIRECTIONAL
SIG
NAL
CHANGES
THEREFORE
A
GOOD
STRATEGY
TO
DETECT
FEATURES
CONSISTS
OF
EXTRACTING
POINTS
ALONG
THE
CONTOUR
WITH
HIGH
CURVATURE
CURVATURE
OF
AN
ANALOG
CURVE
IS
DEFINED
AS
THE
RATE
AT
WHICH
THE
UNIT
TANGENT
VEC
TOR
CHANGES
WITH
RESPECT
TO
ARC
LENGTH
CONTOURS
ARE
OFTEN
ENCODED
IN
CHAINS
OF
POINTS
OR
REPRESENTED
IN
A
PARAMETRIC
FORM
USING
SPLINES
SEVERAL
TECHNIQUES
HAVE
BEEN
DEVELOPED
WHICH
INVOLVE
DETECTING
AND
CHAINING
EDGES
SO
AS
TO
FIND
CORNERS
IN
THE
CHAIN
BY
ANALYZING
THE
CHAIN
CODE
FINDING
MAXIMA
OF
CURVATURE
CHANGE
IN
DIREC
TION
OR
CHANGE
IN
APPEARANCE
OTHERS
AVOID
CHAINING
EDGES
AND
INSTEAD
LOOK
FOR
MAXIMA
OF
CURVATURE
OR
CHANGE
IN
DIRECTION
AT
PLACES
WHERE
THE
GRADIENT
IS
LARGE
SEVERAL
METHODS
FOR
DETECTING
EDGES
BASED
ON
GRAY
LEVEL
GRADIENT
AND
ANGULAR
CHANGES
IN
DIGITAL
CURVES
WERE
PROPOSED
IN
OTHER
SOLUTIONS
FOR
LINE
DRAWING
IMAGES
INCLUDE
METHODS
FOR
DETECTING
CORNERS
IN
A
CHAIN
CODED
PLANE
CURVE
IN
THESE
WORKS
A
MEA
SURE
FOR
THE
CORNERNESS
OF
A
POINT
IS
BASED
ON
MEAN
ANGULAR
DIFFERENCES
BETWEEN
SUCCESSIVE
SEGMENT
POSITIONS
ALONG
THE
CHAIN
ONE
GENERAL
APPROACH
TO
FEATURE
EXTRACTION
IS
TO
DETECT
THE
DOMINANT
POINTS
DIRECTLY
THROUGH
ANGLE
OR
CORNER
DETECTION
USING
VARIOUS
SCHEMES
FOR
APPROXIMATING
DISCRETE
CURVATURE
SUCH
AS
COSINE
OR
LOCAL
CURVATURE
WHICH
DEFINE
CORNERS
AS
DISCONTINUITIES
OF
AN
AVERAGE
CURVE
SLOPE
OTHER
PARAMETRIC
REPRESENTATION
LIKE
B
SPLINES
CURVES
ARE
COMMONLY
USED
IN
RENDERING
A
CURVE
IN
COMPUTER
GRAPHICS
COMPRESSION
AND
CODING
CAD
CAM
SYSTEMS
AND
ALSO
FOR
CURVE
FITTING
AND
SHAPE
DESCRIPTION
IN
CUBIC
POLYNOMIALS
ARE
FIT
TO
A
CURVE
AND
DISCONTINUITIES
ARE
DETECTED
IN
SUCH
CURVE
TO
LOCALIZE
INTEREST
POINTS
SPLINE
APPROXIMATIONS
OF
LINE
IMAGES
ARE
USED
IN
IN
COMBINATION
WITH
A
DYNAMIC
PROGRAMMING
TECHNIQUE
TO
FIND
THE
KNOTS
OF
A
SPLINE
PSEUDO
CODING
OF
LINE
FIGURES
AND
A
COMPLICATED
VECTOR
FINDER
TO
OBTAIN
INTEREST
POINTS
ARE
PROPOSED
IN
IN
DOMINANT
POINTS
ARE
COMPUTED
AT
THE
MAXIMUM
GLOBAL
CURVATURE
BASED
ON
THE
ITERATIVE
AVERAGING
OF
LOCAL
DISCRETIZED
CURVATURE
AT
EACH
POINT
WITH
RESPECT
TO
ITS
IMMEDIATE
NEIGHBORS
IN
TANGENTIAL
DEFLECTION
AND
CURVATURE
OF
DISCRETE
CURVES
ARE
DEFINED
BASED
ON
THE
GEOMETRICAL
AND
STATISTICAL
PROPERTIES
ASSOCIATED
WITH
THE
EIGENVALUE
EIGENVECTOR
STRUCTURE
OF
SAMPLE
COVARIANCE
MATRICES
COMPUTED
ON
CHAIN
CODES
ANOTHER
APPROACH
IS
TO
OBTAIN
A
PIECEWISE
LINEAR
POLYGONAL
APPROX
IMATION
OF
THE
DIGITAL
CURVE
SUBJECT
TO
CERTAIN
CONSTRAINTS
ON
THE
QUAL
ITY
OF
FIT
INDEED
IT
HAS
BEEN
POINTED
OUT
IN
THAT
PIECEWISE
LINEAR
POLYGONAL
APPROXIMATION
WITH
VARIABLE
BREAKPOINTS
WILL
TEND
TO
LOCATE
VERTICES
AT
ACTUAL
CORNER
POINTS
THESE
POINTS
CORRESPOND
APPROXIMATELY
TO
THE
ACTUAL
OR
EXTRAPOLATED
INTERSECTIONS
OF
ADJACENT
LINE
SEGMENTS
OF
THE
POLYGONS
A
SIMILAR
IDEA
WAS
EXPLORED
IN
MORE
RECENTLY
ESTIMATES
THE
PARAMETERS
OF
TWO
LINES
FITTED
TO
THE
TWO
SEGMENTS
NEIGHBORING
TO
THE
CORNER
POINT
A
CORNER
IS
DECLARED
IF
THE
PARAMETERS
ARE
STATISTICALLY
SIGNIFICANTLY
DIFFERENT
A
SIMILAR
APPROACH
IS
TO
IDENTIFY
EDGE
CROSSINGS
AND
JUNCTIONS
BY
FOLLOWING
IMAGE
GRADIENT
MAXIMA
OR
MINIMA
AND
FINDING
GAPS
IN
EDGE
MAPS
DEALING
WITH
SCALE
CORNER
DETECTION
METHODS
BY
CURVATURE
ESTIMATION
NORMALLY
USE
A
SET
OF
PARAMETERS
TO
ELIMINATE
CONTOUR
NOISE
AND
TO
OBTAIN
THE
CORNERS
AT
A
GIVEN
SCALE
ALTHOUGH
OBJECT
CORNERS
CAN
BE
FOUND
AT
MULTIPLE
NAT
URAL
SCALES
TO
SOLVE
THIS
PROBLEM
SOME
DETECTORS
APPLY
THEIR
ALGO
RITHMS
ITERATIVELY
WITHIN
A
CERTAIN
RANGE
OF
PARAMETERS
SELECTING
POINTS
WHICH
APPEAR
IN
A
FIXED
SET
OF
ITERATIONS
THE
STABILITY
OF
THE
POINTS
AND
THE
TIME
SPENT
FOR
THEIR
DETECTION
IS
CLOSELY
RELATED
TO
THE
NUMBER
OF
ITERATIONS
INITIAL
ATTEMPTS
TO
DEAL
WITH
DISCRETIZATION
AND
SCALE
PROBLEMS
VIA
AN
AVERAGING
SCHEME
CAN
BE
FOUND
IN
THE
CURVATURE
PRIMAL
SKETCH
CPS
PROPOSED
IN
IS
A
SCALE
SPACE
REPRESENTATION
OF
SIGNIF
ICANT
CHANGES
IN
CURVATURE
ALONG
CONTOURS
THE
CHANGES
ARE
CLASSIFIED
AS
BASIC
OR
COMPOUND
PRIMITIVES
SUCH
AS
CORNERS
SMOOTH
JOINTS
ENDS
CRANKS
BUMPS
AND
DENTS
THE
FEATURES
ARE
DETECTED
AT
DIFFERENT
SCALES
RESULTING
IN
A
MULTIPLE
SCALE
REPRESENTATION
OF
OBJECT
CONTOURS
A
SIMILAR
IDEA
WAS
EXPLORED
IN
AND
LATER
IN
WHERE
THE
CURVATURE
SCALE
SPACE
ANALYSIS
WAS
PERFORMED
TO
FIND
THE
LOCAL
SCALE
OF
CURVES
THEY
FIND
INFLECTION
POINTS
OF
THE
CURVES
AND
REPRESENT
SHAPES
IN
PARAMETRIC
FORMS
A
B
SPLINE
BASED
ALGORITHM
WAS
ALSO
PROPOSED
IN
THE
GENERAL
IDEA
IS
TO
FIT
A
B
SPLINE
TO
THE
CURVE
THEN
TO
MEASURE
THE
CUR
VATURE
AROUND
EACH
POINT
DIRECTLY
FROM
THE
B
SPLINE
COEFFICIENTS
ANOTHER
ALGORITHM
DEALING
WITH
SCALE
FOR
DETECTING
DOMINANT
POINTS
ON
A
DIGITAL
CLOSED
CURVE
IS
MOTIVATED
BY
THE
ANGLE
DETECTION
PRO
CEDURE
FROM
THEY
INDICATE
THAT
THE
DETECTION
OF
DOMINANT
POINTS
RELIES
PRIMARILY
ON
THE
PRECISE
DETERMINATION
OF
THE
REGION
OF
SUPPORT
RATHER
THAN
ON
THE
ESTIMATION
OF
DISCRETE
CURVATURE
FIRST
THE
REGION
OF
SUPPORT
FOR
EACH
POINT
BASED
ON
ITS
LOCAL
PROPERTIES
IS
DETERMINED
THEN
A
MEASURE
OF
RELATIVE
CURVATURE
OR
LOCAL
SYMMETRY
OF
EACH
POINT
IS
COMPUTED
THE
GAUSSIAN
FILTER
IS
THE
MOST
COMMONLY
USED
FILTER
IN
POINT
DETECTION
HOWEVER
IF
THE
SCALE
OF
A
GAUSSIAN
FILTER
IS
TOO
SMALL
THE
RESULT
MAY
INCLUDE
SOME
REDUNDANT
POINTS
WHICH
ARE
UNNECESSARY
DETAILS
I
E
DUE
TO
NOISE
IF
THE
SCALE
IS
TOO
LARGE
THE
POINTS
WITH
SMALL
SUPPORT
REGIONS
WILL
TEND
TO
BE
SMOOTHED
OUT
TO
SOLVE
THE
PROBLEMS
EXISTING
IN
GAUSSIAN
FILTERING
WITH
FIXED
SCALE
SCALE
SPACE
PROCEDURES
BASED
ON
MULTIPLE
SCALE
DISCRETE
CURVATURE
REPRESENTATION
AND
SEARCH
ING
ARE
PROPOSED
IN
THE
SCHEME
IS
BASED
ON
A
STABILITY
CRITERION
THAT
STATES
THAT
THE
PRESENCE
OF
A
CORNER
MUST
CONCUR
WITH
A
CURVATURE
MAXIMUM
OBSERVABLE
AT
A
MAJORITY
OF
SCALES
NATURAL
SCALES
OF
CURVES
WERE
STUDIED
IN
TO
AVOID
EXHAUSTIVE
REPRESENTATION
OF
CURVES
OVER
A
FULL
RANGE
OF
SCALES
A
SUCCESSFUL
SCALE
SELECTION
MECHANISM
FOR
GAUSSIAN
FILTERS
WITH
A
THEORETICAL
FORMULATION
WAS
ALSO
PROPOSED
IN
IN
A
NONLINEAR
ALGORITHM
FOR
CRITICAL
POINT
DETECTION
IS
PRE
SENTED
THEY
ESTABLISH
A
SET
OF
CRITERIA
FOR
THE
DESIGN
OF
A
POINT
DETECTION
ALGORITHM
TO
OVERCOME
THE
PROBLEMS
ARISING
FROM
CURVATURE
APPROXIMA
TION
AND
GAUSSIAN
FILTERING
ANOTHER
APPROACH
TO
BOUNDARY
SMOOTHING
IS
BASED
ON
SIMULATED
ANNEALING
FOR
CURVATURE
ESTIMATION
IN
THE
CORNER
POINTS
ARE
LOCALIZED
AT
THE
MAXIMA
OF
ABSOLUTE
CURVATURE
OF
EDGES
THE
CORNER
POINTS
ARE
TRACKED
THROUGH
MULTIPLE
CURVATURE
SCALE
LEVELS
TO
IMPROVE
LOCALIZATION
CHANG
AND
HORNG
PROPOSED
AN
ALGORITHM
TO
DETECT
CORNER
POINTS
USING
A
NEST
MOVING
AVERAGE
FILTER
IS
INVESTIGATED
IN
CORNERS
ARE
DETECTED
ON
CURVES
BY
COMPUTING
THE
DIFFERENCE
OF
BLURRED
IMAGES
AND
OBSERVING
THE
SHIFT
OF
HIGH
CURVA
TURE
POINTS
MORE
DETAILED
ANALYSIS
OF
VARIOUS
METHODS
FOR
DETERMINING
NATURAL
SCALES
OF
CURVES
CAN
BE
FOUND
IN
DISCUSSION
ALTHOUGH
THEORETICALLY
WELL
FOUNDED
FOR
ANALOG
CURVES
THE
CONTOUR
CUR
VATURE
CALCULATION
IS
LESS
ROBUST
IN
CASE
OF
DISCRETE
CURVES
POSSIBLE
ERROR
SOURCES
IN
DIGITAL
CURVATURE
ESTIMATION
WERE
INVESTIGATED
IN
FURTHERMORE
THE
OBJECTIVES
FOR
THE
ABOVE
DISCUSSED
DETECTORS
WERE
DIFFERENT
THAN
THE
ONES
WE
TYPICALLY
HAVE
NOWADAYS
IT
WAS
CONSIDERED
DISADVANTAGEOUS
IF
A
METHOD
DETECTED
CORNERS
ON
CIRCULAR
SHAPES
MUL
TIPLE
CORNERS
AT
JUNCTIONS
ETC
AT
THAT
TIME
A
MUCH
STRICTER
DEFINITION
OF
INTEREST
POINTS
CORNERS
WAS
USED
WITH
ONLY
POINTS
CORRESPONDING
TO
TRUE
CORNERS
IN
BEING
CONSIDERED
AS
RELEVANT
NOWADAYS
IN
MOST
PRACTICAL
APPLICATIONS
OF
INTEREST
POINTS
THE
FOCUS
IS
ON
ROBUST
STABLE
AND
DISTINCTIVE
POINTS
IRRESPECTIVE
OF
WHETHER
THEY
CORRE
SPOND
TO
TRUE
CORNERS
OR
NOT
SEE
ALSO
OUR
EARLIER
DISCUSSION
IN
SECTION
THERE
HAS
BEEN
LESS
ACTIVITY
IN
THIS
AREA
RECENTLY
OVER
THE
PAST
TEN
YEARS
DUE
TO
COMPLEXITY
AND
ROBUSTNESS
PROBLEMS
WHILE
METHODS
BASED
DIRECTLY
ON
IMAGE
INTENSITY
ATTRACTED
MORE
ATTENTION
INTENSITY
BASED
METHODS
METHODS
BASED
ON
IMAGE
INTENSITY
HAVE
ONLY
WEAK
ASSUMPTIONS
AND
ARE
TYPICALLY
APPLICABLE
TO
A
WIDE
RANGE
OF
IMAGES
MANY
OF
THESE
APPROACHES
ARE
BASED
ON
FIRST
AND
SECOND
ORDER
GRAY
VALUE
DERIVATIVES
WHILE
OTHERS
USE
HEURISTICS
TO
FIND
REGIONS
OF
HIGH
VARIANCE
DIFFERENTIAL
APPROACHES
HESSIAN
BASED
APPROACHES
ONE
OF
THE
EARLY
INTENSITY
BASED
DETEC
TORS
IS
THE
ROTATION
INVARIANT
HESSIAN
BASED
DETECTOR
PROPOSED
BY
BEAUDET
IT
EXPLORES
THE
SECOND
ORDER
TAYLOR
EXPANSION
OF
THE
INTEN
SITY
SURFACE
AND
ESPECIALLY
THE
HESSIAN
MATRIX
CONTAINING
THE
SECOND
ORDER
DERIVATIVES
THE
DETERMINANT
OF
THIS
MATRIX
REACHES
A
MAXIMUM
FOR
BLOB
LIKE
STRUCTURES
IN
THE
IMAGE
A
MORE
DETAILED
DESCRIPTION
OF
THIS
METHOD
CAN
BE
FOUND
IN
SECTION
IT
HAS
BEEN
EXTENDED
IN
AND
WHERE
THE
INTEREST
POINTS
ARE
LOCALIZED
AT
THE
ZERO
CROSSING
OF
A
CURVE
JOINING
LOCAL
EXTREMA
OF
THE
HESSIAN
DETERMINANT
AROUND
A
CORNER
SIMILARLY
HIGH
CURVATURE
POINTS
CAN
BE
LOCALIZED
BY
COMPUTING
GAUSSIAN
CURVATURE
OF
THE
IMAGE
SURFACE
I
E
SADDLE
POINTS
IN
IMAGE
BRIGHTNESS
IN
A
LOCAL
QUADRATIC
SURFACE
WAS
FIT
TO
THE
IMAGE
INTEN
SITY
FUNCTION
THE
PARAMETERS
OF
THE
SURFACE
WERE
USED
TO
DETERMINE
THE
GRADIENT
MAGNITUDE
AND
THE
RATE
OF
CHANGE
OF
GRADIENT
DIRECTION
THE
RESULTING
DETECTOR
USES
THE
CURVATURE
OF
ISOPHOTES
COMPUTED
FROM
FIRST
AND
SECOND
ORDER
DERIVATIVES
SCALED
BY
IMAGE
GRADIENT
TO
MAKE
IT
MORE
ROBUST
TO
NOISE
A
SIMILAR
IDEA
WAS
PROPOSED
IN
A
DETAILED
INVESTIGATION
IN
AND
LATER
IN
SHOWS
THAT
THE
DETECTORS
OF
ALL
PERFORM
THE
SAME
MEASUREMENTS
ON
THE
IMAGE
AND
HAVE
RELATIVELY
LOW
RELIABILITY
ACCORD
ING
TO
CRITERIA
BASED
ON
LOCALIZATION
PRECISION
NEVERTHELESS
THE
TRACE
AND
DETERMINANT
OF
THE
HESSIAN
MATRIX
WERE
SUCCESSFULLY
USED
LATER
ON
IN
SCALE
AND
AFFINE
INVARIANT
EXTENSIONS
OF
INTEREST
POINT
DETEC
TORS
WHEN
OTHER
FEATURE
PROPERTIES
BECAME
MORE
IMPORTANT
GRADIENT
BASED
APPROACHES
LOCAL
FEATURE
DETECTION
BASED
ON
FIRST
ORDER
DERIVATIVES
IS
ALSO
USED
IN
VARIOUS
APPLICATIONS
A
CORNER
DETECTOR
WHICH
RETURNS
POINTS
AT
THE
LOCAL
MAXIMA
OF
A
DIRECTIONAL
VARIANCE
MEA
SURE
WAS
FIRST
INTRODUCED
IN
IN
THE
CONTEXT
OF
MOBILE
ROBOT
NAVIGATION
IT
WAS
A
HEURISTIC
IMPLEMENTATION
OF
THE
AUTO
CORRELATION
FUNCTION
ALSO
EXPLORED
IN
THE
PROPOSED
CORNER
DETECTOR
INVESTIGATES
A
LOCAL
WINDOW
IN
THE
IMAGE
AND
DETERMINES
THE
AVERAGE
CHANGE
OF
INTEN
SITY
WHICH
RESULTS
FROM
SHIFTING
THE
WINDOW
BY
A
FEW
PIXELS
IN
VARIOUS
DIRECTIONS
THIS
IDEA
IS
TAKEN
FURTHER
IN
AND
FORMALIZED
BY
USING
FIRST
ORDER
DERIVATIVES
IN
A
SO
CALLED
SECOND
MOMENT
MATRIX
TO
EXPLORE
LOCAL
STATISTICS
OF
DIRECTIONAL
IMAGE
INTENSITY
VARIATIONS
THE
METHOD
SEPARATES
CORNER
CANDIDATE
DETECTION
AND
LOCALIZATION
TO
IMPROVE
THE
ACCURACY
TO
SUBPIXEL
PRECISION
AT
THE
COST
OF
HIGHER
COMPUTATIONAL
COM
PLEXITY
HARRIS
AND
STEPHENS
IMPROVED
THE
APPROACH
BY
MORAVEC
BY
PERFORMING
ANALYTICAL
EXPANSION
OF
THE
AVERAGE
INTENSITY
VARI
ANCE
THIS
RESULTS
IN
A
SECOND
MOMENT
MATRIX
COMPUTED
WITH
SOBEL
DERIVATIVES
AND
A
GAUSSIAN
WINDOW
A
FUNCTION
BASED
ON
THE
DETERMI
NANT
AND
TRACE
OF
THAT
MATRIX
WAS
INTRODUCED
WHICH
TOOK
INTO
ACCOUNT
BOTH
EIGENVALUES
OF
THE
MATRIX
THIS
DETECTOR
IS
WIDELY
KNOWN
TODAY
AS
THE
HARRIS
DETECTOR
OR
PLESSEY
DETECTOR
AND
IS
PROBABLY
THE
BEST
KNOWN
INTEREST
POINT
DETECTOR
AROUND
IT
IS
DESCRIBED
IN
MORE
DETAIL
IN
SECTION
IT
HAS
BEEN
EXTENDED
IN
NUMEROUS
PAPERS
E
G
BY
USING
GAUSSIAN
DERIVATIVES
COMBINATIONS
OF
FIRST
AND
SECOND
ORDER
DERIVATIVES
OR
AN
EDGE
BASED
SECOND
MOMENT
MATRIX
BUT
THE
UNDERLYING
IDEA
REMAINS
THE
SAME
THE
HARRIS
DETECTOR
WAS
ALSO
INVESTIGATED
IN
AND
DEMONSTRATED
TO
BE
OPTIMAL
FOR
L
JUNCTIONS
BASED
ON
THE
ASSUMPTION
OF
AN
AFFINE
IMAGE
DEFORMATION
AN
ANALYSIS
IN
LED
TO
THE
CONCLUSION
THAT
IT
IS
MORE
CONVENIENT
TO
USE
THE
SMALLEST
EIGENVALUE
OF
THE
AUTOCORRELATION
MATRIX
AS
THE
CORNER
STRENGTH
FUNCTION
MORE
RECENTLY
THE
SECOND
MOMENT
MATRIX
HAS
ALSO
BEEN
ADOPTED
TO
SCALE
CHANGES
BY
PARAMETERIZING
GAUSSIAN
FILTERS
AND
NORMALIZ
ING
THEM
WITH
RESPECT
TO
SCALE
BASED
ON
SCALE
SPACE
THEORY
ALSO
THE
HARRIS
DETECTOR
WAS
EXTENDED
WITH
SEARCH
OVER
SCALE
AND
AFFINE
SPACE
IN
USING
THE
LAPLACIAN
OPERATOR
AND
EIGENVALUES
OF
THE
SECOND
MOMENT
MATRIX
INSPIRED
BY
THE
PIONEERING
WORK
OF
LINDE
BERG
SEE
SECTION
FOR
DETAILS
THE
APPROACH
FROM
PERFORMS
AN
ANALYSIS
OF
THE
COMPUTATION
OF
THE
SECOND
MOMENT
MATRIX
AND
ITS
APPROXIMATIONS
A
SPEED
INCREASE
IS
ACHIEVED
BY
COMPUTING
ONLY
TWO
SMOOTHED
IMAGES
INSTEAD
OF
THE
THREE
PREVIOUSLY
REQUIRED
A
NUMBER
OF
OTHER
SUGGESTIONS
HAVE
BEEN
MADE
FOR
HOW
TO
COMPUTE
THE
CORNER
STRENGTH
FROM
THE
SECOND
ORDER
MATRIX
AND
THESE
HAVE
ALL
BEEN
SHOWN
TO
BE
EQUIV
ALENT
TO
VARIOUS
MATRIX
NORMS
A
GENERALIZATION
TO
IMAGES
WITH
MULTI
DIMENSIONAL
PIXELS
WAS
ALSO
PROPOSED
IN
PLESSEY
ELECTRONIC
RESEARCH
LTD
IN
THE
HARRIS
CORNER
DETECTOR
IS
EXTENDED
TO
YIELD
STABLE
FEA
TURES
UNDER
MORE
GENERAL
TRANSFORMATIONS
THAN
PURE
TRANSLATIONS
TO
THIS
END
THE
AUTO
CORRELATION
FUNCTION
WAS
STUDIED
UNDER
ROTATIONS
SCALINGS
UP
TO
FULL
AFFINE
TRANSFORMATIONS
INTENSITY
VARIATIONS
A
DIFFERENT
CATEGORY
OF
APPROACHES
BASED
ON
INTENSITY
VARIATIONS
APPLIES
MATHEMATICAL
MORPHOLOGY
TO
EXTRACT
HIGH
CURVATURE
POINTS
THE
USE
OF
ZERO
CROSSINGS
OF
THE
SHAPE
BOUNDARY
CURVATURE
IN
BINARY
IMAGES
DETECTED
WITH
A
MORPHOLOGICAL
OPENING
OPERATOR
WAS
INVESTIGATED
IN
MATHEMATICAL
MORPHOLOGY
WAS
ALSO
USED
TO
EXTRACT
CONVEX
AND
CONCAVE
POINTS
FROM
EDGES
IN
LATER
ON
A
PARALLEL
ALGO
RITHM
BASED
ON
AN
ANALYSIS
OF
MORPHOLOGICAL
RESIDUES
AND
CORNER
CHAR
ACTERISTICS
WAS
PROPOSED
IN
ANOTHER
APPROACH
INDICATES
THAT
FOR
INTEREST
POINTS
THE
MEDIAN
VALUE
OVER
A
SMALL
NEIGHBORHOOD
IS
SIGNIFICANTLY
DIFFERENT
FROM
THE
CORNER
POINT
VALUE
THUS
THE
DIFFERENCE
IN
INTENSITY
BETWEEN
THE
CENTER
AND
MEDIAN
GIVES
A
STRONG
INDICATION
FOR
CORNERS
HOWEVER
THIS
METHOD
CANNOT
DEAL
WITH
MORE
COMPLEX
JUNCTIONS
OR
SMOOTH
EDGES
A
SIMPLE
AND
EFFICIENT
DETECTOR
NAMED
SUSAN
WAS
INTRODUCED
IN
BASED
ON
EARLIER
WORK
FROM
IT
COMPUTES
THE
FRACTION
OF
PIXELS
WITHIN
A
NEIGHBORHOOD
WHICH
HAVE
SIMILAR
INTENSITY
TO
THE
CENTER
PIXEL
CORNERS
CAN
THEN
BE
LOCALIZED
BY
THRESHOLDING
THIS
MEASURE
AND
SELECTING
LOCAL
MINIMA
THE
POSITION
OF
THE
CENTER
OF
GRAVITY
IS
USED
TO
FILTER
OUT
FALSE
POSITIVES
MORE
DETAILS
ON
THE
SUSAN
DETECTOR
CAN
BE
FOUND
IN
SECTION
A
SIMILAR
IDEA
WAS
EXPLORED
IN
WHERE
PIXELS
ON
A
CIRCLE
ARE
CONSIDERED
AND
COMPARED
TO
THE
CENTER
OF
A
PATCH
MORE
RECENTLY
PROPOSED
THE
FAST
DETECTOR
A
POINT
IS
CLAS
SIFIED
AS
A
CORNER
IF
ONE
CAN
FIND
A
SUFFICIENTLY
LARGE
SET
OF
PIXELS
ON
A
CIRCLE
OF
FIXED
RADIUS
AROUND
THE
POINT
SUCH
THAT
THESE
PIXELS
ARE
ALL
SIGNIFICANTLY
BRIGHTER
RESP
DARKER
THAN
THE
CENTRAL
POINT
EFFICIENT
CLASSIFICATION
IS
BASED
ON
A
DECISION
TREE
MORE
DETAILS
ON
FAST
CAN
BE
FOUND
IN
SECTION
LOCAL
RADIAL
SYMMETRY
HAS
BEEN
EXPLORED
IN
TO
IDENTIFY
INTEREST
POINTS
AND
ITS
REAL
TIME
IMPLEMENTATION
WAS
ALSO
PROPOSED
WAVELET
TRANSFORMATION
WAS
ALSO
INVESTIGATED
IN
THE
CONTEXT
OF
FEATURE
POINT
EXTRACTION
WITH
SUCCESSFUL
RESULTS
BASED
ON
MULTI
RESOLUTION
ANALYSIS
IN
SALIENCY
THE
IDEA
OF
SALIENCY
HAS
BEEN
USED
IN
A
NUMBER
OF
COMPUTER
VISION
ALGORITHMS
THE
EARLY
APPROACH
OF
USING
EDGE
DETECTORS
TO
EXTRACT
OBJECT
DESCRIPTIONS
EMBODIES
THE
IDEA
THAT
THE
EDGES
ARE
MORE
SIGNIFICANT
THAN
OTHER
PARTS
OF
THE
IMAGE
MORE
EXPLICIT
USES
OF
SALIENCY
CAN
BE
DIVIDED
INTO
THOSE
THAT
CONCENTRATE
ON
LOW
LEVEL
LOCAL
FEATURES
E
G
AND
THOSE
THAT
COMPUTE
SALIENT
GROUPINGS
OF
LOW
LEVEL
FEATURES
E
G
THOUGH
SOME
APPROACHES
OPERATE
AT
BOTH
LEVELS
E
G
THE
TECHNIQUE
SUGGESTED
IN
IS
BASED
ON
THE
MAXIMIZATION
OF
DESCRIPTOR
VECTORS
ACROSS
A
PARTICULAR
IMAGE
THESE
SALIENT
POINTS
ARE
THE
POINTS
ON
THE
OBJECT
WHICH
ARE
ALMOST
UNIQUE
HENCE
THEY
MAXI
MIZE
THE
DISCRIMINATION
BETWEEN
THE
OBJECTS
A
RELATED
METHOD
IDENTIFIES
SALIENT
FEATURES
FOR
USE
IN
AUTOMATED
GENERATION
OF
STATISTICAL
SHAPE
APPEARANCE
MODELS
THE
METHOD
AIMS
TO
SELECT
THOSE
FEATURES
WHICH
ARE
LESS
LIKELY
TO
BE
MISMATCHED
REGIONS
OF
LOW
DENSITY
IN
A
MUL
TIDIMENSIONAL
FEATURE
SPACE
GENERATED
FROM
THE
IMAGE
ARE
CLASSIFIED
AS
HIGHLY
SALIENT
A
MORE
THEORETICALLY
FOUNDED
APPROACH
BASED
ON
VARIABILITY
OR
COM
PLEXITY
OF
IMAGE
INTENSITY
WITHIN
A
REGION
WAS
PROPOSED
IN
IT
WAS
MOTIVATED
BY
VISUAL
SALIENCY
AND
INFORMATION
CONTENT
WHICH
WE
REVISE
IN
THE
NEXT
SECTION
THE
METHOD
FROM
DEFINES
SALIENCY
IN
TERMS
OF
LOCAL
SIGNAL
COMPLEXITY
OR
UNPREDICTABILITY
MORE
SPECIFICALLY
THE
USE
OF
SHANNON
ENTROPY
OF
LOCAL
ATTRIBUTES
IS
SUGGESTED
THE
IDEA
IS
TO
FIND
A
POINT
NEIGHBORHOOD
WITH
HIGH
COMPLEXITY
AS
A
MEASURE
OF
SALIENCY
OR
INFORMATION
CONTENT
THE
METHOD
MEASURES
THE
CHANGE
IN
ENTROPY
OF
A
GRAY
VALUE
HISTOGRAM
COMPUTED
IN
A
POINT
NEIGHBORHOOD
THE
SEARCH
WAS
EXTENDED
TO
SCALE
AND
AFFINE
PARAMETERIZED
REGIONS
THUS
PROVIDING
POSITION
SCALE
AND
AFFINE
SHAPE
OF
THE
REGION
NEIGHBORHOOD
FOR
A
DETAILED
DISCUSSION
WE
REFER
TO
SECTION
BIOLOGICALLY
PLAUSIBLE
METHODS
MOST
SYSTEMS
PROPOSED
IN
THE
PREVIOUS
SECTIONS
WERE
MAINLY
CONCERNED
WITH
THE
ACCURACY
OF
INTEREST
POINT
LOCALIZATION
THIS
IS
IMPORTANT
IN
THE
CONTEXT
OF
FITTING
PARAMETRIC
CURVES
TO
CONTROL
POINTS
OR
IMAGE
MATCH
ING
FOR
RECOVERING
THE
GEOMETRY
IN
CONTRAST
THE
BIOLOGICALLY
PLAUSIBLE
METHODS
REVIEWED
IN
THIS
SECTION
WERE
MAINLY
PROPOSED
IN
THE
CON
TEXT
OF
ARTIFICIAL
INTELLIGENCE
AND
VISUAL
RECOGNITION
MOST
OF
THEM
DID
NOT
HAVE
A
SPECIFIC
APPLICATION
PURPOSE
AND
THEIR
MAIN
GOAL
WAS
TO
MODEL
THE
PROCESSES
OF
THE
HUMAN
BRAIN
NUMEROUS
MODELS
OF
HUMAN
VISUAL
ATTENTION
OR
SALIENCY
HAVE
BEEN
DISCUSSED
IN
COGNITIVE
PSYCHOL
OGY
AND
COMPUTER
VISION
HOWEVER
THE
VAST
MAJORITY
WERE
ONLY
OF
THEORETICAL
INTEREST
AND
ONLY
FEW
WERE
IMPLEMENTED
AND
TESTED
ON
REAL
IMAGES
FEATURE
DETECTION
AS
PART
OF
THE
PRE
ATTENTIVE
STAGE
ONE
OF
THE
MAIN
MODELS
FOR
EARLY
VISION
IN
HUMANS
ATTRIBUTED
TO
NEISSER
IS
THAT
IT
CONSISTS
OF
A
PRE
ATTENTIVE
AND
AN
ATTEN
TIVE
STAGE
BIOLOGICALLY
PLAUSIBLE
METHODS
FOR
FEATURE
DETECTION
USU
ALLY
REFER
TO
THE
IDEA
THAT
CERTAIN
PARTS
OF
A
SCENE
ARE
PRE
ATTENTIVELY
DISTINCTIVE
AND
CREATE
SOME
FORM
OF
IMMEDIATE
RESPONSE
WITHIN
THE
EARLY
STAGES
OF
THE
HUMAN
VISUAL
SYSTEM
IN
THE
PRE
ATTENTIVE
STAGE
ONLY
POP
OUT
FEATURES
ARE
DETECTED
THESE
ARE
LOCAL
REGIONS
OF
THE
IMAGE
WHICH
PRESENT
SOME
FORM
OF
SPATIAL
DISCONTINUITY
IN
THE
ATTEN
TIVE
STAGE
RELATIONSHIPS
BETWEEN
THESE
FEATURES
ARE
FOUND
AND
GROUP
ING
TAKES
PLACE
THIS
MODEL
HAS
WIDELY
INFLUENCED
THE
COMPUTER
VISION
COMMUNITY
MAINLY
THROUGH
THE
WORK
OF
MARR
AND
IS
REFLECTED
IN
THE
CLASSICAL
COMPUTER
VISION
APPROACH
FEATURE
DETEC
TION
AND
PERCEPTUAL
GROUPING
FOLLOWED
BY
MODEL
MATCHING
AND
CORRE
SPONDENCE
SEARCH
ACTIVITIES
IN
THE
MODELS
OF
ATTENTION
STARTED
IN
THE
MID
FOLLOWING
PROGRESS
IN
NEUROPHYSIOLOGICAL
AND
PSYCHOLOGICAL
RESEARCH
ONE
APPROACH
INSPIRED
BY
NEURO
BIOLOGICAL
MECHANISMS
WAS
PRO
POSED
IN
THEY
APPLY
GABOR
LIKE
FILTERS
TO
COMPUTE
LOCAL
ENERGY
OF
THE
SIGNAL
MAXIMA
OF
THE
FIRST
AND
SECOND
ORDER
DERIVA
TIVES
OF
THAT
ENERGY
INDICATE
THE
PRESENCE
OF
INTEREST
POINTS
THE
IDEA
OF
USING
GABOR
FILTER
RESPONSES
FROM
DIFFERENT
SCALES
WAS
FURTHER
EXPLORED
IN
THE
APPROACH
DEVELOPED
IN
WAS
MOTIVATED
BY
PSY
CHOPHYSICAL
EXPERIMENTS
THEY
COMPUTE
A
SYMMETRY
SCORE
OF
THE
SIGNAL
AT
EACH
IMAGE
PIXEL
IN
DIFFERENT
DIRECTIONS
REGIONS
WITH
SIGNIFICANT
SYM
METRY
ARE
THEN
SELECTED
AS
INTEREST
POINTS
THEORY
ON
TEXTURE
RECOGNITION
AND
THE
IDEA
OF
TEXTONS
AS
SIMPLE
LOCAL
STRUCTURES
LIKE
BLOBS
CORNERS
JUNCTIONS
LINE
ENDS
ETC
WAS
INTRODUCED
IN
HE
SUGGESTED
THAT
STATISTICS
OVER
TEXTON
DISTRIBUTIONS
PLAY
AN
IMPORTANT
ROLE
IN
RECOGNITION
THE
EXTRACTION
OF
SIMPLE
TEXTONS
IS
DONE
IN
THE
PRE
ATTENTIVE
STAGE
AND
THE
CONSTRUCTION
OF
RELATIONS
IN
THE
ATTEN
TIVE
STAGE
A
FEATURE
INTEGRATION
THEORY
BASED
ON
THESE
PRINCIPLES
WAS
PROPOSED
IN
HE
DISTINGUISHED
BETWEEN
A
DISJUNCTIVE
CASE
WHERE
THE
DISTINCTIVE
FEATURES
CAN
BE
DIRECTLY
LOCALIZED
IN
A
FEATURE
MAP
AND
A
CONJUNCTIVE
CASE
WHERE
THE
FEATURE
CAN
BE
EXTRACTED
ONLY
BY
PROCESSING
VARIOUS
FEATURE
MAPS
SIMULTANEOUSLY
THIS
MODEL
WAS
IMPLEMENTED
BY
COMBINING
BOTTOM
UP
AND
TOP
DOWN
MEASURES
OF
INTEREST
THE
BOT
TOM
UP
METHOD
MERGES
VARIOUS
FEATURE
MAPS
AND
LOOKS
FOR
INTERESTING
EVENTS
WHILE
IN
THE
TOP
DOWN
PROCESS
KNOWLEDGE
ABOUT
THE
TARGET
IS
EXPLOITED
THE
MAIN
GOAL
OF
THE
ABOVE
SYSTEMS
WAS
TO
PROVIDE
COMPUTATION
ALLY
PLAUSIBLE
MODELS
OF
VISUAL
ATTENTION
THEIR
INTEREST
WAS
MAINLY
THEORETICAL
HOWEVER
THOSE
SYSTEMS
SERVED
AS
SOURCE
OF
INSPIRATION
FOR
PRACTICAL
SOLUTIONS
FOR
REAL
IMAGES
ONCE
MACHINE
LEARNING
TECHNIQUES
LIKE
NEURAL
NETWORKS
HAD
GROWN
MATURE
ENOUGH
IN
IMAGE
PRO
CESSING
OPERATORS
WERE
COMBINED
WITH
THE
ATTENTIVE
MODELS
TO
MAKE
IT
APPLICABLE
TO
MORE
REALISTIC
IMAGES
HE
APPLIES
A
LAPLACIAN
OF
GAUSSIANS
LOG
LIKE
OPERATOR
TO
FEATURE
MAPS
TO
MODEL
THE
RECEPTIVE
FIELDS
AND
ENHANCE
THE
INTERESTING
EVENTS
THE
IMAGE
WAS
ANALYZED
AT
MULTIPLE
SCALES
THE
APPROACH
FROM
USES
A
SET
OF
FEATURE
TEMPLATES
AND
COR
RELATES
THEM
WITH
THE
IMAGE
TO
PRODUCE
FEATURE
MAPS
WHICH
ARE
THEN
ENHANCED
WITH
LOG
TEMPORAL
DERIVATIVES
WERE
USED
TO
DETECT
MOVING
OBJECTS
KOCH
AND
ULLMAN
PROPOSED
A
VERY
INFLUENTIAL
COMPUTATIONAL
MODEL
OF
VISUAL
ATTENTION
WHICH
ACCOUNTS
FOR
SEVERAL
PSYCHOPHYSICAL
PHENOMENA
THEY
PROPOSED
TO
BUILD
A
SET
OF
MAPS
BASED
ON
ORIENTA
TION
COLOR
DISPARITY
AND
MOTION
AND
TO
SIMULATE
THE
LATERAL
INHIBITION
MECHANISM
BY
EXTRACTING
LOCATIONS
WHICH
DIFFER
SIGNIFICANTLY
FROM
THEIR
NEIGHBORHOOD
INFORMATION
FROM
DIFFERENT
MAPS
IS
THEN
MERGED
INTO
A
SINGLE
SALIENCY
MAP
A
WINNER
TAKE
ALL
WTA
NETWORK
WAS
USED
TO
SELECT
THE
ACTIVE
LOCATION
IN
THE
MAPS
IN
A
HIERARCHICAL
MANNER
USING
A
PYRAMIDAL
STRATEGY
THE
HYPOTHESES
SUGGESTED
IN
WERE
FIRST
IMPLEMENTED
IN
A
SIMILAR
IMPLEMENTATION
OF
THE
WTA
MODEL
WAS
PROPOSED
IN
THE
EXTRACTION
OF
GLOBALLY
SALIENT
STRUCTURES
LIKE
OBJECT
OUTLINES
WAS
INVESTIGATED
IN
BY
GROUPING
LOCAL
INFORMATION
SUCH
AS
CONTOUR
FRAGMENTS
BUT
NO
RELATION
TO
PRE
ATTENTIVE
VISION
WAS
CLAIMED
NON
UNIFORM
RESOLUTION
AND
COARSE
TO
FINE
PROCESSING
ALSO
NON
UNIFORM
RESOLUTION
OF
THE
RETINA
AND
COARSE
TO
FINE
PROCESS
ING
STRATEGIES
HAVE
BEEN
STUDIED
IN
BIOLOGICALLY
PLAUSIBLE
MODELS
THESE
HAVE
BEEN
SIMULATED
MOSTLY
VIA
SCALE
SPACE
TECHNIQUES
HOWEVER
THESE
SYSTEMS
WERE
MOSTLY
FOCUSED
ON
THE
ENGINEERING
AND
REALTIME
ASPECTS
RATHER
THAN
ITS
BIOLOGICAL
PLAUSIBILITY
ONE
OF
THE
FIRST
SYSTEMS
TO
PERFORM
INTEREST
POINT
DETECTION
IN
SCALE
SPACE
WAS
PRO
POSED
IN
THEY
BUILT
A
LAPLACIAN
PYRAMID
FOR
COARSE
TO
FINE
FEA
TURE
SELECTION
TEMPLATES
WERE
USED
TO
LOCALIZE
THE
OBJECTS
IN
THE
LOG
SPACE
TEMPLATES
WERE
ALSO
EMPLOYED
FOR
BUILDING
FEATURES
MAPS
WHICH
WERE
THEN
COMBINED
BY
A
WEIGHTED
SUM
DIFFERENCE
OF
GAUSSIANS
DOG
FILTERS
WERE
USED
IN
THE
SYSTEM
DESIGNED
IN
TO
ACCELERATE
THE
COMPUTATION
BIOLOGICALLY
INSPIRED
SYSTEMS
DEVELOPED
IN
EXPLORED
THE
IDEA
OF
USING
BOUNDARY
AND
INTEREST
POINT
DETECTORS
BASED
ON
DOG
FILTERS
AS
WELL
AS
DIRECTIONAL
DIFFERENCES
OF
OFFSET
GAUSSIANS
DOOG
TO
SIMULATE
SIMPLE
CELLS
IN
THE
SYSTEM
PROPOSED
IN
WAS
MAINLY
CONCERNED
WITH
CLASSIFI
CATION
OF
TEXTURES
STUDIED
EARLIER
IN
THE
FEATURE
EXTRACTION
PART
USED
A
BANK
OF
FILTERS
BASED
ON
ORIENTED
KERNELS
DOG
AND
DOOG
TO
PRODUCE
FEATURE
MAPS
SIMILAR
TO
THE
NEXT
STAGE
CORRESPONDS
TO
A
WTA
MECHANISM
TO
SUPPRESS
WEAK
RESPONSES
AND
SIMULATE
LATERAL
INHIBITION
FINALLY
ALL
THE
RESPONSES
ARE
MERGED
TO
DETECT
TEXTURE
BOUNDARIES
SPATIAL
EVENT
DETECTION
ROBUST
STATISTICS
HAVE
ALSO
BEEN
USED
TO
DETECT
OUTLIERS
IN
A
SET
OF
IMAGE
PRIMITIVES
THE
IDEA
IS
BASED
ON
THE
OBSERVATION
THAT
TEXTURES
CAN
BE
REPRESENTED
BY
THEIR
STATISTICS
AND
THE
LOCATIONS
WHICH
VIOLATE
THOSE
STATISTICS
REPRESENT
INTERESTING
EVENTS
FOR
EXAMPLE
TEXTURE
PRIM
ITIVES
ARE
REPRESENTED
BY
A
NUMBER
OF
ATTRIBUTES
USING
HISTOGRAMS
AND
RANSAC
IN
FIRST
ORDER
STATISTICS
OVER
FEATURE
MAPS
COMPUTED
FROM
ZERO
CROSS
INGS
OF
DOG
AT
DIFFERENT
SCALES
ARE
USED
IN
FOR
EACH
POINT
A
HISTOGRAM
OF
GRADIENT
ORIENTATIONS
IS
THEN
CONSTRUCTED
AND
THE
LOCAL
HISTOGRAMS
ARE
COMBINED
INTO
A
GLOBAL
ONE
WHICH
IS
SIMILAR
IN
SPIRIT
TO
THE
MORE
RECENT
SIFT
DESCRIPTOR
LOCAL
HISTOGRAMS
ARE
THEN
COMPARED
WITH
THE
GLOBAL
ONE
TO
PROVIDE
A
MEASURE
OF
INTEREST
ANOTHER
STATISTICAL
MODEL
WAS
PROPOSED
IN
THEY
MEASURE
THE
EDGE
DENSITY
AT
A
RANGE
OF
DISTANCES
FROM
THE
INTEREST
POINT
TO
BUILD
AN
EDGE
DISTRIBUTION
HISTOGRAM
THIS
IDEA
HAS
BEEN
USED
LATER
IN
THE
SHAPE
CONTEXT
DESCRIPTOR
OF
CELLS
THAT
RESPOND
ONLY
TO
EDGES
AND
BARS
WHICH
TERMINATE
WITHIN
THEIR
RECEPTIVE
FIELD
HAVE
FIRST
BEEN
FOUND
IN
A
CORNER
DETECTION
ALGORITHM
BASED
ON
A
MODEL
FOR
SUCH
END
STOPPED
CELLS
IN
THE
VISUAL
COR
TEX
WAS
PRESENTED
IN
FURTHERMORE
THE
NOTION
OF
END
STOPPED
CELLS
WAS
GENERALIZED
TO
COLOR
CHANNELS
IN
A
BIOLOGICALLY
PLAUSIBLE
WAY
BASED
ON
COLOR
OPPONENT
PROCESSES
A
MORE
RECENT
VISUAL
ATTENTION
SYSTEM
ALSO
MOTIVATED
BY
THE
EARLY
PRIMATE
VISUAL
SYSTEM
IS
PRESENTED
IN
MULTISCALE
IMAGE
FEATURES
DETECTED
AT
LOCAL
SPATIAL
DISCONTINUITIES
IN
INTENSITY
COLOR
AND
ORIEN
TATION
ARE
COMBINED
INTO
A
SINGLE
TOPOGRAPHICAL
SALIENCY
MAP
AND
A
NEURAL
NETWORK
SELECTS
LOCATIONS
DEPENDING
ON
THE
SALIENCY
OTHER
RECENT
VISUAL
RECOGNITION
SYSTEMS
INSPIRED
BY
A
MODEL
OF
VISUAL
CORTEX
WHICH
FOLLOW
MODELS
FROM
CAN
BE
FOUND
IN
THESE
METHODS
ATTEMPT
TO
IMPLEMENT
SIMPLE
AND
COMPLEX
CELLS
COLOR
BASED
METHODS
FROM
VISUAL
CORTEX
WHICH
ARE
MULTISCALE
GABOR
AND
EDGEL
DETECTORS
FOL
LOWED
BY
LOCAL
MAXIMA
SELECTION
METHODS
COLOR
BASED
METHODS
COLOR
PROVIDES
ADDITIONAL
INFORMATION
WHICH
CAN
BE
USED
IN
THE
PROCESS
OF
FEATURE
EXTRACTION
SEVERAL
BIOLOGICALLY
PLAUSIBLE
METHODS
REVIEWED
IN
THE
PREVIOUS
SECTION
USE
COLOR
FOR
BUILDING
SALIENCY
MAPS
GIVEN
THE
HIGH
PERFORMANCE
OF
HARRIS
CORNERS
A
STRAIGHTFORWARD
EXTENSION
OF
THE
SECOND
MOMENT
MATRIX
TO
RGB
COLOR
SPACE
WAS
INTRO
DUCED
IN
INCORPORATING
COLOR
INFORMATION
IN
THE
HARRIS
CORNER
EXTRACTION
PROCESS
SALIENT
POINT
DETECTION
BASED
ON
COLOR
DISTINCTIVENESS
HAS
BEEN
PRO
POSED
IN
SALIENT
POINTS
ARE
THE
MAXIMA
OF
THE
SALIENCY
MAP
WHICH
REPRESENTS
DISTINCTIVENESS
OF
COLOR
DERIVATIVES
IN
A
POINT
NEIGHBORHOOD
IN
RELATED
WORK
THEY
ARGUE
THAT
THE
DISTINCTIVENESS
OF
COLOR
BASED
SALIENT
POINTS
IS
MUCH
HIGHER
THAN
FOR
THE
INTENSITY
ONES
COLOR
RATIOS
BETWEEN
NEIGHBORING
PIXELS
ARE
USED
TO
OBTAIN
DERIVATIVES
INDEPENDENT
OF
ILLUMINATION
WHICH
RESULTS
IN
COLOR
INTEREST
POINTS
THAT
ARE
MORE
ROBUST
TO
ILLUMINATION
CHANGES
MOST
OF
THE
PROPOSED
APPROACHES
BASED
ON
COLOR
ARE
SIMPLE
EXTEN
SIONS
OF
METHODS
BASED
ON
THE
INTENSITY
CHANGE
COLOR
GRADIENTS
ARE
USUALLY
USED
TO
ENHANCE
OR
TO
VALIDATE
THE
INTENSITY
CHANGE
SO
AS
TO
INCREASE
THE
STABILITY
OF
THE
FEATURE
DETECTORS
BUT
THE
PIXEL
INTENSITIES
REMAIN
THE
MAIN
SOURCE
OF
INFORMATION
FOR
FEATURE
DETECTION
MODEL
BASED
METHODS
THERE
HAVE
BEEN
A
FEW
ATTEMPTS
TO
DO
AN
ANALYTICAL
STUDY
OF
CORNER
DETECTION
BY
GIVING
A
FORMAL
REPRESENTATION
OF
CORNER
POINTS
IN
AN
IMAGE
BASED
ON
DIFFERENTIAL
GEOMETRY
TECHNIQUES
OR
CONTOUR
CURVATURE
FOR
INSTANCE
IT
WAS
FOUND
THAT
A
GRAY
LEVEL
CORNER
POINT
CAN
BE
FOUND
AS
THE
POINT
OF
MAXIMAL
PLANAR
CURVATURE
ON
THE
LINE
OF
THE
STEEPEST
GRAY
LEVEL
SLOPE
AN
ANALYTICAL
EXPRESSION
FOR
AN
OPTIMAL
FUNCTION
WHOSE
CONVOLUTION
WITH
AN
IMAGE
HAS
SIGNIFICANT
VALUES
AT
CORNER
POINTS
WAS
INVESTIGATED
IN
THE
METHODS
PRESENTED
IN
ASSUME
THAT
A
CORNER
RESEM
BLES
A
BLURRED
WEDGE
AND
FINDS
THE
CHARACTERISTICS
OF
THE
WEDGE
THE
AMPLITUDE
ANGLE
AND
BLUR
BY
FITTING
IT
TO
THE
LOCAL
IMAGE
SEVERAL
MOD
ELS
OF
JUNCTIONS
OF
MULTIPLE
EDGES
WERE
USED
IN
THE
ASSUMPTION
IS
THAT
THE
JUNCTIONS
ARE
FORMED
BY
HOMOGENEOUS
REGIONS
PARAMETERIZED
MASKS
ARE
USED
TO
FIT
THE
INTENSITY
STRUCTURE
INCLUDING
POSITION
ORIENTA
TION
INTENSITY
BLURRING
AND
EDGES
THE
RESIDUAL
IS
THEN
MINIMIZED
DUR
ING
THE
DETECTION
THE
ACCURACY
IS
HIGH
PROVIDED
A
GOOD
INITIALIZATION
OF
THE
PARAMETERS
THE
EFFICIENCY
OF
THE
APPROACH
IN
WAS
IMPROVED
IN
BY
USING
A
DIFFERENT
BLURRING
FUNCTION
AND
A
METHOD
TO
INITIALIZE
THE
PARAMETERS
FITTING
A
CORNER
MODEL
TO
IMAGE
DATA
WAS
ALSO
CONSID
ERED
IN
FOR
EACH
POSSIBLE
INTERSECTION
OF
LINES
A
TEMPLATE
WAS
CONSTRUCTED
BASED
ON
THE
ANGLE
ORIENTATION
AND
SCALE
OF
THE
HYPOTHE
SIZED
CORNER
THE
TEMPLATE
WAS
THEN
MATCHED
TO
THE
IMAGE
IN
A
SMALL
NEIGHBORHOOD
OF
THE
INTEREST
POINT
TO
VERIFY
THE
MODEL
A
TEMPLATE
BASED
METHOD
FOR
LOCATING
THE
SADDLE
POINTS
WAS
ALSO
DESCRIBED
IN
WHERE
THE
CORNER
POINTS
CORRESPOND
TO
THE
INTERSECTIONS
OF
SADDLE
RIDGE
AND
SADDLE
VALLEY
STRUCTURES
A
SET
OF
FUZZY
PATTERNS
OF
CONTOUR
POINTS
WERE
ESTABLISHED
IN
AND
THE
CORNER
DETECTION
WAS
CHARACTERIZED
AS
A
FUZZY
CLASSIFICATION
PROBLEM
OF
THE
PATTERNS
OTHER
MODEL
BASED
METHODS
AIMED
AT
IMPROVING
THE
DETECTION
ACCURACY
OF
THE
HESSIAN
BASED
CORNER
DETECTOR
WERE
PROPOSED
IN
TO
THIS
END
THE
RESPONSES
OF
THE
CORNER
DETECTOR
ON
A
THE
ORETICAL
MODEL
OVER
SCALE
SPACE
WERE
ANALYZED
IT
WAS
OBSERVED
THAT
THE
OPERATOR
RESPONSES
AT
DIFFERENT
SCALES
MOVE
ALONG
THE
BISECTOR
LINE
IT
IS
WORTH
TO
NOTE
THAT
THIS
OBSERVATION
IS
ALSO
VALID
FOR
THE
POP
ULAR
HARRIS
CORNER
DETECTOR
THE
EXACT
POSITION
OF
THE
CORNER
WAS
THEN
COMPUTED
FROM
TWO
RESPONSES
INDICATING
THE
BISECTOR
AND
ITS
INTERSECTION
WITH
THE
ZERO
CROSSING
OF
THE
LAPLACIAN
RESPONSE
AN
AFFINE
TRANSFORMATION
WAS
ALSO
USED
TO
FIT
A
MODEL
OF
A
CORNER
TO
AN
IMAGE
A
DIFFERENT
MODEL
BASED
APPROACH
IS
PROPOSED
IN
FOR
EACH
TYPE
OF
FEATURE
A
PARAMETRIC
MODEL
IS
DEVELOPED
TO
CHARACTERIZE
THE
LOCAL
INTENSITY
IN
AN
IMAGE
PROJECTIONS
OF
INTENSITY
PROFILE
ONTO
A
SET
OF
ORTHOGONAL
ZERNIKE
MOMENT
GENERATING
POLYNOMIALS
ARE
USED
TO
ESTI
MATE
MODEL
PARAMETERS
AND
GENERATE
THE
FEATURE
MAP
AN
INTERESTING
TECHNIQUE
IS
TO
FIND
CORNERS
BY
FITTING
A
PARAMETERIZED
MODEL
WITH
THE
GENERALIZED
HOUGH
TRANSFORM
IN
IMAGES
WITH
EXTRACTED
EDGES
TWO
LINES
APPEAR
IN
A
PARAMETER
SPACE
FOR
EACH
CORNER
AND
THE
PEAK
OCCURS
AT
THE
CROSSOVER
REAL
CORNER
MODELS
IN
THE
FORM
OF
TEMPLATES
WERE
CONSIDERED
IN
A
SIMILARITY
MEASURE
AND
SEVERAL
ALTERNATIVE
MATCHING
SCHEMES
WERE
APPLIED
DETECTION
AND
LOCALIZATION
ACCURACY
WAS
IMPROVED
BY
MERGING
THE
OUTPUT
OF
THE
DIFFERENT
MATCHING
TECHNIQUES
IN
GENERAL
ONLY
RELATIVELY
SIMPLE
FEATURE
MODELS
WERE
CONSIDERED
IN
THE
ABOVE
METHODS
AND
THE
GENERALIZATION
TO
IMAGES
OTHER
THAN
POLYG
ONAL
IS
NOT
OBVIOUS
THE
COMPLEXITY
IS
ALSO
A
MAJOR
DRAWBACK
IN
SUCH
APPROACHES
TOWARD
VIEWPOINT
INVARIANT
METHODS
MOST
OF
THE
DETECTORS
DESCRIBED
SO
FAR
EXTRACT
FEATURES
AT
A
SINGLE
SCALE
DETERMINED
BY
THE
INTERNAL
PARAMETERS
OF
THE
DETECTOR
AT
THE
END
OF
THE
AS
LOCAL
FEATURES
WERE
MORE
AND
MORE
USED
IN
THE
CONTEXT
OF
WIDE
BASELINE
MATCHING
AND
OBJECT
RECOGNITION
THERE
WAS
A
GROWING
NEED
FOR
FEATURES
THAT
COULD
COPE
WITH
SCALE
CHANGES
OR
EVEN
MORE
GENERAL
VIEWPOINT
CHANGES
MULTI
SCALE
METHODS
MOST
OF
THE
DETECTORS
DESCRIBED
SO
FAR
EXTRACT
FEATURES
AT
A
SINGLE
SCALE
DETERMINED
BY
THE
INTERNAL
PARAMETERS
OF
THE
DETECTOR
TO
DEAL
WITH
SCALE
CHANGES
A
STRAIGHTFORWARD
APPROACH
CONSISTS
OF
EXTRACTING
POINTS
OVER
A
RANGE
OF
SCALES
AND
USING
ALL
THESE
POINTS
TOGETHER
TO
REPRE
SENT
THE
IMAGE
THIS
IS
REFERRED
TO
AS
A
MULTI
SCALE
OR
MULTI
RESOLUTION
APPROACH
IN
A
SCALE
ADAPTED
VERSION
OF
THE
HARRIS
OPERATOR
WAS
PROPOSED
INTEREST
POINTS
ARE
DETECTED
AT
THE
LOCAL
MAXIMA
OF
THE
HARRIS
FUNCTION
APPLIED
AT
SEVERAL
SCALES
THANKS
TO
THE
USE
OF
NORMALIZED
DERIVATIVES
A
COMPARABLE
STRENGTH
OF
THE
CORNERNESS
MEASURE
IS
OBTAINED
FOR
POINTS
DETECTED
AT
DIFFERENT
SCALES
SUCH
THAT
A
SINGLE
THRESHOLD
CAN
BE
USED
TO
REJECT
LESS
SIGNIFICANT
CORNERS
OVER
ALL
SCALES
THIS
SCALE
ADAPTED
DETECTOR
SIGNIFICANTLY
IMPROVES
THE
REPEATABILITY
OF
INTEREST
POINTS
UNDER
SCALE
CHANGES
ON
THE
OTHER
HAND
WHEN
PRIOR
KNOWLEDGE
ON
THE
SCALE
CHANGE
BETWEEN
TWO
IMAGES
IS
GIVEN
THE
DETECTOR
CAN
BE
ADAPTED
SO
AS
TO
EXTRACT
INTEREST
POINTS
ONLY
AT
THE
SELECTED
SCALES
THIS
YIELDS
A
SET
OF
POINTS
FOR
WHICH
THE
RESPECTIVE
LOCALIZATION
AND
SCALE
PERFECTLY
REFLECT
THE
REAL
SCALE
CHANGE
BETWEEN
THE
IMAGES
IN
GENERAL
MULTI
SCALE
APPROACHES
SUFFER
FROM
THE
SAME
PROBLEMS
AS
DENSE
SAMPLING
OF
FEATURES
CF
SECTION
THEY
CANNOT
COPE
WELL
WITH
THE
CASE
WHERE
A
LOCAL
IMAGE
STRUCTURE
IS
PRESENT
OVER
A
RANGE
OF
SCALES
WHICH
RESULTS
IN
MULTIPLE
INTEREST
POINTS
BEING
DETECTED
AT
EACH
SCALE
WITHIN
THIS
RANGE
AS
A
CONSEQUENCE
THERE
ARE
MANY
POINTS
WHICH
REPRESENT
THE
SAME
STRUCTURE
BUT
WITH
SLIGHTLY
DIFFERENT
LOCALIZA
TION
AND
SCALE
THE
HIGH
NUMBER
OF
POINTS
INCREASES
THE
AMBIGUITY
AND
THE
COMPUTATIONAL
COMPLEXITY
OF
MATCHING
AND
RECOGNITION
THEREFORE
EFFICIENT
METHODS
FOR
SELECTING
ACCURATE
CORRESPONDENCES
AND
VERIFYING
THE
RESULTS
ARE
NECESSARY
AT
FURTHER
STEPS
OF
THE
ALGORITHMS
IN
CONTRAST
TO
STRUCTURE
FROM
MOTION
APPLICATIONS
THIS
IS
LESS
OF
AN
ISSUE
IN
THE
CONTEXT
OF
RECOGNITION
WHERE
A
SINGLE
POINT
CAN
HAVE
MULTIPLE
CORRECT
MATCHES
SCALE
INVARIANT
DETECTORS
TO
OVERCOME
THE
PROBLEM
OF
MANY
OVERLAPPING
DETECTIONS
TYPICAL
OF
MULTISCALE
APPROACHES
SCALE
INVARIANT
METHODS
HAVE
BEEN
INTRO
DUCED
THESE
AUTOMATICALLY
DETERMINE
BOTH
THE
LOCATION
AND
SCALE
OF
THE
LOCAL
FEATURES
FEATURES
ARE
TYPICALLY
CIRCULAR
REGIONS
IN
THAT
CASE
MANY
EXISTING
METHODS
SEARCH
FOR
MAXIMA
IN
THE
REPRESENTATION
OF
AN
IMAGE
X
Y
AND
SCALE
THIS
IDEA
FOR
DETECTING
LOCAL
FEATURES
IN
SCALE
SPACE
WAS
INTRODUCED
IN
THE
EARLY
THE
PYRAMID
REPRE
SENTATION
WAS
COMPUTED
WITH
LOW
PASS
FILTERS
A
FEATURE
POINT
IS
DETECTED
IF
IT
IS
AT
A
LOCAL
MAXIMUM
OF
A
SURROUNDING
CUBE
AND
IF
ITS
ABSO
LUTE
VALUE
IS
HIGHER
THAN
A
CERTAIN
THRESHOLD
SINCE
THEN
MANY
METHODS
FOR
SELECTING
POINTS
IN
SCALE
SPACE
HAVE
BEEN
PROPOSED
THE
EXISTING
APPROACHES
MAINLY
DIFFER
IN
THE
DIFFERENTIAL
EXPRESSION
USED
TO
BUILD
THE
SCALE
SPACE
REPRESENTATION
A
NORMALIZED
LOG
FUNCTION
WAS
APPLIED
IN
TO
BUILD
A
SCALE
SPACE
REPRESENTATION
AND
SEARCH
FOR
MAXIMA
THE
SCALE
SPACE
REPRESENTATION
IS
CONSTRUCTED
BY
SMOOTHING
THE
HIGH
RESOLUTION
IMAGE
WITH
DERIVATIVES
OF
GAUSSIAN
KERNELS
OF
INCREASING
SIZE
AUTOMATIC
SCALE
SELECTION
CF
SECTION
IS
PERFORMED
BY
SELECTING
LOCAL
MAXIMA
IN
SCALE
SPACE
THE
LOG
OPERATOR
IS
CIRCULARLY
SYMMETRIC
IT
IS
THEREFORE
NATURALLY
INVARIANT
TO
ROTATION
IT
IS
ALSO
WELL
ADAPTED
FOR
DETECTING
BLOB
LIKE
STRUCTURES
THE
EXPERIMENTAL
EVALUATION
IN
SHOWS
THIS
FUNCTION
IS
WELL
SUITED
FOR
AUTOMATIC
SCALE
SELECTION
THE
SCALE
INVARIANCE
OF
INTER
EST
POINT
DETECTORS
WITH
AUTOMATIC
SCALE
SELECTION
HAS
ALSO
BEEN
EXPLORED
IN
CORNER
DETECTION
AND
BLOB
DETECTION
WITH
AUTOMATIC
SCALE
SELEC
TION
WERE
ALSO
PROPOSED
IN
A
COMBINED
FRAMEWORK
IN
FOR
FEATURE
TRACKING
WITH
ADAPTATION
TO
SPATIAL
AND
TEMPORAL
SIZE
VARIATIONS
THE
INTEREST
POINT
CRITERION
THAT
IS
BEING
OPTIMIZED
FOR
LOCALIZATION
NEED
NOT
BE
THE
SAME
AS
THE
ONE
USED
FOR
OPTIMIZING
THE
SCALE
IN
A
SCALE
INVARIANT
CORNER
DETECTOR
COINED
HARRIS
LAPLACE
AND
A
SCALE
INVARIANT
BLOB
DETECTOR
COINED
HESSIAN
LAPLACE
WERE
INTRODUCED
IN
THESE
METH
ODS
POSITION
AND
SCALE
ARE
ITERATIVELY
UPDATED
UNTIL
CONVERGENCE
MORE
DETAILS
CAN
BE
FOUND
IN
SECTIONS
AND
AN
EFFICIENT
ALGORITHM
FOR
OBJECT
RECOGNITION
BASED
ON
LOCAL
EXTREMA
IN
THE
SCALE
SPACE
PYRAMID
BUILT
WITH
DOG
FILTERS
WAS
INTRO
DUCED
IN
THE
LOCAL
EXTREMA
IN
THE
PYRAMID
REPRESENTATION
DETERMINE
THE
LOCALIZATION
AND
THE
SCALE
OF
INTEREST
POINTS
THIS
METHOD
IS
DISCUSSED
FURTHER
IN
SECTION
AFFINE
INVARIANT
METHODS
AN
AFFINE
INVARIANT
DETECTOR
CAN
BE
SEEN
AS
A
GENERALIZATION
OF
THE
SCALE
INVARIANT
ONES
TO
NON
UNIFORM
SCALING
AND
SKEW
I
E
WITH
A
DIFFERENT
SCALING
FACTOR
IN
TWO
ORTHOGONAL
DIRECTIONS
AND
WITHOUT
PRE
SERVING
ANGLES
THE
NON
UNIFORM
SCALING
AFFECTS
NOT
ONLY
THE
LOCALIZA
TION
AND
THE
SCALE
BUT
ALSO
THE
SHAPE
OF
CHARACTERISTIC
LOCAL
STRUCTURES
THEREFORE
SCALE
INVARIANT
DETECTORS
FAIL
IN
THE
CASE
OF
SIGNIFICANT
AFFINE
TRANSFORMATIONS
AFFINE
INVARIANT
FEATURE
DETECTION
MATCHING
AND
RECOGNITION
HAVE
BEEN
ADDRESSED
FREQUENTLY
IN
THE
PAST
HERE
WE
FOCUS
ON
THE
METHODS
WHICH
DEAL
WITH
INVARIANT
INTEREST
POINT
DETECTION
ONE
CATEGORY
OF
APPROACHES
WAS
CONCERNED
WITH
THE
LOCALIZATION
ACCURACY
UNDER
AFFINE
AND
PERSPECTIVE
TRANSFORMATIONS
AN
AFFINE
INVARI
ANT
ALGORITHM
FOR
CORNER
LOCALIZATION
WAS
PROPOSED
IN
WHICH
BUILDS
ON
THE
OBSERVATIONS
MADE
IN
AFFINE
MORPHOLOGICAL
MULTI
SCALE
ANAL
YSIS
IS
APPLIED
TO
EXTRACT
CORNERS
THE
EVOLUTION
OF
A
CORNER
IS
GIVEN
BY
A
LINEAR
FUNCTION
FORMED
BY
THE
SCALE
AND
DISTANCE
OF
THE
DETECTED
POINTS
FROM
THE
REAL
CORNER
THE
LOCATION
AND
ORIENTATION
OF
THE
CORNER
IS
COM
PUTED
BASED
ON
THE
ASSUMPTION
THAT
THE
MULTISCALE
POINTS
MOVE
ALONG
THE
BISECTOR
LINE
AND
THE
ANGLE
INDICATES
THE
TRUE
LOCATION
HOWEVER
IN
NATURAL
SCENES
A
CORNER
CAN
TAKE
ANY
FORM
OF
A
BI
DIRECTIONAL
SIGNAL
CHANGE
AND
IN
PRACTICE
THE
EVOLUTION
OF
A
POINT
RARELY
FOLLOWS
THE
BISEC
TOR
THE
APPLICABILITY
OF
THE
METHOD
IS
THEREFORE
LIMITED
TO
A
POLYGONAL
LIKE
WORLD
OTHER
APPROACHES
WERE
CONCERNED
WITH
SIMULTANEOUS
DETECTION
OF
LOCATION
SIZE
AND
AFFINE
SHAPE
OF
LOCAL
STRUCTURES
THE
METHOD
INTRO
DUCED
IN
COINED
EBR
EDGE
BASED
REGIONS
STARTS
FROM
HARRIS
CORNERS
AND
NEARBY
INTERSECTING
EDGES
TWO
POINTS
MOVING
ALONG
THE
EDGES
TOGETHER
WITH
THE
HARRIS
POINT
DETERMINE
A
PARALLELOGRAM
THE
POINTS
STOP
AT
POSITIONS
WHERE
SOME
PHOTOMETRIC
QUANTITIES
OF
THE
TEX
TURE
COVERED
BY
THE
PARALLELOGRAM
REACH
AN
EXTREMUM
THE
METHOD
CAN
BE
CATEGORIZED
AS
A
MODEL
BASED
APPROACH
AS
IT
LOOKS
FOR
A
SPECIFIC
STRUCTURE
IN
IMAGES
ALBEIT
NOT
AS
STRICT
AS
MOST
METHODS
DESCRIBED
IN
SECTION
MORE
DETAILS
CAN
BE
FOUND
IN
SECTION
A
SIMILAR
SCHEME
HAS
BEEN
EXPLORED
IN
AN
INTENSITY
BASED
METHOD
IBR
INTENSITY
BASED
REGIONS
WAS
ALSO
PROPOSED
IN
IT
STARTS
WITH
THE
EXTRACTION
OF
LOCAL
INTEN
SITY
EXTREMA
THE
INTENSITY
PROFILES
ALONG
RAYS
EMANATING
FROM
A
LOCAL
EXTREMUM
ARE
INVESTIGATED
A
MARKER
IS
PLACED
ON
EACH
RAY
IN
THE
PLACE
WHERE
THE
INTENSITY
PROFILE
SIGNIFICANTLY
CHANGES
FINALLY
AN
ELLIPSE
IS
FITTED
TO
THE
REGION
DETERMINED
BY
THE
MARKERS
THIS
METHOD
IS
FURTHER
DISCUSSED
IN
SECTION
SOMEWHAT
SIMILAR
IN
SPIRIT
ARE
THE
MAXIMALLY
STABLE
EXTREMAL
REGIONS
OR
MSER
PROPOSED
IN
AND
DESCRIBED
IN
THE
NEXT
SECTION
A
METHOD
TO
FIND
BLOB
LIKE
AFFINE
INVARIANT
FEATURES
USING
AN
ITER
ATIVE
SCHEME
WAS
INTRODUCED
IN
IN
THE
CONTEXT
OF
SHAPE
FROM
TEXTURE
THIS
METHOD
BASED
ON
THE
AFFINE
INVARIANCE
OF
SHAPE
ADAPTED
FIXED
POINTS
WAS
ALSO
USED
FOR
ESTIMATING
SURFACE
ORIENTATION
FROM
BINOC
ULAR
DATA
SHAPE
FROM
DISPARITY
GRADIENTS
THE
ALGORITHM
EXPLORES
THE
PROPERTIES
OF
THE
SECOND
MOMENT
MATRIX
AND
ITERATIVELY
ESTIMATES
THE
AFFINE
DEFORMATION
OF
LOCAL
PATTERNS
IT
EFFECTIVELY
ESTIMATES
THE
TRANSFORMATION
THAT
WOULD
PROJECT
THE
PATCH
TO
A
FRAME
IN
WHICH
THE
EIGENVALUES
OF
THE
SECOND
MOMENT
MATRIX
ARE
EQUAL
THIS
WORK
PROVIDED
A
THEORETICAL
BACKGROUND
FOR
SEVERAL
OTHER
AFFINE
INVARIANT
DETECTORS
IT
WAS
COMBINED
WITH
THE
HARRIS
CORNER
DETECTOR
AND
USED
IN
THE
CONTEXT
OF
MATCHING
IN
HAND
TRACKING
IN
FINGERPRINT
RECOG
NITION
AND
FOR
AFFINE
RECTIFICATION
OF
TEXTURED
REGIONS
IN
IN
INTEREST
POINTS
ARE
EXTRACTED
AT
SEVERAL
SCALES
USING
THE
HARRIS
DETECTOR
AND
THEN
THE
SHAPE
OF
THE
REGIONS
IS
ADAPTED
TO
THE
LOCAL
IMAGE
STRUCTURE
USING
THE
ITERATIVE
PROCEDURE
FROM
THIS
ALLOWS
TO
EXTRACT
AFFINE
INVARIANT
DESCRIPTORS
FOR
A
GIVEN
FIXED
SCALE
AND
LOCA
TION
THAT
IS
THE
SCALE
AND
THE
LOCATION
OF
THE
POINTS
ARE
NOT
EXTRACTED
IN
AN
AFFINE
INVARIANT
WAY
FURTHERMORE
THE
MULTI
SCALE
HAR
RIS
DETECTOR
EXTRACTS
MANY
POINTS
WHICH
ARE
REPEATED
AT
THE
NEIGHBOR
ING
SCALE
LEVELS
THIS
INCREASES
THE
PROBABILITY
OF
A
MISMATCH
AND
THE
COMPLEXITY
THE
HARRIS
LAPLACE
DETECTOR
INTRODUCED
IN
WAS
EXTENDED
IN
BY
AFFINE
NORMALIZATION
WITH
THE
ALGORITHM
PROPOSED
IN
THIS
DETECTOR
SUFFERS
FROM
THE
SAME
DRAWBACKS
AS
THE
INITIAL
LOCATION
AND
SCALE
OF
POINTS
ARE
NOT
EXTRACTED
IN
AN
AFFINE
INVARIANT
WAY
ALTHOUGH
THE
UNIFORM
SCALE
CHANGES
BETWEEN
THE
VIEWS
ARE
HANDLED
BY
THE
SCALE
INVARIANT
HARRIS
LAPLACE
DETECTOR
BEYOND
AFFINE
TRANSFORMATIONS
A
SCHEME
THAT
GOES
EVEN
BEYOND
AFFINE
TRANSFORMATIONS
AND
IS
INVARIANT
TO
PROJECTIVE
TRANSFORMATIONS
WAS
INTRODUCED
IN
HOWEVER
ON
A
LOCAL
SCALE
THE
PERSPECTIVE
EFFECT
IS
USUALLY
NEGLECTABLE
MORE
DAMAGING
IS
THE
EFFECT
OF
NON
PLANARITIES
OR
NON
RIGID
DEFORMATIONS
THIS
IS
WHY
A
THEORETICAL
FRAMEWORK
TO
EXTEND
THE
USE
OF
LOCAL
FEATURES
TO
NON
PLANAR
SURFACES
HAS
BEEN
PROPOSED
IN
BASED
ON
THE
DEFINITION
OF
EQUIVALENCE
CLASSES
HOWEVER
IN
PRAC
TICE
THEY
HAVE
ONLY
SHOWN
RESULTS
ON
STRAIGHT
CORNERS
SIMULTANEOUSLY
AN
APPROACH
INVARIANT
TO
GENERAL
DEFORMATIONS
WAS
DEVELOPED
IN
BY
EMBEDDING
AN
IMAGE
AS
A
SURFACE
IN
SPACE
AND
EXPLOITING
GEODESIC
DISTANCES
SEGMENTATION
BASED
METHODS
SEGMENTATION
TECHNIQUES
HAVE
ALSO
BEEN
EMPLOYED
IN
THE
CONTEXT
OF
FEATURE
EXTRACTION
THESE
METHODS
WERE
EITHER
APPLIED
TO
FIND
HOMOGE
NEOUS
REGIONS
TO
LOCALIZE
JUNCTIONS
ON
THEIR
BOUNDARIES
OR
TO
DIRECTLY
USE
THESE
REGIONS
AS
LOCAL
FEATURES
FOR
THE
GENERIC
FEATURE
EXTRAC
TION
PROBLEM
MOSTLY
BOTTOM
UP
SEGMENTATION
BASED
ON
LOW
LEVEL
PIXEL
GROUPING
WAS
CONSIDERED
ALTHOUGH
IN
SOME
SPECIFIC
TASKS
TOP
DOWN
METHODS
CAN
ALSO
BE
APPLIED
ALTHOUGH
SIGNIFICANT
PROGRESS
HAS
BEEN
MADE
IN
THE
ANALYSIS
AND
FORMALIZATION
OF
THE
SEGMENTATION
PROBLEM
IT
REMAINS
AN
UNSOLVED
PROBLEM
IN
THE
GENERAL
CASE
OPTIMAL
SEGMEN
TATION
IS
INTRACTABLE
IN
GENERAL
DUE
TO
THE
LARGE
SEARCH
SPACE
OF
POSSI
BLE
FEATURE
POINT
GROUPS
IN
PARTICULAR
IN
ALGORITHMS
BASED
ON
MULTIPLE
IMAGE
CUES
MOREOVER
A
MULTITUDE
OF
DEFINITIONS
OF
OPTIMAL
SEGMEN
TATION
EVEN
FOR
THE
SAME
IMAGE
MAKES
IT
DIFFICULT
TO
SOLVE
NONETHE
LESS
SEVERAL
SYSTEMS
USING
SEGMENTATION
BASED
INTEREST
REGIONS
HAVE
BEEN
DEVELOPED
ESPECIALLY
IN
THE
CONTEXT
OF
RETRIEVAL
MATCHING
AND
RECOGNITION
IN
EARLY
YEARS
OF
COMPUTER
VISION
POLYGONAL
APPROXIMATIONS
OF
IMAGES
WERE
POPULAR
IN
SCENE
ANALYSIS
AND
MEDICAL
IMAGE
ANALYSIS
THESE
ALGORITHMS
OFTEN
INVOLVED
EDGE
DETECTION
AND
SUB
SEQUENT
EDGE
FOLLOWING
FOR
REGION
IDENTIFICATION
IN
THE
VERTICES
OF
A
PICTURE
ARE
DEFINED
AS
THOSE
POINTS
WHICH
ARE
COMMON
IN
THREE
OR
MORE
SEGMENTED
REGIONS
IT
CAN
BE
SEEN
AS
ONE
OF
THE
FIRST
ATTEMPTS
TO
EXTRACT
INTEREST
POINTS
USING
SEGMENTATION
SIMPLE
SEGMENTATION
OF
PATCHES
INTO
TWO
REGIONS
IS
USED
IN
AND
THE
REGIONS
ARE
COMPARED
TO
FIND
CORNERS
UNFORTUNATELY
THE
TWO
REGION
ASSUMPTION
MAKES
THE
USEFULNESS
OF
THE
METHOD
LIMITED
ANOTHER
SET
OF
APPROACHES
REPRESENT
REAL
IMAGES
THROUGH
SEG
MENTATION
WELL
PERFORMING
IMAGE
SEGMENTATION
METHODS
ARE
MACHINE
LEARNING
BASED
METHODS
BASED
ON
GRAPH
CUTS
WHERE
GRAPHS
REPRESENT
CONNECTED
IMAGE
PIX
ELS
THESE
METHODS
ALLOW
TO
OBTAIN
SEGMENTATION
AT
THE
REQUIRED
LEVEL
OF
DETAIL
ALTHOUGH
SEMANTIC
SEGMENTATION
IS
NOT
RELIABLE
OVER
SEGMENTING
THE
IMAGE
CAN
PRODUCE
MANY
REGIONS
WHICH
FIT
TO
THE
OBJECTS
THIS
APPROACH
WAS
EXPLORED
IN
AND
IT
IS
PAR
TICULARLY
APPEALING
FOR
IMAGE
RETRIEVAL
PROBLEMS
WHERE
THE
GOAL
IS
TO
FIND
SIMILAR
IMAGES
VIA
REGIONS
WITH
SIMILAR
PROPERTIES
IN
THE
GOAL
IS
TO
CREATE
INTEREST
OPERATORS
THAT
FOCUS
ON
HOMOGENEOUS
REGIONS
AND
COMPUTE
LOCAL
IMAGE
DESCRIPTORS
FOR
THESE
REGIONS
THE
SEGMENTATION
IS
PERFORMED
ON
SEVERAL
FEATURE
SPACES
USING
KERNEL
BASED
OPTIMIZA
TION
METHODS
THE
REGIONS
CAN
BE
INDIVIDUALLY
DESCRIBED
AND
USED
FOR
RECOGNITION
BUT
THEIR
DISTINCTIVENESS
IS
LOW
THIS
DIRECTION
HAS
RECENTLY
GAINED
MORE
INTEREST
AND
SOME
APPROACHES
USE
BOTTOM
UP
SEGMENTATION
TO
EXTRACT
INTEREST
REGIONS
OR
SO
CALLED
SUPERPIXELS
SEE
ALSO
SECTION
IN
GENERAL
THE
DISADVANTAGES
OF
THIS
REPRESENTATION
ARE
THAT
THE
SEGMENTATION
RESULTS
ARE
STILL
UNSTABLE
AND
INEFFICIENT
FOR
PROCESSING
LARGE
AMOUNTS
OF
IMAGES
AN
APPROACH
WHICH
SUCCESSFULLY
DEALS
WITH
THESE
PROBLEMS
WAS
TAKEN
IN
MAXIMALLY
STABLE
EXTREMAL
REGIONS
MSER
ARE
EXTRACTED
WITH
A
WATERSHED
LIKE
SEGMENTATION
ALGORITHM
THE
METHOD
EXTRACTS
HOMOGENEOUS
INTENSITY
REGIONS
WHICH
ARE
STA
BLE
OVER
A
WIDE
RANGE
OF
THRESHOLDS
THE
REGIONS
ARE
THEN
REPLACED
BY
ELLIPSES
WITH
THE
SAME
SHAPE
MOMENTS
UP
TO
THE
SECOND
ORDER
RECENTLY
A
VARIANT
OF
THIS
METHOD
WAS
INTRODUCED
IN
WHICH
HANDLES
THE
PROBLEMS
WITH
BLURRED
REGION
BOUNDARIES
BY
USING
REGION
ISOPHOTES
IN
A
SENSE
THIS
METHOD
IS
ALSO
SIMILAR
TO
THE
IBR
METHOD
DESCRIBED
IN
SECTION
AS
VERY
SIMILAR
REGIONS
ARE
EXTRACTED
MORE
DETAILS
ON
MSER
CAN
BE
FOUND
IN
SECTION
THE
METHOD
WAS
EXTENDED
IN
WITH
TREE
LIKE
REPRESENTATION
OF
WATERSHED
EVOLUTION
IN
THE
IMAGE
MACHINE
LEARNING
BASED
METHODS
THE
PROGRESS
IN
THE
DOMAIN
OF
MACHINE
LEARNING
AND
THE
INCREASE
OF
AVAILABLE
COMPUTATIONAL
POWER
ALLOWED
LEARNING
TECHNIQUES
TO
ENTER
THE
FEATURE
EXTRACTION
DOMAIN
THE
IDEA
OF
LEARNING
THE
ATTRIBUTES
OF
LOCAL
FEATURES
FROM
TRAINING
EXAMPLES
AND
THEN
USING
THIS
INFORMATION
TO
EXTRACT
FEATURES
IN
OTHER
IMAGES
HAS
BEEN
AROUND
IN
THE
VISION
COMMU
NITY
FOR
SOME
TIME
BUT
ONLY
RECENTLY
IT
WAS
MORE
BROADLY
USED
IN
REAL
APPLICATIONS
THE
SUCCESS
OF
THESE
METHODS
IS
DUE
TO
THE
FACT
THAT
EFFI
CIENCY
PROVIDED
BY
CLASSIFIERS
BECAME
A
MORE
DESIRABLE
PROPERTY
THAN
ACCURACY
OF
DETECTION
IN
A
NEURAL
NETWORK
IS
TRAINED
TO
RECOGNIZE
CORNERS
WHERE
EDGES
MEET
AT
A
CERTAIN
DEGREE
NEAR
TO
THE
CENTER
OF
AN
IMAGE
PATCH
THIS
IS
APPLIED
TO
IMAGES
AFTER
EDGE
DETECTION
A
SIMILAR
IDEA
WAS
EXPLORED
IN
TO
IMPROVE
THE
STABILITY
OF
CURVATURE
MEASUREMENT
OF
DIGITAL
CURVES
DECISION
TREES
HAVE
ALSO
BEEN
USED
SUCCESSFULLY
IN
INTEREST
POINT
DETECTION
TASKS
THE
IDEA
OF
USING
INTENSITY
DIFFERENCES
BETWEEN
THE
CEN
TRAL
POINTS
AND
NEIGHBORING
POINTS
HAS
BEEN
ADOPTED
IN
THEY
CONSTRUCT
A
DECISION
TREE
TO
CLASSIFY
POINT
NEIGH
BORHOODS
INTO
CORNERS
THE
MAIN
CONCERN
IN
THEIR
WORK
IS
THE
EFFICIENCY
IN
TESTING
ONLY
A
FRACTION
OF
THE
MANY
POSSIBLE
DIFFERENCES
AND
THE
TREE
IS
TRAINED
TO
OPTIMIZE
THAT
THE
APPROACH
OF
WAS
ALSO
EXTENDED
WITH
LOG
FILTERS
TO
DETECT
MULTISCALE
POINTS
IN
THEY
USE
A
FEATURE
SELECTION
TECHNIQUE
BASED
ON
THE
REPEATABILITY
OF
INDIVIDUAL
INTEREST
POINTS
OVER
PERSPECTIVE
PROJECTED
IMAGES
A
HYBRID
METHODOLOGY
THAT
INTEGRATES
GENETIC
ALGORITHMS
AND
DECISION
TREE
LEARNING
IN
ORDER
TO
EXTRACT
DISCRIMINATORY
FEATURES
FOR
RECOGNIZING
COMPLEX
VISUAL
CONCEPTS
IS
DESCRIBED
IN
IN
INTER
EST
POINT
DETECTION
IS
POSED
AS
AN
OPTIMIZATION
PROBLEM
THEY
USE
A
GENETIC
PROGRAMMING
BASED
LEARNING
APPROACH
TO
CONSTRUCT
OPERATORS
FOR
EXTRACTING
FEATURES
THE
PROBLEM
OF
LEARNING
AN
INTEREST
POINT
OPER
ATOR
WAS
POSED
DIFFERENTLY
IN
WHERE
HUMAN
EYE
MOVEMENT
WAS
STUDIED
TO
FIND
THE
POINTS
OF
FIXATION
AND
TO
TRAIN
AN
SVM
CLASSIFIER
ONE
CAN
EASILY
GENERALIZE
THE
FEATURE
DETECTION
PROBLEM
TO
A
CLASSIFI
CATION
PROBLEM
AND
TRAIN
A
RECOGNITION
SYSTEM
ON
IMAGE
EXAMPLES
PRO
VIDED
BY
ONE
OR
A
COMBINATION
OF
THE
CLASSICAL
DETECTORS
ANY
MACHINE
LEARNING
APPROACH
CAN
BE
USED
FOR
THAT
HAAR
LIKE
FILTERS
IMPLEMENTED
WITH
INTEGRAL
IMAGES
TO
EFFICIENTLY
APPROXIMATE
MULTISCALE
DERIVATIVES
WERE
USED
IN
A
NATURAL
EXTENSION
WOULD
BE
TO
USE
THE
LEARNING
SCHEME
FROM
VIOLA
AND
JONES
SUCCESSFULLY
APPLIED
TO
FACE
DETEC
TION
TO
EFFICIENTLY
CLASSIFY
INTEREST
POINTS
THE
ACCURACY
OF
MACHINE
LEARNING
BASED
METHODS
IN
TERMS
OF
LOCAL
IZATION
SCALE
AND
SHAPE
ESTIMATION
IS
IN
GENERAL
LOWER
THAN
FOR
THE
GENERIC
DETECTORS
BUT
IN
THE
CONTEXT
OF
OBJECT
RECOGNITION
THE
EFFICIENCY
IS
USUALLY
MORE
BENEFICIAL
EVALUATIONS
GIVEN
THE
MULTITUDE
OF
INTEREST
POINT
APPROACHES
THE
NEED
FOR
INDEPEN
DENT
PERFORMANCE
EVALUATIONS
WAS
IDENTIFIED
EARLY
ON
AND
MANY
EXPER
IMENTAL
TESTS
HAVE
BEEN
PERFORMED
OVER
THE
LAST
THREE
DECADES
VARIOUS
EXPERIMENTAL
FRAMEWORKS
AND
CRITERIA
WERE
USED
ONE
OF
THE
FIRST
COM
PARISONS
OF
CORNER
DETECTION
TECHNIQUES
BASED
ON
CHAIN
CODED
CURVES
WAS
PRESENTED
IN
IN
THE
EARLY
PAPERS
VERY
OFTEN
ONLY
VISUAL
INSPEC
TION
WAS
DONE
OTHERS
PERFORMED
MORE
QUANTITATIVE
EVALUATIONS
PROVIDING
SCORES
FOR
INDIVIDUAL
IMAGES
OR
FOR
SMALL
TEST
DATA
CORNER
DETECTORS
WERE
OFTEN
TESTED
ON
ARTIFICIALLY
GENERATED
IMAGES
WITH
DIFFERENT
TYPES
OF
JUNCTIONS
WITH
VARYING
ANGLE
LENGTH
CONTRAST
NOISE
BLUR
ETC
DIFFERENT
AFFINE
PHOTOMETRIC
AND
GEOMETRIC
TRANSFORMATIONS
WERE
USED
TO
GENERATE
THE
TEST
DATA
AND
TO
EVALUATE
CORNER
DETECTORS
IN
THIS
APPROACH
SIMPLIFIES
THE
EVALUATION
PROCESS
BUT
CANNOT
MODEL
ALL
THE
NOISE
AND
DEFORMATIONS
WHICH
AFFECT
THE
DETECTOR
PERFORMANCE
IN
A
REAL
APPLICATION
SCENARIO
THUS
THE
PERFORMANCE
RESULTS
ARE
OFTEN
OVER
OPTIMISTIC
A
SOMEWHAT
DIFFERENT
APPROACH
IS
TAKEN
IN
THERE
PERFORMANCE
COMPARISON
IS
APPROACHED
AS
A
GENERAL
RECOGNITION
PROBLEM
CORNERS
ARE
MANUALLY
ANNOTATED
ON
AFFINE
TRANSFORMED
IMAGES
AND
MEASURES
LIKE
CONSISTENCY
AND
ACCURACY
SIMILAR
TO
DETECTION
RATE
AND
RECALL
ARE
USED
TO
EVALUATE
THE
DETECTORS
IN
SETS
OF
POINTS
ARE
EXTRACTED
FROM
POLYHEDRAL
OBJECTS
AND
PROJECTIVE
INVARIANTS
ARE
USED
TO
CALCULATE
A
MANIFOLD
OF
CONSTRAINTS
ON
THE
COORDINATES
OF
THE
CORNERS
THEY
ESTIMATE
THE
VARIANCE
OF
THE
DISTANCE
FROM
THE
POINT
COORDINATES
TO
THIS
MANIFOLD
INDEPENDENTLY
OF
CAMERA
PARAMETERS
AND
OBJECT
POSE
NONLINEAR
DIFFUSION
WAS
USED
TO
REMOVE
THE
NOISE
AND
THE
METHOD
FROM
PERFORMED
BETTER
THAN
THE
ONE
PROPOSED
IN
THE
IDEA
OF
USING
PLANAR
INVARIANTS
IS
ALSO
EXPLORED
IN
TO
EVALUATE
CORNER
DETECTORS
BASED
ON
EDGES
THEORETICAL
PROPERTIES
OF
FEATURES
AND
LOCALIZATION
ACCURACY
WERE
ALSO
TESTED
IN
BASED
ON
A
PARAMETRIC
L
CORNER
MODEL
TO
EVALUATE
LOCALIZATION
ACCURACY
ALSO
A
RANDOMIZED
GENERATOR
OF
CORNERS
HAS
BEEN
USED
TO
TEST
THE
LOCALIZATION
ERROR
STATE
OF
THE
ART
CURVE
BASED
DETECTORS
ARE
EVAL
UATED
IN
A
QUANTITATIVE
MEASURE
OF
THE
QUALITY
OF
THE
DETECTED
DOMINANT
POINTS
IS
DEFINED
AS
THE
POINTWISE
ERROR
BETWEEN
THE
DIGITAL
CURVE
AND
THE
POLYGON
APPROXIMATED
FROM
INTEREST
POINTS
THE
PERFOR
MANCE
OF
THE
PROPOSED
SCALE
ADAPTED
APPROACH
IS
REPORTED
BETTER
THAN
OF
THE
OTHER
METHODS
THE
REPEATABILITY
RATE
AND
INFORMATION
CONTENT
MEASURES
WERE
INTRO
DUCED
IN
THEY
CONSIDER
A
POINT
IN
AN
IMAGE
INTERESTING
IF
IT
HAS
TWO
MAIN
PROPERTIES
DISTINCTIVENESS
AND
INVARIANCE
THIS
MEANS
THAT
A
POINT
SHOULD
BE
DISTINGUISHABLE
FROM
ITS
IMMEDIATE
NEIGHBORS
MORE
OVER
THE
POSITION
AS
WELL
AS
THE
SELECTION
OF
THE
INTEREST
POINT
SHOULD
BE
INVARIANT
WITH
RESPECT
TO
THE
EXPECTED
GEOMETRIC
AND
RADIOMETRIC
DISTORTIONS
FROM
A
SET
OF
INVESTIGATED
DETECTORS
HARRIS
AND
A
CORNER
LATER
DESCRIBED
AS
SUSAN
PERFORM
BEST
SYSTEMATIC
EVALUATION
OF
SEVERAL
INTEREST
POINT
DETECTORS
BASED
ON
REPEATABILITY
AND
INFORMATION
CONTENT
MEASURED
BY
THE
ENTROPY
OF
THE
DESCRIPTORS
WAS
PERFORMED
IN
THE
EVALUATION
SHOWS
THAT
A
MOD
IFIED
HARRIS
DETECTOR
PROVIDES
THE
MOST
STABLE
RESULTS
ON
IMAGE
PAIRS
WITH
DIFFERENT
GEOMETRIC
TRANSFORMATIONS
THE
REPEATABILITY
RATE
AND
INFORMATION
CONTENT
IN
THE
CONTEXT
OF
IMAGE
RETRIEVAL
WERE
ALSO
EVAL
UATED
IN
TO
SHOW
THAT
A
WAVELET
BASED
SALIENT
POINT
EXTRACTION
ALGORITHM
OUTPERFORMS
THE
HARRIS
DETECTOR
CONSISTENCY
OF
THE
NUMBER
OF
CORNERS
AND
ACCURACY
CRITERIA
WERE
INTRODUCED
AS
EVALUATION
CRITERIA
IN
THIS
OVERCOMES
THE
PROBLEMS
WITH
THE
REPEATABILITY
CRITERION
OF
FAVORING
DETECTORS
PROVIDING
MORE
FEATURES
THE
INTRODUCED
CRITERION
INSTEAD
FAVORS
DETECTORS
WHICH
PRO
VIDE
SIMILAR
NUMBER
OF
POINTS
REGARDLESS
OF
THE
OBJECT
TRANSFORMATION
EVEN
THOUGH
THE
NUMBER
OF
DETAILS
IN
THE
IMAGE
CHANGES
WITH
SCALE
AND
RESOLUTION
SEVERAL
DETECTORS
ARE
COMPARED
WITH
THE
BEST
PERFORMANCE
REPORTED
FOR
A
MODIFIED
IMPLEMENTATION
OF
TRACKING
AND
THE
NUMBER
OF
FRAMES
OVER
WHICH
THE
CORNERS
ARE
DETECTED
DURING
TRACKING
WAS
USED
TO
COMPARE
DETECTORS
IN
SIMILARLY
BAE
ET
AL
USES
CORRELATION
AND
MATCHING
TO
FIND
REPEATED
CORNERS
BETWEEN
FRAMES
AND
COMPARE
THEIR
NUMBERS
TO
THE
REFERENCE
FRAME
EXTENSIVE
EVALUATION
OF
COMMONLY
USED
FEATURE
DETECTORS
AND
DESCRIPTORS
HAS
BEEN
PERFORMED
IN
THE
REPEATABILITY
ON
IMAGE
PAIRS
REPRESENTING
PLANAR
SCENES
RELATED
BY
VARIOUS
GEOMETRIC
TRANSFORMATIONS
WAS
COMPUTED
FOR
DIFFERENT
STATE
OF
THE
ART
SCALE
AND
AFFINE
INVARIANT
DETECTORS
THE
MSER
REGION
DETECTOR
BASED
ON
WATERSHED
SEGMENTATION
SHOWED
THE
HIGHEST
ACCURACY
AND
STABILITY
ON
VARIOUS
STRUCTURED
SCENES
THE
DATA
COLLECTED
BY
MIKOLAJCZYK
AND
TUYTELAARS
BECAME
A
STANDARD
BENCHMARK
FOR
EVALUATING
INTEREST
POINT
DETECTORS
AND
DESCRIPTORS
RECENTLY
THE
PERFORMANCE
OF
FEATURE
DETECTORS
AND
DESCRIPTORS
FROM
HAS
BEEN
INVESTIGATED
IN
IN
THE
CONTEXT
OF
MATCHING
OBJECT
FEATURES
ACROSS
VIEWPOINTS
AND
LIGHTING
CONDI
TIONS
A
METHOD
BASED
ON
INTERSECTING
EPIPOLAR
CONSTRAINTS
PROVIDES
GROUND
TRUTH
CORRESPONDENCES
AUTOMATICALLY
IN
THIS
EVALUATION
THE
AFFINE
INVARIANT
DETECTORS
INTRODUCED
IN
ARE
MOST
ROBUST
TO
VIEW
POINT
CHANGES
DOG
DETECTOR
FROM
WAS
REPORTED
THE
BEST
IN
A
SIMILAR
EVALUATION
BASED
ON
IMAGES
OF
NATURAL
SCENES
IN
FEATURE
DETECTORS
WERE
ALSO
EVALUATED
IN
THE
CONTEXT
OF
RECOGNITION
IN
USING
OBJECT
CATEGORY
TRAINING
DATA
WHERE
DIRECT
CORRE
SPONDENCE
CANNOT
BE
AUTOMATICALLY
VERIFIED
CLUSTERING
PROPERTIES
AND
COMPACTNESS
OF
FEATURE
CLUSTERS
WERE
MEASURED
IN
SOME
SPECIFIC
RECOGNITION
TASKS
LIKE
PEDESTRIAN
DETECTION
WERE
ALSO
USED
TO
COMPARE
THE
PERFORMANCE
OF
DIFFERENT
FEATURES
IN
SEE
VGG
RESEARCH
AFFINE
CORNER
DETECTORS
A
LARGE
NUMBER
OF
CORNER
DETECTOR
METHODS
HAVE
BEEN
PROPOSED
IN
THE
LITERATURE
TO
GUIDE
THE
READER
IN
FINDING
AN
APPROACH
SUITABLE
FOR
A
GIVEN
APPLICATION
REPRESENTATIVE
METHODS
HAVE
BEEN
SELECTED
BASED
ON
THE
UNDERLYING
EXTRACTION
TECHNIQUE
E
G
BASED
ON
IMAGE
DERIVATIVES
MORPHOLOGY
OR
GEOMETRY
AS
WELL
AS
BASED
ON
THE
LEVEL
OF
INVARIANCE
TRANSLATIONS
AND
ROTATIONS
SCALE
OR
AFFINE
INVARIANT
FOR
EACH
CATE
GORY
WE
DESCRIBE
THE
FEATURE
EXTRACTION
PROCESS
FOR
SOME
OF
THE
BEST
PERFORMING
AND
REPRESENTATIVE
METHODS
INTRODUCTION
IT
IS
IMPORTANT
TO
NOTE
THAT
THE
TERM
CORNER
AS
USED
HERE
HAS
A
SPECIFIC
MEANING
THE
DETECTED
POINTS
CORRESPOND
TO
POINTS
IN
THE
IMAGE
WITH
HIGH
CURVATURE
THESE
DO
NOT
NECESSARILY
CORRESPOND
TO
PROJEC
TIONS
OF
CORNERS
CORNERS
ARE
FOUND
AT
VARIOUS
TYPES
OF
JUNCTIONS
ON
HIGHLY
TEXTURED
SURFACES
AT
OCCLUSION
BOUNDARIES
ETC
FOR
MANY
PRAC
TICAL
APPLICATIONS
THIS
IS
SUFFICIENT
SINCE
THE
GOAL
IS
TO
HAVE
A
SET
OF
STABLE
AND
REPEATABLE
FEATURES
WHETHER
THESE
ARE
TRUE
CORNERS
OR
NOT
IS
CONSIDERED
IRRELEVANT
WE
BEGIN
THIS
SECTION
WITH
A
DERIVATIVES
BASED
APPROACH
THE
HARRIS
CORNER
DETECTOR
DESCRIBED
IN
SECTION
NEXT
WE
EXPLAIN
THE
BASIC
IDEAS
OF
THE
SUSAN
DETECTOR
SECTION
WHICH
IS
AN
EXAMPLE
OF
A
METHOD
BASED
ON
EFFICIENT
MORPHOLOGICAL
OPERATORS
WE
THEN
MOVE
ON
TO
DETECTORS
WITH
HIGHER
LEVELS
OF
INVARIANCE
STARTING
WITH
THE
SCALE
AND
AFFINE
INVARIANT
EXTENSIONS
OF
THE
HARRIS
DETECTOR
HARRIS
LAPLACE
AND
HARRIS
AFFINE
SECTION
THIS
IS
FOLLOWED
BY
A
DISCUSSION
OF
EDGE
BASED
REGIONS
IN
SECTION
FINALLY
WE
CONCLUDE
THE
SECTION
WITH
A
SHORT
DISCUSSION
SECTION
HARRIS
DETECTOR
THE
HARRIS
DETECTOR
PROPOSED
BY
HARRIS
AND
STEPHENS
IS
BASED
ON
THE
SECOND
MOMENT
MATRIX
ALSO
CALLED
THE
AUTO
CORRELATION
MATRIX
WHICH
IS
OFTEN
USED
FOR
FEATURE
DETECTION
AND
FOR
DESCRIBING
LOCAL
IMAGE
STRUCTURES
THIS
MATRIX
DESCRIBES
THE
GRADIENT
DISTRIBUTION
IN
A
LOCAL
NEIGHBORHOOD
OF
A
POINT
M
WITH
G
ΣI
X
ΣD
IX
X
ΣD
IY
X
ΣD
IX
X
ΣD
IY
X
ΣD
X
ΣD
IX
X
ΣD
XG
ΣD
I
X
G
Σ
E
THE
LOCAL
IMAGE
DERIVATIVES
ARE
COMPUTED
WITH
GAUSSIAN
KERNELS
OF
SCALE
ΣD
THE
DIFFERENTIATION
SCALE
THE
DERIVATIVES
ARE
THEN
AVERAGED
IN
THE
NEIGHBORHOOD
OF
THE
POINT
BY
SMOOTHING
WITH
A
GAUSSIAN
WIN
DOW
OF
SCALE
ΣI
THE
INTEGRATION
SCALE
THE
EIGENVALUES
OF
THIS
MATRIX
REPRESENT
THE
PRINCIPAL
SIGNAL
CHANGES
IN
TWO
ORTHOGONAL
DIRECTIONS
IN
A
NEIGHBORHOOD
AROUND
THE
POINT
DEFINED
BY
ΣI
BASED
ON
THIS
PROPERTY
CORNERS
CAN
BE
FOUND
AS
LOCATIONS
IN
THE
IMAGE
FOR
WHICH
THE
IMAGE
SIG
NAL
VARIES
SIGNIFICANTLY
IN
BOTH
DIRECTIONS
OR
IN
OTHER
WORDS
FOR
WHICH
BOTH
EIGENVALUES
ARE
LARGE
IN
PRACTICE
HARRIS
PROPOSED
TO
USE
THE
FOL
LOWING
MEASURE
FOR
CORNERNESS
WHICH
COMBINES
THE
TWO
EIGENVALUES
IN
A
SINGLE
MEASURE
AND
IS
COMPUTATIONALLY
LESS
EXPENSIVE
CORNERNESS
DET
M
Λ
TRACE
M
WITH
DET
M
THE
DETERMINANT
AND
TRACE
M
THE
TRACE
OF
THE
MATRIX
M
A
TYPICAL
VALUE
FOR
Λ
IS
SINCE
THE
DETERMINANT
OF
A
MATRIX
IS
EQUAL
TO
THE
PRODUCT
OF
ITS
EIGENVALUES
AND
THE
TRACE
CORRESPONDS
TO
THE
SUM
IT
IS
CLEAR
THAT
HIGH
VALUES
OF
THE
CORNERNESS
MEASURE
CORRE
SPOND
TO
BOTH
EIGENVALUES
BEING
LARGE
ADDING
THE
SECOND
TERM
WITH
THE
TRACE
REDUCES
THE
RESPONSE
OF
THE
OPERATOR
ON
STRONG
STRAIGHT
CON
TOURS
MOREOVER
COMPUTING
THIS
MEASURE
BASED
ON
THE
DETERMINANT
AND
THE
TRACE
IS
COMPUTATIONALLY
LESS
DEMANDING
THAN
ACTUALLY
COM
PUTING
THE
EIGENVALUES
THIS
SEEMS
LESS
RELEVANT
NOW
BUT
IT
WAS
IMPOR
TANT
BACK
IN
WHEN
THE
COMPUTATIONAL
RESOURCES
WERE
STILL
VERY
LIMITED
SUBSEQUENT
STAGES
OF
THE
CORNER
EXTRACTION
PROCESS
ARE
ILLUSTRATED
IN
FIGURE
GIVEN
THE
ORIGINAL
IMAGE
I
X
Y
UPPER
LEFT
THE
FIRST
STEP
CONSISTS
OF
COMPUTING
THE
FIRST
ORDER
DERIVATIVES
IX
AND
IY
LOWER
LEFT
NEXT
ONE
TAKES
THE
PRODUCT
OF
THESE
GRADIENT
IMAGES
LOWER
RIGHT
THEN
THE
IMAGES
ARE
SMOOTHED
WITH
A
GAUSSIAN
KERNEL
THESE
FIG
ILLUSTRATION
OF
THE
COMPONENTS
OF
THE
SECOND
MOMENT
MATRIX
AND
HARRIS
CORNERNESS
MEASURE
IMAGES
CONTAIN
THE
DIFFERENT
ELEMENTS
OF
THE
HESSIAN
MATRIX
WHICH
ARE
THEN
IN
A
FINAL
STEP
COMBINED
INTO
THE
CORNERNESS
MEASURE
FOLLOWING
EQUATION
UPPER
RIGHT
WHEN
USED
AS
AN
INTEREST
POINT
DETECTOR
LOCAL
MAXIMA
OF
THE
COR
NERNESS
FUNCTION
ARE
EXTRACTED
USING
NON
MAXIMUM
SUPPRESSION
SUCH
POINTS
ARE
TRANSLATION
AND
ROTATION
INVARIANT
MOREOVER
THEY
ARE
STABLE
UNDER
VARYING
LIGHTING
CONDITIONS
IN
A
COMPARATIVE
STUDY
OF
DIFFER
ENT
INTEREST
POINT
DETECTORS
THE
HARRIS
CORNER
WAS
PROVEN
TO
BE
THE
MOST
REPEATABLE
AND
MOST
INFORMATIVE
ADDITIONALLY
THEY
CAN
BE
MADE
VERY
PRECISE
SUB
PIXEL
PRECISION
CAN
BE
ACHIEVED
THROUGH
QUADRATIC
APPROXIMATION
OF
THE
CORNERNESS
FUNCTION
IN
THE
NEIGHBOR
HOOD
OF
A
LOCAL
MAXIMUM
DISCUSSION
FIGURE
SHOWS
THE
CORNERS
DETECTED
WITH
THIS
MEASURE
FOR
TWO
EXAM
PLE
IMAGES
RELATED
BY
A
ROTATION
NOTE
THAT
THE
FEATURES
FOUND
CORRE
SPOND
TO
LOCATIONS
IN
THE
IMAGE
SHOWING
TWO
DIMENSIONAL
VARIATIONS
IN
THE
INTENSITY
PATTERN
THESE
MAY
CORRESPOND
TO
REAL
CORNERS
BUT
THE
DETECTOR
ALSO
FIRES
ON
OTHER
STRUCTURES
SUCH
AS
T
JUNCTIONS
POINTS
WITH
HIGH
CURVATURE
ETC
THIS
EQUALLY
HOLDS
FOR
ALL
OTHER
CORNER
DETECTORS
DESCRIBED
IN
THIS
CHAPTER
WHEN
TRUE
CORNERS
ARE
DESIRABLE
MODEL
BASED
APPROACHES
ARE
CERTAINLY
MORE
APPROPRIATE
FIG
HARRIS
CORNERS
DETECTED
ON
ROTATED
IMAGE
EXAMPLES
AS
CAN
BE
SEEN
IN
THE
FIGURE
MANY
BUT
NOT
ALL
OF
THE
FEATURES
DETECTED
IN
THE
ORIGINAL
IMAGE
LEFT
HAVE
ALSO
BEEN
FOUND
IN
THE
ROTATED
VERSION
RIGHT
IN
OTHER
WORDS
THE
REPEATABILITY
OF
THE
HARRIS
DETECTOR
UNDER
ROTATIONS
IS
HIGH
ADDITIONALLY
FEATURES
ARE
TYPICALLY
FOUND
AT
LOCATIONS
WHICH
ARE
INFORMATIVE
I
E
WITH
A
HIGH
VARIABILITY
IN
THE
INTENSITY
PATTERN
THIS
MAKES
THEM
MORE
DISCRIMINATIVE
AND
EASIER
TO
BRING
INTO
CORRESPONDENCE
SUSAN
DETECTOR
THE
SUSAN
CORNER
DETECTOR
HAS
BEEN
INTRODUCED
BY
SMITH
AND
BRADY
AND
RELIES
ON
A
DIFFERENT
TECHNIQUE
RATHER
THAN
EVALUAT
ING
LOCAL
GRADIENTS
WHICH
MIGHT
BE
NOISE
SENSITIVE
AND
COMPUTATIONALLY
MORE
EXPENSIVE
A
MORPHOLOGICAL
APPROACH
IS
USED
SUSAN
STANDS
FOR
SMALLEST
UNIVALUE
SEGMENT
ASSIMILATING
NUCLEUS
AND
IS
A
GENERIC
LOW
LEVEL
IMAGE
PROCESSING
TECHNIQUE
WHICH
APART
FROM
CORNER
DETECTION
HAS
ALSO
BEEN
USED
FOR
EDGE
DETECTION
AND
NOISE
SUPPRESSION
THE
BASIC
PRINCIPLE
GOES
AS
FOLLOWS
SEE
ALSO
FIGURE
FOR
EACH
PIXEL
IN
THE
IMAGE
WE
CONSIDER
A
CIRCULAR
NEIGH
BORHOOD
OF
FIXED
RADIUS
AROUND
IT
THE
CENTER
PIXEL
IS
REFERRED
TO
AS
THE
NUCLEUS
AND
ITS
INTENSITY
VALUE
IS
USED
AS
REFERENCE
THEN
ALL
OTHER
FIG
SUSAN
CORNERS
ARE
DETECTED
BY
SEGMENTING
A
CIRCULAR
NEIGHBORHOOD
INTO
SIMILAR
ORANGE
AND
DISSIMILAR
BLUE
REGIONS
CORNERS
ARE
LOCATED
WHERE
THE
RELATIVE
AREA
OF
THE
SIMILAR
REGION
USAN
REACHES
A
LOCAL
MINIMUM
BELOW
A
CERTAIN
THRESHOLD
SUSAN
DETECTOR
PIXELS
WITHIN
THIS
CIRCULAR
NEIGHBORHOOD
ARE
PARTITIONED
INTO
TWO
CAT
EGORIES
DEPENDING
ON
WHETHER
THEY
HAVE
SIMILAR
INTENSITY
VALUES
AS
THE
NUCLEUS
OR
DIFFERENT
INTENSITY
VALUES
IN
THIS
WAY
EACH
IMAGE
POINT
HAS
ASSOCIATED
WITH
IT
A
LOCAL
AREA
OF
SIMILAR
BRIGHTNESS
COINED
USAN
WHOSE
RELATIVE
SIZE
CONTAINS
IMPORTANT
INFORMATION
ABOUT
THE
STRUCTURE
OF
THE
IMAGE
AT
THAT
POINT
SEE
ALSO
FIGURE
IN
MORE
OR
LESS
HOMO
GENEOUS
PARTS
OF
THE
IMAGE
THE
LOCAL
AREA
OF
SIMILAR
BRIGHTNESS
COVERS
ALMOST
THE
ENTIRE
CIRCULAR
NEIGHBORHOOD
NEAR
EDGES
THIS
RATIO
DROPS
TO
AND
NEAR
CORNERS
IT
DECREASES
FURTHER
TO
ABOUT
HENCE
CORNERS
CAN
BE
DETECTED
AS
LOCATIONS
IN
THE
IMAGE
WHERE
THE
NUMBER
OF
PIXELS
WITH
SIMILAR
INTENSITY
VALUE
IN
A
LOCAL
NEIGHBORHOOD
REACHES
A
LOCAL
MIN
IMUM
AND
IS
BELOW
A
PREDEFINED
THRESHOLD
TO
MAKE
THE
METHOD
MORE
ROBUST
PIXELS
CLOSER
IN
VALUE
TO
THE
NUCLEUS
RECEIVE
A
HIGHER
WEIGHTING
MOREOVER
A
SET
OF
RULES
IS
USED
TO
SUPPRESS
QUALITATIVELY
BAD
FEATURES
LOCAL
MINIMA
OF
THE
SUSANS
SMALLEST
USANS
ARE
THEN
SELECTED
FROM
THE
REMAINING
CANDIDATES
AN
EXAMPLE
OF
DETECTED
SUSAN
CORNERS
IS
SHOWN
IN
FIGURE
DISCUSSION
THE
FEATURES
FOUND
SHOW
A
HIGH
REPEATABILITY
FOR
THIS
ARTIFICIALLY
ROTATED
SET
OF
IMAGES
HOWEVER
MANY
OF
THE
FEATURES
ARE
LOCATED
ON
EDGE
STRUCTURES
AND
NOT
ON
CORNERS
FOR
SUCH
POINTS
THE
LOCALIZATION
FIG
SUSAN
CORNERS
FOUND
FOR
OUR
EXAMPLE
IMAGES
IS
SENSITIVE
TO
NOISE
MOREOVER
EDGE
BASED
POINTS
ARE
ALSO
LESS
DISCRIMINATIVE
THE
TWO
DETECTORS
DESCRIBED
SO
FAR
ARE
INVARIANT
UNDER
TRANSLATION
AND
ROTATION
ONLY
THIS
MEANS
THAT
CORNERS
WILL
BE
DETECTED
AT
COR
RESPONDING
LOCATIONS
ONLY
IF
THE
IMAGES
ARE
RELATED
BY
A
TRANSLATION
AND
OR
ROTATION
IN
THE
NEXT
SECTIONS
WE
WILL
DESCRIBE
DETECTORS
WITH
HIGHER
LEVELS
OF
VIEWPOINT
INVARIANCE
THAT
CAN
WITHSTAND
SCALE
CHANGES
OR
EVEN
AFFINE
DEFORMATIONS
APART
FROM
BETTER
MATCHING
ACROSS
TRANS
FORMED
IMAGES
THESE
ALSO
BRING
THE
ADVANTAGE
OF
DETECTING
FEATURES
OVER
A
RANGE
OF
SCALES
OR
SHAPES
ALTERNATIVELY
THIS
EFFECT
CAN
BE
OBTAINED
BY
USING
A
MULTISCALE
APPROACH
IN
THAT
CASE
A
DETECTOR
WHICH
IS
NOT
SCALE
INVARIANT
IS
APPLIED
TO
THE
INPUT
IMAGE
AT
DIFFERENT
SCALES
I
E
AFTER
SMOOTHING
AND
SAMPLING
HARRIS
LAPLACE
AFFINE
MIKOLAJCZYK
AND
SCHMID
DEVELOPED
BOTH
A
SCALE
INVARIANT
CORNER
DETEC
TOR
REFERRED
TO
AS
HARRIS
LAPLACE
AS
WELL
AS
AN
AFFINE
INVARIANT
ONE
REFERRED
TO
AS
HARRIS
AFFINE
HARRIS
LAPLACE
HARRIS
LAPLACE
STARTS
WITH
A
MULTISCALE
HARRIS
CORNER
DETECTOR
AS
INI
TIALIZATION
TO
DETERMINE
THE
LOCATION
OF
THE
LOCAL
FEATURES
THE
CHARAC
TERISTIC
SCALE
IS
THEN
DETERMINED
BASED
ON
SCALE
SELECTION
AS
PROPOSED
BY
LINDEBERG
ET
AL
THE
IDEA
IS
TO
SELECT
THE
CHARACTERISTIC
SCALE
OF
A
LOCAL
STRUCTURE
FOR
WHICH
A
GIVEN
FUNCTION
ATTAINS
AN
EXTREMUM
OVER
SCALES
SEE
FIGURE
THE
SELECTED
SCALE
IS
CHARACTERISTIC
IN
THE
QUAN
TITATIVE
SENSE
SINCE
IT
MEASURES
THE
SCALE
AT
WHICH
THERE
IS
MAXIMUM
SIMILARITY
BETWEEN
THE
FEATURE
DETECTION
OPERATOR
AND
THE
LOCAL
IMAGE
STRUCTURES
THE
SIZE
OF
THE
REGION
IS
THEREFORE
SELECTED
INDEPENDENTLY
OF
THE
IMAGE
RESOLUTION
FOR
EACH
POINT
AS
THE
NAME
HARRIS
LAPLACE
SUG
GESTS
THE
LAPLACIAN
OPERATOR
IS
USED
FOR
SCALE
SELECTION
THIS
HAS
BEEN
SHOWN
TO
GIVE
THE
BEST
RESULTS
IN
THE
EXPERIMENTAL
COMPARISON
OF
AS
WELL
AS
IN
THESE
RESULTS
CAN
BE
EXPLAINED
BY
THE
CIRCULAR
SHAPE
FIG
EXAMPLE
OF
CHARACTERISTIC
SCALES
THE
TOP
ROW
SHOWS
IMAGES
TAKEN
WITH
DIFFERENT
ZOOM
THE
BOTTOM
ROW
SHOWS
THE
RESPONSES
OF
THE
LAPLACIAN
OVER
SCALES
FOR
TWO
CORRE
SPONDING
POINTS
THE
CHARACTERISTIC
SCALES
ARE
AND
FOR
THE
LEFT
AND
RIGHT
IMAGES
RESPECTIVELY
THE
RATIO
OF
SCALES
CORRESPONDS
TO
THE
SCALE
FACTOR
BETWEEN
THE
TWO
IMAGES
THE
RADIUS
OF
DISPLAYED
REGIONS
IN
THE
TOP
ROW
IS
EQUAL
TO
TIMES
THE
SELECTED
SCALES
OF
THE
LAPLACIAN
KERNEL
WHICH
ACTS
AS
A
MATCHED
FILTER
WHEN
ITS
SCALE
IS
ADAPTED
TO
THE
SCALE
OF
A
LOCAL
IMAGE
STRUCTURE
FIGURE
SHOWS
THE
SCALE
INVARIANT
LOCAL
FEATURES
OBTAINED
BY
APPLYING
THE
HARRIS
LAPLACE
DETECTOR
FOR
TWO
IMAGES
OF
THE
SAME
SCENE
RELATED
BY
A
SCALE
CHANGE
IN
ORDER
NOT
TO
OVERLOAD
THE
IMAGES
ONLY
SOME
OF
THE
CORRESPONDING
REGIONS
THAT
WERE
DETECTED
IN
BOTH
IMAGES
FIG
CORRESPONDING
FEATURES
FOUND
WITH
THE
HARRIS
LAPLACE
DETECTOR
ONLY
A
SUBSET
OF
CORRESPONDING
FEATURES
IS
DISPLAYED
TO
AVOID
CLUTTER
THE
CIRCLES
INDICATE
THE
SCALE
OF
THE
FEATURES
ARE
SHOWN
A
SIMILAR
SELECTION
MECHANISM
HAS
BEEN
USED
FOR
ALL
SUBSE
QUENT
IMAGE
PAIRS
SHOWN
IN
THIS
SURVEY
HARRIS
AFFINE
GIVEN
A
SET
OF
INITIAL
POINTS
EXTRACTED
AT
THEIR
CHARACTERISTIC
SCALES
BASED
ON
THE
HARRIS
LAPLACE
DETECTION
SCHEME
THE
ITERATIVE
ESTIMATION
OF
ELLIPTICAL
AFFINE
REGIONS
AS
PROPOSED
BY
LINDEBERG
ET
AL
ALLOWS
TO
OBTAIN
AFFINE
INVARIANT
CORNERS
INSTEAD
OF
CIRCULAR
REGIONS
THESE
ARE
ELLIPSES
THE
PROCEDURE
CONSISTS
OF
THE
FOLLOWING
STEPS
DETECT
THE
INITIAL
REGION
WITH
THE
HARRIS
LAPLACE
DETECTOR
ESTIMATE
THE
AFFINE
SHAPE
WITH
THE
SECOND
MOMENT
MATRIX
NORMALIZE
THE
AFFINE
REGION
TO
A
CIRCULAR
ONE
RE
DETECT
THE
NEW
LOCATION
AND
SCALE
IN
THE
NORMALIZED
IMAGE
GO
TO
STEP
IF
THE
EIGENVALUES
OF
THE
SECOND
MOMENT
MATRIX
FOR
THE
NEW
POINT
ARE
NOT
EQUAL
THE
ITERATIONS
ARE
ILLUSTRATED
IN
FIGURE
FIG
ITERATIVE
DETECTION
OF
AN
AFFINE
INVARIANT
INTEREST
POINT
IN
THE
PRESENCE
OF
AN
AFFINE
TRANSFORMATION
TOP
AND
BOTTOM
ROWS
THE
FIRST
COLUMN
SHOWS
THE
POINTS
USED
FOR
INITIAL
IZATION
THE
CONSECUTIVE
COLUMNS
SHOW
THE
POINTS
AND
REGIONS
AFTER
ITERATIONS
AND
NOTE
THAT
THE
REGIONS
CONVERGE
AFTER
ITERATIONS
TO
CORRESPONDING
IMAGE
REGIONS
FIG
DIAGRAM
ILLUSTRATING
THE
AFFINE
NORMALIZATION
USING
THE
SECOND
MOMENT
MATRICES
IMAGE
COORDINATES
ARE
TRANSFORMED
WITH
MATRICES
M
AND
M
L
R
THE
EIGENVALUES
OF
THE
SECOND
MOMENT
MATRIX
SEE
EQUATION
ARE
USED
TO
MEASURE
THE
AFFINE
SHAPE
OF
THE
POINT
NEIGHBORHOOD
MORE
PRECISELY
WE
DETERMINE
THE
TRANSFORMATION
THAT
PROJECTS
THE
INTENSITY
PATTERN
OF
THE
POINT
NEIGHBORHOOD
TO
ONE
WITH
EQUAL
EIGENVALUES
THIS
TRANSFORMATION
IS
GIVEN
BY
THE
SQUARE
ROOT
OF
THE
SECOND
MOMENT
MATRIX
M
IT
CAN
BE
SHOWN
THAT
IF
THE
NEIGHBORHOODS
OF
TWO
POINTS
XR
AND
XL
ARE
RELATED
BY
AN
AFFINE
TRANSFORMATION
THEN
THEIR
NORMALIZED
VER
SIONS
XR
M
AND
XL
M
ARE
RELATED
BY
A
SIMPLE
ROTA
R
R
L
L
TION
X
L
RX
R
THIS
PROCESS
IS
ILLUSTRATED
IN
FIGURE
THE
MATRICES
ML
AND
MR
COMPUTED
IN
THE
NORMALIZED
FRAMES
ARE
ROTATION
MATRICES
AS
WELL
NOTE
THAT
ROTATION
PRESERVES
THE
EIGENVALUE
RATIO
FOR
AN
IMAGE
PATCH
THEREFORE
THE
AFFINE
DEFORMATION
CAN
BE
DETERMINED
ONLY
UP
TO
A
ROTATION
FACTOR
THE
ESTIMATION
OF
AFFINE
SHAPE
CAN
BE
APPLIED
TO
ANY
INITIAL
POINT
GIVEN
THAT
THE
DETERMINANT
OF
THE
SECOND
MOMENT
MATRIX
IS
LARGER
THAN
ZERO
AND
THE
SIGNAL
TO
NOISE
RATIO
IS
SUFFICIENTLY
LARGE
WE
CAN
THEREFORE
USE
THIS
TECHNIQUE
TO
ESTIMATE
THE
SHAPE
OF
INITIAL
REGIONS
PROVIDED
BY
THE
HARRIS
LAPLACE
DETECTOR
THE
OUTPUT
OF
THE
HARRIS
AFFINE
DETECTOR
ON
TWO
IMAGES
OF
THE
SAME
SCENE
IS
SHOWN
IN
FIGURE
APART
FROM
THE
SCALE
ALSO
THE
SHAPE
OF
THE
REGIONS
IS
NOW
ADAPTED
TO
THE
UNDERLYING
INTENSITY
PATTERNS
SO
AS
FIG
HARRIS
AFFINE
REGIONS
GENERATED
FOR
TWO
DIFFERENT
VIEWS
OF
A
PLANAR
SCENE
SUBSET
IN
SPITE
OF
THE
AFFINE
DEFORMATION
THE
REGION
SHAPES
CLEARLY
CORRESPOND
TO
ENSURE
THAT
THE
SAME
PART
OF
THE
OBJECT
SURFACE
IS
COVERED
IN
SPITE
OF
THE
DEFORMATIONS
CAUSED
BY
THE
VIEWPOINT
CHANGE
EDGE
BASED
REGIONS
A
MORE
HEURISTIC
TECHNIQUE
TO
OBTAIN
AFFINE
INVARIANCE
IS
TO
EXPLOIT
THE
GEOMETRY
OF
THE
EDGES
THAT
CAN
USUALLY
BE
FOUND
IN
THE
PROXIMITY
OF
A
HARRIS
CORNER
SUCH
A
METHOD
HAS
BEEN
PROPOSED
BY
TUYTELAARS
AND
VAN
GOOL
THE
RATIONALE
BEHIND
THIS
APPROACH
IS
THAT
EDGES
ARE
TYPICALLY
RATHER
STABLE
IMAGE
FEATURES
THAT
CAN
BE
DETECTED
OVER
A
RANGE
OF
VIEWPOINTS
SCALES
AND
ILLUMINATION
CHANGES
MOREOVER
BY
EXPLOITING
THE
EDGE
GEOMETRY
THE
DIMENSIONALITY
OF
THE
PROBLEM
CAN
BE
SIGNIFICANTLY
REDUCED
INDEED
AS
WILL
BE
SHOWN
NEXT
THE
SEARCH
PROBLEM
OVER
ALL
POSSIBLE
AFFINITIES
OR
ONCE
THE
CENTER
POINT
IS
FIXED
CAN
BE
REDUCED
TO
A
ONE
DIMENSIONAL
PROBLEM
BY
EXPLOITING
THE
NEARBY
EDGES
GEOMETRY
IN
PRACTICE
WE
START
FROM
A
HARRIS
CORNER
POINT
P
SEE
SECTION
AND
A
NEARBY
EDGE
EXTRACTED
WITH
THE
CANNY
EDGE
DETECTOR
TO
INCREASE
THE
ROBUSTNESS
TO
SCALE
CHANGES
THESE
BASIC
FEATURES
ARE
EXTRACTED
AT
MULTIPLE
SCALES
TWO
POINTS
AND
MOVE
AWAY
FROM
THE
CORNER
IN
BOTH
DIRECTIONS
ALONG
THE
EDGE
AS
SHOWN
IN
FIGURE
THEIR
RELATIVE
SPEED
IS
COUPLED
THROUGH
THE
EQUALITY
OF
RELATIVE
AFFINE
INVARIANT
PARAMETERS
AND
LI
R
ABS
PI
SI
P
PI
SI
DSI
EDGE
BASED
REGIONS
Q
P
FIG
THE
EDGE
BASED
REGION
DETECTOR
STARTS
FROM
A
CORNER
POINT
P
AND
EXPLOITS
NEARBY
EDGE
INFORMATION
WITH
SI
AN
ARBITRARY
CURVE
PARAMETER
IN
BOTH
DIRECTIONS
I
PI
SI
THE
FIRST
DERIVATIVE
OF
PI
SI
WITH
RESPECT
TO
SI
ABS
THE
ABSO
LUTE
VALUE
AND
THE
DETERMINANT
THIS
CONDITION
PRESCRIBES
THAT
THE
AREAS
BETWEEN
THE
JOINT
P
AND
THE
EDGE
AND
BETWEEN
THE
JOINT
P
AND
THE
EDGE
REMAIN
IDENTICAL
FROM
NOW
ON
WE
SIMPLY
USE
L
WHEN
REFERRING
TO
FOR
EACH
VALUE
L
THE
TWO
POINTS
L
AND
L
TOGETHER
WITH
THE
CORNER
P
DEFINE
A
PARALLELOGRAM
Ω
L
THE
PARALLELOGRAM
SPANNED
BY
THE
VECTORS
L
P
AND
L
P
SEE
FIGURE
THIS
YIELDS
A
ONE
DIMENSIONAL
FAMILY
OF
PARALLELOGRAM
SHAPED
REGIONS
AS
A
FUNCTION
OF
L
FROM
THIS
FAMILY
ONE
OR
A
FEW
PARALLELOGRAM
ARE
SELECTED
FOR
WHICH
THE
FOLLOWING
PHOTOMETRIC
QUANTITIES
OF
THE
TEXTURE
GO
THROUGH
AN
EXTREMUM
INV
ABS
PG
PG
M
P
P
M
M
M
P
P
M
M
M
WITH
N
IN
X
Y
XPYQ
DXDY
Ω
PG
FIG
ORIGINALLY
DETECTED
REGION
SHAPES
FOR
THE
EDGE
BASED
REGIONS
SUBSET
WITH
M
N
THE
NTH
ORDER
P
Q
TH
DEGREE
MOMENT
COMPUTED
OVER
THE
REGION
Ω
L
PG
THE
CENTER
OF
GRAVITY
OF
THE
REGION
WEIGHTED
WITH
INTEN
SITY
I
X
Y
AND
Q
THE
CORNER
OF
THE
PARALLELOGRAM
OPPOSITE
TO
THE
CORNER
POINT
P
SEE
FIGURE
THE
SECOND
FACTOR
IN
THESE
FORMULA
HAS
BEEN
ADDED
TO
ENSURE
INVARIANCE
UNDER
AN
INTENSITY
OFFSET
FOR
STRAIGHT
EDGES
L
ALONG
THE
ENTIRE
EDGE
IN
THAT
CASE
THE
TWO
PHOTOMETRIC
QUANTITIES
GIVEN
IN
EQUATION
ARE
COMBINED
AND
LOCATIONS
WHERE
BOTH
FUNCTIONS
REACH
A
MINIMUM
VALUE
ARE
TAKEN
TO
FIX
THE
PARAMETERS
AND
MOREOVER
INSTEAD
OF
RELYING
ON
THE
HARRIS
CORNER
DETECTION
THE
STRAIGHT
LINES
INTERSECTION
POINT
CAN
BE
USED
INSTEAD
EXAMPLES
OF
DETECTED
REGIONS
ARE
DISPLAYED
IN
FIGURE
FROM
PARALLELOGRAMS
TO
ELLIPSES
NOTE
THAT
THE
REGIONS
FOUND
WITH
THIS
METHOD
ARE
PARALLELOGRAMS
THIS
IS
IN
CONTRAST
TO
MANY
OTHER
AFFINE
INVARIANT
DETECTORS
FOR
EXAMPLE
THOSE
BASED
ON
THE
SECOND
MOMENT
MATRIX
FOR
WHICH
THE
OUTPUT
SHAPE
IS
AN
ELLIPSE
FOR
UNIFORMITY
AND
CONVENIENCE
IN
COMPARISON
IT
IS
SOME
TIMES
ADVANTAGEOUS
TO
CONVERT
THESE
PARALLELOGRAM
SHAPED
REGIONS
INTO
ELLIPSES
THIS
CAN
BE
ACHIEVED
BY
SELECTING
AN
ELLIPSE
WITH
THE
SAME
FIRST
AND
SECOND
ORDER
MOMENTS
AS
THE
ORIGINALLY
DETECTED
REGION
WHICH
IS
AN
AFFINE
COVARIANT
CONSTRUCTION
METHOD
THE
ELLIPTICAL
REGIONS
GENERATED
WITH
THIS
PROCEDURE
ARE
SHOWN
IN
FIGURE
NOTE
THOUGH
THAT
SOME
DISCUSSION
FIG
EDGE
BASED
REGIONS
GENERATED
FOR
THE
TWO
EXAMPLE
IMAGES
REPRESENTED
WITH
ELLIPSES
SUBSET
INFORMATION
IS
LOST
DURING
THIS
CONVERSION
AS
ELLIPSES
HAVE
A
ROTATIONAL
DEGREE
OF
FREEDOM
WHICH
WAS
FIXED
IN
THE
ORIGINAL
REPRESENTATION
DISCUSSION
SEVERAL
METHODS
FOR
CORNER
DETECTION
HAVE
BEEN
DESCRIBED
IN
THIS
CHAP
TER
AS
DISCUSSED
EARLIER
CORNER
BASED
FEATURES
DO
NOT
NECESSARILY
COR
RESPOND
TO
REAL
CORNERS
IN
THE
WORLD
INDEED
THE
GOAL
IS
TO
EXTRACT
STABLE
FEATURES
THAT
CAN
BE
MATCHED
WELL
IN
SPITE
OF
CHANGES
IN
VIEWING
CONDITIONS
THE
HARRIS
DETECTOR
WAS
IDENTIFIED
AS
THE
MOST
STABLE
ONE
IN
MANY
INDEPENDENT
EVALUATIONS
THERE
ARE
ALSO
MULTI
SCALE
AS
WELL
AS
SCALE
AND
AFFINE
INVARIANT
EXTENSIONS
OF
THIS
APPROACH
IT
IS
A
CONVENIENT
TOOL
FOR
PROVIDING
A
LARGE
NUMBER
OF
FEATURES
ALTERNATIVELY
THE
SUSAN
DETECTOR
CAN
BE
USED
IT
IS
MORE
EFFICIENT
BUT
ALSO
MORE
SENSITIVE
TO
NOISE
AN
OPTIMIZED
SUSAN
DETECTOR
USING
MACHINE
LEARN
ING
TECHNIQUES
IS
DESCRIBED
IN
SECTION
AS
DISCUSSED
IN
SECTIONS
AND
CONTOUR
BASED
CORNER
DETECTORS
ARE
SUITABLE
FOR
LINE
DRAWING
IMAGES
BUT
IN
NATURAL
SCENES
INTENSITY
BASED
METHODS
ARE
TYPICALLY
MORE
STABLE
IT
IS
IMPORTANT
TO
NOTE
THAT
THE
AFFINE
TRANSFORMATION
MODEL
ONLY
HOLDS
FOR
VIEWPOINT
CHANGES
IN
CASE
OF
LOCALLY
PLANAR
REGIONS
AND
ASSUM
ING
THE
CAMERA
IS
RELATIVELY
FAR
FROM
THE
OBJECT
HOWEVER
CORNERS
ARE
OFTEN
FOUND
NEAR
OBJECT
BOUNDARIES
AS
THIS
IS
WHERE
THE
INTENSITY
CHANGE
USUALLY
OCCURS
HENCE
THE
REGION
EXTRACTION
PROCESS
IS
OFTEN
BASED
ON
MEASUREMENTS
ON
NON
PLANAR
STRUCTURES
E
G
INCLUDING
BACKGROUND
OR
ANOTHER
FACET
OF
THE
OBJECT
IN
THESE
CASES
THE
VIEWPOINT
INVARIANCE
WILL
BE
LIMITED
AND
ALSO
THE
ROBUSTNESS
TO
BACKGROUND
CHANGES
WILL
BE
AFFECTED
A
POSSIBLE
WAY
OUT
HAS
BEEN
INDICATED
IN
THE
WORK
OF
DETECTORS
THAT
SEARCH
FOR
REGION
BOUNDARIES
LIKE
EBR
ARE
LESS
AFFECTED
BY
THIS
PHENOMENON
THE
MEASUREMENT
REGIONS
CAN
THEN
BE
DELIMITED
BY
THE
DETECTED
CONTOURS
THUS
EXCLUDING
THE
NON
PLANAR
PARTS
IN
MANY
PRACTICAL
SITUATIONS
ON
THE
POSITIVE
SIDE
COMPARED
TO
OTHER
TYPES
OF
FEATURES
CORNERS
ARE
TYPICALLY
BETTER
LOCALIZED
IN
THE
IMAGE
PLANE
THIS
LOCALIZATION
ACCURACY
CAN
BE
IMPORTANT
FOR
SOME
APPLICATIONS
E
G
FOR
CAMERA
CALIBRATION
OR
RECONSTRUCTION
THEIR
SCALE
HOWEVER
IS
NOT
WELL
DEFINED
AS
A
CORNER
STRUCTURE
CHANGES
VERY
LITTLE
OVER
A
WIDE
RANGE
OF
SCALES
THE
REASON
WHY
SCALE
SELECTION
STILL
WORKS
WITH
THE
HARRIS
DETECTOR
IS
THAT
THE
FEATURE
POINT
IS
LOCALIZED
NOT
EXACTLY
ON
THE
CORNER
EDGE
BUT
SLIGHTLY
INSIDE
THE
CORNER
STRUCTURE
BLOB
DETECTORS
AFTER
CORNERS
THE
SECOND
MOST
INTUITIVE
LOCAL
FEATURES
ARE
BLOBS
AS
IT
WAS
THE
CASE
IN
THE
PREVIOUS
SECTION
WE
SELECT
A
FEW
METHODS
THAT
HAVE
PROVED
SUCCESSFUL
IN
MANY
APPLICATIONS
AND
DESCRIBE
THESE
IN
MORE
DETAIL
THESE
METHODS
TYPICALLY
PROVIDE
COMPLEMENTARY
FEATURES
TO
THE
ONES
DISCUSSED
IN
THE
PREVIOUS
CHAPTER
WE
START
WITH
A
DERIVATIVE
BASED
METHOD
THE
HESSIAN
DETECTOR
SECTION
NEXT
WE
CONSIDER
THE
SCALE
INVARIANT
AND
AFFINE
INVARIANT
EXTENSIONS
OF
THIS
METHOD
COINED
HESSIAN
LAPLACE
AND
HESSIAN
AFFINE
SECTION
FINALLY
WE
DESCRIBE
THE
SALIENT
REGION
DETECTOR
WHICH
IS
BASED
ON
THE
ENTROPY
OF
THE
INTEN
SITY
PROBABILITY
DISTRIBUTION
SECTION
WE
CONCLUDE
THE
CHAPTER
WITH
A
SHORT
DISCUSSION
HESSIAN
DETECTOR
THE
SECOND
MATRIX
ISSUED
FROM
THE
TAYLOR
EXPANSION
OF
THE
IMAGE
INTENSITY
FUNCTION
I
X
IS
THE
HESSIAN
MATRIX
IXX
X
ΣD
IXY
X
ΣD
IXY
X
ΣD
IYY
X
ΣD
WITH
IXX
ETC
SECOND
ORDER
GAUSSIAN
SMOOTHED
IMAGE
DERIVATIVES
THESE
ENCODE
THE
SHAPE
INFORMATION
BY
DESCRIBING
HOW
THE
NORMAL
TO
AN
ISOSURFACE
CHANGES
AS
SUCH
THEY
CAPTURE
IMPORTANT
PROPERTIES
OF
LOCAL
IMAGE
STRUCTURE
PARTICULARLY
INTERESTING
ARE
THE
FILTERS
BASED
ON
THE
DETERMINANT
AND
THE
TRACE
OF
THIS
MATRIX
THE
LATTER
IS
OFTEN
REFERRED
TO
AS
THE
LAPLACIAN
LOCAL
MAXIMA
OF
BOTH
MEASURES
CAN
BE
USED
TO
DETECT
BLOB
LIKE
STRUCTURES
IN
AN
IMAGE
THE
LAPLACIAN
IS
A
SEPARABLE
LINEAR
FILTER
AND
CAN
BE
APPROXIMATED
EFFICIENTLY
WITH
A
DIFFERENCE
OF
GAUSSIANS
DOG
FILTER
THE
LAPLA
CIAN
FILTERS
HAVE
ONE
MAJOR
DRAWBACK
IN
THE
CONTEXT
OF
BLOB
EXTRAC
TION
THOUGH
LOCAL
MAXIMA
ARE
OFTEN
FOUND
NEAR
CONTOURS
OR
STRAIGHT
EDGES
WHERE
THE
SIGNAL
CHANGE
IS
ONLY
IN
ONE
DIRECTION
THESE
MAXIMA
ARE
LESS
STABLE
BECAUSE
THEIR
LOCALIZATION
IS
MORE
SENSITIVE
TO
NOISE
OR
SMALL
CHANGES
IN
NEIGHBORING
TEXTURE
THIS
IS
MOSTLY
AN
ISSUE
IN
THE
CONTEXT
OF
FINDING
CORRESPONDENCES
FOR
RECOVERING
IMAGE
TRANSFORMATIONS
A
MORE
SOPHISTICATED
APPROACH
SOLVING
THIS
PROB
LEM
IS
TO
SELECT
A
LOCATION
AND
SCALE
FOR
WHICH
THE
TRACE
AND
THE
DETERMINANT
OF
THE
HESSIAN
MATRIX
SIMULTANEOUSLY
ASSUME
A
LOCAL
EXTREMUM
THIS
GIVES
RISE
TO
POINTS
FOR
WHICH
THE
SECOND
ORDER
DERIVATIVES
DETECT
SIGNAL
CHANGES
IN
TWO
ORTHOGONAL
DIRECTIONS
A
SIMILAR
IDEA
IS
EXPLORED
IN
THE
HARRIS
DETECTOR
ALBEIT
FOR
FIRST
ORDER
DERIVATIVES
ONLY
THE
FEATURE
DETECTION
PROCESS
BASED
ON
THE
HESSIAN
MATRIX
IS
ILLUS
TRATED
IN
FIGURE
GIVEN
THE
ORIGINAL
IMAGE
UPPER
LEFT
ONE
FIRST
COMPUTES
THE
SECOND
ORDER
GAUSSIAN
SMOOTHED
IMAGE
DERIVATIVES
LOWER
PART
WHICH
ARE
THEN
COMBINED
INTO
THE
DETERMINANT
OF
THE
HESSIAN
UPPER
RIGHT
THE
INTEREST
POINTS
DETECTED
WITH
THE
DETERMINANT
OF
THE
HESSIAN
FOR
AN
EXAMPLE
IMAGE
PAIR
ARE
DISPLAYED
IN
FIGURE
THE
SECOND
ORDER
DERIVATIVES
ARE
SYMMETRIC
FILTERS
THUS
THEY
GIVE
WEAK
RESPONSES
EXACTLY
IN
THE
POINT
WHERE
THE
SIGNAL
CHANGE
IS
MOST
SIGNIFICANT
THEREFORE
THE
MAXIMA
ARE
LOCALIZED
AT
RIDGES
AND
BLOBS
FOR
WHICH
THE
SIZE
OF
THE
GAUSSIAN
KERNEL
ΣD
MATCHES
BY
THE
SIZE
OF
THE
BLOB
STRUCTURE
HESSIAN
LAPLACE
AFFINE
THE
HESSIAN
LAPLACE
AND
HESSIAN
AFFINE
DETECTORS
ARE
SIMILAR
IN
SPIRIT
AS
THEIR
HARRIS
BASED
COUNTERPARTS
HARRIS
LAPLACE
AND
HARRIS
AFFINE
HESSIAN
LAPLACE
AFFINE
FIG
ILLUSTRATION
OF
THE
COMPONENTS
OF
THE
HESSIAN
MATRIX
AND
HESSIAN
DETERMINANT
FIG
OUTPUT
OF
THE
HESSIAN
DETECTOR
APPLIED
AT
A
GIVEN
SCALE
TO
EXAMPLE
IMAGES
WITH
ROTATION
SUBSET
DESCRIBED
IN
SECTION
EXCEPT
THAT
THEY
START
FROM
THE
DETERMINANT
OF
THE
HESSIAN
RATHER
THAN
THE
HARRIS
CORNERS
THIS
TURNS
THE
METHODS
INTO
VIEWPOINT
INVARIANT
BLOB
DETECTORS
THEY
HAVE
ALSO
BEEN
PROPOSED
BY
MIKOLAJCZYK
AND
SCHMID
AND
ARE
COMPLEMENTARY
TO
THEIR
HARRIS
BASED
COUNTERPARTS
IN
THE
SENSE
THAT
THEY
RESPOND
TO
A
DIFFERENT
TYPE
OF
FEATURE
IN
THE
IMAGE
AN
EXAMPLE
OF
THE
DETECTION
RESULT
IS
SHOWN
IN
FIGURES
AND
FOR
THE
SCALE
INVARIANT
HESSIAN
LAPLACE
AND
AFFINE
INVARIANT
HESSIAN
AFFINE
RESPECTIVELY
FIG
OUTPUT
OF
HESSIAN
LAPLACE
DETECTOR
APPLIED
TO
EXAMPLE
IMAGES
WITH
SCALE
CHANGE
SUBSET
FIG
HESSIAN
AFFINE
REGIONS
GENERATED
FOR
TWO
VIEWS
OF
THE
EXAMPLE
SCENE
SUBSET
LIKE
IN
THE
HARRIS
BASED
DETECTOR
THE
NUMBER
OF
REGIONS
FOUND
WITH
THE
HESSIAN
LAPLACE
DETECTOR
CAN
BE
CONTROLLED
BY
THRESHOLDING
THE
HESSIAN
DETERMINANT
AS
WELL
AS
THE
LAPLACIAN
RESPONSE
TYPICALLY
A
LARGE
NUMBER
OF
FEATURES
CAN
BE
EXTRACTED
RESULTING
IN
A
GOOD
COVER
AGE
OF
THE
IMAGE
WHICH
IS
ONE
OF
THE
ADVANTAGES
OF
THE
HESSIAN
BASED
DETECTOR
FURTHERMORE
THIS
DETECTOR
ALSO
RESPONDS
TO
SOME
CORNER
STRUCTURES
AT
FINE
SCALE
SEE
FIGURE
THE
RETURNED
LOCATIONS
HOWEVER
ARE
MORE
SUITABLE
FOR
SCALE
ESTIMATION
THAN
THE
HARRIS
POINTS
DUE
TO
THE
USE
OF
SIMILAR
FILTERS
FOR
SPATIAL
AND
SCALE
LOCALIZATION
BOTH
BASED
ON
SECOND
ORDER
GAUSSIAN
DERIVATIVES
ONE
OF
THE
POSSIBLE
EXTENSIONS
OF
THIS
WORK
SALIENT
REGIONS
IS
TO
EXPLORE
THE
HESSIAN
MATRIX
TO
USE
ADDITIONAL
SHAPE
INFORMATION
ENCODED
BY
THE
EIGENVALUES
OF
THIS
MATRIX
SALIENT
REGIONS
RATHER
THAN
BUILDING
ON
THE
DERIVATIVE
INFORMATION
IN
THE
IMAGE
THE
SALIENT
REGION
DETECTOR
PROPOSED
BY
KADIR
AND
BRADY
IS
INSPIRED
BY
INFORMATION
THEORY
THE
BASIC
IDEA
BEHIND
THIS
FEATURE
DETECTOR
IS
TO
LOOK
FOR
SALIENT
FEATURES
WHERE
SALIENCY
IS
DEFINED
AS
LOCAL
COMPLEXITY
OR
UNPREDICTABILITY
IT
IS
MEASURED
BY
THE
ENTROPY
OF
THE
PROBABILITY
DIS
TRIBUTION
FUNCTION
OF
INTENSITY
VALUES
WITHIN
A
LOCAL
IMAGE
REGION
HOW
EVER
LOOKING
AT
ENTROPY
ALONE
DOES
NOT
SUFFICE
TO
ACCURATELY
LOCALIZE
THE
FEATURES
OVER
SCALES
SO
AS
AN
ADDITIONAL
CRITERION
THE
SELF
DISSIMILARITY
IN
SCALE
SPACE
OF
THE
FEATURE
IS
ADDED
AS
AN
EXTRA
WEIGHTING
FUNCTION
FAVORING
WELL
LOCALIZED
COMPLEX
FEATURES
DETECTION
PROCEEDS
IN
TWO
STEPS
FIRST
AT
EACH
PIXEL
X
THE
ENTROPY
OF
THE
PROBABILITY
DISTRIBUTION
P
I
IS
EVALUATED
OVER
A
RANGE
OF
SCALES
H
P
I
LOG
P
I
I
THE
PROBABILITY
DISTRIBUTION
P
I
IS
ESTIMATED
EMPIRICALLY
BASED
ON
THE
INTENSITY
DISTRIBUTION
IN
A
CIRCULAR
NEIGHBOURHOOD
OF
RADIUS
AROUND
X
LOCAL
MAXIMA
OF
THE
ENTROPY
ARE
RECORDED
THESE
ARE
CANDI
DATE
SALIENT
REGIONS
SECOND
FOR
EACH
OF
THE
CANDIDATE
SALIENT
REGIONS
THE
MAGNITUDE
OF
THE
DERIVATIVE
OF
P
I
WITH
RESPECT
TO
SCALE
IS
COMPUTED
AS
P
I
THE
SALIENCY
Y
IS
THEN
COMPUTED
AS
Y
WH
THE
CANDIDATE
SALIENT
REGIONS
OVER
THE
ENTIRE
IMAGE
ARE
RANKED
BY
THEIR
SALIENCY
AND
THE
TOP
P
RANKED
REGIONS
ARE
RETAINED
ALSO
AN
AFFINE
INVARIANT
VERSION
OF
THE
DETECTOR
HAS
BEEN
PROPOSED
WHERE
LOCAL
MAXIMA
OVER
THE
SCALE
AND
THE
SHAPE
PARAMETERS
ORIEN
TATION
Θ
AND
RATIO
OF
MAJOR
TO
MINOR
AXES
Λ
OF
AN
ELLIPTICAL
REGION
FIG
SALIENT
REGIONS
FOUND
FOR
THE
TWO
EXAMPLE
IMAGES
RELATED
TO
A
CHANGE
IN
VIEWPOINT
SUBSET
ARE
SOUGHT
SIMULTANEOUSLY
HOWEVER
THIS
SERIOUSLY
SLOWS
DOWN
THE
COMPUTATION
EXAMPLES
OF
DETECTED
REGIONS
USING
THE
AFFINE
INVARIANT
VERSION
ARE
DISPLAYED
IN
FIGURE
MORE
DETAILS
ABOUT
THIS
METHOD
CAN
BE
FOUND
IN
DISCUSSION
BECAUSE
OF
THE
WEIGHTING
FACTOR
MEASURING
THE
SELF
DISSIMILARITY
OVER
SCALE
THE
DETECTOR
TYPICALLY
FIRES
ON
BLOB
LIKE
STRUCTURES
IN
THE
IMAGE
THAT
IS
WHY
WE
HAVE
CATALOGUED
THE
METHOD
AS
A
BLOB
DETECTOR
BUT
NOTE
THAT
IN
CONTRAST
TO
OTHER
BLOB
DETECTORS
THE
CONTRAST
OF
THE
BLOBS
DOES
NOT
HAVE
ANY
INFLUENCE
ON
THE
DETECTION
THE
NUMBER
OF
FEATURES
FOUND
WITH
THIS
METHOD
IS
TYPICALLY
RELA
TIVELY
LOW
UNLIKE
FOR
MANY
OTHER
DETECTORS
THE
RANKING
OF
THE
EXTRACTED
FEATURES
IS
MEANINGFUL
DUE
TO
THE
ENTROPY
BASED
CRITERIA
WITH
THE
ONES
FROM
THE
TOP
THE
MOST
STABLE
THIS
PROPERTY
HAS
BEEN
EXPLORED
IN
THE
CONTEXT
OF
CATEGORY
LEVEL
OBJECT
RECOGNITION
AND
ESPECIALLY
IN
COMBI
NATION
WITH
CLASSIFIERS
WHERE
THE
COMPLEXITY
LARGELY
DEPENDS
ON
THE
NUMBER
OF
FEATURES
E
G
DISCUSSION
BLOB
DETECTORS
HAVE
BEEN
USED
WIDELY
IN
DIFFERENT
APPLICATION
DOMAINS
APART
FROM
THE
METHODS
DESCRIBED
ABOVE
ALSO
DOG
DIFFERENCE
OF
DISCUSSION
GAUSSIANS
AND
SURF
SPEEDED
UP
ROBUST
FEATURES
CAN
BE
CATALOGUED
AS
BLOB
DETECTORS
HOWEVER
SINCE
THEIR
EXTRACTION
PROCESSES
ARE
FOCUSSED
ON
EFFICIENCY
WE
POSTPONE
THEIR
DISCUSSION
UNTIL
SECTION
SOME
OF
THE
METHODS
DESCRIBED
IN
SECTION
ALSO
SHARE
COMMON
CHARACTERISTICS
WITH
BLOB
DETECTORS
ESPECIALLY
IBR
INTENSITY
BASED
REGIONS
AND
MSER
MAXIMALLY
STABLE
EXTREMAL
REGIONS
OFTEN
FIND
BLOB
LIKE
STRUCTURES
IN
THE
IMAGE
HOWEVER
APART
FROM
BLOB
LIKE
STRUCTURES
THEY
ALSO
DETECT
OTHER
MORE
IRREGULARLY
SHAPED
PATTERNS
WHICH
WE
CONSIDER
THEIR
DISTINCTIVE
PROPERTY
BLOB
DETECTORS
ARE
IN
A
SENSE
COMPLEMENTARY
TO
CORNER
DETECTORS
AS
A
RESULT
THEY
ARE
OFTEN
USED
TOGETHER
BY
USING
SEVERAL
COMPLEMEN
TARY
FEATURE
DETECTORS
THE
IMAGE
IS
BETTER
COVERED
AND
THE
PERFORMANCE
BECOMES
LESS
DEPENDENT
ON
THE
ACTUAL
IMAGE
CONTENT
THIS
HAS
BEEN
EXPLOITED
E
G
IN
IN
GENERAL
BLOB
LIKE
STRUCTURES
TEND
TO
BE
LESS
ACCURATELY
LOCALIZED
IN
THE
IMAGE
PLANE
THAN
CORNERS
ALTHOUGH
THEIR
SCALE
AND
SHAPE
ARE
BET
TER
DEFINED
THAN
FOR
CORNERS
THE
LOCATION
OF
A
CORNER
CAN
BE
IDENTIFIED
BY
A
SINGLE
POINT
WHILE
BLOBS
CAN
ONLY
BE
LOCALIZED
BY
THEIR
BOUNDARIES
WHICH
ARE
OFTEN
IRREGULAR
ON
THE
OTHER
HAND
THE
SCALE
ESTIMATION
OF
A
CORNER
IS
ILL
DEFINED
AS
FOR
EXAMPLE
AN
INTERSECTION
OF
EDGES
EXISTS
AT
A
WIDE
RANGE
OF
SCALES
THE
BOUNDARIES
OF
A
BLOB
HOWEVER
EVEN
IF
IRREGU
LAR
GIVE
A
GOOD
ESTIMATE
OF
THE
SIZE
THUS
SCALE
OF
THE
BLOB
THIS
MAKES
THEM
LESS
SUITED
FOR
E
G
CAMERA
CALIBRATION
OR
RECONSTRUCTION
FOR
OBJECT
RECOGNITION
ON
THE
OTHER
HAND
A
PRECISE
IMAGE
LOCALIZATION
IS
OFTEN
NOT
NECESSARY
SINCE
THE
ENTIRE
RECOGNITION
PROCESS
IS
VERY
NOISY
A
ROBUST
DESCRIPTOR
SUCH
AS
SIFT
CAN
MATCH
SUCH
FEATURES
NEV
ERTHELESS
THE
SCALES
OF
MATCHED
BLOBS
ALLOW
THEN
TO
HYPOTHESIZE
THE
SIZE
OF
THE
OBJECTS
WHICH
MAKES
THEM
VERY
USEFUL
IN
RECOGNITION
APPLICATIONS
FINALLY
THE
NUMBER
OF
FEATURES
DETECTED
WITH
THE
METHODS
DESCRIBED
ABOVE
VARIES
GREATLY
THERE
IS
OFTEN
JUST
A
FEW
TENS
OF
SALIENT
REGIONS
FOUND
IN
AN
IMAGE
WHEREAS
THE
HESSIAN
LAPLACE
OR
HESSIAN
AFFINE
METHODS
ALLOW
TO
EXTRACT
UP
TO
SEVERAL
HUNDREDS
OR
THOUSANDS
OF
FEA
TURES
DEPENDING
ON
THE
APPLICATION
AND
ALGORITHMS
USED
EITHER
CASE
CAN
BE
ADVANTAGEOUS
WE
REFER
TO
SECTION
FOR
A
FURTHER
DISCUSSION
ON
THIS
ISSUE
REGION
DETECTORS
IN
THIS
CHAPTER
WE
DISCUSS
A
NUMBER
OF
FEATURE
DETECTORS
WHICH
DIRECTLY
OR
INDIRECTLY
ARE
CONCERNED
WITH
EXTRACTION
OF
IMAGE
REGIONS
FIRST
WE
DESCRIBE
THE
INTENSITY
BASED
REGIONS
SECTION
FOLLOWED
BY
MAXIMALLY
STABLE
EXTREMAL
REGIONS
SECTION
AT
THE
END
WE
DISCUSS
SUPERPIX
ELS
SECTION
THESE
REGIONS
ARE
PROVIDED
BY
DIFFERENT
METHODS
BUT
FOCUS
ON
SIMILAR
IMAGE
STRUCTURES
AND
SHARE
SIMILAR
PROPERTIES
SUPER
PIXELS
ARE
TRADITIONALLY
NOT
CONSIDERED
AS
LOCAL
FEATURES
AND
HAVE
LIMITED
ROBUSTNESS
TO
CHANGES
IN
VIEWING
CONDITIONS
BUT
THEY
ARE
CURRENTLY
MORE
AND
MORE
USED
IN
THE
CONTEXT
OF
IMAGE
RECOGNITION
WE
THEREFORE
INCLUDE
ALL
THE
ABOVE
FEATURES
IN
THE
SAME
CATEGORY
INTENSITY
BASED
REGIONS
HERE
WE
DESCRIBE
A
METHOD
PROPOSED
BY
TUYTELAARS
AND
VAN
GOOL
TO
DETECT
AFFINE
INVARIANT
REGIONS
IT
STARTS
FROM
INTENSITY
EXTREMA
DETECTED
AT
MULTIPLE
SCALES
AND
EXPLORES
THE
IMAGE
AROUND
THEM
IN
A
RADIAL
WAY
DELINEATING
REGIONS
OF
ARBITRARY
SHAPE
WHICH
ARE
THEN
REPLACED
BY
ELLIPSES
MORE
PRECISELY
GIVEN
A
LOCAL
EXTREMUM
IN
INTENSITY
THE
INTENSITY
FUNCTION
ALONG
RAYS
EMANATING
FROM
THE
EXTREMUM
IS
STUDIED
SEE
I
T
T
F
T
T
T
FIG
CONSTRUCTION
OF
INTENSITY
BASED
REGIONS
FIGURE
THE
FOLLOWING
FUNCTION
IS
EVALUATED
ALONG
EACH
RAY
F
T
ABS
I
T
MAX
T
ABS
I
T
DT
WITH
T
AN
ARBITRARY
PARAMETER
ALONG
THE
RAY
I
T
THE
INTENSITY
AT
POSI
TION
T
THE
INTENSITY
VALUE
AT
THE
EXTREMUM
AND
D
A
SMALL
NUMBER
WHICH
HAS
BEEN
ADDED
TO
PREVENT
A
DIVISION
BY
ZERO
THE
POINT
FOR
WHICH
THIS
FUNCTION
REACHES
AN
EXTREMUM
IS
INVARIANT
UNDER
AFFINE
GEO
METRIC
AND
LINEAR
PHOTOMETRIC
TRANSFORMATIONS
GIVEN
THE
RAY
TYPI
CALLY
A
MAXIMUM
IS
REACHED
AT
POSITIONS
WHERE
THE
INTENSITY
SUDDENLY
INCREASES
OR
DECREASES
THE
FUNCTION
F
T
IS
IN
ITSELF
ALREADY
INVARI
ANT
NEVERTHELESS
POINTS
ARE
SELECTED
WHERE
THIS
FUNCTION
REACHES
AN
EXTREMUM
TO
MAKE
A
ROBUST
SELECTION
NEXT
ALL
POINTS
CORRESPONDING
TO
MAXIMA
OF
F
T
ALONG
RAYS
ORIGINATING
FROM
THE
SAME
LOCAL
EXTREMUM
ARE
LINKED
TO
ENCLOSE
AN
AFFINE
INVARIANT
REGION
THIS
OFTEN
IRREGULARLY
SHAPED
REGION
IS
REPLACED
BY
AN
ELLIPSE
HAVING
THE
SAME
SHAPE
MOMENTS
UP
TO
THE
SECOND
ORDER
THIS
ELLIPSE
FITTING
IS
AN
AFFINE
COVARIANT
CON
STRUCTION
AN
EXAMPLE
OF
REGIONS
DETECTED
WITH
THIS
METHOD
IS
SHOWN
IN
FIGURE
MAXIMALLY
STABLE
EXTREMAL
REGIONS
MSER
OR
MAXIMALLY
STABLE
EXTREMAL
REGIONS
HAVE
BEEN
PROPOSED
BY
MATAS
ET
AL
A
MAXIMALLY
STABLE
EXTREMAL
REGION
IS
A
CON
NECTED
COMPONENT
OF
AN
APPROPRIATELY
THRESHOLDED
IMAGE
THE
WORD
EXTREMAL
REFERS
TO
THE
PROPERTY
THAT
ALL
PIXELS
INSIDE
THE
MSER
HAVE
FIG
INTENSITY
BASED
REGIONS
FOUND
FOR
THE
GRAFFITI
IMAGES
SUBSET
EITHER
HIGHER
BRIGHT
EXTREMAL
REGIONS
OR
LOWER
DARK
EXTREMAL
REGIONS
INTENSITY
THAN
ALL
THE
PIXELS
ON
ITS
OUTER
BOUNDARY
THE
MAXIMALLY
STABLE
IN
MSER
DESCRIBES
THE
PROPERTY
OPTIMIZED
IN
THE
THRESHOLD
SELECTION
PROCESS
THE
SET
OF
EXTREMAL
REGIONS
I
E
THE
SET
OF
ALL
CONNECTED
COMPO
NENTS
OBTAINED
BY
THRESHOLDING
HAS
A
NUMBER
OF
DESIRABLE
PROPERTIES
FIRST
A
MONOTONIC
CHANGE
OF
IMAGE
INTENSITIES
LEAVES
UNCHANGED
SEC
OND
CONTINUOUS
GEOMETRIC
TRANSFORMATIONS
PRESERVE
TOPOLOGY
PIXELS
FROM
A
SINGLE
CONNECTED
COMPONENT
ARE
TRANSFORMED
TO
A
SINGLE
CON
NECTED
COMPONENT
FINALLY
THERE
ARE
NO
MORE
EXTREMAL
REGIONS
THAN
THERE
ARE
PIXELS
IN
THE
IMAGE
SO
A
SET
OF
REGIONS
WAS
DEFINED
THAT
IS
PRESERVED
UNDER
A
BROAD
CLASS
OF
GEOMETRIC
AND
PHOTOMETRIC
CHANGES
AND
YET
HAS
THE
SAME
CARDINALITY
AS
E
G
THE
SET
OF
FIXED
SIZED
SQUARE
WINDOWS
COMMONLY
USED
IN
NARROW
BASELINE
MATCHING
THE
ENUMERATION
OF
THE
SET
OF
EXTREMAL
REGIONS
IS
VERY
EFFICIENT
ALMOST
LINEAR
IN
THE
NUMBER
OF
IMAGE
PIXELS
THE
ENUMERATION
PROCEEDS
AS
FOLLOWS
FIRST
PIXELS
ARE
SORTED
BY
INTENSITY
AFTER
SORTING
PIXELS
ARE
MARKED
IN
THE
IMAGE
EITHER
IN
DECREASING
OR
INCREASING
ORDER
AND
THE
LIST
OF
GROWING
AND
MERGING
CONNECTED
COMPONENTS
AND
THEIR
AREAS
IS
MAINTAINED
USING
THE
UNION
FIND
ALGORITHM
DURING
THE
ENUMER
ATION
PROCESS
THE
AREA
OF
EACH
CONNECTED
COMPONENT
AS
A
FUNCTION
OF
INTENSITY
IS
STORED
AMONG
THE
EXTREMAL
REGIONS
THE
MAXIMALLY
STA
BLE
ONES
ARE
THOSE
CORRESPONDING
TO
THRESHOLDS
FOR
WHICH
THE
RELATIVE
AREA
CHANGE
AS
A
FUNCTION
OF
RELATIVE
CHANGE
OF
THRESHOLD
IS
AT
A
LOCAL
MINIMUM
IN
OTHER
WORDS
THE
MSER
ARE
THE
PARTS
OF
THE
IMAGE
WHERE
LOCAL
BINARIZATION
IS
STABLE
OVER
A
LARGE
RANGE
OF
THRESHOLDS
THE
DEFI
NITION
OF
MSER
STABILITY
BASED
ON
RELATIVE
AREA
CHANGE
IS
INVARIANT
TO
AFFINE
TRANSFORMATIONS
BOTH
PHOTOMETRICALLY
AND
GEOMETRICALLY
DETECTION
OF
MSER
IS
RELATED
TO
THRESHOLDING
SINCE
EVERY
EXTREMAL
REGION
IS
A
CONNECTED
COMPONENT
OF
A
THRESHOLDED
IMAGE
HOWEVER
NO
GLOBAL
OR
OPTIMAL
THRESHOLD
IS
SOUGHT
ALL
THRESHOLDS
ARE
TESTED
AND
THE
STABILITY
OF
THE
CONNECTED
COMPONENTS
EVALUATED
THE
OUTPUT
OF
THE
MSER
DETECTOR
IS
NOT
A
BINARIZED
IMAGE
FOR
SOME
PARTS
OF
THE
IMAGE
MULTIPLE
STABLE
THRESHOLDS
EXIST
AND
A
SYSTEM
OF
NESTED
SUBSETS
IS
OUTPUT
IN
THIS
CASE
FOR
MANY
OF
THE
AFFINE
INVARIANT
DETECTORS
THE
OUTPUT
SHAPE
IS
AN
ELLIPSE
HOWEVER
FOR
MSER
IT
IS
NOT
EXAMPLES
OF
THE
ORIGINAL
REGIONS
DETECTED
ARE
GIVEN
IN
FIGURE
USING
THE
SAME
PROCEDURE
AS
EXPLAINED
ABOVE
FOR
THE
IBR
AN
ELLIPSE
CAN
BE
FITTED
BASED
ON
THE
FIRST
AND
SECOND
SHAPE
MOMENTS
THIS
RESULTS
IN
A
SET
OF
FEATURES
AS
SHOWN
IN
FIGURE
ALTERNATIVELY
A
LOCAL
AFFINE
FRAME
CAN
BE
DEFINED
BASED
ON
A
SET
OF
STA
BLE
POINTS
ALONG
THE
REGION
CONTOUR
THIS
PROVIDES
AN
ALTERNATIVE
SCHEME
TO
NORMALIZE
THE
REGION
AGAINST
AFFINE
DEFORMATIONS
DISCUSSION
THE
MSER
FEATURES
TYPICALLY
ANCHOR
ON
REGION
BOUNDARIES
THUS
THE
RESULTING
REGIONS
ARE
ACCURATELY
LOCALIZED
COMPARED
TO
OTHER
FIG
REGIONS
DETECTED
WITH
MSER
ON
THE
GRAFFITI
IMAGES
SUBSET
FIG
FINAL
MSER
REGIONS
FOR
THE
GRAFFITI
IMAGES
SUBSET
BLOB
DETECTORS
THE
METHOD
WORKS
BEST
FOR
STRUCTURED
IMAGES
WHICH
CAN
BE
SEGMENTED
WELL
IDEALLY
AN
IMAGE
WITH
UNIFORM
REGIONS
SEPARATED
BY
STRONG
INTENSITY
CHANGES
ON
THE
DOWNSIDE
IT
WAS
FOUND
TO
BE
SENSI
TIVE
TO
IMAGE
BLUR
WHICH
CAN
BE
EXPLAINED
BY
THE
FACT
THAT
IMAGE
BLUR
UNDERMINES
THE
STABILITY
CRITERION
THIS
ISSUE
WAS
ADDRESSED
IN
ITS
RECENT
EXTENSION
IN
THE
METHOD
IS
ALSO
RELATIVELY
FAST
IT
IS
CUR
RENTLY
THE
MOST
EFFICIENT
AMONG
THE
AFFINE
INVARIANT
FEATURE
DETECTORS
IT
HAS
BEEN
USED
MOSTLY
FOR
RECOGNIZING
OR
MATCHING
SPECIFIC
OBJECTS
E
G
AND
SHOWED
LOWER
PERFORMANCE
FOR
OBJECT
CLASS
RECOG
NITION
SEGMENTATION
BASED
METHODS
SUPERPIXELS
THE
TWO
METHODS
DESCRIBED
ABOVE
EXTRACT
SMALL
REGIONS
WHOSE
INTENSITY
PATTERNS
CLEARLY
STAND
OUT
WITH
RESPECT
TO
THEIR
IMMEDIATE
SURROUND
INGS
THIS
IS
REMINISCENT
OF
TRADITIONAL
IMAGE
SEGMENTATION
TECHNIQUES
HOWEVER
IMAGE
SEGMENTS
ARE
TYPICALLY
RELATIVELY
LARGE
TOO
LARGE
IN
FACT
TO
BE
USED
AS
LOCAL
FEATURES
BY
INCREASING
THE
NUMBER
OF
SEGMENTS
A
NEW
IMAGE
REPRESENTATION
CAN
BE
OBTAINED
WHERE
THE
IMAGE
SEGMENTS
TYPICALLY
HAVE
THE
RIGHT
TRADE
OFF
BETWEEN
LOCALITY
AND
DISTINCTIVENESS
REQUIRED
IN
MOST
LOCAL
FEATURES
BASED
APPLICATIONS
SEE
FIGURE
THIS
LOW
LEVEL
GROUPING
OF
PIXELS
INTO
ATOMIC
REGIONS
HAS
BEEN
ADVOCATED
BY
MORI
ET
AL
AND
REN
AND
MALIK
WHO
REFER
TO
THE
RESULTING
ATOMIC
REGIONS
AS
SUPERPIXELS
THIS
TERMINOLOGY
REFERS
TO
THE
FACT
THAT
SEGMENTATION
BASED
METHODS
SUPERPIXELS
FIG
SUPERPIXELS
GENERATED
FOR
THE
EXAMPLE
IMAGE
SUPERPIXELS
CAN
BE
CONSIDERED
AS
A
MORE
NATURAL
AND
PERCEPTUALLY
MORE
MEANINGFUL
ALTERNATIVE
FOR
THE
ORIGINAL
IMAGE
PIXELS
IN
SUPERPIXELS
ARE
EXTRACTED
FROM
THE
IMAGE
USING
NOR
MALIZED
CUTS
BUT
ANY
DATA
DRIVEN
SEGMENTATION
METHODS
CAN
BE
USED
HERE
THE
NORMALIZED
CUTS
BASED
APPROACH
IS
A
CLASSICAL
IMAGE
SEGMENTATION
ALGORITHM
WHICH
EXPLOITS
PAIRWISE
BRIGHTNESS
COLOR
OR
TEXTURE
AFFINITIES
BETWEEN
PIXELS
TO
ENFORCE
LOCALITY
ONLY
LOCAL
CONNEC
TIONS
ARE
TAKEN
INTO
ACCOUNT
WHEN
CONSTRUCTING
THE
AFFINITY
MATRIX
AN
EXAMPLE
OF
SUPERPIXELS
IS
SHOWN
IN
FIGURE
IN
CONTRAST
TO
TRADITIONAL
LOCAL
FEATURES
BY
CONSTRUCTION
SUPERPIXELS
COVER
THE
ENTIRE
IMAGE
AND
DO
NOT
OVERLAP
MULTIPLE
SEGMENTATIONS
CAN
ALSO
BE
USED
TO
INCREASE
THE
POSSIBILITY
OF
OBJECT
BOUNDARIES
COINCIDING
WITH
BOUNDARIES
BETWEEN
ADJACENT
SUPERPIXELS
EXCEPT
FOR
SMALL
CONTOUR
DETAILS
AND
INVISIBLE
CONTOURS
ALL
SUPERPIXELS
EXTRACTED
FROM
AN
IMAGE
HAVE
SIMILAR
SCALE
SO
THE
METHOD
IS
NOT
SCALE
INVARIANT
AN
ALTERNATIVE
CONSTRUCTION
METHOD
BASED
ON
CONSTRAINED
DELAUNEY
TRIANGULATION
HAS
BEEN
PROPOSED
TO
OBTAIN
ROBUSTNESS
AGAINST
SCALE
CHANGE
THESE
FEATURES
ARE
LESS
SUITED
FOR
MATCHING
OR
OBJECT
RECOGNITION
AS
THE
REGIONS
ARE
UNIFORM
THEREFORE
NOT
DISCRIMINATIVE
AND
THE
REPEATA
BILITY
OF
BOUNDARY
EXTRACTION
IS
LOW
THEY
HAVE
BEEN
USED
SUCCESSFULLY
FOR
MODELING
AND
EXPLOITING
MID
LEVEL
VISUAL
CUES
SUCH
AS
CURVILINEAR
CONTINUITY
REGION
GROUPING
OR
FIGURE
GROUND
ORGANIZATION
FOR
SEMANTIC
IMAGE
SEGMENTATION
DISCUSSION
THE
LOCAL
FEATURES
DETECTED
WITH
THE
METHODS
DESCRIBED
ABOVE
TYPI
CALLY
REPRESENT
HOMOGENEOUS
REGIONS
WHILE
THIS
IS
ACCEPTABLE
FOR
THE
DETECTION
STEP
IT
MAY
INCUR
PROBLEMS
FOR
THE
LATER
DESCRIPTION
AND
MATCHING
INDEED
HOMOGENEOUS
REGIONS
LACK
DISTINCTIVENESS
FORTU
NATELY
THIS
CAN
EASILY
BE
OVERCOME
BY
INCREASING
THE
MEASUREMENT
REGION
IN
OTHER
WORDS
WE
USE
A
LARGER
SCALE
REGION
TO
COMPUTE
THE
DESCRIPTOR
SUCH
THAT
IT
ALSO
CONTAINS
PART
OF
THE
SURROUNDING
IMAGE
STRUCTURES
AND
CAPTURES
THE
SHAPE
OF
THE
REGION
BOUNDARY
THIS
USUALLY
SUFFICES
TO
INCREASE
THE
DISCRIMINATIVE
POWER
AND
MATCH
REGIONS
BETWEEN
IMAGES
INTENSITY
BASED
REGIONS
AND
MAXIMALLY
STABLE
EXTREMAL
REGIONS
TYP
ICALLY
GIVE
VERY
SIMILAR
FEATURES
THESE
METHODS
ARE
THEREFORE
NOT
COM
PLEMENTARY
IBR
MAY
BREAK
DOWN
WHEN
THE
REGION
IS
NON
CONVEX
BUT
IT
IS
MORE
ROBUST
TO
SMALL
GAPS
IN
THE
REGION
CONTOUR
MSER
ON
THE
OTHER
HAND
HAS
BEEN
SHOWN
TO
BE
RELATIVELY
SENSITIVE
TO
IMAGE
BLUR
IN
AS
THIS
DIRECTLY
AFFECTS
THE
STABILITY
CRITERION
THIS
PROBLEM
HAS
BEEN
RECENTLY
ADDRESSED
IN
HOWEVER
APART
FROM
THE
CASE
OF
IMAGE
BLUR
MSER
SCORES
BEST
WITH
RESPECT
TO
REPEATABILITY
IN
AS
DISCUSSED
EARLIER
REGION
DETECTORS
OFTEN
DETECT
BLOB
LIKE
STRUC
TURES
ALTHOUGH
THEY
ARE
NOT
RESTRICTED
TO
THIS
TYPE
OF
REGIONS
AS
A
RESULT
THEY
ARE
LESS
COMPLEMENTARY
TO
BLOBS
THEN
TO
CORNERS
REGION
BASED
DETECTORS
ARE
TYPICALLY
QUITE
ACCURATE
IN
THEIR
LOCAL
IZATION
THEY
WORK
ESPECIALLY
WELL
FOR
IMAGES
WITH
A
WELL
STRUCTURED
SCENE
CLEARLY
DELINEATED
REGIONS
SUCH
AS
IMAGES
CONTAINING
OBJECTS
WITH
PRINTED
SURFACES
BUILDINGS
ETC
EVEN
THOUGH
SUPERPIXELS
SHARE
SOME
CHARACTERISTICS
WITH
THE
OTHER
REGION
DETECTORS
THEY
ARE
NOT
THE
SAME
THEY
ARE
NON
OVERLAPPING
AND
COVER
THE
ENTIRE
IMAGE
THEIR
REPEATABILITY
SUFFERS
FROM
THE
WEAK
ROBUSTNESS
OF
THE
SEGMENTATION
METHODS
MOST
IMPORTANTLY
THEY
HAVE
BEEN
DEVELOPED
IN
A
DIFFERENT
CONTEXT
WHERE
THE
IDEA
IS
TO
SPEED
UP
THE
IMAGE
ANALYSIS
BY
FOCUSSING
ON
THE
SUPERPIXELS
ONLY
INSTEAD
OF
ANA
LYZING
ALL
PIXELS
SUPERPIXELS
ARE
HENCE
CONSIDERED
AS
A
BIGGER
EQUIV
ALENT
OF
PIXELS
WHICH
CAN
BE
DESCRIBED
TO
A
FIRST
APPROXIMATION
BY
DISCUSSION
A
SINGLE
INTENSITY
OR
COLOR
VALUE
THIS
IS
IN
CONTRAST
TO
LOCAL
FEATURES
WHICH
SHOULD
BE
DISTINCTIVE
AND
IN
THE
IDEAL
CASE
UNIQUELY
IDENTIFIABLE
HOWEVER
USING
REGION
BOUNDARIES
TO
BUILD
DISTINCTIVE
DESCRIPTORS
MAY
OVERCOME
THE
OCCLUSION
PROBLEM
FROM
WHICH
TRADITIONAL
INTEREST
POINTS
SUFFER
EFFICIENT
IMPLEMENTATIONS
MOST
FEATURE
DETECTORS
DESCRIBED
SO
FAR
INVOLVE
THE
COMPUTATION
OF
DERIVATIVES
OR
MORE
COMPLEX
MEASURES
SUCH
AS
THE
SECOND
MOMENT
MATRIX
FOR
THE
HARRIS
DETECTOR
OR
ENTROPY
FOR
THE
SALIENT
REGIONS
DETEC
TOR
SINCE
THIS
STEP
NEEDS
TO
BE
REPEATED
FOR
EACH
AND
EVERY
LOCATION
IN
FEATURE
COORDINATE
SPACE
WHICH
INCLUDES
POSITION
SCALE
AND
SHAPE
THIS
MAKES
THE
FEATURE
EXTRACTION
PROCESS
COMPUTATIONALLY
EXPENSIVE
THUS
NOT
SUITABLE
FOR
MANY
APPLICATIONS
IN
THIS
SECTION
WE
DESCRIBE
SEVERAL
FEATURE
DETECTORS
THAT
HAVE
BEEN
DEVELOPED
WITH
COMPUTATIONAL
EFFICIENCY
AS
ONE
OF
THE
MAIN
OBJECTIVES
THE
DOGS
DETECTOR
APPROXIMATES
THE
LAPLACIAN
USING
MULTIPLE
SCALE
SPACE
PYRAMIDS
SEE
SECTION
SURF
MAKES
USE
OF
INTEGRAL
IMAGES
TO
EFFICIENTLY
COMPUTE
A
ROUGH
APPROXIMATION
OF
THE
HESSIAN
MATRIX
SECTION
FAST
EVALUATES
ONLY
A
LIMITED
NUMBER
OF
INDIVIDUAL
PIXEL
INTENSITIES
USING
DECISION
TREES
SEE
SECTION
DIFFERENCE
OF
GAUSSIANS
THE
DIFFERENCE
OF
GAUSSIANS
DETECTOR
OR
DOG
FOR
SHORT
HAS
BEEN
PRO
POSED
IN
IT
IS
A
SCALE
INVARIANT
DETECTOR
WHICH
DIFFERENCE
OF
GAUSSIANS
EXTRACTS
BLOBS
IN
THE
IMAGE
BY
APPROXIMATING
THE
LAPLACIAN
SEE
ALSO
SECTION
BASED
ON
THE
DIFFUSION
EQUATION
IN
SCALE
SPACE
THEORY
IT
CAN
BE
SHOWN
THAT
THE
LAPLACIAN
CORRESPONDS
TO
THE
DERIVATIVE
OF
THE
IMAGE
IN
THE
SCALE
DIRECTION
SINCE
THE
DIFFER
ENCE
BETWEEN
NEIGHBORING
POINTS
IN
A
GIVEN
DIRECTION
APPROXIMATES
THE
DERIVATIVE
IN
THIS
DIRECTION
THE
DIFFERENCE
BETWEEN
IMAGES
AT
DIFFERENT
SCALES
APPROXIMATES
THE
DERIVATIVE
WITH
RESPECT
TO
SCALE
FURTHERMORE
GAUSSIAN
BLURRING
IS
OFTEN
APPLIED
TO
GENERATE
IMAGES
AT
VARIOUS
SCALES
HENCE
THE
DOG
IMAGES
PRODUCE
RESPONSES
WHICH
APPROXIMATE
THE
LOG
THE
COMPUTATION
OF
SECOND
ORDER
DERIVATIVES
IN
X
AND
Y
DIRECTIONS
IS
THEN
AVOIDED
AS
ILLUSTRATED
IN
FIGURE
THE
ACTUAL
COMPUTATION
SCHEME
IS
ILLUSTRATED
IN
FIGURE
THE
IMAGE
IS
SMOOTHED
SEVERAL
TIMES
WITH
A
GAUSSIAN
CONVOLUTION
MASK
THESE
SMOOTHED
VERSIONS
ARE
COMBINED
PAIRWISE
TO
COMPUTE
A
SET
OF
DOG
BLOB
RESPONSE
MAPS
LOCAL
MAXIMA
IN
THESE
MAPS
ARE
LOCATED
BOTH
OVER
SPACE
AND
OVER
SCALES
WITH
NON
MAXIMAL
SUPPRESSION
AND
THE
LOCATIONS
ARE
FURTHER
REFINED
WITH
QUADRATIC
INTERPOLATION
AFTER
A
FEW
SMOOTHING
STEPS
THE
IMAGE
CAN
BE
SUBSAMPLED
TO
PROCESS
THE
NEXT
OCTAVE
SINCE
THE
LAPLACIAN
GIVES
STRONG
RESPONSE
ON
EDGES
AN
ADDITIONAL
FILTERING
STEP
IS
ADDED
WHERE
THE
EIGENVALUES
OF
THE
FULL
HESSIAN
MATRIX
ARE
COMPUTED
AND
THEIR
STRENGTHS
EVALUATED
THIS
FILTERING
STEP
DOES
NOT
AFFECT
THE
OVERALL
PROCESSING
TIME
TOO
MUCH
AS
IT
IS
ONLY
NEEDED
FOR
A
LIMITED
NUMBER
OF
IMAGE
LOCATIONS
AND
SCALES
THE
DOG
FEATURES
DETECTED
IN
OUR
EXAMPLE
IMAGES
ARE
SHOWN
IN
FIGURE
SEVERAL
FRAMES
PER
SECOND
CAN
BE
PROCESSED
WITH
THIS
METHOD
FIG
THE
LAPLACIAN
CAN
BE
APPROXIMATED
AS
A
DIFFERENCE
OF
TWO
GAUSSIAN
SMOOTHED
IMAGES
FIG
OVERVIEW
OF
THE
DOG
DETECTION
SCHEME
FIG
LOCAL
FEATURES
DETECTED
WITH
THE
DOG
DETECTOR
SURF
SPEEDED
UP
ROBUST
FEATURES
IN
THE
CONTEXT
OF
REALTIME
FACE
DETECTION
VIOLA
AND
JONES
HAVE
PROPOSED
TO
USE
INTEGRAL
IMAGES
WHICH
ALLOW
FOR
VERY
FAST
COMPUTATION
OF
HAAR
WAVELETS
OR
ANY
BOX
TYPE
CONVOLUTION
FILTER
FIRST
WE
WILL
DESCRIBE
THE
BASIC
IDEA
OF
INTEGRAL
IMAGES
THEN
WE
SHOW
HOW
THIS
TECHNIQUE
CAN
BE
USED
TO
OBTAIN
A
FAST
APPROXIMATION
OF
THE
HESSIAN
MATRIX
AS
USED
IN
SURF
SPEEDED
UP
ROBUST
FEATURES
INTEGRAL
IMAGES
THE
ENTRY
OF
AN
INTEGRAL
IMAGE
IΣ
X
AT
A
LOCATION
X
X
Y
REPRESENTS
THE
SUM
OF
ALL
PIXELS
IN
THE
INPUT
IMAGE
I
OF
A
RECTANGULAR
REGION
FORMED
FIG
USING
INTEGRAL
IMAGES
IT
TAKES
ONLY
FOUR
OPERATIONS
TO
CALCULATE
THE
AREA
OF
A
RECTANGULAR
REGION
OF
ANY
SIZE
BY
THE
ORIGIN
AND
X
I
X
J
Y
IΣ
X
I
I
J
I
J
ONCE
THE
INTEGRAL
IMAGE
HAS
BEEN
COMPUTED
IT
TAKES
FOUR
ADDITIONS
TO
CALCULATE
THE
SUM
OF
THE
INTENSITIES
OVER
ANY
UPRIGHT
RECTANGULAR
AREA
AS
SHOWN
IN
FIGURE
MOREOVER
THE
CALCULATION
TIME
IS
INDE
PENDENT
OF
THE
SIZE
OF
THE
RECTANGULAR
AREA
SURF
SURF
OR
SPEEDED
UP
ROBUST
FEATURES
HAVE
BEEN
PROPOSED
BY
BAY
ET
AL
IT
IS
A
SCALE
INVARIANT
FEATURE
DETECTOR
BASED
ON
THE
HESSIAN
MATRIX
AS
IS
E
G
THE
HESSIAN
LAPLACE
DETECTOR
SEE
SEC
TION
HOWEVER
RATHER
THAN
USING
A
DIFFERENT
MEASURE
FOR
SELECTING
THE
LOCATION
AND
THE
SCALE
THE
DETERMINANT
OF
THE
HESSIAN
IS
USED
FOR
BOTH
THE
HESSIAN
MATRIX
IS
ROUGHLY
APPROXIMATED
USING
A
SET
OF
BOX
TYPE
FILTERS
AND
NO
SMOOTHING
IS
APPLIED
WHEN
GOING
FROM
ONE
SCALE
TO
THE
NEXT
GAUSSIANS
ARE
OPTIMAL
FOR
SCALE
SPACE
ANALYSIS
BUT
IN
PRACTICE
THEY
HAVE
TO
BE
DISCRETIZED
FIGURE
LEFT
WHICH
INTRODUCES
ARTIFACTS
IN
PARTICULAR
IN
SMALL
GAUSSIAN
KERNELS
SURF
PUSHES
THE
APPROXIMATION
EVEN
FURTHER
USING
THE
BOX
FILTERS
AS
SHOWN
IN
THE
RIGHT
FIG
LEFT
TO
RIGHT
THE
DISCRETISED
AND
CROPPED
GAUSSIAN
SECOND
ORDER
PARTIAL
DERIVATIVE
IN
Y
DIRECTION
AND
XY
DIRECTION
RESPECTIVELY
SURF
BOX
FILTER
APPROXIMATION
FOR
THE
SECOND
ORDER
GAUSSIAN
PARTIAL
DERIVATIVE
IN
Y
DIRECTION
AND
XY
DIRECTION
THE
GRAY
REGIONS
ARE
EQUAL
TO
ZERO
HALF
OF
FIGURE
THESE
APPROXIMATE
SECOND
ORDER
GAUSSIAN
DERIVA
TIVES
AND
CAN
BE
EVALUATED
VERY
FAST
USING
INTEGRAL
IMAGES
INDEPEN
DENTLY
OF
THEIR
SIZE
SURPRISINGLY
IN
SPITE
OF
THE
ROUGH
APPROXIMATIONS
THE
PERFORMANCE
OF
THE
FEATURE
DETECTOR
IS
COMPARABLE
TO
THE
RESULTS
OBTAINED
WITH
THE
DISCRETIZED
GAUSSIANS
BOX
FILTERS
CAN
PRODUCE
A
SUF
FICIENT
APPROXIMATION
OF
THE
GAUSSIAN
DERIVATIVES
AS
THERE
ARE
MANY
OTHER
SOURCES
OF
SIGNIFICANT
NOISE
IN
THE
PROCESSING
CHAIN
THE
BOX
FILTERS
IN
FIGURE
ARE
APPROXIMATIONS
FOR
A
GAUSSIAN
WITH
Σ
AND
REPRESENT
THE
FINEST
SCALE
I
E
HIGHEST
SPATIAL
RESOLU
TION
WE
WILL
DENOTE
THEM
BY
DXX
DYY
AND
DXY
THE
WEIGHTS
APPLIED
TO
THE
RECTANGULAR
REGIONS
ARE
KEPT
SIMPLE
FOR
COMPUTATIONAL
EFFICIENCY
BUT
WE
NEED
TO
FURTHER
BALANCE
THE
RELATIVE
WEIGHTS
IN
THE
EXPRESSION
FOR
THE
HESSIAN
DETERMINANT
WITH
LXY
F
DXX
YY
F
FOR
THE
SMALLEST
SCALE
WHERE
LXX
YY
F
DXY
F
X
F
IS
THE
FROBENIUS
NORM
THIS
YIELDS
DET
HAPPROX
DXXDYY
THE
APPROXIMATED
DETERMINANT
OF
THE
HESSIAN
REPRESENTS
THE
BLOB
RESPONSE
IN
THE
IMAGE
AT
LOCATION
X
THESE
RESPONSES
ARE
STORED
IN
A
BLOB
RESPONSE
MAP
AND
LOCAL
MAXIMA
ARE
DETECTED
AND
REFINED
USING
QUADRATIC
INTERPOLATION
AS
WITH
DOG
SEE
SECTION
FIGURE
SHOWS
THE
RESULT
OF
THE
SURF
DETECTOR
FOR
OUR
EXAMPLE
IMAGES
SURF
HAS
BEEN
REPORTED
TO
BE
MORE
THAN
FIVE
TIMES
FASTER
THAN
DOG
FAST
FEATURES
FROM
ACCELERATED
SEGMENT
TEST
THE
FAST
DETECTOR
INTRODUCED
BY
ROSTEN
AND
DRUMMOND
IN
BUILDS
ON
THE
SUSAN
DETECTOR
PREVIOUSLY
DISCUSSED
IN
SECTION
SUSAN
COMPUTES
THE
FRACTION
OF
PIXELS
WITHIN
A
NEIGHBORHOOD
WHICH
FIG
LOCAL
FEATURES
DETECTED
WITH
THE
SURF
DETECTOR
HAVE
SIMILAR
INTENSITY
TO
THE
CENTER
PIXEL
THIS
IDEA
IS
TAKEN
FURTHER
BY
FAST
WHICH
COMPARES
PIXELS
ONLY
ON
A
CIRCLE
OF
FIXED
RADIUS
AROUND
THE
POINT
THE
TEST
CRITERION
OPERATES
BY
CONSIDERING
A
CIRCLE
OF
PIX
ELS
AROUND
THE
CORNER
CANDIDATE
SEE
FIGURE
INITIALLY
PIXELS
AND
ARE
COMPARED
WITH
A
THRESHOLD
THEN
AND
AS
WELL
AS
THE
REMAIN
ING
ONES
AT
THE
END
THE
PIXELS
ARE
CLASSIFIED
INTO
DARK
SIMILAR
AND
BRIGHTER
SUBSETS
THE
ALGORITHM
FROM
IS
USED
TO
SELECT
THE
PIXELS
WHICH
YIELD
THE
MOST
INFORMATION
ABOUT
WHETHER
THE
CANDIDATE
PIXEL
IS
A
CORNER
THIS
IS
MEASURED
BY
THE
ENTROPY
OF
THE
POSITIVE
AND
FIG
ILLUSTRATION
OF
PIXELS
EXAMINED
BY
THE
FAST
DETECTOR
FIG
LOCAL
FEATURES
DETECTED
WITH
THE
FAST
DETECTOR
NEGATIVE
CORNER
CLASSIFICATION
RESPONSES
BASED
ON
THIS
PIXEL
THE
PRO
CESS
IS
APPLIED
RECURSIVELY
ON
ALL
THREE
SUBSETS
AND
TERMINATES
WHEN
THE
ENTROPY
OF
A
SUBSET
IS
ZERO
THE
DECISION
TREE
RESULTING
FROM
THIS
PARTITIONING
IS
THEN
CONVERTED
INTO
C
CODE
CREATING
A
LONG
STRING
OF
NESTED
IF
THEN
ELSE
STATEMENTS
WHICH
IS
COMPILED
AND
USED
AS
A
CORNER
DETECTOR
FINALLY
NON
MAXIMA
SUPPRESSION
IS
APPLIED
ON
THE
SUM
OF
THE
ABSOLUTE
DIFFERENCE
BETWEEN
THE
PIXELS
IN
THE
CIRCLE
AND
THE
CENTER
PIXEL
THIS
RESULTS
IN
A
VERY
EFFICIENT
DETECTOR
WHICH
IS
UP
TO
TIMES
FASTER
THAN
THE
DOG
DETECTOR
DISCUSSED
IN
SECTION
ALBEIT
NOT
INVARIANT
TO
SCALE
CHANGES
THE
FAST
FEATURES
FOUND
IN
OUR
EXAMPLE
IMAGES
ARE
DISPLAYED
IN
FIGURE
AN
EXTENSION
TO
A
MULTI
SCALE
DETECTOR
BY
SCALE
SELECTION
WITH
THE
LAPLACIAN
FUNCTION
WAS
PROPOSED
IN
THEY
ESTIMATE
THE
LAPLACIAN
USING
GRAY
LEVEL
DIFFERENCES
BETWEEN
PIXELS
ON
THE
CIRCLE
AND
THE
CENTRAL
ONE
AND
RETAIN
ONLY
THE
LOCATIONS
WHERE
THIS
ESTIMATE
IS
LARGEST
THIS
PROVES
TO
BE
SUFFICIENT
TO
PRODUCE
A
LARGE
NUMBER
OF
KEYPOINT
CANDIDATES
FROM
WHICH
THE
UNSTABLE
ONES
ARE
FILTERED
OUT
DURING
THE
RECOGNITION
PROCESS
DISCUSSION
THE
ULTIMATE
GOAL
OF
METHODS
FOCUSSING
ON
EFFICIENCY
IS
OFTEN
REALTIME
PROCESSING
OF
A
VIDEO
STREAM
OR
DEALING
WITH
LARGE
AMOUNTS
OF
DATA
DISCUSSION
HOWEVER
TO
SOME
EXTENT
THIS
IS
A
MOVING
TARGET
COMPUTATION
POWER
INCREASES
RAPIDLY
OVER
TIME
BUT
SO
DOES
THE
NUMBER
OF
FEATURES
WE
EXTRACT
OR
THE
SIZE
OF
THE
DATABASES
WE
DEAL
WITH
MOREOVER
FEATURE
DETECTION
IS
NOT
THE
FINAL
GOAL
BUT
JUST
THE
FIRST
STEP
IN
A
PROCESSING
CHAIN
FOLLOWED
BY
MATCHING
TRACKING
OBJECT
RECOGNITION
ETC
IN
MANY
APPLICATIONS
SIGNIFICANT
PERFORMANCE
IMPROVEMENT
CAN
BE
OBTAINED
JUST
BY
INCREASING
THE
NUMBER
OF
TRAINING
EXAMPLES
EFFICIENCY
IS
THEREFORE
ONE
OF
THE
MAJOR
PROPERTIES
EQUALLY
IMPORTANT
TO
INVARIANCE
OR
ROBUST
NESS
WHICH
SHOULD
BE
CONSIDERED
WHEN
DESIGNING
OR
SELECTING
A
FEATURE
DETECTOR
COMING
BACK
TO
THE
FIRST
POINT
ESPECIALLY
THE
ADVENT
OF
POWERFUL
GRAPHICAL
PROCESSING
UNITS
OPENS
UP
NEW
POSSIBILITIES
APART
FROM
THE
METHODS
DESCRIBED
ABOVE
WHICH
OBTAIN
A
SPEEDUP
BY
PLATFORM
INDEPEN
DENT
ALGORITHMIC
CHANGES
FURTHER
SPEEDUPS
BECOME
POSSIBLE
BY
EXPLOIT
ING
THE
SPECIAL
STRUCTURE
AND
PARALLELISM
THAT
CAN
BE
REALIZED
WITH
GPUS
SOME
EXAMPLES
OF
SUCH
WORK
CAN
BE
FOUND
IN
AN
FPGA
BASED
IMPLEMENTATION
OF
THE
HARRIS
AFFINE
FEATURE
DETECTOR
SEE
SECTION
IS
DISCUSSED
IN
AND
OF
THE
DOG
DETECTOR
SEE
SECTION
IN
THIS
SIGNIFICANTLY
REDUCES
THE
TIME
NEEDED
TO
COMPUTE
ALL
FEA
TURES
ON
A
NORMAL
SIZED
IMAGE
AND
ENABLES
VIDEO
FRAME
RATE
PROCESSING
IN
SPITE
OF
THIS
NEW
TREND
THE
BASIC
IDEAS
AND
METHODS
DESCRIBED
IN
THIS
SECTION
STILL
HOLD
AS
THEY
ARE
SUFFICIENTLY
GENERAL
AND
WIDELY
APPLICABLE
FINALLY
MORE
EFFICIENT
METHODS
USUALLY
COME
AT
A
PRICE
A
TRADE
OFF
HAS
TO
BE
MADE
BETWEEN
EFFICIENCY
ON
THE
ONE
HAND
AND
ACCURACY
OR
REPEATABILITY
ON
THE
OTHER
HAND
SURPRISINGLY
THE
DOG
SURF
AND
FAST
DETECTORS
ARE
COMPETITIVE
WITH
THE
STANDARD
MORE
COMPUTATION
ALLY
EXPENSIVE
FEATURE
DETECTORS
AND
MAY
PRODUCE
BETTER
RESULTS
FOR
SOME
APPLICATIONS
DISCUSSION
AND
CONCLUSION
IN
THIS
FINAL
SECTION
OF
OUR
SURVEY
WE
GIVE
AN
OVERVIEW
OF
THE
PRE
VIOUSLY
DISCUSSED
METHODS
AND
HIGHLIGHT
THEIR
RESPECTIVE
STRENGTHS
AND
WEAKNESSES
WE
GIVE
SOME
HINTS
ON
HOW
TO
USE
THESE
FEATURES
AND
ON
HOW
TO
SELECT
THE
APPROPRIATE
FEATURE
DETECTOR
FOR
A
GIVEN
APPLICATION
FINALLY
WE
DISCUSS
SOME
OPEN
ISSUES
AND
FUTURE
RESEARCH
DIRECTIONS
HOW
TO
SELECT
YOUR
FEATURE
DETECTOR
BELOW
WE
GIVE
A
FEW
GUIDELINES
ON
WHAT
FEATURE
DETECTOR
TO
USE
FOR
A
GIVEN
APPLICATION
THIS
DOES
NOT
GIVE
A
PRECISE
AND
DEFINITIVE
ANSWER
BUT
INDICATES
A
FEW
POINTS
ONE
NEEDS
TO
CONSIDER
WHEN
SEARCHING
FOR
A
SUITABLE
DETECTOR
WE
REFER
THE
READER
TO
SECTION
WHERE
WE
DEFINE
THE
PROPERTIES
OF
LOCAL
FEATURES
OFTEN
MENTIONED
HERE
FIRST
WE
ORGANIZED
THE
FEATURE
DETECTORS
IN
THIS
SURVEY
BASED
ON
THE
TYPE
OF
IMAGE
STRUCTURES
THEY
EXTRACT
CORNERS
BLOBS
OR
REGIONS
DEPENDING
ON
THE
IMAGE
CONTENT
SOME
OF
THESE
IMAGE
STRUCTURES
ARE
MORE
COMMON
THAN
OTHERS
THUS
THE
NUMBER
OF
FEATURES
FOUND
WITH
A
GIVEN
DETECTOR
MAY
VARY
FOR
DIFFERENT
IMAGE
CATEGORIES
IF
LITTLE
IS
KNOWN
ABOUT
THE
IMAGE
CONTENT
IN
ADVANCE
IT
IS
GENERALLY
RECOMMENDED
TO
HOW
TO
SELECT
YOUR
FEATURE
DETECTOR
COMBINE
DIFFERENT
COMPLEMENTARY
DETECTORS
I
E
EXTRACTING
DIFFERENT
TYPES
OF
FEATURES
SECOND
FEATURE
DETECTORS
CAN
BE
DISTINGUISHED
BASED
ON
THE
LEVEL
OF
INVARIANCE
THERE
HAVE
BEEN
MANY
EVALUATIONS
WHICH
FOCUS
ON
THIS
PROPERTY
ONE
MIGHT
BE
TEMPTED
TO
ALWAYS
SELECT
THE
HIGHEST
LEVEL
OF
INVARIANCE
AVAILABLE
SO
AS
TO
COMPENSATE
FOR
AS
MUCH
VARIABILITY
AS
POSSIBLE
HOWEVER
THE
DISCRIMINATIVE
POWER
OF
FEATURES
IS
REDUCED
AT
INCREASED
LEVELS
OF
INVARIANCE
AS
MORE
PATTERNS
ARE
TO
BE
JUDGED
EQUIVALENT
THERE
IS
MORE
PARAMETERS
TO
ESTIMATE
THUS
MORE
POSSIBLE
SOURCES
OF
NOISE
ALSO
THE
FEATURE
DETECTION
PROCESS
BECOMES
MORE
COMPLEX
WHICH
AFFECTS
BOTH
THE
COMPUTATIONAL
COMPLEXITY
AS
WELL
AS
THE
REPEATABILITY
AS
A
RESULT
A
BASIC
RULE
OF
THUMB
IS
TO
USE
NO
MORE
INVARIANCE
THAN
WHAT
IS
TRULY
NEEDED
BY
THE
APPLICATION
AT
HAND
MORE
OVER
IF
THE
EXPECTED
TRANSFORMATIONS
ARE
RELATIVELY
SMALL
IT
IS
OFTEN
BETTER
TO
COUNT
ON
THE
ROBUSTNESS
OF
THE
FEATURE
DETECTION
AND
DESCRIP
TION
RATHER
THAN
TO
INCREASE
THE
LEVEL
OF
INVARIANCE
THAT
IS
ALSO
THE
REASON
WHY
FEATURE
DETECTORS
INVARIANT
TO
PERSPECTIVE
TRANSFORMATIONS
ARE
OF
LITTLE
USE
ALL
DETECTORS
DISCUSSED
IN
THIS
SURVEY
ARE
INVARIANT
TO
TRANSLATIONS
AND
ROTATIONS
THE
FORMER
AUTOMATICALLY
FOLLOWS
FROM
THE
USE
OF
LOCAL
FEATURES
THE
LATTER
CAN
BE
ACHIEVED
RELATIVELY
EASILY
AT
LIMITED
EXTRA
COST
SOMETIMES
ROTATION
INVARIANCE
IS
NOT
REQUIRED
E
G
IF
ALL
IMAGES
ARE
TAKEN
UPRIGHT
AND
THE
OBJECTS
ARE
ALWAYS
UPRIGHT
AS
WELL
BUILDINGS
CARS
ETC
IN
THESE
CASES
THE
ROTATION
INVARIANT
DETECTORS
CAN
BE
COM
BINED
WITH
A
ROTATION
VARIANT
DESCRIPTOR
TO
ENSURE
GOOD
DISCRIMINATIVE
POWER
IN
ALL
OTHER
CASES
A
DESCRIPTOR
WITH
AT
MOST
THE
SAME
LEVEL
OF
INVARIANCE
AS
THE
DETECTOR
IS
PREFERRED
FINALLY
THERE
ARE
A
NUMBER
OF
QUALITATIVE
PROPERTIES
OF
THE
DETEC
TORS
TO
CONSIDER
DEPENDING
ON
THE
APPLICATION
SCENARIO
SOME
OF
THESE
PROPERTIES
ARE
MORE
CRUCIAL
THAN
OTHERS
WHEN
DEALING
WITH
CATEGORY
LEVEL
OBJECT
RECOGNITION
ROBUSTNESS
TO
SMALL
APPEARANCE
VARIATIONS
IS
IMPORTANT
TO
DEAL
WITH
THE
WITHIN
CLASS
VARIABILITY
WHEN
FITTING
A
PARA
METRIC
MODEL
TO
THE
DATA
AS
FOR
CAMERA
CALIBRATION
OR
MODELING
THE
LOCALIZATION
ACCURACY
IS
ESSENTIAL
FOR
ONLINE
APPLICATIONS
OR
APPLI
CATIONS
WHERE
A
LARGE
AMOUNT
OF
DATA
NEEDS
TO
BE
PROCESSED
EFFICIENCY
IS
THE
MOST
IMPORTANT
CRITERION
SUMMARY
ON
THE
DETECTORS
TABLE
GIVES
AN
OVERVIEW
OF
THE
MOST
IMPORTANT
PROPERTIES
FOR
THE
FEATURE
DETECTORS
DESCRIBED
IN
SECTIONS
THE
FEATURE
DETECTORS
IN
TABLE
ARE
ORGANIZED
IN
GROUPS
ACCORD
ING
TO
THEIR
INVARIANCE
ROTATION
SIMILARITY
AFFINE
AND
PERSPECTIVE
WE
COMPARE
THE
PROPERTIES
WITHIN
EACH
GROUP
FOR
ROTATION
INVARIANT
FEATURES
THE
HIGHEST
REPEATABILITY
AND
LOCALIZATION
ACCURACY
IN
MANY
TESTS
HAS
BEEN
OBTAINED
BY
THE
HARRIS
DETECTOR
THE
HESSIAN
DETECTOR
FINDS
BLOBS
WHICH
ARE
NOT
AS
WELL
LOCALIZED
AND
REQUIRES
SECOND
ORDER
DERIVATIVES
TO
BE
COMPUTED
THE
SUSAN
DETECTOR
AVOIDS
COMPUTATION
OF
DERIVATIVES
AND
IS
KNOWN
FOR
ITS
EFFICIENCY
HOWEVER
THE
ABSENCE
OF
SMOOTHING
MAKES
IT
MORE
SUSCEPTIBLE
TO
NOISE
ALL
THE
ROTATION
INVARIANT
METHODS
ARE
SUITABLE
FOR
APPLICATIONS
WHERE
ONLY
THE
SPATIAL
LOCATION
OF
THE
FEATURES
IS
USED
AND
NO
LARGE
SCALE
CHANGES
ARE
EXPECTED
E
G
STRUCTURE
FROM
MOTION
OR
CAMERA
CALIBRATION
IN
THE
SCALE
INVARIANT
GROUP
HARRIS
LAPLACE
SHOWS
HIGH
REPEATABIL
ITY
AND
LOCALIZATION
ACCURACY
INHERITED
FROM
THE
HARRIS
DETECTOR
HOWEVER
ITS
SCALE
ESTIMATION
IS
LESS
ACCURATE
DUE
TO
THE
MULTISCALE
NATURE
OF
CORNERS
HESSIAN
LAPLACE
IS
MORE
ROBUST
THAN
ITS
SINGLE
SCALE
VERSION
THIS
IS
DUE
TO
THE
FACT
THAT
BLOB
LIKE
STRUCTURES
ARE
BET
TER
LOCALIZED
IN
SCALE
THAN
CORNERS
AND
THE
DETECTOR
BENEFITS
FROM
MUL
TISCALE
ANALYSIS
ALTHOUGH
IT
IS
LESS
ACCURATELY
LOCALIZED
IN
THE
IMAGE
PLANE
DOG
AND
SURF
DETECTORS
WERE
DESIGNED
FOR
EFFICIENCY
AND
THE
OTHER
PROPERTIES
ARE
SLIGHTLY
COMPROMISED
HOWEVER
FOR
MOST
APPLICA
TIONS
THEY
ARE
STILL
MORE
THAN
SUFFICIENT
QUANTITY
AND
GOOD
COVERAGE
OF
THE
IMAGE
ARE
CRUCIAL
IN
RECOGNITION
APPLICATIONS
WHERE
LOCALIZATION
ACCURACY
IS
LESS
IMPORTANT
THUS
HESSIAN
LAPLACE
DETECTORS
HAVE
BEEN
SUCCESSFUL
IN
VARIOUS
CATEGORIZATION
TASKS
ALTHOUGH
THERE
ARE
DETECTORS
WITH
HIGHER
REPEATABILITY
RATE
RANDOM
AND
DENSE
SAMPLING
ALSO
PROVIDE
GOOD
RESULTS
IN
THIS
CONTEXT
WHICH
CONFIRMS
THE
COVERAGE
REQUIREMENTS
OF
RECOGNITION
METHODS
ALTHOUGH
THEY
RESULT
IN
FAR
LESS
COM
PACT
REPRESENTATIONS
THAN
THE
INTEREST
POINTS
DOG
DETECTOR
PERFORMS
EXTREMELY
WELL
IN
MATCHING
AND
IMAGE
RETRIEVAL
PROBABLY
DUE
TO
A
GOOD
BALANCE
BETWEEN
SPATIAL
LOCALIZATION
AND
SCALE
ESTIMATION
ACCURACY
TABLE
OVERVIEW
OF
FEATURE
DETECTORS
NOTE
THAT
FOR
THE
SCALE
AND
AFFINE
INVARIANT
DETECTORS
THE
DIFFERENCE
BETWEEN
CORNER
AND
BLOB
DETECTORS
BECOMES
LESS
OUTSPOKEN
WITH
MOST
DETECTORS
DETECTING
A
MIXTURE
OF
BOTH
FEATURE
TYPES
ALTHOUGH
THEY
STILL
SHOW
A
PREFERENCE
FOR
EITHER
TYPE
THE
AFFINE
INVARIANT
HARRIS
AND
HESSIAN
FOLLOW
THE
OBSERVATIONS
FROM
PREVIOUS
GROUPS
SALIENT
REGIONS
REQUIRE
TO
COMPUTE
A
HISTOGRAM
AND
ITS
ENTROPY
FOR
EACH
REGION
CANDIDATE
IN
SCALE
OR
AFFINE
SPACE
WHICH
RESULTS
IN
LARGE
COMPUTATIONAL
COST
ON
THE
POSITIVE
SIDE
THE
REGIONS
CAN
BE
RANKED
ACCORDING
TO
THEIR
COMPLEXITY
OR
INFORMATION
CONTENT
SOME
APPLICATIONS
EXPLOIT
THIS
AND
USE
ONLY
A
SMALL
SUBSET
OF
THE
SALIENT
REGIONS
WHILE
STILL
OBTAINING
A
GOOD
PERFORMANCE
IN
E
G
RECOGNITION
ORIGINALLY
THEY
WERE
ONLY
SCALE
INVARIANT
BUT
LATER
THEY
HAVE
BEEN
EXTENDED
TO
AFFINE
INVARIANCE
THE
EDGE
BASED
REGIONS
FOCUS
ON
CORNERS
FORMED
BY
EDGE
JUNCTIONS
WHICH
GIVES
GOOD
LOCALIZA
TION
ACCURACY
AND
REPEATABILITY
BUT
THE
NUMBER
OF
DETECTED
FEATURES
IS
SMALL
THE
REGION
DETECTORS
ARE
BASED
ON
THE
IDEA
OF
SEGMENTING
BOUND
ARIES
OF
UNIFORM
REGIONS
INTENSITY
BASED
REGIONS
USE
A
HEURISTIC
METHOD
AND
FIND
SIMILAR
REGIONS
TO
MSER
SUPERPIXELS
ARE
TYPICALLY
BASED
ON
SEGMENTATION
METHODS
WHICH
ARE
COMPUTATIONALLY
EXPENSIVE
LIKE
NOR
MALIZED
CUTS
THE
LEVEL
OF
INVARIANCE
OF
SUPERPIXELS
DEPENDS
MOSTLY
ON
THE
SEGMENTATION
ALGORITHM
USED
IN
CONTRAST
TO
SUPERPIXELS
MSER
SELECTS
ONLY
THE
MOST
STABLE
REGIONS
WHICH
RESULTS
IN
HIGH
REPEATABIL
ITY
MSER
IS
ALSO
EFFICIENT
DUE
TO
THE
USE
OF
A
WATERSHED
SEGMENTA
TION
ALGORITHM
AFFINE
INVARIANT
DETECTORS
ARE
BENEFICIAL
IN
CASES
WHERE
EXTREME
GEOMETRIC
DEFORMATIONS
ARE
EXPECTED
OTHERWISE
THEIR
SCALE
INVARIANT
COUNTERPARTS
USUALLY
PERFORM
BETTER
IN
PARTICULAR
FOR
CATEGORY
RECOGNITION
GIVEN
A
STABLE
LOCATION
SCALE
AND
ORIENTATION
FOR
EACH
KEY
IT
IS
NOW
POSSIBLE
TO
DESCRIBE
THE
LOCAL
IMAGE
REGION
IN
A
MANNER
INVARIANT
TO
THESE
TRANSFORMATIONS
IN
ADDITION
IT
IS
DESIRABLE
TO
MAKE
THIS
REPRESENTATION
ROBUST
AGAINST
SMALL
SHIFTS
IN
LOCAL
GEOMETRY
SUCH
AS
ARISE
FROM
AFFINE
OR
PROJECTION
ONE
APPROACH
TO
THIS
IS
SUGGESTED
BY
THE
RESPONSE
PROPERTIES
OF
COMPLEX
NEURONS
IN
THE
VISUAL
CORTEX
IN
WHICH
A
FEATURE
POSITIONIS
ALLOWED
TO
VARY
OVER
A
SMALL
REGIONWHILE
ORIENTATION
AND
SPATIAL
FREQUENCY
SPECIFICITY
ARE
MAINTAINED
EDELMAN
INTRATOR
POGGIO
HAVE
PERFORMED
EXPERIMENTS
THAT
SIMULATED
THE
RESPONSES
OF
COMPLEX
NEURONS
TO
DIFFERENT
VIEWS
OF
COMPUTER
GRAPHIC
MODELS
AND
FOUND
THAT
THE
COMPLEX
CELL
OUTPUTS
PROVIDED
MUCH
BETTER
DISCRIMINATION
THAN
SIMPLE
CORRELATION
BASEDMATCHING
THIS
CAN
BE
SEEN
FOR
EXAMPLE
IF
AN
AFFINE
PROJECTION
STRETCHES
AN
IMAGE
IN
ONE
DIRECTION
RELATIVE
TO
ANOTHER
WHICH
CHANGES
THE
RELATIVE
LOCATIONS
OF
GRADIENT
FEATURES
WHILE
HAVING
A
SMALLER
EFFECT
ON
THEIR
ORIENTATIONS
AND
SPATIAL
FREQUENCIES
THIS
ROBUSTNESS
TO
LOCAL
GEOMETRIC
DISTORTION
CAN
BE
OBTAINED
BY
REPRESENTING
THE
LOCAL
IMAGE
REGION
WITH
MULTIPLE
IMAGES
REPRESENTING
EACH
OF
A
NUMBER
OF
ORIENTATIONS
REFERRED
TO
AS
ORIENTATION
PLANES
EACH
ORIENTATION
PLANE
CONTAINS
ONLY
THE
GRADIENTS
CORRESPONDING
TO
THAT
ORIENTATION
WITH
LINEAR
INTERPOLATION
USED
FOR
INTERMEDIATE
ORIENTATIONS
EACH
ORIENTATION
PLANE
IS
BLURRED
AND
RESAMPLED
TO
ALLOW
FOR
LARGER
SHIFTS
IN
POSITIONS
OF
THE
GRADIENTS
THIS
APPROACH
CAN
BE
EFFICIENTLY
IMPLEMENTED
BY
USING
THE
SAME
PRECOMPUTED
GRADIENTS
AND
ORIENTATIONS
FOR
EACH
LEVEL
OF
THE
PYRAMID
THAT
WERE
USED
FOR
ORIENTATION
SELECTION
FOR
EACH
KEYPOINT
WE
USE
THE
PIXEL
SAMPLING
FROM
THE
PYRAMID
LEVEL
AT
WHICH
THE
KEY
WAS
DETECTED
THE
PIXELS
THAT
FALL
IN
A
CIRCLE
OF
RADIUS
PIXELS
AROUND
THE
KEY
LOCATION
ARE
INSERTED
INTO
THE
ORIENTATION
PLANES
THE
ORIENTATION
IS
MEASURED
RELATIVE
TO
THAT
OF
THE
KEY
BY
SUBTRACTING
THE
KEY
ORIENTATION
FOR
OUR
EXPERIMENTS
WE
USED
ORIENTATION
PLANES
EACH
SAMPLED
OVER
A
GRID
OF
LOCATIONS
WITH
A
SAMPLE
SPACING
TIMES
THAT
OF
THE
PIXEL
SPACING
USED
FOR
GRADIENT
DETECTION
THE
BLURRING
IS
ACHIEVED
BY
ALLOCATING
THE
GRADIENT
OF
EACH
PIXEL
AMONG
ITS
CLOSEST
NEIGHBORS
IN
THE
SAMPLE
GRID
USING
LINEAR
INTERPOLATIONIN
ORIENTATION
AND
THE
TWO
SPATIAL
DIMENSIONS
THIS
IMPLEMENTATION
IS
MUCH
MORE
EFFICIENT
THAN
PERFORMING
EXPLICIT
BLURRING
AND
RESAMPLING
YET
GIVES
ALMOST
EQUIVALENT
RESULTS
IN
ORDER
TO
SAMPLE
THE
IMAGE
AT
A
LARGER
SCALE
THE
SAME
PROCESS
IS
REPEATED
FOR
A
SECOND
LEVEL
OF
THE
PYRAMID
ONE
OCTAVE
HIGHER
HOWEVER
THIS
TIME
A
RATHER
THAN
A
SAMPLE
REGION
IS
USED
THIS
MEANS
THAT
APPROXIMATELY
THE
SAME
IMAGE
REGION
WILL
BE
EXAMINED
AT
BOTH
SCALES
SO
THAT
ANY
NEARBY
OCCLUSIONSWILL
NOT
AFFECT
ONE
SCALE
MORE
THAN
THE
OTHER
THEREFORE
THE
TOTAL
NUMBER
OF
SAMPLES
IN
THE
SIFT
KEY
VECTOR
FROM
BOTH
SCALES
IS
OR
ELEMENTS
GIVING
ENOUGH
MEASUREMENTS
FOR
HIGH
SPECIFICITY
INDEXING
AND
MATCHING
FOR
INDEXING
WE
NEED
TO
STORE
THE
SIFT
KEYS
FOR
SAMPLE
IMAGES
AND
THEN
IDENTIFYMATCHING
KEYS
FROMNEW
IMAGES
THE
PROBLEMOF
IDENTIFYINGTHEMOST
SIMILAR
KEYS
FOR
HIGH
DIMEN
SIONAL
VECTORS
IS
KNOWN
TO
HAVE
HIGH
COMPLEXITY
IF
AN
EXACT
SOLUTION
IS
REQUIRED
HOWEVER
A
MODIFICATION
OF
THE
K
D
TREE
ALGORITHM
CALLED
THE
BEST
BIN
FIRST
SEARCH
METHOD
BEIS
LOWE
CAN
IDENTIFY
THE
NEAREST
NEIGHBORS
WITH
HIGH
PROBABILITY
USING
ONLY
A
LIMITED
AMOUNT
OF
COMPUTATION
TO
FURTHER
IMPROVE
THE
EFFICIENCY
OF
THE
BEST
BIN
FIRST
ALGORITHM
THE
SIFT
KEY
SAMPLES
GENERATED
AT
THE
LARGER
SCALE
ARE
GIVEN
TWICE
THE
WEIGHT
OF
THOSE
AT
THE
SMALLER
SCALE
THIS
MEANS
THAT
THE
LARGER
SCALE
IS
IN
EFFECT
ABLE
TO
FILTER
THE
MOST
LIKELY
NEIGHBOURS
FOR
CHECKING
AT
THE
SMALLER
SCALE
THIS
ALSO
IMPROVES
RECOGNITION
PERFORMANCE
BY
GIVING
MORE
WEIGHT
TO
THE
LEAST
NOISY
SCALE
IN
OUR
EXPERIMENTS
IT
IS
POSSIBLE
TO
HAVE
A
CUT
OFF
FOR
EXAMINING
AT
MOST
NEIGHBORS
IN
A
PROBABILISTICBEST
BIN
FIRST
SEARCH
OF
KEY
VECTORS
WITH
ALMOST
NO
LOSS
OF
PERFORMANCE
COMPARED
TO
FINDING
AN
EXACT
SOLUTION
AN
EFFICIENT
WAY
TO
CLUSTER
RELIABLE
MODEL
HYPOTHESES
IS
TO
USE
THE
HOUGH
TRANSFORM
TO
SEARCH
FOR
KEYS
THAT
AGREE
UPON
A
PARTICULAR
MODEL
POSE
EACH
MODEL
KEY
IN
THE
DATABASE
CONTAINS
A
RECORD
OF
THE
KEY
PARAMETERS
RELATIVE
TO
THE
MODEL
COORDINATE
SYSTEM
THEREFORE
WE
CAN
CREATE
AN
ENTRY
IN
A
HASH
TABLE
PREDICTING
THE
MODEL
LOCATION
ORIENTATION
AND
SCALE
FROM
THE
MATCH
HYPOTHESIS
WE
USE
A
BIN
SIZE
OF
DEGREES
FOR
ORIENTATION
A
FACTOR
OF
FOR
SCALE
AND
TIMES
THE
MAXIMUM
MODEL
DIMENSION
FOR
LOCATION
THESE
RATHER
BROAD
BIN
SIZES
ALLOW
FOR
CLUSTERING
EVEN
IN
THE
PRESENCE
OF
SUBSTANTIAL
GEOMETRIC
DISTORTION
SUCH
AS
DUE
TO
A
CHANGE
IN
VIEWPOINT
TO
AVOID
THE
PROBLEM
OF
BOUNDARY
EFFECTS
IN
HASHING
EACH
HYPOTHESIS
IS
HASHED
INTO
THE
CLOSEST
BINS
IN
EACH
DIMENSION
GIVING
A
TOTAL
OF
HASH
TABLE
ENTRIES
FOR
EACH
HYPOTHESIS
SOLUTION
FOR
AFFINE
PARAMETERS
THE
HASH
TABLE
IS
SEARCHED
TO
IDENTIFY
ALL
CLUSTERS
OF
AT
LEAST
ENTRIES
IN
A
BIN
AND
THE
BINS
ARE
SORTED
INTO
DECREASING
ORDER
OF
SIZE
EACH
SUCH
CLUSTER
IS
THEN
SUBJECT
TO
A
VERIFICATION
PROCEDURE
IN
WHICH
A
LEAST
SQUARES
SOLUTION
IS
PERFORMED
FOR
THE
AFFINE
PROJECTION
PARAMETERS
RELATING
THEMODEL
TO
THE
IMAGE
THE
AFFINE
TRANSFORMATION
OF
A
MODEL
POINT
X
Y
T
TO
AN
IMAGE
POINT
U
V
T
CAN
BE
WRITTEN
AS
U
V
X
Y
TX
TY
WHERE
THE
MODEL
TRANSLATION
IS
TX
TY
T
AND
THE
AFFINE
ROTATION
SCALE
AND
STRETCH
ARE
REPRESENTED
BY
THEMI
PARAMETERS
WE
WISH
TO
SOLVE
FOR
THE
TRANSFORMATION
PARAMETERS
SO
FIGURE
MODEL
IMAGES
OF
PLANAR
OBJECTS
ARE
SHOWN
IN
THE
TOP
ROW
RECOGNITIONRESULTS
BELOWSHOWMODEL
OUTLINES
AND
IMAGE
KEYS
USED
FOR
MATCHING
THE
EQUATION
ABOVE
CAN
BE
REWRITTEN
AS
X
Y
X
Y
TX
TY
U
V
THIS
EQUATION
SHOWS
A
SINGLE
MATCH
BUT
ANY
NUMBER
OF
FURTHER
MATCHES
CAN
BE
ADDED
WITH
EACH
MATCH
CONTRIBUTING
TWOMORE
ROWS
TO
THE
FIRST
AND
LASTMATRIX
AT
LEAST
ARE
NEEDED
TO
PROVIDE
A
SOLUTION
WE
CAN
WRITE
THIS
LINEAR
SYSTEM
AS
AX
B
THE
LEAST
SQUARES
SOLUTION
FOR
THE
PARAMETERS
X
CAN
BE
DETER
FIGURE
TOP
ROW
SHOWS
MODEL
IMAGES
FOR
OBJECTS
WITH
OUTLINES
FOUND
BY
BACKGROUND
SEGMENTATION
BOTTOM
IMAGE
SHOWS
RECOGNITIONRESULTS
FOR
OUTLINES
AND
IMAGE
KEYS
USED
FOR
MATCHING
MINED
BY
SOLVING
THE
CORRESPONDING
NORMAL
EQUATIONS
X
ATA
WHICH
MINIMIZES
THE
SUM
OF
THE
SQUARES
OF
THE
DISTANCES
FROM
THE
PROJECTED
MODEL
LOCATIONS
TO
THE
CORRESPONDING
IMAGE
LOCATIONS
THIS
LEAST
SQUARES
APPROACH
COULD
READILY
BE
EXTENDED
TO
SOLVING
FOR
POSE
AND
INTERNAL
PARAMETERS
OF
ARTICULATED
AND
FLEXIBLE
OBJECTS
OUTLIERS
CAN
NOW
BE
REMOVED
BY
CHECKING
FOR
AGREEMENT
BETWEEN
EACH
IMAGE
FEATURE
AND
THEMODEL
GIVEN
THE
PARAMETER
SOLUTION
EACH
MATCH
MUST
AGREE
WITHIN
DEGREES
ORIENTATION
CHANGE
IN
SCALE
AND
TIMES
MAXIMUMMODEL
SIZE
IN
TERMS
OF
LOCATION
IF
FEWER
THAN
POINTS
REMAIN
AFTER
DISCARDING
OUTLIERS
THEN
THEMATCH
IS
REJECTED
IF
ANY
OUTLIERS
ARE
DISCARDED
THE
LEAST
SQUARES
SOLUTIONIS
RE
SOLVEDWITH
THE
REMAINING
POINTS
FIGURE
EXAMPLES
OF
RECOGNITIONWITH
OCCLUSION
EXPERIMENTS
THE
AFFINE
SOLUTION
PROVIDES
A
GOOD
APPROXIMATION
TO
PERSPECTIVE
PROJECTION
OF
PLANAR
OBJECTS
SO
PLANAR
MODELS
PROVIDE
A
GOOD
INITIAL
TEST
OF
THE
APPROACH
THE
TOP
ROW
OF
FIGURE
SHOWS
THREE
MODEL
IMAGES
OF
RECTANGULAR
PLANAR
FACES
OF
OBJECTS
THE
FIGURE
ALSO
SHOWS
A
CLUTTERED
IMAGE
CONTAINING
THE
PLANAR
OBJECTS
AND
THE
SAME
IMAGE
IS
SHOWN
OVERLAYED
WITH
THE
MODELS
FOLLOWING
RECOGNITION
THE
MODEL
KEYS
THAT
ARE
DISPLAYED
ARE
THE
ONES
USED
FOR
RECOGNITION
AND
FINAL
LEAST
SQUARES
SOLUTION
SINCE
ONLY
KEYS
ARE
NEEDED
FOR
ROBUST
RECOGNITION
IT
CAN
BE
SEEN
THAT
THE
SOLUTIONS
ARE
HIGHLY
REDUNDANT
AND
WOULD
SURVIVE
SUBSTANTIAL
OCCLUSION
ALSO
SHOWN
ARE
THE
RECTANGULAR
BORDERS
OF
THEMODEL
IMAGES
PROJECTED
USING
THE
AFFINE
TRANSFORM
FROM
THE
LEAST
SQUARE
SOLUTION
THESE
CLOSELY
AGREE
WITH
THE
TRUE
BORDERS
OF
THE
PLANAR
REGIONS
IN
THE
IMAGE
EXCEPT
FOR
SMALL
ERRORS
INTRODUCED
BY
THE
PERSPECTIVE
PROJECTION
SIMILAR
EXPERIMENTS
HAVE
BEEN
PERFORMED
FORMANY
IMAGES
OF
PLANAR
OBJECTS
AND
THE
RECOGNITION
HAS
PROVEN
TO
BE
ROBUST
TO
AT
LEAST
A
DEGREE
ROTATION
OF
THE
OBJECT
IN
ANY
DIRECTION
AWAY
FROMTHE
CAMERA
ALTHOUGH
THE
MODEL
IMAGES
AND
AFFINE
PARAMETERS
DO
NOT
ACCOUNT
FOR
ROTATION
IN
DEPTH
OF
OBJECTS
THEY
ARE
STILL
SUFFICIENT
TO
PERFORM
ROBUST
RECOGNITION
OF
OBJECTS
OVER
ABOUT
A
DEGREE
RANGE
OF
ROTATION
IN
DEPTH
AWAY
FROM
EACH
MODEL
VIEW
AN
EXAMPLE
OF
THREE
MODEL
IMAGES
IS
SHOWN
IN
FIGURE
STABILITY
OF
IMAGE
KEYS
IS
TESTED
UNDER
DIFFERING
ILLUMINATION
THE
FIRST
IMAGE
IS
ILLUMINATED
FROM
UPPER
LEFT
AND
THE
SECOND
FROM
CENTER
RIGHT
KEYS
SHOWN
IN
THE
BOTTOM
IMAGE
WERE
THOSE
USED
TO
MATCH
SECOND
IMAGE
TO
FIRST
THE
TOP
ROW
OF
FIGURE
THEMODELS
WERE
PHOTOGRAPHED
ON
A
BLACK
BACKGROUND
AND
OBJECT
OUTLINES
EXTRACTED
BY
SEGMENTING
OUT
THE
BACKGROUND
REGION
AN
EXAMPLE
OF
RECOGNITION
IS
SHOWN
IN
THE
SAME
FIGURE
AGAIN
SHOWING
THE
SIFT
KEYS
USED
FOR
RECOGNITION
THE
OBJECT
OUTLINES
ARE
PROJECTED
USING
THE
AFFINE
PARAMETER
SOLUTION
BUT
THIS
TIME
THE
AGREEMENT
IS
NOT
AS
CLOSE
BECAUSE
THE
SOLUTION
DOES
NOT
ACCOUNT
FOR
ROTATION
IN
DEPTH
FIGURE
SHOWS
MORE
EXAMPLES
IN
WHICH
THERE
IS
SIGNIFICANT
PARTIAL
OCCLUSION
THE
IMAGES
IN
THESE
EXAMPLES
ARE
OF
SIZE
PIXELS
THE
COMPUTATION
TIMES
FOR
RECOGNITION
OF
ALL
OBJECTS
IN
EACH
IMAGE
ARE
ABOUT
SECONDS
ON
A
SUN
SPARC
PROCESSOR
WITH
ABOUT
SECONDS
REQUIRED
TO
BUILD
THE
SCALESPACE
PYRAMID
AND
IDENTIFY
THE
SIFT
KEYS
AND
ABOUT
SECONDS
TO
PERFORM
INDEXING
AND
LEAST
SQUARES
VERIFICATION
THIS
DOES
NOT
INCLUDE
TIME
TO
PRE
PROCESS
EACH
MODEL
IMAGE
WHICH
WOULD
BE
ABOUT
SECOND
PER
IMAGE
BUT
WOULD
ONLY
NEED
TO
BE
DONE
ONCE
FOR
INITIAL
ENTRY
INTO
A
MODEL
DATABASE
THE
ILLUMINATION
INVARIANCE
OF
THE
SIFT
KEYS
IS
DEMONSTRATED
IN
FIGURE
THE
TWO
IMAGES
ARE
OF
THE
SAME
SCENE
FROM
THE
SAME
VIEWPOINT
EXCEPT
THAT
THE
FIRST
IMAGE
IS
ILLUMINATED
FROM
THE
UPPER
LEFT
AND
THE
SECOND
FROM
THE
CENTER
RIGHT
THE
FULL
RECOGNITION
SYSTEM
IS
RUN
TO
IDENTIFY
THE
SECOND
IMAGE
USING
THE
FIRST
IMAGE
AS
THE
MODEL
AND
THE
SECOND
IMAGE
IS
CORRECTLY
RECOGNIZED
AS
MATCHING
THE
FIRST
ONLY
SIFT
KEYS
THAT
WERE
PART
OF
THE
RECOGNITION
ARE
SHOWN
THERE
WERE
KEYS
THAT
WERE
VERIFIED
AS
PART
OF
THE
FINAL
MATCH
WHICH
MEANS
THAT
IN
EACH
CASE
NOT
ONLYWAS
THE
SAME
KEY
DETECTED
AT
THE
SAME
LOCATION
BUT
IT
ALSO
WAS
THE
CLOSEST
MATCH
TO
THE
CORRECT
CORRESPONDING
KEY
IN
THE
SECOND
IMAGE
ANY
OF
THESE
KEYS
WOULD
BE
SUFFICIENT
FOR
RECOGNITION
WHILE
MATCHING
KEYS
ARE
NOT
FOUND
IN
SOME
REGIONS
WHERE
HIGHLIGHTS
OR
SHADOWS
CHANGE
FOR
EXAMPLE
ON
THE
SHINY
TOP
OF
THE
CAMERA
IN
GENERAL
THE
KEYS
SHOW
GOOD
INVARIANCE
TO
ILLUMINATION
CHANGE
CONNECTIONS
TO
BIOLOGICAL
VISION
THE
PERFORMANCE
OF
HUMAN
VISION
IS
OBVIOUSLY
FAR
SUPERIOR
TO
THAT
OF
CURRENT
COMPUTER
VISION
SYSTEMS
SO
THERE
IS
POTENTIALLYMUCH
TO
BE
GAINED
BY
EMULATING
BIOLOGICAL
PROCESSES
FORTUNATELY
THERE
HAVE
BEEN
DRAMATIC
IMPROVEMENTS
WITHIN
THE
PAST
FEW
YEARS
IN
UNDERSTANDING
HOW
OBJECT
RECOGNITION
IS
ACCOMPLISHED
IN
ANIMALS
AND
HUMANS
RECENT
RESEARCH
IN
NEUROSCIENCE
HAS
SHOWN
THAT
OBJECT
RECOGNITION
IN
PRIMATES
MAKES
USE
OF
FEATURES
OF
INTERMEDIATE
COMPLEXITY
THAT
ARE
LARGELY
INVARIANT
TO
CHANGES
IN
SCALE
LOCATION
AND
ILLUMINATION
TANAKA
PERRETT
ORAM
SOME
EXAMPLES
OF
SUCH
INTERMEDIATE
FEATURES
FOUND
IN
INFERIOR
TEMPORAL
CORTEX
IT
ARE
NEURONS
THAT
RESPOND
TO
A
DARK
FIVE
SIDED
STAR
SHAPE
A
CIRCLE
WITH
A
THIN
PROTRUDING
ELEMENT
OR
A
HORIZONTAL
TEXTURED
REGION
WITHIN
A
TRIANGULAR
BOUNDARY
THESE
NEURONSMAINTAIN
HIGHLY
SPECIFIC
RESPONSES
TO
SHAPE
FEATURES
THAT
APPEAR
ANYWHERE
WITHIN
A
LARGE
PORTION
OF
THE
VISUAL
FIELD
AND
OVER
A
SEVERAL
OCTAVE
RANGE
OF
SCALES
ITO
ET
AL
THE
COMPLEXITY
OF
MANY
OF
THESE
FEATURES
APPEARS
TO
BE
ROUGHLY
THE
SAME
AS
FOR
THE
CURRENT
SIFT
FEATURES
ALTHOUGH
THERE
ARE
ALSO
SOME
NEURONS
THAT
RESPOND
TO
MORE
COMPLEX
SHAPES
SUCH
AS
FACES
MANY
OF
THE
NEURONS
RESPOND
TO
COLOR
AND
TEXTURE
PROPERTIES
IN
ADDITION
TO
SHAPE
THE
FEATURE
RESPONSES
HAVE
BEEN
SHOWN
TO
DEPEND
ON
PREVIOUS
VISUAL
LEARNING
FROM
EXPOSURE
TO
SPECIFIC
OBJECTS
CONTAINING
THE
FEATURES
LOGOTHETIS
PAULS
POGGIO
THESE
FEATURES
APPEAR
TO
BE
DERIVED
IN
THE
BRAIN
BY
A
HIGHLY
COMPUTATION
INTENSIVE
PARALLEL
PROCESS
WHICH
IS
QUITE
DIFFERENT
FROM
THE
STAGED
FILTERING
APPROACH
GIVEN
IN
THIS
PAPER
HOWEVER
THE
RESULTS
ARE
MUCH
THE
SAME
AN
IMAGE
IS
TRANSFORMED
INTO
A
LARGE
SET
OF
LOCAL
FEATURES
THAT
EACH
MATCH
A
SMALL
FRACTION
OF
POTENTIAL
OBJECTS
YET
ARE
LARGELY
INVARIANT
TO
COMMON
VIEWING
TRANSFORMATIONS
IT
IS
ALSO
KNOWN
THAT
OBJECT
RECOGNITION
IN
THE
BRAIN
DEPENDS
ON
A
SERIAL
PROCESS
OF
ATTENTION
TO
BIND
FEATURES
TO
OBJECT
INTERPRETATIONS
DETERMINE
POSE
AND
SEGMENT
AN
OBJECT
FROM
A
CLUTTERED
BACKGROUND
THIS
PROCESS
IS
PRESUMABLY
PLAYING
THE
SAME
ROLE
IN
VERIFICATION
AS
THE
PARAMETER
SOLVING
AND
OUTLIER
DETECTION
USED
IN
THIS
PAPER
SINCE
THE
ACCURACY
OF
INTERPRETATIONS
CAN
OFTEN
DEPEND
ON
ENFORCING
A
SINGLE
VIEWPOINT
CONSTRAINT
CONCLUSIONS
AND
COMMENTS
THE
SIFT
FEATURES
IMPROVE
ON
PREVIOUS
APPROACHES
BY
BEING
LARGELY
INVARIANT
TO
CHANGES
IN
SCALE
ILLUMINATION
AND
LOCAL
AFFINE
DISTORTIONS
THE
LARGE
NUMBER
OF
FEATURES
IN
A
TYPICAL
IMAGE
ALLOW
FOR
ROBUST
RECOGNITION
UNDER
PARTIAL
OCCLUSION
IN
CLUTTERED
IMAGES
A
FINAL
STAGE
THAT
SOLVES
FOR
AFFINE
MODEL
PARAMETERS
ALLOWS
FOR
MORE
ACCURATE
VERIFICATION
AND
POSE
DETERMINATION
THAN
IN
APPROACHES
THAT
RELY
ONLY
ON
INDEXING
AN
IMPORTANT
AREA
FOR
FURTHER
RESEARCH
IS
TO
BUILDMODELS
FROM
MULTIPLE
VIEWS
THAT
REPRESENT
THE
STRUCTURE
OF
OBJECTS
THIS
WOULD
HAVE
THE
FURTHER
ADVANTAGE
THAT
KEYS
FROM
MULTIPLE
VIEWING
CONDITIONS
COULD
BE
COMBINED
INTO
A
SINGLE
MODEL
THEREBY
INCREASING
THE
PROBABILITYOF
FINDINGMATCHES
IN
NEW
VIEWS
THE
MODELS
COULD
BE
TRUE
REPRESENTATIONS
BASED
ON
STRUCTURE
FROM
MOTION
SOLUTIONS
OR
COULD
REPRESENT
THE
SPACE
OF
APPEARANCE
IN
TERMS
OF
AUTOMATED
CLUSTERING
AND
INTERPOLATION
POPE
LOWE
AN
ADVANTAGE
OF
THE
LATTER
APPROACH
IS
THAT
IT
COULD
ALSO
MODEL
NON
RIGID
DEFORMATIONS
THE
RECOGNITION
PERFORMANCE
COULD
BE
FURTHER
IMPROVED
BY
ADDING
NEW
SIFT
FEATURE
TYPES
TO
INCORPORATE
COLOR
TEXTURE
AND
EDGE
GROUPINGS
AS
WELL
AS
VARYING
FEATURE
SIZES
AND
OFFSETS
SCALE
INVARIANT
EDGE
GROUPINGS
THAT
MAKE
LOCAL
FIGURE
GROUND
DISCRIMINATIONS
WOULD
BE
PARTICULARLY
USEFUL
AT
OBJECT
BOUNDARIES
WHERE
BACKGROUND
CLUTTER
CAN
INTERFERE
WITH
OTHER
FEATURES
THE
INDEXING
AND
VERIFICATION
FRAMEWORK
ALLOWS
FOR
ALL
TYPES
OF
SCALE
AND
ROTATION
INVARIANT
FEATURES
TO
BE
INCORPORATED
INTO
A
SINGLE
MODEL
REPRESENTATION
MAXIMUMROBUSTNESS
WOULD
BE
ACHIEVED
BY
DETECTING
MANY
DIFFERENT
FEATURE
TYPES
AND
RELYING
ON
THE
INDEXING
AND
CLUSTERING
TO
SELECT
THOSE
THAT
ARE
MOST
USEFUL
IN
A
PARTICULAR
IMAGE
ADRIANA
KOVASHKA
DEPARTMENT
OF
COMPUTER
SCIENCE
JANUARY
COURSE
INFO
COURSE
WEBSITE
INSTRUCTOR
ADRIANA
KOVASHKA
EMAIL
PLEASE
USE
AT
THE
BEGINNING
OF
THE
SUBJECT
LINE
OFFICE
SENNOTT
SQUARE
OFFICE
HOURS
BY
APPOINTMENT
GRADES
ON
BLACKBOARD
COURSEWEB
FEEDBACK
IS
WELCOME
PLAN
FOR
TODAY
INTRODUCTIONS
WHAT
IS
VISUAL
RECOGNITION
HOW
WELL
DOES
IT
WORK
WHAT
ARE
THE
CHALLENGES
OVERVIEW
OF
TOPICS
COURSE
STRUCTURE
AND
REQUIREMENTS
MACHINE
LEARNING
QUIZ
FOR
SELF
EVALUATION
INTRODUCTIONS
INTRODUCTIONS
WHAT
IS
YOUR
NAME
WHAT
DEPARTMENT
ARE
YOU
AT
WHICH
YEAR
WHAT
ARE
YOUR
RESEARCH
INTERESTS
WHAT
ONE
THING
OUTSIDE
OF
RESEARCH
ARE
YOU
PASSIONATE
ABOUT
WHAT
DO
YOU
HOPE
TO
GET
OUT
OF
THIS
CLASS
VISUAL
RECOGNITION
WHAT
IS
COMPUTER
VISION
AUTOMATIC
UNDERSTANDING
OF
IMAGES
AND
VIDEO
COMPUTING
PROPERTIES
OF
THE
WORLD
FROM
VISUAL
DATA
MEASUREMENT
VISUAL
RECOGNITION
ALGORITHMS
TO
MINE
SEARCH
AND
INTERACT
WITH
VISUAL
DATA
SEARCH
AND
ORGANIZATION
KRISTEN
GRAUMAN
BUILDING
TRUCK
STREET
CARRIAGE
HORSE
BALCONY
TABLE
PERSON
PERSON
CAR
ALLOWS
US
TO
AUTOMATE
VARIOUS
PROCESSES
CHECKOUT
AT
A
STORE
SURVEILLANCE
ANNOTATING
FOOTBALL
VIDEOS
ETC
ENABLES
CONTENT
BASED
IMAGE
SEARCH
AND
ORGANIZATION
PERCEPTION
IS
AN
INTEGRAL
PART
OF
ROBOTICS
HOW
HAS
IT
EVOLVED
HOW
WELL
DOES
IT
WORK
INPUTS
IN
L
G
ROBERTS
PH
D
THESIS
MIT
DEPARTMENT
OF
ELECTRICAL
ENGINEERING
KRISTEN
GRAUMAN
AND
INPUTS
TODAY
PERSONAL
PHOTO
ALBUMS
SURVEILLANCE
AND
SECURITY
SVETLANA
LAZEBNIK
MOVIES
NEWS
SPORTS
MEDICAL
AND
SCIENTIFIC
IMAGES
IMAGENET
CATEGORIES
IMAGES
MICROSOFT
COCO
CATEGORIES
IMAGES
PASCAL
CATEGORIES
IMAGES
SUN
CATEGORIES
IMAGES
YONG
JAE
LEE
AL
MIT
KRISTEN
GRAUMAN
BELHUMEUR
ET
AL
KOOABA
BAY
QUACK
ET
AL
SNAVELY
ET
AL
KRISTEN
GRAUMAN
DEVI
PARIKH
DOLLAR
ET
AL
BMVC
GOOGLE
SELF
DRIVING
CAR
MARS
ROVER
YONG
JAE
LEE
SCHULDT
ET
AL
ICPR
WHAT
ARE
THE
CHALLENGES
ILLUMINATION
OBJECT
POSE
VIEWPOINT
INTRA
CLASS
APPEARANCE
OCCLUSIONS
CLUTTER
KRISTEN
GRAUMAN
ANTONIO
TORRALBA
KRISTEN
GRAUMAN
HOURS
OF
VIDEO
ADDED
TO
YOUTUBE
PER
MINUTE
NEW
TAGGED
PHOTOS
ADDED
TO
FLICKR
PER
MINUTE
THOUSANDS
TO
MILLIONS
OF
PIXELS
IN
AN
IMAGE
DEGREES
OF
FREEDOM
IN
THE
POSE
OF
ARTICULATED
OBJECTS
HUMANS
HUMAN
RECOGNIZABLE
OBJECT
CATEGORIES
HALF
OF
THE
HUMAN
BRAIN
IS
DEVOTED
TO
PROCESSING
VISUAL
INFORMATION
KRISTEN
GRAUMAN
LESS
MORE
KRISTEN
GRAUMAN
OVERVIEW
OF
TOPICS
FEATURE
DETECTION
AND
MATCHING
DETECTING
REPEATABLE
FEATURES
DESCRIBING
IMAGES
WITH
LOCAL
STATISTICS
MATCHING
FEATURES
ACROSS
IMAGES
DETECTION
AND
CLASSIFICATION
DETECTING
NOVEL
INSTANCES
OF
OBJECTS
CLASSIFYING
REGIONS
AS
ONE
OF
SEVERAL
CATEGORIES
ATTRIBUTES
DESCRIBING
THE
HIGH
LEVEL
PROPERTIES
OF
OBJECTS
MEASURING
THE
DEGREE
OF
ATTRIBUTE
PRESENCE
SEGMENTATION
DETECTING
CONTOURS
GROUPING
PIXELS
INTO
SEMANTIC
REGIONS
GROUPS
OF
OBJECTS
SCENES
AND
CONTEXT
EXPLOITING
CONTEXT
TO
DETECT
NOVEL
OBJECTS
PARSING
OUT
THE
ELEMENTS
OF
SCENES
POSE
AND
ACTIONS
AUTOMATICALLY
ANNOTATING
A
HUMAN
POSE
RECOGNIZING
DIFFERENT
HUMAN
ACTIVITIES
UNSUPERVISED
VISUAL
DISCOVERY
FINDING
PATTERNS
IN
UNANNOTATED
DATA
VISION
AND
LANGUAGE
DESCRIBING
IMAGES
WITH
SENTENCES
MAKING
BETTER
USE
OF
TRAINING
DATA
USING
A
KNOWN
CATEGORY
TO
LEARN
ANOTHER
USING
NON
EXPERT
LABELERS
TO
COLLECT
DATA
ENSURING
THE
QUALITY
OF
THIS
DATA
USE
RATIONALES
TO
LEARN
FASTER
EVALUATE
THE
QUALITY
OF
PERFORMED
ACTIONS
LEARNING
WHERE
IN
AN
IMAGE
A
HUMAN
WILL
LOOK
PREDICTING
WHICH
CONTENT
IS
WORTH
MENTIONING
VISUALIZING
AUTOMATIC
PREDICTIONS
AND
OUTPUTS
FINDING
THE
WEAKEST
LINK
IN
SYSTEMS
LETTING
THE
DATA
SOLVE
THE
PROBLEM
LIFE
LONG
LEARNING
AND
MINING
OF
PATTERNS
DATASET
BIAS
RECOGNIZING
ACTIONS
IN
FIRST
PERSON
VIDEO
SUMMARIZING
LONG
VIDEOS
MATCHING
QUERIES
ACROSS
DOMAINS
MODELING
PAINTERLY
STYLES
COURSE
STRUCTURE
AND
REQUIREMENTS
TO
LEARN
ABOUT
THE
STATE
OF
THE
ART
APPROACHES
IN
VISUAL
RECOGNITION
TO
THINK
CRITICALLY
ABOUT
VISION
APPROACHES
AND
TO
SEE
CONNECTIONS
BETWEEN
WORKS
AND
POTENTIAL
FOR
IMPROVEMENT
TO
PRACTICE
CRITICAL
READING
CLEAR
WRITING
AND
ENGAGING
PRESENTATION
SKILLS
EXPECTED
BASIC
KNOWLEDGE
OF
PROBABILITY
AND
LINEAR
ALGEBRA
EXPERIENCE
OR
FAMILIARITY
WITH
MACHINE
LEARNING
IS
RECOMMENDED
PAPER
REVIEWS
PARTICIPATION
DISCUSSION
PAPER
PRESENTATIONS
EXPERIMENT
PRESENTATION
COURSE
PROJECT
PROPOSAL
MID
SEMESTER
PROGRESS
STATUS
REPORT
FINAL
PRESENTATION
FINAL
REPORT
PAGE
REVIEW
FOR
OF
THE
PAPERS
DISCUSSED
EACH
CLASS
USUALLY
THE
PRIMARY
PAPER
IF
ANY
PAPER
MARKED
WITH
READ
THAT
FIRST
DUE
THE
DAY
BEFORE
EACH
CLASS
SEND
EMAIL
TO
INSTRUCTOR
WITH
SUBJECT
PAPER
REVIEW
NAME
YOUR
FILE
FIRST
NAME
LAST
NAME
MONTH
DAY
PDF
OR
DOC
DOCX
SKIP
PAPER
REVIEWS
FOR
PAPERS
YOU
PRESENT
ANSWER
THE
FOLLOWING
QUESTIONS
SUMMARIZE
WHAT
THIS
PAPER
AIMS
TO
DO
AND
WHAT
ITS
MAIN
CONTRIBUTION
IS
SUMMARIZE
THE
PROPOSED
APPROACH
SUMMARIZE
THE
EXPERIMENTAL
VALIDATION
OF
THE
APPROACH
WHAT
ARE
THREE
ADVANTAGES
OF
THE
PROPOSED
APPROACH
WHAT
ARE
THREE
DISADVANTAGES
OR
WEAKNESSES
OF
THE
APPROACH
OR
EXPERIMENTAL
VALIDATION
SUGGEST
ONE
POSSIBLE
EXTENSION
OF
THIS
APPROACH
I
E
ONE
IDEA
FOR
FUTURE
WORK
ANY
OTHER
THOUGHTS
COMMENTS
OR
QUESTIONS
ON
THIS
PAPER
CAREFULLY
READ
THE
ASSIGNED
PAPERS
ASK
MEANINGFUL
QUESTIONS
MAKE
MEANINGFUL
COMMENTS
ABOUT
THE
PAPER
STRENGTHS
AND
WEAKNESSES
ANSWER
QUESTIONS
ASKED
BY
OTHERS
EACH
STUDENT
WILL
GIVE
ABOUT
PRESENTATIONS
EACH
PRESENTATION
WILL
COVER
PAPERS
PRESENTATIONS
SHOULD
BE
MIN
LONG
MIN
IF
NO
EXPERIMENT
PRESENTATION
AND
SHOULD
BE
CLEAR
AND
WELL
REHEARSED
PRESENTATIONS
WILL
BE
FOLLOWED
BY
A
DISCUSSION
MODERATED
BY
THE
PRESENTER
SEE
THE
COURSE
WEBSITE
FOR
QUESTIONS
TO
ADDRESS
IN
PRESENTATION
OFTEN
YOU
CAN
FIND
SLIDES
ON
THE
AUTHORS
WEBSITES
CITE
ALL
SOURCES
AND
SLIDE
CREDITS
AND
USE
YOUR
OWN
WORDS
SLIDES
SHOULD
USE
TEXT
SPARINGLY
SLIDES
WILL
BE
UPLOADED
TO
COURSE
WEBSITE
IMPORTANT
PRESENTERS
SHOULD
MEET
WITH
THE
INSTRUCTOR
ON
FRIDAY
FOR
TUESDAY
PRESENTATIONS
OR
MONDAY
FOR
THURSDAY
PRESENTATIONS
AT
THE
LATEST
IMPORTANT
PRESENTERS
SHOULD
EMAIL
THEIR
DRAFT
SLIDES
AND
MEETING
TIME
AVAILABILITY
TO
THE
INSTRUCTOR
DAYS
BEFORE
DESIRED
MEETING
DATE
WITH
SUBJECT
PRESENTATION
SLIDES
EXCEPTION
DAY
BEFORE
MEETING
IF
PRESENTATION
IS
ON
PRESENT
EXPERIMENTAL
EVALUATION
OF
PAPER
STUDENTS
CAN
VOLUNTEER
TO
PRESENT
AN
EXTRA
PAPER
LATER
BETTER
GRADE
WILL
BE
USED
PRESENTATIONS
WILL
BE
ABOUT
MIN
PICK
ONE
ASPECT
OF
THE
PAPER
TO
EVALUATE
CITE
ANY
CODE
YOU
USED
EXPLAIN
WHAT
WHY
YOU
DID
AND
WHAT
YOU
FOUND
OUT
IN
YOUR
EXPERIMENTS
ASK
FOR
GUIDANCE
IF
NEEDED
STUDENTS
WILL
COMPLETE
IN
DEPTH
STUDY
OF
ONE
TOPIC
COVERED
IN
CLASS
THESE
PROJECTS
CAN
BECOME
CONFERENCE
PUBLICATIONS
FOR
MOST
TYPES
OF
PROJECTS
STUDENTS
CAN
WORK
IN
PAIRS
MOST
PROJECT
TYPES
REQUIRE
EXPERIMENTAL
EVALUATION
PROJECTS
CAN
BE
ONE
OF
THE
FOLLOWING
AN
EXTENSION
OF
ONE
OR
MORE
PAPERS
COVERED
IN
CLASS
A
NOVEL
APPROACH
WITH
EVALUATION
A
DEFINITION
OF
A
NEW
PROBLEM
ALONG
WITH
DETAILED
ARGUMENTATION
OF
WHY
THIS
PROBLEM
IS
IMPORTANT
AND
CHALLENGING
AND
AN
APPROACH
TO
SOLVE
THIS
PROBLEM
EXTENSIVE
ANALYSIS
AND
EXPERIMENTAL
EVALUATION
OF
ONE
OR
MORE
OF
THE
APPROACHES
COVERED
IN
CLASS
EXTENSIVE
LITERATURE
REVIEW
AND
ANALYSIS
ON
ONE
OF
THE
TOPICS
COVERED
IN
CLASS
THIS
CAN
ONLY
BE
DONE
BY
STUDENTS
WORKING
INDIVIDUALLY
TIMELINE
MARCH
PROJECT
PROPOSALS
DUE
OF
COURSE
GRADE
APRIL
PROJECT
STATUS
REPORTS
DUE
APRIL
PROJECT
PRESENTATIONS
APRIL
PROJECT
FINAL
REPORT
DUE
EMAIL
THESE
TO
THE
INSTRUCTOR
WITH
SUBJECT
PROJECT
SEE
COURSE
WEBSITE
FOR
DETAILS
PROPOSAL
PAGES
IN
LENGTH
INCLUDE
CLEAR
PROBLEM
STATEMENT
EXTENSIVE
LITERATURE
REVIEW
DETAILED
OUTLINE
OF
THE
APPROACH
AND
PLANNED
EXPERIMENTAL
SETUP
STUDENTS
ARE
ENCOURAGED
TO
MEET
WITH
THE
INSTRUCTOR
TO
DISCUSS
PROPOSAL
BEFORE
PROPOSALS
ARE
DUE
PROGRESS
REPORT
DESCRIBE
PROGRESS
IDENTIFY
PROBLEM
AREAS
USE
THE
CVPR
LATEX
TEMPLATE
INCLUDE
SECTIONS
INTRODUCTION
RELATED
WORK
APPROACH
AND
RESULTS
PRESENTATION
MIN
LONG
FINAL
REPORT
BY
ON
TOMORROW
EMAIL
THE
INSTRUCTOR
KOVASHKA
CS
PITT
EDU
A
LIST
OF
THE
TEN
TOPICS
FROM
THE
SCHEDULE
ON
THE
COURSE
WEBSITE
THAT
YOU
ARE
MOST
INTERESTED
IN
PRESENTING
AS
A
PAPER
PRESENTATION
A
LIST
OF
TEN
PAPERS
THAT
YOU
MIGHT
LIKE
TO
PRESENT
AS
AN
EXPERIMENT
PRESENTATION
SORT
THESE
FROM
TO
WHERE
DENOTES
MOST
INTERESTED
USE
SUBJECT
TOPIC
PREFERENCES
QUESTIONS
QUIZ
HOW
WOULD
YOU
LEARN
TO
PREDICT
WHETHER
AUTHOR
A
OR
AUTHOR
B
WROTE
A
GIVEN
PIECE
OF
TEXT
WHAT
LEARNING
PROBLEM
CORRESPONDS
TO
PREDICTING
THE
PRICE
OF
A
CAR
BASED
ON
ITS
FEATURES
E
G
ENGINE
SPECS
AND
THE
PRICES
OF
OTHER
CARS
OTHER
THAN
LEARNING
AN
INDIVIDUAL
MODEL
WHAT
ELSE
CAN
YOU
USE
TRAINING
DATA
FOR
GIVEN
A
MODEL
THAT
CAN
PREDICT
WHETHER
AN
IMAGE
HAS
A
CAT
IN
IT
OR
NOT
WHAT
DO
YOU
USUALLY
NEED
TO
DO
TO
A
NEW
IMAGE
IN
ORDER
TO
BE
ABLE
TO
APPLY
THIS
MODEL
TO
IT
WHAT
DOES
OVERFITTING
MEAN
WHY
IS
IT
A
PROBLEM
HOW
CAN
YOU
OVERCOME
THIS
PROBLEM
WHAT
TWO
GOALS
DOES
A
SOFT
MARGIN
LINEAR
SVM
OPTIMIZATION
OBJECTIVE
CAPTURE
HOW
CAN
YOU
USE
MAXIMUM
LIKELIHOOD
TO
SELECT
MODEL
PARAMETERS
STATE
BAYES
THEOREM
WHAT
IS
THE
GOAL
OF
PERFORMING
K
MEANS
HOW
IS
THIS
GOAL
ACCOMPLISHED
WHAT
ARE
Λ
AND
V
IN
RELATIONSHIP
TO
A
ACCORDING
TO
THE
FOLLOWING
EQUATION
A
V
Λ
V
IF
THE
DIMENSIONALITY
OF
V
IS
WHAT
IS
THE
DIMENSIONALITY
OF
A
THIS
THURSDAY
DESCRIBING
IMAGES
WITH
FEATURES
NEXT
TUESDAY
RECOGNITION
BASICS
ADRIANA
RESEARCH
NEXT
THURSDAY
FIRST
PRESENTATION
FEATURES
EVERY
PICTURE
TELLS
A
STORY
GENERATING
SENTENCES
FROM
IMAGES
SLIDES
CREDITED
YUKUN
ZHU
GOA
LL
T
HI
I
A
LOT
OF
T
EC
HNO
LLOGY
SOM
BODYS
SCR
NS
AVE
R
OF
K
N
A
LAC
K
LLA
PTOP
IS
CONNECTED
TO
A
LLACK
EL
L
TOR
THI
I
A
D
U
A
OR
SET
P
O
D
SCHOOL
UT
ER
MONITOR
WIT
LH
WA
Y
TO
ST
I
CKERS
ON
GOAL
A
U
T
O
I
L
U
ST
R
AT
FIND
P
ICT
U
G
G
ES
T
E
BY
G
I
V
EN
TEXT
YELLOW
TRAIN
ON
THE
TRA
CK
MAP
FROM
IMAGE
SPACE
TO
MEANING
SPACE
MAP
FROM
SENTENCE
SPACE
TO
MEANING
SPACE
RETRIEVE
SENTENCES
FOR
IMAGES
VIA
MEANING
SPACE
FARHADI
ET
AL
MAP
FROM
IMAGE
SPACE
TO
MEANING
SPACE
MAP
FROM
SENTENCE
SPACE
TO
MEANING
SPACE
RETRIEVE
SENTENCES
FOR
IMAGES
VIA
MEANING
SPACE
FARHADI
ET
AL
PREDICT
IMAGE
CONTENT
USING
TRAINED
CLASSIFIERS
FARHADI
ET
AL
MAP
FROM
IMAGE
SPACE
TO
MEANING
SPACE
MAP
FROM
SENTENCE
SPACE
TO
MEANING
SPACE
RETRIEVE
SENTENCES
FOR
IMAGES
VIA
MEANING
SPACE
FARHADI
ET
AL
EXTRACT
SUBJECT
VERB
AND
SCENE
FROM
SENTENCES
IN
THE
TRAINING
DATA
BLACK
CAT
OVER
PINK
CHAIR
A
BLACK
COLOR
CAT
SI
NG
ON
CHAIR
IN
A
ROOM
CAT
SI
NG
ON
A
CHAIR
LOOKING
IN
A
MIRROR
USE
TAXONOMY
TREES
OBJECT
SUBJECT
CAT
VERB
SI
NG
SCENE
ROOM
ANIMAL
HUMAN
VEHICLE
CAT
DOG
HORSE
CAR
BIKE
TRAIN
FARHADI
ET
AL
MAP
FROM
IMAGE
SPACE
TO
MEANING
SPACE
MAP
FROM
SENTENCE
SPACE
TO
MEANING
SPACE
RETRIEVE
SENTENCES
FOR
IMAGES
VIA
MEANING
SPACE
FARHADI
ET
AL
FARHADI
ET
AL
FARHADI
ET
AL
FARHADI
ET
AL
IMAGES
IMAGES
MORE
DATA
NEEDED
RASHTCHIAN
ET
AL
FARHADI
ET
AL
DESCRIPTIONS
PER
IMAGE
OBJECT
CATEGORIES
IMAGE
CLEF
CHALLENGE
DESCRIPTIONS
PER
IMAGE
SELECT
IMAGE
CATEGORIES
LARGE
AMOUNTS
OF
PAIRED
DATA
CAN
HELP
US
STUDY
THE
RELATIONSHIP
BERG
A
RIBUTES
TUTORIAL
BABY
TALK
UNDERSTANDING
AND
GENERATING
IMAGE
DESCRIPTIONS
PRESENTED
BY
YINGJIE
TANG
PERSON
CAR
SHOE
BERG
A
RIBUTES
TUTORIAL
CREDIT
TAMARA
CAR
BERG
A
RIBUTES
TUTORIAL
CREDIT
TAMARA
PINK
CAR
A
RIBUTES
OF
OBJECTS
BERG
A
RIBUTES
TUTORIAL
CREDIT
TAMARA
CAR
ON
ROAD
RELATIONSHIPS
BETWEEN
OBJECTS
BERG
A
RIBUTES
TUTORIAL
CREDIT
TAMARA
LITTLE
PINK
SMART
CAR
PARKED
ON
THE
SIDE
OF
A
ROAD
IN
A
LONDON
SHOPPING
DISTRICT
COMPLEX
STRUCTURED
RECOGNITION
OUTPUTS
TELLING
THE
STORY
OF
AN
IMAGE
BERG
A
RIBUTES
TUTORIAL
CREDIT
TAMARA
PROBLEM
GENERATE
NATURAL
LANGUAGE
DESCRIPTIONS
FOR
IMAGES
THIS
PICTURE
SHOWS
ONE
PERSON
ONE
GRASS
ONE
CHAIR
AND
ONE
POTTED
PLANT
THE
PERSON
IS
NEAR
THE
GREEN
GRASS
AND
IN
THE
CHAIR
THE
GREEN
GRASS
IS
BY
THE
CHAIR
AND
NEAR
THE
POTTED
PLANT
PROBLEM
GENERATE
NATURAL
LANGUAGE
DESCRIPTIONS
FOR
IMAGES
DESCRIPTIVE
LANGUAGE
MORE
INFORMATION
ABOUT
THE
VISUAL
WORLD
CONVEY
THE
STYLE
HOW
PEOPLE
DESCRIBE
WORLD
LEARNING
FROM
DESCRIPTIVE
TEXT
IT
WAS
AN
ARRESTING
FACE
POINTED
OF
CHIN
SQUARE
OF
JAW
HER
EYES
WERE
PALE
GREEN
WITHOUT
A
TOUCH
OF
HAZEL
STARRED
WITH
BRISTLY
BLACK
LASHES
AND
SLIGHTLY
TILTED
AT
THE
ENDS
ABOVE
THEM
HER
THICK
BLACK
BROWS
SLANTED
UPWARD
CUTTING
A
STARTLING
OBLIQUE
LINE
IN
HER
MAGNOLIA
WHITE
SKIN
THAT
SKIN
SO
PRIZED
BY
SOUTHERN
WOMEN
AND
SO
CAREFULLY
GUARDED
WITH
BONNETS
VEILS
AND
MITTENS
AGAINST
HOT
GEORGIA
SUNS
SCARLETT
O
HARA
DESCRIBED
IN
GONE
WITH
THE
WIND
VISUALLY
DESCRIPTIVE
LANGUAGE
PROVIDES
INFORMATION
ABOUT
THE
WORLD
ESPECIALLY
THE
VISUAL
WORLD
HOW
DOES
THE
WORLD
WORK
INFORMATION
ABOUT
HOW
PEOPLE
CONSTRUCT
NATURAL
LANGUAGE
FOR
IMAGERY
GUIDANCE
FOR
VISUAL
RECOGNITION
WHAT
SHOULD
RECOGNIZE
HOW
DO
PEOPLE
DESCRIBE
THE
WORLD
BERG
A
RIBUTES
TUTORIAL
CREDIT
TAMARA
PROBLEM
GENERATE
NATURAL
LANGUAGE
DESCRIPTIONS
FOR
IMAGES
STATISTICS
GLEANED
FROM
PARSING
LARGE
QUANTITIES
OF
TEXT
DATA
RECOGNITION
ALGORITHMS
FROM
COMPUTER
VISION
GENERATING
SENTENCES
FOR
IMAGES
MINING
FOR
STATISTIC
MODELS
DETECTORS
SCENE
OBJECTS
N
GRAM
GENERATING
SENTENCES
GENERATING
SENTENCES
FOR
IMAGES
KEY
WORDS
MOST
PREVIOUS
WORK
IN
NLP
ON
AUTOMATICALLY
GENERATING
CAPTIONS
OR
DESCRIPTIONS
FOR
IMAGES
IS
BASED
ON
RETRIEVAL
AND
SUMMARIZATION
GENERATING
SENTENCES
FOR
IMAGES
SCENE
BASED
GENERATED
SENTENCES
ARE
NOT
AS
DESCRIPTIVE
ENOUGH
GENERATING
SENTENCES
FOR
IMAGES
SMALL
NUMBER
OF
INSTANCES
FORM
LARGE
NUMBER
OF
SCENES
AVOID
WHOLE
IMAGE
FEATURES
RECOGNITION
AND
MAKE
TIGHT
CONNECTION
BETWEEN
IMAGE
CONTENT
AND
SENTENCE
GENERATION
INDIVIDUAL
WORDS
WITH
IMAGE
REGIONS
USE
OF
SPATIAL
RELATIONSHIPS
BETWEEN
LABELED
PARTS
OF
IMAGE
USE
THE
ATTRIBUTES
IN
COMPUTER
VISION
TO
ESTIMATE
MODIFIERS
FOR
OBJECTS
IN
IMAGES
USE
HUMAN
LOOP
FOR
HIERARCHICAL
IMAGE
PARSING
CONDITIONAL
RANDOM
FIELDS
A
FRAMEWORK
FOR
BUILDING
PROBABILISTIC
MODELS
TO
SEGMENT
AND
LABEL
SEQUENCE
DATA
CONDITIONAL
RANDOM
FIELDS
OFFER
SEVERAL
ADVANTAGES
OVER
HIDDEN
MARKOV
MODELS
AND
STOCHASTIC
GRAMMARS
FOR
SUCH
TASKS
INCLUDING
THE
ABILITY
TO
RELAX
STRONG
INDEPENDENCE
ASSUMPTIONS
MADE
IN
THOSE
MODELS
LAFFERTY
J
MCCALLUM
A
PEREIRA
F
NODES
OF
THE
CRF
OBJECTS
ATTRIBUTES
PREPOSITIONS
NODES
OF
THE
CRF
OBJECTS
A
LARGE
SET
OF
DETECTORS
COLLECT
A
SET
OF
HIGH
SCORE
DETECTIONS
MERGE
DETECTIONS
THAT
ARE
HIGHLY
OVERLAPPING
INTO
GROUPS
CREATE
AN
OBJECT
NODE
FOR
EACH
GROUP
OBJECTS
NODES
SET
OF
OBJECT
DETECTORS
THAT
FIRED
AT
THAT
REGION
IN
THE
IMAGE
ATTRIBUTE
NODES
A
SET
OF
APPEARANCE
ATTRIBUTES
THAT
CAN
MODIFY
THE
OBJECTS
PREPOSITION
NODES
A
SET
OF
PREPOSITIONAL
RELATIONS
THAT
CAN
OCCUR
BETWEEN
TWO
OBJECTS
LABEL
IMAGE
TEXT
PRIOR
NUMBER
OF
OBJECTS
A
NUMBER
OF
OBJECTS
THE
NUMBER
OF
TRUE
OBJ
LABELS
MINUS
THE
NUMBER
OF
FALSE
OBJ
LABELS
NORMALIZED
BY
THE
NUMBER
OF
OBJECTS
THE
NUMBER
OF
TRUE
MOD
OBJ
LABEL
PAIRS
MINUS
THE
NUMBER
OF
FALSE
MOD
OBJ
PAIRS
THE
NUMBER
OF
TRUE
OBJ
PREP
OBJ
TRIPLES
MINUS
THE
NUMBER
OF
FALSE
OBJ
PREP
OBJ
TRIPLES
NORMALIZED
BY
THE
NUMBER
OF
NODES
AND
THE
NUMBER
OF
PAIRS
OF
OBJECTS
N
CHOOSE
IMAGE
BASED
POTENTIALS
THE
IMAGE
POTENTIALS
COME
FROM
HAND
DESIGNED
DETECTION
STRATEGIES
OPTIMIZED
ON
EXTERNAL
TRAINING
SETS
TEXT
POTENTIAL
THE
TEXT
POTENTIALS
ARE
BASED
ON
TEXT
STATISTICS
COLLECTED
AUTOMATICALLY
FROM
VARIOUS
CORPORA
Ψ
OBJI
OBJDET
OBJECT
AND
STUFF
POTENTIAL
FOR
OBJECT
DETECTORS
PASCAL
OBJECT
CATEGORIES
DETECTORS
TRAINED
ADDITIONAL
NON
PASCAL
OBJECT
CATEGORIES
FOR
FLOWER
LAPTOP
TIGER
AND
WINDOW
FOR
STUFF
DETECTORS
TRAINED
LINEAR
SVMS
ON
THE
LOW
LEVEL
REGION
FEATURES
TO
RECOGNIZE
SKY
ROAD
BUILDING
TREE
WATER
AND
GRASS
STUFF
CATEGORIES
SVM
OUTPUTS
ARE
MAPPED
TO
PROBABILITIES
Ψ
ATTRI
ATTRCL
ATTRIBUTE
POTENTIAL
TRAIN
VISUAL
ATTRIBUTE
CLASSIFIERS
THAT
ARE
RELEVANT
FOR
OUR
OBJECT
AND
STUFF
CATEGORIES
MINE
LARGE
TEXT
CORPUS
OF
FLICKR
DESCRIPTIONS
DESCRIBED
IN
SEC
TO
FIND
ATTRIBUTE
TERMS
THE
RESULTING
LIST
CONSISTS
OF
VISUAL
ATTRIBUTE
TERMS
DESCRIBING
COLOR
E
G
BLUE
GRAY
TEXTURE
E
G
STRIPED
FURRY
MATERIAL
E
G
WOODEN
FEATHERED
GENERAL
APPEARANCE
E
G
RUSTY
DIRTY
SHINY
AND
SHAPE
E
G
RECTANGULAR
CHARACTERISTICS
Ψ
PREPIJ
PREPF
UNS
PREPOSITION
POTENTIAL
PREPOSITION
TERMS
TWO
POTENTIAL
FUNCTIONS
CALCULATED
FROM
LARGE
CORPORA
PAIRWISE
POTENTIAL
ON
ATTRIBUTE
OBJECT
LABEL
PAIRS
Ψ
ATTRI
OBJI
TEXTP
R
A
TRINARY
POTENTIAL
ON
OBJECT
PREPOSITION
OBJECT
TRIPLES
Ψ
OBJI
PREPIJ
OBJJ
TEXTP
R
THESE
POTENTIALS
ARE
THE
PROBABILITY
OF
VARIOUS
ATTRIBUTES
FOR
EACH
OBJECT
GIVEN
THE
OBJECT
AND
THE
PROBABILITIES
OF
PARTICULAR
PREPOSITIONAL
RELATIONSHIPS
BETWEEN
OBJECT
PAIRS
GIVEN
THE
PAIR
OF
OBJECTS
PARSING
POTENTIAL
COLLECT
A
LARGE
SET
OF
FLICKR
IMAGE
DESCRIPTIONS
TO
COUNT
OBJECT
POTENTIAL
ΨP
ATTRI
OBJI
TEXTP
R
COLLECT
STATISTICS
ABOUT
THE
OCCURRENCE
OF
EACH
ATTRIBUTE
AND
OBJECT
PAIR
TO
COUNT
AMOD
ATTRIBUTE
OBJECT
COLLECT
MILLION
FLICKR
IMAGE
DESCRIPTIONS
BY
QUERYING
FOR
PAIRS
OF
OBJECT
TERMS
FOR
ΨP
OBJI
PREPIJ
OBJI
TEXTP
R
REASONS
THE
COUNTS
FOR
SOME
OBJECTS
CAN
BE
TOO
SPARSE
COLLECT
ADDITIONAL
GOOGLE
SEARCH
BASED
POTENTIALS
ΨG
ATTRI
OBJI
TEXTPR
AND
ΨG
OBJI
PREPIJ
OBJJ
TEXTPR
SMOOTH
POTENTIAL
FINAL
POTENTIALS
ARE
COMPUTED
AS
A
SMOOTHED
COMBINATION
OF
THE
PARSING
BASED
POTENTIALS
WITH
THE
GOOGLE
POTENTIALS
ΑΨP
Α
ΨG
OUTPUT
OF
CRF
TRIPLES
OUR
GOAL
LANGUAGE
MODELS
AND
TEMPLATES
N
GRAM
MODEL
THE
PREDICTION
OF
THE
NEXT
WORD
DEPENDS
ONLY
ON
THE
PREVIOUS
N
WORDS
WE
WANT
TO
DETERMINE
WHETHER
TO
INSERT
A
FUNCTION
WORD
X
BETWEEN
A
PAIR
OF
WORDS
Α
AND
Β
IN
THE
MEANING
REPRESENTATION
CALCULATING
P
ΑXΒ
P
Α
P
X
Α
P
Β
X
USING
BIGRAM
GRAM
LANGUAGE
MODELS
WEAKNESS
IT
IS
DIFFICULT
TO
ENFORCE
GRAMMATICALLY
CORRECT
SENTENCES
USING
LANGUAGE
MODELS
ALONE
IT
IS
IGNORANT
OF
DISCOURSE
STRUCTURE
COHERENCY
AMONG
SENTENCES
AS
EACH
SENTENCE
IS
GENERATED
INDEPENDENTLY
TEMPLATES
WITH
LINGUISTIC
CONSTRAINTS
CONSTRUCTING
TEMPLATES
WITH
LINGUISTICALLY
MOTIVATED
CONSTRAINTS
THIS
APPROACH
IS
BASED
ON
THE
ASSUMPTION
THAT
THERE
ARE
A
HANDFUL
OF
SALIENT
SYNTACTIC
PATTERNS
IN
DESCRIPTIVE
LANGUAGE
THAT
WE
CAN
ENCODE
AS
TEMPLATES
TEMPLATES
WITH
LINGUISTIC
CONSTRAINTS
TEMPLATES
WITH
LINGUISTIC
CONSTRAINTS
THIS
IS
A
PICTURE
OF
ONE
SKY
ONE
ROAD
AND
ONE
SHEEP
THE
GRAY
SKY
IS
OVER
THE
GRAY
ROAD
THE
GRAY
SHEEP
IS
BY
THE
GRAY
ROAD
HERE
WE
SEE
ONE
ROAD
ONE
SKY
AND
ONE
BICYCLE
THE
ROAD
IS
NEAR
THE
BLUE
SKY
AND
NEAR
THE
COLORFUL
BICYCLE
THE
COLORFUL
BICYCLE
IS
WITHIN
THE
BLUE
SKY
THIS
IS
A
PICTURE
OF
TWO
DOGS
THE
FIRST
DOG
IS
NEAR
THE
SECOND
FURRY
DOG
KULKARNI
ET
AL
CREDIT
TAMARA
MISSED
DETECTIONS
HERE
WE
SEE
ONE
POTTED
PLANT
THIS
IS
A
PICTURE
OF
ONE
DOG
FALSE
DETECTIONS
THERE
ARE
ONE
ROAD
AND
ONE
CAT
THE
FURRY
ROAD
IS
IN
THE
FURRY
CAT
THIS
IS
A
PICTURE
OF
ONE
TREE
ONE
ROAD
AND
ONE
PERSON
THE
RUSTY
TREE
IS
UNDER
THE
RED
ROAD
THE
COLORFUL
PERSON
IS
NEAR
THE
RUSTY
TREE
AND
UNDER
THE
RED
ROAD
INCORRECT
ATTRIBUTES
THIS
IS
A
PHOTOGRAPH
OF
TWO
SHEEPS
AND
ONE
GRASS
THE
FIRST
BLACK
SHEEP
IS
BY
THE
GREEN
GRASS
AND
BY
THE
SECOND
BLACK
SHEEP
THE
SECOND
BLACK
SHEEP
IS
BY
THE
GREEN
GRASS
THIS
IS
A
PHOTOGRAPH
OF
TWO
HORSES
AND
ONE
GRASS
THE
FIRST
FEATHERED
HORSE
IS
WITHIN
THE
GREEN
GRASS
AND
BY
THE
SECOND
FEATHERED
HORSE
THE
SECOND
FEATHERED
HORSE
IS
WITHIN
THE
GREEN
GRASS
KULKARNI
ET
AL
CREDIT
TAMARA
TRAINING
SETS
TEST
SETS
TRAINING
SET
CRAWLED
WIKIPEDIA
PAGES
THAT
DESCRIBE
OBJECTS
OUR
SYSTEM
CAN
RECOGNIZE
TO
CONSTRUCT
THE
TRAINING
CORPUS
FOR
LANGUAGE
MODELS
TEST
SET
USE
THE
UIUC
PAS
CAL
SENTENCE
WHICH
CONTAINS
UP
TO
FIVE
HUMAN
GENERATED
SENTENCES
THAT
DESCRIBE
IMAGES
AUTOMATIC
EVALUATION
BLEU
A
WIDELY
USED
METRIC
FOR
AUTOMATIC
EVALUATION
OF
MACHINE
TRANSLATION
THAT
MEASURES
THE
N
GRAM
PRECISION
OF
MACHINE
GENERATED
SENTENCES
WITH
RESPECT
TO
HUMAN
GENERATED
SENTENCES
WEAKNESS
BLEU
WILL
INEVITABLY
PENALIZE
MANY
CORRECTLY
GENERATED
SENTENCES
HUMAN
EVALUATION
PERFORM
HUMAN
JUDGMENT
ON
THE
ENTIRE
TEST
SET
TO
DIRECTLY
QUANTIFY
THESE
ASPECTS
OVERALL
THE
TEMPLATE
GENERATION
METHOD
DEMONSTRATES
A
VERY
HIGH
AVERAGE
HUMAN
EVALUATION
SCORE
OF
MAX
FOR
THE
QUALITY
OF
GENERATED
SENTENCES
AN
EFFECTIVE
FULLY
AUTOMATIC
SYSTEM
THAT
GENERATES
NATURAL
LANGUAGE
DESCRIPTIONS
FOR
IMAGES
PRODUCE
RESULTS
MUCH
MORE
SPECIFIC
TO
THE
IMAGE
CONTENT
THAN
PREVIOUS
AUTOMATED
METHODS
HUMAN
EVALUATION
VALIDATES
THE
QUALITY
OF
THE
GENERATED
SENTENCES
KEYS
TO
SUCCESS
AUTOMATICALLY
MINING
AND
PARSING
LARGE
TEXT
COLLECTIONS
TAKING
ADVANTAGE
OF
STATE
OF
THE
ART
VISION
SYSTEMS
AND
COMBINING
ALL
OF
THESE
IN
A
CRF
TO
PRODUCE
INPUT
FOR
LANGUAGE
GENERATION
METHODS
THE
DESCRIPTIONS
OF
A
SENTENCE
FOR
THE
IMAGE
IS
ALWAYS
BIASED
THIS
PICTURE
SHOWS
ONE
PERSON
ONE
GRASS
ONE
CHAIR
AND
ONE
POTTED
PLANT
THE
PERSON
IS
NEAR
THE
GREEN
GRASS
AND
IN
THE
CHAIR
THE
GREEN
GRASS
IS
BY
THE
CHAIR
AND
NEAR
THE
POTTED
PLANT
THE
BOY
IS
HAPPY
TO
STANDING
IN
FRONT
OF
THE
SHOP
WITH
HIS
SNACKS
HOW
CAN
WE
FIND
AN
UNBIASED
WAY
TO
DESCRIBE
THE
IMAGE
SCENE
BASED
DESCRIPTION
IS
IN
A
HIGH
LEVEL
DESCRIPTION
WHILE
THE
THING
STUFF
BASED
DESCRIPTION
IS
THE
LOW
LEVEL
DESCRIPTION
SCENE
BASED
DESCRIPTION
THINGS
STUFF
BASED
DESCRIPTION
SCENE
BASED
DESCRIPTION
THINGS
STUFF
BASED
DESCRIPTION
CAN
WE
REVERSE
BACK
TO
THE
SCENE
BASED
DESCRIPTION
FROM
THE
THINGS
STUFF
INFORMATION
A
DISCRIMINATIVELY
TRAINED
MULTISCALE
DEFORMABLE
PART
MODEL
PEDRO
FELZENSZWALB
UNIVERSITY
OF
CHICAGO
DAVID
MCALLESTER
TOYOTA
TECHNOLOGICAL
INSTITUTE
AT
CHICAGO
DEVA
RAMANAN
UC
IRVINE
ABSTRACT
THIS
PAPER
DESCRIBES
A
DISCRIMINATIVELY
TRAINED
MULTI
SCALE
DEFORMABLE
PART
MODEL
FOR
OBJECT
DETECTION
OUR
SYS
TEM
ACHIEVES
A
TWO
FOLD
IMPROVEMENT
IN
AVERAGE
PRECISION
OVER
THE
BEST
PERFORMANCE
IN
THE
PASCAL
PERSON
DE
TECTION
CHALLENGE
IT
ALSO
OUTPERFORMS
THE
BEST
RESULTS
IN
THE
CHALLENGE
IN
TEN
OUT
OF
TWENTY
CATEGORIES
THE
SYSTEM
RELIES
HEAVILY
ON
DEFORMABLE
PARTS
WHILE
DEFORMABLE
PART
MODELS
HAVE
BECOME
QUITE
POPULAR
THEIR
VALUE
HAD
NOT
BEEN
DEMONSTRATED
ON
DIFFICULT
BENCHMARKS
SUCH
AS
THE
PASCAL
CHALLENGE
OUR
SYSTEM
ALSO
RELIES
HEAVILY
ON
NEW
METHODS
FOR
DISCRIMINATIVE
TRAINING
WE
COMBINE
A
MARGIN
SENSITIVE
APPROACH
FOR
DATA
MINING
HARD
NEGATIVE
EXAMPLES
WITH
A
FORMALISM
WE
CALL
LATENT
SVM
A
LATENT
SVM
LIKE
A
HID
DEN
CRF
LEADS
TO
A
NON
CONVEX
TRAINING
PROBLEM
HOW
EVER
A
LATENT
SVM
IS
SEMI
CONVEX
AND
THE
TRAINING
PROB
LEM
BECOMES
CONVEX
ONCE
LATENT
INFORMATION
IS
SPECIFIED
FOR
THE
POSITIVE
EXAMPLES
WE
BELIEVE
THAT
OUR
TRAINING
METH
ODS
WILL
EVENTUALLY
MAKE
POSSIBLE
THE
EFFECTIVE
USE
OF
MORE
LATENT
INFORMATION
SUCH
AS
HIERARCHICAL
GRAMMAR
MODELS
AND
MODELS
INVOLVING
LATENT
THREE
DIMENSIONAL
POSE
INTRODUCTION
WE
CONSIDER
THE
PROBLEM
OF
DETECTING
AND
LOCALIZING
OB
JECTS
OF
A
GENERIC
CATEGORY
SUCH
AS
PEOPLE
OR
CARS
IN
STATIC
IMAGES
WE
HAVE
DEVELOPED
A
NEW
MULTISCALE
DEFORMABLE
PART
MODEL
FOR
SOLVING
THIS
PROBLEM
THE
MODELS
ARE
TRAINED
USING
A
DISCRIMINATIVE
PROCEDURE
THAT
ONLY
REQUIRES
BOUND
ING
BOX
LABELS
FOR
THE
POSITIVE
EXAMPLES
USING
THESE
MOD
ELS
WE
IMPLEMENTED
A
DETECTION
SYSTEM
THAT
IS
BOTH
HIGHLY
EFFICIENT
AND
ACCURATE
PROCESSING
AN
IMAGE
IN
ABOUT
SEC
ONDS
AND
ACHIEVING
RECOGNITION
RATES
THAT
ARE
SIGNIFICANTLY
BETTER
THAN
PREVIOUS
SYSTEMS
OUR
SYSTEM
ACHIEVES
A
TWO
FOLD
IMPROVEMENT
IN
AVERAGE
PRECISION
OVER
THE
WINNING
SYSTEM
IN
THE
PASCAL
PERSON
DETECTION
CHALLENGE
THE
SYSTEM
ALSO
OUTPERFORMS
THE
BEST
RESULTS
IN
THE
CHALLENGE
IN
TEN
OUT
OF
TWENTY
THIS
MATERIAL
IS
BASED
UPON
WORK
SUPPORTED
BY
THE
NATIONAL
SCIENCE
FOUNDATION
UNDER
GRANT
NO
AND
FIGURE
EXAMPLE
DETECTION
OBTAINED
WITH
THE
PERSON
MODEL
THE
MODEL
IS
DEFINED
BY
A
COARSE
TEMPLATE
SEVERAL
HIGHER
RESOLUTION
PART
TEMPLATES
AND
A
SPATIAL
MODEL
FOR
THE
LOCATION
OF
EACH
PART
OBJECT
CATEGORIES
FIGURE
SHOWS
AN
EXAMPLE
DETECTION
OB
TAINED
WITH
OUR
PERSON
MODEL
THE
NOTION
THAT
OBJECTS
CAN
BE
MODELED
BY
PARTS
IN
A
DE
FORMABLE
CONFIGURATION
PROVIDES
AN
ELEGANT
FRAMEWORK
FOR
REPRESENTING
OBJECT
CATEGORIES
WHILE
THESE
MODELS
ARE
APPEALING
FROM
A
CONCEPTUAL
POINT
OF
VIEW
IT
HAS
BEEN
DIFFICULT
TO
ESTABLISH
THEIR
VALUE
IN
PRAC
TICE
ON
DIFFICULT
DATASETS
DEFORMABLE
MODELS
ARE
OFTEN
OUT
PERFORMED
BY
CONCEPTUALLY
WEAKER
MODELS
SUCH
AS
RIGID
TEMPLATES
OR
BAG
OF
FEATURES
ONE
OF
OUR
MAIN
GOALS
IS
TO
ADDRESS
THIS
PERFORMANCE
GAP
OUR
MODELS
INCLUDE
BOTH
A
COARSE
GLOBAL
TEMPLATE
COV
ERING
AN
ENTIRE
OBJECT
AND
HIGHER
RESOLUTION
PART
TEMPLATES
THE
TEMPLATES
REPRESENT
HISTOGRAM
OF
GRADIENT
FEATURES
AS
IN
WE
TRAIN
MODELS
DISCRIMINATIVELY
HOW
EVER
OUR
SYSTEM
IS
SEMI
SUPERVISED
TRAINED
WITH
A
MAX
MARGIN
FRAMEWORK
AND
DOES
NOT
RELY
ON
FEATURE
DETECTION
WE
ALSO
DESCRIBE
A
SIMPLE
AND
EFFECTIVE
STRATEGY
FOR
LEARN
ING
PARTS
FROM
WEAKLY
LABELED
DATA
IN
CONTRAST
TO
COMPUTA
TIONALLY
DEMANDING
APPROACHES
SUCH
AS
WE
CAN
LEARN
A
MODEL
IN
HOURS
ON
A
SINGLE
CPU
ANOTHER
CONTRIBUTION
OF
OUR
WORK
IS
A
NEW
METHODOLOGY
FOR
DISCRIMINATIVE
TRAINING
WE
GENERALIZE
SVMS
FOR
HAN
DLING
LATENT
VARIABLES
SUCH
AS
PART
POSITIONS
AND
INTRODUCE
A
NEW
METHOD
FOR
DATA
MINING
HARD
NEGATIVE
EXAMPLES
DUR
ING
TRAINING
WE
BELIEVE
THAT
HANDLING
PARTIALLY
LABELED
DATA
IS
A
SIGNIFICANT
ISSUE
IN
MACHINE
LEARNING
FOR
COMPUTER
VI
SION
FOR
EXAMPLE
THE
PASCAL
DATASET
ONLY
SPECIFIES
A
BOUNDING
BOX
FOR
EACH
POSITIVE
EXAMPLE
OF
AN
OBJECT
WE
TREAT
THE
POSITION
OF
EACH
OBJECT
PART
AS
A
LATENT
VARIABLE
WE
ALSO
TREAT
THE
EXACT
LOCATION
OF
THE
OBJECT
AS
A
LATENT
VARI
ABLE
REQUIRING
ONLY
THAT
OUR
CLASSIFIER
SELECT
A
WINDOW
THAT
HAS
LARGE
OVERLAP
WITH
THE
LABELED
BOUNDING
BOX
A
LATENT
SVM
LIKE
A
HIDDEN
CRF
LEADS
TO
A
NON
CONVEX
TRAINING
PROBLEM
HOWEVER
UNLIKE
A
HIDDEN
CRF
A
LATENT
SVM
IS
SEMI
CONVEX
AND
THE
TRAINING
PROBLEM
BE
COMES
CONVEX
ONCE
LATENT
INFORMATION
IS
SPECIFIED
FOR
THE
POSITIVE
TRAINING
EXAMPLES
THIS
LEADS
TO
A
GENERAL
COORDI
NATE
DESCENT
ALGORITHM
FOR
LATENT
SVMS
SYSTEM
OVERVIEW
OUR
SYSTEM
USES
A
SCANNING
WINDOW
APPROACH
A
MODEL
FOR
AN
OBJECT
CONSISTS
OF
A
GLOBAL
ROOT
FILTER
AND
SEVERAL
PART
MODELS
EACH
PART
MODEL
SPECIFIES
A
SPATIAL
MODEL
AND
A
PART
FILTER
THE
SPATIAL
MODEL
DEFINES
A
SET
OF
ALLOWED
PLACEMENTS
FOR
A
PART
RELATIVE
TO
A
DETECTION
WINDOW
AND
A
DEFORMATION
COST
FOR
EACH
PLACEMENT
THE
SCORE
OF
A
DETECTION
WINDOW
IS
THE
SCORE
OF
THE
ROOT
FILTER
ON
THE
WINDOW
PLUS
THE
SUM
OVER
PARTS
OF
THE
MAXI
MUM
OVER
PLACEMENTS
OF
THAT
PART
OF
THE
PART
FILTER
SCORE
ON
THE
RESULTING
SUBWINDOW
MINUS
THE
DEFORMATION
COST
THIS
IS
SIMILAR
TO
CLASSICAL
PART
BASED
MODELS
BOTH
ROOT
AND
PART
FILTERS
ARE
SCORED
BY
COMPUTING
THE
DOT
PRODUCT
BE
TWEEN
A
SET
OF
WEIGHTS
AND
HISTOGRAM
OF
GRADIENT
HOG
FEATURES
WITHIN
A
WINDOW
THE
ROOT
FILTER
IS
EQUIVALENT
TO
A
DALAL
TRIGGS
MODEL
THE
FEATURES
FOR
THE
PART
FILTERS
ARE
COMPUTED
AT
TWICE
THE
SPATIAL
RESOLUTION
OF
THE
ROOT
FILTER
OUR
MODEL
IS
DEFINED
AT
A
FIXED
SCALE
AND
WE
DETECT
OBJECTS
BY
SEARCHING
OVER
AN
IMAGE
PYRAMID
IN
TRAINING
WE
ARE
GIVEN
A
SET
OF
IMAGES
ANNOTATED
WITH
BOUNDING
BOXES
AROUND
EACH
INSTANCE
OF
AN
OBJECT
WE
RE
DUCE
THE
DETECTION
PROBLEM
TO
A
BINARY
CLASSIFICATION
PROB
LEM
EACH
EXAMPLE
X
IS
SCORED
BY
A
FUNCTION
OF
THE
FORM
FΒ
X
MAXZ
Β
Φ
X
Z
HERE
Β
IS
A
VECTOR
OF
MODEL
PA
RAMETERS
AND
Z
ARE
LATENT
VALUES
E
G
THE
PART
PLACEMENTS
TO
LEARN
A
MODEL
WE
DEFINE
A
GENERALIZATION
OF
SVMS
THAT
WE
CALL
LATENT
VARIABLE
SVM
LSVM
AN
IMPORTANT
PROP
ERTY
OF
LSVMS
IS
THAT
THE
TRAINING
PROBLEM
BECOMES
CONVEX
IF
WE
FIX
THE
LATENT
VALUES
FOR
POSITIVE
EXAMPLES
THIS
CAN
BE
USED
IN
A
COORDINATE
DESCENT
ALGORITHM
IN
PRACTICE
WE
ITERATIVELY
APPLY
CLASSICAL
SVM
TRAINING
TO
TRIPLES
XN
ZN
YN
WHERE
ZI
IS
SELECTED
TO
BE
THE
BEST
SCORING
LATENT
LABEL
FOR
XI
UNDER
THE
MODEL
LEARNED
IN
THE
PREVIOUS
ITERATION
AN
INITIAL
ROOT
FILTER
IS
GENERATED
FROM
THE
BOUNDING
BOXES
IN
THE
PASCAL
DATASET
THE
PARTS
ARE
INITIALIZED
FROM
THIS
ROOT
FILTER
MODEL
THE
UNDERLYING
BUILDING
BLOCKS
FOR
OUR
MODELS
ARE
THE
HISTOGRAM
OF
ORIENTED
GRADIENT
HOG
FEATURES
FROM
WE
REPRESENT
HOG
FEATURES
AT
TWO
DIFFERENT
SCALES
COARSE
FEATURES
ARE
CAPTURED
BY
A
RIGID
TEMPLATE
COVERING
AN
ENTIRE
IMAGE
PYRAMID
HOG
FEATURE
PYRAMID
FIGURE
THE
HOG
FEATURE
PYRAMID
AND
AN
OBJECT
HYPOTHESIS
DE
FINED
IN
TERMS
OF
A
PLACEMENT
OF
THE
ROOT
FILTER
NEAR
THE
TOP
OF
THE
PYRAMID
AND
THE
PART
FILTERS
NEAR
THE
BOTTOM
OF
THE
PYRAMID
DETECTION
WINDOW
FINER
SCALE
FEATURES
ARE
CAPTURED
BY
PART
TEMPLATES
THAT
CAN
BE
MOVED
WITH
RESPECT
TO
THE
DETECTION
WINDOW
THE
SPATIAL
MODEL
FOR
THE
PART
LOCATIONS
IS
EQUIV
ALENT
TO
A
STAR
GRAPH
OR
FAN
WHERE
THE
COARSE
TEMPLATE
SERVES
AS
A
REFERENCE
POSITION
HOG
REPRESENTATION
WE
FOLLOW
THE
CONSTRUCTION
IN
TO
DEFINE
A
DENSE
REPRE
SENTATION
OF
AN
IMAGE
AT
A
PARTICULAR
RESOLUTION
THE
IMAGE
IS
FIRST
DIVIDED
INTO
NON
OVERLAPPING
PIXEL
REGIONS
OR
CELLS
FOR
EACH
CELL
WE
ACCUMULATE
A
HISTOGRAM
OF
GRA
DIENT
ORIENTATIONS
OVER
PIXELS
IN
THAT
CELL
THESE
HISTOGRAMS
CAPTURE
LOCAL
SHAPE
PROPERTIES
BUT
ARE
ALSO
SOMEWHAT
INVARI
ANT
TO
SMALL
DEFORMATIONS
THE
GRADIENT
AT
EACH
PIXEL
IS
DISCRETIZED
INTO
ONE
OF
NINE
ORIENTATION
BINS
AND
EACH
PIXEL
VOTES
FOR
THE
ORIENTATION
OF
ITS
GRADIENT
WITH
A
STRENGTH
THAT
DEPENDS
ON
THE
GRADIENT
MAGNITUDE
FOR
COLOR
IMAGES
WE
COMPUTE
THE
GRADIENT
OF
EACH
COLOR
CHANNEL
AND
PICK
THE
CHANNEL
WITH
HIGHEST
GRADI
ENT
MAGNITUDE
AT
EACH
PIXEL
FINALLY
THE
HISTOGRAM
OF
EACH
CELL
IS
NORMALIZED
WITH
RESPECT
TO
THE
GRADIENT
ENERGY
IN
A
NEIGHBORHOOD
AROUND
IT
WE
LOOK
AT
THE
FOUR
BLOCKS
OF
CELLS
THAT
CONTAIN
A
PARTICULAR
CELL
AND
NORMALIZE
THE
HIS
TOGRAM
OF
THE
GIVEN
CELL
WITH
RESPECT
TO
THE
TOTAL
ENERGY
IN
EACH
OF
THESE
BLOCKS
THIS
LEADS
TO
A
VECTOR
OF
LENGTH
REPRESENTING
THE
LOCAL
GRADIENT
INFORMATION
INSIDE
A
CELL
WE
DEFINE
A
HOG
FEATURE
PYRAMID
BY
COMPUTING
HOG
FEATURES
OF
EACH
LEVEL
OF
A
STANDARD
IMAGE
PYRAMID
SEE
FIG
URE
FEATURES
AT
THE
TOP
OF
THIS
PYRAMID
CAPTURE
COARSE
GRADIENTS
HISTOGRAMMED
OVER
FAIRLY
LARGE
AREAS
OF
THE
INPUT
IMAGE
WHILE
FEATURES
AT
THE
BOTTOM
OF
THE
PYRAMID
CAPTURE
FINER
GRADIENTS
HISTOGRAMMED
OVER
SMALL
AREAS
FILTERS
OF
EACH
PART
RELATIVE
TO
THE
ROOT
THE
SPATIAL
TERM
FILTERS
ARE
RECTANGULAR
TEMPLATES
SPECIFYING
WEIGHTS
FOR
N
N
SUBWINDOWS
OF
A
HOG
PYRAMID
A
W
BY
H
FILTER
F
IS
A
VECTOR
WITH
W
H
WEIGHTS
THE
SCORE
OF
A
FILTER
IS
I
I
I
I
DEFINED
BY
TAKING
THE
DOT
PRODUCT
OF
THE
WEIGHT
VECTOR
AND
THE
FEATURES
IN
A
W
H
SUBWINDOW
OF
A
HOG
PYRAMID
THE
SYSTEM
IN
USES
A
SINGLE
FILTER
TO
DEFINE
AN
OBJECT
MODEL
THAT
SYSTEM
DETECTS
OBJECTS
FROM
A
PARTICULAR
CLASS
BY
SCORING
EVERY
W
H
SUBWINDOW
OF
A
HOG
PYRAMID
AND
THRESHOLDING
THE
SCORES
LET
H
BE
A
HOG
PYRAMID
AND
P
X
Y
L
BE
A
CELL
IN
THE
L
TH
LEVEL
OF
THE
PYRAMID
LET
Φ
H
P
W
H
DENOTE
THE
VECTOR
OBTAINED
BY
CONCATENATING
THE
HOG
FEATURES
IN
THE
W
H
SUBWINDOW
OF
H
WITH
TOP
LEFT
CORNER
AT
P
THE
SCORE
OF
F
ON
THIS
DETECTION
WINDOW
IS
F
Φ
H
P
W
H
BELOW
WE
USE
Φ
H
P
TO
DENOTE
Φ
H
P
W
H
WHEN
THE
DIMENSIONS
ARE
CLEAR
FROM
CONTEXT
DEFORMABLE
PARTS
HERE
WE
CONSIDER
MODELS
DEFINED
BY
A
COARSE
ROOT
FILTER
THAT
COVERS
THE
ENTIRE
OBJECT
AND
HIGHER
RESOLUTION
PART
FILTERS
COVERING
SMALLER
PARTS
OF
THE
OBJECT
FIGURE
ILLUSTRATES
A
PLACEMENT
OF
SUCH
A
MODEL
IN
A
HOG
PYRAMID
THE
ROOT
FIL
WHERE
X
I
Y
I
XI
YI
X
Y
VI
SI
GIVES
THE
LO
CATION
OF
THE
I
TH
PART
RELATIVE
TO
THE
ROOT
LOCATION
BOTH
X
I
AND
Y
I
SHOULD
BE
BETWEEN
AND
THERE
IS
A
LARGE
EXPONENTIAL
NUMBER
OF
PLACEMENTS
FOR
A
MODEL
IN
A
HOG
PYRAMID
WE
USE
DYNAMIC
PROGRAMMING
AND
DISTANCE
TRANSFORMS
TECHNIQUES
TO
COMPUTE
THE
BEST
LOCATION
FOR
THE
PARTS
OF
A
MODEL
AS
A
FUNCTION
OF
THE
ROOT
LOCATION
THIS
TAKES
O
NK
TIME
WHERE
N
IS
THE
NUMBER
OF
PARTS
IN
THE
MODEL
AND
K
IS
THE
NUMBER
OF
CELLS
IN
THE
HOG
PYRAMID
TO
DETECT
OBJECTS
IN
AN
IMAGE
WE
SCORE
ROOT
LOCATIONS
ACCORDING
TO
THE
BEST
POSSIBLE
PLACEMENT
OF
THE
PARTS
AND
THRESHOLD
THIS
SCORE
THE
SCORE
OF
A
PLACEMENT
Z
CAN
BE
EXPRESSED
IN
TERMS
OF
THE
DOT
PRODUCT
Β
Ψ
H
Z
BETWEEN
A
VECTOR
OF
MODEL
PARAMETERS
Β
AND
A
VECTOR
Ψ
H
Z
Β
FN
AN
BN
Ψ
H
Z
Φ
H
Φ
H
Φ
H
PN
X
Y
X
Y
X
N
Y
N
X
Y
TER
LOCATION
DEFINES
THE
DETECTION
WINDOW
THE
PIXELS
INSIDE
THE
CELLS
COVERED
BY
THE
FILTER
THE
PART
FILTERS
ARE
PLACED
SEVERAL
LEVELS
DOWN
IN
THE
PYRAMID
SO
THE
HOG
CELLS
AT
THAT
LEVEL
HAVE
HALF
THE
SIZE
OF
CELLS
IN
THE
ROOT
FILTER
LEVEL
WE
HAVE
FOUND
THAT
USING
HIGHER
RESOLUTION
FEATURES
FOR
DEFINING
PART
FILTERS
IS
ESSENTIAL
FOR
OBTAINING
HIGH
RECOGNI
TION
PERFORMANCE
WITH
THIS
APPROACH
THE
PART
FILTERS
REPRE
SENT
FINER
RESOLUTION
EDGES
THAT
ARE
LOCALIZED
TO
GREATER
AC
CURACY
WHEN
COMPARED
TO
THE
EDGES
REPRESENTED
IN
THE
ROOT
FILTER
FOR
EXAMPLE
CONSIDER
BUILDING
A
MODEL
FOR
A
FACE
THE
ROOT
FILTER
COULD
CAPTURE
COARSE
RESOLUTION
EDGES
SUCH
AS
THE
FACE
BOUNDARY
WHILE
THE
PART
FILTERS
COULD
CAPTURE
DETAILS
SUCH
AS
EYES
NOSE
AND
MOUTH
THE
MODEL
FOR
AN
OBJECT
WITH
N
PARTS
IS
FORMALLY
DEFINED
BY
A
ROOT
FILTER
AND
A
SET
OF
PART
MODELS
PN
WHERE
PI
FI
VI
SI
AI
BI
HERE
FI
IS
A
FILTER
FOR
THE
I
TH
PART
VI
IS
A
TWO
DIMENSIONAL
VECTOR
SPECIFYING
THE
CENTER
FOR
A
BOX
OF
POSSIBLE
POSITIONS
FOR
PART
I
RELATIVE
TO
THE
ROOT
PO
SITION
SI
GIVES
THE
SIZE
OF
THIS
BOX
WHILE
AI
AND
BI
ARE
TWO
DIMENSIONAL
VECTORS
SPECIFYING
COEFFICIENTS
OF
A
QUADRATIC
FUNCTION
MEASURING
A
SCORE
FOR
EACH
POSSIBLE
PLACEMENT
OF
THE
I
TH
PART
FIGURE
ILLUSTRATES
A
PERSON
MODEL
A
PLACEMENT
OF
A
MODEL
IN
A
HOG
PYRAMID
IS
GIVEN
BY
Z
PN
WHERE
PI
XI
YI
LI
IS
THE
LOCATION
OF
THE
ROOT
FILTER
WHEN
I
AND
THE
LOCATION
OF
THE
I
TH
PART
WHEN
I
WE
ASSUME
THE
LEVEL
OF
EACH
PART
IS
SUCH
THAT
A
HOG
CELL
AT
THAT
LEVEL
HAS
HALF
THE
SIZE
OF
A
HOG
CELL
AT
THE
ROOT
LEVEL
THE
SCORE
OF
A
PLACEMENT
IS
GIVEN
BY
THE
SCORES
OF
EACH
FILTER
THE
DATA
TERM
PLUS
A
SCORE
OF
THE
PLACEMENT
WE
USE
THIS
REPRESENTATION
FOR
LEARNING
THE
MODEL
PARAME
TERS
AS
IT
MAKES
A
CONNECTION
BETWEEN
OUR
DEFORMABLE
MOD
ELS
AND
LINEAR
CLASSIFIERS
ON
INTERESTING
ASPECT
OF
THE
SPATIAL
MODELS
DEFINED
HERE
IS
THAT
WE
ALLOW
FOR
THE
COEFFICIENTS
AI
BI
TO
BE
NEGATIVE
THIS
IS
MORE
GENERAL
THAN
THE
QUADRATIC
SPRING
COST
THAT
HAS
BEEN
USED
IN
PREVIOUS
WORK
LEARNING
THE
PASCAL
TRAINING
DATA
CONSISTS
OF
A
LARGE
SET
OF
IM
AGES
WITH
BOUNDING
BOXES
AROUND
EACH
INSTANCE
OF
AN
OB
JECT
WE
REDUCE
THE
PROBLEM
OF
LEARNING
A
DEFORMABLE
PART
MODEL
WITH
THIS
DATA
TO
A
BINARY
CLASSIFICATION
PROBLEM
LET
D
XN
YN
BE
A
SET
OF
LABELED
EXAM
PLES
WHERE
YI
AND
XI
SPECIFIES
A
HOG
PYRAMID
H
XI
TOGETHER
WITH
A
RANGE
Z
XI
OF
VALID
PLACEMENTS
FOR
THE
ROOT
AND
PART
FILTERS
WE
CONSTRUCT
A
POSITIVE
EXAM
PLE
FROM
EACH
BOUNDING
BOX
IN
THE
TRAINING
SET
FOR
THESE
EX
AMPLES
WE
DEFINE
Z
XI
SO
THE
ROOT
FILTER
MUST
BE
PLACED
TO
OVERLAP
THE
BOUNDING
BOX
BY
AT
LEAST
NEGATIVE
EXAM
PLES
COME
FROM
IMAGES
THAT
DO
NOT
CONTAIN
THE
TARGET
OBJECT
EACH
PLACEMENT
OF
THE
ROOT
FILTER
IN
SUCH
AN
IMAGE
YIELDS
A
NEGATIVE
TRAINING
EXAMPLE
NOTE
THAT
FOR
THE
POSITIVE
EXAMPLES
WE
TREAT
BOTH
THE
PART
LOCATIONS
AND
THE
EXACT
LOCATION
OF
THE
ROOT
FILTER
AS
LATENT
VARIABLES
WE
HAVE
FOUND
THAT
ALLOWING
UNCERTAINTY
IN
THE
ROOT
LOCATION
DURING
TRAINING
SIGNIFICANTLY
IMPROVES
THE
PER
FORMANCE
OF
THE
SYSTEM
SEE
SECTION
LATENT
SVMS
A
LATENT
SVM
IS
DEFINED
AS
FOLLOWS
WE
ASSUME
THAT
EACH
EXAMPLE
X
IS
SCORED
BY
A
FUNCTION
OF
THE
FORM
NEGATIVE
EXAMPLES
AT
A
TIME
INSTEAD
IT
IS
COMMON
TO
CON
STRUCT
TRAINING
DATA
CONSISTING
OF
THE
POSITIVE
INSTANCES
AND
HARD
NEGATIVE
INSTANCES
WHERE
THE
HARD
NEGATIVES
ARE
DATA
MINED
FROM
THE
VERY
LARGE
SET
OF
POSSIBLE
NEGATIVE
EXAMPLES
FΒ
X
MAX
Z
Z
X
Β
Φ
X
Z
HERE
WE
DESCRIBE
A
GENERAL
METHOD
FOR
DATA
MINING
EX
AMPLES
FOR
SVMS
AND
LATENT
SVMS
THE
METHOD
ITERATIVELY
WHERE
Β
IS
A
VECTOR
OF
MODEL
PARAMETERS
AND
Z
IS
A
SET
OF
LATENT
VALUES
FOR
OUR
DEFORMABLE
MODELS
WE
DEFINE
Φ
X
Z
Ψ
H
X
Z
SO
THAT
Β
Φ
X
Z
IS
THE
SCORE
OF
PLACING
THE
MODEL
ACCORDING
TO
Z
IN
ANALOGY
TO
CLASSICAL
SVMS
WE
WOULD
LIKE
TO
TRAIN
Β
FROM
LABELED
EXAMPLES
D
XN
YN
BY
OPTIMIZING
THE
FOLLOWING
OBJECTIVE
FUNCTION
Β
D
ARGMIN
Λ
Β
MAX
YIFΒ
XI
SOLVES
SUBPROBLEMS
USING
ONLY
HARD
INSTANCES
THE
INNOVA
TION
OF
OUR
APPROACH
IS
A
THEORETICAL
GUARANTEE
THAT
IT
LEADS
TO
THE
EXACT
SOLUTION
OF
THE
TRAINING
PROBLEM
DEFINED
USING
THE
COMPLETE
TRAINING
SET
OUR
RESULTS
REQUIRE
THE
USE
OF
A
MARGIN
SENSITIVE
DEFINITION
OF
HARD
EXAMPLES
THE
RESULTS
DESCRIBED
HERE
APPLY
BOTH
TO
CLASSICAL
SVMS
AND
TO
THE
PROBLEM
DEFINED
BY
STEP
OF
THE
COORDINATE
DE
SCENT
ALGORITHM
FOR
LATENT
SVMS
WE
OMIT
THE
PROOFS
OF
THE
THEOREMS
DUE
TO
LACK
OF
SPACE
THESE
RESULTS
ARE
RELATED
TO
WORKING
SET
METHODS
BY
RESTRICTING
THE
LATENT
DOMAINS
Z
XI
TO
A
SINGLE
CHOICE
FΒ
BECOMES
LINEAR
IN
Β
AND
WE
OBTAIN
LINEAR
SVMS
AS
A
SPECIAL
CASE
OF
LATENT
SVMS
LATENT
SVMS
ARE
INSTANCES
OF
THE
GENERAL
CLASS
OF
ENERGY
BASED
MODELS
SEMI
CONVEXITY
NOTE
THAT
FΒ
X
AS
DEFINED
IN
IS
A
MAXIMUM
OF
FUNC
TIONS
EACH
OF
WHICH
IS
LINEAR
IN
Β
HENCE
FΒ
X
IS
CONVEX
IN
Β
THIS
IMPLIES
THAT
THE
HINGE
LOSS
MAX
YIFΒ
XI
IS
CONVEX
IN
Β
WHEN
YI
THAT
IS
THE
LOSS
FUNCTION
IS
CONVEX
IN
Β
FOR
NEGATIVE
EXAMPLES
WE
CALL
THIS
PROPERTY
OF
THE
LOSS
FUNCTION
SEMI
CONVEXITY
CONSIDER
AN
LSVM
WHERE
THE
LATENT
DOMAINS
Z
XI
FOR
THE
POSITIVE
EXAMPLES
ARE
RESTRICTED
TO
A
SINGLE
CHOICE
THE
LOSS
DUE
TO
EACH
POSITIVE
EXAMPLE
IS
NOW
CONVEX
COMBINED
WITH
THE
SEMI
CONVEXITY
PROPERTY
BECOMES
CONVEX
IN
Β
IF
THE
LABELS
FOR
THE
POSITIVE
EXAMPLES
ARE
NOT
FIXED
WE
CAN
COMPUTE
A
LOCAL
OPTIMUM
OF
USING
A
COORDINATE
DE
SCENT
ALGORITHM
HOLDING
Β
FIXED
OPTIMIZE
THE
LATENT
VALUES
FOR
THE
POS
ITIVE
EXAMPLES
ZI
ARGMAXZ
Z
XI
Β
Φ
X
Z
HOLDING
ZI
FIXED
FOR
POSITIVE
EXAMPLES
OPTIMIZE
Β
BY
SOLVING
THE
CONVEX
PROBLEM
DEFINED
ABOVE
IT
CAN
BE
SHOWN
THAT
BOTH
STEPS
ALWAYS
IMPROVE
OR
MAINTAIN
THE
VALUE
OF
THE
OBJECTIVE
FUNCTION
IN
IF
BOTH
STEPS
MAIN
TAIN
THE
VALUE
WE
HAVE
A
STRONG
LOCAL
OPTIMUM
OF
IN
THE
SENSE
THAT
STEP
SEARCHES
OVER
AN
EXPONENTIALLY
LARGE
SPACE
OF
LATENT
LABELS
FOR
POSITIVE
EXAMPLES
WHILE
STEP
SIMULTA
NEOUSLY
SEARCHES
OVER
WEIGHT
VECTORS
AND
AN
EXPONENTIALLY
LARGE
SPACE
OF
LATENT
LABELS
FOR
NEGATIVE
EXAMPLES
DATA
MINING
HARD
NEGATIVES
IN
OBJECT
DETECTION
THE
VAST
MAJORITY
OF
TRAINING
EXAM
PLES
ARE
NEGATIVE
THIS
MAKES
IT
INFEASIBLE
TO
CONSIDER
ALL
M
Β
D
X
Y
D
YFΒ
X
THAT
IS
M
Β
D
ARE
TRAINING
EXAMPLES
THAT
ARE
INCORRECTLY
CLASSIFIED
OR
NEAR
THE
MARGIN
OF
THE
CLASSIFIER
DEFINED
BY
Β
WE
CAN
SHOW
THAT
Β
D
ONLY
DEPENDS
ON
HARD
INSTANCES
THEOREM
LET
C
BE
A
SUBSET
OF
THE
EXAMPLES
IN
D
IF
M
Β
D
D
C
THEN
Β
C
Β
D
THIS
IMPLIES
THAT
IN
PRINCIPLE
WE
COULD
TRAIN
A
MODEL
US
ING
A
SMALL
SET
OF
EXAMPLES
HOWEVER
THIS
SET
IS
DEFINED
IN
TERMS
OF
THE
OPTIMAL
MODEL
Β
D
GIVEN
A
FIXED
Β
WE
CAN
USE
M
Β
D
TO
APPROXIMATE
M
Β
D
D
THIS
SUGGESTS
AN
ITERATIVE
ALGORITHM
WHERE
WE
REPEATEDLY
COMPUTE
A
MODEL
FROM
THE
HARD
INSTANCES
DE
FINED
BY
THE
MODEL
FROM
THE
LAST
ITERATION
THIS
IS
FURTHER
JUSTIFIED
BY
THE
FOLLOWING
FIXED
POINT
THEOREM
THEOREM
IF
Β
M
Β
D
Β
THEN
Β
Β
D
LET
C
BE
AN
INITIAL
CACHE
OF
EXAMPLES
IN
PRACTICE
WE
CAN
TAKE
THE
POSITIVE
EXAMPLES
TOGETHER
WITH
RANDOM
NEGA
TIVE
EXAMPLES
CONSIDER
THE
FOLLOWING
ITERATIVE
ALGORITHM
LET
Β
Β
C
SHRINK
C
BY
LETTING
C
M
Β
C
GROW
C
BY
ADDING
EXAMPLES
FROM
M
Β
D
UP
TO
A
MEMORY
LIMIT
L
THEOREM
IF
C
L
AFTER
EACH
ITERATION
OF
STEP
THE
ALGORITHM
WILL
CONVERGE
TO
Β
Β
D
IN
FINITE
TIME
IMPLEMENTATION
DETAILS
MANY
OF
THE
IDEAS
DISCUSSED
HERE
ARE
ONLY
APPROXIMATELY
IMPLEMENTED
IN
OUR
CURRENT
SYSTEM
IN
PRACTICE
WHEN
TRAIN
ING
A
LATENT
SVM
WE
ITERATIVELY
APPLY
CLASSICAL
SVM
TRAIN
ING
TO
TRIPLES
XN
ZN
YN
WHERE
ZI
IS
SE
LECTED
TO
BE
THE
BEST
SCORING
LATENT
LABEL
FOR
XI
UNDER
THE
MODEL
TRAINED
IN
THE
PREVIOUS
ITERATION
EACH
OF
THESE
TRIPLES
LEADS
TO
AN
EXAMPLE
Φ
XI
ZI
YI
FOR
TRAINING
A
LINEAR
CLAS
SIFIER
THIS
ALLOWS
US
TO
USE
A
HIGHLY
OPTIMIZED
SVM
PACK
AGE
SVMLIGHT
ON
A
SINGLE
CPU
THE
ENTIRE
TRAINING
PROCESS
TAKES
TO
HOURS
PER
OBJECT
CLASS
IN
THE
PASCAL
DATASETS
INCLUDING
INITIALIZATION
OF
THE
PARTS
ROOT
FILTER
INITIALIZATION
FOR
EACH
CATEGORY
WE
AUTO
MATICALLY
SELECT
THE
DIMENSIONS
OF
THE
ROOT
FILTER
BY
LOOKING
AT
STATISTICS
OF
THE
BOUNDING
BOXES
IN
THE
TRAINING
DATA
WE
TRAIN
AN
INITIAL
ROOT
FILTER
USING
AN
SVM
WITH
NO
LATENT
VARIABLES
THE
POSITIVE
EXAMPLES
ARE
CONSTRUCTED
FROM
THE
UNOCCLUDED
TRAINING
EXAMPLES
AS
LABELED
IN
THE
PASCAL
DATA
THESE
EXAMPLES
ARE
ANISOTROPICALLY
SCALED
TO
THE
SIZE
AND
ASPECT
RATIO
OF
THE
FILTER
WE
USE
RANDOM
SUBWINDOWS
FROM
NEGATIVE
IMAGES
TO
GENERATE
NEGATIVE
EXAMPLES
ROOT
FILTER
UPDATE
GIVEN
THE
INITIAL
ROOT
FILTER
TRAINED
AS
ABOVE
FOR
EACH
BOUNDING
BOX
IN
THE
TRAINING
SET
WE
FIND
THE
BEST
SCORING
PLACEMENT
FOR
THE
FILTER
THAT
SIGNIFICANTLY
OVERLAPS
WITH
THE
BOUNDING
BOX
WE
DO
THIS
USING
THE
ORIG
INAL
UN
SCALED
IMAGES
WE
RETRAIN
WITH
THE
NEW
POSITIVE
SET
AND
THE
ORIGINAL
RANDOM
NEGATIVE
SET
ITERATING
TWICE
PART
INITIALIZATION
WE
EMPLOY
A
SIMPLE
HEURISTIC
TO
INI
TIALIZE
SIX
PARTS
FROM
THE
ROOT
FILTER
TRAINED
ABOVE
FIRST
WE
SELECT
AN
AREA
A
SUCH
THAT
EQUALS
OF
THE
AREA
OF
THE
ROOT
FILTER
WE
GREEDILY
SELECT
THE
RECTANGULAR
REGION
OF
AREA
A
FROM
THE
ROOT
FILTER
THAT
HAS
THE
MOST
POSITIVE
ENERGY
WE
ZERO
OUT
THE
WEIGHTS
IN
THIS
REGION
AND
REPEAT
UNTIL
SIX
PARTS
ARE
SELECTED
THE
PART
FILTERS
ARE
INITIALIZED
FROM
THE
ROOT
FIL
TER
VALUES
IN
THE
SUBWINDOW
SELECTED
FOR
THE
PART
BUT
FILLED
IN
TO
HANDLE
THE
HIGHER
SPATIAL
RESOLUTION
OF
THE
PART
THE
INITIAL
DEFORMATION
COSTS
MEASURE
THE
SQUARED
NORM
OF
A
DIS
PLACEMENT
WITH
AI
AND
BI
MODEL
UPDATE
TO
UPDATE
A
MODEL
WE
CONSTRUCT
NEW
TRAINING
DATA
TRIPLES
FOR
EACH
POSITIVE
BOUNDING
BOX
IN
THE
TRAINING
DATA
WE
APPLY
THE
EXISTING
DETECTOR
AT
ALL
POSITIONS
AND
SCALES
WITH
AT
LEAST
A
OVERLAP
WITH
THE
GIVEN
BOUND
ING
BOX
AMONG
THESE
WE
SELECT
THE
HIGHEST
SCORING
PLACE
MENT
AS
THE
POSITIVE
EXAMPLE
CORRESPONDING
TO
THIS
TRAINING
BOUNDING
BOX
FIGURE
NEGATIVE
EXAMPLES
ARE
SELECTED
BY
FINDING
HIGH
SCORING
DETECTIONS
IN
IMAGES
NOT
CONTAINING
THE
TARGET
OBJECT
WE
ADD
NEGATIVE
EXAMPLES
TO
A
CACHE
UN
TIL
WE
ENCOUNTER
FILE
SIZE
LIMITS
A
NEW
MODEL
IS
TRAINED
BY
RUNNING
SVMLIGHT
ON
THE
POSITIVE
AND
NEGATIVE
EXAMPLES
EACH
LABELED
WITH
PART
PLACEMENTS
WE
UPDATE
THE
MODEL
TIMES
USING
THE
CACHE
SCHEME
DESCRIBED
ABOVE
IN
EACH
IT
ERATION
WE
KEEP
THE
HARD
INSTANCES
FROM
THE
PREVIOUS
CACHE
AND
ADD
AS
MANY
NEW
HARD
INSTANCES
AS
POSSIBLE
WITHIN
THE
MEMORY
LIMIT
TOWARD
THE
FINAL
ITERATIONS
WE
ARE
ABLE
TO
INCLUDE
ALL
HARD
INSTANCES
M
Β
D
IN
THE
CACHE
PICKED
A
SIMPLE
HEURISTIC
BY
CROSS
VALIDATING
OVER
OBJECT
CLASSES
WE
SET
THE
MODEL
ASPECT
TO
BE
THE
MOST
COMMON
MODE
ASPECT
IN
THE
DATA
WE
SET
THE
MODEL
SIZE
TO
BE
THE
LARGEST
SIZE
NOT
LARGER
THAN
OF
THE
DATA
FIGURE
THE
IMAGE
ON
THE
LEFT
SHOWS
THE
OPTIMIZATION
OF
THE
LA
TENT
VARIABLES
FOR
A
POSITIVE
EXAMPLE
THE
DOTTED
BOX
IS
THE
BOUND
ING
BOX
LABEL
PROVIDED
IN
THE
PASCAL
TRAINING
SET
THE
LARGE
SOLID
BOX
SHOWS
THE
PLACEMENT
OF
THE
DETECTION
WINDOW
WHILE
THE
SMALLER
SOLID
BOXES
SHOW
THE
PLACEMENTS
OF
THE
PARTS
THE
IMAGE
ON
THE
RIGHT
SHOWS
A
HARD
NEGATIVE
EXAMPLE
RESULTS
WE
EVALUATED
OUR
SYSTEM
USING
THE
PASCAL
VOC
AND
CHALLENGE
DATASETS
AND
PROTOCOL
WE
REFER
TO
FOR
DETAILS
BUT
EMPHASIZE
THAT
BOTH
CHALLENGES
ARE
WIDELY
ACKNOWLEDGED
AS
DIFFICULT
TESTBEDS
FOR
OBJECT
DETEC
TION
EACH
DATASET
CONTAINS
SEVERAL
THOUSAND
IMAGES
OF
REAL
WORLD
SCENES
THE
DATASETS
SPECIFY
GROUND
TRUTH
BOUNDING
BOXES
FOR
SEVERAL
OBJECT
CLASSES
AND
A
DETECTION
IS
CONSID
ERED
CORRECT
WHEN
IT
OVERLAPS
MORE
THAN
WITH
A
GROUND
TRUTH
BOUNDING
BOX
ONE
SCORES
A
SYSTEM
BY
THE
AVERAGE
PRECISION
AP
OF
ITS
PRECISION
RECALL
CURVE
ACROSS
A
TESTSET
RECENT
WORK
IN
PEDESTRIAN
DETECTION
HAS
TENDED
TO
REPORT
DETECTION
RATES
VERSUS
FALSE
POSITIVES
PER
WINDOW
MEASURED
WITH
CROPPED
POSITIVE
EXAMPLES
AND
NEGATIVE
IMAGES
WITH
OUT
OBJECTS
OF
INTEREST
THESE
SCORES
ARE
TIED
TO
THE
RESO
LUTION
OF
THE
SCANNING
WINDOW
SEARCH
AND
IGNORE
EFFECTS
OF
NON
MAXIMUM
SUPPRESSION
MAKING
IT
DIFFICULT
TO
COMPARE
DIFFERENT
SYSTEMS
WE
BELIEVE
THE
PASCAL
SCORING
METHOD
GIVES
A
MORE
RELIABLE
MEASURE
OF
PERFORMANCE
THE
CHALLENGE
HAS
OBJECT
CATEGORIES
WE
ENTERED
A
PRELIMINARY
VERSION
OF
OUR
SYSTEM
IN
THE
OFFICIAL
COMPETI
TION
AND
OBTAINED
THE
BEST
SCORE
IN
CATEGORIES
OUR
CURRENT
SYSTEM
OBTAINS
THE
HIGHEST
SCORE
IN
CATEGORIES
AND
THE
SECOND
HIGHEST
SCORE
IN
CATEGORIES
TABLE
SUMMARIZES
THE
RESULTS
OUR
SYSTEM
PERFORMS
WELL
ON
RIGID
OBJECTS
SUCH
AS
CARS
AND
SOFAS
AS
WELL
AS
HIGHLY
DEFORMABLE
OBJECTS
SUCH
AS
PER
SONS
AND
HORSES
WE
ALSO
NOTE
THAT
OUR
SYSTEM
IS
SUCCESSFUL
WHEN
GIVEN
A
LARGE
OR
SMALL
AMOUNT
OF
TRAINING
DATA
THERE
ARE
ROUGHLY
POSITIVE
TRAINING
EXAMPLES
IN
THE
PERSON
CATEGORY
BUT
ONLY
IN
THE
SOFA
CATEGORY
FIGURE
SHOWS
SOME
OF
THE
MODELS
WE
LEARNED
FIGURE
SHOWS
SOME
EX
AMPLE
DETECTIONS
WE
EVALUATED
DIFFERENT
COMPONENTS
OF
OUR
SYSTEM
ON
THE
LONGER
ESTABLISHED
PERSON
DATASET
THE
TOP
AP
SCORE
TABLE
PASCAL
VOC
RESULTS
AVERAGE
PRECISION
SCORES
OF
OUR
SYSTEM
AND
OTHER
SYSTEMS
THAT
ENTERED
THE
COMPETITION
EMPTY
BOXES
INDICATE
THAT
A
METHOD
WAS
NOT
TESTED
IN
THE
CORRESPONDING
CLASS
THE
BEST
SCORE
IN
EACH
CLASS
IS
SHOWN
IN
BOLD
OUR
CURRENT
SYSTEM
RANKS
FIRST
IN
OUT
OF
CLASSES
A
PRELIMINARY
VERSION
OF
OUR
SYSTEM
RANKED
FIRST
IN
CLASSES
IN
THE
OFFICIAL
COMPETITION
BOTTLE
CAR
SOFA
BICYCLE
FIGURE
SOME
MODELS
LEARNED
FROM
THE
PASCAL
VOC
DATASET
WE
SHOW
THE
TOTAL
ENERGY
IN
EACH
ORIENTATION
OF
THE
HOG
CELLS
IN
THE
ROOT
AND
PART
FILTERS
WITH
THE
PART
FILTERS
PLACED
AT
THE
CENTER
OF
THE
ALLOWABLE
DISPLACEMENTS
WE
ALSO
SHOW
THE
SPATIAL
MODEL
FOR
EACH
PART
WHERE
BRIGHT
VALUES
REPRESENT
CHEAP
PLACEMENTS
AND
DARK
VALUES
REPRESENT
EXPENSIVE
PLACEMENTS
IN
THE
PASCAL
COMPETITION
WAS
OBTAINED
USING
A
RIGID
TEMPLATE
MODEL
OF
HOG
FEATURES
THE
BEST
PREVIOUS
RE
SULT
OF
ADDS
A
SEGMENTATION
BASED
VERIFICATION
STEP
FIGURE
SUMMARIZES
THE
PERFORMANCE
OF
SEVERAL
MODELS
WE
TRAINED
OUR
ROOT
ONLY
MODEL
IS
EQUIVALENT
TO
THE
MODEL
FROM
AND
IT
SCORES
SLIGHTLY
HIGHER
AT
PERFORMANCE
JUMPS
TO
WHEN
THE
MODEL
IS
TRAINED
WITH
A
LSVM
THAT
SELECTS
A
LATENT
POSITION
AND
SCALE
FOR
EACH
POSITIVE
EXAMPLE
THIS
SUGGESTS
LSVMS
ARE
USEFUL
EVEN
FOR
RIGID
TEMPLATES
BECAUSE
THEY
ALLOW
FOR
SELF
ADJUSTMENT
OF
THE
DETECTION
WIN
DOW
IN
THE
TRAINING
EXAMPLES
ADDING
DEFORMABLE
PARTS
IN
CREASES
PERFORMANCE
TO
AP
A
FACTOR
OF
TWO
ABOVE
THE
BEST
PREVIOUS
SCORE
FINALLY
WE
TRAINED
A
MODEL
WITH
PARTS
BUT
NO
ROOT
FILTER
AND
OBTAINED
AP
THIS
ILLUSTRATES
THE
ADVANTAGE
OF
USING
A
MULTISCALE
REPRESENTATION
WE
ALSO
INVESTIGATED
THE
EFFECT
OF
THE
SPATIAL
MODEL
AND
ALLOWABLE
DEFORMATIONS
ON
THE
PERSON
DATASET
RECALL
THAT
SI
IS
THE
ALLOWABLE
DISPLACEMENT
OF
A
PART
MEASURED
IN
HOG
CELLS
WE
TRAINED
A
RIGID
MODEL
WITH
HIGH
RESOLUTION
PARTS
BY
SETTING
SI
TO
THIS
MODEL
OUTPERFORMS
THE
ROOT
ONLY
SYSTEM
BY
TO
IF
WE
INCREASE
THE
AMOUNT
OF
ALLOWABLE
DISPLACEMENTS
WITHOUT
USING
A
DEFORMATION
COST
WE
START
TO
APPROACH
A
BAG
OF
FEATURES
PERFORMANCE
PEAKS
AT
SI
SUGGESTING
IT
IS
USEFUL
TO
CONSTRAIN
THE
PART
DIS
PLACEMENTS
THE
OPTIMAL
STRATEGY
ALLOWS
FOR
LARGER
DISPLACE
MENTS
WHILE
USING
AN
EXPLICIT
DEFORMATION
COST
THE
FOLLOW
FIGURE
SOME
RESULTS
FROM
THE
PASCAL
DATASET
EACH
ROW
SHOWS
DETECTIONS
USING
A
MODEL
FOR
A
SPECIFIC
CLASS
PERSON
BOTTLE
CAR
SOFA
BICYCLE
HORSE
THE
FIRST
THREE
COLUMNS
SHOW
CORRECT
DETECTIONS
WHILE
THE
LAST
COLUMN
SHOWS
FALSE
POSITIVES
OUR
SYSTEM
IS
ABLE
TO
DETECT
OBJECTS
OVER
A
WIDE
RANGE
OF
SCALES
SUCH
AS
THE
CARS
AND
POSES
SUCH
AS
THE
HORSES
THE
SYSTEM
CAN
ALSO
DETECT
PARTIALLY
OCCLUDED
OBJECTS
SUCH
AS
A
PERSON
BEHIND
A
BUSH
NOTE
HOW
THE
FALSE
DETECTIONS
ARE
OFTEN
QUITE
REASONABLE
FOR
EXAMPLE
DETECTING
A
BUS
WITH
THE
CAR
MODEL
A
BICYCLE
SIGN
WITH
THE
BICYCLE
MODEL
OR
A
DOG
WITH
THE
HORSE
MODEL
IN
GENERAL
THE
PART
FILTERS
REPRESENT
MEANINGFUL
OBJECT
PARTS
THAT
ARE
WELL
LOCALIZED
IN
EACH
DETECTION
SUCH
AS
THE
HEAD
IN
THE
PERSON
MODEL
RELATIVE
ATTRIBUTES
DEVI
PARIKH
TOYOTA
TECHNOLOGICAL
INSTITUTE
CHICAGO
TTIC
KRISTEN
GRAUMAN
UNIVERSITY
OF
TEXAS
AT
AUSTIN
ABSTRACT
HUMAN
NAMEABLE
VISUAL
ATTRIBUTES
CAN
BENEFIT
VARI
OUS
RECOGNITION
TASKS
HOWEVER
EXISTING
TECHNIQUES
RESTRICT
THESE
PROPERTIES
TO
CATEGORICAL
LABELS
FOR
EXAMPLE
A
PER
SON
IS
SMILING
OR
NOT
A
SCENE
IS
DRY
OR
NOT
AND
THUS
FAIL
TO
CAPTURE
MORE
GENERAL
SEMANTIC
RELATIONSHIPS
WE
PROPOSE
TO
MODEL
RELATIVE
ATTRIBUTES
GIVEN
TRAINING
DATA
STATING
HOW
OBJECT
SCENE
CATEGORIES
RELATE
ACCORDING
TO
DIF
FERENT
ATTRIBUTES
WE
LEARN
A
RANKING
FUNCTION
PER
ATTRIBUTE
THE
LEARNED
RANKING
FUNCTIONS
PREDICT
THE
RELATIVE
STRENGTH
OF
EACH
PROPERTY
IN
NOVEL
IMAGES
WE
THEN
BUILD
A
GENERA
TIVE
MODEL
OVER
THE
JOINT
SPACE
OF
ATTRIBUTE
RANKING
OUTPUTS
AND
PROPOSE
A
NOVEL
FORM
OF
ZERO
SHOT
LEARNING
IN
WHICH
THE
SUPERVISOR
RELATES
THE
UNSEEN
OBJECT
CATEGORY
TO
PREVIOUSLY
SEEN
OBJECTS
VIA
ATTRIBUTES
FOR
EXAMPLE
BEARS
ARE
FURRIER
THAN
GIRAFFES
WE
FURTHER
SHOW
HOW
THE
PROPOSED
RELATIVE
ATTRIBUTES
ENABLE
RICHER
TEXTUAL
DESCRIPTIONS
FOR
NEW
IMAGES
WHICH
IN
PRACTICE
ARE
MORE
PRECISE
FOR
HUMAN
INTERPRETA
TION
WE
DEMONSTRATE
THE
APPROACH
ON
DATASETS
OF
FACES
AND
NATURAL
SCENES
AND
SHOW
ITS
CLEAR
ADVANTAGES
OVER
TRADI
TIONAL
BINARY
ATTRIBUTE
PREDICTION
FOR
THESE
NEW
TASKS
INTRODUCTION
WHILE
TRADITIONAL
VISUAL
RECOGNITION
APPROACHES
MAP
LOW
LEVEL
IMAGE
FEATURES
DIRECTLY
TO
OBJECT
CATEGORY
LABELS
RECENT
WORK
PROPOSES
MODELS
USING
VISUAL
ATTRIBUTES
ATTRIBUTES
ARE
PROPERTIES
OBSERVABLE
IN
IMAGES
THAT
HAVE
HUMAN
DESIGNATED
NAMES
E
G
STRIPED
FOUR
LEGGED
AND
THEY
ARE
VALUABLE
AS
A
NEW
SEMANTIC
CUE
IN
VARIOUS
PROBLEMS
FOR
EXAMPLE
RESEARCHERS
HAVE
SHOWN
THEIR
IM
PACT
FOR
STRENGTHENING
FACIAL
VERIFICATION
OBJECT
RECOG
NITION
GENERATING
DESCRIPTIONS
OF
UNFAMILIAR
OB
JECTS
AND
TO
FACILITATE
ZERO
SHOT
TRANSFER
LEARNING
WHERE
ONE
TRAINS
A
CLASSIFIER
FOR
AN
UNSEEN
OBJECT
SIMPLY
BY
SPECIFYING
WHICH
ATTRIBUTES
IT
HAS
PROBLEM
MOST
EXISTING
WORK
FOCUSES
WHOLLY
ON
AT
TRIBUTES
AS
BINARY
PREDICATES
INDICATING
THE
PRESENCE
OR
AB
SENCE
OF
A
CERTAIN
PROPERTY
IN
AN
IMAGE
THIS
MAY
SUFFICE
FOR
PART
BASED
ATTRIBUTES
E
G
HAS
A
HEAD
AND
SOME
A
SMILING
B
C
NOT
SMILING
D
NATURAL
E
F
MANMADE
FIGURE
BINARY
ATTRIBUTES
ARE
AN
ARTIFICIALLY
RESTRICTIVE
WAY
TO
DESCRIBE
IMAGES
WHILE
IT
IS
CLEAR
THAT
A
IS
SMILING
AND
C
IS
NOT
THE
MORE
IN
FORMATIVE
AND
INTUITIVE
DESCRIPTION
FOR
B
IS
VIA
RELATIVE
ATTRIBUTES
HE
IS
SMILING
MORE
THAN
A
BUT
LESS
THAN
C
SIMILARLY
SCENE
E
IS
LESS
NATURAL
THAN
D
BUT
MORE
SO
THAN
F
OUR
MAIN
IDEA
IS
TO
MODEL
RELATIVE
ATTRIBUTES
VIA
LEARNED
RANKING
FUNCTIONS
AND
THEN
DEMONSTRATE
THEIR
IMPACT
ON
NOVEL
FORMS
OF
ZERO
SHOT
LEARNING
AND
GENERATING
IMAGE
DESCRIPTIONS
BINARY
PROPERTIES
E
G
SPOTTED
HOWEVER
FOR
A
LARGE
VA
RIETY
OF
ATTRIBUTES
NOT
ONLY
IS
THIS
BINARY
SETTING
RESTRICTIVE
BUT
IT
IS
ALSO
UNNATURAL
FOR
INSTANCE
IT
IS
NOT
CLEAR
IF
IN
FIG
URE
B
HUGH
LAURIE
IS
SMILING
OR
NOT
DIFFERENT
PEOPLE
ARE
LIKELY
TO
RESPOND
INCONSISTENTLY
IN
PROVIDING
THE
PRESENCE
OR
ABSENCE
OF
THE
SMILING
ATTRIBUTE
FOR
THIS
IMAGE
OR
OF
THE
NATURAL
ATTRIBUTE
FOR
FIGURE
E
INDEED
WE
OBSERVE
THAT
RELATIVE
VISUAL
PROPERTIES
ARE
A
SEMANTICALLY
RICH
WAY
BY
WHICH
HUMANS
DESCRIBE
AND
COM
PARE
OBJECTS
IN
THE
WORLD
THEY
ARE
NECESSARY
FOR
INSTANCE
TO
REFINE
AN
IDENTIFYING
DESCRIPTION
THE
ROUNDER
PILLOW
THE
SAME
EXCEPT
BLUER
OR
TO
SITUATE
WITH
RESPECT
TO
REF
ERENCE
OBJECTS
BRIGHTER
THAN
A
CANDLE
DIMMER
THAN
A
FLASHLIGHT
FURTHERMORE
THEY
HAVE
POTENTIAL
TO
ENHANCE
ACTIVE
AND
INTERACTIVE
LEARNING
FOR
INSTANCE
OFFERING
A
BET
TER
GUIDE
FOR
A
VISUAL
SEARCH
FIND
ME
SIMILAR
SHOES
BUT
SHINIER
OR
REFINE
THE
RETRIEVED
IMAGES
OF
DOWNTOWN
CHICAGO
TO
THOSE
TAKEN
ON
SUNNIER
DAYS
PROPOSAL
IN
THIS
WORK
WE
PROPOSE
TO
MODEL
RELATIVE
AT
TRIBUTES
AS
OPPOSED
TO
PREDICTING
THE
PRESENCE
OF
AN
AT
TRIBUTE
A
RELATIVE
ATTRIBUTE
INDICATES
THE
STRENGTH
OF
AN
AT
TRIBUTE
IN
AN
IMAGE
WITH
RESPECT
TO
OTHER
IMAGES
FOR
EXAM
PLE
IN
FIGURE
WHILE
IT
IS
DIFFICULT
TO
ASSIGN
A
MEANINGFUL
VALUE
TO
THE
BINARY
ATTRIBUTE
SMILING
WE
COULD
ALL
AGREE
ON
THE
RELATIVE
ATTRIBUTE
I
E
HUGH
LAURIE
IS
SMILING
LESS
THAN
SCARLETT
JOHANSSON
BUT
MORE
THAN
JARED
LETO
IN
ADDITION
TO
BEING
MORE
NATURAL
RELATIVE
ATTRIBUTES
WOULD
OFFER
A
RICHER
MODE
OF
COMMUNICATION
THUS
ALLOWING
ACCESS
TO
MORE
DE
TAILED
HUMAN
SUPERVISION
AND
SO
POTENTIALLY
HIGHER
RECOG
NITION
ACCURACY
AS
WELL
AS
THE
ABILITY
TO
GENERATE
MORE
IN
FORMATIVE
DESCRIPTIONS
OF
NOVEL
IMAGES
HOW
CAN
WE
LEARN
RELATIVE
PROPERTIES
WHEREAS
TRADI
TIONAL
SUPERVISED
CLASSIFICATION
IS
APPROPRIATE
TO
LEARN
AT
TRIBUTES
THAT
ARE
INTRINSICALLY
BINARY
IT
FALLS
SHORT
WHEN
WE
WANT
TO
REPRESENT
VISUAL
PROPERTIES
THAT
ARE
NAMEABLE
BUT
NOT
CATEGORICAL
OUR
GOAL
IS
INSTEAD
TO
ESTIMATE
THE
DEGREE
OF
THAT
ATTRIBUTE
PRESENCE
WHICH
IMPORTANTLY
DIFFERS
FROM
THE
PROBABILITY
OF
A
BINARY
CLASSIFIER
PREDICTION
TO
THIS
END
WE
DEVISE
AN
APPROACH
THAT
LEARNS
A
RANKING
FUNCTION
FOR
EACH
ATTRIBUTE
GIVEN
RELATIVE
SIMILARITY
CONSTRAINTS
ON
PAIRS
OF
EXAMPLES
OR
MORE
GENERALLY
A
PARTIAL
ORDERING
ON
SOME
EXAMPLES
THE
LEARNED
RANKING
FUNCTION
CAN
ESTI
MATE
A
REAL
VALUED
FOR
IMAGES
INDICATING
THE
RELATIVE
STRENGTH
OF
THE
ATTRIBUTE
PRESENCE
IN
THEM
THEN
WE
INTRO
DUCE
NOVEL
FORMS
OF
ZERO
SHOT
LEARNING
AND
DESCRIPTION
THAT
EXPLOIT
THE
RELATIVE
ATTRIBUTE
PREDICTIONS
THE
PROPOSED
RANKING
APPROACH
ACCOUNTS
FOR
A
SUBTLE
BUT
IMPORTANT
DIFFERENCE
BETWEEN
RELATIVE
ATTRIBUTES
AND
CON
CEIVABLE
ALTERNATIVES
BASED
ON
REGRESSION
OR
MULTI
WAY
CLAS
SIFICATION
WHILE
SUCH
ALTERNATIVES
COULD
ALSO
ALLOW
FOR
A
RICHER
VOCABULARY
DURING
TRAINING
THEY
COULD
SUFFER
FROM
SIMILAR
INCONSISTENCIES
AS
BINARY
ATTRIBUTES
FOR
EXAMPLE
IT
IS
MORE
DIFFICULT
TO
DEFINE
AND
PERHAPS
MORE
IMPORTANTLY
AGREE
ON
WITH
WHAT
STRENGTH
IS
HE
SMILING
THAN
IS
HE
SMILING
MORE
THAN
SHE
IS
THUS
WE
EXPECT
THE
RELATIVE
MODE
OF
SUPERVISION
OUR
APPROACH
PERMITS
TO
BE
MORE
NATU
RAL
AND
CONSISTENT
FOR
HUMAN
LABELERS
CONTRIBUTIONS
OUR
MAIN
CONTRIBUTION
IS
THE
IDEA
TO
LEARN
RELATIVE
VISUAL
ATTRIBUTES
WHICH
TO
OUR
KNOWLEDGE
HAS
NOT
BEEN
EXPLORED
IN
ANY
PRIOR
WORK
OUR
OTHER
CONTRIBUTION
IS
TO
DEVISE
AND
DEMONSTRATE
TWO
NEW
TASKS
WELL
SERVED
BY
RELATIVE
ATTRIBUTES
ZERO
SHOT
LEARNING
FROM
RELATIVE
COM
PARISONS
AND
IMAGE
DESCRIPTION
IN
REFERENCE
TO
EXAMPLE
IMAGES
OR
CATEGORIES
WE
DEMONSTRATE
THE
APPROACH
FOR
BOTH
TASKS
USING
THE
OUTDOOR
SCENES
DATASET
AND
A
SUBSET
OF
THE
PUBLIC
FIGURE
FACE
DATABASE
WE
FIND
THAT
RELA
TIVE
ATTRIBUTES
YIELD
SIGNIFICANTLY
BETTER
ZERO
SHOT
LEARNING
ACCURACY
WHEN
COMPARED
TO
THEIR
BINARY
COUNTERPARTS
IN
ADDITION
WE
CONDUCT
HUMAN
SUBJECT
STUDIES
TO
EVALUATE
THE
INFORMATIVENESS
OF
THE
AUTOMATICALLY
GENERATED
IMAGE
DE
SCRIPTIONS
AND
FIND
THAT
RELATIVE
ATTRIBUTES
ARE
CLEARLY
MORE
POWERFUL
THAN
EXISTING
BINARY
ATTRIBUTES
IN
UNIQUELY
IDENTI
FYING
AN
IMAGE
THIS
PAPER
WE
REFER
TO
RANK
AS
A
REAL
VALUED
SCORE
RELATED
WORK
WE
REVIEW
RELATED
WORK
ON
VISUAL
ATTRIBUTES
OTHER
USES
OF
RELATIVE
CUES
AND
METHODS
FOR
LEARNING
COMPARISONS
BINARY
ATTRIBUTES
LEARNING
ATTRIBUTE
CATEGORIES
ALLOWS
PREDICTION
OF
COLOR
OR
TEXTURE
TYPES
AND
CAN
ALSO
PRO
VIDE
A
MID
LEVEL
CUE
FOR
OBJECT
OR
FACE
RECOGNITION
BEYOND
OBJECT
RECOGNITION
THE
SEMANTICS
INTRINSIC
TO
AT
TRIBUTES
ENABLE
ZERO
SHOT
TRANSFER
OR
DESCRIP
TION
AND
PART
LOCALIZATION
RATHER
THAN
MANUALLY
DEFINE
ATTRIBUTE
VOCABULARIES
SOME
WORK
AIMS
TO
DISCOVER
ATTRIBUTE
RELATED
CONCEPTS
ON
THE
WEB
EXTRACT
THEM
FROM
EXISTING
KNOWLEDGE
SOURCES
OR
DISCOVER
THEM
INTERACTIVELY
IN
CONTRAST
TO
OUR
APPROACH
ALL
SUCH
METH
ODS
RESTRICT
THE
ATTRIBUTES
TO
BE
CATEGORICAL
AND
IN
FACT
BI
NARY
RELATIVE
INFORMATION
RELATIVE
INFORMATION
HAS
BEEN
EX
PLORED
IN
VISION
IN
A
VARIETY
OF
WAYS
RECENT
WORK
ON
LARGE
SCALE
RECOGNITION
EXPLOITS
WORDNET
BASED
INFORMATION
TO
SPECIFY
A
SEMANTIC
DISTANCE
SENSITIVE
CLASSIFIER
OR
TO
MAKE
DO
WITH
FEW
LABELS
BY
SHARING
TRAINING
IMAGES
AMONG
SEMANTICALLY
SIMILAR
CLASSES
STEMMING
FROM
A
RELATED
MOTIVATION
OF
LIMITED
LABELED
DATA
WANG
ET
AL
MAKE
USE
OF
EXPLICIT
SIMILARITY
BASED
SUPERVISION
SUCH
AS
A
SER
VAL
IS
LIKE
A
LEOPARD
OR
A
ZEBRA
IS
SIMILAR
TO
THE
CROSS
WALK
IN
TEXTURE
TO
SHARE
TRAINING
INSTANCES
FOR
CATEGORIES
WITH
LIMITED
OR
NO
TRAINING
INSTANCES
UNLIKE
OUR
APPROACH
THAT
METHOD
LEARNS
A
MODEL
FOR
EACH
OBJECT
CATEGORY
AND
DOES
NOT
MODEL
ATTRIBUTES
IN
CONTRAST
OUR
ATTRIBUTE
MODELS
ARE
CATEGORY
INDEPENDENT
AND
TRANSFERRABLE
ENABLING
RELA
TIVE
DESCRIPTIONS
BETWEEN
ALL
CLASSES
MOREOVER
WHEREAS
THAT
TECHNIQUE
CAPTURES
SIMILARITY
AMONG
OBJECT
CATEGORIES
OURS
MODELS
A
GENERAL
ORDERING
OF
THE
IMAGES
SORTED
BY
THE
STRENGTH
OF
THEIR
ATTRIBUTES
AS
WELL
AS
A
JOINT
SPACE
OVER
MUL
TIPLE
SUCH
RELATIVE
ATTRIBUTES
KUMAR
ET
AL
EXPLORE
COMPARATIVE
FACIAL
ATTRIBUTES
SUCH
AS
LIPS
LIKE
BARACK
OBAMA
FOR
FACE
VERIFICATION
THESE
ATTRIBUTES
ALTHOUGH
COMPARATIVE
ARE
ALSO
MODELED
AS
BINARY
CLASSIFIERS
AND
ARE
SIMILARITY
BASED
AS
OPPOSED
TO
AN
ORDERING
GUPTA
ET
AL
AND
SIDDIQUIE
ET
AL
USE
PREPOSITIONS
AND
ADJECTIVES
TO
RELATE
OBJECTS
TO
EACH
OTHER
FOR
MORE
EFFECTIVE
CONTEXTUAL
MODELING
AND
ACTIVE
LEARNING
RE
SPECTIVELY
IN
CONTRAST
OUR
WORK
INVOLVES
RELATIVE
MODELING
OF
ATTRIBUTE
STRENGTHS
FOR
A
RICHER
VOCABULARY
THAT
ENHANCES
SUPERVISION
AND
DESCRIPTION
OF
IMAGES
LEARNING
TO
RANK
LEARNING
TO
RANK
HAS
RECEIVED
EXTEN
SIVE
ATTENTION
IN
THE
MACHINE
LEARNING
LITERATURE
FOR
INFORMATION
RETRIEVAL
IN
GENERAL
AND
IMAGE
RETRIEVAL
IN
PARTICULAR
GIVEN
A
QUERY
IMAGE
USER
PREFERENCES
OFTEN
CAPTURED
VIA
CLICK
DATA
ARE
INCORPORATED
TO
LEARN
A
RANKING
FUNCTION
WITH
THE
GOAL
OF
RETRIEVING
MORE
RELEVANT
IMAGES
IN
THE
TOP
SEARCH
RESULTS
LEARNED
DISTANCE
METRICS
E
G
CAN
INDUCE
A
RANKING
ON
IMAGES
HOWEVER
THIS
RANKING
IS
ALSO
SPECIFIC
TO
A
QUERY
IMAGE
AND
TYPICALLY
INTENDED
FOR
NEAREST
NEIGHBOR
BASED
CLASSIFIERS
OUR
WORK
LEARNS
A
RANKING
FUNCTION
ON
IMAGES
BASED
ON
CONSTRAINTS
SPECIFYING
THE
RELATIVE
STRENGTH
OF
ATTRIBUTES
AND
THE
RESULT
ING
FUNCTION
IS
NOT
RELATIVE
TO
ANY
OTHER
IMAGE
IN
THE
DATASET
THUS
UNLIKE
QUERY
CENTRIC
RETRIEVAL
TASKS
WE
CAN
CHARAC
TERIZE
INDIVIDUAL
IMAGES
BY
THE
STRENGTH
OF
THE
ATTRIBUTES
PRESENT
WHICH
WE
SHOW
IS
VALUABLE
FOR
NEW
RECOGNITION
AND
DESCRIPTION
APPLICATIONS
APPROACH
WE
FIRST
PRESENT
OUR
APPROACH
FOR
LEARNING
RELATIVE
AT
TRIBUTES
SECTION
AND
THEN
EXPLAIN
HOW
WE
CAN
USE
RELA
TIVE
ATTRIBUTES
FOR
ENHANCED
ZERO
SHOT
LEARNING
SECTION
AND
IMAGE
DESCRIPTION
GENERATION
SECTION
LEARNING
RELATIVE
ATTRIBUTES
WE
ARE
GIVEN
A
SET
OF
TRAINING
IMAGES
I
I
REPRE
FIGURE
DISTINCTION
BETWEEN
LEARNING
A
WIDE
MARGIN
RANKING
FUNCTION
RIGHT
THAT
ENFORCES
THE
DESIRED
ORDERING
ON
TRAINING
POINTS
AND
A
WIDE
MARGIN
BINARY
CLASSIFIER
LEFT
THAT
ONLY
SEPARATES
THE
TWO
CLASSES
AND
AND
DOES
NOT
NECESSARILY
PRESERVE
A
DESIRED
ORDERING
ON
THE
POINTS
REARRANGING
THE
CONSTRAINTS
REVEALS
THAT
THE
ABOVE
FORMU
LATION
WITHOUT
THE
SIMILARITY
CONSTRAINTS
IN
EQN
IS
QUITE
SIMILAR
TO
THE
SVM
CLASSIFICATION
PROBLEM
BUT
ON
PAIRWISE
DIFFERENCE
VECTORS
MINIMIZE
WT
C
TRIBUTES
A
AM
IN
ADDITION
FOR
EACH
ATTRIBUTE
AM
WE
T
WT
XI
XJ
ΞIJ
I
J
OM
ARE
GIVEN
A
SET
OF
ORDERED
PAIRS
OF
IMAGES
OM
I
J
WT
X
X
Γ
I
J
AND
A
SET
OF
UN
ORDERED
PAIRS
SM
I
J
SUCH
THAT
M
I
J
IJ
M
I
J
OM
I
J
I
E
IMAGE
I
HAS
A
STRONGER
PRES
ENCE
OF
ATTRIBUTE
AM
THAN
J
AND
I
J
SM
I
J
I
E
I
AND
J
HAVE
SIMILAR
RELATIVE
STRENGTHS
OF
AM
WE
NOTE
THAT
OM
AND
SM
CAN
BE
DEDUCED
FROM
ANY
PARTIAL
ORDERING
OF
THE
IMAGES
I
IN
THE
TRAINING
DATA
WITH
RESPECT
TO
STRENGTH
OF
AM
EITHER
OM
OR
SM
BUT
NOT
BOTH
CAN
BE
EMPTY
OUR
GOAL
IS
TO
LEARN
M
RANKING
FUNCTIONS
RM
XI
WT
XI
FOR
M
M
SUCH
THAT
THE
MAXIMUM
NUMBER
OF
THE
FOLLOWING
CONSTRAINTS
IS
SATISFIED
I
J
OM
WT
XI
WT
XJ
I
J
SM
WT
XI
WT
XJ
ΞIJ
ΓIJ
WHERE
C
IS
THE
TRADE
OFF
CONSTANT
BETWEEN
MAXIMIZ
ING
THE
MARGIN
AND
SATISFYING
THE
PAIRWISE
RELATIVE
CON
STRAINTS
WE
SOLVE
THE
ABOVE
PRIMAL
PROBLEM
USING
NEWTON
METHOD
WHILE
WE
USE
A
LINEAR
RANKING
FUNCTION
IN
OUR
EXPERIMENTS
THE
ABOVE
FORMULATION
CAN
BE
EASILY
EXTENDED
TO
KERNELS
WE
NOTE
THAT
THIS
LEARNING
TO
RANK
FORMULATION
LEARNS
A
FUNCTION
THAT
EXPLICITLY
ENFORCES
A
DESIRED
ORDERING
ON
THE
TRAINING
IMAGES
THE
MARGIN
IS
THE
DISTANCE
BETWEEN
THE
CLOS
EST
TWO
PROJECTIONS
WITHIN
ALL
DESIRED
TRAINING
RANKINGS
IN
CONTRAST
IF
ONE
WERE
TO
TRAIN
A
BINARY
CLASSIFIER
ONLY
THE
MARGIN
BETWEEN
THE
NEAREST
BINARY
LABELED
EXAMPLES
IS
EN
FORCED
ORDERING
AMONG
EXAMPLES
BEYOND
THOSE
DEFINING
THE
M
M
MARGIN
IS
ARBITRARY
SEE
FIGURE
OUR
EXPERIMENTS
CONFIRM
WHILE
THIS
IS
AN
NP
HARD
PROBLEM
IT
IS
POSSIBLE
TO
APPROXIMATE
THE
SOLUTION
WITH
THE
INTRODUCTION
OF
NON
NEGATIVE
SLACK
VARIABLES
SIMILAR
TO
SVM
CLASSIFICATION
WE
DIRECTLY
ADAPT
THE
FORMULATION
PROPOSED
IN
WHICH
WAS
ORIGINALLY
APPLIED
TO
WEB
PAGE
RANKING
EXCEPT
WE
USE
A
QUADRATIC
LOSS
FUNCTION
TOGETHER
WITH
SIMILARITY
CONSTRAINTS
LEADING
TO
THE
FOLLOWING
OPTIMIZATION
PROBLEM
MINIMIZE
WT
C
THIS
DISTINCTION
DOES
INDEED
MATTER
IN
PRACTICE
AS
OUR
LEARNT
RANKING
FUNCTION
IS
MORE
EFFECTIVE
AT
CAPTURING
THE
RELATIVE
STRENGTHS
OF
THE
ATTRIBUTES
THAN
THE
SCORE
OF
A
BINARY
CLASSI
FIER
I
E
THE
MAGNITUDE
OF
THE
SVM
DECISION
FUNCTION
IN
ADDITION
TRAINING
WITH
COMPARISONS
IMAGE
I
IS
SIMI
LAR
TO
J
IN
TERMS
OF
ATTRIBUTE
AM
OR
I
EXHIBITS
AM
LESS
THAN
IS
WELL
SUITED
TO
THE
TASK
AT
HAND
ATTRIBUTE
STRENGTHS
ARE
ARGUABLY
MORE
NATURAL
TO
EXPRESS
IN
RELATIVE
TERMS
AS
OP
POSED
TO
REQUIRING
ABSOLUTE
JUDGMENTS
IN
ISOLATION
I
E
I
REPRESENTS
AM
WITH
DEGREE
T
WT
XI
WT
XJ
ΞIJ
I
J
OM
TINGS
ZERO
SHOT
LEARNING
WITH
RELATIVE
RELATIONSHIPS
AND
M
M
T
T
GENERATING
IMAGE
DESCRIPTIONS
WE
NOW
INTRODUCE
OUR
WMXI
WMXJ
ΓIJ
I
J
SM
ΞIJ
ΓIJ
APPROACH
TO
INCORPORATE
RELATIVE
ATTRIBUTES
FOR
EACH
OF
THESE
APPLICATIONS
IN
TURN
ZERO
SHOT
LEARNING
FROM
RELATIONSHIPS
AGES
FROM
CLASS
C
SO
WE
HAVE
C
N
Μ
Σ
FOR
CONSIDER
N
CATEGORIES
OF
INTEREST
FOR
EXAMPLE
EACH
I
I
I
I
I
CATEGORY
MAY
BE
AN
OBJECT
CLASS
OR
A
TYPE
OF
SCENE
DUR
ING
TRAINING
OF
THESE
CATEGORIES
ARE
SEEN
CATEGORIES
FOR
WHICH
TRAINING
IMAGES
ARE
PROVIDED
WHILE
THE
REMAINING
U
N
CATEGORIES
ARE
UNSEEN
FOR
WHICH
NO
TRAINING
IMAGES
ARE
PROVIDED
THE
PARAMETERS
OF
THE
GENERATIVE
MODEL
CORRESPONDING
TO
EACH
OF
THE
U
UNSEEN
CATEGORIES
ARE
SELECTED
UNDER
THE
GUIDANCE
OF
THE
INPUT
RELATIVE
DESCRIPTIONS
IN
PARTICULAR
GIVEN
AN
UNSEEN
CATEGORY
C
U
WE
EMPLOY
THE
FOLLOWING
IF
C
U
IS
DESCRIBED
AS
C
C
U
C
WHERE
C
RESPECT
TO
EACH
OTHER
BE
IT
PAIRWISE
RELATIONSHIPS
OR
PARTIAL
ORDERS
FOR
EXAMPLE
BEARS
ARE
FURRIER
THAN
GIRAFFES
BUT
LESS
AND
C
ARE
SEEN
CATEGORIES
THEN
WE
SET
THE
M
TH
COM
PONENT
OF
THE
MEAN
Μ
U
TO
Μ
Μ
JM
IM
KM
FURRY
THAN
RABBITS
LIONS
ARE
LARGER
THAN
DOGS
AS
LARGE
AS
IF
C
U
IS
DESCRIBED
AS
C
C
WE
SET
Μ
TO
TIGERS
BUT
LESS
LARGE
THAN
ELEPHANTS
ETC
WE
NOTE
THAT
ALL
J
I
J
JM
PAIRS
OF
CATEGORIES
NEED
NOT
BE
RELATED
IN
THE
SUPERVISION
AND
DIFFERENT
SUBSETS
OF
CATEGORIES
CAN
BE
RELATED
FOR
THE
DIF
FERENT
ATTRIBUTES
THE
U
UNSEEN
CATEGORIES
ON
THE
OTHER
HAND
ARE
DE
SCRIBED
RELATIVE
TO
ONE
OR
TWO
SEEN
CATEGORIES
FOR
A
SUBSET
U
Μ
DM
WHERE
DM
IS
THE
AVERAGE
DISTANCE
BETWEEN
THE
SORTED
MEAN
RANKING
SCORES
Μ
OF
SEEN
CLASSES
FOR
ATTRIBUTE
AM
IT
IS
REASONABLE
TO
EXPECT
THE
UNSEEN
CLASS
TO
BE
AS
FAR
FROM
THE
SPECIFIED
SEEN
CLASS
AS
OTHER
SEEN
CLASSES
TEND
TO
BE
FROM
EACH
OTHER
SIMILARLY
IF
C
U
IS
DESCRIBED
AS
C
U
C
WE
SET
U
U
Μ
U
TO
Μ
DM
AS
CI
CJ
CK
FOR
ATTRIBUTE
AM
OR
CI
CJ
OR
JM
IM
C
U
C
WHERE
C
AND
C
ARE
SEEN
CATEGORIES
WE
IF
AM
IS
NOT
USED
TO
DESCRIBE
C
U
WE
SET
Μ
U
TO
BE
THE
NOTE
THE
SIMPLE
AND
FLEXIBLE
SUPERVISION
REQUIRED
FOR
THE
CAT
EGORIES
ESPECIALLY
THE
UNSEEN
ONES
FOR
ANY
ATTRIBUTE
NOT
MEAN
ACROSS
ALL
TRAINING
IMAGE
RANKS
FOR
AM
AND
THE
M
TH
DIAGONAL
ENTRY
OF
Σ
U
TO
BE
THE
VARIANCE
OF
THE
NECESSARILY
ALL
THE
USER
CAN
SELECT
ANY
SEEN
CATEGORY
DE
PICTING
A
STRONGER
AND
OR
WEAKER
PRESENCE
OF
THE
ATTRIBUTE
SAME
IN
THE
FIRST
THREE
CASES
WE
SIMPLY
SET
Σ
U
Σ
LIST
BASED
LEARNING
TO
RANK
TECHNIQUES
ARE
AVAILABLE
WE
CHOOSE
THE
PAIRWISE
LEARNING
TECHNIQUE
AS
DESCRIBED
IN
SEC
TION
TO
ENSURE
THIS
EASE
OF
SUPERVISION
DURING
TESTING
A
NOVEL
IMAGE
IS
TO
BE
CLASSIFIED
INTO
ANY
OF
THE
N
CATEGORIES
OUR
ZERO
SHOT
LEARNING
SETTING
IS
MORE
GIVEN
A
TEST
IMAGE
I
WE
COMPUTE
X
I
RM
INDICATING
THE
RELATIVE
ATTRIBUTE
RANKING
SCORES
FOR
THE
IMAGE
IT
IS
THEN
ASSIGNED
TO
THE
SEEN
OR
UNSEEN
CATEGORY
THAT
ASSIGNS
IT
THE
HIGHEST
LIKELIHOOD
GENERAL
THAN
THE
MODEL
PROPOSED
BY
LAMPERT
ET
AL
IN
THAT
THE
SUPERVISOR
MAY
NOT
ONLY
ASSOCIATE
ATTRIBUTES
WITH
CATEGORIES
BUT
ALSO
EXPRESS
HOW
THE
CATEGORIES
RELATE
ALONG
C
ARGMAX
J
N
P
X
I
ΜJ
ΣJ
ANY
NUMBER
OF
THE
ATTRIBUTES
WE
EXPECT
THIS
RICHER
REPRE
SENTATION
TO
ALLOW
BETTER
DIVISIONS
BETWEEN
BOTH
THE
UNSEEN
AND
SEEN
CATEGORIES
AS
WE
DEMONSTRATE
IN
THE
EXPERIMENTS
WE
PROPAGATE
THE
CATEGORY
RELATIONSHIPS
PROVIDED
DURING
TRAINING
TO
THE
CORRESPONDING
IMAGES
I
E
FOR
SEEN
CLASSES
FROM
A
BAYESIAN
PERSPECTIVE
OUR
APPROACH
TO
SETTING
THE
PARAMETERS
OF
THE
UNSEEN
CATEGORIES
GENERATIVE
MODELS
CAN
BE
CONSIDERED
TO
BE
PRIORS
TRANSFERRED
FROM
THE
KNOWLEDGE
OF
THE
MODELS
FOR
THE
SEEN
CATEGORIES
UNDER
REASONABLE
PRI
ORS
THE
CHOICE
OF
MEAN
AND
COVARIANCES
CORRESPOND
TO
THE
C
AND
C
C
C
I
J
I
C
J
C
MINIMUM
MEAN
SQUARED
ERROR
AND
MAXIMUM
LIKELIHOOD
ES
FOR
ATTRIBUTE
AM
WE
THEN
LEARN
ALL
M
RELATIVE
ATTRIBUTES
AS
DESCRIBED
IN
SECTION
PREDICTING
THE
REAL
VALUED
RANK
OF
ALL
IMAGES
IN
THE
TRAINING
DATASET
I
ALLOWS
US
TO
TRANSFORM
XI
RN
X
I
RM
SUCH
THAT
EACH
IMAGE
I
IS
NOW
REPRESENTED
AS
AN
M
DIMENSIONAL
VECTOR
X
I
INDICATING
ITS
RANK
SCORE
FOR
ALL
M
ATTRIBUTES
WE
NOW
BUILD
A
GENERATIVE
MODEL
FOR
EACH
OF
THE
SEEN
CATEGORIES
IN
RM
WE
USE
A
GAUSSIAN
DISTRIBUTION
AND
ESTIMATE
THE
MEAN
Μ
RM
AND
M
M
COVARI
ANCE
MATRIX
Σ
FROM
THE
RANKING
SCORES
OF
THE
TRAINING
IM
GENERALIZES
NATURALLY
TO
ALLOW
STRONGER
SUPERVISION
PER
IMAGE
IN
STANCE
WHEN
AVAILABLE
TIMATES
RELATED
FORMULATIONS
OF
TRANSFER
THROUGH
PARAMETER
SHARING
HAVE
BEEN
STUDIED
BY
FEI
FEI
ET
AL
AND
STARK
ET
AL
FOR
LEARNING
SHAPE
BASED
OBJECT
MODELS
WITH
FEW
TRAINING
IMAGES
THOUGH
NO
PRIOR
MODELS
CONSIDER
TRANSFER
RING
KNOWLEDGE
BASED
ON
RELATIVE
COMPARISONS
AS
WE
DO
HERE
WE
NOTE
THAT
IF
ONE
OR
MORE
IMAGES
FROM
THE
UNSEEN
CATEGORIES
WERE
SUBSEQUENTLY
TO
BECOME
AVAILABLE
OUR
ESTI
MATED
PARAMETERS
COULD
EASILY
BE
UPDATED
IN
LIGHT
OF
THE
AD
DITIONAL
EVIDENCE
FURTHERMORE
OUR
GENERAL
APPROACH
COULD
POTENTIALLY
SUPPORT
MORE
SPECIFIC
SUPERVISION
ABOUT
THE
RELA
TIVE
RELATIONSHIPS
SHOULD
IT
BE
AVAILABLE
E
G
BEARS
UNSEEN
ARE
SIGNIFICANTLY
MORE
FURRY
THAN
COWS
SEEN
DESCRIBING
IMAGES
IN
RELATIVE
TERMS
THE
SECOND
APPLICATION
OF
RELATIVE
ATTRIBUTES
THAT
WE
PRO
POSE
IS
THAT
OF
DESCRIBING
NOVEL
IMAGES
THE
GOAL
IS
TO
BE
ABLE
TO
RELATE
ANY
NEW
EXAMPLE
TO
OTHER
IMAGES
ACCORDING
TO
DIFFERENT
PROPERTIES
WHETHER
ITS
CLASS
HAPPENS
TO
BE
FA
MILIAR
OR
NOT
THIS
BASIC
FUNCTIONALITY
WOULD
ALLOW
FOR
IN
STANCE
THE
MEANINGFUL
SEARCH
EXAMPLE
APPLICATIONS
GIVEN
IN
THE
INTRODUCTION
SEE
RECENT
WORK
IN
FOR
OTHER
FORMS
OF
IMAGE
DESCRIPTION
BASED
ON
OBJECT
ACTION
SCENE
TAGS
DURING
TRAINING
WE
ARE
GIVEN
A
SET
OF
TRAINING
IMAGES
I
I
EACH
REPRESENTED
BY
A
FEATURE
VECTOR
XI
RN
A
LIST
A
AM
OF
M
ATTRIBUTES
ALONG
WITH
OM
I
J
T
I
J
AND
SM
I
J
T
I
J
IN
RELATIVE
STRENGTH
OF
AM
WE
LEARN
M
RANKING
FUNCTIONS
AS
DESCRIBED
IN
SEC
TION
AND
EVALUATE
THEM
ON
ALL
TRAINING
IMAGES
IN
I
GIVEN
A
NOVEL
IMAGE
J
TO
BE
DESCRIBED
WE
EVALUATE
ALL
LEARNT
RANKING
FUNCTIONS
RM
XJ
FOR
EACH
ATTRIBUTE
AM
WE
IDENTIFY
TWO
REFERENCE
IMAGES
I
AND
K
FROM
I
THAT
WILL
BE
USED
TO
DESCRIBE
J
VIA
RELATIVE
ATTRIBUTES
IN
PRINCIPLE
WITH
A
GOOD
RANKING
FUNCTION
ANY
REFERENCE
IMAGES
COULD
BE
INFOR
MATIVE
IN
OUR
IMPLEMENTATION
WE
ADHERE
TO
THE
FOLLOWING
GUIDELINES
TO
AVOID
GENERATING
AN
OVERLY
PRECISE
DESCRIP
TION
WE
WISH
TO
SELECT
I
AND
K
SUCH
THAT
THEY
ARE
NOT
VERY
SIMILAR
TO
J
IN
TERMS
OF
ATTRIBUTE
STRENGTH
HOWEVER
TO
AVOID
TRIVIAL
DESCRIPTIONS
THEY
MUST
NOT
BE
TOO
FAR
FROM
J
EITHER
HENCE
WE
PICK
I
AND
K
SUCH
THAT
I
J
AND
J
K
IN
STRENGTH
OF
A
AND
TH
OF
THE
IMAGES
IN
I
LIE
BETWEEN
I
AND
J
AS
WELL
AS
BETWEEN
J
AND
K
IN
THE
CASE
OF
BOUNDARY
CONDITIONS
WHERE
NO
SUCH
I
OR
K
EXIST
I
IS
CHOSEN
TO
BE
THE
IMAGE
IN
I
WITH
THE
LEAST
STRENGTH
OF
AM
AND
K
IS
SET
TO
THE
IMAGE
IN
I
WITH
THE
HIGHEST
STRENGTH
OF
AM
THE
IMAGE
J
CAN
THEN
BE
DESCRIBED
IN
TERMS
OF
ALL
OR
A
SUBSET
OF
THE
M
ATTRIBUTES
RELATIVE
TO
ANY
IDENTIFIED
PAIRS
I
K
FIGURE
SHOWS
AN
EXAMPLE
DESCRIPTION
GENERATED
BY
OUR
APPROACH
AS
WELL
AS
AN
ILLUSTRATION
OF
SELECTED
PAIRS
I
K
WHILE
MORE
ELABORATE
ANALYSIS
OF
THE
DATASET
DISTRIBUTION
AND
EVEN
PSYCHOPHYSICS
KNOWLEDGE
OF
THE
SENSITIVITY
OF
HUMANS
TO
CHANGE
IN
DIFFERENT
ATTRIBUTES
COULD
MAKE
THE
SELECTION
OF
REFERENCE
IMAGES
MORE
EFFECTIVE
WE
EMPLOY
THIS
STRAIGHTFORWARD
TECHNIQUE
AS
A
PROOF
OF
CONCEPT
AND
LEAVE
SUCH
ANALYSIS
FOR
FUTURE
WORK
EXPERIMENTS
WE
EVALUATE
OUR
APPROACH
ON
TWO
DATASETS
OUTDOOR
SCENE
RECOGNITION
OSR
DATASET
CONTAINING
IMAGES
FROM
CATEGORIES
WE
USE
THE
DIMENSIONAL
GIST
DESCRIPTOR
AS
OUR
IMAGE
FEATURES
A
SUBSET
OF
THE
PUBLIC
FIGURE
FACE
DATABASE
PUBFIG
CON
TAINING
IMAGES
FROM
RANDOM
IDENTITIES
IMAGES
THIS
APPLICATION
DOES
NOT
REQUIRE
CATEGORY
LABELS
THE
RELATIVE
SUPERVISION
CAN
BE
PROVIDED
FOR
CATEGORIES
WHICH
IS
PROPAGATED
TO
IMAGES
TABLE
BINARY
AND
RELATIVE
ATTRIBUTE
ASSIGNMENTS
USED
IN
OUR
EXPERIMENTS
NOTE
THAT
NONE
OF
THE
RELATIVE
ORDERINGS
VIOLATE
THE
BINARY
MEMBERSHIPS
THE
OSR
DATASET
INCLUDES
IMAGES
FROM
THE
FOLLOWING
CATEGORIES
COAST
C
FOREST
F
HIGHWAY
H
INSIDE
CITY
I
MOUNTAIN
M
OPEN
COUNTRY
O
STREET
AND
TALL
BUILDING
T
THE
ATTRIBUTES
SHOWN
ABOVE
ARE
LISTED
IN
AS
THE
PROPERTIES
SUBJECTS
USED
TO
ORGANIZE
THE
IMAGES
THE
PUBFIG
DATASET
INCLUDES
IMAGES
OF
ALEX
RODRIGUEZ
A
CLIVE
OWEN
C
HUGH
LAURIE
H
JARED
LETO
J
MILEY
CYRUS
M
SCARLETT
JOHANSSON
VIGGO
MORTENSEN
V
AND
ZAC
EFRON
Z
THE
ATTRIBUTES
SHOWN
ABOVE
ARE
A
SUBSET
OF
THE
ATTRIBUTES
PROVIDED
WITH
THE
DATASET
THEY
WERE
CHOSEN
FOR
THEIR
SIMPLICITY
SUFFICIENT
VARIATION
AMONG
THE
CATEGORIES
AND
TO
AVOID
REDUNDANCY
E
G
USING
YOUNG
INSTEAD
OF
OLD
MIDDLE
AGED
YOUTH
CHILD
EACH
WE
USE
A
CONCATENATION
OF
THE
GIST
DESCRIPTOR
AND
A
DIMENSIONAL
LAB
COLOR
HISTOGRAM
AS
OUR
IMAGE
FEATURES
TABLE
PROVIDES
MORE
DETAILS
ABOUT
THE
DATASETS
AND
SHOWS
THE
BINARY
MEMBERSHIPS
AND
RELATIVE
ORDERINGS
OF
CATEGORIES
BY
ATTRIBUTES
THESE
WERE
COLLECTED
USING
THE
JUDGEMENTS
OF
A
COLLEAGUE
UNFAMILIAR
WITH
THE
DETAILS
OF
THIS
WORK
WE
SEE
THE
LIMITATION
OF
BINARY
ATTRIBUTES
IN
DISTIN
GUISHING
BETWEEN
SOME
CATEGORIES
WHILE
THE
SAME
SET
OF
AT
TRIBUTES
USED
RELATIVELY
TEASE
THEM
APART
ALTHOUGH
WE
HAVE
A
FULL
ORDERING
IN
OUR
EXPERIMENTS
WE
SAMPLE
RANDOM
PAIRS
OF
CATEGORIES
AS
SUPERVISION
AS
NOTED
BELOW
RECALL
THAT
DIFFERENT
PAIRS
OF
CATEGORIES
CAN
BE
RELATED
FOR
DIFFERENT
AT
TRIBUTES
NOTE
THAT
WE
COLLECT
THE
BINARY
SUPERVISION
ONLY
TO
TRAIN
BASELINE
APPROACHES
OUR
APPROACH
USES
ONLY
THE
RELA
TIVE
SUPERVISION
AS
A
SANITY
CHECK
WE
FIRST
DEMONSTRATE
THE
SUPERIORITY
OF
OUR
LEARNT
RANKS
TO
CAPTURE
RELATIVE
ORDERINGS
AS
COMPARED
TO
AN
APPROACH
THAT
TREATS
THE
SCORE
OF
BINARY
CLASSIFIERS
AS
A
RANK
SECTION
THEN
WE
EVALUATE
THE
USE
OF
RELATIVE
ATTRIBUTES
FOR
THE
TWO
NEW
TASKS
SECTIONS
AND
LEARNED
RANKING
VS
CLASSIFIER
SCORES
WE
TRAIN
A
BINARY
LINEAR
SVM
HM
BY
TRANSFERRING
THE
BI
NARY
SUPERVISION
LISTED
IN
TABLE
TO
THE
TRAINING
IMAGES
FOR
EACH
ATTRIBUTE
FOR
AN
IMAGE
PAIR
I
J
IN
A
HELD
OUT
TEST
SET
IMAGES
FOR
OSR
FOR
PUBFIG
WE
EVALUATE
THE
LEARNT
CLASSIFIER
AND
IF
HM
XI
HM
XJ
WE
PREDICT
I
J
ELSE
I
J
FOR
AM
FOR
COMPARISON
WE
LEARN
A
LINEAR
RANKING
FUNCTION
RM
FOR
EACH
ATTRIBUTE
USING
THE
RELATIVE
CONSTRAINTS
IN
TABLE
AND
COMPARE
RM
XI
TO
RM
XJ
ON
THE
SAME
TEST
PAIRS
BOTH
METHODS
PREDICTIONS
ARE
THEN
COMPARED
TO
THE
GROUND
TRUTH
RELATIVE
ORDERING
THE
LEARNT
RANKING
FUNCTION
ACCURACY
IS
AND
ON
THE
OSR
AND
PUBFIG
DATASETS
RESPECTIVELY
AS
COMPARED
TO
AND
IF
USING
THE
BINARY
CLASSIFIER
SCORES
CON
FIRMING
THE
ADVANTAGE
OF
A
RANKING
FUNCTION
TO
EFFECTIVELY
OSR
UNSEEN
CATEGORIES
PUBFIG
UNSEEN
CATEGORIES
CAPTURE
RELATIVE
INFORMATION
FIGURE
ZERO
SHOT
LEARNING
PERFORMANCE
AS
THE
PROPORTION
OF
UNSEEN
CAT
EGORIES
INCREASES
TOTAL
NUMBER
OF
CLASSES
N
REMAINS
CONSTANT
AT
ZERO
SHOT
LEARNING
RESULTS
WE
COMPARE
OUR
ZERO
SHOT
APPROACH
TO
TWO
BASELINES
BASELINES
OUR
FIRST
BASELINE
IS
THE
DIRECT
ATTRIBUTE
PRE
DICTION
DAP
MODEL
OF
LAMPERT
ET
AL
WHICH
USES
BI
NARY
ATTRIBUTE
DESCRIPTIONS
FOR
ALL
CATEGORIES
WE
TRAIN
LINEAR
SVMS
BY
TRANSFERRING
THE
BINARY
SUPERVISION
IN
TABLE
TO
TRAINING
IMAGES
FROM
THE
SEEN
CATEGORIES
A
TEST
IMAGE
X
IS
ASSIGNED
TO
A
CATEGORY
USING
OSR
LABELED
PAIRS
PUBFIG
LABELED
PAIRS
M
C
ARGMAX
C
X
C
N
M
FIGURE
ZERO
SHOT
LEARNING
PERFORMANCE
AS
MORE
PAIRS
OF
SEEN
CATEGORIES
ARE
RELATED
I
E
LABELED
DURING
TRAINING
UNSEEN
CATEGORIES
DEMONSTRATE
THE
BENEFIT
OF
THE
GENERA
TIVE
MODELING
OF
THE
CATEGORIES
IN
SRA
WHERE
P
AM
BC
X
IS
COMPUTED
BY
TRANSFORMING
THE
BINARY
CLASSIFIER
SCORE
VIA
A
SIGMOID
FUNCTION
AND
BC
IS
THE
GROUND
TRUTH
BINARY
BIT
TAKEN
BY
ATTRIBUTE
AM
FOR
CLASS
C
AS
SEEN
IN
TABLE
IF
AM
IS
NOT
USED
TO
DESCRIBE
AN
UNSEEN
CATEGORY
P
AM
BC
X
IS
UNIFORM
WE
CALL
OUR
SECOND
BASELINE
SCORE
BASED
RELATIVE
AT
TRIBUTES
SRA
IT
FOLLOWS
THE
SAME
APPROACH
AS
IN
SEC
TION
EXCEPT
THAT
IT
REPLACES
RANK
VALUES
WITH
THE
BINARY
CLASSIFIER
OUTPUT
SCORE
IT
IS
A
STRONGER
BASELINE
THAN
DAP
AS
IT
HAS
THE
SAME
BENEFITS
OF
THE
GENERATIVE
MODELING
OF
SEEN
CLASSES
AND
RELATIVE
DESCRIPTIONS
OF
UNSEEN
CLASSES
AS
OUR
APPROACH
IT
IS
NOT
LIMITED
BY
THE
BINARY
DESCRIPTION
OF
THE
CATEGORIES
WHICH
MAY
BE
DEPRIVED
AS
SEEN
IN
TABLE
SET
UP
WE
COMPARE
ALL
METHODS
IN
SEVERAL
DIFFERENT
SCE
NARIOS
UNLESS
SPECIFIED
WE
USE
UNSEEN
AND
SEEN
CATE
GORIES
TO
TRAIN
THE
RANKING
FUNCTIONS
WE
USE
CATEGORY
PAIRS
AMONG
SEEN
CATEGORIES
AND
UNSEEN
CATEGORIES
ARE
DE
SCRIBED
RELATIVE
TO
THE
TWO
CLOSEST
SEEN
CATEGORIES
FOR
EACH
ATTRIBUTE
ONE
STRONGER
ONE
WEAKER
WE
USE
TRAINING
IM
AGES
PER
CLASS
AND
THE
REST
FOR
TESTING
AND
REPORT
MEAN
PER
CLASS
ACCURACY
OVER
RANDOM
TRAIN
TEST
AND
SEEN
UNSEEN
SPLITS
PROPORTION
OF
UNSEEN
CATEGORIES
WE
FIRST
STUDY
ZERO
SHOT
LEARNING
ACCURACY
AS
THE
PROPORTION
OF
UNSEEN
CATE
GORIES
INCREASES
FIGURE
SHOWS
THE
RESULTS
FIRST
WE
SEE
EVEN
WHEN
ALL
CATEGORIES
ARE
SEEN
UN
SEEN
OUR
APPROACH
SIGNIFICANTLY
OUTPERFORMS
BOTH
BASE
LINES
THIS
VALIDATES
THE
POWER
OF
RELATIVE
ATTRIBUTES
FOR
THE
CLASSICAL
RECOGNITION
TASK
ALSO
SRA
GAINS
OVER
DAP
WITH
FURTHER
AS
WE
WOULD
EXPECT
ACCURACY
FOR
ALL
THREE
AP
PROACHES
DECREASES
WITH
MORE
UNSEEN
CATEGORIES
HOWEVER
OUR
METHOD
REMAINS
BETTER
THAN
THE
BASELINES
FOR
MOST
OF
THE
SPECTRUM
UNTIL
ONLY
SEEN
CATEGORIES
REMAIN
AT
WHICH
POINT
IT
PERFORMS
SIMILARLY
TO
SRA
THIS
IS
EXPECTED
SINCE
BEYOND
THAT
WITH
ONLY
SEEN
CATEGORIES
THE
RELATIVE
AND
BI
NARY
SUPERVISION
BECOMES
EQUIVALENT
BOTH
STILL
COMPARE
FAVORABLY
TO
DAP
DUE
TO
THE
BENEFIT
OF
RELATIVE
DESCRIPTION
IN
GENERAL
WE
CAN
EXPECT
THAT
WITH
EVEN
MORE
TOTAL
CATE
GORIES
THE
DESCRIPTION
POWER
OF
RELATIVE
ATTRIBUTES
WILL
ALSO
INCREASE
AS
UNSEEN
CATEGORIES
WOULD
HAVE
MORE
CATEGORIES
TO
BE
RELATED
TO
EVEN
WITH
A
FIXED
NUMBER
OF
ATTRIBUTES
A
BINARY
DESCRIPTION
ON
THE
OTHER
HAND
CAN
ONLY
LOSE
DISCRIM
INATIVE
POWER
AS
MORE
CATEGORIES
ARE
ADDED
AMOUNT
OF
SUPERVISION
WE
NEXT
STUDY
THE
IMPACT
OF
VARYING
THE
AMOUNT
OF
SUPERVISION
FIGURE
SHOWS
THE
RESULTS
AS
WE
INCREASE
THE
NUMBER
OF
PAIRS
OF
SEEN
CATEGORIES
USED
TO
GENERATE
RELATIVE
CON
STRAINTS
WHERE
FOR
EACH
ATTRIBUTE
WE
RANDOMLY
SELECT
THE
PERFORMANCE
IS
QUITE
ROBUST
TO
THE
NUMBER
AND
CHOICE
OF
PAIRS
AS
FEW
AS
TWO
PAIRS
SUFFICE
WHEN
USING
ONLY
ONE
PAIR
OUR
METHOD
RECEIVES
SIGNIFICANTLY
LESS
SUPERVISION
THAN
THE
TWO
BASELINES
FOR
WHICH
ALL
SIX
CATEGORIES
ARE
LABELED
HENCE
THEIR
FLAT
CURVES
IN
SPITE
OF
THIS
OUR
APPROACH
PER
FORMS
FAVORABLY
ON
OSR
THOUGH
SUFFERS
COMPARED
TO
SRA
ON
PUBFIG
THERE
ARE
A
TOTAL
OF
POSSIBLE
PAIRS
TO
BE
LABELED
AS
FEW
AS
OF
THEM
COULD
DETERMINE
A
UNIQUE
ORDERING
ON
ALL
CATEGORIES
MORE
NATURAL
THAN
LESS
NATURAL
THAN
OSR
MORE
OPEN
THAN
LESS
OPEN
THAN
IMAGE
IS
MORE
OPEN
THAN
LESS
OPEN
THAN
SHOWS
MORE
THAN
SHOWS
LESS
THAN
DAP
SRA
ATT
TO
DESCRIBE
UNSEEN
ATT
TO
FIGURE
PART
OF
EXAMPLE
DESCRIPTION
GENERATED
FOR
LEFT
IMAGE
BY
BINARY
ATTRIBUTE
BASELINE
MIDDLE
AND
OUR
METHOD
RIGHT
SEE
TEXT
FOR
DETAILS
TO
THE
POWER
OF
RELATIVE
ATTRIBUTES
TO
JOINTLY
CARVE
OUT
RE
GIONS
IN
THE
SPACE
OF
ATTRIBUTE
STRENGTHS
CORRESPONDING
TO
FIGURE
ZERO
SHOT
LEARNING
PERFORMANCE
AS
FEWER
DESCRIBE
THE
UNSEEN
CATEGORIES
OSR
THE
UNSEEN
CATEGORY
THIS
MAKES
THE
DISTANCE
OF
THE
REFER
ENCE
CATEGORIES
LESS
RELEVANT
AS
LONG
AS
THE
RELATIONSHIPS
ARE
CORRECTLY
INDICATED
LOOSENESS
OF
CONSTRAINTS
LOOSENESS
OF
CONSTRAINTS
DESCRIBING
IMAGES
RESULTS
NEXT
WE
DEMONSTRATE
OUR
APPROACH
TO
GENERATE
RELATIVE
DESCRIPTIONS
OF
NOVEL
IMAGES
TO
QUANTIFY
THEIR
EFFECTIVE
NESS
WE
PERFORM
A
HUMAN
SUBJECT
STUDY
THAT
PITS
THE
BINARY
ATTRIBUTE
BASELINE
AGAINST
OUR
RELATIVE
APPROACH
OUR
METHOD
REPORTS
THE
PROPERTIES
PREDICTED
RELATIVE
TO
REFERENCE
IMAGES
FIGURE
ZERO
SHOT
LEARNING
PERFORMANCE
AS
THE
UNSEEN
CATEGORIES
ARE
DE
SCRIBED
VIA
LOOSER
RELATIONSHIPS
FIGURE
SHOWS
THE
RESULTS
AS
WE
DECREASE
THE
NUMBER
OF
ATTRIBUTES
USED
TO
DESCRIBE
THE
UNSEEN
CATEGORY
DURING
TRAINING
NOTE
THAT
THE
NUMBER
OF
ATTRIBUTES
USED
TO
DESCRIBE
THE
SEEN
CATEGORIES
DURING
TRAINING
REMAINS
THE
SAME
SEE
ITEM
IN
SEC
THE
ACCURACY
OF
ALL
METHODS
DEGRADES
HOWEVER
THE
APPROACHES
USING
RELATIVE
ATTRIBUTES
SRA
AND
OURS
DECAY
GRACEFULLY
WHEREAS
DAP
SUFFERS
MORE
DRAMAT
ICALLY
THIS
ILLUSTRATES
HOW
EACH
ATTRIBUTE
CONVEYS
STRONGER
DISTINGUISHING
POWER
WHEN
USED
RELATIVELY
THIS
IS
A
KEY
RE
SULT
THIS
SCENARIO
EXEMPLIFIES
THE
HIGH
LEVEL
OF
FLEXIBILITY
IN
SUPERVISION
OF
UNSEEN
CATEGORIES
THAT
OUR
APPROACH
ENABLES
WHICH
IS
CRUCIAL
FOR
PRACTICAL
APPLICATIONS
QUALITY
OF
SUPERVISION
WHAT
HAPPENS
IF
THE
RELATION
SHIPS
DESCRIBED
FOR
AN
UNSEEN
CLASS
ARE
LOOSER
THAT
IS
WHAT
IF
THE
ANNOTATOR
RELATES
IT
TO
SEEN
CLASSES
WHOSE
ATTRIBUTE
STRENGTHS
ARE
MORE
DISTANT
E
G
SAYS
MILEY
IS
YOUNGER
THAN
VITTO
RATHER
THAN
MILEY
IS
YOUNGER
THAN
SCARLETT
A
PERSON
CLOSER
IN
AGE
IDEALLY
THE
SUPERVISOR
WOULD
HAVE
FREEDOM
TO
SPECIFY
ANY
REFERENCE
CATEGORIES
THAT
IS
THE
MOST
NATURAL
FORM
OF
DESCRIPTION
AND
DOES
NOT
REQUIRE
THE
SUPERVISOR
TO
KNOW
THE
EXHAUSTIVE
LIST
OF
SEEN
CATEGORIES
THUS
WE
NEXT
EVALUATE
PERFORMANCE
AS
WE
IN
CREASE
THE
NUMBER
OF
RELATIVE
RANKS
AWAY
LOOSENESS
FROM
THE
SEEN
CATEGORIES
USED
TO
DESCRIBE
THE
UNSEEN
CATEGORY
FIGURE
SHOWS
THE
RESULTS
WE
SEE
OUR
APPROACH
IS
VERY
ROBUST
TO
THE
LOOSENESS
OF
THE
CONSTRAINTS
WE
ATTRIBUTE
THIS
ANY
LEVEL
OF
LOOSENESS
IF
THERE
EXISTS
NO
SEEN
CATEGORY
AT
A
DESIRED
DISTANCE
FROM
THE
UNSEEN
CATEGORY
IN
EITHER
DIRECTION
WE
SIMPLY
USE
A
ONE
ENDED
CONSTRAINT
HENCE
WHEN
THE
CONSTRAINTS
ARE
AT
A
LOOSENESS
OF
SINCE
ONLY
OUT
OF
CATEGORIES
ARE
SEEN
SOME
OF
WHICH
OFTEN
HAVE
SIMILAR
ATTRIBUTE
STRENGTHS
A
LARGE
PERCENTAGE
OF
THE
CONSTRAINTS
ARE
ONE
SIDED
SEE
SEC
WHILE
THE
BASELINE
REPORTS
THE
PREDICTED
PRES
ENCE
ABSENCE
OF
ATTRIBUTES
ONLY
THE
HUMAN
SUBJECT
MUST
GUESS
WHICH
IMAGE
LED
TO
THE
AUTO
GENERATED
DESCRIPTIONS
TO
OUR
KNOWLEDGE
THESE
ARE
THE
FIRST
RESULTS
TO
QUANTIFY
HOW
WELL
ALGORITHM
GENERATED
ATTRIBUTE
DESCRIPTIONS
CAN
COMMU
NICATE
TO
HUMANS
WE
RECRUITED
SUBJECTS
ONLY
SOME
FAMILIAR
WITH
VI
SION
WE
RANDOMLY
SELECTED
PUBFIG
AND
OSR
IM
AGES
FOR
EACH
OF
THE
TEST
CASES
WE
PRESENT
THE
SUBJECT
A
DESCRIPTION
USING
THREE
RANDOMLY
SELECTED
ATTRIBUTES
PLUS
A
MULTIPLE
CHOICE
SET
OF
THREE
IMAGES
ONE
OF
WHICH
IS
COR
RECT
THE
SUBJECT
IS
ASKED
TO
RANK
THEIR
GUESSES
FOR
WHICH
FITS
THE
DESCRIPTION
BEST
SEE
FIGURE
A
TO
AVOID
BIAS
WE
DIVIDED
THE
SUBJECTS
INTO
TWO
GROUPS
EACH
GROUP
SAW
EITHER
THE
BINARY
OR
THE
RELATIVE
ATTRIBUTES
BUT
NOT
BOTH
FURTHER
WE
DISPLAY
REFERENCE
IMAGES
FOR
EITHER
GROUP
TASK
TO
HELP
SUBJECTS
UNDERSTAND
THE
ATTRIBUTE
MEANINGS
FIGURE
B
SHOWS
THE
RESULTS
SUBJECTS
ARE
SIGNIFICANTLY
MORE
LIKELY
TO
IDENTIFY
THE
CORRECT
IMAGE
USING
OUR
METHOD
DESCRIPTION
I
E
VS
IN
THE
FIRST
CHOICE
THIS
RE
INFORCES
OUR
CLAIM
THAT
RELATIVE
ATTRIBUTES
CAN
BETTER
CAPTURE
THE
CONCEPT
OF
THE
IMAGE
AND
SUGGESTS
THEIR
REAL
PROMISE
FOR
IMPROVED
GUIDED
SEARCH
OR
INTERACTIVE
LEARNING
WE
NOTE
THAT
WE
AUGMENTED
THE
BASELINE
BINARY
DE
SCRIPTIONS
WITH
PROTOTYPE
IMAGES
SHOWING
STARK
CONTRAST
OF
ATTRIBUTE
PRESENCE
EVEN
THOUGH
UNLIKE
OUR
APPROACH
THEY
ARE
NOT
AN
INTRINSIC
PART
OF
THE
GENERATED
DESCRIPTION
WE
SUSPECT
THAT
SUBJECTS
WOULD
PERFORM
EVEN
WORSE
WITH
PURELY
TEXTUAL
BINARY
DESCRIPTIONS
THUS
THE
HUMAN
STUDY
IS
IF
ANYTHING
GENEROUS
TO
THE
BASELINE
OUR
APPROACH
CAN
BE
USED
TO
GENERATE
PURELY
TEXTUAL
DE
SCRIPTIONS
AS
WELL
WHERE
AN
IMAGE
IS
DESCRIBED
RELATIVE
TO
OTHER
CATEGORIES
INSTEAD
OF
IMAGES
FIGURE
C
SHOWS
EX
AMPLES
HERE
OUR
METHOD
SELECTS
THE
CATEGORIES
TO
COMPARE
WHICH
IMAGE
IS
MORE
CHUBBY
THAN
LESS
CHUBBY
THAN
MORE
NATURAL
THAN
TALLBUILDING
LESS
NATURAL
THAN
FOREST
MORE
OPEN
THAN
TALLBUILDING
LESS
OPEN
THAN
COAST
MORE
THAN
TALLBUILDING
MORE
NATURAL
THAN
INSIDECITY
LESS
NATURAL
THAN
HIGHWAY
MORE
OPEN
THAN
STREET
LESS
OPEN
THAN
COAST
MORE
THAN
HIGHWAY
LESS
THAN
INSIDECITY
MORE
NATURAL
THAN
TALLBUILDING
LESS
NATURAL
THAN
MOUNTAIN
MORE
OPEN
THAN
MOUNTAIN
LESS
THAN
OPENCOUNTRY
MORE
WHITE
THAN
ALEXRODRIGUEZ
MORE
SMILING
THAN
JAREDLETO
LESS
SMILING
THAN
ZACEFRON
MORE
VISIBLEFOREHEAD
THAN
JAREDLETO
LESS
VISIBLEFOREHEAD
THAN
MILEYCYRUS
MORE
WHITE
THAN
ALEXRODRIGUEZ
LESS
WHITE
THAN
MILEYCYRUS
LESS
SMILING
THAN
HUGHLAURIE
MORE
VISIBLEFOREHEAD
THAN
ZACEFRON
LESS
VISIBLEFOREHEAD
THAN
MILEYCYRUS
MORE
YOUNG
THAN
CLIVEOWEN
LESS
YOUNG
THAN
SCAR
MORE
BUSHYEYEBROWS
THAN
ZACEFRON
LESS
BUSHYEYEB
ALEXRODRIGUEZ
MORE
ROUNDFACE
THAN
CLIVEOWEN
THAN
ZACEFRON
A
B
C
D
E
F
C
IMAGES
DESCRIBED
RELATIVE
TO
CATEGORIES
FIGURE
AUTO
GENERATED
DESCRIPTIONS
OF
IMAGES
IN
C
A
BIN
NOT
NAT
URAL
NOT
OPEN
PERSPECTIVE
REL
MORE
NATURAL
THAN
TALLBUILDING
LESS
NATU
RAL
THAN
FOREST
MORE
OPEN
THAN
TALLBUILDING
LESS
OPEN
THAN
COAST
MORE
PERSPECTIVE
THAN
TALLBUILDING
B
BIN
NOT
NATURAL
NOT
OPEN
PERSPEC
TIVE
REL
MORE
NATURAL
THAN
INSIDECITY
LESS
NATURAL
THAN
HIGHWAY
MORE
OPEN
THAN
STREET
LESS
OPEN
THAN
COAST
MORE
PERSPECTIVE
THAN
HIGHWAY
LESS
PERSPECTIVE
THAN
INSIDECITY
C
BIN
NATURAL
OPEN
PERSPECTIVE
REL
MORE
NATURAL
THAN
TALLBUILDING
LESS
NATURAL
THAN
MOUNTAIN
MORE
OPEN
THAN
MOUNTAIN
LESS
PERSPECTIVE
THAN
OPENCOUNTRY
D
BIN
WHITE
NOT
SMIL
ING
VISIBLEFOREHEAD
REL
MORE
WHITE
THAN
ALEXRODRIGUEZ
MORE
SMILING
THAN
JAREDLETO
LESS
SMILING
THAN
ZACEFRON
MORE
VISIBLEFOREHEAD
THAN
JAREDLETO
LESS
VISIBLEFOREHEAD
THAN
MILEYCYRUS
E
BIN
WHITE
NOT
SMILING
NOT
VISIBLEFOREHEAD
REL
MORE
WHITE
THAN
ALEXRODRIGUEZ
LESS
WHITE
THAN
MILEYCYRUS
LESS
SMILING
THAN
HUGHLAURIE
MORE
VISIBLEFORE
HEAD
THAN
ZACEFRON
LESS
VISIBLEFOREHEAD
THAN
MILEYCYRUS
F
BIN
NOT
YOUNG
BUSHYEYEBROWS
ROUNDFACE
REL
MORE
YOUNG
THAN
CLIVEOWEN
LESS
YOUNG
THAN
SCARLETTJOHANSSON
MORE
BUSHYEYEBROWS
THAN
ZACEFRON
LESS
BUSHYEYEBROWS
THAN
ALEXRODRIGUEZ
MORE
ROUNDFACE
THAN
CLIVE
OWEN
LESS
ROUNDFACE
THAN
ZACEFRON
TO
SUCH
THAT
AT
LEAST
OF
THE
IMAGES
IN
THE
CATEGORY
HAVE
AN
ATTRIBUTE
STRENGTH
LARGER
THAN
LESS
THAN
THAT
COMPUTED
FOR
THE
IMAGE
TO
BE
DESCRIBED
ECHOING
OUR
QUANTITATIVE
RESULTS
WE
CAN
QUALITATIVELY
SEE
THAT
THE
RELATIVE
DESCRIP
TIONS
ARE
MORE
PRECISE
AND
INFORMATIVE
THAN
THE
BINARY
ONES
MORE
RESULTS
CAN
BE
FOUND
ON
THE
AUTHORS
WEBSITES
CONCLUSION
WE
INTRODUCED
RELATIVE
ATTRIBUTES
WHICH
ALLOW
FOR
A
RICHER
LANGUAGE
OF
SUPERVISION
AND
DESCRIPTION
THAN
THE
COMMONLY
USED
CATEGORICAL
BINARY
ATTRIBUTES
WE
PRE
SENTED
TWO
NOVEL
APPLICATIONS
ZERO
SHOT
LEARNING
BASED
ON
RELATIONSHIPS
AND
DESCRIBING
IMAGES
RELATIVE
TO
OTHER
IMAGES
OR
CATEGORIES
THROUGH
EXTENSIVE
EXPERIMENTS
AS
WELL
AS
A
HUMAN
SUBJECT
STUDY
WE
CLEARLY
DEMONSTRATED
THE
ADVAN
TAGES
OF
OUR
IDEA
FUTURE
WORK
INCLUDES
EXPLORING
MORE
NOVEL
APPLICATIONS
OF
RELATIVE
ATTRIBUTES
SUCH
AS
GUIDED
SEARCH
OR
INTERACTIVE
LEARNING
AND
AUTOMATIC
DISCOVERY
OF
RELATIVE
ATTRIBUTES
ACKNOWLEDGEMENTS
WE
THANK
THE
SUBJECTS
OF
OUR
HU
MAN
STUDIES
FOR
THEIR
TIME
THIS
RESEARCH
IS
SUPPORTED
IN
PART
BY
NSF
IIS
ONR
ATL
AND
THE
LUCE
FOUNDATION
TO
APPEAR
PROCEEDINGS
OF
THE
IEEE
CONFERENCE
ON
COMPUTER
VISION
AND
PATTERN
RECOGNITION
CVPR
FINDING
THE
WEAKEST
LINK
IN
PERSON
DETECTORS
DEVI
PARIKH
TOYOTA
TECHNOLOGICAL
INSTITUTE
CHICAGO
TTIC
C
LAWRENCE
ZITNICK
MICROSOFT
RESEARCH
REDMOND
ABSTRACT
DETECTING
PEOPLE
REMAINS
A
POPULAR
AND
CHALLENGING
PROBLEM
IN
COMPUTER
VISION
IN
THIS
PAPER
WE
ANALYZE
PARTS
BASED
MODELS
FOR
PERSON
DETECTION
TO
DETERMINE
WHICH
COMPONENTS
OF
THEIR
PIPELINE
COULD
BENEFIT
THE
MOST
IF
IM
PROVED
WE
ACCOMPLISH
THIS
TASK
BY
STUDYING
NUMEROUS
DE
TECTORS
FORMED
FROM
COMBINATIONS
OF
COMPONENTS
PERFORMED
BY
HUMAN
SUBJECTS
AND
MACHINES
THE
PARTS
BASED
MODEL
WE
STUDY
CAN
BE
ROUGHLY
BROKEN
INTO
FOUR
COMPONENTS
FEA
TURE
DETECTION
PART
DETECTION
SPATIAL
PART
SCORING
AND
CON
TEXTUAL
REASONING
INCLUDING
NON
MAXIMAL
SUPPRESSION
OUR
EXPERIMENTS
CONCLUDE
THAT
PART
DETECTION
IS
THE
WEAKEST
LINK
FOR
CHALLENGING
PERSON
DETECTION
DATASETS
NON
MAXIMAL
SUPPRESSION
AND
CONTEXT
CAN
ALSO
SIGNIFICANTLY
BOOST
PER
FORMANCE
HOWEVER
THE
USE
OF
HUMAN
OR
MACHINE
SPATIAL
MODELS
DOES
NOT
SIGNIFICANTLY
OR
CONSISTENTLY
AFFECT
DETEC
TION
ACCURACY
INTRODUCTION
OBJECT
DETECTION
REMAINS
AN
OPEN
AND
CHALLENGING
PROB
LEM
IN
COMPUTER
VISION
HISTORICALLY
THE
SUBCLASS
OF
DETECT
ING
PEOPLE
HAS
ATTRACTED
INCREASED
ATTENTION
GIVEN
ITS
IMPOR
TANCE
TO
MANY
REAL
WORLD
APPLICATIONS
AND
ITS
CHALLENGING
LEVEL
OF
DIFFICULTY
THE
WIDE
VARIETY
OF
POSES
AND
SHAPES
PEOPLE
EXHIBIT
ALONG
WITH
VARIATIONS
IN
CLOTHING
CREATES
A
VERY
CHALLENGING
TASK
FOR
MODELING
AND
LEARNING
ALGORITHMS
RECENTLY
PERSON
DETECTORS
HAVE
MADE
SIGNIFICANT
PROGRESS
USING
PART
BASED
MODELS
THE
APPEARANCE
OF
EACH
PART
SUCH
AS
A
PERSON
HEAD
FOOT
OR
TORSO
ARE
REPRESENTED
BY
HISTOGRAMS
OF
GRADIENTS
HOG
COLOR
OR
HARR
WAVELETS
THE
SPATIAL
RELATIONSHIPS
OF
OBJECT
PARTS
CAN
BE
REPRESENTED
USING
TREES
K
FANS
OR
CONSTELLATION
MODELS
EACH
OF
THESE
APPROACHES
PROPOSE
A
COMPLEX
SET
OF
INTERDEPENDENT
COMPONENTS
TO
PROVIDE
FINAL
DETECTION
RESULTS
WHILE
THE
ADDITIONAL
COMPLEXITY
OF
THE
APPROACHES
HAVE
LED
TO
INCREASED
PERFORMANCE
UNDERSTANDING
THE
ROLE
OF
EACH
COMPONENT
IN
THE
FINAL
DETECTION
ACCURACY
IS
DIFFI
CULT
IN
THIS
PAPER
WE
PROPOSE
A
THOROUGH
ANALYSIS
OF
PARTS
BASED
MODELS
TO
GAIN
INSIGHT
INTO
WHICH
COMPONENTS
OF
THE
INPUT
OUTPUT
FIGURE
IN
ORDER
TO
GAIN
INSIGHT
INTO
WHICH
COMPONENTS
OF
A
PARTS
BASED
PERSON
DETECTOR
COULD
BENEFIT
THE
MOST
IF
IMPROVED
WE
REPLACE
EACH
COMPONENT
I
E
PART
DETECTION
P
FEATURE
EXTRACT
F
SPATIAL
MODELING
SM
AND
NON
MAXIMA
SUPPRESION
NMS
IN
THE
PIPELINE
WITH
HUMAN
SUB
JECTS
GREEN
BARS
HERE
WE
ILLUSTRATE
THE
VARIOUS
TASKS
PERFORMED
BY
HUMAN
SUBJECTS
VIA
EXAMPLE
INPUT
OUTPUT
PAIRS
PIPELINE
COULD
BENEFIT
THE
MOST
IF
IMPROVED
WE
ACCOMPLISH
THIS
TASK
BY
USING
HUMAN
SUBJECTS
TO
PERFORM
THE
INDIVID
UAL
COMPONENTS
PREVIOUSLY
PERFORMED
BY
THE
MACHINE
AL
GORITHM
FOR
INSTANCE
INSTEAD
OF
USING
A
MACHINE
CLASSIFIER
SUCH
AS
A
LATENT
SVM
TRAINED
ON
HOG
DESCRIPTORS
TO
DE
TECT
OBJECT
PARTS
WE
USE
HUMAN
SUBJECTS
TO
LABEL
WHETHER
A
SMALL
IMAGE
PATCH
CONTAINS
A
HUMAN
HEAD
FOOT
TORSO
ETC
A
PARTS
BASED
DETECTOR
CAN
BE
ROUGHLY
BROKEN
INTO
FOUR
COMPONENTS
FEATURE
DETECTION
PART
DETECTION
SPATIAL
PART
SCORING
AND
CONTEXTUAL
REASONING
INCLUDING
NON
MAXIMAL
SUPPRESSION
WE
COMBINE
NUMEROUS
HUMAN
AND
MACHINE
PERFORMED
COMPONENTS
TO
FORM
COMPLETE
PERSON
DETECTORS
AND
RECOGNIZERS
THE
RESULTS
INDICATE
WHICH
COMPONENTS
LEAD
TO
THE
GREATEST
INCREASE
IN
ACCURACY
OVER
THE
STANDARD
MACHINE
APPROACH
THE
EXPERIMENTS
INCLUDE
THE
USE
OF
VAR
IOUS
FEATURE
TYPES
SUCH
AS
COLOR
EDGES
AND
INTENSITIES
FOR
BOTH
DETECTING
PEOPLE
AND
PARTS
THE
USE
OF
HUMAN
DETECTED
PARTS
WITH
A
MACHINE
SPATIAL
MODEL
AND
MACHINE
DETECTED
PARTS
WITH
USING
A
HUMAN
SPATIAL
MODEL
OTHER
EXPERI
MENTS
ANALYZING
NON
MAXIMAL
SUPPRESSION
TECHNIQUES
AND
CONTEXTUAL
INFORMATION
ARE
ALSO
PERFORMED
RELATED
WORK
WE
NOW
DISCUSS
SOME
EXISTING
TECHNIQUES
FOR
PERSON
DE
TECTION
AS
WELL
WORKS
THAT
CONDUCT
HUMAN
STUDIES
TO
GAIN
INSIGHTS
IN
COMPUTER
VISION
WE
COMPREHENSIVELY
DISCUSS
WORKS
ON
PARTS
SPATIAL
MODELS
AND
CONTEXTUAL
MODELS
IN
SECTION
PERSON
PEDESTRIAN
DETECTION
GIVEN
THE
IMPORTANCE
OF
DETECTING
PEOPLE
IN
IMAGES
NUMEROUS
DETECTORS
HAVE
BEEN
PROPOSED
A
COMPARISON
OF
SEVERAL
APPROACHES
FOR
PEDES
TRIAN
DETECTION
CAN
BE
FOUND
IN
DOLLAR
ET
AL
WOJEK
ET
AL
ANALYZES
SEVERAL
FEATURES
AND
CLASSIFIER
TYPES
DALAL
AND
TRIGGS
FIRST
PROPOSED
THE
LOCALLY
NORMALIZED
HISTOGRAM
OF
GRADIENTS
DETECTOR
WHICH
WAS
IMPROVED
UPON
BY
FELZEN
SZWALB
ET
AL
USING
DEFORMABLE
PARTS
MODELS
INCREASED
PERFORMANCE
WAS
FOUND
USING
NUMEROUS
FEATURE
TYPES
AND
BOOSTING
BY
DOLLAR
ET
AL
AND
USING
MULTI
LEVEL
FEATURES
AND
INTERSECTION
KERNEL
SVMS
BY
MAJI
ET
AL
HUMAN
STUDIES
AN
EARLY
EXAMPLE
OF
DESIGNING
COMPU
TATIONAL
MODELS
WITH
SIMILAR
BEHAVIOR
TO
HUMANS
IS
SHOWN
IN
DAVID
MARR
BOOK
LIU
ET
AL
CONDUCTED
HUMAN
STUDIES
DEMONSTRATING
THAT
THE
HIGH
HUMAN
PERFORMANCE
IN
OBJECT
DISCRIMINATION
CAN
ONLY
BE
EXPLAINED
IF
HUMANS
ARE
USING
INFORMATION
TARR
ET
AL
AND
HINTON
ET
AL
STUDIED
WHETHER
HUMANS
USE
MENTAL
ROTATION
FOR
RECOG
NITION
AND
DETERMINING
IF
SHAPES
HAVE
THE
SAME
HANDINESS
A
COMPARISON
OF
HUMAN
AND
MACHINE
ALGORITHMS
FOR
SELECT
ING
REGIONS
OF
INTEREST
IN
IMAGES
WAS
CONDUCTED
BY
PRIVIT
ERA
ET
AL
FEI
FEI
ET
AL
DEMONSTRATED
THAT
HUMAN
SUBJECTS
CAN
PROVIDE
A
LARGE
AMOUNT
OF
DETAILED
INFORMATION
ABOUT
A
SCENE
EVEN
AFTER
VIEWING
IT
FOR
A
VERY
BRIEF
PERIOD
OF
TIME
BACHMANN
ET
AL
SHOW
THAT
HUMANS
CAN
RELIABLY
RECOGNIZE
FACES
IN
IMAGES
AS
SMALL
AS
PIXELS
AND
OLIVA
ET
AL
PRESENT
SIMILAR
RESULTS
FOR
SCENE
RECOGNI
TION
TORRALBA
ET
AL
AND
PARIKH
ET
AL
SHOW
THAT
HUMANS
CAN
DETECT
OBJECTS
IN
IMAGES
WITH
SIGNIFI
CANTLY
HIGHER
PERFORMANCE
THAN
STATE
OF
THE
ART
MACHINE
AL
GORITHMS
USING
HIGH
RESOLUTION
IMAGES
THE
WORK
OF
PARIKH
ET
AL
USES
HUMAN
STUDIES
TO
DETERMINE
IF
FEATURES
CLAS
SIFICATION
ALGORITHMS
OR
THE
AMOUNT
OF
TRAINING
DATA
IS
MOST
LIKELY
TO
ACCOUNT
FOR
THE
SUPERIORITY
OF
HUMANS
OVER
MA
CHINES
IN
RECOGNIZING
OBJECTS
AND
SCENES
PART
BASED
DETECTOR
IN
THIS
SECTION
WE
DESCRIBE
MACHINE
MODELS
FOR
VARIOUS
COMPONENTS
IN
A
PART
BASED
DETECTOR
INCLUDING
FEATURE
EX
TRACTION
PARTS
MODELING
SPATIAL
MODELS
NON
MAXIMAL
SUP
PRESSION
AND
CONTEXTUAL
REASONING
BEFORE
WE
DESCRIBE
THE
CORRESPONDING
SET
UP
FOR
OUR
HUMAN
STUDIES
FOR
EACH
STAGE
WE
FOLLOW
THE
APPROACH
OF
FELZENSZWALB
ET
AL
THAT
HAS
SHOWN
RECENT
STATE
OF
THE
ART
PERFORMANCE
AND
BRIEFLY
OUT
LINE
OTHER
APPROACHES
OUR
STUDIES
ARE
PERFORMED
ON
SUB
SETS
OF
THE
COMMONLY
USED
INRIA
DATASET
AND
THE
MORE
CHALLENGING
PASCAL
DATASET
FEATURE
EXTRACTION
AND
MODELING
PARTS
NUMEROUS
LOW
LEVEL
FEATURES
AND
REPRESENTATIONS
HAVE
BEEN
PROPOSED
FOR
MODELING
OBJECTS
AND
THEIR
PARTS
REP
RESENTATIONS
HAVE
PROGRESSED
FROM
MODELING
TEXTURES
TO
HISTOGRAMS
OF
GRADIENTS
WITH
GLOBAL
NORMALIZATION
AND
LOCAL
NORMALIZATION
THE
WORK
OF
FELZENSZWALB
ET
AL
IMPROVED
UPON
TO
REDUCE
ITS
DIMENSIONALITY
AND
INCREASE
ACCURACY
METHODS
USING
COLOR
AND
GRADIENTS
WITHOUT
HISTOGRAMS
HAVE
ALSO
BEEN
PROPOSED
WAVELET
APPROACHES
HAVE
SHOWN
BENEFITS
IN
COMPUTATIONAL
EFFICIENCY
SEVERAL
METHODS
COMBINE
VARIOUS
FEATURES
USING
DECISION
TREES
OR
BOOSTING
TECHNIQUES
REPRESEN
TATIONS
MAY
ALSO
BE
LEARNED
USING
RANDOM
DECISION
FORESTS
FEATURE
MINING
DEEP
BELIEF
NETS
MIXTURE
MOD
ELS
OR
BIOLOGICALLY
INSPIRED
MODELS
IN
THIS
PAPER
WE
USE
THE
PART
DETECTORS
OF
FELZENSZWALB
ET
AL
TRAINED
VIA
A
LATENT
SVM
ON
HISTOGRAM
OF
ORIENTED
GRADIENT
FEATURES
THE
MODELS
WERE
PRE
TRAINED
AND
SUP
PLIED
BY
FELZENZSWALB
ET
AL
EACH
COMPONENT
OF
THE
MODEL
CONTAINS
A
ROOT
FILTER
AND
SIX
PART
FILTERS
WHILE
PROVIDES
TWO
COMPONENT
MODELS
WE
ONLY
USED
ONE
COM
PONENT
SINCE
SLIGHTLY
BETTER
RESULTS
WERE
ACHIEVED
USING
A
SINGLE
COMPONENT
MODEL
ON
THE
DATASETS
USED
IN
THIS
PAPER
THE
PART
DETECTIONS
WERE
OBTAINED
BY
INDEPENDENTLY
APPLY
ING
THE
PART
FILTERS
SPATIAL
MODEL
THE
SPATIAL
RELATIONSHIP
OF
PARTS
CAN
BE
MODELED
USING
SEVERAL
PREVIOUSLY
PROPOSED
TECHNIQUES
CONSTELLATION
MOD
ELS
USE
GAUSSIAN
DISTRIBUTIONS
TO
REPRESENT
THE
RELA
TIVE
POSITIONS
OF
PARTS
MORE
RESTRICTIVE
BUT
COMPUTATIONALLY
EFFICIENT
METHODS
HAVE
BEEN
PROPOSED
USING
TREE
AND
K
FAN
MODELS
TREE
BASED
DEFORMABLE
MODELS
CALLED
PICTORIAL
STRUCTURES
PROVIDE
BOTH
EFFICIENT
DETECTION
AND
LEARNING
THE
APPEARANCE
OF
OBJECTS
MAY
ALSO
BE
REPRESENTED
USING
MULTIPLE
TEMPLATES
ASPECT
GRAPHS
OR
BY
LINKING
PARTS
FROM
DIFFERENT
VIEWPOINTS
WE
USE
A
STAR
GRAPH
SPATIAL
MODEL
SIMILAR
TO
FELZEN
SZWALB
ET
AL
THE
MODEL
ASSUMES
THAT
THE
LOCATION
OF
THE
PARTS
ARE
INDEPENDENT
GIVEN
THE
LOCATION
OF
THE
PERSON
THE
LOCATIONS
OF
THE
PARTS
RELATIVE
TO
THE
PERSON
ARE
MODELED
VIA
A
GAUSSIAN
DISTRIBUTION
WITH
MEAN
AND
CO
VARIANCE
PA
RAMETERS
ΜI
ΣI
FOR
THE
ITH
PART
ALL
CO
ORDINATES
ARE
NOR
MALIZED
WITH
RESPECT
TO
THE
HYPOTHESIZED
SIZE
OF
THE
PERSON
EACH
PERSON
CANDIDATE
WINDOW
IS
SCORED
AS
MAX
SI
X
Y
OVERLAPPED
WITH
A
HIGHER
SCORING
WINDOW
OVERLAP
IS
COM
PUTED
AS
THE
RATIO
OF
THE
INTERSECTION
AND
UNION
OF
THE
TWO
WINDOWS
WE
USED
AN
OVERLAP
THRESHOLD
OF
EXPERIMENTAL
SETUPS
FOR
HUMAN
STUDIES
OUR
EXPERIMENTS
INVOLVE
REPLACING
VARIOUS
COMPONENTS
WHERE
K
IS
THE
NUMBER
OF
PARTS
IN
THE
MODEL
AND
SI
IS
THE
SCORE
ASSOCIATED
WITH
PART
I
AT
LOCATION
X
Y
SI
X
Y
SˆI
X
Y
AI
X
BI
Y
CI
X
DI
Y
WHERE
SˆI
X
Y
IS
THE
SCORE
OF
THE
ITHPART
DETECTOR
AT
LOCA
TION
X
Y
AND
X
AND
Y
ARE
THE
POSITIONAL
OFFSETS
FROM
THE
PART
MEAN
POSITION
THE
COEFFICIENTS
AI
BI
CI
DI
WHICH
MODEL
THE
COVARIANCE
ARE
LEARNT
DISCRIMINATIVELY
VIA
A
LINEAR
SVM
TO
DISTINGUISH
POSITIVE
WINDOWS
ACROSS
THE
TRAINING
DATASET
WITH
OVERLAP
WITH
A
GROUND
TRUTH
PERSON
BOUNDING
BOX
FROM
THE
NEGATIVE
WINDOWS
WITH
OVERLAP
WITH
A
GROUND
TRUTH
PERSON
BOUNDING
BOX
THE
MEAN
PARAMETERS
OF
THE
STAR
GRAPH
ΜI
ARE
LEARNT
THROUGH
MAXIMUM
LIKELIHOOD
ESTIMATION
OVER
THE
POSITIVE
TRAINING
WINDOWS
USING
PART
DETECTIONS
THAT
MAXIMIZE
SI
X
Y
WITH
THE
NEWLY
ESTIMATED
MEAN
PARAMETERS
A
NEW
SET
OF
COVARI
ANCE
COEFFICIENTS
AI
BI
CI
DI
ARE
LEARNT
RESULTING
IN
AN
ITERATIVE
LEARNING
PROCEDURE
WE
INITIALIZE
ΜI
AS
THE
WEIGHTED
MEAN
OF
THE
PART
DETEC
TIONS
WITHIN
THE
GROUND
TRUTH
PERSON
BOUNDING
BOXES
IN
THE
TRAINING
IMAGES
THE
WEIGHTS
CORRESPOND
TO
THE
PART
DETEC
TION
SCORES
THE
COEFFICIENTS
AI
BI
CI
DI
ARE
INITIALIZED
TO
CONTEXT
AND
NON
MAXIMAL
SUPPRESSION
RECENTLY
THE
USE
OF
CONTEXT
HAS
RECEIVED
SIGNIFICANT
AT
TENTION
FOR
OBJECT
RECOGNITION
AND
DETECTION
CONTEXT
PRO
VIDES
A
USEFUL
AID
FOR
DETERMINING
LIKELY
POSITIONS
OF
OBJECTS
USING
SCENE
INFORMATION
OR
THE
LOCATION
OF
OTHER
OBJECTS
PAIRWISE
INTERACTIONS
OF
OBJECTS
CAN
BE
MODELED
USING
CRFS
OR
AS
A
MAX
MARGIN
LEARNING
PROBLEM
THE
RELATED
PROBLEM
OF
NON
MAXIMAL
SUPPRESSION
THIS
SECTION
WE
DESCRIBE
THE
TECHNIQUES
WE
EMPLOY
THE
GREEN
BARS
IN
FIGURE
ILLUSTRATE
THE
VARIOUS
HUMAN
STUDIES
WE
PERFORMED
FOR
HUMAN
TESTING
WE
BROKE
THE
PIPELINE
INTO
FOUR
STAGES
FEATURE
EXTRACTION
PART
DETECTION
SPATIAL
MODELING
AND
NMS
CONTEXT
THERE
ARE
POSSIBLE
COMBI
NATIONS
OF
CONTIGUOUS
STAGES
THAT
THE
HUMAN
COULD
PERFORM
OF
WHICH
WE
TEST
WE
DO
NOT
PERFORM
THE
FEATURE
EXTRAC
TION
STAGE
ALONE
SINCE
WE
CANNOT
GET
DIRECT
ACCESS
TO
THE
FEATURES
EXTRACTED
BY
THE
HUMAN
BRAIN
FOR
FURTHER
PROCESS
ING
WITH
A
MACHINE
IN
ADDITION
WE
DO
NOT
PERFORM
THE
NMS
CONTEXT
STAGE
ALONE
ALL
OUR
HUMAN
STUDIES
WERE
PER
FORMED
ON
AMAZON
MECHANICAL
TURK
OUR
EXPERIMENTS
WERE
CONDUCTED
ON
INRIA
AND
PASCAL
IMAGES
CONTAINING
AND
PEOPLE
RE
SPECTIVELY
WE
HAND
LABELED
ALL
THE
FACES
IN
THE
IMAGES
AND
RE
SCALED
THE
IMAGES
SO
THAT
THE
FACES
WERE
A
CANONICAL
SIZE
FIXING
THE
SCALE
REDUCES
OUR
SEARCH
SPACE
MAKING
OUR
HUMAN
STUDIES
FEASIBLE
THE
MACHINE
IMPLEMENTATION
WITH
FIXED
SCALE
GAVE
AN
AVERAGE
PRECISION
AP
OF
FOR
OUR
INRIA
IMAGES
AND
FOR
OUR
PASCAL
IMAGES
FEATURE
EXTRACTION
F
GIVEN
A
NATURAL
IMAGE
HUMAN
SUBJECTS
MAY
EXTRACT
ANY
LOW
LEVEL
FEATURES
NECESSARY
FOR
RECOGNITION
HOWEVER
IF
WE
PRE
PROCESS
IMAGES
TO
RETAIN
ONLY
SOME
OF
THE
INFORMA
TION
WE
CAN
CONSTRAIN
THE
LOW
LEVEL
FEATURES
ACCESSIBLE
TO
THE
HUMAN
SUBJECTS
IN
OUR
EXPERIMENTS
WE
SHOW
SUBJECTS
GREY
SCALE
IMAGES
NORMALIZED
GRADIENT
IMAGES
AND
COLORED
IMAGES
AT
BOTH
HIGH
AND
LOW
RESOLUTIONS
A
NORMALIZED
GRA
DIENT
Gˆ
X
Y
AT
PIXEL
X
Y
WITH
GRADIENT
G
X
Y
IS
COM
PUTED
AS
FOLLOWS
NMS
ATTEMPTS
TO
REMOVE
REDUNDANT
DETECTIONS
OF
THE
SAME
OBJECT
THIS
CAN
BE
VIEWED
AS
CONTEXTUAL
INFORMATION
SHARED
BETWEEN
OBJECTS
OF
THE
SAME
CLASS
I
E
TWO
OF
THE
SAME
OB
G
X
Y
Gˆ
X
Y
G
X
Y
JECT
CANNOT
TYPICALLY
OCCUPY
OVERLAPPING
AREAS
OF
THE
IM
AGE
IN
FACT
SOME
APPROACHES
INHERENTLY
SOLVE
NMS
IN
THEIR
MULTI
OBJECT
CONTEXTUAL
MODELS
IN
THIS
PAPER
WE
ONLY
USE
NMS
AND
NOT
MORE
COMPLEX
CONTEXTUAL
MODELS
AS
THE
PERFORMANCE
GAINS
PROVIDED
BY
THE
COMPLEX
MODELS
WERE
MINIMAL
ON
THE
PASCAL
DATASET
WE
PERFORMED
NMS
BY
REMOVING
WINDOWS
THAT
WHERE
G
X
Y
IS
A
GAUSSIAN
WEIGHTED
AVERAGE
WITH
A
STAN
DARD
DEVIATION
OF
AND
IS
USED
TO
ENSURE
G
X
Y
IS
ABOVE
THE
LEVEL
OF
NOISE
FOR
VISIBILITY
THE
MAXIMUM
NOR
MALIZED
GRADIENT
WITHIN
A
PATCH
IS
SCALED
TO
SEE
FIGURES
AND
FOR
EXAMPLES
FIGURE
ILLUSTRATES
THE
SETTINGS
WHERE
HUMAN
SUBJECTS
USE
THEIR
INTERNAL
FEATURE
EXTRACTOR
A
B
E
OR
ARE
CONSTRAINED
BY
MACHINE
EXTRACTED
FEATURES
C
D
F
HUMAN
MACHINE
COLOR
HIGH
RES
GRAYSCALE
HIGH
RES
NORM
GRAD
HIGH
RES
COLOR
LOW
RES
GRAYSCALE
LOW
RES
NORM
GRAD
LOW
RES
FIGURE
PART
DETECTION
VISUALIZATIONS
CREATED
FOR
HUMAN
AND
MACHINE
DETECTED
PARTS
HP
VE
WINDOWS
B
HP
VE
WINDOWS
FIGURE
EXAMPLE
PATCHES
CLASSIFIED
BY
HUMANS
AS
LEFT
TO
RIGHT
HEAD
TORSO
LEGS
AND
BACKGROUND
IN
TOP
TO
BOTTOM
REGULAR
GREY
SCALE
LOW
RESO
LUTION
AND
NORMALIZED
GRADIENT
IMAGES
PART
DETECTOR
P
SIMILAR
TO
THE
MACHINE
PART
DETECTOR
OUR
HUMAN
STUDIES
USE
A
SLIDING
WINDOW
APPROACH
OVERLAPPING
SMALL
PATCHES
ARE
EXTRACTED
FROM
THE
IMAGES
FIGURE
E
F
HUMAN
SUB
JECTS
WERE
RANDOMLY
SHOWN
THESE
PATCHES
ACROSS
ALL
IMAGES
SO
NO
CONTEXTUAL
INFORMATION
WAS
AVAILABLE
SUBJECTS
WERE
ASKED
TO
CLASSIFY
EACH
PATCH
AS
CONTAINING
A
HEAD
TORSO
ARM
HAND
LEG
FOOT
ANY
OTHER
PART
OF
A
PERSON
OR
NOT
A
PERSON
AT
ALL
EACH
PATCH
WAS
CLASSIFIED
BY
SUBJECTS
EX
AMPLE
PATCHES
SHOWN
TO
HUMANS
USING
COLOR
GREY
SCALE
AND
NORMALIZED
GRADIENT
IMAGES
ARE
SHOWN
IN
FIGURE
VISU
ALIZATIONS
OF
THE
DETECTED
PARTS
AGGREGATED
ACROSS
SUBJECTS
ARE
SHOWN
IN
FIGURES
AND
THE
DIFFERENT
COLORS
COR
RESPOND
TO
DIFFERENT
PARTS
RED
HEAD
BLUE
TORSO
GREEN
ARM
YELLOW
HAND
MAGENTA
LEG
AND
CYAN
FEET
THE
INTENSITY
OF
THE
COLOR
CORRESPONDS
TO
THE
NUMBER
OF
SUBJECTS
THAT
CLASSI
FIED
THE
LOCAL
PATCH
AS
THE
CORRESPONDING
PART
ANALOGOUS
TO
THE
DETECTOR
OF
FELZENSZWALB
ET
AL
WE
ALSO
DETECT
ROOTS
IN
A
SIMILAR
SLIDING
WINDOW
FASHION
WHICH
ARE
LOW
RESOLUTION
TEMPLATES
OF
A
PERSON
SHOWN
IN
WHITE
IN
FIG
URE
WE
USED
ROOT
AND
PART
SIZES
SIMILAR
TO
THE
MACHINE
IM
PLEMENTATION
SPECIFICALLY
FOR
INRIA
THE
PART
PATCH
SIZES
EXTRACTED
FROM
IMAGES
WERE
PIXELS
AND
THE
ROOT
WIN
DOW
SIZES
WERE
FOR
PASCAL
THE
PART
SIZES
WERE
AND
THE
ROOT
SIZES
WERE
IN
FELZEN
SZWALB
ET
AL
THE
SPATIAL
RESOLUTION
OF
THE
ROOT
IS
LOWER
THAN
THAT
OF
THE
OTHER
PARTS
SIMILARLY
WE
DOWNSAMPLE
THE
ROOT
WINDOWS
LEADING
TO
AN
EFFECTIVE
RESOLUTION
OF
ROOT
DETECTIONS
ARE
NOT
SHOWN
IN
FIGURE
FOR
SIMPLICITY
C
MP
VE
WINDOWS
D
MP
VE
WINDOWS
FIGURE
TRAINING
DATA
WITH
POSITIVE
VE
WINDOWS
CONTAINING
A
PERSON
LEFT
AND
NEGATIVE
VE
WINDOWS
NOT
CONTAINING
A
PERSON
RIGHT
SHOWN
TO
HUMAN
SUBJECTS
FOR
LEARNING
A
SPATIAL
MODEL
WINDOWS
ARE
CROPPED
FROM
PART
VISUALIZATIONS
FOR
TOP
HUMAN
DETECTED
PARTS
HP
AND
BOTTOM
MA
CHINE
DETECTED
PARTS
MP
IN
GRAY
SCALE
PASCAL
IMAGES
FOR
INRIA
AND
FOR
PASCAL
FOR
LOW
RESOLUTION
PART
DETECTION
IN
BOTH
INRIA
AND
PASCAL
THE
RESOLUTION
OF
THE
PARTS
WAS
REDUCED
TO
AND
THE
ROOTS
WERE
SCALED
TO
PIXELS
IN
THE
LARGEST
DIMENSION
FOR
EASY
VIEWING
THE
PARTS
AND
ROOTS
WERE
DISPLAYED
TO
SUBJECTS
WITH
THE
LARGEST
DIMENSION
SCALED
TO
PIXELS
THE
PART
PATCHES
WERE
SAM
PLED
WITH
OVERLAP
BETWEEN
CONSECUTIVE
PATCHES
AND
THE
ROOT
WINDOWS
WERE
SAMPLED
AT
OVERLAP
SPATIAL
MODEL
SM
TO
STUDY
THE
ABILITY
OF
THE
HUMAN
SUBJECTS
TO
REASON
ABOUT
SPATIAL
RELATIONSHIPS
WE
TRAIN
THE
SUBJECTS
USING
THE
COLORED
PART
VISUALIZATIONS
AS
SHOWN
IN
FIGURE
SUB
JECTS
ARE
THEN
ASKED
TO
CLASSIFY
WINDOWS
USING
THE
SAME
PART
VISUALIZATIONS
AS
CONTAINING
A
PERSON
OR
NOT
SEE
FIG
URE
H
THE
SET
OF
WINDOWS
ARE
OVERLAPPING
AND
RAN
DOMLY
SAMPLED
SUBJECTS
CLASSIFIED
EACH
WINDOW
A
CON
FIDENCE
SCORE
WAS
COMPUTED
AS
THE
AVERAGE
NUMBER
OF
SUB
JECTS
CLASSIFYING
A
WINDOW
AS
CONTAINING
A
PERSON
STANDARD
NON
MAXIMAL
SUPPRESSION
CAN
BE
PERFORMED
BY
A
MACHINE
USING
THE
CONFIDENCE
SCORES
ON
ALL
WINDOWS
IN
AN
IMAGE
FI
NALLY
A
PRECISION
RECALL
CURVE
IS
COMPUTED
TO
QUANTIFY
THE
HUMAN
SUBJECTS
PERFORMANCE
WE
NOTE
THAT
SIMILAR
PART
DETECTION
VISUALIZATIONS
CAN
BE
CREATED
FOR
BOTH
HUMAN
AND
MACHINE
DETECTED
PARTS
AS
SHOWN
IN
FIGURE
THIS
ALLOWS
VISUALIZE
THE
PART
DETECTIONS
OF
FELZENSZWALB
ET
AL
WHICH
CON
TAIN
HIGHLY
OVERLAPPING
DETECTIONS
WE
PERFORM
NON
MAXIMAL
SUPPRESSION
AMONG
THE
PARTS
EACH
PART
IS
MAPPED
TO
A
COLOR
WITH
INTENSITY
CORRE
SPONDING
TO
THE
ESTIMATED
LIKELIHOOD
OF
A
PERSON
GIVEN
THE
PART
SCORE
FUR
THER
EVALUATION
INDICATES
THAT
OUR
NMS
PROCESSING
OF
THE
PARTS
ACTUALLY
INCREASES
THE
MACHINE
AP
FOR
INRIA
BY
AND
FOR
PASCAL
BY
A
B
C
D
E
F
G
MACHINE
HUMAN
HUMAN
PERSON
DETECTOR
INRIA
PASCAL
HUMAN
PART
DETECTOR
INRIA
PASCAL
H
I
J
K
FIGURE
SUMMARY
OF
ALL
EXPERIMENTS
PERFORMED
TO
TEST
THE
USE
OF
HU
MANS
AND
MACHINES
TO
PERFORM
VARIOUS
COMBINATIONS
OF
THE
COMPONENTS
IN
A
PARTS
BASED
PERSON
DETECTOR
US
TO
EVALUATE
THE
HUMAN
SPATIAL
MODEL
ON
MACHINE
OR
HU
MAN
DETECTED
PARTS
INVERSELY
THE
HUMAN
DETECTED
PARTS
CAN
BE
FED
INTO
A
MACHINE
SPATIAL
MODEL
ONE
VARIATION
OF
THE
ABOVE
EXPERIMENTS
USED
FOR
FURTHER
ANALYSIS
IS
TO
SHOW
SUBJECTS
WINDOWS
EXTRACTED
FROM
NATURAL
IMAGES
INSTEAD
OF
PART
VISUALIZATIONS
FIGURE
B
THIS
IS
EQUIVALENT
TO
USING
HUMAN
EXTRACTED
FEATURES
PARTS
AS
WELL
AS
THE
HUMAN
SPATIAL
MODEL
WE
MAY
ALSO
RESTRICT
THE
FEA
TURES
AVAILABLE
TO
THE
HUMAN
SUBJECTS
BY
PRE
PROCESSING
THE
IMAGES
FIGURE
D
WHILE
STILL
USING
HUMAN
DETECTED
PARTS
AND
SPATIAL
MODELS
CONTEXT
AND
NON
MAXIMAL
SUPPRESSION
NMS
IN
THE
HUMAN
STUDIES
ON
SPATIAL
MODELING
WE
ASKED
THE
SUBJECTS
TO
CLASSIFY
CROPPED
WINDOWS
EXTRACTED
FROM
THE
IM
AGES
WITHOUT
CONTEXT
WE
CAN
STUDY
NMS
AND
CONTEXTUAL
REASONING
BY
SHOWING
THE
SUBJECTS
INFORMATION
OVER
THE
EN
TIRE
IMAGE
AND
ASKING
THEM
TO
DRAW
BOUNDING
BOXES
AROUND
DETECTED
PERSONS
FIGURE
A
C
G
BY
PERFORMING
THIS
TASK
SUBJECTS
ARE
IMPLICITLY
PERFORMING
NMS
AND
CAN
USE
CON
TEXTUAL
INFORMATION
IF
PROVIDED
AS
SHOWN
IN
FIGURE
THE
INFORMATION
SHOWN
TO
THE
SUBJECTS
IS
OF
THREE
TYPES
A
ORIG
INAL
COLOR
IMAGES
C
IMAGES
AFTER
FEATURE
EXTRACTION
AND
G
PART
DETECTIONS
RESULTS
IN
THIS
SECTION
WE
PROVIDE
THE
RESULTS
OF
NUMEROUS
MA
CHINE
AND
HUMAN
STUDIES
WE
ANALYZE
THE
RESULTS
WITH
RE
SPECT
TO
THE
FOUR
DETECTOR
COMPONENTS
FEATURE
EXTRACTION
PART
DETECTION
SPATIAL
MODELS
AND
CONTEXT
NMS
WE
ALSO
ATTEMPT
TO
QUANTITATIVELY
COMPARE
THE
RELATIVE
PERFORMANCE
GAINS
THAT
MAY
BE
ACHIEVED
BY
IMPROVING
EACH
COMPONENT
OF
THE
DETECTOR
AN
ILLUSTRATION
OF
THE
VARIOUS
COMBINATIONS
OF
HUMAN
AND
MACHINE
EXPERIMENTS
IS
SHOWN
IN
FIGURE
FIGURE
EFFECT
OF
FEATURES
ACCURACY
OF
HUMAN
PERSON
DETECTOR
LEFT
AND
HUMAN
PART
DETECTOR
WITH
MACHINE
SPATIAL
MODEL
AND
NMS
RIGHT
IN
HIGH
H
AND
LOW
L
RESOLUTION
COLOR
C
GREY
SCALE
G
AND
NORMALIZED
GRADIENT
N
IMAGES
EFFECT
OF
FEATURES
WE
CAN
ANALYZE
THE
EFFECT
OF
FEATURE
TYPES
USING
HUMANS
AS
PERSON
AND
PART
SLIDING
WINDOW
CLASSIFIERS
WE
COMPARE
RESULTS
USING
THE
ORIGINAL
IMAGE
FIGURE
B
C
WITH
RESULTS
USING
DIFFERENT
FEATURE
TYPES
FIGURE
E
F
THE
RESULTS
ARE
SUMMARIZED
IN
FIGURE
WE
SEE
THAT
THE
LOSS
OF
COLOR
OR
RESOLUTION
DOES
NOT
SIGNIFICANTLY
AFFECT
DETECTION
ACCURACIES
USING
NORMALIZED
GRADIENTS
ALONE
DID
DEGRADE
HUMAN
PER
FORMANCE
SIGNIFICANTLY
ESPECIALLY
ON
THE
PASCAL
DATASET
EFFECT
OF
PARTS
IN
ORDER
TO
QUANTIFY
THE
EFFECT
OF
BETTER
PART
DETECTORS
WE
COMPARE
THE
PERFORMANCE
OF
SLIDING
WINDOW
DETECTORS
ON
PARTS
DETECTED
BY
HUMANS
AND
MACHINES
WE
REPORT
RE
SULTS
FROM
SHOWING
GREY
SCALE
IMAGE
PATCHES
TO
THE
SUB
JECTS
SINCE
THE
MACHINE
MODELS
DO
NOT
USE
COLOR
SIMILAR
RESULTS
WERE
FOUND
WITH
OTHER
FEATURES
TYPES
THERE
ARE
SEV
ERAL
PAIRS
OF
RESULTS
WE
MAY
CONSIDER
AS
SHOWN
IN
FIGURE
THESE
INCLUDE
USING
THE
MACHINE
SPATIAL
MODEL
WITH
NMS
F
K
THE
HUMAN
SPATIAL
MODEL
AND
MACHINE
NMS
G
H
AND
THE
HUMAN
SPATIAL
MODEL
WITH
HUMAN
NMS
I
J
THE
RESULTS
ARE
SHOWN
IN
FIGURE
THE
USE
OF
HUMAN
PART
DE
TECTIONS
SIGNIFICANTLY
IMPROVES
THE
PERFORMANCE
OF
THE
DE
TECTORS
IN
MOST
CASES
FOR
THE
CHALLENGING
PASCAL
DATASET
THE
IMPROVEMENT
IS
AS
HIGH
AS
FOR
HUMAN
DETECTED
PARTS
OVER
MACHINE
DETECTED
PARTS
EFFECT
OF
SPATIAL
MODELS
WE
EVALUATE
THE
EFFECT
OF
SPATIAL
MODELS
USING
A
SERIES
OF
HUMAN
STUDIES
ON
BOTH
MACHINE
AND
HUMAN
DETECTED
PARTS
AS
SHOWN
IN
FIGURE
WE
COMPARE
MACHINE
TO
HUMAN
SPA
TIAL
MODELS
USING
MACHINE
DETECTED
PARTS
H
K
AND
HUMAN
DETECTED
PARTS
B
C
AND
F
G
THE
RESULTS
ARE
SHOWN
IN
FIGURE
THE
RESULTS
INDICATE
THAT
HUMAN
SPATIAL
MODELS
DO
NOT
SIGNIFICANTLY
OR
CONSISTENTLY
AFFECT
THE
AP
SCORES
ACROSS
THE
VARIOUS
SCENARIOS
INRIA
PASCAL
NMS
CONTEXT
INRIA
NMS
PASCAL
NMS
MACHINE
PARTS
HUMAN
PARTS
HSW
MSW
HD
HSW
MSW
HD
INRIA
PASCAL
HP
MP
HP
MP
FIGURE
EFFECT
OF
DIFFERENT
PART
DETECTORS
ACCURACY
OF
HUMANS
HSW
AND
MACHINES
MSW
SPATIAL
MODELS
FOLLOWED
BY
MACHINE
NMS
AS
WELL
AS
USING
HUMANS
HD
FOR
BOTH
SPATIAL
MODELS
AND
NMS
ON
PART
FIGURE
EFFECT
OF
NMS
CONTEXT
ACCURACY
OF
HUMAN
DETECTORS
USING
LEFT
HIGH
RESOLUTION
GREY
SCALE
IMAGES
AND
USING
TWO
RIGHT
PLOTS
HUMAN
PART
HP
AND
MACHINE
PART
MP
VISUALIZATIONS
VISUALIZATIONS
THE
HUMAN
PARTS
ARE
PARTS
DETECTED
BY
SUBJECTS
ON
GREY
SCALE
IMAGES
INRIA
CH
GH
NH
CL
GL
NL
PASCAL
CH
GH
NH
CL
GL
NL
FIGURE
EFFECT
OF
SPATIAL
MODELS
ACCURACY
OF
HUMAN
AND
MACHINE
SPATIAL
MODELS
ON
HUMAN
PART
VISUALIZATIONS
CH
GH
NH
CL
GL
NL
AND
MACHINE
PART
VISUALIZATIONS
MP
EFFECT
OF
NON
MAXIMAL
SUPPRESSION
AND
CONTEXT
FINALLY
WE
STUDY
THE
INFLUENCE
OF
CONTEXT
AND
NON
MAXIMAL
SUPPRESSION
NMS
ON
DETECTION
PERFORMANCE
TWO
SETS
OF
EXPERIMENTS
FROM
FIGURE
CAN
BE
COMPARED
FIRST
WE
CAN
COMPARE
THE
HUMAN
SUBJECTS
AS
SLIDING
WIN
DOW
CLASSIFIERS
FOLLOWED
BY
MACHINE
NMS
E
TO
THE
HUMAN
SUBJECTS
AS
DETECTORS
USING
THE
ENTIRE
IMAGE
D
THE
RESULTS
ARE
SHOWN
IN
FIGURE
LEFT
FOR
GREY
SCALE
IMAGES
THE
USE
OF
THE
SUBJECTS
CONTEXTUAL
MODELS
AND
NMS
SIGNIFICANTLY
INCREASES
PERFORMANCE
BY
UP
TO
ON
THE
PASCAL
DATASET
OVER
USING
THE
MACHINE
NMS
SECOND
WE
CAN
COMPARE
RESULTS
USING
THE
PART
VISUALIZA
TIONS
WITH
HUMAN
SPATIAL
MODELS
AND
MACHINE
NMS
G
H
TO
HUMAN
SPATIAL
MODELS
AND
HUMAN
NMS
I
J
THE
RESULTS
ARE
SHOWN
IN
FIGURE
RIGHT
SINCE
THE
SUBJECTS
ONLY
HAVE
AC
CESS
TO
PART
DETECTIONS
THEY
ARE
LIMITED
TO
PERFORMING
INTRA
CATEGORY
CONTEXTUAL
REASONING
OR
NMS
AS
DEMONSTRATED
IN
PARIKH
ET
AL
THE
USE
OF
INTER
CATEGORY
CONTEXTUAL
INFOR
MATION
MAY
NOT
BE
NECESSARY
GIVEN
HIGH
RESOLUTION
APPEAR
ANCE
INFORMATION
A
SIMILAR
FINDING
IS
FOUND
BY
COMPAR
ING
THE
EXPERIMENTS
IN
FIGURE
THAT
ALL
USE
HIGH
RESOLUTION
INFORMATION
THE
AMOUNT
OF
IMPROVEMENT
IN
THE
LEFT
PLOT
USING
ALL
CONTEXTUAL
INFORMATION
IS
SIMILAR
TO
THE
RIGHT
TWO
PLOTS
THAT
ONLY
USE
INTRA
CATEGORY
CONTEXTUAL
INFORMATION
FIGURE
COMPARES
THE
ACCURACY
OF
HUMAN
SUBJECTS
AS
PERSON
DETECTORS
A
D
TO
HUMANS
AS
SLIDING
WINDOW
CLASSI
FIGURE
EFFECT
OF
CONTEXT
HUMANS
CAN
RELIABLY
LEVERAGE
CONTEXTUAL
INFORMATION
AND
MAINTAIN
ROBUST
DETECTION
ACCURACY
EVEN
WITH
IMPOVER
ISHED
APPEARANCE
INFORMATION
FIERS
FOLLOWED
BY
MACHINE
NMS
B
E
ACROSS
FEATURE
TYPES
THE
ACCURACY
OF
HUMAN
SUBJECTS
AT
CLASSIFYING
IMAGE
WIN
DOWS
IN
ISOLATION
DECREASES
SIGNIFICANTLY
AS
THE
APPEARANCE
INFORMATION
BECOMES
WEAK
E
G
NORMALIZED
GRADIENT
LOW
RESOLUTION
IMAGES
HOWEVER
THEIR
PERFORMANCE
AT
DETECT
ING
PEOPLE
IN
ENTIRE
IMAGES
REMAINS
QUITE
ROBUST
AS
PREVI
OUSLY
SHOWN
IN
THIS
SIGNIFIES
THE
IMPORTANCE
OF
CONTEX
TUAL
INFORMATION
UNDER
IMPOVERISHED
SCENARIOS
SUMMARY
WE
SUMMARIZE
THE
POTENTIAL
IMPROVEMENTS
IN
OBJECT
DE
TECTION
ACCURACIES
BASED
ON
OUR
HUMAN
AND
MACHINE
STUDIES
IN
FIGURE
WE
SHOW
THE
AVERAGE
IMPROVEMENT
IN
ACCURA
CIES
FOR
PART
DETECTION
SPATIAL
MODELING
AND
CONTEXT
NMS
AND
THEIR
VARIANCES
IN
REFERENCE
TO
FIGURE
RESULTS
ARE
COMPUTED
FROM
F
K
G
H
AND
I
J
FOR
PART
DETECTIONS
B
C
F
G
AND
H
K
FOR
SPATIAL
MODELS
AND
G
I
AND
H
J
FOR
CONTEXT
NMS
WE
FIND
PARTS
TO
HAVE
THE
BIGGEST
IMPACT
FOL
LOWED
BY
NON
MAXIMAL
SUPPRESSION
SPATIAL
MODELS
DO
NOT
HAVE
SIGNIFICANT
IMPACT
THE
PEOPLE
IN
THE
PASCAL
IMAGES
AS
COMPARED
TO
THOSE
IN
THE
INRIA
DATASET
DEMONSTRATE
A
WIDER
VARIATION
IN
POSES
AND
OFTEN
EXHIBIT
CHALLENGING
SCE
NARIOS
SUCH
AS
OCCLUSION
AND
TRUNCATION
THUS
WE
OBSERVE
A
HIGHER
POTENTIAL
FOR
IMPROVEMENT
ON
THE
PASCAL
DATASET
AS
SEEN
IN
FIGURE
WE
NOTE
THAT
THE
ESTIMATES
OF
POTENTIAL
IMPROVEMENTS
CAN
BE
VIEWED
AS
LOWER
BOUNDS
SINCE
OUR
HU
MAN
STUDIES
WERE
PERFORMED
ON
AMAZON
MECHANICAL
TURK
WHERE
SUBJECTS
MAY
BE
DISTRACTED
AND
OFTEN
PROVIDE
NOISY
HUMAN
DETECTOR
B
HUMAN
SLIDING
WINDOW
C
MACHINE
SPATIAL
MODEL
FIGURE
EXAMPLE
FAILURE
CASES
FOR
SCENARIOS
WITH
DIFFERENT
AMOUNTS
OF
HUMAN
INVOLVEMENT
CORRECT
DETECTIONS
ARE
SHOWN
IN
WHITE
FALSE
POSITIVES
IN
RED
AND
FALSE
NEGATIVES
IN
YELLOW
A
EVEN
WHEN
SUBJECTS
ARE
SHOWN
THE
ENTIRE
IMAGE
HIGHLY
OCCLUDED
PEOPLE
IN
BAD
LIGHTING
ARE
MISSED
B
WHEN
SUBJECTS
CLASSIFY
WINDOWS
IN
ISOLATION
FROM
THE
REST
OF
THE
IMAGE
AS
CONTAINING
A
PERSON
OR
NOT
LACK
OF
CONTEXT
LEADS
TO
FALSE
POSITIVES
WHEN
THE
WINDOWS
LOCALLY
APPEAR
TO
HAVE
PARTS
OF
A
PERSON
C
A
MACHINE
SPATIAL
MODEL
APPLIED
TO
NEAR
PERFECT
HUMAN
PART
DETECTIONS
FAILS
BECAUSE
OF
SYMMETRIC
PART
DETECTIONS
SUBJECTS
WERE
ASKED
TO
CLASSIFY
PATCHES
AS
CONTAINING
ARMS
LEGS
ETC
AND
WERE
NOT
ASKED
TO
DISTINGUISH
BETWEEN
LEFT
RIGHT
ARMS
LEGS
ETC
PARTS
SPATIAL
MODELS
NMS
INRIA
PASCAL
BE
LEARNT
JOINTLY
WITH
SPATIAL
MODELS
AS
IN
MOREOVER
THE
WEAKER
THE
PART
MODELS
THE
BIGGER
ROLE
SPATIAL
MODELS
COULD
PLAY
IN
THE
FINAL
DETECTION
PERFORMANCE
OUR
ANALY
SIS
DOES
NOT
ACCOUNT
FOR
SUCH
DEPENDENCIES
AMONG
VARIOUS
COMPONENTS
IN
THE
PIPELINE
IN
OUR
HUMAN
STUDIES
SUBJECTS
WERE
INSTRUCTED
TO
FIND
PARTS
WITH
SEMANTIC
MEANING
HEADS
TORSO
ETC
SINCE
THE
PATCHES
WERE
PRESENTED
IN
ISOLATION
WE
DO
NOT
EXPECT
THIS
FIGURE
SUMMARY
OF
OUR
RESULTS
THE
COMPONENT
OF
A
PARTS
BASED
PERSON
DETECTOR
THAT
CAN
IMPROVE
DETECTION
PERFORMANCE
THE
MOST
IS
THE
PART
DETECTION
FOLLOWED
BY
THE
NMS
COMPONENT
SPATIAL
MODELS
DO
NOT
AFFECT
THE
RESULTANT
PERFORMANCE
SIGNIFICANTLY
A
MACHINE
PART
DETECTIONS
B
RESULTANT
DETECTIONS
C
HUMAN
PART
DETECTIONS
D
RESULTANT
DETECTIONS
FIGURE
EXAMPLE
DETECTIONS
WHERE
HUMAN
DETECTED
PARTS
ALLOW
FOR
SUCCESSFUL
DETECTION
WHITE
WHILE
MACHINE
DETECTED
PARTS
LEAD
TO
FALSE
POSITIVE
RED
AND
FALSE
NEGATIVE
YELLOW
DETECTIONS
RESPONSES
EXAMPLE
FAILURE
CASES
FOR
BOTH
HUMAN
SUBJECTS
AND
MACHINES
ARE
SHOWN
IN
FIGURES
AND
DISCUSSION
OUR
ANALYSIS
IS
RESTRICTED
TO
SLIDING
WINDOW
PARTS
BASED
MODELS
IT
ASSUMES
A
PIPELINE
WHERE
THE
PARTS
AND
SPATIAL
MODELS
ARE
CONSIDERED
TO
BE
INDEPENDENT
PART
MODELS
COULD
SEMANTIC
KNOWLEDGE
TO
PROVIDE
CONTEXTUAL
INFORMATION
TO
SUBJECTS
HOWEVER
MACHINE
OBJECT
DETECTORS
HAVE
THE
FREE
DOM
TO
MODEL
PARTS
WITHOUT
SEMANTIC
MEANINGS
THIS
FLEX
IBILITY
MAY
ALLOW
FOR
THE
USE
OF
BETTER
PARTS
BUT
COULD
ALSO
MAKE
THE
UNDERLYING
LEARNING
PROBLEM
INTRACTABLE
THE
ACCURACIES
OF
HUMAN
SUBJECTS
AS
PERSON
DETECTORS
ON
COLOR
AND
GREY
SCALE
IMAGES
IS
HIGHER
THAN
ANY
EXPERIMENT
USING
A
COMBINATION
OF
MACHINE
AND
HUMAN
COMPONENTS
THIS
IMPLIES
THAT
THE
PIPELINE
PROPOSED
FOR
THE
MACHINE
DE
TECTOR
MAY
NOT
BE
THE
SAME
AS
THE
HUMAN
SUBJECTS
IN
CONCLUSION
WE
PRESENTED
NUMEROUS
STUDIES
COMBIN
ING
BOTH
MACHINE
AND
HUMAN
COMPONENTS
FOR
DETECTING
PEO
PLE
BY
ANALYZING
THEIR
RELATIVE
PERFORMANCE
WE
CAN
DETER
MINE
WHICH
COMPONENTS
COULD
OFFER
THE
GREATEST
BOOST
IN
OVERALL
PERFORMANCE
IF
IMPROVED
OUR
RESULTS
SHOW
THAT
PART
DETECTION
IS
THE
WEAKEST
LINK
ON
CHALLENGING
DATASETS
SUCH
AS
PASCAL
FOLLOWED
BY
NON
MAXIMAL
SUPPRESSION
AND
CON
TEXT
HUMAN
SPATIAL
MODELS
APPEAR
TO
OFFER
NEGLIGIBLE
PER
FORMANCE
INCREASE
OVER
MACHINE
SPATIAL
MODELS
GREY
SCALE
INFORMATION
PROVIDED
THE
SAME
LEVEL
OF
ACCURACY
AS
COLOR
HOWEVER
ACCURACIES
SUFFERED
WHEN
USING
ONLY
NORMALIZED
GRADIENTS
FUTURE
WORK
INVOLVES
SIMILAR
ANALYSIS
FOR
DE
TECTING
GENERIC
OBJECT
CATEGORIES
AND
OTHER
OBJECT
DETECTION
MODELS
PEEKABOOM
A
GAME
FOR
LOCATING
OBJECTS
IN
IMAGES
LUIS
VON
AHN
RUORAN
LIU
AND
MANUEL
BLUM
COMPUTER
SCIENCE
DEPARTMENT
CARNEGIE
MELLON
UNIVERSITY
FORBES
AVENUE
PITTSBURGH
PA
BIGLOU
ROYLIU
MBLUM
CS
CMU
EDU
ABSTRACT
WE
INTRODUCE
PEEKABOOM
AN
ENTERTAINING
WEB
BASED
GAME
THAT
CAN
HELP
COMPUTERS
LOCATE
OBJECTS
IN
IMAGES
PEOPLE
PLAY
THE
GAME
BECAUSE
OF
ITS
ENTERTAINMENT
VALUE
AND
AS
A
SIDE
EFFECT
OF
THEM
PLAYING
WE
COLLECT
VALUABLE
IMAGE
METADATA
SUCH
AS
WHICH
PIXELS
BELONG
TO
WHICH
OBJECT
IN
THE
IMAGE
THE
COLLECTED
DATA
COULD
BE
APPLIED
TOWARDS
CONSTRUCTING
MORE
ACCURATE
COMPUTER
VISION
ALGORITHMS
WHICH
REQUIRE
MASSIVE
AMOUNTS
OF
TRAINING
AND
TESTING
DATA
NOT
CURRENTLY
AVAILABLE
PEEKABOOM
HAS
BEEN
PLAYED
BY
THOUSANDS
OF
PEOPLE
SOME
OF
WHOM
HAVE
SPENT
OVER
HOURS
A
DAY
PLAYING
AND
THUS
FAR
HAS
GENERATED
MILLIONS
OF
DATA
POINTS
IN
ADDITION
TO
ITS
PURELY
UTILITARIAN
ASPECT
PEEKABOOM
IS
AN
EXAMPLE
OF
A
NEW
EMERGING
CLASS
OF
GAMES
WHICH
NOT
ONLY
BRING
PEOPLE
TOGETHER
FOR
LEISURE
PURPOSES
BUT
ALSO
EXIST
TO
IMPROVE
ARTIFICIAL
INTELLIGENCE
SUCH
GAMES
APPEAL
TO
A
GENERAL
AUDIENCE
WHILE
PROVIDING
ANSWERS
TO
PROBLEMS
THAT
COMPUTERS
CANNOT
YET
SOLVE
AUTHOR
KEYWORDS
DISTRIBUTED
KNOWLEDGE
ACQUISITION
OBJECT
SEGMENTATION
OBJECT
RECOGNITION
COMPUTER
VISION
WEB
BASED
GAMES
ACM
CLASSIFICATION
KEYWORDS
LEARNING
KNOWLEDGE
ACQUISITION
H
HCI
WEB
BASED
INTERACTION
INTRODUCTION
HUMANS
UNDERSTAND
AND
ANALYZE
EVERYDAY
IMAGES
WITH
LITTLE
EFFORT
WHAT
OBJECTS
ARE
IN
THE
IMAGE
WHERE
THEY
ARE
LOCATED
WHAT
IS
THE
BACKGROUND
WHAT
IS
THE
FOREGROUND
ETC
COMPUTERS
ON
THE
OTHER
HAND
STILL
HAVE
TROUBLE
WITH
SUCH
BASIC
VISUAL
TASKS
AS
READING
DISTORTED
TEXT
OR
FINDING
WHERE
IN
THE
IMAGE
A
SIMPLE
OBJECT
IS
LOCATED
ALTHOUGH
RESEARCHERS
HAVE
PROPOSED
AND
TESTED
MANY
IMPRESSIVE
ALGORITHMS
FOR
COMPUTER
VISION
NONE
HAVE
BEEN
MADE
TO
WORK
RELIABLY
AND
GENERALLY
MOST
OF
THE
BEST
APPROACHES
FOR
COMPUTER
VISION
E
G
RELY
ON
MACHINE
LEARNING
TRAIN
AN
ALGORITHM
TO
PERFORM
A
VISUAL
TASK
BY
SHOWING
IT
EXAMPLE
IMAGES
IN
WHICH
THE
TASK
HAS
ALREADY
BEEN
PERFORMED
FOR
EXAMPLE
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
OR
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
CHI
APRIL
MONTRÉAL
QUÉBEC
CANADA
COPYRIGHT
ACM
TRAINING
AN
ALGORITHM
FOR
TESTING
WHETHER
AN
IMAGE
CONTAINS
A
DOG
WOULD
INVOLVE
PRESENTING
IT
WITH
MULTIPLE
IMAGES
OF
DOGS
EACH
ANNOTATED
WITH
THE
PRECISE
LOCATION
OF
THE
DOG
IN
THE
IMAGE
AFTER
PROCESSING
ENOUGH
IMAGES
THE
ALGORITHM
LEARNS
TO
FIND
DOGS
IN
ARBITRARY
IMAGES
A
MAJOR
PROBLEM
WITH
THIS
APPROACH
HOWEVER
IS
THE
LACK
OF
TRAINING
DATA
WHICH
OBVIOUSLY
MUST
BE
PREPARED
BY
HAND
DATABASES
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
CURRENTLY
HAVE
HUNDREDS
OR
AT
BEST
A
FEW
THOUSAND
IMAGES
ORDERS
OF
MAGNITUDE
LESS
THAN
WHAT
IS
REQUIRED
IN
THIS
PAPER
WE
ADDRESS
THE
PROBLEM
OF
CONSTRUCTING
A
MASSIVELY
LARGE
DATABASE
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
THE
TARGET
DATABASE
WILL
CONTAIN
MILLIONS
OF
IMAGES
ALL
FULLY
ANNOTATED
WITH
INFORMATION
ABOUT
WHAT
OBJECTS
ARE
IN
THE
IMAGE
WHERE
EACH
OBJECT
IS
LOCATED
AND
HOW
MUCH
OF
THE
IMAGE
IS
NECESSARY
TO
RECOGNIZE
IT
OUR
DATABASE
WILL
BE
SIMILAR
TO
THOSE
PREVIOUSLY
SHOWN
TO
BE
USEFUL
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
E
G
TO
CONSTRUCT
SUCH
A
DATABASE
WE
FOLLOW
THE
APPROACH
TAKEN
BY
THE
ESP
GAME
AND
INTRODUCE
A
NEW
GAME
CALLED
PEEKABOOM
PEEKABOOM
IS
AN
EXTREMELY
ENJOYABLE
NETWORKED
GAME
IN
WHICH
SIMPLY
BY
PLAYING
PEOPLE
HELP
CONSTRUCT
A
DATABASE
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
WE
GUARANTEE
THE
DATABASE
CORRECTNESS
EVEN
IF
THE
PEOPLE
PLAYING
THE
GAME
DON
T
INTEND
IT
AS
WE
WILL
SHOW
IN
THIS
PAPER
OUR
GAME
IS
ALSO
VERY
ENJOYABLE
WITH
SOME
PEOPLE
HAVING
PLAYED
OVER
HOURS
A
WEEK
WE
WILL
FURTHER
SHOW
THAT
THIS
GAME
CAN
BE
USED
TO
IMPROVE
IMAGE
SEARCH
RESULTS
AND
TO
CALCULATE
OBJECT
BOUNDING
BOXES
SIMILAR
TO
THOSE
IN
FLICKR
SEE
FIGURE
THE
ESP
GAME
IS
AN
INTERACTIVE
SYSTEM
THAT
ALLOWS
PEOPLE
TO
LABEL
IMAGES
WHILE
HAVING
FUN
THE
ESP
GAME
COLLECTS
RANDOM
IMAGES
FROM
THE
WEB
AND
OUTPUTS
WORD
LABELS
DESCRIBING
THE
CONTENTS
OF
THE
IMAGES
THE
GAME
HAS
ALREADY
COLLECTED
MILLIONS
OF
LABELS
FOR
ARBITRARY
IMAGES
GIVEN
AN
IMAGE
THE
ESP
GAME
CAN
BE
USED
TO
DETERMINE
WHAT
OBJECTS
ARE
IN
THE
IMAGE
BUT
CANNOT
BE
USED
TO
DETERMINE
WHERE
IN
THE
IMAGE
EACH
OBJECT
IS
LOCATED
SUCH
LOCATION
INFORMATION
IS
NECESSARY
FOR
TRAINING
AND
TESTING
COMPUTER
VISION
ALGORITHMS
SO
THE
DATA
COLLECTED
BY
THE
ESP
GAME
IS
NOT
SUFFICIENT
FOR
OUR
PURPOSES
THE
GAME
INTRODUCED
IN
THIS
PAPER
PEEKABOOM
IMPROVES
ON
THE
DATA
COLLECTED
BY
THE
ESP
GAME
AND
FOR
EACH
OBJECT
IN
THE
IMAGE
OUTPUTS
PRECISE
LOCATION
INFORMATION
AS
WELL
AS
OTHER
INFORMATION
USEFUL
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
BY
PLAYING
A
GAME
PEOPLE
HELP
US
COLLECT
DATA
FIGURE
PEEK
AND
BOOM
BOOM
GETS
AN
IMAGE
ALONG
WITH
A
WORD
RELATED
TO
IT
AND
MUST
REVEAL
PARTS
OF
THE
IMAGE
FOR
PEEK
TO
GUESS
THE
CORRECT
WORD
PEEK
CAN
ENTER
MULTIPLE
GUESSES
THAT
BOOM
CAN
SEE
NOT
BECAUSE
THEY
WANT
TO
BE
HELPFUL
BUT
BECAUSE
THEY
HAVE
FUN
INDEED
PEEKABOOM
OR
THE
ESP
GAME
OR
ANY
GAME
BUILT
ON
THIS
PREMISE
CAN
BE
TREATED
AS
A
HUMAN
ALGORITHM
ON
INPUT
AN
IMAGE
IT
OUTPUTS
WITH
ARBITRARILY
HIGH
PROBABILITY
A
CORRECT
ANNOTATION
OF
THE
IMAGE
INSTEAD
OF
USING
A
SILICON
PROCESSOR
THIS
ALGORITHM
RUNS
ON
A
PROCESSOR
CONSISTING
OF
REGULAR
HUMANS
INTERACTING
THROUGHOUT
THE
WEB
IN
ADDITION
TO
APPLICATIONS
IN
COMPUTER
VISION
AND
IMAGE
SEARCH
OUR
SYSTEM
MAKES
A
SIGNIFICANT
CONTRIBUTION
TO
HCI
BECAUSE
OF
THE
WAY
IT
ADDRESSES
THE
PROBLEM
PEEKABOOM
PRESENTS
AN
EXAMPLE
OF
A
NEW
LINE
OF
RESEARCH
DEVOTED
TO
SOLVING
LARGE
SCALE
PROBLEMS
WITH
HUMAN
COMPUTING
POWER
WHERE
PEOPLE
INTERACT
WITH
COMPUTERS
TO
EXTEND
THE
COMPUTATIONAL
ABILITIES
OF
MACHINES
BASIC
GAME
PLAY
PEEKABOOM
AS
THE
NAME
MAY
SUGGEST
IS
A
GAME
WITH
TWO
MAIN
COMPONENTS
PEEK
AND
BOOM
TWO
RANDOM
PLAYERS
FROM
THE
WEB
PARTICIPATE
BY
TAKING
DIFFERENT
ROLES
IN
THE
GAME
WHEN
ONE
PLAYER
IS
PEEK
THE
OTHER
IS
BOOM
PEEK
STARTS
OUT
WITH
A
BLANK
SCREEN
WHILE
BOOM
STARTS
WITH
AN
IMAGE
AND
A
WORD
RELATED
TO
IT
SEE
FIGURE
THE
GOAL
OF
THE
GAME
IS
FOR
BOOM
TO
REVEAL
PARTS
OF
THE
IMAGE
TO
PEEK
SO
THAT
PEEK
CAN
GUESS
THE
ASSOCIATED
WORD
BOOM
REVEALS
CIRCULAR
AREAS
OF
THE
IMAGE
BY
CLICKING
A
CLICK
REVEALS
AN
AREA
WITH
A
PIXEL
RADIUS
PEEK
ON
THE
OTHER
HAND
CAN
ENTER
GUESSES
OF
WHAT
BOOM
WORD
IS
BOOM
CAN
SEE
PEEK
GUESSES
AND
CAN
INDICATE
WHETHER
THEY
ARE
HOT
OR
COLD
WHEN
PEEK
CORRECTLY
GUESSES
THE
WORD
THE
PLAYERS
GET
POINTS
AND
SWITCH
ROLES
PLAY
THEN
PROCEEDS
ON
A
NEW
IMAGE
WORD
PAIR
IF
THE
IMAGE
WORD
PAIR
IS
TOO
DIFFICULT
THE
TWO
PLAYERS
CAN
PASS
OR
OPT
OUT
OF
THE
CURRENT
IMAGE
PASSING
CREATES
THE
SAME
EFFECT
AS
A
CORRECT
GUESS
FROM
PEEK
EXCEPT
THAT
THE
PLAYERS
GET
NO
POINTS
TO
MAXIMIZE
POINTS
BOOM
HAS
AN
INCENTIVE
TO
REVEAL
ONLY
THE
AREAS
OF
THE
IMAGE
NECESSARY
FOR
PEEK
TO
GUESS
THE
CORRECT
WORD
FOR
EXAMPLE
IF
THE
IMAGE
CONTAINS
A
CAR
AND
A
DOG
AND
THE
WORD
ASSOCIATED
TO
THE
IMAGE
IS
DOG
THEN
BOOM
WILL
REVEAL
ONLY
THOSE
PARTS
OF
THE
IMAGE
THAT
CONTAIN
THE
DOG
THUS
GIVEN
AN
IMAGE
WORD
PAIR
DATA
FROM
THE
GAME
YIELD
THE
AREA
OF
THE
IMAGE
PERTAINING
TO
THE
WORD
PINGS
ANOTHER
COMPONENT
OF
THE
GAME
ARE
PINGS
RIPPLES
THAT
APPEAR
ON
PEEK
SCREEN
WHEN
BOOM
RIGHT
CLICKS
ON
THE
IMAGE
SEE
FIGURE
IF
TWO
PLAYERS
WERE
PLAYING
WITH
THE
IMAGE
ON
FIGURE
THEN
MANY
CORRECT
WORDS
ARE
POSSIBLE
FROM
PEEK
POINT
OF
VIEW
ELEPHANT
TRUNK
TUSK
EAR
SUPPOSE
THE
CORRECT
WORD
IS
TRUNK
TO
GET
PEEK
TO
GUESS
CORRECTLY
BOOM
CAN
PING
THE
TRUNK
OF
THE
ELEPHANT
BY
RIGHT
CLICKING
ON
IT
IN
DOING
SO
BOOM
HELPS
TO
DISAMBIGUATE
THE
TRUNK
FROM
THE
REST
OF
THE
ELEPHANT
FIGURE
PINGS
TO
HELP
PEEK
BOOM
CAN
PING
PARTS
OF
THE
IMAGE
BY
RIGHT
CLICKING
ON
THEM
FIGURE
HINTS
BOOM
CAN
FURTHER
HELP
PEEK
BY
GIVING
HINTS
ABOUT
HOW
THE
WORD
RELATES
TO
THE
IMAGE
IS
IT
A
NOUN
DESCRIBING
SOMETHING
IN
THE
IMAGE
A
NOUN
RELATED
TO
THE
IMAGE
TEXT
ON
THE
IMAGE
OR
A
VERB
HINTS
ANOTHER
FEATURE
OF
THE
GAME
ARE
BUTTONS
THAT
ALLOW
BOOM
TO
GIVE
HINTS
TO
PEEK
ABOUT
HOW
THE
WORD
RELATES
TO
THE
IMAGE
SEE
FIGURES
AND
UPON
BOOM
PRESSING
OF
ONE
OF
THE
HINT
BUTTONS
A
CORRESPONDING
FLASHING
PLACARD
APPEARS
ON
PEEK
SCREEN
THE
REASON
FOR
HAVING
HINTS
IS
THAT
OFTEN
THE
WORDS
CAN
RELATE
TO
THE
IMAGE
IN
MULTIPLE
WAYS
AS
NOUNS
VERBS
TEXT
OR
RELATED
NOUNS
SOMETHING
NOT
IN
THE
IMAGE
BUT
RELATED
TO
IT
THE
ORIGIN
OF
IMAGES
AND
LABELS
ALL
WORDS
PRESENTED
TO
THE
PLAYERS
ARE
RELATED
TO
THEIR
CORRESPONDING
IMAGE
ON
INPUT
AN
IMAGE
WORD
PAIR
PEEKABOOM
OUTPUTS
A
REGION
OF
THE
IMAGE
THAT
IS
RELATED
TO
THE
WORD
WE
OBTAIN
MILLIONS
OF
IMAGES
WITH
ASSOCIATED
KEYWORD
LABELS
FROM
THE
ESP
GAME
WHICH
WE
NOW
DESCRIBE
IN
MORE
DETAIL
AS
MENTIONED
BEFORE
THE
ESP
GAME
IS
A
TWO
PLAYER
ONLINE
GAME
THAT
PAIRS
RANDOM
PLAYERS
FROM
THE
WEB
FROM
THE
PLAYER
PERSPECTIVE
THE
GOAL
OF
THE
ESP
GAME
IS
TO
GUESS
THE
WORD
THAT
THEIR
PARTNER
IS
TYPING
FOR
EACH
IMAGE
ONCE
BOTH
PLAYERS
HAVE
TYPED
THE
SAME
STRING
THEY
MOVE
ON
TO
A
NEXT
IMAGE
SINCE
THE
PLAYERS
CAN
T
COMMUNICATE
AND
DON
T
KNOW
ANYTHING
ABOUT
EACH
OTHER
THE
EASIEST
WAY
FOR
BOTH
TO
TYPE
THE
SAME
STRING
IS
BY
TYPING
SOMETHING
RELATED
TO
THE
COMMON
IMAGE
THE
STRING
UPON
WHICH
THE
TWO
PLAYERS
AGREE
IS
A
VERY
GOOD
LABEL
FOR
THE
IMAGE
WE
USE
THE
LABELS
COLLECTED
FROM
THE
ESP
GAME
AS
THE
WORDS
WE
PRESENT
TO
THE
PLAYERS
IN
PEEKABOOM
GAME
POINTS
AND
THE
BONUS
ROUND
ALTHOUGH
THE
EXACT
NUMBER
OF
POINTS
GIVEN
TO
THE
PLAYERS
FOR
DIFFERENT
ACTIONS
IS
NOT
IMPORTANT
WE
MENTION
IT
TO
SHOW
THE
RELATIVE
PROPORTIONS
FURTHERMORE
WE
MENTION
THE
DIFFERENT
POINT
STRATEGIES
USED
BY
PEEKABOOM
TO
KEEP
PLAYERS
ENGAGED
POINTS
ARE
GIVEN
TO
BOTH
PEEK
AND
BOOM
EQUALLY
WHENEVER
PEEK
GUESSES
THE
CORRECT
WORD
IN
THE
CURRENT
IMPLEMENTATION
BOTH
OBTAIN
POINTS
POINTS
ARE
NOT
SUBTRACTED
FOR
PASSING
POINTS
ARE
ALSO
GIVEN
TO
BOTH
PEEK
AND
BOOM
FOR
USING
THE
HINT
BUTTONS
ALTHOUGH
THIS
MIGHT
APPEAR
COUNTERINTUITIVE
SINCE
USING
HINTS
DEDUCTS
POINTS
IN
MANY
OTHER
GAMES
WE
ACTUALLY
WANT
THE
PLAYERS
TO
USE
THE
HINT
BUTTONS
AS
MENTIONED
ABOVE
HINTS
GIVE
US
ADDITIONAL
INFORMATION
ABOUT
THE
RELATIONSHIP
BETWEEN
THE
WORD
AND
THE
IMAGE
AND
THEREFORE
WE
ENCOURAGE
PLAYERS
TO
USE
THEM
TWENTY
FIVE
EXTRA
POINTS
ARE
GIVEN
TO
BOTH
PEEK
AND
BOOM
WHENEVER
PEEK
GUESSES
THE
CORRECT
WORD
AND
BOOM
HAD
USED
A
HINT
POINTS
ARE
NOT
GIVEN
FOR
USAGE
OF
THE
HOT
COLD
BUTTONS
EVERY
TIME
THE
PLAYERS
CORRECTLY
COMPLETE
FOUR
IMAGES
THEY
ARE
SENT
TO
A
BONUS
ROUND
THE
BONUS
ROUND
IS
DIFFERENT
IN
NATURE
FROM
THE
REST
OF
THE
GAME
AND
ALLOWS
PLAYERS
TO
OBTAIN
UP
TO
POINTS
IN
THE
BONUS
ROUND
SEE
FIGURE
PLAYERS
SIMPLY
CLICK
ON
AN
OBJECT
IN
THE
IMAGE
THE
CLOSER
THEY
ARE
TO
EACH
OTHER
CLICKS
THE
MORE
POINTS
THEY
GET
FOR
EXAMPLE
BOTH
PLAYERS
COULD
OBTAIN
AN
IMAGE
OF
A
CAR
AND
BE
TOLD
CLICK
ON
THE
CAR
FIGURE
THE
PEEKABOOM
BONUS
ROUND
PLAYERS
MUST
CLICK
ON
THE
SPECIFIED
OBJECT
WITHIN
THE
IMAGE
THEY
OBTAIN
POINTS
PROPORTIONAL
TO
HOW
CLOSE
THEIR
CLICKS
ARE
TO
EACH
OTHER
CLICKS
PLAYERS
OBTAIN
BETWEEN
AND
POINTS
FOR
EVERY
CLICK
IN
THE
BONUS
ROUND
DEPENDING
ON
HOW
FAR
THE
CLICK
IS
FROM
THEIR
PARTNER
CORRESPONDING
CLICK
THE
BONUS
ROUND
IS
TIMED
PLAYERS
HAVE
TO
CLICK
ON
THE
SAME
PLACE
AS
THEIR
PARTNER
AS
MANY
TIMES
AS
THEY
CAN
IN
SECONDS
IF
THE
OBJECT
IS
NOT
IN
THE
IMAGE
PLAYERS
CAN
PASS
BECAUSE
SOME
IMAGES
DO
NOT
CONTAIN
THE
OBJECT
RELATED
TO
THE
WORD
PASSING
IN
THE
BONUS
ROUND
GENERATES
POINTS
FOR
BOTH
PLAYERS
SO
WE
CAN
LEARN
WHETHER
THE
OBJECT
IS
THERE
PLAYERS
CANNOT
PASS
AFTER
THEY
HAVE
CLICKED
ON
THE
IMAGE
THERE
ARE
TWO
REASONS
FOR
THE
PEEKABOOM
BONUS
ROUND
FIRST
BY
GIVING
PLAYERS
BITE
SIZE
MILESTONES
GETTING
FOUR
IMAGES
CORRECTLY
WE
REINFORCE
THEIR
INCREMENTAL
SUCCESS
IN
THE
GAME
AND
THUS
ENCOURAGE
THEM
TO
CONTINUE
PLAYING
SECOND
THE
BONUS
ROUND
IS
AN
ALTERNATIVE
APPROACH
TO
COLLECTING
TRAINING
DATA
FOR
COMPUTER
VISION
IN
IT
PLAYERS
CLICK
INSIDE
SPECIFIC
OBJECTS
WITHIN
AN
IMAGE
SUCH
CLICKS
GIVE
ADDITIONAL
INFORMATION
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
IN
THIS
PAPER
WE
DO
NOT
CONCERN
OURSELVES
WITH
SUCH
INFORMATION
BUT
REMARK
THAT
IT
IS
ALSO
USEFUL
COLLECTING
IMAGE
METADATA
OUR
GOAL
IS
TO
CONSTRUCT
A
DATABASE
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
HERE
WE
DISCUSS
EXACTLY
WHAT
INFORMATION
IS
COLLECTED
BY
PEEKABOOM
AND
HOW
IT
IS
COLLECTED
ON
INPUT
AN
IMAGE
WORD
PAIR
COMING
DIRECTLY
FROM
THE
ESP
GAME
PEEKABOOM
COLLECTS
THE
FOLLOWING
INFORMATION
HOW
THE
WORD
RELATES
TO
THE
IMAGE
IS
IT
AN
OBJECT
PERSON
OR
ANIMAL
IN
THE
IMAGE
IS
IT
TEXT
IN
THE
IMAGE
IS
IT
A
VERB
DESCRIBING
AN
ACTION
IN
THE
IMAGE
IS
IT
AN
OBJECT
PERSON
OR
ANIMAL
NOT
IN
THE
IMAGE
BUT
RELATED
TO
IT
THE
ESP
GAME
ASSOCIATES
WORDS
TO
IMAGES
BUT
DOES
NOT
SAY
HOW
THE
WORD
IS
RELATED
TO
THE
IMAGE
FIGURE
FOR
INSTANCE
SHOWS
MULTIPLE
WAYS
IN
WHICH
A
WORD
CAN
BE
RELATED
TO
AN
IMAGE
HINT
BUTTONS
IN
PEEKABOOM
ALLOW
US
TO
DETERMINE
THE
RELATION
OF
THE
WORD
TO
THE
IMAGE
THIS
IS
USEFUL
IN
MULTIPLE
WAYS
BUT
FOR
THE
PURPOSES
OF
CONSTRUCTING
TRAINING
SETS
FOR
COMPUTER
VISION
IT
ALLOWS
US
TO
WEED
OUT
RELATED
NOUNS
AND
TO
TREAT
TEXT
SEPARATELY
PIXELS
NECESSARY
TO
GUESS
THE
WORD
WHEN
PEEK
ENTERS
THE
CORRECT
WORD
THE
AREA
THAT
BOOM
HAS
REVEALED
IS
PRECISELY
ENOUGH
TO
GUESS
THE
WORD
THAT
IS
WE
CAN
LEARN
EXACTLY
WHAT
CONTEXT
IS
NECESSARY
TO
DETERMINE
WHAT
THE
WORD
REFERS
TO
THIS
CONTEXT
INFORMATION
IS
ABSOLUTELY
NECESSARY
WHEN
ATTEMPTING
TO
DETERMINE
WHAT
TYPE
OF
OBJECT
A
SET
OF
PIXELS
CONSTITUTES
SEE
FIGURE
THE
PIXELS
INSIDE
THE
OBJECT
ANIMAL
OR
PERSON
IF
THE
WORD
IS
A
NOUN
DIRECTLY
REFERRING
TO
SOMETHING
IN
THE
IMAGE
PINGS
GIVE
US
PIXELS
THAT
ARE
INSIDE
THE
OBJECT
PERSON
OR
ANIMAL
FIGURE
THE
IMAGE
ON
THE
LEFT
CONTAINS
A
CAR
DRIVING
THROUGH
THE
STREET
WHILE
THE
ONE
ON
THE
RIGHT
HAS
A
PERSON
CROSSING
THE
SAME
STREET
BOTH
THE
CAR
AND
THE
PERSON
ARE
EXACTLY
THE
SAME
SET
OF
PIXELS
UP
TO
A
ROTATION
BY
DEGREES
EXAMPLE
TAKEN
FROM
THE
MOST
SALIENT
ASPECTS
OF
THE
OBJECTS
IN
THE
IMAGE
BY
INSPECTING
THE
SEQUENCE
OF
BOOM
CLICKS
WE
GAIN
INFORMATION
ABOUT
WHAT
PARTS
OF
THE
IMAGE
ARE
SALIENT
WITH
RESPECT
TO
THE
WORD
BOOM
TYPICALLY
REVEALS
THE
MOST
SALIENT
PARTS
OF
THE
IMAGE
FIRST
E
G
FACE
OF
A
DOG
INSTEAD
OF
THE
LEGS
ETC
ELIMINATION
OF
POOR
IMAGE
WORD
PAIRS
IF
MANY
INDEPENDENT
PAIRS
OF
PLAYERS
AGREE
TO
PASS
ON
AN
IMAGE
WITHOUT
TAKING
ACTION
ON
IT
THEN
LIKELY
THEY
FOUND
IT
IMPOSSIBLY
HARD
BECAUSE
OF
POOR
PICTURE
QUALITY
OR
A
DUBIOUS
RELATION
BETWEEN
THE
IMAGE
AND
ITS
LABEL
BY
IMPLEMENTING
AN
EVICTION
POLICY
FOR
IMAGES
THAT
WE
DISCOVER
ARE
BAD
WE
CAN
IMPROVE
THE
QUALITY
OF
THE
DATA
COLLECTED
AS
WELL
AS
THE
FUN
LEVEL
OF
THE
GAME
WHEN
MULTIPLE
PLAYERS
HAVE
GONE
THROUGH
THE
SAME
IMAGE
THESE
PIECES
OF
INFORMATION
CAN
BE
COMBINED
INTELLIGENTLY
TO
GIVE
EXTREMELY
ACCURATE
AND
USEFUL
ANNOTATIONS
FOR
COMPUTER
VISION
LATER
IN
THE
PAPER
FOR
EXAMPLE
WE
SHOW
HOW
A
SIMPLE
ALGORITHM
CAN
USE
THE
DATA
PRODUCED
BY
PEEKABOOM
TO
CALCULATE
ACCURATE
OBJECT
BOUNDING
BOXES
SEE
FIGURE
THE
SINGLE
PLAYER
GAME
PEEKABOOM
IS
A
TWO
PLAYER
GAME
OFTENTIMES
HOWEVER
THERE
WILL
BE
AN
ODD
NUMBER
OF
PEOPLE
ATTEMPTING
TO
PLAY
THE
GAME
SO
THE
REMAINING
PERSON
CANNOT
BE
PAIRED
TO
PREVENT
THEIR
FRUSTRATION
WE
ALSO
HAVE
A
SINGLE
PLAYER
VERSION
OF
THE
GAME
IN
WHICH
THE
PLAYER
IS
MATCHED
WITH
A
SERVER
SIDE
BOT
OUR
BOT
ACTS
INTELLIGENTLY
TO
SIMULATE
A
HUMAN
PLAYER
BY
BEING
BASED
ON
PRE
RECORDED
GAMES
IN
OTHER
WORDS
WE
TAKE
DATA
COLLECTED
FROM
PAIRS
OF
HUMANS
AND
USE
IT
AS
THE
BASIS
FOR
THE
COMPUTER
PLAYER
LOGIC
EMULATING
A
BOOM
PLAYER
IS
FAIRLY
SIMPLE
THE
BOT
CAN
REGURGITATE
THE
SEQUENCE
OF
RECORDED
CLICKS
TO
THE
HUMAN
EMULATING
PEEK
IS
MUCH
MORE
COMPLICATED
THE
BOT
NEEDS
TO
HAVE
SOME
CONCEPT
OF
CLOSENESS
OF
THE
HUMAN
CLICKS
TO
THE
SET
OF
RECORDED
CLICKS
FOR
INSTANCE
IF
THE
HUMAN
DOES
NOT
REVEAL
THE
DOG
IN
THE
PICTURE
THE
BOT
SHOULD
NOT
GUESS
DOG
OUR
BOT
ONLY
REVEALS
A
CERTAIN
PRE
RECORDED
GUESS
IF
ENOUGH
AREA
HAS
BEEN
REVEALED
TOWARDS
THIS
END
IT
EMPLOYS
A
SPATIAL
DATA
STRUCTURE
WHOSE
MEMBERS
ARE
CIRCLES
EACH
OF
WHICH
CORRESPONDS
TO
A
CLICK
ELEMENTS
OF
THE
DATA
STRUCTURE
ARE
REMOVED
AS
THEY
ARE
CLICKED
ON
BY
THE
HUMAN
PLAYER
WHEN
THE
DATA
STRUCTURE
BECOMES
EMPTY
THE
BOT
GIVES
THE
CORRECT
ANSWER
MOREOVER
IT
HAS
THE
ABILITY
TO
MAKE
INCORRECT
GUESSES
ALONG
THE
WAY
BASED
ON
THE
RELATIVE
EMPTINESS
OF
THE
SPATIAL
DATA
STRUCTURE
CHEATING
PEEKABOOM
IS
A
COLLABORATIVE
GAME
PARTNERS
WORK
TOGETHER
TO
MAXIMIZE
THEIR
SCORE
WHEN
BOTH
PARTNERS
DO
NOT
COMMUNICATE
OUTSIDE
THE
GAME
ENVIRONMENT
WE
OBTAIN
CORRECT
INFORMATION
HOWEVER
IF
THE
TWO
PARTNERS
COLLUDE
TO
CHEAT
ON
THE
GAME
THE
DATA
COULD
BE
POISONED
FOR
INSTANCE
IF
BOOM
AND
PEEK
KNOW
EACH
OTHER
AND
HAVE
AN
OUTSIDE
MEANS
OF
COMMUNICATION
THEN
BOOM
CAN
SIMPLY
TELL
PEEK
WHAT
WORDS
TO
TYPE
PEEKABOOM
CONTAINS
MULTIPLE
ANTI
CHEATING
MECHANISMS
THROUGH
A
COMBINATION
OF
ONLINE
IN
GAME
ENFORCEMENT
AND
OFFLINE
ANALYSIS
WE
ARE
ABLE
TO
DETECT
AND
DEAL
WITH
CHEATING
BEFORE
DETAILING
PEEKABOOM
ANTI
CHEATING
MEASURES
WE
MENTION
THAT
CHEATING
ATTEMPTS
ARE
UNCOMMON
ALTHOUGH
A
MINORITY
OF
PLAYERS
MIGHT
OBTAIN
SATISFACTION
FROM
GAMING
THE
SYSTEM
THE
MAJORITY
OF
THEM
JUST
WANT
TO
PLAY
THE
GAME
HONESTLY
INDEED
AS
ANECDOTAL
EVIDENCE
WHEN
PEEKABOOM
WAS
TESTED
IN
A
ROOM
WITH
CHILDREN
OF
AGES
THEY
WOULD
COVER
THE
WORD
WITH
THEIR
HAND
TO
PREVENT
OTHERS
IN
THE
ROOM
FROM
SEEING
THE
ANSWERS
NEVERTHELESS
PEEKABOOM
DOES
HAVE
A
FULL
SET
OF
MEASURES
TO
PREVENT
COLLUSION
THE
PLAYER
QUEUE
WHEN
PLAYERS
LOG
ON
TO
THE
GAME
SERVER
THEY
ARE
NOT
IMMEDIATELY
PAIRED
OFF
INSTEAD
THE
SERVER
MAKES
THEM
WAIT
N
SECONDS
WHERE
N
IS
THE
NUMBER
OF
SECONDS
UNTIL
THE
NEXT
MATCHING
INTERVAL
CURRENTLY
MATCHING
INTERVALS
HAPPEN
EVERY
SECONDS
AND
WHEN
THEY
DO
THE
SERVER
MATCHES
EVERYONE
IN
THE
QUEUE
WITH
A
PARTNER
ANY
ODD
PERSON
OUT
WILL
BE
PAIRED
WITH
A
BOT
WITH
A
LARGE
NUMBER
OF
PLAYERS
IN
THE
SYSTEM
WE
CAN
ENSURE
THAT
A
PLAYER
PARTNER
IS
RANDOM
AND
PREVENT
COLLUDERS
FROM
GETTING
MATCHED
JUST
BECAUSE
THEY
CLICKED
START
PLAYING
AT
THE
SAME
TIME
IP
ADDRESS
CHECKS
WE
ALSO
CHECK
PLAYER
IP
ADDRESSES
TO
ENSURE
THAT
THEY
ARE
NOT
PAIRED
WITH
THEMSELVES
OR
WITH
OTHERS
THAT
HAVE
A
SIMILAR
ADDRESS
SIMILARITY
IN
IP
ADDRESSES
CAN
IMPLY
GEOGRAPHICAL
PROXIMITY
SEED
IMAGES
BECAUSE
OUR
SYSTEM
IS
A
WEB
BASED
GAME
ONE
POINT
OF
CONCERN
IS
THAT
BOTS
I
E
AUTOMATED
PLAYERS
MIGHT
PLAY
THE
GAME
AND
POLLUTE
THE
POOL
OF
COLLECTED
DATA
TO
DETECT
THEM
WE
INTRODUCE
SEED
IMAGES
INTO
THE
SYSTEM
IN
OTHER
WORDS
THOSE
FOR
WHICH
WE
HAVE
HAND
VERIFIED
METADATA
ON
BEING
PRESENTED
SEED
IMAGES
IF
A
PLAYER
CONSISTENTLY
FAILS
TO
CLICK
ON
THE
RELEVANT
PARTS
WHEN
PLAYING
BOOM
OR
TO
GUESS
THE
CORRECT
WORDS
WHEN
PLAYING
PEEK
THEY
WILL
BE
ADDED
TO
A
BLACKLIST
WE
DISCARD
ALL
CURRENT
AND
FUTURE
GAME
PLAY
DATA
ASSOCIATED
WITH
ANYONE
ON
THE
BLACKLIST
NOTICE
THAT
ALMOST
BY
DEFINITION
A
COMPUTER
PROGRAM
CANNOT
SUCCESSFULLY
PLAY
PEEKABOOM
IF
IT
WERE
ABLE
TO
DO
SO
THEN
IT
WOULD
BE
ABLE
TO
RECOGNIZE
THE
OBJECTS
IN
THE
IMAGES
THEREFORE
THIS
STRATEGY
PREVENTS
BOTS
AS
WELL
AS
OTHERWISE
MALICIOUS
PLAYERS
FROM
POISONING
OUR
DATA
LIMITED
FREEDOM
TO
ENTER
GUESSES
SINCE
BOOM
CAN
SEE
ALL
OF
PEEK
GUESSES
THE
GAME
ALLOWS
A
LIMITED
FORM
OF
COMMUNICATION
BETWEEN
THE
PLAYERS
INDEED
MANY
OF
THE
PEEKABOOM
PLAYERS
USE
THE
GUESS
FIELD
AS
A
WAY
TO
COMMUNICATE
WITH
THEIR
PARTNER
IT
IS
NOT
UNCOMMON
FOR
THE
FIRST
GUESS
IN
A
GAME
TO
BE
HI
OR
FOR
THE
FIRST
GUESS
AFTER
PASSING
ON
AN
IMAGE
TO
BE
THE
CORRECT
WORD
ASSOCIATED
WITH
THE
PREVIOUS
IMAGE
IT
IS
ALSO
NOT
UNCOMMON
FOR
PLAYERS
TO
TYPE
SORRY
AFTER
TAKING
TOO
LONG
ON
AN
IMAGE
A
POSSIBLE
CHEATING
STRATEGY
IS
TO
EXCHANGE
IM
SCREEN
NAMES
THROUGH
THE
GUESS
FIELD
AND
THEN
USING
IM
COMMUNICATE
THE
CORRECT
WORDS
ALTHOUGH
WE
HAVE
NEVER
OBSERVED
ATTEMPTS
TO
EXECUTE
SUCH
A
STRATEGY
WE
CAN
MITIGATE
IT
BY
NOT
ALLOWING
PEEK
TO
ENTER
ANY
NON
ALPHABETICAL
CHARACTERS
SUCH
AS
NUMBERS
SIMILARLY
WE
CAN
PREVENT
BOOM
FROM
SEEING
ANY
GUESSES
THAT
ARE
NOT
WORDS
IN
THE
DICTIONARY
CURRENTLY
WE
DO
ALLOW
BOOM
TO
SEE
SUCH
GUESSES
BECAUSE
WE
HAVE
NOT
SEEN
PLAYERS
ATTEMPT
TO
CHEAT
IN
THIS
WAY
HOWEVER
EVEN
IF
PLAYERS
ARE
SUCCESSFUL
IN
SUCH
A
STRATEGY
THE
OTHER
ANTI
COLLUSION
MECHANISMS
CAN
DEAL
WITH
THE
CORRUPTED
DATA
AGGREGATING
DATA
FROM
MULTIPLE
PLAYERS
IN
ADDITION
TO
THE
ABOVE
STRATEGIES
WE
AGGREGATE
DATA
FROM
MULTIPLE
PLAYERS
FOR
A
GIVEN
IMAGE
WORD
PAIR
BY
DOING
THIS
WE
CAN
ELIMINATE
OUTLIERS
IMPLEMENTATION
WE
IMPLEMENTED
THE
ARCHITECTURE
OF
THE
GAME
UNDER
THE
CLIENT
SERVER
MODEL
THE
CLIENT
APPLICATION
IS
DELIVERED
AS
A
JAVA
APPLET
WHILE
THE
SERVER
IS
WRITTEN
PURELY
IN
JAVA
APPLETS
CONNECT
TO
A
SERVER
WHICH
THEN
MATCHES
THE
PLAYERS
WITH
GAMES
OF
PEEKABOOM
UPON
TWO
PLAYERS
COMPLETION
OF
A
MATCH
THE
SERVER
WRITES
THEIR
GAME
PLAY
DATA
AND
SCORES
TO
DISK
WE
THEN
COMPILE
THE
COLLECTED
DATA
INTO
DESIRED
FORMATS
OUR
IMPLEMENTATION
OF
THE
GAME
CONTAINS
MANY
FEATURES
TO
IMPROVE
GAME
PLAY
SPELLING
CHECK
INCORRECTLY
SPELLED
WORDS
ARE
DISPLAYED
IN
A
DIFFERENT
COLOR
TO
NOTIFY
PLAYERS
THIS
IS
IMPORTANT
BECAUSE
THE
PEEK
PLAYER
USUALLY
TYPES
MULTIPLE
GUESSES
IN
A
SHORT
TIME
OFTEN
MAKING
SPELLING
MISTAKES
INAPPROPRIATE
WORD
REPLACEMENT
SINCE
BOOM
CAN
SEE
PEEK
GUESSES
WE
DO
NOT
ALLOW
PEEK
TO
ENTER
INAPPROPRIATE
WORDS
WHENEVER
ONE
OF
PEEK
GUESSES
IS
AMONG
A
LIST
OF
POSSIBLE
INAPPROPRIATE
WORDS
WE
SUBSTITUTE
IT
WITH
ANOTHER
WORD
CHOSEN
FROM
A
LIST
OF
INNOCENT
WORDS
SUCH
AS
LOVE
CARING
ILUVPEEKABOOM
ETC
TOP
SCORES
LIST
AND
RANKS
THE
PEEKABOOM
WEBSITE
PROMINENTLY
DISPLAYS
THE
CUMULATIVE
TOP
SCORES
OF
THE
DAY
AS
WELL
AS
THE
TOP
SCORES
OF
ALL
TIME
FURTHERMORE
PLAYERS
ARE
GIVEN
A
RANK
BASED
ON
THE
TOTAL
NUMBER
OF
POINTS
THEY
HAVE
ACCUMULATED
THROUGHOUT
TIME
SEE
FIGURE
THE
DIFFERENT
RANKS
ARE
FRESH
MEAT
POINTS
NEWBIE
POINTS
PLAYER
POINTS
GANGSTER
POINTS
AND
GODFATHER
OR
MORE
POINTS
WE
REMARK
THAT
RANKS
HAVE
PROVEN
AN
IMPORTANT
COMPONENT
OF
PEEKABOOM
INCENTIVE
STRATEGY
OF
THE
PLAYERS
THAT
HAVE
OBTAINED
AN
ACCOUNT
OF
THEM
HAVE
SCORES
THAT
FALL
WITHIN
POINTS
OF
THE
RANK
CUTOFFS
GIVEN
THAT
THESE
INTERVALS
COVER
LESS
THAN
OF
THE
SPACE
OF
POSSIBLE
CUMULATIVE
SCORES
THIS
STRONGLY
SUGGESTS
THAT
MANY
PLAYERS
SIMPLY
PLAY
TO
REACH
A
NEW
RANK
FIGURE
TOP
SCORES
AND
PLAYER
RANKS
PLAYERS
ARE
SHOWN
THEIR
CURRENT
RANK
AND
THE
NUMBER
OF
POINTS
REMAINING
FOR
THE
NEXT
RANK
ADDITIONAL
APPLICATIONS
BEFORE
GOING
TO
THE
EVALUATION
SECTION
WE
MENTION
TWO
ADDITIONAL
APPLICATIONS
FOR
THE
DATA
COLLECTED
BY
PEEKABOOM
A
BENEFIT
OF
THESE
APPLICATIONS
IS
THAT
THEY
ARE
DIRECT
IN
THAT
THEY
DO
NOT
REQUIRE
THE
TRAINING
OF
MACHINE
LEARNING
ALGORITHMS
IMPROVING
IMAGE
SEARCH
RESULTS
PEEKABOOM
GIVES
AN
ACCURATE
ESTIMATE
OF
THE
FRACTION
OF
THE
IMAGE
RELATED
TO
THE
WORD
IN
QUESTION
THIS
ESTIMATE
CAN
BE
CALCULATED
FROM
THE
AREA
REVEALED
BY
BOOM
THE
FRACTION
OF
THE
IMAGE
RELATED
TO
A
WORD
CAN
BE
USED
TO
ORDER
IMAGE
SEARCH
RESULTS
IMAGES
IN
WHICH
THE
WORD
REFERS
TO
A
HIGHER
FRACTION
OF
THE
TOTAL
PIXELS
SHOULD
BE
RANKED
HIGHER
MUCH
LIKE
THE
GOAL
OF
THE
ESP
GAME
IS
TO
LABEL
ALL
IMAGES
ON
THE
WEB
WE
CAN
IMAGINE
PEEKABOOM
DOING
THE
SAME
AND
THUS
FURTHER
IMPROVING
IMAGE
SEARCH
OBJECT
BOUNDING
BOXES
IN
THE
SAME
VEIN
PEEKABOOM
CAN
BE
USED
TO
DIRECTLY
CALCULATE
OBJECT
BOUNDING
BOXES
SIMILAR
TO
THOSE
USED
IN
FLICKR
SEE
FIGURE
FLICKR
IS
A
PHOTO
SHARING
SERVICE
THAT
ALLOWS
USERS
TO
TAG
IMAGES
WITH
KEYWORDS
AND
TO
ASSOCIATE
KEYWORDS
WITH
RECTANGULAR
AREAS
IN
THE
IMAGE
THE
AREAS
AND
TAGS
HOWEVER
ARE
NOT
GUARANTEED
TO
BE
CORRECT
SINCE
A
USER
CAN
ENTER
ANYTHING
THEY
WISH
FOR
THEIR
OWN
IMAGES
TO
EXHIBIT
THE
POWER
OF
THE
DATA
COLLECTED
BY
PEEKABOOM
WE
SHOW
HOW
TO
USE
IT
CALCULATE
SUCH
RECTANGLES
WE
EMPHASIZE
HOWEVER
THAT
THE
DATA
COLLECTED
BY
PEEKABOOM
IS
SIGNIFICANTLY
RICHER
AND
THAT
TO
CALCULATE
THE
RECTANGLES
WE
DISCARD
VAST
AMOUNTS
OF
THE
INFORMATION
COLLECTED
BY
OUR
GAME
SINCE
PEEKABOOM
ANNOTATES
ARBITRARY
IMAGES
ON
THE
WEB
ITS
DATA
ALLOWS
FOR
AN
IMAGE
SEARCH
ENGINE
IN
WHICH
THE
RESULTS
ARE
HIGHLIGHTED
SIMILAR
TO
THE
HIGHLIGHTED
WORDS
IN
GOOGLE
TEXT
SEARCH
RESULTS
USING
THE
DATA
OBTAINED
IN
THE
FIRST
TWO
WEEKS
OF
GAME
PLAY
WE
HAVE
IMPLEMENTED
A
PROTOTYPE
OF
SUCH
A
SEARCH
ENGINE
SEE
FIGURE
THE
SEARCH
ENGINE
CAN
BE
ACCESSED
FROM
THE
PEEKABOOM
WEBSITE
THE
BOUNDING
BOXES
WERE
CALCULATED
AS
FOLLOWS
FOR
A
SINGLE
PLAY
OF
AN
IMAGE
WORD
PAIR
WE
CREATE
A
MATRIX
OF
AND
THE
DIMENSIONS
OF
THE
MATRIX
ARE
THE
SAME
AS
THE
DIMENSIONS
OF
THE
IMAGE
IN
PIXELS
AT
FIRST
EVERY
ENTRY
IN
THE
MATRIX
IS
A
WE
ADD
A
IN
EVERY
PIXEL
CLICKED
BY
BOOM
AS
WELL
AS
IN
THE
CIRCLE
OF
RADIUS
PIXELS
AROUND
THE
CLICK
WE
THUS
OBTAIN
A
MATRIX
OF
AND
CORRESPONDING
TO
THE
EXACT
AREA
THAT
WAS
REVEALED
IN
A
SINGLE
GAME
PLAY
NEXT
WE
COMBINE
DIFFERENT
PLAYS
OF
THE
SAME
IMAGE
WORD
PAIR
BY
ADDING
THEIR
CORRESPONDING
MATRICES
THIS
GIVES
A
MATRIX
WHOSE
ENTRIES
ARE
INTEGERS
CORRESPONDING
TO
THE
NUMBER
OF
DIFFERENT
PLAYERS
THAT
REVEALED
EACH
PIXEL
OF
THE
IMAGE
ON
THIS
COMBINED
MATRIX
WE
APPLY
A
THRESHOLD
OF
MEANING
THAT
WE
SUBSTITUTE
EVERY
VALUE
LESS
THAN
WITH
AND
EVERY
VALUE
GREATER
THAN
WITH
THIS
GIVES
A
MATRIX
CORRESPONDING
TO
ALL
THE
PIXELS
THAT
HAVE
BEEN
REVEALED
BY
AT
LEAST
PLAYERS
NEXT
WE
CLUSTER
THESE
PIXELS
AND
CALCULATE
FIGURE
OBJECT
BOUNDING
BOXES
OBTAINED
FROM
PEEKABOOM
DATA
THE
BOUNDING
BOXES
BY
TAKING
FOR
EACH
CLUSTER
THE
LEFTMOST
RIGHTMOST
TOPMOST
AND
BOTTOMMOST
POINTS
THIS
ALGORITHM
MAY
PRODUCE
MULTIPLE
BOUNDING
BOXES
FOR
A
SINGLE
IMAGE
WORD
PAIR
FOR
INSTANCE
IN
FIGURE
WE
CAN
SEE
THAT
MANY
OF
THE
RESULTS
FOR
EYES
HAVE
TWO
BOUNDING
BOXES
ONE
CORRESPONDING
TO
EACH
EYE
AS
WE
WILL
SEE
THE
RESULTS
PRODUCED
BY
THIS
SIMPLISTIC
ALGORITHM
ARE
EXTREMELY
ACCURATE
SUCH
RESULTS
COULD
BE
IMPROVED
BY
MAKING
INTELLIGENT
USE
OF
THE
ADDITIONAL
DATA
GIVEN
BY
PEEKABOOM
SUCH
AS
PINGS
THE
PRECISE
ORDER
OF
THE
AREAS
REVEALED
ETC
BUT
FOR
THE
PURPOSES
OF
THIS
PAPER
WE
USE
THE
SIMPLISTIC
ALGORITHM
ALTERNATIVE
USING
PING
DATA
FOR
POINTING
INSTEAD
OF
SHOWING
BOUNDING
BOXES
CALCULATED
FROM
REVEALED
AREAS
WE
COULD
SHOW
ARROWS
OR
LINES
POINTING
TO
THE
OBJECTS
SEE
FIGURE
SUCH
POINTERS
CAN
BE
EASILY
CALCULATED
FROM
THE
PING
DATA
THE
SIMPLEST
ALGORITHM
FOR
DOING
SO
IS
TO
SELECT
A
PING
AT
RANDOM
AND
ASSUME
IT
IS
A
GOOD
POINTER
FOR
THE
OBJECT
WE
WILL
SHOW
THAT
THIS
SIMPLISTIC
ALGORITHM
GIVES
VERY
ACCURATE
RESULTS
FIGURE
SHOWS
AN
IMAGE
IN
WHICH
THE
DIFFERENT
OBJECTS
HAVE
BEEN
LOCATED
USING
PING
DATA
MORE
ELABORATE
ALGORITHMS
COULD
GIVE
EVEN
BETTER
RESULTS
WE
REMARK
HOWEVER
THAT
SIMPLY
AVERAGING
THE
PINGS
OVER
MULTIPLE
PLAYERS
TO
OBTAIN
A
SINGLE
POINTER
DOES
NOT
GIVE
ACCURATE
RESULTS
FOR
INSTANCE
IF
THE
OBJECT
WAS
EYES
AVERAGING
THE
PINGS
GIVES
A
POINTER
TO
A
REGION
THAT
IS
NOT
AN
EYE
FIGURE
CALCULATION
OF
OBJECT
POINTERS
USING
PINGS
EVALUATION
USER
STATISTICS
THE
EVALUATION
OF
OUR
CLAIMS
CONSISTS
OF
TWO
PARTS
FIRST
WE
MUST
SHOW
THAT
THE
GAME
IS
INDEED
ENJOYABLE
SECOND
WE
MUST
SHOW
THAT
THE
DATA
PRODUCED
BY
THE
GAME
IS
ACCURATE
IT
IS
DIFFICULT
TO
EVALUATE
HOW
ENJOYABLE
A
GAME
REALLY
IS
ONE
APPROACH
IS
TO
ASK
PARTICIPANTS
A
SERIES
OF
QUESTIONS
REGARDING
HOW
MUCH
THEY
ENJOYED
PLAYING
THE
GAME
OUR
DATA
FOR
SUCH
AN
APPROACH
WERE
EXTREMELY
POSITIVE
BUT
WE
FOLLOW
A
DIFFERENT
APPROACH
IN
THIS
PAPER
WE
PRESENT
USAGE
STATISTICS
FROM
ARBITRARY
PEOPLE
PLAYING
OUR
GAME
ONLINE
THIS
SAME
APPROACH
WAS
USED
BY
THE
ESP
GAME
USAGE
STATISTICS
PEEKABOOM
WAS
RELEASED
TO
A
GENERAL
AUDIENCE
ON
AUGUST
OF
WE
PRESENT
THE
USAGE
STATISTICS
FROM
THE
PERIOD
STARTING
AUGUST
AND
ENDING
SEPTEMBER
A
TOTAL
OF
DIFFERENT
PEOPLE
PLAYED
THE
GAME
DURING
THIS
TIME
GENERATING
PIECES
OF
DATA
BY
DIFFERENT
PEOPLE
WE
MEAN
DIFFERENT
USER
IDS
BY
A
PIECE
OF
DATA
WE
MEAN
A
SUCCESSFUL
ROUND
OF
PEEKABOOM
IN
WHICH
PEEK
CORRECTLY
GUESSED
THE
WORD
GIVEN
BOOM
REVEALED
REGION
WE
MENTION
THAT
AN
IMAGE
WORD
PAIR
CAN
HAVE
MULTIPLE
PIECES
OF
DATA
ASSOCIATED
TO
IT
IF
IT
OCCURS
IN
MULTIPLE
GAMES
IF
PEOPLE
GAVE
US
PIECES
OF
DATA
THEN
ON
AVERAGE
EACH
PERSON
PLAYED
ON
IMAGES
SINCE
EACH
SESSION
OF
THE
GAME
LASTS
MINUTES
AND
ON
AVERAGE
PLAYERS
GO
THROUGH
IMAGES
DURING
A
SESSION
IN
THIS
ONE
MONTH
PERIOD
EACH
PERSON
PLAYED
ON
AVERAGE
MINUTES
WITHOUT
COUNTING
TIME
SPENT
WAITING
FOR
A
PARTNER
ETC
OVER
OF
THE
PEOPLE
PLAYED
ON
MORE
THAN
ONE
OCCASION
THAT
IS
MORE
THAN
OF
THE
PEOPLE
PLAYED
ON
DIFFERENT
DATES
FURTHERMORE
EVERY
PLAYER
IN
THE
TOP
SCORES
LIST
PLAYED
OVER
GAMES
THAT
OVER
HOURS
WITHOUT
INCLUDING
THE
TIME
THEY
SPENT
WAITING
FOR
A
PARTNER
THIS
UNDOUBTEDLY
ATTESTS
TO
HOW
ENJOYABLE
THE
GAME
IS
USER
COMMENTS
TO
GIVE
A
FURTHER
SENSE
FOR
HOW
MUCH
THE
PLAYERS
ENJOYED
THE
GAME
WE
INCLUDE
BELOW
SOME
QUOTES
TAKEN
FROM
COMMENTS
SUBMITTED
BY
PLAYERS
USING
A
LINK
ON
THE
WEBSITE
THE
GAME
ITSELF
IS
EXTREMELY
ADDICTIVE
AS
THERE
IS
AN
ELEMENT
OF
PRESSURE
INVOLVED
IN
BEATING
THE
CLOCK
A
DRIVE
TO
SCORE
MORE
POINTS
THE
FEELING
THAT
YOU
COULD
ALWAYS
DO
BETTER
NEXT
TIME
AND
A
CURIOSITY
ABOUT
WHAT
IS
GOING
TO
COME
UP
NEXT
I
WOULD
SAY
THAT
IT
GIVES
THE
SAME
GUT
FEELING
AS
COMBINING
GAMBLING
WITH
CHARADES
WHILE
RIDING
ON
A
ROLLER
COASTER
THE
GOOD
POINTS
ARE
THAT
YOU
INCREASE
AND
STIMULATE
YOUR
INTELLIGENCE
YOU
DON
T
LOSE
ALL
YOUR
MONEY
AND
YOU
DON
T
FALL
OFF
THE
RIDE
THE
BAD
POINT
IS
THAT
YOU
LOOK
AT
YOUR
WATCH
AND
EIGHT
HOURS
HAVE
JUST
DISAPPEARED
ONE
UNFORTUNATE
SIDE
EFFECT
OF
PLAYING
SO
MUCH
IN
SUCH
A
SHORT
TIME
WAS
A
MILD
CASE
OF
CARPAL
TUNNEL
SYNDROME
IN
MY
RIGHT
HAND
AND
FOREARM
BUT
THAT
DISSIPATED
QUICKLY
THIS
GAME
IS
LIKE
CRACK
I
VE
BEEN
PEEKABOOM
FREE
FOR
HOURS
UNLIKE
OTHER
GAMES
PEEKABOOM
IS
COOPERATIVE
RATHER
THAN
COMPETITIVE
EVALUATION
ACCURACY
OF
COLLECTED
DATA
THE
USEFULNESS
OF
PEEKABOOM
AS
A
DATA
COLLECTION
METHOD
RESTS
IN
THE
QUALITY
OF
THE
DATA
WE
COLLECT
ALTHOUGH
THE
DESIGN
OF
THE
GAME
INHERENTLY
ENSURES
CORRECTNESS
OF
THE
DATA
WE
WANTED
TO
TEST
WHETHER
IT
IS
AS
GOOD
AS
WHAT
WOULD
BE
COLLECTED
DIRECTLY
FROM
VOLUNTEERS
IN
A
NON
GAME
SETTING
TO
DO
SO
WE
CONDUCTED
TWO
EXPERIMENTS
TO
TEST
FIRST
THE
ACCURACY
OF
THE
BOUNDING
BOXES
WE
DEFINED
AND
SECOND
THE
UTILITY
OF
THE
POINTING
BEHAVIOR
IN
THE
GAME
NOTICE
THAT
THESE
EXPERIMENTS
ARE
MEANT
TO
ANALYZE
THE
CORRECTNESS
OF
THE
DATA
AND
NOT
WHETHER
SUCH
DATA
CAN
BE
USED
TO
TRAIN
COMPUTER
VISION
ALGORITHMS
THE
USEFULNESS
OF
DATA
ABOUT
LOCATION
OF
OBJECTS
FOR
TRAINING
COMPUTER
VISION
ALGORITHMS
HAS
BEEN
PREVIOUSLY
ESTABLISHED
EXPERIMENT
ACCURACY
OF
BOUNDING
BOXES
IN
THE
FIRST
EXPERIMENT
WE
TESTED
WHETHER
THE
BOUNDING
BOXES
FOR
OBJECTS
WITHIN
AN
IMAGE
THAT
ARE
CALCULATED
FROM
PEEKABOOM
ARE
AS
GOOD
AS
BOUNDING
BOXES
PEOPLE
WOULD
MAKE
AROUND
AN
OBJECT
IN
A
NON
GAME
SETTING
WE
SELECTED
AT
RANDOM
IMAGE
WORD
PAIRS
FROM
THE
DATA
POOL
THAT
HAD
BEEN
SUCCESSFULLY
PLAYED
ON
BY
AT
LEAST
TWO
INDEPENDENT
PAIRS
OF
PEOPLE
THE
IMAGES
SELECTED
ALL
HAD
NOUNS
AS
THEIR
WORD
AS
OPPOSED
TO
TEXT
IN
THE
IMAGE
OR
AN
ADJECTIVE
ETC
SEE
FIGURE
ALL
THE
IMAGES
CHOSEN
HAD
THE
WORD
REFER
TO
A
SINGLE
OBJECT
IN
THE
IMAGE
FOR
EACH
IMAGE
PEEKABOOM
DATA
WAS
USED
TO
CALCULATE
OBJECT
BOUNDING
BOXES
USING
THE
METHOD
EXPLAINED
IN
PREVIOUS
SECTIONS
WE
THEN
HAD
FOUR
VOLUNTEERS
MAKE
BOUNDING
BOXES
AROUND
THE
OBJECTS
FOR
EACH
IMAGE
PROVIDING
US
WITH
BOUNDING
BOXES
DRAWN
BY
VOLUNTEERS
THE
VOLUNTEERS
WERE
ASKED
FOR
EACH
IMAGE
TO
DRAW
A
BOUNDING
BOX
AROUND
THE
OBJECT
THAT
THE
WORD
REFERRED
TO
WE
THEN
SELECTED
AT
RANDOM
ONE
OF
THE
FOUR
VOLUNTEER
BOUNDING
BOXES
FOR
EACH
IMAGE
SO
AS
TO
END
UP
WITH
ONE
VOLUNTEER
GENERATED
BOUNDING
BOX
FOR
EVERY
ONE
OF
THE
IMAGES
FINALLY
WE
TESTED
THE
AMOUNT
OF
OVERLAP
BETWEEN
THE
BOUNDING
BOXES
GENERATED
BY
PEEKABOOM
AND
THOSE
GENERATED
BY
OUR
VOLUNTEERS
THE
AMOUNT
OF
OVERLAP
WAS
DETERMINED
USING
THE
FORMULA
OVERLAP
A
B
AREA
A
B
AREA
A
B
WHERE
A
AND
B
ARE
THE
BOUNDING
BOXES
NOTICE
THAT
IF
A
B
THEN
OVERLAP
A
B
AND
IF
A
IS
DISJOINT
FROM
B
THEN
OVERLAP
A
B
WE
CALCULATED
THE
AVERAGE
OVERLAP
ACROSS
THE
IMAGES
AS
WELL
AS
THE
STANDARD
DEVIATION
RESULTS
ON
AVERAGE
THE
OVERLAP
BETWEEN
THE
PEEKABOOM
BOUNDING
BOXES
AND
THE
VOLUNTEER
GENERATED
ONES
WAS
WITH
STANDARD
DEVIATION
THIS
MEANS
THAT
THE
PEEKABOOM
BOUNDING
BOXES
WERE
VERY
CLOSE
TO
THOSE
GENERATED
BY
THE
VOLUNTEERS
TO
ILLUSTRATE
WE
SHOW
IN
FIGURE
THE
BOUNDING
BOX
THAT
OBTAINED
THE
LOWEST
OVERLAP
SCORE
FIGURE
EXPERIMENT
IMAGE
WITH
LOWEST
OVERLAP
BETWEEN
A
VOLUNTEER
GENERATED
BOUNDING
BOX
SOLID
LINES
AND
ONE
GENERATED
BY
PEEKABOOM
DASHED
LINES
GIVEN
THAT
PEEKABOOM
WAS
NOT
DIRECTLY
BUILT
TO
CALCULATE
BOUNDING
BOXES
THIS
SHOWS
THE
WIDE
APPLICABILITY
OF
THE
DATA
COLLECTED
EXPERIMENT
ACCURACY
OF
PINGS
IN
THE
SECOND
EXPERIMENT
WE
TESTED
WHETHER
THE
OBJECT
POINTERS
THAT
ARE
CALCULATED
FROM
PEEKABOOM
ARE
INDEED
INSIDE
THE
OBJECTS
AS
IN
THE
PREVIOUS
EXPERIMENT
WE
SELECTED
AT
RANDOM
IMAGE
LABEL
PAIRS
FROM
THE
DATA
POOL
THAT
HAVE
BEEN
SUCCESSFULLY
PLAYED
ON
BY
AT
LEAST
TWO
INDEPENDENT
PAIRS
OF
PEOPLE
THE
IMAGES
SELECTED
ALL
HAD
THE
WORD
AS
A
NOUN
AS
OPPOSED
TO
AS
TEXT
IN
THE
IMAGE
OR
AN
ADJECTIVE
ETC
SEE
FIGURE
ALL
THE
IMAGES
CHOSEN
HAD
THE
WORD
REFER
TO
A
SINGLE
OBJECT
IN
THE
IMAGE
FOR
EACH
IMAGE
PEEKABOOM
DATA
WAS
USED
TO
CALCULATE
OBJECT
POINTERS
USING
THE
METHOD
EXPLAINED
IN
PREVIOUS
SECTIONS
WE
THEN
ASKED
THREE
VOLUNTEER
RATERS
TO
DETERMINE
FOR
EACH
POINTER
WHETHER
IT
WAS
INSIDE
THE
OBJECT
OR
NOT
THE
RATERS
WERE
SHOWN
EXAMPLES
OF
POINTERS
INSIDE
AND
OUTSIDE
THE
OBJECT
AND
WERE
TOLD
THAT
NEAR
AN
OBJECT
DOES
NOT
COUNT
AS
INSIDE
THE
OBJECT
RESULTS
ACCORDING
TO
ALL
THE
RATERS
OF
THE
POINTERS
WERE
INSIDE
THE
OBJECT
REFERRED
TO
BY
THE
WORD
THIS
GIVES
POSITIVE
EVIDENCE
THAT
PING
DATA
IS
ACCURATE
ESPECIALLY
SINCE
IT
WAS
CALCULATED
USING
SUCH
A
SIMPLISTIC
ALGORITHM
GENERALIZING
OUR
APPROACH
THE
APPROACH
PRESENTED
IN
THIS
PAPER
SOLVING
A
PROBLEM
BY
HAVING
PEOPLE
PLAY
GAMES
ONLINE
CAN
BE
GENERALIZED
TO
MANY
OTHER
PROBLEMS
IN
ARTIFICIAL
INTELLIGENCE
IN
FOLLOW
UP
WORK
FOR
EXAMPLE
WE
HAVE
CREATED
TWO
OTHER
GAMES
VERBOSITY
AND
PHETCH
IN
WHICH
PLAYERS
SOLVE
PROBLEMS
THAT
COMPUTERS
CANNOT
YET
SOLVE
VERBOSITY
COLLECTS
COMMON
SENSE
FACTS
TO
TRAIN
REASONING
ALGORITHMS
FOR
INSTANCE
FOR
THE
WORD
MILK
THE
GAME
OUTPUTS
FACTS
SUCH
AS
IT
IS
WHITE
PEOPLE
USUALLY
EAT
CEREAL
WITH
IT
ETC
VERBOSITY
IS
A
TWO
PLAYER
GAME
IN
WHICH
ONE
PLAYER
ATTEMPTS
TO
MAKE
THE
OTHER
SAY
A
TARGET
WORD
E
G
MILK
WITHOUT
USING
THE
WORD
THEY
DO
SO
BY
SAYING
MANY
FACTS
WITHOUT
USING
THE
WORD
ITSELF
IN
THEIR
STATEMENTS
E
G
IT
IS
A
WHITE
LIQUID
THE
UNDERLYING
GAME
MECHANISM
OF
VERBOSITY
IS
SIMILAR
IN
NATURE
TO
THAT
OF
PEEKABOOM
MUCH
LIKE
DESIGNING
AN
ALGORITHM
TO
SOLVE
A
PROBLEM
DESIGNING
A
GAME
TO
HARNESS
VALUABLE
HUMAN
CYCLES
IS
TO
A
LARGE
EXTENT
AN
ART
PROBLEMS
USUALLY
REQUIRE
A
SPECIFICALLY
TAILORED
GAME
IN
ADDITION
TO
AN
ORIGINAL
IDEA
CREATING
SUCH
A
GAME
ALSO
DEPENDS
ON
A
BROADER
SET
OF
CRITERIA
INCLUDING
LOOKS
THE
FLUIDITY
OF
THE
GAME
GRAPHICS
EASE
OF
USE
AN
INTUITIVE
USER
INTERFACE
COGNITIVE
LOAD
THE
AMOUNT
OF
USER
ATTENTION
REQUIRED
TO
PLAY
THE
GAME
AND
ACTION
THE
EXTENT
TO
WHICH
THE
GAME
ABSORBS
THE
USER
IN
THE
EXPERIENCE
ALL
OF
THESE
ASPECTS
HAVE
BEEN
TREATED
IN
THIS
PAPER
AND
WE
BELIEVE
MANY
OF
THE
TECHNIQUES
HERE
PRESENTED
GENERALIZE
TO
CREATING
OTHER
GAMES
WITH
A
PURPOSE
FINALLY
WE
BELIEVE
THAT
THESE
DESIGN
PRINCIPLES
LIKE
THE
SCIENTIFIC
METHOD
DON
T
JUST
PROVIDE
IDEAS
BUT
A
WAY
OF
THINKING
GAMES
PROVIDE
A
VALUABLE
VEHICLE
TO
SOLVE
PROBLEMS
THAT
COMPUTERS
CANNOT
YET
SOLVE
ETHICAL
CONSIDERATIONS
AS
WITH
ALL
SYSTEMS
SOLICITING
INPUT
FROM
HUMANS
WE
MUST
ADDRESS
THE
ETHICAL
ISSUES
BEHIND
THE
USAGE
OF
THE
COLLECTED
DATA
TOWARDS
THIS
END
WE
INFORM
THE
PLAYERS
OF
THE
GAME
PURPOSE
ON
THE
PEEKABOOM
WEBSITE
PLAYERS
PARTICIPATE
WILLINGLY
AND
KNOWINGLY
INDEED
MANY
PEOPLE
PLAY
BECAUSE
THEY
LIKE
THE
FACT
THAT
THE
GAME
HAS
A
PURPOSE
FURTHERMORE
WE
STATE
ON
THE
RECORD
THAT
THE
GAME
PURPOSE
IS
TO
OBTAIN
ACCURATE
SEGMENTATIONS
OF
OBJECTS
FROM
BACKGROUNDS
AND
TO
TRAIN
COMPUTER
VISION
ALGORITHMS
TO
RECOGNIZE
SIMPLE
OBJECTS
WE
HAVE
NO
INTENTION
OF
APPLYING
OUR
DATA
TOWARDS
FOR
EXAMPLE
MILITARY
SURVEILLANCE
RELATED
WORK
WE
HAVE
PRESENTED
A
METHOD
FOR
ANNOTATING
ARBITRARY
IMAGES
AND
WE
HAVE
PRESENTED
EVIDENCE
THAT
IT
PRODUCES
HIGH
QUALITY
DATA
WE
NOW
SURVEY
THE
RELATED
WORK
THE
ESP
GAME
AS
MENTIONED
BEFORE
THE
ESP
GAME
IS
TWO
PLAYER
GAME
THAT
COLLECTS
WORD
LABELS
FOR
ARBITRARY
IMAGES
PEEKABOOM
IS
SIMILAR
TO
THE
ESP
GAME
AND
IN
FACT
WAS
INSPIRED
BY
IT
WE
CONSIDER
PEEKABOOM
AN
EXTENSION
OF
ESP
WHEREAS
ESP
GIVES
DATA
TO
DETERMINE
WHICH
OBJECTS
ARE
IN
THE
IMAGE
PEEKABOOM
CAN
AUGMENT
THIS
DATA
WITH
INFORMATION
ABOUT
WHERE
IN
THE
IMAGE
OBJECTS
ARE
LOCATED
IN
TERMS
OF
GAME
MECHANICS
PEEKABOOM
IS
DIFFERENT
FROM
THE
ESP
GAME
IN
SEVERAL
WAYS
FIRST
PEEKABOOM
IS
ASYMMETRIC
WHEREAS
BOTH
PLAYERS
IN
THE
ESP
GAME
ARE
PERFORMING
THE
SAME
ROLE
PLAYERS
OF
PEEKABOOM
ALTERNATE
IN
PERFORMING
DIFFERENT
ROLES
SECOND
PEEKABOOM
ALLOWS
A
SIGNIFICANTLY
HIGHER
LEVEL
OF
INTERACTION
AMONG
THE
PLAYERS
WHEREAS
IN
THE
ESP
GAME
PLAYERS
CANNOT
COMMUNICATE
AT
ALL
IN
PEEKABOOM
ONE
OF
THE
PLAYERS
CAN
FREELY
COMMUNICATE
WITH
THE
OTHER
THIRD
THE
USAGE
OF
HINT
BUTTONS
HAS
PROVEN
VERY
SUCCESSFUL
IN
PEEKABOOM
AND
SUCH
BUTTONS
COULD
AS
WELL
BE
INCORPORATED
INTO
ESP
SUCH
DIFFERENCES
IN
GAME
MECHANICS
REFLECT
THE
DIFFERENCE
IN
PURPOSE
OF
PEEKABOOM
AND
ESP
THE
OPEN
MIND
INITIATIVE
PERHAPS
LESS
SO
PEEKABOOM
IS
ALSO
SIMILAR
AT
LEAST
IN
SPIRIT
TO
THE
OPEN
MIND
INITIATIVE
E
G
A
WORLDWIDE
EFFORT
TO
DEVELOP
INTELLIGENT
SOFTWARE
OPEN
MIND
COLLECTS
DATA
FROM
REGULAR
INTERNET
USERS
REFERRED
TO
AS
NETIZENS
AND
FEEDS
IT
TO
MACHINE
LEARNING
ALGORITHMS
VOLUNTEERS
PARTICIPATE
BY
ANSWERING
QUESTIONS
AND
TEACHING
CONCEPTS
TO
COMPUTER
PROGRAMS
PEEKABOOM
IS
SIMILAR
TO
OPEN
MIND
IN
THAT
WE
USE
REGULAR
PEOPLE
ON
THE
INTERNET
TO
ANNOTATE
IMAGES
HOWEVER
AS
WITH
THE
ESP
GAME
WE
PUT
MUCH
GREATER
EMPHASIS
ON
OUR
METHOD
BEING
FUN
WE
DON
T
EXPECT
VOLUNTEERS
TO
ANNOTATE
MILLIONS
OF
IMAGES
ON
THE
WEB
WE
EXPECT
IMAGES
TO
BE
ANNOTATED
BECAUSE
PEOPLE
WANT
TO
PLAY
OUR
GAME
WHEREAS
A
TYPICAL
OPEN
MIND
ACTIVITY
WOULD
ASK
PARTICIPANTS
TO
POINT
TO
THE
OBJECT
IN
QUESTION
WE
TRANSFORM
THE
ACTIVITY
INTO
A
TWO
PLAYER
GAME
IN
WHICH
PLAYERS
ARE
NOT
EVEN
ASKED
TO
POINT
TO
THE
OBJECT
THEY
DO
SO
ONLY
AS
A
SIDE
EFFECT
OF
PLAYING
THE
GAME
LABELME
LABELME
IS
A
WEB
BASED
TOOL
FOR
IMAGE
ANNOTATION
ANYBODY
CAN
ANNOTATE
DATA
USING
THIS
TOOL
AND
THUS
CONTRIBUTE
TO
CONSTRUCTING
A
LARGE
DATABASE
OF
ANNOTATED
OBJECTS
THE
INCENTIVE
TO
ANNOTATE
DATA
IS
THE
DATA
ITSELF
YOU
CAN
ONLY
HAVE
ACCESS
TO
THE
DATABASE
ONCE
YOU
HAVE
ANNOTATED
A
CERTAIN
NUMBER
OF
IMAGES
THE
MAIN
DIFFERENCE
BETWEEN
PEEKABOOM
AND
LABELME
IS
THE
GAME
ASPECT
WHEREAS
LABELME
SIMPLY
ASKS
USERS
TO
ANNOTATE
AN
IMAGE
PEEKABOOM
TRANSFORMS
THE
PROCESS
INTO
AN
ENJOYABLE
GAME
LABELME
RELIES
ON
PEOPLE
DESIRE
TO
HELP
AND
THUS
ASSUMES
THAT
THE
ENTERED
DATA
IS
CORRECT
ON
THE
OTHER
HAND
PEEKABOOM
HAS
MULTIPLE
MECHANISMS
TO
PREVENT
PLAYERS
FROM
POLLUTING
THE
DATA
INTERACTIVE
MACHINE
LEARNING
ANOTHER
AREA
OF
RELATED
WORK
IS
THAT
OF
INTERACTIVELY
TRAINING
MACHINE
LEARNING
ALGORITHMS
E
G
IN
THESE
SYSTEMS
A
USER
IS
GIVEN
IMMEDIATE
FEEDBACK
ABOUT
HOW
WELL
AN
ALGORITHM
IS
LEARNING
FROM
THE
EXAMPLES
PROVIDED
BY
THEM
AS
WITH
LABELME
PEEKABOOM
DIFFERS
FROM
THESE
SYSTEMS
IN
THE
GAMING
ASPECT
AS
WELL
AS
IN
THE
ASSUMPTION
THAT
OUR
USERS
ARE
INTERESTED
IN
TRAINING
AN
ALGORITHM
CONCLUSIONS
AND
FUTURE
WORK
PEEKABOOM
IS
A
NOVEL
COMPLETE
GAME
ARCHITECTURE
FOR
COLLECTING
IMAGE
METADATA
SEGMENTING
OBJECTS
IN
IMAGES
IS
A
UNIQUE
CHALLENGE
AND
WE
HAVE
TAILORED
A
GAME
SPECIFICALLY
TO
THIS
END
IN
THE
VERY
NEAR
FUTURE
WE
WOULD
LIKE
TO
MAKE
OUR
PIECES
OF
DATA
AVAILABLE
TO
THE
WORLD
BY
FORMATTING
IT
AS
AN
IMAGE
SEGMENTATION
LIBRARY
LIKE
THE
ESP
GAME
PEEKABOOM
ENCOMPASSES
MUCH
MORE
THAN
JUST
A
JAVA
APPLET
DELIVERED
FROM
A
WEBSITE
RATHER
THE
IDEAS
BEHIND
THE
DESIGN
AND
IMPLEMENTATION
OF
THE
GAME
GENERALIZE
TO
A
WAY
OF
HARNESSING
AND
DIRECTING
THE
POWER
OF
THE
MOST
INTRICATE
COMPUTING
DEVICE
IN
THE
WORLD
THE
HUMAN
MIND
SOME
DAY
COMPUTERS
WILL
BE
ABLE
TO
SEGMENT
OBJECTS
IN
IMAGES
UNASSISTED
BUT
THAT
DAY
IS
NOT
TODAY
TODAY
WE
HAVE
ENGINES
LIKE
PEEKABOOM
THAT
USE
THE
WISDOM
OF
HUMANS
TO
HELP
NAÏVE
COMPUTERS
GET
TO
THAT
POINT
THE
ACTUAL
PROCESS
OF
MAKING
COMPUTERS
SMARTER
GIVEN
SEGMENTATION
METADATA
IS
BEYOND
THE
SCOPE
OF
THIS
PAPER
SINCE
IT
WOULD
REQUIRE
A
FAR
MORE
SOPHISTICATED
INTERPRETATION
OF
THE
DATA
THAN
THE
SIMPLE
BOUNDING
BOX
DERIVATION
WE
HAVE
PRESENTED
THUS
WE
SEE
GREAT
POTENTIAL
IN
FUTURE
WORK
AT
THE
CROSSROADS
OF
HUMAN
COMPUTER
INTERACTION
AND
ARTIFICIAL
INTELLIGENCE
WHERE
THE
OUTPUT
OF
OUR
INTERACTIVE
SYSTEM
HELPS
ADVANCE
THE
STATE
OF
THE
ART
IN
COMPUTER
VISION
ARTICULATED
POSE
ESTIMATION
WITH
FLEXIBLE
MIXTURES
OF
PARTS
YI
YANG
DEVA
RAMANAN
DEPT
OF
COMPUTER
SCIENCE
UNIVERSITY
OF
CALIFORNIA
IRVINE
DRAMANAN
ICS
UCI
EDU
ABSTRACT
WE
DESCRIBE
A
METHOD
FOR
HUMAN
POSE
ESTIMATION
IN
STATIC
IMAGES
BASED
ON
A
NOVEL
REPRESENTATION
OF
PART
MOD
ELS
NOTABLY
WE
DO
NOT
USE
ARTICULATED
LIMB
PARTS
BUT
RATHER
CAPTURE
ORIENTATION
WITH
A
MIXTURE
OF
TEMPLATES
FOR
EACH
PART
WE
DESCRIBE
A
GENERAL
FLEXIBLE
MIXTURE
MODEL
FOR
CAPTURING
CONTEXTUAL
CO
OCCURRENCE
RELATIONS
BETWEEN
PARTS
AUGMENTING
STANDARD
SPRING
MODELS
THAT
ENCODE
SPA
TIAL
RELATIONS
WE
SHOW
THAT
SUCH
RELATIONS
CAN
CAPTURE
NO
TIONS
OF
LOCAL
RIGIDITY
WHEN
CO
OCCURRENCE
AND
SPATIAL
RELA
TIONS
ARE
TREE
STRUCTURED
OUR
MODEL
CAN
BE
EFFICIENTLY
OPTI
MIZED
WITH
DYNAMIC
PROGRAMMING
WE
PRESENT
EXPERIMENTAL
RESULTS
ON
STANDARD
BENCHMARKS
FOR
POSE
ESTIMATION
THAT
IN
DICATE
OUR
APPROACH
IS
THE
STATE
OF
THE
ART
SYSTEM
FOR
POSE
ESTIMATION
OUTPERFORMING
PAST
WORK
BY
WHILE
BEING
ORDERS
OF
MAGNITUDE
FASTER
INTRODUCTION
WE
EXAMINE
THE
TASK
OF
HUMAN
POSE
ESTIMATION
IN
STATIC
IMAGES
A
WORKING
TECHNOLOGY
WOULD
IMMEDIATELY
IMPACT
MANY
KEY
VISION
TASKS
SUCH
AS
IMAGE
UNDERSTANDING
AND
ACTIVITY
RECOGNITION
AN
INFLUENTIAL
APPROACH
IS
THE
PICTO
RIAL
STRUCTURE
FRAMEWORK
WHICH
DECOMPOSES
THE
AP
PEARANCE
OF
OBJECTS
INTO
LOCAL
PART
TEMPLATES
TOGETHER
WITH
FIGURE
ON
THE
LEFT
WE
SHOW
THE
CLASSIC
ARTICULATED
LIMB
MODEL
OF
MARR
AND
NISHIHARA
IN
THE
MIDDLE
WE
SHOW
DIFFERENT
ORIENTATION
AND
FORESHORTENING
STATES
OF
A
LIMB
EACH
OF
WHICH
IS
EVALUATED
SEPARATELY
IN
CLASSIC
ARTICULATED
BODY
MODELS
ON
THE
RIGHT
WE
APPROXIMATE
THESE
TRANS
FORMATIONS
WITH
A
MIXTURE
OF
NON
ORIENTED
PICTORIAL
STRUC
TURES
IN
THIS
CASE
TUNED
TO
REPRESENT
NEAR
VERTICAL
AND
NEAR
HORIZONTAL
LIMBS
GEOMETRIC
CONSTRAINTS
ON
PAIRS
OF
PARTS
OFTEN
VISUALIZED
AS
SPRINGS
WHEN
PARTS
ARE
PARAMETERIZED
BY
PIXEL
LOCATION
AND
ORIENTATION
THE
RESULTING
STRUCTURE
CAN
MODEL
ARTICULATION
THIS
HAS
BEEN
THE
DOMINANT
APPROACH
TO
HUMAN
POSE
ESTIMA
TION
IN
CONTRAST
TRADITIONAL
MODELS
FOR
OBJECT
RECOGNITION
USE
PARTS
PARAMETERIZED
SOLELY
BY
LOCATION
WHICH
SIMPLIFIES
BOTH
INFERENCE
AND
LEARNING
SUCH
MODELS
HAVE
BEEN
SHOWN
TO
BE
VERY
SUCCESSFUL
FOR
OBJECT
RECOGNITION
IN
THIS
WORK
WE
INTRODUCE
A
NOVEL
UNIFIED
REPRESENTATION
FOR
BOTH
MODELS
THAT
PRODUCES
STATE
OF
THE
ART
RESULTS
FOR
HUMAN
POSE
ESTIMATION
REPRESENTATIONS
FOR
ARTICULATED
POSE
FULL
BODY
POSE
ESTIMATION
IS
DIFFICULT
BECAUSE
OF
THE
MANY
DEGREES
OF
FREE
DOMS
TO
BE
ESTIMATED
MOREOVER
LIMBS
VARY
GREATLY
IN
AP
PEARANCE
DUE
TO
CHANGES
IN
CLOTHING
AND
BODY
SHAPE
AS
WELL
AS
CHANGES
IN
VIEWPOINT
MANIFESTED
IN
IN
PLANE
ORIENTATIONS
AND
FORESHORTENING
THESE
DIFFICULTIES
COMPLICATE
INFERENCE
SINCE
ONE
MUST
TYPICALLY
SEARCH
IMAGES
WITH
A
LARGE
NUM
BER
OF
ROTATED
AND
FORESHORTENED
TEMPLATES
WE
ADDRESS
THESE
PROBLEMS
BY
INTRODUCING
A
NOVEL
BUT
SIMPLE
REPRESEN
TATION
FOR
MODELING
A
FAMILY
OF
AFFINELY
WARPED
TEMPLATES
A
MIXTURE
OF
NON
ORIENTED
PICTORIAL
STRUCTURES
FIG
WE
EMPIRICALLY
DEMONSTRATE
THAT
SUCH
APPROXIMATIONS
CAN
OUT
PERFORM
EXPLICITLY
ARTICULATED
PARTS
BECAUSE
MIXTURE
MOD
ELS
CAN
CAPTURE
ORIENTATION
SPECIFIC
STATISTICS
OF
BACKGROUND
FEATURES
FIG
REPRESENTATIONS
FOR
OBJECTS
CURRENT
OBJECT
RECOGNI
TION
SYSTEMS
ARE
BUILT
ON
RELATIVELY
SIMPLE
STRUCTURES
ENCOD
ING
MIXTURES
OF
STAR
MODELS
DEFINED
OVER
TENS
OF
PARTS
OR
IMPLICITLY
DEFINED
SHAPE
MODELS
BUILT
ON
HUNDREDS
OF
PARTS
IN
ORDER
TO
MODEL
THE
VARIED
APPEARANCE
OF
OBJECTS
DUE
TO
DEFORMATION
VIEWPOINT
ETC
WE
ARGUE
THAT
ONE
WILL
NEED
VOCABULARIES
OF
HUNDREDS
OR
THOUSANDS
OF
PARTS
WHERE
ONLY
A
SUBSET
ARE
INSTANCED
AT
A
TIME
WE
AUGMENT
CLAS
SIC
SPRING
MODELS
WITH
CO
OCCURRENCE
CONSTRAINTS
THAT
FAVOR
PARTICULAR
COMBINATIONS
OF
PARTS
SUCH
CONSTRAINTS
CAN
CAP
TURE
NOTIONS
OF
LOCAL
RIGIDITY
FOR
EXAMPLE
TWO
PARTS
ON
THE
SAME
LIMB
SHOULD
BE
CONSTRAINED
TO
HAVE
THE
SAME
ORIENTA
TION
STATE
FIG
WE
SHOW
THAT
ONE
CAN
EMBED
SUCH
CON
STRAINTS
IN
A
TREE
RELATIONAL
GRAPH
THAT
PRESERVES
TRACTABILITY
AN
OPEN
CHALLENGE
IS
THAT
OF
LEARNING
SUCH
COMPLEX
REPRE
HORIZONTAL
MORE
SUSCEPTIBLE
TO
OVERFITTING
TO
STATISTICS
OF
A
PARTICULAR
DATASET
AS
WARNED
BY
DIAGONAL
VERTICAL
AN
ALTERNATE
FAMILY
OF
TECHNIQUES
HAS
EXPLORED
THE
TRADE
OFF
BETWEEN
GENERATIVE
AND
DISCRIMINATIVE
MODELS
TRAINED
EXPLICITLY
FOR
POSE
ESTIMATION
APPROACHES
INCLUDE
CONDI
TIONAL
RANDOM
FIELDS
AND
MARGIN
BASED
OR
BOOSTED
DE
TECTORS
A
FINAL
CRUCIAL
ISSUE
IS
THAT
OF
FEATURE
FIGURE
WE
PLOT
THE
AVERAGE
HOG
FEATURE
AS
A
POLAR
HIS
TOGRAM
OVER
GRADIENT
ORIENTATION
CHANNELS
AS
COMPUTED
FROM
THE
ENTIRE
PASCAL
DATASET
WE
SEE
THAT
ON
AVERAGE
IMAGES
CONTAIN
MORE
HORIZONTAL
GRADIENTS
THAN
VER
TICAL
GRADIENTS
AND
MUCH
STRONGER
HORIZONTAL
GRADIENTS
AS
COMPARED
TO
DIAGONAL
GRADIENTS
THIS
MEANS
THAT
GRADIENT
STATISTICS
ARE
NOT
ORIENTATION
INVARIANT
IN
PRACTICAL
TERMS
WE
ARGUE
THAT
IT
IS
EASIER
TO
FIND
DIAGONAL
LIMBS
AS
OPPOSED
TO
HORIZONTAL
ONES
BECAUSE
ONE
IS
LESS
LIKELY
TO
BE
CONFUSED
BY
DIAGONAL
BACKGROUND
CLUTTER
ARTICULATED
LIMB
MODELS
OBTAINED
BY
ROTATING
A
SINGLE
TEMPLATE
CANNOT
EXPLOIT
SUCH
ORIENTATION
SPECIFIC
CUES
ON
THE
OTHER
HAND
OUR
MIXTURE
MODELS
ARE
TUNED
TO
DETECT
PARTS
AT
PARTICULAR
ORIENTATIONS
AND
SO
CAN
EXPLOIT
SUCH
STATISTICS
SENTATIONS
FROM
DATA
AS
IN
WE
CONCLUDE
THAT
SUPER
VISION
IS
A
KEY
INGREDIENT
FOR
LEARNING
STRUCTURED
RELATIONAL
MODELS
WE
DEMONSTRATE
RESULTS
ON
THE
DIFFICULT
TASK
OF
POSE
ESTI
MATION
WE
USE
TWO
STANDARD
BENCHMARK
DATASETS
WE
OUTPERFORM
ALL
PUBLISHED
PAST
WORK
ON
BOTH
DATASETS
RE
DUCING
ERROR
BY
UP
TO
WE
DO
SO
WITH
A
NOVEL
BUT
SIM
PLE
REPRESENTATION
THAT
IS
ORDERS
OF
MAGNITUDE
FASTER
THAN
DESCRIPTORS
PAST
WORK
HAS
EXPLORED
THE
USE
OF
SUPERPIXELS
CONTOURS
FOREGROUND
BACKGROUND
COLOR
MODELS
AND
GRADIENT
DESCRIPTORS
IN
TERMS
OF
OBJECT
DETECTION
OUR
WORK
IS
MOST
SIMILAR
TO
PICTORIAL
STRUCTURE
MODELS
THAT
REASON
ABOUT
MIXTURES
OF
PARTS
WE
SHOW
THAT
OUR
MODEL
GENERALIZES
SUCH
REPRESENTATIONS
IN
SEC
OUR
MODEL
WHEN
INSTANCED
AS
A
TREE
CAN
BE
WRITTEN
AS
A
RECURSIVE
GRAMMAR
OF
PARTS
MODEL
LET
US
WRITE
I
FOR
AN
IMAGE
PI
X
Y
FOR
THE
PIXEL
LOCATION
OF
PART
I
AND
TI
FOR
THE
MIXTURE
COMPONENT
OF
PART
I
WE
WRITE
I
K
PI
L
AND
TI
T
WE
CALL
TI
THE
TYPE
OF
PART
I
OUR
MOTI
VATING
EXAMPLE
OF
TYPES
INCLUDE
ORIENTATIONS
OF
A
PART
E
G
A
VERTICAL
VERSUS
HORIZONTALLY
ORIENTED
HAND
BUT
TYPES
MAY
SPAN
SEMANTIC
CLASSES
AN
OPEN
VERSUS
CLOSED
HAND
FOR
NO
TATIONAL
CONVENIENCE
WE
DEFINE
THE
LACK
OF
SUBSCRIPT
TO
INDI
CATE
A
SET
SPANNED
BY
THAT
SUBSCRIPT
E
G
T
TK
CO
OCCURRENCE
MODEL
TO
SCORE
OF
A
CONFIGURATION
OF
PARTS
WE
FIRST
DEFINE
A
COMPATIBILITY
FUNCTION
FOR
PART
TYPES
THAT
FACTORS
INTO
A
SUM
OF
LOCAL
AND
PAIRWISE
SCORES
PREVIOUS
APPROACHES
OUR
MODEL
REQUIRES
ROUGHLY
SEC
OND
TO
PROCESS
A
TYPICAL
BENCHMARK
IMAGE
ALLOWING
FOR
THE
POSSIBILITY
OF
REAL
TIME
PERFORMANCE
WITH
FURTHER
SPEEDUPS
TI
TI
TJ
I
IJ
I
V
IJ
E
SUCH
AS
CASCADED
OR
PARALLELIZED
IMPLEMENTATIONS
THE
PARAMETER
BTI
FAVORS
PARTICULAR
TYPE
ASSIGNMENTS
FOR
RELATED
WORK
POSE
ESTIMATION
HAS
TYPICALLY
BEEN
ADDRESSED
IN
THE
VIDEO
DOMAIN
DATING
BACK
TO
CLASSIC
MODEL
BASED
APPROACHES
OF
O
ROURKE
AND
BADLER
HOGG
ROHR
RECENT
WORK
HAS
EXAMINED
THE
PROBLEM
FOR
STATIC
IMAGES
ASSUM
ING
THAT
SUCH
TECHNIQUES
WILL
BE
NEEDED
TO
INITIALIZE
VIDEO
BASED
ARTICULATED
TRACKERS
PROBABILISTIC
FORMULATIONS
ARE
COMMON
ONE
AREA
OF
RESEARCH
IS
THE
ENCODING
OF
SPATIAL
STRUCTURE
TREE
MODELS
ARE
EFFICIENT
AND
ALLOW
FOR
EFFICIENT
INFERENCE
BUT
ARE
PLAGUED
BY
THE
WELL
KNOWN
PHENOM
ENA
OF
DOUBLE
COUNTING
LOOPY
MODELS
REQUIRE
APPROXIMATE
PART
I
WHILE
THE
PAIRWISE
PARAMETER
BTI
TJ
FAVORS
PARTICULAR
CO
OCCURRENCES
OF
PART
TYPES
FOR
EXAMPLE
IF
PART
TYPES
COR
RESPOND
TO
ORIENTATIONS
AND
PART
I
AND
J
ARE
ON
THE
SAME
RIGID
LIMB
THEN
BTI
TJ
WOULD
FAVOR
CONSISTENT
ORIENTATION
ASSIGN
MENTS
WE
WRITE
G
V
E
FOR
A
K
NODE
RELATIONAL
GRAPH
WHOSE
EDGES
SPECIFY
WHICH
PAIRS
OF
PARTS
ARE
CONSTRAINED
TO
HAVE
CONSISTENT
RELATIONS
WE
CAN
NOW
WRITE
THE
FULL
SCORE
ASSOCIATED
WITH
A
CON
FIGURATION
OF
PART
TYPES
AND
POSITIONS
I
P
T
T
WTI
Φ
I
PI
WTI
TJ
Ψ
PI
PJ
INFERENCE
STRATEGIES
SUCH
AS
IMPORTANCE
SAMPLING
LOOPY
BELIEF
PROPAGATION
OR
ITERATIVE
APPROXIMATIONS
I
I
V
IJ
IJ
E
RECENT
WORK
HAS
SUGGESTED
THAT
BRANCH
AND
BOUND
AL
GORITHMS
WITH
TREE
BASED
LOWER
BOUNDS
CAN
GLOBALLY
SOLVE
SUCH
PROBLEMS
ANOTHER
APPROACH
TO
TACKLING
THE
DOUBLE
COUNTING
PHENOMENA
IS
THE
USE
OF
STRONGER
POSE
PRI
ORS
ADVOCATED
BY
HOWEVER
SUCH
APPROACHES
MAYBE
WHERE
Φ
I
PI
IS
A
FEATURE
VECTOR
E
G
HOG
DESCRIPTOR
EXTRACTED
FROM
PIXEL
LOCATION
PI
IN
IMAGE
I
WE
WRITE
Ψ
PI
J
DY
YI
YJ
THE
RELATIVE
LOCATION
OF
PART
I
WITH
RESPECT
TO
J
NOTABLY
THIS
RELATIVE
LOCATION
IS
DEFINED
WITH
RESPECT
TO
THE
PIXEL
GRID
AND
NOT
THE
ORIENTATION
OF
PART
I
AS
IN
CLASSIC
ARTICULATED
PICTORIAL
STRUCTURES
APPEARANCE
MODEL
THE
FIRST
SUM
IN
IS
AN
APPEAR
ANCE
MODEL
THAT
COMPUTES
THE
LOCAL
SCORE
OF
PLACING
A
TEM
PLATE
WTI
FOR
PART
I
TUNED
FOR
TYPE
TI
AT
LOCATION
PI
DEFORMATION
MODEL
THE
SECOND
TERM
CAN
BE
INTER
A
TREE
THIS
CAN
BE
DONE
EFFICIENTLY
WITH
DYNAMIC
PROGRAM
MING
LET
KIDS
I
BE
THE
SET
OF
CHILDREN
OF
PART
I
IN
G
WE
COMPUTE
THE
MESSAGE
PART
I
PASSES
TO
ITS
PARENT
J
BY
THE
FOL
LOWING
SCOREI
TI
PI
BTI
WI
Φ
I
PI
MK
TI
PI
PRETED
AS
A
SWITCHING
SPRING
MODEL
THAT
CONTROLS
THE
RELA
TIVE
PLACEMENT
OF
PART
I
AND
J
BY
SWITCHING
BETWEEN
A
COL
LECTION
OF
SPRINGS
EACH
SPRING
IS
TAILORED
FOR
A
PARTICULAR
I
TI
K
KIDS
I
PAIR
OF
TYPES
TI
TJ
AND
IS
PARAMETERIZED
BY
ITS
REST
LOCA
TION
AND
RIGIDITY
WHICH
ARE
ENCODED
BY
WTI
TJ
MI
TJ
PJ
MAX
BTI
TJ
TI
MAX
SCORE
T
P
WTI
TJ
Ψ
P
P
SPECIAL
CASES
WE
NOW
DESCRIBE
VARIOUS
SPECIAL
CASES
OF
OUR
MODEL
WHICH
HAVE
APPEARED
IN
THE
LITERATURE
ONE
OBVIOUS
CASE
IS
T
IN
WHICH
CASE
OUR
MODEL
REDUCES
TO
A
STANDARD
PICTORIAL
STRUCTURE
MORE
INTERESTING
CASES
ARE
BELOW
SEMANTIC
PART
MODELS
ARGUE
THAT
PART
APPEARANCES
SHOULD
CAPTURE
SEMANTIC
CLASSES
AND
NOT
VISUAL
CLASSES
THIS
CAN
BE
DONE
WITH
A
TYPE
MODEL
CONSIDER
A
FACE
MODEL
WITH
EYE
AND
MOUTH
PARTS
ONE
MAY
WANT
TO
MODEL
DIFFERENT
TYPES
OF
EYES
OPEN
AND
CLOSED
AND
MOUTHS
SMILING
AND
FROWNING
THE
SPATIAL
RELATIONSHIP
BETWEEN
THE
TWO
DOES
NOT
LIKELY
DEPEND
ON
THEIR
TYPE
BUT
OPEN
EYES
MAY
TEND
TO
CO
OCCUR
WITH
SMILING
MOUTHS
THIS
CAN
BE
OBTAINED
AS
A
SPECIAL
CASE
OF
OUR
MODEL
BY
USING
A
SINGLE
SPRING
FOR
ALL
TYPES
OF
A
PARTICULAR
PAIR
OF
PARTS
WTI
TJ
WIJ
MIXTURES
OF
DEFORMABLE
PARTS
DEFINE
A
MIXTURE
OF
MODELS
WHERE
EACH
MODEL
IS
A
STAR
BASED
PICTORIAL
STRUCTURE
THIS
CAN
ACHIEVED
BY
RESTRICTING
THE
CO
OCCURRENCE
MODEL
TO
ALLOW
FOR
ONLY
GLOBALLY
CONSISTENT
TYPES
I
I
IJ
I
J
I
COMPUTES
THE
LOCAL
SCORE
OF
PART
I
AT
ALL
PIXEL
LOCATIONS
PI
AND
FOR
ALL
POSSIBLE
TYPES
TI
BY
COLLECTING
MESSAGES
FROM
THE
CHILDREN
OF
I
COMPUTES
FOR
EVERY
LOCATION
AND
POS
SIBLE
TYPE
OF
PART
J
THE
BEST
SCORING
LOCATION
AND
TYPE
OF
ITS
CHILD
PART
I
ONCE
MESSAGES
ARE
PASSED
TO
THE
ROOT
PART
I
REPRESENTS
THE
BEST
SCORING
CONFIG
URATION
FOR
EACH
ROOT
POSITION
AND
TYPE
ONE
CAN
USE
THESE
ROOT
SCORES
TO
GENERATE
MULTIPLE
DETECTIONS
IN
IMAGE
I
BY
THRESHOLDING
THEM
AND
APPLYING
NON
MAXIMUM
SUPPRESSION
NMS
BY
KEEPING
TRACK
OF
THE
ARGMAX
INDICES
ONE
CAN
BACKTRACK
TO
FIND
THE
LOCATION
AND
TYPE
OF
EACH
PART
IN
EACH
MAXIMAL
CONFIGURATION
COMPUTATION
THE
COMPUTATIONALLY
TAXING
PORTION
OF
DYNAMIC
PROGRAMMING
IS
ONE
HAS
TO
LOOP
OVER
L
T
POSSIBLE
PARENT
LOCATIONS
AND
TYPES
AND
COMPUTE
A
MAX
OVER
L
T
POSSIBLE
CHILD
LOCATIONS
AND
TYPES
MAKING
THE
COMPUTATION
O
FOR
EACH
PART
WHEN
Ψ
PI
PJ
IS
A
QUADRATIC
FUNCTION
AS
IS
THE
CASE
FOR
US
THE
INNER
MAX
IMIZATION
IN
CAN
BE
EFFICIENTLY
COMPUTED
FOR
EACH
COM
BINATION
OF
TI
AND
TJ
IN
O
L
WITH
A
MAX
CONVOLUTION
OR
DISTANCE
TRANSFORM
SINCE
ONE
HAS
TO
PERFORM
T
DIS
TI
TJ
IJ
IF
TI
TJ
OTHERWISE
TANCE
TRANSFORMS
MESSAGE
PASSING
REDUCES
TO
O
LT
PER
PART
ARTICULATION
IN
OUR
EXPERIMENTS
WE
EXPLORE
A
SIMPLI
FIED
VERSION
OF
WITH
A
REDUCED
SET
OF
SPRINGS
SPECIAL
CASES
MODEL
MAINTAINS
ONLY
A
SINGLE
SPRING
PER
PART
SO
MESSAGE
PASSING
REDUCES
TO
O
L
MODELS
AND
MAINTAIN
ONLY
T
SPRINGS
PER
PART
REDUCING
MES
WTI
TJ
WTI
SAGE
PASSING
TO
O
LT
IT
IS
WORTHWHILE
TO
NOTE
THAT
OUR
IJ
IJ
THE
ABOVE
SIMPLIFICATION
STATES
THAT
THE
RELATIVE
LOCATION
OF
PART
WITH
RESPECT
TO
ITS
PARENT
IS
DEPENDANT
ON
PART
TYPE
BUT
NOT
PARENT
TYPE
FOR
EXAMPLE
LET
I
BE
A
HAND
PART
J
ITS
PAR
ENT
ELBOW
PART
AND
ASSUME
PART
TYPES
CAPTURE
ORIENTATION
THE
ABOVE
RELATIONAL
MODEL
STATES
THAT
A
SIDEWAYS
ORIENTED
HAND
SHOULD
TEND
TO
LIE
NEXT
TO
THE
ELBOW
WHILE
A
DOWNWARD
ORIENTED
HAND
SHOULD
LIE
BELOW
THE
ELBOW
REGARDLESS
OF
THE
ORIENTATION
OF
THE
UPPER
ARM
INFERENCE
INFERENCE
CORRESPONDS
TO
MAXIMIZING
X
P
T
FROM
OVER
P
AND
T
WHEN
THE
RELATIONAL
GRAPH
G
V
E
IS
ARTICULATED
MODEL
IS
NO
MORE
COMPUTATIONALLY
COMPLEX
THAN
THE
DEFORMABLE
MIXTURES
OF
PARTS
IN
BUT
IS
CONSIDERABLY
MORE
FLEXIBLE
AS
WE
SHOW
IN
OUR
EXPERIMENTS
IN
PRACTICE
T
IS
SMALL
IN
OUR
EXPERIMENTS
AND
THE
DISTANCE
TRANS
FORM
IS
QUITE
EFFICIENT
SO
THE
COMPUTATION
TIME
IS
DOMINATED
BY
COMPUTING
THE
LOCAL
SCORES
OF
EACH
TYPE
SPECIFIC
APPEAR
ANCE
MODELS
WTI
Φ
I
PI
SINCE
THIS
SCORE
IS
LINEAR
IT
CAN
BE
EFFICIENTLY
COMPUTED
FOR
ALL
POSITIONS
PI
BY
OPTIMIZED
CONVOLUTION
ROUTINES
LEARNING
WE
ASSUME
A
SUPERVISED
LEARNING
PARADIGM
GIVEN
LA
BELED
POSITIVE
EXAMPLES
IN
PN
TN
AND
NEGATIVE
EXAMPLES
IN
WE
WILL
DEFINE
A
STRUCTURED
PREDICTION
OBJECTIVE
FUNC
TION
SIMILAR
TO
THOSE
PROPOSED
IN
TO
DO
SO
LET
US
WRITE
ZN
PN
TN
AND
NOTE
THAT
THE
SCORING
FUNCTION
IS
LINEAR
IN
MODEL
PARAMETERS
Β
W
B
AND
SO
CAN
BE
WRIT
TEN
AS
I
Z
Β
Φ
I
Z
WE
WOULD
LEARN
A
MODEL
OF
THE
FORM
DERIVING
PART
TYPE
FROM
POSITION
ASSUME
THAT
OUR
NTH
TRAINING
IMAGE
IN
HAS
LABELED
JOINT
POSITIONS
PN
LET
PN
BE
THE
RELATIVE
POSITION
OF
PART
I
WITH
RESPECT
TO
ITS
PARENT
IN
IMAGE
IN
FOR
EACH
PART
I
WE
CLUSTER
ITS
RELATIVE
POSITION
OVER
THE
TRAINING
SET
PN
N
TO
OBTAIN
T
CLUSTERS
WE
USE
K
MEANS
WITH
K
T
EACH
CLUSTER
CORRESPONDS
TO
A
COL
ARG
MIN
Β
Β
C
Ξ
LECTION
OF
PART
INSTANCES
WITH
CONSISTENT
RELATIVE
LOCATIONS
W
ΞI
N
T
N
POS
Β
Φ
IN
ZN
ΞN
N
NEG
Z
Β
Φ
IN
Z
ΞN
THE
ABOVE
CONSTRAINT
STATES
THAT
POSITIVE
EXAMPLES
SHOULD
SCORE
BETTER
THAN
THE
MARGIN
WHILE
NEGATIVE
EXAMPLES
FOR
ALL
CONFIGURATIONS
OF
PART
POSITIONS
AND
TYPES
SHOULD
SCORE
LESS
THAN
THE
OBJECTIVE
FUNCTION
PENALIZES
VIO
LATIONS
OF
THESE
CONSTRAINTS
USING
SLACK
VARIABLES
ΞN
DETECTION
VS
POSE
ESTIMATION
TRADITIONAL
STRUCTURED
PREDICTION
TASKS
DO
NOT
REQUIRE
AN
EXPLICIT
NEGATIVE
TRAINING
SET
AND
INSTEAD
GENERATE
NEGATIVE
CONSTRAINTS
FROM
POSITIVE
EXAMPLES
WITH
MIS
ESTIMATED
LABELS
Z
THIS
CORRESPONDS
TO
TRAINING
A
MODEL
THAT
TENDS
TO
SCORE
A
GROUND
TRUTH
POSE
HIGHLY
AND
ALTERNATE
POSES
POORLY
WHILE
THIS
TRANSLATES
DIRECTLY
TO
A
POSE
ESTIMATION
TASK
OUR
ABOVE
FORMULATION
ALSO
INCLUDES
A
DETECTION
COMPONENT
IT
TRAINS
A
MODEL
THAT
SCORES
HIGHLY
ON
GROUND
TRUTH
POSES
BUT
GENERATES
LOW
SCORES
ON
IMAGES
WITHOUT
PEOPLE
WE
FIND
THE
ABOVE
TO
WORK
WELL
FOR
BOTH
POSE
ESTIMATION
AND
PERSON
DETECTION
OPTIMIZATION
THE
ABOVE
OPTIMIZATION
IS
A
QUADRATIC
PROGRAM
QP
WITH
AN
EXPONENTIAL
NUMBER
OF
CONSTRAINTS
SINCE
THE
SPACE
OF
Z
IS
LT
K
FORTUNATELY
ONLY
A
SMALL
MI
NORITY
OF
THE
CONSTRAINTS
WILL
BE
ACTIVE
ON
TYPICAL
PROBLEMS
E
G
THE
SUPPORT
VECTORS
MAKING
THEM
SOLVABLE
IN
PRAC
TICE
THIS
FORM
OF
LEARNING
PROBLEM
IS
KNOWN
AS
A
STRUCTURAL
SVM
AND
THERE
EXISTS
MANY
WELL
TUNED
SOLVERS
SUCH
AS
THE
CUTTING
PLANE
SOLVER
OF
SVMSTRUCT
AND
THE
STOCHASTIC
GRADIENT
DESCENT
SOLVER
IN
WE
FOUND
GOOD
RESULTS
BY
IM
PLEMENTING
OUR
OWN
DUAL
COORDINATE
DESCENT
SOLVER
WHICH
WE
WILL
DESCRIBE
IN
AN
UPCOMING
TECH
REPORT
LEARNING
IN
PRACTICE
MOST
HUMAN
POSE
DATASETS
INCLUDE
IMAGES
WITH
LABELED
JOINT
POSITIONS
WE
DEFINE
PARTS
TO
BE
LOCATED
AT
JOINTS
SO
THESE
PROVIDE
PART
POSITION
LABELS
P
BUT
NOT
PART
TYPE
LABELS
T
WE
NOW
DESCRIBE
A
PROCEDURE
FOR
GENERATING
TYPE
LABELS
FOR
OUR
ARTICULATED
MODEL
WE
FIRST
MANUALLY
DEFINE
THE
EDGE
STRUCTURE
E
BY
CON
NECTING
JOINT
POSITIONS
BASED
ON
AVERAGE
PROXIMITY
BECAUSE
WE
WISH
TO
MODEL
ARTICULATION
WE
CAN
ASSUME
THAT
PART
TYPES
SHOULD
CORRESPOND
TO
DIFFERENT
RELATIVE
LOCATIONS
OF
A
PART
WITH
RESPECT
TO
ITS
PARENT
IN
E
FOR
EXAMPLE
SIDEWAYS
ORIENTED
HANDS
OCCUR
NEXT
TO
ELBOWS
WHILE
DOWNWARD
FACING
HANDS
OCCUR
BELOW
ELBOWS
THIS
MEANS
WE
CAN
USE
RELATIVE
LOCATION
AS
A
SUPERVISORY
CUE
TO
HELP
DERIVE
TYPE
LA
BELS
THAT
CAPTURE
ORIENTATION
AND
HENCE
CONSISTENT
ORIENTATIONS
BY
OUR
ARGUMENTS
ABOVE
WE
DEFINE
THE
TYPE
LABELS
FOR
PARTS
TN
BASED
ON
CLUSTER
MEM
BERSHIP
WE
SHOW
EXAMPLE
RESULTS
IN
FIG
PARTIAL
SUPERVISION
BECAUSE
PART
TYPE
IS
DERIVED
HEURISTICALLY
ABOVE
ONE
COULD
TREAT
TN
AS
A
LATENT
VARIABLE
THAT
IS
ALSO
OPTIMIZED
DURING
LEARNING
THIS
LATENT
SVM
PROBLEM
CAN
BE
SOLVED
BY
COORDINATE
DESCENT
OR
THE
CCP
ALGORITHM
WE
PERFORMED
SOME
INITIAL
EXPERI
MENTS
WITH
LATENT
UPDATING
OF
PART
TYPES
USING
THE
COORDI
NATE
DESCENT
FRAMEWORK
OF
BUT
WE
FOUND
THAT
TYPE
LABELS
TEND
NOT
TO
CHANGE
OVER
ITERATIONS
WE
LEAVE
SUCH
PARTIALLY
SUPERVISED
LEARNING
AS
INTERESTING
FUTURE
WORK
PROBLEM
SIZE
ON
OUR
TRAINING
DATASETS
THE
NUMBER
OF
POSITIVE
EXAMPLES
VARIES
FROM
AND
THE
NUMBER
OF
NEGATIVE
IMAGES
IS
ROUGHLY
WE
TREAT
EACH
POSSIBLE
PLACEMENT
OF
THE
ROOT
ON
A
NEGATIVE
IMAGE
AS
A
UNIQUE
NEGA
TIVE
EXAMPLE
XN
MEANING
WE
HAVE
MILLIONS
OF
NEGATIVE
CON
STRAINTS
FURTHERMORE
WE
CONSIDER
MODELS
WITH
HUNDREDS
OF
THOUSANDS
OF
PARAMETERS
WE
FOUND
THAT
A
CAREFUL
OPTIMIZED
SOLVER
WAS
NECESSARY
TO
MANAGE
LEARNING
AT
THIS
SCALE
EXPERIMENTAL
RESULTS
DATASETS
WE
EVALUATE
RESULTS
USING
THE
IMAGE
PARSE
DATASET
AND
THE
BUFFY
DATASET
THE
PARSE
SET
CON
TAINS
POSE
ANNOTATED
IMAGES
OF
HIGHLY
ARTICULATED
FULL
BODY
IMAGES
OF
HUMAN
POSES
THE
BUFFY
DATASET
CONTAINS
POSE
ANNOTATED
VIDEO
FRAMES
OVER
EPISODES
OF
A
TV
SHOW
BOTH
DATASETS
INCLUDE
A
STANDARD
TRAIN
TEST
SPLIT
AND
A
STANDARDIZED
EVALUATION
PROTOCOL
BASED
ON
THE
PROBABILITY
OF
A
CORRECT
POSE
PCP
WHICH
MEASURES
THE
PERCENTAGE
OF
CORRECTLY
LOCALIZED
BODY
PARTS
NOTABLY
BUFFY
IS
ALSO
DIS
TRIBUTED
WITH
A
SET
OF
VALIDATED
DETECTION
WINDOWS
RETURNED
BY
AN
UPPER
BODY
PERSON
DETECTOR
RUN
ON
THE
TESTSET
MOST
PREVIOUS
WORK
REPORT
RESULTS
ON
THIS
SET
AS
DO
WE
SINCE
OUR
MODEL
ALSO
SERVES
AS
A
PERSON
DETECTOR
WE
CAN
ALSO
PRESENT
PCP
RESULTS
ON
THE
FULL
BUFFY
TESTSET
TO
TRAIN
OUR
MODELS
WE
USE
THE
NEGATIVE
TRAINING
IMAGES
FROM
THE
INRIAPERSON
DATABASE
AS
OUR
NEGATIVE
TRAINING
SET
THESE
IMAGES
TEND
TO
BE
OUTDOOR
SCENES
THAT
DO
NOT
CONTAIN
PEOPLE
MODELS
WE
DEFINE
A
FULL
BODY
SKELETON
FOR
THE
PARSE
SET
AND
A
UPPER
BODY
SKELETON
FOR
THE
BUFFY
SET
TO
DEFINE
A
FULLY
LABELED
DATASET
OF
PART
LOCATIONS
AND
TYPES
WE
GROUP
PARTS
INTO
ORIENTATIONS
BASED
ON
THEIR
RELATIVE
LOCATION
WITH
RESPECT
TO
THEIR
PARENTS
AS
DESCRIBED
IN
SEC
WE
SHOW
CLUSTERING
RESULTS
IN
FIG
WE
USE
THE
DERIVED
TYPE
LABELS
TO
CONSTRUCT
A
FULLY
SUPERVISED
DATASET
FROM
WHICH
WE
LEARN
FLEXIBLE
MIXTURES
OF
PARTS
WE
SHOW
THE
FULL
BODY
MODEL
LEARNED
ON
THE
PARSE
DATASET
IN
FIG
WE
SET
ALL
PARTS
TO
BE
HOG
CELLS
IN
SIZE
TO
VISUALIZE
THE
MODEL
WE
SHOW
TREES
GENERATED
BY
SELECTING
ONE
OF
THE
FOUR
TYPES
OF
EACH
PART
AND
PLACING
IT
AT
ITS
MAXIMUM
SCORING
POSITION
RECALL
THAT
EACH
PART
TYPE
HAS
ITS
OWN
APPEARANCE
TEMPLATE
AND
SPRING
ENCODING
ITS
RELATIVE
LOCATION
WITH
RESPECT
TO
ITS
PARENT
THIS
IS
BECAUSE
WE
EXPECT
PART
TYPES
TO
CORRESPOND
TO
ORIENTATION
BECAUSE
OF
THE
SUPERVISED
LABELING
SHOWN
IN
FIG
THOUGH
WE
VISUALIZE
TREES
WE
EMPHASIZE
THAT
THERE
EXISTS
AN
EXPONENTIAL
NUMBER
OF
TREES
THAT
OUR
MODEL
CAN
GENERATE
BY
COMPOSING
DIFFERENT
PART
TYPES
TOGETHER
STRUCTURE
WE
CONSIDER
THE
EFFECT
OF
VARYING
T
THE
NUMBER
OF
MIXTURES
OR
TYPES
AND
K
NUMBER
OF
PARTS
ON
THE
ACCURACY
OF
POSE
ESTIMATION
ON
THE
PARSE
DATASET
IN
FIG
WE
EXPERIMENT
WITH
A
PART
MODEL
DEFINED
AT
JOINT
POSITIONS
SHOULDER
ELBOW
HAND
ETC
AND
A
PART
MODEL
WHERE
MIDWAY
POINTS
BETWEEN
LIMBS
ARE
ADDED
MID
UPPER
ARM
MID
LOWER
ARM
ETC
TO
INCREASE
COVERAGE
PER
FORMANCE
INCREASES
WITH
DENSER
COVERAGE
AND
AN
INCREASED
NUMBER
OF
PART
TYPES
PRESUMABLY
BECAUSE
ADDITIONAL
ORIEN
TATIONS
ARE
BEING
CAPTURED
FOR
REFERENCE
WE
ALSO
TRAINED
A
STAR
MODEL
BUT
SAW
INFERIOR
PERFORMANCE
COMPARED
TO
THE
TREE
MODELS
SHOWN
IN
FIG
WE
SAW
A
SLIGHT
IMPROVEMENT
USING
A
VARIABLE
NUMBER
OF
MIXTURES
OR
PER
PART
TUNED
BY
CROSS
VALIDATION
THESE
ARE
THE
RESULTS
PRESENTED
BELOW
DETECTION
ACCURACY
WE
USE
OUR
MODEL
AS
AN
UPPER
BODY
DETECTOR
ON
THE
BUFFY
DATASET
IN
TABLE
WE
CORRECTLY
DETECT
OF
THE
PEOPLE
IN
THE
TESTSET
THE
DATASET
IN
CLUDE
TWO
ALTERNATE
DETECTORS
BASED
ON
A
RIGID
HOG
TEM
PLATE
AND
A
MIXTURES
OF
STAR
MODELS
WHICH
PERFORM
AT
AND
RESPECTIVELY
THE
LATTER
IS
WIDELY
REGARDED
AS
A
STATE
OF
THE
ART
SYSTEM
FOR
OBJECT
RECOGNITION
THESE
RESULTS
INDICATE
THE
POTENTIAL
OF
OUR
REPRESENTATION
AND
SU
PERVISED
LEARNING
FRAMEWORK
FOR
GENERAL
OBJECT
DETECTION
PARSE
WE
GIVE
QUANTITATIVE
RESULTS
FOR
PCP
IN
TABLE
AND
SHOW
EXAMPLE
IMAGES
IN
FIG
WE
REFER
THE
READER
TO
THE
CAPTIONS
FOR
A
DETAILED
ANALYSIS
BUT
OUR
METHOD
OUTPER
FORMS
ALL
PREVIOUSLY
PUBLISHED
RESULTS
BY
A
SIGNIFICANT
MAR
GIN
NOTABLY
ALL
PREVIOUS
WORK
USES
ARTICULATED
PARTS
WE
REDUCE
ERROR
BY
WE
BELIEVE
OUR
HIGH
PERFORMANCE
IS
TO
DUE
TO
THE
FACT
THAT
OUR
MODELS
LEVERAGE
ORIENTATION
SPECIFIC
STATISTICS
FIG
AND
BECAUSE
PARTS
AND
RELATIONS
ARE
SIMUL
TANEOUSLY
LEARNED
IN
A
DISCRIMINATIVE
FRAMEWORK
IN
CON
TRAST
ARTICULATED
MODELS
ARE
OFTEN
LEARNED
IN
STAGES
USING
PRE
TRAINED
ORIENTATION
INVARIANT
PART
DETECTORS
DUE
TO
THE
COMPUTATIONAL
BURDEN
OF
INFERENCE
BUFFY
WE
GIVE
QUANTITATIVE
RESULTS
FOR
PCP
IN
TABLE
AND
SHOW
EXAMPLE
IMAGES
IN
FIG
WE
REFER
THE
READER
TO
THE
CAPTIONS
FOR
A
DETAILED
ANALYSIS
BUT
WE
OUTPERFORM
ALL
PAST
APPROACHES
WHEN
EVALUATED
ON
A
SUBSET
OF
STANDARD
IZED
WINDOWS
OR
THE
ENTIRE
TESTSET
NOTABLY
ALL
PREVIOUS
APPROACHES
USE
ARTICULATED
PARTS
OUR
ALGORITHM
IS
SEVERAL
UPPER
BODY
DETECTION
ON
BUFFY
TABLE
OUR
MODEL
CLEARLY
OUTPERFORMS
PAST
APPROACHES
FOR
UPPER
BODY
DETECTION
NOTABLY
USE
A
STAR
STRUCTURED
MODEL
OF
HOG
TEMPLATES
TRAINED
WITH
WEAKLY
SUPERVISED
DATA
OUR
RESULTS
SUGGEST
MORE
COMPLEX
OBJECT
STRUCTURE
WHEN
LEARNED
WITH
SUPERVISION
CAN
YIELD
IMPROVED
RESULTS
FOR
DETECTION
PERFORMANCE
VS
NUMBER
OF
TYPES
PER
PART
FIGURE
WE
SHOW
THE
EFFECT
OF
MODEL
STRUCTURE
ON
POSE
ESTIMATION
BY
EVALUATING
PCP
PERFORMANCE
ON
THE
PARSE
DATASET
OVERALL
INCREASING
THE
NUMBER
OF
PARTS
BY
INSTANC
ING
PARTS
AT
LIMB
MIDPOINTS
IN
ADDITION
TO
JOINTS
IMPROVES
PERFORMANCE
FOR
BOTH
CASES
INCREASING
THE
NUMBER
OF
MIX
TURE
COMPONENTS
IMPROVES
PERFORMANCE
LIKELY
DUE
TO
THE
FACT
THAT
MORE
ORIENTATIONS
CAN
BE
MODELED
ORDERS
OF
MAGNITUDE
FASTER
THAN
THE
NEXT
BEST
APPROACHES
OF
WHEN
EVALUATED
ON
THE
ENTIRE
TESTSET
OUR
APPROACH
REDUCES
ERROR
BY
CONCLUSION
WE
HAVE
DESCRIBED
A
SIMPLE
BUT
FLEXIBLE
EXTENSION
OF
TREE
BASED
MODELS
OF
PART
MIXTURES
WHEN
PART
MIXTURE
MODELS
CORRESPOND
TO
PART
ORIENTATIONS
OUR
REPRE
SENTATION
CAN
MODEL
ARTICULATION
WITH
GREATER
SPEED
AND
AC
CURACY
THAN
CLASSIC
APPROACHES
OUR
REPRESENTATION
PROVIDES
A
GENERAL
FRAMEWORK
FOR
MODELING
CO
OCCURRENCE
RELATIONS
BETWEEN
MIXTURES
OF
PARTS
AS
WELL
AS
CLASSIC
SPATIAL
RELATIONS
BETWEEN
THE
LOCATION
OF
PARTS
WE
SHOW
THAT
SUCH
RELATIONS
CAPTURE
NOTIONS
OF
LOCAL
RIGIDITY
WE
ARE
APPLYING
THIS
AP
PROACH
TO
THE
TASK
OF
GENERAL
OBJECT
DETECTION
BUT
HAVE
AL
READY
DEMONSTRATED
IMPRESSIVE
RESULTS
FOR
THE
CHALLENGING
TASK
OF
HUMAN
POSE
ESTIMATION
ACKNOWLEDGEMENTS
FUNDING
FOR
THIS
RESEARCH
WAS
PRO
VIDED
BY
NSF
GRANT
ONR
MURI
GRANT
AND
SUPPORT
FROM
GOOGLE
AND
INTEL
NECK
WRT
HEAD
LEFT
KNEE
WRT
HIP
LEFT
FOOT
WRT
KNEE
LEFT
ELBOW
WRT
SHOULDER
LEFT
HAND
WRT
ELBOW
X
X
X
X
X
FIGURE
WE
TAKE
A
DATA
DRIVEN
APPROACH
TO
ORIENTATION
MODELING
BY
CLUSTERING
THE
RELATIVE
LOCATIONS
OF
PARTS
WITH
RESPECT
TO
THEIR
PARENTS
THESE
CLUSTERS
ARE
USED
TO
GENERATE
MIXTURE
LABELS
FOR
PARTS
DURING
TRAINING
FOR
EXAMPLE
HEADS
TEND
TO
BE
UPRIGHT
AND
SO
THE
ASSOCIATED
MIXTURE
MODELS
FOCUS
ON
UPRIGHT
ORIENTATIONS
BECAUSE
HANDS
ARTICULATE
TO
A
LARGE
DEGREE
MIXTURE
MODELS
FOR
THE
HAND
ARE
SPREAD
APART
TO
CAPTURE
A
LARGER
VARIETY
OF
RELATIVE
ORIENTATIONS
FIGURE
A
VISUALIZATION
OF
OUR
MODEL
FOR
T
TRAINED
ON
THE
PARSE
DATASET
WE
SHOW
THE
LOCAL
TEMPLATES
ABOVE
AND
THE
TREE
STRUCTURE
BELOW
PLACING
PARTS
AT
THEIR
BEST
SCORING
LOCATION
RELATIVE
TO
THEIR
PARENT
THOUGH
WE
VISUALIZE
TREES
THERE
EXISTS
AN
EXPONENTIAL
NUMBER
OF
REALIZABLE
COMBINATIONS
OBTAINED
COMPOSING
DIFFERENT
PART
TYPES
TOGETHER
THE
SCORE
ASSOCIATED
WITH
EACH
COMBINATION
DECOMPOSES
INTO
A
TREE
AND
SO
IS
EFFICIENT
TO
SEARCH
OVER
IMAGE
PARSE
TESTSET
TABLE
WE
COMPARE
OUR
MODEL
TO
ALL
PREVIOUS
PUBLISHED
RESULTS
ON
THE
PARSE
DATASET
USING
THE
STANDARD
CRITERIA
OF
PCP
OUR
TOTAL
PERFORMANCE
OF
COMPARES
FAVORABLY
TO
THE
BEST
PREVIOUS
RESULT
OF
WE
ALSO
OUTPERFORM
ALL
PREVIOUS
RESULTS
ON
A
PER
PART
BASIS
SUBSET
OF
BUFFY
TESTSET
FULL
BUFFY
TESTSET
TABLE
THE
BUFFY
TESTSET
IS
DISTRIBUTED
WITH
A
SUBSET
OF
WINDOWS
DETECTED
BY
A
RIGID
HOG
UPPER
BODY
DETECTOR
WE
COMPARE
OUR
RESULTS
TO
ALL
PUBLISHED
WORK
ON
THIS
SET
ON
THE
LEFT
WE
OBTAIN
THE
BEST
OVERALL
PCP
WHILE
BEING
ORDERS
OF
MAGNITUDE
FASTER
THAN
THE
NEXT
BEST
APPROACHES
OUR
TOTAL
PIPELINE
REQUIRES
SECOND
TO
PROCESS
AN
IMAGE
WHILE
TAKE
MINUTES
WE
OUTPERFORM
OR
NEARLY
TIE
ALL
PREVIOUS
RESULTS
ON
A
PER
PART
BASIS
AS
POINTED
OUT
BY
THIS
SUBSET
CONTAINS
LITTLE
POSE
VARIATION
BECAUSE
IT
IS
BIASED
TO
BE
RESPONSES
OF
A
RIGID
TEMPLATE
THE
DISTRIBUTED
EVALUATION
PROTOCOL
ALSO
ALLOWS
ONE
TO
COMPUTE
PERFORMANCE
ON
THE
FULL
TEST
VIDEOS
BY
MULTIPLYING
PCP
VALUES
WITH
THE
OVERALL
DETECTION
RATE
WE
DO
THIS
FOR
PUBLISHED
RESULTS
ON
THE
RIGHT
TABLE
BECAUSE
OUR
MODEL
ALSO
SERVES
AS
A
VERY
ACCURATE
DETECTOR
TABLE
WE
OBTAIN
SIGNIFICANTLY
BETTER
RESULTS
THAN
PAST
WORK
WHEN
EVALUATED
ON
THE
FULL
TESTSET
FIGURE
RESULTS
ON
THE
PARSE
DATASET
WE
SHOW
PART
BOUNDING
BOXES
REPORTED
BY
OUR
ALGORITHM
FOR
EACH
IMAGE
THE
TOP
ROWS
SHOW
SUCCESSFUL
EXAMPLES
WHILE
THE
BOTTOM
ROW
SHOWS
FAILURE
CASES
EXAMINING
FAILURE
CASES
FROM
LEFT
TO
RIGHT
WE
FIND
OUR
MODEL
IS
NOT
FLEXIBLE
ENOUGH
TO
MODEL
HORIZONTAL
PEOPLE
IS
CONFUSED
BY
OVERLAPPING
PEOPLE
SUFFERS
FROM
DOUBLE
COUNTING
PHENOMENA
COMMON
TO
TREE
MODELS
BOTH
THE
LEFT
AND
RIGHT
LEGS
FIRE
ON
THE
SAME
IMAGE
REGION
AND
IS
CONFUSED
WHEN
OBJECTS
PARTIALLY
OCCLUDE
PEOPLE
FIGURE
RESULTS
ON
THE
BUFFY
DATASET
WE
SHOW
PART
BOUNDING
BOXES
CORRESPONDING
TO
UPPER
BODY
PARTS
REPORTED
BY
OUR
ALGORITHM
THE
LEFT
COLUMNS
SHOW
SUCCESSFUL
EXAMPLES
WHILE
THE
RIGHT
COLUMN
SHOW
FAILURE
CASES
FROM
TOP
TO
BOTTOM
WE
SEE
THAT
OUR
MODEL
STILL
HAS
DIFFICULTLY
WITH
RAISED
ARMS
AND
IS
CONFUSED
BY
VERTICAL
LIMB
LIKE
CLUTTER
IN
THE
BACKGROUND
WORLD
SCALE
MINING
OF
OBJECTS
AND
EVENTS
FROM
COMMUNITY
PHOTO
COLLECTIONS
TILL
KOOABA
AG
ZURICH
SWITZERLAND
BASTIAN
ETH
ZURICH
BIWI
ZURICH
SWITZERLAND
LUC
VAN
K
U
LEUVEN
IBBT
LEUVEN
BELGIUM
ABSTRACT
IN
THIS
PAPER
WE
DESCRIBE
AN
APPROACH
FOR
MINING
IMAGES
OF
OBJECTS
SUCH
AS
TOURISTIC
SIGHTS
FROM
COMMUNITY
PHOTO
COL
LECTIONS
IN
AN
UNSUPERVISED
FASHION
OUR
APPROACH
RELIES
ON
RETRIEVING
GEOTAGGED
PHOTOS
FROM
THOSE
WEB
SITES
USING
A
GRID
OF
GEOSPATIAL
TILES
THE
DOWNLOADED
PHOTOS
ARE
CLUSTERED
INTO
POTENTIALLY
INTERESTING
ENTITIES
THROUGH
A
PROCESSING
PIPELINE
OF
SEVERAL
MODALITIES
INCLUDING
VISUAL
TEXTUAL
AND
SPATIAL
PROXIMITY
THE
RESULTING
CLUSTERS
ARE
ANALYZED
AND
ARE
AU
TOMATICALLY
CLASSIFIED
INTO
OBJECTS
AND
EVENTS
USING
MINING
TECHNIQUES
WE
THEN
FIND
TEXT
LABELS
FOR
THESE
CLUSTERS
WHICH
ARE
USED
TO
AGAIN
ASSIGN
EACH
CLUSTER
TO
A
CORRESPONDING
WI
KIPEDIA
ARTICLE
IN
A
FULLY
UNSUPERVISED
MANNER
A
FINAL
VER
IFICATION
STEP
USES
THE
CONTENTS
INCLUDING
IMAGES
FROM
THE
SELECTED
WIKIPEDIA
ARTICLE
TO
VERIFY
THE
CLUSTER
ARTICLE
ASSIGN
MENT
WE
DEMONSTRATE
THIS
APPROACH
ON
SEVERAL
URBAN
AREAS
DENSELY
COVERING
AN
AREA
OF
OVER
SQUARE
KILOMETERS
AND
MINING
OVER
PHOTOS
MAKING
IT
PROBABLY
THE
LARGEST
EXPERIMENT
OF
ITS
KIND
TO
DATE
CATEGORIES
AND
SUBJECT
DESCRIPTORS
H
INFORMATION
STORAGE
AND
RETRIEVAL
CONTENT
ANALYSIS
AND
INDEXING
GENERAL
TERMS
ALGORITHMS
DESIGN
EXPERIMENTATION
THEORY
INTRODUCTION
SEVERAL
RECENT
DEVELOPMENTS
HAVE
STRONGLY
INFLUENCED
THE
STATE
OF
THE
ART
IN
RETRIEVAL
FROM
VISUAL
DATABASES
FIRST
MORE
POWERFUL
LOCAL
VISUAL
FEATURES
HAVE
LED
TO
SIGNIFICANT
PROGRESS
IN
RECOGNITION
CAPABILITIES
BOTH
FOR
SPECIFIC
OBJECTS
AND
FOR
OBJECT
CLASSES
SECOND
WHILE
THE
STATE
OF
THE
ART
IN
OBJECT
CLASS
RECOGNITION
SCALES
TO
A
FEW
THOUSAND
IMAGES
SCALEABLE
INDEXING
METHODS
FOR
RETRIEVAL
OF
SPECIFIC
OBJECTS
HAVE
RECENTLY
ALLOWED
SCALING
UP
TO
MILLION
IMAGES
THIRD
WITH
THE
UBIQUITOUS
AVAILABILITY
OF
THE
INTERNET
AND
THE
WIDESPREAD
USE
OF
DIGITAL
PERMISSION
TO
MAKE
DIGITAL
OR
HARD
COPIES
OF
ALL
OR
PART
OF
THIS
WORK
FOR
PERSONAL
OR
CLASSROOM
USE
IS
GRANTED
WITHOUT
FEE
PROVIDED
THAT
COPIES
ARE
NOT
MADE
OR
DISTRIBUTED
FOR
PROFIT
OR
COMMERCIAL
ADVANTAGE
AND
THAT
COPIES
BEAR
THIS
NOTICE
AND
THE
FULL
CITATION
ON
THE
FIRST
PAGE
TO
COPY
OTHERWISE
TO
REPUBLISH
TO
POST
ON
SERVERS
OR
TO
REDISTRIBUTE
TO
LISTS
REQUIRES
PRIOR
SPECIFIC
PERMISSION
AND
OR
A
FEE
CIVR
JULY
NIAGARA
FALLS
ONTARIO
CANADA
COPYRIGHT
ACM
CAMERAS
LARGE
DATABASES
OF
VISUAL
DATA
HAVE
BEEN
CREATED
MOST
NOTABLY
COMMUNITY
PHOTO
COLLECTIONS
SUCH
AS
FLICKR
HTTP
WWW
FLICKR
COM
THESE
COLLECTIONS
CONTAIN
VAST
AMOUNTS
OF
HIGH
QUALITY
IMAGES
OFTEN
LABELED
WITH
KEYWORDS
OR
TAGS
AN
INCREASING
NUMBER
OF
THOSE
PHOTOS
IS
ALSO
AN
NOTATED
WITH
THE
GEOGRAPHIC
LOCATION
THE
PICTURE
WAS
TAKEN
AT
ANNOTATING
PHOTOS
WITH
THEIR
GEOGRAPHIC
POSITION
OF
TEN
CALLED
GEOTAGGING
IS
EITHER
DONE
AUTOMATICALLY
WITH
A
GPS
DEVICE
OR
BY
MANUALLY
PLACING
THE
PHOTO
ON
A
MAP
HOWEVER
THESE
TEXTUAL
AND
GEOGRAPHIC
ANNOTATIONS
ARE
STILL
OF
FAR
LOWER
QUALITY
THAN
THEIR
COUNTERPARTS
IN
TRADITIONAL
DATABASES
SUCH
AS
STOCK
PHOTOGRAPHY
OR
NEWS
ARCHIVES
IN
THIS
WORK
WE
DEAL
WITH
A
CRUCIAL
BUT
OFTEN
NEGLECTED
BUILDING
BLOCK
TOWARDS
INTERNET
SCALE
IMAGE
RETRIEVAL
THE
AUTOMATED
COLLECTION
OF
A
HIGH
QUALITY
IMAGE
DATABASE
WITH
CORRECT
ANNOTATIONS
MORE
PRECISELY
FROM
THE
LARGE
AMOUNT
OF
SPARSELY
LABELED
CONTENT
IN
COMMUNITY
PHOTO
COLLECTIONS
THE
TASK
IS
TO
MINE
CLUSTERS
OF
IMAGES
CONTAINING
OBJECTS
IN
A
FULLY
UNSUPERVISED
MANNER
FOR
EACH
MINED
ITEM
WE
AUTOMATICALLY
DERIVE
A
TEXTUAL
DESCRIPTION
THE
RESULTING
CLEANED
IMAGE
DATABASE
FOR
THE
MINED
OBJECTS
AND
EVENTS
IS
OF
FAR
HIGHER
QUALITY
THAN
THE
ORIGINAL
DATA
AND
FACILITATES
A
VARIETY
OF
APPLICATIONS
FOR
EXAMPLE
THE
MINED
STRUC
TURE
CAN
BE
USED
FOR
AUTOMATED
ANNOTATION
OF
PHOTOS
UP
LOADED
TO
COMMUNITY
COLLECTIONS
FOR
RETRIEVAL
AND
BROWS
ING
OF
LANDMARK
BUILDINGS
AUTOMATIC
RECONSTRUCTION
OF
SIGHTS
OR
FOR
MOBILE
PHONE
TOURIST
GUIDE
APPLICA
TIONS
WHERE
USERS
CAN
POINT
THE
INTEGRATED
CAMERA
TO
A
SIGHT
AND
RETRIEVE
INFORMATION
ABOUT
IT
OUR
APPROACH
IS
BASED
ON
PHOTOGRAPHS
WHICH
HAVE
BEEN
TAGGED
WITH
THEIR
GEOGRAPHIC
LOCATION
FLICKR
REPORTS
THAT
OVER
MILLION
SUCH
GEOTAGGED
PHOTOS
ARE
CURRENTLY
UPLOADED
EACH
MONTH
THIS
ALLOWS
US
TO
MINE
THE
WORLD
IN
A
SCALABLE
MANNER
WITHOUT
ANY
PRIOR
KNOWLEDGE
ON
LANDMARKS
AND
THEIR
LOCATIONS
TO
THAT
END
WE
PARTITION
THE
WORLD
INTO
A
GRID
OF
SQUARE
TILES
AND
RETRIEVE
FOR
EACH
TILE
ALL
THE
CORRESPONDING
GEOTAGGED
PHOTOS
FROM
FLICKR
THE
GEOGRAPHIC
TILING
THUS
ALLOWS
US
TO
HANDLE
THE
SIZE
OF
THIS
VAST
PROBLEM
AND
TO
PARALLELIZE
COMPUTATIONS
IN
DETAIL
THIS
PAPER
MAKES
THE
FOLLOWING
CONTRIBUTIONS
WE
DEMONSTRATE
FULLY
AUTOMATIC
WORLD
SCALE
IMAGE
MIN
ING
FROM
COMMUNITY
PHOTO
COLLECTIONS
TO
OUR
KNOWLEDGE
OUR
APPROACH
IS
THE
FIRST
OF
ITS
KIND
THAT
CAN
STRUCTURE
IN
TERPRET
AND
ANNOTATE
SUCH
AMOUNTS
OF
VISUAL
DATA
WITHOUT
USER
INTERVENTION
WE
CLUSTER
THE
RETRIEVED
PHOTOS
ACCORD
ING
TO
SEVERAL
DIFFERENT
MODALITIES
INCLUDING
VISUAL
CONTENT
AND
TEXT
LABELS
AND
CLUSTERING
STRATEGIES
WE
SHOW
HOW
THE
INTELLIGENT
COMBINATION
OF
THE
RESULTING
CLUSTER
ASSIGNMENTS
CAN
CAPTURE
AND
DISCRIMINATE
BETWEEN
DISTINCT
OBJECTS
IN
SIDE
OUTSIDE
VIEWS
OF
LANDMARK
BUILDINGS
AND
PANORAMAS
AND
HOW
IT
CAN
REPRESENT
THE
NEIGHBORHOOD
RELATION
BETWEEN
THOSE
SIGHTS
FOR
EACH
CLUSTER
WE
ADDITIONALLY
CALCULATE
A
SET
OF
CUES
SUCH
AS
THE
NUMBER
OF
DAYS
COVERED
BY
ITS
PHOTOS
THE
NUMBER
OF
USERS
WHO
TOOK
THE
PHOTOS
ETC
WE
SHOW
HOW
THESE
ADDITIONAL
FEATURES
CAN
BE
USED
TO
TRAIN
A
SUBSEQUENT
CLASSIFIER
WHICH
DETERMINES
IF
AN
IMAGE
CLUSTER
REPRESENTS
AN
OBJECT
OR
AN
EVENT
WE
APPLY
FREQUENT
ITEMSET
MINING
ON
THE
TEXT
ASSOCIATED
WITH
EACH
CLUSTER
IN
ORDER
TO
ASSIGN
CLUSTER
LABELS
WE
PROPOSE
AN
ALGORITHM
THAT
EMPLOYS
THE
RESULTING
FREQUENT
ITEMSET
LABELS
TO
LINK
CLUSTERS
TO
WIKIPE
DIA
PAGES
PROVIDING
ADDITIONAL
INFORMATION
ABOUT
THE
CLUSTER
CONTENT
AND
THAT
THEN
IN
TURN
TAKES
THE
WIKIPEDIA
ENTRIES
TO
VERIFY
CLUSTERS
AND
FILTER
OUT
FALSE
ASSIGNMENTS
CLOS
ING
THE
LOOP
WE
FINALLY
DEMONSTRATE
HOW
THE
VERIFIED
CLUSTERS
CAN
BE
USED
TO
AUTOMATICALLY
LABEL
AND
GEO
LOCATE
ADDITIONAL
PHOTOS
FOR
WHICH
NO
GEOTAGS
WERE
AVAILABLE
THE
PAPER
IS
STRUCTURED
AS
FOLLOWS
THE
NEXT
SECTION
DIS
CUSSES
RELATED
WORK
SECTION
THEN
INTRODUCES
OUR
MINING
APPROACH
AND
DESCRIBES
HOW
WE
CLUSTER
THE
MINED
PHOTOS
SECTION
DETAILS
HOW
WE
CLASSIFY
CLUSTERS
INTO
OBJECTS
AND
EVENTS
AND
HOW
WE
LINK
THEM
TO
WIKIPEDIA
FINALLY
SEC
TION
PRESENTS
EXPERIMENTAL
RESULTS
RELATED
WORK
SINCE
THE
PROPOSED
METHOD
COVERS
AN
ENTIRE
MULTI
MODAL
PROCESSING
PIPELINE
IT
TOUCHES
ON
A
LARGE
VARIETY
OF
PREVI
OUS
PUBLICATIONS
MINING
OBJECTS
FROM
VISUAL
DATA
HAS
BEEN
PROPOSED
FOR
VIDEO
FOR
INSTANCE
IN
THOSE
WORKS
ALSO
BUILT
ON
LOCAL
FEATURES
BUT
FOCUSED
ON
THE
SPATIAL
AR
RANGEMENT
OF
QUANTIZED
FEATURES
FROM
VIDEO
DATA
WORKING
WITH
DATA
FROM
COMMUNITY
PHOTO
COLLECTIONS
HAS
RECEIVED
IN
CREASING
ATTENTION
LATELY
HOWEVER
MOST
OF
THOSE
APPROACHES
ARE
BASED
EITHER
ON
TEXT
OR
ONLY
GLOBAL
VI
SUAL
FEATURES
THE
LOCAL
VISUAL
FEATURES
WHICH
ARE
USED
IN
THIS
WORK
HOWEVER
ALLOW
TO
FIND
VERY
GOOD
AND
EXTREMELY
ACCURATE
MATCHES
BETWEEN
THE
DEPICTED
OBJECTS
EVEN
UNDER
SIGNIFICANT
CHANGES
IN
VIEWPOINT
IMAGING
CONDITIONS
SCALE
LIGHTING
CLUTTER
NOISE
AND
PARTIAL
OCCLUSION
A
SIMILAR
AP
PROACH
WOULD
NOT
BE
POSSIBLE
USING
GLOBAL
MEASURES
SUCH
AS
COLOR
OR
TEXTURE
HISTOGRAMS
PHILBIN
AND
ZISSERMAN
ALSO
WORKED
WITH
LOCAL
FEATURES
AND
MULTIPLE
VIEW
GEOMETRY
ON
A
DATABASE
OF
LANDMARK
BUILDINGS
OBTAINED
FROM
FLICKR
THE
MAIN
GOAL
OF
THAT
WORK
WAS
TO
DERIVE
A
SCALABLE
INDEXING
METHOD
FOR
LOCAL
VISUAL
FEATURES
THE
DATABASE
WAS
RETRIEVED
AND
ANNOTATED
MANUALLY
THE
WORK
MOST
SIMILAR
TO
OURS
IS
PROBABLY
HERE
THE
AUTHORS
ALSO
PROPOSED
CLUSTER
ING
IMAGES
FROM
COMMUNITY
PHOTO
COLLECTIONS
USING
MULTI
VIEW
GEOMETRY
BASED
MATCHING
BETWEEN
IMAGES
THE
GOAL
WAS
TO
DERIVE
CANONICAL
VIEWS
FOR
CERTAIN
LANDMARKS
AND
TO
USE
THOSE
AS
ENTRY
POINTS
FOR
BROWSING
INITIAL
IMAGE
COL
LECTIONS
WERE
RETRIEVED
BY
QUERYING
PHOTO
COLLECTIONS
WITH
KNOWN
KEYWORDS
SUCH
AS
ROME
PANTHEON
ETC
AS
WE
WILL
DEMONSTRATE
OUR
FULLY
UNSUPERVISED
APPROACH
BASED
ON
GEOGRAPHIC
TILING
IS
NOT
ONLY
MORE
FLEXIBLE
BUT
ALSO
MORE
SCALABLE
THE
DATASET
USED
IN
CONTAINED
PHOTOS
WHILE
OURS
IS
ONE
ORDER
OF
MAGNITUDE
LARGER
FURTHERMORE
WE
ADD
SEVERAL
LAYERS
OF
PROCESSING
WHICH
EXTRACT
SEMANTIC
INFORMATION
SUCH
AS
CLASSIFICATION
INTO
OBJECTS
AND
EVENTS
AND
WHICH
AUTOMATICALLY
INCLUDE
OTHER
CONTENT
SOURCES
SUCH
AS
WIKIPEDIA
FOR
UNSUPERVISED
LABELING
OF
OBJECTS
TO
THE
BEST
OF
OUR
KNOWLEDGE
THIS
WORK
IS
THE
FIRST
TO
PROPOSE
THIS
FIGURE
TILES
OVER
PARIS
THE
SIZE
OF
A
TILE
IS
MARKED
IN
RED
NOTE
THE
OVERLAP
OF
KIND
OF
PIPELINE
TAKING
AS
AN
INPUT
ONLY
A
GEOGRAPHIC
TILING
OF
THE
WORLD
AND
RESULTING
IN
AN
OUTPUT
OF
AUTOMATICALLY
MINED
LANDMARK
OBJECTS
TOGETHER
WITH
THEIR
SEMANTICS
IN
THE
FORM
OF
AUTOMATICALLY
CREATED
LINKS
TO
WIKIPEDIA
MINING
APPROACH
IN
SUMMARY
OUR
APPROACH
CONSISTS
OF
THE
FOLLOWING
STEPS
GATHERING
THE
GEOTAGGED
DATA
FROM
THE
WWW
CLUSTERING
TO
GROUP
IMAGES
OF
THE
SAME
OBJECT
EVENT
CLASSIFICATION
OF
CLUSTERS
INTO
OBJECTS
OR
EVENTS
FREQUENT
ITEMSET
MINING
TO
DERIVE
CLUSTER
LABELS
UNSUPERVISED
LINKING
TO
WIKIPEDIA
AND
VERIFICATION
OF
THOSE
LINKS
THE
FOLLOWING
SECTIONS
DESCRIBE
EACH
OF
THOSE
STEPS
IN
DETAIL
GATHERING
THE
DATA
TO
GATHER
THE
RAW
DATA
WE
QUERY
COMMUNITY
PHOTO
COL
LECTIONS
SUCH
AS
FLICKR
FIRST
WE
DIVIDE
THE
EARTH
SUR
FACE
INTO
SQUARE
TILES
TK
OF
ABOUT
SIDE
LENGTH
A
TILE
CENTER
IS
SET
EVERY
IN
LONGITUDE
AND
LATITUDE
DIREC
TION
SUCH
THAT
THE
TILES
HAVE
A
HIGH
OVERLAP
FOR
EACH
TILE
WE
QUERY
THE
FLICKR
API
WITH
THE
TILE
CENTER
COORDINATES
AND
BOUNDING
BOX
TO
OBTAIN
ALL
GEOTAGGED
PHOTOS
FOR
THAT
AREA
FIGURE
SHOWS
A
SECTION
OF
A
MAP
WITH
THE
TILES
USED
FOR
QUERYING
OVERLAID
IN
TOTAL
WE
PROCESSED
ABOUT
TILES
FOR
THIS
WORK
COVERING
SEVERAL
EUROPEAN
URBAN
CEN
TERS
NAMELY
PARIS
ROME
VENICE
OXFORD
ZURICH
MUNICH
TALLINN
PRAGUE
AND
ST
PETERSBURG
TABLE
LISTS
THE
URBAN
AREAS
WE
COVERED
AND
THE
NUMBER
OF
TILES
AND
PHOTOS
RE
TRIEVED
FOR
EACH
AREA
IN
TOTAL
WE
COVERED
AN
AREA
OF
ABOUT
SQUARE
KILOMETERS
THE
MAJORITY
OF
TILES
ABOUT
WERE
EMPTY
THE
REMAINING
TILES
CONTAINED
ON
AVERAGE
AND
A
MAXIMUM
OF
PHOTOS
FOR
EACH
PHOTO
WE
DOWNLOAD
WE
ALSO
OBTAIN
THE
ASSOCI
ATED
METADATA
NAMELY
THE
TEXTUAL
DESCRIPTIONS
TAGS
TITLE
DESCRIPTION
USER
ID
AND
TIMESTAMPS
PHOTO
CLUSTERING
ONCE
THE
PHOTOS
FOR
EACH
TILE
HAVE
BEEN
DOWNLOADED
WE
PROCESS
EACH
CELL
TO
FIND
CLUSTERS
OF
PHOTOS
WITH
SIMILAR
CON
TENT
AS
OBJECT
CANDIDATES
WE
FIRST
CREATE
DISSIMILARITY
MA
TRICES
FOR
SEVERAL
MODALITIES
BY
CALCULATING
THE
PAIRWISE
DIS
TANCES
BETWEEN
PHOTOS
FOR
EACH
MODALITY
A
HIERARCHICAL
CLUSTERING
STEP
ON
THE
DISSIMILARITY
MATRICES
THEN
CREATES
CLUSTERS
OF
PHOTOS
FOR
THE
SAME
OBJECT
OR
EVENT
BELOW
WE
DISCUSS
THE
FEATURES
AND
DISTANCES
USED
FOR
EACH
MODALITY
TABLE
URBAN
AREAS
PROCESSED
IN
THIS
PAPER
AND
THE
NUMBER
OF
TILES
AND
PHOTOS
PER
AREA
VISUAL
FEATURES
AND
SIMILARITY
TO
IDENTIFY
PAIRS
OF
PHOTOS
WHICH
CONTAIN
THE
SAME
OBJECT
WE
EMPLOY
MATCHING
BASED
ON
LOCAL
SCALE
INVARIANT
FEATURES
AND
PROJECTIVE
GEOMETRY
WE
FIRST
EXTRACT
THE
VISUAL
FEATURES
FROM
EACH
PHOTO
FOR
THIS
WE
EMPLOY
SURF
FEATURES
DUE
TO
THEIR
FAST
EXTRACTION
TIMES
AND
COMPACT
DESCRIPTION
SHOWN
IN
EARLIER
WORKS
EACH
IMAGE
IS
THUS
REPRESENTED
AS
A
BAG
OF
DIMENSIONAL
SURF
FEATURE
VECTORS
FOR
EACH
PAIR
OF
IMAGES
IN
A
TILE
TK
WE
FIND
MATCHING
FEATURES
BY
CALCULATING
THE
NEAREST
NEIGHBOR
NN
IN
EUCLIDEAN
DISTANCE
BETWEEN
ALL
FEATURE
PAIRS
FOLLOWED
BY
A
VERIFICATION
WITH
THE
NEAREST
NEIGHBOR
CRITERION
FROM
NOTE
THAT
THIS
LINEAR
MATCHING
PROCEDURE
IS
FAST
ENOUGH
SINCE
THE
PROBLEM
IS
SEPARATED
INTO
THE
GEOGRAPHIC
TILES
USING
SCALEABLE
INDEXING
METHODS
SUCH
AS
COULD
LOWER
THE
PROCESSING
TIMES
OF
THE
SYSTEM
EVEN
FURTHER
TO
FIND
OBJECT
CANDIDATES
FROM
THE
MATCHING
FEATURES
WE
NEXT
CALCULATE
HOMOGRAPHY
MAPPINGS
FOR
EACH
MATCHED
IM
IN
OUR
IMPLEMENTATION
IMAX
SINCE
WE
EXTRACT
AT
MOST
SURF
FEATURES
PER
IMAGE
SORTED
BY
THEIR
DIS
CRIMINANCE
I
E
THE
DISTANCE
RANGES
IN
TEXT
FEATURES
AND
SIMILARITY
THREE
SOURCES
FOR
TEXT
META
DATA
WERE
CONSIDERED
FOR
EACH
PHOTO
DOWNLOADED
FROM
FLICKR
TAGS
TITLE
AND
DESCRIPTION
WE
COMBINE
THESE
THREE
TEXT
FIELDS
INTO
A
SINGLE
TEXT
PER
PHOTO
FOR
FURTHER
PROCESSING
STAGES
THE
FIRST
STAGE
CONSISTS
OF
A
STOPLIST
IN
ADDITION
TO
THE
COMMON
STOPWORDS
THIS
LIST
ALSO
CONTAINS
COLLECTION
SPECIFIC
STOPWORDS
SUCH
AS
YEARS
MONTHS
AND
TERMS
SUCH
AS
GEOTAGGED
TRIP
VACATION
HONEYMOON
ETC
FURTHERMORE
FROM
EACH
PHOTO
GEOTAG
WE
KNOW
ITS
LOCATION
AND
THE
CORRESPONDING
PLACE
NAME
FOR
INSTANCE
ROME
ITALY
THESE
LOCATION
SPECIFIC
PLACE
NAMES
WERE
ADDED
TO
THE
STOPLIST
FOR
EACH
PHOTO
DEPENDING
ON
ITS
GEOTAG
FILTERING
TERMS
WITH
THESE
CUSTOM
STOPLISTS
TURNED
OUT
TO
BE
CRUCIAL
TO
OBTAIN
GOOD
CLUSTER
LABELS
IN
LATER
PRO
CESSING
STAGES
AS
WITH
THE
VISUAL
FEATURES
WE
PROCEED
BY
CALCULATING
THE
PAIRWISE
TEXT
SIMILARITIES
BETWEEN
THE
DOCUMENTS
PHOTOS
A
VECTOR
SPACE
MODEL
WITH
TERM
WEIGHTING
OF
THE
FOLLOWING
FORM
IS
APPLIED
WI
J
LI
J
GI
NJ
NOTE
THAT
IN
THE
STANDARD
TF
IDF
RANKING
LI
J
TF
I
J
GI
LOG
D
AND
NJ
WHERE
TF
I
J
IS
THE
FREQUENCY
OF
TERM
I
IN
DOCUMENT
J
DI
IS
THE
NUMBER
OF
DOCUMENTS
CONTAINING
TERM
I
AND
D
IS
THE
TOTAL
NUMBER
OF
DOCUMENTS
IN
OUR
SYSTEM
THE
WEIGHTING
ELEMENTS
ARE
AS
FOLLOWS
LOG
TF
I
J
AGE
PAIR
I
J
I
J
LI
J
PJ
LOG
TF
I
J
HXN
XN
N
WHERE
H
IS
THE
HOMOGRAPHY
WHOSE
DEGREES
OF
FREEDOM
G
LOG
D
DI
I
CAN
BE
SOLVED
WITH
FOUR
POINT
CORRESPONDENCES
N
TO
UJ
BE
ROBUST
AGAINST
THE
AFOREMENTIONED
OUTLIERS
WE
ESTIMATE
H
USING
RANSAC
THE
QUALITY
OF
SEVERAL
ESTIMATED
MOD
NJ
UJ
ELS
IS
MEASURED
BY
THE
NUMBER
OF
INLIERS
WHERE
AN
INLIER
I
IS
DEFINED
BY
A
THRESHOLD
ON
THE
RESIDUAL
ERROR
THE
RESIDUAL
ERROR
FOR
THE
MODEL
IS
DETERMINED
BY
THE
DISTANCE
OF
THE
TRUE
POINTS
FROM
THE
POINTS
GENERATED
BY
THE
ESTIMATED
H
WE
ACCEPT
HYPOTHESES
WITH
AT
LEAST
INLIERS
I
AS
A
MATCH
USING
THIS
KIND
OF
HOMOGRAPHY
MAPPING
WORKS
WELL
IN
OUR
CASE
SINCE
WE
HAVE
MANY
PHOTOS
TAKEN
FROM
SIMILAR
VIEW
POINTS
A
FUNDAMENTAL
MATRIX
COULD
HANDLE
LARGER
VIEWPOINT
CHANGES
BUT
IT
IS
ALSO
MORE
COSTLY
TO
COMPUTE
SINCE
IT
RE
QUIRES
MORE
INLIERS
TO
FIND
THE
CORRECT
MODEL
FURTHERMORE
MAPPING
PLANAR
ELEMENTS
SUCH
AS
BUILDING
FACADES
WORKS
VERY
WELL
WITH
HOMOGRAPHIES
A
SIMILAR
APPROACH
HAS
ALSO
BEEN
SUCCESSFULLY
APPLIED
IN
FOR
A
RETRIEVAL
ENGINE
ON
A
DATABASE
OF
LANDMARKS
FROM
OXFORD
HANDLING
ASTONISHING
VIEWPOINT
AND
SCALE
CHANGES
AS
MENTIONED
ABOVE
THE
ACCU
RACY
ACHIEVED
WITH
THESE
KINDS
OF
VISUAL
FEATURES
IS
FAR
BETTER
THAN
WITH
ANY
KIND
OF
GLOBAL
FEATURES
WHICH
ARE
STILL
OFTEN
USED
FOR
MINING
AND
RETRIEVAL
IN
VISUAL
DATABASES
THE
DISTANCE
MATRIX
IS
BUILT
FROM
THE
NUMBER
OF
INLYING
FEATURE
MATCHES
IIJ
FOR
EACH
IMAGE
PAIR
NORMALIZED
BY
THE
MAXIMUM
NUMBER
OF
INLIERS
FOUND
IN
THE
WHOLE
DATASET
IIJ
WHERE
UJ
IS
THE
NUMBER
OF
UNIQUE
TERMS
IN
DOCUMENT
J
THE
RATIONALE
BEHIND
THE
MODIFICATIONS
OF
THE
WEIGHTING
TERMS
OVER
THE
STANDARD
TF
IDF
ARE
AS
FOLLOWS
THE
LOGARITHM
IN
LI
J
ADJUSTS
DAMPENS
WEIGHTS
OF
MULTIPLE
OCCURRING
WORDS
PER
DOCUMENT
GI
IS
A
PROBABILISTIC
INVERSE
DOCUMENT
FRE
QUENCY
AS
PROPOSED
IN
WHICH
UNLIKE
IDF
ASSIGNS
NEGA
TIVE
WEIGHTS
TO
TERMS
THAT
APPEAR
IN
MORE
THAN
HALF
THE
DOC
UMENTS
FINALLY
THE
ADDITIONAL
TERM
NJ
IS
A
PIVOTED
UNIQUE
NORMALIZATION
WHICH
IS
USED
TO
CORRECT
FOR
DISCREPANCIES
IN
DOCUMENT
LENGTHS
WE
USE
THE
MYSQL
WWW
MYSQL
COM
FULL
TEXT
SEARCH
WHICH
CAN
BE
CONFIGURED
TO
USE
THE
MODIFIED
TF
IDF
RANKING
TO
COMPUTE
THE
TEXT
DISIMILARITY
MATRIX
FOR
THE
PHOTOS
BELONGING
TO
EACH
GRID
TILE
ADDITIONAL
FEATURES
BESIDES
THE
VISUAL
AND
TEXT
SIMILARITIES
BETWEEN
PHOTOS
WE
ALSO
CONSIDERED
SEVERAL
ADDITIONAL
CUES
WE
STORE
THE
USER
DATA
I
E
WHICH
FLICKR
USER
TOOK
OR
UPLOADED
A
PHOTO
AND
THE
TIMESTAMPS
AS
WE
WILL
SHOW
BELOW
THESE
CUES
ALLOW
US
TO
CLASSIFY
EACH
CLUSTER
CANDIDATE
INTO
EVENT
OR
OBJECT
TYPES
CLUSTERING
DIJ
IMAX
IF
IIJ
IF
IIJ
FOR
EACH
TILE
TK
WE
APPLY
HIERARCHICAL
AGGLOMERATIVE
CLUS
TERING
TO
THE
DISTANCE
MATRIX
OF
EACH
MODALITY
THIS
TABLE
CUT
OFF
DISTANCES
FOR
CLUSTERING
CLUSTERING
APPROACH
WAS
CHOSEN
SINCE
IT
BUILDS
ON
A
DISSIMI
LARITY
MATRIX
AND
IS
NOT
RESTRICTED
TO
METRIC
SPACES
IT
IS
ALSO
RATHER
FLEXIBLE
AND
VERY
FAST
ONCE
THE
FULL
DISTANCE
MATRIX
IS
AVAILABLE
USING
DIFFERENT
LINKING
CRITERIA
FOR
CLUSTER
MERGING
ALLOWS
US
TO
CREATE
DIFFERENT
KINDS
OF
CLUSTERS
WE
EMPLOYED
THE
FOLLOWING
WELL
KNOWN
LINKAGE
METHODS
FIGURE
CLASS
EXAMPLES
OBJECT
EVENT
NONE
OBJECT
IS
DEFINED
AS
ANY
RIGID
PHYSICAL
ITEM
WITH
A
FIXED
POSITION
INCLUDING
LANDMARK
BUILDINGS
STATUES
ETC
AS
EVENTS
WE
CONSIDER
OCCASIONS
THAT
TOOK
PLACE
AT
A
SPECIFIC
TIME
AND
LOCATION
FOR
INSTANCE
CONCERTS
PARTIES
ETC
THUS
SINGLE
LINK
DAB
MIN
I
A
J
B
COMPLETE
LINK
DAB
MAX
I
A
J
B
AVERAGE
LINK
DAB
N
N
DIJ
DIJ
X
DIJ
WE
INCLUDE
AS
FEATURES
THE
NUMBER
OF
UNIQUE
DAYS
THE
PHOTOS
IN
A
CLUSTER
WERE
TAKEN
AT
OBTAINED
FROM
THE
PHOTOS
TIMES
TAMPS
AND
THE
NUMBER
OF
DIFFERENT
USERS
WHO
CONTRIBUTED
PHOTOS
TO
THE
CLUSTER
DIVIDED
BY
THE
CLUSTER
SIZE
D
I
J
I
A
J
B
U
WHERE
A
AND
B
ARE
THE
CLUSTERS
TO
MERGE
AND
I
AND
J
INDEX
THEIR
NI
AND
NJ
ELEMENTS
RESPECTIVELY
THE
MOTIVATION
BEHIND
THESE
MEASURES
IS
TO
CAPTURE
DIF
FERENT
KINDS
OF
VISUAL
PROPERTIES
THAT
ALLOW
US
TO
ASSOCIATE
A
SEMANTIC
INTERPRETATION
WITH
THE
RESULTING
CLUSTERS
SINGLE
LINK
CLUSTERING
ADDS
IMAGES
TO
A
CLUSTER
AS
LONG
AS
THEY
YIELD
A
GOOD
MATCH
TO
AT
LEAST
ONE
CLUSTER
MEMBER
THIS
RESULTS
IN
ELONGATED
CLUSTERS
THAT
TEND
TO
SPAN
A
CERTAIN
AREA
AS
A
RESULT
IF
VISUAL
FEATURES
ARE
THE
BASIS
FOR
CLUSTERING
IT
CAN
GROUP
PANORAMAS
OF
IMAGES
THAT
HAVE
BEEN
TAKEN
FROM
THE
SAME
VIEWPOINT
OR
SERIES
OF
IMAGES
AROUND
AN
OBJECT
IN
CONTRAST
COMPLETE
LINK
CLUSTERING
ENFORCES
THAT
A
NEW
IMAGE
MATCHES
TO
ALL
CLUSTER
MEMBERS
THIS
STRATEGY
WILL
THERE
FORE
RESULT
IN
VERY
TIGHT
CLUSTERS
THAT
CONTAIN
SIMILAR
VIEWS
OF
THE
SAME
OBJECT
OR
BUILDING
AVERAGE
LINK
CLUSTERING
FI
NALLY
TAKES
A
COMPROMISE
BETWEEN
THOSE
TWO
EXTREMES
AND
PROVIDES
CLUSTERS
THAT
STILL
PREFER
VIEWS
OF
THE
SAME
OBJECT
WHILE
ALLOWING
MORE
FLEXIBILITY
IN
VIEWPOINT
SHIFTS
IN
OUR
AP
PROACH
WE
DO
NOT
WANT
TO
RESTRICT
OURSELVES
TO
ANY
SINGLE
OF
THOSE
ALTERNATIVES
INSTEAD
WE
PURSUE
THEM
IN
PARALLEL
SUCH
AN
APPROACH
MAKES
IT
POSSIBLE
TO
DERIVE
ADDITIONAL
INFORMA
TION
FROM
A
COMPARISON
OF
CLUSTER
OUTCOMES
FOR
EXAMPLE
WE
MAY
FIRST
IDENTIFY
DISTINCT
OBJECTS
OR
LANDMARK
BUILDINGS
THROUGH
COMPLETE
OR
AVERAGE
LINK
CLUSTERS
AND
LATER
FIND
OUT
WHICH
OF
THEM
ARE
LOCATED
CLOSE
TO
EACH
OTHER
BY
THEIR
MEM
BERSHIP
IN
THE
SAME
SINGLE
LINK
CLUSTER
TABLE
SUMMARIZES
THE
LINKAGES
AND
CUTOFF
DISTANCES
USED
FOR
EACH
MODALITY
LABELING
CLUSTERS
IN
THE
PRECEDING
SECTIONS
IMAGES
WITH
SIMILAR
CONTENT
OR
ANNOTATIONS
WERE
GROUPED
INTO
CLUSTERS
WHICH
IDEALLY
SHOULD
DEPICT
A
SINGLE
ENTITY
IN
THIS
SECTION
THE
GOAL
IS
TO
LOOK
INTO
THE
CONTENTS
OF
THE
CLUSTERS
IN
MORE
DETAIL
FIRST
WE
CLASSIFY
THE
CLUSTERS
INTO
OBJECTS
LANDMARKS
ETC
AND
EVENTS
IN
A
NEXT
STEP
WE
DERIVE
TEXTUAL
LABELS
FOR
THE
CLUSTERS
FROM
THE
ASSOCIATED
METADATA
FURTHERMORE
WE
INTRODUCE
AN
AP
PROACH
TO
FORMULATE
TEXT
QUERIES
FROM
THE
LABELS
WHICH
ARE
SUBMITTED
TO
WIKIPEDIA
TO
ASSIGN
ARTICLES
TO
THE
CLUSTERS
A
FINAL
VERIFICATION
STEP
USES
THE
IMAGES
FOUND
IN
THE
WIKIPEDIA
ARTICLES
TO
VERIFY
THIS
ASSIGNMENT
CLASSIFICATION
INTO
OBJECTS
AND
EVENTS
TO
DISCRIMINATE
BETWEEN
OBJECTS
AND
EVENTS
WE
RELY
ON
THE
COLLECTED
METADATA
FOR
THE
PHOTOS
IN
EACH
CLUSTER
AN
N
WHERE
D
IS
THE
NUMBER
OF
DAYS
U
THE
NUMBER
OF
USERS
AND
N
THE
NUMBER
OF
PHOTOS
IN
THE
CLUSTER
TYPICALLY
OBJECTS
SUCH
AS
LANDMARKS
ARE
PHOTOGRAPHED
BY
MANY
PEOPLE
THROUGHOUT
THE
YEAR
AN
EVENT
ON
THE
OTHER
HAND
USUALLY
TAKES
PLACE
ONLY
AT
ONE
OR
TWO
DAYS
AND
IS
COVERED
BY
FEWER
USERS
NOTE
THAT
WE
ONLY
CONSIDER
CLUSTERS
WITH
N
HERE
WE
MANUALLY
LABELED
A
GROUND
TRUTH
OF
ABOUT
CLUS
TERS
WITH
THE
CLASS
LABELS
OBJECT
EVENT
AND
NONE
SEE
FIGURE
FOR
AN
EXAMPLE
OF
EACH
CLASS
WE
THEN
TRAINED
AN
INDIVIDUAL
DECISION
TREE
FOR
THE
CLASSES
OBJECT
AND
EVENT
ON
HALF
OF
THE
LABELED
DATA
AND
USED
THE
OTHER
HALF
FOR
VALIDATION
THE
TASK
IN
TRAINING
AND
TESTING
WAS
TO
DISCRIM
INATE
THE
TARGET
CLASS
OBJECT
OR
EVENT
AGAINST
ALL
OTHER
CLASSES
CROSS
VALIDATED
OVER
RANDOM
DATA
PARTITIONS
THIS
SIMPLE
CLASSIFIER
WAS
ABLE
TO
ACHIEVE
PRECISION
FOR
OBJECTS
AND
FOR
EVENTS
WITH
A
STANDARD
DEVIATION
OF
AND
RESPECTIVELY
LINKING
TO
WIKIPEDIA
HAVING
THE
CLUSTERS
CLASSIFIED
INTO
OBJECTS
AND
EVENTS
THE
NEXT
PROCESSING
LAYER
INTENDS
TO
ADD
MORE
DESCRIPTIVE
LABELS
THE
GOAL
IS
TO
NOT
ONLY
LABEL
THE
CLUSTERS
WITH
THE
MOST
DOM
INANT
WORDS
BUT
AUTOMATICALLY
LINK
THEM
TO
CONTENT
ON
THE
INTERNET
SUCH
AS
CORRESPONDING
WIKIPEDIA
ARTICLES
SUCH
A
SOLUTION
ALLOWS
AUTO
ANNOTATION
OF
UNLABELED
IMAGES
EVEN
DOWN
TO
OUTLINING
OBJECT
PARTS
USING
THE
INFORMATION
FROM
OTHER
PICTURES
OF
THE
SAME
ENTITY
POTENTIAL
APPLICATIONS
INCLUDE
MOBILE
TOURIST
GUIDES
WHERE
TOURISTS
USE
THE
INTE
GRATED
CAMERA
OF
THEIR
MOBILE
PHONES
TO
TAKE
A
PICTURE
OF
A
LANDMARK
BUILDING
A
RECOGNITION
SERVICE
BUILDING
UPON
OUR
LABELED
DATABASE
COULD
THEN
MATCH
THE
QUERY
TO
THE
CORRE
SPONDING
DATABASE
ENTRY
AND
RETURN
THE
ASSIGNED
WIKIPEDIA
CONTENT
TO
THE
USER
DEVICE
SUCH
SYSTEMS
HAVE
BEEN
PRO
POSED
BEFORE
E
G
BUT
THE
AUTOMATIC
COLLECTION
OF
THE
DATABASE
FROM
USER
GENERATED
CONTENT
HAS
NOT
BEEN
AD
DRESSED
YET
THE
PROPOSED
APPROACH
FIRST
FINDS
RELEVANT
WORD
COMBI
NATIONS
FROM
THE
TEXT
ASSOCIATED
WITH
EACH
CLUSTER
USING
A
FREQUENT
ITEMSET
MINING
ALGORITHM
THE
RESULTING
FREQUENT
COMBINATIONS
ARE
THEN
USED
TO
QUERY
WIKIPEDIA
IN
A
SECOND
STEP
AN
IMAGE
BASED
MATCHING
STEP
FINALLY
VERIFIES
THAT
THE
LINKS
ARE
INDEED
CORRECT
FREQUENT
LABELS
FLICKR
AND
SIMILAR
COMMUNITY
PHOTO
COLLECTIONS
PROVIDE
US
WITH
TEXT
ASSOCIATED
TO
PHOTOS
HOWEVER
THE
TEXT
IS
OFTEN
NOISY
AND
NOT
ALL
IMAGES
ARE
LABELED
FURTHERMORE
IF
WE
WANT
TO
USE
THE
TEXT
TO
FIND
OUT
MORE
ABOUT
THE
OBJECT
BY
QUERYING
INTERNET
SEARCH
ENGINES
WE
NEED
TO
CREATE
QUERIES
FROM
THE
RAW
TAGS
ANY
COMBINATION
OF
WORDS
FROM
THE
TEXT
COULD
BE
THE
CORRECT
QUERY
HOWEVER
FINDING
AND
TRYING
ALL
POSSIBLE
COMBINATIONS
WOULD
MEAN
CONSIDERING
COM
BINATIONS
OF
WORDS
WHERE
N
CAN
EASILY
BE
IN
THE
HUNDREDS
WE
THEREFORE
RESORT
TO
FREQUENT
ITEMSET
MINING
TO
FIND
THE
MOST
FREQUENT
COMBINATIONS
OF
WORDS
THOSE
CAN
SERVE
AS
LABELS
FOR
THE
OBJECTS
AND
AS
QUERY
INPUT
FOR
THE
NEXT
STAGE
WE
QUICKLY
SUMMARIZE
THE
CONCEPTS
OF
ITEMSET
MINING
ORIGINALLY
FREQUENT
ITEMSET
MINING
ALGORITHMS
WERE
DEVEL
OPED
TO
SOLVE
PROBLEMS
IN
MARKET
BASKET
ANALYSIS
THE
TASK
CONSISTS
OF
DETECTING
RULES
IN
LARGE
NUMBERS
MILLIONS
OF
CUS
TOMER
TRANSACTIONS
WHERE
THE
RULES
DESCRIBE
THE
PROBABILITY
THAT
A
CUSTOMER
BUYS
ITEM
B
GIVEN
THAT
HE
HAS
ALREADY
ITEM
A
IN
HIS
SHOPPING
BASKET
MORE
PRECISELY
AS
SHOWN
IN
THE
PROBLEM
CAN
BE
FORMULATED
AS
FOLLOWS
LET
I
IP
BE
A
SET
OF
P
ITEMS
WE
CALL
A
SUBSET
A
OF
I
WITH
M
ITEMS
AN
M
ITEMSET
A
TRANSACTION
IS
AN
ITEMSET
T
I
WITH
A
TRANSACTION
IDENTIFIER
TID
T
A
TRANSACTION
DATABASE
D
TN
IS
A
SET
OF
TRANSACTIONS
WITH
UNIQUE
IDENTIFIERS
TID
TI
WE
SAY
THAT
A
TRANSACTION
T
SUPPORTS
AN
ITEMSET
A
IF
A
T
WE
CAN
NOW
DEFINE
THE
SUPPORT
OF
AN
ITEMSET
A
D
IN
THE
TRANSACTION
DATABASE
D
AS
FOLLOWS
SUPP
A
T
D
A
T
D
AN
ITEMSET
A
IS
CALLED
FREQUENT
IN
D
IF
SUPP
A
SMIN
WHERE
SMIN
IS
A
THRESHOLD
FOR
THE
MINIMAL
SUPPORT
FRE
QUENT
ITEMSETS
ARE
SUBJECT
TO
THE
MONOTONICITY
PROPERTY
ALL
M
SUBSETS
OF
FREQUENT
M
SETS
ARE
ALSO
FREQUENT
THE
APRIORI
ALGORITHM
WAS
THE
FIRST
TO
TAKE
ADVANTAGE
OF
THE
MONOTONICITY
PROPERTY
TO
FIND
FREQUENT
ITEMSETS
VERY
QUICKLY
IN
OUR
SETTING
THE
TEXT
ASSOCIATED
WITH
EACH
PHOTO
TAGS
CAPTION
TITLES
ETC
GENERATES
A
TRANSACTION
AND
THE
DATABASE
CONSISTS
OF
THE
SET
OF
PHOTOS
IN
A
CLUSTER
WE
USE
AN
IMPLE
MENTATION
OF
THE
FPGROWTH
ALGORITHM
TO
MINE
THE
FREQUENT
ITEMSETS
FOR
EACH
CLUSTER
USING
A
MINIMAL
SUPPORT
THRESH
OLD
OF
IN
ORDER
TO
ENSURE
SCALABILITY
ONLY
THE
TOP
ITEMSETS
PER
CLUSTER
ARE
KEPT
THE
ADVANTAGE
OF
USING
ITEMSET
MINING
OVER
OTHER
PROB
ABILISTIC
METHOD
IS
ITS
SPEED
AND
SCALABILITY
TENS
OF
THOU
SANDS
OF
WORD
COMBINATIONS
CAN
BE
PROCESSED
IN
FRACTIONS
OF
SECONDS
FURTHERMORE
MINING
VARIANTS
SUCH
AS
MAXIMAL
OR
CLOSED
FREQUENT
ITEMSETS
AS
WELL
AS
ADDITIONAL
STATISTI
CAL
TESTS
ON
THE
SETS
OFFER
FURTHER
OPPORTUNITIES
FOR
OPTI
MIZATION
FOR
INSTANCE
MAXIMAL
FREQUENT
ITEMSETS
ITEMSETS
WITH
NO
FREQUENT
SUPERSET
ARE
ESPECIALLY
USEFUL
FOR
HUMAN
READABLE
LABELS
ON
CLUSTERS
SINCE
THEIR
SUBSETS
ARE
NOT
LISTED
AS
ADDITIONAL
LABELS
QUERYING
WIKIPEDIA
AND
LINK
VERIFICATION
WE
USE
EACH
FREQUENT
ITEMSET
MINED
IN
THE
PREVIOUS
SEC
TION
TO
SUBMIT
A
QUERY
TO
AN
INTERNET
SEARCH
ENGINE
MORE
SPECIFICALLY
WE
QUERY
GOOGLE
WWW
GOOGLE
COM
LIMITING
THE
SEARCH
TO
WIKIPEDIA
ORG
BY
DOING
SO
THE
SEARCH
COVERS
WI
KIPEDIA
IN
ALL
AVAILABLE
LANGUAGES
SO
TERMS
IN
DIFFERENT
LAN
GUAGES
CAN
BE
HANDLED
AUTOMATICALLY
FOR
EACH
RESULT
LIST
THE
TOP
RESULTS
ARE
KEPT
NOTE
THAT
IN
THE
WORST
CASE
THIS
GENERATES
POSSIBLE
URLS
PER
CLUSTER
WE
KEEP
A
TABLE
DATASET
STATISTICS
SCORE
FOR
EACH
PAGE
WHICH
COUNTS
HOW
OFTEN
THE
SAME
PAGE
WAS
RETRIEVED
USING
DIFFERENT
QUERIES
NEXT
WE
OPEN
EACH
OF
THE
URLS
AND
PARSE
THE
CORRESPONDING
WIKIPEDIA
PAGE
FOR
IMAGES
THE
IDEA
IS
NOW
TO
USE
THE
WIKIPEDIA
CONTENT
TO
VERIFY
THE
PROPOSED
LINKING
BETWEEN
THE
CLUSTER
AND
THE
WIKIPEDIA
PAGE
CHANCES
ARE
HIGH
THAT
OUR
CLUSTERS
CONTAIN
SOME
IMAGES
TAKEN
FROM
SIMILAR
VIEWPOINTS
AS
THE
ONES
USED
IN
WIKIPEDIA
THUS
WE
EXTRACT
FEATURES
FROM
THE
WIKIPEDIA
IMAGES
AND
TRY
TO
MATCH
THEM
TO
ALL
IMAGES
IN
THE
CLUSTER
USING
THE
SAME
METHOD
AS
DESCRIBED
IN
SECTION
IF
WE
FIND
A
MATCHING
IMAGE
THE
PROPOSED
LINK
IS
KEPT
OTHERWISE
IT
IS
REJECTED
RESULTS
IN
THE
FOLLOWING
WE
PRESENT
RESULTS
ON
THE
WHOLE
DATASET
COLLECTED
TO
THIS
DATE
STEMMING
FROM
THE
GEOGRAPHIC
TILES
THAT
WERE
INSPECTED
BY
OUR
ALGORITHM
WE
FIRST
GIVE
AN
OVERVIEW
OVER
THE
DATASET
FOLLOWED
BY
SUBSECTIONS
DISCUSSING
THE
RESULTS
OF
THE
INDIVIDUAL
PROCESSING
LAYERS
TABLE
SUM
MARIZES
THE
DATASET
STATISTICS
IN
TOTAL
OVER
IMAGES
WERE
DOWNLOADED
FROM
FLICKR
THEIR
VISUAL
FEATURES
AMOUNT
ING
TO
GB
AND
THEIR
METADATA
TAGS
GEOTAGS
EXIF
DATA
ETC
TO
OVER
MILLION
PAIRWISE
SIMILARITIES
HAD
TO
BE
COMPUTED
LESS
THAN
MILLION
WAS
GREATER
THAN
ZERO
NOTE
THAT
WITHOUT
THE
GEOGRAPHIC
TILING
WE
WOULD
HAVE
HAD
TO
CALCULATE
OVER
BILLION
PAIRWISE
SIMILARITIES
IN
THE
END
A
LITTLE
OVER
PHOTOS
COULD
BE
ASSIGNED
TO
A
CLUSTER
CLUSTERS
HERE
WE
PRESENT
RESULTS
FOR
DIFFERENT
TYPES
OF
CLUSTERING
WE
START
WITH
A
SPECIFIC
EXAMPLE
TO
GIVE
AN
IMPRESSION
OF
THE
RESULTS
WE
FOUND
FIGURE
SHOWS
EXAMPLES
FROM
THE
AREA
AROUND
THE
PANTHEON
IN
ROME
THE
CORRESPONDING
TILE
IS
AMONG
THOSE
WITH
THE
LARGEST
NUMBER
OF
ELEMENTS
CONTAIN
ING
250
IMAGES
SEVERAL
TILES
OVERLAP
HERE
WE
REPORT
THE
NUMBERS
FOR
THE
DOMINANT
ONE
IT
IS
WELL
VISIBLE
HOW
THE
CLUSTERING
SPLITS
THE
DATA
INTO
SEVERAL
SEMANTICALLY
SEPARATE
OBJECTS
AND
CONTEXTS
FOR
EXAMPLE
INDOOR
A
AND
FRONTAL
OUTDOOR
VIEWS
B
OF
THE
PANTHEON
ARE
FOUND
AS
SEPARATE
ENTITIES
BOTH
CONTAIN
A
LARGE
NUMBER
OF
PHOTOS
AND
RESPECTIVELY
SMALLER
CLUSTERS
DESCRIBE
MORE
SPECIFIC
EL
EMENTS
SUCH
AS
THE
VIEW
FROM
THE
PANTHEON
ONTO
THE
PIAZZA
E
THE
OBELISK
SITUATED
BEHIND
THE
PANTHEON
C
AND
EVEN
THE
TOMB
OF
VICTOR
EMMANUEL
II
D
INSIDE
THE
PANTHEON
CALCULATING
THE
MEAN
OF
THE
PHOTO
LOCATIONS
IN
EACH
CLUSTER
ALLOWS
US
TO
PLACE
THE
CLUSTER
ON
A
MAP
CLEARLY
THE
LOCA
TIONS
OF
THE
DIFFERENT
CLUSTERS
ARE
ESTIMATED
VERY
CLOSE
TO
THE
TRUE
POSITIONS
OF
THE
CORRESPONDING
ENTITIES
THE
CLUSTERS
SHOWN
IN
THIS
FIGURE
WERE
OBTAINED
USING
SINGLE
LINK
CLUSTER
ING
NOTE
HOW
ESPECIALLY
FOR
CLUSTERS
A
B
AND
C
THIS
ALLOWS
US
TO
MERGE
A
WIDE
VARIETY
OF
VIEWS
OF
THE
SAME
OBJECT
SINCE
ONLY
THE
CLOSEST
MATCHING
PAIR
HAS
TO
BE
CONNECTED
BY
A
DISTANCE
SMALLER
THAN
THE
THRESHOLD
IN
TOTAL
CLUSTERS
WERE
FOUND
IN
THIS
AREA
WITH
A
MEAN
A
B
C
E
D
FIGURE
CLUSTERS
FOUND
AROUND
THE
PANTHEON
AND
THE
NUMBER
OF
PHOTOS
CONTAINED
IN
EACH
NOTE
THE
AUTOMATIC
SEPARATION
INTO
INDOOR
A
OUTDOOR
B
AND
PANORAMA
VIEWS
E
AND
THE
DISCOVERY
OF
SEPARATE
OBJECTS
C
D
MEAN
LOCATIONS
OF
THE
PHOTOS
ARE
SHOWN
ON
THE
MAP
E
IS
ESTIMATED
AT
ABOUT
THE
SAME
POSITION
AS
B
AND
IS
THEREFORE
NOT
DRAWN
ON
THE
MAP
SIZE
OF
PHOTOS
WE
EVALUATE
CLUSTERING
ACCURACY
IN
TERMS
OF
THE
CLUSTER
PRECISION
I
E
THE
NUMBER
OF
CORRECT
IMAGES
DIVIDED
BY
THE
TOTAL
NUMBER
OF
IMAGES
IN
THE
CLUSTER
AS
CORRECT
WE
COUNT
EVERY
IMAGE
WHICH
CONTAINS
THE
OBJECT
THE
CLUSTER
REFERS
TO
IF
THERE
ARE
SPECIAL
CONTEXTS
SUCH
AS
AN
INDOOR
VIEW
FOR
AN
OBJECT
ONLY
THOSE
E
G
INDOOR
VIEWS
ARE
COUNTED
AS
CORRECT
GIVEN
THAT
DEFINITION
THE
MEAN
PRECISION
OF
THE
LARGEST
CLUSTERS
IS
OVER
NOTE
THAT
SINCE
WE
DEAL
WITH
AN
UNSUPERVISED
MINING
PROBLEM
WE
CANNOT
GIVE
RELIABLE
RESULTS
FOR
RECALL
FOR
COMPARISON
WE
ALSO
RAN
A
CLUSTERING
BASED
PURELY
ON
TEXT
USING
ALL
TEXT
SIMILARITIES
BETWEEN
THE
PHOTOS
IN
THIS
AREA
DEPENDING
ON
THE
PARAMETERS
WE
WERE
ONLY
ABLE
TO
GET
CLUSTERS
WITH
A
PRECISION
OF
ABOUT
NOT
ONLY
WERE
WE
NOT
ABLE
TO
DISCRIMINATE
BETWEEN
INDOOR
AND
OUT
DOOR
VIEWS
BASED
ON
TEXT
FEATURES
THE
CLUSTERS
ALSO
CONTAINED
MANY
OUTLIERS
WHICH
DID
NOT
CONTAIN
THE
RELEVANT
OBJECT
AT
ALL
FOR
INSTANCE
ONLY
OF
THE
PHOTOS
IN
THE
AREA
CARRY
TAGS
SUCH
AS
INSIDE
OR
INTERIOR
MAKING
A
DISCRIMINATION
BASED
ON
TEXT
VERY
DIFFICULT
IN
CONTRAST
CLUSTER
A
IN
FIG
URE
CONTAINS
OVER
PHOTOS
OF
THE
INSIDE
OF
THE
PANTHEON
THE
WORD
PANTHEON
APPEARS
WITH
PHOTOS
ALSO
IN
COMPARISON
TO
WE
ARE
ABLE
TO
RETRIEVE
LARGER
CLUSTERS
WHILE
MAINTAINING
HIGH
PRECISION
TO
EXAMINE
THE
RESULTS
OF
THE
DIFFERENT
TYPES
OF
VISUAL
CLUS
TERING
FURTHER
CONSIDER
ANOTHER
EXAMPLE
SHOWN
IN
FIGURE
IT
DEPICTS
THE
AREA
AROUND
THE
LOUVRE
IN
PARIS
FIGURE
A
SHOWS
THE
ESTIMATED
MEAN
POSITIONS
OF
SINGLE
LINK
CLUSTERS
IN
TOTAL
THE
AREA
IS
COVERED
BY
CLUSTERS
THE
LARGEST
CLUS
TER
CONTAINS
ELEMENTS
THE
MEAN
SIZE
IS
ELEMENTS
ONE
OF
THE
CLUSTERS
MARKED
IN
YELLOW
IS
SHOWN
IN
FIGURE
B
HERE
EACH
PIN
REPRESENTS
THE
LOCATION
OF
ONE
PHOTO
NOTE
HOW
STRONGLY
THE
POSITIONS
VARY
SOME
EXAMPLES
OF
THE
CLUS
TERS
CONTENTS
ARE
SHOWN
IN
THE
COLUMN
NEXT
TO
THE
MAP
AGAIN
VISUALIZING
THE
MENTIONED
VARIABILITY
IN
VIEWPOINTS
IN
CONTRAST
FIGURE
C
SHOWS
THE
COMPLETE
LINK
CLUSTERS
FOR
THE
SAME
AREA
THE
MORE
RESTRICTIVE
CLUSTERING
CRITERION
RESULTS
IN
SMALLER
AND
MORE
COMPACT
CLUSTERS
THE
MEAN
SIZE
IS
ONLY
ELEMENTS
AND
THE
MAXIMUM
IS
COMPLETE
LINK
CLUS
TERS
WERE
FOUND
FOR
THIS
REGION
AGAIN
ONE
CLUSTER
IS
SELECTED
AND
ITS
ELEMENTS
ARE
SHOWN
IN
FIGURE
D
THEIR
LOCATIONS
ARE
MORE
COMPACT
AND
THE
CONTENTS
OF
THE
CLUSTER
HAVE
LESS
VARIABILITY
AS
THE
EXAMPLES
NEXT
TO
THE
MAP
DEMONSTRATE
ALSO
NOTE
AGAIN
THE
GRID
OVERLAID
ON
THE
MAPS
IN
A
AND
C
WHICH
SHOWS
THE
TILES
WE
USED
TO
RETRIEVE
PHOTOS
BY
THEIR
GEOTAGS
AGAIN
CELLS
MAKE
UP
A
TILE
OBJECTS
AND
EVENTS
THE
CLASSIFIER
DESCRIBED
IN
SECTION
ALLOWS
US
NOT
ONLY
TO
DETECT
OBJECTS
BUT
IN
SOMETIMES
EVEN
EVENTS
APPLYING
THE
TREE
TO
THE
ENTIRE
DATASET
RESULTED
IN
THE
FOLLOWING
DISTRIBUTION
OF
OBJECTS
AND
EVENTS
OF
CLUSTERS
SINGLE
LINK
WERE
CLASSIFIED
AS
OBJECTS
AS
EVENTS
VI
SUAL
INSPECTION
ON
RANDOMLY
PICKED
CLUSTERS
SHOWED
THAT
THE
CLASSIFICATION
PRECISION
IS
VERY
ACCURATE
SIMILAR
TO
THE
RESULTS
OBTAINED
ON
THE
VALIDATION
SET
IN
SECTION
FIGURE
SHOWS
SOME
EXAMPLES
OF
EVENT
CLUSTERS
THE
FIRST
CLUSTER
CONTAINS
IMAGES
FROM
DIFFERENT
EVENTS
IN
A
SERIES
TAKING
PLACE
ON
DIF
FERENT
DAYS
OXFORD
GEEK
NIGHTS
AND
WAS
RECOGNIZED
DUE
TO
THE
SAME
LOCATION
IT
TOOK
PLACE
IN
THE
SECOND
A
MOVIE
PREMIERE
IN
ITALY
AND
THIRD
EVENT
AN
EXHIBITION
IN
A
GALLERY
IN
PARIS
WERE
BOTH
COVERED
BY
TWO
PHOTOGRAPHERS
THE
LAST
LINE
REPRESENTS
THE
MAJORITY
OF
EVENTS
AN
EVENT
FROM
A
SINGLE
DAY
COVERED
BY
ONLY
ONE
PHOTOGRAPHER
THE
SMALLER
NUM
BER
OF
EVENT
CLUSTERS
CAN
BE
EXPLAINED
BY
TWO
FACTORS
RELYING
MOSTLY
ON
VISUAL
CUES
WE
CAN
ONLY
DETECT
EVENTS
WHICH
TAKE
A
B
C
D
FIGURE
CLUSTERS
AROUND
THE
LOUVRE
A
SHOWS
SINGLE
LINK
CLUSTERS
THE
PHOTOS
OF
THE
CLUSTER
MARKED
IN
YELLOW
ARE
LOCATED
AS
SHOWN
IN
B
C
SHOWS
COMPLETE
LINK
CLUSTERS
FOR
THE
SAME
AREA
AGAIN
WITH
THE
PHOTOS
OF
THE
YELLOW
CLUSTER
IN
D
ONLY
CLUSTERS
WITH
AT
LEAST
ELEMENTS
ARE
SHOWN
FIGURE
TYPICAL
EVENTS
MINED
BY
OUR
METHODS
PLACE
IN
A
ENVIRONMENT
WHERE
THE
BACKGROUND
MATCHES
BE
TWEEN
PHOTOS
SECOND
IT
SEEMS
THAT
SO
FAR
IN
GENERAL
FEWER
PEOPLE
GEOTAG
PHOTOS
OF
EVENTS
LINKING
TO
WIKIPEDIA
FIGURE
VISUALIZES
THE
INDIVIDUAL
STEPS
IN
LINKING
CLUSTERS
TO
WIKIPEDIA
CONTENT
THE
TAGS
FOR
THE
CLUSTER
A
ARE
MINED
TO
CREATE
FREQUENT
ITEMSETS
B
NOTE
HOW
THE
PROXIMITY
TO
THE
LOUVRE
INTRODUCES
NOISY
WORDS
SUCH
AS
MUSEUM
AND
HOW
THE
EXPRESSION
ARC
DU
TRIOMPHE
COULD
REFER
ALSO
TO
THE
OTHER
LARGER
ARC
DU
TRIOMPHE
IN
PARIS
THE
FREQUENT
ITEMSETS
B
ARE
FED
AS
QUERIES
TO
GOOGLE
AND
THE
CANDIDATE
URLS
C
ARE
RETRIEVED
FOR
EACH
URL
THE
IMAGES
CONTAINED
IN
THE
PAGE
ARE
EXTRACTED
AND
MATCHED
BACK
TO
THE
IMAGES
IN
THE
CLUSTER
A
FIGURE
D
SHOWS
THE
BEST
MATCH
FROM
THE
CLUSTER
WITH
THE
IMAGE
FROM
THE
WIKIPEDIA
ARTICLE
E
THE
FINAL
SELECTED
URL
IS
GIVEN
IN
F
FIGURE
SHOWS
SOME
TYPICAL
RESULTS
OF
THIS
PROCESS
EACH
RESULT
IS
REPRESENTED
BY
A
PAIR
OF
IMAGES
THE
LEFT
IMAGE
WAS
EXTRACTED
FROM
WIKIPEDIA
THE
ONE
ON
THE
RIGHT
IS
ITS
CLOSEST
MATCH
IN
THE
CLUSTER
THERE
ARE
TYPICALLY
MANY
MORE
MATCH
ING
IMAGES
IN
EACH
CLUSTER
BELOW
EACH
PAIR
WE
PROVIDE
THE
URL
OF
THE
MINED
WIKIPEDIA
ARTICLE
FOLLOWED
BY
THE
CLUSTER
STATISTICS
FOR
EACH
CLUSTER
WE
REPORT
THE
NUMBER
OF
PHO
TOS
THE
NUMBER
OF
USERS
WHO
TOOK
THEM
AND
THE
NUMBER
OF
DIFFERENT
DAYS
THE
PHOTOS
WERE
TAKEN
AT
WE
ALSO
REPORT
THE
PRECISION
OBTAINED
AGAIN
BY
MANUAL
INSPECTION
AS
DESCRIBED
ABOVE
IN
GENERAL
THE
PRECISION
IS
VERY
HIGH
RANGING
BETWEEN
AND
ESPECIALLY
VERY
WELL
KNOWN
LANDMARKS
SUCH
AS
THE
SACRE
COEUR
THE
COLOSSEUM
OR
THE
TREVI
FOUNTAIN
ARE
COVERED
BY
A
LARGE
NUMBER
OF
PHOTOS
WITH
VERY
FEW
FALSE
POSITIVES
LESSER
KNOWN
OBJECTS
SUCH
AS
THE
RADCLIFFE
CAMERA
HAVE
FEWER
IMAGES
AND
ARE
THUS
ALSO
MORE
VULNERABLE
TO
A
FEW
FALSE
POSITIVES
STAYING
WITH
THE
RADCLIFFE
CAMERA
NOTE
HOW
MULTIPLE
MATCHING
WIKIPE
DIA
ARTICLES
HAVE
BEEN
VERIFIED
FOR
THE
OBJECT
THE
SAME
EFFECT
CAN
BE
OBSERVED
IN
EXAMPLE
OR
EXAMPLE
WHERE
AR
TICLES
IN
MULTIPLE
LANGUAGES
WERE
RETRIEVED
SOME
MATCHES
ARE
TRULY
AMAZING
FOR
INSTANCE
EXAMPLE
WHERE
A
PAINT
ING
MATCHED
TO
A
PHOTO
OF
THE
COLOSSEUM
OR
AND
WITH
STRONG
CLUTTER
AND
VIEWPOINT
CHANGE
WHILE
MOST
EXAMPLES
IN
FIGURE
REFER
TO
RATHER
WELL
KNOWN
OBJECTS
SOME
RARE
GEMS
WERE
MINED
TOO
A
FEW
EXAMPLES
ARE
SHOWN
IN
FIGURE
EXAMPLE
DOES
NOT
ONLY
LINK
TO
THE
ARTICLE
SAINTE
CHAPELLE
BUT
ALSO
TO
AN
ARTICLE
ABOUT
STAINED
GLASS
SIMILARLY
MONA
LISA
IS
LINKED
TO
A
SPECIFIC
ARTICLE
AND
A
MORE
GENERAL
ONE
ABOUT
LEONARDO
DA
VINCI
IN
EXAM
PLE
BOTH
THE
CONTEXT
FORUM
ROMANUM
AND
THE
SPECIFIC
TEMPLE
OF
VESTA
COULD
BE
VERIFIED
EXAMPLES
OF
SMALLER
EVEN
LESSER
KNOWN
ENTITIES
ARE
SHOWN
IN
NOTE
THE
MAY
POLE
ON
VIKTUALIENMARKT
IN
MUNICH
IN
ONE
OF
THE
ARTICLES
EXPLAINS
THE
LOCATION
THE
OTHER
THE
TRADITION
DESTINATIONS
WITH
FEWER
TOURISTS
SUCH
AS
TALLINN
AND
ZURICH
TEND
TO
HAVE
LESS
PHOTO
COVERAGE
AND
ALSO
LESS
CONTENT
ON
WIKIPEDIA
NEVERTHELESS
SOME
LOCATIONS
COULD
BE
IDENTIFIED
BY
OUR
MIN
ING
PIPELINE
FINALLY
EXAMPLE
IS
A
LUCKY
SHOT
WHERE
AN
EVENT
COULD
BE
LINKED
TO
A
PERSON
AND
VERIFIED
BY
COINCI
DENCE
WIKIPEDIA
CONTAINS
AN
IMAGE
OF
AN
EVENT
JULES
VERNE
ADVENTURES
FILM
FESTIVAL
APRIL
WHICH
IS
ALSO
COV
ERED
ON
FLICKR
AND
LABELED
WITH
THE
ATTENDING
ACTORS
NAME
CLEARLY
ONLY
LARGER
EVENTS
ARE
COVERED
IN
WIKIPEDIA
SO
THAT
THE
CHANCE
OF
DETECTING
A
CORRECT
LINK
FOR
ANY
EVENT
IS
RATHER
SMALL
FURTHERMORE
HOMOGRAPHY
BASED
MATCHING
BETWEEN
IMAGES
IS
WELL
SUITED
FOR
RIGID
OBJECTS
AND
SCENES
BUT
LESS
SUITED
FOR
EVENTS
FUTURE
WORK
COULD
THUS
EXTEND
THE
SYSTEM
BY
CLASSIFYING
EVENT
SCENES
WEDDING
CONCERT
ETC
BASED
ON
A
BAG
OF
FEATURES
APPROACH
AND
RATHER
LABEL
IT
USING
THE
TEXTUAL
META
DATA
THAN
LINK
IT
TO
WIKIPEDIA
IN
TOTAL
UNIQUE
WIKIPEDIA
ARTICLES
WERE
VERIFIED
BY
MATCHING
THEIR
IMAGES
TO
OUR
CLUSTERS
AS
DESCRIBED
ABOVE
THE
PRECISION
OF
THIS
ASSIGNMENT
WAS
ABOUT
I
E
OF
THE
ARTICLES
REFERRED
TO
A
CLUSTER
WHICH
CONTAINED
IMAGES
OF
THE
ARTICLE
CORRECT
SUBJECT
THESE
ARTICLES
COVERED
SINGLE
LINK
CLUSTERS
QUERYING
WIKIPEDIA
WITH
THE
QUERIES
GIVEN
BY
THE
FREQUENT
ITEMSETS
HAD
RESULTED
IN
OVER
URLS
FOR
CONSIDERATION
AND
IN
MORE
THAN
TWICE
AS
MANY
IMAGES
THIS
DEMONSTRATES
HOW
EFFECTIVE
OUR
METHOD
IS
IN
MINING
RELEVANT
LINKS
OUT
OF
A
VAST
AMOUNT
OF
IRRELEVANT
DATA
AUTO
ANNOTATION
WITH
THE
DATABASE
WE
BUILT
IN
THIS
PAPER
AUTO
ANNOTATION
OF
UNLABELED
PHOTOS
WITH
THEIR
GEO
LOCATION
AND
CORRESPOND
ING
WIKIPEDIA
ARTICLE
BECOMES
FEASIBLE
A
USER
CAN
SIMPLY
MUSEUM
MUSEUM
LOUVRE
CARROUSEL
CARROUSEL
TRIOMPHE
CARROUSEL
TRIOMPHE
ARC
CARROUSEL
TRIOMPHE
ARC
DU
CARROUSEL
TRIOMPHE
DU
CARROUSEL
ARC
CARROUSEL
ARC
DU
CARROUSEL
DU
TRIOMPHE
TRIOMPHE
ARC
TRIOMPHE
ARC
DU
TRIOMPHE
DU
ARC
ARC
DU
B
C
D
E
F
FIGURE
MATCHING
CLUSTERS
TO
WIKIPEDIA
ARTICLES
THE
TEXT
FOR
THE
PHOTOS
IN
A
CLUSTER
A
IS
MINED
FOR
FREQUENT
WORD
COMBINATIONS
B
WHICH
ARE
USED
TO
SEARCH
WIKIPEDIA
FOR
CANDIDATE
URLS
C
EACH
IMAGE
OF
AN
ARTICLE
IS
IN
RETURN
MATCHED
TO
THE
IMAGES
IN
THE
CLUSTER
IF
A
GOOD
MATCH
E
CAN
BE
FOUND
THE
CANDIDATE
LINK
IS
SELECTED
F
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
SEE
MATCHTED
TO
THE
SAME
CLUSTER
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
FIGURE
A
WORLD
TOUR
WITH
FLICKR
AND
WIKIPEDIA
THE
LEFT
IMAGE
IN
EACH
PAIR
STEMS
FROM
WIKIPEDIA
THE
RIGHT
IMAGE
IS
THE
BEST
MATCH
IN
A
MINED
CLUSTER
THE
WIKIPEDIA
LINKS
WHICH
COULD
BE
VERIFIED
THIS
WAY
ARE
REPORTED
BELOW
THE
IMAGES
TOGETHER
WITH
THE
CLUSTER
STATISTICS
NOTE
THE
HIGH
PRECISION
SCORES
AND
THE
SIZE
OF
SOME
CLUSTERS
SEE
TEXT
FOR
A
DETAILED
DISCUSSION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECSION
ELEMENTS
USERS
DAYS
PRECISON
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
ELEMENTS
USERS
DAYS
PRECISION
FIGURE
ADDITIONAL
SURPRISING
MINING
RESULTS
SEE
TEXT
FOR
A
DISCUSSION
SELECT
THE
ROUGH
GEOGRAPHIC
AREA
E
G
BY
DRAWING
A
BOUND
ING
BOX
AROUND
PARIS
ON
THE
MAP
AND
THE
PHOTOS
WILL
BE
AUTOMATICALLY
PLACED
AT
THEIR
EXACT
POSITION
AND
LINKED
TO
RELEVANT
WIKIPEDIA
ARTICLES
TO
DEMONSTRATE
THIS
CAPABILITY
WE
DOWNLOADED
SAMPLE
QUERY
IMAGES
OF
SIGHTS
IN
PARIS
FROM
GOOGLE
SEE
FIGURE
THESE
ARE
IMAGES
WHICH
ARE
NEITHER
PRESENT
ON
FLICKR
NOR
ON
WIKIPEDIA
WE
LOAD
ALL
CLUSTERS
WHICH
WE
FOUND
IN
THE
PARIS
AREA
FULL
AREA
AS
GIVEN
IN
TA
BLE
AND
WHICH
COULD
BE
ASSIGNED
TO
A
WIKIPEDIA
ARTICLE
AS
DESCRIBED
IN
THE
PREVIOUS
STEPS
THESE
CONDITIONS
HOLD
FOR
CLUSTERS
NOW
WE
SIMPLY
MATCH
THE
QUERY
IMAGES
TO
THE
CLUSTERS
AND
RECORD
THE
BEST
MATCHING
IMAGE
AND
CLUSTER
THIS
PROCESS
ONLY
TAKES
MINUTES
AND
THE
RESULT
IS
SHOWN
IN
FIGURE
THE
RESULT
LOCATION
IS
SELECTED
AS
THE
MEAN
LOCA
TION
OF
ALL
IMAGES
IN
THE
MATCHING
CLUSTER
NOTE
THE
PRECISION
OF
THE
PLACEMENT
IN
THE
MAGNIFIED
MAP
ELEMENTS
ALL
IMAGES
ARE
ALSO
LINKED
TO
THE
CORRECT
WIKIPEDIA
ARTICLE
IN
THE
SPIRIT
OF
FIGURES
AND
THE
LINKS
ARE
NOT
SHOWN
DUE
TO
LACK
OF
SPACE
NOTE
HOW
SIMILAR
THE
ARC
DE
TRIOMPHE
AND
ARC
DE
TRIOMPHE
DU
CAROUSEL
ARE
FIRST
AND
SECOND
IMAGE
IN
THE
LEFT
COLUMN
ALSO
NOTE
HOW
CLOSE
THE
TWO
OBJECTS
ARC
DE
TRIOM
PHE
DU
CAROUSEL
AND
THE
LOUVRE
PYRAMID
ARE
SECOND
AND
THIRD
MAP
IN
THE
LEFT
COLUMN
OUR
METHOD
IS
ABLE
TO
HAN
DLE
THESE
UNCERTAINTIES
ROBUSTLY
AND
TO
DISCRIMINATE
BETWEEN
SIMILAR
OBJECTS
AT
DIFFERENT
LOCATIONS
AND
DIFFERENT
OBJECTS
AT
THE
SAME
LOCATION
IN
CONTRAST
A
DIRECT
MATCHING
OF
QUERY
IMAGES
TO
WIKIPEDIA
IMAGES
WOULD
NOT
BE
POSSIBLE
IN
MOST
CASES
SINCE
THE
VIEWPOINT
CHANGES
MIGHT
BE
TOO
LARGE
THE
NUMBER
OF
IMAGES
IN
OUR
CLUSTERS
LITERALLYBRIDGES
THE
GAP
BETWEEN
THE
UNANNOTATED
QUERY
IMAGE
AND
THE
WIKIPEDIA
IMAGE
VIA
THE
CLUSTERS
CREATED
FROM
FLICKR
DATA
COMBINING
THIS
METHOD
WITH
SCALABLE
INDEXING
FOR
LOCAL
FEATURES
WILL
ALLOW
AUTO
ANNOTATION
OF
MANY
HOLIDAY
SNAPS
WITHIN
SECONDS
CONCLUSIONS
WE
HAVE
PRESENTED
A
FULLY
UNSUPERVISED
MINING
PIPELINE
FOR
COMMUNITY
PHOTO
COLLECTIONS
THE
SOLE
INPUT
IS
A
GRID
OF
TILES
ON
A
WORLD
MAP
THE
OUTPUT
IS
A
DATABASE
OF
MINED
OBJECTS
AND
EVENTS
MANY
OF
THEM
LABELED
WITH
AN
AUTOMAT
ICALLY
CREATED
AND
VERIFIED
LINK
TO
WIKIPEDIA
THE
PIPELINE
CHAINS
PROCESSING
STEPS
OF
SEVERAL
MODALITIES
IN
A
HIGHLY
EF
FECTIVE
WAY
THE
BASIS
IS
A
PAIRWISE
SIMILARITY
CALCULATION
WITH
LOCAL
VISUAL
FEATURES
AND
MULTI
VIEW
GEOMETRY
FOR
EACH
TILE
HIERARCHICAL
CLUSTERING
WAS
DEMONSTRATED
TO
BE
A
VERY
EFFECTIVE
METHOD
TO
EXTRACT
CLUSTERS
OF
THE
SAME
ENTITIES
IN
DIFFERENT
CONTEXTS
INDOOR
OUTDOOR
ETC
WE
OBSERVED
THAT
THE
CLUSTERING
STEP
ON
VISUAL
DATA
IS
FAR
MORE
RELIABLE
THAN
ON
TEXT
LABELS
A
SIMPLE
TREE
BASED
CLASSIFIER
ON
THE
METADATA
OF
PHOTOS
WAS
INTRODUCED
TO
DISCRIMINATE
BETWEEN
OBJECT
AN
EVENT
CLUSTERS
ITEMSET
MINING
ON
THE
TEXT
OF
THE
CLUSTERS
CREATED
WITH
VISUAL
FEATURES
WAS
PROPOSED
TO
MINE
FREQUENT
WORD
COMBINATIONS
PER
CLUSTER
THOSE
WERE
USED
TO
SEARCH
WIKIPEDIA
FOR
POTENTIALLY
RELEVANT
ARTICLES
THE
RELEVANCE
WAS
VERIFIED
BY
MATCHING
IMAGES
FROM
THE
WIKIPEDIA
ARTI
CLES
BACK
TO
THE
MINED
CLUSTERS
BOTH
THE
CLUSTERING
AND
LINKING
TO
WIKIPEDIA
SHOWED
HIGH
PRECISION
FINALLY
IN
A
LAST
STEP
WE
DEMONSTRATED
HOW
THE
DATABASE
CAN
BE
USED
TO
AUTO
ANNOTATE
UNLABELED
IMAGES
WITHOUT
GEOTAGS
BESIDES
THE
EFFECTIVE
MINING
PIPELINE
PROPOSED
IN
THE
PA
PER
WE
ALSO
CARRIED
OUT
ONE
OF
THE
LARGEST
EXPERIMENTS
WITH
LOCAL
VISUAL
FEATURES
ON
DATA
FROM
COMMUNITY
PHOTO
COL
LECTIONS
BY
PROCESSING
OVER
PHOTOS
THE
RESULTS
OF
THIS
LARGE
SCALE
EXPERIMENT
ARE
VERY
ENCOURAGING
AND
OPEN
A
WEALTH
OF
NOVEL
RESEARCH
OPPORTUNITIES
FIGURE
AUTO
ANNOTATION
OF
NOVEL
IMAGES
USING
THE
MINED
CLUSTERS
ASSESSING
THE
QUALITY
OF
ACTIONS
HAMED
PIRSIAVASH
CARL
VONDRICK
ANTONIO
TORRALBA
MASSACHUSETTS
INSTITUTE
OF
TECHNOLOGY
HPIRSIAV
VONDRICK
TORRALBA
MIT
EDU
ABSTRACT
WHILE
RECENT
ADVANCES
IN
COMPUTER
VISION
HAVE
PROVIDED
RELI
ABLE
METHODS
TO
RECOGNIZE
ACTIONS
IN
BOTH
IMAGES
AND
VIDEOS
THE
PROBLEM
OF
ASSESSING
HOW
WELL
PEOPLE
PERFORM
ACTIONS
HAS
BEEN
LARGELY
UNEXPLORED
IN
COMPUTER
VISION
SINCE
METHODS
FOR
ASSESSING
ACTION
QUALITY
HAVE
MANY
REAL
WORLD
APPLICATIONS
IN
HEALTHCARE
SPORTS
AND
VIDEO
RETRIEVAL
WE
BE
LIEVE
THE
COMPUTER
VISION
COMMUNITY
SHOULD
BEGIN
TO
TACKLE
THIS
CHALLENG
ING
PROBLEM
TO
SPUR
PROGRESS
WE
INTRODUCE
A
LEARNING
BASED
FRAMEWORK
THAT
TAKES
STEPS
TOWARDS
ASSESSING
HOW
WELL
PEOPLE
PERFORM
ACTIONS
IN
VIDEOS
OUR
APPROACH
WORKS
BY
TRAINING
A
REGRESSION
MODEL
FROM
SPA
TIOTEMPORAL
POSE
FEATURES
TO
SCORES
OBTAINED
FROM
EXPERT
JUDGES
MORE
OVER
OUR
APPROACH
CAN
PROVIDE
INTERPRETABLE
FEEDBACK
ON
HOW
PEOPLE
CAN
IMPROVE
THEIR
ACTION
WE
EVALUATE
OUR
METHOD
ON
A
NEW
OLYMPIC
SPORTS
DATASET
AND
OUR
EXPERIMENTS
SUGGEST
OUR
FRAMEWORK
IS
ABLE
TO
RANK
THE
ATHLETES
MORE
ACCURATELY
THAN
A
NON
EXPERT
HUMAN
WHILE
PROMISING
OUR
METHOD
IS
STILL
A
LONG
WAY
TO
RIVALING
THE
PERFORMANCE
OF
EXPERT
JUDGES
INDICATING
THAT
THERE
IS
SIGNIFICANT
OPPORTUNITY
IN
COMPUTER
VISION
RE
SEARCH
TO
IMPROVE
ON
THIS
DIFFICULT
YET
IMPORTANT
TASK
INTRODUCTION
RECENT
ADVANCES
IN
COMPUTER
VISION
HAVE
PROVIDED
RELIABLE
METHODS
FOR
REC
OGNIZING
ACTIONS
IN
VIDEOS
AND
IMAGES
HOWEVER
THE
PROBLEM
OF
AUTOMATICALLY
QUANTIFYING
HOW
WELL
PEOPLE
PERFORM
ACTIONS
HAS
BEEN
LARGELY
UNEXPLORED
WE
BELIEVE
THE
COMPUTER
VISION
COMMUNITY
SHOULD
BEGIN
TO
TACKLE
THE
CHAL
LENGING
PROBLEM
OF
ASSESSING
THE
QUALITY
OF
PEOPLE
ACTIONS
BECAUSE
THERE
ARE
MANY
IMPORTANT
REAL
WORLD
APPLICATIONS
FOR
EXAMPLE
IN
HEALTH
CARE
PATIENTS
ARE
OFTEN
MONITORED
AND
EVALUATED
AFTER
HOSPITALIZATION
AS
THEY
PERFORM
DAILY
TASKS
WHICH
IS
EXPENSIVE
UNDERTAKING
WITHOUT
AN
AUTOMATIC
ASSESSMENT
METHOD
IN
SPORTS
ACTION
QUALITY
ASSESSMENTS
WOULD
ALLOW
AN
ATHLETE
TO
PRACTICE
IN
FRONT
OF
FIG
WE
INTRODUCE
A
LEARNING
FRAMEWORK
FOR
ASSESSING
THE
QUALITY
OF
HUMAN
ACTIONS
FROM
VIDEOS
SINCE
WE
ESTIMATE
A
MODEL
FOR
WHAT
CON
STITUTES
A
HIGH
QUALITY
ACTION
OUR
METHOD
CAN
ALSO
PROVIDE
FEEDBACK
ON
HOW
PEOPLE
CAN
IMPROVE
THEIR
AC
TIONS
VISUALIZED
WITH
THE
RED
ARROWS
A
CAMERA
AND
RECEIVE
QUALITY
SCORES
IN
REAL
TIME
PROVIDING
THE
ATHLETE
WITH
RAPID
FEEDBACK
AND
AN
OPPORTUNITY
TO
IMPROVE
THEIR
ACTION
IN
RETRIEVAL
A
VIDEO
SEARCH
ENGINE
MAY
WANT
TO
SORT
RESULTS
BASED
ON
THE
QUALITY
OF
THE
ACTION
PERFORMED
INSTEAD
OF
ONLY
THE
RELEVANCE
HOWEVER
AUTOMATICALLY
ASSESSING
THE
QUALITY
OF
ACTIONS
IS
NOT
AN
EASY
COM
PUTER
VISION
PROBLEM
HUMAN
EXPERTS
FOR
A
PARTICULAR
DOMAIN
SUCH
AS
COACHES
OR
DOCTORS
HAVE
TYPICALLY
BEEN
TRAINED
OVER
MANY
YEARS
TO
DEVELOP
COMPLEX
UN
DERLYING
RULES
TO
ASSESS
ACTION
QUALITY
IF
MACHINES
ARE
TO
ASSESS
ACTION
QUALITY
THEN
THEY
MUST
DISCOVER
SIMILAR
RULES
AS
WELL
IN
THIS
PAPER
WE
PROPOSE
A
DATA
DRIVEN
METHOD
TO
LEARN
HOW
TO
ASSESS
THE
QUALITY
OF
ACTIONS
IN
VIDEOS
TO
OUR
KNOWLEDGE
WE
ARE
THE
FIRST
TO
PROPOSE
A
GENERAL
FRAMEWORK
FOR
LEARNING
TO
ASSESS
THE
QUALITY
OF
HUMAN
BASED
ACTIONS
FROM
VIDEOS
OUR
METHOD
WORKS
BY
EXTRACTING
THE
SPATIO
TEMPORAL
POSE
FEATURES
OF
PEOPLE
AND
WITH
MINIMAL
ANNOTATION
ESTIMATING
A
REGRESSION
MODEL
THAT
PREDICTS
THE
SCORES
OF
ACTIONS
FIG
SHOWS
AN
EXAMPLE
OUTPUT
OF
OUR
SYSTEM
IN
ORDER
TO
QUANTIFY
THE
PERFORMANCE
OF
OUR
METHODS
WE
INTRODUCE
A
NEW
DATASET
FOR
ACTION
QUALITY
ASSESSMENT
COMPRISED
OF
OLYMPIC
SPORTS
FOOTAGE
AL
THOUGH
THE
METHODS
IN
THIS
PAPER
ARE
GENERAL
SPORTS
BROADCAST
FOOTAGE
HAS
THE
ADVANTAGE
THAT
IT
IS
FREELY
AVAILABLE
AND
COMES
ALREADY
RIGOROUSLY
ANNOTATED
BY
THE
OLYMPIC
JUDGES
WE
EVALUATE
OUR
QUALITY
ASSESSMENTS
ON
BOTH
DIVING
AND
FIGURE
SKATING
COMPETITIONS
OUR
RESULTS
ARE
PROMISING
AND
SUGGEST
THAT
OUR
METHOD
IS
SIGNIFICANTLY
BETTER
AT
RANKING
PEOPLE
ACTIONS
BY
THEIR
QUALITY
THAN
NON
EXPERT
HUMANS
HOWEVER
OUR
METHOD
IS
STILL
A
LONG
WAY
FROM
RIVALING
THE
PERFORMANCE
OF
EXPERT
JUDGES
INDICATING
THAT
THERE
IS
SIGNIFICANT
OPPORTUNITY
IN
COMPUTER
VISION
RESEARCH
TO
IMPROVE
ON
THIS
DIFFICULT
YET
IMPORTANT
TASK
MOREOVER
SINCE
OUR
METHOD
LEVERAGES
HIGH
LEVEL
POSE
FEATURES
TO
LEARN
A
MODEL
FOR
ACTION
QUALITY
WE
CAN
USE
THIS
MODEL
TO
HELP
MACHINES
UNDERSTAND
PEOPLE
IN
VIDEOS
AS
WELL
FIRSTLY
WE
CAN
PROVIDE
INTERPRETABLE
FEEDBACK
TO
PERFORMERS
ON
HOW
TO
IMPROVE
THE
QUALITY
OF
THEIR
ACTION
THE
RED
VECTORS
IN
FIG
ARE
OUTPUT
FROM
OUR
SYSTEM
THAT
INSTRUCTS
THE
OLYMPIC
DIVER
TO
STRETCH
HIS
HANDS
AND
LOWER
HIS
FEET
OUR
FEEDBACK
SYSTEM
WORKS
BY
CALCULATING
THE
GRADIENT
FOR
EACH
BODY
JOINT
AGAINST
THE
LEARNED
MODEL
THAT
WOULD
HAVE
MAXIMIZED
PEOPLE
SCORES
SEC
ONDLY
WE
CAN
CREATE
HIGHLIGHTS
OF
VIDEOS
BY
FINDING
WHICH
SEGMENTS
CONTRIBUTED
THE
MOST
TO
THE
ACTION
QUALITY
COMPLEMENTING
WORK
IN
VIDEO
SUMMARIZATION
WE
HYPOTHESIZE
THAT
FURTHER
PROGRESS
IN
BUILDING
BETTER
QUALITY
ASSESSMENT
MODELS
CAN
IMPROVE
BOTH
FEEDBACK
SYSTEMS
AND
VIDEO
HIGHLIGHTS
THE
THREE
PRINCIPAL
CONTRIBUTIONS
OF
THIS
PAPER
REVOLVE
AROUND
AUTOMATICALLY
ASSESSING
THE
QUALITY
OF
PEOPLE
ACTIONS
IN
VIDEOS
FIRSTLY
WE
INTRODUCE
A
GENERAL
LEARNING
BASED
FRAMEWORK
FOR
THE
QUALITY
ASSESSMENT
OF
HUMAN
ACTIONS
USING
SPATIOTEMPORAL
POSE
FEATURES
SECONDLY
WE
THEN
DESCRIBE
A
SYSTEM
TO
GENERATE
FEEDBACK
FOR
PERFORMERS
IN
ORDER
TO
IMPROVE
THEIR
SCORE
FINALLY
WE
RELEASE
A
NEW
DATASET
FOR
ACTION
QUALITY
ASSESSMENT
IN
THE
HOPES
OF
FACILITATING
FUTURE
RESEARCH
ON
THIS
TASK
THE
REMAINDER
OF
THIS
PAPER
DESCRIBES
THESE
CONTRIBUTIONS
IN
DETAIL
RELATED
WORK
THIS
PAPER
BUILDS
UPON
SEVERAL
AREAS
OF
COMPUTER
VISION
WE
BRIEFLY
REVIEW
RE
LATED
WORK
ACTION
ASSESSMENT
THE
PROBLEM
OF
ACTION
QUALITY
ASSESSMENT
HAS
BEEN
RELATIVELY
UNEXPLORED
IN
THE
COMPUTER
VISION
COMMUNITY
THERE
HAVE
BEEN
A
FEW
PROMISING
EFFORTS
TO
JUDGE
HOW
WELL
PEOPLE
PERFORM
ACTIONS
HOWEVER
THESE
PREVIOUS
WORKS
HAVE
SO
FAR
BEEN
HAND
CRAFTED
FOR
SPECIFIC
ACTIONS
THE
MOTIVATION
FOR
ASSESSING
PEOPLES
ACTIONS
IN
HEALTHCARE
APPLICATIONS
HAS
ALSO
BEEN
DISCUSSED
BEFORE
BUT
THE
TECHNICAL
METHOD
IS
LIMITED
TO
RECOGNIZING
ACTIONS
IN
THIS
PAPER
WE
PROPOSE
A
GENERIC
LEARNING
BASED
FRAMEWORK
WITH
STATE
OF
THE
ART
FEATURES
FOR
ACTION
QUALITY
ASSESSMENT
THAT
CAN
BE
APPLIED
TO
MOST
TYPES
OF
HUMAN
ACTIONS
TO
DEMONSTRATE
THIS
GENERALITY
WE
EVALUATE
ON
TWO
DISTINCT
TYPES
OF
ACTIONS
DIVING
AND
FIGURE
SKATING
FURTHERMORE
OUR
SYSTEM
IS
ABLE
TO
GENERATE
INTERPRETABLE
FEEDBACK
ON
HOW
PERFORMERS
CAN
IMPROVE
THEIR
ACTION
PHOTOGRAPH
ASSESSMENT
THERE
ARE
SEVERAL
WORKS
THAT
ASSESS
PHOTOGRAPHS
SUCH
AS
THEIR
QUALITY
INTERESTINGNESS
AND
AESTHETICS
IN
THIS
WORK
WE
INSTEAD
FOCUS
ON
ASSESSING
THE
QUALITY
OF
HUMAN
ACTIONS
AND
NOT
THE
QUALITY
OF
THE
VIDEO
CAPTURE
OR
ITS
ARTISTIC
ASPECTS
ACTION
RECOGNITION
THERE
IS
A
LARGE
BODY
OF
WORK
STUDYING
HOW
TO
REC
OGNIZE
ACTIONS
IN
BOTH
IMAGES
AND
VIDEOS
AND
WE
REFER
READERS
TO
EXCELLENT
SURVEYS
FOR
A
FULL
REVIEW
WHILE
THIS
PAPER
ALSO
STUDIES
ACTIONS
WE
ARE
INTERESTED
IN
ASSESSING
THEIR
QUALITY
RATHER
THAN
RECOGNIZING
THEM
FEATURES
THERE
ARE
MANY
FEATURES
FOR
ACTION
RECOGNITION
USING
SPATIOTEM
PORAL
BAG
OF
WORDS
INTEREST
POINTS
FEATURE
LEARNING
AND
HUMAN
POSE
BASED
HOWEVER
SO
FAR
THESE
FEATURES
HAVE
PRIMARILY
BEEN
SHOWN
TO
WORK
FOR
RECOGNITION
WE
FOUND
THAT
SOME
OF
THESE
FEATURES
NOTABLY
AND
WITH
MINOR
ADJUSTMENTS
CAN
BE
USED
FOR
THE
QUALITY
ASSESSMENT
OF
ACTIONS
TOO
VIDEO
SUMMARIZATION
THIS
PAPER
COMPLEMENTS
WORK
IN
VIDEO
SUMMA
RIZATION
RATHER
THAN
RELYING
ON
SALIENCY
FEATURES
OR
PRIORS
WE
INSTEAD
CAN
SUMMARIZE
VIDEOS
BY
DISCARDING
SEGMENTS
THAT
DID
NOT
IMPACT
THE
QUALITY
SCORE
OF
AN
ACTION
THEREBY
CREATING
A
HIGHLIGHTS
REEL
FOR
THE
VIDEO
ASSESSING
ACTION
QUALITY
WE
NOW
PRESENT
OUR
SYSTEM
FOR
ASSESSING
THE
QUALITY
OF
AN
ACTION
FROM
VIDEOS
ON
A
HIGH
LEVEL
OUR
MODEL
LEARNS
A
REGRESSION
MODEL
FROM
SPATIO
TEMPORAL
FEATURES
AFTER
PRESENTING
OUR
MODEL
WE
THEN
SHOW
HOW
OUR
MODEL
CAN
BE
USED
TO
PROVIDE
FEEDBACK
TO
THE
PEOPLE
IN
VIDEOS
TO
IMPROVE
THEIR
ACTIONS
WE
FINALLY
DESCRIBE
HOW
OUR
MODEL
CAN
HIGHLIGHT
SEGMENTS
OF
THE
VIDEO
THAT
CONTRIBUTE
THE
MOST
TO
THE
QUALITY
SCORE
FEATURES
TO
LEARN
A
REGRESSION
MODEL
TO
THE
ACTION
QUALITY
WE
EXTRACT
SPATIO
TEMPORAL
FEA
TURES
FROM
VIDEOS
WE
CONSIDER
TWO
SETS
OF
FEATURES
LOW
LEVEL
FEATURES
THAT
CAP
TURE
GRADIENTS
AND
VELOCITIES
DIRECTLY
FROM
PIXELS
AND
HIGH
LEVEL
FEATURES
BASED
OFF
THE
TRAJECTORY
OF
HUMAN
POSE
LOW
LEVEL
FEATURES
SINCE
THERE
HAS
BEEN
SIGNIFICANT
PROGRESS
IN
DEVELOPING
FEATURES
FOR
RECOGNIZING
ACTIONS
WE
TRIED
USING
THEM
FOR
ASSESSING
ACTIONS
TOO
WE
USE
A
HIERARCHICAL
FEATURE
THAT
OBTAINS
STATE
OF
THE
ART
PERFORMANCE
IN
ACTION
RECOGNITION
BY
LEARNING
A
FILTER
BANK
WITH
INDEPENDENT
SUBSPACE
ANALYSIS
THE
LEARNED
FILTER
BANK
CONSISTS
OF
SPATIO
TEMPORAL
GABOR
LIKE
FILTERS
THAT
CAPTURE
EDGES
AND
VELOCITIES
IN
OUR
EXPERIMENTS
WE
USE
THE
IMPLEMENTATION
BY
WITH
THE
NETWORK
PRE
TRAINED
ON
THE
DATASET
HIGH
LEVEL
POSE
FEATURES
SINCE
MOST
LOW
LEVEL
FEATURES
CAPTURE
STATISTICS
FROM
PIXELS
DIRECTLY
THEY
ARE
OFTEN
DIFFICULT
TO
INTERPRET
AS
WE
WISH
TO
PROVIDE
FEEDBACK
ON
HOW
A
PERFORMER
CAN
IMPROVE
THEIR
ACTIONS
WE
WANT
THE
FEEDBACK
TO
BE
INTERPRETABLE
INSPIRED
BY
ACTIONLETS
WE
NOW
PRESENT
HIGH
LEVEL
FEATURES
BASED
OFF
HUMAN
POSE
THAT
ARE
INTERPRETABLE
GIVEN
A
VIDEO
WE
ASSUME
THAT
WE
KNOW
THE
POSE
OF
THE
HUMAN
PERFORMER
IN
EVERY
FRAME
OBTAINED
EITHER
THROUGH
GROUND
TRUTH
OR
AUTOMATIC
POSE
ESTIMATION
LET
P
J
T
BE
THE
X
COMPONENT
OF
THE
JTH
JOINT
IN
THE
TTH
FRAME
OF
THE
VIDEO
SINCE
WE
WANT
OUR
FEATURES
TO
BE
TRANSLATION
INVARIANT
WE
NORMALIZE
THE
JOINT
POSITIONS
RELATIVE
TO
THE
HEAD
POSITION
Q
J
T
P
J
T
P
T
WHERE
WE
HAVE
ASSUMED
THAT
P
T
REFERS
TO
THE
HEAD
NOTE
THAT
Q
J
IS
A
FUNCTION
OF
TIME
SO
WE
CAN
REPRESENT
IT
IN
THE
FREQUENCY
DOMAIN
BY
THE
DISCRETE
COSINE
TRANSFORM
DCT
Q
J
AQ
J
WHERE
A
IS
THE
DISCRETE
COSINE
TRANSFORMATION
MATRIX
WE
THEN
USE
THE
K
LOWEST
FREQUENCY
COMPONENTS
TO
CREATE
THE
FEATURE
J
K
ONLY
USING
THE
LOW
FREQUENCIES
HELPS
REMOVE
HIGH
FREQUENCY
NOISE
DUE
TO
POSE
ESTIMATION
ERRORS
WE
USE
THE
ABSOLUTE
VALUE
OF
THE
FREQUENCY
COEFFICIENTS
QI
WE
COMPUTE
ΦJ
FOR
EVERY
JOINT
FOR
BOTH
THE
X
AND
Y
COMPONENTS
AND
CON
CATENATE
THEM
TO
CREATE
THE
FINAL
FEATURE
VECTOR
Φ
WE
NOTE
THAT
IF
THE
VIDEO
IS
LONG
WE
BREAK
IT
UP
INTO
SEGMENTS
AND
CONCATENATE
THE
FEATURES
TO
PRODUCE
ONE
FEATURE
VECTOR
FOR
THE
ENTIRE
VIDEO
THIS
INREASES
THE
TEMPORAL
RESOLUTION
OF
OUR
FEATURES
FOR
LONG
VIDEOS
ACTIONLETS
USES
A
SIMILAR
METHOD
WITH
DISCRETE
FOURIER
TRANSFORM
DFT
INSTEAD
ALTHOUGH
THERE
IS
A
CLOSE
RELATIONSHIP
BETWEEN
DFT
AND
DCT
WE
SEE
BETTER
RESULTS
USING
DCT
WE
BELIEVE
THIS
IS
THE
CASE
SINCE
DCT
PROVIDES
A
MORE
COMPACT
REPRESENTATION
ADDITIONALLY
DCT
COEFFICIENTS
ARE
REAL
NUMBERS
INSTEAD
OF
COMPLEX
SO
LESS
INFORMATION
IS
LOST
IN
THE
ABSOLUTE
VALUE
OPERATION
IN
ORDER
TO
ESTIMATE
THE
JOINTS
OF
THE
PERFORMER
THROUGHOUT
THE
VIDEO
P
J
T
WE
RUN
A
POSE
ESTIMATION
ALGORITHM
TO
FIND
THE
POSITION
OF
THE
JOINTS
IN
EVERY
FRAME
WE
ESTIMATE
THE
POSE
USING
A
FLEXIBLE
PARTS
MODEL
FOR
EACH
FRAME
INDEPENDENTLY
SINCE
FINDS
THE
BEST
POSE
FOR
A
SINGLE
FRAME
USING
DYNAMIC
FIG
POSE
ESTIMATION
CHALLENGES
SOME
RESULTS
FOR
HUMAN
POSE
ESTIMATION
ON
OUR
ACTION
QUALITY
DATASET
SINCE
THE
PERFORMERS
CONTORT
THEIR
BODY
IN
UNUSUAL
CONFIGURATIONS
POSE
ESTIMATION
IS
VERY
CHALLENGING
ON
OUR
DATASET
PROGRAMMING
AND
WE
WANT
THE
BEST
POSE
ACROSS
THE
ENTIRE
VIDEO
WE
FIND
THE
N
BEST
POSE
SOLUTIONS
PER
FRAME
USING
THEN
WE
ASSOCIATE
THE
POSES
USING
A
DYNAMIC
PROGRAMMING
ALGORITHM
TO
FIND
THE
BEST
TRACK
IN
THE
WHOLE
VIDEO
THE
ASSOCIATION
LOOKS
FOR
THE
SINGLE
BEST
SMOOTH
TRACK
COVERING
THE
WHOLE
TEMPORAL
SPAN
OF
THE
VIDEO
FIG
SHOWS
SOME
SUCCESSES
AND
FAILURES
OF
THIS
POSE
ESTIMATION
LEARNING
WE
THEN
POSE
QUALITY
ASSESSMENT
AS
A
SUPERVISED
REGRESSION
PROBLEM
LET
ΦI
RK
N
BE
THE
POSE
FEATURES
FOR
VIDEO
I
IN
MATRIX
FORM
WHERE
N
IS
THE
NUMBER
OF
JOINTS
AND
K
IS
THE
NUMBER
OF
LOW
FREQUENCY
COMPONENTS
WE
WRITE
YI
R
TO
DENOTE
THE
GROUND
TRUTH
QUALITY
SCORE
OF
THE
ACTION
IN
VIDEO
I
OBTAINED
BY
AN
EXPERT
HUMAN
JUDGE
WE
THEN
TRAIN
A
LINEAR
SUPPORT
VECTOR
REGRESSION
L
SVR
TO
PREDICT
YI
GIVEN
FEATURES
ΦI
OVER
A
TRAINING
SET
IN
OUR
EXPERIMENTS
WE
USE
LIBSVM
OPTIMIZATION
IS
FAST
AND
TAKES
LESS
THAN
A
SECOND
ON
TYPICAL
SIZED
PROBLEMS
WE
PERFORM
CROSS
VALIDATION
TO
ESTIMATE
HYPERPARAMETERS
DOMAIN
KNOWLEDGE
WE
NOTE
THAT
A
COMPREHENSIVE
MODEL
FOR
QUALITY
ASSESS
MENT
MIGHT
USE
DOMAIN
EXPERTS
TO
ANNOTATE
FINE
TUNED
KNOWLEDGE
ON
THE
ACTION
QUALITY
E
G
THE
LEG
MUST
BE
STRAIGHT
HOWEVER
RELYING
ON
DOMAIN
EXPERTS
IS
EXPENSIVE
AND
DIFFICULT
TO
SCALE
TO
A
LARGE
NUMBER
OF
ACTIONS
BY
POSING
QUALITY
ASSESSMENT
AS
A
MACHINE
LEARNING
PROBLEM
WITH
MINIMAL
INTERACTION
FROM
AN
EX
PERT
WE
CAN
SCALE
MORE
EFFICIENTLY
IN
OUR
SYSTEM
WE
ONLY
REQUIRE
A
SINGLE
REAL
NUMBER
PER
VIDEO
CORRESPONDING
TO
THE
SCORE
OF
THE
QUALITY
PROTOTYPICAL
EXAMPLE
MOREOVER
A
FAIRLY
SIMPLE
METHOD
TO
ASSESS
QUALITY
IS
TO
CHECK
THE
OBSERVED
VIDEO
AGAINST
A
GROUND
TRUTH
VIDEO
WITH
PERFECT
EXECUTION
AND
THEN
DETERMINE
THE
DIFFERENCE
HOWEVER
IN
PRACTICE
MANY
ACTIONS
CAN
HAVE
MULTIPLE
IDEAL
EXECUTIONS
E
G
A
PERFECT
OVERHAND
SERVE
MIGHT
BE
JUST
AS
GOOD
AS
A
PERFECT
UNDERHAND
SERVE
INSTEAD
OUR
MODEL
CAN
HANDLE
MULTI
MODAL
SCORE
DISTRIBUTIONS
FEEDBACK
PROPOSALS
AS
A
PERFORMER
EXECUTES
AN
ACTION
IN
ADDITION
TO
ASSESSING
THE
QUALITY
WE
ALSO
WISH
TO
PROVIDE
FEEDBACK
ON
HOW
THE
PERFORMER
CAN
IMPROVE
HIS
ACTION
SINCE
OUR
REGRESSION
MODEL
OPERATES
OVER
POSE
BASED
FEATURES
WE
CAN
DETERMINE
HOW
THE
PERFORMER
SHOULD
MOVE
TO
MAXIMIZE
THE
SCORE
WE
ACCOMPLISH
THIS
BY
DIFFERENTIATING
THE
SCORING
FUNCTION
WITH
RESPECT
TO
JOINT
LOCATION
WE
CALCULATE
THE
GRADIENT
OF
THE
SCORE
WITH
RESPECT
TO
THE
LOCATION
OF
EACH
JOINT
WHERE
IS
THE
SCORING
FUNCTION
BY
CALCULATING
THE
MAXIMUM
GRADIENT
WE
CAN
FIND
THE
JOINT
AND
THE
DIRECTION
THAT
THE
PERFORMER
MUST
MOVE
TO
ACHIEVE
THE
LARGEST
IMPROVEMENT
IN
THE
SCORE
WE
ARE
ABLE
TO
ANALYTICALLY
CALCULATE
THE
GRADIENT
RECALL
THAT
L
SVR
LEARNS
A
WEIGHT
VECTOR
W
RK
N
SUCH
THAT
W
PREDICTS
THE
SCORE
OF
THE
ACTION
QUALITY
BY
THE
DOT
PRODUCT
K
N
WFJΦFJ
F
J
WHERE
ΦFJ
IS
THE
F
TH
FREQUENCY
COMPONENET
FOR
THE
JTH
JOINT
AFTER
BASIC
ALGEBRA
WE
CAN
COMPUTE
THE
GRADIENT
OF
THE
SCORE
WITH
RESPECT
TO
THE
LOCATION
OF
EACH
JOINT
P
J
T
P
J
T
F
AFTWFJ
SIGN
T
TI
AFTI
P
J
TL
P
TL
BY
COMPUTING
MAXP
J
WE
CAN
FIND
THE
JOINT
AND
THE
DIRECTION
THE
PERFORMER
MUST
MOVE
TO
MOST
IMPROVE
THE
SCORE
VIDEO
HIGHLIGHTS
IN
ADDITION
TO
FINDING
THE
JOINT
THAT
WILL
RESULT
IN
THE
LARGEST
SCORE
IMPROVEMENT
WE
ALSO
WISH
TO
MEASURE
THE
IMPACT
A
SEGMENT
OF
THE
VIDEO
HAS
ON
THE
QUALITY
SCORE
SUCH
A
MEASURE
COULD
BE
USEFUL
IN
SUMMARIZING
THE
SEGMENTS
OF
ACTIONS
THAT
CONTRIBUTE
TO
HIGH
OR
LOW
SCORES
WE
DEFINE
A
SEGMENT
IMPACT
AS
HOW
MUCH
THE
QUALITY
SCORE
WOULD
CHANGE
IF
THE
SEGMENT
WERE
REMOVED
IN
ORDER
TO
REMOVE
A
SEGMENT
WE
COMPUTE
THE
MOST
LIKELY
FEATURE
VECTOR
HAD
WE
NOT
OBSERVED
THE
MISSING
SEGMENT
THE
KEY
OBSERVATION
IS
THAT
SINCE
WE
ONLY
USE
THE
LOW
FREQUENCY
COMPONENTS
IN
OUR
FEA
TURE
VECTOR
THERE
ARE
MORE
EQUATIONS
THAN
UNKNOWNS
WHEN
ESTIMATING
THE
DCT
COEFFICIENTS
CONSEQUENTLY
REMOVING
A
SEGMENT
CORRESPONDS
TO
SIMPLY
REMOVING
SOME
EQUATIONS
LET
B
A
BE
THE
INVERSE
COSINE
TRANSFORM
WHERE
A
IS
THE
PSUEDO
INVERSE
OF
A
THEN
THE
DCT
EQUATION
CAN
BE
WRITTEN
AS
Q
J
B
Q
J
IF
THE
DATA
FROM
WE
DO
NOT
DIFFERENTIATE
WITH
RESPECT
TO
THE
HEAD
LOCATION
BECAUSE
IT
IS
USED
FOR
NORMALIZATION
REMOVED
SEGMENT
DISPLACEMENT
TIME
FIG
INTERPOLATING
SEGMENTS
THIS
SCHEMATIC
SHOWS
HOW
THE
DISPLACEMENT
VECTOR
CHANGES
WHEN
A
SEGMENT
OF
THE
VIDEO
IS
REMOVED
IN
ORDER
TO
COMPUTE
IMPACT
THE
DASHED
CURVE
IS
THE
ORIGINAL
DISPLACEMENT
AND
THE
SOLID
CURVE
IS
THE
MOST
LIKELY
DISPLACEMENT
GIVEN
OBSERVATIONS
WITH
A
MISSING
SEGMENT
FRAMES
U
THROUGH
V
IS
MISSING
THEN
THE
INFERRED
DCT
COEFFICIENTS
ARE
Qˆ
J
BU
V
Q
J
WHERE
BU
V
IS
THE
SUB
MATRIX
OF
B
THAT
EXCLUDES
ROWS
U
THROUGH
V
THE
FREQUENCY
COMPONENTS
Qˆ
J
ARE
THE
SAME
DIMENSIONALITY
AS
Q
J
BUT
THEY
HAVE
INFERRED
THE
MISSING
SEGMENT
WITH
THE
MOST
LIKELY
JOINT
TRAJECTORY
FIG
VISUALIZES
HOW
THE
FEATURES
CHANGE
WITH
THIS
TRANSFORMATION
WE
USE
Qˆ
J
TO
CREATE
THE
FEATURE
VECTOR
FOR
THE
VIDEO
WITH
THE
MISSING
SEG
MENT
FINALLY
WE
DETERMINE
THE
IMPACT
OF
THE
MISSING
SEGMENT
BY
CALCULATING
THE
DIFFERENCE
IN
SCORES
BETWEEN
THE
ORIGINAL
FEATURE
VECTOR
AND
THE
FEATURE
VECTOR
WITH
THE
MISSING
SEGMENT
EXPERIMENTS
IN
THIS
SECTION
WE
EVALUATE
BOTH
OUR
QUALITY
ASSESSMENT
METHOD
AND
FEEDBACK
SYSTEM
FOR
QUALITY
IMPROVEMENT
WITH
QUANTITATIVE
EXPERIMENTS
SINCE
QUALITY
AS
SESSMENT
HAS
NOT
YET
BEEN
EXTENSIVELY
STUDIED
IN
THE
COMPUTER
VISION
COMMUNITY
WE
FIRST
INTRODUCE
A
NEW
VIDEO
DATASET
FOR
ACTION
QUALITY
ASSESSMENT
ACTION
QUALITY
DATASET
THERE
ARE
TWO
PRIMARY
HURDLES
IN
BUILDING
A
LARGE
DATASET
FOR
ACTION
QUALITY
ASSESSMENT
FIRSTLY
THE
SCORE
ANNOTATIONS
ARE
SUBJECTIVE
AND
REQUIRE
AN
EXPERT
UNFORTUNATELY
HIRING
AN
EXPERT
TO
ANNOTATE
HUNDREDS
OF
VIDEOS
IS
EXPENSIVE
SECONDLY
IN
SOME
APPLICATIONS
SUCH
AS
HEALTH
CARE
THERE
ARE
PRIVACY
AND
LEGAL
ISSUES
INVOLVED
IN
COLLECTING
VIDEOS
FROM
PATIENTS
IN
ORDER
TO
ESTABLISH
A
BASELINE
DATASET
FOR
FURTHER
RESEARCH
WE
DESIRE
FREELY
AVAILABLE
VIDEOS
WE
INTRODUCE
AN
OLYMPICS
VIDEO
DATASET
FOR
ACTION
QUALITY
ASSESSMENT
SPORTS
FOOTAGE
HAS
THE
ADVANTAGE
THAT
IT
CAN
BE
OBTAINED
FREELY
AND
THE
EXPERT
JUDGE
SCORES
ARE
FREQUENTLY
RELEASED
PUBLICLY
WE
COLLECTED
VIDEOS
FROM
YOUTUBE
FOR
TWO
CATEGORIES
OF
SPORTS
DIVING
AND
FIGURE
SKATING
FROM
RECENT
OLYMPICS
AND
OTHER
WORLDWIDE
CHAMPIONSHIPS
THE
VIDEOS
ARE
LONG
WITH
MULTIPLE
INSTANCES
OF
ACTIONS
PERFORMED
BY
MULTIPLE
PEOPLE
WE
ANNOTATED
THE
VIDEOS
WITH
THE
START
AND
END
FRAME
FOR
EACH
INSTANCE
AND
WE
EXTRACTED
THE
JUDGE
SCORE
THE
DATASET
WILL
BE
PUBLICLY
AVAILABLE
FIG
DIVING
DATASET
SOME
OF
THE
BEST
DIVES
FROM
OUR
DIVING
DATASET
EACH
COLUMN
CORRESPONDS
TO
ONE
VIDEO
THERE
IS
A
LARGE
VARIATION
IN
THE
TOP
SCORING
ACTIONS
HENCE
PROVIDING
FEEDBACK
IS
NOT
AS
EASY
AS
PUSHING
THE
ACTION
TOWARDS
A
CANONICAL
GOOD
PERFORMANCE
FIG
FIGURE
SKATING
DATASET
SAMPLE
FRAMES
FROM
OUR
FIGURE
SKATING
DATASET
NOTICE
THE
LARGE
VARIATIONS
OF
ROUTINES
THAT
THE
PERFORMERS
ATTEMPT
THIS
MAKES
AUTOMATIC
POSE
ESTIMATION
CHALLENGING
DIVING
FIG
SHOWS
A
FEW
EXAMPLES
OF
OUR
DIVING
DATASET
OUR
DIVING
DATASET
CONSISTS
OF
VIDEOS
THE
VIDEOS
ARE
SLOW
MOTION
FROM
TELEVISION
BROADCASTING
CHANNELS
SO
THE
EFFECTIVE
FRAME
RATE
IS
FRAMES
PER
SECOND
EACH
VIDEO
IS
ABOUT
FRAMES
AND
THE
ENTIRE
DATASET
CONSISTS
OF
FRAMES
THE
GROUND
TRUTH
JUDGE
SCORES
VARIES
BETWEEN
WORST
AND
BEST
IN
OUR
EXPERIMENTS
WE
USE
INSTANCES
FOR
TRAINING
AND
THE
REST
FOR
TESTING
WE
REPEATED
EVERY
EXPERIMENT
TIMES
WITH
DIFFERENT
RANDOM
SPLITS
AND
AVERAGED
THE
RESULTS
IN
ADDITION
TO
THE
OLYMPIC
JUDGE
SCORE
WE
ALSO
CONSULTED
WITH
THE
MIT
VARSITY
DIVING
COACH
WHO
ANNOTATED
WHICH
JOINTS
A
DIVER
SHOULD
ADJUST
TO
IMPROVE
EACH
DIVE
WE
USE
THIS
DATA
TO
EVALUATE
OUR
FEEDBACK
SYSTEM
FOR
THE
QUALITY
IMPROVEMENT
ALGORITHM
FIGURE
SKATING
FIG
SHOWS
SOME
FRAMES
FROM
OUR
FIGURE
SKATING
DATASET
THIS
DATASET
CONTAINS
VIDEOS
CAPTURED
AT
FRAMES
PER
SECOND
EACH
VIDEO
IS
ALMOST
FRAMES
AND
THE
ENTIRE
DATASET
IS
FRAMES
THE
JUDGE
SCORE
RANGES
BETWEEN
WORST
AND
BEST
WE
USE
INSTANCES
FOR
TRAINING
AND
THE
REST
FOR
TESTING
AS
BEFORE
WE
REPEATED
EVERY
EXPERIMENT
TIMES
WITH
DIFFERENT
RANDOM
SPLITS
AND
AVERAGED
THE
RESULTS
WE
NOTE
THAT
OUR
FIGURE
SKATING
TENDS
TO
BE
MORE
CHALLENGING
FOR
POSE
ESTIMATION
SINCE
IT
IS
AT
A
LOWER
FRAME
RATE
AND
HAS
MORE
VARIATION
IN
THE
HUMAN
POSE
AND
CLOTHING
E
G
WEARING
SKIRT
TABLE
DIVING
EVALUATION
WE
SHOW
MEAN
RANK
CORRELATION
ON
OUR
DIVING
DATASET
HIGHER
IS
BETTER
THE
POSE
BASED
FEATURES
PROVIDE
THE
BEST
PERFORMANCE
TABLE
FIGURE
SKATING
EVALUATION
WE
CALCULATE
MEAN
RANK
CORRELATION
ON
OUR
FIGURE
SKATING
DATASET
HIGHER
IS
BETTER
THE
HIERARCHICAL
NETWORK
FEATURES
PROVIDE
THE
BEST
RESULTS
ALTHOUGH
POSE
BASED
FEATURES
ARE
NOT
SUPERIOR
THEY
STILL
ENABLE
HIGH
LEVEL
ANALYSIS
BY
PROVIDING
FEEDBACK
FOR
QUALITY
IMPROVEMENT
WE
BELIEVE
POSE
BASED
FEATURES
CAN
BENEFIT
FROM
USING
A
BETTER
POSE
ESTIMATION
QUALITY
ASSESSMENT
WE
EVALUATE
OUR
QUALITY
ASSESSMENT
ON
BOTH
THE
FIGURE
SKATING
AND
DIVING
DATASET
IN
ORDER
TO
COMPARE
OUR
RESULTS
AGAINST
THE
GROUND
TRUTH
WE
USE
THE
RANK
COR
RELATION
OF
THE
SCORES
WE
PREDICT
AGAINST
THE
SCORES
THE
OLYMPIC
JUDGES
AWARDED
TAB
AND
TAB
SHOW
THE
MEAN
PERFORMANCE
OVER
RANDOM
TRAIN
TEST
SPLITS
OF
OUR
DATASETS
OUR
RESULTS
SUGGEST
THAT
POSE
BASED
FEATURES
ARE
COMPETITIVE
AND
EVEN
OBTAIN
THE
BEST
PERFORMANCE
ON
THE
DIVING
DATASET
IN
ADDITION
OUR
RESULTS
INDICATE
THAT
FEATURES
LEARNED
TO
RECOGNIZE
ACTIONS
CAN
BE
USED
TO
ASSESS
THE
QUAL
ITY
OF
ACTIONS
TOO
WE
SHOW
SOME
OF
THE
BEST
AND
WORST
VIDEOS
AS
PREDICTED
BY
OUR
MODEL
IN
FIG
WE
COMPARE
OUR
QUALITY
ASSESSMENT
AGAINST
SEVERAL
BASELINES
FIRSTLY
WE
COM
PARE
TO
BOTH
SPACE
TIME
INTEREST
POINTS
STIP
AND
POSE
BASED
FEATURES
WITH
DIS
CRETE
FOURIER
TRANSFORM
DFT
INSTEAD
OF
DCT
SIMILAR
TO
BOTH
OF
THESE
FEATURES
PERFORMED
WORSE
SECONDLY
WE
ALSO
COMPARE
TO
RIDGE
REGRESSION
WITH
ALL
FEATURE
SETS
OUR
RESULTS
SHOW
THAT
SUPPORT
VECTOR
REGRESSION
OFTEN
OBTAINS
SIGNIFICANTLY
BETTER
PERFORMANCE
WE
ALSO
ASKED
NON
EXPERT
HUMAN
ANNOTATORS
TO
PREDICT
THE
QUALITY
OF
EACH
DIVER
IN
THE
DIVING
DATASET
INTERESTINGLY
AFTER
WE
INSTRUCTED
THE
SUBJECTS
TO
READ
THE
WIKIPEDIA
PAGE
ON
DIVING
NON
EXPERT
ANNOTATORS
WERE
ONLY
ABLE
TO
ACHIEVE
A
RANK
CORRELATION
OF
WHICH
IS
HALF
THE
PERFORMANCE
OF
SUPPORT
VECTOR
REGRESSION
WITH
POSE
FEATURES
WE
BELIEVE
THIS
DIFFERENCE
IS
EVIDENCE
THAT
OUR
ALGORITHM
IS
STARTING
TO
LEARN
WHICH
HUMAN
POSES
CONSTITUTE
GOOD
DIVES
WE
NOTE
HOWEVER
THAT
OUR
METHOD
IS
FAR
FROM
MATCHING
OLYMPIC
JUDGES
SINCE
THEY
ARE
ABLE
TO
PREDICT
THE
MEDIAN
JUDGE
SCORE
WITH
A
RANK
CORRELATION
OF
SUGGESTING
THAT
THERE
IS
STILL
SIGNIFICANT
ROOM
FOR
IMPROVEMENT
OLYMPIC
DIVING
COMPETITIONS
HAVE
TWO
SCORES
THE
TECHNICAL
DIFFICULTY
AND
THE
SCORE
THE
FINAL
QUALITY
OF
THE
ACTION
IS
THEN
THE
PRODUCT
OF
THESE
TWO
QUANTITIES
JUDGES
ARE
FIG
EXAMPLES
OF
DIVING
SCORES
WE
SHOW
THE
TWO
BEST
AND
WORST
VIDEOS
SORTED
BY
THE
PREDICTED
SCORE
EACH
COLUMN
IS
ONE
VIDEO
WITH
GROUND
TRUTH
AND
PREDICTED
SCORE
WRITTEN
BELOW
NOTICE
THAT
IN
THE
LAST
PLACE
VIDEO
THE
DIVER
LACKED
STRAIGHT
LEGS
IN
THE
BEGINNING
AND
DID
NOT
HAVE
A
TIGHT
FOLDING
POSE
THESE
TWO
PITFALLS
ARE
PART
OF
COMMON
DIVING
ADVICE
GIVEN
BY
COACHES
AND
OUR
MODEL
HAS
LEARNED
THIS
INDEPENDENTLY
LIMITATIONS
WHILE
OUR
SYSTEM
IS
ABLE
TO
PREDICT
THE
QUALITY
OF
ACTIONS
WITH
SOME
SUCCESS
IT
HAS
MANY
LIMITATIONS
ONE
OF
THE
MAJOR
BOTTLENECKS
IS
THE
POSE
ESTIMATION
FIG
SHOWS
A
FEW
EXAMPLES
OF
THE
SUCCESSES
AND
FAILURES
OF
THE
POSE
ESTIMATION
POSE
ESTIMATION
IN
OUR
DATASETS
IS
VERY
CHALLENGING
SINCE
THE
PERFORMERS
CONTORT
THEIR
BODY
IN
MANY
UNUSUAL
CONFIGURATIONS
WITH
SIGNIFICANT
VARIATION
IN
APPEARANCE
THE
FREQUENT
OCCLUSION
BY
CLOTHING
FOR
FIGURE
SKATING
NOTICEABLY
HARMS
THE
POSE
ESTIMATION
PERFORMANCE
WHEN
THE
POSE
ESTIMATION
IS
POOR
THE
QUALITY
SCORE
IS
STRONGLY
AFFECTED
SUGGESTING
THAT
ADVANCES
IN
POSE
ESTIMATION
OR
USING
DEPTH
SENSORS
FOR
POSE
CAN
IMPROVE
OUR
SYSTEM
FUTURE
WORK
IN
ACTION
QUALITY
CAN
BE
MADE
ROBUST
AGAINST
THESE
TYPES
OF
FAILURES
AS
WELL
BY
ACCOUNTING
FOR
THE
UNCERTAINTY
IN
THE
POSE
ESTIMATION
OUR
SYSTEM
IS
DESIGNED
TO
WORK
FOR
ONE
HUMAN
PERFORMER
ONLY
AND
DOES
NOT
MODEL
COORDINATION
BETWEEN
MULTIPLE
PEOPLE
WHICH
IS
OFTEN
IMPORTANT
FOR
MANY
TYPES
OF
SPORTS
AND
ACTIVITIES
WE
BELIEVE
THAT
FUTURE
WORK
IN
EXPLICITLY
MODELING
TEAM
ACTIVITIES
AND
INTERACTIONS
CAN
SIGNIFICANTLY
ADVANCE
ACTION
QUALITY
ASSESSMENT
MOREOVER
WE
DO
NOT
MODEL
OBJECTS
USED
DURING
ACTIONS
SUCH
AS
SPORTS
BALLS
OR
TOOLS
AND
WE
DO
NOT
CONSIDER
PHYSICAL
OUTCOMES
SUCH
AS
SPLASHES
TOLD
THE
TECHNICAL
DIFFICULTY
APRIORI
WHICH
GIVES
THEM
A
SLIGHT
COMPETITIVE
EDGE
OVER
OUR
ALGORITHMS
WE
DID
NOT
MODEL
THE
TECHNICAL
DIFFICULTY
IN
THE
INTEREST
OF
BUILDING
A
GENERAL
SYSTEM
FIG
DIVING
FEEDBACK
PROPOSALS
WE
SHOW
FEEDBACK
FOR
SOME
OF
THE
DIVERS
THE
RED
VECTORS
ARE
INSTRUCTING
THE
DIVERS
TO
MOVE
THEIR
BODY
IN
THE
DIRECTION
OF
THE
ARROW
IN
GENERAL
THE
FEEDBACK
INSTRUCTS
DIVERS
TO
TUCK
THEIR
LEGS
MORE
AND
STRAIGHTEN
THEIR
BODY
BEFORE
ENTERING
THE
POOL
IN
DIVING
WHICH
MAY
BE
IMPORTANT
FEATURES
FOR
SOME
ACTIVITIES
FINALLY
WHILE
OUR
REPRESENTATION
CAPTURES
THE
MOVEMENTS
OF
HUMAN
JOINT
LOCATIONS
WE
DO
NOT
EXPLICITLY
MODEL
THEIR
SYNCHRONIZATION
E
G
KEEPING
LEGS
TOGETHER
OR
REPETITIONS
E
G
WAVING
HANDS
BACK
AND
FORTH
WE
SUSPECT
A
STRONGER
QUALITY
ASSESSMENT
MODEL
WILL
FACTOR
IN
THESE
VISUAL
ELEMENTS
FEEDBACK
FOR
IMPROVEMENT
IN
ADDITION
TO
QUALITY
ASSESSMENT
WE
EVALUATE
THE
FEEDBACK
VECTORS
THAT
OUR
METHOD
PROVIDES
FIG
AND
FIG
SHOW
QUALITATIVELY
A
SAMPLE
OF
THE
FEEDBACK
THAT
OUR
ALGORITHM
SUGGESTS
IN
GENERAL
THE
FEEDBACK
IS
REASONABLE
OFTEN
MAKING
MODIFICATIONS
TO
THE
EXTREMITIES
OF
THE
PERFORMER
IN
ORDER
TO
QUANTITATIVELY
EVALUATE
OUR
FEEDBACK
METHOD
WE
NEEDED
TO
AC
QUIRE
GROUND
TRUTH
ANNOTATIONS
WE
CONSULTED
WITH
THE
MIT
DIVING
TEAM
COACH
WHO
WATCHED
A
SUBSET
OF
THE
VIDEOS
IN
OUR
DATASET
IN
TOTAL
AND
PROVIDED
SUG
GESTIONS
ON
HOW
TO
IMPROVE
THE
DIVE
THE
DIVING
COACH
GAVE
US
SPECIFIC
FEEDBACK
SUCH
AS
MOVE
LEFT
FOOT
DOWN
AS
WELL
AS
HIGH
LEVEL
FEEDBACK
E
G
LEGS
SHOULD
BE
STRAIGHT
HERE
OR
TUCK
ARMS
MORE
WE
TRANSLATED
EACH
FEEDBACK
FROM
THE
COACH
INTO
ONE
OF
THREE
CLASSES
REFERRING
TO
WHETHER
THE
DIVER
SHOULD
ADJUST
HIS
UPPER
BODY
HIS
LOWER
BODY
OR
MAINTAIN
THE
SAME
POSE
ON
EACH
FRAME
DUE
TO
THE
SUBJECTIVE
NATURE
OF
THE
TASK
THE
DIVING
COACH
WAS
NOT
ABLE
TO
PROVIDE
MORE
DETAILED
FEEDBACK
ANNOTATIONS
HENCE
THE
FEEDBACK
IS
COARSELY
MAPPED
INTO
THESE
THREE
CLASSES
WE
THEN
EVALUATE
OUR
FEEDBACK
AS
A
DETECTION
PROBLEM
WE
CONSIDER
A
FEED
BACK
PROPOSAL
FROM
OUR
ALGORITHM
AS
CORRECT
IF
IT
SUGGESTS
TO
MOVE
A
BODY
PART
WITHIN
A
ONE
SECOND
RANGE
OF
THE
COACH
MAKING
THE
SAME
SUGGESTION
WE
USE
THE
MAGNITUDE
OF
THE
FEEDBACK
GRADIENT
AS
THE
IMPORTANCE
OF
THE
FEEDBACK
PROPOSAL
FIG
FIGURE
SKATING
FEEDBACK
PROPOSALS
WE
SHOW
FEEDBACK
FOR
SOME
OF
THE
FIGURE
SKATERS
WHERE
THE
RED
VECTORS
ARE
INSTRUCTIONS
FOR
THE
FIGURE
SKATERS
FIG
FEEDBACK
LIMITATIONS
THE
FEEDBACK
WE
GENERATE
IS
NOT
PERFECT
IF
THE
FIGURE
SKATER
OR
DIVER
WERE
TO
RELY
COMPLETELY
ON
THE
FEEDBACK
ABOVE
THEY
MAY
FALL
OVER
OUR
MODEL
DOES
NOT
FACTOR
IN
PHYSICAL
LAWS
MOTIVATING
WORK
IN
SUPPORT
INFERENCE
WE
USE
A
LEAVE
ONE
OUT
APPROACH
WHERE
WE
PREDICT
FEEDBACK
ON
A
VIDEO
HELD
OUT
FROM
TRAINING
OUR
FEEDBACK
PROPOSALS
OBTAIN
AP
OVERALL
FOR
DIVING
COMPARED
TO
AP
CHANCE
LEVEL
WE
COMPUTE
CHANCE
BY
RANDOMLY
GENERATING
FEEDBACK
THAT
UNIFORMLY
CHOOSES
BETWEEN
THE
UPPER
BODY
AND
LOWER
BODY
SINCE
OUR
ACTION
QUALITY
ASSESSMENT
MODEL
IS
NOT
AWARE
OF
PHYSICAL
LAWS
THE
FEEDBACK
SUGGESTIONS
CAN
BE
PHYSICALLY
IMPLAUSIBLE
FIG
SHOWS
A
FEW
CASES
WHERE
IF
THE
PERFORMER
LISTENED
TO
OUR
FEEDBACK
THEY
MIGHT
FALL
OVER
OUR
METHOD
LACK
OF
PHYSICAL
MODELS
MOTIVATES
WORK
IN
SUPPORT
INFERENCE
INTERESTINGLY
BY
AVERAGING
THE
FEEDBACK
ACROSS
ALL
DIVERS
IN
OUR
DATASET
WE
CAN
FIND
THE
MOST
COMMON
FEEDBACK
PRODUCED
BY
OUR
MODEL
FIG
SHOWS
THE
MAGNITUDE
OF
FEEDBACK
FOR
EACH
FRAME
AND
EACH
JOINT
AVERAGED
OVER
ALL
DIVERS
FOR
VISUALIZATION
PROPOSES
WE
WARP
ALL
VIDEOS
TO
HAVE
THE
SAME
LENGTH
MOST
OF
THE
FEEDBACK
SUGGESTS
CORRECTING
THE
FEET
AND
HANDS
AND
THE
MOST
IMPORTANT
FRAMES
TURN
OUT
TO
BE
THE
INITIAL
JUMP
OFF
THE
DIVING
BOARD
THE
ZENITH
OF
THE
DIVE
AND
THE
MOMENT
RIGHT
BEFORE
THE
DIVER
ENTERS
THE
WATER
FRAME
MARGINALS
NECK
L
SHOULDER
R
ARM
K
ARM
RU
TORSO
R
SHOULDER
LU
TORSO
R
ELBOW
RD
TORSO
L
ELBOW
LD
TORSO
RU
LEG
LU
LEG
R
FOREARM
L
FOREARM
R
HIP
L
HIP
R
KNEE
L
KNEE
LD
LEG
R
HAND
L
HAND
RD
LEG
L
FOOT
R
FOOT
START
ZENITH
OF
DIVE
FRAME
PREPARING
FOR
WATER
ENTRY
WATER
ENTRY
END
FIG
VISUALIZING
COMMON
FEEDBACK
WE
VISUALIZE
THE
AVERAGE
FEEDBACK
MAGNITUDE
ACROSS
THE
ENTIRE
DIVING
DATASET
FOR
EACH
JOINT
AND
FRAME
RED
MEANS
HIGH
FEEDBACK
AND
BLUE
MEANS
LOW
FEEDBACK
THE
TOP
AND
RIGHT
EDGES
SHOW
MARGINALS
OVER
FRAMES
AND
JOINTS
RESPECTIVELY
R
AND
L
STAND
FOR
RIGHT
AND
LEFT
RESPECTIVELY
AND
U
AND
D
STAND
FOR
UPPER
AND
LOWER
BODY
RESPECTIVELY
FEET
ARE
THE
MOST
COMMON
AREA
FOR
FEEDBACK
ON
OLYMPIC
DIVERS
AND
THAT
THE
BEGINNING
AND
END
OF
THE
DIVE
ARE
THE
MOST
IMPORTANT
TIME
POINTS
HIGHLIGHTING
IMPACT
WE
QUALITATIVELY
ANALYZE
THE
VIDEO
HIGHLIGHTS
PRODUCED
BY
FINDING
THE
SEGMENTS
THAT
CONTRIBUTED
THE
MOST
TO
THE
FINAL
QUALITY
SCORE
WE
BELIEVE
THAT
THIS
MEASURE
CAN
BE
USEFUL
FOR
VIDEO
SUMMARIZATION
SINCE
IT
REVEALS
OUT
OF
A
LONG
VIDEO
WHICH
CLIPS
ARE
THE
MOST
IMPORTANT
FOR
THE
ACTION
QUALITY
WE
COMPUTED
IMPACT
ON
A
ROUTINE
FROM
THE
FIGURE
SKATING
DATASET
IN
FIG
NOTICE
WHEN
THE
IMPACT
IS
NEAR
ZERO
THE
FIGURE
SKATER
IS
IN
A
STANDARD
UP
RIGHT
POSITION
OR
IN
BETWEEN
MANEUVERS
THE
POINTS
OF
MAXIMUM
IMPACT
CORRESPOND
TO
JUMPS
AND
TWISTS
OF
THE
FIGURE
SKATER
WHICH
CONTRIBUTES
POSITIVELY
TO
THE
SCORE
IF
THE
SKATER
PERFORMS
IT
CORRECTLY
AND
NEGATIVELY
OTHERWISE
DISCUSSION
IF
QUALITY
ASSESSMENT
IS
A
SUBJECTIVE
TASK
IS
IT
REASONABLE
FOR
A
MACHINE
TO
STILL
OBTAIN
REASONABLE
RESULTS
REMARKABLY
THE
INDEPENDENT
OLYMPIC
JUDGES
AGREE
WITH
EACH
OTHER
OF
THE
TIME
WHICH
SUGGESTS
THAT
THERE
IS
SOME
UNDERLYING
STRUCTURE
IN
THE
DATA
ONE
HYPOTHESIS
TO
EXPLAIN
THIS
CORRELATION
IS
THAT
THE
JUDGES
ARE
FOLLOWING
A
COMPLEX
SYSTEM
OF
RULES
TO
GAUGE
THE
SCORE
IF
SO
THEN
THE
JOB
OF
A
MACHINE
QUALITY
ASSESSMENT
SYSTEM
IS
TO
EXTRACT
THESE
RULES
WHILE
THE
APPROACH
IN
THIS
PAPER
ATTEMPTS
TO
LEARN
THESE
RULES
WE
ARE
STILL
A
LONG
WAY
FROM
HIGH
PERFORMANCE
ON
THIS
TASK
CONCLUSIONS
ASSESSING
THE
QUALITY
OF
ACTIONS
IS
AN
IMPORTANT
PROBLEM
WITH
MANY
REAL
WORLD
APPLICATIONS
IN
HEALTH
CARE
SPORTS
AND
SEARCH
TO
ENABLE
THESE
APPLICATIONS
WE
HAVE
INTRODUCED
A
GENERAL
LEARNING
BASED
FRAMEWORK
TO
AUTOMATICALLY
ASSESS
AN
A
B
FIG
VIDEO
HIGHLIGHTS
BY
CALCULATING
THE
IMPACT
EACH
FRAME
HAS
ON
THE
SCORE
OF
THE
VIDEO
WE
CAN
SUMMARIZE
LONG
VIDEOS
WITH
THE
SEGMENTS
THAT
HAVE
THE
LARGEST
IMPACT
ON
THE
QUALITY
SCORE
NOTICE
HOW
ABOVE
WHEN
THE
IMPACT
IS
CLOSE
TO
ZERO
THE
SKATER
IS
USUALLY
IN
AN
UPRIGHT
STANDARD
POSITION
AND
WHEN
THE
IMPACT
IS
LARGE
THE
SKATER
IS
PERFORMING
A
MANEUVER
ACTION
QUALITY
FROM
VIDEOS
AS
WELL
AS
TO
PROVIDE
FEEDBACK
FOR
HOW
THE
PERFORMER
CAN
IMPROVE
WE
EVALUATED
OUR
SYSTEM
ON
A
DATASET
OF
OLYMPIC
DIVERS
AND
FIGURE
SKATERS
AND
WE
SHOW
THAT
OUR
APPROACH
IS
SIGNIFICANTLY
BETTER
AT
ASSESSING
AN
ACTION
QUALITY
THAN
A
NON
EXPERT
HUMAN
ALTHOUGH
THE
QUALITY
OF
AN
ACTION
IS
A
SUBJECTIVE
MEASURE
THE
INDEPENDENT
OLYMPIC
JUDGES
HAVE
A
LARGE
CORRELATION
THIS
IMPLIES
THAT
THERE
IS
A
WELL
DEFINED
UNDERLYING
RULE
THAT
A
COMPUTER
VISION
SYSTEM
SHOULD
BE
ABLE
TO
LEARN
FROM
DATA
OUR
HOPE
IS
THAT
THIS
PAPER
WILL
MOTIVATE
MORE
WORK
IN
THIS
RELATIVELY
UNEXPLORED
AREA
TO
APPEAR
PROCEEDINGS
OF
THE
INTERNATIONAL
CONFERENCE
ON
COMPUTER
VISION
ICCV
ANNOTATOR
RATIONALES
FOR
VISUAL
RECOGNITION
JEFF
DONAHUE
AND
KRISTEN
GRAUMAN
DEPT
OF
COMPUTER
SCIENCE
UNIVERSITY
OF
TEXAS
AT
AUSTIN
JDD
GRAUMAN
CS
UTEXAS
EDU
ABSTRACT
TRADITIONAL
SUPERVISED
VISUAL
LEARNING
SIMPLY
ASKS
ANNO
TATORS
WHAT
LABEL
AN
IMAGE
SHOULD
HAVE
WE
PROPOSE
AN
APPROACH
FOR
IMAGE
CLASSIFICATION
PROBLEMS
REQUIRING
SUB
JECTIVE
JUDGMENT
THAT
ALSO
ASKS
WHY
AND
USES
THAT
INFOR
MATION
TO
ENRICH
THE
LEARNED
MODEL
WE
DEVELOP
TWO
FORMS
OF
VISUAL
ANNOTATOR
RATIONALES
IN
THE
FIRST
THE
ANNOTATOR
HIGHLIGHTS
THE
SPATIAL
REGION
OF
INTEREST
HE
FOUND
MOST
INFLU
ENTIAL
TO
THE
LABEL
SELECTED
AND
IN
THE
SECOND
HE
COMMENTS
ON
THE
VISUAL
ATTRIBUTES
THAT
WERE
MOST
IMPORTANT
FOR
EITHER
CASE
WE
SHOW
HOW
TO
MAP
THE
RESPONSE
TO
SYNTHETIC
CONTRAST
EXAMPLES
AND
THEN
EXPLOIT
AN
EXISTING
LARGE
MARGIN
LEARN
ING
TECHNIQUE
TO
REFINE
THE
DECISION
BOUNDARY
ACCORDINGLY
RESULTS
ON
MULTIPLE
SCENE
CATEGORIZATION
AND
HUMAN
ATTRAC
TIVENESS
TASKS
SHOW
THE
PROMISE
OF
OUR
APPROACH
WHICH
CAN
MORE
ACCURATELY
LEARN
COMPLEX
CATEGORIES
WITH
THE
EXPLANA
TIONS
BEHIND
THE
LABEL
CHOICES
INTRODUCTION
IMAGE
CLASSIFICATION
IS
AN
IMPORTANT
CHALLENGE
IN
COM
PUTER
VISION
AND
HAS
A
VARIETY
OF
APPLICATIONS
SUCH
AS
AUTOMATING
CONTENT
BASED
RETRIEVAL
ANALYZING
MEDICAL
IM
AGERY
OR
RECOGNIZING
LOCATIONS
IN
PHOTOS
MUCH
PROGRESS
OVER
THE
LAST
DECADE
SHOWS
THAT
SUPERVISED
LEARNING
ALGO
RITHMS
COUPLED
WITH
EFFECTIVE
IMAGE
DESCRIPTORS
CAN
YIELD
VERY
GOOD
SCENE
OBJECT
AND
ATTRIBUTE
PREDICTIONS
E
G
THE
STANDARD
TRAINING
PROCESS
ENTAILS
GATHERING
CATEGORY
LABELED
IMAGE
EXEMPLARS
ESSENTIALLY
ASKING
HU
MAN
ANNOTATORS
TO
SAY
WHAT
IS
PRESENT
AND
POSSIBLY
WHERE
IN
THE
IMAGE
IT
IS
IN
THIS
RESPECT
CURRENT
AP
PROACHES
GIVE
A
RATHER
RESTRICTED
CHANNEL
OF
INPUT
TO
THE
HUMAN
VIEWER
WHO
UNDOUBTEDLY
HAS
A
MUCH
RICHER
UNDER
STANDING
THAN
A
SIMPLE
LABEL
CAN
CONVEY
THUS
OUR
GOAL
IS
TO
CAPTURE
DEEPER
CUES
FROM
ANNOTATORS
WE
ARE
PARTICULARLY
INTERESTED
IN
COMPLEX
VISUAL
RECOGNI
TION
PROBLEMS
THAT
REQUIRE
SUBJECTIVE
JUDGMENT
E
G
SAYING
WHETHER
A
FACE
IS
ATTRACTIVE
RATING
AN
ATHLETIC
PERFORMANCE
OR
ELSE
LACK
CLEAR
CUT
SEMANTIC
BOUNDARIES
E
G
DESCRIBING
A
SCENE
CATEGORY
CATEGORIZING
BY
APPROXIMATE
AGE
SEE
IS
THIS
ANCHOR
DOING
A
IS
THIS
SCENE
FROM
A
IS
THIS
WOMAN
SERIOUS
OR
LIGHT
STORY
COMEDY
OR
A
DRAMA
HOT
OR
NOT
ARE
THESE
TV
CHARACTERS
IS
THIS
SCENE
A
IS
THIS
FIGURE
FRIENDS
OR
NEMESES
LOUNGE
OR
A
BEDROOM
SKATER
FORM
GOOD
FIGURE
MAIN
PREMISE
SUBJECTIVE
OR
COMPLEX
IMAGE
CLASSIFICA
TION
TASKS
SUCH
AS
THOSE
DEPICTED
ABOVE
MAY
REQUIRE
DEEPER
INSIGHT
FROM
HUMAN
ANNOTATORS
THAN
THE
USUAL
CATEGORY
LABELS
WE
PRO
POSE
TO
ASK
FOR
SPATIAL
OR
ATTRIBUTE
BASED
RATIONALES
FOR
THE
LABELS
CHOSEN
AND
AUGMENT
A
LARGE
MARGIN
CLASSIFIER
OBJECTIVE
TO
EXPLOIT
BOTH
THE
LABELS
AND
THESE
EXPLANATIONS
FIGURE
CAN
WE
REALLY
EXPECT
TO
LEARN
SUCH
SUBTLE
CON
CEPTS
PURELY
BY
TRAINING
SVMS
WITH
HOG
DESCRIPTORS
AND
CATEGORY
NAMES
WE
INSTEAD
PROPOSE
TO
ALLOW
ANNOTATORS
TO
GIVE
A
RATIONALE
FOR
THE
LABEL
THEY
CHOOSE
AND
THEN
DI
RECTLY
USE
THOSE
EXPLANATIONS
TO
STRENGTHEN
A
DISCRIMINATIVE
CLASSIFIER
THEIR
INSIGHT
ABOUT
WHY
SHOULD
NOT
ONLY
ENABLE
MORE
ACCURATE
MODELS
BUT
POTENTIALLY
ALSO
DO
SO
WITH
LESS
TOTAL
HUMAN
EFFORT
SINCE
WE
COULD
AMORTIZE
THE
TIME
SPENT
ANALYZING
THE
IMAGE
TO
DETERMINE
THE
LABEL
ITSELF
HOW
CAN
AN
ANNOTATOR
GIVE
AN
EXPLANATION
WE
PROPOSE
TWO
MODES
IN
THE
FIRST
THE
ANNOTATORS
INDICATE
WHICH
RE
GIONS
OF
THE
IMAGE
MOST
INFLUENCED
THEIR
LABEL
CHOICE
BY
DRAWING
POLYGONS
THAT
IS
THEY
HIGHLIGHT
WHAT
SEEMED
MOST
TELLING
FOR
THE
CLASSIFICATION
TASK
AT
HAND
I
CAN
TELL
IT
CLASS
X
MAINLY
DUE
TO
THIS
REGION
HERE
IN
THE
SECOND
MODE
THE
ANNOTATORS
INDICATE
WHICH
VISUAL
ATTRIBUTES
WERE
DEEMED
MOST
INFLUENTIAL
WHERE
AN
ATTRIBUTE
REFERS
TO
SOME
NAMEABLE
PROPERTY
OR
PART
FOR
EXAMPLE
ASSUMING
WE
HAVE
INTERMEDIATE
DETECTORS
FOR
ATTRIBUTES
LIKE
SIZE
COLOR
AND
SPE
CIFIC
TEXTURES
THEY
CAN
STATE
IT
TOO
ROUND
TO
BE
AN
X
OR
SHE
ATTRACTIVE
BECAUSE
SHE
FIT
IN
EITHER
CASE
THE
RATIONALE
SHOULD
HELP
FOCUS
THE
CLASSI
FIER
ON
THE
LOW
OR
MID
LEVEL
IMAGE
FEATURES
THAT
CAN
BEST
BE
USED
TO
DISCRIMINATE
BETWEEN
THE
DESIRED
IMAGE
CATE
GORIES
TO
THAT
END
WE
DIRECTLY
LEVERAGE
AN
IDEA
ORIGINALLY
DEVELOPED
BY
ZAIDAN
AND
COLLEAGUES
FOR
DOCUMENT
CLASSIFI
CATION
IT
GENERATES
SYNTHETIC
CONTRAST
EXAMPLES
THAT
LACK
THE
FEATURES
IN
THE
RATIONALES
AND
THEN
ADDS
CONSTRAINTS
TO
A
CLASSIFIER
OBJECTIVE
THAT
REQUIRE
THE
CONTRAST
EXAMPLES
TO
BE
CONSIDERED
LESS
POSITIVE
OR
LESS
NEGATIVE
THAN
THE
ORIGINAL
EXAMPLES
IN
THIS
WAY
THE
CONTRAST
EXAMPLES
CAN
REFINE
THE
DECISION
BOUNDARY
IN
THE
TARGET
LABEL
SPACE
WHILE
RECENT
WORK
EXPLORES
VARIOUS
ISSUES
IN
COLLECTING
USEFUL
LABELED
DATASETS
WE
ARE
THE
FIRST
TO
PROPOSE
ASKING
ANNOTATORS
FOR
EXPLANATIONS
OF
THEIR
LABELS
TO
DIRECTLY
IMPROVE
VISUAL
CATEGORY
LEARNING
WITH
OUT
INJECTING
INTO
A
CLASSIFIER
KNOWLEDGE
OF
WHY
A
GIVEN
LA
BEL
WAS
CHOSEN
TRADITIONAL
DISCRIMINATIVE
FEATURE
SELECTION
TECHNIQUES
RISK
OVERFITTING
TO
INADEQUATE
OR
BIASED
TRAINING
EXAMPLES
IN
CONTRAST
OUR
STRATEGY
STANDS
TO
BENEFIT
MORE
IMMEDIATELY
FROM
COMPLEX
HUMAN
INSIGHT
AND
THUS
POTEN
TIALLY
WITH
LESS
TOTAL
TRAINING
DATA
WE
DEMONSTRATE
OUR
APPROACH
WITH
BOTH
SCENE
AND
HU
MAN
ATTRACTIVENESS
CATEGORIZATION
TASKS
AND
REPORT
RESULTS
ON
THE
SCENES
AND
PUBLIC
FIGURES
FACE
DATASETS
AS
WELL
AS
A
NEW
DATASET
OF
HOT
OR
NOT
IMAGES
WE
SHOW
THAT
BOTH
PROPOSED
VISUAL
RATIONALES
CAN
IMPROVE
ABSOLUTE
RECOGNITION
ACCURACY
WE
ALSO
ANALYZE
THEIR
IMPACT
RELA
TIVE
TO
SEVERAL
BASELINES
INCLUDING
FOREGROUND
SEGMENTED
IMAGES
AND
A
STANDARD
MUTUAL
INFORMATION
FEATURE
SELECTION
APPROACH
OVERALL
WE
FIND
THAT
HUMAN
INTUITION
CAN
BE
CAP
TURED
IN
A
NEW
WAY
WITH
THE
PROPOSED
TECHNIQUE
RELATED
WORK
MUCH
WORK
IN
VISUAL
RECOGNITION
RESULTS
IN
UNMAN
AGEABLE
NUMBER
OF
PARAMETERS
IN
ADDITION
ESTIMATES
OF
THE
PARAMETERS
FOR
THE
MAJORITY
OF
EDGES
WOULD
BE
NOISY
THERE
ARE
SERIOUS
SMOOTHING
ISSUES
WE
ADOPT
AN
APPROACH
SIMILAR
TO
GOOD
TURING
SMOOTHING
METHODS
TO
A
CONTROL
THE
NUM
BER
OF
PARAMETERS
B
DO
SMOOTHING
WE
HAVE
MULTIPLE
ESTIMATES
FOR
THE
EDGES
POTENTIALS
WHICH
CAN
PROVIDE
MORE
ACCURATE
ESTIMATES
IF
USED
TOGETHER
WE
FORM
THE
LINEAR
COMBINATIONS
OF
THESE
POTENTIALS
THEREFORE
IN
LEARNING
WE
ARE
INTER
ESTED
IN
FINDING
WEIGHTS
OF
THE
LINEAR
COMBINATION
OF
THE
INITIAL
ESTIMATES
SO
THAT
THE
FINAL
LINEARLY
COMBINED
POTENTIALS
PROVIDE
VALUES
ON
THE
MRF
SO
THAT
THE
GROUND
TRUTH
TRIPLET
IS
THE
HIGHEST
SCORED
TRIPLET
FOR
ALL
EXAMPLES
THIS
WAY
WE
LIMIT
THE
NUMBER
OF
PARAMETERS
TO
THE
NUMBER
OF
INITIAL
ESTIMATES
WE
HAVE
FOUR
DIFFERENT
ESTIMATES
FOR
EDGES
OUR
FINAL
SCORE
ON
THE
EDGES
TAKE
THE
FORM
OF
A
LINEAR
COMBINATION
OF
THESE
ESTIMATES
OUR
FOUR
ESTIMATES
FOR
EDGES
FROM
NODE
A
TO
NODE
B
ARE
THE
NORMALIZED
FREQUENCY
OF
THE
WORD
A
IN
OUR
CORPUS
F
A
THE
NORMALIZED
FREQUENCY
OF
THE
WORD
B
IN
OUR
CORPUS
F
B
THE
NORMALIZED
FREQUENCY
OF
A
AND
B
AT
THE
SAME
TIME
SENTENCE
POTENTIALS
WE
NEED
A
REPRESENTATION
OF
THE
SENTENCES
WE
REPRESENT
A
SENTENCE
BY
COMPUTING
THE
SIMILARITY
BETWEEN
THE
SENTENCE
AND
OUR
TRIPLETS
FOR
THAT
WE
NEED
TO
HAVE
A
NOTION
OF
SIMILARITY
FOR
OBJECTS
SCENES
AND
ACTIONS
IN
TEXT
WE
USED
THE
CURRAN
CLARK
PARSER
TO
GENERATE
A
DEPENDENCY
PARSE
FOR
EACH
SENTENCE
WE
EXTRACTED
THE
SUBJECT
DIRECT
OBJECT
AND
ANY
NMOD
DEPENDEN
CIS
INVOLVING
A
NOUN
AND
A
VERB
THESE
DEPENDENCIES
WERE
USED
TO
GENERATE
THE
OBJECT
ACTION
PAIRS
FOR
THE
SENTENCES
IN
ORDER
TO
EXTRACT
THE
SCENE
INFORMATION
FROM
THE
SENTENCES
WE
EXTRACTED
THE
HEAD
NOUNS
OF
THE
PREPOSITIONAL
PHRASES
EXCEPT
FOR
THE
PREPOSITIONS
OF
AND
WITH
AND
THE
HEAD
NOUNS
OF
THE
PHRASE
X
IN
THE
BACKGROUND
LIN
SIMILARITY
MEASURE
FOR
OBJECTS
AND
SCENES
WE
USE
THE
LIN
SIMILARITY
MEASURE
TO
DETERMINE
THE
SEMANTIC
DISTANCE
BETWEEN
TWO
WORDS
THE
LIN
SIMILARITY
MEASURE
USES
WORDNET
SYNSETS
AS
THE
POSSIBLE
MEANINGS
OF
EACH
WORDS
THE
NOUN
SYNSETS
ARE
ARRANGED
IN
A
HEIRARCHY
BASED
ON
HYPERNYM
IS
A
AND
HYPONYM
INSTANCE
OF
RELATIONS
EACH
SYNSET
IS
DEFINED
AS
HAVING
AN
INFORMATION
CONTENT
BASED
ON
HOW
FREQUENTLY
THE
SYNSET
OR
A
HYPONYM
OF
THE
SYNSET
OCCURS
IN
A
CORPUS
IN
THE
CASE
SEMCOR
THE
SIMILARITY
OF
TWO
SYNSETS
IS
DEFINED
AS
TWICE
THE
INFORMATION
CONTENT
OF
THE
LEAST
COMMON
ANCESTOR
OF
THE
SYNSETS
DIVIDED
BY
THE
SUM
OF
THE
INFORMATION
CONTENT
OF
THE
TWO
SYNSETS
SIMILAR
SYNSETS
WILL
HAVE
A
LCA
THAT
COVERS
THE
TWO
SYNSETS
AND
VERY
LITTLE
ELSE
WHEN
WE
COMPARED
TWO
NOUNS
WE
CONSIDERED
ALL
PAIRS
OF
A
FILTERED
LIST
OF
SYNSETS
FOR
EACH
NOUN
AND
USED
THE
MOST
SIMILAR
SYNSETS
WE
FILTERED
THE
LIST
OF
SYNSETS
FOR
EACH
NOUN
BY
LIMITING
IT
TO
THE
FIRST
FOUR
SYNSETS
THAT
WERE
AT
LEAST
AS
FREQUENT
AS
THE
MOST
COMMON
SYNSET
OF
THAT
NOUN
WE
ALSO
REQUIRED
THE
SYNSETS
TO
BE
PHYSICAL
ENTITIES
ACTION
CO
OCCURRENCE
SCORE
WE
GENERATED
A
SECOND
IMAGE
CAPTION
DATA
SET
CONSISTING
OF
ROUGHLY
IMAGES
PULLED
FROM
SIX
FLICKR
GROUPS
FOR
ALL
PAIRS
OF
VERBS
WE
USED
THE
LIKELIHOOD
RATIO
TO
DETERMINE
IF
THE
TWO
VERBS
CO
OCCURRING
IN
THE
DIFFERENT
CAPTIONS
OF
THE
SAME
IMAGE
WAS
SIGNIFICANT
WE
THEN
USED
THE
LIKELIHOOD
RATIO
AS
THE
SIMILARITY
SCORE
FOR
THE
POSITIVELY
CORRELATED
VERB
PAIRS
AND
THE
NEGATIVE
OF
THE
LIKELIHOOD
RATIO
AS
THE
SIMILARITY
SCORE
FOR
THE
NEGATIVELY
CORRELATED
VERB
PAIRS
TYPICALLY
WE
FOUND
THAT
THIS
PROCEDURE
DISCOVERED
VERBS
THAT
WERE
EITHER
DESCRIBING
THE
SAME
ACTION
OR
DESCRIBING
TWO
ACTIONS
THAT
COMMONLY
CO
OCCURRED
NODE
POTENTIALS
WE
NOW
CAN
PROVIDE
A
SIMILARITY
MEASURE
BETWEEN
SENTENCES
AND
OBJECTS
ACTIONS
AND
SCENES
USING
SCORES
EXPLAINED
ABOVE
BELOW
WE
EXPLAIN
OUR
ESTIMATES
OF
SENTENCE
NODE
POTENTIALS
FIRST
WE
COMPUTE
THE
SIMILARITY
OF
EACH
OBJECT
SCENE
AND
ACTION
EXTRACTED
FROM
EACH
SENTENCE
THIS
GIVES
US
THE
THE
FIRST
ESTIMATES
FOR
THE
POTENTIALS
OVER
THE
NODES
WE
CALL
THIS
THE
SENTENCE
NODE
FEATURE
FOR
EACH
SENTENCE
WE
ALSO
COMPUTE
THE
AVERAGE
OF
SENTENCE
NODE
FEATURES
FOR
OTHER
FOUR
SENTENCES
DESCRIBING
THE
SAME
IMAGES
IN
THE
TRAIN
SET
WE
COMPUTE
THE
AVERAGE
OF
K
NEAREST
NEIGHBORS
IN
THE
SENTENCE
NODE
FEATURES
SPACE
FOR
A
GIVEN
SENTENCE
WE
CONSIDER
THIS
AS
OUR
THIRD
ESTIMATE
FOR
NODES
WE
ALSO
COMPUTE
THE
AVERAGE
OF
THE
IMAGE
NODE
FEATURES
FOR
IMAGES
CORRE
SPONDING
TO
THE
NEAREST
NEIGHBORS
IN
THE
ITEM
ABOVE
THE
AVERAGE
OF
THE
SENTENCE
NODE
FEATURES
OF
REFERENCE
SENTENCES
FOR
THE
NEAREST
NEIGHBORS
IN
THE
ITEM
IS
CONSIDERED
AS
OUR
FIFTH
ESTIMATE
FOR
NODES
WE
ALSO
INCLUDE
THE
SENTENCE
NODE
FEATURE
FOR
THE
REFERENCE
SENTENCE
EDGE
POTENTIALS
THE
EDGE
ESTIMATES
FOR
SENTENCES
ARE
IDENTICAL
TO
TO
EDGE
ESTIMATES
FOR
THE
IMAGES
EXPLAINED
IN
PREVIOUS
SECTION
LEARNING
THERE
ARE
TWO
MAPPINGS
THAT
NEED
TO
BE
LEARNED
THE
MAP
FROM
THE
IMAGE
SPACE
TO
THE
MEANING
SPACE
USES
THE
IMAGE
POTENTIALS
AND
THE
MAP
FROM
THE
SENTENCE
SPACE
TO
THE
MEANING
SPACE
USES
THE
SENTENCE
POTENTIALS
LEARNING
THE
MAPPING
FROM
IMAGES
TO
MEANING
INVOLVES
FINDING
THE
WEIGHTS
ON
THE
LINEAR
COMBINATIONS
OF
OUR
IMAGE
POTENTIALS
ON
NODES
AND
EDGES
SO
THAT
THE
GROUND
TRUTH
TRIPLETS
SCORE
HIGHEST
AMONG
ALL
OTHER
TRIPLETS
FOR
ALL
EXAMPLES
THIS
IS
A
STRUCTURE
LEARNING
PROBLEM
WHICH
TAKES
THE
FORM
OF
MIN
Λ
Ξ
SUBJECT
TO
WΦ
XI
YI
ΞI
Y
MAX
WΦ
XI
Y
L
YI
Y
I
EXAMPLES
MEANING
SPACE
ΞI
I
EXAMPLES
WHERE
Λ
IS
THE
TRADEOFF
FACTOR
BETWEEN
THE
REGULARIZATION
AND
SLACK
VARIABLES
Ξ
Φ
IS
OUR
FEATURE
FUNCTIONS
XI
CORRESPONDS
TO
OUR
ITH
IMAGE
AND
YI
IS
OUR
STRUCTURED
LABEL
FOR
THE
ITH
IMAGE
WE
USE
THE
STOCHASTIC
SUBGRADIENT
DESCENT
METHOD
TO
SOLVE
THIS
MINIMIZATION
EVALUATION
WE
EMPHASIZE
QUANTITATIVE
EVALUATION
IN
OUR
WORK
OUR
VOCABULARY
OF
MEANING
IS
SIGNIFICANTLY
LARGER
THAN
THE
EQUIVALENT
IN
EVALUATION
REQUIRES
INNOVATION
BOTH
IN
DATASETS
AND
IN
MEASUREMENT
DESCRIBED
BELOW
DATASET
WE
NEED
A
DATASET
WITH
IMAGES
AND
CORRESPONDING
SENTENCES
AND
ALSO
LABELS
FOR
OUR
REPRESENTATIONS
OF
THE
MEANING
SPACE
NO
SUCH
DATASET
EXISTS
WE
BUILD
OUR
OWN
DATASET
OF
IMAGES
AND
SENTENCES
AROUND
THE
PASCAL
IMAGES
THIS
MEANS
WE
CAN
USE
AND
COMPARE
TO
STATE
OF
THE
ART
MODELS
AND
IMAGE
ANNOTATIONS
IN
PASCAL
DATASET
PASCAL
SENTENCE
DATA
SET
TO
GENERATE
THE
SENTENCES
WE
STARTED
WITH
THE
PASCAL
DEVELOPMENT
KIT
WE
RANDOMLY
SELECTED
IMAGES
BELONGING
TO
EACH
OF
THE
CATEGORIES
ONCE
WE
HAD
A
SET
OF
IMAGES
WE
USED
AMAZON
MECHANICAL
TURK
TO
GENERATE
FIVE
CAPTIONS
FOR
EACH
IMAGE
WE
REQUIRED
THE
AN
NOTATORS
TO
BE
BASED
IN
THE
US
AND
THAT
THEY
PASS
A
QUALIFICATION
EXAM
TESTING
THEIR
ABILITY
TO
IDENTIFY
SPELLING
ERRORS
GRAMMATICAL
ERRORS
AND
DESCRIPTIVE
CAP
TIONS
MORE
DETAILS
ABOUT
THE
METHODS
OF
COLLECTION
CAN
BE
FOUND
IN
OUR
DATASET
HAS
SENTENCES
FOR
EACH
IMAGE
OF
THE
THOUSAND
IMAGES
RESULTING
IN
SENTENCES
WE
ALSO
MANUALLY
ADD
LABELS
FOR
TRIPLETS
OF
OBJECTS
ACTIONS
SCENES
FOR
EACH
IMAGES
THESE
TRIPLETS
LABEL
THE
MAIN
OBJECT
IN
THE
IMAGE
THE
MAIN
ACTION
AND
THE
MAIN
PLACE
THERE
ARE
DIFFERENT
TRIPLETS
IN
OUR
TRAIN
SET
AND
IN
TEST
SET
THERE
ARE
TRIPLETS
IN
THE
TEST
SET
THAT
APPEARED
IN
THE
TRAIN
SET
THE
DATASET
IS
AVAILABLE
AT
INFERENCE
OUR
MODEL
IS
LEARNED
TO
MAXIMIZE
THE
SUM
OF
THE
SCORES
ALONG
THE
PATH
IDENTI
FIED
BY
A
TRIPLET
IN
INFERENCE
WE
SEARCH
FOR
THE
TRIPLET
WHICH
GIVES
US
THE
BEST
ADDITIVE
SCORE
ARGMAXYWT
Φ
XI
Y
THESE
MODELS
PREFER
TRIPLETS
WITH
COMBINA
TION
OF
STRONG
AND
POOR
RESPONSES
OVER
ALL
MEDIOCRE
RESPONSES
WE
CONJECTURE
THAT
A
MULTIPLICATIVE
INFERENCE
MODEL
WOULD
RESULT
IN
BETTER
PREDICTIONS
AS
THE
MULTIPLICATIVE
MODEL
PREFERS
ALL
THE
RESPONSES
TO
BE
REASONABLY
GOOD
OUR
MUL
TIPLICATIVE
INFERENCE
HAS
THE
FORM
OF
ARGMAXY
WT
Φ
XI
Y
WE
SELECT
THE
BEST
TRIPLET
GIVEN
THE
POTENTIALS
ON
THE
NODES
AND
EDGES
GREEDILY
BY
RELAXING
AN
EDGE
AND
SOLVING
FOR
THE
BEST
PATH
AND
RE
SCORING
THE
RESULTS
USING
THE
RELAXED
EDGE
MATCHING
ONCE
WE
PREDICT
TRIPLETS
FOR
IMAGES
AND
SENTENCES
WE
CAN
SCORE
A
MATCH
BETWEEN
AN
IMAGE
AND
A
SENTENCE
IF
AN
IMAGE
AND
A
SENTENCE
PREDICT
VERY
SIMILAR
TRIPLETS
THEY
SHOULD
BE
PROJECTIONS
OF
NEARBY
POINTS
IN
THE
MEANING
SPACE
AND
SO
THEY
SHOULD
HAVE
A
HIGH
MATCHING
SCORE
A
NATURAL
SCORE
OF
THE
SIMILARITY
OF
SENTENCE
TRIPLETS
AND
IMAGE
TRIPLES
IS
THE
SUM
OF
RANKS
OF
SENTENCE
MEANING
AND
IMAGE
MEANING
THE
PAIR
WITH
SMALLEST
VALUE
OF
THIS
SUM
IS
BOTH
STRONGLY
PREDICTED
BY
THE
IMAGE
AND
STRONGLY
PREDICTED
BY
THE
SENTENCE
HOWEVER
THIS
SCORE
IS
LIKELY
TO
BE
NOISY
AND
IS
DIFFICULT
TO
COMPUTE
BECAUSE
WE
MUST
TOUCH
ALL
PAIRS
OF
MEANINGS
WE
USE
A
GOOD
NOISE
RESISTANT
APPROXIMATION
TO
OBTAIN
THE
SCORE
WE
OBTAIN
THE
TOP
K
RANKING
TRIPLETS
DERIVED
FROM
SENTENCES
AND
COMPUTE
THE
RANK
OF
EACH
AS
AN
IMAGE
TRIPLET
OBTAIN
THE
TOP
K
RANKING
TRIPLETS
DERIVED
FROM
IMAGES
AND
COMPUTE
THE
RANK
OF
EACH
AS
A
SENTENCE
TRIPLET
SUM
THE
SUM
OF
RANKS
FOR
EACH
OF
THESE
SETS
WEIGHTED
BY
IN
THE
INVERSE
RANK
OF
THE
TRIPLET
SO
AS
TO
EMPHASIZE
TRIPLETS
THAT
SCORE
STRONGLY
OUT
OF
VOCABULARY
EXTENSION
WE
GENERATE
SENTENCES
BY
SEARCHING
A
POOL
OF
SENTENCES
FOR
ONE
THAT
HAS
A
GOOD
MATCH
SCORE
TO
THE
IMAGE
WE
CANNOT
LEARN
A
DETECTOR
CLASSIFIER
FOR
EACH
OB
JECT
ACTION
SCENE
THAT
EXISTS
THIS
MEANS
WE
NEED
TO
SCORE
THE
SIMILARITY
BETWEEN
THE
IMAGE
AND
SENTENCES
THAT
CONTAIN
UNFAMILIAR
WORDS
WE
PROPOSE
USING
TEXT
INFORMATION
TO
ATTACK
THIS
PROBLEM
FOR
EACH
UNKNOWN
OBJECT
WE
CAN
PRODUCE
A
SCORE
OF
THE
SIMILARITY
OF
THAT
OBJECT
WITH
ALL
OF
THE
OBJECTS
IN
OUR
VOCABU
LARY
USING
DISTRIBUTIONAL
SEMANTICS
METHODS
EXPLAINED
IN
SECTION
WE
DO
THE
SAME
THING
FOR
VERBS
AND
SCENES
AS
WELL
THESE
SIMILARITY
MEASURES
WORK
AS
A
CRUDE
GUIDE
TO
OUR
MODEL
FOR
EXAMPLE
IN
FIGURE
WE
DON
T
HAVE
A
DETECTOR
FOR
VOLKSWAGEN
HERD
WOMAN
AND
CATTLE
BUT
WE
CAN
RECOGNIZE
THEM
OUR
SIMILARITY
MEASURES
PROVIDES
A
SIMILARITY
DISTRIBUTIONS
OVER
THINGS
WE
KNOW
THIS
SIMILARITY
DISTRIBUTION
HELPS
US
TO
RECOGNIZE
OBJECTS
ACTIONS
AND
SCENES
FOR
WHICH
WE
HAVE
NO
DETECTOR
CLASSIFIER
USING
OBJECTS
ACTIONS
SCENES
WE
KNOW
EXPERIMENTAL
SETTINGS
WE
DIVIDE
OUR
IMAGES
TO
TRAINING
IMAGES
AND
TESTING
IMAGES
WE
USE
NEAREST
NEIGHBORS
IN
BUILDING
POTENTIALS
FOR
IMAGES
AND
SENTENCES
FOR
MATCHING
WE
USE
CLOSEST
TRIPLETS
MAPPING
TO
THE
MEANING
SPACE
TABLE
COMPARES
THE
RESULTS
OF
MAPPING
THE
IMAGES
TO
THE
MEANING
SPACE
PRE
DICTING
TRIPLETS
FOR
IMAGES
TO
DO
THAT
WE
NEED
A
MEASURE
OF
COMPARISONS
BETWEEN
PAIRS
OF
TRIPLETS
THE
ONE
THAT
WE
PREDICT
AND
THE
GROUND
TRUTH
TRIPLETS
ONE
WAY
OF
DOING
THIS
IS
BY
SIMPLE
COMPARISONS
OF
TRIPLETS
A
PREDICTION
IS
CORRECT
IF
ALL
THREE
ELEMENTS
AGREE
AND
WRONG
OTHERWISE
WE
COULD
ALSO
MEASURE
IF
ANY
OF
THE
EL
EMENTS
IN
THE
TRIPLET
MATCH
EACH
SCORE
IS
INSENSITIVE
TO
IMPORTANT
ASPECTS
OF
LOSS
FOR
EXAMPLE
PREDICTING
CAT
SIT
MAT
WHEN
GROUND
TRUTH
IS
DOG
SIT
GROUND
IS
NOT
AS
BAD
AS
PREDICTING
BIKE
RIDE
STREET
THIS
IMPLIES
THAT
THE
PENALTY
FOR
CONFUSING
CATS
WITH
DOGS
SHOULD
BE
SMALLER
THAN
THAT
FOR
CONFUSING
CATS
WITH
BIKES
THE
SAME
ARGUMENT
HOLDS
FOR
ACTIONS
AND
SCENES
AS
WELL
WE
ALSO
NEED
OUR
MEASURE
TO
TAKE
INTO
ACCOUNT
THE
AMOUNT
OF
INFORMATION
A
PREDICTION
CONVEYS
FOR
EXAMPLE
PREDICTING
OBJECT
DO
SCENE
IS
LESS
FAVORABLE
THAN
CAT
SIT
MAT
TREE
MEASURE
TREE
MEASURE
WE
NEED
A
MEASURE
THAT
REFLECTS
TWO
IMPORTANT
INTERACTING
COMPONENTS
ACCURACY
AND
SPECIFICITY
WE
BELIEVE
THE
RIGHT
WAY
TO
SCORE
ERROR
IS
TO
USE
TAXONOMY
TREES
WE
HAVE
TAXONOMY
TREES
FOR
OBJECTS
ACTIONS
AND
SCENES
AND
WE
CAN
USE
THEM
TO
MEASURE
THE
ACCURACY
RELEVANCE
AND
SPECIFICITY
OF
PREDICTIONS
WE
INTRODUCE
A
NOVEL
MEASURE
TREE
WHICH
REFLECTS
HOW
ACCURATE
AND
SPECIFIC
THE
PREDICTION
IS
GIVEN
A
TAXONOMY
TREE
FOR
SAY
OBJECTS
OBJECTS
WE
REPRESENT
EACH
PREDICTION
BY
THE
PATH
FROM
THE
ROOT
OF
THE
TAXONOMY
TREE
TO
THE
PREDICTED
NODE
FOR
EXAMPLE
IF
THE
PREDICTION
IS
CAT
WE
REPRESENT
IT
AS
OBJECTS
ANIMAL
CAT
WE
CAN
THEN
REPORT
THE
STANDARD
MEASURE
USING
THE
PRECISION
AND
RECALL
PRECISION
IS
DEFINED
AS
THE
TOTAL
NUMBER
OF
EDGES
ON
THE
PATH
THAT
MATCHES
THE
EDGES
ON
THE
GROUND
TRUTH
PATH
DIVIDED
BY
THE
TOTAL
NUMBER
OF
EDGES
ON
THE
GROUND
TRUTH
PATH
AND
RECALL
AS
THE
TOTAL
NUMBER
OF
EDGES
ON
THE
PREDICTED
PATH
WHICH
IS
IN
THE
GROUND
TRUTH
PATH
DIVIDED
BY
THE
TOTAL
NUMBER
OF
EDGES
IN
THE
PATH
FOR
EXAMPLE
THE
MEASURE
FOR
PREDICTING
DOG
WHEN
THE
GROUND
TRUTH
IS
CAT
IS
WHERE
THE
PRECISION
IS
AND
RECALL
IS
THE
MEASURE
FOR
PREDICTING
ANIMAL
WHEN
THE
GROUND
TRUTH
IS
CAT
IS
AND
IT
IS
FOR
PREDICTING
BIKE
WHEN
THE
GROUND
TRUTH
IS
CAT
THE
SAME
PROCEDURE
IS
APPLIED
TO
ACTIONS
AND
SCENES
THE
TREE
MEASURE
FOR
A
TRIPLE
IS
THE
MEAN
OF
THE
THREE
MEASURES
FOR
OBJECTS
ACTIONS
AND
SCENES
TABLE
SHOWS
TREE
MEASURES
FOR
SEVERAL
DIFFERENT
EXPERIMENTAL
SETTINGS
BLUE
MEASURE
SIMILAR
TO
MACHINE
TRANSLATION
APPROACHES
WHERE
REPORTS
OF
ACCURACY
INVOLVES
SCORES
FOR
THE
CORRECTNESS
OF
THE
TRANSLATION
AND
THE
CORRECTNESS
OF
THE
GENERATED
TRANSLATION
IN
TERMS
OF
LANGUAGE
AND
LOGIC
WE
ALSO
CONSIDER
ANOTHER
MEASURE
TO
CHECK
IF
THE
TRIPLET
WE
GENERATE
IS
LOGICALLY
VALID
OR
NOT
ANALOGOUS
TO
THE
BLEU
SCORE
IN
MACHINE
TRANSLATION
LITERATURE
WE
INTRODUCE
THE
BLUE
SCORE
WHICH
MEASURES
THIS
FOR
EXAMPLE
BOTTLE
WALK
STREET
IS
NOT
VALID
FOR
THAT
WE
CHECK
IF
THE
TRIPLET
EVER
APPEARED
IN
OUR
CORPUS
OR
NOT
TABLE
SHOWS
THESE
SCORES
FOR
THE
TRIPLETS
PREDICTED
BY
SEVERAL
DIFFERENT
EXPERIMENTAL
SETTINGS
RESULTS
TO
EVALUATE
OUR
METHOD
WE
PROVIDE
QUALITATIVE
AND
QUANTITATIVE
RESULTS
THERE
ARE
TWO
STAGES
IN
OUR
MODEL
FIRST
WE
SHOW
THE
ABILITY
OF
OUR
METHOD
TO
MAP
TABLE
EVALUATION
OF
MAPPING
FROM
THE
IMAGE
SPACE
TO
THE
MEANING
SPACE
OBJ
MEANS
WHEN
WE
ONLY
CONSIDER
THE
POTENTIALS
ON
THE
OBJECT
NODE
AND
USE
UNIFORM
POTEN
TIALS
FOR
OTHER
NODES
AND
EDGES
NO
EDGE
MEANS
ASSUMING
A
UNIFORM
POTENTIAL
OVER
EDGES
FW
A
STANDS
FOR
FIXED
WEIGHTS
WITH
ADDITIVE
INFERENCE
MODEL
THIS
IS
THE
CASE
WHERE
WE
USE
ALL
THE
POTENTIALS
BUT
WE
DON
T
LEARN
ANY
WEIGHTS
FOR
THEM
SL
A
MEANS
USING
STRUCTURE
LEARNING
WITH
ADDITIVE
INFERENCE
MODEL
FW
M
IS
SIMILAR
TO
FW
A
WITH
THE
EXCEPTION
THAT
THE
INFERENCE
MODEL
IS
MULTIPLICATIVE
INSTEAD
OF
ADDITIVE
SL
M
IS
THE
STRUCTURE
LEARNING
WITH
MULTIPLICATIVE
INFERENCE
FROM
THE
IMAGE
SPACE
TO
THE
MEANING
SPACE
WE
THEN
EVALUATE
OUR
RESULTS
ON
PREDICTING
SENTENCES
FOR
IMAGES
ANNOTATION
WE
ALSO
SHOW
QUALITATIVE
RESULTS
FOR
FINDING
IMAGES
FOR
SENTENCES
ILLUSTRATION
MAPPING
IMAGES
TO
MEANINGS
TABLE
COMPARES
SEVERAL
DIFFERENT
EXPERIMENTAL
SETTINGS
IN
TERMS
OF
TWO
MEASURES
EXPLAINED
ABOVE
TREE
AND
BLUE
EACH
COLUMN
IN
TABLE
CORRESPONDS
TO
AN
EXPERIMENTAL
SETTING
WE
REPORT
AVERAGE
TREE
AND
AVERAGE
BLUE
MEASURES
FOR
FIVE
TOP
TRIPLETS
FOR
ALL
IMAGES
WE
ALSO
BREAKDOWN
THE
TREE
TO
OBJECTS
ACTIONS
AND
SCENES
IN
BOTTOM
THREE
ROWS
OF
THE
TABLE
ANNOTATION
GENERATING
SENTENCES
FROM
IMAGES
FIGURE
SHOWS
TOP
PREDICTED
TRIPLETS
AND
TOP
GENERATED
SENTENCES
FOR
EXAMPLE
IMAGES
IN
OUR
TEST
SET
QUANTITATIVE
EVALUATION
OF
GENERATED
SENTENCE
IS
VERY
CHALLENGING
WE
TRAINED
INDIVIDUALS
TO
ANNOTATE
GENERATED
SENTENCES
WE
ASK
THEM
TO
ANNOTATE
EACH
GENERATED
SENTENCE
BY
EITHER
OR
MEANS
THAT
THE
SENTENCE
IS
QUITE
ACCURATE
WITH
POSSIBLE
LITTLE
MISTAKES
ABOUT
DETAILS
IN
THE
SENTENCE
IMPLIES
THAT
THE
SENTENCE
HAVE
A
ROUGH
IDEA
ABOUT
THE
IMAGE
BUT
IT
NOT
VERY
ACCURATE
AND
MEANS
THAT
THE
SENTENCE
IS
NOT
EVEN
REMOTELY
CLOSE
TO
THE
IMAGE
WE
GENERATE
SENTENCES
FOR
EACH
IMAGE
THE
TOTAL
AVERAGE
OF
THE
SCORES
GIVEN
BY
THESE
INDIVIDUALS
IS
THE
AVERAGE
NUMBER
OF
SENTENCES
WITH
SCORE
ONE
PER
IMAGE
IS
THE
AVERAGE
NUMBER
OF
SENTENCES
WITH
SCORE
PER
IMAGE
IS
OF
IMAGES
HAVE
AT
LEAST
ONE
SENTENCE
WITH
SCORE
SENTENCES
OUT
OF
IMAGES
HAVE
AT
LEAST
ONE
SENTENCE
WITH
SCORE
ILLUSTRATION
FINDING
IMAGES
BEST
DESCRIBED
BY
SENTENCES
NOT
ONLY
OUR
MODEL
CAN
PROVIDE
SENTENCES
THAT
DESCRIBE
AN
IMAGE
BUT
IT
ALSO
CAN
FIND
IMAGES
WHICH
ARE
BEST
DESCRIBED
BY
A
GIVEN
SENTENCE
ONCE
THE
CONNECTIONS
TO
THE
MEANING
SPACE
IS
ESTABLISHED
ONE
COULD
GO
IN
BOTH
DIRECTIONS
FROM
IMAGES
TO
SENTENCES
OR
THE
OTHER
WAY
AROUND
FIGURE
SHOWS
EXAMPLES
OF
FINDING
IMAGES
FOR
SENTENCES
FOR
MORE
QUALITATIVE
RESULTS
PLEASE
SEE
THE
SUPPLEMENTARY
MATERIAL
FIG
GENERATING
SENTENCES
FOR
IMAGES
WE
SHOW
TOP
FIVE
PREDICTED
TRIPLETS
IN
THE
MIDDLE
COLUMN
AND
TOP
FIVE
PREDICTED
SENTENCES
IN
THE
RIGHT
COLUMN
OUT
OF
VOCABULARY
EXTENSION
FIGURE
DEPICTS
EXAMPLES
OF
THE
CASES
WHERE
WE
COULD
SUCCESSFULLY
RECOGNIZE
OB
JECTS
ACTIONS
FOR
WHICH
WE
HAVE
NO
DETECTOR
CLASSIFIER
THIS
IS
VERY
INTERESTING
AS
THE
INTERMEDIATE
MEANING
SPACE
ALLOWS
US
TO
BENEFIT
FROM
DISTRIBUTIONAL
SEMAN
TICS
THIS
MEANS
THAT
WE
CAN
LEARN
TO
RECOGNIZE
UNKNOWN
OBJECTS
ACTIONS
SCENES
BY
LOOKING
AT
THE
PATTERNS
OF
RESPONSES
FROM
OTHER
SIMILAR
KNOWN
DETECTOR
CLASSIFIERS
DISCUSSION
AND
FUTURE
WORK
SENTENCES
ARE
RICH
COMPACT
AND
SUBTLE
REPRESENTATIONS
OF
INFORMATION
EVEN
SO
WE
CAN
PREDICT
GOOD
SENTENCES
FOR
IMAGES
THAT
PEOPLE
LIKE
THE
INTERMEDIATE
MEANING
REPRESENTATION
IS
ONE
KEY
COMPONENT
IN
OUR
MODEL
AS
IT
ALLOWS
BENEFITING
FROM
DISTRIBUTIONAL
SEMANTICS
OUR
SENTENCE
MODEL
IS
OVERSIMPLIFIED
WE
THINK
AN
ITERATIVE
PROCEDURE
FOR
GOING
DEEPER
IN
SENTENCES
AND
IMAGES
WOULD
BE
THE
RIGHT
DIRECTION
ONCE
A
SENTENCE
IS
GENERATED
FOR
AN
IMAGE
IT
IS
MUCH
EASIER
TO
CHECK
FOR
ADJECTIVES
AND
ADVERBS
AKNOWLEDGEMENTS
THIS
WORK
WAS
SUPPORTED
IN
PART
BY
THE
NATIONAL
SCIENCE
FOUNDATION
UNDER
IIS
AND
IN
PART
BY
THE
OFFICE
OF
NAVAL
RESEARCH
UNDER
AS
PART
OF
THE
MURI
PROGRAM
IN
PART
BY
A
GIFT
FROM
GOOGLE
ANY
OPINIONS
FINDINGS
AND
CONCLU
SIONS
OR
RECOMMENDATIONS
EXPRESSED
IN
THIS
MATERIAL
ARE
THOSE
OF
THE
AUTHOR
AND
DO
A
TWO
GIRLS
IN
THE
STORE
YELLOW
TRAIN
ON
THE
TRACKS
A
SMALL
HERD
OF
ANIMALS
WITH
A
CALF
IN
THE
GRASS
A
HORSE
BEING
RIDDEN
WITHIN
A
FENCED
AREA
FIG
FINDING
IMAGES
FOR
SENTENCES
ONCE
THE
MATCHING
IN
THE
MEANING
SPACE
IS
ES
TABLISHED
WE
CAN
GENERATE
SENTENCES
FOR
IMAGES
ANNOTATION
AND
ALSO
FIND
IMAGES
THAT
CAN
BE
BEST
DESCRIBE
BY
A
SENTENCE
IN
THIS
PICTURE
WE
SHOW
FOUR
SENTENCES
WITH
FOUR
HIGHEST
RANKED
IMAGES
WE
PROVIDE
A
LIST
OF
HIGHEST
SCORE
IMAGES
FOR
EACH
SENTENCE
FOR
THE
TEST
SET
IN
THE
SUPPLEMENTARY
MATERIAL
FIG
EXAMPLES
OF
FAILURES
IN
GENERATING
SENTENCES
FOR
IMAGES
NOT
NECESSARILY
REFLECT
THOSE
OF
THE
NATIONAL
SCIENCE
FOUNDATION
OR
THE
OFFICE
OF
NAVAL
RESEARCH
ALI
FARHADI
WAS
SUPPORTED
BY
THE
GOOGLE
PHD
FELLOWSHIP
WE
ALSO
WOULD
LIKE
TO
THANK
MAJID
ASHTIANI
FOR
HIS
HELP
ON
CLUSTER
COMPUTING
AND
HADI
KIAPOUR
ATTIYE
HOSSEINI
FOR
THEIR
HELP
ON
EVALUATION
FROM
IMAGES
TO
SENTENCES
FROM
SENTENCES
TO
IMAGES
A
RED
LONDON
UNITED
DOUBLE
DECKER
BUS
DRIVES
DOWN
A
CITY
STREET
A
VERY
COLORFUL
VOLKSWAGEN
BEETLE
TWO
YOUNG
WOMEN
WITH
TWO
LITTLE
GIRL
NEAR
THEM
CATTLE
FEEDING
AT
A
TROUGH
FIG
OUT
OF
VOCABULARY
EXTENSION
WE
DON
T
HAVE
DETECTORS
FOR
DRIVES
WOMEN
VOLKSWAGEN
AND
CATTLE
DESPITE
THIS
FACT
WE
COULD
RECOGNIZE
THESE
OB
JECTS
ACTIONS
DISTRIBUTIONAL
SEMANTICS
PROVIDE
US
WITH
THE
ABILITY
TO
MODEL
UNKNOWN
OBJECTS
ACTIONS
CATEGORIES
WITH
THEIR
SIMILARITIES
TO
KNOWN
CATEGORIES
HERE
WE
SHOW
EXAMPLES
OF
SENTENCES
AND
IMAGES
WHEN
WE
COULD
RECOGNIZE
THESE
UNKNOWNS
FOR
BOTH
GENERATING
SENTENCES
FROM
IMAGES
AND
FINDING
IMAGES
FOR
SENTENCES
VIDEO
GOOGLE
A
TEXT
RETRIEVAL
APPROACH
TO
OBJECT
MATCHING
IN
VIDEOS
JOSEF
SIVIC
AND
ANDREW
ZISSERMAN
ROBOTICS
RESEARCH
GROUP
DEPARTMENT
OF
ENGINEERING
SCIENCE
UNIVERSITY
OF
OXFORD
UNITED
KINGDOM
ABSTRACT
WE
DESCRIBE
AN
APPROACH
TO
OBJECT
AND
SCENE
RETRIEVAL
WHICH
SEARCHES
FOR
AND
LOCALIZES
ALL
THE
OCCURRENCES
OF
A
USER
OUTLINED
OBJECT
IN
A
VIDEO
THE
OBJECT
IS
REPRESENTED
BY
A
SET
OF
VIEWPOINT
INVARIANT
REGION
DESCRIPTORS
SO
THAT
RECOG
NITION
CAN
PROCEED
SUCCESSFULLY
DESPITE
CHANGES
IN
VIEW
POINT
ILLUMINATION
AND
PARTIAL
OCCLUSION
THE
TEMPORAL
CONTINUITY
OF
THE
VIDEO
WITHIN
A
SHOT
IS
USED
TO
TRACK
THE
REGIONS
IN
ORDER
TO
REJECT
UNSTABLE
REGIONS
AND
REDUCE
THE
EFFECTS
OF
NOISE
IN
THE
DESCRIPTORS
THE
ANALOGY
WITH
TEXT
RETRIEVAL
IS
IN
THE
IMPLEMENTATION
WHERE
MATCHES
ON
DESCRIPTORS
ARE
PRE
COMPUTED
USING
VEC
TOR
QUANTIZATION
AND
INVERTED
FILE
SYSTEMS
AND
DOCUMENT
RANKINGS
ARE
USED
THE
RESULT
IS
THAT
RETRIEVAL
IS
IMMEDIATE
RETURNING
A
RANKED
LIST
OF
KEY
FRAMES
SHOTS
IN
THE
MANNER
OF
GOOGLE
THE
METHOD
IS
ILLUSTRATED
FOR
MATCHING
ON
TWO
FULL
LENGTH
FEATURE
FILMS
INTRODUCTION
THE
AIM
OF
THIS
WORK
IS
TO
RETRIEVE
THOSE
KEY
FRAMES
AND
SHOTS
OF
A
VIDEO
CONTAINING
A
PARTICULAR
OBJECT
WITH
THE
EASE
SPEED
AND
ACCURACY
WITH
WHICH
GOOGLE
RETRIEVES
TEXT
DOCU
MENTS
WEB
PAGES
CONTAINING
PARTICULAR
WORDS
THIS
PAPER
INVESTIGATES
WHETHER
A
TEXT
RETRIEVAL
APPROACH
CAN
BE
SUC
CESSFULLY
EMPLOYED
FOR
OBJECT
RECOGNITION
IDENTIFYING
AN
IDENTICAL
OBJECT
IN
A
DATABASE
OF
IMAGES
IS
NOW
REACHING
SOME
MATURITY
IT
IS
STILL
A
CHALLENGING
PROB
LEM
BECAUSE
AN
OBJECT
VISUAL
APPEARANCE
MAY
BE
VERY
DIF
FERENT
DUE
TO
VIEWPOINT
AND
LIGHTING
AND
IT
MAY
BE
PARTIALLY
OCCLUDED
BUT
SUCCESSFUL
METHODS
NOW
EXIST
TYPICALLY
AN
OBJECT
IS
REPRESENTED
BY
A
SET
OF
OVERLAPPING
REGIONS
EACH
REPRESENTED
BY
A
VECTOR
COMPUTED
FROM
THE
REGION
APPEAR
ANCE
THE
REGION
SEGMENTATION
AND
DESCRIPTORS
ARE
BUILT
WITH
A
CONTROLLED
DEGREE
OF
INVARIANCE
TO
VIEWPOINT
AND
ILLU
MINATION
CONDITIONS
SIMILAR
DESCRIPTORS
ARE
COMPUTED
FOR
ALL
IMAGES
IN
THE
DATABASE
RECOGNITION
OF
A
PARTICULAR
OB
JECT
PROCEEDS
BY
NEAREST
NEIGHBOUR
MATCHING
OF
THE
DESCRIP
TOR
VECTORS
FOLLOWED
BY
DISAMBIGUATING
USING
LOCAL
SPA
TIAL
COHERENCE
SUCH
AS
NEIGHBOURHOODS
ORDERING
OR
SPATIAL
LAYOUT
OR
GLOBAL
RELATIONSHIPS
SUCH
AS
EPIPOLAR
GEOMETRY
EXAMPLES
INCLUDE
WE
EXPLORE
WHETHER
THIS
TYPE
OF
APPROACH
TO
RECOGNITION
CAN
BE
RECAST
AS
TEXT
RETRIEVAL
IN
ESSENCE
THIS
REQUIRES
A
VISUAL
ANALOGY
OF
A
WORD
AND
HERE
WE
PROVIDE
THIS
BY
VECTOR
QUANTIZING
THE
DESCRIPTOR
VECTORS
HOWEVER
IT
WILL
BE
SEEN
THAT
PURSUING
THE
ANALOGY
WITH
TEXT
RETRIEVAL
IS
MORE
THAN
A
SIMPLE
OPTIMIZATION
OVER
DIFFERENT
VECTOR
QUANTIZATIONS
THERE
ARE
MANY
LESSONS
AND
RULES
OF
THUMB
THAT
HAVE
BEEN
LEARNT
AND
DEVELOPED
IN
THE
TEXT
RETRIEVAL
LITERATURE
AND
IT
IS
WORTH
ASCERTAINING
IF
THESE
ALSO
CAN
BE
EMPLOYED
IN
VISUAL
RETRIEVAL
THE
BENEFITS
OF
THIS
APPROACH
IS
THAT
MATCHES
ARE
EFFEC
TIVELY
PRE
COMPUTED
SO
THAT
AT
RUN
TIME
FRAMES
AND
SHOTS
CONTAINING
ANY
PARTICULAR
OBJECT
CAN
BE
RETRIEVED
WITH
NO
DELAY
THIS
MEANS
THAT
ANY
OBJECT
OCCURRING
IN
THE
VIDEO
AND
CONJUNCTIONS
OF
OBJECTS
CAN
BE
RETRIEVED
EVEN
THOUGH
THERE
WAS
NO
EXPLICIT
INTEREST
IN
THESE
OBJECTS
WHEN
DE
SCRIPTORS
WERE
BUILT
FOR
THE
VIDEO
HOWEVER
WE
MUST
ALSO
DETERMINE
WHETHER
THIS
VECTOR
QUANTIZED
RETRIEVAL
MISSES
ANY
MATCHES
THAT
WOULD
HAVE
BEEN
OBTAINED
IF
THE
FORMER
METHOD
OF
NEAREST
NEIGHBOUR
MATCHING
HAD
BEEN
USED
REVIEW
OF
TEXT
RETRIEVAL
TEXT
RETRIEVAL
SYSTEMS
GENERALLY
EMPLOY
A
NUMBER
OF
STANDARD
STEPS
THE
DOCUMENTS
ARE
FIRST
PARSED
INTO
WORDS
SECOND
THE
WORDS
ARE
REPRE
SENTED
BY
THEIR
STEMS
FOR
EXAMPLE
WALK
WALKING
AND
WALKS
WOULD
BE
REPRESENTED
BY
THE
STEM
WALK
THIRD
A
STOP
LIST
IS
USED
TO
REJECT
VERY
COMMON
WORDS
SUCH
AS
THE
AND
AN
WHICH
OCCUR
IN
MOST
DOCUMENTS
AND
ARE
THEREFORE
NOT
DISCRIMINATING
FOR
A
PARTICULAR
DOCUMENT
THE
REMAIN
ING
WORDS
ARE
THEN
ASSIGNED
A
UNIQUE
IDENTIFIER
AND
EACH
DOCUMENT
IS
REPRESENTED
BY
A
VECTOR
WITH
COMPONENTS
GIVEN
BY
THE
FREQUENCY
OF
OCCURRENCE
OF
THE
WORDS
THE
DOCUMENT
CONTAINS
IN
ADDITION
THE
COMPONENTS
ARE
WEIGHTED
IN
VARI
OUS
WAYS
DESCRIBED
IN
MORE
DETAIL
IN
SECTION
AND
IN
THE
CASE
OF
GOOGLE
THE
WEIGHTING
OF
A
WEB
PAGE
DEPENDS
ON
THE
NUMBER
OF
WEB
PAGES
LINKING
TO
THAT
PARTICULAR
PAGE
ALL
OF
THE
ABOVE
STEPS
ARE
CARRIED
OUT
IN
ADVANCE
OF
ACTUAL
RE
TRIEVAL
AND
THE
SET
OF
VECTORS
REPRESENTING
ALL
THE
DOCUMENTS
IN
A
CORPUS
ARE
ORGANIZED
AS
AN
INVERTED
FILE
TO
FACILITATE
EFFICIENT
RETRIEVAL
AN
INVERTED
FILE
IS
STRUCTURED
LIKE
AN
IDEAL
BOOK
INDEX
IT
HAS
AN
ENTRY
FOR
EACH
WORD
IN
THE
CORPUS
FOL
LOWED
BY
A
LIST
OF
ALL
THE
DOCUMENTS
AND
POSITION
IN
THAT
DOCUMENT
IN
WHICH
THE
WORD
OCCURS
A
TEXT
IS
RETRIEVED
BY
COMPUTING
ITS
VECTOR
OF
WORD
FREQUENCIES
AND
RETURNING
THE
DOCUMENTS
WITH
THE
CLOSEST
MEASURED
BY
ANGLES
VECTORS
IN
ADDITION
THE
MATCH
ON
THE
ORDERING
AND
SEPARATION
OF
THE
WORDS
MAY
BE
USED
TO
RANK
THE
RETURNED
DOCUMENTS
PAPER
OUTLINE
HERE
WE
EXPLORE
VISUAL
ANALOGIES
OF
EACH
OF
THESE
STEPS
SECTION
DESCRIBES
THE
VISUAL
DESCRIPTORS
USED
SECTION
THEN
DESCRIBES
THEIR
VECTOR
QUANTIZATION
INTO
VISUAL
WORDS
AND
SECTION
WEIGHTING
AND
INDEXING
FOR
THE
VECTOR
MODEL
THESE
IDEAS
ARE
THEN
EVALUATED
ON
A
GROUND
TRUTH
SET
OF
FRAMES
IN
SECTION
FINALLY
A
STOP
LIST
AND
RANKING
BY
A
MATCH
ON
SPATIAL
LAYOUT
ARE
INTRODUCED
IN
SECTION
AND
USED
TO
EVALUATE
OBJECT
RETRIEVAL
THROUGHOUT
TWO
FEATURE
FILMS
RUN
LOLA
RUN
LOLA
RENNT
TYKWER
AND
GROUNDHOG
DAY
RAMIS
ALTHOUGH
PREVIOUS
WORK
HAS
BORROWED
IDEAS
FROM
THE
TEXT
RETRIEVAL
LITERATURE
FOR
IMAGE
RETRIEVAL
FROM
DATABASES
E
G
USED
THE
WEIGHTING
AND
INVERTED
FILE
SCHEMES
TO
THE
BEST
OF
OUR
KNOWLEDGE
THIS
IS
THE
FIRST
SYSTEMATIC
APPLI
CATION
OF
THESE
IDEAS
TO
OBJECT
RETRIEVAL
IN
VIDEOS
VIEWPOINT
INVARIANT
DESCRIPTION
TWO
TYPES
OF
VIEWPOINT
COVARIANT
REGIONS
ARE
COMPUTED
FOR
EACH
FRAME
THE
FIRST
IS
CONSTRUCTED
BY
ELLIPTICAL
SHAPE
ADAP
TATION
ABOUT
AN
INTEREST
POINT
THE
METHOD
INVOLVES
ITERA
TIVELY
DETERMINING
THE
ELLIPSE
CENTRE
SCALE
AND
SHAPE
THE
SCALE
IS
DETERMINED
BY
THE
LOCAL
EXTREMUM
ACROSS
SCALE
OF
A
LAPLACIAN
AND
THE
SHAPE
BY
MAXIMIZING
INTENSITY
GRADIENT
ISOTROPY
OVER
THE
ELLIPTICAL
REGION
THE
IMPLEMENTA
TION
DETAILS
ARE
GIVEN
IN
THIS
REGION
TYPE
IS
REFERRED
TO
AS
SHAPE
ADAPTED
SA
THE
SECOND
TYPE
OF
REGION
IS
CONSTRUCTED
BY
SELECTING
AR
EAS
FROM
AN
INTENSITY
WATERSHED
IMAGE
SEGMENTATION
THE
REGIONS
ARE
THOSE
FOR
WHICH
THE
AREA
IS
APPROXIMATELY
STA
TIONARY
AS
THE
INTENSITY
THRESHOLD
IS
VARIED
THE
IMPLEMEN
TATION
DETAILS
ARE
GIVEN
IN
THIS
REGION
TYPE
IS
REFERRED
TO
AS
MAXIMALLY
STABLE
MS
TWO
TYPES
OF
REGIONS
ARE
EMPLOYED
BECAUSE
THEY
DETECT
DIFFERENT
IMAGE
AREAS
AND
THUS
PROVIDE
COMPLEMENTARY
REP
RESENTATIONS
OF
A
FRAME
THE
SA
REGIONS
TEND
TO
BE
CENTERED
ON
CORNER
LIKE
FEATURES
AND
THE
MS
REGIONS
CORRESPOND
TO
BLOBS
OF
HIGH
CONTRAST
WITH
RESPECT
TO
THEIR
SURROUNDINGS
SUCH
AS
A
DARK
WINDOW
ON
A
GRAY
WALL
BOTH
TYPES
OF
RE
GIONS
ARE
REPRESENTED
BY
ELLIPSES
THESE
ARE
COMPUTED
AT
TWICE
THE
ORIGINALLY
DETECTED
REGION
SIZE
IN
ORDER
FOR
THE
IM
AGE
APPEARANCE
TO
BE
MORE
DISCRIMINATING
FOR
A
PIXEL
VIDEO
FRAME
THE
NUMBER
OF
REGIONS
COMPUTED
IS
TYPI
CALLY
AN
EXAMPLE
IS
SHOWN
IN
FIGURE
EACH
ELLIPTICAL
AFFINE
INVARIANT
REGION
IS
REPRESENTED
BY
A
DIMENSIONAL
VECTOR
USING
THE
SIFT
DESCRIPTOR
DEVEL
FIGURE
TOP
ROW
TWO
FRAMES
SHOWING
THE
SAME
SCENE
FROM
VERY
DIFFERENT
CAMERA
VIEWPOINTS
FROM
THE
FILM
RUN
LOLA
RUN
MIDDLE
ROW
FRAMES
WITH
DETECTED
AFFINE
INVARIANT
REGIONS
SUPER
IMPOSED
MAXIMALLY
STABLE
MS
REGIONS
ARE
IN
YELLOW
SHAPE
ADAPTED
SA
REGIONS
ARE
IN
CYAN
BOTTOM
ROW
FINAL
MATCHED
REGIONS
AFTER
INDEXING
AND
SPATIAL
CONSENSUS
NOTE
THAT
THE
CORRE
SPONDENCES
DEFINE
THE
SCENE
OVERLAP
BETWEEN
THE
TWO
FRAMES
OPED
BY
LOWE
IN
THIS
DESCRIPTOR
WAS
SHOWN
TO
BE
SU
PERIOR
TO
OTHERS
USED
IN
THE
LITERATURE
SUCH
AS
THE
RESPONSE
OF
A
SET
OF
STEERABLE
FILTERS
OR
ORTHOGONAL
FILTERS
AND
WE
HAVE
ALSO
FOUND
SIFT
TO
BE
SUPERIOR
BY
COMPARING
SCENE
RETRIEVAL
RESULTS
AGAINST
GROUND
TRUTH
AS
IN
SECTION
THE
REASON
FOR
THIS
SUPERIOR
PERFORMANCE
IS
THAT
SIFT
UNLIKE
THE
OTHER
DESCRIPTORS
IS
DESIGNED
TO
BE
INVARIANT
TO
A
SHIFT
OF
A
FEW
PIXELS
IN
THE
REGION
POSITION
AND
THIS
LOCALIZATION
ER
ROR
IS
ONE
THAT
OFTEN
OCCURS
COMBINING
THE
SIFT
DESCRIPTOR
WITH
AFFINE
COVARIANT
REGIONS
GIVES
REGION
DESCRIPTION
VEC
TORS
WHICH
ARE
INVARIANT
TO
AFFINE
TRANSFORMATIONS
OF
THE
IM
AGE
NOTE
BOTH
REGION
DETECTION
AND
THE
DESCRIPTION
IS
COM
PUTED
ON
MONOCHROME
VERSIONS
OF
THE
FRAMES
COLOUR
INFOR
MATION
IS
NOT
CURRENTLY
USED
IN
THIS
WORK
TO
REDUCE
NOISE
AND
REJECT
UNSTABLE
REGIONS
INFORMATION
IS
AGGREGATED
OVER
A
SEQUENCE
OF
FRAMES
THE
REGIONS
DE
TECTED
IN
EACH
FRAME
OF
THE
VIDEO
ARE
TRACKED
USING
A
SIMPLE
CONSTANT
VELOCITY
DYNAMICAL
MODEL
AND
CORRELATION
ANY
RE
GION
WHICH
DOES
NOT
SURVIVE
FOR
MORE
THAN
THREE
FRAMES
IS
REJECTED
EACH
REGION
OF
THE
TRACK
CAN
BE
REGARDED
AS
AN
INDEPENDENT
MEASUREMENT
OF
A
COMMON
SCENE
REGION
THE
PRE
IMAGE
OF
THE
DETECTED
REGION
AND
THE
ESTIMATE
OF
THE
DESCRIPTOR
FOR
THIS
SCENE
REGION
IS
COMPUTED
BY
AVERAGING
THE
DESCRIPTORS
THROUGHOUT
THE
TRACK
THIS
GIVES
A
MEASUR
ABLE
IMPROVEMENT
IN
THE
SIGNAL
TO
NOISE
OF
THE
DESCRIPTORS
WHICH
AGAIN
HAS
BEEN
DEMONSTRATED
USING
THE
GROUND
TRUTH
TESTS
OF
SECTION
BUILDING
A
VISUAL
VOCABULARY
THE
OBJECTIVE
HERE
IS
TO
VECTOR
QUANTIZE
THE
DESCRIPTORS
INTO
CLUSTERS
WHICH
WILL
BE
THE
VISUAL
WORDS
FOR
TEXT
RETRIEVAL
THEN
WHEN
A
NEW
FRAME
OF
THE
MOVIE
IS
OBSERVED
EACH
DE
SCRIPTOR
OF
THE
FRAME
IS
ASSIGNED
TO
THE
NEAREST
CLUSTER
AND
THIS
IMMEDIATELY
GENERATES
MATCHES
FOR
ALL
FRAMES
THROUGH
OUT
THE
MOVIE
THE
VOCABULARY
IS
CONSTRUCTED
FROM
A
SUB
PART
OF
THE
MOVIE
AND
ITS
MATCHING
ACCURACY
AND
EXPRESSIVE
POWER
ARE
EVALUATED
ON
THE
REMAINDER
OF
THE
MOVIE
AS
DE
SCRIBED
IN
THE
FOLLOWING
SECTIONS
THE
VECTOR
QUANTIZATION
IS
CARRIED
OUT
HERE
BY
K
MEANS
CLUSTERING
THOUGH
OTHER
METHODS
K
MEDOIDS
HISTOGRAM
BINNING
ETC
ARE
CERTAINLY
POSSIBLE
IMPLEMENTATION
REGIONS
ARE
TRACKED
THROUGH
CONTIGUOUS
FRAMES
AND
A
MEAN
VECTOR
DESCRIPTOR
X
I
COMPUTED
FOR
EACH
OF
THE
I
REGIONS
TO
REJECT
UNSTABLE
REGIONS
THE
OF
TRACKS
WITH
THE
LARGEST
DIAGONAL
COVARIANCE
MATRIX
ARE
REJECTED
THIS
GENERATES
AN
AVERAGE
OF
ABOUT
REGIONS
PER
FRAME
EACH
DESCRIPTOR
IS
A
VECTOR
AND
TO
SIMULTANEOUSLY
CLUSTER
ALL
THE
DESCRIPTORS
OF
THE
MOVIE
WOULD
BE
A
GARGAN
TUAN
TASK
INSTEAD
A
SUBSET
OF
SHOTS
IS
SELECTED
THESE
SHOTS
ARE
DISCUSSED
IN
MORE
DETAIL
IN
SECTION
COVER
ING
ABOUT
FRAMES
WHICH
REPRESENT
ABOUT
OF
ALL
THE
FRAMES
IN
THE
MOVIE
EVEN
WITH
THIS
REDUCTION
THERE
ARE
STILL
AVERAGED
TRACK
DESCRIPTORS
THAT
MUST
BE
CLUSTERED
TO
DETERMINE
THE
DISTANCE
FUNCTION
FOR
CLUSTERING
THE
MA
HALANOBIS
DISTANCE
IS
COMPUTED
AS
FOLLOWS
IT
IS
ASSUMED
THAT
THE
COVARIANCE
IS
THE
SAME
FOR
ALL
TRACKS
AND
THIS
IS
COMPUTED
BY
ESTIMATING
FROM
ALL
THE
AVAILABLE
DATA
I
E
ALL
DESCRIPTORS
FOR
ALL
TRACKS
IN
THE
SHOTS
THE
MAHA
LANOBIS
DISTANCE
ENABLES
THE
MORE
NOISY
COMPONENTS
OF
THE
VECTOR
TO
BE
WEIGHTED
DOWN
AND
ALSO
DECORRELATES
THE
COMPONENTS
EMPIRICALLY
THERE
IS
A
SMALL
DEGREE
OF
CORRELA
TION
THE
DISTANCE
FUNCTION
BETWEEN
TWO
DESCRIPTORS
REPRE
SENTED
BY
THEIR
MEAN
TRACK
DESCRIPTORS
X
X
IS
THEN
GIVEN
A
B
FIGURE
SAMPLES
FROM
THE
CLUSTERS
CORRESPONDING
TO
A
SINGLE
VI
SUAL
WORD
A
TWO
EXAMPLES
OF
CLUSTERS
OF
SHAPE
ADAPTED
REGIONS
B
TWO
EXAMPLES
OF
CLUSTERS
OF
MAXIMALLY
STABLE
REGIONS
OF
EACH
TYPE
THE
NUMBER
OF
CLUSTERS
IS
CHOSEN
EMPIRICALLY
TO
MAXIMIZE
RETRIEVAL
RESULTS
ON
THE
GROUND
TRUTH
SET
OF
SEC
TION
THE
K
MEANS
ALGORITHM
IS
RUN
SEVERAL
TIMES
WITH
RANDOM
INITIAL
ASSIGNMENTS
OF
POINTS
AS
CLUSTER
CENTRES
AND
THE
BEST
RESULT
USED
FIGURE
SHOWS
EXAMPLES
OF
REGIONS
BELONGING
TO
PAR
TICULAR
CLUSTERS
I
E
WHICH
WILL
BE
TREATED
AS
THE
SAME
VI
SUAL
WORD
THE
CLUSTERED
REGIONS
REFLECT
THE
PROPERTIES
OF
THE
SIFT
DESCRIPTORS
WHICH
PENALIZE
VARIATIONS
AMONGST
RE
GIONS
LESS
THAN
CROSS
CORRELATION
THIS
IS
BECAUSE
SIFT
EM
PHASIZES
ORIENTATION
OF
GRADIENTS
RATHER
THAN
THE
POSITION
OF
A
PARTICULAR
INTENSITY
WITHIN
THE
REGION
THE
REASON
THAT
SA
AND
MS
REGIONS
ARE
CLUSTERED
SEPA
RATELY
IS
THAT
THEY
COVER
DIFFERENT
AND
LARGELY
INDEPENDENT
REGIONS
OF
THE
SCENE
CONSEQUENTLY
THEY
MAY
BE
THOUGHT
OF
AS
DIFFERENT
VOCABULARIES
FOR
DESCRIBING
THE
SAME
SCENE
AND
THUS
SHOULD
HAVE
THEIR
OWN
WORD
SETS
IN
THE
SAME
WAY
AS
ONE
VOCABULARY
MIGHT
DESCRIBE
ARCHITECTURAL
FEATURES
AND
ANOTHER
THE
STATE
OF
REPAIR
OF
A
BUILDING
VISUAL
INDEXING
USING
TEXT
RETRIEVAL
METHODS
BY
D
X
X
X
X
X
X
AS
IS
STANDARD
THE
DESCRIPTOR
SPACE
IS
AFFINE
TRANSFORMED
BY
THE
SQUARE
ROOT
OF
SO
THAT
EUCLIDEAN
DISTANCE
MAY
BE
USED
ABOUT
CLUSTERS
ARE
USED
FOR
SHAPE
ADAPTED
REGIONS
AND
ABOUT
CLUSTERS
FOR
MAXIMALLY
STABLE
REGIONS
THE
RATIO
OF
THE
NUMBER
OF
CLUSTERS
FOR
EACH
TYPE
IS
CHOSEN
TO
BE
APPROXIMATELY
THE
SAME
AS
THE
RATIO
OF
DETECTED
DESCRIPTORS
IN
TEXT
RETRIEVAL
EACH
DOCUMENT
IS
REPRESENTED
BY
A
VECTOR
OF
WORD
FREQUENCIES
HOWEVER
IT
IS
USUAL
TO
APPLY
A
WEIGHTING
TO
THE
COMPONENTS
OF
THIS
VECTOR
RATHER
THAN
USE
THE
FRE
QUENCY
VECTOR
DIRECTLY
FOR
INDEXING
HERE
WE
DESCRIBE
THE
STANDARD
WEIGHTING
THAT
IS
EMPLOYED
AND
THEN
THE
VISUAL
ANALOGY
OF
DOCUMENT
RETRIEVAL
TO
FRAME
RETRIEVAL
THE
STANDARD
WEIGHTING
IS
KNOWN
AS
TERM
FREQUENCY
INVERSE
DOCUMENT
FREQUENCY
TF
IDF
AND
IS
COMPUTED
AS
FOLLOWS
SUPPOSE
THERE
IS
A
VOCABULARY
OF
K
WORDS
THEN
EACH
DOCUMENT
IS
REPRESENTED
BY
A
K
VECTOR
VD
TI
TK
OF
WEIGHTED
WORD
FREQUENCIES
WITH
COM
RANK
OF
THE
ITH
RELEVANT
IMAGE
IN
ESSENCE
RANK
IS
ZERO
IF
ALL
NREL
IMAGES
ARE
RETURNED
FIRST
THE
RANK
MEASURE
LIES
IN
THE
RANGE
TO
WITH
CORRESPONDING
TO
RANDOM
RETRIEVAL
GROUND
TRUTH
IMAGE
SET
RESULTS
PONENTS
TI
NID
LOG
N
FIGURE
SHOWS
THE
AVERAGE
NORMALIZED
RANK
USING
EACH
IMAGE
OF
THE
DATA
SET
AS
A
QUERY
IMAGE
WITH
THE
TF
IDF
WEIGHT
ND
NI
WHERE
NID
IS
THE
NUMBER
OF
OCCURRENCES
OF
WORD
I
IN
DOC
UMENT
D
ND
IS
THE
TOTAL
NUMBER
OF
WORDS
IN
THE
DOCUMENT
D
NI
IS
THE
NUMBER
OF
OCCURRENCES
OF
TERM
I
IN
THE
WHOLE
DATABASE
AND
N
IS
THE
NUMBER
OF
DOCUMENTS
IN
THE
WHOLE
DATABASE
THE
WEIGHTING
IS
A
PRODUCT
OF
TWO
TERMS
THE
WORD
FREQUENCY
NID
ND
AND
THE
INVERSE
DOCUMENT
FREQUENCY
LOG
N
NI
THE
INTUITION
IS
THAT
WORD
FREQUENCY
WEIGHTS
WORDS
OCCURRING
OFTEN
IN
A
PARTICULAR
DOCUMENT
AND
THUS
DE
SCRIBE
IT
WELL
WHILST
THE
INVERSE
DOCUMENT
FREQUENCY
DOWN
WEIGHTS
WORDS
THAT
APPEAR
OFTEN
IN
THE
DATABASE
AT
THE
RETRIEVAL
STAGE
DOCUMENTS
ARE
RANKED
BY
THEIR
NOR
MALIZED
SCALAR
PRODUCT
COSINE
OF
ANGLE
BETWEEN
THE
QUERY
VECTOR
VQ
AND
ALL
DOCUMENT
VECTORS
VD
IN
THE
DATABASE
IN
OUR
CASE
THE
QUERY
VECTOR
IS
GIVEN
BY
THE
VISUAL
WORDS
CONTAINED
IN
A
USER
SPECIFIED
SUB
PART
OF
A
FRAME
AND
THE
OTHER
FRAMES
ARE
RANKED
ACCORDING
TO
THE
SIMILARITY
OF
THEIR
WEIGHTED
VECTORS
TO
THIS
QUERY
VECTOR
VARIOUS
WEIGHTING
MODELS
ARE
EVALUATED
IN
THE
FOLLOWING
SECTION
EXPERIMENTAL
EVALUATION
OF
SCENE
MATCHING
USING
VISUAL
WORDS
HERE
THE
OBJECTIVE
IS
TO
MATCH
SCENE
LOCATIONS
WITHIN
A
CLOSED
WORLD
OF
SHOTS
THE
METHOD
IS
EVALUATED
ON
FRAMES
FROM
SHOTS
TAKEN
AT
DIFFERENT
LOCATIONS
IN
THE
MOVIE
RUN
LOLA
RUN
WE
HAVE
BETWEEN
FRAMES
FROM
EACH
LOCATION
EXAMPLES
OF
THREE
FRAMES
FROM
EACH
OF
FOUR
DIFFERENT
LOCATIONS
ARE
SHOWN
IN
FIGURE
THERE
ARE
SIGNIF
ICANT
VIEWPOINT
CHANGES
OVER
THE
TRIPLETS
OF
FRAMES
SHOWN
FOR
THE
SAME
LOCATION
EACH
FRAME
OF
THE
TRIPLET
IS
FROM
A
DIFFERENT
AND
DISTANT
IN
TIME
SHOT
IN
THE
MOVIE
IN
THE
RETRIEVAL
TESTS
THE
ENTIRE
FRAME
IS
USED
AS
A
QUERY
REGION
THE
RETRIEVAL
PERFORMANCE
IS
MEASURED
OVER
ALL
FRAMES
USING
EACH
IN
TURN
AS
A
QUERY
REGION
THE
CORRECT
RE
TRIEVAL
CONSISTS
OF
ALL
THE
OTHER
FRAMES
WHICH
SHOW
THE
SAME
LOCATION
AND
THIS
GROUND
TRUTH
IS
DETERMINED
BY
HAND
FOR
THE
COMPLETE
FRAME
SET
THE
RETRIEVAL
PERFORMANCE
IS
MEASURED
USING
THE
AVERAGE
NORMALIZED
RANK
OF
RELEVANT
IMAGES
GIVEN
BY
ING
DESCRIBED
IN
SECTION
THE
BENEFIT
IN
HAVING
TWO
FEATURE
TYPES
IS
EVIDENT
THE
COMBINATION
OF
BOTH
CLEARLY
GIVES
BET
TER
PERFORMANCE
THAN
EITHER
ONE
ALONE
THE
PERFORMANCE
OF
EACH
FEATURE
TYPE
VARIES
FOR
DIFFERENT
FRAMES
OR
LOCATIONS
FOR
EXAMPLE
IN
FRAMES
MS
REGIONS
PERFORM
BETTER
AND
CONVERSELY
FOR
FRAMES
SA
REGIONS
ARE
SUPERIOR
THE
RETRIEVAL
RANKING
IS
PERFECT
FOR
OF
THE
LOCATIONS
EVEN
THOSE
WITH
SIGNIFICANT
VIEWPOINT
CHANGES
THE
RANKING
RESULTS
ARE
LESS
IMPRESSIVE
FOR
IMAGES
AND
THOUGH
EVEN
IN
THESE
CASES
THE
FRAME
MATCHES
ARE
NOT
MISSED
JUST
LOW
RANKED
THIS
IS
DUE
TO
A
LACK
OF
REGIONS
IN
THE
OVER
LAPPING
PART
OF
THE
SCENE
SEE
FIGURE
THIS
IS
NOT
A
PROBLEM
OF
VECTOR
QUANTIZATION
THE
REGIONS
THAT
ARE
IN
COMMON
ARE
CORRECTLY
MATCHED
BUT
DUE
TO
FEW
FEATURES
BEING
DETECTED
FOR
THIS
TYPE
OF
SCENE
PAVEMENT
TEXTURE
WE
RETURN
TO
THIS
POINT
IN
SECTION
TABLE
SHOWS
THE
MEAN
OF
THE
RANK
MEASURE
COMPUTED
FROM
ALL
IMAGES
FOR
THREE
STANDARD
TEXT
RETRIEVAL
TERM
WEIGHTING
METHODS
THE
TF
IDF
WEIGHTING
OUTPERFORMS
BOTH
THE
BINARY
WEIGHTS
I
E
THE
VECTOR
COMPONENTS
ARE
ONE
IF
THE
IMAGE
CONTAINS
THE
DESCRIPTOR
ZERO
OTHERWISE
AND
TERM
FREQUENCY
WEIGHTS
THE
COMPONENTS
ARE
THE
FREQUENCY
OF
WORD
OCCURRENCE
THE
DIFFERENCES
ARE
NOT
VERY
SIGNIFI
CANT
FOR
THE
RANKS
AVERAGED
OVER
THE
WHOLE
GROUND
TRUTH
SET
HOWEVER
FOR
PARTICULAR
FRAMES
E
G
THE
DIFFERENCE
CAN
BE
AS
HIGH
AS
THE
AVERAGE
PRECISION
RECALL
CURVE
FOR
ALL
FRAMES
IS
SHOWN
IN
FIGURE
FOR
EACH
FRAME
AS
A
QUERY
WE
HAVE
COMPUTED
PRECISION
AS
THE
NUMBER
OF
RELEVANT
IMAGES
I
E
OF
THE
SAME
LOCATION
RELATIVE
TO
THE
TOTAL
NUMBER
OF
FRAMES
RETRIEVED
AND
RECALL
AS
THE
NUMBER
OF
CORRECTLY
RETRIEVED
FRAMES
RELATIVE
TO
THE
NUMBER
OF
RELEVANT
FRAMES
AGAIN
THE
BENEFIT
OF
COMBINING
THE
TWO
FEATURE
TYPES
IS
CLEAR
THESE
RETRIEVAL
RESULTS
DEMONSTRATE
THAT
THERE
IS
NO
LOSS
OF
PERFORMANCE
IN
USING
VECTOR
QUANTIZATION
VISUAL
WORDS
COMPARED
TO
DIRECT
NEAREST
NEIGHBOUR
OR
NEAREST
NEIGH
BOUR
MATCHING
OF
INVARIANTS
THIS
GROUND
TRUTH
SET
IS
ALSO
USED
TO
LEARN
THE
SYSTEM
PA
RAMETERS
INCLUDING
THE
NUMBER
OF
CLUSTER
CENTRES
THE
MINI
MUM
TRACKING
LENGTH
FOR
STABLE
FEATURES
AND
THE
PROPORTION
RANK
NNREL
NREL
RI
I
NREL
NREL
OF
UNSTABLE
DESCRIPTORS
TO
REJECT
BASED
ON
THEIR
COVARIANCE
OBJECT
RETRIEVAL
WHERE
NREL
IS
THE
NUMBER
OF
RELEVANT
IMAGES
FOR
PARTICULAR
QUERY
IMAGE
N
IS
THE
SIZE
OF
THE
IMAGE
SET
AND
RI
IS
THE
IN
THIS
SECTION
WE
EVALUATE
SEARCHING
FOR
OBJECTS
THROUGHOUT
THE
ENTIRE
MOVIE
THE
OBJECT
OF
INTEREST
IS
SPECIFIED
BY
THE
TABLE
THE
MEAN
OF
THE
RANK
MEASURE
COMPUTED
FROM
ALL
IMAGES
OF
THE
GROUND
TRUTH
SET
FOR
DIFFERENT
TERM
WEIGHTING
METH
ODS
A
AVERAGE
NORMALIZED
RANK
OF
RELEVANT
FRAMES
SHAPE
ADAPTED
MAXIMALLY
STABLE
SHAPE
ADAPTED
MAXIMALLY
STABLE
FIGURE
TOP
FRAMES
AND
FROM
THE
GROUND
TRUTH
DATA
SET
A
POOR
RANKING
SCORE
IS
OBTAINED
FOR
THIS
PAIR
BOTTOM
SUPERIMPOSED
DETECTED
AFFINE
INVARIANT
REGIONS
THE
CAREFUL
READER
WILL
NOTE
THAT
DUE
TO
THE
VERY
DIFFERENT
VIEWPOINTS
ONLY
TWO
OF
THE
LEFT
AND
RIGHT
REGIONS
CORRESPOND
BETWEEN
FRAMES
IMAGE
NUMBER
B
AVERAGE
PRECISION
RECALL
CURVE
USER
AS
A
SUB
PART
OF
ANY
FRAME
A
FEATURE
LENGTH
FILM
TYPICALLY
HAS
FRAMES
TO
REDUCE
COMPLEXITY
ONE
KEYFRAME
IS
USED
PER
SECOND
OF
THE
VIDEO
DESCRIPTORS
ARE
COMPUTED
FOR
STABLE
REGIONS
IN
EACH
KEYFRAME
AND
THE
MEAN
VALUES
ARE
COMPUTED
USING
TWO
FRAMES
EITHER
SIDE
OF
THE
KEYFRAME
THE
DESCRIPTORS
ARE
VEC
TOR
QUANTIZED
USING
THE
CENTRES
CLUSTERED
FROM
THE
GROUND
TRUTH
SET
HERE
WE
ARE
ALSO
EVALUATING
THE
EXPRESSIVENESS
OF
THE
VI
SUAL
VOCABULARY
SINCE
FRAMES
OUTSIDE
THE
GROUND
TRUTH
SET
CONTAIN
NEW
OBJECTS
AND
SCENES
AND
THEIR
DETECTED
REGIONS
HAVE
NOT
BEEN
INCLUDED
IN
FORMING
THE
CLUSTERS
RECALL
C
FIGURE
GROUND
TRUTH
DATA
A
EACH
ROW
SHOWS
A
FRAME
FROM
THREE
DIFFERENT
SHOTS
OF
THE
SAME
LOCATION
IN
THE
GROUND
TRUTH
DATA
SET
B
AVERAGE
NORMALIZED
RANK
FOR
LOCATION
MATCHING
ON
THE
GROUND
TRUTH
SET
C
AVERAGE
PRECISION
RECALL
CURVE
FOR
LOCATION
MATCHING
ON
THE
GROUND
TRUTH
SET
STOP
LIST
USING
A
STOP
LIST
ANALOGY
THE
MOST
FREQUENT
VISUAL
WORDS
THAT
OCCUR
IN
ALMOST
ALL
IMAGES
ARE
SUPPRESSED
FIGURE
SHOWS
THE
FREQUENCY
OF
VISUAL
WORDS
OVER
ALL
THE
KEYFRAMES
OF
LOLA
THE
TOP
AND
BOTTOM
ARE
STOPPED
IN
OUR
CASE
THE
VERY
COMMON
WORDS
ARE
DUE
TO
LARGE
CLUSTERS
OF
OVER
POINTS
THE
STOP
LIST
BOUNDARIES
WERE
DETERMINED
EMPIRICALLY
TO
REDUCE
THE
NUMBER
OF
MISMATCHES
AND
SIZE
OF
THE
INVERTED
FILE
WHILE
KEEPING
SUFFICIENT
VISUAL
VOCABULARY
FIGURES
SHOW
THE
BENEFIT
OF
IMPOSING
A
STOP
LIST
THE
VERY
COMMON
VISUAL
WORDS
OCCUR
AT
MANY
PLACES
IN
THE
IM
AGE
AND
ARE
RESPONSIBLE
FOR
MIS
MATCHES
MOST
OF
THESE
ARE
REMOVED
ONCE
THE
STOP
LIST
IS
APPLIED
THE
REMOVAL
OF
THE
REMAINING
MIS
MATCHES
IS
DESCRIBED
NEXT
SPATIAL
CONSISTENCY
FREQUENCY
OF
VISUAL
WORDS
OVER
ALL
KEYFRAMES
FREQUENCY
OF
VISUAL
WORDS
OVER
ALL
KEYFRAMES
GOOGLE
INCREASES
THE
RANKING
FOR
DOCUMENTS
WHERE
THE
WORD
RANK
SORTED
BY
FREQUENCY
WORD
RANK
SORTED
BY
FREQUENCY
SEARCHED
FOR
WORDS
APPEAR
CLOSE
TOGETHER
IN
THE
RETRIEVED
TEXTS
MEASURED
BY
WORD
ORDER
THIS
ANALOGY
IS
ESPECIALLY
RELEVANT
FOR
QUERYING
OBJECTS
BY
A
SUBPART
OF
THE
IMAGE
WHERE
MATCHED
COVARIANT
REGIONS
IN
THE
RETRIEVED
FRAMES
SHOULD
HAVE
A
SIMILAR
SPATIAL
ARRANGEMENT
E
G
COMPACTNESS
TO
THOSE
OF
THE
OUTLINED
REGION
IN
THE
QUERY
IMAGE
THE
IDEA
IS
IMPLEMENTED
HERE
BY
FIRST
RETRIEVING
FRAMES
USING
THE
WEIGHTED
FREQUENCY
VECTOR
ALONE
AND
THEN
RE
RANKING
THEM
BASED
ON
A
MEASURE
OF
SPATIAL
CONSISTENCY
SPATIAL
CONSISTENCY
CAN
BE
MEASURED
QUITE
LOOSELY
SIM
PLY
BY
REQUIRING
THAT
NEIGHBOURING
MATCHES
IN
THE
QUERY
RE
GION
LIE
IN
A
SURROUNDING
AREA
IN
THE
RETRIEVED
FRAME
IT
CAN
ALSO
BE
MEASURED
VERY
STRICTLY
BY
REQUIRING
THAT
NEIGHBOUR
ING
MATCHES
HAVE
THE
SAME
SPATIAL
LAYOUT
IN
THE
QUERY
RE
GION
AND
RETRIEVED
FRAME
IN
OUR
CASE
THE
MATCHED
REGIONS
PROVIDE
THE
AFFINE
TRANSFORMATION
BETWEEN
THE
QUERY
AND
RE
TRIEVED
IMAGE
SO
A
POINT
TO
POINT
MAP
IS
AVAILABLE
FOR
THIS
STRICT
MEASURE
WE
HAVE
FOUND
THAT
THE
BEST
PERFORMANCE
IS
OBTAINED
IN
THE
MIDDLE
OF
THIS
POSSIBLE
RANGE
OF
MEASURES
A
SEARCH
AREA
IS
DEFINED
BY
THE
NEAREST
NEIGHBOURS
OF
EACH
MATCH
AND
EACH
REGION
WHICH
ALSO
MATCHES
WITHIN
THIS
AREA
CASTS
A
VOTE
FOR
THAT
FRAME
MATCHES
WITH
NO
SUPPORT
ARE
REJECTED
THE
TOTAL
NUMBER
OF
VOTES
DETERMINES
THE
RANK
OF
THE
FRAME
THIS
WORKS
VERY
WELL
AS
IS
DEMONSTRATED
IN
THE
LAST
ROW
OF
FIGURE
WHICH
SHOWS
THE
SPATIAL
CONSISTENCY
REJECTION
OF
IN
CORRECT
MATCHES
THE
OBJECT
RETRIEVAL
EXAMPLES
OF
FIGURES
TO
EMPLOY
THIS
RANKING
MEASURE
AND
AMPLY
DEMONSTRATE
ITS
USEFULNESS
OTHER
MEASURES
WHICH
TAKE
ACCOUNT
OF
THE
AFFINE
MAP
PING
BETWEEN
IMAGES
MAY
BE
REQUIRED
IN
SOME
SITUATIONS
BUT
THIS
INVOLVES
A
GREATER
COMPUTATIONAL
EXPENSE
OBJECT
RETRIEVAL
IMPLEMENTATION
USE
OF
INVERTED
FILES
IN
A
CLASSICAL
FILE
STRUCTURE
ALL
WORDS
ARE
STORED
IN
THE
DOCUMENT
THEY
APPEAR
IN
AN
INVERTED
FILE
STRUCTURE
HAS
AN
ENTRY
HIT
LIST
FOR
EACH
WORD
WHERE
ALL
OCCURRENCES
OF
THE
WORD
IN
ALL
DOCUMENTS
ARE
STORED
IN
OUR
CASE
THE
INVERTED
FILE
HAS
AN
ENTRY
FOR
EACH
VISUAL
WORD
WHICH
STORES
ALL
THE
MATCHES
I
E
OCCURRENCES
OF
THE
SAME
WORD
IN
ALL
FRAMES
THE
DOCUMENT
VECTOR
IS
VERY
SPARSE
AND
USE
OF
AN
INVERTED
FILE
MAKES
THE
RETRIEVAL
VERY
FAST
QUERYING
A
DATABASE
OF
FRAMES
TAKES
ABOUT
SECOND
WITH
A
MATLAB
IMPLEMENTATION
ON
A
PENTIUM
B
FIGURE
FREQUENCY
OF
MS
VISUAL
WORDS
AMONG
ALL
KEYFRAMES
OF
RUN
LOLA
RUN
A
BEFORE
AND
B
AFTER
APPLICATION
OF
A
STOPLIST
FIGURE
MATCHING
STAGES
TOP
ROW
LEFT
QUERY
REGION
AND
RIGHT
ITS
CLOSE
UP
SECOND
ROW
ORIGINAL
WORD
MATCHES
THIRD
ROW
MATCHES
AFTER
USING
STOP
LIST
LAST
ROW
FINAL
SET
OF
MATCHES
AFTER
FILTERING
ON
SPATIAL
CONSISTENCY
EXAMPLE
QUERIES
FIGURES
AND
SHOW
RESULTS
OF
TWO
OBJECT
QUERIES
FOR
THE
MOVIE
RUN
LOLA
RUN
AND
FIGURE
SHOWS
THE
RESULT
OF
AN
OBJECT
QUERY
ON
THE
FILM
GROUND
HOG
DAY
BOTH
MOVIES
CONTAIN
ABOUT
KEYFRAMES
BOTH
THE
ACTUAL
FRAMES
RETURNED
AND
THEIR
RANKING
ARE
EXCELLENT
AS
FAR
AS
IT
IS
POSSIBLE
TO
TELL
NO
FRAMES
CONTAINING
THE
OB
JECT
ARE
MISSED
NO
FALSE
NEGATIVES
AND
THE
HIGHLY
RANKED
FRAMES
ALL
DO
CONTAIN
THE
OBJECT
GOOD
PRECISION
THE
OBJECT
QUERY
RESULTS
DO
DEMONSTRATE
THE
EXPRESSIVE
POWER
OF
THE
VISUAL
VOCABULARY
THE
VISUAL
WORDS
LEARNT
FOR
LOLA
ARE
USED
UNCHANGED
FOR
THE
GROUNDHOG
DAY
RETRIEVAL
SUMMARY
AND
CONCLUSIONS
THE
ANALOGY
WITH
TEXT
RETRIEVAL
REALLY
HAS
DEMONSTRATED
ITS
WORTH
WE
HAVE
IMMEDIATE
RUN
TIME
OBJECT
RETRIEVAL
THROUGHOUT
A
MOVIE
DATABASE
DESPITE
SIGNIFICANT
VIEWPOINT
CHANGES
IN
MANY
FRAMES
THE
OBJECT
IS
SPECIFIED
AS
A
SUB
PART
OF
AN
IMAGE
AND
THIS
HAS
PROVED
SUFFICIENT
FOR
QUASI
PLANAR
RIGID
OBJECTS
THERE
ARE
OF
COURSE
IMPROVEMENTS
THAT
CAN
BE
MADE
MAINLY
TO
OVERCOME
PROBLEMS
IN
THE
VISUAL
PROCESSING
LOW
RANKINGS
ARE
CURRENTLY
DUE
TO
A
LACK
OF
VISUAL
DESCRIPTORS
FOR
SOME
SCENE
TYPES
HOWEVER
THE
FRAMEWORK
ALLOWS
OTHER
EX
ISTING
AFFINE
CO
VARIANT
REGIONS
TO
BE
ADDED
THEY
WILL
DEFINE
AN
EXTENDED
VISUAL
VOCABULARY
FOR
EXAMPLE
THOSE
OF
ANOTHER
IMPROVEMENT
WOULD
BE
TO
DEFINE
THE
OBJECT
OF
IN
TEREST
OVER
MORE
THAN
A
SINGLE
FRAME
TO
ALLOW
FOR
SEARCH
ON
ALL
ITS
VISUAL
ASPECTS
THE
TEXT
RETRIEVAL
ANALOGY
ALSO
RAISES
INTERESTING
QUES
TIONS
FOR
FUTURE
WORK
IN
TEXT
RETRIEVAL
SYSTEMS
THE
TEX
TUAL
VOCABULARY
IS
NOT
STATIC
GROWING
AS
NEW
DOCUMENTS
ARE
ADDED
TO
THE
COLLECTION
SIMILARLY
WE
DO
NOT
CLAIM
THAT
OUR
VECTOR
QUANTIZATION
IS
UNIVERSAL
FOR
ALL
IMAGES
SO
FAR
WE
HAVE
LEARNT
VECTOR
QUANTIZATIONS
SUFFICIENT
FOR
TWO
MOVIES
BUT
WAYS
OF
UPGRADING
THE
VISUAL
VOCABULARY
WILL
NEED
TO
BE
FOUND
ONE
COULD
THINK
OF
LEARNING
VISUAL
VOCABULARIES
FOR
DIFFERENT
SCENE
TYPES
E
G
CITY
SCAPE
VS
A
FOREST
FINALLY
WE
NOW
HAVE
THE
INTRIGUING
POSSIBILITY
OF
FOLLOW
ING
OTHER
SUCCESSES
OF
THE
TEXT
RETRIEVAL
COMMUNITY
SUCH
AS
LATENT
SEMANTIC
INDEXING
TO
FIND
CONTENT
AND
AUTOMATIC
CLUS
TERING
TO
FIND
THE
PRINCIPAL
OBJECTS
THAT
OCCUR
THROUGHOUT
THE
MOVIE
ACKNOWLEDGEMENTS
WE
ARE
GRATEFUL
TO
DAVID
LOWE
JIRI
MATAS
KRYSTIAN
MIKOLAJCZYK
AND
FREDERIK
SCHAFFALITZKY
FOR
SUP
PLYING
THEIR
REGION
DETECTOR
DESCRIPTOR
CODES
THANKS
TO
ANDREW
BLAKE
MARK
EVERINGHAM
ANDREW
FITZGIBBON
KRYSTIAN
MIKOLA
JCZYK
AND
FREDERIK
SCHAFFALITZKY
FOR
FRUITFUL
DISCUSSIONS
THIS
WORK
WAS
FUNDED
BY
EC
PROJECT
VIBES
FIGURE
OBJECT
QUERY
EXAMPLE
I
FIRST
ROW
LEFT
FRAME
WITH
USER
SPECIFIED
QUERY
REGION
A
POSTER
IN
YELLOW
AND
RIGHT
CLOSE
UP
OF
THE
QUERY
REGION
THE
FOUR
REMAINING
ROWS
SHOW
LEFT
THE
AND
RETRIEVED
FRAMES
WITH
THE
IDENTIFIED
RE
GION
OF
INTEREST
SHOWN
IN
YELLOW
AND
RIGHT
A
CLOSE
UP
OF
THE
IM
AGE
WITH
MATCHED
ELLIPTICAL
REGIONS
SUPERIMPOSED
IN
THIS
CASE
KEYFRAMES
WERE
RETRIEVED
SIX
FROM
THE
SAME
SHOT
AS
THE
QUERY
IMAGE
THE
REST
FROM
DIFFERENT
SHOTS
AT
LATER
POINTS
IN
THE
MOVIE
ALL
RETRIEVED
FRAMES
CONTAIN
THE
SPECIFIED
OBJECT
NOTE
THE
POSTER
APPEARS
ON
VARIOUS
BILLBOARDS
THROUGHOUT
THE
MOVIE
AND
BERLIN
FIGURE
OBJECT
QUERY
EXAMPLE
II
RUN
LOLA
RUN
FIRST
ROW
LEFT
QUERY
REGION
AND
RIGHT
ITS
CLOSE
UP
NEXT
ROWS
THE
AND
RETRIEVED
FRAMES
LEFT
AND
OBJECT
CLOSE
UPS
RIGHT
WITH
MATCHED
REGIONS
KEYFRAMES
WERE
RETRIEVED
CONTAINED
THE
OBJECT
THE
TWO
INCORRECT
FRAMES
WERE
RANKED
AND
FIGURE
OBJECT
QUERY
EXAMPLE
III
GROUNDHOG
DAY
FIRST
ROW
LEFT
QUERY
REGION
AND
RIGHT
ITS
CLOSE
UP
NEXT
ROWS
THE
AND
RETRIEVED
FRAMES
LEFT
AND
OBJECT
CLOSE
UPS
WITH
MATCHED
REGIONS
RIGHT
KEYFRAMES
WERE
RETRIEVED
OF
WHICH
CONTAINED
THE
OBJECT
THE
FIRST
INCORRECT
FRAME
WAS
RANKED
STYLE
AWARE
MID
LEVEL
REPRESENTATION
FOR
DISCOVERING
VISUAL
CONNECTIONS
IN
SPACE
AND
TIME
YONG
JAE
LEE
ALEXEI
A
EFROS
AND
MARTIAL
HEBERT
ROBOTICS
INSTITUTE
CARNEGIE
MELLON
UNIVERSITY
EFROS
HEBERT
CS
CMU
EDU
ABSTRACT
WE
PRESENT
A
WEAKLY
SUPERVISED
VISUAL
DATA
MINING
AP
PROACH
THAT
DISCOVERS
CONNECTIONS
BETWEEN
RECURRING
MID
LEVEL
VISUAL
ELEMENTS
IN
HISTORIC
TEMPORAL
AND
GEOGRAPHIC
SPATIAL
IMAGE
COLLECTIONS
AND
ATTEMPTS
TO
CAPTURE
THE
UN
DERLYING
VISUAL
STYLE
IN
CONTRAST
TO
EXISTING
DISCOVERY
METH
ODS
THAT
MINE
FOR
PATTERNS
THAT
REMAIN
VISUALLY
CONSISTENT
THROUGHOUT
THE
DATASET
OUR
GOAL
IS
TO
DISCOVER
VISUAL
ELE
MENTS
WHOSE
APPEARANCE
CHANGES
DUE
TO
CHANGE
IN
TIME
OR
LOCATION
I
E
EXHIBIT
CONSISTENT
STYLISTIC
VARIATIONS
ACROSS
THE
LABEL
SPACE
DATE
OR
GEO
LOCATION
TO
DISCOVER
THESE
ELEMENTS
WE
FIRST
IDENTIFY
GROUPS
OF
PATCHES
THAT
ARE
STYLE
SENSITIVE
WE
THEN
INCREMENTALLY
BUILD
CORRESPONDENCES
TO
FIND
THE
SAME
ELEMENT
ACROSS
THE
ENTIRE
DATASET
FINALLY
WE
TRAIN
STYLE
AWARE
REGRESSORS
THAT
MODEL
EACH
ELEMENT
RANGE
OF
STYLISTIC
DIFFERENCES
WE
APPLY
OUR
APPROACH
TO
DATE
AND
GEO
LOCATION
PREDICTION
AND
SHOW
SUBSTANTIAL
IMPROVE
MENT
OVER
SEVERAL
BASELINES
THAT
DO
NOT
MODEL
VISUAL
STYLE
WE
ALSO
DEMONSTRATE
THE
METHOD
EFFECTIVENESS
ON
THE
RE
LATED
TASK
OF
FINE
GRAINED
CLASSIFICATION
INTRODUCTION
LEARN
HOW
TO
SEE
REALIZE
THAT
EVERYTHING
CONNECTS
TO
EVERYTHING
ELSE
LEONARDO
DA
VINCI
LONG
BEFORE
THE
AGE
OF
DATA
MINING
HISTORIANS
GEOG
RAPHERS
ANTHROPOLOGISTS
AND
PALEONTOLOGISTS
HAVE
BEEN
DIS
COVERING
AND
ANALYZING
PATTERNS
IN
DATA
ONE
OF
THEIR
MAIN
MOTIVATIONS
IS
FINDING
PATTERNS
THAT
CORRELATE
WITH
SPATIAL
GEOGRAPHICAL
AND
OR
TEMPORAL
HISTORICAL
INFORMATION
AL
LOWING
THEM
TO
ADDRESS
TWO
CRUCIAL
QUESTIONS
WHERE
GEO
LOCALIZATION
AND
WHEN
HISTORICAL
DATING
INTERESTINGLY
MANY
SUCH
PATTERNS
BE
IT
THE
SHAPE
OF
THE
HANDLE
ON
AN
ETRUSCAN
VASE
OR
THE
PATTERN
OF
BARK
OF
A
NORWEGIAN
PINE
ARE
PREDOMINANTLY
VISUAL
THE
RECENT
EXPLOSION
IN
THE
SHEER
VOLUME
OF
VISUAL
INFORMATION
THAT
HUMANITY
HAS
BEEN
CAP
TURING
POSES
BOTH
A
CHALLENGE
IT
IMPOSSIBLE
TO
GO
THROUGH
BY
HAND
AND
AN
OPPORTUNITY
DISCOVERING
THINGS
THAT
WOULD
NEVER
HAVE
BEEN
NOTICED
BEFORE
FOR
THESE
FIELDS
IN
THIS
NOW
WITH
THE
EECS
DEPARTMENT
AT
UC
BERKELEY
FIGURE
GIVEN
HISTORIC
CAR
IMAGES
OUR
ALGORITHM
IS
NOT
ONLY
ABLE
TO
AUTOMATICALLY
DISCOVER
CORRESPONDING
VISUAL
ELEMENTS
E
G
YEL
LOW
GREEN
BOXES
DESPITE
THE
LARGE
VISUAL
VARIATIONS
BUT
CAN
MODEL
THESE
VARIATIONS
TO
CAPTURE
THE
CHANGES
IN
VISUAL
STYLE
ACROSS
TIME
WORK
WE
TAKE
THE
FIRST
STEPS
IN
CONSIDERING
TEMPORALLY
AS
WELL
AS
SPATIALLY
VARYING
VISUAL
DATA
AND
DEVELOPING
A
METHOD
FOR
AUTOMATICALLY
DISCOVERING
VISUAL
PATTERNS
THAT
CORRELATE
WITH
TIME
AND
SPACE
OF
COURSE
FINDING
RECURRING
VISUAL
PATTERNS
IN
DATA
UN
DERLIES
MUCH
OF
MODERN
COMPUTER
VISION
ITSELF
IT
IS
WHAT
CONNECTS
THE
DISPARATE
FRAGMENTS
OF
OUR
VISUAL
WORLD
INTO
A
COHERENT
NARRATIVE
AT
THE
LOW
LEVEL
THIS
IS
TYPICALLY
DONE
VIA
SIMPLE
UNSUPERVISED
CLUSTERING
E
G
K
MEANS
IN
VISUAL
WORDS
BUT
CLUSTERING
VISUAL
PATTERNS
THAT
ARE
MORE
COMPLEX
THAN
SIMPLE
BLOBS
CORNERS
AND
ORIENTED
BARS
TURNS
OUT
TO
BE
RATHER
DIFFICULT
BECAUSE
EVERYTHING
BE
COMES
MORE
DISSIMILAR
IN
HIGHER
DIMENSIONS
THE
EMERG
ING
SUBFIELD
OF
VISUAL
CATEGORY
DISCOVERY
VISUAL
DATA
MIN
ING
PROPOSES
WAYS
TO
ADDRESS
THIS
ISSUE
MOST
SUCH
APPROACHES
LOOK
FOR
TIGHT
CLUMPS
IN
THE
DATA
DISCOVERING
VISUAL
PATTERNS
THAT
STAY
GLOBALLY
CON
SISTENT
THROUGHOUT
THE
DATASET
MORE
RECENT
DISCRIMINATIVE
METHODS
SUCH
AS
TAKE
ADVANTAGE
OF
WEAK
SUPERVI
SION
TO
DIVIDE
THE
DATASET
INTO
DISCRETE
SUBSETS
E
G
KITCHEN
VS
BATHROOM
PARIS
VS
NOT
PARIS
TO
DISCOVER
SPECIFIC
VISUAL
PATTERNS
THAT
REPEATEDLY
OCCUR
IN
ONE
SUBSET
WHILE
NOT
OCCURRING
IN
OTHERS
BUT
IN
ADDITION
TO
THE
GLOBALLY
CONSISTENT
VISUAL
PATTERNS
E
G
THE
PEPSI
LOGO
IS
EXACTLY
THE
SAME
ALL
OVER
THE
WORLD
AND
THE
SPECIFIC
ONES
E
G
TOILETS
ARE
ONLY
FOUND
IN
BATH
ROOMS
MUCH
IN
OUR
VISUAL
WORLD
IS
NEITHER
GLOBAL
NOR
SPE
CIFIC
BUT
RATHER
UNDERGOES
A
GRADUAL
VISUAL
CHANGE
THIS
IS
NOWHERE
MORE
EVIDENT
THAN
IN
THE
VISUAL
CHANGES
ACROSS
LARGE
EXTENTS
OF
SPACE
GEOGRAPHY
AND
TIME
HISTORY
CON
SIDER
THE
THREE
CARS
SHOWN
IN
FIGURE
ONE
ANTIQUE
ONE
CLASSIC
AND
ONE
FROM
THE
ALTHOUGH
THESE
CARS
ARE
QUITE
DIFFERENT
VISUALLY
THEY
CLEARLY
SHARE
SOME
COMMON
ELEMENTS
E
G
A
HEADLIGHT
OR
A
WHEEL
BUT
NOTICE
THAT
EVEN
THESE
COMMON
ELEMENTS
DIFFER
SUBSTANTIALLY
IN
THEIR
AP
PEARANCE
ACROSS
THE
THREE
CAR
TYPES
MAKING
THIS
A
VERY
CHAL
LENGING
CORRESPONDENCE
PROBLEM
NOTICE
FURTHER
THAT
THE
WAY
IN
WHICH
THEY
DIFFER
IS
NOT
MERELY
RANDOM
I
E
A
STATIS
TICAL
NOISE
TERM
RATHER
THESE
SUBTLE
YET
CONSISTENT
DIF
FERENCES
CURVY
VS
BOXY
HOOD
THE
LENGTH
OF
THE
LEDGE
UNDER
THE
DOOR
ETC
TEND
TO
REFLECT
THE
PARTICULAR
VISUAL
STYLE
THAT
IS
BOTH
SPECIFIC
TO
AN
ERA
YET
CHANGING
GRADUALLY
OVER
TIME
FIGURE
IF
NOW
WE
WERE
GIVEN
A
PHOTO
OF
A
DIFFERENT
CAR
AND
ASKED
TO
ESTIMATE
ITS
MODEL
YEAR
WE
WOULD
NOT
ONLY
NEED
TO
DETECT
THE
COMMON
VISUAL
ELEMENTS
ON
THE
NEW
CAR
BUT
ALSO
UNDERSTAND
WHAT
ITS
STYLISTIC
DIFFERENCES
E
G
THE
LENGTH
OF
THAT
LEDGE
TELL
US
ABOUT
ITS
AGE
IN
THIS
PAPER
WE
PROPOSE
A
METHOD
FOR
DISCOVERING
CONNECTIONS
BETWEEN
SIMILAR
MID
LEVEL
VISUAL
ELEMENTS
IN
TEMPORALLY
AND
SPATIALLY
VARYING
DATASETS
AND
MODELING
THEIR
VISUAL
STYLE
HERE
WE
DEFINE
VISUAL
STYLE
AS
APPEAR
ANCE
VARIATIONS
OF
THE
SAME
VISUAL
ELEMENT
DUE
TO
CHANGE
IN
TIME
OR
LOCATION
OUR
CENTRAL
IDEA
IS
TO
CREATE
RELI
ABLE
GENERIC
VISUAL
ELEMENT
DETECTORS
THAT
FIRE
ACROSS
THE
ENTIRE
DATASET
INDEPENDENT
OF
STYLE
AND
THEN
MODEL
THEIR
STYLE
SPECIFIC
DIFFERENCES
USING
WEAKLY
SUPERVISED
IMAGE
LA
BELS
DATE
GEO
LOCATION
ETC
THE
REASON
FOR
DOING
THE
FIRST
STEP
IS
THAT
EACH
GENERIC
DETECTOR
PUTS
ALL
OF
ITS
DETECTIONS
INTO
CORRESPONDENCE
LOWER
RIGHT
IN
FIGURE
CREATING
A
CLOSED
WORLD
FOCUSED
ON
ONE
VISUAL
THEME
WHERE
IT
IS
MUCH
EASIER
TO
SUBTRACT
AWAY
THE
COMMONALITIES
AND
FO
CUS
ON
THE
STYLISTIC
DIFFERENCES
FURTHERMORE
WITHOUT
CON
DITIONING
ON
THE
GENERIC
DETECTOR
IT
WOULD
BE
VERY
DIFFICULT
TO
EVEN
DETECT
THE
STYLISTICALLY
INFORMATIVE
FEATURES
FOR
IN
STANCE
THE
LEDGE
IN
FIGURE
GREEN
BOX
IS
SO
TINY
THAT
IT
IS
UNLIKELY
TO
BE
DETECTABLE
IN
ISOLATION
BUT
IN
COMBINATION
WITH
THE
WHEEL
AND
PART
OF
THE
DOOR
THE
GENERIC
PART
IT
BECOMES
HIGHLY
DISCRIMINABLE
WE
EVALUATE
OUR
METHOD
ON
THE
TASK
OF
DATE
AND
GEO
LOCATION
PREDICTION
IN
THREE
SCENARIOS
TWO
HISTORIC
CAR
DATASETS
WITH
MODEL
YEAR
ANNOTATIONS
AND
A
STREET
VIEW
IM
AGERY
DATASET
ANNOTATED
WITH
GPS
COORDINATES
WE
SHOW
THAT
OUR
METHOD
OUTPERFORMS
SEVERAL
BASELINES
WHICH
DO
NOT
EXPLICITLY
MODEL
VISUAL
STYLE
MOREOVER
WE
ALSO
DEMON
STRATE
HOW
OUR
APPROACH
CAN
BE
APPLIED
TO
THE
RELATED
TASK
OF
FINE
GRAINED
RECOGNITION
OF
BIRDS
RELATED
WORK
MODELING
SPACE
AND
TIME
GEO
TAGGED
DATASETS
HAVE
BEEN
USED
FOR
GEO
LOCALIZATION
ON
THE
LOCAL
RE
GIONAL
AND
PLANETARY
SCALES
BUT
WE
ARE
NOT
AWARE
OF
ANY
PRIOR
WORK
ON
IMPROVING
GEO
LOCATION
BY
EXPLICITLY
CAPTURING
STYLISTIC
DIFFERENCES
BETWEEN
GEO
INFORMATIVE
VI
SUAL
ELEMENTS
BUT
SEE
FOR
ANECDOTAL
EVIDENCE
OF
SUCH
POSSIBILITY
LONGITUDINAL
I
E
LONG
TERM
TEMPORAL
VISUAL
MODELING
HAS
RECEIVED
RELATIVELY
LITTLE
ATTENTION
MOST
PREVI
OUS
RESEARCH
HAS
BEEN
ON
THE
SPECIAL
CASE
OF
AGE
ESTIMATION
FOR
FACES
SEE
FOR
A
SURVEY
RECENT
WORK
INCLUDES
MOD
ELING
THE
TEMPORAL
EVOLUTION
OF
WEB
IMAGE
COLLECTIONS
AND
DATING
OF
HISTORICAL
COLOR
PHOTOGRAPHS
WE
ARE
NOT
AWARE
OF
ANY
PRIOR
WORK
ON
MODELING
HISTORICAL
VISUAL
STYLE
VISUAL
DATA
MINING
EXISTING
VISUAL
DATA
MINING
OBJECT
DISCOVERY
APPROACHES
HAVE
BEEN
USED
TO
DISCOVER
OBJECT
CATEGORIES
MID
LEVEL
PATCHES
ATTRIBUTES
AND
LOW
LEVEL
FOREGROUND
FEATURES
TYPICALLY
AN
APPROPRIATE
SIMILARITY
MEASURE
IS
DEFINED
BE
TWEEN
VISUAL
PATTERNS
I
E
IMAGES
PATCHES
OR
CONTOURS
AND
THOSE
THAT
ARE
MOST
SIMILAR
ARE
GROUPED
INTO
DISCOVERED
ENTITIES
OF
THESE
METHODS
MID
LEVEL
DISCRIMINATIVE
PATCH
MINING
SHARES
THE
MOST
ALGORITHMIC
SIMILARITIES
WITH
OUR
WORK
WE
ALSO
REPRESENT
OUR
VISUAL
ELEMENTS
WITH
HOG
PATCHES
AND
REFINE
THE
CLUSTERS
THROUGH
DISCRIMINATIVE
CROSS
VALIDATION
TRAINING
HOWEVER
UNLIKE
AND
ALL
EXISTING
DISCOVERY
METHODS
WE
GO
BEYOND
SIMPLY
DETECT
ING
RECURRING
VISUAL
ELEMENTS
AND
MODEL
THE
STYLISTIC
DIFFER
ENCES
AMONG
THE
COMMON
DISCOVERED
ELEMENTS
VISUAL
STYLE
ANALYSIS
THE
SEMINAL
PAPER
ON
STYLE
CONTENT
SEPARATION
USES
BILINEAR
MODELS
TO
FACTOR
OUT
THE
STYLE
AND
CONTENT
COMPONENTS
IN
PRE
SEGMENTED
PRE
ALIGNED
VI
SUAL
DATA
E
G
IMAGES
OF
LETTERS
IN
DIFFERENT
FONTS
WHILE
WE
ALSO
USE
THE
TERM
STYLE
TO
DESCRIBE
THE
DIFFERENCES
BETWEEN
CORRESPONDING
VISUAL
ELEMENTS
WE
ARE
SOLVING
A
RATHER
DIFFERENT
PROBLEM
OUR
AIM
IS
TO
AUTOMATICALLY
DIS
COVER
RECURRING
VISUAL
ELEMENTS
DESPITE
THEIR
DIFFERENCES
IN
VISUAL
STYLE
AND
THEN
MODEL
THOSE
DIFFERENCES
WHILE
OUR
GENERIC
DETECTORS
COULD
PERHAPS
BE
THOUGHT
OF
AS
CAPTUR
ING
CONTENT
INDEPENDENT
OF
STYLE
WE
DO
NOT
EXPLICITLY
FACTOR
OUT
THE
STYLE
BUT
MODEL
IT
CONDITIONED
ON
THE
CONTENT
FINE
GRAINED
CATEGORIZATION
CAN
ALSO
BE
VIEWED
AS
A
FORM
OF
STYLE
ANALYSIS
AS
SUBTLE
DIFFERENCES
WITHIN
THE
SAME
BASIC
LEVEL
CATEGORY
DIFFERENTIATE
ONE
SUBORDINATE
CATEGORY
FROM
ANOTHER
EXISTING
APPROACHES
USE
HUMAN
LABELED
AT
TRIBUTES
AND
KEYPOINT
ANNOTATIONS
OR
TEMPLATE
MATCHING
BECAUSE
THESE
METHODS
ARE
FOCUSED
ON
CLASSIFICATION
THEY
LIMIT
THEMSELVES
TO
THE
SIMPLER
VI
SUAL
WORLD
OF
MANUALLY
ANNOTATED
OBJECT
BOUNDING
BOXES
WHEREAS
OUR
METHOD
OPERATES
ON
FULL
IMAGES
FURTHERMORE
DISCOVERING
ONE
TO
ONE
CORRESPONDENCES
IS
GIVEN
A
PRI
MARY
ROLE
IN
OUR
METHOD
WHEREAS
IN
MOST
FINE
GRAINED
AP
PEAKY
LOW
ENTROPY
CLUSTERS
B
UNIFORM
HIGH
ENTROPY
CLUSTERS
FIGURE
MINING
STYLE
SENSITIVE
VISUAL
ELEMENTS
CLUSTERS
ARE
CONSIDERED
STYLE
SENSITIVE
IF
THEY
HAVE
PEAKY
LOW
ENTROPY
DISTRIBUTION
ACROSS
TIME
A
AND
STYLE
INSENSITIVE
IF
THEIR
INSTANCES
ARE
DISTRIBUTED
MORE
UNIFORMLY
B
NOTICE
HOW
THE
HIGH
ENTROPY
DISTRIBUTIONS
B
REPRESENT
NOT
ONLY
STYLE
INSENSITIVITY
E
G
NONDESCRIPT
SIDE
OF
CAR
BUT
ALSO
VISUALLY
NOISY
CLUSTERS
BOTH
ARE
DISREGARDED
BY
OUR
METHOD
PROACHES
THE
CORRESPONDENCES
ARE
ALREADY
PROVIDED
WHILE
TEMPLATE
MATCHING
METHODS
ALSO
TRY
TO
DISCOVER
COR
RESPONDENCES
UNLIKE
OUR
APPROACH
THEY
DO
NOT
EXPLICITLY
MODEL
THE
STYLE
SPECIFIC
DIFFERENCES
WITHIN
EACH
CORRESPON
DENCE
SET
FINALLY
THESE
APPROACHES
HAVE
NOT
BEEN
APPLIED
TO
PROBLEMS
WITH
CONTINUOUS
LABELS
REGRESSION
WHERE
CAP
TURING
THE
RANGE
OF
STYLES
IS
PARTICULARLY
IMPORTANT
LASTLY
RELATIVE
ATTRIBUTES
MODEL
HOW
OBJECTS
SCENES
RELATE
TO
ONE
ANOTHER
VIA
ORDERED
PAIRS
OF
LABELS
A
IS
FUR
RIER
THAN
B
WE
ALSO
SHARE
THE
IDEA
OF
RELATING
THINGS
HOWEVER
INSTEAD
OF
USING
STRONG
SUPERVISION
TO
DEFINE
THESE
RELATIONSHIPS
WE
AUTOMATICALLY
MINE
FOR
VISUAL
PATTERNS
THAT
EXHIBIT
SUCH
BEHAVIOR
APPROACH
OUR
GOAL
IS
TO
DISCOVER
AND
CONNECT
MID
LEVEL
VISUAL
EL
EMENTS
ACROSS
TEMPORALLY
AND
SPATIALLY
VARYING
IMAGE
COL
LECTIONS
AND
MODEL
THEIR
STYLE
SPECIFIC
DIFFERENCES
WE
AS
SUME
THAT
THE
IMAGE
COLLECTIONS
ARE
WEAKLY
SUPERVISED
WITH
DATE
OR
LOCATION
LABELS
THERE
ARE
THREE
MAIN
STEPS
TO
OUR
APPROACH
FIRST
AS
INI
TIALIZATION
WE
MINE
FOR
STYLE
SENSITIVE
IMAGE
PATCH
CLUS
TERS
THAT
IS
GROUPS
OF
VISUALLY
SIMILAR
PATCHES
WITH
SIMILAR
LABELS
DATE
OR
LOCATION
THEN
FOR
EACH
INITIAL
CLUSTER
WE
TRY
TO
GENERALIZE
IT
BY
TRAINING
A
GENERIC
DETECTOR
THAT
COM
PUTES
CORRESPONDENCES
ACROSS
THE
ENTIRE
IMAGE
COLLECTION
TO
FIND
THE
SAME
VISUAL
ELEMENT
INDEPENDENT
OF
STYLE
FINALLY
FOR
EACH
SET
OF
CORRESPONDENCES
WE
TRAIN
A
STYLE
AWARE
RE
GRESSION
MODEL
THAT
LEARNS
TO
DIFFERENTIATE
THE
SUBTLE
STYLISTIC
DIFFERENCES
BETWEEN
DIFFERENT
INSTANCES
OF
THE
SAME
GENERIC
ELEMENT
IN
THE
FOLLOWING
SECTIONS
WE
DESCRIBE
EACH
OF
THE
STEPS
IN
TURN
WE
WILL
USE
AN
IMAGE
COLLECTION
OF
HISTORIC
CARS
AS
OUR
RUNNING
EXAMPLE
BUT
NOTE
THAT
THERE
IS
NOTHING
SPECIFIC
TO
CARS
IN
OUR
ALGORITHM
MINING
STYLE
SENSITIVE
VISUAL
ELEMENTS
MOST
RECURRING
VISUAL
PATTERNS
IN
OUR
DATA
WILL
BE
EX
TREMELY
BORING
SKY
ASPHALT
ETC
THEY
WILL
ALSO
NOT
EX
HIBIT
ANY
STYLISTIC
VARIATION
OVER
TIME
OR
SPACE
AND
NOT
BE
OF
ANY
USE
IN
HISTORICAL
DATING
OR
GEO
LOCALIZATION
AFTER
ALL
ASPHALT
IS
ALWAYS
JUST
ASPHALT
EVEN
SOME
PARTS
OF
THE
CAR
E
G
A
WINDOW
DO
NOT
REALLY
CHANGE
MUCH
OVER
THE
DECADES
ON
THE
OTHER
HAND
WE
WOULD
EXPECT
THE
SHAPE
OF
THE
HOOD
BETWEEN
TWO
CARS
TO
BE
MORE
SIMILAR
THAN
BETWEEN
A
AND
A
CAR
THEREFORE
OUR
FIRST
TASK
IS
TO
MINE
FOR
VISUAL
ELEMENTS
WHOSE
APPEARANCE
SOMEHOW
CORRELATES
WITH
ITS
LABELS
I
E
DATE
OR
LOCATION
WE
CALL
VI
SUAL
ELEMENTS
THAT
EXHIBIT
THIS
BEHAVIOR
STYLE
SENSITIVE
SINCE
WE
DO
NOT
KNOW
A
PRIORI
THE
CORRECT
SCALE
LOCA
TION
AND
SPATIAL
EXTENT
OF
THE
STYLE
SENSITIVE
ELEMENTS
WE
RANDOMLY
SAMPLE
PATCHES
ACROSS
VARIOUS
SCALES
AND
LOCA
TIONS
FROM
EACH
IMAGE
IN
THE
DATASET
FOLLOWING
WE
REPRESENT
EACH
PATCH
WITH
A
HISTOGRAM
OF
GRADIENTS
HOG
DESCRIPTOR
AND
FIND
ITS
TOP
N
NEAREST
NEIGHBOR
PATCHES
IN
THE
DATABASE
USING
NORMALIZED
CORRELATION
BY
MATCHING
IT
TO
EACH
IMAGE
IN
A
SLIDING
WINDOW
FASHION
OVER
MULTIPLE
SCALES
AND
LOCATIONS
TO
ENSURE
THAT
REDUNDANT
OVERLAPPING
PATCHES
ARE
NOT
CHOSEN
MORE
THAN
ONCE
FOR
EACH
MATCHING
IMAGE
WE
ONLY
TAKE
ITS
BEST
MATCHING
PATCH
EACH
SAMPLED
PATCH
AND
ITS
N
NEAREST
NEIGHBORS
IDEALLY
FORM
A
CLUSTER
OF
A
RECURRING
VISUAL
ELEMENT
ALTHOUGH
MANY
CLUSTERS
WILL
BE
VERY
NOISY
DUE
TO
INADEQUACIES
OF
SIMPLE
HOG
MATCHING
TO
IDENTIFY
THE
STYLE
SENSITIVE
CLUSTERS
WE
CAN
ANALYZE
THE
TEMPORAL
DISTRIBUTION
OF
LABELS
FOR
EACH
CLUSTER
INSTANCES
INTUITIVELY
A
CLUSTER
THAT
HAS
A
TIGHTLY
GROUPED
PEAKY
LABEL
DISTRIBUTION
SUGGESTS
A
VISUAL
ELE
MENT
THAT
PREFERS
A
PARTICULAR
TIME
PERIOD
AND
IS
THUS
STYLE
SENSITIVE
WHILE
A
CLUSTER
THAT
HAS
A
UNIFORM
LABEL
DISTRIBU
TION
SUGGESTS
A
PATTERN
THAT
DOESN
T
CHANGE
OVER
TIME
AS
EXTRA
BONUS
MOST
NOISY
CLUSTERS
WILL
ALSO
HAVE
A
UNIFORM
DISTRIBUTION
SINCE
IT
IS
VERY
UNLIKELY
TO
BE
STYLE
SENSITIVE
BY
RANDOM
CHANCE
TO
MEASURE
THE
STYLE
SENSITIVITY
OF
CLUS
TER
C
WE
HISTOGRAM
ITS
LABELS
AND
COMPUTE
ITS
ENTROPY
N
HISTOGRAM
COUNT
FOR
BIN
I
AND
N
DENOTES
THE
NUMBER
OF
QUAN
TIZED
LABEL
BINS
WE
NORMALIZE
THE
HISTOGRAM
TO
SUM
TO
WE
THEN
SORT
THE
CLUSTERS
IN
ASCENDING
ORDER
OF
ENTROPY
FIG
URE
A
AND
B
SHOW
EXAMPLES
OF
THE
HIGHEST
AND
LOW
EST
RANKED
CLUSTERS
FOR
THE
CAR
DATASET
IMAGES
NOTICE
HOW
THE
HIGHEST
RANKED
CLUSTERS
CORRESPOND
TO
STYLE
SENSITIVE
CAR
ELEMENTS
WHILE
THE
LOWEST
RANKED
CLUSTERS
CONTAIN
NOISY
OR
STYLE
INSENSITIVE
ONES
WE
TAKE
THE
TOP
M
CLUSTERS
AS
FIGURE
TO
ACCOUNT
FOR
A
VISUAL
ELEMENT
VARIATION
IN
STYLE
OVER
SPACE
OR
TIME
WE
INCREMENTALLY
REVISE
ITS
DETECTOR
BY
AUGMENTING
THE
POSITIVE
TRAINING
SET
WITH
THE
TOP
DETECTIONS
FIRED
ONLY
ON
IMAGES
WITH
NEARBY
LABELS
THIS
PRODUCES
AN
ACCURATE
GENERIC
DETECTOR
THAT
IS
INVARIANT
TO
THE
VISUAL
ELEMENT
CHANGES
IN
STYLE
SINGH
ET
AL
OUR
APPROACH
FIGURE
ESTABLISHING
CORRESPONDENCES
ACROSS
TIME
A
CORRESPONDENCES
MADE
USING
THE
DISCRIMINATIVE
PATCH
MINING
APPROACH
USING
A
POSITIVE
SET
OF
FRONTAL
CARS
NOTE
HOW
THE
CORRESPONDENCES
BREAK
DOWN
A
THIRD
OF
THE
WAY
THROUGH
B
STARTING
WITH
THE
SAME
INITIAL
SET
OF
FRONTAL
CARS
OUR
ALGORITHM
GRADUALLY
EXPANDS
THE
POSITIVE
SET
OVER
THE
CONTINUOUS
LABEL
SPACE
UNTIL
IT
IS
ABLE
TO
CONNECT
THE
SAME
VISUAL
ELEMENT
ACROSS
THE
ENTIRE
TEMPORAL
EXTENT
OF
THE
DATASET
OUR
DISCOVERED
STYLE
SENSITIVE
VISUAL
ELEMENTS
AFTER
REJECT
ING
NEAR
DUPLICATE
CLUSTERS
MEASURED
BY
SPATIAL
OVERLAP
OF
MORE
THAN
BETWEEN
ANY
OF
THEIR
CLUSTER
MEMBERS
ESTABLISHING
CORRESPONDENCES
EACH
OF
THE
TOP
M
CLUSTERS
CORRESPONDS
TO
A
STYLE
SENSITIVE
VISUAL
ELEMENT
IN
A
LOCAL
REGION
OF
THE
LABEL
SPACE
A
FEW
OF
THESE
ELEMENTS
REPRESENT
VERY
SPECIFIC
VISUAL
FEA
TURES
THAT
JUST
DO
NOT
OCCUR
IN
OTHER
PARTS
OF
THE
DATA
E
G
CAR
TAILFINS
FROM
BUT
MOST
OTHERS
HAVE
SIMILAR
COUNTER
PARTS
IN
OTHER
TIME
PERIODS
AND
OUR
GOAL
IS
TO
CONNECT
THEM
TOGETHER
WHICH
WILL
ALLOW
US
TO
MODEL
THE
CHANGE
IN
STYLE
OF
THE
SAME
VISUAL
ELEMENT
OVER
THE
ENTIRE
LABEL
SPACE
FOR
INSTANCE
ONE
OF
THE
STYLE
SENSITIVE
ELEMENTS
COULD
REPRESENT
FRONTAL
CARS
FROM
WE
WANT
TO
FIND
CORRESPONDING
FRONTAL
CAR
PATCHES
ACROSS
ALL
TIME
PERIODS
THE
SAME
VISUAL
ELEMENT
HOWEVER
CAN
LOOK
QUITE
DIF
FERENT
ACROSS
THE
LABEL
SPACE
ESPECIALLY
OVER
LARGER
TEMPO
RAL
EXTENTS
FIGURE
TO
OBTAIN
ACCURATE
CORRESPONDENCES
ACROSS
ALL
STYLE
VARIATIONS
WE
PROPOSE
TO
TRAIN
A
DISCRIMI
NATIVE
DETECTOR
USING
AN
ITERATIVE
PROCEDURE
THAT
EXPLOITS
THE
CONTINUOUS
NATURE
OF
THE
LABEL
SPACE
IN
GENERAL
WE
EXPECT
THE
APPEARANCE
OF
A
VISUAL
ELEMENT
TO
CHANGE
GRADUALLY
AS
A
FUNCTION
OF
ITS
LABEL
OUR
KEY
IDEA
IS
TO
INITIALIZE
THE
DETECTOR
USING
A
STYLE
SENSITIVE
CLUSTER
AS
THE
INITIAL
POSITIVE
TRAINING
DATA
BUT
THEN
INCREMENTALLY
REVISE
IT
BY
AUGMENTING
THE
POS
ITIVE
SET
WITH
DETECTIONS
FIRED
ONLY
ON
IMAGES
WITH
NEARBY
LABELS
E
G
DECADES
AS
SHOWN
IN
FIGURE
SPECIFICALLY
WE
FIRST
TRAIN
A
LINEAR
SVM
DETECTOR
WITH
THE
CLUSTER
PATCHES
AS
POSITIVES
AND
PATCHES
SAMPLED
FROM
THOU
SANDS
OF
RANDOM
FLICKR
IMAGES
AS
NEGATIVES
THESE
NEGA
TIVES
WILL
MAKE
THE
DETECTOR
DISCRIMINATIVE
AGAINST
GENERIC
PATTERNS
OCCURRING
IN
THE
NATURAL
WORLD
WHICH
HELPS
IT
TO
FIRE
ACCURATELY
ON
UNSEEN
IMAGES
WE
THEN
INCREMEN
TALLY
REVISE
THE
DETECTOR
AT
EACH
STEP
WE
RUN
THE
CURRENT
DETECTOR
ON
A
NEW
SUBSET
OF
THE
DATA
THAT
COVERS
A
SLIGHTLY
BROADER
RANGE
IN
LABEL
SPACE
AND
RETRAIN
IT
BY
AUGMENTING
THE
POSITIVE
TRAINING
SET
WITH
THE
TOP
DETECTIONS
WE
REPEAT
THIS
PROCESS
UNTIL
ALL
LABELS
HAVE
BEEN
ACCOUNTED
FOR
MAK
ING
THESE
TRANSITIVE
CONNECTIONS
PRODUCES
A
FINAL
GENERIC
DE
TECTOR
THAT
FIRES
ACCURATELY
ACROSS
THE
ENTIRE
LABEL
SPACE
AS
SHOWN
IN
FIGURE
B
NOTE
THAT
AUTOMATIC
DISCOVERY
OF
TRANSITIVE
VISUAL
CORRESPONDENCES
ACROSS
A
DATASET
IS
VERY
MUCH
IN
THE
SPIRIT
OF
THE
VISUAL
MEMEX
OPENING
UP
SEVERAL
PROMISING
FUTURE
DIRECTIONS
FOR
INVESTIGATION
THERE
IS
AN
IMPORTANT
ISSUE
THAT
WE
MUST
ADDRESS
TO
EN
SURE
THAT
THE
DETECTOR
IS
ROBUST
TO
NOISE
THE
INITIAL
CLUSTER
CAN
CONTAIN
IRRELEVANT
OUTLIER
PATCHES
SINCE
SOME
OF
THE
TOP
N
NEAREST
NEIGHBORS
OF
THE
QUERY
PATCH
COULD
BE
BAD
MATCHES
TO
PRUNE
OUT
THE
NOISY
INSTANCES
AT
EACH
STEP
OF
THE
INCREMENTAL
REVISION
OF
OUR
DETECTOR
WE
APPLY
CROSS
VALIDATION
TRAINING
SPECIFICALLY
WE
CREATE
MULTIPLE
PARTITIONS
OF
THE
TRAINING
SET
AND
ITERATIVELY
REFINE
THE
CUR
RENT
DETECTOR
BY
TRAINING
ON
ONE
PARTITION
TESTING
ON
ANOTHER
TAKING
THE
RESULTING
TOP
DETECTIONS
AS
THE
NEW
TRAINING
INSTANCES
AND
REPEATING
STEPS
UNTIL
CONVER
GENCE
I
E
THE
TOP
DETECTIONS
DO
NOT
CHANGE
EFFECTIVELY
AT
EACH
ITERATION
THE
DETECTOR
LEARNS
TO
BOOST
THE
COMMON
PATTERNS
SHARED
ACROSS
THE
TOP
DETECTIONS
AND
DOWN
WEIGHTS
THEIR
DISCREPANCIES
WITHOUT
OVER
FITTING
WHICH
LEADS
TO
MORE
CAR
DATABASE
CARDB
INTERNET
MOVIE
CAR
DATABASE
IMCDB
EAST
COAST
DATABASE
EDB
FIGURE
EACH
CARDB
AND
IMCDB
IMAGE
IS
LABELED
WITH
THE
CAR
MODEL
YEAR
EACH
EDB
IMAGE
IS
LABELED
WITH
ITS
GPS
COORDINATE
ACCURATE
DETECTIONS
IN
THE
NEXT
ITERATION
NOTE
THAT
A
DIRECT
APPLICATION
OF
WILL
NOT
WORK
FOR
OUR
CASE
OF
CONTINUOUS
STYLE
VARYING
DATA
BECAUSE
THE
VARIABILITY
CAN
BE
TOO
GREAT
FIGURE
A
SHOWS
DETECTIONS
MADE
BY
A
DETECTOR
TRAINED
WITH
USING
THE
SAME
INI
TIAL
STYLE
SENSITIVE
CLUSTER
OF
CARS
AS
POSITIVES
THE
DETECTOR
PRODUCES
ACCURATE
MATCHES
IN
NEARBY
DECADES
BUT
THE
CORRESPONDENCE
BREAKS
DOWN
ACROSS
LARGER
TEMPORAL
EX
TENTS
BECAUSE
IT
FAILS
TO
MODEL
THE
VARIATION
IN
STYLE
FINALLY
WE
FIRE
EACH
TRAINED
GENERIC
DETECTOR
ON
ALL
IM
AGES
AND
TAKE
THE
TOP
DETECTION
PER
IMAGE
AND
WITH
SVM
SCORE
GREATER
THAN
TO
OBTAIN
THE
FINAL
CORRESPONDENCES
TRAINING
STYLE
AWARE
REGRESSION
MODELS
THE
RESULT
OF
THE
PREVIOUS
STEP
IS
A
SET
OF
GENERIC
MID
LEVEL
DETECTORS
EACH
TUNED
TO
A
PARTICULAR
VISUAL
ELEMENT
AND
ABLE
TO
PRODUCE
A
SET
OF
CORRESPONDING
INSTANCES
UNDER
MANY
DIFFERENT
STYLES
NOW
WE
ARE
FINALLY
READY
TO
MODEL
THAT
VARIATION
IN
STYLE
AND
BECAUSE
THE
CORRESPONDENCES
ARE
SO
GOOD
WE
CAN
NOW
FORGET
ABOUT
THE
LARGER
DATASET
AND
FOCUS
ENTIRELY
ON
EACH
SET
OF
CORRESPONDING
INSTANCES
IN
ISO
LATION
MAKING
OUR
MODELING
PROBLEM
MUCH
SIMPLER
THE
FINAL
STEP
IS
TO
TRAIN
A
STYLE
AWARE
REGRESSOR
FOR
EACH
ELEMENT
THAT
MODELS
ITS
STYLISTIC
VARIATION
OVER
THE
LABEL
SPACE
IT
IS
SAFE
TO
ASSUME
THAT
STYLE
WILL
NOT
CHANGE
LINEARLY
OVER
THE
LABEL
SPACE
E
G
WITH
CARS
IT
IS
POSSIBLE
THAT
STYLIS
TIC
ELEMENTS
FROM
ONE
DECADE
COULD
BE
REINTRODUCED
AS
VIN
TAGE
IN
A
LATER
DECADE
TO
ACCOUNT
FOR
THIS
WE
TRAIN
A
STANDARD
NON
LINEAR
SUPPORT
VECTOR
REGRESSOR
SVR
WITH
AN
E
INSENSITIVE
LOSS
FUNCTION
USING
GROUND
TRUTH
WEAKLY
SUPERVISED
IMAGE
LABELS
E
G
DATE
GEO
LOCATION
AS
THE
TARGET
SCORE
WE
USE
GAUSSIAN
KERNELS
K
XI
XJ
EXP
Γ
XI
XJ
WHERE
Γ
IS
THE
MEAN
OF
THE
PAIR
WISE
DISTANCES
AMONG
ALL
INSTANCES
AND
XI
IS
THE
HOG
FEA
TURE
FOR
INSTANCE
I
UNDER
THIS
KERNEL
INSTANCES
WITH
SIMILAR
APPEARANCE
ARE
MOST
LIKELY
TO
HAVE
SIMILAR
REGRESSION
OUT
PUTS
FURTHERMORE
TO
HANDLE
POSSIBLE
MIS
DETECTIONS
MADE
BY
THE
GENERIC
DETECTOR
WHICH
COULD
ADD
NOISE
WE
WEIGHT
EACH
INSTANCE
PROPORTIONAL
TO
ITS
DETECTION
SCORE
WHEN
TRAIN
ING
THE
SVR
WE
MAP
A
DETECTION
SCORE
TO
A
WEIGHT
IN
VIA
A
LOGISTIC
FUNCTION
EXP
EACH
RE
SULTING
MODEL
CAPTURES
THE
STYLISTIC
DIFFERENCES
OF
THE
SAME
VISUAL
ELEMENT
FOUND
BY
THE
GENERIC
DETECTOR
RESULTS
IN
THIS
SECTION
WE
EVALUATE
OUR
METHOD
ABILITY
TO
PRE
DICT
DATE
LOCATION
COMPARED
TO
SEVERAL
BASELINES
PROVIDE
IN
DEPTH
COMPARISONS
TO
THE
DISCRIMINATIVE
PATCH
MINING
AP
PROACH
OF
SHOW
QUALITATIVE
EXAMPLES
OF
DISCOVERED
CORRESPONDENCES
AND
LEARNED
STYLES
AND
APPLY
OUR
AP
PROACH
TO
FINE
GRAINED
RECOGNITION
OF
BIRDS
DATASETS
WE
USE
THREE
DATASETS
CAR
DATABASE
CARDB
PHOTOS
OF
CARS
MADE
IN
CRAWLED
FROM
WWW
CARDATABASE
NET
INTERNET
MOVIE
CAR
DATABASE
IM
CDB
MOVIE
IMAGES
OF
CARS
MADE
IN
CRAWLED
FROM
WWW
IMCDB
ORG
AND
EAST
COAST
DATABASE
EDB
GOOGLE
STREET
VIEW
IMAGES
ALONG
THE
EASTERN
COASTS
OF
GEORGIA
SOUTH
CAROLINA
AND
NORTH
CAROLINA
EX
AMPLE
IMAGES
ARE
SHOWN
IN
FIGURE
CARDB
AND
IMCDB
IMAGES
ARE
LABELED
WITH
THE
MODEL
YEAR
OF
THE
MAIN
CAR
IN
THE
IMAGE
AND
EDB
IMAGES
ARE
LABELED
WITH
THEIR
GPS
CO
ORDINATES
THESE
ARE
THE
STYLE
LABELS
AND
THE
ONLY
SUPER
VISORY
INFORMATION
WE
USE
FOR
EDB
SINCE
OUR
SVRS
EX
PECT
OUTPUTS
ALTHOUGH
A
MULTIVARIATE
REGRESSION
METHOD
COULD
ALSO
BE
USED
WE
PROJECT
THE
IMAGES
GPS
COOR
DINATES
TO
USING
PCA
THIS
WORKS
BECAUSE
THE
AREA
OF
INTEREST
IS
ROUGHLY
LINEAR
I
E
LONG
AND
NARROW
SEE
FIGURE
C
THESE
DATASETS
EXHIBIT
A
NUMBER
OF
CHALLENGES
INCLUD
ING
CLUTTER
OCCLUSION
SCALE
LOCATION
AND
VIEWPOINT
CHANGE
AND
LARGE
APPEARANCE
VARIATIONS
OF
THE
OBJECTS
IMPORTANTLY
UNLIKE
STANDARD
OBJECT
RECOGNITION
DATASETS
OURS
HAVE
CON
TINUOUS
LABELS
WE
PARTITION
THE
CARDB
AND
EDB
DATASETS
INTO
TRAIN
TEST
SETS
WITH
SPLITS
WE
EVALUATE
ON
ALL
DATASETS
AND
FOCUS
ADDITIONAL
ANALYSIS
ON
CARDB
SINCE
IT
HAS
THE
LARGEST
NUMBER
OF
IMAGES
IMAGE
LEVEL
DATE
LOCATION
PREDICTION
TO
EVALUATE
ON
A
LABEL
PREDICTION
TASK
WE
NEED
TO
COMBINE
ALL
OF
OUR
VISUAL
ELEMENT
PREDICTORS
TOGETHER
WE
TRAIN
AN
IMAGE
LEVEL
PREDIC
TION
MODEL
USING
AS
FEATURES
THE
OUTPUTS
OF
EACH
STYLE
AWARE
REGRESSOR
ON
AN
IMAGE
SPECIFICALLY
WE
REPRESENT
AN
IMAGE
I
WITH
FEATURE
Φ
I
WHICH
IS
THE
CONCATENATION
OF
THE
MAX
IMUM
SVM
DETECTION
SCORES
OF
THE
GENERIC
DETECTORS
OVER
THE
IMAGE
AND
THE
SVR
SCORES
OF
THEIR
CORRESPONDING
STYLE
AWARE
REGRESSORS
WHEN
TESTING
ON
EDB
WE
AGGREGATE
THE
FEATURES
IN
SPATIAL
BINS
VIA
A
SPATIAL
PYRAMID
SINCE
WE
EXPECT
THERE
TO
BE
SPATIAL
CONSISTENCY
OF
VISUAL
PATTERNS
ACROSS
IMAGES
WE
USE
THESE
FEATURES
TO
TRAIN
AN
IMAGE
LEVEL
CARDB
DATE
PREDICTION
ERROR
OURS
SINGH
ET
AL
SP
BOW
EDB
GEO
LOCATION
PREDICTION
ERROR
OURS
SP
SINGH
ET
AL
BOW
TABLE
MEAN
ABSOLUTE
ERROR
ON
CARDB
EDB
AND
IMCDB
FOR
ALL
METHODS
THE
RESULT
ON
IMCDB
EVALUATES
CROSS
DATASET
GENERAL
IZATION
PERFORMANCE
LOWER
VALUES
ARE
BETTER
FIGURE
BOX
PLOTS
SHOWING
DATE
AND
LOCATION
PREDICTION
ERROR
ON
THE
CARDB
AND
EDB
DATASETS
RESPECTIVELY
LOWER
VALUES
ARE
BET
TER
OUR
APPROACH
MODELS
THE
SUBTLE
STYLISTIC
DIFFERENCES
FOR
EACH
DISCOVERED
ELEMENT
IN
THE
DATA
WHICH
LEADS
TO
LOWER
ERROR
RATES
GAUSSIAN
SVR
THIS
MODEL
ESSENTIALLY
SELECTS
THE
MOST
USE
FUL
STYLE
AWARE
REGRESSORS
FOR
PREDICTING
STYLE
GIVEN
THE
EN
TIRE
IMAGE
TO
ENSURE
THAT
THE
IMAGE
LEVEL
MODEL
DOES
NOT
OVERFIT
WE
TRAIN
IT
ON
A
SEPARATE
VALIDATION
SET
BASELINES
FOR
DATE
LOCATION
PREDICTION
WE
COMPARE
TO
THREE
BASELINES
BAG
OF
WORDS
BOW
SPATIAL
PYRAMID
SP
AND
SINGH
ET
AL
FOR
THE
FIRST
TWO
WE
DE
TECT
DENSE
SIFT
FEATURES
COMPUTE
A
GLOBAL
VISUAL
WORD
DIC
TIONARY
ON
THE
FULL
DATASET
AND
THEN
TRAIN
AN
INTERSECTION
KERNEL
SVR
USING
THE
DATE
LOCATION
LABELS
FOR
SINGH
ET
AL
WHICH
MINES
DISCRIMINATIVE
PATCHES
BUT
DOES
NOT
MODEL
THEIR
CHANGE
IN
STYLE
WE
ADAPT
THE
APPROACH
TO
TRAIN
DATE
LOCATION
SPECIFIC
PATCH
DETECTORS
USING
THE
INITIAL
STYLE
SENSITIVE
CLUSTERS
DISCOVERED
IN
SEC
SPECIFICALLY
WE
TAKE
EACH
SPECIFIC
CLUSTER
INSTANCES
AS
POSITIVES
AND
ALL
PATCHES
FROM
THE
REMAINING
TRAINING
IMAGES
THAT
DO
NOT
SHARE
THE
SAME
LABELS
WITH
A
SMALL
DON
T
CARE
REGION
IN
BETWEEN
AS
NEGATIVES
NOW
JUST
LIKE
IN
THE
PREVIOUS
PARA
GRAPH
WE
CONCATENATE
THE
MAX
OUTPUT
OF
THE
DETECTORS
AS
FEATURES
TO
TRAIN
AN
IMAGE
LEVEL
GAUSSIAN
SVR
WE
OPTI
MIZE
ALL
BASELINES
PARAMETERS
BY
CROSS
VALIDATION
IMPLEMENTATION
DETAILS
WE
SAMPLE
PIXEL
PATCHES
OVER
AN
IMAGE
PYRAMID
AT
SCALES
I
E
MIN
MAX
PATCH
IS
PIXELS
WIDE
IN
ORIGINAL
IMAGE
AND
REPRESENT
THEM
WITH
A
HOG
DESCRIPTOR
FOR
EDB
PATCHES
WE
AUGMENT
HOG
WITH
A
TINY
IMAGE
IN
LAB
COLORSPACE
WHEN
TRAINING
THE
STYLE
AWARE
SVRS
WE
SET
N
N
AND
M
FOR
CARDB
AND
EDB
RESPECTIVELY
FOR
OUR
GENERIC
SVM
DETECTORS
WE
FIX
CSVM
AND
COVER
OF
THE
LABEL
SPACE
AT
EACH
TRAINING
STEP
CARDB
YEARS
EDB
MILES
FOR
OUR
SVRS
WE
FIX
E
AND
SET
CSVR
AND
FOR
CARDB
AND
EDB
RESPECTIVELY
TUNED
USING
CROSS
VALIDATION
ON
THE
TRAINING
SET
DATE
AND
LOCATION
PREDICTION
ACCURACY
WE
FIRST
EVALUATE
OUR
METHOD
ABILITY
TO
PREDICT
THE
COR
RECT
DATE
GEO
LOCATION
OF
THE
IMAGES
IN
CARDB
EDB
FIG
URE
AND
TABLE
TOP
ROWS
SHOW
THE
ABSOLUTE
ERROR
RATES
FOR
ALL
METHODS
THIS
METRIC
IS
COMPUTED
BY
TAKING
THE
ABSOLUTE
DIFFERENCE
BETWEEN
THE
GROUND
TRUTH
AND
PREDICTED
LABELS
OUR
APPROACH
OUTPERFORMS
ALL
BASELINES
ON
BOTH
DATASETS
THE
BASELINES
HAVE
NO
MECHANISM
TO
EXPLICITLY
MODEL
STYLISTIC
DIFFERENCES
AS
THEY
ARE
EITHER
MINING
DISCRIMI
NATE
PATCHES
OVER
A
SUBREGION
IN
LABEL
SPACE
SINGH
ET
AL
OR
USING
QUANTIZED
LOCAL
FEATURES
BOW
AND
SP
THAT
RE
SULT
IN
LOSS
OF
FINE
DETAIL
NECESSARY
TO
MODEL
SUBTLE
STYLIS
TIC
CHANGES
WITHOUT
EXPLICITLY
MAKING
CONNECTIONS
OVER
SPACE
TIME
THE
BASELINES
APPEAR
TO
HAVE
DIFFICULTY
TELLING
APART
SIGNAL
FROM
NOISE
IN
PARTICULAR
WE
SHOW
SUBSTAN
TIAL
IMPROVEMENT
ON
CARDB
BECAUSE
CARS
EXHIBIT
MORE
PRO
NOUNCED
STYLISTIC
DIFFERENCES
ACROSS
ERAS
THAT
REQUIRE
ACCU
RATE
MODELING
THE
STYLISTIC
DIFFERENCES
IN
ARCHITECTURE
AND
VEGETATION
FOR
EDB
ARE
MUCH
MORE
SUBTLE
THIS
MAKES
SENSE
SINCE
THE
GEOGRAPHIC
REGION
OF
INTEREST
ONLY
SPANS
ABOUT
MILES
ALONG
THE
U
EAST
COAST
STILL
OUR
METHOD
IS
ABLE
TO
CAPTURE
MORE
OF
THE
STYLISTIC
DIFFERENCES
TO
PRO
DUCE
BETTER
RESULTS
NOTE
THAT
CHANCE
PERFORMANCE
IS
AROUND
YEARS
AND
MILES
FOR
TEST
SET
ALL
RESULTS
ARE
AVERAGED
OVER
RANDOM
TRIALS
THE
PROPOSED
MULTI
LEVEL
ACTIVE
SELECTION
YIELDS
THE
STEEPEST
LEARNING
CURVES
RANDOM
SELECTION
LAGS
BEHIND
WASTING
ANNOTATION
EFFORT
ON
LESS
INFORMATIVE
EXAM
PLES
SINGLE
LEVEL
ACTIVE
IS
PREFERABLE
TO
RANDOM
SELECTION
YET
WE
GET
BEST
RESULTS
WHEN
OUR
ACTIVE
LEARNER
CAN
CHOOSE
BETWEEN
MULTIPLE
TYPES
OF
ANNOTATIONS
INCLUDING
SEGMEN
TATIONS
OR
IMAGE
FLAGS
THE
TOTAL
GAINS
AFTER
ARE
SIG
NIFICANT
GIVEN
THE
COMPLEXITY
OF
THE
WAY
CLASSIFICATION
PROBLEM
WITH
A
TEST
SET
CONTAINING
IMAGE
REGIONS
NOTE
THAT
THE
RANDOM
SELECTION
CURVE
IS
PROBABLY
AN
OVER
ESTIMATE
OF
ITS
QUALITY
SINCE
WE
LIMIT
THE
UNLABELED
POOL
TO
ONLY
IM
AGES
FROM
THE
MSRC
ANY
EXAMPLE
IT
REQUESTS
IS
GOING
TO
BE
FAIRLY
INFORMATIVE
FIGURE
RIGHT
SHOWS
RESULTS
FOR
THE
SAME
SETTING
WHEN
RANDOM
IMAGES
ARE
ADDED
TO
THE
UNLA
BELED
POOL
INDICATING
THAT
WHEN
UNINFORMATIVE
IMAGES
ARE
PRESENT
RANDOM
SELECTION
LAGS
EVEN
FURTHER
BEHIND
ACTIVE
SELECTION
WITH
A
LEARNED
COST
FUNCTION
FINALLY
WE
SHOW
THE
IMPACT
OF
USING
THE
PREDICTED
COST
WHILE
MAK
ING
ACTIVE
CHOICES
WE
TRAIN
A
BINARY
MULTI
INSTANCE
CLASSI
FIER
FOR
EACH
CATEGORY
USING
IMAGE
LABELS
ON
TH
OF
THE
DATA
PER
CLASS
IN
DIFFERENT
RUNS
THE
REST
IS
USED
FOR
TESTING
WE
COMPARE
TWO
MIL
ACTIVE
LEARNERS
ONE
USING
COST
PRE
DICTION
AND
ONE
ASSIGNING
A
FLAT
COST
TO
ANNOTATIONS
AT
TEST
TIME
BOTH
LEARNERS
ARE
CHARGED
THE
GROUND
TRUTH
COST
OF
GETTING
THE
REQUESTED
ANNOTATION
FIGURE
SHOWS
REPRESENTATIVE
GOOD
AND
BAD
LEARNING
CURVES
WITH
ACCURACY
MEASURED
BY
THE
AUROC
VALUE
FOR
TREE
AND
AIRPLANE
USING
THE
PREDICTED
COST
LEADS
TO
MUCH
BETTER
ACCURACIES
AT
A
LOWER
COST
WHEREAS
FOR
SKY
THERE
IS
LITTLE
DIFFERENCE
THIS
MAY
BE
BECAUSE
MOST
SKY
REGIONS
LOOK
SIMILAR
AND
TAKE
SIMILAR
AMOUNTS
OF
TIME
TO
ANNOTATE
FIGURE
RIGHT
SHOWS
THE
COST
REQUIRED
TO
IMPROVE
THE
BASE
CLASSIFIER
TO
DIFFERENT
LEVELS
OF
ACCURACY
THE
COL
UMN
SHOWS
THE
RELATIVE
TIME
SAVINGS
OUR
COST
PREDICTION
EN
ABLES
OVER
A
COST
BLIND
ACTIVE
LEARNER
THAT
USES
THE
SAME
PARAMETER
RL
SHOULD
REFLECT
THE
REAL
COST
OF
A
CLASSIFICATION
MISTAKE
WE
SET
IT
TO
SINCE
AN
ERROR
MADE
BY
THE
AUTOMATIC
LABELING
WOULD
TAKE
AROUND
TO
MANUALLY
FIX
FOR
THE
AVERAGE
IMAGE
ACTIVE
RESULTS
FOR
CLASS
TREE
ACTIVE
RESULTS
FOR
CLASS
AEROPLANE
ACTIVE
RESULTS
FOR
CLASS
SKY
WITH
COST
PREDICTION
WITH
COST
PREDICTION
WITH
COST
PREDICTION
WITHOUT
COST
WITHOUT
COST
WITHOUT
COST
MANUAL
COST
SECS
MANUAL
COST
SECS
200
MANUAL
COST
SECS
FIGURE
REPRESENTATIVE
LEARNING
CURVES
LEFT
AND
A
SUMMARY
OVER
ALL
CLASSES
RIGHT
WHEN
USING
ACTIVE
SELECTION
WITH
THE
LEARNED
COST
PREDICTOR
AS
COMPARED
TO
A
BASELINE
THAT
MAKES
ACTIVE
SELECTIONS
USING
A
FLAT
COST
VALUE
RIGHTMOST
SAVINGS
IN
COST
WHEN
USING
COST
PREDICTION
WITHIN
THE
ACTIVE
LEARNER
CP
REFERS
TO
USING
COST
PREDICTION
AND
NC
IS
WITHOUT
COST
OVERALL
OUR
ACTIVE
SELECTION
TAKES
LESS
EFFORT
TO
ATTAIN
THE
SAME
LEVEL
OF
ACCURACY
AS
A
COST
BLIND
ACTIVE
LEARNER
SELECTION
STRATEGY
FOR
LARGER
IMPROVEMENTS
PREDICTING
THE
COST
LEADS
TO
NOTICEABLY
GREATER
SAVINGS
IN
MANUAL
EFFORT
OVER
SAVINGS
TO
ATTAIN
A
ACCURACY
IMPROVEMENT
WITH
OUR
IMPLEMENTATION
OF
THE
INCREMENTAL
SVM
TECH
NIQUE
OF
IT
TAKES
ON
AVERAGE
SECS
TO
EVALUATE
A
SIN
GLE
REGION
AND
SECS
TO
EVALUATE
A
BAG
IMAGE
ON
A
GHZ
PC
WE
ARE
CURRENTLY
CONSIDERING
WAYS
TO
ALLEVIATE
THE
COMPUTATIONAL
COST
HOWEVER
EVEN
WITHOUT
REAL
TIME
PER
FORMANCE
A
DISTRIBUTED
FRAMEWORK
FOR
IMAGE
LABELING
THAT
INVOLVES
MULTIPLE
ANNOTATORS
COULD
BE
RUN
EFFICIENTLY
CONCLUSIONS
WE
PROPOSED
AN
ACTIVE
LEARNING
FRAMEWORK
THAT
NOT
ONLY
CHOOSES
EXAMPLES
BASED
ON
THEIR
INFORMATION
CONTENT
BUT
ALSO
ON
THE
PREDICTED
COST
OF
OBTAINING
THE
INFORMATION
OUR
FRAMEWORK
OPERATES
IN
THE
CHALLENGING
REAL
WORLD
DOMAIN
OF
MULTI
LABEL
IMAGES
WITH
MULTIPLE
POSSIBLE
TYPES
OF
ANNO
TATIONS
OUR
RESULTS
DEMONSTRATE
THAT
THE
ACTIVE
LEARNER
OBTAINS
ACCURATE
MODELS
WITH
MUCH
LESS
MANUAL
EFFORT
THAN
TYPICAL
PASSIVE
LEARNERS
WE
CAN
FAIRLY
RELIABLY
ESTIMATE
HOW
MUCH
A
PUTATIVE
ANNOTATION
WILL
COST
GIVEN
THE
IMAGE
CONTENT
ALONE
AND
OUR
MULTI
LABEL
MULTI
LEVEL
STRATEGY
OUTPERFORMS
CONVENTIONAL
ACTIVE
METHODS
THAT
ARE
RESTRICTED
TO
REQUESTING
A
SINGLE
TYPE
OF
ANNOTATION
ACKNOWLEDGEMENTS
MANY
THANKS
TO
ALEX
SOROKIN
FOR
HELPING
US
ARRANGE
THE
MECHANICAL
TURK
DATA
COLLECTION
THIS
RESEARCH
WAS
SUPPORTED
IN
PART
BY
NSF
CAREER
MICROSOFT
RESEARCH
TEXAS
HIGHER
EDUCATION
CO
ORDINATING
BOARD
AWARD
DARPA
VI
RAT
NSF
EIA
AND
THE
HENRY
LUCE
FOUNDATION
LEARNING
TO
PREDICT
WHERE
HUMANS
LOOK
TILKE
JUDD
KRISTA
EHINGER
FRE
DO
DURAND
ANTONIO
TORRALBA
MIT
COMPUTER
SCIENCE
ARTIFICIAL
INTELLIGENCE
LABORATORY
AND
MIT
BRAIN
AND
COGNITIVE
SCIENCES
ABSTRACT
FOR
MANY
APPLICATIONS
IN
GRAPHICS
DESIGN
AND
HUMAN
COMPUTER
INTERACTION
IT
IS
ESSENTIAL
TO
UNDERSTAND
WHERE
HUMANS
LOOK
IN
A
SCENE
WHERE
EYE
TRACKING
DEVICES
ARE
NOT
A
VIABLE
OPTION
MODELS
OF
SALIENCY
CAN
BE
USED
TO
PRE
DICT
FIXATION
LOCATIONS
MOST
SALIENCY
APPROACHES
ARE
BASED
ON
BOTTOM
UP
COMPUTATION
THAT
DOES
NOT
CONSIDER
TOP
DOWN
IMAGE
SEMANTICS
AND
OFTEN
DOES
NOT
MATCH
ACTUAL
EYE
MOVE
MENTS
TO
ADDRESS
THIS
PROBLEM
WE
COLLECTED
EYE
TRACKING
DATA
OF
VIEWERS
ON
IMAGES
AND
USE
THIS
DATABASE
AS
TRAINING
AND
TESTING
EXAMPLES
TO
LEARN
A
MODEL
OF
SALIENCY
BASED
ON
LOW
MIDDLE
AND
HIGH
LEVEL
IMAGE
FEATURES
THIS
LARGE
DATABASE
OF
EYE
TRACKING
DATA
IS
PUBLICLY
AVAILABLE
WITH
THIS
PAPER
INTRODUCTION
FOR
MANY
APPLICATIONS
IN
GRAPHICS
DESIGN
AND
HUMAN
COMPUTER
INTERACTION
IT
IS
ESSENTIAL
TO
UNDERSTAND
WHERE
HU
MANS
LOOK
IN
A
SCENE
FOR
EXAMPLE
AN
UNDERSTANDING
OF
VI
SUAL
ATTENTION
IS
USEFUL
FOR
AUTOMATIC
IMAGE
CROPPING
THUMBNAILING
OR
IMAGE
SEARCH
IT
CAN
BE
USED
TO
DIRECT
FOVEATED
IMAGE
AND
VIDEO
COMPRESSION
AND
LEVELS
OF
DETAIL
IN
NON
PHOTOREALISTIC
RENDERING
IT
CAN
ALSO
BE
USED
IN
ADVERTISING
DESIGN
ADAPTIVE
IMAGE
DISPLAY
ON
SMALL
DEVICES
OR
SEAM
CARVING
SOME
OF
THESE
APPLICATIONS
HAVE
BEEN
DEMONSTRATED
BY
INCORPORATING
EYE
TRACKING
INTO
THE
PROCESS
A
USER
SITS
IN
FRONT
OF
A
COMPUTER
WITH
AN
EYE
TRACKER
THAT
RECORDS
THE
USER
FIXATIONS
AND
FEEDS
THE
DATA
INTO
THE
METHOD
HOW
EVER
EYE
TRACKING
IS
NOT
ALWAYS
AN
OPTION
EYE
TRACKERS
ARE
EXPENSIVE
AND
INTERACTIVE
TECHNIQUES
ARE
A
BURDEN
WHEN
PRO
CESSING
LOTS
OF
DATA
THEREFORE
IT
IS
NECESSARY
TO
HAVE
A
WAY
TO
PREDICT
WHERE
USERS
WILL
LOOK
WITHOUT
THE
EYE
TRACK
ING
HARDWARE
AS
AN
ALTERNATIVE
MODELS
OF
SALIENCY
HAVE
BEEN
USED
TO
MEASURE
THE
CONSPICUITY
OF
A
LOCATION
OR
THE
LIKELIHOOD
OF
A
LOCATION
TO
ATTRACT
THE
ATTENTION
OF
HUMAN
OB
SERVERS
MOST
MODELS
OF
SALIENCY
ARE
BIOLOGICALLY
FIGURE
EYE
TRACKING
DATA
WE
COLLECTED
EYE
TRACKING
DATA
ON
IMAGES
FROM
VIEWERS
TO
USE
AS
GROUND
TRUTH
DATA
TO
TRAIN
A
MODEL
OF
SALIENCY
USING
MACHINE
LEARNING
GAZE
TRACKING
PATHS
AND
FIXATION
LOCATIONS
ARE
RECORDED
FOR
EACH
VIEWER
B
A
CONTINUOUS
SALIENCY
MAP
C
IS
FOUND
BY
CONVOLVING
A
GAUSSIAN
OVER
THE
FIXATION
LOCATIONS
OF
ALL
USERS
THIS
SALIENCY
MAP
CAN
BE
THRESHOLDED
TO
SHOW
THE
MOST
SALIENT
PERCENT
OF
THE
IMAGE
D
INSPIRED
AND
BASED
ON
A
BOTTOM
UP
COMPUTATIONAL
MODEL
TYPICALLY
MULTIPLE
LOW
LEVEL
VISUAL
FEATURES
SUCH
AS
INTEN
SITY
COLOR
ORIENTATION
TEXTURE
AND
MOTION
ARE
EXTRACTED
FROM
THE
IMAGE
AT
MULTIPLE
SCALES
AFTER
A
SALIENCY
MAP
IS
COMPUTED
FOR
EACH
OF
THE
FEATURES
THEY
ARE
NORMALIZED
AND
COMBINED
IN
A
LINEAR
OR
NON
LINEAR
FASHION
INTO
A
MAS
TER
SALIENCY
MAP
THAT
REPRESENTS
THE
SALIENCY
OF
EACH
PIXEL
SOMETIMES
SPECIFIC
LOCATIONS
ARE
IDENTIFIED
THROUGH
A
COM
BINATION
OF
WINNER
TAKE
ALL
AND
INHIBITION
OF
RETURN
OPERA
TIONS
THOUGH
THE
MODELS
DO
WELL
QUALITATIVELY
THE
MODELS
HAVE
LIMITED
USE
BECAUSE
THEY
FREQUENTLY
DO
NOT
MATCH
AC
TUAL
HUMAN
SACCADES
FROM
EYE
TRACKING
DATA
AS
IN
FIG
AND
FINDING
A
CLOSER
MATCH
DEPENDS
ON
TUNING
MANY
DESIGN
PARAMETERS
FIGURE
CURRENT
SALIENCY
MODELS
DO
NOT
ACCURATELY
PREDICT
HUMAN
FIXATIONS
IN
ROW
ONE
THE
LOW
LEVEL
MODEL
SELECTS
BRIGHT
SPOTS
OF
LIGHT
AS
SALIENT
WHILE
VIEWERS
LOOK
AT
THE
HUMAN
IN
ROW
TWO
THE
LOW
LEVEL
MODEL
SELECTS
THE
BUILDING
STRONG
EDGES
AND
WINDOWS
AS
SALIENT
WHILE
VIEWERS
FIXATE
ON
THE
TEXT
WE
MAKE
TWO
CONTRIBUTIONS
IN
THIS
PAPER
THE
FIRST
IS
A
LARGE
DATABASE
OF
EYE
TRACKING
EXPERIMENTS
WITH
LA
BELS
AND
ANALYSIS
AND
THE
SECOND
IS
A
SUPERVISED
LEARNING
MODEL
OF
SALIENCY
WHICH
COMBINES
BOTH
BOTTOM
UP
IMAGE
BASED
SALIENCY
CUES
AND
TOP
DOWN
IMAGE
SEMANTIC
DEPEN
DENT
CUES
OUR
DATABASE
CONSISTS
OF
EYE
TRACKING
DATA
FROM
DIFFERENT
USERS
ACROSS
IMAGES
TO
OUR
KNOWLEDGE
IT
IS
THE
FIRST
TIME
SUCH
AN
EXTENSIVE
COLLECTION
OF
EYE
TRACKING
DATA
IS
AVAILABLE
FOR
QUANTITATIVE
ANALYSIS
FOR
A
GIVEN
IM
AGE
THE
EYE
TRACKING
DATA
IS
USED
TO
CREATE
A
GROUND
TRUTH
SALIENCY
MAP
WHICH
REPRESENTS
WHERE
VIEWERS
ACTUALLY
LOOK
FIG
WE
PROPOSE
A
SET
OF
LOW
MID
AND
HIGH
LEVEL
IM
AGE
FEATURES
USED
TO
DEFINE
SALIENT
LOCATIONS
AND
USE
A
LINEAR
SUPPORT
VECTOR
MACHINE
TO
TRAIN
A
MODEL
OF
SALIENCY
WE
COMPARE
THE
PERFORMANCE
OF
SALIENCY
MODELS
CREATED
WITH
DIFFERENT
FEATURES
AND
SHOW
HOW
COMBINING
ALL
FEATURES
PRO
DUCES
THE
HIGHEST
PERFORMING
MODEL
AS
A
DEMONSTRATION
THAT
OUR
MODEL
CAN
BE
USED
FOR
GRAPHICS
APPLICATIONS
WE
SHOW
THE
DECARLO
AND
SANTELLA
ABSTRACTED
NONPHOTOREAL
ISTIC
RENDERING
TECHNIQUE
ADAPTED
TO
USE
OUR
SALIENCY
MODEL
INSTEAD
OF
EYE
TRACKING
INPUT
OTHER
RESEARCHERS
HAVE
ALSO
MADE
SOME
HEADWAY
ON
IM
PROVING
LOW
LEVEL
SALIENCY
MODELS
BRUCE
AND
TSOTSOS
PRESENT
A
MODEL
FOR
VISUAL
SALIENCY
BUILT
ON
A
FIRST
PRIN
CIPLES
INFORMATION
THEORETIC
FORMULATION
DUBBED
ATTENTION
BASED
ON
INFORMATION
MAXIMIZATION
AIM
WHICH
PERFORMS
MARGINALLY
BETTER
THAN
THE
ITTI
MODEL
AVRAHAM
AND
LIN
DENBAUM
WORK
ON
ESALIENCY
USES
A
STOCHASTIC
MODEL
TO
ESTIMATE
THE
MOST
PROBABLE
TARGETS
MATHEMATICALLY
THE
MAIN
DIFFERENCE
BETWEEN
THESE
WORKS
AND
OURS
IS
THAT
THEIR
MODELS
ARE
DERIVED
MATHEMATICALLY
AND
NOT
TRAINED
DIRECTLY
FROM
A
LARGE
DATABASE
OF
EYE
TRACKING
DATA
CERF
ET
AL
IMPROVE
UPON
THE
ITTI
MODEL
BY
ADDING
FACE
DETECTION
TO
THE
MODEL
IN
ADDITION
TO
ADDING
FACE
DETECTION
WE
ADD
SEV
ERAL
OTHER
HIGHER
LEVEL
FEATURES
WHICH
PROVIDE
US
WITH
AN
INCREASED
PERFORMANCE
OVER
BOTH
THE
ITTI
AND
CERF
MODELS
OUR
WORK
IS
MOST
CLOSELY
RELATED
TO
THE
WORK
OF
KIEN
ZLE
ET
AL
WHO
ALSO
LEARN
A
MODEL
OF
SALIENCY
DIRECTLY
FROM
HUMAN
EYE
MOVEMENT
DATA
THEIR
MODEL
CONSISTS
OF
A
NONLINEAR
MAPPING
FROM
A
NORMALIZED
IMAGE
PATCH
TO
A
REAL
VALUE
TRAINED
TO
YIELD
POSITIVE
OUTPUTS
ON
FIXATED
PATCHES
AND
NEGATIVE
OUTPUTS
ON
RANDOMLY
SELECTED
IMAGE
PATCHES
IN
CONTRAST
TO
OUR
WORK
THEY
ONLY
USED
LOW
LEVEL
FEATURES
FURTHERMORE
THEIR
TRAINING
SET
COMPRISES
ONLY
GRAYSCALE
NATURAL
SCENE
IMAGES
IN
THE
SPECIFIC
SITUATION
OF
TRYING
TO
PREDICT
WHERE
PEO
PLE
LOOK
IN
A
PEDESTRIAN
SEARCH
TASK
EHINGER
ET
AL
SHOW
THAT
A
MODEL
OF
SEARCH
GUIDANCE
COMBINING
THREE
SOURCES
LOW
LEVEL
SALIENCY
TARGET
FEATURES
AND
SCENE
CONTEXT
OUT
PERFORMS
MODELS
BASED
ON
ANY
OF
THESE
SINGLE
SOURCES
OUR
WORK
FOCUSES
ON
PREDICTING
SALIENCY
IN
A
FREE
VIEWING
CON
TEXT
AND
CREATES
A
MODEL
WITH
A
LARGER
SET
OF
IMAGE
FEATURES
DATABASE
OF
EYE
TRACKING
DATA
WE
COLLECTED
A
LARGE
DATABASE
OF
EYE
TRACKING
DATA
TO
AL
LOW
LARGE
SCALE
QUANTITATIVE
ANALYSIS
OF
FIXATION
POINTS
AND
GAZE
PATHS
AND
TO
PROVIDE
GROUND
TRUTH
DATA
FOR
SALIENCY
MODEL
RESEARCH
THE
IMAGES
EYE
TRACKING
DATA
AND
ACCOM
PANYING
CODE
IN
MATLAB
ARE
ALL
AVAILABLE
ON
THE
WEB
TO
FA
CILITATE
RESEARCH
IN
PERCEPTION
AND
SALIENCY
ACROSS
THE
VISION
AND
GRAPHICS
COMMUNITY
DATA
GATHERING
PROTOCOL
WE
COLLECTED
RANDOM
IMAGES
FROM
FLICKR
CREATIVE
COMMONS
AND
LABELME
FIG
AND
RECORDED
EYE
TRACK
ING
DATA
FROM
FIFTEEN
USERS
WHO
FREE
VIEWED
THESE
IMAGES
THE
LONGEST
DIMENSION
OF
EACH
IMAGE
WAS
PIXELS
AND
THE
OTHER
DIMENSION
RANGED
FROM
TO
WITH
THE
MA
JORITY
AT
PIXELS
THERE
WERE
LANDSCAPE
IMAGES
AND
PORTRAIT
IMAGES
THE
USERS
WERE
MALES
AND
FEMALES
BETWEEN
THE
AGES
OF
AND
TWO
OF
THE
VIEWERS
WERE
RESEARCHERS
ON
THE
PROJECT
AND
THE
OTHERS
WERE
NAIVE
VIEW
ERS
ALL
VIEWERS
SAT
AT
A
DISTANCE
OF
APPROXIMATELY
TWO
FEET
FROM
A
INCH
COMPUTER
SCREEN
OF
RESOLUTION
IN
A
DARK
ROOM
AND
USED
A
CHIN
REST
TO
STABILIZE
THEIR
HEAD
AN
EYE
TRACKER
RECORDED
THEIR
GAZE
PATH
ON
A
SEPARATE
COMPUTER
AS
THEY
VIEWED
EACH
IMAGE
AT
FULL
RESOLUTION
FOR
SECONDS
SEPARATED
BY
SECOND
OF
VIEWING
A
GRAY
SCREEN
TO
ENSURE
HIGH
QUALITY
TRACKING
RESULTS
WE
CHECKED
CAMERA
CALIBRA
TION
EVERY
IMAGES
WE
DIVIDED
THE
VIEWING
INTO
TWO
SES
SIONS
OF
RANDOMLY
ORDERED
IMAGES
EACH
SESSION
WAS
DONE
ON
AVERAGE
AT
ONE
WEEK
APART
WE
PROVIDED
A
MEM
FIGURE
IMAGES
A
SAMPLE
OF
THE
IMAGES
THAT
WE
COLLECTED
FROM
FLICKR
AND
LABELME
THOUGH
THEY
WERE
SHOWN
AT
ORIGINAL
RES
OLUTION
AND
ASPECT
RATIO
IN
THE
EXPERIMENT
THEY
HAVE
BEEN
RESIZED
FOR
VIEWING
HERE
ORY
TEST
AT
THE
END
OF
BOTH
VIEWINGS
TO
MOTIVATE
USERS
TO
PAY
ATTENTION
TO
THE
IMAGES
WE
SHOWED
THEM
IMAGES
AND
ASKED
THEM
TO
INDICATE
WHICH
ONES
THEY
HAD
SEEN
BEFORE
WE
DISCARDED
THE
FIRST
FIXATION
FROM
EACH
SCANPATH
TO
AVOID
ADDING
TRIVIAL
INFORMATION
FROM
THE
INITIAL
CENTER
FIXATION
IN
ORDER
TO
OBTAIN
A
CONTINUOUS
SALIENCY
MAP
OF
AN
IM
AGE
FROM
THE
EYE
TRACKING
DATA
OF
A
USER
WE
CONVOLVE
A
GAUSSIAN
FILTER
ACROSS
THE
USER
FIXATION
LOCATIONS
SIMILAR
TO
THE
LANDSCAPE
MAP
OF
WE
ALSO
GENERATE
A
SALIENCY
MAP
OF
THE
AVERAGE
LOCATIONS
FIXATED
BY
ALL
VIEWERS
WE
CAN
CHOOSE
TO
THRESHOLD
THIS
CONTINUOUS
SALIENCY
MAP
TO
GET
A
BI
NARY
MAP
OF
THE
TOP
N
PERCENT
SALIENT
LOCATIONS
OF
THE
IMAGE
FIG
ANALYSIS
OF
DATASET
FOR
SOME
IMAGES
ALL
VIEWERS
FIXATE
ON
THE
SAME
LOCA
TIONS
WHILE
IN
OTHER
IMAGES
VIEWERS
FIXATIONS
ARE
DISPERSED
ALL
OVER
THE
IMAGE
WE
ANALYZE
THIS
CONSISTENCY
OF
HUMAN
FIXATIONS
OVER
AN
IMAGE
BY
MEASURING
THE
ENTROPY
OF
THE
AV
ERAGE
CONTINUOUS
SALIENCY
MAP
ACROSS
VIEWERS
THOUGH
THE
ORIGINAL
IMAGES
WERE
OF
VARYING
ASPECT
RATIONS
WE
RESIZED
THEM
TO
PIXEL
IMAGES
BEFORE
CALCULATING
ENTROPY
FIGURE
SHOWS
A
HISTOGRAM
OF
THE
ENTROPIES
OF
THE
IMAGES
IN
OUR
DATABASE
IT
ALSO
SHOWS
A
SAMPLE
OF
SALIENCY
MAPS
WITH
LOWEST
AND
HIGHEST
ENTROPY
AND
THEIR
CORRESPONDING
IM
AGES
OUR
DATA
INDICATES
A
STRONG
BIAS
FOR
HUMAN
FIXATIONS
TO
BE
NEAR
THE
CENTER
OF
THE
IMAGE
AS
IS
CONSISTENT
WITH
PREVIOUSLY
ANALYZED
EYE
TRACKING
DATASETS
FIGURE
SHOWS
THE
AVERAGE
HUMAN
SALIENCY
MAP
FROM
ALL
IMAGES
OF
FIXATIONS
LIE
WITHIN
THE
CENTER
OF
THE
IMAGE
OF
FIXATIONS
LIE
WITHIN
THE
CENTER
OF
THE
IMAGE
THIS
BIAS
HAS
OFTEN
BEEN
ATTRIBUTED
TO
THE
SETUP
OF
THE
EXPERIMENT
WHERE
USERS
ARE
PLACED
CENTRALLY
IN
FRONT
OF
THE
SCREEN
AND
TO
THE
FACT
THAT
HUMAN
PHOTOGRAPHERS
TEND
TO
PLACE
OBJECTS
OF
INTEREST
IN
THE
CENTER
OF
PHOTOGRAPHS
WE
USE
AN
ROC
METRIC
TO
EVALUATE
THE
PERFORMANCE
OF
HUMAN
SALIENCY
MAPS
TO
PREDICT
EYE
FIXATIONS
USING
THIS
METHOD
THE
SALIENCY
MAP
FROM
THE
FIXATION
LOCATIONS
OF
ONE
FIGURE
ANALYSIS
OF
FIXATION
LOCATIONS
THE
FIRST
TWO
ROWS
SHOW
EXAMPLES
OF
SALIENCY
MAPS
MADE
FROM
HUMAN
FIXATIONS
WITH
LOW
AND
HIGH
ENTROPY
AND
THEIR
CORRESPONDING
IMAGES
IMAGES
WITH
HIGH
CONSISTENCY
LOW
ENTROPY
TEND
TO
HAVE
ONE
CENTRAL
OBJECT
WHILE
IMAGES
WITH
LOW
CONSISTENCY
HIGH
ENTROPY
ARE
OFTEN
IMAGES
WITH
SEVERAL
DIFFERENT
TEXTURES
BOTTOM
LEFT
IS
A
HISTOGRAM
OF
THE
SALIENCY
MAP
ENTROPIES
BOTTOM
RIGHT
IS
A
PLOT
OF
ALL
THE
SALIENCY
MAPS
FROM
HUMAN
EYE
FIXATIONS
INDICATING
A
STRONG
BIAS
TO
THE
CENTER
OF
THE
IMAGE
AND
OF
FIXATIONS
LIE
WITHIN
THE
INDICATED
RECTANGLES
USER
IS
TREATED
AS
A
BINARY
CLASSIFIER
ON
EVERY
PIXEL
IN
THE
IM
AGE
SALIENCY
MAPS
ARE
THRESHOLDED
SUCH
THAT
A
GIVEN
PER
CENT
OF
THE
IMAGE
PIXELS
ARE
CLASSIFIED
AS
FIXATED
AND
THE
REST
ARE
CLASSIFIED
AS
NOT
FIXATED
THE
HUMAN
FIXATIONS
FROM
THE
OTHER
HUMANS
ARE
USED
AS
GROUND
TRUTH
BY
VARYING
THE
THRESHOLD
THE
ROC
CURVE
IS
DRAWN
AND
THE
AREA
UNDER
THE
CURVE
INDICATES
HOW
WELL
THE
SALIENCY
MAP
FROM
ONE
USER
CAN
PREDICT
THE
GROUND
TRUTH
FIXATIONS
FIGURE
SHOWS
THE
AVERAGE
ROC
CURVE
OVER
ALL
USERS
AND
ALL
IMAGES
NOTE
THAT
HUMAN
PERFORMANCE
IS
REMARKABLY
GOOD
OF
THE
GROUND
TRUTH
HUMAN
FIXATIONS
ARE
WITHIN
THE
TOP
SALIENT
AREAS
OF
A
NOVEL
VIEWER
SALIENCY
MAP
AND
PERCENT
ARE
WITHIN
THE
TOP
PERCENT
SALIENT
LOCATIONS
AS
STATED
BEFORE
THE
FIXATIONS
IN
THE
DATABASE
HAVE
A
STRONG
BIAS
TOWARDS
THE
CENTER
BECAUSE
OF
THIS
WE
FIND
THAT
SIMPLY
USING
A
GAUSSIAN
BLOB
CENTERED
IN
THE
MIDDLE
OF
THE
IMAGE
AS
THE
SALIENCY
MAP
PRODUCES
EXCELLENT
RESULTS
FIGURE
IN
THIS
ROC
CURVE
HUMAN
PERFORMANCE
IS
VERY
HIGH
DEMONSTRATING
THAT
THE
LOCATIONS
WHERE
A
HUMAN
LOOKS
ARE
VERY
INDICATIVE
OF
WHERE
OTHER
HUMANS
HAVE
LOOKED
THE
GAUSSIAN
CEN
TER
MODEL
PERFORMS
MUCH
BETTER
THAN
CHANCE
BECAUSE
OF
THE
STRONG
BIAS
OF
THE
FIXATIONS
IN
THE
DATABASE
TOWARDS
THE
CENTER
AS
NOTED
FOR
OTHER
DATASETS
AS
WELL
BY
WE
PLOT
THE
ROC
CURVE
FOR
THE
CENTER
GAUSSIAN
ON
FIGURE
IN
ORDER
TO
ANALYZE
FIXATIONS
ON
SPECIFIC
OBJECTS
AND
IM
AGE
FEATURES
WE
HAND
LABELED
OUR
IMAGE
DATASET
FOR
EACH
IMAGE
WE
LABELED
BOUNDING
BOXES
AROUND
ANY
FACES
AND
TEXT
AND
INDICATED
A
LINE
FOR
THE
HORIZON
IF
PRESENT
US
ING
THESE
LABELED
BOUNDING
BOXES
WE
CALCULATED
THAT
OF
FIXATIONS
ARE
ON
FACES
FIG
THOUGH
WE
DID
NOT
LABEL
ALL
PEOPLE
WE
NOTICED
THAT
MANY
FIXATIONS
LANDED
ON
PEOPLE
INCLUDING
REPRESENTATIONS
OF
PEOPLE
LIKE
DRAWINGS
OR
SCULP
TURES
EVEN
IF
THEIR
FACES
WERE
NOT
VISIBLE
IN
ADDITION
OF
FIXATIONS
ARE
ON
TEXT
THIS
MAY
BE
BECAUSE
SIGNS
ARE
IN
NATELY
DESIGNED
TO
BE
SALIENT
FOR
EXAMPLE
A
STOP
SIGN
OR
A
STORE
SIGN
ARE
CREATED
SPECIFICALLY
TO
DRAW
ATTENTION
WE
USE
THESE
GROUND
TRUTH
LABELS
TO
STUDY
FIXATION
PREDICTION
PERFOR
MANCE
ON
FACES
AND
AS
A
GROUND
TRUTH
FOR
FACE
AND
HORIZON
DETECTION
WE
ALSO
QUALITATIVELY
FOUND
THAT
FIXATIONS
FROM
OUR
DATABASE
ARE
OFTEN
ON
ANIMALS
CARS
AND
HUMAN
BODY
PARTS
LIKE
EYES
AND
HANDS
THESE
OBJECTS
REFLECT
BOTH
A
NO
TION
OF
WHAT
HUMANS
ARE
ATTRACTED
TO
AND
WHAT
OBJECTS
ARE
IN
OUR
DATASET
BY
ANALYZING
IMAGES
WITH
FACES
WE
NOTICED
THAT
VIEWERS
FIXATE
ON
FACES
WHEN
THEY
ARE
WITHIN
A
CERTAIN
SIZE
OF
THE
IMAGE
BUT
FIXATE
OF
PARTS
OF
THE
FACE
EYES
NOSE
LIPS
WHEN
PRESENTED
WITH
A
CLOSE
UP
OF
A
FACE
FIG
THIS
SUGGESTS
THAT
THERE
IS
A
CERTAIN
SIZE
FOR
A
REGION
OF
INTEREST
ROI
THAT
A
PERSON
FIXATES
ON
TO
GET
A
QUICK
SENSE
OF
THE
SIZE
OF
ROIS
WE
DREW
A
ROUGH
BOUNDING
BOX
AROUND
CLUSTERED
FIXATIONS
ON
IMAGES
FIGURE
SHOWS
THE
HISTOGRAM
OF
THE
RADII
OF
THE
RESULTING
ROIS
INVESTIGATING
THIS
CONCEPT
IS
AN
INTERESTING
AREA
OF
FUTURE
WORK
FIGURE
OBJECTS
OF
INTEREST
IN
OUR
DATABASE
VIEWERS
FREQUENTLY
FIXATED
ON
FACES
PEOPLE
AND
TEXT
OTHER
FIXATIONS
WERE
ON
BODY
PARTS
SUCH
AS
EYES
AND
HANDS
CARS
AND
ANIMALS
WE
FOUND
THE
ABOVE
IMAGE
AREAS
BY
SELECTING
BOUNDING
BOXES
AROUND
CONNECTED
AREAS
OF
SALIENT
PIXELS
ON
AN
IMAGE
OVERLAYED
WITH
ITS
SALIENT
MASK
FIGURE
SIZE
OF
REGIONS
OF
INTEREST
IN
MANY
IMAGES
VIEWERS
FIX
ATE
ON
HUMAN
FACES
HOWEVER
WHEN
VIEWING
THE
CLOSE
UP
OF
A
FACE
THEY
LOOK
AT
SPECIFIC
PARTS
OF
A
FACE
RATHER
THAN
THE
FACE
AS
A
WHOLE
SUGGESTING
A
CONSTRAINED
AREA
OF
THE
REGION
OF
INTEREST
ON
THE
RIGHT
IS
A
HISTOGRAM
OF
THE
RADII
OF
THE
REGIONS
OF
INTEREST
IN
PIXELS
LEARNING
A
MODEL
OF
SALIENCY
IN
CONTRAST
TO
PREVIOUS
COMPUTATIONAL
MODELS
THAT
COM
BINE
A
SET
OF
BIOLOGICALLY
PLAUSIBLE
FILTERS
TOGETHER
TO
ESTI
MATE
VISUAL
SALIENCY
WE
USE
A
LEARNING
APPROACH
TO
TRAIN
A
CLASSIFIER
DIRECTLY
FROM
HUMAN
EYE
TRACKING
DATA
FEATURES
USED
FOR
MACHINE
LEARNING
THE
FOLLOWING
ARE
THE
LOW
MID
AND
HIGH
LEVEL
FEATURES
THAT
WE
WERE
MOTIVATED
TO
WORK
WITH
AFTER
ANALYZING
OUR
DATASET
FOR
EACH
IMAGE
WE
PRECOMPUTED
THE
FEATURES
FOR
EVERY
PIXEL
OF
THE
IMAGE
RESIZED
TO
AND
USED
THESE
TO
TRAIN
OUR
MODEL
FIGURE
COMPARISON
OF
SALIENCY
MAPS
EACH
ROW
OF
IMAGES
COMPARES
THE
PREDICTORS
OF
OUR
SVM
SALIENCY
MODEL
THE
ITTI
SALIENCY
MAP
THE
CENTER
PRIOR
AND
THE
HUMAN
GROUND
TRUTH
ALL
THRESHOLDED
TO
SHOW
THE
TOP
PERCENT
SALIENT
LOCATIONS
FIGURE
FEATURES
A
SAMPLE
IMAGE
BOTTOM
RIGHT
AND
OF
THE
FEATURES
THAT
WE
USE
TO
TRAIN
THE
MODEL
THESE
INCLUDE
SUBBAND
FEATURES
ITTI
AND
KOCH
SALIENCY
CHANNELS
DISTANCE
TO
THE
CENTER
COLOR
FEATURES
AND
AUTOMATIC
HORIZON
FACE
PERSON
AND
CAR
DETEC
TORS
THE
LABELS
FOR
OUR
TRAINING
ON
THIS
IMAGE
ARE
BASED
ON
A
THRESHOLDED
SALIENCY
MAP
DERIVED
FROM
HUMAN
FIXATIONS
TO
THE
LEFT
OF
BOTTOM
RIGHT
LOW
LEVEL
FEATURES
BECAUSE
THEY
ARE
PHYSIOLOGICALLY
PLAU
SIBLE
AND
HAVE
BEEN
SHOWN
TO
CORRELATE
WITH
VISUAL
AT
TENTION
WE
USE
THE
LOCAL
ENERGY
OF
THE
STEERABLE
PYRA
MID
FILTERS
AS
FEATURES
WE
CURRENTLY
FIND
THE
PYRA
MID
SUBBANDS
IN
FOUR
ORIENTATIONS
AND
THREE
SCALES
SEE
FIG
FIRST
IMAGES
WE
ALSO
INCLUDE
FEATURES
USED
IN
A
SIMPLE
SALIENCY
MODEL
DESCRIBED
BY
TORRALBA
AND
ROSENHOLTZ
BASED
ON
SUBBAND
PYRAMIDS
FIG
BOTTOM
LEFT
INTENSITY
ORIENTATION
AND
COLOR
CONTRAST
HAVE
LONG
BEEN
SEEN
AS
IMPORTANT
FEATURES
FOR
BOTTOM
UP
SALIENCY
WE
INCLUDE
THE
THREE
CHANNELS
CORRESPONDING
TO
THESE
IM
AGE
FEATURES
AS
CALCULATED
BY
ITTI
AND
KOCH
SALIENCY
METHOD
WE
INCLUDE
THE
VALUES
OF
THE
RED
GREEN
AND
BLUE
CHAN
NELS
AS
WELL
AS
THE
PROBABILITIES
OF
EACH
OF
THESE
CHAN
NELS
AS
FEATURES
FIG
IMAGES
TO
AND
THE
PROB
ABILITY
OF
EACH
COLOR
AS
COMPUTED
FROM
COLOR
HIS
TOGRAMS
OF
THE
IMAGE
FILTERED
WITH
A
MEDIAN
FILTER
AT
DIFFERENT
SCALES
FIG
IMAGES
TO
MID
LEVEL
FEATURES
BECAUSE
MOST
OBJECTS
REST
ON
THE
SUR
FACE
OF
THE
EARTH
THE
HORIZON
IS
A
PLACE
HUMANS
NATU
RALLY
LOOK
FOR
SALIENT
OBJECTS
WE
TRAIN
A
HORIZON
LINE
DETECTOR
FROM
MID
LEVEL
GIST
FEATURES
HIGH
LEVEL
FEATURES
BECAUSE
WE
FOUND
THAT
HUMANS
FIXATED
SO
CONSISTENTLY
ON
PEOPLE
AND
FACES
WE
RUN
THE
VIOLA
JONES
FACE
DETECTOR
AND
THE
FELZENSZWALB
PERSON
DETECTOR
AND
INCLUDE
THESE
AS
FEATURES
TO
OUR
MODEL
CENTER
PRIOR
WHEN
HUMANS
TAKE
PICTURES
THEY
NATURALLY
FRAME
AN
OBJECT
OF
INTEREST
NEAR
THE
CENTER
OF
THE
IMAGE
FOR
THIS
REASON
WE
INCLUDE
A
FEATURE
WHICH
INDICATES
THE
DISTANCE
TO
THE
CENTER
FOR
EACH
PIXEL
TRAINING
IN
ORDER
TO
TRAIN
AND
TEST
OUR
MODEL
WE
DIVIDED
OUR
SET
OF
IMAGES
INTO
TRAINING
IMAGES
AND
TESTING
IMAGES
FROM
EACH
IMAGE
WE
CHOSE
POSITIVELY
LABELED
PIXELS
RAN
DOMLY
FROM
THE
TOP
SALIENT
LOCATIONS
OF
THE
HUMAN
GROUND
TRUTH
SALIENCY
MAP
AND
NEGATIVELY
LABELED
PIX
ELS
FROM
THE
BOTTOM
SALIENT
LOCATIONS
TO
YIELD
A
TRAINING
SET
OF
SAMPLES
AND
TESTING
SET
OF
SAMPLES
WE
FOUND
THAT
INCREASING
THE
NUMBER
OF
SAMPLES
CHOSEN
PER
IM
AGE
ABOVE
DID
NOT
INCREASE
PERFORMANCE
IT
IS
PROBABLE
THAT
AFTER
A
CERTAIN
NUMBER
OF
SAMPLES
PER
IMAGE
NEW
SAM
PLES
ONLY
PROVIDE
REDUNDANT
INFORMATION
WE
CHOSE
SAMPLES
FROM
THE
TOP
AND
BOTTOM
IN
ORDER
TO
HAVE
SAM
PLES
THAT
WERE
STRONGLY
POSITIVE
AND
STRONGLY
NEGATIVE
WE
AVOIDED
SAMPLES
ON
THE
BOUNDARY
BETWEEN
THE
TWO
WE
DID
NOT
CHOOSE
ANY
SAMPLES
WITHIN
PIXELS
OF
THE
BOUNDARY
OF
THE
IMAGE
OUR
TESTS
ON
MODELS
TRAINED
USING
RATIOS
OF
NEGATIVE
TO
POSITIVE
SAMPLES
RANGING
FROM
TO
SHOWED
NO
CHANGE
IN
THE
RESULTING
ROC
CURVE
SO
WE
CHOSE
TO
USE
A
RATIO
OF
WE
NORMALIZED
THE
FEATURES
OF
OUR
TRAINING
SET
TO
HAVE
ZERO
MEAN
AND
UNIT
VARIANCE
AND
USED
THE
SAME
NORMALIZA
TION
PARAMETERS
TO
NORMALIZE
OUR
TEST
DATA
WE
USED
THE
LIBLINEAR
SUPPORT
VECTOR
MACHINE
TO
TRAIN
A
MODEL
ON
THE
POSITIVE
AND
NEGATIVE
TRAINING
SAM
PLES
WE
USED
MODELS
WITH
LINEAR
KERNELS
BECAUSE
WE
FOUND
FROM
EXPERIMENTATION
THAT
THEY
PERFORMED
AS
WELL
AS
MOD
ELS
WITH
RADIAL
BASIS
FUNCTION
KERNELS
AND
MODELS
FOUND
WITH
MULTIPLE
KERNEL
LEARNING
FOR
OUR
SPECIFIC
TASK
LINEAR
MODELS
ARE
ALSO
FASTER
TO
COMPUTE
AND
THE
RESULTING
WEIGHTS
OF
FEATURES
ARE
EASIER
TO
UNDERSTAND
WE
SET
THE
MISCLASSIFI
CATION
COST
C
AT
WE
FOUND
THAT
PERFORMANCE
WAS
THE
SAME
FOR
C
TO
C
AND
DECREASED
WHEN
SMALLER
THAN
PERFORMANCE
WE
MEASURE
PERFORMANCE
OF
SALIENCY
MODELS
IN
TWO
WAYS
FIRST
WE
MEASURE
PERFORMANCE
OF
EACH
MODEL
BY
ITS
ROC
CURVE
SECOND
WE
EXAMINE
THE
PERFORMANCE
OF
DIFFER
ENT
MODELS
ON
SPECIFIC
SUBSETS
OF
SAMPLES
SAMPLES
INSIDE
AND
OUTSIDE
A
CENTRAL
AREA
OF
THE
IMAGE
AND
ON
FACES
PERFORMANCE
ON
TESTING
IMAGES
IN
FIGURE
WE
SEE
A
ROC
CURVE
DESCRIBING
THE
PERFORMANCE
OF
DIFFERENT
SALIENCY
MODELS
AVERAGED
OVER
ALL
TESTING
IMAGES
FOR
EACH
IMAGE
WE
PREDICT
THE
SALIENCY
PER
PIXEL
USING
A
SPECIFIC
TRAINED
MODEL
INSTEAD
OF
USING
THE
PREDICTED
LABELS
INDICATED
BY
THE
SIGN
OF
WT
X
B
WHERE
W
AND
B
ARE
LEARNED
PARAMETERS
AND
X
REFERS
TO
THE
FEATURE
VECTOR
WE
USE
THE
VALUE
OF
WT
X
B
AS
A
CONTINUOUS
SALIENCY
MAP
WHICH
INDICATES
HOW
SALIENT
EACH
PIXEL
IS
THEN
WE
THRESHOLD
THIS
SALIENCY
MAP
AT
N
AND
PERCENT
OF
THE
IMAGE
FOR
BINARY
SALIENCY
MAPS
WHICH
ARE
TYPICALLY
RELEVANT
FOR
APPLICATIONS
FOR
EACH
BINARY
MAP
WE
FIND
THE
PERCENTAGE
OF
HUMAN
FIX
ATIONS
WITHIN
THE
SALIENT
AREAS
OF
THE
MAP
AS
THE
MEASURE
OF
PERFORMANCE
NOTICE
THAT
AS
THE
PERCENTAGE
OF
THE
IMAGE
CONSIDERED
SALIENT
GOES
TO
THE
PREDICTABILITY
OR
PER
CENTAGE
OF
HUMAN
FIXATIONS
WITHIN
THE
SALIENT
LOCATIONS
ALSO
GOES
TO
WE
MAKE
THE
FOLLOWING
OBSERVATIONS
FROM
THE
ROC
CURVES
THE
MODEL
WITH
ALL
FEATURES
COMBINED
OUTPER
FORMS
MODELS
TRAINED
ON
SINGLE
SETS
OF
FEATURES
AND
MODELS
TRAINED
ON
COMPETING
SALIENCY
FEATURES
FROM
TORRALBA
AND
ROZENHOLTZ
ITTI
AND
KOCH
AND
CERF
ET
AL
NOTE
THAT
WE
IM
PLEMENT
THE
CERF
ET
AL
METHOD
BY
TRAINING
AN
SVM
ON
ITTI
FEATURES
AND
FACE
DETECTION
ALONE
WE
LEARN
THE
BEST
WEIGHTS
FOR
THE
LINEAR
COMBINATION
OF
FEATURES
INSTEAD
OF
USING
EQUAL
WEIGHTS
AS
THEY
DO
THE
MODEL
WITH
ALL
FEATURES
REACHES
OF
THE
WAY
TO
HUMAN
PERFORMANCE
FOR
EXAMPLE
WHEN
IMAGES
ARE
THRESHOLDED
AT
SALIENT
OUR
MODEL
PERFORMS
AT
WHILE
HUMANS
ARE
AT
THE
MODEL
WITH
ALL
FEATURES
EXCEPT
THE
DISTANCE
TO
THE
CENTER
PERFORMS
AS
WELL
AS
THE
MODEL
BASED
ON
THE
DISTANCE
TO
THE
CENTER
THIS
IS
QUITE
GOOD
CONSIDERING
THIS
MODEL
DOES
NOT
LEVERAGE
ANY
OF
THE
INFORMATION
ABOUT
LOCATION
AND
THUS
DOES
NOT
AT
ALL
BENEFIT
FROM
THE
HUGE
BIAS
OF
FIXATIONS
TOWARD
THE
CENTER
THE
MODEL
TRAINED
ON
ALL
FEATURES
EXCEPT
THE
CENTER
PER
FORMS
MUCH
BETTER
THAN
ANY
OF
THE
MODELS
TRAINED
ON
SINGLE
SETS
OF
FEATURES
FOR
EXAMPLE
AT
THE
SALIENT
LOCATION
THRESHOLD
THE
TORRALBA
BASED
MODEL
PERFORMS
AT
WHILE
THE
ALL
IN
WITHOUT
CENTER
MODEL
PERFORMS
AT
FOR
A
JUMP
IN
PERFORMANCE
THOUGH
OBJECT
DETECTORS
MAY
BE
FIGURE
THE
ROC
CURVE
OF
PERFORMANCES
FOR
SVMS
TRAINED
ON
EACH
SET
OF
FEATURES
INDIVIDUALLY
AND
COMBINED
TOGETHER
WE
ALSO
PLOT
HUMAN
PERFORMANCE
AND
CHANCE
FOR
COMPARISON
VERY
GOOD
AT
LOCATING
SALIENT
OBJECTS
WHEN
THOSE
OBJECTS
ARE
PRESENT
IN
AN
IMAGE
IT
IS
NOT
GOOD
AT
LOCATING
OTHER
SALIENT
LOCATIONS
WHEN
THE
OBJECTS
ARE
NOT
PRESENT
THUS
THE
OVER
ALL
PERFORMANCE
FOR
THE
OBJECT
DETECTOR
MODEL
IS
LOW
AND
THESE
FEATURES
SHOULD
BE
USED
ONLY
IN
CONJUNCTION
WITH
OTHER
FEATURES
ALL
MODELS
PERFORM
SIGNIFICANTLY
BETTER
THAN
CHANCE
INDICATING
THAT
EACH
OF
THE
FEATURES
INDIVIDUALLY
DO
HAVE
SOME
POWER
TO
PREDICT
SALIENT
LOCATIONS
WE
MEASURE
WHICH
FEATURES
ADD
MOST
TO
THE
MODEL
BY
CALCULATING
THE
DELTA
IMPROVEMENT
BETWEEN
THE
CENTER
MODEL
AND
THE
CENTER
MODEL
WITH
A
GIVEN
SET
OF
FEATURES
WE
OB
SERVE
THAT
SUBBAND
FEATURES
AND
TORRALBA
FEATURES
WHICH
USE
SUBBAND
FEATURES
ADD
THE
GREATEST
IMPROVEMENT
AF
TER
THAT
IS
COLOR
FEATURES
HORIZON
DETECTION
FACE
AND
OBJECT
DETECTORS
AND
ITTI
CHANNELS
PERFORMANCE
ON
TESTING
SAMPLES
TO
UNDERSTAND
THE
IM
PACT
OF
THE
BIAS
TOWARDS
THE
CENTER
OF
THE
DATASET
FOR
SOME
MODELS
WE
DIVIDED
EACH
IMAGE
INTO
A
CIRCULAR
CENTRAL
AND
A
PERIPHERAL
REGION
THE
CENTRAL
REGION
WAS
DEFINED
BY
THE
MODEL
BASED
ONLY
ON
THE
FEATURE
WHICH
GAVE
THE
DISTANCE
OF
THE
EXAMPLE
TO
THE
CENTER
IN
THIS
MODEL
ANY
SAMPLE
FARTHER
THAN
UNITS
AWAY
FROM
THE
CENTER
WHERE
THE
DISTANCE
FROM
THE
CENTER
TO
THE
CORNER
IS
WAS
LABELED
NEGATIVE
AND
ANYTHING
CLOSER
WAS
LABELED
POSITIVE
THIS
IS
EQUIVALENT
TO
THE
CENTER
OF
THE
IMAGE
GIVEN
THIS
THRESHOLD
WE
DI
FIGURE
HERE
WE
SHOW
THE
AVERAGE
RATE
OF
TRUE
POSITIVES
AND
TRUE
NEGATIVES
FOR
SVMS
TRAINED
WITH
DIFFERENT
FEATURE
SETS
ON
DIFFERENT
SUBSETS
OF
SAMPLES
THIS
VALUE
IS
EQUIVALENT
TO
THE
PERFORMANCE
OF
THE
MODEL
IF
THERE
WERE
AN
EQUAL
NUMBER
OF
POSITIVE
AND
NEGATIVE
SAMPLES
IN
EACH
SUBSET
VIDED
THE
SAMPLES
TO
THOSE
INSIDE
AND
OUTSIDE
THE
CENTER
IN
ADDITION
WE
CHOSE
TO
LOOK
AT
SAMPLES
THAT
LANDED
ON
FACES
SINCE
VIEWERS
WERE
PARTICULARLY
ATTRACTED
BY
THEM
IN
FIGURE
WE
PLOT
PERFORMANCE
OF
THE
MODEL
FOR
DIF
FERENT
SUBSETS
OF
SAMPLES
THE
PERFORMANCE
HERE
IS
DEFINED
AS
THE
AVERAGE
OF
THE
TRUE
POSITIVE
AND
TRUE
NEGATIVE
RATES
THIS
IS
EQUIVALENT
TO
THE
PERFORMANCE
OF
THE
MODEL
IF
THERE
WERE
AN
EQUAL
NUMBER
OF
POSITIVE
AND
NEGATIVE
SAMPLES
IN
EACH
SUBSET
WE
MAKE
THE
FOLLOWING
OBSERVATIONS
ABOUT
THE
TRAINED
MODELS
FROM
THIS
MEASURE
OF
PERFORMANCE
EVEN
THOUGH
CENTER
MODEL
PERFORMS
WELL
OVER
ALL
THE
SAMPLES
BOTH
SAM
PLES
INSIDE
AND
OUTSIDE
THE
CENTER
IT
PERFORMS
ONLY
AS
WELL
AS
CHANCE
FOR
THE
OTHER
SUBSETS
OF
SAMPLES
WHILE
OVER
ALL
SAMPLES
THE
PERFORMANCE
OF
THE
CENTER
MODEL
AND
THE
ALL
FEATURES
WITHOUT
CENTER
MODEL
PERFORM
THE
SAME
THE
LATER
MODEL
PERFORMS
MORE
ROBUSTLY
OVER
ALL
SUBSETS
OF
SAMPLES
UNDERSTANDABLY
THE
MODEL
TRAINED
ON
FEATURES
FROM
OB
JECT
DETECTORS
FOR
FACES
PEOPLE
AND
CARS
PERFORMS
BETTER
ON
THE
SUBSETS
WITH
FACES
THE
SVMS
USING
THE
CENTER
PRIOR
FEATURE
AND
THE
ONE
USING
ALL
FEATURES
PERFORM
VERY
WELL
ON
POSITIVE
AND
NEGATIVE
RANDOM
TESTING
POINTS
BUT
ARE
OUTPERFORMED
BOTH
IN
THE
INSIDE
AND
OUTSIDE
REGION
THIS
PARADOX
STEMS
FROM
THE
FACT
THAT
OF
THE
SALIENT
TESTING
POINTS
ARE
IN
THE
INSIDE
REGION
WHEREAS
OF
THE
NON
SALIENT
TESTING
POINTS
ARE
IN
THE
OUTSIDE
ONE
CAN
SHOW
THAT
THIS
BIASED
DISTRIBUTION
PROVIDES
A
LIFT
IN
PERFORMANCE
FIGURE
STYLIZATION
AND
ABSTRACTION
OF
PHOTOGRAPHS
DECARLO
AND
SANTELLA
USE
EYE
TRACKING
DATA
TO
DECIDE
HOW
TO
RENDER
A
PHOTOGRAPH
WITH
DIFFERING
LEVELS
OF
DETAIL
WE
REPLICATE
THIS
APPLI
CATION
WITHOUT
THE
NEED
FOR
EYE
TRACKING
HARDWARE
FOR
METHODS
THAT
WOULD
EITHER
HAVE
A
HIGH
TRUE
NEGATIVE
RATE
OUTSIDE
OR
A
HIGH
TRUE
POSITIVE
RATE
INSIDE
SUCH
AS
THE
CENTER
PRIOR
DISCUSSION
THIS
EYE
TRACKING
DATABASE
ALLOWS
US
TO
QUAN
TIFY
HOW
CONSISTENT
HUMAN
FIXATIONS
ARE
ACROSS
AN
IMAGE
IN
GENERAL
THE
FIXATION
LOCATIONS
OF
SEVERAL
HUMANS
IS
STRONGLY
INDICATIVE
OF
WHERE
A
NEW
VIEWER
WILL
LOOK
SO
FAR
COMPUTER
GENERATED
MODELS
HAVE
NOT
MATCHED
HUMANS
ABILITY
TO
PRE
DICT
FIXATION
LOCATIONS
THOUGH
WE
FEEL
WE
HAVE
MOVED
A
STEP
CLOSER
IN
THAT
DIRECTION
BY
USING
A
MODEL
THAT
COMBINES
LOW
MID
AND
HIGH
LEVEL
FEATURES
QUALITATIVELY
WE
LEARNED
THAT
WHEN
FREE
VIEWING
IMAGES
HUMANS
CONSISTENTLY
LOOK
AT
SOME
COMMON
OBJECTS
THEY
LOOK
AT
TEXT
OTHER
PEOPLE
AND
SPECIFICALLY
FACES
IF
NOT
PEO
PLE
THEY
LOOK
AT
OTHER
LIVING
ANIMALS
AND
SPECIFICALLY
THEIR
FACES
IN
THE
ABSENCE
OF
SPECIFIC
OBJECTS
OR
TEXT
HUMANS
TEND
TOWARDS
THE
CENTER
OF
THE
IMAGE
OR
LOCATIONS
WHERE
LOW
LEVEL
FEATURES
ARE
SALIENT
AS
TEXT
FACE
PERSON
AND
OTHER
OBJECT
DETECTORS
GET
BETTER
MODELS
OF
SALIENCY
WHICH
INCLUDE
OBJECT
DETECTORS
WILL
ALSO
GET
BETTER
THOUGH
ALL
THESE
TRENDS
ARE
NOT
SURPRISING
WE
ARE
EXCITED
THAT
THIS
DATABASE
WILL
ALLOW
US
TO
MEASURE
THE
TRENDS
QUANTITATIVELY
APPLICATIONS
A
GOOD
SALIENCY
MODEL
ENABLES
MANY
APPLICATIONS
THAT
AUTOMATICALLY
TAKE
INTO
ACCOUNT
A
NOTION
OF
HUMAN
PERCEP
TION
WHERE
HUMANS
LOOK
AND
WHAT
THEY
ARE
INTERESTED
IN
AS
AN
EXAMPLE
WE
USE
OUR
MODEL
IN
CONJUNCTION
WITH
THE
TECH
NIQUE
OF
DECARLO
AND
SANTELLA
TO
AUTOMATICALLY
CREATE
A
NON
PHOTOREALISTIC
RENDERING
OF
A
PHOTOGRAPH
WITH
DIFFERENT
LEVELS
OF
DETAIL
FIG
THEY
RENDER
MORE
DETAILS
AT
THE
LOCATIONS
USERS
FIXATED
ON
AND
LESS
DETAIL
IN
THE
REST
OF
THE
IMAGE
WHILE
THEY
REQUIRE
INFORMATION
FROM
AN
EYE
TRACK
ING
DEVICE
IN
ORDER
TO
TAILOR
THE
LEVEL
OF
DETAIL
WE
USE
OUR
SALIENCY
MODEL
TO
PREDICT
LOCATIONS
WHERE
PEOPLE
LOOK
CONCLUSION
IN
THIS
WORK
WE
MAKE
THE
FOLLOWING
CONTRIBUTIONS
WE
DEVELOP
A
COLLECTION
OF
EYE
TRACKING
DATA
FROM
PEOPLE
ACROSS
IMAGES
AND
HAVE
MADE
IT
PUBLIC
FOR
RESEARCH
USE
THIS
IS
THE
LARGEST
EYE
TRACKING
DATABASE
OF
NATURAL
IM
AGES
THAT
WE
ARE
AWARE
OF
AND
PERMITS
LARGE
SCALE
QUANTI
TATIVE
ANALYSIS
OF
FIXATIONS
P
O
INTS
A
N
D
G
AZE
P
A
THS
W
E
USE
MACHINE
LEARNING
TO
TRAIN
A
BOTTOM
UP
TOP
DOWN
MODEL
OF
SALIENCY
BASED
ON
LOW
MID
AND
HIGH
LEVEL
IMAGE
FEATURES
WE
DEMONSTRATE
THAT
OUR
MODEL
OUTPERFORMS
SEVERAL
EXIST
ING
MODELS
AND
THE
CENTER
PRIOR
FINALLY
WE
SHOW
AN
EXAM
PLE
OF
HOW
OUR
MODEL
CAN
BE
USED
IN
PRACTICE
FOR
GRAPHICS
APPLICATIONS
FOR
FUTURE
WORK
WE
ARE
INTERESTED
IN
UNDERSTANDING
THE
IMPACT
OF
FRAMING
CROPPING
AND
SCALING
IMAGES
ON
FIXA
TIONS
WE
BELIEVE
THAT
THE
SAME
IMAGE
CROPPED
AT
DIFFERENT
SIZES
WILL
LEAD
VIEWERS
TO
FIXATE
O
N
D
IFFERENT
O
BJECTS
I
N
THE
IMAGE
AND
SHOULD
BE
MORE
CAREFULLY
EXAMINED
PREVIEW
INTEREST
IN
DIGITAL
IMAGE
PROCESSING
METHODS
STEMS
FROM
TWO
PRINCIPAL
APPLICA
TION
AREAS
IMPROVEMENT
OF
PICTORIAL
INFORMATION
FOR
HUMAN
INTERPRETATION
AND
PROCESSING
OF
IMAGE
DATA
FOR
STORAGE
TRANSMISSION
AND
REPRESENTATION
FOR
AU
TONOMOUS
MACHINE
PERCEPTION
THIS
CHAPTER
HAS
SEVERAL
OBJECTIVES
TO
DEFINE
THE
SCOPE
OF
THE
FIELD
THAT
WE
CALL
IMAGE
PROCESSING
TO
GIVE
A
HISTORICAL
PER
SPECTIVE
OF
THE
ORIGINS
OF
THIS
FIELD
TO
GIVE
YOU
AN
IDEA
OF
THE
STATE
OF
THE
ART
IN
IMAGE
PROCESSING
BY
EXAMINING
SOME
OF
THE
PRINCIPAL
AREAS
IN
WHICH
IT
IS
AP
PLIED
TO
DISCUSS
BRIEFLY
THE
PRINCIPAL
APPROACHES
USED
IN
DIGITAL
IMAGE
PRO
CESSING
TO
GIVE
AN
OVERVIEW
OF
THE
COMPONENTS
CONTAINED
IN
A
TYPICAL
GENERAL
PURPOSE
IMAGE
PROCESSING
SYSTEM
AND
TO
PROVIDE
DIRECTION
TO
THE
BOOKS
AND
OTHER
LITERATURE
WHERE
IMAGE
PROCESSING
WORK
NORMALLY
IS
REPORTED
WHAT
IS
DIGITAL
IMAGE
PROCESSING
AN
IMAGE
MAY
BE
DEFINED
AS
A
TWO
DIMENSIONAL
FUNCTION
F
X
Y
WHERE
X
AND
Y
ARE
SPATIAL
PLANE
COORDINATES
AND
THE
AMPLITUDE
OF
F
AT
ANY
PAIR
OF
COORDI
NATES
X
Y
IS
CALLED
THE
INTENSITY
OR
GRAY
LEVEL
OF
THE
IMAGE
AT
THAT
POINT
WHEN
X
Y
AND
THE
INTENSITY
VALUES
OF
F
ARE
ALL
FINITE
DISCRETE
QUANTITIES
WE
CALL
THE
IMAGE
A
DIGITAL
IMAGE
THE
FIELD
OF
DIGITAL
IMAGE
PROCESSING
REFERS
TO
PROCESSING
DIGITAL
IMAGES
BY
MEANS
OF
A
DIGITAL
COMPUTER
NOTE
THAT
A
DIGITAL
IMAGE
IS
COM
POSED
OF
A
FINITE
NUMBER
OF
ELEMENTS
EACH
OF
WHICH
HAS
A
PARTICULAR
LOCATION
AND
VALUE
THESE
ELEMENTS
ARE
CALLED
PICTURE
ELEMENTS
IMAGE
ELEMENTS
PELS
AND
PIXELS
PIXEL
IS
THE
TERM
USED
MOST
WIDELY
TO
DENOTE
THE
ELEMENTS
OF
A
DIGITAL
IMAGE
WE
CONSIDER
THESE
DEFINITIONS
IN
MORE
FORMAL
TERMS
IN
CHAPTER
VISION
IS
THE
MOST
ADVANCED
OF
OUR
SENSES
SO
IT
IS
NOT
SURPRISING
THAT
IMAGES
PLAY
THE
SINGLE
MOST
IMPORTANT
ROLE
IN
HUMAN
PERCEPTION
HOWEVER
UNLIKE
HU
MANS
WHO
ARE
LIMITED
TO
THE
VISUAL
BAND
OF
THE
ELECTROMAGNETIC
EM
SPEC
TRUM
IMAGING
MACHINES
COVER
ALMOST
THE
ENTIRE
EM
SPECTRUM
RANGING
FROM
GAMMA
TO
RADIO
WAVES
THEY
CAN
OPERATE
ON
IMAGES
GENERATED
BY
SOURCES
THAT
HUMANS
ARE
NOT
ACCUSTOMED
TO
ASSOCIATING
WITH
IMAGES
THESE
INCLUDE
ULTRA
SOUND
ELECTRON
MICROSCOPY
AND
COMPUTER
GENERATED
IMAGES
THUS
DIGITAL
IMAGE
PROCESSING
ENCOMPASSES
A
WIDE
AND
VARIED
FIELD
OF
APPLICATIONS
THERE
IS
NO
GENERAL
AGREEMENT
AMONG
AUTHORS
REGARDING
WHERE
IMAGE
PROCESSING
STOPS
AND
OTHER
RELATED
AREAS
SUCH
AS
IMAGE
ANALYSIS
AND
COMPUT
ER
VISION
START
SOMETIMES
A
DISTINCTION
IS
MADE
BY
DEFINING
IMAGE
PROCESSING
AS
A
DISCIPLINE
IN
WHICH
BOTH
THE
INPUT
AND
OUTPUT
OF
A
PROCESS
ARE
IMAGES
WE
BELIEVE
THIS
TO
BE
A
LIMITING
AND
SOMEWHAT
ARTIFICIAL
BOUNDARY
FOR
EXAMPLE
UNDER
THIS
DEFINITION
EVEN
THE
TRIVIAL
TASK
OF
COMPUTING
THE
AVERAGE
INTENSITY
OF
AN
IMAGE
WHICH
YIELDS
A
SINGLE
NUMBER
WOULD
NOT
BE
CONSIDERED
AN
IMAGE
PROCESSING
OPERATION
ON
THE
OTHER
HAND
THERE
ARE
FIELDS
SUCH
AS
COM
PUTER
VISION
WHOSE
ULTIMATE
GOAL
IS
TO
USE
COMPUTERS
TO
EMULATE
HUMAN
VI
SION
INCLUDING
LEARNING
AND
BEING
ABLE
TO
MAKE
INFERENCES
AND
TAKE
ACTIONS
BASED
ON
VISUAL
INPUTS
THIS
AREA
ITSELF
IS
A
BRANCH
OF
ARTIFICIAL
INTELLIGENCE
AI
WHOSE
OBJECTIVE
IS
TO
EMULATE
HUMAN
INTELLIGENCE
THE
FIELD
OF
AI
IS
IN
ITS
EARLIEST
STAGES
OF
INFANCY
IN
TERMS
OF
DEVELOPMENT
WITH
PROGRESS
HAVING
BEEN
MUCH
SLOWER
THAN
ORIGINALLY
ANTICIPATED
THE
AREA
OF
IMAGE
ANALYSIS
ALSO
CALLED
IMAGE
UNDERSTANDING
IS
IN
BETWEEN
IMAGE
PROCESSING
AND
COM
PUTER
VISION
THERE
ARE
NO
CLEAR
CUT
BOUNDARIES
IN
THE
CONTINUUM
FROM
IMAGE
PROCESSING
AT
ONE
END
TO
COMPUTER
VISION
AT
THE
OTHER
HOWEVER
ONE
USEFUL
PARADIGM
IS
TO
CONSIDER
THREE
TYPES
OF
COMPUTERIZED
PROCESSES
IN
THIS
CONTINUUM
LOW
MID
AND
HIGH
LEVEL
PROCESSES
LOW
LEVEL
PROCESSES
INVOLVE
PRIMITIVE
OPERA
TIONS
SUCH
AS
IMAGE
PREPROCESSING
TO
REDUCE
NOISE
CONTRAST
ENHANCEMENT
AND
IMAGE
SHARPENING
A
LOW
LEVEL
PROCESS
IS
CHARACTERIZED
BY
THE
FACT
THAT
BOTH
ITS
INPUTS
AND
OUTPUTS
ARE
IMAGES
MID
LEVEL
PROCESSING
ON
IMAGES
INVOLVES
TASKS
SUCH
AS
SEGMENTATION
PARTITIONING
AN
IMAGE
INTO
REGIONS
OR
OBJECTS
DE
SCRIPTION
OF
THOSE
OBJECTS
TO
REDUCE
THEM
TO
A
FORM
SUITABLE
FOR
COMPUTER
PRO
CESSING
AND
CLASSIFICATION
RECOGNITION
OF
INDIVIDUAL
OBJECTS
A
MID
LEVEL
PROCESS
IS
CHARACTERIZED
BY
THE
FACT
THAT
ITS
INPUTS
GENERALLY
ARE
IMAGES
BUT
ITS
OUTPUTS
ARE
ATTRIBUTES
EXTRACTED
FROM
THOSE
IMAGES
E
G
EDGES
CONTOURS
AND
THE
IDENTITY
OF
INDIVIDUAL
OBJECTS
FINALLY
HIGHER
LEVEL
PROCESSING
INVOLVES
MAKING
SENSE
OF
AN
ENSEMBLE
OF
RECOGNIZED
OBJECTS
AS
IN
IMAGE
ANALYSIS
AND
AT
THE
FAR
END
OF
THE
CONTINUUM
PERFORMING
THE
COGNITIVE
FUNCTIONS
NORMALLY
ASSOCIATED
WITH
VISION
BASED
ON
THE
PRECEDING
COMMENTS
WE
SEE
THAT
A
LOGICAL
PLACE
OF
OVERLAP
BE
TWEEN
IMAGE
PROCESSING
AND
IMAGE
ANALYSIS
IS
THE
AREA
OF
RECOGNITION
OF
INDI
VIDUAL
REGIONS
OR
OBJECTS
IN
AN
IMAGE
THUS
WHAT
WE
CALL
IN
THIS
BOOK
DIGITAL
IMAGE
PROCESSING
ENCOMPASSES
PROCESSES
WHOSE
INPUTS
AND
OUTPUTS
ARE
IMAGES
AND
IN
ADDITION
ENCOMPASSES
PROCESSES
THAT
EXTRACT
ATTRIBUTES
FROM
IMAGES
UP
TO
AND
INCLUDING
THE
RECOGNITION
OF
INDIVIDUAL
OBJECTS
AS
AN
ILLUSTRATION
TO
CLAR
IFY
THESE
CONCEPTS
CONSIDER
THE
AREA
OF
AUTOMATED
ANALYSIS
OF
TEXT
THE
PROCESSES
OF
ACQUIRING
AN
IMAGE
OF
THE
AREA
CONTAINING
THE
TEXT
PREPROCESSING
THAT
IMAGE
EXTRACTING
SEGMENTING
THE
INDIVIDUAL
CHARACTERS
DESCRIBING
THE
CHARACTERS
IN
A
FORM
SUITABLE
FOR
COMPUTER
PROCESSING
AND
RECOGNIZING
THOSE
INDIVIDUAL
CHARACTERS
ARE
IN
THE
SCOPE
OF
WHAT
WE
CALL
DIGITAL
IMAGE
PROCESSING
IN
THIS
BOOK
MAKING
SENSE
OF
THE
CONTENT
OF
THE
PAGE
MAY
BE
VIEWED
AS
BEING
IN
THE
DOMAIN
OF
IMAGE
ANALYSIS
AND
EVEN
COMPUTER
VISION
DEPENDING
ON
THE
LEVEL
OF
COMPLEXITY
IMPLIED
BY
THE
STATEMENT
MAKING
SENSE
AS
WILL
BECOME
EVIDENT
SHORTLY
DIGITAL
IMAGE
PROCESSING
AS
WE
HAVE
DEFINED
IT
IS
USED
SUCCESSFULLY
IN
A
BROAD
RANGE
OF
AREAS
OF
EXCEPTIONAL
SOCIAL
AND
ECONOMIC
VALUE
THE
CONCEPTS
DEVELOPED
IN
THE
FOLLOWING
CHAPTERS
ARE
THE
FOUNDATION
FOR
THE
METHODS
USED
IN
THOSE
APPLICATION
AREAS
THE
ORIGINS
OF
DIGITAL
IMAGE
PROCESSING
ONE
OF
THE
FIRST
APPLICATIONS
OF
DIGITAL
IMAGES
WAS
IN
THE
NEWSPAPER
INDUS
TRY
WHEN
PICTURES
WERE
FIRST
SENT
BY
SUBMARINE
CABLE
BETWEEN
LONDON
AND
NEW
YORK
INTRODUCTION
OF
THE
BARTLANE
CABLE
PICTURE
TRANSMISSION
SYSTEM
IN
THE
EARLY
REDUCED
THE
TIME
REQUIRED
TO
TRANSPORT
A
PICTURE
ACROSS
THE
ATLANTIC
FROM
MORE
THAN
A
WEEK
TO
LESS
THAN
THREE
HOURS
SPECIALIZED
PRINTING
EQUIPMENT
CODED
PICTURES
FOR
CABLE
TRANSMISSION
AND
THEN
RECON
STRUCTED
THEM
AT
THE
RECEIVING
END
FIGURE
WAS
TRANSMITTED
IN
THIS
WAY
AND
REPRODUCED
ON
A
TELEGRAPH
PRINTER
FITTED
WITH
TYPEFACES
SIMULATING
A
HALFTONE
PATTERN
SOME
OF
THE
INITIAL
PROBLEMS
IN
IMPROVING
THE
VISUAL
QUALITY
OF
THESE
EARLY
DIGITAL
PICTURES
WERE
RELATED
TO
THE
SELECTION
OF
PRINTING
PROCEDURES
AND
THE
DISTRIBUTION
OF
INTENSITY
LEVELS
THE
PRINTING
METHOD
USED
TO
OBTAIN
FIG
WAS
ABANDONED
TOWARD
THE
END
OF
IN
FAVOR
OF
A
TECHNIQUE
BASED
ON
PHOTO
GRAPHIC
REPRODUCTION
MADE
FROM
TAPES
PERFORATED
AT
THE
TELEGRAPH
RECEIVING
TERMINAL
FIGURE
SHOWS
AN
IMAGE
OBTAINED
USING
THIS
METHOD
THE
IMPROVE
MENTS
OVER
FIG
ARE
EVIDENT
BOTH
IN
TONAL
QUALITY
AND
IN
RESOLUTION
REFERENCES
IN
THE
BIBLIOGRAPHY
AT
THE
END
OF
THE
BOOK
ARE
LISTED
IN
ALPHABETICAL
ORDER
BY
AUTHORS
LAST
NAMES
FIGURE
A
DIGITAL
PICTURE
PRODUCED
IN
FROM
A
CODED
TAPE
BY
A
TELEGRAPH
PRINTER
WITH
SPECIAL
TYPE
FACES
MCFARLANE
FIGURE
A
DIGITAL
PICTURE
MADE
IN
FROM
A
TAPE
PUNCHED
AFTER
THE
SIGNALS
HAD
CROSSED
THE
ATLANTIC
TWICE
MCFARLANE
FIGURE
UNRETOUCHED
CABLE
PICTURE
OF
GENERALS
PERSHING
AND
FOCH
TRANSMITTED
IN
FROM
LONDON
TO
NEW
YORK
BY
TONE
EQUIPMENT
MCFARLANE
THE
EARLY
BARTLANE
SYSTEMS
WERE
CAPABLE
OF
CODING
IMAGES
IN
FIVE
DISTINCT
LEVELS
OF
GRAY
THIS
CAPABILITY
WAS
INCREASED
TO
LEVELS
IN
FIGURE
IS
TYPICAL
OF
THE
TYPE
OF
IMAGES
THAT
COULD
BE
OBTAINED
USING
THE
TONE
EQUIP
MENT
DURING
THIS
PERIOD
INTRODUCTION
OF
A
SYSTEM
FOR
DEVELOPING
A
FILM
PLATE
VIA
LIGHT
BEAMS
THAT
WERE
MODULATED
BY
THE
CODED
PICTURE
TAPE
IMPROVED
THE
REPRODUCTION
PROCESS
CONSIDERABLY
ALTHOUGH
THE
EXAMPLES
JUST
CITED
INVOLVE
DIGITAL
IMAGES
THEY
ARE
NOT
CON
SIDERED
DIGITAL
IMAGE
PROCESSING
RESULTS
IN
THE
CONTEXT
OF
OUR
DEFINITION
BE
CAUSE
COMPUTERS
WERE
NOT
INVOLVED
IN
THEIR
CREATION
THUS
THE
HISTORY
OF
DIGITAL
IMAGE
PROCESSING
IS
INTIMATELY
TIED
TO
THE
DEVELOPMENT
OF
THE
DIGITAL
COMPUTER
IN
FACT
DIGITAL
IMAGES
REQUIRE
SO
MUCH
STORAGE
AND
COMPUTATIONAL
POWER
THAT
PROGRESS
IN
THE
FIELD
OF
DIGITAL
IMAGE
PROCESSING
HAS
BEEN
DEPEN
DENT
ON
THE
DEVELOPMENT
OF
DIGITAL
COMPUTERS
AND
OF
SUPPORTING
TECHNOLOGIES
THAT
INCLUDE
DATA
STORAGE
DISPLAY
AND
TRANSMISSION
THE
IDEA
OF
A
COMPUTER
GOES
BACK
TO
THE
INVENTION
OF
THE
ABACUS
IN
ASIA
MINOR
MORE
THAN
YEARS
AGO
MORE
RECENTLY
THERE
WERE
DEVELOPMENTS
IN
THE
PAST
TWO
CENTURIES
THAT
ARE
THE
FOUNDATION
OF
WHAT
WE
CALL
A
COMPUTER
TODAY
HOWEVER
THE
BASIS
FOR
WHAT
WE
CALL
A
MODERN
DIGITAL
COMPUTER
DATES
BACK
TO
ONLY
THE
WITH
THE
INTRODUCTION
BY
JOHN
VON
NEUMANN
OF
TWO
KEY
CON
CEPTS
A
MEMORY
TO
HOLD
A
STORED
PROGRAM
AND
DATA
AND
CONDITIONAL
BRANCHING
THESE
TWO
IDEAS
ARE
THE
FOUNDATION
OF
A
CENTRAL
PROCESSING
UNIT
CPU
WHICH
IS
AT
THE
HEART
OF
COMPUTERS
TODAY
STARTING
WITH
VON
NEUMANN
THERE
WERE
A
SERIES
OF
KEY
ADVANCES
THAT
LED
TO
COMPUTERS
POWERFUL
ENOUGH
TO
BE
USED
FOR
DIGITAL
IMAGE
PROCESSING
BRIEFLY
THESE
ADVANCES
MAY
BE
SUMMA
RIZED
AS
FOLLOWS
THE
INVENTION
OF
THE
TRANSISTOR
AT
BELL
LABORATORIES
IN
THE
DEVELOPMENT
IN
THE
AND
OF
THE
HIGH
LEVEL
PROGRAMMING
LAN
GUAGES
COBOL
COMMON
BUSINESS
ORIENTED
LANGUAGE
AND
FORTRAN
FORMULA
TRANSLATOR
THE
INVENTION
OF
THE
INTEGRATED
CIRCUIT
IC
AT
TEXAS
INSTRUMENTS
IN
THE
DEVELOPMENT
OF
OPERATING
SYSTEMS
IN
THE
EARLY
THE
DEVELOPMENT
OF
THE
MICROPROCESSOR
A
SINGLE
CHIP
CONSISTING
OF
THE
CENTRAL
PROCESSING
UNIT
MEMORY
AND
INPUT
AND
OUTPUT
CONTROLS
BY
INTEL
IN
THE
EARLY
INTRODUCTION
BY
IBM
OF
THE
PERSONAL
COMPUTER
IN
AND
PROGRESSIVE
MINIATURIZATION
OF
COMPONENTS
STARTING
WITH
LARGE
SCALE
INTEGRA
TION
LI
IN
THE
LATE
THEN
VERY
LARGE
SCALE
INTEGRATION
VLSI
IN
THE
TO
THE
PRESENT
USE
OF
ULTRA
LARGE
SCALE
INTEGRATION
ULSI
CONCURRENT
WITH
THESE
ADVANCES
WERE
DEVELOPMENTS
IN
THE
AREAS
OF
MASS
STORAGE
AND
DISPLAY
SYS
TEMS
BOTH
OF
WHICH
ARE
FUNDAMENTAL
REQUIREMENTS
FOR
DIGITAL
IMAGE
PROCESSING
THE
FIRST
COMPUTERS
POWERFUL
ENOUGH
TO
CARRY
OUT
MEANINGFUL
IMAGE
PRO
CESSING
TASKS
APPEARED
IN
THE
EARLY
THE
BIRTH
OF
WHAT
WE
CALL
DIGITAL
IMAGE
PROCESSING
TODAY
CAN
BE
TRACED
TO
THE
AVAILABILITY
OF
THOSE
MACHINES
AND
TO
THE
ONSET
OF
THE
SPACE
PROGRAM
DURING
THAT
PERIOD
IT
TOOK
THE
COMBINATION
OF
THOSE
TWO
DEVELOPMENTS
TO
BRING
INTO
FOCUS
THE
POTENTIAL
OF
DIGITAL
IMAGE
PROCESSING
CONCEPTS
WORK
ON
USING
COMPUTER
TECHNIQUES
FOR
IMPROVING
IM
AGES
FROM
A
SPACE
PROBE
BEGAN
AT
THE
JET
PROPULSION
LABORATORY
PASADENA
CALIFORNIA
IN
WHEN
PICTURES
OF
THE
MOON
TRANSMITTED
BY
RANGER
WERE
PROCESSED
BY
A
COMPUTER
TO
CORRECT
VARIOUS
TYPES
OF
IMAGE
DISTORTION
INHERENT
IN
THE
ON
BOARD
TELEVISION
CAMERA
FIGURE
SHOWS
THE
FIRST
IMAGE
OF
THE
MOON
TAKEN
BY
RANGER
ON
JULY
AT
A
M
EASTERN
DAYLIGHT
TIME
EDT
ABOUT
MINUTES
BEFORE
IMPACTING
THE
LUNAR
SURFACE
THE
MARKERS
CALLED
RESEAU
MARKS
ARE
USED
FOR
GEOMETRIC
CORRECTIONS
AS
DISCUSSED
IN
CHAPTER
THIS
ALSO
IS
THE
FIRST
IMAGE
OF
THE
MOON
TAKEN
BY
A
U
SPACECRAFT
THE
IMAGING
LESSONS
LEARNED
WITH
RANGER
SERVED
AS
THE
BASIS
FOR
IMPROVED
METHODS
USED
TO
ENHANCE
AND
RESTORE
IMAGES
FROM
THE
SURVEYOR
MISSIONS
TO
THE
MOON
THE
MARINER
SERIES
OF
FLYBY
MISSIONS
TO
MARS
THE
APOLLO
MANNED
FLIGHTS
TO
THE
MOON
AND
OTHERS
FIGURE
THE
FIRST
PICTURE
OF
THE
MOON
BY
A
U
SPACECRAFT
RANGER
TOOK
THIS
IMAGE
ON
JULY
AT
A
M
EDT
ABOUT
MINUTES
BEFORE
IMPACTING
THE
LUNAR
SURFACE
COURTESY
OF
NASA
IN
PARALLEL
WITH
SPACE
APPLICATIONS
DIGITAL
IMAGE
PROCESSING
TECHNIQUES
BEGAN
IN
THE
LATE
AND
EARLY
TO
BE
USED
IN
MEDICAL
IMAGING
REMOTE
EARTH
RESOURCES
OBSERVATIONS
AND
ASTRONOMY
THE
INVENTION
IN
THE
EARLY
OF
COMPUTERIZED
AXIAL
TOMOGRAPHY
CAT
ALSO
CALLED
COMPUTERIZED
TOMOGRA
PHY
CT
FOR
SHORT
IS
ONE
OF
THE
MOST
IMPORTANT
EVENTS
IN
THE
APPLICATION
OF
IMAGE
PROCESSING
IN
MEDICAL
DIAGNOSIS
COMPUTERIZED
AXIAL
TOMOGRAPHY
IS
A
PROCESS
IN
WHICH
A
RING
OF
DETECTORS
ENCIRCLES
AN
OBJECT
OR
PATIENT
AND
AN
X
RAY
SOURCE
CONCENTRIC
WITH
THE
DETECTOR
RING
ROTATES
ABOUT
THE
OBJECT
THE
X
RAYS
PASS
THROUGH
THE
OBJECT
AND
ARE
COLLECTED
AT
THE
OPPOSITE
END
BY
THE
CORRESPONDING
DETECTORS
IN
THE
RING
AS
THE
SOURCE
ROTATES
THIS
PROCEDURE
IS
RE
PEATED
TOMOGRAPHY
CONSISTS
OF
ALGORITHMS
THAT
USE
THE
SENSED
DATA
TO
CON
STRUCT
AN
IMAGE
THAT
REPRESENTS
A
SLICE
THROUGH
THE
OBJECT
MOTION
OF
THE
OBJECT
IN
A
DIRECTION
PERPENDICULAR
TO
THE
RING
OF
DETECTORS
PRODUCES
A
SET
OF
SUCH
SLICES
WHICH
CONSTITUTE
A
THREE
DIMENSIONAL
D
RENDITION
OF
THE
INSIDE
OF
THE
OBJECT
TOMOGRAPHY
WAS
INVENTED
INDEPENDENTLY
BY
SIR
GODFREY
N
HOUNSFIELD
AND
PROFESSOR
ALLAN
M
CORMACK
WHO
SHARED
THE
NOBEL
PRIZE
IN
MEDICINE
FOR
THEIR
INVENTION
IT
IS
INTERESTING
TO
NOTE
THAT
X
RAYS
WERE
DISCOVERED
IN
BY
WILHELM
CONRAD
ROENTGEN
FOR
WHICH
HE
RECEIVED
THE
NOBEL
PRIZE
FOR
PHYSICS
THESE
TWO
INVENTIONS
NEARLY
YEARS
APART
LED
TO
SOME
OF
THE
MOST
IMPORTANT
APPLICATIONS
OF
IMAGE
PROCESSING
TODAY
FROM
THE
UNTIL
THE
PRESENT
THE
FIELD
OF
IMAGE
PROCESSING
HAS
GROWN
VIGOROUSLY
IN
ADDITION
TO
APPLICATIONS
IN
MEDICINE
AND
THE
SPACE
PROGRAM
DIG
ITAL
IMAGE
PROCESSING
TECHNIQUES
NOW
ARE
USED
IN
A
BROAD
RANGE
OF
APPLICA
TIONS
COMPUTER
PROCEDURES
ARE
USED
TO
ENHANCE
THE
CONTRAST
OR
CODE
THE
INTENSITY
LEVELS
INTO
COLOR
FOR
EASIER
INTERPRETATION
OF
X
RAYS
AND
OTHER
IMAGES
USED
IN
INDUSTRY
MEDICINE
AND
THE
BIOLOGICAL
SCIENCES
GEOGRAPHERS
USE
THE
SAME
OR
SIMILAR
TECHNIQUES
TO
STUDY
POLLUTION
PATTERNS
FROM
AERIAL
AND
SATELLITE
IMAGERY
IMAGE
ENHANCEMENT
AND
RESTORATION
PROCEDURES
ARE
USED
TO
PROCESS
DEGRADED
IMAGES
OF
UNRECOVERABLE
OBJECTS
OR
EXPERIMENTAL
RESULTS
TOO
EXPEN
SIVE
TO
DUPLICATE
IN
ARCHEOLOGY
IMAGE
PROCESSING
METHODS
HAVE
SUCCESSFULLY
RESTORED
BLURRED
PICTURES
THAT
WERE
THE
ONLY
AVAILABLE
RECORDS
OF
RARE
ARTIFACTS
LOST
OR
DAMAGED
AFTER
BEING
PHOTOGRAPHED
IN
PHYSICS
AND
RELATED
FIELDS
COM
PUTER
TECHNIQUES
ROUTINELY
ENHANCE
IMAGES
OF
EXPERIMENTS
IN
AREAS
SUCH
AS
HIGH
ENERGY
PLASMAS
AND
ELECTRON
MICROSCOPY
SIMILARLY
SUCCESSFUL
APPLICA
TIONS
OF
IMAGE
PROCESSING
CONCEPTS
CAN
BE
FOUND
IN
ASTRONOMY
BIOLOGY
NUCLEAR
MEDICINE
LAW
ENFORCEMENT
DEFENSE
AND
INDUSTRY
THESE
EXAMPLES
ILLUSTRATE
PROCESSING
RESULTS
INTENDED
FOR
HUMAN
INTERPRETA
TION
THE
SECOND
MAJOR
AREA
OF
APPLICATION
OF
DIGITAL
IMAGE
PROCESSING
TECH
NIQUES
MENTIONED
AT
THE
BEGINNING
OF
THIS
CHAPTER
IS
IN
SOLVING
PROBLEMS
DEALING
WITH
MACHINE
PERCEPTION
IN
THIS
CASE
INTEREST
IS
ON
PROCEDURES
FOR
EXTRACTING
FROM
AN
IMAGE
INFORMATION
IN
A
FORM
SUITABLE
FOR
COMPUTER
PROCESSING
OFTEN
THIS
INFORMATION
BEARS
LITTLE
RESEMBLANCE
TO
VISUAL
FEATURES
THAT
HUMANS
USE
IN
INTERPRETING
THE
CONTENT
OF
AN
IMAGE
EXAMPLES
OF
THE
TYPE
OF
INFORMATION
USED
IN
MACHINE
PERCEPTION
ARE
STATISTICAL
MOMENTS
FOURIER
TRANSFORM
COEFFICIENTS
AND
MULTIDIMENSIONAL
DISTANCE
MEASURES
TYPICAL
PROBLEMS
IN
MACHINE
PERCEP
TION
THAT
ROUTINELY
UTILIZE
IMAGE
PROCESSING
TECHNIQUES
ARE
AUTOMATIC
CHARACTER
RECOGNITION
INDUSTRIAL
MACHINE
VISION
FOR
PRODUCT
ASSEMBLY
AND
INSPECTION
MILITARY
RECOGNIZANCE
AUTOMATIC
PROCESSING
OF
FINGERPRINTS
SCREENING
OF
X
RAYS
AND
BLOOD
SAMPLES
AND
MACHINE
PROCESSING
OF
AERIAL
AND
SATELLITE
IMAGERY
FOR
WEATHER
PREDICTION
AND
ENVIRONMENTAL
ASSESSMENT
THE
CONTINUING
DECLINE
IN
THE
RATIO
OF
COMPUTER
PRICE
TO
PERFORMANCE
AND
THE
EXPANSION
OF
NETWORKING
AND
COMMUNICATION
BANDWIDTH
VIA
THE
WORLD
WIDE
WEB
AND
THE
INTERNET
HAVE
CRE
ATED
UNPRECEDENTED
OPPORTUNITIES
FOR
CONTINUED
GROWTH
OF
DIGITAL
IMAGE
PRO
CESSING
SOME
OF
THESE
APPLICATION
AREAS
ARE
ILLUSTRATED
IN
THE
FOLLOWING
SECTION
EXAMPLES
OF
FIELDS
THAT
USE
DIGITAL
IMAGE
PROCESSING
TODAY
THERE
IS
ALMOST
NO
AREA
OF
TECHNICAL
ENDEAVOR
THAT
IS
NOT
IMPACTED
IN
SOME
WAY
BY
DIGITAL
IMAGE
PROCESSING
WE
CAN
COVER
ONLY
A
FEW
OF
THESE
APPLI
CATIONS
IN
THE
CONTEXT
AND
SPACE
OF
THE
CURRENT
DISCUSSION
HOWEVER
LIMITED
AS
IT
IS
THE
MATERIAL
PRESENTED
IN
THIS
SECTION
WILL
LEAVE
NO
DOUBT
IN
YOUR
MIND
RE
GARDING
THE
BREADTH
AND
IMPORTANCE
OF
DIGITAL
IMAGE
PROCESSING
WE
SHOW
IN
THIS
SECTION
NUMEROUS
AREAS
OF
APPLICATION
EACH
OF
WHICH
ROUTINELY
UTILIZES
THE
DIGITAL
IMAGE
PROCESSING
TECHNIQUES
DEVELOPED
IN
THE
FOLLOWING
CHAPTERS
MANY
OF
THE
IMAGES
SHOWN
IN
THIS
SECTION
ARE
USED
LATER
IN
ONE
OR
MORE
OF
THE
EXAM
PLES
GIVEN
IN
THE
BOOK
ALL
IMAGES
SHOWN
ARE
DIGITAL
THE
AREAS
OF
APPLICATION
OF
DIGITAL
IMAGE
PROCESSING
ARE
SO
VARIED
THAT
SOME
FORM
OF
ORGANIZATION
IS
DESIRABLE
IN
ATTEMPTING
TO
CAPTURE
THE
BREADTH
OF
THIS
FIELD
ONE
OF
THE
SIMPLEST
WAYS
TO
DEVELOP
A
BASIC
UNDERSTANDING
OF
THE
EXTENT
OF
IMAGE
PROCESSING
APPLICATIONS
IS
TO
CATEGORIZE
IMAGES
ACCORDING
TO
THEIR
SOURCE
E
G
VISUAL
X
RAY
AND
SO
ON
THE
PRINCIPAL
ENERGY
SOURCE
FOR
IMAGES
IN
USE
TODAY
IS
THE
ELECTROMAGNETIC
ENERGY
SPECTRUM
OTHER
IMPORTANT
SOURCES
OF
ENERGY
IN
CLUDE
ACOUSTIC
ULTRASONIC
AND
ELECTRONIC
IN
THE
FORM
OF
ELECTRON
BEAMS
USED
IN
ELECTRON
MICROSCOPY
SYNTHETIC
IMAGES
USED
FOR
MODELING
AND
VISUALIZATION
ARE
GENERATED
BY
COMPUTER
IN
THIS
SECTION
WE
DISCUSS
BRIEFLY
HOW
IMAGES
ARE
GENER
ATED
IN
THESE
VARIOUS
CATEGORIES
AND
THE
AREAS
IN
WHICH
THEY
ARE
APPLIED
METHODS
FOR
CONVERTING
IMAGES
INTO
DIGITAL
FORM
ARE
DISCUSSED
IN
THE
NEXT
CHAPTER
IMAGES
BASED
ON
RADIATION
FROM
THE
EM
SPECTRUM
ARE
THE
MOST
FAMILIAR
ESPECIALLY
IMAGES
IN
THE
X
RAY
AND
VISUAL
BANDS
OF
THE
SPECTRUM
ELECTROMAG
NETIC
WAVES
CAN
BE
CONCEPTUALIZED
AS
PROPAGATING
SINUSOIDAL
WAVES
OF
VARYING
WAVELENGTHS
OR
THEY
CAN
BE
THOUGHT
OF
AS
A
STREAM
OF
MASSLESS
PARTICLES
EACH
TRAVELING
IN
A
WAVELIKE
PATTERN
AND
MOVING
AT
THE
SPEED
OF
LIGHT
EACH
MASS
LESS
PARTICLE
CONTAINS
A
CERTAIN
AMOUNT
OR
BUNDLE
OF
ENERGY
EACH
BUNDLE
OF
ENERGY
IS
CALLED
A
PHOTON
IF
SPECTRAL
BANDS
ARE
GROUPED
ACCORDING
TO
ENERGY
PER
PHOTON
WE
OBTAIN
THE
SPECTRUM
SHOWN
IN
FIG
RANGING
FROM
GAMMA
RAYS
HIGHEST
ENERGY
AT
ONE
END
TO
RADIO
WAVES
LOWEST
ENERGY
AT
THE
OTHER
ENERGY
OF
ONE
PHOTON
ELECTRON
VOLTS
GAMMA
RAYS
X
RAYS
ULTRAVIOLET
VISIBLE
INFRARED
MICROWAVES
RADIO
WAVES
FIGURE
THE
ELECTROMAGNETIC
SPECTRUM
ARRANGED
ACCORDING
TO
ENERGY
PER
PHOTON
FIGURE
EXAMPLES
OF
GAMMA
RAY
IMAGING
A
BONE
SCAN
B
PET
IMAGE
C
CYGNUS
LOOP
D
GAMMA
RADIATION
BRIGHT
SPOT
FROM
A
REACTOR
VALVE
IMAGES
COURTESY
OF
A
G
E
MEDICAL
SYSTEMS
B
DR
MICHAEL
E
CASEY
CTI
PET
SYSTEMS
C
NASA
D
PROFESSORS
ZHONG
HE
AND
DAVID
K
WEHE
UNIVERSITY
OF
MICHIGAN
THE
BANDS
ARE
SHOWN
SHADED
TO
CONVEY
THE
FACT
THAT
BANDS
OF
THE
EM
SPEC
TRUM
ARE
NOT
DISTINCT
BUT
RATHER
TRANSITION
SMOOTHLY
FROM
ONE
TO
THE
OTHER
GAMMA
RAY
IMAGING
MAJOR
USES
OF
IMAGING
BASED
ON
GAMMA
RAYS
INCLUDE
NUCLEAR
MEDICINE
AND
AS
TRONOMICAL
OBSERVATIONS
IN
NUCLEAR
MEDICINE
THE
APPROACH
IS
TO
INJECT
A
PA
TIENT
WITH
A
RADIOACTIVE
ISOTOPE
THAT
EMITS
GAMMA
RAYS
AS
IT
DECAYS
IMAGES
ARE
PRODUCED
FROM
THE
EMISSIONS
COLLECTED
BY
GAMMA
RAY
DETECTORS
FIGURE
A
SHOWS
AN
IMAGE
OF
A
COMPLETE
BONE
SCAN
OBTAINED
BY
USING
GAMMA
RAY
IMAGING
IMAGES
OF
THIS
SORT
ARE
USED
TO
LOCATE
SITES
OF
BONE
PATHOLOGY
SUCH
AS
INFECTIONS
OR
TUMORS
FIGURE
B
SHOWS
ANOTHER
MAJOR
MODALITY
OF
NUCLEAR
IMAGING
CALLED
POSITRON
EMISSION
TOMOGRAPHY
PET
THE
PRINCIPLE
IS
THE
SAME
AS
WITH
X
RAY
TOMOGRAPHY
MENTIONED
BRIEFLY
IN
SECTION
HOWEVER
INSTEAD
OF
USING
AN
EXTERNAL
SOURCE
OF
X
RAY
ENERGY
THE
PATIENT
IS
GIVEN
A
RADIOACTIVE
ISOTOPE
THAT
EMITS
POSITRONS
AS
IT
DECAYS
WHEN
A
POSITRON
MEETS
AN
ELECTRON
BOTH
ARE
ANNIHILATED
AND
TWO
GAMMA
RAYS
ARE
GIVEN
OFF
THESE
ARE
DETECTED
AND
A
TOMO
GRAPHIC
IMAGE
IS
CREATED
USING
THE
BASIC
PRINCIPLES
OF
TOMOGRAPHY
THE
IMAGE
SHOWN
IN
FIG
B
IS
ONE
SAMPLE
OF
A
SEQUENCE
THAT
CONSTITUTES
A
D
RENDITION
OF
THE
PATIENT
THIS
IMAGE
SHOWS
A
TUMOR
IN
THE
BRAIN
AND
ONE
IN
THE
LUNG
EASILY
VISIBLE
AS
SMALL
WHITE
MASSES
A
STAR
IN
THE
CONSTELLATION
OF
CYGNUS
EXPLODED
ABOUT
YEARS
AGO
GENER
ATING
A
SUPERHEATED
STATIONARY
GAS
CLOUD
KNOWN
AS
THE
CYGNUS
LOOP
THAT
GLOWS
IN
A
SPECTACULAR
ARRAY
OF
COLORS
FIGURE
C
SHOWS
AN
IMAGE
OF
THE
CYGNUS
LOOP
IN
THE
GAMMA
RAY
BAND
UNLIKE
THE
TWO
EXAMPLES
IN
FIGS
A
AND
B
THIS
IMAGE
WAS
OBTAINED
USING
THE
NATURAL
RADIATION
OF
THE
OBJECT
BEING
IMAGED
FINALLY
FIG
D
SHOWS
AN
IMAGE
OF
GAMMA
RADIATION
FROM
A
VALVE
IN
A
NUCLEAR
REACTOR
AN
AREA
OF
STRONG
RADIATION
IS
SEEN
IN
THE
LOWER
LEFT
SIDE
OF
THE
IMAGE
X
RAY
IMAGING
X
RAYS
ARE
AMONG
THE
OLDEST
SOURCES
OF
EM
RADIATION
USED
FOR
IMAGING
THE
BEST
KNOWN
USE
OF
X
RAYS
IS
MEDICAL
DIAGNOSTICS
BUT
THEY
ALSO
ARE
USED
EXTEN
SIVELY
IN
INDUSTRY
AND
OTHER
AREAS
LIKE
ASTRONOMY
X
RAYS
FOR
MEDICAL
AND
IN
DUSTRIAL
IMAGING
ARE
GENERATED
USING
AN
X
RAY
TUBE
WHICH
IS
A
VACUUM
TUBE
WITH
A
CATHODE
AND
ANODE
THE
CATHODE
IS
HEATED
CAUSING
FREE
ELECTRONS
TO
BE
RELEASED
THESE
ELECTRONS
FLOW
AT
HIGH
SPEED
TO
THE
POSITIVELY
CHARGED
ANODE
WHEN
THE
ELECTRONS
STRIKE
A
NUCLEUS
ENERGY
IS
RELEASED
IN
THE
FORM
OF
X
RAY
RADIATION
THE
ENERGY
PENETRATING
POWER
OF
X
RAYS
IS
CONTROLLED
BY
A
VOLTAGE
APPLIED
ACROSS
THE
ANODE
AND
BY
A
CURRENT
APPLIED
TO
THE
FILAMENT
IN
THE
CATHODE
FIGURE
A
SHOWS
A
FAMILIAR
CHEST
X
RAY
GENERATED
SIMPLY
BY
PLAC
ING
THE
PATIENT
BETWEEN
AN
X
RAY
SOURCE
AND
A
FILM
SENSITIVE
TO
X
RAY
ENERGY
THE
INTENSITY
OF
THE
X
RAYS
IS
MODIFIED
BY
ABSORPTION
AS
THEY
PASS
THROUGH
THE
PATIENT
AND
THE
RESULTING
ENERGY
FALLING
ON
THE
FILM
DEVELOPS
IT
MUCH
IN
THE
SAME
WAY
THAT
LIGHT
DEVELOPS
PHOTOGRAPHIC
FILM
IN
DIGITAL
RADIOGRAPHY
DIGITAL
IMAGES
ARE
OBTAINED
BY
ONE
OF
TWO
METHODS
BY
DIGITIZING
X
RAY
FILMS
OR
BY
HAVING
THE
X
RAYS
THAT
PASS
THROUGH
THE
PATIENT
FALL
DIRECTLY
ONTO
DEVICES
SUCH
AS
A
PHOSPHOR
SCREEN
THAT
CONVERT
X
RAYS
TO
LIGHT
THE
LIGHT
SIGNAL
IN
TURN
IS
CAPTURED
BY
A
LIGHT
SENSITIVE
DIGITIZING
SYSTEM
WE
DISCUSS
DIGITIZATION
IN
MORE
DETAIL
IN
CHAPTERS
AND
ANGIOGRAPHY
IS
ANOTHER
MAJOR
APPLICATION
IN
AN
AREA
CALLED
CONTRAST
ENHANCEMENT
RADIOGRAPHY
THIS
PROCEDURE
IS
USED
TO
OBTAIN
IMAGES
CALLED
ANGIOGRAMS
OF
BLOOD
VESSELS
A
CATHETER
A
SMALL
FLEXIBLE
HOLLOW
TUBE
IS
IN
SERTED
FOR
EXAMPLE
INTO
AN
ARTERY
OR
VEIN
IN
THE
GROIN
THE
CATHETER
IS
THREADED
INTO
THE
BLOOD
VESSEL
AND
GUIDED
TO
THE
AREA
TO
BE
STUDIED
WHEN
THE
CATHETER
REACHES
THE
SITE
UNDER
INVESTIGATION
AN
X
RAY
CONTRAST
MEDIUM
IS
INJECTED
THROUGH
THE
TUBE
THIS
ENHANCES
CONTRAST
OF
THE
BLOOD
VESSELS
AND
ENABLES
THE
RADIOLOGIST
TO
SEE
ANY
IRREGULARITIES
OR
BLOCKAGES
FIGURE
B
SHOWS
AN
EXAM
PLE
OF
AN
AORTIC
ANGIOGRAM
THE
CATHETER
CAN
BE
SEEN
BEING
INSERTED
INTO
THE
FIGURE
EXAMPLES
OF
X
RAY
IMAGING
A
CHEST
X
RAY
B
AORTIC
ANGIOGRAM
C
HEAD
CT
D
CIRCUIT
BOARDS
E
CYGNUS
LOOP
IMAGES
COURTESY
OF
A
AND
C
DR
DAVID
R
PICKENS
DEPT
OF
RADIOLOGY
RADIOLOGICAL
SCIENCES
VANDERBILT
UNIVERSITY
MEDICAL
CENTER
B
DR
THOMAS
R
GEST
DIVISION
OF
ANATOMICAL
SCIENCES
UNIVERSITY
OF
MICHIGAN
MEDICAL
SCHOOL
D
MR
JOSEPH
E
PASCENTE
LIXI
INC
AND
E
NASA
LARGE
BLOOD
VESSEL
ON
THE
LOWER
LEFT
OF
THE
PICTURE
NOTE
THE
HIGH
CONTRAST
OF
THE
LARGE
VESSEL
AS
THE
CONTRAST
MEDIUM
FLOWS
UP
IN
THE
DIRECTION
OF
THE
KIDNEYS
WHICH
ARE
ALSO
VISIBLE
IN
THE
IMAGE
AS
DISCUSSED
IN
CHAPTER
ANGIOGRAPHY
IS
A
MAJOR
AREA
OF
DIGITAL
IMAGE
PROCESSING
WHERE
IMAGE
SUBTRACTION
IS
USED
TO
EN
HANCE
FURTHER
THE
BLOOD
VESSELS
BEING
STUDIED
ANOTHER
IMPORTANT
USE
OF
X
RAYS
IN
MEDICAL
IMAGING
IS
COMPUTERIZED
AXIAL
TO
MOGRAPHY
CAT
DUE
TO
THEIR
RESOLUTION
AND
D
CAPABILITIES
CAT
SCANS
REVO
LUTIONIZED
MEDICINE
FROM
THE
MOMENT
THEY
FIRST
BECAME
AVAILABLE
IN
THE
EARLY
AS
NOTED
IN
SECTION
EACH
CAT
IMAGE
IS
A
SLICE
TAKEN
PERPENDICULARLY
THROUGH
THE
PATIENT
NUMEROUS
SLICES
ARE
GENERATED
AS
THE
PATIENT
IS
MOVED
IN
A
LONGITUDINAL
DIRECTION
THE
ENSEMBLE
OF
SUCH
IMAGES
CONSTITUTES
A
D
RENDITION
OF
THE
INSIDE
OF
THE
BODY
WITH
THE
LONGITUDINAL
RESOLUTION
BEING
PROPORTIONAL
TO
THE
NUMBER
OF
SLICE
IMAGES
TAKEN
FIGURE
C
SHOWS
A
TYPICAL
HEAD
CAT
SLICE
IMAGE
TECHNIQUES
SIMILAR
TO
THE
ONES
JUST
DISCUSSED
BUT
GENERALLY
INVOLVING
HIGHER
ENERGY
X
RAYS
ARE
APPLICABLE
IN
INDUSTRIAL
PROCESSES
FIGURE
D
SHOWS
AN
X
RAY
IMAGE
OF
AN
ELECTRONIC
CIRCUIT
BOARD
SUCH
IMAGES
REPRESENTATIVE
OF
LITERALLY
HUN
DREDS
OF
INDUSTRIAL
APPLICATIONS
OF
X
RAYS
ARE
USED
TO
EXAMINE
CIRCUIT
BOARDS
FOR
FLAWS
IN
MANUFACTURING
SUCH
AS
MISSING
COMPONENTS
OR
BROKEN
TRACES
INDUSTRIAL
CAT
SCANS
ARE
USEFUL
WHEN
THE
PARTS
CAN
BE
PENETRATED
BY
X
RAYS
SUCH
AS
IN
PLASTIC
ASSEMBLIES
AND
EVEN
LARGE
BODIES
LIKE
SOLID
PROPELLANT
ROCKET
MOTORS
FIGURE
E
SHOWS
AN
EXAMPLE
OF
X
RAY
IMAGING
IN
ASTRONOMY
THIS
IMAGE
IS
THE
CYGNUS
LOOP
OF
FIG
C
BUT
IMAGED
THIS
TIME
IN
THE
X
RAY
BAND
IMAGING
IN
THE
ULTRAVIOLET
BAND
APPLICATIONS
OF
ULTRAVIOLET
LIGHT
ARE
VARIED
THEY
INCLUDE
LITHOGRAPHY
INDUSTRIAL
INSPECTION
MICROSCOPY
LASERS
BIOLOGICAL
IMAGING
AND
ASTRONOMICAL
OBSERVATIONS
WE
ILLUSTRATE
IMAGING
IN
THIS
BAND
WITH
EXAMPLES
FROM
MICROSCOPY
AND
ASTRONOMY
ULTRAVIOLET
LIGHT
IS
USED
IN
FLUORESCENCE
MICROSCOPY
ONE
OF
THE
FASTEST
GROW
ING
AREAS
OF
MICROSCOPY
FLUORESCENCE
IS
A
PHENOMENON
DISCOVERED
IN
THE
MID
DLE
OF
THE
NINETEENTH
CENTURY
WHEN
IT
WAS
FIRST
OBSERVED
THAT
THE
MINERAL
FLUORSPAR
FLUORESCES
WHEN
ULTRAVIOLET
LIGHT
IS
DIRECTED
UPON
IT
THE
ULTRAVIOLET
LIGHT
ITSELF
IS
NOT
VISIBLE
BUT
WHEN
A
PHOTON
OF
ULTRAVIOLET
RADIATION
COLLIDES
WITH
AN
ELECTRON
IN
AN
ATOM
OF
A
FLUORESCENT
MATERIAL
IT
ELEVATES
THE
ELECTRON
TO
A
HIGHER
ENERGY
LEVEL
SUBSEQUENTLY
THE
EXCITED
ELECTRON
RELAXES
TO
A
LOWER
LEVEL
AND
EMITS
LIGHT
IN
THE
FORM
OF
A
LOWER
ENERGY
PHOTON
IN
THE
VISIBLE
RED
LIGHT
REGION
THE
BASIC
TASK
OF
THE
FLUORESCENCE
MICROSCOPE
IS
TO
USE
AN
EXCITATION
LIGHT
TO
IRRADIATE
A
PREPARED
SPECIMEN
AND
THEN
TO
SEPARATE
THE
MUCH
WEAKER
RADIATING
FLUORES
CENT
LIGHT
FROM
THE
BRIGHTER
EXCITATION
LIGHT
THUS
ONLY
THE
EMISSION
LIGHT
REACHES
THE
EYE
OR
OTHER
DETECTOR
THE
RESULTING
FLUORESCING
AREAS
SHINE
AGAINST
A
DARK
BACKGROUND
WITH
SUFFICIENT
CONTRAST
TO
PERMIT
DETECTION
THE
DARKER
THE
BACK
GROUND
OF
THE
NONFLUORESCING
MATERIAL
THE
MORE
EFFICIENT
THE
INSTRUMENT
FLUORESCENCE
MICROSCOPY
IS
AN
EXCELLENT
METHOD
FOR
STUDYING
MATERIALS
THAT
CAN
BE
MADE
TO
FLUORESCE
EITHER
IN
THEIR
NATURAL
FORM
PRIMARY
FLUORESCENCE
OR
WHEN
TREATED
WITH
CHEMICALS
CAPABLE
OF
FLUORESCING
SECONDARY
FLUORESCENCE
FIGURES
A
AND
B
SHOW
RESULTS
TYPICAL
OF
THE
CAPABILITY
OF
FLUORESCENCE
MICROSCOPY
FIGURE
A
SHOWS
A
FLUORESCENCE
MICROSCOPE
IMAGE
OF
NORMAL
CORN
AND
FIG
B
SHOWS
CORN
INFECTED
BY
SMUT
A
DISEASE
OF
CEREALS
CORN
A
B
C
FIGURE
EXAMPLES
OF
ULTRAVIOLET
IMAGING
A
NORMAL
CORN
B
SMUT
CORN
C
CYGNUS
LOOP
IMAGES
COURTESY
OF
A
AND
B
DR
MICHAEL
W
DAVIDSON
FLORIDA
STATE
UNIVERSITY
C
NASA
GRASSES
ONIONS
AND
SORGHUM
THAT
CAN
BE
CAUSED
BY
ANY
OF
MORE
THAN
SPECIES
OF
PARASITIC
FUNGI
CORN
SMUT
IS
PARTICULARLY
HARMFUL
BECAUSE
CORN
IS
ONE
OF
THE
PRINCIPAL
FOOD
SOURCES
IN
THE
WORLD
AS
ANOTHER
ILLUSTRATION
FIG
C
SHOWS
THE
CYGNUS
LOOP
IMAGED
IN
THE
HIGH
ENERGY
REGION
OF
THE
ULTRAVIOLET
BAND
IMAGING
IN
THE
VISIBLE
AND
INFRARED
BANDS
CONSIDERING
THAT
THE
VISUAL
BAND
OF
THE
ELECTROMAGNETIC
SPECTRUM
IS
THE
MOST
FAMILIAR
IN
ALL
OUR
ACTIVITIES
IT
IS
NOT
SURPRISING
THAT
IMAGING
IN
THIS
BAND
OUT
WEIGHS
BY
FAR
ALL
THE
OTHERS
IN
TERMS
OF
BREADTH
OF
APPLICATION
THE
INFRARED
BAND
OFTEN
IS
USED
IN
CONJUNCTION
WITH
VISUAL
IMAGING
SO
WE
HAVE
GROUPED
THE
VISIBLE
AND
INFRARED
BANDS
IN
THIS
SECTION
FOR
THE
PURPOSE
OF
ILLUSTRATION
WE
CONSIDER
IN
THE
FOLLOWING
DISCUSSION
APPLICATIONS
IN
LIGHT
MICROSCOPY
ASTRONO
MY
REMOTE
SENSING
INDUSTRY
AND
LAW
ENFORCEMENT
FIGURE
SHOWS
SEVERAL
EXAMPLES
OF
IMAGES
OBTAINED
WITH
A
LIGHT
MICROSCOPE
THE
EXAMPLES
RANGE
FROM
PHARMACEUTICALS
AND
MICROINSPECTION
TO
MATERIALS
CHARACTERIZATION
EVEN
IN
MICROSCOPY
ALONE
THE
APPLICATION
AREAS
ARE
TOO
NUMER
OUS
TO
DETAIL
HERE
IT
IS
NOT
DIFFICULT
TO
CONCEPTUALIZE
THE
TYPES
OF
PROCESSES
ONE
MIGHT
APPLY
TO
THESE
IMAGES
RANGING
FROM
ENHANCEMENT
TO
MEASUREMENTS
A
B
C
D
E
F
FIGURE
EXAMPLES
OF
LIGHT
MICROSCOPY
IMAGES
A
TAXOL
ANTICANCER
AGENT
MAGNIFIED
B
CHOLESTEROL
C
MICROPROCESSOR
D
NICKEL
OXIDE
THIN
FILM
E
SURFACE
OF
AUDIO
CD
F
ORGANIC
SUPERCONDUCTOR
IMAGES
COURTESY
OF
DR
MICHAEL
W
DAVIDSON
FLORIDA
STATE
UNIVERSITY
TABLE
THEMATIC
BANDS
IN
NASA
LANDSAT
SATELLITE
BAND
NO
NAME
WAVELENGTH
ΜM
CHARACTERISTICS
AND
USES
VISIBLE
BLUE
MAXIMUM
WATER
PENETRATION
VISIBLE
GREEN
GOOD
FOR
MEASURING
PLANT
VIGOR
VISIBLE
RED
VEGETATION
DISCRIMINATION
NEAR
INFRARED
BIOMASS
AND
SHORELINE
MAPPING
MIDDLE
INFRARED
MOISTURE
CONTENT
OF
SOIL
AND
VEGETATION
THERMAL
INFRARED
SOIL
MOISTURE
THERMAL
MAPPING
MIDDLE
INFRARED
MINERAL
MAPPING
ANOTHER
MAJOR
AREA
OF
VISUAL
PROCESSING
IS
REMOTE
SENSING
WHICH
USUALLY
IN
CLUDES
SEVERAL
BANDS
IN
THE
VISUAL
AND
INFRARED
REGIONS
OF
THE
SPECTRUM
TABLE
SHOWS
THE
SO
CALLED
THEMATIC
BANDS
IN
NASA
LANDSAT
SATELLITE
THE
PRIMARY
FUNCTION
OF
LANDSAT
IS
TO
OBTAIN
AND
TRANSMIT
IMAGES
OF
THE
EARTH
FROM
SPACE
FOR
PURPOSES
OF
MONITORING
ENVIRONMENTAL
CONDITIONS
ON
THE
PLANET
THE
BANDS
ARE
EXPRESSED
IN
TERMS
OF
WAVELENGTH
WITH
ΜM
BEING
EQUAL
TO
M
WE
DIS
CUSS
THE
WAVELENGTH
REGIONS
OF
THE
ELECTROMAGNETIC
SPECTRUM
IN
MORE
DETAIL
IN
CHAPTER
NOTE
THE
CHARACTERISTICS
AND
USES
OF
EACH
BAND
IN
TABLE
IN
ORDER
TO
DEVELOP
A
BASIC
APPRECIATION
FOR
THE
POWER
OF
THIS
TYPE
OF
MULTISPECTRAL
IMAGING
CONSIDER
FIG
WHICH
SHOWS
ONE
IMAGE
FOR
EACH
OF
FIGURE
LANDSAT
SATELLITE
IMAGES
OF
THE
WASHINGTON
D
C
AREA
THE
NUMBERS
REFER
TO
THE
THEMATIC
BANDS
IN
TABLE
IMAGES
COURTESY
OF
NASA
THE
SPECTRAL
BANDS
IN
TABLE
THE
AREA
IMAGED
IS
WASHINGTON
D
C
WHICH
IN
CLUDES
FEATURES
SUCH
AS
BUILDINGS
ROADS
VEGETATION
AND
A
MAJOR
RIVER
THE
PO
TOMAC
GOING
THOUGH
THE
CITY
IMAGES
OF
POPULATION
CENTERS
ARE
USED
ROUTINELY
OVER
TIME
TO
ASSESS
POPULATION
GROWTH
AND
SHIFT
PATTERNS
POLLUTION
AND
OTHER
FACTORS
HARMFUL
TO
THE
ENVIRONMENT
THE
DIFFERENCES
BETWEEN
VISUAL
AND
IN
FRARED
IMAGE
FEATURES
ARE
QUITE
NOTICEABLE
IN
THESE
IMAGES
OBSERVE
FOR
EXAM
PLE
HOW
WELL
DEFINED
THE
RIVER
IS
FROM
ITS
SURROUNDINGS
IN
BANDS
AND
WEATHER
OBSERVATION
AND
PREDICTION
ALSO
ARE
MAJOR
APPLICATIONS
OF
MULTI
SPECTRAL
IMAGING
FROM
SATELLITES
FOR
EXAMPLE
FIG
IS
AN
IMAGE
OF
HURRICANE
KATRINA
ONE
OF
THE
MOST
DEVASTATING
STORMS
IN
RECENT
MEMORY
IN
THE
WESTERN
HEMISPHERE
THIS
IMAGE
WAS
TAKEN
BY
A
NATIONAL
OCEANOGRAPHIC
AND
ATMOS
PHERIC
ADMINISTRATION
NOAA
SATELLITE
USING
SENSORS
IN
THE
VISIBLE
AND
IN
FRARED
BANDS
THE
EYE
OF
THE
HURRICANE
IS
CLEARLY
VISIBLE
IN
THIS
IMAGE
FIGURES
AND
SHOW
AN
APPLICATION
OF
INFRARED
IMAGING
THESE
IMAGES
ARE
PART
OF
THE
NIGHTTIME
LIGHTS
OF
THE
WORLD
DATA
SET
WHICH
PROVIDES
A
GLOBAL
INVENTORY
OF
HUMAN
SETTLEMENTS
THE
IMAGES
WERE
GENERATED
BY
THE
INFRARED
IMAGING
SYSTEM
MOUNTED
ON
A
NOAA
DMSP
DEFENSE
METEOROLOGICAL
SATEL
LITE
PROGRAM
SATELLITE
THE
INFRARED
IMAGING
SYSTEM
OPERATES
IN
THE
BAND
TO
ΜM
AND
HAS
THE
UNIQUE
CAPABILITY
TO
OBSERVE
FAINT
SOURCES
OF
VISIBLE
NEAR
INFRARED
EMISSIONS
PRESENT
ON
THE
EARTH
SURFACE
INCLUDING
CITIES
TOWNS
VILLAGES
GAS
FLARES
AND
FIRES
EVEN
WITHOUT
FORMAL
TRAINING
IN
IMAGE
PROCESSING
IT
IS
NOT
DIFFICULT
TO
IMAGINE
WRITING
A
COMPUTER
PROGRAM
THAT
WOULD
USE
THESE
IM
AGES
TO
ESTIMATE
THE
PERCENT
OF
TOTAL
ELECTRICAL
ENERGY
USED
BY
VARIOUS
REGIONS
OF
THE
WORLD
A
MAJOR
AREA
OF
IMAGING
IN
THE
VISUAL
SPECTRUM
IS
IN
AUTOMATED
VISUAL
IN
SPECTION
OF
MANUFACTURED
GOODS
FIGURE
SHOWS
SOME
EXAMPLES
FIGURE
A
IS
A
CONTROLLER
BOARD
FOR
A
CD
ROM
DRIVE
A
TYPICAL
IMAGE
PROCESSING
TASK
WITH
PRODUCTS
LIKE
THIS
IS
TO
INSPECT
THEM
FOR
MISSING
PARTS
THE
BLACK
SQUARE
ON
THE
TOP
RIGHT
QUADRANT
OF
THE
IMAGE
IS
AN
EXAMPLE
OF
A
MISSING
COMPONENT
FIGURE
SATELLITE
IMAGE
OF
HURRICANE
KATRINA
TAKEN
ON
AUGUST
COURTESY
OF
NOAA
FIGURE
INFRARED
SATELLITE
IMAGES
OF
THE
AMERICAS
THE
SMALL
GRAY
MAP
IS
PROVIDED
FOR
REFERENCE
COURTESY
OF
NOAA
FIGURE
B
IS
AN
IMAGED
PILL
CONTAINER
THE
OBJECTIVE
HERE
IS
TO
HAVE
A
MA
CHINE
LOOK
FOR
MISSING
PILLS
FIGURE
C
SHOWS
AN
APPLICATION
IN
WHICH
IMAGE
PROCESSING
IS
USED
TO
LOOK
FOR
BOTTLES
THAT
ARE
NOT
FILLED
UP
TO
AN
ACCEPTABLE
LEVEL
FIGURE
D
SHOWS
A
CLEAR
PLASTIC
PART
WITH
AN
UNACCEPTABLE
NUMBER
OF
AIR
POCKETS
IN
IT
DETECTING
ANOMALIES
LIKE
THESE
IS
A
MAJOR
THEME
OF
INDUSTRIAL
INSPECTION
THAT
INCLUDES
OTHER
PRODUCTS
SUCH
AS
WOOD
AND
CLOTH
FIGURE
E
SHOWS
A
BATCH
OF
CEREAL
DURING
INSPECTION
FOR
COLOR
AND
THE
PRESENCE
OF
ANOM
ALIES
SUCH
AS
BURNED
FLAKES
FINALLY
FIG
F
SHOWS
AN
IMAGE
OF
AN
INTRAOCULAR
IMPLANT
REPLACEMENT
LENS
FOR
THE
HUMAN
EYE
A
STRUCTURED
LIGHT
ILLUMINA
TION
TECHNIQUE
WAS
USED
TO
HIGHLIGHT
FOR
EASIER
DETECTION
FLAT
LENS
DEFORMATIONS
TOWARD
THE
CENTER
OF
THE
LENS
THE
MARKINGS
AT
O
CLOCK
AND
O
CLOCK
ARE
TWEEZER
DAMAGE
MOST
OF
THE
OTHER
SMALL
SPECKLE
DETAIL
IS
DEBRIS
THE
OBJECTIVE
IN
THIS
TYPE
OF
INSPECTION
IS
TO
FIND
DAMAGED
OR
INCORRECTLY
MANUFACTURED
IM
PLANTS
AUTOMATICALLY
PRIOR
TO
PACKAGING
AS
A
FINAL
ILLUSTRATION
OF
IMAGE
PROCESSING
IN
THE
VISUAL
SPECTRUM
CONSIDER
FIG
FIGURE
A
SHOWS
A
THUMB
PRINT
IMAGES
OF
FINGERPRINTS
ARE
ROU
TINELY
PROCESSED
BY
COMPUTER
EITHER
TO
ENHANCE
THEM
OR
TO
FIND
FEATURES
THAT
AID
IN
THE
AUTOMATED
SEARCH
OF
A
DATABASE
FOR
POTENTIAL
MATCHES
FIGURE
B
SHOWS
AN
IMAGE
OF
PAPER
CURRENCY
APPLICATIONS
OF
DIGITAL
IMAGE
PROCESSING
IN
THIS
AREA
INCLUDE
AUTOMATED
COUNTING
AND
IN
LAW
ENFORCEMENT
THE
READING
OF
THE
SERIAL
NUMBER
FOR
THE
PURPOSE
OF
TRACKING
AND
IDENTIFYING
BILLS
THE
TWO
VE
HICLE
IMAGES
SHOWN
IN
FIGS
C
AND
D
ARE
EXAMPLES
OF
AUTOMATED
LICENSE
PLATE
READING
THE
LIGHT
RECTANGLES
INDICATE
THE
AREA
IN
WHICH
THE
IMAGING
SYSTEM
FIGURE
INFRARED
SATELLITE
IMAGES
OF
THE
REMAINING
POPULATED
PART
OF
THE
WORLD
THE
SMALL
GRAY
MAP
IS
PROVIDED
FOR
REFERENCE
COURTESY
OF
NOAA
A
B
C
D
E
F
FIGURE
SOME
EXAMPLES
OF
MANUFACTURED
GOODS
OFTEN
CHECKED
USING
DIGITAL
IMAGE
PROCESSING
A
A
CIRCUIT
BOARD
CONTROLLER
B
PACKAGED
PILLS
C
BOTTLES
D
AIR
BUBBLES
IN
A
CLEAR
PLASTIC
PRODUCT
E
CEREAL
F
IMAGE
OF
INTRAOCULAR
IMPLANT
FIG
F
COURTESY
OF
MR
PETE
SITES
PERCEPTICS
CORPORATION
DETECTED
THE
PLATE
THE
BLACK
RECTANGLES
SHOW
THE
RESULTS
OF
AUTOMATED
READING
OF
THE
PLATE
CONTENT
BY
THE
SYSTEM
LICENSE
PLATE
AND
OTHER
APPLICATIONS
OF
CHAR
ACTER
RECOGNITION
ARE
USED
EXTENSIVELY
FOR
TRAFFIC
MONITORING
AND
SURVEILLANCE
IMAGING
IN
THE
MICROWAVE
BAND
THE
DOMINANT
APPLICATION
OF
IMAGING
IN
THE
MICROWAVE
BAND
IS
RADAR
THE
UNIQUE
FEATURE
OF
IMAGING
RADAR
IS
ITS
ABILITY
TO
COLLECT
DATA
OVER
VIRTUALLY
ANY
REGION
AT
ANY
TIME
REGARDLESS
OF
WEATHER
OR
AMBIENT
LIGHTING
CONDITIONS
SOME
A
B
C
D
FIGURE
SOME
ADDITIONAL
EXAMPLES
OF
IMAGING
IN
THE
VISUAL
SPECTRUM
A
THUMB
PRINT
B
PAPER
CURRENCY
C
AND
D
AUTOMATED
LICENSE
PLATE
READING
FIGURE
A
COURTESY
OF
THE
NATIONAL
INSTITUTE
OF
STANDARDS
AND
TECHNOLOGY
FIGURES
C
AND
D
COURTESY
OF
DR
JUAN
HERRERA
PERCEPTICS
CORPORATION
RADAR
WAVES
CAN
PENETRATE
CLOUDS
AND
UNDER
CERTAIN
CONDITIONS
CAN
ALSO
SEE
THROUGH
VEGETATION
ICE
AND
DRY
SAND
IN
MANY
CASES
RADAR
IS
THE
ONLY
WAY
TO
EXPLORE
INACCESSIBLE
REGIONS
OF
THE
EARTH
SURFACE
AN
IMAGING
RADAR
WORKS
LIKE
A
FLASH
CAMERA
IN
THAT
IT
PROVIDES
ITS
OWN
ILLUMINATION
MICROWAVE
PULSES
TO
ILLUMINATE
AN
AREA
ON
THE
GROUND
AND
TAKE
A
SNAPSHOT
IMAGE
INSTEAD
OF
A
CAMERA
LENS
A
RADAR
USES
AN
ANTENNA
AND
DIGITAL
COMPUTER
PROCESSING
TO
RECORD
ITS
IMAGES
IN
A
RADAR
IMAGE
ONE
CAN
SEE
ONLY
THE
MICROWAVE
ENERGY
THAT
WAS
REFLECTED
BACK
TOWARD
THE
RADAR
ANTENNA
FIGURE
SHOWS
A
SPACEBORNE
RADAR
IMAGE
COVERING
A
RUGGED
MOUNTAIN
OUS
AREA
OF
SOUTHEAST
TIBET
ABOUT
KM
EAST
OF
THE
CITY
OF
LHASA
IN
THE
LOWER
RIGHT
CORNER
IS
A
WIDE
VALLEY
OF
THE
LHASA
RIVER
WHICH
IS
POPULATED
BY
TIBETAN
FARMERS
AND
YAK
HERDERS
AND
INCLUDES
THE
VILLAGE
OF
MENBA
MOUNTAINS
IN
THIS
AREA
REACH
ABOUT
M
FT
ABOVE
SEA
LEVEL
WHILE
THE
VALLEY
FLOORS
LIE
ABOUT
M
FT
ABOVE
SEA
LEVEL
NOTE
THE
CLARITY
AND
DETAIL
OF
THE
IMAGE
UNENCUMBERED
BY
CLOUDS
OR
OTHER
ATMOSPHERIC
CONDITIONS
THAT
NORMALLY
INTERFERE
WITH
IMAGES
IN
THE
VISUAL
BAND
FIGURE
SPACEBORNE
RADAR
IMAGE
OF
MOUNTAINS
IN
SOUTHEAST
TIBET
COURTESY
OF
NASA
IMAGING
IN
THE
RADIO
BAND
AS
IN
THE
CASE
OF
IMAGING
AT
THE
OTHER
END
OF
THE
SPECTRUM
GAMMA
RAYS
THE
MAJOR
APPLICATIONS
OF
IMAGING
IN
THE
RADIO
BAND
ARE
IN
MEDICINE
AND
ASTRONOMY
IN
MEDICINE
RADIO
WAVES
ARE
USED
IN
MAGNETIC
RESONANCE
IMAGING
MRI
THIS
TECHNIQUE
PLACES
A
PATIENT
IN
A
POWERFUL
MAGNET
AND
PASSES
RADIO
WAVES
THROUGH
HIS
OR
HER
BODY
IN
SHORT
PULSES
EACH
PULSE
CAUSES
A
RESPONDING
PULSE
OF
RADIO
WAVES
TO
BE
EMITTED
BY
THE
PATIENT
TISSUES
THE
LOCATION
FROM
WHICH
THESE
SIG
NALS
ORIGINATE
AND
THEIR
STRENGTH
ARE
DETERMINED
BY
A
COMPUTER
WHICH
PRODUCES
A
TWO
DIMENSIONAL
PICTURE
OF
A
SECTION
OF
THE
PATIENT
MRI
CAN
PRODUCE
PICTURES
IN
ANY
PLANE
FIGURE
SHOWS
MRI
IMAGES
OF
A
HUMAN
KNEE
AND
SPINE
THE
LAST
IMAGE
TO
THE
RIGHT
IN
FIG
SHOWS
AN
IMAGE
OF
THE
CRAB
PULSAR
IN
THE
RADIO
BAND
ALSO
SHOWN
FOR
AN
INTERESTING
COMPARISON
ARE
IMAGES
OF
THE
SAME
REGION
BUT
TAKEN
IN
MOST
OF
THE
BANDS
DISCUSSED
EARLIER
NOTE
THAT
EACH
IMAGE
GIVES
A
TOTALLY
DIFFERENT
VIEW
OF
THE
PULSAR
EXAMPLES
IN
WHICH
OTHER
IMAGING
MODALITIES
ARE
USED
ALTHOUGH
IMAGING
IN
THE
ELECTROMAGNETIC
SPECTRUM
IS
DOMINANT
BY
FAR
THERE
ARE
A
NUMBER
OF
OTHER
IMAGING
MODALITIES
THAT
ALSO
ARE
IMPORTANT
SPECIFICALLY
WE
DISCUSS
IN
THIS
SECTION
ACOUSTIC
IMAGING
ELECTRON
MICROSCOPY
AND
SYNTHETIC
COMPUTER
GENERATED
IMAGING
IMAGING
USING
SOUND
FINDS
APPLICATION
IN
GEOLOGICAL
EXPLORATION
INDUSTRY
AND
MEDICINE
GEOLOGICAL
APPLICATIONS
USE
SOUND
IN
THE
LOW
END
OF
THE
SOUND
SPECTRUM
HUNDREDS
OF
HZ
WHILE
IMAGING
IN
OTHER
AREAS
USE
ULTRASOUND
MIL
LIONS
OF
HZ
THE
MOST
IMPORTANT
COMMERCIAL
APPLICATIONS
OF
IMAGE
PROCESSING
IN
GEOLOGY
ARE
IN
MINERAL
AND
OIL
EXPLORATION
FOR
IMAGE
ACQUISITION
OVER
LAND
ONE
OF
THE
MAIN
APPROACHES
IS
TO
USE
A
LARGE
TRUCK
AND
A
LARGE
FLAT
STEEL
PLATE
THE
PLATE
IS
PRESSED
ON
THE
GROUND
BY
THE
TRUCK
AND
THE
TRUCK
IS
VIBRATED
THROUGH
A
FREQUENCY
SPECTRUM
UP
TO
HZ
THE
STRENGTH
AND
SPEED
OF
THE
A
B
FIGURE
MRI
IMAGES
OF
A
HUMAN
A
KNEE
AND
B
SPINE
IMAGE
A
COURTESY
OF
DR
THOMAS
R
GEST
DIVISION
OF
ANATOMICAL
SCIENCES
UNIVERSITY
OF
MICHIGAN
MEDICAL
SCHOOL
AND
B
COURTESY
OF
DR
DAVID
R
PICKENS
DEPARTMENT
OF
RADIOLOGY
AND
RADIOLOGICAL
SCIENCES
VANDERBILT
UNIVERSITY
MEDICAL
CENTER
RETURNING
SOUND
WAVES
ARE
DETERMINED
BY
THE
COMPOSITION
OF
THE
EARTH
BELOW
THE
SURFACE
THESE
ARE
ANALYZED
BY
COMPUTER
AND
IMAGES
ARE
GENERATED
FROM
THE
RESULTING
ANALYSIS
FOR
MARINE
ACQUISITION
THE
ENERGY
SOURCE
CONSISTS
USUALLY
OF
TWO
AIR
GUNS
TOWED
BEHIND
A
SHIP
RETURNING
SOUND
WAVES
ARE
DETECTED
BY
HYDROPHONES
PLACED
IN
CABLES
THAT
ARE
EITHER
TOWED
BEHIND
THE
SHIP
LAID
ON
THE
BOTTOM
OF
THE
OCEAN
OR
HUNG
FROM
BUOYS
VERTICAL
CABLES
THE
TWO
AIR
GUNS
ARE
ALTER
NATELY
PRESSURIZED
TO
PSI
AND
THEN
SET
OFF
THE
CONSTANT
MOTION
OF
THE
SHIP
PROVIDES
A
TRANSVERSAL
DIRECTION
OF
MOTION
THAT
TOGETHER
WITH
THE
RETURN
ING
SOUND
WAVES
IS
USED
TO
GENERATE
A
D
MAP
OF
THE
COMPOSITION
OF
THE
EARTH
BELOW
THE
BOTTOM
OF
THE
OCEAN
FIGURE
SHOWS
A
CROSS
SECTIONAL
IMAGE
OF
A
WELL
KNOWN
D
MODEL
AGAINST
WHICH
THE
PERFORMANCE
OF
SEISMIC
IMAGING
ALGORITHMS
IS
TESTED
THE
ARROW
POINTS
TO
A
HYDROCARBON
OIL
AND
OR
GAS
TRAP
THIS
TARGET
IS
BRIGHTER
THAN
THE
SURROUNDING
LAYERS
BECAUSE
THE
CHANGE
IN
DENSITY
IN
THE
TARGET
REGION
IS
GAMMA
X
RAY
OPTICAL
INFRARED
RADIO
FIGURE
IMAGES
OF
THE
CRAB
PULSAR
IN
THE
CENTER
OF
EACH
IMAGE
COVERING
THE
ELECTROMAGNETIC
SPECTRUM
COURTESY
OF
NASA
FIGURE
CROSS
SECTIONAL
IMAGE
OF
A
SEISMIC
MODEL
THE
ARROW
POINTS
TO
A
HYDROCARBON
OIL
AND
OR
GAS
TRAP
COURTESY
OF
DR
CURTIS
OBER
SANDIA
NATIONAL
LABORATORIES
LARGER
SEISMIC
INTERPRETERS
LOOK
FOR
THESE
BRIGHT
SPOTS
TO
FIND
OIL
AND
GAS
THE
LAYERS
ABOVE
ALSO
ARE
BRIGHT
BUT
THEIR
BRIGHTNESS
DOES
NOT
VARY
AS
STRONGLY
ACROSS
THE
LAYERS
MANY
SEISMIC
RECONSTRUCTION
ALGORITHMS
HAVE
DIFFICULTY
IMAG
ING
THIS
TARGET
BECAUSE
OF
THE
FAULTS
ABOVE
IT
ALTHOUGH
ULTRASOUND
IMAGING
IS
USED
ROUTINELY
IN
MANUFACTURING
THE
BEST
KNOWN
APPLICATIONS
OF
THIS
TECHNIQUE
ARE
IN
MEDICINE
ESPECIALLY
IN
OBSTETRICS
WHERE
UNBORN
BABIES
ARE
IMAGED
TO
DETERMINE
THE
HEALTH
OF
THEIR
DEVELOP
MENT
A
BYPRODUCT
OF
THIS
EXAMINATION
IS
DETERMINING
THE
SEX
OF
THE
BABY
UL
TRASOUND
IMAGES
ARE
GENERATED
USING
THE
FOLLOWING
BASIC
PROCEDURE
THE
ULTRASOUND
SYSTEM
A
COMPUTER
ULTRASOUND
PROBE
CONSISTING
OF
A
SOURCE
AND
RECEIVER
AND
A
DISPLAY
TRANSMITS
HIGH
FREQUENCY
TO
MHZ
SOUND
PULSES
INTO
THE
BODY
THE
SOUND
WAVES
TRAVEL
INTO
THE
BODY
AND
HIT
A
BOUNDARY
BETWEEN
TISSUES
E
G
BETWEEN
FLUID
AND
SOFT
TISSUE
SOFT
TISSUE
AND
BONE
SOME
OF
THE
SOUND
WAVES
ARE
REFLECTED
BACK
TO
THE
PROBE
WHILE
SOME
TRAVEL
ON
FURTHER
UNTIL
THEY
REACH
ANOTHER
BOUNDARY
AND
GET
REFLECTED
THE
REFLECTED
WAVES
ARE
PICKED
UP
BY
THE
PROBE
AND
RELAYED
TO
THE
COM
PUTER
THE
MACHINE
CALCULATES
THE
DISTANCE
FROM
THE
PROBE
TO
THE
TISSUE
OR
ORGAN
BOUNDARIES
USING
THE
SPEED
OF
SOUND
IN
TISSUE
M
AND
THE
TIME
OF
EACH
ECHO
RETURN
THE
SYSTEM
DISPLAYS
THE
DISTANCES
AND
INTENSITIES
OF
THE
ECHOES
ON
THE
SCREEN
FORMING
A
TWO
DIMENSIONAL
IMAGE
IN
A
TYPICAL
ULTRASOUND
IMAGE
MILLIONS
OF
PULSES
AND
ECHOES
ARE
SENT
AND
RE
CEIVED
EACH
SECOND
THE
PROBE
CAN
BE
MOVED
ALONG
THE
SURFACE
OF
THE
BODY
AND
ANGLED
TO
OBTAIN
VARIOUS
VIEWS
FIGURE
SHOWS
SEVERAL
EXAMPLES
WE
CONTINUE
THE
DISCUSSION
ON
IMAGING
MODALITIES
WITH
SOME
EXAMPLES
OF
ELECTRON
MICROSCOPY
ELECTRON
MICROSCOPES
FUNCTION
AS
THEIR
OPTICAL
COUNTER
PARTS
EXCEPT
THAT
THEY
USE
A
FOCUSED
BEAM
OF
ELECTRONS
INSTEAD
OF
LIGHT
TO
IMAGE
A
SPECIMEN
THE
OPERATION
OF
ELECTRON
MICROSCOPES
INVOLVES
THE
FOLLOW
ING
BASIC
STEPS
A
STREAM
OF
ELECTRONS
IS
PRODUCED
BY
AN
ELECTRON
SOURCE
AND
AC
CELERATED
TOWARD
THE
SPECIMEN
USING
A
POSITIVE
ELECTRICAL
POTENTIAL
THIS
STREAM
FIGURE
EXAMPLES
OF
ULTRASOUND
IMAGING
A
BABY
B
ANOTHER
VIEW
OF
BABY
C
THYROIDS
D
MUSCLE
LAYERS
SHOWING
LESION
COURTESY
OF
SIEMENS
MEDICAL
SYSTEMS
INC
ULTRASOUND
GROUP
IS
CONFINED
AND
FOCUSED
USING
METAL
APERTURES
AND
MAGNETIC
LENSES
INTO
A
THIN
MONOCHROMATIC
BEAM
THIS
BEAM
IS
FOCUSED
ONTO
THE
SAMPLE
USING
A
MAGNETIC
LENS
INTERACTIONS
OCCUR
INSIDE
THE
IRRADIATED
SAMPLE
AFFECTING
THE
ELECTRON
BEAM
THESE
INTERACTIONS
AND
EFFECTS
ARE
DETECTED
AND
TRANSFORMED
INTO
AN
IMAGE
MUCH
IN
THE
SAME
WAY
THAT
LIGHT
IS
REFLECTED
FROM
OR
ABSORBED
BY
OB
JECTS
IN
A
SCENE
THESE
BASIC
STEPS
ARE
CARRIED
OUT
IN
ALL
ELECTRON
MICROSCOPES
A
TRANSMISSION
ELECTRON
MICROSCOPE
TEM
WORKS
MUCH
LIKE
A
SLIDE
PROJEC
TOR
A
PROJECTOR
SHINES
TRANSMITS
A
BEAM
OF
LIGHT
THROUGH
A
SLIDE
AS
THE
LIGHT
PASSES
THROUGH
THE
SLIDE
IT
IS
MODULATED
BY
THE
CONTENTS
OF
THE
SLIDE
THIS
TRANS
MITTED
BEAM
IS
THEN
PROJECTED
ONTO
THE
VIEWING
SCREEN
FORMING
AN
ENLARGED
IMAGE
OF
THE
SLIDE
TEMS
WORK
THE
SAME
WAY
EXCEPT
THAT
THEY
SHINE
A
BEAM
OF
ELECTRONS
THROUGH
A
SPECIMEN
ANALOGOUS
TO
THE
SLIDE
THE
FRACTION
OF
THE
BEAM
TRANSMITTED
THROUGH
THE
SPECIMEN
IS
PROJECTED
ONTO
A
PHOSPHOR
SCREEN
THE
INTERACTION
OF
THE
ELECTRONS
WITH
THE
PHOSPHOR
PRODUCES
LIGHT
AND
THERE
FORE
A
VIEWABLE
IMAGE
A
SCANNING
ELECTRON
MICROSCOPE
SEM
ON
THE
OTHER
HAND
ACTUALLY
SCANS
THE
ELECTRON
BEAM
AND
RECORDS
THE
INTERACTION
OF
BEAM
AND
SAMPLE
AT
EACH
LOCATION
THIS
PRODUCES
ONE
DOT
ON
A
PHOSPHOR
SCREEN
A
COMPLETE
IMAGE
IS
FORMED
BY
A
RASTER
SCAN
OF
THE
BEAM
THROUGH
THE
SAMPLE
MUCH
LIKE
A
TV
CAMERA
THE
ELECTRONS
INTERACT
WITH
A
PHOSPHOR
SCREEN
AND
PRODUCE
LIGHT
SEMS
ARE
SUITABLE
FOR
BULKY
SAMPLES
WHILE
TEMS
REQUIRE
VERY
THIN
SAMPLES
ELECTRON
MICROSCOPES
ARE
CAPABLE
OF
VERY
HIGH
MAGNIFICATION
WHILE
LIGHT
MICROSCOPY
IS
LIMITED
TO
MAGNIFICATIONS
ON
THE
ORDER
ELECTRON
MICROSCOPES
A
B
FIGURE
A
SEM
IMAGE
OF
A
TUNGSTEN
FILAMENT
FOLLOWING
THERMAL
FAILURE
NOTE
THE
SHATTERED
PIECES
ON
THE
LOWER
LEFT
B
SEM
IMAGE
OF
DAMAGED
INTEGRATED
CIRCUIT
THE
WHITE
FIBERS
ARE
OXIDES
RESULTING
FROM
THERMAL
DESTRUCTION
FIGURE
A
COURTESY
OF
MR
MICHAEL
SHAFFER
DEPARTMENT
OF
GEOLOGICAL
SCIENCES
UNIVERSITY
OF
OREGON
EUGENE
B
COURTESY
OF
DR
J
M
HUDAK
MCMASTER
UNIVERSITY
HAMILTON
ONTARIO
CANADA
CAN
ACHIEVE
MAGNIFICATION
OF
OR
MORE
FIGURE
SHOWS
TWO
SEM
IM
AGES
OF
SPECIMEN
FAILURES
DUE
TO
THERMAL
OVERLOAD
WE
CONCLUDE
THE
DISCUSSION
OF
IMAGING
MODALITIES
BY
LOOKING
BRIEFLY
AT
IM
AGES
THAT
ARE
NOT
OBTAINED
FROM
PHYSICAL
OBJECTS
INSTEAD
THEY
ARE
GENERATED
BY
COMPUTER
FRACTALS
ARE
STRIKING
EXAMPLES
OF
COMPUTER
GENERATED
IMAGES
LU
BASICALLY
A
FRACTAL
IS
NOTHING
MORE
THAN
AN
ITERATIVE
REPRODUCTION
OF
A
BASIC
PATTERN
ACCORDING
TO
SOME
MATHEMATICAL
RULES
FOR
INSTANCE
TILING
IS
ONE
OF
THE
SIMPLEST
WAYS
TO
GENERATE
A
FRACTAL
IMAGE
A
SQUARE
CAN
BE
SUBDI
VIDED
INTO
FOUR
SQUARE
SUBREGIONS
EACH
OF
WHICH
CAN
BE
FURTHER
SUBDIVIDED
INTO
FOUR
SMALLER
SQUARE
REGIONS
AND
SO
ON
DEPENDING
ON
THE
COMPLEXITY
OF
THE
RULES
FOR
FILLING
EACH
SUBSQUARE
SOME
BEAUTIFUL
TILE
IMAGES
CAN
BE
GENERATED
USING
THIS
METHOD
OF
COURSE
THE
GEOMETRY
CAN
BE
ARBITRARY
FOR
INSTANCE
THE
FRACTAL
IMAGE
COULD
BE
GROWN
RADIALLY
OUT
OF
A
CENTER
POINT
FIGURE
A
SHOWS
A
FRACTAL
GROWN
IN
THIS
WAY
FIGURE
B
SHOWS
ANOTHER
FRACTAL
A
MOONSCAPE
THAT
PROVIDES
AN
INTERESTING
ANALOGY
TO
THE
IMAGES
OF
SPACE
USED
AS
ILLUSTRATIONS
IN
SOME
OF
THE
PRECEDING
SECTIONS
FRACTAL
IMAGES
TEND
TOWARD
ARTISTIC
MATHEMATICAL
FORMULATIONS
OF
GROWTH
OF
SUBIMAGE
ELEMENTS
ACCORDING
TO
A
SET
OF
RULES
THEY
ARE
USEFUL
SOMETIMES
AS
RANDOM
TEXTURES
A
MORE
STRUCTURED
APPROACH
TO
IMAGE
GENERATION
BY
COMPUTER
LIES
IN
D
MODELING
THIS
IS
AN
AREA
THAT
PROVIDES
AN
IMPORTANT
INTERSECTION
BETWEEN
IMAGE
PROCESSING
AND
COMPUTER
GRAPHICS
AND
IS
THE
BASIS
FOR
MANY
D
VISUALIZATION
SYSTEMS
E
G
FLIGHT
SIMULATORS
FIGURES
C
AND
D
SHOW
EXAMPLES
OF
COMPUTER
GENERATED
IMAGES
SINCE
THE
ORIGINAL
OBJECT
IS
CREATED
IN
D
IMAGES
CAN
BE
GENERATED
IN
ANY
PERSPECTIVE
FROM
PLANE
PROJECTIONS
OF
THE
D
VOLUME
IMAGES
OF
THIS
TYPE
CAN
BE
USED
FOR
MEDICAL
TRAINING
AND
FOR
A
HOST
OF
OTHER
APPLICATIONS
SUCH
AS
CRIMINAL
FORENSICS
AND
SPECIAL
EFFECTS
FIGURE
A
AND
B
FRACTAL
IMAGES
C
AND
D
IMAGES
GENERATED
FROM
D
COMPUTER
MODELS
OF
THE
OBJECTS
SHOWN
FIGURES
A
AND
B
COURTESY
OF
MS
MELISSA
D
BINDE
SWARTHMORE
COLLEGE
C
AND
D
COURTESY
OF
NASA
FUNDAMENTAL
STEPS
IN
DIGITAL
IMAGE
PROCESSING
IT
IS
HELPFUL
TO
DIVIDE
THE
MATERIAL
COVERED
IN
THE
FOLLOWING
CHAPTERS
INTO
THE
TWO
BROAD
CATEGORIES
DEFINED
IN
SECTION
METHODS
WHOSE
INPUT
AND
OUTPUT
ARE
IMAGES
AND
METHODS
WHOSE
INPUTS
MAY
BE
IMAGES
BUT
WHOSE
OUTPUTS
ARE
ATTRIBUTES
EXTRACTED
FROM
THOSE
IMAGES
THIS
ORGANIZATION
IS
SUMMARIZED
IN
FIG
THE
DIAGRAM
DOES
NOT
IMPLY
THAT
EVERY
PROCESS
IS
APPLIED
TO
AN
IMAGE
RATHER
THE
INTENTION
IS
TO
CONVEY
AN
IDEA
OF
ALL
THE
METHODOLOGIES
THAT
CAN
BE
APPLIED
TO
IMAGES
FOR
DIFFERENT
PURPOSES
AND
POSSIBLY
WITH
DIFFERENT
OBJECTIVES
THE
DISCUSSION
IN
THIS
SECTION
MAY
BE
VIEWED
AS
A
BRIEF
OVERVIEW
OF
THE
MATERIAL
IN
THE
REMAINDER
OF
THE
BOOK
IMAGE
ACQUISITION
IS
THE
FIRST
PROCESS
IN
FIG
THE
DISCUSSION
IN
SECTION
GAVE
SOME
HINTS
REGARDING
THE
ORIGIN
OF
DIGITAL
IMAGES
THIS
TOPIC
IS
CONSIDERED
IN
MUCH
MORE
DETAIL
IN
CHAPTER
WHERE
WE
ALSO
INTRODUCE
A
NUMBER
OF
BASIC
DIGITAL
IMAGE
CONCEPTS
THAT
ARE
USED
THROUGHOUT
THE
BOOK
NOTE
THAT
ACQUISI
TION
COULD
BE
AS
SIMPLE
AS
BEING
GIVEN
AN
IMAGE
THAT
IS
ALREADY
IN
DIGITAL
FORM
GENERALLY
THE
IMAGE
ACQUISITION
STAGE
INVOLVES
PREPROCESSING
SUCH
AS
SCALING
IMAGE
ENHANCEMENT
IS
THE
PROCESS
OF
MANIPULATING
AN
IMAGE
SO
THAT
THE
RE
SULT
IS
MORE
SUITABLE
THAN
THE
ORIGINAL
FOR
A
SPECIFIC
APPLICATION
THE
WORD
SPECIFIC
IS
IMPORTANT
HERE
BECAUSE
IT
ESTABLISHES
AT
THE
OUTSET
THAT
ENHANCEMENT
TECHNIQUES
ARE
PROBLEM
ORIENTED
THUS
FOR
EXAMPLE
A
METHOD
THAT
IS
QUITE
USE
FUL
FOR
ENHANCING
X
RAY
IMAGES
MAY
NOT
BE
THE
BEST
APPROACH
FOR
ENHANCING
SATELLITE
IMAGES
TAKEN
IN
THE
INFRARED
BAND
OF
THE
ELECTROMAGNETIC
SPECTRUM
FIGURE
FUNDAMENTAL
STEPS
IN
DIGITAL
IMAGE
PROCESSING
THE
CHAPTER
INDICATED
IN
THE
BOXES
IS
WHERE
THE
MATERIAL
DESCRIBED
IN
THE
BOX
IS
DISCUSSED
OUTPUTS
OF
THESE
PROCESSES
GENERALLY
ARE
IMAGES
PROBLEM
DOMAIN
THERE
IS
NO
GENERAL
THEORY
OF
IMAGE
ENHANCEMENT
WHEN
AN
IMAGE
IS
PROCESSED
FOR
VISUAL
INTERPRETATION
THE
VIEWER
IS
THE
ULTIMATE
JUDGE
OF
HOW
WELL
A
PARTICULAR
METHOD
WORKS
ENHANCEMENT
TECHNIQUES
ARE
SO
VARIED
AND
USE
SO
MANY
DIFFERENT
IMAGE
PROCESSING
APPROACHES
THAT
IT
IS
DIFFICULT
TO
AS
SEMBLE
A
MEANINGFUL
BODY
OF
TECHNIQUES
SUITABLE
FOR
ENHANCEMENT
IN
ONE
CHAPTER
WITHOUT
EXTENSIVE
BACKGROUND
DEVELOPMENT
FOR
THIS
REASON
AND
ALSO
BECAUSE
BEGINNERS
IN
THE
FIELD
OF
IMAGE
PROCESSING
GENERALLY
FIND
ENHANCE
MENT
APPLICATIONS
VISUALLY
APPEALING
INTERESTING
AND
RELATIVELY
SIMPLE
TO
UN
DERSTAND
WE
USE
IMAGE
ENHANCEMENT
AS
EXAMPLES
WHEN
INTRODUCING
NEW
CONCEPTS
IN
PARTS
OF
CHAPTER
AND
IN
CHAPTERS
AND
THE
MATERIAL
IN
THE
LATTER
TWO
CHAPTERS
SPAN
MANY
OF
THE
METHODS
USED
TRADITIONALLY
FOR
IMAGE
EN
HANCEMENT
THEREFORE
USING
EXAMPLES
FROM
IMAGE
ENHANCEMENT
TO
INTRODUCE
NEW
IMAGE
PROCESSING
METHODS
DEVELOPED
IN
THESE
EARLY
CHAPTERS
NOT
ONLY
SAVES
HAVING
AN
EXTRA
CHAPTER
IN
THE
BOOK
DEALING
WITH
IMAGE
ENHANCEMENT
BUT
MORE
IMPORTANTLY
IS
AN
EFFECTIVE
APPROACH
FOR
INTRODUCING
NEWCOMERS
TO
THE
DETAILS
OF
PROCESSING
TECHNIQUES
EARLY
IN
THE
BOOK
HOWEVER
AS
YOU
WILL
SEE
IN
PROGRESSING
THROUGH
THE
REST
OF
THE
BOOK
THE
MATERIAL
DEVELOPED
IN
THESE
CHAPTERS
IS
APPLICABLE
TO
A
MUCH
BROADER
CLASS
OF
PROBLEMS
THAN
JUST
IMAGE
ENHANCEMENT
IMAGE
RESTORATION
IS
AN
AREA
THAT
ALSO
DEALS
WITH
IMPROVING
THE
APPEARANCE
OF
AN
IMAGE
HOWEVER
UNLIKE
ENHANCEMENT
WHICH
IS
SUBJECTIVE
IMAGE
RESTORA
TION
IS
OBJECTIVE
IN
THE
SENSE
THAT
RESTORATION
TECHNIQUES
TEND
TO
BE
BASED
ON
MATHEMATICAL
OR
PROBABILISTIC
MODELS
OF
IMAGE
DEGRADATION
ENHANCEMENT
ON
THE
OTHER
HAND
IS
BASED
ON
HUMAN
SUBJECTIVE
PREFERENCES
REGARDING
WHAT
CON
STITUTES
A
GOOD
ENHANCEMENT
RESULT
COLOR
IMAGE
PROCESSING
IS
AN
AREA
THAT
HAS
BEEN
GAINING
IN
IMPORTANCE
BE
CAUSE
OF
THE
SIGNIFICANT
INCREASE
IN
THE
USE
OF
DIGITAL
IMAGES
OVER
THE
INTERNET
CHAPTER
COVERS
A
NUMBER
OF
FUNDAMENTAL
CONCEPTS
IN
COLOR
MODELS
AND
BASIC
COLOR
PROCESSING
IN
A
DIGITAL
DOMAIN
COLOR
IS
USED
ALSO
IN
LATER
CHAPTERS
AS
THE
BASIS
FOR
EXTRACTING
FEATURES
OF
INTEREST
IN
AN
IMAGE
WAVELETS
ARE
THE
FOUNDATION
FOR
REPRESENTING
IMAGES
IN
VARIOUS
DEGREES
OF
RESOLUTION
IN
PARTICULAR
THIS
MATERIAL
IS
USED
IN
THIS
BOOK
FOR
IMAGE
DATA
COM
PRESSION
AND
FOR
PYRAMIDAL
REPRESENTATION
IN
WHICH
IMAGES
ARE
SUBDIVIDED
SUCCESSIVELY
INTO
SMALLER
REGIONS
COMPRESSION
AS
THE
NAME
IMPLIES
DEALS
WITH
TECHNIQUES
FOR
REDUCING
THE
STORAGE
REQUIRED
TO
SAVE
AN
IMAGE
OR
THE
BANDWIDTH
REQUIRED
TO
TRANSMIT
IT
AL
THOUGH
STORAGE
TECHNOLOGY
HAS
IMPROVED
SIGNIFICANTLY
OVER
THE
PAST
DECADE
THE
SAME
CANNOT
BE
SAID
FOR
TRANSMISSION
CAPACITY
THIS
IS
TRUE
PARTICULARLY
IN
USES
OF
THE
INTERNET
WHICH
ARE
CHARACTERIZED
BY
SIGNIFICANT
PICTORIAL
CONTENT
IMAGE
COMPRESSION
IS
FAMILIAR
PERHAPS
INADVERTENTLY
TO
MOST
USERS
OF
COMPUTERS
IN
THE
FORM
OF
IMAGE
FILE
EXTENSIONS
SUCH
AS
THE
JPG
FILE
EXTENSION
USED
IN
THE
JPEG
JOINT
PHOTOGRAPHIC
EXPERTS
GROUP
IMAGE
COMPRESSION
STANDARD
MORPHOLOGICAL
PROCESSING
DEALS
WITH
TOOLS
FOR
EXTRACTING
IMAGE
COMPONENTS
THAT
ARE
USEFUL
IN
THE
REPRESENTATION
AND
DESCRIPTION
OF
SHAPE
THE
MATERIAL
IN
THIS
CHAPTER
BEGINS
A
TRANSITION
FROM
PROCESSES
THAT
OUTPUT
IMAGES
TO
PROCESSES
THAT
OUTPUT
IMAGE
ATTRIBUTES
AS
INDICATED
IN
SECTION
SEGMENTATION
PROCEDURES
PARTITION
AN
IMAGE
INTO
ITS
CONSTITUENT
PARTS
OR
OBJECTS
IN
GENERAL
AUTONOMOUS
SEGMENTATION
IS
ONE
OF
THE
MOST
DIFFICULT
TASKS
IN
DIGITAL
IMAGE
PROCESSING
A
RUGGED
SEGMENTATION
PROCEDURE
BRINGS
THE
PROCESS
A
LONG
WAY
TOWARD
SUCCESSFUL
SOLUTION
OF
IMAGING
PROBLEMS
THAT
REQUIRE
OBJECTS
TO
BE
IDENTIFIED
INDIVIDUALLY
ON
THE
OTHER
HAND
WEAK
OR
ER
RATIC
SEGMENTATION
ALGORITHMS
ALMOST
ALWAYS
GUARANTEE
EVENTUAL
FAILURE
IN
GENERAL
THE
MORE
ACCURATE
THE
SEGMENTATION
THE
MORE
LIKELY
RECOGNITION
IS
TO
SUCCEED
GENERATIVE
MODELS
FOR
DISCRETE
DATA
INTRODUCTION
IN
SECTION
WE
DISCUSSED
HOW
TO
CLASSIFY
A
FEATURE
VECTOR
X
BY
APPLYING
BAYES
RULE
TO
A
GENERATIVE
CLASSIFIER
OF
THE
FORM
P
Y
C
X
Θ
P
X
Y
C
Θ
P
Y
C
Θ
THE
KEY
TO
USING
SUCH
MODELS
IS
SPECIFYING
A
SUITABLE
FORM
FOR
THE
CLASS
CONDITIONAL
DENSITY
P
X
Y
C
Θ
WHICH
DEFINES
WHAT
KIND
OF
DATA
WE
EXPECT
TO
SEE
IN
EACH
CLASS
IN
THIS
CHAPTER
WE
FOCUS
ON
THE
CASE
WHERE
THE
OBSERVED
DATA
ARE
DISCRETE
SYMBOLS
WE
ALSO
DISCUSS
HOW
TO
INFER
THE
UNKNOWN
PARAMETERS
Θ
OF
SUCH
MODELS
BAYESIAN
CONCEPT
LEARNING
CONSIDER
HOW
A
CHILD
LEARNS
TO
UNDERSTAND
THE
MEANING
OF
A
WORD
SUCH
AS
DOG
PRESUMABLY
THE
CHILD
PARENTS
POINT
OUT
POSITIVE
EXAMPLES
OF
THIS
CONCEPT
SAYING
SUCH
THINGS
AS
LOOK
AT
THE
CUTE
DOG
OR
MIND
THE
DOGGY
ETC
HOWEVER
IT
IS
VERY
UNLIKELY
THAT
THEY
PROVIDE
NEGATIVE
EXAMPLES
BY
SAYING
LOOK
AT
THAT
NON
DOG
CERTAINLY
NEGATIVE
EXAMPLES
MAY
BE
OBTAINED
DURING
AN
ACTIVE
LEARNING
PROCESS
THE
CHILD
SAYS
LOOK
AT
THE
DOG
AND
THE
PARENT
SAYS
THAT
A
CAT
DEAR
NOT
A
DOG
BUT
PSYCHOLOGICAL
RESEARCH
HAS
SHOWN
THAT
PEOPLE
CAN
LEARN
CONCEPTS
FROM
POSITIVE
EXAMPLES
ALONE
XU
AND
TENENBAUM
WE
CAN
THINK
OF
LEARNING
THE
MEANING
OF
A
WORD
AS
EQUIVALENT
TO
CONCEPT
LEARNING
WHICH
IN
TURN
IS
EQUIVALENT
TO
BINARY
CLASSIFICATION
TO
SEE
THIS
DEFINE
F
X
IF
X
IS
AN
EXAMPLE
OF
THE
CONCEPT
C
AND
F
X
OTHERWISE
THEN
THE
GOAL
IS
TO
LEARN
THE
INDICATOR
FUNCTION
F
WHICH
JUST
DEFINES
WHICH
ELEMENTS
ARE
IN
THE
SET
C
BY
ALLOWING
FOR
UNCERTAINTY
ABOUT
THE
DEFINITION
OF
F
OR
EQUIVALENTLY
THE
ELEMENTS
OF
C
WE
CAN
EMULATE
FUZZY
SET
THEORY
BUT
USING
STANDARD
PROBABILITY
CALCULUS
NOTE
THAT
STANDARD
BINARY
CLASSIFICATION
TECHNIQUES
REQUIRE
POSITIVE
AND
NEGATIVE
EXAMPLES
BY
CONTRAST
WE
WILL
DEVISE
A
WAY
TO
LEARN
FROM
POSITIVE
EXAMPLES
ALONE
FOR
PEDAGOGICAL
PURPOSES
WE
WILL
CONSIDER
A
VERY
SIMPLE
EXAMPLE
OF
CONCEPT
LEARNING
CALLED
THE
NUMBER
GAME
BASED
ON
PART
OF
JOSH
TENENBAUM
PHD
THESIS
TENENBAUM
THE
GAME
PROCEEDS
AS
FOLLOWS
I
CHOOSE
SOME
SIMPLE
ARITHMETICAL
CONCEPT
C
SUCH
AS
PRIME
NUMBER
OR
A
NUMBER
BETWEEN
AND
I
THEN
GIVE
YOU
A
SERIES
OF
RANDOMLY
CHOSEN
POSITIVE
EXAMPLES
XN
DRAWN
FROM
C
AND
ASK
YOU
WHETHER
SOME
NEW
TEST
CASE
X
BELONGS
TO
C
I
E
I
ASK
YOU
TO
CLASSIFY
X
EXAMPLES
FIGURE
EMPIRICAL
PREDICTIVE
DISTRIBUTION
AVERAGED
OVER
HUMANS
IN
THE
NUMBER
GAME
FIRST
TWO
ROWS
AFTER
SEEING
AND
THIS
ILLUSTRATES
DIFFUSE
SIMILARITY
THIRD
ROW
AFTER
SEEING
THIS
ILLUSTRATES
RULE
LIKE
BEHAVIOR
POWERS
OF
BOTTOM
ROW
AFTER
SEEING
THIS
ILLUSTRATES
FOCUSSED
SIMILARITY
NUMBERS
NEAR
SOURCE
FIGURE
OF
TENENBAUM
USED
WITH
KIND
PERMISSION
OF
JOSH
TENENBAUM
SUPPOSE
FOR
SIMPLICITY
THAT
ALL
NUMBERS
ARE
INTEGERS
BETWEEN
AND
NOW
SUPPOSE
I
TELL
YOU
IS
A
POSITIVE
EXAMPLE
OF
THE
CONCEPT
WHAT
OTHER
NUMBERS
DO
YOU
THINK
ARE
POSITIVE
IT
HARD
TO
TELL
WITH
ONLY
ONE
EXAMPLE
SO
YOUR
PREDICTIONS
WILL
BE
QUITE
VAGUE
PRESUMABLY
NUMBERS
THAT
ARE
SIMILAR
IN
SOME
SENSE
TO
ARE
MORE
LIKELY
BUT
SIMILAR
IN
WHAT
WAY
IS
SIMILAR
BECAUSE
IT
IS
CLOSE
BY
IS
SIMILAR
BECAUSE
IT
HAS
A
DIGIT
IN
COMMON
IS
SIMILAR
BECAUSE
IT
IS
ALSO
EVEN
AND
A
POWER
OF
BUT
DOES
NOT
SEEM
SIMILAR
THUS
SOME
NUMBERS
ARE
MORE
LIKELY
THAN
OTHERS
WE
CAN
REPRESENT
THIS
AS
A
PROBABILITY
DISTRIBUTION
P
X
WHICH
IS
THE
PROBABILITY
THAT
X
C
GIVEN
THE
DATA
FOR
ANY
X
THIS
IS
CALLED
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
FIGURE
TOP
SHOWS
THE
PREDICTIVE
DISTRIBUTION
OF
PEOPLE
DERIVED
FROM
A
LAB
EXPERIMENT
WE
SEE
THAT
PEOPLE
PREDICT
NUMBERS
THAT
ARE
SIMILAR
TO
UNDER
A
VARIETY
OF
KINDS
OF
SIMILARITY
NOW
SUPPOSE
I
TELL
YOU
THAT
AND
ARE
ALSO
POSITIVE
EXAMPLES
NOW
YOU
MAY
GUESS
THAT
THE
HIDDEN
CONCEPT
IS
POWERS
OF
TWO
THIS
IS
AN
EXAMPLE
OF
INDUCTION
GIVEN
THIS
HYPOTHESIS
THE
PREDICTIVE
DISTRIBUTION
IS
QUITE
SPECIFIC
AND
PUTS
MOST
OF
ITS
MASS
ON
POWERS
OF
AS
SHOWN
IN
FIGURE
THIRD
ROW
IF
INSTEAD
I
TELL
YOU
THE
DATA
IS
YOU
WILL
GET
A
DIFFERENT
KIND
OF
GENERALIZATION
GRADIENT
AS
SHOWN
IN
FIGURE
BOTTOM
HOW
CAN
WE
EXPLAIN
THIS
BEHAVIOR
AND
EMULATE
IT
IN
A
MACHINE
THE
CLASSIC
APPROACH
TO
INDUCTION
IS
TO
SUPPOSE
WE
HAVE
A
HYPOTHESIS
SPACE
OF
CONCEPTS
SUCH
AS
ODD
NUMBERS
EVEN
NUMBERS
ALL
NUMBERS
BETWEEN
AND
POWERS
OF
TWO
ALL
NUMBERS
ENDING
IN
J
FOR
J
ETC
THE
SUBSET
OF
THAT
IS
CONSISTENT
WITH
THE
DATA
D
IS
CALLED
THE
VERSION
SPACE
AS
WE
SEE
MORE
EXAMPLES
THE
VERSION
SPACE
SHRINKS
AND
WE
BECOME
INCREASINGLY
CERTAIN
ABOUT
THE
CONCEPT
MITCHELL
HOWEVER
THE
VERSION
SPACE
IS
NOT
THE
WHOLE
STORY
AFTER
SEEING
THERE
ARE
MANY
CONSISTENT
RULES
HOW
DO
YOU
COMBINE
THEM
TO
PREDICT
IF
X
C
ALSO
AFTER
SEEING
WHY
DID
YOU
CHOOSE
THE
RULE
POWERS
OF
TWO
AND
NOT
SAY
ALL
EVEN
NUMBERS
OR
POWERS
OF
TWO
EXCEPT
FOR
BOTH
OF
WHICH
ARE
EQUALLY
CONSISTENT
WITH
THE
EVIDENCE
WE
WILL
NOW
PROVIDE
A
BAYESIAN
EXPLANATION
FOR
THIS
LIKELIHOOD
WE
MUST
EXPLAIN
WHY
WE
CHOSE
HTWO
POWERS
OF
TWO
AND
NOT
SAY
HEVEN
EVEN
NUMBERS
AFTER
SEEING
GIVEN
THAT
BOTH
HYPOTHESES
ARE
CONSISTENT
WITH
THE
EVIDENCE
THE
KEY
INTUITION
IS
THAT
WE
WANT
TO
AVOID
SUSPICIOUS
COINCIDENCES
IF
THE
TRUE
CONCEPT
WAS
EVEN
NUMBERS
HOW
COME
WE
ONLY
SAW
NUMBERS
THAT
HAPPENED
TO
BE
POWERS
OF
TWO
TO
FORMALIZE
THIS
LET
US
ASSUME
THAT
EXAMPLES
ARE
SAMPLED
UNIFORMLY
AT
RANDOM
FROM
THE
EXTENSION
OF
A
CONCEPT
THE
EXTENSION
OF
A
CONCEPT
IS
JUST
THE
SET
OF
NUMBERS
THAT
BELONG
TO
IT
E
G
THE
EXTENSION
OF
HEVEN
IS
THE
EXTENSION
OF
NUMBERS
ENDING
IN
IS
TENENBAUM
CALLS
THIS
THE
STRONG
SAMPLING
ASSUMPTION
GIVEN
THIS
ASSUMPTION
THE
PROBABILITY
OF
INDEPENDENTLY
SAMPLING
N
ITEMS
WITH
REPLACEMENT
FROM
H
IS
GIVEN
BY
LN
LN
P
D
H
SIZE
H
H
THIS
CRUCIAL
EQUATION
EMBODIES
WHAT
TENENBAUM
CALLS
THE
SIZE
PRINCIPLE
WHICH
MEANS
THE
MODEL
FAVORS
THE
SIMPLEST
SMALLEST
HYPOTHESIS
CONSISTENT
WITH
THE
DATA
THIS
IS
MORE
COMMONLY
KNOWN
AS
OCCAM
RAZOR
TO
SEE
HOW
IT
WORKS
LET
THEN
P
HTWO
SINCE
THERE
ARE
ONLY
POWERS
OF
TWO
LESS
THAN
BUT
P
HEVEN
SINCE
THERE
ARE
EVEN
NUMBERS
SO
THE
LIKELIHOOD
THAT
H
HTWO
IS
HIGHER
THAN
IF
H
HEVEN
AFTER
EXAMPLES
THE
LIKELIHOOD
OF
HTWO
IS
WHEREAS
THE
LIKELIHOOD
OF
HEVEN
IS
THIS
IS
A
LIKELIHOOD
RATIO
OF
ALMOST
IN
FAVOR
OF
HTWO
THIS
QUANTIFIES
OUR
EARLIER
INTUITION
THAT
D
WOULD
BE
A
VERY
SUSPICIOUS
COINCIDENCE
IF
GENERATED
BY
HEVEN
PRIOR
SUPPOSE
D
GIVEN
THIS
DATA
THE
CONCEPT
H
POWERS
OF
TWO
EXCEPT
IS
MORE
LIKELY
THAN
H
POWERS
OF
TWO
SINCE
H
DOES
NOT
NEED
TO
EXPLAIN
THE
COINCIDENCE
THAT
IS
MISSING
FROM
THE
SET
OF
EXAMPLES
HOWEVER
THE
HYPOTHESIS
H
POWERS
OF
TWO
EXCEPT
SEEMS
CONCEPTUALLY
UNNATURAL
WE
CAN
CAPTURE
SUCH
INTUTION
BY
ASSIGNING
LOW
PRIOR
PROBABILITY
TO
UNNATURAL
CONCEPTS
OF
COURSE
YOUR
PRIOR
MIGHT
BE
DIFFERENT
THAN
MINE
THIS
SUBJECTIVE
ASPECT
OF
BAYESIAN
REASONING
IS
A
SOURCE
OF
MUCH
CONTROVERSY
SINCE
IT
MEANS
FOR
EXAMPLE
THAT
A
CHILD
AND
A
MATH
PROFESSOR
WILLIAM
OF
OCCAM
ALSO
SPELT
OCKHAM
WAS
AN
ENGLISH
MONK
AND
PHILOSOPHER
WILL
REACH
DIFFERENT
ANSWERS
IN
FACT
THEY
PRESUMABLY
NOT
ONLY
HAVE
DIFFERENT
PRIORS
BUT
ALSO
DIFFERENT
HYPOTHESIS
SPACES
HOWEVER
WE
CAN
FINESSE
THAT
BY
DEFINING
THE
HYPOTHESIS
SPACE
OF
THE
CHILD
AND
THE
MATH
PROFESSOR
TO
BE
THE
SAME
AND
THEN
SETTING
THE
CHILD
PRIOR
WEIGHT
TO
BE
ZERO
ON
CERTAIN
ADVANCED
CONCEPTS
THUS
THERE
IS
NO
SHARP
DISTINCTION
BETWEEN
THE
PRIOR
AND
THE
HYPOTHESIS
SPACE
ALTHOUGH
THE
SUBJECTIVITY
OF
THE
PRIOR
IS
CONTROVERSIAL
IT
IS
ACTUALLY
QUITE
USEFUL
IF
YOU
ARE
TOLD
THE
NUMBERS
ARE
FROM
SOME
ARITHMETIC
RULE
THEN
GIVEN
AND
YOU
MAY
THINK
IS
LIKELY
BUT
IS
UNLIKELY
BUT
IF
YOU
ARE
TOLD
THAT
THE
NUMBERS
ARE
EXAMPLES
OF
HEALTHY
CHOLESTEROL
LEVELS
YOU
WOULD
PROBABLY
THINK
IS
UNLIKELY
AND
IS
LIKELY
THUS
WE
SEE
THAT
THE
PRIOR
IS
THE
MECHANISM
BY
WHICH
BACKGROUND
KNOWLEDGE
CAN
BE
BROUGHT
TO
BEAR
ON
A
PROBLEM
WITHOUT
THIS
RAPID
LEARNING
I
E
FROM
SMALL
SAMPLES
SIZES
IS
IMPOSSIBLE
SO
WHAT
PRIOR
SHOULD
WE
USE
FOR
ILLUSTRATION
PURPOSES
LET
US
USE
A
SIMPLE
PRIOR
WHICH
PUTS
UNIFORM
PROBABILITY
ON
SIMPLE
ARITHMETICAL
CONCEPTS
SUCH
AS
EVEN
NUMBERS
ODD
NUMBERS
PRIME
NUMBERS
NUMBERS
ENDING
IN
ETC
TO
MAKE
THINGS
MORE
INTERESTING
WE
MAKE
THE
CONCEPTS
EVEN
AND
ODD
MORE
LIKELY
APRIORI
WE
ALSO
INCLUDE
TWO
UNNATURAL
CONCEPTS
NAMELY
POWERS
OF
PLUS
AND
POWERS
OF
EXCEPT
BUT
GIVE
THEM
LOW
PRIOR
WEIGHT
SEE
FIGURE
A
FOR
A
PLOT
OF
THIS
PRIOR
WE
WILL
CONSIDER
A
SLIGHTLY
MORE
SOPHISTICATED
PRIOR
LATER
ON
POSTERIOR
THE
POSTERIOR
IS
SIMPLY
THE
LIKELIHOOD
TIMES
THE
PRIOR
NORMALIZED
IN
THIS
CONTEXT
WE
HAVE
P
D
H
P
H
P
H
I
D
H
H
N
P
H
D
HT
H
P
D
H
HT
H
P
H
I
D
H
H
N
WHERE
I
H
IS
IFF
IFF
AND
ONLY
IF
ALL
THE
DATA
ARE
IN
THE
EXTENSION
OF
THE
HYPOTHESIS
H
FIGURE
PLOTS
THE
PRIOR
LIKELIHOOD
AND
POSTERIOR
AFTER
SEEING
WE
SEE
THAT
THE
POSTERIOR
IS
A
COMBINATION
OF
PRIOR
AND
LIKELIHOOD
IN
THE
CASE
OF
MOST
OF
THE
CONCEPTS
THE
PRIOR
IS
UNIFORM
SO
THE
POSTERIOR
IS
PROPORTIONAL
TO
THE
LIKELIHOOD
HOWEVER
THE
UNNATURAL
CONCEPTS
OF
POWERS
OF
PLUS
AND
POWERS
OF
EXCEPT
HAVE
LOW
POSTERIOR
SUPPORT
DESPITE
HAVING
HIGH
LIKELIHOOD
DUE
TO
THE
LOW
PRIOR
CONVERSELY
THE
CONCEPT
OF
ODD
NUMBERS
HAS
LOW
POSTERIOR
SUPPORT
DESPITE
HAVING
A
HIGH
PRIOR
DUE
TO
THE
LOW
LIKELIHOOD
FIGURE
PLOTS
THE
PRIOR
LIKELIHOOD
AND
POSTERIOR
AFTER
SEEING
NOW
THE
LIKELIHOOD
IS
MUCH
MORE
PEAKED
ON
THE
POWERS
OF
TWO
CONCEPT
SO
THIS
DOMINATES
THE
POSTERIOR
ESSENTIALLY
THE
LEARNER
HAS
AN
AHA
MOMENT
AND
FIGURES
OUT
THE
TRUE
CONCEPT
HERE
WE
SEE
THE
NEED
FOR
THE
LOW
PRIOR
ON
THE
UNNATURAL
CONCEPTS
OTHERWISE
WE
WOULD
HAVE
OVERFIT
THE
DATA
AND
PICKED
POWERS
OF
EXCEPT
FOR
IN
GENERAL
WHEN
WE
HAVE
ENOUGH
DATA
THE
POSTERIOR
P
H
BECOMES
PEAKED
ON
A
SINGLE
CONCEPT
NAMELY
THE
MAP
ESTIMATE
I
E
P
H
D
ΔHˆMAP
H
WHERE
HˆMAP
ARGMAXH
P
H
IS
THE
POSTERIOR
MODE
AND
WHERE
Δ
IS
THE
DIRAC
MEASURE
DEFINED
BY
Δ
IF
X
A
IF
X
A
DATA
EVEN
ODD
SQUARES
MULT
OF
MULT
OF
MULT
OF
MULT
OF
MULT
OF
MULT
OF
MULT
OF
MULT
OF
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
ALL
POWERS
OF
POWERS
OF
PRIOR
LIK
POST
FIGURE
PRIOR
LIKELIHOOD
AND
POSTERIOR
FOR
D
BASED
ON
TENENBAUM
FIGURE
GENERATED
BY
NUMBERSGAME
NOTE
THAT
THE
MAP
ESTIMATE
CAN
BE
WRITTEN
AS
HˆMAP
ARGMAX
P
D
H
P
H
ARGMAX
LOG
P
D
H
LOG
P
H
H
H
SINCE
THE
LIKELIHOOD
TERM
DEPENDS
EXPONENTIALLY
ON
N
AND
THE
PRIOR
STAYS
CONSTANT
AS
WE
GET
MORE
AND
MORE
DATA
THE
MAP
ESTIMATE
CONVERGES
TOWARDS
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
OR
MLE
HˆMLE
ARGMAX
P
D
H
ARGMAX
LOG
P
D
H
H
H
IN
OTHER
WORDS
IF
WE
HAVE
ENOUGH
DATA
WE
SEE
THAT
THE
DATA
OVERWHELMS
THE
PRIOR
IN
THIS
DATA
EVEN
ODD
SQUARES
MULT
OF
MULT
OF
MULT
OF
MULT
OF
MULT
OF
MULT
OF
MULT
OF
MULT
OF
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
ENDS
IN
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
POWERS
OF
ALL
POWERS
OF
POWERS
OF
PRIOR
LIK
X
POST
FIGURE
PRIOR
LIKELIHOOD
AND
POSTERIOR
FOR
D
BASED
ON
TENENBAUM
FIGURE
GENERATED
BY
NUMBERSGAME
CASE
THE
MAP
ESTIMATE
CONVERGES
TOWARDS
THE
MLE
IF
THE
TRUE
HYPOTHESIS
IS
IN
THE
HYPOTHESIS
SPACE
THEN
THE
MAP
ML
ESTIMATE
WILL
CONVERGE
UPON
THIS
HYPOTHESIS
THUS
WE
SAY
THAT
BAYESIAN
INFERENCE
AND
ML
ESTIMATION
ARE
CONSISTENT
ESTIMATORS
SEE
SECTION
FOR
DETAILS
WE
ALSO
SAY
THAT
THE
HYPOTHESIS
SPACE
IS
IDENTIFIABLE
IN
THE
LIMIT
MEANING
WE
CAN
RECOVER
THE
TRUTH
IN
THE
LIMIT
OF
INFINITE
DATA
IF
OUR
HYPOTHESIS
CLASS
IS
NOT
RICH
ENOUGH
TO
REPRESENT
THE
TRUTH
WHICH
WILL
USUALLY
BE
THE
CASE
WE
WILL
CONVERGE
ON
THE
HYPOTHESIS
THAT
IS
AS
CLOSE
AS
POSSIBLE
TO
THE
TRUTH
HOWEVER
FORMALIZING
THIS
NOTION
OF
CLOSENESS
IS
BEYOND
THE
SCOPE
OF
THIS
CHAPTER
POWERS
OF
POWERS
OF
ENDS
IN
SQUARES
EVEN
MULT
OF
MULT
OF
ALL
POWERS
OF
POWERS
OF
P
H
FIGURE
POSTERIOR
OVER
HYPOTHESES
AND
THE
CORRESPONDING
PREDICTIVE
DISTRIBUTION
AFTER
SEEING
ONE
EXAMPLE
D
A
DOT
MEANS
THIS
NUMBER
IS
CONSISTENT
WITH
THIS
HYPOTHESIS
THE
GRAPH
P
H
D
ON
THE
RIGHT
IS
THE
WEIGHT
GIVEN
TO
HYPOTHESIS
H
BY
TAKING
A
WEIGHED
SUM
OF
DOTS
WE
GET
P
X
C
D
TOP
BASED
ON
FIGURE
OF
TENENBAUM
FIGURE
GENERATED
BY
NUMBERSGAME
POSTERIOR
PREDICTIVE
DISTRIBUTION
THE
POSTERIOR
IS
OUR
INTERNAL
BELIEF
STATE
ABOUT
THE
WORLD
THE
WAY
TO
TEST
IF
OUR
BELIEFS
ARE
JUSTIFIED
IS
TO
USE
THEM
TO
PREDICT
OBJECTIVELY
OBSERVABLE
QUANTITIES
THIS
IS
THE
BASIS
OF
THE
SCIENTIFIC
METHOD
SPECIFICALLY
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
IN
THIS
CONTEXT
IS
GIVEN
BY
P
X
C
D
P
Y
X
H
P
H
D
H
THIS
IS
JUST
A
WEIGHTED
AVERAGE
OF
THE
PREDICTIONS
OF
EACH
INDIVIDUAL
HYPOTHESIS
AND
IS
CALLED
BAYES
MODEL
AVERAGING
HOETING
ET
AL
THIS
IS
ILLUSTRATED
IN
FIGURE
THE
DOTS
AT
THE
BOTTOM
SHOW
THE
PREDICTIONS
FROM
EACH
HYPOTHESIS
THE
VERTICAL
CURVE
ON
THE
RIGHT
SHOWS
THE
WEIGHT
ASSOCIATED
WITH
EACH
HYPOTHESIS
IF
WE
MULTIPLY
EACH
ROW
BY
ITS
WEIGHT
AND
ADD
UP
WE
GET
THE
DISTRIBUTION
AT
THE
TOP
WHEN
WE
HAVE
A
SMALL
AND
OR
AMBIGUOUS
DATASET
THE
POSTERIOR
P
H
IS
VAGUE
WHICH
INDUCES
A
BROAD
PREDICTIVE
DISTRIBUTION
HOWEVER
ONCE
WE
HAVE
FIGURED
THINGS
OUT
THE
POSTERIOR
BECOMES
A
DELTA
FUNCTION
CENTERED
AT
THE
MAP
ESTIMATE
IN
THIS
CASE
THE
PREDICTIVE
DISTRIBUTION
BECOMES
P
X
C
D
P
X
H
ΔHˆ
H
P
X
Hˆ
H
THIS
IS
CALLED
A
PLUG
IN
APPROXIMATION
TO
THE
PREDICTIVE
DENSITY
AND
IS
VERY
WIDELY
USED
DUE
TO
ITS
SIMPLICITY
HOWEVER
IN
GENERAL
THIS
UNDER
REPRESENTS
OUR
UNCERTAINTY
AND
OUR
PREDICTIONS
WILL
NOT
BE
AS
SMOOTH
AS
WHEN
USING
BMA
WE
WILL
SEE
MORE
EXAMPLES
OF
THIS
LATER
IN
THE
BOOK
ALTHOUGH
MAP
LEARNING
IS
SIMPLE
IT
CANNOT
EXPLAIN
THE
GRADUAL
SHIFT
FROM
SIMILARITY
BASED
REASONING
WITH
UNCERTAIN
POSTERIORS
TO
RULE
BASED
REASONING
WITH
CERTAIN
POSTERIORS
FOR
EXAMPLE
SUPPOSE
WE
OBSERVE
IF
WE
USE
THE
SIMPLE
PRIOR
ABOVE
THE
MINIMAL
CONSISTENT
HYPOTHESIS
IS
ALL
POWERS
OF
SO
ONLY
AND
GET
A
NON
ZERO
PROBABILITY
OF
BEING
PREDICTED
THIS
IS
OF
COURSE
AN
EXAMPLE
OF
OVERFITTING
GIVEN
THE
MAP
HYPOTHESIS
IS
ALL
POWERS
OF
TWO
THUS
THE
PLUG
IN
PREDICTIVE
DISTRIBUTION
GETS
BROADER
OR
STAYS
THE
SAME
AS
WE
SEE
MORE
DATA
IT
STARTS
NARROW
BUT
IS
FORCED
TO
BROADEN
AS
IT
SEEMS
MORE
DATA
IN
CONTRAST
IN
THE
BAYESIAN
APPROACH
WE
START
BROAD
AND
THEN
NARROW
DOWN
AS
WE
LEARN
MORE
WHICH
MAKES
MORE
INTUITIVE
SENSE
IN
PARTICULAR
GIVEN
THERE
ARE
MANY
HYPOTHESES
WITH
NON
NEGLIGIBLE
POSTERIOR
SUPPORT
SO
THE
PREDICTIVE
DISTRIBUTION
IS
BROAD
HOWEVER
WHEN
WE
SEE
THE
POSTERIOR
CONCENTRATES
ITS
MASS
ON
ONE
HYPOTHESIS
SO
THE
PREDICTIVE
DISTRIBUTION
BECOMES
NARROWER
SO
THE
PREDICTIONS
MADE
BY
A
PLUG
IN
APPROACH
AND
A
BAYESIAN
APPROACH
ARE
QUITE
DIFFERENT
IN
THE
SMALL
SAMPLE
REGIME
ALTHOUGH
THEY
CONVERGE
TO
THE
SAME
ANSWER
AS
WE
SEE
MORE
DATA
A
MORE
COMPLEX
PRIOR
TO
MODEL
HUMAN
BEHAVIOR
TENENBAUM
USED
A
SLIGHTLY
MORE
SOPHISTICATED
PRIOR
WHICH
WAS
DE
RIVED
BY
ANALYSING
SOME
EXPERIMENTAL
DATA
OF
HOW
PEOPLE
MEASURE
SIMILARITY
BETWEEN
NUMBERS
SEE
TENENBAUM
FOR
DETAILS
THE
RESULT
IS
A
SET
OF
ARITHMETICAL
CONCEPTS
SIMILAR
TO
THOSE
MENTIONED
ABOVE
PLUS
ALL
INTERVALS
BETWEEN
N
AND
M
FOR
N
M
NOTE
THAT
THESE
HYPOTHESES
ARE
NOT
MUTUALLY
EXCLUSIVE
THUS
THE
PRIOR
IS
A
MIXTURE
OF
TWO
PRIORS
ONE
OVER
ARITHMETICAL
RULES
AND
ONE
OVER
INTERVALS
P
H
H
PINTERVAL
H
THE
ONLY
FREE
PARAMETER
IN
THE
MODEL
IS
THE
RELATIVE
WEIGHT
GIVEN
TO
THESE
TWO
PARTS
OF
THE
PRIOR
THE
RESULTS
ARE
NOT
VERY
SENSITIVE
TO
THIS
VALUE
SO
LONG
AS
REFLECTING
THE
FACT
THAT
PEOPLE
ARE
MORE
LIKELY
TO
THINK
OF
CONCEPTS
DEFINED
BY
RULES
THE
PREDICTIVE
DISTRIBUTION
OF
THE
MODEL
USING
THIS
LARGER
HYPOTHESIS
SPACE
IS
SHOWN
IN
FIGURE
IT
IS
STRIKINGLY
SIMILAR
TO
THE
HUMAN
PREDICTIVE
DISTRIBUTION
SHOWN
IN
FIGURE
EVEN
THOUGH
IT
WAS
NOT
FIT
TO
HUMAN
DATA
MODULO
THE
CHOICE
OF
HYPOTHESIS
SPACE
THE
BETA
BINOMIAL
MODEL
THE
NUMBER
GAME
INVOLVED
INFERRING
A
DISTRIBUTION
OVER
A
DISCRETE
VARIABLE
DRAWN
FROM
A
FINITE
HYPOTHESIS
SPACE
H
GIVEN
A
SERIES
OF
DISCRETE
OBSERVATIONS
THIS
MADE
THE
COMPUTATIONS
PARTICULARLY
SIMPLE
WE
JUST
NEEDED
TO
SUM
MULTIPLY
AND
DIVIDE
HOWEVER
IN
MANY
APPLICATIONS
THE
UNKNOWN
PARAMETERS
ARE
CONTINUOUS
SO
THE
HYPOTHESIS
SPACE
IS
SOME
SUBSET
OF
RK
WHERE
EXAMPLES
FIGURE
PREDICTIVE
DISTRIBUTIONS
FOR
THE
MODEL
USING
THE
FULL
HYPOTHESIS
SPACE
COMPARE
TO
FIGURE
THE
PREDICTIONS
OF
THE
BAYESIAN
MODEL
ARE
ONLY
PLOTTED
FOR
THOSE
VALUES
OF
X
FOR
WHICH
HUMAN
DATA
IS
AVAILABLE
THIS
IS
WHY
THE
TOP
LINE
LOOKS
SPARSER
THAN
FIGURE
SOURCE
FIGURE
OF
TENENBAUM
USED
WITH
KIND
PERMISSION
OF
JOSH
TENENBAUM
K
IS
THE
NUMBER
OF
PARAMETERS
THIS
COMPLICATES
THE
MATHEMATICS
SINCE
WE
HAVE
TO
REPLACE
SUMS
WITH
INTEGRALS
HOWEVER
THE
BASIC
IDEAS
ARE
THE
SAME
WE
WILL
ILLUSTRATE
THIS
BY
CONSIDERING
THE
PROBLEM
OF
INFERRING
THE
PROBABILITY
THAT
A
COIN
SHOWS
UP
HEADS
GIVEN
A
SERIES
OF
OBSERVED
COIN
TOSSES
ALTHOUGH
THIS
MIGHT
SEEM
TRIVIAL
IT
TURNS
OUT
THAT
THIS
MODEL
FORMS
THE
BASIS
OF
MANY
OF
THE
METHODS
WE
WILL
CONSIDER
LATER
IN
THIS
BOOK
INCLUDING
NAIVE
BAYES
CLASSIFIERS
MARKOV
MODELS
ETC
IT
IS
HISTORICALLY
IMPORTANT
SINCE
IT
WAS
THE
EXAMPLE
WHICH
WAS
ANALYZED
IN
BAYES
ORIGINAL
PAPER
OF
BAYES
ANALYSIS
WAS
SUBSEQUENTLY
GENERALIZED
BY
PIERRE
SIMON
LAPLACE
CREATING
WHAT
WE
NOW
CALL
BAYES
RULE
SEE
STIGLER
FOR
FURTHER
HISTORICAL
DETAILS
WE
WILL
FOLLOW
OUR
NOW
FAMILIAR
RECIPE
OF
SPECIFYING
THE
LIKELIHOOD
AND
PRIOR
AND
DERIVING
THE
POSTERIOR
AND
POSTERIOR
PREDICTIVE
LIKELIHOOD
SUPPOSE
XI
BER
Θ
WHERE
XI
REPRESENTS
HEADS
XI
REPRESENTS
TAILS
AND
Θ
IS
THE
RATE
PARAMETER
PROBABILITY
OF
HEADS
IF
THE
DATA
ARE
IID
THE
LIKELIHOOD
HAS
THE
FORM
P
D
Θ
Θ
WHERE
WE
HAVE
N
I
XI
HEADS
AND
N
I
XI
TAILS
THESE
TWO
COUNTS
ARE
CALLED
THE
SUFFICIENT
STATISTICS
OF
THE
DATA
SINCE
THIS
IS
ALL
WE
NEED
TO
KNOW
ABOUT
TO
INFER
Θ
AN
ALTERNATIVE
SET
OF
SUFFICIENT
STATISTICS
ARE
AND
N
MORE
FORMALLY
WE
SAY
IS
A
SUFFICIENT
STATISTIC
FOR
DATA
IF
P
Θ
P
Θ
DATA
IF
WE
USE
A
UNIFORM
PRIOR
THIS
IS
EQUIVALENT
TO
SAYING
P
Θ
P
Θ
CONSEQUENTLY
IF
WE
HAVE
TWO
DATASETS
WITH
THE
SAME
SUFFICIENT
STATISTICS
WE
WILL
INFER
THE
SAME
VALUE
FOR
Θ
NOW
SUPPOSE
THE
DATA
CONSISTS
OF
THE
COUNT
OF
THE
NUMBER
OF
HEADS
OBSERVED
IN
A
FIXED
NUMBER
N
OF
TRIALS
IN
THIS
CASE
WE
HAVE
BIN
N
Θ
WHERE
BIN
REPRESENTS
THE
BINOMIAL
DISTRIBUTION
WHICH
HAS
THE
FOLLOWING
PMF
BIN
K
N
Θ
N
ΘK
Θ
N
K
SINCE
N
IS
A
CONSTANT
INDEPENDENT
OF
Θ
THE
LIKELIHOOD
FOR
THE
BINOMIAL
SAMPLING
MODEL
IS
THE
K
SAME
AS
THE
LIKELIHOOD
FOR
THE
BERNOULLI
MODEL
SO
ANY
INFERENCES
WE
MAKE
ABOUT
Θ
WILL
BE
THE
SAME
WHETHER
WE
OBSERVE
THE
COUNTS
D
N
OR
A
SEQUENCE
OF
TRIALS
D
XN
PRIOR
WE
NEED
A
PRIOR
WHICH
HAS
SUPPORT
OVER
THE
INTERVAL
TO
MAKE
THE
MATH
EASIER
IT
WOULD
CONVENIENT
IF
THE
PRIOR
HAD
THE
SAME
FORM
AS
THE
LIKELIHOOD
I
E
IF
THE
PRIOR
LOOKED
LIKE
P
Θ
Θ
FOR
SOME
PRIOR
PARAMETERS
AND
IF
THIS
WERE
THE
CASE
THEN
WE
COULD
EASILY
EVALUATE
THE
POSTERIOR
BY
SIMPLY
ADDING
UP
THE
EXPONENTS
P
Θ
P
D
Θ
P
Θ
Θ
Θ
Θ
WHEN
THE
PRIOR
AND
THE
POSTERIOR
HAVE
THE
SAME
FORM
WE
SAY
THAT
THE
PRIOR
IS
A
CONJUGATE
PRIOR
FOR
THE
CORRESPONDING
LIKELIHOOD
CONJUGATE
PRIORS
ARE
WIDELY
USED
BECAUSE
THEY
SIMPLIFY
COMPUTATION
AND
ARE
EASY
TO
INTERPRET
AS
WE
SEE
BELOW
IN
THE
CASE
OF
THE
BERNOULLI
THE
CONJUGATE
PRIOR
IS
THE
BETA
DISTRIBUTION
WHICH
WE
ENCOUNTERED
IN
SECTION
BETA
Θ
A
B
ΘA
Θ
B
THE
PARAMETERS
OF
THE
PRIOR
ARE
CALLED
HYPER
PARAMETERS
WE
CAN
SET
THEM
IN
ORDER
TO
ENCODE
OUR
PRIOR
BELIEFS
FOR
EXAMPLE
TO
ENCODE
OUR
BELIEFS
THAT
Θ
HAS
MEAN
AND
STANDARD
DEVIATION
WE
SET
A
AND
B
EXERCISE
OR
TO
ENCODE
OUR
BELIEFS
THAT
Θ
HAS
MEAN
AND
THAT
WE
THINK
IT
LIVES
IN
THE
INTERVAL
WITH
PROBABILITY
THEN
WE
FIND
A
AND
B
EXERCISE
IF
WE
KNOW
NOTHING
ABOUT
Θ
EXCEPT
THAT
IT
LIES
IN
THE
INTERVAL
WE
CAN
USE
A
UNI
FORM
PRIOR
WHICH
IS
A
KIND
OF
UNINFORMATIVE
PRIOR
SEE
SECTION
FOR
DETAILS
THE
UNIFORM
DISTRIBUTION
CAN
BE
REPRESENTED
BY
A
BETA
DISTRIBUTION
WITH
A
B
A
B
FIGURE
A
UPDATING
A
BETA
PRIOR
WITH
A
BINOMIAL
LIKELIHOOD
WITH
SUFFICIENT
STATISTICS
TO
YIELD
A
BETA
POSTERIOR
B
UPDATING
A
BETA
PRIOR
WITH
A
BINOMIAL
LIKELI
HOOD
WITH
SUFFICIENT
STATISTICS
TO
YIELD
A
BETA
POSTERIOR
FIGURE
GENERATED
BY
BINOMIALBETAPOSTERIORDEMO
POSTERIOR
IF
WE
MULTIPLY
THE
LIKELIHOOD
BY
THE
BETA
PRIOR
WE
GET
THE
FOLLOWING
POSTERIOR
FOLLOWING
EQUA
TION
P
Θ
D
BIN
Θ
BETA
Θ
A
B
BETA
Θ
A
B
IN
PARTICULAR
THE
POSTERIOR
IS
OBTAINED
BY
ADDING
THE
PRIOR
HYPER
PARAMETERS
TO
THE
EMPIRICAL
COUNTS
FOR
THIS
REASON
THE
HYPER
PARAMETERS
ARE
KNOWN
AS
PSEUDO
COUNTS
THE
STRENGTH
OF
THE
PRIOR
ALSO
KNOWN
AS
THE
EFFECTIVE
SAMPLE
SIZE
OF
THE
PRIOR
IS
THE
SUM
OF
THE
PSEUDO
COUNTS
A
B
THIS
PLAYS
A
ROLE
ANALOGOUS
TO
THE
DATA
SET
SIZE
N
FIGURE
A
GIVES
AN
EXAMPLE
WHERE
WE
UPDATE
A
WEAK
BETA
PRIOR
WITH
A
PEAKED
LIKELIHOOD
FUNCTION
CORRESPONDING
TO
A
LARGE
SAMPLE
SIZE
WE
SEE
THAT
THE
POSTERIOR
IS
ESSENTIALLY
IDENTICAL
TO
THE
LIKELIHOOD
SINCE
THE
DATA
HAS
OVERWHELMED
THE
PRIOR
FIGURE
B
GIVES
AN
EXAMPLE
WHERE
WE
UPDATE
A
STRONG
BETA
PRIOR
WITH
A
PEAKED
LIKELIHOOD
FUNCTION
NOW
WE
SEE
THAT
THE
POSTERIOR
IS
A
COMPROMISE
BETWEEN
THE
PRIOR
AND
LIKELIHOOD
NOTE
THAT
UPDATING
THE
POSTERIOR
SEQUENTIALLY
IS
EQUIVALENT
TO
UPDATING
IN
A
SINGLE
BATCH
TO
SEE
THIS
SUPPOSE
WE
HAVE
TWO
DATA
SETS
DA
AND
DB
WITH
SUFFICIENT
STATISTICS
N
A
N
A
AND
N
B
N
B
LET
N
A
N
B
AND
N
A
N
B
BE
THE
SUFFICIENT
STATISTICS
OF
THE
COMBINED
DATASETS
IN
BATCH
MODE
WE
HAVE
P
Θ
DA
DB
BIN
Θ
BETA
Θ
A
B
BETA
Θ
A
B
IN
SEQUENTIAL
MODE
WE
HAVE
P
Θ
DA
DB
P
DB
Θ
P
Θ
DA
B
B
B
A
A
BIN
Θ
BETA
Θ
A
B
A
B
A
B
BETA
Θ
A
B
THIS
MAKES
BAYESIAN
INFERENCE
PARTICULARLY
WELL
SUITED
TO
ONLINE
LEARNING
AS
WE
WILL
SEE
LATER
POSTERIOR
MEAN
AND
MODE
FROM
EQUATION
THE
MAP
ESTIMATE
IS
GIVEN
BY
ΘˆMAP
A
A
B
N
IF
WE
USE
A
UNIFORM
PRIOR
THEN
THE
MAP
ESTIMATE
REDUCES
TO
THE
MLE
WHICH
IS
JUST
THE
EMPIRICAL
FRACTION
OF
HEADS
ΘˆMLE
N
THIS
MAKES
INTUITIVE
SENSE
BUT
IT
CAN
ALSO
BE
DERIVED
BY
APPLYING
ELEMENTARY
CALCULUS
TO
MAXIMIZE
THE
LIKELIHOOD
FUNCTION
IN
EQUATION
EXERCISE
BY
CONTRAST
THE
POSTERIOR
MEAN
IS
GIVEN
BY
A
Θ
A
B
N
THIS
DIFFERENCE
BETWEEN
THE
MODE
AND
THE
MEAN
WILL
PROVE
IMPORTANT
LATER
WE
WILL
NOW
SHOW
THAT
THE
POSTERIOR
MEAN
IS
CONVEX
COMBINATION
OF
THE
PRIOR
MEAN
AND
THE
MLE
WHICH
CAPTURES
THE
NOTION
THAT
THE
POSTERIOR
IS
A
COMPROMISE
BETWEEN
WHAT
WE
PREVIOUSLY
BELIEVED
AND
WHAT
THE
DATA
IS
TELLING
US
LET
A
B
BE
THE
EQUIVALENT
SAMPLE
SIZE
OF
THE
PRIOR
WHICH
CONTROLS
ITS
STRENGTH
AND
LET
THE
PRIOR
MEAN
BE
A
THEN
THE
POSTERIOR
MEAN
IS
GIVEN
BY
E
Θ
D
M
N
ΛM
Λ
Θˆ
N
N
N
N
MLE
WHERE
Λ
IS
THE
RATIO
OF
THE
PRIOR
TO
POSTERIOR
EQUIVALENT
SAMPLE
SIZE
SO
THE
WEAKER
THE
PRIOR
THE
SMALLER
IS
Λ
AND
HENCE
THE
CLOSER
THE
POSTERIOR
MEAN
IS
TO
THE
MLE
ONE
CAN
SHOW
SIMILARLY
THAT
THE
POSTERIOR
MODE
IS
A
CONVEX
COMBINATION
OF
THE
PRIOR
MODE
AND
THE
MLE
AND
THAT
IT
TOO
CONVERGES
TO
THE
MLE
POSTERIOR
VARIANCE
THE
MEAN
AND
MODE
ARE
POINT
ESTIMATES
BUT
IT
IS
USEFUL
TO
KNOW
HOW
MUCH
WE
CAN
TRUST
THEM
THE
VARIANCE
OF
THE
POSTERIOR
IS
ONE
WAY
TO
MEASURE
THIS
THE
VARIANCE
OF
THE
BETA
POSTERIOR
IS
GIVEN
BY
A
B
VAR
Θ
D
A
N
B
N
A
N
B
N
WE
CAN
SIMPLIFY
THIS
FORMIDABLE
EXPRESSION
IN
THE
CASE
THAT
N
A
B
TO
GET
VAR
Θ
NNN
Θˆ
Θˆ
N
WHERE
Θˆ
IS
THE
MLE
HENCE
THE
ERROR
BAR
IN
OUR
ESTIMATE
I
E
THE
POSTERIOR
STANDARD
DEVIATION
IS
GIVEN
BY
Σ
IVAR
Θ
D
Θˆ
Θˆ
WE
SEE
THAT
THE
UNCERTAINTY
GOES
DOWN
AT
A
RATE
OF
N
NOTE
HOWEVER
THAT
THE
UNCERTAINTY
VARIANCE
IS
MAXIMIZED
WHEN
Θˆ
AND
IS
MINIMIZED
WHEN
Θˆ
IS
CLOSE
TO
OR
THIS
MEANS
IT
IS
EASIER
TO
BE
SURE
THAT
A
COIN
IS
BIASED
THAN
TO
BE
SURE
THAT
IT
IS
FAIR
POSTERIOR
PREDICTIVE
DISTRIBUTION
SO
FAR
WE
HAVE
BEEN
FOCUSING
ON
INFERENCE
OF
THE
UNKNOWN
PARAMETER
LET
US
NOW
TURN
OUR
ATTENTION
TO
PREDICTION
OF
FUTURE
OBSERVABLE
DATA
CONSIDER
PREDICTING
THE
PROBABILITY
OF
HEADS
IN
A
SINGLE
FUTURE
TRIAL
UNDER
A
BETA
A
B
POSTE
RIOR
WE
HAVE
P
X
D
P
X
Θ
P
Θ
D
DΘ
A
Θ
BETA
Θ
A
B
DΘ
E
Θ
D
A
B
THUS
WE
SEE
THAT
THE
MEAN
OF
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
IS
EQUIVALENT
IN
THIS
CASE
TO
PLUGGING
IN
THE
POSTERIOR
MEAN
PARAMETERS
P
X
D
BER
X
E
Θ
D
OVERFITTING
AND
THE
BLACK
SWAN
PARADOX
SUPPOSE
INSTEAD
THAT
WE
PLUG
IN
THE
MLE
I
E
WE
USE
P
X
BER
X
ΘˆMLE
UNFORTUNATELY
THIS
APPROXIMATION
CAN
PERFORM
QUITE
POORLY
WHEN
THE
SAMPLE
SIZE
IS
SMALL
FOR
EXAMPLE
SUPPOSE
WE
HAVE
SEEN
N
TAILS
IN
A
ROW
THE
MLE
IS
Θˆ
SINCE
THIS
MAKES
THE
OBSERVED
DATA
AS
PROBABLE
AS
POSSIBLE
HOWEVER
USING
THIS
ESTIMATE
WE
PREDICT
THAT
HEADS
ARE
IMPOSSIBLE
THIS
IS
CALLED
THE
ZERO
COUNT
PROBLEM
OR
THE
SPARSE
DATA
PROBLEM
AND
FREQUENTLY
OCCURS
WHEN
ESTIMATING
COUNTS
FROM
SMALL
AMOUNTS
OF
DATA
ONE
MIGHT
THINK
THAT
IN
THE
ERA
OF
BIG
DATA
SUCH
CONCERNS
ARE
IRRELEVANT
BUT
NOTE
THAT
ONCE
WE
PARTITION
THE
DATA
BASED
ON
CERTAIN
CRITERIA
SUCH
AS
THE
NUMBER
OF
TIMES
A
SPECIFIC
PERSON
HAS
ENGAGED
IN
A
SPECIFIC
ACTIVITY
THE
SAMPLE
SIZES
CAN
BECOME
MUCH
SMALLER
THIS
PROBLEM
ARISES
FOR
EXAMPLE
WHEN
TRYING
TO
PERFORM
PERSONALIZED
RECOMMENDATION
OF
WEB
PAGES
THUS
BAYESIAN
METHODS
ARE
STILL
USEFUL
EVEN
IN
THE
BIG
DATA
REGIME
JORDAN
THE
ZERO
COUNT
PROBLEM
IS
ANALOGOUS
TO
A
PROBLEM
IN
PHILOSOPHY
CALLED
THE
BLACK
SWAN
PARADOX
THIS
IS
BASED
ON
THE
ANCIENT
WESTERN
CONCEPTION
THAT
ALL
SWANS
WERE
WHITE
IN
THAT
CONTEXT
A
BLACK
SWAN
WAS
A
METAPHOR
FOR
SOMETHING
THAT
COULD
NOT
EXIST
BLACK
SWANS
WERE
DISCOVERED
IN
AUSTRALIA
BY
EUROPEAN
EXPLORERS
IN
THE
CENTURY
THE
TERM
BLACK
SWAN
PARADOX
WAS
FIRST
COINED
BY
THE
FAMOUS
PHILOSOPHER
OF
SCIENCE
KARL
POPPER
THE
TERM
HAS
ALSO
BEEN
USED
AS
THE
TITLE
OF
A
RECENT
POPULAR
BOOK
TALEB
THIS
PARADOX
WAS
USED
TO
ILLUSTRATE
THE
PROBLEM
OF
INDUCTION
WHICH
IS
THE
PROBLEM
OF
HOW
TO
DRAW
GENERAL
CONCLUSIONS
ABOUT
THE
FUTURE
FROM
SPECIFIC
OBSERVATIONS
FROM
THE
PAST
LET
US
NOW
DERIVE
A
SIMPLE
BAYESIAN
SOLUTION
TO
THE
PROBLEM
WE
WILL
USE
A
UNIFORM
PRIOR
SO
A
B
IN
THIS
CASE
PLUGGING
IN
THE
POSTERIOR
MEAN
GIVES
LAPLACE
RULE
OF
SUCCESSION
P
X
D
N
N
THIS
JUSTIFIES
THE
COMMON
PRACTICE
OF
ADDING
TO
THE
EMPIRICAL
COUNTS
NORMALIZING
AND
THEN
PLUGGING
THEM
IN
A
TECHNIQUE
KNOWN
AS
ADD
ONE
SMOOTHING
NOTE
THAT
PLUGGING
IN
THE
MAP
PARAMETERS
WOULD
NOT
HAVE
THIS
SMOOTHING
EFFECT
SINCE
THE
MODE
HAS
THE
FORM
Θˆ
A
WHICH
BECOMES
THE
MLE
IF
A
B
N
A
B
PREDICTING
THE
OUTCOME
OF
MULTIPLE
FUTURE
TRIALS
SUPPOSE
NOW
WE
WERE
INTERESTED
IN
PREDICTING
THE
NUMBER
OF
HEADS
X
IN
M
FUTURE
TRIALS
THIS
IS
GIVEN
BY
P
X
D
M
BIN
X
Θ
M
BETA
Θ
A
B
DΘ
M
R
ΘX
Θ
M
XΘA
Θ
B
X
B
A
B
WE
RECOGNIZE
THE
INTEGRAL
AS
THE
NORMALIZATION
CONSTANT
FOR
A
BETA
A
X
M
X
B
DISTRIBUTION
HENCE
R
THUS
WE
FIND
THAT
THE
POSTERIOR
PREDICTIVE
IS
GIVEN
BY
THE
FOLLOWING
KNOWN
AS
THE
COMPOUND
BETA
BINOMIAL
DISTRIBUTION
BB
X
A
B
M
M
B
X
A
M
X
B
THIS
DISTRIBUTION
HAS
THE
FOLLOWING
MEAN
AND
VARIANCE
E
X
M
A
A
B
MAB
VAR
X
A
B
A
B
M
A
B
IF
M
AND
HENCE
X
WE
SEE
THAT
THE
MEAN
BECOMES
E
X
D
P
X
D
A
WHICH
IS
CONSISTENT
WITH
EQUATION
THIS
PROCESS
IS
ILLUSTRATED
IN
FIGURE
A
WE
START
WITH
A
BETA
PRIOR
AND
PLOT
THE
POSTERIOR
PREDICTIVE
DENSITY
AFTER
SEEING
HEADS
AND
TAILS
FIGURE
B
PLOTS
A
PLUG
IN
APPROXIMATION
USING
A
MAP
ESTIMATE
WE
SEE
THAT
THE
BAYESIAN
PREDICTION
HAS
LONGER
TAILS
SPREADING
ITS
PROBABLITY
MASS
MORE
WIDELY
AND
IS
THEREFORE
LESS
PRONE
TO
OVERFITTING
AND
BLACKSWAN
TYPE
PARADOXES
THE
DIRICHLET
MULTINOMIAL
MODEL
IN
THE
PREVIOUS
SECTION
WE
DISCUSSED
HOW
TO
INFER
THE
PROBABILITY
THAT
A
COIN
COMES
UP
HEADS
IN
THIS
SECTION
WE
GENERALIZE
THESE
RESULTS
TO
INFER
THE
PROBABILITY
THAT
A
DICE
WITH
K
SIDES
COMES
UP
AS
FACE
K
THIS
MIGHT
SEEM
LIKE
ANOTHER
TOY
EXERCISE
BUT
THE
METHODS
WE
WILL
STUDY
ARE
WIDELY
USED
TO
ANALYSE
TEXT
DATA
BIOSEQUENCE
DATA
ETC
AS
WE
WILL
SEE
LATER
POSTERIOR
PREDICTIVE
PLUGIN
PREDICTIVE
A
B
FIGURE
A
POSTERIOR
PREDICTIVE
DISTRIBUTIONS
AFTER
SEEING
B
PLUGIN
APPROXIMATION
FIGURE
GENERATED
BY
BETABINOMPOSTPREDDEMO
LIKELIHOOD
SUPPOSE
WE
OBSERVE
N
DICE
ROLLS
XN
WHERE
XI
K
IF
WE
ASSUME
THE
DATA
IS
IID
THE
LIKELIHOOD
HAS
THE
FORM
K
P
D
Θ
K
K
WHERE
NK
N
I
YI
K
IS
THE
NUMBER
OF
TIMES
EVENT
K
OCCURED
THESE
ARE
THE
SUFFICIENT
STATISTICS
FOR
THIS
MODEL
THE
LIKELIHOOD
FOR
THE
MULTINOMIAL
MODEL
HAS
THE
SAME
FORM
UP
TO
AN
IRRELEVANT
CONSTANT
FACTOR
PRIOR
SINCE
THE
PARAMETER
VECTOR
LIVES
IN
THE
K
DIMENSIONAL
PROBABILITY
SIMPLEX
WE
NEED
A
PRIOR
THAT
HAS
SUPPORT
OVER
THIS
SIMPLEX
IDEALLY
IT
WOULD
ALSO
BE
CONJUGATE
FORTUNATELY
THE
DIRICHLET
DISTRIBUTION
SECTION
SATISFIES
BOTH
CRITERIA
SO
WE
WILL
USE
THE
FOLLOWING
PRIOR
DIR
Θ
Α
B
Α
K
ΑK
K
K
K
POSTERIOR
MULTIPLYING
THE
LIKELIHOOD
BY
THE
PRIOR
WE
FIND
THAT
THE
POSTERIOR
IS
ALSO
DIRICHLET
P
Θ
D
P
D
Θ
P
Θ
K
K
TT
ΘNKΘΑK
TT
ΘΑK
NK
DIR
Θ
ΑK
NK
WE
SEE
THAT
THE
POSTERIOR
IS
OBTAINED
BY
ADDING
THE
PRIOR
HYPER
PARAMETERS
PSEUDO
COUNTS
ΑK
TO
THE
EMPIRICAL
COUNTS
NK
WE
CAN
DERIVE
THE
MODE
OF
THIS
POSTERIOR
I
E
THE
MAP
ESTIMATE
BY
USING
CALCULUS
HOWEVER
WE
MUST
ENFORCE
THE
CONSTRAINT
THAT
K
ΘK
WE
CAN
DO
THIS
BY
USING
A
LAGRANGE
MULTIPLIER
THE
CONSTRAINED
OBJECTIVE
FUNCTION
OR
LAGRANGIAN
IS
GIVEN
BY
THE
LOG
LIKELIHOOD
PLUS
LOG
PRIOR
PLUS
THE
CONSTRAINT
Θ
Λ
NK
LOG
ΘK
ΑK
LOG
ΘK
Λ
ΘK
TO
SIMPLIFY
NOTATION
WE
DEFINE
NK
NK
ΑK
TAKING
DERIVATIVES
WITH
RESPECT
TO
Λ
YIELDS
THE
ORIGINAL
CONSTRAINT
Λ
ΘK
TAKING
DERIVATIVES
WITH
RESPECT
TO
ΘK
YIELDS
ΘK
NK
NK
Λ
ΘK
ΛΘK
WE
CAN
SOLVE
FOR
Λ
USING
THE
SUM
TO
ONE
CONSTRAINT
NK
K
Λ
ΘK
K
N
K
Λ
WHERE
K
ΑK
IS
THE
EQUIVALENT
SAMPLE
SIZE
OF
THE
PRIOR
THUS
THE
MAP
ESTIMATE
IS
GIVEN
BY
ΘˆK
NK
ΑK
N
K
WHICH
IS
CONSISTENT
WITH
EQUATION
IF
WE
USE
A
UNIFORM
PRIOR
ΑK
WE
RECOVER
THE
MLE
ΘˆK
NK
N
THIS
IS
JUST
THE
EMPIRICAL
FRACTION
OF
TIMES
FACE
K
SHOWS
UP
WE
DO
NOT
NEED
TO
EXPLICITLY
ENFORCE
THE
CONSTRAINT
THAT
ΘK
SINCE
THE
GRADIENT
OF
THE
OBJECTIVE
HAS
THE
FORM
NK
ΘK
Λ
SO
NEGATIVE
VALUES
WOULD
REDUCE
THE
OBJECTIVE
RATHER
THAN
MAXIMIZE
IT
OF
COURSE
THIS
DOES
NOT
PRECLUDE
SETTING
ΘK
AND
INDEED
THIS
IS
THE
OPTIMAL
SOLUTION
IF
NK
AND
ΑK
POSTERIOR
PREDICTIVE
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
FOR
A
SINGLE
MULTINOULLI
TRIAL
IS
GIVEN
BY
THE
FOLLOWING
EXPRESSION
P
X
J
D
R
P
X
J
Θ
P
Θ
D
DΘ
R
P
X
J
ΘJ
R
P
Θ
J
ΘJ
D
DΘ
JL
DΘJ
R
Θ
P
Θ
D
DΘ
ΑJ
NJ
Θ
D
ΑJ
NJ
WHERE
Θ
J
ARE
ALL
THE
COMPONENTS
OF
Θ
EXCEPT
ΘJ
SEE
ALSO
EXERCISE
THE
ABOVE
EXPRESSION
AVOIDS
THE
ZERO
COUNT
PROBLEM
JUST
AS
WE
SAW
IN
SECTION
IN
FACT
THIS
FORM
OF
BAYESIAN
SMOOTHING
IS
EVEN
MORE
IMPORTANT
IN
THE
MULTINOMIAL
CASE
THAN
THE
BINARY
CASE
SINCE
THE
LIKELIHOOD
OF
DATA
SPARSITY
INCREASES
ONCE
WE
START
PARTITIONING
THE
DATA
INTO
MANY
CATEGORIES
WORKED
EXAMPLE
LANGUAGE
MODELS
USING
BAG
OF
WORDS
ONE
APPLICATION
OF
BAYESIAN
SMOOTHING
USING
THE
DIRICHLET
MULTINOMIAL
MODEL
IS
TO
LANGUAGE
MODELING
WHICH
MEANS
PREDICTING
WHICH
WORDS
MIGHT
OCCUR
NEXT
IN
A
SEQUENCE
HERE
WE
WILL
TAKE
A
VERY
SIMPLE
MINDED
APPROACH
AND
ASSUME
THAT
THE
I
TH
WORD
XI
K
IS
SAMPLED
INDEPENDENTLY
FROM
ALL
THE
OTHER
WORDS
USING
A
CAT
Θ
DISTRIBUTION
THIS
IS
CALLED
THE
BAG
OF
WORDS
MODEL
GIVEN
A
PAST
SEQUENCE
OF
WORDS
HOW
CAN
WE
PREDICT
WHICH
ONE
IS
LIKELY
TO
COME
NEXT
FOR
EXAMPLE
SUPPOSE
WE
OBSERVE
THE
FOLLOWING
SEQUENCE
PART
OF
A
CHILDREN
NURSERY
RHYME
MARY
HAD
A
LITTLE
LAMB
LITTLE
LAMB
LITTLE
LAMB
MARY
HAD
A
LITTLE
LAMB
ITS
FLEECE
AS
WHITE
AS
SNOW
FURTHERMORE
SUPPOSE
OUR
VOCABULARY
CONSISTS
OF
THE
FOLLOWING
WORDS
MARY
LAMB
LITTLE
BIG
FLEECE
WHITE
BLACK
SNOW
RAIN
UNK
HERE
UNK
STANDS
FOR
UNKNOWN
AND
REPRESENTS
ALL
OTHER
WORDS
THAT
DO
NOT
APPEAR
ELSEWHERE
ON
THE
LIST
TO
ENCODE
EACH
LINE
OF
THE
NURSERY
RHYME
WE
FIRST
STRIP
OFF
PUNCTUATION
AND
REMOVE
ANY
STOP
WORDS
SUCH
AS
A
AS
THE
ETC
WE
CAN
ALSO
PERFORM
STEMMING
WHICH
MEANS
REDUCING
WORDS
TO
THEIR
BASE
FORM
SUCH
AS
STRIPPING
OFF
THE
FINAL
IN
PLURAL
WORDS
OR
THE
ING
FROM
VERBS
E
G
RUNNING
BECOMES
RUN
IN
THIS
EXAMPLE
NO
WORDS
NEED
STEMMING
FINALLY
WE
REPLACE
EACH
WORD
BY
ITS
INDEX
INTO
THE
VOCABULARY
TO
GET
WE
NOW
IGNORE
THE
WORD
ORDER
AND
COUNT
HOW
OFTEN
EACH
WORD
OCCURRED
RESULTING
IN
A
HISTOGRAM
OF
WORD
COUNTS
TOKEN
WORD
MARY
LAMB
LITTLE
BIG
FLEECE
WHITE
BLACK
SNOW
RAIN
UNK
COUNT
DENOTE
THE
ABOVE
COUNTS
BY
NJ
IF
WE
USE
A
DIR
Α
PRIOR
FOR
Θ
THE
POSTERIOR
PREDICTIVE
IS
JUST
ΑJ
NJ
NJ
P
X
J
D
E
ΘJ
D
Α
T
N
T
P
X
J
D
THE
MODES
OF
THE
PREDICTIVE
DISTRIBUTION
ARE
X
LAMB
AND
X
UNK
NOTE
THAT
THE
WORDS
BIG
BLACK
AND
RAIN
ARE
PREDICTED
TO
OCCUR
WITH
NON
ZERO
PROBABILITY
IN
THE
FUTURE
EVEN
THOUGH
THEY
HAVE
NEVER
BEEN
SEEN
BEFORE
LATER
ON
WE
WILL
SEE
MORE
SOPHISTICATED
LANGUAGE
MODELS
NAIVE
BAYES
CLASSIFIERS
IN
THIS
SECTION
WE
DISCUSS
HOW
TO
CLASSIFY
VECTORS
OF
DISCRETE
VALUED
FEATURES
X
K
D
WHERE
K
IS
THE
NUMBER
OF
VALUES
FOR
EACH
FEATURE
AND
D
IS
THE
NUMBER
OF
FEATURES
WE
WILL
USE
A
GENERATIVE
APPROACH
THIS
REQUIRES
US
TO
SPECIFY
THE
CLASS
CONDITIONAL
DISTRIBUTION
P
X
Y
C
THE
SIMPLEST
APPROACH
IS
TO
ASSUME
THE
FEATURES
ARE
CONDITIONALLY
INDEPENDENT
GIVEN
THE
CLASS
LABEL
THIS
ALLOWS
US
TO
WRITE
THE
CLASS
CONDITIONAL
DENSITY
AS
A
PRODUCT
OF
ONE
DIMENSIONAL
DENSITIES
D
P
X
Y
C
Θ
P
XJ
Y
C
ΘJC
J
THE
RESULTING
MODEL
IS
CALLED
A
NAIVE
BAYES
CLASSIFIER
NBC
THE
MODEL
IS
CALLED
NAIVE
SINCE
WE
DO
NOT
EXPECT
THE
FEATURES
TO
BE
INDEPENDENT
EVEN
CONDITIONAL
ON
THE
CLASS
LABEL
HOWEVER
EVEN
IF
THE
NAIVE
BAYES
ASSUMPTION
IS
NOT
TRUE
IT
OFTEN
RESULTS
IN
CLASSIFIERS
THAT
WORK
WELL
DOMINGOS
AND
PAZZANI
ONE
REASON
FOR
THIS
IS
THAT
THE
MODEL
IS
QUITE
SIMPLE
IT
ONLY
HAS
O
CD
PARAMETERS
FOR
C
CLASSES
AND
D
FEATURES
AND
HENCE
IT
IS
RELATIVELY
IMMUNE
TO
OVERFITTING
THE
FORM
OF
THE
CLASS
CONDITIONAL
DENSITY
DEPENDS
ON
THE
TYPE
OF
EACH
FEATURE
WE
GIVE
SOME
POSSIBILITIES
BELOW
IN
THE
CASE
OF
REAL
VALUED
FEATURES
WE
CAN
USE
THE
GAUSSIAN
DISTRIBUTION
P
X
Y
C
Θ
D
N
XJ
ΜJC
WHERE
ΜJC
IS
THE
MEAN
OF
FEATURE
J
IN
OBJECTS
OF
CLASS
C
AND
IS
ITS
IN
THE
CASE
OF
BINARY
FEATURES
XJ
WE
CAN
USE
THE
BERNOULLI
DISTRIBUTION
P
X
Y
C
Θ
D
BER
XJ
ΜJC
WHERE
ΜJC
IS
THE
PROBABILITY
THAT
FEATURE
J
OCCURS
IN
CLASS
C
THIS
IS
SOMETIMES
CALLED
THE
MULTIVARIATE
BERNOULLI
NAIVE
BAYES
MODEL
WE
WILL
SEE
AN
APPLICATION
OF
THIS
BELOW
IN
THE
CASE
OF
CATEGORICAL
FEATURES
XJ
K
WE
CAN
MODEL
USE
THE
MULTINOULLI
DISTRIBUTION
P
X
Y
C
Θ
D
CAT
XJ
Μ
WHERE
Μ
IS
A
HISTOGRAM
OVER
THE
K
JC
JC
POSSIBLE
VALUES
FOR
XJ
IN
CLASS
C
OBVIOUSLY
WE
CAN
HANDLE
OTHER
KINDS
OF
FEATURES
OR
USE
DIFFERENT
DISTRIBUTIONAL
ASSUMPTIONS
ALSO
IT
IS
EASY
TO
MIX
AND
MATCH
FEATURES
OF
DIFFERENT
TYPES
MODEL
FITTING
WE
NOW
DISCUSS
HOW
TO
TRAIN
A
NAIVE
BAYES
CLASSIFIER
THIS
USUALLY
MEANS
COMPUTING
THE
MLE
OR
THE
MAP
ESTIMATE
FOR
THE
PARAMETERS
HOWEVER
WE
WILL
ALSO
DISCUSS
HOW
TO
COMPUTE
THE
FULL
POSTERIOR
P
Θ
D
MLE
FOR
NBC
THE
PROBABILITY
FOR
A
SINGLE
DATA
CASE
IS
GIVEN
BY
P
XI
YI
Θ
P
YI
Π
TT
P
XIJ
ΘJ
TT
ΠI
YI
C
TT
TT
P
XIJ
ΘJC
I
YI
C
J
C
J
C
HENCE
THE
LOG
LIKELIHOOD
IS
GIVEN
BY
C
D
C
LOG
P
D
Θ
NC
LOG
ΠC
LOG
P
XIJ
ΘJC
WE
SEE
THAT
THIS
EXPRESSION
DECOMPOSES
INTO
A
SERIES
OF
TERMS
ONE
CONCERNING
Π
AND
DC
TERMS
CONTAINING
THE
ΘJC
HENCE
WE
CAN
OPTIMIZE
ALL
THESE
PARAMETERS
SEPARATELY
FROM
EQUATION
THE
MLE
FOR
THE
CLASS
PRIOR
IS
GIVEN
BY
ΠˆC
NC
N
WHERE
NC
I
I
YI
C
IS
THE
NUMBER
OF
EXAMPLES
IN
CLASS
C
THE
MLE
FOR
THE
LIKELIHOOD
DEPENDS
ON
THE
TYPE
OF
DISTRIBUTION
WE
CHOOSE
TO
USE
FOR
EACH
FEATURE
FOR
SIMPLICITY
LET
US
SUPPOSE
ALL
FEATURES
ARE
BINARY
SO
XJ
Y
C
BER
ΘJC
IN
THIS
CASE
THE
MLE
BECOMES
ΘˆJC
NJC
NC
IT
IS
EXTREMELY
SIMPLE
TO
IMPLEMENT
THIS
MODEL
FITTING
PROCEDURE
SEE
ALGORITHM
FOR
SOME
PSEUDO
CODE
AND
NAIVEBAYESFIT
FOR
SOME
MATLAB
CODE
THIS
ALGORITHM
OBVIOUSLY
TAKES
O
ND
TIME
THE
METHOD
IS
EASILY
GENERALIZED
TO
HANDLE
FEATURES
OF
MIXED
TYPE
THIS
SIMPLICITY
IS
ONE
REASON
THE
METHOD
IS
SO
WIDELY
USED
FIGURE
GIVES
AN
EXAMPLE
WHERE
WE
HAVE
CLASSES
AND
BINARY
FEATURES
REPRESENTING
THE
PRESENCE
OR
ABSENCE
OF
WORDS
IN
A
BAG
OF
WORDS
MODEL
THE
PLOT
VISUALIZES
THE
ΘC
VECTORS
FOR
THE
TWO
CLASSES
THE
BIG
SPIKE
AT
INDEX
CORRESPONDS
TO
THE
WORD
SUBJECT
WHICH
OCCURS
IN
BOTH
CLASSES
WITH
PROBABILITY
IN
SECTION
WE
DISCUSS
HOW
TO
FILTER
OUT
SUCH
UNINFORMATIVE
FEATURES
ALGORITHM
FITTING
A
NAIVE
BAYES
CLASSIFIER
TO
BINARY
FEATURES
NC
NJC
FOR
I
N
DO
C
YI
CLASS
LABEL
OF
I
TH
EXAMPLE
NC
NC
FOR
J
D
DO
IF
XIJ
THEN
NJC
NJC
ΠˆC
NC
ΘˆJC
NJC
N
N
P
XJ
Y
A
P
XJ
Y
B
FIGURE
CLASS
CONDITIONAL
DENSITIES
P
XJ
Y
C
FOR
TWO
DOCUMENT
CLASSES
CORRESPONDING
TO
X
WINDOWS
AND
MS
WINDOWS
FIGURE
GENERATED
BY
NAIVEBAYESBOWDEMO
BAYESIAN
NAIVE
BAYES
THE
TROUBLE
WITH
MAXIMUM
LIKELIHOOD
IS
THAT
IT
CAN
OVERFIT
FOR
EXAMPLE
CONSIDER
THE
EXAMPLE
IN
FIGURE
THE
FEATURE
CORRESPONDING
TO
THE
WORD
SUBJECT
CALL
IT
FEATURE
J
ALWAYS
OCCURS
IN
BOTH
CLASSES
SO
WE
ESTIMATE
ΘˆJC
WHAT
WILL
HAPPEN
IF
WE
ENCOUNTER
A
NEW
EMAIL
WHICH
DOES
NOT
HAVE
THIS
WORD
IN
IT
OUR
ALGORITHM
WILL
CRASH
AND
BURN
SINCE
WE
WILL
FIND
THAT
P
Y
C
X
Θˆ
FOR
BOTH
CLASSES
THIS
IS
ANOTHER
MANIFESTATION
OF
THE
BLACK
SWAN
PARADOX
DISCUSSED
IN
SECTION
A
SIMPLE
SOLUTION
TO
OVERFITTING
IS
TO
BE
BAYESIAN
FOR
SIMPLICITY
WE
WILL
USE
A
FACTORED
PRIOR
D
C
P
Θ
P
Π
P
ΘJC
J
C
WE
WILL
USE
A
DIR
Α
PRIOR
FOR
Π
AND
A
BETA
PRIOR
FOR
EACH
ΘJC
OFTEN
WE
JUST
TAKE
Α
AND
Β
CORRESPONDING
TO
ADD
ONE
OR
LAPLACE
SMOOTHING
COMBINING
THE
FACTORED
LIKELIHOOD
IN
EQUATION
WITH
THE
FACTORED
PRIOR
ABOVE
GIVES
THE
FOLLOWING
FACTORED
POSTERIOR
D
C
P
Θ
D
P
Π
D
P
ΘJC
D
J
C
P
Π
D
DIR
NC
ΑC
P
ΘJC
D
BETA
NC
NJC
NJC
IN
OTHER
WORDS
TO
COMPUTE
THE
POSTERIOR
WE
JUST
UPDATE
THE
PRIOR
COUNTS
WITH
THE
EMPIRICAL
COUNTS
FROM
THE
LIKELIHOOD
IT
IS
STRAIGHTFORWARD
TO
MODIFY
ALGORITHM
TO
HANDLE
THIS
VERSION
OF
MODEL
FITTING
USING
THE
MODEL
FOR
PREDICTION
AT
TEST
TIME
THE
GOAL
IS
TO
COMPUTE
D
P
Y
C
X
D
P
Y
C
D
P
XJ
Y
C
D
J
THE
CORRECT
BAYESIAN
PROCEDURE
IS
TO
INTEGRATE
OUT
THE
UNKNOWN
PARAMETERS
P
Y
C
X
D
R
CAT
Y
C
Π
P
Π
D
DΠL
JTT
R
BER
XJ
Y
C
ΘJC
P
ΘJC
D
L
FORTUNATELY
THIS
IS
EASY
TO
DO
AT
LEAST
IF
THE
POSTERIOR
IS
DIRICHLET
IN
PARTICULAR
FROM
EQUA
TION
WE
KNOW
THE
POSTERIOR
PREDICTIVE
DENSITY
CAN
BE
OBTAINED
BY
SIMPLY
PLUGGING
IN
THE
POSTERIOR
MEAN
PARAMETERS
Θ
HENCE
D
P
Y
C
X
D
ΠC
ΘJC
I
XJ
ΘJC
I
XJ
J
ΘJK
NJC
NC
Π
NC
ΑC
C
N
WHERE
C
ΑC
IF
WE
HAVE
APPROXIMATED
THE
POSTERIOR
BY
A
SINGLE
POINT
P
Θ
ΔΘˆ
Θ
WHERE
Θˆ
MAY
BE
THE
ML
OR
MAP
ESTIMATE
THEN
THE
POSTERIOR
PREDICTIVE
DENSITY
IS
OBTAINED
BY
SIMPLY
PLUGGING
IN
THE
PARAMETERS
TO
YIELD
A
VIRTUALLY
IDENTICAL
RULE
P
Y
C
X
D
D
ΠˆC
ΘˆJC
I
XJ
ΘˆJC
I
XJ
J
THE
ONLY
DIFFERENCE
IS
WE
REPLACED
THE
POSTERIOR
MEAN
Θ
WITH
THE
POSTERIOR
MODE
OR
MLE
Θˆ
HOWEVER
THIS
SMALL
DIFFERENCE
CAN
BE
IMPORTANT
IN
PRACTICE
SINCE
THE
POSTERIOR
MEAN
WILL
RESULT
IN
LESS
OVERFITTING
SEE
SECTION
THE
LOG
SUM
EXP
TRICK
WE
NOW
DISCUSS
ONE
IMPORTANT
PRACTICAL
DETAIL
THAT
ARISES
WHEN
USING
GENERATIVE
CLASSIFIERS
OF
ANY
KIND
WE
CAN
COMPUTE
THE
POSTERIOR
OVER
CLASS
LABELS
USING
EQUATION
USING
THE
APPROPRIATE
CLASS
CONDITIONAL
DENSITY
AND
A
PLUG
IN
APPROXIMATION
UNFORTUNATELY
A
NAIVE
IMPLEMENTATION
OF
EQUATION
CAN
FAIL
DUE
TO
NUMERICAL
UNDERFLOW
THE
PROBLEM
IS
THAT
P
X
Y
C
IS
OFTEN
A
VERY
SMALL
NUMBER
ESPECIALLY
IF
X
IS
A
HIGH
DIMENSIONAL
VECTOR
THIS
IS
BECAUSE
WE
REQUIRE
THAT
X
P
X
Y
SO
THE
PROBABILITY
OF
OBSERVING
ANY
PARTICULAR
HIGH
DIMENSIONAL
VECTOR
IS
SMALL
THE
OBVIOUS
SOLUTION
IS
TO
TAKE
LOGS
WHEN
APPLYING
BAYES
RULE
AS
FOLLOWS
LOG
P
Y
C
X
BC
LOG
C
CT
EBCT
BC
LOG
P
X
Y
C
LOG
P
Y
C
HOWEVER
THIS
REQUIRES
EVALUATING
THE
FOLLOWING
EXPRESSION
LOG
EBCT
LOG
P
Y
C
X
LOG
P
X
AND
WE
CAN
T
ADD
UP
IN
THE
LOG
DOMAIN
FORTUNATELY
WE
CAN
FACTOR
OUT
THE
LARGEST
TERM
AND
JUST
REPRESENT
THE
REMAINING
NUMBERS
RELATIVE
TO
THAT
FOR
EXAMPLE
LOG
E
E
LOG
E
E
LOG
E
IN
GENERAL
WE
HAVE
LOG
EBC
LOG
R
EBC
B
RLOG
EBC
B
B
WHERE
B
MAXC
BC
THIS
IS
CALLED
THE
LOG
SUM
EXP
TRICK
AND
IS
WIDELY
USED
SEE
THE
FUNCTION
LOGSUMEXP
FOR
AN
IMPLEMENTATION
THIS
TRICK
IS
USED
IN
ALGORITHM
WHICH
GIVES
PSEUDO
CODE
FOR
USING
AN
NBC
TO
COMPUTE
P
YI
XI
Θˆ
SEE
NAIVEBAYESPREDICT
FOR
THE
MATLAB
CODE
NOTE
THAT
WE
DO
NOT
NEED
THE
LOG
SUM
EXP
TRICK
IF
WE
ONLY
WANT
TO
COMPUTE
YˆI
SINCE
WE
CAN
JUST
MAXIMIZE
THE
UNNORMALIZED
QUANTITY
LOG
P
YI
C
LOG
P
XI
Y
C
FEATURE
SELECTION
USING
MUTUAL
INFORMATION
SINCE
AN
NBC
IS
FITTING
A
JOINT
DISTRIBUTION
OVER
POTENTIALLY
MANY
FEATURES
IT
CAN
SUFFER
FROM
OVERFITTING
IN
ADDITION
THE
RUN
TIME
COST
IS
O
D
WHICH
MAY
BE
TOO
HIGH
FOR
SOME
APPLICATIONS
ONE
COMMON
APPROACH
TO
TACKLING
BOTH
OF
THESE
PROBLEMS
IS
TO
PERFORM
FEATURE
SELECTION
TO
REMOVE
IRRELEVANT
FEATURES
THAT
DO
NOT
HELP
MUCH
WITH
THE
CLASSIFICATION
PROBLEM
THE
SIMPLEST
APPROACH
TO
FEATURE
SELECTION
IS
TO
EVALUATE
THE
RELEVANCE
OF
EACH
FEATURE
SEPARATELY
AND
THEN
ALGORITHM
PREDICTING
WITH
A
NAIVE
BAYES
CLASSIFIER
FOR
BINARY
FEATURES
FOR
I
N
DO
FOR
C
C
DO
LIC
LOG
ΠˆC
FOR
J
D
DO
IF
XIJ
THEN
LIC
LIC
LOG
ΘˆJC
ELSE
LIC
LIC
LOG
ΘˆJC
PIC
EXP
LIC
LOGSUMEXP
LI
YˆI
ARGMAXC
PIC
TAKE
THE
TOP
K
WHERE
K
IS
CHOSEN
BASED
ON
SOME
TRADEOFF
BETWEEN
ACCURACY
AND
COMPLEXITY
THIS
APPROACH
IS
KNOWN
AS
VARIABLE
RANKING
FILTERING
OR
SCREENING
ONE
WAY
TO
MEASURE
RELEVANCE
IS
TO
USE
MUTUAL
INFORMATION
SECTION
BETWEEN
FEATURE
XJ
AND
THE
CLASS
LABEL
Y
I
X
Y
P
X
Y
LOG
P
XJ
Y
THE
MUTUAL
INFORMATION
CAN
BE
THOUGHT
OF
AS
THE
REDUCTION
IN
ENTROPY
ON
THE
LABEL
DISTRIBUTION
ONCE
WE
OBSERVE
THE
VALUE
OF
FEATURE
J
IF
THE
FEATURES
ARE
BINARY
IT
IS
EASY
TO
SHOW
EXERCISE
THAT
THE
MI
CAN
BE
COMPUTED
AS
FOLLOWS
IJ
ΘJCΠC
LOG
ΘJC
Θ
ΘJ
JC
ΠC
LOG
ΘJC
ΘJ
WHERE
ΠC
P
Y
C
ΘJC
P
XJ
Y
C
AND
ΘJ
P
XJ
C
ΠCΘJC
ALL
OF
THESE
QUANTITIES
CAN
BE
COMPUTED
AS
A
BY
PRODUCT
OF
FITTING
A
NAIVE
BAYES
CLASSIFIER
FIGURE
ILLUSTRATES
WHAT
HAPPENS
IF
WE
APPLY
THIS
TO
THE
BINARY
BAG
OF
WORDS
DATASET
USED
IN
FIGURE
WE
SEE
THAT
THE
WORDS
WITH
HIGHEST
MUTUAL
INFORMATION
ARE
MUCH
MORE
DISCRIMINATIVE
THAN
THE
WORDS
WHICH
ARE
MOST
PROBABLE
FOR
EXAMPLE
THE
MOST
PROBABLE
WORD
IN
BOTH
CLASSES
IS
SUBJECT
WHICH
ALWAYS
OCCURS
BECAUSE
THIS
IS
NEWSGROUP
DATA
WHICH
ALWAYS
HAS
A
SUBJECT
LINE
BUT
OBVIOUSLY
THIS
IS
NOT
VERY
DISCRIMINATIVE
THE
WORDS
WITH
HIGHEST
MI
WITH
THE
CLASS
LABEL
ARE
IN
DECREASING
ORDER
WINDOWS
MICROSOFT
DOS
AND
MOTIF
WHICH
MAKES
SENSE
SINCE
THE
CLASSES
CORRESPOND
TO
MICROSOFT
WINDOWS
AND
X
WINDOWS
CLASSIFYING
DOCUMENTS
USING
BAG
OF
WORDS
DOCUMENT
CLASSIFICATION
IS
THE
PROBLEM
OF
CLASSIFYING
TEXT
DOCUMENTS
INTO
DIFFERENT
CATEGORIES
ONE
SIMPLE
APPROACH
IS
TO
REPRESENT
EACH
DOCUMENT
AS
A
BINARY
VECTOR
WHICH
RECORDS
WHETHER
EACH
WORD
IS
PRESENT
OR
NOT
SO
XIJ
IFF
WORD
J
OCCURS
IN
DOCUMENT
I
OTHERWISE
XIJ
WE
CAN
THEN
USE
THE
FOLLOWING
CLASS
CONDITIONAL
DENSITY
D
D
P
XI
YI
C
Θ
TT
BER
XIJ
ΘJC
TT
ΘI
XIJ
ΘJC
I
XIJ
J
J
CLASS
PROB
CLASS
PROB
HIGHEST
MI
MI
SUBJECT
SUBJECT
WINDOWS
THIS
WINDOWS
MICROSOFT
WITH
THIS
DOS
BUT
WITH
MOTIF
YOU
BUT
WINDOW
TABLE
WE
LIST
THE
MOST
LIKELY
WORDS
FOR
CLASS
X
WINDOWS
AND
CLASS
MS
WINDOWS
WE
ALSO
SHOW
THE
WORDS
WITH
HIGHEST
MUTUAL
INFORMATION
WITH
CLASS
LABEL
PRODUCED
BY
NAIVEBAYESBOWDEMO
THIS
IS
CALLED
THE
BERNOULLI
PRODUCT
MODEL
OR
THE
BINARY
INDEPENDENCE
MODEL
HOWEVER
IGNORING
THE
NUMBER
OF
TIMES
EACH
WORD
OCCURS
IN
A
DOCUMENT
LOSES
SOME
IN
FORMATION
MCCALLUM
AND
NIGAM
A
MORE
ACCURATE
REPRESENTATION
COUNTS
THE
NUMBER
OF
OCCURRENCES
OF
EACH
WORD
SPECIFICALLY
LET
XI
BE
A
VECTOR
OF
COUNTS
FOR
DOCUMENT
I
SO
XIJ
NI
WHERE
NI
IS
THE
NUMBER
OF
TERMS
IN
DOCUMENT
I
SO
D
XIJ
NI
FOR
THE
CLASS
CONDITIONAL
DENSITIES
WE
CAN
USE
A
MULTINOMIAL
DISTRIBUTION
P
X
Y
C
Θ
MU
X
N
Θ
NI
TT
ΘXIJ
WHERE
WE
HAVE
IMPLICITLY
ASSUMED
THAT
THE
DOCUMENT
LENGTH
NI
IS
INDEPENDENT
OF
THE
CLASS
HERE
ΘJC
IS
THE
PROBABILITY
OF
GENERATING
WORD
J
IN
DOCUMENTS
OF
CLASS
C
THESE
PARAMETERS
SATISFY
THE
CONSTRAINT
THAT
D
ΘJC
FOR
EACH
CLASS
C
ALTHOUGH
THE
MULTINOMIAL
CLASSIFIER
IS
EASY
TO
TRAIN
AND
EASY
TO
USE
AT
TEST
TIME
IT
DOES
NOT
WORK
PARTICULARLY
WELL
FOR
DOCUMENT
CLASSIFICATION
ONE
REASON
FOR
THIS
IS
THAT
IT
DOES
NOT
TAKE
INTO
ACCOUNT
THE
BURSTINESS
OF
WORD
USAGE
THIS
REFERS
TO
THE
PHENOMENON
THAT
MOST
WORDS
NEVER
APPEAR
IN
ANY
GIVEN
DOCUMENT
BUT
IF
THEY
DO
APPEAR
ONCE
THEY
ARE
LIKELY
TO
APPEAR
MORE
THAN
ONCE
I
E
WORDS
OCCUR
IN
BURSTS
THE
MULTINOMIAL
MODEL
CANNOT
CAPTURE
THE
BURSTINESS
PHENOMENON
TO
SEE
WHY
NOTE
THAT
EQUATION
HAS
THE
FORM
ΘNIJ
AND
SINCE
ΘJC
FOR
RARE
WORDS
IT
BECOMES
INCREASINGLY
UNLIKELY
TO
GENERATE
MANY
OF
THEM
FOR
MORE
FREQUENT
WORDS
THE
DECAY
RATE
IS
NOT
AS
FAST
TO
SEE
WHY
INTUITIVELY
NOTE
THAT
THE
MOST
FREQUENT
WORDS
ARE
FUNCTION
WORDS
WHICH
ARE
NOT
SPECIFIC
TO
THE
CLASS
SUCH
AS
AND
THE
AND
BUT
THE
CHANCE
OF
THE
WORD
AND
OCCURING
IS
PRETTY
MUCH
THE
SAME
NO
MATTER
HOW
MANY
TIME
IT
HAS
PREVIOUSLY
OCCURRED
MODULO
DOCUMENT
LENGTH
SO
THE
INDEPENDENCE
ASSUMPTION
IS
MORE
REASONABLE
FOR
COMMON
WORDS
HOWEVER
SINCE
RARE
WORDS
ARE
THE
ONES
THAT
MATTER
MOST
FOR
CLASSIFICATION
PURPOSES
THESE
ARE
THE
ONES
WE
WANT
TO
MODEL
THE
MOST
CAREFULLY
VARIOUS
AD
HOC
HEURISTICS
HAVE
BEEN
PROPOSED
TO
IMPROVE
THE
PERFORMANCE
OF
THE
MULTINOMIAL
DOCUMENT
CLASSIFIER
RENNIE
ET
AL
WE
NOW
PRESENT
AN
ALTERNATIVE
CLASS
CONDITIONAL
DENSITY
THAT
PERFORMS
AS
WELL
AS
THESE
AD
HOC
METHODS
YET
IS
PROBABILISTICALLY
SOUND
MADSEN
ET
AL
SINCE
EQUATION
MODELS
EACH
WORD
INDEPENDENTLY
THIS
MODEL
IS
OFTEN
CALLED
A
NAIVE
BAYES
CLASSIFIER
ALTHOUGH
TECHNICALLY
THE
FEATURES
XIJ
ARE
NOT
INDEPENDENT
BECAUSE
OF
THE
CONSTRAINT
J
XIJ
NI
SUPPOSE
WE
SIMPLY
REPLACE
THE
MULTINOMIAL
CLASS
CONDITIONAL
DENSITY
WITH
THE
DIRICHLET
COMPOUND
MULTINOMIAL
OR
DCM
DENSITY
DEFINED
AS
FOLLOWS
P
X
Y
C
Α
R
MU
X
N
Θ
DIR
Θ
Α
DΘ
NI
B
XI
ΑC
THIS
EQUATION
IS
DERIVED
IN
EQUATION
SURPRISINGLY
THIS
SIMPLE
CHANGE
IS
ALL
THAT
IS
NEEDED
TO
CAPTURE
THE
BURSTINESS
PHENOMENON
THE
INTUITIVE
REASON
FOR
THIS
IS
AS
FOLLOWS
AFTER
SEEING
ONE
OCCURENCE
OF
A
WORD
SAY
WORD
J
THE
POSTERIOR
COUNTS
ON
ΘJ
GETS
UPDATED
MAKING
ANOTHER
OCCURENCE
OF
WORD
J
MORE
LIKELY
BY
CONTRAST
IF
ΘJ
IS
FIXED
THEN
THE
OCCURENCES
OF
EACH
WORD
ARE
INDEPENDENT
THE
MULTINOMIAL
MODEL
CORRESPONDS
TO
DRAWING
A
BALL
FROM
AN
URN
WITH
K
COLORS
OF
BALL
RECORDING
ITS
COLOR
AND
THEN
REPLACING
IT
BY
CONTRAST
THE
DCM
MODEL
CORRESPONDS
TO
DRAWING
A
BALL
RECORDING
ITS
COLOR
AND
THEN
REPLACING
IT
WITH
ONE
ADDITIONAL
COPY
THIS
IS
CALLED
THE
POLYA
URN
USING
THE
DCM
AS
THE
CLASS
CONDITIONAL
DENSITY
GIVES
MUCH
BETTER
RESULTS
THAN
USING
THE
MULTINOMIAL
AND
HAS
PERFORMANCE
COMPARABLE
TO
STATE
OF
THE
ART
METHODS
AS
DESCRIBED
IN
MADSEN
ET
AL
THE
ONLY
DISADVANTAGE
IS
THAT
FITTING
THE
DCM
MODEL
IS
MORE
COMPLEX
SEE
MINKA
ELKAN
FOR
THE
DETAILS
EXERCISES
EXERCISE
MLE
FOR
THE
BERNOULLI
BINOMIAL
MODEL
DERIVE
EQUATION
BY
OPTIMIZING
THE
LOG
OF
THE
LIKELIHOOD
IN
EQUATION
EXERCISE
MARGINAL
LIKELIHOOD
FOR
THE
BETA
BERNOULLI
MODEL
IN
EQUATION
WE
SHOWED
THAT
THE
MARGINAL
LIKELIHOOD
IS
THE
RATIO
OF
THE
NORMALIZING
CONSTANTS
P
D
Z
Γ
Γ
Γ
Z
Γ
N
Γ
Γ
WE
WILL
NOW
DERIVE
AN
ALTERNATIVE
DERIVATION
OF
THIS
FACT
BY
THE
CHAIN
RULE
OF
PROBABILITY
P
N
P
P
P
IN
SECTION
WE
SHOWED
THAT
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
IS
NK
ΑK
NK
ΑK
P
X
K
N
N
ΑI
N
Α
WHERE
K
AND
N
IS
THE
DATA
SEEN
SO
FAR
NOW
SUPPOSE
D
H
T
T
H
H
OR
D
THEN
P
D
Α
Α
Α
Α
Α
N
SHOW
HOW
THIS
REDUCES
TO
EQUATION
BY
USING
THE
FACT
THAT
FOR
INTEGERS
Α
Γ
Α
EXERCISE
POSTERIOR
PREDICTIVE
FOR
BETA
BINOMIAL
MODEL
RECALL
FROM
EQUATION
THAT
THE
POSTERIOR
PREDICTIVE
FOR
THE
BETA
BINOMIAL
IS
GIVEN
BY
P
X
N
D
BB
X
N
B
X
N
X
N
PROVE
THAT
THIS
REDUCES
TO
P
X
D
B
X
WHEN
N
AND
HENCE
X
I
E
SHOW
THAT
BB
ΑI
ΑI
ΑI
ΑI
HINT
USE
THE
FACT
THAT
Γ
Γ
EXERCISE
BETA
UPDATING
FROM
CENSORED
LIKELIHOOD
SOURCE
GELMAN
SUPPOSE
WE
TOSS
A
COIN
N
TIMES
LET
X
BE
THE
NUMBER
OF
HEADS
WE
OBSERVE
THAT
THERE
ARE
FEWER
THAN
HEADS
BUT
WE
DON
T
KNOW
EXACTLY
HOW
MANY
LET
THE
PRIOR
PROBABILITY
OF
HEADS
BE
P
Θ
BETA
Θ
COMPUTE
THE
POSTERIOR
P
Θ
X
UP
TO
NORMALIZATION
CONSTANTS
I
E
DERIVE
AN
EXPRESSION
PROPORTIONAL
TO
P
Θ
X
HINT
THE
ANSWER
IS
A
MIXTURE
DISTRIBUTION
EXERCISE
UNINFORMATIVE
PRIOR
FOR
LOG
ODDS
RATIO
LET
Φ
LOGIT
Θ
LOG
Θ
Θ
SHOW
THAT
IF
P
Φ
THEN
P
Θ
BETA
Θ
HINT
USE
THE
CHANGE
OF
VARIABLES
FORMULA
EXERCISE
MLE
FOR
THE
POISSON
DISTRIBUTION
THE
POISSON
PMF
IS
DEFINED
AS
POI
X
Λ
E
Λ
ΛX
FOR
X
WHERE
Λ
IS
THE
RATE
PARAMETER
DERIVE
THE
MLE
EXERCISE
BAYESIAN
ANALYSIS
OF
THE
POISSON
DISTRIBUTION
IN
EXERCISE
WE
DEFINED
THE
POISSON
DISTRIBUTION
WITH
RATE
Λ
AND
DERIVED
ITS
MLE
HERE
WE
PERFORM
A
CONJUGATE
BAYESIAN
ANALYSIS
A
DERIVE
THE
POSTERIOR
P
Λ
D
ASSUMING
A
CONJUGATE
PRIOR
P
Λ
GA
Λ
A
B
ΛA
ΛB
HINT
THE
POSTERIOR
IS
ALSO
A
GAMMA
DISTRIBUTION
B
WHAT
DOES
THE
POSTERIOR
MEAN
TEND
TO
AS
A
AND
B
RECALL
THAT
THE
MEAN
OF
A
GA
A
B
DISTRIBUTION
IS
A
B
EXERCISE
MLE
FOR
THE
UNIFORM
DISTRIBUTION
SOURCE
KAELBLING
CONSIDER
A
UNIFORM
DISTRIBUTION
CENTERED
ON
WITH
WIDTH
THE
DENSITY
FUNCTION
IS
GIVEN
BY
P
X
I
X
A
A
A
GIVEN
A
DATA
SET
XN
WHAT
IS
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
OF
A
CALL
IT
Aˆ
B
WHAT
PROBABILITY
WOULD
THE
MODEL
ASSIGN
TO
A
NEW
DATA
POINT
XN
USING
Aˆ
C
DO
YOU
SEE
ANY
PROBLEM
WITH
THE
ABOVE
APPROACH
BRIEFLY
SUGGEST
IN
WORDS
A
BETTER
APPROACH
EXERCISE
BAYESIAN
ANALYSIS
OF
THE
UNIFORM
DISTRIBUTION
CONSIDER
THE
UNIFORM
DISTRIBUTION
UNIF
Θ
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
IS
Θˆ
MAX
AS
WE
SAW
IN
EXERCISE
BUT
THIS
IS
UNSUITABLE
FOR
PREDICTING
FUTURE
DATA
SINCE
IT
PUTS
ZERO
PROBABILITY
MASS
OUTSIDE
THE
TRAINING
DATA
IN
THIS
EXERCISE
WE
WILL
PERFORM
A
BAYESIAN
ANALYSIS
OF
THE
UNIFORM
DISTRIBUTION
FOLLOWING
MINKA
THE
CONJUGATE
PRIOR
IS
THE
PARETO
DISTRIBUTION
P
Θ
PARETO
Θ
B
K
DEFINED
IN
SECTION
GIVEN
A
PARETO
PRIOR
THE
JOINT
DISTRIBUTION
OF
Θ
AND
D
XN
IS
KBK
P
D
Θ
ΘN
K
I
Θ
MAX
D
LET
M
MAX
THE
EVIDENCE
THE
PROBABILITY
THAT
ALL
N
SAMPLES
CAME
FROM
THE
SAME
UNIFORM
DISTRIBUTION
IS
P
D
KBK
DΘ
M
ΘN
K
K
N
K
BN
KBK
N
K
MN
K
IF
M
B
IF
M
B
DERIVE
THE
POSTERIOR
P
Θ
D
AND
SHOW
THAT
IF
CAN
BE
EXPRESSED
AS
A
PARETO
DISTRIBUTION
EXERCISE
TAXICAB
TRAMCAR
PROBLEM
SUPPOSE
YOU
ARRIVE
IN
A
NEW
CITY
AND
SEE
A
TAXI
NUMBERED
HOW
MANY
TAXIS
ARE
THERE
IN
THIS
CITY
LET
US
ASSUME
TAXIS
ARE
NUMBERED
SEQUENTIALLY
AS
INTEGERS
STARTING
FROM
UP
TO
SOME
UNKNOWN
UPPER
BOUND
Θ
WE
NUMBER
TAXIS
FROM
FOR
SIMPLICITY
WE
CAN
ALSO
COUNT
FROM
WITHOUT
CHANGING
THE
ANALYSIS
HENCE
THE
LIKELIHOOD
FUNCTION
IS
P
X
U
Θ
THE
UNIFORM
DISTRIBUTION
THE
GOAL
IS
TO
ESTIMATE
Θ
WE
WILL
USE
THE
BAYESIAN
ANALYSIS
FROM
EXERCISE
A
SUPPOSE
WE
SEE
ONE
TAXI
NUMBERED
SO
D
M
N
USING
AN
IMPROPER
NON
INFORMATIVE
PRIOR
ON
Θ
OF
THE
FORM
P
Θ
P
A
Θ
Θ
WHAT
IS
THE
POSTERIOR
P
Θ
D
B
COMPUTE
THE
POSTERIOR
MEAN
MODE
AND
MEDIAN
NUMBER
OF
TAXIS
IN
THE
CITY
IF
SUCH
QUANTITIES
EXIST
C
RATHER
THAN
TRYING
TO
COMPUTE
A
POINT
ESTIMATE
OF
THE
NUMBER
OF
TAXIS
WE
CAN
COMPUTE
THE
PREDICTIVE
DENSITY
OVER
THE
NEXT
TAXICAB
NUMBER
USING
P
DI
D
Α
P
DI
Θ
P
Θ
D
Α
DΘ
P
DI
Β
WHERE
Α
B
K
ARE
THE
HYPER
PARAMETERS
Β
C
N
K
ARE
THE
UPDATED
HYPER
PARAMETERS
NOW
CONSIDER
THE
CASE
D
M
AND
DI
X
USING
EQUATION
WRITE
DOWN
AN
EXPRESSION
FOR
P
X
D
Α
AS
ABOVE
USE
A
NON
INFORMATIVE
PRIOR
B
K
D
USE
THE
PREDICTIVE
DENSITY
FORMULA
TO
COMPUTE
THE
PROBABILITY
THAT
THE
NEXT
TAXI
YOU
WILL
SEE
SAY
THE
NEXT
DAY
HAS
NUMBER
OR
I
E
COMPUTE
P
X
D
Α
P
X
D
Α
P
X
D
Α
E
BRIEFLY
DESCRIBE
SENTENCES
SOME
WAYS
WE
MIGHT
MAKE
THE
MODEL
MORE
ACCURATE
AT
PREDICTION
EXERCISE
BAYESIAN
ANALYSIS
OF
THE
EXPONENTIAL
DISTRIBUTION
A
LIFETIME
X
OF
A
MACHINE
IS
MODELED
BY
AN
EXPONENTIAL
DISTRIBUTION
WITH
UNKNOWN
PARAMETER
Θ
THE
LIKELIHOOD
IS
P
X
Θ
ΘE
ΘX
FOR
X
Θ
A
SHOW
THAT
THE
MLE
IS
Θˆ
X
WHERE
X
N
XI
B
SUPPOSE
WE
OBSERVE
THE
LIFETIMES
IN
YEARS
OF
DIFFERENT
IID
MACHINES
WHAT
IS
THE
MLE
GIVEN
THIS
DATA
C
ASSUME
THAT
AN
EXPERT
BELIEVES
Θ
SHOULD
HAVE
A
PRIOR
DISTRIBUTION
THAT
IS
ALSO
EXPONENTIAL
P
Θ
EXPON
Θ
Λ
CHOOSE
THE
PRIOR
PARAMETER
CALL
IT
Λˆ
SUCH
THAT
E
Θ
HINT
RECALL
THAT
THE
GAMMA
DISTRIBUTION
HAS
THE
FORM
GA
Θ
A
B
ΘA
ΘB
AND
ITS
MEAN
IS
A
B
D
WHAT
IS
THE
POSTERIOR
P
Θ
D
Λˆ
E
IS
THE
EXPONENTIAL
PRIOR
CONJUGATE
TO
THE
EXPONENTIAL
LIKELIHOOD
F
WHAT
IS
THE
POSTERIOR
MEAN
E
Θ
D
Λˆ
G
EXPLAIN
WHY
THE
MLE
AND
POSTERIOR
MEAN
DIFFER
WHICH
IS
MORE
REASONABLE
IN
THIS
EXAMPLE
EXERCISE
MAP
ESTIMATION
FOR
THE
BERNOULLI
WITH
NON
CONJUGATE
PRIORS
SOURCE
JAAKKOLA
IN
THE
BOOK
WE
DISCUSSED
BAYESIAN
INFERENCE
OF
A
BERNOULLI
RATE
PARAMETER
WITH
THE
PRIOR
P
Θ
BETA
Θ
Α
Β
WE
KNOW
THAT
WITH
THIS
PRIOR
THE
MAP
ESTIMATE
IS
GIVEN
BY
Θˆ
Α
N
Α
Β
WHERE
IS
THE
NUMBER
OF
HEADS
IS
THE
NUMBER
OF
TAILS
AND
N
IS
THE
TOTAL
NUMBER
OF
TRIALS
A
NOW
CONSIDER
THE
FOLLOWING
PRIOR
THAT
BELIEVES
THE
COIN
IS
FAIR
OR
IS
SLIGHTLY
BIASED
TOWARDS
TAILS
P
Θ
IF
Θ
OTHERWISE
DERIVE
THE
MAP
ESTIMATE
UNDER
THIS
PRIOR
AS
A
FUNCTION
OF
AND
N
B
SUPPOSE
THE
TRUE
PARAMETER
IS
Θ
WHICH
PRIOR
LEADS
TO
A
BETTER
ESTIMATE
WHEN
N
IS
SMALL
WHICH
PRIOR
LEADS
TO
A
BETTER
ESTIMATE
WHEN
N
IS
LARGE
EXERCISE
POSTERIOR
PREDICTIVE
DISTRIBUTION
FOR
A
BATCH
OF
DATA
WITH
THE
DIRICHLET
MULTINOMIAL
MODEL
IN
EQUATION
WE
GAVE
THE
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
FOR
A
SINGLE
MULTINOMIAL
TRIAL
USING
A
DIRICHLET
PRIOR
NOW
CONSIDER
PREDICTING
A
BATCH
OF
NEW
DATA
XM
CONSISTING
OF
M
SINGLE
MULTINOMIAL
TRIALS
THINK
OF
PREDICTING
THE
NEXT
M
WORDS
IN
A
SENTENCE
ASSUMING
THEY
ARE
DRAWN
IID
DERIVE
AN
EXPRESSION
FOR
P
D
D
Α
YOUR
ANSWER
SHOULD
BE
A
FUNCTION
OF
Α
AND
THE
OLD
AND
NEW
COUNTS
SUFFICIENT
STATISTICS
DEFINED
AS
OLD
K
NEW
K
I
XI
K
I
D
I
XI
K
I
D
HINT
RECALL
THAT
FOR
A
VECTOR
OF
COUNTS
K
THE
MARGINAL
LIKELIHOOD
EVIDENCE
IS
GIVEN
BY
P
D
Α
Γ
Α
Γ
NK
ΑK
WHERE
Α
K
ΑK
AND
N
K
NK
EXERCISE
POSTERIOR
PREDICTIVE
FOR
DIRICHLET
MULTINOMIAL
SOURCE
KOLLER
A
SUPPOSE
WE
COMPUTE
THE
EMPIRICAL
DISTRIBUTION
OVER
LETTERS
OF
THE
ROMAN
ALPHABET
PLUS
THE
SPACE
CHARACTER
A
DISTRIBUTION
OVER
VALUES
FROM
SAMPLES
SUPPOSE
WE
SEE
THE
LETTER
E
TIMES
WHAT
IS
P
E
D
IF
WE
ASSUME
Θ
DIR
WHERE
ΑK
FOR
ALL
K
B
SUPPOSE
IN
THE
SAMPLES
WE
SAW
E
TIMES
A
TIMES
AND
P
TIMES
WHAT
IS
P
P
A
IF
WE
ASSUME
Θ
DIR
WHERE
ΑK
FOR
ALL
K
SHOW
YOUR
WORK
EXERCISE
SETTING
THE
BETA
HYPER
PARAMETERS
SUPPOSE
Θ
Β
AND
WE
BELIEVE
THAT
E
Θ
M
AND
VAR
Θ
V
USING
EQUATION
SOLVE
FOR
AND
IN
TERMS
OF
M
AND
V
WHAT
VALUES
DO
YOU
GET
IF
M
AND
V
EXERCISE
SETTING
THE
BETA
HYPER
PARAMETERS
II
SOURCE
DRAPER
SUPPOSE
Θ
Β
AND
WE
BELIEVE
THAT
E
Θ
M
AND
P
T
Θ
U
WRITE
A
PROGRAM
THAT
CAN
SOLVE
FOR
AND
IN
TERMS
OF
M
T
AND
U
HINT
WRITE
AS
A
FUNCTION
OF
AND
M
SO
THE
PDF
ONLY
HAS
ONE
UNKNOWN
THEN
WRITE
DOWN
THE
PROBABILITY
MASS
CONTAINED
IN
THE
INTERVAL
AS
AN
INTEGRAL
AND
MINIMIZE
ITS
SQUARED
DISCREPANCY
FROM
WHAT
VALUES
DO
YOU
GET
IF
M
T
AND
U
WHAT
IS
THE
EQUIVALENT
SAMPLE
SIZE
OF
THIS
PRIOR
EXERCISE
MARGINAL
LIKELIHOOD
FOR
BETA
BINOMIAL
UNDER
UNIFORM
PRIOR
SUPPOSE
WE
TOSS
A
COIN
N
TIMES
AND
OBSERVE
HEADS
LET
BIN
N
Θ
AND
Θ
BETA
SHOW
THAT
THE
MARGINAL
LIKELIHOOD
IS
P
N
N
HINT
Γ
X
X
IF
X
IS
AN
INTEGER
EXERCISE
BAYES
FACTOR
FOR
COIN
TOSSING
SUPPOSE
WE
TOSS
A
COIN
N
TIMES
AND
OBSERVE
HEADS
LET
THE
NULL
HYPOTHESIS
BE
THAT
THE
COIN
IS
FAIR
AND
THE
ALTERNATIVE
BE
THAT
THE
COIN
CAN
HAVE
ANY
BIAS
SO
P
Θ
UNIF
DERIVE
THE
BAYES
FACTOR
IN
FAVOR
OF
THE
BIASED
COIN
HYPOTHESIS
WHAT
IF
N
AND
HINT
SEE
EXERCISE
EXERCISE
IRRELEVANT
FEATURES
WITH
NAIVE
BAYES
SOURCE
JAAKKOLA
LET
XIW
IF
WORD
W
OCCURS
IN
DOCUMENT
I
AND
XIW
OTHERWISE
LET
ΘCW
BE
THE
ESTIMATED
PROBABILITY
THAT
WORD
W
OCCURS
IN
DOCUMENTS
OF
CLASS
C
THEN
THE
LOG
LIKELIHOOD
THAT
DOCUMENT
X
BELONGS
TO
CLASS
C
IS
LOG
P
XI
C
Θ
LOG
ΘXIW
ΘCW
XIW
W
W
XIW
LOG
ΘCW
XIW
LOG
ΘCW
W
W
XIW
LOG
ΘCW
LOG
Θ
Θ
WHERE
W
IS
THE
NUMBER
OF
WORDS
IN
THE
VOCABULARY
WE
CAN
WRITE
THIS
MORE
SUCCINTLY
AS
LOG
P
XI
C
Θ
Φ
XI
T
Β
WHERE
XI
XIW
IS
A
BIT
VECTOR
Φ
XI
XI
AND
Β
LOG
LOG
ΘCW
LOG
Θ
T
WE
SEE
THAT
THIS
IS
A
LINEAR
CLASSIFIER
SINCE
THE
CLASS
CONDITIONAL
DENSITY
IS
A
LINEAR
FUNCTION
AN
INNER
PRODUCT
OF
THE
PARAMETERS
ΒC
A
ASSUMING
P
C
P
C
WRITE
DOWN
AN
EXPRESSION
FOR
THE
LOG
POSTERIOR
ODDS
RATIO
LOG
P
C
XI
IN
TERMS
OF
THE
FEATURES
Φ
XI
AND
THE
PARAMETERS
Β
P
C
XI
AND
B
INTUITIVELY
WORDS
THAT
OCCUR
IN
BOTH
CLASSES
ARE
NOT
VERY
DISCRIMINATIVE
AND
THEREFORE
SHOULD
NOT
AFFECT
OUR
BELIEFS
ABOUT
THE
CLASS
LABEL
CONSIDER
A
PARTICULAR
WORD
W
STATE
THE
CONDITIONS
ON
W
AND
W
OR
EQUIVALENTLY
THE
CONDITIONS
ON
W
W
UNDER
WHICH
THE
PRESENCE
OR
ABSENCE
OF
W
IN
A
TEST
DOCUMENT
WILL
HAVE
NO
EFFECT
ON
THE
CLASS
POSTERIOR
SUCH
A
WORD
WILL
BE
IGNORED
BY
THE
CLASSIFIER
HINT
USING
YOUR
PREVIOUS
RESULT
FIGURE
OUT
WHEN
THE
POSTERIOR
ODDS
RATIO
IS
C
THE
POSTERIOR
MEAN
ESTIMATE
OF
Θ
USING
A
BETA
PRIOR
IS
GIVEN
BY
ΘˆCW
I
C
XIW
N
C
WHERE
THE
SUM
IS
OVER
THE
NC
DOCUMENTS
IN
CLASS
C
CONSIDER
A
PARTICULAR
WORD
W
AND
SUPPOSE
IT
ALWAYS
OCCURS
IN
EVERY
DOCUMENT
REGARDLESS
OF
CLASS
LET
THERE
BE
DOCUMENTS
OF
CLASS
AND
BE
THE
NUMBER
OF
DOCUMENTS
IN
CLASS
WHERE
SINCE
E
G
WE
GET
MUCH
MORE
NON
SPAM
THAN
SPAM
THIS
IS
AN
EXAMPLE
OF
CLASS
IMBALANCE
IF
WE
USE
THE
ABOVE
ESTIMATE
FOR
ΘCW
WILL
WORD
W
BE
IGNORED
BY
OUR
CLASSIFIER
EXPLAIN
WHY
OR
WHY
NOT
D
WHAT
OTHER
WAYS
CAN
YOU
THINK
OF
WHICH
ENCOURAGE
IRRELEVANT
WORDS
TO
BE
IGNORED
EXERCISE
CLASS
CONDITIONAL
DENSITIES
FOR
BINARY
DATA
CONSIDER
A
GENERATIVE
CLASSIFIER
FOR
C
CLASSES
WITH
CLASS
CONDITIONAL
DENSITY
P
X
Y
AND
UNIFORM
CLASS
PRIOR
P
Y
SUPPOSE
ALL
THE
D
FEATURES
ARE
BINARY
XJ
IF
WE
ASSUME
ALL
THE
FEATURES
ARE
CONDITIONALLY
INDEPENDENT
THE
NAIVE
BAYES
ASSUMPTION
WE
CAN
WRITE
D
P
X
Y
C
BER
XJ
ΘJC
J
THIS
REQUIRES
DC
PARAMETERS
A
NOW
CONSIDER
A
DIFFERENT
MODEL
WHICH
WE
WILL
CALL
THE
FULL
MODEL
IN
WHICH
ALL
THE
FEATURES
ARE
FULLY
DEPENDENT
I
E
WE
MAKE
NO
FACTORIZATION
ASSUMPTIONS
HOW
MIGHT
WE
REPRESENT
P
X
Y
C
IN
THIS
CASE
HOW
MANY
PARAMETERS
ARE
NEEDED
TO
REPRESENT
P
X
Y
C
B
ASSUME
THE
NUMBER
OF
FEATURES
D
IS
FIXED
LET
THERE
BE
N
TRAINING
CASES
IF
THE
SAMPLE
SIZE
N
IS
VERY
SMALL
WHICH
MODEL
NAIVE
BAYES
OR
FULL
IS
LIKELY
TO
GIVE
LOWER
TEST
SET
ERROR
AND
WHY
C
IF
THE
SAMPLE
SIZE
N
IS
VERY
LARGE
WHICH
MODEL
NAIVE
BAYES
OR
FULL
IS
LIKELY
TO
GIVE
LOWER
TEST
SET
ERROR
AND
WHY
D
WHAT
IS
THE
COMPUTATIONAL
COMPLEXITY
OF
FITTING
THE
FULL
AND
NAIVE
BAYES
MODELS
AS
A
FUNCTION
OF
N
AND
D
USE
BIG
OH
NOTATION
FITTING
THE
MODEL
HERE
MEANS
COMPUTING
THE
MLE
OR
MAP
PARAMETER
ESTIMATES
YOU
MAY
ASSUME
YOU
CAN
CONVERT
A
D
BIT
VECTOR
TO
AN
ARRAY
INDEX
IN
O
D
TIME
E
WHAT
IS
THE
COMPUTATIONAL
COMPLEXITY
OF
APPLYING
THE
FULL
AND
NAIVE
BAYES
MODELS
AT
TEST
TIME
TO
A
SINGLE
TEST
CASE
F
SUPPOSE
THE
TEST
CASE
HAS
MISSING
DATA
LET
XV
BE
THE
VISIBLE
FEATURES
OF
SIZE
V
AND
XH
BE
THE
HIDDEN
MISSING
FEATURES
OF
SIZE
H
WHERE
V
H
D
WHAT
IS
THE
COMPUTATIONAL
COMPLEXITY
OF
COMPUTING
P
Y
XV
Θˆ
FOR
THE
FULL
AND
NAIVE
BAYES
MODELS
AS
A
FUNCTION
OF
V
AND
H
EXERCISE
MUTUAL
INFORMATION
FOR
NAIVE
BAYES
CLASSIFIERS
WITH
BINARY
FEATURES
DERIVE
EQUATION
EXERCISE
FITTING
A
NAIVE
BAYES
SPAM
FILTER
BY
HAND
SOURCE
DAPHNE
KOLLER
CONSIDER
A
NAIVE
BAYES
MODEL
MULTIVARIATE
BERNOULLI
VERSION
FOR
SPAM
CLASSIFICA
TION
WITH
THE
VOCABULARY
V
SECRET
OFFER
LOW
PRICE
VALUED
CUSTOMER
TODAY
DOLLAR
MILLION
SPORTS
IS
FOR
PLAY
HEALTHY
PIZZA
WE
HAVE
THE
FOLLOWING
EXAMPLE
SPAM
MESSAGES
MILLION
DOLLAR
OFFER
SECRET
OFFER
TODAY
SECRET
IS
SECRET
AND
NORMAL
MESSAGES
LOW
PRICE
FOR
VALUED
CUSTOMER
PLAY
SECRET
SPORTS
TODAY
SPORTS
IS
HEALTHY
LOW
PRICE
PIZZA
GIVE
THE
MLES
FOR
THE
FOLLOWING
PARAMETERS
ΘSPAM
ΘSECRET
SPAM
ΘSECRET
NON
SPAM
ΘSPORTS
NON
SPAM
ΘDOLLAR
SPAM
GAUSSIAN
MODELS
INTRODUCTION
IN
THIS
CHAPTER
WE
DISCUSS
THE
MULTIVARIATE
GAUSSIAN
OR
MULTIVARIATE
NORMAL
MVN
WHICH
IS
THE
MOST
WIDELY
USED
JOINT
PROBABILITY
DENSITY
FUNCTION
FOR
CONTINUOUS
VARIABLES
IT
WILL
FORM
THE
BASIS
FOR
MANY
OF
THE
MODELS
WE
WILL
ENCOUNTER
IN
LATER
CHAPTERS
UNFORTUNATELY
THE
LEVEL
OF
MATHEMATICS
IN
THIS
CHAPTER
IS
HIGHER
THAN
IN
MANY
OTHER
CHAPTERS
IN
PARTICULAR
WE
RELY
HEAVILY
ON
LINEAR
ALGEBRA
AND
MATRIX
CALCULUS
THIS
IS
THE
PRICE
ONE
MUST
PAY
IN
ORDER
TO
DEAL
WITH
HIGH
DIMENSIONAL
DATA
BEGINNERS
MAY
CHOOSE
TO
SKIP
SECTIONS
MARKED
WITH
A
IN
ADDITION
SINCE
THERE
ARE
SO
MANY
EQUATIONS
IN
THIS
CHAPTER
WE
HAVE
PUT
A
BOX
AROUND
THOSE
THAT
ARE
PARTICULARLY
IMPORTANT
NOTATION
LET
US
BRIEFLY
SAY
A
FEW
WORDS
ABOUT
NOTATION
WE
DENOTE
VECTORS
BY
BOLDFACE
LOWER
CASE
LETTERS
SUCH
AS
X
WE
DENOTE
MATRICES
BY
BOLDFACE
UPPER
CASE
LETTERS
SUCH
AS
X
WE
DENOTE
ENTRIES
IN
A
MATRIX
BY
NON
BOLD
UPPER
CASE
LETTERS
SUCH
AS
XIJ
ALL
VECTORS
ARE
ASSUMED
TO
BE
COLUMN
VECTORS
UNLESS
NOTED
OTHERWISE
WE
USE
XD
TO
DENOTE
A
COLUMN
VECTOR
CREATED
BY
STACKING
D
SCALARS
SIMILARLY
IF
WE
WRITE
X
XD
WHERE
THE
LEFT
HAND
SIDE
IS
A
TALL
COLUMN
VECTOR
WE
MEAN
TO
STACK
THE
XI
ALONG
THE
ROWS
THIS
IS
USUALLY
WRITTEN
AS
X
XT
XT
T
BUT
THAT
IS
RATHER
UGLY
IF
WE
WRITE
X
XD
D
WHERE
THE
LEFT
HAND
SIDE
IS
A
MATRIX
WE
MEAN
TO
STACK
THE
XI
ALONG
THE
COLUMNS
CREATING
A
MATRIX
BASICS
RECALL
FROM
SECTION
THAT
THE
PDF
FOR
AN
MVN
IN
D
DIMENSIONS
IS
DEFINED
BY
THE
FOLLOWING
FIGURE
VISUALIZATION
OF
A
DIMENSIONAL
GAUSSIAN
DENSITY
THE
MAJOR
AND
MINOR
AXES
OF
THE
ELLIPSE
ARE
DEFINED
BY
THE
FIRST
TWO
EIGENVECTORS
OF
THE
COVARIANCE
MATRIX
NAMELY
AND
BASED
ON
FIGURE
OF
BISHOP
THE
EXPRESSION
INSIDE
THE
EXPONENT
IS
THE
MAHALANOBIS
DISTANCE
BETWEEN
A
DATA
VECTOR
X
AND
THE
MEAN
VECTOR
Μ
WE
CAN
GAIN
A
BETTER
UNDERSTANDING
OF
THIS
QUANTITY
BY
PERFORMING
AN
EIGENDECOMPOSITION
OF
Σ
THAT
IS
WE
WRITE
Σ
UΛUT
WHERE
U
IS
AN
ORTHONORMAL
MATRIX
OF
EIGENVECTORS
SATSIFYING
UT
U
I
AND
Λ
IS
A
DIAGONAL
MATRIX
OF
EIGENVALUES
USING
THE
EIGENDECOMPOSITION
WE
HAVE
THAT
Σ
U
T
Λ
UΛ
U
UT
WHERE
UI
IS
THE
I
TH
COLUMN
OF
U
CONTAINING
THE
I
TH
EIGENVECTOR
HENCE
WE
CAN
REWRITE
THE
MAHALANOBIS
DISTANCE
AS
FOLLOWS
X
Μ
T
Σ
X
Μ
X
Μ
T
D
I
UIUT
ΛI
X
Μ
D
D
X
Μ
T
U
UT
X
Μ
YI
WHERE
YI
UT
X
Μ
RECALL
THAT
THE
EQUATION
FOR
AN
ELLIPSE
IN
IS
HENCE
WE
SEE
THAT
THE
CONTOURS
OF
EQUAL
PROBABILITY
DENSITY
OF
A
GAUSSIAN
LIE
ALONG
ELLIPSES
THIS
IS
ILLUSTRATED
IN
FIGURE
THE
EIGENVECTORS
DETERMINE
THE
ORIENTATION
OF
THE
ELLIPSE
AND
THE
EIGENVALUES
DETERMINE
HOW
ELOGONATED
IT
IS
IN
GENERAL
WE
SEE
THAT
THE
MAHALANOBIS
DISTANCE
CORRESPONDS
TO
EUCLIDEAN
DISTANCE
IN
A
TRANSFORMED
COORDINATE
SYSTEM
WHERE
WE
SHIFT
BY
Μ
AND
ROTATE
BY
U
INTRODUCTION
MLE
FOR
AN
MVN
WE
NOW
DESCRIBE
ONE
WAY
TO
ESTIMATE
THE
PARAMETERS
OF
AN
MVN
USING
MLE
IN
LATER
SECTIONS
WE
WILL
DISCUSS
BAYESIAN
INFERENCE
FOR
THE
PARAMETERS
WHICH
CAN
MITIGATE
OVERFITTING
AND
CAN
PROVIDE
A
MEASURE
OF
CONFIDENCE
IN
OUR
ESTIMATES
THEOREM
MLE
FOR
A
GAUSSIAN
IF
WE
HAVE
N
IID
SAMPLES
XI
Μ
Σ
THEN
THE
MLE
FOR
THE
PARAMETERS
IS
GIVEN
BY
ΜˆMLE
N
XI
N
I
N
X
N
Σˆ
X
X
X
X
T
X
XT
X
XT
THAT
IS
THE
MLE
IS
JUST
THE
EMPIRICAL
MEAN
AND
EMPIRICAL
COVARIANCE
IN
THE
UNIVARIATE
CASE
WE
GET
THE
FOLLOWING
FAMILIAR
RESULTS
Μˆ
X
N
I
I
X
X
X
X
PROOF
TO
PROVE
THIS
RESULT
WE
WILL
NEED
SEVERAL
RESULTS
FROM
MATRIX
ALGEBRA
WHICH
WE
SUMMARIZE
BELOW
IN
THE
EQUATIONS
A
AND
B
ARE
VECTORS
AND
A
AND
B
ARE
MATRICES
ALSO
THE
NOTATION
TR
A
REFERS
TO
THE
TRACE
OF
A
MATRIX
WHICH
IS
THE
SUM
OF
ITS
DIAGONALS
TR
A
I
AII
THE
LAST
EQUATION
IS
CALLED
THE
CYCLIC
PERMUTATION
PROPERTY
OF
THE
TRACE
OPERATOR
USING
THIS
WE
CAN
DERIVE
THE
WIDELY
USED
TRACE
TRICK
WHICH
REORDERS
THE
SCALAR
INNER
PRODUCT
XT
AX
AS
FOLLOWS
XT
AX
TR
XT
AX
TR
XXT
A
TR
AXXT
PROOF
WE
CAN
NOW
BEGIN
WITH
THE
PROOF
THE
LOG
LIKELIHOOD
IS
Μ
Σ
LOG
P
D
Μ
Σ
N
LOG
Λ
X
Μ
Λ
X
Μ
WHERE
Λ
Σ
IS
THE
PRECISION
MATRIX
I
I
I
USING
THE
SUBSTITUTION
YI
XI
Μ
AND
THE
CHAIN
RULE
OF
CALCULUS
WE
HAVE
YI
X
Μ
T
Σ
X
Μ
YT
Σ
Μ
I
I
YI
I
I
Μ
HENCE
Σ
Σ
T
YI
N
N
Μ
Σ
X
Μ
I
I
N
Μ
Σ
XI
I
Μ
Μˆ
X
N
I
I
X
SO
THE
MLE
OF
Μ
IS
JUST
THE
EMPIRICAL
MEAN
NOW
WE
CAN
USE
THE
TRACE
TRICK
TO
REWRITE
THE
LOG
LIKELIHOOD
FOR
Λ
AS
FOLLOWS
N
I
I
I
N
Μ
Λ
WHERE
LOG
Λ
TR
SΜΛ
N
SΜ
XI
Μ
XI
Μ
T
I
IS
THE
SCATTER
MATRIX
CENTERED
ON
Μ
TAKING
DERIVATIVES
OF
THIS
EXPRESSION
WITH
RESPECT
TO
Λ
YIELDS
Λ
N
Λ
T
ST
Λ
T
Λ
Σ
N
Μ
SO
N
Σˆ
XI
N
I
Μ
XI
Μ
WHICH
IS
JUST
THE
EMPIRICAL
COVARIANCE
MATRIX
CENTERED
ON
Μ
IF
WE
PLUG
IN
THE
MLE
Μ
X
SINCE
BOTH
PARAMETERS
MUST
BE
SIMULTANEOUSLY
OPTIMIZED
WE
GET
THE
STANDARD
EQUATION
FOR
THE
MLE
OF
A
COVARIANCE
MATRIX
MAXIMUM
ENTROPY
DERIVATION
OF
THE
GAUSSIAN
IN
THIS
SECTION
WE
SHOW
THAT
THE
MULTIVARIATE
GAUSSIAN
IS
THE
DISTRIBUTION
WITH
MAXIMUM
ENTROPY
SUBJECT
TO
HAVING
A
SPECIFIED
MEAN
AND
COVARIANCE
SEE
ALSO
SECTION
THIS
IS
ONE
REASON
THE
GAUSSIAN
IS
SO
WIDELY
USED
THE
FIRST
TWO
MOMENTS
ARE
USUALLY
ALL
THAT
WE
CAN
RELIABLY
ESTIMATE
FROM
DATA
SO
WE
WANT
A
DISTRIBUTION
THAT
CAPTURES
THESE
PROPERTIES
BUT
OTHERWISE
MAKES
AS
FEW
ADDTIONAL
ASSUMPTIONS
AS
POSSIBLE
TO
SIMPLIFY
NOTATION
WE
WILL
ASSUME
THE
MEAN
IS
ZERO
THE
PDF
HAS
THE
FORM
P
X
EXP
XT
Σ
IF
WE
DEFINE
FIJ
X
XIXJ
AND
ΛIJ
Σ
IJ
FOR
I
J
D
WE
SEE
THAT
THIS
IS
IN
THE
SAME
FORM
AS
EQUATION
THE
DIFFERENTIAL
ENTROPY
OF
THIS
DISTRIBUTION
USING
LOG
BASE
E
IS
GIVEN
BY
H
Μ
Σ
LN
D
Σ
WE
NOW
SHOW
THE
MVN
HAS
MAXIMUM
ENTROPY
AMONGST
ALL
DISTRIBUTIONS
WITH
A
SPECIFIED
CO
VARIANCE
Σ
THEOREM
LET
Q
X
BE
ANY
DENSITY
SATISFYING
Q
X
XIXJ
ΣIJ
LET
P
N
Σ
THEN
H
Q
H
P
PROOF
FROM
COVER
AND
THOMAS
WE
HAVE
KL
Q
P
Q
X
LOG
Q
X
DX
P
X
H
Q
R
Q
X
LOG
P
X
DX
H
Q
R
P
X
LOG
P
X
DX
H
Q
H
P
WHERE
THE
KEY
STEP
IN
EQUATION
MARKED
WITH
A
FOLLOWS
SINCE
Q
AND
P
YIELD
THE
SAME
MOMENTS
FOR
THE
QUADRATIC
FORM
ENCODED
BY
LOG
P
X
GAUSSIAN
DISCRIMINANT
ANALYSIS
ONE
IMPORTANT
APPLICATION
OF
MVNS
IS
TO
DEFINE
THE
THE
CLASS
CONDITIONAL
DENSITIES
IN
A
GENERATIVE
CLASSIFIER
I
E
P
X
Y
C
Θ
N
X
ΜC
ΣC
THE
RESULTING
TECHNIQUE
IS
CALLED
GAUSSIAN
DISCRIMINANT
ANALYSIS
OR
GDA
EVEN
THOUGH
IT
IS
A
GENERATIVE
NOT
DISCRIMINATIVE
CLASSIFIER
SEE
SECTION
FOR
MORE
ON
THIS
DISTINCTION
IF
ΣC
IS
DIAGONAL
THIS
IS
EQUIVALENT
TO
NAIVE
BAYES
RED
FEMALE
BLUE
MALE
RED
FEMALE
BLUE
MALE
HEIGHT
A
HEIGHT
B
FIGURE
A
HEIGHT
WEIGHT
DATA
B
VISUALIZATION
OF
GAUSSIANS
FIT
TO
EACH
CLASS
OF
THE
PROBABILITY
MASS
IS
INSIDE
THE
ELLIPSE
FIGURE
GENERATED
BY
GAUSSHEIGHTWEIGHT
WE
CAN
CLASSIFY
A
FEATURE
VECTOR
USING
THE
FOLLOWING
DECISION
RULE
DERIVED
FROM
EQUATION
Yˆ
X
ARGMAX
LOG
P
Y
C
Π
LOG
P
X
ΘC
WHEN
WE
COMPUTE
THE
PROBABILITY
OF
X
UNDER
EACH
CLASS
CONDITIONAL
DENSITY
WE
ARE
MEASURING
THE
DISTANCE
FROM
X
TO
THE
CENTER
OF
EACH
CLASS
ΜC
USING
MAHALANOBIS
DISTANCE
THIS
CAN
BE
THOUGHT
OF
AS
A
NEAREST
CENTROIDS
CLASSIFIER
AS
AN
EXAMPLE
FIGURE
SHOWS
TWO
GAUSSIAN
CLASS
CONDITIONAL
DENSITIES
IN
REPRESENTING
THE
HEIGHT
AND
WEIGHT
OF
MEN
AND
WOMEN
WE
CAN
SEE
THAT
THE
FEATURES
ARE
CORRELATED
AS
IS
TO
BE
EXPECTED
TALL
PEOPLE
TEND
TO
WEIGH
MORE
THE
ELLIPSES
FOR
EACH
CLASS
CONTAIN
OF
THE
PROBABILITY
MASS
IF
WE
HAVE
A
UNIFORM
PRIOR
OVER
CLASSES
WE
CAN
CLASSIFY
A
NEW
TEST
VECTOR
AS
FOLLOWS
Yˆ
X
ARGMIN
X
ΜC
T
Σ
C
X
ΜC
QUADRATIC
DISCRIMINANT
ANALYSIS
QDA
THE
POSTERIOR
OVER
CLASS
LABELS
IS
GIVEN
BY
EQUATION
WE
CAN
GAIN
FURTHER
INSIGHT
INTO
THIS
MODEL
BY
PLUGGING
IN
THE
DEFINITION
OF
THE
GAUSSIAN
DENSITY
AS
FOLLOWS
P
Y
C
X
Θ
C
C
T
THRESHOLDING
THIS
RESULTS
IN
A
QUADRATIC
FUNCTION
OF
X
THE
RESULT
IS
KNOWN
AS
QUADRATIC
DISCRIMINANT
ANALYSIS
QDA
FIGURE
GIVES
SOME
EXAMPLES
OF
WHAT
THE
DECISION
BOUNDARIES
LOOK
LIKE
IN
PARABOLIC
BOUNDARY
SOME
LINEAR
SOME
QUADRATIC
A
B
FIGURE
QUADRATIC
DECISION
BOUNDARIES
IN
FOR
THE
AND
CLASS
CASE
FIGURE
GENERATED
BY
DISCRIMANALYSISDBOUNDARIESDEMO
T
T
T
T
FIGURE
SOFTMAX
DISTRIBUTION
Η
T
WHERE
Η
AT
DIFFERENT
TEMPERATURES
T
WHEN
THE
TEMPERATURE
IS
HIGH
LEFT
THE
DISTRIBUTION
IS
UNIFORM
WHEREAS
WHEN
THE
TEMPERATURE
IS
LOW
RIGHT
THE
DISTRIBUTION
IS
SPIKY
WITH
ALL
ITS
MASS
ON
THE
LARGEST
ELEMENT
FIGURE
GENERATED
BY
LINEAR
DISCRIMINANT
ANALYSIS
LDA
WE
NOW
CONSIDER
A
SPECIAL
CASE
IN
WHICH
THE
COVARIANCE
MATRICES
ARE
TIED
OR
SHARED
ACROSS
CLASSES
ΣC
Σ
IN
THIS
CASE
WE
CAN
SIMPLIFY
EQUATION
AS
FOLLOWS
P
Y
C
X
Θ
Π
EXP
ΜT
Σ
XT
Σ
ΜT
Σ
L
EXP
ΜT
Σ
ΜT
Σ
LOG
Π
L
EXP
XT
Σ
SINCE
THE
QUADRATIC
TERM
XT
Σ
IS
INDEPENDENT
OF
C
IT
WILL
CANCEL
OUT
IN
THE
NUMERATOR
AND
DENOMINATOR
IF
WE
DEFINE
Γ
ΜT
Σ
LOG
Π
ΒC
Σ
THEN
WE
CAN
WRITE
P
Y
C
X
Θ
EΒT
X
ΓC
EΒT
X
Γ
T
Η
C
WHERE
Η
ΒT
X
ΒT
X
ΓC
AND
IS
THE
SOFTMAX
FUNCTION
DEFINED
AS
FOLLOWS
C
EΗC
Η
C
C
CT
EΗCT
THE
SOFTMAX
FUNCTION
IS
SO
CALLED
SINCE
IT
ACTS
A
BIT
LIKE
THE
MAX
FUNCTION
TO
SEE
THIS
LET
US
DIVIDE
EACH
ΗC
BY
A
CONSTANT
T
CALLED
THE
TEMPERATURE
THEN
AS
T
WE
FIND
Η
T
C
IF
C
ARGMAXCT
ΗCT
OTHERWISE
IN
OTHER
WORDS
AT
LOW
TEMPERATURES
THE
DISTRIBUTION
SPENDS
ESSENTIALLY
ALL
OF
ITS
TIME
IN
THE
MOST
PROBABLE
STATE
WHEREAS
AT
HIGH
TEMPERATURES
IT
VISITS
ALL
STATES
UNIFORMLY
SEE
FIGURE
FOR
AN
ILLUSTRATION
NOTE
THAT
THIS
TERMINOLOGY
COMES
FROM
THE
AREA
OF
STATISTICAL
PHYSICS
WHERE
IT
IS
COMMON
TO
USE
THE
BOLTZMANN
DISTRIBUTION
WHICH
HAS
THE
SAME
FORM
AS
THE
SOFTMAX
FUNCTION
AN
INTERESTING
PROPERTY
OF
EQUATION
IS
THAT
IF
WE
TAKE
LOGS
WE
END
UP
WITH
A
LINEAR
FUNCTION
OF
X
THE
REASON
IT
IS
LINEAR
IS
BECAUSE
THE
XT
Σ
CANCELS
FROM
THE
NUMERATOR
AND
DENOMINATOR
THUS
THE
DECISION
BOUNDARY
BETWEEN
ANY
TWO
CLASSES
SAY
C
AND
C
WILL
BE
A
STRAIGHT
LINE
HENCE
THIS
TECHNIQUE
IS
CALLED
LINEAR
DISCRIMINANT
ANALYSIS
OR
LDA
WE
CAN
DERIVE
THE
FORM
OF
THIS
LINE
AS
FOLLOWS
P
Y
C
X
Θ
P
Y
C
X
Θ
T
T
ΒC
X
ΓC
ΒCTX
ΓCT
XT
ΒCT
Β
ΓCT
ΓC
SEE
FIGURE
FOR
SOME
EXAMPLES
AN
ALTERNATIVE
TO
FITTING
AN
LDA
MODEL
AND
THEN
DERIVING
THE
CLASS
POSTERIOR
IS
TO
DIRECTLY
FIT
P
Y
X
W
CAT
Y
WX
FOR
SOME
C
D
WEIGHT
MATRIX
W
THIS
IS
CALLED
MULTI
CLASS
LOGISTIC
REGRESSION
OR
MULTINOMIAL
LOGISTIC
REGRESSION
WE
WILL
DISCUSS
THIS
MODEL
IN
DETAIL
IN
SECTION
THE
DIFFERENCE
BETWEEN
THE
TWO
APPROACHES
IS
EXPLAINED
IN
SECTION
TWO
CLASS
LDA
TO
GAIN
FURTHER
INSIGHT
INTO
THE
MEANING
OF
THESE
EQUATIONS
LET
US
CONSIDER
THE
BINARY
CASE
IN
THIS
CASE
THE
POSTERIOR
IS
GIVEN
BY
EΒT
X
P
Y
X
Θ
EΒT
X
EΒT
X
SIGM
Β
E
T
X
X
THE
ABBREVIATION
LDA
COULD
EITHER
STAND
FOR
LINEAR
DISCRIMINANT
ANALYSIS
OR
LATENT
DIRICHLET
ALLOCATION
SEC
TION
WE
HOPE
THE
MEANING
IS
CLEAR
FROM
TEXT
IN
THE
LANGUAGE
MODELING
COMMUNITY
THIS
MODEL
IS
CALLED
A
MAXIMUM
ENTROPY
MODEL
FOR
REASONS
EXPLAINED
IN
SECTION
LINEAR
BOUNDARY
ALL
LINEAR
BOUNDARIES
A
B
FIGURE
LINEAR
DECISION
BOUNDARIES
IN
FOR
THE
AND
CLASS
CASE
FIGURE
GENERATED
BY
DISCRIMANALYSISDBOUNDARIESDEMO
FIGURE
GEOMETRY
OF
LDA
IN
THE
CLASS
CASE
WHERE
I
WHERE
SIGM
Η
REFERS
TO
THE
SIGMOID
FUNCTION
EQUATION
NOW
Γ
Γ
ΜT
Σ
T
LOG
Π
Π
Μ
Μ
T
Σ
Μ
Μ
LOG
Π
Π
SO
IF
WE
DEFINE
W
Σ
X
Μ
Μ
Μ
LOG
Μ
Μ
T
Σ
THEN
WE
HAVE
WT
AND
HENCE
P
Y
X
Θ
SIGM
WT
X
THIS
IS
CLOSELY
RELATED
TO
LOGISTIC
REGRESSION
WHICH
WE
WILL
DISCUSS
IN
SECTION
SO
THE
FINAL
DECISION
RULE
IS
AS
FOLLOWS
SHIFT
X
BY
PROJECT
ONTO
THE
LINE
W
AND
SEE
IF
THE
RESULT
IS
POSITIVE
OR
NEGATIVE
IF
Σ
THEN
W
IS
IN
THE
DIRECTION
OF
SO
WE
CLASSIFY
THE
POINT
BASED
ON
WHETHER
ITS
PROJECTION
IS
CLOSER
TO
OR
THIS
IS
ILLUSTRATED
IN
FIGURE
FURTHEMORE
IF
THEN
WHICH
IS
HALF
WAY
BETWEEN
THE
MEANS
IF
WE
MAKE
THEN
GETS
CLOSER
TO
SO
MORE
OF
THE
LINE
BELONGS
TO
CLASS
A
PRIORI
CONVERSELY
IF
THE
BOUNDARY
SHIFTS
RIGHT
THUS
WE
SEE
THAT
THE
CLASS
PRIOR
ΠC
JUST
CHANGES
THE
DECISION
THRESHOLD
AND
NOT
THE
OVERALL
GEOMETRY
AS
WE
CLAIMED
ABOVE
A
SIMILAR
ARGUMENT
APPLIES
IN
THE
MULTI
CLASS
CASE
THE
MAGNITUDE
OF
W
DETERMINES
THE
STEEPNESS
OF
THE
LOGISTIC
FUNCTION
AND
DEPENDS
ON
HOW
WELL
SEPARATED
THE
MEANS
ARE
RELATIVE
TO
THE
VARIANCE
IN
PSYCHOLOGY
AND
SIGNAL
DETECTION
THEORY
IT
IS
COMMON
TO
DEFINE
THE
DISCRIMINABILITY
OF
A
SIGNAL
FROM
THE
BACKGROUND
NOISE
USING
A
QUANTITY
CALLED
D
PRIME
D
Σ
WHERE
IS
THE
MEAN
OF
THE
SIGNAL
AND
IS
THE
MEAN
OF
THE
NOISE
AND
Σ
IS
THE
STANDARD
DEVIATION
OF
THE
NOISE
IF
D
IS
LARGE
THE
SIGNAL
WILL
BE
EASIER
TO
DISCRIMINATE
FROM
THE
NOISE
MLE
FOR
DISCRIMINANT
ANALYSIS
WE
NOW
DISCUSS
HOW
TO
FIT
A
DISCRIMINANT
ANALYSIS
MODEL
THE
SIMPLEST
WAY
IS
TO
USE
MAXIMUM
LIKELIHOOD
THE
LOG
LIKELIHOOD
FUNCTION
IS
AS
FOLLOWS
LOG
P
D
Θ
R
N
I
YI
C
LOG
LOG
N
X
ΜC
ΣC
WE
SEE
THAT
THIS
FACTORIZES
INTO
A
TERM
FOR
Π
AND
C
TERMS
FOR
EACH
ΜC
AND
ΣC
HENCE
WE
CAN
ESTIMATE
THESE
PARAMETERS
SEPARATELY
FOR
THE
CLASS
PRIOR
WE
HAVE
ΠˆC
NC
AS
WITH
NAIVE
BAYES
FOR
THE
CLASS
CONDITIONAL
DENSITIES
WE
JUST
PARTITION
THE
DATA
BASED
ON
ITS
CLASS
LABEL
AND
COMPUTE
THE
MLE
FOR
EACH
GAUSSIAN
ΜˆC
C
I
YI
C
XI
Σˆ
C
NC
XI
I
YI
C
Μˆ
C
XI
Μˆ
C
T
SEE
DISCRIMANALYSISFIT
FOR
A
MATLAB
IMPLEMENTATION
ONCE
THE
MODEL
HAS
BEEN
FIT
YOU
CAN
MAKE
PREDICTIONS
USING
DISCRIMANALYSISPREDICT
WHICH
USES
A
PLUG
IN
APPROXIMATION
STRATEGIES
FOR
PREVENTING
OVERFITTING
THE
SPEED
AND
SIMPLICITY
OF
THE
MLE
METHOD
IS
ONE
OF
ITS
GREATEST
APPEALS
HOWEVER
THE
MLE
CAN
BADLY
OVERFIT
IN
HIGH
DIMENSIONS
IN
PARTICULAR
THE
MLE
FOR
A
FULL
COVARIANCE
MATRIX
IS
SINGULAR
IF
NC
D
AND
EVEN
WHEN
NC
D
THE
MLE
CAN
BE
ILL
CONDITIONED
MEANING
IT
IS
CLOSE
TO
SINGULAR
THERE
ARE
SEVERAL
POSSIBLE
SOLUTIONS
TO
THIS
PROBLEM
USE
A
DIAGONAL
COVARIANCE
MATRIX
FOR
EACH
CLASS
WHICH
ASSUMES
THE
FEATURES
ARE
CONDITIONALLY
INDEPENDENT
THIS
IS
EQUIVALENT
TO
USING
A
NAIVE
BAYES
CLASSIFIER
SECTION
USE
A
FULL
COVARIANCE
MATRIX
BUT
FORCE
IT
TO
BE
THE
SAME
FOR
ALL
CLASSES
ΣC
Σ
THIS
IS
AN
EXAMPLE
OF
PARAMETER
TYING
OR
PARAMETER
SHARING
AND
IS
EQUIVALENT
TO
LDA
SECTION
USE
A
DIAGONAL
COVARIANCE
MATRIX
AND
FORCED
IT
TO
BE
SHARED
THIS
IS
CALLED
DIAGONAL
COVARIANCE
LDA
AND
IS
DISCUSSED
IN
SECTION
USE
A
FULL
COVARIANCE
MATRIX
BUT
IMPOSE
A
PRIOR
AND
THEN
INTEGRATE
IT
OUT
IF
WE
USE
A
CONJUGATE
PRIOR
THIS
CAN
BE
DONE
IN
CLOSED
FORM
USING
THE
RESULTS
FROM
SECTION
THIS
IS
ANALOGOUS
TO
THE
BAYESIAN
NAIVE
BAYES
METHOD
IN
SECTION
SEE
MINKA
FOR
DETAILS
FIT
A
FULL
OR
DIAGONAL
COVARIANCE
MATRIX
BY
MAP
ESTIMATION
WE
DISCUSS
TWO
DIFFERENT
KINDS
OF
PRIOR
BELOW
PROJECT
THE
DATA
INTO
A
LOW
DIMENSIONAL
SUBSPACE
AND
FIT
THE
GAUSSIANS
THERE
SEE
SEC
TION
FOR
A
WAY
TO
FIND
THE
BEST
MOST
DISCRIMINATIVE
LINEAR
PROJECTION
WE
DISCUSS
SOME
OF
THESE
OPTIONS
BELOW
REGULARIZED
LDA
SUPPOSE
WE
TIE
THE
COVARIANCE
MATRICES
SO
ΣC
Σ
AS
IN
LDA
AND
FURTHERMORE
WE
PERFORM
MAP
ESTIMATION
OF
Σ
USING
AN
INVERSE
WISHART
PRIOR
OF
THE
FORM
IW
DIAG
Σˆ
MLE
SEE
SECTION
THEN
WE
HAVE
Σˆ
ΛDIAG
Σˆ
MLE
Λ
Σˆ
MLE
WHERE
Λ
CONTROLS
THE
AMOUNT
OF
REGULARIZATION
WHICH
IS
RELATED
TO
THE
STRENGTH
OF
THE
PRIOR
SEE
SECTION
FOR
DETAILS
THIS
TECHNIQUE
IS
KNOWN
AS
REGULARIZED
DISCRIMINANT
ANALYSIS
OR
RDA
HASTIE
ET
AL
WHEN
WE
EVALUATE
THE
CLASS
CONDITIONAL
DENSITIES
WE
NEED
TO
COMPUTE
Σˆ
AND
HENCE
Σˆ
WHICH
IS
IMPOSSIBLE
TO
COMPUTE
IF
D
N
HOWEVER
WE
CAN
USE
THE
SVD
OF
X
SECTION
TO
GET
AROUND
THIS
AS
WE
SHOW
BELOW
NOTE
THAT
THIS
TRICK
CANNOT
BE
APPLIED
TO
QDA
WHICH
IS
A
NONLINEAR
FUNCTION
OF
X
LET
X
UDVT
BE
THE
SVD
OF
THE
DESIGN
MATRIX
WHERE
V
IS
D
N
U
IS
AN
N
N
ORTHOGONAL
MATRIX
AND
D
IS
A
DIAGONAL
MATRIX
OF
SIZE
N
FURTHERMORE
DEFINE
THE
N
N
MATRIX
Z
UD
THIS
IS
LIKE
A
DESIGN
MATRIX
IN
A
LOWER
DIMENSIONAL
SPACE
SINCE
WE
ASSUME
N
D
ALSO
DEFINE
ΜZ
VT
Μ
AS
THE
MEAN
OF
THE
DATA
IN
THIS
REDUCED
SPACE
WE
CAN
RECOVER
THE
ORIGINAL
MEAN
USING
Μ
VΜZ
SINCE
VT
V
VVT
I
WITH
THESE
DEFINITIONS
WE
CAN
REWRITE
THE
MLE
AS
FOLLOWS
Σˆ
MLE
XT
X
ΜΜT
N
ZVT
T
ZVT
VΜ
VΜ
T
VZT
ZVT
VΜ
ΜT
VT
V
ZT
Z
Μ
ΜT
VT
VΣˆ
ZVT
WHERE
Σˆ
Z
IS
THE
EMPIRICAL
COVARIANCE
OF
Z
HENCE
WE
CAN
REWRITE
THE
MAP
ESTIMATE
AS
Σˆ
MAP
VΣ
ZVT
Σ
Z
ΛDIAG
Σˆ
Z
Λ
Σˆ
Z
NOTE
HOWEVER
THAT
WE
NEVER
NEED
TO
ACTUALLY
COMPUTE
THE
D
D
MATRIX
Σˆ
MAP
THIS
IS
BECAUSE
EQUATION
TELLS
US
THAT
TO
CLASSIFY
USING
LDA
ALL
WE
NEED
TO
COMPUTE
IS
P
Y
C
X
Θ
EXP
ΔC
WHERE
Δ
XT
Β
Γ
Β
Σˆ
Γ
T
Μ
Β
LOG
Π
WE
CAN
COMPUTE
THE
CRUCIAL
ΒC
TERM
FOR
RDA
WITHOUT
INVERTING
THE
D
D
MATRIX
AS
FOLLOWS
ˆ
C
MAP
C
VΣ
ZVT
VΣ
Μ
VΣ
Z
C
WHERE
ΜZ
C
VT
ΜC
IS
THE
MEAN
OF
THE
Z
MATRIX
FOR
DATA
BELONGING
TO
CLASS
C
SEE
RDAFIT
FOR
THE
CODE
DIAGONAL
LDA
A
SIMPLE
ALTERNATIVE
TO
RDA
IS
TO
TIE
THE
COVARIANCE
MATRICES
SO
ΣC
Σ
AS
IN
LDA
AND
THEN
TO
USE
A
DIAGONAL
COVARIANCE
MATRIX
FOR
EACH
CLASS
THIS
IS
CALLED
THE
DIAGONAL
LDA
MODEL
AND
IS
EQUIVALENT
TO
RDA
WITH
Λ
THE
CORRESPONDING
DISCRIMINANT
FUNCTION
IS
AS
FOLLOWS
COMPARE
TO
EQUATION
Δ
X
LOG
P
X
Y
C
Θ
XJ
ΜCJ
LOG
Π
TYPICALLY
WE
SET
ΜˆCJ
XCJ
AND
WHICH
IS
THE
POOLED
EMPIRICAL
VARIANCE
OF
FEATURE
J
J
J
POOLED
ACROSS
CLASSES
DEFINED
BY
C
C
I
YI
C
XIJ
XCJ
N
C
IN
HIGH
DIMENSIONAL
SETTINGS
THIS
MODEL
CAN
WORK
MUCH
BETTER
THAN
LDA
AND
RDA
BICKEL
AND
LEVINA
NUMBER
OF
GENES
TEST
TRAIN
CV
FIGURE
ERROR
VERSUS
AMOUNT
OF
SHRINKAGE
FOR
NEAREST
SHRUNKEN
CENTROID
CLASSIFIER
APPLIED
TO
THE
SRBCT
GENE
EXPRESSION
DATA
BASED
ON
FIGURE
OF
HASTIE
ET
AL
FIGURE
GENERATED
BY
SHRUNKENCENTROIDSSRBCTDEMO
NEAREST
SHRUNKEN
CENTROIDS
CLASSIFIER
ONE
DRAWBACK
OF
DIAGONAL
LDA
IS
THAT
IT
DEPENDS
ON
ALL
OF
THE
FEATURES
IN
HIGH
DIMENSIONAL
PROBLEMS
WE
MIGHT
PREFER
A
METHOD
THAT
ONLY
DEPENDS
ON
A
SUBSET
OF
THE
FEATURES
FOR
REASONS
OF
ACCURACY
AND
INTERPRETABILITY
ONE
APPROACH
IS
TO
USE
A
SCREENING
METHOD
PERHAPS
BASED
ON
MUTUAL
INFORMATION
AS
IN
SECTION
WE
NOW
DISCUSS
ANOTHER
APPROACH
TO
THIS
PROBLEM
KNOWN
AS
THE
NEAREST
SHRUNKEN
CENTROIDS
CLASSIFIER
HASTIE
ET
AL
THE
BASIC
IDEA
IS
TO
PERFORM
MAP
ESTIMATION
FOR
DIAGONAL
LDA
WITH
A
SPARSITY
PROMOTING
LAPLACE
PRIOR
SEE
SECTION
MORE
PRECISELY
DEFINE
THE
CLASS
SPECIFIC
FEATURE
MEAN
ΜCJ
IN
TERMS
OF
THE
CLASS
INDEPENDENT
FEATURE
MEAN
MJ
AND
A
CLASS
SPECIFIC
OFFSET
ΔCJ
THUS
WE
HAVE
ΜCJ
MJ
ΔCJ
WE
WILL
THEN
PUT
A
PRIOR
ON
THE
ΔCJ
TERMS
TO
ENCOURAGE
THEM
TO
BE
STRICTLY
ZERO
AND
COMPUTE
A
MAP
ESTIMATE
IF
FOR
FEATURE
J
WE
FIND
THAT
ΔCJ
FOR
ALL
C
THEN
FEATURE
J
WILL
PLAY
NO
ROLE
IN
THE
CLASSIFICATION
DECISION
SINCE
ΜCJ
WILL
BE
INDEPENDENT
OF
C
THUS
FEATURES
THAT
ARE
NOT
DISCRIMINATIVE
ARE
AUTOMATICALLY
IGNORED
THE
DETAILS
CAN
BE
FOUND
IN
HASTIE
ET
AL
AND
GREENSHTEIN
AND
PARK
SEE
SHRUNKENCENTROIDSFIT
FOR
SOME
CODE
LET
US
GIVE
AN
EXAMPLE
OF
THE
METHOD
IN
ACTION
BASED
ON
HASTIE
ET
AL
CONSIDER
THE
PROBLEM
OF
CLASSIFYING
A
GENE
EXPRESSION
DATASET
WHICH
GENES
CLASSES
TRAINING
SAMPLES
AND
TEST
SAMPLES
USING
A
DIAGONAL
LDA
CLASSIFIER
PRODUCES
ERRORS
ON
THE
TEST
SET
USING
THE
NEAREST
SHRUNKEN
CENTROIDS
CLASSIFIER
PRODUCED
ERRORS
ON
THE
TEST
SET
FOR
A
RANGE
OF
Λ
VALUES
SEE
FIGURE
MORE
IMPORTANTLY
THE
MODEL
IS
SPARSE
AND
HENCE
MORE
INTERPRETABLE
FIGURE
PLOTS
AN
UNPENALIZED
ESTIMATE
OF
THE
DIFFERENCE
DCJ
IN
GRAY
AS
WELL
AS
THE
SHRUNKEN
ESTIMATES
ΔCJ
IN
BLUE
THESE
ESTIMATES
ARE
COMPUTED
USING
THE
VALUE
OF
Λ
ESTIMATED
BY
CV
WE
SEE
THAT
ONLY
GENES
ARE
USED
OUT
OF
THE
ORIGINAL
NOW
CONSIDER
AN
EVEN
HARDER
PROBLEM
WITH
GENES
A
TRAINING
SET
OF
PATIENTS
A
TEST
SET
OF
PATIENTS
AND
DIFFERENT
TYPES
OF
CANCER
RAMASWAMY
ET
AL
HASTIE
ET
AL
HASTIE
ET
AL
REPORT
THAT
NEAREST
SHRUNKEN
CENTROIDS
PRODUCED
ERRORS
ON
THE
TEST
CLASS
CLASS
A
B
CLASS
CLASS
C
D
FIGURE
PROFILE
OF
THE
SHRUNKEN
CENTROIDS
CORRESPONDING
TO
Λ
CV
OPTIMAL
IN
FIG
URE
THIS
SELECTS
GENES
BASED
ON
FIGURE
OF
HASTIE
ET
AL
FIGURE
GENERATED
BY
SHRUNKENCENTROIDSSRBCTDEMO
SET
USING
GENES
AND
THAT
RDA
SECTION
PRODUCED
ERRORS
ON
THE
TEST
SET
USING
ALL
GENES
THE
PMTK
FUNCTION
CANCERHIGHDIMCLASSIFDEMO
CAN
BE
USED
TO
REPRODUCE
THESE
NUMBERS
INFERENCE
IN
JOINTLY
GAUSSIAN
DISTRIBUTIONS
GIVEN
A
JOINT
DISTRIBUTION
P
IT
IS
USEFUL
TO
BE
ABLE
TO
COMPUTE
MARGINALS
P
AND
CONDITIONALS
P
WE
DISCUSS
HOW
TO
DO
THIS
BELOW
AND
THEN
GIVE
SOME
APPLICATIONS
THESE
OPERATIONS
TAKE
O
TIME
IN
THE
WORST
CASE
SEE
SECTION
FOR
FASTER
METHODS
STATEMENT
OF
THE
RESULT
THEOREM
MARGINALS
AND
CONDITIONALS
OF
AN
MVN
SUPPOSE
X
IS
JOINTLY
GAUSSIAN
WITH
PARAMETERS
Μ
Σ
Λ
Σ
THEN
THE
MARGINALS
ARE
GIVEN
BY
P
N
P
N
AND
THE
POSTERIOR
CONDITIONAL
IS
GIVEN
BY
EQUATION
IS
OF
SUCH
CRUCIAL
IMPORTANCE
IN
THIS
BOOK
THAT
WE
HAVE
PUT
A
BOX
AROUND
IT
SO
YOU
CAN
EASILY
FIND
IT
FOR
THE
PROOF
SEE
SECTION
WE
SEE
THAT
BOTH
THE
MARGINAL
AND
CONDITIONAL
DISTRIBUTIONS
ARE
THEMSELVES
GAUSSIAN
FOR
THE
MARGINALS
WE
JUST
EXTRACT
THE
ROWS
AND
COLUMNS
CORRESPONDING
TO
OR
FOR
THE
CONDITIONAL
WE
HAVE
TO
DO
A
BIT
MORE
WORK
HOWEVER
IT
IS
NOT
THAT
COMPLICATED
THE
CONDITIONAL
MEAN
IS
JUST
A
LINEAR
FUNCTION
OF
AND
THE
CONDITIONAL
COVARIANCE
IS
JUST
A
CONSTANT
MATRIX
THAT
IS
INDEPENDENT
OF
WE
GIVE
THREE
DIFFERENT
BUT
EQUIVALENT
EXPRESSIONS
FOR
THE
POSTERIOR
MEAN
AND
TWO
DIFFERENT
BUT
EQUIVALENT
EXPRESSIONS
FOR
THE
POSTERIOR
COVARIANCE
EACH
ONE
IS
USEFUL
IN
DIFFERENT
CIRCUMSTANCES
EXAMPLES
BELOW
WE
GIVE
SOME
EXAMPLES
OF
THESE
EQUATIONS
IN
ACTION
WHICH
WILL
MAKE
THEM
SEEM
MORE
INTUITIVE
MARGINALS
AND
CONDITIONALS
OF
A
GAUSSIAN
LET
US
CONSIDER
A
EXAMPLE
THE
COVARIANCE
MATRIX
IS
Σ
THE
MARGINAL
P
IS
A
GAUSSIAN
OBTAINED
BY
PROJECTING
THE
JOINT
DISTRIBUTION
ONTO
THE
LINE
P
N
P
P
P
A
B
C
FIGURE
A
A
JOINT
GAUSSIAN
DISTRIBUTION
P
WITH
A
CORRELATION
COEFFICIENT
OF
WE
PLOT
THE
CONTOUR
AND
THE
PRINCIPAL
AXES
B
THE
UNCONDITIONAL
MARGINAL
P
C
THE
CONDITIONAL
P
N
OBTAINED
BY
SLICING
A
AT
HEIGHT
FIGURE
GENERATED
BY
SUPPOSE
WE
OBSERVE
THE
CONDITIONAL
P
IS
OBTAINED
BY
SLICING
THE
JOINT
DISTRIBUTION
THROUGH
THE
LINE
SEE
FIGURE
P
N
IF
Σ
WE
GET
P
N
Ρ
IN
FIGURE
WE
SHOW
AN
EXAMPLE
WHERE
Ρ
Μ
AND
WE
SEE
THAT
E
WHICH
MAKES
SENSE
SINCE
Ρ
MEANS
THAT
WE
BELIEVE
THAT
IF
INCREASES
BY
BEYOND
ITS
MEAN
THEN
INCREASES
BY
WE
ALSO
SEE
VAR
THIS
ALSO
MAKES
SENSE
OUR
UNCERTAINTY
ABOUT
HAS
GONE
DOWN
SINCE
WE
HAV
E
LEARNED
OMETHING
ABOUT
INDIRECTLY
BY
OBSERVING
IF
Ρ
WE
GET
P
INTERPOLATING
NOISE
FREE
DATA
SUPPOSE
WE
WANT
TO
ESTIMATE
A
FUNCTION
DEFINED
ON
THE
INTERVAL
T
SUCH
THAT
YI
F
TI
FOR
N
OBSERVED
POINTS
TI
WE
ASSUME
FOR
NOW
THAT
THE
DATA
IS
NOISE
FREE
SO
WE
WANT
TO
INTERPOLATE
IT
THAT
IS
FIT
A
FUNCTION
THAT
GOES
EXACTLY
THROUGH
THE
DATA
SEE
SECTION
FOR
THE
NOISY
DATA
CASE
THE
QUESTION
IS
HOW
DOES
THE
FUNCTION
BEHAVE
IN
BETWEEN
THE
OBSERVED
DATA
POINTS
IT
IS
OFTEN
REASONABLE
TO
ASSUME
THAT
THE
UNKNOWN
FUNCTION
IS
SMOOTH
IN
CHAPTER
WE
SHALL
SEE
HOW
TO
ENCODE
PRIORS
OVER
FUNCTIONS
AND
HOW
TO
UPDATE
SUCH
A
PRIOR
WITH
OBSERVED
VALUES
TO
GET
A
POSTERIOR
OVER
FUNCTIONS
BUT
IN
THIS
SECTION
WE
TAKE
A
SIMPLER
APPROACH
WHICH
IS
ADEQUATE
FOR
MAP
ESTIMATION
OF
FUNCTIONS
DEFINED
ON
INPUTS
WE
FOLLOW
THE
PRESENTATION
OF
CALVETTI
AND
SOMERSALO
WE
START
BY
DISCRETIZING
THE
PROBLEM
FIRST
WE
DIVIDE
THE
SUPPORT
OF
THE
FUNCTION
INTO
D
EQUAL
SUBINTERVALS
WE
THEN
DEFINE
T
XJ
F
SJ
SJ
JH
H
D
J
D
A
B
FIGURE
INTERPOLATING
NOISE
FREE
DATA
USING
A
GAUSSIAN
WITH
PRIOR
PRECISION
Λ
A
Λ
B
Λ
SEE
ALSO
FIGURE
BASED
ON
FIGURE
OF
CALVETTI
AND
SOMERSALO
FIGURE
GENERATED
BY
GAUSSINTERPDEMO
WE
CAN
ENCODE
OUR
SMOOTHNESS
PRIOR
BY
ASSUMING
THAT
XJ
IS
AN
AVERAGE
OF
ITS
NEIGHBORS
XJ
AND
XJ
PLUS
SOME
GAUSSIAN
NOISE
XJ
XJ
XJ
EJ
J
D
WHERE
E
Λ
I
THE
PRECISION
TERM
Λ
CONTROLS
HOW
MUCH
WE
THINK
THE
FUNCTION
WILL
VARY
A
LARGE
Λ
CORRESPONDS
TO
A
BELIEF
THAT
THE
FUNCTION
IS
VERY
SMOOTH
A
SMALL
Λ
CORRESPONDS
TO
A
BELIEF
THAT
THE
FUNCTION
IS
QUITE
WIGGLY
IN
VECTOR
FORM
THE
ABOVE
EQUATION
CAN
BE
WRITTEN
AS
FOLLOWS
LX
E
WHERE
L
IS
THE
D
D
SECOND
ORDER
FINITE
DIFFERENCE
MATRIX
L
THE
CORRESPONDING
PRIOR
HAS
THE
FORM
T
WE
WILL
HENCEFORTH
ASSUME
WE
HAVE
SCALED
L
BY
Λ
SO
WE
CAN
IGNORE
THE
Λ
TERM
AND
JUST
WRITE
Λ
LT
L
FOR
THE
PRECISION
MATRIX
NOTE
THAT
ALTHOUGH
X
IS
D
DIMENSIONAL
THE
PRECISION
MATRIX
Λ
ONLY
HAS
RANK
D
THUS
THIS
IS
AN
IMPROPER
PRIOR
KNOWN
AS
AN
INTRINSIC
GAUSSIAN
RANDOM
FIELD
SEE
SECTION
FOR
MORE
INFORMATION
HOWEVER
PROVIDING
WE
OBSERVE
N
DATA
POINTS
THE
POSTERIOR
WILL
BE
PROPER
NOW
LET
BE
THE
N
NOISE
FREE
OBSERVATIONS
OF
THE
FUNCTION
AND
BE
THE
D
N
UNKNOWN
FUNCTION
VALUES
WITHOUT
LOSS
OF
GENERALITY
ASSUME
THAT
THE
UNKNOWN
VARIABLES
ARE
ORDERED
FIRST
THEN
THE
KNOWN
VARIABLES
THEN
WE
CAN
PARTITION
THE
L
MATRIX
AS
FOLLOWS
L
R
D
D
N
R
D
N
WE
CAN
ALSO
PARTITION
THE
PRECISION
MATRIX
OF
THE
JOINT
DISTRIBUTION
T
LT
LT
USING
EQUATION
WE
CAN
WRITE
THE
CONDITIONAL
DISTRIBUTION
AS
FOLLOWS
P
N
Λ
LT
Λ
NOTE
THAT
WE
CAN
COMPUTE
THE
MEAN
BY
SOLVING
THE
FOLLOWING
SYSTEM
OF
LINEAR
EQUATIONS
THIS
IS
EFFICIENT
SINCE
IS
TRIDIAGONAL
FIGURE
GIVES
AN
ILLUSTRATION
OF
THESE
EQUATIONS
WE
SEE
THAT
THE
POSTERIOR
MEAN
EQUALS
THE
OBSERVED
DATA
AT
THE
SPECIFIED
POINTS
AND
SMOOTHLY
INTERPOLATES
IN
BETWEEN
AS
DESIRED
IIT
IS
ALSO
INTERESTING
TO
PLOT
THE
POINTWISE
MARGINAL
CREDIBILITY
INTERVALS
ΜJ
DATA
WE
ALSO
SEE
THAT
THE
VARIANCE
GOES
UP
AS
WE
DECREASE
THE
PRECISION
OF
THE
PRIOR
Λ
IN
TERESTINGLY
Λ
HAS
NO
EFFECT
ON
THE
POSTERIOR
MEAN
SINCE
IT
CANCELS
OUT
WHEN
MULTIPLYING
AND
BY
CONTRAST
WHEN
WE
CONSIDER
NOISY
DATA
IN
SECTION
WE
WILL
SEE
THAT
THE
PRIOR
PRECISION
AFFECTS
THE
SMOOTHNESS
OF
POSTERIOR
MEAN
ESTIMATE
THE
MARGINAL
CREDIBILITY
INTERVALS
DO
NOT
CAPTURE
THE
FACT
THAT
NEIGHBORING
LOCATIONS
ARE
CORRELATED
WE
CAN
REPRESENT
THAT
BY
DRAWING
COMPLETE
FUNCTIONS
I
E
VECTORS
X
FROM
THE
POSTERIOR
AND
PLOTTING
THEM
THESE
ARE
SHOWN
BY
THE
THIN
LINES
IN
FIGURE
THESE
ARE
NOT
QUITE
AS
SMOOTH
AS
THE
POSTERIOR
MEAN
ITSELF
THIS
IS
BECAUSE
THE
PRIOR
ONLY
PENALIZES
FIRST
ORDER
DIFFERENCES
SEE
SECTION
FOR
FURTHER
DISCUSSION
OF
THIS
POINT
DATA
IMPUTATION
SUPPOSE
WE
ARE
MISSING
SOME
ENTRIES
IN
A
DESIGN
MATRIX
IF
THE
COLUMNS
ARE
CORRELATED
WE
CAN
USE
THE
OBSERVED
ENTRIES
TO
PREDICT
THE
MISSING
ENTRIES
FIGURE
SHOWS
A
SIMPLE
EXAMPLE
WE
SAMPLED
SOME
DATA
FROM
A
DIMENSIONAL
GAUSSIAN
AND
THEN
DELIBERATELY
HID
OF
THE
DATA
IN
EACH
ROW
WE
THEN
INFERRED
THE
MISSING
ENTRIES
GIVEN
THE
OBSERVED
ENTRIES
USING
THE
TRUE
GENERATING
MODEL
MORE
PRECISELY
FOR
EACH
ROW
I
WE
COMPUTE
P
XHI
XVI
Θ
WHERE
HI
AND
VI
ARE
THE
INDICES
OF
THE
HIDDEN
AND
VISIBLE
ENTRIES
IN
CASE
I
FROM
THIS
WE
COMPUTE
THE
MARGINAL
DISTRIBUTION
OF
EACH
MISSING
VARIABLE
P
XHIJ
XVI
Θ
WE
THEN
PLOT
THE
MEAN
OF
THIS
DISTRIBUTION
XˆIJ
E
XJ
XVI
Θ
THIS
REPRESENTS
OUR
BEST
GUESS
ABOUT
THE
TRUE
VALUE
OF
THAT
ENTRY
IN
THE
OBSERVED
IMPUTED
TRUTH
FIGURE
ILLUSTRATION
OF
DATA
IMPUTATION
LEFT
COLUMN
VISUALIZATION
OF
THREE
ROWS
OF
THE
DATA
MATRIX
WITH
MISSING
ENTRIES
MIDDLE
COLUMN
MEAN
OF
THE
POSTERIOR
PREDICTIVE
BASED
ON
PARTIALLY
OBSERVED
DATA
IN
THAT
ROW
BUT
THE
TRUE
MODEL
PARAMETERS
RIGHT
COLUMN
TRUE
VALUES
FIGURE
GENERATED
BY
GAUSSIMPUTATIONDEMO
SENSE
THAT
IT
MINIMIZES
OUR
EXPECTED
SQUARED
ERROR
SEE
SECTION
FOR
DETAILS
FIGURE
SHOWS
THAT
THE
ESTIMATES
ARE
QUITE
CLOSE
TO
THE
TRUTH
OF
COURSE
IF
J
VI
THE
EXPECTED
VALUE
IS
EQUAL
TO
THE
OBSERVED
VALUE
XˆIJ
XIJ
WE
CAN
USE
VAR
XHIJ
XVI
Θ
AS
A
MEASURE
OF
CONFIDENCE
IN
THIS
GUESS
ALTHOUGH
THIS
IS
NOT
SHOWN
ALTERNATIVELY
WE
COULD
DRAW
MULTIPLE
SAMPLES
FROM
P
XHI
XVI
Θ
THIS
IS
CALLED
MULTIPLE
IMPUTATION
IN
ADDITION
TO
IMPUTING
THE
MISSING
ENTRIES
WE
MAY
BE
INTERESTED
IN
COMPUTING
THE
LIKE
LIHOOD
OF
EACH
PARTIALLY
OBSERVED
ROW
IN
THE
TABLE
P
XVI
Θ
WHICH
CAN
BE
COMPUTED
USING
EQUATION
THIS
IS
USEFUL
FOR
DETECTING
OUTLIERS
ATYPICAL
OBSERVATIONS
INFORMATION
FORM
SUPPOSE
X
Μ
Σ
ONE
CAN
SHOW
THAT
E
X
Μ
IS
THE
MEAN
VECTOR
AND
COV
X
Σ
IS
THE
COVARIANCE
MATRIX
THESE
ARE
CALLED
THE
MOMENT
PARAMETERS
OF
THE
DISTRIBUTION
HOWEVER
IT
IS
SOMETIMES
USEFUL
TO
USE
THE
CANONICAL
PARAMETERS
OR
NATURAL
PARAMETERS
DEFINED
AS
Λ
Σ
Ξ
Σ
WE
CAN
CONVERT
BACK
TO
THE
MOMENT
PARAMETERS
USING
Μ
Λ
Σ
Λ
USING
THE
CANONICAL
PARAMETERS
WE
CAN
WRITE
THE
MVN
IN
INFORMATION
FORM
I
E
IN
EXPONENTIAL
FAMILY
FORM
DEFINED
IN
SECTION
NC
X
Ξ
Λ
D
Λ
EXP
XT
ΛX
ΞT
Λ
Ξ
L
WHERE
WE
USE
THE
NOTATION
C
TO
DISTINGUISH
FROM
THE
MOMENT
PARAMETERIZATION
IT
IS
ALSO
POSSIBLE
TO
DERIVE
THE
MARGINALIZATION
AND
CONDITIONING
FORMULAS
IN
INFORMATION
FORM
WE
FIND
P
NC
P
NC
THUS
WE
SEE
THAT
MARGINALIZATION
IS
EASIER
IN
MOMENT
FORM
AND
CONDITIONING
IS
EASIER
IN
INFORMATION
FORM
ANOTHER
OPERATION
THAT
IS
SIGNIFICANTLY
EASIER
IN
INFORMATION
FORM
IS
MULTIPLYING
TWO
GAUSSIANS
ONE
CAN
SHOW
THAT
NC
ΞF
ΛF
NC
ΞG
ΛG
NC
ΞF
ΞG
ΛF
ΛG
HOWEVER
IN
MOMENT
FORM
THINGS
ARE
MUCH
MESSIER
PROOF
OF
THE
RESULT
WE
NOW
PROVE
THEOREM
READERS
WHO
ARE
INTIMIDATED
BY
HEAVY
MATRIX
ALGEBRA
CAN
SAFELY
SKIP
THIS
SECTION
WE
FIRST
DERIVE
SOME
RESULTS
THAT
WE
WILL
NEED
HERE
AND
ELSEWHERE
IN
THE
BOOK
WE
WILL
RETURN
TO
THE
PROOF
AT
THE
END
INVERSE
OF
A
PARTITIONED
MATRIX
USING
SCHUR
COMPLEMENTS
THE
KEY
TOOL
WE
NEED
IS
A
WAY
TO
INVERT
A
PARTITIONED
MATRIX
THIS
CAN
BE
DONE
USING
THE
FOLLOWING
RESULT
THEOREM
INVERSE
OF
A
PARTITIONED
MATRIX
CONSIDER
A
GENERAL
PARTITIONED
MATRIX
M
E
F
G
H
WHERE
WE
ASSUME
E
AND
H
ARE
INVERTIBLE
WE
HAVE
M
WHERE
M
H
M
H
H
M
H
H
H
M
H
E
E
M
E
E
M
E
M
E
M
E
M
H
E
FH
M
E
H
GE
WE
SAY
THAT
M
H
IS
THE
SCHUR
COMPLEMENT
OF
M
WRT
H
EQUATION
IS
CALLED
THE
PARTITIONED
INVERSE
FORMULA
PROOF
IF
WE
COULD
BLOCK
DIAGONALIZE
M
IT
WOULD
BE
EASIER
TO
INVERT
TO
ZERO
OUT
THE
TOP
RIGHT
BLOCK
OF
M
WE
CAN
PRE
MULTIPLY
AS
FOLLOWS
I
FH
E
F
E
FH
SIMILARLY
TO
ZERO
OUT
THE
BOTTOM
LEFT
WE
CAN
POST
MULTIPLY
AS
FOLLOWS
E
FH
I
E
FH
G
H
H
I
H
PUTTING
IT
ALL
TOGETHER
WE
GET
I
FH
E
F
I
E
FH
X
M
Z
TAKING
THE
INVERSE
OF
BOTH
SIDES
YIELDS
W
Z
W
AND
HENCE
M
ZW
SUBSTITUTING
IN
THE
DEFINITIONS
WE
GET
E
F
I
M
H
I
FH
M
H
H
M
H
H
I
FH
M
H
M
H
H
M
H
H
H
M
H
ALTERNATIVELY
WE
COULD
HAVE
DECOMPOSED
THE
MATRIX
M
IN
TERMS
OF
E
AND
M
E
H
GE
YIELDING
E
F
G
H
E
E
M
E
E
M
E
M
E
M
E
THE
MATRIX
INVERSION
LEMMA
WE
NOW
DERIVE
SOME
USEFUL
COROLLARIES
OF
THE
ABOVE
RESULT
COROLLARY
MATRIX
INVERSION
LEMMA
CONSIDER
A
GENERAL
PARTITIONED
MATRIX
M
E
F
G
H
WHERE
WE
ASSUME
E
AND
H
ARE
INVERTIBLE
WE
HAVE
E
FH
E
E
H
GE
E
FH
E
H
GE
E
FH
H
GE
H
E
THE
FIRST
TWO
EQUATIONS
ARE
KNOWN
AS
THE
MATRIX
INVERSION
LEMMA
OR
THE
SHERMAN
MORRISON
WOODBURY
FORMULA
THE
THIRD
EQUATION
IS
KNOWN
AS
THE
MATRIX
DETERMINANT
LEMMA
A
TYPICAL
APPLICATION
IN
MACHINE
LEARNING
STATISTICS
IS
THE
FOLLOWING
LET
E
Σ
BE
A
N
N
DIAGONAL
MATRIX
LET
F
GT
X
OF
SIZE
N
D
WHERE
N
D
AND
LET
H
I
THEN
WE
HAVE
Σ
XXT
Σ
Σ
I
XT
Σ
Σ
THE
LHS
TAKES
O
N
TIME
TO
COMPUTE
THE
RHS
TAKES
TIME
O
TO
COMPUTE
ANOTHER
APPLICATION
CONCERNS
COMPUTING
A
RANK
ONE
UPDATE
OF
AN
INVERSE
MATRIX
LET
H
A
SCALAR
F
U
A
COLUMN
VECTOR
AND
G
VT
A
ROW
VECTOR
THEN
WE
HAVE
E
UVT
E
E
VT
E
E
E
E
E
VT
E
THIS
IS
USEFUL
WHEN
WE
INCREMENTALLY
ADD
A
DATA
VECTOR
TO
A
DESIGN
MATRIX
AND
WANT
TO
UPDATE
OUR
SUFFICIENT
STATISTICS
ONE
CAN
DERIVE
AN
ANALOGOUS
FORMULA
FOR
REMOVING
A
DATA
VECTOR
PROOF
TO
PROVE
EQUATION
WE
SIMPLY
EQUATE
THE
TOP
LEFT
BLOCK
OF
EQUATION
AND
EQUA
TION
TO
PROVE
EQUATION
WE
SIMPLE
EQUATE
THE
TOP
RIGHT
BLOCKS
OF
EQUATIONS
AND
THE
PROOF
OF
EQUATION
IS
LEFT
AS
AN
EXERCISE
PROOF
OF
GAUSSIAN
CONDITIONING
FORMULAS
WE
CAN
NOW
RETURN
TO
OUR
ORIGINAL
GOAL
WHICH
IS
TO
DERIVE
EQUATION
LET
US
FACTOR
THE
JOINT
P
AS
P
P
AS
FOLLOWS
E
EXP
Μ
T
Σ
X
USING
EQUATION
THE
ABOVE
EXPONENT
BECOMES
X
Μ
T
I
Σ
Σ
I
I
EXP
X
Μ
Σ
Σ
X
Μ
Σ
Σ
X
Μ
Σ
Σ
X
Μ
EXP
X
Μ
T
Σ
X
Μ
THIS
IS
OF
THE
FORM
EXP
QUADRATIC
FORM
IN
EXP
QUADRATIC
FORM
IN
HENCE
WE
HAVE
SUCCESSFULLY
FACTORIZED
THE
JOINT
AS
P
P
P
N
N
WHERE
THE
PARAMETERS
OF
THE
CONDITIONAL
DISTRIBUTION
CAN
BE
READ
OFF
FROM
THE
ABOVE
EQUATIONS
USING
Σ
WE
CAN
ALSO
USE
THE
FACT
THAT
M
M
H
H
TO
CHECK
THE
NORMALIZATION
CONSTANTS
ARE
CORRECT
Σ
Σ
Σ
Σ
WHERE
DIM
AND
DIM
Σ
WE
LEAVE
THE
PROOF
OF
THE
OTHER
FORMS
OF
THE
RESULT
IN
EQUATION
AS
AN
EXERCISE
LINEAR
GAUSSIAN
SYSTEMS
SUPPOSE
WE
HAVE
TWO
VARIABLES
X
AND
Y
LET
X
RDX
BE
A
HIDDEN
VARIABLE
AND
Y
RDY
BE
A
NOISY
OBSERVATION
OF
X
LET
US
ASSUME
WE
HAVE
THE
FOLLOWING
PRIOR
AND
LIKELIHOOD
WHERE
A
IS
A
MATRIX
OF
SIZE
DY
DX
THIS
IS
AN
EXAMPLE
OF
A
LINEAR
GAUSSIAN
SYSTEM
WE
CAN
REPRESENT
THIS
SCHEMATICALLY
AS
X
Y
MEANING
X
GENERATES
Y
IN
THIS
SECTION
WE
SHOW
HOW
TO
INVERT
THE
ARROW
THAT
IS
HOW
TO
INFER
X
FROM
Y
WE
STATE
THE
RESULT
BELOW
THEN
GIVE
SEVERAL
EXAMPLES
AND
FINALLY
WE
DERIVE
THE
RESULT
WE
WILL
SEE
MANY
MORE
APPLICATIONS
OF
THESE
RESULTS
IN
LATER
CHAPTERS
STATEMENT
OF
THE
RESULT
THEOREM
BAYES
RULE
FOR
LINEAR
GAUSSIAN
SYSTEMS
GIVEN
A
LINEAR
GAUSSIAN
SYSTEM
AS
IN
EQUATION
THE
POSTERIOR
P
X
Y
IS
GIVEN
BY
THE
FOLLOWING
IN
ADDITION
THE
NORMALIZATION
CONSTANT
P
Y
IS
GIVEN
BY
FOR
THE
PROOF
SEE
SECTION
EXAMPLES
IN
THIS
SECTION
WE
GIVE
SOME
EXAMPLE
APPLICATIONS
OF
THE
ABOVE
RESULT
INFERRING
AN
UNKNOWN
SCALAR
FROM
NOISY
MEASUREMENTS
SUPPOSE
WE
MAKE
N
NOISY
MEASUREMENTS
YI
OF
SOME
UNDERLYING
QUANTITY
X
LET
US
ASSUME
THE
MEASUREMENT
NOISE
HAS
FIXED
PRECISION
ΛY
SO
THE
LIKELIHOOD
IS
P
YI
X
N
YI
X
Λ
Y
NOW
LET
US
USE
A
GAUSSIAN
PRIOR
FOR
THE
VALUE
OF
THE
UNKNOWN
SOURCE
P
X
N
X
Λ
WE
WANT
TO
COMPUTE
P
X
YN
WE
CAN
CONVERT
THIS
TO
A
FORM
THAT
LETS
US
APPLY
BAYES
RULE
FOR
GAUSSIANS
BY
DEFINING
Y
YN
A
AN
N
ROW
VECTOR
OF
AND
Σ
Y
DIAG
ΛYI
THEN
WE
GET
P
X
Y
N
X
ΜN
Λ
ΛN
NΛY
NΛYY
N
ΛY
Μ
Y
Μ
N
ΛN
NΛY
NΛY
THESE
EQUATIONS
ARE
QUITE
INTUITIVE
THE
POSTERIOR
PRECISION
ΛN
IS
THE
PRIOR
PRECISION
PLUS
N
UNITS
OF
MEASUREMENT
PRECISION
ΛY
ALSO
THE
POSTERIOR
MEAN
ΜN
IS
A
CONVEX
COMBINATION
OF
THE
MLE
Y
AND
THE
PRIOR
MEAN
THIS
MAKES
IT
CLEAR
THAT
THE
POSTERIOR
MEAN
IS
A
COMPROMISE
BETWEEN
THE
MLE
AND
THE
PRIOR
IF
THE
PRIOR
IS
WEAK
RELATIVE
TO
THE
SIGNAL
STRENGTH
IS
SMALL
RELATIVE
TO
ΛY
WE
PUT
MORE
WEIGHT
ON
THE
MLE
IF
THE
PRIOR
IS
STRONG
RELATIVE
TO
THE
SIGNAL
STRENGTH
IS
LARGE
RELATIVE
TO
ΛY
WE
PUT
MORE
WEIGHT
ON
THE
PRIOR
THIS
IS
ILLUSTRATED
IN
FIGURE
WHICH
IS
VERY
SIMILAR
TO
THE
ANALOGOUS
RESULTS
FOR
THE
BETA
BINOMIAL
MODEL
IN
FIGURE
NOTE
THAT
THE
POSTERIOR
MEAN
IS
WRITTEN
IN
TERMS
OF
N
ΛYY
SO
HAVING
N
MEASUREMENTS
EACH
OF
PRECISION
ΛY
IS
LIKE
HAVING
ONE
MEASUREMENT
WITH
VALUE
Y
AND
PRECISION
NΛY
WE
CAN
REWRITE
THE
RESULTS
IN
TERMS
OF
THE
POSTERIOR
VARIANCE
RATHER
THAN
POSTERIOR
PRECISION
PRIOR
VARIANCE
PRIOR
VARIANCE
FIGURE
INFERENCE
ABOUT
X
GIVEN
A
NOISY
OBSERVATION
Y
A
STRONG
PRIOR
N
THE
POSTERIOR
MEAN
IS
SHRUNK
TOWARDS
THE
PRIOR
MEAN
WHICH
IS
A
WEAK
PRIOR
N
THE
POSTERIOR
MEAN
IS
SIMILAR
TO
THE
MLE
FIGURE
GENERATED
BY
AS
FOLLOWS
P
X
D
N
X
ΜN
Τ
N
N
Τ
NΤ
NY
NΤ
WHERE
Τ
IS
THE
PRIOR
VARIANCE
AND
Τ
ΛN
IS
THE
POSTERIOR
VARIANCE
N
WE
CAN
ALSO
COMPUTE
THE
POSTERIOR
SEQUENTIALLY
BY
UPDATING
AFTER
EACH
OBSERVATION
IF
N
WE
CAN
REWRITE
THE
POSTERIOR
AFTER
SEEING
A
SINGLE
OBSERVATION
AS
FOLLOWS
WHERE
WE
DEFINE
ΣY
Τ
AND
Τ
TO
BE
THE
VARIANCES
OF
THE
LIKELIHOOD
PRIOR
AND
POSTERIOR
P
X
Y
N
X
ΣY
ΣY
Μ
Σ
Y
WE
CAN
REWRITE
THE
POSTERIOR
MEAN
IN
DIFFERENT
WAYS
ΣY
ΣY
Y
ΣY
Μ
Y
Μ
ΣY
ΣY
Y
Y
Μ
ΣY
THE
FIRST
EQUATION
IS
A
CONVEX
COMBINATION
OF
THE
PRIOR
AND
THE
DATA
THE
SECOND
EQUATION
IS
THE
PRIOR
MEAN
ADJUSTED
TOWARDS
THE
DATA
THE
THIRD
EQUATION
IS
THE
DATA
ADJUSTED
TOWARDS
THE
PRIOR
MEAN
THIS
IS
CALLED
SHRINKAGE
THESE
ARE
ALL
EQUIVALENT
WAYS
OF
EXPRESSING
THE
TRADEOFF
BETWEEN
LIKELIHOOD
AND
PRIOR
IF
IS
SMALL
RELATIVE
TO
ΣY
CORRESPONDING
TO
A
STRONG
PRIOR
THE
AMOUNT
OF
SHRINKAGE
IS
LARGE
SEE
FIGURE
A
WHEREAS
IF
IS
LARGE
RELATIVE
TO
ΣY
CORRESPONDING
TO
A
WEAK
PRIOR
THE
AMOUNT
OF
SHRINKAGE
IS
SMALL
SEE
FIGURE
B
ANOTHER
WAY
TO
QUANTIFY
THE
AMOUNT
OF
SHRINKAGE
IS
IN
TERMS
OF
THE
SIGNAL
TO
NOISE
RATIO
WHICH
IS
DEFINED
AS
FOLLOWS
SNR
E
E
ΣY
WHERE
X
IS
THE
TRUE
SIGNAL
Y
X
E
IS
THE
OBSERVED
SIGNAL
AND
E
ΣY
IS
THE
NOISE
TERM
INFERRING
AN
UNKNOWN
VECTOR
FROM
NOISY
MEASUREMENTS
NOW
CONSIDER
N
VECTOR
VALUED
OBSERVATIONS
YI
N
X
ΣY
AND
A
GAUSSIAN
PRIOR
X
SETTING
A
I
B
AND
USING
Y
FOR
THE
EFFECTIVE
OBSERVATION
WITH
PRECISION
N
ΣY
WE
HAVE
P
X
YN
N
X
ΜN
ΣN
Σ
Σ
N
Σ
Y
ΜN
ΣN
Σ
Y
N
Y
Σ
SEE
FIGURE
FOR
A
EXAMPLE
WE
CAN
THINK
OF
X
AS
REPRESENTING
THE
TRUE
BUT
UNKNOWN
LOCATION
OF
AN
OBJECT
IN
SPACE
SUCH
AS
A
MISSILE
OR
AIRPLANE
AND
THE
YI
AS
BEING
NOISY
OBSERVATIONS
SUCH
AS
RADAR
BLIPS
AS
WE
RECEIVE
MORE
BLIPS
WE
ARE
BETTER
ABLE
TO
LOCALIZE
THE
SOURCE
IN
SECTION
WE
WILL
SEE
HOW
TO
EXTEND
THIS
EXAMPLE
TO
TRACK
MOVING
OBJECTS
USING
THE
FAMOUS
KALMAN
FILTER
ALGORITHM
NOW
SUPPOSE
WE
HAVE
MULTIPLE
MEASURING
DEVICES
AND
WE
WANT
TO
COMBINE
THEM
TOGETHER
THIS
IS
KNOWN
AS
SENSOR
FUSION
IF
WE
HAVE
MULTIPLE
OBSERVATIONS
WITH
DIFFERENT
COVARIANCES
COR
RESPONDING
TO
SENSORS
WITH
DIFFERENT
RELIABILITIES
THE
POSTERIOR
WILL
BE
AN
APPROPRIATE
WEIGHTED
AVERAGE
OF
THE
DATA
CONSIDER
THE
EXAMPLE
IN
FIGURE
WE
USE
AN
UNINFORMATIVE
PRIOR
ON
X
NAMELY
P
X
WE
GET
NOISY
OBSERVATIONS
X
ΣY
AND
X
ΣY
WE
THEN
COMPUTE
P
X
IN
FIGURE
A
WE
SET
ΣY
ΣY
SO
BOTH
SENSORS
ARE
EQUALLY
RELIABLE
IN
THIS
CASE
THE
POSTERIOR
MEAN
IS
HALF
WAY
BETWEEN
THE
TWO
OBSERVATIONS
AND
IN
FIGURE
B
WE
SET
ΣY
AND
ΣY
SO
SENSOR
IS
MORE
RELIABLE
THAN
SENSOR
IN
THIS
CASE
THE
POSTERIOR
MEAN
IS
CLOSER
TO
IN
FIGURE
C
WE
SET
ΣY
ΣY
SO
SENSOR
IS
MORE
RELIABLE
IN
THE
COMPONENT
VERTICAL
DIRECTION
AND
SENSOR
IS
MORE
RELIABLE
IN
THE
COMPONENT
HORIZONTAL
DIRECTION
IN
THIS
CASE
THE
POSTERIOR
MEAN
USES
VERTICAL
COMPONENT
AND
HORIZONTAL
COMPONENT
DATA
PRIOR
POST
AFTER
OBS
FIGURE
ILLUSTRATION
OF
BAYESIAN
INFERENCE
FOR
THE
MEAN
OF
A
GAUSSIAN
A
THE
DATA
IS
GENERATED
FROM
YI
X
ΣY
WHERE
X
T
AND
ΣY
WE
ASSUME
THE
SENSOR
NOISE
COVARIANCE
ΣY
IS
KNOWN
BUT
X
IS
UNKNOWN
THE
BLACK
CROSS
REPRESENTS
X
B
THE
PRIOR
IS
P
X
N
X
C
WE
SHOW
THE
POSTERIOR
AFTER
DATA
POINTS
HAVE
BEEN
OBSERVED
FIGURE
GENERATED
BY
A
B
C
FIGURE
WE
OBSERVE
RED
CROSS
AND
GREEN
CROSS
AND
INFER
E
Μ
Θ
BLACK
CROSS
A
EQUALLY
RELIABLE
SENSORS
SO
THE
POSTERIOR
MEAN
ESTIMATE
IS
IN
BETWEEN
THE
TWO
CIRCLES
B
SENSOR
IS
MORE
RELIABLE
SO
THE
ESTIMATE
SHIFTS
MORE
TOWARDS
THE
GREEN
CIRCLE
C
SENSOR
IS
MORE
RELIABLE
IN
THE
VERTICAL
DIRECTION
SENSOR
IS
MORE
RELIABLE
IN
THE
HORIZONTAL
DIRECTION
THE
ESTIMATE
IS
AN
APPROPRIATE
COMBINATION
OF
THE
TWO
MEASUREMENTS
FIGURE
GENERATED
BY
NOTE
THAT
THIS
TECHNIQUE
CRUCIALLY
RELIES
ON
MODELING
OUR
UNCERTAINTY
OF
EACH
SENSOR
COMPUT
ING
AN
UNWEIGHTED
AVERAGE
WOULD
GIVE
THE
WRONG
RESULT
HOWEVER
WE
HAVE
ASSUMED
THE
SENSOR
PRECISIONS
ARE
KNOWN
WHEN
THEY
ARE
NOT
WE
SHOULD
MODEL
OUT
UNCERTAINTY
ABOUT
AND
AS
WELL
SEE
SECTION
FOR
DETAILS
INTERPOLATING
NOISY
DATA
WE
NOW
REVISIT
THE
EXAMPLE
OF
SECTION
THIS
TIME
WE
NO
LONGER
ASSUME
NOISE
FREE
OBSERVATIONS
INSTEAD
LET
US
ASSUME
THAT
WE
OBTAIN
N
NOISY
OBSERVATIONS
YI
WITHOUT
LOSS
OF
GENERALITY
ASSUME
THESE
CORRESPOND
TO
XN
WE
CAN
MODEL
THIS
SETUP
AS
A
LINEAR
GAUSSIAN
SYSTEM
Y
AX
E
WHERE
E
ΣY
ΣY
IS
THE
OBSERVATION
NOISE
AND
A
IS
A
N
D
PROJECTION
MATRIX
THAT
SELECTS
OUT
THE
OBSERVED
ELEMENTS
FOR
EXAMPLE
IF
N
AND
D
WE
HAVE
A
USING
THE
SAME
IMPROPER
PRIOR
AS
BEFORE
ΣX
LT
L
WE
CAN
EASILY
COMPUTE
THE
POSTERIOR
MEAN
AND
VARIANCE
IN
FIGURE
WE
PLOT
THE
POSTERIOR
MEAN
POSTERIOR
VARIANCE
AND
SOME
POSTERIOR
SAMPLES
NOW
WE
SEE
THAT
THE
PRIOR
PRECISION
Λ
EFFECTS
THE
POSTERIOR
MEAN
AS
WELL
AS
THE
POSTERIOR
VARIANCE
IN
PARTICULAR
FOR
A
STRONG
PRIOR
LARGE
Λ
THE
ESTIMATE
IS
VERY
SMOOTH
AND
THE
UNCERTAINTY
IS
LOW
BUT
FOR
A
WEAK
PRIOR
SMALL
Λ
THE
ESTIMATE
IS
WIGGLY
AND
THE
UNCERTAINTY
AWAY
FROM
THE
DATA
IS
HIGH
THE
POSTERIOR
MEAN
CAN
ALSO
BE
COMPUTED
BY
SOLVING
THE
FOLLOWING
OPTIMIZATION
PROBLEM
MIN
X
N
I
XI
YI
D
J
I
XJ
XJ
XJ
XJ
WHERE
WE
HAVE
DEFINED
AND
XD
XD
FOR
NOTATIONAL
SIMPLICITY
WE
RECOGNIZE
THIS
AS
A
DISCRETE
APPROXIMATION
TO
THE
FOLLOWING
PROBLEM
MIN
R
F
T
Y
T
Λ
R
F
T
WHERE
F
T
IS
THE
FIRST
DERIVATIVE
OF
F
THE
FIRST
TERM
MEASURES
FIT
TO
THE
DATA
AND
THE
SECOND
TERM
PENALIZES
FUNCTIONS
THAT
ARE
TOO
WIGGLY
THIS
IS
AN
EXAMPLE
OF
TIKHONOV
REGULARIZATION
WHICH
IS
A
POPULAR
APPROACH
TO
FUNCTIONAL
DATA
ANALYSIS
SEE
CHAPTER
FOR
MORE
SOPHISTICATED
APPROACHES
WHICH
ENFORCE
HIGHER
ORDER
SMOOTHNESS
SO
THE
RESULTING
SAMPLES
LOOK
LESS
JAGGED
PROOF
OF
THE
RESULT
WE
NOW
DERIVE
EQUATION
THE
BASIC
IDEA
IS
TO
DERIVE
THE
JOINT
DISTRIBUTION
P
X
Y
P
X
P
Y
X
AND
THEN
TO
USE
THE
RESULTS
FROM
SECTION
FOR
COMPUTING
P
X
Y
IN
MORE
DETAIL
WE
PROCEED
AS
FOLLOWS
THE
LOG
OF
THE
JOINT
DISTRIBUTION
IS
AS
FOLLOWS
DROPPING
IRRELEVANT
CONSTANTS
LOG
P
X
Y
X
Μ
T
Σ
X
Μ
Y
AX
B
T
Σ
Y
AX
B
THIS
IS
CLEARLY
A
JOINT
GAUSSIAN
DISTRIBUTION
SINCE
IT
IS
THE
EXPONENTIAL
OF
A
QUADRATIC
FORM
EXPANDING
OUT
THE
QUADRATIC
TERMS
INVOLVING
X
AND
Y
AND
IGNORING
LINEAR
AND
CONSTANT
TERMS
WE
HAVE
Q
XT
Σ
YT
Σ
AX
T
Σ
AX
YT
Σ
X
T
Σ
AT
Σ
AT
Σ
X
Y
ΣY
A
ΣY
Y
X
T
Σ
X
A
B
FIGURE
INTERPOLATING
NOISY
DATA
NOISE
VARIANCE
USING
A
GAUSSIAN
WITH
PRIOR
PRECISION
Λ
A
Λ
B
Λ
SEE
ALSO
FIGURE
BASED
ON
FIGURE
OF
CALVETTI
AND
SOMERSALO
FIGURE
GENERATED
BY
GAUSSINTERPNOISYDEMO
SEE
ALSO
SPLINEBASISDEMO
WHERE
THE
PRECISION
MATRIX
OF
THE
JOINT
IS
DEFINED
AS
Σ
Σ
X
AT
Σ
Y
AT
Σ
Y
Σ
Σ
Λ
ΛXX
ΛXY
ΛYX
ΛYY
FROM
EQUATION
AND
USING
THE
FACT
THAT
ΜY
AΜX
B
WE
HAVE
XX
X
Y
ΣX
Y
ΛXXΜX
ΛXY
Y
ΜY
ΣX
Y
Σ
X
AT
Σ
Y
Y
B
DIGRESSION
THE
WISHART
DISTRIBUTION
THE
WISHART
DISTRIBUTION
IS
THE
GENERALIZATION
OF
THE
GAMMA
DISTRIBUTION
TO
POSITIVE
DEFINITE
MATRICES
PRESS
PRESS
HAS
SAID
THE
WISHART
DISTRIBUTION
RANKS
NEXT
TO
THE
MULTI
VARIATE
NORMAL
DISTRIBUTION
IN
ORDER
OF
IMPORTANCE
AND
USEFULENESS
IN
MULTIVARIATE
STATISTICS
WE
WILL
MOSTLY
USE
IT
TO
MODEL
OUR
UNCERTAINTY
IN
COVARIANCE
MATRICES
Σ
OR
THEIR
INVERSES
Λ
Σ
THE
PDF
OF
THE
WISHART
IS
DEFINED
AS
FOLLOWS
WI
Λ
Ν
Λ
Ν
D
EXP
TR
ΛS
HERE
Ν
IS
CALLED
THE
DEGREES
OF
FREEDOM
AND
IS
THE
SCALE
MATRIX
WE
SHALL
GET
MORE
INTUITION
FOR
THESE
PARAMETERS
SHORTLY
THE
NORMALIZATION
CONSTANT
FOR
THIS
DISTRIBUTION
WHICH
REQUIRES
INTEGRATING
OVER
ALL
SYMMETRIC
PD
MATRICES
IS
THE
FOLLOWING
FORMIDABLE
EXPRESSION
ZWI
Ν
Ν
WHERE
ΓD
A
IS
THE
MULTIVARIATE
GAMMA
FUNCTION
D
ΓD
X
ΠD
D
Γ
X
I
I
HENCE
A
Γ
A
AND
Γ
Ν
TT
Γ
I
THE
NORMALIZATION
CONSTANT
ONLY
EXISTS
AND
HENCE
THE
PDF
IS
ONLY
WELL
DEFINED
IF
Ν
D
THERE
IS
A
CONNECTION
BETWEEN
THE
WISHART
DISTRIBUTION
AND
THE
GAUSSIAN
IN
PARTICULAR
LET
XI
N
Σ
THEN
THE
SCATTER
MATRIX
N
XIXT
HAS
A
WISHART
DISTRIBUTION
WI
Σ
HENCE
E
N
Σ
MORE
GENERALLY
ONE
CAN
SHOW
THAT
THE
MEAN
AND
MODE
OF
WI
Ν
ARE
GIVEN
BY
MEAN
ΝS
MODE
Ν
D
WHERE
THE
MODE
ONLY
EXISTS
IF
Ν
D
IF
D
THE
WISHART
REDUCES
TO
THE
GAMMA
DISTRIBUTION
WI
Λ
Ν
GA
Λ
Ν
INVERSE
WISHART
DISTRIBUTION
RECALL
THAT
WE
SHOWED
EXERCISE
THAT
IF
Λ
GA
A
B
THEN
THAT
IG
A
B
SIMILARLY
IF
Σ
WI
Ν
THEN
Σ
IW
Ν
D
WHERE
IW
IS
THE
INVERSE
WISHART
THE
MULTIDIMENSIONAL
GENERALIZATION
OF
THE
INVERSE
GAMMA
IT
IS
DEFINED
AS
FOLLOWS
FOR
Ν
D
AND
IW
Σ
Ν
Σ
Ν
D
EXP
TR
ZIW
Ν
Ν
ONE
CAN
SHOW
THAT
THE
DISTRIBUTION
HAS
THESE
PROPERTIES
MEAN
Ν
D
MODE
Ν
D
IF
D
THIS
REDUCES
TO
THE
INVERSE
GAMMA
IW
Ν
IG
Ν
WI
DOF
E
A
B
FIGURE
VISUALIZATION
OF
THE
WISHART
DISTRIBUTION
LEFT
SOME
SAMPLES
FROM
THE
WISHART
DISTRIBUTION
Σ
WI
Ν
WHERE
AND
Ν
RIGHT
PLOTS
OF
THE
MARGINALS
WHICH
ARE
GAMMA
AND
THE
APPROXIMATE
SAMPLE
BASED
MARGINAL
ON
THE
CORRELATION
COEFFICIENT
IF
Ν
THERE
IS
A
LOT
OF
UNCERTAINTY
ABOUT
THE
VALUE
OF
THE
CORRELATION
COEFFICIENT
Ρ
SEE
THE
ALMOST
UNIFORM
DISTRIBUTION
ON
THE
SAMPLED
MATRICES
ARE
HIGHLY
VARIABLE
AND
SOME
ARE
NEARLY
SINGULAR
AS
Ν
INCREASES
THE
SAMPLED
MATRICES
ARE
MORE
CONCENTRATED
ON
THE
PRIOR
FIGURE
GENERATED
BY
WIPLOTDEMO
VISUALIZING
THE
WISHART
DISTRIBUTION
SINCE
THE
WISHART
IS
A
DISTRIBUTION
OVER
MATRICES
IT
IS
HARD
TO
PLOT
AS
A
DENSITY
FUNCTION
HOWEVER
WE
CAN
EASILY
SAMPLE
FROM
IT
AND
IN
THE
CASE
WE
CAN
USE
THE
EIGENVECTORS
OF
THE
RESULTING
MATRIX
TO
DEFINE
AN
ELLIPSE
AS
EXPLAINED
IN
SECTION
SEE
FIGURE
FOR
SOME
EXAMPLES
FOR
HIGHER
DIMENSIONAL
MATRICES
WE
CAN
PLOT
MARGINALS
OF
THE
DISTRIBUTION
THE
DIAGONALS
OF
A
WISHART
DISTRIBUTED
MATRIX
HAVE
GAMMA
DISTRIBUTIONS
SO
ARE
EASY
TO
PLOT
IT
IS
HARD
IN
GENERAL
TO
WORK
OUT
THE
DISTRIBUTION
OF
THE
OFF
DIAGONAL
ELEMENTS
BUT
WE
CAN
SAMPLE
MATRICES
FROM
THE
DISTRIBUTION
AND
THEN
COMPUTE
THE
DISTRIBUTION
EMPIRICALLY
IN
PARTICULAR
WE
CAN
CONVERT
EACH
SAMPLED
MATRIX
TO
A
CORRELATION
MATRIX
AND
THUS
COMPUTE
A
MONTE
CARLO
APPROXIMATION
SECTION
TO
THE
EXPECTED
CORRELATION
COEFFICIENTS
E
RIJ
R
Σ
IJ
WHERE
Σ
WI
Σ
Ν
AND
R
Σ
CONVERTS
MATRIX
Σ
INTO
A
CORRELATION
MATRIX
ΣIJ
RIJ
II
ΣJJ
WE
CAN
THEN
USE
KERNEL
DENSITY
ESTIMATION
SECTION
TO
PRODUCE
A
SMOOTH
APPROXIMATION
TO
THE
UNIVARIATE
DENSITY
E
RIJ
FOR
PLOTTING
PURPOSES
SEE
FIGURE
FOR
SOME
EXAMPLES
INFERRING
THE
PARAMETERS
OF
AN
MVN
SO
FAR
WE
HAVE
DISCUSSED
INFERENCE
IN
A
GAUSSIAN
ASSUMING
THE
PARAMETERS
Θ
Μ
Σ
ARE
KNOWN
WE
NOW
DISCUSS
HOW
TO
INFER
THE
PARAMETERS
THEMSELVES
WE
WILL
ASSUME
THE
DATA
HAS
THE
FORM
XI
Μ
Σ
FOR
I
N
AND
IS
FULLY
OBSERVED
SO
WE
HAVE
NO
MISSING
DATA
SEE
SECTION
FOR
HOW
TO
ESTIMATE
PARAMETERS
OF
AN
MVN
IN
THE
PRESENCE
OF
MISSING
VALUES
TO
SIMPLIFY
THE
PRESENTATION
WE
DERIVE
THE
POSTERIOR
IN
THREE
PARTS
FIRST
WE
COMPUTE
P
Μ
D
Σ
THEN
WE
COMPUTE
P
Σ
D
Μ
FINALLY
WE
COMPUTE
THE
JOINT
P
Μ
Σ
D
POSTERIOR
DISTRIBUTION
OF
Μ
WE
HAVE
DISCUSSED
HOW
TO
COMPUTE
THE
MLE
FOR
Μ
WE
NOW
DISCUSS
HOW
TO
COMPUTE
ITS
POSTERIOR
WHICH
IS
USEFUL
FOR
MODELING
OUR
UNCERTAINTY
ABOUT
ITS
VALUE
THE
LIKELIHOOD
HAS
THE
FORM
P
D
Μ
N
X
Μ
N
Σ
FOR
SIMPLICITY
WE
WILL
USE
A
CONJUGATE
PRIOR
WHICH
IN
THIS
CASE
IS
A
GAUSSIAN
IN
PARTICULAR
IF
P
Μ
Μ
THEN
WE
CAN
DERIVE
A
GAUSSIAN
POSTERIOR
FOR
Μ
BASED
ON
THE
RESULTS
IN
SECTION
WE
GET
P
Μ
D
Σ
N
Μ
MN
VN
V
V
N
Σ
MN
VN
Σ
N
X
V
THIS
IS
EXACTLY
THE
SAME
PROCESS
AS
INFERRING
THE
LOCATION
OF
AN
OBJECT
BASED
ON
NOISY
RADAR
BLIPS
EXCEPT
NOW
WE
ARE
INFERRING
THE
MEAN
OF
A
DISTRIBUTION
BASED
ON
NOISY
SAMPLES
TO
A
BAYESIAN
THERE
IS
NO
DIFFERENCE
BETWEEN
UNCERTAINTY
ABOUT
PARAMETERS
AND
UNCERTAINTY
ABOUT
ANYTHING
ELSE
WE
CAN
MODEL
AN
UNINFORMATIVE
PRIOR
BY
SETTING
I
IN
THIS
CASE
WE
HAVE
P
Μ
D
Σ
X
N
Σ
SO
THE
POSTERIOR
MEAN
IS
EQUAL
TO
THE
MLE
WE
ALSO
SEE
THAT
THE
POSTERIOR
VARIANCE
GOES
DOWN
AS
N
WHICH
IS
A
STANDARD
RESULT
FROM
FREQUENTIST
STATISTICS
POSTERIOR
DISTRIBUTION
OF
Σ
WE
NOW
DISCUSS
HOW
TO
COMPUTE
P
Σ
D
Μ
THE
LIKELIHOOD
HAS
THE
FORM
P
D
Μ
Σ
Σ
N
EXP
TR
Σ
THE
CORRESPONDING
CONJUGATE
PRIOR
IS
KNOWN
AS
THE
INVERSE
WISHART
DISTRIBUTION
SECTION
RECALL
THAT
THIS
HAS
THE
FOLLOWING
PDF
IW
Σ
Ν
Σ
D
EXP
TR
Σ
HERE
D
IS
THE
DEGREES
OF
FREEDOM
DOF
AND
IS
A
SYMMETRIC
PD
MATRIX
WE
SEE
THAT
PLAYS
THE
ROLE
OF
THE
PRIOR
SCATTER
MATRIX
AND
D
CONTROLS
THE
STRENGTH
OF
THE
PRIOR
AND
HENCE
PLAYS
A
ROLE
ANALOGOUS
TO
THE
SAMPLE
SIZE
N
N
D
N
D
N
D
FIGURE
ESTIMATING
A
COVARIANCE
MATRIX
IN
D
DIMENSIONS
USING
N
SAMPLES
WE
PLOT
THE
EIGENVALUES
IN
DESCENDING
ORDER
FOR
THE
TRUE
COVARIANCE
MATRIX
SOLID
BLACK
THE
MLE
DOTTED
BLUE
AND
THE
MAP
ESTIMATE
DASHED
RED
USING
EQUATION
WITH
Λ
WE
ALSO
LIST
THE
CONDITION
NUMBER
OF
EACH
MATRIX
IN
THE
LEGEND
BASED
ON
FIGURE
OF
SCHAEFER
AND
STRIMMER
FIGURE
GENERATED
BY
SHRINKCOVDEMO
MULTIPLYING
THE
LIKELIHOOD
AND
PRIOR
WE
FIND
THAT
THE
POSTERIOR
IS
ALSO
INVERSE
WISHART
P
Σ
D
Μ
Σ
N
EXP
TR
Σ
Σ
D
EXP
TR
Σ
Σ
N
D
EXP
TR
Σ
IW
Σ
SN
ΝN
ΝN
N
SΜ
IN
WORDS
THIS
SAYS
THAT
THE
POSTERIOR
STRENGTH
ΝN
IS
THE
PRIOR
STRENGTH
PLUS
THE
NUMBER
OF
OBSERVATIONS
N
AND
THE
POSTERIOR
SCATTER
MATRIX
SN
IS
THE
PRIOR
SCATTER
MATRIX
PLUS
THE
DATA
SCATTER
MATRIX
SΜ
MAP
ESTIMATION
WE
SEE
FROM
EQUATION
THAT
Σˆ
MLE
IS
A
RANK
MIN
N
D
MATRIX
IF
N
D
THIS
IS
NOT
FULL
RANK
AND
HENCE
WILL
BE
UNINVERTIBLE
AND
EVEN
IF
N
D
IT
MAY
BE
THE
CASE
THAT
Σˆ
IS
ILL
CONDITIONED
MEANING
IT
IS
NEARLY
SINGULAR
TO
SOLVE
THESE
PROBLEMS
WE
CAN
USE
THE
POSTERIOR
MODE
OR
MEAN
ONE
CAN
SHOW
USING
TECHNIQUES
ANALOGOUS
TO
THE
DERIVATION
OF
THE
MLE
THAT
THE
MAP
ESTIMATE
IS
GIVEN
BY
SN
SΜ
Σ
MAP
ΝN
D
N
IF
WE
USE
AN
IMPROPER
UNIFORM
PRIOR
CORRESPONDING
TO
AND
WE
RECOVER
THE
MLE
LET
US
NOW
CONSIDER
THE
USE
OF
A
PROPER
INFORMATIVE
PRIOR
WHICH
IS
NECESSARY
WHENEVER
D
N
IS
LARGE
SAY
BIGGER
THAN
LET
Μ
X
SO
SΜ
SX
THEN
WE
CAN
REWRITE
THE
MAP
ESTIMATE
AS
A
CONVEX
COMBINATION
OF
THE
PRIOR
MODE
AND
THE
MLE
TO
SEE
THIS
LET
BE
THE
PRIOR
MODE
THEN
THE
POSTERIOR
MODE
CAN
BE
REWRITTEN
AS
SX
N
Σ
ΛΣ
Λ
Σˆ
MAP
N
N
N
N
MLE
WHERE
Λ
CONTROLS
THE
AMOUNT
OF
SHRINKAGE
TOWARDS
THE
PRIOR
THIS
BEGS
THE
QUESTION
WHERE
DO
THE
PARAMETERS
OF
THE
PRIOR
COME
FROM
IT
IS
COMMON
TO
SET
Λ
BY
CROSS
VALIDATION
ALTERNATIVELY
WE
CAN
USE
THE
CLOSED
FORM
FORMULA
PROVIDED
IN
LEDOIT
AND
WOLF
A
SCHAEFER
AND
STRIMMER
WHICH
IS
THE
OPTIMAL
FREQUENTIST
ESTIMATE
IF
WE
USE
SQUARED
LOSS
THIS
IS
ARGUABLY
NOT
THE
MOST
NATURAL
LOSS
FUNCTION
FOR
COVARIANCE
MATRICES
BECAUSE
IT
IGNORES
THE
POSTIVE
DEFINITE
CONSTRAINT
BUT
IT
RESULTS
IN
A
SIMPLE
ESTIMATOR
WHICH
IS
IMPLEMENTED
IN
THE
PMTK
FUNCTION
SHRINKCOV
WE
DISCUSS
BAYESIAN
WAYS
OF
ESTIMATING
Λ
LATER
AS
FOR
THE
PRIOR
COVARIANCE
MATRIX
IT
IS
COMMON
TO
USE
THE
FOLLOWING
DATA
DEPENDENT
PRIOR
DIAG
Σˆ
MLE
IN
THIS
CASE
THE
MAP
ESTIMATE
IS
GIVEN
BY
Σˆ
MAP
Σˆ
MLE
I
J
IF
I
J
Λ
Σˆ
MLE
I
J
OTHERWISE
THUS
WE
SEE
THAT
THE
DIAGONAL
ENTRIES
ARE
EQUAL
TO
THEIR
ML
ESTIMATES
AND
THE
OFF
DIAGO
NAL
ELEMENTS
ARE
SHRUNK
SOMEWHAT
TOWARDS
THIS
TECHNIQUE
IS
THEREFORE
CALLED
SHRINKAGE
ESTIMATION
OR
REGULARIZED
ESTIMATION
THE
BENEFITS
OF
MAP
ESTIMATION
ARE
ILLUSTRATED
IN
FIGURE
WE
CONSIDER
FITTING
A
DIMEN
SIONAL
GAUSSIAN
TO
N
N
AND
N
DATA
POINTS
WE
SEE
THAT
THE
MAP
ESTIMATE
IS
ALWAYS
WELL
CONDITIONED
UNLIKE
THE
MLE
IN
PARTICULAR
WE
SEE
THAT
THE
EIGENVALUE
SPECTRUM
OF
THE
MAP
ESTIMATE
IS
MUCH
CLOSER
TO
THAT
OF
THE
TRUE
MATRIX
THAN
THE
MLE
THE
EIGENVECTORS
HOWEVER
ARE
UNAFFECTED
THE
IMPORTANCE
OF
REGULARIZING
THE
ESTIMATE
OF
Σ
WILL
BECOME
APPARENT
IN
LATER
CHAPTERS
WHEN
WE
CONSIDER
FITTING
COVARIANCE
MATRICES
TO
HIGH
DIMENSIONAL
DATA
UNIVARIATE
POSTERIOR
IN
THE
CASE
THE
LIKELIHOOD
HAS
THE
FORM
N
P
D
N
EXP
XI
Μ
I
THE
STANDARD
CONJUGATE
PRIOR
IS
THE
INVERSE
GAMMA
DISTRIBUTION
WHICH
IS
JUST
THE
SCALAR
VERSION
OF
THE
INVERSE
WISHART
IG
A
B
EXP
FIGURE
SEQUENTIAL
UPDATING
OF
THE
POSTERIOR
FOR
STARTING
FROM
AN
UNINFORMATIVE
PRIOR
THE
DATA
WAS
GENERATED
FROM
A
GAUSSIAN
WITH
KNOWN
MEAN
Μ
AND
UNKNOWN
VARIANCE
FIGURE
GENERATED
BY
MULTIPLYING
THE
LIKELIHOOD
AND
THE
PRIOR
WE
SEE
THAT
THE
POSTERIOR
IS
ALSO
IG
P
D
IG
AN
BN
AN
N
N
BN
X
I
I
Μ
SEE
FIGURE
FOR
AN
ILLUSTRATION
THE
FORM
OF
THE
POSTERIOR
IS
NOT
QUITE
AS
PRETTY
AS
THE
MULTIVARIATE
CASE
BECAUSE
OF
THE
FACTORS
OF
THIS
ARISES
BECAUSE
IW
IG
ANOTHER
PROBLEM
WITH
USING
THE
IG
DISTRIBUTION
IS
THAT
THE
STRENGTH
OF
THE
PRIOR
IS
ENCODED
IN
BOTH
AND
TO
AVOID
BOTH
OF
THESE
PROBLEMS
IT
IS
COMMON
IN
THE
STATISTICS
LITERATURE
TO
USE
AN
ALTERNATIVE
PARAMETERIZATION
OF
THE
IG
DISTRIBUTION
KNOWN
AS
THE
SCALED
INVERSE
CHI
SQUARED
DISTRIBUTION
THIS
IS
DEFINED
AS
FOLLOWS
Χ
IG
EXP
HERE
CONTROLS
THE
STRENGTH
OF
THE
PRIOR
AND
ENCODES
THE
VALUE
OF
THE
PRIOR
WITH
THIS
PRIOR
THE
POSTERIOR
BECOMES
P
D
Μ
Χ
ΝN
ΝN
N
N
XI
Μ
WE
SEE
THAT
THE
POSTERIOR
DOF
ΝN
IS
THE
PRIOR
DOF
PLUS
N
AND
THE
POSTERIOR
SUM
OF
SQUARES
IS
THE
PRIOR
SUM
OF
SQUARES
PLUS
THE
DATA
SUM
OF
SQUARES
WE
CAN
EMULATE
AN
UNINFORMATIVE
PRIOR
P
Σ
BY
SETTING
WHICH
MAKES
INTUITIVE
SENSE
SINCE
IT
CORRESPONDS
TO
A
ZERO
VIRTUAL
SAMPLE
SIZE
POSTERIOR
DISTRIBUTION
OF
Μ
AND
Σ
WE
NOW
DISCUSS
HOW
TO
COMPUTE
P
Μ
Σ
THESE
RESULTS
ARE
A
BIT
COMPLEX
BUT
WILL
PROVE
USEFUL
LATER
ON
IN
THIS
BOOK
FEEL
FREE
TO
SKIP
THIS
SECTION
ON
A
FIRST
READING
LIKELIHOOD
THE
LIKELIHOOD
IS
GIVEN
BY
P
D
Μ
Σ
ND
Σ
EXP
NOW
ONE
CAN
SHOW
THAT
I
XI
Μ
T
Σ
XI
Μ
N
XI
Μ
T
Σ
XI
Μ
TR
Σ
N
X
Μ
T
Σ
X
Μ
I
HENCE
WE
CAN
REWRITE
THE
LIKELIHOOD
AS
FOLLOWS
P
D
Μ
Σ
ND
Σ
N
EXP
N
Μ
X
T
Σ
Μ
X
EXP
N
TR
Σ
WE
WILL
USE
THIS
FORM
BELOW
PRIOR
THE
OBVIOUS
PRIOR
TO
USE
IS
THE
FOLLOWING
P
Μ
Σ
N
Μ
IW
Σ
UNFORTUNATELY
THIS
IS
NOT
CONJUGATE
TO
THE
LIKELIHOOD
TO
SEE
WHY
NOTE
THAT
Μ
AND
Σ
APPEAR
TOGETHER
IN
A
NON
FACTORIZED
WAY
IN
THE
LIKELIHOOD
HENCE
THEY
WILL
ALSO
BE
COUPLED
TOGETHER
IN
THE
POSTERIOR
THE
ABOVE
PRIOR
IS
SOMETIMES
CALLED
SEMI
CONJUGATE
OR
CONDITIONALLY
CONJUGATE
SINCE
BOTH
CONDITIONALS
P
Μ
Σ
AND
P
Σ
Μ
ARE
INDIVIDUALLY
CONJUGATE
TO
CREATE
A
FULL
CONJUGATE
PRIOR
WE
NEED
TO
USE
A
PRIOR
WHERE
Μ
AND
Σ
ARE
DEPENDENT
ON
EACH
OTHER
WE
WILL
USE
A
JOINT
DISTRIBUTION
OF
THE
FORM
P
Μ
Σ
P
Σ
P
Μ
Σ
LOOKING
AT
THE
FORM
OF
THE
LIKELIHOOD
EQUATION
EQUATION
WE
SEE
THAT
A
NATURAL
CONJUGATE
PRIOR
HAS
THE
FORM
OF
A
NORMAL
INVERSE
WISHART
OR
NIW
DISTRIBUTION
DEFINED
AS
FOLLOWS
NIW
Μ
Σ
N
Μ
Κ
Σ
IW
Σ
Σ
EXP
Μ
M
T
Σ
Μ
M
D
Σ
D
ZNIW
EXP
Μ
M
T
Σ
Μ
M
TR
Σ
ZNIW
D
WHERE
ΓD
A
IS
THE
MULTIVARIATE
GAMMA
FUNCTION
THE
PARAMETERS
OF
THE
NIW
CAN
BE
INTERPRETED
AS
FOLLOWS
IS
OUR
PRIOR
MEAN
FOR
Μ
AND
IS
HOW
STRONGLY
WE
BELIEVE
THIS
PRIOR
AND
IS
PROPORTIONAL
TO
OUR
PRIOR
MEAN
FOR
Σ
AND
IS
HOW
STRONGLY
WE
BELIEVE
THIS
PRIOR
ONE
CAN
SHOW
MINKA
THAT
THE
IMPROPER
UNINFORMATIVE
PRIOR
HAS
THE
FORM
LIM
Μ
Σ
K
IW
Σ
K
Σ
K
Σ
NIW
Μ
Σ
IN
PRACTICE
IT
IS
OFTEN
BETTER
TO
USE
A
WEAKLY
INFORMATIVE
DATA
DEPENDENT
PRIOR
A
COMMON
CHOICE
SEE
E
G
CHIPMAN
ET
AL
FRALEY
AND
RAFTERY
IS
TO
USE
DIAG
SX
N
AND
D
TO
ENSURE
E
Σ
AND
TO
SET
X
AND
TO
SOME
SMALL
NUMBER
SUCH
AS
ALTHOUGH
THIS
PRIOR
HAS
FOUR
PARAMETERS
THERE
ARE
REALLY
ONLY
THREE
FREE
PARAMETERS
SINCE
OUR
UNCERTAINTY
IN
THE
MEAN
IS
PROPORTIONAL
TO
THE
VARIANCE
IN
PARTICULAR
IF
WE
BELIEVE
THAT
THE
VARIANCE
IS
LARGE
THEN
OUR
UNCERTAINTY
IN
Μ
MUST
BE
LARGE
TOO
THIS
MAKES
SENSE
INTUITIVELY
SINCE
IF
THE
DATA
HAS
LARGE
SPREAD
IT
MAY
BE
HARD
TO
PIN
DOWN
ITS
MEAN
SEE
ALSO
EXERCISE
WHERE
WE
WILL
SEE
THE
THREE
FREE
PARAMETERS
MORE
EXPLICITLY
IF
WE
WANT
SEPARATE
CONTROL
OVER
OUR
CONFIDENCE
IN
Μ
AND
Σ
WE
MUST
USE
A
SEMI
CONJUGATE
PRIOR
POSTERIOR
THE
POSTERIOR
CAN
BE
SHOWN
EXERCISE
TO
BE
NIW
WITH
UPDATED
PARAMETERS
P
Μ
Σ
D
NIW
Μ
Σ
MN
ΚN
ΝN
SN
N
X
N
M
M
X
N
N
N
N
ΚN
N
ΝN
N
X
M
X
M
T
N
X
N
T
T
ΚNMNMN
WHERE
WE
HAVE
DEFINED
N
XIXT
AS
THE
UNCENTERED
SUM
OF
SQUARES
MATRIX
THIS
IS
EASIER
TO
UPDATE
INCREMENTALLY
THAN
THE
CENTERED
VERSION
THIS
RESULT
IS
ACTUALLY
QUITE
INTUITIVE
THE
POSTERIOR
MEAN
IS
A
CONVEX
COMBINATION
OF
THE
PRIOR
MEAN
AND
THE
MLE
WITH
STRENGTH
N
AND
THE
POSTERIOR
SCATTER
MATRIX
SN
IS
THE
PRIOR
SCATTER
MATRIX
PLUS
THE
EMPIRICAL
SCATTER
MATRIX
SX
PLUS
AN
EXTRA
TERM
DUE
TO
THE
UNCERTAINTY
IN
THE
MEAN
WHICH
CREATES
ITS
OWN
VIRTUAL
SCATTER
MATRIX
POSTERIOR
MODE
THE
MODE
OF
THE
JOINT
DISTRIBUTION
HAS
THE
FOLLOWING
FORM
ARGMAX
P
Μ
Σ
D
MN
SN
ΝN
D
IF
WE
SET
THIS
REDUCES
TO
SX
ARGMAX
P
Μ
Σ
X
N
D
THE
CORRESPONDING
ESTIMATE
Σˆ
IS
ALMOST
THE
SAME
AS
EQUATION
BUT
DIFFERS
BY
IN
THE
DENOMINATOR
BECAUSE
THIS
IS
THE
MODE
OF
THE
JOINT
NOT
THE
MODE
OF
THE
MARGINAL
POSTERIOR
MARGINALS
THE
POSTERIOR
MARGINAL
FOR
Σ
IS
SIMPLY
P
Σ
D
R
P
Μ
Σ
D
DΜ
IW
Σ
SN
ΝN
THE
MODE
AND
MEAN
OF
THIS
MARGINAL
ARE
GIVEN
BY
SN
SN
Σ
E
Σ
MAP
ΝN
D
ΝN
D
ONE
CAN
SHOW
THAT
THE
POSTERIOR
MARGINAL
FOR
Μ
HAS
A
MULTIVARIATE
STUDENT
T
DISTRIBUTION
P
Μ
D
R
P
Μ
Σ
D
DΣ
T
Μ
MN
SN
ΚN
ΝN
D
ΝN
D
THIS
FOLLOWS
FROM
THE
FACT
THAT
THE
STUDENT
DISTRIBUTION
CAN
BE
REPRESENTED
AS
A
SCALED
MIXTURE
OF
GAUSSIANS
SEE
EQUATION
FIGURE
THE
DISTRIBUTION
IS
THE
PRIOR
MEAN
AND
IS
HOW
STRONGLY
WE
BELIEVE
THIS
IS
THE
PRIOR
VARIANCE
AND
IS
HOW
STRONGLY
WE
BELIEVE
THIS
A
NOTICE
THAT
THE
CONTOUR
PLOT
UNDERNEATH
THE
SURFACE
IS
SHAPED
LIKE
A
SQUASHED
EGG
B
WE
INCREASE
THE
STRENGTH
OF
OUR
BELIEF
IN
THE
MEAN
SO
IT
GETS
NARROWER
C
WE
INCREASE
THE
STRENGTH
OF
OUR
BELIEF
IN
THE
VARIANCE
SO
IT
GETS
NARROWER
FIGURE
GENERATED
BY
POSTERIOR
PREDICTIVE
THE
POSTERIOR
PREDICTIVE
IS
GIVEN
BY
P
X
P
X
D
P
D
SO
IT
CAN
BE
EASILY
EVALUATED
IN
TERMS
OF
A
RATIO
OF
MARGINAL
LIKELIHOODS
IT
TURNS
OUT
THAT
THIS
RATIO
HAS
THE
FORM
OF
A
MULTIVARIATE
STUDENT
T
DISTRIBUTION
P
X
D
R
R
N
X
Μ
Σ
NIW
Μ
Σ
MN
ΚN
ΝN
SN
DΜDΣ
T
X
MN
ΚN
SN
ΚN
ΝN
D
ΝN
D
THE
STUDENT
T
HAS
WIDER
TAILS
THAN
A
GAUSSIAN
WHICH
TAKES
INTO
ACCOUNT
THE
FACT
THAT
Σ
IS
UNKNOWN
HOWEVER
THIS
RAPIDLY
BECOMES
GAUSSIAN
LIKE
POSTERIOR
FOR
SCALAR
DATA
WE
NOW
SPECIALISE
THE
ABOVE
RESULTS
TO
THE
CASE
WHERE
XI
IS
THESE
RESULTS
ARE
WIDELY
USED
IN
THE
STATISTICS
LITERATURE
AS
IN
SECTION
IT
IS
CONVENTIONAL
NOT
TO
USE
THE
NORMAL
INVERSE
WISHART
BUT
TO
USE
THE
NORMAL
INVERSE
CHI
SQUARED
OR
NIX
DISTRIBUTION
DEFINED
BY
Μ
N
Μ
Χ
EXP
SEE
FIGURE
FOR
SOME
PLOTS
ALONG
THE
Μ
AXIS
THE
DISTRIBUTION
IS
SHAPED
LIKE
A
GAUSSIAN
AND
ALONG
THE
AXIS
THE
DISTRIBUTION
IS
SHAPED
LIKE
A
Χ
THE
CONTOURS
OF
THE
JOINT
DENSITY
HAVE
A
SQUASHED
EGG
APPEARANCE
INTERESTINGLY
WE
SEE
THAT
THE
CONTOURS
FOR
Μ
ARE
MORE
PEAKED
FOR
SMALL
VALUES
OF
WHICH
MAKES
SENSE
SINCE
IF
THE
DATA
IS
LOW
VARIANCE
WE
WILL
BE
ABLE
TO
ESTIMATE
ITS
MEAN
MORE
RELIABLY
ONE
CAN
SHOW
THAT
THE
POSTERIOR
IS
GIVEN
BY
P
Μ
D
Μ
MN
ΚN
ΝN
M
NX
N
ΚN
ΚN
N
ΝN
N
N
Ν
Ν
X
X
M
X
THE
POSTERIOR
MARGINAL
FOR
IS
JUST
P
D
R
P
Μ
D
DΜ
Χ
ΝN
WITH
THE
POSTERIOR
MEAN
GIVEN
BY
E
D
ΝN
HAS
A
STUDENT
T
DISTRIBUTION
WHICH
FOLLOWS
FROM
THE
SCALE
MIXTURE
REPRESENTATION
OF
THE
STUDENT
P
Μ
D
R
P
Μ
D
T
Μ
MN
ΚN
ΝN
WITH
THE
POSTERIOR
MEAN
GIVEN
BY
E
Μ
MN
LET
US
SEE
HOW
THESE
RESULTS
LOOK
IF
WE
USE
THE
FOLLOWING
UNINFORMATIVE
PRIOR
P
Μ
P
Μ
P
Σ
Μ
WITH
THIS
PRIOR
THE
POSTERIOR
HAS
THE
FORM
P
Μ
D
Μ
MN
X
ΚN
N
ΝN
N
WHERE
X
N
X
Σˆ
IS
THE
THE
SAMPLE
STANDARD
DEVIATION
IN
SECTION
WE
SHOW
THAT
THIS
IS
AN
UNBIASED
ESTIMATE
OF
THE
VARIANCE
HENCE
THE
MARGINAL
POSTERIOR
FOR
THE
MEAN
IS
GIVEN
BY
P
Μ
D
T
Μ
X
N
N
AND
THE
POSTERIOR
VARIANCE
OF
Μ
IS
ΝN
N
VAR
Μ
D
Ν
ΣN
N
N
N
THE
SQUARE
ROOT
OF
THIS
IS
CALLED
THE
STANDARD
ERROR
OF
THE
MEAN
I
VAR
Μ
D
N
THUS
AN
APPROXIMATE
POSTERIOR
CREDIBLE
INTERVAL
FOR
THE
MEAN
IS
I
Μ
D
X
N
BAYESIAN
CREDIBLE
INTERVALS
ARE
DISCUSSED
IN
MORE
DETAIL
IN
SECTION
THEY
ARE
CONTRASTED
WITH
FREQUENTIST
CONFIDENCE
INTERVALS
IN
SECTION
BAYESIAN
T
TEST
SUPPOSE
WE
WANT
TO
TEST
THE
HYPOTHESIS
THAT
Μ
FOR
SOME
KNOWN
VALUE
OFTEN
GIVEN
VALUES
XI
Μ
THIS
IS
CALLED
A
TWO
SIDED
ONE
SAMPLE
T
TEST
A
SIMPLE
WAY
TO
PERFORM
SUCH
A
TEST
IS
JUST
TO
CHECK
IF
Μ
IF
IT
IS
NOT
THEN
WE
CAN
BE
SURE
THAT
Μ
A
MORE
COMMON
SCENARIO
IS
WHEN
WE
WANT
TO
TEST
IF
TWO
PAIRED
SAMPLES
HAVE
THE
SAME
MEAN
MORE
PRECISELY
SUPPOSE
YI
AND
ZI
WE
WANT
TO
DETERMINE
IF
Μ
USING
XI
YI
ZI
AS
OUR
DATA
WE
CAN
EVALUATE
THIS
QUANTITY
AS
FOLLOWS
P
Μ
Μ
D
R
P
Μ
D
DΜ
THIS
IS
CALLED
A
ONE
SIDED
PAIRED
T
TEST
FOR
A
SIMILAR
APPROACH
TO
UNPAIRED
TESTS
COMPARING
THE
DIFFERENCE
IN
BINOMIAL
PROPORTIONS
SEE
SECTION
TO
CALCULATE
THE
POSTERIOR
WE
MUST
SPECIFY
A
PRIOR
SUPPOSE
WE
USE
AN
UNINFORMATIVE
PRIOR
AS
WE
SHOWED
ABOVE
WE
FIND
THAT
THE
POSTERIOR
MARGINAL
ON
Μ
HAS
THE
FORM
P
Μ
D
T
Μ
X
N
N
NOW
LET
US
DEFINE
THE
FOLLOWING
T
STATISTIC
T
N
WHERE
THE
DENOMINATOR
IS
THE
STANDARD
ERROR
OF
THE
MEAN
WE
SEE
THAT
P
Μ
D
FN
T
WHERE
FΝ
T
IS
THE
CDF
OF
THE
STANDARD
STUDENT
T
DISTRIBUTION
T
Ν
A
MORE
COMPLEX
APPROACH
IS
TO
PERFORM
BAYESIAN
MODEL
COMPARISON
THAT
IS
WE
COMPUTE
THE
BAYES
FACTOR
DESCRIBED
IN
SECTION
P
D
P
D
WHERE
IS
THE
POINT
NULL
HYPOTHESIS
THAT
Μ
AND
IS
THE
ALTERNATIVE
HYPOTHESIS
THAT
Μ
SEE
GONEN
ET
AL
ROUDER
ET
AL
FOR
DETAILS
CONNECTION
WITH
FREQUENTIST
STATISTICS
IF
WE
USE
AN
UNINFORMATIVE
PRIOR
IT
TURNS
OUT
THAT
THE
ABOVE
BAYESIAN
ANALYSIS
GIVES
THE
SAME
RESULT
AS
DERIVED
USING
FREQUENTIST
METHODS
WE
DISCUSS
FREQUENTIST
STATISTICS
IN
CHAPTER
SPECIFICALLY
FROM
THE
ABOVE
RESULTS
WE
SEE
THAT
Μ
X
N
THIS
HAS
THE
SAME
FORM
AS
THE
SAMPLING
DISTRIBUTION
OF
THE
MLE
Μ
X
IS
N
Μ
TN
THE
REASON
IS
THAT
THE
STUDENT
DISTRIBUTION
IS
SYMMETRIC
IN
ITS
FIRST
TWO
ARGUMENTS
SO
X
Μ
Ν
Μ
X
Ν
HENCE
STATEMENTS
ABOUT
THE
POSTERIOR
FOR
Μ
HAVE
THE
SAME
FORM
AS
STATEMENTS
ABOUT
THE
SAMPLING
DISTRIBUTION
OF
X
CONSEQUENTLY
THE
ONE
SIDED
P
VALUE
DEFINED
IN
SEC
TION
RETURNED
BY
A
FREQUENTIST
TEST
IS
THE
SAME
AS
P
Μ
RETURNED
BY
THE
BAYESIAN
METHOD
SEE
BAYESTTESTDEMO
FOR
AN
EXAMPLE
DESPITE
THE
SUPERFICIAL
SIMILARITY
THESE
TWO
RESULTS
HAVE
A
DIFFERENT
INTERPRETATION
IN
THE
BAYESIAN
APPROACH
Μ
IS
UNKNOWN
AND
X
IS
FIXED
WHEREAS
IN
THE
FREQUENTIST
APPROACH
X
IS
UNKNOWN
AND
Μ
IS
FIXED
MORE
EQUIVALENCES
BETWEEN
FREQUENTIST
AND
BAYESIAN
INFERENCE
IN
SIMPLE
MODELS
USING
UNINFORMATIVE
PRIORS
CAN
BE
FOUND
IN
BOX
AND
TIAO
SEE
ALSO
SECTION
SENSOR
FUSION
WITH
UNKNOWN
PRECISIONS
IN
THIS
SECTION
WE
APPLY
THE
RESULTS
IN
SECTION
TO
THE
PROBLEM
OF
SENSOR
FUSION
IN
THE
CASE
WHERE
THE
PRECISION
OF
EACH
MEASUREMENT
DEVICE
IS
UNKNOWN
THIS
GENERALIZES
THE
RESULTS
OF
SECTION
WHERE
THE
MEASUREMENT
MODEL
WAS
ASSUMED
TO
BE
GAUSSIAN
WITH
KNOWN
PRECISION
THE
UNKNOWN
PRECISION
CASE
TURNS
OUT
TO
GIVE
QUALITATIVELY
DIFFERENT
RESULTS
YIELDING
A
POTENTIALLY
MULTI
MODAL
POSTERIOR
AS
WE
WILL
SEE
OUR
PRESENTATION
IS
BASED
ON
MINKA
SUPPOSE
WE
WANT
TO
POOL
DATA
FROM
MULTIPLE
SOURCES
TO
ESTIMATE
SOME
QUANTITY
Μ
R
BUT
THE
RELIABILITY
OF
THE
SOURCES
IS
UNKNOWN
SPECIFICALLY
SUPPOSE
WE
HAVE
TWO
DIFFERENT
MEASUREMENT
DEVICES
X
AND
Y
WITH
DIFFERENT
PRECISIONS
XI
Μ
Μ
Λ
X
AND
YI
Μ
Μ
Λ
Y
WE
MAKE
TWO
INDEPENDENT
MEASUREMENTS
WITH
EACH
DEVICE
WHICH
TURN
OUT
TO
BE
WE
WILL
USE
A
NON
INFORMATIVE
PRIOR
FOR
Μ
P
Μ
WHICH
WE
CAN
EMULATE
USING
AN
INFINITELY
BROAD
GAUSSIAN
P
Μ
Μ
IF
THE
ΛX
AND
ΛY
TERMS
WERE
KNOWN
THEN
THE
POSTERIOR
WOULD
BE
GAUSSIAN
P
Μ
D
ΛX
ΛY
N
Μ
MN
Λ
ΛN
NXΛX
NYΛY
M
ΛXNXX
ΛYNYY
N
NXΛX
NYΛY
WHERE
NX
IS
THE
NUMBER
OF
X
MEASUREMENTS
NY
IS
THE
NUMBER
OF
Y
MEASUREMENTS
X
NX
XI
AND
Y
NY
YI
THIS
RESULT
FOLLOWS
BECAUSE
THE
POSTERIOR
PRECISION
IS
THE
SUM
OF
THE
MEASUREMENT
PRECISIONS
AND
THE
POSTERIOR
MEAN
IS
A
WEIGHTED
SUM
OF
THE
PRIOR
MEAN
WHICH
IS
AND
THE
DATA
MEANS
HOWEVER
THE
MEASUREMENT
PRECISIONS
ARE
NOT
KNOWN
INITIALLY
WE
WILL
ESTIMATE
THEM
BY
MAXIMUM
LIKELIHOOD
THE
LOG
LIKELIHOOD
IS
GIVEN
BY
Μ
ΛX
ΛY
LOG
ΛX
ΛX
X
I
I
Μ
LOG
ΛY
ΛY
Y
I
I
Μ
THE
MLE
IS
OBTAINED
BY
SOLVING
THE
FOLLOWING
SIMULTANEOUS
EQUATIONS
Μ
ΛXNX
X
Μ
ΛYNY
Y
Μ
NX
ΛX
ΛX
NX
XI
I
NY
Y
Μ
Μ
ΛY
THIS
GIVES
ΛY
NY
I
I
NXΛˆXX
NYΛˆYY
Μˆ
NXΛˆX
NYΛˆY
ΛˆX
Λˆ
XI
N
I
Y
Μˆ
Μˆ
WE
NOTICE
THAT
THE
MLE
FOR
Μ
HAS
THE
SAME
FORM
AS
THE
POSTERIOR
MEAN
MN
WE
CAN
SOLVE
THESE
EQUATIONS
BY
FIXED
POINT
ITERATION
LET
US
INITIALIZE
BY
ESTIMATING
ΛX
AND
ΛY
WHERE
NX
XI
X
AND
NY
YI
Y
USING
THIS
WE
GET
Μˆ
SO
P
Μ
D
ΛˆX
ΛˆY
N
Μ
IF
WE
NOW
ITERATE
WE
CONVERGE
TO
ΛˆX
ΛˆY
P
Μ
ΛˆX
ΛˆY
Μ
THE
PLUG
IN
APPROXIMATION
TO
THE
POSTERIOR
IS
PLOTTED
IN
FIGURE
A
THIS
WEIGHTS
EACH
SENSOR
ACCORDING
TO
ITS
ESTIMATED
PRECISION
SINCE
SENSOR
Y
WAS
ESTIMATED
TO
BE
MUCH
LESS
RELIABLE
THAN
SENSOR
X
WE
HAVE
E
Μ
D
ΛˆX
ΛˆY
X
SO
WE
EFFECTIVELY
IGNORE
THE
Y
SENSOR
NOW
WE
WILL
ADOPT
A
BAYESIAN
APPROACH
AND
INTEGRATE
OUT
THE
UNKNOWN
PRECISIONS
RATHER
THAN
TRYING
TO
ESTIMATE
THEM
THAT
IS
WE
COMPUTE
P
Μ
D
P
Μ
R
P
DX
Μ
ΛX
P
ΛX
Μ
DΛXL
R
P
DY
Μ
ΛY
P
ΛY
Μ
DΛYL
WE
WILL
USE
UNINFORMATIVE
JEFFREY
PRIORS
P
Μ
P
ΛX
Μ
ΛX
AND
P
ΛY
Μ
ΛY
SINCE
THE
X
AND
Y
TERMS
ARE
SYMMETRIC
WE
WILL
JUST
FOCUS
ON
ONE
OF
THEM
THE
KEY
INTEGRAL
IS
I
R
P
DX
Μ
ΛX
P
ΛX
Μ
DΛX
R
Λ
X
NXΛX
NX
EXP
NX
Λ
X
Μ
NX
DΛ
X
EXPLOITING
THE
FACT
THAT
NX
THIS
SIMPLIFIES
TO
X
X
X
I
R
Λ
X
EXP
ΛX
X
Μ
DΛX
WE
RECOGNIZE
THIS
AS
PROPORTIONAL
TO
THE
INTEGRAL
OF
AN
UNNORMALIZED
GAMMA
DENSITY
GA
Λ
A
B
ΛA
ΛB
WHERE
A
AND
B
X
Μ
HENCE
THE
INTEGRAL
IS
PROPORTIONAL
TO
THE
NORMALIZING
CONSTANT
OF
THE
GAMMA
DISTRIBUTION
Γ
A
B
A
SO
WE
GET
I
R
P
DX
Μ
ΛX
P
ΛX
Μ
DΛX
X
Μ
AND
THE
POSTERIOR
BECOMES
P
Μ
D
X
Μ
Y
Μ
THE
EXACT
POSTERIOR
IS
PLOTTED
IN
FIGURE
B
WE
SEE
THAT
IT
HAS
TWO
MODES
ONE
NEAR
X
AND
ONE
NEAR
Y
THESE
CORRESPOND
TO
THE
BELIEFS
THAT
THE
X
SENSOR
IS
MORE
RELIABLE
THAN
THE
Y
ONE
AND
VICE
VERSA
THE
WEIGHT
OF
THE
FIRST
MODE
IS
LARGER
SINCE
THE
DATA
FROM
THE
X
SENSOR
AGREE
MORE
WITH
EACH
OTHER
SO
IT
SEEMS
SLIGHTLY
MORE
LIKELY
THAT
THE
X
SENSOR
IS
THE
RELIABLE
ONE
THEY
OBVIOUSLY
CANNOT
BOTH
BE
RELIABLE
SINCE
THEY
DISAGREE
ON
THE
VALUES
THAT
THEY
ARE
REPORTING
HOWEVER
THE
BAYESIAN
SOLUTION
KEEPS
OPEN
THE
POSSIBILITY
THAT
THE
Y
SENSOR
IS
THE
MORE
RELIABLE
ONE
FROM
TWO
MEASUREMENTS
WE
CANNOT
TELL
AND
CHOOSING
JUST
THE
X
SENSOR
AS
THE
PLUG
IN
APPROXIMATION
DOES
RESULTS
IN
OVER
CONFIDENCE
A
POSTERIOR
THAT
IS
TOO
NARROW
EXERCISES
EXERCISE
UNCORRELATED
DOES
NOT
IMPLY
INDEPENDENT
LET
X
U
AND
Y
CLEARLY
Y
IS
DEPENDENT
ON
X
IN
FACT
Y
IS
UNIQUELY
DETERMINED
BY
X
HOWEVER
SHOW
THAT
Ρ
X
Y
HINT
IF
X
U
A
B
THEN
E
X
A
B
AND
VAR
X
B
A
EXERCISE
UNCORRELATED
AND
GAUSSIAN
DOES
NOT
IMPLY
INDEPENDENT
UNLESS
JOINTLY
GAUSSIAN
LET
X
AND
Y
WX
WHERE
P
W
P
W
IT
IS
CLEAR
THAT
X
AND
Y
ARE
NOT
INDEPENDENT
SINCE
Y
IS
A
FUNCTION
OF
X
A
SHOW
Y
N
A
B
FIGURE
POSTERIOR
FOR
Μ
A
PLUG
IN
APPROXIMATION
B
EXACT
POSTERIOR
FIGURE
GENERATED
BY
SENSORFUSIONUNKNOWNPREC
B
SHOW
COV
X
Y
THUS
X
AND
Y
ARE
UNCORRELATED
BUT
DEPENDENT
EVEN
THOUGH
THEY
ARE
GAUSSIAN
HINT
USE
THE
DEFINITION
OF
COVARIANCE
COV
X
Y
E
XY
E
X
E
Y
AND
THE
RULE
OF
ITERATED
EXPECTATION
E
XY
E
E
XY
W
EXERCISE
CORRELATION
COEFFICIENT
IS
BETWEEN
AND
PROVE
THAT
Ρ
X
Y
EXERCISE
CORRELATION
COEFFICIENT
FOR
LINEARLY
RELATED
VARIABLES
IS
SHOW
THAT
IF
Y
AX
B
FOR
SOME
PARAMETERS
A
AND
B
THEN
Ρ
X
Y
SIMILARLY
SHOW
THAT
IF
A
THEN
Ρ
X
Y
EXERCISE
NORMALIZATION
CONSTANT
FOR
A
MULTIDIMENSIONAL
GAUSSIAN
PROVE
THAT
THE
NORMALIZATION
CONSTANT
FOR
A
D
DIMENSIONAL
GAUSSIAN
IS
GIVEN
BY
D
Σ
EXP
X
Μ
T
Σ
X
Μ
DX
HINT
DIAGONALIZE
Σ
AND
USE
THE
FACT
THAT
Σ
I
ΛI
TO
WRITE
THE
JOINT
PDF
AS
A
PRODUCT
OF
D
ONE
DIMENSIONAL
GAUSSIANS
IN
A
TRANSFORMED
COORDINATE
SYSTEM
YOU
WILL
NEED
THE
CHANGE
OF
VARIABLES
FORMULA
FINALLY
USE
THE
NORMALIZATION
CONSTANT
FOR
UNIVARIATE
GAUSSIANS
EXERCISE
BIVARIATE
GAUSSIAN
LET
X
N
Μ
Σ
WHERE
X
AND
Σ
WHERE
Ρ
IS
THE
CORRELATION
COEFFICIENT
SHOW
THAT
THE
PDF
IS
GIVEN
BY
P
Σ
RAW
STANDARIZED
WHITENED
FIGURE
A
HEIGHT
WEIGHT
DATA
FOR
THE
MEN
B
STANDARDIZED
C
WHITENED
EXERCISE
CONDITIONING
A
BIVARIATE
GAUSSIAN
CONSIDER
A
BIVARIATE
GAUSSIAN
DISTRIBUTION
P
N
X
Μ
Σ
WHERE
Σ
Ρ
Σ
WHERE
THE
CORRELATION
COEFFICIENT
IS
GIVEN
BY
Ρ
A
WHAT
IS
P
SIMPLIFY
YOUR
ANSWER
BY
EXPRESSING
IT
IN
TERMS
OF
Ρ
AND
B
ASSUME
WHAT
IS
P
NOW
EXERCISE
WHITENING
VS
STANDARDIZING
A
LOAD
THE
HEIGHT
WEIGHT
DATA
USING
RAWDATA
DLMREAD
HEIGHTWEIGHTDATA
TXT
THE
FIRST
COL
UMN
IS
THE
CLASS
LABEL
MALE
FEMALE
THE
SECOND
COLUMN
IS
HEIGHT
THE
THIRD
WEIGHT
EXTRACT
THE
HEIGHT
WEIGHT
DATA
CORRESPONDING
TO
THE
MALES
FIT
A
GAUSSIAN
TO
THE
MALE
DATA
USING
THE
EMPIRICAL
MEAN
AND
COVARIANCE
PLOT
YOUR
GAUSSIAN
AS
AN
ELLIPSE
USE
SUPERIMPOSING
ON
YOUR
SCATTER
PLOT
IT
SHOULD
LOOK
LIKE
FIGURE
A
WHERE
HAVE
LABELED
EACH
DATAPOINT
BY
ITS
INDEX
TURN
IN
YOUR
FIGURE
AND
CODE
B
STANDARDIZING
THE
DATA
MEANS
ENSURING
THE
EMPIRICAL
VARIANCE
ALONG
EACH
DIMENSION
IS
THIS
CAN
BE
DONE
BY
COMPUTING
XIJ
XJ
WHERE
Σ
J
IS
THE
EMPIRICAL
STD
OF
DIMENSION
J
STANDARDIZE
THE
DATA
AND
REPLOT
IT
SHOULD
LOOK
LIKE
FIGURE
B
USE
AXIS
EQUAL
TURN
IN
YOUR
FIGURE
AND
CODE
C
WHITENING
OR
SPHEREING
THE
DATA
MEANS
ENSURING
ITS
EMPIRICAL
COVARIANCE
MATRIX
IS
PROPORTIONAL
TO
I
SO
THE
DATA
IS
UNCORRELATED
AND
OF
EQUAL
VARIANCE
ALONG
EACH
DIMENSION
THIS
CAN
BE
DONE
BY
COMPUTING
Λ
UT
X
FOR
EACH
DATA
VECTOR
X
WHERE
U
ARE
THE
EIGENVECTORS
AND
Λ
THE
EIGENVALUES
OF
X
WHITEN
THE
DATA
AND
REPLOT
IT
SHOULD
LOOK
LIKE
FIGURE
C
NOTE
THAT
WHITENING
ROTATES
THE
DATA
SO
PEOPLE
MOVE
TO
COUNTER
INTUITIVE
LOCATIONS
IN
THE
NEW
COORDINATE
SYSTEM
SEE
E
G
PERSON
WHO
MOVES
FROM
THE
RIGHT
HAND
SIDE
TO
THE
LEFT
EXERCISE
SENSOR
FUSION
WITH
KNOWN
VARIANCES
IN
SUPPOSE
WE
HAVE
TWO
SENSORS
WITH
KNOWN
AND
DIFFERENT
VARIANCES
AND
BUT
UNKNOWN
AND
THE
SAME
MEAN
Μ
SUPPOSE
WE
OBSERVE
OBSERVATIONS
Y
N
Μ
FROM
THE
FIRST
SENSOR
AND
OBSERVATIONS
Y
Μ
FROM
THE
SECOND
SENSOR
FOR
EXAMPLE
SUPPOSE
Μ
IS
THE
TRUE
TEMPERATURE
OUTSIDE
AND
SENSOR
IS
A
PRECISE
LOW
VARIANCE
DIGITAL
THERMOSENSING
DEVICE
AND
SENSOR
IS
AN
IMPRECISE
HIGH
VARIANCE
MERCURY
THERMOMETER
LET
REPRESENT
ALL
THE
DATA
FROM
BOTH
SENSORS
WHAT
IS
THE
POSTERIOR
P
Μ
ASSUMING
A
NON
INFORMATIVE
PRIOR
FOR
Μ
WHICH
WE
CAN
SIMULATE
USING
A
GAUSSIAN
WITH
A
PRECISION
OF
GIVE
AN
EXPLICIT
EXPRESSION
FOR
THE
POSTERIOR
MEAN
AND
VARIANCE
EXERCISE
DERIVATION
OF
INFORMATION
FORM
FORMULAE
FOR
MARGINALIZING
AND
CONDITIONING
DERIVE
THE
INFORMATION
FORM
RESULTS
OF
SECTION
EXERCISE
DERIVATION
OF
THE
NIW
POSTERIOR
DERIVE
EQUATION
HINT
ONE
CAN
SHOW
THAT
N
X
Μ
X
Μ
T
Μ
Μ
T
T
T
Κ
Μ
M
Μ
M
X
M
X
M
THIS
IS
A
MATRIX
GENERALIZATION
OF
AN
OPERATION
CALLED
COMPLETING
THE
SQUARE
DERIVE
THE
CORRESPONDING
RESULT
FOR
THE
NORMAL
WISHART
MODEL
EXERCISE
BIC
FOR
GAUSSIANS
SOURCE
JAAKKOLA
THE
BAYESIAN
INFORMATION
CRITERION
BIC
IS
A
PENALIZED
LOG
LIKELIHOOD
FUNCTION
THAT
CAN
BE
USED
FOR
MODEL
SELECTION
SEE
SECTION
IT
IS
DEFINED
AS
BIC
LOG
P
D
Θˆ
D
LOG
N
WHERE
D
IS
THE
NUMBER
OF
FREE
PARAMETERS
IN
THE
MODEL
AND
N
IS
THE
NUMBER
OF
SAMPLES
IN
THIS
QUESTION
WE
WILL
SEE
HOW
TO
USE
THIS
TO
CHOOSE
BETWEEN
A
FULL
COVARIANCE
GAUSSIAN
AND
A
GAUSSIAN
WITH
A
DIAGONAL
COVARIANCE
OBVIOUSLY
A
FULL
COVARIANCE
GAUSSIAN
HAS
HIGHER
LIKELIHOOD
BUT
IT
MAY
NOT
BE
WORTH
THE
EXTRA
PARAMETERS
IF
THE
IMPROVEMENT
OVER
A
DIAGONAL
COVARIANCE
MATRIX
IS
TOO
SMALL
SO
WE
USE
THE
BIC
SCORE
TO
CHOOSE
THE
MODEL
FOLLOWING
SECTION
WE
CAN
WRITE
LOG
P
D
Σˆ
Μˆ
N
TR
Σˆ
N
LOG
Σˆ
N
Sˆ
X
N
I
I
X
XI
X
WHERE
Sˆ
IS
THE
SCATTER
MATRIX
EMPIRICAL
COVARIANCE
THE
TRACE
OF
A
MATRIX
IS
THE
SUM
OF
ITS
DIAGONALS
AND
WE
HAVE
USED
THE
TRACE
TRICK
A
DERIVE
THE
BIC
SCORE
FOR
A
GAUSSIAN
IN
D
DIMENSIONS
WITH
FULL
COVARIANCE
MATRIX
SIMPLIFY
YOUR
ANSWER
AS
MUCH
AS
POSSIBLE
EXPLOITING
THE
FORM
OF
THE
MLE
BE
SURE
TO
SPECIFY
THE
NUMBER
OF
FREE
PARAMETERS
D
B
DERIVE
THE
BIC
SCORE
FOR
A
GAUSSIAN
IN
D
DIMENSIONS
WITH
A
DIAGONAL
COVARIANCE
MATRIX
BE
SURE
TO
SPECIFY
THE
NUMBER
OF
FREE
PARAMETERS
D
HINT
FOR
THE
DIGAONAL
CASE
THE
ML
ESTIMATE
OF
Σ
IS
THE
SAME
AS
Σˆ
ML
EXCEPT
THE
OFF
DIAGONAL
TERMS
ARE
ZERO
Σˆ
DIAG
DIAG
Σˆ
ML
Σˆ
ML
D
D
IN
THE
SCALAR
CASE
COMPLETING
THE
SQUARE
MEANS
REWRITING
AS
A
X
B
W
WHERE
A
B
AND
W
EXERCISE
GAUSSIAN
POSTERIOR
CREDIBLE
INTERVAL
SOURCE
DEGROOT
LET
X
Μ
WHERE
Μ
IS
UNKNOWN
BUT
HAS
PRIOR
Μ
THE
POSTERIOR
AFTER
SEEING
N
SAMPLES
IS
Μ
ΜN
THIS
IS
CALLED
A
CREDIBLE
INTERVAL
AND
IS
THE
BAYESIAN
ANALOG
OF
A
CONFIDENCE
INTERVAL
HOW
BIG
DOES
N
HAVE
TO
BE
TO
ENSURE
P
T
ΜN
U
D
WHERE
T
U
IS
AN
INTERVAL
CENTERED
ON
ΜN
OF
WIDTH
AND
D
IS
THE
DATA
HINT
RECALL
THAT
OF
THE
PROBABILITY
MASS
OF
A
GAUSSIAN
IS
WITHIN
OF
THE
MEAN
EXERCISE
MAP
ESTIMATION
FOR
GAUSSIANS
SOURCE
JAAKKOLA
CONSIDER
SAMPLES
XN
FROM
A
GAUSSIAN
RANDOM
VARIABLE
WITH
KNOWN
VARIANCE
AND
UNKNOWN
MEAN
Μ
WE
FURTHER
ASSUME
A
PRIOR
DISTRIBUTION
ALSO
GAUSSIAN
OVER
THE
MEAN
Μ
M
WITH
FIXED
MEAN
M
AND
FIXED
VARIANCE
THUS
THE
ONLY
UNKNOWN
IS
Μ
A
CALCULATE
THE
MAP
ESTIMATE
ΜˆMAP
YOU
CAN
STATE
THE
RESULT
WITHOUT
PROOF
ALTERNATIVELY
WITH
A
LOT
MORE
WORK
YOU
CAN
COMPUTE
DERIVATIVES
OF
THE
LOG
POSTERIOR
SET
TO
ZERO
AND
SOLVE
B
SHOW
THAT
AS
THE
NUMBER
OF
SAMPLES
N
INCREASE
THE
MAP
ESTIMATE
CONVERGES
TO
THE
MAXIMUM
LIKELIHOOD
ESTIMATE
C
SUPPOSE
N
IS
SMALL
AND
FIXED
WHAT
DOES
THE
MAP
ESTIMATOR
CONVERGE
TO
IF
WE
INCREASE
THE
PRIOR
VARIANCE
D
SUPPOSE
N
IS
SMALL
AND
FIXED
WHAT
DOES
THE
MAP
ESTIMATOR
CONVERGE
TO
IF
WE
DECREASE
THE
PRIOR
VARIANCE
EXERCISE
SEQUENTIAL
RECURSIVE
UPDATING
OF
Σˆ
SOURCE
DUDA
ET
AL
THE
UNBIASED
ESTIMATES
FOR
THE
COVARIANCE
OF
A
D
DIMENSIONAL
GAUSSIAN
BASED
ON
N
SAMPLES
IS
GIVEN
BY
Σˆ
CN
N
X
N
I
MN
XI
MN
T
IT
IS
CLEAR
THAT
IT
TAKES
O
TIME
TO
COMPUTE
CN
IF
THE
DATA
POINTS
ARRIVE
ONE
AT
A
TIME
IT
IS
MORE
EFFICIENT
TO
INCREMENTALLY
UPDATE
THESE
ESTIMATES
THAN
TO
RECOMPUTE
FROM
SCRATCH
A
SHOW
THAT
THE
COVARIANCE
CAN
BE
SEQUENTIALLY
UDPATED
AS
FOLLOWS
CN
N
C
N
X
N
N
MN
X
N
MN
T
B
HOW
MUCH
TIME
DOES
IT
TAKE
PER
SEQUENTIAL
UPDATE
USE
BIG
O
NOTATION
C
SHOW
THAT
WE
CAN
SEQUENTIALLY
UPDATE
THE
PRECISION
MATRIX
USING
N
R
C
N
XN
MN
XN
MN
T
C
N
L
CN
N
CN
X
N
MN
T
C
N
X
N
MN
HINT
NOTICE
THAT
THE
UPDATE
TO
CN
CONSISTS
OF
ADDING
A
RANK
ONE
MATRIX
NAMELY
UUT
WHERE
U
XN
MN
USE
THE
MATRIX
INVERSION
LEMMA
FOR
RANK
ONE
UPDATES
EQUATION
WHICH
WE
REPEAT
HERE
FOR
CONVENIENCE
E
UVT
E
E
E
VT
E
D
WHAT
IS
THE
TIME
COMPLEXITY
PER
UPDATE
EXERCISE
LIKELIHOOD
RATIO
FOR
GAUSSIANS
SOURCE
SOURCE
ALPAYDIN
EX
CONSIDER
A
BINARY
CLASSIFIER
WHERE
THE
K
CLASS
CONDITIONAL
DENSITIES
ARE
MVN
P
X
Y
J
N
X
ΜJ
ΣJ
BY
BAYES
RULE
WE
HAVE
LOG
P
Y
X
LOG
P
X
Y
LOG
P
Y
P
Y
X
P
X
Y
P
Y
IN
OTHER
WORDS
THE
LOG
POSTERIOR
RATIO
IS
THE
LOG
LIKELIHOOD
RATIO
PLUS
THE
LOG
PRIOR
RATIO
FOR
EACH
OF
THE
CASES
IN
THE
TABLE
BELOW
DERIVE
AN
EXPRESSION
FOR
THE
LOG
LIKELIHOOD
RATIO
LOG
P
X
Y
SIMPLIFYING
AS
MUCH
AS
POSSIBLE
P
X
Y
FORM
OF
ΣJ
COV
NUM
PARAMETERS
ARBITRARY
ΣJ
KD
D
SHARED
ΣJ
Σ
D
D
SHARED
AXIS
ALIGNED
ΣJ
Σ
WITH
ΣIJ
FOR
I
J
D
SHARED
SPHERICAL
ΣJ
EXERCISE
LDA
QDA
ON
HEIGHT
WEIGHT
DATA
THE
FUNCTION
DISCRIMANALYSISHEIGHTWEIGHTDEMO
FITS
AN
LDA
AND
QDA
MODEL
TO
THE
HEIGHT
WEIGHT
DATA
COMPUTE
THE
MISCLASSIFICATION
RATE
OF
BOTH
OF
THESE
MODELS
ON
THE
TRAINING
SET
TURN
IN
YOUR
NUMBERS
AND
CODE
EXERCISE
NAIVE
BAYES
WITH
MIXED
FEATURES
CONSIDER
A
CLASS
NAIVE
BAYES
CLASSIFIER
WITH
ONE
BINARY
FEATURE
AND
ONE
GAUSSIAN
FEATURE
Y
MU
Y
Π
Y
C
BER
ΘC
Y
C
N
ΜC
LET
THE
PARAMETER
VECTORS
BE
AS
FOLLOWS
Π
Θ
Μ
A
COMPUTE
P
Y
THE
RESULT
SHOULD
BE
A
VECTOR
OF
NUMBERS
THAT
SUMS
TO
B
COMPUTE
P
Y
C
COMPUTE
P
Y
D
EXPLAIN
ANY
INTERESTING
PATTERNS
YOU
SEE
IN
YOUR
RESULTS
HINT
LOOK
AT
THE
PARAMETER
VECTOR
Θ
EXERCISE
DECISION
BOUNDARY
FOR
LDA
WITH
SEMI
TIED
COVARIANCES
CONSIDER
A
GENERATIVE
CLASSIFIER
WITH
CLASS
CONDITIONAL
DENSITIES
OF
THE
FORM
X
ΜC
ΣC
IN
LDA
WE
ASSUME
ΣC
Σ
AND
IN
QDA
EACH
ΣC
IS
ARBITRARY
HERE
WE
CONSIDER
THE
CLASS
CASE
IN
WHICH
FOR
K
THAT
IS
THE
GAUSSIAN
ELLIPSOIDS
HAVE
THE
SAME
SHAPE
BUT
THE
ONE
FOR
CLASS
IS
WIDER
DERIVE
AN
EXPRESSION
FOR
P
Y
X
Θ
SIMPLIFYING
AS
MUCH
AS
POSSIBLE
GIVE
A
GEOMETRIC
INTERPRETATION
OF
YOUR
RESULT
IF
POSSIBLE
EXERCISE
LOGISTIC
REGRESSION
VS
LDA
QDA
SOURCE
JAAKKOLA
SUPPOSE
WE
TRAIN
THE
FOLLOWING
BINARY
CLASSIFIERS
VIA
MAXIMUM
LIKELIHOOD
A
GAUSSI
A
GENERATIVE
CLASSIFIER
WHERE
THE
CLASS
CONDITIONAL
DENSITIES
ARE
GAUSSIAN
WITH
BOTH
COVARIANCE
MATRICES
SET
TO
I
IDENTITY
MATRIX
I
E
P
X
Y
C
N
X
ΜC
I
WE
ASSUME
P
Y
IS
UNIFORM
B
GAUSSX
AS
FOR
GAUSSI
BUT
THE
COVARIANCE
MATRICES
ARE
UNCONSTRAINED
I
E
P
X
Y
C
N
X
ΜC
ΣC
C
LINLOG
A
LOGISTIC
REGRESSION
MODEL
WITH
LINEAR
FEATURES
D
QUADLOG
A
LOGISTIC
REGRESSION
MODEL
USING
LINEAR
AND
QUADRATIC
FEATURES
I
E
POLYNOMIAL
BASIS
FUNCTION
EXPANSION
OF
DEGREE
AFTER
TRAINING
WE
COMPUTE
THE
PERFORMANCE
OF
EACH
MODEL
M
ON
THE
TRAINING
SET
AS
FOLLOWS
L
M
LOG
P
Y
X
Θˆ
M
NOTE
THAT
THIS
IS
THE
CONDITIONAL
LOG
LIKELIHOOD
P
Y
X
Θˆ
AND
NOT
THE
JOINT
LOG
LIKELIHOOD
P
Y
X
Θˆ
WE
NOW
WANT
TO
COMPARE
THE
PERFORMANCE
OF
EACH
MODEL
WE
WILL
WRITE
L
M
L
M
I
IF
MODEL
M
MUST
HAVE
LOWER
OR
EQUAL
LOG
LIKELIHOOD
ON
THE
TRAINING
SET
THAN
M
I
FOR
ANY
TRAINING
SET
IN
OTHER
WORDS
M
IS
WORSE
THAN
M
I
AT
LEAST
AS
FAR
AS
TRAINING
SET
LOGPROB
IS
CONCERNED
FOR
EACH
OF
THE
FOLLOWING
MODEL
PAIRS
STATE
WHETHER
L
M
L
M
I
L
M
L
M
I
OR
WHETHER
NO
SUCH
STATEMENT
CAN
BE
MADE
I
E
M
MIGHT
SOMETIMES
BE
BETTER
THAN
M
I
AND
SOMETIMES
WORSE
ALSO
FOR
EACH
QUESTION
BRIEFLY
SENTENCES
EXPLAIN
WHY
A
GAUSSI
LINLOG
B
GAUSSX
QUADLOG
C
LINLOG
QUADLOG
D
GAUSSI
QUADLOG
E
NOW
SUPPOSE
WE
MEASURE
PERFORMANCE
IN
TERMS
OF
THE
AVERAGE
MISCLASSIFICATION
RATE
ON
THE
TRAINING
SET
N
R
M
I
Y
N
I
I
Yˆ
XI
IS
IT
TRUE
IN
GENERAL
THAT
L
M
L
M
I
IMPLIES
THAT
R
M
R
M
I
EXPLAIN
WHY
OR
WHY
NOT
EXERCISE
GAUSSIAN
DECISION
BOUNDARIES
SOURCE
DUDA
ET
AL
LET
P
X
Y
J
N
X
ΜJ
ΣJ
WHERE
J
AND
LET
THE
CLASS
PRIORS
BE
EQUAL
P
Y
P
Y
A
FIND
THE
DECISION
REGION
X
P
X
P
X
SKETCH
THE
RESULT
HINT
DRAW
THE
CURVES
AND
FIND
WHERE
THEY
INTERSECT
FIND
BOTH
SOLUTIONS
OF
THE
EQUATION
P
X
P
X
HINT
RECALL
THAT
TO
SOLVE
A
QUADRATIC
EQUATION
BX
C
WE
USE
B
X
B
NOW
SUPPOSE
AND
ALL
OTHER
PARAMETERS
REMAIN
THE
SAME
WHAT
IS
IN
THIS
CASE
EXERCISE
QDA
WITH
CLASSES
CONSIDER
A
THREE
CATEGORY
CLASSIFICATION
PROBLEM
LET
THE
PRIOR
PROBABILITES
P
Y
P
Y
P
Y
THE
CLASS
CONDITIONAL
DENSITIES
ARE
MULTIVARIATE
NORMAL
DENSITIES
WITH
PARAMETERS
T
T
T
L
L
L
CLASSIFY
THE
FOLLOWING
POINTS
A
X
B
X
EXERCISE
SCALAR
QDA
NOTE
YOU
CAN
SOLVE
THIS
EXERCISE
BY
HAND
OR
USING
A
COMPUTER
MATLAB
R
WHATEVER
IN
EITHER
CASE
SHOW
YOUR
WORK
CONSIDER
THE
FOLLOWING
TRAINING
SET
OF
HEIGHTS
X
IN
INCHES
AND
GENDER
Y
MALE
FEMALE
OF
SOME
US
COLLEGE
STUDENTS
X
Y
M
M
M
F
F
F
A
FIT
A
BAYES
CLASSIFIER
TO
THIS
DATA
USING
MAXIMUM
LIKELIHOOD
ESTIMATION
I
E
ESTIMATE
THE
PARAMETERS
OF
THE
CLASS
CONDITIONAL
LIKELIHOODS
P
X
Y
C
N
X
ΜC
ΣC
AND
THE
CLASS
PRIOR
P
Y
C
ΠC
WHAT
ARE
YOUR
VALUES
OF
ΜC
ΣC
ΠC
FOR
C
M
F
SHOW
YOUR
WORK
SO
YOU
CAN
GET
PARTIAL
CREDIT
IF
YOU
MAKE
AN
ARITHMETIC
ERROR
B
COMPUTE
P
Y
M
X
Θˆ
WHERE
X
AND
Θˆ
ARE
THE
MLE
PARAMETERS
THIS
IS
CALLED
A
PLUG
IN
PREDICTION
C
WHAT
WOULD
BE
A
SIMPLE
WAY
TO
EXTEND
THIS
TECHNIQUE
IF
YOU
HAD
MULTIPLE
ATTRIBUTES
PER
PERSON
SUCH
AS
HEIGHT
AND
WEIGHT
WRITE
DOWN
YOUR
PROPOSED
MODEL
AS
AN
EQUATION
BAYESIAN
STATISTICS
INTRODUCTION
WE
HAVE
NOW
SEEN
A
VARIETY
OF
DIFFERENT
PROBABILITY
MODELS
AND
WE
HAVE
DISCUSSED
HOW
TO
FIT
THEM
TO
DATA
I
E
WE
HAVE
DISCUSSED
HOW
TO
COMPUTE
MAP
PARAMETER
ESTIMATES
Θˆ
ARGMAX
P
Θ
USING
A
VARIETY
OF
DIFFERENT
PRIORS
WE
HAVE
ALSO
DISCUSSED
HOW
TO
COMPUTE
THE
FULL
POSTERIOR
P
Θ
AS
WELL
AS
THE
POSTERIOR
PREDICTIVE
DENSITY
P
X
FOR
CERTAIN
SPECIAL
CASES
AND
IN
LATER
CHAPTERS
WE
WILL
DISCUSS
ALGORITHMS
FOR
THE
GENERAL
CASE
USING
THE
POSTERIOR
DISTRIBUTION
TO
SUMMARIZE
EVERYTHING
WE
KNOW
ABOUT
A
SET
OF
UNKNOWN
VARIABLES
IS
AT
THE
CORE
OF
BAYESIAN
STATISTICS
IN
THIS
CHAPTER
WE
DISCUSS
THIS
APPROACH
TO
STATISTICS
IN
MORE
DETAIL
IN
CHAPTER
WE
DISCUSS
AN
ALTERNATIVE
APPROACH
TO
STATISTICS
KNOWN
AS
FREQUENTIST
OR
CLASSICAL
STATISTICS
SUMMARIZING
POSTERIOR
DISTRIBUTIONS
THE
POSTERIOR
P
Θ
SUMMARIZES
EVERYTHING
WE
KNOW
ABOUT
THE
UNKNOWN
QUANTITIES
Θ
IN
THIS
SECTION
WE
DISCUSS
SOME
SIMPLE
QUANTITIES
THAT
CAN
BE
DERIVED
FROM
A
PROBABILITY
DISTRIBUTION
SUCH
AS
A
POSTERIOR
THESE
SUMMARY
STATISTICS
ARE
OFTEN
EASIER
TO
UNDERSTAND
AND
VISUALIZE
THAN
THE
FULL
JOINT
MAP
ESTIMATION
WE
CAN
EASILY
COMPUTE
A
POINT
ESTIMATE
OF
AN
UNKNOWN
QUANTITY
BY
COMPUTING
THE
POSTERIOR
MEAN
MEDIAN
OR
MODE
IN
SECTION
WE
DISCUSS
HOW
TO
USE
DECISION
THEORY
TO
CHOOSE
BETWEEN
THESE
METHODS
TYPICALLY
THE
POSTERIOR
MEAN
OR
MEDIAN
IS
THE
MOST
APPROPRIATE
CHOICE
FOR
A
REAL
VALUED
QUANTITY
AND
THE
VECTOR
OF
POSTERIOR
MARGINALS
IS
THE
BEST
CHOICE
FOR
A
DISCRETE
QUANTITY
HOWEVER
THE
POSTERIOR
MODE
AKA
THE
MAP
ESTIMATE
IS
THE
MOST
POPULAR
CHOICE
BECAUSE
IT
REDUCES
TO
AN
OPTIMIZATION
PROBLEM
FOR
WHICH
EFFICIENT
ALGORITHMS
OFTEN
EXIST
FUTHERMORE
MAP
ESTIMATION
CAN
BE
INTERPRETED
IN
NON
BAYESIAN
TERMS
BY
THINKING
OF
THE
LOG
PRIOR
AS
A
REGULARIZER
SEE
SECTION
FOR
MORE
DETAILS
ALTHOUGH
THIS
APPROACH
IS
COMPUTATIONALLY
APPEALING
IT
IS
IMPORTANT
TO
POINT
OUT
THAT
THERE
ARE
VARIOUS
DRAWBACKS
TO
MAP
ESTIMATION
WHICH
WE
BRIEFLY
DISCUSS
BELOW
THIS
WILL
PROVIDE
MOTIVATION
FOR
THE
MORE
THOROUGHLY
BAYESIAN
APPROACH
WHICH
WE
WILL
STUDY
LATER
IN
THIS
CHAPTER
AND
ELSEWHERE
IN
THIS
BOOK
A
B
FIGURE
A
A
BIMODAL
DISTRIBUTION
IN
WHICH
THE
MODE
IS
VERY
UNTYPICAL
OF
THE
DISTRIBUTION
THE
THIN
BLUE
VERTICAL
LINE
IS
THE
MEAN
WHICH
IS
ARGUABLY
A
BETTER
SUMMARY
OF
THE
DISTRIBUTION
SINCE
IT
IS
NEAR
THE
MAJORITY
OF
THE
PROBABILITY
MASS
FIGURE
GENERATED
BY
BIMODALDEMO
B
A
SKEWED
DISTRIBUTION
IN
WHICH
THE
MODE
IS
QUITE
DIFFERENT
FROM
THE
MEAN
FIGURE
GENERATED
BY
GAMMAPLOTDEMO
NO
MEASURE
OF
UNCERTAINTY
THE
MOST
OBVIOUS
DRAWBACK
OF
MAP
ESTIMATION
AND
INDEED
OF
ANY
OTHER
POINT
ESTIMATE
SUCH
AS
THE
POSTERIOR
MEAN
OR
MEDIAN
IS
THAT
IT
DOES
NOT
PROVIDE
ANY
MEASURE
OF
UNCERTAINTY
IN
MANY
APPLICATIONS
IT
IS
IMPORTANT
TO
KNOW
HOW
MUCH
ONE
CAN
TRUST
A
GIVEN
ESTIMATE
WE
CAN
DERIVE
SUCH
CONFIDENCE
MEASURES
FROM
THE
POSTERIOR
AS
WE
DISCUSS
IN
SECTION
PLUGGING
IN
THE
MAP
ESTIMATE
CAN
RESULT
IN
OVERFITTING
IN
MACHINE
LEARNING
WE
OFTEN
CARE
MORE
ABOUT
PREDICTIVE
ACCURACY
THAN
IN
INTERPRETING
THE
PARAMETERS
OF
OUR
MODELS
HOWEVER
IF
WE
DON
T
MODEL
THE
UNCERTAINTY
IN
OUR
PARAMETERS
THEN
OUR
PREDICTIVE
DISTRIBUTION
WILL
BE
OVERCONFIDENT
WE
SAW
SEVERAL
EXAMPLES
OF
THIS
IN
CHAPTER
AND
WE
WILL
SEE
MORE
EXAMPLES
LATER
OVERCONFIDENCE
IN
PREDICTIONS
IS
PARTICULARLY
PROBLEMATIC
IN
SITUATIONS
WHERE
WE
MAY
BE
RISK
AVERSE
SEE
SECTION
FOR
DETAILS
THE
MODE
IS
AN
UNTYPICAL
POINT
CHOOSING
THE
MODE
AS
A
SUMMARY
OF
A
POSTERIOR
DISTRIBUTION
IS
OFTEN
A
VERY
POOR
CHOICE
SINCE
THE
MODE
IS
USUALLY
QUITE
UNTYPICAL
OF
THE
DISTRIBUTION
UNLIKE
THE
MEAN
OR
MEDIAN
THIS
IS
ILLUSTRATED
IN
FIGURE
A
FOR
A
CONTINUOUS
SPACE
THE
BASIC
PROBLEM
IS
THAT
THE
MODE
IS
A
POINT
OF
MEASURE
ZERO
WHEREAS
THE
MEAN
AND
MEDIAN
TAKE
THE
VOLUME
OF
THE
SPACE
INTO
ACCOUNT
ANOTHER
EXAMPLE
IS
SHOWN
IN
FIGURE
B
HERE
THE
MODE
IS
BUT
THE
MEAN
IS
NON
ZERO
SUCH
SKEWED
DISTRIBUTIONS
OFTEN
ARISE
WHEN
INFERRING
VARIANCE
PARAMETERS
ESPECIALLY
IN
HIERARCHICAL
MODELS
IN
SUCH
CASES
THE
MAP
ESTIMATE
AND
HENCE
THE
MLE
IS
OBVIOUSLY
A
VERY
BAD
ESTIMATE
HOW
SHOULD
WE
SUMMARIZE
A
POSTERIOR
IF
THE
MODE
IS
NOT
A
GOOD
CHOICE
THE
ANSWER
IS
TO
USE
DECISION
THEORY
WHICH
WE
DISCUSS
IN
SECTION
THE
BASIC
IDEA
IS
TO
SPECIFY
A
LOSS
FUNCTION
WHERE
L
Θ
Θˆ
IS
THE
LOSS
YOU
INCUR
IF
THE
TRUTH
IS
Θ
AND
YOUR
ESTIMATE
IS
Θˆ
IF
WE
USE
LOSS
L
Θ
Θˆ
I
Θ
Θˆ
THEN
THE
OPTIMAL
ESTIMATE
IS
THE
POSTERIOR
MODE
LOSS
MEANS
YOU
ONLY
GET
POINTS
IF
YOU
MAKE
NO
ERRORS
OTHERWISE
YOU
GET
NOTHING
THERE
IS
NO
PARTIAL
CREDIT
UNDER
FIGURE
EXAMPLE
OF
THE
TRANSFORMATION
OF
A
DENSITY
UNDER
A
NONLINEAR
TRANSFORM
NOTE
HOW
THE
MODE
OF
THE
TRANSFORMED
DISTRIBUTION
IS
NOT
THE
TRANSFORM
OF
THE
ORIGINAL
MODE
BASED
ON
EXERCISE
OF
BISHOP
FIGURE
GENERATED
BY
BAYESCHANGEOFVAR
THIS
LOSS
FUNCTION
FOR
CONTINUOUS
VALUED
QUANTITIES
WE
OFTEN
PREFER
TO
USE
SQUARED
ERROR
LOSS
L
Θ
Θˆ
Θ
Θˆ
THE
CORRESPONDING
OPTIMAL
ESTIMATOR
IS
THEN
THE
POSTERIOR
MEAN
AS
WE
SHOW
IN
SECTION
OR
WE
CAN
USE
A
MORE
ROBUST
LOSS
FUNCTION
L
Θ
Θˆ
Θ
Θˆ
WHICH
GIVES
RISE
TO
THE
POSTERIOR
MEDIAN
MAP
ESTIMATION
IS
NOT
INVARIANT
TO
REPARAMETERIZATION
A
MORE
SUBTLE
PROBLEM
WITH
MAP
ESTIMATION
IS
THAT
THE
RESULT
WE
GET
DEPENDS
ON
HOW
WE
PA
RAMETERIZE
THE
PROBABILITY
DISTRIBUTION
CHANGING
FROM
ONE
REPRESENTATION
TO
ANOTHER
EQUIVALENT
REPRESENTATION
CHANGES
THE
RESULT
WHICH
IS
NOT
VERY
DESIRABLE
SINCE
THE
UNITS
OF
MEASUREMENT
ARE
ARBITRARY
E
G
WHEN
MEASURING
DISTANCE
WE
CAN
USE
CENTIMETRES
OR
INCHES
TO
UNDERSTAND
THE
PROBLEM
SUPPOSE
WE
COMPUTE
THE
POSTERIOR
FOR
X
IF
WE
DEFINE
Y
F
X
THE
DISTRIBUTION
FOR
Y
IS
GIVEN
BY
EQUATION
WHICH
WE
REPEAT
HERE
FOR
CONVENIENCE
PY
Y
PX
X
DY
THE
DX
TERM
IS
CALLED
THE
JACOBIAN
AND
IT
MEASURES
THE
CHANGE
IN
SIZE
OF
A
UNIT
VOLUME
PASSED
THROUGH
F
LET
Xˆ
ARGMAXX
PX
X
BE
THE
MAP
ESTIMATE
FOR
X
IN
GENERAL
IT
IS
NOT
THE
CASE
THAT
Yˆ
ARGMAXY
PY
Y
IS
GIVEN
BY
F
Xˆ
FOR
EXAMPLE
LET
X
N
AND
Y
F
X
WHERE
F
X
EXP
X
WE
CAN
DERIVE
THE
DISTRIBUTION
OF
Y
USING
MONTE
CARLO
SIMULATION
SEE
SECTION
THE
RESULT
IS
SHOWN
IN
FIGURE
WE
SEE
THAT
THE
ORIGINAL
GAUSSIAN
HAS
BECOME
SQUASHED
BY
THE
SIGMOID
NONLINEARITY
IN
PARTICULAR
WE
SEE
THAT
THE
MODE
OF
THE
TRANSFORMED
DISTRIBUTION
IS
NOT
EQUAL
TO
THE
TRANSFORM
OF
THE
ORIGINAL
MODE
TO
SEE
HOW
THIS
PROBLEM
ARISES
IN
THE
CONTEXT
OF
MAP
ESTIMATION
CONSIDER
THE
FOLLOWING
EXAMPLE
DUE
TO
MICHAEL
JORDAN
THE
BERNOULLI
DISTRIBUTION
IS
TYPICALLY
PARAMETERIZED
BY
ITS
MEAN
Μ
SO
P
Y
Μ
Μ
WHERE
Y
SUPPOSE
WE
HAVE
A
UNIFORM
PRIOR
ON
THE
UNIT
INTERVAL
PΜ
Μ
I
Μ
IF
THERE
IS
NO
DATA
THE
MAP
ESTIMATE
IS
JUST
THE
MODE
OF
THE
PRIOR
WHICH
CAN
BE
ANYWHERE
BETWEEN
AND
WE
WILL
NOW
SHOW
THAT
DIFFERENT
PARAMETERIZATIONS
CAN
PICK
DIFFERENT
POINTS
IN
THIS
INTERVAL
ARBITRARILY
FIRST
LET
Θ
Μ
SO
Μ
THE
NEW
PRIOR
IS
PΘ
Θ
PΜ
Μ
DΘ
FOR
Θ
SO
THE
NEW
MODE
IS
ΘˆMAP
ARG
MAX
Θ
NOW
LET
Φ
Μ
THE
NEW
PRIOR
IS
PΦ
Φ
PΜ
Μ
DΦ
Φ
FOR
Φ
SO
THE
NEW
MODE
IS
ΦˆMAP
ARG
MAX
Φ
THUS
THE
MAP
ESTIMATE
DEPENDS
ON
THE
PARAMETERIZATION
THE
MLE
DOES
NOT
SUFFER
FROM
THIS
SINCE
THE
LIKELIHOOD
IS
A
FUNCTION
NOT
A
PROBABILITY
DENSITY
BAYESIAN
INFERENCE
DOES
NOT
SUFFER
FROM
THIS
PROBLEM
EITHER
SINCE
THE
CHANGE
OF
MEASURE
IS
TAKEN
INTO
ACCOUNT
WHEN
INTEGRATING
OVER
THE
PARAMETER
SPACE
ONE
SOLUTION
TO
THE
PROBLEM
IS
TO
OPTIMIZE
THE
FOLLOWING
OBJECTIVE
FUNCTION
Θ
ARGMAX
P
Θ
P
Θ
I
Θ
Θ
HERE
I
Θ
IS
THE
FISHER
INFORMATION
MATRIX
ASSOCIATED
WITH
P
X
Θ
SEE
SECTION
THIS
ESTIMATE
IS
PARAMETERIZATION
INDEPENDENT
FOR
REASONS
EXPLAINED
IN
JERMYN
DRUILHET
AND
MARIN
UNFORTUNATELY
OPTIMIZING
EQUATION
IS
OFTEN
DIFFICULT
WHICH
MINIMIZES
THE
APPEAL
OF
THE
WHOLE
APPROACH
CREDIBLE
INTERVALS
IN
ADDITION
TO
POINT
ESTIMATES
WE
OFTEN
WANT
A
MEASURE
OF
CONFIDENCE
A
STANDARD
MEASURE
OF
CONFIDENCE
IN
SOME
SCALAR
QUANTITY
Θ
IS
THE
WIDTH
OF
ITS
POSTERIOR
DISTRIBUTION
THIS
CAN
BE
MEASURED
USING
A
Α
CREDIBLE
INTERVAL
WHICH
IS
A
CONTIGUOUS
REGION
C
U
STANDING
FOR
LOWER
AND
UPPER
WHICH
CONTAINS
Α
OF
THE
POSTERIOR
PROBABILITY
MASS
I
E
CΑ
D
U
P
Θ
U
D
Α
THERE
MAY
BE
MANY
SUCH
INTERVALS
SO
WE
CHOOSE
ONE
SUCH
THAT
THERE
IS
Α
MASS
IN
EACH
TAIL
THIS
IS
CALLED
A
CENTRAL
INTERVAL
A
B
FIGURE
A
CENTRAL
INTERVAL
AND
B
HPD
REGION
FOR
A
BETA
POSTERIOR
THE
CI
IS
AND
THE
HPD
IS
BASED
ON
FIGURE
OF
HOFF
FIGURE
GENERATED
BY
BETAHPD
IF
THE
POSTERIOR
HAS
A
KNOWN
FUNCTIONAL
FORM
WE
CAN
COMPUTE
THE
POSTERIOR
CENTRAL
INTERVAL
USING
F
Α
AND
U
F
Α
WHERE
F
IS
THE
CDF
OF
THE
POSTERIOR
FOR
EXAMPLE
IF
THE
POSTERIOR
IS
GAUSSIAN
P
Θ
AND
Α
THEN
WE
HAVE
Φ
Α
AND
U
Φ
Α
WHERE
Φ
DENOTES
THE
CDF
OF
THE
GAUSSIAN
THIS
IS
ILLUSTRATED
IN
FIGURE
C
THIS
JUSTIFIES
THE
COMMON
PRACTICE
OF
QUOTING
A
CREDIBLE
INTERVAL
IN
THE
FORM
OF
Μ
WHERE
Μ
REPRESENTS
THE
POSTERIOR
MEAN
Σ
REPRESENTS
THE
POSTERIOR
STANDARD
DEVIATION
AND
IS
A
GOOD
APPROXIMATION
TO
OF
COURSE
THE
POSTERIOR
IS
NOT
ALWAYS
GAUSSIAN
FOR
EXAMPLE
IN
OUR
COIN
EXAMPLE
IF
WE
USE
A
UNIFORM
PRIOR
AND
WE
OBSERVE
HEADS
OUT
OF
N
TRIALS
THEN
THE
POSTERIOR
IS
A
BETA
DISTRIBUTION
P
Θ
BETA
WE
FIND
THE
POSTERIOR
CREDIBLE
INTERVAL
IS
SEE
BETACREDIBLEINT
FOR
THE
ONE
LINE
OF
MATLAB
CODE
WE
USED
TO
COMPUTE
THIS
IF
WE
DON
T
KNOW
THE
FUNCTIONAL
FORM
BUT
WE
CAN
DRAW
SAMPLES
FROM
THE
POSTERIOR
THEN
WE
CAN
USE
A
MONTE
CARLO
APPROXIMATION
TO
THE
POSTERIOR
QUANTILES
WE
SIMPLY
SORT
THE
SAMPLES
AND
FIND
THE
ONE
THAT
OCCURS
AT
LOCATION
Α
ALONG
THE
SORTED
LIST
AS
THIS
CONVERGES
TO
THE
TRUE
QUANTILE
SEE
MCQUANTILEDEMO
FOR
A
DEMO
PEOPLE
OFTEN
CONFUSE
BAYESIAN
CREDIBLE
INTERVALS
WITH
FREQUENTIST
CONFIDENCE
INTERVALS
HOW
EVER
THEY
ARE
NOT
THE
SAME
THING
AS
WE
DISCUSS
IN
SECTION
IN
GENERAL
CREDIBLE
INTERVALS
ARE
USUALLY
WHAT
PEOPLE
WANT
TO
COMPUTE
BUT
CONFIDENCE
INTERVALS
ARE
USUALLY
WHAT
THEY
ACTUALLY
COMPUTE
BECAUSE
MOST
PEOPLE
ARE
TAUGHT
FREQUENTIST
STATISTICS
BUT
NOT
BAYESIAN
STATISTICS
FORTU
NATELY
THE
MECHANICS
OF
COMPUTING
A
CREDIBLE
INTERVAL
IS
JUST
AS
EASY
AS
COMPUTING
A
CONFIDENCE
INTERVAL
SEE
E
G
BETACREDIBLEINT
FOR
HOW
TO
DO
IT
IN
MATLAB
HIGHEST
POSTERIOR
DENSITY
REGIONS
A
PROBLEM
WITH
CENTRAL
INTERVALS
IS
THAT
THERE
MIGHT
BE
POINTS
OUTSIDE
THE
CI
WHICH
HAVE
HIGHER
PROBABILITY
DENSITY
THIS
IS
ILLUSTRATED
IN
FIGURE
A
WHERE
WE
SEE
THAT
POINTS
OUTSIDE
THE
LEFT
MOST
CI
BOUNDARY
HAVE
HIGHER
DENSITY
THAN
THOSE
JUST
INSIDE
THE
RIGHT
MOST
CI
BOUNDARY
THIS
MOTIVATES
AN
ALTERNATIVE
QUANTITY
KNOWN
AS
THE
HIGHEST
POSTERIOR
DENSITY
OR
HPD
REGION
THIS
IS
DEFINED
AS
THE
SET
OF
MOST
PROBABLE
POINTS
THAT
IN
TOTAL
CONSTITUTE
Α
OF
THE
A
PMIN
B
FIGURE
A
CENTRAL
INTERVAL
AND
B
HPD
REGION
FOR
A
HYPOTHETICAL
MULTIMODAL
POSTERIOR
BASED
ON
FIGURE
OF
GELMAN
ET
AL
FIGURE
GENERATED
BY
POSTDENSITYINTERVALS
PROBABILITY
MASS
MORE
FORMALLY
WE
FIND
THE
THRESHOLD
P
ON
THE
PDF
SUCH
THAT
Α
R
Θ
P
Θ
D
P
P
Θ
D
DΘ
AND
THEN
DEFINE
THE
HPD
AS
CΑ
D
Θ
P
Θ
D
P
IN
THE
HPD
REGION
IS
SOMETIMES
CALLED
A
HIGHEST
DENSITY
INTERVAL
OR
HDI
FOR
EXAMPLE
FIGURE
B
SHOWS
THE
HDI
OF
A
BETA
DISTRIBUTION
WHICH
IS
WE
SEE
THAT
THIS
IS
NARROWER
THAN
THE
CI
EVEN
THOUGH
IT
STILL
CONTAINS
OF
THE
MASS
FURTHERMORE
EVERY
POINT
INSIDE
OF
IT
HAS
HIGHER
DENSITY
THAN
EVERY
POINT
OUTSIDE
OF
IT
FOR
A
UNIMODAL
DISTRIBUTION
THE
HDI
WILL
BE
THE
NARROWEST
INTERVAL
AROUND
THE
MODE
CONTAIN
ING
OF
THE
MASS
TO
SEE
THIS
IMAGINE
WATER
FILLING
IN
REVERSE
WHERE
WE
LOWER
THE
LEVEL
UNTIL
OF
THE
MASS
IS
REVEALED
AND
ONLY
IS
SUBMERGED
THIS
GIVES
A
SIMPLE
ALGORITHM
FOR
COMPUTING
HDIS
IN
THE
CASE
SIMPLY
SEARCH
OVER
POINTS
SUCH
THAT
THE
INTERVAL
CONTAINS
OF
THE
MASS
AND
HAS
MINIMAL
WIDTH
THIS
CAN
BE
DONE
BY
NUMERICAL
OPTIMIZATION
IF
WE
KNOW
THE
INVERSE
CDF
OF
THE
DISTRIBUTION
OR
BY
SEARCH
OVER
THE
SORTED
DATA
POINTS
IF
WE
HAVE
A
BAG
OF
SAMPLES
SEE
BETAHPD
FOR
A
DEMO
IF
THE
POSTERIOR
IS
MULTIMODAL
THE
HDI
MAY
NOT
EVEN
BE
A
CONNECTED
REGION
SEE
FIGURE
B
FOR
AN
EXAMPLE
HOWEVER
SUMMARIZING
MULTIMODAL
POSTERIORS
IS
ALWAYS
DIFFICULT
INFERENCE
FOR
A
DIFFERENCE
IN
PROPORTIONS
SOMETIMES
WE
HAVE
MULTIPLE
PARAMETERS
AND
WE
ARE
INTERESTED
IN
COMPUTING
THE
POSTERIOR
DISTRIBUTION
OF
SOME
FUNCTION
OF
THESE
PARAMETERS
FOR
EXAMPLE
SUPPOSE
YOU
ARE
ABOUT
TO
BUY
SOMETHING
FROM
AMAZON
COM
AND
THERE
ARE
TWO
SELLERS
OFFERING
IT
FOR
THE
SAME
PRICE
SELLER
HAS
POSITIVE
REVIEWS
AND
NEGATIVE
REVIEWS
SELLER
HAS
POSITIVE
REVIEWS
AND
NEGATIVE
REVIEWS
WHO
SHOULD
YOU
BUY
FROM
THIS
EXAMPLE
IS
FROM
WWW
JOHNDCOOK
COM
BLOG
BAYESIAN
AMAZON
SEE
ALSO
LINGPIPE
BLOG
C
OM
BAYESIAN
COUNTERPART
TO
FISHER
EXACT
TEST
ON
CONTINGENCY
TABLES
A
B
FIGURE
A
EXACT
POSTERIORS
P
ΘI
I
B
MONTE
CARLO
APPROXIMATION
TO
P
Δ
WE
USE
KERNEL
DENSITY
ESTIMATION
TO
GET
A
SMOOTH
PLOT
THE
VERTICAL
LINES
ENCLOSE
THE
CENTRAL
INTERVAL
FIGURE
GENERATED
BY
AMAZONSELLERDEMO
ON
THE
FACE
OF
IT
YOU
SHOULD
PICK
SELLER
BUT
WE
CANNOT
BE
VERY
CONFIDENT
THAT
SELLER
IS
BETTER
SINCE
IT
HAS
HAD
SO
FEW
REVIEWS
IN
THIS
SECTION
WE
SKETCH
A
BAYESIAN
ANALYSIS
OF
THIS
PROBLEM
SIMILAR
METHODOLOGY
CAN
BE
USED
TO
COMPARE
RATES
OR
PROPORTIONS
ACROSS
GROUPS
FOR
A
VARIETY
OF
OTHER
SETTINGS
LET
AND
BE
THE
UNKNOWN
RELIABILITIES
OF
THE
TWO
SELLERS
SINCE
WE
DON
T
KNOW
MUCH
ABOUT
THEM
WE
LL
ENDOW
THEM
BOTH
WITH
UNIFORM
PRIORS
ΘI
BETA
THE
POSTERIORS
ARE
P
BETA
AND
P
BETA
WE
WANT
TO
COMPUTE
P
FOR
CONVENIENCE
LET
US
DEFINE
Δ
AS
THE
DIFFERENCE
IN
THE
RATES
ALTERNATIVELY
WE
MIGHT
WANT
TO
WORK
IN
TERMS
OF
THE
LOG
ODDS
RATIO
WE
CAN
COMPUTE
THE
DESIRED
QUANTITY
USING
NUMERICAL
INTEGRATION
R
R
BETA
WE
FIND
P
Δ
WHICH
MEANS
YOU
ARE
BETTER
OFF
BUYING
FROM
SELLER
SEE
AMAZONSELLERDEMO
FOR
THE
CODE
IT
IS
ALSO
POSSIBLE
TO
SOLVE
THE
INTEGRAL
ANALYTICALLY
COOK
A
SIMPLER
WAY
TO
SOLVE
THE
PROBLEM
IS
TO
APPROXIMATE
THE
POSTERIOR
P
Δ
BY
MONTE
CARLO
SAMPLING
THIS
IS
EASY
SINCE
AND
ARE
INDEPENDENT
IN
THE
POSTERIOR
AND
BOTH
HAVE
BETA
DISTRIBUTIONS
WHICH
CAN
BE
SAMPLED
FROM
USING
STANDARD
METHODS
THE
DISTRIBUTIONS
P
ΘI
I
ARE
SHOWN
IN
FIGURE
A
AND
A
MC
APPROXIMATION
TO
P
Δ
TOGETHER
WITH
A
HPD
IS
SHOWN
FIGURE
B
AN
MC
APPROXIMATION
TO
P
Δ
IS
OBTAINED
BY
COUNTING
THE
FRACTION
OF
SAMPLES
WHERE
THIS
TURNS
OUT
TO
BE
WHICH
IS
VERY
CLOSE
TO
THE
EXACT
VALUE
SEE
AMAZONSELLERDEMO
FOR
THE
CODE
BAYESIAN
MODEL
SELECTION
IN
FIGURE
WE
SAW
THAT
USING
TOO
HIGH
A
DEGREE
POLYNOMIAL
RESULTS
IN
OVERFITTING
AND
USING
TOO
LOW
A
DEGREE
RESULTS
IN
UNDERFITTING
SIMILARLY
IN
FIGURE
A
WE
SAW
THAT
USING
TOO
SMALL
A
REGULARIZATION
PARAMETER
RESULTS
IN
OVERFITTING
AND
TOO
LARGE
A
VALUE
RESULTS
IN
UNDERFITTING
IN
GENERAL
WHEN
FACED
WITH
A
SET
OF
MODELS
I
E
FAMILIES
OF
PARAMETRIC
DISTRIBUTIONS
OF
DIFFERENT
COMPLEXITY
HOW
SHOULD
WE
CHOOSE
THE
BEST
ONE
THIS
IS
CALLED
THE
MODEL
SELECTION
PROBLEM
ONE
APPROACH
IS
TO
USE
CROSS
VALIDATION
TO
ESTIMATE
THE
GENERALIZATION
ERROR
OF
ALL
THE
CANDIATE
MODELS
AND
THEN
TO
PICK
THE
MODEL
THAT
SEEMS
THE
BEST
HOWEVER
THIS
REQUIRES
FITTING
EACH
MODEL
K
TIMES
WHERE
K
IS
THE
NUMBER
OF
CV
FOLDS
A
MORE
EFFICIENT
APPROACH
IS
TO
COMPUTE
THE
POSTERIOR
OVER
MODELS
P
D
M
P
M
P
M
D
M
M
P
M
D
FROM
THIS
WE
CAN
EASILY
COMPUTE
THE
MAP
MODEL
Mˆ
BAYESIAN
MODEL
SELECTION
ARGMAX
P
M
D
THIS
IS
CALLED
IF
WE
USE
A
UNIFORM
PRIOR
OVER
MODELS
P
M
THIS
AMOUNTS
TO
PICKING
THE
MODEL
WHICH
MAXIMIZES
P
D
M
R
P
D
Θ
P
Θ
M
DΘ
THIS
QUANTITY
IS
CALLED
THE
MARGINAL
LIKELIHOOD
THE
INTEGRATED
LIKELIHOOD
OR
THE
EVIDENCE
FOR
MODEL
M
THE
DETAILS
ON
HOW
TO
PERFORM
THIS
INTEGRAL
WILL
BE
DISCUSSED
IN
SECTION
BUT
FIRST
WE
GIVE
AN
INTUITIVE
INTERPRETATION
OF
WHAT
THIS
QUANTITY
MEANS
BAYESIAN
OCCAM
RAZOR
ONE
MIGHT
THINK
THAT
USING
P
D
M
TO
SELECT
MODELS
WOULD
ALWAYS
FAVOR
THE
MODEL
WITH
THE
MOST
PARAMETERS
THIS
IS
TRUE
IF
WE
USE
P
ΘˆM
TO
SELECT
MODELS
WHERE
ΘˆM
IS
THE
MLE
OR
MAP
ESTIMATE
OF
THE
PARAMETERS
FOR
MODEL
M
BECAUSE
MODELS
WITH
MORE
PARAMETERS
WILL
FIT
THE
DATA
BETTER
AND
HENCE
ACHIEVE
HIGHER
LIKELIHOOD
HOWEVER
IF
WE
INTEGRATE
OUT
THE
PARAMETERS
RATHER
THAN
MAXIMIZING
THEM
WE
ARE
AUTOMATICALLY
PROTECTED
FROM
OVERFITTING
MODELS
WITH
MORE
PARAMETERS
DO
NOT
NECESSARILY
HAVE
HIGHER
MARGINAL
LIKELIHOOD
THIS
IS
CALLED
THE
BAYESIAN
OCCAM
RAZOR
EFFECT
MACKAY
MURRAY
AND
GHAHRAMANI
NAMED
AFTER
THE
PRINCIPLE
KNOWN
AS
OCCAM
RAZOR
WHICH
SAYS
ONE
SHOULD
PICK
THE
SIMPLEST
MODEL
THAT
ADEQUATELY
EXPLAINS
THE
DATA
ONE
WAY
TO
UNDERSTAND
THE
BAYESIAN
OCCAM
RAZOR
IS
TO
NOTICE
THAT
THE
MARGINAL
LIKELIHOOD
CAN
BE
REWRITTEN
AS
FOLLOWS
BASED
ON
THE
CHAIN
RULE
OF
PROBABILITY
EQUATION
P
D
P
P
P
P
YN
N
WHERE
WE
HAVE
DROPPED
THE
CONDITIONING
ON
X
FOR
BREVITY
THIS
IS
SIMILAR
TO
A
LEAVE
ONE
OUT
CROSS
VALIDATION
ESTIMATE
SECTION
OF
THE
LIKELIHOOD
SINCE
WE
PREDICT
EACH
FUTURE
POINT
GIVEN
ALL
THE
PREVIOUS
ONES
OF
COURSE
THE
ORDER
OF
THE
DATA
DOES
NOT
MATTER
IN
THE
ABOVE
EXPRESSION
IF
A
MODEL
IS
TOO
COMPLEX
IT
WILL
OVERFIT
THE
EARLY
EXAMPLES
AND
WILL
THEN
PREDICT
THE
REMAINING
ONES
POORLY
ANOTHER
WAY
TO
UNDERSTAND
THE
BAYESIAN
OCCAM
RAZOR
EFFECT
IS
TO
NOTE
THAT
PROBABILITIES
MUST
SUM
TO
ONE
HENCE
T
P
M
WHERE
THE
SUM
IS
OVER
ALL
POSSIBLE
DATA
SETS
COMPLEX
MODELS
WHICH
CAN
PREDICT
MANY
THINGS
MUST
SPREAD
THEIR
PROBABILITY
MASS
THINLY
AND
HENCE
WILL
NOT
OBTAIN
AS
LARGE
A
PROBABILITY
FOR
ANY
GIVEN
DATA
SET
AS
SIMPLER
MODELS
THIS
IS
SOMETIMES
FIGURE
A
SCHEMATIC
ILLUSTRATION
OF
THE
BAYESIAN
OCCAM
RAZOR
THE
BROAD
GREEN
CURVE
CORRESPONDS
TO
A
COMPLEX
MODEL
THE
NARROW
BLUE
CURVE
TO
A
SIMPLE
MODEL
AND
THE
MIDDLE
RED
CURVE
IS
JUST
RIGHT
BASED
ON
FIGURE
OF
BISHOP
SEE
ALSO
MURRAY
AND
GHAHRAMANI
FIGURE
FOR
A
SIMILAR
PLOT
PRODUCED
ON
REAL
DATA
CALLED
THE
CONSERVATION
OF
PROBABILITY
MASS
PRINCIPLE
AND
IS
ILLUSTRATED
IN
FIGURE
ON
THE
HORIZONTAL
AXIS
WE
PLOT
ALL
POSSIBLE
DATA
SETS
IN
ORDER
OF
INCREASING
COMPLEXITY
MEASURED
IN
SOME
ABSTRACT
SENSE
ON
THE
VERTICAL
AXIS
WE
PLOT
THE
PREDICTIONS
OF
POSSIBLE
MODELS
A
SIMPLE
ONE
A
MEDIUM
ONE
AND
A
COMPLEX
ONE
WE
ALSO
INDICATE
THE
ACTUALLY
OBSERVED
DATA
BY
A
VERTICAL
LINE
MODEL
IS
TOO
SIMPLE
AND
ASSIGNS
LOW
PROBABILITY
TO
MODEL
ALSO
ASSIGNS
RELATIVELY
LOW
PROBABILITY
BECAUSE
IT
CAN
PREDICT
MANY
DATA
SETS
AND
HENCE
IT
SPREADS
ITS
PROBABILITY
QUITE
WIDELY
AND
THINLY
MODEL
IS
JUST
RIGHT
IT
PREDICTS
THE
OBSERVED
DATA
WITH
A
REASONABLE
DEGREE
OF
CONFIDENCE
BUT
DOES
NOT
PREDICT
TOO
MANY
OTHER
THINGS
HENCE
MODEL
IS
THE
MOST
PROBABLE
MODEL
AS
A
CONCRETE
EXAMPLE
OF
THE
BAYESIAN
OCCAM
RAZOR
CONSIDER
THE
DATA
IN
FIGURE
WE
PLOT
POLYNOMIALS
OF
DEGREES
AND
FIT
TO
N
DATA
POINTS
IT
ALSO
SHOWS
THE
POSTERIOR
OVER
MODELS
WHERE
WE
USE
A
GAUSSIAN
PRIOR
SEE
SECTION
FOR
DETAILS
THERE
IS
NOT
ENOUGH
DATA
TO
JUSTIFY
A
COMPLEX
MODEL
SO
THE
MAP
MODEL
IS
D
FIGURE
SHOWS
WHAT
HAPPENS
WHEN
N
NOW
IT
IS
CLEAR
THAT
D
IS
THE
RIGHT
MODEL
THE
DATA
WAS
IN
FACT
GENERATED
FROM
A
QUADRATIC
AS
ANOTHER
EXAMPLE
FIGURE
C
PLOTS
LOG
P
Λ
VS
LOG
Λ
FOR
THE
POLYNOMIAL
RIDGE
REGRES
SION
MODEL
WHERE
Λ
RANGES
OVER
THE
SAME
SET
OF
VALUES
USED
IN
THE
CV
EXPERIMENT
WE
SEE
THAT
THE
MAXIMUM
EVIDENCE
OCCURS
AT
ROUGHLY
THE
SAME
POINT
AS
THE
MINIMUM
OF
THE
TEST
MSE
WHICH
ALSO
CORRESPONDS
TO
THE
POINT
CHOSEN
BY
CV
WHEN
USING
THE
BAYESIAN
APPROACH
WE
ARE
NOT
RESTRICTED
TO
EVALUATING
THE
EVIDENCE
AT
A
FINITE
GRID
OF
VALUES
INSTEAD
WE
CAN
USE
NUMERICAL
OPTIMIZATION
TO
FIND
Λ
ARGMAXΛ
P
Λ
THIS
TECHNIQUE
IS
CALLED
EMPIRICAL
BAYES
OR
TYPE
II
MAXIMUM
LIKELIHOOD
SEE
SECTION
FOR
DETAILS
AN
EXAMPLE
IS
SHOWN
IN
FIGURE
B
WE
SEE
THAT
THE
CURVE
HAS
A
SIMILAR
SHAPE
TO
THE
CV
ESTIMATE
BUT
IT
CAN
BE
COMPUTED
MORE
EFFICIENTLY
D
LOGEV
EB
D
LOGEV
EB
A
D
LOGEV
EB
B
N
METHOD
EB
C
M
D
FIGURE
A
C
WE
PLOT
POLYNOMIALS
OF
DEGREES
AND
FIT
TO
N
DATA
POINTS
USING
EMPIRICAL
BAYES
THE
SOLID
GREEN
CURVE
IS
THE
TRUE
FUNCTION
THE
DASHED
RED
CURVE
IS
THE
PREDICTION
DOTTED
BLUE
LINES
REPRESENT
Σ
AROUND
THE
MEAN
D
WE
PLOT
THE
POSTERIOR
OVER
MODELS
P
D
D
ASSUMING
A
UNIFORM
PRIOR
P
D
BASED
ON
A
FIGURE
BY
ZOUBIN
GHAHRAMANI
FIGURE
GENERATED
BY
LINREGEBMODELSELVSN
COMPUTING
THE
MARGINAL
LIKELIHOOD
EVIDENCE
WHEN
DISCUSSING
PARAMETER
INFERENCE
FOR
A
FIXED
MODEL
WE
OFTEN
WROTE
P
Θ
D
M
P
Θ
M
P
D
Θ
M
THUS
IGNORING
THE
NORMALIZATION
CONSTANT
P
M
THIS
IS
VALID
SINCE
P
M
IS
CONSTANT
WRT
Θ
HOWEVER
WHEN
COMPARING
MODELS
WE
NEED
TO
KNOW
HOW
TO
COMPUTE
THE
MARGINAL
LIKELIHOOD
P
M
IN
GENERAL
THIS
CAN
BE
QUITE
HARD
SINCE
WE
HAVE
TO
INTEGRATE
OVER
ALL
POSSIBLE
PARAMETER
VALUES
BUT
WHEN
WE
HAVE
A
CONJUGATE
PRIOR
IT
IS
EASY
TO
COMPUTE
AS
WE
NOW
SHOW
LET
P
Θ
Q
Θ
BE
OUR
PRIOR
WHERE
Q
Θ
IS
AN
UNNORMALIZED
DISTRIBUTION
AND
IS
THE
NORMALIZATION
CONSTANT
OF
THE
PRIOR
LET
P
D
Θ
Q
D
Θ
ZE
BE
THE
LIKELIHOOD
WHERE
ZE
CONTAINS
ANY
CONSTANT
FACTORS
IN
THE
LIKELIHOOD
FINALLY
LET
P
Θ
D
Q
Θ
D
ZN
BE
OUR
POSTE
D
LOGEV
EB
D
LOGEV
EB
A
D
LOGEV
EB
B
N
METHOD
EB
C
M
D
FIGURE
SAME
AS
FIGURE
EXCEPT
NOW
N
FIGURE
GENERATED
BY
LINREGEBMODELSELVSN
RIOR
WHERE
Q
Θ
Q
Θ
Q
Θ
IS
THE
UNNORMALIZED
POSTERIOR
AND
ZN
IS
THE
NORMALIZATION
CONSTANT
OF
THE
POSTERIOR
WE
HAVE
P
Θ
P
D
Θ
P
Θ
P
D
Q
Θ
D
ZN
Q
D
Θ
Q
Θ
D
ZN
P
D
Z
Z
SO
ASSUMING
THE
RELEVANT
NORMALIZATION
CONSTANTS
ARE
TRACTABLE
WE
HAVE
AN
EASY
WAY
TO
COMPUTE
THE
MARGINAL
LIKELIHOOD
WE
GIVE
SOME
EXAMPLES
BELOW
BETA
BINOMIAL
MODEL
LET
US
APPLY
THE
ABOVE
RESULT
TO
THE
BETA
BINOMIAL
MODEL
SINCE
WE
KNOW
P
Θ
BETA
Θ
A
B
WHERE
A
A
AND
B
B
WE
KNOW
THE
NORMALIZATION
CONSTANT
OF
THE
POSTERIOR
IS
B
A
B
HENCE
P
Θ
P
D
Θ
P
Θ
P
D
ΘA
Θ
B
N
Θ
P
D
B
A
B
P
D
B
A
B
SO
N
B
A
B
P
D
B
A
B
P
D
N
B
A
B
THE
MARGINAL
LIKELIHOOD
FOR
THE
BETA
BERNOULLI
MODEL
IS
THE
SAME
AS
ABOVE
EXCEPT
IT
IS
MISSING
N
DIRICHLET
MULTINOULLI
MODEL
BY
THE
SAME
REASONING
AS
THE
BETA
BERNOULLI
CASE
ONE
CAN
SHOW
THAT
THE
MARGINAL
LIKELIHOOD
FOR
THE
DIRICHLET
MULTINOULLI
MODEL
IS
GIVEN
BY
B
N
Α
P
D
WHERE
B
Α
B
Α
K
K
Γ
Γ
ΑK
Α
HENCE
WE
CAN
REWRITE
THE
ABOVE
RESULT
IN
THE
FOLLOWING
FORM
WHICH
IS
WHAT
IS
USUALLY
PRESENTED
IN
THE
LITERATURE
Γ
K
ΑK
TT
Γ
NK
ΑK
WE
WILL
SEE
MANY
APPLICATIONS
OF
THIS
EQUATION
LATER
GAUSSIAN
GAUSSIAN
WISHART
MODEL
CONSIDER
THE
CASE
OF
AN
MVN
WITH
A
CONJUGATE
NIW
PRIOR
LET
BE
THE
NORMALIZER
FOR
THE
PRIOR
ZN
BE
NORMALIZER
FOR
THE
POSTERIOR
AND
LET
ZL
ND
BE
THE
NORMALIZER
FOR
THE
LIKELIHOOD
THEN
IT
IS
EASY
TO
SEE
THAT
ZN
P
D
Z
Z
D
ΝN
N
D
ΝN
ΠND
D
Ν
D
Ν
D
ΓD
ΝN
ΠND
ΚN
SN
ΝN
ΓD
THIS
EQUATION
WILL
PROVE
USEFUL
LATER
BIC
APPROXIMATION
TO
LOG
MARGINAL
LIKELIHOOD
IN
GENERAL
COMPUTING
THE
INTEGRAL
IN
EQUATION
CAN
BE
QUITE
DIFFICULT
ONE
SIMPLE
BUT
POPULAR
APPROXIMATION
IS
KNOWN
AS
THE
BAYESIAN
INFORMATION
CRITERION
OR
BIC
WHICH
HAS
THE
FOLLOWING
FORM
SCHWARZ
BIC
LOG
P
D
Θˆ
DOF
Θˆ
LOG
N
LOG
P
D
WHERE
DOF
Θˆ
IS
THE
NUMBER
OF
DEGREES
OF
FREEDOM
IN
THE
MODEL
AND
Θˆ
IS
THE
MLE
FOR
THE
MODEL
WE
SEE
THAT
THIS
HAS
THE
FORM
OF
A
PENALIZED
LOG
LIKELIHOOD
WHERE
THE
PENALTY
TERM
DEPENDS
ON
THE
MODEL
COMPLEXITY
SEE
SECTION
FOR
THE
DERIVATION
OF
THE
BIC
SCORE
AS
AN
EXAMPLE
CONSIDER
LINEAR
REGRESSION
AS
WE
SHOW
IN
SECTION
THE
MLE
IS
GIVEN
BY
Wˆ
XT
X
Y
AND
RSS
N
WHERE
RSS
N
YI
Wˆ
T
XI
THE
CORRESPONDING
LOG
LIKELIHOOD
IS
GIVEN
BY
LOG
P
D
Θˆ
N
LOG
N
HENCE
THE
BIC
SCORE
IS
AS
FOLLOWS
DROPPING
CONSTANT
TERMS
BIC
N
LOG
D
LOG
N
WHERE
D
IS
THE
NUMBER
OF
VARIABLES
IN
THE
MODEL
IN
THE
STATISTICS
LITERATURE
IT
IS
COMMON
TO
USE
AN
ALTERNATIVE
DEFINITION
OF
BIC
WHICH
WE
CALL
THE
BIC
COST
SINCE
WE
WANT
TO
MINIMIZE
IT
BIC
COST
LOG
P
D
Θˆ
DOF
Θˆ
LOG
N
LOG
P
D
IN
THE
CONTEXT
OF
LINEAR
REGRESSION
THIS
BECOMES
BIC
COST
N
LOG
D
LOG
N
TRADITIONALLY
THE
BIC
SCORE
IS
DEFINED
USING
THE
ML
ESTIMATE
Θˆ
SO
IT
IS
INDEPENDENT
OF
THE
PRIOR
HOWEVER
FOR
MODELS
SUCH
AS
MIXTURES
OF
GAUSSIANS
THE
ML
ESTIMATE
CAN
BE
POORLY
BEHAVED
SO
IT
IS
BETTER
TO
EVALUATE
THE
BIC
SCORE
USING
THE
MAP
ESTIMATE
AS
IN
FRALEY
AND
RAFTERY
THE
BIC
METHOD
IS
VERY
CLOSELY
RELATED
TO
THE
MINIMUM
DESCRIPTION
LENGTH
OR
MDL
PRINCIPLE
WHICH
CHARACTERIZES
THE
SCORE
FOR
A
MODEL
IN
TERMS
OF
HOW
WELL
IT
FITS
THE
DATA
MINUS
HOW
COMPLEX
THE
MODEL
IS
TO
DEFINE
SEE
HANSEN
AND
YU
FOR
DETAILS
THERE
IS
A
VERY
SIMILAR
EXPRESSION
TO
BIC
MDL
CALLED
THE
AKAIKE
INFORMATION
CRITERION
OR
AIC
DEFINED
AS
AIC
M
D
LOG
P
D
ΘˆMLE
DOF
M
THIS
IS
DERIVED
FROM
A
FREQUENTIST
FRAMEWORK
AND
CANNOT
BE
INTERPRETED
AS
AN
APPROXIMATION
TO
THE
MARGINAL
LIKELIHOOD
NEVERTHELESS
THE
FORM
OF
THIS
EXPRESSION
IS
VERY
SIMILAR
TO
BIC
WE
SEE
THAT
THE
PENALTY
FOR
AIC
IS
LESS
THAN
FOR
BIC
THIS
CAUSES
AIC
TO
PICK
MORE
COMPLEX
MODELS
HOWEVER
THIS
CAN
RESULT
IN
BETTER
PREDICTIVE
ACCURACY
SEE
E
G
CLARKE
ET
AL
SEC
FOR
FURTHER
DISCUSSION
ON
SUCH
INFORMATION
CRITERIA
EFFECT
OF
THE
PRIOR
SOMETIMES
IT
IS
NOT
CLEAR
HOW
TO
SET
THE
PRIOR
WHEN
WE
ARE
PERFORMING
POSTERIOR
INFERENCE
THE
DETAILS
OF
THE
PRIOR
MAY
NOT
MATTER
TOO
MUCH
SINCE
THE
LIKELIHOOD
OFTEN
OVERWHELMS
THE
PRIOR
ANYWAY
BUT
WHEN
COMPUTING
THE
MARGINAL
LIKELIHOOD
THE
PRIOR
PLAYS
A
MUCH
MORE
IMPORTANT
ROLE
SINCE
WE
ARE
AVERAGING
THE
LIKELIHOOD
OVER
ALL
POSSIBLE
PARAMETER
SETTINGS
AS
WEIGHTED
BY
THE
PRIOR
IN
FIGURES
AND
WHERE
WE
DEMONSTRATED
MODEL
SELECTION
FOR
LINEAR
REGRESSION
WE
USED
A
PRIOR
OF
THE
FORM
P
W
Α
HERE
Α
IS
A
TUNING
PARAMETER
THAT
CONTROLS
HOW
STRONG
THE
PRIOR
IS
THIS
PARAMETER
CAN
HAVE
A
LARGE
EFFECT
AS
WE
DISCUSS
IN
SECTION
INTUITIVELY
IF
Α
IS
LARGE
THE
WEIGHTS
ARE
FORCED
TO
BE
SMALL
SO
WE
NEED
TO
USE
A
COMPLEX
MODEL
WITH
MANY
SMALL
PARAMETERS
E
G
A
HIGH
DEGREE
POLYNOMIAL
TO
FIT
THE
DATA
CONVERSELY
IF
Α
IS
SMALL
WE
WILL
FAVOR
SIMPLER
MODELS
SINCE
EACH
PARAMETER
IS
ALLOWED
TO
VARY
IN
MAGNITUDE
BY
A
LOT
IF
THE
PRIOR
IS
UNKNOWN
THE
CORRECT
BAYESIAN
PROCEDURE
IS
TO
PUT
A
PRIOR
ON
THE
PRIOR
THAT
IS
WE
SHOULD
PUT
A
PRIOR
ON
THE
HYPER
PARAMETER
Α
AS
WELL
AS
THE
PARAMETRS
W
TO
COMPUTE
THE
MARGINAL
LIKELIHOOD
WE
SHOULD
INTEGRATE
OUT
ALL
UNKNOWNS
I
E
WE
SHOULD
COMPUTE
P
D
M
R
R
P
D
W
P
W
Α
M
P
Α
M
DWDΑ
OF
COURSE
THIS
REQUIRES
SPECIFYING
THE
HYPER
PRIOR
FORTUNATELY
THE
HIGHER
UP
WE
GO
IN
THE
BAYESIAN
HIERARCHY
THE
LESS
SENSITIVE
ARE
THE
RESULTS
TO
THE
PRIOR
SETTINGS
SO
WE
CAN
USUALLY
MAKE
THE
HYPER
PRIOR
UNINFORMATIVE
A
COMPUTATIONAL
SHORTCUT
IS
TO
OPTIMIZE
Α
RATHER
THAN
INTEGRATING
IT
OUT
THAT
IS
WE
USE
P
D
M
R
P
D
W
P
W
Αˆ
M
DW
WHERE
Αˆ
ARGMAX
P
D
Α
M
ARGMAX
R
P
D
W
P
W
Α
M
DW
THIS
APPROACH
IS
CALLED
EMPIRICAL
BAYES
EB
AND
IS
DISCUSSED
IN
MORE
DETAIL
IN
SECTION
THIS
IS
THE
METHOD
USED
IN
FIGURES
AND
BAYES
FACTOR
BF
INTERPRETATION
BF
DECISIVE
EVIDENCE
FOR
STRONG
EVIDENCE
FOR
BF
MODERATE
EVIDENCE
FOR
BF
WEAK
EVIDENCE
FOR
BF
WEAK
EVIDENCE
FOR
BF
MODERATE
EVIDENCE
FOR
BF
STRONG
EVIDENCE
FOR
BF
DECISIVE
EVIDENCE
FOR
TABLE
JEFFREYS
SCALE
OF
EVIDENCE
FOR
INTERPRETING
BAYES
FACTORS
BAYES
FACTORS
SUPPOSE
OUR
PRIOR
ON
MODELS
IS
UNIFORM
P
M
THEN
MODEL
SELECTION
IS
EQUIVALENT
TO
PICKING
THE
MODEL
WITH
THE
HIGHEST
MARGINAL
LIKELIHOOD
NOW
SUPPOSE
WE
JUST
HAVE
TWO
MODELS
WE
ARE
CONSIDERING
CALL
THEM
THE
NULL
HYPOTHESIS
AND
THE
ALTERNATIVE
HYPOTHESIS
DEFINE
THE
BAYES
FACTOR
AS
THE
RATIO
OF
MARGINAL
LIKELIHOODS
BF
P
D
P
D
P
P
D
P
D
P
THIS
IS
LIKE
A
LIKELIHOOD
RATIO
EXCEPT
WE
INTEGRATE
OUT
THE
PARAMETERS
WHICH
ALLOWS
US
TO
COMPARE
MODELS
OF
DIFFERENT
COMPLEXITY
IF
THEN
WE
PREFER
MODEL
OTHERWISE
WE
PREFER
MODEL
OF
COURSE
IT
MIGHT
BE
THAT
IS
ONLY
SLIGHTLY
GREATER
THAN
IN
THAT
CASE
WE
ARE
NOT
VERY
CONFIDENT
THAT
MODEL
IS
BETTER
JEFFREYS
PROPOSED
A
SCALE
OF
EVIDENCE
FOR
INTERPRETING
THE
MAGNITUDE
OF
A
BAYES
FACTOR
WHICH
IS
SHOWN
IN
TABLE
THIS
IS
A
BAYESIAN
ALTERNATIVE
TO
THE
FREQUENTIST
CONCEPT
OF
A
P
VALUE
ALTERNATIVELY
WE
CAN
JUST
CONVERT
THE
BAYES
FACTOR
TO
A
POSTERIOR
OVER
MODELS
IF
P
P
WE
HAVE
P
M
D
EXAMPLE
TESTING
IF
A
COIN
IS
FAIR
SUPPOSE
WE
OBSERVE
SOME
COIN
TOSSES
AND
WANT
TO
DECIDE
IF
THE
DATA
WAS
GENERATED
BY
A
FAIR
COIN
Θ
OR
A
POTENTIALLY
BIASED
COIN
WHERE
Θ
COULD
BE
ANY
VALUE
IN
LET
US
DENOTE
THE
FIRST
MODEL
BY
AND
THE
SECOND
MODEL
BY
THE
MARGINAL
LIKELIHOOD
UNDER
IS
SIMPLY
N
A
P
VALUE
IS
DEFINED
AS
THE
PROBABILITY
UNDER
THE
NULL
HYPOTHESIS
OF
OBSERVING
SOME
TEST
STATISTIC
F
D
SUCH
AS
THE
CHI
SQUARED
STATISTIC
THAT
IS
AS
LARGE
OR
LARGER
THAN
THAT
ACTUALLY
OBSERVED
I
E
PVALUE
D
P
F
D
F
D
D
NOTE
THAT
HAS
ALMOST
NOTHING
TO
DO
WITH
WHAT
WE
REALLY
WANT
TO
KNOW
WHICH
IS
P
D
LOG
P
D
BIC
APPROXIMATION
TO
LOG
P
D
A
B
FIGURE
A
LOG
MARGINAL
LIKELIHOOD
FOR
THE
COINS
EXAMPLE
B
BIC
APPROXIMATION
FIGURE
GENERATED
BY
COINSMODELSELDEMO
WHERE
N
IS
THE
NUMBER
OF
COIN
TOSSES
THE
MARGINAL
LIKELIHOOD
UNDER
USING
A
BETA
PRIOR
IS
P
D
M
R
P
D
Θ
P
Θ
DΘ
B
WE
PLOT
LOG
P
VS
THE
NUMBER
OF
HEADS
IN
FIGURE
A
ASSUMING
N
AND
THE
SHAPE
OF
THE
CURVE
IS
NOT
VERY
SENSITIVE
TO
AND
AS
LONG
AS
IF
WE
OBSERVE
OR
HEADS
THE
UNBIASED
COIN
HYPOTHESIS
IS
MORE
LIKELY
THAN
SINCE
IS
A
SIMPLER
MODEL
IT
HAS
NO
FREE
PARAMETERS
IT
WOULD
BE
A
SUSPICIOUS
COINCIDENCE
IF
THE
COIN
WERE
BIASED
BUT
HAPPENED
TO
PRODUCE
ALMOST
EXACTLY
HEADS
TAILS
HOWEVER
AS
THE
COUNTS
BECOME
MORE
EXTREME
WE
FAVOR
THE
BIASED
COIN
HYPOTHESIS
NOTE
THAT
IF
WE
PLOT
THE
LOG
BAYES
FACTOR
LOG
IT
WILL
HAVE
EXACTLY
THE
SAME
SHAPE
SINCE
LOG
P
IS
A
CONSTANT
SEE
ALSO
EXERCISE
IN
FIGURE
B
SHOWS
THE
BIC
APPROXIMATION
TO
LOG
P
FOR
OUR
BIASED
COIN
EXAMPLE
FROM
SECTION
WE
SEE
THAT
THE
CURVE
HAS
APPROXIMATELY
THE
SAME
SHAPE
AS
THE
EXACT
LOG
MARGINAL
LIKELIHOOD
WHICH
IS
ALL
THAT
MATTERS
FOR
MODEL
SELECTION
PURPOSES
SINCE
THE
ABSOLUTE
SCALE
IS
IRRELEVANT
IN
PARTICULAR
IT
FAVORS
THE
SIMPLER
MODEL
UNLESS
THE
DATA
IS
OVERWHELMINGLY
IN
SUPPORT
OF
THE
MORE
COMPLEX
MODEL
JEFFREYS
LINDLEY
PARADOX
PROBLEMS
CAN
ARISE
WHEN
WE
USE
IMPROPER
PRIORS
I
E
PRIORS
THAT
DO
NOT
INTEGRATE
TO
FOR
MODEL
SELECTION
HYPOTHESIS
TESTING
EVEN
THOUGH
SUCH
PRIORS
MAY
BE
ACCEPTABLE
FOR
OTHER
PURPOSES
FOR
EXAMPLE
CONSIDER
TESTING
THE
HYPOTHESES
Θ
VS
Θ
TO
DEFINE
THE
MARGINAL
DENSITY
ON
Θ
WE
USE
THE
FOLLOWING
MIXTURE
MODEL
P
Θ
P
Θ
P
P
Θ
P
THIS
IS
ONLY
MEANINGFUL
IF
P
Θ
AND
P
Θ
ARE
PROPER
NORMALIZED
DENSITY
FUNCTIONS
IN
THIS
CASE
THE
POSTERIOR
IS
GIVEN
BY
P
M
D
P
P
D
P
P
D
P
P
D
P
P
D
Θ
P
Θ
DΘ
NOW
SUPPOSE
WE
USE
IMPROPER
PRIORS
P
Θ
AND
P
Θ
THEN
P
D
P
P
M
C
P
Θ
DΘ
P
D
Θ
DΘ
P
P
D
Θ
DΘ
P
P
P
WHERE
I
ΘI
P
D
Θ
DΘ
IS
THE
INTEGRATED
OR
MARGINAL
LIKELIHOOD
FOR
MODEL
I
NOW
LET
P
P
HENCE
P
M
D
THUS
WE
CAN
CHANGE
THE
POSTERIOR
ARBITRARILY
BY
CHOOSING
AND
AS
WE
PLEASE
NOTE
THAT
USING
PROPER
BUT
VERY
VAGUE
PRIORS
CAN
CAUSE
SIMILAR
PROBLEMS
IN
PARTICULAR
THE
BAYES
FACTOR
WILL
ALWAYS
FAVOR
THE
SIMPLER
MODEL
SINCE
THE
PROBABILITY
OF
THE
OBSERVED
DATA
UNDER
A
COMPLEX
MODEL
WITH
A
VERY
DIFFUSE
PRIOR
WILL
BE
VERY
SMALL
THIS
IS
CALLED
THE
JEFFREYS
LINDLEY
PARADOX
THUS
IT
IS
IMPORTANT
TO
USE
PROPER
PRIORS
WHEN
PERFORMING
MODEL
SELECTION
NOTE
HOWEVER
THAT
IF
AND
SHARE
THE
SAME
PRIOR
OVER
A
SUBSET
OF
THE
PARAMETERS
THIS
PART
OF
THE
PRIOR
CAN
BE
IMPROPER
SINCE
THE
CORRESPONDING
NORMALIZATION
CONSTANT
WILL
CANCEL
OUT
PRIORS
THE
MOST
CONTROVERSIAL
ASPECT
OF
BAYESIAN
STATISTICS
IS
ITS
RELIANCE
ON
PRIORS
BAYESIANS
ARGUE
THIS
IS
UNAVOIDABLE
SINCE
NOBODY
IS
A
TABULA
RASA
OR
BLANK
SLATE
ALL
INFERENCE
MUST
BE
DONE
CONDITIONAL
ON
CERTAIN
ASSUMPTIONS
ABOUT
THE
WORLD
NEVERTHELESS
ONE
MIGHT
BE
INTERESTED
IN
MINIMIZING
THE
IMPACT
OF
ONE
PRIOR
ASSUMPTIONS
WE
BRIEFLY
DISCUSS
SOME
WAYS
TO
DO
THIS
BELOW
UNINFORMATIVE
PRIORS
IF
WE
DON
T
HAVE
STRONG
BELIEFS
ABOUT
WHAT
Θ
SHOULD
BE
IT
IS
COMMON
TO
USE
AN
UNINFORMATIVE
OR
NON
INFORMATIVE
PRIOR
AND
TO
LET
THE
DATA
SPEAK
FOR
ITSELF
THE
ISSUE
OF
DESIGNING
UNINFORMATIVE
PRIORS
IS
ACTUALLY
SOMEWHAT
TRICKY
AS
AN
EXAMPLE
OF
THE
DIFFICULTY
CONSIDER
A
BERNOULLI
PARAMETER
Θ
ONE
MIGHT
THINK
THAT
THE
MOST
UNINFORMATIVE
PRIOR
WOULD
BE
THE
UNIFORM
DISTRIBUTION
BETA
BUT
THE
POSTERIOR
MEAN
IN
THIS
CASE
IS
E
Θ
D
WHEREAS
THE
MLE
IS
HENCE
ONE
COULD
ARGUE
THAT
THE
PRIOR
WASN
T
COMPLETELY
UNINFORMATIVE
AFTER
ALL
CLEARLY
BY
DECREASING
THE
MAGNITUDE
OF
THE
PSEUDO
COUNTS
WE
CAN
LESSEN
THE
IMPACT
OF
THE
PRIOR
BY
THE
ABOVE
ARGUMENT
THE
MOST
NON
INFORMATIVE
PRIOR
IS
LIM
BETA
C
C
BETA
C
WHICH
IS
A
MIXTURE
OF
TWO
EQUAL
POINT
MASSES
AT
AND
SEE
ZHU
AND
LU
THIS
IS
ALSO
CALLED
THE
HALDANE
PRIOR
NOTE
THAT
THE
HALDANE
PRIOR
IS
AN
IMPROPER
PRIOR
MEANING
IT
DOES
NOT
INTEGRATE
TO
HOWEVER
AS
LONG
AS
WE
SEE
AT
LEAST
ONE
HEAD
AND
AT
LEAST
ONE
TAIL
THE
POSTERIOR
WILL
BE
PROPER
IN
SECTION
WE
WILL
ARGUE
THAT
THE
RIGHT
UNINFORMATIVE
PRIOR
IS
IN
FACT
BETA
CLEARLY
THE
DIFFERENCE
IN
PRACTICE
BETWEEN
THESE
THREE
PRIORS
IS
VERY
LIKELY
NEGLIGIBLE
IN
GENERAL
IT
IS
ADVISABLE
TO
PERFORM
SOME
KIND
OF
SENSITIVITY
ANALYSIS
IN
WHICH
ONE
CHECKS
HOW
MUCH
ONE
CONCLUSIONS
OR
PREDICTIONS
CHANGE
IN
RESPONSE
TO
CHANGE
IN
THE
MODELING
ASSUMPTIONS
WHICH
INCLUDES
THE
CHOICE
OF
PRIOR
BUT
ALSO
THE
CHOICE
OF
LIKELIHOOD
AND
ANY
KIND
OF
DATA
PRE
PROCESSING
IF
THE
CONCLUSIONS
ARE
RELATIVELY
INSENSITIVE
TO
THE
MODELING
ASSUMPTIONS
ONE
CAN
HAVE
MORE
CONFIDENCE
IN
THE
RESULTS
JEFFREYS
PRIORS
HAROLD
DESIGNED
A
GENERAL
PURPOSE
TECHNIQUE
FOR
CREATING
NON
INFORMATIVE
PRIORS
THE
RESULT
IS
KNOWN
AS
THE
JEFFREYS
PRIOR
THE
KEY
OBSERVATION
IS
THAT
IF
P
Φ
IS
NON
INFORMATIVE
THEN
ANY
RE
PARAMETERIZATION
OF
THE
PRIOR
SUCH
AS
Θ
H
Φ
FOR
SOME
FUNCTION
H
SHOULD
ALSO
BE
NON
INFORMATIVE
NOW
BY
THE
CHANGE
OF
VARIABLES
FORMULA
PΘ
Θ
PΦ
Φ
DΘ
SO
THE
PRIOR
WILL
IN
GENERAL
CHANGE
HOWEVER
LET
US
PICK
PΦ
Φ
I
Φ
WHERE
I
Φ
IS
THE
FISHER
INFORMATION
I
Φ
E
D
LOG
P
X
Φ
THIS
IS
A
MEASURE
OF
CURVATURE
OF
THE
EXPECTED
NEGATIVE
LOG
LIKELIHOOD
AND
HENCE
A
MEASURE
OF
STABILITY
OF
THE
MLE
SEE
SECTION
NOW
D
LOG
P
X
Θ
D
LOG
P
X
Φ
DΦ
DΘ
DΦ
DΘ
SQUARING
AND
TAKING
EXPECTATIONS
OVER
X
WE
HAVE
I
Θ
E
R
D
LOG
P
X
Θ
I
Φ
DΦ
DΘ
I
Θ
DΦ
I
Φ
DΘ
HAROLD
JEFFREYS
WAS
AN
ENGLISH
MATHEMATICIAN
STATISTICIAN
GEOPHYSICIST
AND
ASTRONOMER
SO
WE
FIND
THE
TRANSFORMED
PRIOR
IS
P
Θ
P
DΦ
DΦ
Φ
I
Φ
I
Θ
SO
PΘ
Θ
AND
PΦ
Φ
ARE
THE
SAME
SOME
EXAMPLES
WILL
MAKE
THIS
CLEARER
EXAMPLE
JEFFREYS
PRIOR
FOR
THE
BERNOULLI
AND
MULTINOULLI
SUPPOSE
X
BER
Θ
THE
LOG
LIKELIHOOD
FOR
A
SINGLE
SAMPLE
IS
LOG
P
X
Θ
X
LOG
Θ
X
LOG
Θ
THE
SCORE
FUNCTION
IS
JUST
THE
GRADIENT
OF
THE
LOG
LIKELIHOOD
Θ
LOG
P
X
Θ
X
X
DΘ
Θ
Θ
THE
OBSERVED
INFORMATION
IS
THE
SECOND
DERIVATIVE
OF
THE
LOG
LIKELIHOOD
X
X
J
Θ
LOG
P
X
Θ
Θ
X
Θ
THE
FISHER
INFORMATION
IS
THE
EXPECTED
INFORMATION
Θ
I
Θ
E
J
Θ
X
X
Θ
Θ
HENCE
JEFFREYS
PRIOR
IS
Θ
Θ
Θ
P
Θ
Θ
Θ
BETA
Θ
Θ
NOW
CONSIDER
A
MULTINOULLI
RANDOM
VARIABLE
WITH
K
STATES
ONE
CAN
SHOW
THAT
THE
JEFFREYS
PRIOR
IS
GIVEN
BY
P
Θ
DIR
NOTE
THAT
THIS
IS
DIFFERENT
FROM
THE
MORE
OBVIOUS
CHOICES
OF
DIR
OR
DIR
K
K
EXAMPLE
JEFFREYS
PRIOR
FOR
LOCATION
AND
SCALE
PARAMETERS
ONE
CAN
SHOW
THAT
THE
JEFFREYS
PRIOR
FOR
A
LOCATION
PARAMETER
SUCH
AS
THE
GAUSSIAN
MEAN
IS
P
Μ
THUS
IS
AN
EXAMPLE
OF
A
TRANSLATION
INVARIANT
PRIOR
WHICH
SATISFIES
THE
PROPERTY
THAT
THE
PROBABILITY
MASS
ASSIGNED
TO
ANY
INTERVAL
A
B
IS
THE
SAME
AS
THAT
ASSIGNED
TO
ANY
OTHER
SHIFTED
INTERVAL
OF
THE
SAME
WIDTH
SUCH
AS
A
C
B
C
THAT
IS
B
C
A
C
B
P
Μ
DΜ
A
C
B
C
A
B
P
Μ
DΜ
THIS
CAN
BE
ACHIEVED
USING
P
Μ
WHICH
WE
CAN
APPROXIMATE
BY
USING
A
GAUSSIAN
WITH
INFINITE
VARIANCE
P
Μ
Μ
NOTE
THAT
THIS
IS
AN
IMPROPER
PRIOR
SINCE
IT
DOES
NOT
INTEGRATE
TO
USING
IMPROPER
PRIORS
IS
FINE
AS
LONG
AS
THE
POSTERIOR
IS
PROPER
WHICH
WILL
BE
THE
CASE
PROVIDED
WE
HAVE
SEEN
N
DATA
POINTS
SINCE
WE
CAN
NAIL
DOWN
THE
LOCATION
AS
SOON
AS
WE
HAVE
SEEN
A
SINGLE
DATA
POINT
SIMILARLY
ONE
CAN
SHOW
THAT
THE
JEFFREYS
PRIOR
FOR
A
SCALE
PARAMETER
SUCH
AS
THE
GAUSSIAN
VARIANCE
IS
P
THIS
IS
AN
EXAMPLE
OF
A
SCALE
INVARIANT
PRIOR
WHICH
SATISFIES
THE
PROPERTY
THAT
THE
PROBABILITY
MASS
ASSIGNED
TO
ANY
INTERVAL
A
B
IS
THE
SAME
AS
THAT
ASSIGNED
TO
ANY
OTHER
INTERVAL
A
C
B
C
WHICH
IS
SCALED
IN
SIZE
BY
SOME
CONSTANT
FACTOR
C
FOR
EXAMPLE
IF
WE
CHANGE
UNITS
FROM
METERS
TO
FEET
WE
DO
NOT
WANT
THAT
TO
AFFECT
OUR
INFERENCES
THIS
CAN
BE
ACHIEVED
BY
USING
P
TO
SEE
THIS
NOTE
THAT
B
C
A
C
P
DS
LOG
B
C
LOG
B
C
LOG
A
C
R
B
WE
CAN
APPROXIMATE
THIS
USING
A
DEGENERATE
GAMMA
DISTRIBUTION
SECTION
P
GA
THE
PRIOR
P
IS
ALSO
IMPROPER
BUT
THE
POSTERIOR
IS
PROPER
AS
SOON
AS
WE
HAVE
SEEN
N
DATA
POINTS
SINCE
WE
NEED
AT
LEAST
TWO
DATA
POINTS
TO
ESTIMATE
A
VARIANCE
ROBUST
PRIORS
IN
MANY
CASES
WE
ARE
NOT
VERY
CONFIDENT
IN
OUR
PRIOR
SO
WE
WANT
TO
MAKE
SURE
IT
DOES
NOT
HAVE
AN
UNDUE
INFLUENCE
ON
THE
RESULT
THIS
CAN
BE
DONE
BY
USING
ROBUST
PRIORS
INSUA
AND
RUGGERI
WHICH
TYPICALLY
HAVE
HEAVY
TAILS
WHICH
AVOIDS
FORCING
THINGS
TO
BE
TOO
CLOSE
TO
THE
PRIOR
MEAN
LET
US
CONSIDER
AN
EXAMPLE
FROM
BERGER
SUPPOSE
X
N
Θ
WE
OBSERVE
THAT
X
AND
WE
WANT
TO
ESTIMATE
Θ
THE
MLE
IS
OF
COURSE
Θ
WHICH
SEEMS
REASONABLE
THE
POSTERIOR
MEAN
UNDER
A
UNIFORM
PRIOR
IS
ALSO
Θ
BUT
NOW
SUPPOSE
WE
KNOW
THAT
THE
PRIOR
MEDIAN
IS
AND
THE
PRIOR
QUANTILES
ARE
AT
AND
SO
P
Θ
P
Θ
P
Θ
P
Θ
LET
US
ALSO
ASSUME
THE
PRIOR
IS
SMOOTH
AND
UNIMODAL
IT
IS
EASY
TO
SHOW
THAT
A
GAUSSIAN
PRIOR
OF
THE
FORM
Θ
SATISFIES
THESE
PRIOR
CONSTRAINTS
BUT
IN
THIS
CASE
THE
POSTERIOR
MEAN
IS
GIVEN
BY
WHICH
DOESN
T
SEEM
VERY
SATISFACTORY
NOW
SUPPOSE
WE
USE
AS
A
CAUCHY
PRIOR
Θ
THIS
ALSO
SATISFIES
THE
PRIOR
CONSTRAINTS
OF
OUR
EXAMPLE
BUT
THIS
TIME
WE
FIND
USING
NUMERICAL
METHOD
INTEGRATION
SEE
ROBUSTPRIORDEMO
FOR
THE
CODE
THAT
THE
POSTERIOR
MEAN
IS
ABOUT
WHICH
SEEMS
MUCH
MORE
REASONABLE
MIXTURES
OF
CONJUGATE
PRIORS
ROBUST
PRIORS
ARE
USEFUL
BUT
CAN
BE
COMPUTATIONALLY
EXPENSIVE
TO
USE
CONJUGATE
PRIORS
SIMPLIFY
THE
COMPUTATION
BUT
ARE
OFTEN
NOT
ROBUST
AND
NOT
FLEXIBLE
ENOUGH
TO
ENCODE
OUR
PRIOR
KNOWL
EDGE
HOWEVER
IT
TURNS
OUT
THAT
A
MIXTURE
OF
CONJUGATE
PRIORS
IS
ALSO
CONJUGATE
EXERCISE
AND
CAN
APPROXIMATE
ANY
KIND
OF
PRIOR
DALLAL
AND
HALL
DIACONIS
AND
YLVISAKER
THUS
SUCH
PRIORS
PROVIDE
A
GOOD
COMPROMISE
BETWEEN
COMPUTATIONAL
CONVENIENCE
AND
FLEXIBILITY
FOR
EXAMPLE
SUPPOSE
WE
ARE
MODELING
COIN
TOSSES
AND
WE
THINK
THE
COIN
IS
EITHER
FAIR
OR
IS
BIASED
TOWARDS
HEADS
THIS
CANNOT
BE
REPRESENTED
BY
A
BETA
DISTRIBUTION
HOWEVER
WE
CAN
MODEL
IT
USING
A
MIXTURE
OF
TWO
BETA
DISTRIBUTIONS
FOR
EXAMPLE
WE
MIGHT
USE
P
Θ
BETA
Θ
BETA
Θ
IF
Θ
COMES
FROM
THE
FIRST
DISTRIBUTION
THE
COIN
IS
FAIR
BUT
IF
IT
COMES
FROM
THE
SECOND
IT
IS
BIASED
TOWARDS
HEADS
WE
CAN
REPRESENT
A
MIXTURE
BY
INTRODUCING
A
LATENT
INDICATOR
VARIABLE
Z
WHERE
Z
K
MEANS
THAT
Θ
COMES
FROM
MIXTURE
COMPONENT
K
THE
PRIOR
HAS
THE
FORM
P
Θ
P
Z
K
P
Θ
Z
K
K
WHERE
EACH
P
Θ
Z
K
IS
CONJUGATE
AND
P
Z
K
ARE
CALLED
THE
PRIOR
MIXING
WEIGHTS
ONE
CAN
SHOW
EXERCISE
THAT
THE
POSTERIOR
CAN
ALSO
BE
WRITTEN
AS
A
MIXTURE
OF
CONJUGATE
DISTRIBUTIONS
AS
FOLLOWS
P
Θ
D
P
Z
K
D
P
Θ
D
Z
K
K
WHERE
P
Z
K
D
ARE
THE
POSTERIOR
MIXING
WEIGHTS
GIVEN
BY
P
Z
K
P
D
Z
K
P
Z
K
D
KT
P
Z
K
P
D
Z
K
HERE
THE
QUANTITY
P
Z
K
IS
THE
MARGINAL
LIKELIHOOD
FOR
MIXTURE
COMPONENT
K
SEE
SEC
TION
EXAMPLE
SUPPOSE
WE
USE
THE
MIXTURE
PRIOR
P
Θ
Θ
Θ
WHERE
AND
AND
WE
OBSERVE
HEADS
AND
TAILS
THE
POSTERIOR
BECOMES
P
Θ
D
P
Z
D
BETA
Θ
P
Z
D
BETA
Θ
IF
HEADS
AND
TAILS
THEN
USING
EQUATION
THE
POSTERIOR
BECOMES
P
Θ
D
BETA
Θ
BETA
Θ
SEE
FIGURE
FOR
AN
ILLUSTRATION
MIXTURE
OF
BETA
DISTRIBUTIONS
FIGURE
A
MIXTURE
OF
TWO
BETA
DISTRIBUTIONS
FIGURE
GENERATED
BY
MIXBETADEMO
APPLICATION
FINDING
CONSERVED
REGIONS
IN
DNA
AND
PROTEIN
SEQUENCES
WE
MENTIONED
THAT
DIRICHLET
MULTINOMIAL
MODELS
ARE
WIDELY
USED
IN
BIOSEQUENCE
ANALYSIS
LET
US
GIVE
A
SIMPLE
EXAMPLE
TO
ILLUSTRATE
SOME
OF
THE
MACHINERY
THAT
HAS
DEVELOPED
SPECIFICALLY
CONSIDER
THE
SEQUENCE
LOGO
DISCUSSED
IN
SECTION
NOW
SUPPOSE
WE
WANT
TO
FIND
LOCATIONS
WHICH
REPRESENT
CODING
REGIONS
OF
THE
GENOME
SUCH
LOCATIONS
OFTEN
HAVE
THE
SAME
LETTER
ACROSS
ALL
SEQUENCES
BECAUSE
OF
EVOLUTIONARY
PRESSURE
SO
WE
NEED
TO
FIND
COLUMNS
WHICH
ARE
PURE
OR
NEARLY
SO
IN
THE
SENSE
THAT
THEY
ARE
MOSTLY
ALL
AS
MOSTLY
ALL
TS
MOSTLY
ALL
CS
OR
MOSTLY
ALL
GS
ONE
APPROACH
IS
TO
LOOK
FOR
LOW
ENTROPY
COLUMNS
THESE
WILL
BE
ONES
WHOSE
DISTRIBUTION
IS
NEARLY
DETERMINISTIC
PURE
BUT
SUPPOSE
WE
WANT
TO
ASSOCIATE
A
CONFIDENCE
MEASURE
WITH
OUR
ESTIMATES
OF
PURITY
THIS
CAN
BE
USEFUL
IF
WE
BELIEVE
ADJACENT
LOCATIONS
ARE
CONSERVED
TOGETHER
IN
THIS
CASE
WE
CAN
LET
IF
LOCATION
T
IS
CONSERVED
AND
LET
ZT
OTHERWISE
WE
CAN
THEN
ADD
A
DEPENDENCE
BETWEEN
ADJACENT
ZT
VARIABLES
USING
A
MARKOV
CHAIN
SEE
CHAPTER
FOR
DETAILS
IN
ANY
CASE
WE
NEED
TO
DEFINE
A
LIKELIHOOD
MODEL
P
NT
ZT
WHERE
NT
IS
THE
VECTOR
OF
A
C
G
T
COUNTS
FOR
COLUMN
T
IT
IS
NATURAL
TO
MAKE
THIS
BE
A
MULTINOMIAL
DISTRIBUTION
WITH
PARAMETER
ΘT
SINCE
EACH
COLUMN
HAS
A
DIFFERENT
DISTRIBUTION
WE
WILL
WANT
TO
INTEGRATE
OUT
ΘT
AND
THUS
COMPUTE
THE
MARGINAL
LIKELIHOOD
P
NT
ZT
R
P
NT
ΘT
P
ΘT
ZT
DΘT
BUT
WHAT
PRIOR
SHOULD
WE
USE
FOR
ΘT
WHEN
ZT
WE
CAN
USE
A
UNIFORM
PRIOR
P
Θ
ZT
DIR
BUT
WHAT
SHOULD
WE
USE
IF
ZT
AFTER
ALL
IF
THE
COLUMN
IS
CONSERVED
IT
COULD
BE
A
NEARLY
PURE
COLUMN
OF
AS
CS
GS
OR
TS
A
NATURAL
APPROACH
IS
TO
USE
A
MIXTURE
OF
DIRICHLET
PRIORS
EACH
ONE
OF
WHICH
IS
TILTED
TOWARDS
THE
APPROPRIATE
CORNER
OF
THE
DIMENSIONAL
SIMPLEX
E
G
P
Θ
ZT
DIR
Θ
DIR
Θ
SINCE
THIS
IS
CONJUGATE
WE
CAN
EASILY
COMPUTE
P
NT
ZT
SEE
BROWN
ET
AL
FOR
AN
APPLICATION
OF
THESE
IDEAS
TO
A
REAL
BIO
SEQUENCE
PROBLEM
HIERARCHICAL
BAYES
A
KEY
REQUIREMENT
FOR
COMPUTING
THE
POSTERIOR
P
Θ
IS
THE
SPECIFICATION
OF
A
PRIOR
P
Θ
Η
WHERE
Η
ARE
THE
HYPER
PARAMETERS
WHAT
IF
WE
DON
T
KNOW
HOW
TO
SET
Η
IN
SOME
CASES
WE
CAN
USE
UNINFORMATIVE
PRIORS
WE
WE
DISCUSSED
ABOVE
A
MORE
BAYESIAN
APPROACH
IS
TO
PUT
A
PRIOR
ON
OUR
PRIORS
IN
TERMS
OF
GRAPHICAL
MODELS
CHAPTER
WE
CAN
REPRESENT
THE
SITUATION
AS
FOLLOWS
Η
Θ
D
THIS
IS
AN
EXAMPLE
OF
A
HIERARCHICAL
BAYESIAN
MODEL
ALSO
CALLED
A
MULTI
LEVEL
MODEL
SINCE
THERE
ARE
MULTIPLE
LEVELS
OF
UNKNOWN
QUANTITIES
WE
GIVE
A
SIMPLE
EXAMPLE
BELOW
AND
WE
WILL
SEE
MANY
OTHERS
LATER
IN
THE
BOOK
EXAMPLE
MODELING
RELATED
CANCER
RATES
CONSIDER
THE
PROBLEM
OF
PREDICTING
CANCER
RATES
IN
VARIOUS
CITIES
THIS
EXAMPLE
IS
FROM
JOHNSON
AND
ALBERT
IN
PARTICULAR
SUPPOSE
WE
MEASURE
THE
NUMBER
OF
PEOPLE
IN
VARIOUS
CITIES
NI
AND
THE
NUMBER
OF
PEOPLE
WHO
DIED
OF
CANCER
IN
THESE
CITIES
XI
WE
ASSUME
XI
BIN
NI
ΘI
AND
WE
WANT
TO
ESTIMATE
THE
CANCER
RATES
ΘI
ONE
APPROACH
IS
TO
ESTIMATE
THEM
ALL
SEPARATELY
BUT
THIS
WILL
SUFFER
FROM
THE
SPARSE
DATA
PROBLEM
UNDERESTIMATION
OF
THE
RATE
OF
CANCER
DUE
TO
SMALL
NI
ANOTHER
APPROACH
IS
TO
ASSUME
ALL
THE
ΘI
ARE
THE
SAME
THIS
IS
CALLED
PARAMETER
TYING
THE
RESULTING
POOLED
MLE
IS
JUST
Θ
I
NI
BUT
THE
ASSUMPTION
THAT
ALL
THE
CITIES
HAVE
THE
SAME
RATE
IS
A
RATHER
STRONG
ONE
A
COMPROMISE
APPROACH
IS
TO
ASSUME
THAT
THE
ΘI
ARE
SIMILAR
BUT
THAT
THERE
MAY
BE
CITY
SPECIFIC
VARIATIONS
THIS
CAN
BE
MODELED
BY
ASSUMING
THE
ΘI
ARE
DRAWN
FROM
SOME
COMMON
DISTRIBUTION
SAY
ΘI
BETA
A
B
THE
FULL
JOINT
DISTRIBUTION
CAN
BE
WRITTEN
AS
N
P
D
Θ
Η
N
P
Η
BIN
XI
NI
ΘI
BETA
ΘI
Η
I
WHERE
Η
A
B
NOTE
THAT
IT
IS
CRUCIAL
THAT
WE
INFER
Η
A
B
FROM
THE
DATA
IF
WE
JUST
CLAMP
IT
TO
A
CONSTANT
THE
ΘI
WILL
BE
CONDITIONALLY
INDEPENDENT
AND
THERE
WILL
BE
NO
INFORMATION
FLOW
BETWEEN
THEM
BY
CONTRAST
BY
TREATING
Η
AS
AN
UNKNOWN
HIDDEN
VARIABLE
WE
ALLOW
THE
DATA
POOR
CITIES
TO
BORROW
STATISTICAL
STRENGTH
FROM
DATA
RICH
ONES
SUPPOSE
WE
COMPUTE
THE
JOINT
POSTERIOR
P
Η
Θ
FROM
THIS
WE
CAN
GET
THE
POSTERIOR
MARGINALS
P
ΘI
IN
FIGURE
A
WE
PLOT
THE
POSTERIOR
MEANS
E
ΘI
AS
BLUE
BARS
AS
WELL
AS
THE
POPULATION
LEVEL
MEAN
E
A
A
B
SHOWN
AS
A
RED
LINE
THIS
REPRESENTS
THE
AVERAGE
OF
THE
ΘI
WE
SEE
THAT
THE
POSTERIOR
MEAN
IS
SHRUNK
TOWARDS
THE
POOLED
ESTIMATE
MORE
STRONGLY
FOR
CITIES
WITH
SMALL
SAMPLE
SIZES
NI
FOR
EXAMPLE
CITY
AND
CITY
BOTH
HAVE
A
OBSERVED
CANCER
INCIDENCE
RATE
BUT
CITY
HAS
A
SMALLER
POPULATION
SO
ITS
RATE
IS
SHRUNK
MORE
TOWARDS
THE
POPULATION
LEVEL
ESTIMATE
I
E
IT
IS
CLOSER
TO
THE
HORIZONTAL
RED
LINE
THAN
CITY
FIGURE
B
SHOWS
THE
POSTERIOR
CREDIBLE
INTERVALS
FOR
ΘI
WE
SEE
THAT
CITY
WHICH
HAS
A
VERY
LARGE
POPULATION
PEOPLE
HAS
SMALL
POSTERIOR
UNCERTAINTY
CONSEQUENTLY
THIS
CITY
NUMBER
OF
PEOPLE
WITH
CANCER
TRUNCATED
AT
CREDIBLE
INTERVAL
ON
THETA
MEDIAN
POP
OF
CITY
TRUNCATED
AT
MLE
RED
LINE
POOLED
MLE
POSTERIOR
MEAN
RED
LINE
POP
MEAN
X
A
B
FIGURE
A
RESULTS
OF
FITTING
THE
MODEL
USING
THE
DATA
FROM
JOHNSON
AND
ALBERT
FIRST
ROW
NUMBER
OF
CANCER
INCIDENTS
XI
IN
CITIES
IN
MISSOURI
SECOND
ROW
POPULATION
SIZE
NI
THE
LARGEST
CITY
NUMBER
HAS
A
POPULATION
OF
AND
INCIDENTS
BUT
WE
TRUNCATE
THE
VERTICAL
AXES
OF
THE
FIRST
TWO
ROWS
SO
THAT
THE
DIFFERENCES
BETWEEN
THE
OTHER
CITIES
ARE
VISIBLE
THIRD
ROW
MLE
ΘˆI
THE
RED
LINE
IS
THE
POOLED
MLE
FOURTH
ROW
POSTERIOR
MEAN
E
ΘI
THE
RED
LINE
IS
E
A
A
B
THE
POPULATION
LEVEL
MEAN
B
POSTERIOR
CREDIBLE
INTERVALS
ON
THE
CANCER
RATES
FIGURE
GENERATED
BY
CANCERRATESEB
HAS
THE
LARGEST
IMPACT
ON
THE
POSTERIOR
ESTIMATE
OF
Η
WHICH
IN
TURN
WILL
IMPACT
THE
ESTIMATE
OF
THE
CANCER
RATES
FOR
OTHER
CITIES
CITIES
AND
WHICH
HAVE
THE
HIGHEST
MLE
ALSO
HAVE
THE
HIGHEST
POSTERIOR
UNCERTAINTY
REFLECTING
THE
FACT
THAT
SUCH
A
HIGH
ESTIMATE
IS
IN
CONFLICT
WITH
THE
PRIOR
WHICH
IS
ESTIMATED
FROM
ALL
THE
OTHER
CITIES
IN
THE
ABOVE
EXAMPLE
WE
HAVE
ONE
PARAMETER
PER
CITY
MODELING
THE
PROBABILITY
THE
RESPONSE
IS
ON
BY
MAKING
THE
BERNOULLI
RATE
PARAMETER
BE
A
FUNCTION
OF
COVARIATES
ΘI
SIGM
WT
X
WE
CAN
MODEL
MULTIPLE
CORRELATED
LOGISTIC
REGRESSION
TASKS
THIS
IS
CALLED
MULTI
TASK
LEARNING
AND
WILL
BE
DISCUSSED
IN
MORE
DETAIL
IN
SECTION
EMPIRICAL
BAYES
IN
HIERARCHICAL
BAYESIAN
MODELS
WE
NEED
TO
COMPUTE
THE
POSTERIOR
ON
MULTIPLE
LEVELS
OF
LATENT
VARIABLES
FOR
EXAMPLE
IN
A
TWO
LEVEL
MODEL
WE
NEED
TO
COMPUTE
P
Η
Θ
D
P
D
Θ
P
Θ
Η
P
Η
IN
SOME
CASES
WE
CAN
ANALYTICALLY
MARGINALIZE
OUT
Θ
THIS
LEAVES
IS
WITH
THE
SIMPLER
PROBLEM
OF
JUST
COMPUTING
P
Η
AS
A
COMPUTATIONAL
SHORTCUT
WE
CAN
APPROXIMATE
THE
POSTERIOR
ON
THE
HYPER
PARAMETERS
WITH
A
POINT
ESTIMATE
P
Η
D
ΔΗˆ
Η
WHERE
Ηˆ
ARGMAX
P
Η
D
SINCE
Η
IS
TYPICALLY
MUCH
SMALLER
THAN
Θ
IN
DIMENSIONALITY
IT
IS
LESS
PRONE
TO
OVERFITTING
SO
WE
CAN
SAFELY
USE
A
UNIFORM
PRIOR
ON
Η
THEN
THE
ESTIMATE
BECOMES
Ηˆ
ARGMAX
P
D
Η
ARGMAX
R
P
D
Θ
P
Θ
Η
DΘL
WHERE
THE
QUANTITY
INSIDE
THE
BRACKETS
IS
THE
MARGINAL
OR
INTEGRATED
LIKELIHOOD
SOMETIMES
CALLED
THE
EVIDENCE
THIS
OVERALL
APPROACH
IS
CALLED
EMPIRICAL
BAYES
EB
OR
TYPE
II
MAXIMUM
LIKELIHOOD
IN
MACHINE
LEARNING
IT
IS
SOMETIMES
CALLED
THE
EVIDENCE
PROCEDURE
EMPIRICAL
BAYES
VIOLATES
THE
PRINCIPLE
THAT
THE
PRIOR
SHOULD
BE
CHOSEN
INDEPENDENTLY
OF
THE
DATA
HOWEVER
WE
CAN
JUST
VIEW
IT
AS
A
COMPUTATIONALLY
CHEAP
APPROXIMATION
TO
INFERENCE
IN
A
HIERARCHICAL
BAYESIAN
MODEL
JUST
AS
WE
VIEWED
MAP
ESTIMATION
AS
AN
APPROXIMATION
TO
INFERENCE
IN
THE
ONE
LEVEL
MODEL
Θ
IN
FACT
WE
CAN
CONSTRUCT
A
HIERARCHY
IN
WHICH
THE
MORE
INTEGRALS
ONE
PERFORMS
THE
MORE
BAYESIAN
ONE
BECOMES
METHOD
DEFINITION
MAXIMUM
LIKELIHOOD
MAP
ESTIMATION
ML
II
EMPIRICAL
BAYES
Θˆ
ARGMAXΘ
P
D
Θ
Θˆ
ARGMAXΘ
P
D
Θ
P
Θ
Η
Ηˆ
ARGMAXΗ
P
D
Θ
P
Θ
Η
DΘ
ARGMAXΗ
P
D
Η
FULL
BAYES
P
Θ
Η
D
P
D
Θ
P
Θ
Η
P
Η
NOTE
THAT
EB
CAN
BE
SHOWN
TO
HAVE
GOOD
FREQUENTIST
PROPERTIES
SEE
E
G
CARLIN
AND
LOUIS
EFRON
SO
IT
IS
WIDELY
USED
BY
NON
BAYESIANS
FOR
EXAMPLE
THE
POPULAR
JAMES
STEIN
ESTIMATOR
DISCUSSED
IN
SECTION
CAN
BE
DERIVED
USING
EB
EXAMPLE
BETA
BINOMIAL
MODEL
LET
US
RETURN
TO
THE
CANCER
RATES
MODEL
WE
CAN
ANALYTICALLY
INTEGRATE
OUT
THE
ΘI
AND
WRITE
DOWN
THE
MARGINAL
LIKELIHOOD
DIRECTLY
AS
FOLLOWS
P
D
A
B
TT
R
BIN
XI
NI
ΘI
BETA
ΘI
A
B
DΘI
B
A
XI
B
NI
XI
B
A
B
I
VARIOUS
WAYS
OF
MAXIMIZING
THIS
WRT
A
AND
B
ARE
DISCUSSED
IN
MINKA
HAVING
ESTIMATED
A
AND
B
WE
CAN
PLUG
IN
THE
HYPER
PARAMETERS
TO
COMPUTE
THE
POSTERIOR
P
ΘI
Aˆ
ˆB
IN
THE
USUAL
WAY
USING
CONJUGATE
ANALYSIS
THE
NET
RESULT
IS
THAT
THE
POSTERIOR
MEAN
OF
EACH
ΘI
IS
A
WEIGHTED
AVERAGE
OF
ITS
LOCAL
MLE
AND
THE
PRIOR
MEANS
WHICH
DEPENDS
ON
Η
A
B
BUT
SINCE
Η
IS
ESTIMATED
BASED
ON
ALL
THE
DATA
EACH
ΘI
IS
INFLUENCED
BY
ALL
THE
DATA
EXAMPLE
GAUSSIAN
GAUSSIAN
MODEL
WE
NOW
STUDY
ANOTHER
EXAMPLE
THAT
IS
ANALOGOUS
TO
THE
CANCER
RATES
EXAMPLE
EXCEPT
THE
DATA
IS
REAL
VALUED
WE
WILL
USE
A
GAUSSIAN
LIKELIHOOD
AND
A
GAUSSIAN
PRIOR
THIS
WILL
ALLOW
US
TO
WRITE
DOWN
THE
SOLUTION
ANALYTICALLY
IN
PARTICULAR
SUPPOSE
WE
HAVE
DATA
FROM
MULTIPLE
RELATED
GROUPS
FOR
EXAMPLE
XIJ
COULD
BE
THE
TEST
SCORE
FOR
STUDENT
I
IN
SCHOOL
J
FOR
J
D
AND
I
NJ
WE
WANT
TO
ESTIMATE
THE
MEAN
SCORE
FOR
EACH
SCHOOL
ΘJ
HOWEVER
SINCE
THE
SAMPLE
SIZE
NJ
MAY
BE
SMALL
FOR
SOME
SCHOOLS
WE
CAN
REGULARIZE
THE
PROBLEM
BY
USING
A
HIERARCHICAL
BAYESIAN
MODEL
WHERE
WE
ASSUME
ΘJ
COME
FROM
A
COMMON
PRIOR
Μ
Τ
THE
JOINT
DISTRIBUTION
HAS
THE
FOLLOWING
FORM
D
NJ
P
Θ
D
Η
TT
N
ΘJ
Μ
Τ
TT
N
XIJ
ΘJ
WHERE
WE
ASSUME
IS
KNOWN
FOR
SIMPLICITY
WE
RELAX
THIS
ASSUMPTION
IN
EXERCISE
WE
EXPLAIN
HOW
TO
ESTIMATE
Η
BELOW
ONCE
WE
HAVE
ESTIMATED
Η
Μ
Τ
WE
CAN
COMPUTE
THE
POSTERIORS
OVER
THE
ΘJ
TO
DO
THAT
IT
SIMPLIFIES
MATTERS
TO
REWRITE
THE
JOINT
DISTRIBUTION
IN
THE
FOLLOWING
FORM
EXPLOITING
THE
FACT
THAT
NJ
GAUSSIAN
MEASUREMENTS
WITH
VALUES
XIJ
AND
VARIANCE
ARE
EQUIVALENT
TO
ONE
MEASUREMENT
OF
VALUE
XJ
NJ
I
XIJ
WITH
VARIANCE
NJ
THIS
YIELDS
P
Θ
D
Ηˆ
TT
N
ΘJ
Μˆ
N
XJ
ΘJ
J
FROM
THIS
IT
FOLLOWS
FROM
THE
RESULTS
OF
SECTION
THAT
THE
POSTERIORS
ARE
GIVEN
BY
P
ΘJ
D
Μˆ
N
ΘJ
BˆJΜˆ
BˆJ
XJ
BˆJ
BˆJ
WHERE
Μˆ
X
AND
WILL
BE
DEFINED
BELOW
THE
QUANTITY
BˆJ
CONTROLS
THE
DEGREE
OF
SHRINKAGE
TOWARDS
THE
OVERALL
MEAN
Μ
IF
THE
DATA
IS
RELIABLE
FOR
GROUP
J
E
G
BECAUSE
THE
SAMPLE
SIZE
NJ
IS
LARGE
THEN
WILL
BE
SMALL
RELATIVE
TO
Τ
HENCE
BˆJ
WILL
BE
SMALL
AND
WE
WILL
PUT
MORE
WEIGHT
ON
XJ
WHEN
WE
ESTIMATE
ΘJ
HOWEVER
GROUPS
WITH
SMALL
SAMPLE
SIZES
WILL
GET
REGULARIZED
SHRUNK
TOWARDS
THE
OVERALL
MEAN
Μ
MORE
HEAVILY
WE
WILL
SEE
AN
EXAMPLE
OF
THIS
BELOW
IF
ΣJ
Σ
FOR
ALL
GROUPS
J
THE
POSTERIOR
MEAN
BECOMES
ΘˆJ
BˆX
Bˆ
XJ
X
Bˆ
XJ
X
THIS
HAS
EXACTLY
THE
SAME
FORM
AS
THE
JAMES
STEIN
ESTIMATOR
DISCUSSED
IN
SECTION
EXAMPLE
PREDICTING
BASEBALL
SCORES
WE
NOW
GIVE
AN
EXAMPLE
OF
SHRINKAGE
APPLIED
TO
BASEBALL
BATTING
AVERAGES
FROM
EFRON
AND
MORRIS
WE
OBSERVE
THE
NUMBER
OF
HITS
FOR
D
PLAYERS
DURING
THE
FIRST
T
GAMES
CALL
THE
NUMBER
OF
HITS
BI
WE
ASSUME
BJ
BIN
T
ΘJ
WHERE
ΘJ
IS
THE
TRUE
BATTING
AVERAGE
FOR
PLAYER
J
THE
GOAL
IS
TO
ESTIMATE
THE
ΘJ
THE
MLE
IS
OF
COURSE
ΘˆJ
XJ
WHERE
XJ
BJ
T
IS
THE
EMPIRICAL
BATTING
AVERAGE
HOWEVER
WE
CAN
USE
AN
EB
APPROACH
TO
DO
BETTER
TO
APPLY
THE
GAUSSIAN
SHRINKAGE
APPROACH
DESCRIBED
ABOVE
WE
REQUIRE
THAT
THE
LIKELIHOOD
BE
GAUSSIAN
XJ
N
ΘJ
FOR
KNOWN
WE
DROP
THE
I
SUBSCRIPT
SINCE
WE
ASSUME
NJ
MLE
TOP
AND
SHRINKAGE
ESTIMATES
BOTTOM
A
MSE
MLE
MSE
SHRUNK
PLAYER
NUMBER
A
FIGURE
A
MLE
PARAMETERS
TOP
AND
CORRESPONDING
SHRUNKEN
ESTIMATES
BOTTOM
B
WE
PLOT
THE
TRUE
PARAMETERS
BLUE
THE
POSTERIOR
MEAN
ESTIMATE
GREEN
AND
THE
MLES
RED
FOR
OF
THE
PLAYERS
FIGURE
GENERATED
BY
SHRINKAGEDEMOBASEBALL
SINCE
XJ
ALREADY
REPRESENTS
THE
AVERAGE
FOR
PLAYER
J
HOWEVER
IN
THIS
EXAMPLE
WE
HAVE
A
BINOMIAL
LIKELIHOOD
WHILE
THIS
HAS
THE
RIGHT
MEAN
E
XJ
ΘJ
THE
VARIANCE
IS
NOT
CONSTANT
TΘJ
ΘJ
VAR
X
VAR
B
J
T
J
T
SO
LET
US
APPLY
A
VARIANCE
STABILIZING
TO
XJ
TO
BETTER
MATCH
THE
GAUSSIAN
ASSUMP
TION
YJ
F
YJ
T
ARCSIN
NOW
WE
HAVE
APPROXIMATELY
YJ
F
ΘJ
ΜJ
WE
USE
GAUSSIAN
SHRINKAGE
TO
ESTIMATE
THE
ΜJ
USING
EQUATION
WITH
AND
WE
THEN
TRANSFORM
BACK
TO
GET
ΘˆJ
SIN
Μˆ
T
THE
RESULTS
ARE
SHOWN
IN
FIGURE
A
B
IN
A
WE
PLOT
THE
MLE
ΘˆJ
AND
THE
POSTERIOR
MEAN
ΘJ
WE
SEE
THAT
ALL
THE
ESTIMATES
HAVE
SHRUNK
TOWARDS
THE
GLOBAL
MEAN
IN
B
WE
PLOT
THE
TRUE
VALUE
ΘJ
THE
MLE
ΘˆJ
AND
THE
POSTERIOR
MEAN
ΘJ
THE
TRUE
VALUES
OF
ΘJ
ARE
ESTIMATED
FROM
A
LARGE
NUMBER
OF
INDEPENDENT
GAMES
WE
SEE
THAT
ON
AVERAGE
THE
SHRUNKEN
ESTIMATE
IS
MUCH
CLOSER
TO
THE
TRUE
PARAMETERS
THAN
THE
MLE
IS
SPECIFICALLY
THE
MEAN
SQUARED
ERROR
DEFINED
BY
MSE
D
ΘJ
ΘJ
IS
OVER
THREE
TIMES
SMALLER
USING
THE
SHRINKAGE
ESTIMATES
ESTIMATING
THE
HYPER
PARAMETERS
IN
THIS
SECTION
WE
GIVE
AN
ALGORITHM
FOR
ESTIMATING
Η
SUPPOSE
INITIALLY
THAT
IS
THE
SAME
FOR
ALL
GROUPS
IN
THIS
CASE
WE
CAN
DERIVE
THE
EB
ESTIMATE
IN
CLOSED
FORM
AS
WE
NOW
SHOW
FROM
EQUATION
WE
HAVE
P
XJ
Μ
Τ
R
N
XJ
ΘJ
N
ΘJ
Μ
Τ
DΘJ
N
XJ
Μ
Τ
SUPPOSE
E
X
Μ
AND
VAR
X
Μ
LET
Y
F
X
THEN
A
TAYLOR
SERIES
EXPANSIONS
GIVES
Y
F
Μ
X
Μ
F
I
Μ
HENCE
VAR
Y
F
I
Μ
X
Μ
F
I
Μ
Μ
A
VARIANCE
STABILIZING
TRANSFORMATION
IS
A
FUNCTION
F
SUCH
THAT
F
I
Μ
Μ
IS
INDEPENDENT
OF
Μ
HENCE
THE
MARGINAL
LIKELIHOOD
IS
D
P
D
Μ
Τ
N
XJ
Μ
Τ
J
THUS
WE
CAN
ESTIMATE
THE
HYPER
PARAMETERS
USING
THE
USUAL
MLES
FOR
A
GAUSSIAN
FOR
Μ
WE
HAVE
D
Μˆ
XJ
D
J
X
WHICH
IS
THE
OVERALL
MEAN
FOR
THE
VARIANCE
WE
CAN
USE
MOMENT
MATCHING
WHICH
IS
EQUIVALENT
TO
THE
MLE
FOR
A
GAUSSIAN
WE
SIMPLY
EQUATE
THE
MODEL
VARIANCE
TO
THE
EMPIRICAL
VARIANCE
D
XJ
D
J
X
SO
SINCE
WE
KNOW
Τ
MUST
BE
POSITIVE
IT
IS
COMMON
TO
USE
THE
FOLLOWING
REVISED
ESTIMATE
MAX
HENCE
THE
SHRINKAGE
FACTOR
IS
ˆ
B
IN
THE
CASE
WHERE
THE
ARE
DIFFERENT
WE
CAN
NO
LONGER
DERIVE
A
SOLUTION
IN
CLOSED
FORM
EXERCISE
DISCUSSES
HOW
TO
USE
THE
EM
ALGORITHM
TO
DERIVE
AN
EB
ESTIMATE
AND
EXERCISE
DISCUSSES
HOW
TO
PERFORM
FULL
BAYESIAN
INFERENCE
IN
THIS
HIERARCHICAL
MODEL
BAYESIAN
DECISION
THEORY
WE
HAVE
SEEN
HOW
PROBABILITY
THEORY
CAN
BE
USED
TO
REPRESENT
AND
UPDATES
OUR
BELIEFS
ABOUT
THE
STATE
OF
THE
WORLD
HOWEVER
ULTIMATELY
OUR
GOAL
IS
TO
CONVERT
OUR
BELIEFS
INTO
ACTIONS
IN
THIS
SECTION
WE
DISCUSS
THE
OPTIMAL
WAY
TO
DO
THIS
WE
CAN
FORMALIZE
ANY
GIVEN
STATISTICAL
DECISION
PROBLEM
AS
A
GAME
AGAINST
NATURE
AS
OPPOSED
TO
A
GAME
AGAINST
OTHER
STRATEGIC
PLAYERS
WHICH
IS
THE
TOPIC
OF
GAME
THEORY
SEE
E
G
SHOHAM
AND
LEYTON
BROWN
FOR
DETAILS
IN
THIS
GAME
NATURE
PICKS
A
STATE
OR
PARAMETER
OR
LABEL
Y
UNKNOWN
TO
US
AND
THEN
GENERATES
AN
OBSERVATION
X
WHICH
WE
GET
TO
SEE
WE
THEN
HAVE
TO
MAKE
A
DECISION
THAT
IS
WE
HAVE
TO
CHOOSE
AN
ACTION
A
FROM
SOME
ACTION
SPACE
A
FINALLY
WE
INCUR
SOME
LOSS
L
Y
A
WHICH
MEASURES
HOW
COMPATIBLE
OUR
ACTION
A
IS
WITH
NATURE
HIDDEN
STATE
Y
FOR
EXAMPLE
WE
MIGHT
USE
MISCLASSIFICATION
LOSS
L
Y
A
I
Y
A
OR
SQUARED
LOSS
L
Y
A
Y
A
WE
WILL
SEE
SOME
OTHER
EXAMPLES
BELOW
OUR
GOAL
IS
TO
DEVISE
A
DECISION
PROCEDURE
OR
POLICY
Δ
WHICH
SPECIFIES
THE
OPTIMAL
ACTION
FOR
EACH
POSSIBLE
INPUT
BY
OPTIMAL
WE
MEAN
THE
ACTION
THAT
MINIMIZES
THE
EXPECTED
LOSS
Δ
X
ARGMIN
E
L
Y
A
A
A
IN
ECONOMICS
IT
IS
MORE
COMMON
TO
TALK
OF
A
UTILITY
FUNCTION
THIS
IS
JUST
NEGATIVE
LOSS
U
Y
A
L
Y
A
THUS
THE
ABOVE
RULE
BECOMES
Δ
X
ARGMAX
E
U
Y
A
A
A
THIS
IS
CALLED
THE
MAXIMUM
EXPECTED
UTILITY
PRINCIPLE
AND
IS
THE
ESSENCE
OF
WHAT
WE
MEAN
BY
RATIONAL
BEHAVIOR
NOTE
THAT
THERE
ARE
TWO
DIFFERENT
INTERPRETATIONS
OF
WHAT
WE
MEAN
BY
EXPECTED
IN
THE
BAYESIAN
VERSION
WHICH
WE
DISCUSS
BELOW
WE
MEAN
THE
EXPECTED
VALUE
OF
Y
GIVEN
THE
DATA
WE
HAVE
SEEN
SO
FAR
IN
THE
FREQUENTIST
VERSION
WHICH
WE
DISCUSS
IN
SECTION
WE
MEAN
THE
EXPECTED
VALUE
OF
Y
AND
X
THAT
WE
EXPECT
TO
SEE
IN
THE
FUTURE
IN
THE
BAYESIAN
APPROACH
TO
DECISION
THEORY
THE
OPTIMAL
ACTION
HAVING
OBSERVED
X
IS
DEFINED
AS
THE
ACTION
A
THAT
MINIMIZES
THE
POSTERIOR
EXPECTED
LOSS
Ρ
A
X
EP
Y
X
L
Y
A
L
Y
A
P
Y
X
Y
IF
Y
IS
CONTINUOUS
E
G
WHEN
WE
WANT
TO
ESTIMATE
A
PARAMETER
VECTOR
WE
SHOULD
REPLACE
THE
SUM
WITH
AN
INTEGRAL
HENCE
THE
BAYES
ESTIMATOR
ALSO
CALLED
THE
BAYES
DECISION
RULE
IS
GIVEN
BY
Δ
X
ARG
MIN
Ρ
A
X
A
A
BAYES
ESTIMATORS
FOR
COMMON
LOSS
FUNCTIONS
IN
THIS
SECTION
WE
SHOW
HOW
TO
CONSTRUCT
BAYES
ESTIMATORS
FOR
THE
LOSS
FUNCTIONS
MOST
COMMONLY
ARISING
IN
MACHINE
LEARNING
MAP
ESTIMATE
MINIMIZES
LOSS
THE
LOSS
IS
DEFINED
BY
L
Y
A
I
Y
A
IF
A
Y
IF
A
Y
THIS
IS
COMMONLY
USED
IN
CLASSIFICATION
PROBLEMS
WHERE
Y
IS
THE
TRUE
CLASS
LABEL
AND
A
Yˆ
IS
THE
ESTIMATE
FOR
EXAMPLE
IN
THE
TWO
CLASS
CASE
WE
CAN
WRITE
THE
LOSS
MATRIX
AS
FOLLOWS
THRESHOLD
P
Y
I
X
P
Y
I
X
X
REJECT
REGION
FIGURE
FOR
SOME
REGIONS
OF
INPUT
SPACE
WHERE
THE
CLASS
POSTERIORS
ARE
UNCERTAIN
WE
MAY
PREFER
NOT
TO
CHOOSE
CLASS
OR
INSTEAD
WE
MAY
PREFER
THE
REJECT
OPTION
BASED
ON
FIGURE
OF
BISHOP
IN
SECTION
WE
GENERALIZE
THIS
LOSS
FUNCTION
SO
IT
PENALIZES
THE
TWO
KINDS
OF
ERRORS
ON
THE
OFF
DIAGONAL
DIFFERENTLY
THE
POSTERIOR
EXPECTED
LOSS
IS
Ρ
A
X
P
A
Y
X
P
Y
X
HENCE
THE
ACTION
THAT
MINIMIZES
THE
EXPECTED
LOSS
IS
THE
POSTERIOR
MODE
OR
MAP
ESTIMATE
Y
X
ARG
MAX
P
Y
X
Y
Y
REJECT
OPTION
IN
CLASSIFICATION
PROBLEMS
WHERE
P
Y
X
IS
VERY
UNCERTAIN
WE
MAY
PREFER
TO
CHOOSE
A
REJECT
ACTION
IN
WHICH
WE
REFUSE
TO
CLASSIFY
THE
EXAMPLE
AS
ANY
OF
THE
SPECIFIED
CLASSES
AND
INSTEAD
SAY
DON
T
KNOW
SUCH
AMBIGUOUS
CASES
CAN
BE
HANDLED
BY
E
G
A
HUMAN
EXPERT
SEE
FIGURE
FOR
AN
ILLUSTRATION
THIS
IS
USEFUL
IN
RISK
AVERSE
DOMAINS
SUCH
AS
MEDICINE
AND
FINANCE
WE
CAN
FORMALIZE
THE
REJECT
OPTION
AS
FOLLOWS
LET
CHOOSING
A
C
CORRESPOND
TO
PICKING
THE
REJECT
ACTION
AND
CHOOSING
A
C
CORRESPOND
TO
PICKING
ONE
OF
THE
CLASSES
SUPPOSE
WE
DEFINE
THE
LOSS
FUNCTION
AS
L
Y
J
A
I
IF
I
J
AND
I
J
C
ΛR
IF
I
C
ΛS
OTHERWISE
WHERE
ΛR
IS
THE
COST
OF
THE
REJECT
ACTION
AND
ΛS
IS
THE
COST
OF
A
SUBSTITUTION
ERROR
IN
EXERCISE
YOU
WILL
SHOW
THAT
THE
OPTIMAL
ACTION
IS
TO
PICK
THE
REJECT
ACTION
IF
THE
MOST
PROBABLE
CLASS
HAS
A
PROBABILITY
BELOW
ΛR
OTHERWISE
YOU
SHOULD
JUST
PICK
THE
MOST
PROBABLE
CLASS
X
X
X
A
B
B
FIGURE
A
C
PLOTS
OF
THE
L
Y
A
Y
A
Q
VS
Y
A
FOR
Q
Q
AND
Q
FIGURE
GENERATED
BY
LOSSFUNCTIONFIG
POSTERIOR
MEAN
MINIMIZES
QUADRATIC
LOSS
FOR
CONTINUOUS
PARAMETERS
A
MORE
APPROPRIATE
LOSS
FUNCTION
IS
SQUARED
ERROR
LOSS
OR
QUADRATIC
LOSS
DEFINED
AS
L
Y
A
Y
A
THE
POSTERIOR
EXPECTED
LOSS
IS
GIVEN
BY
Ρ
A
X
E
Y
A
X
E
X
Y
X
HENCE
THE
OPTIMAL
ESTIMATE
IS
THE
POSTERIOR
MEAN
AΡ
A
X
Y
X
Yˆ
E
Y
X
R
YP
Y
X
DY
THIS
IS
OFTEN
CALLED
THE
MINIMUM
MEAN
SQUARED
ERROR
ESTIMATE
OR
MMSE
ESTIMATE
IN
A
LINEAR
REGRESSION
PROBLEM
WE
HAVE
P
Y
X
Θ
N
Y
XT
W
IN
THIS
CASE
THE
OPTIMAL
ESTIMATE
GIVEN
SOME
TRAINING
DATA
D
IS
GIVEN
BY
E
Y
X
D
XT
E
W
D
THAT
IS
WE
JUST
PLUG
IN
THE
POSTERIOR
MEAN
PARAMETER
ESTIMATE
NOTE
THAT
THIS
IS
THE
OPTIMAL
THING
TO
DO
NO
MATTER
WHAT
PRIOR
WE
USE
FOR
W
POSTERIOR
MEDIAN
MINIMIZES
ABSOLUTE
LOSS
THE
LOSS
PENALIZES
DEVIATIONS
FROM
THE
TRUTH
QUADRATICALLY
AND
THUS
IS
SENSITIVE
TO
OUTLIERS
A
MORE
ROBUST
ALTERNATIVE
IS
THE
ABSOLUTE
OR
LOSS
L
Y
A
Y
A
SEE
FIGURE
THE
OPTIMAL
ESTIMATE
IS
THE
POSTERIOR
MEDIAN
I
E
A
VALUE
A
SUCH
THAT
P
Y
A
X
P
Y
A
X
SEE
EXERCISE
FOR
A
PROOF
SUPERVISED
LEARNING
CONSIDER
A
PREDICTION
FUNCTION
Δ
AND
SUPPOSE
WE
HAVE
SOME
COST
FUNCTION
Y
Y
WHICH
GIVES
THE
COST
OF
PREDICTING
Y
WHEN
THE
TRUTH
IS
Y
WE
CAN
DEFINE
THE
LOSS
INCURRED
BY
TAKING
ACTION
Δ
I
E
USING
THIS
PREDICTOR
WHEN
THE
UNKNOWN
STATE
OF
NATURE
IS
Θ
THE
PARAMETERS
OF
THE
DATA
GENERATING
MECHANISM
AS
FOLLOWS
L
Θ
Δ
E
X
Y
P
X
Y
Θ
Y
Δ
X
L
Y
Δ
X
P
X
Y
Θ
THIS
IS
KNOWN
AS
THE
GENERALIZATION
ERROR
OUR
GOAL
IS
TO
MINIMIZE
THE
POSTERIOR
EXPECTED
LOSS
GIVEN
BY
Ρ
Δ
D
R
P
Θ
D
L
Θ
Δ
DΘ
THIS
SHOULD
BE
CONTRASTED
WITH
THE
FREQUENTIST
RISK
WHICH
IS
DEFINED
IN
EQUATION
THE
FALSE
POSITIVE
VS
FALSE
NEGATIVE
TRADEOFF
IN
THIS
SECTION
WE
FOCUS
ON
BINARY
DECISION
PROBLEMS
SUCH
AS
HYPOTHESIS
TESTING
TWO
CLASS
CLASSIFICATION
OBJECT
EVENT
DETECTION
ETC
THERE
ARE
TWO
TYPES
OF
ERROR
WE
CAN
MAKE
A
FALSE
POSITIVE
AKA
FALSE
ALARM
WHICH
ARISES
WHEN
WE
ESTIMATE
Yˆ
BUT
THE
TRUTH
IS
Y
OR
A
FALSE
NEGATIVE
AKA
MISSED
DETECTION
WHICH
ARISES
WHEN
WE
ESTIMATE
Yˆ
BUT
THE
TRUTH
IS
Y
THE
LOSS
TREATS
THESE
TWO
KINDS
OF
ERRORS
EQUIVALENTLY
HOWEVER
WE
CAN
CONSIDER
THE
FOLLOWING
MORE
GENERAL
LOSS
MATRIX
WHERE
LFN
IS
THE
COST
OF
A
FALSE
NEGATIVE
AND
LFP
IS
THE
COST
OF
A
FALSE
POSITIVE
THE
POSTERIOR
EXPECTED
LOSS
FOR
THE
TWO
POSSIBLE
ACTIONS
IS
GIVEN
BY
P
Y
X
P
Y
X
LFP
LFN
IF
LFN
CLFP
IT
IS
EASY
TO
SHOW
EXERCISE
THAT
WE
SHOULD
PICK
Yˆ
IFF
P
Y
X
P
Y
X
Τ
WHERE
Τ
C
C
SEE
ALSO
MULLER
ET
AL
FOR
EXAMPLE
IF
A
FALSE
NEGATIVE
COSTS
TWICE
AS
MUCH
AS
FALSE
POSITIVE
SO
C
THEN
WE
USE
A
DECISION
THRESHOLD
OF
BEFORE
DECLARING
A
POSITIVE
BELOW
WE
DISCUSS
ROC
CURVES
WHICH
PROVIDE
A
WAY
TO
STUDY
THE
FP
FN
TRADEOFF
WITHOUT
HAVING
TO
CHOOSE
A
SPECIFIC
THRESHOLD
ROC
CURVES
AND
ALL
THAT
SUPPOSE
WE
ARE
SOLVING
A
BINARY
DECISION
PROBLEM
SUCH
AS
CLASSIFICATION
HYPOTHESIS
TESTING
OBJECT
DETECTION
ETC
ALSO
ASSUME
WE
HAVE
A
LABELED
DATA
SET
D
XI
YI
LET
Δ
X
TRUTH
Σ
ESTIMATE
TP
FP
FN
TN
Nˆ
TP
FP
Nˆ
FN
TN
Σ
N
TP
FN
N
FP
TN
N
TP
FP
FN
TN
TABLE
QUANTITIES
DERIVABLE
FROM
A
CONFUSION
MATRIX
N
IS
THE
TRUE
NUMBER
OF
POSITIVES
Nˆ
IS
THE
CALLED
NUMBER
OF
POSITIVES
N
IS
THE
TRUE
NUMBER
OF
NEGATIVES
Nˆ
IS
THE
CALLED
NUMBER
OF
NEGATIVES
Y
Y
Yˆ
TP
N
TPR
SENSITIVITY
RECALL
FP
N
FPR
TYPE
I
Yˆ
FN
N
FNR
MISS
RATE
TYPE
II
TN
N
TNR
SPECIFITY
TABLE
ESTIMATING
P
Yˆ
Y
FROM
A
CONFUSION
MATRIX
ABBREVIATIONS
FNR
FALSE
NEGATIVE
RATE
FPR
FALSE
POSITIVE
RATE
TNR
TRUE
NEGATIVE
RATE
TPR
TRUE
POSITIVE
RATE
I
F
X
Τ
BE
OUR
DECISION
RULE
WHERE
F
X
IS
A
MEASURE
OF
CONFIDENCE
THAT
Y
THIS
SHOULD
BE
MONOTONICALLY
RELATED
TO
P
Y
X
BUT
DOES
NOT
NEED
TO
BE
A
PROBABILITY
AND
Τ
IS
SOME
THRESHOLD
PARAMETER
FOR
EACH
GIVEN
VALUE
OF
Τ
WE
CAN
APPLY
OUR
DECISION
RULE
AND
COUNT
THE
NUMBER
OF
TRUE
POSITIVES
FALSE
POSITIVES
TRUE
NEGATIVES
AND
FALSE
NEGATIVES
THAT
OCCUR
AS
SHOWN
IN
TABLE
THIS
TABLE
OF
ERRORS
IS
CALLED
A
CONFUSION
MATRIX
FROM
THIS
TABLE
WE
CAN
COMPUTE
THE
TRUE
POSITIVE
RATE
TPR
ALSO
KNOWN
AS
THE
SENSITIVITY
RECALL
OR
HIT
RATE
BY
USING
TPR
TP
N
P
Yˆ
Y
WE
CAN
ALSO
COMPUTE
THE
FALSE
POSITIVE
RATE
FPR
ALSO
CALLED
THE
FALSE
ALARM
RATE
OR
THE
TYPE
I
ERROR
RATE
BY
USING
FPR
FP
N
P
Yˆ
Y
THESE
AND
OTHER
DEFINITIONS
ARE
SUMMARIZED
IN
TABLES
AND
WE
CAN
COMBINE
THESE
ERRORS
IN
ANY
WAY
WE
CHOOSE
TO
COMPUTE
A
LOSS
FUNCTION
HOWEVER
RATHER
THAN
THAN
COMPUTING
THE
TPR
AND
FPR
FOR
A
FIXED
THRESHOLD
Τ
WE
CAN
RUN
OUR
DETECTOR
FOR
A
SET
OF
THRESHOLDS
AND
THEN
PLOT
THE
TPR
VS
FPR
AS
AN
IMPLICIT
FUNCTION
OF
Τ
THIS
IS
CALLED
A
RECEIVER
OPERATING
CHARACTERISTIC
OR
ROC
CURVE
SEE
FIGURE
A
FOR
AN
EXAMPLE
ANY
SYSTEM
CAN
ACHIEVE
THE
POINT
ON
THE
BOTTOM
LEFT
FPR
TPR
BY
SETTING
Τ
AND
THUS
CLASSIFYING
EVERYTHING
AS
NEGATIVE
SIMILARLY
ANY
SYSTEM
CAN
ACHIEVE
THE
POINT
ON
THE
TOP
RIGHT
FPR
T
PR
BY
SETTING
Τ
AND
THUS
CLASSIFYING
EVERYTHING
AS
POSITIVE
IF
A
SYSTEM
IS
PERFORMING
AT
CHANCE
LEVEL
THEN
WE
CAN
ACHIEVE
ANY
POINT
ON
THE
DIAGONAL
LINE
TPR
FPR
BY
CHOOSING
AN
APPROPRIATE
THRESHOLD
A
SYSTEM
THAT
PERFECTLY
SEPARATES
THE
POSITIVES
FROM
NEGATIVES
HAS
A
THRESHOLD
THAT
CAN
ACHIEVE
THE
TOP
LEFT
CORNER
FPR
T
PR
BY
VARYING
THE
THRESHOLD
SUCH
A
SYSTEM
WILL
HUG
THE
LEFT
AXIS
AND
THEN
THE
TOP
AXIS
AS
SHOWN
IN
FIGURE
A
THE
QUALITY
OF
A
ROC
CURVE
IS
OFTEN
SUMMARIZED
AS
A
SINGLE
NUMBER
USING
THE
AREA
UNDER
THE
CURVE
OR
AUC
HIGHER
AUC
SCORES
ARE
BETTER
THE
MAXIMUM
IS
OBVIOUSLY
ANOTHER
SUMMARY
STATISTIC
THAT
IS
USED
IS
THE
EQUAL
ERROR
RATE
OR
EER
ALSO
CALLED
THE
CROSS
OVER
RATE
DEFINED
AS
THE
VALUE
WHICH
SATISFIES
FPR
FNR
SINCE
FNR
TPR
WE
CAN
COMPUTE
THE
EER
BY
DRAWING
A
LINE
FROM
THE
TOP
LEFT
TO
THE
BOTTOM
RIGHT
AND
SEEING
WHERE
IT
INTERSECTS
THE
ROC
CURVE
SEE
POINTS
A
AND
B
IN
FIGURE
A
LOWER
EER
SCORES
ARE
BETTER
THE
MINIMUM
IS
OBVIOUSLY
FPR
A
RECALL
B
FIGURE
A
ROC
CURVES
FOR
TWO
HYPOTHETICAL
CLASSIFICATION
SYSTEMS
A
IS
BETTER
THAN
B
WE
PLOT
THE
TRUE
POSITIVE
RATE
TPR
VS
THE
FALSE
POSITIVE
RATE
FPR
AS
WE
VARY
THE
THRESHOLD
Τ
WE
ALSO
INDICATE
THE
EQUAL
ERROR
RATE
EER
WITH
THE
RED
AND
BLUE
DOTS
AND
THE
AREA
UNDER
THE
CURVE
AUC
FOR
CLASSIFIER
B
B
A
PRECISION
RECALL
CURVE
FOR
TWO
HYPOTHETICAL
CLASSIFICATION
SYSTEMS
A
IS
BETTER
THAN
B
FIGURE
GENERATED
BY
PRHAND
Y
Y
Yˆ
TP
Nˆ
PRECISION
PPV
FP
Nˆ
FDP
Yˆ
FN
Nˆ
TN
Nˆ
NPV
TABLE
ESTIMATING
P
Y
Yˆ
FROM
A
CONFUSION
MATRIX
ABBREVIATIONS
FDP
FALSE
DISCOVERY
PROBABILITY
NPV
NEGATIVE
PREDICTIVE
VALUE
PPV
POSITIVE
PREDICTIVE
VALUE
PRECISION
RECALL
CURVES
WHEN
TRYING
TO
DETECT
A
RARE
EVENT
SUCH
AS
RETRIEVING
A
RELEVANT
DOCUMENT
OR
FINDING
A
FACE
IN
AN
IMAGE
THE
NUMBER
OF
NEGATIVES
IS
VERY
LARGE
HENCE
COMPARING
TPR
TP
N
TO
FPR
FP
N
IS
NOT
VERY
INFORMATIVE
SINCE
THE
FPR
WILL
BE
VERY
SMALL
HENCE
ALL
THE
ACTION
IN
THE
ROC
CURVE
WILL
OCCUR
ON
THE
EXTREME
LEFT
IN
SUCH
CASES
IT
IS
COMMON
TO
PLOT
THE
TPR
VERSUS
THE
NUMBER
OF
FALSE
POSITIVES
RATHER
THAN
VS
THE
FALSE
POSITIVE
RATE
HOWEVER
IN
SOME
CASES
THE
VERY
NOTION
OF
NEGATIVE
IS
NOT
WELL
DEFINED
FOR
EXAMPLE
WHEN
DETECTING
OBJECTS
IN
IMAGES
SEE
SECTION
IF
THE
DETECTOR
WORKS
BY
CLASSIFYING
PATCHES
THEN
THE
NUMBER
OF
PATCHES
EXAMINED
AND
HENCE
THE
NUMBER
OF
TRUE
NEGATIVES
IS
A
PARAMETER
OF
THE
ALGORITHM
NOT
PART
OF
THE
PROBLEM
DEFINITION
SO
WE
WOULD
LIKE
TO
USE
A
MEASURE
THAT
ONLY
TALKS
ABOUT
POSITIVES
THE
PRECISION
IS
DEFINED
AS
TP
Nˆ
P
Y
Yˆ
AND
THE
RECALL
IS
DEFINED
AS
TP
N
P
Yˆ
Y
PRECISION
MEASURES
WHAT
FRACTION
OF
OUR
DETECTIONS
ARE
ACTUALLY
POSITIVE
AND
RECALL
MEASURES
WHAT
FRACTION
OF
THE
POSITIVES
WE
ACTUALLY
DETECTED
IF
YˆI
IS
THE
PREDICTED
LABEL
AND
YI
IS
THE
TRUE
LABEL
WE
CAN
ESTIMATE
PRECISION
AND
RECALL
USING
P
I
YIYˆI
R
I
YIYˆI
A
PRECISION
RECALL
CURVE
IS
A
PLOT
OF
PRECISION
VS
RECALL
AS
WE
VARY
THE
THRESHOLD
Τ
SEE
FIGURE
B
HUGGING
THE
TOP
RIGHT
IS
THE
BEST
ONE
CAN
DO
THIS
CURVE
CAN
BE
SUMMARIZED
AS
A
SINGLE
NUMBER
USING
THE
MEAN
PRECISION
AVERAGING
OVER
CLASS
CLASS
POOLED
Y
Y
Y
Y
Y
Y
Yˆ
Yˆ
Yˆ
Yˆ
Yˆ
Yˆ
TABLE
ILLUSTRATION
OF
THE
DIFFERENCE
BETWEEN
MACRO
AND
MICRO
AVERAGING
Y
IS
THE
TRUE
LABEL
AND
Yˆ
IS
THE
CALLED
LABEL
IN
THIS
EXAMPLE
THE
MACRO
AVERAGED
PRECISION
IS
THE
MICRO
AVERAGED
PRECISION
IS
BASED
ON
TABLE
OF
MANNING
ET
AL
RECALL
VALUES
WHICH
APPROXIMATES
THE
AREA
UNDER
THE
CURVE
ALTERNATIVELY
ONE
CAN
QUOTE
THE
PRECISION
FOR
A
FIXED
RECALL
LEVEL
SUCH
AS
THE
PRECISION
OF
THE
FIRST
K
ENTITIES
RECALLED
THIS
IS
CALLED
THE
AVERAGE
PRECISION
AT
K
SCORE
THIS
MEASURE
IS
WIDELY
USED
WHEN
EVALUATING
INFORMATION
RETRIEVAL
SYSTEMS
F
SCORES
FOR
A
FIXED
THRESHOLD
ONE
CAN
COMPUTE
A
SINGLE
PRECISION
AND
RECALL
VALUE
THESE
ARE
OFTEN
COMBINED
INTO
A
SINGLE
STATISTIC
CALLED
THE
F
SCORE
OR
SCORE
WHICH
IS
THE
HARMONIC
MEAN
OF
PRECISION
AND
RECALL
P
R
R
P
USING
EQUATION
WE
CAN
WRITE
THIS
AS
N
F
I
YIYˆI
N
I
YI
N
I
YˆI
THIS
IS
A
WIDELY
USED
MEASURE
IN
INFORMATION
RETRIEVAL
SYSTEMS
TO
UNDERSTAND
WHY
WE
USE
THE
HARMONIC
MEAN
INSTEAD
OF
THE
ARITHMETIC
MEAN
P
R
CONSIDER
THE
FOLLOWING
SCENARIO
SUPPOSE
WE
RECALL
ALL
ENTRIES
SO
R
THE
PRECISION
WILL
BE
GIVEN
BY
THE
PREVALENCE
P
Y
SUPPOSE
THE
PREVALENCE
IS
LOW
SAY
P
Y
THE
ARITHMETIC
MEAN
OF
P
AND
R
IS
GIVEN
BY
P
R
BY
CONTRAST
THE
HARMONIC
MEAN
OF
THIS
STRATEGY
IS
ONLY
IN
THE
MULTI
CLASS
CASE
E
G
FOR
DOCUMENT
CLASSIFICATION
PROBLEMS
THERE
ARE
TWO
WAYS
TO
GENERALIZE
SCORES
THE
FIRST
IS
CALLED
MACRO
AVERAGED
AND
IS
DEFINED
AS
C
C
C
WHERE
C
IS
THE
SCORE
OBTAINED
ON
THE
TASK
OF
DISTINGUISHING
CLASS
C
FROM
ALL
THE
OTHERS
THE
OTHER
IS
CALLED
MICRO
AVERAGED
AND
IS
DEFINED
AS
THE
SCORE
WHERE
WE
POOL
ALL
THE
COUNTS
FROM
EACH
CLASS
CONTINGENCY
TABLE
TABLE
GIVES
A
WORKED
EXAMPLE
THAT
ILLUSTRATES
THE
DIFFERENCE
WE
SEE
THAT
THE
PRECISION
OF
CLASS
IS
AND
OF
CLASS
IS
THE
MACRO
AVERAGED
PRECISION
IS
THEREFORE
WHEREAS
THE
MICRO
AVERAGED
PRECISION
IS
THE
LATTER
IS
MUCH
CLOSER
TO
THE
PRECISION
OF
CLASS
THAN
TO
THE
PRECISION
OF
CLASS
SINCE
CLASS
IS
FIVE
TIMES
LARGER
THAN
CLASS
TO
GIVE
EQUAL
WEIGHT
TO
EACH
CLASS
USE
MACRO
AVERAGING
FALSE
DISCOVERY
RATES
SUPPOSE
WE
ARE
TRYING
TO
DISCOVER
A
RARE
PHENOMENON
USING
SOME
KIND
OF
HIGH
THROUGHPUT
MEASUREMENT
DEVICE
SUCH
AS
A
GENE
EXPRESSION
MICRO
ARRAY
OR
A
RADIO
TELESCOPE
WE
WILL
NEED
TO
MAKE
MANY
BINARY
DECISIONS
OF
THE
FORM
P
YI
D
Τ
WHERE
D
XI
N
AND
N
MAY
BE
LARGE
THIS
IS
CALLED
MULTIPLE
HYPOTHESIS
TESTING
NOTE
THAT
THE
DIFFERENCE
FROM
STANDARD
BINARY
CLASSIFICATION
IS
THAT
WE
ARE
CLASSIFYING
YI
BASED
ON
ALL
THE
DATA
NOT
JUST
BASED
ON
XI
SO
THIS
IS
A
SIMULTANEOUS
CLASSIFICATION
PROBLEM
WHERE
WE
MIGHT
HOPE
TO
DO
BETTER
THAN
A
SERIES
OF
INDIVIDUAL
CLASSIFICATION
PROBLEMS
HOW
SHOULD
WE
SET
THE
THRESHOLD
Τ
A
NATURAL
APPROACH
IS
TO
TRY
TO
MINIMIZE
THE
EXPECTED
NUMBER
OF
FALSE
POSITIVES
IN
THE
BAYESIAN
APPROACH
THIS
CAN
BE
COMPUTED
AS
FOLLOWS
FD
Τ
D
PI
I
PI
Τ
I
P
R
E
RRO
R
DISC
O
VERY
WHERE
PI
P
YI
IS
YOUR
BELIEF
THAT
THIS
OBJECT
EXHIBITS
THE
PHENOMENON
IN
QUESTION
WE
THEN
DEFINE
THE
POSTERIOR
EXPECTED
FALSE
DISCOVERY
RATE
AS
FOLLOWS
FDR
Τ
D
FD
Τ
D
N
Τ
D
WHERE
N
Τ
I
I
PI
Τ
IS
THE
NUMBER
OF
DISCOVERED
ITEMS
GIVEN
A
DESIRED
FDR
TOLERANCE
SAY
Α
ONE
CAN
THEN
ADAPT
Τ
TO
ACHIEVE
THIS
THIS
IS
CALLED
THE
DIRECT
POSTERIOR
PROBABILITY
APPROACH
TO
CONTROLLING
THE
FDR
NEWTON
ET
AL
MULLER
ET
AL
IN
ORDER
TO
CONTROL
THE
FDR
IT
IS
VERY
HELPFUL
TO
ESTIMATE
THE
PI
JOINTLY
E
G
USING
A
HIERAR
CHICAL
BAYESIAN
MODEL
AS
IN
SECTION
RATHER
THAN
INDEPENDENTLY
THIS
ALLOWS
THE
POOLING
OF
STATISTICAL
STRENGTH
AND
THUS
LOWER
FDR
SEE
E
G
BERRY
AND
HOCHBERG
FOR
MORE
INFORMATION
OTHER
TOPICS
IN
THIS
SECTION
WE
BRIEFLY
MENTION
A
FEW
OTHER
TOPICS
RELATED
TO
BAYESIAN
DECISION
THEORY
WE
DO
NOT
HAVE
SPACE
TO
GO
INTO
DETAIL
BUT
WE
INCLUDE
POINTERS
TO
THE
RELEVANT
LITERATURE
CONTEXTUAL
BANDITS
A
ONE
ARMED
BANDIT
IS
A
COLLOQUIAL
TERM
FOR
A
SLOT
MACHINE
FOUND
IN
CASINOS
AROUND
THE
WORLD
THE
GAME
IS
THIS
YOU
INSERT
SOME
MONEY
PULL
AN
ARM
AND
WAIT
FOR
THE
MACHINE
TO
STOP
IF
YOU
RE
LUCKY
YOU
WIN
SOME
MONEY
NOW
IMAGINE
THERE
IS
A
BANK
OF
K
SUCH
MACHINES
TO
CHOOSE
FROM
WHICH
ONE
SHOULD
YOU
USE
THIS
IS
CALLED
A
MULTI
ARMED
BANDIT
AND
CAN
BE
MODELED
USING
BAYESIAN
DECISION
THEORY
THERE
ARE
K
POSSIBLE
ACTIONS
AND
EACH
ACTION
HAS
AN
UNKNOWN
REWARD
PAYOFF
FUNCTION
RK
BY
MAINTAINING
A
BELIEF
STATE
P
K
K
P
RK
ONE
CAN
DEVISE
AN
OPTIMAL
POLICY
THIS
CAN
BE
COMPILED
INTO
A
SERIES
OF
GITTINS
INDICES
GITTINS
THIS
OPTIMALLY
SOLVES
THE
EXPLORATION
EXPLOITATION
TRADEOFF
WHICH
SPECIFIES
HOW
MANY
TIMES
ONE
SHOULD
TRY
EACH
ACTION
BEFORE
DECIDING
TO
GO
WITH
THE
WINNER
NOW
CONSIDER
AN
EXTENSION
WHERE
EACH
ARM
AND
THE
PLAYER
HAS
AN
ASSOCIATED
FEATURE
VECTOR
CALL
ALL
THESE
FEATURES
X
THIS
IS
CALLED
A
CONTEXTUAL
BANDIT
SEE
E
G
SARKAR
SCOTT
LI
ET
AL
FOR
EXAMPLE
THE
ARMS
COULD
REPRESENT
ADS
OR
NEWS
ARTICLES
WHICH
WE
WANT
TO
SHOW
TO
THE
USER
AND
THE
FEATURES
COULD
REPRESENT
PROPERTIES
OF
THESE
ADS
OR
ARTICLES
SUCH
AS
A
BAG
OF
WORDS
AS
WELL
AS
PROPERTIES
OF
THE
USER
SUCH
AS
DEMOGRAPHICS
IF
WE
ASSUME
A
LINEAR
MODEL
FOR
REWARD
RK
ΘT
X
WE
CAN
MAINTAIN
A
DISTRIBUTION
OVER
THE
PARAMETERS
OF
EACH
ARM
P
ΘK
WHERE
IS
A
SERIES
OF
TUPLES
OF
THE
FORM
A
X
R
WHICH
SPECIFIES
WHICH
ARM
WAS
PULLED
WHAT
ITS
FEATURES
WERE
AND
WHAT
THE
RESULTING
OUTCOME
WAS
E
G
R
IF
THE
USER
CLICKED
ON
THE
AD
AND
R
OTHERWISE
WE
DISCUSS
WAYS
TO
COMPUTE
P
ΘK
FROM
LINEAR
AND
LOGISTIC
REGRESSION
MODELS
IN
LATER
CHAPTERS
GIVEN
THE
POSTERIOR
WE
MUST
DECIDE
WHAT
ACTION
TO
TAKE
ONE
COMMON
HEURISTIC
KNOWN
AS
UCB
WHICH
STANDS
FOR
UPPER
CONFIDENCE
BOUND
IS
TO
TAKE
THE
ACTION
WHICH
MAXIMIZES
K
ARGMAX
ΜK
ΛΣK
K
WHERE
ΜK
E
RK
VAR
RK
AND
Λ
IS
A
TUNING
PARAMETER
THAT
TRADES
OFF
EXPLORATION
AND
EXPLOITATION
THE
INTUITION
IS
THAT
WE
SHOULD
PICK
ACTIONS
ABOUT
WHICH
WE
BELIEVE
ARE
GOOD
ΜK
IS
LARGE
AND
OR
ACTIONS
ABOUT
WHICH
WE
ARE
UNCERTAIN
ΣK
IS
LARGE
AN
EVEN
SIMPLER
METHOD
KNOWN
AS
THOMPSON
SAMPLING
IS
AS
FOLLOWS
AT
EACH
STEP
WE
PICK
ACTION
K
WITH
A
PROBABILITY
THAT
IS
EQUAL
TO
ITS
PROBABILITY
OF
BEING
THE
OPTIMAL
ACTION
PK
R
I
E
R
A
X
Θ
MAX
E
R
A
X
Θ
P
Θ
D
DΘ
WE
CAN
APPROXIMATE
THIS
BY
DRAWING
A
SINGLE
SAMPLE
FROM
THE
POSTERIOR
ΘT
P
Θ
D
AND
THEN
UTILITY
THEORY
SUPPOSE
WE
ARE
A
DOCTOR
TRYING
TO
DECIDE
WHETHER
TO
OPERATE
ON
A
PATIENT
OR
NOT
WE
IMAGINE
THERE
ARE
STATES
OF
NATURE
THE
PATIENT
HAS
NO
CANCER
THE
PATIENT
HAS
LUNG
CANCER
OR
THE
PATIENT
HAS
BREAST
CANCER
SINCE
THE
ACTION
AND
STATE
SPACE
IS
DISCRETE
WE
CAN
REPRESENT
THE
LOSS
FUNCTION
L
Θ
A
AS
A
LOSS
MATRIX
SUCH
AS
THE
FOLLOWING
SURGERY
NO
SURGERY
NO
CANCER
LUNG
CANCER
BREAST
CANCER
THESE
NUMBERS
REFLECTS
THE
FACT
THAT
NOT
PERFORMING
SURGERY
WHEN
THE
PATIENT
HAS
CANCER
IS
VERY
BAD
LOSS
OF
OR
DEPENDING
ON
THE
TYPE
OF
CANCER
SINCE
THE
PATIENT
MIGHT
DIE
NOT
PERFORMING
SURGERY
WHEN
THE
PATIENT
DOES
NOT
HAVE
CANCER
INCURS
NO
LOSS
PERFORMING
SURGERY
WHEN
THE
PATIENT
DOES
NOT
HAVE
CANCER
IS
WASTEFUL
LOSS
OF
AND
PERFORMING
SURGERY
WHEN
THE
PATIENT
DOES
HAVE
CANCER
IS
PAINFUL
BUT
NECESSARY
IT
IS
NATURAL
TO
ASK
WHERE
THESE
NUMBERS
COME
FROM
ULTIMATELY
THEY
REPRESENT
THE
PERSONAL
PREFERENCES
OR
VALUES
OF
A
FICTITIOUS
DOCTOR
AND
ARE
SOMEWHAT
ARBITRARY
JUST
AS
SOME
PEOPLE
PREFER
CHOCOLATE
ICE
CREAM
AND
OTHERS
PREFER
VANILLA
THERE
IS
NO
SUCH
THING
AS
THE
RIGHT
LOSS
UTILITY
FUNCTION
HOWEVER
IT
CAN
BE
SHOWN
SEE
E
G
DEGROOT
THAT
ANY
SET
OF
CONSISTENT
PREFERENCES
CAN
BE
CONVERTED
TO
A
SCALAR
LOSS
UTILITY
FUNCTION
NOTE
THAT
UTILITY
CAN
BE
MEASURED
ON
AN
ARBITRARY
SCALE
SUCH
AS
DOLLARS
SINCE
IT
IS
ONLY
RELATIVE
VALUES
THAT
MATTER
PEOPLE
ARE
OFTEN
SQUEAMISH
ABOUT
TALKING
ABOUT
HUMAN
LIVES
IN
MONETARY
TERMS
BUT
ALL
DECISION
MAKING
REQUIRES
SEQUENTIAL
DECISION
THEORY
SO
FAR
WE
HAVE
CONCENTRATED
ON
ONE
SHOT
DECISION
PROBLEMS
WHERE
WE
ONLY
HAVE
TO
MAKE
ONE
DECISION
AND
THEN
THE
GAME
ENDS
IN
SETION
WE
WILL
GENERALIZE
THIS
TO
MULTI
STAGE
OR
SEQUENTIAL
DECISION
PROBLEMS
SUCH
PROBLEMS
FREQUENTLY
ARISE
IN
MANY
BUSINESS
AND
ENGINEERING
SETTINGS
THIS
IS
CLOSELY
RELATED
TO
THE
PROBLEM
OF
REINFORCEMENT
LEARNING
HOWEVER
FURTHER
DISCUSSION
OF
THIS
POINT
IS
BEYOND
THE
SCOPE
OF
THIS
BOOK
EXERCISES
EXERCISE
PROOF
THAT
A
MIXTURE
OF
CONJUGATE
PRIORS
IS
INDEED
CONJUGATE
DERIVE
EQUATION
EXERCISE
OPTIMAL
THRESHOLD
ON
CLASSIFICATION
PROBABILITY
CONSIDER
A
CASE
WHERE
WE
HAVE
LEARNED
A
CONDITIONAL
PROBABILITY
DISTRIBUTION
P
Y
X
SUPPOSE
THERE
ARE
ONLY
TWO
CLASSES
AND
LET
P
Y
X
AND
P
Y
X
CONSIDER
THE
LOSS
MATRIX
BELOW
PREDICTED
LABEL
Yˆ
TRUE
LABEL
Y
A
SHOW
THAT
THE
DECISION
Yˆ
THAT
MINIMIZES
THE
EXPECTED
LOSS
IS
EQUIVALENT
TO
SETTING
A
PROBABILITY
THRESHOLD
Θ
AND
PREDICTING
Yˆ
IF
Θ
AND
Yˆ
IF
Θ
WHAT
IS
Θ
AS
A
FUNCTION
OF
AND
SHOW
YOUR
WORK
B
SHOW
A
LOSS
MATRIX
WHERE
THE
THRESHOLD
IS
SHOW
YOUR
WORK
EXERCISE
REJECT
OPTION
IN
CLASSIFIERS
SOURCE
DUDA
ET
AL
IN
MANY
CLASSIFICATION
PROBLEMS
ONE
HAS
THE
OPTION
EITHER
OF
ASSIGNING
X
TO
CLASS
J
OR
IF
YOU
ARE
TOO
UNCERTAIN
OF
CHOOSING
THE
REJECT
OPTION
IF
THE
COST
FOR
REJECTS
IS
LESS
THAN
THE
COST
OF
FALSELY
CLASSIFYING
THE
OBJECT
IT
MAY
BE
THE
OPTIMAL
ACTION
LET
ΑI
MEAN
YOU
CHOOSE
ACTION
I
FOR
I
C
WHERE
C
IS
THE
NUMBER
OF
CLASSES
AND
C
IS
THE
REJECT
ACTION
LET
Y
J
BE
THE
TRUE
BUT
UNKNOWN
STATE
OF
NATURE
DEFINE
THE
LOSS
FUNCTION
AS
FOLLOWS
Λ
ΑI
Y
J
IF
I
J
AND
I
J
C
ΛR
I
C
ΛS
OTHERWISE
IN
OTHERWORDS
YOU
INCUR
LOSS
IF
YOU
CORRECTLY
CLASSIFY
YOU
INCUR
ΛR
LOSS
COST
IF
YOU
CHOOSE
THE
REJECT
OPTION
AND
YOU
INCUR
ΛS
LOSS
COST
IF
YOU
MAKE
A
SUBSTITUTION
ERROR
MISCLASSIFICATION
TRADEOFFS
AND
ONE
NEEDS
TO
USE
SOME
KIND
OF
CURRENCY
TO
COMPARE
DIFFERENT
COURSES
OF
ACTION
INSURANCE
COMPANIES
DO
THIS
ALL
THE
TIME
ROSS
SCHACHTER
A
DECISION
THEORIST
AT
STANFORD
UNIVERSITY
LIKES
TO
TELL
A
STORY
OF
A
SCHOOL
BOARD
WHO
REJECTED
A
STUDY
ON
ABSESTOS
REMOVAL
FROM
SCHOOLS
BECAUSE
IT
PERFORMED
A
COST
BENEFIT
ANALYSIS
WHICH
WAS
CONSIDERED
INHUMANE
BECAUSE
THEY
PUT
A
DOLLAR
VALUE
ON
CHILDREN
HEALTH
THE
RESULT
OF
REJECTING
THE
REPORT
WAS
THAT
THE
ABSESTOS
WAS
NOT
REMOVED
WHICH
IS
SURELY
MORE
INHUMANE
IN
MEDICAL
DOMAINS
ONE
OFTEN
MEASURES
UTILITY
IN
TERMS
OF
QALY
OR
QUALITY
ADJUSTED
LIFE
YEARS
INSTEAD
OF
DOLLARS
BUT
IT
THE
SAME
IDEA
OF
COURSE
EVEN
IF
YOU
DO
NOT
EXPLICITLY
SPECIFY
HOW
MUCH
YOU
VALUE
DIFFERENT
PEOPLE
LIVES
YOUR
BEHAVIOR
WILL
REVEAL
YOUR
IMPLICIT
VALUES
PREFERENCES
AND
THESE
PREFERENCES
CAN
THEN
BE
CONVERTED
TO
A
REAL
VALUED
SCALE
SUCH
AS
DOLLARS
OR
QALY
INFERRING
A
UTILITY
FUNCTION
FROM
BEHAVIOR
IS
CALLED
INVERSE
REINFORCEMENT
LEARNING
DECISION
Yˆ
TRUE
LABEL
Y
PREDICT
PREDICT
REJECT
A
SHOW
THAT
THE
MINIMUM
RISK
IS
OBTAINED
IF
WE
DECIDE
Y
J
IF
P
Y
J
X
P
Y
K
X
FOR
ALL
K
I
E
J
IS
THE
MOST
PROBABLE
CLASS
AND
IF
P
Y
J
X
ΛR
OTHERWISE
WE
DECIDE
TO
REJECT
B
DESCRIBE
QUALITATIVELY
WHAT
HAPPENS
AS
ΛR
ΛS
IS
INCREASED
FROM
TO
I
E
THE
RELATIVE
COST
OF
REJECTION
INCREASES
EXERCISE
MORE
REJECT
OPTIONS
IN
MANY
APPLICATIONS
THE
CLASSIFIER
IS
ALLOWED
TO
REJECT
A
TEST
EXAMPLE
RATHER
THAN
CLASSIFYING
IT
INTO
ONE
OF
THE
CLASSES
CONSIDER
FOR
EXAMPLE
A
CASE
IN
WHICH
THE
COST
OF
A
MISCLASSIFICATION
IS
BUT
THE
COST
OF
HAVING
A
HUMAN
MANUALLY
MAKE
THE
DECISON
IS
ONLY
WE
CAN
FORMULATE
THIS
AS
THE
FOLLOWING
LOSS
MATRIX
A
SUPPOSE
P
Y
X
IS
PREDICTED
TO
BE
WHICH
DECISION
MINIMIZES
THE
EXPECTED
LOSS
B
NOW
SUPPOSE
P
Y
X
NOW
WHICH
DECISION
MINIMIZES
THE
EXPECTED
LOSS
C
SHOW
THAT
IN
GENERAL
FOR
THIS
LOSS
MATRIX
BUT
FOR
ANY
POSTERIOR
DISTRIBUTION
THERE
WILL
BE
TWO
THRESHOLDS
AND
SUCH
THAT
THE
OPTIMAL
DECISIONN
IS
TO
PREDICT
IF
REJECT
IF
AND
PREDICT
IF
WHERE
P
Y
X
WHAT
ARE
THESE
THRESHOLDS
EXERCISE
NEWSVENDOR
PROBLEM
CONSIDER
THE
FOLLOWING
CLASSIC
PROBLEM
IN
DECISION
THEORY
ECONOMICS
SUPPOSE
YOU
ARE
TRYING
TO
DECIDE
HOW
MUCH
QUANTITY
Q
OF
SOME
PRODUCT
E
G
NEWSPAPERS
TO
BUY
TO
MAXIMIZE
YOUR
PROFITS
THE
OPTIMAL
AMOUNT
WILL
DEPEND
ON
HOW
MUCH
DEMAND
D
YOU
THINK
THERE
IS
FOR
YOUR
PRODUCT
AS
WELL
AS
ITS
COST
TO
YOU
C
AND
ITS
SELLING
PRICE
P
SUPPOSE
D
IS
UNKNOWN
BUT
HAS
PDF
F
D
AND
CDF
F
D
WE
CAN
EVALUATE
THE
EXPECTED
PROFIT
BY
CONSIDERING
TWO
CASES
IF
D
Q
THEN
WE
SELL
ALL
Q
ITEMS
AND
MAKE
PROFIT
Π
P
C
Q
BUT
IF
D
Q
WE
ONLY
SELL
D
ITEMS
AT
PROFIT
P
C
D
BUT
HAVE
WASTED
C
Q
D
ON
THE
UNSOLD
ITEMS
SO
THE
EXPECTED
PROFIT
IF
WE
BUY
QUANTITY
Q
IS
EΠ
Q
P
C
QF
D
DD
Q
Q
P
C
DF
D
Q
C
Q
D
F
D
DD
SIMPLIFY
THIS
EXPRESSION
AND
THEN
TAKE
DERIVATIVES
WRT
Q
TO
SHOW
THAT
THE
OPTIMAL
QUANTITY
Q
WHICH
MAXIMIZES
THE
EXPECTED
PROFIT
SATISFIES
F
Q
P
C
P
EXERCISE
BAYES
FACTORS
AND
ROC
CURVES
LET
B
P
D
P
D
BE
THE
BAYES
FACTOR
IN
FAVOR
OF
MODEL
SUPPOSE
WE
PLOT
TWO
ROC
CURVES
ONE
COMPUTED
BY
THRESHOLDING
B
AND
THE
OTHER
COMPUTED
BY
THRESHOLDING
P
D
WILL
THEY
BE
THE
SAME
OR
DIFFERENT
EXPLAIN
WHY
EXERCISE
BAYES
MODEL
AVERAGING
HELPS
PREDICTIVE
ACCURACY
LET
Δ
BE
A
QUANTITY
THAT
WE
WANT
TO
PREDICT
LET
BE
THE
OBSERVED
DATA
AND
BE
A
FINITE
SET
OF
MODELS
SUPPOSE
OUR
ACTION
IS
TO
PROVIDE
A
PROBABILISTIC
PREDICTION
P
AND
THE
LOSS
FUNCTION
IS
L
Δ
P
LOG
P
Δ
WE
CAN
EITHER
PERFORM
BAYES
MODEL
AVERAGING
AND
PREDICT
USING
PBMA
Δ
P
Δ
M
D
P
M
D
M
M
OR
WE
COULD
PREDICT
USING
ANY
SINGLE
MODEL
A
PLUGIN
APPROXIMATION
PM
Δ
P
Δ
M
D
SHOW
THAT
FOR
ALL
MODELS
M
M
THE
POSTERIOR
EXPECTED
LOSS
USING
BMA
IS
LOWER
I
E
E
L
Δ
PBMA
L
E
L
Δ
PM
WHERE
THE
EXPECTATION
OVER
Δ
IS
WITH
RESPECT
TO
P
Δ
D
P
Δ
M
D
P
M
D
M
M
HINT
USE
THE
NON
NEGATIVITY
OF
THE
KL
DIVERGENCE
EXERCISE
MLE
AND
MODEL
SELECTION
FOR
A
DISCRETE
DISTRIBUTION
SOURCE
JAAKKOLA
LET
X
DENOTE
THE
RESULT
OF
A
COIN
TOSS
X
FOR
TAILS
X
FOR
HEADS
THE
COIN
IS
POTENTIALLY
BIASED
SO
THAT
HEADS
OCCURS
WITH
PROBABILITY
SUPPOSE
THAT
SOMEONE
ELSE
OBSERVES
THE
COIN
FLIP
AND
REPORTS
TO
YOU
THE
OUTCOME
Y
BUT
THIS
PERSON
IS
UNRELIABLE
AND
ONLY
REPORTS
THE
RESULT
CORRECTLY
WITH
PROBABILITY
I
E
P
Y
X
IS
GIVEN
BY
ASSUME
THAT
IS
INDEPENDENT
OF
X
AND
A
WRITE
DOWN
THE
JOINT
PROBABILITY
DISTRIBUTION
P
X
Y
Θ
AS
A
TABLE
IN
TERMS
OF
Θ
B
SUPPOSE
HAVE
THE
FOLLOWING
DATASET
X
Y
WHAT
ARE
THE
MLES
FOR
AND
JUSTIFY
YOUR
ANSWER
HINT
NOTE
THAT
THE
LIKELIHOOD
FUNCTION
FACTORIZES
P
X
Y
Θ
P
Y
X
P
X
WHAT
IS
P
Θˆ
WHERE
DENOTES
THIS
PARAMETER
MODEL
YOU
MAY
LEAVE
YOUR
ANSWER
IN
FRACTIONAL
FORM
IF
YOU
WISH
C
NOW
CONSIDER
A
MODEL
WITH
PARAMETERS
Θ
REPRESENTING
P
X
Y
Θ
ΘX
Y
ONLY
OF
THESE
PARAMETERS
ARE
FREE
TO
VARY
SINCE
THEY
MUST
SUM
TO
ONE
WHAT
IS
THE
MLE
OF
Θ
WHAT
IS
P
D
Θˆ
WHERE
DENOTES
THIS
PARAMETER
MODEL
D
SUPPOSE
WE
ARE
NOT
SURE
WHICH
MODEL
IS
CORRECT
WE
COMPUTE
THE
LEAVE
ONE
OUT
CROSS
VALIDATED
LOG
LIKELIHOOD
OF
THE
PARAMETER
MODEL
AND
THE
PARAMETER
MODEL
AS
FOLLOWS
N
L
M
LOG
P
XI
YI
M
Θˆ
D
I
I
AND
Θˆ
I
DENOTES
THE
MLE
COMPUTED
ON
EXCLUDING
ROW
I
WHICH
MODEL
WILL
CV
PICK
AND
WHY
HINT
NOTICE
HOW
THE
TABLE
OF
COUNTS
CHANGES
WHEN
YOU
OMIT
EACH
TRAINING
CASE
ONE
AT
A
TIME
E
RECALL
THAT
AN
ALTERNATIVE
TO
CV
IS
TO
USE
THE
BIC
SCORE
DEFINED
AS
BIC
M
D
LOG
P
D
Θˆ
MLE
DOF
M
LOG
N
WHERE
DOF
M
IS
THE
NUMBER
OF
FREE
PARAMETERS
IN
THE
MODEL
COMPUTE
THE
BIC
SCORES
FOR
BOTH
MODELS
USE
LOG
BASE
E
WHICH
MODEL
DOES
BIC
PREFER
EXERCISE
POSTERIOR
MEDIAN
IS
OPTIMAL
ESTIMATE
UNDER
LOSS
PROVE
THAT
THE
POSTERIOR
MEDIAN
IS
OPTIMAL
ESTIMATE
UNDER
LOSS
EXERCISE
DECISION
RULE
FOR
TRADING
OFF
FPS
AND
FNS
IF
LFN
CLFP
SHOW
THAT
WE
SHOULD
PICK
Yˆ
IFF
P
Y
X
P
Y
X
Τ
WHERE
Τ
C
C
FREQUENTIST
STATISTICS
INTRODUCTION
THE
APPROACH
TO
STATISTICAL
INFERENCE
THAT
WE
DESCRIBED
IN
CHAPTER
IS
KNOWN
AS
BAYESIAN
STATISTICS
PERHAPS
SURPRISINGLY
THIS
IS
CONSIDERED
CONTROVERSIAL
BY
SOME
PEOPLE
WHEREAS
THE
AP
PLICATION
OF
BAYES
RULE
TO
NON
STATISTICAL
PROBLEMS
SUCH
AS
MEDICAL
DIAGNOSIS
SECTION
SPAM
FILTERING
SECTION
OR
AIRPLANE
TRACKING
SECTION
IS
NOT
CONTROVERSIAL
THE
REASON
FOR
THE
OBJECTION
HAS
TO
DO
WITH
A
MISGUIDED
DISTINCTION
BETWEEN
PARAMETERS
OF
A
STATIS
TICAL
MODEL
AND
OTHER
KINDS
OF
UNKNOWN
QUANTITIES
ATTEMPTS
HAVE
BEEN
MADE
TO
DEVISE
APPROACHES
TO
STATISTICAL
INFERENCE
THAT
AVOID
TREATING
PARAMETERS
LIKE
RANDOM
VARIABLES
AND
WHICH
THUS
AVOID
THE
USE
OF
PRIORS
AND
BAYES
RULE
SUCH
APPROACHES
ARE
KNOWN
AS
FREQUENTIST
STATISTICS
CLASSICAL
STATISTICS
OR
ORTHODOX
STATISTICS
INSTEAD
OF
BEING
BASED
ON
THE
POSTERIOR
DISTRIBUTION
THEY
ARE
BASED
ON
THE
CONCEPT
OF
A
SAMPLING
DISTRIBUTION
THIS
IS
THE
DISTRIBUTION
THAT
AN
ESTIMATOR
HAS
WHEN
APPLIED
TO
MULTIPLE
DATA
SETS
SAMPLED
FROM
THE
TRUE
BUT
UNKNOWN
DISTRIBUTION
SEE
SECTION
FOR
DETAILS
IT
IS
THIS
NOTION
OF
VARIATION
ACROSS
REPEATED
TRIALS
THAT
FORMS
THE
BASIS
FOR
MODELING
UNCERTAINTY
USED
BY
THE
FREQUENTIST
APPROACH
BY
CONTRAST
IN
THE
BAYESIAN
APPROACH
WE
ONLY
EVER
CONDITION
ON
THE
ACTUALLY
OBSERVED
DATA
THERE
IS
NO
NOTION
OF
REPEATED
TRIALS
THIS
ALLOWS
THE
BAYESIAN
TO
COMPUTE
THE
PROBABILITY
OF
ONE
OFF
EVENTS
AS
WE
DISCUSSED
IN
SECTION
PERHAPS
MORE
IMPORTANTLY
THE
BAYESIAN
APPROACH
AVOIDS
CERTAIN
PARADOXES
THAT
PLAGUE
THE
FREQUENTIST
APPROACH
SEE
SECTION
NEVERTHELESS
IT
IS
IMPORTANT
TO
BE
FAMILIAR
WITH
FREQUENTIST
STATISTICS
ESPECIALLY
SECTION
SINCE
IT
IS
WIDELY
USED
IN
MACHINE
LEARNING
SAMPLING
DISTRIBUTION
OF
AN
ESTIMATOR
IN
FREQUENTIST
STATISTICS
A
PARAMETER
ESTIMATE
Θˆ
IS
COMPUTED
BY
APPLYING
AN
ESTIMATOR
Δ
TO
SOME
DATA
SO
Θˆ
Δ
THE
PARAMETER
IS
VIEWED
AS
FIXED
AND
THE
DATA
AS
RANDOM
WHICH
IS
THE
EXACT
OPPOSITE
OF
THE
BAYESIAN
APPROACH
THE
UNCERTAINTY
IN
THE
PARAMETER
ESTIMATE
CAN
BE
MEASURED
BY
COMPUTING
THE
SAMPLING
DISTRIBUTION
OF
THE
ESTIMATOR
TO
UNDERSTAND
THIS
PARAMETERS
ARE
SOMETIMES
CONSIDERED
TO
REPRESENT
TRUE
BUT
UNKNOWN
PHYSICAL
QUANTITIES
WHICH
ARE
THEREFORE
NOT
RANDOM
HOWEVER
WE
HAVE
SEEN
THAT
IT
IS
PERFECTLY
REASONABLE
TO
USE
A
PROBABILITY
DISTRIBUTION
TO
REPRESENT
ONE
UNCERTAINTY
ABOUT
AN
UNKNOWN
CONSTANT
BOOT
TRUE
N
MLE
SE
BOOT
TRUE
N
MLE
SE
1000
A
B
FIGURE
A
BOOTSTRAP
APPROXIMATION
TO
THE
SAMPLING
DISTRIBUTION
OF
Θˆ
FOR
A
BERNOULLI
DISTRIBUTION
WE
USE
B
BOOTSTRAP
SAMPLES
THE
N
DATACASES
WERE
GENERATED
FROM
BER
Θ
A
MLE
WITH
N
B
MLE
WITH
N
FIGURE
GENERATED
BY
BOOTSTRAPDEMOBER
CONCEPT
IMAGINE
SAMPLING
MANY
DIFFERENT
DATA
SETS
D
FROM
SOME
TRUE
MODEL
P
Θ
I
E
LET
D
X
N
WHERE
XS
P
Θ
AND
Θ
IS
THE
TRUE
PARAMETER
HERE
INDEXES
THE
SAMPLED
DATA
SET
AND
N
IS
THE
SIZE
OF
EACH
SUCH
DATASET
NOW
APPLY
THE
ESTIMATOR
Θˆ
TO
EACH
D
TO
GET
A
SET
OF
ESTIMATES
Θˆ
D
AS
WE
LET
THE
DISTRIBUTION
INDUCED
ON
Θ
IS
THE
SAMPLING
DISTRIBUTION
OF
THE
ESTIMATOR
WE
WILL
DISCUSS
VARIOUS
WAYS
TO
USE
THE
SAMPLING
DISTRIBUTION
IN
LATER
SECTIONS
BUT
FIRST
WE
SKETCH
TWO
APPROACHES
FOR
COMPUTING
THE
SAMPLING
DISTRIBUTION
ITSELF
BOOTSTRAP
THE
BOOTSTRAP
IS
A
SIMPLE
MONTE
CARLO
TECHNIQUE
TO
APPROXIMATE
THE
SAMPLING
DISTRIBUTION
THIS
IS
PARTICULARLY
USEFUL
IN
CASES
WHERE
THE
ESTIMATOR
IS
A
COMPLEX
FUNCTION
OF
THE
TRUE
PARAMETERS
THE
IDEA
IS
SIMPLE
IF
WE
KNEW
THE
TRUE
PARAMETERS
Θ
WE
COULD
GENERATE
MANY
SAY
FAKE
DATASETS
EACH
OF
SIZE
N
FROM
THE
TRUE
DISTRIBUTION
XS
P
Θ
FOR
I
N
WE
COULD
THEN
COMPUTE
OUR
ESTIMATOR
FROM
EACH
SAMPLE
ΘˆS
F
XS
AND
USE
THE
EMPIRICAL
DISTRIBUTION
OF
THE
RESULTING
SAMPLES
AS
OUR
ESTIMATE
OF
THE
SAMPLING
DISTRIBUTION
SINCE
Θ
IS
UNKNOWN
THE
IDEA
OF
THE
PARAMETRIC
BOOTSTRAP
IS
TO
GENERATE
THE
SAMPLES
USING
Θˆ
INSTEAD
AN
ALTERNATIVE
CALLED
THE
NON
PARAMETRIC
BOOTSTRAP
IS
TO
SAMPLE
THE
XS
WITH
REPLACEMENT
FROM
THE
ORIGINAL
DATA
AND
THEN
COMPUTE
THE
INDUCED
DISTRIBUTION
AS
BEFORE
SOME
METHODS
FOR
SPEEDING
UP
THE
BOOTSTRAP
WHEN
APPLIED
TO
MASSIVE
DATA
SETS
ARE
DISCUSSED
IN
KLEINER
ET
AL
FIGURE
SHOWS
AN
EXAMPLE
WHERE
WE
COMPUTE
THE
SAMPLING
DISTRIBUTION
OF
THE
MLE
FOR
A
BERNOULLI
USING
THE
PARAMETRIC
BOOTSTRAP
RESULTS
USING
THE
NON
PARAMETRIC
BOOTSTRAP
ARE
ESSENTIALLY
THE
SAME
WE
SEE
THAT
THE
SAMPLING
DISTRIBUTION
IS
ASYMMETRIC
AND
THEREFORE
QUITE
FAR
FROM
GAUSSIAN
WHEN
N
WHEN
N
THE
DISTRIBUTION
LOOKS
MORE
GAUSSIAN
AS
THEORY
SUGGESTS
SEE
BELOW
A
NATURAL
QUESTION
IS
WHAT
IS
THE
CONNECTION
BETWEEN
THE
PARAMETER
ESTIMATES
ΘˆS
Θˆ
XS
COMPUTED
BY
THE
BOOTSTRAP
AND
PARAMETER
VALUES
SAMPLED
FROM
THE
POSTERIOR
ΘS
P
D
CONCEPTUALLY
THEY
ARE
QUITE
DIFFERENT
BUT
IN
THE
COMMON
CASE
THAT
THAT
THE
PRIOR
IS
NOT
VERY
STRONG
THEY
CAN
BE
QUITE
SIMILAR
FOR
EXAMPLE
FIGURE
C
D
SHOWS
AN
EXAMPLE
WHERE
WE
COMPUTE
THE
POSTERIOR
USING
A
UNIFORM
BETA
PRIOR
AND
THEN
SAMPLE
FROM
IT
WE
SEE
THAT
THE
POSTERIOR
AND
THE
SAMPLING
DISTRIBUTION
ARE
QUITE
SIMILAR
SO
ONE
CAN
THINK
OF
THE
BOOTSTRAP
DISTRIBUTION
AS
A
POOR
MAN
POSTERIOR
SEE
HASTIE
ET
AL
FOR
DETAILS
HOWEVER
PERHAPS
SURPRISINGLY
BOOTSTRAP
CAN
BE
SLOWER
THAN
POSTERIOR
SAMPLING
THE
REASON
IS
THAT
THE
BOOTSTRAP
HAS
TO
FIT
THE
MODEL
TIMES
WHEREAS
IN
POSTERIOR
SAMPLING
WE
USUALLY
ONLY
FIT
THE
MODEL
ONCE
TO
FIND
A
LOCAL
MODE
AND
THEN
PERFORM
LOCAL
EXPLORATION
AROUND
THE
MODE
SUCH
LOCAL
EXPLORATION
IS
USUALLY
MUCH
FASTER
THAN
FITTING
A
MODEL
FROM
SCRATCH
LARGE
SAMPLE
THEORY
FOR
THE
MLE
IN
SOME
CASES
THE
SAMPLING
DISTRIBUTION
FOR
SOME
ESTIMATORS
CAN
BE
COMPUTED
ANALYTICALLY
IN
PARTICULAR
IT
CAN
BE
SHOWN
THAT
UNDER
CERTAIN
CONDITIONS
AS
THE
SAMPLE
SIZE
TENDS
TO
INFINITY
THE
SAMPLING
DISTRIBUTION
OF
THE
MLE
BECOMES
GAUSSIAN
INFORMALLY
THE
REQUIREMENT
FOR
THIS
RESULT
TO
HOLD
IS
THAT
EACH
PARAMETER
IN
THE
MODEL
GETS
TO
SEE
AN
INFINITE
AMOUNT
OF
DATA
AND
THAT
THE
MODEL
BE
IDENTIFIABLE
UNFORTUNATELY
THIS
EXCLUDES
MANY
OF
THE
MODELS
OF
INTEREST
TO
MACHINE
LEARNING
NEVERTHELESS
LET
US
ASSUME
WE
ARE
IN
A
SIMPLE
SETTING
WHERE
THE
THEOREM
HOLDS
THE
CENTER
OF
THE
GAUSSIAN
WILL
BE
THE
MLE
Θˆ
BUT
WHAT
ABOUT
THE
VARIANCE
OF
THIS
GAUSSIAN
INTUITIVELY
THE
VARIANCE
OF
THE
ESTIMATOR
WILL
BE
INVERSELY
RELATED
TO
THE
AMOUNT
OF
CURVATURE
OF
THE
LIKELIHOOD
SURFACE
AT
ITS
PEAK
IF
THE
CURVATURE
IS
LARGE
THE
PEAK
WILL
BE
SHARP
AND
THE
VARIANCE
LOW
IN
THIS
CASE
THE
ESTIMATE
IS
WELL
DETERMINED
BY
CONTRAST
IF
THE
CURVATURE
IS
SMALL
THE
PEAK
WILL
BE
NEARLY
FLAT
SO
THE
VARIANCE
IS
HIGH
LET
US
NOW
FORMALIZE
THIS
INTUITION
DEFINE
THE
SCORE
FUNCTION
AS
THE
GRADIENT
OF
THE
LOG
LIKELIHOOD
EVALUATED
AT
SOME
POINT
Θˆ
Θˆ
LOG
P
D
Θ
Θˆ
DEFINE
THE
OBSERVED
INFORMATION
MATRIX
AS
THE
GRADIENT
OF
THE
NEGATIVE
SCORE
FUNCTION
OR
EQUIVALENTLY
THE
HESSIAN
OF
THE
NLL
J
Θˆ
D
Θˆ
LOG
P
D
Θ
ˆ
IN
THIS
BECOMES
J
Θˆ
D
Θ
Θ
LOG
P
D
Θ
Θˆ
THIS
IS
JUST
A
MEASURE
OF
CURVATURE
OF
THE
LOG
LIKELIHOOD
FUNCTION
AT
Θˆ
SINCE
WE
ARE
STUDYING
THE
SAMPLING
DISTRIBUTION
XN
IS
A
SET
OF
RANDOM
VARIABLES
THE
FISHER
INFORMATION
MATRIX
IS
DEFINED
TO
BE
THE
EXPECTED
VALUE
OF
THE
OBSERVED
INFORMATION
MATRIX
IN
Θˆ
Θ
EΘ
IJ
Θˆ
D
THIS
IS
NOT
THE
USUAL
DEFINITION
BUT
IS
EQUIVALENT
TO
IT
UNDER
STANDARD
ASSUMPTIONS
MORE
PRECISELY
THE
STANDARD
DEFINITION
IS
AS
FOLLOWS
WE
JUST
GIVE
THE
SCALAR
CASE
TO
SIMPLIFY
NOTATION
I
Θˆ
Θ
VARΘ
D
LOG
P
X
Θ
ˆ
THAT
IS
THE
VARIANCE
OF
THE
SCORE
FUNCTION
IF
Θˆ
IS
THE
MLE
IT
IS
EASY
TO
SEE
THAT
EΘ
D
LOG
P
X
Θ
ˆ
SINCE
WHERE
EΘ
F
D
N
F
XI
P
XI
Θ
IS
THE
EXPECTED
VALUE
OF
THE
FUNCTION
F
WHEN
APPLIED
TO
DATA
SAMPLED
FROM
Θ
OFTEN
Θ
REPRESENTING
THE
TRUE
PARAMETER
THAT
GENERATED
THE
DATA
IS
ASSUMED
KNOWN
SO
WE
JUST
WRITE
IN
Θˆ
IN
Θˆ
Θ
FOR
SHORT
FURTHERMORE
IT
IS
EASY
TO
SEE
THAT
IN
Θˆ
N
Θˆ
BECAUSE
THE
LOG
LIKELIHOOD
FOR
A
SAMPLE
OF
SIZE
N
IS
JUST
N
TIMES
STEEPER
THAN
THE
LOG
LIKELIHOOD
FOR
A
SAMPLE
OF
SIZE
SO
WE
CAN
DROP
THE
SUBSCRIPT
AND
JUST
WRITE
I
Θˆ
Θˆ
THIS
IS
THE
NOTATION
THAT
IS
USUALLY
USED
NOW
LET
Θˆ
ΘˆMLE
D
BE
THE
MLE
WHERE
D
Θ
IT
CAN
BE
SHOWN
THAT
Θˆ
N
Θ
IN
Θ
AS
N
SEE
E
G
RICE
FOR
A
PROOF
WE
SAY
THAT
THE
SAMPLING
DISTRIBUTION
OF
THE
MLE
IS
ASYMPTOTICALLY
NORMAL
WHAT
ABOUT
THE
VARIANCE
OF
THE
MLE
WHICH
CAN
BE
USED
AS
SOME
MEASURE
OF
CONFIDENCE
IN
THE
MLE
UNFORTUNATELY
Θ
IS
UNKNOWN
SO
WE
CAN
T
EVALUATE
THE
VARIANCE
OF
THE
SAMPLING
DISTRIBUTION
HOWEVER
WE
CAN
APPROXIMATE
THE
SAMPLING
DISTRIBUTION
BY
REPLACING
Θ
WITH
Θˆ
CONSEQUENTLY
THE
APPROXIMATE
STANDARD
ERRORS
OF
ΘˆK
ARE
GIVEN
BY
FOR
EXAMPLE
FROM
EQUATION
WE
KNOW
THAT
THE
FISHER
INFORMATION
FOR
A
BINOMIAL
SAMPLING
MODEL
IS
I
Θ
Θ
Θ
SO
THE
APPROXIMATE
STANDARD
ERROR
OF
THE
MLE
IS
ˆ
ˆ
SˆE
I
ˆ
I
ˆ
Θ
Θ
N
WHERE
Θˆ
I
XI
COMPARE
THIS
TO
EQUATION
WHICH
IS
THE
POSTERIOR
STANDARD
DEVIATION
UNDER
A
UNIFORM
PRIOR
FREQUENTIST
DECISION
THEORY
IN
FREQUENTIST
OR
CLASSICAL
DECISION
THEORY
THERE
IS
A
LOSS
FUNCTION
AND
A
LIKELIHOOD
BUT
THERE
IS
NO
PRIOR
AND
HENCE
NO
POSTERIOR
OR
POSTERIOR
EXPECTED
LOSS
THUS
THERE
IS
NO
AUTOMATIC
WAY
OF
DERIVING
AN
OPTIMAL
ESTIMATOR
UNLIKE
THE
BAYESIAN
CASE
INSTEAD
IN
THE
FREQUENTIST
APPROACH
WE
ARE
FREE
TO
CHOOSE
ANY
ESTIMATOR
OR
DECISION
PROCEDURE
Δ
X
A
WE
WANT
THE
GRADIENT
MUST
BE
ZERO
AT
A
MAXIMUM
SO
THE
VARIANCE
REDUCES
TO
THE
EXPECTED
SQUARE
OF
THE
SCORE
FUNCTION
I
Θˆ
Θ
EΘ
D
LOG
P
X
Θ
IT
CAN
BE
SHOWN
E
G
RICE
THAT
EΘ
D
LOG
P
X
Θ
DΘ
DΘ
IS
A
MUCH
MORE
INTUITIVE
QUANTITY
THAN
THE
VARIANCE
OF
THE
SCORE
IN
PRACTICE
THE
FREQUENTIST
APPROACH
IS
USUALLY
ONLY
APPLIED
TO
ONE
SHOT
STATISTICAL
DECISION
PROBLEMS
SUCH
AS
CLASSIFICATION
REGRESSION
AND
PARAMETER
ESTIMATION
SINCE
ITS
NON
CONSTRUCTIVE
NATURE
MAKES
IT
DIFFICULT
TO
APPLY
TO
SEQUENTIAL
DECISION
PROBLEMS
WHICH
ADAPT
TO
DATA
ONLINE
HAVING
CHOSEN
AN
ESTIMATOR
WE
DEFINE
ITS
EXPECTED
LOSS
OR
RISK
AS
FOLLOWS
R
Θ
Δ
EP
D
Θ
IL
Θ
Δ
D
R
L
Θ
Δ
D
P
D
Θ
DD
WHERE
IS
DATA
SAMPLED
FROM
NATURE
DISTRIBUTION
WHICH
IS
REPRESENTED
BY
PARAMETER
Θ
IN
OTHER
WORDS
THE
EXPECTATION
IS
WRT
THE
SAMPLING
DISTRIBUTION
OF
THE
ESTIMATOR
COMPARE
THIS
TO
THE
BAYESIAN
POSTERIOR
EXPECTED
LOSS
Ρ
A
D
Π
EP
Θ
D
Π
L
Θ
A
R
L
Θ
A
P
Θ
D
Π
DΘ
WE
SEE
THAT
THE
BAYESIAN
APPROACH
AVERAGES
OVER
Θ
WHICH
IS
UNKNOWN
AND
CONDITIONS
ON
WHICH
IS
KNOWN
WHEREAS
THE
FREQUENTIST
APPROACH
AVERAGES
OVER
THUS
IGNORING
THE
OBSERVED
DATA
AND
CONDITIONS
ON
Θ
WHICH
IS
UNKNOWN
NOT
ONLY
IS
THE
FREQUENTIST
DEFINITION
UNNATURAL
IT
CANNOT
EVEN
BE
COMPUTED
BECAUSE
Θ
IS
UNKNOWN
CONSEQUENTLY
WE
CANNOT
COMPARE
DIFFERENT
ESTIMATORS
IN
TERMS
OF
THEIR
FREQUENTIST
RISK
WE
DISCUSS
VARIOUS
SOLUTIONS
TO
THIS
BELOW
BAYES
RISK
HOW
DO
WE
CHOOSE
AMONGST
ESTIMATORS
WE
NEED
SOME
WAY
TO
CONVERT
R
Θ
Δ
INTO
A
SINGLE
MEASURE
OF
QUALITY
R
Δ
WHICH
DOES
NOT
DEPEND
ON
KNOWING
Θ
ONE
APPROACH
IS
TO
PUT
A
PRIOR
ON
Θ
AND
THEN
TO
DEFINE
BAYES
RISK
OR
INTEGRATED
RISK
OF
AN
ESTIMATOR
AS
FOLLOWS
RB
Δ
EP
Θ
R
Θ
Δ
R
R
Θ
Δ
P
Θ
DΘ
A
BAYES
ESTIMATOR
OR
BAYES
DECISION
RULE
IS
ONE
WHICH
MINIMIZES
THE
EXPECTED
RISK
ΔB
ARGMIN
RB
Δ
Δ
NOTE
THAT
THE
INTEGRATED
RISK
IS
ALSO
CALLED
THE
PREPOSTERIOR
RISK
SINCE
IT
IS
BEFORE
WE
HAVE
SEEN
THE
DATA
MINIMIZING
THIS
CAN
BE
USEFUL
FOR
EXPERIMENT
DESIGN
WE
WILL
NOW
PROVE
A
VERY
IMPORTANT
THEOREM
THAT
CONNECTS
THE
BAYESIAN
AND
FREQUENTIST
APPROACHES
TO
DECISION
THEORY
THEOREM
A
BAYES
ESTIMATOR
CAN
BE
OBTAINED
BY
MINIMIZING
THE
POSTERIOR
EXPECTED
LOSS
FOR
EACH
X
PROOF
BY
SWITCHING
THE
ORDER
OF
INTEGRATION
WE
HAVE
RB
Δ
R
R
L
Y
Δ
X
P
X
Y
Θ
P
Θ
DΘ
X
Y
R
L
Y
Δ
X
P
X
Y
Θ
DΘ
R
L
Y
Δ
X
P
Y
X
P
X
Ρ
Δ
X
X
P
X
X
Θ
FIGURE
RISK
FUNCTIONS
FOR
TWO
DECISION
PROCEDURES
AND
SINCE
HAS
LOWER
WORST
CASE
RISK
IT
IS
THE
MINIMAX
ESTIMATOR
EVEN
THOUGH
HAS
LOWER
RISK
FOR
MOST
VALUES
OF
Θ
THUS
MINIMAX
ESTIMATORS
ARE
OVERLY
CONSERVATIVE
TO
MINIMIZE
THE
OVERALL
EXPECTATION
WE
JUST
MINIMIZE
THE
TERM
INSIDE
FOR
EACH
X
SO
OUR
DECISION
RULE
IS
TO
PICK
ΔB
X
ARGMIN
Ρ
A
X
A
A
HENCE
WE
SEE
THAT
THE
PICKING
THE
OPTIMAL
ACTION
ON
A
CASE
BY
CASE
BASIS
AS
IN
THE
BAYESIAN
APPROACH
IS
OPTIMAL
ON
AVERAGE
AS
IN
THE
FREQUENTIST
APPROACH
IN
OTHER
WORDS
THE
BAYESIAN
APPROACH
PROVIDES
A
GOOD
WAY
OF
ACHIEVING
FREQUENTIST
GOALS
IN
FACT
ONE
CAN
GO
FURTHER
AND
PROVE
THE
FOLLOWING
THEOREM
WALD
EVERY
ADMISSABLE
DECISION
RULE
IS
A
BAYES
DECISION
RULE
WITH
RESPECT
TO
SOME
POSSIBLY
IMPROPER
PRIOR
DISTRIBUTION
THIS
THEOREM
SHOWS
THAT
THE
BEST
WAY
TO
MINIMIZE
FREQUENTIST
RISK
IS
TO
BE
BAYESIAN
SEE
BERNARDO
AND
SMITH
FOR
FURTHER
DISCUSSION
OF
THIS
POINT
MINIMAX
RISK
OBVIOUSLY
SOME
FREQUENTISTS
DISLIKE
USING
BAYES
RISK
SINCE
IT
REQUIRES
THE
CHOICE
OF
A
PRIOR
AL
THOUGH
THIS
IS
ONLY
IN
THE
EVALUATION
OF
THE
ESTIMATOR
NOT
NECESSARILY
AS
PART
OF
ITS
CONSTRUCTION
AN
ALTERNATIVE
APPROACH
IS
AS
FOLLOWS
DEFINE
THE
MAXIMUM
RISK
OF
AN
ESTIMATOR
AS
RMAX
Δ
MAX
R
Θ
Δ
Θ
A
MINIMAX
RULE
IS
ONE
WHICH
MINIMIZES
THE
MAXIMUM
RISK
ΔMM
ARGMIN
RMAX
Δ
Δ
FOR
EXAMPLE
IN
FIGURE
WE
SEE
THAT
HAS
LOWER
WORST
CASE
RISK
THAN
RANGING
OVER
ALL
POSSIBLE
VALUES
OF
Θ
SO
IT
IS
THE
MINIMAX
ESTIMATOR
SEE
SECTION
FOR
AN
EXPLANATION
OF
HOW
TO
COMPUTE
A
RISK
FUNCTION
FOR
AN
ACTUAL
MODEL
MINIMAX
ESTIMATORS
HAVE
A
CERTAIN
APPEAL
HOWEVER
COMPUTING
THEM
CAN
BE
HARD
AND
FURTHERMORE
THEY
ARE
VERY
PESSIMISTIC
IN
FACT
ONE
CAN
SHOW
THAT
ALL
MINIMAX
ESTIMATORS
ARE
EQUIVALENT
TO
BAYES
ESTIMATORS
UNDER
A
LEAST
FAVORABLE
PRIOR
IN
MOST
STATISTICAL
SITUATIONS
EXCLUDING
GAME
THEORETIC
ONES
ASSUMING
NATURE
IS
AN
ADVERSARY
IS
NOT
A
REASONABLE
ASSUMPTION
ADMISSIBLE
ESTIMATORS
THE
BASIC
PROBLEM
WITH
FREQUENTIST
DECISION
THEORY
IS
THAT
IT
RELIES
ON
KNOWING
THE
TRUE
DISTRI
BUTION
P
Θ
IN
ORDER
TO
EVALUATE
THE
RISK
HOWEVER
IT
MIGHT
BE
THE
CASE
THAT
SOME
ESTIMATORS
ARE
WORSE
THAN
OTHERS
REGARDLESS
OF
THE
VALUE
OF
Θ
IN
PARTICULAR
IF
R
Θ
R
Θ
FOR
ALL
Θ
Θ
THEN
WE
SAY
THAT
DOMINATES
THE
DOMINATION
IS
SAID
TO
BE
STRICT
IF
THE
INEQUALITY
IS
STRICT
FOR
SOME
Θ
AN
ESTIMATOR
IS
SAID
TO
BE
ADMISSIBLE
IF
IT
IS
NOT
STRICTLY
DOMINATED
BY
ANY
OTHER
ESTIMATOR
EXAMPLE
LET
US
GIVE
AN
EXAMPLE
BASED
ON
BERNARDO
AND
SMITH
CONSIDER
THE
PROBLEM
OF
ESTIMATING
THE
MEAN
OF
A
GAUSSIAN
WE
ASSUME
THE
DATA
IS
SAMPLED
FROM
XI
Θ
AND
USE
QUADRATIC
LOSS
L
Θ
Θˆ
Θ
Θˆ
THE
CORRESPONDING
RISK
FUNCTION
IS
THE
MSE
SOME
POSSIBLE
DECISION
RULES
OR
ESTIMATORS
Θˆ
X
Δ
X
ARE
AS
FOLLOWS
X
X
THE
SAMPLE
MEAN
X
X
THE
SAMPLE
MEDIAN
X
A
FIXED
VALUE
ΔΚ
X
THE
POSTERIOR
MEAN
UNDER
A
N
Θ
Κ
PRIOR
N
Κ
ΔΚ
X
N
ΚX
N
WX
W
FOR
ΔΚ
WE
CONSIDER
A
WEAK
PRIOR
Κ
AND
A
STRONGER
PRIOR
Κ
THE
PRIOR
MEAN
IS
SOME
FIXED
VALUE
WE
ASSUME
IS
KNOWN
THUS
X
IS
THE
SAME
AS
ΔΚ
X
WITH
AN
INFINITELY
STRONG
PRIOR
Κ
LET
US
NOW
DERIVE
THE
RISK
FUNCTIONS
ANALYTICALLY
WE
CAN
DO
THIS
SINCE
IN
THIS
TOY
EXAMPLE
WE
KNOW
THE
TRUE
PARAMETER
Θ
IN
SECTION
WE
SHOW
THAT
THE
MSE
CAN
BE
DECOMPOSED
INTO
SQUARED
BIAS
PLUS
VARIANCE
MSE
Θˆ
Θ
VAR
IΘˆ
Θˆ
THE
SAMPLE
MEAN
IS
UNBIASED
SO
ITS
RISK
IS
MSE
Θ
VAR
X
N
RISK
FUNCTIONS
FOR
N
RISK
FUNCTIONS
FOR
N
A
B
FIGURE
RISK
FUNCTIONS
FOR
ESTIMATING
THE
MEAN
OF
A
GAUSSIAN
USING
DATA
SAMPLED
Θ
THE
SOLID
DARK
BLUE
HORIZONTAL
LINE
IS
THE
MLE
THE
SOLID
LIGHT
BLUE
CURVED
LINE
IS
THE
POSTERIOR
MEAN
WHEN
Κ
LEFT
N
SAMPLES
RIGHT
N
SAMPLES
BASED
ON
FIGURE
B
OF
BERNARDO
AND
SMITH
FIGURE
GENERATED
BY
RISKFNGAUSS
THE
SAMPLE
MEDIAN
IS
ALSO
UNBIASED
ONE
CAN
SHOW
THAT
THE
VARIANCE
IS
APPROXIMATELY
Π
SO
MSE
Δ
Θ
Π
FOR
X
THE
VARIANCE
IS
ZERO
SO
MSE
Θ
Θ
FINALLY
FOR
THE
POSTERIOR
MEAN
WE
HAVE
MSE
ΔΚ
Θ
E
I
WX
W
Θ
E
I
W
X
Θ
W
Θ
W
N
W
Θ
Θ
N
Κ
Θ
THESE
FUNCTIONS
ARE
PLOTTED
IN
FIGURE
FOR
N
WE
SEE
THAT
IN
GENERAL
THE
BEST
ESTIMATOR
DEPENDS
ON
THE
VALUE
OF
Θ
WHICH
IS
UNKNOWN
IF
Θ
IS
VERY
CLOSE
TO
THEN
WHICH
JUST
PREDICTS
IS
BEST
IF
Θ
IS
WITHIN
SOME
REASONABLE
RANGE
AROUND
THEN
THE
POSTERIOR
MEAN
WHICH
COMBINES
THE
PRIOR
GUESS
OF
WITH
THE
ACTUAL
DATA
IS
BEST
IF
Θ
IS
FAR
FROM
THE
MLE
IS
BEST
NONE
OF
THIS
SHOULD
BE
SUPRISING
A
SMALL
AMOUNT
OF
SHRINKAGE
USING
THE
POSTERIOR
MEAN
WITH
A
WEAK
PRIOR
IS
USUALLY
DESIRABLE
ASSUMING
OUR
PRIOR
MEAN
IS
SENSIBLE
WHAT
IS
MORE
SURPRISING
IS
THAT
THE
RISK
OF
DECISION
RULE
SAMPLE
MEDIAN
IS
ALWAYS
HIGHER
THAN
THAT
OF
SAMPLE
MEAN
FOR
EVERY
VALUE
OF
Θ
CONSEQUENTLY
THE
SAMPLE
MEDIAN
IS
AN
INADMISSIBLE
ESTIMATOR
FOR
THIS
PARTICULAR
PROBLEM
WHERE
THE
DATA
IS
ASSUMED
TO
COME
FROM
A
GAUSSIAN
IN
PRACTICE
THE
SAMPLE
MEDIAN
IS
OFTEN
BETTER
THAN
THE
SAMPLE
MEAN
BECAUSE
IT
IS
MORE
ROBUST
TO
OUTLIERS
ONE
CAN
SHOW
MINKA
THAT
THE
MEDIAN
IS
THE
BAYES
ESTIMATOR
UNDER
SQUARED
LOSS
IF
WE
ASSUME
THE
DATA
COMES
FROM
A
LAPLACE
DISTRIBUTION
WHICH
HAS
HEAVIER
TAILS
THAN
A
GAUSSIAN
MORE
GENERALLY
WE
CAN
CONSTRUCT
ROBUST
ESTIMATORS
BY
USING
FLEXIBLE
MODELS
OF
OUR
DATA
SUCH
AS
MIXTURE
MODELS
OR
NON
PARAMETRIC
DENSITY
ESTIMATORS
SECTION
AND
THEN
COMPUTING
THE
POSTERIOR
MEAN
OR
MEDIAN
STEIN
PARADOX
SUPPOSE
WE
HAVE
N
IID
RANDOM
VARIABLES
XI
N
ΘI
AND
WE
WANT
TO
ESTIMATE
THE
ΘI
THE
OBVIOUS
ESTIMATOR
IS
THE
MLE
WHICH
IN
THIS
CASE
SETS
ΘˆI
XI
IT
TURNS
OUT
THAT
THIS
IS
AN
INADMISSIBLE
ESTIMATOR
UNDER
QUADRATIC
LOSS
WHEN
N
TO
SHOW
THIS
IT
SUFFICES
TO
CONSTRUCT
AN
ESTIMATOR
THAT
IS
BETTER
THE
JAMES
STEIN
ESTIMATOR
IS
ONE
SUCH
ESTIMATOR
AND
IS
DEFINED
AS
FOLLOWS
ΘˆI
BˆX
Bˆ
XI
X
Bˆ
XI
X
WHERE
X
N
XI
AND
B
IS
SOME
TUNING
CONSTANT
THIS
ESTIMATE
SHRINKS
THE
ΘI
TOWARDS
THE
OVERALL
MEAN
WE
DERIVE
THIS
ESTIMATOR
USING
AN
EMPIRICAL
BAYES
APPROACH
IN
SECTION
IT
CAN
BE
SHOWN
THAT
THIS
SHRINKAGE
ESTIMATOR
HAS
LOWER
FREQUENTIST
RISK
MSE
THAN
THE
MLE
SAMPLE
MEAN
FOR
N
THIS
IS
KNOWN
AS
STEIN
PARADOX
THE
REASON
IT
IS
CALLED
A
PARADOX
IS
ILLUSTRATED
BY
THE
FOLLOWING
EXAMPLE
SUPPOSE
ΘI
IS
THE
TRUE
IQ
OF
STUDENT
I
AND
XI
IS
HIS
TEST
SCORE
WHY
SHOULD
MY
ESTIMATE
OF
ΘI
DEPEND
ON
THE
GLOBAL
MEAN
X
AND
HENCE
ON
SOME
OTHER
STUDENT
SCORES
ONE
CAN
CREATE
EVEN
MORE
PARADOXICAL
EXAMPLES
BY
MAKING
THE
DIFFERENT
DIMENSIONS
BE
QUALITATIVELY
DIFFERENT
E
G
IS
MY
IQ
IS
THE
AVERAGE
RAINFALL
IN
VANCOUVER
ETC
THE
SOLUTION
TO
THE
PARADOX
IS
THE
FOLLOWING
IF
YOUR
GOAL
IS
TO
ESTIMATE
JUST
ΘI
YOU
CANNOT
DO
BETTER
THAN
USING
XI
BUT
IF
THE
GOAL
IS
TO
ESTIMATE
THE
WHOLE
VECTOR
Θ
AND
YOU
USE
SQUARED
ERROR
AS
YOUR
LOSS
FUNCTION
THEN
SHRINKAGE
HELPS
TO
SEE
THIS
SUPPOSE
WE
WANT
TO
ESTIMATE
Θ
FROM
A
SINGLE
SAMPLE
X
N
Θ
I
A
SIMPLE
ESTIMATE
IS
X
BUT
THIS
WILL
OVERESTIMATE
THE
RESULT
SINCE
E
E
I
I
I
N
Θ
CONSEQUENTLY
WE
CAN
REDUCE
OUR
RISK
BY
POOLING
INFORMATION
EVEN
FROM
UNRELATED
SOURCES
AND
SHRINKING
TOWARDS
THE
OVERALL
MEAN
IN
SECTION
WE
GIVE
A
BAYESIAN
EXPLANATION
FOR
THIS
SEE
ALSO
EFRON
AND
MORRIS
ADMISSIBILITY
IS
NOT
ENOUGH
IT
SEEMS
CLEAR
THAT
WE
CAN
RESTRICT
OUR
SEARCH
FOR
GOOD
ESTIMATORS
TO
THE
CLASS
OF
ADMISSIBLE
ESTIMATORS
BUT
IN
FACT
IT
IS
EASY
TO
CONSTRUCT
ADMISSIBLE
ESTIMATORS
AS
WE
SHOW
IN
THE
FOLLOWING
EXAMPLE
THEOREM
LET
X
Θ
AND
CONSIDER
ESTIMATING
Θ
UNDER
SQUARED
LOSS
LET
X
A
CONSTANT
INDEPENDENT
OF
THE
DATA
THIS
IS
AN
ADMISSIBLE
ESTIMATOR
PROOF
SUPPOSE
NOT
THEN
THERE
IS
SOME
OTHER
ESTIMATOR
WITH
SMALLER
RISK
SO
R
Θ
R
Θ
WHERE
THE
INEQUALITY
MUST
BE
STRICT
FOR
SOME
Θ
SUPPOSE
THE
TRUE
PARAMETER
IS
Θ
THEN
R
Θ
AND
R
Θ
R
X
X
DX
SINCE
R
Θ
R
Θ
FOR
ALL
Θ
AND
R
WE
HAVE
R
AND
HENCE
X
X
THUS
THE
ONLY
WAY
CAN
AVOID
HAVING
HIGHER
RISK
THAN
AT
SOME
SPECIFIC
POINT
IS
BY
BEING
EQUAL
TO
HENCE
THERE
IS
NO
OTHER
ESTIMATOR
WITH
STRICTLY
LOWER
RISK
SO
IS
ADMISSIBLE
DESIRABLE
PROPERTIES
OF
ESTIMATORS
SINCE
FREQUENTIST
DECISION
THEORY
DOES
NOT
PROVIDE
AN
AUTOMATIC
WAY
TO
CHOOSE
THE
BEST
ESTIMATOR
WE
NEED
TO
COME
UP
WITH
OTHER
HEURISTICS
FOR
CHOOSING
AMONGST
THEM
IN
THIS
SECTION
WE
DISCUSS
SOME
PROPERTIES
WE
WOULD
LIKE
ESTIMATORS
TO
HAVE
UNFORTUNATELY
WE
WILL
SEE
THAT
WE
CANNOT
ACHIEVE
ALL
OF
THESE
PROPERTIES
AT
THE
SAME
TIME
CONSISTENT
ESTIMATORS
AN
ESTIMATOR
IS
SAID
TO
BE
CONSISTENT
IF
IT
EVENTUALLY
RECOVERS
THE
TRUE
PARAMETERS
THAT
GENERATED
THE
DATA
AS
THE
SAMPLE
SIZE
GOES
TO
INFINITY
I
E
Θˆ
Θ
AS
WHERE
THE
ARROW
DENOTES
CONVERGENCE
IN
PROBABILITY
OF
COURSE
THIS
CONCEPT
ONLY
MAKES
SENSE
IF
THE
DATA
ACTUALLY
COMES
FROM
THE
SPECIFIED
MODEL
WITH
PARAMETERS
Θ
WHICH
IS
NOT
USUALLY
THE
CASE
WITH
REAL
DATA
NEVERTHELESS
IT
CAN
BE
A
USEFUL
THEORETICAL
PROPERTY
IT
CAN
BE
SHOWN
THAT
THE
MLE
IS
A
CONSISTENT
ESTIMATOR
THE
INTUITIVE
REASON
IS
THAT
MAXI
MIZING
LIKELIHOOD
IS
EQUIVALENT
TO
MINIMIZING
KL
P
Θ
P
Θˆ
WHERE
P
Θ
IS
THE
TRUE
DISTRIBUTION
AND
P
Θˆ
IS
OUR
ESTIMATE
WE
CAN
ACHIEVE
KL
DIVERGENCE
IFF
Θˆ
Θ
UNBIASED
ESTIMATORS
THE
BIAS
OF
AN
ESTIMATOR
IS
DEFINED
AS
BIAS
Θˆ
EP
D
Θ
IΘˆ
D
Θ
WHERE
Θ
IS
THE
TRUE
PARAMETER
VALUE
IF
THE
BIAS
IS
ZERO
THE
ESTIMATOR
IS
CALLED
UNBIASED
THIS
MEANS
THE
SAMPLING
DISTRIBUTION
IS
CENTERED
ON
THE
TRUE
PARAMETER
FOR
EXAMPLE
THE
MLE
FOR
A
GAUSSIAN
MEAN
IS
UNBIASED
BIAS
Μˆ
E
X
Μ
E
N
N
I
Μ
NΜ
N
Μ
IF
THE
MODEL
IS
UNIDENTIFIABLE
THE
MLE
MAY
SELECT
A
SET
OF
PARAMETERS
THAT
IS
DIFFERENT
FROM
THE
TRUE
PARAMETERS
BUT
FOR
WHICH
THE
INDUCED
DISTRIBUTION
P
Θˆ
IS
THE
SAME
AS
THE
EXACT
DISTRIBUTION
SUCH
PARAMETERS
ARE
SAID
TO
BE
LIKELIHOOD
EQUIVALENT
HOWEVER
THE
MLE
FOR
A
GAUSSIAN
VARIANCE
IS
NOT
AN
UNBIASED
ESTIMATOR
OF
IN
FACT
ONE
CAN
SHOW
EXERCISE
THAT
E
N
N
HOWEVER
THE
FOLLOWING
ESTIMATOR
N
X
X
IS
AN
UNBIASED
ESTIMATOR
WHICH
WE
CAN
EASILY
PROVE
AS
FOLLOWS
E
E
N
N
N
IN
MATLAB
VAR
X
RETURNS
WHEREAS
VAR
X
RETURNS
THE
MLE
FOR
LARGE
ENOUGH
N
N
THE
DIFFERENCE
WILL
BE
NEGLIGIBLE
ALTHOUGH
THE
MLE
MAY
SOMETIMES
BE
A
BIASED
ESTIMATOR
ONE
CAN
SHOW
THAT
ASYMPTOTICALLY
IT
IS
ALWAYS
UNBIASED
THIS
IS
NECESSARY
FOR
THE
MLE
TO
BE
A
CONSISTENT
ESTIMATOR
ALTHOUGH
BEING
UNBIASED
SOUNDS
LIKE
A
DESIRABLE
PROPERTY
THIS
IS
NOT
ALWAYS
TRUE
SEE
SEC
TION
AND
LINDLEY
FOR
DISCUSSION
OF
THIS
POINT
MINIMUM
VARIANCE
ESTIMATORS
IT
SEEMS
INTUITIVELY
REASONABLE
THAT
WE
WANT
OUR
ESTIMATOR
TO
BE
UNBIASED
ALTHOUGH
WE
SHALL
GIVE
SOME
ARGUMENTS
AGAINST
THIS
CLAIM
BELOW
HOWEVER
BEING
UNBIASED
IS
NOT
ENOUGH
FOR
EXAMPLE
SUPPOSE
WE
WANT
TO
ESTIMATE
THE
MEAN
OF
A
GAUSSIAN
FROM
D
XN
THE
ESTIMATOR
THAT
JUST
LOOKS
AT
THE
FIRST
DATA
POINT
Θˆ
D
IS
AN
UNBIASED
ESTIMATOR
BUT
WILL
GENERALLY
BE
FURTHER
FROM
Θ
THAN
THE
EMPIRICAL
MEAN
X
WHICH
IS
ALSO
UNBIASED
SO
THE
VARIANCE
OF
AN
ESTIMATOR
IS
ALSO
IMPORTANT
A
NATURAL
QUESTION
IS
HOW
LONG
CAN
THE
VARIANCE
GO
A
FAMOUS
RESULT
CALLED
THE
CRAMER
RAO
LOWER
BOUND
PROVIDES
A
LOWER
BOUND
ON
THE
VARIANCE
OF
ANY
UNBIASED
ESTIMATOR
MORE
PRECISELY
THEOREM
CRAMER
RAO
INEQUALITY
LET
XN
P
X
AND
Θˆ
Θˆ
XN
BE
AN
UNBIASED
ESTIMATOR
OF
THEN
UNDER
VARIOUS
SMOOTHNESS
ASSUMPTIONS
ON
P
X
WE
HAVE
VAR
Θˆ
NI
WHERE
I
IS
THE
FISHER
INFORMATION
MATRIX
SEE
SECTION
A
PROOF
CAN
BE
FOUND
E
G
IN
RICE
IT
CAN
BE
SHOWN
THAT
THE
MLE
ACHIEVES
THE
CRAMER
RAO
LOWER
BOUND
AND
HENCE
HAS
THE
SMALLEST
ASYMPTOTIC
VARIANCE
OF
ANY
UNBIASED
ESTIMATOR
THUS
MLE
IS
SAID
TO
BE
ASYMPTOTICALLY
OPTIMAL
THE
BIAS
VARIANCE
TRADEOFF
ALTHOUGH
USING
AN
UNBIASED
ESTIMATOR
SEEMS
LIKE
A
GOOD
IDEA
THIS
IS
NOT
ALWAYS
THE
CASE
TO
SEE
WHY
SUPPOSE
WE
USE
QUADRATIC
LOSS
AS
WE
SHOWED
ABOVE
THE
CORRESPONDING
RISK
IS
THE
MSE
WE
NOW
DERIVE
A
VERY
USEFUL
DECOMPOSITION
OF
THE
MSE
ALL
EXPECTATIONS
AND
VARIANCES
ARE
WRT
Θˆ
Θˆ
D
DENOTE
THE
ESTIMATE
AND
Θ
E
Θˆ
VARY
D
THEN
WE
HAVE
DENOTE
THE
EXPECTED
VALUE
OF
THE
ESTIMATE
AS
WE
E
Θˆ
Θ
E
Θˆ
Θ
Θ
Θ
E
Θˆ
Θ
Θ
Θ
E
Θˆ
Θ
Θ
Θ
E
Θˆ
Θ
Θ
Θ
VAR
Θˆ
Θˆ
IN
WORDS
THIS
IS
CALLED
THE
BIAS
VARIANCE
TRADEOFF
SEE
E
G
GEMAN
ET
AL
WHAT
IT
MEANS
IS
THAT
IT
MIGHT
BE
WISE
TO
USE
A
BIASED
ESTIMATOR
SO
LONG
AS
IT
REDUCES
OUR
VARIANCE
ASSUMING
OUR
GOAL
IS
TO
MINIMIZE
SQUARED
ERROR
EXAMPLE
ESTIMATING
A
GAUSSIAN
MEAN
LET
US
GIVE
AN
EXAMPLE
BASED
ON
HOFF
SUPPOSE
WE
WANT
TO
ESTIMATE
THE
MEAN
OF
A
GAUSSIAN
FROM
X
XN
WE
ASSUME
THE
DATA
IS
SAMPLED
FROM
XI
Θ
AN
OBVIOUS
ESTIMATE
IS
THE
MLE
THIS
HAS
A
BIAS
OF
AND
A
VARIANCE
OF
VAR
X
Θ
N
BUT
WE
COULD
ALSO
USE
A
MAP
ESTIMATE
IN
SECTION
WE
SHOW
THAT
THE
MAP
ESTIMATE
UNDER
A
GAUSSIAN
PRIOR
OF
THE
FORM
N
IS
GIVEN
BY
N
X
X
Θ
WX
W
Θ
N
N
WHERE
W
CONTROLS
HOW
MUCH
WE
TRUST
THE
MLE
COMPARED
TO
OUR
PRIOR
THIS
IS
ALSO
THE
POSTERIOR
MEAN
SINCE
THE
MEAN
AND
MODE
OF
A
GAUSSIAN
ARE
THE
SAME
THE
BIAS
AND
VARIANCE
ARE
GIVEN
BY
E
X
Θ
W
Θ
W
Θ
VAR
X
W
N
SAMPLING
DISTRIBUTION
TRUTH
PRIOR
N
MSE
OF
POSTMEAN
MSE
OF
MLE
A
SAMPLE
SIZE
B
FIGURE
LEFT
SAMPLING
DISTRIBUTION
OF
THE
MAP
ESTIMATE
WITH
DIFFERENT
PRIOR
STRENGTHS
THE
MLE
CORRESPONDS
TO
RIGHT
MSE
RELATIVE
TO
THAT
OF
THE
MLE
VERSUS
SAMPLE
SIZE
BASED
ON
FIGURE
OF
HOFF
FIGURE
GENERATED
BY
SAMPLINGDISTGAUSSSHRINKAGE
SO
ALTHOUGH
THE
MAP
ESTIMATE
IS
BIASED
ASSUMING
W
IT
HAS
LOWER
VARIANCE
LET
US
ASSUME
THAT
OUR
PRIOR
IS
SLIGHTLY
MISSPECIFIED
SO
WE
USE
WHEREAS
THE
TRUTH
IS
Θ
IN
FIGURE
A
WE
SEE
THAT
THE
SAMPLING
DISTRIBUTION
OF
THE
MAP
ESTIMATE
FOR
IS
BIASED
AWAY
FROM
THE
TRUTH
BUT
HAS
LOWER
VARIANCE
IS
NARROWER
THAN
THAT
OF
THE
MLE
IN
FIGURE
B
WE
PLOT
MSE
X
MSE
X
VS
N
WE
SEE
THAT
THE
MAP
ESTIMATE
HAS
LOWER
MSE
THAN
THE
MLE
ESPECIALLY
FOR
SMALL
SAMPLE
SIZE
FOR
THE
CASE
CORRESPONDS
TO
THE
MLE
AND
THE
CASE
CORRESPONDS
TO
A
STRONG
PRIOR
WHICH
HURTS
PERFORMANCE
BECAUSE
THE
PRIOR
MEAN
IS
WRONG
IT
IS
CLEARLY
IMPORTANT
TO
TUNE
THE
STRENGTH
OF
THE
PRIOR
A
TOPIC
WE
DISCUSS
LATER
EXAMPLE
RIDGE
REGRESSION
ANOTHER
IMPORTANT
EXAMPLE
OF
THE
BIAS
VARIANCE
TRADEOFF
ARISES
IN
RIDGE
REGRESSION
WHICH
WE
DISCUSS
IN
SECTION
IN
BRIEF
THIS
CORRESPONDS
TO
MAP
ESTIMATION
FOR
LINEAR
REGRESSION
UNDER
A
GAUSSIAN
PRIOR
P
W
W
Λ
THE
ZERO
MEAN
PRIOR
ENCOURAGES
THE
WEIGHTS
TO
BE
SMALL
WHICH
REDUCES
OVERFITTING
THE
PRECISION
TERM
Λ
CONTROLS
THE
STRENGTH
OF
THIS
PRIOR
SETTING
Λ
RESULTS
IN
THE
MLE
USING
Λ
RESULTS
IN
A
BIASED
ESTIMATE
TO
ILLUSTRATE
THE
EFFECT
ON
THE
VARIANCE
CONSIDER
A
SIMPLE
EXAMPLE
FIGURE
ON
THE
LEFT
PLOTS
EACH
INDIVIDUAL
FITTED
CURVE
AND
ON
THE
RIGHT
PLOTS
THE
AVERAGE
FITTED
CURVE
WE
SEE
THAT
AS
WE
INCREASE
THE
STRENGTH
OF
THE
REGULARIZER
THE
VARIANCE
DECREASES
BUT
THE
BIAS
INCREASES
BIAS
VARIANCE
TRADEOFF
FOR
CLASSIFICATION
IF
WE
USE
LOSS
INSTEAD
OF
SQUARED
ERROR
THE
ABOVE
ANALYSIS
BREAKS
DOWN
SINCE
THE
FREQUENTIST
RISK
IS
NO
LONGER
EXPRESSIBLE
AS
SQUARED
BIAS
PLUS
VARIANCE
IN
FACT
ONE
CAN
SHOW
EXERCISE
OF
HASTIE
ET
AL
THAT
THE
BIAS
AND
VARIANCE
COMBINE
MULTIPLICATIVELY
IF
THE
ESTIMATE
IS
ON
LN
LN
LN
LN
FIGURE
ILLUSTRATION
OF
BIAS
VARIANCE
TRADEOFF
FOR
RIDGE
REGRESSION
WE
GENERATE
DATA
SETS
FROM
THE
TRUE
FUNCTION
SHOWN
IN
SOLID
GREEN
LEFT
WE
PLOT
THE
REGULARIZED
FIT
FOR
DIFFERENT
DATA
SETS
WE
USE
LINEAR
REGRESSION
WITH
A
GAUSSIAN
RBF
EXPANSION
WITH
CENTERS
EVENLY
SPREAD
OVER
THE
INTERVAL
RIGHT
WE
PLOT
THE
AVERAGE
OF
THE
FITS
AVERAGED
OVER
ALL
DATASETS
TOP
ROW
STRONGLY
REGULARIZED
WE
SEE
THAT
THE
INDIVIDUAL
FITS
ARE
SIMILAR
TO
EACH
OTHER
LOW
VARIANCE
BUT
THE
AVERAGE
IS
FAR
FROM
THE
TRUTH
HIGH
BIAS
BOTTOM
ROW
LIGHTLY
REGULARIZED
WE
SEE
THAT
THE
INDIVIDUAL
FITS
ARE
QUITE
DIFFERENT
FROM
EACH
OTHER
HIGH
VARIANCE
BUT
THE
AVERAGE
IS
CLOSE
TO
THE
TRUTH
LOW
BIAS
BASED
ON
BISHOP
FIGURE
FIGURE
GENERATED
BY
THE
CORRECT
SIDE
OF
THE
DECISION
BOUNDARY
THEN
THE
BIAS
IS
NEGATIVE
AND
DECREASING
THE
VARIANCE
WILL
DECREASE
THE
MISCLASSIFICATION
RATE
BUT
IF
THE
ESTIMATE
IS
ON
THE
WRONG
SIDE
OF
THE
DECISION
BOUNDARY
THEN
THE
BIAS
IS
POSITIVE
SO
IT
PAYS
TO
INCREASE
THE
VARIANCE
FRIEDMAN
THIS
LITTLE
KNOWN
FACT
ILLUSTRATES
THAT
THE
BIAS
VARIANCE
TRADEOFF
IS
NOT
VERY
USEFUL
FOR
CLASSIFICATION
IT
IS
BETTER
TO
FOCUS
ON
EXPECTED
LOSS
SEE
BELOW
NOT
DIRECTLY
ON
BIAS
AND
VARIANCE
WE
CAN
APPROXIMATE
THE
EXPECTED
LOSS
USING
CROSS
VALIDATINON
AS
WE
DISCUSS
IN
SECTION
EMPIRICAL
RISK
MINIMIZATION
FREQUENTIST
DECISION
THEORY
SUFFERS
FROM
THE
FUNDAMENTAL
PROBLEM
THAT
ONE
CANNOT
ACTUALLY
COMPUTE
THE
RISK
FUNCTION
SINCE
IT
RELIES
ON
KNOWING
THE
TRUE
DATA
DISTRIBUTION
BY
CONTRAST
THE
BAYESIAN
POSTERIOR
EXPECTED
LOSS
CAN
ALWAYS
BE
COMPUTED
SINCE
IT
CONDITIONS
ON
THE
THE
DATA
RATHER
THAN
CONDITIONING
ON
Θ
HOWEVER
THERE
IS
ONE
SETTING
WHICH
AVOIDS
THIS
PROBLEM
AND
THAT
IS
WHERE
THE
TASK
IS
TO
PREDICT
OBSERVABLE
QUANTITIES
AS
OPPOSED
TO
ESTIMATING
HIDDEN
VARIABLES
OR
PARAMETERS
THAT
IS
INSTEAD
OF
LOOKING
AT
LOSS
FUNCTIONS
OF
THE
FORM
L
Θ
Δ
D
WHERE
Θ
IS
THE
TRUE
BUT
UNKNOWN
PARAMETER
AND
Δ
D
IS
OUR
ESTIMATOR
LET
US
LOOK
AT
LOSS
FUNCTIONS
OF
THE
FORM
L
Y
Δ
X
WHERE
Y
IS
THE
TRUE
BUT
UNKNOWN
RESPONSE
AND
Δ
X
IS
OUR
PREDICTION
GIVEN
THE
INPUT
X
IN
THIS
CASE
THE
FREQUENTIST
RISK
BECOMES
R
P
Δ
E
X
Y
P
L
Y
Δ
X
L
Y
Δ
X
P
X
Y
WHERE
P
REPRESENTS
NATURE
DISTRIBUTION
OF
COURSE
THIS
DISTRIBUTION
IS
UNKNOWN
BUT
A
SIMPLE
APPROACH
IS
TO
USE
THE
EMPIRICAL
DISTRIBUTION
DERIVED
FROM
SOME
TRAINING
DATA
TO
APPROXIMATE
P
I
E
P
X
Y
P
EMP
N
X
Y
ΔXI
N
I
X
ΔYI
Y
WE
THEN
DEFINE
THE
EMPIRICAL
RISK
AS
FOLLOWS
R
D
D
R
P
Δ
L
Y
Δ
X
IN
THE
CASE
OF
LOSS
L
Y
Δ
X
I
Y
Δ
X
THIS
BECOMES
THE
MISCLASSIFICATION
RATE
IN
THE
CASE
OF
SQUARED
ERROR
LOSS
L
Y
Δ
X
Y
Δ
X
THIS
BECOMES
THE
MEAN
SQUARED
ERROR
WE
DEFINE
THE
TASK
OF
EMPIRICAL
RISK
MINIMIZATION
OR
ERM
AS
FINDING
A
DECISION
PROCEDURE
TYPICALLY
A
CLASSIFICATION
RULE
TO
MINIMIZE
THE
EMPIRICAL
RISK
ΔERM
D
ARGMIN
REMP
D
Δ
IN
THE
UNSUPERVISED
CASE
WE
ELIMINATE
ALL
REFERENCES
TO
Y
AND
REPLACE
L
Y
Δ
X
WITH
L
X
Δ
X
WHERE
FOR
EXAMPLE
L
X
Δ
X
X
Δ
X
WHICH
MEASURES
THE
RECONSTRUC
TION
ERROR
WE
CAN
DEFINE
THE
DECISION
RULE
USING
Δ
X
DECODE
ENCODE
X
AS
IN
VECTOR
QUANTIZATION
SECTION
OR
PCA
SECTION
FINALLY
WE
DEFINE
THE
EMPIRICAL
RISK
AS
R
D
Δ
L
X
Δ
X
OF
COURSE
WE
CAN
ALWAYS
TRIVIALLY
MINIMIZE
THIS
RISK
BY
SETTING
Δ
X
X
SO
IT
IS
CRITICAL
THAT
THE
ENCODER
DECODER
GO
VIA
SOME
KIND
OF
BOTTLENECK
REGULARIZED
RISK
MINIMIZATION
NOTE
THAT
THE
EMPIRICAL
RISK
IS
EQUAL
TO
THE
BAYES
RISK
IF
OUR
PRIOR
ABOUT
NATURE
DISTRIBUTION
IS
THAT
IT
IS
EXACTLY
EQUAL
TO
THE
EMPIRICAL
DISTRIBUTION
MINKA
E
R
P
Δ
P
PEMP
REMP
D
Δ
THEREFORE
MINIMIZING
THE
EMPIRICAL
RISK
WILL
TYPICALLY
RESULT
IN
OVERFITTING
IT
IS
THEREFORE
OFTEN
NECESSARY
TO
ADD
A
COMPLEXITY
PENALTY
TO
THE
OBJECTIVE
FUNCTION
RT
D
Δ
REMP
D
Δ
ΛC
Δ
WHERE
C
Δ
MEASURES
THE
COMPLEXITY
OF
THE
PREDICTION
FUNCTION
Δ
X
AND
Λ
CONTROLS
THE
STRENGTH
OF
THE
COMPLEXITY
PENALTY
THIS
APPROACH
IS
KNOWN
AS
REGULARIZED
RISK
MINIMIZATION
RRM
NOTE
THAT
IF
THE
LOSS
FUNCTION
IS
NEGATIVE
LOG
LIKELIHOOD
AND
THE
REGULARIZER
IS
A
NEGATIVE
LOG
PRIOR
THIS
IS
EQUIVALENT
TO
MAP
ESTIMATION
THE
TWO
KEY
ISSUES
IN
RRM
ARE
HOW
DO
WE
MEASURE
COMPLEXITY
AND
HOW
DO
WE
PICK
Λ
FOR
A
LINEAR
MODEL
WE
CAN
DEFINE
THE
COMPLEXITY
OF
IN
TERMS
OF
ITS
DEGREES
OF
FREEDOM
DISCUSSED
IN
SECTION
FOR
MORE
GENERAL
MODELS
WE
CAN
USE
THE
VC
DIMENSION
DISCUSSED
IN
SECTION
TO
PICK
Λ
WE
CAN
USE
THE
METHODS
DISCUSSED
IN
SECTION
STRUCTURAL
RISK
MINIMIZATION
THE
REGULARIZED
RISK
MINIMIZATION
PRINCIPLE
SAYS
THAT
WE
SHOULD
FIT
THE
MODEL
FOR
A
GIVEN
COMPLEXITY
PENALTY
BY
USING
ΔˆΛ
ARGMIN
REMP
Δ
ΛC
Δ
Δ
BUT
HOW
SHOULD
WE
PICK
Λ
WE
CANNOT
USING
THE
TRAINING
SET
SINCE
THIS
WILL
UNDERESTIMATE
THE
TRUE
RISK
A
PROBLEM
KNOWN
AS
OPTIMISM
OF
THE
TRAINING
ERROR
AS
AN
ALTERNATIVE
WE
CAN
USE
THE
FOLLOWING
RULE
KNOWN
AS
THE
STRUCTURAL
RISK
MINIMIZATION
PRINCIPLE
VAPNIK
Λˆ
ARGMIN
Rˆ
ΔˆΛ
Λ
WHERE
Rˆ
Δ
IS
AN
ESTIMATE
OF
THE
RISK
THERE
ARE
TWO
WIDELY
USED
ESTIMATES
CROSS
VALIDATION
AND
THEORETICAL
UPPER
BOUNDS
ON
THE
RISK
WE
DISCUSS
BOTH
OF
THESE
BELOW
ESTIMATING
THE
RISK
USING
CROSS
VALIDATION
WE
CAN
ESTIMATE
THE
RISK
OF
SOME
ESTIMATOR
USING
A
VALIDATION
SET
IF
WE
DON
T
HAVE
A
SEPARATE
VALIDATION
SET
WE
CAN
USE
CROSS
VALIDATION
CV
AS
WE
BRIEFLY
DISCUSSED
IN
SECTION
MORE
PRECISELY
CV
IS
DEFINED
AS
FOLLOWS
LET
THERE
BE
N
D
DATA
CASES
IN
THE
TRAINING
SET
DENOTE
THE
DATA
IN
THE
K
TH
TEST
FOLD
BY
K
AND
ALL
THE
OTHER
DATA
BY
K
IN
STRATIFIED
CV
THESE
FOLDS
ARE
CHOSEN
SO
THE
CLASS
PROPORTIONS
IF
DISCRETE
LABELS
ARE
PRESENT
ARE
ROUGHLY
EQUAL
IN
EACH
FOLD
LET
BE
A
LEARNING
ALGORITHM
OR
FITTING
FUNCTION
THAT
TAKES
A
DATASET
AND
A
MODEL
INDEX
M
THIS
COULD
A
DISCRETE
INDEX
SUCH
AS
THE
DEGREE
OF
A
POLYNOMIAL
OR
A
CONTINUOUS
INDEX
SUCH
AS
THE
STRENGTH
OF
A
REGULARIZER
AND
RETURNS
A
PARAMETER
VECTOR
ΘˆM
F
D
M
FINALLY
LET
BE
A
PREDICTION
FUNCTION
THAT
TAKES
AN
INPUT
AND
A
PARAMETER
VECTOR
AND
RETURNS
A
PREDICTION
Yˆ
P
X
Θˆ
F
X
Θˆ
THUS
THE
COMBINED
FIT
PREDICT
CYCLE
IS
DENOTED
AS
FM
X
D
P
X
F
D
M
THE
K
FOLD
CV
ESTIMATE
OF
THE
RISK
OF
FM
IS
DEFINED
BY
K
R
M
D
K
L
Y
P
X
F
D
M
NOTE
THAT
WE
CAN
CALL
THE
FITTING
ALGORITHM
ONCE
PER
FOLD
LET
F
K
X
X
K
M
BE
THE
FUNCTION
THAT
WAS
TRAINED
ON
ALL
THE
DATA
EXCEPT
FOR
THE
TEST
DATA
IN
FOLD
K
THEN
WE
CAN
REWRITE
THE
CV
ESTIMATE
AS
K
R
M
D
K
N
L
YI
FK
XI
N
L
YI
FK
I
XI
N
K
I
DK
I
WHERE
K
I
IS
THE
FOLD
IN
WHICH
I
IS
USED
AS
TEST
DATA
IN
OTHER
WORDS
WE
PREDICT
YI
USING
A
MODEL
THAT
WAS
TRAINED
ON
DATA
THAT
DOES
NOT
CONTAIN
XI
OF
K
N
THE
METHOD
IS
KNOWN
AS
LEAVE
ONE
OUT
CROSS
VALIDATION
OR
LOOCV
IN
THIS
CASE
N
THE
ESTIMATED
RISK
BECOMES
R
M
D
N
L
Y
F
I
X
WHERE
F
I
X
P
X
F
D
I
M
THIS
REQUIRES
FITTING
THE
MODEL
N
TIMES
WHERE
FOR
F
I
WE
OMIT
THE
I
TH
TRAINING
CASE
FORTUNATELY
FOR
SOME
MODEL
CLASSES
AND
LOSS
FUNCTIONS
NAMELY
LINEAR
MODELS
AND
QUADRATIC
LOSS
WE
CAN
FIT
THE
MODEL
ONCE
AND
ANALYTICALLY
REMOVE
THE
EFFECT
OF
THE
I
TH
TRAINING
CASE
THIS
IS
KNOWN
AS
GENERALIZED
CROSS
VALIDATION
OR
GCV
EXAMPLE
USING
CV
TO
PICK
Λ
FOR
RIDGE
REGRESSION
AS
A
CONCRETE
EXAMPLE
CONSIDER
PICKING
THE
STRENGTH
OF
THE
REGULARIZER
IN
PENALIZED
LINEAR
REGRESSION
WE
USE
THE
FOLLOWING
RULE
Λˆ
ARG
MIN
Λ
ΛMIN
ΛMAX
R
Λ
DTRAIN
K
WHERE
ΛMIN
ΛMAX
IS
A
FINITE
RANGE
OF
Λ
VALUES
THAT
WE
SEARCH
OVER
AND
R
Λ
TRAIN
K
IS
THE
K
FOLD
CV
ESTIMATE
OF
THE
RISK
OF
USING
Λ
GIVEN
BY
K
K
R
Λ
D
K
L
Y
F
X
WHERE
F
K
X
XT
Wˆ
Λ
K
IS
THE
PREDICTION
FUNCTION
TRAINED
ON
DATA
EXCLUDING
FOLD
K
AND
Wˆ
Λ
ARG
MINW
NLL
W
Λ
W
IS
THE
MAP
ESTIMATE
FIGURE
B
GIVES
AN
EXAMPLE
OF
A
CV
ESTIMATE
OF
THE
RISK
VS
LOG
Λ
WHERE
THE
LOSS
FUNCTION
IS
SQUARED
ERROR
WHEN
PERFORMING
CLASSIFICATION
WE
USUALLY
USE
LOSS
IN
THIS
CASE
WE
OPTIMIZE
A
CONVEX
UPPER
BOUND
ON
THE
EMPIRICAL
RISK
TO
ESTIMATE
WΛM
BUT
WE
OPTIMIZE
THE
CV
ESTIMATE
OF
THE
RISK
ITSELF
TO
ESTIMATE
Λ
WE
CAN
HANDLE
THE
NON
SMOOTH
LOSS
FUNCTION
WHEN
ESTIMATING
Λ
BECAUSE
WE
ARE
USING
BRUTE
FORCE
SEARCH
OVER
THE
ENTIRE
ONE
DIMENSIONAL
SPACE
WHEN
WE
HAVE
MORE
THAN
ONE
OR
TWO
TUNING
PARAMETERS
THIS
APPROACH
BECOMES
INFEASIBLE
IN
SUCH
CASES
ONE
CAN
USE
EMPIRICAL
BAYES
WHICH
ALLOWS
ONE
TO
OPTIMIZE
LARGE
NUMBERS
OF
HYPER
PARAMETERS
USING
GRADIENT
BASED
OPTIMIZERS
INSTEAD
OF
BRUTE
FORCE
SEARCH
SEE
SECTION
FOR
DETAILS
MEAN
SQUARED
ERROR
FOLD
CROSS
VALIDATION
NTRAIN
LOG
LAMBDA
A
LOG
LAMBDA
B
FIGURE
A
MEAN
SQUARED
ERROR
FOR
T
PENALIZED
DEGREE
POLYNOMIAL
REGRESSION
VS
LOG
REGULARIZER
SAME
AS
IN
FIGURES
EXCEPT
NOW
WE
HAVE
N
TRAINING
POINTS
INSTEAD
OF
THE
STARS
CORRESPOND
TO
THE
VALUES
USED
TO
PLOT
THE
FUNCTIONS
IN
FIGURE
B
CV
ESTIMATE
THE
VERTICAL
SCALE
IS
TRUNCATED
FOR
CLARITY
THE
BLUE
LINE
CORRESPONDS
TO
THE
VALUE
CHOSEN
BY
THE
ONE
STANDARD
ERROR
RULE
FIGURE
GENERATED
BY
LINREGPOLYVSREGDEMO
THE
ONE
STANDARD
ERROR
RULE
THE
ABOVE
PROCEDURE
ESTIMATES
THE
RISK
BUT
DOES
NOT
GIVE
ANY
MEASURE
OF
UNCERTAINTY
A
STANDARD
FREQUENTIST
MEASURE
OF
UNCERTAINTY
OF
AN
ESTIMATE
IS
THE
STANDARD
ERROR
OF
THE
MEAN
DEFINED
BY
Σˆ
SE
N
N
WHERE
IS
AN
ESTIMATE
OF
THE
VARIANCE
OF
THE
LOSS
L
L
L
L
Y
FK
I
X
L
L
NOTE
THAT
Σ
MEASURES
THE
INTRINSIC
VARIABILITY
OF
LI
ACROSS
SAMPLES
WHEREAS
SE
MEASURES
OUR
UNCERTAINTY
ABOUT
THE
MEAN
L
SUPPOSE
WE
APPLY
CV
TO
A
SET
OF
MODELS
AND
COMPUTE
THE
MEAN
AND
SE
OF
THEIR
ESTIMATED
RISKS
A
COMMON
HEURISTIC
FOR
PICKING
A
MODEL
FROM
THESE
NOISY
ESTIMATES
IS
TO
PICK
THE
VALUE
WHICH
CORRESPONDS
TO
THE
SIMPLEST
MODEL
WHOSE
RISK
IS
NO
MORE
THAN
ONE
STANDARD
ERROR
ABOVE
THE
RISK
OF
THE
BEST
MODEL
THIS
IS
CALLED
THE
ONE
STANDARD
ERROR
RULE
HASTIE
ET
AL
FOR
EXAMPLE
IN
FIGURE
WE
SEE
THAT
THIS
HEURISTIC
DOES
NOT
CHOOSE
THE
LOWEST
POINT
ON
THE
CURVE
BUT
ONE
THAT
IS
SLIGHTLY
TO
ITS
RIGHT
SINCE
THAT
CORRESPONDS
TO
A
MORE
HEAVILY
REGULARIZED
MODEL
WITH
ESSENTIALLY
THE
SAME
EMPIRICAL
PERFORMANCE
CV
FOR
MODEL
SELECTION
IN
NON
PROBABILISTIC
UNSUPERVISED
LEARNING
IF
WE
ARE
PERFORMING
UNSUPERVISED
LEARNING
WE
MUST
USE
A
LOSS
FUNCTION
SUCH
AS
L
X
Δ
X
X
Δ
X
WHICH
MEASURES
RECONSTRUCTION
ERROR
HERE
Δ
X
IS
SOME
ENCODE
DECODE
SCHEME
HOWEVER
AS
WE
DISCUSSED
IN
SECTION
WE
CANNOT
USE
CV
TO
DETERMINE
THE
COMPLEXITY
OF
Δ
SINCE
WE
WILL
ALWAYS
GET
LOWER
LOSS
WITH
A
MORE
COMPLEX
MODEL
EVEN
IF
EVALUATED
ON
THE
TEST
SET
THIS
IS
BECAUSE
MORE
COMPLEX
MODELS
WILL
COMPRESS
THE
DATA
LESS
AND
INDUCE
LESS
DISTORTION
CONSEQUENTLY
WE
MUST
EITHER
USE
PROBABILISTIC
MODELS
OR
INVENT
OTHER
HEURISTICS
UPPER
BOUNDING
THE
RISK
USING
STATISTICAL
LEARNING
THEORY
THE
PRINCIPLE
PROBLEM
WITH
CROSS
VALIDATION
IS
THAT
IT
IS
SLOW
SINCE
WE
HAVE
TO
FIT
THE
MODEL
MULTIPLE
TIMES
THIS
MOTIVATES
THE
DESIRE
TO
COMPUTE
ANALYTIC
APPROXIMATIONS
OR
BOUNDS
TO
THE
GENERALIZATION
ERROR
THIS
IS
THE
STUDIED
IN
THE
FIELD
OF
STATISTICAL
LEARNING
THEORY
SLT
MORE
PRECISELY
SLT
TRIES
TO
BOUND
THE
RISK
R
P
H
FOR
ANY
DATA
DISTRIBUTION
P
AND
HYPOTHESIS
H
IN
TERMS
OF
THE
EMPIRICAL
RISK
REMP
H
THE
SAMPLE
SIZE
N
AND
THE
SIZE
OF
THE
HYPOTHESIS
SPACE
LET
US
INITIALLY
CONSIDER
THE
CASE
WHERE
THE
HYPOTHESIS
SPACE
IS
FINITE
WITH
SIZE
DIM
IN
OTHER
WORDS
WE
ARE
SELECTING
A
MODEL
HYPOTHESIS
FROM
A
FINITE
LIST
RATHER
THAN
OPTIMIZING
REAL
VALUED
PARAMETERS
THEN
WE
CAN
PROVE
THE
FOLLOWING
THEOREM
FOR
ANY
DATA
DISTRIBUTION
P
AND
ANY
DATASET
OF
SIZE
N
DRAWN
FROM
P
THE
PROBABILITY
THAT
OUR
ESTIMATE
OF
THE
ERROR
RATE
WILL
BE
MORE
THAN
E
WRONG
IN
THE
WORST
CASE
IS
UPPER
BOUNDED
AS
FOLLOWS
P
MAX
R
H
H
EMP
D
H
R
P
H
E
DIM
H
E
PROOF
TO
PROVE
THIS
WE
NEED
TWO
USEFUL
RESULTS
FIRST
HOEFFDING
INEQUALITY
WHICH
STATES
THAT
IF
XN
BER
Θ
THEN
FOR
ANY
E
P
X
Θ
E
WHERE
X
N
XI
SECOND
THE
UNION
BOUND
WHICH
SAYS
THAT
IF
AD
ARE
A
SET
OF
FINALLY
FOR
NOTATIONAL
BREVITY
LET
R
H
R
H
P
BE
THE
TRUE
RISK
AND
RˆN
H
REMP
H
BE
THE
EMPIRICAL
RISK
USING
THESE
RESULTS
WE
HAVE
P
MAX
RˆN
H
R
H
E
P
F
I
RˆN
H
R
H
E
H
H
H
H
P
RˆN
H
R
H
E
H
H
DIM
H
E
H
H
THS
BOUND
TELLS
US
THAT
THE
OPTIMISM
OF
THE
TRAINING
ERROR
INCREASES
WITH
DIM
BUT
DE
CREASES
WITH
N
AS
IS
TO
BE
EXPECTED
IF
THE
HYPOTHESIS
SPACE
IS
INFINITE
E
G
WE
HAVE
REAL
VALUED
PARAMETERS
WE
CANNOT
USE
DIM
INSTEAD
WE
CAN
USE
A
QUANTITY
CALLED
THE
VAPNIK
CHERVONENKIS
OR
VC
DIMEN
SION
OF
THE
HYPOTHESIS
CLASS
SEE
VAPNIK
FOR
DETAILS
STEPPING
BACK
FROM
ALL
THE
THEORY
THE
KEY
INTUITION
BEHIND
STATISTICAL
LEARNING
THEORY
IS
QUITE
SIMPLE
SUPPOSE
WE
FIND
A
MODEL
WITH
LOW
EMPIRICAL
RISK
IF
THE
HYPOTHESIS
SPACE
IS
VERY
BIG
RELATIVE
TO
THE
DATA
SIZE
THEN
IT
IS
QUITE
LIKELY
THAT
WE
JUST
GOT
LUCKY
AND
WERE
GIVEN
A
DATA
SET
THAT
IS
WELL
MODELED
BY
OUR
CHOSEN
FUNCTION
BY
CHANCE
HOWEVER
THIS
DOES
NOT
MEAN
THAT
SUCH
A
FUNCTION
WILL
HAVE
LOW
GENERALIZATION
ERROR
BUT
IF
THE
HYPOTHESIS
CLASS
IS
SUFFICIENTLY
CONSTRAINED
IN
SIZE
AND
OR
THE
TRAINING
SET
IS
SUFFICIENTLY
LARGE
THEN
WE
ARE
UNLIKELY
TO
GET
LUCKY
IN
THIS
WAY
SO
A
LOW
EMPIRICAL
RISK
IS
EVIDENCE
OF
A
LOW
TRUE
RISK
NOTE
THAT
OPTIMISM
OF
THE
TRAINING
ERROR
DOES
NOT
NECESSARILY
INCREASE
WITH
MODEL
COMPLEXITY
BUT
IT
DOES
INCREASE
WITH
THE
NUMBER
OF
DIFFERENT
MODELS
THAT
ARE
BEING
SEARCHED
OVER
THE
ADVANTAGE
OF
STATISTICAL
LEARNING
THEORY
COMPARED
TO
CV
IS
THAT
THE
BOUNDS
ON
THE
RISK
ARE
QUICKER
TO
COMPUTE
THAN
USING
CV
THE
DISADVANTAGE
IS
THAT
IT
IS
HARD
TO
COMPUTE
THE
VC
DIMENSION
FOR
MANY
INTERESTING
MODELS
AND
THE
UPPER
BOUNDS
ARE
USUALLY
VERY
LOOSE
ALTHOUGH
SEE
KAARIAINEN
AND
LANGFORD
ONE
CAN
EXTEND
STATISTICAL
LEARNING
THEORY
BY
TAKING
COMPUTATIONAL
COMPLEXITY
OF
THE
LEARNER
INTO
ACCOUNT
THIS
FIELD
IS
CALLED
COMPUTATIONAL
LEARNING
THEORY
OR
COLT
MOST
OF
THIS
WORK
FOCUSES
ON
THE
CASE
WHERE
H
IS
A
BINARY
CLASSIFIER
AND
THE
LOSS
FUNCTION
IS
LOSS
IF
WE
OBSERVE
A
LOW
EMPIRICAL
RISK
AND
THE
HYPOTHESIS
SPACE
IS
SUITABLY
SMALL
THEN
WE
CAN
SAY
THAT
OUR
ESTIMATED
FUNCTION
IS
PROBABLY
APPROXIMATELY
CORRECT
OR
PAC
A
HYPOTHESIS
SPACE
IS
SAID
TO
BE
EFFICIENTLY
PAC
LEARNABLE
IF
THERE
IS
A
POLYNOMIAL
TIME
ALGORITHM
THAT
CAN
IDENTIFY
A
FUNCTION
THAT
IS
PAC
SEE
KEARNS
AND
VAZIRANI
FOR
DETAILS
SURROGATE
LOSS
FUNCTIONS
MINIMIZING
THE
LOSS
IN
THE
ERM
RRM
FRAMEWORK
IS
NOT
ALWAYS
EASY
FOR
EXAMPLE
WE
MIGHT
WANT
TO
OPTIMIZE
THE
AUC
OR
SCORES
OR
MORE
SIMPLY
WE
MIGHT
JUST
WANT
TO
MINIMIZE
THE
LOSS
AS
IS
COMMON
IN
CLASSIFICATION
UNFORTUNATELY
THE
RISK
IS
A
VERY
NON
SMOOTH
OBJECTIVE
AND
HENCE
IS
HARD
TO
OPTIMIZE
ONE
ALTERNATIVE
IS
TO
USE
MAXIMUM
LIKELIHOOD
ESTIMATION
INSTEAD
SINCE
LOG
LIKELIHOOD
IS
A
SMOOTH
CONVEX
UPPER
BOUND
ON
THE
RISK
AS
WE
SHOW
BELOW
TO
SEE
THIS
CONSIDER
BINARY
LOGISTIC
REGRESSION
AND
LET
YI
SUPPOSE
OUR
DECISION
FUNCTION
COMPUTES
THE
LOG
ODDS
RATIO
F
X
LOG
P
Y
XI
W
WT
X
Η
P
Y
XI
W
THEN
THE
CORRESPONDING
PROBABILITY
DISTRIBUTION
ON
THE
OUTPUT
LABEL
IS
P
YI
XI
W
SIGM
YIΗI
LET
US
DEFINE
THE
LOG
LOSS
AS
AS
LNLL
Y
Η
LOG
P
Y
X
W
LOG
E
YΗ
FIGURE
ILLUSTRATION
OF
VARIOUS
LOSS
FUNCTIONS
FOR
BINARY
CLASSIFICATION
THE
HORIZONTAL
AXIS
IS
THE
MARGIN
YΗ
THE
VERTICAL
AXIS
IS
THE
LOSS
THE
LOG
LOSS
USES
LOG
BASE
FIGURE
GENERATED
BY
HINGELOSSPLOT
IT
IS
CLEAR
THAT
MINIMIZING
THE
AVERAGE
LOG
LOSS
IS
EQUIVALENT
TO
MAXIMIZING
THE
LIKELIHOOD
NOW
CONSIDER
COMPUTING
THE
MOST
PROBABLE
LABEL
WHICH
IS
EQUIVALENT
TO
USING
Yˆ
IF
ΗI
AND
Yˆ
IF
ΗI
THE
LOSS
OF
OUR
FUNCTION
BECOMES
Y
Η
I
Y
Yˆ
I
YΗ
FIGURE
PLOTS
THESE
TWO
LOSS
FUNCTIONS
WE
SEE
THAT
THE
NLL
IS
INDEED
AN
UPPER
BOUND
ON
THE
LOSS
LOG
LOSS
IS
AN
EXAMPLE
OF
A
SURROGATE
LOSS
FUNCTION
ANOTHER
EXAMPLE
IS
THE
HINGE
LOSS
LHINGE
Y
Η
MAX
YΗ
SEE
FIGURE
FOR
A
PLOT
WE
SEE
THAT
THE
FUNCTION
LOOKS
LIKE
A
DOOR
HINGE
HENCE
ITS
NAME
THIS
LOSS
FUNCTION
FORMS
THE
BASIS
OF
A
POPULAR
CLASSIFICATION
METHOD
KNOWN
AS
SUPPORT
VECTOR
MACHINES
SVM
WHICH
WE
WILL
DISCUSS
IN
SECTION
THE
SURROGATE
IS
USUALLY
CHOSEN
TO
BE
A
CONVEX
UPPER
BOUND
SINCE
CONVEX
FUNCTIONS
ARE
EASY
TO
MINIMIZE
SEE
E
G
BARTLETT
ET
AL
FOR
MORE
INFORMATION
PATHOLOGIES
OF
FREQUENTIST
STATISTICS
I
BELIEVE
THAT
IT
WOULD
BE
VERY
DIFFICULT
TO
PERSUADE
AN
INTELLIGENT
PERSON
THAT
CURRENT
FREQUENTIST
STATISTICAL
PRACTICE
WAS
SENSIBLE
BUT
THAT
THERE
WOULD
BE
MUCH
LESS
DIFFICULTY
WITH
AN
APPROACH
VIA
LIKELIHOOD
AND
BAYES
THEOREM
GEORGE
BOX
FREQUENTIST
STATISTICS
EXHIBITS
VARIOUS
FORMS
OF
WEIRD
AND
UNDESIRABLE
BEHAVIORS
KNOWN
AS
PATHOLOGIES
WE
GIVE
A
FEW
EXAMPLES
BELOW
IN
ORDER
TO
CAUTION
THE
READER
THESE
AND
OTHER
EXAMPLES
ARE
EXPLAINED
IN
MORE
DETAIL
IN
LINDLEY
LINDLEY
AND
PHILLIPS
LINDLEY
BERGER
JAYNES
MINKA
COUNTER
INTUITIVE
BEHAVIOR
OF
CONFIDENCE
INTERVALS
A
CONFIDENCE
INTERVAL
IS
AN
INTERVAL
DERIVED
FROM
THE
SAMPLING
DISTRIBUTION
OF
AN
ESTIMATOR
WHEREAS
A
BAYESIAN
CREDIBLE
INTERVAL
IS
DERIVED
FROM
THE
POSTERIOR
OF
A
PARAMETER
AS
WE
DIS
CUSSED
IN
SECTION
MORE
PRECISELY
A
FREQUENTIST
CONFIDENCE
INTERVAL
FOR
SOME
PARAMETER
Θ
IS
DEFINED
BY
THE
FOLLOWING
RATHER
UN
NATURAL
EXPRESSION
CΑT
Θ
U
P
D
Θ
U
D
D
Θ
Α
THAT
IS
IF
WE
SAMPLE
HYPOTHETICAL
FUTURE
DATA
FROM
Θ
THEN
U
IS
A
CONFIDENCE
INTERVAL
IF
THE
PARAMETER
Θ
LIES
INSIDE
THIS
INTERVAL
Α
PERCENT
OF
THE
TIME
LET
US
STEP
BACK
FOR
A
MOMENT
AND
THINK
ABOUT
WHAT
IS
GOING
ON
IN
BAYESIAN
STATISTICS
WE
CONDITION
ON
WHAT
IS
KNOWN
NAMELY
THE
OBSERVED
DATA
AND
AVERAGE
OVER
WHAT
IS
NOT
KNOWN
NAMELY
THE
PARAMETER
Θ
IN
FREQUENTIST
STATISTICS
WE
DO
EXACTLY
THE
OPPOSITE
WE
CONDITION
ON
WHAT
IS
UNKNOWN
NAMELY
THE
TRUE
PARAMETER
VALUE
Θ
AND
AVERAGE
OVER
HYPOTHETICAL
FUTURE
DATA
SETS
THIS
COUNTER
INTUITIVE
DEFINITION
OF
CONFIDENCE
INTERVALS
CAN
LEAD
TO
BIZARRE
RESULTS
CONSIDER
THE
FOLLOWING
EXAMPLE
FROM
BERGER
SUPPOSE
WE
DRAW
TWO
INTEGERS
D
FROM
IF
X
Θ
OTHERWISE
IF
Θ
WE
WOULD
EXPECT
THE
FOLLOWING
OUTCOMES
EACH
WITH
PROBABILITY
LET
M
MIN
AND
DEFINE
THE
FOLLOWING
CONFIDENCE
INTERVAL
D
U
D
M
M
FOR
THE
ABOVE
SAMPLES
THIS
YIELDS
HENCE
EQUATION
IS
CLEARLY
A
CI
SINCE
IS
CONTAINED
IN
OF
THESE
INTERVALS
HOWEVER
IF
THEN
P
Θ
SO
WE
KNOW
THAT
Θ
MUST
BE
YET
WE
ONLY
HAVE
CONFIDENCE
IN
THIS
FACT
ANOTHER
LESS
CONTRIVED
EXAMPLE
IS
AS
FOLLOWS
SUPPOSE
WE
WANT
TO
ESTIMATE
THE
PARAMETER
Θ
OF
A
BERNOULLI
DISTRIBUTION
LET
X
N
XI
BE
THE
SAMPLE
MEAN
THE
MLE
IS
Θˆ
X
AN
N
I
CALLED
A
WALD
INTERVAL
AND
IS
BASED
ON
A
GAUSSIAN
APPROXIMATION
TO
THE
BINOMIAL
DISTRIBUTION
COMPARE
TO
EQUATION
NOW
CONSIDER
A
SINGLE
TRIAL
WHERE
N
AND
THE
MLE
IS
WHICH
OVERFITS
AS
WE
SAW
IN
SECTION
BUT
OUR
CONFIDENCE
INTERVAL
IS
ALSO
WHICH
SEEMS
EVEN
WORSE
IT
CAN
BE
ARGUED
THAT
THE
ABOVE
FLAW
IS
BECAUSE
WE
APPROXIMATED
THE
TRUE
SAMPLING
DISTRIBUTION
WITH
A
GAUSSIAN
OR
BECAUSE
THE
SAMPLE
SIZE
WAS
TO
SMALL
OR
THE
PARAMETER
TOO
EXTREME
HOWEVER
THE
WALD
INTERVAL
CAN
BEHAVE
BADLY
EVEN
FOR
LARGE
N
AND
NON
EXTREME
PARAMETERS
BROWN
ET
AL
P
VALUES
CONSIDERED
HARMFUL
SUPPOSE
WE
WANT
TO
DECIDE
WHETHER
TO
ACCEPT
OR
REJECT
SOME
BASELINE
MODEL
WHICH
WE
WILL
CALL
THE
NULL
HYPOTHESIS
WE
NEED
TO
DEFINE
SOME
DECISION
RULE
IN
FREQUENTIST
STATISTICS
IT
IS
STANDARD
TO
FIRST
COMPUTE
A
QUANTITY
CALLED
THE
P
VALUE
WHICH
IS
DEFINED
AS
THE
PROBABILITY
UNDER
THE
NULL
OF
OBSERVING
SOME
TEST
STATISTIC
F
SUCH
AS
THE
CHI
SQUARED
STATISTIC
THAT
IS
AS
LARGE
OR
LARGER
THAN
THAT
ACTUALLY
OBSERVED
PVALUE
D
P
F
D
F
D
D
THIS
QUANTITY
RELIES
ON
COMPUTING
A
TAIL
AREA
PROBABILITY
OF
THE
SAMPLING
DISTRIBUTION
WE
GIVE
AN
EXAMPLE
OF
HOW
TO
DO
THIS
BELOW
GIVEN
THE
P
VALUE
WE
DEFINE
OUR
DECISION
RULE
AS
FOLLOWS
WE
REJECT
THE
NULL
HYPOTHESIS
IFF
THE
P
VALUE
IS
LESS
THAN
SOME
THRESHOLD
SUCH
AS
Α
IF
WE
DO
REJECT
IT
WE
SAY
THE
DIFFERENCE
BETWEEN
THE
OBSERVED
TEST
STATISTIC
AND
THE
EXPECTED
TEST
STATISTIC
IS
STATISTICALLY
SIGNIFICANT
AT
LEVEL
Α
THIS
APPROACH
IS
KNOWN
AS
NULL
HYPOTHESIS
SIGNIFICANCE
TESTING
OR
NHST
THIS
PROCEDURE
GUARANTEES
THAT
OUR
EXPECTED
TYPE
I
FALSE
POSITIVE
ERROR
RATE
IS
AT
MOST
Α
THIS
IS
SOMETIMES
INTERPRETED
AS
SAYING
THAT
FREQUENTIST
HYPOTHESIS
TESTING
IS
VERY
CONSERVATIVE
SINCE
IT
IS
UNLIKELY
TO
ACCIDENTLY
REJECT
THE
NULL
HYPOTHESIS
BUT
IN
FACT
THE
OPPOSITE
IS
THE
CASE
BECAUSE
THIS
METHOD
ONLY
WORRIES
ABOUT
TRYING
TO
REJECT
THE
NULL
IT
CAN
NEVER
GATHER
EVIDENCE
IN
FAVOR
OF
THE
NULL
NO
MATTER
HOW
LARGE
THE
SAMPLE
SIZE
BECAUSE
OF
THIS
P
VALUES
TEND
TO
OVERSTATE
THE
EVIDENCE
AGAINST
THE
NULL
AND
ARE
THUS
VERY
TRIGGER
HAPPY
IN
GENERAL
THERE
CAN
BE
HUGE
DIFFERENCES
BETWEEN
P
VALUES
AND
THE
QUANTITY
THAT
WE
REALLY
CARE
ABOUT
WHICH
IS
THE
POSTERIOR
PROBABILITY
OF
THE
NULL
HYPOTHESIS
GIVEN
THE
DATA
P
IN
PARTICULAR
SELLKE
ET
AL
SHOW
THAT
EVEN
IF
THE
P
VALUE
IS
AS
SLOW
AS
THE
POSTERIOR
PROBABILITY
OF
IS
AT
LEAST
AND
OFTEN
MUCH
HIGHER
SO
FREQUENTISTS
OFTEN
CLAIM
TO
HAVE
SIGNIFICANT
EVIDENCE
OF
AN
EFFECT
THAT
CANNOT
BE
EXPLAINED
BY
THE
NULL
HYPOTHESIS
WHEREAS
BAYESIANS
ARE
USUALLY
MORE
CONSERVATIVE
IN
THEIR
CLAIMS
FOR
EXAMPLE
P
VALUES
HAVE
BEEN
USED
TO
PROVE
THAT
ESP
EXTRA
SENSORY
PERCEPTION
IS
REAL
WAGENMAKERS
ET
AL
EVEN
THOUGH
ESP
IS
CLEARLY
VERY
IMPROBABLE
FOR
THIS
REASON
P
VALUES
HAVE
BEEN
BANNED
FROM
CERTAIN
MEDICAL
JOURNALS
MATTHEWS
ANOTHER
PROBLEM
WITH
P
VALUES
IS
THAT
THEIR
COMPUTATION
DEPENDS
ON
DECISIONS
YOU
MAKE
ABOUT
WHEN
TO
STOP
COLLECTING
DATA
EVEN
IF
THESE
DECISIONS
DON
T
CHANGE
THE
DATA
YOU
ACTUALLY
OBSERVED
FOR
EXAMPLE
SUPPOSE
I
TOSS
A
COIN
N
TIMES
AND
OBSERVE
SUCCESSES
HEADS
AND
F
FAILURES
TAILS
SO
N
F
IN
THIS
CASE
N
IS
FIXED
AND
AND
HENCE
F
IS
RANDOM
THE
RELEVANT
SAMPLING
MODEL
IS
THE
BINOMIAL
BIN
N
Θ
N
ΘS
Θ
N
LET
THE
NULL
HYPOTHESIS
BE
THAT
THE
COIN
IS
FAIR
Θ
WHERE
Θ
IS
THE
PROBABILITY
OF
SUCCESS
HEADS
THE
ONE
SIDED
P
VALUE
USING
TEST
STATISTIC
T
IS
THE
REASON
WE
CANNOT
JUST
COMPUTE
THE
PROBABILITY
OF
THE
OBSERVED
VALUE
OF
THE
TEST
STATISTIC
IS
THAT
THIS
WILL
HAVE
PROBABILITY
ZERO
UNDER
A
PDF
THE
P
VALUE
IS
DEFINED
IN
TERMS
OF
THE
CDF
SO
IS
ALWAYS
A
NUMBER
BETWEEN
AND
THE
TWO
SIDED
P
VALUE
IS
BIN
BIN
IN
EITHER
CASE
THE
P
VALUE
IS
LARGER
THAN
THE
MAGICAL
THRESHOLD
SO
A
FREQUENTIST
WOULD
NOT
REJECT
THE
NULL
HYPOTHESIS
NOW
SUPPOSE
I
TOLD
YOU
THAT
I
ACTUALLY
KEPT
TOSSING
THE
COIN
UNTIL
I
OBSERVED
F
TAILS
IN
THIS
CASE
F
IS
FIXED
AND
N
AND
HENCE
N
F
IS
RANDOM
THE
PROBABILITY
MODEL
BECOMES
THE
NEGATIVE
BINOMIAL
DISTRIBUTION
GIVEN
BY
NEGBINOM
F
Θ
F
ΘS
Θ
F
WHERE
F
N
NOTE
THAT
THE
TERM
WHICH
DEPENDS
ON
Θ
IS
THE
SAME
IN
EQUATIONS
AND
SO
THE
POSTERIOR
OVER
Θ
WOULD
BE
THE
SAME
IN
BOTH
CASES
HOWEVER
THESE
TWO
INTERPRETATIONS
OF
THE
SAME
DATA
GIVE
DIFFERENT
P
VALUES
IN
PARTICULAR
UNDER
THE
NEGATIVE
BINOMIAL
MODEL
WE
GET
P
P
H
SO
THE
P
VALUE
IS
AND
SUDDENLY
THERE
SEEMS
TO
BE
SIGNIFICANT
EVIDENCE
OF
BIAS
IN
THE
COIN
OBVIOUSLY
THIS
IS
RIDICULOUS
THE
DATA
IS
THE
SAME
SO
OUR
INFERENCES
ABOUT
THE
COIN
SHOULD
BE
THE
SAME
AFTER
ALL
I
COULD
HAVE
CHOSEN
THE
EXPERIMENTAL
PROTOCOL
AT
RANDOM
IT
IS
THE
OUTCOME
OF
THE
EXPERIMENT
THAT
MATTERS
NOT
THE
DETAILS
OF
HOW
I
DECIDED
WHICH
ONE
TO
RUN
ALTHOUGH
THIS
MIGHT
SEEM
LIKE
JUST
A
MATHEMATICAL
CURIOSITY
THIS
ALSO
HAS
SIGNIFICANT
PRACTICAL
IMPLICATIONS
IN
PARTICULAR
THE
FACT
THAT
THE
STOPPING
RULE
AFFECTS
THE
COMPUTATION
OF
THE
P
VALUE
MEANS
THAT
FREQUENTISTS
OFTEN
DO
NOT
TERMINATE
EXPERIMENTS
EARLY
EVEN
WHEN
IT
IS
OBVIOUS
WHAT
THE
CONCLUSIONS
ARE
LEST
IT
ADVERSELY
AFFECT
THEIR
STATISTICAL
ANALYSIS
IF
THE
EXPERIMENTS
ARE
COSTLY
OR
HARMFUL
TO
PEOPLE
THIS
IS
OBVIOUSLY
A
BAD
IDEA
PERHAPS
IT
IS
NOT
SURPRISING
THEN
THAT
THE
US
FOOD
AND
DRUG
ADMINISTRATION
FDA
WHICH
REGULATES
CLINICAL
TRIALS
OF
NEW
DRUGS
HAS
RECENTLY
BECOME
SUPPORTIVE
OF
BAYESIAN
SINCE
BAYESIAN
METHODS
ARE
NOT
AFFECTED
BY
THE
STOPPING
RULE
THE
LIKELIHOOD
PRINCIPLE
THE
FUNDAMENTAL
REASON
FOR
MANY
OF
THESE
PATHOLOGIES
IS
THAT
FREQUENTIST
INFERENCE
VIOLATES
THE
LIKELIHOOD
PRINCIPLE
WHICH
SAYS
THAT
INFERENCE
SHOULD
BE
BASED
ON
THE
LIKELIHOOD
OF
THE
OBSERVED
DATA
NOT
BASED
ON
HYPOTHETICAL
FUTURE
DATA
THAT
YOU
HAVE
NOT
OBSERVED
BAYES
OBVIOUSLY
SATISFIES
THE
LIKELIHOOD
PRINCIPLE
AND
CONSEQUENTLY
DOES
NOT
SUFFER
FROM
THESE
PATHOLOGIES
A
COMPELLING
ARGUMENT
IN
FAVOR
OF
THE
LIKELIHOOD
PRINCIPLE
WAS
PRESENTED
IN
BIRNBAUM
WHO
SHOWED
THAT
IT
FOLLOWED
AUTOMATICALLY
FROM
TWO
SIMPLER
PRINCIPLES
THE
FIRST
OF
THESE
IS
THE
SUFFICIENCY
PRINCIPLE
WHICH
SAYS
THAT
A
SUFFICIENT
STATISTIC
CONTAINS
ALL
THE
RELEVANT
INFORMATION
SEE
HTTP
YAMLB
WORDPRESS
COM
THE
US
FDA
IS
BECOMING
PROGRESSIVELY
MORE
BAYES
IAN
ABOUT
AN
UNKNOWN
PARAMETER
ARGUABLY
THIS
IS
TRUE
BY
DEFINITION
THE
SECOND
PRINCIPLE
IS
KNOWN
AS
WEAK
CONDITIONALITY
WHICH
SAYS
THAT
INFERENCES
SHOULD
BE
BASED
ON
THE
EVENTS
THAT
HAPPENED
NOT
WHICH
MIGHT
HAVE
HAPPENED
TO
MOTIVATE
THIS
CONSIDER
AN
EXAMPLE
FROM
BERGER
SUPPOSE
WE
NEED
TO
ANALYSE
A
SUBSTANCE
AND
CAN
SEND
IT
EITHER
TO
A
LABORATORY
IN
NEW
YORK
OR
IN
CALIFORNIA
THE
TWO
LABS
SEEM
EQUALLY
GOOD
SO
A
FAIR
COIN
IS
USED
TO
DECIDE
BETWEEN
THEM
THE
COIN
COMES
UP
HEADS
SO
THE
CALIFORNIA
LAB
IS
CHOSEN
WHEN
THE
RESULTS
COME
BACK
SHOULD
IT
BE
TAKEN
INTO
ACCOUNT
THAT
THE
COIN
COULD
HAVE
COME
UP
TAILS
AND
THUS
THE
NEW
YORK
LAB
COULD
HAVE
BEEN
USED
MOST
PEOPLE
WOULD
ARGUE
THAT
THE
NEW
YORK
LAB
IS
IRRELEVANT
SINCE
THE
TAILS
EVENT
DIDN
T
HAPPEN
THIS
IS
AN
EXAMPLE
OF
WEAK
CONDITIONALITY
GIVEN
THIS
PRINCIPLE
ONE
CAN
SHOW
THAT
ALL
INFERENCES
SHOULD
ONLY
BE
BASED
ON
WHAT
WAS
OBSERVED
WHICH
IS
IN
CONTRAST
TO
STANDARD
FREQUENTIST
PROCEDURES
SEE
BERGER
AND
WOLPERT
FOR
FURTHER
DETAILS
ON
THE
LIKELIHOOD
PRINCIPLE
WHY
ISN
T
EVERYONE
A
BAYESIAN
GIVEN
THESE
FUNDAMENTAL
FLAWS
OF
FREQUENTIST
STATISTICS
AND
THE
FACT
THAT
BAYESIAN
METHODS
DO
NOT
HAVE
SUCH
FLAWS
AN
OBVIOUS
QUESTION
TO
ASK
IS
WHY
ISN
T
EVERYONE
A
BAYESIAN
THE
FREQUENTIST
STATISTICIAN
BRADLEY
EFRON
WROTE
A
PAPER
WITH
EXACTLY
THIS
TITLE
EFRON
HIS
SHORT
PAPER
IS
WELL
WORTH
READING
FOR
ANYONE
INTERESTED
IN
THIS
TOPIC
BELOW
WE
QUOTE
HIS
OPENING
SECTION
THE
TITLE
IS
A
REASONABLE
QUESTION
TO
ASK
ON
AT
LEAST
TWO
COUNTS
FIRST
OF
ALL
EVERONE
USED
TO
BE
A
BAYESIAN
LAPLACE
WHOLEHEATEDLY
ENDORSED
BAYES
FORMULATION
OF
THE
INFERENCE
PROBLEM
AND
MOST
CENTURY
SCIENTISTS
FOLLOWED
SUIT
THIS
INCLUDED
GAUSS
WHOSE
STATISTICAL
WORK
IS
USUALLY
PRESENTED
IN
FREQUENTIST
TERMS
A
SECOND
AND
MORE
IMPORTANT
POINT
IS
THE
COGENCY
OF
THE
BAYESIAN
ARGUMENT
MODERN
STATISTICIANS
FOLLOWING
THE
LEAD
OF
SAVAGE
AND
DE
FINETTI
HAVE
ADVANCED
POWERFUL
THEORET
ICAL
ARGUMENTS
FOR
PREFERRING
BAYESIAN
INFERENCE
A
BYPRODUCT
OF
THIS
WORK
IS
A
DISTURBING
CATALOGUE
OF
INCONSISTENCIES
IN
THE
FREQUENTIST
POINT
OF
VIEW
NEVERTHELESS
EVERYONE
IS
NOT
A
BAYESIAN
THE
CURRENT
ERA
IS
THE
FIRST
CENTURY
IN
WHICH
STATISTICS
HAS
BEEN
WIDELY
USED
FOR
SCIENTIFIC
REPORTING
AND
IN
FACT
CENTURY
STATISTICS
IS
MAINLY
NON
BAYESIAN
HOWEVER
LINDLEY
PREDICTS
A
CHANGE
FOR
THE
CENTURY
TIME
WILL
TELL
WHETHER
LINDLEY
WAS
RIGHT
EXERCISES
EXERCISE
PESSIMISM
OF
LOOCV
SOURCE
SUPPOSE
WE
HAVE
A
COMPLETELY
RANDOM
LABELED
DATASET
I
E
THE
FEATURES
X
TELL
US
NOTHING
ABOUT
THE
CLASS
LABELS
Y
WITH
EXAMPLES
OF
CLASS
AND
EXAMPLES
OF
CLASS
WHERE
WHAT
IS
THE
BEST
MISCLASSIFICATION
RATE
ANY
METHOD
CAN
ACHIEVE
WHAT
IS
THE
ESTIMATED
MISCLASSIFICATION
RATE
OF
THE
SAME
METHOD
USING
LOOCV
EXERCISE
JAMES
STEIN
ESTIMATOR
FOR
GAUSSIAN
MEANS
CONSIDER
THE
STAGE
MODEL
YI
ΘI
N
ΘI
AND
ΘI
Μ
N
Τ
SUPPOSE
IS
KNOWN
AND
WE
OBSERVE
THE
FOLLOWING
DATA
POINTS
I
A
FIND
THE
ML
II
ESTIMATES
OF
AND
Τ
B
FIND
THE
POSTERIOR
ESTIMATES
E
ΘI
YI
AND
VAR
ΘI
YI
FOR
I
THE
OTHER
TERMS
I
ARE
COMPUTED
SIMILARLY
C
GIVE
A
CREDIBLE
INTERVAL
FOR
P
ΘI
YI
FOR
I
DO
YOU
TRUST
THIS
INTERVAL
ASSUMING
THE
GAUSSIAN
ASSUMPTION
IS
REASONABLE
I
E
IS
IT
LIKELY
TO
BE
TOO
LARGE
OR
TOO
SMALL
OR
JUST
RIGHT
D
WHAT
DO
YOU
EXPECT
WOULD
HAPPEN
TO
YOUR
ESTIMATES
IF
WERE
MUCH
SMALLER
SAY
YOU
DO
NOT
NEED
TO
COMPUTE
THE
NUMERICAL
ANSWER
JUST
BRIEFLY
EXPLAIN
WHAT
WOULD
HAPPEN
QUALITATIVELY
AND
WHY
EXERCISE
IS
BIASED
SHOW
THAT
N
XN
Μˆ
IS
A
BIASED
ESTIMATOR
OF
I
E
SHOW
XN
N
Μ
Σ
XN
HINT
NOTE
THAT
XN
ARE
INDEPENDENT
AND
USE
THE
FACT
THAT
THE
EXPECTATION
OF
A
PRODUCT
OF
INDEPENDENT
RANDOM
VARIABLES
IS
THE
PRODUCT
OF
THE
EXPECTATIONS
EXERCISE
ESTIMATION
OF
WHEN
Μ
IS
KNOWN
SUPPOSE
WE
SAMPLE
XN
Μ
WHERE
Μ
IS
A
KNOWN
CONSTANT
DERIVE
AN
EXPRESSION
FOR
THE
MLE
FOR
IN
THIS
CASE
IS
IT
UNBIASED
LINEAR
REGRESSION
INTRODUCTION
LINEAR
REGRESSION
IS
THE
WORK
HORSE
OF
STATISTICS
AND
SUPERVISED
MACHINE
LEARNING
WHEN
AUGMENTED
WITH
KERNELS
OR
OTHER
FORMS
OF
BASIS
FUNCTION
EXPANSION
IT
CAN
MODEL
ALSO
NON
LINEAR
RELATIONSHIPS
AND
WHEN
THE
GAUSSIAN
OUTPUT
IS
REPLACED
WITH
A
BERNOULLI
OR
MULTINOULLI
DISTRIBUTION
IT
CAN
BE
USED
FOR
CLASSIFICATION
AS
WE
WILL
SEE
BELOW
SO
IT
PAYS
TO
STUDY
THIS
MODEL
IN
DETAIL
MODEL
SPECIFICATION
AS
WE
DISCUSSED
IN
SECTION
LINEAR
REGRESSION
IS
A
MODEL
OF
THE
FORM
P
Y
X
Θ
N
Y
WT
X
LINEAR
REGRESSION
CAN
BE
MADE
TO
MODEL
NON
LINEAR
RELATIONSHIPS
BY
REPLACING
X
WITH
SOME
NON
LINEAR
FUNCTION
OF
THE
INPUTS
Φ
X
THAT
IS
WE
USE
P
Y
X
Θ
N
Y
WT
Φ
X
THIS
IS
KNOWN
AS
BASIS
FUNCTION
EXPANSION
NOTE
THAT
THE
MODEL
IS
STILL
LINEAR
IN
THE
PARAMETERS
W
SO
IT
IS
STILL
CALLED
LINEAR
REGRESSION
THE
IMPORTANCE
OF
THIS
WILL
BECOME
CLEAR
BELOW
A
SIMPLE
EXAMPLE
ARE
POLYNOMIAL
BASIS
FUNCTIONS
WHERE
THE
MODEL
HAS
THE
FORM
Φ
X
X
XD
FIGURE
ILLUSTRATES
THE
EFFECT
OF
CHANGING
D
INCREASING
THE
DEGREE
D
ALLOWS
US
TO
CREATE
INCREASINGLY
COMPLEX
FUNCTIONS
WE
CAN
ALSO
APPLY
LINEAR
REGRESSION
TO
MORE
THAN
INPUT
FOR
EXAMPLE
CONSIDER
MODELING
TEMPERATURE
AS
A
FUNCTION
OF
LOCATION
FIGURE
A
PLOTS
E
Y
X
AND
FIGURE
B
PLOTS
E
Y
X
MAXIMUM
LIKELIHOOD
ESTIMATION
LEAST
SQUARES
A
COMMON
WAY
TO
ESITMATE
THE
PARAMETERS
OF
A
STATISTICAL
MODEL
IS
TO
COMPUTE
THE
MLE
WHICH
IS
DEFINED
AS
Θˆ
ARG
MAX
LOG
P
D
Θ
A
B
FIGURE
LINEAR
REGRESSION
APPLIED
TO
DATA
VERTICAL
AXIS
IS
TEMPERATURE
HORIZONTAL
AXES
ARE
LOCATION
WITHIN
A
ROOM
DATA
WAS
COLLECTED
BY
SOME
REMOTE
SENSING
MOTES
AT
INTEL
LAB
IN
BERKELEY
CA
DATA
COURTESY
OF
ROMAIN
THIBAUX
A
THE
FITTED
PLANE
HAS
THE
FORM
Fˆ
X
B
TEMPERATURE
DATA
IS
FITTED
WITH
A
QUADRATIC
OF
THE
FORM
Fˆ
X
PRODUCED
BY
SURFACEFITDEMO
IT
IS
COMMON
TO
ASSUME
THE
TRAINING
EXAMPLES
ARE
INDEPENDENT
AND
IDENTICALLY
DISTRIBUTED
COMMONLY
ABBREVIATED
TO
IID
THIS
MEANS
WE
CAN
WRITE
THE
LOG
LIKELIHOOD
AS
FOLLOWS
N
Θ
LOG
P
D
Θ
LOG
P
YI
XI
Θ
I
INSTEAD
OF
MAXIMIZING
THE
LOG
LIKELIHOOD
WE
CAN
EQUIVALENTLY
MINIMIZE
THE
NEGATIVE
LOG
LIKELI
HOOD
OR
NLL
N
NLL
Θ
LOG
P
YI
XI
Θ
I
THE
NLL
FORMULATION
IS
SOMETIMES
MORE
CONVENIENT
SINCE
MANY
OPTIMIZATION
SOFTWARE
PACKAGES
ARE
DESIGNED
TO
FIND
THE
MINIMA
OF
FUNCTIONS
RATHER
THAN
MAXIMA
NOW
LET
US
APPLY
THE
METHOD
OF
MLE
TO
THE
LINEAR
REGRESSION
SETTING
INSERTING
THE
DEFINITION
OF
THE
GAUSSIAN
INTO
THE
ABOVE
WE
FIND
THAT
THE
LOG
LIKELIHOOD
IS
GIVEN
BY
T
RSS
W
N
LOG
RSS
STANDS
FOR
RESIDUAL
SUM
OF
SQUARES
AND
IS
DEFINED
BY
RSS
W
N
YI
WT
XI
I
THE
RSS
IS
ALSO
CALLED
THE
SUM
OF
SQUARED
ERRORS
OR
SSE
AND
SSE
N
IS
CALLED
THE
MEAN
SQUARED
ERROR
OR
MSE
IT
CAN
ALSO
BE
WRITTEN
AS
THE
SQUARE
OF
THE
NORM
OF
THE
VECTOR
OF
SUM
OF
SQUARES
ERROR
CONTOURS
FOR
LINEAR
REGRESSION
A
B
FIGURE
A
IN
LINEAR
LEAST
SQUARES
WE
TRY
TO
MINIMIZE
THE
SUM
OF
SQUARED
DISTANCES
FROM
EACH
TRAINING
POINT
DENOTED
BY
A
RED
CIRCLE
TO
ITS
APPROXIMATION
DENOTED
BY
A
BLUE
CROSS
THAT
IS
WE
MINIMIZE
THE
SUM
OF
THE
LENGTHS
OF
THE
LITTLE
VERTICAL
BLUE
LINES
THE
RED
DIAGONAL
LINE
REPRESENTS
Yˆ
X
WHICH
IS
THE
LEAST
SQUARES
REGRESSION
LINE
NOTE
THAT
THESE
RESIDUAL
LINES
ARE
NOT
PERPENDICULAR
TO
THE
LEAST
SQUARES
LINE
IN
CONTRAST
TO
FIGURE
FIGURE
GENERATED
BY
RESIDUALSDEMO
B
CONTOURS
OF
THE
RSS
ERROR
SURFACE
FOR
THE
SAME
EXAMPLE
THE
RED
CROSS
REPRESENTS
THE
MLE
W
FIGURE
GENERATED
BY
CONTOURSSSEDEMO
RESIDUAL
ERRORS
RSS
W
E
WHERE
EI
YI
WT
XI
WE
SEE
THAT
THE
MLE
FOR
W
IS
THE
ONE
THAT
MINIMIZES
THE
RSS
SO
THIS
METHOD
IS
KNOWN
AS
LEAST
SQUARES
THIS
METHOD
IS
ILLUSTRATED
IN
FIGURE
A
THE
TRAINING
DATA
XI
YI
ARE
SHOWN
AS
RED
CIRCLES
THE
ESTIMATED
VALUES
XI
YˆI
ARE
SHOWN
AS
BLUE
CROSSES
AND
THE
RESIDUALS
EI
YI
YˆI
ARE
SHOWN
AS
VERTICAL
BLUE
LINES
THE
GOAL
IS
TO
FIND
THE
SETTING
OF
THE
PARAMETERS
THE
SLOPE
AND
INTERCEPT
SUCH
THAT
THE
RESULTING
RED
LINE
MINIMIZES
THE
SUM
OF
SQUARED
RESIDUALS
THE
LENGTHS
OF
THE
VERTICAL
BLUE
LINES
IN
FIGURE
B
WE
PLOT
THE
NLL
SURFACE
FOR
OUR
LINEAR
REGRESSION
EXAMPLE
WE
SEE
THAT
IT
IS
A
QUADRATIC
BOWL
WITH
A
UNIQUE
MINIMUM
WHICH
WE
NOW
DERIVE
IMPORTANTLY
THIS
IS
TRUE
EVEN
IF
WE
USE
BASIS
FUNCTION
EXPANSION
SUCH
AS
POLYNOMIALS
BECAUSE
THE
NLL
IS
STILL
LINEAR
IN
THE
PARAMETERS
W
EVEN
IF
IT
IS
NOT
LINEAR
IN
THE
INPUTS
X
DERIVATION
OF
THE
MLE
FIRST
WE
REWRITE
THE
OBJECTIVE
IN
A
FORM
THAT
IS
MORE
AMENABLE
TO
DIFFERENTIATION
NLL
W
Y
XW
T
Y
XW
WT
XT
X
W
W
X
Y
WHERE
N
N
I
XI
D
XT
X
XIXT
I
I
XI
DXI
IS
THE
SUM
OF
SQUARES
MATRIX
AND
N
XT
Y
XIYI
I
USING
RESULTS
FROM
EQUATION
WE
SEE
THAT
THE
GRADIENT
OF
THIS
IS
GIVEN
BY
N
G
W
XT
XW
XT
Y
XI
WT
XI
YI
I
EQUATING
TO
ZERO
WE
GET
XT
XW
XT
Y
THIS
IS
KNOWN
AS
THE
NORMAL
EQUATION
THE
CORRESPONDING
SOLUTION
Wˆ
TO
THIS
LINEAR
SYSTEM
OF
EQUATIONS
IS
CALLED
THE
ORDINARY
LEAST
SQUARES
OR
OLS
SOLUTION
WHICH
IS
GIVEN
BY
GEOMETRIC
INTERPRETATION
THIS
EQUATION
HAS
AN
ELEGANT
GEOMETRICAL
INTREPRETATION
AS
WE
NOW
EXPLAIN
WE
ASSUME
N
D
SO
WE
HAVE
MORE
EXAMPLES
THAN
FEATURES
THE
COLUMNS
OF
X
DEFINE
A
LINEAR
SUBSPACE
OF
DIMENSIONALITY
D
WHICH
IS
EMBEDDED
IN
N
DIMENSIONS
LET
THE
J
TH
COLUMN
BE
X
J
WHICH
IS
A
VECTOR
IN
RN
THIS
SHOULD
NOT
BE
CONFUSED
WITH
XI
RD
WHICH
REPRESENTS
THE
I
TH
DATA
CASE
SIMILARLY
Y
IS
A
VECTOR
IN
RN
FOR
EXAMPLE
SUPPOSE
WE
HAVE
N
EXAMPLES
IN
D
DIMENSIONS
X
Y
THESE
VECTORS
ARE
ILLUSTRATED
IN
FIGURE
WE
SEEK
A
VECTOR
Yˆ
RN
THAT
LIES
IN
THIS
LINEAR
SUBSPACE
AND
IS
AS
CLOSE
AS
POSSIBLE
TO
Y
I
E
WE
WANT
TO
FIND
ARGMIN
Yˆ
SPAN
X
X
D
Y
Yˆ
SINCE
Yˆ
SPAN
X
THERE
EXISTS
SOME
WEIGHT
VECTOR
W
SUCH
THAT
Yˆ
WDX
D
XW
Yˆ
Y
FIGURE
GRAPHICAL
INTERPRETATION
OF
LEAST
SQUARES
FOR
N
EXAMPLES
AND
D
FEATURES
X
AND
X
ARE
VECTORS
IN
TOGETHER
THEY
DEFINE
A
PLANE
Y
IS
ALSO
A
VECTOR
IN
BUT
DOES
NOT
LIE
ON
THIS
PLANE
THE
ORTHOGONAL
PROJECTION
OF
Y
ONTO
THIS
PLANE
IS
DENOTED
Yˆ
THE
RED
LINE
FROM
Y
TO
Yˆ
IS
THE
RESIDUAL
WHOSE
NORM
WE
WANT
TO
MINIMIZE
FOR
VISUAL
CLARITY
ALL
VECTORS
HAVE
BEEN
CONVERTED
TO
UNIT
NORM
FIGURE
GENERATED
BY
LEASTSQUARESPROJECTION
TO
MINIMIZE
THE
NORM
OF
THE
RESIDUAL
Y
Yˆ
WE
WANT
THE
RESIDUAL
VECTOR
TO
BE
ORTHOGONAL
TO
EVERY
COLUMN
OF
X
SO
X
T
Y
Yˆ
FOR
J
D
HENCE
X
T
Y
Yˆ
XT
Y
XW
W
XT
X
Y
HENCE
OUR
PROJECTED
VALUE
OF
Y
IS
GIVEN
BY
Yˆ
XWˆ
X
XT
X
Y
THIS
CORRESPONDS
TO
AN
ORTHOGONAL
PROJECTION
OF
Y
ONTO
THE
COLUMN
SPACE
OF
X
THE
PROJECTION
MATRIX
P
X
XT
X
IS
CALLED
THE
HAT
MATRIX
SINCE
IT
PUTS
THE
HAT
ON
Y
CONVEXITY
WHEN
DISCUSSING
LEAST
SQUARES
WE
NOTED
THAT
THE
NLL
HAD
A
BOWL
SHAPE
WITH
A
UNIQUE
MINIMUM
THE
TECHNICAL
TERM
FOR
FUNCTIONS
LIKE
THIS
IS
CONVEX
CONVEX
FUNCTIONS
PLAY
A
VERY
IMPORTANT
ROLE
IN
MACHINE
LEARNING
LET
US
DEFINE
THIS
CONCEPT
MORE
PRECISELY
WE
SAY
A
SET
IS
CONVEX
IF
FOR
ANY
Θ
ΘT
WE
HAVE
ΛΘ
Λ
ΘT
Λ
A
B
FIGURE
A
ILLUSTRATION
OF
A
CONVEX
SET
B
ILLUSTRATION
OF
A
NONCONVEX
SET
X
Y
A
A
B
B
FIGURE
A
ILLUSTRATION
OF
A
CONVEX
FUNCTION
WE
SEE
THAT
THE
CHORD
JOINING
X
F
X
TO
Y
F
Y
LIES
ABOVE
THE
FUNCTION
B
A
FUNCTION
THAT
IS
NEITHER
CONVEX
NOR
CONCAVE
A
IS
A
LOCAL
MINIMUM
B
IS
A
GLOBAL
MINIMUM
FIGURE
GENERATED
BY
CONVEXFNHAND
THAT
IS
IF
WE
DRAW
A
LINE
FROM
Θ
TO
ΘT
ALL
POINTS
ON
THE
LINE
LIE
INSIDE
THE
SET
SEE
FIGURE
A
FOR
AN
ILLUSTRATION
OF
A
CONVEX
SET
AND
FIGURE
B
FOR
AN
ILLUSTRATION
OF
A
NON
CONVEX
SET
A
FUNCTION
F
Θ
IS
CALLED
CONVEX
IF
ITS
EPIGRAPH
THE
SET
OF
POINTS
ABOVE
THE
FUNCTION
DEFINES
A
CONVEX
SET
EQUIVALENTLY
A
FUNCTION
F
Θ
IS
CALLED
CONVEX
IF
IT
IS
DEFINED
ON
A
CONVEX
SET
AND
IF
FOR
ANY
Θ
ΘT
AND
FOR
ANY
Λ
WE
HAVE
F
ΛΘ
Λ
ΘT
ΛF
Θ
Λ
F
ΘT
SEE
FIGURE
FOR
A
EXAMPLE
A
FUNCTION
IS
CALLED
STRICTLY
CONVEX
IF
THE
INEQUALITY
IS
STRICT
A
FUNCTION
F
Θ
IS
CONCAVE
IF
F
Θ
IS
CONVEX
EXAMPLES
OF
SCALAR
CONVEX
FUNCTIONS
IN
CLUDE
EΘ
AND
Θ
LOG
Θ
FOR
Θ
EXAMPLES
OF
SCALAR
CONCAVE
FUNCTIONS
INCLUDE
LOG
Θ
AND
Θ
INTUITIVELY
A
STRICTLY
CONVEX
FUNCTION
HAS
A
BOWL
SHAPE
AND
HENCE
HAS
A
UNIQUE
GLOBAL
MINIMUM
Θ
CORRESPONDING
TO
THE
BOTTOM
OF
THE
BOWL
HENCE
ITS
SECOND
DERIVATIVE
MUST
BE
POSITIVE
EVERYWHERE
D
F
Θ
A
TWICE
CONTINUOUSLY
DIFFERENTIABLE
MULTIVARIATE
FUNCTION
F
IS
DΘ
CONVEX
IFF
ITS
HESSIAN
IS
POSITIVE
DEFINITE
FOR
ALL
Θ
F
OFTEN
CORRESPONDS
TO
THE
NLL
IN
THE
MACHINE
LEARNING
CONTEXT
THE
FUNCTION
RECALL
THAT
THE
HESSIAN
IS
THE
MATRIX
OF
SECOND
PARTIAL
DERIVATIVES
DEFINED
BY
HJK
MATRIX
H
IS
POSITIVE
DEFINITE
IFF
VT
HV
FOR
ANY
NON
ZERO
VECTOR
V
Θ
ALSO
RECALL
THAT
A
ΘJ
ΘK
LINEAR
DATA
WITH
NOISE
AND
OUTLIERS
LEAST
SQUARES
LAPLACE
A
B
FIGURE
A
ILLUSTRATION
OF
ROBUST
LINEAR
REGRESSION
FIGURE
GENERATED
BY
LINREGROBUSTDEMOCOMBINED
B
ILLUSTRATION
OF
T
T
AND
HUBER
LOSS
FUNCTIONS
FIGURE
GENERATED
BY
HUBERLOSSDEMO
MODELS
WHERE
THE
NLL
IS
CONVEX
ARE
DESIRABLE
SINCE
THIS
MEANS
WE
CAN
ALWAYS
FIND
THE
GLOBALLY
OPTIMAL
MLE
WE
WILL
SEE
MANY
EXAMPLES
OF
THIS
LATER
IN
THE
BOOK
HOWEVER
MANY
MODELS
OF
INTEREST
WILL
NOT
HAVE
CONCAVE
LIKELIHOODS
IN
SUCH
CASES
WE
WILL
DISCUSS
WAYS
TO
DERIVE
LOCALLY
OPTIMAL
PARAMETER
ESTIMATES
ROBUST
LINEAR
REGRESSION
IT
IS
VERY
COMMON
TO
MODEL
THE
NOISE
IN
REGRESSION
MODELS
USING
A
GAUSSIAN
DISTRIBUTION
WITH
ZERO
MEAN
AND
CONSTANT
VARIANCE
EI
WHERE
EI
YI
WT
XI
IN
THIS
CASE
MAXIMIZING
LIKELIHOOD
IS
EQUIVALENT
TO
MINIMIZING
THE
SUM
OF
SQUARED
RESIDUALS
AS
WE
HAVE
SEEN
HOWEVER
IF
WE
HAVE
OUTLIERS
IN
OUR
DATA
THIS
CAN
RESULT
IN
A
POOR
FIT
AS
ILLUSTRATED
IN
FIGURE
A
THE
OUTLIERS
ARE
THE
POINTS
ON
THE
BOTTOM
OF
THE
FIGURE
THIS
IS
BECAUSE
SQUARED
ERROR
PENALIZES
DEVIATIONS
QUADRATICALLY
SO
POINTS
FAR
FROM
THE
LINE
HAVE
MORE
AFFECT
ON
THE
FIT
THAN
POINTS
NEAR
TO
THE
LINE
ONE
WAY
TO
ACHIEVE
ROBUSTNESS
TO
OUTLIERS
IS
TO
REPLACE
THE
GAUSSIAN
DISTRIBUTION
FOR
THE
RESPONSE
VARIABLE
WITH
A
DISTRIBUTION
THAT
HAS
HEAVY
TAILS
SUCH
A
DISTRIBUTION
WILL
ASSIGN
HIGHER
LIKELIHOOD
TO
OUTLIERS
WITHOUT
HAVING
TO
PERTURB
THE
STRAIGHT
LINE
TO
EXPLAIN
THEM
ONE
POSSIBILITY
IS
TO
USE
THE
LAPLACE
DISTRIBUTION
INTRODUCED
IN
SECTION
IF
WE
USE
THIS
AS
OUR
OBSERVATION
MODEL
FOR
REGRESSION
WE
GET
THE
FOLLOWING
LIKELIHOOD
P
Y
X
W
B
LAP
Y
WT
X
B
EXP
B
Y
W
X
THE
ROBUSTNESS
ARISES
FROM
THE
USE
OF
Y
WT
X
INSTEAD
OF
Y
WT
X
FOR
SIMPLICITY
WE
WILL
ASSUME
B
IS
FIXED
LET
RI
YI
WT
XI
BE
THE
I
TH
RESIDUAL
THE
NLL
HAS
THE
FORM
W
RI
W
I
LIKELIHOOD
PRIOR
NAME
SECTION
GAUSSIAN
UNIFORM
LEAST
SQUARES
GAUSSIAN
GAUSSIAN
RIDGE
GAUSSIAN
LAPLACE
LASSO
LAPLACE
UNIFORM
ROBUST
REGRESSION
STUDENT
UNIFORM
ROBUST
REGRESSION
EXERCISE
TABLE
SUMMARY
OF
VARIOUS
LIKELIHOODS
AND
PRIORS
USED
FOR
LINEAR
REGRESSION
THE
LIKELIHOOD
REFERS
TO
THE
DISTRIBUTIONAL
FORM
OF
P
Y
X
W
AND
THE
PRIOR
REFERS
TO
THE
DISTRIBUTIONAL
FORM
OF
P
W
MAP
ESTIMATION
WITH
A
UNIFORM
DISTRIBUTION
CORRESPONDS
TO
MLE
UNFORTUNATELY
THIS
IS
A
NON
LINEAR
OBJECTIVE
FUNCTION
WHICH
IS
HARD
TO
OPTIMIZE
FORTUNATELY
WE
CAN
CONVERT
THE
NLL
TO
A
LINEAR
OBJECTIVE
SUBJECT
TO
LINEAR
CONSTRAINTS
USING
THE
FOLLOWING
SPLIT
VARIABLE
TRICK
FIRST
WE
DEFINE
RI
R
R
AND
THEN
WE
IMPOSE
THE
LINEAR
INEQUALITY
CONSTRAINTS
THAT
R
AND
RI
NOW
THE
CONSTRAINED
OBJECTIVE
BECOMES
MIN
R
R
T
R
R
WT
XI
R
R
YI
THIS
IS
AN
EXAMPLE
OF
A
LINEAR
PROGRAM
WITH
D
UNKNOWNS
AND
CONSTRAINTS
SINCE
THIS
IS
A
CONVEX
OPTIMIZATION
PROBLEM
IT
HAS
A
UNIQUE
SOLUTION
TO
SOLVE
AN
LP
WE
MUST
FIRST
WRITE
IT
IN
STANDARD
FORM
WHICH
AS
FOLLOWS
MIN
F
T
Θ
T
AΘ
B
AEQΘ
BEQ
L
Θ
U
IN
OUR
CURRENT
EXAMPLE
Θ
W
R
R
F
A
B
AEQ
X
I
I
BEQ
Y
L
U
THIS
CAN
BE
SOLVED
BY
ANY
LP
SOLVER
SEE
E
G
BOYD
AND
VANDENBERGHE
SEE
FIGURE
A
FOR
AN
EXAMPLE
OF
THE
METHOD
IN
ACTION
AN
ALTERNATIVE
TO
USING
NLL
UNDER
A
LAPLACE
LIKELIHOOD
IS
TO
MINIMIZE
THE
HUBER
LOSS
FUNCTION
HUBER
DEFINED
AS
FOLLOWS
LH
R
Δ
Δ
R
IF
R
Δ
THIS
IS
EQUIVALENT
TO
FOR
ERRORS
THAT
ARE
SMALLER
THAN
Δ
AND
IS
EQUIVALENT
TO
FOR
LARGER
ERRORS
SEE
FIGURE
B
THE
ADVANTAGE
OF
THIS
LOSS
FUNCTION
IS
THAT
IT
IS
EVERYWHERE
DIFFERENTIABLE
USING
THE
FACT
THAT
D
R
SIGN
R
IF
R
WE
CAN
ALSO
CHECK
THAT
THE
FUNCTION
IS
CONTINUOUS
SINCE
THE
GRADIENTS
OF
THE
TWO
PARTS
OF
THE
FUNCTION
MATCH
AT
R
Δ
NAMELY
D
LH
R
Δ
R
Δ
Δ
CONSEQUENTLY
OPTIMIZING
THE
HUBER
LOSS
IS
MUCH
FASTER
THAN
USING
THE
LAPLACE
LIKELIHOOD
SINCE
WE
CAN
USE
STANDARD
SMOOTH
OPTIMIZATION
METHODS
SUCH
AS
QUASI
NEWTON
INSTEAD
OF
LINEAR
PROGRAMMING
FIGURE
A
GIVES
AN
ILLUSTRATION
OF
THE
HUBER
LOSS
FUNCTION
THE
RESULTS
ARE
QUALITATIVELY
SIMILIAR
TO
THE
PROBABILISTIC
METHODS
IN
FACT
IT
TURNS
OUT
THAT
THE
HUBER
METHOD
ALSO
HAS
A
PROBABILISTIC
INTERPRETATION
ALTHOUGH
IT
IS
RATHER
UNNATURAL
PONTIL
ET
AL
LN
LAMBDA
LN
LAMBDA
A
B
FIGURE
DEGREE
POLYNOMIAL
FIT
TO
N
DATA
POINTS
WITH
INCREASING
AMOUNTS
OF
T
REGULARIZATION
DATA
WAS
GENERATED
FROM
NOISE
WITH
VARIANCE
THE
ERROR
BARS
REPRESENTING
THE
NOISE
VARIANCE
GET
WIDER
AS
THE
FIT
GETS
SMOOTHER
SINCE
WE
ARE
ASCRIBING
MORE
OF
THE
DATA
VARIATION
TO
THE
NOISE
FIGURE
GENERATED
BY
LINREGPOLYVSREGDEMO
RIDGE
REGRESSION
ONE
PROBLEM
WITH
ML
ESTIMATION
IS
THAT
IT
CAN
RESULT
IN
OVERFITTING
IN
THIS
SECTION
WE
DISCUSS
A
WAY
TO
AMELIORATE
THIS
PROBLEM
BY
USING
MAP
ESTIMATION
WITH
A
GAUSSIAN
PRIOR
FOR
SIMPLICITY
WE
ASSUME
A
GAUSSIAN
LIKELIHOOD
RATHER
THAN
A
ROBUST
LIKELIHOOD
BASIC
IDEA
THE
REASON
THAT
THE
MLE
CAN
OVERFIT
IS
THAT
IT
IS
PICKING
THE
PARAMETER
VALUES
THAT
ARE
THE
BEST
FOR
MODELING
THE
TRAINING
DATA
BUT
IF
THE
DATA
IS
NOISY
SUCH
PARAMETERS
OFTEN
RESULT
IN
COMPLEX
FUNCTIONS
AS
A
SIMPLE
EXAMPLE
SUPPOSE
WE
FIT
A
DEGREE
POLYNOMIAL
TO
N
DATA
POINTS
USING
LEAST
SQUARES
THE
RESULTING
CURVE
IS
VERY
WIGGLY
AS
SHOWN
IN
FIGURE
A
THE
CORRESPONDING
LEAST
SQUARES
COEFFICIENTS
EXCLUDING
ARE
AS
FOLLOWS
058
WE
SEE
THAT
THERE
ARE
MANY
LARGE
POSITIVE
AND
NEGATIVE
NUMBERS
THESE
BALANCE
OUT
EXACTLY
TO
MAKE
THE
CURVE
WIGGLE
IN
JUST
THE
RIGHT
WAY
SO
THAT
IT
ALMOST
PERFECTLY
INTERPOLATES
THE
DATA
BUT
THIS
SITUATION
IS
UNSTABLE
IF
WE
CHANGED
THE
DATA
A
LITTLE
THE
COEFFICIENTS
WOULD
CHANGE
A
LOT
WE
CAN
ENCOURAGE
THE
PARAMETERS
TO
BE
SMALL
THUS
RESULTING
IN
A
SMOOTHER
CURVE
BY
USING
A
ZERO
MEAN
GAUSSIAN
PRIOR
P
W
TT
N
WJ
Τ
WHERE
Τ
CONTROLS
THE
STRENGTH
OF
THE
PRIOR
THE
CORRESPONDING
MAP
ESTIMATION
PROBLEM
BECOMES
N
D
ARGMAX
LOG
N
YI
WT
XI
LOG
N
WJ
Τ
MEAN
SQUARED
ERROR
LOG
LAMBDA
A
LOG
LAMBDA
B
FIGURE
A
TRAINING
ERROR
DOTTED
BLUE
AND
TEST
ERROR
SOLID
RED
FOR
A
DEGREE
POLYNOMIAL
FIT
BY
RIDGE
REGRESSION
PLOTTED
VS
LOG
Λ
DATA
WAS
GENERATED
FROM
NOISE
WITH
VARIANCE
TRAINING
SET
HAS
SIZE
N
NOTE
MODELS
ARE
ORDERED
FROM
COMPLEX
SMALL
REGULARIZER
ON
THE
LEFT
TO
SIMPLE
LARGE
REGULARIZER
ON
THE
RIGHT
THE
STARS
CORRESPOND
TO
THE
VALUES
USED
TO
PLOT
THE
FUNCTIONS
IN
FIGURE
B
ESTIMATE
OF
PERFORMANCE
USING
TRAINING
SET
DOTTED
BLUE
FOLD
CROSS
VALIDATION
ESTIMATE
OF
FUTURE
MSE
SOLID
BLACK
NEGATIVE
LOG
MARGINAL
LIKELIHOOD
LOG
P
D
Λ
BOTH
CURVES
HAVE
BEEN
VERTICALLY
RESCALED
TO
TO
MAKE
THEM
COMPARABLE
FIGURE
GENERATED
BY
LINREGPOLYVSREGDEMO
IT
IS
A
SIMPLE
EXERCISE
TO
SHOW
THAT
THIS
IS
EQUIVALENT
TO
MINIMIZING
THE
FOLLOWING
J
W
Y
W
WT
X
Λ
W
WHERE
Λ
Τ
AND
W
J
WT
W
IS
THE
SQUARED
TWO
NORM
HERE
THE
FIRST
TERM
IS
THE
MSE
NLL
AS
USUAL
AND
THE
SECOND
TERM
Λ
IS
A
COMPLEXITY
PENALTY
THE
CORRESPONDING
SOLUTION
IS
GIVEN
BY
THIS
TECHNIQUE
IS
KNOWN
AS
RIDGE
REGRESSION
OR
PENALIZED
LEAST
SQUARES
IN
GENERAL
ADDING
A
GAUSSIAN
PRIOR
TO
THE
PARAMETERS
OF
A
MODEL
TO
ENCOURAGE
THEM
TO
BE
SMALL
IS
CALLED
REGULARIZATION
OR
WEIGHT
DECAY
NOTE
THAT
THE
OFFSET
TERM
IS
NOT
REGULARIZED
SINCE
THIS
JUST
AFFECTS
THE
HEIGHT
OF
THE
FUNCTION
NOT
ITS
COMPLEXITY
BY
PENALIZING
THE
SUM
OF
THE
MAGNITUDES
OF
THE
WEIGHTS
WE
ENSURE
THE
FUNCTION
IS
SIMPLE
SINCE
W
CORRESPONDS
TO
A
STRAIGHT
LINE
WHICH
IS
THE
SIMPLEST
POSSIBLE
FUNCTION
CORRESPONDING
TO
A
CONSTANT
WE
ILLUSTRATE
THIS
IDEA
IN
FIGURE
WHERE
WE
SEE
THAT
INCREASING
Λ
RESULTS
IN
SMOOTHER
FUNCTIONS
THE
RESULTING
COEFFICIENTS
ALSO
BECOME
SMALLER
FOR
EXAMPLE
USING
Λ
WE
HAVE
704
360
IN
FIGURE
A
WE
PLOT
THE
MSE
ON
THE
TRAINING
AND
TEST
SETS
VS
LOG
Λ
WE
SEE
THAT
AS
WE
INCREASE
Λ
SO
THE
MODEL
BECOMES
MORE
CONSTRAINED
THE
ERROR
ON
THE
TRAINING
SET
INCREASES
FOR
THE
TEST
SET
WE
SEE
THE
CHARACTERISTIC
U
SHAPED
CURVE
WHERE
THE
MODEL
OVERFITS
AND
THEN
UNDERFITS
IT
IS
COMMON
TO
USE
CROSS
VALIDATION
TO
PICK
Λ
AS
SHOWN
IN
FIGURE
B
IN
SECTION
WE
WILL
DISCUSS
A
MORE
PROBABILISTIC
APPROACH
WE
WILL
CONSIDER
A
VARIETY
OF
DIFFERENT
PRIORS
IN
THIS
BOOK
EACH
OF
THESE
CORRESPONDS
TO
A
DIFFERENT
FORM
OF
REGULARIZATION
THIS
TECHNIQUE
IS
VERY
WIDELY
USED
TO
PREVENT
OVERFITTING
NUMERICALLY
STABLE
COMPUTATION
INTERESTINGLY
RIDGE
REGRESSION
WHICH
WORKS
BETTER
STATISTICALLY
IS
ALSO
EASIER
TO
FIT
NUMERICALLY
SINCE
ΛID
XT
X
IS
MUCH
BETTER
CONDITIONED
AND
HENCE
MORE
LIKELY
TO
BE
INVERTIBLE
THAN
XT
X
AT
LEAST
FOR
SUITABLE
LARGY
Λ
NEVERTHELESS
INVERTING
MATRICES
IS
STILL
BEST
AVOIDED
FOR
REASONS
OF
NUMERICAL
STABILITY
INDEED
IF
YOU
WRITE
W
INV
X
X
X
Y
IN
MATLAB
IT
WILL
GIVE
YOU
A
WARNING
WE
NOW
DESCRIBE
A
USEFUL
TRICK
FOR
FITTING
RIDGE
REGRESSION
MODELS
AND
HENCE
BY
EXTENSION
COMPUTING
VANILLA
OLS
ESTIMATES
THAT
IS
MORE
NUMERICALLY
ROBUST
WE
ASSUME
THE
PRIOR
HAS
THE
FORM
P
W
Λ
WHERE
Λ
IS
THE
PRECISION
MATRIX
IN
THE
CASE
OF
RIDGE
REGRESSION
Λ
Τ
I
TO
AVOID
PENALIZING
THE
TERM
WE
SHOULD
CENTER
THE
DATA
FIRST
AS
EXPLAINED
IN
EXERCISE
FIRST
LET
US
AUGMENT
THE
ORIGINAL
DATA
WITH
SOME
VIRTUAL
DATA
COMING
FROM
THE
PRIOR
X
X
Σ
Y
Y
Σ
WHERE
Λ
Λ
ΛT
IS
A
CHOLESKY
DECOMPOSITION
OF
Λ
WE
SEE
THAT
X
WHERE
THE
EXTRA
ROWS
REPRESENT
PSEUDO
DATA
FROM
THE
PRIOR
IS
N
D
D
WE
NOW
SHOW
THAT
THE
NLL
ON
THIS
EXPANDED
DATA
IS
EQUIVALENT
TO
PENALIZED
NLL
ON
THE
ORIGINAL
DATA
F
W
Y
X
W
T
Y
X
W
Y
Σ
X
Σ
T
Y
Σ
X
Σ
W
Σ
Σ
ΛW
ΛW
Y
XW
T
Y
XW
ΛW
T
ΛW
Y
XW
T
Y
XW
WT
ΛW
HENCE
THE
MAP
ESTIMATE
IS
GIVEN
BY
Wˆ
RIDGE
X
T
X
T
Y
AS
WE
CLAIMED
NOW
LET
X
QR
BE
THE
QR
DECOMPOSITION
OF
X
WHERE
Q
IS
ORTHONORMAL
MEANING
QT
Q
QQT
I
AND
R
IS
UPPER
TRIANGULAR
THEN
X
T
X
RT
QT
QR
RT
R
R
T
HENCE
Wˆ
RIDGE
R
T
RT
QT
Y
R
NOTE
THAT
R
IS
EASY
TO
INVERT
SINCE
IT
IS
UPPER
TRIANGULAR
THIS
GIVES
US
A
WAY
TO
COMPUTE
THE
RIDGE
ESTIMATE
WHILE
AVOIDING
HAVING
TO
INVERT
Λ
XT
X
WE
CAN
USE
THIS
TECHNIQUE
TO
FIND
THE
MLE
BY
SIMPLY
COMPUTING
THE
QR
DECOMPOSITION
OF
THE
UNAUGMENTED
MATRIX
X
AND
USING
THE
ORIGINAL
Y
THIS
IS
THE
METHOD
OF
CHOICE
FOR
SOLVING
LEAST
SQUARES
PROBLEMS
IN
FACT
IT
IS
SO
SOMMON
THAT
IT
CAN
BE
IMPLEMENTED
IN
ONE
LINE
OF
MATLAB
USING
THE
BACKSLASH
OPERATOR
W
X
Y
NOTE
THAT
COMPUTING
THE
QR
DECOMPOSITION
OF
AN
N
D
MATRIX
TAKES
O
TIME
AND
IS
NUMERICALLY
VERY
STABLE
IF
D
N
WE
SHOULD
FIRST
PERFORM
AN
SVD
DECOMPOSITION
IN
PARTICULAR
LET
X
USVT
BE
THE
SVD
OF
X
WHERE
VT
V
IN
UUT
UT
U
IN
AND
IS
A
DIAGONAL
N
N
MATRIX
NOW
LET
Z
UD
BE
AN
N
N
MATRIX
THEN
WE
CAN
REWRITE
THE
RIDGE
ESTIMATE
THUS
Wˆ
RIDGE
V
ZT
Z
ΛIN
Y
IN
OTHER
WORDS
WE
CAN
REPLACE
THE
D
DIMENSIONAL
VECTORS
XI
WITH
THE
N
DIMENSIONAL
VECTORS
ZI
AND
PERFORM
OUR
PENALIZED
FIT
AS
BEFORE
WE
THEN
TRANSFORM
THE
N
DIMENSIONAL
SOLUTION
TO
THE
D
DIMENSIONAL
SOLUTION
BY
MULTIPLYING
BY
V
GEOMETRICALLY
WE
ARE
ROTATING
TO
A
NEW
COORDINATE
SYSTEM
IN
WHICH
ALL
BUT
THE
FIRST
N
COORDINATES
ARE
ZERO
THIS
DOES
NOT
AFFECT
THE
SOLUTION
SINCE
THE
SPHERICAL
GAUSSIAN
PRIOR
IS
ROTATIONALLY
INVARIANT
THE
OVERALL
TIME
IS
NOW
O
DN
OPERATIONS
CONNECTION
WITH
PCA
IN
THIS
SECTION
WE
DISCUSS
AN
INTERESTING
CONNECTION
BETWEEN
RIDGE
REGRESSION
AND
PCA
SEC
TION
WHICH
GIVES
FURTHER
INSIGHT
INTO
WHY
RIDGE
REGRESSION
WORKS
WELL
OUR
DISCUSSION
IS
BASED
ON
HASTIE
ET
AL
LET
X
USVT
BE
THE
SVD
OF
X
FROM
EQUATION
WE
HAVE
Wˆ
RIDGE
V
ΛI
Y
HENCE
THE
RIDGE
PREDICTIONS
ON
THE
TRAINING
SET
ARE
GIVEN
BY
Yˆ
XWˆ
RIDGE
USVT
V
ΛI
Y
D
T
T
J
J
FIGURE
GEOMETRY
OF
RIDGE
REGRESSION
THE
LIKELIHOOD
IS
SHOWN
AS
AN
ELLIPSE
AND
THE
PRIOR
IS
SHOWN
AS
A
CIRCLE
CENTERED
ON
THE
ORIGIN
BASED
ON
FIGURE
OF
BISHOP
FIGURE
GENERATED
BY
GEOMRIDGE
WHERE
JJ
ΛI
JJ
J
J
AND
ΣJ
ARE
THE
SINGULAR
VALUES
OF
X
HENCE
Yˆ
XWˆ
RIDGE
D
U
J
J
J
Λ
UT
Y
IN
CONTRAST
THE
LEAST
SQUARES
PREDICTION
IS
Yˆ
XWˆ
LS
USVT
VS
Y
UUT
Y
UJUT
Y
J
IF
IS
SMALL
COMPARED
TO
Λ
THEN
DIRECTION
UJ
WILL
NOT
HAVE
MUCH
EFFECT
ON
THE
PREDICTION
IN
VIEW
OF
THIS
WE
DEFINE
THE
EFFECTIVE
NUMBER
OF
DEGREES
OF
FREEDOM
OF
THE
MODEL
AS
FOLLOWS
D
DOF
Λ
J
J
Λ
WHEN
Λ
DOF
Λ
D
AND
AS
Λ
DOF
Λ
LET
US
TRY
TO
UNDERSTAND
WHY
THIS
BEHAVIOR
IS
DESIRABLE
IN
SECTION
WE
SHOW
THAT
COV
W
XT
X
IF
WE
USE
A
UNIFORM
PRIOR
FOR
W
THUS
THE
DIRECTIONS
IN
WHICH
WE
ARE
MOST
UNCERTAIN
ABOUT
W
ARE
DETERMINED
BY
THE
EIGENVECTORS
OF
THIS
MATRIX
WITH
THE
SMALLEST
EIGENVALUES
AS
SHOWN
IN
FIGURE
FURTHERMORE
IN
SECTION
WE
SHOW
THAT
THE
SQUARED
SINGULAR
VALUES
ARE
EQUAL
TO
THE
EIGENVALUES
OF
XT
X
HENCE
SMALL
SINGULAR
VALUES
ΣJ
CORRESPOND
TO
DIRECTIONS
WITH
HIGH
POSTERIOR
VARIANCE
IT
IS
THESE
DIRECTIONS
WHICH
RIDGE
SHRINKS
THE
MOST
THIS
PROCESS
IS
ILLUSTRATED
IN
FIGURE
THE
HORIZONTAL
PARAMETER
IS
NOT
WELL
DETERMINED
BY
THE
DATA
HAS
HIGH
POSTERIOR
VARIANCE
BUT
THE
VERTICAL
PARAMETER
IS
WELL
DETERMINED
HENCE
WMAP
IS
CLOSE
TO
WˆMLE
BUT
WMAP
IS
SHIFTED
STRONGLY
TOWARDS
THE
PRIOR
MEAN
WHICH
IS
COMPARE
TO
FIGURE
C
WHICH
ILLUSTRATED
SENSOR
FUSION
WITH
SENSORS
OF
DIFFERENT
RELIABILITIES
IN
THIS
WAY
ILL
DETERMINED
PARAMETERS
ARE
REDUCED
IN
SIZE
TOWARDS
THIS
IS
CALLED
SHRINKAGE
THERE
IS
A
RELATED
BUT
DIFFERENT
TECHNIQUE
CALLED
PRINCIPAL
COMPONENTS
REGRESSION
THE
IDEA
IS
THIS
FIRST
USE
PCA
TO
REDUCE
THE
DIMENSIONALITY
TO
K
DIMENSIONS
AND
THEN
USE
THESE
LOW
DIMENSIONAL
FEATURES
AS
INPUT
TO
REGRESSION
HOWEVER
THIS
TECHNIQUE
DOES
NOT
WORK
AS
WELL
AS
RIDGE
IN
TERMS
OF
PREDICTIVE
ACCURACY
HASTIE
ET
AL
THE
REASON
IS
THAT
IN
PC
REGRESSION
ONLY
THE
FIRST
K
DERIVED
DIMENSIONS
ARE
RETAINED
AND
THE
REMAINING
D
K
DIMENSIONS
ARE
ENTIRELY
IGNORED
BY
CONTRAST
RIDGE
REGRESSION
USES
A
SOFT
WEIGHTING
OF
ALL
THE
DIMENSIONS
REGULARIZATION
EFFECTS
OF
BIG
DATA
REGULARIZATION
IS
THE
MOST
COMMON
WAY
TO
AVOID
OVERFITTING
HOWEVER
ANOTHER
EFFECTIVE
APPROACH
WHICH
IS
NOT
ALWAYS
AVAILABLE
IS
TO
USE
LOTS
OF
DATA
IT
SHOULD
BE
INTUITIVELY
OBVIOUS
THAT
THE
MORE
TRAINING
DATA
WE
HAVE
THE
BETTER
WE
WILL
BE
ABLE
TO
LEARN
SO
WE
EXPECT
THE
TEST
SET
ERROR
TO
DECREASE
TO
SOME
PLATEAU
AS
N
INCREASES
THIS
IS
ILLUSTRATED
IN
FIGURE
WHERE
WE
PLOT
THE
MEAN
SQUARED
ERROR
INCURRED
ON
THE
TEST
SET
ACHIEVED
BY
POLYNOMIAL
REGRESSION
MODELS
OF
DIFFERENT
DEGREES
VS
N
A
PLOT
OF
ERROR
VS
TRAINING
SET
SIZE
IS
KNOWN
AS
A
LEARNING
CURVE
THE
LEVEL
OF
THE
PLATEAU
FOR
THE
TEST
ERROR
CONSISTS
OF
TWO
TERMS
AN
IRREDUCIBLE
COMPONENT
THAT
ALL
MODELS
INCUR
DUE
TO
THE
INTRINSIC
VARIABILITY
OF
THE
GENERATING
PROCESS
THIS
IS
CALLED
THE
NOISE
FLOOR
AND
A
COMPONENT
THAT
DEPENDS
ON
THE
DISCREPANCY
BETWEEN
THE
GENERATING
PROCESS
THE
TRUTH
AND
THE
MODEL
THIS
IS
CALLED
STRUCTURAL
ERROR
IN
FIGURE
THE
TRUTH
IS
A
DEGREE
POLYNOMIAL
AND
WE
TRY
FITTING
POLYNOMIALS
OF
DEGREES
AND
TO
THIS
DATA
CALL
THE
MODELS
AND
WE
SEE
THAT
THE
STRUCTURAL
ERROR
FOR
MODELS
AND
IS
ZERO
SINCE
BOTH
ARE
ABLE
TO
CAPTURE
THE
TRUE
GENERATING
PROCESS
HOWEVER
THE
STRUCTURAL
ERROR
FOR
IS
SUBSTANTIAL
WHICH
IS
EVIDENT
FROM
THE
FACT
THAT
THE
PLATEAU
OCCURS
HIGH
ABOVE
THE
NOISE
FLOOR
FOR
ANY
MODEL
THAT
IS
EXPRESSIVE
ENOUGH
TO
CAPTURE
THE
TRUTH
I
E
ONE
WITH
SMALL
STRUCTURAL
ERROR
THE
TEST
ERROR
WILL
GO
TO
THE
NOISE
FLOOR
AS
N
HOWEVER
IT
WILL
TYPICALLY
GO
TO
ZERO
FASTER
FOR
SIMPLER
MODELS
SINCE
THERE
ARE
FEWER
PARAMETERS
TO
ESTIMATE
IN
PARTICULAR
FOR
FINITE
TRAINING
SETS
THERE
WILL
BE
SOME
DISCREPANCY
BETWEEN
THE
PARAMETERS
THAT
WE
ESTIMATE
AND
THE
BEST
PARAMETERS
THAT
WE
COULD
ESTIMATE
GIVEN
THE
PARTICULAR
MODEL
CLASS
THIS
IS
CALLED
APPROXIMATION
ERROR
AND
GOES
TO
ZERO
AS
N
BUT
IT
GOES
TO
ZERO
FASTER
FOR
SIMPLER
MODELS
THIS
IS
ILLUSTRATED
IN
FIGURE
SEE
ALSO
EXERCISE
IN
DOMAINS
WITH
LOTS
OF
DATA
SIMPLE
METHODS
CAN
WORK
SURPRISINGLY
WELL
HALEVY
ET
AL
HOWEVER
THERE
ARE
STILL
REASONS
TO
STUDY
MORE
SOPHISTICATED
LEARNING
METHODS
BECAUSE
THERE
WILL
ALWAYS
BE
PROBLEMS
FOR
WHICH
WE
HAVE
LITTLE
DATA
FOR
EXAMPLE
EVEN
IN
SUCH
A
DATA
RICH
DOMAIN
AS
WEB
SEARCH
AS
SOON
AS
WE
WANT
TO
START
PERSONALIZING
THE
RESULTS
THE
AMOUNT
OF
DATA
AVAILABLE
FOR
ANY
GIVEN
USER
STARTS
TO
LOOK
SMALL
AGAIN
RELATIVE
TO
THE
COMPLEXITY
OF
THE
PROBLEM
THIS
ASSUMES
THE
TRAINING
DATA
IS
RANDOMLY
SAMPLED
AND
WE
DON
T
JUST
GET
REPETITIONS
OF
THE
SAME
EXAMPLES
HAVING
INFORMATIVELY
SAMPLED
DATA
CAN
HELP
EVEN
MORE
THIS
IS
THE
MOTIVATION
FOR
AN
APPROACH
KNOWN
AS
ACTIVE
LEARNING
WHERE
YOU
GET
TO
CHOOSE
YOUR
TRAINING
DATA
TRUTH
DEGREE
MODEL
DEGREE
TRUTH
DEGREE
MODEL
DEGREE
SIZE
OF
TRAINING
SET
A
SIZE
OF
TRAINING
SET
B
TRUTH
DEGREE
MODEL
DEGREE
TRUTH
DEGREE
MODEL
DEGREE
200
SIZE
OF
TRAINING
SET
C
160
200
SIZE
OF
TRAINING
SET
D
FIGURE
MSE
ON
TRAINING
AND
TEST
SETS
VS
SIZE
OF
TRAINING
SET
FOR
DATA
GENERATED
FROM
A
DEGREE
POLYNOMIAL
WITH
GAUSSIAN
NOISE
OF
VARIANCE
WE
FIT
POLYNOMIAL
MODELS
OF
VARYING
DEGREE
TO
THIS
DATA
A
DEGREE
B
DEGREE
C
DEGREE
D
DEGREE
NOTE
THAT
FOR
SMALL
TRAINING
SET
SIZES
THE
TEST
ERROR
OF
THE
DEGREE
POLYNOMIAL
IS
HIGHER
THAN
THAT
OF
THE
DEGREE
POLYNOMIAL
DUE
TO
OVERFITTING
BUT
THIS
DIFFERENCE
VANISHES
ONCE
WE
HAVE
ENOUGH
DATA
NOTE
ALSO
THAT
THE
DEGREE
POLYNOMIAL
IS
TOO
SIMPLE
AND
HAS
HIGH
TEST
ERROR
EVEN
GIVEN
LARGE
AMOUNTS
OF
TRAINING
DATA
FIGURE
GENERATED
BY
LINREGPOLYVSN
IN
SUCH
CASES
WE
MAY
WANT
TO
LEARN
MULTIPLE
RELATED
MODELS
AT
THE
SAME
TIME
WHICH
IS
KNOWN
AS
MULTI
TASK
LEARNING
THIS
WILL
ALLOW
US
TO
BORROW
STATISTICAL
STRENGTH
FROM
TASKS
WITH
LOTS
OF
DATA
AND
TO
SHARE
IT
WITH
TASKS
WITH
LITTLE
DATA
WE
WILL
DISCUSS
WAYS
TO
DO
LATER
IN
THE
BOOK
BAYESIAN
LINEAR
REGRESSION
ALTHOUGH
RIDGE
REGRESSION
IS
A
USEFUL
WAY
TO
COMPUTE
A
POINT
ESTIMATE
SOMETIMES
WE
WANT
TO
COMPUTE
THE
FULL
POSTERIOR
OVER
W
AND
FOR
SIMPLICITY
WE
WILL
INITIALLY
ASSUME
THE
NOISE
VARIANCE
IS
KNOWN
SO
WE
FOCUS
ON
COMPUTING
P
W
D
THEN
IN
SECTION
WE
CONSIDER
THE
GENERAL
CASE
WHERE
WE
COMPUTE
P
W
WE
ASSUME
THROUGHOUT
A
GAUSSIAN
LIKELIHOOD
MODEL
PERFORMING
BAYESIAN
INFERENCE
WITH
A
ROBUST
LIKELIHOOD
IS
ALSO
POSSIBLE
BUT
REQUIRES
MORE
ADVANCED
TECHNIQUES
SEE
EXERCISE
COMPUTING
THE
POSTERIOR
IN
LINEAR
REGRESSION
THE
LIKELIHOOD
IS
GIVEN
BY
P
Y
X
W
Μ
N
Y
Μ
XW
EXP
Y
XW
Y
XW
WHERE
Μ
IS
AN
OFFSET
TERM
IF
THE
INPUTS
ARE
CENTERED
SO
I
XIJ
FOR
EACH
J
THE
MEAN
OF
THE
OUTPUT
IS
EQUALLY
LIKELY
TO
BE
POSITIVE
OR
NEGATIVE
SO
LET
US
PUT
AN
IMPROPER
PRIOR
ON
Μ
OF
THE
FORM
P
Μ
AND
THEN
INTEGRATE
IT
OUT
TO
GET
P
Y
X
W
EXP
Y
XW
N
WHERE
Y
N
YI
IS
THE
EMPIRICAL
MEAN
OF
THE
OUTPUT
FOR
NOTATIONAL
SIMPLICITY
WE
SHALL
ASSUME
THE
OUTPUT
HAS
BEEN
CENTERED
AND
WRITE
Y
FOR
Y
THE
CONJUGATE
PRIOR
TO
THE
ABOVE
GAUSSIAN
LIKELIHOOD
IS
ALSO
A
GAUSSIAN
WHICH
WE
WILL
DENOTE
BY
P
W
W
USING
BAYES
RULE
FOR
GAUSSIANS
EQUATION
THE
POSTERIOR
IS
GIVEN
BY
P
W
X
Y
N
W
N
Y
XW
N
W
WN
VN
WN
VN
VN
XT
Y
V
V
XT
X
N
VN
XT
X
IF
AND
Τ
THEN
THE
POSTERIOR
MEAN
REDUCES
TO
THE
RIDGE
ESTIMATE
IF
WE
DEFINE
Λ
THIS
IS
BECAUSE
THE
MEAN
AND
MODE
OF
A
GAUSSIAN
ARE
THE
SAME
TO
GAIN
INSIGHT
INTO
THE
POSTERIOR
DISTRIBUTION
AND
NOT
JUST
ITS
MODE
LET
US
CONSIDER
A
EXAMPLE
Y
X
W
E
WHERE
THE
TRUE
PARAMETERS
ARE
AND
IN
FIGURE
WE
PLOT
THE
PRIOR
THE
LIKELIHOOD
THE
POSTERIOR
AND
SOME
SAMPLES
FROM
THE
POSTERIOR
PREDICTIVE
IN
PARTICULAR
THE
RIGHT
HAND
COLUMN
PLOTS
THE
FUNCTION
Y
X
W
WHERE
X
RANGES
OVER
AND
W
W
WN
VN
IS
A
SAMPLE
FROM
THE
PARAMETER
POSTERIOR
INITIALLY
WHEN
WE
SAMPLE
FROM
THE
PRIOR
FIRST
ROW
OUR
PREDICTIONS
ARE
ALL
OVER
THE
PLACE
SINCE
OUR
PRIOR
IS
UNIFORM
AFTER
WE
SEE
ONE
DATA
POINT
SECOND
ROW
OUR
POSTERIOR
BECOMES
CONSTRAINED
BY
THE
CORRESPONDING
LIKELIHOOD
AND
OUR
PREDICTIONS
PASS
CLOSE
TO
THE
OBSERVED
DATA
HOWEVER
WE
SEE
THAT
THE
POSTERIOR
HAS
A
RIDGE
LIKE
SHAPE
REFLECTING
THE
FACT
THAT
THERE
ARE
MANY
POSSIBLE
SOLUTIONS
WITH
DIFFERENT
LIKELIHOOD
PRIOR
POSTERIOR
DATA
SPACE
Y
X
Y
X
Y
X
W1
Y
X
FIGURE
SEQUENTIAL
BAYESIAN
UPDATING
OF
A
LINEAR
REGRESSION
MODEL
P
Y
X
Y
ROW
REPRESENTS
THE
PRIOR
ROW
REPRESENTS
THE
FIRST
DATA
POINT
ROW
REPRESENTS
THE
SECOND
DATA
POINT
ROW
REPRESENTS
THE
DATA
POINT
LEFT
COLUMN
LIKELIHOOD
FUNCTION
FOR
CURRENT
DATA
POINT
MIDDLE
COLUMN
POSTERIOR
GIVEN
DATA
SO
FAR
P
W
N
N
SO
THE
FIRST
LINE
IS
THE
PRIOR
RIGHT
COLUMN
SAMPLES
FROM
THE
CURRENT
PRIOR
POSTERIOR
PREDICTIVE
DISTRIBUTION
THE
WHITE
CROSS
IN
COLUMNS
AND
REPRESENTS
THE
TRUE
PARAMETER
VALUE
WE
SEE
THAT
THE
MODE
OF
THE
POSTERIOR
RAPIDLY
AFTER
SAMPLES
CONVERGES
TO
THIS
POINT
THE
BLUE
CIRCLES
IN
COLUMN
ARE
THE
OBSERVED
DATA
POINTS
BASED
ON
FIGURE
OF
BISHOP
FIGURE
GENERATED
BY
SLOPES
INTERCEPTS
THIS
MAKES
SENSE
SINCE
WE
CANNOT
UNIQUELY
INFER
TWO
PARAMETERS
FROM
ONE
OBSERVATION
AFTER
WE
SEE
TWO
DATA
POINTS
THIRD
ROW
THE
POSTERIOR
BECOMES
MUCH
NARROWER
AND
OUR
PREDICTIONS
ALL
HAVE
SIMILAR
SLOPES
AND
INTERCEPTS
AFTER
WE
OBSERVE
DATA
POINTS
LAST
ROW
THE
POSTERIOR
IS
ESSENTIALLY
A
DELTA
FUNCTION
CENTERED
ON
THE
TRUE
VALUE
INDICATED
BY
A
WHITE
CROSS
THE
ESTIMATE
CONVERGES
TO
THE
TRUTH
SINCE
THE
DATA
WAS
GENERATED
FROM
THIS
MODEL
AND
BECAUSE
BAYES
IS
A
CONSISTENT
ESTIMATOR
SEE
SECTION
FOR
DISCUSSION
OF
THIS
POINT
COMPUTING
THE
POSTERIOR
PREDICTIVE
IT
TOUGH
TO
MAKE
PREDICTIONS
ESPECIALLY
ABOUT
THE
FUTURE
YOGI
BERRA
IN
MACHINE
LEARNING
WE
OFTEN
CARE
MORE
ABOUT
PREDICTIONS
THAN
ABOUT
INTERPRETING
THE
PARAME
TERS
USING
EQUATION
WE
CAN
EASILY
SHOW
THAT
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
AT
A
TEST
POINT
X
IS
ALSO
GAUSSIAN
P
Y
X
D
N
Y
XT
W
N
W
WN
VN
DW
N
Y
WT
X
X
X
XT
VNX
THE
VARIANCE
IN
THIS
PREDICTION
X
DEPENDS
ON
TWO
TERMS
THE
VARIANCE
OF
THE
OBSERVATION
NOISE
AND
THE
VARIANCE
IN
THE
PARAMETERS
VN
THE
LATTER
TRANSLATES
INTO
VARIANCE
ABOUT
OBSERVATIONS
IN
A
WAY
WHICH
DEPENDS
ON
HOW
CLOSE
X
IS
TO
THE
TRAINING
DATA
THIS
IS
ILLUSTRATED
IN
FIGURE
WHERE
WE
SEE
THAT
THE
ERROR
BARS
GET
LARGER
AS
WE
MOVE
AWAY
FROM
THE
TRAINING
POINTS
REPRESENTING
INCREASED
UNCERTAINTY
THIS
IS
IMPORTANT
FOR
APPLICATIONS
SUCH
AS
ACTIVE
LEARNING
WHERE
WE
WANT
TO
MODEL
WHAT
WE
DON
T
KNOW
AS
WELL
AS
WHAT
WE
DO
BY
CONTRAST
THE
PLUGIN
APPROXIMATION
HAS
CONSTANT
SIZED
ERROR
BARS
SINCE
P
Y
X
D
N
Y
XT
W
ΔWˆ
W
DW
P
Y
X
Wˆ
SEE
FIGURE
A
BAYESIAN
INFERENCE
WHEN
IS
UNKNOWN
IN
THIS
SECTION
WE
APPLY
THE
RESULTS
IN
SECTION
TO
THE
PROBLEM
OF
COMPUTING
P
W
FOR
A
LINEAR
REGRESSION
MODEL
THIS
GENERALIZES
THE
RESULTS
FROM
SECTION
WHERE
WE
ASSUMED
WAS
KNOWN
IN
THE
CASE
WHERE
WE
USE
AN
UNINFORMATIVE
PRIOR
WE
WILL
SEE
SOME
INTERESTING
CONNECTIONS
TO
FREQUENTIST
STATISTICS
CONJUGATE
PRIOR
AS
USUAL
THE
LIKELIHOOD
HAS
THE
FORM
P
Y
X
W
N
Y
XW
BY
ANALOGY
TO
SECTION
ONE
CAN
SHOW
THAT
THE
NATURAL
CONJUGATE
PRIOR
HAS
THE
FOLLOWING
FORM
P
W
NIG
W
N
W
IG
D
D
V
Γ
A
EXP
W
T
W
PLUGIN
APPROXIMATION
MLE
POSTERIOR
PREDICTIVE
KNOWN
VARIANCE
A
B
FUNCTIONS
SAMPLED
FROM
PLUGIN
APPROXIMATION
TO
POSTERIOR
FUNCTIONS
SAMPLED
FROM
POSTERIOR
C
D
FIGURE
A
PLUG
IN
APPROXIMATION
TO
PREDICTIVE
DENSITY
WE
PLUG
IN
THE
MLE
OF
THE
PARAMETERS
B
POSTERIOR
PREDICTIVE
DENSITY
OBTAINED
BY
INTEGRATING
OUT
THE
PARAMETERS
BLACK
CURVE
IS
POSTERIOR
MEAN
ERROR
BARS
ARE
STANDARD
DEVIATIONS
OF
THE
POSTERIOR
PREDICTIVE
DENSITY
C
SAMPLES
FROM
THE
PLUGIN
APPROXIMATION
TO
POSTERIOR
PREDICTIVE
D
SAMPLES
FROM
THE
POSTERIOR
PREDICTIVE
FIGURE
GENERATED
BY
LINREGPOSTPREDDEMO
WITH
THIS
PRIOR
AND
LIKELIHOOD
ONE
CAN
SHOW
THAT
THE
POSTERIOR
HAS
THE
FOLLOWING
FORM
P
W
D
NIG
W
WN
VN
AN
BN
WN
VN
XT
Y
VN
XT
X
AN
N
B
B
WT
V
YT
Y
WT
V
THE
EXPRESSIONS
FOR
WN
AND
VN
ARE
SIMILAR
TO
THE
CASE
WHERE
IS
KNOWN
THE
EXPRESSION
FOR
AN
IS
ALSO
INTUITIVE
SINCE
IT
JUST
UPDATES
THE
COUNTS
THE
EXPRESSION
FOR
BN
CAN
BE
INTERPRETED
AS
FOLLOWS
IT
IS
THE
PRIOR
SUM
OF
SQUARES
PLUS
THE
EMPIRICAL
SUM
OF
SQUARES
YT
Y
PLUS
A
TERM
DUE
TO
THE
ERROR
IN
THE
PRIOR
ON
W
THE
POSTERIOR
MARGINALS
ARE
AS
FOLLOWS
P
D
IG
AN
BN
P
W
D
T
WN
BN
V
AN
N
WE
GIVE
A
WORKED
EXAMPLE
OF
USING
THESE
EQUATIONS
IN
SECTION
BY
ANALOGY
TO
SECTION
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
IS
A
STUDENT
T
DISTRIBUTION
IN
PARTICULAR
GIVEN
M
NEW
TEST
INPUTS
X
WE
HAVE
P
Y
X
D
T
Y
X
W
BN
I
X
V
X
T
N
AN
M
N
N
THE
PREDICTIVE
VARIANCE
HAS
TWO
COMPONENTS
BN
AN
IM
DUE
TO
THE
MEASUREMENT
NOISE
AND
BN
AN
X
VNX
T
DUE
TO
THE
UNCERTAINTY
IN
W
THIS
LATTER
TERMS
VARIES
DEPENDING
ON
HOW
CLOSE
THE
TEST
INPUTS
ARE
TO
THE
TRAINING
DATA
IT
IS
COMMON
TO
SET
CORRESPONDING
TO
AN
UNINFORMATIVE
PRIOR
FOR
AND
TO
SET
AND
G
XT
X
FOR
ANY
POSITIVE
VALUE
G
THIS
IS
CALLED
ZELLNER
G
PRIOR
ZELLNER
HERE
G
PLAYS
A
ROLE
ANALOGOUS
TO
Λ
IN
RIDGE
REGRESSION
HOWEVER
THE
PRIOR
COVARIANCE
IS
PROPORTIONAL
TO
XT
X
RATHER
THAN
I
THIS
ENSURES
THAT
THE
POSTERIOR
IS
INVARIANT
TO
SCALING
OF
THE
INPUTS
MINKA
SEE
ALSO
EXERCISE
WE
WILL
SEE
BELOW
THAT
IF
WE
USE
AN
UNINFORMATIVE
PRIOR
THE
POSTERIOR
PRECISION
GIVEN
N
MEASUREMENTS
IS
V
XT
X
THE
UNIT
INFORMATION
PRIOR
IS
DEFINED
TO
CONTAIN
AS
MUCH
INFORMATION
AS
ONE
SAMPLE
KASS
AND
WASSERMAN
TO
CREATE
A
UNIT
INFORMATION
PRIOR
FOR
LINEAR
REGRESSION
WE
NEED
TO
USE
G
N
XT
X
WHICH
IS
EQUIVALENT
TO
THE
G
PRIOR
WITH
UNINFORMATIVE
PRIOR
AN
UNINFORMATIVE
PRIOR
CAN
BE
OBTAINED
BY
CONSIDERING
THE
UNINFORMATIVE
LIMIT
OF
THE
CONJUGATE
G
PRIOR
WHICH
CORRESPONDS
TO
SETTING
G
THIS
IS
EQUIVALENT
TO
AN
IMPROPER
NIG
PRIOR
WITH
I
AND
WHICH
GIVES
P
W
Σ
D
ALTERNATIVELY
WE
CAN
START
WITH
THE
SEMI
CONJUGATE
PRIOR
P
W
P
W
P
AND
TAKE
EACH
TERM
TO
ITS
UNINFORMATIVE
LIMIT
INDIVIDUALLY
WHICH
GIVES
P
W
Σ
THIS
IS
EQUIVALENT
TO
AN
IMPROPER
NIG
PRIOR
WITH
V
I
D
AND
THE
CORRESPONDING
POSTERIOR
IS
GIVEN
BY
P
W
D
NIG
W
WN
VN
AN
BN
WN
Wˆ
MLE
XT
X
Y
VN
XT
X
A
N
D
BN
Y
XWˆ
MLE
T
Y
XWˆ
MLE
WJ
E
WJ
D
VAR
WJ
D
CI
SIG
W3
274
324
285
TABLE
POSTERIOR
MEAN
STANDARD
DEVIATION
AND
CREDIBLE
INTERVALS
FOR
A
LINEAR
REGRESSION
MODEL
WITH
AN
UNINFORMATIVE
PRIOR
FIT
TO
THE
CATERPILLAR
DATA
PRODUCED
BY
LINREGBAYESCATERPILLAR
THE
MARGINAL
DISTRIBUTION
OF
THE
WEIGHTS
IS
GIVEN
BY
P
W
D
T
W
Wˆ
N
D
C
N
D
WHERE
C
XT
X
AND
Wˆ
IS
THE
MLE
WE
DISCUSS
THE
IMPLICATIONS
OF
THESE
EQUATIONS
BELOW
AN
EXAMPLE
WHERE
BAYESIAN
AND
FREQUENTIST
INFERENCE
COINCIDE
THE
USE
OF
A
SEMI
CONJUGATE
UNINFORMATIVE
PRIOR
IS
INTERESTING
BECAUSE
THE
RESULTING
POSTERIOR
TURNS
OUT
TO
BE
EQUIVALENT
TO
THE
RESULTS
FROM
FREQUENTIST
STATISTICS
SEE
ALSO
SECTION
IN
PARTICULAR
FROM
EQUATION
WE
HAVE
CJJ
P
WJ
D
T
WJ
WˆJ
N
D
N
D
THIS
IS
EQUIVALENT
TO
THE
SAMPLING
DISTRIBUTION
OF
THE
MLE
WHICH
IS
GIVEN
BY
THE
FOLLOWING
SEE
E
G
RICE
CASELLA
AND
BERGER
WJ
WˆJ
T
SJ
N
D
WHERE
SJ
N
D
IS
THE
STANDARD
ERROR
OF
THE
ESTIMATED
PARAMETER
SEE
SECTION
FOR
A
DISCUSSION
OF
SAMPLING
DISTRIBUTIONS
CONSEQUENTLY
THE
FREQUENTIST
CONFIDENCE
INTERVAL
AND
THE
BAYESIAN
MARGINAL
CREDIBLE
INTERVAL
FOR
THE
PARAMETERS
ARE
THE
SAME
IN
THIS
CASE
AS
A
WORKED
EXAMPLE
OF
THIS
CONSIDER
THE
CATERPILLAR
DATASET
FROM
MARIN
AND
ROBERT
THE
DETAILS
OF
WHAT
THE
DATA
MEAN
DON
T
MATTER
FOR
OUR
PRESENT
PURPOSES
WE
CAN
COMPUTE
THE
POSTERIOR
MEAN
AND
STANDARD
DEVIATION
AND
THE
CREDIBLE
INTERVALS
CI
FOR
THE
REGRESSION
COEFFICIENTS
USING
EQUATION
THE
RESULTS
ARE
SHOWN
IN
TABLE
IT
IS
EASY
TO
CHECK
THAT
THESE
CREDIBLE
INTERVALS
ARE
IDENTICAL
TO
THE
CONFIDENCE
INTERVALS
COMPUTED
USING
STANDARD
FREQUENTIST
METHODS
SEE
LINREGBAYESCATERPILLAR
FOR
THE
CODE
WE
CAN
ALSO
USE
THESE
MARGINAL
POSTERIORS
TO
COMPUTE
IF
THE
COEFFICIENTS
ARE
SIGNIFICANTLY
DIFFERENT
FROM
AN
INFORMAL
WAY
TO
DO
THIS
WITHOUT
USING
DECISION
THEORY
IS
TO
CHECK
IF
ITS
CI
EXCLUDES
FROM
TABLE
WE
SEE
THAT
THE
CIS
FOR
COEFFICIENTS
ARE
ALL
SIGNIFICANT
BY
THIS
MEASURE
SO
WE
PUT
A
LITTLE
STAR
BY
THEM
IT
IS
EASY
TO
CHECK
THAT
THESE
RESULTS
ARE
THE
SAME
AS
THOSE
PRODUCED
BY
STANDARD
FREQUENTIST
SOFTWARE
PACKAGES
WHICH
COMPUTE
P
VALUES
AT
THE
LEVEL
ALTHOUGH
THE
CORRESPONDENCE
BETWEEN
THE
BAYESIAN
AND
FREQUENTIST
RESULTS
MIGHT
SEEM
AP
PEALING
TO
SOME
READERS
RECALL
FROM
SECTION
THAT
FREQUENTIST
INFERENCE
IS
RIDDLED
WITH
PATHOLO
GIES
ALSO
NOTE
THAT
THE
MLE
DOES
NOT
EVEN
EXIST
WHEN
N
D
SO
STANDARD
FREQUENTIST
INFERENCE
THEORY
BREAKS
DOWN
IN
THIS
SETTING
BAYESIAN
INFERENCE
THEORY
STILL
WORKS
ALTHOUGH
IT
REQUIRES
THE
USE
OF
PROPER
PRIORS
SEE
MARUYAMA
AND
GEORGE
FOR
ONE
EXTENSION
OF
THE
G
PRIOR
TO
THE
CASE
WHERE
D
N
EB
FOR
LINEAR
REGRESSION
EVIDENCE
PROCEDURE
SO
FAR
WE
HAVE
ASSUMED
THE
PRIOR
IS
KNOWN
IN
THIS
SECTION
WE
DESCRIBE
AN
EMPIRICAL
BAYES
PROCEDURE
FOR
PICKING
THE
HYPER
PARAMETERS
MORE
PRECISELY
WE
CHOOSE
Η
Α
Λ
TO
MAXIMIZE
THE
MARIGNAL
LIKELIHOOD
WHERE
Λ
BE
THE
PRECISION
OF
THE
OBSERVATION
NOISE
AND
Α
IS
THE
PRECISION
OF
THE
PRIOR
P
W
W
Α
THIS
IS
KNOWN
AS
THE
EVIDENCE
PROCEDURE
MACKAY
SEE
SECTION
FOR
THE
ALGORITHMIC
DETAILS
THE
EVIDENCE
PROCEDURE
PROVIDES
AN
ALTERNATIVE
TO
USING
CROSS
VALIDATION
FOR
EXAMPLE
IN
FIGURE
B
WE
PLOT
THE
LOG
MARGINAL
LIKELIHOOD
FOR
DIFFERENT
VALUES
OF
Α
AS
WELL
AS
THE
MAXIMUM
VALUE
FOUND
BY
THE
OPTIMIZER
WE
SEE
THAT
IN
THIS
EXAMPLE
WE
GET
THE
SAME
RESULT
AS
CV
SHOWN
IN
FIGURE
A
WE
KEPT
Λ
FIXED
IN
BOTH
METHODS
TO
MAKE
THEM
COMPARABLE
THE
PRINCIPLE
PRACTICAL
ADVANTAGE
OF
THE
EVIDENCE
PROCEDURE
OVER
CV
WILL
BECOME
APPARENT
IN
SECTION
WHERE
WE
GENERALIZE
THE
PRIOR
BY
ALLOWING
A
DIFFERENT
ΑJ
FOR
EVERY
FEATURE
THIS
CAN
BE
USED
TO
PERFORM
FEATURE
SELECTION
USING
A
TECHNIQUE
KNOWN
AS
AUTOMATIC
RELEVANCY
DETERMINATION
OR
ARD
BY
CONTRAST
IT
WOULD
NOT
BE
POSSIBLE
TO
USE
CV
TO
TUNE
D
DIFFERENT
HYPER
PARAMETERS
THE
EVIDENCE
PROCEDURE
IS
ALSO
USEFUL
WHEN
COMPARING
DIFFERENT
KINDS
OF
MODELS
SINCE
IT
PROVIDES
A
GOOD
APPROXIMATION
TO
THE
EVIDENCE
P
D
M
P
D
W
M
P
W
M
Η
P
Η
M
DWDΗ
MAX
P
D
W
M
P
W
M
Η
P
Η
M
DW
IT
IS
IMPORTANT
TO
AT
LEAST
APPROXIMATELY
INTEGRATE
OVER
Η
RATHER
THAN
SETTING
IT
ARBITRARILY
FOR
REASONS
DISCUSSED
IN
SECTION
INDEED
THIS
IS
THE
METHOD
WE
USED
TO
EVALUATE
THE
MARGINAL
ALTERNATIVELY
WE
COULD
INTEGRATE
OUT
Λ
ANALYTICALLY
AS
SHOWN
IN
SECTION
AND
JUST
OPTIMIZE
Α
BUNTINE
AND
WEIGEND
HOWEVER
IT
TURNS
OUT
THAT
THIS
IS
LESS
ACCURATE
THAN
OPTIMIZING
BOTH
Α
AND
Λ
MACKAY
FOLD
CROSS
VALIDATION
NTRAIN
LOG
EVIDENCE
LOG
LAMBDA
A
150
LOG
ALPHA
B
FIGURE
A
ESTIMATE
OF
TEST
MSE
PRODUCED
BY
FOLD
CROSS
VALIDATION
VS
LOG
Λ
THE
SMALLEST
VALUE
IS
INDICATED
BY
THE
VERTICAL
LINE
NOTE
THE
VERTICAL
SCALE
IS
IN
LOG
UNITS
C
LOG
MARGINAL
LIKELIHOOD
VS
LOG
Α
THE
LARGEST
VALUE
IS
INDICATED
BY
THE
VERTICAL
LINE
FIGURE
GENERATED
BY
LINREGPOLYVSREGDEMO
LIKELIHOOD
FOR
THE
POLYNOMIAL
REGRESSION
MODELS
IN
FIGURES
AND
FOR
A
MORE
BAYESIAN
APPROACH
IN
WHICH
WE
MODEL
OUR
UNCERTAINTY
ABOUT
Η
RATHER
THAN
COMPUTING
POINT
ESTIMATES
SEE
SECTION
EXERCISES
EXERCISE
BEHAVIOR
OF
TRAINING
SET
ERROR
WITH
INCREASING
SAMPLE
SIZE
THE
ERROR
ON
THE
TEST
WILL
ALWAYS
DECREASE
AS
WE
GET
MORE
TRAINING
DATA
SINCE
THE
MODEL
WILL
BE
BETTER
ESTIMATED
HOWEVER
AS
SHOWN
IN
FIGURE
FOR
SUFFICIENTLY
COMPLEX
MODELS
THE
ERROR
ON
THE
TRAINING
SET
CAN
INCREASE
WE
WE
GET
MORE
TRAINING
DATA
UNTIL
WE
REACH
SOME
PLATEAU
EXPLAIN
WHY
EXERCISE
MULTI
OUTPUT
LINEAR
REGRESSION
SOURCE
JAAKKOLA
WHEN
WE
HAVE
MULTIPLE
INDEPENDENT
OUTPUTS
IN
LINEAR
REGRESSION
THE
MODEL
BECOMES
P
Y
X
W
N
YJ
WT
XI
SINCE
THE
LIKELIHOOD
FACTORIZES
ACROSS
DIMENSIONS
SO
DOES
THE
MLE
THUS
Wˆ
Wˆ
Wˆ
M
WHERE
Wˆ
J
XT
X
J
IN
THIS
EXERCISE
WE
APPLY
THIS
RESULT
TO
A
MODEL
WITH
DIMENSIONAL
RESPONSE
VECTOR
YI
SUPPOSE
WE
HAVE
SOME
BINARY
INPUT
DATA
XI
THE
TRAINING
DATA
IS
AS
FOLLOWS
LET
US
EMBED
EACH
XI
INTO
USING
THE
FOLLOWING
BASIS
FUNCTION
Φ
T
Φ
T
THE
MODEL
BECOMES
Yˆ
WT
Φ
X
WHERE
W
IS
A
MATRIX
COMPUTE
THE
MLE
FOR
W
FROM
THE
ABOVE
DATA
EXERCISE
CENTERING
AND
RIDGE
REGRESSION
ASSUME
THAT
X
SO
THE
INPUT
DATA
HAS
BEEN
CENTERED
SHOW
THAT
THE
OPTIMIZER
OF
J
W
Y
XW
T
Y
XW
ΛWT
W
IS
Y
W
XT
X
ΛI
Y
EXERCISE
MLE
FOR
FOR
LINEAR
REGRESSION
SHOW
THAT
THE
MLE
FOR
THE
ERROR
VARIANCE
IN
LINEAR
REGRESSION
IS
GIVEN
BY
Y
XT
Wˆ
THIS
IS
JUST
THE
EMPIRICAL
VARIANCE
OF
THE
RESIDUAL
ERRORS
WHEN
WE
PLUG
IN
OUR
ESTIMATE
OF
Wˆ
EXERCISE
MLE
FOR
THE
OFFSET
TERM
IN
LINEAR
REGRESSION
LINEAR
REGRESSION
HAS
THE
FORM
E
Y
X
WT
X
IT
IS
COMMON
TO
INCLUDE
A
COLUMN
OF
IN
THE
DESIGN
MATRIX
SO
WE
CAN
SOLVE
FOR
THE
OFFSET
TERM
TERM
AND
THE
OTHER
PARAMETERS
W
AT
THE
SAME
TIME
USING
THE
NORMAL
EQUATIONS
HOWEVER
IT
IS
ALSO
POSSIBLE
TO
SOLVE
FOR
W
AND
SEPARATELY
SHOW
THAT
Wˆ
Y
XT
W
Y
XT
W
SO
MODELS
THE
DIFFERENCE
IN
THE
AVERAGE
OUTPUT
FROM
THE
AVERAGE
PREDICTED
OUTPUT
ALSO
SHOW
THAT
N
Wˆ
XT
XC
YC
XI
X
XI
X
T
N
YI
Y
XI
X
C
C
I
I
WHERE
XC
IS
THE
CENTERED
INPUT
MATRIX
CONTAINING
XC
XI
X
ALONG
ITS
ROWS
AND
YC
Y
Y
IS
THE
CENTERED
OUTPUT
VECTOR
THUS
WE
CAN
FIRST
COMPUTE
Wˆ
ON
CENTERED
DATA
AND
THEN
ESTIMATE
USING
Y
XT
Wˆ
EXERCISE
MLE
FOR
SIMPLE
LINEAR
REGRESSION
SIMPLE
LINEAR
REGRESSION
REFERS
TO
THE
CASE
WHERE
THE
INPUT
IS
SCALAR
SO
D
SHOW
THAT
THE
MLE
IN
THIS
CASE
IS
GIVEN
BY
THE
FOLLOWING
EQUATIONS
WHICH
MAY
BE
FAMILIAR
FROM
BASIC
STATISTICS
CLASSES
I
XI
X
YI
Y
I
XIYI
NX
Y
COV
X
Y
X
X
VAR
X
Y
E
Y
X
SEE
FOR
A
DEMO
EXERCISE
SUFFICIENT
STATISTICS
FOR
ONLINE
LINEAR
REGRESSION
SOURCE
JAAKKOLA
CONSIDER
FITTING
THE
MODEL
Yˆ
USING
LEAST
SQUARES
UNFORTUNATELY
WE
DID
NOT
KEEP
THE
ORIGINAL
DATA
XI
YI
BUT
WE
DO
HAVE
THE
FOLLOWING
FUNCTIONS
STATISTICS
OF
THE
DATA
N
N
X
N
X
Y
N
Y
N
I
I
N
N
I
I
N
N
C
N
X
X
C
N
X
X
Y
Y
C
N
Y
Y
A
WHAT
ARE
THE
MINIMAL
SET
OF
STATISTICS
THAT
WE
NEED
TO
ESTIMATE
HINT
SEE
EQUATION
B
WHAT
ARE
THE
MINIMAL
SET
OF
STATISTICS
THAT
WE
NEED
TO
ESTIMATE
HINT
SEE
EQUATION
C
SUPPOSE
A
NEW
DATA
POINT
XN
YN
ARRIVES
AND
WE
WANT
TO
UPDATE
OUR
SUFFICIENT
STATISTICS
WITHOUT
LOOKING
AT
THE
OLD
DATA
WHICH
WE
HAVE
NOT
STORED
THIS
IS
USEFUL
FOR
ONLINE
LEARNING
SHOW
THAT
WE
CAN
THIS
FOR
X
AS
FOLLOWS
N
X
N
N
I
XI
N
N
N
X
N
X
N
N
X
THIS
HAS
THE
FORM
NEW
ESTIMATE
IS
OLD
ESTIMATE
PLUS
CORRECTION
WE
SEE
THAT
THE
SIZE
OF
THE
CORRECTION
DIMINISHES
OVER
TIME
I
E
AS
WE
GET
MORE
SAMPLES
DERIVE
A
SIMILAR
EXPRESSION
TO
UPDATE
Y
D
SHOW
THAT
ONE
CAN
UPDATE
C
N
RECURSIVELY
USING
N
XY
X
N
N
YN
NC
N
NX
N
Y
N
N
X
N
Y
N
L
DERIVE
A
SIMILAR
EXPRESSION
TO
UPDATE
CXX
E
IMPLEMENT
THE
ONLINE
LEARNING
ALGORITHM
I
E
WRITE
A
FUNCTION
OF
THE
FORM
W
SS
LINREGUPDATESS
SS
X
Y
WHERE
X
AND
Y
ARE
SCALARS
AND
SS
IS
A
STRUCTURE
CONTAINING
THE
SUFFICIENT
STATISTICS
F
PLOT
THE
COEFFICIENTS
OVER
TIME
USING
THE
DATASET
IN
SPECIFICALLY
USE
X
Y
POLYDATAMAKE
SAMPLING
THIBAUX
CHECK
THAT
THEY
CONVERGE
TO
THE
SOLUTION
GIVEN
BY
THE
BATCH
OFFLINE
LEARNER
I
E
ORDINARY
LEAST
SQUARES
YOUR
RESULT
SHOULD
LOOK
LIKE
FIGURE
TURN
IN
YOUR
DERIVATION
CODE
AND
PLOT
EXERCISE
BAYESIAN
LINEAR
REGRESSION
IN
WITH
KNOWN
SOURCE
BOLSTAD
CONSIDER
FITTING
A
MODEL
OF
THE
FORM
P
Y
X
Θ
N
Y
TO
THE
DATA
SHOWN
BELOW
ONLINE
LINEAR
REGRESSION
TIME
FIGURE
REGRESSION
COEFFICIENTS
OVER
TIME
PRODUCED
BY
EXERCISE
X
Y
60
A
COMPUTE
AN
UNBIASED
ESTIMATE
OF
USING
Y
Yˆ
THE
DENOMINATOR
IS
N
SINCE
WE
HAVE
INPUTS
NAMELY
THE
OFFSET
TERM
AND
X
HERE
YˆI
AND
Wˆ
IS
THE
MLE
B
NOW
ASSUME
THE
FOLLOWING
PRIOR
ON
W
P
W
P
P
USE
AN
IMPROPER
UNIFORM
PRIOR
ON
AND
A
N
PRIOR
ON
SHOW
THAT
THIS
CAN
BE
WRITTEN
AS
A
GAUSSIAN
PRIOR
OF
THE
FORM
P
W
N
W
WHAT
ARE
AND
C
COMPUTE
THE
MARGINAL
POSTERIOR
OF
THE
SLOPE
D
WHERE
D
IS
THE
DALTA
ABOVE
AND
IS
THE
CAN
USE
MATLAB
IF
YOU
LIKE
HINT
THE
POSTERIOR
VARIANCE
IS
A
VERY
SMALL
NUMBER
D
WHAT
IS
A
CREDIBLE
INTERVAL
FOR
EXERCISE
GENERATIVE
MODEL
FOR
LINEAR
REGRESSION
LINEAR
REGRESSION
IS
THE
PROBLEM
OF
ESTIMATING
E
Y
X
USING
A
LINEAR
FUNCTION
OF
THE
FORM
WT
X
TYPICALLY
WE
ASSUME
THAT
THE
CONDITIONAL
DISTRIBUTION
OF
Y
GIVEN
X
IS
GAUSSIAN
WE
CAN
EITHER
ESTIMATE
THIS
CONDITIONAL
GAUSSIAN
DIRECTLY
A
DISCRIMINATIVE
APPROACH
OR
WE
CAN
FIT
A
GAUSSIAN
TO
THE
JOINT
DISTRIBUTION
OF
X
Y
AND
THEN
DERIVE
E
Y
X
X
IN
EXERCISE
WE
SHOWED
THAT
THE
DISCRIMINATIVE
APPROACH
LEADS
TO
THESE
EQUATIONS
E
Y
X
WT
X
Y
X
W
W
XT
XC
YC
C
C
WHERE
XC
X
X
IS
THE
CENTERED
INPUT
MATRIX
AND
X
REPLICATES
X
ACROSS
THE
ROWS
SIMILARLY
YC
Y
Y
IS
THE
CENTERED
OUTPUT
VECTOR
AND
Y
REPLICATES
Y
ACROSS
THE
ROWS
A
BY
FINDING
THE
MAXIMUM
LIKELIHOOD
ESTIMATES
OF
ΣXX
ΣXY
ΜX
AND
ΜY
DERIVE
THE
ABOVE
EQUATIONS
BY
FITTING
A
JOINT
GAUSSIAN
TO
X
Y
AND
USING
THE
FORMULA
FOR
CONDITIONING
A
GAUSSIAN
SEE
SECTION
SHOW
YOUR
WORK
B
WHAT
ARE
THE
ADVANTAGES
AND
DISADVANTAGES
OF
THIS
APPROACH
COMPARED
TO
THE
STANDARD
DISCRIMINATIVE
APPROACH
EXERCISE
BAYESIAN
LINEAR
REGRESSION
USING
THE
G
PRIOR
SHOW
THAT
WHEN
WE
USE
THE
G
PRIOR
P
W
NIG
W
G
XT
X
THE
POSTERIOR
HAS
THE
FOLLOWING
FORM
P
W
D
NIG
W
WN
VN
AN
BN
V
G
XT
X
N
G
W
G
Wˆ
N
G
MLE
AN
N
T
T
BN
G
Wˆ
MLEX
XWˆ
MLE
LOGISTIC
REGRESSION
INTRODUCTION
ONE
WAY
TO
BUILD
A
PROBABILISTIC
CLASSIFIER
IS
TO
CREATE
A
JOINT
MODEL
OF
THE
FORM
P
Y
X
AND
THEN
TO
CONDITION
ON
X
THEREBY
DERIVING
P
Y
X
THIS
IS
CALLED
THE
GENERATIVE
APPROACH
AN
ALTERNATIVE
APPROACH
IS
TO
FIT
A
MODEL
OF
THE
FORM
P
Y
X
DIRECTLY
THIS
IS
CALLED
THE
DISCRIMI
NATIVE
APPROACH
AND
IS
THE
APPROACH
WE
ADOPT
IN
THIS
CHAPTER
IN
PARTICULAR
WE
WILL
ASSUME
DISCRIMINATIVE
MODELS
WHICH
ARE
LINEAR
IN
THE
PARAMETERS
THIS
WILL
TURN
OUT
TO
SIGNIFICANTLY
SIM
PLIFY
MODEL
FITTING
AS
WE
WILL
SEE
IN
SECTION
WE
COMPARE
THE
GENERATIVE
AND
DISCRIMINATIVE
APPROACHES
AND
IN
LATER
CHAPTERS
WE
WILL
CONSIDER
NON
LINEAR
AND
NON
PARAMETRIC
DISCRIMINATIVE
MODELS
MODEL
SPECIFICATION
AS
WE
DISCUSSED
IN
SECTION
LOGISTIC
REGRESSION
CORRESPONDS
TO
THE
FOLLOWING
BINARY
CLASSIFI
CATION
MODEL
P
Y
X
W
BER
Y
SIGM
WT
X
A
EXAMPLE
IS
SHOWN
IN
FIGURE
B
LOGISTIC
REGRESSION
CAN
EASILY
BE
EXTENDED
TO
HIGHER
DIMENSIONAL
INPUTS
FOR
EXAMPLE
FIGURE
SHOWS
PLOTS
OF
P
Y
X
W
SIGM
WT
X
FOR
INPUT
AND
DIFFERENT
WEIGHT
VECTORS
W
IF
WE
THRESHOLD
THESE
PROBABILITIES
AT
WE
INDUCE
A
LINEAR
DECISION
BOUNDARY
WHOSE
NORMAL
PERPENDICULAR
IS
GIVEN
BY
W
MODEL
FITTING
IN
THIS
SECTION
WE
DISCUSS
ALGORITHMS
FOR
ESTIMATING
THE
PARAMETERS
OF
A
LOGISTIC
REGRESSION
MODEL
W
W
W
W
X
X
X
W
W
W
X
X
W
W
X
W
X
X
X
X
X
W
W
X
FIGURE
PLOTS
OF
SIGM
HERE
W
DEFINES
THE
NORMAL
TO
THE
DECISION
BOUNDARY
POINTS
TO
THE
RIGHT
OF
THIS
HAVE
SIGM
WT
X
AND
POINTS
TO
THE
LEFT
HAVE
SIGM
WT
X
BASED
ON
FIGURE
OF
MACKAY
FIGURE
GENERATED
BY
MLE
THE
NEGATIVE
LOG
LIKELIHOOD
FOR
LOGISTIC
REGRESSION
IS
GIVEN
BY
NLL
W
LOG
ΜI
YI
ΜI
I
YI
I
N
YI
LOG
ΜI
YI
LOG
ΜI
I
THIS
IS
ALSO
CALLED
THE
CROSS
ENTROPY
ERROR
FUNCTION
SEE
SECTION
ANOTHER
WAY
OF
WRITING
THIS
IS
AS
FOLLOWS
SUPPOSE
Y
I
INSTEAD
OF
YI
WE
HAVE
P
Y
EXP
WT
X
N
AND
P
Y
T
X
HENCE
NLL
W
LOG
EXP
Y
IWT
XI
I
UNLIKE
LINEAR
REGRESSION
WE
CAN
NO
LONGER
WRITE
DOWN
THE
MLE
IN
CLOSED
FORM
INSTEAD
WE
NEED
TO
USE
AN
OPTIMIZATION
ALGORITHM
TO
COMPUTE
IT
FOR
THIS
WE
NEED
TO
DERIVE
THE
GRADIENT
AND
HESSIAN
IN
THE
CASE
OF
LOGISTIC
REGRESSION
ONE
CAN
SHOW
EXERCISE
THAT
THE
GRADIENT
AND
HESSIAN
50
A
B
FIGURE
GRADIENT
DESCENT
ON
A
SIMPLE
FUNCTION
STARTING
FROM
FOR
STEPS
USING
A
FIXED
LEARNING
RATE
STEP
SIZE
Η
THE
GLOBAL
MINIMUM
IS
AT
A
Η
B
Η
FIGURE
GENERATED
BY
STEEPESTDESCENTDEMO
OF
THIS
ARE
GIVEN
BY
THE
FOLLOWING
D
G
F
W
ΜI
DW
I
YI
XI
XT
Μ
Y
H
D
G
W
T
Μ
XT
Μ
Μ
X
XT
XT
SX
WHERE
DIAG
ΜI
ΜI
ONE
CAN
ALSO
SHOW
EXERCISE
THAT
H
IS
POSITIVE
DEFINITE
HENCE
THE
NLL
IS
CONVEX
AND
HAS
A
UNIQUE
GLOBAL
MINIMUM
BELOW
WE
DISCUSS
SOME
METHODS
FOR
FINDING
THIS
MINIMUM
STEEPEST
DESCENT
PERHAPS
THE
SIMPLEST
ALGORITHM
FOR
UNCONSTRAINED
OPTIMIZATION
IS
GRADIENT
DESCENT
ALSO
KNOWN
AS
STEEPEST
DESCENT
THIS
CAN
BE
WRITTEN
AS
FOLLOWS
ΘK
ΘK
ΗKGK
WHERE
ΗK
IS
THE
STEP
SIZE
OR
LEARNING
RATE
THE
MAIN
ISSUE
IN
GRADIENT
DESCENT
IS
HOW
SHOULD
WE
SET
THE
STEP
SIZE
THIS
TURNS
OUT
TO
BE
QUITE
TRICKY
IF
WE
USE
A
CONSTANT
LEARNING
RATE
BUT
MAKE
IT
TOO
SMALL
CONVERGENCE
WILL
BE
VERY
SLOW
BUT
IF
WE
MAKE
IT
TOO
LARGE
THE
METHOD
CAN
FAIL
TO
CONVERGE
AT
ALL
THIS
IS
ILLUSTRATED
IN
FIGURE
WHERE
WE
PLOT
THE
FOLLOWING
CONVEX
FUNCTION
F
Θ
WE
ARBITRARILY
DECIDE
TO
START
FROM
IN
FIGURE
A
WE
USE
A
FIXED
STEP
SIZE
OF
Η
WE
SEE
THAT
IT
MOVES
SLOWLY
ALONG
THE
VALLEY
IN
FIGURE
B
WE
USE
A
FIXED
STEP
SIZE
OF
Η
WE
SEE
THAT
THE
ALGORITHM
STARTS
OSCILLATING
UP
AND
DOWN
THE
SIDES
OF
THE
VALLEY
AND
NEVER
CONVERGES
TO
THE
OPTIMUM
EXACT
LINE
SEARCHING
50
A
B
FIGURE
A
STEEPEST
DESCENT
ON
THE
SAME
FUNCTION
AS
FIGURE
STARTING
FROM
USING
LINE
SEARCH
FIGURE
GENERATED
BY
STEEPESTDESCENTDEMO
B
ILLUSTRATION
OF
THE
FACT
THAT
AT
THE
END
OF
A
LINE
SEARCH
TOP
OF
PICTURE
THE
LOCAL
GRADIENT
OF
THE
FUNCTION
WILL
BE
PERPENDICULAR
TO
THE
SEARCH
DIRECTION
BASED
ON
FIGURE
OF
PRESS
ET
AL
LET
US
DEVELOP
A
MORE
STABLE
METHOD
FOR
PICKING
THE
STEP
SIZE
SO
THAT
THE
METHOD
IS
GUARAN
TEED
TO
CONVERGE
TO
A
LOCAL
OPTIMUM
NO
MATTER
WHERE
WE
START
THIS
PROPERTY
IS
CALLED
GLOBAL
CONVERGENCE
WHICH
SHOULD
NOT
BE
CONFUSED
WITH
CONVERGENCE
TO
THE
GLOBAL
OPTIMUM
BY
TAYLOR
THEOREM
WE
HAVE
F
Θ
ΗD
F
Θ
ΗGT
D
WHERE
D
IS
OUR
DESCENT
DIRECTION
SO
IF
Η
IS
CHOSEN
SMALL
ENOUGH
THEN
F
Θ
ΗD
F
Θ
SINCE
THE
GRADIENT
WILL
BE
NEGATIVE
BUT
WE
DON
T
WANT
TO
CHOOSE
THE
STEP
SIZE
Η
TOO
SMALL
OR
WE
WILL
MOVE
VERY
SLOWLY
AND
MAY
NOT
REACH
THE
MINIMUM
SO
LET
US
PICK
Η
TO
MINIMIZE
Φ
Η
F
ΘK
ΗDK
THIS
IS
CALLED
LINE
MINIMIZATION
OR
LINE
SEARCH
THERE
ARE
VARIOUS
METHODS
FOR
SOLVING
THIS
OPTIMIZATION
PROBLEM
SEE
NOCEDAL
AND
WRIGHT
FOR
DETAILS
FIGURE
A
DEMONSTRATES
THAT
LINE
SEARCH
DOES
INDEED
WORK
FOR
OUR
SIMPLE
PROBLEM
HOWEVER
WE
SEE
THAT
THE
STEEPEST
DESCENT
PATH
WITH
EXACT
LINE
SEARCHES
EXHIBITS
A
CHARACTERISTIC
ZIG
ZAG
BEHAVIOR
TO
SEE
WHY
NOTE
THAT
AN
EXACT
LINE
SEARCH
SATISFIES
ΗK
ARG
MINΗ
Φ
Η
A
NECESSARY
CONDITION
FOR
THE
OPTIMUM
IS
ΦT
Η
BY
THE
CHAIN
RULE
ΦT
Η
DT
G
WHERE
G
F
T
Θ
ΗD
IS
THE
GRADIENT
AT
THE
END
OF
THE
STEP
SO
WE
EITHER
HAVE
G
WHICH
MEANS
WE
HAVE
FOUND
A
STATIONARY
POINT
OR
G
D
WHICH
MEANS
THAT
EXACT
SEARCH
STOPS
AT
A
POINT
WHERE
THE
LOCAL
GRADIENT
IS
PERPENDICULAR
TO
THE
SEARCH
DIRECTION
HENCE
CONSECUTIVE
DIRECTIONS
WILL
BE
ORTHOGONAL
SEE
FIGURE
B
THIS
EXPLAINS
THE
ZIG
ZAG
BEHAVIOR
ONE
SIMPLE
HEURISTIC
TO
REDUCE
THE
EFFECT
OF
ZIG
ZAGGING
IS
TO
ADD
A
MOMENTUM
TERM
ΘK
ΘK
AS
FOLLOWS
ΘK
ΘK
ΗKGK
ΜK
ΘK
ΘK
WHERE
ΜK
CONTROLS
THE
IMPORTANCE
OF
THE
MOMENTUM
TERM
IN
THE
OPTIMIZATION
COMMUNITY
THIS
IS
KNOWN
AS
THE
HEAVY
BALL
METHOD
SEE
E
G
BERTSEKAS
AN
ALTERNATIVE
WAY
TO
MINIMIZE
ZIG
ZAGGING
IS
TO
USE
THE
METHOD
OF
CONJUGATE
GRADIENTS
SEE
E
G
NOCEDAL
AND
WRIGHT
CH
OR
GOLUB
AND
VAN
LOAN
SEC
THIS
IS
THE
METHOD
OF
CHOICE
FOR
QUADRATIC
OBJECTIVES
OF
THE
FORM
F
Θ
ΘT
AΘ
WHICH
ARISE
WHEN
SOLVING
LINEAR
SYSTEMS
HOWEVER
NON
LINEAR
CG
IS
LESS
POPULAR
NEWTON
METHOD
ALGORITHM
NEWTON
METHOD
FOR
MINIMIZING
A
STRICTLY
CONVEX
FUNCTION
INITIALIZE
FOR
K
UNTIL
CONVERGENCE
DO
EVALUATE
GK
F
ΘK
EVALUATE
HK
ΘK
SOLVE
HKDK
GK
FOR
DK
USE
LINE
SEARCH
TO
FIND
STEPSIZE
ΗK
ALONG
DK
ΘK
ΘK
ΗKDK
ONE
CAN
DERIVE
FASTER
OPTIMIZATION
METHODS
BY
TAKING
THE
CURVATURE
OF
THE
SPACE
I
E
THE
HESSIAN
INTO
ACCOUNT
THESE
ARE
CALLED
SECOND
ORDER
OPTIMIZATION
METODS
THE
PRIMARY
EXAMPLE
IS
NEWTON
ALGORITHM
THIS
IS
AN
ITERATIVE
ALGORITHM
WHICH
CONSISTS
OF
UPDATES
OF
THE
FORM
ΘK
ΘK
ΗKH
THE
FULL
PSEUDO
CODE
IS
GIVEN
IN
ALGORITHM
THIS
ALGORITHM
CAN
BE
DERIVED
AS
FOLLOWS
CONSIDER
MAKING
A
SECOND
ORDER
TAYLOR
SERIES
APPROXIMATION
OF
F
Θ
AROUND
ΘK
FQUAD
Θ
FK
GT
Θ
ΘK
Θ
ΘK
T
HK
Θ
ΘK
LET
US
REWRITE
THIS
AS
FQUAD
Θ
ΘT
AΘ
BT
Θ
C
WHERE
A
H
B
G
H
Θ
T
C
F
THE
MINIMUM
OF
FQUAD
IS
AT
Θ
A
Θ
K
H
THUS
THE
NEWTON
STEP
DK
H
IS
WHAT
SHOULD
BE
ADDED
TO
ΘK
TO
MINIMIZE
THE
SECOND
ORDER
APPROXIMATION
OF
F
AROUND
ΘK
SEE
FIGURE
A
FOR
AN
ILLUSTRATION
XK
XK
DK
A
B
XK
XK
DK
FIGURE
ILLUSTRATION
OF
NEWTON
METHOD
FOR
MINIMIZING
A
FUNCTION
A
THE
SOLID
CURVE
IS
THE
FUNCTION
F
X
THE
DOTTED
LINE
FQUAD
X
IS
ITS
SECOND
ORDER
APPROXIMATION
AT
XK
THE
NEWTON
STEP
DK
IS
WHAT
MUST
BE
ADDED
TO
XK
TO
GET
TO
THE
MINIMUM
OF
FQUAD
X
BASED
ON
FIGURE
OF
VANDENBERGHE
FIGURE
GENERATED
BY
NEWTONSMETHODMINQUAD
B
ILLUSTRATION
OF
NEWTON
METHOD
APPLIED
TO
A
NONCONVEX
FUNCTION
WE
FIT
A
QUADRATIC
AROUND
THE
CURRENT
POINT
XK
AND
MOVE
TO
ITS
STATIONARY
POINT
XK
XK
DK
UNFORTUNATELY
THIS
IS
A
LOCAL
MAXIMUM
NOT
MINIMUM
THIS
MEANS
WE
NEED
TO
BE
CAREFUL
ABOUT
THE
EXTENT
OF
OUR
QUADRATIC
APPROXIMATION
BASED
ON
FIGURE
OF
VANDENBERGHE
FIGURE
GENERATED
BY
NEWTONSMETHODNONCONVEX
IN
ITS
SIMPLEST
FORM
AS
LISTED
NEWTON
METHOD
REQUIRES
THAT
HK
BE
POSITIVE
DEFINITE
WHICH
WILL
HOLD
IF
THE
FUNCTION
IS
STRICTLY
CONVEX
IF
NOT
THE
OBJECTIVE
FUNCTION
IS
NOT
CONVEX
THEN
HK
MAY
NOT
BE
POSITIVE
DEFINITE
SO
DK
H
MAY
NOT
BE
A
DESCENT
DIRECTION
SEE
FIGURE
B
FOR
AN
EXAMPLE
IN
THIS
CASE
ONE
SIMPLE
STRATEGY
IS
TO
REVERT
TO
STEEPEST
DESCENT
DK
GK
THE
LEVENBERG
MARQUARDT
ALGORITHM
IS
AN
ADAPTIVE
WAY
TO
BLEND
BETWEEN
NEWTON
STEPS
AND
STEEPEST
DESCENT
STEPS
THIS
METHOD
IS
WIDELY
USED
WHEN
SOLVING
NONLINEAR
LEAST
SQUARES
PROBLEMS
AN
ALTERNATIVE
APPROACH
IS
THIS
RATHER
THAN
COMPUTING
DK
H
DIRECTLY
WE
CAN
SOLVE
THE
LINEAR
SYSTEM
OF
EQUATIONS
HKDK
GK
FOR
DK
USING
CONJUGATE
GRADIENT
CG
IF
HK
IS
NOT
POSITIVE
DEFINITE
WE
CAN
SIMPLY
TRUNCATE
THE
CG
ITERATIONS
AS
SOON
AS
NEGATIVE
CURVATURE
IS
DETECTED
THIS
IS
CALLED
TRUNCATED
NEWTON
ITERATIVELY
REWEIGHTED
LEAST
SQUARES
IRLS
LET
US
NOW
APPLY
NEWTON
ALGORITHM
TO
FIND
THE
MLE
FOR
BINARY
LOGISTIC
REGRESSION
THE
NEWTON
UPDATE
AT
ITERATION
K
FOR
THIS
MODEL
IS
AS
FOLLOWS
USING
ΗK
SINCE
THE
HESSIAN
IS
EXACT
WK
WK
H
WK
XT
SKX
Y
ΜK
X
SKX
X
SKX
WK
X
Y
ΜK
XT
SKX
SKXWK
Y
ΜK
XT
SKX
SKZK
WHERE
WE
HAVE
DEFINED
THE
WORKING
RESPONSE
AS
ZK
XWK
Y
ΜK
EQUATION
IS
AN
EXAMPLE
OF
A
WEIGHTED
LEAST
SQUARES
PROBLEM
WHICH
IS
A
MINIMIZER
OF
N
SKI
ZKI
WT
XI
I
SINCE
SK
IS
A
DIAGONAL
MATRIX
WE
CAN
REWRITE
THE
TARGETS
IN
COMPONENT
FORM
FOR
EACH
CASE
I
N
AS
ZKI
WT
XI
YI
ΜKI
ΜKI
ΜKI
THIS
ALGORITHM
IS
KNOWN
AS
ITERATIVELY
REWEIGHTED
LEAST
SQUARES
OR
IRLS
FOR
SHORT
SINCE
AT
EACH
ITERATION
WE
SOLVE
A
WEIGHTED
LEAST
SQUARES
PROBLEM
WHERE
THE
WEIGHT
MATRIX
SK
CHANGES
AT
EACH
ITERATION
SEE
ALGORITHM
FOR
SOME
PSEUDOCODE
ALGORITHM
ITERATIVELY
REWEIGHTED
LEAST
SQUARES
IRLS
W
LOG
Y
Y
REPEAT
ΗI
WT
XI
ΜI
SIGM
ΗI
SI
ΜI
ΜI
ZI
ΗI
YI
ΜI
DIAG
N
W
XT
SX
SZ
UNTIL
CONVERGED
QUASI
NEWTON
VARIABLE
METRIC
METHODS
THE
MOTHER
OF
ALL
SECOND
ORDER
OPTIMIZATION
ALGORITHM
IS
NEWTON
ALGORITHM
WHICH
WE
DIS
CUSSED
IN
SECTION
UNFORTUNATELY
IT
MAY
BE
TOO
EXPENSIVE
TO
COMPUTE
H
EXPLICITLY
QUASI
NEWTON
METHODS
ITERATIVELY
BUILD
UP
AN
APPROXIMATION
TO
THE
HESSIAN
USING
INFORMATION
GLEANED
FROM
THE
GRADIENT
VECTOR
AT
EACH
STEP
THE
MOST
COMMON
METHOD
IS
CALLED
BFGS
NAMED
AFTER
ITS
INVENTORS
BROYDEN
FLETCHER
GOLDFARB
AND
SHANNO
WHICH
UPDATES
THE
APPROXIMATION
TO
THE
HESSIAN
BK
HK
AS
FOLLOWS
Y
YT
BK
BK
K
YT
SK
BKSK
BKSK
T
ST
BKSK
SK
ΘK
ΘK
YK
GK
GK
THIS
IS
A
RANK
TWO
UPDATE
TO
THE
MATRIX
AND
ENSURES
THAT
THE
MATRIX
REMAINS
POSITIVE
DEFINITE
UNDER
CERTAIN
RESTRICTIONS
ON
THE
STEP
SIZE
WE
TYPICALLY
START
WITH
A
DIAGONAL
APPROXIMATION
I
THUS
BFGS
CAN
BE
THOUGHT
OF
AS
A
DIAGONAL
PLUS
LOW
RANK
APPROXIMATION
TO
THE
HESSIAN
ALTERNATIVELY
BFGS
CAN
ITERATIVELY
UPDATE
AN
APPROXIMATION
TO
THE
INVERSE
HESSIAN
CK
H
AS
FOLLOWS
CK
I
T
K
YT
SK
CK
I
T
K
YT
SK
SKST
YT
SK
SINCE
STORING
THE
HESSIAN
TAKES
O
SPACE
FOR
VERY
LARGE
PROBLEMS
ONE
CAN
USE
LIMITED
MEMORY
BFGS
OR
L
BFGS
WHERE
HK
OR
H
IS
APPROXIMATED
BY
A
DIAGONAL
PLUS
LOW
RANK
MATRIX
IN
PARTICULAR
THE
PRODUCT
H
CAN
BE
OBTAINED
BY
PERFORMING
A
SEQUENCE
OF
INNER
PRODUCTS
WITH
SK
AND
YK
USING
ONLY
THE
M
MOST
RECENT
SK
YK
PAIRS
AND
IGNORING
OLDER
INFORMATION
THE
STORAGE
REQUIREMENTS
ARE
THEREFORE
O
MD
TYPICALLY
M
20
SUFFICES
FOR
GOOD
PERFORMANCE
SEE
NOCEDAL
AND
WRIGHT
FOR
MORE
INFORMATION
L
BFGS
IS
OFTEN
THE
METHOD
OF
CHOICE
FOR
MOST
UNCONSTRAINED
SMOOTH
OPTIMIZATION
PROBLEMS
THAT
ARISE
IN
MACHINE
LEARNING
ALTHOUGH
SEE
SECTION
REGULARIZATION
JUST
AS
WE
PREFER
RIDGE
REGRESSION
TO
LINEAR
REGRESSION
SO
WE
SHOULD
PREFER
MAP
ESTIMATION
FOR
LOGISTIC
REGRESSION
TO
COMPUTING
THE
MLE
IN
FACT
REGULARIZATION
IS
IMPORTANT
IN
THE
CLASSIFICATION
SETTING
EVEN
IF
WE
HAVE
LOTS
OF
DATA
TO
SEE
WHY
SUPPOSE
THE
DATA
IS
LINEARLY
SEPARABLE
IN
THIS
CASE
THE
MLE
IS
OBTAINED
WHEN
W
CORRESPONDING
TO
AN
INFINITELY
STEEP
SIGMOID
FUNCTION
I
WT
X
ALSO
KNOWN
AS
A
LINEAR
THRESHOLD
UNIT
THIS
ASSIGNS
THE
MAXIMAL
AMOUNT
OF
PROBABILITY
MASS
TO
THE
TRAINING
DATA
HOWEVER
SUCH
A
SOLUTION
IS
VERY
BRITTLE
AND
WILL
NOT
GENERALIZE
WELL
TO
PREVENT
THIS
WE
CAN
USE
REGULARIZATION
JUST
AS
WE
DID
WITH
RIDGE
REGRESSION
WE
NOTE
THAT
THE
NEW
OBJECTIVE
GRADIENT
AND
HESSIAN
HAVE
THE
FOLLOWING
FORMS
F
T
W
NLL
W
ΛWT
W
GT
W
G
W
ΛW
HT
W
H
W
ΛI
IT
IS
A
SIMPLE
MATTER
TO
PASS
THESE
MODIFIED
EQUATIONS
INTO
ANY
GRADIENT
BASED
OPTIMIZER
MULTI
CLASS
LOGISTIC
REGRESSION
NOW
WE
CONSIDER
MULTINOMIAL
LOGISTIC
REGRESSION
SOMETIMES
CALLED
A
MAXIMUM
ENTROPY
CLASSIFIER
THIS
IS
A
MODEL
OF
THE
FORM
EXP
WT
X
P
Y
C
X
W
C
CT
C
EXP
WT
X
A
SLIGHT
VARIANT
KNOWN
AS
A
CONDITIONAL
LOGIT
MODEL
NORMALIZES
OVER
A
DIFFERENT
SET
OF
CLASSES
FOR
EACH
DATA
CASE
THIS
CAN
BE
USEFUL
FOR
MODELING
CHOICES
THAT
USERS
MAKE
BETWEEN
DIFFERENT
SETS
OF
ITEMS
THAT
ARE
OFFERED
TO
THEM
LET
US
NOW
INTRODUCE
SOME
NOTATION
LET
ΜIC
P
YI
C
XI
W
ΗI
C
WHERE
ΗI
WT
XI
IS
A
C
VECTOR
ALSO
LET
YIC
I
YI
C
BE
THE
ONE
OF
C
ENCODING
OF
YI
THUS
YI
IS
A
BIT
VECTOR
IN
WHICH
THE
C
TH
BIT
TURNS
ON
IFF
YI
C
FOLLOWING
KRISHNAPURAM
ET
AL
LET
US
SET
WC
TO
ENSURE
IDENTIFIABILITY
AND
DEFINE
W
VEC
W
C
TO
BE
A
D
C
COLUMN
VECTOR
WITH
THIS
THE
LOG
LIKELIHOOD
CAN
BE
WRITTEN
AS
N
C
N
C
W
LOG
TT
TT
ΜYIC
YIC
LOG
ΜIC
I
C
C
I
C
F
C
DEFINE
THE
NLL
AS
F
W
W
WE
NOW
PROCEED
TO
COMPUTE
THE
GRADIENT
AND
HESSIAN
OF
THIS
EXPRESSION
SINCE
W
IS
BLOCK
STRUCTURED
THE
NOTATION
GETS
A
BIT
HEAVY
BUT
THE
IDEAS
ARE
SIMPLE
IT
HELPS
TO
DEFINE
A
B
BE
THE
KRONECKER
PRODUCT
OF
MATRICES
A
AND
B
IF
A
IS
AN
M
N
MATRIX
AND
B
IS
A
P
Q
MATRIX
THEN
A
B
IS
THE
MP
NQ
BLOCK
MATRIX
AMNB
RETURNING
TO
THE
TASK
AT
HAND
ONE
CAN
SHOW
EXERCISE
THAT
THE
GRADIENT
IS
GIVEN
BY
N
G
W
F
W
ΜI
YI
XI
I
WHERE
YI
I
YI
I
YI
C
AND
ΜI
W
P
YI
XI
W
P
YI
C
XI
W
ARE
COLUMN
VECTORS
OF
LENGTH
C
FOR
EXAMPLE
IF
WE
HAVE
D
FEATURE
DIMENSIONS
AND
C
CLASSES
THIS
BECOMES
G
W
I
IN
OTHER
WORDS
FOR
EACH
CLASS
C
THE
DERIVATIVE
FOR
THE
WEIGHTS
IN
THE
C
TH
COLUMN
IS
WCF
W
ΜIC
YIC
XI
I
THIS
HAS
THE
SAME
FORM
AS
IN
THE
BINARY
LOGISTIC
REGRESSION
CASE
NAMELY
AN
ERROR
TERM
TIMES
XI
THIS
TURNS
OUT
TO
BE
A
GENERAL
PROPERTY
OF
DISTRIBUTIONS
IN
THE
EXPONENTIAL
FAMILY
AS
WE
WILL
SEE
IN
SECTION
ONE
CAN
ALSO
SHOW
EXERCISE
THAT
THE
HESSIAN
IS
THE
FOLLOWING
BLOCK
STRUCTURED
D
C
D
C
MATRIX
H
W
W
DIAG
ΜI
ΜIΜT
XIXT
FOR
EXAMPLE
IF
WE
HAVE
FEATURES
AND
CLASSES
THIS
BECOMES
XI
WHERE
XI
XIXT
IN
OTHER
WORDS
THE
BLOCK
C
CT
SUBMATRIX
IS
GIVEN
BY
HC
CT
W
ΜIC
ΔC
CT
ΜI
CT
XIXT
I
THIS
IS
ALSO
A
POSITIVE
DEFINITE
MATRIX
SO
THERE
IS
A
UNIQUE
MLE
NOW
CONSIDER
MINIMIZING
F
T
W
LOG
P
D
W
LOG
P
W
WHERE
P
W
TTC
N
WC
THE
NEW
OBJECTIVE
ITS
GRADIENT
AND
HESSIAN
ARE
GIVEN
BY
F
T
W
F
W
W
V
GT
W
G
W
WC
C
HT
W
H
W
IC
THIS
CAN
BE
PASSED
TO
ANY
GRADIENT
BASED
OPTIMIZER
TO
FIND
THE
MAP
ESTIMATE
NOTE
HOWEVER
THAT
THE
HESSIAN
HAS
SIZE
O
CD
CD
WHICH
IS
C
TIMES
MORE
ROW
AND
COLUMNS
THAN
IN
THE
BINARY
CASE
SO
LIMITED
MEMORY
BFGS
IS
MORE
APPROPRIATE
THAN
NEWTON
METHOD
SEE
LOGREGFIT
FOR
SOME
MATLAB
CODE
BAYESIAN
LOGISTIC
REGRESSION
IT
IS
NATURAL
TO
WANT
TO
COMPUTE
THE
FULL
POSTERIOR
OVER
THE
PARAMETERS
P
W
FOR
LOGISTIC
REGRESSION
MODELS
THIS
CAN
BE
USEFUL
FOR
ANY
SITUATION
WHERE
WE
WANT
TO
ASSOCIATE
CONFIDENCE
INTERVALS
WITH
OUR
PREDICTIONS
E
G
THIS
IS
NECESSARY
WHEN
SOLVING
CONTEXTUAL
BANDIT
PROBLEMS
DISCUSSED
IN
SECTION
UNFORTUNATELY
UNLIKE
THE
LINEAR
REGRESSION
CASE
THIS
CANNOT
BE
DONE
EXACTLY
SINCE
THERE
IS
NO
CONVENIENT
CONJUGATE
PRIOR
FOR
LOGISTIC
REGRESSION
WE
DISCUSS
ONE
SIMPLE
APPROXIMATION
BELOW
SOME
OTHER
APPROACHES
INCLUDE
MCMC
SECTION
VARIATIONAL
INFERENCE
SECTION
EXPECTATION
PROPAGATION
KUSS
AND
RASMUSSEN
ETC
FOR
NOTATIONAL
SIMPLICITY
WE
STICK
TO
BINARY
LOGISTIC
REGRESSION
LAPLACE
APPROXIMATION
IN
THIS
SECTION
WE
DISCUSS
HOW
TO
MAKE
A
GAUSSIAN
APPROXIMATION
TO
A
POSTERIOR
DISTRIBUTION
THE
APPROXIMATION
WORKS
AS
FOLLOWS
SUPPOSE
Θ
RD
LET
P
Θ
D
Z
E
E
Θ
WHERE
E
Θ
IS
CALLED
AN
ENERGY
FUNCTION
AND
IS
EQUAL
TO
THE
NEGATIVE
LOG
OF
THE
UNNORMAL
IZED
LOG
POSTERIOR
E
Θ
LOG
P
Θ
WITH
Z
P
BEING
THE
NORMALIZATION
CONSTANT
PERFORMING
A
TAYLOR
SERIES
EXPANSION
AROUND
THE
MODE
Θ
I
E
THE
LOWEST
ENERGY
STATE
WE
GET
E
Θ
E
Θ
Θ
Θ
T
G
Θ
Θ
T
H
Θ
Θ
50
WHERE
G
IS
THE
GRADIENT
AND
H
IS
THE
HESSIAN
OF
THE
ENERGY
FUNCTION
EVALUATED
AT
THE
MODE
G
E
Θ
H
Θ
Θ
ΘT
Θ
SINCE
Θ
IS
THE
MODE
THE
GRADIENT
TERM
IS
ZERO
HENCE
Pˆ
Θ
D
E
E
Θ
EXP
Θ
Θ
T
H
Θ
Θ
L
N
Θ
Θ
H
Z
P
D
Pˆ
Θ
D
DΘ
E
E
Θ
D
H
THE
LAST
LINE
FOLLOWS
FROM
NORMALIZATION
CONSTANT
OF
THE
MULTIVARIATE
GAUSSIAN
EQUATION
IS
KNOWN
AS
THE
LAPLACE
APPROXIMATION
TO
THE
MARGINAL
LIKELIHOOD
THEREFORE
EQUATION
IS
SOMETIMES
CALLED
THE
THE
LAPLACE
APPROXIMATION
TO
THE
POSTERIOR
HOWEVER
IN
THE
STATISTICS
COMMUNITY
THE
TERM
LAPLACE
APPROXIMATION
REFERS
TO
A
MORE
SOPHISTICATED
METHOD
SEE
E
G
RUE
ET
AL
FOR
DETAILS
IT
MAY
THEREFORE
BE
BETTER
TO
USE
THE
TERM
GAUSSIAN
APPROXIMATION
TO
REFER
TO
EQUATION
A
GAUSSIAN
APPROXIMATION
IS
OFTEN
A
REASONABLE
APPROXIMATION
SINCE
POSTERIORS
OFTEN
BECOME
MORE
GAUSSIAN
LIKE
AS
THE
SAMPLE
SIZE
INCREASES
FOR
REASONS
ANALOGOUS
TO
THE
CENTRAL
LIMIT
THEOREM
IN
PHYSICS
THERE
IS
AN
ANALOGOUS
TECHNIQUE
KNOWN
AS
A
SADDLE
POINT
APPROXIMATION
DERIVATION
OF
THE
BIC
WE
CAN
USE
THE
GAUSSIAN
APPROXIMATION
TO
WRITE
THE
LOG
MARGINAL
LIKELIHOOD
AS
FOLLOWS
DROPPING
IRRELEVANT
CONSTANTS
LOG
P
D
LOG
P
D
Θ
LOG
P
Θ
LOG
H
THE
PENALIZATION
TERMS
WHICH
ARE
ADDED
TO
THE
LOG
P
Θ
ARE
SOMETIMES
CALLED
THE
OCCAM
FACTOR
AND
ARE
A
MEASURE
OF
MODEL
COMPLEXITY
IF
WE
HAVE
A
UNIFORM
PRIOR
P
Θ
WE
CAN
DROP
THE
SECOND
TERM
AND
REPLACE
Θ
WITH
THE
MLE
Θˆ
WE
NOW
FOCUS
ON
APPROXIMATING
THE
THIRD
TERM
WE
HAVE
H
N
HI
WHERE
HI
LOG
P
DI
Θ
LET
US
APPROXIMATE
EACH
HI
BY
A
FIXED
MATRIX
Hˆ
THEN
WE
HAVE
LOG
H
LOG
N
Hˆ
LOG
N
D
Hˆ
D
LOG
N
LOG
Hˆ
WHERE
D
DIM
Θ
AND
WE
HAVE
ASSUMED
H
IS
FULL
RANK
WE
CAN
DROP
THE
LOG
Hˆ
TERM
SINCE
IT
IS
INDEPENDENT
OF
N
AND
THUS
WILL
GET
OVERWHELMED
BY
THE
LIKELIHOOD
PUTTING
ALL
THE
PIECES
TOGETHER
WE
RECOVER
THE
BIC
SCORE
SECTION
LOG
P
D
LOG
P
D
Θˆ
D
LOG
N
GAUSSIAN
APPROXIMATION
FOR
LOGISTIC
REGRESSION
NOW
LET
US
APPLY
THE
GAUSSIAN
APPROXIMATION
TO
LOGISTIC
REGRESSION
WE
WILL
USE
A
A
GAUSSIAN
PRIOR
OF
THE
FORM
P
W
W
JUST
AS
WE
DID
IN
MAP
ESTIMATION
THE
APPROXIMATE
POSTERIOR
IS
GIVEN
BY
P
W
D
N
W
Wˆ
H
WHERE
Wˆ
ARG
MINW
E
W
E
W
LOG
P
W
LOG
P
W
AND
H
W
Wˆ
AS
AN
EXAMPLE
CONSIDER
THE
LINEARLY
SEPARABLE
DATA
IN
FIGURE
A
THERE
ARE
MANY
PARAMETER
SETTINGS
THAT
CORRESPOND
TO
LINES
THAT
PERFECTLY
SEPARATE
THE
TRAINING
DATA
WE
SHOW
EXAMPLES
THE
LIKELIHOOD
SURFACE
IS
SHOWN
IN
FIGURE
B
WHERE
WE
SEE
THAT
THE
LIKELIHOOD
IS
UNBOUNDED
AS
WE
MOVE
UP
AND
TO
THE
RIGHT
IN
PARAMETER
SPACE
ALONG
A
RIDGE
WHERE
THIS
IS
INDICATED
BY
THE
DIAGONAL
LINE
THE
REASONS
FOR
THIS
IS
THAT
WE
CAN
MAXIMIZE
THE
LIKELIHOOD
BY
DRIVING
W
TO
INFINITY
SUBJECT
TO
BEING
ON
THIS
LINE
SINCE
LARGE
REGRESSION
WEIGHTS
MAKE
THE
SIGMOID
FUNCTION
VERY
STEEP
TURNING
IT
INTO
A
STEP
FUNCTION
CONSEQUENTLY
THE
MLE
IS
NOT
WELL
DEFINED
WHEN
THE
DATA
IS
LINEARLY
SEPARABLE
TO
REGULARIZE
THE
PROBLEM
LET
US
USE
A
VAGUE
SPHERICAL
PRIOR
CENTERED
AT
THE
ORIGIN
W
MULTIPLYING
THIS
SPHERICAL
PRIOR
BY
THE
LIKELIHOOD
SURFACE
RESULTS
IN
A
HIGHLY
SKEWED
POSTERIOR
SHOWN
IN
FIGURE
C
THE
POSTERIOR
IS
SKEWED
BECAUSE
THE
LIKELIHOOD
FUNCTION
CHOPS
OFF
REGIONS
OF
PARAMETER
SPACE
IN
A
SOFT
FASHION
WHICH
DISAGREE
WITH
THE
DATA
THE
MAP
ESTIMATE
IS
SHOWN
BY
THE
BLUE
DOT
UNLIKE
THE
MLE
THIS
IS
NOT
AT
INFINITY
THE
GAUSSIAN
APPROXIMATION
TO
THIS
POSTERIOR
IS
SHOWN
IN
FIGURE
D
WE
SEE
THAT
THIS
IS
A
SYMMETRIC
DISTRIBUTION
AND
THEREFORE
NOT
A
GREAT
APPROXIMATION
OF
COURSE
IT
GETS
THE
MODE
CORRECT
BY
CONSTRUCTION
AND
IT
AT
LEAST
REPRESENTS
THE
FACT
THAT
THERE
IS
MORE
UNCERTAINTY
ALONG
THE
SOUTHWEST
NORTHEAST
DIRECTION
WHICH
CORRESPONDS
TO
UNCERTAINTY
ABOUT
THE
ORIENTATION
OF
SEPARATING
LINES
THAN
PERPENDICULAR
TO
THIS
ALTHOUGH
A
CRUDE
APPROXIMATION
THIS
IS
SURELY
BETTER
THAN
APPROXIMATING
THE
POSTERIOR
BY
A
DELTA
FUNCTION
WHICH
IS
WHAT
MAP
ESTIMATION
DOES
APPROXIMATING
THE
POSTERIOR
PREDICTIVE
GIVEN
THE
POSTERIOR
WE
CAN
COMPUTE
CREDIBLE
INTERVALS
PERFORM
HYPOTHESIS
TESTS
ETC
JUST
AS
WE
DID
IN
SECTION
IN
THE
CASE
OF
LINEAR
REGRESSION
BUT
IN
MACHINE
LEARNING
INTEREST
USUALLY
FOCUSSES
ON
PREDICTION
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
HAS
THE
FORM
P
Y
X
D
P
Y
X
W
P
W
D
DW
DATA
LOG
LIKELIHOOD
A
B
LOG
UNNORMALISED
POSTERIOR
LAPLACE
APPROXIMATION
TO
POSTERIOR
C
D
FIGURE
A
TWO
CLASS
DATA
IN
B
LOG
LIKELIHOOD
FOR
A
LOGISTIC
REGRESSION
MODEL
THE
LINE
IS
DRAWN
FROM
THE
ORIGIN
IN
THE
DIRECTION
OF
THE
MLE
WHICH
IS
AT
INFINITY
THE
NUMBERS
CORRESPOND
TO
POINTS
IN
PARAMETER
SPACE
CORRESPONDING
TO
THE
LINES
IN
A
C
UNNORMALIZED
LOG
POSTERIOR
ASSUMING
VAGUE
SPHERICAL
PRIOR
D
LAPLACE
APPROXIMATION
TO
POSTERIOR
BASED
ON
A
FIGURE
BY
MARK
GIROLAMI
FIGURE
GENERATED
BY
LOGREGLAPLACEGIROLAMIDEMO
UNFORTUNATELY
THIS
INTEGRAL
IS
INTRACTABLE
THE
SIMPLEST
APPROXIMATION
IS
THE
PLUG
IN
APPROXIMATION
WHICH
IN
THE
BINARY
CASE
TAKES
THE
FORM
P
Y
X
D
P
Y
X
E
W
60
WHERE
E
W
IS
THE
POSTERIOR
MEAN
IN
THIS
CONTEXT
E
W
IS
CALLED
THE
BAYES
POINT
OF
COURSE
SUCH
A
PLUG
IN
ESTIMATE
UNDERESTIMATES
THE
UNCERTAINTY
WE
DISCUSS
SOME
BETTER
APPROXIMATIONS
BELOW
P
Y
X
WMAP
DECISION
BOUNDARY
FOR
SAMPLED
W
A
B
MC
APPROX
OF
P
Y
X
NUMERICAL
APPROX
OF
P
Y
X
C
D
FIGURE
POSTERIOR
PREDICTIVE
DISTRIBUTION
FOR
A
LOGISTIC
REGRESSION
MODEL
IN
TOP
LEFT
CONTOURS
OF
P
Y
X
Wˆ
MAP
TOP
RIGHT
SAMPLES
FROM
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
BOTTOM
LEFT
AVERAGING
OVER
THESE
SAMPLES
BOTTOM
RIGHT
MODERATED
OUTPUT
PROBIT
APPROXIMATION
BASED
ON
A
FIGURE
BY
MARK
GIROLAMI
FIGURE
GENERATED
BY
LOGREGLAPLACEGIROLAMIDEMO
MONTE
CARLO
APPROXIMATION
A
BETTER
APPROACH
IS
TO
USE
A
MONTE
CARLO
APPROXIMATION
AS
FOLLOWS
P
Y
X
SIGM
WS
T
X
WHERE
WS
P
W
ARE
SAMPLES
FROM
THE
POSTERIOR
THIS
TECHNIQUE
CAN
BE
TRIVIALLY
EXTENDED
TO
THE
MULTI
CLASS
CASE
IF
WE
HAVE
APPROXIMATED
THE
POSTERIOR
USING
MONTE
CARLO
WE
CAN
REUSE
THESE
SAMPLES
FOR
PREDICTION
IF
WE
MADE
A
GAUSSIAN
APPROXIMATION
TO
THE
POSTERIOR
WE
CAN
DRAW
INDEPENDENT
SAMPLES
FROM
THE
GAUSSIAN
USING
STANDARD
METHODS
FIGURE
B
SHOWS
SAMPLES
FROM
THE
POSTEIROR
PREDICTIVE
FOR
OUR
EXAMPLE
FIGURE
C
540
A
B
FIGURE
A
POSTERIOR
PREDICTIVE
DENSITY
FOR
SAT
DATA
THE
RED
CIRCLE
DENOTES
THE
POSTERIOR
MEAN
THE
BLUE
CROSS
THE
POSTERIOR
MEDIAN
AND
THE
BLUE
LINES
DENOTE
THE
AND
PERCENTILES
OF
THE
PREDICTIVE
DISTRIBUTION
FIGURE
GENERATED
BY
LOGREGSATDEMOBAYES
B
THE
LOGISTIC
SIGMOID
FUNCTION
SIGM
X
IN
SOLID
RED
WITH
THE
RESCALED
PROBIT
FUNCTION
Φ
ΛX
IN
DOTTED
BLUE
SUPERIMPOSED
HERE
Λ
Π
WHICH
WAS
CHOSEN
SO
THAT
THE
DERIVATIVES
OF
THE
TWO
CURVES
MATCH
AT
X
BASED
ON
FIGURE
OF
BISHOP
FIGURE
GENERATED
BY
PROBITPLOT
FIGURE
GENERATED
BY
PROBITREGDEMO
SHOWS
THE
AVERAGE
OF
THESE
SAMPLES
BY
AVERAGING
OVER
MULTIPLE
PREDICTIONS
WE
SEE
THAT
THE
UNCERTAINTY
IN
THE
DECISION
BOUNDARY
SPLAYS
OUT
AS
WE
MOVE
FURTHER
FROM
THE
TRAINING
DATA
SO
ALTHOUGH
THE
DECISION
BOUNDARY
IS
LINEAR
THE
POSTERIOR
PREDICTIVE
DENSITY
IS
NOT
LINEAR
NOTE
ALSO
THAT
THE
POSTERIOR
MEAN
DECISION
BOUNDARY
IS
ROUGHLY
EQUALLY
FAR
FROM
BOTH
CLASSES
THIS
IS
THE
BAYESIAN
ANALOG
OF
THE
LARGE
MARGIN
PRINCIPLE
DISCUSSED
IN
SECTION
FIGURE
A
SHOWS
AN
EXAMPLE
IN
THE
RED
DOTS
DENOTE
THE
MEAN
OF
THE
POSTERIOR
PREDICTIVE
EVALUATED
AT
THE
TRAINING
DATA
THE
VERTICAL
BLUE
LINES
DENOTE
CREDIBLE
INTERVALS
FOR
THE
POSTERIOR
PREDICTIVE
THE
SMALL
BLUE
STAR
IS
THE
MEDIAN
WE
SEE
THAT
WITH
THE
BAYESIAN
APPROACH
WE
ARE
ABLE
TO
MODEL
OUR
UNCERTAINTY
ABOUT
THE
PROBABILITY
A
STUDENT
WILL
PASS
THE
EXAM
BASED
ON
HIS
SAT
SCORE
RATHER
THAN
JUST
GETTING
A
POINT
ESTIMATE
PROBIT
APPROXIMATION
MODERATED
OUTPUT
IF
WE
HAVE
A
GAUSSIAN
APPROXIMATION
TO
THE
POSTERIOR
P
W
W
MN
VN
WE
CAN
ALSO
COMPUTE
A
DETERMINISTIC
APPROXIMATION
TO
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
AT
LEAST
IN
THE
BINARY
CASE
WE
PROCEED
AS
FOLLOWS
P
Y
X
D
SIGM
WT
X
P
W
D
DW
SIGM
A
N
A
ΜA
DA
A
WT
X
ΜA
E
A
MT
X
VAR
A
P
A
D
E
DA
P
W
D
WT
X
MT
X
DW
XT
VNX
THUS
WE
SEE
THAT
WE
NEED
TO
EVALUATE
THE
EXPECTATION
OF
A
SIGMOID
WITH
RESPECT
TO
A
GAUSSIAN
THIS
CAN
BE
APPROXIMATED
BY
EXPLOITING
THE
FACT
THAT
THE
SIGMOID
FUNCTION
IS
SIMILAR
TO
THE
PROBIT
FUNCTION
WHICH
IS
GIVEN
BY
THE
CDF
OF
THE
STANDARD
NORMAL
A
FIGURE
B
PLOTS
THE
SIGMOID
AND
PROBIT
FUNCTIONS
WE
HAVE
RESCALED
THE
AXES
SO
THAT
SIGM
A
HAS
THE
SAME
SLOPE
AS
Φ
ΛA
AT
THE
ORIGIN
WHERE
Π
THE
ADVANTAGE
OF
USING
THE
PROBIT
IS
THAT
ONE
CAN
CONVOLVE
IT
WITH
A
GAUSSIAN
ANALYTICALLY
Φ
ΛA
A
Μ
DA
Φ
A
Λ
WE
NOW
PLUG
IN
THE
APPROXIMATION
SIGM
A
Φ
ΛA
TO
BOTH
SIDES
OF
THIS
EQUATION
TO
GET
SIGM
A
N
A
Μ
DA
SIGM
Κ
Μ
Κ
APPLYING
THIS
TO
THE
LOGISTIC
REGRESSION
MODEL
WE
GET
THE
FOLLOWING
EXPRESSION
FIRST
SUGGESTED
IN
SPIEGELHALTER
AND
LAURITZEN
P
Y
X
D
SIGM
Κ
ΜA
FIGURE
D
INDICATES
THAT
THIS
GIVES
VERY
SIMILAR
RESULTS
TO
THE
MONTE
CARLO
APPROXIMATION
USING
EQUATION
IS
SOMETIMES
CALLED
A
MODERATED
OUTPUT
SINCE
IT
IS
LESS
EXTREME
THAN
THE
PLUG
IN
ESTIMATE
TO
SEE
THIS
NOTE
THAT
Κ
AND
HENCE
SIGM
Κ
Μ
SIGM
Μ
P
Y
X
Wˆ
WHERE
THE
INEQUALITY
IS
STRICT
IF
Μ
IF
Μ
WE
HAVE
P
Y
X
Wˆ
BUT
THE
MODERATED
PREDICTION
IS
ALWAYS
CLOSER
TO
SO
IT
IS
LESS
CONFIDENT
HOWEVER
THE
DECISION
BOUNDARY
OCCURS
WHENEVER
P
Y
X
SIGM
Κ
Μ
WHICH
IMPLIES
Μ
Wˆ
T
X
HENCE
THE
DECISION
BOUNDARY
FOR
THE
MODERATED
APPROXIMATION
IS
THE
SAME
AS
FOR
THE
PLUG
IN
APPROXIMATION
SO
THE
NUMBER
OF
MISCLASSIFICATIONS
WILL
BE
THE
SAME
FOR
THE
TWO
METHODS
BUT
THE
LOG
LIKELIHOOD
WILL
NOT
NOTE
THAT
IN
THE
MULTICLASS
CASE
TAKING
INTO
ACCOUNT
POSTERIOR
COVARIANCE
GIVES
DIFFERENT
ANSWERS
THAN
THE
PLUG
IN
APPROACH
SEE
EXERCISE
OF
RASMUSSEN
AND
WILLIAMS
RESIDUAL
ANALYSIS
OUTLIER
DETECTION
IT
IS
SOMETIMES
USEFUL
TO
DETECT
DATA
CASES
WHICH
ARE
OUTLIERS
THIS
IS
CALLED
RESIDUAL
ANALYSIS
OR
CASE
ANALYSIS
IN
A
REGRESSION
SETTING
THIS
CAN
BE
PERFORMED
BY
COMPUTING
RI
YI
YˆI
WHERE
YˆI
Wˆ
T
XI
THESE
VALUES
SHOULD
FOLLOW
A
DISTRIBUTION
IF
THE
MODELLING
ASSUMPTIONS
ARE
CORRECT
THIS
CAN
BE
ASSESSED
BY
CREATING
A
QQ
PLOT
WHERE
WE
PLOT
THE
N
THEORETICAL
QUANTILES
OF
A
GAUSSIAN
DISTRIBUTION
AGAINST
THE
N
EMPIRICAL
QUANTILES
OF
THE
RI
POINTS
THAT
DEVIATE
FROM
THE
STRAIGHTLINE
ARE
POTENTIAL
OUTLIERS
CLASSICAL
METHODS
BASED
ON
RESIDUALS
DO
NOT
WORK
WELL
FOR
BINARY
DATA
BECAUSE
THEY
RELY
ON
ASYMPTOTIC
NORMALITY
OF
THE
TEST
STATISTICS
HOWEVER
ADOPTING
A
BAYESIAN
APPROACH
WE
CAN
JUST
DEFINE
OUTLIERS
TO
BE
POINTS
WHICH
WHICH
P
YI
YˆI
IS
SMALL
WHERE
WE
TYPICALLY
USE
YˆI
SIGM
Wˆ
T
XI
NOTE
THAT
Wˆ
WAS
ESTIMATED
FROM
ALL
THE
DATA
A
BETTER
METHOD
IS
TO
EXCLUDE
XI
YI
FROM
THE
ESTIMATE
OF
W
WHEN
PREDICTING
YI
THAT
IS
WE
DEFINE
OUTLIERS
TO
BE
POINTS
WHICH
HAVE
LOW
PROBABILITY
UNDER
THE
CROSS
VALIDATED
POSTERIOR
PREDICTIVE
DISTRIBUTION
DEFINED
BY
P
YI
XI
X
I
Y
I
P
YI
XI
W
P
YIT
XIT
W
P
W
DW
IT
I
THIS
CAN
BE
EFFICIENTLY
APPROXIMATED
BY
SAMPLING
METHODS
GELFAND
FOR
FURTHER
DISCUSSION
OF
RESIDUAL
ANALYSIS
IN
LOGISTIC
REGRESSION
MODELS
SEE
E
G
JOHNSON
AND
ALBERT
SEC
ONLINE
LEARNING
AND
STOCHASTIC
OPTIMIZATION
TRADITIONALLY
MACHINE
LEARNING
IS
PERFORMED
OFFLINE
WHICH
MEANS
WE
HAVE
A
BATCH
OF
DATA
AND
WE
OPTIMIZE
AN
EQUATION
OF
THE
FOLLOWING
FORM
F
Θ
F
Θ
Z
WHERE
ZI
XI
YI
IN
THE
SUPERVISED
CASE
OR
JUST
XI
IN
THE
UNSUPERVISED
CASE
AND
F
Θ
ZI
IS
SOME
KIND
OF
LOSS
FUNCTION
FOR
EXAMPLE
WE
MIGHT
USE
F
Θ
ZI
LOG
P
YI
XI
Θ
IN
WHICH
CASE
WE
ARE
TRYING
TO
MAXIMIZE
THE
LIKELIHOOD
ALTERNATIVELY
WE
MIGHT
USE
F
Θ
ZI
L
YI
H
XI
Θ
WHERE
H
XI
Θ
IS
A
PREDICTION
FUNCTION
AND
L
Y
Yˆ
IS
SOME
OTHER
LOSS
FUNCTION
SUCH
AS
SQUARED
ERROR
OR
THE
HUBER
LOSS
IN
FREQUENTIST
DECISION
THEORY
THE
AVERAGE
LOSS
IS
CALLED
THE
RISK
SEE
SECTION
SO
THIS
OVERALL
APPROACH
IS
CALLED
EMPIRICAL
RISK
MINIMIZATION
OR
ERM
SEE
SECTION
FOR
DETAILS
HOWEVER
IF
WE
HAVE
STREAMING
DATA
WE
NEED
TO
PERFORM
ONLINE
LEARNING
SO
WE
CAN
UPDATE
OUR
ESTIMATES
AS
EACH
NEW
DATA
POINT
ARRIVES
RATHER
THAN
WAITING
UNTIL
THE
END
WHICH
MAY
NEVER
OCCUR
AND
EVEN
IF
WE
HAVE
A
BATCH
OF
DATA
WE
MIGHT
WANT
TO
TREAT
IT
LIKE
A
STREAM
IF
IT
IS
TOO
LARGE
TO
HOLD
IN
MAIN
MEMORY
BELOW
WE
DISCUSS
LEARNING
METHODS
FOR
THIS
KIND
OF
SCENARIO
A
SIMPLE
IMPLEMENTATION
TRICK
CAN
BE
USED
TO
SPEED
UP
BATCH
LEARNING
ALGORITHMS
WHEN
APPLIED
TO
DATA
SETS
THAT
ARE
TOO
LARGE
TO
HOLD
IN
MEMORY
FIRST
NOTE
THAT
THE
NAIVE
IMPLEMENTATION
MAKES
A
PASS
OVER
THE
DATA
FILE
FROM
THE
BEGINNING
TO
END
ACCUMULATING
THE
SUFFICIENT
STATISTICS
AND
GRADIENTS
AS
IT
GOES
THEN
AN
UPDATE
IS
PERFORMED
AND
THE
PROCESS
REPEATS
UNFORTUNATELY
AT
THE
END
OF
EACH
PASS
THE
DATA
FROM
THE
BEGINNING
OF
THE
FILE
WILL
HAVE
BEEN
EVICTED
FROM
THE
CACHE
SINCE
ARE
ARE
ASSUMING
IT
CANNOT
ALL
FIT
INTO
MEMORY
RATHER
THAN
GOING
BACK
TO
THE
BEGINNING
OF
THE
FILE
AND
RELOADING
IT
WE
CAN
SIMPLY
WORK
BACKWARDS
FROM
THE
END
OF
THE
FILE
WHICH
IS
ALREADY
IN
MEMORY
WE
THEN
REPEAT
THIS
FORWARDS
BACKWARDS
PATTERN
OVER
THE
DATA
THIS
SIMPLE
TRICK
IS
KNOWN
AS
ROCKING
ONLINE
LEARNING
AND
REGRET
MINIMIZATION
SUPPOSE
THAT
AT
EACH
STEP
NATURE
PRESENTS
A
SAMPLE
ZK
AND
THE
LEARNER
MUST
RESPOND
WITH
A
PARAMETER
ESTIMATE
ΘK
IN
THE
THEORETICAL
MACHINE
LEARNING
COMMUNITY
THE
OBJECTIVE
USED
IN
ONLINE
LEARNING
IS
THE
REGRET
WHICH
IS
THE
AVERAGED
LOSS
INCURRED
RELATIVE
TO
THE
BEST
WE
COULD
HAVE
GOTTEN
IN
HINDSIGHT
USING
A
SINGLE
FIXED
PARAMETER
VALUE
K
K
REGRET
F
Θ
Z
MIN
F
Θ
Z
FOR
EXAMPLE
IMAGINE
WE
ARE
INVESTING
IN
THE
STOCK
MARKET
LET
ΘJ
BE
THE
AMOUNT
WE
INVEST
IN
STOCK
J
AND
LET
ZJ
BE
THE
RETURN
ON
THIS
STOCK
OUR
LOSS
FUNCTION
IS
F
Θ
Z
ΘT
Z
THE
REGRET
IS
HOW
MUCH
BETTER
OR
WORSE
WE
DID
BY
TRADING
AT
EACH
STEP
RATHER
THAN
ADOPTING
A
BUY
AND
HOLD
STRATEGY
USING
AN
ORACLE
TO
CHOOSE
WHICH
STOCKS
TO
BUY
ONE
SIMPLE
ALGORITHM
FOR
ONLINE
LEARNING
IS
ONLINE
GRADIENT
DESCENT
ZINKEVICH
WHICH
IS
AS
FOLLOWS
AT
EACH
STEP
K
UPDATE
THE
PARAMETERS
USING
ΘK
PROJΘ
ΘK
ΗKGK
WHERE
PROJV
V
ARGMINW
V
W
V
IS
THE
PROJECTION
OF
VECTOR
V
ONTO
SPACE
GK
F
ΘK
ZK
IS
THE
GRADIENT
AND
ΗK
IS
THE
STEP
SIZE
THE
PROJECTION
STEP
IS
ONLY
NEEDED
IF
THE
PARAMETER
MUST
BE
CONSTRAINED
TO
LIVE
IN
A
CERTAIN
SUBSET
OF
RD
SEE
SECTION
FOR
DETAILS
BELOW
WE
WILL
SEE
HOW
THIS
APPROACH
TO
REGRET
MINIMIZATION
RELATES
TO
MORE
TRADITIONAL
OBJECTIVES
SUCH
AS
MLE
THERE
ARE
A
VARIETY
OF
OTHER
APPROACHES
TO
REGRET
MINIMIZATION
WHICH
ARE
BEYOND
THE
SCOPE
OF
THIS
BOOK
SEE
E
G
CESA
BIANCHI
AND
LUGOSI
FOR
DETAILS
STOCHASTIC
OPTIMIZATION
AND
RISK
MINIMIZATION
NOW
SUPPOSE
THAT
INSTEAD
OF
MINIMIZING
REGRET
WITH
RESPECT
TO
THE
PAST
WE
WANT
TO
MINIMIZE
EXPECTED
LOSS
IN
THE
FUTURE
AS
IS
MORE
COMMON
IN
FREQUENTIST
STATISTICAL
LEARNING
THEORY
THAT
IS
WE
WANT
TO
MINIMIZE
F
Θ
E
F
Θ
Z
WHERE
THE
EXPECTATION
IS
TAKEN
OVER
FUTURE
DATA
OPTIMIZING
FUNCTIONS
WHERE
SOME
OF
THE
VARIABLES
IN
THE
OBJECTIVE
ARE
RANDOM
IS
CALLED
STOCHASTIC
OPTIMIZATION
SUPPOSE
WE
RECEIVE
AN
INFINITE
STREAM
OF
SAMPLES
FROM
THE
DISTRIBUTION
ONE
WAY
TO
OPTIMIZE
STOCHASTIC
OBJECTIVES
SUCH
AS
EQUATION
IS
TO
PERFORM
THE
UPDATE
IN
EQUATION
AT
EACH
STEP
THIS
IS
CALLED
STOCHASTIC
GRADIENT
DESCENT
OR
SGD
NEMIROVSKI
AND
YUDIN
SINCE
WE
TYPICALLY
WANT
A
SINGLE
PARAMETER
ESTIMATE
WE
CAN
USE
A
RUNNING
AVERAGE
K
Θ
Θ
K
K
T
T
NOTE
THAT
IN
STOCHASTIC
OPTIMIZATION
THE
OBJECTIVE
IS
STOCHASTIC
AND
THEREFORE
THE
ALGORITHMS
WILL
BE
TOO
HOWEVER
IT
IS
ALSO
POSSIBLE
TO
APPLY
STOCHASTIC
OPTIMIZATION
ALGORITHMS
TO
DETERMINISTIC
OBJECTIVES
EXAMPLES
INCLUDE
SIMULATED
ANNEALING
SECTION
AND
STOCHASTIC
GRADIENT
DESCENT
APPLIED
TO
THE
EMPIRICAL
RISK
MINIMIZATION
PROBLEM
THERE
ARE
SOME
INTERESTING
THEORETICAL
CONNECTIONS
BETWEEN
ONLINE
LEARNING
AND
STOCHASTIC
OPTIMIZATION
CESA
BIANCHI
AND
LUGOSI
BUT
THIS
IS
BEYOND
THE
SCOPE
OF
THIS
BOOK
THIS
IS
CALLED
POLYAK
RUPPERT
AVERAGING
AND
CAN
BE
IMPLEMENTED
RECURSIVELY
AS
FOLLOWS
ΘK
ΘK
K
ΘK
ΘK
SEE
E
G
SPALL
KUSHNER
AND
YIN
FOR
DETAILS
SETTING
THE
STEP
SIZE
WE
NOW
DISCUSS
SOME
SUFFICIENT
CONDITIONS
ON
THE
LEARNING
RATE
TO
GUARANTEE
CONVERGENCE
OF
SGD
THESE
ARE
KNOWN
AS
THE
ROBBINS
MONRO
CONDITIONS
ΗK
K
K
THE
SET
OF
VALUES
OF
ΗK
OVER
TIME
IS
CALLED
THE
LEARNING
RATE
SCHEDULE
VARIOUS
FORMULAS
ARE
USED
SUCH
AS
ΗK
K
OR
THE
FOLLOWING
BOTTOU
BACH
AND
MOULINES
ΗK
K
Κ
WHERE
SLOWS
DOWN
EARLY
ITERATIONS
OF
THE
ALGORITHM
AND
Κ
CONTROLS
THE
RATE
AT
WHICH
OLD
VALUES
OF
ARE
FORGOTTEN
THE
NEED
TO
ADJUST
THESE
TUNING
PARAMETERS
IS
ONE
OF
THE
MAIN
DRAWBACK
OF
STOCHASTIC
OPTIMIZATION
ONE
SIMPLE
HEURISTIC
BOTTOU
IS
AS
FOLLOWS
STORE
AN
INITIAL
SUBSET
OF
THE
DATA
AND
TRY
A
RANGE
OF
Η
VALUES
ON
THIS
SUBSET
THEN
CHOOSE
THE
ONE
THAT
RESULTS
IN
THE
FASTEST
DECREASE
IN
THE
OBJECTIVE
AND
APPLY
IT
TO
ALL
THE
REST
OF
THE
DATA
NOTE
THAT
THIS
MAY
NOT
RESULT
IN
CONVERGENCE
BUT
THE
ALGORITHM
CAN
BE
TERMINATED
WHEN
THE
PERFORMANCE
IMPROVEMENT
ON
A
HOLD
OUT
SET
PLATEAUS
THIS
IS
CALLED
EARLY
STOPPING
PER
PARAMETER
STEP
SIZES
ONE
DRAWBACK
OF
SGD
IS
THAT
IT
USES
THE
SAME
STEP
SIZE
FOR
ALL
PARAMETERS
WE
NOW
BRIEFLY
PRESENT
A
METHOD
KNOWN
AS
ADAGRAD
SHORT
FOR
ADAPTIVE
GRADIENT
DUCHI
ET
AL
WHICH
IS
SIMILAR
IN
SPIRIT
TO
A
DIAGONAL
HESSIAN
APPROXIMATION
SEE
ALSO
SCHAUL
ET
AL
FOR
A
SIMILAR
APPROACH
IN
PARTICULAR
IF
ΘI
K
IS
PARAMETER
I
AT
TIME
K
AND
GI
K
IS
ITS
GRADIENT
THEN
WE
MAKE
AN
UPDATE
AS
FOLLOWS
GI
K
ΘI
K
ΘI
K
Η
Τ
SI
K
WHERE
THE
DIAGONAL
STEP
SIZE
VECTOR
IS
THE
GRADIENT
VECTOR
SQUARED
SUMMED
OVER
ALL
TIME
STEPS
THIS
CAN
BE
RECURSIVELY
UPDATED
AS
FOLLOWS
SI
K
SI
K
GI
K
THE
RESULT
IS
A
PER
PARAMETER
STEP
SIZE
THAT
ADAPTS
TO
THE
CURVATURE
OF
THE
LOSS
FUNCTION
THIS
METHOD
WAS
ORIGINAL
DERIVED
FOR
THE
REGRET
MINIMIZATION
CASE
BUT
IT
CAN
BE
APPLIED
MORE
GENERALLY
SGD
COMPARED
TO
BATCH
LEARNING
IF
WE
DON
T
HAVE
AN
INFINITE
DATA
STREAM
WE
CAN
SIMULATE
ONE
BY
SAMPLING
DATA
POINTS
AT
RANDOM
FROM
OUR
TRAINING
SET
ESSENTIALLY
WE
ARE
OPTIMIZING
EQUATION
BY
TREATING
IT
AS
AN
EXPECTATION
WITH
RESPECT
TO
THE
EMPIRICAL
DISTRIBUTION
ALGORITHM
STOCHASTIC
GRADIENT
DESCENT
INITIALIZE
Θ
Η
REPEAT
RANDOMLY
PERMUTE
DATA
FOR
I
N
DO
G
F
Θ
ZI
Θ
PROJΘ
Θ
ΗG
UPDATE
Η
UNTIL
CONVERGED
IN
THEORY
WE
SHOULD
SAMPLE
WITH
REPLACEMENT
ALTHOUGH
IN
PRACTICE
IT
IS
USUALLY
BETTER
TO
RANDOMLY
PERMUTE
THE
DATA
AND
SAMPLE
WITHOUT
REPLACEMENT
AND
THEN
TO
REPEAT
A
SINGLE
SUCH
PASS
OVER
THE
ENTIRE
DATA
SET
IS
CALLED
AN
EPOCH
SEE
ALGORITHM
FOR
SOME
PSEUDOCODE
IN
THIS
OFFLINE
CASE
IT
IS
OFTEN
BETTER
TO
COMPUTE
THE
GRADIENT
OF
A
MINI
BATCH
OF
B
DATA
CASES
IF
B
THIS
IS
STANDARD
SGD
AND
IF
B
N
THIS
IS
STANDARD
STEEPEST
DESCENT
TYPICALLY
B
IS
USED
ALTHOUGH
A
SIMPLE
FIRST
ORDER
METHOD
SGD
PERFORMS
SURPRISINGLY
WELL
ON
SOME
PROBLEMS
ESPECIALLY
ONES
WITH
LARGE
DATA
SETS
BOTTOU
THE
INTUITIVE
REASON
FOR
THIS
IS
THAT
ONE
CAN
GET
A
FAIRLY
GOOD
ESTIMATE
OF
THE
GRADIENT
BY
LOOKING
AT
JUST
A
FEW
EXAMPLES
CAREFULLY
EVALUATING
PRECISE
GRADIENTS
USING
LARGE
DATASETS
IS
OFTEN
A
WASTE
OF
TIME
SINCE
THE
ALGORITHM
WILL
HAVE
TO
RECOMPUTE
THE
GRADIENT
AGAIN
ANYWAY
AT
THE
NEXT
STEP
IT
IS
OFTEN
A
BETTER
USE
OF
COMPUTER
TIME
TO
HAVE
A
NOISY
ESTIMATE
AND
TO
MOVE
RAPIDLY
THROUGH
PARAMETER
SPACE
AS
AN
EXTREME
EXAMPLE
SUPPOSE
WE
DOUBLE
THE
TRAINING
SET
BY
DUPLICATING
EVERY
EXAMPLE
BATCH
METHODS
WILL
TAKE
TWICE
AS
LONG
BUT
ONLINE
METHODS
WILL
BE
UNAFFECTED
SINCE
THE
DIRECTION
OF
THE
GRADIENT
HAS
NOT
CHANGED
DOUBLING
THE
SIZE
OF
THE
DATA
CHANGES
THE
MAGNITUDE
OF
THE
GRADIENT
BUT
THAT
IS
IRRELEVANT
SINCE
THE
GRADIENT
IS
BEING
SCALED
BY
THE
STEP
SIZE
ANYWAY
IN
ADDITION
TO
ENHANCED
SPEED
SGD
IS
OFTEN
LESS
PRONE
TO
GETTING
STUCK
IN
SHALLOW
LOCAL
MINIMA
BECAUSE
IT
ADDS
A
CERTAIN
AMOUNT
OF
NOISE
CONSEQUENTLY
IT
IS
QUITE
POPULAR
IN
THE
MACHINE
LEARNING
COMMUNITY
FOR
FITTING
MODELS
WITH
NON
CONVEX
OBJECTIVES
SUCH
AS
NEURAL
NETWORKS
SECTION
AND
DEEP
BELIEF
NETWORKS
SECTION
THE
LMS
ALGORITHM
AS
AN
EXAMPLE
OF
SGD
LET
US
CONSIDER
HOW
TO
COMPUTE
THE
MLE
FOR
LINEAR
REGRESSION
IN
AN
ONLINE
FASHION
WE
DERIVED
THE
BATCH
GRADIENT
IN
EQUATION
THE
ONLINE
GRADIENT
AT
ITERATION
K
IS
GIVEN
BY
GK
XI
ΘT
XI
YI
BLACK
LINE
LMS
TRAJECTORY
TOWARDS
LS
SOLN
RED
CROSS
RSS
VS
ITERATION
A
20
B
FIGURE
ILLUSTRATION
OF
THE
LMS
ALGORITHM
LEFT
WE
START
FROM
Θ
AND
SLOWLY
CONVERGING
TO
THE
LEAST
SQUARES
SOLUTION
OF
Θˆ
RED
CROSS
RIGHT
PLOT
OF
OBJECTIVE
FUNCTION
OVER
TIME
NOTE
THAT
IT
DOES
NOT
DECREASE
MONOTONICALLY
FIGURE
GENERATED
BY
LMSDEMO
WHERE
I
I
K
IS
THE
TRAINING
EXAMPLE
TO
USE
AT
ITERATION
K
IF
THE
DATA
SET
IS
STREAMING
WE
USE
I
K
K
WE
SHALL
ASSUME
THIS
FROM
NOW
ON
FOR
NOTATIONAL
SIMPLICITY
EQUATION
IS
EASY
TO
INTERPRET
IT
IS
THE
FEATURE
VECTOR
XK
WEIGHTED
BY
THE
DIFFERENCE
BETWEEN
WHAT
WE
PREDICTED
YˆK
ΘT
XK
AND
THE
TRUE
RESPONSE
YK
HENCE
THE
GRADIENT
ACTS
LIKE
AN
ERROR
SIGNAL
AFTER
COMPUTING
THE
GRADIENT
WE
TAKE
A
STEP
ALONG
IT
AS
FOLLOWS
ΘK
ΘK
ΗK
YˆK
YK
XK
THERE
IS
NO
NEED
FOR
A
PROJECTION
STEP
SINCE
THIS
IS
AN
UNCONSTRAINED
OPTIMIZATION
PROBLEM
THIS
ALGORITHM
IS
CALLED
THE
LEAST
MEAN
SQUARES
OR
LMS
ALGORITHM
AND
IS
ALSO
KNOWN
AS
THE
DELTA
RULE
OR
THE
WIDROW
HOFF
RULE
FIGURE
SHOWS
THE
RESULTS
OF
APPLYING
THIS
ALGORITHM
TO
THE
DATA
SHOWN
IN
FIGURE
WE
START
AT
Θ
AND
CONVERGE
IN
THE
SENSE
THAT
ΘK
ΘK
DROPS
BELOW
A
THRESHOLD
OF
IN
ABOUT
ITERATIONS
NOTE
THAT
LMS
MAY
REQUIRE
MULTIPLE
PASSES
THROUGH
THE
DATA
TO
FIND
THE
OPTIMUM
BY
CONTRAST
THE
RECURSIVE
LEAST
SQUARES
ALGORITHM
WHICH
IS
BASED
ON
THE
KALMAN
FILTER
AND
WHICH
USES
SECOND
ORDER
INFORMATION
FINDS
THE
OPTIMUM
IN
A
SINGLE
PASS
SEE
SECTION
SEE
ALSO
EXERCISE
THE
PERCEPTRON
ALGORITHM
NOW
LET
US
CONSIDER
HOW
TO
FIT
A
BINARY
LOGISTIC
REGRESSION
MODEL
IN
AN
ONLINE
MANNER
THE
BATCH
GRADIENT
WAS
GIVEN
IN
EQUATION
IN
THE
ONLINE
CASE
THE
WEIGHT
UPDATE
HAS
THE
SIMPLE
FORM
ΘK
ΘK
ΗKGI
ΘK
ΗK
ΜI
YI
XI
WHERE
ΜI
P
YI
XI
ΘK
E
YI
XI
ΘK
WE
SEE
THAT
THIS
HAS
EXACTLY
THE
SAME
FORM
AS
THE
LMS
ALGORITHM
INDEED
THIS
PROPERTY
HOLDS
FOR
ALL
GENERALIZED
LINEAR
MODELS
SECTION
WE
NOW
CONSIDER
AN
APPROXIMATION
TO
THIS
ALGORITHM
SPECIFICALLY
LET
YˆI
ARG
MAX
P
Y
XI
Θ
REPRESENT
THE
MOST
PROBABLE
CLASS
LABEL
WE
REPLACE
ΜI
P
Y
XI
Θ
SIGM
ΘT
XI
IN
THE
GRADIENT
EXPRESSION
WITH
YˆI
THUS
THE
APPROXIMATE
GRADIENT
BECOMES
GI
YˆI
YI
XI
IT
WILL
MAKE
THE
ALGEBRA
SIMPLER
IF
WE
ASSUME
Y
RATHER
THAN
Y
IN
THIS
CASE
OUR
PREDICTION
BECOMES
YˆI
SIGN
ΘT
XI
THEN
IF
YˆIYI
WE
HAVE
MADE
AN
ERROR
BUT
IF
YˆIYI
WE
GUESSED
THE
RIGHT
LABEL
AT
EACH
STEP
WE
UPDATE
THE
WEIGHT
VECTOR
BY
ADDING
ON
THE
GRADIENT
THE
KEY
OBSERVATION
IS
THAT
IF
WE
PREDICTED
CORRECTLY
THEN
YˆI
YI
SO
THE
APPROXIMATE
GRADIENT
IS
ZERO
AND
WE
DO
NOT
CHANGE
THE
WEIGHT
VECTOR
BUT
IF
XI
IS
MISCLASSIFIED
WE
UPDATE
THE
WEIGHTS
AS
FOLLOWS
IF
YˆI
BUT
YI
THEN
THE
NEGATIVE
GRADIENT
IS
YˆI
YI
XI
AND
IF
YˆI
BUT
YI
THEN
THE
NEGATIVE
GRADIENT
IS
YˆI
YI
XI
WE
CAN
ABSORB
THE
FACTOR
OF
INTO
THE
LEARNING
RATE
Η
AND
JUST
WRITE
THE
UPDATE
IN
THE
CASE
OF
A
MISCLASSIFICATION
AS
ΘK
ΘK
ΗKYIXI
SINCE
IT
IS
ONLY
THE
SIGN
OF
THE
WEIGHTS
THAT
MATTER
NOT
THE
MAGNITUDE
WE
WILL
SET
ΗK
SEE
ALGORITHM
FOR
THE
PSEUDOCODE
ONE
CAN
SHOW
THAT
THIS
METHOD
KNOWN
AS
THE
PERCEPTRON
ALGORITHM
ROSENBLATT
WILL
CONVERGE
PROVIDED
THE
DATA
IS
LINEARLY
SEPARABLE
I
E
THAT
THERE
EXIST
PARAMETERS
Θ
SUCH
THAT
PREDICTING
WITH
SIGN
ΘT
X
ACHIEVES
ERROR
ON
THE
TRAINING
SET
HOWEVER
IF
THE
DATA
IS
NOT
LINEARLY
SEPARABLE
THE
ALGORITHM
WILL
NOT
CONVERGE
AND
EVEN
IF
IT
DOES
CONVERGE
IT
MAY
TAKE
A
LONG
TIME
THERE
ARE
MUCH
BETTER
WAYS
TO
TRAIN
LOGISTIC
REGRESSION
MODELS
SUCH
AS
USING
PROPER
SGD
WITHOUT
THE
GRADIENT
APPROXIMATION
OR
IRLS
DISCUSSED
IN
SECTION
HOWEVER
THE
PERCEPTRON
ALGORITHM
IS
HISTORICALLY
IMPORTANT
IT
WAS
ONE
OF
THE
FIRST
MACHINE
LEARNING
ALGORITHMS
EVER
DERIVED
BY
FRANK
ROSENBLATT
IN
AND
WAS
EVEN
IMPLEMENTED
IN
ANALOG
HARDWARE
IN
ADDITION
THE
ALGORITHM
CAN
BE
USED
TO
FIT
MODELS
WHERE
COMPUTING
MARGINALS
P
YI
X
Θ
IS
MORE
EXPENSIVE
THAN
COMPUTING
THE
MAP
OUTPUT
ARG
MAXY
P
Y
X
Θ
THIS
ARISES
IN
SOME
STRUCTURED
OUTPUT
CLASSIFICATION
PROBLEMS
SEE
SECTION
FOR
DETAILS
A
BAYESIAN
VIEW
ANOTHER
APPROACH
TO
ONLINE
LEARNING
IS
TO
ADOPT
A
BAYESIAN
VIEW
THIS
IS
CONCEPTUALLY
QUITE
SIMPLE
WE
JUST
APPLY
BAYES
RULE
RECURSIVELY
P
Θ
K
P
DK
Θ
P
Θ
K
THIS
HAS
THE
OBVIOUS
ADVANTAGE
OF
RETURNING
A
POSTERIOR
INSTEAD
OF
JUST
A
POINT
ESTIMATE
IT
ALSO
ALLOWS
FOR
THE
ONLINE
ADAPTATION
OF
HYPER
PARAMETERS
WHICH
IS
IMPORTANT
SINCE
CROSS
VALIDATION
CANNOT
BE
USED
IN
AN
ONLINE
SETTING
FINALLY
IT
HAS
THE
LESS
OBVIOUS
ADVANTAGE
THAT
IT
CAN
BE
ALGORITHM
PERCEPTRON
ALGORITHM
INPUT
LINEARLY
SEPARABLE
DATA
SET
XI
RD
YI
FOR
I
N
INITIALIZE
K
REPEAT
K
K
I
K
MOD
N
IF
YˆI
YI
THEN
ΘK
ΘK
YIXI
ELSE
NO
OP
UNTIL
CONVERGED
QUICKER
THAN
SGD
TO
SEE
WHY
NOTE
THAT
BY
MODELING
THE
POSTERIOR
VARIANCE
OF
EACH
PARAMETER
IN
ADDITION
TO
ITS
MEAN
WE
EFFECTIVELY
ASSOCIATE
A
DIFFERENT
LEARNING
RATE
FOR
EACH
PARAMETER
DE
FREITAS
ET
AL
WHICH
IS
A
SIMPLE
WAY
TO
MODEL
THE
CURVATURE
OF
THE
SPACE
THESE
VARIANCES
CAN
THEN
BE
ADAPTED
USING
THE
USUAL
RULES
OF
PROBABILITY
THEORY
BY
CONTRAST
GETTING
SECOND
ORDER
OPTIMIZATION
METHODS
TO
WORK
ONLINE
IS
MORE
TRICKY
SEE
E
G
SCHRAUDOLPH
ET
AL
SUNEHAG
ET
AL
BORDES
ET
AL
AS
A
SIMPLE
EXAMPLE
IN
SECTION
WE
SHOW
HOW
TO
USE
THE
KALMAN
FILTER
TO
FIT
A
LINEAR
REGRESSION
MODEL
ONLINE
UNLIKE
THE
LMS
ALGORITHM
THIS
CONVERGES
TO
THE
OPTIMAL
OFFLINE
ANSWER
IN
A
SINGLE
PASS
OVER
THE
DATA
AN
EXTENSION
WHICH
CAN
LEARN
A
ROBUST
NON
LINEAR
REGRESSION
MODEL
IN
AN
ONLINE
FASHION
IS
DESCRIBED
IN
TING
ET
AL
FOR
THE
GLM
CASE
WE
CAN
USE
AN
ASSUMED
DENSITY
FILTER
SECTION
WHERE
WE
APPROXIMATE
THE
POSTERIOR
BY
A
GAUSSIAN
WITH
A
DIAGONAL
COVARIANCE
THE
VARIANCE
TERMS
SERVE
AS
A
PER
PARAMETER
STEP
SIZE
SEE
SECTION
FOR
DETAILS
ANOTHER
APPROACH
IS
TO
USE
PARTICLE
FILTERING
SECTION
THIS
WAS
USED
IN
ANDRIEU
ET
AL
FOR
SEQUENTIALLY
LEARNING
A
KERNELIZED
LINEAR
LOGISTIC
REGRESSION
MODEL
GENERATIVE
VS
DISCRIMINATIVE
CLASSIFIERS
IN
SECTION
WE
SHOWED
THAT
THE
POSTERIOR
OVER
CLASS
LABELS
INDUCED
BY
GAUSSIAN
DISCRIM
INANT
ANALYSIS
GDA
HAS
EXACTLY
THE
SAME
FORM
AS
LOGISTIC
REGRESSION
NAMELY
P
Y
X
SIGM
WT
X
THE
DECISION
BOUNDARY
IS
THEREFORE
A
LINEAR
FUNCTION
OF
X
IN
BOTH
CASES
NOTE
HOWEVER
THAT
MANY
GENERATIVE
MODELS
CAN
GIVE
RISE
TO
A
LOGISTIC
REGRESSION
POSTERIOR
E
G
IF
EACH
CLASS
CONDITIONAL
DENSITY
IS
POISSON
P
X
Y
C
POI
X
ΛC
SO
THE
ASSUMPTIONS
MADE
BY
GDA
ARE
MUCH
STRONGER
THAN
THE
ASSUMPTIONS
MADE
BY
LOGISTIC
REGRESSION
A
FURTHER
DIFFERENCE
BETWEEN
THESE
MODELS
IS
THE
WAY
THEY
ARE
TRAINED
WHEN
FITTING
A
DISCRIM
INATIVE
MODEL
WE
USUALLY
MAXIMIZE
THE
CONDITIONAL
LOG
LIKELIHOOD
N
LOG
P
YI
XI
Θ
WHEREAS
WHEN
FITTING
A
GENERATIVE
MODEL
WE
USUALLY
MAXIMIZE
THE
JOINT
LOG
LIKELIHOOD
N
LOG
P
YI
XI
Θ
IT
IS
CLEAR
THAT
THESE
CAN
IN
GENERAL
GIVE
DIFFERENT
RESULTS
SEE
EXERCISE
20
WHEN
THE
GAUSSIAN
ASSUMPTIONS
MADE
BY
GDA
ARE
CORRECT
THE
MODEL
WILL
NEED
LESS
TRAINING
DATA
THAN
LOGISTIC
REGRESSION
TO
ACHIEVE
A
CERTAIN
LEVEL
OF
PERFORMANCE
BUT
IF
THE
GAUSSIAN
ASSUMPTIONS
ARE
INCORRECT
LOGISTIC
REGRESSION
WILL
DO
BETTER
NG
AND
JORDAN
THIS
IS
BECAUSE
DISCRIMINATIVE
MODELS
DO
NOT
NEED
TO
MODEL
THE
DISTRIBUTION
OF
THE
FEATURES
THIS
IS
ILLUSTRATED
IN
FIGURE
WE
SEE
THAT
THE
CLASS
CONDITIONAL
DENSITIES
ARE
RATHER
COMPLEX
IN
PARTICULAR
P
X
Y
IS
A
MULTIMODAL
DISTRIBUTION
WHICH
MIGHT
BE
HARD
TO
ESTIMATE
HOWEVER
THE
CLASS
POSTERIOR
P
Y
C
X
IS
A
SIMPLE
SIGMOIDAL
FUNCTION
CENTERED
ON
THE
THRESHOLD
VALUE
OF
THIS
SUGGESTS
THAT
IN
GENERAL
DISCRIMINATIVE
METHODS
WILL
BE
MORE
ACCURATE
SINCE
THEIR
JOB
IS
IN
SOME
SENSE
EASIER
HOWEVER
ACCURACY
IS
NOT
THE
ONLY
IMPORTANT
FACTOR
WHEN
CHOOSING
A
METHOD
BELOW
WE
DISCUSS
SOME
OTHER
ADVANTAGES
AND
DISADVANTAGES
OF
EACH
APPROACH
PROS
AND
CONS
OF
EACH
APPROACH
EASY
TO
FIT
AS
WE
HAVE
SEEN
IT
IS
USUALLY
VERY
EASY
TO
FIT
GENERATIVE
CLASSIFIERS
FOR
EXAMPLE
IN
SECTIONS
AND
WE
SHOW
THAT
WE
CAN
FIT
A
NAIVE
BAYES
MODEL
AND
AN
LDA
MODEL
BY
SIMPLE
COUNTING
AND
AVERAGING
BY
CONTRAST
LOGISTIC
REGRESSION
REQUIRES
SOLVING
A
CONVEX
OPTIMIZATION
PROBLEM
SEE
SECTION
FOR
THE
DETAILS
WHICH
IS
MUCH
SLOWER
FIT
CLASSES
SEPARATELY
IN
A
GENERATIVE
CLASSIFIER
WE
ESTIMATE
THE
PARAMETERS
OF
EACH
CLASS
CONDITIONAL
DENSITY
INDEPENDENTLY
SO
WE
DO
NOT
HAVE
TO
RETRAIN
THE
MODEL
WHEN
WE
ADD
MORE
CLASSES
IN
CONTRAST
IN
DISCRIMINATIVE
MODELS
ALL
THE
PARAMETERS
INTERACT
SO
THE
WHOLE
MODEL
MUST
BE
RETRAINED
IF
WE
ADD
A
NEW
CLASS
THIS
IS
ALSO
THE
CASE
IF
WE
TRAIN
A
GENERATIVE
MODEL
TO
MAXIMIZE
A
DISCRIMINATIVE
OBJECTIVE
SALOJARVI
ET
AL
HANDLE
MISSING
FEATURES
EASILY
SOMETIMES
SOME
OF
THE
INPUTS
COMPONENTS
OF
X
ARE
NOT
OBSERVED
IN
A
GENERATIVE
CLASSIFIER
THERE
IS
A
SIMPLE
METHOD
FOR
DEALING
WITH
THIS
AS
WE
DISCUSS
IN
SECTION
HOWEVER
IN
A
DISCRIMINATIVE
CLASSIFIER
THERE
IS
NO
PRINCIPLED
SOLUTION
TO
THIS
PROBLEM
SINCE
THE
MODEL
ASSUMES
THAT
X
IS
ALWAYS
AVAILABLE
TO
BE
CONDITIONED
ON
ALTHOUGH
SEE
MARLIN
FOR
SOME
HEURISTIC
APPROACHES
CAN
HANDLE
UNLABELED
TRAINING
DATA
THERE
IS
MUCH
INTEREST
IN
SEMI
SUPERVISED
LEARNING
WHICH
USES
UNLABELED
DATA
TO
HELP
SOLVE
A
SUPERVISED
TASK
THIS
IS
FAIRLY
EASY
TO
DO
USING
GENERATIVE
MODELS
SEE
E
G
LASSERRE
ET
AL
LIANG
ET
AL
BUT
IS
MUCH
HARDER
TO
DO
WITH
DISCRIMINATIVE
MODELS
SYMMETRIC
IN
INPUTS
AND
OUTPUTS
WE
CAN
RUN
A
GENERATIVE
MODEL
BACKWARDS
AND
INFER
PROBABLE
INPUTS
GIVEN
THE
OUTPUT
BY
COMPUTING
P
X
Y
THIS
IS
NOT
POSSIBLE
WITH
A
DISCRIMINATIVE
MODEL
THE
REASON
IS
THAT
A
GENERATIVE
MODEL
DEFINES
A
JOINT
DISTRIBUTION
ON
X
AND
Y
AND
HENCE
TREATS
BOTH
INPUTS
AND
OUTPUTS
SYMMETRICALLY
CAN
HANDLE
FEATURE
PREPROCESSING
A
BIG
ADVANTAGE
OF
DISCRIMINATIVE
METHODS
IS
THAT
THEY
ALLOW
US
TO
PREPROCESS
THE
INPUT
IN
ARBITRARY
WAYS
E
G
WE
CAN
REPLACE
X
WITH
Φ
X
WHICH
COULD
BE
SOME
BASIS
FUNCTION
EXPANSION
AS
ILLUSTRATED
IN
FIGURE
IT
IS
OFTEN
HARD
TO
DEFINE
A
GENERATIVE
MODEL
ON
SUCH
PRE
PROCESSED
DATA
SINCE
THE
NEW
FEATURES
ARE
CORRELATED
IN
COMPLEX
WAYS
WELL
CALIBRATED
PROBABILITIES
SOME
GENERATIVE
MODELS
SUCH
AS
NAIVE
BAYES
MAKE
STRONG
INDEPENDENCE
ASSUMPTIONS
WHICH
ARE
OFTEN
NOT
VALID
THIS
CAN
RESULT
IN
VERY
EXTREME
POSTE
RIOR
CLASS
PROBABILITIES
VERY
NEAR
OR
DISCRIMINATIVE
MODELS
SUCH
AS
LOGISTIC
REGRESSION
ARE
USUALLY
BETTER
CALIBRATED
IN
TERMS
OF
THEIR
PROBABILITY
ESTIMATES
WE
SEE
THAT
THERE
ARE
ARGUMENTS
FOR
AND
AGAINST
BOTH
KINDS
OF
MODELS
IT
IS
THEREFORE
USEFUL
TO
HAVE
BOTH
KINDS
IN
YOUR
TOOLBOX
SEE
TABLE
FOR
A
SUMMARY
OF
THE
CLASSIFICATION
AND
LINEAR
MULTINOMIAL
LOGISTIC
REGRESSION
KERNEL
RBF
MULTINOMIAL
LOGISTIC
REGRESSION
A
B
FIGURE
A
MULTINOMIAL
LOGISTIC
REGRESSION
FOR
CLASSES
IN
THE
ORIGINAL
FEATURE
SPACE
B
AFTER
BASIS
FUNCTION
EXPANSION
USING
RBF
KERNELS
WITH
A
BANDWIDTH
OF
AND
USING
ALL
THE
DATA
POINTS
AS
CENTERS
FIGURE
GENERATED
BY
LOGREGMULTINOMKERNELDEMO
5
X
A
X
B
FIGURE
THE
CLASS
CONDITIONAL
DENSITIES
P
X
Y
C
LEFT
MAY
BE
MORE
COMPLEX
THAN
THE
CLASS
POSTERIORS
P
Y
C
X
RIGHT
BASED
ON
FIGURE
OF
BISHOP
FIGURE
GENERATED
BY
GENERATIVEVSDISCRIM
REGRESSION
TECHNIQUES
WE
COVER
IN
THIS
BOOK
DEALING
WITH
MISSING
DATA
SOMETIMES
SOME
OF
THE
INPUTS
COMPONENTS
OF
X
ARE
NOT
OBSERVED
THIS
COULD
BE
DUE
TO
A
SENSOR
FAILURE
OR
A
FAILURE
TO
COMPLETE
AN
ENTRY
IN
A
SURVEY
ETC
THIS
IS
CALLED
THE
MISSING
DATA
PROBLEM
LITTLE
AND
RUBIN
THE
ABILITY
TO
HANDLE
MISSING
DATA
IN
A
PRINCIPLED
WAY
IS
ONE
OF
THE
BIGGEST
ADVANTAGES
OF
GENERATIVE
MODELS
TO
FORMALIZE
OUR
ASSUMPTIONS
WE
CAN
ASSOCIATE
A
BINARY
RESPONSE
VARIABLE
RI
THAT
SPECIFIES
WHETHER
EACH
VALUE
XI
IS
OBSERVED
OR
NOT
THE
JOINT
MODEL
HAS
THE
FORM
P
XI
RI
Θ
Φ
P
RI
XI
Φ
P
XI
Θ
WHERE
Φ
ARE
THE
PARAMETERS
CONTROLLING
WHETHER
THE
ITEM
MODEL
CLASSIF
REGR
GEN
DISCR
PARAM
NON
SECTION
DISCRIMINANT
ANALYSIS
CLASSIF
GEN
PARAM
SEC
NAIVE
BAYES
CLASSIFIER
CLASSIF
GEN
PARAM
SEC
5
5
TREE
AUGMENTED
NAIVE
BAYES
CLASSIFIER
CLASSIF
GEN
PARAM
SEC
LINEAR
REGRESSION
REGR
DISCRIM
PARAM
SEC
5
LOGISTIC
REGRESSION
CLASSIF
DISCRIM
PARAM
SEC
SPARSE
LINEAR
LOGISTIC
REGRESSION
BOTH
DISCRIM
PARAM
CH
MIXTURE
OF
EXPERTS
BOTH
DISCRIM
PARAM
SEC
MULTILAYER
PERCEPTRON
MLP
NEURAL
NETWORK
BOTH
DISCRIM
PARAM
CH
CONDITIONAL
RANDOM
FIELD
CRF
CLASSIF
DISCRIM
PARAM
SEC
K
NEAREST
NEIGHBOR
CLASSIFIER
CLASSIF
GEN
NON
SEC
INFINITE
MIXTURE
DISCRIMINANT
ANALYSIS
CLASSIF
GEN
NON
SEC
CLASSIFICATION
AND
REGRESSION
TREES
CART
BOTH
DISCRIM
NON
SEC
BOOSTED
MODEL
BOTH
DISCRIM
NON
SEC
SPARSE
KERNELIZED
LIN
LOGREG
SKLR
BOTH
DISCRIM
NON
SEC
RELEVANCE
VECTOR
MACHINE
RVM
BOTH
DISCRIM
NON
SEC
SUPPORT
VECTOR
MACHINE
SVM
BOTH
DISCRIM
NON
SEC
5
GAUSSIAN
PROCESSES
GP
BOTH
DISCRIM
NON
CH
SMOOTHING
SPLINES
REGR
DISCRIM
NON
SECTION
TABLE
LIST
OF
VARIOUS
MODELS
FOR
CLASSIFICATION
AND
REGRESSION
WHICH
WE
DISCUSS
IN
THIS
BOOK
COLUMNS
ARE
AS
FOLLOWS
MODEL
NAME
IS
THE
MODEL
SUITABLE
FOR
CLASSIFICATION
REGRESSION
OR
BOTH
IS
THE
MODEL
GENERATIVE
OR
DISCRIMINATIVE
IS
THE
MODEL
PARAMETRIC
OR
NON
PARAMETRIC
LIST
OF
SECTIONS
IN
BOOK
WHICH
DISCUSS
THE
MODEL
SEE
ALSO
HTTP
GOOGLECODE
COM
SVN
TRUNK
DOCS
TUTORIAL
HTML
TU
TSUPERVISED
HTML
FOR
THE
PMTK
EQUIVALENTS
OF
THESE
MODELS
ANY
GENERATIVE
PROBABILISTIC
MODEL
E
G
HMMS
BOLTZMANN
MACHINES
BAYESIAN
NETWORKS
ETC
CAN
BE
TURNED
INTO
A
CLASSIFIER
BY
USING
IT
AS
A
CLASS
CONDITIONAL
DENSITY
IS
OBSERVED
OR
NOT
IF
WE
ASSUME
P
RI
XI
Φ
P
RI
Φ
WE
SAY
THE
DATA
IS
MISSING
COMPLETELY
AT
RANDOM
OR
MCAR
IF
WE
ASSUME
P
RI
XI
Φ
P
RI
XO
Φ
WHERE
XO
IS
THE
OBSERVED
PART
OF
XI
WE
SAY
THE
DATA
IS
MISSING
AT
RANDOM
OR
MAR
IF
NEITHER
OF
THESE
ASSUMPTIONS
HOLD
WE
SAY
THE
DATA
IS
NOT
MISSING
AT
RANDOM
OR
NMAR
IN
THIS
CASE
WE
HAVE
TO
MODEL
THE
MISSING
DATA
MECHANISM
SINCE
THE
PATTERN
OF
MISSINGNESS
IS
INFORMATIVE
ABOUT
THE
VALUES
OF
THE
MISSING
DATA
AND
THE
CORRESPONDING
PARAMETERS
THIS
IS
THE
CASE
IN
MOST
COLLABORATIVE
FILTERING
PROBLEMS
FOR
EXAMPLE
SEE
E
G
MARLIN
FOR
FURTHER
DISCUSSION
WE
WILL
HENCEFORTH
ASSUME
THE
DATA
IS
MAR
WHEN
DEALING
WITH
MISSING
DATA
IT
IS
HELPFUL
TO
DISTINGUISH
THE
CASES
WHEN
THERE
IS
MISSING
NESS
ONLY
AT
TEST
TIME
SO
THE
TRAINING
DATA
IS
COMPLETE
DATA
FROM
THE
HARDER
CASE
WHEN
THERE
IS
MISSINGNESS
ALSO
AT
TRAINING
TIME
WE
WILL
DISCUSS
THESE
TWO
CASES
BELOW
NOTE
THAT
THE
CLASS
LABEL
IS
ALWAYS
MISSING
AT
TEST
TIME
BY
DEFINITION
IF
THE
CLASS
LABEL
IS
ALSO
SOMETIMES
MISSING
AT
TRAINING
TIME
THE
PROBLEM
IS
CALLED
SEMI
SUPERVISED
LEARNING
MISSING
DATA
AT
TEST
TIME
IN
A
GENERATIVE
CLASSIFIER
WE
CAN
HANDLE
FEATURES
THAT
ARE
MAR
BY
MARGINALIZING
THEM
OUT
FOR
EXAMPLE
IF
WE
ARE
MISSING
THE
VALUE
OF
WE
CAN
COMPUTE
P
Y
C
D
Θ
P
Y
C
Θ
P
D
Y
C
Θ
P
Y
C
Θ
P
D
Y
C
Θ
IF
WE
MAKE
THE
NAIVE
BAYES
ASSUMPTION
THE
MARGINALIZATION
CAN
BE
PERFORMED
AS
FOLLOWS
P
D
Y
C
Θ
P
TTD
P
XJ
ΘJC
TT
P
XJ
ΘJC
J
J
WHERE
WE
EXPLOITED
THE
FACT
THAT
P
Y
C
Θ
HENCE
IN
A
NAIVE
BAYES
CLASSIFIER
WE
CAN
SIMPLY
IGNORE
MISSING
FEATURES
AT
TEST
TIME
SIMILARLY
IN
DISCRIMINANT
ANALYSIS
NO
MATTER
WHAT
REGULARIZATION
METHOD
WAS
USED
TO
ESTIMATE
THE
PARAMETERS
WE
CAN
ALWAYS
ANALYTICALLY
MARGINALIZE
OUT
THE
MISSING
VARIABLES
SEE
SECTION
P
D
Y
C
Θ
N
D
ΜC
D
ΣC
D
D
MISSING
DATA
AT
TRAINING
TIME
MISSING
DATA
AT
TRAINING
TIME
IS
HARDER
TO
DEAL
WITH
IN
PARTICULAR
COMPUTING
THE
MLE
OR
MAP
ESTIMATE
IS
NO
LONGER
A
SIMPLE
OPTIMIZATION
PROBLEM
FOR
REASONS
DISCUSSED
IN
SECTION
HOWEVER
SOON
WE
WILL
STUDY
ARE
A
VARIETY
OF
MORE
SOPHISTICATED
ALGORITHMS
SUCH
AS
EM
ALGO
RITHM
IN
SECTION
FOR
FINDING
APPROXIMATE
ML
OR
MAP
ESTIMATES
IN
SUCH
CASES
FISHER
LINEAR
DISCRIMINANT
ANALYSIS
FLDA
DISCRIMINANT
ANALYSIS
IS
A
GENERATIVE
APPROACH
TO
CLASSIFICATION
WHICH
REQUIRES
FITTING
AN
MVN
TO
THE
FEATURES
AS
WE
HAVE
DISCUSSED
THIS
CAN
BE
PROBLEMATIC
IN
HIGH
DIMENSIONS
AN
ALTERNATIVE
APPROACH
IS
TO
REDUCE
THE
DIMENSIONALITY
OF
THE
FEATURES
X
RD
AND
THEN
FIT
AN
MVN
TO
THE
RESULTING
LOW
DIMENSIONAL
FEATURES
Z
RL
THE
SIMPLEST
APPROACH
IS
TO
USE
A
LINEAR
PROJECTION
MATRIX
Z
WX
WHERE
W
IS
A
L
D
MATRIX
ONE
APPROACH
TO
FINDING
W
WOULD
BE
TO
USE
PCA
SECTION
THE
RESULT
WOULD
BE
VERY
SIMILAR
TO
RDA
SECTION
SINCE
SVD
AND
PCA
ARE
ESSENTIALLY
EQUIVALENT
HOWEVER
PCA
IS
AN
UNSUPERVISED
TECHNIQUE
THAT
DOES
NOT
TAKE
CLASS
LABELS
INTO
ACCOUNT
THUS
THE
RESULTING
LOW
DIMENSIONAL
FEATURES
ARE
NOT
NECESSARILY
OPTIMAL
FOR
CLASSIFICATION
AS
ILLUSTRATED
IN
FIGURE
AN
ALTERNATIVE
APPROACH
IS
TO
FIND
THE
MATRIX
W
SUCH
THAT
THE
LOW
DIMENSIONAL
DATA
CAN
BE
CLASSIFIED
AS
WELL
AS
POSSIBLE
USING
A
GAUSSIAN
CLASS
CONDITIONAL
DENSITY
MODEL
THE
ASSUMPTION
OF
GAUSSIANITY
IS
REASONABLE
SINCE
WE
ARE
COMPUTING
LINEAR
COMBINATIONS
OF
POTENTIALLY
NON
GAUSSIAN
FEATURES
THIS
APPROACH
IS
CALLED
FISHER
LINEAR
DISCRIMINANT
ANALYSIS
OR
FLDA
FLDA
IS
AN
INTERESTING
HYBRID
OF
DISCRIMINATIVE
AND
GENERATIVE
TECHNIQUES
THE
DRAWBACK
OF
THIS
TECHNIQUE
IS
THAT
IT
IS
RESTRICTED
TO
USING
L
C
DIMENSIONS
REGARDLESS
OF
D
FOR
REASONS
THAT
WE
WILL
EXPLAIN
BELOW
IN
THE
TWO
CLASS
CASE
THIS
MEANS
WE
ARE
SEEKING
A
SINGLE
VECTOR
W
ONTO
WHICH
WE
CAN
PROJECT
THE
DATA
BELOW
WE
DERIVE
THE
OPTIMAL
W
IN
THE
TWO
CLASS
CASE
WE
MEANS
FISHER
PCA
04
A
FISHER
20
5
20
5
B
PCA
20
5
08
6
C
FIGURE
EXAMPLE
OF
FISHER
LINEAR
DISCRIMINANT
A
TWO
CLASS
DATA
IN
DASHED
GREEN
LINE
FIRST
PRINCIPAL
BASIS
VECTOR
DOTTED
RED
LINE
FISHER
LINEAR
DISCRIMINANT
VECTOR
SOLID
BLACK
LINE
JOINS
THE
CLASS
CONDITIONAL
MEANS
B
PROJECTION
OF
POINTS
ONTO
FISHER
VECTOR
SHOWS
GOOD
CLASS
SEPARATION
C
PROJECTION
OF
POINTS
ONTO
PCA
VECTOR
SHOWS
POOR
CLASS
SEPARATION
FIGURE
GENERATED
BY
FISHERLDADEMO
THEN
GENERALIZE
TO
THE
MULTI
CLASS
CASE
AND
FINALLY
WE
GIVE
A
PROBABILISTIC
INTERPRETATION
OF
THIS
TECHNIQUE
6
DERIVATION
OF
THE
OPTIMAL
PROJECTION
WE
NOW
DERIVE
THIS
OPTIMAL
DIRECTION
W
FOR
THE
TWO
CLASS
CASE
FOLLOWING
THE
PRESENTATION
OF
BISHOP
SEC
DEFINE
THE
CLASS
CONDITIONAL
MEANS
AS
I
YI
XI
XI
I
YI
LET
MK
WT
ΜK
BE
THE
PROJECTION
OF
EACH
MEAN
ONTO
THE
LINE
W
ALSO
LET
ZI
WT
XI
BE
THE
PROJECTION
OF
THE
DATA
ONTO
THE
LINE
THE
VARIANCE
OF
THE
PROJECTED
POINTS
IS
PROPORTIONAL
TO
ZI
MK
I
YI
K
THE
GOAL
IS
TO
FIND
W
SUCH
THAT
WE
MAXIMIZE
THE
DISTANCE
BETWEEN
THE
MEANS
WHILE
ALSO
ENSURING
THE
PROJECTED
CLUSTERS
ARE
TIGHT
J
W
S2
WE
CAN
REWRITE
THE
RIGHT
HAND
SIDE
OF
THE
ABOVE
IN
TERMS
OF
W
AS
FOLLOWS
WT
SBW
J
W
WT
W
WHERE
SB
IS
THE
BETWEEN
CLASS
SCATTER
MATRIX
GIVEN
BY
SB
T
AND
SW
IS
THE
WITHIN
CLASS
SCATTER
MATRIX
GIVEN
BY
SW
XI
XI
T
XI
XI
T
I
YI
TO
SEE
THIS
NOTE
THAT
I
YI
WT
SBW
WT
T
W
AND
WT
SW
W
I
YI
WT
XI
XI
T
W
I
YI
WT
XI
XI
T
W
ZI
ZI
EQUATION
IS
A
RATIO
OF
TWO
SCALARS
WE
CAN
TAKE
ITS
DERIVATIVE
WITH
RESPECT
TO
W
AND
EQUATE
TO
ZERO
ONE
CAN
SHOW
EXERCISE
6
THAT
THAT
J
W
IS
MAXIMIZED
WHEN
SBW
ΛSW
W
5
5
5
5
5
A
5
5
5
5
5
B
FIGURE
A
PCA
PROJECTION
OF
VOWEL
DATA
TO
B
FLDA
PROJECTION
OF
VOWEL
DATA
TO
WE
SEE
THERE
IS
BETTER
CLASS
SEPARATION
IN
THE
FLDA
CASE
BASED
ON
FIGURE
OF
HASTIE
ET
AL
FIGURE
GENERATED
BY
FISHERDISCRIMVOWELDEMO
BY
HANNES
BRETSCHNEIDER
WHERE
WT
SBW
Λ
WT
W
EQUATION
IS
CALLED
A
GENERALIZED
EIGENVALUE
PROBLEM
IF
SW
IS
INVERTIBLE
WE
CAN
CONVERT
IT
TO
A
REGULAR
EIGENVALUE
PROBLEM
ΛW
HOWEVER
IN
THE
TWO
CLASS
CASE
THERE
IS
A
SIMPLER
SOLUTION
IN
PARTICULAR
SINCE
SBW
T
W
THEN
FROM
EQUATION
WE
HAVE
Λ
W
M1
W
SINCE
WE
ONLY
CARE
ABOUT
THE
DIRECTIONALITY
AND
NOT
THE
SCALE
FACTOR
WE
CAN
JUST
SET
W
THIS
IS
THE
OPTIMAL
SOLUTION
IN
THE
TWO
CLASS
CASE
IF
SW
I
MEANING
THE
POOLED
COVARIANCE
MATRIX
IS
ISOTROPIC
THEN
W
IS
PROPORTIONAL
TO
THE
VECTOR
THAT
JOINS
THE
CLASS
MEANS
THIS
IS
AN
INTUITIVELY
REASONABLE
DIRECTION
TO
PROJECT
ONTO
AS
SHOWN
IN
FIGURE
6
EXTENSION
TO
HIGHER
DIMENSIONS
AND
MULTIPLE
CLASSES
WE
CAN
EXTEND
THE
ABOVE
IDEA
TO
MULTIPLE
CLASSES
AND
TO
HIGHER
DIMENSIONAL
SUBSPACES
BY
FINDING
A
PROJECTION
MATRIX
W
WHICH
MAPS
FROM
D
TO
L
SO
AS
TO
MAXIMIZE
WΣBWT
J
W
WΣW
WT
WHERE
ΣB
Σ
NC
Μ
N
C
C
NC
Σ
Μ
ΜC
Μ
116
C
NC
XI
I
YI
C
ΜC
XI
ΜC
THE
SOLUTION
CAN
BE
SHOWN
TO
BE
W
WHERE
U
ARE
THE
L
LEADING
EIGENVECTORS
OF
Σ
ΣBΣ
ASSUMING
ΣW
IS
NON
SINGULAR
IF
IT
W
W
IS
SINGULAR
WE
CAN
FIRST
PERFORM
PCA
ON
ALL
THE
DATA
FIGURE
GIVES
AN
EXAMPLE
OF
THIS
METHOD
APPLIED
TO
SOME
D
DIMENSIONAL
SPEECH
DATA
REPRESENTING
C
DIFFERENT
VOWEL
SOUNDS
WE
SEE
THAT
FLDA
GIVES
BETTER
CLASS
SEPARATION
THAN
PCA
NOTE
THAT
FLDA
IS
RESTRICTED
TO
FINDING
AT
MOST
A
L
C
DIMENSIONAL
LINEAR
SUBSPACE
NO
MATTER
HOW
LARGE
D
BECAUSE
THE
RANK
OF
THE
BETWEEN
CLASS
COVARIANCE
MATRIX
ΣB
IS
C
THE
TERM
ARISES
BECAUSE
OF
THE
Μ
TERM
WHICH
IS
A
LINEAR
FUNCTION
OF
THE
ΜC
THIS
IS
A
RATHER
SEVERE
RESTRICTION
WHICH
LIMITS
THE
USEFULNESS
OF
FLDA
6
PROBABILISTIC
INTERPRETATION
OF
FLDA
TO
FIND
A
VALID
PROBABILISTIC
INTERPRETATION
OF
FLDA
WE
FOLLOW
THE
APPROACH
OF
KUMAR
AND
ANDREO
ZHOU
ET
AL
THEY
PROPOSED
A
MODEL
KNOWN
AS
HETEROSCEDASTIC
LDA
HLDA
WHICH
WORKS
AS
FOLLOWS
LET
W
BE
A
D
D
INVERTIBLE
MATRIX
AND
LET
ZI
WXI
BE
A
TRANSFORMED
VERSION
OF
THE
DATA
WE
NOW
FIT
FULL
COVARIANCE
GAUSSIANS
TO
THE
TRANSFORMED
DATA
ONE
PER
CLASS
BUT
WITH
THE
CONSTRAINT
THAT
ONLY
THE
FIRST
L
COMPONENTS
WILL
BE
CLASS
SPECIFIC
THE
REMAINING
H
D
L
COMPONENTS
WILL
BE
SHARED
ACROSS
CLASSES
AND
WILL
THUS
NOT
BE
DISCRIMINATIVE
THAT
IS
WE
USE
P
ZI
Θ
YI
C
N
ZI
ΜC
ΣC
ΜC
MC
ΣC
SC
WHERE
IS
THE
SHARED
H
DIMENSIONAL
MEAN
AND
IS
THE
SHARED
H
H
COVARIACE
THE
PDF
OF
THE
ORIGINAL
UNTRANSFORMED
DATA
IS
GIVEN
BY
P
XI
YI
C
W
Θ
W
N
WXI
ΜC
ΣC
W
N
WLXI
MC
SC
N
WHXI
WHERE
W
WL
FOR
FIXED
W
IT
IS
EASY
TO
DERIVE
THE
MLE
FOR
Θ
ONE
CAN
THEN
OPTIMIZE
WH
W
USING
GRADIENT
METHODS
IN
THE
SPECIAL
CASE
THAT
THE
ΣC
ARE
DIAGONAL
THERE
IS
A
CLOSED
FORM
SOLUTION
FOR
W
GALES
AND
IN
THE
SPECIAL
CASE
THE
ΣC
ARE
ALL
EQUAL
WE
RECOVER
CLASSICAL
LDA
ZHOU
ET
AL
IN
VIEW
OF
THIS
THIS
RESULT
IT
SHOULD
BE
CLEAR
THAT
HLDA
WILL
OUTPERFORM
LDA
IF
THE
CLASS
COVARIANCES
ARE
NOT
EQUAL
WITHIN
THE
DISCRIMINATIVE
SUBSPACE
I
E
IF
THE
ASSUMPTION
THAT
ΣC
IS
INDEPENDENT
OF
C
IS
A
POOR
ASSUMPTION
THIS
IS
EASY
TO
DEMONSTRATE
ON
SYNTHETIC
DATA
AND
IS
ALSO
THE
CASE
ON
MORE
CHALLENGING
TASKS
SUCH
AS
SPEECH
RECOGNITION
KUMAR
AND
ANDREO
FURTHERMORE
WE
CAN
EXTEND
THE
MODEL
BY
ALLOWING
EACH
CLASS
TO
USE
ITS
OWN
PROJECTION
MATRIX
THIS
IS
KNOWN
AS
MULTIPLE
LDA
GALES
EXERCISES
EXERCISE
SPAM
CLASSIFICATION
USING
LOGISTIC
REGRESSION
CONSIDER
THE
EMAIL
SPAM
DATA
SET
DISCUSSED
ON
OF
HASTIE
ET
AL
THIS
CONSISTS
OF
EMAIL
MESSAGES
FROM
WHICH
FEATURES
HAVE
BEEN
EXTRACTED
THESE
ARE
AS
FOLLOWS
FEATURES
IN
GIVING
THE
PERCENTAGE
OF
WORDS
IN
A
GIVEN
MESSAGE
WHICH
MATCH
A
GIVEN
WORD
ON
THE
LIST
THE
LIST
CONTAINS
WORDS
SUCH
AS
BUSINESS
FREE
GEORGE
ETC
THE
DATA
WAS
COLLECTED
BY
GEORGE
FORMAN
SO
HIS
NAME
OCCURS
QUITE
A
LOT
6
FEATURES
IN
100
GIVING
THE
PERCENTAGE
OF
CHARACTERS
IN
THE
EMAIL
THAT
MATCH
A
GIVEN
CHARACTER
ON
THE
LIST
THE
CHARACTERS
ARE
FEATURE
THE
AVERAGE
LENGTH
OF
AN
UNINTERRUPTED
SEQUENCE
OF
CAPITAL
LETTERS
MAX
IS
MEAN
IS
FEATURE
THE
LENGTH
OF
THE
LONGEST
UNINTERRUPTED
SEQUENCE
OF
CAPITAL
LETTERS
MAX
IS
MEAN
IS
6
FEATURE
THE
SUM
OF
THE
LENGTS
OF
UNINTERRUPTED
SEQUENCE
OF
CAPITAL
LETTERS
MAX
IS
6
MEAN
IS
LOAD
THE
DATA
FROM
SPAMDATA
MAT
WHICH
CONTAINS
A
TRAINING
SET
OF
SIZE
AND
A
TEST
SET
OF
SIZE
ONE
CAN
IMAGINE
PERFORMING
SEVERAL
KINDS
OF
PREPROCESSING
TO
THIS
DATA
TRY
EACH
OF
THE
FOLLOWING
SEPARATELY
A
STANDARDIZE
THE
COLUMNS
SO
THEY
ALL
HAVE
MEAN
AND
UNIT
VARIANCE
B
TRANSFORM
THE
FEATURES
USING
LOG
XIJ
C
BINARIZE
THE
FEATURES
USING
I
XIJ
FOR
EACH
VERSION
OF
THE
DATA
FIT
A
LOGISTIC
REGRESSION
MODEL
USE
CROSS
VALIDATION
TO
CHOOSE
THE
STRENGTH
OF
THE
T
REGULARIZER
REPORT
THE
MEAN
ERROR
RATE
ON
THE
TRAINING
AND
TEST
SETS
YOU
SHOULD
GET
NUMBERS
SIMILAR
TO
THIS
METHOD
TRAIN
TEST
STND
079
LOG
059
BINARY
072
THE
PRECISE
VALUES
WILL
DEPEND
ON
WHAT
REGULARIZATION
VALUE
YOU
CHOOSE
TURN
IN
YOUR
CODE
AND
NUMERICAL
RESULTS
SEE
ALSO
EXERCISE
EXERCISE
SPAM
CLASSIFICATION
USING
NAIVE
BAYES
WE
WILL
RE
EXAMINE
THE
DATASET
FROM
EXERCISE
A
USE
NAIVEBAYESFIT
AND
NAIVEBAYESPREDICT
ON
THE
BINARIZED
SPAM
DATA
WHAT
IS
THE
TRAINING
AND
TEST
ERROR
YOU
CAN
TRY
DIFFERENT
SETTINGS
OF
THE
PSEUDOCOUNT
Α
IF
YOU
LIKE
THIS
CORRESPONDS
TO
THE
BETA
Α
Α
PRIOR
EACH
ΘJC
ALTHOUGH
THE
DEFAULT
OF
Α
IS
PROBABLY
FINE
TURN
IN
YOUR
ERROR
RATES
B
MODIFY
THE
CODE
SO
IT
CAN
HANDLE
REAL
VALUED
FEATURES
USE
A
GAUSSIAN
DENSITY
FOR
EACH
FEATURE
FIT
IT
WITH
MAXIMUM
LIKELIHOOD
WHAT
ARE
THE
TRAINING
AND
TEST
ERROR
RATES
ON
THE
STANDARDIZED
DATA
AND
THE
LOG
TRANSFORMED
DATA
TURN
IN
YOUR
ERROR
RATES
AND
CODE
EXERCISE
GRADIENT
AND
HESSIAN
OF
LOG
LIKELIHOOD
FOR
LOGISTIC
REGRESSION
A
LET
Σ
A
BE
THE
SIGMOID
FUNCTION
SHOW
THAT
DΣ
A
Σ
A
Σ
A
DA
B
USING
THE
PREVIOUS
RESULT
AND
THE
CHAIN
RULE
OF
CALCULUS
DERIVE
AN
EXPRESSION
FOR
THE
GRADIENT
OF
THE
LOG
LIKELIHOOD
EQUATION
5
C
THE
HESSIAN
CAN
BE
WRITTEN
AS
H
XT
SX
WHERE
DIAG
ΜN
ΜN
SHOW
THAT
H
IS
POSITIVE
DEFINITE
YOU
MAY
ASSUME
THAT
ΜI
SO
THE
ELEMENTS
OF
WILL
BE
STRICTLY
POSITIVE
AND
THAT
X
IS
FULL
RANK
EXERCISE
GRADIENT
AND
HESSIAN
OF
LOG
LIKELIHOOD
FOR
MULTINOMIAL
LOGISTIC
REGRESSION
A
LET
ΜIK
ΗI
K
PROVE
THAT
THE
JACOBIAN
OF
THE
SOFTMAX
IS
ΜIK
Μ
ΗIJ
ΔKJ
ΜIJ
WHERE
ΔKJ
I
K
J
B
HENCE
SHOW
THAT
WCT
YIC
ΜIC
XI
I
HINT
USE
THE
CHAIN
RULE
AND
THE
FACT
THAT
C
YIC
C
SHOW
THAT
THE
BLOCK
SUBMATRIX
OF
THE
HESSIAN
FOR
CLASSES
C
AND
CI
IS
GIVEN
BY
HC
CT
ΜIC
ΔC
CT
ΜI
CT
XIXT
I
EXERCISE
5
SYMMETRIC
VERSION
OF
T
REGULARIZED
MULTINOMIAL
LOGISTIC
REGRESSION
SOURCE
EX
OF
HASTIE
ET
AL
MULTICLASS
LOGISTIC
REGRESSION
HAS
THE
FORM
EXP
WT
X
P
Y
C
X
W
C
K
C
EXP
WT
X
WHERE
W
IS
A
D
C
WEIGHT
MATRIX
WE
CAN
ARBITRARILY
DEFINE
W
FOR
ONE
OF
THE
CLASSES
SAY
C
C
SINCE
P
Y
C
X
W
C
P
Y
C
X
W
IN
THIS
CASE
THE
MODEL
HAS
THE
FORM
EXP
WT
X
P
Y
C
X
W
C
C
EXP
WT
X
K
K
IF
WE
DON
T
CLAMP
ONE
OF
THE
VECTORS
TO
SOME
CONSTANT
VALUE
THE
PARAMETERS
WILL
BE
UNIDENTIFIABLE
HOWEVER
SUPPOSE
WE
DON
T
CLAMP
WC
SO
WE
ARE
USING
EQUATION
128
BUT
WE
ADD
T
REGULARIZATION
BY
OPTIMIZING
N
C
LOG
P
YI
XI
W
Λ
WC
I
C
SHOW
THAT
AT
THE
OPTIMUM
WE
HAVE
C
WˆCJ
FOR
J
D
FOR
THE
UNREGULARIZED
TERMS
WE
STILL
NEED
TO
ENFORCE
THAT
TO
ENSURE
IDENTIFIABILITY
OF
THE
OFFSET
EXERCISE
6
ELEMENTARY
PROPERTIES
OF
T
REGULARIZED
LOGISTIC
REGRESSION
SOURCE
JAAAKKOLA
CONSIDER
MINIMIZING
J
W
T
W
D
WHERE
TRAIN
Λ
W
T
W
D
LOG
Σ
Y
XT
W
132
IS
THE
AVERAGE
LOG
LIKELIHOOD
ON
DATA
SET
D
FOR
YI
ANSWER
THE
FOLLOWING
TRUE
FALSE
QUESTIONS
A
J
W
HAS
MULTIPLE
LOCALLY
OPTIMAL
SOLUTIONS
T
F
B
LET
Wˆ
ARG
MINW
J
W
BE
A
GLOBAL
OPTIMUM
Wˆ
IS
SPARSE
HAS
MANY
ZERO
ENTRIES
T
F
C
IF
THE
TRAINING
DATA
IS
LINEARLY
SEPARABLE
THEN
SOME
WEIGHTS
WJ
MIGHT
BECOME
INFINITE
IF
Λ
T
F
D
T
Wˆ
DTRAIN
ALWAYS
INCREASES
AS
WE
INCREASE
Λ
T
F
E
T
Wˆ
DTEST
ALWAYS
INCREASES
AS
WE
INCREASE
Λ
T
F
EXERCISE
REGULARIZING
SEPARATE
TERMS
IN
LOGISTIC
REGRESSION
SOURCE
JAAAKKOLA
A
CONSIDER
THE
DATA
IN
FIGURE
WHERE
WE
FIT
THE
MODEL
P
Y
X
W
Σ
SUPPOSE
WE
FIT
THE
MODEL
BY
MAXIMUM
LIKELIHOOD
I
E
WE
MINIMIZE
J
W
T
W
DTRAIN
133
WHERE
T
W
TRAIN
IS
THE
LOG
LIKELIHOOD
ON
THE
TRAINING
SET
SKETCH
A
POSSIBLE
DECISION
BOUNDARY
CORRESPONDING
TO
Wˆ
COPY
THE
FIGURE
FIRST
A
ROUGH
SKETCH
IS
ENOUGH
AND
THEN
SUPERIMPOSE
YOUR
ANSWER
ON
YOUR
COPY
SINCE
YOU
WILL
NEED
MULTIPLE
VERSIONS
OF
THIS
FIGURE
IS
YOUR
ANSWER
DECISION
BOUNDARY
UNIQUE
HOW
MANY
CLASSIFICATION
ERRORS
DOES
YOUR
METHOD
MAKE
ON
THE
TRAINING
SET
B
NOW
SUPPOSE
WE
REGULARIZE
ONLY
THE
PARAMETER
I
E
WE
MINIMIZE
W
T
W
D
TRAIN
SUPPOSE
Λ
IS
A
VERY
LARGE
NUMBER
SO
WE
REGULARIZE
ALL
THE
WAY
TO
BUT
ALL
OTHER
PARAMETERS
ARE
UNREGULARIZED
SKETCH
A
POSSIBLE
DECISION
BOUNDARY
HOW
MANY
CLASSIFICATION
ERRORS
DOES
YOUR
METHOD
MAKE
ON
THE
TRAINING
SET
HINT
CONSIDER
THE
BEHAVIOR
OF
SIMPLE
LINEAR
REGRESSION
WHEN
C
NOW
SUPPOSE
WE
HEAVILY
REGULARIZE
ONLY
THE
PARAMETER
I
E
WE
MINIMIZE
W
T
W
D
TRAIN
135
SKETCH
A
POSSIBLE
DECISION
BOUNDARY
HOW
MANY
CLASSIFICATION
ERRORS
DOES
YOUR
METHOD
MAKE
ON
THE
TRAINING
SET
FIGURE
13
DATA
FOR
LOGISTIC
REGRESSION
QUESTION
D
NOW
SUPPOSE
WE
HEAVILY
REGULARIZE
ONLY
THE
PARAMETER
SKETCH
A
POSSIBLE
DECISION
BOUNDARY
HOW
MANY
CLASSIFICATION
ERRORS
DOES
YOUR
METHOD
MAKE
ON
THE
TRAINING
SET
GENERALIZED
LINEAR
MODELS
AND
THE
EXPONENTIAL
FAMILY
INTRODUCTION
WE
HAVE
NOW
ENCOUNTERED
A
WIDE
VARIETY
OF
PROBABILITY
DISTRIBUTIONS
THE
GAUSSIAN
THE
BERNOULLI
THE
STUDENT
T
THE
UNIFORM
THE
GAMMA
ETC
IT
TURNS
OUT
THAT
MOST
OF
THESE
ARE
MEMBERS
OF
A
BROADER
CLASS
OF
DISTRIBUTIONS
KNOWN
AS
THE
EXPONENTIAL
FAMILY
IN
THIS
CHAPTER
WE
DISCUSS
VARIOUS
PROPERTIES
OF
THIS
FAMILY
THIS
ALLOWS
US
TO
DERIVE
THEOREMS
AND
ALGORITHMS
WITH
VERY
BROAD
APPLICABILITY
WE
WILL
SEE
HOW
WE
CAN
EASILY
USE
ANY
MEMBER
OF
THE
EXPONENTIAL
FAMILY
AS
A
CLASS
CONDITIONAL
DENSITY
IN
ORDER
TO
MAKE
A
GENERATIVE
CLASSIFIER
IN
ADDITION
WE
WILL
DISCUSS
HOW
TO
BUILD
DISCRIMINATIVE
MODELS
WHERE
THE
RESPONSE
VARIABLE
HAS
AN
EXPONENTIAL
FAMILY
DISTRIBUTION
WHOSE
MEAN
IS
A
LINEAR
FUNCTION
OF
THE
INPUTS
THIS
IS
KNOWN
AS
A
GENERALIZED
LINEAR
MODEL
AND
GENERALIZES
THE
IDEA
OF
LOGISTIC
REGRESSION
TO
OTHER
KINDS
OF
RESPONSE
VARIABLES
THE
EXPONENTIAL
FAMILY
BEFORE
DEFINING
THE
EXPONENTIAL
FAMILY
WE
MENTION
SEVERAL
REASONS
WHY
IT
IS
IMPORTANT
IT
CAN
BE
SHOWN
THAT
UNDER
CERTAIN
REGULARITY
CONDITIONS
THE
EXPONENTIAL
FAMILY
IS
THE
ONLY
FAMILY
OF
DISTRIBUTIONS
WITH
FINITE
SIZED
SUFFICIENT
STATISTICS
MEANING
THAT
WE
CAN
COMPRESS
THE
DATA
INTO
A
FIXED
SIZED
SUMMARY
WITHOUT
LOSS
OF
INFORMATION
THIS
IS
PARTICULARLY
USEFUL
FOR
ONLINE
LEARNING
AS
WE
WILL
SEE
LATER
THE
EXPONENTIAL
FAMILY
IS
THE
ONLY
FAMILY
OF
DISTRIBUTIONS
FOR
WHICH
CONJUGATE
PRIORS
EXIST
WHICH
SIMPLIFIES
THE
COMPUTATION
OF
THE
POSTERIOR
SEE
SECTION
5
THE
EXPONENTIAL
FAMILY
CAN
BE
SHOWN
TO
BE
THE
FAMILY
OF
DISTRIBUTIONS
THAT
MAKES
THE
LEAST
SET
OF
ASSUMPTIONS
SUBJECT
TO
SOME
USER
CHOSEN
CONSTRAINTS
SEE
SECTION
6
THE
EXPONENTIAL
FAMILY
IS
AT
THE
CORE
OF
GENERALIZED
LINEAR
MODELS
AS
DISCUSSED
IN
SECTION
THE
EXPONENTIAL
FAMILY
IS
AT
THE
CORE
OF
VARIATIONAL
INFERENCE
AS
DISCUSSED
IN
SECTION
THE
EXCEPTIONS
ARE
THE
STUDENT
T
WHICH
DOES
NOT
HAVE
THE
RIGHT
FORM
AND
THE
UNIFORM
DISTRIBUTION
WHICH
DOES
NOT
HAVE
FIXED
SUPPORT
INDEPENDENT
OF
THE
PARAMETER
VALUES
DEFINITION
A
PDF
OR
PMF
P
X
Θ
FOR
X
XM
M
AND
Θ
Θ
RD
IS
SAID
TO
BE
IN
THE
EXPONENTIAL
FAMILY
IF
IT
IS
OF
THE
FORM
P
X
Θ
H
X
EXP
ΘT
Φ
X
Z
Θ
H
X
EXP
ΘT
Φ
X
A
Θ
WHERE
Z
Θ
X
M
H
X
EXP
ΘT
Φ
X
DX
A
Θ
LOG
Z
Θ
HERE
Θ
ARE
CALLED
THE
NATURAL
PARAMETERS
OR
CANONICAL
PARAMETERS
Φ
X
RD
IS
CALLED
A
VECTOR
OF
SUFFICIENT
STATISTICS
Z
Θ
IS
CALLED
THE
PARTITION
FUNCTION
A
Θ
IS
CALLED
THE
LOG
PARTITION
FUNCTION
OR
CUMULANT
FUNCTION
AND
H
X
IS
THE
A
SCALING
CONSTANT
OFTEN
IF
Φ
X
X
WE
SAY
IT
IS
A
NATURAL
EXPONENTIAL
FAMILY
EQUATION
CAN
BE
GENERALIZED
BY
WRITING
P
X
Θ
H
X
EXP
Η
Θ
T
Φ
X
A
Η
Θ
5
WHERE
Η
IS
A
FUNCTION
THAT
MAPS
THE
PARAMETERS
Θ
TO
THE
CANONICAL
PARAMETERS
Η
Η
Θ
IF
DIM
Θ
DIM
Η
Θ
IT
IS
CALLED
A
CURVED
EXPONENTIAL
FAMILY
WHICH
MEANS
WE
HAVE
MORE
SUFFICIENT
STATISTICS
THAN
PARAMETERS
IF
Η
Θ
Θ
THE
MODEL
IS
SAID
TO
BE
IN
CANONICAL
FORM
WE
WILL
ASSUME
MODELS
ARE
IN
CANONICAL
FORM
UNLESS
WE
STATE
OTHERWISE
EXAMPLES
LET
US
CONSIDER
SOME
EXAMPLES
TO
MAKE
THINGS
CLEARER
BERNOULLI
THE
BERNOULLI
FOR
X
CAN
BE
WRITTEN
IN
EXPONENTIAL
FAMILY
FORM
AS
FOLLOWS
BER
X
Μ
ΜX
Μ
X
EXP
X
LOG
Μ
X
LOG
Μ
EXP
Φ
X
T
Θ
6
WHERE
Φ
X
I
X
I
X
AND
Θ
LOG
Μ
LOG
Μ
HOWEVER
THIS
REPRESENTATION
IS
OVER
COMPLETE
SINCE
THERE
IS
A
LINEAR
DEPENDENDENCE
BETWEEN
THE
FEATURES
Φ
X
I
X
I
X
CONSEQUENTLY
Θ
IS
NOT
UNIQUELY
IDENTIFIABLE
IT
IS
COMMON
TO
REQUIRE
THAT
THE
REPRESENTATION
BE
MINIMAL
WHICH
MEANS
THERE
IS
A
UNIQUE
Θ
ASSOCIATED
WITH
THE
DISTRIBUTION
IN
THIS
CASE
WE
CAN
JUST
DEFINE
BER
X
Μ
Μ
EXP
X
LOG
Μ
L
NOW
WE
HAVE
Φ
X
X
Θ
LOG
Μ
WHICH
IS
THE
LOG
ODDS
RATIO
AND
Z
Μ
WE
Μ
CAN
RECOVER
THE
MEAN
PARAMETER
Μ
FROM
THE
CANONICAL
PARAMETER
USING
Μ
SIGM
Θ
MULTINOULLI
E
Θ
WE
CAN
REPRESENT
THE
MULTINOULLI
AS
A
MINIMAL
EXPONENTIAL
FAMILY
AS
FOLLOWS
WHERE
XK
I
X
K
CAT
X
Μ
KTT
ΜXK
EXP
K
K
XK
LOG
ΜK
EXP
K
K
XK
LOG
ΜK
K
K
XK
LOG
K
K
ΜK
EXP
X
LOG
F
ΜK
LOG
K
ΜK
EXP
XK
LOG
LOG
ΜK
ΜK
13
ΜK
K
WHERE
ΜK
K
ΜK
WE
CAN
WRITE
THIS
IN
EXPONENTIAL
FAMILY
FORM
AS
FOLLOWS
CAT
X
Θ
EXP
ΘT
Φ
X
A
Θ
Θ
LOG
LOG
ΜK
ΜK
ΜK
Φ
X
I
X
I
X
K
WE
CAN
RECOVER
THE
MEAN
PARAMETERS
FROM
THE
CANONICAL
PARAMETERS
USING
EΘK
ΜK
K
EΘJ
FROM
THIS
WE
FIND
K
EΘJ
AND
HENCE
A
Θ
LOG
K
K
EΘK
IF
WE
DEFINE
ΘK
WE
CAN
WRITE
Μ
Θ
AND
A
Θ
LOG
K
EΘK
WHERE
IS
THE
SOFTMAX
FUNCTION
IN
EQUATION
UNIVARIATE
GAUSSIAN
THE
UNIVARIATE
GAUSSIAN
CAN
BE
WRITTEN
IN
EXPONENTIAL
FAMILY
FORM
AS
FOLLOWS
N
X
Μ
Σ
EXP
X
Μ
20
EXP
Μ
X
WHERE
Z
Θ
EXP
ΘT
Φ
X
Μ
Θ
Φ
X
X
Z
Μ
Σ
EXP
2Σ2
A
Θ
LOG
LOG
4
NON
EXAMPLES
NOT
ALL
DISTRIBUTIONS
OF
INTEREST
BELONG
TO
THE
EXPONENTIAL
FAMILY
FOR
EXAMPLE
THE
UNIFORM
DISTRIBUTION
X
UNIF
A
B
DOES
NOT
SINCE
THE
SUPPORT
OF
THE
DISTRIBUTION
DEPENDS
ON
THE
PARAMETERS
ALSO
THE
STUDENT
T
DISTRIBUTION
SECTION
4
5
DOES
NOT
BELONG
SINCE
IT
DOES
NOT
HAVE
THE
REQUIRED
FORM
3
LOG
PARTITION
FUNCTION
AN
IMPORTANT
PROPERTY
OF
THE
EXPONENTIAL
FAMILY
IS
THAT
DERIVATIVES
OF
THE
LOG
PARTITION
FUNCTION
CAN
BE
USED
TO
GENERATE
CUMULANTS
OF
THE
SUFFICIENT
STATISTICS
FOR
THIS
REASON
A
Θ
IS
SOMETIMES
CALLED
A
CUMULANT
FUNCTION
WE
WILL
PROVE
THIS
FOR
A
PARAMETER
DISTRIBUTION
THIS
CAN
BE
GENERALIZED
TO
A
K
PARAMETER
DISTRIBUTION
IN
A
STRAIGHTFORWARD
WAY
FOR
THE
FIRST
THE
FIRST
AND
SECOND
CUMULANTS
OF
A
DISTRIBUTION
ARE
ITS
MEAN
E
X
AND
VARIANCE
VAR
X
WHEREAS
THE
FIRST
AND
SECOND
MOMENTS
ARE
ITS
MEAN
E
X
AND
E
DERIVATIVE
WE
HAVE
DA
D
DΘ
DΘ
LOG
EXP
ΘΦ
X
H
X
DX
D
EXP
ΘΦ
X
H
X
DX
R
EXP
ΘΦ
X
H
X
DX
Φ
X
EXP
ΘΦ
X
H
X
DX
EXP
A
Θ
Φ
X
EXP
ΘΦ
X
A
Θ
H
X
DX
30
Φ
X
P
X
DX
E
Φ
X
FOR
THE
SECOND
DERIVATIVE
WE
HAVE
Φ
X
EXP
ΘΦ
X
A
Θ
H
X
Φ
X
AT
Θ
DX
Φ
X
P
X
Φ
X
AT
Θ
DX
X
P
X
DX
AT
Θ
Φ
X
P
X
DX
E
X
E
Φ
X
VAR
Φ
X
35
WHERE
WE
USED
THE
FACT
THAT
AT
Θ
DA
E
Φ
X
IN
THE
MULTIVARIATE
CASE
WE
HAVE
THAT
Θ
Θ
E
ΦI
X
ΦJ
X
E
ΦI
X
E
ΦJ
X
I
J
AND
HENCE
A
Θ
COV
Φ
X
SINCE
THE
COVARIANCE
IS
POSITIVE
DEFINITE
WE
SEE
THAT
A
Θ
IS
A
CONVEX
FUNCTION
SEE
SECTION
3
3
3
EXAMPLE
THE
BERNOULLI
DISTRIBUTION
FOR
EXAMPLE
CONSIDER
THE
BERNOULLI
DISTRIBUTION
WE
HAVE
A
Θ
LOG
EΘ
SO
THE
MEAN
IS
GIVEN
BY
DA
EΘ
DΘ
EΘ
E
Θ
SIGM
Θ
Μ
THE
VARIANCE
IS
GIVEN
BY
D
E
DΘ
Θ
E
Θ
E
Θ
E
Θ
E
Θ
E
Θ
EΘ
E
Θ
Μ
Μ
40
4
MLE
FOR
THE
EXPONENTIAL
FAMILY
THE
LIKELIHOOD
OF
AN
EXPONENTIAL
FAMILY
MODEL
HAS
THE
FORM
N
P
D
Θ
I
H
XI
G
Θ
N
EXP
FΗ
Θ
T
I
Φ
XI
WE
SEE
THAT
THE
SUFFICIENT
STATISTICS
ARE
N
AND
N
N
Φ
D
XI
ΦK
XI
FOR
EXAMPLE
FOR
THE
BER
NOULLI
MODEL
WE
HAVE
Φ
I
I
XI
AND
FOR
THE
UNIVARIATE
THE
PITMAN
KOOPMAN
DARMOIS
THEOREM
STATES
THAT
UNDER
CERTAIN
REGULARITY
CONDITIONS
THE
EXPONENTIAL
FAMILY
IS
THE
ONLY
FAMILY
OF
DISTRIBUTIONS
WITH
FINITE
SUFFICIENT
STATISTICS
HERE
FINITE
MEANS
OF
A
SIZE
INDEPENDENT
OF
THE
SIZE
OF
THE
DATA
SET
ONE
OF
THE
CONDITIONS
REQUIRED
IN
THIS
THEOREM
IS
THAT
THE
SUPPORT
OF
THE
DISTRIBUTION
NOT
BE
DEPENDENT
ON
THE
PARAMETER
FOR
A
SIMPLE
EXAMPLE
OF
SUCH
A
DISTRIBUTION
CONSIDER
THE
UNIFORM
DISTRIBUTION
P
X
Θ
U
X
Θ
Θ
I
X
Θ
THE
LIKELIHOOD
IS
GIVEN
BY
P
D
Θ
Θ
NI
MAX
XI
Θ
SO
THE
SUFFICIENT
STATISTICS
ARE
N
AND
MAXI
XI
THIS
IS
FINITE
IN
SIZE
BUT
THE
UNI
FORM
DISTRIBUTION
IS
NOT
IN
THE
EXPONENTIAL
FAMILY
BECAUSE
ITS
SUPPORT
SET
DEPENDS
ON
THE
PARAMETERS
WE
NOW
DESCIBE
HOW
TO
COMPUTE
THE
MLE
FOR
A
CANONICAL
EXPONENTIAL
FAMILY
MODEL
GIVEN
N
IID
DATA
POINTS
D
XN
THE
LOG
LIKELIHOOD
IS
LOG
P
D
Θ
ΘT
Φ
D
N
A
Θ
SINCE
A
Θ
IS
CONCAVE
IN
Θ
AND
ΘT
Φ
IS
LINEAR
IN
Θ
WE
SEE
THAT
THE
LOG
LIKELIHOOD
IS
CONCAVE
AND
HENCE
HAS
A
UNIQUE
GLOBAL
MAXIMUM
TO
DERIVE
THIS
MAXIMUM
WE
USE
THE
FACT
THAT
THE
DERIVATIVE
OF
THE
LOG
PARTITION
FUNCTION
YIELDS
THE
EXPECTED
VALUE
OF
THE
SUFFICIENT
STATISTIC
VECTOR
SECTION
3
Θ
LOG
P
D
Θ
Φ
D
N
E
Φ
X
SETTING
THIS
GRADIENT
TO
ZERO
WE
SEE
THAT
AT
THE
MLE
THE
EMPIRICAL
AVERAGE
OF
THE
SUFFICIENT
STATISTICS
MUST
EQUAL
THE
MODEL
THEORETICAL
EXPECTED
SUFFICIENT
STATISTICS
I
E
Θˆ
MUST
SATISFY
E
Φ
X
Φ
X
THIS
IS
CALLED
MOMENT
MATCHING
FOR
EXAMPLE
IN
THE
BERNOULLI
DISTRIBUTION
WE
HAVE
Φ
X
I
X
SO
THE
MLE
SATISFIES
N
E
Φ
X
P
X
Μˆ
I
XI
N
I
5
BAYES
FOR
THE
EXPONENTIAL
FAMILY
WE
HAVE
SEEN
THAT
EXACT
BAYESIAN
ANALYSIS
IS
CONSIDERABLY
SIMPLIFIED
IF
THE
PRIOR
IS
CONJUGATE
TO
THE
LIKELIHOOD
INFORMALLY
THIS
MEANS
THAT
THE
PRIOR
P
Θ
Τ
HAS
THE
SAME
FORM
AS
THE
LIKELIHOOD
P
Θ
FOR
THIS
TO
MAKE
SENSE
WE
REQUIRE
THAT
THE
LIKELIHOOD
HAVE
FINITE
SUFFICIENT
STATISTICS
SO
THAT
WE
CAN
WRITE
P
Θ
P
Θ
THIS
SUGGESTS
THAT
THE
ONLY
FAMILY
OF
DISTRIBUTIONS
FOR
WHICH
CONJUGATE
PRIORS
EXIST
IS
THE
EXPONENTIAL
FAMILY
WE
WILL
DERIVE
THE
FORM
OF
THE
PRIOR
AND
POSTERIOR
BELOW
5
LIKELIHOOD
THE
LIKELIHOOD
OF
THE
EXPONENTIAL
FAMILY
IS
GIVEN
BY
P
D
Θ
G
Θ
EXP
Η
Θ
SN
WHERE
SN
N
XI
IN
TERMS
OF
THE
CANONICAL
PARAMETERS
THIS
BECOMES
P
D
Η
EXP
N
ΗT
NA
Η
50
WHERE
SN
5
PRIOR
THE
NATURAL
CONJUGATE
PRIOR
HAS
THE
FORM
P
Θ
Τ
G
Θ
EXP
Η
Θ
T
Τ
51
LET
US
WRITE
Τ
TO
SEPARATE
OUT
THE
SIZE
OF
THE
PRIOR
PSEUDO
DATA
FROM
THE
MEAN
OF
THE
SUFFICIENT
STATISTICS
ON
THIS
PSEUDO
DATA
Τ
IN
CANONICAL
FORM
THE
PRIOR
BECOMES
P
Η
Τ
EXP
Τ
Ν0A
Η
5
3
POSTERIOR
THE
POSTERIOR
IS
GIVEN
BY
P
Θ
D
P
Θ
ΝN
Τ
N
P
Θ
N
Τ
SN
SO
WE
SEE
THAT
WE
JUST
UPDATE
THE
HYPER
PARAMETERS
BY
ADDING
IN
CANONICAL
FORM
THIS
BECOMES
P
Η
D
EXP
ΗT
N
N
A
Η
P
Η
N
N
N
SO
WE
SEE
THAT
THE
POSTERIOR
HYPER
PARAMETERS
ARE
A
CONVEX
COMBINATION
OF
THE
PRIOR
MEAN
HYPER
PARAMETERS
AND
THE
AVERAGE
OF
THE
SUFFICIENT
STATISTICS
5
4
POSTERIOR
PREDICTIVE
DENSITY
LET
US
DERIVE
A
GENERIC
EXPRESSION
FOR
THE
PREDICTIVE
DENSITY
FOR
FUTURE
OBSERVABLES
DT
X
X
NT
GIVEN
PAST
DATA
D
XN
AS
FOLLOWS
FOR
NOTATIONAL
BREVITY
WE
WILL
COMBINE
THE
SUFFICIENT
STATISTICS
WITH
THE
SIZE
OF
THE
DATA
AS
FOLLOWS
Τ
Τ
D
N
D
AND
DT
N
T
DT
SO
THE
PRIOR
BECOMES
P
Θ
Τ
G
Θ
EXP
Η
Θ
T
Τ
Z
Τ
THE
LIKELIHOOD
AND
POSTERIOR
HAVE
A
SIMILAR
FORM
HENCE
P
DT
D
P
DT
Θ
P
Θ
D
DΘ
N
T
I
H
X
I
Z
Τ
D
G
Θ
N
N
TDΘ
EXP
ΗK
Θ
ΤK
I
SK
XI
N
T
I
SK
X
I
DΘ
TTN
T
Z
Τ
D
DT
I
H
X
I
Z
Τ
60
D
IF
N
THIS
BECOMES
THE
MARGINAL
LIKELIHOOD
OF
T
WHICH
REDUCES
TO
THE
FAMILIAR
FORM
OF
NORMALIZER
OF
THE
POSTERIOR
DIVIDED
BY
THE
NORMALIZER
OF
THE
PRIOR
MULTIPLIED
BY
A
CONSTANT
5
5
EXAMPLE
BERNOULLI
DISTRIBUTION
AS
A
SIMPLE
EXAMPLE
LET
US
REVISIT
THE
BETA
BERNOULLI
MODEL
IN
OUR
NEW
NOTATION
THE
LIKELIHOOD
IS
GIVEN
BY
Θ
I
HENCE
THE
CONJUGATE
PRIOR
IS
GIVEN
BY
P
Θ
Ν
Τ
Θ
EXP
LOG
Θ
Τ
Θ
Θ
IF
WE
DEFINE
Α
AND
Β
WE
SEE
THAT
THIS
IS
A
BETA
DISTRIBUTION
P
Θ
D
Θ
N
ΘΤN
Θ
ΝN
ΤN
WE
CAN
DERIVE
THE
POSTERIOR
PREDICTIVE
DISTRIBUTION
AS
FOLLOWS
ASSUME
P
Θ
BETA
Θ
Α
Β
AND
LET
D
BE
THE
NUMBER
OF
HEADS
IN
THE
PAST
DATA
WE
CAN
PREDICT
THE
PROBABILITY
OF
A
GIVEN
SEQUENCE
OF
FUTURE
HEADS
DT
X
X
M
WITH
SUFFICIENT
STATISTIC
ST
M
I
X
I
AS
FOLLOWS
P
DT
D
P
DT
Θ
BETA
Θ
ΑN
ΒN
DΘ
66
Γ
ΑN
ΒN
T
T
WHERE
Γ
ΑN
ΒN
Γ
ΑN
M
Γ
ΒN
M
Γ
ΑN
Γ
ΒN
Γ
ΑN
M
ΒN
M
ΑN
M
ΑN
ST
Α
S
ST
ΒN
M
ΒN
M
ST
Β
N
S
M
ST
6
MAXIMUM
ENTROPY
DERIVATION
OF
THE
EXPONENTIAL
FAMILY
ALTHOUGH
THE
EXPONENTIAL
FAMILY
IS
CONVENIENT
IS
THERE
ANY
DEEPER
JUSTIFICATION
FOR
ITS
USE
IT
TURNS
OUT
THAT
THERE
IS
IT
IS
THE
DISTRIBUTION
THAT
MAKES
THE
LEAST
NUMBER
OF
ASSUMPTIONS
ABOUT
THE
DATA
SUBJECT
TO
A
SPECIFIC
SET
OF
USER
SPECIFIED
CONSTRAINTS
AS
WE
EXPLAIN
BELOW
IN
PARTICULAR
SUPPOSE
ALL
WE
KNOW
IS
THE
EXPECTED
VALUES
OF
CERTAIN
FEATURES
OR
FUNCTIONS
FK
X
P
X
FK
X
WHERE
FK
ARE
KNOWN
CONSTANTS
AND
FK
X
IS
AN
ARBITRARY
FUNCTION
THE
PRINCIPLE
OF
MAXIMUM
ENTROPY
OR
MAXENT
SAYS
WE
SHOULD
PICK
THE
DISTRIBUTION
WITH
MAXIMUM
ENTROPY
CLOSEST
TO
UNIFORM
SUBJECT
TO
THE
CONSTRAINTS
THAT
THE
MOMENTS
OF
THE
DISTRIBUTION
MATCH
THE
EMPIRICAL
MOMENTS
OF
THE
SPECIFIED
FUNCTIONS
TO
MAXIMIZE
ENTROPY
SUBJECT
TO
THE
CONSTRAINTS
IN
EQUATION
AND
THE
CONSTRAINTS
THAT
P
X
AND
X
P
X
WE
NEED
TO
USE
LAGRANGE
MULTIPLIERS
THE
LAGRANGIAN
IS
GIVEN
BY
J
P
Λ
P
X
LOG
P
X
P
X
ΛK
FK
P
X
FK
X
WE
CAN
USE
THE
CALCULUS
OF
VARIATIONS
TO
TAKE
DERIVATIVES
WRT
THE
FUNCTION
P
BUT
WE
WILL
ADOPT
A
SIMPLER
APPROACH
AND
TREAT
P
AS
A
FIXED
LENGTH
VECTOR
SINCE
WE
ARE
ASSUMING
X
IS
DISCRETE
THEN
WE
HAVE
J
LOG
P
X
Λ
Λ
F
X
P
X
SETTING
J
YIELDS
K
K
K
P
X
EXP
Λ
F
X
W
G
ΗI
G
XI
Ψ
ΜI
ΘI
Ψ
FIGURE
A
VISUALIZATION
OF
THE
VARIOUS
FEATURES
OF
A
GLM
BASED
ON
FIGURE
3
OF
JORDAN
WHERE
Z
USING
THE
SUM
TO
ONE
CONSTRAINT
WE
HAVE
P
X
EXP
Λ
F
X
HENCE
THE
NORMALIZATION
CONSTANT
IS
GIVEN
BY
Z
EXP
ΛKFK
X
THUS
THE
MAXENT
DISTRIBUTION
P
X
HAS
THE
FORM
OF
THE
EXPONENTIAL
FAMILY
SECTION
ALSO
KNOWN
AS
THE
GIBBS
DISTRIBUTION
3
GENERALIZED
LINEAR
MODELS
GLMS
LINEAR
AND
LOGISTIC
REGRESSION
ARE
EXAMPLES
OF
GENERALIZED
LINEAR
MODELS
OR
GLMS
MCCULLAGH
AND
NELDER
THESE
ARE
MODELS
IN
WHICH
THE
OUTPUT
DENSITY
IS
IN
THE
EXPONENTIAL
FAMILY
SECTION
AND
IN
WHICH
THE
MEAN
PARAMETERS
ARE
A
LINEAR
COMBINATION
OF
THE
INPUTS
PASSED
THROUGH
A
POSSIBLY
NONLINEAR
FUNCTION
SUCH
AS
THE
LOGISTIC
FUNCTION
WE
DESCRIBE
GLMS
IN
MORE
DETAIL
BELOW
WE
FOCUS
ON
SCALAR
OUTPUTS
FOR
NOTATIONAL
SIMPLICITY
THIS
EXCLUDES
MULTINOMIAL
LOGISTIC
REGRESSION
BUT
THIS
IS
JUST
TO
SIMPLIFY
THE
PRESENTATION
3
BASICS
TO
UNDERSTAND
GLMS
LET
US
FIRST
CONSIDER
THE
CASE
OF
AN
UNCONDITIONAL
DSTRIBUTION
FOR
A
SCALAR
RESPONSE
VARIABLE
P
Y
Θ
EXP
YIΘ
A
Θ
C
Y
L
WHERE
IS
THE
DISPERSION
PARAMETER
OFTEN
SET
TO
Θ
IS
THE
NATURAL
PARAMETER
A
IS
THE
PARTITION
FUNCTION
AND
C
IS
A
NORMALIZATION
CONSTANT
FOR
EXAMPLE
IN
THE
CASE
OF
LOGISTIC
REGRESSION
Θ
IS
THE
LOG
ODDS
RATIO
Θ
LOG
Μ
WHERE
Μ
E
Y
P
Y
IS
THE
MEAN
PARAMETER
SEE
SECTION
TO
CONVERT
FROM
THE
MEAN
PARAMETER
TO
THE
NATURAL
PARAMETER
3
GENERALIZED
LINEAR
MODELS
GLMS
DISTRIB
LINK
G
Μ
Θ
Ψ
Μ
Μ
Ψ
Θ
E
Y
N
Μ
IDENTITY
Θ
Μ
Μ
Θ
BIN
N
Μ
LOGIT
Θ
LOG
Μ
Μ
Μ
SIGM
Θ
Θ
POI
Μ
LOG
Θ
LOG
Μ
Μ
E
TABLE
CANONICAL
LINK
FUNCTIONS
Ψ
AND
THEIR
INVERSES
FOR
SOME
COMMON
GLMS
WE
CAN
USE
A
FUNCTION
Ψ
SO
Θ
Ψ
Μ
THIS
FUNCTION
IS
UNIQUELY
DETERMINED
BY
THE
FORM
OF
THE
EXPONENTIAL
FAMILY
DISTRIBUTION
IN
FACT
THIS
IS
AN
INVERTIBLE
MAPPING
SO
WE
HAVE
Μ
Ψ
Θ
FURTHERMORE
WE
KNOW
FROM
SECTION
3
THAT
THE
MEAN
IS
GIVEN
BY
THE
DERIVATIVE
OF
THE
PARTITION
FUNCTION
SO
WE
HAVE
Μ
Ψ
Θ
AT
Θ
NOW
LET
US
ADD
INPUTS
COVARIATES
WE
FIRST
DEFINE
A
LINEAR
FUNCTION
OF
THE
INPUTS
ΗI
WT
XI
WE
NOW
MAKE
THE
MEAN
OF
THE
DISTRIBUTION
BE
SOME
INVERTIBLE
MONOTONIC
FUNCTION
OF
THIS
LINEAR
COMBINATION
BY
CONVENTION
THIS
FUNCTION
KNOWN
AS
THE
MEAN
FUNCTION
IS
DENOTED
BY
G
SO
ΜI
G
ΗI
G
WT
XI
SEE
FIGURE
FOR
A
SUMMARY
OF
THE
BASIC
MODEL
THE
INVERSE
OF
THE
MEAN
FUNCTION
NAMELY
G
IS
CALLED
THE
LINK
FUNCTION
WE
ARE
FREE
TO
CHOOSE
ALMOST
ANY
FUNCTION
WE
LIKE
FOR
G
SO
LONG
AS
IT
IS
INVERTIBLE
AND
SO
LONG
AS
G
HAS
THE
APPROPRIATE
RANGE
FOR
EXAMPLE
IN
LOGISTIC
REGRESSION
WE
SET
ΜI
G
ΗI
SIGM
ΗI
ONE
PARTICULARLY
SIMPLE
FORM
OF
LINK
FUNCTION
IS
TO
USE
G
Ψ
THIS
IS
CALLED
THE
CANONICAL
LINK
FUNCTION
IN
THIS
CASE
ΘI
ΗI
WT
XI
SO
THE
MODEL
BECOMES
YIWT
XI
A
WT
XI
L
IN
TABLE
WE
LIST
SOME
DISTRIBUTIONS
AND
THEIR
CANONICAL
LINK
FUNCTIONS
WE
SEE
THAT
FOR
THE
BERNOULLI
BINOMIAL
DISTRIBUTION
THE
CANONICAL
LINK
IS
THE
LOGIT
FUNCTION
G
Μ
LOG
Η
Η
WHOSE
INVERSE
IS
THE
LOGISTIC
FUNCTION
Μ
SIGM
Η
BASED
ON
THE
RESULTS
IN
SECTION
3
WE
CAN
SHOW
THAT
THE
MEAN
AND
VARIANCE
OF
THE
RESPONSE
VARIABLE
ARE
AS
FOLLOWS
E
Y
XI
W
ΜI
AT
ΘI
VAR
Y
XI
W
ATT
ΘI
TO
MAKE
THE
NOTATION
CLEARER
LET
US
CONSIDER
SOME
SIMPLE
EXAMPLES
FOR
LINEAR
REGRESSION
WE
HAVE
LOG
P
YI
XI
W
YIΜI
Μ2
LOG
WHERE
YI
R
AND
ΘI
ΜI
WT
XI
HERE
A
Θ
SO
E
YI
ΜI
AND
VAR
YI
FOR
BINOMIAL
REGRESSION
WE
HAVE
LOG
P
Y
X
W
Y
LOG
ΠI
N
LOG
Π
LOG
NI
I
I
I
ΠI
I
YI
WHERE
YI
NI
ΠI
SIGM
WT
XI
ΘI
LOG
ΠI
ΠI
WT
XI
AND
HERE
A
Θ
NI
LOG
EΘ
SO
E
YI
NIΠI
ΜI
VAR
YI
NIΠI
ΠI
FOR
POISSON
REGRESSION
WE
HAVE
LOG
P
YI
XI
W
YI
LOG
ΜI
ΜI
LOG
YI
WHERE
YI
ΜI
EXP
WT
XI
ΘI
LOG
ΜI
WT
XI
AND
HERE
A
Θ
EΘ
SO
E
YI
VAR
YI
ΜI
POISSON
REGRESSION
IS
WIDELY
USED
IN
BIO
STATISTICAL
APPLICATIONS
WHERE
YI
MIGHT
REPRESENT
THE
NUMBER
OF
DISEASES
OF
A
GIVEN
PERSON
OR
PLACE
OR
THE
NUMBER
OF
READS
AT
A
GENOMIC
LOCATION
IN
A
HIGH
THROUGHPUT
SEQUENCING
CONTEXT
SEE
E
G
KUAN
ET
AL
3
ML
AND
MAP
ESTIMATION
ONE
OF
THE
APPEALING
PROPERTIES
OF
GLMS
IS
THAT
THEY
CAN
BE
FIT
USING
EXACTLY
THE
SAME
METHODS
THAT
WE
USED
TO
FIT
LOGISTIC
REGRESSION
IN
PARTICULAR
THE
LOG
LIKELIHOOD
HAS
THE
FOLLOWING
FORM
W
LOG
P
D
W
N
I
I
86
I
ΘIYI
A
ΘI
WE
CAN
COMPUTE
THE
GRADIENT
VECTOR
USING
THE
CHAIN
RULE
AS
FOLLOWS
D
I
DWJ
D
I
DΘI
DΜI
DΗI
DΘI
DΜI
DΗI
DWJ
Y
AT
Θ
DΘI
DΜI
X
I
I
DΜI
DΗI
IJ
Y
DΘI
DΜI
Μ
X
I
I
DΜI
DΗI
IJ
IF
WE
USE
A
CANONICAL
LINK
ΘI
ΗI
THIS
SIMPLIFIES
TO
W
W
N
I
YI
ΜI
XI
WHICH
IS
A
SUM
OF
THE
INPUT
VECTORS
WEIGHTED
BY
THE
ERRORS
THIS
CAN
BE
USED
INSIDE
A
STOCHASTIC
GRADIENT
DESCENT
PROCEDURE
DISCUSSED
IN
SECTION
5
HOWEVER
FOR
IMPROVED
EFFICIENCY
WE
SHOULD
USE
A
SECOND
ORDER
METHOD
IF
WE
USE
A
CANONICAL
LINK
THE
HESSIAN
IS
GIVEN
BY
H
DΜI
X
XT
XT
SX
NAME
FORMULA
LOGISTIC
G
Η
SIGM
Η
EΗ
PROBIT
G
Η
Φ
Η
LOG
LOG
G
Η
EXP
EXP
Η
COMPLEMENTARY
LOG
LOG
G
Η
EXP
EXP
Η
TABLE
SUMMARY
OF
SOME
POSSIBLE
MEAN
FUNCTIONS
FOR
BINARY
REGRESSION
WHERE
S
DIAG
DΜN
IS
A
DIAGONAL
WEIGHTING
MATRIX
THIS
CAN
BE
USED
INSIDE
THE
DΘN
IRLS
ALGORITHM
SECTION
3
4
SPECIFICALLY
WE
HAVE
THE
FOLLOWING
NEWTON
UPDATE
WT
XT
STX
STZT
93
ZT
ΘT
S
T
Y
ΜT
WHERE
ΘT
XWT
AND
ΜT
G
ΗT
IF
WE
EXTEND
THE
DERIVATION
TO
HANDLE
NON
CANONICAL
LINKS
WE
FIND
THAT
THE
HESSIAN
HAS
ANOTHER
TERM
HOWEVER
IT
TURNS
OUT
THAT
THE
EXPECTED
HESSIAN
IS
THE
SAME
AS
IN
EQUATION
USING
THE
EXPECTED
HESSIAN
KNOWN
AS
THE
FISHER
INFORMATION
MATRIX
INSTEAD
OF
THE
ACTUAL
HESSIAN
IS
KNOWN
AS
THE
FISHER
SCORING
METHOD
IT
IS
STRAIGHTFORWARD
TO
MODIFY
THE
ABOVE
PROCEDURE
TO
PERFORM
MAP
ESTIMATION
WITH
A
GAUS
SIAN
PRIOR
WE
JUST
MODIFY
THE
OBJECTIVE
GRADIENT
AND
HESSIAN
JUST
AS
WE
ADDED
REGULARIZATION
TO
LOGISTIC
REGRESSION
IN
SECTION
3
6
3
3
BAYESIAN
INFERENCE
BAYESIAN
INFERENCE
FOR
GLMS
IS
USUALLY
CONDUCTED
USING
MCMC
CHAPTER
POSSIBLE
METHODS
INCLUDE
METROPOLIS
HASTINGS
WITH
AN
IRLS
BASED
PROPOSAL
GAMERMAN
GIBBS
SAMPLING
USING
ADAPTIVE
REJECTION
SAMPLING
ARS
FOR
EACH
FULL
CONDITIONAL
DELLAPORTAS
AND
SMITH
ETC
SEE
E
G
DEY
ET
AL
FOR
FUTHER
INFORMATION
IT
IS
ALSO
POSSIBLE
TO
USE
THE
GAUSSIAN
APPROXIMATION
SECTION
4
OR
VARIATIONAL
INFERENCE
SECTION
8
4
PROBIT
REGRESSION
IN
BINARY
LOGISTIC
REGRESSION
WE
USE
A
MODEL
OF
THE
FORM
P
Y
XI
W
SIGM
WT
XI
IN
GENERAL
WE
CAN
WRITE
P
Y
XI
W
G
WT
XI
FOR
ANY
FUNCTION
G
THAT
MAPS
TO
SEVERAL
POSSIBLE
MEAN
FUNCTIONS
ARE
LISTED
IN
TABLE
IN
THIS
SECTION
WE
FOCUS
ON
THE
CASE
WHERE
G
Η
Φ
Η
WHERE
Φ
Η
IS
THE
CDF
OF
THE
STANDARD
NORMAL
THIS
IS
KNOWN
AS
PROBIT
REGRESSION
THE
PROBIT
FUNCTION
IS
VERY
SIMILAR
TO
THE
LOGISTIC
FUNCTION
AS
SHOWN
IN
FIGURE
8
7
B
HOWEVER
THIS
MODEL
HAS
SOME
ADVANTAGES
OVER
LOGISTIC
REGRESSION
AS
WE
WILL
SEE
4
ML
MAP
ESTIMATION
USING
GRADIENT
BASED
OPTIMIZATION
WE
CAN
FIND
THE
MLE
FOR
PROBIT
REGRESSION
USING
STANDARD
GRADIENT
METHODS
LET
ΜI
WT
XI
AND
LET
Y
I
THEN
THE
GRADIENT
OF
THE
LOG
LIKELIHOD
FOR
A
SPECIFIC
CASE
IS
GIVEN
BY
G
D
LOG
P
Y
WT
X
DΜI
D
LOG
P
Y
WT
X
X
Y
IΦ
ΜI
I
DW
I
I
DW
DΜI
I
I
I
Φ
Y
IΜI
WHERE
Φ
IS
THE
STANDARD
NORMAL
PDF
AND
Φ
IS
ITS
CDF
SIMILARLY
THE
HESSIAN
FOR
A
SINGLE
CASE
IS
GIVEN
BY
D
T
HI
LOG
P
Y
I
W
XI
XI
Φ
Μ
Φ
Y
Μ
Y
IΜIΦ
ΜI
Φ
Y
Μ
XT
I
I
I
I
WE
CAN
MODIFY
THESE
EXPRESSIONS
TO
COMPUTE
THE
MAP
ESTIMATE
IN
A
STRAIGHTFORWARD
MANNER
IN
PARTICULAR
IF
WE
USE
THE
PRIO
R
P
W
N
V0
THE
GRADIENT
AND
HESSIAN
OF
THE
PENALIZED
4
LATENT
VARIABLE
INTERPRETATION
WE
CAN
INTERPRET
THE
PROBIT
AND
LOGISTIC
MODEL
AS
FOLLOWS
FIRST
LET
US
ASSOCIATE
EACH
ITEM
XI
WITH
TWO
LATENT
UTILITIES
AND
CORRESPONDING
TO
THE
POSSIBLE
CHOICES
OF
YI
AND
YI
WE
THEN
ASSUME
THAT
THE
OBSERVED
CHOICE
IS
WHICHEVER
ACTION
HAS
LARGER
UTILITY
MORE
PRECISELY
THE
MODEL
IS
AS
FOLLOWS
WT
XI
WT
XI
YI
I
99
WHERE
Δ
S
ARE
ERROR
TERMS
REPRESENTING
ALL
THE
OTHER
FACTORS
THAT
MIGHT
BE
RELEVANT
IN
DECISION
MAKING
THAT
WE
HAVE
CHOSEN
NOT
TO
OR
ARE
UNABLE
TO
MODEL
THIS
IS
CALLED
A
RANDOM
UTILITY
MODEL
OR
RUM
MCFADDEN
TRAIN
SINCE
IT
IS
ONLY
THE
DIFFERENCE
IN
UTILITIES
THAT
MATTERS
LET
US
DEFINE
ZI
EI
WHERE
EI
IF
THE
Δ
S
HAVE
A
GAUSSIAN
DISTRIBUTION
THEN
SO
DOES
EI
THUS
WE
CAN
WRITE
ZI
WT
XI
EI
100
EI
N
YI
I
ZI
102
FOLLOWING
FRUHWIRTH
SCHNATTER
AND
FRUHWIRTH
WE
CALL
THIS
THE
DIFFERENCE
RUM
OR
DRUM
MODEL
WHEN
WE
MARGINALIZE
OUT
ZI
WE
RECOVER
THE
PROBIT
MODEL
P
YI
XI
W
I
ZI
N
ZI
WT
XI
DZI
P
WT
XI
E
P
E
WT
XI
Φ
WT
XI
Φ
WT
XI
WHERE
WE
USED
THE
SYMMETRY
OF
THE
GAUSSIAN
3
THIS
LATENT
VARIABLE
INTERPRETATION
PROVIDES
AN
ALTERNATIVE
WAY
TO
FIT
THE
MODEL
AS
DISCUSSED
IN
SECTION
4
6
INTERESTINGLY
IF
WE
USE
A
GUMBEL
DISTRIBUTION
FOR
THE
Δ
S
WE
INDUCE
A
LOGISTIC
DISTIBUTION
FOR
EI
AND
THE
MODEL
REDUCES
TO
LOGISTIC
REGRESSION
SEE
SECTION
5
FOR
FURTHER
DETAILS
4
3
ORDINAL
PROBIT
REGRESSION
ONE
ADVANTAGE
OF
THE
LATENT
VARIABLE
INTERPRETATION
OF
PROBIT
REGRESSION
IS
THAT
IT
IS
EASY
TO
EXTEND
TO
THE
CASE
WHERE
THE
RESPONSE
VARIABLE
IS
ORDINAL
THAT
IS
IT
CAN
TAKE
ON
C
DISCRETE
VALUES
WHICH
CAN
BE
ORDERED
IN
SOME
WAY
SUCH
AS
LOW
MEDIUM
AND
HIGH
THIS
IS
CALLED
ORDINAL
REGRESSION
THE
BASIC
IDEA
IS
AS
FOLLOWS
WE
INTRODUCE
C
THRESHOLDS
ΓJ
AND
SET
YI
J
IF
ΓJ
ZI
ΓJ
106
WHERE
ΓC
FOR
IDENTIFIABILITY
REASONS
WE
SET
AND
ΓC
FOR
EXAMPLE
IF
C
THIS
REDUCES
TO
THE
STANDARD
BINARY
PROBIT
MODEL
WHEREBY
ZI
PRODUCES
YI
AND
ZI
PRODUCES
YI
IF
C
3
WE
PARTITION
THE
REAL
LINE
INTO
3
INTERVALS
WE
CAN
VARY
THE
PARAMETER
TO
ENSURE
THE
RIGHT
RELATIVE
AMOUNT
OF
PROBABILITY
MASS
FALLS
IN
EACH
INTERVAL
SO
AS
TO
MATCH
THE
EMPIRICAL
FREQUENCIES
OF
EACH
CLASS
LABEL
FINDING
THE
MLES
FOR
THIS
MODEL
IS
A
BIT
TRICKIER
THAN
FOR
BINARY
PROBIT
REGRESSION
SINCE
WE
NEED
TO
OPTIMIZE
FOR
W
AND
Γ
AND
THE
LATTER
MUST
OBEY
AN
ORDERING
CONSTRAINT
SEE
E
G
KAWAKATSU
AND
LARGEY
FOR
AN
APPROACH
BASED
ON
EM
IT
IS
ALSO
POSSIBLE
TO
DERIVE
A
SIMPLE
GIBBS
SAMPLING
ALGORITHM
FOR
THIS
MODEL
SEE
E
G
HOFF
4
4
MULTINOMIAL
PROBIT
MODELS
NOW
CONSIDER
THE
CASE
WHERE
THE
RESPONSE
VARIABLE
CAN
TAKE
ON
C
UNORDERED
CATEGORICAL
VALUES
YI
C
THE
MULTINOMIAL
PROBIT
MODEL
IS
DEFINED
AS
FOLLOWS
ZIC
WT
XIC
EIC
E
N
R
YI
ARG
MAX
ZIC
C
SEE
E
G
DOW
AND
ENDERSBY
SCOTT
FRUHWIRTH
SCHNATTER
AND
FRUHWIRTH
FOR
MORE
DETAILS
ON
THE
MODEL
AND
ITS
CONNECTION
TO
MULTINOMIAL
LOGISTIC
REGRESSION
BY
DEFINING
W
WC
AND
XIC
XI
WE
CAN
RECOVER
THE
MORE
FAMILIAR
FORMULATION
ZIC
XT
WC
SINCE
ONLY
RELATIVE
UTILITIES
MATTER
WE
CONSTRAIN
R
TO
BE
A
CORRELATION
MATRIX
IF
INSTEAD
OF
SETTING
YI
ARGMAXC
ZIC
WE
USE
YIC
I
ZIC
WE
GET
A
MODEL
KNOWN
AS
MULTIVARIATE
PROBIT
WHICH
IS
ONE
WAY
TO
MODEL
C
CORRELATED
BINARY
OUTCOMES
SEE
E
G
TALHOUK
ET
AL
3
NOTE
THAT
THE
ASSUMPTION
THAT
THE
GAUSSIAN
NOISE
TERM
IS
ZERO
MEAN
AND
UNIT
VARIANCE
IS
MADE
WITHOUT
LOSS
OF
GENERALITY
TO
SEE
WHY
SUPPOSE
WE
USED
SOME
OTHER
MEAN
Μ
AND
VARIANCE
THEN
WE
COULD
EASILY
RESCALE
W
AND
ADD
AN
OFFSET
TERM
WITHOUT
CHANGING
THE
LIKELIHOOD
SINCE
P
N
WT
X
P
N
Μ
WT
X
Μ
Σ
5
MULTI
TASK
LEARNING
SOMETIMES
WE
WANT
TO
FIT
MANY
RELATED
CLASSIFICATION
OR
REGRESSION
MODELS
IT
IS
OFTEN
REASONABLE
TO
ASSUME
THE
INPUT
OUTPUT
MAPPING
IS
SIMILAR
ACROSS
THESE
DIFFERENT
MODELS
SO
WE
CAN
GET
BETTER
PERFORMANCE
BY
FITTING
ALL
THE
PARAMETERS
AT
THE
SAME
TIME
IN
MACHINE
LEARNING
THIS
SETUP
IS
OFTEN
CALLED
MULTI
TASK
LEARNING
CARUANA
TRANSFER
LEARNING
E
G
RAINA
ET
AL
OR
LEARNING
TO
LEARN
THRUN
AND
PRATT
IN
STATISTICS
THIS
IS
USUALLY
TACKLED
USING
HIERARCHICAL
BAYESIAN
MODELS
BAKKER
AND
HESKES
AS
WE
DISCUSS
BELOW
ALTHOUGH
THERE
ARE
OTHER
POSSIBLE
METHODS
SEE
E
G
CHAI
5
HIERARCHICAL
BAYES
FOR
MULTI
TASK
LEARNING
LET
YIJ
BE
THE
RESPONSE
OF
THE
I
TH
ITEM
IN
GROUP
J
FOR
I
NJ
AND
J
J
FOR
EXAMPLE
J
MIGHT
INDEX
SCHOOLS
I
MIGHT
INDEX
STUDENTS
WITHIN
A
SCHOOL
AND
YIJ
MIGHT
BE
THE
TEST
SCORE
AS
IN
SECTION
5
6
OR
J
MIGHT
INDEX
PEOPLE
AND
I
MIGHT
INDEX
PURCHAES
AND
YIJ
MIGHT
BE
THE
IDENTITY
OF
THE
ITEM
THAT
WAS
PURCHASED
THIS
IS
KNOWN
AS
DISCRETE
CHOICE
MODELING
TRAIN
LET
XIJ
BE
A
FEATURE
VECTOR
ASSOCIATED
WITH
YIJ
THE
GOAL
IS
TO
FIT
THE
MODELS
P
YJ
XJ
FOR
ALL
J
ALTHOUGH
SOME
GROUPS
MAY
HAVE
LOTS
OF
DATA
THERE
IS
OFTEN
A
LONG
TAIL
WHERE
THE
MAJORITY
OF
GROUPS
HAVE
LITTLE
DATA
THUS
WE
CAN
T
RELIABLY
FIT
EACH
MODEL
SEPARATELY
BUT
WE
DON
T
WANT
TO
USE
THE
SAME
MODEL
FOR
ALL
GROUPS
AS
A
COMPROMISE
WE
CAN
FIT
A
SEPARATE
MODEL
FOR
EACH
GROUP
BUT
ENCOURAGE
THE
MODEL
PARAMETERS
TO
BE
SIMILAR
ACROSS
GROUPS
MORE
PRECISELY
SUPPOSE
E
YIJ
XIJ
G
XT
ΒJ
WHERE
G
IS
THE
LINK
FUNCTION
FOR
THE
GLM
FURTHERMORE
SUPPOSE
ΒJ
N
Β
AND
THAT
Β
N
Μ
IN
THIS
MODEL
GROUPS
WITH
SMALL
SAMPLE
SIZE
BORROW
STATISTICAL
STRENGTH
FROM
THE
GROUPS
WITH
LARGER
SAMPLE
SIZE
BECAUSE
THE
ΒJ
S
ARE
CORRELATED
VIA
THE
LATENT
COMMON
PARENTS
Β
SEE
SECTION
5
5
FOR
FURTHER
DISCUSSION
OF
THIS
POINT
THE
TERM
CONTROLS
HOW
MUCH
GROUP
J
DEPENDS
ON
THE
COMMON
PARENTS
AND
THE
TERM
J
CONTROLS
THE
STRENGTH
OF
THE
OVERALL
PRIOR
SUPPOSE
FOR
SIMPLICITY
THAT
Μ
AND
THAT
AND
ARE
ALL
KNOWN
E
G
THEY
COULD
BE
SET
J
BY
CROSS
VALIDATION
THE
OVERALL
LOG
PROBABILITY
HAS
THE
FORM
1
ΒJ
Β
Β
WE
CAN
PERFORM
MAP
ESTIMATION
OF
Β
J
Β
USING
STANDARD
GRADIENT
METHODS
ALTER
NATIVELY
WE
CAN
PERFORM
AN
ITERATIVE
OPTIMIZATION
SCHEME
ALTERNATING
BETWEEN
OPTIMIZING
THE
ΒJ
AND
THE
Β
SINCE
THE
LIKELIHOOD
AND
PRIOR
ARE
CONVEX
THIS
IS
GUARANTEED
TO
CONVERGE
TO
THE
GLOBAL
OPTIMUM
NOTE
THAT
ONCE
THE
MODELS
ARE
TRAINED
WE
CAN
DISCARD
Β
AND
USE
EACH
MODEL
SEPARATELY
5
APPLICATION
TO
PERSONALIZED
EMAIL
SPAM
FILTERING
AN
INTERESTING
APPLICATION
OF
MULTI
TASK
LEARNING
IS
PERSONALIZED
SPAM
FILTERING
SUPPOSE
WE
WANT
TO
FIT
ONE
CLASSIFIER
PER
USER
ΒJ
SINCE
MOST
USERS
DO
NOT
LABEL
THEIR
EMAIL
AS
SPAM
OR
NOT
IT
WILL
BE
HARD
TO
ESTIMATE
THESE
MODELS
INDEPENDENTLY
SO
WE
WILL
LET
THE
ΒJ
HAVE
A
COMMON
PRIOR
Β
REPRESENTING
THE
PARAMETERS
OF
A
GENERIC
USER
IN
THIS
CASE
WE
CAN
EMULATE
THE
BEHAVIOR
OF
THE
ABOVE
MODEL
WITH
A
SIMPLE
TRICK
DAUME
ATTENBERG
ET
AL
WEINBERGER
ET
AL
WE
MAKE
TWO
COPIES
OF
EACH
FEATURE
XI
ONE
CONCATENATED
WITH
THE
USER
ID
AND
ONE
NOT
THE
EFFECT
WILL
BE
TO
LEARN
A
PREDICTOR
OF
THE
FORM
E
YI
XI
U
Β
WJ
T
XI
I
U
1
XI
I
U
J
XI
WHERE
U
IS
THE
USER
ID
IN
OTHER
WORDS
E
YI
XI
U
J
ΒT
WJ
T
XI
THUS
Β
WILL
BE
ESTIMATED
FROM
EVERYONE
S
EMAIL
WHEREAS
WJ
WILL
JUST
BE
ESTIMATED
FROM
USER
J
S
EMAIL
TO
SEE
THE
CORRESPONDENCE
WITH
THE
ABOVE
HIERARCHICAL
BAYESIAN
MODEL
DEFINE
WJ
ΒJ
Β
THEN
THE
LOG
PROBABILITY
OF
THE
ORIGINAL
MODEL
CAN
BE
REWRITTEN
AS
P
DJ
Β
WJ
WJ
2Σ2
Β
2Σ
IF
WE
ASSUME
THE
EFFECT
IS
THE
SAME
AS
USING
THE
AUGMENTED
FEATURE
TRICK
WITH
THE
J
SAME
REGULARIZER
STRENGTH
FOR
BOTH
WJ
AND
Β
HOWEVER
ONE
TYPICALLY
GETS
BETTER
PERFORMANCE
BY
NOT
REQUIRING
THAT
BE
EQUAL
TO
FINKEL
AND
MANNING
J
5
3
APPLICATION
TO
DOMAIN
ADAPTATION
DOMAIN
ADAPTATION
IS
THE
PROBLEM
OF
TRAINING
A
SET
OF
CLASSIFIERS
ON
DATA
DRAWN
FROM
DIFFERENT
DISTRIBUTIONS
SUCH
AS
EMAIL
AND
NEWSWIRE
TEXT
THIS
PROBLEM
IS
OBVIOUSLY
A
SPECIAL
CASE
OF
MULTI
TASK
LEARNING
WHERE
THE
TASKS
ARE
THE
SAME
FINKEL
AND
MANNING
USED
THE
ABOVE
HIERARCHICAL
BAYESIAN
MODEL
TO
PERFORM
DOMAIN
ADAPTATION
FOR
TWO
NLP
TASKS
NAMELY
NAMED
ENTITY
RECOGNITION
AND
PARSING
THEY
REPORT
REASON
ABLY
LARGE
IMPROVEMENTS
OVER
FITTING
SEPARATE
MODELS
TO
EACH
DATASET
AND
SMALL
IMPROVEMENTS
OVER
THE
APPROACH
OF
POOLING
ALL
THE
DATA
AND
FITTING
A
SINGLE
MODEL
5
4
OTHER
KINDS
OF
PRIOR
IN
MULTI
TASK
LEARNING
IT
IS
COMMON
TO
ASSUME
THAT
THE
PRIOR
IS
GAUSSIAN
HOWEVER
SOMETIMES
OTHER
PRIORS
ARE
MORE
SUITABLE
FOR
EXAMPLE
CONSIDER
THE
TASK
OF
CONJOINT
ANALYSIS
WHICH
REQUIRES
FIGURING
OUT
WHICH
FEATURES
OF
A
PRODUCT
CUSTOMERS
LIKE
BEST
THIS
CAN
BE
MODELLED
USING
THE
SAME
HIERARCHICAL
BAYESIAN
SETUP
AS
ABOVE
BUT
WHERE
WE
USE
A
SPARSITY
PROMOTING
PRIOR
ON
ΒJ
RATHER
THAN
A
GAUSSIAN
PRIOR
THIS
IS
CALLED
MULTI
TASK
FEATURE
SELECTION
SEE
E
G
LENK
ET
AL
ARGYRIOU
ET
AL
FOR
SOME
POSSIBLE
APPROACHES
IT
IS
NOT
ALWAYS
REASONABLE
TO
ASSUME
THAT
ALL
TASKS
ARE
ALL
EQUALLY
SIMILAR
IF
WE
POOL
THE
PARAMETERS
ACROSS
TASKS
THAT
ARE
QUALITATIVELY
DIFFERENT
THE
PERFORMANCE
WILL
BE
WORSE
THAN
NOT
USING
POOLING
BECAUSE
THE
INDUCTIVE
BIAS
OF
OUR
PRIOR
IS
WRONG
INDEED
IT
HAS
BEEN
FOUND
EXPERIMENTALLY
THAT
SOMETIMES
MULTI
TASK
LEARNING
DOES
WORSE
THAN
SOLVING
EACH
TASK
SEPARATELY
THIS
IS
CALLED
NEGATIVE
TRANSFER
ONE
WAY
AROUND
THIS
PROBLEM
IS
TO
USE
A
MORE
FLEXIBLE
PRIOR
SUCH
AS
A
MIXTURE
OF
GAUSSIANS
SUCH
FLEXIBLE
PRIORS
CAN
PROVIDE
ROBUSTNESS
AGAINST
PRIOR
MIS
SPECIFICATION
SEE
E
G
XUE
ET
AL
JACOB
ET
AL
FOR
DETAILS
ONE
CAN
OF
COURSE
COMBINE
MIXTURES
WITH
SPARSITY
PROMOTING
PRIORS
JI
ET
AL
MANY
OTHER
VARIANTS
ARE
POSSIBLE
6
GENERALIZED
LINEAR
MIXED
MODELS
SUPPOSE
WE
GENERALIZE
THE
MULTI
TASK
LEARNING
SCENARIO
TO
ALLOW
THE
RESPONSE
TO
INCLUDE
INFOR
MATION
AT
THE
GROUP
LEVEL
XJ
AS
WELL
AS
AT
THE
ITEM
LEVEL
XIJ
SIMILARLY
WE
CAN
ALLOW
THE
PARAMETERS
TO
VARY
ACROSS
GROUPS
ΒJ
OR
TO
BE
TIED
ACROSS
GROUPS
Α
THIS
GIVES
RISE
TO
THE
FOLLOWING
MODEL
E
YIJ
XIJ
XJ
G
XIJ
T
ΒJ
XJ
T
ΒJT
XIJ
T
Α
XJ
T
ΑT
114
WHERE
THE
ΦK
ARE
BASIS
FUNCTIONS
THIS
MODEL
CAN
BE
REPRESENTED
PICTORIALLY
AS
SHOWN
IN
FIGURE
2
A
SUCH
FIGURES
WILL
BE
EXPLAINED
IN
CHAPTER
NOTE
THAT
THE
NUMBER
OF
ΒJ
PARAMETERS
GROWS
WITH
THE
NUMBER
OF
GROUPS
WHEREAS
THE
SIZE
OF
Α
IS
FIXED
FREQUENTISTS
CALL
THE
TERMS
ΒJ
RANDOM
EFFECTS
SINCE
THEY
VARY
RANDOMLY
ACROSS
GROUPS
BUT
THEY
CALL
Α
A
FIXED
EFFECT
SINCE
IT
IS
VIEWED
AS
A
FIXED
BUT
UNKNOWN
CONSTANT
A
MODEL
WITH
BOTH
FIXED
AND
RANDOM
EFFECTS
IS
CALLED
A
MIXED
MODEL
IF
P
Y
X
IS
A
GLM
THE
OVERALL
MODEL
IS
CALLED
A
GENERALIZED
LINEAR
MIXED
EFFECTS
MODEL
OR
GLMM
SUCH
MODELS
ARE
WIDELY
USED
IN
STATISTICS
6
1
EXAMPLE
SEMI
PARAMETRIC
GLMMS
FOR
MEDICAL
DATA
CONSIDER
THE
FOLLOWING
EXAMPLE
FROM
WAND
SUPPOSE
YIJ
IS
THE
AMOUNT
OF
SPINAL
BONE
MINERAL
DENSITY
SBMD
FOR
PERSON
J
AT
MEASUREMENT
I
LET
XIJ
BE
THE
AGE
OF
PERSON
AND
LET
XJ
BE
THEIR
ETHNICITY
WHICH
CAN
BE
ONE
OF
WHITE
ASIAN
BLACK
OR
HISPANIC
THE
PRIMARY
GOAL
IS
TO
DETERMINE
IF
THERE
ARE
SIGNIFICANT
DIFFERENCES
IN
THE
MEAN
SBMD
AMONG
THE
FOUR
ETHNIC
GROUPS
AFTER
ACCOUNTING
FOR
AGE
THE
DATA
IS
SHOWN
IN
THE
LIGHT
GRAY
LINES
IN
FIGURE
2
B
WE
SEE
THAT
THERE
IS
A
NONLINEAR
EFFECT
OF
SBMD
VS
AGE
SO
WE
WILL
USE
A
SEMI
PARAMETRIC
MODEL
WHICH
COMBINES
LINEAR
REGRESSION
WITH
NON
PARAMETRIC
REGRESSION
RUPPERT
ET
AL
WE
ALSO
SEE
THAT
THERE
IS
VARIATION
ACROSS
INDIVIDUALS
WITHIN
EACH
GROUP
SO
WE
WILL
USE
A
MIXED
EFFECTS
MODEL
SPECIFICALLY
WE
WILL
USE
XIJ
1
TO
ACCOUNT
FOR
THE
RANDOM
EFFECT
OF
EACH
PERSON
XIJ
SINCE
NO
OTHER
COEFFICIENTS
ARE
PERSON
SPECIFIC
XIJ
BK
XIJ
WHERE
BK
IS
THE
K
TH
SPLINE
BASIS
FUNCTIONS
SEE
SECTION
4
6
2
TO
ACCOUNT
FOR
THE
NONLINEAR
EFFECT
OF
AGE
AND
XJ
I
XJ
W
I
XJ
A
I
XJ
B
I
XJ
H
TO
ACCOUNT
FOR
THE
EFFECT
OF
THE
DIFFERENT
ETHNICITIES
FURTHERMORE
WE
USE
A
LINEAR
LINK
FUNCTION
THE
OVERALL
MODEL
IS
THEREFORE
E
YIJ
XIJ
XJ
ΒJ
ΑT
B
XIJ
EIJ
115
ΑWT
I
XJ
W
ΑAT
I
XJ
A
ΑBT
I
XJ
B
ΑHT
I
XJ
H
116
WHERE
EIJ
Α
CONTAINS
THE
NON
PARAMETRIC
PART
OF
THE
MODEL
RELATED
TO
AGE
ΑT
CONTAINS
THE
PARAMETRIC
PART
OF
THE
MODEL
RELATED
TO
ETHNICITY
AND
ΒJ
IS
A
RANDOM
OFFSET
FOR
PERSON
J
WE
ENDOW
ALL
OF
THESE
REGRESSION
COEFFICIENTS
WITH
SEPARATE
GAUSSIAN
PRIORS
WE
CAN
THEN
PERFORM
POSTERIOR
INFERENCE
TO
COMPUTE
P
Α
ΑT
Β
Σ2
D
SEE
SECTION
6
2
FOR
3
INFERENCE
WE
HAVE
SEEN
THAT
GRAPHICAL
MODELS
PROVIDE
A
COMPACT
WAY
TO
DEFINE
JOINT
PROBABILITY
DISTRIBU
TIONS
GIVEN
SUCH
A
JOINT
DISTRIBUTION
WHAT
CAN
WE
DO
WITH
IT
THE
MAIN
USE
FOR
SUCH
A
JOINT
DISTRIBUTION
IS
TO
PERFORM
PROBABILISTIC
INFERENCE
THIS
REFERS
TO
THE
TASK
OF
ESTIMATING
UNKNOWN
QUANTITIES
FROM
KNOWN
QUANTITIES
FOR
EXAMPLE
IN
SECTION
2
2
WE
INTRODUCED
HMMS
AND
SAID
THAT
ONE
OF
THE
GOALS
IS
TO
ESTIMATE
THE
HIDDEN
STATES
E
G
WORDS
FROM
THE
OBSERVATIONS
E
G
SPEECH
SIGNAL
AND
IN
SECTION
10
2
4
WE
DISCUSSED
GENETIC
LINKAGE
ANALYSIS
AND
SAID
THAT
ONE
OF
THE
GOALS
IS
TO
ESTIMATE
THE
LIKELIHOOD
OF
THE
DATA
UNDER
VARIOUS
DAGS
CORRESPONDING
TO
DIFFERENT
HYPOTHESES
ABOUT
THE
LOCATION
OF
THE
DISEASE
CAUSING
GENE
IN
GENERAL
WE
CAN
POSE
THE
INFERENCE
PROBLEM
AS
FOLLOWS
SUPPOSE
WE
HAVE
A
SET
OF
CORRELATED
RANDOM
VARIABLES
WITH
JOINT
DISTRIBUTION
P
V
Θ
IN
THIS
SECTION
WE
ARE
ASSUMING
THE
PARAMETERS
Θ
OF
THE
MODEL
ARE
KNOWN
WE
DISCUSS
HOW
TO
LEARN
THE
PARAMETERS
IN
SECTION
10
4
LET
US
PARTITION
THIS
VECTOR
INTO
THE
VISIBLE
VARIABLES
XV
WHICH
ARE
OBSERVED
AND
THE
HIDDEN
VARIABLES
XH
WHICH
ARE
UNOBSERVED
INFERENCE
REFERS
TO
COMPUTING
THE
POSTERIOR
DISTRIBUTION
OF
THE
UNKNOWNS
GIVEN
THE
KNOWNS
VISUAL
RECOGNITION
WITH
HUMANS
IN
THE
LOOP
ABSTRACT
WE
PRESENT
AN
INTERACTIVE
HYBRID
HUMAN
COMPUTER
METHOD
FOR
OBJECT
CLASSIFICATION
THE
METHOD
APPLIES
TO
CLASSES
OF
OBJECTS
THAT
ARE
RECOGNIZABLE
BY
PEOPLE
WITH
APPROPRIATE
EXPERTISE
E
G
ANIMAL
SPECIES
OR
AIRPLANE
MODEL
BUT
NOT
IN
GENERAL
BY
PEOPLE
WITHOUT
SUCH
EXPERTISE
IT
CAN
BE
SEEN
AS
A
VISUAL
VERSION
OF
THE
QUESTIONS
GAME
WHERE
QUESTIONS
BASED
ON
SIMPLE
VISUAL
ATTRIBUTES
ARE
POSED
INTERACTIVELY
THE
GOAL
IS
TO
IDENTIFY
THE
TRUE
CLASS
WHILE
MINIMIZING
THE
NUMBER
OF
QUESTIONS
ASKED
USING
THE
VISUAL
CONTENT
OF
THE
IMAGE
WE
INTRODUCE
A
GENERAL
FRAMEWORK
FOR
INCORPORATING
ALMOST
ANY
OFF
THE
SHELF
MULTI
CLASS
OBJECT
RECOGNITION
ALGORITHM
INTO
THE
VISUAL
QUESTIONS
GAME
AND
PROVIDE
METHODOLOGIES
TO
ACCOUNT
FOR
IMPERFECT
USER
RESPONSES
AND
UNRELIABLE
COMPUTER
VISION
ALGORITHMS
WE
EVALUATE
OUR
METHODS
ON
BIRDS
A
DIFFICULT
DATASET
OF
TIGHTLY
RELATED
BIRD
SPECIES
AND
ON
THE
ANIMALS
WITH
ATTRIBUTES
DATASET
OUR
RESULTS
DEMONSTRATE
THAT
INCORPORATING
USER
INPUT
DRIVES
UP
RECOGNITION
ACCURACY
TO
LEVELS
THAT
ARE
GOOD
ENOUGH
FOR
PRACTICAL
APPLI
CATIONS
WHILE
AT
THE
SAME
TIME
COMPUTER
VISION
REDUCES
THE
AMOUNT
OF
HUMAN
INTERACTION
REQUIRED
INTRODUCTION
MULTI
CLASS
OBJECT
RECOGNITION
HAS
UNDERGONE
RAPID
CHANGE
AND
PROGRESS
OVER
THE
LAST
DECADE
THESE
ADVANCES
HAVE
LARGELY
FOCUSED
ON
TYPES
OF
OBJECT
CATEGORIES
THAT
ARE
EASY
FOR
HUMANS
TO
RECOGNIZE
SUCH
AS
MOTORBIKES
CHAIRS
HORSES
BOT
TLES
ETC
FINER
GRAINED
CATEGORIES
SUCH
AS
SPECIFIC
TYPES
OF
MOTORBIKES
CHAIRS
OR
HORSES
ARE
MORE
DIFFICULT
FOR
HUMANS
AND
HAVE
RECEIVED
COMPARATIVELY
LITTLE
ATTENTION
ONE
COULD
ARGUE
THAT
OBJECT
RECOGNITION
AS
A
FIELD
IS
SIMPLY
NOT
MA
TURE
ENOUGH
TO
TACKLE
THESE
TYPES
OF
FINER
GRAINED
CATEGORIES
PERFORMANCE
ON
BASIC
LEVEL
CATEGORIES
IS
STILL
LOWER
THAN
WHAT
PEOPLE
WOULD
CONSIDER
ACCEPTABLE
FOR
PRACTICAL
APPLICATIONS
STATE
OF
THE
ART
ACCURACY
ON
CALTECH
IS
AND
IN
THE
VOC
DETECTION
CHALLENGE
MOREOVER
THE
NUMBER
OF
OBJECT
CATEGORIES
IN
MOST
OBJECT
RECOGNITION
DATASETS
IS
STILL
FAIRLY
LOW
AND
IN
CREASING
THE
NUMBER
OF
CATEGORIES
FURTHER
IS
USUALLY
DETRIMENTAL
TO
PERFORMANCE
A
EASY
FOR
HUMANS
B
HARD
FOR
HUMANS
C
EASY
FOR
HUMANS
CHAIR
AIRPLANE
FINCH
BUNTING
YELLOW
BELLY
BLUE
BELLY
FIG
EXAMPLES
OF
CLASSIFICATION
PROBLEMS
THAT
ARE
EASY
OR
HARD
FOR
HUMANS
WHILE
BASIC
LEVEL
CATEGORY
RECOGNITION
LEFT
AND
RECOGNITION
OF
LOW
LEVEL
VISUAL
AT
TRIBUTES
RIGHT
ARE
EASY
FOR
HUMANS
MOST
PEOPLE
STRUGGLE
WITH
FINER
GRAINED
CATEGORIES
MIDDLE
BY
DEFINING
CATEGORIES
IN
TERMS
OF
LOW
LEVEL
VISUAL
PROPERTIES
HARD
CLASSIFICA
TION
PROBLEMS
CAN
BE
TURNED
INTO
A
SEQUENCE
OF
EASY
ONES
ON
THE
OTHER
HAND
RECOGNITION
OF
FINER
GRAINED
SUBORDINATE
CATEGORIES
IS
AN
IMPORTANT
PROBLEM
TO
STUDY
IT
CAN
HELP
PEOPLE
RECOGNIZE
TYPES
OF
OBJECTS
THEY
DON
T
YET
KNOW
HOW
TO
IDENTIFY
WE
BELIEVE
A
HYBRID
HUMAN
COMPUTER
RECOGNITION
METHOD
IS
A
PRACTICAL
INTERMEDIATE
SOLUTION
TOWARD
APPLYING
CONTEMPORARY
COM
PUTER
VISION
ALGORITHMS
TO
THESE
TYPES
OF
PROBLEMS
RATHER
THAN
TRYING
TO
SOLVE
OBJECT
RECOGNITION
ENTIRELY
WE
TAKE
ON
THE
OBJECTIVE
OF
MINIMIZING
THE
AMOUNT
OF
HUMAN
LABOR
REQUIRED
AS
RESEARCH
IN
OBJECT
RECOGNITION
PROGRESSES
TASKS
WILL
BECOME
INCREASINGLY
AUTOMATED
UNTIL
EVENTUALLY
WE
WILL
NO
LONGER
NEED
HUMANS
IN
THE
LOOP
THIS
APPROACH
DIFFERS
FROM
SOME
OF
THE
PREVAILING
WAYS
IN
WHICH
PEO
PLE
APPROACH
RESEARCH
IN
COMPUTER
VISION
WHERE
RESEARCHERS
BEGIN
WITH
SIMPLER
AND
LESS
REALISTIC
DATASETS
AND
PROGRESSIVELY
MAKE
THEM
MORE
DIFFICULT
AND
REALIS
TIC
AS
COMPUTER
VISION
IMPROVES
E
G
CALTECH
CALTECH
CALTECH
THE
ADVANTAGE
OF
THE
HUMAN
COMPUTER
PARADIGM
IS
THAT
WE
CAN
PROVIDE
USABLE
SERVICES
TO
PEOPLE
IN
THE
INTERIM
PERIOD
WHERE
COMPUTER
VISION
IS
STILL
UNSOLVED
THIS
MAY
HELP
INCREASE
DEMAND
FOR
COMPUTER
VISION
SPUR
DATA
COLLECTION
AND
PROVIDE
SOLUTIONS
FOR
THE
TYPES
OF
PROBLEMS
PEOPLE
OUTSIDE
THE
FIELD
WANT
SOLVED
IN
THIS
WORK
OUR
GOAL
IS
TO
PROVIDE
A
SIMPLE
FRAMEWORK
THAT
MAKES
IT
AS
EFFORTLESS
AS
POSSIBLE
FOR
RESEARCHERS
TO
PLUG
THEIR
EXISTING
ALGORITHMS
INTO
THE
HUMAN
COMPUTER
FRAMEWORK
AND
USE
HUMANS
TO
DRIVE
UP
PERFORMANCE
TO
LEV
ELS
THAT
ARE
GOOD
ENOUGH
FOR
REAL
LIFE
APPLICATIONS
IMPLICIT
TO
OUR
MODEL
IS
THE
ASSUMPTION
THAT
LAY
PEOPLE
GENERALLY
CANNOT
RECOGNIZE
FINER
GRAINED
CATEGORIES
E
G
MYRTLE
WARBLER
THRUXTON
JACKAROO
ETC
DUE
TO
IMPERFECT
MEMORY
OR
LIMITED
EXPERIENCES
HOWEVER
THEY
DO
HAVE
THE
FUNDAMENTAL
VISUAL
CAPABILITIES
TO
RECOGNIZE
THE
PARTS
AND
ATTRIBUTES
THAT
COLLECTIVELY
MAKE
RECOGNITION
POSSI
BLE
SEE
FIG
BY
CONTRAST
COMPUTERS
LACK
MANY
OF
THE
FUNDAMENTAL
VISUAL
CAPABILITIES
THAT
HUMANS
HAVE
BUT
HAVE
PERFECT
MEMORY
AND
ARE
ABLE
TO
POOL
KNOWLEDGE
COLLECTED
FROM
LARGE
GROUPS
OF
PEOPLE
USERS
INTERACT
WITH
OUR
SYSTEM
BY
ANSWERING
SIMPLE
YES
NO
OR
MULTIPLE
CHOICE
QUESTIONS
ABOUT
AN
IMAGE
OR
OB
FIG
EXAMPLES
OF
THE
VISUAL
QUESTIONS
GAME
ON
THE
CLASS
BIRD
DATASET
HUMAN
RESPONSES
SHOWN
IN
RED
TO
QUESTIONS
POSED
BY
THE
COMPUTER
SHOWN
IN
BLUE
ARE
USED
TO
DRIVE
UP
RECOGNITION
ACCURACY
IN
THE
LEFT
IMAGE
COMPUTER
VISION
ALGORITHMS
CAN
GUESS
THE
BIRD
SPECIES
CORRECTLY
WITHOUT
ANY
USER
INTERACTION
IN
THE
MIDDLE
IMAGE
COMPUTER
VISION
REDUCES
THE
NUMBER
OF
QUESTIONS
TO
IN
THE
RIGHT
IMAGE
COMPUTER
VISION
PROVIDES
LITTLE
HELP
JECT
AS
SHOWN
IN
FIG
SIMILAR
TO
THE
QUESTIONS
WE
OBSERVE
THAT
THE
NUMBER
OF
QUESTIONS
NEEDED
TO
CLASSIFY
AN
OBJECT
FROM
A
DATABASE
OF
C
CLASSES
IS
USUALLY
O
LOG
C
WHEN
USER
RESPONSES
ARE
ACCURATE
AND
CAN
BE
FASTER
WHEN
COMPUTER
VISION
IS
IN
THE
LOOP
OUR
METHOD
OF
CHOOSING
THE
NEXT
QUESTION
TO
ASK
USES
AN
INFORMATION
GAIN
CRITERION
AND
CAN
DEAL
WITH
NOISY
PROBABILISTIC
USER
RESPONSES
WE
SHOW
THAT
IT
IS
EASY
TO
INCORPORATE
ANY
COMPUTER
VISION
ALGORITHM
THAT
CAN
BE
MADE
TO
PRODUCE
A
PROBABILISTIC
OUTPUT
OVER
OBJECT
CLASSES
OUR
EXPERIMENTS
IN
THIS
PAPER
FOCUS
ON
BIRD
SPECIES
CATEGORIZATION
WHICH
WE
TAKE
TO
BE
A
REPRESENTATIVE
EXAMPLE
OF
RECOGNITION
OF
TIGHTLY
RELATED
CATEGORIES
THE
BIRD
DATASET
CONTAINS
BIRD
SPECIES
AND
OVER
IMAGES
WE
BELIEVE
THAT
SIMILAR
METHODOLOGIES
WILL
APPLY
TO
OTHER
OBJECT
DOMAINS
THE
STRUCTURE
OF
THE
PAPER
IS
AS
FOLLOWS
IN
SECTION
WE
DISCUSS
RELATED
WORK
IN
SECTION
WE
DEFINE
THE
HYBRID
HUMAN
COMPUTER
PROBLEM
AND
BASIC
ALGORITHM
WHICH
INCLUDES
METHODOLOGIES
FOR
MODELING
NOISY
USER
RESPONSES
AND
INCORPORATING
COMPUTER
VISION
INTO
THE
FRAMEWORK
WE
DESCRIBE
OUR
DATASETS
AND
IMPLEMENTATION
DETAILS
IN
SECTION
AND
PRESENT
EMPIRICAL
RESULTS
IN
SECTION
RELATED
WORK
RECOGNITION
OF
TIGHTLY
RELATED
CATEGORIES
IS
STILL
AN
OPEN
AREA
IN
COMPUTER
VI
SION
ALTHOUGH
THERE
HAS
BEEN
SUCCESS
IN
A
FEW
AREAS
SUCH
AS
BOOK
COVERS
AND
MOVIE
POSTERS
E
G
RIGID
MOSTLY
FLAT
OBJECTS
THE
PROBLEM
IS
CHALLENGING
BECAUSE
THE
NUMBER
OF
OBJECT
CATEGORIES
IS
LARGER
WITH
LOW
INTERCLASS
VARIANCE
AND
VARIABILITY
IN
POSE
LIGHTING
AND
BACKGROUND
CAUSES
HIGH
INTRACLASS
VARIANCE
ABILITY
TO
EXPLOIT
DOMAIN
KNOWLEDGE
AND
CROSS
CATEGORY
PATTERNS
AND
SIMILARITIES
BECOMES
INCREASINGLY
IMPORTANT
THERE
EXIST
A
VARIETY
OF
DATASETS
RELATED
TO
RECOGNITION
OF
TIGHTLY
RELATED
CAT
EGORIES
INCLUDING
OXFORD
FLOWERS
UIUC
BIRDS
AND
WHILE
THESE
WORKS
REPRESENT
PROGRESS
THEY
STILL
HAVE
SHORTCOMINGS
IN
SCALING
TO
LARGE
NUMBERS
OF
CATEGORIES
APPLYING
TO
OTHER
TYPES
OF
OBJECT
DOMAINS
OR
SEE
FOR
EXAMPLE
HTTP
NET
FIG
VISUALIZATION
OF
THE
BASIC
ALGORITHM
FLOW
THE
SYSTEM
POSES
QUESTIONS
TO
THE
USER
WHICH
ALONG
WITH
COMPUTER
VISION
INCREMENTALLY
REFINE
THE
PROBABILITY
DISTRIBUTION
OVER
CLASSES
ACHIEVING
PERFORMANCE
LEVELS
THAT
ARE
GOOD
ENOUGH
FOR
REAL
WORLD
APPLICATIONS
PERHAPS
MOST
SIMILAR
IN
SPIRIT
TO
OUR
WORK
IS
THE
BOTANIST
FIELD
GUIDE
A
SYSTEM
FOR
PLANT
SPECIES
RECOGNITION
WITH
HUNDREDS
OF
CATEGORIES
AND
TENS
OF
THOUSANDS
OF
IMAGES
ONE
KEY
DIFFERENCE
IS
THAT
THEIR
SYSTEM
IS
INTENDED
PRI
MARILY
FOR
EXPERTS
AND
REQUIRES
PLANT
LEAVES
TO
BE
PHOTOGRAPHED
IN
A
CONTROLLED
MANNER
AT
TRAINING
AND
TEST
TIME
MAKING
SEGMENTATION
AND
POSE
NORMALIZATION
POSSIBLE
IN
CONTRAST
ALL
OF
OUR
TRAINING
AND
TESTING
IMAGES
ARE
OBTAINED
FROM
FLICKR
IN
UNCONSTRAINED
SETTINGS
SEE
FIG
AND
THE
SYSTEM
IS
INTENDED
TO
BE
USED
BY
LAY
PEOPLE
THERE
EXISTS
A
MULTITUDE
OF
DIFFERENT
AREAS
IN
COMPUTER
SCIENCE
THAT
INTERLEAVE
VISION
LEARNING
OR
OTHER
PROCESSING
WITH
HUMAN
INPUT
RELEVANCE
FEEDBACK
IS
A
METHOD
FOR
INTERACTIVE
IMAGE
RETRIEVAL
IN
WHICH
USERS
MARK
THE
RELEVANCE
OF
IMAGE
SEARCH
RESULTS
WHICH
ARE
IN
TURN
USED
TO
CREATE
A
REFINED
SEARCH
QUERY
AC
TIVE
LEARNING
ALGORITHMS
INTERLEAVE
TRAINING
A
CLASSIFIER
WITH
ASKING
USERS
TO
LABEL
EXAMPLES
WHERE
THE
OBJECTIVE
IS
TO
MINIMIZE
THE
TOTAL
NUMBER
OF
LA
BELING
TASKS
OUR
OBJECTIVES
ARE
SOMEWHAT
SIMILAR
EXCEPT
THAT
WE
ARE
QUERYING
INFORMATION
AT
RUNTIME
RATHER
THAN
TRAINING
TIME
EXPERT
SYSTEMS
INVOLVE
CONSTRUCTION
OF
A
KNOWLEDGE
BASE
AND
INFERENCE
RULES
THAT
CAN
HELP
NON
EXPERTS
SOLVE
A
PROBLEM
OUR
APPROACH
DIFFERS
DUE
TO
THE
ADDED
ABILITY
TO
OBSERVE
IMAGE
PIXELS
AS
AN
ADDITIONAL
SOURCE
OF
INFORMATION
COMPUTATIONALLY
OUR
METHOD
ALSO
HAS
SIMILARITIES
TO
ALGORITHMS
BASED
ON
INFORMATION
GAIN
ENTROPY
CALCULATION
AND
DECISION
TREES
FINALLY
A
LOT
OF
PROGRESS
HAS
BEEN
MADE
ON
TRYING
TO
SCALE
OBJECT
RECOGNITION
TO
LARGE
NUMBERS
OF
CATEGORIES
SUCH
APPROACHES
INCLUDE
USING
CLASS
TAXONOMIES
FEATURE
SHARING
ERROR
CORRECTING
OUTPUT
CODES
ECOC
AND
ATTRIBUTE
BASED
CLASSIFICATION
METHODS
ALL
OF
THESE
METHODS
COULD
BE
EASILY
PLUGGED
INTO
OUR
FRAMEWORK
TO
INCORPORATE
USER
INTERACTION
VISUAL
RECOGNITION
WITH
HUMANS
IN
THE
LOOP
GIVEN
AN
IMAGE
X
OUR
GOAL
IS
TO
DETERMINE
THE
TRUE
OBJECT
CLASS
C
C
BY
POSING
QUESTIONS
BASED
ON
VISUAL
PROPERTIES
THAT
ARE
EASY
FOR
THE
USER
TO
ANSWER
SEE
FIG
AT
EACH
STEP
WE
AIM
TO
EXPLOIT
THE
VISUAL
CONTENT
OF
THE
IMAGE
AND
ALGORITHM
VISUAL
QUESTIONS
GAME
U
FOR
T
TO
DO
J
T
MAXK
I
C
UK
X
U
T
ASK
USER
QUESTION
QJ
T
AND
U
T
U
T
UJ
T
END
FOR
RETURN
CLASS
C
MAXC
P
C
X
U
T
THE
CURRENT
HISTORY
OF
QUESTION
RESPONSES
TO
INTELLIGENTLY
SELECT
THE
NEXT
QUESTION
THE
BASIC
ALGORITHM
FLOW
IS
SUMMARIZED
IN
FIG
LET
Q
QN
BE
A
SET
OF
POSSIBLE
QUESTIONS
E
G
ISRED
HASSTRIPES
ETC
AND
AI
BE
THE
SET
OF
POSSIBLE
ANSWERS
TO
QI
THE
USER
ANSWER
IS
SOME
RANDOM
VARIABLE
AI
AI
WE
ALSO
ALLOW
USERS
TO
QUALIFY
EACH
RESPONSE
WITH
A
CONFIDENCE
VALUE
RI
V
E
G
V
GUESSING
PROBABLY
DEFINITELY
THE
USER
RESPONSE
IS
THEN
A
PAIR
OF
RANDOM
VARIABLES
UI
AI
RI
AT
EACH
TIME
STEP
T
WE
SELECT
A
QUESTION
QJ
T
TO
POSE
TO
THE
USER
WHERE
J
T
N
LET
J
N
T
BE
AN
ARRAY
OF
T
INDICES
TO
QUESTIONS
WE
WILL
ASK
THE
USER
U
T
UJ
UJ
T
IS
THE
SET
OF
RESPONSES
OBTAINED
BY
TIME
STEP
T
WE
USE
MAXIMUM
INFORMATION
GAIN
AS
THE
CRITERION
TO
SELECT
QJ
T
INFORMATION
GAIN
IS
WIDELY
USED
IN
DECISION
TREES
E
G
AND
CAN
BE
COMPUTED
FROM
AN
ESTIMATE
OF
P
C
X
U
T
WE
DEFINE
I
C
UI
X
U
T
THE
EXPECTED
INFORMATION
GAIN
OF
POSING
THE
ADDI
TIONAL
QUESTION
QI
AS
FOLLOWS
UI
AI
V
P
UI
X
U
T
H
C
X
UI
U
T
H
C
X
U
T
WHERE
H
C
X
U
T
IS
THE
ENTROPY
OF
P
C
X
U
T
C
H
C
X
U
T
P
C
X
U
T
LOG
P
C
X
U
T
C
THE
GENERAL
ALGORITHM
FOR
INTERACTIVE
OBJECT
RECOGNITION
IS
SHOWN
IN
ALGORITHM
IN
THE
NEXT
SECTIONS
WE
DESCRIBE
IN
GREATER
DETAIL
METHODS
FOR
MODELING
USER
RESPONSES
AND
DIFFERENT
METHODS
FOR
INCORPORATING
COMPUTER
VISION
ALGORITHMS
WHICH
CORRESPOND
TO
DIFFERENT
WAYS
TO
ESTIMATE
P
C
X
U
T
INCORPORATING
COMPUTER
VISION
WHEN
NO
COMPUTER
VISION
IS
INVOLVED
IT
IS
POSSIBLE
TO
PRE
COMPUTE
A
DECISION
TREE
THAT
DEFINES
WHICH
QUESTION
TO
ASK
FOR
EVERY
POSSIBLE
SEQUENCE
OF
USER
RE
SPONSES
WITH
COMPUTER
VISION
IN
THE
LOOP
HOWEVER
THE
BEST
QUESTIONS
DEPEND
DYNAMICALLY
ON
THE
CONTENTS
OF
THE
IMAGE
IN
THIS
SECTION
WE
PROPOSE
A
SIMPLE
FRAMEWORK
FOR
INCORPORATING
ANY
MULTI
CLASS
OBJECT
RECOGNITION
ALGORITHM
THAT
PRODUCES
A
PROBABILISTIC
OUTPUT
OVER
CLASSES
WE
CAN
COMPUTE
P
C
X
U
WHERE
U
IS
ANY
ARBITRARY
SEQUENCE
OF
RE
SPONSES
AS
FOLLOWS
P
C
X
U
P
U
C
X
P
C
X
P
U
C
P
C
X
WHERE
Z
C
P
U
C
P
C
X
HERE
WE
MAKE
THE
ASSUMPTION
THAT
P
U
C
X
P
U
C
EFFECTIVELY
THIS
ASSUMES
THAT
THE
TYPES
OF
NOISE
OR
RANDOMNESS
THAT
WE
SEE
IN
USER
RESPONSES
IS
CLASS
DEPENDENT
AND
NOT
IMAGE
DEPENDENT
WE
CAN
STILL
ACCOMMODATE
VARIATION
IN
RESPONSES
DUE
TO
USER
ERROR
SUBJECTIVITY
EXTERNAL
FACTORS
AND
INTRACLASS
VARIANCE
HOWEVER
WE
THROW
AWAY
SOME
IMAGE
RELATED
IN
FORMATION
FOR
EXAMPLE
WE
LOSE
ABILITY
TO
MODEL
A
CHANGE
IN
THE
DISTRIBUTION
OF
USER
RESPONSES
AS
A
RESULT
OF
A
COMPUTER
VISION
BASED
ESTIMATE
OF
OBJECT
POSE
IN
TERMS
OF
COMPUTATION
WE
ESTIMATE
P
C
X
USING
A
CLASSIFIER
TRAINED
OFFLINE
MORE
DETAILS
IN
SECTION
UPON
RECEIVING
AN
IMAGE
WE
RUN
THE
CLASSIFIER
ONCE
AT
THE
BEGINNING
OF
THE
PROCESS
AND
INCREMENTALLY
UPDATE
P
C
X
U
BY
GATHERING
MORE
ANSWERS
TO
QUESTIONS
FROM
THE
USER
ONE
COULD
IMAGINE
A
SYSTEM
WHERE
A
LEARNING
ALGORITHM
IS
INVOKED
SEVERAL
TIMES
DURING
THE
PROCESS
AS
CATEGORIES
ARE
WEEDED
OUT
BY
ANSWERS
THE
SYSTEM
WOULD
USE
A
MORE
TUNED
CLASSIFIER
TO
UPDATE
THE
ESTIMATE
OF
P
C
X
HOWEVER
OUR
PRELIMINARY
EXPERIMENTS
WITH
SUCH
METHODS
DID
NOT
SHOW
AN
NOTE
THAT
WHEN
NO
COMPUTER
VISION
IS
INVOLVED
WE
SIMPLY
REPLACE
P
C
X
WITH
A
PRIOR
P
C
MODELING
USER
RESPONSES
RECALL
THAT
FOR
EACH
QUESTION
WE
MAY
ALSO
ASK
A
CORRESPONDING
CONFIDENCE
VALUE
FROM
THE
USER
WHICH
MAY
BE
NECESSARY
WHEN
AN
ATTRIBUTE
CANNOT
BE
DETERMINED
FOR
EXAMPLE
WHEN
THE
ASSOCIATED
PART
ARE
NOT
VISIBLE
WE
ASSUME
THAT
THE
QUESTIONS
ARE
ANSWERED
INDEPENDENTLY
GIVEN
THE
CATEGORY
T
P
U
C
P
UI
C
I
THE
SAME
ASSUMPTION
ALLOWS
US
TO
EXPRESS
P
UI
X
U
T
IN
EQUATION
AS
C
P
UI
X
U
T
P
UI
C
P
C
X
U
T
C
IT
MAY
ALSO
BE
POSSIBLE
TO
USE
A
MORE
SOPHISTICATED
MODEL
IN
WHICH
WE
ESTIMATE
A
FULL
JOINT
DISTRIBUTION
FOR
P
U
T
C
IN
OUR
PRELIMINARY
EXPERIMENTS
THIS
APPROACH
DID
NOT
WORK
WELL
DUE
TO
INSUFFICIENT
TRAINING
DATA
SEE
SUPPLEMENTARY
MATERIAL
HTTP
WWW
VISION
CALTECH
EDU
VISIPEDIA
HTML
FOR
MORE
DETAILS
FIG
EXAMPLES
OF
USER
RESPONSES
FOR
EACH
OF
THE
ATTRIBUTES
THE
DISTRIBU
TION
OVER
GUESSING
PROBABLY
DEFINITELY
IS
COLOR
CODED
WITH
BLUE
DENOTING
AND
RED
DENOTING
OF
THE
FIVE
ANSWERS
PER
IMAGE
ATTRIBUTE
PAIR
TO
COMPUTE
P
UI
C
P
AI
RI
C
P
AI
RI
C
P
RI
C
WE
ASSUME
THAT
P
RI
C
P
RI
NEXT
WE
COMPUTE
EACH
P
AI
RI
C
AS
THE
POSTERIOR
OF
A
MULTINOMIAL
DIS
TRIBUTION
WITH
DIRICHLET
PRIOR
DIR
ΑRP
AI
RI
ΑCP
AI
C
WHERE
ΑR
AND
ΑC
ARE
CONSTANTS
P
AI
RI
IS
A
GLOBAL
ATTRIBUTE
PRIOR
AND
P
AI
C
IS
ESTIMATED
BY
POOLING
TOGETHER
CERTAINTY
LABELS
IN
PRACTICE
WE
USE
A
LARGER
PRIOR
TERM
FOR
GUESSING
THAN
DEFINITELY
ΑGUESS
ΑDEF
WHICH
EFFECTIVELY
DOWN
WEIGHTS
THE
IMPORTANCE
OF
ANY
RESPONSE
WITH
CERTAINTY
LEVEL
GUESSING
DATASETS
AND
IMPLEMENTATION
DETAILS
IN
THIS
SECTION
WE
PROVIDE
A
BRIEF
OVERVIEW
OF
THE
DATASETS
WE
USED
METHODS
USED
TO
CONSTRUCT
VISUAL
QUESTIONS
COMPUTER
VISION
ALGORITHMS
WE
TESTED
AND
PARAMETER
SETTINGS
BIRDS
DATASET
BIRDS
IS
A
DATASET
OF
IMAGES
OVER
BIRD
SPECIES
SUCH
AS
MYRTLE
WARBLERS
POMARINE
JAEGARS
AND
BLACK
FOOTED
ALBATROSSES
CLASSES
THAT
CANNOT
USUALLY
BE
IDENTIFIED
BY
NON
EXPERTS
IN
MANY
CASES
DIFFERENT
BIRD
SPECIES
ARE
NEARLY
VISUALLY
IDENTICAL
SEE
FIG
WE
ASSEMBLED
A
SET
OF
VISUAL
QUESTIONS
LIST
SHOWN
IN
FIG
WHICH
ENCOM
PASS
BINARY
ATTRIBUTES
E
G
THE
QUESTION
HASBELLYCOLOR
CAN
TAKE
ON
DIF
FERENT
POSSIBLE
COLORS
THE
LIST
OF
ATTRIBUTES
WAS
EXTRACTED
FROM
WHATBIRD
A
BIRD
FIELD
GUIDE
WEBSITE
HTTP
WWW
WHATBIRD
COM
WE
COLLECTED
DETERMINISTIC
CLASS
ATTRIBUTES
BY
PARSING
ATTRIBUTES
FROM
WHAT
BIRD
COM
ADDITIONALLY
WE
COLLECTED
DATA
OF
HOW
NON
EXPERT
USERS
RESPOND
TO
AT
TRIBUTE
QUESTIONS
VIA
A
MECHANICAL
TURK
INTERFACE
TO
MINIMIZE
THE
EFFECTS
OF
USER
SUBJECTIVITY
AND
ERROR
OUR
INTERFACE
PROVIDES
PROTOTYPICAL
IMAGES
OF
EACH
POSSI
BLE
ATTRIBUTE
RESPONSE
THE
READER
IS
ENCOURAGED
TO
LOOK
AT
THE
SUPPLEMENTARY
MATERIAL
FOR
SCREENSHOTS
OF
THE
QUESTION
ANSWERING
USER
INTERFACE
AND
EXAMPLE
IMAGES
OF
THE
DATASET
FIG
SHOWS
A
VISUALIZATION
OF
THE
TYPES
OF
USER
RESPONSE
RESULTS
WE
GET
ON
THE
BIRDS
DATASET
IT
SHOULD
BE
NOTED
THAT
THE
UNCERTAINTY
OF
THE
USER
RESPONSES
STRONGLY
CORRELATES
WITH
THE
PARTS
THAT
ARE
VISIBLE
IN
AN
IMAGE
AS
WELL
AS
OVERALL
DIFFICULTY
OF
THE
CORRESPONDING
BIRD
SPECIES
WHEN
EVALUATING
PERFORMANCE
TEST
RESULTS
ARE
GENERATED
BY
RANDOMLY
SELECT
ING
A
RESPONSE
RETURNED
BY
AN
MTURK
USER
FOR
THE
APPROPRIATE
TEST
IMAGE
ANIMALS
WITH
ATTRIBUTES
WE
ALSO
TESTED
PERFORMANCE
ON
THE
ANIMALS
WITH
ATTRIBUTES
AWA
A
DATASET
OF
ANIMAL
CLASSES
AND
BINARY
ATTRIBUTES
WE
CONSIDER
THIS
DATASET
LESS
RELEVANT
THAN
BIRDS
BECAUSE
CLASSES
ARE
RECOGNIZABLE
BY
NON
EXPERTS
AND
THEREFORE
DO
NOT
FOCUS
AS
MUCH
ON
THIS
DATASET
IMPLEMENTATION
DETAILS
AND
PARAMETER
SETTINGS
FOR
BOTH
DATASETS
OUR
COMPUTER
VISION
ALGORITHMS
ARE
BASED
ON
ANDREA
VEDALDI
PUBLICLY
AVAILABLE
SOURCE
CODE
WHICH
COMBINES
VECTOR
QUANTIZED
GEOMETRIC
BLUR
AND
COLOR
GRAY
SIFT
FEATURES
USING
SPATIAL
PYRAMIDS
MULTIPLE
KERNEL
LEARN
ING
AND
PER
CLASS
VS
ALL
SVMS
WE
ADDED
FEATURES
BASED
ON
FULL
IMAGE
COLOR
HISTOGRAMS
AND
VECTOR
QUANTIZED
COLOR
HISTOGRAMS
FOR
EACH
CLASSIFIER
WE
USED
PLATT
SCALING
TO
LEARN
PARAMETERS
FOR
P
C
X
ON
A
VALIDATION
SET
WE
USED
TRAINING
EXAMPLES
FOR
EACH
BIRDS
CLASS
AND
TRAINING
EXAMPLES
FOR
EACH
AWA
CLASS
BIRD
TRAINING
AND
TESTING
IMAGES
ARE
ROUGHLY
CROPPED
ADDITIONALLY
WE
COMPARE
PERFORMANCE
TO
A
SECOND
COMPUTER
VISION
ALGORITHM
BASED
ON
ATTRIBUTE
CLASSIFIERS
WHICH
WE
TRAIN
USING
THE
SAME
FEATURES
TRAINING
CODE
WITH
POSITIVE
AND
NEGATIVE
EXAMPLES
SET
USING
WHATBIRD
COM
ATTRIBUTE
LA
BELS
WE
COMBINED
ATTRIBUTE
CLASSIFIERS
INTO
PER
CLASS
PROBABILITIES
P
C
X
USING
THE
METHOD
DESCRIBED
IN
FOR
ESTIMATING
USER
RESPONSE
STATISTICS
ON
THE
BIRDS
DATASET
WE
USED
ΑGUESS
ΑPROB
ΑDEF
AND
ΑC
SEE
SECTION
EXPERIMENTS
IN
THIS
SECTION
WE
PROVIDE
EXPERIMENTAL
RESULTS
AND
ANALYSIS
OF
THE
HYBRID
HUMAN
COMPUTER
CLASSIFICATION
PARADIGM
DUE
TO
SPACE
LIMITATIONS
OUR
DISCUSSION
FO
CUSES
ON
THE
BIRDS
DATASET
WE
INCLUDE
RESULTS
SEE
FIG
FROM
WHICH
THE
USER
CAN
VERIFY
THAT
TRENDS
ARE
SIMILAR
ON
BIRDS
AND
AWA
AND
WE
INCLUDE
ADDI
TIONAL
RESULTS
ON
AWA
IN
THE
SUPPLEMENTARY
MATERIAL
NUMBER
OF
BINARY
QUESTIONS
ASKED
FIG
DIFFERENT
MODELS
OF
USER
RESPONSES
LEFT
CLASSIFICATION
PERFORMANCE
ON
BIRDS
METHOD
WITHOUT
COMPUTER
VISION
PERFORMANCE
RISES
QUICKLY
BLUE
CURVE
IF
USERS
RESPOND
DETERMINISTICALLY
ACCORDING
TO
WHATBIRD
COM
ATTRIBUTES
MTURK
USERS
RESPOND
QUITE
DIFFERENTLY
RESULTING
IN
LOW
PERFORMANCE
GREEN
CURVE
A
LEARNED
MODEL
OF
MTURK
RESPONSES
IS
MUCH
MORE
ROBUST
RED
CURVE
RIGHT
A
TEST
IMAGE
WHERE
USERS
ANSWER
SEVERAL
QUESTIONS
INCORRECTLY
AND
OUR
MODEL
STILL
CLASSIFIES
THE
IMAGE
CORRECTLY
MEASURING
PERFORMANCE
WE
USE
TWO
MAIN
METHODOLOGIES
FOR
MEASURING
PERFORMANCE
WHICH
CORRESPOND
TO
TWO
DIFFERENT
POSSIBLE
USER
INTERFACES
METHOD
WE
ASK
THE
USER
EXACTLY
T
QUESTIONS
PREDICT
THE
CLASS
WITH
HIGHEST
PROBABILITY
AND
MEASURE
THE
PERCENT
OF
THE
TIME
THAT
WE
ARE
CORRECT
METHOD
AFTER
ASKING
EACH
QUESTION
WE
PRESENT
A
SMALL
GALLERY
OF
IMAGES
OF
THE
HIGHEST
PROBABILITY
CLASS
AND
ALLOW
THE
USER
TO
STOP
THE
SYSTEM
EARLY
WE
MEASURE
THE
AVERAGE
NUMBER
OF
QUESTIONS
ASKED
PER
TEST
IMAGE
FOR
THE
SECOND
METHOD
WE
ASSUME
THAT
PEOPLE
ARE
PERFECT
VERIFIERS
E
G
THEY
WILL
STOP
THE
SYSTEM
IF
AND
ONLY
IF
THEY
HAVE
BEEN
PRESENTED
WITH
THE
CORRECT
CLASS
WHILE
THIS
IS
NOT
ALWAYS
POSSIBLE
IN
REALITY
THERE
IS
SOME
TRADE
OFF
BETWEEN
CLASSIFICATION
ACCURACY
AND
AMOUNT
OF
HUMAN
LABOR
AND
WE
BELIEVE
THAT
THESE
TWO
METRICS
COLLECTIVELY
CAPTURE
THE
MOST
IMPORTANT
CONSIDERATIONS
RESULTS
IN
THIS
SECTION
WE
PRESENT
OUR
RESULTS
AND
DISCUSS
SOME
INTERESTING
TRENDS
TOWARD
UNDERSTANDING
THE
VISUAL
QUESTIONS
CLASSIFICATION
PARADIGM
USER
RESPONSES
ARE
STOCHASTIC
IN
FIG
WE
SHOW
THE
EFFECTS
OF
DIFFERENT
MODELS
OF
USER
RESPONSES
WITHOUT
USING
ANY
COMPUTER
VISION
WHEN
USERS
ARE
ASSUMED
TO
RESPOND
DETERMINISTICALLY
IN
ACCORDANCE
WITH
THE
ATTRIBUTES
FROM
WHATBIRD
COM
PERFORMANCE
RISES
QUICKLY
TO
WITHIN
QUESTIONS
ROUGHLY
HOWEVER
THIS
ASSUMPTION
IS
NOT
REALISTIC
WHEN
TESTING
WITH
RESPONSES
15
40
NUMBER
OF
BINARY
QUESTIONS
ASKED
12
NUMBER
OF
BINARY
QUESTIONS
ASKED
FIG
PERFORMANCE
ON
BIRDS
WHEN
USING
COMPUTER
VISION
LEFT
PLOT
COMPARISON
OF
CLASSIFICATION
ACCURACY
METHOD
WITH
AND
WITHOUT
COMPUTER
VISION
WHEN
USING
MTURK
USER
RESPONSES
TWO
DIFFERENT
COMPUTER
VISION
ALGORITHMS
ARE
SHOWN
ONE
BASED
ON
PER
CLASS
VS
ALL
CLASSIFIERS
AND
ANOTHER
BASED
ON
ATTRIBUTE
CLASSIFIERS
RIGHT
PLOT
THE
NUMBER
OF
QUESTIONS
NEEDED
TO
IDENTIFY
THE
TRUE
CLASS
METHOD
DROPS
FROM
TO
ON
AVERAGE
WHEN
INCORPORATING
COMPUTER
VISION
FROM
MECHANICAL
TURK
PERFORMANCE
SATURATES
AT
AROUND
LOW
PERFORMANCE
CAUSED
BY
SUBJECTIVE
ANSWERS
ARE
UNAVOIDABLE
E
G
PERCEPTION
OF
THE
COLOR
BROWN
VS
THE
COLOR
BUFF
AND
THE
PROBABILITY
OF
THE
CORRECT
CLASS
DROPS
TO
ZERO
AFTER
ANY
INCONSISTENT
RESPONSE
ALTHOUGH
PERFORMANCE
IS
TIMES
BETTER
THAN
RANDOM
CHANCE
IT
RENDERS
THE
SYSTEM
USELESS
THIS
DEMONSTRATES
A
CHALLENGE
FOR
EXIST
ING
FIELD
GUIDE
WEBSITES
WHEN
OUR
LEARNED
MODEL
OF
USER
RESPONSES
SEE
SECTION
IS
INCORPORATED
PERFORMANCE
JUMPS
TO
DUE
TO
THE
ABILITY
TO
TOLERATE
A
REASONABLE
DEGREE
OF
ERROR
IN
USER
RESPONSES
SEE
FIG
FOR
AN
EXAMPLE
NEVER
THELESS
STOCHASTIC
USER
RESPONSES
INCREASE
THE
NUMBER
OF
QUESTIONS
REQUIRED
TO
ACHIEVE
A
GIVEN
ACCURACY
LEVEL
AND
SOME
IMAGES
CAN
NEVER
BE
CLASSIFIED
CORRECTLY
EVEN
WHEN
ASKING
ALL
POSSIBLE
QUESTIONS
IN
SECTION
WE
DISCUSS
THE
REASONS
WHY
PERFORMANCE
SATURATES
AT
LOWER
THAN
PERFORMANCE
COMPUTER
VISION
REDUCES
MANUAL
LABOR
THE
MAIN
BENEFIT
OF
COMPUTER
VISION
OCCURS
DUE
TO
REDUCTION
IN
HUMAN
LABOR
IN
TERMS
OF
THE
NUMBER
OF
QUES
TIONS
A
USER
HAS
TO
ANSWER
IN
FIG
WE
SEE
THAT
COMPUTER
VISION
REDUCES
THE
AVERAGE
NUMBER
OF
YES
NO
QUESTIONS
NEEDED
TO
IDENTIFY
THE
TRUE
BIRD
SPECIES
FROM
TO
USING
RESPONSES
FROM
MTURK
USERS
WITHOUT
COMPUTER
VISION
THE
DISTRIBUTION
OF
QUESTION
COUNTS
IS
BELL
SHAPED
AND
CENTERED
AROUND
QUESTIONS
WHEN
COMPUTER
VISION
IS
INCORPORATED
THE
DISTRIBUTION
PEAKS
AT
0
QUESTIONS
BUT
IS
MORE
HEAVY
TAILED
WHICH
SUGGESTS
THAT
COMPUTER
VISION
ALGORITHMS
ARE
OFTEN
GOOD
AT
RECOGNIZING
THE
EASY
TEST
EXAMPLES
EXAMPLES
THAT
ARE
SUFFICIENTLY
SIM
ILAR
TO
THE
TRAINING
DATA
BUT
PROVIDE
DIMINISHING
RETURNS
TOWARD
CLASSIFYING
THE
HARDER
EXAMPLES
THAT
ARE
NOT
SUFFICIENTLY
SIMILAR
TO
TRAINING
DATA
AS
A
RESULT
COMPUTER
VISION
IS
MORE
EFFECTIVE
AT
REDUCING
THE
AVERAGE
AMOUNT
OF
TIME
THAN
REDUCING
THE
TIME
SPENT
ON
THE
MOST
DIFFICULT
IMAGES
USER
RESPONSES
DRIVE
UP
PERFORMANCE
AN
ALTERNATIVE
WAY
OF
INTERPRET
ING
THE
RESULTS
IS
THAT
USER
RESPONSES
DRIVE
UP
THE
ACCURACY
OF
COMPUTER
VISION
FIG
EXAMPLES
WHERE
COMPUTER
VISION
AND
USER
RESPONSES
WORK
TOGETHER
LEFT
AN
IMAGE
THAT
IS
ONLY
CLASSIFIED
CORRECTLY
WHEN
COMPUTER
VISION
IS
INCORPORATED
ADDITIONALLY
THE
COMPUTER
VISION
BASED
METHOD
SELECTS
THE
QUESTION
HASTHROATCOLOR
WHITE
A
DIFFERENT
AND
MORE
RELEVANT
QUESTION
THAN
WHEN
VISION
IS
NOT
USED
IN
THE
RIGHT
IMAGE
THE
USER
RESPONSE
TO
HASCROWNCOLORBLACK
HELPS
CORRECT
COMPUTER
VISION
WHEN
ITS
INITIAL
PREDICTION
IS
WRONG
ALGORITHMS
IN
FIG
WE
SEE
THAT
USER
RESPONSES
IMPROVE
OVERALL
PERFORMANCE
FROM
USING
0
QUESTIONS
TO
COMPUTER
VISION
IMPROVES
OVERALL
PERFORMANCE
EVEN
WHEN
USERS
AN
SWER
ALL
QUESTIONS
PERFORMANCE
SATURATES
AT
A
HIGHER
LEVEL
WHEN
USING
COMPUTER
VISION
VS
SEE
FIG
THE
LEFT
IMAGE
IN
FIG
SHOWS
AN
EXAMPLE
OF
AN
IMAGE
CLASSIFIED
CORRECTLY
USING
COMPUTER
VISION
WHICH
IS
NOT
CLASSIFIED
COR
RECTLY
WITHOUT
COMPUTER
VISION
EVEN
AFTER
ASKING
QUESTIONS
IN
THIS
EXAMPLE
SOME
VISUALLY
SALIENT
FEATURES
LIKE
THE
LONG
NECK
ARE
NOT
CAPTURED
IN
OUR
LIST
OF
VISUAL
ATTRIBUTE
QUESTIONS
THE
FEATURES
USED
BY
OUR
VISION
ALGORITHMS
ALSO
CAP
TURE
OTHER
CUES
SUCH
AS
GLOBAL
TEXTURE
STATISTICS
THAT
ARE
NOT
WELL
REPRESENTED
IN
OUR
LIST
OF
ATTRIBUTES
WHICH
CAPTURE
MOSTLY
COLOR
AND
PART
LOCALIZED
PATTERNS
DIFFERENT
QUESTIONS
ARE
ASKED
WITH
AND
WITHOUT
COMPUTER
VISION
IN
GENERAL
THE
INFORMATION
GAIN
CRITERION
FAVORS
QUESTIONS
THAT
CAN
BE
AN
SWERED
RELIABLY
AND
SPLIT
THE
SET
OF
POSSIBLE
CLASSES
ROUGHLY
IN
HALF
QUESTIONS
LIKE
HASSHAPEPERCHINGLIKE
WHICH
DIVIDE
THE
CLASSES
FAIRLY
EVENLY
AND
HASUNDER
PARTSCOLORYELLOW
WHICH
TENDS
TO
BE
ANSWERED
RELIABLY
ARE
COMMONLY
CHOSEN
WHEN
COMPUTER
VISION
IS
INCORPORATED
THE
LIKELIHOOD
OF
CLASSES
CHANGE
AND
DIFFERENT
QUESTIONS
ARE
SELECTED
IN
THE
LEFT
IMAGE
OF
FIG
WE
SEE
AN
EXAMPLE
WHERE
A
DIFFERENT
QUESTION
IS
ASKED
WITH
AND
WITHOUT
COMPUTER
VISION
WHICH
ALLOWS
THE
SYSTEM
TO
FIND
THE
CORRECT
CLASS
USING
ONE
QUESTION
RECOGNITION
IS
NOT
ALWAYS
SUCCESSFUL
ACCORDING
THE
THE
CORNELL
ORNITHOL
OGY
THE
FOUR
KEYS
TO
BIRD
SPECIES
RECOGNITION
ARE
SIZE
AND
SHAPE
HTTP
WWW
ALLABOUTBIRDS
ORG
NETCOMMUNITY
PAGE
ASPX
PID
FIG
IMAGES
THAT
ARE
MISCLASSIFIED
BY
OUR
SYSTEM
LEFT
THE
PARAKEET
AUKLET
IMAGE
IS
MISCLASSIFIED
DUE
TO
A
CROPPED
IMAGE
WHICH
CAUSES
AN
INCORRECT
ANSWER
TO
THE
BELLY
PATTERN
QUESTION
THE
PARAKEET
AUKLET
HAS
A
PLAIN
WHITE
BELLY
SEE
FIG
RIGHT
THE
SAYORNIS
AND
GRAY
KINGBIRD
ARE
COMMONLY
CONFUSED
DUE
TO
VISUAL
SIMILARITY
COLOR
AND
PATTERN
BEHAVIOR
AND
HABITAT
BIRD
SPECIES
CLASSIFICATION
IS
A
DIFFICULT
PROBLEM
AND
IS
NOT
ALWAYS
POSSIBLE
USING
A
SINGLE
IMAGE
ONE
PO
TENTIAL
ADVANTAGE
OF
THE
VISUAL
QUESTIONS
PARADIGM
IS
THAT
OTHER
CONTEXTUAL
SOURCES
OF
INFORMATION
SUCH
AS
BEHAVIOR
AND
HABITAT
CAN
EASILY
BE
INCORPORATED
AS
ADDITIONAL
QUESTIONS
FIG
ILLUSTRATES
SOME
EXAMPLE
FAILURES
THE
MOST
COMMON
FAILURE
CONDITIONS
OCCUR
DUE
TO
CLASSES
THAT
ARE
NEARLY
VISUALLY
IDENTICAL
2
IMAGES
OF
POOR
VIEWPOINT
OR
LOW
RESOLUTION
SUCH
THAT
SOME
PARTS
ARE
NOT
VISIBLE
SIGNIFICANT
MISTAKES
MADE
BY
MTURKERS
OR
INADEQUACIES
IN
THE
SET
OF
ATTRIBUTES
WE
USED
VS
ALL
VS
ATTRIBUTE
BASED
CLASSIFICATION
IN
GENERAL
VS
ALL
CLASSIFIERS
SLIGHTLY
OUTPERFORM
ATTRIBUTE
BASED
CLASSIFIERS
HOWEVER
THEY
CONVERGE
TO
SIMILAR
PERFORMANCE
AS
THE
NUMBER
OF
QUESTION
INCREASES
AS
SHOWN
IN
FIG
AND
THE
FEATURES
WE
USE
KERNELIZED
AND
BASED
ON
BAG
OF
WORDS
MAY
NOT
BE
WELL
SUITED
TO
THE
TYPES
OF
ATTRIBUTES
WE
ARE
USING
WHICH
TEND
TO
BE
LOCALIZED
AND
ASSOCIATED
WITH
A
PARTICULAR
PART
ONE
POTENTIAL
ADVANTAGE
OF
ATTRIBUTE
BASED
METHODS
IS
COMPUTATIONAL
SCALABILITY
WHEN
THE
NUMBER
OF
CLASSES
INCREASES
WHEREAS
VS
ALL
METHODS
ALWAYS
REQUIRE
C
CLASSIFIERS
THE
NUMBER
OF
ATTRIBUTE
CLASSIFIERS
CAN
BE
VARIED
IN
ORDER
TO
TRADE
OFF
ACCURACY
AND
COMPUTATION
TIME
THE
TABLE
BELOW
DISPLAYS
THE
AVERAGE
NUMBER
OF
QUESTIONS
NEEDED
METHOD
ON
THE
BIRDS
DATASET
USING
DIFFERENT
NUMBER
OF
ATTRIBUTE
CLASSIFIERS
WHICH
WERE
SELECTED
RANDOMLY
VS
ALL
ATTR
ATTR
ATTR
ATTR
ATTR
72
7
8
CONCLUSION
OBJECT
RECOGNITION
REMAINS
A
CHALLENGING
PROBLEM
FOR
COMPUTER
VISION
FURTHER
MORE
RECOGNIZING
TIGHTLY
RELATED
CATEGORIES
IN
ONE
SHOT
IS
DIFFICULT
EVEN
FOR
HU
FIG
PERFORMANCE
ON
ANIMALS
WITH
ATTRIBUTES
LEFT
PLOT
CLASSIFICATION
PER
FORMANCE
METHOD
1
SIMULATING
USER
RESPONSES
USING
SOFT
CLASS
ATTRIBUTES
SEE
RIGHT
PLOT
THE
REQUIRED
NUMBER
OF
QUESTIONS
NEEDED
TO
IDENTIFY
THE
TRUE
CLASS
METHOD
2
DROPS
FROM
TO
ON
AVERAGE
WHEN
INCORPORATING
COMPUTER
VISION
MANS
WITHOUT
PROPER
EXPERTISE
OUR
WORK
ATTEMPTS
TO
LEVERAGE
THE
POWER
OF
BOTH
HUMAN
RECOGNITION
ABILITIES
AND
THAT
OF
COMPUTER
VISION
WE
PRESENTED
A
SIM
PLE
WAY
OF
DESIGNING
A
HYBRID
HUMAN
COMPUTER
CLASSIFICATION
SYSTEM
WHICH
CAN
BE
USED
IN
CONJUNCTION
WITH
A
LARGE
VARIETY
OF
COMPUTER
VISION
ALGORITHMS
OUR
RESULTS
SHOW
THAT
USER
INPUT
SIGNIFICANTLY
DRIVES
UP
PERFORMANCE
WHILE
IT
MAY
TAKE
MANY
YEARS
BEFORE
OBJECT
RECOGNITION
ALGORITHMS
ACHIEVE
REASONABLE
PERFOR
MANCE
ON
THEIR
OWN
INCORPORATING
HUMAN
INPUT
CAN
PRODUCE
USABLE
RECOGNITION
SYSTEMS
ON
THE
OTHER
HAND
HAVING
COMPUTER
VISION
IN
THE
LOOP
REDUCES
THE
AMOUNT
OF
REQUIRED
HUMAN
LABOR
TO
SUCCESSFULLY
CLASSIFY
AN
IMAGE
FINALLY
WE
SHOWED
THAT
INCORPORATING
MODELS
OF
STOCHASTIC
USER
RESPONSES
LEADS
TO
MUCH
BET
TER
RELIABILITY
IN
COMPARISON
TO
DETERMINISTIC
FIELD
GUIDES
GENERATED
BY
EXPERTS
WE
BELIEVE
OUR
WORK
OPENS
THE
DOOR
TO
MANY
INTERESTING
SUB
PROBLEMS
THE
MOST
OBVIOUS
NEXT
STEP
IS
TO
EXPLORE
OTHER
TYPES
OF
DOMAINS
WHILE
WE
WERE
ABLE
TO
EXTRACT
A
SET
OF
REASONABLE
ATTRIBUTES
QUESTIONS
FOR
THE
BIRD
DATASET
THIS
MAY
BE
MORE
DIFFICULT
FOR
OTHER
DOMAINS
ONE
POSSIBLE
TOPIC
FOR
FUTURE
WORK
IS
TO
FIND
A
MORE
PRINCIPLED
WAY
OF
DISCOVERING
A
SET
OF
USEFUL
QUESTIONS
ALTERNATIVE
TYPES
OF
USER
INPUT
SUCH
AS
ASKING
THE
USER
TO
CLICK
ON
THE
LOCATION
OF
CERTAIN
PARTS
COULD
ALSO
BE
INVESTIGATED
LASTLY
WHILE
WE
USED
OFF
THE
SHELF
COMPUTER
VISION
ALGORITHMS
IN
THIS
WORK
IT
MAY
BE
POSSIBLE
TO
IMPROVE
THEM
TO
BETTER
SUIT
THE
CHALLENGES
OF
TIGHTLY
RELATED
CATEGORY
RECOGNITION
SUCH
AS
ALGORITHMS
THAT
INCORPORATE
A
PART
BASED
MODEL
CHAP
TER
LOCAL
FEATURES
DETECTION
AND
DESCRIPTION
SIGNIFICANT
PROGRESS
TOWARDS
ROBUSTLY
RECOGNIZING
OBJECTS
HAS
BEEN
MADE
IN
THE
PAST
DECADE
THROUGH
THE
DEVELOPMENT
OF
LOCAL
INVARIANT
FEATURES
THESE
FEATURES
ALLOW
THE
ALGORITHM
TO
FIND
LOCAL
IMAGE
STRUCTURES
IN
A
REPEATABLE
FASHION
AND
TO
ENCODE
THEM
IN
A
REPRESENTATION
THAT
IS
INVARIANT
TO
A
RANGE
OF
IMAGE
TRANSFORMATIONS
SUCH
AS
TRANSLATION
ROTATION
SCALING
AND
AFFINE
DEFORMATION
THE
RESULTING
FEATURES
THEN
FORM
THE
BASIS
OF
APPROACHES
FOR
RECOGNIZING
BOTH
SPECIFIC
OBJECTS
AND
OBJECT
CATEGORIES
IN
THIS
CHAPTER
WE
WILL
EXPLAIN
THE
BASIC
IDEAS
AND
IMPLEMENTATION
STEPS
BEHIND
STATE
OF
THE
ART
LOCAL
FEATURE
DETECTORS
AND
DESCRIPTORS
A
MORE
EXTENSIVE
TREATMENT
OF
LOCAL
FEATURES
IN
CLUDING
DETAILED
COMPARISONS
AND
USAGE
GUIDELINES
CAN
BE
FOUND
IN
TUYTELAARS
AND
MIKOLAJCZYK
SYSTEMATIC
EXPERIMENTAL
COMPARISONS
ARE
REPORTED
IN
MIKOLAJCZYK
AND
SCHMID
MIKOLAJCZYK
ET
AL
INTRODUCTION
THE
PURPOSE
OF
LOCAL
INVARIANT
FEATURES
IS
TO
PROVIDE
A
REPRESENTATION
THAT
ALLOWS
TO
EFFICIENTLY
MATCH
LOCAL
STRUCTURES
BETWEEN
IMAGES
THAT
IS
WE
WANT
TO
OBTAIN
A
SPARSE
SET
OF
LOCAL
MEASUREMENTS
THAT
CAPTURE
THE
ESSENCE
OF
THE
UNDERLYING
INPUT
IMAGES
AND
THAT
ENCODE
THEIR
INTERESTING
STRUCTURE
TO
MEET
THIS
GOAL
THE
FEATURE
EXTRACTORS
MUST
FULFILL
TWO
IMPORTANT
CRITERIA
THE
FEATURE
EXTRACTION
PROCESS
SHOULD
BE
REPEATABLE
AND
PRECISE
SO
THAT
THE
SAME
FEATURES
ARE
EXTRACTED
FROM
TWO
IMAGES
SHOWING
THE
SAME
OBJECT
AT
THE
SAME
TIME
THE
FEATURES
SHOULD
BE
DISTINCTIVE
SO
THAT
DIFFERENT
IMAGE
STRUCTURES
CAN
BE
TOLD
APART
FROM
EACH
OTHER
IN
ADDITION
WE
TYPICALLY
REQUIRE
A
SUFFICIENT
NUMBER
OF
FEATURE
REGIONS
TO
COVER
THE
TARGET
OBJECT
SO
THAT
IT
CAN
STILL
BE
RECOGNIZED
UNDER
PARTIAL
OCCLUSION
THIS
IS
ACHIEVED
BY
THE
FOLLOWING
FEATURE
EXTRACTION
PIPELINE
ILLUSTRATED
IN
FIGURE
FIND
A
SET
OF
DISTINCTIVE
KEYPOINTS
DEFINE
A
REGION
AROUND
EACH
KEYPOINT
IN
A
SCALE
OR
AFFINE
INVARIANT
MANNER
EXTRACT
AND
NORMALIZE
THE
REGION
CONTENT
N
PIXELS
D
F
A
FB
T
FIGURE
AN
ILLUSTRATION
OF
THE
RECOGNITION
PROCEDURE
WITH
LOCAL
FEATURES
WE
FIRST
FIND
DISTINCTIVE
KEYPOINTS
IN
BOTH
IMAGES
FOR
EACH
SUCH
KEYPOINT
WE
THEN
DEFINE
A
SURROUNDING
REGION
IN
A
SCALE
AND
ROTATION
INVARIANT
MANNER
WE
EXTRACT
AND
NORMALIZE
THE
REGION
CONTENT
AND
COMPUTE
A
LOCAL
DESCRIPTOR
FOR
EACH
REGION
FEATURE
MATCHING
IS
THEN
PERFORMED
BY
COMPARING
THE
LOCAL
DESCRIPTORS
USING
A
SUITABLE
SIMILARITY
MEASURE
COURTESY
OF
KRYSTIAN
MIKOLAJCZYK
COMPUTE
A
DESCRIPTOR
FROM
THE
NORMALIZED
REGION
MATCH
THE
LOCAL
DESCRIPTORS
IN
THE
REMAINDER
OF
THIS
CHAPTER
WE
WILL
DISCUSS
THE
KEYPOINT
AND
DESCRIPTOR
STEPS
IN
DETAIL
THEN
IN
THE
FOLLOWING
CHAPTER
WE
WILL
DESCRIBE
METHODS
FOR
COMPUTING
CANDIDATE
MATCHES
AMONG
THE
DESCRIPTORS
ONCE
WE
HAVE
CANDIDATE
MATCHES
WE
CAN
THEN
PROCEED
TO
VERIFY
THEIR
GEOMETRIC
RELATIONSHIPS
AS
WE
WILL
DESCRIBE
IN
CHAPTER
DETECTION
OF
INTEREST
POINTS
AND
REGIONS
KEYPOINT
LOCALIZATION
THE
FIRST
STEP
OF
THE
LOCAL
FEATURE
EXTRACTION
PIPELINE
IS
TO
FIND
A
SET
OF
DISTINCTIVE
KEYPOINTS
THAT
CAN
BE
RELIABLY
LOCALIZED
UNDER
VARYING
IMAGING
CONDITIONS
VIEWPOINT
CHANGES
AND
IN
THE
PRESENCE
OF
NOISE
IN
PARTICULAR
THE
EXTRACTION
PROCEDURE
SHOULD
YIELD
THE
SAME
FEATURE
LOCATIONS
IF
THE
INPUT
IMAGE
IS
TRANSLATED
OR
ROTATED
IT
IS
OBVIOUS
THAT
THOSE
CRITERIA
CANNOT
BE
MET
FOR
ALL
IMAGE
POINTS
FOR
INSTANCE
IF
WE
CONSIDER
A
POINT
LYING
IN
A
UNIFORM
REGION
WE
CANNOT
DETERMINE
ITS
EXACT
MOTION
SINCE
WE
CANNOT
DISTINGUISH
THE
POINT
FROM
ITS
NEIGHBORS
SIMILARLY
IF
WE
CONSIDER
A
POINT
ON
A
STRAIGHT
LINE
WE
CAN
ONLY
MEASURE
ITS
MOTION
PERPENDICULAR
TO
THE
LINE
THIS
MOTIVATES
US
TO
FOCUS
ON
A
PARTICULAR
SUBSET
OF
POINTS
NAMELY
THOSE
EXHIBITING
SIGNAL
CHANGES
IN
TWO
DIRECTIONS
IN
THE
FOLLOWING
WE
WILL
PRESENT
TWO
KEYPOINT
DETECTORS
THAT
EMPLOY
DIFFERENT
CRITERIA
FOR
FINDING
SUCH
REGIONS
THE
HESSIAN
DETECTOR
AND
THE
HARRIS
DETECTOR
THE
HESSIAN
DETECTOR
THE
HESSIAN
DETECTOR
BEAUDET
SEARCHES
FOR
IMAGE
LOCATIONS
THAT
EXHIBIT
STRONG
DERIVATIVES
IN
TWO
ORTHOGONAL
DIRECTIONS
IT
IS
BASED
ON
THE
MATRIX
OF
SECOND
DERIVATIVES
THE
SO
CALLED
HESSIAN
AS
DERIVATIVE
OPERATIONS
ARE
SENSITIVE
TO
NOISE
WE
ALWAYS
USE
GAUSSIAN
DERIVATIVES
IN
THE
FOLLOWING
I
E
WE
COMBINE
THE
DERIVATIVE
OPERATION
WITH
A
GAUSSIAN
SMOOTHING
STEP
WITH
SMOOTHING
PARAMETER
Σ
H
X
IXX
X
Σ
IXY
X
Σ
IXY
X
Σ
IYY
X
Σ
L
THE
DETECTOR
COMPUTES
THE
SECOND
DERIVATIVES
IXX
IXY
AND
IYY
FOR
EACH
IMAGE
POINT
AND
THEN
SEARCHES
FOR
POINTS
WHERE
THE
DETERMINANT
OF
THE
HESSIAN
BECOMES
MAXIMAL
DET
H
IXX
IYY
I
THIS
SEARCH
IS
USUALLY
PERFORMED
BY
COMPUTING
A
RESULT
IMAGE
CONTAINING
THE
HESSIAN
DETERMINANT
VALUES
AND
THEN
APPLYING
NON
MAXIMUM
SUPPRESSION
USING
A
WINDOW
IN
THIS
PROCEDURE
THE
SEARCH
WINDOW
IS
SWEPT
OVER
THE
ENTIRE
IMAGE
KEEPING
ONLY
PIXELS
WHOSE
VALUE
IS
LARGER
THAN
THE
VALUES
OF
ALL
IMMEDIATE
NEIGHBORS
INSIDE
THE
WINDOW
THE
DETECTOR
THEN
RETURNS
ALL
REMAINING
LOCATIONS
WHOSE
VALUE
IS
ABOVE
A
PRE
DEFINED
THRESHOLD
Θ
AS
SHOWN
IN
FIGURE
TOP
LEFT
THE
RESULTING
DETECTOR
RESPONSES
ARE
MAINLY
LOCATED
ON
CORNERS
AND
IN
STRONGLY
TEXTURED
IMAGE
AREAS
THE
HARRIS
DETECTOR
THE
POPULAR
HARRIS
FÖRSTNER
DETECTOR
FÖRSTNER
AND
GÜLCH
HARRIS
AND
STEPHENS
WAS
EXPLICITLY
DESIGNED
FOR
GEOMETRIC
STABILITY
IT
DEFINES
KEYPOINTS
TO
BE
POINTS
THAT
HAVE
LOCALLY
MAXIMAL
SELF
MATCHING
PRECISION
UNDER
TRANSLATIONAL
LEAST
SQUARES
TEMPLATE
MATCHING
TRIGGS
IN
PRACTICE
THESE
KEYPOINTS
OFTEN
CORRESPOND
TO
CORNER
LIKE
STRUCTURES
THE
DETECTION
PROCEDURE
IS
VISUALIZED
IN
FIGURE
THE
HARRIS
DETECTOR
PROCEEDS
BY
SEARCHING
FOR
POINTS
X
WHERE
THE
SECOND
MOMENT
MATRIX
C
AROUND
X
HAS
TWO
LARGE
EIGENVALUES
THE
MATRIX
C
CAN
BE
COMPUTED
FROM
THE
FIRST
DERIVATIVES
IN
A
WINDOW
AROUND
X
WEIGHTED
BY
A
GAUSSIAN
G
X
Σ
C
X
X
I
X
Σ
IXIY
X
Σ
L
FIGURE
THE
HARRIS
DETECTOR
SEARCHES
FOR
IMAGE
NEIGHBORHOODS
WHERE
THE
SECOND
MOMENT
MATRIX
C
HAS
TWO
LARGE
EIGENVALUES
CORRESPONDING
TO
TWO
DOMINANT
ORIENTATIONS
THE
RESULTING
POINTS
OFTEN
CORRESPOND
TO
CORNER
LIKE
STRUCTURES
COURTESY
OF
DENNIS
SIMAKOV
AND
DARYA
FROLOVA
IN
THIS
FORMULATION
THE
CONVOLUTION
WITH
THE
GAUSSIAN
G
X
Σ
TAKES
THE
ROLE
OF
SUMMING
OVER
ALL
PIXELS
IN
A
CIRCULAR
LOCAL
NEIGHBORHOOD
WHERE
EACH
PIXEL
CONTRIBUTION
IS
ADDITIONALLY
WEIGHTED
BY
ITS
PROXIMITY
TO
THE
CENTER
POINT
INSTEAD
OF
EXPLICITLY
COMPUTING
THE
EIGENVALUES
OF
C
THE
FOLLOWING
EQUIVALENCES
ARE
USED
DET
C
TRACE
C
TO
CHECK
IF
THEIR
RATIO
R
IS
BELOW
A
CERTAIN
THRESHOLD
WITH
C
R
DET
C
R
THIS
CAN
BE
EXPRESSED
BY
THE
FOLLOWING
CONDITION
DET
C
C
T
WHICH
AVOIDS
THE
NEED
TO
COMPUTE
THE
EXACT
EIGENVALUES
TYPICAL
VALUES
FOR
Α
ARE
IN
THE
RANGE
OF
THE
PARAMETER
Σ
IS
USUALLY
SET
TO
SO
THAT
THE
CONSIDERED
IMAGE
NEIGHBORHOOD
IS
SLIGHTLY
LARGER
THAN
THE
SUPPORT
OF
THE
DERIVATIVE
OPERATOR
USED
FIGURE
TOP
RIGHT
SHOWS
THE
RESULTS
OF
THE
HARRIS
DETECTOR
AND
COMPARES
THEM
TO
THOSE
OF
THE
HESSIAN
AS
CAN
BE
SEEN
THE
RETURNED
LOCATIONS
ARE
SLIGHTLY
DIFFERENT
AS
A
RESULT
OF
THE
CHANGED
SELECTION
CRITERION
IN
GENERAL
IT
CAN
BE
STATED
THAT
HARRIS
LOCATIONS
ARE
MORE
SPECIFIC
TO
CORNERS
F
II
I
X
F
II
I
X
M
M
FIGURE
THE
PRINCIPLE
BEHIND
AUTOMATIC
SCALE
SELECTION
GIVEN
A
KEYPOINT
LOCATION
WE
EVALUATE
A
SCALE
DEPENDENT
SIGNATURE
FUNCTION
ON
THE
KEYPOINT
NEIGHBORHOOD
AND
PLOT
THE
RESULTING
VALUE
AS
A
FUNCTION
OF
THE
SCALE
IF
THE
TWO
KEYPOINTS
CORRESPOND
TO
THE
SAME
STRUCTURE
THEN
THEIR
SIGNATURE
FUNCTIONS
WILL
TAKE
SIMILAR
SHAPES
AND
CORRESPONDING
NEIGHBORHOOD
SIZES
CAN
BE
DETERMINED
BY
SEARCHING
FOR
SCALE
SPACE
EXTREMA
OF
THE
SIGNATURE
FUNCTION
INDEPENDENTLY
IN
BOTH
IMAGES
COURTESY
OF
KRYSTIAN
MIKOLAJCZYK
WHILE
THE
HESSIAN
DETECTOR
ALSO
RETURNS
MANY
RESPONSES
ON
REGIONS
WITH
STRONG
TEXTURE
VARIATION
IN
ADDITION
HARRIS
POINTS
ARE
TYPICALLY
MORE
PRECISELY
LOCATED
AS
A
RESULT
OF
USING
FIRST
DERIVATIVES
RATHER
THAN
SECOND
DERIVATIVES
AND
OF
TAKING
INTO
ACCOUNT
A
LARGER
IMAGE
NEIGHBORHOOD
THUS
HARRIS
POINTS
ARE
PREFERABLE
WHEN
LOOKING
FOR
EXACT
CORNERS
OR
WHEN
PRECISE
LOCALIZATION
IS
REQUIRED
WHEREAS
HESSIAN
POINTS
CAN
PROVIDE
ADDITIONAL
LOCATIONS
OF
INTEREST
THAT
RESULT
IN
A
DENSER
COVERAGE
OF
THE
OBJECT
SCALE
INVARIANT
REGION
DETECTION
WHILE
SHOWN
TO
BE
REMARKABLY
ROBUST
TO
IMAGE
PLANE
ROTATIONS
ILLUMINATION
CHANGES
AND
NOISE
SCHMID
ET
AL
THE
LOCATIONS
RETURNED
BY
THE
HARRIS
AND
HESSIAN
DETECTORS
ARE
ONLY
REPEATABLE
UP
TO
RELATIVELY
SMALL
SCALE
CHANGES
THE
REASON
FOR
THIS
IS
THAT
BOTH
DETECTORS
RELY
ON
GAUSSIAN
DERIVATIVES
COMPUTED
AT
A
CERTAIN
FIXED
BASE
SCALE
Σ
IF
THE
IMAGE
SCALE
DIFFERS
TOO
MUCH
BETWEEN
THE
TEST
IMAGES
THEN
THE
EXTRACTED
STRUCTURES
WILL
ALSO
BE
DIFFERENT
FOR
SCALE
INVARIANT
FEATURE
EXTRACTION
IT
IS
THUS
NECESSARY
TO
DETECT
STRUCTURES
THAT
CAN
BE
RELIABLY
EXTRACTED
UNDER
SCALE
CHANGES
FIGURE
THE
SCALE
NORMALIZED
LAPLACIAN
OF
GAUSSIAN
LOG
IS
A
POPULAR
CHOICE
FOR
A
SCALE
SELECTION
FILTER
ITS
FILTER
MASK
TAKES
THE
SHAPE
OF
A
CIRCULAR
CENTER
REGION
WITH
POSITIVE
WEIGHTS
SURROUNDED
BY
ANOTHER
CIRCULAR
REGION
WITH
NEGATIVE
WEIGHTS
THE
FILTER
RESPONSE
IS
THEREFORE
STRONGEST
FOR
CIRCULAR
IMAGE
STRUCTURES
WHOSE
RADIUS
CORRESPONDS
TO
THE
FILTER
SCALE
AUTOMATIC
SCALE
SELECTION
THE
BASIC
IDEA
BEHIND
AUTOMATIC
SCALE
SELECTION
IS
VISUALIZED
IN
FIGURE
GIVEN
A
KEYPOINT
IN
EACH
IMAGE
OF
AN
IMAGE
PAIR
WE
WANT
TO
DETERMINE
WHETHER
THE
SURROUNDING
IMAGE
NEIGHBORHOODS
CONTAIN
THE
SAME
STRUCTURE
UP
TO
AN
UNKNOWN
SCALE
FACTOR
IN
PRINCIPLE
WE
COULD
ACHIEVE
THIS
BY
SAMPLING
EACH
IMAGE
NEIGHBORHOOD
AT
A
RANGE
OF
SCALES
AND
PERFORMING
N
N
PAIRWISE
COMPARISONS
TO
FIND
THE
BEST
MATCH
THIS
IS
HOWEVER
TOO
EXPENSIVE
TO
BE
OF
PRACTICAL
USE
INSTEAD
WE
EVALUATE
A
SIGNATURE
FUNCTION
ON
EACH
SAMPLED
IMAGE
NEIGHBORHOOD
AND
PLOT
THE
RESULT
VALUE
AS
A
FUNCTION
OF
THE
NEIGHBORHOOD
SCALE
SINCE
THE
SIGNATURE
FUNCTION
MEASURES
PROPERTIES
OF
THE
LOCAL
IMAGE
NEIGHBORHOOD
AT
A
CERTAIN
RADIUS
IT
SHOULD
TAKE
A
SIMILAR
QUALITATIVE
SHAPE
IF
THE
TWO
KEYPOINTS
ARE
CENTERED
ON
CORRESPONDING
IMAGE
STRUCTURES
THE
ONLY
DIFFERENCE
WILL
BE
THAT
ONE
FUNCTION
SHAPE
WILL
BE
SQUASHED
OR
EXPANDED
COMPARED
TO
THE
OTHER
AS
A
RESULT
OF
THE
SCALING
FACTOR
BETWEEN
THE
TWO
IMAGES
THUS
CORRESPONDING
NEIGHBORHOOD
SIZES
CAN
BE
DETECTED
BY
SEARCHING
FOR
EXTREMA
OF
THE
SIGNATURE
FUNCTION
INDEPENDENTLY
IN
BOTH
IMAGES
IF
CORRESPONDING
EXTREMA
Σ
AND
Σ
T
ARE
FOUND
IN
BOTH
CASES
THEN
THE
SCALING
FACTOR
BETWEEN
THE
TWO
IMAGES
CAN
BE
OBTAINED
AS
Σ
T
EFFECTIVELY
THIS
PROCEDURE
BUILDS
UP
A
SCALE
SPACE
WITKIN
OF
THE
RESPONSES
PRODUCED
BY
THE
APPLICATION
OF
A
LOCAL
KERNEL
WITH
VARYING
SCALE
PARAMETER
Σ
IN
ORDER
FOR
THIS
IDEA
TO
WORK
THE
SIGNATURE
FUNCTION
OR
KERNEL
NEEDS
TO
HAVE
CERTAIN
SPECIFIC
PROPERTIES
IT
CAN
BE
SHOWN
THAT
THE
ONLY
OPERATOR
THAT
FULFILLS
ALL
NECESSARY
CONDITIONS
FOR
THIS
PURPOSE
IS
THE
SCALE
NORMALIZED
GAUSSIAN
KERNEL
G
X
Σ
AND
ITS
DERIVATIVES
LINDEBERG
LXX
LYY
FIGURE
THE
LAPLACIAN
OF
GAUSSIAN
LOG
DETECTOR
SEARCHES
FOR
SCALE
SPACE
EXTREMA
OF
THE
LOG
FUNCTION
COURTESY
OF
KRYSTIAN
MIKOLAJCZYK
THE
LAPLACIAN
OF
GAUSSIAN
LOG
DETECTOR
BASED
ON
THE
ABOVE
IDEA
LINDEBERG
PROPOSED
A
DETECTOR
FOR
BLOB
LIKE
FEATURES
THAT
SEARCHES
FOR
SCALE
SPACE
EXTREMA
OF
A
SCALE
NORMALIZED
LAPLACIAN
OF
GAUSSIAN
LOG
LINDEBERG
L
X
Σ
Σ
IXX
X
Σ
IYY
X
Σ
AS
SHOWN
IN
FIGURE
THE
LOG
FILTER
MASK
CORRESPONDS
TO
A
CIRCULAR
CENTER
SURROUND
STRUCTURE
WITH
POSITIVE
WEIGHTS
IN
THE
CENTER
REGION
AND
NEGATIVE
WEIGHTS
IN
THE
SURROUNDING
RING
STRUCTURE
THUS
IT
WILL
YIELD
MAXIMAL
RESPONSES
IF
APPLIED
TO
AN
IMAGE
NEIGHBORHOOD
THAT
CONTAINS
A
SIMILAR
ROUGHLY
CIRCULAR
BLOB
STRUCTURE
AT
A
CORRESPONDING
SCALE
BY
SEARCHING
FOR
SCALE
SPACE
EXTREMA
OF
THE
LOG
WE
CAN
THEREFORE
DETECT
CIRCULAR
BLOB
STRUCTURES
NOTE
THAT
FOR
SUCH
BLOBS
A
REPEATABLE
KEYPOINT
LOCATION
CAN
ALSO
BE
DEFINED
AS
THE
BLOB
CENTER
THE
LOG
CAN
THUS
BOTH
BE
APPLIED
FOR
FINDING
THE
CHARACTERISTIC
SCALE
FOR
A
GIVEN
IMAGE
LOCATION
AND
FOR
DIRECTLY
DETECTING
SCALE
INVARIANT
REGIONS
BY
SEARCHING
FOR
LOCATION
SCALE
EXTREMA
OF
THE
LOG
THIS
LATTER
PROCEDURE
IS
VISUALIZED
IN
FIGURE
AND
RESULTING
INTEREST
REGIONS
ARE
SHOWN
IN
FIGURE
BOTTOM
LEFT
THE
DIFFERENCE
OF
GAUSSIAN
DOG
DETECTOR
AS
SHOWN
BY
LOWE
THE
SCALE
SPACE
LAPLACIAN
CAN
BE
APPROXIMATED
BY
A
DIFFERENCE
OF
GAUSSIAN
DOG
D
X
Σ
WHICH
CAN
BE
MORE
EFFICIENTLY
OBTAINED
FROM
THE
DIFFERENCE
OF
TWO
ADJACENT
SAMPLING
WITH
STEP
ORIGINAL
IMAGE
FIGURE
THE
DIFFERENCE
OF
GAUSSIAN
DOG
PROVIDES
A
GOOD
APPROXIMATION
FOR
THE
LAPLACIAN
OF
GAUSSIAN
IT
CAN
BE
EFFICIENTLY
COMPUTED
BY
SUBTRACTING
ADJACENT
SCALE
LEVELS
OF
A
GAUSSIAN
PYRA
MID
THE
DOG
REGION
DETECTOR
THEN
SEARCHES
FOR
SCALE
SPACE
EXTREMA
OF
THE
DOG
FUNCTION
FROM
TUYTELAARS
AND
MIKOLAJCZYK
SCALES
THAT
ARE
SEPARATED
BY
A
FACTOR
OF
K
D
X
Σ
G
X
KΣ
G
X
Σ
I
X
LOWE
SHOWS
THAT
WHEN
THIS
FACTOR
IS
CONSTANT
THE
COMPUTATION
ALREADY
INCLUDES
THE
REQUIRED
SCALE
NORMALIZATION
ONE
CAN
THEREFORE
DIVIDE
EACH
SCALE
OCTAVE
INTO
AN
EQUAL
NUMBER
K
OF
INTERVALS
SUCH
THAT
K
K
AND
ΣN
FOR
MORE
EFFICIENT
COMPUTATION
THE
RESULTING
SCALE
SPACE
CAN
BE
IMPLEMENTED
WITH
A
GAUSSIAN
PYRAMID
WHICH
RESAMPLES
THE
IMAGE
BY
A
FACTOR
OF
AFTER
EACH
SCALE
OCTAVE
SEE
FIGURE
AS
IN
THE
CASE
OF
THE
LOG
DETECTOR
DOG
INTEREST
REGIONS
ARE
DEFINED
AS
LOCATIONS
THAT
ARE
SIMULTANEOUSLY
EXTREMA
IN
THE
IMAGE
PLANE
AND
ALONG
THE
SCALE
COORDINATE
OF
THE
D
X
Σ
FUNCTION
SUCH
POINTS
ARE
FOUND
BY
COMPARING
THE
D
X
Σ
VALUE
OF
EACH
POINT
WITH
ITS
NEIGHBORHOOD
ON
THE
SAME
SCALE
LEVEL
AND
WITH
THE
CLOSEST
NEIGHBORS
ON
EACH
OF
THE
TWO
ADJACENT
LEVELS
AS
DEPICTED
IN
THE
RIGHT
SIDE
OF
FIGURE
SINCE
THE
SCALE
COORDINATE
IS
ONLY
SAMPLED
AT
DISCRETE
LEVELS
IT
IS
IMPORTANT
IN
BOTH
THE
LOG
AND
THE
DOG
DETECTOR
TO
INTERPOLATE
THE
RESPONSES
AT
NEIGHBORING
SCALES
IN
ORDER
TO
INCREASE
THE
ACCURACY
OF
DETECTED
KEYPOINT
LOCATIONS
IN
THE
SIMPLEST
VERSION
THIS
COULD
BE
DONE
BY
FITTING
A
SECOND
ORDER
POLYNOMIAL
TO
EACH
CANDIDATE
POINT
AND
ITS
TWO
CLOSEST
NEIGHBORS
A
MORE
EXACT
APPROACH
WAS
INTRODUCED
BY
BROWN
AND
LOWE
THIS
APPROACH
SIMULTANEOUSLY
INTERPOLATES
BOTH
THE
LOCATION
AND
SCALE
COORDINATES
OF
DETECTED
PEAKS
BY
FITTING
A
QUADRIC
FUNCTION
FIGURE
EXAMPLE
RESULTS
OF
THE
TOP
LEFT
HESSIAN
DETECTOR
TOP
RIGHT
HARRIS
DETECTOR
BOTTOM
LEFT
LAPLACIAN
OF
GAUSSIAN
DETECTOR
BOTTOM
RIGHT
DIFFERENCE
OF
GAUSSIAN
DETECTOR
COURTESY
OF
KRYSTIAN
MIKOLAJCZYK
FROM
TUYTELAARS
AND
MIKOLAJCZYK
FINALLY
THOSE
REGIONS
ARE
KEPT
THAT
PASS
A
THRESHOLD
T
AND
WHOSE
ESTIMATED
SCALE
FALLS
INTO
A
CERTAIN
SCALE
RANGE
SMIN
SMAX
THE
RESULTING
INTEREST
POINT
OPERATOR
REACTS
TO
BLOB
LIKE
STRUCTURES
THAT
HAVE
THEIR
MAXIMAL
EXTENT
IN
A
RADIUS
OF
APPROXIMATELY
OF
THE
DETECTED
POINTS
AS
CAN
BE
DERIVED
FROM
THE
ZERO
CROSSINGS
OF
THE
MODELED
LAPLACIAN
IN
ORDER
TO
ALSO
CAPTURE
SOME
OF
THE
SURROUNDING
STRUCTURE
THE
EXTRACTED
REGION
IS
TYPICALLY
LARGER
MOST
CURRENT
INTEREST
REGION
DETECTORS
CHOOSE
A
RADIUS
OF
R
AROUND
THE
DETECTED
POINTS
FIGURE
BOTTOM
RIGHT
SHOWS
THE
RESULT
REGIONS
RETURNED
BY
THE
DOG
DETECTOR
ON
AN
EXAMPLE
IMAGE
IT
CAN
BE
SEEN
THAT
THE
OBTAINED
REGIONS
ARE
VERY
SIMILAR
TO
THOSE
OF
THE
LOG
DETECTOR
IN
PRACTICE
THE
DOG
DETECTOR
IS
THEREFORE
OFTEN
THE
PREFERRED
CHOICE
SINCE
IT
CAN
BE
COMPUTED
FAR
MORE
EFFICIENTLY
THE
HARRIS
LAPLACIAN
DETECTOR
THE
HARRIS
LAPLACIAN
OPERATOR
MIKOLAJCZYK
AND
SCHMID
WAS
PROPOSED
FOR
INCREASED
DISCRIMINATIVE
POWER
COMPARED
TO
THE
LAPLACIAN
OR
DOG
OPERATORS
DESCRIBED
SO
FAR
IT
COMBINES
THE
HARRIS
OPERATOR
SPECIFICITY
FOR
CORNER
LIKE
STRUCTURES
WITH
THE
SCALE
SELECTION
MECHANISM
BY
LINDEBERG
THE
METHOD
FIRST
BUILDS
UP
TWO
SEPARATE
SCALE
SPACES
FOR
THE
HARRIS
FUNCTION
AND
THE
LAPLACIAN
IT
THEN
USES
THE
HARRIS
FUNCTION
TO
LOCALIZE
CANDIDATE
POINTS
ON
EACH
SCALE
LEVEL
AND
SELECTS
THOSE
POINTS
FOR
WHICH
THE
LAPLACIAN
SIMULTANEOUSLY
ATTAINS
AN
EXTREMUM
OVER
SCALES
THE
RESULTING
POINTS
ARE
ROBUST
TO
CHANGES
IN
SCALE
IMAGE
ROTATION
ILLUMINATION
AND
CAMERA
NOISE
IN
ADDITION
THEY
ARE
HIGHLY
DISCRIMINATIVE
AS
SEVERAL
COMPARATIVE
STUDIES
SHOW
MIKOLAJCZYK
AND
SCHMID
AS
A
DRAWBACK
HOWEVER
THE
ORIGINAL
HARRIS
LAPLACIAN
DETECTOR
TYPICALLY
RETURNS
A
MUCH
SMALLER
NUMBER
OF
POINTS
THAN
THE
LAPLACIAN
OR
DOG
DETECTORS
THIS
IS
NOT
A
RESULT
OF
CHANGED
THRESHOLD
SETTINGS
BUT
OF
THE
ADDITIONAL
CONSTRAINT
THAT
EACH
POINT
HAS
TO
FULFILL
TWO
DIFFERENT
MAXIMA
CONDITIONS
SIMULTANEOUSLY
FOR
MANY
PRACTICAL
OBJECT
RECOGNITION
APPLICATIONS
THE
LOWER
NUMBER
OF
INTEREST
REGIONS
MAY
BE
A
DISADVANTAGE
AS
IT
REDUCES
ROBUSTNESS
TO
PARTIAL
OCCLUSION
THIS
IS
ESPECIALLY
THE
CASE
FOR
OBJECT
CATEGORIZATION
WHERE
THE
POTENTIAL
NUMBER
OF
CORRESPONDING
FEATURES
IS
FURTHER
REDUCED
BY
INTRA
CATEGORY
VARIABILITY
FOR
THIS
REASON
AN
UPDATED
VERSION
OF
THE
HARRIS
LAPLACIAN
DETECTOR
HAS
BEEN
PROPOSED
BASED
ON
A
LESS
STRICT
CRITERION
MIKOLAJCZYK
AND
SCHMID
INSTEAD
OF
SEARCHING
FOR
SIMULTANEOUS
MAXIMA
IT
SELECTS
SCALE
MAXIMA
OF
THE
LAPLACIAN
AT
LOCATIONS
FOR
WHICH
THE
HARRIS
FUNCTION
ALSO
ATTAINS
A
MAXIMUM
AT
ANY
SCALE
AS
A
RESULT
THIS
MODIFIED
DETECTOR
YIELDS
MORE
INTEREST
POINTS
AT
A
SLIGHTLY
LOWER
PRECISION
WHICH
RESULTS
IN
IMPROVED
PERFORMANCE
FOR
APPLICATIONS
WHERE
A
LARGER
ABSOLUTE
NUMBER
OF
INTEREST
REGIONS
IS
REQUIRED
MIKOLAJCZYK
ET
AL
THE
HESSIAN
LAPLACE
DETECTOR
AS
IN
THE
CASE
OF
THE
HARRIS
LAPLACE
THE
SAME
IDEA
CAN
ALSO
BE
APPLIED
TO
THE
HESSIAN
LEADING
TO
THE
HESSIAN
LAPLACE
DETECTOR
AS
WITH
THE
SINGLE
SCALE
VERSIONS
THE
HESSIAN
LAPLACE
DETECTOR
TYPICALLY
RETURNS
MORE
INTEREST
REGIONS
THAN
HARRIS
LAPLACE
AT
A
SLIGHTLY
LOWER
REPEATABILITY
MIKOLAJCZYK
ET
AL
AFFINE
COVARIANT
REGION
DETECTION
THE
APPROACHES
DISCUSSED
SO
FAR
YIELD
LOCAL
FEATURES
THAT
CAN
BE
EXTRACTED
IN
A
MANNER
THAT
IS
INVARIANT
TO
TRANSLATION
AND
SCALE
CHANGES
FOR
MANY
PRACTICAL
PROBLEMS
HOWEVER
IT
ALSO
BECOMES
IMPORTANT
TO
FIND
FEATURES
THAT
CAN
BE
RELIABLY
EXTRACTED
UNDER
LARGE
VIEWPOINT
CHANGES
IF
WE
ASSUME
THAT
THE
SCENE
STRUCTURE
WE
ARE
INTERESTED
IN
IS
LOCALLY
PLANAR
THEN
THIS
WOULD
BOIL
DOWN
TO
ESTIMATING
AND
CORRECTING
FOR
THE
PERSPECTIVE
DISTORTION
A
LOCAL
IMAGE
PATCH
UNDERGOES
WHEN
SEEN
FROM
A
DIF
FERENT
VIEWPOINT
UNFORTUNATELY
SUCH
A
PERSPECTIVE
CORRECTION
IS
BOTH
COMPUTATIONALLY
EXPENSIVE
AND
ERROR
PRONE
SINCE
THE
LOCAL
FEATURE
PATCHES
TYPICALLY
CONTAIN
ONLY
A
SMALL
NUMBER
OF
PIXELS
IT
HAS
HOWEVER
BEEN
SHOWN
BY
A
NUMBER
OF
RESEARCHERS
MATAS
ET
AL
MIKOLAJCZYK
AND
SCHMID
SCHAFFALITZKY
AND
ZISSERMAN
TUYTELAARS
AND
VAN
GOOL
THAT
A
LOCAL
AFFINE
APPROXIMATION
IS
SUFFICIENT
IN
SUCH
CASES
WE
THEREFORE
AIM
TO
EXTEND
THE
REGION
EXTRACTION
PROCEDURE
TO
AFFINE
COVARIANT
WHILE
A
SCALE
AND
ROTATION
INVARIANT
REGION
CAN
BE
DESCRIBED
BY
A
CIRCLE
AN
AFFINE
DEFORMATION
TRANSFORMS
THIS
CIRCLE
TO
AN
ELLIPSE
WE
THUS
AIM
TO
FIND
LOCAL
REGIONS
FOR
WHICH
SUCH
AN
ELLIPSE
CAN
BE
RELIABLY
AND
REPEATEDLY
EXTRACTED
PURELY
FROM
LOCAL
IMAGE
PROPERTIES
LITERATURE
SPEAKS
OF
AFFINE
COVARIANT
EXTRACTION
HERE
IN
ORDER
TO
EMPHASIZE
THE
PROPERTY
THAT
EXTRACTED
REGION
SHAPES
VARY
ACCORDING
TO
THE
UNDERLYING
AFFINE
DEFORMATION
THIS
IS
REQUIRED
SO
THAT
THE
REGION
CONTENT
WILL
BE
INVARIANT
HARRIS
AND
HESSIAN
AFFINE
DETECTORS
BOTH
THE
HARRIS
LAPLACE
AND
HESSIAN
LAPLACE
DETECTORS
CAN
BE
EXTENDED
TO
YIELD
AFFINE
COVARIANT
REGIONS
THIS
IS
DONE
BY
THE
FOLLOWING
ITERATIVE
ESTIMATION
SCHEME
THE
PROCEDURE
IS
INITIALIZED
WITH
A
CIRCULAR
REGION
RETURNED
BY
THE
ORIGINAL
SCALE
INVARIANT
DETECTOR
IN
EACH
ITERATION
WE
BUILD
UP
THE
REGION
SECOND
MOMENT
MATRIX
AND
COMPUTE
THE
EIGENVALUES
OF
THIS
MATRIX
THIS
YIELDS
AN
ELLIPTICAL
SHAPE
AS
SHOWN
IN
FIGURE
CORRESPONDING
TO
A
LOCAL
AFFINE
DEFORMATION
WE
THEN
TRANSFORM
THE
IMAGE
NEIGHBORHOOD
SUCH
THAT
THIS
ELLIPSE
IS
TRANSFORMED
TO
A
CIRCLE
AND
UPDATE
THE
LOCATION
AND
SCALE
ESTIMATE
IN
THE
TRANSFORMED
IMAGE
THIS
PROCEDURE
IS
REPEATED
UNTIL
THE
EIGENVALUES
OF
THE
SECOND
MOMENT
MATRIX
ARE
APPROXIMATELY
EQUAL
AS
A
RESULT
OF
THIS
ITERATIVE
ESTIMATION
SCHEME
WE
OBTAIN
A
SET
OF
ELLIPTICAL
REGIONS
WHICH
ARE
ADAPTED
TO
THE
LOCAL
INTENSITY
PATTERNS
SO
THAT
THE
SAME
OBJECT
STRUCTURES
ARE
COVERED
DESPITE
THE
DEFORMATIONS
CAUSED
BY
VIEWPOINT
CHANGES
MAXIMALLY
STABLE
EXTREMAL
REGIONS
MSER
A
DIFFERENT
APPROACH
FOR
FINDING
AFFINE
COVARIANT
REGIONS
HAS
BEEN
PROPOSED
BY
MATAS
ET
AL
IN
CONTRAST
TO
THE
ABOVE
METHODS
WHICH
START
FROM
KEYPOINTS
AND
PROGRESSIVELY
ADD
INVARIANCE
LEVELS
THIS
APPROACH
STARTS
FROM
A
SEGMENTATION
PERSPECTIVE
IT
APPLIES
A
WATERSHED
SEGMENTATION
ALGORITHM
TO
THE
IMAGE
AND
EXTRACTS
HOMOGENEOUS
INTENSITY
REGIONS
WHICH
ARE
STABLE
OVER
A
LARGE
RANGE
OF
THRESHOLDS
THUS
ENDING
UP
WITH
MAXIMALLY
STABLE
EXTREMAL
REGIONS
MSER
BY
CONSTRUCTION
THOSE
REGIONS
ARE
STABLE
OVER
A
RANGE
OF
IMAGING
CONDITIONS
AND
CAN
STILL
BE
RELIABLY
EXTRACTED
UNDER
VIEWPOINT
CHANGES
SINCE
THEY
ARE
GENERATED
BY
A
SEGMENTATION
PROCESS
THEY
ARE
NOT
RESTRICTED
TO
ELLIPTICAL
SHAPES
BUT
CAN
HAVE
COMPLICATED
CONTOURS
IN
FACT
THE
CONTOUR
SHAPE
ITSELF
IS
OFTEN
A
GOOD
FEATURE
WHICH
HAS
LED
TO
THE
CONSTRUCTION
OF
SPECIALIZED
CONTOUR
FEATURE
DESCRIPTORS
MATAS
ET
AL
FOR
CONSISTENCY
WITH
THE
OTHER
FEATURE
EXTRACTION
STEPS
DISCUSSED
HERE
AN
ELLIPTICAL
REGION
CAN
HOWEVER
ALSO
EASILY
BE
FITTED
TO
THE
MAXIMALLY
STABLE
REGIONS
BY
COMPUTING
THE
EIGENVECTORS
OF
THEIR
SECOND
MOMENT
MATRICES
OTHER
INTEREST
REGION
DETECTORS
SEVERAL
OTHER
INTEREST
REGION
DETECTORS
HAVE
BEEN
PROPOSED
THAT
ARE
NOT
DISCUSSED
HERE
TUYTE
LAARS
VAN
GOOL
INTRODUCED
DETECTORS
FOR
AFFINE
COVARIANT
INTENSITY
BASED
REGIONS
IBR
AND
EDGE
BASED
REGIONS
EBR
TUYTELAARS
AND
VAN
GOOL
KADIR
BRADY
PROPOSED
A
SALIENT
REGIONS
DETECTOR
THAT
WAS
LATER
ON
ALSO
EXTENDED
TO
AFFINE
COVARIANT
EXTRACTION
KADIR
AND
BRADY
KADIR
ET
AL
AN
OVERVIEW
OVER
THOSE
DETECTORS
AND
A
DISCUSSION
OF
THEIR
MERITS
CAN
BE
FOUND
IN
TUYTELAARS
AND
MIKOLAJCZYK
ORIENTATION
NORMALIZATION
AFTER
A
SCALE
INVARIANT
REGION
HAS
BEEN
DETECTED
ITS
CONTENT
NEEDS
TO
BE
NORMALIZED
FOR
ROTATION
INVARIANCE
THIS
IS
TYPICALLY
DONE
BY
FINDING
THE
REGION
DOMINANT
ORIENTATION
AND
THEN
ROTATING
THE
REGION
CONTENT
ACCORDING
TO
THIS
ANGLE
IN
ORDER
TO
BRING
THE
REGION
INTO
A
CANONICAL
ORIENTATION
LOWE
SUGGESTS
THE
FOLLOWING
PROCEDURE
FOR
THE
ORIENTATION
NORMALIZATION
STEP
FOR
EACH
DETECTED
INTEREST
REGION
THE
REGION
SCALE
IS
USED
TO
SELECT
THE
CLOSEST
LEVEL
OF
THE
GAUSSIAN
PYRAMID
SO
THAT
ALL
FOLLOWING
COMPUTATIONS
ARE
PERFORMED
IN
A
SCALE
INVARIANT
MANNER
WE
THEN
BUILD
UP
A
GRADIENT
ORIENTATION
HISTOGRAM
WITH
BINS
COVERING
THE
RANGE
OF
ORIENTATIONS
FOR
EACH
PIXEL
IN
THE
REGION
THE
CORRESPONDING
GRADIENT
ORIENTATION
IS
ENTERED
INTO
THE
HISTOGRAM
WEIGHTED
BY
THE
PIXEL
GRADIENT
MAGNITUDE
AND
BY
A
GAUSSIAN
WINDOW
CENTERED
ON
THE
KEYPOINT
WITH
A
SCALE
OF
THE
HIGHEST
PEAK
IN
THE
ORIENTATION
HISTOGRAM
IS
TAKEN
AS
THE
DOMINANT
ORIENTATION
AND
A
PARABOLA
IS
FITTED
TO
THE
ADJACENT
HISTOGRAM
VALUES
TO
INTERPOLATE
THE
PEAK
POSITION
FOR
BETTER
ACCURACY
IN
PRACTICE
IT
MAY
HAPPEN
THAT
MULTIPLE
EQUALLY
STRONG
ORIENTATIONS
ARE
FOUND
FOR
A
SINGLE
INTEREST
REGION
IN
SUCH
CASES
SELECTING
ONLY
ONE
OF
THEM
WOULD
ENDANGER
THE
RECOGNITION
PROCEDURE
SINCE
SMALL
CHANGES
IN
THE
IMAGE
SIGNAL
COULD
CAUSE
ONE
OF
THE
OTHER
ORIENTATIONS
TO
BE
CHOSEN
INSTEAD
WHICH
COULD
LEAD
TO
FAILED
MATCHES
FOR
THIS
REASON
LOWE
SUGGESTS
TO
CREATE
A
SEPARATE
INTEREST
REGION
FOR
EACH
ORIENTATION
PEAK
THAT
REACHES
AT
LEAST
OF
THE
DOMINANT
PEAK
VALUE
LOWE
THIS
STRATEGY
SIGNIFICANTLY
IMPROVES
THE
REGION
DETECTOR
REPEATABILITY
AT
A
RELATIVELY
SMALL
ADDITIONAL
COST
ACCORDING
TO
LOWE
ONLY
ABOUT
OF
THE
POINTS
ARE
ASSIGNED
MULTIPLE
ORIENTATIONS
SUMMARY
OF
LOCAL
DETECTORS
SUMMARIZING
THE
ABOVE
WE
HAVE
SEEN
THE
FOLLOWING
LOCAL
FEATURE
DETECTORS
SO
FAR
IF
PRECISELY
LOCALIZED
POINTS
ARE
OF
INTEREST
WE
CAN
USE
THE
HARRIS
AND
HESSIAN
DETECTORS
WHEN
LOOKING
FOR
SCALE
INVARIANT
REGIONS
WE
CAN
CHOOSE
BETWEEN
THE
LOG
OR
DOG
DETECTORS
BOTH
OF
WHICH
REACT
TO
BLOB
SHAPED
STRUCTURES
IN
ADDITION
WE
CAN
COMBINE
THE
HARRIS
AND
HESSIAN
POINT
DETECTORS
WITH
THE
LAPLA
CIAN
SCALE
SELECTION
IDEA
TO
OBTAIN
THE
HARRIS
LAPLACIAN
AND
HESSIAN
LAPLACIAN
DETECTORS
FINALLY
WE
CAN
FURTHER
GENERALIZE
THOSE
DETECTORS
TO
AFFINE
COVARIANT
REGION
EXTRACTION
RESULTING
IN
THE
HARRIS
AFFINE
AND
HESSIAN
AFFINE
DETECTORS
THE
AFFINE
COVARIANT
REGION
DETECTORS
ARE
COMPLEMENTED
BY
THE
MSER
DETECTOR
WHICH
IS
BASED
ON
MAXIMALLY
STABLE
SEGMENTATION
REGIONS
ALL
OF
THOSE
DE
TECTORS
HAVE
BEEN
USED
IN
PRACTICAL
APPLICATIONS
DETAILED
EXPERIMENTAL
COMPARISONS
CAN
BE
FOUND
IN
MIKOLAJCZYK
AND
SCHMID
TUYTELAARS
AND
MIKOLAJCZYK
LOCAL
DESCRIPTORS
ONCE
A
SET
OF
INTEREST
REGIONS
HAS
BEEN
EXTRACTED
FROM
AN
IMAGE
THEIR
CONTENT
NEEDS
TO
BE
ENCODED
IN
A
DESCRIPTOR
THAT
IS
SUITABLE
FOR
DISCRIMINATIVE
MATCHING
THE
MOST
POPULAR
CHOICE
FOR
THIS
STEP
IS
THE
SIFT
DESCRIPTOR
LOWE
WHICH
WE
PRESENT
IN
DETAIL
IN
THE
FOLLOWING
THE
SIFT
DESCRIPTOR
THE
SCALE
INVARIANT
FEATURE
TRANSFORM
SIFT
WAS
ORIGINALLY
INTRODUCED
BY
LOWE
AS
COMBINATION
OF
A
DOG
INTEREST
REGION
DETECTOR
AND
A
CORRESPONDING
FEATURE
DESCRIPTOR
LOWE
HOWEVER
BOTH
COMPONENTS
HAVE
SINCE
THEN
ALSO
BEEN
USED
IN
ISOLATION
IN
PARTICULAR
A
SERIES
OF
STUDIES
HAS
LOCAL
DESCRIPTORS
KEYPOINT
DESCRIPTOR
FIGURE
VISUALIZATION
OF
THE
SIFT
DESCRIPTOR
COMPUTATION
FOR
EACH
ORIENTATION
NORMALIZED
SCALE
INVARIANT
REGION
IMAGE
GRADIENTS
ARE
SAMPLED
IN
A
REGULAR
GRID
AND
ARE
THEN
ENTERED
INTO
A
LARGER
GRID
OF
LOCAL
GRADIENT
ORIENTATION
HISTOGRAMS
FOR
VISIBILITY
REASONS
ONLY
A
GRID
IS
SHOWN
HERE
BASED
ON
LOWE
AND
LOWE
CONFIRMED
THAT
THE
SIFT
DESCRIPTOR
IS
SUITABLE
FOR
COMBINATION
WITH
ALL
OF
THE
ABOVE
MENTIONED
REGION
DETECTORS
AND
THAT
IT
ACHIEVES
GENERALLY
GOOD
PERFORMANCE
MIKOLAJCZYK
AND
SCHMID
THE
SIFT
DESCRIPTOR
AIMS
TO
ACHIEVE
ROBUSTNESS
TO
LIGHTING
VARIATIONS
AND
SMALL
POSITIONAL
SHIFTS
BY
ENCODING
THE
IMAGE
INFORMATION
IN
A
LOCALIZED
SET
OF
GRADIENT
ORIENTATION
HISTOGRAMS
THE
DESCRIPTOR
COMPUTATION
STARTS
FROM
A
SCALE
AND
ROTATION
NORMALIZED
REGION
EXTRACTED
WITH
ONE
OF
THE
ABOVE
MENTIONED
DETECTORS
AS
A
FIRST
STEP
THE
IMAGE
GRADIENT
MAGNITUDE
AND
ORIENTATION
IS
SAMPLED
AROUND
THE
KEYPOINT
LOCATION
USING
THE
REGION
SCALE
TO
SELECT
THE
LEVEL
OF
GAUSSIAN
BLUR
I
E
THE
LEVEL
OF
THE
GAUSSIAN
PYRAMID
AT
WHICH
THIS
COMPUTATION
IS
PERFORMED
SAMPLING
IS
PERFORMED
IN
A
REGULAR
GRID
OF
LOCATIONS
COVERING
THE
INTEREST
REGION
FOR
EACH
SAMPLED
LOCATION
THE
GRADIENT
ORIENTATION
IS
ENTERED
INTO
A
COARSER
GRID
OF
GRADIENT
ORIENTATION
HISTOGRAMS
WITH
ORIENTATION
BINS
EACH
WEIGHTED
BY
THE
CORRESPONDING
PIXEL
GRADIENT
MAGNITUDE
AND
BY
A
CIRCULAR
GAUSSIAN
WEIGHTING
FUNCTION
WITH
A
Σ
OF
HALF
THE
REGION
SIZE
THE
PURPOSE
OF
THIS
GAUSSIAN
WINDOW
IS
TO
GIVE
HIGHER
WEIGHTS
TO
PIXELS
CLOSER
TO
THE
MIDDLE
OF
THE
REGION
WHICH
ARE
LESS
AFFECTED
BY
SMALL
LOCALIZATION
INACCURACIES
OF
THE
INTEREST
REGION
DETECTOR
THIS
PROCEDURE
IS
VISUALIZED
FOR
A
SMALLER
GRID
IN
FIGURE
THE
MOTIVATION
FOR
THIS
CHOICE
OF
REPRESENTATION
IS
THAT
THE
COARSE
SPATIAL
BINNING
ALLOWS
FOR
SMALL
SHIFTS
DUE
TO
REGISTRATION
ERRORS
WITHOUT
OVERLY
AFFECTING
THE
DESCRIPTOR
AT
THE
SAME
TIME
THE
HIGH
DIMENSIONAL
REPRESENTATION
PROVIDES
ENOUGH
DISCRIMINATIVE
POWER
TO
RELIABLY
DISTINGUISH
A
LARGE
NUMBER
OF
KEYPOINTS
WHEN
COMPUTING
THE
DESCRIPTOR
IT
IS
IMPORTANT
TO
AVOID
ALL
BOUNDARY
EFFECTS
BOTH
WITH
RESPECT
TO
SPATIAL
SHIFTS
AND
TO
SMALL
ORIENTATION
CHANGES
THUS
WHEN
ENTERING
A
SAMPLED
PIXEL
GRADIENT
INFORMATION
INTO
THE
DIMENSIONAL
SPATIAL
ORIENTATION
HISTOGRAM
ITS
CONTRIBUTION
SHOULD
BE
SMOOTHLY
DISTRIBUTED
AMONG
THE
ADJOINING
HISTOGRAM
BINS
USING
TRILINEAR
INTERPOLATION
ONCE
ALL
ORIENTATION
HISTOGRAM
ENTRIES
HAVE
BEEN
COMPLETED
THOSE
ENTRIES
ARE
CONCATENATED
TO
FORM
A
SINGLE
DIMENSIONAL
FEATURE
VECTOR
A
FINAL
ILLUMINATION
NORMALIZATION
COMPLETES
THE
EXTRACTION
PROCEDURE
FOR
THIS
THE
VECTOR
IS
FIRST
NORMALIZED
TO
UNIT
LENGTH
THUS
ADJUSTING
FOR
CHANGING
IMAGE
CONTRAST
THEN
ALL
FEATURE
DIMENSIONS
ARE
THRESHOLDED
TO
A
MAXIMUM
VALUE
OF
AND
THE
VECTOR
IS
AGAIN
NORMALIZED
TO
UNIT
LENGTH
THIS
LAST
STEP
COMPENSATES
FOR
NON
LINEAR
ILLUMINATION
CHANGES
DUE
TO
CAMERA
SATURATION
OR
SIMILAR
EFFECTS
THE
SURF
DETECTOR
DESCRIPTOR
AS
LOCAL
FEATURE
DETECTORS
AND
DESCRIPTORS
HAVE
BECOME
MORE
WIDESPREAD
EFFICIENT
IMPLEMENTATIONS
ARE
GETTING
MORE
AND
MORE
IMPORTANT
CONSEQUENTLY
SEVERAL
APPROACHES
HAVE
BEEN
PROPOSED
IN
ORDER
TO
SPEED
UP
THE
INTEREST
REGION
EXTRACTION
AND
OR
DESCRIPTION
STAGES
BAY
ET
AL
CORNELIS
AND
VAN
GOOL
ROSTEN
AND
DRUMMOND
AMONG
THOSE
WE
WANT
TO
PICK
OUT
THE
SURF
SPEEDED
UP
ROBUST
FEATURES
APPROACH
WHICH
HAS
BEEN
DESIGNED
AS
AN
EFFICIENT
ALTERNATIVE
TO
SIFT
BAY
ET
AL
FIGURE
THE
SURF
DETECTOR
AND
DESCRIPTOR
WERE
DESIGNED
AS
AN
EFFICIENT
ALTERNATIVE
TO
SIFT
INSTEAD
OF
RELYING
ON
IDEAL
GAUSSIAN
DERIVATIVES
THEIR
COMPUTATION
IS
BASED
ON
SIMPLE
BOX
FILTERS
WHICH
CAN
BE
EFFICIENTLY
EVALUATED
USING
INTEGRAL
IMAGES
BASED
ON
BAY
ET
AL
SURF
COMBINES
A
HESSIAN
LAPLACE
REGION
DETECTOR
WITH
ITS
OWN
GRADIENT
ORIENTATION
BASED
FEATURE
DESCRIPTOR
INSTEAD
OF
RELYING
ON
GAUSSIAN
DERIVATIVES
FOR
ITS
INTERNAL
COMPUTATIONS
IT
IS
HOW
EVER
BASED
ON
SIMPLE
BOX
FILTERS
HAAR
WAVELETS
AS
SHOWN
IN
FIGURE
THOSE
BOX
FILTERS
APPROXIMATE
THE
EFFECTS
OF
THE
DERIVATIVE
FILTER
KERNELS
BUT
THEY
CAN
BE
EFFICIENTLY
EVALUATED
USING
INTEGRAL
IMAGES
VIOLA
AND
JONES
IN
PARTICULAR
THIS
EVALUATION
REQUIRES
THE
SAME
CONSTANT
NUMBER
OF
LOOKUPS
REGARDLESS
OF
THE
IMAGE
SCALE
THUS
REMOVING
THE
NEED
FOR
A
GAUSSIAN
PYRAMID
CONCLUDING
REMARKS
DESPITE
THIS
SIMPLIFICATION
SURF
HAS
BEEN
SHOWN
TO
ACHIEVE
COMPARABLE
REPEATABILITY
AS
DETEC
TORS
BASED
ON
STANDARD
GAUSSIAN
DERIVATIVES
WHILE
YIELDING
SPEEDUPS
OF
MORE
THAN
A
FACTOR
OF
FIVE
COMPARED
TO
STANDARD
DOG
THE
SURF
DESCRIPTOR
IS
ALSO
MOTIVATED
BY
SIFT
AND
PURSUES
A
SIMILAR
SPATIAL
BINNING
STRATEGY
DIVIDING
THE
FEATURE
REGION
INTO
A
GRID
HOWEVER
INSTEAD
OF
BUILDING
UP
A
GRADIENT
ORIENTATION
HISTOGRAM
FOR
EACH
BIN
SURF
ONLY
COMPUTES
A
SET
OF
SUMMARY
STATISTICS
DX
DX
DY
AND
DY
RESULTING
IN
A
DIMENSIONAL
DESCRIPTOR
OR
A
SLIGHTLY
EXTENDED
SET
RESULTING
IN
A
DIMENSIONAL
DESCRIPTOR
VERSION
MOTIVATED
BY
THE
SUCCESS
OF
SURF
A
FURTHER
OPTIMIZED
VERSION
HAS
BEEN
PROPOSED
BY
CORNELIS
AND
VAN
GOOL
THAT
TAKES
ADVANTAGE
OF
THE
COMPUTATIONAL
POWER
AVAILABLE
IN
CURRENT
CUDA
ENABLED
GRAPHICS
CARDS
THIS
GPUSURF
IMPLEMENTATION
HAS
BEEN
REPORTED
TO
PER
FORM
FEATURE
EXTRACTION
FOR
A
IMAGE
AT
FRAME
RATES
UP
TO
HZ
I
E
TAKING
ONLY
PER
FRAME
THUS
MAKING
FEATURE
EXTRACTION
A
TRULY
AFFORDABLE
PROCESSING
STEP
CONCLUDING
REMARKS
THE
FIRST
STEP
IN
THE
SPECIFIC
OBJECT
RECOGNITION
PIPELINE
IS
TO
EXTRACT
LOCAL
FEATURES
FROM
BOTH
THE
TRAINING
AND
TEST
IMAGES
FOR
THIS
WE
CAN
USE
ANY
OF
THE
FEATURE
DETECTORS
AND
DESCRIPTORS
DESCRIBED
IN
THIS
CHAPTER
THE
APPROPRIATE
CHOICE
OF
DETECTOR
CLASS
SINGLE
SCALE
SCALE
INVARIANT
OR
AFFINE
INVARIANT
MAINLY
DEPENDS
ON
THE
TYPE
OF
VIEWPOINT
VARIATIONS
FORESEEN
FOR
THE
TARGET
APPLICATION
FOR
MANY
PRACTICAL
RECOGNITION
APPLICATIONS
SCALE
INVARIANT
FEATURES
IN
PARTICULAR
SIFT
LOWE
HAVE
PROVEN
A
GOOD
COMPROMISE
SINCE
THEY
ARE
FAST
TO
EXTRACT
ARE
ROBUST
TO
MODERATE
VIEWPOINT
VARIATIONS
AND
RETAIN
ENOUGH
DISCRIMINATIVE
POWER
TO
ALLOW
FOR
RELIABLE
MATCHING
WHEN
DEALING
WITH
LARGER
VIEWPOINT
CHANGES
AS
IN
WIDE
BASELINE
STEREO
MATCHING
APPLICATIONS
AFFINE
INVARIANCE
BECOMES
IMPORTANT
IN
ORDER
TO
STILL
ESTABLISH
CORRESPONDENCES
HOWEVER
THE
ADDED
INVARIANCE
COMES
AT
THE
PRICE
OF
REDUCED
DISCRIMINATIVE
POWER
SINCE
SEVERAL
DIFFERENT
ELLIPTICAL
REGIONS
CAN
BE
MAPPED
ONTO
THE
SAME
CIRCULAR
DESCRIPTOR
NEIGHBORHOOD
AND
GENERALLY
ALSO
A
SMALLER
NUMBER
OF
FEATURES
SINCE
NOT
ALL
REGIONS
HAVE
A
CHARACTERISTIC
AFFINE
COVARIANT
NEIGHBORHOOD
WHENEVER
POSSIBLE
IT
IS
THEREFORE
ADVISABLE
TO
USE
THE
SIMPLER
FEATURE
REPRESENTATION
THE
DEVELOPMENT
OF
LOCAL
INVARIANT
FEATURES
HAS
HAD
AN
ENORMOUS
IMPACT
IN
MANY
AREAS
OF
COMPUTER
VISION
INCLUDING
WIDE
BASELINE
STEREO
MATCHING
IMAGE
RETRIEVAL
OBJECT
RECOGNITION
AND
CATEGORIZATION
THEY
HAVE
PROVIDED
THE
BASIS
FOR
MANY
STATE
OF
THE
ART
ALGORITHMS
AND
HAVE
LED
TO
A
NUMBER
OF
NEW
DEVELOPMENTS
MOREOVER
EFFICIENT
IMPLEMENTATIONS
FOR
ALL
DETECTORS
DISCUSSED
IN
THIS
CHAPTER
ARE
FREELY
AVAILABLE
GPU
OXF
SUR
MAKING
THEM
TRULY
BUILDING
BLOCKS
THAT
OTHER
RESEARCHERS
CAN
BUILD
ON
IN
THE
NEXT
TWO
CHAPTERS
WE
DISCUSS
HOW
TO
COMPUTE
CANDIDATE
MATCHING
DESCRIPTORS
AND
THEN
DESCRIBE
HOW
THE
GEOMETRIC
CONSISTENCY
OF
THOSE
CANDIDATE
MATCHES
ARE
VERIFIED
TO
PERFORM
SPECIFIC
OBJECT
RECOGNITION
IN
LATER
CHAPTERS
WE
WILL
AGAIN
DRAW
ON
LOCAL
DESCRIPTORS
TO
BUILD
MODELS
FOR
GENERIC
OBJECT
CATEGORIZATION
CHAP
TER
MATCHING
LOCAL
FEATURES
IN
THE
PREVIOUS
CHAPTER
WE
SAW
SEVERAL
SPECIFIC
TECHNIQUES
TO
DETECT
REPEATABLE
INTEREST
POINTS
IN
AN
IMAGE
SECTION
AND
THEN
ROBUSTLY
DESCRIBE
THE
LOCAL
APPEARANCE
AT
EACH
SUCH
POINT
SECTION
NOW
GIVEN
AN
IMAGE
AND
ITS
LOCAL
FEATURES
WE
NEED
TO
BE
ABLE
TO
MATCH
THEM
TO
SIMILAR
LOOKING
LOCAL
FEATURES
IN
OTHER
IMAGES
E
G
TO
MODEL
IMAGES
OF
THE
SPECIFIC
OBJECTS
WE
ARE
TRYING
TO
RECOGNIZE
SEE
FIGURE
TO
IDENTIFY
CANDIDATE
MATCHES
WE
ESSENTIALLY
WANT
TO
SEARCH
AMONG
ALL
PREVIOUSLY
SEEN
LOCAL
DESCRIPTORS
AND
RETRIEVE
THOSE
THAT
ARE
NEAREST
ACCORDING
TO
EUCLIDEAN
DISTANCE
IN
THE
FEATURE
SPACE
SUCH
AS
THE
DIMENSIONAL
SIFT
SPACE
BECAUSE
THE
LOCAL
DESCRIPTIONS
ARE
BY
DESIGN
INVARIANT
TO
ROTATIONS
TRANSLATIONS
SCALINGS
AND
SOME
PHOTOMETRIC
EFFECTS
THIS
MATCHING
STAGE
WILL
BE
ABLE
TO
TOLERATE
REASONABLE
VARIATIONS
IN
VIEW
POINT
POSE
AND
ILLUMINATION
ACROSS
THE
VIEWS
OF
THE
OBJECT
FURTHER
DUE
TO
THE
FEATURES
DISTINCTIVE
NESS
IF
WE
DETECT
A
GOOD
CORRESPONDENCE
BASED
ON
THE
LOCAL
FEATURE
MATCHES
ALONE
WE
WILL
ALREADY
HAVE
A
REASONABLE
MEASURE
OF
HOW
LIKELY
IT
IS
THAT
TWO
IMAGES
SHARE
THE
SAME
OBJECT
HOWEVER
TO
STRENGTHEN
CONFIDENCE
AND
ELIMINATE
AMBIGUOUS
MATCHES
IT
IS
COMMON
TO
FOLLOW
THE
MATCHING
PRO
CESS
DISCUSSED
IN
THIS
CHAPTER
WITH
A
CHECK
FOR
GEOMETRIC
CONSISTENCY
AS
WE
WILL
DISCUSS
IN
CHAPTER
THE
NAIVE
SOLUTION
TO
IDENTIFYING
LOCAL
FEATURE
MATCHES
IS
STRAIGHTFORWARD
SIMPLY
SCAN
THROUGH
ALL
PREVIOUSLY
SEEN
DESCRIPTORS
COMPARE
THEM
TO
THE
CURRENT
INPUT
DESCRIPTOR
AND
TAKE
THOSE
WITHIN
SOME
THRESHOLD
AS
CANDIDATES
UNFORTUNATELY
HOWEVER
SUCH
A
LINEAR
TIME
SCAN
IS
USUALLY
UNREALISTIC
IN
TERMS
OF
COMPUTATIONAL
COMPLEXITY
IN
MANY
PRACTICAL
APPLICATIONS
ONE
HAS
TO
SEARCH
FOR
MATCHES
IN
A
DATABASE
OF
MILLIONS
OF
FEATURES
THUS
EFFICIENT
ALGORITHMS
FOR
NEAREST
NEIGHBOR
OR
SIMILARITY
SEARCH
ARE
CRUCIAL
THE
FOCUS
OF
THIS
CHAPTER
IS
TO
DESCRIBE
THE
ALGORITHMS
FREQUENTLY
USED
IN
THE
RECOGNITION
PIPELINE
TO
RAPIDLY
MATCH
LOCAL
DESCRIPTORS
SPECIFICALLY
IN
THE
FIRST
SECTION
OF
THIS
CHAPTER
WE
OVERVIEW
BOTH
TREE
BASED
ALGORITHMS
FOR
EXACT
NEAR
NEIGHBOR
SEARCH
AS
WELL
AS
APPROXIMATE
NEAREST
NEIGHBOR
ALGORITHMS
LARGELY
HASHING
BASED
THAT
ARE
MORE
AMENABLE
FOR
HIGH
DIMENSIONAL
DESCRIPTORS
THEN
IN
SECTION
WE
DESCRIBE
A
FREQUENTLY
USED
ALTERNATIVE
BASED
ON
VISUAL
VOCABULARIES
INSTEAD
OF
PERFORMING
SIMILARITY
SEARCH
IN
THE
RAW
VECTOR
SPACE
OF
THE
DESCRIPTORS
THE
VOCABULARY
BASED
METHOD
FIRST
QUANTIZES
THE
FEATURE
SPACE
INTO
DISCRETE
VISUAL
WORDS
MAKING
IT
POSSIBLE
TO
INDEX
FEATURE
MATCHES
EASILY
WITH
AN
INVERTED
FILE
FACT
WHILE
OUR
FOCUS
AT
THIS
STAGE
IS
ON
LOCAL
FEATURE
MATCHING
FOR
SPECIFIC
OBJECT
RECOGNITION
MOST
OF
THE
ALGORITHMS
DISCUSSED
ARE
QUITE
GENERAL
AND
ALSO
COME
INTO
PLAY
FOR
OTHER
RECOGNITION
RELATED
SEARCH
TASKS
SUCH
AS
NEAR
NEIGHBOR
IMAGE
RETRIEVAL
MODEL
IMAGES
OR
EXEMPLARS
NPUT
FEATURES
IN
NEW
IMAGE
LOCAL
FEATURE
DESCRIPTORS
FROM
MODEL
IMAGES
CANDIDATE
MATCHES
BASE
ON
DESCRIPTOR
SIMILARITY
FIGURE
THE
GOAL
WHEN
MATCHING
LOCAL
FEATURES
IS
TO
FIND
THOSE
DESCRIPTORS
FROM
ANY
PREVIOUSLY
SEEN
MODEL
EXEMPLAR
THAT
ARE
NEAR
IN
THE
FEATURE
SPACE
TO
THOSE
LOCAL
FEATURES
IN
A
NOVEL
IMAGE
DEPICTED
ON
THE
LEFT
SINCE
EACH
EXEMPLAR
IMAGE
MAY
EASILY
CONTAIN
ON
THE
ORDER
OF
HUNDREDS
TO
THOUSANDS
OF
INTEREST
POINTS
THE
DATABASE
OF
DESCRIPTORS
QUICKLY
BECOMES
VERY
LARGE
TO
MAKE
SEARCHING
FOR
MATCHES
PRACTICAL
THE
DATABASE
MUST
BE
MAPPED
TO
DATA
STRUCTURES
FOR
EFFICIENT
SIMILARITY
SEARCH
EFFICIENT
SIMILARITY
SEARCH
WHAT
METHODS
ARE
EFFECTIVE
FOR
RETRIEVING
DESCRIPTORS
RELEVANT
TO
A
NEW
IMAGE
THE
CHOICE
FIRST
DEPENDS
ON
THE
DIMENSIONALITY
OF
THE
DESCRIPTORS
FOR
LOW
DIMENSIONAL
POINTS
EFFECTIVE
DATA
STRUC
TURES
FOR
EXACT
NEAREST
NEIGHBOR
SEARCH
ARE
KNOWN
E
G
KD
TREES
FRIEDMAN
ET
AL
FOR
HIGH
DIMENSIONAL
POINTS
THESE
METHODS
BECOME
INEFFICIENT
AND
SO
RESEARCHERS
OFTEN
EMPLOY
APPROXIMATE
SIMILARITY
SEARCH
METHODS
THIS
SECTION
OVERVIEWS
EXAMPLES
OF
BOTH
SUCH
TECHNIQUES
THAT
ARE
WIDELY
USED
IN
SPECIFIC
OBJECT
MATCHING
TREE
BASED
ALGORITHMS
DATA
STRUCTURES
USING
SPATIAL
PARTITIONS
AND
RECURSIVE
HYPERPLANE
DECOMPOSITION
PROVIDE
AN
EFFI
CIENT
MEANS
TO
SEARCH
LOW
DIMENSIONAL
VECTOR
DATA
EXACTLY
THE
KD
TREE
FRIEDMAN
ET
AL
IS
ONE
SUCH
APPROACH
THAT
HAS
OFTEN
BEEN
EMPLOYED
TO
MATCH
LOCAL
DESCRIPTORS
IN
SEVERAL
VARIANTS
E
G
BEIS
AND
LOWE
LOWE
MUJA
AND
LOWE
SILPA
ANAN
AND
HARTLEY
THE
KD
TREE
IS
A
BINARY
TREE
STORING
A
DATABASE
OF
K
DIMENSIONAL
POINTS
IN
ITS
LEAF
NODES
IT
RECUR
SIVELY
PARTITIONS
THE
POINTS
INTO
AXIS
ALIGNED
CELLS
DIVIDING
THE
POINTS
APPROXIMATELY
IN
HALF
BY
A
LINE
PERPENDICULAR
TO
ONE
OF
THE
K
COORDINATE
AXES
THE
DIVISION
STRATEGIES
AIM
TO
MAINTAIN
BALANCED
TREES
AND
OR
UNIFORMLY
SHAPED
CELLS
FOR
EXAMPLE
BY
CHOOSING
THE
NEXT
AXIS
TO
SPLIT
ACCORDING
TO
THAT
WHICH
HAS
THE
LARGEST
VARIANCE
AMONG
THE
DATABASE
POINTS
OR
BY
CYCLING
THROUGH
THE
AXES
IN
ORDER
TO
FIND
THE
POINT
NEAREST
TO
SOME
QUERY
ONE
TRAVERSES
THE
TREE
FOLLOWING
THE
SAME
DIVISIONS
THAT
WERE
USED
TO
ENTER
THE
DATABASE
POINTS
UPON
REACHING
A
LEAF
NODE
THE
POINTS
FOUND
THERE
ARE
COMPARED
TO
THE
QUERY
THE
NEAREST
ONE
BECOMES
THE
CURRENT
BEST
WHILE
THE
POINT
IS
NEARER
THAN
OTHERS
TO
THE
QUERY
IT
NEED
NOT
BE
THE
ABSOLUTE
NEAREST
FOR
EXAMPLE
CONSIDER
A
QUERY
OCCURRING
NEAR
THE
INITIAL
DIVIDING
SPLIT
AT
THE
TOP
OF
THE
TREE
WHICH
CAN
EASILY
BE
NEARER
TO
SOME
POINTS
ON
THE
OTHER
SIDE
OF
THE
DIVIDING
HYPERPLANE
THUS
THE
SEARCH
CONTINUES
BY
BACKTRACKING
ALONG
THE
UNEXPLORED
BRANCHES
CHECKING
WHETHER
THE
CIRCLE
FORMED
ABOUT
THE
QUERY
BY
THE
RADIUS
GIVEN
BY
THE
CURRENT
BEST
MATCH
INTERSECTS
WITH
A
SUBTREE
CELL
AREA
IF
IT
DOES
THAT
SUBTREE
IS
CONSIDERED
FURTHER
AND
ANY
NEARER
POINTS
FOUND
AS
THE
SEARCH
RECURSES
ARE
USED
TO
UPDATE
THE
CURRENT
BEST
IF
NOT
THE
SUBTREE
CAN
BE
PRUNED
SEE
FIGURE
FOR
A
SKETCH
OF
AN
EXAMPLE
TREE
AND
QUERY
THE
PROCEDURE
GUARANTEES
THAT
THE
NEAREST
POINT
WILL
BE
FOUND
CONSTRUCTING
THE
TREE
FOR
N
DATABASE
POINTS
AN
OFFLINE
COST
FOR
A
SINGLE
DATABASE
REQUIRES
O
N
LOG
N
TIME
INSERTING
POINTS
REQUIRES
O
LOG
N
TIME
PROCESSING
A
QUERY
REQUIRES
O
N
TIME
AND
THE
ALGORITHM
IS
KNOWN
TO
BE
QUITE
EFFECTIVE
FOR
LOW
DIMENSIONAL
DATA
I
E
FEWER
THAN
DIMENSIONS
IN
HIGH
DIMENSIONAL
SPACES
HOWEVER
THE
ALGORITHM
ENDS
UP
NEEDING
TO
VISIT
MANY
MORE
BRANCHES
DURING
THE
BACKTRACKING
STAGE
AND
IN
GENERAL
DEGRADES
TO
WORST
CASE
LINEAR
SCAN
PERFORMANCE
IN
PRACTICE
THE
PARTICULAR
BEHAVIOR
DEPENDS
NOT
ONLY
ON
THE
DIMENSION
OF
THE
POINTS
BUT
ALSO
THE
DISTRIBUTION
OF
THE
DATABASE
EXAMPLES
THAT
HAVE
BEEN
INDEXED
COMBINED
WITH
THE
CHOICES
IN
HOW
DIVISIONS
ARE
COMPUTED
OTHER
TYPES
OF
TREE
DATA
STRUCTURES
CAN
OPERATE
WITH
ARBITRARY
METRICS
CIACCIA
ET
AL
UHLMANN
REMOVING
THE
REQUIREMENT
OF
HAVING
DATA
IN
A
VECTOR
SPACE
BY
EXPLOITING
THE
TRIANGLE
INEQUALITY
HOWEVER
SIMILAR
TO
KD
TREES
THE
METRIC
TREES
IN
PRACTICE
RELY
ON
GOOD
HEURISTICS
FOR
SELECTING
USEFUL
PARTITIONING
STRATEGIES
AND
IN
SPITE
OF
LOGARITHMIC
QUERY
TIMES
IN
THE
EXPECTATION
ALSO
DEGENERATE
TO
A
LINEAR
TIME
SCAN
OF
ALL
ITEMS
DEPENDING
ON
THE
DISTRIBUTION
OF
DISTANCES
FOR
THE
DATA
SET
SINCE
HIGH
DIMENSIONAL
IMAGE
DESCRIPTORS
ARE
COMMONLY
USED
IN
OBJECT
RECOGNITION
SEVERAL
STRATEGIES
TO
MITIGATE
THESE
FACTORS
HAVE
BEEN
EXPLORED
ONE
IDEA
IS
TO
RELAX
THE
SEARCH
REQUIREMENT
TO
ALLOW
THE
RETURN
OF
APPROXIMATE
NEAREST
NEIGHBORS
USING
A
VARIANT
OF
KD
TREES
TOGETHER
WITH
A
PRIORITY
QUEUE
ARYA
ET
AL
BEIS
AND
LOWE
ANOTHER
IDEA
IS
TO
GENERATE
MULTIPLE
RANDOMIZED
KD
TREES
E
G
BY
SAMPLING
SPLITS
ACCORDING
TO
THE
COORDINATES
VARIANCE
AND
THEN
PROCESS
THE
QUERY
IN
ALL
TREES
USING
A
SINGLE
PRIORITY
QUEUE
ACROSS
THEM
SILPA
ANAN
AND
HARTLEY
GIVEN
THE
SENSITIVITY
OF
THE
ALGORITHMS
TO
THE
DATA
DISTRIBUTION
SOME
RECENT
WORK
ALSO
ATTEMPTS
TO
AUTOMATICALLY
SELECT
ALGORITHM
CONFIGURATIONS
AND
PARAMETERS
FOR
SATISFACTORY
PERFORMANCE
BY
USING
A
CROSS
VALIDATION
APPROACH
MUJA
AND
LOWE
ANOTHER
INTERESTING
DIRECTION
PURSUED
FOR
IMPROVING
THE
EFFICIENCY
AND
EFFECTIVENESS
OF
TREE
BASED
SEARCH
INVOLVES
INTEGRATING
LEARNING
OR
THE
MATCHING
TASK
INTO
THE
QUERIES
AND
K
NN
QUERIES
ARE
ALSO
SUPPORTED
A
BUILD
TREE
AND
POPULATE
WITH
DATABASE
POINTS
B
PERFORM
INITIAL
TRAVERSAL
FOR
A
QUERY
POINT
C
RECORD
CURRENT
BEST
NEIGHBOR
D
BACKTRACK
TO
THE
UNEXPLORED
SUB
TREES
E
UPDATE
NEAREST
POINT
AND
DISTANCE
IF
A
CLOSER
POINT
IS
FOUND
F
DISREGARD
SUBTREES
THAT
CANNOT
BE
ANY
CLOSER
FIGURE
SKETCH
OF
KD
TREE
PROCESSING
A
THE
DATABASE
POINTS
ARE
ENTERED
INTO
A
BINARY
TREE
WHERE
EACH
DIVISION
IS
AN
AXIS
ALIGNED
HYPERPLANE
B
GIVEN
A
NEW
QUERY
RED
POINT
FOR
WHICH
WE
WISH
TO
RETRIEVE
THE
NEAREST
NEIGHBOR
FIRST
THE
TREE
IS
TRAVERSED
CHOOSING
THE
LEFT
OR
RIGHT
SUBTREE
AT
EACH
NODE
ACCORDING
TO
THE
QUERY
VALUE
IN
THE
COORDINATE
ALONG
WHICH
THIS
DIVISION
WAS
KEYED
THE
GREEN
DOTTED
BOX
DENOTES
THE
CELL
CONTAINING
THE
POINTS
IN
THE
LEAF
NODE
REACHED
BY
THIS
QUERY
C
AT
THIS
POINT
WE
KNOW
THE
CURRENT
BEST
POINT
IS
THAT
IN
THE
LEAF
NODE
THAT
IS
CLOSEST
TO
THE
QUERY
DENOTED
WITH
THE
OUTER
RED
CIRCLE
D
THEN
WE
BACKTRACK
AND
CONSIDER
THE
OTHER
BRANCH
AT
EACH
NODE
THAT
WAS
VISITED
CHECKING
IF
ITS
CELL
INTERSECTS
THE
CURRENT
BEST
CIRCLE
AROUND
THE
QUERY
E
IF
SO
ITS
SUBTREE
IS
EXPLORED
FURTHER
AND
THE
CURRENT
BEST
RADIUS
IS
UPDATED
IF
A
NEARER
POINT
IS
FOUND
F
CONTINUE
AND
PRUNE
SUBTREES
ONCE
A
COMPARISON
AT
ITS
ROOT
SHOWS
THAT
IT
CANNOT
IMPROVE
ON
THE
CURRENT
NEAREST
POINT
COURTESY
OF
THE
AUTON
LAB
CARNEGIE
MELLON
UNIVERSITY
TREE
CONSTRUCTION
FOR
EXAMPLE
BY
USING
DECISION
TREES
IN
WHICH
EACH
INTERNAL
NODE
IS
ASSOCIATED
WITH
A
WEAK
CLASSIFIER
BUILT
WITH
SIMPLE
MEASUREMENTS
FROM
THE
FEATURE
PATCHES
LEPETIT
ET
AL
OBDRZALEK
AND
MATAS
HASHING
BASED
ALGORITHMS
AND
BINARY
CODES
HASHING
ALGORITHMS
ARE
AN
EFFECTIVE
ALTERNATIVE
TO
TREE
BASED
DATA
STRUCTURES
MOTIVATED
BY
THE
INADEQUACY
OF
EXISTING
EXACT
NEAREST
NEIGHBOR
TECHNIQUES
TO
PROVIDE
SUB
LINEAR
TIME
SEARCH
FOR
HIGH
DIMENSIONAL
DATA
INCLUDING
THE
KD
TREE
AND
METRIC
TREE
APPROACHES
DISCUSSED
ABOVE
RANDOMIZED
APPROXIMATE
HASHING
BASED
SIMILARITY
SEARCH
ALGORITHMS
HAVE
BEEN
EXPLORED
THE
IDEA
IN
APPROXIMATE
SIMILARITY
SEARCH
IS
TO
TRADE
OFF
SOME
PRECISION
IN
THE
SEARCH
FOR
THE
SAKE
OF
SUBSTANTIAL
QUERY
TIME
REDUCTIONS
MORE
SPECIFICALLY
GUARANTEES
ARE
OF
THE
GENERAL
FORM
IF
FOR
A
QUERY
POINT
Q
THERE
EXISTS
A
DATABASE
POINT
X
SUCH
THAT
D
Q
X
R
FOR
SOME
SEARCH
RADIUS
R
THEN
WITH
HIGH
PROBABILITY
A
POINT
XT
IS
RETURNED
SUCH
THAT
D
Q
XT
E
R
OTHERWISE
THE
ABSENCE
OF
SUCH
A
POINT
IS
REPORTED
LOCALITY
SENSITIVE
HASHING
LOCALITY
SENSITIVE
HASHING
LSH
CHARIKAR
DATAR
ET
AL
GIONIS
ET
AL
INDYK
AND
MOTWANI
IS
ONE
SUCH
ALGORITHM
THAT
OFFERS
SUB
LINEAR
TIME
SEARCH
BY
HASHING
HIGHLY
SIMILAR
EXAMPLES
TOGETHER
IN
A
HASH
TABLE
THE
IDEA
IS
THAT
IF
ONE
CAN
GUARANTEE
THAT
A
RAN
DOMIZED
HASH
FUNCTION
WILL
MAP
TWO
INPUTS
TO
THE
SAME
BUCKET
WITH
HIGH
PROBABILITY
ONLY
IF
THEY
ARE
SIMILAR
THEN
GIVEN
A
NEW
QUERY
ONE
NEEDS
ONLY
TO
SEARCH
THE
COLLIDING
DATABASE
EXAMPLES
TO
FIND
THOSE
THAT
ARE
MOST
PROBABLE
TO
LIE
IN
THE
INPUT
NEAR
NEIGHBORHOOD
THE
SEARCH
IS
APPROXIMATE
HOWEVER
AND
ONE
SACRIFICES
A
PREDICTABLE
DEGREE
OF
ERROR
IN
THE
SEARCH
IN
EXCHANGE
FOR
A
SIGNIFICANT
IMPROVEMENT
IN
QUERY
TIME
MORE
FORMALLY
A
FAMILY
OF
LSH
FUNCTIONS
IS
A
DISTRIBUTION
OF
FUNCTIONS
WHERE
FOR
ANY
TWO
OBJECTS
XI
AND
XJ
PR
H
F
RH
XI
H
XJ
SIM
XI
XJ
WHERE
SIM
XI
XJ
IS
SOME
SIMILARITY
FUNCTION
AND
H
X
IS
A
HASH
FUNCTION
DRAWN
FROM
F
THAT
RETURNS
A
SINGLE
BIT
CHARIKAR
CONCATENATING
A
SERIES
OF
B
HASH
FUNCTIONS
DRAWN
FROM
F
YIELDS
B
DIMENSIONAL
HASH
KEYS
WHEN
H
XI
H
XJ
XI
AND
XJ
COLLIDE
IN
THE
HASH
TABLE
BECAUSE
THE
PROBABILITY
THAT
TWO
INPUTS
COLLIDE
IS
EQUAL
TO
THE
SIMILARITY
BETWEEN
THEM
HIGHLY
SIMILAR
OBJECTS
ARE
INDEXED
TOGETHER
IN
THE
HASH
TABLE
WITH
HIGH
PROBABILITY
ON
THE
OTHER
HAND
IF
TWO
OBJECTS
ARE
VERY
DISSIMILAR
THEY
ARE
UNLIKELY
TO
SHARE
A
HASH
KEY
SEE
FIGURE
AT
QUERY
TIME
ONE
MAPS
THE
QUERY
TO
ITS
HASH
BUCKET
PULLS
UP
ANY
DATABASE
INSTANCES
ALSO
IN
THAT
BUCKET
AND
THEN
EXHAUSTIVELY
SEARCHES
ONLY
THOSE
FEW
EXAMPLES
IN
PRACTICE
MULTIPLE
HASH
TABLES
ARE
OFTEN
USED
EACH
WITH
INDEPENDENTLY
OF
THIS
GUARANTEE
FOR
NEAREST
NEIGHBORS
RATHER
THAN
R
RADIUS
NEIGHBORS
ALSO
EXIST
SENSITIVE
HASHING
HAS
BEEN
FORMULATED
IN
TWO
RELATED
CONTEXTS
ONE
IN
WHICH
THE
LIKELIHOOD
OF
COLLISION
IS
GUARANTEED
REL
ATIVE
TO
A
THRESHOLD
ON
THE
RADIUS
SURROUNDING
A
QUERY
POINT
INDYK
AND
MOTWANI
AND
ANOTHER
WHERE
COLLISION
PROBABILITIES
ARE
EQUATED
WITH
A
SIMILARITY
FUNCTION
SCORE
CHARIKAR
WE
USE
THE
LATTER
DEFINITION
HERE
H
RK
N
HR
R
FIGURE
OVERVIEW
OF
LOCALITY
SENSITIVE
HASHING
IF
HASH
FUNCTIONS
GUARANTEE
A
HIGH
PROBABILITY
OF
COLLISION
FOR
FEATURES
THAT
ARE
SIMILAR
UNDER
A
METRIC
OF
INTEREST
ONE
CAN
SEARCH
A
LARGE
DATABASE
IN
SUB
LINEAR
TIME
VIA
LOCALITY
SENSITIVE
HASHING
TECHNIQUES
CHARIKAR
INDYK
AND
MOTWANI
A
LIST
OF
K
HASH
FUNCTIONS
HRK
ARE
APPLIED
TO
MAP
N
DATABASE
IMAGES
TO
A
HASH
TABLE
WHERE
SIMILAR
ITEMS
ARE
LIKELY
TO
SHARE
A
BUCKET
AFTER
HASHING
A
QUERY
Q
ONE
MUST
ONLY
EVALUATE
THE
SIMILARITY
BETWEEN
Q
AND
THE
DATABASE
EXAMPLES
WITH
WHICH
IT
COLLIDES
TO
OBTAIN
THE
APPROXIMATE
NEAR
NEIGHBORS
FROM
KULIS
ET
AL
COPYRIGHT
IEEE
DRAWN
HASH
FUNCTIONS
AND
THE
QUERY
IS
COMPARED
AGAINST
THE
UNION
OF
THE
DATABASE
POINTS
TO
WHICH
IT
HASHES
IN
ALL
TABLES
GIVEN
VALID
LSH
FUNCTIONS
THE
QUERY
TIME
FOR
RETRIEVING
E
NEAR
NEIGHBORS
IS
BOUNDED
BY
O
N
E
FOR
THE
HAMMING
DISTANCE
AND
A
DATABASE
OF
SIZE
N
GIONIS
ET
AL
ONE
CAN
THEREFORE
TRADE
OFF
THE
ACCURACY
OF
THE
SEARCH
WITH
THE
QUERY
TIME
REQUIRED
EARLY
LSH
FUNCTIONS
WERE
DEVELOPED
TO
ACCOMMODATE
THE
HAMMING
DISTANCE
INDYK
AND
MOTWANI
INNER
PROD
UCTS
CHARIKAR
AND
FP
NORMS
DATAR
ET
AL
THESE
METHODS
WERE
QUICKLY
ADOPTED
BY
VISION
RESEARCHERS
FOR
A
VARIETY
OF
IMAGE
SEARCH
APPLICATIONS
SHAKHNAROVICH
ET
AL
SINCE
MEANINGFUL
IMAGE
COMPARISONS
FOR
RECOGNITION
OFTEN
DEMAND
RICHER
COMPARISON
MEA
SURES
WORK
IN
THE
VISION
COMMUNITY
HAS
DEVELOPED
NOVEL
LOCALITY
SENSITIVE
HASH
FUNCTIONS
FOR
ADDI
TIONAL
CLASSES
OF
METRICS
FOR
EXAMPLE
AN
EMBEDDING
OF
THE
NORMALIZED
PARTIAL
MATCHING
BETWEEN
TWO
SETS
OF
LOCAL
FEATURES
IS
GIVEN
IN
GRAUMAN
AND
DARRELL
THAT
ALLOWS
SUB
LINEAR
TIME
HASHING
FOR
THE
PYRAMID
MATCH
KERNEL
SEE
SECTION
BELOW
A
RELATED
FORM
OF
HASHING
COM
PUTES
SKETCHES
OF
FEATURE
SETS
AND
ALLOWS
SEARCH
ACCORDING
TO
THE
SETS
OVERLAP
BRODER
THIS
MIN
HASH
FRAMEWORK
HAS
BEEN
DEMONSTRATED
AND
EXTENDED
FOR
NEAR
DUPLICATE
DETECTION
AND
IM
AGE
SEARCH
IN
CHUM
ET
AL
MOST
RECENTLY
A
KERNELIZED
FORM
OF
LSH
KLSH
IS
PROPOSED
IN
KULIS
AND
GRAUMAN
WHICH
MAKES
IT
POSSIBLE
TO
PERFORM
LOCALITY
SENSITIVE
HASHING
FOR
ARBITRARY
KERNEL
FUNCTIONS
RESULTS
ARE
SHOWN
FOR
VARIOUS
KERNELS
RELEVANT
TO
OBJECT
RECOGNITION
IN
CLUDING
THE
KERNEL
THAT
IS
OFTEN
EMPLOYED
FOR
COMPARING
BAG
OF
WORDS
DESCRIPTORS
TO
BE
DEFINED
BELOW
ASIDE
FROM
WIDENING
THE
CLASS
OF
METRICS
AND
KERNELS
SUPPORTABLE
WITH
LSH
RESEARCHERS
HAVE
ALSO
CONSIDERED
HOW
TO
INTEGRATE
MACHINE
LEARNING
ELEMENTS
SO
THAT
THE
HASH
FUNCTIONS
ARE
BETTER
SUITED
FOR
A
PARTICULAR
TASK
FOR
OBJECT
RECOGNITION
THIS
MEANS
THAT
ONE
WANTS
HASH
FUNCTIONS
THAT
ARE
MORE
LIKELY
TO
MAP
INSTANCES
OF
THE
SAME
OBJECT
TO
THE
SAME
HASH
BUCKETS
OR
SIMILARLY
PATCH
DESCRIPTORS
FROM
THE
SAME
REAL
WORLD
OBJECT
POINT
TO
THE
SAME
BUCKET
FOR
EXAMPLE
PARAMETER
SEN
SITIVE
HASHING
PSH
SHAKHNAROVICH
ET
AL
IS
AN
LSH
BASED
ALGORITHM
THAT
USES
BOOSTING
TO
SELECT
HASH
FUNCTIONS
THAT
BEST
REFLECT
SIMILARITY
IN
A
PARAMETER
SPACE
OF
INTEREST
SEMI
SUPERVISED
HASH
FUNCTIONS
MAKE
IT
POSSIBLE
TO
EFFICIENTLY
INDEX
DATA
ACCORDING
TO
LEARNED
DISTANCES
JAIN
ET
AL
KULIS
ET
AL
STRECHA
ET
AL
WANG
KUMAR
AND
CHANG
TYPICALLY
SUPERVISION
IS
GIVEN
IN
THE
FORM
OF
SIMILAR
AND
DISSIMILAR
PAIRS
OF
INSTANCES
AND
THEN
WHILE
THE
METRIC
LEARNING
ALGORITHM
UPDATES
ITS
PARAMETERS
TO
BEST
CAPTURE
THOSE
CONSTRAINTS
THE
HASH
FUNCTIONS
PARAMETERS
ARE
SIMULTANEOUSLY
ADJUSTED
WHILE
MOST
METHODS
ASSUME
THAT
ALL
SUPERVISION
IS
AVAILABLE
IN
BATCH
AT
THE
ONSET
ONLINE
METRIC
LEARNERS
THAT
ACCUMULATE
CONSTRAINTS
OVER
TIME
TOGETHER
WITH
HASH
TABLES
THAT
CAN
BE
UPDATED
INCREMENTALLY
HAVE
ALSO
BEEN
DEVELOPED
JAIN
ET
AL
BINARY
EMBEDDING
FUNCTIONS
EMBEDDING
FUNCTIONS
ARE
A
RELATED
MECHANISM
THAT
ARE
USED
TO
MAP
EXPENSIVE
DISTANCE
FUNCTIONS
INTO
SOMETHING
MORE
MANAGEABLE
COMPUTATIONALLY
EITHER
CONSTRUCTED
OR
LEARNED
THESE
EMBEDDINGS
AIM
TO
APPROXIMATELY
PRESERVE
THE
DESIRED
DISTANCE
FUNCTION
WHEN
MAPPING
TO
A
LOW
DIMENSIONAL
SPACE
THAT
IS
MORE
EASILY
SEARCHABLE
WITH
KNOWN
TECHNIQUES
INFORMALLY
GIVEN
AN
ORIGINAL
FEATURE
SPACE
X
AND
ASSOCIATED
DISTANCE
FUNCTION
DX
THE
BASIC
IDEA
IS
TO
DESIGNATE
A
FUNCTION
F
X
E
THAT
MAPS
THE
INPUTS
INTO
A
NEW
SPACE
E
WITH
ASSOCIATED
DISTANCE
DE
IN
SUCH
A
WAY
THAT
DE
F
X
F
Y
DX
X
Y
FOR
ANY
X
Y
OFTEN
THE
TARGET
SPACE
FOR
THE
EMBEDDING
IS
THE
HAMMING
SPACE
SUCH
BINARY
CODES
HAVE
THE
ADVANTAGE
OF
REQUIRING
MINIMAL
MEMORY
THEY
ALSO
PERMIT
FAST
BIT
COUNTING
ROUTINES
FOR
THE
HAMMING
DISTANCE
AND
CAN
BE
INDEXED
DIRECTLY
USING
THE
COMPUTER
MEMORY
ADDRESSES
WORK
IN
THE
VISION
AND
LEARNING
COMMUNITY
HAS
DEVELOPED
USEFUL
EMBEDDING
FUNCTIONS
THAT
AIM
TO
PRESERVE
A
VARIETY
OF
SIMILARITY
METRICS
WITH
SIMPLE
LOW
DIMENSIONAL
BINARY
CODES
FOR
EXAMPLE
THE
BOOSTMAP
ATHITSOS
ET
AL
AND
BOOSTED
SIMILARITY
SENSITIVE
CODING
BOOST
SSC
SHAKHNAROVICH
ALGORITHMS
LEARN
AN
EMBEDDING
USING
DIFFERENT
FORMS
OF
BOOSTING
COMBINING
MULTIPLE
WEIGHTED
EMBEDDINGS
SO
AS
TO
PRESERVE
THE
PROXIMITY
STRUCTURE
GIVEN
BY
THE
ORIGINAL
DISTANCE
FUNCTION
BUILDING
ON
THIS
NOTION
MORE
RECENT
WORK
DEVELOPS
SEMANTIC
HASHING
ALGORITHMS
THAT
TRAIN
EMBEDDING
FUNCTIONS
USING
BOOSTING
OR
MULTIPLE
LAYERS
OF
RESTRICTED
BOLTZMANN
MACHINES
SALAKHUTDINOV
AND
HINTON
TORRALBA
ET
AL
RESULTS
SHOW
THE
IM
PACT
FOR
SEARCHING
GIST
IMAGE
DESCRIPTORS
TORRALBA
ET
AL
EMBEDDINGS
BASED
ON
RANDOM
PROJECTIONS
HAVE
ALSO
BEEN
EXPLORED
FOR
SHIFT
INVARIANT
KERNELS
WHICH
INCLUDES
A
GAUSSIAN
KER
NEL
RAGINSKY
AND
LAZEBNIK
SUCH
METHODS
ARE
RELATED
TO
LSH
IN
THE
SENSE
THAT
BOTH
SEEK
SMALL
KEYS
THAT
CAN
BE
USED
TO
ENCODE
SIMILAR
INPUTS
AND
OFTEN
THESE
KEYS
EXIST
IN
HAMMING
SPACE
HOWEVER
NOTE
THAT
WHILE
HASH
FUNCTIONS
ALSO
TYPICALLY
MAP
THE
DATA
TO
BINARY
STRINGS
THE
HASH
KEYS
IN
THAT
CASE
THE
CODES
SERVE
TO
INSERT
INSTANCES
INTO
BUCKETS
WHEREAS
TECHNICALLY
THE
EMBEDDING
FUNCTION
OUTPUTS
ARE
TREATED
AS
A
NEW
FEATURE
SPACE
IN
WHICH
TO
PERFORM
THE
SIMILARITY
SEARCH
A
RULE
OF
THUMB
FOR
REDUCING
AMBIGUOUS
MATCHES
WHEN
MATCHING
LOCAL
FEATURE
SETS
EXTRACTED
FROM
REAL
WORLD
IMAGES
MANY
FEATURES
WILL
STEM
FROM
BACKGROUND
CLUTTER
AND
WILL
THEREFORE
HAVE
NO
MEANINGFUL
NEIGHBOR
IN
THE
OTHER
SET
OTHER
FEATURES
LIE
ON
REPETITIVE
STRUCTURES
AND
MAY
THEREFORE
HAVE
AMBIGUOUS
MATCHES
FOR
EXAMPLE
IMAGINE
AN
IMAGE
CONTAINING
A
BUILDING
WITH
MANY
IDENTICAL
WINDOWS
HENCE
ONE
NEEDS
TO
FIND
A
WAY
TO
DISTINGUISH
RELIABLE
MATCHES
FROM
UNRELIABLE
ONES
THIS
CANNOT
BE
DONE
BASED
ON
THE
DESCRIPTOR
DISTANCE
ALONE
SINCE
SOME
DESCRIPTORS
ARE
MORE
DISCRIMINATIVE
THAN
OTHERS
AN
OFTEN
USED
STRATEGY
INITIALLY
PROPOSED
BY
LOWE
IS
TO
CONSIDER
THE
RATIO
OF
THE
DISTANCE
TO
THE
CLOSEST
NEIGHBOR
TO
THAT
OF
THE
SECOND
CLOSEST
ONE
AS
A
DECISION
CRITERION
SPECIFICALLY
WE
IDENTIFY
THE
NEAREST
NEIGHBOR
LOCAL
FEATURE
ORIGINATING
FROM
AN
EXEMPLAR
IN
THE
DATABASE
OF
TRAINING
IMAGES
AND
THEN
CONSIDER
THE
SECOND
NEAREST
NEIGHBOR
THAT
ORIGINATES
FROM
A
DIFFERENT
OBJECT
THAN
THE
NEAREST
NEIGHBOR
FEATURE
IF
THE
RATIO
OF
THE
DISTANCE
TO
THE
FIRST
NEIGHBOR
OVER
THE
DISTANCE
TO
THE
SECOND
NEIGHBOR
IS
RELATIVELY
LARGE
THIS
IS
A
SIGN
THAT
THE
MATCH
MAY
BE
AMBIGUOUS
SIMILARLY
IF
THE
RATIO
IS
LOW
IT
SUGGESTS
THAT
IT
IS
A
RELIABLE
MATCH
THIS
STRATEGY
EFFECTIVELY
PENALIZES
FEATURES
THAT
COME
FROM
A
DENSELY
POPULATED
REGION
OF
FEATURE
SPACE
AND
THAT
ARE
THEREFORE
MORE
AMBIGUOUS
BY
COMPARING
THE
PROBABILITY
DENSITY
FUNCTIONS
OF
CORRECT
AND
INCORRECT
MATCHES
IN
QUANTITATIVE
EXPERIMENTS
LOWE
ARRIVES
AT
THE
RECOMMENDATION
TO
REJECT
ALL
MATCHES
IN
WHICH
THE
DISTANCE
RATIO
IS
GREATER
THAN
WHICH
IN
HIS
EXPERIMENTS
ELIMINATED
OF
THE
FALSE
MATCHES
WHILE
DISCARDING
LESS
THAN
CORRECT
MATCHES
LOWE
INDEXING
FEATURES
WITH
VISUAL
VOCABULARIES
IN
THIS
SECTION
WE
OVERVIEW
THE
CONCEPT
OF
A
VISUAL
VOCABULARY
A
STRATEGY
THAT
DRAWS
INSPIRATION
FROM
THE
TEXT
RETRIEVAL
COMMUNITY
AND
ENABLES
EFFICIENT
INDEXING
FOR
LOCAL
IMAGE
FEATURES
RATHER
THAN
PREPARING
A
TREE
OR
HASHING
DATA
STRUCTURE
TO
AID
IN
DIRECT
SIMILARITY
SEARCH
THE
IDEA
IS
TO
QUANTIZE
THE
LOCAL
FEATURE
SPACE
BY
MAPPING
THE
LOCAL
DESCRIPTORS
TO
DISCRETE
TOKENS
WE
CAN
THEN
MATCH
THEM
BY
SIMPLY
LOOKING
UP
FEATURES
ASSIGNED
TO
THE
IDENTICAL
TOKEN
IN
THE
FOLLOWING
WE
FIRST
DESCRIBE
THE
FORMATION
OF
VISUAL
WORDS
SECTIONS
THROUGH
AND
THEN
DESCRIBE
THEIR
UTILITY
FOR
INDEXING
SECTION
NOTE
THAT
WE
WILL
RETURN
TO
THIS
REPRE
SENTATION
LATER
IN
SECTION
IN
THE
CONTEXT
OF
OBJECT
CATEGORIZATION
AS
IT
IS
THE
BASIS
FOR
THE
SIMPLE
BUT
EFFECTIVE
BAG
OF
WORDS
IMAGE
DESCRIPTOR
A
B
C
D
FIGURE
A
SCHEMATIC
TO
ILLUSTRATE
VISUAL
VOCABULARY
CONSTRUCTION
AND
WORD
ASSIGNMENT
A
A
LARGE
CORPUS
OF
REPRESENTATIVE
IMAGES
ARE
USED
TO
POPULATE
THE
FEATURE
SPACE
WITH
DESCRIPTOR
INSTANCES
THE
WHITE
ELLIPSES
DENOTE
LOCAL
FEATURE
REGIONS
AND
THE
BLACK
DOTS
DENOTE
POINTS
IN
SOME
FEATURE
SPACE
E
G
SIFT
B
NEXT
THE
SAMPLED
FEATURES
ARE
CLUSTERED
IN
ORDER
TO
QUANTIZE
THE
SPACE
INTO
A
DISCRETE
NUMBER
OF
VISUAL
WORDS
THE
VISUAL
WORDS
ARE
THE
CLUSTER
CENTERS
DENOTED
WITH
THE
LARGE
GREEN
CIRCLES
THE
DOTTED
GREEN
LINES
SIGNIFY
THE
IMPLIED
VORONOI
CELLS
BASED
ON
THE
SELECTED
WORD
CENTERS
C
NOW
GIVEN
A
NEW
IMAGE
THE
NEAREST
VISUAL
WORD
IS
IDENTIFIED
FOR
EACH
OF
ITS
FEATURES
THIS
MAPS
THE
IMAGE
FROM
A
SET
OF
HIGH
DIMENSIONAL
DESCRIPTORS
TO
A
LIST
OF
WORD
NUMBERS
D
A
BAG
OF
VISUAL
WORDS
HISTOGRAM
CAN
BE
USED
TO
SUMMARIZE
THE
ENTIRE
IMAGE
SEE
SECTION
IT
COUNTS
HOW
MANY
TIMES
EACH
OF
THE
VISUAL
WORDS
OCCURS
IN
THE
IMAGE
CREATING
A
VISUAL
VOCABULARY
METHODS
FOR
INDEXING
AND
EFFICIENT
RETRIEVAL
WITH
TEXT
DOCUMENTS
ARE
MATURE
AND
EFFECTIVE
ENOUGH
TO
OPERATE
WITH
MILLIONS
OR
BILLIONS
OF
DOCUMENTS
AT
ONCE
BAEZA
YATES
AND
RIBEIRO
NETO
DOCUMENTS
OF
TEXT
CONTAIN
SOME
DISTRIBUTION
OF
WORDS
AND
THUS
CAN
BE
COMPACTLY
SUMMARIZED
BY
THEIR
WORD
COUNTS
KNOWN
AS
A
BAG
OF
WORDS
SINCE
THE
OCCURRENCE
OF
A
GIVEN
WORD
TENDS
TO
BE
SPARSE
ACROSS
DIFFERENT
DOCUMENTS
AN
INDEX
THAT
MAPS
WORDS
TO
THE
FILES
IN
WHICH
THEY
OCCUR
CAN
MAP
A
KEYWORD
QUERY
DIRECTLY
TO
POTENTIALLY
RELEVANT
CONTENT
FOR
EXAMPLE
IF
WE
QUERY
A
DOCUMENT
DATABASE
WITH
THE
WORD
CAR
WE
SHOULD
IMMEDIATELY
ELIMINATE
THE
MANY
DOCUMENTS
THAT
NEVER
MENTION
THE
WORD
CAR
WHAT
CUES
THEN
CAN
ONE
TAKE
FROM
TEXT
PROCESSING
TO
AID
VISUAL
SEARCH
AN
IMAGE
IS
A
SORT
OF
DOCUMENT
AND
USING
THE
REPRESENTATIONS
INTRODUCED
IN
CHAPTER
IT
CONTAINS
A
SET
OF
LOCAL
FEATURE
DESCRIPTORS
HOWEVER
AT
FIRST
GLANCE
THE
ANALOGY
WOULD
STOP
THERE
TEXT
WORDS
ARE
DISCRETE
TOKENS
WHEREAS
LOCAL
IMAGE
DESCRIPTORS
ARE
HIGH
DIMENSIONAL
REAL
VALUED
FEATURE
POINTS
HOW
COULD
ONE
OBTAIN
DISCRETE
VISUAL
WORDS
TO
DO
SO
WE
MUST
IMPOSE
A
QUANTIZATION
ON
THE
FEATURE
SPACE
OF
LOCAL
IMAGE
DESCRIPTORS
THAT
WAY
ANY
NOVEL
DESCRIPTOR
VECTOR
CAN
BE
CODED
IN
TERMS
OF
THE
DISCRETIZED
REGION
OF
FEATURE
SPACE
TO
WHICH
IT
BELONGS
THE
STANDARD
PIPELINE
TO
FORM
A
SO
CALLED
VISUAL
VOCABULARY
CONSISTS
OF
COLLECTING
A
LARGE
SAMPLE
OF
FEATURES
FROM
A
REPRESENTATIVE
CORPUS
OF
IMAGES
AND
QUANTIZING
THE
FEATURE
SPACE
ACCORDING
TO
THEIR
STATISTICS
OFTEN
SIMPLE
K
MEANS
CLUSTERING
IS
USED
TO
PERFORM
THE
QUANTIZATION
ONE
INITIALIZES
THE
K
CLUSTER
CENTERS
WITH
RANDOMLY
SELECTED
FEATURES
IN
THE
CORPUS
AND
THEN
ITERATES
BETWEEN
UPDATING
EACH
POINT
CLUSTER
MEMBERSHIP
BASED
ON
WHICH
CLUSTER
CENTER
IT
IS
NEAREST
TO
AND
UPDATING
THE
K
MEANS
BASED
ON
THE
MEAN
OF
THE
POINTS
PREVIOUSLY
ASSIGNED
TO
EACH
CLUSTER
IN
THAT
CASE
THE
VISUAL
WORDS
ARE
THE
K
CLUSTER
CENTERS
AND
THE
SIZE
OF
THE
VOCABULARY
K
IS
A
USER
SUPPLIED
PARAMETER
ONCE
THE
VOCABULARY
IS
ESTABLISHED
THE
CORPUS
OF
SAMPLED
FEATURES
CAN
BE
DISCARDED
THEN
A
NOVEL
IMAGE
FEATURES
CAN
BE
TRANSLATED
INTO
WORDS
BY
DETERMINING
WHICH
VISUAL
WORD
THEY
ARE
NEAREST
TO
IN
THE
FEATURE
SPACE
I
E
BASED
ON
THE
EUCLIDEAN
DISTANCE
BETWEEN
THE
CLUSTER
CENTERS
AND
THE
INPUT
DESCRIPTOR
SEE
FIGURE
FOR
A
DIAGRAM
OF
THE
PROCEDURE
DRAWING
INSPIRATION
FROM
TEXT
RETRIEVAL
METHODS
SIVIC
AND
ZISSERMAN
PROPOSED
QUAN
TIZING
LOCAL
IMAGE
DESCRIPTORS
FOR
THE
SAKE
OF
RAPIDLY
INDEXING
VIDEO
FRAMES
WITH
AN
INVERTED
FILE
SIVIC
AND
ZISSERMAN
THEY
SHOWED
THAT
LOCAL
DESCRIPTORS
EXTRACTED
AT
INTEREST
POINTS
COULD
BE
MAPPED
TO
VISUAL
WORDS
BY
COMPUTING
PROTOTYPICAL
DESCRIPTORS
WITH
K
MEANS
CLUSTERING
AND
THAT
HAVING
THESE
TOKENS
ENABLED
FASTER
RETRIEVAL
OF
FRAMES
CONTAINING
THE
SAME
WORDS
FURTHER
MORE
THEY
SHOWED
THE
POTENTIAL
OF
EXPLOITING
A
TERM
FREQUENCY
INVERSE
DOCUMENT
FREQUENCY
WEIGHTING
ON
THE
WORDS
WHICH
DE
EMPHASIZES
THOSE
WORDS
THAT
ARE
COMMON
TO
MANY
IMAGES
AND
THUS
POSSIBLY
LESS
INFORMATIVE
AND
A
STOP
LIST
WHICH
IGNORES
EXTREMELY
FREQUENT
WORDS
THAT
APPEAR
IN
NEARLY
EVERY
IMAGE
ANALOGOUS
TO
A
OR
THE
IN
TEXT
WHAT
WILL
A
VISUAL
WORD
CAPTURE
THE
ANSWER
DEPENDS
ON
SEVERAL
FACTORS
INCLUDING
WHAT
CORPUS
OF
FEATURES
IS
USED
TO
BUILD
THE
VOCABULARY
THE
NUMBER
OF
WORDS
SELECTED
THE
QUANTIZATION
ALGORITHM
USED
AND
THE
INTEREST
POINT
OR
SAMPLING
MECHANISM
CHOSEN
FOR
FEATURE
EXTRACTION
INTUITIVELY
THE
FIGURE
FOUR
EXAMPLES
OF
VISUAL
WORDS
EACH
GROUP
SHOWS
INSTANCES
OF
PATCHES
THAT
ARE
ASSIGNED
TO
THE
SAME
VISUAL
WORD
FROM
SIVIC
AND
ZISSERMAN
COPYRIGHT
IEEE
LARGER
THE
VOCABULARY
THE
MORE
FINE
GRAINED
THE
VISUAL
WORDS
IN
GENERAL
PATCHES
ASSIGNED
TO
THE
SAME
VISUAL
WORD
SHOULD
HAVE
SIMILAR
LOW
LEVEL
APPEARANCE
SEE
FIGURE
PARTICULARLY
WHEN
THE
VOCABULARY
IS
FORMED
IN
AN
UNSUPERVISED
MANNER
THERE
ARE
NO
CONSTRAINTS
THAT
THE
COMMON
TYPES
OF
LOCAL
PATTERNS
BE
CORRELATED
WITH
OBJECT
LEVEL
PARTS
HOWEVER
IN
LATER
CHAPTERS
WE
WILL
SEE
SOME
METHODS
THAT
USE
VISUAL
VOCABULARIES
OR
CODEBOOKS
TO
PROVIDE
CANDIDATE
PARTS
TO
A
PART
BASED
CATEGORY
MODEL
VOCABULARY
TREES
THE
DISCUSSION
ABOVE
ASSUMES
A
FLAT
QUANTIZATION
OF
THE
FEATURE
SPACE
BUT
MANY
CURRENT
TECH
NIQUES
EXPLOIT
HIERARCHICAL
PARTITIONS
BOSCH
ET
AL
GRAUMAN
AND
DARRELL
MOOSMANN
ET
AL
NISTER
AND
STEWENIUS
YEH
ET
AL
IN
PARTICULAR
THE
VOCABU
LARY
TREE
APPROACH
NISTER
AND
STEWENIUS
USES
HIERARCHICAL
K
MEANS
TO
RECURSIVELY
SUBDIVIDE
THE
FEATURE
SPACE
GIVEN
A
CHOICE
OF
THE
BRANCHING
FACTOR
AND
NUMBER
OF
LEVELS
VOCABULARY
TREES
OFFER
A
SIGNIFICANT
ADVANTAGE
IN
TERMS
OF
THE
COMPUTATIONAL
COST
OF
ASSIGNING
NOVEL
IMAGE
FEATURES
TO
WORDS
FROM
LINEAR
TO
LOGARITHMIC
IN
THE
SIZE
OF
THE
VOCABULARY
THIS
IN
TURN
MAKES
IT
PRACTICAL
TO
USE
MUCH
LARGER
VOCABULARIES
E
G
ON
THE
ORDER
OF
ONE
MILLION
WORDS
EXPERIMENTAL
RESULTS
SUGGEST
THAT
THESE
MORE
SPECIFIC
WORDS
SMALLER
QUANTIZED
BINS
ARE
PARTICULARLY
USEFUL
FOR
MATCHING
FEATURES
FOR
SPECIFIC
INSTANCES
OF
OBJECTS
NISTER
AND
STEWENIUS
PHILBIN
ET
AL
SINCE
QUANTIZATION
ENTAILS
A
HARD
PARTITIONING
OF
THE
FEATURE
SPACE
IT
CAN
ALSO
BE
USEFUL
IN
PRACTICE
TO
USE
MULTIPLE
RANDOMIZED
HIERARCHICAL
PARTITIONS
AND
OR
TO
PERFORM
A
SOFT
ASSIGNMENT
IN
WHICH
A
FEATURE
RESULTS
IN
MULTIPLE
WEIGHTED
ENTRIES
IN
NEARBY
BINS
CHOICES
IN
VOCABULARY
FORMATION
AN
IMPORTANT
CONCERN
IN
CREATING
THE
VISUAL
VOCABULARY
IS
THE
CHOICE
OF
DATA
USED
TO
CONSTRUCT
IT
GENERALLY
RESEARCHERS
REPORT
THAT
THE
MOST
ACCURATE
RESULTS
ARE
OBTAINED
WHEN
USING
THE
SAME
DATA
SOURCE
TO
CREATE
THE
VOCABULARY
AS
IS
GOING
TO
BE
USED
FOR
THE
CLASSIFICATION
OR
RETRIEVAL
TASK
THIS
CAN
BE
ESPECIALLY
NOTICEABLE
WHEN
THE
APPLICATION
IS
FOR
SPECIFIC
LEVEL
RECOGNITION
RATHER
THAN
GENERIC
CATEGORIZATION
FOR
EXAMPLE
TO
INDEX
THE
FRAMES
FROM
A
PARTICULAR
MOVIE
THE
VOCABULARY
MADE
FROM
A
SAMPLE
OF
THOSE
FRAMES
WOULD
BE
MOST
ACCURATE
USING
A
SECOND
MOVIE
TO
FORM
THE
VOCABULARY
SHOULD
STILL
PRODUCE
MEANINGFUL
RESULTS
THOUGH
LIKELY
WEAKER
ACCURACY
WHEN
TRAINING
A
RECOGNITION
SYSTEM
FOR
A
PARTICULAR
SET
OF
CATEGORIES
ONE
WOULD
TYPICALLY
SAMPLE
DESCRIPTORS
FROM
TRAINING
EXAMPLES
COVERING
ALL
CATEGORIES
TO
TRY
AND
ENSURE
GOOD
COVERAGE
THAT
SAID
WITH
A
LARGE
ENOUGH
POOL
OF
FEATURES
TAKEN
FROM
DIVERSE
IMAGES
ADMITTEDLY
A
VAGUE
CRITERION
IT
DOES
APPEAR
WORKABLE
TO
TREAT
THE
VOCABULARY
AS
UNIVERSAL
FOR
ANY
FUTURE
WORD
ASSIGNMENTS
FURTHERMORE
RESEARCHERS
HAVE
DEVELOPED
METHODS
TO
INJECT
SUPERVISION
INTO
THE
VOCABU
LARY
MOOSMANN
ET
AL
PERRONNIN
ET
AL
WINN
ET
AL
AND
EVEN
TO
INTEGRATE
THE
CLASSIFIER
CONSTRUCTION
AND
VOCABULARY
FORMATION
PROCESSES
YANG
ET
AL
IN
THIS
WAY
ONE
CAN
ESSENTIALLY
LEARN
AN
APPLICATION
SPECIFIC
VOCABULARY
THE
CHOICE
OF
FEATURE
DETECTOR
OR
INTEREST
OPERATOR
WILL
ALSO
HAVE
NOTABLE
IMPACT
ON
THE
TYPES
OF
WORDS
GENERATED
FACTORS
TO
CONSIDER
ARE
THE
INVARIANCE
PROPERTIES
REQUIRED
THE
TYPE
OF
IMAGES
TO
BE
DESCRIBED
AND
THE
COMPUTATIONAL
COST
ALLOWABLE
USING
AN
INTEREST
OPERATOR
E
G
A
DOG
DETECTOR
YIELDS
A
SPARSE
SET
OF
POINTS
THAT
IS
BOTH
COMPACT
AND
REPEATABLE
DUE
TO
THE
DETECTOR
AUTOMATIC
SCALE
SELECTION
FOR
SPECIFIC
LEVEL
RECOGNITION
E
G
IDENTIFYING
A
PARTICULAR
OBJECT
OR
LANDMARK
BUILDING
THESE
POINTS
CAN
ALSO
PROVIDE
AN
ADEQUATELY
DISTINCT
DESCRIPTION
A
COMMON
RULE
OF
THUMB
IS
TO
USE
MULTIPLE
COMPLEMENTARY
DETECTORS
THAT
IS
TO
COMBINE
THE
OUTPUTS
FROM
A
CORNER
FAVORING
INTEREST
OPERATOR
WITH
THOSE
FROM
A
BLOB
FAVORING
INTEREST
OPERATOR
SEE
SECTION
OF
CHAPTER
FOR
A
DISCUSSION
OF
VISUAL
WORD
REPRESENTATIONS
AND
CHOICES
FOR
CATEGORY
LEVEL
RECOGNITION
INVERTED
FILE
INDEXING
VISUAL
VOCABULARIES
OFFER
A
SIMPLE
BUT
EFFECTIVE
WAY
TO
INDEX
IMAGES
EFFICIENTLY
WITH
AN
INVERTED
FILE
AN
INVERTED
FILE
INDEX
IS
JUST
LIKE
AN
INDEX
IN
A
BOOK
WHERE
THE
KEYWORDS
ARE
MAPPED
TO
THE
PAGE
NUMBERS
WHERE
THOSE
WORDS
ARE
USED
IN
THE
VISUAL
WORD
CASE
WE
HAVE
A
TABLE
THAT
POINTS
FROM
THE
WORD
NUMBER
TO
THE
INDICES
OF
THE
DATABASE
IMAGES
IN
WHICH
THAT
WORD
OCCURS
FOR
EXAMPLE
IN
THE
CARTOON
ILLUSTRATION
IN
FIGURE
THE
DATABASE
IS
PROCESSED
AND
THE
TABLE
IS
POPULATED
WITH
IMAGE
INDICES
IN
PART
A
IN
PART
B
THE
WORDS
FROM
THE
NEW
IMAGE
ARE
USED
TO
INDEX
INTO
THAT
TABLE
THEREBY
DIRECTLY
RETRIEVING
THE
DATABASE
IMAGES
THAT
SHARE
ITS
DISTINCTIVE
WORDS
RETRIEVAL
VIA
THE
INVERTED
FILE
IS
FASTER
THAN
SEARCHING
EVERY
IMAGE
ASSUMING
THAT
NOT
ALL
IMAGES
CONTAIN
EVERY
WORD
IN
PRACTICE
AN
IMAGE
DISTRIBUTION
OF
WORDS
IS
INDEED
SPARSE
SINCE
THE
INDEX
MAINTAINS
NO
INFORMATION
ABOUT
THE
RELATIVE
SPATIAL
LAYOUT
OF
THE
WORDS
PER
IMAGE
TYPICALLY
A
SPATIAL
VERIFICATION
STEP
IS
PERFORMED
ON
THE
IMAGES
RETRIEVED
FOR
A
GIVEN
QUERY
AS
WE
DISCUSS
IN
DETAIL
IN
THE
FOLLOWING
CHAPTER
A
ALL
DATABASE
IMAGES
ARE
LOADED
INTO
THE
INDEX
MAPPING
WORDS
TO
IMAGE
NUMBERS
B
A
NEW
QUERY
IMAGE
IS
MAPPED
TO
INDICES
OF
DATABASE
IMAGES
THAT
SHARE
A
WORD
FIGURE
MAIN
IDEA
OF
AN
INVERTED
FILE
INDEX
FOR
IMAGES
REPRESENTED
BY
VISUAL
WORDS
CONCLUDING
REMARKS
IN
SHORT
THE
ABOVE
METHODS
OFFER
WAYS
TO
REDUCE
THE
COMPUTATIONAL
COST
OF
FINDING
SIMILAR
IMAGE
DESCRIPTORS
WITHIN
A
LARGE
DATABASE
WHILE
CERTAINLY
CRUCIAL
TO
PRACTICAL
APPLICATIONS
OF
SPECIFIC
OBJECT
RECOGNITION
BASED
ON
LOCAL
FEATURES
THE
FOCUS
OF
THIS
SEGMENT
OF
THE
LECTURE
THEY
ARE
ALSO
COMMONLY
USED
FOR
OTHER
SEARCH
PROBLEMS
RANGING
FROM
IMAGE
RETRIEVAL
TO
EXAMPLE
BASED
CATEGORY
RECOGNITION
MAKING
THIS
SECTION
ALSO
RELEVANT
TO
GENERIC
CATEGORY
ALGORITHMS
THAT
WE
WILL
DISCUSS
STARTING
IN
CHAPTER
WHICH
MATCHING
ALGORITHM
SHOULD
BE
USED
WHEN
THE
TREE
OR
HASHING
ALGORITHMS
DIRECTLY
PER
FORM
SIMILARITY
SEARCH
OFFERING
THE
ALGORITHM
DESIGNER
THE
MOST
CONTROL
ON
HOW
CANDIDATE
MATCHES
ARE
GATHERED
IN
CONTRAST
A
VISUAL
VOCABULARY
CORRESPONDS
TO
A
FIXED
QUANTIZATION
OF
A
VECTOR
SPACE
AND
LACKS
SUCH
CONTROL
ON
THE
OTHER
HAND
A
VISUAL
VOCABULARY
APPROACH
HAS
THE
ABILITY
TO
COMPACTLY
SUMMARIZE
ALL
LOCAL
DESCRIPTORS
IN
AN
IMAGE
OR
WINDOW
ALLOWING
A
FAST
CHECK
FOR
OVERALL
AGREEMENT
BETWEEN
TWO
IMAGES
IN
GENERAL
THE
APPROPRIATE
CHOICE
FOR
AN
APPLICATION
WILL
DEPEND
ON
THE
SIMILAR
ITY
METRIC
THAT
IS
REQUIRED
FOR
THE
SEARCH
THE
DIMENSIONALITY
OF
THE
DATA
THE
AVAILABLE
ONLINE
MEMORY
AND
THE
OFFLINE
RESOURCES
FOR
DATA
STRUCTURE
SETUP
OR
OTHER
OVERHEAD
COSTS
AT
THIS
POINT
WE
HAVE
SHOWN
HOW
TO
DETECT
DESCRIBE
AND
MATCH
LOCAL
FEATURES
GOOD
LOCAL
FEATURE
MATCHES
BETWEEN
IMAGES
CAN
ALONE
SUGGEST
A
SPECIFIC
OBJECT
HAS
BEEN
FOUND
HOWEVER
TO
DISCOUNT
SPURIOUS
MATCHES
OR
TO
RECOGNIZE
AN
OBJECT
FROM
VERY
SPARSE
LOCAL
FEATURES
IT
IS
IMPORTANT
TO
ALSO
PERFORM
A
GEOMETRIC
VERIFICATION
STAGE
SEE
FIGURE
THUS
THE
FOLLOWING
CHAPTER
CLOSES
OUR
DISCUSSION
OF
SPECIFIC
OBJECT
RECOGNITION
WITH
TECHNIQUES
TO
VERIFY
SPATIAL
CONSISTENCY
OF
THE
MATCHES
A
MATCHED
FEATURES
ALONE
DO
NOT
ENSURE
A
CONFIDENT
OBJECT
MATCH
B
CANDIDATE
MATCHES
MUST
NEXT
BE
VERIFIED
FOR
GEOMETRIC
CONSISTENCY
FIGURE
THE
CANDIDATE
FEATURE
MATCHES
ESTABLISHED
USING
THE
METHODS
DESCRIBED
IN
THIS
CHAPTER
MAY
STRONGLY
SUGGEST
WHETHER
A
SPECIFIC
OBJECT
IS
PRESENT
BUT
ARE
TYPICALLY
VERIFIED
FOR
GEOMETRIC
CONSISTENCY
IN
THIS
EXAMPLE
THE
GOOD
APPEARANCE
MATCHES
FOUND
IN
THE
TOP
RIGHT
EXAMPLE
CAN
BE
DISCARDED
ONCE
WE
FIND
THEY
DO
NOT
FIT
A
GEOMETRIC
TRANSFORMATION
WELL
WHEREAS
THOSE
FOUND
IN
THE
TOP
LEFT
EXAMPLE
WILL
CHECK
OUT
IN
TERMS
OF
BOTH
APPEARANCE
AND
GEOMETRIC
CONSISTENCY
COURTESY
OF
ONDREJ
CHUM
COORDINATES
EACH
POINT
VECTOR
IS
EXTENDED
BY
AN
ADDITIONAL
COORDINATE
W
E
G
X
X
Y
W
IF
W
FIGURE
THE
DIFFERENT
LEVELS
OF
GEOMETRIC
TRANSFORMATIONS
COURTESY
OF
KRYSTIAN
MIKOLAJCZYK
THEN
THE
POINT
LIES
ON
THE
PLANE
AT
INFINITY
ELSE
THE
POINT
LOCATION
IN
ORDINARY
CARTESIAN
COORDINATES
CAN
BE
OBTAINED
BY
DIVIDING
EACH
COORDINATE
BY
W
I
E
X
W
Y
W
ESTIMATING
SIMILARITY
TRANSFORMATIONS
A
SIMILARITY
TRANSFORMATION
CAN
ALREADY
BE
HYPOTHESIZED
FROM
A
SINGLE
SCALE
AND
ROTATION
INVARIANT
INTEREST
REGION
OBSERVED
IN
BOTH
IMAGES
LET
FA
XA
YA
ΘA
SA
AND
FB
XB
YB
ΘB
SB
BE
THE
TWO
CORRESPONDING
REGIONS
WITH
CENTER
COORDINATES
X
Y
ROTATION
Θ
AND
SCALE
THEN
WE
CAN
OBTAIN
THE
TRANSFORMATION
FROM
A
TO
B
IN
HOMOGENOUS
COORDINATES
AS
DS
COS
DΘ
SIN
DΘ
DX
DX
XB
XA
TSIM
SIN
DΘ
DS
COS
DΘ
DY
WHERE
DY
YB
YA
DΘ
ΘB
ΘA
DS
SB
SA
IF
ONLY
FEATURE
LOCATIONS
ARE
AVAILABLE
WE
REQUIRE
AT
LEAST
TWO
POINT
CORRESPONDENCES
THEN
WE
CAN
COMPUTE
THE
TWO
VECTORS
BETWEEN
POINT
PAIRS
IN
THE
SAME
IMAGE
AND
WE
OBTAIN
TSIM
AS
THE
TRANSFORMATION
THAT
PROJECTS
ONE
SUCH
VECTOR
ONTO
ITS
CORRESPONDING
VECTOR
IN
THE
OTHER
IMAGE
ESTIMATING
AFFINE
TRANSFORMATIONS
SIMILAR
TO
THE
ABOVE
AN
AFFINE
TRANSFORMATION
CAN
ALREADY
BE
OBTAINED
FROM
A
SINGLE
AFFINE
COVARIANT
REGION
CORRESPONDENCE
RECALL
FROM
CHAPTER
THAT
FOR
ESTIMATING
THE
ELLIPTICAL
REGION
SHAPE
WE
HAD
TO
COMPUTE
THE
REGION
SECOND
MOMENT
MATRIX
M
THE
TRANSFORMATION
THAT
PROJECTS
THE
ELLIPTICAL
REGION
ONTO
A
CIRCLE
IS
GIVEN
BY
THE
SQUARE
ROOT
OF
THIS
MATRIX
WE
CAN
THUS
OBTAIN
THE
TRANSFOR
MATION
FROM
REGION
FA
XA
YA
ΘA
MA
ONTO
REGION
FB
XB
YB
ΘB
MB
BY
FIRST
PROJECTING
FA
ONTO
A
CIRCLE
ROTATING
IT
ACCORDING
TO
DΘ
ΘB
ΘA
AND
THEN
PROJECTING
THE
ROTATED
CIRCLE
BACK
ONTO
FB
THIS
LEADS
TO
THE
FOLLOWING
TRANSFORMATION
COS
DΘ
SIN
DΘ
TAFF
MB
RMA
WITH
R
SIN
DΘ
COS
DΘ
ALTERNATIVELY
WE
CAN
ESTIMATE
THE
AFFINE
TRANSFORMATION
FROM
THREE
OR
MORE
NON
COLLINEAR
POINT
CORRESPONDENCES
IF
MORE
THAN
THREE
SUCH
CORRESPONDENCES
ARE
AVAILABLE
WE
CAN
USE
ALL
OF
THEM
IN
ORDER
TO
COUNTERACT
THE
INFLUENCE
OF
NOISE
AND
OBTAIN
A
MORE
ACCURATE
TRANSFORMATION
ESTIMATE
THIS
IS
DONE
AS
FOLLOWS
WE
START
BY
WRITING
DOWN
THE
AFFINE
TRANSFORMATION
WE
WANT
TO
ESTIMATE
IN
NON
HOMOGENEOUS
COORDINATES
THIS
TRANSFORMATION
IS
GIVEN
BY
A
2
MATRIX
M
AND
A
TRANSLATION
VECTOR
T
SUCH
THAT
XB
MXA
T
XB
L
XA
L
L
YB
YA
WE
CAN
NOW
COLLECT
THE
UNKNOWN
PARAMETERS
INTO
ONE
VECTOR
B
T
AND
WRITE
THE
EQUATION
IN
MATRIX
FORM
FOR
A
NUMBER
OF
POINT
CORRESPONDENCES
XAI
AND
XBI
A
B
XB
XAI
YAI
XAI
YAI
XBI
YBI
IF
WE
HAVE
EXACTLY
THREE
POINT
CORRESPONDENCES
A
WILL
BE
SQUARE
AND
WE
CAN
OBTAIN
THE
SOLUTION
FROM
ITS
INVERSE
AS
B
A
IF
MORE
THAN
THREE
CORRESPONDENCES
ARE
AVAILABLE
WE
CAN
SOLVE
THE
EQUATION
BY
BUILDING
THE
PSEUDO
INVERSE
OF
A
B
ATA
A
XB
IT
CAN
BE
SHOWN
THAT
THIS
SOLUTION
MINIMIZES
THE
ESTIMATION
ERROR
IN
THE
LEAST
SQUARES
SENSE
THE
RESULTS
OF
AN
AFFINE
ESTIMATION
PROCEDURE
ON
A
REAL
WORLD
RECOGNITION
EXAMPLE
ARE
SHOWN
IN
FIGURE
2
HOMOGRAPHY
ESTIMATION
A
HOMOGRAPHY
I
E
A
PROJECTION
OF
A
PLANE
ONTO
ANOTHER
PLANE
CAN
BE
ESTIMATED
FROM
AT
LEAST
FOUR
POINT
CORRESPONDENCES
WHEN
USING
MORE
THAN
FOUR
CORRESPONDENCES
THIS
AGAIN
HAS
THE
ADVANTAGE
THAT
WE
CAN
SMOOTH
OUT
NOISE
BY
SEARCHING
FOR
A
LEAST
SQUARES
ESTIMATE
COMPARED
TO
THE
AFFINE
ESTIMATION
ABOVE
THE
ESTIMATION
BECOMES
A
BIT
MORE
COMPLICATED
SINCE
WE
NOW
NEED
TO
WORK
WITH
FIGURE
2
EXAMPLE
RESULTS
OF
AFFINE
TRANSFORMATION
ESTIMATION
FOR
RECOGNITION
TOP
LEFT
MODEL
IMAGES
BOTTOM
LEFT
TEST
IMAGE
RIGHT
ESTIMATED
AFFINE
MODELS
AND
SUPPORTING
FEATURES
FROM
LOWE
COPYRIGHT
IEEE
PROJECTIVE
GEOMETRY
WE
CAN
DO
THAT
BY
USING
HOMOGENEOUS
COORDINATES
THE
HOMOGRAPHY
TRANSFOR
MATION
FROM
A
POINT
XA
TO
ITS
COUNTERPART
XB
CAN
THEN
BE
WRITTEN
AS
FOLLOWS
XB
XBT
WITH
XBT
HXA
XB
XBT
XBT
XA
YB
ZBT
YBT
YBT
YA
THE
SIMPLEST
WAY
TO
ESTIMATE
A
HOMOGRAPHY
FROM
FEATURE
CORRESPONDENCES
IS
THE
DIRECT
LINEAR
TRANS
FORMATION
DLT
METHOD
HARTLEY
AND
ZISSERMAN
USING
SEVERAL
ALGEBRAIC
MANIPULATIONS
THIS
METHOD
SETS
UP
A
SIMILAR
ESTIMATION
PROCEDURE
AS
ABOVE
RESULTING
IN
THE
FOLLOWING
MATRIX
EQUATION
FOR
THE
HOMOGRAPHY
PARAMETERS
H
AH
YB1
THE
SOLUTION
TO
THIS
EQUATION
IS
THE
NULL
SPACE
VECTOR
OF
A
THIS
CAN
BE
OBTAINED
BY
COMPUTING
THE
SINGULAR
VALUE
DECOMPOSITION
SVD
OF
A
WHERE
THE
SOLUTION
IS
GIVEN
BY
THE
SINGULAR
VECTOR
CORRESPONDING
TO
THE
SMALLEST
SINGULAR
VALUE
THE
SVD
OF
A
RESULTS
IN
THE
FOLLOWING
DECOMPOSITION
0
AND
THE
SOLUTION
FOR
H
IS
GIVEN
BY
THE
LAST
COLUMN
OF
VT
AS
ABOVE
THIS
SOLUTION
MINIMIZES
THE
LEAST
SQUARES
ESTIMATION
ERROR
SINCE
THE
HOMOGRAPHY
HAS
ONLY
DEGREES
OF
FREEDOM
WE
ARE
FREE
TO
BRING
THE
RESULT
VECTOR
INTO
A
CANONICAL
FORM
BY
AN
APPROPRIATE
NORMALIZATION
THIS
COULD
BE
DONE
BY
NORMALIZING
THE
RESULT
VECTOR
BY
ITS
LAST
ENTRY
H
ALTHOUGH
THIS
PROCEDURE
IS
OFTEN
USED
IT
IS
PROBLEMATIC
SINCE
MAY
ALSO
BE
ZERO
HARTLEY
AND
ZISSERMAN
THEREFORE
RECOMMEND
TO
NORMALIZE
THE
VECTOR
LENGTH
INSTEAD
WHICH
AVOIDS
THIS
PROBLEM
H
10
IT
SHOULD
BE
NOTED
THAT
THERE
ARE
ALSO
SEVERAL
MORE
ELABORATE
ESTIMATION
PROCEDURES
BASED
ON
NONLINEAR
OPTIMIZATIONS
FOR
THOSE
WE
HOWEVER
REFER
THE
READER
TO
THE
DETAILED
TREATMENT
IN
HARTLEY
AND
ZISSERMAN
MORE
GENERAL
TRANSFORMATIONS
THE
TRANSFORMATIONS
DISCUSSED
IN
THIS
SECTION
CAN
ALSO
BE
INTERPRETED
IN
TERMS
OF
THE
CAMERA
MODELS
THEY
AFFORD
AFFINE
TRANSFORMATIONS
CAN
ONLY
DESCRIBE
THE
EFFECTS
OF
AFFINE
CAMERAS
A
SIMPLIFIED
CAMERA
MODEL
THAT
ONLY
ALLOWS
FOR
ORTHOGRAPHIC
OR
PARALLEL
PROJECTION
THEY
ARE
A
SUITABLE
REPRESENTATION
IF
THE
EFFECTS
OF
PERSPECTIVE
DISTORTION
ARE
SMALL
SUCH
AS
WHEN
THE
OBJECT
OF
INTEREST
IS
FAR
AWAY
FROM
THE
CAMERA
AND
ITS
EXTENT
IN
DEPTH
IS
COMPARATIVELY
SMALL
AFFINE
TRANSFORMATIONS
CAN
ALSO
BE
USED
TO
APPROXIMATE
THE
EFFECTS
OF
PERSPECTIVE
PROJECTION
FOR
SMALL
REGIONS
SUCH
AS
THE
LOCAL
NEIGHBORHOOD
OF
AN
INTEREST
REGION
IN
ORDER
TO
DESCRIBE
THE
EFFECTS
OF
GENERAL
PERSPECTIVE
CAMERAS
A
PROJECTIVE
TRANSFORMATION
IS
NEEDED
SECTION
PRESENTED
AN
APPROACH
FOR
ESTIMATING
HOMOGRAPHIES
WHICH
CAPTURE
THE
PER
SPECTIVE
PROJECTION
OF
A
PLANAR
SURFACE
IN
THE
CASE
OF
A
MORE
GENERAL
SCENE
HOMOGRAPHIES
ARE
NO
LONGER
SUFFICIENT
AND
WE
NEED
TO
CHECK
IF
THE
POINT
CORRESPONDENCES
ARE
CONSISTENT
WITH
AN
EPIPOLAR
GEOMETRY
IF
THE
INTERNAL
CAMERA
CALIBRATION
IS
KNOWN
THE
CORRESPONDING
CONSTRAINTS
CAN
BE
EXPRESSED
BY
THE
SO
CALLED
ESSENTIAL
MATRIX
WHICH
CAPTURES
THE
RIGID
TRANSFORMATION
TRANSLATION
ROTATION
OF
THE
CAMERA
WITH
RESPECT
TO
A
STATIC
SCENE
THE
ESSENTIAL
MATRIX
CAN
BE
ESTIMATED
FROM
CORRESPONDENCE
PAIRS
IF
THE
INTERNAL
CAMERA
CALIBRATION
IS
UNKNOWN
WE
NEED
TO
ESTIMATE
THE
FUNDAMENTAL
MATRIX
WHICH
CAN
BE
ESTIMATED
FROM
CORRESPONDENCE
PAIRS
USING
A
NON
LINEAR
APPROACH
OR
FROM
CORRESPONDENCE
PAIRS
USING
A
LINEAR
APPROACH
ALTHOUGH
THOSE
CONSTRAINTS
ARE
ROUTINELY
USED
IN
RECONSTRUCTION
THEY
ARE
HOWEVER
ONLY
RARELY
USED
FOR
OBJECT
RECOGNITION
SINCE
THEIR
ESTIMATION
IS
GENERALLY
LESS
ROBUST
WE
THEREFORE
DO
NOT
COVER
THEM
HERE
AND
REFER
TO
HARTLEY
AND
ZISSERMAN
FOR
DETAILS
2
DEALING
WITH
OUTLIERS
THE
ASSUMPTION
THAT
ALL
FEATURE
CORRESPONDENCES
ARE
CORRECT
RARELY
HOLDS
IN
PRACTICE
IN
REAL
WORLD
PROBLEMS
WE
OFTEN
HAVE
TO
DEAL
WITH
A
LARGE
FRACTION
OF
OUTLIER
CORRESPONDENCES
I
E
CORRESPONDENCES
THAT
ARE
NOT
EXPLAINED
BY
THE
CHOSEN
TRANSFORMATION
MODEL
THESE
OUTLIERS
CAN
STEM
FROM
TWO
SOURCES
THEY
CAN
EITHER
BE
CAUSED
BY
WRONG
OR
AMBIGUOUS
FEATURE
MATCHES
OR
THEY
CAN
BE
DUE
TO
CORRECT
MATCHES
THAT
ARE
JUST
NOT
EXPLAINED
BY
AN
OVERLY
SIMPLISTIC
TRANSFORMATION
MODEL
E
G
IF
AN
AFFINE
TRANSFORMATION
MODEL
IS
USED
TO
APPROXIMATE
A
PROJECTIVE
TRANSFORMATION
IN
BOTH
CASES
THE
NET
EFFECT
IS
THE
SAME
NAMELY
THAT
THE
TRANSFORMED
LOCATION
OF
A
POINT
FROM
ONE
IMAGE
PROJECTED
INTO
THE
OTHER
IMAGE
DIFFERS
FROM
ITS
CORRESPONDENCE
LOCATION
IN
THAT
IMAGE
BY
MORE
THAN
A
CERTAIN
TOLERANCE
THRESHOLD
THE
PROBLEM
WITH
OUTLIERS
IS
THAT
THEY
CAN
LEAD
TO
ARBITRARILY
WRONG
ESTIMATION
RESULTS
IN
CONNECTION
WITH
LEAST
SQUARES
ESTIMATION
IMAGINE
A
SIMPLE
ESTIMATION
PROBLEM
OF
FINDING
THE
BEST
FITTING
LINE
GIVEN
A
SAMPLE
OF
DATA
POINTS
IF
WE
USE
A
LEAST
SQUARES
ERROR
CRITERION
THEN
MOVING
A
SINGLE
DATA
POINT
SUFFICIENTLY
FAR
AWAY
FROM
THE
CORRECT
LINE
WILL
BIAS
THE
ESTIMATED
SOLUTION
TOWARDS
THIS
POINT
AND
MAY
MOVE
THE
ESTIMATION
RESULT
ARBITRARILY
FAR
FROM
THE
DESIRED
SOLUTION
THE
SAME
THING
WILL
HAPPEN
WITH
ALL
TRANSFORMATION
METHODS
DISCUSSED
IN
SECTION
SINCE
THEY
ARE
ALSO
BASED
ON
LEAST
SQUARES
ESTIMATION
IN
ORDER
TO
OBTAIN
ROBUST
ESTIMATION
RESULTS
IT
IS
THEREFORE
NECESSARY
TO
LIMIT
THE
EFFECT
OF
OUTLIERS
ON
THE
OBTAINED
SOLUTION
FOR
THIS
WE
CAN
USE
THE
PROPERTY
THAT
THE
CORRECT
SOLUTION
WILL
RESULT
IN
CONSISTENT
TRANSFORMATIONS
FOR
ALL
INLIER
DATA
POINTS
WHILE
ANY
INCORRECT
SOLUTION
WILL
GENERALLY
ONLY
BE
SUPPORTED
BY
A
SMALLER
RANDOM
SUBSET
OF
DATA
POINTS
THE
RECOGNITION
TASK
THUS
BOILS
DOWN
TO
FINDING
A
CONSISTENT
TRANSFORMATION
TOGETHER
WITH
A
MAXIMAL
SET
OF
INLIERS
SUPPORTING
THIS
TRANSFORMATION
IN
THE
FOLLOWING
WE
WILL
PRESENT
TWO
POPULAR
APPROACHES
FOR
THIS
TASK
RANSAC
AND
THE
GENERALIZED
HOUGH
TRANSFORM
BOTH
APPROACHES
HAVE
BEEN
SUCCESSFULLY
USED
FOR
REAL
WORLD
ESTIMATION
PROBLEMS
IN
THE
PAST
WE
WILL
THEN
BRIEFLY
COMPARE
THE
TWO
ESTIMATION
SCHEMES
AND
DISCUSS
THEIR
RELATIVE
ADVANTAGES
AND
DISADVANTAGES
2
RANSAC
RANSAC
OR
RANDOM
SAMPLE
CONSENSUS
FISCHLER
AND
BOLLES
HAS
BECOME
A
POPULAR
TOOL
FOR
SOLVING
GEOMETRIC
ESTIMATION
PROBLEMS
IN
DATASETS
CONTAINING
OUTLIERS
RANSAC
IS
A
NON
DETERMINISTIC
ALGORITHM
THAT
OPERATES
IN
A
HYPOTHESIZE
AND
TEST
FRAMEWORK
THUS
IT
ONLY
RETURNS
A
GOOD
RESULT
WITH
A
CERTAIN
PROBABILITY
BUT
THIS
PROBABILITY
INCREASES
WITH
THE
NUMBER
OF
ITERATIONS
FIGURE
VISUALIZATION
OF
THE
RANSAC
PROCEDURE
FOR
A
SIMPLE
PROBLEM
OF
FITTING
LINES
TO
A
DATASET
OF
POINTS
IN
IN
EACH
ITERATION
A
MINIMAL
SET
OF
TWO
POINTS
IS
SAMPLED
TO
DEFINE
A
LINE
AND
THE
NUMBER
OF
INLIER
POINTS
WITHIN
A
CERTAIN
DISTANCE
TO
THIS
LINE
IS
TAKEN
AS
ITS
SCORE
IN
THE
SHOWN
EXAMPLE
THE
HYPOTHESIS
ON
THE
LEFT
HAS
7
INLIERS
WHILE
THE
ONE
ON
THE
RIGHT
HAS
MAKING
IT
A
BETTER
EXPLANATION
FOR
THE
OBSERVED
DATA
COURTESY
OF
JINXIANG
CHAI
GIVEN
A
SET
OF
TENTATIVE
CORRESPONDENCES
RANSAC
RANDOMLY
SAMPLES
A
MINIMAL
SUBSET
OF
M
CORRESPONDENCES
FROM
THIS
SET
IN
ORDER
TO
HYPOTHESIZE
A
GEOMETRIC
MODEL
E
G
USING
ANY
OF
THE
TECHNIQUES
DESCRIBED
IN
SECTION
THIS
MODEL
IS
THEN
VERIFIED
AGAINST
THE
REMAINING
CORRESPONDENCES
AND
THE
NUMBER
OF
INLIERS
IS
DETERMINED
AS
ITS
SCORE
THIS
PROCESS
IS
ITERATED
UNTIL
A
TERMINATION
CRITERION
IS
MET
THUS
THE
RANSAC
PROCEDURE
CAN
BE
SUMMARIZED
AS
FOLLOWS
SAMPLE
A
MINIMAL
SUBSET
OF
M
CORRESPONDENCES
2
ESTIMATE
A
GEOMETRIC
MODEL
T
FROM
THESE
M
CORRESPONDENCES
VERIFY
THE
MODEL
T
AGAINST
ALL
REMAINING
CORRESPONDENCES
AND
CALCULATE
THE
NUMBER
OF
INLIERS
I
IF
I
I
STORE
THE
NEW
MODEL
T
T
TOGETHER
WITH
ITS
NUMBER
OF
INLIERS
I
I
REPEAT
UNTIL
THE
TERMINATION
CRITERION
IS
MET
SEE
BELOW
THE
RANSAC
PROCEDURE
IS
VISUALIZED
IN
FIGURE
FOR
AN
EXAMPLE
OF
FITTING
LINES
TO
A
SET
OF
POINTS
IN
THE
PLANE
FOR
THIS
KIND
OF
PROBLEM
THE
SIZE
OF
THE
MINIMAL
SAMPLE
SET
IS
M
2
I
E
TWO
POINTS
ARE
SUFFICIENT
TO
DEFINE
A
LINE
IN
IN
EACH
ITERATION
WE
THUS
SAMPLE
TWO
POINTS
TO
DEFINE
A
LINE
AND
WE
DETERMINE
THE
NUMBER
OF
INLIERS
TO
THIS
MODEL
BY
SEARCHING
FOR
ALL
POINTS
WITHIN
A
CERTAIN
DISTANCE
TO
THE
LINE
IN
THE
EXAMPLE
IN
FIGURE
THE
HYPOTHESIS
ON
THE
LEFT
HAS
7
INLIERS
WHILE
THE
ONE
ON
THE
RIGHT
HAS
INLIERS
THUS
THE
SECOND
HYPOTHESIS
IS
A
BETTER
EXPLANATION
FOR
THE
OBSERVED
DATA
AND
WILL
REPLACE
THE
FIRST
ONE
IF
CHOSEN
IN
THE
RANDOM
SAMPLING
PROCEDURE
IN
THE
ABOVE
EXAMPLE
RANSAC
IS
APPLIED
TO
THE
TASK
OF
FINDING
LINES
IN
NOTE
HOWEVER
THAT
RANSAC
IS
NOT
LIMITED
TO
THIS
TASK
BUT
IT
CAN
BE
APPLIED
TO
ARBITRARY
TRANSFORMATION
MODELS
INCLUDING
THOSE
DERIVED
IN
SECTION
IN
SUCH
A
CASE
WE
DEFINE
INLIERS
TO
BE
THOSE
POINTS
WHOSE
ALGORITHM
RANSAC
K
0
Ε
M
N
I
0
WHILE
Η
ΕM
K
DO
SAMPLE
M
RANDOM
CORRESPONDENCES
COMPUTE
A
MODEL
T
FROM
THESE
SAMPLES
COMPUTE
THE
NUMBER
I
OF
INLIERS
FOR
T
IF
I
I
THEN
I
I
Ε
I
N
STORE
T
END
IF
K
K
END
WHILE
TRANSFORMATION
ERROR
I
E
THE
DISTANCE
OF
THE
TRANSFORMED
POINT
TO
ITS
CORRESPONDING
POINT
IN
THE
OTHER
IMAGE
IS
BELOW
A
CERTAIN
THRESHOLD
IT
CAN
BE
SEEN
THAT
THE
MORE
INLIERS
A
CERTAIN
MODEL
HAS
THE
MORE
LIKELY
IT
IS
ALSO
TO
BE
SAMPLED
SINCE
ANY
SUBSET
OF
M
OF
ITS
INLIERS
WILL
GIVE
RISE
TO
A
VERY
SIMILAR
MODEL
HYPOTHESIS
MORE
GENERALLY
AN
IMPORTANT
ROLE
IN
THIS
ESTIMATION
IS
PLAYED
BY
THE
TRUE
INLIER
RATIO
Ε
I
N
OF
THE
DATASET
I
E
BY
THE
RATIO
OF
THE
INLIERS
OF
THE
CORRECT
SOLUTION
TO
ALL
AVAILABLE
CORRESPONDENCES
IF
THIS
RATIO
IS
KNOWN
THEN
IT
BECOMES
POSSIBLE
TO
ESTIMATE
THE
NUMBER
OF
SAMPLES
THAT
MUST
BE
DRAWN
UNTIL
AN
UNCONTAMINATED
SAMPLE
IS
FOUND
WITH
PROBABILITY
WE
CAN
THUS
DERIVE
A
RUN
TIME
BOUND
AS
FOLLOWS
LET
Ε
BE
THE
FRACTION
OF
INLIERS
AS
DEFINED
ABOVE
AND
LET
M
BE
THE
SIZE
OF
THE
SAMPLING
SET
THEN
THE
PROBABILITY
THAT
A
SINGLE
SAMPLE
OF
M
POINTS
IS
CORRECT
IS
ΕM
AND
THE
PROBABILITY
THAT
NO
CORRECT
SAMPLE
IS
FOUND
IN
K
RANSAC
ITERATIONS
IS
GIVEN
BY
Η
ΕM
K
WE
THEREFORE
NEED
TO
CHOOSE
K
HIGH
ENOUGH
SUCH
THAT
Η
IS
KEPT
BELOW
THE
DESIRED
FAILURE
RATE
AS
THE
TRUE
INLIER
RATIO
IS
TYPICALLY
UNKNOWN
A
COMMON
STRATEGY
IS
TO
USE
THE
INLIER
RATIO
OF
THE
BEST
SOLUTION
FOUND
THUS
FAR
IN
ORDER
TO
FORMULATE
THE
TERMINATION
CRITERION
THE
RESULTING
PROCEDURE
IS
SUMMARIZED
IN
ALGORITHM
RANSAC
HAS
PROVEN
ITS
WORTH
IN
A
LARGE
NUMBER
OF
PRACTICAL
APPLICATIONS
IN
MANY
CASES
YIELDING
GOOD
SOLUTIONS
ALREADY
IN
A
MODERATE
NUMBER
OF
ITERATIONS
AS
A
RESULT
OF
THE
RATHER
COARSE
QUALITY
CRITERION
THE
NUMBER
OF
INLIERS
IN
A
CERTAIN
TOLERANCE
BAND
THE
INITIAL
SOLUTION
RETURNED
BY
RANSAC
WILL
HOWEVER
ONLY
PROVIDE
A
ROUGH
ALIGNMENT
OF
THE
MODEL
A
COMMON
STRATEGY
IS
THEREFORE
TO
REFINE
THIS
SOLUTION
FURTHER
E
G
THROUGH
A
STANDARD
LEAST
SQUARES
MINIMIZATION
THAT
OPERATES
ONLY
ON
THE
INLIER
SET
HOWEVER
AS
SUCH
A
STEP
MAY
CHANGE
THE
STATUS
OF
SOME
INLIER
OR
OUTLIER
POINTS
AN
ITERATIVE
PROCEDURE
WITH
ALTERNATING
FITTING
AND
INLIER
OUTLIER
CLASSIFICATION
STEPS
IS
ADVISABLE
SINCE
RANSAC
INTRODUCTION
BY
FISCHLER
BOLLES
IN
VAR
IOUS
IMPROVEMENTS
AND
EXTENSIONS
HAVE
BEEN
PROPOSED
IN
THE
LITERATURE
Y
MODEL
IMAGE
TEST
IMAGE
GHT
VOTING
SPACE
FIGURE
VISUALIZATION
OF
THE
GENERALIZED
HOUGH
TRANSFORM
GHT
FOR
OBJECT
RECOGNITION
EACH
LOCAL
FEATURE
MATCHED
BETWEEN
MODEL
AND
TEST
IMAGE
SHOWN
IN
YELLOW
DEFINES
A
TRANSFORMATION
OF
THE
ENTIRE
OBJECT
REFERENCE
FRAME
SHOWN
IN
BLUE
THE
GHT
LETS
EACH
SUCH
FEATURE
PAIR
VOTE
FOR
THE
PARAMETERS
OF
THE
CORRESPONDING
TRANSFORMATION
AND
ACCUMULATES
THOSE
VOTES
IN
A
BINNED
VOTING
SPACE
IN
THIS
EXAMPLE
A
DIMENSIONAL
VOTING
SPACE
IS
SHOWN
FOR
TRANSLATION
X
Y
AND
ROTATION
Θ
IN
PRACTICE
SCALE
COULD
BE
ADDED
AS
A
DIMENSION
OF
THE
VOTING
SPACE
COURTESY
OF
SVETLANA
LAZEBNIK
DAVID
LOWE
AND
LOWE
LEFT
AND
FROM
LEIBE
SCHINDLER
AND
VAN
GOOL
RIGHT
COPYRIGHT
SPRINGER
VERLAG
SEE
PROC
IEEE
INT
L
WORKSHOP
YEARS
OF
RANSAC
IN
CONJUNCTION
WITH
CVPR
FOR
SOME
EXAMPLES
AMONG
THOSE
ARE
EXTENSIONS
TO
SPEED
UP
THE
DIFFERENT
RANSAC
STAGES
CAPEL
CHUM
AND
MATAS
MATAS
AND
CHUM
SATTLER
ET
AL
TO
DELIVER
RUN
TIME
GUARANTEES
FOR
REAL
TIME
PERFORMANCE
NISTÉR
RAGURAM
ET
AL
AND
TO
IMPROVE
THE
QUALITY
OF
THE
ESTIMATED
SOLUTION
CHUM
ET
AL
FRAHM
AND
POLLEFEYS
TORR
AND
ZISSERMAN
WE
REFER
TO
THE
RICH
LITERATURE
FOR
DETAILS
2
2
GENERALIZED
HOUGH
TRANSFORM
ANOTHER
ROBUST
FITTING
TECHNIQUE
IS
THE
HOUGH
TRANSFORM
THE
HOUGH
TRANSFORM
NAMED
AFTER
ITS
INVENTOR
P
V
C
HOUGH
WAS
ORIGINALLY
INTRODUCED
IN
AS
AN
EFFICIENT
METHOD
FOR
FINDING
STRAIGHT
LINES
IN
IMAGES
HOUGH
ITS
BASIC
IDEA
IS
TO
TAKE
THE
PARAMETRIC
FORM
OF
A
MODEL
E
G
THE
EQUATION
FOR
A
LINE
IN
AND
SWAP
THE
ROLE
OF
THE
VARIABLES
AND
PARAMETERS
IN
ORDER
TO
OBTAIN
AN
EQUIVALENT
REPRESENTATION
IN
THE
PARAMETER
SPACE
SUCH
THAT
DATA
POINTS
LYING
ON
THE
SAME
PARAMETRIC
MODEL
ARE
PROJECTED
ONTO
THE
SAME
POINT
IN
PARAMETER
SPACE
BALLARD
LATER
ON
SHOWED
HOW
THIS
IDEA
COULD
BE
GENERALIZED
TO
DETECT
ARBITRARY
SHAPES
LEADING
TO
THE
GENERALIZED
HOUGH
TRANSFORM
GHT
THE
BASIC
IDEA
OF
THIS
EXTENSION
IS
THAT
WE
CAN
LET
OBSERVED
SINGLE
FEATURE
CORRESPONDENCES
VOTE
FOR
THE
PARAMETERS
OF
THE
TRANSFORMATION
THAT
WOULD
PROJECT
THE
OBJECT
IN
THE
MODEL
IMAGE
TO
THE
CORRECT
VIEW
IN
THE
TEST
IMAGE
FOR
THIS
WE
USE
THE
SINGLE
FEATURE
ESTIMATION
APPROACHES
DESCRIBED
IN
SECTIONS
AND
2
FOR
SCALE
INVARIANT
AND
AFFINE
INVARIANT
TRANSFORMATION
MODELS
SIMILAR
TO
THE
ABOVE
WE
SUBDIVIDE
THE
PARAMETER
SPACE
INTO
A
DISCRETE
GRID
OF
ACCUMULATOR
CELLS
AND
ENTER
THE
VOTE
FROM
EACH
FEATURE
CORRESPONDENCE
BY
INCREMENTING
THE
CORRESPONDING
ACCUMULATOR
CELL
VALUE
LOCAL
MAXIMA
IN
THE
HOUGH
VOTING
SPACE
THEN
CORRESPOND
TO
CONSISTENT
FEATURE
CONFIGURATIONS
AND
THUS
TO
OBJECT
DETECTION
HYPOTHESES
FIGURE
VISUALIZES
THE
CORRESPONDING
GHT
PROCEDURE
FOR
AN
EXAMPLE
OF
A
ROTATION
INVARIANT
RECOGNITION
PROBLEM
AS
POINTED
OUT
BY
LOWE
IT
IS
IMPORTANT
TO
AVOID
ALL
QUANTIZATION
ARTIFACTS
WHEN
PER
FORMING
THE
GHT
THIS
CAN
BE
DONE
E
G
BY
INTERPOLATING
THE
VOTE
CONTRIBUTION
INTO
ALL
ADJACENT
CELLS
ALTERNATIVELY
OR
IN
ADDITION
DEPENDING
ON
THE
LEVEL
OF
NOISE
AND
THE
GRANULARITY
OF
THE
PARAMETER
SPACE
DISCRETIZATION
WE
CAN
APPLY
GAUSSIAN
SMOOTHING
ON
THE
FILLED
VOTING
SPACE
THIS
BECOMES
ALL
THE
MORE
IMPORTANT
THE
HIGHER
THE
DIMENSIONALITY
OF
THE
VOTING
SPACE
GETS
AS
THE
INFLUENCE
OF
NOISE
WILL
THEN
SPREAD
THE
VOTES
OVER
A
LARGER
NUMBER
OF
CELLS
2
3
DISCUSSION
COMPARING
RANSAC
WITH
THE
GHT
THERE
IS
CLEARLY
A
DUALITY
BETWEEN
BOTH
APPROACHES
BOTH
TRY
TO
FIND
A
CONSISTENT
MODEL
CONFIGURATION
UNDER
A
SIGNIFICANT
FRACTION
OF
OUTLIER
CORRESPONDENCES
THE
GHT
ACHIEVES
THIS
BY
STARTING
FROM
A
SINGLE
FEATURE
CORRESPONDENCE
AND
CASTING
VOTES
FOR
ALL
MODEL
PARAMETERS
WITH
WHICH
THIS
CORRESPONDENCE
IS
CONSISTENT
IN
CONTRAST
RANSAC
STARTS
FROM
A
MINIMAL
SUBSET
OF
CORRESPONDENCES
TO
ESTIMATE
A
MODEL
AND
THEN
COUNTS
THE
NUMBER
OF
CORRESPONDENCES
THAT
ARE
CONSISTENT
WITH
THIS
MODEL
THUS
THE
GHT
REPRESENTS
THE
UNCERTAINTY
OF
THE
ESTIMATION
IN
THE
MODEL
PARAMETER
SPACE
THROUGH
THE
VOTING
SPACE
BIN
SIZE
AND
OPTIONAL
GAUSSIAN
SMOOTHING
WHILE
RANSAC
REPRESENTS
THE
UNCERTAINTY
IN
THE
IMAGE
SPACE
BY
SETTING
A
BOUND
ON
THE
PROJECTION
ERROR
THE
COMPLEXITY
OF
THE
GHT
IS
LINEAR
IN
THE
NUMBER
OF
FEATURE
CORRESPONDENCES
ASSUMING
A
SINGLE
VOTE
IS
CAST
FOR
EACH
FEATURE
AND
IN
THE
NUMBER
OF
VOTING
SPACE
CELLS
THIS
MEANS
THAT
THE
GHT
CAN
BE
EFFICIENTLY
EXECUTED
IF
THE
SIZE
OF
THE
VOTING
SPACE
IS
SMALL
BUT
THAT
IT
CAN
QUICKLY
BECOME
PROHIBITIVE
FOR
HIGHER
DIMENSIONAL
DATA
IN
PRACTICE
A
VOTING
SPACE
IS
OFTEN
CONSIDERED
THE
UPPER
LIMIT
FOR
EFFICIENT
EXECUTION
AS
A
POSITIVE
POINT
HOWEVER
THE
GHT
CAN
HANDLE
A
LARGER
PERCENTAGE
OF
OUTLIERS
WITH
HIGHER
DIMENSIONALITY
IN
SOME
CASES
SINCE
INCONSISTENT
VOTES
ARE
THEN
SPREAD
OUT
OVER
A
HIGHER
DIMENSIONAL
VOLUME
AND
ARE
THUS
LESS
LIKELY
TO
CREATE
SPURIOUS
PEAKS
IN
ADDITION
THE
ALGORITHM
RUNTIME
IS
INDEPENDENT
OF
THE
INLIER
RATIO
IN
CONTRAST
RANSAC
REQUIRES
A
SEARCH
THROUGH
ALL
DATA
POINTS
IN
EACH
ITERATION
IN
OR
DER
TO
FIND
THE
INLIERS
TO
THE
CURRENT
MODEL
HYPOTHESIS
THUS
IT
BECOMES
MORE
EXPENSIVE
FOR
LARGER
DATASETS
AND
FOR
LOWER
INLIER
RATIOS
ON
THE
OTHER
HAND
ADVANTAGES
OF
RANSAC
ARE
THAT
IT
IS
A
GENERAL
METHOD
SUITED
TO
A
LARGE
RANGE
OF
ESTIMATION
PROBLEMS
THAT
IT
IS
EASY
TO
IMPLE
MENT
AND
THAT
IT
SCALES
BETTER
TO
HIGHER
DIMENSIONAL
MODELS
THAN
THE
GHT
IN
ADDITION
NU
MEROUS
EXTENSIONS
HAVE
BEEN
PROPOSED
TO
ALLEVIATE
RANSAC
SHORTCOMINGS
FOR
A
RANGE
OF
PROB
LEMS
PROC
IEEE
INT
L
WORKSHOP
YEARS
OF
RANSAC
IN
CONJUNCTION
WITH
CVPR
WE
HAVE
NOW
SEEN
THE
THREE
KEY
STEPS
THAT
STATE
OF
THE
ART
METHODS
USE
TO
PERFORM
SPECIFIC
OBJECT
RECOGNITION
LOCAL
FEATURE
DESCRIPTION
MATCHING
AND
GEOMETRIC
VERIFICATION
IN
THE
NEXT
CHAPTER
WE
WILL
GIVE
EXAMPLES
OF
SPECIFIC
SYSTEMS
USING
THIS
GENERAL
APPROACH
CHAP
TER
EXAMPLE
SYSTEMS
SPECIFIC
OBJECT
RECOGNITION
IN
THE
FOLLOWING
WE
WILL
PRESENT
SOME
APPLICATIONS
WHERE
THE
SPECIFIC
OBJECT
RECOGNITION
TECHNIQUES
PRESENTED
ABOVE
ARE
USED
IN
PRACTICE
THE
PURPOSE
OF
THIS
OVERVIEW
IS
TO
GIVE
THE
READER
A
FEELING
FOR
THE
RANGE
OF
POSSIBILITIES
BUT
IT
SHOULD
BY
NO
MEANS
BE
THOUGHT
OF
AS
AN
EXCLUSIVE
LIST
IMAGE
MATCHING
A
CENTRAL
MOTIVATION
FOR
THE
DEVELOPMENT
OF
AFFINE
INVARIANT
LOCAL
FEATURES
WAS
THEIR
USE
FOR
WIDE
BASELINE
STEREO
MATCHING
ALTHOUGH
THIS
IS
NOT
DIRECTLY
A
RECOGNITION
PROBLEM
IT
BEARS
MANY
PARALLELS
THINKING
OF
ONE
CAMERA
IMAGE
AS
THE
MODEL
VIEW
WE
ARE
INTERESTED
IN
FINDING
A
CONSISTENT
SET
OF
CORRESPONDENCES
IN
THE
OTHER
VIEW
UNDER
AN
EPIPOLAR
GEOMETRY
TRANSFORMATION
MODEL
NOT
COVERED
IN
SECTION
1
FIGURE
1
SHOWS
AN
EXAMPLE
FOR
SUCH
AN
APPLICATION
IN
WHICH
FEATURE
CORRESPONDENCES
ARE
FIRST
ESTABLISHED
USING
AFFINE
COVARIANT
REGIONS
AND
RANSAC
IS
THEN
USED
TO
FIND
CONSISTENT
MATCHES
TUYTELAARS
AND
VAN
GOOL
FIGURE
2
SHOWS
ANOTHER
APPLICATION
WHERE
LOCAL
FEATURE
BASED
MATCHING
IS
USED
FOR
CRE
ATING
PANORAMAS
THIS
APPROACH
IS
AGAIN
BASED
ON
SIFT
FEATURES
WHICH
ARE
USED
TO
BOTH
FIND
OVERLAPPING
IMAGE
PAIRS
AND
ESTIMATE
A
HOMOGRAPHY
BETWEEN
THEM
IN
ORDER
TO
STITCH
THE
IMAGES
TOGETHER
BROWN
AND
LOWE
2
OBJECT
RECOGNITION
THE
INTRODUCTION
OF
LOCAL
SCALE
AND
ROTATION
INVARIANT
FEATURES
SUCH
AS
SIFT
LOWE
HAS
MADE
IT
POSSIBLE
TO
DEVELOP
ROBUST
AND
EFFICIENT
APPROACHES
FOR
SPECIFIC
OBJECT
RECOGNITION
A
POPULAR
EXAMPLE
IS
THE
APPROACH
PROPOSED
BY
LOWE
BASED
ON
THE
GENERALIZED
HOUGH
TRANSFORM
DESCRIBED
IN
SECTION
2
2
THIS
APPROACH
HAS
BEEN
WIDELY
USED
IN
MOBILE
ROBOTIC
APPLICATIONS
AND
NOW
FORMS
PART
OF
THE
STANDARD
REPERTOIRE
OF
VISION
LIBRARIES
FOR
ROBOTICS
FIGURE
3
SHOWS
RECOGNITION
RESULTS
OBTAINED
WITH
THE
GHT
IN
LOWE
THE
APPROACH
DESCRIBED
IN
THAT
PAPER
FIRST
EXTRACTS
SCALE
AND
ROTATION
INVARIANT
SIFT
FEATURES
IN
EACH
IMAGE
AND
THEN
USES
MATCHING
FEATURE
PAIRS
IN
ORDER
TO
CAST
VOTES
IN
A
COARSELY
BINNED
DIMENSIONAL
X
Y
Θ
VOTING
SPACE
THE
RESULTING
SIMILARITY
TRANSFORMATION
IS
IN
GENERAL
NOT
SUFFICIENT
TO
REPRESENT
A
OBJECT
POSE
IN
SPACE
HOWEVER
THE
HOUGH
VOTING
STEP
PROVIDES
AN
EFFICIENT
WAY
OF
CLUSTERING
CONSISTENT
FEATURES
BY
THEIR
CONTRIBUTION
TO
THE
SAME
VOTING
BIN
THE
RESULTING
POSE
HYPOTHESES
FIGURE
1
EXAMPLE
APPLICATION
WIDE
BASELINE
STEREO
MATCHING
FROM
TUYTELAARS
AND
VAN
GOOL
COPYRIGHT
SPRINGER
VERLAG
FIGURE
2
EXAMPLE
APPLICATION
IMAGE
STITCHING
COURTESY
OF
MATTHEW
BROWN
AND
FROM
BROWN
AND
LOWE
COPYRIGHT
SPRINGER
VERLAG
2
OBJECT
RECOGNITION
BACKGROUND
SUBTRACTION
FOR
MODEL
BOUNDARIES
OBJECTS
RECOGNIZED
RECOGNITION
IN
SPITE
OF
OCCLUSION
FIGURE
3
OBJECT
RECOGNITION
RESULTS
WITH
THE
APPROACH
BY
LOWE
BASED
ON
THE
GENERALIZED
HOUGH
TRANSFORM
BASED
ON
LOWE
FIGURE
4
EXAMPLE
APPLICATION
LARGE
SCALE
IMAGE
RETRIEVAL
THE
FIRST
COLUMN
SHOWS
A
USER
SPECIFIED
QUERY
REGION
THE
OTHER
COLUMNS
CONTAIN
AUTOMATICALLY
RETRIEVED
MATCHES
FROM
A
DATABASE
OF
ABOUT
IMAGES
FROM
PHILBIN
ET
AL
COPYRIGHT
IEEE
FIGURE
EXAMPLE
APPLICATION
IMAGE
AUTO
ANNOTATION
THE
GREEN
BOUNDING
BOXES
SHOW
AUTOMATICALLY
CREATED
ANNOTATIONS
OF
INTERESTING
BUILDINGS
IN
NOVEL
TEST
IMAGES
EACH
SUCH
BOUNDING
BOX
IS
AUTOMATICALLY
LINKED
TO
THE
CORRESPONDING
ARTICLE
IN
WIKIPEDIA
FROM
GAMMETER
ET
AL
COPYRIGHT
IEEE
ARE
THEN
REFINED
BY
FITTING
AN
AFFINE
TRANSFORMATION
TO
THE
FEATURE
CLUSTERS
IN
THE
DOMINANT
VOTING
BINS
AND
COUNTING
THE
NUMBER
OF
INLIER
POINTS
AS
HYPOTHESIS
SCORE
AS
A
RESULT
THE
APPROACH
CAN
CORRECTLY
RECOGNIZE
COMPLEX
OBJECTS
AND
ESTIMATE
THEIR
ROUGH
POSE
DESPITE
VIEWPOINT
CHANGES
AND
CONSIDERABLE
PARTIAL
OCCLUSION
3
LARGE
SCALE
IMAGE
RETRIEVAL
THE
TECHNIQUES
FROM
CHAPTER
3
THROUGH
MAKE
IT
POSSIBLE
TO
SCALE
THE
RECOGNITION
PROCEDURE
TO
VERY
LARGE
DATA
SETS
A
LARGE
SCALE
RECOGNITION
APPLICATION
MAKING
USE
OF
THIS
CAPABILITY
WAS
PRESENTED
BY
PHILBIN
ET
AL
SEE
FIGURE
4
HERE
A
NUMBER
OF
DIFFERENT
AFFINE
COVARIANT
REGION
DETECTORS
ARE
POOLED
IN
ORDER
TO
CREATE
A
FEATURE
REPRESENTATION
FOR
EACH
DATABASE
IMAGE
THE
EXTRACTED
FEATURES
ARE
STORED
IN
AN
EFFICIENT
INDEXING
STRUCTURE
SEE
CHAPTER
4
IN
ORDER
TO
ALLOW
EFFICIENT
RETRIEVAL
FROM
LARGE
IMAGE
DATABASES
CONTAINING
TO
1
IMAGES
GIVEN
A
USER
SPECIFIED
QUERY
REGION
IN
ONE
IMAGE
THE
SYSTEM
FIRST
RETRIEVES
A
SHORTLIST
OF
DATABASE
IMAGES
CONTAINING
MATCHING
FEATURES
AND
THEN
PERFORMS
A
GEOMETRIC
VERIFICATION
STEP
USING
RANSAC
WITH
AN
AFFINE
TRANSFORMATION
MODEL
IN
ORDER
TO
VERIFY
AND
RANK
MATCHING
REGIONS
IN
OTHER
IMAGES
4
MOBILE
VISUAL
SEARCH
4
MOBILE
VISUAL
SEARCH
A
PARTICULAR
FIELD
WHERE
LARGE
SCALE
CONTENT
BASED
IMAGE
RETRIEVAL
IS
ACTIVELY
USED
TODAY
IS
VISUAL
SEARCH
FROM
MOBILE
PHONES
HERE
THE
IDEA
IS
THAT
A
USER
TAKES
A
PHOTO
OF
AN
INTERESTING
OBJECT
FROM
HIS
HER
MOBILE
PHONE
AND
SENDS
IT
AS
A
QUERY
TO
A
RECOGNITION
SERVER
THE
SERVER
RECOGNIZES
THE
DEPICTED
OBJECT
AND
SENDS
BACK
OBJECT
SPECIFIC
CONTENT
TO
BE
DISPLAYED
ON
THE
MOBILE
DEVICE
ONE
OF
THE
FIRST
APPROACHES
TO
DEMONSTRATE
PRACTICAL
LARGE
SCALE
MOBILE
VISUAL
SEARCH
WAS
PRO
POSED
BY
NISTER
AND
STEWENIUS
THEIR
APPROACH
BASED
ON
LOCAL
FEATURES
AND
THE
VOCABULARY
TREE
INDEXING
SCHEME
DESCRIBED
IN
SECTION
4
1
1
COULD
RECOGNIZE
EXAMPLES
FROM
A
DATABASE
OF
000
CD
COVERS
IN
LESS
THAN
A
SECOND
WHILE
RUNNING
ON
A
SINGLE
LAPTOP
IN
THE
MEANTIME
A
NUMBER
OF
COMMERCIAL
SERVICES
HAVE
SPRUNG
UP
THAT
OFFER
MOBILE
VISUAL
SEARCH
CAPABILITIES
COVERING
DATABASES
OF
SEVERAL
MILLION
IMAGES
AMONG
THEM
GOOGLE
GOGGLES
WWW
GOOGLE
COM
MOBILE
GOGGLES
KOOABA
VISUAL
SEARCH
HTTP
WWW
KOOABA
COM
AND
AMAZON
REMEMBERS
ALMOST
ALL
SUCH
SERVICES
ARE
BASED
ON
THE
LOCAL
FEATURE
BASED
RECOGNITION
MATCHING
AND
GEOMETRIC
VERIFICATION
PIPELINE
DESCRIBED
IN
THE
PREVIOUS
CHAPTERS
AS
LARGE
DATABASES
HAVE
TO
BE
SEARCHED
SCALABLE
MATCHING
AND
INDEXING
TECHNIQUES
ARE
KEY
DESPITE
THE
LARGE
DATABASE
SIZES
THE
EMPLOYED
TECHNIQUES
HAVE
HOWEVER
BEEN
OPTIMIZED
SO
FAR
THAT
RESPONSE
TIMES
OF
1
2S
ARE
FEASIBLE
INCLUDING
FEATURE
EXTRACTION
MATCHING
AND
GEOMETRIC
VERIFICATION
BUT
WITHOUT
CONSIDERING
COMMUNICATION
DELAYS
FOR
IMAGE
TRANSMISSION
AS
A
RESULT
OF
THE
LOCAL
FEATURE
BASED
RECOGNITION
PIPELINE
THE
APPROACHES
WORK
PARTICULARLY
WELL
FOR
TEXTURED
LOCALLY
PLANAR
OBJECTS
SUCH
AS
BOOK
CD
DVD
COVERS
MOVIE
POSTERS
WINE
BOTTLE
LABELS
OR
BUILDING
FACADES
5
IMAGE
AUTO
ANNOTATION
AS
A
FINAL
APPLICATION
EXAMPLE
WE
PRESENT
AN
APPROACH
FOR
LARGE
SCALE
IMAGE
AUTO
TAGGING
GAMMETER
ET
AL
QUACK
ET
AL
I
E
FOR
DETECTION
OF
INTERESTING
OBJECTS
IN
CONSUMER
PHOTOS
AND
THE
AUTOMATIC
ASSIGNMENT
OF
MEANINGFUL
LABELS
OR
TAGS
TO
THOSE
OBJECTS
THIS
IS
VISUALIZED
IN
FIGURE
5
THE
APPROACH
BY
GAMMETER
ET
AL
QUACK
ET
AL
STARTS
BY
AUTOMATICALLY
MINING
GEOTAGGED
PHOTOS
FROM
INTERNET
PHOTO
COLLECTIONS
AND
ROUGHLY
BINS
THEM
INTO
GEOSPATIAL
GRID
CELLS
BY
THEIR
GEOTAGS
THE
IMAGES
IN
EACH
CELL
ARE
THEN
MATCHED
IN
ORDER
TO
FIND
CLUS
TERS
OF
IMAGES
SHOWING
THE
SAME
BUILDINGS
USING
SURF
FEATURES
AND
RANSAC
WITH
A
HOMOGRAPHY
MODEL
WHICH
ARE
THEN
ALSO
AUTOMATICALLY
LINKED
TO
WIKIPEDIA
PAGES
THROUGH
THEIR
TAGS
GIVEN
A
NOVEL
TEST
IMAGE
THE
PREVIOUSLY
EXTRACTED
IMAGE
CLUSTERS
ARE
USED
AS
BEACONS
AGAINST
WHICH
THE
TEST
IMAGE
IS
MATCHED
AGAIN
USING
SURF
FEATURES
AND
RANSAC
WITH
A
HOMOGRAPHY
MODEL
IF
A
MATCH
CAN
BE
ESTABLISHED
THE
MATCHING
IMAGE
REGION
IS
AUTOMATICALLY
ANNOTATED
WITH
THE
BUILDING
NAME
LOCATION
AND
WITH
A
LINK
TO
THE
ASSOCIATED
WEB
CONTENT
